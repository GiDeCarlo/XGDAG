{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "import networkx as nx\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATv2Conv\n",
    "from torch_geometric.nn.conv import GCNConv\n",
    "from torch_geometric.nn.conv import SAGEConv\n",
    "from torch_geometric.utils import from_networkx\n",
    "from torch_geometric.data import InMemoryDataset\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GNN(s)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN2L_GAT (nn.Module):\n",
    "    def __init__(self, data):\n",
    "        super().__init__()\n",
    "        self.conv1 = GATv2Conv(data.num_features, 16, dropout=0.3)\n",
    "        self.conv2 = GATv2Conv(16, int(data.num_classes))\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "class GNN4L_GAT (nn.Module):\n",
    "    def __init__(self, data):\n",
    "        super().__init__()\n",
    "        self.conv1 = GATv2Conv(data.num_features, 16)\n",
    "        self.conv2 = GATv2Conv(16, 16)\n",
    "        self.conv3 = GATv2Conv(16, 16)\n",
    "        self.conv4 = GATv2Conv(16, int(data.num_classes), dropout=0.3)\n",
    "    \n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        \n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        x = F.relu(self.conv3(x, edge_index))\n",
    "        x = self.conv4(x, edge_index)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "class GNN7L_GAT (nn.Module):\n",
    "    def __init__(self, data):\n",
    "        super().__init__()\n",
    "        self.conv1 = GATv2Conv(data.num_features, 16)\n",
    "        self.conv2 = GATv2Conv(16, 16)\n",
    "        self.conv3 = GATv2Conv(16, 16)\n",
    "        self.conv4 = GATv2Conv(16, 16)\n",
    "        self.conv5 = GATv2Conv(16, 16)\n",
    "        self.conv6 = GATv2Conv(16, 16)\n",
    "        self.conv7 = GATv2Conv(16, int(data.num_classes), dropout=0.3)\n",
    "    \n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        \n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        x = F.relu(self.conv3(x, edge_index))\n",
    "        x = F.relu(self.conv4(x, edge_index))\n",
    "        x = F.relu(self.conv5(x, edge_index))\n",
    "        x = F.relu(self.conv6(x, edge_index))\n",
    "        x = self.conv7(x, edge_index)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN2L_GCN (nn.Module):\n",
    "    def __init__(self, data):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(data.num_features, 16, improved=True)\n",
    "        self.conv2 = GCNConv(16, int(data.num_classes), improved=True)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "class GNN4L_GCN (nn.Module):\n",
    "    def __init__(self, data):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(data.num_features, 16, improved=True)\n",
    "        self.conv2 = GCNConv(16, 16, improved=True)\n",
    "        self.conv3 = GCNConv(16, 16, improved=True)\n",
    "        self.conv4 = GCNConv(16, int(data.num_classes), improved=True)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        x = F.relu(self.conv3(x, edge_index))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv4(x, edge_index)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "class GNN7L_GCN (nn.Module):\n",
    "    def __init__(self, data):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(data.num_features, 16, improved=True)\n",
    "        self.conv2 = GCNConv(16, 16, improved=True)\n",
    "        self.conv3 = GCNConv(16, 16, improved=True)\n",
    "        self.conv4 = GCNConv(16, 16, improved=True)\n",
    "        self.conv5 = GCNConv(16, 16, improved=True)\n",
    "        self.conv6 = GCNConv(16, 16, improved=True)\n",
    "        self.conv7 = GCNConv(16, int(data.num_classes), improved=True)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        x = F.relu(self.conv3(x, edge_index))\n",
    "        x = F.relu(self.conv4(x, edge_index))\n",
    "        x = F.relu(self.conv5(x, edge_index))\n",
    "        x = F.relu(self.conv6(x, edge_index))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv7(x, edge_index)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN2L_SAGE (nn.Module):\n",
    "    def __init__(self, data):\n",
    "        super().__init__()\n",
    "        self.conv1 = SAGEConv(data.num_features, 16, aggr='max')\n",
    "        self.conv2 = SAGEConv(16, 16, aggr='max')\n",
    "        self.conv3 = SAGEConv(16, 16, aggr='max')\n",
    "        self.conv4 = SAGEConv(16, int(data.num_classes), aggr='max')\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        x = F.relu(self.conv3(x, edge_index))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv4(x, edge_index)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "class GNN4L_SAGE (nn.Module):\n",
    "    def __init__(self, data):\n",
    "        super().__init__()\n",
    "        self.conv1 = SAGEConv(data.num_features, 16, aggr='max')\n",
    "        self.conv2 = SAGEConv(16, 16, aggr='max')\n",
    "        self.conv3 = SAGEConv(16, 16, aggr='max')\n",
    "        self.conv4 = SAGEConv(16, int(data.num_classes), aggr='max')\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        x = F.relu(self.conv3(x, edge_index))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv4(x, edge_index)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "class GNN7L_SAGE (nn.Module):\n",
    "    def __init__(self, data):\n",
    "        super().__init__()\n",
    "        self.conv1 = SAGEConv(data.num_features, 16, aggr='max')\n",
    "        self.conv2 = SAGEConv(16, 16, aggr='max')\n",
    "        self.conv3 = SAGEConv(16, 16, aggr='max')\n",
    "        self.conv4 = SAGEConv(16, 16, aggr='max')\n",
    "        self.conv5 = SAGEConv(16, 16, aggr='max')\n",
    "        self.conv6 = SAGEConv(16, 16, aggr='max')\n",
    "        self.conv7 = SAGEConv(16, int(data.num_classes), aggr='max')\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        x = F.relu(self.conv3(x, edge_index))\n",
    "        x = F.relu(self.conv4(x, edge_index))\n",
    "        x = F.relu(self.conv5(x, edge_index))\n",
    "        x = F.relu(self.conv6(x, edge_index))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv7(x, edge_index)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset class: it allows to translate a vector (Graph, Attributes, Labels)\n",
    "# into a dataset compatible with the PyTorch models.\n",
    "# \n",
    "# Parameters:\n",
    "# - G: NetworkX graph\n",
    "# - Labels: of the nodes used for classification\n",
    "# - attributes: List of the nodes' attributes\n",
    "\n",
    "class MyDataset(InMemoryDataset):\n",
    "  def __init__(self, G, labels, attributes, num_classes=2):\n",
    "    super(MyDataset, self).__init__('.', None, None, None)\n",
    "\n",
    "    # import data from the networkx graph with the attributes of the nodes\n",
    "    data = from_networkx(G, attributes)\n",
    "      \n",
    "    y = torch.from_numpy(labels).type(torch.long)\n",
    "\n",
    "    data.x = data.x.float()\n",
    "    data.y = y.clone().detach()\n",
    "    data.num_classes = num_classes\n",
    "\n",
    "    # Using train_test_split function from sklearn to stratify train/test/val sets\n",
    "    indices = range(G.number_of_nodes())\n",
    "    # Stratified split of train/test/val sets. Returned indices are used to create the masks\n",
    "    X_train, X_test, y_train, y_test, train_idx, test_idx = train_test_split(data.x, data.y, indices, test_size=0.3, stratify=labels, random_state=42)\n",
    "    # To create validation set, test set is splitted in half\n",
    "    X_test, X_val, y_test, y_val, test_idx, val_idx = train_test_split(X_test, y_test, test_idx, test_size=0.5, stratify=y_test, random_state=42)\n",
    "\n",
    "    n_nodes = G.number_of_nodes()\n",
    "    train_mask  = torch.zeros(n_nodes, dtype=torch.bool)\n",
    "    test_mask   = torch.zeros(n_nodes, dtype=torch.bool)\n",
    "    val_mask    = torch.zeros(n_nodes, dtype=torch.bool)\n",
    "    \n",
    "    for idx in train_idx:\n",
    "      train_mask[idx] = True\n",
    "\n",
    "    for idx in test_idx:\n",
    "      test_mask[idx] = True\n",
    "    \n",
    "    for idx in val_idx:\n",
    "      val_mask[idx] = True\n",
    "\n",
    "    data['train_mask']  = train_mask\n",
    "    data['test_mask']   = test_mask\n",
    "    data['val_mask']    = val_mask\n",
    "\n",
    "    self.data, self.slices = self.collate([data])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data, epochs = 200, classes = ['0','1'], lr = 0.001, weight_decay=0, cm_title = 'GNN', arch='GAT', layers=2):\n",
    "    title = cm_title + '_' + str(epochs) + '_' + str(weight_decay).replace('.', '_')\n",
    "\n",
    "    model_path  = '../Models/' + title\n",
    "    image_path  = '../Images/' + title\n",
    "    report_path = '../Reports/' + title + '.csv'\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    train_mask  = data['train_mask']\n",
    "    test_mask   = data['test_mask']\n",
    "    val_mask    = data['val_mask']\n",
    "\n",
    "    labels    = data.y\n",
    "    output = ''\n",
    "\n",
    "    # list to plot the train accuracy\n",
    "    train_acc_curve = []\n",
    "    train_lss_curve = []\n",
    "\n",
    "    best_train_acc  = 0\n",
    "    best_val_acc    = 0\n",
    "    # best_test_acc   = 0\n",
    "    best_train_lss  = 999\n",
    "\n",
    "    for e in tqdm(range(epochs+1)):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        logits      = model(data)\n",
    "        output      = logits.argmax(1)\n",
    "        # train_loss  = F.cross_entropy(logits[train_mask], labels[train_mask])\n",
    "        train_loss  = F.nll_loss(logits[train_mask], labels[train_mask])\n",
    "        train_acc   = (output[train_mask] == labels[train_mask]).float().mean()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Append train acc. to plot curve later\n",
    "        train_acc_curve.append(train_acc.item())\n",
    "        train_lss_curve.append(train_loss.item())\n",
    "\n",
    "        if train_acc > best_train_acc:\n",
    "            best_train_acc = train_acc\n",
    "\n",
    "        # Evaluation and test\n",
    "        model.eval()\n",
    "        logits      = model(data)\n",
    "        output      = logits.argmax(1)\n",
    "        # val_loss    = F.cross_entropy(logits[val_mask], labels[val_mask])\n",
    "        val_loss    = F.nll_loss(logits[val_mask], labels[val_mask])\n",
    "        val_acc     = (output[val_mask] == labels[val_mask]).float().mean()\n",
    "        # test_acc    = (output[test_mask] == labels[test_mask]).float().mean()\n",
    "\n",
    "        # Update best test/val acc.\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "        \n",
    "        # if test_acc > best_test_acc:\n",
    "        #     best_test_acc = test_acc\n",
    "        \n",
    "        # Save model with best train loss\n",
    "        if train_loss < best_train_lss:\n",
    "            best_train_lss = train_loss\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "\n",
    "        if e % 20 == 0 or e == epochs:\n",
    "            print('[Epoch: {:04d}]'.format(e),\n",
    "            'train loss: {:.4f},'.format(train_loss.item()),\n",
    "            'train acc: {:.4f},'.format(train_acc.item()),\n",
    "            'val loss: {:.4f},'.format(val_loss.item()),\n",
    "            'val acc: {:.4f} '.format(val_acc.item()),\n",
    "            '(best train acc: {:.4f},'.format(best_train_acc.item()),\n",
    "            'best val acc: {:.4f})'.format(best_val_acc.item()))\n",
    "    \n",
    "    # Plot training accuracy curve\n",
    "    plt.figure(figsize = (12,7))\n",
    "    plt.plot(train_acc_curve)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Train Accuracy')\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize = (12,7))\n",
    "    plt.plot(train_lss_curve)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Train Loss')\n",
    "    plt.show()\n",
    "\n",
    "    # Load best model\n",
    "    if layers == 2:\n",
    "        if arch == 'GAT':\n",
    "            loaded_model = GNN2L_GAT(data).to(device)\n",
    "        elif arch == 'GCN':\n",
    "            loaded_model = GNN2L_GCN(data).to(device)\n",
    "        else:\n",
    "            loaded_model = GNN2L_SAGE(data).to(device)\n",
    "    elif layers == 4:\n",
    "        if arch == 'GAT':\n",
    "            loaded_model = GNN4L_GAT(data).to(device)\n",
    "        elif arch == 'GCN':\n",
    "            loaded_model = GNN4L_GCN(data).to(device)\n",
    "        else:\n",
    "            loaded_model = GNN4L_SAGE(data).to(device)\n",
    "    else:\n",
    "        if arch == 'GAT':\n",
    "            loaded_model = GNN7L_GAT(data).to(device)\n",
    "        elif arch == 'GCN':\n",
    "            loaded_model = GNN7L_GCN(data).to(device)\n",
    "        else:\n",
    "            loaded_model = GNN7L_SAGE(data).to(device)\n",
    "\n",
    "    loaded_model.load_state_dict(torch.load(model_path))\n",
    "    loaded_model.eval()\n",
    "    logits = loaded_model(data)\n",
    "    output = logits.argmax(1)\n",
    "\n",
    "    print(classification_report(labels[test_mask].to('cpu'), output[test_mask].to('cpu')))\n",
    "\n",
    "    class_report = classification_report(labels[test_mask].to('cpu'), output[test_mask].to('cpu'), output_dict=True)\n",
    "    classification_report_dataframe = pd.DataFrame(class_report)\n",
    "    classification_report_dataframe.to_csv(report_path)\n",
    "\n",
    "    #Confusion Matrix\n",
    "    norms = [None, \"true\"]\n",
    "    for norm in norms:\n",
    "        cm = confusion_matrix(labels[test_mask].to('cpu'), output[test_mask].to('cpu'), normalize=norm)\n",
    "\n",
    "        plt.figure(figsize=(7,7))\n",
    "        \n",
    "        if norm == \"true\":\n",
    "            sn.heatmap(cm, annot=True, fmt=\".3f\", linewidths=.5, square = True, cmap = 'BuPu', xticklabels = classes, yticklabels = classes)\n",
    "        else:\n",
    "            sn.heatmap(cm, annot=True, fmt=\".0f\", linewidths=.5, square = True, cmap = 'BuPu', xticklabels = classes, yticklabels = classes)\n",
    "        plt.title(cm_title)\n",
    "        plt.ylabel('Actual label')\n",
    "        plt.xlabel('Predicted label')\n",
    "\n",
    "        #if norm == None:\n",
    "        #    plt.savefig(image_path + '_notNorm.png')\n",
    "        #else:\n",
    "        #    plt.savefig(image_path + '_Norm.png')\n",
    "\n",
    "    return output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NeDBIT Multiclass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18736, 2)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "G = nx.read_gml('../Graphs/grafo_nedbit_C0006142.gml')\n",
    "\n",
    "seed_genes          = pd.read_csv('../Datasets/C0006142_seed_genes.txt', header=None, sep=' ')\n",
    "seed_genes.columns  = [\"name\", \"GDA Score\"]\n",
    "seeds_list          = seed_genes[\"name\"].values.tolist()\n",
    "\n",
    "nedbit_scores = pd.read_csv('../Datasets/C0006142_features_Score.csv')\n",
    "\n",
    "# Remove seed genes\n",
    "nedbit_scores_not_seed = nedbit_scores[~nedbit_scores['name'].isin(seeds_list)]\n",
    "print(nedbit_scores_not_seed.shape)\n",
    "\n",
    "# Sort scores for quartile division\n",
    "nedbit_scores_not_seed = nedbit_scores_not_seed.sort_values(by = \"out\", ascending = False)\n",
    "pseudo_labels = pd.qcut(x = nedbit_scores_not_seed[\"out\"], q = 4, labels = [\"RN\", \"LN\", \"WN\", \"LP\"])\n",
    "\n",
    "plt.hist(pseudo_labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19761"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nedbit_scores_not_seed['label'] = pseudo_labels\n",
    "\n",
    "nedbit_scores_seed = nedbit_scores[nedbit_scores['name'].isin(seeds_list)]\n",
    "nedbit_scores_seed = nedbit_scores_seed.assign(label = 'P')\n",
    "\n",
    "# Convert dataframe to dict for searching nodes and their labels\n",
    "not_seed_labels = dict(zip(nedbit_scores_not_seed['name'], nedbit_scores_not_seed['label']))\n",
    "seed_labels     = dict(zip(nedbit_scores_seed['name'], nedbit_scores_seed['label']))\n",
    "\n",
    "labels_dict = {'P':0, 'LP': 1, 'WN': 2, 'LN': 3, 'RN': 4}\n",
    "labels = []\n",
    "\n",
    "for node in G:\n",
    "    if node in not_seed_labels:\n",
    "        labels.append(labels_dict[not_seed_labels[node]])\n",
    "    else:\n",
    "        labels.append(labels_dict[seed_labels[node]])\n",
    "\n",
    "labels = np.asarray(labels)\n",
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD4CAYAAAAAczaOAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAAOv0lEQVR4nO3dcaid9X3H8fen0Vqhkyq5upCb7ToIY1FoqyHLEIbUglktjX9USKE1DEeYWGjZoIv9Y6V/BPyrFMd0SFuMtKsEWmawlSFppRSc7tra2pg6s+k0GEzqaGvZcGi/++P8Boebc+85N957Tszv/YLDec73+T3n+Z6fJ588eZ5zjqkqJEl9eNesG5AkTY+hL0kdMfQlqSOGviR1xNCXpI5cMOsGxtm4cWMtLCzMug1Jekd56qmnflFVc0vr53zoLywssLi4OOs2JOkdJcl/jqp7ekeSOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpyzn8jV6uzsP87M9nvi3fdNJP9ztKs5hpmN989vr/Ot9fskb4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOTBz6STYk+XGSh9vjy5I8muT5dn/p0Ng7kxxP8lySG4fq1yZ5pq27O0nW9uVIklaymiP9zwDHhh7vB45U1VbgSHtMkm3AHuAqYBdwT5INbZt7gX3A1nbb9ba6lyStykShn2QeuAn4ylB5N3CwLR8Ebh6qP1hVb1TVC8BxYEeSTcAlVfV4VRXwwNA2kqQpmPRI/8vA54DfDtWuqKqTAO3+8lbfDLw8NO5Eq21uy0vrZ0iyL8liksXTp09P2KIkaZyxoZ/ko8CpqnpqwuccdZ6+VqifWay6r6q2V9X2ubm5CXcrSRrnggnGXAd8LMlHgPcAlyT5OvBqkk1VdbKdujnVxp8AtgxtPw+80urzI+qSpCkZe6RfVXdW1XxVLTC4QPu9qvokcBjY24btBR5qy4eBPUkuSnIlgwu2T7ZTQK8n2dk+tXPr0DaSpCmY5Eh/OXcBh5LcBrwE3AJQVUeTHAKeBd4E7qiqt9o2twP3AxcDj7SbJGlKVhX6VfUY8Fhbfg24YZlxB4ADI+qLwNWrbVKStDb8Rq4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktSRsaGf5D1JnkzykyRHk3yx1S9L8miS59v9pUPb3JnkeJLnktw4VL82yTNt3d1Jsj4vS5I0yiRH+m8AH6qq9wMfAHYl2QnsB45U1VbgSHtMkm3AHuAqYBdwT5IN7bnuBfYBW9tt19q9FEnSOGNDvwZ+0x5e2G4F7AYOtvpB4Oa2vBt4sKreqKoXgOPAjiSbgEuq6vGqKuCBoW0kSVMw0Tn9JBuSPA2cAh6tqieAK6rqJEC7v7wN3wy8PLT5iVbb3JaX1kftb1+SxSSLp0+fXsXLkSStZKLQr6q3quoDwDyDo/arVxg+6jx9rVAftb/7qmp7VW2fm5ubpEVJ0gRW9emdqvol8BiDc/GvtlM2tPtTbdgJYMvQZvPAK60+P6IuSZqSST69M5fkfW35YuDDwM+Bw8DeNmwv8FBbPgzsSXJRkisZXLB9sp0Cej3JzvapnVuHtpEkTcEFE4zZBBxsn8B5F3Coqh5O8jhwKMltwEvALQBVdTTJIeBZ4E3gjqp6qz3X7cD9wMXAI+0mSZqSsaFfVT8FPjii/hpwwzLbHAAOjKgvAitdD5AkrSO/kStJHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHVkbOgn2ZLk+0mOJTma5DOtflmSR5M83+4vHdrmziTHkzyX5Mah+rVJnmnr7k6S9XlZkqRRJjnSfxP466r6I2AncEeSbcB+4EhVbQWOtMe0dXuAq4BdwD1JNrTnuhfYB2xtt11r+FokSWOMDf2qOllVP2rLrwPHgM3AbuBgG3YQuLkt7wYerKo3quoF4DiwI8km4JKqeryqCnhgaBtJ0hSs6px+kgXgg8ATwBVVdRIGfzEAl7dhm4GXhzY70Wqb2/LS+qj97EuymGTx9OnTq2lRkrSCiUM/yXuBbwGfrapfrzR0RK1WqJ9ZrLqvqrZX1fa5ublJW5QkjTFR6Ce5kEHgf6Oqvt3Kr7ZTNrT7U61+AtgytPk88Eqrz4+oS5KmZJJP7wT4KnCsqr40tOowsLct7wUeGqrvSXJRkisZXLB9sp0Cej3Jzvactw5tI0maggsmGHMd8CngmSRPt9rngbuAQ0luA14CbgGoqqNJDgHPMvjkzx1V9Vbb7nbgfuBi4JF2kyRNydjQr6ofMvp8PMANy2xzADgwor4IXL2aBiVJa8dv5EpSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0ZG/pJvpbkVJKfDdUuS/Jokufb/aVD6+5McjzJc0luHKpfm+SZtu7uJFn7lyNJWskkR/r3A7uW1PYDR6pqK3CkPSbJNmAPcFXb5p4kG9o29wL7gK3ttvQ5JUnr7IJxA6rqB0kWlpR3A9e35YPAY8DftPqDVfUG8EKS48COJC8Cl1TV4wBJHgBuBh55269gBQv7v7OeT7+sF++6aSb7laRxzvac/hVVdRKg3V/e6puBl4fGnWi1zW15aV2SNEVrfSF31Hn6WqE++kmSfUkWkyyePn16zZqTpN6dbei/mmQTQLs/1eongC1D4+aBV1p9fkR9pKq6r6q2V9X2ubm5s2xRkrTU2Yb+YWBvW94LPDRU35PkoiRXMrhg+2Q7BfR6kp3tUzu3Dm0jSZqSsRdyk3yTwUXbjUlOAF8A7gIOJbkNeAm4BaCqjiY5BDwLvAncUVVvtae6ncEngS5mcAF3XS/iSpLONMmndz6xzKoblhl/ADgwor4IXL2q7iRJa8pv5EpSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI1MP/SS7kjyX5HiS/dPevyT1bKqhn2QD8PfAnwHbgE8k2TbNHiSpZ9M+0t8BHK+q/6iq/wUeBHZPuQdJ6laqano7Sz4O7Kqqv2iPPwX8cVV9esm4fcC+9vAPgefOcpcbgV+c5bbryb5Wx75Wx75W53zt6/eram5p8YK38YRnIyNqZ/ytU1X3Afe97Z0li1W1/e0+z1qzr9Wxr9Wxr9Xpra9pn945AWwZejwPvDLlHiSpW9MO/X8Ftia5Msm7gT3A4Sn3IEndmurpnap6M8mngX8GNgBfq6qj67jLt32KaJ3Y1+rY1+rY1+p01ddUL+RKkmbLb+RKUkcMfUnqyHkR+uN+2iEDd7f1P01yzTnS1/VJfpXk6Xb72yn09LUkp5L8bJn1s5qrcX1Nfa7afrck+X6SY0mOJvnMiDFTn7MJ+5rF++s9SZ5M8pPW1xdHjJnFfE3S10zeY23fG5L8OMnDI9at7XxV1Tv6xuCC8L8DfwC8G/gJsG3JmI8AjzD4nsBO4IlzpK/rgYenPF9/ClwD/GyZ9VOfqwn7mvpctf1uAq5py78D/Ns58v6apK9ZvL8CvLctXwg8Aew8B+Zrkr5m8h5r+/4r4B9H7X+t5+t8ONKf5KcddgMP1MC/AO9Lsukc6GvqquoHwH+tMGQWczVJXzNRVSer6kdt+XXgGLB5ybCpz9mEfU1dm4PftIcXttvST4vMYr4m6WsmkswDNwFfWWbIms7X+RD6m4GXhx6f4Mw3/yRjZtEXwJ+0f3I+kuSqde5pErOYq0nNdK6SLAAfZHCUOGymc7ZCXzCDOWunKp4GTgGPVtU5MV8T9AWzeY99Gfgc8Ntl1q/pfJ0PoT/JTztM9PMPa2ySff6Iwe9jvB/4O+Cf1rmnScxiriYx07lK8l7gW8Bnq+rXS1eP2GQqczamr5nMWVW9VVUfYPCN+x1Jrl4yZCbzNUFfU5+vJB8FTlXVUysNG1E76/k6H0J/kp92mMXPP4zdZ1X9+v//yVlV3wUuTLJxnfsa55z8qYxZzlWSCxkE6zeq6tsjhsxkzsb1Nev3V1X9EngM2LVk1UzfY8v1NaP5ug74WJIXGZwC/lCSry8Zs6bzdT6E/iQ/7XAYuLVdBd8J/KqqTs66ryS/myRteQeD/x6vrXNf48xirsaa1Vy1fX4VOFZVX1pm2NTnbJK+ZjFnSeaSvK8tXwx8GPj5kmGzmK+xfc1ivqrqzqqar6oFBhnxvar65JJhazpf0/6VzTVXy/y0Q5K/bOv/Afgugyvgx4H/Bv78HOnr48DtSd4E/gfYU+1y/XpJ8k0Gn1LYmOQE8AUGF7VmNlcT9jX1uWquAz4FPNPOBwN8Hvi9od5mMWeT9DWLOdsEHMzgf5j0LuBQVT086z+PE/Y1q/fYGdZzvvwZBknqyPlwekeSNCFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXk/wAFp4Zt8hthegAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "attributes = ['degree', 'ring', 'NetRank', 'NetShort', 'HeatDiff', 'InfoDiff']\n",
    "\n",
    "dataset_with_nedbit = MyDataset(G, labels, attributes, num_classes=5)\n",
    "data_with_nedbit = dataset_with_nedbit[0]\n",
    "\n",
    "plt.hist(labels)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bfa4f5f4f64441980509da148d5c73e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40001 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch: 0000] train loss: 2316.9851, train acc: 0.2370, val loss: 1583.0420, val acc: 0.2364  (best train acc: 0.2370, best val acc: 0.2364)\n",
      "[Epoch: 0020] train loss: 1066.7542, train acc: 0.2344, val loss: 1038.7040, val acc: 0.2368  (best train acc: 0.3477, best val acc: 0.3528)\n",
      "[Epoch: 0040] train loss: 780.5931, train acc: 0.2405, val loss: 756.3060, val acc: 0.2368  (best train acc: 0.3477, best val acc: 0.3528)\n",
      "[Epoch: 0060] train loss: 568.3892, train acc: 0.2250, val loss: 552.0001, val acc: 0.2266  (best train acc: 0.3477, best val acc: 0.3528)\n",
      "[Epoch: 0080] train loss: 278.3428, train acc: 0.2092, val loss: 335.0265, val acc: 0.2067  (best train acc: 0.3477, best val acc: 0.3528)\n",
      "[Epoch: 0100] train loss: 42.5660, train acc: 0.2339, val loss: 195.8217, val acc: 0.0971  (best train acc: 0.3477, best val acc: 0.3528)\n",
      "[Epoch: 0120] train loss: 37.1442, train acc: 0.2600, val loss: 290.1096, val acc: 0.1396  (best train acc: 0.3477, best val acc: 0.3528)\n",
      "[Epoch: 0140] train loss: 32.8991, train acc: 0.2611, val loss: 313.7754, val acc: 0.1585  (best train acc: 0.3477, best val acc: 0.3528)\n",
      "[Epoch: 0160] train loss: 32.9349, train acc: 0.2637, val loss: 321.5175, val acc: 0.1599  (best train acc: 0.3477, best val acc: 0.3528)\n",
      "[Epoch: 0180] train loss: 29.1891, train acc: 0.2689, val loss: 329.5266, val acc: 0.1636  (best train acc: 0.3477, best val acc: 0.3528)\n",
      "[Epoch: 0200] train loss: 25.7749, train acc: 0.2661, val loss: 335.5252, val acc: 0.1659  (best train acc: 0.3477, best val acc: 0.3528)\n",
      "[Epoch: 0220] train loss: 21.9431, train acc: 0.2715, val loss: 342.2498, val acc: 0.1673  (best train acc: 0.3477, best val acc: 0.3528)\n",
      "[Epoch: 0240] train loss: 22.7712, train acc: 0.2730, val loss: 346.7657, val acc: 0.1663  (best train acc: 0.3477, best val acc: 0.3528)\n",
      "[Epoch: 0260] train loss: 18.0962, train acc: 0.2672, val loss: 343.7685, val acc: 0.1734  (best train acc: 0.3477, best val acc: 0.3528)\n",
      "[Epoch: 0280] train loss: 16.5799, train acc: 0.2674, val loss: 340.4958, val acc: 0.1750  (best train acc: 0.3477, best val acc: 0.3528)\n",
      "[Epoch: 0300] train loss: 19.6440, train acc: 0.2681, val loss: 334.6162, val acc: 0.1791  (best train acc: 0.3477, best val acc: 0.3528)\n",
      "[Epoch: 0320] train loss: 15.6373, train acc: 0.2681, val loss: 327.9655, val acc: 0.1811  (best train acc: 0.3477, best val acc: 0.3528)\n",
      "[Epoch: 0340] train loss: 14.4077, train acc: 0.2649, val loss: 319.7522, val acc: 0.1852  (best train acc: 0.3477, best val acc: 0.3528)\n",
      "[Epoch: 0360] train loss: 13.6616, train acc: 0.2774, val loss: 331.2878, val acc: 0.1872  (best train acc: 0.3477, best val acc: 0.3528)\n",
      "[Epoch: 0380] train loss: 15.5587, train acc: 0.2789, val loss: 320.1061, val acc: 0.1889  (best train acc: 0.3477, best val acc: 0.3528)\n",
      "[Epoch: 0400] train loss: 17.8386, train acc: 0.2814, val loss: 419.3909, val acc: 0.2118  (best train acc: 0.3477, best val acc: 0.3528)\n",
      "[Epoch: 0420] train loss: 12.4081, train acc: 0.2736, val loss: 393.8672, val acc: 0.2132  (best train acc: 0.3477, best val acc: 0.3528)\n",
      "[Epoch: 0440] train loss: 17.1558, train acc: 0.2701, val loss: 382.2922, val acc: 0.2135  (best train acc: 0.3477, best val acc: 0.3528)\n",
      "[Epoch: 0460] train loss: 17.6647, train acc: 0.2714, val loss: 367.4641, val acc: 0.2142  (best train acc: 0.3477, best val acc: 0.3528)\n",
      "[Epoch: 0480] train loss: 17.1587, train acc: 0.2735, val loss: 352.8504, val acc: 0.2118  (best train acc: 0.3477, best val acc: 0.3528)\n",
      "[Epoch: 0500] train loss: 17.3284, train acc: 0.2804, val loss: 339.2156, val acc: 0.2108  (best train acc: 0.3477, best val acc: 0.3528)\n",
      "[Epoch: 0520] train loss: 12.7183, train acc: 0.2754, val loss: 326.4957, val acc: 0.2118  (best train acc: 0.3477, best val acc: 0.3528)\n",
      "[Epoch: 0540] train loss: 8.7631, train acc: 0.2681, val loss: 313.4950, val acc: 0.2121  (best train acc: 0.3477, best val acc: 0.3528)\n",
      "[Epoch: 0560] train loss: 13.4959, train acc: 0.2754, val loss: 307.3463, val acc: 0.2179  (best train acc: 0.3477, best val acc: 0.3528)\n",
      "[Epoch: 0580] train loss: 16.7760, train acc: 0.2820, val loss: 298.5885, val acc: 0.2105  (best train acc: 0.3477, best val acc: 0.3528)\n",
      "[Epoch: 0600] train loss: 9.9317, train acc: 0.2734, val loss: 289.6565, val acc: 0.2138  (best train acc: 0.3477, best val acc: 0.3528)\n",
      "[Epoch: 0620] train loss: 15.2922, train acc: 0.2844, val loss: 281.8468, val acc: 0.2179  (best train acc: 0.3477, best val acc: 0.3528)\n",
      "[Epoch: 0640] train loss: 12.0506, train acc: 0.2690, val loss: 271.2370, val acc: 0.2138  (best train acc: 0.3477, best val acc: 0.3528)\n",
      "[Epoch: 0660] train loss: 13.5055, train acc: 0.2647, val loss: 260.2094, val acc: 0.2138  (best train acc: 0.3477, best val acc: 0.3528)\n",
      "[Epoch: 0680] train loss: 12.6826, train acc: 0.2706, val loss: 247.4792, val acc: 0.2108  (best train acc: 0.3477, best val acc: 0.3528)\n",
      "[Epoch: 0700] train loss: 10.8139, train acc: 0.2658, val loss: 238.0390, val acc: 0.2192  (best train acc: 0.3477, best val acc: 0.3528)\n",
      "[Epoch: 0720] train loss: 10.9053, train acc: 0.2793, val loss: 225.8107, val acc: 0.2148  (best train acc: 0.3477, best val acc: 0.3528)\n",
      "[Epoch: 0740] train loss: 10.0398, train acc: 0.2692, val loss: 214.5627, val acc: 0.2155  (best train acc: 0.3477, best val acc: 0.3528)\n",
      "[Epoch: 0760] train loss: 6.2023, train acc: 0.2632, val loss: 202.5310, val acc: 0.2199  (best train acc: 0.3477, best val acc: 0.3528)\n",
      "[Epoch: 0780] train loss: 10.2966, train acc: 0.2728, val loss: 167.7649, val acc: 0.2266  (best train acc: 0.3477, best val acc: 0.3528)\n",
      "[Epoch: 0800] train loss: 7.2717, train acc: 0.2765, val loss: 157.7937, val acc: 0.2287  (best train acc: 0.3477, best val acc: 0.3528)\n",
      "[Epoch: 0820] train loss: 5.9866, train acc: 0.2769, val loss: 145.3603, val acc: 0.2266  (best train acc: 0.3477, best val acc: 0.3528)\n",
      "[Epoch: 0840] train loss: 7.7945, train acc: 0.2853, val loss: 135.1427, val acc: 0.2273  (best train acc: 0.3477, best val acc: 0.3528)\n",
      "[Epoch: 0860] train loss: 8.6990, train acc: 0.2433, val loss: 138.3606, val acc: 0.2300  (best train acc: 0.3477, best val acc: 0.3528)\n",
      "[Epoch: 0880] train loss: 9.8944, train acc: 0.3060, val loss: 194.6378, val acc: 0.2371  (best train acc: 0.3477, best val acc: 0.3528)\n",
      "[Epoch: 0900] train loss: 7.7383, train acc: 0.2781, val loss: 148.6076, val acc: 0.2206  (best train acc: 0.3477, best val acc: 0.3528)\n",
      "[Epoch: 0920] train loss: 8.3036, train acc: 0.2590, val loss: 137.2656, val acc: 0.2304  (best train acc: 0.3477, best val acc: 0.3528)\n",
      "[Epoch: 0940] train loss: 7.7524, train acc: 0.2731, val loss: 125.1007, val acc: 0.2310  (best train acc: 0.3477, best val acc: 0.3528)\n",
      "[Epoch: 0960] train loss: 7.3447, train acc: 0.2757, val loss: 113.2926, val acc: 0.2300  (best train acc: 0.3477, best val acc: 0.3528)\n",
      "[Epoch: 0980] train loss: 7.3344, train acc: 0.2802, val loss: 94.3263, val acc: 0.2479  (best train acc: 0.3477, best val acc: 0.3528)\n",
      "[Epoch: 1000] train loss: 7.5512, train acc: 0.2774, val loss: 81.0224, val acc: 0.2513  (best train acc: 0.3477, best val acc: 0.3528)\n",
      "[Epoch: 1020] train loss: 5.1212, train acc: 0.2710, val loss: 64.9517, val acc: 0.2513  (best train acc: 0.3477, best val acc: 0.3528)\n",
      "[Epoch: 1040] train loss: 4.8946, train acc: 0.2666, val loss: 50.3964, val acc: 0.2594  (best train acc: 0.3477, best val acc: 0.3528)\n",
      "[Epoch: 1060] train loss: 3.8554, train acc: 0.2706, val loss: 36.5456, val acc: 0.2506  (best train acc: 0.3477, best val acc: 0.3528)\n",
      "[Epoch: 1080] train loss: 4.6177, train acc: 0.2794, val loss: 35.3068, val acc: 0.2641  (best train acc: 0.3477, best val acc: 0.3528)\n",
      "[Epoch: 1100] train loss: 2.6538, train acc: 0.2809, val loss: 31.9045, val acc: 0.2509  (best train acc: 0.3477, best val acc: 0.3528)\n",
      "[Epoch: 1120] train loss: 3.6783, train acc: 0.2858, val loss: 33.9559, val acc: 0.2506  (best train acc: 0.3477, best val acc: 0.3528)\n",
      "[Epoch: 1140] train loss: 3.7674, train acc: 0.2874, val loss: 33.3898, val acc: 0.2455  (best train acc: 0.3477, best val acc: 0.3528)\n",
      "[Epoch: 1160] train loss: 3.9146, train acc: 0.2906, val loss: 31.5723, val acc: 0.2503  (best train acc: 0.3477, best val acc: 0.3528)\n",
      "[Epoch: 1180] train loss: 3.9930, train acc: 0.2848, val loss: 37.0984, val acc: 0.2479  (best train acc: 0.3477, best val acc: 0.3528)\n",
      "[Epoch: 1200] train loss: 4.1534, train acc: 0.2883, val loss: 35.7691, val acc: 0.2415  (best train acc: 0.3477, best val acc: 0.3528)\n",
      "[Epoch: 1220] train loss: 3.9273, train acc: 0.2410, val loss: 43.8678, val acc: 0.2442  (best train acc: 0.3477, best val acc: 0.3528)\n",
      "[Epoch: 1240] train loss: 3.1559, train acc: 0.2773, val loss: 34.4101, val acc: 0.2472  (best train acc: 0.3477, best val acc: 0.3528)\n",
      "[Epoch: 1260] train loss: 3.4000, train acc: 0.2860, val loss: 26.4022, val acc: 0.2304  (best train acc: 0.3477, best val acc: 0.3528)\n",
      "[Epoch: 1280] train loss: 3.0408, train acc: 0.2858, val loss: 26.6646, val acc: 0.2351  (best train acc: 0.3477, best val acc: 0.3528)\n",
      "[Epoch: 1300] train loss: 2.9768, train acc: 0.2880, val loss: 25.2611, val acc: 0.2395  (best train acc: 0.3477, best val acc: 0.3528)\n",
      "[Epoch: 1320] train loss: 2.9632, train acc: 0.2843, val loss: 23.7233, val acc: 0.2378  (best train acc: 0.3477, best val acc: 0.3528)\n",
      "[Epoch: 1340] train loss: 3.3490, train acc: 0.2911, val loss: 22.8262, val acc: 0.2705  (best train acc: 0.3477, best val acc: 0.3528)\n",
      "[Epoch: 1360] train loss: 2.8433, train acc: 0.2976, val loss: 23.2542, val acc: 0.2358  (best train acc: 0.3477, best val acc: 0.3528)\n",
      "[Epoch: 1380] train loss: 3.4571, train acc: 0.2866, val loss: 22.2650, val acc: 0.2358  (best train acc: 0.3477, best val acc: 0.3528)\n",
      "[Epoch: 1400] train loss: 2.6361, train acc: 0.2953, val loss: 23.3990, val acc: 0.2327  (best train acc: 0.3477, best val acc: 0.3528)\n",
      "[Epoch: 1420] train loss: 3.1664, train acc: 0.2700, val loss: 23.9604, val acc: 0.2351  (best train acc: 0.3477, best val acc: 0.3528)\n",
      "[Epoch: 1440] train loss: 3.2850, train acc: 0.3057, val loss: 22.9565, val acc: 0.2354  (best train acc: 0.3477, best val acc: 0.3528)\n",
      "[Epoch: 1460] train loss: 3.4473, train acc: 0.2791, val loss: 29.8715, val acc: 0.2320  (best train acc: 0.3477, best val acc: 0.3528)\n",
      "[Epoch: 1480] train loss: 3.8835, train acc: 0.2792, val loss: 26.5157, val acc: 0.2364  (best train acc: 0.3477, best val acc: 0.3528)\n",
      "[Epoch: 1500] train loss: 3.1551, train acc: 0.2937, val loss: 29.0368, val acc: 0.2344  (best train acc: 0.3477, best val acc: 0.3528)\n",
      "[Epoch: 1520] train loss: 2.4493, train acc: 0.2817, val loss: 23.1766, val acc: 0.2408  (best train acc: 0.3477, best val acc: 0.3528)\n",
      "[Epoch: 1540] train loss: 2.3722, train acc: 0.2782, val loss: 21.9816, val acc: 0.1909  (best train acc: 0.3477, best val acc: 0.3528)\n",
      "[Epoch: 1560] train loss: 2.9442, train acc: 0.2907, val loss: 21.4408, val acc: 0.2270  (best train acc: 0.3477, best val acc: 0.3528)\n",
      "[Epoch: 1580] train loss: 2.2706, train acc: 0.3058, val loss: 20.1240, val acc: 0.2283  (best train acc: 0.3477, best val acc: 0.3528)\n",
      "[Epoch: 1600] train loss: 1.9030, train acc: 0.3125, val loss: 19.1321, val acc: 0.2246  (best train acc: 0.3477, best val acc: 0.3528)\n",
      "[Epoch: 1620] train loss: 2.0576, train acc: 0.3195, val loss: 19.0661, val acc: 0.2243  (best train acc: 0.3477, best val acc: 0.3528)\n",
      "[Epoch: 1640] train loss: 2.5901, train acc: 0.3033, val loss: 19.3039, val acc: 0.2270  (best train acc: 0.3477, best val acc: 0.3528)\n",
      "[Epoch: 1660] train loss: 2.0184, train acc: 0.3005, val loss: 18.4379, val acc: 0.2304  (best train acc: 0.3477, best val acc: 0.3528)\n",
      "[Epoch: 1680] train loss: 2.6553, train acc: 0.3196, val loss: 17.8403, val acc: 0.2283  (best train acc: 0.3477, best val acc: 0.3528)\n",
      "[Epoch: 1700] train loss: 2.9861, train acc: 0.2814, val loss: 21.3103, val acc: 0.2300  (best train acc: 0.3477, best val acc: 0.3528)\n",
      "[Epoch: 1720] train loss: 3.0221, train acc: 0.3326, val loss: 17.9877, val acc: 0.2310  (best train acc: 0.3477, best val acc: 0.3528)\n",
      "[Epoch: 1740] train loss: 2.9486, train acc: 0.2779, val loss: 20.6416, val acc: 0.2273  (best train acc: 0.3587, best val acc: 0.3528)\n",
      "[Epoch: 1760] train loss: 2.1963, train acc: 0.2916, val loss: 19.7648, val acc: 0.2287  (best train acc: 0.3587, best val acc: 0.3528)\n",
      "[Epoch: 1780] train loss: 3.7270, train acc: 0.2963, val loss: 48.7534, val acc: 0.2364  (best train acc: 0.3587, best val acc: 0.3528)\n",
      "[Epoch: 1800] train loss: 4.2237, train acc: 0.2749, val loss: 44.7248, val acc: 0.2664  (best train acc: 0.3587, best val acc: 0.3528)\n",
      "[Epoch: 1820] train loss: 3.5575, train acc: 0.2900, val loss: 31.1871, val acc: 0.2658  (best train acc: 0.3587, best val acc: 0.3528)\n",
      "[Epoch: 1840] train loss: 3.2788, train acc: 0.3071, val loss: 21.3766, val acc: 0.2334  (best train acc: 0.3587, best val acc: 0.3528)\n",
      "[Epoch: 1860] train loss: 2.6428, train acc: 0.3034, val loss: 15.3887, val acc: 0.2334  (best train acc: 0.3587, best val acc: 0.3528)\n",
      "[Epoch: 1880] train loss: 2.5009, train acc: 0.2451, val loss: 13.2014, val acc: 0.2266  (best train acc: 0.3587, best val acc: 0.3528)\n",
      "[Epoch: 1900] train loss: 2.7327, train acc: 0.2871, val loss: 13.8478, val acc: 0.2253  (best train acc: 0.3587, best val acc: 0.3528)\n",
      "[Epoch: 1920] train loss: 2.1161, train acc: 0.2675, val loss: 12.9326, val acc: 0.2216  (best train acc: 0.3587, best val acc: 0.3528)\n",
      "[Epoch: 1940] train loss: 2.4409, train acc: 0.2855, val loss: 12.8646, val acc: 0.2263  (best train acc: 0.3587, best val acc: 0.3528)\n",
      "[Epoch: 1960] train loss: 2.2030, train acc: 0.2738, val loss: 13.4740, val acc: 0.2253  (best train acc: 0.3587, best val acc: 0.3528)\n",
      "[Epoch: 1980] train loss: 2.3779, train acc: 0.2559, val loss: 13.6477, val acc: 0.2320  (best train acc: 0.3587, best val acc: 0.3528)\n",
      "[Epoch: 2000] train loss: 2.3884, train acc: 0.2639, val loss: 13.4687, val acc: 0.1966  (best train acc: 0.3587, best val acc: 0.3528)\n",
      "[Epoch: 2020] train loss: 2.1685, train acc: 0.2723, val loss: 18.4782, val acc: 0.2314  (best train acc: 0.3587, best val acc: 0.3528)\n",
      "[Epoch: 2040] train loss: 2.0869, train acc: 0.3588, val loss: 15.7076, val acc: 0.2361  (best train acc: 0.3588, best val acc: 0.3528)\n",
      "[Epoch: 2060] train loss: 1.8628, train acc: 0.3037, val loss: 18.1982, val acc: 0.2351  (best train acc: 0.3588, best val acc: 0.3528)\n",
      "[Epoch: 2080] train loss: 1.9101, train acc: 0.2764, val loss: 13.9709, val acc: 0.2351  (best train acc: 0.3588, best val acc: 0.3528)\n",
      "[Epoch: 2100] train loss: 2.0657, train acc: 0.2744, val loss: 9.1776, val acc: 0.2132  (best train acc: 0.3588, best val acc: 0.3528)\n",
      "[Epoch: 2120] train loss: 1.9184, train acc: 0.2606, val loss: 9.3918, val acc: 0.2030  (best train acc: 0.3588, best val acc: 0.3528)\n",
      "[Epoch: 2140] train loss: 2.0704, train acc: 0.2812, val loss: 9.0020, val acc: 0.2243  (best train acc: 0.3588, best val acc: 0.3528)\n",
      "[Epoch: 2160] train loss: 1.7396, train acc: 0.2838, val loss: 6.9101, val acc: 0.2260  (best train acc: 0.3588, best val acc: 0.3528)\n",
      "[Epoch: 2180] train loss: 1.7398, train acc: 0.2945, val loss: 7.0684, val acc: 0.2364  (best train acc: 0.3588, best val acc: 0.3528)\n",
      "[Epoch: 2200] train loss: 3.3249, train acc: 0.2911, val loss: 45.2318, val acc: 0.2371  (best train acc: 0.3588, best val acc: 0.3528)\n",
      "[Epoch: 2220] train loss: 3.0915, train acc: 0.2736, val loss: 35.5659, val acc: 0.2921  (best train acc: 0.3588, best val acc: 0.3528)\n",
      "[Epoch: 2240] train loss: 2.6802, train acc: 0.2857, val loss: 16.0751, val acc: 0.2830  (best train acc: 0.3588, best val acc: 0.3528)\n",
      "[Epoch: 2260] train loss: 2.0596, train acc: 0.2771, val loss: 10.3895, val acc: 0.2553  (best train acc: 0.3588, best val acc: 0.3528)\n",
      "[Epoch: 2280] train loss: 1.7851, train acc: 0.2895, val loss: 5.7049, val acc: 0.2209  (best train acc: 0.3588, best val acc: 0.3528)\n",
      "[Epoch: 2300] train loss: 1.7012, train acc: 0.2738, val loss: 4.6405, val acc: 0.2425  (best train acc: 0.3588, best val acc: 0.3528)\n",
      "[Epoch: 2320] train loss: 2.2241, train acc: 0.2857, val loss: 22.8693, val acc: 0.2358  (best train acc: 0.3588, best val acc: 0.3528)\n",
      "[Epoch: 2340] train loss: 1.7999, train acc: 0.2885, val loss: 7.9227, val acc: 0.2297  (best train acc: 0.3588, best val acc: 0.3528)\n",
      "[Epoch: 2360] train loss: 1.5856, train acc: 0.2809, val loss: 4.5696, val acc: 0.2368  (best train acc: 0.3588, best val acc: 0.3528)\n",
      "[Epoch: 2380] train loss: 1.5784, train acc: 0.2627, val loss: 4.8007, val acc: 0.2371  (best train acc: 0.3588, best val acc: 0.3528)\n",
      "[Epoch: 2400] train loss: 1.6582, train acc: 0.2337, val loss: 4.3469, val acc: 0.2371  (best train acc: 0.3588, best val acc: 0.3528)\n",
      "[Epoch: 2420] train loss: 1.5585, train acc: 0.2604, val loss: 3.4720, val acc: 0.2334  (best train acc: 0.3588, best val acc: 0.3642)\n",
      "[Epoch: 2440] train loss: 1.5206, train acc: 0.3094, val loss: 3.6424, val acc: 0.2371  (best train acc: 0.3588, best val acc: 0.3642)\n",
      "[Epoch: 2460] train loss: 1.5210, train acc: 0.2953, val loss: 4.5111, val acc: 0.3363  (best train acc: 0.3588, best val acc: 0.3642)\n",
      "[Epoch: 2480] train loss: 1.8195, train acc: 0.2864, val loss: 9.1649, val acc: 0.2384  (best train acc: 0.3588, best val acc: 0.3642)\n",
      "[Epoch: 2500] train loss: 1.6918, train acc: 0.2615, val loss: 4.1810, val acc: 0.2479  (best train acc: 0.3588, best val acc: 0.3642)\n",
      "[Epoch: 2520] train loss: 1.6369, train acc: 0.2801, val loss: 9.2652, val acc: 0.2401  (best train acc: 0.3588, best val acc: 0.3642)\n",
      "[Epoch: 2540] train loss: 1.5866, train acc: 0.2877, val loss: 6.0601, val acc: 0.2371  (best train acc: 0.3588, best val acc: 0.3642)\n",
      "[Epoch: 2560] train loss: 1.9996, train acc: 0.2824, val loss: 16.9776, val acc: 0.2091  (best train acc: 0.3588, best val acc: 0.3727)\n",
      "[Epoch: 2580] train loss: 1.6747, train acc: 0.2964, val loss: 7.0938, val acc: 0.2371  (best train acc: 0.3588, best val acc: 0.3727)\n",
      "[Epoch: 2600] train loss: 1.5867, train acc: 0.2692, val loss: 6.6676, val acc: 0.2489  (best train acc: 0.3588, best val acc: 0.3727)\n",
      "[Epoch: 2620] train loss: 1.6466, train acc: 0.2893, val loss: 8.2031, val acc: 0.2371  (best train acc: 0.3588, best val acc: 0.3727)\n",
      "[Epoch: 2640] train loss: 6.3141, train acc: 0.2881, val loss: 123.4465, val acc: 0.2371  (best train acc: 0.3588, best val acc: 0.3727)\n",
      "[Epoch: 2660] train loss: 6.3393, train acc: 0.2804, val loss: 96.4503, val acc: 0.2371  (best train acc: 0.3588, best val acc: 0.3727)\n",
      "[Epoch: 2680] train loss: 2.9908, train acc: 0.2329, val loss: 42.4907, val acc: 0.0631  (best train acc: 0.3588, best val acc: 0.3727)\n",
      "[Epoch: 2700] train loss: 2.2495, train acc: 0.2958, val loss: 26.8547, val acc: 0.2820  (best train acc: 0.3588, best val acc: 0.3727)\n",
      "[Epoch: 2720] train loss: 1.6102, train acc: 0.2966, val loss: 5.4757, val acc: 0.2371  (best train acc: 0.3588, best val acc: 0.3727)\n",
      "[Epoch: 2740] train loss: 2.8706, train acc: 0.2945, val loss: 30.8818, val acc: 0.2371  (best train acc: 0.3588, best val acc: 0.3727)\n",
      "[Epoch: 2760] train loss: 1.8401, train acc: 0.2971, val loss: 5.9193, val acc: 0.2371  (best train acc: 0.3588, best val acc: 0.3727)\n",
      "[Epoch: 2780] train loss: 1.6463, train acc: 0.2958, val loss: 12.5806, val acc: 0.2452  (best train acc: 0.3588, best val acc: 0.3727)\n",
      "[Epoch: 2800] train loss: 2.1908, train acc: 0.2681, val loss: 23.5488, val acc: 0.2273  (best train acc: 0.3588, best val acc: 0.3727)\n",
      "[Epoch: 2820] train loss: 1.7545, train acc: 0.2930, val loss: 7.8162, val acc: 0.2371  (best train acc: 0.3588, best val acc: 0.3727)\n",
      "[Epoch: 2840] train loss: 1.7001, train acc: 0.2632, val loss: 8.9948, val acc: 0.2371  (best train acc: 0.3588, best val acc: 0.3727)\n",
      "[Epoch: 2860] train loss: 2.0384, train acc: 0.2484, val loss: 15.7880, val acc: 0.2371  (best train acc: 0.3588, best val acc: 0.3727)\n",
      "[Epoch: 2880] train loss: 1.8922, train acc: 0.2861, val loss: 10.0699, val acc: 0.2411  (best train acc: 0.3588, best val acc: 0.3727)\n",
      "[Epoch: 2900] train loss: 1.6709, train acc: 0.2834, val loss: 8.0353, val acc: 0.2371  (best train acc: 0.3588, best val acc: 0.3727)\n",
      "[Epoch: 2920] train loss: 2.0614, train acc: 0.2817, val loss: 14.8639, val acc: 0.2914  (best train acc: 0.3588, best val acc: 0.3727)\n",
      "[Epoch: 2940] train loss: 1.6237, train acc: 0.2874, val loss: 5.6482, val acc: 0.3626  (best train acc: 0.3588, best val acc: 0.3727)\n",
      "[Epoch: 2960] train loss: 1.9377, train acc: 0.2776, val loss: 12.0455, val acc: 0.2371  (best train acc: 0.3588, best val acc: 0.3727)\n",
      "[Epoch: 2980] train loss: 3.5724, train acc: 0.2833, val loss: 55.1764, val acc: 0.2371  (best train acc: 0.3588, best val acc: 0.3727)\n",
      "[Epoch: 3000] train loss: 2.3878, train acc: 0.2696, val loss: 19.9561, val acc: 0.3140  (best train acc: 0.3588, best val acc: 0.3727)\n",
      "[Epoch: 3020] train loss: 2.2596, train acc: 0.2916, val loss: 29.5388, val acc: 0.2371  (best train acc: 0.3588, best val acc: 0.3727)\n",
      "[Epoch: 3040] train loss: 2.0949, train acc: 0.3000, val loss: 16.9453, val acc: 0.2024  (best train acc: 0.3588, best val acc: 0.3727)\n",
      "[Epoch: 3060] train loss: 1.5936, train acc: 0.2965, val loss: 4.7245, val acc: 0.2860  (best train acc: 0.3588, best val acc: 0.3727)\n",
      "[Epoch: 3080] train loss: 1.5749, train acc: 0.2971, val loss: 4.1540, val acc: 0.2371  (best train acc: 0.3588, best val acc: 0.3727)\n",
      "[Epoch: 3100] train loss: 1.6221, train acc: 0.2807, val loss: 3.3295, val acc: 0.2843  (best train acc: 0.3588, best val acc: 0.3727)\n",
      "[Epoch: 3120] train loss: 4.0494, train acc: 0.2695, val loss: 71.8621, val acc: 0.3046  (best train acc: 0.3588, best val acc: 0.3727)\n",
      "[Epoch: 3140] train loss: 3.1174, train acc: 0.2821, val loss: 38.0831, val acc: 0.2415  (best train acc: 0.3588, best val acc: 0.3727)\n",
      "[Epoch: 3160] train loss: 1.9259, train acc: 0.2819, val loss: 9.5587, val acc: 0.2435  (best train acc: 0.3588, best val acc: 0.3727)\n",
      "[Epoch: 3180] train loss: 1.6997, train acc: 0.2746, val loss: 4.6796, val acc: 0.2084  (best train acc: 0.3588, best val acc: 0.3727)\n",
      "[Epoch: 3200] train loss: 2.0680, train acc: 0.2940, val loss: 15.3793, val acc: 0.2371  (best train acc: 0.3588, best val acc: 0.3727)\n",
      "[Epoch: 3220] train loss: 1.6160, train acc: 0.2819, val loss: 5.5134, val acc: 0.2371  (best train acc: 0.3588, best val acc: 0.3727)\n",
      "[Epoch: 3240] train loss: 1.7585, train acc: 0.2345, val loss: 5.1969, val acc: 0.2371  (best train acc: 0.3588, best val acc: 0.3727)\n",
      "[Epoch: 3260] train loss: 1.5170, train acc: 0.2906, val loss: 3.3050, val acc: 0.2405  (best train acc: 0.3588, best val acc: 0.3727)\n",
      "[Epoch: 3280] train loss: 1.5075, train acc: 0.2942, val loss: 2.6294, val acc: 0.3636  (best train acc: 0.3588, best val acc: 0.3727)\n",
      "[Epoch: 3300] train loss: 1.6404, train acc: 0.2815, val loss: 7.6874, val acc: 0.2368  (best train acc: 0.3588, best val acc: 0.3727)\n",
      "[Epoch: 3320] train loss: 1.5065, train acc: 0.2757, val loss: 2.2738, val acc: 0.2428  (best train acc: 0.3588, best val acc: 0.3727)\n",
      "[Epoch: 3340] train loss: 1.4983, train acc: 0.2831, val loss: 2.9595, val acc: 0.2371  (best train acc: 0.3588, best val acc: 0.3727)\n",
      "[Epoch: 3360] train loss: 1.5107, train acc: 0.2846, val loss: 3.1657, val acc: 0.2442  (best train acc: 0.3588, best val acc: 0.3727)\n",
      "[Epoch: 3380] train loss: 2.0894, train acc: 0.2880, val loss: 17.6412, val acc: 0.2678  (best train acc: 0.3588, best val acc: 0.3727)\n",
      "[Epoch: 3400] train loss: 2.2592, train acc: 0.2875, val loss: 22.1682, val acc: 0.2371  (best train acc: 0.3588, best val acc: 0.3727)\n",
      "[Epoch: 3420] train loss: 1.7377, train acc: 0.2779, val loss: 5.8608, val acc: 0.2513  (best train acc: 0.3588, best val acc: 0.3727)\n",
      "[Epoch: 3440] train loss: 1.5485, train acc: 0.3167, val loss: 3.9660, val acc: 0.2482  (best train acc: 0.3588, best val acc: 0.3727)\n",
      "[Epoch: 3460] train loss: 1.6386, train acc: 0.2880, val loss: 6.2845, val acc: 0.2405  (best train acc: 0.3588, best val acc: 0.3727)\n",
      "[Epoch: 3480] train loss: 1.5566, train acc: 0.2624, val loss: 2.9899, val acc: 0.2438  (best train acc: 0.3588, best val acc: 0.3727)\n",
      "[Epoch: 3500] train loss: 1.6542, train acc: 0.2533, val loss: 6.2417, val acc: 0.3302  (best train acc: 0.3588, best val acc: 0.3727)\n",
      "[Epoch: 3520] train loss: 1.6962, train acc: 0.2839, val loss: 8.5938, val acc: 0.2405  (best train acc: 0.3588, best val acc: 0.3727)\n",
      "[Epoch: 3540] train loss: 2.6836, train acc: 0.2885, val loss: 33.9869, val acc: 0.2371  (best train acc: 0.3588, best val acc: 0.3727)\n",
      "[Epoch: 3560] train loss: 2.3277, train acc: 0.2646, val loss: 16.1812, val acc: 0.2297  (best train acc: 0.3588, best val acc: 0.3727)\n",
      "[Epoch: 3580] train loss: 1.8978, train acc: 0.2908, val loss: 11.2174, val acc: 0.2371  (best train acc: 0.3588, best val acc: 0.3727)\n",
      "[Epoch: 3600] train loss: 2.6113, train acc: 0.2861, val loss: 29.4854, val acc: 0.2384  (best train acc: 0.3588, best val acc: 0.3727)\n",
      "[Epoch: 3620] train loss: 2.6034, train acc: 0.2353, val loss: 22.5070, val acc: 0.2371  (best train acc: 0.3588, best val acc: 0.3727)\n",
      "[Epoch: 3640] train loss: 1.9015, train acc: 0.2896, val loss: 12.0105, val acc: 0.2371  (best train acc: 0.3588, best val acc: 0.3727)\n",
      "[Epoch: 3660] train loss: 1.6345, train acc: 0.2908, val loss: 6.3787, val acc: 0.2766  (best train acc: 0.3588, best val acc: 0.3727)\n",
      "[Epoch: 3680] train loss: 1.7879, train acc: 0.2965, val loss: 10.8444, val acc: 0.2371  (best train acc: 0.3588, best val acc: 0.3727)\n",
      "[Epoch: 3700] train loss: 2.5365, train acc: 0.2573, val loss: 17.4879, val acc: 0.2398  (best train acc: 0.3588, best val acc: 0.3727)\n",
      "[Epoch: 3720] train loss: 1.9449, train acc: 0.2945, val loss: 16.3097, val acc: 0.2371  (best train acc: 0.3588, best val acc: 0.3727)\n",
      "[Epoch: 3740] train loss: 1.6059, train acc: 0.2785, val loss: 6.8756, val acc: 0.1460  (best train acc: 0.3588, best val acc: 0.3727)\n",
      "[Epoch: 3760] train loss: 2.0400, train acc: 0.2888, val loss: 19.8269, val acc: 0.2371  (best train acc: 0.3588, best val acc: 0.3727)\n",
      "[Epoch: 3780] train loss: 1.8089, train acc: 0.2982, val loss: 11.0337, val acc: 0.2371  (best train acc: 0.3588, best val acc: 0.3727)\n",
      "[Epoch: 3800] train loss: 2.1777, train acc: 0.2834, val loss: 18.7343, val acc: 0.2371  (best train acc: 0.3588, best val acc: 0.3727)\n",
      "[Epoch: 3820] train loss: 2.0112, train acc: 0.2815, val loss: 22.9219, val acc: 0.2391  (best train acc: 0.3588, best val acc: 0.3727)\n",
      "[Epoch: 3840] train loss: 2.5249, train acc: 0.2953, val loss: 35.2098, val acc: 0.2597  (best train acc: 0.3588, best val acc: 0.3727)\n",
      "[Epoch: 3860] train loss: 1.7161, train acc: 0.2695, val loss: 8.4362, val acc: 0.2084  (best train acc: 0.3588, best val acc: 0.3727)\n",
      "[Epoch: 3880] train loss: 3.6148, train acc: 0.2764, val loss: 51.2852, val acc: 0.2361  (best train acc: 0.3588, best val acc: 0.3727)\n",
      "[Epoch: 3900] train loss: 3.0334, train acc: 0.2903, val loss: 31.5997, val acc: 0.2364  (best train acc: 0.3588, best val acc: 0.3727)\n",
      "[Epoch: 3920] train loss: 1.8238, train acc: 0.2674, val loss: 5.4930, val acc: 0.3002  (best train acc: 0.3588, best val acc: 0.3727)\n",
      "[Epoch: 3940] train loss: 1.7623, train acc: 0.2879, val loss: 7.4593, val acc: 0.2658  (best train acc: 0.3588, best val acc: 0.3727)\n",
      "[Epoch: 3960] train loss: 1.8558, train acc: 0.2460, val loss: 8.6506, val acc: 0.2371  (best train acc: 0.3588, best val acc: 0.3727)\n",
      "[Epoch: 3980] train loss: 2.2249, train acc: 0.2973, val loss: 23.4972, val acc: 0.2371  (best train acc: 0.3588, best val acc: 0.3727)\n",
      "[Epoch: 4000] train loss: 1.6455, train acc: 0.2957, val loss: 5.0624, val acc: 0.2371  (best train acc: 0.3588, best val acc: 0.3727)\n",
      "[Epoch: 4020] train loss: 2.8721, train acc: 0.2867, val loss: 29.2571, val acc: 0.2378  (best train acc: 0.3588, best val acc: 0.3727)\n",
      "[Epoch: 4040] train loss: 1.9059, train acc: 0.2309, val loss: 8.6066, val acc: 0.2371  (best train acc: 0.3588, best val acc: 0.3727)\n",
      "[Epoch: 4060] train loss: 3.0624, train acc: 0.2810, val loss: 26.4330, val acc: 0.2371  (best train acc: 0.3588, best val acc: 0.3727)\n",
      "[Epoch: 4080] train loss: 1.7172, train acc: 0.2815, val loss: 7.2522, val acc: 0.2428  (best train acc: 0.3588, best val acc: 0.3727)\n",
      "[Epoch: 4100] train loss: 1.5393, train acc: 0.2790, val loss: 2.5930, val acc: 0.2445  (best train acc: 0.3588, best val acc: 0.3727)\n",
      "[Epoch: 4120] train loss: 1.7605, train acc: 0.2797, val loss: 11.5857, val acc: 0.2371  (best train acc: 0.3588, best val acc: 0.3727)\n",
      "[Epoch: 4140] train loss: 1.5595, train acc: 0.2835, val loss: 5.2639, val acc: 0.2398  (best train acc: 0.3588, best val acc: 0.3727)\n",
      "[Epoch: 4160] train loss: 1.8971, train acc: 0.2830, val loss: 10.7440, val acc: 0.1811  (best train acc: 0.3588, best val acc: 0.3727)\n",
      "[Epoch: 4180] train loss: 2.0933, train acc: 0.2799, val loss: 22.3501, val acc: 0.2128  (best train acc: 0.3588, best val acc: 0.3727)\n",
      "[Epoch: 4200] train loss: 1.6456, train acc: 0.2788, val loss: 6.8624, val acc: 0.3248  (best train acc: 0.3588, best val acc: 0.3727)\n",
      "[Epoch: 4220] train loss: 4.7461, train acc: 0.2846, val loss: 73.7871, val acc: 0.2398  (best train acc: 0.3588, best val acc: 0.3727)\n",
      "[Epoch: 4240] train loss: 3.3547, train acc: 0.2856, val loss: 51.4221, val acc: 0.1707  (best train acc: 0.3588, best val acc: 0.3727)\n",
      "[Epoch: 4260] train loss: 2.5293, train acc: 0.2966, val loss: 21.7471, val acc: 0.2533  (best train acc: 0.3588, best val acc: 0.3727)\n",
      "[Epoch: 4280] train loss: 1.7812, train acc: 0.2860, val loss: 10.5372, val acc: 0.2489  (best train acc: 0.3588, best val acc: 0.3727)\n",
      "[Epoch: 4300] train loss: 1.8131, train acc: 0.2778, val loss: 5.0156, val acc: 0.2371  (best train acc: 0.3588, best val acc: 0.3727)\n",
      "[Epoch: 4320] train loss: 1.6532, train acc: 0.2790, val loss: 5.0876, val acc: 0.3089  (best train acc: 0.3588, best val acc: 0.3727)\n",
      "[Epoch: 4340] train loss: 1.7013, train acc: 0.2877, val loss: 6.1511, val acc: 0.2530  (best train acc: 0.3588, best val acc: 0.3727)\n",
      "[Epoch: 4360] train loss: 1.6146, train acc: 0.2821, val loss: 2.3774, val acc: 0.2364  (best train acc: 0.3588, best val acc: 0.3727)\n",
      "[Epoch: 4380] train loss: 1.7334, train acc: 0.2856, val loss: 7.5541, val acc: 0.2371  (best train acc: 0.3588, best val acc: 0.3727)\n",
      "[Epoch: 4400] train loss: 3.8079, train acc: 0.2781, val loss: 50.4418, val acc: 0.2371  (best train acc: 0.3588, best val acc: 0.3727)\n",
      "[Epoch: 4420] train loss: 2.3537, train acc: 0.3004, val loss: 22.0660, val acc: 0.1926  (best train acc: 0.3588, best val acc: 0.3727)\n",
      "[Epoch: 4440] train loss: 2.1861, train acc: 0.2396, val loss: 15.0904, val acc: 0.2361  (best train acc: 0.3588, best val acc: 0.3727)\n",
      "[Epoch: 4460] train loss: 1.6902, train acc: 0.2953, val loss: 5.4031, val acc: 0.2411  (best train acc: 0.3588, best val acc: 0.3727)\n",
      "[Epoch: 4480] train loss: 1.7136, train acc: 0.2865, val loss: 9.4245, val acc: 0.3046  (best train acc: 0.3588, best val acc: 0.3727)\n",
      "[Epoch: 4500] train loss: 2.3457, train acc: 0.2475, val loss: 23.9207, val acc: 0.2388  (best train acc: 0.3588, best val acc: 0.3727)\n",
      "[Epoch: 4520] train loss: 2.9383, train acc: 0.1844, val loss: 10.0106, val acc: 0.3170  (best train acc: 0.3588, best val acc: 0.3727)\n",
      "[Epoch: 4540] train loss: 4.4789, train acc: 0.2692, val loss: 51.6625, val acc: 0.2401  (best train acc: 0.3588, best val acc: 0.3727)\n",
      "[Epoch: 4560] train loss: 2.9477, train acc: 0.2679, val loss: 36.2353, val acc: 0.2398  (best train acc: 0.3588, best val acc: 0.3727)\n",
      "[Epoch: 4580] train loss: 2.5496, train acc: 0.2875, val loss: 30.6274, val acc: 0.2462  (best train acc: 0.3588, best val acc: 0.3727)\n",
      "[Epoch: 4600] train loss: 5.0978, train acc: 0.2888, val loss: 101.8356, val acc: 0.2371  (best train acc: 0.3588, best val acc: 0.3727)\n",
      "[Epoch: 4620] train loss: 2.7916, train acc: 0.2533, val loss: 51.3915, val acc: 0.1204  (best train acc: 0.3588, best val acc: 0.3727)\n",
      "[Epoch: 4640] train loss: 4.9471, train acc: 0.2600, val loss: 77.7603, val acc: 0.2371  (best train acc: 0.3588, best val acc: 0.3727)\n",
      "[Epoch: 4660] train loss: 6.4073, train acc: 0.2781, val loss: 98.0220, val acc: 0.2371  (best train acc: 0.3588, best val acc: 0.3727)\n",
      "[Epoch: 4680] train loss: 4.5969, train acc: 0.2350, val loss: 47.7732, val acc: 0.2580  (best train acc: 0.3588, best val acc: 0.3727)\n",
      "[Epoch: 4700] train loss: 2.1575, train acc: 0.2757, val loss: 12.0123, val acc: 0.2944  (best train acc: 0.3588, best val acc: 0.3727)\n",
      "[Epoch: 4720] train loss: 2.0737, train acc: 0.2835, val loss: 11.3202, val acc: 0.2820  (best train acc: 0.3588, best val acc: 0.3727)\n",
      "[Epoch: 4740] train loss: 1.5607, train acc: 0.2978, val loss: 5.0969, val acc: 0.2479  (best train acc: 0.3588, best val acc: 0.3727)\n",
      "[Epoch: 4760] train loss: 1.6620, train acc: 0.3011, val loss: 7.2127, val acc: 0.3349  (best train acc: 0.3588, best val acc: 0.3727)\n",
      "[Epoch: 4780] train loss: 2.2693, train acc: 0.2867, val loss: 31.3873, val acc: 0.2371  (best train acc: 0.3588, best val acc: 0.3727)\n",
      "[Epoch: 4800] train loss: 3.3183, train acc: 0.2548, val loss: 33.7300, val acc: 0.2371  (best train acc: 0.3588, best val acc: 0.3727)\n",
      "[Epoch: 4820] train loss: 9.5952, train acc: 0.3036, val loss: 187.9499, val acc: 0.2300  (best train acc: 0.3588, best val acc: 0.3727)\n",
      "[Epoch: 4840] train loss: 8.4306, train acc: 0.2908, val loss: 123.3527, val acc: 0.2401  (best train acc: 0.3588, best val acc: 0.3727)\n",
      "[Epoch: 4860] train loss: 6.0343, train acc: 0.2812, val loss: 85.9249, val acc: 0.2371  (best train acc: 0.3588, best val acc: 0.3727)\n",
      "[Epoch: 4880] train loss: 4.1841, train acc: 0.2921, val loss: 52.8223, val acc: 0.2428  (best train acc: 0.3588, best val acc: 0.3727)\n",
      "[Epoch: 4900] train loss: 2.6435, train acc: 0.2974, val loss: 27.6332, val acc: 0.2445  (best train acc: 0.3588, best val acc: 0.3727)\n",
      "[Epoch: 4920] train loss: 1.7894, train acc: 0.2882, val loss: 7.9086, val acc: 0.2371  (best train acc: 0.3588, best val acc: 0.3727)\n",
      "[Epoch: 4940] train loss: 1.6734, train acc: 0.2864, val loss: 4.7674, val acc: 0.2334  (best train acc: 0.3588, best val acc: 0.3727)\n",
      "[Epoch: 4960] train loss: 1.8675, train acc: 0.2915, val loss: 7.0155, val acc: 0.2874  (best train acc: 0.3588, best val acc: 0.3727)\n",
      "[Epoch: 4980] train loss: 1.5293, train acc: 0.3007, val loss: 2.8865, val acc: 0.2391  (best train acc: 0.3588, best val acc: 0.3727)\n",
      "[Epoch: 5000] train loss: 2.2619, train acc: 0.2721, val loss: 20.5331, val acc: 0.2395  (best train acc: 0.3588, best val acc: 0.3727)\n",
      "[Epoch: 5020] train loss: 1.7312, train acc: 0.2883, val loss: 7.5294, val acc: 0.2411  (best train acc: 0.3588, best val acc: 0.3727)\n",
      "[Epoch: 5040] train loss: 7.0338, train acc: 0.2874, val loss: 110.4056, val acc: 0.2388  (best train acc: 0.3588, best val acc: 0.3727)\n",
      "[Epoch: 5060] train loss: 3.9663, train acc: 0.2952, val loss: 72.2984, val acc: 0.2374  (best train acc: 0.3588, best val acc: 0.3727)\n",
      "[Epoch: 5080] train loss: 3.8063, train acc: 0.2464, val loss: 46.2961, val acc: 0.2415  (best train acc: 0.3588, best val acc: 0.3727)\n",
      "[Epoch: 5100] train loss: 2.6804, train acc: 0.2931, val loss: 24.3397, val acc: 0.2391  (best train acc: 0.3588, best val acc: 0.3727)\n",
      "[Epoch: 5120] train loss: 1.8483, train acc: 0.2856, val loss: 9.2597, val acc: 0.2371  (best train acc: 0.3588, best val acc: 0.3727)\n",
      "[Epoch: 5140] train loss: 1.6617, train acc: 0.2935, val loss: 9.0061, val acc: 0.2664  (best train acc: 0.3588, best val acc: 0.3727)\n",
      "[Epoch: 5160] train loss: 1.5413, train acc: 0.2705, val loss: 3.9385, val acc: 0.2435  (best train acc: 0.3588, best val acc: 0.3727)\n",
      "[Epoch: 5180] train loss: 1.5415, train acc: 0.3032, val loss: 3.3528, val acc: 0.2857  (best train acc: 0.3588, best val acc: 0.3727)\n",
      "[Epoch: 5200] train loss: 1.4941, train acc: 0.2877, val loss: 2.0636, val acc: 0.2718  (best train acc: 0.3588, best val acc: 0.3727)\n",
      "[Epoch: 5220] train loss: 1.6253, train acc: 0.3018, val loss: 7.0254, val acc: 0.2492  (best train acc: 0.3588, best val acc: 0.3727)\n",
      "[Epoch: 5240] train loss: 5.7841, train acc: 0.2851, val loss: 109.7330, val acc: 0.2401  (best train acc: 0.3588, best val acc: 0.3727)\n",
      "[Epoch: 5260] train loss: 3.7684, train acc: 0.2447, val loss: 62.4924, val acc: 0.2384  (best train acc: 0.3588, best val acc: 0.3727)\n",
      "[Epoch: 5280] train loss: 3.1347, train acc: 0.2659, val loss: 35.4683, val acc: 0.2337  (best train acc: 0.3588, best val acc: 0.3727)\n",
      "[Epoch: 5300] train loss: 1.6953, train acc: 0.2973, val loss: 7.0464, val acc: 0.2411  (best train acc: 0.3588, best val acc: 0.3727)\n",
      "[Epoch: 5320] train loss: 1.5125, train acc: 0.2943, val loss: 4.2618, val acc: 0.2378  (best train acc: 0.3588, best val acc: 0.3727)\n",
      "[Epoch: 5340] train loss: 1.5464, train acc: 0.2859, val loss: 3.3155, val acc: 0.2347  (best train acc: 0.3588, best val acc: 0.3727)\n",
      "[Epoch: 5360] train loss: 1.5059, train acc: 0.2676, val loss: 3.3942, val acc: 0.2428  (best train acc: 0.3588, best val acc: 0.3727)\n",
      "[Epoch: 5380] train loss: 1.4757, train acc: 0.2967, val loss: 3.2207, val acc: 0.2405  (best train acc: 0.3588, best val acc: 0.3727)\n",
      "[Epoch: 5400] train loss: 1.5119, train acc: 0.2966, val loss: 2.3672, val acc: 0.2462  (best train acc: 0.3588, best val acc: 0.3727)\n",
      "[Epoch: 5420] train loss: 1.5534, train acc: 0.2721, val loss: 2.9634, val acc: 0.2415  (best train acc: 0.3588, best val acc: 0.3727)\n",
      "[Epoch: 5440] train loss: 1.5179, train acc: 0.2893, val loss: 3.7884, val acc: 0.2384  (best train acc: 0.3588, best val acc: 0.3727)\n",
      "[Epoch: 5460] train loss: 1.5509, train acc: 0.2843, val loss: 4.9670, val acc: 0.2351  (best train acc: 0.3588, best val acc: 0.3727)\n",
      "[Epoch: 5480] train loss: 1.4732, train acc: 0.3007, val loss: 2.7908, val acc: 0.2472  (best train acc: 0.3588, best val acc: 0.3727)\n",
      "[Epoch: 5500] train loss: 1.4780, train acc: 0.2948, val loss: 3.6913, val acc: 0.2374  (best train acc: 0.3588, best val acc: 0.3727)\n",
      "[Epoch: 5520] train loss: 1.5018, train acc: 0.2841, val loss: 2.4637, val acc: 0.2459  (best train acc: 0.3588, best val acc: 0.3727)\n",
      "[Epoch: 5540] train loss: 1.4950, train acc: 0.2987, val loss: 4.5844, val acc: 0.2408  (best train acc: 0.3588, best val acc: 0.3727)\n",
      "[Epoch: 5560] train loss: 1.4849, train acc: 0.2985, val loss: 3.8509, val acc: 0.2374  (best train acc: 0.3588, best val acc: 0.3727)\n",
      "[Epoch: 5580] train loss: 1.8668, train acc: 0.2784, val loss: 12.4690, val acc: 0.2411  (best train acc: 0.3588, best val acc: 0.3727)\n",
      "[Epoch: 5600] train loss: 3.0808, train acc: 0.2320, val loss: 32.8472, val acc: 0.2395  (best train acc: 0.3588, best val acc: 0.3727)\n",
      "[Epoch: 5620] train loss: 2.3678, train acc: 0.2956, val loss: 17.9712, val acc: 0.2438  (best train acc: 0.3588, best val acc: 0.3727)\n",
      "[Epoch: 5640] train loss: 1.8739, train acc: 0.2903, val loss: 9.6287, val acc: 0.2371  (best train acc: 0.3588, best val acc: 0.3727)\n",
      "[Epoch: 5660] train loss: 1.6330, train acc: 0.2908, val loss: 14.6432, val acc: 0.2371  (best train acc: 0.3588, best val acc: 0.3727)\n",
      "[Epoch: 5680] train loss: 2.3461, train acc: 0.2442, val loss: 15.4022, val acc: 0.2411  (best train acc: 0.3588, best val acc: 0.3727)\n",
      "[Epoch: 5700] train loss: 1.5741, train acc: 0.2895, val loss: 5.6437, val acc: 0.2425  (best train acc: 0.3588, best val acc: 0.3727)\n",
      "[Epoch: 5720] train loss: 1.7126, train acc: 0.2447, val loss: 5.6510, val acc: 0.2371  (best train acc: 0.3588, best val acc: 0.3727)\n",
      "[Epoch: 5740] train loss: 4.8999, train acc: 0.2934, val loss: 103.7672, val acc: 0.2395  (best train acc: 0.3588, best val acc: 0.3727)\n",
      "[Epoch: 5760] train loss: 9.7264, train acc: 0.2445, val loss: 210.2868, val acc: 0.2401  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 5780] train loss: 7.1369, train acc: 0.2952, val loss: 141.3895, val acc: 0.2371  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 5800] train loss: 6.0683, train acc: 0.2778, val loss: 87.7828, val acc: 0.2398  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 5820] train loss: 3.1242, train acc: 0.2850, val loss: 57.1335, val acc: 0.2371  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 5840] train loss: 2.7576, train acc: 0.2885, val loss: 28.0681, val acc: 0.2371  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 5860] train loss: 1.7292, train acc: 0.2632, val loss: 5.3961, val acc: 0.2472  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 5880] train loss: 1.6340, train acc: 0.2961, val loss: 6.9333, val acc: 0.2492  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 5900] train loss: 1.5757, train acc: 0.2630, val loss: 4.3466, val acc: 0.2556  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 5920] train loss: 1.7367, train acc: 0.2907, val loss: 9.9667, val acc: 0.2371  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 5940] train loss: 1.5844, train acc: 0.2974, val loss: 5.6304, val acc: 0.2354  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 5960] train loss: 1.6071, train acc: 0.3017, val loss: 5.2322, val acc: 0.2371  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 5980] train loss: 1.7135, train acc: 0.2815, val loss: 9.8418, val acc: 0.2371  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 6000] train loss: 1.5829, train acc: 0.2550, val loss: 2.7875, val acc: 0.2391  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 6020] train loss: 1.7571, train acc: 0.2964, val loss: 9.5446, val acc: 0.2371  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 6040] train loss: 1.5199, train acc: 0.2903, val loss: 6.1348, val acc: 0.2371  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 6060] train loss: 1.6638, train acc: 0.2980, val loss: 8.0383, val acc: 0.2260  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 6080] train loss: 1.5706, train acc: 0.2906, val loss: 5.1342, val acc: 0.2445  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 6100] train loss: 1.6023, train acc: 0.2608, val loss: 3.3533, val acc: 0.2503  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 6120] train loss: 1.5251, train acc: 0.2718, val loss: 3.0425, val acc: 0.2587  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 6140] train loss: 1.8347, train acc: 0.2966, val loss: 7.6094, val acc: 0.2442  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 6160] train loss: 1.5615, train acc: 0.2982, val loss: 7.2412, val acc: 0.2371  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 6180] train loss: 1.5954, train acc: 0.3023, val loss: 7.3389, val acc: 0.2408  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 6200] train loss: 1.5903, train acc: 0.3046, val loss: 5.5566, val acc: 0.2411  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 6220] train loss: 1.5817, train acc: 0.2815, val loss: 5.4999, val acc: 0.2229  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 6240] train loss: 1.5741, train acc: 0.2950, val loss: 7.8726, val acc: 0.2371  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 6260] train loss: 1.6184, train acc: 0.3010, val loss: 4.1663, val acc: 0.2422  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 6280] train loss: 1.5068, train acc: 0.3012, val loss: 2.4995, val acc: 0.2320  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 6300] train loss: 1.6775, train acc: 0.2984, val loss: 6.9344, val acc: 0.2391  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 6320] train loss: 1.6088, train acc: 0.2903, val loss: 6.6196, val acc: 0.2374  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 6340] train loss: 1.9483, train acc: 0.2906, val loss: 18.9199, val acc: 0.2371  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 6360] train loss: 1.6988, train acc: 0.2940, val loss: 5.9006, val acc: 0.2422  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 6380] train loss: 1.6480, train acc: 0.2541, val loss: 6.3925, val acc: 0.2371  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 6400] train loss: 1.5607, train acc: 0.2835, val loss: 3.5723, val acc: 0.0776  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 6420] train loss: 1.5899, train acc: 0.2995, val loss: 6.3003, val acc: 0.2411  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 6440] train loss: 1.6624, train acc: 0.2843, val loss: 7.5407, val acc: 0.2442  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 6460] train loss: 1.6975, train acc: 0.2697, val loss: 7.3029, val acc: 0.2553  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 6480] train loss: 1.5813, train acc: 0.2978, val loss: 4.0717, val acc: 0.2438  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 6500] train loss: 1.5388, train acc: 0.2899, val loss: 5.5620, val acc: 0.2374  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 6520] train loss: 1.6268, train acc: 0.2531, val loss: 6.9429, val acc: 0.2391  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 6540] train loss: 1.6117, train acc: 0.2891, val loss: 4.9566, val acc: 0.2425  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 6560] train loss: 1.6457, train acc: 0.2964, val loss: 6.3420, val acc: 0.2371  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 6580] train loss: 1.5722, train acc: 0.2919, val loss: 3.3083, val acc: 0.2573  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 6600] train loss: 1.6324, train acc: 0.2813, val loss: 7.3578, val acc: 0.2583  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 6620] train loss: 1.4841, train acc: 0.2823, val loss: 2.0225, val acc: 0.2449  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 6640] train loss: 1.4680, train acc: 0.2898, val loss: 2.1812, val acc: 0.2435  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 6660] train loss: 1.5137, train acc: 0.2984, val loss: 3.3435, val acc: 0.2445  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 6680] train loss: 1.5266, train acc: 0.2841, val loss: 3.5606, val acc: 0.2449  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 6700] train loss: 1.7471, train acc: 0.2965, val loss: 9.3137, val acc: 0.2371  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 6720] train loss: 4.5426, train acc: 0.2949, val loss: 90.1124, val acc: 0.2395  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 6740] train loss: 3.0027, train acc: 0.2454, val loss: 35.9071, val acc: 0.2371  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 6760] train loss: 2.1479, train acc: 0.2780, val loss: 16.6814, val acc: 0.2371  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 6780] train loss: 6.0774, train acc: 0.2979, val loss: 137.9717, val acc: 0.2371  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 6800] train loss: 4.3132, train acc: 0.2420, val loss: 78.2473, val acc: 0.2371  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 6820] train loss: 2.2716, train acc: 0.2846, val loss: 40.1433, val acc: 0.2371  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 6840] train loss: 1.5563, train acc: 0.2840, val loss: 4.4312, val acc: 0.2408  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 6860] train loss: 1.7625, train acc: 0.2847, val loss: 6.7839, val acc: 0.2371  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 6880] train loss: 1.5917, train acc: 0.2874, val loss: 4.9281, val acc: 0.2543  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 6900] train loss: 1.6482, train acc: 0.3049, val loss: 7.4058, val acc: 0.2388  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 6920] train loss: 1.5958, train acc: 0.2963, val loss: 5.1674, val acc: 0.2580  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 6940] train loss: 1.5606, train acc: 0.3025, val loss: 5.5339, val acc: 0.2536  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 6960] train loss: 1.6056, train acc: 0.2828, val loss: 4.4596, val acc: 0.1960  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 6980] train loss: 1.5675, train acc: 0.2898, val loss: 5.0857, val acc: 0.2712  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 7000] train loss: 1.4904, train acc: 0.2976, val loss: 3.5022, val acc: 0.2567  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 7020] train loss: 1.5240, train acc: 0.3027, val loss: 5.4542, val acc: 0.2449  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 7040] train loss: 1.6257, train acc: 0.2689, val loss: 7.8538, val acc: 0.2371  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 7060] train loss: 1.6173, train acc: 0.2903, val loss: 5.2733, val acc: 0.2610  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 7080] train loss: 1.6321, train acc: 0.2731, val loss: 5.3194, val acc: 0.2395  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 7100] train loss: 1.4932, train acc: 0.2995, val loss: 2.7533, val acc: 0.2587  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 7120] train loss: 1.5368, train acc: 0.2979, val loss: 4.6968, val acc: 0.0830  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 7140] train loss: 1.6684, train acc: 0.2985, val loss: 7.3269, val acc: 0.2398  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 7160] train loss: 1.6511, train acc: 0.2970, val loss: 7.9436, val acc: 0.2637  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 7180] train loss: 1.7162, train acc: 0.2948, val loss: 5.9833, val acc: 0.2809  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 7200] train loss: 1.6309, train acc: 0.2788, val loss: 5.3655, val acc: 0.2388  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 7220] train loss: 1.7874, train acc: 0.2998, val loss: 9.2358, val acc: 0.2395  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 7240] train loss: 1.6826, train acc: 0.2923, val loss: 4.6658, val acc: 0.2560  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 7260] train loss: 1.5977, train acc: 0.2996, val loss: 7.8395, val acc: 0.2425  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 7280] train loss: 1.7993, train acc: 0.2921, val loss: 8.8193, val acc: 0.2486  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 7300] train loss: 1.7099, train acc: 0.2856, val loss: 9.0375, val acc: 0.2364  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 7320] train loss: 1.8443, train acc: 0.2854, val loss: 9.9948, val acc: 0.2651  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 7340] train loss: 1.5879, train acc: 0.2927, val loss: 3.4604, val acc: 0.2492  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 7360] train loss: 1.5484, train acc: 0.2764, val loss: 4.1171, val acc: 0.2425  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 7380] train loss: 1.5271, train acc: 0.2968, val loss: 2.5187, val acc: 0.2671  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 7400] train loss: 1.5036, train acc: 0.2997, val loss: 3.7497, val acc: 0.2449  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 7420] train loss: 1.4973, train acc: 0.3094, val loss: 3.1783, val acc: 0.2567  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 7440] train loss: 1.5934, train acc: 0.2677, val loss: 7.1936, val acc: 0.2371  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 7460] train loss: 1.5518, train acc: 0.3061, val loss: 4.0108, val acc: 0.2563  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 7480] train loss: 1.5481, train acc: 0.2912, val loss: 4.5321, val acc: 0.2648  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 7500] train loss: 1.5565, train acc: 0.2850, val loss: 3.2412, val acc: 0.2496  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 7520] train loss: 1.5335, train acc: 0.2991, val loss: 5.4345, val acc: 0.2695  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 7540] train loss: 1.7149, train acc: 0.2447, val loss: 5.1549, val acc: 0.2465  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 7560] train loss: 1.7420, train acc: 0.2949, val loss: 9.5473, val acc: 0.2624  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 7580] train loss: 1.8452, train acc: 0.3151, val loss: 10.8646, val acc: 0.2371  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 7600] train loss: 1.6541, train acc: 0.2874, val loss: 8.2953, val acc: 0.2583  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 7620] train loss: 1.6076, train acc: 0.3008, val loss: 8.4197, val acc: 0.2371  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 7640] train loss: 1.5715, train acc: 0.3001, val loss: 3.6701, val acc: 0.2351  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 7660] train loss: 1.7817, train acc: 0.2940, val loss: 8.8149, val acc: 0.2452  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 7680] train loss: 1.7560, train acc: 0.2992, val loss: 13.2650, val acc: 0.2371  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 7700] train loss: 1.5848, train acc: 0.2901, val loss: 8.1216, val acc: 0.2482  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 7720] train loss: 1.8255, train acc: 0.2909, val loss: 11.7806, val acc: 0.2391  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 7740] train loss: 1.5773, train acc: 0.2799, val loss: 6.3590, val acc: 0.2486  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 7760] train loss: 1.8999, train acc: 0.2913, val loss: 13.5056, val acc: 0.2368  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 7780] train loss: 1.6908, train acc: 0.2813, val loss: 8.3384, val acc: 0.2742  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 7800] train loss: 1.8059, train acc: 0.2913, val loss: 10.6342, val acc: 0.2486  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 7820] train loss: 5.1731, train acc: 0.2893, val loss: 100.7391, val acc: 0.2371  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 7840] train loss: 6.7862, train acc: 0.2959, val loss: 111.4746, val acc: 0.2401  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 7860] train loss: 4.1046, train acc: 0.2854, val loss: 64.3897, val acc: 0.2374  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 7880] train loss: 2.0057, train acc: 0.2888, val loss: 16.1771, val acc: 0.2371  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 7900] train loss: 1.6179, train acc: 0.2662, val loss: 4.6172, val acc: 0.2418  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 7920] train loss: 1.5587, train acc: 0.2514, val loss: 2.1496, val acc: 0.2715  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 7940] train loss: 2.1146, train acc: 0.2494, val loss: 5.2549, val acc: 0.2388  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 7960] train loss: 1.8732, train acc: 0.2812, val loss: 10.8573, val acc: 0.2472  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 7980] train loss: 1.6570, train acc: 0.3006, val loss: 8.7394, val acc: 0.2371  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 8000] train loss: 1.7134, train acc: 0.2642, val loss: 6.4055, val acc: 0.2452  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 8020] train loss: 1.6213, train acc: 0.3036, val loss: 7.9772, val acc: 0.2567  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 8040] train loss: 1.6593, train acc: 0.2979, val loss: 4.5102, val acc: 0.2442  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 8060] train loss: 1.6599, train acc: 0.2574, val loss: 8.7723, val acc: 0.2378  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 8080] train loss: 1.8399, train acc: 0.2961, val loss: 12.0848, val acc: 0.2432  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 8100] train loss: 1.5861, train acc: 0.3069, val loss: 5.7465, val acc: 0.2681  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 8120] train loss: 1.5287, train acc: 0.2694, val loss: 4.4298, val acc: 0.2428  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 8140] train loss: 1.4940, train acc: 0.2819, val loss: 3.2441, val acc: 0.2627  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 8160] train loss: 1.5247, train acc: 0.2898, val loss: 4.1535, val acc: 0.2567  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 8180] train loss: 1.6510, train acc: 0.2950, val loss: 8.4630, val acc: 0.2540  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 8200] train loss: 1.7818, train acc: 0.3072, val loss: 9.5383, val acc: 0.2462  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 8220] train loss: 1.5724, train acc: 0.2915, val loss: 6.6786, val acc: 0.2418  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 8240] train loss: 1.7453, train acc: 0.2953, val loss: 11.9411, val acc: 0.2371  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 8260] train loss: 1.6813, train acc: 0.2901, val loss: 7.1425, val acc: 0.2462  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 8280] train loss: 1.6559, train acc: 0.2893, val loss: 5.9690, val acc: 0.2418  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 8300] train loss: 1.7858, train acc: 0.3094, val loss: 6.7772, val acc: 0.2351  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 8320] train loss: 1.6594, train acc: 0.2774, val loss: 5.8511, val acc: 0.2526  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 8340] train loss: 1.6226, train acc: 0.2958, val loss: 7.0241, val acc: 0.2556  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 8360] train loss: 1.6929, train acc: 0.2872, val loss: 8.4526, val acc: 0.2395  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 8380] train loss: 1.7268, train acc: 0.2493, val loss: 5.5074, val acc: 0.2492  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 8400] train loss: 1.6180, train acc: 0.3128, val loss: 6.3863, val acc: 0.2486  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 8420] train loss: 1.5037, train acc: 0.2907, val loss: 4.3331, val acc: 0.2411  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 8440] train loss: 1.5582, train acc: 0.2883, val loss: 3.3614, val acc: 0.2617  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 8460] train loss: 1.5266, train acc: 0.3047, val loss: 6.8547, val acc: 0.2577  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 8480] train loss: 1.6267, train acc: 0.2815, val loss: 6.4508, val acc: 0.2553  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 8500] train loss: 1.8692, train acc: 0.2995, val loss: 9.9793, val acc: 0.2442  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 8520] train loss: 1.6256, train acc: 0.2838, val loss: 7.6194, val acc: 0.2378  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 8540] train loss: 2.0309, train acc: 0.2989, val loss: 15.0088, val acc: 0.2438  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 8560] train loss: 1.9490, train acc: 0.2893, val loss: 12.0071, val acc: 0.2391  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 8580] train loss: 1.7076, train acc: 0.2753, val loss: 3.2658, val acc: 0.2455  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 8600] train loss: 1.6875, train acc: 0.2941, val loss: 8.0280, val acc: 0.2476  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 8620] train loss: 1.5751, train acc: 0.2959, val loss: 2.9105, val acc: 0.2634  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 8640] train loss: 1.5284, train acc: 0.3041, val loss: 3.1162, val acc: 0.2563  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 8660] train loss: 1.6280, train acc: 0.3062, val loss: 4.6254, val acc: 0.2536  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 8680] train loss: 1.5644, train acc: 0.3063, val loss: 5.6668, val acc: 0.2401  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 8700] train loss: 2.2112, train acc: 0.3008, val loss: 17.4386, val acc: 0.2519  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 8720] train loss: 1.7022, train acc: 0.2999, val loss: 6.8096, val acc: 0.2398  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 8740] train loss: 1.5924, train acc: 0.2877, val loss: 3.6246, val acc: 0.2675  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 8760] train loss: 1.5518, train acc: 0.2967, val loss: 8.1627, val acc: 0.2371  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 8780] train loss: 1.4739, train acc: 0.3005, val loss: 2.7977, val acc: 0.2577  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 8800] train loss: 1.4797, train acc: 0.2972, val loss: 2.6852, val acc: 0.2722  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 8820] train loss: 1.5819, train acc: 0.2927, val loss: 5.6434, val acc: 0.2381  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 8840] train loss: 1.5436, train acc: 0.2935, val loss: 2.9224, val acc: 0.1019  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 8860] train loss: 1.7140, train acc: 0.2950, val loss: 7.5038, val acc: 0.2745  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 8880] train loss: 1.7618, train acc: 0.2926, val loss: 10.1335, val acc: 0.2644  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 8900] train loss: 1.8465, train acc: 0.2891, val loss: 13.9792, val acc: 0.2422  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 8920] train loss: 1.5645, train acc: 0.2954, val loss: 5.1607, val acc: 0.2820  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 8940] train loss: 1.5843, train acc: 0.2692, val loss: 6.1056, val acc: 0.2695  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 8960] train loss: 1.6235, train acc: 0.3010, val loss: 6.4518, val acc: 0.2688  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 8980] train loss: 1.7038, train acc: 0.2859, val loss: 9.4889, val acc: 0.2492  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 9000] train loss: 1.7445, train acc: 0.2834, val loss: 13.7725, val acc: 0.2492  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 9020] train loss: 1.7582, train acc: 0.3002, val loss: 9.8510, val acc: 0.2411  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 9040] train loss: 1.7852, train acc: 0.2919, val loss: 11.9111, val acc: 0.2378  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 9060] train loss: 1.6586, train acc: 0.2697, val loss: 7.3953, val acc: 0.2749  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 9080] train loss: 1.7886, train acc: 0.2951, val loss: 15.2328, val acc: 0.2550  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 9100] train loss: 1.6530, train acc: 0.2767, val loss: 9.2405, val acc: 0.2607  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 9120] train loss: 1.5131, train acc: 0.3025, val loss: 5.2257, val acc: 0.2658  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 9140] train loss: 1.5864, train acc: 0.2890, val loss: 4.5680, val acc: 0.2644  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 9160] train loss: 1.7307, train acc: 0.2909, val loss: 7.7207, val acc: 0.2621  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 9180] train loss: 1.8873, train acc: 0.3040, val loss: 13.8371, val acc: 0.2371  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 9200] train loss: 1.9531, train acc: 0.3051, val loss: 11.8926, val acc: 0.2371  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 9220] train loss: 1.7019, train acc: 0.2981, val loss: 9.7834, val acc: 0.2371  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 9240] train loss: 1.5836, train acc: 0.2957, val loss: 6.9043, val acc: 0.2432  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 9260] train loss: 1.8540, train acc: 0.3005, val loss: 10.9629, val acc: 0.2499  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 9280] train loss: 1.6387, train acc: 0.2945, val loss: 8.3591, val acc: 0.2398  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 9300] train loss: 1.5398, train acc: 0.2593, val loss: 3.2604, val acc: 0.2536  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 9320] train loss: 1.5698, train acc: 0.2846, val loss: 5.9834, val acc: 0.2415  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 9340] train loss: 1.5377, train acc: 0.2818, val loss: 4.0897, val acc: 0.2749  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 9360] train loss: 1.6230, train acc: 0.2963, val loss: 5.0157, val acc: 0.2742  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 9380] train loss: 1.7311, train acc: 0.2869, val loss: 10.5361, val acc: 0.2492  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 9400] train loss: 1.6635, train acc: 0.2952, val loss: 6.8531, val acc: 0.2634  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 9420] train loss: 1.9186, train acc: 0.2913, val loss: 12.5327, val acc: 0.2509  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 9440] train loss: 1.6864, train acc: 0.2914, val loss: 12.3882, val acc: 0.2371  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 9460] train loss: 1.6660, train acc: 0.2509, val loss: 3.2794, val acc: 0.2533  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 9480] train loss: 1.6581, train acc: 0.2666, val loss: 8.2781, val acc: 0.2374  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 9500] train loss: 1.8160, train acc: 0.2987, val loss: 11.2974, val acc: 0.2445  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 9520] train loss: 1.6659, train acc: 0.3024, val loss: 9.6632, val acc: 0.2513  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 9540] train loss: 1.6981, train acc: 0.2958, val loss: 9.0178, val acc: 0.2722  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 9560] train loss: 1.6399, train acc: 0.2833, val loss: 7.0978, val acc: 0.2631  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 9580] train loss: 1.6186, train acc: 0.2857, val loss: 5.3305, val acc: 0.2445  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 9600] train loss: 1.5916, train acc: 0.2540, val loss: 6.2573, val acc: 0.2388  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 9620] train loss: 1.5906, train acc: 0.2856, val loss: 7.3864, val acc: 0.2435  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 9640] train loss: 1.5923, train acc: 0.2955, val loss: 7.1554, val acc: 0.2658  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 9660] train loss: 1.7266, train acc: 0.2553, val loss: 7.9911, val acc: 0.2371  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 9680] train loss: 1.5731, train acc: 0.3000, val loss: 4.3919, val acc: 0.2546  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 9700] train loss: 2.1682, train acc: 0.2465, val loss: 11.3522, val acc: 0.2354  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 9720] train loss: 1.7934, train acc: 0.2743, val loss: 10.8037, val acc: 0.2813  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 9740] train loss: 1.6652, train acc: 0.2958, val loss: 8.4794, val acc: 0.2759  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 9760] train loss: 1.7861, train acc: 0.3043, val loss: 10.7420, val acc: 0.2418  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 9780] train loss: 1.8667, train acc: 0.2968, val loss: 12.0820, val acc: 0.2432  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 9800] train loss: 1.7484, train acc: 0.3023, val loss: 11.0910, val acc: 0.2371  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 9820] train loss: 1.8622, train acc: 0.2989, val loss: 16.4204, val acc: 0.2371  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 9840] train loss: 1.5132, train acc: 0.2948, val loss: 3.8516, val acc: 0.2675  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 9860] train loss: 1.6356, train acc: 0.2495, val loss: 4.1676, val acc: 0.2624  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 9880] train loss: 1.7531, train acc: 0.2931, val loss: 7.0766, val acc: 0.2462  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 9900] train loss: 1.7025, train acc: 0.2847, val loss: 7.3986, val acc: 0.2570  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 9920] train loss: 1.6968, train acc: 0.2450, val loss: 4.4652, val acc: 0.2499  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 9940] train loss: 1.7387, train acc: 0.2914, val loss: 13.6310, val acc: 0.2634  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 9960] train loss: 1.4832, train acc: 0.3104, val loss: 4.0177, val acc: 0.2745  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 9980] train loss: 1.5743, train acc: 0.3036, val loss: 4.4563, val acc: 0.2691  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 10000] train loss: 1.5158, train acc: 0.2880, val loss: 2.8869, val acc: 0.2567  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 10020] train loss: 1.7277, train acc: 0.2927, val loss: 9.9546, val acc: 0.2816  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 10040] train loss: 1.8905, train acc: 0.3000, val loss: 11.7642, val acc: 0.2371  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 10060] train loss: 1.7720, train acc: 0.2825, val loss: 16.0193, val acc: 0.2455  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 10080] train loss: 1.9009, train acc: 0.2864, val loss: 11.4959, val acc: 0.2715  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 10100] train loss: 1.9165, train acc: 0.2941, val loss: 15.4854, val acc: 0.2675  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 10120] train loss: 1.6961, train acc: 0.3018, val loss: 9.2082, val acc: 0.2826  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 10140] train loss: 1.4865, train acc: 0.3024, val loss: 2.9617, val acc: 0.2675  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 10160] train loss: 1.5352, train acc: 0.2997, val loss: 5.2215, val acc: 0.2462  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 10180] train loss: 1.5203, train acc: 0.2995, val loss: 5.4699, val acc: 0.2567  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 10200] train loss: 1.5923, train acc: 0.3002, val loss: 3.8607, val acc: 0.2894  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 10220] train loss: 1.7104, train acc: 0.3032, val loss: 12.8386, val acc: 0.2371  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 10240] train loss: 1.7958, train acc: 0.2760, val loss: 9.3656, val acc: 0.2843  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 10260] train loss: 1.7270, train acc: 0.2906, val loss: 10.6705, val acc: 0.2938  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 10280] train loss: 1.6783, train acc: 0.3056, val loss: 7.4293, val acc: 0.2735  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 10300] train loss: 1.6232, train acc: 0.2681, val loss: 10.4427, val acc: 0.2503  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 10320] train loss: 1.7500, train acc: 0.2785, val loss: 11.1909, val acc: 0.2681  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 10340] train loss: 1.9408, train acc: 0.2850, val loss: 15.5414, val acc: 0.2101  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 10360] train loss: 1.6131, train acc: 0.2602, val loss: 4.2231, val acc: 0.2857  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 10380] train loss: 1.5627, train acc: 0.2953, val loss: 5.8080, val acc: 0.2459  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 10400] train loss: 1.7699, train acc: 0.3128, val loss: 9.0962, val acc: 0.2927  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 10420] train loss: 1.5699, train acc: 0.2778, val loss: 5.5169, val acc: 0.2492  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 10440] train loss: 1.7106, train acc: 0.2993, val loss: 6.6799, val acc: 0.2152  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 10460] train loss: 1.7951, train acc: 0.2664, val loss: 10.6344, val acc: 0.2472  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 10480] train loss: 1.6236, train acc: 0.2973, val loss: 4.7109, val acc: 0.2769  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 10500] train loss: 1.7150, train acc: 0.2968, val loss: 8.9250, val acc: 0.2725  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 10520] train loss: 1.5395, train acc: 0.3108, val loss: 7.5543, val acc: 0.2685  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 10540] train loss: 1.6984, train acc: 0.2942, val loss: 9.5596, val acc: 0.2445  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 10560] train loss: 1.7642, train acc: 0.2971, val loss: 7.8623, val acc: 0.2648  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 10580] train loss: 1.7004, train acc: 0.2987, val loss: 11.2692, val acc: 0.2418  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 10600] train loss: 1.6430, train acc: 0.2949, val loss: 6.1657, val acc: 0.2614  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 10620] train loss: 1.7257, train acc: 0.3022, val loss: 8.8919, val acc: 0.2577  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 10640] train loss: 1.7055, train acc: 0.2960, val loss: 15.7002, val acc: 0.2371  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 10660] train loss: 1.5950, train acc: 0.3050, val loss: 3.6441, val acc: 0.3002  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 10680] train loss: 1.8075, train acc: 0.2502, val loss: 7.3793, val acc: 0.2465  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 10700] train loss: 1.6210, train acc: 0.2888, val loss: 6.0258, val acc: 0.2813  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 10720] train loss: 1.6924, train acc: 0.3034, val loss: 7.7037, val acc: 0.2981  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 10740] train loss: 1.7769, train acc: 0.2932, val loss: 9.6287, val acc: 0.2587  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 10760] train loss: 1.5890, train acc: 0.2834, val loss: 4.6203, val acc: 0.2890  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 10780] train loss: 1.6871, train acc: 0.2846, val loss: 9.2746, val acc: 0.2449  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 10800] train loss: 1.5979, train acc: 0.2806, val loss: 7.8668, val acc: 0.2799  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 10820] train loss: 1.5144, train acc: 0.3091, val loss: 5.1151, val acc: 0.2860  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 10840] train loss: 1.6565, train acc: 0.2682, val loss: 7.4213, val acc: 0.2472  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 10860] train loss: 1.6012, train acc: 0.3015, val loss: 6.7559, val acc: 0.2877  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 10880] train loss: 1.4670, train acc: 0.3026, val loss: 2.3965, val acc: 0.2961  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 10900] train loss: 1.5878, train acc: 0.2501, val loss: 5.0845, val acc: 0.2685  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 10920] train loss: 1.5603, train acc: 0.2689, val loss: 5.5134, val acc: 0.2985  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 10940] train loss: 1.7225, train acc: 0.3080, val loss: 12.4539, val acc: 0.2752  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 10960] train loss: 1.7589, train acc: 0.2629, val loss: 9.8959, val acc: 0.2442  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 10980] train loss: 1.5231, train acc: 0.2940, val loss: 4.0324, val acc: 0.2671  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 11000] train loss: 1.7190, train acc: 0.2970, val loss: 13.1714, val acc: 0.2411  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 11020] train loss: 1.7884, train acc: 0.2898, val loss: 10.8887, val acc: 0.2985  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 11040] train loss: 1.6452, train acc: 0.2673, val loss: 6.9047, val acc: 0.2820  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 11060] train loss: 1.6428, train acc: 0.2747, val loss: 8.0860, val acc: 0.2954  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 11080] train loss: 1.7481, train acc: 0.3002, val loss: 9.0297, val acc: 0.3019  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 11100] train loss: 1.6397, train acc: 0.3008, val loss: 7.6122, val acc: 0.2590  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 11120] train loss: 1.5899, train acc: 0.3094, val loss: 5.8094, val acc: 0.2614  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 11140] train loss: 1.6543, train acc: 0.2997, val loss: 8.3697, val acc: 0.2874  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 11160] train loss: 1.5853, train acc: 0.2872, val loss: 3.9180, val acc: 0.3073  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 11180] train loss: 1.8086, train acc: 0.2736, val loss: 9.3192, val acc: 0.2637  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 11200] train loss: 1.5114, train acc: 0.2983, val loss: 5.1891, val acc: 0.2745  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 11220] train loss: 1.6154, train acc: 0.2923, val loss: 4.8653, val acc: 0.2550  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 11240] train loss: 1.4870, train acc: 0.2979, val loss: 5.1493, val acc: 0.2944  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 11260] train loss: 1.4715, train acc: 0.2927, val loss: 2.0724, val acc: 0.3187  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 11280] train loss: 1.5685, train acc: 0.3060, val loss: 5.2763, val acc: 0.2887  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 11300] train loss: 1.4982, train acc: 0.2974, val loss: 3.6500, val acc: 0.2654  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 11320] train loss: 1.6976, train acc: 0.2528, val loss: 6.5632, val acc: 0.2499  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 11340] train loss: 2.1590, train acc: 0.3065, val loss: 17.3413, val acc: 0.2583  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 11360] train loss: 1.5075, train acc: 0.2963, val loss: 3.2152, val acc: 0.2860  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 11380] train loss: 1.8847, train acc: 0.2906, val loss: 10.9562, val acc: 0.2617  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 11400] train loss: 1.8857, train acc: 0.2938, val loss: 14.1993, val acc: 0.2408  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 11420] train loss: 1.5155, train acc: 0.2799, val loss: 4.1405, val acc: 0.2944  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 11440] train loss: 1.8054, train acc: 0.2962, val loss: 16.1589, val acc: 0.2371  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 11460] train loss: 1.7337, train acc: 0.3021, val loss: 6.8642, val acc: 0.2907  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 11480] train loss: 1.6750, train acc: 0.2747, val loss: 5.7941, val acc: 0.2941  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 11500] train loss: 1.6825, train acc: 0.2867, val loss: 11.1605, val acc: 0.2857  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 11520] train loss: 1.8342, train acc: 0.2694, val loss: 11.4135, val acc: 0.2759  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 11540] train loss: 1.7133, train acc: 0.2707, val loss: 9.2456, val acc: 0.2732  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 11560] train loss: 1.5980, train acc: 0.3037, val loss: 7.5927, val acc: 0.2823  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 11580] train loss: 1.5790, train acc: 0.2971, val loss: 3.6720, val acc: 0.2890  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 11600] train loss: 1.7533, train acc: 0.2911, val loss: 19.2689, val acc: 0.2371  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 11620] train loss: 1.7325, train acc: 0.2944, val loss: 9.1787, val acc: 0.2904  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 11640] train loss: 1.4740, train acc: 0.2945, val loss: 3.2831, val acc: 0.2971  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 11660] train loss: 1.5704, train acc: 0.2884, val loss: 5.4689, val acc: 0.2536  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 11680] train loss: 1.5265, train acc: 0.2961, val loss: 6.5601, val acc: 0.2526  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 11700] train loss: 1.4958, train acc: 0.3117, val loss: 4.8335, val acc: 0.2772  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 11720] train loss: 1.8467, train acc: 0.2924, val loss: 10.8070, val acc: 0.2553  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 11740] train loss: 1.7035, train acc: 0.2985, val loss: 10.8113, val acc: 0.2739  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 11760] train loss: 1.7784, train acc: 0.3063, val loss: 7.7011, val acc: 0.2887  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 11780] train loss: 1.7154, train acc: 0.2902, val loss: 8.4752, val acc: 0.2809  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 11800] train loss: 1.6109, train acc: 0.2980, val loss: 8.2480, val acc: 0.2688  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 11820] train loss: 1.6476, train acc: 0.2887, val loss: 12.8735, val acc: 0.2455  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 11840] train loss: 1.6147, train acc: 0.3001, val loss: 3.9462, val acc: 0.2931  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 11860] train loss: 1.5553, train acc: 0.3009, val loss: 6.5709, val acc: 0.2820  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 11880] train loss: 2.0196, train acc: 0.2951, val loss: 14.9459, val acc: 0.2465  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 11900] train loss: 2.1429, train acc: 0.2961, val loss: 18.6819, val acc: 0.2452  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 11920] train loss: 1.7110, train acc: 0.2950, val loss: 5.9834, val acc: 0.2793  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 11940] train loss: 1.7361, train acc: 0.2899, val loss: 9.8498, val acc: 0.2405  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 11960] train loss: 1.5503, train acc: 0.2850, val loss: 4.1638, val acc: 0.2938  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 11980] train loss: 1.5826, train acc: 0.3090, val loss: 4.9785, val acc: 0.2874  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 12000] train loss: 1.6081, train acc: 0.3018, val loss: 5.8363, val acc: 0.2739  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 12020] train loss: 1.9590, train acc: 0.2426, val loss: 10.7645, val acc: 0.2597  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 12040] train loss: 1.8825, train acc: 0.2493, val loss: 8.3267, val acc: 0.2607  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 12060] train loss: 1.7943, train acc: 0.2674, val loss: 8.1087, val acc: 0.2981  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 12080] train loss: 1.6480, train acc: 0.3027, val loss: 10.9643, val acc: 0.2388  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 12100] train loss: 1.6335, train acc: 0.2980, val loss: 7.3634, val acc: 0.2678  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 12120] train loss: 1.7788, train acc: 0.2898, val loss: 14.9546, val acc: 0.2442  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 12140] train loss: 1.9763, train acc: 0.3089, val loss: 17.8912, val acc: 0.2560  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 12160] train loss: 1.5994, train acc: 0.2953, val loss: 5.5562, val acc: 0.2499  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 12180] train loss: 1.7505, train acc: 0.3016, val loss: 9.3693, val acc: 0.2870  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 12200] train loss: 1.6983, train acc: 0.2916, val loss: 7.3497, val acc: 0.2678  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 12220] train loss: 1.6098, train acc: 0.3050, val loss: 6.9588, val acc: 0.2803  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 12240] train loss: 1.8785, train acc: 0.2953, val loss: 11.9318, val acc: 0.2688  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 12260] train loss: 1.5882, train acc: 0.2859, val loss: 6.5750, val acc: 0.2914  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 12280] train loss: 1.9036, train acc: 0.2916, val loss: 15.5570, val acc: 0.2661  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 12300] train loss: 1.6977, train acc: 0.2927, val loss: 10.2120, val acc: 0.2776  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 12320] train loss: 1.7089, train acc: 0.3151, val loss: 9.2714, val acc: 0.2455  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 12340] train loss: 1.8628, train acc: 0.2786, val loss: 10.8617, val acc: 0.2779  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 12360] train loss: 1.7771, train acc: 0.2665, val loss: 14.5170, val acc: 0.2401  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 12380] train loss: 1.5910, train acc: 0.2942, val loss: 6.3389, val acc: 0.2796  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 12400] train loss: 1.5699, train acc: 0.2895, val loss: 5.8017, val acc: 0.1245  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 12420] train loss: 1.8426, train acc: 0.2895, val loss: 12.6653, val acc: 0.2449  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 12440] train loss: 1.7737, train acc: 0.3026, val loss: 12.2023, val acc: 0.2492  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 12460] train loss: 1.6360, train acc: 0.2922, val loss: 6.7003, val acc: 0.2027  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 12480] train loss: 1.7909, train acc: 0.2993, val loss: 10.8037, val acc: 0.2513  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 12500] train loss: 1.5648, train acc: 0.3036, val loss: 6.8687, val acc: 0.2337  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 12520] train loss: 1.7152, train acc: 0.2989, val loss: 8.2522, val acc: 0.2337  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 12540] train loss: 1.5776, train acc: 0.2985, val loss: 8.3045, val acc: 0.2546  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 12560] train loss: 1.7617, train acc: 0.3088, val loss: 9.5648, val acc: 0.2449  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 12580] train loss: 1.6463, train acc: 0.2970, val loss: 8.2090, val acc: 0.2445  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 12600] train loss: 1.6596, train acc: 0.2636, val loss: 6.8730, val acc: 0.2442  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 12620] train loss: 1.5657, train acc: 0.2864, val loss: 6.1109, val acc: 0.2297  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 12640] train loss: 1.6402, train acc: 0.2872, val loss: 10.0895, val acc: 0.2526  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 12660] train loss: 1.5796, train acc: 0.2854, val loss: 9.1439, val acc: 0.2472  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 12680] train loss: 1.8901, train acc: 0.2916, val loss: 14.3796, val acc: 0.2398  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 12700] train loss: 1.6678, train acc: 0.2968, val loss: 8.4293, val acc: 0.2580  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 12720] train loss: 1.7434, train acc: 0.3055, val loss: 10.6091, val acc: 0.2084  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 12740] train loss: 1.6645, train acc: 0.3032, val loss: 9.3285, val acc: 0.2455  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 12760] train loss: 1.5671, train acc: 0.2810, val loss: 5.5384, val acc: 0.2455  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 12780] train loss: 1.7341, train acc: 0.2862, val loss: 11.9710, val acc: 0.2489  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 12800] train loss: 1.8431, train acc: 0.2705, val loss: 13.6464, val acc: 0.2263  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 12820] train loss: 1.6310, train acc: 0.2936, val loss: 6.7976, val acc: 0.2155  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 12840] train loss: 1.7854, train acc: 0.2671, val loss: 7.8704, val acc: 0.2570  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 12860] train loss: 1.8120, train acc: 0.2787, val loss: 11.0580, val acc: 0.2452  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 12880] train loss: 1.5985, train acc: 0.3047, val loss: 3.6034, val acc: 0.2179  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 12900] train loss: 1.7352, train acc: 0.2859, val loss: 7.1560, val acc: 0.2327  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 12920] train loss: 1.8586, train acc: 0.2914, val loss: 10.8788, val acc: 0.2519  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 12940] train loss: 1.7710, train acc: 0.2784, val loss: 10.1033, val acc: 0.2047  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 12960] train loss: 1.5479, train acc: 0.2966, val loss: 4.7986, val acc: 0.2442  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 12980] train loss: 2.2409, train acc: 0.1816, val loss: 17.6989, val acc: 0.2624  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 13000] train loss: 1.7000, train acc: 0.2863, val loss: 7.3821, val acc: 0.2634  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 13020] train loss: 1.5488, train acc: 0.3078, val loss: 5.1373, val acc: 0.2550  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 13040] train loss: 1.9908, train acc: 0.2504, val loss: 10.9318, val acc: 0.2503  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 13060] train loss: 1.6047, train acc: 0.3028, val loss: 5.7259, val acc: 0.2293  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 13080] train loss: 1.7828, train acc: 0.2932, val loss: 7.9620, val acc: 0.2648  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 13100] train loss: 1.5148, train acc: 0.3118, val loss: 5.2776, val acc: 0.2607  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 13120] train loss: 1.8075, train acc: 0.3015, val loss: 10.9706, val acc: 0.2583  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 13140] train loss: 1.5671, train acc: 0.2974, val loss: 6.0702, val acc: 0.2202  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 13160] train loss: 1.6324, train acc: 0.3112, val loss: 6.4347, val acc: 0.2347  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 13180] train loss: 1.7504, train acc: 0.2997, val loss: 6.9152, val acc: 0.2664  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 13200] train loss: 1.6248, train acc: 0.3062, val loss: 5.0101, val acc: 0.2422  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 13220] train loss: 1.7067, train acc: 0.3004, val loss: 8.8673, val acc: 0.2563  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 13240] train loss: 1.8684, train acc: 0.3015, val loss: 9.5174, val acc: 0.1379  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 13260] train loss: 1.7682, train acc: 0.2395, val loss: 12.6906, val acc: 0.2371  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 13280] train loss: 1.6032, train acc: 0.3109, val loss: 5.3870, val acc: 0.2543  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 13300] train loss: 1.6041, train acc: 0.2889, val loss: 5.8047, val acc: 0.2604  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 13320] train loss: 1.6983, train acc: 0.3033, val loss: 8.6184, val acc: 0.2337  (best train acc: 0.3590, best val acc: 0.3727)\n",
      "[Epoch: 13340] train loss: 1.5684, train acc: 0.3068, val loss: 13.0331, val acc: 0.2371  (best train acc: 0.3741, best val acc: 0.3727)\n",
      "[Epoch: 13360] train loss: 1.6338, train acc: 0.2888, val loss: 9.6886, val acc: 0.2577  (best train acc: 0.3741, best val acc: 0.3727)\n",
      "[Epoch: 13380] train loss: 1.7065, train acc: 0.3096, val loss: 9.5331, val acc: 0.2678  (best train acc: 0.3741, best val acc: 0.3727)\n",
      "[Epoch: 13400] train loss: 1.8465, train acc: 0.2952, val loss: 12.8386, val acc: 0.2239  (best train acc: 0.3741, best val acc: 0.3727)\n",
      "[Epoch: 13420] train loss: 1.7076, train acc: 0.3663, val loss: 11.9031, val acc: 0.2371  (best train acc: 0.3849, best val acc: 0.3727)\n",
      "[Epoch: 13440] train loss: 1.9558, train acc: 0.2613, val loss: 12.1123, val acc: 0.2617  (best train acc: 0.3849, best val acc: 0.3727)\n",
      "[Epoch: 13460] train loss: 1.7877, train acc: 0.3031, val loss: 12.7911, val acc: 0.2415  (best train acc: 0.3849, best val acc: 0.3727)\n",
      "[Epoch: 13480] train loss: 1.5982, train acc: 0.2559, val loss: 7.4345, val acc: 0.2388  (best train acc: 0.3849, best val acc: 0.3727)\n",
      "[Epoch: 13500] train loss: 1.8725, train acc: 0.2851, val loss: 13.5317, val acc: 0.2212  (best train acc: 0.3849, best val acc: 0.3727)\n",
      "[Epoch: 13520] train loss: 1.6960, train acc: 0.2846, val loss: 6.4524, val acc: 0.2607  (best train acc: 0.3849, best val acc: 0.3727)\n",
      "[Epoch: 13540] train loss: 1.7496, train acc: 0.2898, val loss: 11.7074, val acc: 0.2047  (best train acc: 0.3849, best val acc: 0.3727)\n",
      "[Epoch: 13560] train loss: 1.6908, train acc: 0.2886, val loss: 6.1725, val acc: 0.2277  (best train acc: 0.3849, best val acc: 0.3727)\n",
      "[Epoch: 13580] train loss: 1.7537, train acc: 0.2979, val loss: 12.6202, val acc: 0.2523  (best train acc: 0.3849, best val acc: 0.3727)\n",
      "[Epoch: 13600] train loss: 2.2292, train acc: 0.2498, val loss: 14.2835, val acc: 0.2371  (best train acc: 0.3849, best val acc: 0.3727)\n",
      "[Epoch: 13620] train loss: 1.6430, train acc: 0.3008, val loss: 12.7509, val acc: 0.2445  (best train acc: 0.3849, best val acc: 0.3727)\n",
      "[Epoch: 13640] train loss: 1.8446, train acc: 0.2966, val loss: 15.1278, val acc: 0.2486  (best train acc: 0.3849, best val acc: 0.3727)\n",
      "[Epoch: 13660] train loss: 1.7315, train acc: 0.2481, val loss: 8.1900, val acc: 0.2398  (best train acc: 0.3849, best val acc: 0.3727)\n",
      "[Epoch: 13680] train loss: 1.5918, train acc: 0.2960, val loss: 5.4731, val acc: 0.2482  (best train acc: 0.3849, best val acc: 0.3727)\n",
      "[Epoch: 13700] train loss: 1.5055, train acc: 0.2175, val loss: 3.1394, val acc: 0.2587  (best train acc: 0.3849, best val acc: 0.3727)\n",
      "[Epoch: 13720] train loss: 1.9703, train acc: 0.2525, val loss: 10.7317, val acc: 0.2378  (best train acc: 0.3849, best val acc: 0.3727)\n",
      "[Epoch: 13740] train loss: 1.6407, train acc: 0.3062, val loss: 9.8430, val acc: 0.2570  (best train acc: 0.3849, best val acc: 0.3727)\n",
      "[Epoch: 13760] train loss: 1.6996, train acc: 0.3036, val loss: 6.5807, val acc: 0.2560  (best train acc: 0.3849, best val acc: 0.3727)\n",
      "[Epoch: 13780] train loss: 1.7874, train acc: 0.3005, val loss: 15.4578, val acc: 0.2371  (best train acc: 0.3849, best val acc: 0.3727)\n",
      "[Epoch: 13800] train loss: 1.6768, train acc: 0.3052, val loss: 6.1471, val acc: 0.2270  (best train acc: 0.3849, best val acc: 0.3727)\n",
      "[Epoch: 13820] train loss: 1.7538, train acc: 0.3517, val loss: 9.9814, val acc: 0.2627  (best train acc: 0.3849, best val acc: 0.3727)\n",
      "[Epoch: 13840] train loss: 1.7148, train acc: 0.2974, val loss: 9.0255, val acc: 0.2408  (best train acc: 0.3849, best val acc: 0.3727)\n",
      "[Epoch: 13860] train loss: 1.7694, train acc: 0.2986, val loss: 10.8098, val acc: 0.2594  (best train acc: 0.3849, best val acc: 0.3727)\n",
      "[Epoch: 13880] train loss: 1.7521, train acc: 0.3030, val loss: 12.6308, val acc: 0.2415  (best train acc: 0.3849, best val acc: 0.3727)\n",
      "[Epoch: 13900] train loss: 1.5996, train acc: 0.2959, val loss: 6.3313, val acc: 0.2546  (best train acc: 0.3849, best val acc: 0.3727)\n",
      "[Epoch: 13920] train loss: 1.7607, train acc: 0.3072, val loss: 18.2667, val acc: 0.2371  (best train acc: 0.3849, best val acc: 0.3727)\n",
      "[Epoch: 13940] train loss: 1.7731, train acc: 0.2830, val loss: 11.5294, val acc: 0.2476  (best train acc: 0.3849, best val acc: 0.3727)\n",
      "[Epoch: 13960] train loss: 1.5708, train acc: 0.2973, val loss: 6.5200, val acc: 0.2546  (best train acc: 0.3849, best val acc: 0.3727)\n",
      "[Epoch: 13980] train loss: 1.6622, train acc: 0.3062, val loss: 11.6473, val acc: 0.2452  (best train acc: 0.3849, best val acc: 0.3727)\n",
      "[Epoch: 14000] train loss: 1.8933, train acc: 0.3049, val loss: 13.5506, val acc: 0.2422  (best train acc: 0.3849, best val acc: 0.3727)\n",
      "[Epoch: 14020] train loss: 1.7574, train acc: 0.3019, val loss: 9.0081, val acc: 0.2354  (best train acc: 0.3849, best val acc: 0.3727)\n",
      "[Epoch: 14040] train loss: 1.7404, train acc: 0.3517, val loss: 13.0472, val acc: 0.2391  (best train acc: 0.3890, best val acc: 0.3727)\n",
      "[Epoch: 14060] train loss: 1.8860, train acc: 0.3587, val loss: 11.8208, val acc: 0.2391  (best train acc: 0.3890, best val acc: 0.3727)\n",
      "[Epoch: 14080] train loss: 1.6554, train acc: 0.3013, val loss: 6.8770, val acc: 0.2594  (best train acc: 0.3890, best val acc: 0.3727)\n",
      "[Epoch: 14100] train loss: 1.8587, train acc: 0.3068, val loss: 8.4668, val acc: 0.2405  (best train acc: 0.3890, best val acc: 0.3727)\n",
      "[Epoch: 14120] train loss: 1.8333, train acc: 0.2643, val loss: 7.9455, val acc: 0.2530  (best train acc: 0.3890, best val acc: 0.3727)\n",
      "[Epoch: 14140] train loss: 1.5771, train acc: 0.3061, val loss: 9.0305, val acc: 0.2644  (best train acc: 0.3890, best val acc: 0.3727)\n",
      "[Epoch: 14160] train loss: 1.8580, train acc: 0.2924, val loss: 8.7971, val acc: 0.2536  (best train acc: 0.3890, best val acc: 0.3727)\n",
      "[Epoch: 14180] train loss: 1.8670, train acc: 0.3090, val loss: 13.2869, val acc: 0.2621  (best train acc: 0.3890, best val acc: 0.3727)\n",
      "[Epoch: 14200] train loss: 1.5703, train acc: 0.2961, val loss: 6.5233, val acc: 0.2074  (best train acc: 0.3890, best val acc: 0.3727)\n",
      "[Epoch: 14220] train loss: 1.7795, train acc: 0.2815, val loss: 6.0671, val acc: 0.2492  (best train acc: 0.3890, best val acc: 0.3727)\n",
      "[Epoch: 14240] train loss: 1.5271, train acc: 0.3641, val loss: 7.9076, val acc: 0.2503  (best train acc: 0.3890, best val acc: 0.3727)\n",
      "[Epoch: 14260] train loss: 1.6480, train acc: 0.2916, val loss: 15.8336, val acc: 0.2374  (best train acc: 0.3890, best val acc: 0.3727)\n",
      "[Epoch: 14280] train loss: 1.6978, train acc: 0.2915, val loss: 9.8639, val acc: 0.2374  (best train acc: 0.3890, best val acc: 0.3727)\n",
      "[Epoch: 14300] train loss: 1.7899, train acc: 0.2959, val loss: 9.3147, val acc: 0.2658  (best train acc: 0.3890, best val acc: 0.3727)\n",
      "[Epoch: 14320] train loss: 1.8078, train acc: 0.2747, val loss: 12.2870, val acc: 0.2570  (best train acc: 0.3890, best val acc: 0.3727)\n",
      "[Epoch: 14340] train loss: 1.8561, train acc: 0.2723, val loss: 11.5117, val acc: 0.2479  (best train acc: 0.3890, best val acc: 0.3727)\n",
      "[Epoch: 14360] train loss: 1.6911, train acc: 0.2755, val loss: 11.9578, val acc: 0.2482  (best train acc: 0.3890, best val acc: 0.3727)\n",
      "[Epoch: 14380] train loss: 1.7437, train acc: 0.3616, val loss: 8.5958, val acc: 0.2469  (best train acc: 0.3890, best val acc: 0.3727)\n",
      "[Epoch: 14400] train loss: 1.7675, train acc: 0.2997, val loss: 12.7653, val acc: 0.2442  (best train acc: 0.3890, best val acc: 0.3727)\n",
      "[Epoch: 14420] train loss: 1.5278, train acc: 0.3070, val loss: 4.8959, val acc: 0.2422  (best train acc: 0.3890, best val acc: 0.3727)\n",
      "[Epoch: 14440] train loss: 1.7827, train acc: 0.2511, val loss: 7.0096, val acc: 0.2607  (best train acc: 0.3890, best val acc: 0.3727)\n",
      "[Epoch: 14460] train loss: 1.9519, train acc: 0.2862, val loss: 13.9110, val acc: 0.2408  (best train acc: 0.3890, best val acc: 0.3727)\n",
      "[Epoch: 14480] train loss: 2.0137, train acc: 0.3751, val loss: 16.5874, val acc: 0.2415  (best train acc: 0.3890, best val acc: 0.3727)\n",
      "[Epoch: 14500] train loss: 1.6086, train acc: 0.3869, val loss: 8.8838, val acc: 0.2371  (best train acc: 0.3890, best val acc: 0.3727)\n",
      "[Epoch: 14520] train loss: 1.6452, train acc: 0.3040, val loss: 9.1367, val acc: 0.2506  (best train acc: 0.3890, best val acc: 0.3727)\n",
      "[Epoch: 14540] train loss: 1.5680, train acc: 0.3038, val loss: 8.2236, val acc: 0.2614  (best train acc: 0.3890, best val acc: 0.3727)\n",
      "[Epoch: 14560] train loss: 1.8266, train acc: 0.2941, val loss: 15.9876, val acc: 0.2445  (best train acc: 0.3890, best val acc: 0.3727)\n",
      "[Epoch: 14580] train loss: 1.5894, train acc: 0.2900, val loss: 10.0089, val acc: 0.2371  (best train acc: 0.3890, best val acc: 0.3727)\n",
      "[Epoch: 14600] train loss: 1.8225, train acc: 0.3012, val loss: 10.0301, val acc: 0.2165  (best train acc: 0.3890, best val acc: 0.3727)\n",
      "[Epoch: 14620] train loss: 1.8566, train acc: 0.3043, val loss: 11.0985, val acc: 0.2543  (best train acc: 0.3890, best val acc: 0.3727)\n",
      "[Epoch: 14640] train loss: 1.5242, train acc: 0.3704, val loss: 3.1809, val acc: 0.2550  (best train acc: 0.3890, best val acc: 0.3727)\n",
      "[Epoch: 14660] train loss: 1.8552, train acc: 0.3062, val loss: 13.3067, val acc: 0.2614  (best train acc: 0.3890, best val acc: 0.3727)\n",
      "[Epoch: 14680] train loss: 1.5127, train acc: 0.3699, val loss: 5.3211, val acc: 0.2624  (best train acc: 0.3890, best val acc: 0.3727)\n",
      "[Epoch: 14700] train loss: 1.6976, train acc: 0.3073, val loss: 10.1016, val acc: 0.2425  (best train acc: 0.3890, best val acc: 0.3727)\n",
      "[Epoch: 14720] train loss: 1.6327, train acc: 0.2988, val loss: 9.5492, val acc: 0.2152  (best train acc: 0.3890, best val acc: 0.3727)\n",
      "[Epoch: 14740] train loss: 1.6418, train acc: 0.3798, val loss: 9.6436, val acc: 0.2489  (best train acc: 0.3890, best val acc: 0.3727)\n",
      "[Epoch: 14760] train loss: 1.6853, train acc: 0.3802, val loss: 6.6726, val acc: 0.2435  (best train acc: 0.3890, best val acc: 0.3727)\n",
      "[Epoch: 14780] train loss: 1.7139, train acc: 0.3280, val loss: 6.2049, val acc: 0.2310  (best train acc: 0.3890, best val acc: 0.3727)\n",
      "[Epoch: 14800] train loss: 1.9421, train acc: 0.2939, val loss: 13.7431, val acc: 0.2435  (best train acc: 0.3963, best val acc: 0.3727)\n",
      "[Epoch: 14820] train loss: 1.7704, train acc: 0.3049, val loss: 12.7968, val acc: 0.2587  (best train acc: 0.3963, best val acc: 0.3727)\n",
      "[Epoch: 14840] train loss: 1.6241, train acc: 0.3089, val loss: 6.9841, val acc: 0.2482  (best train acc: 0.3963, best val acc: 0.3727)\n",
      "[Epoch: 14860] train loss: 1.7205, train acc: 0.3495, val loss: 12.7769, val acc: 0.2449  (best train acc: 0.3963, best val acc: 0.3727)\n",
      "[Epoch: 14880] train loss: 1.4982, train acc: 0.2854, val loss: 6.5979, val acc: 0.2530  (best train acc: 0.3963, best val acc: 0.3727)\n",
      "[Epoch: 14900] train loss: 1.6170, train acc: 0.2885, val loss: 4.1108, val acc: 0.2573  (best train acc: 0.3963, best val acc: 0.3727)\n",
      "[Epoch: 14920] train loss: 1.6766, train acc: 0.2895, val loss: 7.7015, val acc: 0.2648  (best train acc: 0.3963, best val acc: 0.3727)\n",
      "[Epoch: 14940] train loss: 1.6172, train acc: 0.2880, val loss: 9.2066, val acc: 0.2445  (best train acc: 0.3963, best val acc: 0.3727)\n",
      "[Epoch: 14960] train loss: 1.5983, train acc: 0.3018, val loss: 7.3306, val acc: 0.2516  (best train acc: 0.3963, best val acc: 0.3727)\n",
      "[Epoch: 14980] train loss: 1.8721, train acc: 0.3772, val loss: 16.5298, val acc: 0.2432  (best train acc: 0.3963, best val acc: 0.3727)\n",
      "[Epoch: 15000] train loss: 1.6449, train acc: 0.2980, val loss: 8.5006, val acc: 0.2452  (best train acc: 0.3963, best val acc: 0.3727)\n",
      "[Epoch: 15020] train loss: 1.6072, train acc: 0.2332, val loss: 6.7156, val acc: 0.2455  (best train acc: 0.3963, best val acc: 0.3727)\n",
      "[Epoch: 15040] train loss: 1.6718, train acc: 0.2903, val loss: 8.3332, val acc: 0.2411  (best train acc: 0.3963, best val acc: 0.3727)\n",
      "[Epoch: 15060] train loss: 1.5225, train acc: 0.2837, val loss: 5.5203, val acc: 0.2499  (best train acc: 0.3963, best val acc: 0.3727)\n",
      "[Epoch: 15080] train loss: 1.5570, train acc: 0.3014, val loss: 4.7195, val acc: 0.2270  (best train acc: 0.3963, best val acc: 0.3727)\n",
      "[Epoch: 15100] train loss: 1.6164, train acc: 0.2689, val loss: 5.3706, val acc: 0.2516  (best train acc: 0.3963, best val acc: 0.3727)\n",
      "[Epoch: 15120] train loss: 1.5700, train acc: 0.3010, val loss: 5.0901, val acc: 0.2563  (best train acc: 0.3963, best val acc: 0.3727)\n",
      "[Epoch: 15140] train loss: 1.5784, train acc: 0.3044, val loss: 11.2100, val acc: 0.2368  (best train acc: 0.3963, best val acc: 0.3727)\n",
      "[Epoch: 15160] train loss: 1.7705, train acc: 0.2295, val loss: 8.9471, val acc: 0.2536  (best train acc: 0.3963, best val acc: 0.3727)\n",
      "[Epoch: 15180] train loss: 1.9182, train acc: 0.2982, val loss: 15.6872, val acc: 0.2378  (best train acc: 0.3963, best val acc: 0.3727)\n",
      "[Epoch: 15200] train loss: 2.2348, train acc: 0.3057, val loss: 23.1013, val acc: 0.2445  (best train acc: 0.3963, best val acc: 0.3727)\n",
      "[Epoch: 15220] train loss: 1.6915, train acc: 0.2924, val loss: 7.3047, val acc: 0.2266  (best train acc: 0.3963, best val acc: 0.3727)\n",
      "[Epoch: 15240] train loss: 1.6526, train acc: 0.3002, val loss: 5.3497, val acc: 0.2374  (best train acc: 0.3963, best val acc: 0.3727)\n",
      "[Epoch: 15260] train loss: 1.5690, train acc: 0.3014, val loss: 8.0197, val acc: 0.2563  (best train acc: 0.3963, best val acc: 0.3727)\n",
      "[Epoch: 15280] train loss: 1.7952, train acc: 0.2893, val loss: 10.3744, val acc: 0.2648  (best train acc: 0.3963, best val acc: 0.3727)\n",
      "[Epoch: 15300] train loss: 1.6757, train acc: 0.2846, val loss: 7.0712, val acc: 0.2300  (best train acc: 0.3963, best val acc: 0.3727)\n",
      "[Epoch: 15320] train loss: 1.5971, train acc: 0.2843, val loss: 5.7244, val acc: 0.2556  (best train acc: 0.3963, best val acc: 0.3727)\n",
      "[Epoch: 15340] train loss: 1.5753, train acc: 0.2833, val loss: 5.8456, val acc: 0.2610  (best train acc: 0.3963, best val acc: 0.3727)\n",
      "[Epoch: 15360] train loss: 1.7162, train acc: 0.3024, val loss: 8.2013, val acc: 0.2489  (best train acc: 0.3963, best val acc: 0.3727)\n",
      "[Epoch: 15380] train loss: 1.5750, train acc: 0.3078, val loss: 3.5990, val acc: 0.2253  (best train acc: 0.3963, best val acc: 0.3727)\n",
      "[Epoch: 15400] train loss: 1.5847, train acc: 0.3052, val loss: 6.3155, val acc: 0.2374  (best train acc: 0.3963, best val acc: 0.3727)\n",
      "[Epoch: 15420] train loss: 1.6931, train acc: 0.3052, val loss: 9.6182, val acc: 0.2597  (best train acc: 0.3963, best val acc: 0.3727)\n",
      "[Epoch: 15440] train loss: 1.8998, train acc: 0.2751, val loss: 12.3647, val acc: 0.2560  (best train acc: 0.3963, best val acc: 0.3727)\n",
      "[Epoch: 15460] train loss: 1.6104, train acc: 0.2991, val loss: 8.5941, val acc: 0.2371  (best train acc: 0.3963, best val acc: 0.3727)\n",
      "[Epoch: 15480] train loss: 1.7086, train acc: 0.2955, val loss: 9.8976, val acc: 0.2540  (best train acc: 0.3963, best val acc: 0.3727)\n",
      "[Epoch: 15500] train loss: 1.6359, train acc: 0.3070, val loss: 4.4950, val acc: 0.2216  (best train acc: 0.3963, best val acc: 0.3727)\n",
      "[Epoch: 15520] train loss: 2.0995, train acc: 0.2513, val loss: 12.2304, val acc: 0.2371  (best train acc: 0.3963, best val acc: 0.3727)\n",
      "[Epoch: 15540] train loss: 1.8141, train acc: 0.2998, val loss: 10.7804, val acc: 0.2604  (best train acc: 0.3963, best val acc: 0.3727)\n",
      "[Epoch: 15560] train loss: 1.6189, train acc: 0.3036, val loss: 7.6518, val acc: 0.2644  (best train acc: 0.3963, best val acc: 0.3727)\n",
      "[Epoch: 15580] train loss: 1.8529, train acc: 0.3023, val loss: 12.6597, val acc: 0.2530  (best train acc: 0.3963, best val acc: 0.3727)\n",
      "[Epoch: 15600] train loss: 1.5354, train acc: 0.2771, val loss: 6.8034, val acc: 0.2587  (best train acc: 0.3963, best val acc: 0.3727)\n",
      "[Epoch: 15620] train loss: 1.7779, train acc: 0.2559, val loss: 7.9595, val acc: 0.2425  (best train acc: 0.3963, best val acc: 0.3727)\n",
      "[Epoch: 15640] train loss: 1.6779, train acc: 0.2998, val loss: 7.2474, val acc: 0.2516  (best train acc: 0.3963, best val acc: 0.3727)\n",
      "[Epoch: 15660] train loss: 1.7082, train acc: 0.3092, val loss: 8.5264, val acc: 0.2270  (best train acc: 0.3963, best val acc: 0.3727)\n",
      "[Epoch: 15680] train loss: 1.9410, train acc: 0.2806, val loss: 16.1315, val acc: 0.2577  (best train acc: 0.3963, best val acc: 0.3727)\n",
      "[Epoch: 15700] train loss: 1.6225, train acc: 0.2855, val loss: 6.8316, val acc: 0.2452  (best train acc: 0.3963, best val acc: 0.3727)\n",
      "[Epoch: 15720] train loss: 1.6172, train acc: 0.2751, val loss: 9.5526, val acc: 0.2492  (best train acc: 0.3963, best val acc: 0.3727)\n",
      "[Epoch: 15740] train loss: 1.7260, train acc: 0.3013, val loss: 11.0489, val acc: 0.2476  (best train acc: 0.3963, best val acc: 0.3727)\n",
      "[Epoch: 15760] train loss: 1.7469, train acc: 0.2856, val loss: 11.5600, val acc: 0.2314  (best train acc: 0.3963, best val acc: 0.3727)\n",
      "[Epoch: 15780] train loss: 1.8733, train acc: 0.3041, val loss: 11.9274, val acc: 0.2583  (best train acc: 0.3963, best val acc: 0.3727)\n",
      "[Epoch: 15800] train loss: 1.7511, train acc: 0.2713, val loss: 6.7617, val acc: 0.2253  (best train acc: 0.3963, best val acc: 0.3727)\n",
      "[Epoch: 15820] train loss: 1.6733, train acc: 0.3092, val loss: 10.9858, val acc: 0.2418  (best train acc: 0.3963, best val acc: 0.3727)\n",
      "[Epoch: 15840] train loss: 1.6907, train acc: 0.3018, val loss: 9.1611, val acc: 0.2648  (best train acc: 0.3963, best val acc: 0.3727)\n",
      "[Epoch: 15860] train loss: 1.7270, train acc: 0.2590, val loss: 9.3156, val acc: 0.2631  (best train acc: 0.3963, best val acc: 0.3727)\n",
      "[Epoch: 15880] train loss: 1.5300, train acc: 0.3034, val loss: 6.2676, val acc: 0.2223  (best train acc: 0.3963, best val acc: 0.3727)\n",
      "[Epoch: 15900] train loss: 1.6361, train acc: 0.2927, val loss: 8.0403, val acc: 0.2395  (best train acc: 0.3963, best val acc: 0.3727)\n",
      "[Epoch: 15920] train loss: 1.7896, train acc: 0.2564, val loss: 9.4084, val acc: 0.2641  (best train acc: 0.3963, best val acc: 0.3727)\n",
      "[Epoch: 15940] train loss: 1.9349, train acc: 0.2903, val loss: 15.5611, val acc: 0.2624  (best train acc: 0.3963, best val acc: 0.3727)\n",
      "[Epoch: 15960] train loss: 1.5590, train acc: 0.2975, val loss: 6.9753, val acc: 0.2499  (best train acc: 0.3963, best val acc: 0.3727)\n",
      "[Epoch: 15980] train loss: 1.7138, train acc: 0.2921, val loss: 9.9251, val acc: 0.2432  (best train acc: 0.3963, best val acc: 0.3727)\n",
      "[Epoch: 16000] train loss: 1.6577, train acc: 0.2882, val loss: 6.8946, val acc: 0.2314  (best train acc: 0.3963, best val acc: 0.3727)\n",
      "[Epoch: 16020] train loss: 1.6204, train acc: 0.2436, val loss: 5.9836, val acc: 0.2503  (best train acc: 0.3963, best val acc: 0.3727)\n",
      "[Epoch: 16040] train loss: 1.8044, train acc: 0.2964, val loss: 11.9851, val acc: 0.2438  (best train acc: 0.3963, best val acc: 0.3727)\n",
      "[Epoch: 16060] train loss: 1.6053, train acc: 0.2736, val loss: 4.6608, val acc: 0.2229  (best train acc: 0.3963, best val acc: 0.3727)\n",
      "[Epoch: 16080] train loss: 1.7423, train acc: 0.2542, val loss: 9.2716, val acc: 0.2408  (best train acc: 0.3963, best val acc: 0.3727)\n",
      "[Epoch: 16100] train loss: 1.8801, train acc: 0.2895, val loss: 16.2228, val acc: 0.2371  (best train acc: 0.3963, best val acc: 0.3727)\n",
      "[Epoch: 16120] train loss: 1.8075, train acc: 0.3065, val loss: 7.8348, val acc: 0.2108  (best train acc: 0.3963, best val acc: 0.3727)\n",
      "[Epoch: 16140] train loss: 1.8650, train acc: 0.2911, val loss: 11.2578, val acc: 0.2391  (best train acc: 0.3963, best val acc: 0.3727)\n",
      "[Epoch: 16160] train loss: 1.8402, train acc: 0.3099, val loss: 9.6396, val acc: 0.2513  (best train acc: 0.3963, best val acc: 0.3727)\n",
      "[Epoch: 16180] train loss: 1.6433, train acc: 0.2937, val loss: 8.8576, val acc: 0.1956  (best train acc: 0.3963, best val acc: 0.3727)\n",
      "[Epoch: 16200] train loss: 1.7740, train acc: 0.2820, val loss: 8.6788, val acc: 0.2583  (best train acc: 0.3963, best val acc: 0.3727)\n",
      "[Epoch: 16220] train loss: 1.5307, train acc: 0.2937, val loss: 6.8009, val acc: 0.2428  (best train acc: 0.3963, best val acc: 0.3727)\n",
      "[Epoch: 16240] train loss: 1.7689, train acc: 0.2932, val loss: 16.9267, val acc: 0.2371  (best train acc: 0.3963, best val acc: 0.3727)\n",
      "[Epoch: 16260] train loss: 1.6735, train acc: 0.2624, val loss: 6.2147, val acc: 0.2354  (best train acc: 0.3963, best val acc: 0.3727)\n",
      "[Epoch: 16280] train loss: 1.6212, train acc: 0.3041, val loss: 12.0376, val acc: 0.2371  (best train acc: 0.3963, best val acc: 0.3727)\n",
      "[Epoch: 16300] train loss: 1.6827, train acc: 0.2976, val loss: 8.1569, val acc: 0.2155  (best train acc: 0.3963, best val acc: 0.3727)\n",
      "[Epoch: 16320] train loss: 1.6961, train acc: 0.3005, val loss: 9.1503, val acc: 0.2479  (best train acc: 0.3963, best val acc: 0.3727)\n",
      "[Epoch: 16340] train loss: 1.7043, train acc: 0.3030, val loss: 8.8213, val acc: 0.2310  (best train acc: 0.3963, best val acc: 0.3727)\n",
      "[Epoch: 16360] train loss: 1.5617, train acc: 0.2990, val loss: 9.1356, val acc: 0.2374  (best train acc: 0.3963, best val acc: 0.3727)\n",
      "[Epoch: 16380] train loss: 1.9399, train acc: 0.3087, val loss: 10.3157, val acc: 0.2206  (best train acc: 0.3963, best val acc: 0.3727)\n",
      "[Epoch: 16400] train loss: 1.5545, train acc: 0.2941, val loss: 7.2392, val acc: 0.2401  (best train acc: 0.3963, best val acc: 0.3727)\n",
      "[Epoch: 16420] train loss: 1.5627, train acc: 0.3013, val loss: 10.2683, val acc: 0.2563  (best train acc: 0.3963, best val acc: 0.3727)\n",
      "[Epoch: 16440] train loss: 1.6882, train acc: 0.2844, val loss: 7.5257, val acc: 0.2563  (best train acc: 0.3963, best val acc: 0.3727)\n",
      "[Epoch: 16460] train loss: 1.6344, train acc: 0.2955, val loss: 6.4335, val acc: 0.2378  (best train acc: 0.3963, best val acc: 0.3727)\n",
      "[Epoch: 16480] train loss: 1.5924, train acc: 0.2934, val loss: 7.7583, val acc: 0.1511  (best train acc: 0.3963, best val acc: 0.3727)\n",
      "[Epoch: 16500] train loss: 1.7989, train acc: 0.2952, val loss: 13.3798, val acc: 0.2411  (best train acc: 0.3963, best val acc: 0.3727)\n",
      "[Epoch: 16520] train loss: 1.7354, train acc: 0.2896, val loss: 8.2594, val acc: 0.2010  (best train acc: 0.3963, best val acc: 0.3727)\n",
      "[Epoch: 16540] train loss: 1.6858, train acc: 0.2864, val loss: 6.5737, val acc: 0.2061  (best train acc: 0.3963, best val acc: 0.3727)\n",
      "[Epoch: 16560] train loss: 1.9049, train acc: 0.3005, val loss: 12.1829, val acc: 0.2526  (best train acc: 0.3963, best val acc: 0.3727)\n",
      "[Epoch: 16580] train loss: 1.6442, train acc: 0.2915, val loss: 13.0765, val acc: 0.2371  (best train acc: 0.3963, best val acc: 0.3727)\n",
      "[Epoch: 16600] train loss: 1.6253, train acc: 0.3021, val loss: 7.8518, val acc: 0.2536  (best train acc: 0.3963, best val acc: 0.3727)\n",
      "[Epoch: 16620] train loss: 1.5699, train acc: 0.2784, val loss: 2.7651, val acc: 0.2408  (best train acc: 0.3963, best val acc: 0.3727)\n",
      "[Epoch: 16640] train loss: 1.7625, train acc: 0.2880, val loss: 10.4544, val acc: 0.2422  (best train acc: 0.3963, best val acc: 0.3727)\n",
      "[Epoch: 16660] train loss: 1.6178, train acc: 0.2937, val loss: 7.2955, val acc: 0.2580  (best train acc: 0.3963, best val acc: 0.3727)\n",
      "[Epoch: 16680] train loss: 1.5934, train acc: 0.2908, val loss: 7.7044, val acc: 0.2476  (best train acc: 0.3963, best val acc: 0.3727)\n",
      "[Epoch: 16700] train loss: 1.9160, train acc: 0.2965, val loss: 13.7046, val acc: 0.2428  (best train acc: 0.3963, best val acc: 0.3727)\n",
      "[Epoch: 16720] train loss: 1.6167, train acc: 0.2646, val loss: 6.4233, val acc: 0.2327  (best train acc: 0.3963, best val acc: 0.3727)\n",
      "[Epoch: 16740] train loss: 1.6658, train acc: 0.2981, val loss: 5.9667, val acc: 0.2047  (best train acc: 0.3963, best val acc: 0.3727)\n",
      "[Epoch: 16760] train loss: 1.5885, train acc: 0.2943, val loss: 5.9150, val acc: 0.2078  (best train acc: 0.3963, best val acc: 0.3727)\n",
      "[Epoch: 16780] train loss: 1.7676, train acc: 0.2433, val loss: 7.3211, val acc: 0.2354  (best train acc: 0.3963, best val acc: 0.3727)\n",
      "[Epoch: 16800] train loss: 1.7811, train acc: 0.3013, val loss: 11.0314, val acc: 0.2600  (best train acc: 0.3963, best val acc: 0.3727)\n",
      "[Epoch: 16820] train loss: 1.5574, train acc: 0.2976, val loss: 8.0893, val acc: 0.2610  (best train acc: 0.3963, best val acc: 0.3727)\n",
      "[Epoch: 16840] train loss: 1.5891, train acc: 0.2847, val loss: 4.7975, val acc: 0.1268  (best train acc: 0.3963, best val acc: 0.3727)\n",
      "[Epoch: 16860] train loss: 1.7592, train acc: 0.2919, val loss: 11.3350, val acc: 0.2523  (best train acc: 0.3963, best val acc: 0.3727)\n",
      "[Epoch: 16880] train loss: 1.6432, train acc: 0.2955, val loss: 7.6432, val acc: 0.2442  (best train acc: 0.3963, best val acc: 0.3727)\n",
      "[Epoch: 16900] train loss: 1.5127, train acc: 0.2918, val loss: 5.2811, val acc: 0.2570  (best train acc: 0.3963, best val acc: 0.3727)\n",
      "[Epoch: 16920] train loss: 1.9032, train acc: 0.2922, val loss: 13.0681, val acc: 0.2263  (best train acc: 0.3963, best val acc: 0.3727)\n",
      "[Epoch: 16940] train loss: 1.8099, train acc: 0.2602, val loss: 11.3282, val acc: 0.2374  (best train acc: 0.3963, best val acc: 0.3727)\n",
      "[Epoch: 16960] train loss: 1.7518, train acc: 0.3000, val loss: 9.5513, val acc: 0.2401  (best train acc: 0.3963, best val acc: 0.3727)\n",
      "[Epoch: 16980] train loss: 1.6771, train acc: 0.2995, val loss: 5.3503, val acc: 0.2354  (best train acc: 0.3963, best val acc: 0.3727)\n",
      "[Epoch: 17000] train loss: 1.5742, train acc: 0.2453, val loss: 7.9492, val acc: 0.2533  (best train acc: 0.3963, best val acc: 0.3727)\n",
      "[Epoch: 17020] train loss: 1.5816, train acc: 0.2967, val loss: 8.6294, val acc: 0.2563  (best train acc: 0.3963, best val acc: 0.3727)\n",
      "[Epoch: 17040] train loss: 1.7021, train acc: 0.3028, val loss: 8.7025, val acc: 0.2604  (best train acc: 0.3963, best val acc: 0.3727)\n",
      "[Epoch: 17060] train loss: 1.6266, train acc: 0.2747, val loss: 6.1247, val acc: 0.2553  (best train acc: 0.3963, best val acc: 0.3727)\n",
      "[Epoch: 17080] train loss: 1.7849, train acc: 0.2723, val loss: 13.9823, val acc: 0.2405  (best train acc: 0.3963, best val acc: 0.3727)\n",
      "[Epoch: 17100] train loss: 1.7381, train acc: 0.2580, val loss: 10.4191, val acc: 0.2530  (best train acc: 0.3963, best val acc: 0.3727)\n",
      "[Epoch: 17120] train loss: 1.7807, train acc: 0.2530, val loss: 6.9885, val acc: 0.2287  (best train acc: 0.3963, best val acc: 0.3727)\n",
      "[Epoch: 17140] train loss: 1.5829, train acc: 0.3018, val loss: 9.2885, val acc: 0.2415  (best train acc: 0.3963, best val acc: 0.3727)\n",
      "[Epoch: 17160] train loss: 1.5875, train acc: 0.2998, val loss: 8.9971, val acc: 0.2405  (best train acc: 0.3963, best val acc: 0.3727)\n",
      "[Epoch: 17180] train loss: 1.6680, train acc: 0.3061, val loss: 10.7043, val acc: 0.2395  (best train acc: 0.3963, best val acc: 0.3727)\n",
      "[Epoch: 17200] train loss: 1.5567, train acc: 0.3058, val loss: 5.1370, val acc: 0.2540  (best train acc: 0.3963, best val acc: 0.3727)\n",
      "[Epoch: 17220] train loss: 1.7444, train acc: 0.3045, val loss: 9.4744, val acc: 0.2556  (best train acc: 0.3963, best val acc: 0.3727)\n",
      "[Epoch: 17240] train loss: 1.8127, train acc: 0.2925, val loss: 13.5079, val acc: 0.2172  (best train acc: 0.3963, best val acc: 0.3727)\n",
      "[Epoch: 17260] train loss: 1.5981, train acc: 0.3099, val loss: 9.6687, val acc: 0.2405  (best train acc: 0.3963, best val acc: 0.3727)\n",
      "[Epoch: 17280] train loss: 1.9252, train acc: 0.3116, val loss: 10.2586, val acc: 0.2229  (best train acc: 0.3963, best val acc: 0.3727)\n",
      "[Epoch: 17300] train loss: 1.6064, train acc: 0.2832, val loss: 8.1292, val acc: 0.2256  (best train acc: 0.3963, best val acc: 0.3727)\n",
      "[Epoch: 17320] train loss: 1.6241, train acc: 0.2966, val loss: 4.9799, val acc: 0.2351  (best train acc: 0.3963, best val acc: 0.3727)\n",
      "[Epoch: 17340] train loss: 1.5799, train acc: 0.2987, val loss: 4.8966, val acc: 0.1592  (best train acc: 0.3963, best val acc: 0.3727)\n",
      "[Epoch: 17360] train loss: 1.9483, train acc: 0.2889, val loss: 13.6531, val acc: 0.2496  (best train acc: 0.3963, best val acc: 0.3727)\n",
      "[Epoch: 17380] train loss: 1.8847, train acc: 0.2888, val loss: 18.9039, val acc: 0.2519  (best train acc: 0.3963, best val acc: 0.3727)\n",
      "[Epoch: 17400] train loss: 1.5404, train acc: 0.3020, val loss: 6.4180, val acc: 0.2509  (best train acc: 0.3963, best val acc: 0.3727)\n",
      "[Epoch: 17420] train loss: 1.6146, train acc: 0.3109, val loss: 10.7861, val acc: 0.2573  (best train acc: 0.3963, best val acc: 0.3727)\n",
      "[Epoch: 17440] train loss: 1.7310, train acc: 0.2795, val loss: 8.2101, val acc: 0.2519  (best train acc: 0.3963, best val acc: 0.3727)\n",
      "[Epoch: 17460] train loss: 1.5846, train acc: 0.2944, val loss: 7.2964, val acc: 0.2455  (best train acc: 0.3963, best val acc: 0.3727)\n",
      "[Epoch: 17480] train loss: 1.6039, train acc: 0.2885, val loss: 6.9632, val acc: 0.2455  (best train acc: 0.3963, best val acc: 0.3727)\n",
      "[Epoch: 17500] train loss: 1.7652, train acc: 0.2655, val loss: 12.1115, val acc: 0.2405  (best train acc: 0.3963, best val acc: 0.3727)\n",
      "[Epoch: 17520] train loss: 1.6734, train acc: 0.3025, val loss: 8.5723, val acc: 0.2344  (best train acc: 0.3963, best val acc: 0.3727)\n",
      "[Epoch: 17540] train loss: 1.7116, train acc: 0.2953, val loss: 9.0192, val acc: 0.2233  (best train acc: 0.3963, best val acc: 0.3727)\n",
      "[Epoch: 17560] train loss: 1.5927, train acc: 0.3316, val loss: 8.4658, val acc: 0.1976  (best train acc: 0.3963, best val acc: 0.3727)\n",
      "[Epoch: 17580] train loss: 1.5120, train acc: 0.3352, val loss: 8.1260, val acc: 0.2462  (best train acc: 0.3963, best val acc: 0.3727)\n",
      "[Epoch: 17600] train loss: 1.5214, train acc: 0.3443, val loss: 7.9695, val acc: 0.2469  (best train acc: 0.3963, best val acc: 0.3727)\n",
      "[Epoch: 17620] train loss: 1.5663, train acc: 0.3402, val loss: 8.1970, val acc: 0.2472  (best train acc: 0.3963, best val acc: 0.3727)\n",
      "[Epoch: 17640] train loss: 1.4925, train acc: 0.3383, val loss: 7.5801, val acc: 0.2479  (best train acc: 0.3963, best val acc: 0.3727)\n",
      "[Epoch: 17660] train loss: 1.5289, train acc: 0.3242, val loss: 6.5151, val acc: 0.2617  (best train acc: 0.3963, best val acc: 0.3727)\n",
      "[Epoch: 17680] train loss: 2.0971, train acc: 0.3374, val loss: 9.4776, val acc: 0.2374  (best train acc: 0.3963, best val acc: 0.3872)\n",
      "[Epoch: 17700] train loss: 1.4924, train acc: 0.3395, val loss: 5.4456, val acc: 0.2975  (best train acc: 0.3963, best val acc: 0.3872)\n",
      "[Epoch: 17720] train loss: 1.5230, train acc: 0.3254, val loss: 6.1140, val acc: 0.2408  (best train acc: 0.3963, best val acc: 0.4263)\n",
      "[Epoch: 17740] train loss: 1.7108, train acc: 0.3300, val loss: 7.0512, val acc: 0.2411  (best train acc: 0.3963, best val acc: 0.4476)\n",
      "[Epoch: 17760] train loss: 1.6007, train acc: 0.3100, val loss: 7.1428, val acc: 0.2415  (best train acc: 0.3963, best val acc: 0.4476)\n",
      "[Epoch: 17780] train loss: 1.5114, train acc: 0.3159, val loss: 5.9081, val acc: 0.2422  (best train acc: 0.3963, best val acc: 0.4476)\n",
      "[Epoch: 17800] train loss: 1.4901, train acc: 0.3341, val loss: 4.3986, val acc: 0.2766  (best train acc: 0.3963, best val acc: 0.4476)\n",
      "[Epoch: 17820] train loss: 1.5098, train acc: 0.3644, val loss: 4.7595, val acc: 0.2513  (best train acc: 0.3963, best val acc: 0.4476)\n",
      "[Epoch: 17840] train loss: 1.7131, train acc: 0.3148, val loss: 5.4987, val acc: 0.2418  (best train acc: 0.3963, best val acc: 0.4476)\n",
      "[Epoch: 17860] train loss: 1.5216, train acc: 0.3282, val loss: 4.3278, val acc: 0.2459  (best train acc: 0.3963, best val acc: 0.4476)\n",
      "[Epoch: 17880] train loss: 1.4715, train acc: 0.3224, val loss: 3.5224, val acc: 0.2459  (best train acc: 0.3963, best val acc: 0.4476)\n",
      "[Epoch: 17900] train loss: 1.4536, train acc: 0.3373, val loss: 2.8094, val acc: 0.2418  (best train acc: 0.3963, best val acc: 0.4476)\n",
      "[Epoch: 17920] train loss: 1.4492, train acc: 0.3464, val loss: 2.5635, val acc: 0.2432  (best train acc: 0.3963, best val acc: 0.4476)\n",
      "[Epoch: 17940] train loss: 1.4414, train acc: 0.3412, val loss: 2.4437, val acc: 0.2462  (best train acc: 0.3963, best val acc: 0.4476)\n",
      "[Epoch: 17960] train loss: 1.4294, train acc: 0.3490, val loss: 2.0057, val acc: 0.3865  (best train acc: 0.3963, best val acc: 0.4476)\n",
      "[Epoch: 17980] train loss: 1.6115, train acc: 0.3292, val loss: 5.7655, val acc: 0.2408  (best train acc: 0.3963, best val acc: 0.4863)\n",
      "[Epoch: 18000] train loss: 1.4657, train acc: 0.3263, val loss: 4.7178, val acc: 0.2452  (best train acc: 0.3963, best val acc: 0.4863)\n",
      "[Epoch: 18020] train loss: 1.4482, train acc: 0.3117, val loss: 3.1769, val acc: 0.2452  (best train acc: 0.3963, best val acc: 0.4863)\n",
      "[Epoch: 18040] train loss: 1.4429, train acc: 0.3266, val loss: 2.8930, val acc: 0.2465  (best train acc: 0.3963, best val acc: 0.4863)\n",
      "[Epoch: 18060] train loss: 1.4448, train acc: 0.3284, val loss: 2.7850, val acc: 0.2465  (best train acc: 0.3963, best val acc: 0.4863)\n",
      "[Epoch: 18080] train loss: 1.4384, train acc: 0.3386, val loss: 2.6297, val acc: 0.2428  (best train acc: 0.3963, best val acc: 0.4863)\n",
      "[Epoch: 18100] train loss: 1.4244, train acc: 0.3449, val loss: 2.5757, val acc: 0.2452  (best train acc: 0.3963, best val acc: 0.4863)\n",
      "[Epoch: 18120] train loss: 1.4330, train acc: 0.3424, val loss: 2.3850, val acc: 0.2432  (best train acc: 0.3963, best val acc: 0.4863)\n",
      "[Epoch: 18140] train loss: 1.4467, train acc: 0.3305, val loss: 2.3036, val acc: 0.2452  (best train acc: 0.3963, best val acc: 0.4863)\n",
      "[Epoch: 18160] train loss: 1.4366, train acc: 0.3393, val loss: 2.1977, val acc: 0.2462  (best train acc: 0.3963, best val acc: 0.4863)\n",
      "[Epoch: 18180] train loss: 1.4008, train acc: 0.3510, val loss: 2.1470, val acc: 0.2452  (best train acc: 0.3963, best val acc: 0.4863)\n",
      "[Epoch: 18200] train loss: 1.3884, train acc: 0.3572, val loss: 2.1196, val acc: 0.2452  (best train acc: 0.3963, best val acc: 0.4863)\n",
      "[Epoch: 18220] train loss: 1.4086, train acc: 0.3498, val loss: 2.1117, val acc: 0.2455  (best train acc: 0.3963, best val acc: 0.4863)\n",
      "[Epoch: 18240] train loss: 1.4151, train acc: 0.3453, val loss: 2.1085, val acc: 0.2455  (best train acc: 0.3963, best val acc: 0.4863)\n",
      "[Epoch: 18260] train loss: 1.4228, train acc: 0.3445, val loss: 2.0508, val acc: 0.2425  (best train acc: 0.3963, best val acc: 0.4863)\n",
      "[Epoch: 18280] train loss: 1.4016, train acc: 0.3496, val loss: 2.0133, val acc: 0.2442  (best train acc: 0.3963, best val acc: 0.4863)\n",
      "[Epoch: 18300] train loss: 1.4184, train acc: 0.3492, val loss: 2.0007, val acc: 0.2469  (best train acc: 0.3963, best val acc: 0.4863)\n",
      "[Epoch: 18320] train loss: 1.4042, train acc: 0.3572, val loss: 1.4200, val acc: 0.4597  (best train acc: 0.3963, best val acc: 0.4863)\n",
      "[Epoch: 18340] train loss: 1.5388, train acc: 0.3506, val loss: 2.8112, val acc: 0.2378  (best train acc: 0.4110, best val acc: 0.4863)\n",
      "[Epoch: 18360] train loss: 1.3460, train acc: 0.3931, val loss: 1.5096, val acc: 0.4698  (best train acc: 0.4110, best val acc: 0.4863)\n",
      "[Epoch: 18380] train loss: 2.0709, train acc: 0.3360, val loss: 5.7302, val acc: 0.2411  (best train acc: 0.4110, best val acc: 0.4863)\n",
      "[Epoch: 18400] train loss: 2.0487, train acc: 0.3381, val loss: 5.1418, val acc: 0.2384  (best train acc: 0.4110, best val acc: 0.4863)\n",
      "[Epoch: 18420] train loss: 1.6444, train acc: 0.3622, val loss: 3.5825, val acc: 0.2415  (best train acc: 0.4110, best val acc: 0.4863)\n",
      "[Epoch: 18440] train loss: 1.4178, train acc: 0.3540, val loss: 2.1629, val acc: 0.2415  (best train acc: 0.4110, best val acc: 0.4863)\n",
      "[Epoch: 18460] train loss: 1.4138, train acc: 0.3506, val loss: 1.9118, val acc: 0.2472  (best train acc: 0.4110, best val acc: 0.4863)\n",
      "[Epoch: 18480] train loss: 1.3989, train acc: 0.3618, val loss: 2.0626, val acc: 0.2482  (best train acc: 0.4110, best val acc: 0.4863)\n",
      "[Epoch: 18500] train loss: 1.3989, train acc: 0.3641, val loss: 2.1102, val acc: 0.2445  (best train acc: 0.4110, best val acc: 0.4863)\n",
      "[Epoch: 18520] train loss: 1.4252, train acc: 0.3438, val loss: 2.0959, val acc: 0.2455  (best train acc: 0.4110, best val acc: 0.4863)\n",
      "[Epoch: 18540] train loss: 1.3774, train acc: 0.3670, val loss: 2.0717, val acc: 0.2465  (best train acc: 0.4110, best val acc: 0.4863)\n",
      "[Epoch: 18560] train loss: 1.3909, train acc: 0.3613, val loss: 2.0907, val acc: 0.2449  (best train acc: 0.4110, best val acc: 0.4863)\n",
      "[Epoch: 18580] train loss: 1.4028, train acc: 0.3578, val loss: 2.0763, val acc: 0.2452  (best train acc: 0.4110, best val acc: 0.4863)\n",
      "[Epoch: 18600] train loss: 1.3913, train acc: 0.3595, val loss: 2.0904, val acc: 0.2455  (best train acc: 0.4110, best val acc: 0.4863)\n",
      "[Epoch: 18620] train loss: 1.3950, train acc: 0.3564, val loss: 2.0743, val acc: 0.2452  (best train acc: 0.4110, best val acc: 0.4863)\n",
      "[Epoch: 18640] train loss: 1.3737, train acc: 0.3681, val loss: 2.0645, val acc: 0.2459  (best train acc: 0.4110, best val acc: 0.4863)\n",
      "[Epoch: 18660] train loss: 1.3854, train acc: 0.3607, val loss: 2.0733, val acc: 0.2452  (best train acc: 0.4110, best val acc: 0.4863)\n",
      "[Epoch: 18680] train loss: 1.3709, train acc: 0.3590, val loss: 2.0872, val acc: 0.2449  (best train acc: 0.4110, best val acc: 0.4863)\n",
      "[Epoch: 18700] train loss: 1.3679, train acc: 0.3701, val loss: 2.0867, val acc: 0.2462  (best train acc: 0.4110, best val acc: 0.4863)\n",
      "[Epoch: 18720] train loss: 1.3736, train acc: 0.3642, val loss: 2.0574, val acc: 0.2455  (best train acc: 0.4110, best val acc: 0.4863)\n",
      "[Epoch: 18740] train loss: 1.3789, train acc: 0.3618, val loss: 2.0513, val acc: 0.2449  (best train acc: 0.4110, best val acc: 0.4863)\n",
      "[Epoch: 18760] train loss: 1.3552, train acc: 0.3749, val loss: 2.0685, val acc: 0.2462  (best train acc: 0.4110, best val acc: 0.4863)\n",
      "[Epoch: 18780] train loss: 1.3543, train acc: 0.3801, val loss: 2.0909, val acc: 0.2445  (best train acc: 0.4110, best val acc: 0.4863)\n",
      "[Epoch: 18800] train loss: 1.3794, train acc: 0.3757, val loss: 1.9912, val acc: 0.2435  (best train acc: 0.4110, best val acc: 0.4863)\n",
      "[Epoch: 18820] train loss: 1.3580, train acc: 0.3974, val loss: 2.0403, val acc: 0.2482  (best train acc: 0.4110, best val acc: 0.4863)\n",
      "[Epoch: 18840] train loss: 1.3673, train acc: 0.3850, val loss: 2.0419, val acc: 0.2432  (best train acc: 0.4110, best val acc: 0.4863)\n",
      "[Epoch: 18860] train loss: 1.3637, train acc: 0.3837, val loss: 1.9968, val acc: 0.2459  (best train acc: 0.4110, best val acc: 0.4863)\n",
      "[Epoch: 18880] train loss: 1.3599, train acc: 0.3928, val loss: 1.9781, val acc: 0.2449  (best train acc: 0.4110, best val acc: 0.4863)\n",
      "[Epoch: 18900] train loss: 1.3562, train acc: 0.3924, val loss: 1.9529, val acc: 0.2489  (best train acc: 0.4110, best val acc: 0.4863)\n",
      "[Epoch: 18920] train loss: 1.3325, train acc: 0.4004, val loss: 1.9310, val acc: 0.2489  (best train acc: 0.4110, best val acc: 0.4863)\n",
      "[Epoch: 18940] train loss: 1.3577, train acc: 0.3916, val loss: 1.8978, val acc: 0.2499  (best train acc: 0.4110, best val acc: 0.4863)\n",
      "[Epoch: 18960] train loss: 1.3276, train acc: 0.4023, val loss: 1.8500, val acc: 0.2499  (best train acc: 0.4123, best val acc: 0.4863)\n",
      "[Epoch: 18980] train loss: 1.3631, train acc: 0.3939, val loss: 1.8029, val acc: 0.2489  (best train acc: 0.4148, best val acc: 0.4863)\n",
      "[Epoch: 19000] train loss: 1.3324, train acc: 0.4087, val loss: 1.7653, val acc: 0.2516  (best train acc: 0.4148, best val acc: 0.4863)\n",
      "[Epoch: 19020] train loss: 1.3298, train acc: 0.4058, val loss: 1.7197, val acc: 0.2604  (best train acc: 0.4157, best val acc: 0.4863)\n",
      "[Epoch: 19040] train loss: 1.3201, train acc: 0.4057, val loss: 1.7049, val acc: 0.2678  (best train acc: 0.4157, best val acc: 0.4863)\n",
      "[Epoch: 19060] train loss: 1.3051, train acc: 0.4177, val loss: 1.6819, val acc: 0.2722  (best train acc: 0.4177, best val acc: 0.4863)\n",
      "[Epoch: 19080] train loss: 1.3052, train acc: 0.4209, val loss: 1.5694, val acc: 0.3106  (best train acc: 0.4209, best val acc: 0.4863)\n",
      "[Epoch: 19100] train loss: 2.5189, train acc: 0.3621, val loss: 6.0045, val acc: 0.2371  (best train acc: 0.4420, best val acc: 0.4863)\n",
      "[Epoch: 19120] train loss: 2.3594, train acc: 0.3565, val loss: 6.3286, val acc: 0.2418  (best train acc: 0.4420, best val acc: 0.4863)\n",
      "[Epoch: 19140] train loss: 1.6229, train acc: 0.3430, val loss: 2.7207, val acc: 0.2438  (best train acc: 0.4420, best val acc: 0.4863)\n",
      "[Epoch: 19160] train loss: 1.4034, train acc: 0.3524, val loss: 1.6113, val acc: 0.2637  (best train acc: 0.4420, best val acc: 0.4863)\n",
      "[Epoch: 19180] train loss: 1.3728, train acc: 0.3763, val loss: 1.6505, val acc: 0.2705  (best train acc: 0.4420, best val acc: 0.4863)\n",
      "[Epoch: 19200] train loss: 1.3715, train acc: 0.3798, val loss: 1.5669, val acc: 0.2951  (best train acc: 0.4420, best val acc: 0.4863)\n",
      "[Epoch: 19220] train loss: 1.3653, train acc: 0.3843, val loss: 1.5482, val acc: 0.3153  (best train acc: 0.4420, best val acc: 0.4863)\n",
      "[Epoch: 19240] train loss: 1.3558, train acc: 0.3874, val loss: 1.5107, val acc: 0.3292  (best train acc: 0.4420, best val acc: 0.4863)\n",
      "[Epoch: 19260] train loss: 1.3496, train acc: 0.3884, val loss: 1.4776, val acc: 0.3531  (best train acc: 0.4420, best val acc: 0.4863)\n",
      "[Epoch: 19280] train loss: 1.3470, train acc: 0.3935, val loss: 1.4741, val acc: 0.3595  (best train acc: 0.4420, best val acc: 0.4863)\n",
      "[Epoch: 19300] train loss: 1.3381, train acc: 0.4018, val loss: 1.4529, val acc: 0.3622  (best train acc: 0.4420, best val acc: 0.4863)\n",
      "[Epoch: 19320] train loss: 1.3419, train acc: 0.4059, val loss: 1.4321, val acc: 0.3750  (best train acc: 0.4420, best val acc: 0.4863)\n",
      "[Epoch: 19340] train loss: 1.3400, train acc: 0.4117, val loss: 1.4278, val acc: 0.3713  (best train acc: 0.4420, best val acc: 0.4863)\n",
      "[Epoch: 19360] train loss: 1.3364, train acc: 0.4078, val loss: 1.4141, val acc: 0.3693  (best train acc: 0.4420, best val acc: 0.4863)\n",
      "[Epoch: 19380] train loss: 1.3302, train acc: 0.4106, val loss: 1.4134, val acc: 0.3686  (best train acc: 0.4420, best val acc: 0.4863)\n",
      "[Epoch: 19400] train loss: 1.3247, train acc: 0.4204, val loss: 1.4102, val acc: 0.3686  (best train acc: 0.4420, best val acc: 0.4863)\n",
      "[Epoch: 19420] train loss: 1.3422, train acc: 0.4157, val loss: 1.4079, val acc: 0.3680  (best train acc: 0.4420, best val acc: 0.4863)\n",
      "[Epoch: 19440] train loss: 1.3244, train acc: 0.4176, val loss: 1.4099, val acc: 0.3683  (best train acc: 0.4420, best val acc: 0.4863)\n",
      "[Epoch: 19460] train loss: 1.3207, train acc: 0.4203, val loss: 1.4010, val acc: 0.3747  (best train acc: 0.4420, best val acc: 0.4863)\n",
      "[Epoch: 19480] train loss: 1.3214, train acc: 0.4165, val loss: 1.4169, val acc: 0.3673  (best train acc: 0.4420, best val acc: 0.4863)\n",
      "[Epoch: 19500] train loss: 1.3213, train acc: 0.4200, val loss: 1.3922, val acc: 0.3767  (best train acc: 0.4420, best val acc: 0.4863)\n",
      "[Epoch: 19520] train loss: 1.3186, train acc: 0.4192, val loss: 1.3868, val acc: 0.3710  (best train acc: 0.4420, best val acc: 0.4863)\n",
      "[Epoch: 19540] train loss: 1.3094, train acc: 0.4250, val loss: 1.3905, val acc: 0.3734  (best train acc: 0.4420, best val acc: 0.4863)\n",
      "[Epoch: 19560] train loss: 1.3292, train acc: 0.4198, val loss: 1.3733, val acc: 0.3815  (best train acc: 0.4420, best val acc: 0.4863)\n",
      "[Epoch: 19580] train loss: 1.3053, train acc: 0.4235, val loss: 1.3772, val acc: 0.3781  (best train acc: 0.4420, best val acc: 0.4863)\n",
      "[Epoch: 19600] train loss: 1.3103, train acc: 0.4195, val loss: 1.3772, val acc: 0.3788  (best train acc: 0.4420, best val acc: 0.4863)\n",
      "[Epoch: 19620] train loss: 1.3114, train acc: 0.4208, val loss: 1.3737, val acc: 0.3757  (best train acc: 0.4420, best val acc: 0.4863)\n",
      "[Epoch: 19640] train loss: 1.3079, train acc: 0.4234, val loss: 1.3729, val acc: 0.3801  (best train acc: 0.4420, best val acc: 0.4863)\n",
      "[Epoch: 19660] train loss: 1.3095, train acc: 0.4203, val loss: 1.3653, val acc: 0.3821  (best train acc: 0.4420, best val acc: 0.4863)\n",
      "[Epoch: 19680] train loss: 1.3235, train acc: 0.4213, val loss: 1.3648, val acc: 0.3811  (best train acc: 0.4420, best val acc: 0.4863)\n",
      "[Epoch: 19700] train loss: 1.3210, train acc: 0.4224, val loss: 1.3729, val acc: 0.3794  (best train acc: 0.4420, best val acc: 0.4863)\n",
      "[Epoch: 19720] train loss: 1.3087, train acc: 0.4204, val loss: 1.3599, val acc: 0.3821  (best train acc: 0.4420, best val acc: 0.4863)\n",
      "[Epoch: 19740] train loss: 1.3087, train acc: 0.4164, val loss: 1.3673, val acc: 0.3808  (best train acc: 0.4420, best val acc: 0.4863)\n",
      "[Epoch: 19760] train loss: 1.3041, train acc: 0.4271, val loss: 1.3687, val acc: 0.3821  (best train acc: 0.4420, best val acc: 0.4863)\n",
      "[Epoch: 19780] train loss: 1.3065, train acc: 0.4153, val loss: 1.3660, val acc: 0.3835  (best train acc: 0.4420, best val acc: 0.4863)\n",
      "[Epoch: 19800] train loss: 1.3159, train acc: 0.4231, val loss: 1.3679, val acc: 0.3754  (best train acc: 0.4420, best val acc: 0.4863)\n",
      "[Epoch: 19820] train loss: 1.3042, train acc: 0.4221, val loss: 1.3663, val acc: 0.3798  (best train acc: 0.4420, best val acc: 0.4863)\n",
      "[Epoch: 19840] train loss: 1.3037, train acc: 0.4254, val loss: 1.3708, val acc: 0.3815  (best train acc: 0.4420, best val acc: 0.4863)\n",
      "[Epoch: 19860] train loss: 1.3074, train acc: 0.4249, val loss: 1.3680, val acc: 0.3761  (best train acc: 0.4420, best val acc: 0.4863)\n",
      "[Epoch: 19880] train loss: 1.2981, train acc: 0.4228, val loss: 1.3634, val acc: 0.3788  (best train acc: 0.4420, best val acc: 0.4863)\n",
      "[Epoch: 19900] train loss: 1.2978, train acc: 0.4297, val loss: 1.3569, val acc: 0.3791  (best train acc: 0.4420, best val acc: 0.4863)\n",
      "[Epoch: 19920] train loss: 1.3002, train acc: 0.4216, val loss: 1.3571, val acc: 0.3791  (best train acc: 0.4420, best val acc: 0.4863)\n",
      "[Epoch: 19940] train loss: 1.2943, train acc: 0.4274, val loss: 1.3517, val acc: 0.3801  (best train acc: 0.4420, best val acc: 0.4863)\n",
      "[Epoch: 19960] train loss: 1.3001, train acc: 0.4245, val loss: 1.3610, val acc: 0.3815  (best train acc: 0.4420, best val acc: 0.4863)\n",
      "[Epoch: 19980] train loss: 1.2999, train acc: 0.4231, val loss: 1.3564, val acc: 0.3777  (best train acc: 0.4420, best val acc: 0.4863)\n",
      "[Epoch: 20000] train loss: 1.3039, train acc: 0.4307, val loss: 1.3576, val acc: 0.3801  (best train acc: 0.4420, best val acc: 0.4863)\n",
      "[Epoch: 20020] train loss: 1.3012, train acc: 0.4238, val loss: 1.3552, val acc: 0.3784  (best train acc: 0.4420, best val acc: 0.4863)\n",
      "[Epoch: 20040] train loss: 1.2870, train acc: 0.4320, val loss: 1.3489, val acc: 0.3825  (best train acc: 0.4420, best val acc: 0.4863)\n",
      "[Epoch: 20060] train loss: 1.2982, train acc: 0.4230, val loss: 1.3578, val acc: 0.3811  (best train acc: 0.4420, best val acc: 0.4863)\n",
      "[Epoch: 20080] train loss: 1.2928, train acc: 0.4275, val loss: 1.3501, val acc: 0.3788  (best train acc: 0.4420, best val acc: 0.4863)\n",
      "[Epoch: 20100] train loss: 1.2981, train acc: 0.4305, val loss: 1.3469, val acc: 0.3862  (best train acc: 0.4420, best val acc: 0.4863)\n",
      "[Epoch: 20120] train loss: 1.2930, train acc: 0.4271, val loss: 1.3516, val acc: 0.3821  (best train acc: 0.4420, best val acc: 0.4863)\n",
      "[Epoch: 20140] train loss: 1.2943, train acc: 0.4320, val loss: 1.3490, val acc: 0.3784  (best train acc: 0.4420, best val acc: 0.4863)\n",
      "[Epoch: 20160] train loss: 1.2986, train acc: 0.4296, val loss: 1.3574, val acc: 0.3811  (best train acc: 0.4420, best val acc: 0.4863)\n",
      "[Epoch: 20180] train loss: 1.2996, train acc: 0.4307, val loss: 1.3606, val acc: 0.3801  (best train acc: 0.4420, best val acc: 0.4863)\n",
      "[Epoch: 20200] train loss: 1.3017, train acc: 0.4221, val loss: 1.3600, val acc: 0.3815  (best train acc: 0.4420, best val acc: 0.4863)\n",
      "[Epoch: 20220] train loss: 1.2925, train acc: 0.4317, val loss: 1.3520, val acc: 0.3791  (best train acc: 0.4420, best val acc: 0.4863)\n",
      "[Epoch: 20240] train loss: 1.2934, train acc: 0.4247, val loss: 1.3470, val acc: 0.3808  (best train acc: 0.4420, best val acc: 0.4863)\n",
      "[Epoch: 20260] train loss: 1.2970, train acc: 0.4257, val loss: 1.3481, val acc: 0.3777  (best train acc: 0.4420, best val acc: 0.4863)\n",
      "[Epoch: 20280] train loss: 1.2976, train acc: 0.4204, val loss: 1.3539, val acc: 0.3801  (best train acc: 0.4420, best val acc: 0.4863)\n",
      "[Epoch: 20300] train loss: 1.3009, train acc: 0.4279, val loss: 1.3529, val acc: 0.3804  (best train acc: 0.4420, best val acc: 0.4863)\n",
      "[Epoch: 20320] train loss: 1.2912, train acc: 0.4278, val loss: 1.3540, val acc: 0.3808  (best train acc: 0.4420, best val acc: 0.4863)\n",
      "[Epoch: 20340] train loss: 1.2887, train acc: 0.4289, val loss: 1.3554, val acc: 0.3815  (best train acc: 0.4420, best val acc: 0.4863)\n",
      "[Epoch: 20360] train loss: 1.2932, train acc: 0.4231, val loss: 1.3490, val acc: 0.3788  (best train acc: 0.4420, best val acc: 0.4863)\n",
      "[Epoch: 20380] train loss: 1.2882, train acc: 0.4347, val loss: 1.3623, val acc: 0.3828  (best train acc: 0.4420, best val acc: 0.4863)\n",
      "[Epoch: 20400] train loss: 1.2882, train acc: 0.4260, val loss: 1.3481, val acc: 0.3798  (best train acc: 0.4420, best val acc: 0.4863)\n",
      "[Epoch: 20420] train loss: 1.2919, train acc: 0.4231, val loss: 1.3517, val acc: 0.3788  (best train acc: 0.4420, best val acc: 0.4863)\n",
      "[Epoch: 20440] train loss: 1.2880, train acc: 0.4340, val loss: 1.3498, val acc: 0.3798  (best train acc: 0.4420, best val acc: 0.4863)\n",
      "[Epoch: 20460] train loss: 1.2997, train acc: 0.4276, val loss: 1.3533, val acc: 0.3818  (best train acc: 0.4420, best val acc: 0.4863)\n",
      "[Epoch: 20480] train loss: 1.2887, train acc: 0.4250, val loss: 1.3503, val acc: 0.3784  (best train acc: 0.4420, best val acc: 0.4863)\n",
      "[Epoch: 20500] train loss: 1.2890, train acc: 0.4299, val loss: 1.3449, val acc: 0.3818  (best train acc: 0.4420, best val acc: 0.4863)\n",
      "[Epoch: 20520] train loss: 1.2890, train acc: 0.4285, val loss: 1.3456, val acc: 0.3804  (best train acc: 0.4420, best val acc: 0.4863)\n",
      "[Epoch: 20540] train loss: 1.2881, train acc: 0.4269, val loss: 1.3433, val acc: 0.3791  (best train acc: 0.4420, best val acc: 0.4863)\n",
      "[Epoch: 20560] train loss: 1.2886, train acc: 0.4248, val loss: 1.3515, val acc: 0.3821  (best train acc: 0.4420, best val acc: 0.4863)\n",
      "[Epoch: 20580] train loss: 1.2866, train acc: 0.4267, val loss: 1.3450, val acc: 0.3788  (best train acc: 0.4420, best val acc: 0.4863)\n",
      "[Epoch: 20600] train loss: 1.2826, train acc: 0.4315, val loss: 1.3413, val acc: 0.3811  (best train acc: 0.4420, best val acc: 0.4863)\n",
      "[Epoch: 20620] train loss: 1.2908, train acc: 0.4327, val loss: 1.3468, val acc: 0.3794  (best train acc: 0.4420, best val acc: 0.4863)\n",
      "[Epoch: 20640] train loss: 1.2905, train acc: 0.4280, val loss: 1.3509, val acc: 0.3777  (best train acc: 0.4420, best val acc: 0.4863)\n",
      "[Epoch: 20660] train loss: 1.2859, train acc: 0.4376, val loss: 1.3454, val acc: 0.3771  (best train acc: 0.4420, best val acc: 0.4863)\n",
      "[Epoch: 20680] train loss: 1.2875, train acc: 0.4294, val loss: 1.3472, val acc: 0.3784  (best train acc: 0.4420, best val acc: 0.4863)\n",
      "[Epoch: 20700] train loss: 1.2878, train acc: 0.4293, val loss: 1.3389, val acc: 0.3801  (best train acc: 0.4420, best val acc: 0.4863)\n",
      "[Epoch: 20720] train loss: 1.2909, train acc: 0.4310, val loss: 1.3410, val acc: 0.3794  (best train acc: 0.4420, best val acc: 0.4863)\n",
      "[Epoch: 20740] train loss: 1.2937, train acc: 0.4291, val loss: 1.3476, val acc: 0.3771  (best train acc: 0.4420, best val acc: 0.4863)\n",
      "[Epoch: 20760] train loss: 1.2800, train acc: 0.4345, val loss: 1.3408, val acc: 0.3791  (best train acc: 0.4420, best val acc: 0.4863)\n",
      "[Epoch: 20780] train loss: 1.2943, train acc: 0.4279, val loss: 1.3382, val acc: 0.3828  (best train acc: 0.4420, best val acc: 0.4863)\n",
      "[Epoch: 20800] train loss: 1.2844, train acc: 0.4345, val loss: 1.3414, val acc: 0.3825  (best train acc: 0.4420, best val acc: 0.4863)\n",
      "[Epoch: 20820] train loss: 1.2969, train acc: 0.4281, val loss: 1.3380, val acc: 0.3831  (best train acc: 0.4420, best val acc: 0.4863)\n",
      "[Epoch: 20840] train loss: 1.2809, train acc: 0.4328, val loss: 1.3458, val acc: 0.3777  (best train acc: 0.4420, best val acc: 0.4863)\n",
      "[Epoch: 20860] train loss: 1.2981, train acc: 0.4221, val loss: 1.3551, val acc: 0.3818  (best train acc: 0.4420, best val acc: 0.4863)\n",
      "[Epoch: 20880] train loss: 1.2888, train acc: 0.4298, val loss: 1.3630, val acc: 0.3781  (best train acc: 0.4420, best val acc: 0.4863)\n",
      "[Epoch: 20900] train loss: 1.2875, train acc: 0.4307, val loss: 1.3537, val acc: 0.3818  (best train acc: 0.4420, best val acc: 0.4863)\n",
      "[Epoch: 20920] train loss: 1.2899, train acc: 0.4281, val loss: 1.3384, val acc: 0.3815  (best train acc: 0.4420, best val acc: 0.4863)\n",
      "[Epoch: 20940] train loss: 1.2879, train acc: 0.4290, val loss: 1.3391, val acc: 0.3838  (best train acc: 0.4420, best val acc: 0.4863)\n",
      "[Epoch: 20960] train loss: 1.2878, train acc: 0.4304, val loss: 1.3403, val acc: 0.3804  (best train acc: 0.4420, best val acc: 0.4863)\n",
      "[Epoch: 20980] train loss: 1.2866, train acc: 0.4308, val loss: 1.3347, val acc: 0.3818  (best train acc: 0.4420, best val acc: 0.4863)\n",
      "[Epoch: 21000] train loss: 1.2963, train acc: 0.4247, val loss: 1.3379, val acc: 0.3788  (best train acc: 0.4420, best val acc: 0.4863)\n",
      "[Epoch: 21020] train loss: 1.2877, train acc: 0.4278, val loss: 1.3295, val acc: 0.3821  (best train acc: 0.4420, best val acc: 0.4863)\n",
      "[Epoch: 21040] train loss: 1.2855, train acc: 0.4289, val loss: 1.3452, val acc: 0.3784  (best train acc: 0.4420, best val acc: 0.4863)\n",
      "[Epoch: 21060] train loss: 1.2846, train acc: 0.4310, val loss: 1.3358, val acc: 0.3825  (best train acc: 0.4420, best val acc: 0.4863)\n",
      "[Epoch: 21080] train loss: 1.2809, train acc: 0.4341, val loss: 1.3446, val acc: 0.3815  (best train acc: 0.4420, best val acc: 0.4863)\n",
      "[Epoch: 21100] train loss: 1.2851, train acc: 0.4295, val loss: 1.3362, val acc: 0.3808  (best train acc: 0.4420, best val acc: 0.4863)\n",
      "[Epoch: 21120] train loss: 1.2907, train acc: 0.4273, val loss: 1.3350, val acc: 0.3818  (best train acc: 0.4420, best val acc: 0.4863)\n",
      "[Epoch: 21140] train loss: 1.2856, train acc: 0.4326, val loss: 1.3348, val acc: 0.3862  (best train acc: 0.4420, best val acc: 0.4863)\n",
      "[Epoch: 21160] train loss: 1.2905, train acc: 0.4323, val loss: 1.3513, val acc: 0.3808  (best train acc: 0.4420, best val acc: 0.4863)\n",
      "[Epoch: 21180] train loss: 1.2797, train acc: 0.4313, val loss: 1.3435, val acc: 0.3862  (best train acc: 0.4430, best val acc: 0.4863)\n",
      "[Epoch: 21200] train loss: 1.2842, train acc: 0.4369, val loss: 1.3540, val acc: 0.3825  (best train acc: 0.4430, best val acc: 0.4863)\n",
      "[Epoch: 21220] train loss: 1.2875, train acc: 0.4400, val loss: 1.3623, val acc: 0.3852  (best train acc: 0.4430, best val acc: 0.4863)\n",
      "[Epoch: 21240] train loss: 1.2845, train acc: 0.4357, val loss: 1.3523, val acc: 0.3794  (best train acc: 0.4430, best val acc: 0.4863)\n",
      "[Epoch: 21260] train loss: 1.2785, train acc: 0.4334, val loss: 1.3328, val acc: 0.3831  (best train acc: 0.4430, best val acc: 0.4863)\n",
      "[Epoch: 21280] train loss: 1.3551, train acc: 0.4018, val loss: 1.5077, val acc: 0.3629  (best train acc: 0.4430, best val acc: 0.4863)\n",
      "[Epoch: 21300] train loss: 1.2916, train acc: 0.4299, val loss: 1.3426, val acc: 0.3808  (best train acc: 0.4430, best val acc: 0.4863)\n",
      "[Epoch: 21320] train loss: 1.2928, train acc: 0.4218, val loss: 1.3529, val acc: 0.3781  (best train acc: 0.4430, best val acc: 0.4863)\n",
      "[Epoch: 21340] train loss: 1.2818, train acc: 0.4247, val loss: 1.3330, val acc: 0.3852  (best train acc: 0.4430, best val acc: 0.4863)\n",
      "[Epoch: 21360] train loss: 1.2888, train acc: 0.4315, val loss: 1.3653, val acc: 0.3801  (best train acc: 0.4430, best val acc: 0.4863)\n",
      "[Epoch: 21380] train loss: 1.2867, train acc: 0.4336, val loss: 1.3554, val acc: 0.3788  (best train acc: 0.4430, best val acc: 0.4863)\n",
      "[Epoch: 21400] train loss: 1.2807, train acc: 0.4394, val loss: 1.3423, val acc: 0.3841  (best train acc: 0.4430, best val acc: 0.4863)\n",
      "[Epoch: 21420] train loss: 1.9279, train acc: 0.3552, val loss: 2.7305, val acc: 0.3356  (best train acc: 0.4445, best val acc: 0.4863)\n",
      "[Epoch: 21440] train loss: 1.4072, train acc: 0.4213, val loss: 1.7739, val acc: 0.3521  (best train acc: 0.4445, best val acc: 0.4863)\n",
      "[Epoch: 21460] train loss: 1.3163, train acc: 0.4258, val loss: 1.4768, val acc: 0.3582  (best train acc: 0.4445, best val acc: 0.4863)\n",
      "[Epoch: 21480] train loss: 1.2936, train acc: 0.4218, val loss: 1.3590, val acc: 0.3781  (best train acc: 0.4445, best val acc: 0.4863)\n",
      "[Epoch: 21500] train loss: 1.2852, train acc: 0.4234, val loss: 1.3378, val acc: 0.3818  (best train acc: 0.4445, best val acc: 0.4863)\n",
      "[Epoch: 21520] train loss: 1.2838, train acc: 0.4326, val loss: 1.3425, val acc: 0.3835  (best train acc: 0.4445, best val acc: 0.4863)\n",
      "[Epoch: 21540] train loss: 1.3210, train acc: 0.4346, val loss: 1.4180, val acc: 0.3700  (best train acc: 0.4445, best val acc: 0.4863)\n",
      "[Epoch: 21560] train loss: 1.3061, train acc: 0.4227, val loss: 1.3661, val acc: 0.3808  (best train acc: 0.4445, best val acc: 0.4863)\n",
      "[Epoch: 21580] train loss: 1.2848, train acc: 0.4305, val loss: 1.3510, val acc: 0.3801  (best train acc: 0.4445, best val acc: 0.4863)\n",
      "[Epoch: 21600] train loss: 1.2842, train acc: 0.4169, val loss: 1.3599, val acc: 0.3784  (best train acc: 0.4445, best val acc: 0.4863)\n",
      "[Epoch: 21620] train loss: 1.3147, train acc: 0.4310, val loss: 1.3657, val acc: 0.3852  (best train acc: 0.4445, best val acc: 0.4863)\n",
      "[Epoch: 21640] train loss: 1.2883, train acc: 0.4350, val loss: 1.3454, val acc: 0.3848  (best train acc: 0.4445, best val acc: 0.4863)\n",
      "[Epoch: 21660] train loss: 1.2737, train acc: 0.4302, val loss: 1.3502, val acc: 0.3821  (best train acc: 0.4445, best val acc: 0.4863)\n",
      "[Epoch: 21680] train loss: 1.3183, train acc: 0.3953, val loss: 1.3724, val acc: 0.3868  (best train acc: 0.4445, best val acc: 0.4863)\n",
      "[Epoch: 21700] train loss: 1.2987, train acc: 0.4277, val loss: 1.3783, val acc: 0.3818  (best train acc: 0.4445, best val acc: 0.4863)\n",
      "[Epoch: 21720] train loss: 1.2851, train acc: 0.4268, val loss: 1.3502, val acc: 0.3794  (best train acc: 0.4445, best val acc: 0.4863)\n",
      "[Epoch: 21740] train loss: 1.2828, train acc: 0.4316, val loss: 1.3570, val acc: 0.3828  (best train acc: 0.4445, best val acc: 0.4863)\n",
      "[Epoch: 21760] train loss: 1.2851, train acc: 0.4255, val loss: 1.3385, val acc: 0.3831  (best train acc: 0.4445, best val acc: 0.4863)\n",
      "[Epoch: 21780] train loss: 1.2912, train acc: 0.4233, val loss: 1.3486, val acc: 0.3781  (best train acc: 0.4445, best val acc: 0.4863)\n",
      "[Epoch: 21800] train loss: 1.2951, train acc: 0.4305, val loss: 1.3435, val acc: 0.3828  (best train acc: 0.4445, best val acc: 0.4863)\n",
      "[Epoch: 21820] train loss: 1.2941, train acc: 0.4263, val loss: 1.3487, val acc: 0.3825  (best train acc: 0.4445, best val acc: 0.4863)\n",
      "[Epoch: 21840] train loss: 1.2855, train acc: 0.4234, val loss: 1.3707, val acc: 0.3734  (best train acc: 0.4445, best val acc: 0.4863)\n",
      "[Epoch: 21860] train loss: 1.2866, train acc: 0.4312, val loss: 1.3315, val acc: 0.3831  (best train acc: 0.4445, best val acc: 0.4863)\n",
      "[Epoch: 21880] train loss: 1.2869, train acc: 0.4355, val loss: 1.3445, val acc: 0.3831  (best train acc: 0.4445, best val acc: 0.4863)\n",
      "[Epoch: 21900] train loss: 1.2766, train acc: 0.4372, val loss: 1.3432, val acc: 0.3831  (best train acc: 0.4445, best val acc: 0.4863)\n",
      "[Epoch: 21920] train loss: 1.3244, train acc: 0.4182, val loss: 1.3866, val acc: 0.3815  (best train acc: 0.4445, best val acc: 0.4863)\n",
      "[Epoch: 21940] train loss: 1.2892, train acc: 0.4250, val loss: 1.3819, val acc: 0.3757  (best train acc: 0.4445, best val acc: 0.4863)\n",
      "[Epoch: 21960] train loss: 1.2775, train acc: 0.4379, val loss: 1.3549, val acc: 0.3852  (best train acc: 0.4445, best val acc: 0.4863)\n",
      "[Epoch: 21980] train loss: 1.3231, train acc: 0.4031, val loss: 1.3478, val acc: 0.3815  (best train acc: 0.4445, best val acc: 0.4863)\n",
      "[Epoch: 22000] train loss: 1.2810, train acc: 0.4268, val loss: 1.3633, val acc: 0.3747  (best train acc: 0.4445, best val acc: 0.4863)\n",
      "[Epoch: 22020] train loss: 1.2871, train acc: 0.4297, val loss: 1.3415, val acc: 0.3838  (best train acc: 0.4445, best val acc: 0.4863)\n",
      "[Epoch: 22040] train loss: 1.4492, train acc: 0.4140, val loss: 1.9319, val acc: 0.3899  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 22060] train loss: 1.3787, train acc: 0.4139, val loss: 1.5164, val acc: 0.4091  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 22080] train loss: 1.3239, train acc: 0.4096, val loss: 1.4721, val acc: 0.3821  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 22100] train loss: 1.2974, train acc: 0.4321, val loss: 1.4240, val acc: 0.3693  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 22120] train loss: 1.2881, train acc: 0.4328, val loss: 1.3610, val acc: 0.3798  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 22140] train loss: 1.2855, train acc: 0.4258, val loss: 1.3371, val acc: 0.3815  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 22160] train loss: 1.2817, train acc: 0.4307, val loss: 1.3394, val acc: 0.3855  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 22180] train loss: 1.2770, train acc: 0.4312, val loss: 1.3332, val acc: 0.3835  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 22200] train loss: 1.3046, train acc: 0.4237, val loss: 1.3411, val acc: 0.3815  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 22220] train loss: 1.2807, train acc: 0.4357, val loss: 1.3553, val acc: 0.3744  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 22240] train loss: 1.2793, train acc: 0.4351, val loss: 1.3465, val acc: 0.3774  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 22260] train loss: 1.2743, train acc: 0.4336, val loss: 1.3260, val acc: 0.3855  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 22280] train loss: 1.2931, train acc: 0.4312, val loss: 1.3733, val acc: 0.3750  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 22300] train loss: 1.2873, train acc: 0.4221, val loss: 1.3606, val acc: 0.3794  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 22320] train loss: 1.2850, train acc: 0.4275, val loss: 1.3363, val acc: 0.3767  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 22340] train loss: 1.2770, train acc: 0.4346, val loss: 1.3455, val acc: 0.3862  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 22360] train loss: 1.2898, train acc: 0.4301, val loss: 1.3195, val acc: 0.3818  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 22380] train loss: 1.2767, train acc: 0.4364, val loss: 1.3299, val acc: 0.3879  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 22400] train loss: 1.3545, train acc: 0.4155, val loss: 1.5223, val acc: 0.3508  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 22420] train loss: 1.2894, train acc: 0.4146, val loss: 1.3711, val acc: 0.3727  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 22440] train loss: 1.2804, train acc: 0.4218, val loss: 1.3337, val acc: 0.3815  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 22460] train loss: 1.2731, train acc: 0.4320, val loss: 1.3262, val acc: 0.3808  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 22480] train loss: 1.3038, train acc: 0.4119, val loss: 1.3993, val acc: 0.3794  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 22500] train loss: 1.2952, train acc: 0.4226, val loss: 1.3612, val acc: 0.3737  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 22520] train loss: 1.2747, train acc: 0.4318, val loss: 1.3236, val acc: 0.3835  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 22540] train loss: 1.2723, train acc: 0.4347, val loss: 1.3382, val acc: 0.3848  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 22560] train loss: 1.2819, train acc: 0.4259, val loss: 1.3322, val acc: 0.3815  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 22580] train loss: 1.2849, train acc: 0.4282, val loss: 1.3632, val acc: 0.3713  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 22600] train loss: 1.2776, train acc: 0.4294, val loss: 1.3408, val acc: 0.3757  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 22620] train loss: 1.2768, train acc: 0.4354, val loss: 1.3209, val acc: 0.3811  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 22640] train loss: 1.3169, train acc: 0.4110, val loss: 1.3792, val acc: 0.3774  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 22660] train loss: 1.2784, train acc: 0.4338, val loss: 1.3304, val acc: 0.3781  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 22680] train loss: 1.2732, train acc: 0.4369, val loss: 1.3225, val acc: 0.3865  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 22700] train loss: 1.8295, train acc: 0.3585, val loss: 2.7871, val acc: 0.3919  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 22720] train loss: 1.5303, train acc: 0.3701, val loss: 1.5327, val acc: 0.4044  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 22740] train loss: 1.3164, train acc: 0.4248, val loss: 1.4351, val acc: 0.3690  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 22760] train loss: 1.2978, train acc: 0.4156, val loss: 1.3684, val acc: 0.3666  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 22780] train loss: 1.2871, train acc: 0.4156, val loss: 1.3480, val acc: 0.3740  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 22800] train loss: 1.2743, train acc: 0.4269, val loss: 1.3267, val acc: 0.3818  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 22820] train loss: 1.2803, train acc: 0.4245, val loss: 1.3286, val acc: 0.3828  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 22840] train loss: 1.2743, train acc: 0.4371, val loss: 1.3239, val acc: 0.3784  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 22860] train loss: 1.2760, train acc: 0.4338, val loss: 1.3174, val acc: 0.3865  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 22880] train loss: 1.2709, train acc: 0.4373, val loss: 1.3277, val acc: 0.3825  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 22900] train loss: 1.3523, train acc: 0.4281, val loss: 1.4374, val acc: 0.3693  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 22920] train loss: 1.2983, train acc: 0.4054, val loss: 1.3597, val acc: 0.3828  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 22940] train loss: 1.2823, train acc: 0.4223, val loss: 1.3341, val acc: 0.3852  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 22960] train loss: 1.2739, train acc: 0.4291, val loss: 1.3329, val acc: 0.3798  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 22980] train loss: 1.2920, train acc: 0.4237, val loss: 1.3254, val acc: 0.3791  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 23000] train loss: 1.2794, train acc: 0.4339, val loss: 1.3580, val acc: 0.3781  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 23020] train loss: 1.2805, train acc: 0.4294, val loss: 1.3504, val acc: 0.3740  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 23040] train loss: 1.2773, train acc: 0.4341, val loss: 1.3295, val acc: 0.3771  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 23060] train loss: 1.2741, train acc: 0.4315, val loss: 1.3213, val acc: 0.3852  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 23080] train loss: 1.2878, train acc: 0.4283, val loss: 1.3268, val acc: 0.3791  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 23100] train loss: 1.4284, train acc: 0.4049, val loss: 1.4390, val acc: 0.3329  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 23120] train loss: 1.2987, train acc: 0.4117, val loss: 1.4248, val acc: 0.3734  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 23140] train loss: 1.2934, train acc: 0.4169, val loss: 1.3207, val acc: 0.3865  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 23160] train loss: 1.2810, train acc: 0.4192, val loss: 1.3321, val acc: 0.3784  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 23180] train loss: 1.2769, train acc: 0.4347, val loss: 1.3404, val acc: 0.3754  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 23200] train loss: 1.2831, train acc: 0.4295, val loss: 1.3189, val acc: 0.3831  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 23220] train loss: 1.2798, train acc: 0.4379, val loss: 1.3216, val acc: 0.3872  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 23240] train loss: 1.5026, train acc: 0.3928, val loss: 1.9423, val acc: 0.3993  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 23260] train loss: 1.3093, train acc: 0.3999, val loss: 1.4573, val acc: 0.3669  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 23280] train loss: 1.2882, train acc: 0.4242, val loss: 1.3324, val acc: 0.3835  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 23300] train loss: 1.2727, train acc: 0.4343, val loss: 1.3276, val acc: 0.3852  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 23320] train loss: 1.2724, train acc: 0.4250, val loss: 1.3192, val acc: 0.3771  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 23340] train loss: 1.2792, train acc: 0.4300, val loss: 1.3253, val acc: 0.3767  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 23360] train loss: 1.2660, train acc: 0.4529, val loss: 1.3387, val acc: 0.3862  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 23380] train loss: 1.4583, train acc: 0.3917, val loss: 2.2746, val acc: 0.3993  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 23400] train loss: 1.7799, train acc: 0.3705, val loss: 2.7634, val acc: 0.3730  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 23420] train loss: 1.3429, train acc: 0.3976, val loss: 1.4869, val acc: 0.3484  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 23440] train loss: 1.3017, train acc: 0.3916, val loss: 1.3512, val acc: 0.3754  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 23460] train loss: 1.2957, train acc: 0.4036, val loss: 1.3374, val acc: 0.3828  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 23480] train loss: 1.2852, train acc: 0.4274, val loss: 1.3242, val acc: 0.3831  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 23500] train loss: 1.2759, train acc: 0.4293, val loss: 1.3161, val acc: 0.3862  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 23520] train loss: 1.2828, train acc: 0.4235, val loss: 1.3129, val acc: 0.3855  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 23540] train loss: 1.2805, train acc: 0.4323, val loss: 1.3135, val acc: 0.3838  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 23560] train loss: 1.2768, train acc: 0.4369, val loss: 1.3061, val acc: 0.3852  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 23580] train loss: 1.2857, train acc: 0.4242, val loss: 1.3126, val acc: 0.3774  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 23600] train loss: 1.2782, train acc: 0.4346, val loss: 1.3075, val acc: 0.3828  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 23620] train loss: 1.2804, train acc: 0.4290, val loss: 1.3113, val acc: 0.3862  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 23640] train loss: 1.2763, train acc: 0.4330, val loss: 1.3116, val acc: 0.3845  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 23660] train loss: 1.2753, train acc: 0.4336, val loss: 1.3141, val acc: 0.3811  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 23680] train loss: 1.2748, train acc: 0.4397, val loss: 1.3155, val acc: 0.3821  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 23700] train loss: 1.2732, train acc: 0.4337, val loss: 1.3160, val acc: 0.3791  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 23720] train loss: 1.2751, train acc: 0.4337, val loss: 1.3152, val acc: 0.3821  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 23740] train loss: 1.2780, train acc: 0.4226, val loss: 1.3130, val acc: 0.3811  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 23760] train loss: 1.2698, train acc: 0.4413, val loss: 1.3172, val acc: 0.3841  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 23780] train loss: 1.2802, train acc: 0.4299, val loss: 1.3375, val acc: 0.3835  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 23800] train loss: 1.5915, train acc: 0.3943, val loss: 1.9535, val acc: 0.3747  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 23820] train loss: 1.3016, train acc: 0.4055, val loss: 1.3256, val acc: 0.3815  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 23840] train loss: 1.2832, train acc: 0.4310, val loss: 1.3504, val acc: 0.3761  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 23860] train loss: 1.2766, train acc: 0.4150, val loss: 1.3235, val acc: 0.3845  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 23880] train loss: 1.2743, train acc: 0.4322, val loss: 1.3172, val acc: 0.3838  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 23900] train loss: 1.2683, train acc: 0.4409, val loss: 1.3200, val acc: 0.3848  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 23920] train loss: 1.6324, train acc: 0.4132, val loss: 2.0358, val acc: 0.3379  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 23940] train loss: 1.3230, train acc: 0.4169, val loss: 1.3432, val acc: 0.3740  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 23960] train loss: 1.2854, train acc: 0.4258, val loss: 1.3545, val acc: 0.3761  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 23980] train loss: 1.2747, train acc: 0.4276, val loss: 1.3341, val acc: 0.3815  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 24000] train loss: 1.2763, train acc: 0.4284, val loss: 1.3253, val acc: 0.3811  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 24020] train loss: 1.2757, train acc: 0.4352, val loss: 1.3275, val acc: 0.3777  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 24040] train loss: 1.2769, train acc: 0.4336, val loss: 1.3237, val acc: 0.3767  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 24060] train loss: 1.2752, train acc: 0.4346, val loss: 1.3255, val acc: 0.3771  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 24080] train loss: 1.2697, train acc: 0.4338, val loss: 1.3171, val acc: 0.3872  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 24100] train loss: 1.4758, train acc: 0.4127, val loss: 1.7102, val acc: 0.3616  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 24120] train loss: 1.2967, train acc: 0.4248, val loss: 1.3781, val acc: 0.3791  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 24140] train loss: 1.2759, train acc: 0.4323, val loss: 1.3283, val acc: 0.3868  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 24160] train loss: 1.2732, train acc: 0.4359, val loss: 1.3308, val acc: 0.3788  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 24180] train loss: 1.2770, train acc: 0.4318, val loss: 1.3316, val acc: 0.3794  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 24200] train loss: 1.2724, train acc: 0.4342, val loss: 1.3389, val acc: 0.3784  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 24220] train loss: 1.2685, train acc: 0.4407, val loss: 1.3388, val acc: 0.3798  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 24240] train loss: 1.3274, train acc: 0.3949, val loss: 1.4195, val acc: 0.4010  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 24260] train loss: 1.2931, train acc: 0.4213, val loss: 1.4231, val acc: 0.3413  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 24280] train loss: 1.2840, train acc: 0.4144, val loss: 1.3408, val acc: 0.3740  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 24300] train loss: 1.2754, train acc: 0.4244, val loss: 1.3156, val acc: 0.3767  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 24320] train loss: 1.2674, train acc: 0.4365, val loss: 1.3363, val acc: 0.3781  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 24340] train loss: 1.2698, train acc: 0.4360, val loss: 1.3306, val acc: 0.3831  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 24360] train loss: 1.2868, train acc: 0.4323, val loss: 1.4030, val acc: 0.3521  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 24380] train loss: 1.5164, train acc: 0.3950, val loss: 1.5663, val acc: 0.3393  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 24400] train loss: 1.3339, train acc: 0.4099, val loss: 1.4507, val acc: 0.3622  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 24420] train loss: 1.2863, train acc: 0.4159, val loss: 1.3437, val acc: 0.3767  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 24440] train loss: 1.2857, train acc: 0.4065, val loss: 1.3323, val acc: 0.3788  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 24460] train loss: 1.2785, train acc: 0.4183, val loss: 1.3253, val acc: 0.3750  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 24480] train loss: 1.2714, train acc: 0.4311, val loss: 1.3225, val acc: 0.3804  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 24500] train loss: 1.2734, train acc: 0.4294, val loss: 1.3140, val acc: 0.3831  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 24520] train loss: 1.2670, train acc: 0.4367, val loss: 1.3189, val acc: 0.3761  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 24540] train loss: 1.2594, train acc: 0.4391, val loss: 1.3191, val acc: 0.3872  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 24560] train loss: 1.4290, train acc: 0.3986, val loss: 1.4134, val acc: 0.3970  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 24580] train loss: 1.2939, train acc: 0.4138, val loss: 1.3380, val acc: 0.3619  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 24600] train loss: 1.2751, train acc: 0.4184, val loss: 1.3442, val acc: 0.3750  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 24620] train loss: 1.2710, train acc: 0.4336, val loss: 1.3224, val acc: 0.3771  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 24640] train loss: 1.2757, train acc: 0.4338, val loss: 1.3154, val acc: 0.3777  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 24660] train loss: 1.2752, train acc: 0.4331, val loss: 1.3264, val acc: 0.3757  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 24680] train loss: 1.2625, train acc: 0.4266, val loss: 1.3017, val acc: 0.3858  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 24700] train loss: 1.3132, train acc: 0.4176, val loss: 1.3691, val acc: 0.3673  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 24720] train loss: 1.3056, train acc: 0.4117, val loss: 1.4515, val acc: 0.3636  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 24740] train loss: 2.5346, train acc: 0.3577, val loss: 5.9122, val acc: 0.3015  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 24760] train loss: 1.7406, train acc: 0.3652, val loss: 2.2117, val acc: 0.3801  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 24780] train loss: 1.3403, train acc: 0.3931, val loss: 1.4152, val acc: 0.3946  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 24800] train loss: 1.3247, train acc: 0.3916, val loss: 1.3362, val acc: 0.3808  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 24820] train loss: 1.2928, train acc: 0.4032, val loss: 1.3368, val acc: 0.3838  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 24840] train loss: 1.2799, train acc: 0.4104, val loss: 1.3118, val acc: 0.3852  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 24860] train loss: 1.2792, train acc: 0.4332, val loss: 1.3052, val acc: 0.3788  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 24880] train loss: 1.2742, train acc: 0.4289, val loss: 1.3103, val acc: 0.3818  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 24900] train loss: 1.2753, train acc: 0.4323, val loss: 1.3012, val acc: 0.3798  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 24920] train loss: 1.2757, train acc: 0.4334, val loss: 1.3059, val acc: 0.3771  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 24940] train loss: 1.2770, train acc: 0.4398, val loss: 1.3104, val acc: 0.3804  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 24960] train loss: 1.2740, train acc: 0.4340, val loss: 1.3051, val acc: 0.3794  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 24980] train loss: 1.2730, train acc: 0.4294, val loss: 1.3069, val acc: 0.3808  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 25000] train loss: 1.2731, train acc: 0.4344, val loss: 1.3001, val acc: 0.3815  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 25020] train loss: 1.2775, train acc: 0.4287, val loss: 1.3083, val acc: 0.3821  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 25040] train loss: 1.2704, train acc: 0.4323, val loss: 1.3043, val acc: 0.3828  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 25060] train loss: 1.2677, train acc: 0.4342, val loss: 1.3077, val acc: 0.3835  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 25080] train loss: 1.2631, train acc: 0.4408, val loss: 1.3091, val acc: 0.3818  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 25100] train loss: 1.8358, train acc: 0.3625, val loss: 2.5842, val acc: 0.3892  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 25120] train loss: 1.3291, train acc: 0.4178, val loss: 1.4182, val acc: 0.3592  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 25140] train loss: 1.2887, train acc: 0.4195, val loss: 1.3572, val acc: 0.3740  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 25160] train loss: 1.2747, train acc: 0.4198, val loss: 1.3057, val acc: 0.3828  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 25180] train loss: 1.2717, train acc: 0.4249, val loss: 1.3015, val acc: 0.3798  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 25200] train loss: 1.2694, train acc: 0.4279, val loss: 1.2997, val acc: 0.3771  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 25220] train loss: 1.2627, train acc: 0.4352, val loss: 1.3000, val acc: 0.3767  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 25240] train loss: 1.2685, train acc: 0.4379, val loss: 1.2988, val acc: 0.3774  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 25260] train loss: 1.2594, train acc: 0.4401, val loss: 1.2971, val acc: 0.3801  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 25280] train loss: 1.2619, train acc: 0.4372, val loss: 1.2912, val acc: 0.3791  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 25300] train loss: 1.2657, train acc: 0.4378, val loss: 1.2925, val acc: 0.3794  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 25320] train loss: 1.2582, train acc: 0.4451, val loss: 1.2919, val acc: 0.3828  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 25340] train loss: 1.2466, train acc: 0.4450, val loss: 1.2882, val acc: 0.3855  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 25360] train loss: 1.2512, train acc: 0.4382, val loss: 1.2851, val acc: 0.3875  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 25380] train loss: 1.2514, train acc: 0.4434, val loss: 1.2886, val acc: 0.3865  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 25400] train loss: 1.2448, train acc: 0.4432, val loss: 1.2921, val acc: 0.3909  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 25420] train loss: 1.2528, train acc: 0.4454, val loss: 1.2891, val acc: 0.3879  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 25440] train loss: 1.2385, train acc: 0.4505, val loss: 1.2753, val acc: 0.3956  (best train acc: 0.4534, best val acc: 0.4863)\n",
      "[Epoch: 25460] train loss: 1.5808, train acc: 0.4184, val loss: 1.6401, val acc: 0.3815  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 25480] train loss: 1.7362, train acc: 0.3588, val loss: 2.1764, val acc: 0.3761  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 25500] train loss: 1.3692, train acc: 0.4170, val loss: 1.5189, val acc: 0.3727  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 25520] train loss: 1.3186, train acc: 0.4259, val loss: 1.4390, val acc: 0.3764  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 25540] train loss: 1.3125, train acc: 0.4231, val loss: 1.3793, val acc: 0.3831  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 25560] train loss: 1.3002, train acc: 0.4296, val loss: 1.3701, val acc: 0.3811  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 25580] train loss: 1.3011, train acc: 0.4262, val loss: 1.3552, val acc: 0.3811  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 25600] train loss: 1.2888, train acc: 0.4238, val loss: 1.3433, val acc: 0.3868  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 25620] train loss: 1.2931, train acc: 0.4348, val loss: 1.3442, val acc: 0.3808  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 25640] train loss: 1.2907, train acc: 0.4250, val loss: 1.3277, val acc: 0.3811  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 25660] train loss: 1.2912, train acc: 0.4244, val loss: 1.3213, val acc: 0.3808  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 25680] train loss: 1.2823, train acc: 0.4309, val loss: 1.3086, val acc: 0.3788  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 25700] train loss: 1.2805, train acc: 0.4325, val loss: 1.3108, val acc: 0.3791  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 25720] train loss: 1.2805, train acc: 0.4295, val loss: 1.3065, val acc: 0.3771  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 25740] train loss: 1.2820, train acc: 0.4307, val loss: 1.3058, val acc: 0.3801  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 25760] train loss: 1.2805, train acc: 0.4363, val loss: 1.3071, val acc: 0.3804  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 25780] train loss: 1.2786, train acc: 0.4313, val loss: 1.3002, val acc: 0.3788  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 25800] train loss: 1.2746, train acc: 0.4386, val loss: 1.3035, val acc: 0.3801  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 25820] train loss: 1.2871, train acc: 0.4213, val loss: 1.2989, val acc: 0.3825  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 25840] train loss: 1.2740, train acc: 0.4373, val loss: 1.3028, val acc: 0.3835  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 25860] train loss: 1.2802, train acc: 0.4326, val loss: 1.3143, val acc: 0.3848  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 25880] train loss: 1.2807, train acc: 0.4281, val loss: 1.2961, val acc: 0.3825  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 25900] train loss: 1.2924, train acc: 0.4312, val loss: 1.3232, val acc: 0.3818  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 25920] train loss: 1.2863, train acc: 0.4361, val loss: 1.3032, val acc: 0.3848  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 25940] train loss: 1.2753, train acc: 0.4330, val loss: 1.3088, val acc: 0.3852  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 25960] train loss: 1.2720, train acc: 0.4380, val loss: 1.2981, val acc: 0.3848  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 25980] train loss: 1.2776, train acc: 0.4252, val loss: 1.3004, val acc: 0.3838  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 26000] train loss: 1.2873, train acc: 0.4172, val loss: 1.3285, val acc: 0.3791  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 26020] train loss: 1.2988, train acc: 0.4149, val loss: 1.3207, val acc: 0.3825  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 26040] train loss: 1.2782, train acc: 0.4221, val loss: 1.3029, val acc: 0.3845  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 26060] train loss: 1.2771, train acc: 0.4417, val loss: 1.3112, val acc: 0.3835  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 26080] train loss: 1.2695, train acc: 0.4398, val loss: 1.3061, val acc: 0.3852  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 26100] train loss: 2.1648, train acc: 0.3624, val loss: 3.6480, val acc: 0.3265  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 26120] train loss: 1.4300, train acc: 0.4109, val loss: 1.5633, val acc: 0.3747  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 26140] train loss: 1.3115, train acc: 0.4140, val loss: 1.3560, val acc: 0.3794  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 26160] train loss: 1.2940, train acc: 0.4149, val loss: 1.3443, val acc: 0.3875  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 26180] train loss: 1.2869, train acc: 0.4205, val loss: 1.3147, val acc: 0.3818  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 26200] train loss: 1.2838, train acc: 0.4223, val loss: 1.3058, val acc: 0.3825  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 26220] train loss: 1.2862, train acc: 0.4268, val loss: 1.3004, val acc: 0.3818  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 26240] train loss: 1.2787, train acc: 0.4288, val loss: 1.2984, val acc: 0.3825  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 26260] train loss: 1.2826, train acc: 0.4317, val loss: 1.2950, val acc: 0.3811  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 26280] train loss: 1.2784, train acc: 0.4296, val loss: 1.2943, val acc: 0.3821  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 26300] train loss: 1.2824, train acc: 0.4344, val loss: 1.2894, val acc: 0.3794  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 26320] train loss: 1.2807, train acc: 0.4327, val loss: 1.3055, val acc: 0.3838  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 26340] train loss: 1.2801, train acc: 0.4237, val loss: 1.3000, val acc: 0.3828  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 26360] train loss: 1.2766, train acc: 0.4303, val loss: 1.2966, val acc: 0.3815  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 26380] train loss: 1.2856, train acc: 0.4121, val loss: 1.3072, val acc: 0.3828  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 26400] train loss: 1.2769, train acc: 0.4362, val loss: 1.2932, val acc: 0.3798  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 26420] train loss: 1.2772, train acc: 0.4385, val loss: 1.2934, val acc: 0.3818  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 26440] train loss: 1.2793, train acc: 0.4331, val loss: 1.2981, val acc: 0.3801  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 26460] train loss: 1.2760, train acc: 0.4304, val loss: 1.2985, val acc: 0.3838  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 26480] train loss: 1.2707, train acc: 0.4362, val loss: 1.2963, val acc: 0.3831  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 26500] train loss: 1.2846, train acc: 0.4090, val loss: 1.3043, val acc: 0.3808  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 26520] train loss: 1.2825, train acc: 0.4352, val loss: 1.3015, val acc: 0.3855  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 26540] train loss: 1.2729, train acc: 0.4310, val loss: 1.2980, val acc: 0.3879  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 26560] train loss: 1.2863, train acc: 0.4353, val loss: 1.3028, val acc: 0.3838  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 26580] train loss: 2.2357, train acc: 0.3569, val loss: 4.4815, val acc: 0.3929  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 26600] train loss: 1.9121, train acc: 0.3610, val loss: 2.7886, val acc: 0.3828  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 26620] train loss: 1.3593, train acc: 0.4091, val loss: 1.5509, val acc: 0.3723  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 26640] train loss: 1.3199, train acc: 0.4036, val loss: 1.3988, val acc: 0.3868  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 26660] train loss: 1.2941, train acc: 0.4179, val loss: 1.3187, val acc: 0.3895  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 26680] train loss: 1.2846, train acc: 0.4211, val loss: 1.2971, val acc: 0.3825  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 26700] train loss: 1.2856, train acc: 0.4169, val loss: 1.2940, val acc: 0.3865  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 26720] train loss: 1.2833, train acc: 0.4286, val loss: 1.2948, val acc: 0.3815  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 26740] train loss: 1.2796, train acc: 0.4233, val loss: 1.2882, val acc: 0.3801  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 26760] train loss: 1.2854, train acc: 0.4297, val loss: 1.2991, val acc: 0.3865  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 26780] train loss: 1.2814, train acc: 0.4265, val loss: 1.2898, val acc: 0.3852  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 26800] train loss: 1.2768, train acc: 0.4327, val loss: 1.2930, val acc: 0.3865  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 26820] train loss: 1.2816, train acc: 0.4181, val loss: 1.2902, val acc: 0.3815  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 26840] train loss: 1.2776, train acc: 0.4064, val loss: 1.2961, val acc: 0.3838  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 26860] train loss: 1.2751, train acc: 0.4085, val loss: 1.3005, val acc: 0.3875  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 26880] train loss: 1.6353, train acc: 0.3764, val loss: 1.6777, val acc: 0.4162  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 26900] train loss: 1.3165, train acc: 0.4127, val loss: 1.3581, val acc: 0.3949  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 26920] train loss: 1.2978, train acc: 0.4234, val loss: 1.3281, val acc: 0.3848  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 26940] train loss: 1.2848, train acc: 0.4325, val loss: 1.3187, val acc: 0.3818  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 26960] train loss: 1.3881, train acc: 0.3960, val loss: 1.4556, val acc: 0.3993  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 26980] train loss: 1.2980, train acc: 0.4294, val loss: 1.3453, val acc: 0.3868  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 27000] train loss: 1.2920, train acc: 0.4159, val loss: 1.3241, val acc: 0.3892  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 27020] train loss: 1.2911, train acc: 0.4258, val loss: 1.3049, val acc: 0.3865  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 27040] train loss: 1.2872, train acc: 0.4307, val loss: 1.2967, val acc: 0.3879  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 27060] train loss: 1.2876, train acc: 0.4194, val loss: 1.2927, val acc: 0.3791  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 27080] train loss: 1.2824, train acc: 0.4270, val loss: 1.2921, val acc: 0.3808  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 27100] train loss: 1.2809, train acc: 0.4257, val loss: 1.2923, val acc: 0.3825  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 27120] train loss: 1.2761, train acc: 0.4296, val loss: 1.2946, val acc: 0.3865  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 27140] train loss: 1.2741, train acc: 0.4332, val loss: 1.2928, val acc: 0.3825  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 27160] train loss: 1.2759, train acc: 0.4341, val loss: 1.2927, val acc: 0.3815  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 27180] train loss: 1.2750, train acc: 0.4176, val loss: 1.3057, val acc: 0.3841  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 27200] train loss: 2.1534, train acc: 0.3623, val loss: 3.7146, val acc: 0.3366  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 27220] train loss: 1.4988, train acc: 0.3755, val loss: 1.8251, val acc: 0.3170  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 27240] train loss: 1.3307, train acc: 0.4305, val loss: 1.4164, val acc: 0.3845  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 27260] train loss: 1.2984, train acc: 0.4126, val loss: 1.3191, val acc: 0.3868  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 27280] train loss: 1.2882, train acc: 0.4195, val loss: 1.3160, val acc: 0.3838  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 27300] train loss: 1.2852, train acc: 0.4245, val loss: 1.2993, val acc: 0.3818  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 27320] train loss: 1.2825, train acc: 0.4293, val loss: 1.2870, val acc: 0.3828  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 27340] train loss: 1.2830, train acc: 0.4245, val loss: 1.2994, val acc: 0.3845  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 27360] train loss: 1.2800, train acc: 0.4313, val loss: 1.3009, val acc: 0.3841  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 27380] train loss: 1.2791, train acc: 0.4205, val loss: 1.2980, val acc: 0.3848  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 27400] train loss: 1.2781, train acc: 0.4324, val loss: 1.2890, val acc: 0.3825  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 27420] train loss: 1.2848, train acc: 0.4038, val loss: 1.2888, val acc: 0.3801  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 27440] train loss: 1.8041, train acc: 0.3800, val loss: 2.3973, val acc: 0.3841  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 27460] train loss: 1.3559, train acc: 0.4096, val loss: 1.3557, val acc: 0.4024  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 27480] train loss: 1.2989, train acc: 0.4275, val loss: 1.3445, val acc: 0.3828  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 27500] train loss: 1.2865, train acc: 0.4231, val loss: 1.2997, val acc: 0.3835  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 27520] train loss: 1.2874, train acc: 0.4225, val loss: 1.3002, val acc: 0.3784  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 27540] train loss: 1.2781, train acc: 0.4307, val loss: 1.3009, val acc: 0.3821  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 27560] train loss: 1.2784, train acc: 0.4333, val loss: 1.2989, val acc: 0.3801  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 27580] train loss: 1.2781, train acc: 0.4226, val loss: 1.2934, val acc: 0.3838  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 27600] train loss: 1.2771, train acc: 0.4331, val loss: 1.2950, val acc: 0.3794  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 27620] train loss: 1.2728, train acc: 0.4259, val loss: 1.2937, val acc: 0.3808  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 27640] train loss: 1.2807, train acc: 0.4075, val loss: 1.2910, val acc: 0.3794  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 27660] train loss: 1.7235, train acc: 0.3670, val loss: 2.6209, val acc: 0.4202  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 27680] train loss: 1.5606, train acc: 0.3751, val loss: 2.2165, val acc: 0.4040  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 27700] train loss: 1.4649, train acc: 0.3751, val loss: 2.0809, val acc: 0.3717  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 27720] train loss: 1.4379, train acc: 0.3967, val loss: 1.8255, val acc: 0.3811  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 27740] train loss: 1.3774, train acc: 0.4057, val loss: 1.4820, val acc: 0.3868  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 27760] train loss: 1.3142, train acc: 0.4166, val loss: 1.3556, val acc: 0.3872  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 27780] train loss: 1.3043, train acc: 0.4255, val loss: 1.3199, val acc: 0.3798  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 27800] train loss: 1.2954, train acc: 0.4280, val loss: 1.3129, val acc: 0.3902  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 27820] train loss: 1.2899, train acc: 0.4301, val loss: 1.3024, val acc: 0.3811  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 27840] train loss: 1.2897, train acc: 0.4315, val loss: 1.3045, val acc: 0.3862  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 27860] train loss: 1.2809, train acc: 0.4299, val loss: 1.3079, val acc: 0.3848  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 27880] train loss: 1.2839, train acc: 0.4349, val loss: 1.3020, val acc: 0.3838  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 27900] train loss: 1.2861, train acc: 0.4311, val loss: 1.3046, val acc: 0.3852  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 27920] train loss: 1.2818, train acc: 0.4247, val loss: 1.3056, val acc: 0.3821  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 27940] train loss: 2.4496, train acc: 0.3611, val loss: 5.4483, val acc: 0.3595  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 27960] train loss: 2.0264, train acc: 0.3641, val loss: 3.4284, val acc: 0.3700  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 27980] train loss: 1.4609, train acc: 0.3559, val loss: 1.4910, val acc: 0.3990  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 28000] train loss: 1.3189, train acc: 0.4317, val loss: 1.3543, val acc: 0.3889  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 28020] train loss: 1.2975, train acc: 0.4249, val loss: 1.3222, val acc: 0.3936  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 28040] train loss: 1.2955, train acc: 0.4224, val loss: 1.3132, val acc: 0.3919  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 28060] train loss: 1.2841, train acc: 0.4334, val loss: 1.3083, val acc: 0.3852  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 28080] train loss: 1.2858, train acc: 0.4406, val loss: 1.2958, val acc: 0.3855  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 28100] train loss: 1.2804, train acc: 0.4295, val loss: 1.3046, val acc: 0.3845  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 28120] train loss: 1.2813, train acc: 0.4320, val loss: 1.3006, val acc: 0.3777  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 28140] train loss: 1.2793, train acc: 0.4328, val loss: 1.3023, val acc: 0.3845  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 28160] train loss: 1.2802, train acc: 0.4305, val loss: 1.3021, val acc: 0.3879  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 28180] train loss: 1.2870, train acc: 0.4352, val loss: 1.3068, val acc: 0.3838  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 28200] train loss: 1.8389, train acc: 0.3673, val loss: 2.5729, val acc: 0.3761  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 28220] train loss: 1.3806, train acc: 0.4072, val loss: 1.5747, val acc: 0.3875  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 28240] train loss: 1.3041, train acc: 0.4242, val loss: 1.3271, val acc: 0.3909  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 28260] train loss: 1.2902, train acc: 0.4113, val loss: 1.3214, val acc: 0.3858  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 28280] train loss: 1.2891, train acc: 0.4252, val loss: 1.3048, val acc: 0.3892  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 28300] train loss: 1.2828, train acc: 0.4312, val loss: 1.3008, val acc: 0.3852  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 28320] train loss: 1.2768, train acc: 0.4316, val loss: 1.2995, val acc: 0.3845  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 28340] train loss: 1.2837, train acc: 0.4278, val loss: 1.3035, val acc: 0.3855  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 28360] train loss: 1.2800, train acc: 0.4375, val loss: 1.3024, val acc: 0.3872  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 28380] train loss: 1.2784, train acc: 0.4380, val loss: 1.3145, val acc: 0.3862  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 28400] train loss: 1.2574, train acc: 0.4525, val loss: 1.3233, val acc: 0.3841  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 28420] train loss: 1.6384, train acc: 0.3727, val loss: 2.2272, val acc: 0.4172  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 28440] train loss: 1.3541, train acc: 0.4045, val loss: 1.3812, val acc: 0.3804  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 28460] train loss: 1.2972, train acc: 0.4257, val loss: 1.3024, val acc: 0.3970  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 28480] train loss: 1.2878, train acc: 0.4238, val loss: 1.3087, val acc: 0.3825  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 28500] train loss: 1.2824, train acc: 0.4220, val loss: 1.3088, val acc: 0.3912  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 28520] train loss: 1.2783, train acc: 0.4303, val loss: 1.2928, val acc: 0.3835  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 28540] train loss: 1.2723, train acc: 0.4312, val loss: 1.2966, val acc: 0.3825  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 28560] train loss: 1.2730, train acc: 0.4296, val loss: 1.2972, val acc: 0.3828  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 28580] train loss: 1.2519, train acc: 0.4334, val loss: 1.3094, val acc: 0.3868  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 28600] train loss: 1.7035, train acc: 0.3763, val loss: 2.4088, val acc: 0.3872  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 28620] train loss: 1.3430, train acc: 0.4068, val loss: 1.3266, val acc: 0.3990  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 28640] train loss: 1.2892, train acc: 0.4286, val loss: 1.3511, val acc: 0.3872  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 28660] train loss: 1.2835, train acc: 0.4307, val loss: 1.3124, val acc: 0.3889  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 28680] train loss: 1.2767, train acc: 0.4282, val loss: 1.3073, val acc: 0.3875  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 28700] train loss: 1.2727, train acc: 0.4281, val loss: 1.2933, val acc: 0.3858  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 28720] train loss: 1.2714, train acc: 0.4325, val loss: 1.2932, val acc: 0.3841  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 28740] train loss: 1.2654, train acc: 0.4331, val loss: 1.2907, val acc: 0.3875  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 28760] train loss: 1.8313, train acc: 0.4145, val loss: 1.7675, val acc: 0.3848  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 28780] train loss: 2.2114, train acc: 0.3660, val loss: 3.9962, val acc: 0.3781  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 28800] train loss: 1.4018, train acc: 0.3801, val loss: 1.5247, val acc: 0.3669  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 28820] train loss: 1.2994, train acc: 0.4295, val loss: 1.3166, val acc: 0.3862  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 28840] train loss: 1.2860, train acc: 0.4179, val loss: 1.3115, val acc: 0.3889  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 28860] train loss: 1.2795, train acc: 0.4231, val loss: 1.2999, val acc: 0.3892  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 28880] train loss: 1.2780, train acc: 0.4292, val loss: 1.2955, val acc: 0.3858  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 28900] train loss: 1.2744, train acc: 0.4304, val loss: 1.2849, val acc: 0.3852  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 28920] train loss: 1.2695, train acc: 0.4336, val loss: 1.2921, val acc: 0.3879  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 28940] train loss: 1.2674, train acc: 0.4325, val loss: 1.2893, val acc: 0.3865  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 28960] train loss: 1.2645, train acc: 0.4233, val loss: 1.2855, val acc: 0.3868  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 28980] train loss: 1.7334, train acc: 0.3717, val loss: 2.4461, val acc: 0.3713  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 29000] train loss: 1.3317, train acc: 0.4190, val loss: 1.3518, val acc: 0.4057  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 29020] train loss: 1.2901, train acc: 0.4276, val loss: 1.3081, val acc: 0.3889  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 29040] train loss: 1.2844, train acc: 0.4146, val loss: 1.3097, val acc: 0.3862  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 29060] train loss: 1.2744, train acc: 0.4281, val loss: 1.2940, val acc: 0.3885  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 29080] train loss: 1.3943, train acc: 0.4179, val loss: 1.4266, val acc: 0.3858  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 29100] train loss: 1.2739, train acc: 0.4312, val loss: 1.3346, val acc: 0.3872  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 29120] train loss: 1.3444, train acc: 0.4085, val loss: 1.5587, val acc: 0.4034  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 29140] train loss: 1.3300, train acc: 0.3856, val loss: 1.4187, val acc: 0.3906  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 29160] train loss: 1.2782, train acc: 0.4271, val loss: 1.3033, val acc: 0.3906  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 29180] train loss: 1.2672, train acc: 0.4312, val loss: 1.2969, val acc: 0.3865  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 29200] train loss: 1.2584, train acc: 0.4392, val loss: 1.3037, val acc: 0.3899  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 29220] train loss: 2.1195, train acc: 0.3603, val loss: 3.8303, val acc: 0.3818  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 29240] train loss: 1.3582, train acc: 0.3764, val loss: 1.4440, val acc: 0.4098  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 29260] train loss: 1.2935, train acc: 0.4062, val loss: 1.3695, val acc: 0.3804  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 29280] train loss: 1.2782, train acc: 0.4180, val loss: 1.3185, val acc: 0.3916  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 29300] train loss: 1.2690, train acc: 0.4254, val loss: 1.2966, val acc: 0.3906  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 29320] train loss: 1.2670, train acc: 0.4302, val loss: 1.2963, val acc: 0.3895  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 29340] train loss: 1.2581, train acc: 0.4448, val loss: 1.2904, val acc: 0.3909  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 29360] train loss: 2.0571, train acc: 0.3657, val loss: 3.5474, val acc: 0.3487  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 29380] train loss: 1.3545, train acc: 0.3897, val loss: 1.4460, val acc: 0.3835  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 29400] train loss: 1.3039, train acc: 0.4010, val loss: 1.3558, val acc: 0.3892  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 29420] train loss: 1.2723, train acc: 0.4156, val loss: 1.3025, val acc: 0.3892  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 29440] train loss: 1.2659, train acc: 0.4268, val loss: 1.2945, val acc: 0.3868  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 29460] train loss: 1.2569, train acc: 0.4356, val loss: 1.2959, val acc: 0.3858  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 29480] train loss: 1.8144, train acc: 0.3708, val loss: 2.6449, val acc: 0.3889  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 29500] train loss: 1.3241, train acc: 0.4258, val loss: 1.4137, val acc: 0.3828  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 29520] train loss: 1.2881, train acc: 0.4257, val loss: 1.3247, val acc: 0.3858  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 29540] train loss: 1.2715, train acc: 0.4226, val loss: 1.2992, val acc: 0.3821  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 29560] train loss: 1.2685, train acc: 0.4242, val loss: 1.2930, val acc: 0.3895  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 29580] train loss: 1.2685, train acc: 0.4280, val loss: 1.2877, val acc: 0.3855  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 29600] train loss: 1.2582, train acc: 0.4332, val loss: 1.2961, val acc: 0.3909  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 29620] train loss: 1.4227, train acc: 0.4269, val loss: 1.5466, val acc: 0.3848  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 29640] train loss: 1.7690, train acc: 0.3738, val loss: 2.4557, val acc: 0.3936  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 29660] train loss: 1.3197, train acc: 0.3898, val loss: 1.3155, val acc: 0.3960  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 29680] train loss: 1.2809, train acc: 0.4210, val loss: 1.2980, val acc: 0.3926  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 29700] train loss: 1.2716, train acc: 0.4232, val loss: 1.3062, val acc: 0.3882  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 29720] train loss: 1.2651, train acc: 0.4299, val loss: 1.2936, val acc: 0.3865  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 29740] train loss: 1.2627, train acc: 0.4338, val loss: 1.2831, val acc: 0.3889  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 29760] train loss: 1.2596, train acc: 0.4307, val loss: 1.2800, val acc: 0.3862  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 29780] train loss: 1.2553, train acc: 0.4380, val loss: 1.2867, val acc: 0.3875  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 29800] train loss: 1.2572, train acc: 0.4342, val loss: 1.2834, val acc: 0.3882  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 29820] train loss: 1.3219, train acc: 0.4032, val loss: 1.3577, val acc: 0.4064  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 29840] train loss: 1.2723, train acc: 0.4294, val loss: 1.3070, val acc: 0.3882  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 29860] train loss: 1.2614, train acc: 0.4346, val loss: 1.2911, val acc: 0.3902  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 29880] train loss: 1.2578, train acc: 0.4353, val loss: 1.2854, val acc: 0.3892  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 29900] train loss: 1.2866, train acc: 0.4309, val loss: 1.3212, val acc: 0.4013  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 29920] train loss: 1.8775, train acc: 0.3596, val loss: 3.4634, val acc: 0.3565  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 29940] train loss: 1.6903, train acc: 0.3712, val loss: 2.8255, val acc: 0.4010  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 29960] train loss: 1.5059, train acc: 0.3715, val loss: 2.5978, val acc: 0.3821  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 29980] train loss: 1.5164, train acc: 0.3645, val loss: 2.4969, val acc: 0.3801  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 30000] train loss: 1.5276, train acc: 0.3733, val loss: 2.3748, val acc: 0.3828  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 30020] train loss: 1.4736, train acc: 0.3791, val loss: 2.2688, val acc: 0.3879  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 30040] train loss: 1.4646, train acc: 0.3809, val loss: 2.1590, val acc: 0.3879  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 30060] train loss: 1.4574, train acc: 0.3838, val loss: 2.0471, val acc: 0.3939  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 30080] train loss: 1.4203, train acc: 0.3851, val loss: 1.9467, val acc: 0.3990  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 30100] train loss: 1.4109, train acc: 0.3870, val loss: 1.7493, val acc: 0.4003  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 30120] train loss: 1.3815, train acc: 0.4114, val loss: 1.5873, val acc: 0.3997  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 30140] train loss: 1.3199, train acc: 0.4300, val loss: 1.3591, val acc: 0.4189  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 30160] train loss: 1.2887, train acc: 0.4168, val loss: 1.3151, val acc: 0.3902  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 30180] train loss: 1.2704, train acc: 0.4219, val loss: 1.2948, val acc: 0.3960  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 30200] train loss: 1.2655, train acc: 0.4250, val loss: 1.2889, val acc: 0.3926  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 30220] train loss: 1.2717, train acc: 0.4211, val loss: 1.2929, val acc: 0.3902  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 30240] train loss: 1.2688, train acc: 0.4305, val loss: 1.2905, val acc: 0.3885  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 30260] train loss: 1.2688, train acc: 0.4254, val loss: 1.2973, val acc: 0.3922  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 30280] train loss: 1.2595, train acc: 0.4396, val loss: 1.2808, val acc: 0.3926  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 30300] train loss: 1.2650, train acc: 0.4336, val loss: 1.2835, val acc: 0.3889  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 30320] train loss: 1.2656, train acc: 0.4381, val loss: 1.2858, val acc: 0.3899  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 30340] train loss: 1.2556, train acc: 0.4354, val loss: 1.2836, val acc: 0.3909  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 30360] train loss: 1.2613, train acc: 0.4351, val loss: 1.2762, val acc: 0.3885  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 30380] train loss: 1.2537, train acc: 0.4404, val loss: 1.2842, val acc: 0.3916  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 30400] train loss: 1.2572, train acc: 0.4388, val loss: 1.2796, val acc: 0.3879  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 30420] train loss: 1.2577, train acc: 0.4390, val loss: 1.2777, val acc: 0.3889  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 30440] train loss: 1.2509, train acc: 0.4425, val loss: 1.2800, val acc: 0.3933  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 30460] train loss: 1.2482, train acc: 0.4356, val loss: 1.2844, val acc: 0.3852  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 30480] train loss: 1.2505, train acc: 0.4453, val loss: 1.2767, val acc: 0.3885  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 30500] train loss: 1.2480, train acc: 0.4371, val loss: 1.2763, val acc: 0.3899  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 30520] train loss: 1.2507, train acc: 0.4345, val loss: 1.2783, val acc: 0.3858  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 30540] train loss: 1.2502, train acc: 0.4367, val loss: 1.2774, val acc: 0.3872  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 30560] train loss: 1.2461, train acc: 0.4406, val loss: 1.2798, val acc: 0.3895  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 30580] train loss: 1.2521, train acc: 0.4397, val loss: 1.2801, val acc: 0.3889  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 30600] train loss: 1.2432, train acc: 0.4400, val loss: 1.2842, val acc: 0.3882  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 30620] train loss: 1.2506, train acc: 0.4385, val loss: 1.2664, val acc: 0.3980  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 30640] train loss: 1.2473, train acc: 0.4375, val loss: 1.2810, val acc: 0.3885  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 30660] train loss: 1.2419, train acc: 0.4453, val loss: 1.2701, val acc: 0.3926  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 30680] train loss: 1.2482, train acc: 0.4425, val loss: 1.2736, val acc: 0.3868  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 30700] train loss: 1.2411, train acc: 0.4380, val loss: 1.2752, val acc: 0.3855  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 30720] train loss: 1.2517, train acc: 0.4341, val loss: 1.2763, val acc: 0.3912  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 30740] train loss: 1.2432, train acc: 0.4430, val loss: 1.2728, val acc: 0.3906  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 30760] train loss: 1.2444, train acc: 0.4419, val loss: 1.2753, val acc: 0.3909  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 30780] train loss: 1.2463, train acc: 0.4400, val loss: 1.2762, val acc: 0.3909  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 30800] train loss: 1.2383, train acc: 0.4350, val loss: 1.2783, val acc: 0.3855  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 30820] train loss: 1.2435, train acc: 0.4390, val loss: 1.2651, val acc: 0.3983  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 30840] train loss: 1.2429, train acc: 0.4445, val loss: 1.2750, val acc: 0.3933  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 30860] train loss: 1.2375, train acc: 0.4419, val loss: 1.2842, val acc: 0.3882  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 30880] train loss: 1.2360, train acc: 0.4403, val loss: 1.2751, val acc: 0.3895  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 30900] train loss: 1.2369, train acc: 0.4384, val loss: 1.2745, val acc: 0.3916  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 30920] train loss: 1.2283, train acc: 0.4456, val loss: 1.2889, val acc: 0.3902  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 30940] train loss: 1.2435, train acc: 0.4406, val loss: 1.2758, val acc: 0.3919  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 30960] train loss: 1.2934, train acc: 0.4250, val loss: 1.3303, val acc: 0.3777  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 30980] train loss: 2.5092, train acc: 0.3592, val loss: 5.4777, val acc: 0.3477  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 31000] train loss: 1.5673, train acc: 0.3677, val loss: 2.5378, val acc: 0.3912  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 31020] train loss: 1.4807, train acc: 0.3704, val loss: 2.0694, val acc: 0.3801  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 31040] train loss: 1.4203, train acc: 0.3798, val loss: 1.6768, val acc: 0.3966  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 31060] train loss: 1.3465, train acc: 0.4060, val loss: 1.3900, val acc: 0.4266  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 31080] train loss: 1.3042, train acc: 0.4255, val loss: 1.3452, val acc: 0.3956  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 31100] train loss: 1.2887, train acc: 0.4208, val loss: 1.3194, val acc: 0.3912  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 31120] train loss: 1.2791, train acc: 0.4219, val loss: 1.3176, val acc: 0.3899  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 31140] train loss: 1.2738, train acc: 0.4280, val loss: 1.3025, val acc: 0.3879  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 31160] train loss: 1.2647, train acc: 0.4263, val loss: 1.2957, val acc: 0.3970  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 31180] train loss: 1.2693, train acc: 0.4263, val loss: 1.2959, val acc: 0.3929  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 31200] train loss: 1.2537, train acc: 0.4305, val loss: 1.2871, val acc: 0.3848  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 31220] train loss: 1.2571, train acc: 0.4314, val loss: 1.2885, val acc: 0.3912  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 31240] train loss: 1.2589, train acc: 0.4328, val loss: 1.2939, val acc: 0.3858  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 31260] train loss: 1.2545, train acc: 0.4419, val loss: 1.2987, val acc: 0.3872  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 31280] train loss: 1.2547, train acc: 0.4409, val loss: 1.2951, val acc: 0.3912  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 31300] train loss: 1.2490, train acc: 0.4312, val loss: 1.2837, val acc: 0.3885  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 31320] train loss: 1.7585, train acc: 0.3608, val loss: 2.5192, val acc: 0.4108  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 31340] train loss: 1.4134, train acc: 0.3962, val loss: 1.6616, val acc: 0.3531  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 31360] train loss: 1.2819, train acc: 0.4080, val loss: 1.3041, val acc: 0.3798  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 31380] train loss: 1.2639, train acc: 0.4216, val loss: 1.3000, val acc: 0.3794  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 31400] train loss: 1.2553, train acc: 0.4286, val loss: 1.3001, val acc: 0.3875  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 31420] train loss: 1.2469, train acc: 0.4356, val loss: 1.2884, val acc: 0.3916  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 31440] train loss: 1.2401, train acc: 0.4385, val loss: 1.2846, val acc: 0.3909  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 31460] train loss: 1.5744, train acc: 0.3884, val loss: 2.5389, val acc: 0.4145  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 31480] train loss: 1.9574, train acc: 0.3610, val loss: 3.6849, val acc: 0.2931  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 31500] train loss: 1.4906, train acc: 0.3624, val loss: 1.8399, val acc: 0.3423  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 31520] train loss: 1.2940, train acc: 0.4292, val loss: 1.3643, val acc: 0.3676  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 31540] train loss: 1.2768, train acc: 0.4104, val loss: 1.3188, val acc: 0.3892  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 31560] train loss: 1.2583, train acc: 0.4264, val loss: 1.2995, val acc: 0.3825  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 31580] train loss: 1.2646, train acc: 0.4330, val loss: 1.3100, val acc: 0.3747  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 31600] train loss: 1.2568, train acc: 0.4303, val loss: 1.3004, val acc: 0.3798  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 31620] train loss: 1.2542, train acc: 0.4310, val loss: 1.2969, val acc: 0.3734  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 31640] train loss: 1.2512, train acc: 0.4380, val loss: 1.2899, val acc: 0.3744  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 31660] train loss: 1.2499, train acc: 0.4336, val loss: 1.2824, val acc: 0.3788  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 31680] train loss: 1.2567, train acc: 0.4277, val loss: 1.2992, val acc: 0.3788  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 31700] train loss: 1.2498, train acc: 0.4291, val loss: 1.2917, val acc: 0.3808  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 31720] train loss: 1.2429, train acc: 0.4376, val loss: 1.2908, val acc: 0.3922  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 31740] train loss: 1.2461, train acc: 0.4363, val loss: 1.3009, val acc: 0.3858  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 31760] train loss: 1.6768, train acc: 0.3609, val loss: 2.0306, val acc: 0.3825  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 31780] train loss: 1.2819, train acc: 0.4047, val loss: 1.3987, val acc: 0.3966  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 31800] train loss: 1.2571, train acc: 0.4281, val loss: 1.3345, val acc: 0.3700  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 31820] train loss: 1.2498, train acc: 0.4323, val loss: 1.3120, val acc: 0.3811  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 31840] train loss: 1.2385, train acc: 0.4359, val loss: 1.2880, val acc: 0.3916  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 31860] train loss: 1.2331, train acc: 0.4402, val loss: 1.2785, val acc: 0.3936  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 31880] train loss: 1.2387, train acc: 0.4421, val loss: 1.2759, val acc: 0.3946  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 31900] train loss: 1.2220, train acc: 0.4558, val loss: 1.2759, val acc: 0.3926  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 31920] train loss: 1.2724, train acc: 0.4305, val loss: 1.3236, val acc: 0.3700  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 31940] train loss: 1.2410, train acc: 0.4362, val loss: 1.2934, val acc: 0.3835  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 31960] train loss: 1.2349, train acc: 0.4371, val loss: 1.2811, val acc: 0.3875  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 31980] train loss: 1.2274, train acc: 0.4412, val loss: 1.2819, val acc: 0.3939  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 32000] train loss: 1.2314, train acc: 0.4472, val loss: 1.2791, val acc: 0.3939  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 32020] train loss: 1.2234, train acc: 0.4426, val loss: 1.2734, val acc: 0.3973  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 32040] train loss: 1.2620, train acc: 0.4022, val loss: 1.3737, val acc: 0.3767  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 32060] train loss: 1.2380, train acc: 0.4325, val loss: 1.3150, val acc: 0.3673  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 32080] train loss: 1.2304, train acc: 0.4318, val loss: 1.2946, val acc: 0.3895  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 32100] train loss: 1.2370, train acc: 0.4372, val loss: 1.2805, val acc: 0.3916  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 32120] train loss: 1.2288, train acc: 0.4393, val loss: 1.2778, val acc: 0.3895  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 32140] train loss: 1.2239, train acc: 0.4422, val loss: 1.2743, val acc: 0.3919  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 32160] train loss: 1.2139, train acc: 0.4506, val loss: 1.2697, val acc: 0.3966  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 32180] train loss: 1.2345, train acc: 0.4394, val loss: 1.2992, val acc: 0.3761  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 32200] train loss: 1.2290, train acc: 0.4365, val loss: 1.2724, val acc: 0.3939  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 32220] train loss: 1.2349, train acc: 0.4390, val loss: 1.2837, val acc: 0.3909  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 32240] train loss: 1.2243, train acc: 0.4432, val loss: 1.2854, val acc: 0.3895  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 32260] train loss: 1.2298, train acc: 0.4356, val loss: 1.2800, val acc: 0.3865  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 32280] train loss: 1.2252, train acc: 0.4417, val loss: 1.2881, val acc: 0.3929  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 32300] train loss: 1.2162, train acc: 0.4493, val loss: 1.2717, val acc: 0.3949  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 32320] train loss: 1.2595, train acc: 0.4412, val loss: 1.2866, val acc: 0.3976  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 32340] train loss: 1.2262, train acc: 0.4376, val loss: 1.2673, val acc: 0.3963  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 32360] train loss: 1.3099, train acc: 0.4009, val loss: 1.3908, val acc: 0.4145  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 32380] train loss: 1.2383, train acc: 0.4238, val loss: 1.2653, val acc: 0.3939  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 32400] train loss: 1.2242, train acc: 0.4412, val loss: 1.2614, val acc: 0.3946  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 32420] train loss: 1.2218, train acc: 0.4372, val loss: 1.2657, val acc: 0.3909  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 32440] train loss: 1.2217, train acc: 0.4383, val loss: 1.2706, val acc: 0.3895  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 32460] train loss: 1.2168, train acc: 0.4391, val loss: 1.2676, val acc: 0.3943  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 32480] train loss: 1.2219, train acc: 0.4406, val loss: 1.2634, val acc: 0.3943  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 32500] train loss: 1.2126, train acc: 0.4373, val loss: 1.2612, val acc: 0.3885  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 32520] train loss: 1.2170, train acc: 0.4448, val loss: 1.2570, val acc: 0.3919  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 32540] train loss: 1.2100, train acc: 0.4487, val loss: 1.2476, val acc: 0.3976  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 32560] train loss: 1.2141, train acc: 0.4422, val loss: 1.2539, val acc: 0.3976  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 32580] train loss: 1.2107, train acc: 0.4456, val loss: 1.2507, val acc: 0.3943  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 32600] train loss: 1.2119, train acc: 0.4465, val loss: 1.2547, val acc: 0.3926  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 32620] train loss: 1.2063, train acc: 0.4422, val loss: 1.2455, val acc: 0.3966  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 32640] train loss: 1.2061, train acc: 0.4411, val loss: 1.2427, val acc: 0.3970  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 32660] train loss: 1.2085, train acc: 0.4503, val loss: 1.2457, val acc: 0.4027  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 32680] train loss: 1.2026, train acc: 0.4510, val loss: 1.2442, val acc: 0.4030  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 32700] train loss: 1.2044, train acc: 0.4434, val loss: 1.2436, val acc: 0.3966  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 32720] train loss: 1.2068, train acc: 0.4437, val loss: 1.2469, val acc: 0.3956  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 32740] train loss: 1.2084, train acc: 0.4472, val loss: 1.2494, val acc: 0.3879  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 32760] train loss: 1.2016, train acc: 0.4491, val loss: 1.2416, val acc: 0.3912  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 32780] train loss: 1.2026, train acc: 0.4409, val loss: 1.2419, val acc: 0.3960  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 32800] train loss: 1.2084, train acc: 0.4435, val loss: 1.2410, val acc: 0.3885  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 32820] train loss: 1.2098, train acc: 0.4419, val loss: 1.2406, val acc: 0.3926  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 32840] train loss: 1.2089, train acc: 0.4370, val loss: 1.2420, val acc: 0.3899  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 32860] train loss: 1.2000, train acc: 0.4400, val loss: 1.2509, val acc: 0.3983  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 32880] train loss: 1.1970, train acc: 0.4434, val loss: 1.2438, val acc: 0.3926  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 32900] train loss: 1.2042, train acc: 0.4438, val loss: 1.2521, val acc: 0.3966  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 32920] train loss: 1.2041, train acc: 0.4450, val loss: 1.2409, val acc: 0.3987  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 32940] train loss: 1.1993, train acc: 0.4421, val loss: 1.2603, val acc: 0.3966  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 32960] train loss: 1.2067, train acc: 0.4421, val loss: 1.2474, val acc: 0.3987  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 32980] train loss: 1.2042, train acc: 0.4298, val loss: 1.2508, val acc: 0.3885  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 33000] train loss: 1.2049, train acc: 0.4377, val loss: 1.2603, val acc: 0.3892  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 33020] train loss: 1.2175, train acc: 0.4349, val loss: 1.2432, val acc: 0.3902  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 33040] train loss: 1.1925, train acc: 0.4465, val loss: 1.2405, val acc: 0.3997  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 33060] train loss: 1.2073, train acc: 0.4391, val loss: 1.2395, val acc: 0.3970  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 33080] train loss: 1.1948, train acc: 0.4435, val loss: 1.2410, val acc: 0.3980  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 33100] train loss: 1.1999, train acc: 0.4470, val loss: 1.2576, val acc: 0.3926  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 33120] train loss: 1.1964, train acc: 0.4407, val loss: 1.2367, val acc: 0.3956  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 33140] train loss: 1.2006, train acc: 0.4485, val loss: 1.2317, val acc: 0.3936  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 33160] train loss: 1.1963, train acc: 0.4489, val loss: 1.2313, val acc: 0.3966  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 33180] train loss: 1.1916, train acc: 0.4491, val loss: 1.2335, val acc: 0.3922  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 33200] train loss: 1.1961, train acc: 0.4470, val loss: 1.2348, val acc: 0.3997  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 33220] train loss: 1.1914, train acc: 0.4471, val loss: 1.2396, val acc: 0.3892  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 33240] train loss: 1.2001, train acc: 0.4425, val loss: 1.2343, val acc: 0.3976  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 33260] train loss: 1.1906, train acc: 0.4440, val loss: 1.2459, val acc: 0.3963  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 33280] train loss: 1.1985, train acc: 0.4444, val loss: 1.2395, val acc: 0.3993  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 33300] train loss: 1.1955, train acc: 0.4425, val loss: 1.2250, val acc: 0.3936  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 33320] train loss: 1.2143, train acc: 0.4341, val loss: 1.2606, val acc: 0.3841  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 33340] train loss: 1.1945, train acc: 0.4477, val loss: 1.2462, val acc: 0.3973  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 33360] train loss: 1.2133, train acc: 0.4349, val loss: 1.2539, val acc: 0.3858  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 33380] train loss: 1.1884, train acc: 0.4515, val loss: 1.2350, val acc: 0.3983  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 33400] train loss: 1.1992, train acc: 0.4485, val loss: 1.2398, val acc: 0.3916  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 33420] train loss: 1.2053, train acc: 0.4517, val loss: 1.2450, val acc: 0.3990  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 33440] train loss: 1.1920, train acc: 0.4476, val loss: 1.2272, val acc: 0.3990  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 33460] train loss: 1.2008, train acc: 0.4379, val loss: 1.2353, val acc: 0.3966  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 33480] train loss: 1.1916, train acc: 0.4490, val loss: 1.2310, val acc: 0.3976  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 33500] train loss: 1.1960, train acc: 0.4432, val loss: 1.2277, val acc: 0.3933  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 33520] train loss: 1.2007, train acc: 0.4404, val loss: 1.2253, val acc: 0.4003  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 33540] train loss: 1.1937, train acc: 0.4502, val loss: 1.2443, val acc: 0.3946  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 33560] train loss: 1.1993, train acc: 0.4532, val loss: 1.2348, val acc: 0.3926  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 33580] train loss: 1.1972, train acc: 0.4396, val loss: 1.2320, val acc: 0.3963  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 33600] train loss: 1.1962, train acc: 0.4487, val loss: 1.2439, val acc: 0.3939  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 33620] train loss: 1.2059, train acc: 0.4403, val loss: 1.2457, val acc: 0.3976  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 33640] train loss: 1.1981, train acc: 0.4463, val loss: 1.2414, val acc: 0.3953  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 33660] train loss: 1.1907, train acc: 0.4462, val loss: 1.2234, val acc: 0.3997  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 33680] train loss: 1.1956, train acc: 0.4529, val loss: 1.2502, val acc: 0.3902  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 33700] train loss: 1.1850, train acc: 0.4482, val loss: 1.2205, val acc: 0.3946  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 33720] train loss: 1.1862, train acc: 0.4453, val loss: 1.2239, val acc: 0.3946  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 33740] train loss: 1.1915, train acc: 0.4496, val loss: 1.2381, val acc: 0.3926  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 33760] train loss: 1.1794, train acc: 0.4507, val loss: 1.2162, val acc: 0.4007  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 33780] train loss: 1.1764, train acc: 0.4542, val loss: 1.2126, val acc: 0.3987  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 33800] train loss: 1.1769, train acc: 0.4468, val loss: 1.2330, val acc: 0.3980  (best train acc: 0.4666, best val acc: 0.4863)\n",
      "[Epoch: 33820] train loss: 1.1763, train acc: 0.4537, val loss: 1.2262, val acc: 0.4024  (best train acc: 0.4672, best val acc: 0.4863)\n",
      "[Epoch: 33840] train loss: 1.1957, train acc: 0.4383, val loss: 1.2234, val acc: 0.4078  (best train acc: 0.4672, best val acc: 0.4863)\n",
      "[Epoch: 33860] train loss: 1.1865, train acc: 0.4545, val loss: 1.2608, val acc: 0.3895  (best train acc: 0.4672, best val acc: 0.4863)\n",
      "[Epoch: 33880] train loss: 1.1768, train acc: 0.4563, val loss: 1.2284, val acc: 0.4101  (best train acc: 0.4672, best val acc: 0.4863)\n",
      "[Epoch: 33900] train loss: 1.1809, train acc: 0.4512, val loss: 1.2269, val acc: 0.4084  (best train acc: 0.4672, best val acc: 0.4863)\n",
      "[Epoch: 33920] train loss: 1.1780, train acc: 0.4513, val loss: 1.2213, val acc: 0.4091  (best train acc: 0.4672, best val acc: 0.4863)\n",
      "[Epoch: 33940] train loss: 1.1812, train acc: 0.4605, val loss: 1.2360, val acc: 0.3983  (best train acc: 0.4672, best val acc: 0.4863)\n",
      "[Epoch: 33960] train loss: 1.1900, train acc: 0.4573, val loss: 1.2458, val acc: 0.4037  (best train acc: 0.4672, best val acc: 0.4863)\n",
      "[Epoch: 33980] train loss: 1.1802, train acc: 0.4585, val loss: 1.2378, val acc: 0.4027  (best train acc: 0.4672, best val acc: 0.4863)\n",
      "[Epoch: 34000] train loss: 1.1812, train acc: 0.4525, val loss: 1.2215, val acc: 0.4074  (best train acc: 0.4672, best val acc: 0.4863)\n",
      "[Epoch: 34020] train loss: 1.1742, train acc: 0.4560, val loss: 1.2332, val acc: 0.4003  (best train acc: 0.4672, best val acc: 0.4863)\n",
      "[Epoch: 34040] train loss: 1.1714, train acc: 0.4582, val loss: 1.2270, val acc: 0.4037  (best train acc: 0.4672, best val acc: 0.4863)\n",
      "[Epoch: 34060] train loss: 1.1930, train acc: 0.4587, val loss: 1.2420, val acc: 0.4013  (best train acc: 0.4672, best val acc: 0.4863)\n",
      "[Epoch: 34080] train loss: 1.1763, train acc: 0.4529, val loss: 1.2419, val acc: 0.4007  (best train acc: 0.4672, best val acc: 0.4863)\n",
      "[Epoch: 34100] train loss: 1.1670, train acc: 0.4665, val loss: 1.2148, val acc: 0.4121  (best train acc: 0.4695, best val acc: 0.4863)\n",
      "[Epoch: 34120] train loss: 1.1682, train acc: 0.4677, val loss: 1.2116, val acc: 0.4152  (best train acc: 0.4703, best val acc: 0.4863)\n",
      "[Epoch: 34140] train loss: 1.1556, train acc: 0.4650, val loss: 1.1946, val acc: 0.4368  (best train acc: 0.4713, best val acc: 0.4863)\n",
      "[Epoch: 34160] train loss: 1.1844, train acc: 0.4467, val loss: 1.2297, val acc: 0.4044  (best train acc: 0.4826, best val acc: 0.4863)\n",
      "[Epoch: 34180] train loss: 1.1739, train acc: 0.4558, val loss: 1.2375, val acc: 0.4064  (best train acc: 0.4826, best val acc: 0.4863)\n",
      "[Epoch: 34200] train loss: 1.1901, train acc: 0.4503, val loss: 1.2396, val acc: 0.4078  (best train acc: 0.4826, best val acc: 0.4863)\n",
      "[Epoch: 34220] train loss: 1.1765, train acc: 0.4615, val loss: 1.2330, val acc: 0.4125  (best train acc: 0.4826, best val acc: 0.4863)\n",
      "[Epoch: 34240] train loss: 1.1729, train acc: 0.4628, val loss: 1.2524, val acc: 0.4024  (best train acc: 0.4826, best val acc: 0.4863)\n",
      "[Epoch: 34260] train loss: 1.1762, train acc: 0.4553, val loss: 1.2388, val acc: 0.4044  (best train acc: 0.4826, best val acc: 0.4863)\n",
      "[Epoch: 34280] train loss: 1.1722, train acc: 0.4548, val loss: 1.2294, val acc: 0.4101  (best train acc: 0.4826, best val acc: 0.4863)\n",
      "[Epoch: 34300] train loss: 1.1899, train acc: 0.4503, val loss: 1.2109, val acc: 0.4094  (best train acc: 0.4826, best val acc: 0.4863)\n",
      "[Epoch: 34320] train loss: 1.1751, train acc: 0.4532, val loss: 1.2344, val acc: 0.3997  (best train acc: 0.4826, best val acc: 0.4863)\n",
      "[Epoch: 34340] train loss: 1.1864, train acc: 0.4607, val loss: 1.2155, val acc: 0.4071  (best train acc: 0.4826, best val acc: 0.4863)\n",
      "[Epoch: 34360] train loss: 1.1781, train acc: 0.4552, val loss: 1.2590, val acc: 0.4125  (best train acc: 0.4826, best val acc: 0.4863)\n",
      "[Epoch: 34380] train loss: 1.1748, train acc: 0.4602, val loss: 1.2278, val acc: 0.4115  (best train acc: 0.4826, best val acc: 0.4863)\n",
      "[Epoch: 34400] train loss: 1.1841, train acc: 0.4585, val loss: 1.2351, val acc: 0.4064  (best train acc: 0.4826, best val acc: 0.4863)\n",
      "[Epoch: 34420] train loss: 1.1766, train acc: 0.4605, val loss: 1.2240, val acc: 0.4132  (best train acc: 0.4826, best val acc: 0.4863)\n",
      "[Epoch: 34440] train loss: 1.1749, train acc: 0.4572, val loss: 1.2775, val acc: 0.4057  (best train acc: 0.4826, best val acc: 0.4863)\n",
      "[Epoch: 34460] train loss: 1.1712, train acc: 0.4623, val loss: 1.2413, val acc: 0.4044  (best train acc: 0.4826, best val acc: 0.4863)\n",
      "[Epoch: 34480] train loss: 1.1731, train acc: 0.4599, val loss: 1.2184, val acc: 0.4091  (best train acc: 0.4826, best val acc: 0.4863)\n",
      "[Epoch: 34500] train loss: 1.1710, train acc: 0.4584, val loss: 1.2304, val acc: 0.4118  (best train acc: 0.4826, best val acc: 0.4863)\n",
      "[Epoch: 34520] train loss: 1.1654, train acc: 0.4554, val loss: 1.2343, val acc: 0.4108  (best train acc: 0.4826, best val acc: 0.4863)\n",
      "[Epoch: 34540] train loss: 1.1805, train acc: 0.4665, val loss: 1.2482, val acc: 0.3990  (best train acc: 0.4826, best val acc: 0.4863)\n",
      "[Epoch: 34560] train loss: 1.1657, train acc: 0.4605, val loss: 1.2200, val acc: 0.4101  (best train acc: 0.4826, best val acc: 0.4863)\n",
      "[Epoch: 34580] train loss: 1.1623, train acc: 0.4574, val loss: 1.2275, val acc: 0.4159  (best train acc: 0.4826, best val acc: 0.4863)\n",
      "[Epoch: 34600] train loss: 1.1683, train acc: 0.4645, val loss: 1.2366, val acc: 0.4132  (best train acc: 0.4826, best val acc: 0.4863)\n",
      "[Epoch: 34620] train loss: 1.1731, train acc: 0.4615, val loss: 1.2111, val acc: 0.4128  (best train acc: 0.4826, best val acc: 0.4863)\n",
      "[Epoch: 34640] train loss: 1.1681, train acc: 0.4548, val loss: 1.2471, val acc: 0.4027  (best train acc: 0.4826, best val acc: 0.4863)\n",
      "[Epoch: 34660] train loss: 1.1674, train acc: 0.4687, val loss: 1.2287, val acc: 0.4182  (best train acc: 0.4826, best val acc: 0.4863)\n",
      "[Epoch: 34680] train loss: 1.1745, train acc: 0.4612, val loss: 1.2353, val acc: 0.4142  (best train acc: 0.4826, best val acc: 0.4863)\n",
      "[Epoch: 34700] train loss: 1.1735, train acc: 0.4631, val loss: 1.2344, val acc: 0.4169  (best train acc: 0.4826, best val acc: 0.4863)\n",
      "[Epoch: 34720] train loss: 1.1614, train acc: 0.4671, val loss: 1.2245, val acc: 0.4084  (best train acc: 0.4826, best val acc: 0.4863)\n",
      "[Epoch: 34740] train loss: 1.1703, train acc: 0.4624, val loss: 1.2154, val acc: 0.4175  (best train acc: 0.4826, best val acc: 0.4863)\n",
      "[Epoch: 34760] train loss: 1.2064, train acc: 0.4445, val loss: 1.2885, val acc: 0.3865  (best train acc: 0.4826, best val acc: 0.4863)\n",
      "[Epoch: 34780] train loss: 1.1517, train acc: 0.4787, val loss: 1.2546, val acc: 0.4128  (best train acc: 0.4826, best val acc: 0.4863)\n",
      "[Epoch: 34800] train loss: 1.2450, train acc: 0.4382, val loss: 1.4163, val acc: 0.3659  (best train acc: 0.4826, best val acc: 0.4863)\n",
      "[Epoch: 34820] train loss: 1.2044, train acc: 0.4600, val loss: 1.3215, val acc: 0.4030  (best train acc: 0.4826, best val acc: 0.4863)\n",
      "[Epoch: 34840] train loss: 1.1715, train acc: 0.4612, val loss: 1.2257, val acc: 0.4138  (best train acc: 0.4826, best val acc: 0.4863)\n",
      "[Epoch: 34860] train loss: 1.1671, train acc: 0.4573, val loss: 1.2257, val acc: 0.4128  (best train acc: 0.4826, best val acc: 0.4863)\n",
      "[Epoch: 34880] train loss: 1.1698, train acc: 0.4619, val loss: 1.2246, val acc: 0.4118  (best train acc: 0.4826, best val acc: 0.4863)\n",
      "[Epoch: 34900] train loss: 1.1725, train acc: 0.4602, val loss: 1.2457, val acc: 0.4071  (best train acc: 0.4826, best val acc: 0.4863)\n",
      "[Epoch: 34920] train loss: 1.1741, train acc: 0.4575, val loss: 1.2678, val acc: 0.4138  (best train acc: 0.4826, best val acc: 0.4863)\n",
      "[Epoch: 34940] train loss: 1.1755, train acc: 0.4581, val loss: 1.2501, val acc: 0.4128  (best train acc: 0.4826, best val acc: 0.4863)\n",
      "[Epoch: 34960] train loss: 1.1690, train acc: 0.4563, val loss: 1.2351, val acc: 0.4071  (best train acc: 0.4826, best val acc: 0.4863)\n",
      "[Epoch: 34980] train loss: 1.1709, train acc: 0.4628, val loss: 1.2194, val acc: 0.4121  (best train acc: 0.4826, best val acc: 0.4863)\n",
      "[Epoch: 35000] train loss: 1.1599, train acc: 0.4561, val loss: 1.2249, val acc: 0.4142  (best train acc: 0.4826, best val acc: 0.4863)\n",
      "[Epoch: 35020] train loss: 1.1834, train acc: 0.4509, val loss: 1.2260, val acc: 0.4071  (best train acc: 0.4826, best val acc: 0.4863)\n",
      "[Epoch: 35040] train loss: 1.1682, train acc: 0.4592, val loss: 1.2383, val acc: 0.4105  (best train acc: 0.4826, best val acc: 0.4863)\n",
      "[Epoch: 35060] train loss: 1.1665, train acc: 0.4520, val loss: 1.2318, val acc: 0.4057  (best train acc: 0.4826, best val acc: 0.4863)\n",
      "[Epoch: 35080] train loss: 1.1592, train acc: 0.4674, val loss: 1.2167, val acc: 0.4128  (best train acc: 0.4826, best val acc: 0.4863)\n",
      "[Epoch: 35100] train loss: 1.1697, train acc: 0.4612, val loss: 1.2352, val acc: 0.4179  (best train acc: 0.4826, best val acc: 0.4863)\n",
      "[Epoch: 35120] train loss: 1.1736, train acc: 0.4619, val loss: 1.2336, val acc: 0.4064  (best train acc: 0.4826, best val acc: 0.4863)\n",
      "[Epoch: 35140] train loss: 1.1622, train acc: 0.4611, val loss: 1.2494, val acc: 0.4155  (best train acc: 0.4826, best val acc: 0.4863)\n",
      "[Epoch: 35160] train loss: 1.1653, train acc: 0.4573, val loss: 1.2176, val acc: 0.4121  (best train acc: 0.4826, best val acc: 0.4863)\n",
      "[Epoch: 35180] train loss: 1.1792, train acc: 0.4666, val loss: 1.2274, val acc: 0.4111  (best train acc: 0.4826, best val acc: 0.4863)\n",
      "[Epoch: 35200] train loss: 1.1701, train acc: 0.4519, val loss: 1.2311, val acc: 0.4145  (best train acc: 0.4826, best val acc: 0.4863)\n",
      "[Epoch: 35220] train loss: 1.1702, train acc: 0.4689, val loss: 1.2222, val acc: 0.4138  (best train acc: 0.4826, best val acc: 0.4863)\n",
      "[Epoch: 35240] train loss: 1.1640, train acc: 0.4678, val loss: 1.2308, val acc: 0.4148  (best train acc: 0.4826, best val acc: 0.4863)\n",
      "[Epoch: 35260] train loss: 1.1617, train acc: 0.4683, val loss: 1.2255, val acc: 0.4128  (best train acc: 0.4826, best val acc: 0.4863)\n",
      "[Epoch: 35280] train loss: 1.1614, train acc: 0.4654, val loss: 1.2175, val acc: 0.4159  (best train acc: 0.4826, best val acc: 0.4863)\n",
      "[Epoch: 35300] train loss: 1.1574, train acc: 0.4665, val loss: 1.2210, val acc: 0.4152  (best train acc: 0.4826, best val acc: 0.4863)\n",
      "[Epoch: 35320] train loss: 1.1632, train acc: 0.4636, val loss: 1.2277, val acc: 0.4142  (best train acc: 0.4826, best val acc: 0.4863)\n",
      "[Epoch: 35340] train loss: 1.1753, train acc: 0.4712, val loss: 1.2308, val acc: 0.4155  (best train acc: 0.4826, best val acc: 0.4863)\n",
      "[Epoch: 35360] train loss: 1.1590, train acc: 0.4631, val loss: 1.2247, val acc: 0.4074  (best train acc: 0.4826, best val acc: 0.4863)\n",
      "[Epoch: 35380] train loss: 1.1605, train acc: 0.4641, val loss: 1.2335, val acc: 0.4152  (best train acc: 0.4826, best val acc: 0.4863)\n",
      "[Epoch: 35400] train loss: 1.1732, train acc: 0.4674, val loss: 1.2244, val acc: 0.4212  (best train acc: 0.4826, best val acc: 0.4863)\n",
      "[Epoch: 35420] train loss: 1.1615, train acc: 0.4617, val loss: 1.2212, val acc: 0.4142  (best train acc: 0.4826, best val acc: 0.4863)\n",
      "[Epoch: 35440] train loss: 1.1654, train acc: 0.4571, val loss: 1.2172, val acc: 0.4179  (best train acc: 0.4826, best val acc: 0.4863)\n",
      "[Epoch: 35460] train loss: 1.1755, train acc: 0.4487, val loss: 1.2232, val acc: 0.4125  (best train acc: 0.4826, best val acc: 0.4863)\n",
      "[Epoch: 35480] train loss: 1.1636, train acc: 0.4633, val loss: 1.2565, val acc: 0.4067  (best train acc: 0.4826, best val acc: 0.4863)\n",
      "[Epoch: 35500] train loss: 1.1683, train acc: 0.4592, val loss: 1.2201, val acc: 0.4121  (best train acc: 0.4826, best val acc: 0.4863)\n",
      "[Epoch: 35520] train loss: 1.1686, train acc: 0.4524, val loss: 1.2231, val acc: 0.4142  (best train acc: 0.4826, best val acc: 0.4863)\n",
      "[Epoch: 35540] train loss: 1.1672, train acc: 0.4597, val loss: 1.2261, val acc: 0.4094  (best train acc: 0.4826, best val acc: 0.4863)\n",
      "[Epoch: 35560] train loss: 1.1684, train acc: 0.4530, val loss: 1.2374, val acc: 0.4169  (best train acc: 0.4826, best val acc: 0.4863)\n",
      "[Epoch: 35580] train loss: 1.1839, train acc: 0.4634, val loss: 1.2351, val acc: 0.4189  (best train acc: 0.4826, best val acc: 0.4863)\n",
      "[Epoch: 35600] train loss: 1.1747, train acc: 0.4678, val loss: 1.2395, val acc: 0.4108  (best train acc: 0.4826, best val acc: 0.4863)\n",
      "[Epoch: 35620] train loss: 1.1694, train acc: 0.4598, val loss: 1.2345, val acc: 0.4175  (best train acc: 0.4826, best val acc: 0.4863)\n",
      "[Epoch: 35640] train loss: 1.1673, train acc: 0.4729, val loss: 1.2433, val acc: 0.4169  (best train acc: 0.4826, best val acc: 0.4863)\n",
      "[Epoch: 35660] train loss: 1.1637, train acc: 0.4632, val loss: 1.2491, val acc: 0.4118  (best train acc: 0.4826, best val acc: 0.4863)\n",
      "[Epoch: 35680] train loss: 1.1793, train acc: 0.4675, val loss: 1.2795, val acc: 0.4034  (best train acc: 0.4826, best val acc: 0.4863)\n",
      "[Epoch: 35700] train loss: 1.1573, train acc: 0.4700, val loss: 1.2333, val acc: 0.4179  (best train acc: 0.4826, best val acc: 0.4863)\n",
      "[Epoch: 35720] train loss: 1.1599, train acc: 0.4722, val loss: 1.2228, val acc: 0.4175  (best train acc: 0.4852, best val acc: 0.4863)\n",
      "[Epoch: 35740] train loss: 1.1555, train acc: 0.4756, val loss: 1.2460, val acc: 0.4105  (best train acc: 0.4852, best val acc: 0.4863)\n",
      "[Epoch: 35760] train loss: 1.1506, train acc: 0.4793, val loss: 1.2084, val acc: 0.4145  (best train acc: 0.4852, best val acc: 0.4863)\n",
      "[Epoch: 35780] train loss: 1.1605, train acc: 0.4725, val loss: 1.2524, val acc: 0.4091  (best train acc: 0.4852, best val acc: 0.4863)\n",
      "[Epoch: 35800] train loss: 1.1973, train acc: 0.4569, val loss: 1.2376, val acc: 0.4078  (best train acc: 0.4852, best val acc: 0.4863)\n",
      "[Epoch: 35820] train loss: 1.1825, train acc: 0.4693, val loss: 1.2621, val acc: 0.4128  (best train acc: 0.4852, best val acc: 0.4863)\n",
      "[Epoch: 35840] train loss: 1.1560, train acc: 0.4746, val loss: 1.2415, val acc: 0.4199  (best train acc: 0.4852, best val acc: 0.4863)\n",
      "[Epoch: 35860] train loss: 1.1520, train acc: 0.4756, val loss: 1.2320, val acc: 0.4138  (best train acc: 0.4852, best val acc: 0.4863)\n",
      "[Epoch: 35880] train loss: 1.1458, train acc: 0.4773, val loss: 1.2358, val acc: 0.4132  (best train acc: 0.4852, best val acc: 0.4863)\n",
      "[Epoch: 35900] train loss: 1.1387, train acc: 0.4789, val loss: 1.1881, val acc: 0.4300  (best train acc: 0.4853, best val acc: 0.4863)\n",
      "[Epoch: 35920] train loss: 1.2642, train acc: 0.4247, val loss: 1.2636, val acc: 0.4000  (best train acc: 0.4853, best val acc: 0.4863)\n",
      "[Epoch: 35940] train loss: 1.1794, train acc: 0.4712, val loss: 1.3068, val acc: 0.4051  (best train acc: 0.4853, best val acc: 0.4863)\n",
      "[Epoch: 35960] train loss: 1.1710, train acc: 0.4592, val loss: 1.2691, val acc: 0.4105  (best train acc: 0.4853, best val acc: 0.4863)\n",
      "[Epoch: 35980] train loss: 1.1614, train acc: 0.4683, val loss: 1.2714, val acc: 0.4061  (best train acc: 0.4853, best val acc: 0.4863)\n",
      "[Epoch: 36000] train loss: 1.1633, train acc: 0.4641, val loss: 1.2347, val acc: 0.4165  (best train acc: 0.4853, best val acc: 0.4863)\n",
      "[Epoch: 36020] train loss: 1.1598, train acc: 0.4641, val loss: 1.2218, val acc: 0.4219  (best train acc: 0.4853, best val acc: 0.4863)\n",
      "[Epoch: 36040] train loss: 1.1633, train acc: 0.4623, val loss: 1.2335, val acc: 0.4135  (best train acc: 0.4853, best val acc: 0.4863)\n",
      "[Epoch: 36060] train loss: 1.1649, train acc: 0.4617, val loss: 1.2218, val acc: 0.4189  (best train acc: 0.4853, best val acc: 0.4863)\n",
      "[Epoch: 36080] train loss: 1.1645, train acc: 0.4703, val loss: 1.2668, val acc: 0.4074  (best train acc: 0.4853, best val acc: 0.4863)\n",
      "[Epoch: 36100] train loss: 1.1669, train acc: 0.4607, val loss: 1.2267, val acc: 0.4162  (best train acc: 0.4853, best val acc: 0.4863)\n",
      "[Epoch: 36120] train loss: 1.1572, train acc: 0.4678, val loss: 1.2660, val acc: 0.4088  (best train acc: 0.4853, best val acc: 0.4863)\n",
      "[Epoch: 36140] train loss: 1.1556, train acc: 0.4740, val loss: 1.2408, val acc: 0.4159  (best train acc: 0.4853, best val acc: 0.4863)\n",
      "[Epoch: 36160] train loss: 1.1588, train acc: 0.4680, val loss: 1.2309, val acc: 0.4135  (best train acc: 0.4853, best val acc: 0.4863)\n",
      "[Epoch: 36180] train loss: 1.1821, train acc: 0.4536, val loss: 1.5053, val acc: 0.3690  (best train acc: 0.4853, best val acc: 0.4863)\n",
      "[Epoch: 36200] train loss: 1.1691, train acc: 0.4631, val loss: 1.2779, val acc: 0.4044  (best train acc: 0.4853, best val acc: 0.4863)\n",
      "[Epoch: 36220] train loss: 1.1580, train acc: 0.4694, val loss: 1.2292, val acc: 0.4192  (best train acc: 0.4853, best val acc: 0.4863)\n",
      "[Epoch: 36240] train loss: 1.1520, train acc: 0.4777, val loss: 1.2176, val acc: 0.4138  (best train acc: 0.4853, best val acc: 0.4863)\n",
      "[Epoch: 36260] train loss: 1.1475, train acc: 0.4735, val loss: 1.2118, val acc: 0.4243  (best train acc: 0.4853, best val acc: 0.4863)\n",
      "[Epoch: 36280] train loss: 1.1427, train acc: 0.4873, val loss: 1.2439, val acc: 0.4057  (best train acc: 0.4873, best val acc: 0.4863)\n",
      "[Epoch: 36300] train loss: 1.1413, train acc: 0.4788, val loss: 1.2207, val acc: 0.4142  (best train acc: 0.4914, best val acc: 0.4863)\n",
      "[Epoch: 36320] train loss: 1.1644, train acc: 0.4684, val loss: 1.2906, val acc: 0.4175  (best train acc: 0.4914, best val acc: 0.4863)\n",
      "[Epoch: 36340] train loss: 1.1554, train acc: 0.4774, val loss: 1.2140, val acc: 0.4260  (best train acc: 0.4914, best val acc: 0.4863)\n",
      "[Epoch: 36360] train loss: 1.1501, train acc: 0.4800, val loss: 1.2938, val acc: 0.4084  (best train acc: 0.4914, best val acc: 0.4863)\n",
      "[Epoch: 36380] train loss: 1.1537, train acc: 0.4726, val loss: 1.2361, val acc: 0.4277  (best train acc: 0.4933, best val acc: 0.4863)\n",
      "[Epoch: 36400] train loss: 1.2146, train acc: 0.4505, val loss: 1.3246, val acc: 0.3966  (best train acc: 0.4933, best val acc: 0.4863)\n",
      "[Epoch: 36420] train loss: 1.1726, train acc: 0.4672, val loss: 1.2663, val acc: 0.4132  (best train acc: 0.4933, best val acc: 0.4863)\n",
      "[Epoch: 36440] train loss: 1.1659, train acc: 0.4580, val loss: 1.2404, val acc: 0.4115  (best train acc: 0.4933, best val acc: 0.4863)\n",
      "[Epoch: 36460] train loss: 1.1625, train acc: 0.4623, val loss: 1.2407, val acc: 0.4111  (best train acc: 0.4933, best val acc: 0.4863)\n",
      "[Epoch: 36480] train loss: 1.1759, train acc: 0.4743, val loss: 1.2756, val acc: 0.4081  (best train acc: 0.4933, best val acc: 0.4863)\n",
      "[Epoch: 36500] train loss: 1.1595, train acc: 0.4662, val loss: 1.2322, val acc: 0.4132  (best train acc: 0.4933, best val acc: 0.4863)\n",
      "[Epoch: 36520] train loss: 1.1668, train acc: 0.4604, val loss: 1.2351, val acc: 0.4125  (best train acc: 0.4933, best val acc: 0.4863)\n",
      "[Epoch: 36540] train loss: 1.1708, train acc: 0.4586, val loss: 1.2221, val acc: 0.4165  (best train acc: 0.4933, best val acc: 0.4863)\n",
      "[Epoch: 36560] train loss: 1.1535, train acc: 0.4700, val loss: 1.2481, val acc: 0.4132  (best train acc: 0.4933, best val acc: 0.4863)\n",
      "[Epoch: 36580] train loss: 1.1871, train acc: 0.4418, val loss: 1.2639, val acc: 0.4078  (best train acc: 0.4933, best val acc: 0.4863)\n",
      "[Epoch: 36600] train loss: 1.1603, train acc: 0.4734, val loss: 1.2079, val acc: 0.4185  (best train acc: 0.4933, best val acc: 0.4863)\n",
      "[Epoch: 36620] train loss: 1.1388, train acc: 0.4758, val loss: 1.2062, val acc: 0.4233  (best train acc: 0.4933, best val acc: 0.4863)\n",
      "[Epoch: 36640] train loss: 1.1944, train acc: 0.4539, val loss: 1.1882, val acc: 0.4229  (best train acc: 0.4933, best val acc: 0.4863)\n",
      "[Epoch: 36660] train loss: 1.1553, train acc: 0.4746, val loss: 1.2021, val acc: 0.4189  (best train acc: 0.4933, best val acc: 0.4863)\n",
      "[Epoch: 36680] train loss: 1.1685, train acc: 0.4633, val loss: 1.2747, val acc: 0.4138  (best train acc: 0.4933, best val acc: 0.4863)\n",
      "[Epoch: 36700] train loss: 1.1572, train acc: 0.4672, val loss: 1.2233, val acc: 0.4216  (best train acc: 0.4933, best val acc: 0.4863)\n",
      "[Epoch: 36720] train loss: 1.1473, train acc: 0.4802, val loss: 1.2071, val acc: 0.4320  (best train acc: 0.4933, best val acc: 0.4863)\n",
      "[Epoch: 36740] train loss: 1.1469, train acc: 0.4796, val loss: 1.2158, val acc: 0.4324  (best train acc: 0.4933, best val acc: 0.4863)\n",
      "[Epoch: 36760] train loss: 1.2073, train acc: 0.4395, val loss: 1.4952, val acc: 0.3744  (best train acc: 0.4933, best val acc: 0.4863)\n",
      "[Epoch: 36780] train loss: 1.2074, train acc: 0.4440, val loss: 1.2591, val acc: 0.4074  (best train acc: 0.4933, best val acc: 0.4863)\n",
      "[Epoch: 36800] train loss: 1.1683, train acc: 0.4606, val loss: 1.2515, val acc: 0.4192  (best train acc: 0.4933, best val acc: 0.4863)\n",
      "[Epoch: 36820] train loss: 1.1438, train acc: 0.4770, val loss: 1.1982, val acc: 0.4219  (best train acc: 0.4933, best val acc: 0.4863)\n",
      "[Epoch: 36840] train loss: 1.1653, train acc: 0.4735, val loss: 1.2773, val acc: 0.3966  (best train acc: 0.4933, best val acc: 0.4863)\n",
      "[Epoch: 36860] train loss: 1.1399, train acc: 0.4856, val loss: 1.2003, val acc: 0.4199  (best train acc: 0.4933, best val acc: 0.4863)\n",
      "[Epoch: 36880] train loss: 1.2268, train acc: 0.4581, val loss: 1.5881, val acc: 0.3784  (best train acc: 0.4933, best val acc: 0.4863)\n",
      "[Epoch: 36900] train loss: 1.1522, train acc: 0.4668, val loss: 1.2783, val acc: 0.4125  (best train acc: 0.4933, best val acc: 0.4863)\n",
      "[Epoch: 36920] train loss: 1.1673, train acc: 0.4694, val loss: 1.3511, val acc: 0.4007  (best train acc: 0.4933, best val acc: 0.4863)\n",
      "[Epoch: 36940] train loss: 1.1669, train acc: 0.4787, val loss: 1.2136, val acc: 0.4226  (best train acc: 0.4933, best val acc: 0.4863)\n",
      "[Epoch: 36960] train loss: 1.1957, train acc: 0.4487, val loss: 1.3448, val acc: 0.3980  (best train acc: 0.4933, best val acc: 0.4863)\n",
      "[Epoch: 36980] train loss: 1.1592, train acc: 0.4659, val loss: 1.2474, val acc: 0.4091  (best train acc: 0.4933, best val acc: 0.4863)\n",
      "[Epoch: 37000] train loss: 1.1863, train acc: 0.4610, val loss: 1.3346, val acc: 0.4061  (best train acc: 0.4933, best val acc: 0.4863)\n",
      "[Epoch: 37020] train loss: 1.1472, train acc: 0.4757, val loss: 1.2472, val acc: 0.4239  (best train acc: 0.4933, best val acc: 0.4863)\n",
      "[Epoch: 37040] train loss: 1.1896, train acc: 0.4598, val loss: 1.2643, val acc: 0.4125  (best train acc: 0.4933, best val acc: 0.4863)\n",
      "[Epoch: 37060] train loss: 1.1389, train acc: 0.4816, val loss: 1.1904, val acc: 0.4337  (best train acc: 0.4933, best val acc: 0.4863)\n",
      "[Epoch: 37080] train loss: 1.1739, train acc: 0.4784, val loss: 1.2850, val acc: 0.4084  (best train acc: 0.4933, best val acc: 0.4863)\n",
      "[Epoch: 37100] train loss: 1.1903, train acc: 0.4748, val loss: 1.3050, val acc: 0.4101  (best train acc: 0.4933, best val acc: 0.4863)\n",
      "[Epoch: 37120] train loss: 1.1417, train acc: 0.4841, val loss: 1.2669, val acc: 0.4152  (best train acc: 0.4933, best val acc: 0.4863)\n",
      "[Epoch: 37140] train loss: 1.1515, train acc: 0.4709, val loss: 1.2364, val acc: 0.4179  (best train acc: 0.4933, best val acc: 0.4863)\n",
      "[Epoch: 37160] train loss: 1.2351, train acc: 0.4445, val loss: 1.3188, val acc: 0.4216  (best train acc: 0.4933, best val acc: 0.4863)\n",
      "[Epoch: 37180] train loss: 1.1713, train acc: 0.4557, val loss: 1.2643, val acc: 0.4115  (best train acc: 0.4933, best val acc: 0.4863)\n",
      "[Epoch: 37200] train loss: 1.1442, train acc: 0.4845, val loss: 1.2512, val acc: 0.4179  (best train acc: 0.4933, best val acc: 0.4863)\n",
      "[Epoch: 37220] train loss: 1.1502, train acc: 0.4727, val loss: 1.2629, val acc: 0.4125  (best train acc: 0.4933, best val acc: 0.4863)\n",
      "[Epoch: 37240] train loss: 1.1379, train acc: 0.4840, val loss: 1.2812, val acc: 0.4125  (best train acc: 0.4933, best val acc: 0.4863)\n",
      "[Epoch: 37260] train loss: 1.2973, train acc: 0.4505, val loss: 1.2817, val acc: 0.4270  (best train acc: 0.4933, best val acc: 0.4863)\n",
      "[Epoch: 37280] train loss: 1.1630, train acc: 0.4670, val loss: 1.2713, val acc: 0.4108  (best train acc: 0.4933, best val acc: 0.4863)\n",
      "[Epoch: 37300] train loss: 1.1590, train acc: 0.4780, val loss: 1.2456, val acc: 0.4175  (best train acc: 0.4933, best val acc: 0.4863)\n",
      "[Epoch: 37320] train loss: 1.1375, train acc: 0.4852, val loss: 1.2314, val acc: 0.4169  (best train acc: 0.4933, best val acc: 0.4863)\n",
      "[Epoch: 37340] train loss: 1.1347, train acc: 0.4753, val loss: 1.1990, val acc: 0.4324  (best train acc: 0.4933, best val acc: 0.4863)\n",
      "[Epoch: 37360] train loss: 1.2240, train acc: 0.4385, val loss: 1.4845, val acc: 0.3801  (best train acc: 0.4933, best val acc: 0.4863)\n",
      "[Epoch: 37380] train loss: 1.1579, train acc: 0.4765, val loss: 1.2491, val acc: 0.4219  (best train acc: 0.4933, best val acc: 0.4863)\n",
      "[Epoch: 37400] train loss: 1.1515, train acc: 0.4750, val loss: 1.2660, val acc: 0.4125  (best train acc: 0.4933, best val acc: 0.4863)\n",
      "[Epoch: 37420] train loss: 1.1829, train acc: 0.4540, val loss: 1.3277, val acc: 0.4013  (best train acc: 0.4933, best val acc: 0.4863)\n",
      "[Epoch: 37440] train loss: 1.1500, train acc: 0.4777, val loss: 1.2273, val acc: 0.4246  (best train acc: 0.4933, best val acc: 0.4863)\n",
      "[Epoch: 37460] train loss: 1.1347, train acc: 0.4820, val loss: 1.2830, val acc: 0.4162  (best train acc: 0.4933, best val acc: 0.4863)\n",
      "[Epoch: 37480] train loss: 1.1471, train acc: 0.4776, val loss: 1.1921, val acc: 0.4307  (best train acc: 0.4933, best val acc: 0.4863)\n",
      "[Epoch: 37500] train loss: 1.1678, train acc: 0.4837, val loss: 1.3290, val acc: 0.4074  (best train acc: 0.4933, best val acc: 0.4863)\n",
      "[Epoch: 37520] train loss: 1.1574, train acc: 0.4803, val loss: 1.2025, val acc: 0.4239  (best train acc: 0.4933, best val acc: 0.4863)\n",
      "[Epoch: 37540] train loss: 1.1885, train acc: 0.4782, val loss: 1.2868, val acc: 0.4067  (best train acc: 0.4933, best val acc: 0.4863)\n",
      "[Epoch: 37560] train loss: 1.1667, train acc: 0.4696, val loss: 1.3649, val acc: 0.3973  (best train acc: 0.4933, best val acc: 0.4863)\n",
      "[Epoch: 37580] train loss: 1.1513, train acc: 0.4747, val loss: 1.2355, val acc: 0.4212  (best train acc: 0.4933, best val acc: 0.4863)\n",
      "[Epoch: 37600] train loss: 1.2119, train acc: 0.4671, val loss: 1.3367, val acc: 0.3980  (best train acc: 0.4933, best val acc: 0.4863)\n",
      "[Epoch: 37620] train loss: 1.1595, train acc: 0.4732, val loss: 1.2619, val acc: 0.4155  (best train acc: 0.4933, best val acc: 0.4863)\n",
      "[Epoch: 37640] train loss: 1.1387, train acc: 0.4853, val loss: 1.2624, val acc: 0.4165  (best train acc: 0.4933, best val acc: 0.4863)\n",
      "[Epoch: 37660] train loss: 1.1601, train acc: 0.4667, val loss: 1.2494, val acc: 0.4229  (best train acc: 0.4933, best val acc: 0.4863)\n",
      "[Epoch: 37680] train loss: 1.1482, train acc: 0.4730, val loss: 1.2194, val acc: 0.4192  (best train acc: 0.4933, best val acc: 0.4863)\n",
      "[Epoch: 37700] train loss: 1.1426, train acc: 0.4788, val loss: 1.2234, val acc: 0.4219  (best train acc: 0.4933, best val acc: 0.4863)\n",
      "[Epoch: 37720] train loss: 1.1577, train acc: 0.4799, val loss: 1.3030, val acc: 0.4118  (best train acc: 0.4933, best val acc: 0.4863)\n",
      "[Epoch: 37740] train loss: 1.1780, train acc: 0.4775, val loss: 1.2422, val acc: 0.4138  (best train acc: 0.4933, best val acc: 0.4863)\n",
      "[Epoch: 37760] train loss: 1.1375, train acc: 0.4819, val loss: 1.2077, val acc: 0.4253  (best train acc: 0.4933, best val acc: 0.4863)\n",
      "[Epoch: 37780] train loss: 1.1762, train acc: 0.4719, val loss: 1.2525, val acc: 0.4074  (best train acc: 0.4933, best val acc: 0.4863)\n",
      "[Epoch: 37800] train loss: 1.1390, train acc: 0.4847, val loss: 1.2404, val acc: 0.4185  (best train acc: 0.4933, best val acc: 0.4863)\n",
      "[Epoch: 37820] train loss: 1.1705, train acc: 0.4555, val loss: 1.2972, val acc: 0.4081  (best train acc: 0.4933, best val acc: 0.4863)\n",
      "[Epoch: 37840] train loss: 1.1184, train acc: 0.4918, val loss: 1.2529, val acc: 0.4148  (best train acc: 0.4933, best val acc: 0.4863)\n",
      "[Epoch: 37860] train loss: 1.1381, train acc: 0.4806, val loss: 1.3393, val acc: 0.4017  (best train acc: 0.4933, best val acc: 0.4863)\n",
      "[Epoch: 37880] train loss: 1.2642, train acc: 0.4594, val loss: 1.6523, val acc: 0.3781  (best train acc: 0.4933, best val acc: 0.4863)\n",
      "[Epoch: 37900] train loss: 1.1558, train acc: 0.4696, val loss: 1.3378, val acc: 0.4081  (best train acc: 0.4933, best val acc: 0.4863)\n",
      "[Epoch: 37920] train loss: 1.1581, train acc: 0.4675, val loss: 1.2783, val acc: 0.4172  (best train acc: 0.4933, best val acc: 0.4863)\n",
      "[Epoch: 37940] train loss: 1.1764, train acc: 0.4507, val loss: 1.2832, val acc: 0.4091  (best train acc: 0.4933, best val acc: 0.4863)\n",
      "[Epoch: 37960] train loss: 1.2845, train acc: 0.4796, val loss: 1.6222, val acc: 0.3609  (best train acc: 0.4933, best val acc: 0.4863)\n",
      "[Epoch: 37980] train loss: 1.1831, train acc: 0.4617, val loss: 1.2806, val acc: 0.4239  (best train acc: 0.4933, best val acc: 0.4863)\n",
      "[Epoch: 38000] train loss: 1.1534, train acc: 0.4733, val loss: 1.2601, val acc: 0.4179  (best train acc: 0.4933, best val acc: 0.4863)\n",
      "[Epoch: 38020] train loss: 1.1415, train acc: 0.4787, val loss: 1.2328, val acc: 0.4185  (best train acc: 0.4933, best val acc: 0.4863)\n",
      "[Epoch: 38040] train loss: 1.2005, train acc: 0.4609, val loss: 1.3293, val acc: 0.4088  (best train acc: 0.4933, best val acc: 0.4863)\n",
      "[Epoch: 38060] train loss: 1.1526, train acc: 0.4770, val loss: 1.2735, val acc: 0.4175  (best train acc: 0.4933, best val acc: 0.4863)\n",
      "[Epoch: 38080] train loss: 1.1348, train acc: 0.4752, val loss: 1.2117, val acc: 0.4196  (best train acc: 0.4933, best val acc: 0.4863)\n",
      "[Epoch: 38100] train loss: 1.1704, train acc: 0.4547, val loss: 1.2401, val acc: 0.4111  (best train acc: 0.4933, best val acc: 0.4863)\n",
      "[Epoch: 38120] train loss: 1.1468, train acc: 0.4811, val loss: 1.2371, val acc: 0.4169  (best train acc: 0.4933, best val acc: 0.4863)\n",
      "[Epoch: 38140] train loss: 1.1458, train acc: 0.4779, val loss: 1.2579, val acc: 0.4138  (best train acc: 0.4933, best val acc: 0.4863)\n",
      "[Epoch: 38160] train loss: 1.1882, train acc: 0.4560, val loss: 1.3221, val acc: 0.4108  (best train acc: 0.4933, best val acc: 0.4863)\n",
      "[Epoch: 38180] train loss: 1.1574, train acc: 0.4773, val loss: 1.2672, val acc: 0.4111  (best train acc: 0.4933, best val acc: 0.4863)\n",
      "[Epoch: 38200] train loss: 1.2055, train acc: 0.4646, val loss: 1.3583, val acc: 0.4030  (best train acc: 0.4936, best val acc: 0.4863)\n",
      "[Epoch: 38220] train loss: 1.1569, train acc: 0.4800, val loss: 1.3301, val acc: 0.4061  (best train acc: 0.4936, best val acc: 0.4863)\n",
      "[Epoch: 38240] train loss: 1.1286, train acc: 0.4805, val loss: 1.1872, val acc: 0.4384  (best train acc: 0.4936, best val acc: 0.4863)\n",
      "[Epoch: 38260] train loss: 1.1527, train acc: 0.4765, val loss: 1.2625, val acc: 0.4051  (best train acc: 0.4936, best val acc: 0.4863)\n",
      "[Epoch: 38280] train loss: 1.1614, train acc: 0.4790, val loss: 1.2898, val acc: 0.4169  (best train acc: 0.4964, best val acc: 0.4863)\n",
      "[Epoch: 38300] train loss: 1.1488, train acc: 0.4853, val loss: 1.2191, val acc: 0.4250  (best train acc: 0.4964, best val acc: 0.4863)\n",
      "[Epoch: 38320] train loss: 1.1623, train acc: 0.4756, val loss: 1.2506, val acc: 0.4148  (best train acc: 0.4964, best val acc: 0.4863)\n",
      "[Epoch: 38340] train loss: 1.1400, train acc: 0.4685, val loss: 1.2421, val acc: 0.4283  (best train acc: 0.4964, best val acc: 0.4863)\n",
      "[Epoch: 38360] train loss: 1.1961, train acc: 0.4610, val loss: 1.3909, val acc: 0.4057  (best train acc: 0.4964, best val acc: 0.4863)\n",
      "[Epoch: 38380] train loss: 1.1469, train acc: 0.4717, val loss: 1.2434, val acc: 0.4482  (best train acc: 0.4964, best val acc: 0.4863)\n",
      "[Epoch: 38400] train loss: 1.1374, train acc: 0.4798, val loss: 1.2553, val acc: 0.4162  (best train acc: 0.4964, best val acc: 0.4863)\n",
      "[Epoch: 38420] train loss: 1.1553, train acc: 0.4713, val loss: 1.3283, val acc: 0.3980  (best train acc: 0.4964, best val acc: 0.4863)\n",
      "[Epoch: 38440] train loss: 1.1490, train acc: 0.4691, val loss: 1.2585, val acc: 0.4125  (best train acc: 0.4964, best val acc: 0.4863)\n",
      "[Epoch: 38460] train loss: 1.1295, train acc: 0.4913, val loss: 1.1961, val acc: 0.4415  (best train acc: 0.4964, best val acc: 0.4863)\n",
      "[Epoch: 38480] train loss: 1.1854, train acc: 0.4686, val loss: 1.4128, val acc: 0.3902  (best train acc: 0.4964, best val acc: 0.4863)\n",
      "[Epoch: 38500] train loss: 1.1569, train acc: 0.4800, val loss: 1.2961, val acc: 0.4121  (best train acc: 0.4964, best val acc: 0.4863)\n",
      "[Epoch: 38520] train loss: 1.1362, train acc: 0.4844, val loss: 1.2129, val acc: 0.4239  (best train acc: 0.4964, best val acc: 0.4863)\n",
      "[Epoch: 38540] train loss: 1.1667, train acc: 0.4629, val loss: 1.2622, val acc: 0.3960  (best train acc: 0.4964, best val acc: 0.4863)\n",
      "[Epoch: 38560] train loss: 1.1313, train acc: 0.4752, val loss: 1.1870, val acc: 0.4374  (best train acc: 0.4964, best val acc: 0.4863)\n",
      "[Epoch: 38580] train loss: 1.1775, train acc: 0.4634, val loss: 1.2953, val acc: 0.4084  (best train acc: 0.4964, best val acc: 0.4863)\n",
      "[Epoch: 38600] train loss: 1.1675, train acc: 0.4678, val loss: 1.3330, val acc: 0.4020  (best train acc: 0.4964, best val acc: 0.4863)\n",
      "[Epoch: 38620] train loss: 1.1570, train acc: 0.4684, val loss: 1.3083, val acc: 0.4115  (best train acc: 0.4964, best val acc: 0.4863)\n",
      "[Epoch: 38640] train loss: 1.1743, train acc: 0.4700, val loss: 1.2608, val acc: 0.4192  (best train acc: 0.4964, best val acc: 0.4863)\n",
      "[Epoch: 38660] train loss: 1.1323, train acc: 0.4871, val loss: 1.2302, val acc: 0.4179  (best train acc: 0.4964, best val acc: 0.4863)\n",
      "[Epoch: 38680] train loss: 1.2206, train acc: 0.4280, val loss: 1.4382, val acc: 0.3754  (best train acc: 0.5012, best val acc: 0.4863)\n",
      "[Epoch: 38700] train loss: 1.1634, train acc: 0.4676, val loss: 1.2629, val acc: 0.4125  (best train acc: 0.5012, best val acc: 0.4863)\n",
      "[Epoch: 38720] train loss: 1.1250, train acc: 0.4840, val loss: 1.2045, val acc: 0.4374  (best train acc: 0.5012, best val acc: 0.4863)\n",
      "[Epoch: 38740] train loss: 1.1657, train acc: 0.4735, val loss: 1.3227, val acc: 0.3980  (best train acc: 0.5012, best val acc: 0.4863)\n",
      "[Epoch: 38760] train loss: 1.1467, train acc: 0.4797, val loss: 1.2702, val acc: 0.4199  (best train acc: 0.5012, best val acc: 0.4863)\n",
      "[Epoch: 38780] train loss: 1.2620, train acc: 0.4560, val loss: 1.3315, val acc: 0.4020  (best train acc: 0.5012, best val acc: 0.4863)\n",
      "[Epoch: 38800] train loss: 1.1590, train acc: 0.4696, val loss: 1.1955, val acc: 0.4277  (best train acc: 0.5012, best val acc: 0.4863)\n",
      "[Epoch: 38820] train loss: 1.1387, train acc: 0.4738, val loss: 1.2859, val acc: 0.4027  (best train acc: 0.5012, best val acc: 0.4863)\n",
      "[Epoch: 38840] train loss: 1.1399, train acc: 0.4836, val loss: 1.2667, val acc: 0.4132  (best train acc: 0.5012, best val acc: 0.4863)\n",
      "[Epoch: 38860] train loss: 1.1512, train acc: 0.4858, val loss: 1.2946, val acc: 0.4111  (best train acc: 0.5012, best val acc: 0.4863)\n",
      "[Epoch: 38880] train loss: 1.1222, train acc: 0.4803, val loss: 1.2058, val acc: 0.4411  (best train acc: 0.5012, best val acc: 0.4863)\n",
      "[Epoch: 38900] train loss: 1.1691, train acc: 0.4720, val loss: 1.3579, val acc: 0.3963  (best train acc: 0.5012, best val acc: 0.4863)\n",
      "[Epoch: 38920] train loss: 1.1353, train acc: 0.4833, val loss: 1.2322, val acc: 0.4226  (best train acc: 0.5012, best val acc: 0.4863)\n",
      "[Epoch: 38940] train loss: 1.1441, train acc: 0.4922, val loss: 1.2526, val acc: 0.4145  (best train acc: 0.5012, best val acc: 0.4863)\n",
      "[Epoch: 38960] train loss: 1.1163, train acc: 0.4852, val loss: 1.2089, val acc: 0.4202  (best train acc: 0.5012, best val acc: 0.4863)\n",
      "[Epoch: 38980] train loss: 1.1543, train acc: 0.4657, val loss: 1.2363, val acc: 0.4081  (best train acc: 0.5012, best val acc: 0.4863)\n",
      "[Epoch: 39000] train loss: 1.1529, train acc: 0.4855, val loss: 1.2923, val acc: 0.4111  (best train acc: 0.5012, best val acc: 0.4863)\n",
      "[Epoch: 39020] train loss: 1.1549, train acc: 0.4793, val loss: 1.2143, val acc: 0.4327  (best train acc: 0.5012, best val acc: 0.4863)\n",
      "[Epoch: 39040] train loss: 1.1311, train acc: 0.4818, val loss: 1.2199, val acc: 0.4300  (best train acc: 0.5012, best val acc: 0.4863)\n",
      "[Epoch: 39060] train loss: 1.1195, train acc: 0.4858, val loss: 1.2040, val acc: 0.4384  (best train acc: 0.5012, best val acc: 0.4863)\n",
      "[Epoch: 39080] train loss: 1.1960, train acc: 0.4521, val loss: 1.4066, val acc: 0.3973  (best train acc: 0.5012, best val acc: 0.4863)\n",
      "[Epoch: 39100] train loss: 1.1351, train acc: 0.4806, val loss: 1.2460, val acc: 0.4172  (best train acc: 0.5012, best val acc: 0.4863)\n",
      "[Epoch: 39120] train loss: 1.1292, train acc: 0.4850, val loss: 1.2238, val acc: 0.4223  (best train acc: 0.5012, best val acc: 0.4863)\n",
      "[Epoch: 39140] train loss: 1.1692, train acc: 0.4627, val loss: 1.3860, val acc: 0.4010  (best train acc: 0.5012, best val acc: 0.4863)\n",
      "[Epoch: 39160] train loss: 1.1492, train acc: 0.4766, val loss: 1.2810, val acc: 0.4098  (best train acc: 0.5012, best val acc: 0.4863)\n",
      "[Epoch: 39180] train loss: 1.1638, train acc: 0.4769, val loss: 1.2418, val acc: 0.4165  (best train acc: 0.5012, best val acc: 0.4863)\n",
      "[Epoch: 39200] train loss: 1.3870, train acc: 0.4469, val loss: 1.6761, val acc: 0.4020  (best train acc: 0.5012, best val acc: 0.4863)\n",
      "[Epoch: 39220] train loss: 1.1657, train acc: 0.4642, val loss: 1.3225, val acc: 0.4030  (best train acc: 0.5012, best val acc: 0.4863)\n",
      "[Epoch: 39240] train loss: 1.1370, train acc: 0.4812, val loss: 1.2085, val acc: 0.4280  (best train acc: 0.5012, best val acc: 0.4863)\n",
      "[Epoch: 39260] train loss: 1.1860, train acc: 0.4482, val loss: 1.2718, val acc: 0.4125  (best train acc: 0.5012, best val acc: 0.4863)\n",
      "[Epoch: 39280] train loss: 1.1397, train acc: 0.4770, val loss: 1.2198, val acc: 0.4226  (best train acc: 0.5012, best val acc: 0.4863)\n",
      "[Epoch: 39300] train loss: 1.1154, train acc: 0.4900, val loss: 1.2269, val acc: 0.4320  (best train acc: 0.5012, best val acc: 0.4863)\n",
      "[Epoch: 39320] train loss: 1.1561, train acc: 0.4829, val loss: 1.2524, val acc: 0.4145  (best train acc: 0.5012, best val acc: 0.4863)\n",
      "[Epoch: 39340] train loss: 1.1301, train acc: 0.4811, val loss: 1.2238, val acc: 0.4169  (best train acc: 0.5012, best val acc: 0.4863)\n",
      "[Epoch: 39360] train loss: 1.1194, train acc: 0.4782, val loss: 1.2694, val acc: 0.4165  (best train acc: 0.5012, best val acc: 0.4863)\n",
      "[Epoch: 39380] train loss: 1.1401, train acc: 0.4747, val loss: 1.2442, val acc: 0.4175  (best train acc: 0.5012, best val acc: 0.4863)\n",
      "[Epoch: 39400] train loss: 1.2324, train acc: 0.4632, val loss: 1.4275, val acc: 0.3956  (best train acc: 0.5012, best val acc: 0.4863)\n",
      "[Epoch: 39420] train loss: 1.1700, train acc: 0.4702, val loss: 1.2159, val acc: 0.4216  (best train acc: 0.5012, best val acc: 0.4863)\n",
      "[Epoch: 39440] train loss: 1.1241, train acc: 0.4926, val loss: 1.2228, val acc: 0.4206  (best train acc: 0.5012, best val acc: 0.4863)\n",
      "[Epoch: 39460] train loss: 1.1874, train acc: 0.4506, val loss: 1.2269, val acc: 0.4199  (best train acc: 0.5012, best val acc: 0.4863)\n",
      "[Epoch: 39480] train loss: 1.1498, train acc: 0.4715, val loss: 1.2079, val acc: 0.4219  (best train acc: 0.5012, best val acc: 0.4863)\n",
      "[Epoch: 39500] train loss: 1.1360, train acc: 0.4861, val loss: 1.2175, val acc: 0.4185  (best train acc: 0.5012, best val acc: 0.4863)\n",
      "[Epoch: 39520] train loss: 1.1462, train acc: 0.4809, val loss: 1.2144, val acc: 0.4148  (best train acc: 0.5012, best val acc: 0.4863)\n",
      "[Epoch: 39540] train loss: 1.1962, train acc: 0.4521, val loss: 1.2901, val acc: 0.4108  (best train acc: 0.5012, best val acc: 0.4863)\n",
      "[Epoch: 39560] train loss: 1.1554, train acc: 0.4701, val loss: 1.2406, val acc: 0.4152  (best train acc: 0.5012, best val acc: 0.4863)\n",
      "[Epoch: 39580] train loss: 1.1431, train acc: 0.4751, val loss: 1.2360, val acc: 0.4108  (best train acc: 0.5012, best val acc: 0.4863)\n",
      "[Epoch: 39600] train loss: 1.1349, train acc: 0.4782, val loss: 1.2440, val acc: 0.4091  (best train acc: 0.5012, best val acc: 0.4863)\n",
      "[Epoch: 39620] train loss: 1.1229, train acc: 0.4850, val loss: 1.2085, val acc: 0.4155  (best train acc: 0.5012, best val acc: 0.4863)\n",
      "[Epoch: 39640] train loss: 1.2077, train acc: 0.4691, val loss: 1.2951, val acc: 0.4088  (best train acc: 0.5012, best val acc: 0.4863)\n",
      "[Epoch: 39660] train loss: 1.1278, train acc: 0.4843, val loss: 1.1906, val acc: 0.4324  (best train acc: 0.5012, best val acc: 0.4863)\n",
      "[Epoch: 39680] train loss: 1.1783, train acc: 0.4529, val loss: 1.3516, val acc: 0.4071  (best train acc: 0.5012, best val acc: 0.4863)\n",
      "[Epoch: 39700] train loss: 1.1532, train acc: 0.4804, val loss: 1.2219, val acc: 0.4196  (best train acc: 0.5012, best val acc: 0.4863)\n",
      "[Epoch: 39720] train loss: 1.1409, train acc: 0.4772, val loss: 1.1976, val acc: 0.4368  (best train acc: 0.5012, best val acc: 0.4863)\n",
      "[Epoch: 39740] train loss: 1.1503, train acc: 0.4802, val loss: 1.2529, val acc: 0.4125  (best train acc: 0.5012, best val acc: 0.4863)\n",
      "[Epoch: 39760] train loss: 1.2845, train acc: 0.4357, val loss: 1.4532, val acc: 0.4162  (best train acc: 0.5012, best val acc: 0.4863)\n",
      "[Epoch: 39780] train loss: 1.1610, train acc: 0.4697, val loss: 1.2084, val acc: 0.4260  (best train acc: 0.5012, best val acc: 0.4863)\n",
      "[Epoch: 39800] train loss: 1.1362, train acc: 0.4814, val loss: 1.1838, val acc: 0.4361  (best train acc: 0.5012, best val acc: 0.4863)\n",
      "[Epoch: 39820] train loss: 1.1485, train acc: 0.4696, val loss: 1.2098, val acc: 0.4182  (best train acc: 0.5012, best val acc: 0.4863)\n",
      "[Epoch: 39840] train loss: 1.1391, train acc: 0.4737, val loss: 1.2543, val acc: 0.4101  (best train acc: 0.5012, best val acc: 0.4863)\n",
      "[Epoch: 39860] train loss: 1.2313, train acc: 0.4516, val loss: 1.3442, val acc: 0.3990  (best train acc: 0.5012, best val acc: 0.4863)\n",
      "[Epoch: 39880] train loss: 1.2636, train acc: 0.4354, val loss: 1.4484, val acc: 0.3929  (best train acc: 0.5012, best val acc: 0.4863)\n",
      "[Epoch: 39900] train loss: 1.1549, train acc: 0.4672, val loss: 1.2411, val acc: 0.4142  (best train acc: 0.5012, best val acc: 0.4863)\n",
      "[Epoch: 39920] train loss: 1.1433, train acc: 0.4781, val loss: 1.2362, val acc: 0.4132  (best train acc: 0.5012, best val acc: 0.4863)\n",
      "[Epoch: 39940] train loss: 1.1278, train acc: 0.4866, val loss: 1.2004, val acc: 0.4263  (best train acc: 0.5012, best val acc: 0.4863)\n",
      "[Epoch: 39960] train loss: 1.1236, train acc: 0.4853, val loss: 1.1731, val acc: 0.4337  (best train acc: 0.5012, best val acc: 0.4863)\n",
      "[Epoch: 39980] train loss: 1.1475, train acc: 0.4668, val loss: 1.2319, val acc: 0.4179  (best train acc: 0.5012, best val acc: 0.4863)\n",
      "[Epoch: 40000] train loss: 1.1251, train acc: 0.4880, val loss: 1.2071, val acc: 0.4341  (best train acc: 0.5012, best val acc: 0.4863)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtcAAAGpCAYAAABGThpxAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAABzRUlEQVR4nO3dd5wU5f0H8M/3OgdHOTiOztGb9N4FUSkq9hI1ttii0cTEiDV20agxRiM/o8ZoYuyFCAiCivQmvR9wwEk7eufa8/tjy83uzu7O7M7s7O593q8XL3Znpzw7t3f7nWe+z/cRpRSIiIiIiCh6KU43gIiIiIgoWTC4JiIiIiKyCINrIiIiIiKLMLgmIiIiIrIIg2siIiIiIoukOd0AKzVo0EAVFBQ43QwiIiIiSmLLli3br5TK03stqYLrgoICLF261OlmEBEREVESE5HtwV5jWggRERERkUUYXBMRERERWYTBNRERERGRRRhcExERERFZhME1EREREZFFGFwTEREREVmEwTURERERkUUYXBMRERERWYTBNRERERGRRRhcExERERFZhME1EREREZFFGFwTEREREVmEwTURERERkUUYXBMRERERWcTW4FpERovIRhEpFJEJOq+fLSJHRGSF+99jRrclIiIiosSglMLR02VONyMmbAuuRSQVwOsAxgDoDOAaEemss+ocpVQP978nTW5LRERERHHu/YXb0e3xGdh+4IT5bRcU4fq3F9nQKnvY2XPdD0ChUmqrUqoUwIcAxsdgWyIiIiKKI9+u2wsA2H7gpOltH/1qLeZs3m91k2xjZ3DdFMBOzfNi9zJ/A0VkpYhME5EuJreFiNwmIktFZGlJSYkV7SYiIiIiC1UqBQAQcbghMWBncK13+pTf858AtFRKdQfwNwBfmtjWtVCpN5VSfZRSffLy8iJtKxERERHZTHRDvORiZ3BdDKC55nkzALu0KyiljiqljrsfTwWQLiINjGxLRERERPFFKYW5m/ejslL5LXf9r+25nr2pBBv3HIvoOMu2H8J/Fm2PtJm2sjO4XgKgnYi0EpEMAFcDmKxdQUQaibhOs4j0c7fngJFtiYiIiCi+TFuzB9e9vQjvLSjyWe5NC9Esu+GdxTj/lR+D7utUaQWembJO97XL3piPh79YgyOn4q8CSZpdO1ZKlYvI3QCmA0gF8I5Saq2I3OF+fRKAywHcKSLlAE4BuFoppQDobmtXW4mIiIgoersOnwIAbD/oO3Cxquc6fFrIfxfvwIkz5ThZWoF/zNkWct3S8srIGmoj24JrwJvqMdVv2STN49cAvGZ0WyIiIiIypryiEqt/PoKeLerF5Hhrdx3BDxtdxSWUJitk895j3h7m5TsPYWCb+iH38+Dnq21rYyxwhkYiIiKiJPTnGRtxyd/nY+2uI4bW/3FTCXo+OQMnS8sjOt64V+dibmFVybylRQfR9U/Tce5ffsQGd271C99sxCNfrsaRk/GXzmEVBtdERERESeZ0WQX+Oa8IALD/eKmhbV6YvgGHTpZhy77wE72MfuVH9H7q25Dr/HXWZhw7Exio/3vhDlz79kLdbZ76Wj/HGgDeW1CE8or4SwPxx+CaiIiIKMk8+uUabz6yJ8t5/pb9+HRZMbaWHMcPG/cBcKVyPDt1PZRS3lSOC1+bixM6QTEAfP5TMV6ZuQkb9hzDgROlqKxU+M+i7QG5z4dOloac+GXf0TPexyfOlOOrFT/jgr/Nwdtzg+dYP/bVWvxn0Y5wb91xtuZcExEREVHsbdxbVeLOM4bwF//wnUK8aOI4XDlpAU6UVqCsohIHND3cr39fiD+O7oh1u45i+c5DOHKqDOd3aYT7Pl7ps49PfyrGw1+swVcrfCsm+z/3t+9YVXDd5U/TDb+vo37VQVbsPIxzO+cb3j4WGFwTERERJbFgE7f88dOVOFFaAQDeFBKPcned6rGvzvEue+GbjTr7WAUAWLztoBVNDetMeSUOnqi6CLj1vaUomjguJsc2isE1ERERUZJZVVw1iPFMeYXuOh8vLQ66vVIKhfuOW96uaL32faHPoMl4xJxrIiIiIgrwu49WON0EXSt2Hna6CSExuCYiIiJKMEdPu3KPj58pR9H+0NU9IumB3lpyIi4naEkEDK6JiIiIEsj3G/ah2+MzsHDrAVzz5kKc/eIPIdd/btoG08eYtWGfz6BIMo7BNREREVECWbjtAABg+Y7DWP2zK7e6srJqSsRFWw840i5yYXBNREREZKHKSoV/ztsW8UyHRilUBdT7j1eVtrv6H/oTtFBsMLgmIiIistC36/fiif+tw8QI0jGM8JTW05bG03Rck8MYXBMRERFZ6HSZq/TdoZNlYda0ToWqiq4VA21HMbgmIiIiSiBHTpUGLKtk13XcYHBNREREZCFxzzeuwnQh7zhwEpNmbzG9//8u3hmwbElRbGZIpPA4QyMRERGRhTyTjYdLz/jlO4tQdOAkLu3VFA1zsqI65o6DrkD9qxW7otoPRY/BNREREZGFRMKvAwAnSt3TkluQ0fHKzM0hX3/0yzXRH4QMYVoIERERkQ2UFVGzRd5fuN3pJlQbDK6JiIiILJTizbkOvR6reiQnBtdEREREFjKac+2d+EWAJ/+3Ds9OXQ8AOFVagd9/vBK/+2gFtpYch1IKi7dxwGKiYM41ERERkQ0Mp4Uo4J152wAAvz+vPc79y2wUHzoFAJiyajceHtcJf5q8FkPaNsCEMR3tai5ZhME1ERERkYU8AxpD9VxvLTmuu7zDI9/4PC+tqPSuO7dwv22zPpJ1mBZCREREZKEvlv8MAJixbm/QdfYdO2N4f/9aUDUYcW7h/sgbRjHBnmsiIiIiC01fGxhUV1YqnCqrgAjwv5W78MYP5iePocTA4JqIiIjIJkoplBw/g37PzAIAXNmnGT5eWuy7jhMNSxIjOuQ53YQADK6JiIiIonC6rAJZ6am6r706qxB/mbnJ+9w/sAZcgxYpMrk1M51uQgDmXBMREUVp39HT+HRZYNBEyW/yyl3o+Og32LT3GLYfOIHKSt9+aG1gHcyTX6+zq3nkAPZcExERRemmd5dg7a6jOLtDHhrUir+eNLLPt+5Bi1NW7cZfZ23GTYMLnG0QOY4910RERFHyVH6oqGT2bHVT6a63t+fIaQDAP+cVOdgaigcMromIiKIk4VehJPTNmj3efOmPlu50uDXVk8ThLx+DayIiIqIIvDRjo9NNqPbCTTHvBAbXRERERBGIx15Tch6DayIiIovEYy8aWe/IyTJUVioIE4IcF48XOKwWQkREFCXPF7zidCBJ7+CJUvR66ltc1qtZXAZ21U08XtAyuCYiIoqS5wuePZnJb/9xV2WYz34qRu0shlFOi8cLHKaFEBERWSQev+jJWpWartKjp8sdbAnFKwbXREREUYrVnemyikoUTJiC9xYUxeiI5G/34dNONyEpDGxdP+TrLXKzDe2ne7M6VjTHUgyuiYiILGJ3x/XJMxUAgBenR1YC7utVu/DTjkNWNimuHD1dhscnr8XpsgrL9vnF8mLMXLcXpeWVOH6mHH+O8NyTr//eNgD9W+UGff3a/i0M7adzk9pWNckyTBYiIiKKUqwGVUU7YPLuD5YDAIomjrOiOTGllMLcwv0Y0rYBJEj+zd9mbca784vQsn42bhrcypLj/u6jlQCApnVr4OfDpyzZZyKrXzMDtbLSsP3Ayaj39dHtA1EwYUqUe4m/XCz2XBMRUcyVVVRi8baDTjcj4XgHTsZxcvf63UehbLja+GjJTlz/9mJ8sfznoOuUVbiOa/bwz01bj/8u3gEAKC2vxCNfrsaRU2VY8/MR7zoMrF3mPzgS7RrmON2MuMbgmoiIYu75aRtw5f8t8Ale9GzeewzPTVsfNlg7eKIU17+9yFvJgcIrOXYGS4tcFzjvzN2Gt+ZsNbTdlpLjKJgwBUuKAi+OFmw5gDF/nYP3Fmy3tK0AUHzolM//ejyfEzPXHit3Hsb/zd6KBz9fjcJ9x3H+Kz/i3wt3YMJnq3DB3+ZG1WY7tc6rGbCsTo1024+bmZYak4G7cVhhzzBbg2sRGS0iG0WkUEQmhFivr4hUiMjlmmVFIrJaRFaIyFI720lERLG1ce8xAK6gOJRr/rEI/zd7K/YfD73e+wu2Y87m/fjX/CJL2neqtAIFE6bgoyU7DG7hrcWXMPo+MxOXT1oAAHjy63V4esr6gHV6PjkDf/pqjc+yeYX7AQCTV+wKWH/HwRMAgLW7Ql80RWL2phIAwL5jwQcUegKyFL/ob9+x0/jbrM26F2njX5/nfTzq5dnYtt/1HvYcTbyBiyv/dB5uGNhS97V/39I/xq2JjtG7D/F4E8e24FpEUgG8DmAMgM4ArhGRzkHWex7AdJ3djFBK9VBK9bGrnUREFHuVBnsYKyorAQCpKaFX9OQiW/U9e+CEqwf81VmFYdct3HcsbPBvFU+8EauA4tDJMvzLrxfaSNDz8dJiPPTFakvbstp9l2P1z0excudhVFYGNsTzufJ8XE6VVuBf84tw9wfL8dK3m7BgywGf9d+Zuy3o8ZbvOGxNw6M0877h+PPl3QKWB/sIBEsZalw3K6LjX9S9Cd6+wTcMS43yA3jH8DYhX3/ioi5R7d9pdvZc9wNQqJTaqpQqBfAhgPE66/0GwGcA9tnYFiIiiiNGJ13xxE9hYuuqgC/CL/3pa/dg+4ET3ueeYL7cHdyHMurlHyM6ZjTsiq0PnyzFxa/Pw86DwQerGU29+GCR0V5/X/9buQurig8HfX3lzsMY//o8vDMvMDD2fA4e/Wotxvx1Dp7/ZgP+NHmtN79f2zuvlMKTX6+LqI2x1LZhLbTPD5/j3LRuDQDBfy7lFZElWjSum4VzOuX7LHtyfBdkpEYeQk4Y0zHk6wNa18fYro0i3r/T7AyumwLYqXle7F7mJSJNAVwCYJLO9grADBFZJiK3BTuIiNwmIktFZGlJSYkFzSYiIrspg0FzVQ+3sXAy0qDz9veX4VxNkOxJK9DpHPWx3L+sXSInigKYsno3Vuw8jL//sCXgtcpK5Rqs6H5udYB/+GQpftxUgt/8dzkuem0evlmzO+T663YfDVimPf3rdx/FNL99eLYp3Hccn/0UfGCklSZe2tXU+l/dNTii44QbcHn8TFlE+9XTsHYWnhxvX++ygkLL+jUDeswThZ3Btd7vnf+fnVcAPKCU0itIOVgp1QuutJK7RGSY3kGUUm8qpfoopfrk5eVF1WAiIooNZTBH2WgQbiSm3XX4FI6eDh5glFZU4nRZBSbN3oLycFG12yV/n29oPY8Ne45ia8lxU9to2VGFw6g3Zm/BmL/OwapiV3qGFRVLig+dxGvfuXKhb3p3CX75zmLva3f8+6eQ2363IfCGt//52Xs0cIDriTPlGPXybPzhk5URttqcq/tV1WsOlg+t1b153YBSiR0bV/Vc3zioAID585+aElnIF+7ukp5WDQIHW9olDlOuba1zXQygueZ5MwD+ox/6APjQ/QFpAGCsiJQrpb5USu0CAKXUPhH5Aq40k9jfeyMiIssZjRGrcmiNReGhVhs08Ts0rpOFeQ+MxKwN+zCqU8OAAOX/Zm/FX2ZuwvEIp7UO97ZGvzIHQPR1pu0qxaf9ufjnJ3tSNawoSVdRqdDx0Wne0nlnyitN5zifLA3slzOQxYMuf9Ib4hUbjerUiGi7zLRUDGpTH/O3HECjOq7c6U6Na6NwX+CFWrT50FaYdu9QfLJ0Jx79am1E2xv9+3BZr2bo2rR6zdC4BEA7EWklIhkArgYwWbuCUqqVUqpAKVUA4FMAv1ZKfSkiNUUkBwBEpCaA8wD4DlcmIqKEFayqgz+jwXVVqoLvekX7T+BvszZj/hZXhYvdR07jwyU7cet7S/HJ0uKA/XhunVs5w5+VYtVvLQJc84+FPssqjd1sMOT17wu9gTUA/O278ANH/endzTB6x8EpoSYBuuecdkhPDX52PWX2+rSshy/vGozfjmqnu16K+8SM8suTrojw3JiN1fNyMpGVnoqs9NSIjmfGS1d2R1oUud92sa3nWilVLiJ3w1UFJBXAO0qptSJyh/t1vTxrj3wAX7ivzNMAfKCU+sauthIRkXUWbDmAlvWz0aRu8F46zwCzcN/b3oBOXLf8lQKW7TiE33+8Es3q1cAHtw4AgKD1rc9+8YeAZbvcPa96pdbMZF1MnLYh6PbHz5Tj7D//gIfGdkTjOjUwsE194zs2wK6+yWBvf87mEny7bi8AYJEFk/9s3HMs6n2cLgvspv7sp8ALpnjSp2XVdN9/HN0BL3xTNZX6fee2x33ntg+67cRLu6F/q1z0blkPIuIzAFcr2IDTWKUUeQZWGj1avex0HDrpm66lNL/3icjW6c+VUlMBTPVbphtUK6Vu1DzeCqC7nW0jIiJ7XPOPhcjOSMW6J0f7LF9dfAQXvjYXv9TknYZLb1CanusRL/6AkmNncMKdDrBDU9Hiv4td4+fX7Q5fXzlU2b5wRUfu/2QlPl/+M7Y8OxaTZgcO+vPYsPso9h8/g/s+duX1PndpV5zfRb/6wbpdR5GTlYbyShXTXNVQ/N/+9W8vDlwnisBnsc4ENKGUVYTO99hx4CTq1rR/ApVo9WuVi79e3QNnt2+IOtnpPsG1v3dv6osTZ6ruoNTJTseNmindw+VC+78aaWht9sf80NhOpg742i964dq3FrkvoE0eLE7ZGlwTEVHy++e8bXj9+y1Y+sgo7zJPPuyN/1yMczrl4/oBLTFzvavnUzt7n16ANr9wP/JyMtEuP8ebOlCpFIoOBJaHK6+oxKualIKKSuDQiVKs33MUg9o00G2vZ4Y/T+UI/9xi7WuH/Ca5+WRZZD2jD36+Gg9+rl/3eeyrc7yPHxnXCb8a2jrofqIJPvaamBTFrhhn39HT+MvMTSg5Zm4mzXA58MP+/H00zbLN2R0CCy2M79FUZ029bRtGdMxgnxG9uuDRCHZx1a9Vrv4LQejNKhkqfSYRMLgmIkpwOw+exNAXvsf03w5Dh0bh6+Fa7Yn/uWoFLyk6iD4t63mXX/L3eVi+4zB+2FiC6we01P0yFrjKohXuO4Y+Bbk4U16JX7y1CIDvoL9g06S/M28bXp21uWp/Alz39iKs3XUUQ9vpB9ffrXdVmZi2Zg8KJkzxec0TmMwrdAXcnhzePUdO+/Sefq9TqQLQ9IpH2Kv79JT1IYPrv323OeL99392lvex9r1Emi5gtorEwROl6Kdpg6ljJWh6gJM9sf7jFMLF1vee0w4Lth7wpmxFK9ED5GgwuCYiSnDfrNkDAPhk6U48ckHARLgxc8WkBbj//A7e59rqD5Nmb8ErMzcHbFO47zgmuHt001PFZ5CbVrAZGuds3u/zXFCVz+v/mkdliIhHb2ISABjwnG9Q+LuPVwTdR1VLrPee32yJWsfPlOP5aRvw4NiOyM5IQ3lFJQY//x0eGtsJQ9r6XmhoB7ed0hm8aaT12tjt4IlS1K2R7r0o0XPOSz8Y2Ku+3/x3edDXjpyyrn5ztAa2ro8FW6vOgeezNvWeoaibHZu0lWCf7nDBbtuGtXDiTHlAcB3phY3ZCwtBVduNTjIVrxhcExFRxJb45c7+ebp+Dqne4D/Ad1Chf2CtnSUwWLWQYAF0KCd0SriF8vHSnQHLgn3lmx2IFWyilLW7juD+T1YhOyMVn9wxMGhu+qz1e9EmrxYKGtTEW3O24v2F25FfOxN3j2yH9buPYe/RM7j3wxUY3NZ3QKX2PXkGKgLAxKmuGQz1BgsGs+/oafR7dhZ+O6odJq/0r7hbxX/QmhnBfs53/ecnTFkderKZWOrevC4a183C5+4Jajyfh85NasesDcE+g0bKFEarc+Oq91l9+63tLcVHRERJ7MSZclwxaUFU+9Cb5MNj6AtVebTBeq79iVhfju2Pn64KWGY2QNfz9apdQSdKGffqXKzbfRRLtx/Cy99u0llD8MnSnbjlX0u9FVE8vXxbSlxVJG751xLv2v49yo9p6g/f++EK72PP+9p3zHh+tudn6Mmpj6V4CqwB1+fv5St74P1b+gFwJjUiWGpSy/rZEe0vWO9xuF7lUHeI9CRTMM7gmogowVn5BX66rALl7nzc02UVePDz1dh39DQ+WrIDSikcPV2G57/ZgHNfnm3JZBz/XbzD0HqeQYjhxOo2cmm5fjeg0Z/E6bIK3P2BfqqDtscecNWAXrfLd6pvEeCf84p8lr0735XS8sVyV6/pPpODBrWMxEULtx7A0Be+Q/EhV3v1JjSpDrTl89LcF4Gei8G0CGdFNCLc3RH/34XmudlY/fh5IbexMsA1GltXpYDoSMysEKaFEBEli0hyIzfvPYa2DWt50w46PvoNRnZsiHdu7Iuz//wD9hw97Q2AyyoUHvnSmfm87vj3MkeOG4njZ8pxqclp0bX0coj//kMhHvSUOIMrINEGs6fLKnzSLoIF/0aVhil9BwBr3QH/nf/5yd2GGOQdxKHfjGzrvbvgGYzav1V93D6sNW4Z0irUprYIFdTmZAXP+1ZhtjXfDmM7q+ppr6rFFzLgTgDsuSYiqgZOl1Xg6GnfoO37Dftw7l9+xNNT1nsnVgGA79yVMPwnWXEqsDbDxo5CQ5RSeGbK+qj2oZfq8PWq3bhWM2Pi/uNnfAJg/4D8F36zK5oVqwlHEtXDmgsdbT68p6xcaorgwbGd0LB2Vszb5mVRZBrxgEaj6+kE0oleaYTBNRFRkggVD41/bR66PT7DZ9nGva6qGm/P3YZBE7/zee2+sNUw4pPT1QW+XP6zoVSXPUeC5zS/8YP+5DR6db49TpzxrQO9dPuhsG0IZUlRdNtrVVQq9HtmpmX7s0PnxrXx0NiOuq89Mq6Tz/O7RrTBrcNa492b+uLv1/aKRfNM0U68ZNaQdiZmEg2z+2iuzzy/x4kaYjO4JiKqBjyBtFaF38A/7cA5T7WDhOPwfeQXZ+gNPgykNy17NEa+NNvS/Vnp2OmyiPO/XVPe2x9iXdKzKW4b1kb3NW3d8V8ObInfn+sqN3l2h4YY27Wx7W2LVCS/CiM75mPdk+dHvR/A9+7Hfee2x9V9m+uvp7ss+M/8qfFdImxR7DC4JiJKEmY7qvxn7NNOxpKotjg0qK5787qOHDcRRBMbt3pwKq78v+gq0oTTryAXl/VuBgBhc6SfHH8WUgxWrnFKtJci2RnGhuOFOwuea/fzOufjnnPaBf0ceIJw7d+vRM+55oBGIqJqKtSEJInqTJQD+SJ1bf8WWLnzsCPHjndmS7L5szJFxWPhg+egvLIS2RlpyK2Z4V0+YUxHvD3XVXXltV/0RG/NjKOJwmyt9artor9D0D6/VtX+3P83rVfDUHtEM42MZ1u9+u6JkCrC4JqIKMGZ+U78z6LtePiLNWieW8O+BjmoPBYzZfi5Y3ibhO1hi4V4DIYa1dEfaJieWnVD/4JuTWLVHEt5q29YtUODUfp/bx2AHpo7ON4e6TAticfPR7SYFkJElCT0enlWFx/BlpKqVImHv3BV/Nh50Fjd6ETjxPuaMKZj0BkUKfqe60QQT5kintMdyYBGM/w/8wPb1EeNjFTvc8/jnKzQ/bhmPx6J8HFizzURUZLQu6174WtzHWgJkUaIYOj24a1xx7A2eGH6RsMTCtmtU+PaKD4YvDKLntWPnx83FxHeVmhi32iC/2Cb1soMHUJe1ac5jp8uxw2DClztCnp69HKulc//iYY910RECY6dps7jjyC4iiAB0sU9muC+c9ujXs0MPHdpV2x4ajReuapHbBunY9q9Q7H6ifPDr6hRMzMt5AQtsVQ1GNDeT+X5XfLxwGj98oUAkJaagtuHt0FWuqsHO1gFEL0c8VA514mAwTURUYLTi13KKirjpieQqje9z+d7N/fDK1f3RGZaVRpBVnoqmudme59rBxpSoOBxZ2BPsB1BqojgqiDl9XTXDxLsh+qbTszQmsE1EVHS0H6BvvnjVjz4+WoHW1O9JGgHW0zopUsMa5+nu27vlvUw+e7BuLZ/C7ublfQkyGPT+wmxsZm0jWD7iVVPeywxuCYiSkIlEU7aQZFZsOWA002IS0op072m3ZrVRYpIwubbOs2u03bPyLboV5Drs6zSxLEOnCjVXZ6R5gpF62ZXpdWEeg+J8LnggEYiIkpKtTLTcNxvWnC7LNjK4DqY9AhG04nEV4m2H+8fgaIDJ5xuRkhD2zUAYH21EE+P8n3ndQh4LdygRq0jp8p0l3dvVgePX9gZ7fJzcO1bi3xeS42nMiwmsOeaiIiS0vd/ONvS/d1/foegk4owLSQ4/yD5wu7h60fH2+lsUT87aCpLvOjT0tWrrHRzru05prb0XlhBrpZEBDcOboU6NdIDVq7pDt4b1Eqs/HsG10RESSgRbp3aoWFOpvdxnuaxFdrk1cKfLuys+5rdNYUTmX/O9YtXdDO0nZmPcHUc/OifbhOsGgdgLJ/Z9j8ZJn5F/NvStG5iTXrF4JqIKMF5Jokp1Uz9vc+GnOs5fxzh87xZvRoomjjO8uNEw1Nt4p5z2lm+7xRx5QPrKa8IjEw2PT3G8jYkGqUCAyVthZBgErUEWzzQnf7cwOkMdsqt+lFYleGRCN0GDK6JiBLcx0uLAQDLdxzyLpu2Zo8l+/7HL/ugaOI4FE0ch+a52ejfynXr+ZWremDmfcMBAC005dOctmy76xzcM7Kt5fvW653+9y39AQA/Hw6cGTIjLQUdG+VY3o5E88PGkoi287/7csPAlrrrDWidm7B3aqzMKfacgqozITqP9N04qABjzmpsWVuslmg/XQbXRERk2GMXdkbbhrUwslND7+QQM343DKO7NHK4Zb48gXCahcGLXg/eEPcgsmB+d257y46fqIoPmZvtMJhrB+gH1+O6NQk6WM7jkXGdLGlDIvAE2WbSoh6/qIu3aoc/q36D6tc03h7vJDIWHTvWGFwTESUJO3p3/HsEuzSpg5n3DUdtzWx0WempePyiLjYcPXKeQLhjY2d6jnOyXAOxUpneEPHn0n+7YGeyUe2skCXh1j15Pn41tHWErUg8ntzr5vVq4OvfDHG4NVXSU0P/LmgHNCbojQgvBtdERElmz5HTptZvk1cTADBhTNVUxh/c2h8XdW+Cszs0NLSPeIsh7ZmRzth67RrWwp8v72758RORgm+g1CHf2MWOCAxH5eFuTmRnVI+qw57Tdduw1sjLycSIjg3R2v27Heyz+8Dojnjxivj4rDbPzUZtz0Wp+4faPj8HzerV8PnblAiqxyeOiChJaW+He4KY02UVhrb97M6BaJNXC699V4gtJduQKoI2eTWxpeQEBrVpgEFtQqc8aMVZbO2ob9256OSirWJh5prHaOdlda3UEuxdd2xUG0seHgUAOFla7l5Xf+02eTVxXpiUrpQY1pr+/g9n45NlxejVoi4AV6m/uQ+M9L7eMCczIXq12XNNRJTAuj8xw/vYE8SsLD4cdrv5E0aid8tc1M3OwNX9miMrPQVjujbClHuGYs0T55tvSDWIbyKZnjme4z7tjHi20gRDRkuqCSSghF/QcxnH5zgcS+NWnahTLxCded8wU3Wjw6VzWKl+rUzcMbyN7p2nr+4ajKn3Do1ZW6LB4JqIKEFV+CWarvn5KADgqM7grkcv6Iw7hrfxPm+iCXLaNszBhqfGoFm9bGSlp5qadc0jksAzFp6wMBe8b6vc8CuZ8GAEt7o/um2AZcf/3Sj7B1sqpdCiflU1mZev6mF4W09wnZOVhh/vHxF0vYL6NSNun9NGuyt0RPJZMMI7MFDz69m2YQ56NNefDElPWkp8hIrdm9dFg1rW1q63C9NCiIgSVHllpe5yvdu4twxpBQA4p1ND9Gph/IvVqHjtoe3d0rqAOKKLjhDnJZIybB0b1za9TTCxut3fNq+W97HvLHzBicA7SPH2Ya3Ron42Cvcd0123Sd2sqNvoFE+vcH0Lgka9bAlPtZx2DWvpvGos9Sa/tjXn1+psjqFhKvU4KT4uR4iIyLSHPl+ju3zyil0+z/92TU/v474FuZbW1qXQKvSvfwAAl/Rsanp/ll7ExCh5tSLC43gq1YQbnFpdc66NyEpPxfu39MO7N/XzWW70lL1xbS+M7RpfZTY92uTpXzDEAwbXRERx6K8zN+OBT1eFXOezn4p9nvctcPVIL9p20Gf5hd2bWNs4MqwiyN0FwNVbeVWf5qb2VyM9+OyGZmeljNW4sEmzt5reRlDVcx0uEExPTfxQxs5JcIa2y0O9INPDhzvsmK6N4262zAQYz8jgmogoHv1l5iZ8tHSnqW308p5rZoSfajpeBZuRLx4FG6hXpjMtulbD2sbTASZe2jVkIJnjl7YSLibSC6ysjqMUgPmF+yPb1tNzbXE+f+M6iZtGouX/szITn8dXuByZOIv5fTC4JiJKFjpfNgUNYjPYy8j3nNlZ8lom0EC1tkFyWv0HnfoLF3yb4alpbJReb2m0HaiX9WoWsCyStA1tznVVFpM10VTzetnhV4ox53qHE6EfOPEwuCYiimPBbheX6yTzemraav1qaCvL26THyFd0dZolz+PczvkhXzdakxwI31PnX1rPiXCt5PiZgGWedt8+zPjPXxtsxnMPZTxRDJTjBoNrIqI49v3GfbrL7/1oRcCycp1e0Et6BvYkxpunLj5Ld3kiBVXB2lozMw1vXNsr6HZnd8gzfoww4XLTur49sk70huod0VM+rWYE1VZc+wz+PuoHySXW47nQGdDa2pKKRP4YXBMRxbHDJwNrVgPAlFW7A5Z1bOQ7tXQ8FgXRq+cbrJ1x2PygIm3r2R0aYs4fXTWc/Wv4ZpvMl29UJ8tncpBwbdLr57x+QHR57np5wHcMd/VYX9bb+IWedjehrhEeu7Cz4X16qktkxdE4hAfHdMKlvZrigm6No96XmYHLiXTh6s/OwZ9WYXBNRJQkOjfxrYH80FhzOc6x0LRe4MC/YD2T8ValIFJD2jVAt2Z1kJOl33PbPDcbRRPH4Y3rqnq4uzevi7V+M2UaOR05WVWpIdeHGRCqF6O0rB9dPnK3ZnUDlqW6JyFJi/Bqz+rPQTzFZnk5mXj5yh7IClEFJpxGtbNQNHEcOjYyXwM91LloZFF9ayPHikS8TlwFMLgmIkoaZ8p887DLwwymc4LeF2ywabhDxWLRVEEJVc7ODjlZ6Zh89xAMax86BUT7dtNTJCCoNBtkNqnjeyFz//kdTG3fPNfYVOVanXUmuan0Vv0wQXQfkkXCBaZFE8dh4UPnxKg15vRu6So5OqRdfYdbEhyDayKiJLHvmO9gsrLyEDOYxJGg1ST8lntm9+vdsl7E+bsAkJZqfbhmJPB9/rJuYfZR9djoRD/jexhPBfj12W3CrqP9WTx5kX4uvBlKO8wuwtPuORVJciODotSzRT2sf3I0RnYMPVjYSbYG1yIyWkQ2ikihiEwIsV5fEakQkcvNbktERC6pKYJKTW/1pSZyXKNVafCer95abRvql5Dr4pfmkpnm+soa3SW6GeOcmtHPzPTpesG1/5LnLu2KHs3r+izbtv+E9/F5XXyDDyMXANrDRpKqoHuICOpVa9cN1W6912beN9zwcRKZFWkWsby3ZeVFbY04ypvXY1twLSKpAF4HMAZAZwDXiEjAyAP3es8DmG52WyKiZGfmCzQ1RfDM1PXe58EmNrFDvWxjVRsydL5g2zasGoh5Tb8W3sfpKcG/ouIh4eWfN/b1PrYmbKjai15w3S7ft5Z2uGPm5QSfoCboINIoS+DpxtZR7C+S7bQ1x5O5t9tzXiNJZXfivFzuvthvnx+/05Zbxc6e634ACpVSW5VSpQA+BDBeZ73fAPgMwL4ItiUiIrcUAd6eu82RYxudgvrczo3w+3Pb4xf9W+i+/tylXb2PgwUACiqqYNaqagN1NLniVgQr4dJC/Cc/UQgdYIfqKVbQv0DRtiGSt6Tbk7xev5xk6P0E7tNMezyT2XTSDPJLtkDbc5cqmgGfsRzcmeH+G6EddJus7AyumwLQzt1b7F7mJSJNAVwCYJLZbTX7uE1ElorI0pKSkqgbTUQUT8x8b6bEY+09P6kpgt+c0y5gqm49/jMOWhUHWFV9QkI8i3Z/rRsE9u75NzvcEcNPfx54Rn17riOYWRHAzPuG4XZ3+T2lgB83ub6bzUyY479Ps695UmKaaO7e9GrhGgjXrVmdiNoRryK6w+DAn4rsDNfvfKMkmX4+FDuD61B3hzxeAfCAUsr/N87Itq6FSr2plOqjlOqTl2e8GD8RUaIK1vOaabD3OFFkZ6ThmUsCB9UJJKpA2xP4RUsbfNYOUmYv0v1d06+5gfWtO553WZT7F3Gl+XgGn4Y7XtD9RNkOz6+IdttzO+dj8cPnYGi75IgVvFVYIjg/npKJeqUx7dK5SW389eoemKi5O5Wsov9rEFwxAO1fh2YAdvmt0wfAh+5fuAYAxopIucFtiYiqpXfmFekuN5qaERdCBASzfj8ce46cBuB729qqW9hGes3NemBMR3y+/GfL9tcuPydgmX+aR7gBgqGCLqX0L9K0gz2t7tyM9MaKp026FwNh9un/csOc5Ok19fz4Ihmge9vQ1hjZsSHa63zO7DS+h24SQtKx8y/xEgDtRKSViGQAuBrAZO0KSqlWSqkCpVQBgE8B/Fop9aWRbYmIAOBX/1qKgglTnG6GbfQCyqe+Xqe/rs1tsVSIxrbJq4XBbRvEri0WsKJ6QdgQyUAM1U4zmC+SixHfXOfotvdnJgiMNvc7wX4bIuLpuY4kuE5JkZgH1tWJbcG1UqocwN1wVQFZD+BjpdRaEblDRO6IZFu72kpEiWvm+r1ONyFuRDLph5Wy0lPwwGjf6c23PjtWf2WD8YAK8iyeZtkDrOnlNR0jhVk/kjsZ2t7l4kOnTG8fqjfdzPvzLcVn/nh6aSHJxlN1M4nfYsKyMy0ESqmpAKb6LfMfvOhZfmO4bYmIKLjHJ+v3aMfKhqfGAACe/2YDAKBFbralgyw9AZNyoFdSr3yY1UFNNGkeeusYnYgmWBt2HDhpenu9/XhEWl88kmohodqRPOIjuh5zVqOAmurVna3BNRERxc6RU2VON8GHJ5ZqkZuNHQcjC9TiJTR6cEyngGV65eKiYXYXesFqtJ8B7S7LKiPJK4nq8Lrt+DlED3rwco3JL5qcayu9cV1vR48fjxJo9AsRUfWTDLe1p9wzBPMmjIxoW6Xz2NUbGXn4FNGWcfhzEAQG9XuPnjG07ZAgOe3aQC2SeuChTlNdnQoiRrw1Z2vQ14LNDFqd0kISoAJntcPgmoiIbOH5zs/JSg87W+RF3ZugU+PaIdeJhd+f2977WJsKohe/eOr2BnvdalYe44JujXWXh5gUMyLa2DfNRA649r3mu+si6wXKHy8t1t0+I811rJo2VIaJF9EMaCR7MbgmIopj8TZwzwy9Wsdefu/r1Wt6Ytq9Q3XWi+0JuEYzc+SVfULXmtZOs22FcDGSJ2A0un44eqdWm6McrFc4FG9+tES+D396KTkeB47r99Sf07Eh7j+/Ax67sHPUx49XVpxbsgeDayKKK+t2HcX5f/kRx07HV/5wvCirqHS6CYad16VR8BcjmhjEmmohdbMzgr4WrBcwXE61JdOfhzkpmWm+5f6iOWaw06fdZyQp1z1b1PV5vqXkuPmd+DUkO0SZw2DnICVFcNeItqidxFNtey5eezSv62xDKACDayKKKy/N2IiNe49h0daDIderqFS48G9zY9Sq+PH694VON8GwSEqohdpJi/o1Q6xoXL+C3OCHC7K8cwxSVswGy9FUwlBK/3i+OddVy3+h6dEPpUa6byBcEUmE7sfTJL33m9zVQEJrVi8b/7t7CJ4Y38XpppAfBtdElDAqKxUe+XI1Nu89hjYPTcXqn4843aSYKzkWeBv8V0NaOdCS8KwOfP7xy94Y1j4PV/RpFtV+6tU035uZl5MZ8nWz77VvQT3TbQg4ZpSn15Mz3r9V1cWGdp/aqbEzQuRLn92hajpx8fv/+40lEbXNZ/pzi2pnJ6OuzeoE3NEg5zG4JqKEse3ACfx74Q7c/v4y3deXFh3E6uLkCriNBA9D2+cFLmvn/AyHlgQ+mu7ThjlZeO/mfqibnRFVqbV4CEYimcrbStpa4a3zqu4IaKcHj6QKhf+gxVdnbTa/Ez+h2uH/0rggAzWJYonBNRHFzKKtB/DGD1ts2//lkxbgwteqX6qIXuzx3s39Yt4Of6FiMzsnghnctr6p9bUzW2ZnVgXeZmpXm0/p0FtmbifR1NbOSE3R/Rm0DTJ9+tiusQ1a9WqI671d/3OQWt27sikuJG+NGiKKO1e9uRAAcOfZbQxv87dZm9G0Xg1c2qtZyEFsfZ+ZGW3z4pL/e16762jAOnqD8KyY1CRaljQhgp20yA2fmz1vwkikpwoqK4G62eno+Og3AFy92l/eNRi1MlMxe9N+08eOhhUBulH5tbOw81DgxD7afWrzpfu1ysVndw7CZW/Mj+KokQnZc+38x5woAINrIoor/vHzS99uAgBc2it0nq1eLnIyOlMeWC2kdo34/FNu52CzUBOcGAm4QtXd9lRfsDO41u2FtWg/RihU/Xy0p1J7oRbNYMRog17tZyceLhSJzGBaCBHFpVDfp9W1uuvOgyexfndgz3VqnE7RFqrOtZ2Bd3yeDV+6lS/M9lxHW+fa/Zvksx/NYysqfVghdNUZX/HRYqruGFwTUULYefAkjpxy1b7etv+Ew61xxtAXvg9Y1suvrjAA/HJgyxi0JrzLe0dX1SOUs5rWAaA/cNPqjs7Lwtw1ieSYIzoGDkINpWjiuMBjRnEZ4Xs9pu0lrlpa7mBw7dOOilB3KRLhUoqqGwbXROS4rSXH8bI7/SOYoS9870i+p9NOllWEfP1fOgMXW+Rm29UcU1Js7FFv7J4Se2THhgGvWd0rbkf8pj/7o9kBjaEDz1AGtWkQZIbGKhf3bBLRvq2gbcfxM8EnlGJoTfEoPhP1iKhaGfnSbABAn5bR1/5NNjsPBg460/KftIOs41+zOfS6JgNjK9JCNNuM6hR4kRGKNpVIrzIHADSu45+XbjyQtyPtR79aiP9xiZzHnmsiihu/fGcxTpwpd7oZcWn3kVO6y/Xu3F/ZV69XNLnceXZbdMjPwbmd88Ou+8fRHdC9WZ2IjxWrzAOzhxGpCpIb1ckKs3YgvVA5XtL3fQL+kJPIxEmDiTQYXBNRXFm0zTXt+bHTDLK1Bj73ne7ySqUCgo/aWeZnIIw1w3Wug1QFadWgJqb/bhjqZWcEvOYfIP767Lb46u4hZpvoZaQX1ooYz3ygaE1gaXQ2xHjEAY0UjxhcE1Fc+m7DPqebEBfChTqZaYn5Z9xoEBdJsGR1b2aKHafYkmA8+n2YEarOfDhmB7dqf4aVSgUs8/BcjBPFk8T8q0xERAB4Wzw2PZUGeq5jcpRQ20awtU60bOTjlF870+e5Xsm+Y37pXXePaGuubWH279EhPyfi/RLZhcE1EcXEhM9WmVr/0MnSkBOFVBvVO3aOiNXXG3ZcvxgZnBcrRo/r+W30Tzuaszlwsp3Cfcd8nkdTwSZURcCbhxREvF8iu7BaCBHFxIdLdppa/+ipMsxaz9SQUC7o1tjpJiSUL349CLUyjX/tmQl2rbiDEOt8ZzsvXU+V+paQjKYsY/v8WgD0Z+VMtBxxqh4YXBNRXFpZfAS7glTIqE5Kjp3BDe8s1n3tgm7O1SGOFStvXvRsEVmpRzsqaOjtMpJSfFacHzsC1My06EpEas9F6zxPcB3VLolihsE1EcWt9xZsj2g7pVTS5CJ//tPPQV8b3LY+AOfSCeJBlntA5+C29TGv8AAAGyaRiVHOdazlZLlCgHrZVWkeoT5LnuA2xcAHLrdWYBUXWyTiiaekx+CaiOJWqIFMVKU6B9dpqSkomjgO32/YVxVcJ0TOtfM/tPHdm+JkaQWu6N0cr35XCMC6C5Nox0sYboffYThOg+IBg2siilv8ojRGG4jcMLClgy2xnuczMKRtg9DraaIsq8JWz8fP0AyNJg+qd+FoOi1Es0EksXpKiuDa/qE/L+/f0i+idAz+6lJ1xuCaiOIWv5+N0QZWtbKS8896m7yahte1vufa+l7mk6XlqFPDt+pGPPRm+zdhaLs872PlrTcdfj9OBdfxcA6JWIqPiOIWe79C85yeRJxhz+gMjUY/AtrPSk0TFUGsODZgPqjT+2xH8pNz4u6Okfeq/flGUtVm37HTgfvUe6uJ8XGnaobBNRFRgtPGOgnTcWcyJgwX0GmzLG4d2jqCBgUy01NrVoYFM2uGatbCB8+xfJ8eFZWVBtapevzA6I4RtcWf3gVZv4Jc33V4RU5xgME1ESWdRPh+PV1Wgf3Hz0S1j4xUz59wTe5tVHuMHat/RNqgyorAVctIdQyz/CdiAVypImY0DzExS6M6WabbFM7+46UAgE17j4ddt1Lz87CqLXq/1zlJmgZFiY3BNRHFLaOpA4noxn8uRp+nZ0a1j6x0Vy1hnzrMCdJ1bXUPY+0agcGqGaM65eN3o9r7LPMEiHacUb0f0+mywB7hUDMbdmhk3dTfj17QGQNb1w95h+DAicguBlMt+kzqZoUkyOedqhde8hFRzJ0uq/AGhtXVwq0HLduXNsDYceCEZfu1kye2Ht2lkaH1whnQun5U7Xnrhj5RbW+FTJ0e96n3DjXdox2JW4a0wi1DWqEyRPnLsgr91/q3ysWibb6f5x83lXgfRzI7o97PvTIRbkkRwUDPtYjcLSKRTWtFRKTjjE4PnZ6OjWrb3BJnrN11xNL9aUOXL1fssnTfdvGESb1bhv56cTKc8pbic3CGxlqZaWiYEzytIs/9WtO6NSxqWXDlFfq/t3oDSA+cKLX8+FYNAiWym5G0kEYAlojIxyIyWngPhoiilMzpHsEopXDkZBkA4IK/zQ25rtngOxH/Ksc6cI2EZ3bMaWv2WLTHKmmp1mRlju3aCG9e3xu/cg/i7Ny4Noa1zwuzVXChfh4VQXqOB4epQW5dO8L/3ah+f1koHoX97VZKPQKgHYC3AdwIYLOIPCsibWxuGxFR0vjv4p3o/uQMFO47FjTVYdfhU5i9qQTjXg0dfPtLlPJ7Wt585jDRtSc3245BheEUHzrp/v9UzI9tlIjgvC6NkOpOvZh671C8d3M/W46V5pfekZOVhgUPjsTNgwtsOZ4/vYwVz8diuPuCok1erZi0hSgUQ5fOyvXXbY/7XzmAegA+FZEXbGwbaZQcO4PFfjlty7YfxA8b9znUIvudKa/ALe8uwcY9x5xuCjkk0hTLeOy9em9BEQDglZmbg64zaOJ3uOGdxTFqUXwIl47r+QxEkLYbtdiPC7D/TS59ZBQWPDgyeAtCXMSc786Pf+riswAAqSmCxnVq2DKo8KcdhwOWhfp7cFZTVwpZRmriXWhS8gk7oFFE7gFwA4D9AN4CcL9SqkxEUgBsBvBHe5tIAHDx6/Pw8+FTKJo4zrvssjcWAIDPsmSyuvgIZm3Yh8OnyvDZnYOcbg5ZqDqOS9rgvkj8etVuS/Y3qE3VAL5EHOh1VtM6AIB2DUNXvPC8t0gGxcVa24a1cOvQViHXmXbvUOw9GjhBClDVC9u6gfHZKM1qUCsz4m1b1q+JoonjUFpeiUe/XGNZ/Wo9K3ceDliml06WiHdtKPkZqRbSAMClSqnt2oVKqUoRucCeZpG/nw8Hvy15xaT5+O2o9rbkvRGRvQomTMFLV3THZb2bmdpOO1FKIgbXl/Vqil4t6qJ1mNv4lRHkZlvVk2r2rM68b3jYdTo1ro1OjUMP1I33n2ZGWkpAp86r1/REE0096yv7NMPHS4vxyR0DLTtuAn7MqZoykhYyFYA3H0FEckSkPwAopdbb1TAybknRIdz38Qqnm0FkGL8jff138Q7T22h78UJUT4tbIhI2sAY0PdcO5FyXlhuramOVRO6Dvah7E/TRzJb43KXdMOv3w9HXbwbFaDSrp1MRRTz/JfLZo2RjJLh+A4B2OqYT7mUUR3hFT4nE6NdgIlbBiMTS7YewfMchU9vUSNfeeEzePwCeAY1WTURixokQ9aWjSa8Ix8oJdhq7e5Mfv7Czqe3qZUc3KU9qilg+uDBHZ1ZLj9uGt8aVfZrhxsGh03KIYsFIWogozW+6Ox2Ek8/EmdNlFU43gYii4D9gOZwBrat6BNNSkney3Yt7NsW784twZZ/mhrexKgyvWyMdh06WoWeLuj7LNzw12pYLvwY5roD9yr7G32s4z1xyFp783zr8on9Lw9tMvnswWubal/dtJc/PoXZWOl64vLuzjSFyMxIkb3UPavT0Vv8awFb7mkSROHra/hm8iKxitF8u4mohSiGxb7KHdsPAlj55xQU2DoBzWrN62Vj6yLmOHLt9fg4WbTuIP5zXwWe5XVVEamelY8uzYy2tjDKyYz5Gdsw3tU23ZnWta4DNkve3nBKZke6OOwAMAvAzgGIA/QHcZmTn7klnNopIoYhM0Hl9vIisEpEVIrJURIZoXisSkdWe14y9HSKi5NcrzKyGZI1bhrhSDDqHGYBopdQUsaW0HRHFTtiea6XUPgBXm92xiKQCeB3AuXAF5UtEZLJSap1mtVkAJiullIh0A/AxAG1tnxFKqf1mj01E8Y2hQ+Q65OdgbNfGTjcjpH/f0h81MmJdI9p653VpFLelTi/t2dTpJtjq4bGd8MzU8DUTeCFC8chInessALcA6ALAW2dHKXVzmE37AShUSm117+dDAOMBeINrpZR2oGRNJPOoHCKKQPL8SRAJneby8rebDO3no9sHIN2iqbPtMqSds2VBkz3eiteA30qN62aFX4koThn5C/0+gEYAzgcwG0AzAEamzGsKYKfmebF7mQ8RuURENgCYAkAbsCsAM0RkmYgETUMRkdvcKSVLS0pKDDQreR0+Wep0E4gMSZ6Q2TpnDJZ9q5udEfL1F6/goC6qPpL8OooSlJHguq1S6lEAJ5RS/wIwDkBXA9vpfeYDvlOVUl8opToCuBjAU5qXBiulegEYA+AuERmmdxCl1JtKqT5KqT55eXkGmmWt4kMnsbr4SMyPq+fQyTKnm0BEDqnhHmR3Xhdzg9eIEsk3vx2KRQ+d43QziEIyElx7IrbDInIWgDoACgxsVwxAW0+oGYBdwVZWSv0IoI2INHA/3+X+fx+AL+BKM4k7Q57/Hhe+NtfpZhAlFLt7m+KxZ9yK9/zhbQOCvuaZVKY69+Rd3KOJ000giwSbGKljo9rIr12VMpLsKUCUmIwE12+KSD0AjwCYDFfO9PMGtlsCoJ2ItBKRDLgGRU7WriAibcU9GkFEegHIAHBARGqKSI57eU0A5wFYY/A9EVGci8fgNxEMaF0/7DrVeYDXn6/ojuWPnlutz0GysHIiHaJYCzmgUURSABxVSh0C8COA1kZ3rJQqF5G7AUwHkArgHaXUWhG5w/36JACXAfiliJQBOAXgKnflkHwAX7j/QKYB+EAp9Y35t5ccvlmzx9B6/Dohil8SbkRjlDy7rs5/B9JTU1CvZuicdEoMjK0pkYUMrt2zMd4NV4k805RSUwFM9Vs2SfP4eej0grsrjHBUjtvCrQecboKj2IORfKpzAGg3dtpSMqg0+Hdf+NeE4pCRtJBvReQPItJcRHI9/2xvGVV7DBKSV7xdLq3ceRh3/nsZKoIlelog2n0/NLZj+JXAYIOSQ81MIxNIE8UnI59eT3m8uzTLFEykiFB03p1fZGi9ZAtG2WFNsXLXBz+h+NAp7Dp8Cs1zs51ujq7bhrUxtF6y/R2g6ik91fVBPrtD7KuAEUXLyAyNrWLREKJgODip+or0Aqs6X5hV5/dORBQPjMzQ+Eu95Uqp96xvDhFVB7xcIiIjwv2tUHGXZEZkLOe6r+bfUACPA7jIxjZRhGat3+d0E4gMMfp1yLsWLr/o38LpJhDFlYfHdgIAZKQaCWOIYstIWshvtM9FpA5cU6JTnHny63W4eQizeCh5NKjFsmoA8NgFncOuw+sQSkbBLsRvHdYatw7j0C+KT5Fc8p0E0M7qhpAxr323GWUVlUFf/2TpThRMmIJZ6/fGsFVE9mDA6JLlntqciIjiX9jgWkT+JyKT3f++BrARwFf2N430vDhjE7o8Nj3o6/d/ugoA8NKMTbFqEpGNGF0TVWf8C0CJyEgpvhc1j8sBbFdKFdvUHjKgNETPdbwrmDAFV/dtjomXdXO6KZQQIhusFOkgp2SotMEBXpRM+GmmRGQkLWQHgEVKqdlKqXkADohIgb3NIo8Ne45GtF08/0H6cMlOp5tA5IPpJ0TxhZMhUSIzElx/AkDbVVrhXkYxMPqVOU43gYiIKKZ4B4YSmZHgOk0pVep54n7MIfwOm72pJOTrKk7ub5eWV6Jo/wmnm0GU9NjTR0QUH4wE1yUi4q1rLSLjAey3r0lkxNaS4yFf37DnGE6cKY9o35WVKmRFEjMe+XI1zn7xBxw+WRp+ZaIkVVFp/8Xuc5d2RaPaWchMY2URSny8WKREZiS4vgPAQyKyQ0R2AHgAwO32Nous8NdZmyPa7sHPV6Pdw9MsacO8wgMAgOMRBvpA/PTCkxOS4wt2x8GTth/j4p5NsfChc5CakhznjIgoURmZRGYLgAEiUguAKKWO2d8sCsdIvHmmrCKifX+01LoBh9EExhxklryMfy6qb9UPIiJKTEbqXD8rInWVUseVUsdEpJ6IPB2LxlF0pqzeg3W7Iqs2YhVPjBNuGuvCfccsS0UhIiIicoqRtJAxSqnDnidKqUMAxtrWIjLESMfc/uNnMPZVZ6uN7D5yGgBC5lzvOnwKo17+EU9/vS5WzaIEkSw90LwJQ0RUfRgJrlNFJNPzRERqAMgMsT5ZZEnRQcv2deD4GVTGYFBVMD9sDF7d5OAJV+C9pOiQz/JkCawoEH+0of37lv5ON4EoLvB7gBKRkeD63wBmicgtInIzgG8BvGdvs6iyUuGKSQuCvm4ml7n40En0fnom3pi9xYqmxVy4lBKiZDOkXQOnm0BERBEKG1wrpV4A8DSATgC6AHhKKfW83Q2r7iotvFzfddiVmvHDxn2W7ZMoFmJ1XXX4RBkA4ERp5FVtQjHzPnIyw44zJ6o22LdCichIzzWUUt8opf4A4DEAeSIyxd5mkR2cvL3GP5CkFW+3eo+5S0V+tMS6SjlaZkpRntelkS1tICKi2DBSLSRDRC4WkY8B7AZwDoBJtresmrMy9vAEtmb3Gev60lv3h54Yh6qfeAvCI/Xtur2G171jeGsbW0JERHYLGlyLyLki8g6AbQAuB/A+gINKqZuUUv+LVQNJX/GhU4bXTZRO49NlLMVHvj60sCd53a6jOBkm7cOuC0ozY4l5l4eIKLGF6rmeDqANgCFKqevcATWjnxgJ9x3/7vyiCPZpLnCwMs7gVLakpWJcL+R0WQXGvjoHd/77p5Dr2dYqE79MnlUfGdfJpsYQEZGdQgXXvQEsBDBTRL4VkVsApMamWWSliNNCbGiDGR9bOFMkxZdZ62M7uNYzQdGy7YfCrGkPMz3XnlVza2bY0haiRJIsqWFUvQQNrpVSy5VSDyil2gB4HEBPABkiMk1EbotVA6sra3v2Ius1jnXOtb+PlxY7enyyz/HT9lTl8CirqMS01bsDPsPhPtNWfuQXbDmAfcdclXrMVP/xrOpgWXoiIoqC0Woh85RSdwNoCuAVAAPtbBTZc7VuZJ+nSiusPzCcz/suq6hEaXlgVtO2/Sewxz2LJFlnzuYSFEyYgiMny3Rftzuv+LXvC3Hnf37CjHV70e3x6d4qILGMV6/5x0Jc9Ld5UErh7z8YrzHvubC2shwnUaLiGARKRIaCaw+lVKVSarpS6ia7GkTWC/fH6VRpBbo89g2+XbcXD3+52rtc+9X+6qzNeETzmt2mr91T1Q4LgozhL3yP9o9MC1g+4sUfMOC5WVHvv7q57q1FKJgQvCLn698XAgBu+OdiFO47FvC63RMD7XbXdt937AyOni7H01PWAwBO2nTxGMyeo6fxw6bgs5OG4vSdI6J4wF8DSkSmgmtKTjsPncSJ0gpMnLYe2/af8C5XChj9yo+498PlePnbTfj3wh04dKIUHyzaYfoY/rGUXi+yx5i/zsHt7y8zfYxQdhnond5Schz3frjcm59Lvg6fLMXt7y/F4ZOlmFu439A2K3YexnVvLQ76ekWlwuGTpVY10cvzedtzJLCqzvjX5mJSkNlK7RhoaTYFplWDmgCYFkLVHHusKYExuI5TtqSFhHl9S8mJgGUb9hzDVyt2eZ//9qMVeOiL1diw56ipYz87dYNPQP3k12uDrrt+d/B9j39tLt5bUGTq2Ebd/8lKfLViF1YVHza0/umyCjzxv7WmJgix0vIdh9D/2ZkhL1S0/rNoOwomTEF5hBcP78wrwvS1ew1VqtFWh6kI8WF+fPJa9Hjy24jaE0qKO7p+/fvAIHpl8RFMnLZBdzu9c/n+giJc9NpcaxsYxKTreiEzzTVunGkhVK3x408JzFBwLSKpItJERFp4/tndsOrOyh40b5ijFI6fKcfHS3b63HLWfof7PNZpw4ETZwAAZeXm27fjYFXwvqr4iOntAVdg9NhXwQNzKxw9ZSxYfnH6RvxzXhEe/iLydJlDJ0rx82HjNcs99h09jUv+Ph97j57BizM2epcfOVmG1Zpz++Xyn3HguOtn9qw7NaL9I9Mw4sUfIm6zEdo7FSXHzgRUflFKoXDfcby/cLs9x49wO71BtI9+tRario9gadFBnC4zllaizTU385ui/f1jzzURUWIyMkPjbwDsBfAtgCnuf1/b3K5qz9Ia05pI59Ev1+CPn63yKUkWrOc1VBuiDf4rlcJZf5qODxebTzExoqJSYdPewFzfYLSpCTe9uyToemt3HcG2/Sew9+hpvDV3GwD49Owb8dAXq1EwYQpOlVag/3OzMHjid97X/vHjVszdHD7l4vq3q1ItdmmC86veXIAL3WkPxYdO4rcfrcBN7y7Byp2HccKdb1yp4JP+M2dzCVbuPIy1u45g3a6jKJgwBTsOnAx67HW7qu4sVBiMAP/46aqAZd+s2W1o24gYiK7/Nmszvt9gvCTg5ZMWYMBzswIuhiav3IVDJ3xTWzZqPnuVJqJk7ZrMuaZqjWkhlMDSDKxzL4AOSqkDdjeG7KXg6kUEgFOaHjijaQWAdZPBlJZX4viZcvxp8lp8duegiNoSyl++3YTXvi/Et78bhnb5Od7lv/toBUrLK/H6tb18cquHPP892ufXCrvfca+60gPa5NWMuG2enPVOj30T8NozU129y0UTxwW8VlZRCQGQlpriE+At3HrQ+3jDHldQN3HaBhxyXzCsKj6C8a/PC9hfZaXC8dJyn0DdY+b6vbh5SCvd9i8pqjpeWUUlUlMiK39/5JR+JRErGPmcvvTtJgD65zqYwyfLcMGrc7D8sfMAABv2HMU9/12OAa1z8X/X98HMdXvRvXkdPDNlnXebR75cY7L1LmaCciIiih9G0kJ2AojsHj5FzKkq105/nb/t7g0Op0jT83rkZBkK9x33ed3TM7/PfTHh8cXynzFltavHdNb6vd7lx8+U46cdh4Meb9v+Ez49xHr56VrbD5xAu4enYmvJ8ZDrafm/h50HT6JgwhR8s8ZVOaXDI9Mw/M8/BGy3//iZgGUA8H+zt4Y8XuuHpmJbkPehAJ+Lj2emrMOrszYD8L0TMuavc/D1ql1o9/BUvDVnK16esREb9hwNW6Gm+NAp/GOOsZ91JKItRjJ3835MW70bd30QOKPjIU3Kx4TPXClBC7ceRPcnZuD3n6zEqJd/xEpNak6kOfmMrYmIEpORnuutAH4QkSkAvN/iSqmXbWsV2UJ7l7m8UuH+T1bi9uFtgqZ4TA6R7hDtHWtPz6L/bp7/Rn+gmb/Xvi/EExd1Qc3MNFzw2hzsPHjKpwfS855CBVlG38PyHYdwyd/nG1sZrjSLJdsOoqxCYcJnq/Gvm/vh+W82YGi7Bth7VD8QBoDXvtvs83ztLleA9vlPxRjRMQ+VCvj58CndEniVlQpzDFbw0NLr0QaA56dtwFNfr8PGp0fj9e+3+ATCBzUpENv2n8DdHywHAG+5u1e/K0SN9NC92UYGRcbKqJdn+zwf/9pcn+BYT99nZuKRcZ2wYudh29rFAY1ERInJSHC9w/0vw/2PbLb36OmQFTPMqpr+XHkfb9h9DJ8sK8baXUfxyAWdqlbWfKGv2RUYYFhVntiTk2okDUSvJvKny4pRXlGJV67uiZ0HAwcEenr9BIJn3akW/j77ydgMkNtD5B97FEyYgvdv6Ycjp8q8wSYALC46iEvfmI/1u4+GDCiveXMhmtSt4X0+7tU5WOvObS6vVHjws9CDJl+csdHURCXhlLp7rc+UV3p7rM04ZXDgn11STHxO/e8YhAusAVd61b0frjDZqvAYTxMRJb6wwbVS6olYNISq9H82dpOa+H+XawOLWH3RB0tr0Pr8p2Lc9/FKn2VfrtiFV67uGbDu0qKD3qBdBHjzx8D0iD1HTmPmemOD2YxeUHy5fJduwG7kQmnB1gNokZvtfb5WM2jwOwOD7qwMrJOBXtWPRJORxkqpRESJKOhfbxF5xf3//0Rksv+/mLUwCe07ehqPfrkm7icridVt6Rv/Gbw6B+DKn/YPrD20qRSb9h7D5r3HcPmkBd7b9SlBIuNwszI++b91KJgwBafLKgz3UEZb3WHHwfA95JR86manex9rU7Su6ceKp1R99W+Vi94t6+GhsZ3Cr0wUZ0L1XL/v/v/FWDSkOnnoizWYuX4vhrfPw6jO+T6vfbdhb5CtIldW4frCDhr7BVkeqsxaJGGkHbH6izM2eR8/8Nkq/OG8Dj6vR5rG8s48V46xkbJ4Hp8v/zmyg8UxM6XqKDLjuzfBvxYE1vtOT3X1fYw5q1Gsm0TkuOyMNJ9KUkSJJGhwrZRa5v5/drB1KDKheoRvfnep5ce77A3XYLy1u46ie7M6ADR52Cp4xeoPl+wM8kp82nX4FK59a5HPsmhTxKt7CqwdecXkKzuz6s9wv4Jcn9c2PT0GaWYSyImIyHFGJpFpJyKfisg6Ednq+ReLxiU7qwYHmmFksFai0qvE8fWq6CYqeWXmpvArEUXh3nPaeR+fLvNNFctIS0EKg2siooRiZMTMPwG8AaAcwAgA76EqZSQkERktIhtFpFBEJui8Pl5EVonIChFZKiJDjG6byOJh5rUXp1dNmW2mOYn2NR9tyTftwEIiq3VuXBtZYcoWEhFRYjESXNdQSs0CIEqp7UqpxwGMDLeRiKQCeB3AGACdAVwjIp39VpsFoLtSqgeAmwG8ZWLbhLX6Z1fvsRM91x7lUc5QsXHPUXy2zFxFhqveXBjVMYmSzdiurnzq5rmuMozBk7SIiChRGAmuT4tICoDNInK3iFwCoKGB7foBKFRKbVVKlQL4EMB47QpKqeOqqhu3JqpSXMNum8j2Hy8Nv1Kc2uyuCfzAZ6vx+09WomDCFJz3F1da/mvfbUbBhClBa1drJx8hqq7a59cKWOadVImxNRFRwjMSXP8WQDaAewD0BnAdgBsMbNcUrqnTPYrdy3yIyCUisgHAFLh6rw1v697+NndKydKSkhIDzYof/l+k/1kUWDHAbhv2HMN1by8Kv6LbydLAyUE27T2O2ZtKvCkYh08xiCYKRjTJVZ6/AZ60as7KSESU+EJOIuNOz7hSKXU/gOMAbjKxb72kh4BvDqXUFwC+EJFhAJ4CMMrotu7t3wTwJgD06dMnob6ZtD28o16eHTBTXCK54Z3F3scVlQr3fbwC2/afcLBFRPFJmw7m+YPlqcceZbYWERHFgaDBtYikKaXKRaS3iIgyPwqvGEBzzfNmAHYFW1kp9aOItBGRBma3TVTaL9lEDqz9Ldt+CJ//lHw1n4ls4/1bwOiaiCjRhUoL8XRFLgfwlYhcLyKXev4Z2PcSAO1EpJWIZAC4GoDPzI4i0lbEFWKKSC8AGQAOGNmW4tfdHyx3uglEcev5y7p5H1elhTDnmogoWYRMC3HLhSvgHQlXt4q4//881EbuXu+7AUwHkArgHaXUWhG5w/36JACXAfiliJQBOAXgKncPue62kbzBeKYUUF5RiZ2HTjndFCKKke7N63ofe6qDVOVcO9AgIiKyVKjguqGI3AdgDaqCag9DXwFKqakApvotm6R5/DyA541um4xuf38ZZnGKaaJqpU6NdBw5VYZ62RkAqgY5ckAjEVHiC5UWkgqglvtfjuax5x9FSQEMrImqIU996/RU159gz/gLxtZERIkvVM/1bqXUkzFrSTXwxP/WYn7hAe/zmev2OtgaInKaJy2kc5Pa2LDnGGplGsnUIyKieBbqL3mizXQd9/45r8jn+efLWVGDqHry/fP67CVdcU2/FmhRP9uh9hARkVVCpYWcE7NWEBFVK775H1npqehbkOtQW4iIyEpBg2ul1MFYNiTZXPr3eSiYMAUvzdjodFOIiIiIKEaMTH9OJlVWKvy04zAA4G/fFTrbGCKKQ8y6IyJKVgyubfDB4h0+z0+VVjjUEiIiIiKKJQbXNth//IzP8zv/swyHTpQ61BoiIiIiihUG1zYQv1u+P2wsQc+nvnWoNUQUb67s0wwAMKxdnsMtISIiq7GoKhFRjPVsUQ9FE8c53QwiIrIBe65tIByrRERERFQtMbi2AWNrIiIiouqJwbUN2HNNREREVD0xuLbBizM2Od0EIiIiInIAg2siIiIiIoswuCYiIiIisgiDawvtP34Gt7+/1OlmEBEREZFDGFxb6LGv1mD62r1ON4OIiIiIHMLg2kJTV+9xuglERERE5CAG10REREREFmFwTURERERkEQbXREREREQWYXBNRERERGQRBtcJoF9BrtNNICIiIiIDGFwngGsHtHC6CURERERkAINri/xrfpFt+04RsW3fRERERGQdBtcW+dPktbbtm7E1ERERUWJgcJ0ARnRo6HQTiMgCeTmZTjeBiIhsxuA6AdTMTHO6CURkgev6t3S6CUREZDMG10REMZJfmz3XRETJjsG1zdo1rOV0E4goTqSmcAAFEVGyY3Bts3M75zvdBCKKExf3bOp0E4iIyGYMrm2m3P+P6JCHL+8a7GhbiMhZ6an8k0tElOz4lz5G+hTkokfzuk43g4iIiIhsxOCaiIiIiMgiDK5jrE/LemHXqZudHoOWEFEsTRjT0ekmEBFRDDC4ttllvZohNUVwQbfGAIBP7xzkfW3KPUNM7+/pi8+yrG1EFDsDWtd3uglERBQDnJ3EZm0b1sKWZ8fqvtalSR3T+2udVzPaJhERERGRTdhzHYeUCr8OESUWVrgmIqoeGFwnGgbetuAdAXuM7doo6n1cN6CFBS0hIiKKDQbXcUhV067rGumpjh37xkEFjh07mZmt6/ynCzvjszsH+izLrckpw4mIKHHYGlyLyGgR2SgihSIyQef1a0VklfvffBHprnmtSERWi8gKEVlqZzut9N9bB9iy35EdG+KmwQW27JvIKtFO733T4Fbo3TLXZ1njOllht+vezPz4BSIiIjvYFlyLSCqA1wGMAdAZwDUi0tlvtW0AhiulugF4CsCbfq+PUEr1UEr1saudVhvYJvqKADcPaRWw7J0b++JPF3aJet+kLy0luW/i/P3aXiFf79gox5LjvH9Lv4i3vf/8DgHLGtfJMhSwD2nXAADQxEAg7pSC+kw9IiKqDuyMKPoBKFRKbVVKlQL4EMB47QpKqflKqUPupwsBNLOxPQnjt6Pao3+r3PArkmUu753cH70uTWqHfL1Ojehrq7fIzUa/At/Pbc1M4wWJ8nKq0j+ClZwc36OJ7nJPSlFGWhxfJHFEIxFRtWDnN1FTADs1z4vdy4K5BcA0zXMFYIaILBOR24JtJCK3ichSEVlaUlISVYOp+op1UOZkfrldRACRqggyNUVwUXf9YFiXZqjByI4NA5a3bVgLf726J35/bnufzYwMRp1271D8+fJuuH14a9RzaJImYXBNRFQt2BlR6H2V6I7UE5ERcAXXD2gWD1ZK9YIrreQuERmmt61S6k2lVB+lVJ+8vLxo2xw3gg1pTPahjjPvG+50E2LipSu7h1/JQs3rZeOafi3QL8gdEavSQrSu7d8CKSYiyhoZoS84ejavCwBonpttui2dGtfGFX2a48ExnXwuAMz68LYBGGRB6hcRESUvO4PrYgDNNc+bAdjlv5KIdAPwFoDxSqkDnuVKqV3u//cB+AKuNJO4NqpTw/ArhXGhp6fPgSi6Qa2M2B9UQ8TVOxkP2tncjsFtG9i6f38pKYLnLu0aNIju0rQOfrx/hOXH9a988+kdA4OsCYzr2tjQPmvX8E01CRcqX9WnedDXHhnXydAxPQa0ro9aJlJdiIio+rEzuF4CoJ2ItBKRDABXA5isXUFEWgD4HMD1SqlNmuU1RSTH8xjAeQDW2NhWS9S3sGSYMhFdh8unNap7s7qW7CeeXdPPVTNZm0owqlN+wHrBSsjpdXrePaItAKB3y3rIrZkRdJ9akeY43zOybUTbeQQLRAVAbpQXV6Kzf6Of4n6tcpESZOCi/+/CiA7BL2L1eqXr1vQ91y9r7hoYGSzZPr8WFjw4Eu/c6BpX/Tu/tBR/M343DE3r1gi7XyIiSk62BddKqXIAdwOYDmA9gI+VUmtF5A4RucO92mMA6gP4u1/JvXwAc0VkJYDFAKYopb6xq61W8RScuMKCwXGRlLru2aJuVMf89Yg2UW2fCJ67tCuKJo7zWfbqNT18nv9xdGDVilD+cH4HFE0ch8/uHISXrnAFbqHSPmpnRd7zed95xtqWX9v8hV609dXza4ev1NHVYMk8T056lyZV63viZv8AOr92lvf3RTdU9ntbZ2uCc6NvuXGdGhjZ0XXB1Klx6IvZ9vk5+PKuwfjoNnvKchIRUXyzdRSXUmqqUqq9UqqNUuoZ97JJSqlJ7se/UkrVc5fb85bcc1cY6e7+18WzbaL48xXR59MG+87v3bKebTmfZif8MMts0Gor9wmeed9wZGf4Bruheka1nr+sKz6+3TfNYUTHhiiaOC5kz/Q1/fVnHExNEZ8LsxSB4drmb/3St1qlBOmjNlIqMpK0h1eu6oE3rusddr3MtFT9Sjh+H/h6NTPw+a8H4a9X9wgZAP/lqu54/Rehywzqbe6ZNMiuQYZ5OZno35q52URE1VEc161KPDUzrMvFDNaLmJWeig9uHYCnxidezetfnx06paF5vfAD1ZrV873d7n8e0lIE/7rZeHq+Nrha/fh5WPbIKN2eya5NA3tcr+obfICg1twHjOUyb3l2rM+F2QuXdzdc23xU53y8e1NfAMCk63oHDRpHn9UYH9za39A+tRY/dE7I1y/u2RS5NTN8jmumI1wvDapXi3o+pfz0Lhgu6dkM9fyOG7BvnYY8flGXgDsYREREVmBwbaE0gz2/U+4ZEhAA/vXqHj7Pw8Ul1w8ssHzQXba7WsOTMQjcX9QGkZd1AwBMGNMx7HYvaba7tFdTXD+wAOO6uQbC3Ty4FQqfHYvh7fOQofOzCDfFeU5WOurX0k+n8ORom+noPL+LK40gJzMdl/QMrEL5+a8H4UOd1IFh7V1Vb8zeSDi7Q0Mse2QURp/VKGRgO6hNg4Cc4Np+Pe3+Aw89ueR2iTIjJSSzZRY9udUeOVmRl+574qKq36XsJCy/SEREgRhcW8jozM9dmtTB8PahywYaCTb8V4n2DndqSgqKJo7DLwcWRLCtuaNrJ225sm9zFE0chxEdw6djeHoy69RIx8tX9vB5TZtz7pmxz+OxCzrjcU2g82v3IMRGQfKEf+GXuvHcpV0BAFf3a4FHL+isGxT7e+Wqnvj6N0NQJztd92fTq0U9DNBJHfDUYTZTxs5D7+KgQ35ghZDBbauOWy87Hed1zvf5PPUpyA0IMrc8Oxaf/3pQyONr86FF9D/Hem8r1Mdd77XOjWvjHL3Pi86+7xphbhBobb9g+nejAgcweqqMZKWnYNJ1wdNSbhhUgKKJ41A0cZzhi28iIkpsrClloUMnyyLedqA7yLpxUEurmmNaNMH5kLYNMHtT7CbxaRJlNYbLezcLOSvjdQNa4pEvqwrUZGekYcNTo5GRmhK0qoW/GhmpOEsnnSRYPrTHI+M6o2ZmGsacZaw0nZ70tKpjXBRkVkOPt27o6xsUu/8f2TEfKQJUKlfQnJoi6NWinuE2KKWf7qEXcBsZTKkNyqfeO9RwO/xz6sO1xT/4z0wPDIqz3L3Ql/ZqhtFR/JyIiCj5sCvFQv9dvMP7uGndGvjtqHZht/FM59ywdhaKJo5D75auHF4jd8n9A5JoJseIV5398p/13qInBSRUT2/jOuErWYSTlZ5qOLAOYGKzvJxMPHtJ16hmjezWtG7I1+1Mw0gWaSmCa/u3MHVBQURExJ5rm8ybMNLQei9d0R3PXNI18AUHop9IY/NIp/Lu07Ielm4/FHIdTx74C5d3Q73sDDRzD3q8YWBVD/9jF3RG/ZoZ3hxnwPfC46o+zTH6rEYRtRFw9cpbKSbXQJpj5ERR+u+ec9rhlZmbda8NujWrg1XFR3DLkFY+y+8/vwP+PH2jZe/z4h5NsXjbQfzh/ODVZiL9dQnVxua52fq/mxYcl4iIkheDa4elpaaglk4uppHv7Pb5OdhScsIbgJqNZdJTBWUVVUcKl67g7w/ntceF3ZsgJysd/1m43XRayLs398Puw6cMrduqQU30LXD16vtXeahXMwOPXNBZd7uM1BQ8e2nXqHr1uxmszRzK1X1b4POffgYQWam7UNY/OTroa71b1sMv+umX/vPnuQPQu6Cqp/a3o9rjtzo5xwDw5a8HQyEw3z7c+zObc10jIxV/uapHyH3aIVSqShLeJCIiIoswLSROGekRe/GK7vjPr/p7849HGpx+/fVf9MJndw7C9384O4oWurSsXxO5NTNw14i2+O73w03NTFcrMw3tdAbbaUXbMTjp+l6mB1t6ePLgrei57tcqF4XPjMEj4zrh1qGto96fVo2MwDsH957TDu0a1sI7N/TVHUind16z0lMx9Z6h+Pu1oetGe6SkSPhzq5tfbWxZJMz+pMsqKi05rqcqSIMg1WaIiKj6YM91nDIy/XnNzDQMbtsA2Rmp+GrFLlzeqxmy0lLx5NfrQm7nKV33s1+vcTS9cSkpgtZ5wUsDekq5FdTPxlV9jfWkajnZUWjVnf+01BT8Siew7pCfg73HTofdvmOj0BciWu3zc/DtfcMNrl31Djs3CT37oF2cyq6Yv+WAzlLzn7brBrTE4m0Hcesway+ciIgo8TC4tlCkPaR6xpzVGGt+Pmpo3Z4t6kU0IYb/be9MnQF0DXMyse/YGdP71vryrsHemtw/3G9sQhV/ZoOvrs3q4vuNJYam5A4mVrf+p/9umKH1bh7sm9v8n1/1j/ii4/wujfDpsmIAcZI3HBeNcMlyVwdpWb+m4W1SUwSvG+zxJyKi5Mbg2kLhJikx49dnt8HyHYdwaa/g5eL0dND0bo7u0ghdm9XBn6dvDLlNTmYa/npNDzTUCUS/uGswmtTJQtfHZ+D4mXJTbQGAu0a0QY/mdU1v5xFp8HjvOe1wfpd8dGkSeb70L/q3wPwtB9Au39rJeqwyOIp0lXM756NXi7r4acdh6xrkx1WKL5DZnOtYa1ynBt6+oQ/6FISffZOIiMgfc64tZGSGQaNEBG/d0Bdju5qroTu4bQP8cbSrqkJKimsCjeyMVLTIDT61eO0a6RjZMd9n2arHz8PGp0ejad0aEBF8ducg3D2iLdY+cX7I4/v33utNwGFGpEFXaopEFVgDwAXdmqBo4jg0zIm+jF888gzyjKfANlKe93BOp/yQ6/nTflrrZldNHnNOp3zUqRH5zIxERFR9Mbi2UCQz6tmhZa7v7ex1T47GvecEr7mt1+zaWenITKsaKNehUQ7+cH4H7wyJwbx7U1+f55HMSvesTvmz+Dizzriyj+vuxfAOoWf1NMtzTu3KyAg2Q6OdAxr1UpuIiIhiid9E1cwwzbTrTevWwO3DWuPdm/pFtC+9XsJQgxqN8p96vLp74fLuKJo4Lqr8cT2entr0VHsuXcwEzEYG8NqtOl/AERGRdZhzbaF4+XLOznT1OHsqdABVvdO5mlvfIoIHx3aK+DidGttXWeLTOwZia8kJfLR0J4DkSF2IN3++vDs+X/5zVDnxkYiTGzxERES2YHCdhM5un4fnLu2Ki3s09S7zBDSJEqT2KchFn4JcDGxTH3//YQt6xjgArA7q1cwImF3RSmaCaKeKhWhTuRLld4OIiOIb00Ki9OT4Lt7H8dIjJyK4pl8Ln8lFPLMvxlHFM0Oa52bjuUu7RpS7Tc5SyvjvhFOfS732xcmvMRERJShGLFFKS0mMU2hl4D/put744Ff9rdshJRXtZ21A6/oB9bmv6ts8YBsrY+uW9YNXxgk4rt7gSgvbQkRE1U9iRIZxTDsQS+Kl61pHc3cpvq5NoytPBwCjz2qEQRZMCU7JSRuwpqYIHruws8/rl/RsFjDpkf+ERtGYff8I9C2oZ2hd7YyUZn57R3ZsCAC4loNviYjID3Ouq4leLeph+m+HoX0MJ0R57Rc9Y3Ysij9xfK3ppa1lbSa8b1ynRkSzohIRUfJjcB2lRMph1s7eGAsXdGsS0+NRfEmk3w2tBLgmICKiOMa0kCglaPxARERERDZgcE1EtnCyFJ+w/5mIiBzCtJBoJeq9b0pYyx4ZFdeDZz0SbYbG7PRUHEaZ080gIqIEx+A6Ss6HBFTd1K+V6XQTQook7o/2GjVg+wja8MGtA/DN2j2op5nZlIiIyCymhUSJHddEviL5nbDq1yia/vyCBjVxx/A2FrWEiIiqKwbXROS4xnWyHDluIqTXEBFRYmFwTUSWiiRevbx3M+sbYoCVk9cQEREBDK6jxi9nIl/8lSAiouqMwTUR2SJcD3ZOln3jqY12njMthIiIrMbgOkrspCPSF64He8GD52BY+7zYNCYIhtZERGQ1BtdR4i1wosjUykxD3RrpluwrHupkExERAQyuicgmjmRcuA/KbA8iInIKg+sosb+MSJ+pGRod+kXi7y8REVmNwXWUWC2EyJeZXmPPukzrICKiZMHgmogsZeZ60+nsDaePT0REyYfBNRE5jmkhRESULOwrNFtNMCuEyJdeWsjLV3ZHvewMnXWt6Tuulen6U5bj/l/YJ01ERA5hcE1EltK74Ly0l73Tm98wqAAigl8ObGlqO4bgRERkNQbXUerVsq7TTSBKWJ7gNto7QOmpKbhlSKuq/RqMmlmyj4iIrMbgOkq9W+binI4NMahtA6ebQhQXTAWs3moh1spI43ASIiJyhq3fQCIyWkQ2ikihiEzQef1aEVnl/jdfRLob3TaevH1jX59eMyJy1guXd3O6CUREVE3ZFlyLSCqA1wGMAdAZwDUi0tlvtW0AhiulugF4CsCbJrYlojgUSYqH1fXiG+ZkWbo/IiIio+zsue4HoFAptVUpVQrgQwDjtSsopeYrpQ65ny4E0MzotkSU+FjVg4iIko2dwXVTADs1z4vdy4K5BcA0s9uKyG0islRElpaUlETRXCKyQiINEkygphIRUYKwM7jW+97SvfcrIiPgCq4fMLutUupNpVQfpVSfvLy8iBpKRNYxk+FxVtPaAIDmudk2tSY0q+psExERedhZLaQYQHPN82YAdvmvJCLdALwFYIxS6oCZbYkosd04qAD9W9VH5ya1nW4KERGRJezsuV4CoJ2ItBKRDABXA5isXUFEWgD4HMD1SqlNZrYlovhkpjNYRBwNrNlxTUREVrOt51opVS4idwOYDiAVwDtKqbUicof79UkAHgNQH8Df3bdny90pHrrb2tVWIrKOxYU/bMXYmoiIrGbrJDJKqakApvotm6R5/CsAvzK6LRGRpdh1TUREFuM0ZkRkqUSKVzPdMznmZHKyWiIisgaDayKqtjzXAed2yXe0HURElDwYXBNRtcfJbIiIyCoMromIiIiILMLgmoiqPaU/RxUREZFpDK6JyFKJVIqPiIjIagyuiajaY841ERFZhcE1EVkqkUrxeTAthIiIrMLgmoiqLUnEKwEiIoprDK7JFimMWYiIiKga4rRkZLlXr+mJs5rUdroZRGE1rpMFAGiTV8vhlhARUbJgcE2Wu6h7E6ebQGTI4LYN8NFtA9CnINfpphARUZJgcE1Elsqv7eoNbpGb7XBLjOnfur7TTSAioiTC4JqILHVe53y8d3M/DGnbwOmmEBERxRyDayKylIhgWPs8p5tBRETkCFYLISIiIiKyCINrIiIiIiKLMLgmIiIiIrIIg2siIiIiIoswuCYiIiIisgiDayIiIiIiizC4JiIiIiKyCINrIiIiIiKLMLgmIiIiIrIIg2siIiIiIoswuCaipHVZr2ZON4GIiKqZNKcbQERkh6KJ45xuAhERVUPsuSYiIiIisgiDayIiIiIiizC4JiIiIiKyCINrIiIiIiKLMLgmIiIiIrIIg2siIiIiIoswuCYiIiIisgiDayIiIiIiizC4JiIiIiKyCINrIiIiIiKLMLgmIiIiIrIIg2siMqV+zQynm0BERBS30pxuABElltl/HIEzZRVON4OIiCguMbgmIlNqZaahVib/dBAREemxNS1EREaLyEYRKRSRCTqvdxSRBSJyRkT+4PdakYisFpEVIrLUznYSEREREVnBtu4nEUkF8DqAcwEUA1giIpOVUus0qx0EcA+Ai4PsZoRSar9dbSQiIiIispKdPdf9ABQqpbYqpUoBfAhgvHYFpdQ+pdQSAGU2toOIiIiIKCbsDK6bAtipeV7sXmaUAjBDRJaJyG3BVhKR20RkqYgsLSkpibCpRERERETRszO4Fp1lysT2g5VSvQCMAXCXiAzTW0kp9aZSqo9Sqk9eXl4k7SQiIiIisoSdwXUxgOaa580A7DK6sVJql/v/fQC+gCvNhIiIiIgobtkZXC8B0E5EWolIBoCrAUw2sqGI1BSRHM9jAOcBWGNbS4mIiIiILGBbtRClVLmI3A1gOoBUAO8opdaKyB3u1yeJSCMASwHUBlApIr8F0BlAAwBfiIinjR8opb6xq61ERERERFawdSYIpdRUAFP9lk3SPN4DV7qIv6MAutvZNiIiIiIiq9k6iQwRERERUXXC4JqIiIiIyCIMromIiIiILMLgmoiIiIjIIgyuiYiIiIgswuCaiIiIiMgiopSZGcnjm4iUANjuwKEbANjvwHETFc+XOTxf5vB8mcPzZQ7Pl3k8Z+bwfJnj1PlqqZTK03shqYJrp4jIUqVUH6fbkSh4vszh+TKH58scni9zeL7M4zkzh+fLnHg8X0wLISIiIiKyCINrIiIiIiKLMLi2xptONyDB8HyZw/NlDs+XOTxf5vB8mcdzZg7Plzlxd76Yc01EREREZBH2XBMRERERWYTBNRERERGRRRhcR0FERovIRhEpFJEJTrfHSSJSJCKrRWSFiCx1L8sVkW9FZLP7/3qa9R90n7eNInK+Znlv934KReRVEREn3o/VROQdEdknIms0yyw7PyKSKSIfuZcvEpGCmL5BiwU5X4+LyM/uz9gKERmrea26n6/mIvK9iKwXkbUicq97OT9jOkKcL37GdIhIlogsFpGV7vP1hHs5P186Qpwvfr5CEJFUEVkuIl+7nyfu50spxX8R/AOQCmALgNYAMgCsBNDZ6XY5eD6KADTwW/YCgAnuxxMAPO9+3Nl9vjIBtHKfx1T3a4sBDAQgAKYBGOP0e7Po/AwD0AvAGjvOD4BfA5jkfnw1gI+cfs82nK/HAfxBZ12eL6AxgF7uxzkANrnPCz9j5s4XP2P650sA1HI/TgewCMAAfr5Mny9+vkKft/sAfADga/fzhP18sec6cv0AFCqltiqlSgF8CGC8w22KN+MB/Mv9+F8ALtYs/1ApdUYptQ1AIYB+ItIYQG2l1ALl+g14T7NNQlNK/QjgoN9iK8+Pdl+fAjjHc8WeiIKcr2B4vpTarZT6yf34GID1AJqCnzFdIc5XMNX9fCml1HH303T3PwV+vnSFOF/BVOvzBQAi0gzAOABvaRYn7OeLwXXkmgLYqXlejNB/nJOdAjBDRJaJyG3uZflKqd2A68sMQEP38mDnrqn7sf/yZGXl+fFuo5QqB3AEQH3bWu6cu0VklbjSRjy3CHm+NNy3O3vC1VvGz1gYfucL4GdMl/uW/QoA+wB8q5Ti5yuEIOcL4OcrmFcA/BFApWZZwn6+GFxHTu+KpzrXNRyslOoFYAyAu0RkWIh1g507nlOXSM5PdTh3bwBoA6AHgN0AXnIv5/lyE5FaAD4D8Ful1NFQq+osq3bnTOd88TMWhFKqQinVA0AzuHoJzwqxOs+X/vni50uHiFwAYJ9SapnRTXSWxdX5YnAduWIAzTXPmwHY5VBbHKeU2uX+fx+AL+BKm9nrvk0D9//73KsHO3fF7sf+y5OVlefHu42IpAGoA+NpFQlBKbXX/YVVCeAfcH3GAJ4vAICIpMMVKP5HKfW5ezE/Y0HonS9+xsJTSh0G8AOA0eDnKyzt+eLnK6jBAC4SkSK4UmxHisi/kcCfLwbXkVsCoJ2ItBKRDLgS5Cc73CZHiEhNEcnxPAZwHoA1cJ2PG9yr3QDgK/fjyQCudo/ebQWgHYDF7ts+x0RkgDsX6peabZKRledHu6/LAXznzjlLGp4/sm6XwPUZA3i+4H5/bwNYr5R6WfMSP2M6gp0vfsb0iUieiNR1P64BYBSADeDnS1ew88XPlz6l1INKqWZKqQK4YqnvlFLXIZE/XyoORogm6j8AY+EaZb4FwMNOt8fB89AarpG7KwGs9ZwLuPKZZgHY7P4/V7PNw+7zthGaiiAA+sD1B2cLgNfgnkU00f8B+C9ctwHL4LqCvsXK8wMgC8AncA3sWAygtdPv2Ybz9T6A1QBWwfWHsjHPl/d9DoHrFucqACvc/8byM2b6fPEzpn++ugFY7j4vawA85l7Oz5e588XPV/hzdzaqqoUk7OeL058TEREREVmEaSFERERERBZhcE1EREREZBEG10REREREFmFwTURERERkEQbXREREREQWYXBNRJQERKRCRFZo/k2wcN8FIrIm/JpERJTmdAOIiMgSp5RrumUiInIQe66JiJKYiBSJyPMistj9r617eUsRmSUiq9z/t3AvzxeRL0RkpfvfIPeuUkXkHyKyVkRmuGeeIyIiPwyuiYiSQw2/tJCrNK8dVUr1g2vGslfcy14D8J5SqhuA/wB41b38VQCzlVLdAfSCa9ZVwDXF8OtKqS4ADgO4zNZ3Q0SUoDhDIxFREhCR40qpWjrLiwCMVEptFZF0AHuUUvVFZD9c0y+XuZfvVko1EJESAM2UUmc0+ygA8K1Sqp37+QMA0pVST8fgrRERJRT2XBMRJT8V5HGwdfSc0TyuAMfsEBHpYnBNRJT8rtL8v8D9eD6Aq92PrwUw1/14FoA7AUBEUkWkdqwaSUSUDNjzQESUHGqIyArN82+UUp5yfJkisgiuDpVr3MvuAfCOiNwPoATATe7l9wJ4U0RugauH+k4Au+1uPBFRsmDONRFREnPnXPdRSu13ui1ERNUB00KIiIiIiCzCnmsiIiIiIouw55qIiIiIyCIMromIiIiILMLgmoiIiIjIIgyuiYiIiIgswuCaiIiIiMgi/w9owmIm+jZsgAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 864x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtoAAAGpCAYAAACzsJHBAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAAmIUlEQVR4nO3de5BkV30n+O8vs6ofekuWhIVamhagXS8QBoOsxeOJCcCzA/bOGrw2tti1TXjZ1QYLfuy8LHZi18zsaMfjYDxe1mMm8ACGGRsMg1mwsccm5DEOrzGiNQgkgSXEU20JvUBv9aMqz/6RN7tTrapSNdzb2dX1+URk5M2TNzNPnb6d9a2Tvzy3WmsBAAD6NVp0BwAA4FQkaAMAwAAEbQAAGICgDQAAAxC0AQBgAEuL7sBQzj///LZ3795FdwMAgFPYDTfccF9r7YK17jtlg/bevXuzb9++RXcDAIBTWFV9Zb37lI4AAMAABG0AABiAoA0AAAMQtAEAYACCNgAADEDQBgCAAQjaAAAwAEEbAAAGIGgDAMAABG0AABiAoA0AAAMQtAEAYACCNgAADEDQBgCAAQjaPbrrwcdz+z2PLLobAACcBATtHv3Lj96Wn3j7JxbdDQAATgKCNgAADEDQBgCAAQjaPWtt0T0AAOBkIGj3qFKL7gIAACcJQRsAAAYgaAMAwAAE7Z61KNIGAEDQ7lUp0QYAoCNoAwDAAATtnlneDwCARNDuldIRAABmBG0AABiAoA0AAAMQtHumRBsAgETQ7pkibQAApgRtAAAYgKANAAADELR7Zh1tAAASQbtX1tEGAGBG0AYAgAEI2r1TOwIAgKDdK5UjAADMCNoAADAAQRsAAAYgaPfM8n4AACSCdq8s7wcAwIygDQAAAxC0AQBgAIJ2z5RoAwCQCNq9KitpAwDQEbQBAGAAgnbPmvX9AACIoN0ry/sBADAjaAMAwAAEbQAAGICg3TMV2gAAJIJ2r5RoAwAwI2gDAMAABG0AABiAoN0zy2gDAJAI2r0qC2kDANARtAEAYACCds+cgh0AgETQBgCAQQwWtKvqkqr6j1X1uaq6pap+tms/r6o+WlWf767PnXvMG6vq9qq6tapeNtf+wqq6qbvvLaUYGgCAk9yQM9orSf5ea+2/SPKiJK+vqmcnuSbJda21y5Nc191Od99VSZ6T5OVJfq2qxt1zvTXJ1Uku7y4vH7DfAADwLRssaLfW7mqt/adu++Ekn0tycZJXJHlXt9u7kryy235Fkve21g621r6U5PYkV1bVRUnOaq19vE0LoN8995iTjgptAACSE1SjXVV7k3xXkk8keVpr7a5kGsaTXNjtdnGSO+Yetr9ru7jbPrZ9rde5uqr2VdW+e++9t9efYTMUtAAAMDN40K6qM5J8IMnPtdYe2mjXNdraBu1Pbmztba21K1prV1xwwQXH31kAAOjJoEG7qpYzDdm/2Vr7na757q4cJN31PV37/iSXzD18T5I7u/Y9a7QDAMBJa8hVRyrJ25N8rrX2y3N3fTjJa7rt1yT50Fz7VVW1s6ouy/RLj9d35SUPV9WLuuf8ybnHnHwUaQMAkGRpwOf+3iQ/keSmqrqxa/vfkvxikvdV1WuTfDXJq5KktXZLVb0vyWczXbHk9a211e5xr0vyG0l2J/mD7nLSqTWrXAAA2I4GC9qttT/L2vXVSfJ96zzm2iTXrtG+L8lz++sdAAAMy5khe6ZyBACARNDuleX9AACYEbQBAGAAgjYAAAxA0O7Z9CzxAABsd4J2j5RoAwAwI2gDAMAABG0AABiAoN0zFdoAACSCdq+sow0AwIygDQAAAxC0AQBgAIJ2zyyjDQBAImj3qhRpAwDQEbQBAGAAgnbPmgX+AACIoN0rhSMAAMwI2gAAMABBGwAABiBo98zyfgAAJIJ2vxRpAwDQEbQBAGAAgjYAAAxA0O6ZEm0AABJBu1elSBsAgI6gDQAAAxC0+6Z2BACACNq9KpUjAAB0BG0AABiAoA0AAAMQtHvWFGkDABBBu1dKtAEAmBG0AQBgAII2AAAMQNDuWVOiDQBABO1eWUcbAIAZQRsAAAYgaPdM5QgAAImg3auywB8AAB1BGwAABiBoAwDAAATtnjXr+wEAEEG7V5b3AwBgRtAGAIABCNoAADAAQbtnKrQBAEgE7V4p0QYAYEbQBgCAAQjaPbO6HwAAiaDdL+v7AQDQEbQBAGAAgjYAAAxA0AYAgAEI2j1SoQ0AwIygDQAAAxC0AQBgAIL2AJrFtAEAtj1Bu0eW0QYAYEbQBgCAAQjaA1A5AgCAoN2jssAfAAAdQRsAAAYgaAMAwAAE7QEo0QYAQNDukeX9AACYEbQBAGAAgwXtqnpHVd1TVTfPtb2pqv6qqm7sLj8wd98bq+r2qrq1ql421/7Cqrqpu+8tVeaNAQA4+Q05o/0bSV6+Rvu/bK09v7v8fpJU1bOTXJXkOd1jfq2qxt3+b01ydZLLu8taz3lScQp2AAAGC9qttT9N8vVN7v6KJO9trR1srX0pye1Jrqyqi5Kc1Vr7eJum13cneeUgHe6BqXYAAGYWUaP9hqr6TFdacm7XdnGSO+b22d+1XdxtH9u+pqq6uqr2VdW+e++9t+9+AwDApp3ooP3WJM9M8vwkdyX5F137WpPBbYP2NbXW3tZau6K1dsUFF1zwLXb1m6dwBACAExq0W2t3t9ZWW2uTJL+e5Mrurv1JLpnbdU+SO7v2PWu0n5R8TRMAgJkTGrS7muuZH0oyW5Hkw0muqqqdVXVZpl96vL61dleSh6vqRd1qIz+Z5EMnss8AAPDNWBrqiavqPUlenOT8qtqf5BeSvLiqnp9pdcWXk/zPSdJau6Wq3pfks0lWkry+tbbaPdXrMl3BZHeSP+guAABwUhssaLfWXr1G89s32P/aJNeu0b4vyXN77NrgrO4HAIAzQ/bIuXQAAJgRtAEAYACCNgAADEDQHkCzkjYAwLYnaAMAwAAEbQAAGICgPQDL+wEAIGj3yOp+AADMCNoAADAAQRsAAAYgaAMAwAAE7R5VFGkDADAlaAMAwAAEbQAAGICgPQDraAMAIGj3yDraAADMCNoAADAAQXsALWpHAAC2O0G7RypHAACYEbQBAGAAgjYAAAxA0B6A5f0AAHjKoF1Vz6yqnd32i6vqZ6rqnMF7tgVZ3g8AgJnNzGh/IMlqVT0ryduTXJbktwbtFQAAbHGbCdqT1tpKkh9K8iuttf81yUXDdgsAALa2zQTtw1X16iSvSfJ7XdvycF3a+pRoAwCwmaD9U0m+J8m1rbUvVdVlSf7dsN3amspK2gAAdJaeaofW2meT/EySVNW5Sc5srf3i0B0DAICtbDOrjvxJVZ1VVecl+XSSd1bVLw/fta2rWd8PAGDb20zpyNmttYeS/LdJ3tlae2GSvzVst7Ymy/sBADCzmaC9VFUXJfnRHP0yJAAAsIHNBO1/kuQPk3yhtfbJqnpGks8P2y0AANjaNvNlyPcnef/c7S8m+eEhO7XVqdAGAGAzX4bcU1UfrKp7quruqvpAVe05EZ0DAICtajOlI+9M8uEkT09ycZLf7doAAIB1bCZoX9Bae2drbaW7/EaSCwbuFwAAbGmbCdr3VdWPV9W4u/x4kvuH7thWZhltAAA2E7T/h0yX9vtakruS/Eimp2XnGGUhbQAAOptZdeSrSX5wvq2q3pzk7w/VKQAA2Oo2M6O9lh/ttRcAAHCK+WaDthqJjajRBgDY9tYtHamq89a7K4L2mgwKAAAzG9Vo35Dp3Oxa+fHQMN0BAIBTw7pBu7V22YnsyKmkqR0BANj2vtkabdZgdT8AAGYEbQAAGICgDQAAA3jKE9YkSVWNkzxtfv/uRDaswSnYAQB4yqBdVT+d5BeS3J1k0jW3JN85YL+2JCXaAADMbGZG+2eT/OettfuH7gwAAJwqNlOjfUeSB4fuCAAAnEo2M6P9xSR/UlUfSXJw1tha++XBerXFKdEGAGAzQfur3WVHd2EdZSFtAAA6Txm0W2v/+ER05FRwx9cfS5IcXFldcE8AAFi0dYN2Vf1Ka+3nqup3s0Y1RGvtBwft2Rb0b/7sS0mSP73t3vzYd1+64N4AALBIG81o/9vu+s0noiOnkpESEgCAbW/doN1au6G7/tiJ686pYTwStAEAtrvNnLDm8iT/LMmzk+yatbfWnjFgv7Y0M9oAAGxmHe13JnlrkpUkL0ny7hwtK2HOcy8+K0lywZk7F9wTAAAWbTNBe3dr7bok1Vr7SmvtTUleOmy3tqY3vORZSZJzTltecE8AAFi0zayjfaCqRkk+X1VvSPJXSS4ctltb06xkpDljDQDAtreZGe2fS3Jakp9J8sIkP57kNQP2acsqQRsAgM6GM9pVNU7yo621f5DkkSQ/dUJ6tUXNvgLZnIQdAGDbW3dGu6qWWmurSV5Yzi2+KaNuNM1oAwCw0Yz29UlekORTST5UVe9P8ujsztba7wzcty2nujntiaQNALDtbaZG+7wk92e60sjfSfLfdNcbqqp3VNU9VXXzXNt5VfXRqvp8d33u3H1vrKrbq+rWqnrZXPsLq+qm7r63nNSz613PxGwAADYK2hdW1d9NcnOSm7rrW7rrmzd43MxvJHn5MW3XJLmutXZ5kuu626mqZye5Kslzusf8WlcfnkzX8L46yeXd5djnPGlYdQQAgJmNgvY4yRnd5cy57dllQ621P03y9WOaX5HkXd32u5K8cq79va21g621LyW5PcmVVXVRkrNaax9vrbVMT5bzypykjnwZUtIGANj2NqrRvqu19k96fr2ntdbuSpLW2l1VNVuP++IkfzG33/6u7XC3fWz7mqrq6kxnv3PppZf22O3NKaUjAAB0NprRPpG10Gu9VtugfU2ttbe11q5orV1xwQUX9Na5zVI6AgDAzEZB+/sGeL27u3KQdNf3dO37k1wyt9+eJHd27XvWaD8pzf4qsOoIAADrBu3W2rH11X34cI6eVfI1ST40135VVe2sqssy/dLj9V2ZycNV9aJutZGfnHvMyWdWOiJnAwBsexueGfJbUVXvSfLiJOdX1f4kv5DkF5O8r6pem+SrSV6VJK21W6rqfUk+m2Qlyeu7k+UkyesyXcFkd5I/6C4npy5g3/nA44vtBwAACzdY0G6tvXqdu9YsSWmtXZvk2jXa9yV5bo9dG8y+r3wjSfL33v/p/PAL9zzF3gAAnMo2c8IaAADgOAnaPTp5T1kJAMCJJmj3aDQStQEAmBK0e1RyNgAAHUG7R6V4BACAjqDdI5UjAADMCNo9GkvaAAB0BG0AABiAoN2jkW9DAgDQEbR7pHIEAIAZQbtHZUYbAICOoN0jM9oAAMwI2j1yZkgAAGYE7R45YQ0AADOCdo+UaAMAMCNo90jlCAAAM4J2j6w6AgDAjKDdIyesAQBgRtDu0Zm7lhbdBQAAThKCdo++e+95SZKff/l3LLgnAAAsmqDdo1nhyK5lwwoAsN1JhD1Sog0AwIygPYDWFt0DAAAWTdDu0ezMkHI2AACCdp+UjgAA0BG0B3Db1x5edBcAAFgwQbtHBw6vJkl+e98dC+4JAACLJmgDAMAABO0enbVredFdAADgJCFo92j3jnGS5PUveeaCewIAwKIJ2j1bHpd1tAEAELT7VlWZCNoAANueoN2zUSXNlDYAwLYnaPdsVJVVU9oAANueoN2zsdIRAAAiaPeuKpkoHQEA2PYE7Z6NRqVGGwAAQbtvI6UjAABE0O7dSOkIAAARtHtnHW0AABJBu3fW0QYAIBG0e2cdbQAAEkG7d74MCQBAImj3bjRSOgIAgKDdu+mMtqANALDdCdo9UzoCAEAiaPfOKdgBAEgE7d6NqiJnAwAgaPdsVLG8HwAAgnbffBkSAIBE0O6dL0MCAJAI2r2zjjYAAImg3TulIwAAJIJ270rpCAAAEbR7N7KONgAAEbR7p3QEAIBE0O7dqJLJZNG9AABg0QTtnpnRBgAgEbR75xTsAAAkgnbvRiNfhgQAQNDundIRAAASQbt31tEGACARtHs3KqdgBwBA0O7dqCqrgjYAwLYnaPdsVGUdbQAABO2+OQU7AADJgoJ2VX25qm6qqhural/Xdl5VfbSqPt9dnzu3/xur6vaqurWqXraIPm+WdbQBAEgWO6P9ktba81trV3S3r0lyXWvt8iTXdbdTVc9OclWS5yR5eZJfq6rxIjq8GdbRBgAgOblKR16R5F3d9ruSvHKu/b2ttYOttS8luT3JlSe+e5tT1tEGACCLC9otyR9V1Q1VdXXX9rTW2l1J0l1f2LVfnOSOucfu79qepKqurqp9VbXv3nvvHajrG1M6AgBAkiwt6HW/t7V2Z1VdmOSjVfWXG+xba7StGWVba29L8rYkueKKKxYSd0cVy/sBALCYGe3W2p3d9T1JPphpKcjdVXVRknTX93S7709yydzD9yS588T19viMlY4AAJAFBO2qOr2qzpxtJ/nbSW5O8uEkr+l2e02SD3XbH05yVVXtrKrLklye5PoT2+vNK+toAwCQxZSOPC3JB6tq9vq/1Vr7D1X1ySTvq6rXJvlqklclSWvtlqp6X5LPJllJ8vrW2uoC+r0pTsEOAECygKDdWvtikuet0X5/ku9b5zHXJrl24K71YlSViZwNALDtnUzL+50SrKMNAEAiaPeuzGgDABBBu3ejMqMNAICg3buR5f0AAIig3btRVSZqRwAAtj1Bu2dOwQ4AQCJo906NNgAAiaDdu9HIqiMAAAjavSsz2gAARNDunRptAAASQbt3o0pWJW0AgG1P0O7Z2DraAABE0O5ddaUjTdgGANjWBO2ejaqSRJ02AMA2J2j3bDTN2cpHAAC2OUG7Z6MuaVtLGwBgexO0e1ZmtAEAiKDdu1mNtqANALC9Cdo9G5fSEQAABO3eKR0BACARtHt3ZHm/yYI7AgDAQgnaPbO8HwAAiaDdu6PL+wnaAADbmaDds/JlSAAAImj3blY60sxoAwBsa4J2z2ZfhlwVtAEAtjVBu2fW0QYAIBG0e3dkHW1JGwBgWxO0e3ZkHW05GwBgWxO0ezbqRtTyfgAA25ug3bNRWUcbAABBu3fW0QYAIBG0e+cU7AAAJIJ278ZKRwAAiKDduyOlI5MFdwQAgIUStHumdAQAgETQ7p11tAEASATt3llHGwCARNDuXfkyJAAAEbR754Q1AAAkgnbvjn4ZcrH9AABgsQTtnh1ZR1vSBgDY1gTtnjkFOwAAiaDdu1npSFOjDQCwrQnaPRuNzGgDACBo986ZIQEASATt3llHGwCARNDunXW0AQBIBO3eHV3eb8EdAQBgoQTtnpUabQAAImj3bmQdbQAAImj3btSNqHW0AQC2N0G7Z2a0AQBIBO3eWUcbAIBE0O6ddbQBAEgE7d6NBW0AACJo925kHW0AACJo98462gAAJIJ270bdtyHlbACA7U3Q7plVRwAASATt3llHGwCARNDu3axG+xNfun+xHQEAYKEE7Z4dXp1OZX/oxjsX3BMAABZJ0O7Z08/edWR77zUfWWBPAABYJEG7Z7MzQ87sveYjueEr38jeaz6Sv/iichIAgO2i2im6OsYVV1zR9u3bt5DXfvTgSp7zC3+44T5//Znflt/6n150gnoEAMAQquqG1toVa923ZWa0q+rlVXVrVd1eVdcsuj8bOX3nUm6/9vs33OfPv3B/9l7zkSdcrvnAZwbt12TS8v/dfl8W8cfVe6//au74+mP58y/cl73XfCTv++QdSZKvP3ooB1dWk2TDfh04vHpknL4ZX3/0UN78h7d+U49Npn88ffBT+7/pxwMA28+WmNGuqnGS25L8V0n2J/lkkle31j673mMWOaM9M5m0XPl/XZf7Hjl4Ql7vkvN254ydy/ncXQ8lSd78qudlx9IorbUcOLyan//ATUf2/T9f+dzsGFc+9dUH8t5P3pGzdi3loQMrSZJf+pHvzFm7lvPQgcNZnbTc+/DBvPvjX8lznn5WLjv/9Nz/6KH87qenX/Z86XdcmD/+y3vyvD1n59P7H0ySPP+Sc/KsC8/Iv79h42D66isvzXuu/+qR/vzv/+/NSZJ/+srn5vwzdqYqObQyyU+/51O57PzT86X7Hj3y2P/7qudnPKocWplkeTzKUreA+eFJy6iSg4cnufvhA3nG+afnY7fdd+R1kuRX/7vvyspqy9K4smP8xL81J226csxk0o4s0fjowZX8w7k/gv6fV39XxqPK4dVJdi6NcnBlkqXRKFXTddQfObiaHUujjCpZnbQsj0d57NBqTt8xTpLs/8bjOfu05Xzs1nvzku+4MLuWp/3/xmOHc/bu5RxenSRJ/uqBx3P+GTuzc2mUqkoleeCxQ/nITXflJ160N+NR8tih1Yyqjqx2M6ppv6qSXUvjI31e6X6YlckkO5fGqSTjUeXxw9PH/8UX7893fPuZueDMXTm0OsnqZJKHD6zkK/c/lu/ee25WJ9NxSKYnZZq0lrRk145xVifTf4NxVVa795PDq5OMarrf44cmOe/0HXOjfPQ9Z/7tZ7b56MGV3PnAgZx3xo5ccMaO3PXggaxOWr69+/5Da0fH9VNf/Ub+s28/M5NJy+5ufGflW5Xpv2Wluuvp7dVJcnBlNYdXJzl7944jrzxp03Eaz5V/tbTcdvcjeUZ33J9/xo60Nv3Db/eO8ZHlPO975GDO2rWclukx0Fqya3mcQ6urufOBA7nwzJ1JMj1Wx5VxVdpcHzPXx0mb/tvM1uSv7vk+s//BPPj44fyNZ52fUXfYTo+KJ5q0lo/ddm/2ftvpuez80590/3F78kss4imeVJZ3ovuwOmm54xuPZc+5uzMejVKZHjmz36Hzx13L9N+htZbVydHtSXfsTm9P23cvj7N7x/jI//EceezsPWBHdu9Y6uXnevzwau5+6EAuOfe0jEZPPnrmf57Zz3Tvwwezc2mUM3YuZXXSsjJp3fUkh1dbxqPpcb1raZwdS6O5x849bzs6JrP31vkxmXTX09stk0meMEarreXxQ6u55LzTuvfW6XvLSvf6Bw5PsjppOWv38pGfaTOHy1r/f55wfx8H7hNeb2Mt0/fO5bnfS5vpwvTfrRuz7jGjOvoestb+T2pbJwq2Nffe+KR841Ftfuza0f63tCPPOzsWx6PKePaDdPt2b5lP6Mv0vbRywZk786wLz9jki/dnoxntrRK0vyfJm1prL+tuvzFJWmv/bL3HnAxB+1ittXztoQM5Z/eOvP3Pvpg3/9Fti+4SAMAp4Uev2JNf+pHnnfDX3Shor/9n8snl4iR3zN3en+S/PHanqro6ydVJcumll56Ynh2HqspFZ+9OkrzhpZfnDS+9fM39VudmHw+tTFJVeeTASkaj5N6HD2b38jgrk5ZLzj0tDx84nP0PPJ4zdy7lsUOrWZm0XHjmzuxYGuXg4UkOra7m8Or0L//RKLn0vNPy8IGVPHZoJUujUQ50ZRurk5adS+McXFlNpdIynbUYVU1nK0eVnUujHDi8mqrKymSSXUvjPHpoJaftWMqjB1eyPB5lz7m7c/8jh3JodZLl8XR29enn7M7OpXEeObiS+x45mHNP25FxVe74xmM5Y+dSvv3sXbnj649leTzK6TuXcu/DB4/8Zbsyadm1PJ0t3b1jnIvO3p2v3P9oDq5MZ213jEc5vNqOjNmOpZrOeHazHbOZj13L45xz2nJu+9rD01mPSpZGoyOzx0f/jaZ/HU9nFKePHY8qDx84nEvPOz37uz6vTNoTZrVnM8azmZhk+hzTGZfKA48fytm7l7PazQbtWh7nzgcez55zTzsy/o8fnv5b7Fyajvdjh1Zz+s5xlkajJNMZn8cPrebRgyvZvWOcXcvjPHZoNeectpzJ5Oi8w4OPH57OMC2PprMELRmNkkcPrubMXdNZqdms8IGVadvXHjyQM3ctZ+fSKLuWxxmPKquTSQ4cnuTQ6iSn71jK7uVxRqPp8x1cWc1DB1YymbSMRjUdk9XWzRi3nN7Nfh1enc40zWaW58d5ze1un6/c/2iefs7uVCUHV6bPcdqOcZbH008KqqafZhzoZuRXJpOcc9qOuVmWdmQW7diZkqpkZbXlrgcP5OJzdj+hD0ujOjKO0+Nv0v28k7TWstTN3E/7tZozdy2nteknL6ttOhu+NK48cnClm+Fuue/hQ9nRzQiORtNPW5bHoyNjeWz/skafW6bH+tK48vih1SNt89rccTc79r7VaZQ+JmL6mMr51rvRw8/RvS/MZtpaO/ppxPzrTNuns4nTWcXKaDS33R2/425G+RuPHeo+PZvvY3X/B6ef0B29f6056OMzHo2698v1HltH3geT6Yz8aTvGObw6/eRudnyNR5Wl7tOtBx4/fGR2eTZWT3rWuTGZzXgeOybzY1Y1/fRs1nZ4dZKvP3royCcC033qyKznyqQdOV43c7w81S59z0Fu9v/S0rgy+7V07GPWOgJmZp9QVNWR34eT1tbdf61PiNabhV7/OZ7cNvvd0o5pW2/fo582PvGTx9krT1o78jtrdv+xI3n006Xkgu7Tw5PJVpnRflWSl7XW/sfu9k8kubK19tPrPeZknNEGAODUcip8GXJ/kkvmbu9J4owwAACctLZK0P5kksur6rKq2pHkqiQfXnCfAABgXVuiRru1tlJVb0jyh0nGSd7RWrtlwd0CAIB1bYmgnSSttd9P8vuL7gcAAGzGVikdAQCALUXQBgCAAQjaAAAwAEEbAAAGIGgDAMAABG0AABiAoA0AAAMQtAEAYACCNgAADEDQBgCAAQjaAAAwgGqtLboPg6iqe5N8ZQEvfX6S+xbwuluV8To+xuv4GK/jZ8yOj/E6Psbr+Biv47Oo8fprrbUL1rrjlA3ai1JV+1prVyy6H1uF8To+xuv4GK/jZ8yOj/E6Psbr+Biv43MyjpfSEQAAGICgDQAAAxC0+/e2RXdgizFex8d4HR/jdfyM2fExXsfHeB0f43V8TrrxUqMNAAADMKMNAAADELQBAGAAgnaPqurlVXVrVd1eVdcsuj+LUlVfrqqbqurGqtrXtZ1XVR+tqs931+fO7f/GbsxuraqXzbW/sHue26vqLVVVi/h5hlBV76iqe6rq5rm23saoqnZW1W937Z+oqr0n9Afs2Trj9aaq+qvuOLuxqn5g7r5tO15VdUlV/ceq+lxV3VJVP9u1O77WsMF4Ob7WUFW7qur6qvp0N17/uGt3fK1jgzFzjK2jqsZV9amq+r3u9tY9vlprLj1ckoyTfCHJM5LsSPLpJM9edL8WNBZfTnL+MW2/lOSabvuaJP+82352N1Y7k1zWjeG4u+/6JN+TpJL8QZLvX/TP1uMY/c0kL0hy8xBjlOR/SfKvu+2rkvz2on/mAcbrTUn+/hr7buvxSnJRkhd022cmua0bE8fX8Y2X42vt8aokZ3Tby0k+keRFjq9vaswcY+uP2d9N8ltJfq+7vWWPLzPa/bkyye2ttS+21g4leW+SVyy4TyeTVyR5V7f9riSvnGt/b2vtYGvtS0luT3JlVV2U5KzW2sfb9H/Du+ces+W11v40ydePae5zjOaf698n+b7ZX/Nb0TrjtZ5tPV6ttbtaa/+p2344yeeSXBzH15o2GK/1bPfxaq21R7qby92lxfG1rg3GbD3besyqak+S/zrJv5lr3rLHl6Ddn4uT3DF3e382frM+lbUkf1RVN1TV1V3b01prdyXTX2xJLuza1xu3i7vtY9tPZX2O0ZHHtNZWkjyY5NsG6/nivKGqPlPT0pLZR4nGq9N9JPpdmc6gOb6ewjHjlTi+1tR9rH9jknuSfLS15vh6CuuMWeIYW8uvJPmHSSZzbVv2+BK0+7PWX0Pbde3E722tvSDJ9yd5fVX9zQ32XW/cjOdR38wYbYfxe2uSZyZ5fpK7kvyLrt14JamqM5J8IMnPtdYe2mjXNdqMl+NrXa211dba85PsyXT28Lkb7L7txytZd8wcY8eoqr+T5J7W2g2bfcgabSfVWAna/dmf5JK523uS3LmgvixUa+3O7vqeJB/MtKzm7u6jnHTX93S7rzdu+7vtY9tPZX2O0ZHHVNVSkrOz+dKLLaG1dnf3y2uS5NczPc4S45WqWs40NP5ma+13umbH1zrWGi/H11NrrT2Q5E+SvDyOr02ZHzPH2Jq+N8kPVtWXMy3BfWlV/bts4eNL0O7PJ5NcXlWXVdWOTAvsP7zgPp1wVXV6VZ05207yt5PcnOlYvKbb7TVJPtRtfzjJVd23gC9LcnmS67uPhh6uqhd1tVM/OfeYU1WfYzT/XD+S5I+7OrVTxuxNt/NDmR5nyTYfr+5ne3uSz7XWfnnuLsfXGtYbL8fX2qrqgqo6p9veneRvJfnLOL7Wtd6YOcaerLX2xtbantba3kxz1B+31n48W/n4aifBt0tPlUuSH8j0G+tfSPKPFt2fBY3BMzL9BvCnk9wyG4dM65+uS/L57vq8ucf8o27Mbs3cyiJJrsj0jecLSX413ZlMT4VLkvdk+lHh4Uz/un5tn2OUZFeS92f6xZDrkzxj0T/zAOP1b5PclOQzmb5xXmS8WpL8jUw/Bv1Mkhu7yw84vo57vBxfa4/Xdyb5VDcuNyf5P7p2x9fxj5ljbONxe3GOrjqyZY8vp2AHAIABKB0BAIABCNoAADAAQRsAAAYgaAMAwAAEbQAAGICgDXCKqarVqrpx7nJNj8+9t6pufuo9AVhadAcA6N3jbXq6ZwAWyIw2wDZRVV+uqn9eVdd3l2d17X+tqq6rqs9015d27U+rqg9W1ae7y1/vnmpcVb9eVbdU1R91Z7sD4BiCNsCpZ/cxpSM/NnffQ621KzM9U9qvdG2/muTdrbXvTPKbSd7Stb8lycdaa89L8oJMz/aaTE9z/K9aa89J8kCSHx70pwHYopwZEuAUU1WPtNbOWKP9y0le2lr7YlUtJ/laa+3bquq+TE//fLhrv6u1dn5V3ZtkT2vt4Nxz7E3y0dba5d3tn0+y3Fr7pyfgRwPYUsxoA2wvbZ3t9fZZy8G57dX4vg/AmgRtgO3lx+auP95t/3mSq7rt/z7Jn3Xb1yV5XZJU1biqzjpRnQQ4FZiFADj17K6qG+du/4fW2myJv51V9YlMJ1pe3bX9TJJ3VNU/SHJvkp/q2n82yduq6rWZzly/LsldQ3ce4FShRhtgm+hqtK9ord236L4AbAdKRwAAYABmtAEAYABmtAEAYACCNgAADEDQBgCAAQjaAAAwAEEbAAAG8P8Dou06rLXzTHMAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 864x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       154\n",
      "           1       0.37      0.09      0.14       703\n",
      "           2       0.32      0.29      0.31       702\n",
      "           3       0.30      0.48      0.37       703\n",
      "           4       0.65      0.97      0.78       702\n",
      "\n",
      "    accuracy                           0.43      2964\n",
      "   macro avg       0.33      0.37      0.32      2964\n",
      "weighted avg       0.39      0.43      0.38      2964\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 504x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 504x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "gnn2l_gat = GNN2L_GAT(data_with_nedbit).to(device)\n",
    "pred = train(gnn2l_gat, data_with_nedbit.to(device), 40000, cm_title='GAT2L, multiclass, 16HC', classes=['P', 'LP', 'WN', 'LN', 'RN'], weight_decay=0.0005, layers=2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "382c7b0ca1a74f37aa65ab0fc37336b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40001 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch: 0000] train loss: 448.9114, train acc: 0.0518, val loss: 427.6403, val acc: 0.0519  (best train acc: 0.0518, best val acc: 0.0519)\n",
      "[Epoch: 0020] train loss: 37.0807, train acc: 0.0521, val loss: 35.9511, val acc: 0.0519  (best train acc: 0.1109, best val acc: 0.0519)\n",
      "[Epoch: 0040] train loss: 19.9049, train acc: 0.2308, val loss: 19.5501, val acc: 0.2371  (best train acc: 0.2308, best val acc: 0.2371)\n",
      "[Epoch: 0060] train loss: 11.3411, train acc: 0.2119, val loss: 10.8531, val acc: 0.2371  (best train acc: 0.2317, best val acc: 0.2371)\n",
      "[Epoch: 0080] train loss: 3.5807, train acc: 0.2415, val loss: 3.6045, val acc: 0.2324  (best train acc: 0.3271, best val acc: 0.3511)\n",
      "[Epoch: 0100] train loss: 2.8448, train acc: 0.2332, val loss: 2.8281, val acc: 0.2435  (best train acc: 0.3271, best val acc: 0.3511)\n",
      "[Epoch: 0120] train loss: 1.8311, train acc: 0.2389, val loss: 1.8061, val acc: 0.2324  (best train acc: 0.3271, best val acc: 0.3511)\n",
      "[Epoch: 0140] train loss: 1.5858, train acc: 0.2422, val loss: 1.5881, val acc: 0.2368  (best train acc: 0.3271, best val acc: 0.3511)\n",
      "[Epoch: 0160] train loss: 1.5722, train acc: 0.2363, val loss: 1.5633, val acc: 0.2324  (best train acc: 0.3271, best val acc: 0.3511)\n",
      "[Epoch: 0180] train loss: 1.5471, train acc: 0.2394, val loss: 1.5409, val acc: 0.2381  (best train acc: 0.3271, best val acc: 0.3511)\n",
      "[Epoch: 0200] train loss: 1.5317, train acc: 0.2392, val loss: 1.5280, val acc: 0.2378  (best train acc: 0.3271, best val acc: 0.3511)\n",
      "[Epoch: 0220] train loss: 1.5228, train acc: 0.2397, val loss: 1.5191, val acc: 0.2378  (best train acc: 0.3271, best val acc: 0.3511)\n",
      "[Epoch: 0240] train loss: 1.5158, train acc: 0.2405, val loss: 1.5141, val acc: 0.2374  (best train acc: 0.3271, best val acc: 0.3511)\n",
      "[Epoch: 0260] train loss: 1.5140, train acc: 0.2403, val loss: 1.5116, val acc: 0.2395  (best train acc: 0.3271, best val acc: 0.3511)\n",
      "[Epoch: 0280] train loss: 1.5136, train acc: 0.2450, val loss: 1.5100, val acc: 0.2415  (best train acc: 0.3271, best val acc: 0.3511)\n",
      "[Epoch: 0300] train loss: 1.5129, train acc: 0.2475, val loss: 1.5091, val acc: 0.2749  (best train acc: 0.3271, best val acc: 0.3511)\n",
      "[Epoch: 0320] train loss: 1.5099, train acc: 0.2748, val loss: 1.5087, val acc: 0.2992  (best train acc: 0.3271, best val acc: 0.3511)\n",
      "[Epoch: 0340] train loss: 1.5107, train acc: 0.2804, val loss: 1.5068, val acc: 0.2830  (best train acc: 0.3271, best val acc: 0.3511)\n",
      "[Epoch: 0360] train loss: 1.5104, train acc: 0.2717, val loss: 1.5080, val acc: 0.3278  (best train acc: 0.3271, best val acc: 0.3511)\n",
      "[Epoch: 0380] train loss: 1.5112, train acc: 0.2692, val loss: 1.5069, val acc: 0.3143  (best train acc: 0.3271, best val acc: 0.3511)\n",
      "[Epoch: 0400] train loss: 1.5099, train acc: 0.2961, val loss: 1.5060, val acc: 0.3133  (best train acc: 0.3271, best val acc: 0.3511)\n",
      "[Epoch: 0420] train loss: 1.5091, train acc: 0.2778, val loss: 1.5065, val acc: 0.3359  (best train acc: 0.3271, best val acc: 0.3511)\n",
      "[Epoch: 0440] train loss: 1.5071, train acc: 0.2945, val loss: 1.5051, val acc: 0.3292  (best train acc: 0.3271, best val acc: 0.3511)\n",
      "[Epoch: 0460] train loss: 1.5073, train acc: 0.2992, val loss: 1.5046, val acc: 0.3174  (best train acc: 0.3271, best val acc: 0.3511)\n",
      "[Epoch: 0480] train loss: 1.5074, train acc: 0.2841, val loss: 1.5043, val acc: 0.3204  (best train acc: 0.3271, best val acc: 0.3511)\n",
      "[Epoch: 0500] train loss: 1.5078, train acc: 0.3208, val loss: 1.5037, val acc: 0.3228  (best train acc: 0.3271, best val acc: 0.3511)\n",
      "[Epoch: 0520] train loss: 1.5068, train acc: 0.3203, val loss: 1.5039, val acc: 0.3386  (best train acc: 0.3271, best val acc: 0.3589)\n",
      "[Epoch: 0540] train loss: 1.5077, train acc: 0.3060, val loss: 1.5033, val acc: 0.3373  (best train acc: 0.3271, best val acc: 0.3703)\n",
      "[Epoch: 0560] train loss: 1.5061, train acc: 0.3067, val loss: 1.5035, val acc: 0.3437  (best train acc: 0.3271, best val acc: 0.3703)\n",
      "[Epoch: 0580] train loss: 1.5068, train acc: 0.3082, val loss: 1.5031, val acc: 0.3535  (best train acc: 0.3271, best val acc: 0.3703)\n",
      "[Epoch: 0600] train loss: 1.5066, train acc: 0.3118, val loss: 1.5020, val acc: 0.3477  (best train acc: 0.3271, best val acc: 0.3703)\n",
      "[Epoch: 0620] train loss: 1.5053, train acc: 0.3121, val loss: 1.5021, val acc: 0.3595  (best train acc: 0.3271, best val acc: 0.3703)\n",
      "[Epoch: 0640] train loss: 1.5044, train acc: 0.3066, val loss: 1.5015, val acc: 0.3420  (best train acc: 0.3460, best val acc: 0.3784)\n",
      "[Epoch: 0660] train loss: 1.5049, train acc: 0.3232, val loss: 1.5009, val acc: 0.3639  (best train acc: 0.3460, best val acc: 0.3784)\n",
      "[Epoch: 0680] train loss: 1.5050, train acc: 0.3128, val loss: 1.5015, val acc: 0.3562  (best train acc: 0.3460, best val acc: 0.3784)\n",
      "[Epoch: 0700] train loss: 1.5031, train acc: 0.3458, val loss: 1.5008, val acc: 0.3595  (best train acc: 0.3460, best val acc: 0.3784)\n",
      "[Epoch: 0720] train loss: 1.5053, train acc: 0.3110, val loss: 1.5002, val acc: 0.3541  (best train acc: 0.3460, best val acc: 0.3784)\n",
      "[Epoch: 0740] train loss: 1.5044, train acc: 0.3539, val loss: 1.4996, val acc: 0.3703  (best train acc: 0.3539, best val acc: 0.3909)\n",
      "[Epoch: 0760] train loss: 1.5024, train acc: 0.3428, val loss: 1.5033, val acc: 0.3373  (best train acc: 0.3551, best val acc: 0.3909)\n",
      "[Epoch: 0780] train loss: 1.5085, train acc: 0.3118, val loss: 1.4996, val acc: 0.3511  (best train acc: 0.3551, best val acc: 0.3909)\n",
      "[Epoch: 0800] train loss: 1.5052, train acc: 0.3263, val loss: 1.5010, val acc: 0.3417  (best train acc: 0.3551, best val acc: 0.3909)\n",
      "[Epoch: 0820] train loss: 1.5028, train acc: 0.3372, val loss: 1.4969, val acc: 0.3663  (best train acc: 0.3551, best val acc: 0.3909)\n",
      "[Epoch: 0840] train loss: 1.4971, train acc: 0.3529, val loss: 1.4924, val acc: 0.3572  (best train acc: 0.3776, best val acc: 0.4057)\n",
      "[Epoch: 0860] train loss: 1.4931, train acc: 0.2744, val loss: 1.4873, val acc: 0.3845  (best train acc: 0.3776, best val acc: 0.4057)\n",
      "[Epoch: 0880] train loss: 1.4915, train acc: 0.3101, val loss: 1.4827, val acc: 0.4027  (best train acc: 0.3776, best val acc: 0.4057)\n",
      "[Epoch: 0900] train loss: 1.4837, train acc: 0.3609, val loss: 1.4780, val acc: 0.3791  (best train acc: 0.3814, best val acc: 0.4091)\n",
      "[Epoch: 0920] train loss: 1.4774, train acc: 0.3610, val loss: 1.4701, val acc: 0.3916  (best train acc: 0.3871, best val acc: 0.4132)\n",
      "[Epoch: 0940] train loss: 1.4708, train acc: 0.3319, val loss: 1.4615, val acc: 0.3997  (best train acc: 0.3871, best val acc: 0.4132)\n",
      "[Epoch: 0960] train loss: 1.4636, train acc: 0.3777, val loss: 1.4528, val acc: 0.3801  (best train acc: 0.3882, best val acc: 0.4192)\n",
      "[Epoch: 0980] train loss: 1.4782, train acc: 0.2938, val loss: 1.4588, val acc: 0.4169  (best train acc: 0.3882, best val acc: 0.4192)\n",
      "[Epoch: 1000] train loss: 1.4379, train acc: 0.3814, val loss: 1.4217, val acc: 0.3966  (best train acc: 0.3911, best val acc: 0.4192)\n",
      "[Epoch: 1020] train loss: 1.4411, train acc: 0.3536, val loss: 1.4170, val acc: 0.3653  (best train acc: 0.3979, best val acc: 0.4236)\n",
      "[Epoch: 1040] train loss: 1.4321, train acc: 0.3201, val loss: 1.4143, val acc: 0.4202  (best train acc: 0.3979, best val acc: 0.4280)\n",
      "[Epoch: 1060] train loss: 1.3965, train acc: 0.3912, val loss: 1.3831, val acc: 0.4185  (best train acc: 0.4001, best val acc: 0.4337)\n",
      "[Epoch: 1080] train loss: 1.3929, train acc: 0.3775, val loss: 1.3823, val acc: 0.3646  (best train acc: 0.4001, best val acc: 0.4337)\n",
      "[Epoch: 1100] train loss: 1.3859, train acc: 0.3902, val loss: 1.3578, val acc: 0.4334  (best train acc: 0.4024, best val acc: 0.4381)\n",
      "[Epoch: 1120] train loss: 1.3662, train acc: 0.3992, val loss: 1.3376, val acc: 0.4465  (best train acc: 0.4096, best val acc: 0.4469)\n",
      "[Epoch: 1140] train loss: 1.3693, train acc: 0.3994, val loss: 1.3371, val acc: 0.4283  (best train acc: 0.4143, best val acc: 0.4519)\n",
      "[Epoch: 1160] train loss: 1.3598, train acc: 0.3819, val loss: 1.3316, val acc: 0.4361  (best train acc: 0.4143, best val acc: 0.4519)\n",
      "[Epoch: 1180] train loss: 1.3460, train acc: 0.4134, val loss: 1.3158, val acc: 0.4560  (best train acc: 0.4143, best val acc: 0.4560)\n",
      "[Epoch: 1200] train loss: 1.3401, train acc: 0.4145, val loss: 1.3089, val acc: 0.4519  (best train acc: 0.4151, best val acc: 0.4570)\n",
      "[Epoch: 1220] train loss: 1.3329, train acc: 0.4161, val loss: 1.2996, val acc: 0.4617  (best train acc: 0.4187, best val acc: 0.4617)\n",
      "[Epoch: 1240] train loss: 1.3329, train acc: 0.4126, val loss: 1.2929, val acc: 0.4604  (best train acc: 0.4187, best val acc: 0.4637)\n",
      "[Epoch: 1260] train loss: 1.3189, train acc: 0.4224, val loss: 1.2844, val acc: 0.4691  (best train acc: 0.4235, best val acc: 0.4691)\n",
      "[Epoch: 1280] train loss: 1.3183, train acc: 0.4263, val loss: 1.2757, val acc: 0.4668  (best train acc: 0.4264, best val acc: 0.4732)\n",
      "[Epoch: 1300] train loss: 1.3127, train acc: 0.4263, val loss: 1.2680, val acc: 0.4648  (best train acc: 0.4271, best val acc: 0.4816)\n",
      "[Epoch: 1320] train loss: 1.3049, train acc: 0.4303, val loss: 1.2590, val acc: 0.4654  (best train acc: 0.4315, best val acc: 0.4816)\n",
      "[Epoch: 1340] train loss: 1.2981, train acc: 0.4357, val loss: 1.2506, val acc: 0.4877  (best train acc: 0.4357, best val acc: 0.4877)\n",
      "[Epoch: 1360] train loss: 1.2892, train acc: 0.4309, val loss: 1.2467, val acc: 0.4779  (best train acc: 0.4357, best val acc: 0.4877)\n",
      "[Epoch: 1380] train loss: 1.2960, train acc: 0.4337, val loss: 1.2348, val acc: 0.4823  (best train acc: 0.4357, best val acc: 0.4911)\n",
      "[Epoch: 1400] train loss: 1.2850, train acc: 0.4326, val loss: 1.2290, val acc: 0.4823  (best train acc: 0.4357, best val acc: 0.4911)\n",
      "[Epoch: 1420] train loss: 1.2814, train acc: 0.4354, val loss: 1.2240, val acc: 0.4725  (best train acc: 0.4375, best val acc: 0.4911)\n",
      "[Epoch: 1440] train loss: 1.2748, train acc: 0.4344, val loss: 1.2207, val acc: 0.4779  (best train acc: 0.4408, best val acc: 0.4911)\n",
      "[Epoch: 1460] train loss: 1.2683, train acc: 0.4378, val loss: 1.2128, val acc: 0.4793  (best train acc: 0.4423, best val acc: 0.4944)\n",
      "[Epoch: 1480] train loss: 1.2703, train acc: 0.4313, val loss: 1.2059, val acc: 0.4779  (best train acc: 0.4423, best val acc: 0.4944)\n",
      "[Epoch: 1500] train loss: 1.2639, train acc: 0.4430, val loss: 1.1998, val acc: 0.4651  (best train acc: 0.4479, best val acc: 0.4944)\n",
      "[Epoch: 1520] train loss: 1.2666, train acc: 0.4147, val loss: 1.2035, val acc: 0.4786  (best train acc: 0.4487, best val acc: 0.4944)\n",
      "[Epoch: 1540] train loss: 1.2417, train acc: 0.4315, val loss: 1.1937, val acc: 0.4860  (best train acc: 0.4546, best val acc: 0.4944)\n",
      "[Epoch: 1560] train loss: 1.2359, train acc: 0.4333, val loss: 1.1833, val acc: 0.4914  (best train acc: 0.4555, best val acc: 0.4944)\n",
      "[Epoch: 1580] train loss: 1.2277, train acc: 0.4425, val loss: 1.1588, val acc: 0.4904  (best train acc: 0.4627, best val acc: 0.4961)\n",
      "[Epoch: 1600] train loss: 1.1961, train acc: 0.4678, val loss: 1.1443, val acc: 0.4934  (best train acc: 0.4704, best val acc: 0.5015)\n",
      "[Epoch: 1620] train loss: 1.1880, train acc: 0.4693, val loss: 1.1272, val acc: 0.5062  (best train acc: 0.4763, best val acc: 0.5069)\n",
      "[Epoch: 1640] train loss: 1.1604, train acc: 0.4806, val loss: 1.1134, val acc: 0.5113  (best train acc: 0.4860, best val acc: 0.5120)\n",
      "[Epoch: 1660] train loss: 1.1521, train acc: 0.4858, val loss: 1.0988, val acc: 0.5133  (best train acc: 0.4902, best val acc: 0.5187)\n",
      "[Epoch: 1680] train loss: 1.1376, train acc: 0.4808, val loss: 1.0890, val acc: 0.5116  (best train acc: 0.4902, best val acc: 0.5187)\n",
      "[Epoch: 1700] train loss: 1.1211, train acc: 0.4709, val loss: 1.0830, val acc: 0.4981  (best train acc: 0.4902, best val acc: 0.5187)\n",
      "[Epoch: 1720] train loss: 1.1117, train acc: 0.4735, val loss: 1.0733, val acc: 0.5015  (best train acc: 0.4902, best val acc: 0.5187)\n",
      "[Epoch: 1740] train loss: 1.0921, train acc: 0.4683, val loss: 1.0589, val acc: 0.4914  (best train acc: 0.4902, best val acc: 0.5187)\n",
      "[Epoch: 1760] train loss: 1.0794, train acc: 0.4680, val loss: 1.0496, val acc: 0.4907  (best train acc: 0.4902, best val acc: 0.5187)\n",
      "[Epoch: 1780] train loss: 1.0720, train acc: 0.4683, val loss: 1.0498, val acc: 0.5032  (best train acc: 0.4902, best val acc: 0.5187)\n",
      "[Epoch: 1800] train loss: 1.0689, train acc: 0.4701, val loss: 1.0486, val acc: 0.5029  (best train acc: 0.4902, best val acc: 0.5204)\n",
      "[Epoch: 1820] train loss: 1.0748, train acc: 0.4708, val loss: 1.0481, val acc: 0.5029  (best train acc: 0.4902, best val acc: 0.5204)\n",
      "[Epoch: 1840] train loss: 1.0668, train acc: 0.4740, val loss: 1.0473, val acc: 0.5123  (best train acc: 0.4902, best val acc: 0.5204)\n",
      "[Epoch: 1860] train loss: 1.0698, train acc: 0.4798, val loss: 1.0479, val acc: 0.5201  (best train acc: 0.4902, best val acc: 0.5221)\n",
      "[Epoch: 1880] train loss: 1.0686, train acc: 0.4693, val loss: 1.0444, val acc: 0.5099  (best train acc: 0.4902, best val acc: 0.5228)\n",
      "[Epoch: 1900] train loss: 1.0631, train acc: 0.4857, val loss: 1.0395, val acc: 0.5029  (best train acc: 0.4902, best val acc: 0.5228)\n",
      "[Epoch: 1920] train loss: 1.0608, train acc: 0.4791, val loss: 1.0378, val acc: 0.5130  (best train acc: 0.4902, best val acc: 0.5228)\n",
      "[Epoch: 1940] train loss: 1.0595, train acc: 0.4813, val loss: 1.0350, val acc: 0.5113  (best train acc: 0.4902, best val acc: 0.5228)\n",
      "[Epoch: 1960] train loss: 1.0548, train acc: 0.4886, val loss: 1.0309, val acc: 0.5245  (best train acc: 0.4960, best val acc: 0.5255)\n",
      "[Epoch: 1980] train loss: 1.0517, train acc: 0.4991, val loss: 1.0243, val acc: 0.5251  (best train acc: 0.4991, best val acc: 0.5315)\n",
      "[Epoch: 2000] train loss: 1.0549, train acc: 0.4926, val loss: 1.0247, val acc: 0.5292  (best train acc: 0.5042, best val acc: 0.5342)\n",
      "[Epoch: 2020] train loss: 1.0495, train acc: 0.4980, val loss: 1.0237, val acc: 0.5332  (best train acc: 0.5068, best val acc: 0.5383)\n",
      "[Epoch: 2040] train loss: 1.0526, train acc: 0.5067, val loss: 1.0218, val acc: 0.5366  (best train acc: 0.5098, best val acc: 0.5383)\n",
      "[Epoch: 2060] train loss: 1.0559, train acc: 0.5049, val loss: 1.0212, val acc: 0.5359  (best train acc: 0.5098, best val acc: 0.5400)\n",
      "[Epoch: 2080] train loss: 1.0541, train acc: 0.5019, val loss: 1.0205, val acc: 0.5366  (best train acc: 0.5098, best val acc: 0.5403)\n",
      "[Epoch: 2100] train loss: 1.0479, train acc: 0.5035, val loss: 1.0202, val acc: 0.5356  (best train acc: 0.5107, best val acc: 0.5413)\n",
      "[Epoch: 2120] train loss: 1.0500, train acc: 0.5061, val loss: 1.0195, val acc: 0.5363  (best train acc: 0.5126, best val acc: 0.5423)\n",
      "[Epoch: 2140] train loss: 1.0502, train acc: 0.5084, val loss: 1.0186, val acc: 0.5433  (best train acc: 0.5148, best val acc: 0.5433)\n",
      "[Epoch: 2160] train loss: 1.0505, train acc: 0.5059, val loss: 1.0172, val acc: 0.5413  (best train acc: 0.5148, best val acc: 0.5440)\n",
      "[Epoch: 2180] train loss: 1.0505, train acc: 0.5085, val loss: 1.0165, val acc: 0.5393  (best train acc: 0.5158, best val acc: 0.5460)\n",
      "[Epoch: 2200] train loss: 1.0480, train acc: 0.5161, val loss: 1.0156, val acc: 0.5359  (best train acc: 0.5161, best val acc: 0.5477)\n",
      "[Epoch: 2220] train loss: 1.0446, train acc: 0.5152, val loss: 1.0150, val acc: 0.5423  (best train acc: 0.5172, best val acc: 0.5477)\n",
      "[Epoch: 2240] train loss: 1.0497, train acc: 0.4926, val loss: 1.0158, val acc: 0.5349  (best train acc: 0.5172, best val acc: 0.5477)\n",
      "[Epoch: 2260] train loss: 1.0438, train acc: 0.5108, val loss: 1.0151, val acc: 0.5305  (best train acc: 0.5186, best val acc: 0.5477)\n",
      "[Epoch: 2280] train loss: 1.0442, train acc: 0.5161, val loss: 1.0136, val acc: 0.5390  (best train acc: 0.5212, best val acc: 0.5477)\n",
      "[Epoch: 2300] train loss: 1.0432, train acc: 0.5172, val loss: 1.0135, val acc: 0.5336  (best train acc: 0.5212, best val acc: 0.5477)\n",
      "[Epoch: 2320] train loss: 1.0453, train acc: 0.5152, val loss: 1.0143, val acc: 0.5376  (best train acc: 0.5222, best val acc: 0.5477)\n",
      "[Epoch: 2340] train loss: 1.0434, train acc: 0.5194, val loss: 1.0139, val acc: 0.5288  (best train acc: 0.5225, best val acc: 0.5477)\n",
      "[Epoch: 2360] train loss: 1.0435, train acc: 0.5140, val loss: 1.0126, val acc: 0.5346  (best train acc: 0.5225, best val acc: 0.5477)\n",
      "[Epoch: 2380] train loss: 1.0416, train acc: 0.5174, val loss: 1.0135, val acc: 0.5312  (best train acc: 0.5225, best val acc: 0.5477)\n",
      "[Epoch: 2400] train loss: 1.0413, train acc: 0.5134, val loss: 1.0116, val acc: 0.5349  (best train acc: 0.5235, best val acc: 0.5477)\n",
      "[Epoch: 2420] train loss: 1.0420, train acc: 0.5173, val loss: 1.0134, val acc: 0.5369  (best train acc: 0.5235, best val acc: 0.5477)\n",
      "[Epoch: 2440] train loss: 1.0415, train acc: 0.5111, val loss: 1.0134, val acc: 0.5207  (best train acc: 0.5235, best val acc: 0.5477)\n",
      "[Epoch: 2460] train loss: 1.0388, train acc: 0.5139, val loss: 1.0111, val acc: 0.5319  (best train acc: 0.5242, best val acc: 0.5477)\n",
      "[Epoch: 2480] train loss: 1.0406, train acc: 0.5227, val loss: 1.0168, val acc: 0.4907  (best train acc: 0.5242, best val acc: 0.5477)\n",
      "[Epoch: 2500] train loss: 1.0487, train acc: 0.5187, val loss: 1.0105, val acc: 0.5400  (best train acc: 0.5242, best val acc: 0.5477)\n",
      "[Epoch: 2520] train loss: 1.0393, train acc: 0.5194, val loss: 1.0106, val acc: 0.5454  (best train acc: 0.5242, best val acc: 0.5477)\n",
      "[Epoch: 2540] train loss: 1.0380, train acc: 0.5174, val loss: 1.0093, val acc: 0.5336  (best train acc: 0.5243, best val acc: 0.5477)\n",
      "[Epoch: 2560] train loss: 1.0386, train acc: 0.5187, val loss: 1.0098, val acc: 0.5292  (best train acc: 0.5262, best val acc: 0.5477)\n",
      "[Epoch: 2580] train loss: 1.0386, train acc: 0.5241, val loss: 1.0099, val acc: 0.5295  (best train acc: 0.5262, best val acc: 0.5477)\n",
      "[Epoch: 2600] train loss: 1.0366, train acc: 0.5249, val loss: 1.0095, val acc: 0.5376  (best train acc: 0.5286, best val acc: 0.5477)\n",
      "[Epoch: 2620] train loss: 1.0350, train acc: 0.5233, val loss: 1.0096, val acc: 0.5241  (best train acc: 0.5286, best val acc: 0.5477)\n",
      "[Epoch: 2640] train loss: 1.0356, train acc: 0.5184, val loss: 1.0086, val acc: 0.5332  (best train acc: 0.5286, best val acc: 0.5477)\n",
      "[Epoch: 2660] train loss: 1.0391, train acc: 0.5207, val loss: 1.0181, val acc: 0.5292  (best train acc: 0.5286, best val acc: 0.5477)\n",
      "[Epoch: 2680] train loss: 1.0423, train acc: 0.4984, val loss: 1.0131, val acc: 0.5373  (best train acc: 0.5286, best val acc: 0.5477)\n",
      "[Epoch: 2700] train loss: 1.0359, train acc: 0.5217, val loss: 1.0124, val acc: 0.5363  (best train acc: 0.5286, best val acc: 0.5477)\n",
      "[Epoch: 2720] train loss: 1.0399, train acc: 0.4965, val loss: 1.0098, val acc: 0.5349  (best train acc: 0.5286, best val acc: 0.5477)\n",
      "[Epoch: 2740] train loss: 1.0442, train acc: 0.4920, val loss: 1.0068, val acc: 0.5369  (best train acc: 0.5286, best val acc: 0.5477)\n",
      "[Epoch: 2760] train loss: 1.0335, train acc: 0.5199, val loss: 1.0063, val acc: 0.5400  (best train acc: 0.5286, best val acc: 0.5477)\n",
      "[Epoch: 2780] train loss: 1.0312, train acc: 0.5249, val loss: 1.0058, val acc: 0.5336  (best train acc: 0.5286, best val acc: 0.5477)\n",
      "[Epoch: 2800] train loss: 1.0353, train acc: 0.5200, val loss: 1.0051, val acc: 0.5346  (best train acc: 0.5286, best val acc: 0.5477)\n",
      "[Epoch: 2820] train loss: 1.0315, train acc: 0.5298, val loss: 1.0053, val acc: 0.5447  (best train acc: 0.5298, best val acc: 0.5477)\n",
      "[Epoch: 2840] train loss: 1.0400, train acc: 0.5236, val loss: 1.0055, val acc: 0.5383  (best train acc: 0.5298, best val acc: 0.5477)\n",
      "[Epoch: 2860] train loss: 1.0328, train acc: 0.5273, val loss: 1.0089, val acc: 0.5228  (best train acc: 0.5298, best val acc: 0.5484)\n",
      "[Epoch: 2880] train loss: 1.0334, train acc: 0.5254, val loss: 1.0051, val acc: 0.5396  (best train acc: 0.5298, best val acc: 0.5484)\n",
      "[Epoch: 2900] train loss: 1.0306, train acc: 0.5311, val loss: 1.0049, val acc: 0.5258  (best train acc: 0.5311, best val acc: 0.5484)\n",
      "[Epoch: 2920] train loss: 1.0447, train acc: 0.5222, val loss: 1.0034, val acc: 0.5444  (best train acc: 0.5311, best val acc: 0.5484)\n",
      "[Epoch: 2940] train loss: 1.0346, train acc: 0.5268, val loss: 1.0069, val acc: 0.5315  (best train acc: 0.5311, best val acc: 0.5484)\n",
      "[Epoch: 2960] train loss: 1.0308, train acc: 0.5296, val loss: 1.0027, val acc: 0.5359  (best train acc: 0.5341, best val acc: 0.5497)\n",
      "[Epoch: 2980] train loss: 1.0342, train acc: 0.5255, val loss: 1.0027, val acc: 0.5427  (best train acc: 0.5341, best val acc: 0.5548)\n",
      "[Epoch: 3000] train loss: 1.0297, train acc: 0.5301, val loss: 1.0025, val acc: 0.5356  (best train acc: 0.5341, best val acc: 0.5548)\n",
      "[Epoch: 3020] train loss: 1.0367, train acc: 0.5244, val loss: 1.0022, val acc: 0.5390  (best train acc: 0.5341, best val acc: 0.5548)\n",
      "[Epoch: 3040] train loss: 1.0282, train acc: 0.5289, val loss: 1.0015, val acc: 0.5410  (best train acc: 0.5341, best val acc: 0.5548)\n",
      "[Epoch: 3060] train loss: 1.0267, train acc: 0.5314, val loss: 1.0019, val acc: 0.5410  (best train acc: 0.5341, best val acc: 0.5548)\n",
      "[Epoch: 3080] train loss: 1.0296, train acc: 0.5278, val loss: 1.0019, val acc: 0.5386  (best train acc: 0.5357, best val acc: 0.5548)\n",
      "[Epoch: 3100] train loss: 1.0256, train acc: 0.5321, val loss: 1.0014, val acc: 0.5369  (best train acc: 0.5374, best val acc: 0.5548)\n",
      "[Epoch: 3120] train loss: 1.0245, train acc: 0.5351, val loss: 1.0028, val acc: 0.5508  (best train acc: 0.5374, best val acc: 0.5548)\n",
      "[Epoch: 3140] train loss: 1.0266, train acc: 0.5285, val loss: 0.9997, val acc: 0.5386  (best train acc: 0.5374, best val acc: 0.5548)\n",
      "[Epoch: 3160] train loss: 1.0278, train acc: 0.5294, val loss: 0.9992, val acc: 0.5420  (best train acc: 0.5374, best val acc: 0.5548)\n",
      "[Epoch: 3180] train loss: 1.0265, train acc: 0.5297, val loss: 1.0001, val acc: 0.5383  (best train acc: 0.5374, best val acc: 0.5548)\n",
      "[Epoch: 3200] train loss: 1.0229, train acc: 0.5299, val loss: 1.0025, val acc: 0.5383  (best train acc: 0.5374, best val acc: 0.5548)\n",
      "[Epoch: 3220] train loss: 1.0275, train acc: 0.5276, val loss: 1.0049, val acc: 0.5352  (best train acc: 0.5374, best val acc: 0.5548)\n",
      "[Epoch: 3240] train loss: 1.0279, train acc: 0.5307, val loss: 0.9974, val acc: 0.5383  (best train acc: 0.5374, best val acc: 0.5548)\n",
      "[Epoch: 3260] train loss: 1.0275, train acc: 0.5266, val loss: 1.0000, val acc: 0.5390  (best train acc: 0.5374, best val acc: 0.5548)\n",
      "[Epoch: 3280] train loss: 1.0285, train acc: 0.5297, val loss: 0.9974, val acc: 0.5460  (best train acc: 0.5374, best val acc: 0.5548)\n",
      "[Epoch: 3300] train loss: 1.0287, train acc: 0.5294, val loss: 0.9970, val acc: 0.5440  (best train acc: 0.5388, best val acc: 0.5548)\n",
      "[Epoch: 3320] train loss: 1.0231, train acc: 0.5348, val loss: 0.9963, val acc: 0.5447  (best train acc: 0.5388, best val acc: 0.5548)\n",
      "[Epoch: 3340] train loss: 1.0236, train acc: 0.5314, val loss: 0.9995, val acc: 0.5511  (best train acc: 0.5390, best val acc: 0.5548)\n",
      "[Epoch: 3360] train loss: 1.0261, train acc: 0.5301, val loss: 0.9955, val acc: 0.5383  (best train acc: 0.5395, best val acc: 0.5548)\n",
      "[Epoch: 3380] train loss: 1.0433, train acc: 0.5313, val loss: 1.0061, val acc: 0.5403  (best train acc: 0.5395, best val acc: 0.5565)\n",
      "[Epoch: 3400] train loss: 1.0218, train acc: 0.5328, val loss: 0.9993, val acc: 0.5541  (best train acc: 0.5395, best val acc: 0.5565)\n",
      "[Epoch: 3420] train loss: 1.0223, train acc: 0.5341, val loss: 0.9953, val acc: 0.5494  (best train acc: 0.5395, best val acc: 0.5565)\n",
      "[Epoch: 3440] train loss: 1.0252, train acc: 0.5358, val loss: 0.9941, val acc: 0.5460  (best train acc: 0.5395, best val acc: 0.5565)\n",
      "[Epoch: 3460] train loss: 1.0230, train acc: 0.5339, val loss: 0.9926, val acc: 0.5477  (best train acc: 0.5395, best val acc: 0.5565)\n",
      "[Epoch: 3480] train loss: 1.0204, train acc: 0.5331, val loss: 0.9930, val acc: 0.5447  (best train acc: 0.5395, best val acc: 0.5565)\n",
      "[Epoch: 3500] train loss: 1.0188, train acc: 0.5356, val loss: 0.9974, val acc: 0.5437  (best train acc: 0.5401, best val acc: 0.5568)\n",
      "[Epoch: 3520] train loss: 1.0237, train acc: 0.5343, val loss: 0.9916, val acc: 0.5423  (best train acc: 0.5420, best val acc: 0.5589)\n",
      "[Epoch: 3540] train loss: 1.0198, train acc: 0.5364, val loss: 0.9936, val acc: 0.5390  (best train acc: 0.5420, best val acc: 0.5609)\n",
      "[Epoch: 3560] train loss: 1.0194, train acc: 0.5369, val loss: 0.9950, val acc: 0.5616  (best train acc: 0.5420, best val acc: 0.5616)\n",
      "[Epoch: 3580] train loss: 1.0208, train acc: 0.5384, val loss: 0.9940, val acc: 0.5447  (best train acc: 0.5420, best val acc: 0.5616)\n",
      "[Epoch: 3600] train loss: 1.0291, train acc: 0.5266, val loss: 0.9983, val acc: 0.5467  (best train acc: 0.5420, best val acc: 0.5616)\n",
      "[Epoch: 3620] train loss: 1.0288, train acc: 0.5171, val loss: 0.9980, val acc: 0.5383  (best train acc: 0.5420, best val acc: 0.5616)\n",
      "[Epoch: 3640] train loss: 1.0266, train acc: 0.5095, val loss: 1.0008, val acc: 0.5285  (best train acc: 0.5420, best val acc: 0.5616)\n",
      "[Epoch: 3660] train loss: 1.0224, train acc: 0.5343, val loss: 0.9946, val acc: 0.5612  (best train acc: 0.5420, best val acc: 0.5636)\n",
      "[Epoch: 3680] train loss: 1.0238, train acc: 0.5322, val loss: 0.9961, val acc: 0.5400  (best train acc: 0.5420, best val acc: 0.5636)\n",
      "[Epoch: 3700] train loss: 1.0228, train acc: 0.5406, val loss: 0.9946, val acc: 0.5460  (best train acc: 0.5420, best val acc: 0.5636)\n",
      "[Epoch: 3720] train loss: 1.0193, train acc: 0.5421, val loss: 0.9900, val acc: 0.5400  (best train acc: 0.5429, best val acc: 0.5636)\n",
      "[Epoch: 3740] train loss: 1.0195, train acc: 0.5393, val loss: 0.9896, val acc: 0.5400  (best train acc: 0.5429, best val acc: 0.5636)\n",
      "[Epoch: 3760] train loss: 1.0117, train acc: 0.5469, val loss: 0.9802, val acc: 0.5420  (best train acc: 0.5492, best val acc: 0.5636)\n",
      "[Epoch: 3780] train loss: 1.0076, train acc: 0.5432, val loss: 0.9790, val acc: 0.5518  (best train acc: 0.5512, best val acc: 0.5636)\n",
      "[Epoch: 3800] train loss: 1.0141, train acc: 0.5357, val loss: 0.9815, val acc: 0.5457  (best train acc: 0.5512, best val acc: 0.5636)\n",
      "[Epoch: 3820] train loss: 1.0224, train acc: 0.5450, val loss: 0.9785, val acc: 0.5508  (best train acc: 0.5512, best val acc: 0.5636)\n",
      "[Epoch: 3840] train loss: 1.0053, train acc: 0.5482, val loss: 0.9855, val acc: 0.5467  (best train acc: 0.5518, best val acc: 0.5636)\n",
      "[Epoch: 3860] train loss: 1.0120, train acc: 0.5442, val loss: 0.9801, val acc: 0.5413  (best train acc: 0.5531, best val acc: 0.5636)\n",
      "[Epoch: 3880] train loss: 1.0098, train acc: 0.5500, val loss: 0.9747, val acc: 0.5592  (best train acc: 0.5531, best val acc: 0.5636)\n",
      "[Epoch: 3900] train loss: 1.0063, train acc: 0.5469, val loss: 0.9748, val acc: 0.5514  (best train acc: 0.5531, best val acc: 0.5636)\n",
      "[Epoch: 3920] train loss: 1.0055, train acc: 0.5500, val loss: 0.9745, val acc: 0.5558  (best train acc: 0.5541, best val acc: 0.5636)\n",
      "[Epoch: 3940] train loss: 1.0069, train acc: 0.5481, val loss: 0.9742, val acc: 0.5535  (best train acc: 0.5541, best val acc: 0.5636)\n",
      "[Epoch: 3960] train loss: 1.0087, train acc: 0.5449, val loss: 0.9741, val acc: 0.5514  (best train acc: 0.5541, best val acc: 0.5636)\n",
      "[Epoch: 3980] train loss: 1.0055, train acc: 0.5484, val loss: 0.9769, val acc: 0.5491  (best train acc: 0.5541, best val acc: 0.5636)\n",
      "[Epoch: 4000] train loss: 1.0013, train acc: 0.5467, val loss: 0.9755, val acc: 0.5484  (best train acc: 0.5541, best val acc: 0.5636)\n",
      "[Epoch: 4020] train loss: 1.0065, train acc: 0.5414, val loss: 0.9736, val acc: 0.5504  (best train acc: 0.5541, best val acc: 0.5636)\n",
      "[Epoch: 4040] train loss: 1.0066, train acc: 0.5457, val loss: 0.9739, val acc: 0.5531  (best train acc: 0.5541, best val acc: 0.5636)\n",
      "[Epoch: 4060] train loss: 1.0048, train acc: 0.5461, val loss: 0.9762, val acc: 0.5491  (best train acc: 0.5541, best val acc: 0.5636)\n",
      "[Epoch: 4080] train loss: 1.0016, train acc: 0.5494, val loss: 0.9747, val acc: 0.5494  (best train acc: 0.5541, best val acc: 0.5636)\n",
      "[Epoch: 4100] train loss: 1.0024, train acc: 0.5435, val loss: 0.9721, val acc: 0.5521  (best train acc: 0.5541, best val acc: 0.5636)\n",
      "[Epoch: 4120] train loss: 1.0022, train acc: 0.5437, val loss: 0.9737, val acc: 0.5511  (best train acc: 0.5541, best val acc: 0.5636)\n",
      "[Epoch: 4140] train loss: 1.0034, train acc: 0.5496, val loss: 0.9724, val acc: 0.5589  (best train acc: 0.5551, best val acc: 0.5636)\n",
      "[Epoch: 4160] train loss: 0.9985, train acc: 0.5475, val loss: 0.9761, val acc: 0.5521  (best train acc: 0.5551, best val acc: 0.5636)\n",
      "[Epoch: 4180] train loss: 1.0050, train acc: 0.5463, val loss: 0.9676, val acc: 0.5508  (best train acc: 0.5551, best val acc: 0.5636)\n",
      "[Epoch: 4200] train loss: 0.9949, train acc: 0.5488, val loss: 0.9670, val acc: 0.5558  (best train acc: 0.5551, best val acc: 0.5636)\n",
      "[Epoch: 4220] train loss: 0.9972, train acc: 0.5459, val loss: 0.9654, val acc: 0.5562  (best train acc: 0.5551, best val acc: 0.5636)\n",
      "[Epoch: 4240] train loss: 1.0013, train acc: 0.5486, val loss: 0.9649, val acc: 0.5528  (best train acc: 0.5551, best val acc: 0.5636)\n",
      "[Epoch: 4260] train loss: 1.0007, train acc: 0.5468, val loss: 0.9631, val acc: 0.5595  (best train acc: 0.5551, best val acc: 0.5636)\n",
      "[Epoch: 4280] train loss: 1.0028, train acc: 0.5497, val loss: 0.9690, val acc: 0.5555  (best train acc: 0.5551, best val acc: 0.5636)\n",
      "[Epoch: 4300] train loss: 1.0016, train acc: 0.5473, val loss: 0.9657, val acc: 0.5572  (best train acc: 0.5551, best val acc: 0.5636)\n",
      "[Epoch: 4320] train loss: 1.0035, train acc: 0.5445, val loss: 0.9628, val acc: 0.5585  (best train acc: 0.5553, best val acc: 0.5636)\n",
      "[Epoch: 4340] train loss: 0.9960, train acc: 0.5471, val loss: 0.9628, val acc: 0.5589  (best train acc: 0.5553, best val acc: 0.5636)\n",
      "[Epoch: 4360] train loss: 0.9985, train acc: 0.5484, val loss: 0.9648, val acc: 0.5548  (best train acc: 0.5559, best val acc: 0.5636)\n",
      "[Epoch: 4380] train loss: 1.0131, train acc: 0.5450, val loss: 0.9640, val acc: 0.5538  (best train acc: 0.5559, best val acc: 0.5636)\n",
      "[Epoch: 4400] train loss: 1.0244, train acc: 0.5484, val loss: 0.9712, val acc: 0.5521  (best train acc: 0.5559, best val acc: 0.5636)\n",
      "[Epoch: 4420] train loss: 1.0070, train acc: 0.5474, val loss: 0.9729, val acc: 0.5545  (best train acc: 0.5559, best val acc: 0.5646)\n",
      "[Epoch: 4440] train loss: 0.9943, train acc: 0.5515, val loss: 0.9657, val acc: 0.5551  (best train acc: 0.5559, best val acc: 0.5666)\n",
      "[Epoch: 4460] train loss: 0.9960, train acc: 0.5526, val loss: 0.9632, val acc: 0.5562  (best train acc: 0.5559, best val acc: 0.5666)\n",
      "[Epoch: 4480] train loss: 0.9991, train acc: 0.5510, val loss: 0.9680, val acc: 0.5578  (best train acc: 0.5570, best val acc: 0.5666)\n",
      "[Epoch: 4500] train loss: 1.0024, train acc: 0.5500, val loss: 0.9681, val acc: 0.5572  (best train acc: 0.5570, best val acc: 0.5666)\n",
      "[Epoch: 4520] train loss: 0.9955, train acc: 0.5557, val loss: 0.9632, val acc: 0.5548  (best train acc: 0.5570, best val acc: 0.5666)\n",
      "[Epoch: 4540] train loss: 1.0022, train acc: 0.5470, val loss: 0.9740, val acc: 0.5612  (best train acc: 0.5570, best val acc: 0.5666)\n",
      "[Epoch: 4560] train loss: 1.0272, train acc: 0.5466, val loss: 0.9921, val acc: 0.5359  (best train acc: 0.5570, best val acc: 0.5666)\n",
      "[Epoch: 4580] train loss: 1.0082, train acc: 0.5486, val loss: 0.9755, val acc: 0.5568  (best train acc: 0.5574, best val acc: 0.5666)\n",
      "[Epoch: 4600] train loss: 0.9944, train acc: 0.5508, val loss: 0.9638, val acc: 0.5511  (best train acc: 0.5574, best val acc: 0.5666)\n",
      "[Epoch: 4620] train loss: 0.9976, train acc: 0.5555, val loss: 0.9622, val acc: 0.5619  (best train acc: 0.5574, best val acc: 0.5666)\n",
      "[Epoch: 4640] train loss: 1.0010, train acc: 0.5423, val loss: 0.9685, val acc: 0.5531  (best train acc: 0.5574, best val acc: 0.5666)\n",
      "[Epoch: 4660] train loss: 0.9997, train acc: 0.5487, val loss: 0.9728, val acc: 0.5636  (best train acc: 0.5574, best val acc: 0.5666)\n",
      "[Epoch: 4680] train loss: 1.0049, train acc: 0.5455, val loss: 0.9713, val acc: 0.5460  (best train acc: 0.5574, best val acc: 0.5666)\n",
      "[Epoch: 4700] train loss: 1.0044, train acc: 0.5512, val loss: 0.9772, val acc: 0.5457  (best train acc: 0.5574, best val acc: 0.5666)\n",
      "[Epoch: 4720] train loss: 1.0037, train acc: 0.5500, val loss: 0.9649, val acc: 0.5555  (best train acc: 0.5574, best val acc: 0.5666)\n",
      "[Epoch: 4740] train loss: 0.9951, train acc: 0.5523, val loss: 0.9633, val acc: 0.5568  (best train acc: 0.5574, best val acc: 0.5666)\n",
      "[Epoch: 4760] train loss: 0.9955, train acc: 0.5550, val loss: 0.9631, val acc: 0.5545  (best train acc: 0.5574, best val acc: 0.5666)\n",
      "[Epoch: 4780] train loss: 1.0048, train acc: 0.5506, val loss: 0.9628, val acc: 0.5572  (best train acc: 0.5574, best val acc: 0.5666)\n",
      "[Epoch: 4800] train loss: 0.9995, train acc: 0.5523, val loss: 0.9629, val acc: 0.5599  (best train acc: 0.5574, best val acc: 0.5666)\n",
      "[Epoch: 4820] train loss: 0.9952, train acc: 0.5493, val loss: 0.9637, val acc: 0.5575  (best train acc: 0.5574, best val acc: 0.5666)\n",
      "[Epoch: 4840] train loss: 0.9978, train acc: 0.5500, val loss: 0.9700, val acc: 0.5642  (best train acc: 0.5574, best val acc: 0.5666)\n",
      "[Epoch: 4860] train loss: 0.9989, train acc: 0.5523, val loss: 0.9629, val acc: 0.5575  (best train acc: 0.5574, best val acc: 0.5666)\n",
      "[Epoch: 4880] train loss: 0.9967, train acc: 0.5547, val loss: 0.9630, val acc: 0.5589  (best train acc: 0.5574, best val acc: 0.5666)\n",
      "[Epoch: 4900] train loss: 0.9980, train acc: 0.5555, val loss: 0.9737, val acc: 0.5585  (best train acc: 0.5574, best val acc: 0.5666)\n",
      "[Epoch: 4920] train loss: 0.9975, train acc: 0.5531, val loss: 0.9697, val acc: 0.5518  (best train acc: 0.5574, best val acc: 0.5666)\n",
      "[Epoch: 4940] train loss: 1.0048, train acc: 0.5526, val loss: 0.9634, val acc: 0.5578  (best train acc: 0.5574, best val acc: 0.5666)\n",
      "[Epoch: 4960] train loss: 0.9917, train acc: 0.5542, val loss: 0.9663, val acc: 0.5616  (best train acc: 0.5574, best val acc: 0.5666)\n",
      "[Epoch: 4980] train loss: 1.0010, train acc: 0.5547, val loss: 0.9881, val acc: 0.5481  (best train acc: 0.5574, best val acc: 0.5666)\n",
      "[Epoch: 5000] train loss: 1.0107, train acc: 0.5313, val loss: 0.9888, val acc: 0.5454  (best train acc: 0.5574, best val acc: 0.5666)\n",
      "[Epoch: 5020] train loss: 1.0011, train acc: 0.5512, val loss: 0.9861, val acc: 0.5460  (best train acc: 0.5574, best val acc: 0.5666)\n",
      "[Epoch: 5040] train loss: 1.0056, train acc: 0.5479, val loss: 0.9751, val acc: 0.5541  (best train acc: 0.5574, best val acc: 0.5666)\n",
      "[Epoch: 5060] train loss: 1.0037, train acc: 0.5407, val loss: 0.9713, val acc: 0.5531  (best train acc: 0.5574, best val acc: 0.5673)\n",
      "[Epoch: 5080] train loss: 1.0037, train acc: 0.5518, val loss: 0.9814, val acc: 0.5467  (best train acc: 0.5574, best val acc: 0.5673)\n",
      "[Epoch: 5100] train loss: 0.9955, train acc: 0.5478, val loss: 0.9731, val acc: 0.5568  (best train acc: 0.5574, best val acc: 0.5676)\n",
      "[Epoch: 5120] train loss: 0.9978, train acc: 0.5495, val loss: 0.9691, val acc: 0.5541  (best train acc: 0.5574, best val acc: 0.5683)\n",
      "[Epoch: 5140] train loss: 1.0008, train acc: 0.5450, val loss: 0.9800, val acc: 0.5477  (best train acc: 0.5574, best val acc: 0.5683)\n",
      "[Epoch: 5160] train loss: 1.0123, train acc: 0.5428, val loss: 0.9624, val acc: 0.5572  (best train acc: 0.5574, best val acc: 0.5683)\n",
      "[Epoch: 5180] train loss: 1.0051, train acc: 0.5479, val loss: 0.9654, val acc: 0.5551  (best train acc: 0.5574, best val acc: 0.5683)\n",
      "[Epoch: 5200] train loss: 0.9940, train acc: 0.5532, val loss: 0.9727, val acc: 0.5531  (best train acc: 0.5574, best val acc: 0.5683)\n",
      "[Epoch: 5220] train loss: 1.0003, train acc: 0.5499, val loss: 0.9730, val acc: 0.5548  (best train acc: 0.5574, best val acc: 0.5683)\n",
      "[Epoch: 5240] train loss: 0.9990, train acc: 0.5518, val loss: 0.9819, val acc: 0.5535  (best train acc: 0.5574, best val acc: 0.5683)\n",
      "[Epoch: 5260] train loss: 1.0068, train acc: 0.5460, val loss: 0.9820, val acc: 0.5460  (best train acc: 0.5574, best val acc: 0.5683)\n",
      "[Epoch: 5280] train loss: 0.9984, train acc: 0.5487, val loss: 0.9748, val acc: 0.5541  (best train acc: 0.5574, best val acc: 0.5683)\n",
      "[Epoch: 5300] train loss: 0.9989, train acc: 0.5495, val loss: 0.9720, val acc: 0.5501  (best train acc: 0.5574, best val acc: 0.5693)\n",
      "[Epoch: 5320] train loss: 0.9959, train acc: 0.5547, val loss: 0.9696, val acc: 0.5565  (best train acc: 0.5574, best val acc: 0.5703)\n",
      "[Epoch: 5340] train loss: 0.9960, train acc: 0.5476, val loss: 0.9646, val acc: 0.5535  (best train acc: 0.5574, best val acc: 0.5703)\n",
      "[Epoch: 5360] train loss: 0.9912, train acc: 0.5517, val loss: 0.9634, val acc: 0.5548  (best train acc: 0.5579, best val acc: 0.5703)\n",
      "[Epoch: 5380] train loss: 1.0004, train acc: 0.5512, val loss: 0.9622, val acc: 0.5595  (best train acc: 0.5579, best val acc: 0.5703)\n",
      "[Epoch: 5400] train loss: 0.9958, train acc: 0.5495, val loss: 0.9613, val acc: 0.5575  (best train acc: 0.5579, best val acc: 0.5703)\n",
      "[Epoch: 5420] train loss: 0.9882, train acc: 0.5552, val loss: 0.9646, val acc: 0.5548  (best train acc: 0.5579, best val acc: 0.5703)\n",
      "[Epoch: 5440] train loss: 0.9904, train acc: 0.5557, val loss: 0.9695, val acc: 0.5565  (best train acc: 0.5579, best val acc: 0.5703)\n",
      "[Epoch: 5460] train loss: 1.0067, train acc: 0.5536, val loss: 0.9712, val acc: 0.5514  (best train acc: 0.5579, best val acc: 0.5703)\n",
      "[Epoch: 5480] train loss: 0.9930, train acc: 0.5481, val loss: 0.9762, val acc: 0.5636  (best train acc: 0.5579, best val acc: 0.5703)\n",
      "[Epoch: 5500] train loss: 1.0069, train acc: 0.5477, val loss: 0.9722, val acc: 0.5599  (best train acc: 0.5579, best val acc: 0.5703)\n",
      "[Epoch: 5520] train loss: 1.0019, train acc: 0.5461, val loss: 0.9683, val acc: 0.5676  (best train acc: 0.5579, best val acc: 0.5703)\n",
      "[Epoch: 5540] train loss: 0.9927, train acc: 0.5488, val loss: 0.9631, val acc: 0.5565  (best train acc: 0.5579, best val acc: 0.5703)\n",
      "[Epoch: 5560] train loss: 0.9902, train acc: 0.5531, val loss: 0.9615, val acc: 0.5575  (best train acc: 0.5579, best val acc: 0.5703)\n",
      "[Epoch: 5580] train loss: 0.9935, train acc: 0.5515, val loss: 0.9616, val acc: 0.5575  (best train acc: 0.5579, best val acc: 0.5703)\n",
      "[Epoch: 5600] train loss: 0.9925, train acc: 0.5525, val loss: 0.9611, val acc: 0.5585  (best train acc: 0.5579, best val acc: 0.5703)\n",
      "[Epoch: 5620] train loss: 0.9907, train acc: 0.5531, val loss: 0.9624, val acc: 0.5582  (best train acc: 0.5579, best val acc: 0.5703)\n",
      "[Epoch: 5640] train loss: 0.9957, train acc: 0.5538, val loss: 0.9630, val acc: 0.5511  (best train acc: 0.5579, best val acc: 0.5703)\n",
      "[Epoch: 5660] train loss: 0.9943, train acc: 0.5499, val loss: 0.9651, val acc: 0.5501  (best train acc: 0.5579, best val acc: 0.5703)\n",
      "[Epoch: 5680] train loss: 0.9936, train acc: 0.5523, val loss: 0.9608, val acc: 0.5575  (best train acc: 0.5579, best val acc: 0.5703)\n",
      "[Epoch: 5700] train loss: 0.9933, train acc: 0.5541, val loss: 0.9611, val acc: 0.5626  (best train acc: 0.5579, best val acc: 0.5703)\n",
      "[Epoch: 5720] train loss: 0.9934, train acc: 0.5536, val loss: 0.9609, val acc: 0.5589  (best train acc: 0.5579, best val acc: 0.5703)\n",
      "[Epoch: 5740] train loss: 0.9898, train acc: 0.5524, val loss: 0.9647, val acc: 0.5568  (best train acc: 0.5579, best val acc: 0.5703)\n",
      "[Epoch: 5760] train loss: 0.9941, train acc: 0.5528, val loss: 0.9619, val acc: 0.5589  (best train acc: 0.5579, best val acc: 0.5703)\n",
      "[Epoch: 5780] train loss: 0.9956, train acc: 0.5488, val loss: 0.9673, val acc: 0.5575  (best train acc: 0.5579, best val acc: 0.5703)\n",
      "[Epoch: 5800] train loss: 0.9910, train acc: 0.5517, val loss: 0.9607, val acc: 0.5578  (best train acc: 0.5579, best val acc: 0.5703)\n",
      "[Epoch: 5820] train loss: 0.9989, train acc: 0.5495, val loss: 0.9618, val acc: 0.5568  (best train acc: 0.5579, best val acc: 0.5703)\n",
      "[Epoch: 5840] train loss: 0.9935, train acc: 0.5537, val loss: 0.9633, val acc: 0.5558  (best train acc: 0.5579, best val acc: 0.5703)\n",
      "[Epoch: 5860] train loss: 0.9910, train acc: 0.5502, val loss: 0.9606, val acc: 0.5582  (best train acc: 0.5579, best val acc: 0.5703)\n",
      "[Epoch: 5880] train loss: 0.9923, train acc: 0.5546, val loss: 0.9640, val acc: 0.5548  (best train acc: 0.5579, best val acc: 0.5703)\n",
      "[Epoch: 5900] train loss: 0.9949, train acc: 0.5466, val loss: 0.9654, val acc: 0.5551  (best train acc: 0.5579, best val acc: 0.5703)\n",
      "[Epoch: 5920] train loss: 0.9919, train acc: 0.5561, val loss: 0.9606, val acc: 0.5632  (best train acc: 0.5579, best val acc: 0.5703)\n",
      "[Epoch: 5940] train loss: 0.9932, train acc: 0.5484, val loss: 0.9620, val acc: 0.5575  (best train acc: 0.5579, best val acc: 0.5703)\n",
      "[Epoch: 5960] train loss: 0.9945, train acc: 0.5484, val loss: 0.9616, val acc: 0.5582  (best train acc: 0.5579, best val acc: 0.5703)\n",
      "[Epoch: 5980] train loss: 0.9921, train acc: 0.5526, val loss: 0.9624, val acc: 0.5548  (best train acc: 0.5579, best val acc: 0.5703)\n",
      "[Epoch: 6000] train loss: 0.9898, train acc: 0.5560, val loss: 0.9647, val acc: 0.5558  (best train acc: 0.5579, best val acc: 0.5703)\n",
      "[Epoch: 6020] train loss: 0.9951, train acc: 0.5508, val loss: 0.9643, val acc: 0.5602  (best train acc: 0.5579, best val acc: 0.5703)\n",
      "[Epoch: 6040] train loss: 0.9911, train acc: 0.5527, val loss: 0.9633, val acc: 0.5555  (best train acc: 0.5580, best val acc: 0.5703)\n",
      "[Epoch: 6060] train loss: 0.9920, train acc: 0.5497, val loss: 0.9612, val acc: 0.5568  (best train acc: 0.5580, best val acc: 0.5703)\n",
      "[Epoch: 6080] train loss: 0.9944, train acc: 0.5479, val loss: 0.9612, val acc: 0.5595  (best train acc: 0.5580, best val acc: 0.5703)\n",
      "[Epoch: 6100] train loss: 0.9903, train acc: 0.5547, val loss: 0.9606, val acc: 0.5595  (best train acc: 0.5580, best val acc: 0.5703)\n",
      "[Epoch: 6120] train loss: 0.9893, train acc: 0.5522, val loss: 0.9634, val acc: 0.5622  (best train acc: 0.5580, best val acc: 0.5703)\n",
      "[Epoch: 6140] train loss: 0.9888, train acc: 0.5517, val loss: 0.9636, val acc: 0.5572  (best train acc: 0.5580, best val acc: 0.5703)\n",
      "[Epoch: 6160] train loss: 0.9937, train acc: 0.5536, val loss: 0.9615, val acc: 0.5555  (best train acc: 0.5580, best val acc: 0.5703)\n",
      "[Epoch: 6180] train loss: 0.9911, train acc: 0.5538, val loss: 0.9647, val acc: 0.5551  (best train acc: 0.5580, best val acc: 0.5703)\n",
      "[Epoch: 6200] train loss: 0.9902, train acc: 0.5485, val loss: 0.9641, val acc: 0.5595  (best train acc: 0.5580, best val acc: 0.5703)\n",
      "[Epoch: 6220] train loss: 0.9953, train acc: 0.5480, val loss: 0.9624, val acc: 0.5599  (best train acc: 0.5580, best val acc: 0.5703)\n",
      "[Epoch: 6240] train loss: 0.9998, train acc: 0.5506, val loss: 0.9633, val acc: 0.5582  (best train acc: 0.5580, best val acc: 0.5703)\n",
      "[Epoch: 6260] train loss: 0.9959, train acc: 0.5504, val loss: 0.9600, val acc: 0.5622  (best train acc: 0.5580, best val acc: 0.5703)\n",
      "[Epoch: 6280] train loss: 0.9962, train acc: 0.5489, val loss: 0.9616, val acc: 0.5578  (best train acc: 0.5585, best val acc: 0.5703)\n",
      "[Epoch: 6300] train loss: 0.9943, train acc: 0.5533, val loss: 0.9791, val acc: 0.5686  (best train acc: 0.5585, best val acc: 0.5703)\n",
      "[Epoch: 6320] train loss: 1.0048, train acc: 0.5337, val loss: 0.9938, val acc: 0.5447  (best train acc: 0.5585, best val acc: 0.5703)\n",
      "[Epoch: 6340] train loss: 1.0187, train acc: 0.5417, val loss: 0.9804, val acc: 0.5713  (best train acc: 0.5585, best val acc: 0.5767)\n",
      "[Epoch: 6360] train loss: 1.0343, train acc: 0.5195, val loss: 0.9644, val acc: 0.5609  (best train acc: 0.5585, best val acc: 0.5767)\n",
      "[Epoch: 6380] train loss: 1.0214, train acc: 0.5159, val loss: 0.9743, val acc: 0.5582  (best train acc: 0.5585, best val acc: 0.5767)\n",
      "[Epoch: 6400] train loss: 1.0335, train acc: 0.5015, val loss: 0.9854, val acc: 0.5450  (best train acc: 0.5585, best val acc: 0.5767)\n",
      "[Epoch: 6420] train loss: 1.0162, train acc: 0.5254, val loss: 0.9820, val acc: 0.5616  (best train acc: 0.5585, best val acc: 0.5767)\n",
      "[Epoch: 6440] train loss: 0.9958, train acc: 0.5448, val loss: 0.9597, val acc: 0.5686  (best train acc: 0.5585, best val acc: 0.5767)\n",
      "[Epoch: 6460] train loss: 0.9906, train acc: 0.5498, val loss: 0.9613, val acc: 0.5626  (best train acc: 0.5585, best val acc: 0.5767)\n",
      "[Epoch: 6480] train loss: 0.9904, train acc: 0.5544, val loss: 0.9710, val acc: 0.5572  (best train acc: 0.5585, best val acc: 0.5767)\n",
      "[Epoch: 6500] train loss: 0.9884, train acc: 0.5531, val loss: 0.9643, val acc: 0.5578  (best train acc: 0.5585, best val acc: 0.5767)\n",
      "[Epoch: 6520] train loss: 0.9944, train acc: 0.5487, val loss: 0.9655, val acc: 0.5599  (best train acc: 0.5585, best val acc: 0.5767)\n",
      "[Epoch: 6540] train loss: 0.9905, train acc: 0.5505, val loss: 0.9620, val acc: 0.5626  (best train acc: 0.5585, best val acc: 0.5767)\n",
      "[Epoch: 6560] train loss: 1.0024, train acc: 0.5518, val loss: 0.9655, val acc: 0.5589  (best train acc: 0.5585, best val acc: 0.5767)\n",
      "[Epoch: 6580] train loss: 0.9909, train acc: 0.5518, val loss: 0.9649, val acc: 0.5592  (best train acc: 0.5585, best val acc: 0.5767)\n",
      "[Epoch: 6600] train loss: 0.9931, train acc: 0.5495, val loss: 0.9610, val acc: 0.5558  (best train acc: 0.5585, best val acc: 0.5767)\n",
      "[Epoch: 6620] train loss: 0.9955, train acc: 0.5521, val loss: 0.9676, val acc: 0.5572  (best train acc: 0.5585, best val acc: 0.5767)\n",
      "[Epoch: 6640] train loss: 0.9911, train acc: 0.5516, val loss: 0.9618, val acc: 0.5592  (best train acc: 0.5585, best val acc: 0.5767)\n",
      "[Epoch: 6660] train loss: 0.9855, train acc: 0.5531, val loss: 0.9641, val acc: 0.5582  (best train acc: 0.5585, best val acc: 0.5767)\n",
      "[Epoch: 6680] train loss: 1.0030, train acc: 0.5430, val loss: 0.9826, val acc: 0.5535  (best train acc: 0.5585, best val acc: 0.5767)\n",
      "[Epoch: 6700] train loss: 0.9996, train acc: 0.5445, val loss: 0.9670, val acc: 0.5514  (best train acc: 0.5585, best val acc: 0.5767)\n",
      "[Epoch: 6720] train loss: 0.9880, train acc: 0.5520, val loss: 0.9658, val acc: 0.5673  (best train acc: 0.5585, best val acc: 0.5767)\n",
      "[Epoch: 6740] train loss: 0.9883, train acc: 0.5536, val loss: 0.9586, val acc: 0.5622  (best train acc: 0.5585, best val acc: 0.5767)\n",
      "[Epoch: 6760] train loss: 0.9963, train acc: 0.5487, val loss: 0.9590, val acc: 0.5616  (best train acc: 0.5585, best val acc: 0.5767)\n",
      "[Epoch: 6780] train loss: 0.9904, train acc: 0.5484, val loss: 0.9595, val acc: 0.5649  (best train acc: 0.5585, best val acc: 0.5767)\n",
      "[Epoch: 6800] train loss: 0.9868, train acc: 0.5541, val loss: 0.9600, val acc: 0.5558  (best train acc: 0.5585, best val acc: 0.5767)\n",
      "[Epoch: 6820] train loss: 0.9887, train acc: 0.5537, val loss: 0.9613, val acc: 0.5609  (best train acc: 0.5585, best val acc: 0.5767)\n",
      "[Epoch: 6840] train loss: 0.9863, train acc: 0.5512, val loss: 0.9618, val acc: 0.5602  (best train acc: 0.5585, best val acc: 0.5767)\n",
      "[Epoch: 6860] train loss: 0.9980, train acc: 0.5494, val loss: 0.9585, val acc: 0.5599  (best train acc: 0.5585, best val acc: 0.5767)\n",
      "[Epoch: 6880] train loss: 0.9851, train acc: 0.5485, val loss: 0.9653, val acc: 0.5592  (best train acc: 0.5585, best val acc: 0.5767)\n",
      "[Epoch: 6900] train loss: 1.0010, train acc: 0.5495, val loss: 0.9587, val acc: 0.5562  (best train acc: 0.5585, best val acc: 0.5767)\n",
      "[Epoch: 6920] train loss: 0.9885, train acc: 0.5527, val loss: 0.9623, val acc: 0.5575  (best train acc: 0.5585, best val acc: 0.5767)\n",
      "[Epoch: 6940] train loss: 0.9874, train acc: 0.5500, val loss: 0.9693, val acc: 0.5592  (best train acc: 0.5585, best val acc: 0.5767)\n",
      "[Epoch: 6960] train loss: 1.0115, train acc: 0.5500, val loss: 0.9656, val acc: 0.5578  (best train acc: 0.5585, best val acc: 0.5767)\n",
      "[Epoch: 6980] train loss: 1.0219, train acc: 0.5420, val loss: 0.9853, val acc: 0.5454  (best train acc: 0.5585, best val acc: 0.5767)\n",
      "[Epoch: 7000] train loss: 1.0109, train acc: 0.5469, val loss: 0.9715, val acc: 0.5521  (best train acc: 0.5585, best val acc: 0.5767)\n",
      "[Epoch: 7020] train loss: 1.0269, train acc: 0.5215, val loss: 1.0632, val acc: 0.5238  (best train acc: 0.5585, best val acc: 0.5767)\n",
      "[Epoch: 7040] train loss: 1.0068, train acc: 0.5446, val loss: 0.9890, val acc: 0.5585  (best train acc: 0.5585, best val acc: 0.5767)\n",
      "[Epoch: 7060] train loss: 0.9998, train acc: 0.5492, val loss: 0.9802, val acc: 0.5562  (best train acc: 0.5585, best val acc: 0.5767)\n",
      "[Epoch: 7080] train loss: 0.9936, train acc: 0.5535, val loss: 0.9727, val acc: 0.5616  (best train acc: 0.5585, best val acc: 0.5767)\n",
      "[Epoch: 7100] train loss: 1.0037, train acc: 0.5498, val loss: 0.9675, val acc: 0.5622  (best train acc: 0.5585, best val acc: 0.5767)\n",
      "[Epoch: 7120] train loss: 0.9986, train acc: 0.5487, val loss: 0.9707, val acc: 0.5585  (best train acc: 0.5585, best val acc: 0.5767)\n",
      "[Epoch: 7140] train loss: 1.0029, train acc: 0.5475, val loss: 0.9704, val acc: 0.5575  (best train acc: 0.5585, best val acc: 0.5767)\n",
      "[Epoch: 7160] train loss: 0.9936, train acc: 0.5510, val loss: 0.9754, val acc: 0.5636  (best train acc: 0.5585, best val acc: 0.5767)\n",
      "[Epoch: 7180] train loss: 1.0094, train acc: 0.5465, val loss: 0.9656, val acc: 0.5690  (best train acc: 0.5585, best val acc: 0.5767)\n",
      "[Epoch: 7200] train loss: 0.9995, train acc: 0.5504, val loss: 0.9739, val acc: 0.5582  (best train acc: 0.5585, best val acc: 0.5767)\n",
      "[Epoch: 7220] train loss: 1.0003, train acc: 0.5518, val loss: 0.9650, val acc: 0.5649  (best train acc: 0.5585, best val acc: 0.5767)\n",
      "[Epoch: 7240] train loss: 0.9899, train acc: 0.5514, val loss: 0.9752, val acc: 0.5535  (best train acc: 0.5585, best val acc: 0.5767)\n",
      "[Epoch: 7260] train loss: 0.9981, train acc: 0.5471, val loss: 0.9713, val acc: 0.5602  (best train acc: 0.5585, best val acc: 0.5767)\n",
      "[Epoch: 7280] train loss: 1.0079, train acc: 0.5489, val loss: 0.9627, val acc: 0.5605  (best train acc: 0.5585, best val acc: 0.5767)\n",
      "[Epoch: 7300] train loss: 0.9870, train acc: 0.5489, val loss: 0.9743, val acc: 0.5565  (best train acc: 0.5585, best val acc: 0.5767)\n",
      "[Epoch: 7320] train loss: 0.9984, train acc: 0.5530, val loss: 0.9588, val acc: 0.5602  (best train acc: 0.5585, best val acc: 0.5767)\n",
      "[Epoch: 7340] train loss: 0.9906, train acc: 0.5511, val loss: 0.9584, val acc: 0.5558  (best train acc: 0.5585, best val acc: 0.5767)\n",
      "[Epoch: 7360] train loss: 0.9839, train acc: 0.5502, val loss: 0.9577, val acc: 0.5589  (best train acc: 0.5585, best val acc: 0.5767)\n",
      "[Epoch: 7380] train loss: 0.9937, train acc: 0.5492, val loss: 0.9835, val acc: 0.5504  (best train acc: 0.5585, best val acc: 0.5767)\n",
      "[Epoch: 7400] train loss: 1.0014, train acc: 0.5474, val loss: 0.9788, val acc: 0.5474  (best train acc: 0.5585, best val acc: 0.5767)\n",
      "[Epoch: 7420] train loss: 0.9918, train acc: 0.5505, val loss: 0.9820, val acc: 0.5548  (best train acc: 0.5585, best val acc: 0.5767)\n",
      "[Epoch: 7440] train loss: 0.9887, train acc: 0.5500, val loss: 0.9679, val acc: 0.5680  (best train acc: 0.5585, best val acc: 0.5767)\n",
      "[Epoch: 7460] train loss: 0.9864, train acc: 0.5512, val loss: 0.9587, val acc: 0.5501  (best train acc: 0.5585, best val acc: 0.5767)\n",
      "[Epoch: 7480] train loss: 0.9854, train acc: 0.5541, val loss: 0.9588, val acc: 0.5518  (best train acc: 0.5585, best val acc: 0.5767)\n",
      "[Epoch: 7500] train loss: 0.9840, train acc: 0.5531, val loss: 0.9564, val acc: 0.5636  (best train acc: 0.5585, best val acc: 0.5767)\n",
      "[Epoch: 7520] train loss: 0.9889, train acc: 0.5505, val loss: 0.9563, val acc: 0.5545  (best train acc: 0.5585, best val acc: 0.5767)\n",
      "[Epoch: 7540] train loss: 0.9870, train acc: 0.5513, val loss: 0.9557, val acc: 0.5585  (best train acc: 0.5585, best val acc: 0.5767)\n",
      "[Epoch: 7560] train loss: 0.9855, train acc: 0.5541, val loss: 0.9586, val acc: 0.5511  (best train acc: 0.5585, best val acc: 0.5767)\n",
      "[Epoch: 7580] train loss: 0.9886, train acc: 0.5522, val loss: 0.9617, val acc: 0.5551  (best train acc: 0.5585, best val acc: 0.5767)\n",
      "[Epoch: 7600] train loss: 0.9841, train acc: 0.5519, val loss: 0.9561, val acc: 0.5589  (best train acc: 0.5585, best val acc: 0.5767)\n",
      "[Epoch: 7620] train loss: 0.9922, train acc: 0.5468, val loss: 0.9628, val acc: 0.5602  (best train acc: 0.5585, best val acc: 0.5767)\n",
      "[Epoch: 7640] train loss: 0.9868, train acc: 0.5513, val loss: 0.9587, val acc: 0.5494  (best train acc: 0.5585, best val acc: 0.5767)\n",
      "[Epoch: 7660] train loss: 0.9874, train acc: 0.5499, val loss: 0.9578, val acc: 0.5558  (best train acc: 0.5585, best val acc: 0.5767)\n",
      "[Epoch: 7680] train loss: 0.9857, train acc: 0.5453, val loss: 0.9557, val acc: 0.5609  (best train acc: 0.5585, best val acc: 0.5767)\n",
      "[Epoch: 7700] train loss: 0.9870, train acc: 0.5505, val loss: 0.9625, val acc: 0.5578  (best train acc: 0.5585, best val acc: 0.5767)\n",
      "[Epoch: 7720] train loss: 0.9897, train acc: 0.5450, val loss: 0.9576, val acc: 0.5575  (best train acc: 0.5585, best val acc: 0.5767)\n",
      "[Epoch: 7740] train loss: 0.9881, train acc: 0.5510, val loss: 0.9562, val acc: 0.5599  (best train acc: 0.5585, best val acc: 0.5767)\n",
      "[Epoch: 7760] train loss: 0.9884, train acc: 0.5510, val loss: 0.9585, val acc: 0.5545  (best train acc: 0.5585, best val acc: 0.5767)\n",
      "[Epoch: 7780] train loss: 0.9887, train acc: 0.5472, val loss: 0.9574, val acc: 0.5612  (best train acc: 0.5585, best val acc: 0.5767)\n",
      "[Epoch: 7800] train loss: 0.9890, train acc: 0.5505, val loss: 0.9665, val acc: 0.5501  (best train acc: 0.5585, best val acc: 0.5767)\n",
      "[Epoch: 7820] train loss: 0.9853, train acc: 0.5500, val loss: 0.9575, val acc: 0.5518  (best train acc: 0.5585, best val acc: 0.5767)\n",
      "[Epoch: 7840] train loss: 0.9878, train acc: 0.5482, val loss: 0.9599, val acc: 0.5531  (best train acc: 0.5585, best val acc: 0.5767)\n",
      "[Epoch: 7860] train loss: 0.9850, train acc: 0.5515, val loss: 0.9586, val acc: 0.5609  (best train acc: 0.5585, best val acc: 0.5767)\n",
      "[Epoch: 7880] train loss: 0.9903, train acc: 0.5489, val loss: 0.9589, val acc: 0.5538  (best train acc: 0.5585, best val acc: 0.5767)\n",
      "[Epoch: 7900] train loss: 0.9847, train acc: 0.5505, val loss: 0.9565, val acc: 0.5578  (best train acc: 0.5585, best val acc: 0.5767)\n",
      "[Epoch: 7920] train loss: 0.9824, train acc: 0.5526, val loss: 0.9566, val acc: 0.5575  (best train acc: 0.5585, best val acc: 0.5767)\n",
      "[Epoch: 7940] train loss: 0.9888, train acc: 0.5451, val loss: 0.9620, val acc: 0.5562  (best train acc: 0.5585, best val acc: 0.5767)\n",
      "[Epoch: 7960] train loss: 0.9894, train acc: 0.5487, val loss: 0.9573, val acc: 0.5568  (best train acc: 0.5585, best val acc: 0.5767)\n",
      "[Epoch: 7980] train loss: 0.9909, train acc: 0.5473, val loss: 0.9693, val acc: 0.5636  (best train acc: 0.5585, best val acc: 0.5767)\n",
      "[Epoch: 8000] train loss: 0.9832, train acc: 0.5527, val loss: 0.9610, val acc: 0.5565  (best train acc: 0.5585, best val acc: 0.5767)\n",
      "[Epoch: 8020] train loss: 0.9842, train acc: 0.5559, val loss: 0.9611, val acc: 0.5616  (best train acc: 0.5585, best val acc: 0.5767)\n",
      "[Epoch: 8040] train loss: 0.9966, train acc: 0.5501, val loss: 0.9584, val acc: 0.5602  (best train acc: 0.5585, best val acc: 0.5767)\n",
      "[Epoch: 8060] train loss: 0.9998, train acc: 0.5495, val loss: 0.9559, val acc: 0.5612  (best train acc: 0.5585, best val acc: 0.5767)\n",
      "[Epoch: 8080] train loss: 0.9887, train acc: 0.5567, val loss: 0.9600, val acc: 0.5541  (best train acc: 0.5585, best val acc: 0.5767)\n",
      "[Epoch: 8100] train loss: 0.9865, train acc: 0.5497, val loss: 0.9599, val acc: 0.5545  (best train acc: 0.5585, best val acc: 0.5767)\n",
      "[Epoch: 8120] train loss: 0.9850, train acc: 0.5523, val loss: 0.9549, val acc: 0.5636  (best train acc: 0.5585, best val acc: 0.5767)\n",
      "[Epoch: 8140] train loss: 0.9880, train acc: 0.5527, val loss: 0.9553, val acc: 0.5619  (best train acc: 0.5585, best val acc: 0.5767)\n",
      "[Epoch: 8160] train loss: 0.9889, train acc: 0.5489, val loss: 0.9561, val acc: 0.5609  (best train acc: 0.5585, best val acc: 0.5767)\n",
      "[Epoch: 8180] train loss: 0.9836, train acc: 0.5534, val loss: 0.9552, val acc: 0.5619  (best train acc: 0.5585, best val acc: 0.5767)\n",
      "[Epoch: 8200] train loss: 0.9890, train acc: 0.5521, val loss: 0.9610, val acc: 0.5535  (best train acc: 0.5585, best val acc: 0.5767)\n",
      "[Epoch: 8220] train loss: 0.9898, train acc: 0.5508, val loss: 0.9557, val acc: 0.5609  (best train acc: 0.5585, best val acc: 0.5767)\n",
      "[Epoch: 8240] train loss: 0.9865, train acc: 0.5513, val loss: 0.9700, val acc: 0.5612  (best train acc: 0.5585, best val acc: 0.5767)\n",
      "[Epoch: 8260] train loss: 1.0165, train acc: 0.5432, val loss: 0.9868, val acc: 0.5457  (best train acc: 0.5585, best val acc: 0.5767)\n",
      "[Epoch: 8280] train loss: 0.9975, train acc: 0.5518, val loss: 0.9786, val acc: 0.5531  (best train acc: 0.5585, best val acc: 0.5767)\n",
      "[Epoch: 8300] train loss: 0.9899, train acc: 0.5506, val loss: 0.9570, val acc: 0.5599  (best train acc: 0.5585, best val acc: 0.5767)\n",
      "[Epoch: 8320] train loss: 1.0132, train acc: 0.5258, val loss: 0.9759, val acc: 0.5548  (best train acc: 0.5585, best val acc: 0.5767)\n",
      "[Epoch: 8340] train loss: 0.9927, train acc: 0.5556, val loss: 0.9888, val acc: 0.5602  (best train acc: 0.5585, best val acc: 0.5767)\n",
      "[Epoch: 8360] train loss: 1.0025, train acc: 0.5521, val loss: 0.9807, val acc: 0.5450  (best train acc: 0.5585, best val acc: 0.5767)\n",
      "[Epoch: 8380] train loss: 0.9941, train acc: 0.5551, val loss: 0.9930, val acc: 0.5454  (best train acc: 0.5585, best val acc: 0.5767)\n",
      "[Epoch: 8400] train loss: 0.9944, train acc: 0.5495, val loss: 0.9625, val acc: 0.5589  (best train acc: 0.5585, best val acc: 0.5767)\n",
      "[Epoch: 8420] train loss: 0.9994, train acc: 0.5500, val loss: 0.9576, val acc: 0.5589  (best train acc: 0.5585, best val acc: 0.5767)\n",
      "[Epoch: 8440] train loss: 0.9790, train acc: 0.5557, val loss: 0.9543, val acc: 0.5622  (best train acc: 0.5592, best val acc: 0.5767)\n",
      "[Epoch: 8460] train loss: 0.9855, train acc: 0.5530, val loss: 0.9548, val acc: 0.5595  (best train acc: 0.5592, best val acc: 0.5767)\n",
      "[Epoch: 8480] train loss: 0.9873, train acc: 0.5497, val loss: 0.9633, val acc: 0.5528  (best train acc: 0.5592, best val acc: 0.5767)\n",
      "[Epoch: 8500] train loss: 0.9815, train acc: 0.5549, val loss: 0.9537, val acc: 0.5612  (best train acc: 0.5592, best val acc: 0.5767)\n",
      "[Epoch: 8520] train loss: 0.9843, train acc: 0.5461, val loss: 0.9554, val acc: 0.5639  (best train acc: 0.5592, best val acc: 0.5767)\n",
      "[Epoch: 8540] train loss: 0.9850, train acc: 0.5495, val loss: 0.9595, val acc: 0.5541  (best train acc: 0.5592, best val acc: 0.5767)\n",
      "[Epoch: 8560] train loss: 0.9782, train acc: 0.5570, val loss: 0.9539, val acc: 0.5639  (best train acc: 0.5592, best val acc: 0.5767)\n",
      "[Epoch: 8580] train loss: 0.9926, train acc: 0.5482, val loss: 0.9576, val acc: 0.5605  (best train acc: 0.5592, best val acc: 0.5767)\n",
      "[Epoch: 8600] train loss: 0.9869, train acc: 0.5536, val loss: 0.9571, val acc: 0.5538  (best train acc: 0.5592, best val acc: 0.5767)\n",
      "[Epoch: 8620] train loss: 0.9968, train acc: 0.5481, val loss: 0.9538, val acc: 0.5589  (best train acc: 0.5592, best val acc: 0.5767)\n",
      "[Epoch: 8640] train loss: 1.0067, train acc: 0.5442, val loss: 0.9864, val acc: 0.5514  (best train acc: 0.5592, best val acc: 0.5767)\n",
      "[Epoch: 8660] train loss: 0.9960, train acc: 0.5453, val loss: 0.9652, val acc: 0.5575  (best train acc: 0.5592, best val acc: 0.5767)\n",
      "[Epoch: 8680] train loss: 1.0030, train acc: 0.5484, val loss: 0.9631, val acc: 0.5589  (best train acc: 0.5592, best val acc: 0.5767)\n",
      "[Epoch: 8700] train loss: 1.0134, train acc: 0.5524, val loss: 0.9585, val acc: 0.5616  (best train acc: 0.5592, best val acc: 0.5767)\n",
      "[Epoch: 8720] train loss: 0.9854, train acc: 0.5497, val loss: 0.9690, val acc: 0.5754  (best train acc: 0.5592, best val acc: 0.5767)\n",
      "[Epoch: 8740] train loss: 0.9942, train acc: 0.5486, val loss: 0.9784, val acc: 0.5551  (best train acc: 0.5592, best val acc: 0.5767)\n",
      "[Epoch: 8760] train loss: 0.9960, train acc: 0.5461, val loss: 0.9833, val acc: 0.5599  (best train acc: 0.5592, best val acc: 0.5767)\n",
      "[Epoch: 8780] train loss: 0.9951, train acc: 0.5486, val loss: 0.9745, val acc: 0.5518  (best train acc: 0.5592, best val acc: 0.5774)\n",
      "[Epoch: 8800] train loss: 0.9991, train acc: 0.5486, val loss: 0.9654, val acc: 0.5686  (best train acc: 0.5592, best val acc: 0.5774)\n",
      "[Epoch: 8820] train loss: 1.0019, train acc: 0.5497, val loss: 0.9686, val acc: 0.5676  (best train acc: 0.5592, best val acc: 0.5774)\n",
      "[Epoch: 8840] train loss: 1.0149, train acc: 0.5447, val loss: 0.9807, val acc: 0.5315  (best train acc: 0.5592, best val acc: 0.5774)\n",
      "[Epoch: 8860] train loss: 0.9954, train acc: 0.5481, val loss: 0.9811, val acc: 0.5622  (best train acc: 0.5592, best val acc: 0.5774)\n",
      "[Epoch: 8880] train loss: 1.0217, train acc: 0.5492, val loss: 0.9684, val acc: 0.5612  (best train acc: 0.5592, best val acc: 0.5774)\n",
      "[Epoch: 8900] train loss: 0.9987, train acc: 0.5497, val loss: 0.9692, val acc: 0.5494  (best train acc: 0.5592, best val acc: 0.5774)\n",
      "[Epoch: 8920] train loss: 0.9822, train acc: 0.5495, val loss: 0.9565, val acc: 0.5545  (best train acc: 0.5592, best val acc: 0.5774)\n",
      "[Epoch: 8940] train loss: 0.9862, train acc: 0.5513, val loss: 0.9544, val acc: 0.5575  (best train acc: 0.5592, best val acc: 0.5774)\n",
      "[Epoch: 8960] train loss: 0.9841, train acc: 0.5479, val loss: 0.9554, val acc: 0.5639  (best train acc: 0.5592, best val acc: 0.5774)\n",
      "[Epoch: 8980] train loss: 0.9876, train acc: 0.5513, val loss: 0.9570, val acc: 0.5562  (best train acc: 0.5592, best val acc: 0.5774)\n",
      "[Epoch: 9000] train loss: 0.9852, train acc: 0.5472, val loss: 0.9571, val acc: 0.5562  (best train acc: 0.5592, best val acc: 0.5774)\n",
      "[Epoch: 9020] train loss: 0.9875, train acc: 0.5476, val loss: 0.9613, val acc: 0.5575  (best train acc: 0.5592, best val acc: 0.5774)\n",
      "[Epoch: 9040] train loss: 0.9846, train acc: 0.5513, val loss: 0.9543, val acc: 0.5616  (best train acc: 0.5592, best val acc: 0.5774)\n",
      "[Epoch: 9060] train loss: 0.9824, train acc: 0.5531, val loss: 0.9523, val acc: 0.5653  (best train acc: 0.5592, best val acc: 0.5774)\n",
      "[Epoch: 9080] train loss: 0.9848, train acc: 0.5547, val loss: 0.9547, val acc: 0.5585  (best train acc: 0.5592, best val acc: 0.5774)\n",
      "[Epoch: 9100] train loss: 0.9846, train acc: 0.5493, val loss: 0.9536, val acc: 0.5653  (best train acc: 0.5592, best val acc: 0.5774)\n",
      "[Epoch: 9120] train loss: 0.9839, train acc: 0.5517, val loss: 0.9522, val acc: 0.5669  (best train acc: 0.5592, best val acc: 0.5774)\n",
      "[Epoch: 9140] train loss: 0.9820, train acc: 0.5558, val loss: 0.9534, val acc: 0.5676  (best train acc: 0.5592, best val acc: 0.5774)\n",
      "[Epoch: 9160] train loss: 0.9856, train acc: 0.5536, val loss: 0.9522, val acc: 0.5673  (best train acc: 0.5592, best val acc: 0.5774)\n",
      "[Epoch: 9180] train loss: 0.9801, train acc: 0.5502, val loss: 0.9531, val acc: 0.5605  (best train acc: 0.5592, best val acc: 0.5774)\n",
      "[Epoch: 9200] train loss: 0.9856, train acc: 0.5497, val loss: 0.9572, val acc: 0.5562  (best train acc: 0.5592, best val acc: 0.5774)\n",
      "[Epoch: 9220] train loss: 0.9833, train acc: 0.5498, val loss: 0.9587, val acc: 0.5491  (best train acc: 0.5592, best val acc: 0.5774)\n",
      "[Epoch: 9240] train loss: 0.9823, train acc: 0.5509, val loss: 0.9536, val acc: 0.5629  (best train acc: 0.5592, best val acc: 0.5774)\n",
      "[Epoch: 9260] train loss: 0.9842, train acc: 0.5525, val loss: 0.9557, val acc: 0.5562  (best train acc: 0.5592, best val acc: 0.5774)\n",
      "[Epoch: 9280] train loss: 0.9849, train acc: 0.5450, val loss: 0.9539, val acc: 0.5646  (best train acc: 0.5592, best val acc: 0.5774)\n",
      "[Epoch: 9300] train loss: 0.9840, train acc: 0.5544, val loss: 0.9625, val acc: 0.5616  (best train acc: 0.5592, best val acc: 0.5774)\n",
      "[Epoch: 9320] train loss: 0.9933, train acc: 0.5441, val loss: 0.9589, val acc: 0.5639  (best train acc: 0.5592, best val acc: 0.5774)\n",
      "[Epoch: 9340] train loss: 0.9941, train acc: 0.5513, val loss: 0.9543, val acc: 0.5585  (best train acc: 0.5592, best val acc: 0.5774)\n",
      "[Epoch: 9360] train loss: 0.9932, train acc: 0.5524, val loss: 0.9586, val acc: 0.5568  (best train acc: 0.5592, best val acc: 0.5774)\n",
      "[Epoch: 9380] train loss: 0.9848, train acc: 0.5527, val loss: 0.9599, val acc: 0.5595  (best train acc: 0.5592, best val acc: 0.5774)\n",
      "[Epoch: 9400] train loss: 0.9923, train acc: 0.5459, val loss: 0.9537, val acc: 0.5585  (best train acc: 0.5592, best val acc: 0.5774)\n",
      "[Epoch: 9420] train loss: 0.9927, train acc: 0.5485, val loss: 0.9788, val acc: 0.5585  (best train acc: 0.5592, best val acc: 0.5774)\n",
      "[Epoch: 9440] train loss: 0.9801, train acc: 0.5491, val loss: 0.9556, val acc: 0.5578  (best train acc: 0.5592, best val acc: 0.5774)\n",
      "[Epoch: 9460] train loss: 0.9859, train acc: 0.5505, val loss: 0.9654, val acc: 0.5568  (best train acc: 0.5592, best val acc: 0.5774)\n",
      "[Epoch: 9480] train loss: 0.9900, train acc: 0.5513, val loss: 0.9577, val acc: 0.5524  (best train acc: 0.5592, best val acc: 0.5774)\n",
      "[Epoch: 9500] train loss: 0.9914, train acc: 0.5479, val loss: 0.9543, val acc: 0.5541  (best train acc: 0.5592, best val acc: 0.5774)\n",
      "[Epoch: 9520] train loss: 0.9862, train acc: 0.5508, val loss: 0.9586, val acc: 0.5605  (best train acc: 0.5592, best val acc: 0.5774)\n",
      "[Epoch: 9540] train loss: 0.9995, train acc: 0.5526, val loss: 0.9799, val acc: 0.5612  (best train acc: 0.5592, best val acc: 0.5774)\n",
      "[Epoch: 9560] train loss: 1.0065, train acc: 0.5524, val loss: 0.9835, val acc: 0.5491  (best train acc: 0.5592, best val acc: 0.5798)\n",
      "[Epoch: 9580] train loss: 1.0065, train acc: 0.5385, val loss: 0.9797, val acc: 0.5565  (best train acc: 0.5592, best val acc: 0.5798)\n",
      "[Epoch: 9600] train loss: 0.9905, train acc: 0.5520, val loss: 0.9611, val acc: 0.5541  (best train acc: 0.5592, best val acc: 0.5818)\n",
      "[Epoch: 9620] train loss: 0.9806, train acc: 0.5527, val loss: 0.9524, val acc: 0.5629  (best train acc: 0.5592, best val acc: 0.5818)\n",
      "[Epoch: 9640] train loss: 0.9863, train acc: 0.5485, val loss: 0.9595, val acc: 0.5646  (best train acc: 0.5592, best val acc: 0.5818)\n",
      "[Epoch: 9660] train loss: 0.9854, train acc: 0.5471, val loss: 0.9527, val acc: 0.5612  (best train acc: 0.5592, best val acc: 0.5818)\n",
      "[Epoch: 9680] train loss: 0.9827, train acc: 0.5453, val loss: 0.9619, val acc: 0.5572  (best train acc: 0.5592, best val acc: 0.5818)\n",
      "[Epoch: 9700] train loss: 0.9832, train acc: 0.5495, val loss: 0.9552, val acc: 0.5572  (best train acc: 0.5592, best val acc: 0.5818)\n",
      "[Epoch: 9720] train loss: 0.9875, train acc: 0.5495, val loss: 0.9544, val acc: 0.5555  (best train acc: 0.5592, best val acc: 0.5818)\n",
      "[Epoch: 9740] train loss: 0.9961, train acc: 0.5482, val loss: 0.9662, val acc: 0.5612  (best train acc: 0.5592, best val acc: 0.5818)\n",
      "[Epoch: 9760] train loss: 1.0072, train acc: 0.5463, val loss: 0.9734, val acc: 0.5524  (best train acc: 0.5592, best val acc: 0.5818)\n",
      "[Epoch: 9780] train loss: 0.9913, train acc: 0.5513, val loss: 1.0012, val acc: 0.5484  (best train acc: 0.5592, best val acc: 0.5818)\n",
      "[Epoch: 9800] train loss: 0.9961, train acc: 0.5523, val loss: 0.9896, val acc: 0.5602  (best train acc: 0.5592, best val acc: 0.5818)\n",
      "[Epoch: 9820] train loss: 0.9855, train acc: 0.5487, val loss: 0.9693, val acc: 0.5690  (best train acc: 0.5592, best val acc: 0.5818)\n",
      "[Epoch: 9840] train loss: 0.9821, train acc: 0.5495, val loss: 0.9516, val acc: 0.5622  (best train acc: 0.5592, best val acc: 0.5818)\n",
      "[Epoch: 9860] train loss: 0.9820, train acc: 0.5489, val loss: 0.9517, val acc: 0.5642  (best train acc: 0.5592, best val acc: 0.5818)\n",
      "[Epoch: 9880] train loss: 0.9815, train acc: 0.5500, val loss: 0.9635, val acc: 0.5734  (best train acc: 0.5592, best val acc: 0.5818)\n",
      "[Epoch: 9900] train loss: 0.9857, train acc: 0.5508, val loss: 0.9558, val acc: 0.5676  (best train acc: 0.5592, best val acc: 0.5818)\n",
      "[Epoch: 9920] train loss: 0.9922, train acc: 0.5494, val loss: 0.9523, val acc: 0.5642  (best train acc: 0.5592, best val acc: 0.5818)\n",
      "[Epoch: 9940] train loss: 0.9879, train acc: 0.5532, val loss: 0.9577, val acc: 0.5555  (best train acc: 0.5592, best val acc: 0.5818)\n",
      "[Epoch: 9960] train loss: 0.9858, train acc: 0.5556, val loss: 0.9598, val acc: 0.5575  (best train acc: 0.5592, best val acc: 0.5818)\n",
      "[Epoch: 9980] train loss: 0.9829, train acc: 0.5499, val loss: 0.9528, val acc: 0.5589  (best train acc: 0.5592, best val acc: 0.5818)\n",
      "[Epoch: 10000] train loss: 0.9858, train acc: 0.5504, val loss: 0.9525, val acc: 0.5572  (best train acc: 0.5592, best val acc: 0.5818)\n",
      "[Epoch: 10020] train loss: 0.9781, train acc: 0.5527, val loss: 0.9568, val acc: 0.5693  (best train acc: 0.5592, best val acc: 0.5818)\n",
      "[Epoch: 10040] train loss: 0.9850, train acc: 0.5513, val loss: 0.9510, val acc: 0.5568  (best train acc: 0.5592, best val acc: 0.5818)\n",
      "[Epoch: 10060] train loss: 0.9802, train acc: 0.5516, val loss: 0.9510, val acc: 0.5572  (best train acc: 0.5592, best val acc: 0.5818)\n",
      "[Epoch: 10080] train loss: 0.9859, train acc: 0.5542, val loss: 0.9561, val acc: 0.5582  (best train acc: 0.5592, best val acc: 0.5818)\n",
      "[Epoch: 10100] train loss: 0.9790, train acc: 0.5511, val loss: 0.9510, val acc: 0.5612  (best train acc: 0.5592, best val acc: 0.5818)\n",
      "[Epoch: 10120] train loss: 0.9815, train acc: 0.5490, val loss: 0.9573, val acc: 0.5710  (best train acc: 0.5592, best val acc: 0.5818)\n",
      "[Epoch: 10140] train loss: 0.9803, train acc: 0.5518, val loss: 0.9696, val acc: 0.5575  (best train acc: 0.5592, best val acc: 0.5818)\n",
      "[Epoch: 10160] train loss: 0.9986, train acc: 0.5485, val loss: 0.9556, val acc: 0.5609  (best train acc: 0.5592, best val acc: 0.5818)\n",
      "[Epoch: 10180] train loss: 1.0000, train acc: 0.5486, val loss: 0.9563, val acc: 0.5612  (best train acc: 0.5592, best val acc: 0.5818)\n",
      "[Epoch: 10200] train loss: 0.9863, train acc: 0.5533, val loss: 0.9498, val acc: 0.5612  (best train acc: 0.5592, best val acc: 0.5818)\n",
      "[Epoch: 10220] train loss: 0.9779, train acc: 0.5479, val loss: 0.9550, val acc: 0.5575  (best train acc: 0.5592, best val acc: 0.5818)\n",
      "[Epoch: 10240] train loss: 0.9827, train acc: 0.5497, val loss: 0.9508, val acc: 0.5612  (best train acc: 0.5592, best val acc: 0.5818)\n",
      "[Epoch: 10260] train loss: 0.9800, train acc: 0.5500, val loss: 0.9506, val acc: 0.5575  (best train acc: 0.5592, best val acc: 0.5818)\n",
      "[Epoch: 10280] train loss: 0.9803, train acc: 0.5534, val loss: 0.9522, val acc: 0.5551  (best train acc: 0.5592, best val acc: 0.5818)\n",
      "[Epoch: 10300] train loss: 0.9871, train acc: 0.5554, val loss: 0.9537, val acc: 0.5582  (best train acc: 0.5592, best val acc: 0.5818)\n",
      "[Epoch: 10320] train loss: 0.9766, train acc: 0.5492, val loss: 0.9678, val acc: 0.5720  (best train acc: 0.5592, best val acc: 0.5818)\n",
      "[Epoch: 10340] train loss: 0.9782, train acc: 0.5563, val loss: 0.9523, val acc: 0.5575  (best train acc: 0.5592, best val acc: 0.5818)\n",
      "[Epoch: 10360] train loss: 0.9757, train acc: 0.5555, val loss: 0.9495, val acc: 0.5656  (best train acc: 0.5592, best val acc: 0.5818)\n",
      "[Epoch: 10380] train loss: 1.0382, train acc: 0.5082, val loss: 0.9856, val acc: 0.5346  (best train acc: 0.5592, best val acc: 0.5818)\n",
      "[Epoch: 10400] train loss: 0.9960, train acc: 0.5488, val loss: 0.9755, val acc: 0.5521  (best train acc: 0.5592, best val acc: 0.5818)\n",
      "[Epoch: 10420] train loss: 0.9985, train acc: 0.5513, val loss: 0.9561, val acc: 0.5585  (best train acc: 0.5592, best val acc: 0.5818)\n",
      "[Epoch: 10440] train loss: 1.0177, train acc: 0.5495, val loss: 0.9752, val acc: 0.5599  (best train acc: 0.5592, best val acc: 0.5818)\n",
      "[Epoch: 10460] train loss: 1.0177, train acc: 0.5452, val loss: 0.9728, val acc: 0.5427  (best train acc: 0.5592, best val acc: 0.5831)\n",
      "[Epoch: 10480] train loss: 1.0221, train acc: 0.5474, val loss: 0.9625, val acc: 0.5501  (best train acc: 0.5592, best val acc: 0.5831)\n",
      "[Epoch: 10500] train loss: 1.0082, train acc: 0.5513, val loss: 0.9660, val acc: 0.5518  (best train acc: 0.5592, best val acc: 0.5831)\n",
      "[Epoch: 10520] train loss: 1.0002, train acc: 0.5552, val loss: 0.9657, val acc: 0.5713  (best train acc: 0.5592, best val acc: 0.5831)\n",
      "[Epoch: 10540] train loss: 0.9877, train acc: 0.5427, val loss: 0.9515, val acc: 0.5592  (best train acc: 0.5592, best val acc: 0.5831)\n",
      "[Epoch: 10560] train loss: 0.9795, train acc: 0.5497, val loss: 0.9469, val acc: 0.5578  (best train acc: 0.5592, best val acc: 0.5831)\n",
      "[Epoch: 10580] train loss: 0.9790, train acc: 0.5508, val loss: 0.9486, val acc: 0.5562  (best train acc: 0.5592, best val acc: 0.5831)\n",
      "[Epoch: 10600] train loss: 0.9798, train acc: 0.5515, val loss: 0.9475, val acc: 0.5555  (best train acc: 0.5592, best val acc: 0.5831)\n",
      "[Epoch: 10620] train loss: 0.9784, train acc: 0.5508, val loss: 0.9459, val acc: 0.5653  (best train acc: 0.5592, best val acc: 0.5831)\n",
      "[Epoch: 10640] train loss: 0.9761, train acc: 0.5484, val loss: 0.9481, val acc: 0.5612  (best train acc: 0.5592, best val acc: 0.5831)\n",
      "[Epoch: 10660] train loss: 0.9836, train acc: 0.5490, val loss: 0.9497, val acc: 0.5582  (best train acc: 0.5592, best val acc: 0.5831)\n",
      "[Epoch: 10680] train loss: 0.9818, train acc: 0.5431, val loss: 0.9456, val acc: 0.5626  (best train acc: 0.5592, best val acc: 0.5831)\n",
      "[Epoch: 10700] train loss: 0.9794, train acc: 0.5479, val loss: 0.9462, val acc: 0.5632  (best train acc: 0.5592, best val acc: 0.5831)\n",
      "[Epoch: 10720] train loss: 0.9801, train acc: 0.5463, val loss: 0.9468, val acc: 0.5616  (best train acc: 0.5592, best val acc: 0.5831)\n",
      "[Epoch: 10740] train loss: 0.9833, train acc: 0.5531, val loss: 0.9461, val acc: 0.5653  (best train acc: 0.5592, best val acc: 0.5831)\n",
      "[Epoch: 10760] train loss: 0.9759, train acc: 0.5486, val loss: 0.9470, val acc: 0.5609  (best train acc: 0.5592, best val acc: 0.5831)\n",
      "[Epoch: 10780] train loss: 0.9751, train acc: 0.5521, val loss: 0.9461, val acc: 0.5605  (best train acc: 0.5592, best val acc: 0.5831)\n",
      "[Epoch: 10800] train loss: 0.9799, train acc: 0.5496, val loss: 0.9447, val acc: 0.5653  (best train acc: 0.5592, best val acc: 0.5831)\n",
      "[Epoch: 10820] train loss: 0.9841, train acc: 0.5476, val loss: 0.9447, val acc: 0.5656  (best train acc: 0.5592, best val acc: 0.5831)\n",
      "[Epoch: 10840] train loss: 0.9805, train acc: 0.5492, val loss: 0.9455, val acc: 0.5639  (best train acc: 0.5592, best val acc: 0.5831)\n",
      "[Epoch: 10860] train loss: 0.9757, train acc: 0.5513, val loss: 0.9461, val acc: 0.5693  (best train acc: 0.5592, best val acc: 0.5831)\n",
      "[Epoch: 10880] train loss: 0.9750, train acc: 0.5496, val loss: 0.9450, val acc: 0.5680  (best train acc: 0.5592, best val acc: 0.5831)\n",
      "[Epoch: 10900] train loss: 0.9792, train acc: 0.5486, val loss: 0.9446, val acc: 0.5632  (best train acc: 0.5592, best val acc: 0.5831)\n",
      "[Epoch: 10920] train loss: 0.9786, train acc: 0.5510, val loss: 0.9453, val acc: 0.5632  (best train acc: 0.5592, best val acc: 0.5831)\n",
      "[Epoch: 10940] train loss: 0.9769, train acc: 0.5464, val loss: 0.9458, val acc: 0.5636  (best train acc: 0.5592, best val acc: 0.5831)\n",
      "[Epoch: 10960] train loss: 0.9770, train acc: 0.5519, val loss: 0.9471, val acc: 0.5585  (best train acc: 0.5592, best val acc: 0.5831)\n",
      "[Epoch: 10980] train loss: 0.9732, train acc: 0.5488, val loss: 0.9478, val acc: 0.5619  (best train acc: 0.5592, best val acc: 0.5831)\n",
      "[Epoch: 11000] train loss: 0.9775, train acc: 0.5518, val loss: 0.9441, val acc: 0.5659  (best train acc: 0.5592, best val acc: 0.5831)\n",
      "[Epoch: 11020] train loss: 0.9762, train acc: 0.5549, val loss: 0.9441, val acc: 0.5683  (best train acc: 0.5592, best val acc: 0.5831)\n",
      "[Epoch: 11040] train loss: 0.9784, train acc: 0.5494, val loss: 0.9447, val acc: 0.5686  (best train acc: 0.5592, best val acc: 0.5831)\n",
      "[Epoch: 11060] train loss: 0.9786, train acc: 0.5541, val loss: 0.9456, val acc: 0.5656  (best train acc: 0.5592, best val acc: 0.5831)\n",
      "[Epoch: 11080] train loss: 0.9740, train acc: 0.5512, val loss: 0.9464, val acc: 0.5592  (best train acc: 0.5592, best val acc: 0.5831)\n",
      "[Epoch: 11100] train loss: 0.9794, train acc: 0.5505, val loss: 0.9453, val acc: 0.5639  (best train acc: 0.5592, best val acc: 0.5831)\n",
      "[Epoch: 11120] train loss: 0.9766, train acc: 0.5496, val loss: 0.9445, val acc: 0.5669  (best train acc: 0.5592, best val acc: 0.5831)\n",
      "[Epoch: 11140] train loss: 0.9756, train acc: 0.5513, val loss: 0.9473, val acc: 0.5632  (best train acc: 0.5592, best val acc: 0.5831)\n",
      "[Epoch: 11160] train loss: 0.9784, train acc: 0.5534, val loss: 0.9446, val acc: 0.5666  (best train acc: 0.5592, best val acc: 0.5831)\n",
      "[Epoch: 11180] train loss: 0.9815, train acc: 0.5508, val loss: 0.9451, val acc: 0.5646  (best train acc: 0.5592, best val acc: 0.5831)\n",
      "[Epoch: 11200] train loss: 0.9776, train acc: 0.5522, val loss: 0.9455, val acc: 0.5565  (best train acc: 0.5592, best val acc: 0.5831)\n",
      "[Epoch: 11220] train loss: 0.9781, train acc: 0.5543, val loss: 0.9449, val acc: 0.5649  (best train acc: 0.5592, best val acc: 0.5831)\n",
      "[Epoch: 11240] train loss: 0.9752, train acc: 0.5537, val loss: 0.9454, val acc: 0.5592  (best train acc: 0.5592, best val acc: 0.5831)\n",
      "[Epoch: 11260] train loss: 0.9769, train acc: 0.5526, val loss: 0.9453, val acc: 0.5686  (best train acc: 0.5592, best val acc: 0.5831)\n",
      "[Epoch: 11280] train loss: 0.9757, train acc: 0.5538, val loss: 0.9458, val acc: 0.5636  (best train acc: 0.5592, best val acc: 0.5831)\n",
      "[Epoch: 11300] train loss: 0.9769, train acc: 0.5524, val loss: 0.9436, val acc: 0.5592  (best train acc: 0.5592, best val acc: 0.5831)\n",
      "[Epoch: 11320] train loss: 0.9778, train acc: 0.5507, val loss: 0.9458, val acc: 0.5646  (best train acc: 0.5592, best val acc: 0.5831)\n",
      "[Epoch: 11340] train loss: 0.9777, train acc: 0.5516, val loss: 0.9452, val acc: 0.5690  (best train acc: 0.5592, best val acc: 0.5831)\n",
      "[Epoch: 11360] train loss: 0.9747, train acc: 0.5526, val loss: 0.9454, val acc: 0.5663  (best train acc: 0.5592, best val acc: 0.5831)\n",
      "[Epoch: 11380] train loss: 0.9761, train acc: 0.5508, val loss: 0.9484, val acc: 0.5646  (best train acc: 0.5592, best val acc: 0.5831)\n",
      "[Epoch: 11400] train loss: 0.9783, train acc: 0.5535, val loss: 0.9460, val acc: 0.5582  (best train acc: 0.5592, best val acc: 0.5831)\n",
      "[Epoch: 11420] train loss: 0.9747, train acc: 0.5528, val loss: 0.9448, val acc: 0.5680  (best train acc: 0.5592, best val acc: 0.5831)\n",
      "[Epoch: 11440] train loss: 0.9763, train acc: 0.5514, val loss: 0.9466, val acc: 0.5609  (best train acc: 0.5592, best val acc: 0.5831)\n",
      "[Epoch: 11460] train loss: 0.9800, train acc: 0.5523, val loss: 0.9433, val acc: 0.5659  (best train acc: 0.5592, best val acc: 0.5831)\n",
      "[Epoch: 11480] train loss: 0.9750, train acc: 0.5526, val loss: 0.9448, val acc: 0.5646  (best train acc: 0.5592, best val acc: 0.5831)\n",
      "[Epoch: 11500] train loss: 0.9744, train acc: 0.5567, val loss: 0.9437, val acc: 0.5642  (best train acc: 0.5592, best val acc: 0.5831)\n",
      "[Epoch: 11520] train loss: 0.9760, train acc: 0.5534, val loss: 0.9444, val acc: 0.5669  (best train acc: 0.5592, best val acc: 0.5831)\n",
      "[Epoch: 11540] train loss: 0.9816, train acc: 0.5471, val loss: 0.9485, val acc: 0.5599  (best train acc: 0.5592, best val acc: 0.5831)\n",
      "[Epoch: 11560] train loss: 0.9771, train acc: 0.5506, val loss: 0.9445, val acc: 0.5669  (best train acc: 0.5592, best val acc: 0.5831)\n",
      "[Epoch: 11580] train loss: 0.9775, train acc: 0.5537, val loss: 0.9443, val acc: 0.5605  (best train acc: 0.5592, best val acc: 0.5831)\n",
      "[Epoch: 11600] train loss: 0.9802, train acc: 0.5497, val loss: 0.9460, val acc: 0.5609  (best train acc: 0.5592, best val acc: 0.5831)\n",
      "[Epoch: 11620] train loss: 0.9791, train acc: 0.5557, val loss: 0.9436, val acc: 0.5619  (best train acc: 0.5592, best val acc: 0.5831)\n",
      "[Epoch: 11640] train loss: 0.9822, train acc: 0.5542, val loss: 0.9465, val acc: 0.5723  (best train acc: 0.5592, best val acc: 0.5831)\n",
      "[Epoch: 11660] train loss: 0.9787, train acc: 0.5528, val loss: 0.9489, val acc: 0.5636  (best train acc: 0.5592, best val acc: 0.5831)\n",
      "[Epoch: 11680] train loss: 0.9790, train acc: 0.5458, val loss: 0.9486, val acc: 0.5622  (best train acc: 0.5592, best val acc: 0.5831)\n",
      "[Epoch: 11700] train loss: 0.9808, train acc: 0.5534, val loss: 0.9458, val acc: 0.5636  (best train acc: 0.5592, best val acc: 0.5831)\n",
      "[Epoch: 11720] train loss: 0.9771, train acc: 0.5515, val loss: 0.9467, val acc: 0.5589  (best train acc: 0.5592, best val acc: 0.5831)\n",
      "[Epoch: 11740] train loss: 0.9791, train acc: 0.5521, val loss: 0.9444, val acc: 0.5646  (best train acc: 0.5592, best val acc: 0.5831)\n",
      "[Epoch: 11760] train loss: 0.9750, train acc: 0.5554, val loss: 0.9434, val acc: 0.5696  (best train acc: 0.5592, best val acc: 0.5831)\n",
      "[Epoch: 11780] train loss: 0.9729, train acc: 0.5547, val loss: 0.9455, val acc: 0.5612  (best train acc: 0.5592, best val acc: 0.5831)\n",
      "[Epoch: 11800] train loss: 0.9793, train acc: 0.5580, val loss: 0.9443, val acc: 0.5690  (best train acc: 0.5592, best val acc: 0.5831)\n",
      "[Epoch: 11820] train loss: 0.9770, train acc: 0.5534, val loss: 0.9454, val acc: 0.5622  (best train acc: 0.5592, best val acc: 0.5831)\n",
      "[Epoch: 11840] train loss: 0.9791, train acc: 0.5587, val loss: 0.9441, val acc: 0.5710  (best train acc: 0.5592, best val acc: 0.5831)\n",
      "[Epoch: 11860] train loss: 0.9748, train acc: 0.5516, val loss: 0.9445, val acc: 0.5680  (best train acc: 0.5592, best val acc: 0.5831)\n",
      "[Epoch: 11880] train loss: 0.9756, train acc: 0.5509, val loss: 0.9452, val acc: 0.5636  (best train acc: 0.5592, best val acc: 0.5831)\n",
      "[Epoch: 11900] train loss: 0.9773, train acc: 0.5482, val loss: 0.9446, val acc: 0.5683  (best train acc: 0.5594, best val acc: 0.5831)\n",
      "[Epoch: 11920] train loss: 0.9773, train acc: 0.5523, val loss: 0.9443, val acc: 0.5696  (best train acc: 0.5594, best val acc: 0.5831)\n",
      "[Epoch: 11940] train loss: 0.9766, train acc: 0.5549, val loss: 0.9448, val acc: 0.5673  (best train acc: 0.5594, best val acc: 0.5831)\n",
      "[Epoch: 11960] train loss: 0.9766, train acc: 0.5502, val loss: 0.9464, val acc: 0.5609  (best train acc: 0.5594, best val acc: 0.5831)\n",
      "[Epoch: 11980] train loss: 0.9830, train acc: 0.5600, val loss: 0.9467, val acc: 0.5626  (best train acc: 0.5600, best val acc: 0.5831)\n",
      "[Epoch: 12000] train loss: 0.9760, train acc: 0.5599, val loss: 0.9487, val acc: 0.5558  (best train acc: 0.5602, best val acc: 0.5831)\n",
      "[Epoch: 12020] train loss: 0.9727, train acc: 0.5588, val loss: 0.9440, val acc: 0.5649  (best train acc: 0.5611, best val acc: 0.5831)\n",
      "[Epoch: 12040] train loss: 0.9744, train acc: 0.5536, val loss: 0.9444, val acc: 0.5696  (best train acc: 0.5611, best val acc: 0.5831)\n",
      "[Epoch: 12060] train loss: 0.9760, train acc: 0.5538, val loss: 0.9453, val acc: 0.5642  (best train acc: 0.5611, best val acc: 0.5831)\n",
      "[Epoch: 12080] train loss: 0.9732, train acc: 0.5566, val loss: 0.9445, val acc: 0.5656  (best train acc: 0.5611, best val acc: 0.5831)\n",
      "[Epoch: 12100] train loss: 0.9775, train acc: 0.5524, val loss: 0.9460, val acc: 0.5609  (best train acc: 0.5611, best val acc: 0.5831)\n",
      "[Epoch: 12120] train loss: 0.9737, train acc: 0.5545, val loss: 0.9439, val acc: 0.5676  (best train acc: 0.5611, best val acc: 0.5831)\n",
      "[Epoch: 12140] train loss: 0.9770, train acc: 0.5554, val loss: 0.9451, val acc: 0.5609  (best train acc: 0.5611, best val acc: 0.5831)\n",
      "[Epoch: 12160] train loss: 0.9778, train acc: 0.5503, val loss: 0.9480, val acc: 0.5626  (best train acc: 0.5611, best val acc: 0.5831)\n",
      "[Epoch: 12180] train loss: 0.9782, train acc: 0.5547, val loss: 0.9448, val acc: 0.5605  (best train acc: 0.5614, best val acc: 0.5831)\n",
      "[Epoch: 12200] train loss: 0.9786, train acc: 0.5554, val loss: 0.9448, val acc: 0.5669  (best train acc: 0.5614, best val acc: 0.5831)\n",
      "[Epoch: 12220] train loss: 0.9750, train acc: 0.5559, val loss: 0.9441, val acc: 0.5700  (best train acc: 0.5614, best val acc: 0.5831)\n",
      "[Epoch: 12240] train loss: 0.9786, train acc: 0.5562, val loss: 0.9448, val acc: 0.5649  (best train acc: 0.5614, best val acc: 0.5831)\n",
      "[Epoch: 12260] train loss: 0.9797, train acc: 0.5589, val loss: 0.9485, val acc: 0.5632  (best train acc: 0.5614, best val acc: 0.5831)\n",
      "[Epoch: 12280] train loss: 0.9799, train acc: 0.5533, val loss: 0.9459, val acc: 0.5666  (best train acc: 0.5614, best val acc: 0.5831)\n",
      "[Epoch: 12300] train loss: 0.9785, train acc: 0.5580, val loss: 0.9468, val acc: 0.5659  (best train acc: 0.5614, best val acc: 0.5831)\n",
      "[Epoch: 12320] train loss: 0.9780, train acc: 0.5521, val loss: 0.9451, val acc: 0.5669  (best train acc: 0.5614, best val acc: 0.5831)\n",
      "[Epoch: 12340] train loss: 0.9802, train acc: 0.5522, val loss: 0.9456, val acc: 0.5626  (best train acc: 0.5614, best val acc: 0.5831)\n",
      "[Epoch: 12360] train loss: 0.9715, train acc: 0.5573, val loss: 0.9448, val acc: 0.5663  (best train acc: 0.5614, best val acc: 0.5831)\n",
      "[Epoch: 12380] train loss: 0.9773, train acc: 0.5577, val loss: 0.9449, val acc: 0.5595  (best train acc: 0.5614, best val acc: 0.5831)\n",
      "[Epoch: 12400] train loss: 0.9765, train acc: 0.5552, val loss: 0.9451, val acc: 0.5619  (best train acc: 0.5614, best val acc: 0.5831)\n",
      "[Epoch: 12420] train loss: 0.9769, train acc: 0.5578, val loss: 0.9449, val acc: 0.5632  (best train acc: 0.5615, best val acc: 0.5831)\n",
      "[Epoch: 12440] train loss: 0.9772, train acc: 0.5548, val loss: 0.9450, val acc: 0.5578  (best train acc: 0.5615, best val acc: 0.5831)\n",
      "[Epoch: 12460] train loss: 0.9787, train acc: 0.5569, val loss: 0.9458, val acc: 0.5626  (best train acc: 0.5615, best val acc: 0.5831)\n",
      "[Epoch: 12480] train loss: 0.9770, train acc: 0.5573, val loss: 0.9445, val acc: 0.5558  (best train acc: 0.5615, best val acc: 0.5831)\n",
      "[Epoch: 12500] train loss: 0.9787, train acc: 0.5535, val loss: 0.9464, val acc: 0.5619  (best train acc: 0.5615, best val acc: 0.5831)\n",
      "[Epoch: 12520] train loss: 0.9755, train acc: 0.5551, val loss: 0.9464, val acc: 0.5646  (best train acc: 0.5615, best val acc: 0.5831)\n",
      "[Epoch: 12540] train loss: 0.9893, train acc: 0.5533, val loss: 0.9492, val acc: 0.5605  (best train acc: 0.5615, best val acc: 0.5831)\n",
      "[Epoch: 12560] train loss: 0.9818, train acc: 0.5481, val loss: 0.9589, val acc: 0.5619  (best train acc: 0.5615, best val acc: 0.5831)\n",
      "[Epoch: 12580] train loss: 0.9792, train acc: 0.5575, val loss: 0.9500, val acc: 0.5642  (best train acc: 0.5615, best val acc: 0.5831)\n",
      "[Epoch: 12600] train loss: 0.9776, train acc: 0.5513, val loss: 0.9513, val acc: 0.5609  (best train acc: 0.5615, best val acc: 0.5831)\n",
      "[Epoch: 12620] train loss: 0.9778, train acc: 0.5545, val loss: 0.9476, val acc: 0.5632  (best train acc: 0.5615, best val acc: 0.5831)\n",
      "[Epoch: 12640] train loss: 0.9917, train acc: 0.5342, val loss: 0.9513, val acc: 0.5646  (best train acc: 0.5616, best val acc: 0.5831)\n",
      "[Epoch: 12660] train loss: 0.9844, train acc: 0.5467, val loss: 0.9513, val acc: 0.5568  (best train acc: 0.5616, best val acc: 0.5831)\n",
      "[Epoch: 12680] train loss: 0.9805, train acc: 0.5502, val loss: 0.9461, val acc: 0.5659  (best train acc: 0.5616, best val acc: 0.5831)\n",
      "[Epoch: 12700] train loss: 0.9767, train acc: 0.5609, val loss: 0.9491, val acc: 0.5669  (best train acc: 0.5638, best val acc: 0.5831)\n",
      "[Epoch: 12720] train loss: 0.9782, train acc: 0.5549, val loss: 0.9488, val acc: 0.5666  (best train acc: 0.5638, best val acc: 0.5831)\n",
      "[Epoch: 12740] train loss: 0.9831, train acc: 0.5462, val loss: 0.9485, val acc: 0.5659  (best train acc: 0.5638, best val acc: 0.5831)\n",
      "[Epoch: 12760] train loss: 0.9817, train acc: 0.5619, val loss: 0.9469, val acc: 0.5690  (best train acc: 0.5638, best val acc: 0.5831)\n",
      "[Epoch: 12780] train loss: 0.9775, train acc: 0.5580, val loss: 0.9470, val acc: 0.5700  (best train acc: 0.5638, best val acc: 0.5831)\n",
      "[Epoch: 12800] train loss: 0.9748, train acc: 0.5546, val loss: 0.9477, val acc: 0.5683  (best train acc: 0.5638, best val acc: 0.5831)\n",
      "[Epoch: 12820] train loss: 0.9769, train acc: 0.5534, val loss: 0.9459, val acc: 0.5663  (best train acc: 0.5638, best val acc: 0.5831)\n",
      "[Epoch: 12840] train loss: 0.9767, train acc: 0.5540, val loss: 0.9462, val acc: 0.5666  (best train acc: 0.5638, best val acc: 0.5831)\n",
      "[Epoch: 12860] train loss: 0.9757, train acc: 0.5568, val loss: 0.9468, val acc: 0.5673  (best train acc: 0.5638, best val acc: 0.5831)\n",
      "[Epoch: 12880] train loss: 0.9776, train acc: 0.5552, val loss: 0.9463, val acc: 0.5663  (best train acc: 0.5638, best val acc: 0.5831)\n",
      "[Epoch: 12900] train loss: 0.9754, train acc: 0.5558, val loss: 0.9469, val acc: 0.5683  (best train acc: 0.5638, best val acc: 0.5831)\n",
      "[Epoch: 12920] train loss: 0.9765, train acc: 0.5583, val loss: 0.9490, val acc: 0.5632  (best train acc: 0.5638, best val acc: 0.5831)\n",
      "[Epoch: 12940] train loss: 0.9815, train acc: 0.5513, val loss: 0.9464, val acc: 0.5629  (best train acc: 0.5638, best val acc: 0.5831)\n",
      "[Epoch: 12960] train loss: 0.9770, train acc: 0.5493, val loss: 0.9490, val acc: 0.5703  (best train acc: 0.5638, best val acc: 0.5831)\n",
      "[Epoch: 12980] train loss: 0.9781, train acc: 0.5500, val loss: 0.9435, val acc: 0.5642  (best train acc: 0.5638, best val acc: 0.5831)\n",
      "[Epoch: 13000] train loss: 0.9762, train acc: 0.5539, val loss: 0.9386, val acc: 0.5683  (best train acc: 0.5638, best val acc: 0.5831)\n",
      "[Epoch: 13020] train loss: 0.9699, train acc: 0.5575, val loss: 0.9395, val acc: 0.5690  (best train acc: 0.5638, best val acc: 0.5831)\n",
      "[Epoch: 13040] train loss: 0.9768, train acc: 0.5583, val loss: 0.9396, val acc: 0.5767  (best train acc: 0.5638, best val acc: 0.5831)\n",
      "[Epoch: 13060] train loss: 0.9718, train acc: 0.5569, val loss: 0.9442, val acc: 0.5676  (best train acc: 0.5638, best val acc: 0.5831)\n",
      "[Epoch: 13080] train loss: 0.9720, train acc: 0.5567, val loss: 0.9392, val acc: 0.5713  (best train acc: 0.5638, best val acc: 0.5831)\n",
      "[Epoch: 13100] train loss: 0.9744, train acc: 0.5495, val loss: 0.9435, val acc: 0.5639  (best train acc: 0.5638, best val acc: 0.5831)\n",
      "[Epoch: 13120] train loss: 0.9774, train acc: 0.5540, val loss: 0.9396, val acc: 0.5686  (best train acc: 0.5638, best val acc: 0.5831)\n",
      "[Epoch: 13140] train loss: 0.9658, train acc: 0.5599, val loss: 0.9403, val acc: 0.5690  (best train acc: 0.5638, best val acc: 0.5831)\n",
      "[Epoch: 13160] train loss: 0.9747, train acc: 0.5561, val loss: 0.9407, val acc: 0.5646  (best train acc: 0.5638, best val acc: 0.5831)\n",
      "[Epoch: 13180] train loss: 0.9733, train acc: 0.5548, val loss: 0.9394, val acc: 0.5659  (best train acc: 0.5638, best val acc: 0.5831)\n",
      "[Epoch: 13200] train loss: 0.9743, train acc: 0.5557, val loss: 0.9394, val acc: 0.5656  (best train acc: 0.5638, best val acc: 0.5831)\n",
      "[Epoch: 13220] train loss: 0.9753, train acc: 0.5502, val loss: 0.9417, val acc: 0.5663  (best train acc: 0.5638, best val acc: 0.5831)\n",
      "[Epoch: 13240] train loss: 0.9722, train acc: 0.5525, val loss: 0.9401, val acc: 0.5666  (best train acc: 0.5638, best val acc: 0.5831)\n",
      "[Epoch: 13260] train loss: 0.9721, train acc: 0.5538, val loss: 0.9397, val acc: 0.5707  (best train acc: 0.5638, best val acc: 0.5831)\n",
      "[Epoch: 13280] train loss: 0.9764, train acc: 0.5549, val loss: 0.9394, val acc: 0.5663  (best train acc: 0.5638, best val acc: 0.5831)\n",
      "[Epoch: 13300] train loss: 0.9749, train acc: 0.5569, val loss: 0.9383, val acc: 0.5676  (best train acc: 0.5638, best val acc: 0.5831)\n",
      "[Epoch: 13320] train loss: 0.9777, train acc: 0.5543, val loss: 0.9393, val acc: 0.5690  (best train acc: 0.5638, best val acc: 0.5831)\n",
      "[Epoch: 13340] train loss: 0.9743, train acc: 0.5561, val loss: 0.9396, val acc: 0.5649  (best train acc: 0.5638, best val acc: 0.5831)\n",
      "[Epoch: 13360] train loss: 0.9750, train acc: 0.5508, val loss: 0.9410, val acc: 0.5656  (best train acc: 0.5638, best val acc: 0.5831)\n",
      "[Epoch: 13380] train loss: 0.9778, train acc: 0.5581, val loss: 0.9406, val acc: 0.5683  (best train acc: 0.5638, best val acc: 0.5831)\n",
      "[Epoch: 13400] train loss: 0.9745, train acc: 0.5540, val loss: 0.9387, val acc: 0.5680  (best train acc: 0.5638, best val acc: 0.5831)\n",
      "[Epoch: 13420] train loss: 0.9690, train acc: 0.5532, val loss: 0.9403, val acc: 0.5669  (best train acc: 0.5638, best val acc: 0.5831)\n",
      "[Epoch: 13440] train loss: 0.9684, train acc: 0.5575, val loss: 0.9392, val acc: 0.5713  (best train acc: 0.5638, best val acc: 0.5831)\n",
      "[Epoch: 13460] train loss: 0.9712, train acc: 0.5515, val loss: 0.9397, val acc: 0.5696  (best train acc: 0.5638, best val acc: 0.5831)\n",
      "[Epoch: 13480] train loss: 0.9749, train acc: 0.5570, val loss: 0.9514, val acc: 0.5548  (best train acc: 0.5638, best val acc: 0.5831)\n",
      "[Epoch: 13500] train loss: 0.9737, train acc: 0.5568, val loss: 0.9409, val acc: 0.5616  (best train acc: 0.5638, best val acc: 0.5831)\n",
      "[Epoch: 13520] train loss: 0.9750, train acc: 0.5524, val loss: 0.9444, val acc: 0.5649  (best train acc: 0.5638, best val acc: 0.5831)\n",
      "[Epoch: 13540] train loss: 0.9768, train acc: 0.5587, val loss: 0.9385, val acc: 0.5720  (best train acc: 0.5638, best val acc: 0.5831)\n",
      "[Epoch: 13560] train loss: 0.9762, train acc: 0.5509, val loss: 0.9460, val acc: 0.5616  (best train acc: 0.5638, best val acc: 0.5831)\n",
      "[Epoch: 13580] train loss: 0.9768, train acc: 0.5604, val loss: 0.9395, val acc: 0.5703  (best train acc: 0.5638, best val acc: 0.5831)\n",
      "[Epoch: 13600] train loss: 0.9731, train acc: 0.5570, val loss: 0.9392, val acc: 0.5683  (best train acc: 0.5638, best val acc: 0.5831)\n",
      "[Epoch: 13620] train loss: 0.9731, train acc: 0.5549, val loss: 0.9392, val acc: 0.5737  (best train acc: 0.5638, best val acc: 0.5831)\n",
      "[Epoch: 13640] train loss: 0.9731, train acc: 0.5565, val loss: 0.9383, val acc: 0.5720  (best train acc: 0.5638, best val acc: 0.5831)\n",
      "[Epoch: 13660] train loss: 0.9725, train acc: 0.5580, val loss: 0.9391, val acc: 0.5680  (best train acc: 0.5638, best val acc: 0.5831)\n",
      "[Epoch: 13680] train loss: 0.9723, train acc: 0.5528, val loss: 0.9405, val acc: 0.5693  (best train acc: 0.5638, best val acc: 0.5831)\n",
      "[Epoch: 13700] train loss: 0.9783, train acc: 0.5633, val loss: 0.9396, val acc: 0.5703  (best train acc: 0.5638, best val acc: 0.5831)\n",
      "[Epoch: 13720] train loss: 0.9745, train acc: 0.5574, val loss: 0.9414, val acc: 0.5673  (best train acc: 0.5638, best val acc: 0.5831)\n",
      "[Epoch: 13740] train loss: 0.9694, train acc: 0.5576, val loss: 0.9418, val acc: 0.5663  (best train acc: 0.5638, best val acc: 0.5831)\n",
      "[Epoch: 13760] train loss: 0.9751, train acc: 0.5549, val loss: 0.9386, val acc: 0.5686  (best train acc: 0.5638, best val acc: 0.5831)\n",
      "[Epoch: 13780] train loss: 0.9711, train acc: 0.5568, val loss: 0.9380, val acc: 0.5632  (best train acc: 0.5638, best val acc: 0.5831)\n",
      "[Epoch: 13800] train loss: 0.9764, train acc: 0.5560, val loss: 0.9408, val acc: 0.5680  (best train acc: 0.5638, best val acc: 0.5831)\n",
      "[Epoch: 13820] train loss: 0.9752, train acc: 0.5472, val loss: 0.9397, val acc: 0.5622  (best train acc: 0.5638, best val acc: 0.5831)\n",
      "[Epoch: 13840] train loss: 0.9729, train acc: 0.5547, val loss: 0.9385, val acc: 0.5710  (best train acc: 0.5638, best val acc: 0.5831)\n",
      "[Epoch: 13860] train loss: 0.9700, train acc: 0.5577, val loss: 0.9386, val acc: 0.5720  (best train acc: 0.5638, best val acc: 0.5831)\n",
      "[Epoch: 13880] train loss: 0.9773, train acc: 0.5568, val loss: 0.9376, val acc: 0.5676  (best train acc: 0.5646, best val acc: 0.5831)\n",
      "[Epoch: 13900] train loss: 0.9706, train acc: 0.5598, val loss: 0.9387, val acc: 0.5619  (best train acc: 0.5646, best val acc: 0.5831)\n",
      "[Epoch: 13920] train loss: 0.9734, train acc: 0.5584, val loss: 0.9373, val acc: 0.5680  (best train acc: 0.5646, best val acc: 0.5831)\n",
      "[Epoch: 13940] train loss: 0.9722, train acc: 0.5556, val loss: 0.9391, val acc: 0.5653  (best train acc: 0.5646, best val acc: 0.5831)\n",
      "[Epoch: 13960] train loss: 0.9712, train acc: 0.5585, val loss: 0.9383, val acc: 0.5673  (best train acc: 0.5646, best val acc: 0.5831)\n",
      "[Epoch: 13980] train loss: 0.9701, train acc: 0.5560, val loss: 0.9426, val acc: 0.5707  (best train acc: 0.5646, best val acc: 0.5831)\n",
      "[Epoch: 14000] train loss: 0.9699, train acc: 0.5565, val loss: 0.9391, val acc: 0.5676  (best train acc: 0.5646, best val acc: 0.5831)\n",
      "[Epoch: 14020] train loss: 0.9710, train acc: 0.5581, val loss: 0.9378, val acc: 0.5730  (best train acc: 0.5646, best val acc: 0.5831)\n",
      "[Epoch: 14040] train loss: 0.9738, train acc: 0.5593, val loss: 0.9377, val acc: 0.5717  (best train acc: 0.5646, best val acc: 0.5831)\n",
      "[Epoch: 14060] train loss: 0.9732, train acc: 0.5562, val loss: 0.9393, val acc: 0.5669  (best train acc: 0.5646, best val acc: 0.5831)\n",
      "[Epoch: 14080] train loss: 0.9707, train acc: 0.5543, val loss: 0.9382, val acc: 0.5669  (best train acc: 0.5646, best val acc: 0.5831)\n",
      "[Epoch: 14100] train loss: 0.9734, train acc: 0.5544, val loss: 0.9444, val acc: 0.5666  (best train acc: 0.5646, best val acc: 0.5831)\n",
      "[Epoch: 14120] train loss: 0.9766, train acc: 0.5534, val loss: 0.9400, val acc: 0.5710  (best train acc: 0.5646, best val acc: 0.5831)\n",
      "[Epoch: 14140] train loss: 0.9784, train acc: 0.5599, val loss: 0.9376, val acc: 0.5707  (best train acc: 0.5646, best val acc: 0.5831)\n",
      "[Epoch: 14160] train loss: 0.9694, train acc: 0.5582, val loss: 0.9381, val acc: 0.5636  (best train acc: 0.5646, best val acc: 0.5831)\n",
      "[Epoch: 14180] train loss: 0.9726, train acc: 0.5556, val loss: 0.9394, val acc: 0.5683  (best train acc: 0.5646, best val acc: 0.5831)\n",
      "[Epoch: 14200] train loss: 0.9745, train acc: 0.5524, val loss: 0.9381, val acc: 0.5693  (best train acc: 0.5646, best val acc: 0.5831)\n",
      "[Epoch: 14220] train loss: 0.9658, train acc: 0.5591, val loss: 0.9381, val acc: 0.5717  (best train acc: 0.5646, best val acc: 0.5831)\n",
      "[Epoch: 14240] train loss: 0.9836, train acc: 0.5497, val loss: 0.9439, val acc: 0.5676  (best train acc: 0.5646, best val acc: 0.5831)\n",
      "[Epoch: 14260] train loss: 0.9797, train acc: 0.5489, val loss: 0.9407, val acc: 0.5666  (best train acc: 0.5646, best val acc: 0.5831)\n",
      "[Epoch: 14280] train loss: 0.9770, train acc: 0.5534, val loss: 0.9392, val acc: 0.5707  (best train acc: 0.5646, best val acc: 0.5831)\n",
      "[Epoch: 14300] train loss: 0.9710, train acc: 0.5541, val loss: 0.9408, val acc: 0.5649  (best train acc: 0.5646, best val acc: 0.5831)\n",
      "[Epoch: 14320] train loss: 0.9761, train acc: 0.5540, val loss: 0.9477, val acc: 0.5504  (best train acc: 0.5655, best val acc: 0.5831)\n",
      "[Epoch: 14340] train loss: 0.9732, train acc: 0.5509, val loss: 0.9453, val acc: 0.5649  (best train acc: 0.5655, best val acc: 0.5831)\n",
      "[Epoch: 14360] train loss: 0.9747, train acc: 0.5531, val loss: 0.9437, val acc: 0.5541  (best train acc: 0.5655, best val acc: 0.5831)\n",
      "[Epoch: 14380] train loss: 0.9772, train acc: 0.5549, val loss: 0.9426, val acc: 0.5673  (best train acc: 0.5655, best val acc: 0.5831)\n",
      "[Epoch: 14400] train loss: 0.9719, train acc: 0.5583, val loss: 0.9398, val acc: 0.5690  (best train acc: 0.5655, best val acc: 0.5831)\n",
      "[Epoch: 14420] train loss: 0.9719, train acc: 0.5632, val loss: 0.9412, val acc: 0.5713  (best train acc: 0.5655, best val acc: 0.5831)\n",
      "[Epoch: 14440] train loss: 0.9695, train acc: 0.5573, val loss: 0.9389, val acc: 0.5713  (best train acc: 0.5655, best val acc: 0.5831)\n",
      "[Epoch: 14460] train loss: 0.9760, train acc: 0.5565, val loss: 0.9410, val acc: 0.5592  (best train acc: 0.5655, best val acc: 0.5831)\n",
      "[Epoch: 14480] train loss: 0.9708, train acc: 0.5581, val loss: 0.9383, val acc: 0.5727  (best train acc: 0.5655, best val acc: 0.5831)\n",
      "[Epoch: 14500] train loss: 0.9745, train acc: 0.5513, val loss: 0.9398, val acc: 0.5666  (best train acc: 0.5655, best val acc: 0.5831)\n",
      "[Epoch: 14520] train loss: 0.9704, train acc: 0.5581, val loss: 0.9393, val acc: 0.5673  (best train acc: 0.5655, best val acc: 0.5831)\n",
      "[Epoch: 14540] train loss: 0.9684, train acc: 0.5587, val loss: 0.9410, val acc: 0.5669  (best train acc: 0.5655, best val acc: 0.5831)\n",
      "[Epoch: 14560] train loss: 0.9753, train acc: 0.5555, val loss: 0.9400, val acc: 0.5659  (best train acc: 0.5655, best val acc: 0.5831)\n",
      "[Epoch: 14580] train loss: 0.9713, train acc: 0.5551, val loss: 0.9409, val acc: 0.5663  (best train acc: 0.5655, best val acc: 0.5831)\n",
      "[Epoch: 14600] train loss: 0.9745, train acc: 0.5576, val loss: 0.9395, val acc: 0.5690  (best train acc: 0.5655, best val acc: 0.5831)\n",
      "[Epoch: 14620] train loss: 0.9708, train acc: 0.5562, val loss: 0.9382, val acc: 0.5713  (best train acc: 0.5655, best val acc: 0.5831)\n",
      "[Epoch: 14640] train loss: 0.9708, train acc: 0.5579, val loss: 0.9384, val acc: 0.5720  (best train acc: 0.5655, best val acc: 0.5831)\n",
      "[Epoch: 14660] train loss: 0.9683, train acc: 0.5550, val loss: 0.9369, val acc: 0.5740  (best train acc: 0.5655, best val acc: 0.5831)\n",
      "[Epoch: 14680] train loss: 0.9751, train acc: 0.5583, val loss: 0.9391, val acc: 0.5730  (best train acc: 0.5655, best val acc: 0.5831)\n",
      "[Epoch: 14700] train loss: 0.9670, train acc: 0.5551, val loss: 0.9401, val acc: 0.5676  (best train acc: 0.5655, best val acc: 0.5831)\n",
      "[Epoch: 14720] train loss: 0.9771, train acc: 0.5562, val loss: 0.9391, val acc: 0.5703  (best train acc: 0.5655, best val acc: 0.5831)\n",
      "[Epoch: 14740] train loss: 0.9715, train acc: 0.5603, val loss: 0.9388, val acc: 0.5696  (best train acc: 0.5655, best val acc: 0.5831)\n",
      "[Epoch: 14760] train loss: 0.9715, train acc: 0.5555, val loss: 0.9401, val acc: 0.5656  (best train acc: 0.5655, best val acc: 0.5831)\n",
      "[Epoch: 14780] train loss: 0.9721, train acc: 0.5536, val loss: 0.9429, val acc: 0.5592  (best train acc: 0.5655, best val acc: 0.5831)\n",
      "[Epoch: 14800] train loss: 0.9680, train acc: 0.5621, val loss: 0.9392, val acc: 0.5646  (best train acc: 0.5655, best val acc: 0.5831)\n",
      "[Epoch: 14820] train loss: 0.9716, train acc: 0.5551, val loss: 0.9378, val acc: 0.5717  (best train acc: 0.5655, best val acc: 0.5831)\n",
      "[Epoch: 14840] train loss: 0.9719, train acc: 0.5571, val loss: 0.9399, val acc: 0.5673  (best train acc: 0.5655, best val acc: 0.5831)\n",
      "[Epoch: 14860] train loss: 0.9739, train acc: 0.5555, val loss: 0.9373, val acc: 0.5686  (best train acc: 0.5655, best val acc: 0.5831)\n",
      "[Epoch: 14880] train loss: 0.9687, train acc: 0.5566, val loss: 0.9377, val acc: 0.5717  (best train acc: 0.5655, best val acc: 0.5831)\n",
      "[Epoch: 14900] train loss: 0.9706, train acc: 0.5608, val loss: 0.9390, val acc: 0.5683  (best train acc: 0.5655, best val acc: 0.5831)\n",
      "[Epoch: 14920] train loss: 0.9671, train acc: 0.5537, val loss: 0.9384, val acc: 0.5713  (best train acc: 0.5655, best val acc: 0.5831)\n",
      "[Epoch: 14940] train loss: 0.9754, train acc: 0.5560, val loss: 0.9415, val acc: 0.5737  (best train acc: 0.5655, best val acc: 0.5831)\n",
      "[Epoch: 14960] train loss: 0.9802, train acc: 0.5551, val loss: 0.9426, val acc: 0.5723  (best train acc: 0.5655, best val acc: 0.5831)\n",
      "[Epoch: 14980] train loss: 0.9887, train acc: 0.5499, val loss: 0.9588, val acc: 0.5164  (best train acc: 0.5655, best val acc: 0.5831)\n",
      "[Epoch: 15000] train loss: 0.9704, train acc: 0.5555, val loss: 0.9413, val acc: 0.5690  (best train acc: 0.5655, best val acc: 0.5831)\n",
      "[Epoch: 15020] train loss: 0.9747, train acc: 0.5484, val loss: 0.9419, val acc: 0.5653  (best train acc: 0.5655, best val acc: 0.5831)\n",
      "[Epoch: 15040] train loss: 0.9725, train acc: 0.5561, val loss: 0.9382, val acc: 0.5703  (best train acc: 0.5655, best val acc: 0.5831)\n",
      "[Epoch: 15060] train loss: 0.9726, train acc: 0.5580, val loss: 0.9384, val acc: 0.5734  (best train acc: 0.5655, best val acc: 0.5831)\n",
      "[Epoch: 15080] train loss: 0.9665, train acc: 0.5603, val loss: 0.9372, val acc: 0.5754  (best train acc: 0.5655, best val acc: 0.5831)\n",
      "[Epoch: 15100] train loss: 0.9687, train acc: 0.5555, val loss: 0.9398, val acc: 0.5680  (best train acc: 0.5655, best val acc: 0.5831)\n",
      "[Epoch: 15120] train loss: 0.9705, train acc: 0.5588, val loss: 0.9407, val acc: 0.5680  (best train acc: 0.5655, best val acc: 0.5831)\n",
      "[Epoch: 15140] train loss: 0.9740, train acc: 0.5490, val loss: 0.9427, val acc: 0.5656  (best train acc: 0.5655, best val acc: 0.5831)\n",
      "[Epoch: 15160] train loss: 0.9707, train acc: 0.5557, val loss: 0.9366, val acc: 0.5750  (best train acc: 0.5655, best val acc: 0.5831)\n",
      "[Epoch: 15180] train loss: 0.9731, train acc: 0.5599, val loss: 0.9376, val acc: 0.5710  (best train acc: 0.5655, best val acc: 0.5831)\n",
      "[Epoch: 15200] train loss: 0.9700, train acc: 0.5568, val loss: 0.9386, val acc: 0.5646  (best train acc: 0.5655, best val acc: 0.5831)\n",
      "[Epoch: 15220] train loss: 0.9717, train acc: 0.5560, val loss: 0.9368, val acc: 0.5710  (best train acc: 0.5655, best val acc: 0.5831)\n",
      "[Epoch: 15240] train loss: 0.9703, train acc: 0.5534, val loss: 0.9385, val acc: 0.5669  (best train acc: 0.5655, best val acc: 0.5831)\n",
      "[Epoch: 15260] train loss: 0.9670, train acc: 0.5597, val loss: 0.9380, val acc: 0.5703  (best train acc: 0.5655, best val acc: 0.5831)\n",
      "[Epoch: 15280] train loss: 0.9744, train acc: 0.5605, val loss: 0.9377, val acc: 0.5683  (best train acc: 0.5655, best val acc: 0.5831)\n",
      "[Epoch: 15300] train loss: 0.9708, train acc: 0.5551, val loss: 0.9389, val acc: 0.5680  (best train acc: 0.5655, best val acc: 0.5831)\n",
      "[Epoch: 15320] train loss: 0.9677, train acc: 0.5630, val loss: 0.9413, val acc: 0.5659  (best train acc: 0.5655, best val acc: 0.5831)\n",
      "[Epoch: 15340] train loss: 0.9666, train acc: 0.5614, val loss: 0.9369, val acc: 0.5693  (best train acc: 0.5655, best val acc: 0.5831)\n",
      "[Epoch: 15360] train loss: 0.9681, train acc: 0.5598, val loss: 0.9381, val acc: 0.5696  (best train acc: 0.5655, best val acc: 0.5831)\n",
      "[Epoch: 15380] train loss: 0.9738, train acc: 0.5479, val loss: 0.9369, val acc: 0.5710  (best train acc: 0.5655, best val acc: 0.5831)\n",
      "[Epoch: 15400] train loss: 0.9690, train acc: 0.5603, val loss: 0.9384, val acc: 0.5703  (best train acc: 0.5655, best val acc: 0.5831)\n",
      "[Epoch: 15420] train loss: 0.9745, train acc: 0.5475, val loss: 0.9403, val acc: 0.5683  (best train acc: 0.5655, best val acc: 0.5831)\n",
      "[Epoch: 15440] train loss: 0.9703, train acc: 0.5552, val loss: 0.9394, val acc: 0.5717  (best train acc: 0.5655, best val acc: 0.5831)\n",
      "[Epoch: 15460] train loss: 0.9698, train acc: 0.5591, val loss: 0.9369, val acc: 0.5680  (best train acc: 0.5655, best val acc: 0.5831)\n",
      "[Epoch: 15480] train loss: 0.9665, train acc: 0.5565, val loss: 0.9362, val acc: 0.5703  (best train acc: 0.5655, best val acc: 0.5831)\n",
      "[Epoch: 15500] train loss: 0.9663, train acc: 0.5576, val loss: 0.9448, val acc: 0.5666  (best train acc: 0.5655, best val acc: 0.5831)\n",
      "[Epoch: 15520] train loss: 0.9698, train acc: 0.5612, val loss: 0.9412, val acc: 0.5673  (best train acc: 0.5655, best val acc: 0.5831)\n",
      "[Epoch: 15540] train loss: 0.9693, train acc: 0.5571, val loss: 0.9410, val acc: 0.5693  (best train acc: 0.5655, best val acc: 0.5831)\n",
      "[Epoch: 15560] train loss: 0.9738, train acc: 0.5586, val loss: 0.9400, val acc: 0.5720  (best train acc: 0.5655, best val acc: 0.5831)\n",
      "[Epoch: 15580] train loss: 0.9684, train acc: 0.5588, val loss: 0.9381, val acc: 0.5703  (best train acc: 0.5655, best val acc: 0.5831)\n",
      "[Epoch: 15600] train loss: 0.9738, train acc: 0.5586, val loss: 0.9397, val acc: 0.5686  (best train acc: 0.5655, best val acc: 0.5831)\n",
      "[Epoch: 15620] train loss: 0.9762, train acc: 0.5478, val loss: 0.9378, val acc: 0.5693  (best train acc: 0.5655, best val acc: 0.5831)\n",
      "[Epoch: 15640] train loss: 0.9707, train acc: 0.5544, val loss: 0.9370, val acc: 0.5723  (best train acc: 0.5655, best val acc: 0.5831)\n",
      "[Epoch: 15660] train loss: 0.9662, train acc: 0.5611, val loss: 0.9361, val acc: 0.5777  (best train acc: 0.5655, best val acc: 0.5831)\n",
      "[Epoch: 15680] train loss: 0.9687, train acc: 0.5609, val loss: 0.9408, val acc: 0.5690  (best train acc: 0.5655, best val acc: 0.5831)\n",
      "[Epoch: 15700] train loss: 0.9660, train acc: 0.5579, val loss: 0.9405, val acc: 0.5676  (best train acc: 0.5655, best val acc: 0.5831)\n",
      "[Epoch: 15720] train loss: 0.9711, train acc: 0.5507, val loss: 0.9377, val acc: 0.5727  (best train acc: 0.5664, best val acc: 0.5831)\n",
      "[Epoch: 15740] train loss: 0.9709, train acc: 0.5547, val loss: 0.9431, val acc: 0.5669  (best train acc: 0.5664, best val acc: 0.5831)\n",
      "[Epoch: 15760] train loss: 0.9675, train acc: 0.5534, val loss: 0.9405, val acc: 0.5676  (best train acc: 0.5664, best val acc: 0.5831)\n",
      "[Epoch: 15780] train loss: 0.9711, train acc: 0.5565, val loss: 0.9373, val acc: 0.5727  (best train acc: 0.5664, best val acc: 0.5831)\n",
      "[Epoch: 15800] train loss: 0.9648, train acc: 0.5602, val loss: 0.9365, val acc: 0.5713  (best train acc: 0.5664, best val acc: 0.5831)\n",
      "[Epoch: 15820] train loss: 0.9701, train acc: 0.5587, val loss: 0.9357, val acc: 0.5686  (best train acc: 0.5664, best val acc: 0.5831)\n",
      "[Epoch: 15840] train loss: 0.9682, train acc: 0.5588, val loss: 0.9376, val acc: 0.5710  (best train acc: 0.5664, best val acc: 0.5831)\n",
      "[Epoch: 15860] train loss: 0.9658, train acc: 0.5595, val loss: 0.9369, val acc: 0.5669  (best train acc: 0.5664, best val acc: 0.5831)\n",
      "[Epoch: 15880] train loss: 0.9686, train acc: 0.5573, val loss: 0.9368, val acc: 0.5696  (best train acc: 0.5664, best val acc: 0.5831)\n",
      "[Epoch: 15900] train loss: 0.9679, train acc: 0.5618, val loss: 0.9367, val acc: 0.5717  (best train acc: 0.5664, best val acc: 0.5831)\n",
      "[Epoch: 15920] train loss: 0.9727, train acc: 0.5514, val loss: 0.9376, val acc: 0.5683  (best train acc: 0.5664, best val acc: 0.5831)\n",
      "[Epoch: 15940] train loss: 0.9720, train acc: 0.5607, val loss: 0.9366, val acc: 0.5680  (best train acc: 0.5664, best val acc: 0.5831)\n",
      "[Epoch: 15960] train loss: 0.9789, train acc: 0.5451, val loss: 0.9376, val acc: 0.5696  (best train acc: 0.5664, best val acc: 0.5831)\n",
      "[Epoch: 15980] train loss: 0.9718, train acc: 0.5583, val loss: 0.9374, val acc: 0.5747  (best train acc: 0.5664, best val acc: 0.5831)\n",
      "[Epoch: 16000] train loss: 0.9682, train acc: 0.5596, val loss: 0.9374, val acc: 0.5619  (best train acc: 0.5690, best val acc: 0.5831)\n",
      "[Epoch: 16020] train loss: 0.9702, train acc: 0.5580, val loss: 0.9380, val acc: 0.5656  (best train acc: 0.5690, best val acc: 0.5831)\n",
      "[Epoch: 16040] train loss: 0.9709, train acc: 0.5515, val loss: 0.9372, val acc: 0.5680  (best train acc: 0.5690, best val acc: 0.5831)\n",
      "[Epoch: 16060] train loss: 0.9661, train acc: 0.5618, val loss: 0.9368, val acc: 0.5700  (best train acc: 0.5690, best val acc: 0.5831)\n",
      "[Epoch: 16080] train loss: 0.9644, train acc: 0.5627, val loss: 0.9359, val acc: 0.5734  (best train acc: 0.5690, best val acc: 0.5831)\n",
      "[Epoch: 16100] train loss: 0.9686, train acc: 0.5541, val loss: 0.9367, val acc: 0.5696  (best train acc: 0.5690, best val acc: 0.5831)\n",
      "[Epoch: 16120] train loss: 0.9701, train acc: 0.5556, val loss: 0.9378, val acc: 0.5690  (best train acc: 0.5690, best val acc: 0.5831)\n",
      "[Epoch: 16140] train loss: 0.9838, train acc: 0.5395, val loss: 0.9384, val acc: 0.5646  (best train acc: 0.5690, best val acc: 0.5831)\n",
      "[Epoch: 16160] train loss: 0.9694, train acc: 0.5547, val loss: 0.9382, val acc: 0.5595  (best train acc: 0.5690, best val acc: 0.5831)\n",
      "[Epoch: 16180] train loss: 0.9673, train acc: 0.5599, val loss: 0.9371, val acc: 0.5703  (best train acc: 0.5690, best val acc: 0.5831)\n",
      "[Epoch: 16200] train loss: 0.9675, train acc: 0.5632, val loss: 0.9354, val acc: 0.5764  (best train acc: 0.5690, best val acc: 0.5831)\n",
      "[Epoch: 16220] train loss: 0.9690, train acc: 0.5487, val loss: 0.9439, val acc: 0.5639  (best train acc: 0.5690, best val acc: 0.5831)\n",
      "[Epoch: 16240] train loss: 0.9642, train acc: 0.5594, val loss: 0.9445, val acc: 0.5666  (best train acc: 0.5690, best val acc: 0.5831)\n",
      "[Epoch: 16260] train loss: 0.9640, train acc: 0.5588, val loss: 0.9367, val acc: 0.5730  (best train acc: 0.5690, best val acc: 0.5831)\n",
      "[Epoch: 16280] train loss: 0.9687, train acc: 0.5596, val loss: 0.9380, val acc: 0.5690  (best train acc: 0.5690, best val acc: 0.5831)\n",
      "[Epoch: 16300] train loss: 0.9673, train acc: 0.5594, val loss: 0.9370, val acc: 0.5676  (best train acc: 0.5690, best val acc: 0.5831)\n",
      "[Epoch: 16320] train loss: 0.9724, train acc: 0.5641, val loss: 0.9362, val acc: 0.5720  (best train acc: 0.5690, best val acc: 0.5831)\n",
      "[Epoch: 16340] train loss: 0.9697, train acc: 0.5620, val loss: 0.9410, val acc: 0.5757  (best train acc: 0.5690, best val acc: 0.5831)\n",
      "[Epoch: 16360] train loss: 0.9740, train acc: 0.5646, val loss: 0.9366, val acc: 0.5727  (best train acc: 0.5690, best val acc: 0.5831)\n",
      "[Epoch: 16380] train loss: 0.9756, train acc: 0.5555, val loss: 0.9390, val acc: 0.5605  (best train acc: 0.5690, best val acc: 0.5831)\n",
      "[Epoch: 16400] train loss: 0.9666, train acc: 0.5583, val loss: 0.9372, val acc: 0.5680  (best train acc: 0.5690, best val acc: 0.5831)\n",
      "[Epoch: 16420] train loss: 0.9641, train acc: 0.5616, val loss: 0.9351, val acc: 0.5693  (best train acc: 0.5693, best val acc: 0.5831)\n",
      "[Epoch: 16440] train loss: 0.9707, train acc: 0.5640, val loss: 0.9377, val acc: 0.5747  (best train acc: 0.5693, best val acc: 0.5831)\n",
      "[Epoch: 16460] train loss: 0.9712, train acc: 0.5573, val loss: 0.9351, val acc: 0.5717  (best train acc: 0.5693, best val acc: 0.5831)\n",
      "[Epoch: 16480] train loss: 0.9646, train acc: 0.5591, val loss: 0.9373, val acc: 0.5649  (best train acc: 0.5693, best val acc: 0.5831)\n",
      "[Epoch: 16500] train loss: 0.9716, train acc: 0.5505, val loss: 0.9360, val acc: 0.5710  (best train acc: 0.5693, best val acc: 0.5831)\n",
      "[Epoch: 16520] train loss: 0.9656, train acc: 0.5597, val loss: 0.9377, val acc: 0.5690  (best train acc: 0.5693, best val acc: 0.5831)\n",
      "[Epoch: 16540] train loss: 0.9661, train acc: 0.5641, val loss: 0.9438, val acc: 0.5629  (best train acc: 0.5693, best val acc: 0.5831)\n",
      "[Epoch: 16560] train loss: 0.9652, train acc: 0.5599, val loss: 0.9377, val acc: 0.5727  (best train acc: 0.5693, best val acc: 0.5831)\n",
      "[Epoch: 16580] train loss: 0.9636, train acc: 0.5593, val loss: 0.9367, val acc: 0.5754  (best train acc: 0.5693, best val acc: 0.5831)\n",
      "[Epoch: 16600] train loss: 0.9663, train acc: 0.5575, val loss: 0.9382, val acc: 0.5707  (best train acc: 0.5693, best val acc: 0.5831)\n",
      "[Epoch: 16620] train loss: 0.9667, train acc: 0.5633, val loss: 0.9372, val acc: 0.5693  (best train acc: 0.5693, best val acc: 0.5831)\n",
      "[Epoch: 16640] train loss: 0.9686, train acc: 0.5594, val loss: 0.9388, val acc: 0.5673  (best train acc: 0.5693, best val acc: 0.5831)\n",
      "[Epoch: 16660] train loss: 0.9708, train acc: 0.5596, val loss: 0.9435, val acc: 0.5602  (best train acc: 0.5693, best val acc: 0.5831)\n",
      "[Epoch: 16680] train loss: 0.9681, train acc: 0.5625, val loss: 0.9390, val acc: 0.5669  (best train acc: 0.5693, best val acc: 0.5831)\n",
      "[Epoch: 16700] train loss: 0.9690, train acc: 0.5654, val loss: 0.9367, val acc: 0.5605  (best train acc: 0.5693, best val acc: 0.5831)\n",
      "[Epoch: 16720] train loss: 0.9730, train acc: 0.5518, val loss: 0.9423, val acc: 0.5693  (best train acc: 0.5701, best val acc: 0.5831)\n",
      "[Epoch: 16740] train loss: 0.9679, train acc: 0.5615, val loss: 0.9391, val acc: 0.5673  (best train acc: 0.5701, best val acc: 0.5831)\n",
      "[Epoch: 16760] train loss: 0.9694, train acc: 0.5620, val loss: 0.9349, val acc: 0.5636  (best train acc: 0.5701, best val acc: 0.5831)\n",
      "[Epoch: 16780] train loss: 0.9659, train acc: 0.5613, val loss: 0.9425, val acc: 0.5659  (best train acc: 0.5701, best val acc: 0.5831)\n",
      "[Epoch: 16800] train loss: 0.9672, train acc: 0.5618, val loss: 0.9364, val acc: 0.5693  (best train acc: 0.5701, best val acc: 0.5831)\n",
      "[Epoch: 16820] train loss: 0.9715, train acc: 0.5601, val loss: 0.9351, val acc: 0.5740  (best train acc: 0.5701, best val acc: 0.5831)\n",
      "[Epoch: 16840] train loss: 0.9682, train acc: 0.5577, val loss: 0.9371, val acc: 0.5700  (best train acc: 0.5701, best val acc: 0.5831)\n",
      "[Epoch: 16860] train loss: 0.9641, train acc: 0.5581, val loss: 0.9377, val acc: 0.5686  (best train acc: 0.5701, best val acc: 0.5831)\n",
      "[Epoch: 16880] train loss: 0.9656, train acc: 0.5625, val loss: 0.9392, val acc: 0.5575  (best train acc: 0.5701, best val acc: 0.5831)\n",
      "[Epoch: 16900] train loss: 0.9687, train acc: 0.5638, val loss: 0.9362, val acc: 0.5727  (best train acc: 0.5701, best val acc: 0.5831)\n",
      "[Epoch: 16920] train loss: 0.9701, train acc: 0.5596, val loss: 0.9362, val acc: 0.5696  (best train acc: 0.5701, best val acc: 0.5831)\n",
      "[Epoch: 16940] train loss: 0.9622, train acc: 0.5607, val loss: 0.9362, val acc: 0.5693  (best train acc: 0.5701, best val acc: 0.5831)\n",
      "[Epoch: 16960] train loss: 0.9699, train acc: 0.5565, val loss: 0.9348, val acc: 0.5707  (best train acc: 0.5701, best val acc: 0.5831)\n",
      "[Epoch: 16980] train loss: 0.9674, train acc: 0.5571, val loss: 0.9338, val acc: 0.5754  (best train acc: 0.5701, best val acc: 0.5831)\n",
      "[Epoch: 17000] train loss: 0.9859, train acc: 0.5425, val loss: 0.9376, val acc: 0.5720  (best train acc: 0.5701, best val acc: 0.5831)\n",
      "[Epoch: 17020] train loss: 0.9718, train acc: 0.5552, val loss: 0.9385, val acc: 0.5666  (best train acc: 0.5701, best val acc: 0.5831)\n",
      "[Epoch: 17040] train loss: 0.9658, train acc: 0.5644, val loss: 0.9365, val acc: 0.5757  (best train acc: 0.5701, best val acc: 0.5831)\n",
      "[Epoch: 17060] train loss: 0.9593, train acc: 0.5596, val loss: 0.9369, val acc: 0.5622  (best train acc: 0.5701, best val acc: 0.5831)\n",
      "[Epoch: 17080] train loss: 0.9668, train acc: 0.5551, val loss: 0.9378, val acc: 0.5700  (best train acc: 0.5701, best val acc: 0.5831)\n",
      "[Epoch: 17100] train loss: 0.9625, train acc: 0.5669, val loss: 0.9361, val acc: 0.5669  (best train acc: 0.5701, best val acc: 0.5831)\n",
      "[Epoch: 17120] train loss: 0.9632, train acc: 0.5662, val loss: 0.9375, val acc: 0.5686  (best train acc: 0.5701, best val acc: 0.5831)\n",
      "[Epoch: 17140] train loss: 0.9586, train acc: 0.5592, val loss: 0.9376, val acc: 0.5592  (best train acc: 0.5701, best val acc: 0.5831)\n",
      "[Epoch: 17160] train loss: 0.9640, train acc: 0.5589, val loss: 0.9446, val acc: 0.5673  (best train acc: 0.5701, best val acc: 0.5831)\n",
      "[Epoch: 17180] train loss: 0.9655, train acc: 0.5623, val loss: 0.9371, val acc: 0.5723  (best train acc: 0.5701, best val acc: 0.5831)\n",
      "[Epoch: 17200] train loss: 0.9641, train acc: 0.5576, val loss: 0.9365, val acc: 0.5659  (best train acc: 0.5701, best val acc: 0.5831)\n",
      "[Epoch: 17220] train loss: 0.9656, train acc: 0.5662, val loss: 0.9377, val acc: 0.5666  (best train acc: 0.5701, best val acc: 0.5831)\n",
      "[Epoch: 17240] train loss: 0.9629, train acc: 0.5609, val loss: 0.9380, val acc: 0.5656  (best train acc: 0.5701, best val acc: 0.5831)\n",
      "[Epoch: 17260] train loss: 0.9611, train acc: 0.5711, val loss: 0.9359, val acc: 0.5703  (best train acc: 0.5711, best val acc: 0.5831)\n",
      "[Epoch: 17280] train loss: 0.9630, train acc: 0.5607, val loss: 0.9357, val acc: 0.5642  (best train acc: 0.5711, best val acc: 0.5831)\n",
      "[Epoch: 17300] train loss: 0.9598, train acc: 0.5645, val loss: 0.9386, val acc: 0.5723  (best train acc: 0.5711, best val acc: 0.5831)\n",
      "[Epoch: 17320] train loss: 0.9593, train acc: 0.5588, val loss: 0.9395, val acc: 0.5744  (best train acc: 0.5711, best val acc: 0.5831)\n",
      "[Epoch: 17340] train loss: 0.9666, train acc: 0.5604, val loss: 0.9372, val acc: 0.5639  (best train acc: 0.5711, best val acc: 0.5831)\n",
      "[Epoch: 17360] train loss: 0.9658, train acc: 0.5648, val loss: 0.9378, val acc: 0.5612  (best train acc: 0.5711, best val acc: 0.5831)\n",
      "[Epoch: 17380] train loss: 0.9624, train acc: 0.5542, val loss: 0.9391, val acc: 0.5578  (best train acc: 0.5711, best val acc: 0.5831)\n",
      "[Epoch: 17400] train loss: 0.9627, train acc: 0.5595, val loss: 0.9389, val acc: 0.5710  (best train acc: 0.5711, best val acc: 0.5831)\n",
      "[Epoch: 17420] train loss: 0.9596, train acc: 0.5625, val loss: 0.9386, val acc: 0.5737  (best train acc: 0.5711, best val acc: 0.5831)\n",
      "[Epoch: 17440] train loss: 0.9611, train acc: 0.5651, val loss: 0.9368, val acc: 0.5605  (best train acc: 0.5711, best val acc: 0.5831)\n",
      "[Epoch: 17460] train loss: 0.9608, train acc: 0.5615, val loss: 0.9369, val acc: 0.5696  (best train acc: 0.5711, best val acc: 0.5831)\n",
      "[Epoch: 17480] train loss: 0.9617, train acc: 0.5634, val loss: 0.9377, val acc: 0.5646  (best train acc: 0.5720, best val acc: 0.5831)\n",
      "[Epoch: 17500] train loss: 0.9624, train acc: 0.5661, val loss: 0.9375, val acc: 0.5680  (best train acc: 0.5720, best val acc: 0.5831)\n",
      "[Epoch: 17520] train loss: 0.9613, train acc: 0.5648, val loss: 0.9363, val acc: 0.5649  (best train acc: 0.5720, best val acc: 0.5831)\n",
      "[Epoch: 17540] train loss: 0.9578, train acc: 0.5602, val loss: 0.9380, val acc: 0.5693  (best train acc: 0.5720, best val acc: 0.5831)\n",
      "[Epoch: 17560] train loss: 0.9601, train acc: 0.5563, val loss: 0.9384, val acc: 0.5703  (best train acc: 0.5720, best val acc: 0.5831)\n",
      "[Epoch: 17580] train loss: 0.9591, train acc: 0.5690, val loss: 0.9394, val acc: 0.5754  (best train acc: 0.5720, best val acc: 0.5831)\n",
      "[Epoch: 17600] train loss: 0.9694, train acc: 0.5469, val loss: 0.9402, val acc: 0.5649  (best train acc: 0.5733, best val acc: 0.5831)\n",
      "[Epoch: 17620] train loss: 0.9740, train acc: 0.5522, val loss: 0.9371, val acc: 0.5663  (best train acc: 0.5733, best val acc: 0.5831)\n",
      "[Epoch: 17640] train loss: 0.9639, train acc: 0.5647, val loss: 0.9392, val acc: 0.5578  (best train acc: 0.5733, best val acc: 0.5831)\n",
      "[Epoch: 17660] train loss: 0.9683, train acc: 0.5589, val loss: 0.9612, val acc: 0.5268  (best train acc: 0.5733, best val acc: 0.5831)\n",
      "[Epoch: 17680] train loss: 0.9577, train acc: 0.5642, val loss: 0.9377, val acc: 0.5686  (best train acc: 0.5733, best val acc: 0.5831)\n",
      "[Epoch: 17700] train loss: 0.9611, train acc: 0.5716, val loss: 0.9375, val acc: 0.5703  (best train acc: 0.5733, best val acc: 0.5831)\n",
      "[Epoch: 17720] train loss: 0.9606, train acc: 0.5675, val loss: 0.9390, val acc: 0.5690  (best train acc: 0.5733, best val acc: 0.5831)\n",
      "[Epoch: 17740] train loss: 0.9621, train acc: 0.5614, val loss: 0.9374, val acc: 0.5676  (best train acc: 0.5733, best val acc: 0.5831)\n",
      "[Epoch: 17760] train loss: 0.9591, train acc: 0.5565, val loss: 0.9413, val acc: 0.5686  (best train acc: 0.5733, best val acc: 0.5831)\n",
      "[Epoch: 17780] train loss: 0.9606, train acc: 0.5659, val loss: 0.9383, val acc: 0.5669  (best train acc: 0.5733, best val acc: 0.5831)\n",
      "[Epoch: 17800] train loss: 0.9643, train acc: 0.5682, val loss: 0.9401, val acc: 0.5673  (best train acc: 0.5733, best val acc: 0.5831)\n",
      "[Epoch: 17820] train loss: 0.9646, train acc: 0.5614, val loss: 0.9401, val acc: 0.5585  (best train acc: 0.5733, best val acc: 0.5831)\n",
      "[Epoch: 17840] train loss: 0.9598, train acc: 0.5638, val loss: 0.9378, val acc: 0.5659  (best train acc: 0.5733, best val acc: 0.5831)\n",
      "[Epoch: 17860] train loss: 0.9637, train acc: 0.5633, val loss: 0.9413, val acc: 0.5622  (best train acc: 0.5733, best val acc: 0.5831)\n",
      "[Epoch: 17880] train loss: 0.9663, train acc: 0.5599, val loss: 0.9544, val acc: 0.5680  (best train acc: 0.5733, best val acc: 0.5831)\n",
      "[Epoch: 17900] train loss: 0.9664, train acc: 0.5555, val loss: 0.9492, val acc: 0.5686  (best train acc: 0.5733, best val acc: 0.5831)\n",
      "[Epoch: 17920] train loss: 0.9646, train acc: 0.5623, val loss: 0.9517, val acc: 0.5646  (best train acc: 0.5733, best val acc: 0.5831)\n",
      "[Epoch: 17940] train loss: 0.9611, train acc: 0.5591, val loss: 0.9512, val acc: 0.5669  (best train acc: 0.5733, best val acc: 0.5831)\n",
      "[Epoch: 17960] train loss: 0.9681, train acc: 0.5638, val loss: 0.9521, val acc: 0.5656  (best train acc: 0.5733, best val acc: 0.5831)\n",
      "[Epoch: 17980] train loss: 0.9630, train acc: 0.5568, val loss: 0.9614, val acc: 0.5612  (best train acc: 0.5733, best val acc: 0.5831)\n",
      "[Epoch: 18000] train loss: 0.9769, train acc: 0.5456, val loss: 0.9559, val acc: 0.5656  (best train acc: 0.5733, best val acc: 0.5831)\n",
      "[Epoch: 18020] train loss: 0.9678, train acc: 0.5583, val loss: 0.9509, val acc: 0.5700  (best train acc: 0.5733, best val acc: 0.5831)\n",
      "[Epoch: 18040] train loss: 0.9630, train acc: 0.5612, val loss: 0.9490, val acc: 0.5639  (best train acc: 0.5733, best val acc: 0.5831)\n",
      "[Epoch: 18060] train loss: 0.9653, train acc: 0.5604, val loss: 0.9441, val acc: 0.5595  (best train acc: 0.5733, best val acc: 0.5831)\n",
      "[Epoch: 18080] train loss: 0.9625, train acc: 0.5651, val loss: 0.9474, val acc: 0.5585  (best train acc: 0.5733, best val acc: 0.5831)\n",
      "[Epoch: 18100] train loss: 0.9671, train acc: 0.5565, val loss: 0.9505, val acc: 0.5740  (best train acc: 0.5733, best val acc: 0.5831)\n",
      "[Epoch: 18120] train loss: 0.9601, train acc: 0.5649, val loss: 0.9473, val acc: 0.5686  (best train acc: 0.5733, best val acc: 0.5831)\n",
      "[Epoch: 18140] train loss: 0.9586, train acc: 0.5578, val loss: 0.9474, val acc: 0.5696  (best train acc: 0.5733, best val acc: 0.5831)\n",
      "[Epoch: 18160] train loss: 0.9600, train acc: 0.5665, val loss: 0.9461, val acc: 0.5683  (best train acc: 0.5733, best val acc: 0.5831)\n",
      "[Epoch: 18180] train loss: 0.9604, train acc: 0.5566, val loss: 0.9485, val acc: 0.5616  (best train acc: 0.5733, best val acc: 0.5831)\n",
      "[Epoch: 18200] train loss: 0.9615, train acc: 0.5601, val loss: 0.9469, val acc: 0.5730  (best train acc: 0.5733, best val acc: 0.5831)\n",
      "[Epoch: 18220] train loss: 0.9590, train acc: 0.5656, val loss: 0.9468, val acc: 0.5656  (best train acc: 0.5733, best val acc: 0.5831)\n",
      "[Epoch: 18240] train loss: 0.9570, train acc: 0.5688, val loss: 0.9468, val acc: 0.5710  (best train acc: 0.5733, best val acc: 0.5831)\n",
      "[Epoch: 18260] train loss: 0.9592, train acc: 0.5628, val loss: 0.9497, val acc: 0.5673  (best train acc: 0.5733, best val acc: 0.5831)\n",
      "[Epoch: 18280] train loss: 0.9621, train acc: 0.5645, val loss: 0.9490, val acc: 0.5663  (best train acc: 0.5733, best val acc: 0.5831)\n",
      "[Epoch: 18300] train loss: 0.9582, train acc: 0.5656, val loss: 0.9473, val acc: 0.5629  (best train acc: 0.5733, best val acc: 0.5831)\n",
      "[Epoch: 18320] train loss: 0.9609, train acc: 0.5612, val loss: 0.9507, val acc: 0.5683  (best train acc: 0.5733, best val acc: 0.5831)\n",
      "[Epoch: 18340] train loss: 0.9551, train acc: 0.5662, val loss: 0.9489, val acc: 0.5680  (best train acc: 0.5733, best val acc: 0.5831)\n",
      "[Epoch: 18360] train loss: 0.9587, train acc: 0.5630, val loss: 0.9466, val acc: 0.5673  (best train acc: 0.5733, best val acc: 0.5831)\n",
      "[Epoch: 18380] train loss: 0.9622, train acc: 0.5596, val loss: 0.9506, val acc: 0.5730  (best train acc: 0.5733, best val acc: 0.5831)\n",
      "[Epoch: 18400] train loss: 0.9593, train acc: 0.5588, val loss: 0.9527, val acc: 0.5757  (best train acc: 0.5733, best val acc: 0.5831)\n",
      "[Epoch: 18420] train loss: 0.9577, train acc: 0.5653, val loss: 0.9451, val acc: 0.5653  (best train acc: 0.5733, best val acc: 0.5831)\n",
      "[Epoch: 18440] train loss: 0.9551, train acc: 0.5633, val loss: 0.9443, val acc: 0.5619  (best train acc: 0.5733, best val acc: 0.5831)\n",
      "[Epoch: 18460] train loss: 0.9547, train acc: 0.5688, val loss: 0.9479, val acc: 0.5659  (best train acc: 0.5733, best val acc: 0.5831)\n",
      "[Epoch: 18480] train loss: 0.9559, train acc: 0.5683, val loss: 0.9456, val acc: 0.5653  (best train acc: 0.5733, best val acc: 0.5831)\n",
      "[Epoch: 18500] train loss: 0.9582, train acc: 0.5683, val loss: 0.9446, val acc: 0.5632  (best train acc: 0.5733, best val acc: 0.5831)\n",
      "[Epoch: 18520] train loss: 0.9587, train acc: 0.5652, val loss: 0.9450, val acc: 0.5666  (best train acc: 0.5733, best val acc: 0.5831)\n",
      "[Epoch: 18540] train loss: 0.9614, train acc: 0.5617, val loss: 0.9498, val acc: 0.5676  (best train acc: 0.5733, best val acc: 0.5831)\n",
      "[Epoch: 18560] train loss: 0.9637, train acc: 0.5705, val loss: 0.9453, val acc: 0.5629  (best train acc: 0.5733, best val acc: 0.5831)\n",
      "[Epoch: 18580] train loss: 0.9606, train acc: 0.5585, val loss: 0.9490, val acc: 0.5747  (best train acc: 0.5733, best val acc: 0.5831)\n",
      "[Epoch: 18600] train loss: 0.9555, train acc: 0.5639, val loss: 0.9458, val acc: 0.5707  (best train acc: 0.5733, best val acc: 0.5831)\n",
      "[Epoch: 18620] train loss: 0.9587, train acc: 0.5659, val loss: 0.9459, val acc: 0.5740  (best train acc: 0.5733, best val acc: 0.5831)\n",
      "[Epoch: 18640] train loss: 0.9636, train acc: 0.5609, val loss: 0.9475, val acc: 0.5707  (best train acc: 0.5733, best val acc: 0.5831)\n",
      "[Epoch: 18660] train loss: 0.9552, train acc: 0.5690, val loss: 0.9419, val acc: 0.5572  (best train acc: 0.5733, best val acc: 0.5831)\n",
      "[Epoch: 18680] train loss: 0.9613, train acc: 0.5566, val loss: 0.9446, val acc: 0.5656  (best train acc: 0.5733, best val acc: 0.5831)\n",
      "[Epoch: 18700] train loss: 0.9795, train acc: 0.5617, val loss: 0.9451, val acc: 0.5723  (best train acc: 0.5733, best val acc: 0.5831)\n",
      "[Epoch: 18720] train loss: 0.9604, train acc: 0.5575, val loss: 0.9490, val acc: 0.5730  (best train acc: 0.5733, best val acc: 0.5831)\n",
      "[Epoch: 18740] train loss: 0.9606, train acc: 0.5695, val loss: 0.9445, val acc: 0.5737  (best train acc: 0.5733, best val acc: 0.5831)\n",
      "[Epoch: 18760] train loss: 0.9578, train acc: 0.5624, val loss: 0.9431, val acc: 0.5710  (best train acc: 0.5733, best val acc: 0.5831)\n",
      "[Epoch: 18780] train loss: 0.9595, train acc: 0.5641, val loss: 0.9451, val acc: 0.5703  (best train acc: 0.5733, best val acc: 0.5831)\n",
      "[Epoch: 18800] train loss: 0.9565, train acc: 0.5591, val loss: 0.9434, val acc: 0.5659  (best train acc: 0.5733, best val acc: 0.5831)\n",
      "[Epoch: 18820] train loss: 0.9534, train acc: 0.5665, val loss: 0.9429, val acc: 0.5703  (best train acc: 0.5733, best val acc: 0.5831)\n",
      "[Epoch: 18840] train loss: 0.9570, train acc: 0.5645, val loss: 0.9422, val acc: 0.5686  (best train acc: 0.5733, best val acc: 0.5831)\n",
      "[Epoch: 18860] train loss: 0.9612, train acc: 0.5600, val loss: 0.9442, val acc: 0.5622  (best train acc: 0.5733, best val acc: 0.5831)\n",
      "[Epoch: 18880] train loss: 0.9608, train acc: 0.5616, val loss: 0.9474, val acc: 0.5717  (best train acc: 0.5733, best val acc: 0.5831)\n",
      "[Epoch: 18900] train loss: 0.9598, train acc: 0.5603, val loss: 0.9486, val acc: 0.5669  (best train acc: 0.5733, best val acc: 0.5831)\n",
      "[Epoch: 18920] train loss: 0.9602, train acc: 0.5564, val loss: 0.9433, val acc: 0.5629  (best train acc: 0.5733, best val acc: 0.5831)\n",
      "[Epoch: 18940] train loss: 0.9550, train acc: 0.5666, val loss: 0.9422, val acc: 0.5602  (best train acc: 0.5733, best val acc: 0.5831)\n",
      "[Epoch: 18960] train loss: 0.9652, train acc: 0.5651, val loss: 0.9441, val acc: 0.5656  (best train acc: 0.5733, best val acc: 0.5831)\n",
      "[Epoch: 18980] train loss: 0.9609, train acc: 0.5643, val loss: 0.9575, val acc: 0.5663  (best train acc: 0.5733, best val acc: 0.5831)\n",
      "[Epoch: 19000] train loss: 0.9574, train acc: 0.5588, val loss: 0.9619, val acc: 0.5582  (best train acc: 0.5733, best val acc: 0.5831)\n",
      "[Epoch: 19020] train loss: 0.9605, train acc: 0.5615, val loss: 0.9431, val acc: 0.5632  (best train acc: 0.5733, best val acc: 0.5831)\n",
      "[Epoch: 19040] train loss: 0.9621, train acc: 0.5680, val loss: 0.9448, val acc: 0.5703  (best train acc: 0.5733, best val acc: 0.5831)\n",
      "[Epoch: 19060] train loss: 0.9611, train acc: 0.5650, val loss: 0.9416, val acc: 0.5737  (best train acc: 0.5733, best val acc: 0.5831)\n",
      "[Epoch: 19080] train loss: 0.9575, train acc: 0.5628, val loss: 0.9448, val acc: 0.5713  (best train acc: 0.5733, best val acc: 0.5831)\n",
      "[Epoch: 19100] train loss: 0.9581, train acc: 0.5630, val loss: 0.9416, val acc: 0.5690  (best train acc: 0.5733, best val acc: 0.5831)\n",
      "[Epoch: 19120] train loss: 0.9595, train acc: 0.5633, val loss: 0.9440, val acc: 0.5663  (best train acc: 0.5733, best val acc: 0.5831)\n",
      "[Epoch: 19140] train loss: 0.9602, train acc: 0.5538, val loss: 0.9498, val acc: 0.5673  (best train acc: 0.5733, best val acc: 0.5831)\n",
      "[Epoch: 19160] train loss: 0.9612, train acc: 0.5609, val loss: 0.9456, val acc: 0.5771  (best train acc: 0.5733, best val acc: 0.5831)\n",
      "[Epoch: 19180] train loss: 0.9547, train acc: 0.5635, val loss: 0.9449, val acc: 0.5666  (best train acc: 0.5733, best val acc: 0.5831)\n",
      "[Epoch: 19200] train loss: 0.9625, train acc: 0.5667, val loss: 0.9494, val acc: 0.5784  (best train acc: 0.5733, best val acc: 0.5831)\n",
      "[Epoch: 19220] train loss: 0.9626, train acc: 0.5594, val loss: 0.9492, val acc: 0.5737  (best train acc: 0.5733, best val acc: 0.5831)\n",
      "[Epoch: 19240] train loss: 0.9573, train acc: 0.5675, val loss: 0.9420, val acc: 0.5693  (best train acc: 0.5733, best val acc: 0.5831)\n",
      "[Epoch: 19260] train loss: 0.9716, train acc: 0.5602, val loss: 0.9430, val acc: 0.5710  (best train acc: 0.5733, best val acc: 0.5831)\n",
      "[Epoch: 19280] train loss: 0.9569, train acc: 0.5697, val loss: 0.9431, val acc: 0.5723  (best train acc: 0.5733, best val acc: 0.5831)\n",
      "[Epoch: 19300] train loss: 0.9582, train acc: 0.5632, val loss: 0.9418, val acc: 0.5727  (best train acc: 0.5733, best val acc: 0.5831)\n",
      "[Epoch: 19320] train loss: 0.9553, train acc: 0.5654, val loss: 0.9438, val acc: 0.5754  (best train acc: 0.5733, best val acc: 0.5831)\n",
      "[Epoch: 19340] train loss: 0.9565, train acc: 0.5644, val loss: 0.9414, val acc: 0.5754  (best train acc: 0.5733, best val acc: 0.5831)\n",
      "[Epoch: 19360] train loss: 0.9590, train acc: 0.5711, val loss: 0.9404, val acc: 0.5636  (best train acc: 0.5733, best val acc: 0.5831)\n",
      "[Epoch: 19380] train loss: 0.9572, train acc: 0.5677, val loss: 0.9413, val acc: 0.5713  (best train acc: 0.5733, best val acc: 0.5831)\n",
      "[Epoch: 19400] train loss: 0.9583, train acc: 0.5674, val loss: 0.9426, val acc: 0.5707  (best train acc: 0.5733, best val acc: 0.5831)\n",
      "[Epoch: 19420] train loss: 0.9591, train acc: 0.5620, val loss: 0.9490, val acc: 0.5636  (best train acc: 0.5733, best val acc: 0.5831)\n",
      "[Epoch: 19440] train loss: 0.9595, train acc: 0.5620, val loss: 0.9420, val acc: 0.5734  (best train acc: 0.5733, best val acc: 0.5831)\n",
      "[Epoch: 19460] train loss: 0.9633, train acc: 0.5686, val loss: 0.9411, val acc: 0.5690  (best train acc: 0.5733, best val acc: 0.5831)\n",
      "[Epoch: 19480] train loss: 0.9573, train acc: 0.5655, val loss: 0.9412, val acc: 0.5680  (best train acc: 0.5733, best val acc: 0.5831)\n",
      "[Epoch: 19500] train loss: 0.9585, train acc: 0.5644, val loss: 0.9441, val acc: 0.5690  (best train acc: 0.5733, best val acc: 0.5831)\n",
      "[Epoch: 19520] train loss: 0.9627, train acc: 0.5662, val loss: 0.9384, val acc: 0.5686  (best train acc: 0.5733, best val acc: 0.5831)\n",
      "[Epoch: 19540] train loss: 0.9588, train acc: 0.5628, val loss: 0.9487, val acc: 0.5663  (best train acc: 0.5733, best val acc: 0.5831)\n",
      "[Epoch: 19560] train loss: 0.9603, train acc: 0.5572, val loss: 0.9456, val acc: 0.5713  (best train acc: 0.5733, best val acc: 0.5831)\n",
      "[Epoch: 19580] train loss: 0.9569, train acc: 0.5597, val loss: 0.9494, val acc: 0.5673  (best train acc: 0.5733, best val acc: 0.5831)\n",
      "[Epoch: 19600] train loss: 0.9585, train acc: 0.5631, val loss: 0.9439, val acc: 0.5740  (best train acc: 0.5733, best val acc: 0.5831)\n",
      "[Epoch: 19620] train loss: 0.9574, train acc: 0.5675, val loss: 0.9458, val acc: 0.5717  (best train acc: 0.5733, best val acc: 0.5831)\n",
      "[Epoch: 19640] train loss: 0.9615, train acc: 0.5675, val loss: 0.9414, val acc: 0.5700  (best train acc: 0.5733, best val acc: 0.5831)\n",
      "[Epoch: 19660] train loss: 0.9662, train acc: 0.5414, val loss: 0.9490, val acc: 0.5744  (best train acc: 0.5733, best val acc: 0.5831)\n",
      "[Epoch: 19680] train loss: 0.9641, train acc: 0.5688, val loss: 0.9408, val acc: 0.5703  (best train acc: 0.5733, best val acc: 0.5831)\n",
      "[Epoch: 19700] train loss: 0.9543, train acc: 0.5648, val loss: 0.9429, val acc: 0.5723  (best train acc: 0.5733, best val acc: 0.5831)\n",
      "[Epoch: 19720] train loss: 0.9565, train acc: 0.5616, val loss: 0.9390, val acc: 0.5686  (best train acc: 0.5733, best val acc: 0.5831)\n",
      "[Epoch: 19740] train loss: 0.9571, train acc: 0.5615, val loss: 0.9422, val acc: 0.5740  (best train acc: 0.5733, best val acc: 0.5831)\n",
      "[Epoch: 19760] train loss: 0.9586, train acc: 0.5648, val loss: 0.9423, val acc: 0.5690  (best train acc: 0.5733, best val acc: 0.5831)\n",
      "[Epoch: 19780] train loss: 0.9585, train acc: 0.5657, val loss: 0.9401, val acc: 0.5663  (best train acc: 0.5733, best val acc: 0.5831)\n",
      "[Epoch: 19800] train loss: 0.9627, train acc: 0.5503, val loss: 0.9392, val acc: 0.5703  (best train acc: 0.5733, best val acc: 0.5831)\n",
      "[Epoch: 19820] train loss: 0.9559, train acc: 0.5642, val loss: 0.9533, val acc: 0.5700  (best train acc: 0.5733, best val acc: 0.5831)\n",
      "[Epoch: 19840] train loss: 0.9631, train acc: 0.5564, val loss: 0.9424, val acc: 0.5734  (best train acc: 0.5733, best val acc: 0.5831)\n",
      "[Epoch: 19860] train loss: 0.9556, train acc: 0.5635, val loss: 0.9431, val acc: 0.5730  (best train acc: 0.5733, best val acc: 0.5831)\n",
      "[Epoch: 19880] train loss: 0.9599, train acc: 0.5672, val loss: 0.9401, val acc: 0.5764  (best train acc: 0.5733, best val acc: 0.5831)\n",
      "[Epoch: 19900] train loss: 0.9599, train acc: 0.5646, val loss: 0.9397, val acc: 0.5764  (best train acc: 0.5733, best val acc: 0.5831)\n",
      "[Epoch: 19920] train loss: 0.9550, train acc: 0.5656, val loss: 0.9379, val acc: 0.5669  (best train acc: 0.5733, best val acc: 0.5831)\n",
      "[Epoch: 19940] train loss: 0.9568, train acc: 0.5664, val loss: 0.9405, val acc: 0.5734  (best train acc: 0.5733, best val acc: 0.5831)\n",
      "[Epoch: 19960] train loss: 0.9559, train acc: 0.5628, val loss: 0.9404, val acc: 0.5717  (best train acc: 0.5733, best val acc: 0.5831)\n",
      "[Epoch: 19980] train loss: 0.9582, train acc: 0.5625, val loss: 0.9412, val acc: 0.5686  (best train acc: 0.5733, best val acc: 0.5831)\n",
      "[Epoch: 20000] train loss: 0.9711, train acc: 0.5590, val loss: 0.9434, val acc: 0.5754  (best train acc: 0.5742, best val acc: 0.5831)\n",
      "[Epoch: 20020] train loss: 0.9566, train acc: 0.5709, val loss: 0.9430, val acc: 0.5686  (best train acc: 0.5742, best val acc: 0.5831)\n",
      "[Epoch: 20040] train loss: 0.9736, train acc: 0.5398, val loss: 0.9701, val acc: 0.5632  (best train acc: 0.5742, best val acc: 0.5831)\n",
      "[Epoch: 20060] train loss: 0.9581, train acc: 0.5611, val loss: 0.9394, val acc: 0.5673  (best train acc: 0.5742, best val acc: 0.5831)\n",
      "[Epoch: 20080] train loss: 0.9510, train acc: 0.5611, val loss: 0.9421, val acc: 0.5730  (best train acc: 0.5742, best val acc: 0.5831)\n",
      "[Epoch: 20100] train loss: 1.0519, train acc: 0.4699, val loss: 1.0666, val acc: 0.4567  (best train acc: 0.5742, best val acc: 0.5831)\n",
      "[Epoch: 20120] train loss: 1.0224, train acc: 0.4760, val loss: 1.0217, val acc: 0.4658  (best train acc: 0.5742, best val acc: 0.5831)\n",
      "[Epoch: 20140] train loss: 0.9813, train acc: 0.5550, val loss: 0.9604, val acc: 0.5686  (best train acc: 0.5742, best val acc: 0.5831)\n",
      "[Epoch: 20160] train loss: 0.9698, train acc: 0.5586, val loss: 0.9530, val acc: 0.5599  (best train acc: 0.5742, best val acc: 0.5831)\n",
      "[Epoch: 20180] train loss: 0.9597, train acc: 0.5630, val loss: 0.9493, val acc: 0.5666  (best train acc: 0.5742, best val acc: 0.5831)\n",
      "[Epoch: 20200] train loss: 0.9615, train acc: 0.5577, val loss: 0.9480, val acc: 0.5666  (best train acc: 0.5742, best val acc: 0.5831)\n",
      "[Epoch: 20220] train loss: 0.9595, train acc: 0.5680, val loss: 0.9425, val acc: 0.5663  (best train acc: 0.5742, best val acc: 0.5831)\n",
      "[Epoch: 20240] train loss: 0.9603, train acc: 0.5595, val loss: 0.9437, val acc: 0.5666  (best train acc: 0.5742, best val acc: 0.5831)\n",
      "[Epoch: 20260] train loss: 0.9624, train acc: 0.5607, val loss: 0.9442, val acc: 0.5669  (best train acc: 0.5742, best val acc: 0.5831)\n",
      "[Epoch: 20280] train loss: 0.9605, train acc: 0.5563, val loss: 0.9431, val acc: 0.5700  (best train acc: 0.5742, best val acc: 0.5831)\n",
      "[Epoch: 20300] train loss: 0.9625, train acc: 0.5614, val loss: 0.9445, val acc: 0.5636  (best train acc: 0.5742, best val acc: 0.5831)\n",
      "[Epoch: 20320] train loss: 0.9648, train acc: 0.5533, val loss: 0.9423, val acc: 0.5744  (best train acc: 0.5742, best val acc: 0.5831)\n",
      "[Epoch: 20340] train loss: 0.9603, train acc: 0.5651, val loss: 0.9434, val acc: 0.5636  (best train acc: 0.5742, best val acc: 0.5831)\n",
      "[Epoch: 20360] train loss: 0.9637, train acc: 0.5708, val loss: 0.9416, val acc: 0.5646  (best train acc: 0.5742, best val acc: 0.5831)\n",
      "[Epoch: 20380] train loss: 0.9618, train acc: 0.5519, val loss: 0.9473, val acc: 0.5646  (best train acc: 0.5742, best val acc: 0.5831)\n",
      "[Epoch: 20400] train loss: 0.9663, train acc: 0.5491, val loss: 0.9420, val acc: 0.5703  (best train acc: 0.5742, best val acc: 0.5831)\n",
      "[Epoch: 20420] train loss: 0.9597, train acc: 0.5644, val loss: 0.9408, val acc: 0.5646  (best train acc: 0.5742, best val acc: 0.5831)\n",
      "[Epoch: 20440] train loss: 0.9577, train acc: 0.5665, val loss: 0.9421, val acc: 0.5663  (best train acc: 0.5742, best val acc: 0.5831)\n",
      "[Epoch: 20460] train loss: 0.9598, train acc: 0.5610, val loss: 0.9444, val acc: 0.5737  (best train acc: 0.5742, best val acc: 0.5831)\n",
      "[Epoch: 20480] train loss: 0.9597, train acc: 0.5608, val loss: 0.9433, val acc: 0.5727  (best train acc: 0.5742, best val acc: 0.5831)\n",
      "[Epoch: 20500] train loss: 0.9637, train acc: 0.5685, val loss: 0.9408, val acc: 0.5690  (best train acc: 0.5742, best val acc: 0.5831)\n",
      "[Epoch: 20520] train loss: 0.9616, train acc: 0.5673, val loss: 0.9470, val acc: 0.5686  (best train acc: 0.5742, best val acc: 0.5831)\n",
      "[Epoch: 20540] train loss: 0.9585, train acc: 0.5672, val loss: 0.9447, val acc: 0.5696  (best train acc: 0.5742, best val acc: 0.5831)\n",
      "[Epoch: 20560] train loss: 0.9566, train acc: 0.5671, val loss: 0.9461, val acc: 0.5663  (best train acc: 0.5742, best val acc: 0.5831)\n",
      "[Epoch: 20580] train loss: 0.9544, train acc: 0.5643, val loss: 0.9425, val acc: 0.5642  (best train acc: 0.5742, best val acc: 0.5831)\n",
      "[Epoch: 20600] train loss: 0.9601, train acc: 0.5560, val loss: 0.9420, val acc: 0.5686  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 20620] train loss: 0.9587, train acc: 0.5709, val loss: 0.9418, val acc: 0.5642  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 20640] train loss: 0.9552, train acc: 0.5645, val loss: 0.9440, val acc: 0.5626  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 20660] train loss: 0.9575, train acc: 0.5563, val loss: 0.9510, val acc: 0.5663  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 20680] train loss: 0.9605, train acc: 0.5530, val loss: 0.9481, val acc: 0.5693  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 20700] train loss: 0.9629, train acc: 0.5543, val loss: 0.9433, val acc: 0.5589  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 20720] train loss: 0.9734, train acc: 0.5588, val loss: 0.9453, val acc: 0.5696  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 20740] train loss: 0.9642, train acc: 0.5649, val loss: 0.9452, val acc: 0.5568  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 20760] train loss: 0.9619, train acc: 0.5583, val loss: 0.9462, val acc: 0.5734  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 20780] train loss: 0.9557, train acc: 0.5590, val loss: 0.9461, val acc: 0.5734  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 20800] train loss: 0.9565, train acc: 0.5658, val loss: 0.9415, val acc: 0.5642  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 20820] train loss: 0.9594, train acc: 0.5651, val loss: 0.9436, val acc: 0.5680  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 20840] train loss: 0.9652, train acc: 0.5518, val loss: 0.9495, val acc: 0.5693  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 20860] train loss: 0.9594, train acc: 0.5603, val loss: 0.9445, val acc: 0.5710  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 20880] train loss: 0.9611, train acc: 0.5670, val loss: 0.9430, val acc: 0.5609  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 20900] train loss: 0.9630, train acc: 0.5647, val loss: 0.9447, val acc: 0.5703  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 20920] train loss: 0.9570, train acc: 0.5605, val loss: 0.9423, val acc: 0.5690  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 20940] train loss: 0.9592, train acc: 0.5596, val loss: 0.9458, val acc: 0.5720  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 20960] train loss: 0.9570, train acc: 0.5659, val loss: 0.9422, val acc: 0.5663  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 20980] train loss: 0.9618, train acc: 0.5583, val loss: 0.9423, val acc: 0.5696  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 21000] train loss: 0.9621, train acc: 0.5573, val loss: 0.9442, val acc: 0.5686  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 21020] train loss: 0.9595, train acc: 0.5628, val loss: 0.9431, val acc: 0.5696  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 21040] train loss: 0.9634, train acc: 0.5617, val loss: 0.9468, val acc: 0.5555  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 21060] train loss: 0.9580, train acc: 0.5575, val loss: 0.9466, val acc: 0.5734  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 21080] train loss: 0.9571, train acc: 0.5647, val loss: 0.9438, val acc: 0.5629  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 21100] train loss: 0.9616, train acc: 0.5648, val loss: 0.9417, val acc: 0.5622  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 21120] train loss: 0.9551, train acc: 0.5715, val loss: 0.9414, val acc: 0.5700  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 21140] train loss: 0.9597, train acc: 0.5705, val loss: 0.9444, val acc: 0.5707  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 21160] train loss: 0.9577, train acc: 0.5654, val loss: 0.9445, val acc: 0.5673  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 21180] train loss: 0.9613, train acc: 0.5646, val loss: 0.9414, val acc: 0.5734  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 21200] train loss: 0.9586, train acc: 0.5643, val loss: 0.9473, val acc: 0.5656  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 21220] train loss: 0.9569, train acc: 0.5689, val loss: 0.9445, val acc: 0.5653  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 21240] train loss: 0.9605, train acc: 0.5536, val loss: 0.9426, val acc: 0.5740  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 21260] train loss: 0.9634, train acc: 0.5544, val loss: 0.9451, val acc: 0.5696  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 21280] train loss: 0.9612, train acc: 0.5614, val loss: 0.9429, val acc: 0.5619  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 21300] train loss: 0.9601, train acc: 0.5672, val loss: 0.9413, val acc: 0.5676  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 21320] train loss: 0.9601, train acc: 0.5635, val loss: 0.9444, val acc: 0.5713  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 21340] train loss: 0.9556, train acc: 0.5578, val loss: 0.9422, val acc: 0.5673  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 21360] train loss: 0.9564, train acc: 0.5600, val loss: 0.9444, val acc: 0.5666  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 21380] train loss: 0.9600, train acc: 0.5615, val loss: 0.9423, val acc: 0.5616  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 21400] train loss: 0.9578, train acc: 0.5608, val loss: 0.9432, val acc: 0.5730  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 21420] train loss: 0.9599, train acc: 0.5683, val loss: 0.9419, val acc: 0.5713  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 21440] train loss: 0.9592, train acc: 0.5635, val loss: 0.9409, val acc: 0.5723  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 21460] train loss: 0.9579, train acc: 0.5621, val loss: 0.9414, val acc: 0.5734  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 21480] train loss: 0.9597, train acc: 0.5630, val loss: 0.9461, val acc: 0.5646  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 21500] train loss: 0.9631, train acc: 0.5544, val loss: 0.9439, val acc: 0.5717  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 21520] train loss: 0.9568, train acc: 0.5683, val loss: 0.9496, val acc: 0.5649  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 21540] train loss: 0.9556, train acc: 0.5646, val loss: 0.9429, val acc: 0.5713  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 21560] train loss: 0.9639, train acc: 0.5663, val loss: 0.9424, val acc: 0.5703  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 21580] train loss: 0.9797, train acc: 0.5377, val loss: 0.9515, val acc: 0.5646  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 21600] train loss: 0.9631, train acc: 0.5667, val loss: 0.9455, val acc: 0.5703  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 21620] train loss: 0.9684, train acc: 0.5404, val loss: 0.9592, val acc: 0.5575  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 21640] train loss: 0.9582, train acc: 0.5612, val loss: 0.9465, val acc: 0.5693  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 21660] train loss: 0.9616, train acc: 0.5544, val loss: 0.9475, val acc: 0.5680  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 21680] train loss: 0.9565, train acc: 0.5637, val loss: 0.9411, val acc: 0.5659  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 21700] train loss: 0.9583, train acc: 0.5689, val loss: 0.9457, val acc: 0.5572  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 21720] train loss: 0.9555, train acc: 0.5656, val loss: 0.9417, val acc: 0.5669  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 21740] train loss: 0.9615, train acc: 0.5570, val loss: 0.9428, val acc: 0.5693  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 21760] train loss: 0.9558, train acc: 0.5715, val loss: 0.9428, val acc: 0.5707  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 21780] train loss: 0.9613, train acc: 0.5607, val loss: 0.9429, val acc: 0.5663  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 21800] train loss: 0.9665, train acc: 0.5612, val loss: 0.9442, val acc: 0.5669  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 21820] train loss: 0.9565, train acc: 0.5625, val loss: 0.9432, val acc: 0.5629  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 21840] train loss: 0.9605, train acc: 0.5599, val loss: 0.9450, val acc: 0.5690  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 21860] train loss: 0.9605, train acc: 0.5629, val loss: 0.9412, val acc: 0.5642  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 21880] train loss: 0.9656, train acc: 0.5707, val loss: 0.9437, val acc: 0.5700  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 21900] train loss: 0.9592, train acc: 0.5701, val loss: 0.9429, val acc: 0.5727  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 21920] train loss: 0.9589, train acc: 0.5627, val loss: 0.9425, val acc: 0.5690  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 21940] train loss: 0.9547, train acc: 0.5622, val loss: 0.9418, val acc: 0.5730  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 21960] train loss: 0.9551, train acc: 0.5620, val loss: 0.9410, val acc: 0.5656  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 21980] train loss: 0.9582, train acc: 0.5606, val loss: 0.9447, val acc: 0.5713  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 22000] train loss: 0.9567, train acc: 0.5577, val loss: 0.9423, val acc: 0.5703  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 22020] train loss: 0.9580, train acc: 0.5643, val loss: 0.9434, val acc: 0.5754  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 22040] train loss: 0.9620, train acc: 0.5663, val loss: 0.9426, val acc: 0.5686  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 22060] train loss: 0.9566, train acc: 0.5648, val loss: 0.9420, val acc: 0.5747  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 22080] train loss: 0.9554, train acc: 0.5591, val loss: 0.9437, val acc: 0.5669  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 22100] train loss: 0.9583, train acc: 0.5651, val loss: 0.9419, val acc: 0.5686  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 22120] train loss: 0.9547, train acc: 0.5622, val loss: 0.9429, val acc: 0.5757  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 22140] train loss: 0.9597, train acc: 0.5619, val loss: 0.9417, val acc: 0.5609  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 22160] train loss: 0.9570, train acc: 0.5695, val loss: 0.9418, val acc: 0.5636  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 22180] train loss: 0.9572, train acc: 0.5652, val loss: 0.9429, val acc: 0.5690  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 22200] train loss: 0.9566, train acc: 0.5650, val loss: 0.9416, val acc: 0.5700  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 22220] train loss: 0.9571, train acc: 0.5654, val loss: 0.9411, val acc: 0.5676  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 22240] train loss: 0.9611, train acc: 0.5625, val loss: 0.9398, val acc: 0.5666  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 22260] train loss: 0.9611, train acc: 0.5734, val loss: 0.9449, val acc: 0.5605  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 22280] train loss: 0.9648, train acc: 0.5513, val loss: 0.9515, val acc: 0.5740  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 22300] train loss: 0.9572, train acc: 0.5651, val loss: 0.9478, val acc: 0.5730  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 22320] train loss: 0.9605, train acc: 0.5605, val loss: 0.9439, val acc: 0.5703  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 22340] train loss: 0.9635, train acc: 0.5615, val loss: 0.9411, val acc: 0.5750  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 22360] train loss: 0.9540, train acc: 0.5593, val loss: 0.9431, val acc: 0.5659  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 22380] train loss: 0.9537, train acc: 0.5648, val loss: 0.9424, val acc: 0.5700  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 22400] train loss: 0.9584, train acc: 0.5648, val loss: 0.9410, val acc: 0.5619  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 22420] train loss: 0.9583, train acc: 0.5655, val loss: 0.9410, val acc: 0.5642  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 22440] train loss: 0.9564, train acc: 0.5658, val loss: 0.9455, val acc: 0.5686  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 22460] train loss: 0.9533, train acc: 0.5654, val loss: 0.9417, val acc: 0.5747  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 22480] train loss: 0.9575, train acc: 0.5661, val loss: 0.9410, val acc: 0.5717  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 22500] train loss: 0.9665, train acc: 0.5424, val loss: 0.9435, val acc: 0.5717  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 22520] train loss: 0.9592, train acc: 0.5642, val loss: 0.9403, val acc: 0.5676  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 22540] train loss: 0.9546, train acc: 0.5677, val loss: 0.9416, val acc: 0.5619  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 22560] train loss: 0.9579, train acc: 0.5734, val loss: 0.9398, val acc: 0.5642  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 22580] train loss: 0.9534, train acc: 0.5632, val loss: 0.9378, val acc: 0.5690  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 22600] train loss: 0.9580, train acc: 0.5527, val loss: 0.9492, val acc: 0.5636  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 22620] train loss: 0.9558, train acc: 0.5616, val loss: 0.9410, val acc: 0.5737  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 22640] train loss: 0.9571, train acc: 0.5609, val loss: 0.9423, val acc: 0.5723  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 22660] train loss: 0.9574, train acc: 0.5605, val loss: 0.9448, val acc: 0.5659  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 22680] train loss: 0.9634, train acc: 0.5505, val loss: 0.9433, val acc: 0.5693  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 22700] train loss: 0.9615, train acc: 0.5683, val loss: 0.9442, val acc: 0.5700  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 22720] train loss: 0.9536, train acc: 0.5706, val loss: 0.9400, val acc: 0.5693  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 22740] train loss: 0.9530, train acc: 0.5675, val loss: 0.9456, val acc: 0.5676  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 22760] train loss: 0.9584, train acc: 0.5630, val loss: 0.9495, val acc: 0.5639  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 22780] train loss: 0.9559, train acc: 0.5608, val loss: 0.9412, val acc: 0.5740  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 22800] train loss: 0.9628, train acc: 0.5535, val loss: 0.9510, val acc: 0.5639  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 22820] train loss: 0.9553, train acc: 0.5659, val loss: 0.9390, val acc: 0.5710  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 22840] train loss: 0.9600, train acc: 0.5627, val loss: 0.9438, val acc: 0.5754  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 22860] train loss: 0.9586, train acc: 0.5653, val loss: 0.9442, val acc: 0.5774  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 22880] train loss: 0.9591, train acc: 0.5611, val loss: 0.9390, val acc: 0.5636  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 22900] train loss: 0.9598, train acc: 0.5663, val loss: 0.9408, val acc: 0.5680  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 22920] train loss: 0.9567, train acc: 0.5642, val loss: 0.9516, val acc: 0.5629  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 22940] train loss: 0.9570, train acc: 0.5598, val loss: 0.9433, val acc: 0.5663  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 22960] train loss: 0.9602, train acc: 0.5617, val loss: 0.9409, val acc: 0.5666  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 22980] train loss: 0.9568, train acc: 0.5654, val loss: 0.9453, val acc: 0.5703  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 23000] train loss: 0.9578, train acc: 0.5620, val loss: 0.9434, val acc: 0.5734  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 23020] train loss: 0.9633, train acc: 0.5653, val loss: 0.9408, val acc: 0.5740  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 23040] train loss: 0.9563, train acc: 0.5651, val loss: 0.9402, val acc: 0.5683  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 23060] train loss: 0.9573, train acc: 0.5592, val loss: 0.9410, val acc: 0.5676  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 23080] train loss: 0.9548, train acc: 0.5591, val loss: 0.9408, val acc: 0.5696  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 23100] train loss: 0.9578, train acc: 0.5651, val loss: 0.9447, val acc: 0.5693  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 23120] train loss: 0.9561, train acc: 0.5657, val loss: 0.9391, val acc: 0.5740  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 23140] train loss: 0.9573, train acc: 0.5613, val loss: 0.9407, val acc: 0.5750  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 23160] train loss: 0.9573, train acc: 0.5594, val loss: 0.9409, val acc: 0.5642  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 23180] train loss: 0.9567, train acc: 0.5558, val loss: 0.9395, val acc: 0.5794  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 23200] train loss: 0.9557, train acc: 0.5691, val loss: 0.9412, val acc: 0.5720  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 23220] train loss: 0.9593, train acc: 0.5636, val loss: 0.9391, val acc: 0.5629  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 23240] train loss: 0.9562, train acc: 0.5610, val loss: 0.9412, val acc: 0.5707  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 23260] train loss: 0.9578, train acc: 0.5596, val loss: 0.9395, val acc: 0.5673  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 23280] train loss: 0.9531, train acc: 0.5679, val loss: 0.9404, val acc: 0.5690  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 23300] train loss: 0.9572, train acc: 0.5627, val loss: 0.9392, val acc: 0.5707  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 23320] train loss: 0.9580, train acc: 0.5685, val loss: 0.9405, val acc: 0.5700  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 23340] train loss: 0.9605, train acc: 0.5590, val loss: 0.9461, val acc: 0.5656  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 23360] train loss: 0.9552, train acc: 0.5690, val loss: 0.9419, val acc: 0.5656  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 23380] train loss: 0.9551, train acc: 0.5677, val loss: 0.9465, val acc: 0.5750  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 23400] train loss: 0.9665, train acc: 0.5455, val loss: 0.9454, val acc: 0.5680  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 23420] train loss: 0.9668, train acc: 0.5637, val loss: 0.9431, val acc: 0.5808  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 23440] train loss: 0.9534, train acc: 0.5561, val loss: 0.9422, val acc: 0.5686  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 23460] train loss: 0.9633, train acc: 0.5576, val loss: 0.9410, val acc: 0.5666  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 23480] train loss: 0.9665, train acc: 0.5528, val loss: 0.9479, val acc: 0.5626  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 23500] train loss: 0.9562, train acc: 0.5580, val loss: 0.9393, val acc: 0.5632  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 23520] train loss: 0.9541, train acc: 0.5620, val loss: 0.9394, val acc: 0.5683  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 23540] train loss: 0.9578, train acc: 0.5669, val loss: 0.9387, val acc: 0.5595  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 23560] train loss: 0.9544, train acc: 0.5703, val loss: 0.9451, val acc: 0.5653  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 23580] train loss: 0.9574, train acc: 0.5689, val loss: 0.9426, val acc: 0.5690  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 23600] train loss: 0.9524, train acc: 0.5646, val loss: 0.9552, val acc: 0.5619  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 23620] train loss: 0.9521, train acc: 0.5652, val loss: 0.9402, val acc: 0.5696  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 23640] train loss: 0.9532, train acc: 0.5671, val loss: 0.9430, val acc: 0.5683  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 23660] train loss: 0.9537, train acc: 0.5657, val loss: 0.9404, val acc: 0.5653  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 23680] train loss: 0.9559, train acc: 0.5555, val loss: 0.9426, val acc: 0.5676  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 23700] train loss: 0.9603, train acc: 0.5650, val loss: 0.9418, val acc: 0.5693  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 23720] train loss: 0.9574, train acc: 0.5622, val loss: 0.9404, val acc: 0.5663  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 23740] train loss: 0.9532, train acc: 0.5651, val loss: 0.9422, val acc: 0.5723  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 23760] train loss: 0.9552, train acc: 0.5588, val loss: 0.9403, val acc: 0.5791  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 23780] train loss: 0.9627, train acc: 0.5533, val loss: 0.9417, val acc: 0.5727  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 23800] train loss: 0.9566, train acc: 0.5581, val loss: 0.9413, val acc: 0.5680  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 23820] train loss: 0.9532, train acc: 0.5589, val loss: 0.9468, val acc: 0.5666  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 23840] train loss: 0.9549, train acc: 0.5656, val loss: 0.9393, val acc: 0.5811  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 23860] train loss: 0.9565, train acc: 0.5659, val loss: 0.9402, val acc: 0.5761  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 23880] train loss: 0.9607, train acc: 0.5669, val loss: 0.9382, val acc: 0.5622  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 23900] train loss: 0.9530, train acc: 0.5678, val loss: 0.9403, val acc: 0.5663  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 23920] train loss: 0.9555, train acc: 0.5577, val loss: 0.9399, val acc: 0.5656  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 23940] train loss: 0.9560, train acc: 0.5565, val loss: 0.9449, val acc: 0.5673  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 23960] train loss: 0.9556, train acc: 0.5562, val loss: 0.9415, val acc: 0.5666  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 23980] train loss: 0.9533, train acc: 0.5681, val loss: 0.9364, val acc: 0.5700  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 24000] train loss: 0.9574, train acc: 0.5624, val loss: 0.9413, val acc: 0.5629  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 24020] train loss: 0.9540, train acc: 0.5645, val loss: 0.9423, val acc: 0.5663  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 24040] train loss: 0.9559, train acc: 0.5684, val loss: 0.9372, val acc: 0.5669  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 24060] train loss: 0.9557, train acc: 0.5659, val loss: 0.9384, val acc: 0.5693  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 24080] train loss: 0.9546, train acc: 0.5656, val loss: 0.9379, val acc: 0.5680  (best train acc: 0.5748, best val acc: 0.5831)\n",
      "[Epoch: 24100] train loss: 0.9536, train acc: 0.5603, val loss: 0.9320, val acc: 0.5821  (best train acc: 0.5748, best val acc: 0.5848)\n",
      "[Epoch: 24120] train loss: 0.9642, train acc: 0.5498, val loss: 0.9284, val acc: 0.5740  (best train acc: 0.5748, best val acc: 0.5848)\n",
      "[Epoch: 24140] train loss: 0.9581, train acc: 0.5541, val loss: 0.9255, val acc: 0.5693  (best train acc: 0.5748, best val acc: 0.5848)\n",
      "[Epoch: 24160] train loss: 0.9532, train acc: 0.5706, val loss: 0.9239, val acc: 0.5723  (best train acc: 0.5748, best val acc: 0.5848)\n",
      "[Epoch: 24180] train loss: 0.9525, train acc: 0.5622, val loss: 0.9265, val acc: 0.5774  (best train acc: 0.5748, best val acc: 0.5868)\n",
      "[Epoch: 24200] train loss: 0.9591, train acc: 0.5588, val loss: 0.9277, val acc: 0.5696  (best train acc: 0.5748, best val acc: 0.5868)\n",
      "[Epoch: 24220] train loss: 0.9567, train acc: 0.5677, val loss: 0.9245, val acc: 0.5798  (best train acc: 0.5748, best val acc: 0.5868)\n",
      "[Epoch: 24240] train loss: 0.9560, train acc: 0.5621, val loss: 0.9298, val acc: 0.5747  (best train acc: 0.5748, best val acc: 0.5868)\n",
      "[Epoch: 24260] train loss: 0.9568, train acc: 0.5582, val loss: 0.9333, val acc: 0.5730  (best train acc: 0.5748, best val acc: 0.5868)\n",
      "[Epoch: 24280] train loss: 0.9512, train acc: 0.5652, val loss: 0.9327, val acc: 0.5730  (best train acc: 0.5748, best val acc: 0.5868)\n",
      "[Epoch: 24300] train loss: 0.9519, train acc: 0.5655, val loss: 0.9383, val acc: 0.5680  (best train acc: 0.5748, best val acc: 0.5868)\n",
      "[Epoch: 24320] train loss: 0.9556, train acc: 0.5651, val loss: 0.9358, val acc: 0.5707  (best train acc: 0.5748, best val acc: 0.5868)\n",
      "[Epoch: 24340] train loss: 0.9547, train acc: 0.5559, val loss: 0.9322, val acc: 0.5663  (best train acc: 0.5748, best val acc: 0.5868)\n",
      "[Epoch: 24360] train loss: 0.9535, train acc: 0.5688, val loss: 0.9410, val acc: 0.5669  (best train acc: 0.5748, best val acc: 0.5868)\n",
      "[Epoch: 24380] train loss: 0.9569, train acc: 0.5693, val loss: 0.9298, val acc: 0.5771  (best train acc: 0.5748, best val acc: 0.5868)\n",
      "[Epoch: 24400] train loss: 0.9524, train acc: 0.5583, val loss: 0.9353, val acc: 0.5686  (best train acc: 0.5748, best val acc: 0.5868)\n",
      "[Epoch: 24420] train loss: 0.9552, train acc: 0.5658, val loss: 0.9372, val acc: 0.5727  (best train acc: 0.5748, best val acc: 0.5868)\n",
      "[Epoch: 24440] train loss: 0.9535, train acc: 0.5639, val loss: 0.9413, val acc: 0.5673  (best train acc: 0.5748, best val acc: 0.5868)\n",
      "[Epoch: 24460] train loss: 0.9564, train acc: 0.5566, val loss: 0.9299, val acc: 0.5720  (best train acc: 0.5748, best val acc: 0.5868)\n",
      "[Epoch: 24480] train loss: 0.9583, train acc: 0.5645, val loss: 0.9362, val acc: 0.5754  (best train acc: 0.5748, best val acc: 0.5868)\n",
      "[Epoch: 24500] train loss: 0.9552, train acc: 0.5607, val loss: 0.9413, val acc: 0.5750  (best train acc: 0.5748, best val acc: 0.5868)\n",
      "[Epoch: 24520] train loss: 0.9502, train acc: 0.5594, val loss: 0.9386, val acc: 0.5669  (best train acc: 0.5748, best val acc: 0.5868)\n",
      "[Epoch: 24540] train loss: 0.9507, train acc: 0.5685, val loss: 0.9293, val acc: 0.5696  (best train acc: 0.5748, best val acc: 0.5868)\n",
      "[Epoch: 24560] train loss: 0.9565, train acc: 0.5597, val loss: 0.9334, val acc: 0.5737  (best train acc: 0.5748, best val acc: 0.5868)\n",
      "[Epoch: 24580] train loss: 0.9549, train acc: 0.5615, val loss: 0.9392, val acc: 0.5696  (best train acc: 0.5748, best val acc: 0.5868)\n",
      "[Epoch: 24600] train loss: 0.9537, train acc: 0.5676, val loss: 0.9322, val acc: 0.5680  (best train acc: 0.5748, best val acc: 0.5868)\n",
      "[Epoch: 24620] train loss: 0.9568, train acc: 0.5616, val loss: 0.9492, val acc: 0.5609  (best train acc: 0.5748, best val acc: 0.5868)\n",
      "[Epoch: 24640] train loss: 0.9514, train acc: 0.5711, val loss: 0.9297, val acc: 0.5747  (best train acc: 0.5748, best val acc: 0.5868)\n",
      "[Epoch: 24660] train loss: 0.9536, train acc: 0.5633, val loss: 0.9434, val acc: 0.5626  (best train acc: 0.5748, best val acc: 0.5868)\n",
      "[Epoch: 24680] train loss: 0.9562, train acc: 0.5676, val loss: 0.9354, val acc: 0.5747  (best train acc: 0.5748, best val acc: 0.5868)\n",
      "[Epoch: 24700] train loss: 0.9516, train acc: 0.5633, val loss: 0.9310, val acc: 0.5811  (best train acc: 0.5748, best val acc: 0.5868)\n",
      "[Epoch: 24720] train loss: 0.9516, train acc: 0.5701, val loss: 0.9363, val acc: 0.5710  (best train acc: 0.5748, best val acc: 0.5868)\n",
      "[Epoch: 24740] train loss: 0.9487, train acc: 0.5677, val loss: 0.9312, val acc: 0.5774  (best train acc: 0.5748, best val acc: 0.5868)\n",
      "[Epoch: 24760] train loss: 0.9502, train acc: 0.5641, val loss: 0.9319, val acc: 0.5717  (best train acc: 0.5748, best val acc: 0.5868)\n",
      "[Epoch: 24780] train loss: 0.9506, train acc: 0.5653, val loss: 0.9323, val acc: 0.5757  (best train acc: 0.5748, best val acc: 0.5868)\n",
      "[Epoch: 24800] train loss: 0.9546, train acc: 0.5626, val loss: 0.9314, val acc: 0.5750  (best train acc: 0.5748, best val acc: 0.5868)\n",
      "[Epoch: 24820] train loss: 0.9521, train acc: 0.5716, val loss: 0.9289, val acc: 0.5700  (best train acc: 0.5748, best val acc: 0.5868)\n",
      "[Epoch: 24840] train loss: 0.9549, train acc: 0.5599, val loss: 0.9301, val acc: 0.5781  (best train acc: 0.5748, best val acc: 0.5868)\n",
      "[Epoch: 24860] train loss: 0.9535, train acc: 0.5699, val loss: 0.9317, val acc: 0.5808  (best train acc: 0.5748, best val acc: 0.5868)\n",
      "[Epoch: 24880] train loss: 0.9514, train acc: 0.5688, val loss: 0.9293, val acc: 0.5794  (best train acc: 0.5748, best val acc: 0.5868)\n",
      "[Epoch: 24900] train loss: 0.9522, train acc: 0.5710, val loss: 0.9354, val acc: 0.5754  (best train acc: 0.5748, best val acc: 0.5868)\n",
      "[Epoch: 24920] train loss: 0.9548, train acc: 0.5626, val loss: 0.9305, val acc: 0.5710  (best train acc: 0.5748, best val acc: 0.5868)\n",
      "[Epoch: 24940] train loss: 0.9508, train acc: 0.5662, val loss: 0.9318, val acc: 0.5777  (best train acc: 0.5748, best val acc: 0.5868)\n",
      "[Epoch: 24960] train loss: 0.9498, train acc: 0.5664, val loss: 0.9300, val acc: 0.5747  (best train acc: 0.5748, best val acc: 0.5868)\n",
      "[Epoch: 24980] train loss: 0.9547, train acc: 0.5667, val loss: 0.9330, val acc: 0.5794  (best train acc: 0.5748, best val acc: 0.5868)\n",
      "[Epoch: 25000] train loss: 0.9532, train acc: 0.5661, val loss: 0.9331, val acc: 0.5680  (best train acc: 0.5748, best val acc: 0.5868)\n",
      "[Epoch: 25020] train loss: 0.9452, train acc: 0.5631, val loss: 0.9295, val acc: 0.5696  (best train acc: 0.5748, best val acc: 0.5868)\n",
      "[Epoch: 25040] train loss: 0.9519, train acc: 0.5628, val loss: 0.9332, val acc: 0.5717  (best train acc: 0.5748, best val acc: 0.5868)\n",
      "[Epoch: 25060] train loss: 0.9531, train acc: 0.5660, val loss: 0.9349, val acc: 0.5784  (best train acc: 0.5748, best val acc: 0.5868)\n",
      "[Epoch: 25080] train loss: 0.9524, train acc: 0.5677, val loss: 0.9304, val acc: 0.5723  (best train acc: 0.5748, best val acc: 0.5868)\n",
      "[Epoch: 25100] train loss: 0.9569, train acc: 0.5656, val loss: 0.9338, val acc: 0.5730  (best train acc: 0.5748, best val acc: 0.5868)\n",
      "[Epoch: 25120] train loss: 0.9563, train acc: 0.5595, val loss: 0.9326, val acc: 0.5720  (best train acc: 0.5748, best val acc: 0.5868)\n",
      "[Epoch: 25140] train loss: 0.9664, train acc: 0.5680, val loss: 0.9321, val acc: 0.5734  (best train acc: 0.5748, best val acc: 0.5868)\n",
      "[Epoch: 25160] train loss: 0.9565, train acc: 0.5513, val loss: 0.9368, val acc: 0.5734  (best train acc: 0.5748, best val acc: 0.5868)\n",
      "[Epoch: 25180] train loss: 0.9437, train acc: 0.5679, val loss: 0.9301, val acc: 0.5784  (best train acc: 0.5748, best val acc: 0.5868)\n",
      "[Epoch: 25200] train loss: 0.9546, train acc: 0.5591, val loss: 0.9321, val acc: 0.5747  (best train acc: 0.5748, best val acc: 0.5868)\n",
      "[Epoch: 25220] train loss: 0.9539, train acc: 0.5675, val loss: 0.9295, val acc: 0.5788  (best train acc: 0.5748, best val acc: 0.5868)\n",
      "[Epoch: 25240] train loss: 0.9518, train acc: 0.5690, val loss: 0.9309, val acc: 0.5750  (best train acc: 0.5748, best val acc: 0.5868)\n",
      "[Epoch: 25260] train loss: 0.9507, train acc: 0.5638, val loss: 0.9274, val acc: 0.5707  (best train acc: 0.5748, best val acc: 0.5868)\n",
      "[Epoch: 25280] train loss: 0.9506, train acc: 0.5631, val loss: 0.9345, val acc: 0.5717  (best train acc: 0.5748, best val acc: 0.5868)\n",
      "[Epoch: 25300] train loss: 0.9516, train acc: 0.5620, val loss: 0.9343, val acc: 0.5750  (best train acc: 0.5748, best val acc: 0.5868)\n",
      "[Epoch: 25320] train loss: 0.9499, train acc: 0.5634, val loss: 0.9359, val acc: 0.5740  (best train acc: 0.5748, best val acc: 0.5868)\n",
      "[Epoch: 25340] train loss: 0.9512, train acc: 0.5590, val loss: 0.9291, val acc: 0.5700  (best train acc: 0.5748, best val acc: 0.5868)\n",
      "[Epoch: 25360] train loss: 0.9505, train acc: 0.5668, val loss: 0.9288, val acc: 0.5781  (best train acc: 0.5748, best val acc: 0.5868)\n",
      "[Epoch: 25380] train loss: 0.9484, train acc: 0.5693, val loss: 0.9276, val acc: 0.5710  (best train acc: 0.5748, best val acc: 0.5868)\n",
      "[Epoch: 25400] train loss: 0.9528, train acc: 0.5680, val loss: 0.9288, val acc: 0.5737  (best train acc: 0.5748, best val acc: 0.5868)\n",
      "[Epoch: 25420] train loss: 0.9481, train acc: 0.5641, val loss: 0.9299, val acc: 0.5761  (best train acc: 0.5748, best val acc: 0.5868)\n",
      "[Epoch: 25440] train loss: 0.9544, train acc: 0.5603, val loss: 0.9300, val acc: 0.5750  (best train acc: 0.5748, best val acc: 0.5868)\n",
      "[Epoch: 25460] train loss: 0.9514, train acc: 0.5693, val loss: 0.9305, val acc: 0.5754  (best train acc: 0.5748, best val acc: 0.5868)\n",
      "[Epoch: 25480] train loss: 0.9483, train acc: 0.5685, val loss: 0.9284, val acc: 0.5744  (best train acc: 0.5748, best val acc: 0.5868)\n",
      "[Epoch: 25500] train loss: 0.9519, train acc: 0.5686, val loss: 0.9315, val acc: 0.5730  (best train acc: 0.5748, best val acc: 0.5868)\n",
      "[Epoch: 25520] train loss: 0.9513, train acc: 0.5640, val loss: 0.9273, val acc: 0.5761  (best train acc: 0.5748, best val acc: 0.5868)\n",
      "[Epoch: 25540] train loss: 0.9511, train acc: 0.5697, val loss: 0.9275, val acc: 0.5784  (best train acc: 0.5748, best val acc: 0.5868)\n",
      "[Epoch: 25560] train loss: 0.9592, train acc: 0.5519, val loss: 0.9275, val acc: 0.5690  (best train acc: 0.5753, best val acc: 0.5868)\n",
      "[Epoch: 25580] train loss: 0.9591, train acc: 0.5642, val loss: 0.9330, val acc: 0.5653  (best train acc: 0.5753, best val acc: 0.5868)\n",
      "[Epoch: 25600] train loss: 0.9565, train acc: 0.5677, val loss: 0.9329, val acc: 0.5777  (best train acc: 0.5753, best val acc: 0.5868)\n",
      "[Epoch: 25620] train loss: 0.9556, train acc: 0.5551, val loss: 0.9445, val acc: 0.5642  (best train acc: 0.5753, best val acc: 0.5868)\n",
      "[Epoch: 25640] train loss: 0.9554, train acc: 0.5563, val loss: 0.9451, val acc: 0.5663  (best train acc: 0.5753, best val acc: 0.5868)\n",
      "[Epoch: 25660] train loss: 0.9518, train acc: 0.5607, val loss: 0.9324, val acc: 0.5774  (best train acc: 0.5753, best val acc: 0.5868)\n",
      "[Epoch: 25680] train loss: 0.9527, train acc: 0.5661, val loss: 0.9338, val acc: 0.5781  (best train acc: 0.5753, best val acc: 0.5868)\n",
      "[Epoch: 25700] train loss: 0.9520, train acc: 0.5630, val loss: 0.9311, val acc: 0.5771  (best train acc: 0.5753, best val acc: 0.5868)\n",
      "[Epoch: 25720] train loss: 0.9522, train acc: 0.5727, val loss: 0.9305, val acc: 0.5821  (best train acc: 0.5753, best val acc: 0.5868)\n",
      "[Epoch: 25740] train loss: 0.9487, train acc: 0.5630, val loss: 0.9327, val acc: 0.5727  (best train acc: 0.5753, best val acc: 0.5868)\n",
      "[Epoch: 25760] train loss: 0.9506, train acc: 0.5640, val loss: 0.9273, val acc: 0.5734  (best train acc: 0.5753, best val acc: 0.5868)\n",
      "[Epoch: 25780] train loss: 0.9513, train acc: 0.5588, val loss: 0.9315, val acc: 0.5747  (best train acc: 0.5753, best val acc: 0.5868)\n",
      "[Epoch: 25800] train loss: 0.9548, train acc: 0.5584, val loss: 0.9254, val acc: 0.5734  (best train acc: 0.5753, best val acc: 0.5868)\n",
      "[Epoch: 25820] train loss: 0.9543, train acc: 0.5630, val loss: 0.9251, val acc: 0.5696  (best train acc: 0.5753, best val acc: 0.5868)\n",
      "[Epoch: 25840] train loss: 0.9515, train acc: 0.5598, val loss: 0.9406, val acc: 0.5663  (best train acc: 0.5753, best val acc: 0.5868)\n",
      "[Epoch: 25860] train loss: 0.9531, train acc: 0.5609, val loss: 0.9264, val acc: 0.5828  (best train acc: 0.5753, best val acc: 0.5868)\n",
      "[Epoch: 25880] train loss: 0.9546, train acc: 0.5707, val loss: 0.9268, val acc: 0.5744  (best train acc: 0.5753, best val acc: 0.5868)\n",
      "[Epoch: 25900] train loss: 0.9497, train acc: 0.5690, val loss: 0.9263, val acc: 0.5727  (best train acc: 0.5753, best val acc: 0.5868)\n",
      "[Epoch: 25920] train loss: 0.9523, train acc: 0.5647, val loss: 0.9298, val acc: 0.5767  (best train acc: 0.5753, best val acc: 0.5868)\n",
      "[Epoch: 25940] train loss: 0.9503, train acc: 0.5630, val loss: 0.9249, val acc: 0.5771  (best train acc: 0.5753, best val acc: 0.5875)\n",
      "[Epoch: 25960] train loss: 0.9461, train acc: 0.5687, val loss: 0.9251, val acc: 0.5767  (best train acc: 0.5753, best val acc: 0.5875)\n",
      "[Epoch: 25980] train loss: 0.9479, train acc: 0.5664, val loss: 0.9361, val acc: 0.5781  (best train acc: 0.5753, best val acc: 0.5875)\n",
      "[Epoch: 26000] train loss: 0.9594, train acc: 0.5521, val loss: 0.9335, val acc: 0.5720  (best train acc: 0.5753, best val acc: 0.5875)\n",
      "[Epoch: 26020] train loss: 0.9555, train acc: 0.5617, val loss: 0.9403, val acc: 0.5710  (best train acc: 0.5753, best val acc: 0.5875)\n",
      "[Epoch: 26040] train loss: 0.9613, train acc: 0.5506, val loss: 0.9257, val acc: 0.5761  (best train acc: 0.5753, best val acc: 0.5875)\n",
      "[Epoch: 26060] train loss: 0.9502, train acc: 0.5706, val loss: 0.9279, val acc: 0.5710  (best train acc: 0.5753, best val acc: 0.5875)\n",
      "[Epoch: 26080] train loss: 0.9563, train acc: 0.5569, val loss: 0.9281, val acc: 0.5737  (best train acc: 0.5753, best val acc: 0.5875)\n",
      "[Epoch: 26100] train loss: 0.9476, train acc: 0.5670, val loss: 0.9269, val acc: 0.5774  (best train acc: 0.5753, best val acc: 0.5875)\n",
      "[Epoch: 26120] train loss: 0.9590, train acc: 0.5544, val loss: 0.9283, val acc: 0.5669  (best train acc: 0.5753, best val acc: 0.5875)\n",
      "[Epoch: 26140] train loss: 0.9547, train acc: 0.5617, val loss: 0.9290, val acc: 0.5737  (best train acc: 0.5753, best val acc: 0.5875)\n",
      "[Epoch: 26160] train loss: 0.9543, train acc: 0.5671, val loss: 0.9286, val acc: 0.5717  (best train acc: 0.5753, best val acc: 0.5875)\n",
      "[Epoch: 26180] train loss: 0.9511, train acc: 0.5674, val loss: 0.9281, val acc: 0.5747  (best train acc: 0.5753, best val acc: 0.5875)\n",
      "[Epoch: 26200] train loss: 0.9485, train acc: 0.5635, val loss: 0.9342, val acc: 0.5710  (best train acc: 0.5753, best val acc: 0.5875)\n",
      "[Epoch: 26220] train loss: 0.9510, train acc: 0.5547, val loss: 0.9314, val acc: 0.5710  (best train acc: 0.5753, best val acc: 0.5875)\n",
      "[Epoch: 26240] train loss: 0.9478, train acc: 0.5683, val loss: 0.9243, val acc: 0.5784  (best train acc: 0.5753, best val acc: 0.5875)\n",
      "[Epoch: 26260] train loss: 0.9513, train acc: 0.5706, val loss: 0.9288, val acc: 0.5757  (best train acc: 0.5753, best val acc: 0.5875)\n",
      "[Epoch: 26280] train loss: 0.9519, train acc: 0.5662, val loss: 0.9296, val acc: 0.5794  (best train acc: 0.5753, best val acc: 0.5875)\n",
      "[Epoch: 26300] train loss: 0.9540, train acc: 0.5552, val loss: 0.9311, val acc: 0.5791  (best train acc: 0.5753, best val acc: 0.5875)\n",
      "[Epoch: 26320] train loss: 0.9488, train acc: 0.5704, val loss: 0.9242, val acc: 0.5774  (best train acc: 0.5753, best val acc: 0.5875)\n",
      "[Epoch: 26340] train loss: 0.9502, train acc: 0.5697, val loss: 0.9281, val acc: 0.5791  (best train acc: 0.5753, best val acc: 0.5875)\n",
      "[Epoch: 26360] train loss: 0.9504, train acc: 0.5605, val loss: 0.9292, val acc: 0.5750  (best train acc: 0.5753, best val acc: 0.5875)\n",
      "[Epoch: 26380] train loss: 0.9562, train acc: 0.5596, val loss: 0.9268, val acc: 0.5767  (best train acc: 0.5753, best val acc: 0.5875)\n",
      "[Epoch: 26400] train loss: 0.9530, train acc: 0.5698, val loss: 0.9324, val acc: 0.5626  (best train acc: 0.5753, best val acc: 0.5875)\n",
      "[Epoch: 26420] train loss: 0.9503, train acc: 0.5657, val loss: 0.9304, val acc: 0.5818  (best train acc: 0.5753, best val acc: 0.5875)\n",
      "[Epoch: 26440] train loss: 0.9505, train acc: 0.5712, val loss: 0.9301, val acc: 0.5838  (best train acc: 0.5753, best val acc: 0.5875)\n",
      "[Epoch: 26460] train loss: 0.9546, train acc: 0.5724, val loss: 0.9273, val acc: 0.5757  (best train acc: 0.5753, best val acc: 0.5875)\n",
      "[Epoch: 26480] train loss: 0.9531, train acc: 0.5690, val loss: 0.9302, val acc: 0.5777  (best train acc: 0.5753, best val acc: 0.5875)\n",
      "[Epoch: 26500] train loss: 0.9477, train acc: 0.5588, val loss: 0.9308, val acc: 0.5703  (best train acc: 0.5753, best val acc: 0.5875)\n",
      "[Epoch: 26520] train loss: 0.9570, train acc: 0.5646, val loss: 0.9393, val acc: 0.5605  (best train acc: 0.5753, best val acc: 0.5875)\n",
      "[Epoch: 26540] train loss: 0.9536, train acc: 0.5581, val loss: 0.9316, val acc: 0.5730  (best train acc: 0.5753, best val acc: 0.5875)\n",
      "[Epoch: 26560] train loss: 0.9537, train acc: 0.5669, val loss: 0.9295, val acc: 0.5791  (best train acc: 0.5753, best val acc: 0.5875)\n",
      "[Epoch: 26580] train loss: 0.9532, train acc: 0.5662, val loss: 0.9267, val acc: 0.5713  (best train acc: 0.5753, best val acc: 0.5875)\n",
      "[Epoch: 26600] train loss: 0.9473, train acc: 0.5666, val loss: 0.9268, val acc: 0.5771  (best train acc: 0.5753, best val acc: 0.5875)\n",
      "[Epoch: 26620] train loss: 0.9498, train acc: 0.5713, val loss: 0.9254, val acc: 0.5754  (best train acc: 0.5753, best val acc: 0.5875)\n",
      "[Epoch: 26640] train loss: 0.9565, train acc: 0.5588, val loss: 0.9465, val acc: 0.5639  (best train acc: 0.5753, best val acc: 0.5875)\n",
      "[Epoch: 26660] train loss: 0.9521, train acc: 0.5593, val loss: 0.9297, val acc: 0.5781  (best train acc: 0.5753, best val acc: 0.5875)\n",
      "[Epoch: 26680] train loss: 0.9516, train acc: 0.5622, val loss: 0.9276, val acc: 0.5831  (best train acc: 0.5753, best val acc: 0.5875)\n",
      "[Epoch: 26700] train loss: 0.9475, train acc: 0.5689, val loss: 0.9278, val acc: 0.5703  (best train acc: 0.5753, best val acc: 0.5875)\n",
      "[Epoch: 26720] train loss: 0.9496, train acc: 0.5647, val loss: 0.9278, val acc: 0.5771  (best train acc: 0.5753, best val acc: 0.5875)\n",
      "[Epoch: 26740] train loss: 0.9514, train acc: 0.5669, val loss: 0.9269, val acc: 0.5717  (best train acc: 0.5753, best val acc: 0.5875)\n",
      "[Epoch: 26760] train loss: 0.9514, train acc: 0.5637, val loss: 0.9238, val acc: 0.5798  (best train acc: 0.5753, best val acc: 0.5875)\n",
      "[Epoch: 26780] train loss: 0.9489, train acc: 0.5661, val loss: 0.9262, val acc: 0.5774  (best train acc: 0.5753, best val acc: 0.5875)\n",
      "[Epoch: 26800] train loss: 0.9512, train acc: 0.5644, val loss: 0.9284, val acc: 0.5723  (best train acc: 0.5753, best val acc: 0.5875)\n",
      "[Epoch: 26820] train loss: 0.9511, train acc: 0.5623, val loss: 0.9253, val acc: 0.5761  (best train acc: 0.5753, best val acc: 0.5875)\n",
      "[Epoch: 26840] train loss: 0.9545, train acc: 0.5590, val loss: 0.9358, val acc: 0.5656  (best train acc: 0.5753, best val acc: 0.5879)\n",
      "[Epoch: 26860] train loss: 0.9684, train acc: 0.5378, val loss: 0.9563, val acc: 0.5592  (best train acc: 0.5753, best val acc: 0.5879)\n",
      "[Epoch: 26880] train loss: 0.9509, train acc: 0.5680, val loss: 0.9242, val acc: 0.5700  (best train acc: 0.5753, best val acc: 0.5879)\n",
      "[Epoch: 26900] train loss: 0.9589, train acc: 0.5627, val loss: 0.9262, val acc: 0.5690  (best train acc: 0.5753, best val acc: 0.5879)\n",
      "[Epoch: 26920] train loss: 0.9559, train acc: 0.5610, val loss: 0.9297, val acc: 0.5784  (best train acc: 0.5753, best val acc: 0.5879)\n",
      "[Epoch: 26940] train loss: 0.9488, train acc: 0.5686, val loss: 0.9260, val acc: 0.5808  (best train acc: 0.5753, best val acc: 0.5879)\n",
      "[Epoch: 26960] train loss: 0.9519, train acc: 0.5650, val loss: 0.9256, val acc: 0.5767  (best train acc: 0.5753, best val acc: 0.5879)\n",
      "[Epoch: 26980] train loss: 0.9536, train acc: 0.5613, val loss: 0.9247, val acc: 0.5767  (best train acc: 0.5753, best val acc: 0.5879)\n",
      "[Epoch: 27000] train loss: 0.9578, train acc: 0.5505, val loss: 0.9250, val acc: 0.5788  (best train acc: 0.5753, best val acc: 0.5879)\n",
      "[Epoch: 27020] train loss: 0.9471, train acc: 0.5661, val loss: 0.9304, val acc: 0.5730  (best train acc: 0.5753, best val acc: 0.5879)\n",
      "[Epoch: 27040] train loss: 0.9517, train acc: 0.5632, val loss: 0.9257, val acc: 0.5707  (best train acc: 0.5753, best val acc: 0.5879)\n",
      "[Epoch: 27060] train loss: 0.9509, train acc: 0.5658, val loss: 0.9303, val acc: 0.5781  (best train acc: 0.5753, best val acc: 0.5879)\n",
      "[Epoch: 27080] train loss: 0.9542, train acc: 0.5655, val loss: 0.9235, val acc: 0.5784  (best train acc: 0.5753, best val acc: 0.5879)\n",
      "[Epoch: 27100] train loss: 0.9477, train acc: 0.5675, val loss: 0.9231, val acc: 0.5771  (best train acc: 0.5753, best val acc: 0.5879)\n",
      "[Epoch: 27120] train loss: 0.9565, train acc: 0.5541, val loss: 0.9519, val acc: 0.5642  (best train acc: 0.5753, best val acc: 0.5879)\n",
      "[Epoch: 27140] train loss: 0.9489, train acc: 0.5665, val loss: 0.9279, val acc: 0.5740  (best train acc: 0.5753, best val acc: 0.5879)\n",
      "[Epoch: 27160] train loss: 0.9452, train acc: 0.5667, val loss: 0.9267, val acc: 0.5781  (best train acc: 0.5753, best val acc: 0.5879)\n",
      "[Epoch: 27180] train loss: 0.9553, train acc: 0.5650, val loss: 0.9435, val acc: 0.5713  (best train acc: 0.5753, best val acc: 0.5879)\n",
      "[Epoch: 27200] train loss: 0.9501, train acc: 0.5695, val loss: 0.9225, val acc: 0.5794  (best train acc: 0.5753, best val acc: 0.5879)\n",
      "[Epoch: 27220] train loss: 0.9471, train acc: 0.5659, val loss: 0.9282, val acc: 0.5821  (best train acc: 0.5753, best val acc: 0.5882)\n",
      "[Epoch: 27240] train loss: 0.9480, train acc: 0.5672, val loss: 0.9282, val acc: 0.5791  (best train acc: 0.5753, best val acc: 0.5882)\n",
      "[Epoch: 27260] train loss: 0.9471, train acc: 0.5637, val loss: 0.9248, val acc: 0.5734  (best train acc: 0.5753, best val acc: 0.5882)\n",
      "[Epoch: 27280] train loss: 0.9502, train acc: 0.5620, val loss: 0.9249, val acc: 0.5784  (best train acc: 0.5753, best val acc: 0.5882)\n",
      "[Epoch: 27300] train loss: 0.9465, train acc: 0.5717, val loss: 0.9223, val acc: 0.5767  (best train acc: 0.5753, best val acc: 0.5882)\n",
      "[Epoch: 27320] train loss: 0.9529, train acc: 0.5596, val loss: 0.9269, val acc: 0.5828  (best train acc: 0.5753, best val acc: 0.5882)\n",
      "[Epoch: 27340] train loss: 0.9482, train acc: 0.5665, val loss: 0.9222, val acc: 0.5764  (best train acc: 0.5753, best val acc: 0.5882)\n",
      "[Epoch: 27360] train loss: 0.9486, train acc: 0.5677, val loss: 0.9245, val acc: 0.5784  (best train acc: 0.5753, best val acc: 0.5882)\n",
      "[Epoch: 27380] train loss: 0.9501, train acc: 0.5645, val loss: 0.9252, val acc: 0.5764  (best train acc: 0.5753, best val acc: 0.5882)\n",
      "[Epoch: 27400] train loss: 0.9480, train acc: 0.5680, val loss: 0.9261, val acc: 0.5686  (best train acc: 0.5753, best val acc: 0.5882)\n",
      "[Epoch: 27420] train loss: 0.9509, train acc: 0.5667, val loss: 0.9226, val acc: 0.5761  (best train acc: 0.5753, best val acc: 0.5882)\n",
      "[Epoch: 27440] train loss: 0.9504, train acc: 0.5701, val loss: 0.9260, val acc: 0.5821  (best train acc: 0.5753, best val acc: 0.5882)\n",
      "[Epoch: 27460] train loss: 0.9584, train acc: 0.5630, val loss: 0.9242, val acc: 0.5723  (best train acc: 0.5753, best val acc: 0.5882)\n",
      "[Epoch: 27480] train loss: 0.9507, train acc: 0.5606, val loss: 0.9253, val acc: 0.5841  (best train acc: 0.5753, best val acc: 0.5882)\n",
      "[Epoch: 27500] train loss: 0.9539, train acc: 0.5541, val loss: 0.9320, val acc: 0.5784  (best train acc: 0.5753, best val acc: 0.5882)\n",
      "[Epoch: 27520] train loss: 0.9523, train acc: 0.5634, val loss: 0.9339, val acc: 0.5720  (best train acc: 0.5753, best val acc: 0.5882)\n",
      "[Epoch: 27540] train loss: 0.9512, train acc: 0.5591, val loss: 0.9259, val acc: 0.5821  (best train acc: 0.5753, best val acc: 0.5882)\n",
      "[Epoch: 27560] train loss: 0.9483, train acc: 0.5623, val loss: 0.9257, val acc: 0.5801  (best train acc: 0.5753, best val acc: 0.5882)\n",
      "[Epoch: 27580] train loss: 0.9485, train acc: 0.5693, val loss: 0.9242, val acc: 0.5804  (best train acc: 0.5753, best val acc: 0.5882)\n",
      "[Epoch: 27600] train loss: 0.9463, train acc: 0.5720, val loss: 0.9241, val acc: 0.5710  (best train acc: 0.5753, best val acc: 0.5882)\n",
      "[Epoch: 27620] train loss: 0.9533, train acc: 0.5571, val loss: 0.9282, val acc: 0.5794  (best train acc: 0.5753, best val acc: 0.5882)\n",
      "[Epoch: 27640] train loss: 0.9578, train acc: 0.5455, val loss: 0.9314, val acc: 0.5774  (best train acc: 0.5753, best val acc: 0.5882)\n",
      "[Epoch: 27660] train loss: 0.9553, train acc: 0.5539, val loss: 0.9279, val acc: 0.5767  (best train acc: 0.5753, best val acc: 0.5882)\n",
      "[Epoch: 27680] train loss: 0.9491, train acc: 0.5624, val loss: 0.9279, val acc: 0.5757  (best train acc: 0.5753, best val acc: 0.5882)\n",
      "[Epoch: 27700] train loss: 0.9457, train acc: 0.5656, val loss: 0.9269, val acc: 0.5781  (best train acc: 0.5753, best val acc: 0.5882)\n",
      "[Epoch: 27720] train loss: 0.9520, train acc: 0.5628, val loss: 0.9234, val acc: 0.5804  (best train acc: 0.5753, best val acc: 0.5882)\n",
      "[Epoch: 27740] train loss: 0.9519, train acc: 0.5583, val loss: 0.9242, val acc: 0.5804  (best train acc: 0.5753, best val acc: 0.5882)\n",
      "[Epoch: 27760] train loss: 0.9510, train acc: 0.5565, val loss: 0.9338, val acc: 0.5744  (best train acc: 0.5753, best val acc: 0.5882)\n",
      "[Epoch: 27780] train loss: 0.9560, train acc: 0.5624, val loss: 0.9315, val acc: 0.5703  (best train acc: 0.5753, best val acc: 0.5882)\n",
      "[Epoch: 27800] train loss: 0.9493, train acc: 0.5654, val loss: 0.9297, val acc: 0.5582  (best train acc: 0.5753, best val acc: 0.5882)\n",
      "[Epoch: 27820] train loss: 0.9475, train acc: 0.5639, val loss: 0.9322, val acc: 0.5744  (best train acc: 0.5753, best val acc: 0.5882)\n",
      "[Epoch: 27840] train loss: 0.9462, train acc: 0.5675, val loss: 0.9250, val acc: 0.5777  (best train acc: 0.5753, best val acc: 0.5882)\n",
      "[Epoch: 27860] train loss: 0.9469, train acc: 0.5680, val loss: 0.9379, val acc: 0.5676  (best train acc: 0.5753, best val acc: 0.5882)\n",
      "[Epoch: 27880] train loss: 0.9430, train acc: 0.5690, val loss: 0.9219, val acc: 0.5750  (best train acc: 0.5753, best val acc: 0.5882)\n",
      "[Epoch: 27900] train loss: 0.9528, train acc: 0.5664, val loss: 0.9287, val acc: 0.5754  (best train acc: 0.5753, best val acc: 0.5882)\n",
      "[Epoch: 27920] train loss: 0.9546, train acc: 0.5584, val loss: 0.9238, val acc: 0.5865  (best train acc: 0.5753, best val acc: 0.5882)\n",
      "[Epoch: 27940] train loss: 0.9514, train acc: 0.5583, val loss: 0.9265, val acc: 0.5771  (best train acc: 0.5753, best val acc: 0.5882)\n",
      "[Epoch: 27960] train loss: 0.9467, train acc: 0.5600, val loss: 0.9238, val acc: 0.5794  (best train acc: 0.5753, best val acc: 0.5882)\n",
      "[Epoch: 27980] train loss: 0.9458, train acc: 0.5630, val loss: 0.9245, val acc: 0.5777  (best train acc: 0.5753, best val acc: 0.5882)\n",
      "[Epoch: 28000] train loss: 0.9518, train acc: 0.5645, val loss: 0.9222, val acc: 0.5798  (best train acc: 0.5753, best val acc: 0.5882)\n",
      "[Epoch: 28020] train loss: 0.9478, train acc: 0.5604, val loss: 0.9230, val acc: 0.5761  (best train acc: 0.5753, best val acc: 0.5882)\n",
      "[Epoch: 28040] train loss: 0.9451, train acc: 0.5596, val loss: 0.9280, val acc: 0.5774  (best train acc: 0.5753, best val acc: 0.5882)\n",
      "[Epoch: 28060] train loss: 0.9566, train acc: 0.5508, val loss: 0.9270, val acc: 0.5838  (best train acc: 0.5753, best val acc: 0.5882)\n",
      "[Epoch: 28080] train loss: 0.9574, train acc: 0.5680, val loss: 0.9275, val acc: 0.5798  (best train acc: 0.5753, best val acc: 0.5882)\n",
      "[Epoch: 28100] train loss: 0.9447, train acc: 0.5586, val loss: 0.9262, val acc: 0.5767  (best train acc: 0.5753, best val acc: 0.5882)\n",
      "[Epoch: 28120] train loss: 0.9469, train acc: 0.5651, val loss: 0.9254, val acc: 0.5835  (best train acc: 0.5753, best val acc: 0.5882)\n",
      "[Epoch: 28140] train loss: 0.9531, train acc: 0.5585, val loss: 0.9214, val acc: 0.5771  (best train acc: 0.5753, best val acc: 0.5882)\n",
      "[Epoch: 28160] train loss: 0.9502, train acc: 0.5627, val loss: 0.9261, val acc: 0.5794  (best train acc: 0.5753, best val acc: 0.5882)\n",
      "[Epoch: 28180] train loss: 0.9462, train acc: 0.5648, val loss: 0.9247, val acc: 0.5838  (best train acc: 0.5753, best val acc: 0.5882)\n",
      "[Epoch: 28200] train loss: 0.9480, train acc: 0.5682, val loss: 0.9219, val acc: 0.5798  (best train acc: 0.5753, best val acc: 0.5882)\n",
      "[Epoch: 28220] train loss: 0.9482, train acc: 0.5674, val loss: 0.9223, val acc: 0.5845  (best train acc: 0.5753, best val acc: 0.5882)\n",
      "[Epoch: 28240] train loss: 0.9505, train acc: 0.5676, val loss: 0.9216, val acc: 0.5838  (best train acc: 0.5753, best val acc: 0.5882)\n",
      "[Epoch: 28260] train loss: 0.9479, train acc: 0.5646, val loss: 0.9228, val acc: 0.5862  (best train acc: 0.5753, best val acc: 0.5882)\n",
      "[Epoch: 28280] train loss: 0.9486, train acc: 0.5624, val loss: 0.9220, val acc: 0.5761  (best train acc: 0.5753, best val acc: 0.5882)\n",
      "[Epoch: 28300] train loss: 0.9462, train acc: 0.5644, val loss: 0.9227, val acc: 0.5754  (best train acc: 0.5753, best val acc: 0.5892)\n",
      "[Epoch: 28320] train loss: 0.9512, train acc: 0.5628, val loss: 0.9214, val acc: 0.5828  (best train acc: 0.5753, best val acc: 0.5892)\n",
      "[Epoch: 28340] train loss: 0.9456, train acc: 0.5625, val loss: 0.9276, val acc: 0.5791  (best train acc: 0.5753, best val acc: 0.5892)\n",
      "[Epoch: 28360] train loss: 0.9497, train acc: 0.5672, val loss: 0.9217, val acc: 0.5835  (best train acc: 0.5753, best val acc: 0.5892)\n",
      "[Epoch: 28380] train loss: 0.9459, train acc: 0.5619, val loss: 0.9214, val acc: 0.5771  (best train acc: 0.5753, best val acc: 0.5892)\n",
      "[Epoch: 28400] train loss: 0.9524, train acc: 0.5640, val loss: 0.9239, val acc: 0.5754  (best train acc: 0.5753, best val acc: 0.5892)\n",
      "[Epoch: 28420] train loss: 0.9573, train acc: 0.5670, val loss: 0.9277, val acc: 0.5703  (best train acc: 0.5753, best val acc: 0.5892)\n",
      "[Epoch: 28440] train loss: 0.9504, train acc: 0.5604, val loss: 0.9237, val acc: 0.5848  (best train acc: 0.5753, best val acc: 0.5892)\n",
      "[Epoch: 28460] train loss: 0.9540, train acc: 0.5625, val loss: 0.9398, val acc: 0.5747  (best train acc: 0.5753, best val acc: 0.5892)\n",
      "[Epoch: 28480] train loss: 0.9464, train acc: 0.5688, val loss: 0.9237, val acc: 0.5801  (best train acc: 0.5753, best val acc: 0.5892)\n",
      "[Epoch: 28500] train loss: 0.9519, train acc: 0.5655, val loss: 0.9226, val acc: 0.5646  (best train acc: 0.5753, best val acc: 0.5892)\n",
      "[Epoch: 28520] train loss: 0.9458, train acc: 0.5622, val loss: 0.9199, val acc: 0.5821  (best train acc: 0.5753, best val acc: 0.5892)\n",
      "[Epoch: 28540] train loss: 0.9461, train acc: 0.5699, val loss: 0.9221, val acc: 0.5730  (best train acc: 0.5753, best val acc: 0.5892)\n",
      "[Epoch: 28560] train loss: 0.9453, train acc: 0.5638, val loss: 0.9221, val acc: 0.5798  (best train acc: 0.5753, best val acc: 0.5892)\n",
      "[Epoch: 28580] train loss: 0.9506, train acc: 0.5685, val loss: 0.9228, val acc: 0.5669  (best train acc: 0.5753, best val acc: 0.5892)\n",
      "[Epoch: 28600] train loss: 0.9516, train acc: 0.5523, val loss: 0.9263, val acc: 0.5801  (best train acc: 0.5753, best val acc: 0.5892)\n",
      "[Epoch: 28620] train loss: 0.9488, train acc: 0.5640, val loss: 0.9216, val acc: 0.5794  (best train acc: 0.5753, best val acc: 0.5892)\n",
      "[Epoch: 28640] train loss: 0.9515, train acc: 0.5642, val loss: 0.9208, val acc: 0.5653  (best train acc: 0.5753, best val acc: 0.5892)\n",
      "[Epoch: 28660] train loss: 0.9498, train acc: 0.5629, val loss: 0.9324, val acc: 0.5734  (best train acc: 0.5753, best val acc: 0.5892)\n",
      "[Epoch: 28680] train loss: 0.9523, train acc: 0.5593, val loss: 0.9244, val acc: 0.5774  (best train acc: 0.5753, best val acc: 0.5892)\n",
      "[Epoch: 28700] train loss: 0.9552, train acc: 0.5596, val loss: 0.9204, val acc: 0.5774  (best train acc: 0.5753, best val acc: 0.5892)\n",
      "[Epoch: 28720] train loss: 0.9466, train acc: 0.5677, val loss: 0.9225, val acc: 0.5750  (best train acc: 0.5753, best val acc: 0.5892)\n",
      "[Epoch: 28740] train loss: 0.9474, train acc: 0.5571, val loss: 0.9287, val acc: 0.5764  (best train acc: 0.5753, best val acc: 0.5892)\n",
      "[Epoch: 28760] train loss: 0.9482, train acc: 0.5727, val loss: 0.9206, val acc: 0.5831  (best train acc: 0.5753, best val acc: 0.5892)\n",
      "[Epoch: 28780] train loss: 0.9472, train acc: 0.5695, val loss: 0.9190, val acc: 0.5798  (best train acc: 0.5753, best val acc: 0.5892)\n",
      "[Epoch: 28800] train loss: 0.9503, train acc: 0.5571, val loss: 0.9250, val acc: 0.5794  (best train acc: 0.5753, best val acc: 0.5892)\n",
      "[Epoch: 28820] train loss: 0.9530, train acc: 0.5556, val loss: 0.9215, val acc: 0.5825  (best train acc: 0.5753, best val acc: 0.5892)\n",
      "[Epoch: 28840] train loss: 0.9451, train acc: 0.5711, val loss: 0.9216, val acc: 0.5777  (best train acc: 0.5753, best val acc: 0.5892)\n",
      "[Epoch: 28860] train loss: 0.9479, train acc: 0.5702, val loss: 0.9226, val acc: 0.5747  (best train acc: 0.5753, best val acc: 0.5892)\n",
      "[Epoch: 28880] train loss: 0.9540, train acc: 0.5602, val loss: 0.9253, val acc: 0.5777  (best train acc: 0.5753, best val acc: 0.5892)\n",
      "[Epoch: 28900] train loss: 0.9488, train acc: 0.5674, val loss: 0.9190, val acc: 0.5737  (best train acc: 0.5753, best val acc: 0.5892)\n",
      "[Epoch: 28920] train loss: 0.9478, train acc: 0.5630, val loss: 0.9247, val acc: 0.5845  (best train acc: 0.5753, best val acc: 0.5892)\n",
      "[Epoch: 28940] train loss: 0.9499, train acc: 0.5570, val loss: 0.9201, val acc: 0.5794  (best train acc: 0.5753, best val acc: 0.5892)\n",
      "[Epoch: 28960] train loss: 0.9511, train acc: 0.5630, val loss: 0.9225, val acc: 0.5821  (best train acc: 0.5753, best val acc: 0.5892)\n",
      "[Epoch: 28980] train loss: 0.9447, train acc: 0.5648, val loss: 0.9222, val acc: 0.5862  (best train acc: 0.5753, best val acc: 0.5892)\n",
      "[Epoch: 29000] train loss: 0.9458, train acc: 0.5656, val loss: 0.9200, val acc: 0.5845  (best train acc: 0.5753, best val acc: 0.5892)\n",
      "[Epoch: 29020] train loss: 0.9463, train acc: 0.5685, val loss: 0.9194, val acc: 0.5754  (best train acc: 0.5753, best val acc: 0.5892)\n",
      "[Epoch: 29040] train loss: 0.9530, train acc: 0.5593, val loss: 0.9368, val acc: 0.5744  (best train acc: 0.5753, best val acc: 0.5892)\n",
      "[Epoch: 29060] train loss: 0.9486, train acc: 0.5556, val loss: 0.9181, val acc: 0.5730  (best train acc: 0.5753, best val acc: 0.5892)\n",
      "[Epoch: 29080] train loss: 0.9496, train acc: 0.5676, val loss: 0.9213, val acc: 0.5831  (best train acc: 0.5753, best val acc: 0.5892)\n",
      "[Epoch: 29100] train loss: 0.9470, train acc: 0.5625, val loss: 0.9197, val acc: 0.5754  (best train acc: 0.5753, best val acc: 0.5892)\n",
      "[Epoch: 29120] train loss: 0.9452, train acc: 0.5670, val loss: 0.9204, val acc: 0.5828  (best train acc: 0.5753, best val acc: 0.5892)\n",
      "[Epoch: 29140] train loss: 0.9458, train acc: 0.5706, val loss: 0.9194, val acc: 0.5848  (best train acc: 0.5753, best val acc: 0.5892)\n",
      "[Epoch: 29160] train loss: 0.9470, train acc: 0.5643, val loss: 0.9212, val acc: 0.5788  (best train acc: 0.5753, best val acc: 0.5892)\n",
      "[Epoch: 29180] train loss: 0.9453, train acc: 0.5599, val loss: 0.9224, val acc: 0.5808  (best train acc: 0.5753, best val acc: 0.5892)\n",
      "[Epoch: 29200] train loss: 0.9713, train acc: 0.5376, val loss: 0.9216, val acc: 0.5764  (best train acc: 0.5753, best val acc: 0.5892)\n",
      "[Epoch: 29220] train loss: 0.9539, train acc: 0.5509, val loss: 0.9256, val acc: 0.5791  (best train acc: 0.5753, best val acc: 0.5892)\n",
      "[Epoch: 29240] train loss: 0.9471, train acc: 0.5653, val loss: 0.9183, val acc: 0.5821  (best train acc: 0.5770, best val acc: 0.5892)\n",
      "[Epoch: 29260] train loss: 0.9466, train acc: 0.5669, val loss: 0.9216, val acc: 0.5639  (best train acc: 0.5770, best val acc: 0.5892)\n",
      "[Epoch: 29280] train loss: 0.9468, train acc: 0.5683, val loss: 0.9218, val acc: 0.5831  (best train acc: 0.5770, best val acc: 0.5892)\n",
      "[Epoch: 29300] train loss: 0.9479, train acc: 0.5625, val loss: 0.9229, val acc: 0.5777  (best train acc: 0.5770, best val acc: 0.5892)\n",
      "[Epoch: 29320] train loss: 0.9480, train acc: 0.5646, val loss: 0.9227, val acc: 0.5791  (best train acc: 0.5770, best val acc: 0.5892)\n",
      "[Epoch: 29340] train loss: 0.9451, train acc: 0.5716, val loss: 0.9192, val acc: 0.5831  (best train acc: 0.5770, best val acc: 0.5892)\n",
      "[Epoch: 29360] train loss: 0.9492, train acc: 0.5646, val loss: 0.9209, val acc: 0.5848  (best train acc: 0.5770, best val acc: 0.5892)\n",
      "[Epoch: 29380] train loss: 0.9465, train acc: 0.5697, val loss: 0.9203, val acc: 0.5825  (best train acc: 0.5770, best val acc: 0.5892)\n",
      "[Epoch: 29400] train loss: 0.9478, train acc: 0.5689, val loss: 0.9209, val acc: 0.5788  (best train acc: 0.5770, best val acc: 0.5892)\n",
      "[Epoch: 29420] train loss: 0.9476, train acc: 0.5665, val loss: 0.9183, val acc: 0.5825  (best train acc: 0.5770, best val acc: 0.5892)\n",
      "[Epoch: 29440] train loss: 0.9464, train acc: 0.5571, val loss: 0.9212, val acc: 0.5788  (best train acc: 0.5770, best val acc: 0.5892)\n",
      "[Epoch: 29460] train loss: 0.9496, train acc: 0.5716, val loss: 0.9214, val acc: 0.5767  (best train acc: 0.5770, best val acc: 0.5892)\n",
      "[Epoch: 29480] train loss: 0.9460, train acc: 0.5651, val loss: 0.9280, val acc: 0.5754  (best train acc: 0.5770, best val acc: 0.5892)\n",
      "[Epoch: 29500] train loss: 0.9482, train acc: 0.5601, val loss: 0.9305, val acc: 0.5747  (best train acc: 0.5770, best val acc: 0.5892)\n",
      "[Epoch: 29520] train loss: 0.9456, train acc: 0.5676, val loss: 0.9171, val acc: 0.5794  (best train acc: 0.5770, best val acc: 0.5892)\n",
      "[Epoch: 29540] train loss: 0.9499, train acc: 0.5653, val loss: 0.9167, val acc: 0.5811  (best train acc: 0.5770, best val acc: 0.5892)\n",
      "[Epoch: 29560] train loss: 0.9469, train acc: 0.5708, val loss: 0.9177, val acc: 0.5737  (best train acc: 0.5770, best val acc: 0.5892)\n",
      "[Epoch: 29580] train loss: 0.9511, train acc: 0.5655, val loss: 0.9160, val acc: 0.5750  (best train acc: 0.5770, best val acc: 0.5892)\n",
      "[Epoch: 29600] train loss: 0.9573, train acc: 0.5670, val loss: 0.9216, val acc: 0.5683  (best train acc: 0.5770, best val acc: 0.5892)\n",
      "[Epoch: 29620] train loss: 0.9587, train acc: 0.5518, val loss: 0.9311, val acc: 0.5612  (best train acc: 0.5770, best val acc: 0.5892)\n",
      "[Epoch: 29640] train loss: 0.9467, train acc: 0.5542, val loss: 0.9209, val acc: 0.5788  (best train acc: 0.5770, best val acc: 0.5892)\n",
      "[Epoch: 29660] train loss: 0.9485, train acc: 0.5654, val loss: 0.9184, val acc: 0.5788  (best train acc: 0.5770, best val acc: 0.5892)\n",
      "[Epoch: 29680] train loss: 0.9464, train acc: 0.5698, val loss: 0.9245, val acc: 0.5713  (best train acc: 0.5770, best val acc: 0.5892)\n",
      "[Epoch: 29700] train loss: 0.9460, train acc: 0.5660, val loss: 0.9154, val acc: 0.5784  (best train acc: 0.5770, best val acc: 0.5892)\n",
      "[Epoch: 29720] train loss: 0.9491, train acc: 0.5657, val loss: 0.9178, val acc: 0.5794  (best train acc: 0.5770, best val acc: 0.5892)\n",
      "[Epoch: 29740] train loss: 0.9482, train acc: 0.5740, val loss: 0.9144, val acc: 0.5811  (best train acc: 0.5770, best val acc: 0.5892)\n",
      "[Epoch: 29760] train loss: 0.9480, train acc: 0.5625, val loss: 0.9144, val acc: 0.5801  (best train acc: 0.5770, best val acc: 0.5892)\n",
      "[Epoch: 29780] train loss: 0.9482, train acc: 0.5656, val loss: 0.9136, val acc: 0.5811  (best train acc: 0.5770, best val acc: 0.5892)\n",
      "[Epoch: 29800] train loss: 0.9490, train acc: 0.5655, val loss: 0.9362, val acc: 0.5545  (best train acc: 0.5770, best val acc: 0.5892)\n",
      "[Epoch: 29820] train loss: 0.9432, train acc: 0.5672, val loss: 0.9224, val acc: 0.5771  (best train acc: 0.5770, best val acc: 0.5892)\n",
      "[Epoch: 29840] train loss: 0.9445, train acc: 0.5668, val loss: 0.9173, val acc: 0.5757  (best train acc: 0.5770, best val acc: 0.5892)\n",
      "[Epoch: 29860] train loss: 0.9477, train acc: 0.5648, val loss: 0.9169, val acc: 0.5808  (best train acc: 0.5770, best val acc: 0.5892)\n",
      "[Epoch: 29880] train loss: 0.9478, train acc: 0.5624, val loss: 0.9143, val acc: 0.5798  (best train acc: 0.5770, best val acc: 0.5892)\n",
      "[Epoch: 29900] train loss: 0.9484, train acc: 0.5698, val loss: 0.9143, val acc: 0.5804  (best train acc: 0.5770, best val acc: 0.5892)\n",
      "[Epoch: 29920] train loss: 0.9466, train acc: 0.5690, val loss: 0.9163, val acc: 0.5750  (best train acc: 0.5770, best val acc: 0.5892)\n",
      "[Epoch: 29940] train loss: 0.9507, train acc: 0.5684, val loss: 0.9165, val acc: 0.5777  (best train acc: 0.5770, best val acc: 0.5892)\n",
      "[Epoch: 29960] train loss: 0.9468, train acc: 0.5663, val loss: 0.9186, val acc: 0.5794  (best train acc: 0.5770, best val acc: 0.5892)\n",
      "[Epoch: 29980] train loss: 0.9520, train acc: 0.5693, val loss: 0.9169, val acc: 0.5764  (best train acc: 0.5770, best val acc: 0.5892)\n",
      "[Epoch: 30000] train loss: 0.9487, train acc: 0.5594, val loss: 0.9146, val acc: 0.5757  (best train acc: 0.5770, best val acc: 0.5892)\n",
      "[Epoch: 30020] train loss: 0.9477, train acc: 0.5748, val loss: 0.9248, val acc: 0.5771  (best train acc: 0.5770, best val acc: 0.5892)\n",
      "[Epoch: 30040] train loss: 0.9485, train acc: 0.5680, val loss: 0.9139, val acc: 0.5808  (best train acc: 0.5770, best val acc: 0.5892)\n",
      "[Epoch: 30060] train loss: 0.9508, train acc: 0.5714, val loss: 0.9124, val acc: 0.5825  (best train acc: 0.5770, best val acc: 0.5892)\n",
      "[Epoch: 30080] train loss: 0.9534, train acc: 0.5641, val loss: 0.9231, val acc: 0.5707  (best train acc: 0.5770, best val acc: 0.5892)\n",
      "[Epoch: 30100] train loss: 0.9396, train acc: 0.5678, val loss: 0.9138, val acc: 0.5841  (best train acc: 0.5770, best val acc: 0.5892)\n",
      "[Epoch: 30120] train loss: 0.9436, train acc: 0.5717, val loss: 0.9125, val acc: 0.5818  (best train acc: 0.5770, best val acc: 0.5892)\n",
      "[Epoch: 30140] train loss: 0.9442, train acc: 0.5688, val loss: 0.9142, val acc: 0.5798  (best train acc: 0.5770, best val acc: 0.5892)\n",
      "[Epoch: 30160] train loss: 0.9469, train acc: 0.5625, val loss: 0.9161, val acc: 0.5784  (best train acc: 0.5770, best val acc: 0.5892)\n",
      "[Epoch: 30180] train loss: 0.9473, train acc: 0.5708, val loss: 0.9171, val acc: 0.5791  (best train acc: 0.5770, best val acc: 0.5892)\n",
      "[Epoch: 30200] train loss: 0.9486, train acc: 0.5671, val loss: 0.9133, val acc: 0.5781  (best train acc: 0.5770, best val acc: 0.5892)\n",
      "[Epoch: 30220] train loss: 0.9458, train acc: 0.5701, val loss: 0.9142, val acc: 0.5781  (best train acc: 0.5770, best val acc: 0.5892)\n",
      "[Epoch: 30240] train loss: 0.9460, train acc: 0.5698, val loss: 0.9125, val acc: 0.5862  (best train acc: 0.5770, best val acc: 0.5892)\n",
      "[Epoch: 30260] train loss: 0.9439, train acc: 0.5738, val loss: 0.9145, val acc: 0.5754  (best train acc: 0.5770, best val acc: 0.5892)\n",
      "[Epoch: 30280] train loss: 0.9488, train acc: 0.5709, val loss: 0.9123, val acc: 0.5818  (best train acc: 0.5770, best val acc: 0.5892)\n",
      "[Epoch: 30300] train loss: 0.9446, train acc: 0.5697, val loss: 0.9167, val acc: 0.5794  (best train acc: 0.5770, best val acc: 0.5892)\n",
      "[Epoch: 30320] train loss: 0.9603, train acc: 0.5518, val loss: 0.9160, val acc: 0.5737  (best train acc: 0.5770, best val acc: 0.5892)\n",
      "[Epoch: 30340] train loss: 0.9436, train acc: 0.5724, val loss: 0.9140, val acc: 0.5804  (best train acc: 0.5770, best val acc: 0.5892)\n",
      "[Epoch: 30360] train loss: 0.9479, train acc: 0.5678, val loss: 0.9132, val acc: 0.5801  (best train acc: 0.5770, best val acc: 0.5892)\n",
      "[Epoch: 30380] train loss: 0.9483, train acc: 0.5742, val loss: 0.9173, val acc: 0.5794  (best train acc: 0.5770, best val acc: 0.5892)\n",
      "[Epoch: 30400] train loss: 0.9506, train acc: 0.5549, val loss: 0.9237, val acc: 0.5639  (best train acc: 0.5770, best val acc: 0.5892)\n",
      "[Epoch: 30420] train loss: 0.9439, train acc: 0.5697, val loss: 0.9121, val acc: 0.5825  (best train acc: 0.5770, best val acc: 0.5892)\n",
      "[Epoch: 30440] train loss: 0.9383, train acc: 0.5703, val loss: 0.9163, val acc: 0.5777  (best train acc: 0.5770, best val acc: 0.5892)\n",
      "[Epoch: 30460] train loss: 0.9468, train acc: 0.5671, val loss: 0.9128, val acc: 0.5831  (best train acc: 0.5770, best val acc: 0.5892)\n",
      "[Epoch: 30480] train loss: 0.9461, train acc: 0.5669, val loss: 0.9157, val acc: 0.5815  (best train acc: 0.5770, best val acc: 0.5892)\n",
      "[Epoch: 30500] train loss: 0.9558, train acc: 0.5557, val loss: 0.9189, val acc: 0.5831  (best train acc: 0.5770, best val acc: 0.5892)\n",
      "[Epoch: 30520] train loss: 0.9470, train acc: 0.5722, val loss: 0.9147, val acc: 0.5801  (best train acc: 0.5770, best val acc: 0.5892)\n",
      "[Epoch: 30540] train loss: 0.9475, train acc: 0.5711, val loss: 0.9161, val acc: 0.5720  (best train acc: 0.5770, best val acc: 0.5892)\n",
      "[Epoch: 30560] train loss: 0.9454, train acc: 0.5706, val loss: 0.9133, val acc: 0.5808  (best train acc: 0.5770, best val acc: 0.5892)\n",
      "[Epoch: 30580] train loss: 0.9474, train acc: 0.5634, val loss: 0.9164, val acc: 0.5747  (best train acc: 0.5770, best val acc: 0.5892)\n",
      "[Epoch: 30600] train loss: 0.9490, train acc: 0.5657, val loss: 0.9130, val acc: 0.5808  (best train acc: 0.5770, best val acc: 0.5892)\n",
      "[Epoch: 30620] train loss: 0.9452, train acc: 0.5742, val loss: 0.9159, val acc: 0.5727  (best train acc: 0.5770, best val acc: 0.5892)\n",
      "[Epoch: 30640] train loss: 0.9488, train acc: 0.5685, val loss: 0.9137, val acc: 0.5798  (best train acc: 0.5770, best val acc: 0.5892)\n",
      "[Epoch: 30660] train loss: 0.9493, train acc: 0.5591, val loss: 0.9177, val acc: 0.5740  (best train acc: 0.5770, best val acc: 0.5892)\n",
      "[Epoch: 30680] train loss: 0.9437, train acc: 0.5698, val loss: 0.9114, val acc: 0.5811  (best train acc: 0.5770, best val acc: 0.5892)\n",
      "[Epoch: 30700] train loss: 0.9429, train acc: 0.5689, val loss: 0.9108, val acc: 0.5828  (best train acc: 0.5770, best val acc: 0.5892)\n",
      "[Epoch: 30720] train loss: 0.9531, train acc: 0.5615, val loss: 0.9243, val acc: 0.5713  (best train acc: 0.5770, best val acc: 0.5892)\n",
      "[Epoch: 30740] train loss: 0.9410, train acc: 0.5716, val loss: 0.9136, val acc: 0.5774  (best train acc: 0.5770, best val acc: 0.5892)\n",
      "[Epoch: 30760] train loss: 0.9458, train acc: 0.5646, val loss: 0.9161, val acc: 0.5811  (best train acc: 0.5770, best val acc: 0.5892)\n",
      "[Epoch: 30780] train loss: 0.9447, train acc: 0.5729, val loss: 0.9110, val acc: 0.5831  (best train acc: 0.5770, best val acc: 0.5892)\n",
      "[Epoch: 30800] train loss: 0.9477, train acc: 0.5701, val loss: 0.9118, val acc: 0.5841  (best train acc: 0.5770, best val acc: 0.5892)\n",
      "[Epoch: 30820] train loss: 0.9513, train acc: 0.5680, val loss: 0.9291, val acc: 0.5632  (best train acc: 0.5770, best val acc: 0.5892)\n",
      "[Epoch: 30840] train loss: 0.9479, train acc: 0.5687, val loss: 0.9133, val acc: 0.5811  (best train acc: 0.5770, best val acc: 0.5892)\n",
      "[Epoch: 30860] train loss: 0.9428, train acc: 0.5728, val loss: 0.9154, val acc: 0.5730  (best train acc: 0.5770, best val acc: 0.5892)\n",
      "[Epoch: 30880] train loss: 0.9459, train acc: 0.5684, val loss: 0.9153, val acc: 0.5744  (best train acc: 0.5770, best val acc: 0.5892)\n",
      "[Epoch: 30900] train loss: 0.9467, train acc: 0.5718, val loss: 0.9127, val acc: 0.5784  (best train acc: 0.5774, best val acc: 0.5892)\n",
      "[Epoch: 30920] train loss: 0.9449, train acc: 0.5666, val loss: 0.9149, val acc: 0.5771  (best train acc: 0.5774, best val acc: 0.5892)\n",
      "[Epoch: 30940] train loss: 0.9547, train acc: 0.5508, val loss: 0.9127, val acc: 0.5831  (best train acc: 0.5774, best val acc: 0.5892)\n",
      "[Epoch: 30960] train loss: 0.9479, train acc: 0.5698, val loss: 0.9149, val acc: 0.5734  (best train acc: 0.5774, best val acc: 0.5892)\n",
      "[Epoch: 30980] train loss: 0.9469, train acc: 0.5675, val loss: 0.9123, val acc: 0.5818  (best train acc: 0.5774, best val acc: 0.5892)\n",
      "[Epoch: 31000] train loss: 0.9467, train acc: 0.5737, val loss: 0.9135, val acc: 0.5767  (best train acc: 0.5774, best val acc: 0.5892)\n",
      "[Epoch: 31020] train loss: 0.9451, train acc: 0.5685, val loss: 0.9121, val acc: 0.5831  (best train acc: 0.5774, best val acc: 0.5892)\n",
      "[Epoch: 31040] train loss: 0.9450, train acc: 0.5623, val loss: 0.9102, val acc: 0.5852  (best train acc: 0.5774, best val acc: 0.5892)\n",
      "[Epoch: 31060] train loss: 0.9461, train acc: 0.5633, val loss: 0.9185, val acc: 0.5676  (best train acc: 0.5774, best val acc: 0.5892)\n",
      "[Epoch: 31080] train loss: 0.9502, train acc: 0.5627, val loss: 0.9116, val acc: 0.5811  (best train acc: 0.5774, best val acc: 0.5892)\n",
      "[Epoch: 31100] train loss: 0.9529, train acc: 0.5653, val loss: 0.9106, val acc: 0.5801  (best train acc: 0.5774, best val acc: 0.5892)\n",
      "[Epoch: 31120] train loss: 0.9434, train acc: 0.5692, val loss: 0.9130, val acc: 0.5771  (best train acc: 0.5774, best val acc: 0.5892)\n",
      "[Epoch: 31140] train loss: 0.9464, train acc: 0.5703, val loss: 0.9135, val acc: 0.5767  (best train acc: 0.5774, best val acc: 0.5892)\n",
      "[Epoch: 31160] train loss: 0.9440, train acc: 0.5676, val loss: 0.9122, val acc: 0.5794  (best train acc: 0.5774, best val acc: 0.5892)\n",
      "[Epoch: 31180] train loss: 0.9432, train acc: 0.5714, val loss: 0.9115, val acc: 0.5734  (best train acc: 0.5782, best val acc: 0.5892)\n",
      "[Epoch: 31200] train loss: 0.9435, train acc: 0.5725, val loss: 0.9097, val acc: 0.5818  (best train acc: 0.5782, best val acc: 0.5892)\n",
      "[Epoch: 31220] train loss: 0.9578, train acc: 0.5614, val loss: 0.9153, val acc: 0.5761  (best train acc: 0.5782, best val acc: 0.5892)\n",
      "[Epoch: 31240] train loss: 0.9461, train acc: 0.5577, val loss: 0.9112, val acc: 0.5909  (best train acc: 0.5782, best val acc: 0.5909)\n",
      "[Epoch: 31260] train loss: 0.9444, train acc: 0.5688, val loss: 0.9110, val acc: 0.5791  (best train acc: 0.5782, best val acc: 0.5909)\n",
      "[Epoch: 31280] train loss: 0.9401, train acc: 0.5723, val loss: 0.9130, val acc: 0.5737  (best train acc: 0.5782, best val acc: 0.5909)\n",
      "[Epoch: 31300] train loss: 0.9454, train acc: 0.5688, val loss: 0.9105, val acc: 0.5788  (best train acc: 0.5782, best val acc: 0.5909)\n",
      "[Epoch: 31320] train loss: 0.9485, train acc: 0.5561, val loss: 0.9184, val acc: 0.5673  (best train acc: 0.5782, best val acc: 0.5909)\n",
      "[Epoch: 31340] train loss: 0.9416, train acc: 0.5677, val loss: 0.9112, val acc: 0.5818  (best train acc: 0.5782, best val acc: 0.5909)\n",
      "[Epoch: 31360] train loss: 0.9421, train acc: 0.5691, val loss: 0.9091, val acc: 0.5825  (best train acc: 0.5782, best val acc: 0.5909)\n",
      "[Epoch: 31380] train loss: 0.9483, train acc: 0.5610, val loss: 0.9170, val acc: 0.5750  (best train acc: 0.5782, best val acc: 0.5909)\n",
      "[Epoch: 31400] train loss: 0.9544, train acc: 0.5545, val loss: 0.9101, val acc: 0.5855  (best train acc: 0.5782, best val acc: 0.5909)\n",
      "[Epoch: 31420] train loss: 0.9458, train acc: 0.5731, val loss: 0.9138, val acc: 0.5740  (best train acc: 0.5782, best val acc: 0.5909)\n",
      "[Epoch: 31440] train loss: 0.9401, train acc: 0.5684, val loss: 0.9111, val acc: 0.5828  (best train acc: 0.5782, best val acc: 0.5909)\n",
      "[Epoch: 31460] train loss: 0.9524, train acc: 0.5614, val loss: 0.9119, val acc: 0.5855  (best train acc: 0.5782, best val acc: 0.5939)\n",
      "[Epoch: 31480] train loss: 0.9417, train acc: 0.5740, val loss: 0.9077, val acc: 0.5899  (best train acc: 0.5782, best val acc: 0.5939)\n",
      "[Epoch: 31500] train loss: 0.9386, train acc: 0.5745, val loss: 0.9069, val acc: 0.5889  (best train acc: 0.5782, best val acc: 0.5939)\n",
      "[Epoch: 31520] train loss: 0.9414, train acc: 0.5688, val loss: 0.9067, val acc: 0.5852  (best train acc: 0.5782, best val acc: 0.5939)\n",
      "[Epoch: 31540] train loss: 0.9537, train acc: 0.5630, val loss: 0.9080, val acc: 0.5838  (best train acc: 0.5782, best val acc: 0.5939)\n",
      "[Epoch: 31560] train loss: 0.9408, train acc: 0.5741, val loss: 0.9100, val acc: 0.5912  (best train acc: 0.5782, best val acc: 0.5939)\n",
      "[Epoch: 31580] train loss: 0.9432, train acc: 0.5732, val loss: 0.9063, val acc: 0.5916  (best train acc: 0.5782, best val acc: 0.5939)\n",
      "[Epoch: 31600] train loss: 0.9358, train acc: 0.5745, val loss: 0.9252, val acc: 0.5720  (best train acc: 0.5782, best val acc: 0.5939)\n",
      "[Epoch: 31620] train loss: 0.9424, train acc: 0.5690, val loss: 0.9073, val acc: 0.5939  (best train acc: 0.5782, best val acc: 0.5939)\n",
      "[Epoch: 31640] train loss: 0.9403, train acc: 0.5755, val loss: 0.9077, val acc: 0.5821  (best train acc: 0.5782, best val acc: 0.5946)\n",
      "[Epoch: 31660] train loss: 0.9403, train acc: 0.5696, val loss: 0.9071, val acc: 0.5912  (best train acc: 0.5782, best val acc: 0.5946)\n",
      "[Epoch: 31680] train loss: 0.9384, train acc: 0.5734, val loss: 0.9074, val acc: 0.5892  (best train acc: 0.5782, best val acc: 0.5946)\n",
      "[Epoch: 31700] train loss: 0.9694, train acc: 0.5571, val loss: 0.9182, val acc: 0.5771  (best train acc: 0.5782, best val acc: 0.5946)\n",
      "[Epoch: 31720] train loss: 0.9396, train acc: 0.5714, val loss: 0.9114, val acc: 0.5825  (best train acc: 0.5782, best val acc: 0.5983)\n",
      "[Epoch: 31740] train loss: 0.9396, train acc: 0.5747, val loss: 0.9087, val acc: 0.5845  (best train acc: 0.5782, best val acc: 0.5983)\n",
      "[Epoch: 31760] train loss: 0.9737, train acc: 0.5641, val loss: 0.9103, val acc: 0.5774  (best train acc: 0.5782, best val acc: 0.5983)\n",
      "[Epoch: 31780] train loss: 0.9424, train acc: 0.5641, val loss: 0.9163, val acc: 0.5788  (best train acc: 0.5782, best val acc: 0.5983)\n",
      "[Epoch: 31800] train loss: 0.9383, train acc: 0.5737, val loss: 0.9077, val acc: 0.5862  (best train acc: 0.5782, best val acc: 0.5983)\n",
      "[Epoch: 31820] train loss: 0.9391, train acc: 0.5749, val loss: 0.9113, val acc: 0.5852  (best train acc: 0.5792, best val acc: 0.5987)\n",
      "[Epoch: 31840] train loss: 0.9395, train acc: 0.5717, val loss: 0.9065, val acc: 0.5916  (best train acc: 0.5792, best val acc: 0.5987)\n",
      "[Epoch: 31860] train loss: 0.9395, train acc: 0.5695, val loss: 0.9184, val acc: 0.5723  (best train acc: 0.5792, best val acc: 0.5987)\n",
      "[Epoch: 31880] train loss: 0.9645, train acc: 0.5432, val loss: 0.9122, val acc: 0.5774  (best train acc: 0.5792, best val acc: 0.5987)\n",
      "[Epoch: 31900] train loss: 0.9405, train acc: 0.5686, val loss: 0.9057, val acc: 0.5929  (best train acc: 0.5792, best val acc: 0.5987)\n",
      "[Epoch: 31920] train loss: 0.9392, train acc: 0.5695, val loss: 0.9038, val acc: 0.5922  (best train acc: 0.5792, best val acc: 0.5990)\n",
      "[Epoch: 31940] train loss: 0.9420, train acc: 0.5727, val loss: 0.9043, val acc: 0.5980  (best train acc: 0.5792, best val acc: 0.5990)\n",
      "[Epoch: 31960] train loss: 0.9347, train acc: 0.5727, val loss: 0.9053, val acc: 0.5885  (best train acc: 0.5792, best val acc: 0.5990)\n",
      "[Epoch: 31980] train loss: 0.9410, train acc: 0.5695, val loss: 0.9040, val acc: 0.5899  (best train acc: 0.5792, best val acc: 0.5990)\n",
      "[Epoch: 32000] train loss: 0.9364, train acc: 0.5698, val loss: 0.9053, val acc: 0.5916  (best train acc: 0.5792, best val acc: 0.5990)\n",
      "[Epoch: 32020] train loss: 0.9391, train acc: 0.5753, val loss: 0.9066, val acc: 0.5919  (best train acc: 0.5792, best val acc: 0.5990)\n",
      "[Epoch: 32040] train loss: 0.9352, train acc: 0.5717, val loss: 0.9058, val acc: 0.5956  (best train acc: 0.5792, best val acc: 0.6007)\n",
      "[Epoch: 32060] train loss: 0.9368, train acc: 0.5739, val loss: 0.9049, val acc: 0.5926  (best train acc: 0.5792, best val acc: 0.6007)\n",
      "[Epoch: 32080] train loss: 0.9379, train acc: 0.5739, val loss: 0.9052, val acc: 0.5909  (best train acc: 0.5792, best val acc: 0.6007)\n",
      "[Epoch: 32100] train loss: 0.9396, train acc: 0.5643, val loss: 0.9090, val acc: 0.5885  (best train acc: 0.5805, best val acc: 0.6007)\n",
      "[Epoch: 32120] train loss: 0.9355, train acc: 0.5787, val loss: 0.9035, val acc: 0.5956  (best train acc: 0.5809, best val acc: 0.6007)\n",
      "[Epoch: 32140] train loss: 0.9365, train acc: 0.5706, val loss: 0.9049, val acc: 0.5926  (best train acc: 0.5809, best val acc: 0.6007)\n",
      "[Epoch: 32160] train loss: 0.9365, train acc: 0.5701, val loss: 0.9062, val acc: 0.5892  (best train acc: 0.5809, best val acc: 0.6007)\n",
      "[Epoch: 32180] train loss: 0.9327, train acc: 0.5765, val loss: 0.9066, val acc: 0.5939  (best train acc: 0.5809, best val acc: 0.6007)\n",
      "[Epoch: 32200] train loss: 0.9353, train acc: 0.5755, val loss: 0.9100, val acc: 0.5828  (best train acc: 0.5809, best val acc: 0.6007)\n",
      "[Epoch: 32220] train loss: 0.9444, train acc: 0.5680, val loss: 0.9072, val acc: 0.5906  (best train acc: 0.5809, best val acc: 0.6007)\n",
      "[Epoch: 32240] train loss: 0.9360, train acc: 0.5754, val loss: 0.9049, val acc: 0.5912  (best train acc: 0.5809, best val acc: 0.6007)\n",
      "[Epoch: 32260] train loss: 0.9412, train acc: 0.5712, val loss: 0.9046, val acc: 0.5953  (best train acc: 0.5809, best val acc: 0.6007)\n",
      "[Epoch: 32280] train loss: 0.9410, train acc: 0.5757, val loss: 0.9065, val acc: 0.5919  (best train acc: 0.5809, best val acc: 0.6007)\n",
      "[Epoch: 32300] train loss: 0.9326, train acc: 0.5773, val loss: 0.9050, val acc: 0.5899  (best train acc: 0.5809, best val acc: 0.6007)\n",
      "[Epoch: 32320] train loss: 0.9400, train acc: 0.5695, val loss: 0.9168, val acc: 0.5744  (best train acc: 0.5809, best val acc: 0.6007)\n",
      "[Epoch: 32340] train loss: 0.9371, train acc: 0.5692, val loss: 0.9170, val acc: 0.5767  (best train acc: 0.5813, best val acc: 0.6007)\n",
      "[Epoch: 32360] train loss: 0.9370, train acc: 0.5771, val loss: 0.9035, val acc: 0.5946  (best train acc: 0.5813, best val acc: 0.6007)\n",
      "[Epoch: 32380] train loss: 0.9344, train acc: 0.5701, val loss: 0.9075, val acc: 0.5872  (best train acc: 0.5813, best val acc: 0.6007)\n",
      "[Epoch: 32400] train loss: 0.9361, train acc: 0.5788, val loss: 0.9075, val acc: 0.5919  (best train acc: 0.5813, best val acc: 0.6007)\n",
      "[Epoch: 32420] train loss: 0.9308, train acc: 0.5776, val loss: 0.9013, val acc: 0.5956  (best train acc: 0.5813, best val acc: 0.6017)\n",
      "[Epoch: 32440] train loss: 0.9339, train acc: 0.5755, val loss: 0.9032, val acc: 0.5946  (best train acc: 0.5813, best val acc: 0.6017)\n",
      "[Epoch: 32460] train loss: 0.9346, train acc: 0.5736, val loss: 0.9019, val acc: 0.5980  (best train acc: 0.5813, best val acc: 0.6017)\n",
      "[Epoch: 32480] train loss: 0.9306, train acc: 0.5765, val loss: 0.9025, val acc: 0.5916  (best train acc: 0.5836, best val acc: 0.6017)\n",
      "[Epoch: 32500] train loss: 0.9309, train acc: 0.5780, val loss: 0.9094, val acc: 0.5882  (best train acc: 0.5836, best val acc: 0.6017)\n",
      "[Epoch: 32520] train loss: 0.9322, train acc: 0.5732, val loss: 0.9046, val acc: 0.5936  (best train acc: 0.5836, best val acc: 0.6017)\n",
      "[Epoch: 32540] train loss: 0.9331, train acc: 0.5735, val loss: 0.9073, val acc: 0.5929  (best train acc: 0.5836, best val acc: 0.6017)\n",
      "[Epoch: 32560] train loss: 0.9320, train acc: 0.5723, val loss: 0.9055, val acc: 0.5909  (best train acc: 0.5836, best val acc: 0.6017)\n",
      "[Epoch: 32580] train loss: 0.9373, train acc: 0.5704, val loss: 0.9053, val acc: 0.5949  (best train acc: 0.5836, best val acc: 0.6017)\n",
      "[Epoch: 32600] train loss: 0.9391, train acc: 0.5722, val loss: 0.9077, val acc: 0.5868  (best train acc: 0.5836, best val acc: 0.6017)\n",
      "[Epoch: 32620] train loss: 0.9331, train acc: 0.5750, val loss: 0.9023, val acc: 0.5983  (best train acc: 0.5836, best val acc: 0.6017)\n",
      "[Epoch: 32640] train loss: 0.9358, train acc: 0.5750, val loss: 0.9049, val acc: 0.5906  (best train acc: 0.5836, best val acc: 0.6017)\n",
      "[Epoch: 32660] train loss: 0.9469, train acc: 0.5576, val loss: 0.9109, val acc: 0.5865  (best train acc: 0.5836, best val acc: 0.6017)\n",
      "[Epoch: 32680] train loss: 0.9347, train acc: 0.5730, val loss: 0.9203, val acc: 0.5669  (best train acc: 0.5836, best val acc: 0.6017)\n",
      "[Epoch: 32700] train loss: 0.9356, train acc: 0.5763, val loss: 0.9206, val acc: 0.5771  (best train acc: 0.5836, best val acc: 0.6017)\n",
      "[Epoch: 32720] train loss: 0.9317, train acc: 0.5810, val loss: 0.9042, val acc: 0.5976  (best train acc: 0.5836, best val acc: 0.6017)\n",
      "[Epoch: 32740] train loss: 0.9348, train acc: 0.5745, val loss: 0.9065, val acc: 0.5865  (best train acc: 0.5836, best val acc: 0.6017)\n",
      "[Epoch: 32760] train loss: 0.9352, train acc: 0.5779, val loss: 0.9033, val acc: 0.5943  (best train acc: 0.5836, best val acc: 0.6024)\n",
      "[Epoch: 32780] train loss: 0.9343, train acc: 0.5771, val loss: 0.9042, val acc: 0.5933  (best train acc: 0.5836, best val acc: 0.6024)\n",
      "[Epoch: 32800] train loss: 0.9476, train acc: 0.5732, val loss: 0.9175, val acc: 0.5710  (best train acc: 0.5836, best val acc: 0.6024)\n",
      "[Epoch: 32820] train loss: 0.9379, train acc: 0.5734, val loss: 0.9024, val acc: 0.5966  (best train acc: 0.5836, best val acc: 0.6024)\n",
      "[Epoch: 32840] train loss: 0.9317, train acc: 0.5732, val loss: 0.9025, val acc: 0.5953  (best train acc: 0.5836, best val acc: 0.6024)\n",
      "[Epoch: 32860] train loss: 0.9324, train acc: 0.5789, val loss: 0.9021, val acc: 0.5963  (best train acc: 0.5836, best val acc: 0.6024)\n",
      "[Epoch: 32880] train loss: 0.9326, train acc: 0.5757, val loss: 0.9019, val acc: 0.5953  (best train acc: 0.5836, best val acc: 0.6024)\n",
      "[Epoch: 32900] train loss: 0.9347, train acc: 0.5781, val loss: 0.9010, val acc: 0.5993  (best train acc: 0.5836, best val acc: 0.6030)\n",
      "[Epoch: 32920] train loss: 0.9310, train acc: 0.5812, val loss: 0.9108, val acc: 0.5825  (best train acc: 0.5836, best val acc: 0.6030)\n",
      "[Epoch: 32940] train loss: 0.9368, train acc: 0.5744, val loss: 0.9066, val acc: 0.5872  (best train acc: 0.5836, best val acc: 0.6030)\n",
      "[Epoch: 32960] train loss: 0.9341, train acc: 0.5770, val loss: 0.9027, val acc: 0.5973  (best train acc: 0.5836, best val acc: 0.6030)\n",
      "[Epoch: 32980] train loss: 0.9361, train acc: 0.5771, val loss: 0.9030, val acc: 0.6000  (best train acc: 0.5836, best val acc: 0.6030)\n",
      "[Epoch: 33000] train loss: 0.9387, train acc: 0.5707, val loss: 0.9046, val acc: 0.5987  (best train acc: 0.5836, best val acc: 0.6030)\n",
      "[Epoch: 33020] train loss: 0.9383, train acc: 0.5720, val loss: 0.9069, val acc: 0.5902  (best train acc: 0.5836, best val acc: 0.6030)\n",
      "[Epoch: 33040] train loss: 0.9356, train acc: 0.5780, val loss: 0.9001, val acc: 0.5973  (best train acc: 0.5841, best val acc: 0.6030)\n",
      "[Epoch: 33060] train loss: 0.9319, train acc: 0.5762, val loss: 0.9000, val acc: 0.5949  (best train acc: 0.5841, best val acc: 0.6030)\n",
      "[Epoch: 33080] train loss: 0.9383, train acc: 0.5728, val loss: 0.9021, val acc: 0.5973  (best train acc: 0.5841, best val acc: 0.6030)\n",
      "[Epoch: 33100] train loss: 0.9384, train acc: 0.5736, val loss: 0.8992, val acc: 0.5966  (best train acc: 0.5841, best val acc: 0.6030)\n",
      "[Epoch: 33120] train loss: 0.9308, train acc: 0.5799, val loss: 0.9016, val acc: 0.5980  (best train acc: 0.5841, best val acc: 0.6030)\n",
      "[Epoch: 33140] train loss: 0.9411, train acc: 0.5766, val loss: 0.9011, val acc: 0.5949  (best train acc: 0.5841, best val acc: 0.6030)\n",
      "[Epoch: 33160] train loss: 0.9310, train acc: 0.5779, val loss: 0.9037, val acc: 0.6010  (best train acc: 0.5841, best val acc: 0.6030)\n",
      "[Epoch: 33180] train loss: 0.9413, train acc: 0.5711, val loss: 0.9096, val acc: 0.5828  (best train acc: 0.5841, best val acc: 0.6030)\n",
      "[Epoch: 33200] train loss: 0.9327, train acc: 0.5743, val loss: 0.8993, val acc: 0.6013  (best train acc: 0.5841, best val acc: 0.6030)\n",
      "[Epoch: 33220] train loss: 0.9333, train acc: 0.5779, val loss: 0.9031, val acc: 0.5895  (best train acc: 0.5841, best val acc: 0.6030)\n",
      "[Epoch: 33240] train loss: 0.9289, train acc: 0.5782, val loss: 0.9001, val acc: 0.5983  (best train acc: 0.5841, best val acc: 0.6030)\n",
      "[Epoch: 33260] train loss: 0.9301, train acc: 0.5779, val loss: 0.9027, val acc: 0.5906  (best train acc: 0.5841, best val acc: 0.6030)\n",
      "[Epoch: 33280] train loss: 0.9358, train acc: 0.5719, val loss: 0.9073, val acc: 0.5926  (best train acc: 0.5841, best val acc: 0.6030)\n",
      "[Epoch: 33300] train loss: 0.9266, train acc: 0.5782, val loss: 0.8985, val acc: 0.5983  (best train acc: 0.5841, best val acc: 0.6030)\n",
      "[Epoch: 33320] train loss: 0.9258, train acc: 0.5818, val loss: 0.9057, val acc: 0.5966  (best train acc: 0.5841, best val acc: 0.6030)\n",
      "[Epoch: 33340] train loss: 0.9331, train acc: 0.5729, val loss: 0.9006, val acc: 0.5953  (best train acc: 0.5841, best val acc: 0.6030)\n",
      "[Epoch: 33360] train loss: 0.9303, train acc: 0.5778, val loss: 0.9026, val acc: 0.5956  (best train acc: 0.5841, best val acc: 0.6030)\n",
      "[Epoch: 33380] train loss: 0.9292, train acc: 0.5821, val loss: 0.9008, val acc: 0.6007  (best train acc: 0.5841, best val acc: 0.6030)\n",
      "[Epoch: 33400] train loss: 0.9340, train acc: 0.5806, val loss: 0.9016, val acc: 0.5943  (best train acc: 0.5841, best val acc: 0.6030)\n",
      "[Epoch: 33420] train loss: 0.9435, train acc: 0.5711, val loss: 0.9019, val acc: 0.5973  (best train acc: 0.5841, best val acc: 0.6030)\n",
      "[Epoch: 33440] train loss: 0.9320, train acc: 0.5800, val loss: 0.9022, val acc: 0.5960  (best train acc: 0.5841, best val acc: 0.6040)\n",
      "[Epoch: 33460] train loss: 0.9349, train acc: 0.5829, val loss: 0.9034, val acc: 0.6020  (best train acc: 0.5841, best val acc: 0.6040)\n",
      "[Epoch: 33480] train loss: 0.9315, train acc: 0.5791, val loss: 0.8998, val acc: 0.5970  (best train acc: 0.5841, best val acc: 0.6040)\n",
      "[Epoch: 33500] train loss: 0.9317, train acc: 0.5838, val loss: 0.9007, val acc: 0.5963  (best train acc: 0.5841, best val acc: 0.6040)\n",
      "[Epoch: 33520] train loss: 0.9329, train acc: 0.5791, val loss: 0.8985, val acc: 0.5993  (best train acc: 0.5865, best val acc: 0.6040)\n",
      "[Epoch: 33540] train loss: 0.9325, train acc: 0.5769, val loss: 0.9070, val acc: 0.5919  (best train acc: 0.5865, best val acc: 0.6040)\n",
      "[Epoch: 33560] train loss: 0.9275, train acc: 0.5815, val loss: 0.8991, val acc: 0.5970  (best train acc: 0.5865, best val acc: 0.6040)\n",
      "[Epoch: 33580] train loss: 0.9315, train acc: 0.5792, val loss: 0.8993, val acc: 0.6007  (best train acc: 0.5865, best val acc: 0.6040)\n",
      "[Epoch: 33600] train loss: 0.9360, train acc: 0.5719, val loss: 0.9125, val acc: 0.5771  (best train acc: 0.5865, best val acc: 0.6040)\n",
      "[Epoch: 33620] train loss: 0.9263, train acc: 0.5799, val loss: 0.9010, val acc: 0.5983  (best train acc: 0.5865, best val acc: 0.6057)\n",
      "[Epoch: 33640] train loss: 0.9299, train acc: 0.5816, val loss: 0.9009, val acc: 0.5960  (best train acc: 0.5865, best val acc: 0.6057)\n",
      "[Epoch: 33660] train loss: 0.9297, train acc: 0.5803, val loss: 0.9033, val acc: 0.5960  (best train acc: 0.5865, best val acc: 0.6057)\n",
      "[Epoch: 33680] train loss: 0.9351, train acc: 0.5774, val loss: 0.9056, val acc: 0.5906  (best train acc: 0.5865, best val acc: 0.6057)\n",
      "[Epoch: 33700] train loss: 0.9274, train acc: 0.5799, val loss: 0.9004, val acc: 0.6013  (best train acc: 0.5865, best val acc: 0.6057)\n",
      "[Epoch: 33720] train loss: 0.9305, train acc: 0.5832, val loss: 0.9009, val acc: 0.5990  (best train acc: 0.5865, best val acc: 0.6057)\n",
      "[Epoch: 33740] train loss: 0.9335, train acc: 0.5792, val loss: 0.9048, val acc: 0.5909  (best train acc: 0.5865, best val acc: 0.6057)\n",
      "[Epoch: 33760] train loss: 0.9306, train acc: 0.5749, val loss: 0.9065, val acc: 0.5943  (best train acc: 0.5865, best val acc: 0.6057)\n",
      "[Epoch: 33780] train loss: 0.9335, train acc: 0.5789, val loss: 0.9026, val acc: 0.5970  (best train acc: 0.5865, best val acc: 0.6057)\n",
      "[Epoch: 33800] train loss: 0.9310, train acc: 0.5781, val loss: 0.9005, val acc: 0.5993  (best train acc: 0.5865, best val acc: 0.6057)\n",
      "[Epoch: 33820] train loss: 0.9333, train acc: 0.5825, val loss: 0.9046, val acc: 0.5855  (best train acc: 0.5865, best val acc: 0.6057)\n",
      "[Epoch: 33840] train loss: 0.9300, train acc: 0.5813, val loss: 0.9054, val acc: 0.5943  (best train acc: 0.5865, best val acc: 0.6057)\n",
      "[Epoch: 33860] train loss: 0.9282, train acc: 0.5836, val loss: 0.9062, val acc: 0.5868  (best train acc: 0.5865, best val acc: 0.6057)\n",
      "[Epoch: 33880] train loss: 0.9302, train acc: 0.5758, val loss: 0.9027, val acc: 0.6010  (best train acc: 0.5865, best val acc: 0.6057)\n",
      "[Epoch: 33900] train loss: 0.9380, train acc: 0.5797, val loss: 0.8985, val acc: 0.5976  (best train acc: 0.5865, best val acc: 0.6057)\n",
      "[Epoch: 33920] train loss: 0.9286, train acc: 0.5795, val loss: 0.8998, val acc: 0.5976  (best train acc: 0.5865, best val acc: 0.6057)\n",
      "[Epoch: 33940] train loss: 0.9322, train acc: 0.5807, val loss: 0.9001, val acc: 0.6013  (best train acc: 0.5865, best val acc: 0.6057)\n",
      "[Epoch: 33960] train loss: 0.9322, train acc: 0.5768, val loss: 0.9005, val acc: 0.6007  (best train acc: 0.5865, best val acc: 0.6057)\n",
      "[Epoch: 33980] train loss: 0.9312, train acc: 0.5696, val loss: 0.9083, val acc: 0.5902  (best train acc: 0.5865, best val acc: 0.6057)\n",
      "[Epoch: 34000] train loss: 0.9320, train acc: 0.5833, val loss: 0.9013, val acc: 0.5906  (best train acc: 0.5865, best val acc: 0.6057)\n",
      "[Epoch: 34020] train loss: 0.9337, train acc: 0.5773, val loss: 0.9066, val acc: 0.5983  (best train acc: 0.5865, best val acc: 0.6057)\n",
      "[Epoch: 34040] train loss: 0.9256, train acc: 0.5826, val loss: 0.9011, val acc: 0.5966  (best train acc: 0.5865, best val acc: 0.6057)\n",
      "[Epoch: 34060] train loss: 0.9293, train acc: 0.5813, val loss: 0.9028, val acc: 0.5943  (best train acc: 0.5865, best val acc: 0.6057)\n",
      "[Epoch: 34080] train loss: 0.9289, train acc: 0.5830, val loss: 0.9037, val acc: 0.5997  (best train acc: 0.5865, best val acc: 0.6057)\n",
      "[Epoch: 34100] train loss: 0.9335, train acc: 0.5735, val loss: 0.9091, val acc: 0.5858  (best train acc: 0.5865, best val acc: 0.6057)\n",
      "[Epoch: 34120] train loss: 0.9311, train acc: 0.5774, val loss: 0.9043, val acc: 0.5973  (best train acc: 0.5865, best val acc: 0.6057)\n",
      "[Epoch: 34140] train loss: 0.9267, train acc: 0.5834, val loss: 0.9001, val acc: 0.5997  (best train acc: 0.5865, best val acc: 0.6057)\n",
      "[Epoch: 34160] train loss: 0.9332, train acc: 0.5808, val loss: 0.8995, val acc: 0.5983  (best train acc: 0.5865, best val acc: 0.6057)\n",
      "[Epoch: 34180] train loss: 0.9309, train acc: 0.5766, val loss: 0.9044, val acc: 0.5966  (best train acc: 0.5865, best val acc: 0.6057)\n",
      "[Epoch: 34200] train loss: 0.9342, train acc: 0.5742, val loss: 0.9051, val acc: 0.5922  (best train acc: 0.5865, best val acc: 0.6057)\n",
      "[Epoch: 34220] train loss: 0.9292, train acc: 0.5792, val loss: 0.9000, val acc: 0.5983  (best train acc: 0.5865, best val acc: 0.6057)\n",
      "[Epoch: 34240] train loss: 0.9299, train acc: 0.5778, val loss: 0.9001, val acc: 0.6007  (best train acc: 0.5865, best val acc: 0.6057)\n",
      "[Epoch: 34260] train loss: 0.9321, train acc: 0.5801, val loss: 0.9038, val acc: 0.5993  (best train acc: 0.5865, best val acc: 0.6057)\n",
      "[Epoch: 34280] train loss: 0.9272, train acc: 0.5797, val loss: 0.8994, val acc: 0.5997  (best train acc: 0.5865, best val acc: 0.6057)\n",
      "[Epoch: 34300] train loss: 0.9307, train acc: 0.5791, val loss: 0.9003, val acc: 0.6017  (best train acc: 0.5865, best val acc: 0.6057)\n",
      "[Epoch: 34320] train loss: 0.9414, train acc: 0.5750, val loss: 0.9059, val acc: 0.5889  (best train acc: 0.5865, best val acc: 0.6057)\n",
      "[Epoch: 34340] train loss: 0.9275, train acc: 0.5821, val loss: 0.9009, val acc: 0.6017  (best train acc: 0.5865, best val acc: 0.6057)\n",
      "[Epoch: 34360] train loss: 0.9266, train acc: 0.5808, val loss: 0.9018, val acc: 0.5993  (best train acc: 0.5865, best val acc: 0.6057)\n",
      "[Epoch: 34380] train loss: 0.9266, train acc: 0.5829, val loss: 0.9077, val acc: 0.5875  (best train acc: 0.5865, best val acc: 0.6057)\n",
      "[Epoch: 34400] train loss: 0.9383, train acc: 0.5797, val loss: 0.9000, val acc: 0.5949  (best train acc: 0.5865, best val acc: 0.6057)\n",
      "[Epoch: 34420] train loss: 0.9316, train acc: 0.5730, val loss: 0.9090, val acc: 0.5848  (best train acc: 0.5865, best val acc: 0.6057)\n",
      "[Epoch: 34440] train loss: 0.9535, train acc: 0.5624, val loss: 0.9232, val acc: 0.5656  (best train acc: 0.5865, best val acc: 0.6057)\n",
      "[Epoch: 34460] train loss: 0.9386, train acc: 0.5764, val loss: 0.9066, val acc: 0.5916  (best train acc: 0.5865, best val acc: 0.6057)\n",
      "[Epoch: 34480] train loss: 0.9351, train acc: 0.5795, val loss: 0.9003, val acc: 0.6010  (best train acc: 0.5865, best val acc: 0.6057)\n",
      "[Epoch: 34500] train loss: 0.9289, train acc: 0.5792, val loss: 0.9002, val acc: 0.6013  (best train acc: 0.5865, best val acc: 0.6057)\n",
      "[Epoch: 34520] train loss: 0.9285, train acc: 0.5829, val loss: 0.9020, val acc: 0.5987  (best train acc: 0.5865, best val acc: 0.6057)\n",
      "[Epoch: 34540] train loss: 0.9386, train acc: 0.5728, val loss: 0.9008, val acc: 0.6000  (best train acc: 0.5865, best val acc: 0.6057)\n",
      "[Epoch: 34560] train loss: 0.9293, train acc: 0.5776, val loss: 0.9034, val acc: 0.5976  (best train acc: 0.5865, best val acc: 0.6057)\n",
      "[Epoch: 34580] train loss: 0.9305, train acc: 0.5842, val loss: 0.9012, val acc: 0.5946  (best train acc: 0.5865, best val acc: 0.6057)\n",
      "[Epoch: 34600] train loss: 0.9282, train acc: 0.5809, val loss: 0.9028, val acc: 0.5885  (best train acc: 0.5865, best val acc: 0.6057)\n",
      "[Epoch: 34620] train loss: 0.9295, train acc: 0.5792, val loss: 0.9043, val acc: 0.5828  (best train acc: 0.5865, best val acc: 0.6057)\n",
      "[Epoch: 34640] train loss: 0.9344, train acc: 0.5786, val loss: 0.9010, val acc: 0.5943  (best train acc: 0.5865, best val acc: 0.6057)\n",
      "[Epoch: 34660] train loss: 0.9275, train acc: 0.5829, val loss: 0.9005, val acc: 0.5966  (best train acc: 0.5865, best val acc: 0.6057)\n",
      "[Epoch: 34680] train loss: 0.9286, train acc: 0.5774, val loss: 0.9028, val acc: 0.5939  (best train acc: 0.5865, best val acc: 0.6057)\n",
      "[Epoch: 34700] train loss: 0.9331, train acc: 0.5775, val loss: 0.9007, val acc: 0.5990  (best train acc: 0.5865, best val acc: 0.6057)\n",
      "[Epoch: 34720] train loss: 0.9335, train acc: 0.5759, val loss: 0.9005, val acc: 0.5997  (best train acc: 0.5865, best val acc: 0.6057)\n",
      "[Epoch: 34740] train loss: 0.9295, train acc: 0.5771, val loss: 0.9001, val acc: 0.6020  (best train acc: 0.5865, best val acc: 0.6057)\n",
      "[Epoch: 34760] train loss: 0.9358, train acc: 0.5716, val loss: 0.9125, val acc: 0.5774  (best train acc: 0.5865, best val acc: 0.6057)\n",
      "[Epoch: 34780] train loss: 0.9306, train acc: 0.5800, val loss: 0.9040, val acc: 0.5889  (best train acc: 0.5865, best val acc: 0.6057)\n",
      "[Epoch: 34800] train loss: 0.9622, train acc: 0.5492, val loss: 0.9203, val acc: 0.5680  (best train acc: 0.5865, best val acc: 0.6057)\n",
      "[Epoch: 34820] train loss: 0.9438, train acc: 0.5689, val loss: 0.9042, val acc: 0.5949  (best train acc: 0.5865, best val acc: 0.6057)\n",
      "[Epoch: 34840] train loss: 0.9346, train acc: 0.5742, val loss: 0.9018, val acc: 0.5956  (best train acc: 0.5865, best val acc: 0.6057)\n",
      "[Epoch: 34860] train loss: 0.9309, train acc: 0.5844, val loss: 0.8985, val acc: 0.6010  (best train acc: 0.5865, best val acc: 0.6057)\n",
      "[Epoch: 34880] train loss: 0.9309, train acc: 0.5733, val loss: 0.9129, val acc: 0.5791  (best train acc: 0.5865, best val acc: 0.6057)\n",
      "[Epoch: 34900] train loss: 0.9415, train acc: 0.5659, val loss: 0.9190, val acc: 0.5808  (best train acc: 0.5865, best val acc: 0.6057)\n",
      "[Epoch: 34920] train loss: 0.9426, train acc: 0.5675, val loss: 0.9093, val acc: 0.5815  (best train acc: 0.5865, best val acc: 0.6057)\n",
      "[Epoch: 34940] train loss: 0.9381, train acc: 0.5746, val loss: 0.9061, val acc: 0.5885  (best train acc: 0.5865, best val acc: 0.6057)\n",
      "[Epoch: 34960] train loss: 0.9398, train acc: 0.5742, val loss: 0.9088, val acc: 0.5875  (best train acc: 0.5865, best val acc: 0.6057)\n",
      "[Epoch: 34980] train loss: 0.9416, train acc: 0.5773, val loss: 0.9031, val acc: 0.6007  (best train acc: 0.5865, best val acc: 0.6057)\n",
      "[Epoch: 35000] train loss: 0.9334, train acc: 0.5775, val loss: 0.9018, val acc: 0.6003  (best train acc: 0.5865, best val acc: 0.6057)\n",
      "[Epoch: 35020] train loss: 0.9264, train acc: 0.5814, val loss: 0.9037, val acc: 0.5862  (best train acc: 0.5865, best val acc: 0.6071)\n",
      "[Epoch: 35040] train loss: 0.9326, train acc: 0.5779, val loss: 0.9001, val acc: 0.6034  (best train acc: 0.5865, best val acc: 0.6071)\n",
      "[Epoch: 35060] train loss: 0.9298, train acc: 0.5774, val loss: 0.9022, val acc: 0.5889  (best train acc: 0.5865, best val acc: 0.6071)\n",
      "[Epoch: 35080] train loss: 0.9329, train acc: 0.5820, val loss: 0.9007, val acc: 0.5949  (best train acc: 0.5865, best val acc: 0.6071)\n",
      "[Epoch: 35100] train loss: 0.9275, train acc: 0.5848, val loss: 0.9005, val acc: 0.6010  (best train acc: 0.5865, best val acc: 0.6071)\n",
      "[Epoch: 35120] train loss: 0.9300, train acc: 0.5796, val loss: 0.9004, val acc: 0.6034  (best train acc: 0.5865, best val acc: 0.6071)\n",
      "[Epoch: 35140] train loss: 0.9332, train acc: 0.5756, val loss: 0.9131, val acc: 0.5798  (best train acc: 0.5881, best val acc: 0.6071)\n",
      "[Epoch: 35160] train loss: 0.9290, train acc: 0.5801, val loss: 0.8998, val acc: 0.6020  (best train acc: 0.5881, best val acc: 0.6071)\n",
      "[Epoch: 35180] train loss: 0.9311, train acc: 0.5828, val loss: 0.9020, val acc: 0.5956  (best train acc: 0.5881, best val acc: 0.6071)\n",
      "[Epoch: 35200] train loss: 0.9279, train acc: 0.5800, val loss: 0.8993, val acc: 0.5987  (best train acc: 0.5885, best val acc: 0.6071)\n",
      "[Epoch: 35220] train loss: 0.9259, train acc: 0.5817, val loss: 0.8988, val acc: 0.6017  (best train acc: 0.5885, best val acc: 0.6071)\n",
      "[Epoch: 35240] train loss: 0.9288, train acc: 0.5823, val loss: 0.9008, val acc: 0.5973  (best train acc: 0.5885, best val acc: 0.6071)\n",
      "[Epoch: 35260] train loss: 0.9370, train acc: 0.5763, val loss: 0.8999, val acc: 0.5960  (best train acc: 0.5885, best val acc: 0.6071)\n",
      "[Epoch: 35280] train loss: 0.9278, train acc: 0.5834, val loss: 0.9056, val acc: 0.5939  (best train acc: 0.5885, best val acc: 0.6071)\n",
      "[Epoch: 35300] train loss: 0.9315, train acc: 0.5774, val loss: 0.9063, val acc: 0.5926  (best train acc: 0.5885, best val acc: 0.6071)\n",
      "[Epoch: 35320] train loss: 0.9303, train acc: 0.5847, val loss: 0.8998, val acc: 0.6007  (best train acc: 0.5885, best val acc: 0.6071)\n",
      "[Epoch: 35340] train loss: 0.9288, train acc: 0.5792, val loss: 0.9049, val acc: 0.5936  (best train acc: 0.5889, best val acc: 0.6071)\n",
      "[Epoch: 35360] train loss: 0.9385, train acc: 0.5625, val loss: 0.9046, val acc: 0.5899  (best train acc: 0.5889, best val acc: 0.6071)\n",
      "[Epoch: 35380] train loss: 0.9342, train acc: 0.5755, val loss: 0.8995, val acc: 0.5990  (best train acc: 0.5889, best val acc: 0.6071)\n",
      "[Epoch: 35400] train loss: 0.9364, train acc: 0.5802, val loss: 0.9040, val acc: 0.5899  (best train acc: 0.5889, best val acc: 0.6071)\n",
      "[Epoch: 35420] train loss: 0.9248, train acc: 0.5805, val loss: 0.9002, val acc: 0.6040  (best train acc: 0.5889, best val acc: 0.6071)\n",
      "[Epoch: 35440] train loss: 0.9255, train acc: 0.5846, val loss: 0.9002, val acc: 0.6034  (best train acc: 0.5889, best val acc: 0.6071)\n",
      "[Epoch: 35460] train loss: 0.9308, train acc: 0.5807, val loss: 0.9007, val acc: 0.5976  (best train acc: 0.5889, best val acc: 0.6071)\n",
      "[Epoch: 35480] train loss: 0.9248, train acc: 0.5845, val loss: 0.9006, val acc: 0.5993  (best train acc: 0.5889, best val acc: 0.6081)\n",
      "[Epoch: 35500] train loss: 0.9283, train acc: 0.5795, val loss: 0.8984, val acc: 0.6030  (best train acc: 0.5889, best val acc: 0.6081)\n",
      "[Epoch: 35520] train loss: 0.9277, train acc: 0.5779, val loss: 0.9058, val acc: 0.5926  (best train acc: 0.5889, best val acc: 0.6081)\n",
      "[Epoch: 35540] train loss: 0.9289, train acc: 0.5810, val loss: 0.9004, val acc: 0.6003  (best train acc: 0.5889, best val acc: 0.6081)\n",
      "[Epoch: 35560] train loss: 0.9303, train acc: 0.5832, val loss: 0.8991, val acc: 0.6007  (best train acc: 0.5889, best val acc: 0.6081)\n",
      "[Epoch: 35580] train loss: 0.9267, train acc: 0.5805, val loss: 0.8993, val acc: 0.6013  (best train acc: 0.5889, best val acc: 0.6081)\n",
      "[Epoch: 35600] train loss: 0.9294, train acc: 0.5839, val loss: 0.8991, val acc: 0.6017  (best train acc: 0.5889, best val acc: 0.6081)\n",
      "[Epoch: 35620] train loss: 0.9277, train acc: 0.5819, val loss: 0.9011, val acc: 0.6010  (best train acc: 0.5889, best val acc: 0.6081)\n",
      "[Epoch: 35640] train loss: 0.9305, train acc: 0.5808, val loss: 0.8992, val acc: 0.6037  (best train acc: 0.5889, best val acc: 0.6081)\n",
      "[Epoch: 35660] train loss: 0.9389, train acc: 0.5718, val loss: 0.9105, val acc: 0.5841  (best train acc: 0.5889, best val acc: 0.6081)\n",
      "[Epoch: 35680] train loss: 0.9288, train acc: 0.5811, val loss: 0.9051, val acc: 0.6003  (best train acc: 0.5889, best val acc: 0.6081)\n",
      "[Epoch: 35700] train loss: 0.9285, train acc: 0.5864, val loss: 0.8998, val acc: 0.6044  (best train acc: 0.5889, best val acc: 0.6081)\n",
      "[Epoch: 35720] train loss: 0.9303, train acc: 0.5783, val loss: 0.8990, val acc: 0.6013  (best train acc: 0.5889, best val acc: 0.6081)\n",
      "[Epoch: 35740] train loss: 0.9381, train acc: 0.5682, val loss: 0.9013, val acc: 0.6000  (best train acc: 0.5889, best val acc: 0.6081)\n",
      "[Epoch: 35760] train loss: 0.9319, train acc: 0.5795, val loss: 0.9007, val acc: 0.5963  (best train acc: 0.5889, best val acc: 0.6081)\n",
      "[Epoch: 35780] train loss: 0.9252, train acc: 0.5818, val loss: 0.8998, val acc: 0.6013  (best train acc: 0.5889, best val acc: 0.6081)\n",
      "[Epoch: 35800] train loss: 0.9341, train acc: 0.5742, val loss: 0.9054, val acc: 0.5902  (best train acc: 0.5889, best val acc: 0.6081)\n",
      "[Epoch: 35820] train loss: 0.9287, train acc: 0.5774, val loss: 0.9108, val acc: 0.5882  (best train acc: 0.5889, best val acc: 0.6081)\n",
      "[Epoch: 35840] train loss: 0.9322, train acc: 0.5737, val loss: 0.9148, val acc: 0.5818  (best train acc: 0.5889, best val acc: 0.6081)\n",
      "[Epoch: 35860] train loss: 0.9277, train acc: 0.5866, val loss: 0.8992, val acc: 0.5973  (best train acc: 0.5889, best val acc: 0.6081)\n",
      "[Epoch: 35880] train loss: 0.9575, train acc: 0.5631, val loss: 0.9037, val acc: 0.5895  (best train acc: 0.5889, best val acc: 0.6081)\n",
      "[Epoch: 35900] train loss: 0.9292, train acc: 0.5811, val loss: 0.9085, val acc: 0.5841  (best train acc: 0.5889, best val acc: 0.6081)\n",
      "[Epoch: 35920] train loss: 0.9264, train acc: 0.5808, val loss: 0.8992, val acc: 0.5976  (best train acc: 0.5889, best val acc: 0.6081)\n",
      "[Epoch: 35940] train loss: 0.9368, train acc: 0.5771, val loss: 0.9016, val acc: 0.5976  (best train acc: 0.5889, best val acc: 0.6081)\n",
      "[Epoch: 35960] train loss: 0.9297, train acc: 0.5745, val loss: 0.9059, val acc: 0.5899  (best train acc: 0.5889, best val acc: 0.6081)\n",
      "[Epoch: 35980] train loss: 0.9277, train acc: 0.5800, val loss: 0.8973, val acc: 0.6003  (best train acc: 0.5889, best val acc: 0.6081)\n",
      "[Epoch: 36000] train loss: 0.9272, train acc: 0.5797, val loss: 0.8986, val acc: 0.6000  (best train acc: 0.5889, best val acc: 0.6081)\n",
      "[Epoch: 36020] train loss: 0.9392, train acc: 0.5775, val loss: 0.9050, val acc: 0.5939  (best train acc: 0.5889, best val acc: 0.6081)\n",
      "[Epoch: 36040] train loss: 0.9328, train acc: 0.5723, val loss: 0.9100, val acc: 0.5879  (best train acc: 0.5889, best val acc: 0.6081)\n",
      "[Epoch: 36060] train loss: 0.9277, train acc: 0.5822, val loss: 0.8996, val acc: 0.5953  (best train acc: 0.5889, best val acc: 0.6081)\n",
      "[Epoch: 36080] train loss: 0.9250, train acc: 0.5810, val loss: 0.9013, val acc: 0.5960  (best train acc: 0.5889, best val acc: 0.6081)\n",
      "[Epoch: 36100] train loss: 0.9307, train acc: 0.5754, val loss: 0.9124, val acc: 0.5808  (best train acc: 0.5889, best val acc: 0.6081)\n",
      "[Epoch: 36120] train loss: 0.9326, train acc: 0.5712, val loss: 0.9013, val acc: 0.5993  (best train acc: 0.5889, best val acc: 0.6081)\n",
      "[Epoch: 36140] train loss: 0.9330, train acc: 0.5811, val loss: 0.8982, val acc: 0.6034  (best train acc: 0.5889, best val acc: 0.6081)\n",
      "[Epoch: 36160] train loss: 0.9236, train acc: 0.5852, val loss: 0.9056, val acc: 0.5919  (best train acc: 0.5889, best val acc: 0.6081)\n",
      "[Epoch: 36180] train loss: 0.9305, train acc: 0.5751, val loss: 0.9084, val acc: 0.5872  (best train acc: 0.5889, best val acc: 0.6081)\n",
      "[Epoch: 36200] train loss: 0.9271, train acc: 0.5864, val loss: 0.8981, val acc: 0.6040  (best train acc: 0.5889, best val acc: 0.6081)\n",
      "[Epoch: 36220] train loss: 0.9256, train acc: 0.5799, val loss: 0.8982, val acc: 0.6017  (best train acc: 0.5889, best val acc: 0.6081)\n",
      "[Epoch: 36240] train loss: 0.9280, train acc: 0.5830, val loss: 0.9015, val acc: 0.5963  (best train acc: 0.5889, best val acc: 0.6081)\n",
      "[Epoch: 36260] train loss: 0.9331, train acc: 0.5797, val loss: 0.9003, val acc: 0.6034  (best train acc: 0.5889, best val acc: 0.6081)\n",
      "[Epoch: 36280] train loss: 0.9326, train acc: 0.5821, val loss: 0.8994, val acc: 0.6051  (best train acc: 0.5889, best val acc: 0.6081)\n",
      "[Epoch: 36300] train loss: 0.9325, train acc: 0.5793, val loss: 0.8986, val acc: 0.6044  (best train acc: 0.5889, best val acc: 0.6081)\n",
      "[Epoch: 36320] train loss: 0.9259, train acc: 0.5817, val loss: 0.9024, val acc: 0.5987  (best train acc: 0.5889, best val acc: 0.6081)\n",
      "[Epoch: 36340] train loss: 0.9249, train acc: 0.5843, val loss: 0.8994, val acc: 0.6000  (best train acc: 0.5889, best val acc: 0.6081)\n",
      "[Epoch: 36360] train loss: 0.9258, train acc: 0.5815, val loss: 0.9001, val acc: 0.6017  (best train acc: 0.5889, best val acc: 0.6081)\n",
      "[Epoch: 36380] train loss: 0.9247, train acc: 0.5842, val loss: 0.9047, val acc: 0.5912  (best train acc: 0.5889, best val acc: 0.6081)\n",
      "[Epoch: 36400] train loss: 0.9323, train acc: 0.5789, val loss: 0.9065, val acc: 0.5949  (best train acc: 0.5889, best val acc: 0.6081)\n",
      "[Epoch: 36420] train loss: 0.9314, train acc: 0.5774, val loss: 0.9075, val acc: 0.5929  (best train acc: 0.5889, best val acc: 0.6081)\n",
      "[Epoch: 36440] train loss: 0.9322, train acc: 0.5829, val loss: 0.8996, val acc: 0.6037  (best train acc: 0.5889, best val acc: 0.6081)\n",
      "[Epoch: 36460] train loss: 0.9269, train acc: 0.5810, val loss: 0.8996, val acc: 0.6010  (best train acc: 0.5889, best val acc: 0.6081)\n",
      "[Epoch: 36480] train loss: 0.9315, train acc: 0.5755, val loss: 0.9108, val acc: 0.5875  (best train acc: 0.5889, best val acc: 0.6081)\n",
      "[Epoch: 36500] train loss: 0.9250, train acc: 0.5857, val loss: 0.8984, val acc: 0.6044  (best train acc: 0.5889, best val acc: 0.6081)\n",
      "[Epoch: 36520] train loss: 0.9238, train acc: 0.5807, val loss: 0.9014, val acc: 0.5970  (best train acc: 0.5889, best val acc: 0.6084)\n",
      "[Epoch: 36540] train loss: 0.9297, train acc: 0.5754, val loss: 0.9002, val acc: 0.5953  (best train acc: 0.5889, best val acc: 0.6091)\n",
      "[Epoch: 36560] train loss: 0.9309, train acc: 0.5821, val loss: 0.8975, val acc: 0.6007  (best train acc: 0.5889, best val acc: 0.6091)\n",
      "[Epoch: 36580] train loss: 0.9288, train acc: 0.5813, val loss: 0.9004, val acc: 0.5990  (best train acc: 0.5889, best val acc: 0.6091)\n",
      "[Epoch: 36600] train loss: 0.9291, train acc: 0.5749, val loss: 0.9025, val acc: 0.5899  (best train acc: 0.5889, best val acc: 0.6091)\n",
      "[Epoch: 36620] train loss: 0.9239, train acc: 0.5844, val loss: 0.9008, val acc: 0.6020  (best train acc: 0.5889, best val acc: 0.6098)\n",
      "[Epoch: 36640] train loss: 0.9318, train acc: 0.5750, val loss: 0.9087, val acc: 0.5862  (best train acc: 0.5889, best val acc: 0.6098)\n",
      "[Epoch: 36660] train loss: 0.9260, train acc: 0.5843, val loss: 0.9010, val acc: 0.5983  (best train acc: 0.5889, best val acc: 0.6098)\n",
      "[Epoch: 36680] train loss: 0.9234, train acc: 0.5822, val loss: 0.8981, val acc: 0.6000  (best train acc: 0.5889, best val acc: 0.6098)\n",
      "[Epoch: 36700] train loss: 0.9262, train acc: 0.5789, val loss: 0.8996, val acc: 0.6000  (best train acc: 0.5889, best val acc: 0.6098)\n",
      "[Epoch: 36720] train loss: 0.9272, train acc: 0.5824, val loss: 0.9003, val acc: 0.5973  (best train acc: 0.5889, best val acc: 0.6098)\n",
      "[Epoch: 36740] train loss: 0.9259, train acc: 0.5849, val loss: 0.8994, val acc: 0.6013  (best train acc: 0.5889, best val acc: 0.6098)\n",
      "[Epoch: 36760] train loss: 0.9261, train acc: 0.5813, val loss: 0.8988, val acc: 0.6047  (best train acc: 0.5889, best val acc: 0.6098)\n",
      "[Epoch: 36780] train loss: 0.9304, train acc: 0.5794, val loss: 0.9026, val acc: 0.5997  (best train acc: 0.5889, best val acc: 0.6098)\n",
      "[Epoch: 36800] train loss: 0.9250, train acc: 0.5831, val loss: 0.9027, val acc: 0.5922  (best train acc: 0.5889, best val acc: 0.6098)\n",
      "[Epoch: 36820] train loss: 0.9350, train acc: 0.5774, val loss: 0.9077, val acc: 0.6024  (best train acc: 0.5889, best val acc: 0.6098)\n",
      "[Epoch: 36840] train loss: 0.9293, train acc: 0.5793, val loss: 0.8981, val acc: 0.6003  (best train acc: 0.5889, best val acc: 0.6098)\n",
      "[Epoch: 36860] train loss: 0.9310, train acc: 0.5765, val loss: 0.8983, val acc: 0.6067  (best train acc: 0.5889, best val acc: 0.6098)\n",
      "[Epoch: 36880] train loss: 0.9301, train acc: 0.5761, val loss: 0.8978, val acc: 0.6054  (best train acc: 0.5889, best val acc: 0.6098)\n",
      "[Epoch: 36900] train loss: 0.9237, train acc: 0.5836, val loss: 0.8997, val acc: 0.6044  (best train acc: 0.5889, best val acc: 0.6098)\n",
      "[Epoch: 36920] train loss: 0.9371, train acc: 0.5682, val loss: 0.9242, val acc: 0.5710  (best train acc: 0.5889, best val acc: 0.6098)\n",
      "[Epoch: 36940] train loss: 0.9400, train acc: 0.5721, val loss: 0.9111, val acc: 0.5926  (best train acc: 0.5889, best val acc: 0.6098)\n",
      "[Epoch: 36960] train loss: 0.9313, train acc: 0.5786, val loss: 0.9025, val acc: 0.5956  (best train acc: 0.5889, best val acc: 0.6098)\n",
      "[Epoch: 36980] train loss: 0.9269, train acc: 0.5826, val loss: 0.9014, val acc: 0.5987  (best train acc: 0.5889, best val acc: 0.6098)\n",
      "[Epoch: 37000] train loss: 0.9294, train acc: 0.5816, val loss: 0.8997, val acc: 0.6034  (best train acc: 0.5889, best val acc: 0.6098)\n",
      "[Epoch: 37020] train loss: 0.9262, train acc: 0.5792, val loss: 0.8999, val acc: 0.6013  (best train acc: 0.5889, best val acc: 0.6098)\n",
      "[Epoch: 37040] train loss: 0.9302, train acc: 0.5769, val loss: 0.8977, val acc: 0.6054  (best train acc: 0.5889, best val acc: 0.6098)\n",
      "[Epoch: 37060] train loss: 0.9263, train acc: 0.5829, val loss: 0.9005, val acc: 0.5949  (best train acc: 0.5889, best val acc: 0.6098)\n",
      "[Epoch: 37080] train loss: 0.9277, train acc: 0.5826, val loss: 0.8986, val acc: 0.6017  (best train acc: 0.5889, best val acc: 0.6098)\n",
      "[Epoch: 37100] train loss: 0.9282, train acc: 0.5797, val loss: 0.8991, val acc: 0.6010  (best train acc: 0.5889, best val acc: 0.6098)\n",
      "[Epoch: 37120] train loss: 0.9259, train acc: 0.5813, val loss: 0.8996, val acc: 0.6024  (best train acc: 0.5889, best val acc: 0.6098)\n",
      "[Epoch: 37140] train loss: 0.9274, train acc: 0.5847, val loss: 0.8984, val acc: 0.6010  (best train acc: 0.5889, best val acc: 0.6098)\n",
      "[Epoch: 37160] train loss: 0.9266, train acc: 0.5805, val loss: 0.9018, val acc: 0.5987  (best train acc: 0.5889, best val acc: 0.6098)\n",
      "[Epoch: 37180] train loss: 0.9257, train acc: 0.5795, val loss: 0.8978, val acc: 0.6044  (best train acc: 0.5889, best val acc: 0.6098)\n",
      "[Epoch: 37200] train loss: 0.9331, train acc: 0.5722, val loss: 0.9104, val acc: 0.5868  (best train acc: 0.5889, best val acc: 0.6098)\n",
      "[Epoch: 37220] train loss: 0.9333, train acc: 0.5847, val loss: 0.9002, val acc: 0.5946  (best train acc: 0.5889, best val acc: 0.6098)\n",
      "[Epoch: 37240] train loss: 0.9282, train acc: 0.5803, val loss: 0.9035, val acc: 0.5960  (best train acc: 0.5889, best val acc: 0.6098)\n",
      "[Epoch: 37260] train loss: 0.9298, train acc: 0.5767, val loss: 0.8980, val acc: 0.6094  (best train acc: 0.5889, best val acc: 0.6098)\n",
      "[Epoch: 37280] train loss: 0.9325, train acc: 0.5781, val loss: 0.9142, val acc: 0.5831  (best train acc: 0.5889, best val acc: 0.6098)\n",
      "[Epoch: 37300] train loss: 0.9257, train acc: 0.5813, val loss: 0.8992, val acc: 0.5987  (best train acc: 0.5889, best val acc: 0.6098)\n",
      "[Epoch: 37320] train loss: 0.9624, train acc: 0.5504, val loss: 0.9157, val acc: 0.5788  (best train acc: 0.5889, best val acc: 0.6098)\n",
      "[Epoch: 37340] train loss: 0.9244, train acc: 0.5807, val loss: 0.8992, val acc: 0.6000  (best train acc: 0.5889, best val acc: 0.6098)\n",
      "[Epoch: 37360] train loss: 0.9228, train acc: 0.5846, val loss: 0.8981, val acc: 0.5990  (best train acc: 0.5889, best val acc: 0.6098)\n",
      "[Epoch: 37380] train loss: 0.9266, train acc: 0.5787, val loss: 0.8995, val acc: 0.5980  (best train acc: 0.5889, best val acc: 0.6101)\n",
      "[Epoch: 37400] train loss: 0.9374, train acc: 0.5700, val loss: 0.9051, val acc: 0.5892  (best train acc: 0.5889, best val acc: 0.6101)\n",
      "[Epoch: 37420] train loss: 0.9298, train acc: 0.5807, val loss: 0.9010, val acc: 0.5997  (best train acc: 0.5889, best val acc: 0.6101)\n",
      "[Epoch: 37440] train loss: 0.9398, train acc: 0.5682, val loss: 0.9261, val acc: 0.5629  (best train acc: 0.5889, best val acc: 0.6101)\n",
      "[Epoch: 37460] train loss: 0.9247, train acc: 0.5836, val loss: 0.9136, val acc: 0.5868  (best train acc: 0.5889, best val acc: 0.6101)\n",
      "[Epoch: 37480] train loss: 0.9213, train acc: 0.5840, val loss: 0.8998, val acc: 0.6010  (best train acc: 0.5889, best val acc: 0.6101)\n",
      "[Epoch: 37500] train loss: 0.9428, train acc: 0.5690, val loss: 0.8983, val acc: 0.5953  (best train acc: 0.5889, best val acc: 0.6101)\n",
      "[Epoch: 37520] train loss: 0.9284, train acc: 0.5816, val loss: 0.9091, val acc: 0.5825  (best train acc: 0.5889, best val acc: 0.6101)\n",
      "[Epoch: 37540] train loss: 0.9301, train acc: 0.5753, val loss: 0.9018, val acc: 0.5933  (best train acc: 0.5889, best val acc: 0.6101)\n",
      "[Epoch: 37560] train loss: 0.9268, train acc: 0.5803, val loss: 0.9003, val acc: 0.5990  (best train acc: 0.5889, best val acc: 0.6101)\n",
      "[Epoch: 37580] train loss: 0.9283, train acc: 0.5788, val loss: 0.8980, val acc: 0.6061  (best train acc: 0.5889, best val acc: 0.6101)\n",
      "[Epoch: 37600] train loss: 0.9288, train acc: 0.5792, val loss: 0.8994, val acc: 0.5993  (best train acc: 0.5889, best val acc: 0.6101)\n",
      "[Epoch: 37620] train loss: 0.9244, train acc: 0.5870, val loss: 0.8970, val acc: 0.6057  (best train acc: 0.5889, best val acc: 0.6101)\n",
      "[Epoch: 37640] train loss: 0.9215, train acc: 0.5837, val loss: 0.8985, val acc: 0.5956  (best train acc: 0.5889, best val acc: 0.6101)\n",
      "[Epoch: 37660] train loss: 0.9307, train acc: 0.5774, val loss: 0.8997, val acc: 0.5946  (best train acc: 0.5889, best val acc: 0.6101)\n",
      "[Epoch: 37680] train loss: 0.9216, train acc: 0.5876, val loss: 0.9002, val acc: 0.6013  (best train acc: 0.5889, best val acc: 0.6101)\n",
      "[Epoch: 37700] train loss: 0.9291, train acc: 0.5787, val loss: 0.9053, val acc: 0.5926  (best train acc: 0.5889, best val acc: 0.6101)\n",
      "[Epoch: 37720] train loss: 0.9259, train acc: 0.5813, val loss: 0.8979, val acc: 0.6024  (best train acc: 0.5889, best val acc: 0.6101)\n",
      "[Epoch: 37740] train loss: 0.9269, train acc: 0.5807, val loss: 0.8986, val acc: 0.6051  (best train acc: 0.5896, best val acc: 0.6101)\n",
      "[Epoch: 37760] train loss: 0.9383, train acc: 0.5700, val loss: 0.9178, val acc: 0.5734  (best train acc: 0.5896, best val acc: 0.6101)\n",
      "[Epoch: 37780] train loss: 0.9447, train acc: 0.5693, val loss: 0.9043, val acc: 0.5929  (best train acc: 0.5896, best val acc: 0.6101)\n",
      "[Epoch: 37800] train loss: 0.9302, train acc: 0.5824, val loss: 0.9038, val acc: 0.5926  (best train acc: 0.5896, best val acc: 0.6101)\n",
      "[Epoch: 37820] train loss: 0.9248, train acc: 0.5837, val loss: 0.8999, val acc: 0.6037  (best train acc: 0.5896, best val acc: 0.6101)\n",
      "[Epoch: 37840] train loss: 0.9289, train acc: 0.5822, val loss: 0.8980, val acc: 0.6010  (best train acc: 0.5896, best val acc: 0.6101)\n",
      "[Epoch: 37860] train loss: 0.9236, train acc: 0.5823, val loss: 0.9024, val acc: 0.5993  (best train acc: 0.5896, best val acc: 0.6101)\n",
      "[Epoch: 37880] train loss: 0.9231, train acc: 0.5851, val loss: 0.8967, val acc: 0.5987  (best train acc: 0.5896, best val acc: 0.6105)\n",
      "[Epoch: 37900] train loss: 0.9256, train acc: 0.5839, val loss: 0.8983, val acc: 0.5983  (best train acc: 0.5896, best val acc: 0.6105)\n",
      "[Epoch: 37920] train loss: 0.9285, train acc: 0.5780, val loss: 0.9077, val acc: 0.5916  (best train acc: 0.5896, best val acc: 0.6105)\n",
      "[Epoch: 37940] train loss: 0.9284, train acc: 0.5750, val loss: 0.9034, val acc: 0.5912  (best train acc: 0.5896, best val acc: 0.6105)\n",
      "[Epoch: 37960] train loss: 0.9259, train acc: 0.5782, val loss: 0.8983, val acc: 0.6030  (best train acc: 0.5896, best val acc: 0.6105)\n",
      "[Epoch: 37980] train loss: 0.9288, train acc: 0.5830, val loss: 0.8997, val acc: 0.6000  (best train acc: 0.5896, best val acc: 0.6105)\n",
      "[Epoch: 38000] train loss: 0.9288, train acc: 0.5825, val loss: 0.8972, val acc: 0.6040  (best train acc: 0.5896, best val acc: 0.6105)\n",
      "[Epoch: 38020] train loss: 0.9642, train acc: 0.5449, val loss: 0.9475, val acc: 0.5501  (best train acc: 0.5896, best val acc: 0.6105)\n",
      "[Epoch: 38040] train loss: 0.9310, train acc: 0.5813, val loss: 0.9008, val acc: 0.5993  (best train acc: 0.5896, best val acc: 0.6105)\n",
      "[Epoch: 38060] train loss: 0.9266, train acc: 0.5828, val loss: 0.8998, val acc: 0.5997  (best train acc: 0.5896, best val acc: 0.6105)\n",
      "[Epoch: 38080] train loss: 0.9296, train acc: 0.5826, val loss: 0.9264, val acc: 0.5622  (best train acc: 0.5896, best val acc: 0.6105)\n",
      "[Epoch: 38100] train loss: 0.9264, train acc: 0.5789, val loss: 0.9034, val acc: 0.5997  (best train acc: 0.5896, best val acc: 0.6105)\n",
      "[Epoch: 38120] train loss: 0.9320, train acc: 0.5816, val loss: 0.8976, val acc: 0.6010  (best train acc: 0.5896, best val acc: 0.6105)\n",
      "[Epoch: 38140] train loss: 0.9409, train acc: 0.5737, val loss: 0.9066, val acc: 0.5885  (best train acc: 0.5896, best val acc: 0.6105)\n",
      "[Epoch: 38160] train loss: 0.9259, train acc: 0.5789, val loss: 0.8988, val acc: 0.6064  (best train acc: 0.5896, best val acc: 0.6105)\n",
      "[Epoch: 38180] train loss: 0.9246, train acc: 0.5831, val loss: 0.8972, val acc: 0.6044  (best train acc: 0.5896, best val acc: 0.6105)\n",
      "[Epoch: 38200] train loss: 0.9248, train acc: 0.5847, val loss: 0.8988, val acc: 0.5966  (best train acc: 0.5896, best val acc: 0.6105)\n",
      "[Epoch: 38220] train loss: 0.9262, train acc: 0.5881, val loss: 0.8974, val acc: 0.6054  (best train acc: 0.5896, best val acc: 0.6105)\n",
      "[Epoch: 38240] train loss: 0.9255, train acc: 0.5828, val loss: 0.9052, val acc: 0.5919  (best train acc: 0.5896, best val acc: 0.6105)\n",
      "[Epoch: 38260] train loss: 0.9237, train acc: 0.5813, val loss: 0.8979, val acc: 0.6000  (best train acc: 0.5896, best val acc: 0.6105)\n",
      "[Epoch: 38280] train loss: 0.9235, train acc: 0.5845, val loss: 0.8972, val acc: 0.6034  (best train acc: 0.5896, best val acc: 0.6105)\n",
      "[Epoch: 38300] train loss: 0.9226, train acc: 0.5880, val loss: 0.8990, val acc: 0.6034  (best train acc: 0.5896, best val acc: 0.6105)\n",
      "[Epoch: 38320] train loss: 0.9307, train acc: 0.5802, val loss: 0.9058, val acc: 0.5906  (best train acc: 0.5896, best val acc: 0.6105)\n",
      "[Epoch: 38340] train loss: 0.9228, train acc: 0.5842, val loss: 0.9006, val acc: 0.5970  (best train acc: 0.5896, best val acc: 0.6105)\n",
      "[Epoch: 38360] train loss: 0.9273, train acc: 0.5811, val loss: 0.9082, val acc: 0.5858  (best train acc: 0.5896, best val acc: 0.6105)\n",
      "[Epoch: 38380] train loss: 0.9260, train acc: 0.5855, val loss: 0.9003, val acc: 0.6067  (best train acc: 0.5896, best val acc: 0.6105)\n",
      "[Epoch: 38400] train loss: 0.9267, train acc: 0.5863, val loss: 0.8975, val acc: 0.6010  (best train acc: 0.5896, best val acc: 0.6105)\n",
      "[Epoch: 38420] train loss: 0.9353, train acc: 0.5774, val loss: 0.8993, val acc: 0.6017  (best train acc: 0.5896, best val acc: 0.6105)\n",
      "[Epoch: 38440] train loss: 0.9341, train acc: 0.5805, val loss: 0.9125, val acc: 0.5831  (best train acc: 0.5896, best val acc: 0.6105)\n",
      "[Epoch: 38460] train loss: 0.9299, train acc: 0.5776, val loss: 0.8978, val acc: 0.6064  (best train acc: 0.5896, best val acc: 0.6105)\n",
      "[Epoch: 38480] train loss: 0.9306, train acc: 0.5766, val loss: 0.9017, val acc: 0.5973  (best train acc: 0.5896, best val acc: 0.6105)\n",
      "[Epoch: 38500] train loss: 0.9292, train acc: 0.5862, val loss: 0.9000, val acc: 0.6030  (best train acc: 0.5896, best val acc: 0.6105)\n",
      "[Epoch: 38520] train loss: 0.9272, train acc: 0.5851, val loss: 0.9068, val acc: 0.5916  (best train acc: 0.5896, best val acc: 0.6105)\n",
      "[Epoch: 38540] train loss: 0.9225, train acc: 0.5834, val loss: 0.9100, val acc: 0.5801  (best train acc: 0.5896, best val acc: 0.6115)\n",
      "[Epoch: 38560] train loss: 0.9241, train acc: 0.5849, val loss: 0.9003, val acc: 0.5980  (best train acc: 0.5896, best val acc: 0.6115)\n",
      "[Epoch: 38580] train loss: 0.9425, train acc: 0.5677, val loss: 0.9128, val acc: 0.5801  (best train acc: 0.5896, best val acc: 0.6115)\n",
      "[Epoch: 38600] train loss: 0.9333, train acc: 0.5788, val loss: 0.8990, val acc: 0.6007  (best train acc: 0.5896, best val acc: 0.6115)\n",
      "[Epoch: 38620] train loss: 0.9305, train acc: 0.5745, val loss: 0.8968, val acc: 0.6044  (best train acc: 0.5896, best val acc: 0.6115)\n",
      "[Epoch: 38640] train loss: 0.9246, train acc: 0.5805, val loss: 0.8968, val acc: 0.6020  (best train acc: 0.5896, best val acc: 0.6115)\n",
      "[Epoch: 38660] train loss: 0.9322, train acc: 0.5773, val loss: 0.9071, val acc: 0.5879  (best train acc: 0.5896, best val acc: 0.6115)\n",
      "[Epoch: 38680] train loss: 0.9277, train acc: 0.5827, val loss: 0.8971, val acc: 0.6034  (best train acc: 0.5896, best val acc: 0.6115)\n",
      "[Epoch: 38700] train loss: 0.9291, train acc: 0.5787, val loss: 0.8985, val acc: 0.5993  (best train acc: 0.5896, best val acc: 0.6115)\n",
      "[Epoch: 38720] train loss: 0.9272, train acc: 0.5796, val loss: 0.8981, val acc: 0.6061  (best train acc: 0.5896, best val acc: 0.6115)\n",
      "[Epoch: 38740] train loss: 0.9233, train acc: 0.5840, val loss: 0.8963, val acc: 0.6030  (best train acc: 0.5896, best val acc: 0.6115)\n",
      "[Epoch: 38760] train loss: 0.9249, train acc: 0.5844, val loss: 0.8975, val acc: 0.6057  (best train acc: 0.5896, best val acc: 0.6115)\n",
      "[Epoch: 38780] train loss: 0.9248, train acc: 0.5830, val loss: 0.8962, val acc: 0.6057  (best train acc: 0.5896, best val acc: 0.6115)\n",
      "[Epoch: 38800] train loss: 0.9256, train acc: 0.5831, val loss: 0.8976, val acc: 0.6013  (best train acc: 0.5896, best val acc: 0.6115)\n",
      "[Epoch: 38820] train loss: 0.9291, train acc: 0.5810, val loss: 0.8962, val acc: 0.6064  (best train acc: 0.5896, best val acc: 0.6115)\n",
      "[Epoch: 38840] train loss: 0.9259, train acc: 0.5813, val loss: 0.8973, val acc: 0.6037  (best train acc: 0.5896, best val acc: 0.6115)\n",
      "[Epoch: 38860] train loss: 0.9226, train acc: 0.5879, val loss: 0.8995, val acc: 0.5976  (best train acc: 0.5896, best val acc: 0.6115)\n",
      "[Epoch: 38880] train loss: 0.9251, train acc: 0.5853, val loss: 0.8981, val acc: 0.6078  (best train acc: 0.5896, best val acc: 0.6115)\n",
      "[Epoch: 38900] train loss: 0.9280, train acc: 0.5780, val loss: 0.9024, val acc: 0.5949  (best train acc: 0.5896, best val acc: 0.6115)\n",
      "[Epoch: 38920] train loss: 0.9276, train acc: 0.5816, val loss: 0.9015, val acc: 0.5987  (best train acc: 0.5896, best val acc: 0.6115)\n",
      "[Epoch: 38940] train loss: 0.9254, train acc: 0.5873, val loss: 0.8994, val acc: 0.6000  (best train acc: 0.5896, best val acc: 0.6115)\n",
      "[Epoch: 38960] train loss: 0.9208, train acc: 0.5830, val loss: 0.8974, val acc: 0.6054  (best train acc: 0.5896, best val acc: 0.6115)\n",
      "[Epoch: 38980] train loss: 0.9250, train acc: 0.5862, val loss: 0.8959, val acc: 0.6061  (best train acc: 0.5896, best val acc: 0.6115)\n",
      "[Epoch: 39000] train loss: 0.9228, train acc: 0.5874, val loss: 0.8971, val acc: 0.6054  (best train acc: 0.5896, best val acc: 0.6115)\n",
      "[Epoch: 39020] train loss: 0.9278, train acc: 0.5789, val loss: 0.8978, val acc: 0.5987  (best train acc: 0.5896, best val acc: 0.6115)\n",
      "[Epoch: 39040] train loss: 0.9254, train acc: 0.5849, val loss: 0.8977, val acc: 0.5976  (best train acc: 0.5896, best val acc: 0.6115)\n",
      "[Epoch: 39060] train loss: 0.9268, train acc: 0.5825, val loss: 0.8979, val acc: 0.6088  (best train acc: 0.5896, best val acc: 0.6115)\n",
      "[Epoch: 39080] train loss: 0.9276, train acc: 0.5840, val loss: 0.8972, val acc: 0.6051  (best train acc: 0.5896, best val acc: 0.6115)\n",
      "[Epoch: 39100] train loss: 0.9224, train acc: 0.5830, val loss: 0.8988, val acc: 0.6051  (best train acc: 0.5896, best val acc: 0.6115)\n",
      "[Epoch: 39120] train loss: 0.9333, train acc: 0.5740, val loss: 0.9104, val acc: 0.5815  (best train acc: 0.5896, best val acc: 0.6115)\n",
      "[Epoch: 39140] train loss: 0.9530, train acc: 0.5617, val loss: 0.9019, val acc: 0.5909  (best train acc: 0.5896, best val acc: 0.6115)\n",
      "[Epoch: 39160] train loss: 0.9364, train acc: 0.5782, val loss: 0.9010, val acc: 0.5960  (best train acc: 0.5896, best val acc: 0.6115)\n",
      "[Epoch: 39180] train loss: 0.9250, train acc: 0.5854, val loss: 0.9034, val acc: 0.5987  (best train acc: 0.5896, best val acc: 0.6115)\n",
      "[Epoch: 39200] train loss: 0.9306, train acc: 0.5761, val loss: 0.8966, val acc: 0.6037  (best train acc: 0.5896, best val acc: 0.6115)\n",
      "[Epoch: 39220] train loss: 0.9252, train acc: 0.5829, val loss: 0.8996, val acc: 0.5997  (best train acc: 0.5896, best val acc: 0.6115)\n",
      "[Epoch: 39240] train loss: 0.9240, train acc: 0.5825, val loss: 0.8983, val acc: 0.5966  (best train acc: 0.5896, best val acc: 0.6115)\n",
      "[Epoch: 39260] train loss: 0.9272, train acc: 0.5845, val loss: 0.9014, val acc: 0.6013  (best train acc: 0.5896, best val acc: 0.6115)\n",
      "[Epoch: 39280] train loss: 0.9234, train acc: 0.5830, val loss: 0.8977, val acc: 0.6037  (best train acc: 0.5896, best val acc: 0.6128)\n",
      "[Epoch: 39300] train loss: 0.9271, train acc: 0.5826, val loss: 0.8964, val acc: 0.6084  (best train acc: 0.5896, best val acc: 0.6128)\n",
      "[Epoch: 39320] train loss: 0.9280, train acc: 0.5844, val loss: 0.8943, val acc: 0.6111  (best train acc: 0.5896, best val acc: 0.6128)\n",
      "[Epoch: 39340] train loss: 0.9256, train acc: 0.5865, val loss: 0.8986, val acc: 0.5970  (best train acc: 0.5896, best val acc: 0.6128)\n",
      "[Epoch: 39360] train loss: 0.9269, train acc: 0.5794, val loss: 0.8963, val acc: 0.6037  (best train acc: 0.5896, best val acc: 0.6128)\n",
      "[Epoch: 39380] train loss: 0.9243, train acc: 0.5854, val loss: 0.8968, val acc: 0.6067  (best train acc: 0.5896, best val acc: 0.6128)\n",
      "[Epoch: 39400] train loss: 0.9278, train acc: 0.5860, val loss: 0.8948, val acc: 0.5990  (best train acc: 0.5896, best val acc: 0.6128)\n",
      "[Epoch: 39420] train loss: 0.9293, train acc: 0.5847, val loss: 0.8984, val acc: 0.6027  (best train acc: 0.5896, best val acc: 0.6128)\n",
      "[Epoch: 39440] train loss: 0.9255, train acc: 0.5851, val loss: 0.8975, val acc: 0.5993  (best train acc: 0.5914, best val acc: 0.6128)\n",
      "[Epoch: 39460] train loss: 0.9249, train acc: 0.5836, val loss: 0.8967, val acc: 0.6057  (best train acc: 0.5914, best val acc: 0.6132)\n",
      "[Epoch: 39480] train loss: 0.9258, train acc: 0.5869, val loss: 0.8973, val acc: 0.5997  (best train acc: 0.5914, best val acc: 0.6132)\n",
      "[Epoch: 39500] train loss: 0.9198, train acc: 0.5883, val loss: 0.8971, val acc: 0.6044  (best train acc: 0.5914, best val acc: 0.6132)\n",
      "[Epoch: 39520] train loss: 0.9327, train acc: 0.5771, val loss: 0.8993, val acc: 0.5960  (best train acc: 0.5914, best val acc: 0.6132)\n",
      "[Epoch: 39540] train loss: 0.9274, train acc: 0.5797, val loss: 0.9007, val acc: 0.5987  (best train acc: 0.5914, best val acc: 0.6132)\n",
      "[Epoch: 39560] train loss: 0.9465, train acc: 0.5574, val loss: 0.9286, val acc: 0.5589  (best train acc: 0.5914, best val acc: 0.6132)\n",
      "[Epoch: 39580] train loss: 0.9310, train acc: 0.5822, val loss: 0.8984, val acc: 0.6064  (best train acc: 0.5914, best val acc: 0.6132)\n",
      "[Epoch: 39600] train loss: 0.9279, train acc: 0.5845, val loss: 0.8970, val acc: 0.6078  (best train acc: 0.5914, best val acc: 0.6132)\n",
      "[Epoch: 39620] train loss: 0.9269, train acc: 0.5801, val loss: 0.9048, val acc: 0.5845  (best train acc: 0.5914, best val acc: 0.6132)\n",
      "[Epoch: 39640] train loss: 0.9384, train acc: 0.5719, val loss: 0.8977, val acc: 0.6074  (best train acc: 0.5914, best val acc: 0.6132)\n",
      "[Epoch: 39660] train loss: 0.9306, train acc: 0.5841, val loss: 0.9110, val acc: 0.5788  (best train acc: 0.5914, best val acc: 0.6132)\n",
      "[Epoch: 39680] train loss: 0.9296, train acc: 0.5841, val loss: 0.9006, val acc: 0.5936  (best train acc: 0.5914, best val acc: 0.6132)\n",
      "[Epoch: 39700] train loss: 0.9469, train acc: 0.5583, val loss: 0.9151, val acc: 0.5754  (best train acc: 0.5914, best val acc: 0.6132)\n",
      "[Epoch: 39720] train loss: 0.9235, train acc: 0.5862, val loss: 0.8978, val acc: 0.6057  (best train acc: 0.5914, best val acc: 0.6132)\n",
      "[Epoch: 39740] train loss: 0.9247, train acc: 0.5816, val loss: 0.9073, val acc: 0.5912  (best train acc: 0.5914, best val acc: 0.6132)\n",
      "[Epoch: 39760] train loss: 0.9231, train acc: 0.5849, val loss: 0.8956, val acc: 0.6047  (best train acc: 0.5914, best val acc: 0.6132)\n",
      "[Epoch: 39780] train loss: 0.9351, train acc: 0.5764, val loss: 0.9056, val acc: 0.5841  (best train acc: 0.5914, best val acc: 0.6132)\n",
      "[Epoch: 39800] train loss: 0.9242, train acc: 0.5800, val loss: 0.8965, val acc: 0.6044  (best train acc: 0.5914, best val acc: 0.6132)\n",
      "[Epoch: 39820] train loss: 0.9253, train acc: 0.5787, val loss: 0.8952, val acc: 0.6051  (best train acc: 0.5914, best val acc: 0.6132)\n",
      "[Epoch: 39840] train loss: 0.9239, train acc: 0.5846, val loss: 0.8985, val acc: 0.6003  (best train acc: 0.5914, best val acc: 0.6132)\n",
      "[Epoch: 39860] train loss: 0.9306, train acc: 0.5813, val loss: 0.9034, val acc: 0.5949  (best train acc: 0.5914, best val acc: 0.6132)\n",
      "[Epoch: 39880] train loss: 0.9262, train acc: 0.5870, val loss: 0.8971, val acc: 0.6040  (best train acc: 0.5914, best val acc: 0.6132)\n",
      "[Epoch: 39900] train loss: 0.9205, train acc: 0.5907, val loss: 0.8953, val acc: 0.6013  (best train acc: 0.5914, best val acc: 0.6132)\n",
      "[Epoch: 39920] train loss: 0.9303, train acc: 0.5789, val loss: 0.9000, val acc: 0.6020  (best train acc: 0.5914, best val acc: 0.6132)\n",
      "[Epoch: 39940] train loss: 0.9250, train acc: 0.5826, val loss: 0.8961, val acc: 0.6013  (best train acc: 0.5914, best val acc: 0.6132)\n",
      "[Epoch: 39960] train loss: 0.9285, train acc: 0.5775, val loss: 0.8991, val acc: 0.5956  (best train acc: 0.5914, best val acc: 0.6132)\n",
      "[Epoch: 39980] train loss: 0.9221, train acc: 0.5826, val loss: 0.8986, val acc: 0.5997  (best train acc: 0.5914, best val acc: 0.6132)\n",
      "[Epoch: 40000] train loss: 0.9219, train acc: 0.5849, val loss: 0.8982, val acc: 0.5997  (best train acc: 0.5914, best val acc: 0.6132)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtQAAAGpCAYAAACteaFyAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAAdy0lEQVR4nO3df7Cld10f8Pfn3t1swk+JLDHmhwmacQpWEXYiSsexYEtES7CKxiltxjKTjsUCtdaGOlN/TNNixzLUKnaigvEnhqJDpLaaiaLjFAmJBCRgTPi9JmQDFiEKSXb30z/Oc7Inm3vv3vs8e/bem329Zs6c53zPc57zOd/73Lvv/Z7neb7V3QEAAMZZ2e4CAABgNxOoAQBgAoEaAAAmEKgBAGACgRoAACbYs90FTPHUpz61L7roou0uAwCAx7hbb731U929f63ndnWgvuiii3LLLbdsdxkAADzGVdXH1nvOIR8AADCBQA0AABMI1AAAMIFADQAAEwjUAAAwgUANAAATCNQAADCBQA0AABMI1AAAMIFADQAAEwjUAAAwgUANAAATCNQAADCBQA0AABMI1CPc8cnP5dDnvrDdZQAAsAMI1CN8y3/7o/zyOz+23WUAALADCNQAADCBQA0AABMI1AAAMIFADQAAEwjUI3VvdwUAAOwEAvUIVbXdJQAAsEMI1AAAMIFADQAAEwjUAAAwgUA9UsdZiQAACNSjOCURAIA5gRoAACYQqAEAYAKBGgAAJhCoAQBgAoF6JFOPAwCQCNSjmHkcAIA5gRoAACYQqAEAYAKBGgAAJhCoR3JOIgAAiUA9Spl8HACAgUANAAATCNQAADCBQA0AABMI1COZKREAgESgHsc5iQAADARqAACYQKAGAIAJBGoAAJhAoAYAgAkE6pHa5OMAAESgHsVFPgAAmBOoAQBggqUH6qparar3VNXbh8dnV9WNVXXncP+UhXVfU1V3VdUdVfXCZdcGAABTnYoR6lcl+eDC46uT3NTdlyS5aXicqnpGkiuSPDPJZUneUFWrp6A+AAAYbamBuqrOT/KtSX5+ofnyJNcNy9cleclC+5u7+4Hu/kiSu5Jcusz6JnFOIgAAWf4I9euT/FCSowtt53T3PUky3D9taD8vyScW1js4tD1CVV1VVbdU1S333XffUoo+kXJWIgAAg6UF6qr6tiSHuvvWzb5kjbZHjQN397XdfaC7D+zfv39SjQAAMNWeJW77eUleXFUvSnJmkidV1a8kubeqzu3ue6rq3CSHhvUPJrlg4fXnJ7l7ifUBAMBkSxuh7u7XdPf53X1RZicb/n53vyzJDUmuHFa7MsnbhuUbklxRVfuq6uIklyS5eVn1AQDAybDMEer1vDbJ9VX18iQfT/LSJOnu26vq+iQfSHI4ySu6+8g21AcAAJt2SgJ1d78jyTuG5U8necE6612T5JpTUdNULvIBAEBipsRRyuTjAAAMBGoAAJhAoAYAgAkEagAAmECgHqnbaYkAAAjUo5h6HACAOYEaAAAmEKgBAGACgRoAACYQqEdyTiIAAIlAPYpzEgEAmBOoAQBgAoEaAAAmEKgBAGACgRoAACYQqEdykQ8AABKBepQy9zgAAAOBGgAAJhCoAQBgAoEaAAAmEKhHMvU4AACJQD2KUxIBAJgTqAEAYAKBGgAAJhCoAQBgAoEaAAAmEKhHapOPAwAQgXocl/kAAGAgUAMAwAQCNQAATCBQAwDABAL1SKYeBwAgEahHcU4iAABzAjUAAEwgUAMAwAQCNQAATCBQAwDABAL1CFVOSwQAYEagBgCACQRqAACYQKAGAIAJBGoAAJhAoB6pzT0OAEAE6lFc5AMAgDmBGgAAJhCoAQBgAoEaAAAmEKhHckoiAACJQD2KcxIBAJgTqAEAYAKBGgAAJhCoAQBgAoEaAAAmEKhHMvM4AACJQD1KmXscAICBQA0AABMI1AAAMIFADQAAEwjUI7XJxwEAiEA9ilMSAQCYE6gBAGACgRoAACYQqAEAYAKBeiQzJQIAkAjUo5goEQCAOYEaAAAmEKgBAGACgRoAACZYWqCuqjOr6uaqem9V3V5VPza0n11VN1bVncP9UxZe85qququq7qiqFy6rNgAAOFmWOUL9QJLnd/fXJHlWksuq6rlJrk5yU3dfkuSm4XGq6hlJrkjyzCSXJXlDVa0usb5JXOQDAIBkiYG6Z+4fHu4dbp3k8iTXDe3XJXnJsHx5kjd39wPd/ZEkdyW5dFn1TeMyHwAAzCz1GOqqWq2q25IcSnJjd78ryTndfU+SDPdPG1Y/L8knFl5+cGgDAIAda6mBuruPdPezkpyf5NKq+qoNVl9r2PdRR1ZU1VVVdUtV3XLfffedpEoBAGCcU3KVj+7+TJJ3ZHZs9L1VdW6SDPeHhtUOJrlg4WXnJ7l7jW1d290HuvvA/v37l1k2AACc0DKv8rG/qr5oWD4ryTcn+fMkNyS5cljtyiRvG5ZvSHJFVe2rqouTXJLk5mXVN5WpxwEASJI9S9z2uUmuG67UsZLk+u5+e1W9M8n1VfXyJB9P8tIk6e7bq+r6JB9IcjjJK7r7yBLrG83U4wAAzC0tUHf3+5J87Rrtn07ygnVec02Sa5ZVEwAAnGxmSgQAgAkEagAAmECgHs1ZiQAACNQAADCJQD2Ci3wAADAnUAMAwAQCNQAATCBQAwDABAL1SKYeBwAgEahHMfU4AABzAjUAAEwgUAMAwAQCNQAATCBQj+SkRAAAEoF6lDJXIgAAA4EaAAAmEKgBAGACgRoAACYQqAEAYAKBeqSOy3wAACBQj2LqcQAA5gRqAACYQKAGAIAJBGoAAJhAoB7J1OMAACQC9SjOSQQAYE6gBgCACQRqAACYQKAGAIAJBOqRnJMIAECyiUBdVV9eVfuG5W+qqldW1RctvTIAANgFNjNC/dYkR6rqK5L8QpKLk/zaUqva4crc4wAADDYTqI929+Ek357k9d39r5Ocu9yyAABgd9hMoH6oqr4nyZVJ3j607V1eSQAAsHtsJlB/b5KvT3JNd3+kqi5O8ivLLQsAAHaHPSdaobs/kOSVSVJVT0nyxO5+7bIL2+lMPQ4AQLK5q3y8o6qeVFVnJ3lvkjdV1euWXxoAAOx8mznk48nd/dkk/zjJm7r7OUm+ebllAQDA7rCZQL2nqs5N8l05dlIiAACQzQXqH0/yu0k+1N3vrqqnJ7lzuWUBAMDusJmTEt+S5C0Ljz+c5DuWWdRu0CYfBwAgmzsp8fyq+q2qOlRV91bVW6vq/FNR3E5lokQAAOY2c8jHm5LckORLk5yX5LeHNgAAOO1tJlDv7+43dffh4faLSfYvuS4AANgVNhOoP1VVL6uq1eH2siSfXnZhAACwG2wmUP/zzC6Z98kk9yT5zsymIwcAgNPeZq7y8fEkL15sq6qfTPKDyypqV3CRDwAAsrkR6rV810mtYpdxlQ8AAObGBmqREgAAssEhH1V19npPRaAGAIAkGx9DfWtmRwqvFZ4fXE45AACwu6wbqLv74lNZyG7jnEQAAJLxx1Cf1soRLwAADARqAACYQKAGAIAJTjixS5JU1WqScxbXHyZ8AQCA09oJA3VV/askP5Lk3iRHh+ZO8tVLrGvH63ZaIgAAmxuhflWSr+zuTy+7mN3CTIkAAMxt5hjqTyT562UXAgAAu9FmRqg/nOQdVfW/kjwwb+zu1y2tKgAA2CU2E6g/PtzOGG4AAMDghIG6u3/sVBQCAAC70bqBuqpe392vrqrfzhozbXf3i5da2Q7nGh8AACQbj1D/8nD/k6eikN3ERT4AAJhbN1B3963D/R+eunIAAGB32czELpck+c9JnpHkzHl7dz99iXUBAMCusJnrUL8pyc8mOZzk7yf5pRw7HAQAAE5rmwnUZ3X3TUmquz/W3T+a5PnLLWvnM/M4AADJ5q5D/YWqWklyZ1V9f5K/TPK05Za1s5W5xwEAGGxmhPrVSR6X5JVJnpPkZUmuPNGLquqCqvqDqvpgVd1eVa8a2s+uqhur6s7h/ikLr3lNVd1VVXdU1QtHfSIAADiFNgzUVbWa5Lu6+/7uPtjd39vd39Hdf7KJbR9O8m+6++8keW6SV1TVM5JcneSm7r4kyU3D4wzPXZHkmUkuS/KG4f0BAGDHWjdQV9We7j6S5Dk14hiH7r6nu/90WP5ckg8mOS/J5UmuG1a7LslLhuXLk7y5ux/o7o8kuSvJpVt9XwAAOJU2Oob65iTPTvKeJG+rqrck+Zv5k939m5t9k6q6KMnXJnlXknO6+55hG/dU1fx47POSLI58Hxzajt/WVUmuSpILL7xwsyUAAMBSbOakxLOTfDqzK3t0ZhMFdpJNBeqqekKStyZ5dXd/doPB7rWeWGvK82uTXJskBw4c2LZrbbjIBwAAycaB+mlV9QNJ3p9jQXpuU3myqvZmFqZ/dWFE+96qOncYnT43yaGh/WCSCxZefn6SuzfzPqeaa3wAADC30UmJq0meMNyeuLA8v21oOO76F5J8sLtft/DUDTl2lZArk7xtof2KqtpXVRcnuSSzw04AAGDH2miE+p7u/vEJ235ekn+a5M+q6rah7d8neW2S66vq5Uk+nuSlSdLdt1fV9Uk+kNkVQl4xnBQJAAA71kaBetKRDd39xxts4wXrvOaaJNdMeV8AADiVNjrkY83Qy0ybexwAgGwQqLv7r05lIbuKsxIBABhsZupxAABgHQI1AABMIFADAMAEAvVITkkEACARqEdxTiIAAHMCNQAATCBQAwDABAI1AABMIFADAMAEAvVYLvMBAEAE6lGqXOcDAIAZgRoAACYQqAEAYAKBGgAAJhCoR2pnJQIAEIF6FKckAgAwJ1ADAMAEAjUAAEwgUAMAwAQCNQAATCBQj9Qu8gEAQATqUcw8DgDAnEANAAATCNQAADCBQA0AABMI1CM5KREAgESgHqVMPg4AwECgBgCACQRqAACYQKAGAIAJBOqROs5KBABAoB7FTIkAAMwJ1AAAMIFADQAAEwjUAAAwgUANAAATCNQjmXocAIBEoAYAgEkEagAAmECgBgCACQRqAACYQKAeyTmJAAAkAvUoZe5xAAAGAjUAAEwgUAMAwAQCNQAATCBQAwDABAL1SKYeBwAgEahHcY0PAADmBGoAAJhAoAYAgAkEagAAmECgHs1ZiQAACNSjmHkcAIA5gRoAACYQqAEAYAKBGgAAJhCoRzJTIgAAiUA9ipMSAQCYE6gBAGACgRoAACYQqAEAYAKBGgAAJhCoR3KRDwAAEoF6lIrLfAAAMCNQAwDABEsL1FX1xqo6VFXvX2g7u6purKo7h/unLDz3mqq6q6ruqKoXLqsuAAA4mZY5Qv2LSS47ru3qJDd19yVJbhoep6qekeSKJM8cXvOGqlpdYm0AAHBSLC1Qd/cfJfmr45ovT3LdsHxdkpcstL+5ux/o7o8kuSvJpcuq7WRoc48DAJBTfwz1Od19T5IM908b2s9L8omF9Q4ObY9SVVdV1S1Vdct999231GLXY+pxAADmdspJiWtF1DWHgLv72u4+0N0H9u/fv+SyAABgY6c6UN9bVecmyXB/aGg/mOSChfXOT3L3Ka4NAAC27FQH6huSXDksX5nkbQvtV1TVvqq6OMklSW4+xbUBAMCW7VnWhqvq15N8U5KnVtXBJD+S5LVJrq+qlyf5eJKXJkl3315V1yf5QJLDSV7R3UeWVRsAAJwsSwvU3f096zz1gnXWvybJNcuq52RzjQ8AAJKdc1LiruIiHwAAzAnUAAAwgUANAAATCNQAADCBQD2SmccBAEgE6nHMPQ4AwECgBgCACQRqAACYQKAGAIAJBOqRnJMIAEAiUI/ilEQAAOYEagAAmECgBgCACQRqAACYQKAGAIAJBOqR2tzjAABEoB7FzOMAAMwJ1AAAMIFADQAAEwjUAAAwgUANAAATCNQjOCcRAIA5gRoAACYQqAEAYAKBGgAAJhCoAQBgAoF6JDOPAwCQCNSjlLnHAQAYCNQAADCBQA0AABMI1AAAMIFAPVLHWYkAAAjUozglEQCAOYEaAAAmEKgBAGACgRoAACYQqEcyUyIAAIlAPcrKSuWoRA0AQATqUfasVI4cFagBABCoR1ldqRwWqAEAiEA9ihFqAADmBOoRVldW8tARgRoAAIF6lL2rlSNHj253GQAA7AAC9QiOoQYAYE6gHsEx1AAAzAnUI6yurOSwY6gBAIhAPcqelcphx1ADABCBepQ9qw75AABgRqAeYY+TEgEAGAjUI6yurOSIY6gBAIhAPcqeVSPUAADMCNQjrDopEQCAgUA9wr49s6nHnZgIAIBAPcJZe1eTJF946Mg2VwIAwHYTqEd43BmzQP23DwrUAACnO4F6hLPO2JMk+bxADQBw2hOoR3h4hPqhw9tcCQAA202gHmF+DLURagAABOoRzjpDoAYAYEagHsFJiQAAzAnUI8wD9eddNg8A4LQnUI9wpmOoAQAYCNQjPH64bN7fPugqHwAApzuBeoTH7ZuNUP+NEWoAgNOeQD3Cvj2rOWPPSj73BSPUAACnO4F6pCfs25P7H3hou8sAAGCbCdQjPWHfntxvhBoA4LQnUI/0uDNWc/vdn93uMgAA2GY7LlBX1WVVdUdV3VVVV293Pet5/L49ufPQ/XnoyNHtLgUAgG20Z7sLWFRVq0l+Jsk/SHIwybur6obu/sD2VvZoV37DRbn1Y/8v/+i//3HOedKZWalHPl81a1ip2fJqVVZWZst7Vir79qzkjD0r2bOyktWVWdv8ub2rK3n8vtXs27uafXtWsjpsq2q4pVJ1fEXH3vPhxw+3z16zkdk6x5aP38pa75ck3ceeW1yer97rrFtDvd2dTrIyPNF97BXHf57jP9cj6khytPsR2+kkq1U5MmyzhvVWhvddOW77j6y112yfb2fxtZ9/6Eiqkr2rK+me/cxXqtbsl/lme3iPxfV2is3Us9A9G+4bm9lupdKP6uVjz23JNvXlVt927U+79e0v/o6st88ev5+zPFN7evHnNvsbufBcH/u7Mf9Rr64c+/u82d+3+bbWWvf43+vFv12beX4z77dY62Z3zfnfgU4/YvloH+un+b+LW9322u+3Ocf/vHatXVj8M7/0yXnyWXu3u4xH2FGBOsmlSe7q7g8nSVW9OcnlSXZcoH7RV31J/uTrLsxffPJz+czfPvio4JjMfuG7kyNHZ/dHu3OkO4ePdB48fDQPHjmah44cTXcevj989GiObvVfWwCA08T1/+Lrc+nFZ293GY+w0wL1eUk+sfD4YJKvW1yhqq5KclWSXHjhhaeusuPsWV3Jf/r2v7uUbR852vmbBw/nCw8dyQMPHR1GJvoRIxTJo0d/83DrbCx2cTR0cb3jR07n23/kdh75H4PjrTcyffw25tYaAdloGxtZa/vz93jEKHglX3joSPbtWV2z9qPdjxr5WHcU9bjPN3/t/D9J+/asHNvu0Uf252Jd89rnI9NH1/sw22SjcuZ71txao2JrrbPRduf7c9Wjx6KP/8ZgvW8sFtc5lca+2/xbjK28z5Gjj/42Y6Pfs+TYz2G2jy1+Z7QLh6N2vI3/Rq71eH2P/LZu8SdWwz6w+LNd69uizfwqrPWt2Xrf/tXC/fx189HhrTrRHnj8qO9GH2Wl8vAA1KP+Fm3wN+NEz23F/JvW3Wq3Vv6VX/LE7S7hUXZaoF7v9/nYg+5rk1ybJAcOHNit+8KGVlcqTzpzb5505s76OgMAgEfbaSclHkxywcLj85PcvU21AADACe20QP3uJJdU1cVVdUaSK5LcsM01AQDAunbUIR/dfbiqvj/J7yZZTfLG7r59m8sCAIB17ahAnSTd/TtJfme76wAAgM3YaYd8AADAriJQAwDABAI1AABMIFADAMAEAjUAAEwgUAMAwAQCNQAATCBQAwDABAI1AABMIFADAMAEAjUAAExQ3b3dNYxWVfcl+dg2vf1Tk3xqm957N9JfW6O/tkZ/bY3+2hr9tTX6a2v019ZsZ399WXfvX+uJXR2ot1NV3dLdB7a7jt1Cf22N/toa/bU1+mtr9NfW6K+t0V9bs1P7yyEfAAAwgUANAAATCNTjXbvdBewy+mtr9NfW6K+t0V9bo7+2Rn9tjf7amh3ZX46hBgCACYxQAwDABAI1AABMIFBvUVVdVlV3VNVdVXX1dteznarqo1X1Z1V1W1XdMrSdXVU3VtWdw/1TFtZ/zdBvd1TVCxfanzNs566q+qmqqu34PCdbVb2xqg5V1fsX2k5a/1TVvqr6jaH9XVV10Sn9gCfZOv31o1X1l8M+dltVvWjhudO9vy6oqj+oqg9W1e1V9aqh3T62hg36yz62hqo6s6purqr3Dv31Y0O7/WsNG/SX/WsDVbVaVe+pqrcPj3fv/tXdbpu8JVlN8qEkT09yRpL3JnnGdte1jf3x0SRPPa7tvyS5eli+OslPDMvPGPprX5KLh35cHZ67OcnXJ6kk/zvJt2z3ZztJ/fONSZ6d5P3L6J8k/zLJ/xiWr0jyG9v9mZfQXz+a5AfXWFd/Jecmefaw/MQkfzH0i31sa/1lH1u7vyrJE4blvUneleS59q8t95f9a+N++4Ekv5bk7cPjXbt/GaHemkuT3NXdH+7uB5O8Ocnl21zTTnN5kuuG5euSvGSh/c3d/UB3fyTJXUkurapzkzypu9/Zs73+lxZes6t19x8l+avjmk9m/yxu638mecH8f+a70Tr9tR791X1Pd//psPy5JB9Mcl7sY2vaoL/Wc7r3V3f3/cPDvcOtY/9a0wb9tZ7Tur+SpKrOT/KtSX5+oXnX7l8C9dacl+QTC48PZuM/yI91neT3qurWqrpqaDunu+9JZv+AJXna0L5e3503LB/f/lh1Mvvn4dd09+Ekf53ki5dW+fb5/qp6X80OCZl//ae/FgxfZX5tZqNi9rETOK6/EvvYmoav429LcijJjd1t/9rAOv2V2L/W8/okP5Tk6ELbrt2/BOqtWet/NqfzdQef193PTvItSV5RVd+4wbrr9Z0+nRnTP6dD3/1ski9P8qwk9yT5r0O7/hpU1ROSvDXJq7v7sxutukbbaddna/SXfWwd3X2ku5+V5PzMRgO/aoPV9dfa/WX/WkNVfVuSQ91962ZfskbbjuovgXprDia5YOHx+Unu3qZatl133z3cH0ryW5kdEnPv8BVMhvtDw+rr9d3BYfn49seqk9k/D7+mqvYkeXI2f8jErtDd9w7/SB1N8nOZ7WOJ/kqSVNXezMLhr3b3bw7N9rF1rNVf9rET6+7PJHlHksti/zqhxf6yf63reUleXFUfzezw2edX1a9kF+9fAvXWvDvJJVV1cVWdkdlB7jdsc03boqoeX1VPnC8n+YdJ3p9Zf1w5rHZlkrcNyzckuWI46/biJJckuXn4SudzVfXc4dimf7bwmseik9k/i9v6ziS/PxxD9pgx/8M6+PbM9rFEf2X4fL+Q5IPd/bqFp+xja1ivv+xja6uq/VX1RcPyWUm+Ocmfx/61pvX6y/61tu5+TXef390XZZalfr+7X5bdvH/1DjjLczfdkrwos7PDP5Tkh7e7nm3sh6dndsbte5PcPu+LzI5PuinJncP92Quv+eGh3+7IwpU8khzI7I/Mh5L8dIYZPHf7LcmvZ/YV30OZ/U/55Sezf5KcmeQtmZ2ccXOSp2/3Z15Cf/1ykj9L8r7M/jieq78e/px/L7OvL9+X5Lbh9iL72Jb7yz62dn99dZL3DP3y/iT/YWi3f22tv+xfJ+67b8qxq3zs2v3L1OMAADCBQz4AAGACgRoAACYQqAEAYAKBGgAAJhCoAQBgAoEaYJeqqiNVddvC7eqTuO2Lqur9J14TgD3bXQAAo32+Z1MdA7CNjFADPMZU1Uer6ieq6ubh9hVD+5dV1U1V9b7h/sKh/Zyq+q2qeu9w+4ZhU6tV9XNVdXtV/d4wAxwAxxGoAXavs4475OO7F577bHdfmtnMYa8f2n46yS9191cn+dUkPzW0/1SSP+zur0ny7MxmP01m0/v+THc/M8lnknzHUj8NwC5lpkSAXaqq7u/uJ6zR/tEkz+/uD1fV3iSf7O4vrqpPZTb18UND+z3d/dSqui/J+d39wMI2LkpyY3dfMjz+d0n2dvd/PAUfDWBXMUIN8NjU6yyvt85aHlhYPhLn3QCsSaAGeGz67oX7dw7L/zfJFcPyP0nyx8PyTUm+L0mqarWqnnSqigR4LDDaALB7nVVVty08/j/dPb903r6qeldmAyffM7S9Mskbq+rfJrkvyfcO7a9Kcm1VvTyzkejvS3LPsosHeKxwDDXAY8xwDPWB7v7UdtcCcDpwyAcAAExghBoAACYwQg0AABMI1AAAMIFADQAAEwjUAAAwgUANAAAT/H8s6ySsqknNcQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 864x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.45      0.06      0.10       154\n",
      "           1       0.52      0.58      0.55       703\n",
      "           2       0.33      0.24      0.28       702\n",
      "           3       0.52      0.66      0.58       703\n",
      "           4       0.94      1.00      0.97       702\n",
      "\n",
      "    accuracy                           0.59      2964\n",
      "   macro avg       0.55      0.51      0.50      2964\n",
      "weighted avg       0.57      0.59      0.57      2964\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 504x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 504x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "gnn4l_gat = GNN4L_GAT(data_with_nedbit).to(device)\n",
    "pred = train(gnn4l_gat, data_with_nedbit.to(device), 40000, cm_title='GAT4L, multiclass, 16HC', classes=['P', 'LP', 'WN', 'LN', 'RN'], weight_decay=0.0005, layers=4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7 Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b53c737dc3247fa9ac96368fbe21233",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40001 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch: 0000] train loss: 1312.4592, train acc: 0.2361, val loss: 1228.2772, val acc: 0.2368  (best train acc: 0.2361, best val acc: 0.2368)\n",
      "[Epoch: 0020] train loss: 166.0461, train acc: 0.2297, val loss: 153.9472, val acc: 0.2368  (best train acc: 0.2473, best val acc: 0.2368)\n",
      "[Epoch: 0040] train loss: 64.6313, train acc: 0.2364, val loss: 60.4559, val acc: 0.2368  (best train acc: 0.2473, best val acc: 0.2371)\n",
      "[Epoch: 0060] train loss: 25.7274, train acc: 0.2214, val loss: 23.2570, val acc: 0.2371  (best train acc: 0.2473, best val acc: 0.2371)\n",
      "[Epoch: 0080] train loss: 1.6299, train acc: 0.2363, val loss: 1.6277, val acc: 0.2391  (best train acc: 0.2473, best val acc: 0.2391)\n",
      "[Epoch: 0100] train loss: 1.6345, train acc: 0.2418, val loss: 1.6312, val acc: 0.2476  (best train acc: 0.2473, best val acc: 0.2476)\n",
      "[Epoch: 0120] train loss: 1.6190, train acc: 0.2513, val loss: 1.6154, val acc: 0.2617  (best train acc: 0.2600, best val acc: 0.2685)\n",
      "[Epoch: 0140] train loss: 1.6012, train acc: 0.1995, val loss: 1.5965, val acc: 0.2250  (best train acc: 0.2600, best val acc: 0.2685)\n",
      "[Epoch: 0160] train loss: 1.5794, train acc: 0.2101, val loss: 1.5734, val acc: 0.2354  (best train acc: 0.2600, best val acc: 0.2685)\n",
      "[Epoch: 0180] train loss: 1.5627, train acc: 0.2127, val loss: 1.5577, val acc: 0.2368  (best train acc: 0.2600, best val acc: 0.2685)\n",
      "[Epoch: 0200] train loss: 1.5524, train acc: 0.2127, val loss: 1.5474, val acc: 0.2368  (best train acc: 0.2600, best val acc: 0.2685)\n",
      "[Epoch: 0220] train loss: 1.5450, train acc: 0.2124, val loss: 1.5403, val acc: 0.2361  (best train acc: 0.2600, best val acc: 0.2685)\n",
      "[Epoch: 0240] train loss: 1.5400, train acc: 0.2120, val loss: 1.5353, val acc: 0.2354  (best train acc: 0.2600, best val acc: 0.2685)\n",
      "[Epoch: 0260] train loss: 1.5373, train acc: 0.2076, val loss: 1.5316, val acc: 0.2354  (best train acc: 0.2600, best val acc: 0.2685)\n",
      "[Epoch: 0280] train loss: 1.5351, train acc: 0.2063, val loss: 1.5289, val acc: 0.2341  (best train acc: 0.2600, best val acc: 0.2685)\n",
      "[Epoch: 0300] train loss: 1.5324, train acc: 0.2005, val loss: 1.5269, val acc: 0.2300  (best train acc: 0.2600, best val acc: 0.2685)\n",
      "[Epoch: 0320] train loss: 1.5316, train acc: 0.1888, val loss: 1.5251, val acc: 0.2185  (best train acc: 0.2600, best val acc: 0.2685)\n",
      "[Epoch: 0340] train loss: 1.5303, train acc: 0.1831, val loss: 1.5238, val acc: 0.2013  (best train acc: 0.2600, best val acc: 0.2685)\n",
      "[Epoch: 0360] train loss: 1.5277, train acc: 0.2128, val loss: 1.5226, val acc: 0.2179  (best train acc: 0.2600, best val acc: 0.2685)\n",
      "[Epoch: 0380] train loss: 1.5262, train acc: 0.2470, val loss: 1.5216, val acc: 0.2513  (best train acc: 0.2600, best val acc: 0.2685)\n",
      "[Epoch: 0400] train loss: 1.5267, train acc: 0.2599, val loss: 1.5208, val acc: 0.2661  (best train acc: 0.2600, best val acc: 0.2685)\n",
      "[Epoch: 0420] train loss: 1.5261, train acc: 0.2736, val loss: 1.5200, val acc: 0.2644  (best train acc: 0.2751, best val acc: 0.2718)\n",
      "[Epoch: 0440] train loss: 1.5263, train acc: 0.2687, val loss: 1.5194, val acc: 0.2624  (best train acc: 0.2766, best val acc: 0.2718)\n",
      "[Epoch: 0460] train loss: 1.5243, train acc: 0.2743, val loss: 1.5189, val acc: 0.2631  (best train acc: 0.2766, best val acc: 0.2718)\n",
      "[Epoch: 0480] train loss: 1.5236, train acc: 0.2814, val loss: 1.5184, val acc: 0.2634  (best train acc: 0.2814, best val acc: 0.2718)\n",
      "[Epoch: 0500] train loss: 1.5241, train acc: 0.2753, val loss: 1.5180, val acc: 0.2641  (best train acc: 0.2818, best val acc: 0.2718)\n",
      "[Epoch: 0520] train loss: 1.5225, train acc: 0.2788, val loss: 1.5177, val acc: 0.2688  (best train acc: 0.2826, best val acc: 0.2718)\n",
      "[Epoch: 0540] train loss: 1.5231, train acc: 0.2835, val loss: 1.5175, val acc: 0.2641  (best train acc: 0.2856, best val acc: 0.2718)\n",
      "[Epoch: 0560] train loss: 1.5238, train acc: 0.2867, val loss: 1.5172, val acc: 0.2685  (best train acc: 0.2911, best val acc: 0.2718)\n",
      "[Epoch: 0580] train loss: 1.5227, train acc: 0.2888, val loss: 1.5170, val acc: 0.2742  (best train acc: 0.2922, best val acc: 0.2742)\n",
      "[Epoch: 0600] train loss: 1.5221, train acc: 0.2925, val loss: 1.5169, val acc: 0.2820  (best train acc: 0.2975, best val acc: 0.2820)\n",
      "[Epoch: 0620] train loss: 1.5214, train acc: 0.2977, val loss: 1.5167, val acc: 0.2860  (best train acc: 0.3010, best val acc: 0.2874)\n",
      "[Epoch: 0640] train loss: 1.5225, train acc: 0.2976, val loss: 1.5166, val acc: 0.2887  (best train acc: 0.3011, best val acc: 0.2907)\n",
      "[Epoch: 0660] train loss: 1.5226, train acc: 0.2954, val loss: 1.5165, val acc: 0.2867  (best train acc: 0.3011, best val acc: 0.2907)\n",
      "[Epoch: 0680] train loss: 1.5213, train acc: 0.2958, val loss: 1.5164, val acc: 0.2789  (best train acc: 0.3011, best val acc: 0.2907)\n",
      "[Epoch: 0700] train loss: 1.5220, train acc: 0.2858, val loss: 1.5164, val acc: 0.2739  (best train acc: 0.3011, best val acc: 0.2907)\n",
      "[Epoch: 0720] train loss: 1.5225, train acc: 0.2850, val loss: 1.5164, val acc: 0.2702  (best train acc: 0.3011, best val acc: 0.2907)\n",
      "[Epoch: 0740] train loss: 1.5213, train acc: 0.2820, val loss: 1.5163, val acc: 0.2708  (best train acc: 0.3011, best val acc: 0.2907)\n",
      "[Epoch: 0760] train loss: 1.5213, train acc: 0.2799, val loss: 1.5163, val acc: 0.2688  (best train acc: 0.3011, best val acc: 0.2907)\n",
      "[Epoch: 0780] train loss: 1.5229, train acc: 0.2800, val loss: 1.5163, val acc: 0.2641  (best train acc: 0.3011, best val acc: 0.2907)\n",
      "[Epoch: 0800] train loss: 1.5219, train acc: 0.2735, val loss: 1.5163, val acc: 0.2627  (best train acc: 0.3011, best val acc: 0.2907)\n",
      "[Epoch: 0820] train loss: 1.5218, train acc: 0.2760, val loss: 1.5163, val acc: 0.2644  (best train acc: 0.3011, best val acc: 0.2907)\n",
      "[Epoch: 0840] train loss: 1.5215, train acc: 0.2675, val loss: 1.5163, val acc: 0.2597  (best train acc: 0.3011, best val acc: 0.2907)\n",
      "[Epoch: 0860] train loss: 1.5208, train acc: 0.2672, val loss: 1.5160, val acc: 0.2671  (best train acc: 0.3011, best val acc: 0.2907)\n",
      "[Epoch: 0880] train loss: 1.5209, train acc: 0.2590, val loss: 1.5160, val acc: 0.2712  (best train acc: 0.3011, best val acc: 0.2907)\n",
      "[Epoch: 0900] train loss: 1.5207, train acc: 0.2627, val loss: 1.5159, val acc: 0.2695  (best train acc: 0.3011, best val acc: 0.2907)\n",
      "[Epoch: 0920] train loss: 1.5215, train acc: 0.2561, val loss: 1.5159, val acc: 0.2648  (best train acc: 0.3011, best val acc: 0.2907)\n",
      "[Epoch: 0940] train loss: 1.5209, train acc: 0.2629, val loss: 1.5160, val acc: 0.2668  (best train acc: 0.3011, best val acc: 0.2907)\n",
      "[Epoch: 0960] train loss: 1.5208, train acc: 0.2590, val loss: 1.5160, val acc: 0.2610  (best train acc: 0.3011, best val acc: 0.2907)\n",
      "[Epoch: 0980] train loss: 1.5211, train acc: 0.2597, val loss: 1.5160, val acc: 0.2631  (best train acc: 0.3011, best val acc: 0.2907)\n",
      "[Epoch: 1000] train loss: 1.5205, train acc: 0.2556, val loss: 1.5159, val acc: 0.2654  (best train acc: 0.3011, best val acc: 0.2907)\n",
      "[Epoch: 1020] train loss: 1.5209, train acc: 0.2525, val loss: 1.5159, val acc: 0.2671  (best train acc: 0.3011, best val acc: 0.2907)\n",
      "[Epoch: 1040] train loss: 1.5214, train acc: 0.2576, val loss: 1.5160, val acc: 0.2631  (best train acc: 0.3011, best val acc: 0.2907)\n",
      "[Epoch: 1060] train loss: 1.5204, train acc: 0.2514, val loss: 1.5160, val acc: 0.2577  (best train acc: 0.3011, best val acc: 0.2907)\n",
      "[Epoch: 1080] train loss: 1.5209, train acc: 0.2500, val loss: 1.5160, val acc: 0.2583  (best train acc: 0.3011, best val acc: 0.2907)\n",
      "[Epoch: 1100] train loss: 1.5207, train acc: 0.2478, val loss: 1.5160, val acc: 0.2550  (best train acc: 0.3011, best val acc: 0.2907)\n",
      "[Epoch: 1120] train loss: 1.5208, train acc: 0.2514, val loss: 1.5160, val acc: 0.2580  (best train acc: 0.3011, best val acc: 0.2907)\n",
      "[Epoch: 1140] train loss: 1.5198, train acc: 0.2484, val loss: 1.5160, val acc: 0.2556  (best train acc: 0.3011, best val acc: 0.2907)\n",
      "[Epoch: 1160] train loss: 1.5209, train acc: 0.2428, val loss: 1.5160, val acc: 0.2600  (best train acc: 0.3011, best val acc: 0.2907)\n",
      "[Epoch: 1180] train loss: 1.5209, train acc: 0.2464, val loss: 1.5160, val acc: 0.2590  (best train acc: 0.3011, best val acc: 0.2907)\n",
      "[Epoch: 1200] train loss: 1.5209, train acc: 0.2454, val loss: 1.5160, val acc: 0.2583  (best train acc: 0.3011, best val acc: 0.2907)\n",
      "[Epoch: 1220] train loss: 1.5202, train acc: 0.2454, val loss: 1.5160, val acc: 0.2570  (best train acc: 0.3011, best val acc: 0.2907)\n",
      "[Epoch: 1240] train loss: 1.5196, train acc: 0.2493, val loss: 1.5158, val acc: 0.2634  (best train acc: 0.3011, best val acc: 0.2907)\n",
      "[Epoch: 1260] train loss: 1.5202, train acc: 0.2475, val loss: 1.5157, val acc: 0.2597  (best train acc: 0.3011, best val acc: 0.2907)\n",
      "[Epoch: 1280] train loss: 1.5195, train acc: 0.2401, val loss: 1.5156, val acc: 0.2530  (best train acc: 0.3011, best val acc: 0.2907)\n",
      "[Epoch: 1300] train loss: 1.5190, train acc: 0.2353, val loss: 1.5143, val acc: 0.2398  (best train acc: 0.3011, best val acc: 0.2907)\n",
      "[Epoch: 1320] train loss: 1.5192, train acc: 0.2318, val loss: 1.5134, val acc: 0.2489  (best train acc: 0.3011, best val acc: 0.2907)\n",
      "[Epoch: 1340] train loss: 1.5176, train acc: 0.2239, val loss: 1.5130, val acc: 0.2395  (best train acc: 0.3011, best val acc: 0.2907)\n",
      "[Epoch: 1360] train loss: 1.5183, train acc: 0.2231, val loss: 1.5128, val acc: 0.2395  (best train acc: 0.3011, best val acc: 0.2907)\n",
      "[Epoch: 1380] train loss: 1.5189, train acc: 0.2345, val loss: 1.5126, val acc: 0.2418  (best train acc: 0.3011, best val acc: 0.2907)\n",
      "[Epoch: 1400] train loss: 1.5183, train acc: 0.2313, val loss: 1.5125, val acc: 0.2432  (best train acc: 0.3011, best val acc: 0.2907)\n",
      "[Epoch: 1420] train loss: 1.5176, train acc: 0.2379, val loss: 1.5124, val acc: 0.2503  (best train acc: 0.3011, best val acc: 0.2907)\n",
      "[Epoch: 1440] train loss: 1.5187, train acc: 0.2361, val loss: 1.5123, val acc: 0.2449  (best train acc: 0.3011, best val acc: 0.2907)\n",
      "[Epoch: 1460] train loss: 1.5181, train acc: 0.2311, val loss: 1.5122, val acc: 0.2469  (best train acc: 0.3011, best val acc: 0.2907)\n",
      "[Epoch: 1480] train loss: 1.5172, train acc: 0.2346, val loss: 1.5121, val acc: 0.2482  (best train acc: 0.3011, best val acc: 0.2907)\n",
      "[Epoch: 1500] train loss: 1.5180, train acc: 0.2321, val loss: 1.5121, val acc: 0.2469  (best train acc: 0.3011, best val acc: 0.2907)\n",
      "[Epoch: 1520] train loss: 1.5178, train acc: 0.2352, val loss: 1.5120, val acc: 0.2479  (best train acc: 0.3011, best val acc: 0.2907)\n",
      "[Epoch: 1540] train loss: 1.5173, train acc: 0.2349, val loss: 1.5119, val acc: 0.2459  (best train acc: 0.3011, best val acc: 0.2907)\n",
      "[Epoch: 1560] train loss: 1.5179, train acc: 0.2384, val loss: 1.5119, val acc: 0.2509  (best train acc: 0.3011, best val acc: 0.2907)\n",
      "[Epoch: 1580] train loss: 1.5172, train acc: 0.2401, val loss: 1.5118, val acc: 0.2516  (best train acc: 0.3011, best val acc: 0.2907)\n",
      "[Epoch: 1600] train loss: 1.5183, train acc: 0.2298, val loss: 1.5117, val acc: 0.2476  (best train acc: 0.3011, best val acc: 0.2907)\n",
      "[Epoch: 1620] train loss: 1.5172, train acc: 0.2370, val loss: 1.5116, val acc: 0.2499  (best train acc: 0.3011, best val acc: 0.2907)\n",
      "[Epoch: 1640] train loss: 1.5180, train acc: 0.2380, val loss: 1.5116, val acc: 0.2499  (best train acc: 0.3011, best val acc: 0.2907)\n",
      "[Epoch: 1660] train loss: 1.5178, train acc: 0.2382, val loss: 1.5114, val acc: 0.2526  (best train acc: 0.3011, best val acc: 0.2907)\n",
      "[Epoch: 1680] train loss: 1.5165, train acc: 0.2335, val loss: 1.5110, val acc: 0.2624  (best train acc: 0.3011, best val acc: 0.2907)\n",
      "[Epoch: 1700] train loss: 1.5175, train acc: 0.2280, val loss: 1.5102, val acc: 0.2384  (best train acc: 0.3011, best val acc: 0.2907)\n",
      "[Epoch: 1720] train loss: 1.5169, train acc: 0.2370, val loss: 1.5095, val acc: 0.2408  (best train acc: 0.3011, best val acc: 0.2958)\n",
      "[Epoch: 1740] train loss: 1.5155, train acc: 0.2347, val loss: 1.5091, val acc: 0.2691  (best train acc: 0.3011, best val acc: 0.2961)\n",
      "[Epoch: 1760] train loss: 1.5158, train acc: 0.2236, val loss: 1.5087, val acc: 0.2637  (best train acc: 0.3011, best val acc: 0.2985)\n",
      "[Epoch: 1780] train loss: 1.5147, train acc: 0.2273, val loss: 1.5079, val acc: 0.2550  (best train acc: 0.3011, best val acc: 0.2985)\n",
      "[Epoch: 1800] train loss: 1.5137, train acc: 0.2284, val loss: 1.5071, val acc: 0.2688  (best train acc: 0.3011, best val acc: 0.2985)\n",
      "[Epoch: 1820] train loss: 1.5129, train acc: 0.2287, val loss: 1.5050, val acc: 0.2445  (best train acc: 0.3011, best val acc: 0.2985)\n",
      "[Epoch: 1840] train loss: 1.5075, train acc: 0.2162, val loss: 1.4988, val acc: 0.2408  (best train acc: 0.3011, best val acc: 0.2985)\n",
      "[Epoch: 1860] train loss: 1.4843, train acc: 0.2086, val loss: 1.4616, val acc: 0.2600  (best train acc: 0.3011, best val acc: 0.2985)\n",
      "[Epoch: 1880] train loss: 1.4348, train acc: 0.3225, val loss: 1.3992, val acc: 0.3524  (best train acc: 0.3730, best val acc: 0.3970)\n",
      "[Epoch: 1900] train loss: 1.4233, train acc: 0.3490, val loss: 1.3792, val acc: 0.3852  (best train acc: 0.3730, best val acc: 0.3970)\n",
      "[Epoch: 1920] train loss: 1.4143, train acc: 0.3674, val loss: 1.3650, val acc: 0.4236  (best train acc: 0.3730, best val acc: 0.4236)\n",
      "[Epoch: 1940] train loss: 1.4179, train acc: 0.3788, val loss: 1.3772, val acc: 0.3916  (best train acc: 0.3868, best val acc: 0.4236)\n",
      "[Epoch: 1960] train loss: 1.4115, train acc: 0.3843, val loss: 1.3613, val acc: 0.3987  (best train acc: 0.3869, best val acc: 0.4236)\n",
      "[Epoch: 1980] train loss: 1.4111, train acc: 0.3827, val loss: 1.3572, val acc: 0.4030  (best train acc: 0.3869, best val acc: 0.4236)\n",
      "[Epoch: 2000] train loss: 1.4137, train acc: 0.3810, val loss: 1.3567, val acc: 0.4078  (best train acc: 0.3875, best val acc: 0.4236)\n",
      "[Epoch: 2020] train loss: 1.4066, train acc: 0.3825, val loss: 1.3540, val acc: 0.4030  (best train acc: 0.3875, best val acc: 0.4236)\n",
      "[Epoch: 2040] train loss: 1.4049, train acc: 0.3818, val loss: 1.3545, val acc: 0.4047  (best train acc: 0.3875, best val acc: 0.4236)\n",
      "[Epoch: 2060] train loss: 1.4092, train acc: 0.3786, val loss: 1.3513, val acc: 0.4081  (best train acc: 0.3875, best val acc: 0.4236)\n",
      "[Epoch: 2080] train loss: 1.4040, train acc: 0.3772, val loss: 1.3481, val acc: 0.4101  (best train acc: 0.3875, best val acc: 0.4236)\n",
      "[Epoch: 2100] train loss: 1.4089, train acc: 0.3793, val loss: 1.3518, val acc: 0.4064  (best train acc: 0.3875, best val acc: 0.4236)\n",
      "[Epoch: 2120] train loss: 1.4023, train acc: 0.3788, val loss: 1.3477, val acc: 0.4051  (best train acc: 0.3875, best val acc: 0.4236)\n",
      "[Epoch: 2140] train loss: 1.4015, train acc: 0.3813, val loss: 1.3481, val acc: 0.4084  (best train acc: 0.3875, best val acc: 0.4236)\n",
      "[Epoch: 2160] train loss: 1.4032, train acc: 0.3808, val loss: 1.3474, val acc: 0.4037  (best train acc: 0.3875, best val acc: 0.4236)\n",
      "[Epoch: 2180] train loss: 1.3966, train acc: 0.3879, val loss: 1.3463, val acc: 0.4128  (best train acc: 0.3879, best val acc: 0.4236)\n",
      "[Epoch: 2200] train loss: 1.4052, train acc: 0.3830, val loss: 1.3444, val acc: 0.4111  (best train acc: 0.3880, best val acc: 0.4236)\n",
      "[Epoch: 2220] train loss: 1.3996, train acc: 0.3837, val loss: 1.3456, val acc: 0.4078  (best train acc: 0.3890, best val acc: 0.4236)\n",
      "[Epoch: 2240] train loss: 1.3987, train acc: 0.3814, val loss: 1.3422, val acc: 0.4138  (best train acc: 0.3897, best val acc: 0.4236)\n",
      "[Epoch: 2260] train loss: 1.3972, train acc: 0.3832, val loss: 1.3441, val acc: 0.4057  (best train acc: 0.3908, best val acc: 0.4243)\n",
      "[Epoch: 2280] train loss: 1.3981, train acc: 0.3813, val loss: 1.3416, val acc: 0.4094  (best train acc: 0.3908, best val acc: 0.4243)\n",
      "[Epoch: 2300] train loss: 1.3960, train acc: 0.3883, val loss: 1.3386, val acc: 0.4206  (best train acc: 0.3932, best val acc: 0.4273)\n",
      "[Epoch: 2320] train loss: 1.3903, train acc: 0.3949, val loss: 1.3402, val acc: 0.4280  (best train acc: 0.3959, best val acc: 0.4280)\n",
      "[Epoch: 2340] train loss: 1.3890, train acc: 0.3914, val loss: 1.3437, val acc: 0.4115  (best train acc: 0.4006, best val acc: 0.4307)\n",
      "[Epoch: 2360] train loss: 1.3844, train acc: 0.3861, val loss: 1.3416, val acc: 0.4145  (best train acc: 0.4006, best val acc: 0.4361)\n",
      "[Epoch: 2380] train loss: 1.3891, train acc: 0.3892, val loss: 1.3372, val acc: 0.4169  (best train acc: 0.4010, best val acc: 0.4381)\n",
      "[Epoch: 2400] train loss: 1.3843, train acc: 0.3911, val loss: 1.3345, val acc: 0.4219  (best train acc: 0.4010, best val acc: 0.4381)\n",
      "[Epoch: 2420] train loss: 1.3845, train acc: 0.3962, val loss: 1.3317, val acc: 0.4277  (best train acc: 0.4010, best val acc: 0.4381)\n",
      "[Epoch: 2440] train loss: 1.3816, train acc: 0.3904, val loss: 1.3351, val acc: 0.4250  (best train acc: 0.4010, best val acc: 0.4381)\n",
      "[Epoch: 2460] train loss: 1.3844, train acc: 0.3941, val loss: 1.3373, val acc: 0.4128  (best train acc: 0.4010, best val acc: 0.4381)\n",
      "[Epoch: 2480] train loss: 1.3794, train acc: 0.3941, val loss: 1.3334, val acc: 0.4192  (best train acc: 0.4010, best val acc: 0.4381)\n",
      "[Epoch: 2500] train loss: 1.3867, train acc: 0.3910, val loss: 1.3330, val acc: 0.4209  (best train acc: 0.4010, best val acc: 0.4381)\n",
      "[Epoch: 2520] train loss: 1.3778, train acc: 0.3978, val loss: 1.3287, val acc: 0.4263  (best train acc: 0.4010, best val acc: 0.4381)\n",
      "[Epoch: 2540] train loss: 1.3886, train acc: 0.3848, val loss: 1.3285, val acc: 0.4347  (best train acc: 0.4037, best val acc: 0.4381)\n",
      "[Epoch: 2560] train loss: 1.3750, train acc: 0.3986, val loss: 1.3339, val acc: 0.4172  (best train acc: 0.4050, best val acc: 0.4381)\n",
      "[Epoch: 2580] train loss: 1.3813, train acc: 0.3976, val loss: 1.3333, val acc: 0.4155  (best train acc: 0.4080, best val acc: 0.4381)\n",
      "[Epoch: 2600] train loss: 1.3783, train acc: 0.4004, val loss: 1.3299, val acc: 0.4216  (best train acc: 0.4080, best val acc: 0.4381)\n",
      "[Epoch: 2620] train loss: 1.3813, train acc: 0.3938, val loss: 1.3330, val acc: 0.4192  (best train acc: 0.4080, best val acc: 0.4381)\n",
      "[Epoch: 2640] train loss: 1.3790, train acc: 0.3921, val loss: 1.3318, val acc: 0.4175  (best train acc: 0.4080, best val acc: 0.4381)\n",
      "[Epoch: 2660] train loss: 1.3794, train acc: 0.3957, val loss: 1.3270, val acc: 0.4209  (best train acc: 0.4080, best val acc: 0.4381)\n",
      "[Epoch: 2680] train loss: 1.3763, train acc: 0.4029, val loss: 1.3323, val acc: 0.4196  (best train acc: 0.4080, best val acc: 0.4381)\n",
      "[Epoch: 2700] train loss: 1.3741, train acc: 0.3979, val loss: 1.3302, val acc: 0.4185  (best train acc: 0.4080, best val acc: 0.4381)\n",
      "[Epoch: 2720] train loss: 1.3829, train acc: 0.3967, val loss: 1.3373, val acc: 0.4111  (best train acc: 0.4080, best val acc: 0.4381)\n",
      "[Epoch: 2740] train loss: 1.3708, train acc: 0.4030, val loss: 1.3354, val acc: 0.4132  (best train acc: 0.4080, best val acc: 0.4381)\n",
      "[Epoch: 2760] train loss: 1.3767, train acc: 0.3976, val loss: 1.3303, val acc: 0.4182  (best train acc: 0.4084, best val acc: 0.4381)\n",
      "[Epoch: 2780] train loss: 1.3818, train acc: 0.4019, val loss: 1.3316, val acc: 0.4148  (best train acc: 0.4084, best val acc: 0.4381)\n",
      "[Epoch: 2800] train loss: 1.3767, train acc: 0.4018, val loss: 1.3287, val acc: 0.4287  (best train acc: 0.4084, best val acc: 0.4381)\n",
      "[Epoch: 2820] train loss: 1.3719, train acc: 0.3955, val loss: 1.3227, val acc: 0.4324  (best train acc: 0.4084, best val acc: 0.4381)\n",
      "[Epoch: 2840] train loss: 1.3723, train acc: 0.4091, val loss: 1.3220, val acc: 0.4334  (best train acc: 0.4091, best val acc: 0.4381)\n",
      "[Epoch: 2860] train loss: 1.3656, train acc: 0.4064, val loss: 1.3207, val acc: 0.4202  (best train acc: 0.4106, best val acc: 0.4381)\n",
      "[Epoch: 2880] train loss: 1.3825, train acc: 0.4008, val loss: 1.3239, val acc: 0.4132  (best train acc: 0.4111, best val acc: 0.4381)\n",
      "[Epoch: 2900] train loss: 1.3656, train acc: 0.4132, val loss: 1.3212, val acc: 0.4209  (best train acc: 0.4132, best val acc: 0.4381)\n",
      "[Epoch: 2920] train loss: 1.3673, train acc: 0.4141, val loss: 1.3164, val acc: 0.4401  (best train acc: 0.4152, best val acc: 0.4401)\n",
      "[Epoch: 2940] train loss: 1.3624, train acc: 0.4135, val loss: 1.3223, val acc: 0.4135  (best train acc: 0.4214, best val acc: 0.4401)\n",
      "[Epoch: 2960] train loss: 1.3887, train acc: 0.4126, val loss: 1.3756, val acc: 0.3788  (best train acc: 0.4214, best val acc: 0.4415)\n",
      "[Epoch: 2980] train loss: 1.5595, train acc: 0.2829, val loss: 1.4829, val acc: 0.3221  (best train acc: 0.4214, best val acc: 0.4415)\n",
      "[Epoch: 3000] train loss: 1.4073, train acc: 0.3785, val loss: 1.3803, val acc: 0.3653  (best train acc: 0.4214, best val acc: 0.4415)\n",
      "[Epoch: 3020] train loss: 1.3819, train acc: 0.3732, val loss: 1.3428, val acc: 0.4027  (best train acc: 0.4214, best val acc: 0.4415)\n",
      "[Epoch: 3040] train loss: 1.3696, train acc: 0.4219, val loss: 1.3211, val acc: 0.4371  (best train acc: 0.4219, best val acc: 0.4422)\n",
      "[Epoch: 3060] train loss: 1.3581, train acc: 0.4165, val loss: 1.3194, val acc: 0.4351  (best train acc: 0.4263, best val acc: 0.4469)\n",
      "[Epoch: 3080] train loss: 1.3539, train acc: 0.4158, val loss: 1.3174, val acc: 0.4266  (best train acc: 0.4263, best val acc: 0.4469)\n",
      "[Epoch: 3100] train loss: 1.3530, train acc: 0.4250, val loss: 1.3231, val acc: 0.4358  (best train acc: 0.4263, best val acc: 0.4469)\n",
      "[Epoch: 3120] train loss: 1.3541, train acc: 0.4020, val loss: 1.3175, val acc: 0.4236  (best train acc: 0.4277, best val acc: 0.4469)\n",
      "[Epoch: 3140] train loss: 1.3489, train acc: 0.4102, val loss: 1.3073, val acc: 0.4212  (best train acc: 0.4294, best val acc: 0.4469)\n",
      "[Epoch: 3160] train loss: 1.3369, train acc: 0.4284, val loss: 1.2946, val acc: 0.4374  (best train acc: 0.4294, best val acc: 0.4469)\n",
      "[Epoch: 3180] train loss: 1.8479, train acc: 0.2603, val loss: 1.8243, val acc: 0.2368  (best train acc: 0.4320, best val acc: 0.4469)\n",
      "[Epoch: 3200] train loss: 1.5445, train acc: 0.2710, val loss: 1.5329, val acc: 0.2378  (best train acc: 0.4320, best val acc: 0.4469)\n",
      "[Epoch: 3220] train loss: 1.5327, train acc: 0.2551, val loss: 1.5227, val acc: 0.2604  (best train acc: 0.4320, best val acc: 0.4469)\n",
      "[Epoch: 3240] train loss: 1.5192, train acc: 0.2151, val loss: 1.5126, val acc: 0.2496  (best train acc: 0.4320, best val acc: 0.4469)\n",
      "[Epoch: 3260] train loss: 1.5174, train acc: 0.2165, val loss: 1.5103, val acc: 0.2462  (best train acc: 0.4320, best val acc: 0.4469)\n",
      "[Epoch: 3280] train loss: 1.5175, train acc: 0.2180, val loss: 1.5087, val acc: 0.2516  (best train acc: 0.4320, best val acc: 0.4469)\n",
      "[Epoch: 3300] train loss: 1.5151, train acc: 0.2141, val loss: 1.5073, val acc: 0.2503  (best train acc: 0.4320, best val acc: 0.4469)\n",
      "[Epoch: 3320] train loss: 1.5148, train acc: 0.2209, val loss: 1.5060, val acc: 0.2556  (best train acc: 0.4320, best val acc: 0.4469)\n",
      "[Epoch: 3340] train loss: 1.5109, train acc: 0.2261, val loss: 1.5049, val acc: 0.2560  (best train acc: 0.4320, best val acc: 0.4469)\n",
      "[Epoch: 3360] train loss: 1.5121, train acc: 0.2233, val loss: 1.5038, val acc: 0.2577  (best train acc: 0.4320, best val acc: 0.4469)\n",
      "[Epoch: 3380] train loss: 1.5124, train acc: 0.2259, val loss: 1.5027, val acc: 0.2590  (best train acc: 0.4320, best val acc: 0.4469)\n",
      "[Epoch: 3400] train loss: 1.5119, train acc: 0.2231, val loss: 1.5017, val acc: 0.2600  (best train acc: 0.4320, best val acc: 0.4469)\n",
      "[Epoch: 3420] train loss: 1.5098, train acc: 0.2303, val loss: 1.5007, val acc: 0.2597  (best train acc: 0.4320, best val acc: 0.4469)\n",
      "[Epoch: 3440] train loss: 1.5063, train acc: 0.2337, val loss: 1.4998, val acc: 0.2631  (best train acc: 0.4320, best val acc: 0.4469)\n",
      "[Epoch: 3460] train loss: 1.5085, train acc: 0.2335, val loss: 1.4989, val acc: 0.2617  (best train acc: 0.4320, best val acc: 0.4469)\n",
      "[Epoch: 3480] train loss: 1.5067, train acc: 0.2370, val loss: 1.4984, val acc: 0.2617  (best train acc: 0.4320, best val acc: 0.4469)\n",
      "[Epoch: 3500] train loss: 1.5072, train acc: 0.2413, val loss: 1.4975, val acc: 0.2637  (best train acc: 0.4320, best val acc: 0.4469)\n",
      "[Epoch: 3520] train loss: 1.5061, train acc: 0.2405, val loss: 1.4968, val acc: 0.2644  (best train acc: 0.4320, best val acc: 0.4469)\n",
      "[Epoch: 3540] train loss: 1.5076, train acc: 0.2412, val loss: 1.4962, val acc: 0.2661  (best train acc: 0.4320, best val acc: 0.4469)\n",
      "[Epoch: 3560] train loss: 1.5062, train acc: 0.2440, val loss: 1.4959, val acc: 0.2658  (best train acc: 0.4320, best val acc: 0.4469)\n",
      "[Epoch: 3580] train loss: 1.5045, train acc: 0.2397, val loss: 1.4953, val acc: 0.2661  (best train acc: 0.4320, best val acc: 0.4469)\n",
      "[Epoch: 3600] train loss: 1.5043, train acc: 0.2518, val loss: 1.4949, val acc: 0.2637  (best train acc: 0.4320, best val acc: 0.4469)\n",
      "[Epoch: 3620] train loss: 1.5028, train acc: 0.2470, val loss: 1.4937, val acc: 0.2678  (best train acc: 0.4320, best val acc: 0.4469)\n",
      "[Epoch: 3640] train loss: 1.5031, train acc: 0.2479, val loss: 1.4932, val acc: 0.2705  (best train acc: 0.4320, best val acc: 0.4469)\n",
      "[Epoch: 3660] train loss: 1.5029, train acc: 0.2448, val loss: 1.4911, val acc: 0.2762  (best train acc: 0.4320, best val acc: 0.4469)\n",
      "[Epoch: 3680] train loss: 1.4956, train acc: 0.2686, val loss: 1.4816, val acc: 0.2867  (best train acc: 0.4320, best val acc: 0.4469)\n",
      "[Epoch: 3700] train loss: 1.4568, train acc: 0.3356, val loss: 1.4380, val acc: 0.3791  (best train acc: 0.4320, best val acc: 0.4469)\n",
      "[Epoch: 3720] train loss: 1.4445, train acc: 0.3573, val loss: 1.4103, val acc: 0.4044  (best train acc: 0.4320, best val acc: 0.4469)\n",
      "[Epoch: 3740] train loss: 1.4359, train acc: 0.3668, val loss: 1.3934, val acc: 0.4162  (best train acc: 0.4320, best val acc: 0.4469)\n",
      "[Epoch: 3760] train loss: 1.4299, train acc: 0.3784, val loss: 1.3845, val acc: 0.4229  (best train acc: 0.4320, best val acc: 0.4469)\n",
      "[Epoch: 3780] train loss: 1.4213, train acc: 0.3830, val loss: 1.3782, val acc: 0.4283  (best train acc: 0.4320, best val acc: 0.4469)\n",
      "[Epoch: 3800] train loss: 1.4196, train acc: 0.3947, val loss: 1.3733, val acc: 0.4293  (best train acc: 0.4320, best val acc: 0.4469)\n",
      "[Epoch: 3820] train loss: 1.4158, train acc: 0.4007, val loss: 1.3692, val acc: 0.4438  (best train acc: 0.4320, best val acc: 0.4469)\n",
      "[Epoch: 3840] train loss: 1.4140, train acc: 0.4073, val loss: 1.3645, val acc: 0.4442  (best train acc: 0.4320, best val acc: 0.4469)\n",
      "[Epoch: 3860] train loss: 1.4146, train acc: 0.4037, val loss: 1.3612, val acc: 0.4472  (best train acc: 0.4320, best val acc: 0.4492)\n",
      "[Epoch: 3880] train loss: 1.4085, train acc: 0.4124, val loss: 1.3573, val acc: 0.4519  (best train acc: 0.4320, best val acc: 0.4519)\n",
      "[Epoch: 3900] train loss: 1.4098, train acc: 0.4100, val loss: 1.3560, val acc: 0.4519  (best train acc: 0.4320, best val acc: 0.4533)\n",
      "[Epoch: 3920] train loss: 1.4094, train acc: 0.4056, val loss: 1.3539, val acc: 0.4469  (best train acc: 0.4320, best val acc: 0.4533)\n",
      "[Epoch: 3940] train loss: 1.4107, train acc: 0.4077, val loss: 1.3513, val acc: 0.4469  (best train acc: 0.4320, best val acc: 0.4533)\n",
      "[Epoch: 3960] train loss: 1.4098, train acc: 0.4123, val loss: 1.3495, val acc: 0.4506  (best train acc: 0.4320, best val acc: 0.4533)\n",
      "[Epoch: 3980] train loss: 1.4064, train acc: 0.4145, val loss: 1.3485, val acc: 0.4482  (best train acc: 0.4320, best val acc: 0.4533)\n",
      "[Epoch: 4000] train loss: 1.4112, train acc: 0.4119, val loss: 1.3468, val acc: 0.4465  (best train acc: 0.4320, best val acc: 0.4533)\n",
      "[Epoch: 4020] train loss: 1.4021, train acc: 0.4112, val loss: 1.3455, val acc: 0.4469  (best train acc: 0.4320, best val acc: 0.4533)\n",
      "[Epoch: 4040] train loss: 1.4029, train acc: 0.4182, val loss: 1.3435, val acc: 0.4486  (best train acc: 0.4320, best val acc: 0.4533)\n",
      "[Epoch: 4060] train loss: 1.4028, train acc: 0.4204, val loss: 1.3426, val acc: 0.4462  (best train acc: 0.4320, best val acc: 0.4533)\n",
      "[Epoch: 4080] train loss: 1.4017, train acc: 0.4174, val loss: 1.3355, val acc: 0.4577  (best train acc: 0.4320, best val acc: 0.4577)\n",
      "[Epoch: 4100] train loss: 1.3861, train acc: 0.4271, val loss: 1.3201, val acc: 0.4573  (best train acc: 0.4320, best val acc: 0.4587)\n",
      "[Epoch: 4120] train loss: 1.3809, train acc: 0.4242, val loss: 1.3189, val acc: 0.4546  (best train acc: 0.4320, best val acc: 0.4610)\n",
      "[Epoch: 4140] train loss: 1.3752, train acc: 0.4278, val loss: 1.3161, val acc: 0.4560  (best train acc: 0.4320, best val acc: 0.4621)\n",
      "[Epoch: 4160] train loss: 1.3811, train acc: 0.4236, val loss: 1.3183, val acc: 0.4509  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 4180] train loss: 1.9044, train acc: 0.2587, val loss: 1.8555, val acc: 0.2388  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 4200] train loss: 1.5692, train acc: 0.2851, val loss: 1.5599, val acc: 0.2405  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 4220] train loss: 1.5527, train acc: 0.2504, val loss: 1.5422, val acc: 0.2320  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 4240] train loss: 1.5513, train acc: 0.2393, val loss: 1.5346, val acc: 0.2654  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 4260] train loss: 1.5477, train acc: 0.2415, val loss: 1.5327, val acc: 0.2651  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 4280] train loss: 1.5430, train acc: 0.2548, val loss: 1.5302, val acc: 0.2685  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 4300] train loss: 1.5398, train acc: 0.2704, val loss: 1.5259, val acc: 0.2492  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 4320] train loss: 1.5405, train acc: 0.2684, val loss: 1.5253, val acc: 0.2543  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 4340] train loss: 1.5346, train acc: 0.2616, val loss: 1.5247, val acc: 0.2681  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 4360] train loss: 1.5353, train acc: 0.2568, val loss: 1.5242, val acc: 0.2675  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 4380] train loss: 1.5321, train acc: 0.2678, val loss: 1.5235, val acc: 0.2702  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 4400] train loss: 1.5295, train acc: 0.2484, val loss: 1.5228, val acc: 0.2654  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 4420] train loss: 1.5279, train acc: 0.2494, val loss: 1.5220, val acc: 0.2675  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 4440] train loss: 1.5270, train acc: 0.2395, val loss: 1.5212, val acc: 0.2698  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 4460] train loss: 1.5283, train acc: 0.2557, val loss: 1.5205, val acc: 0.2654  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 4480] train loss: 1.5250, train acc: 0.2602, val loss: 1.5197, val acc: 0.2678  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 4500] train loss: 1.5227, train acc: 0.2664, val loss: 1.5182, val acc: 0.2698  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 4520] train loss: 1.5210, train acc: 0.2664, val loss: 1.5159, val acc: 0.2654  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 4540] train loss: 1.5190, train acc: 0.2341, val loss: 1.5143, val acc: 0.2627  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 4560] train loss: 1.5161, train acc: 0.2166, val loss: 1.5118, val acc: 0.2293  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 4580] train loss: 1.4883, train acc: 0.2282, val loss: 1.4756, val acc: 0.2405  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 4600] train loss: 1.4469, train acc: 0.3338, val loss: 1.4153, val acc: 0.3707  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 4620] train loss: 1.4375, train acc: 0.3474, val loss: 1.3977, val acc: 0.3929  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 4640] train loss: 1.4319, train acc: 0.3733, val loss: 1.3866, val acc: 0.4088  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 4660] train loss: 1.4243, train acc: 0.3832, val loss: 1.3771, val acc: 0.4138  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 4680] train loss: 1.4323, train acc: 0.3817, val loss: 1.3790, val acc: 0.4159  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 4700] train loss: 1.4215, train acc: 0.3809, val loss: 1.3753, val acc: 0.4057  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 4720] train loss: 1.4281, train acc: 0.3726, val loss: 1.3760, val acc: 0.3987  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 4740] train loss: 1.4228, train acc: 0.3871, val loss: 1.3741, val acc: 0.4027  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 4760] train loss: 1.4256, train acc: 0.3784, val loss: 1.3712, val acc: 0.4078  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 4780] train loss: 1.4206, train acc: 0.3790, val loss: 1.3668, val acc: 0.4219  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 4800] train loss: 1.4189, train acc: 0.3861, val loss: 1.3680, val acc: 0.4128  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 4820] train loss: 1.4214, train acc: 0.3856, val loss: 1.3681, val acc: 0.4138  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 4840] train loss: 1.4221, train acc: 0.3892, val loss: 1.3685, val acc: 0.4118  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 4860] train loss: 1.4202, train acc: 0.3823, val loss: 1.3671, val acc: 0.4105  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 4880] train loss: 1.4208, train acc: 0.3808, val loss: 1.3678, val acc: 0.4084  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 4900] train loss: 1.4182, train acc: 0.3861, val loss: 1.3653, val acc: 0.4175  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 4920] train loss: 1.4230, train acc: 0.3878, val loss: 1.3727, val acc: 0.3946  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 4940] train loss: 1.4237, train acc: 0.3407, val loss: 1.3830, val acc: 0.3575  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 4960] train loss: 1.4202, train acc: 0.3814, val loss: 1.3732, val acc: 0.4074  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 4980] train loss: 1.4192, train acc: 0.3916, val loss: 1.3712, val acc: 0.4118  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 5000] train loss: 1.4122, train acc: 0.3905, val loss: 1.3691, val acc: 0.4148  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 5020] train loss: 1.4248, train acc: 0.3803, val loss: 1.3691, val acc: 0.4034  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 5040] train loss: 1.4118, train acc: 0.4012, val loss: 1.3657, val acc: 0.4236  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 5060] train loss: 1.4162, train acc: 0.4016, val loss: 1.3651, val acc: 0.4196  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 5080] train loss: 1.4078, train acc: 0.4024, val loss: 1.3582, val acc: 0.4175  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 5100] train loss: 9.0420, train acc: 0.2219, val loss: 2.0220, val acc: 0.2368  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 5120] train loss: 3.5031, train acc: 0.2588, val loss: 3.3531, val acc: 0.2368  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 5140] train loss: 2.0450, train acc: 0.2596, val loss: 1.9992, val acc: 0.2371  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 5160] train loss: 1.6580, train acc: 0.2597, val loss: 1.6508, val acc: 0.2371  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 5180] train loss: 1.5596, train acc: 0.2640, val loss: 1.5572, val acc: 0.2371  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 5200] train loss: 1.5409, train acc: 0.2473, val loss: 1.5343, val acc: 0.2381  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 5220] train loss: 1.5435, train acc: 0.1951, val loss: 1.5303, val acc: 0.2401  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 5240] train loss: 1.5415, train acc: 0.1857, val loss: 1.5290, val acc: 0.2455  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 5260] train loss: 1.5394, train acc: 0.1883, val loss: 1.5282, val acc: 0.2455  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 5280] train loss: 1.5369, train acc: 0.2024, val loss: 1.5276, val acc: 0.2455  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 5300] train loss: 1.5296, train acc: 0.1999, val loss: 1.5272, val acc: 0.2546  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 5320] train loss: 1.5402, train acc: 0.2029, val loss: 1.5269, val acc: 0.2492  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 5340] train loss: 1.5337, train acc: 0.2036, val loss: 1.5264, val acc: 0.2479  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 5360] train loss: 1.5369, train acc: 0.2125, val loss: 1.5262, val acc: 0.2523  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 5380] train loss: 1.5319, train acc: 0.2089, val loss: 1.5259, val acc: 0.2536  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 5400] train loss: 1.5312, train acc: 0.2097, val loss: 1.5257, val acc: 0.2523  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 5420] train loss: 1.5333, train acc: 0.2070, val loss: 1.5255, val acc: 0.2540  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 5440] train loss: 1.5367, train acc: 0.2326, val loss: 1.5253, val acc: 0.2540  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 5460] train loss: 1.5347, train acc: 0.2286, val loss: 1.5251, val acc: 0.2519  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 5480] train loss: 1.5341, train acc: 0.2374, val loss: 1.5249, val acc: 0.2543  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 5500] train loss: 1.5324, train acc: 0.2353, val loss: 1.5248, val acc: 0.2556  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 5520] train loss: 1.5276, train acc: 0.2375, val loss: 1.5246, val acc: 0.2573  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 5540] train loss: 1.5316, train acc: 0.2287, val loss: 1.5245, val acc: 0.2516  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 5560] train loss: 1.5350, train acc: 0.2295, val loss: 1.5243, val acc: 0.2526  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 5580] train loss: 1.5271, train acc: 0.2372, val loss: 1.5242, val acc: 0.2536  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 5600] train loss: 1.5302, train acc: 0.2334, val loss: 1.5241, val acc: 0.2536  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 5620] train loss: 1.5381, train acc: 0.2370, val loss: 1.5239, val acc: 0.2523  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 5640] train loss: 1.5327, train acc: 0.2293, val loss: 1.5238, val acc: 0.2516  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 5660] train loss: 1.5281, train acc: 0.2272, val loss: 1.5237, val acc: 0.2499  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 5680] train loss: 1.5327, train acc: 0.2340, val loss: 1.5235, val acc: 0.2506  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 5700] train loss: 1.5262, train acc: 0.2318, val loss: 1.5234, val acc: 0.2519  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 5720] train loss: 1.5334, train acc: 0.2334, val loss: 1.5232, val acc: 0.2519  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 5740] train loss: 1.5321, train acc: 0.2414, val loss: 1.5231, val acc: 0.2533  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 5760] train loss: 1.5306, train acc: 0.2404, val loss: 1.5229, val acc: 0.2526  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 5780] train loss: 1.5350, train acc: 0.2346, val loss: 1.5228, val acc: 0.2530  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 5800] train loss: 1.5343, train acc: 0.2326, val loss: 1.5227, val acc: 0.2523  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 5820] train loss: 1.5301, train acc: 0.2250, val loss: 1.5226, val acc: 0.2492  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 5840] train loss: 1.5297, train acc: 0.2374, val loss: 1.5225, val acc: 0.2516  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 5860] train loss: 1.5293, train acc: 0.2298, val loss: 1.5223, val acc: 0.2509  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 5880] train loss: 1.5291, train acc: 0.2360, val loss: 1.5222, val acc: 0.2513  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 5900] train loss: 1.5304, train acc: 0.2312, val loss: 1.5221, val acc: 0.2516  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 5920] train loss: 1.5288, train acc: 0.2349, val loss: 1.5220, val acc: 0.2530  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 5940] train loss: 1.5306, train acc: 0.2384, val loss: 1.5219, val acc: 0.2516  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 5960] train loss: 1.5272, train acc: 0.2389, val loss: 1.5219, val acc: 0.2519  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 5980] train loss: 1.5252, train acc: 0.2341, val loss: 1.5217, val acc: 0.2516  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 6000] train loss: 1.5321, train acc: 0.2330, val loss: 1.5216, val acc: 0.2513  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 6020] train loss: 1.5306, train acc: 0.2309, val loss: 1.5215, val acc: 0.2492  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 6040] train loss: 1.5294, train acc: 0.2297, val loss: 1.5214, val acc: 0.2469  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 6060] train loss: 1.5305, train acc: 0.2358, val loss: 1.5213, val acc: 0.2523  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 6080] train loss: 1.5280, train acc: 0.2402, val loss: 1.5213, val acc: 0.2513  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 6100] train loss: 1.5256, train acc: 0.2360, val loss: 1.5211, val acc: 0.2526  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 6120] train loss: 1.5226, train acc: 0.2388, val loss: 1.5211, val acc: 0.2533  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 6140] train loss: 1.5275, train acc: 0.2321, val loss: 1.5210, val acc: 0.2435  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 6160] train loss: 1.5281, train acc: 0.2290, val loss: 1.5209, val acc: 0.2449  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 6180] train loss: 1.5292, train acc: 0.2282, val loss: 1.5207, val acc: 0.2503  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 6200] train loss: 1.5234, train acc: 0.2344, val loss: 1.5207, val acc: 0.2516  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 6220] train loss: 1.5280, train acc: 0.2286, val loss: 1.5206, val acc: 0.2523  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 6240] train loss: 1.5249, train acc: 0.2351, val loss: 1.5205, val acc: 0.2438  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 6260] train loss: 1.5285, train acc: 0.2401, val loss: 1.5205, val acc: 0.2523  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 6280] train loss: 1.5266, train acc: 0.2363, val loss: 1.5203, val acc: 0.2499  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 6300] train loss: 1.5263, train acc: 0.2222, val loss: 1.5202, val acc: 0.2472  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 6320] train loss: 1.5274, train acc: 0.2202, val loss: 1.5200, val acc: 0.2472  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 6340] train loss: 1.5273, train acc: 0.2296, val loss: 1.5201, val acc: 0.2533  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 6360] train loss: 1.5248, train acc: 0.2172, val loss: 1.5199, val acc: 0.2445  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 6380] train loss: 1.5252, train acc: 0.2254, val loss: 1.5198, val acc: 0.2479  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 6400] train loss: 1.5229, train acc: 0.2326, val loss: 1.5197, val acc: 0.2503  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 6420] train loss: 1.5242, train acc: 0.2236, val loss: 1.5195, val acc: 0.2472  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 6440] train loss: 1.5235, train acc: 0.2413, val loss: 1.5195, val acc: 0.2489  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 6460] train loss: 1.5246, train acc: 0.2366, val loss: 1.5195, val acc: 0.2486  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 6480] train loss: 1.5239, train acc: 0.2431, val loss: 1.5193, val acc: 0.2513  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 6500] train loss: 1.5231, train acc: 0.2340, val loss: 1.5192, val acc: 0.2499  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 6520] train loss: 1.5242, train acc: 0.2290, val loss: 1.5191, val acc: 0.2479  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 6540] train loss: 1.5227, train acc: 0.2291, val loss: 1.5189, val acc: 0.2472  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 6560] train loss: 1.5244, train acc: 0.2318, val loss: 1.5188, val acc: 0.2462  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 6580] train loss: 1.5235, train acc: 0.2402, val loss: 1.5188, val acc: 0.2479  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 6600] train loss: 1.5211, train acc: 0.2414, val loss: 1.5187, val acc: 0.2513  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 6620] train loss: 1.5229, train acc: 0.2393, val loss: 1.5186, val acc: 0.2523  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 6640] train loss: 1.5232, train acc: 0.2340, val loss: 1.5184, val acc: 0.2509  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 6660] train loss: 1.5208, train acc: 0.2321, val loss: 1.5183, val acc: 0.2476  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 6680] train loss: 1.5213, train acc: 0.2437, val loss: 1.5182, val acc: 0.2459  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 6700] train loss: 1.5221, train acc: 0.2334, val loss: 1.5180, val acc: 0.2442  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 6720] train loss: 1.5216, train acc: 0.2319, val loss: 1.5179, val acc: 0.2492  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 6740] train loss: 1.5223, train acc: 0.2386, val loss: 1.5179, val acc: 0.2496  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 6760] train loss: 1.5214, train acc: 0.2316, val loss: 1.5177, val acc: 0.2489  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 6780] train loss: 1.5210, train acc: 0.2441, val loss: 1.5176, val acc: 0.2519  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 6800] train loss: 1.5202, train acc: 0.2482, val loss: 1.5176, val acc: 0.2523  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 6820] train loss: 1.5197, train acc: 0.2497, val loss: 1.5175, val acc: 0.2503  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 6840] train loss: 1.5203, train acc: 0.2392, val loss: 1.5173, val acc: 0.2486  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 6860] train loss: 1.5200, train acc: 0.2420, val loss: 1.5172, val acc: 0.2513  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 6880] train loss: 1.5193, train acc: 0.2452, val loss: 1.5171, val acc: 0.2435  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 6900] train loss: 1.5186, train acc: 0.2374, val loss: 1.5170, val acc: 0.2469  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 6920] train loss: 1.5193, train acc: 0.2277, val loss: 1.5169, val acc: 0.2438  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 6940] train loss: 1.5185, train acc: 0.2431, val loss: 1.5169, val acc: 0.2509  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 6960] train loss: 1.5185, train acc: 0.2353, val loss: 1.5169, val acc: 0.2472  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 6980] train loss: 1.5188, train acc: 0.2331, val loss: 1.5168, val acc: 0.2476  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 7000] train loss: 1.5181, train acc: 0.2397, val loss: 1.5167, val acc: 0.2442  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 7020] train loss: 1.5190, train acc: 0.2143, val loss: 1.5166, val acc: 0.2469  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 7040] train loss: 1.5184, train acc: 0.2217, val loss: 1.5165, val acc: 0.2472  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 7060] train loss: 1.5184, train acc: 0.2232, val loss: 1.5165, val acc: 0.2462  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 7080] train loss: 1.5180, train acc: 0.2318, val loss: 1.5165, val acc: 0.2459  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 7100] train loss: 1.5184, train acc: 0.2366, val loss: 1.5164, val acc: 0.2438  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 7120] train loss: 1.5182, train acc: 0.2175, val loss: 1.5164, val acc: 0.2479  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 7140] train loss: 1.5184, train acc: 0.2159, val loss: 1.5164, val acc: 0.2455  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 7160] train loss: 1.5182, train acc: 0.2261, val loss: 1.5164, val acc: 0.2449  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 7180] train loss: 1.5187, train acc: 0.2438, val loss: 1.5164, val acc: 0.2479  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 7200] train loss: 1.5180, train acc: 0.2099, val loss: 1.5162, val acc: 0.2374  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 7220] train loss: 1.5182, train acc: 0.2307, val loss: 1.5163, val acc: 0.2395  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 7240] train loss: 1.5180, train acc: 0.2214, val loss: 1.5163, val acc: 0.2486  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 7260] train loss: 1.5176, train acc: 0.2177, val loss: 1.5162, val acc: 0.2479  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 7280] train loss: 1.5180, train acc: 0.2348, val loss: 1.5163, val acc: 0.2465  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 7300] train loss: 1.5180, train acc: 0.2099, val loss: 1.5162, val acc: 0.2381  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 7320] train loss: 1.5177, train acc: 0.2347, val loss: 1.5163, val acc: 0.2479  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 7340] train loss: 1.5182, train acc: 0.2117, val loss: 1.5161, val acc: 0.2324  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 7360] train loss: 1.5182, train acc: 0.2281, val loss: 1.5163, val acc: 0.2432  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 7380] train loss: 1.5178, train acc: 0.2417, val loss: 1.5162, val acc: 0.2442  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 7400] train loss: 1.5180, train acc: 0.2243, val loss: 1.5162, val acc: 0.2408  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 7420] train loss: 1.5176, train acc: 0.2208, val loss: 1.5161, val acc: 0.2422  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 7440] train loss: 1.5178, train acc: 0.2164, val loss: 1.5161, val acc: 0.2445  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 7460] train loss: 1.5182, train acc: 0.2389, val loss: 1.5162, val acc: 0.2452  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 7480] train loss: 1.5173, train acc: 0.2146, val loss: 1.5161, val acc: 0.2411  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 7500] train loss: 1.5180, train acc: 0.2240, val loss: 1.5161, val acc: 0.2489  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 7520] train loss: 1.5178, train acc: 0.2196, val loss: 1.5161, val acc: 0.2395  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 7540] train loss: 1.5178, train acc: 0.2198, val loss: 1.5161, val acc: 0.2438  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 7560] train loss: 1.5177, train acc: 0.2424, val loss: 1.5163, val acc: 0.2496  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 7580] train loss: 1.5176, train acc: 0.2336, val loss: 1.5161, val acc: 0.2428  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 7600] train loss: 1.5178, train acc: 0.2368, val loss: 1.5161, val acc: 0.2445  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 7620] train loss: 1.5175, train acc: 0.2225, val loss: 1.5161, val acc: 0.2492  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 7640] train loss: 1.5179, train acc: 0.2042, val loss: 1.5160, val acc: 0.2341  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 7660] train loss: 1.5178, train acc: 0.2460, val loss: 1.5163, val acc: 0.2438  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 7680] train loss: 1.5175, train acc: 0.2214, val loss: 1.5160, val acc: 0.2425  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 7700] train loss: 1.5180, train acc: 0.2418, val loss: 1.5161, val acc: 0.2469  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 7720] train loss: 1.5179, train acc: 0.2363, val loss: 1.5161, val acc: 0.2438  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 7740] train loss: 1.5176, train acc: 0.2218, val loss: 1.5160, val acc: 0.2486  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 7760] train loss: 1.5179, train acc: 0.2244, val loss: 1.5160, val acc: 0.2422  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 7780] train loss: 1.5177, train acc: 0.2397, val loss: 1.5160, val acc: 0.2563  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 7800] train loss: 1.5180, train acc: 0.2412, val loss: 1.5160, val acc: 0.2398  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 7820] train loss: 1.5180, train acc: 0.2413, val loss: 1.5162, val acc: 0.2492  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 7840] train loss: 1.5181, train acc: 0.2185, val loss: 1.5161, val acc: 0.2459  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 7860] train loss: 1.5177, train acc: 0.2311, val loss: 1.5161, val acc: 0.2381  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 7880] train loss: 1.5179, train acc: 0.2385, val loss: 1.5161, val acc: 0.2452  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 7900] train loss: 1.5180, train acc: 0.2178, val loss: 1.5160, val acc: 0.2395  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 7920] train loss: 1.5178, train acc: 0.2420, val loss: 1.5161, val acc: 0.2452  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 7940] train loss: 1.5178, train acc: 0.2472, val loss: 1.5161, val acc: 0.2627  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 7960] train loss: 1.5180, train acc: 0.2204, val loss: 1.5159, val acc: 0.2452  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 7980] train loss: 1.5173, train acc: 0.2455, val loss: 1.5160, val acc: 0.2442  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 8000] train loss: 1.5175, train acc: 0.2423, val loss: 1.5160, val acc: 0.2523  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 8020] train loss: 1.5176, train acc: 0.2393, val loss: 1.5159, val acc: 0.2506  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 8040] train loss: 1.5179, train acc: 0.2072, val loss: 1.5159, val acc: 0.2331  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 8060] train loss: 1.5175, train acc: 0.2340, val loss: 1.5159, val acc: 0.2411  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 8080] train loss: 1.5174, train acc: 0.2333, val loss: 1.5160, val acc: 0.2503  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 8100] train loss: 1.5180, train acc: 0.2235, val loss: 1.5159, val acc: 0.2499  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 8120] train loss: 1.5177, train acc: 0.2479, val loss: 1.5159, val acc: 0.2435  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 8140] train loss: 1.5177, train acc: 0.2345, val loss: 1.5159, val acc: 0.2489  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 8160] train loss: 1.5176, train acc: 0.2250, val loss: 1.5158, val acc: 0.2452  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 8180] train loss: 1.5176, train acc: 0.2562, val loss: 1.5160, val acc: 0.2503  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 8200] train loss: 1.5175, train acc: 0.2257, val loss: 1.5159, val acc: 0.2526  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 8220] train loss: 1.5175, train acc: 0.2514, val loss: 1.5159, val acc: 0.2553  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 8240] train loss: 1.5177, train acc: 0.2527, val loss: 1.5159, val acc: 0.2580  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 8260] train loss: 1.5173, train acc: 0.2449, val loss: 1.5159, val acc: 0.2418  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 8280] train loss: 1.5175, train acc: 0.2542, val loss: 1.5160, val acc: 0.2621  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 8300] train loss: 1.5174, train acc: 0.2386, val loss: 1.5160, val acc: 0.2597  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 8320] train loss: 1.5175, train acc: 0.2353, val loss: 1.5159, val acc: 0.2519  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 8340] train loss: 1.5173, train acc: 0.2697, val loss: 1.5160, val acc: 0.2658  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 8360] train loss: 1.5175, train acc: 0.2191, val loss: 1.5158, val acc: 0.2499  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 8380] train loss: 1.5174, train acc: 0.2203, val loss: 1.5158, val acc: 0.2472  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 8400] train loss: 1.5178, train acc: 0.2454, val loss: 1.5159, val acc: 0.2428  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 8420] train loss: 1.5174, train acc: 0.2371, val loss: 1.5160, val acc: 0.2526  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 8440] train loss: 1.5174, train acc: 0.2406, val loss: 1.5159, val acc: 0.2465  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 8460] train loss: 1.5176, train acc: 0.2280, val loss: 1.5159, val acc: 0.2526  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 8480] train loss: 1.5175, train acc: 0.2508, val loss: 1.5159, val acc: 0.2513  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 8500] train loss: 1.5175, train acc: 0.2614, val loss: 1.5159, val acc: 0.2668  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 8520] train loss: 1.5177, train acc: 0.2346, val loss: 1.5158, val acc: 0.2428  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 8540] train loss: 1.5174, train acc: 0.2445, val loss: 1.5159, val acc: 0.2469  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 8560] train loss: 1.5175, train acc: 0.2458, val loss: 1.5159, val acc: 0.2621  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 8580] train loss: 1.5172, train acc: 0.2468, val loss: 1.5159, val acc: 0.2553  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 8600] train loss: 1.5174, train acc: 0.2490, val loss: 1.5159, val acc: 0.2577  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 8620] train loss: 1.5175, train acc: 0.2564, val loss: 1.5159, val acc: 0.2519  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 8640] train loss: 1.5174, train acc: 0.2600, val loss: 1.5159, val acc: 0.2594  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 8660] train loss: 1.5176, train acc: 0.2431, val loss: 1.5158, val acc: 0.2492  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 8680] train loss: 1.5176, train acc: 0.2495, val loss: 1.5159, val acc: 0.2506  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 8700] train loss: 1.5176, train acc: 0.2746, val loss: 1.5159, val acc: 0.2671  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 8720] train loss: 1.5175, train acc: 0.2538, val loss: 1.5159, val acc: 0.2587  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 8740] train loss: 1.5174, train acc: 0.2503, val loss: 1.5159, val acc: 0.2499  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 8760] train loss: 1.5178, train acc: 0.2535, val loss: 1.5158, val acc: 0.2546  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 8780] train loss: 1.5176, train acc: 0.2786, val loss: 1.5159, val acc: 0.2688  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 8800] train loss: 1.5172, train acc: 0.2963, val loss: 1.5159, val acc: 0.2708  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 8820] train loss: 1.5176, train acc: 0.2600, val loss: 1.5160, val acc: 0.2567  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 8840] train loss: 1.5173, train acc: 0.2381, val loss: 1.5158, val acc: 0.2452  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 8860] train loss: 1.5177, train acc: 0.2657, val loss: 1.5159, val acc: 0.2530  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 8880] train loss: 1.5173, train acc: 0.2546, val loss: 1.5160, val acc: 0.2634  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 8900] train loss: 1.5176, train acc: 0.2324, val loss: 1.5158, val acc: 0.2489  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 8920] train loss: 1.5174, train acc: 0.2542, val loss: 1.5157, val acc: 0.2604  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 8940] train loss: 1.5172, train acc: 0.2465, val loss: 1.5158, val acc: 0.2503  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 8960] train loss: 1.5172, train acc: 0.2444, val loss: 1.5158, val acc: 0.2472  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 8980] train loss: 1.5176, train acc: 0.2601, val loss: 1.5158, val acc: 0.2519  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 9000] train loss: 1.5171, train acc: 0.2695, val loss: 1.5160, val acc: 0.2675  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 9020] train loss: 1.5174, train acc: 0.2509, val loss: 1.5160, val acc: 0.2550  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 9040] train loss: 1.5175, train acc: 0.2573, val loss: 1.5159, val acc: 0.2621  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 9060] train loss: 1.5173, train acc: 0.2717, val loss: 1.5160, val acc: 0.2637  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 9080] train loss: 1.5174, train acc: 0.2874, val loss: 1.5160, val acc: 0.2799  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 9100] train loss: 1.5172, train acc: 0.2614, val loss: 1.5160, val acc: 0.2705  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 9120] train loss: 1.5173, train acc: 0.2697, val loss: 1.5160, val acc: 0.2728  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 9140] train loss: 1.5173, train acc: 0.2455, val loss: 1.5158, val acc: 0.2415  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 9160] train loss: 1.5173, train acc: 0.2402, val loss: 1.5157, val acc: 0.2351  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 9180] train loss: 1.5176, train acc: 0.2460, val loss: 1.5157, val acc: 0.2580  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 9200] train loss: 1.5170, train acc: 0.2653, val loss: 1.5158, val acc: 0.2627  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 9220] train loss: 1.5172, train acc: 0.2634, val loss: 1.5160, val acc: 0.2594  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 9240] train loss: 1.5172, train acc: 0.2721, val loss: 1.5159, val acc: 0.2671  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 9260] train loss: 1.5176, train acc: 0.2741, val loss: 1.5159, val acc: 0.2708  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 9280] train loss: 1.5175, train acc: 0.2489, val loss: 1.5158, val acc: 0.2600  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 9300] train loss: 1.5174, train acc: 0.2587, val loss: 1.5158, val acc: 0.2546  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 9320] train loss: 1.5175, train acc: 0.2567, val loss: 1.5158, val acc: 0.2661  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 9340] train loss: 1.5174, train acc: 0.2682, val loss: 1.5159, val acc: 0.2712  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 9360] train loss: 1.5175, train acc: 0.2659, val loss: 1.5158, val acc: 0.2688  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 9380] train loss: 1.5173, train acc: 0.2484, val loss: 1.5158, val acc: 0.2546  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 9400] train loss: 1.5174, train acc: 0.2470, val loss: 1.5157, val acc: 0.2509  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 9420] train loss: 1.5170, train acc: 0.2647, val loss: 1.5158, val acc: 0.2637  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 9440] train loss: 1.5174, train acc: 0.2694, val loss: 1.5158, val acc: 0.2621  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 9460] train loss: 1.5173, train acc: 0.2780, val loss: 1.5158, val acc: 0.2759  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 9480] train loss: 1.5172, train acc: 0.2463, val loss: 1.5158, val acc: 0.2415  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 9500] train loss: 1.5174, train acc: 0.2527, val loss: 1.5157, val acc: 0.2644  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 9520] train loss: 1.5173, train acc: 0.2329, val loss: 1.5159, val acc: 0.2523  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 9540] train loss: 1.5173, train acc: 0.2266, val loss: 1.5157, val acc: 0.2455  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 9560] train loss: 1.5173, train acc: 0.2497, val loss: 1.5157, val acc: 0.2826  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 9580] train loss: 1.5171, train acc: 0.2467, val loss: 1.5157, val acc: 0.2567  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 9600] train loss: 1.5172, train acc: 0.2467, val loss: 1.5157, val acc: 0.2695  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 9620] train loss: 1.5169, train acc: 0.2321, val loss: 1.5157, val acc: 0.2492  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 9640] train loss: 1.5171, train acc: 0.2423, val loss: 1.5157, val acc: 0.2567  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 9660] train loss: 1.5169, train acc: 0.2392, val loss: 1.5157, val acc: 0.2614  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 9680] train loss: 1.5170, train acc: 0.2544, val loss: 1.5155, val acc: 0.2803  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 9700] train loss: 1.5169, train acc: 0.2431, val loss: 1.5155, val acc: 0.2637  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 9720] train loss: 1.5171, train acc: 0.2567, val loss: 1.5157, val acc: 0.2742  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 9740] train loss: 1.5171, train acc: 0.2556, val loss: 1.5158, val acc: 0.2786  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 9760] train loss: 1.5171, train acc: 0.2645, val loss: 1.5157, val acc: 0.2813  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 9780] train loss: 1.5172, train acc: 0.2350, val loss: 1.5156, val acc: 0.2556  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 9800] train loss: 1.5172, train acc: 0.2666, val loss: 1.5156, val acc: 0.2786  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 9820] train loss: 1.5172, train acc: 0.2778, val loss: 1.5154, val acc: 0.2998  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 9840] train loss: 1.5172, train acc: 0.2179, val loss: 1.5154, val acc: 0.2482  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 9860] train loss: 1.5169, train acc: 0.2339, val loss: 1.5158, val acc: 0.2786  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 9880] train loss: 1.5169, train acc: 0.2624, val loss: 1.5159, val acc: 0.2712  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 9900] train loss: 1.5173, train acc: 0.2323, val loss: 1.5154, val acc: 0.2384  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 9920] train loss: 1.5171, train acc: 0.2513, val loss: 1.5156, val acc: 0.2631  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 9940] train loss: 1.5173, train acc: 0.2733, val loss: 1.5156, val acc: 0.2911  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 9960] train loss: 1.5171, train acc: 0.2501, val loss: 1.5159, val acc: 0.2627  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 9980] train loss: 1.5171, train acc: 0.2347, val loss: 1.5158, val acc: 0.2516  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 10000] train loss: 1.5173, train acc: 0.2319, val loss: 1.5157, val acc: 0.2506  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 10020] train loss: 1.5171, train acc: 0.2603, val loss: 1.5157, val acc: 0.2452  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 10040] train loss: 1.5170, train acc: 0.2572, val loss: 1.5156, val acc: 0.2556  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 10060] train loss: 1.5171, train acc: 0.2321, val loss: 1.5156, val acc: 0.2813  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 10080] train loss: 1.5172, train acc: 0.2750, val loss: 1.5156, val acc: 0.2867  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 10100] train loss: 1.5173, train acc: 0.2517, val loss: 1.5158, val acc: 0.2675  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 10120] train loss: 1.5172, train acc: 0.2574, val loss: 1.5156, val acc: 0.2651  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 10140] train loss: 1.5172, train acc: 0.2411, val loss: 1.5158, val acc: 0.2688  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 10160] train loss: 1.5173, train acc: 0.2822, val loss: 1.5158, val acc: 0.2860  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 10180] train loss: 1.5174, train acc: 0.2752, val loss: 1.5156, val acc: 0.2755  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 10200] train loss: 1.5172, train acc: 0.2540, val loss: 1.5155, val acc: 0.2577  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 10220] train loss: 1.5172, train acc: 0.2659, val loss: 1.5157, val acc: 0.2806  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 10240] train loss: 1.5167, train acc: 0.2733, val loss: 1.5153, val acc: 0.2567  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 10260] train loss: 1.5171, train acc: 0.2750, val loss: 1.5153, val acc: 0.2857  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 10280] train loss: 1.5172, train acc: 0.2698, val loss: 1.5152, val acc: 0.2769  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 10300] train loss: 1.5172, train acc: 0.2723, val loss: 1.5150, val acc: 0.2772  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 10320] train loss: 1.5168, train acc: 0.2517, val loss: 1.5150, val acc: 0.2621  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 10340] train loss: 1.5170, train acc: 0.2424, val loss: 1.5150, val acc: 0.2675  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 10360] train loss: 1.5169, train acc: 0.2638, val loss: 1.5152, val acc: 0.2745  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 10380] train loss: 1.5169, train acc: 0.2653, val loss: 1.5153, val acc: 0.2786  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 10400] train loss: 1.5166, train acc: 0.2429, val loss: 1.5150, val acc: 0.2691  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 10420] train loss: 1.5167, train acc: 0.2475, val loss: 1.5151, val acc: 0.2607  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 10440] train loss: 1.5169, train acc: 0.2272, val loss: 1.5147, val acc: 0.2482  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 10460] train loss: 1.5166, train acc: 0.2449, val loss: 1.5149, val acc: 0.2492  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 10480] train loss: 1.5165, train acc: 0.2427, val loss: 1.5147, val acc: 0.2604  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 10500] train loss: 1.5164, train acc: 0.2657, val loss: 1.5149, val acc: 0.2789  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 10520] train loss: 1.5165, train acc: 0.2807, val loss: 1.5150, val acc: 0.2772  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 10540] train loss: 1.5168, train acc: 0.2355, val loss: 1.5149, val acc: 0.2631  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 10560] train loss: 1.5164, train acc: 0.2679, val loss: 1.5148, val acc: 0.2654  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 10580] train loss: 1.5164, train acc: 0.2432, val loss: 1.5148, val acc: 0.2708  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 10600] train loss: 1.5166, train acc: 0.2461, val loss: 1.5146, val acc: 0.2617  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 10620] train loss: 1.5162, train acc: 0.2439, val loss: 1.5145, val acc: 0.2563  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 10640] train loss: 1.5165, train acc: 0.2415, val loss: 1.5146, val acc: 0.2651  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 10660] train loss: 1.5166, train acc: 0.2500, val loss: 1.5146, val acc: 0.2533  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 10680] train loss: 1.5169, train acc: 0.2504, val loss: 1.5140, val acc: 0.2614  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 10700] train loss: 1.5163, train acc: 0.2481, val loss: 1.5144, val acc: 0.2594  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 10720] train loss: 1.5165, train acc: 0.2732, val loss: 1.5145, val acc: 0.2820  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 10740] train loss: 1.5158, train acc: 0.2624, val loss: 1.5142, val acc: 0.2745  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 10760] train loss: 1.5163, train acc: 0.2522, val loss: 1.5142, val acc: 0.2668  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 10780] train loss: 1.5163, train acc: 0.2656, val loss: 1.5143, val acc: 0.2664  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 10800] train loss: 1.5162, train acc: 0.2475, val loss: 1.5142, val acc: 0.2621  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 10820] train loss: 1.5162, train acc: 0.2655, val loss: 1.5139, val acc: 0.2718  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 10840] train loss: 1.5161, train acc: 0.2520, val loss: 1.5136, val acc: 0.2637  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 10860] train loss: 1.5163, train acc: 0.2472, val loss: 1.5136, val acc: 0.2604  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 10880] train loss: 1.5160, train acc: 0.2653, val loss: 1.5137, val acc: 0.2732  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 10900] train loss: 1.5156, train acc: 0.2710, val loss: 1.5132, val acc: 0.2685  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 10920] train loss: 1.5155, train acc: 0.2556, val loss: 1.5133, val acc: 0.2671  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 10940] train loss: 1.5160, train acc: 0.2495, val loss: 1.5123, val acc: 0.2793  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 10960] train loss: 1.5295, train acc: 0.2372, val loss: 1.5163, val acc: 0.2408  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 10980] train loss: 1.5125, train acc: 0.3094, val loss: 1.5117, val acc: 0.2432  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 11000] train loss: 1.5091, train acc: 0.2702, val loss: 1.5079, val acc: 0.2499  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 11020] train loss: 1.5016, train acc: 0.2995, val loss: 1.4998, val acc: 0.2860  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 11040] train loss: 1.4875, train acc: 0.3253, val loss: 1.4836, val acc: 0.3285  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 11060] train loss: 1.4647, train acc: 0.3249, val loss: 1.4553, val acc: 0.3339  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 11080] train loss: 1.4591, train acc: 0.3221, val loss: 1.4442, val acc: 0.3248  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 11100] train loss: 1.4504, train acc: 0.3331, val loss: 1.4383, val acc: 0.3430  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 11120] train loss: 1.4535, train acc: 0.3462, val loss: 1.4360, val acc: 0.3541  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 11140] train loss: 1.4459, train acc: 0.3583, val loss: 1.4343, val acc: 0.3379  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 11160] train loss: 1.4462, train acc: 0.3631, val loss: 1.4310, val acc: 0.3605  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 11180] train loss: 1.4458, train acc: 0.3725, val loss: 1.4272, val acc: 0.3781  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 11200] train loss: 1.4469, train acc: 0.3352, val loss: 1.4312, val acc: 0.3413  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 11220] train loss: 1.4382, train acc: 0.3660, val loss: 1.4247, val acc: 0.3508  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 11240] train loss: 1.4463, train acc: 0.3078, val loss: 1.4242, val acc: 0.3740  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 11260] train loss: 1.4316, train acc: 0.3487, val loss: 1.4193, val acc: 0.3423  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 11280] train loss: 1.4285, train acc: 0.3796, val loss: 1.4129, val acc: 0.3757  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 11300] train loss: 1.4268, train acc: 0.4004, val loss: 1.4088, val acc: 0.3906  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 11320] train loss: 1.4223, train acc: 0.3921, val loss: 1.4047, val acc: 0.4088  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 11340] train loss: 1.5148, train acc: 0.2230, val loss: 1.4675, val acc: 0.3535  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 11360] train loss: 1.4547, train acc: 0.3376, val loss: 1.4314, val acc: 0.3484  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 11380] train loss: 1.4325, train acc: 0.3347, val loss: 1.4173, val acc: 0.3464  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 11400] train loss: 1.4282, train acc: 0.3821, val loss: 1.4039, val acc: 0.3987  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 11420] train loss: 1.4204, train acc: 0.3900, val loss: 1.3996, val acc: 0.3963  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 11440] train loss: 1.4207, train acc: 0.3864, val loss: 1.3969, val acc: 0.4044  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 11460] train loss: 1.4220, train acc: 0.3775, val loss: 1.3960, val acc: 0.4138  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 11480] train loss: 1.4163, train acc: 0.4038, val loss: 1.3932, val acc: 0.4074  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 11500] train loss: 1.4140, train acc: 0.4075, val loss: 1.3915, val acc: 0.4064  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 11520] train loss: 1.4357, train acc: 0.3587, val loss: 1.4207, val acc: 0.3568  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 11540] train loss: 1.4472, train acc: 0.3135, val loss: 1.4228, val acc: 0.3336  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 11560] train loss: 1.4315, train acc: 0.3325, val loss: 1.4038, val acc: 0.3690  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 11580] train loss: 1.4146, train acc: 0.4015, val loss: 1.3919, val acc: 0.3916  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 11600] train loss: 1.4113, train acc: 0.4025, val loss: 1.3856, val acc: 0.4007  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 11620] train loss: 1.4079, train acc: 0.4071, val loss: 1.3835, val acc: 0.4044  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 11640] train loss: 1.4068, train acc: 0.4050, val loss: 1.3816, val acc: 0.4074  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 11660] train loss: 1.4062, train acc: 0.3978, val loss: 1.3800, val acc: 0.4017  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 11680] train loss: 1.4045, train acc: 0.4067, val loss: 1.3786, val acc: 0.4040  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 11700] train loss: 1.4022, train acc: 0.4005, val loss: 1.3773, val acc: 0.4030  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 11720] train loss: 1.4026, train acc: 0.3919, val loss: 1.3839, val acc: 0.3781  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 11740] train loss: 1.3985, train acc: 0.4085, val loss: 1.3763, val acc: 0.4101  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 11760] train loss: 1.3947, train acc: 0.4111, val loss: 1.3721, val acc: 0.4105  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 11780] train loss: 1.3992, train acc: 0.4032, val loss: 1.3702, val acc: 0.4145  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 11800] train loss: 1.4200, train acc: 0.3679, val loss: 1.4004, val acc: 0.3642  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 11820] train loss: 1.3962, train acc: 0.3951, val loss: 1.3759, val acc: 0.4027  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 11840] train loss: 1.3919, train acc: 0.4101, val loss: 1.3699, val acc: 0.4159  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 11860] train loss: 1.3943, train acc: 0.4116, val loss: 1.3678, val acc: 0.4239  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 11880] train loss: 1.3921, train acc: 0.4121, val loss: 1.3650, val acc: 0.4219  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 11900] train loss: 1.3896, train acc: 0.4177, val loss: 1.3680, val acc: 0.4037  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 11920] train loss: 1.3851, train acc: 0.4124, val loss: 1.3627, val acc: 0.4196  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 11940] train loss: 1.3892, train acc: 0.4118, val loss: 1.3609, val acc: 0.4229  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 11960] train loss: 1.3866, train acc: 0.4143, val loss: 1.3620, val acc: 0.4297  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 11980] train loss: 1.3855, train acc: 0.4184, val loss: 1.3603, val acc: 0.4145  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 12000] train loss: 1.4365, train acc: 0.3511, val loss: 1.4161, val acc: 0.3686  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 12020] train loss: 1.4042, train acc: 0.3806, val loss: 1.3748, val acc: 0.3949  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 12040] train loss: 1.3869, train acc: 0.4155, val loss: 1.3593, val acc: 0.4243  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 12060] train loss: 1.3863, train acc: 0.4164, val loss: 1.3591, val acc: 0.4280  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 12080] train loss: 1.3836, train acc: 0.4178, val loss: 1.3570, val acc: 0.4236  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 12100] train loss: 1.3919, train acc: 0.4060, val loss: 1.3665, val acc: 0.4007  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 12120] train loss: 1.3886, train acc: 0.4154, val loss: 1.3563, val acc: 0.4266  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 12140] train loss: 1.3846, train acc: 0.4088, val loss: 1.3535, val acc: 0.4290  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 12160] train loss: 1.3820, train acc: 0.4258, val loss: 1.3540, val acc: 0.4246  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 12180] train loss: 1.3906, train acc: 0.3944, val loss: 1.3529, val acc: 0.4270  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 12200] train loss: 1.3825, train acc: 0.4152, val loss: 1.3529, val acc: 0.4250  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 12220] train loss: 1.3780, train acc: 0.4228, val loss: 1.3501, val acc: 0.4297  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 12240] train loss: 1.3796, train acc: 0.4232, val loss: 1.3490, val acc: 0.4273  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 12260] train loss: 1.3777, train acc: 0.4244, val loss: 1.3504, val acc: 0.4206  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 12280] train loss: 1.3806, train acc: 0.4214, val loss: 1.3907, val acc: 0.3885  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 12300] train loss: 1.3819, train acc: 0.4098, val loss: 1.3748, val acc: 0.4128  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 12320] train loss: 1.3787, train acc: 0.4247, val loss: 1.3482, val acc: 0.4263  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 12340] train loss: 1.3769, train acc: 0.4123, val loss: 1.3458, val acc: 0.4293  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 12360] train loss: 1.3788, train acc: 0.4151, val loss: 1.3467, val acc: 0.4253  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 12380] train loss: 1.3749, train acc: 0.4186, val loss: 1.3452, val acc: 0.4260  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 12400] train loss: 1.3783, train acc: 0.4235, val loss: 1.3453, val acc: 0.4239  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 12420] train loss: 1.3705, train acc: 0.4240, val loss: 1.3466, val acc: 0.4354  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 12440] train loss: 1.3785, train acc: 0.4164, val loss: 1.3513, val acc: 0.4152  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 12460] train loss: 1.3774, train acc: 0.4172, val loss: 1.3471, val acc: 0.4206  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 12480] train loss: 1.3704, train acc: 0.4158, val loss: 1.3432, val acc: 0.4236  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 12500] train loss: 1.3725, train acc: 0.4235, val loss: 1.3413, val acc: 0.4246  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 12520] train loss: 1.3730, train acc: 0.4283, val loss: 1.3414, val acc: 0.4206  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 12540] train loss: 1.3716, train acc: 0.4161, val loss: 1.3546, val acc: 0.4260  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 12560] train loss: 1.3690, train acc: 0.4256, val loss: 1.3401, val acc: 0.4341  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 12580] train loss: 1.3675, train acc: 0.4291, val loss: 1.3390, val acc: 0.4246  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 12600] train loss: 1.3669, train acc: 0.4278, val loss: 1.3376, val acc: 0.4290  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 12620] train loss: 1.3688, train acc: 0.4153, val loss: 1.3379, val acc: 0.4378  (best train acc: 0.4320, best val acc: 0.4651)\n",
      "[Epoch: 12640] train loss: 1.3676, train acc: 0.4268, val loss: 1.3372, val acc: 0.4364  (best train acc: 0.4344, best val acc: 0.4651)\n",
      "[Epoch: 12660] train loss: 1.3851, train acc: 0.3936, val loss: 1.3401, val acc: 0.4256  (best train acc: 0.4344, best val acc: 0.4651)\n",
      "[Epoch: 12680] train loss: 1.4219, train acc: 0.3511, val loss: 1.3738, val acc: 0.3973  (best train acc: 0.4344, best val acc: 0.4651)\n",
      "[Epoch: 12700] train loss: 1.3759, train acc: 0.4134, val loss: 1.3523, val acc: 0.4192  (best train acc: 0.4344, best val acc: 0.4651)\n",
      "[Epoch: 12720] train loss: 1.3669, train acc: 0.4181, val loss: 1.3378, val acc: 0.4408  (best train acc: 0.4344, best val acc: 0.4651)\n",
      "[Epoch: 12740] train loss: 1.3628, train acc: 0.4258, val loss: 1.3350, val acc: 0.4398  (best train acc: 0.4344, best val acc: 0.4651)\n",
      "[Epoch: 12760] train loss: 1.3627, train acc: 0.4314, val loss: 1.3347, val acc: 0.4314  (best train acc: 0.4344, best val acc: 0.4651)\n",
      "[Epoch: 12780] train loss: 1.3626, train acc: 0.4240, val loss: 1.3330, val acc: 0.4415  (best train acc: 0.4344, best val acc: 0.4651)\n",
      "[Epoch: 12800] train loss: 1.3605, train acc: 0.4315, val loss: 1.3328, val acc: 0.4358  (best train acc: 0.4344, best val acc: 0.4651)\n",
      "[Epoch: 12820] train loss: 1.3575, train acc: 0.4249, val loss: 1.3345, val acc: 0.4452  (best train acc: 0.4344, best val acc: 0.4651)\n",
      "[Epoch: 12840] train loss: 1.3605, train acc: 0.4275, val loss: 1.3311, val acc: 0.4371  (best train acc: 0.4344, best val acc: 0.4651)\n",
      "[Epoch: 12860] train loss: 1.3662, train acc: 0.4253, val loss: 1.3322, val acc: 0.4317  (best train acc: 0.4344, best val acc: 0.4651)\n",
      "[Epoch: 12880] train loss: 1.3588, train acc: 0.4250, val loss: 1.3302, val acc: 0.4415  (best train acc: 0.4344, best val acc: 0.4651)\n",
      "[Epoch: 12900] train loss: 1.3613, train acc: 0.4203, val loss: 1.3341, val acc: 0.4314  (best train acc: 0.4344, best val acc: 0.4651)\n",
      "[Epoch: 12920] train loss: 1.3621, train acc: 0.4163, val loss: 1.3305, val acc: 0.4361  (best train acc: 0.4344, best val acc: 0.4651)\n",
      "[Epoch: 12940] train loss: 1.3603, train acc: 0.4218, val loss: 1.3285, val acc: 0.4422  (best train acc: 0.4344, best val acc: 0.4651)\n",
      "[Epoch: 12960] train loss: 1.3563, train acc: 0.4350, val loss: 1.3276, val acc: 0.4391  (best train acc: 0.4353, best val acc: 0.4651)\n",
      "[Epoch: 12980] train loss: 1.3540, train acc: 0.4289, val loss: 1.3273, val acc: 0.4401  (best train acc: 0.4353, best val acc: 0.4651)\n",
      "[Epoch: 13000] train loss: 1.3526, train acc: 0.4315, val loss: 1.3280, val acc: 0.4452  (best train acc: 0.4353, best val acc: 0.4651)\n",
      "[Epoch: 13020] train loss: 1.3572, train acc: 0.4197, val loss: 1.3265, val acc: 0.4354  (best train acc: 0.4353, best val acc: 0.4651)\n",
      "[Epoch: 13040] train loss: 1.3554, train acc: 0.4267, val loss: 1.3343, val acc: 0.4374  (best train acc: 0.4353, best val acc: 0.4651)\n",
      "[Epoch: 13060] train loss: 1.3629, train acc: 0.4205, val loss: 1.3256, val acc: 0.4388  (best train acc: 0.4353, best val acc: 0.4651)\n",
      "[Epoch: 13080] train loss: 1.3613, train acc: 0.4208, val loss: 1.3326, val acc: 0.4226  (best train acc: 0.4353, best val acc: 0.4651)\n",
      "[Epoch: 13100] train loss: 1.3540, train acc: 0.4313, val loss: 1.3292, val acc: 0.4331  (best train acc: 0.4353, best val acc: 0.4651)\n",
      "[Epoch: 13120] train loss: 1.3507, train acc: 0.4263, val loss: 1.3251, val acc: 0.4378  (best train acc: 0.4366, best val acc: 0.4651)\n",
      "[Epoch: 13140] train loss: 1.3515, train acc: 0.4314, val loss: 1.3227, val acc: 0.4449  (best train acc: 0.4377, best val acc: 0.4651)\n",
      "[Epoch: 13160] train loss: 1.3506, train acc: 0.4284, val loss: 1.3219, val acc: 0.4411  (best train acc: 0.4377, best val acc: 0.4651)\n",
      "[Epoch: 13180] train loss: 1.3512, train acc: 0.4262, val loss: 1.3210, val acc: 0.4438  (best train acc: 0.4405, best val acc: 0.4651)\n",
      "[Epoch: 13200] train loss: 1.3545, train acc: 0.4291, val loss: 1.3328, val acc: 0.4165  (best train acc: 0.4405, best val acc: 0.4651)\n",
      "[Epoch: 13220] train loss: 1.3442, train acc: 0.4262, val loss: 1.3211, val acc: 0.4418  (best train acc: 0.4405, best val acc: 0.4651)\n",
      "[Epoch: 13240] train loss: 1.3424, train acc: 0.4380, val loss: 1.3208, val acc: 0.4445  (best train acc: 0.4405, best val acc: 0.4651)\n",
      "[Epoch: 13260] train loss: 1.3477, train acc: 0.4337, val loss: 1.3190, val acc: 0.4432  (best train acc: 0.4405, best val acc: 0.4651)\n",
      "[Epoch: 13280] train loss: 1.3441, train acc: 0.4349, val loss: 1.3170, val acc: 0.4452  (best train acc: 0.4405, best val acc: 0.4651)\n",
      "[Epoch: 13300] train loss: 1.3436, train acc: 0.4328, val loss: 1.3161, val acc: 0.4445  (best train acc: 0.4405, best val acc: 0.4651)\n",
      "[Epoch: 13320] train loss: 1.3430, train acc: 0.4341, val loss: 1.3158, val acc: 0.4465  (best train acc: 0.4405, best val acc: 0.4651)\n",
      "[Epoch: 13340] train loss: 1.3468, train acc: 0.4333, val loss: 1.3138, val acc: 0.4469  (best train acc: 0.4405, best val acc: 0.4651)\n",
      "[Epoch: 13360] train loss: 1.3410, train acc: 0.4344, val loss: 1.3124, val acc: 0.4503  (best train acc: 0.4435, best val acc: 0.4651)\n",
      "[Epoch: 13380] train loss: 1.3481, train acc: 0.4250, val loss: 1.3123, val acc: 0.4492  (best train acc: 0.4435, best val acc: 0.4651)\n",
      "[Epoch: 13400] train loss: 1.3417, train acc: 0.4339, val loss: 1.3121, val acc: 0.4472  (best train acc: 0.4435, best val acc: 0.4651)\n",
      "[Epoch: 13420] train loss: 1.3372, train acc: 0.4394, val loss: 1.3104, val acc: 0.4523  (best train acc: 0.4435, best val acc: 0.4651)\n",
      "[Epoch: 13440] train loss: 1.3393, train acc: 0.4375, val loss: 1.3085, val acc: 0.4553  (best train acc: 0.4435, best val acc: 0.4651)\n",
      "[Epoch: 13460] train loss: 1.3373, train acc: 0.4422, val loss: 1.3075, val acc: 0.4526  (best train acc: 0.4453, best val acc: 0.4651)\n",
      "[Epoch: 13480] train loss: 1.3361, train acc: 0.4457, val loss: 1.3085, val acc: 0.4506  (best train acc: 0.4457, best val acc: 0.4651)\n",
      "[Epoch: 13500] train loss: 1.3365, train acc: 0.4398, val loss: 1.3103, val acc: 0.4442  (best train acc: 0.4475, best val acc: 0.4651)\n",
      "[Epoch: 13520] train loss: 1.3359, train acc: 0.4451, val loss: 1.3058, val acc: 0.4550  (best train acc: 0.4483, best val acc: 0.4651)\n",
      "[Epoch: 13540] train loss: 1.3351, train acc: 0.4407, val loss: 1.3027, val acc: 0.4580  (best train acc: 0.4483, best val acc: 0.4651)\n",
      "[Epoch: 13560] train loss: 1.3334, train acc: 0.4423, val loss: 1.3019, val acc: 0.4600  (best train acc: 0.4483, best val acc: 0.4651)\n",
      "[Epoch: 13580] train loss: 1.9195, train acc: 0.2337, val loss: 2.4780, val acc: 0.2368  (best train acc: 0.4483, best val acc: 0.4651)\n",
      "[Epoch: 13600] train loss: 1.5361, train acc: 0.2716, val loss: 1.5363, val acc: 0.2368  (best train acc: 0.4483, best val acc: 0.4651)\n",
      "[Epoch: 13620] train loss: 1.5105, train acc: 0.2149, val loss: 1.5100, val acc: 0.2334  (best train acc: 0.4483, best val acc: 0.4651)\n",
      "[Epoch: 13640] train loss: 1.5080, train acc: 0.2330, val loss: 1.5035, val acc: 0.3288  (best train acc: 0.4483, best val acc: 0.4651)\n",
      "[Epoch: 13660] train loss: 1.5033, train acc: 0.2739, val loss: 1.5016, val acc: 0.2671  (best train acc: 0.4483, best val acc: 0.4651)\n",
      "[Epoch: 13680] train loss: 1.5034, train acc: 0.2772, val loss: 1.4999, val acc: 0.3120  (best train acc: 0.4483, best val acc: 0.4651)\n",
      "[Epoch: 13700] train loss: 1.5020, train acc: 0.2991, val loss: 1.4985, val acc: 0.3440  (best train acc: 0.4483, best val acc: 0.4651)\n",
      "[Epoch: 13720] train loss: 1.5004, train acc: 0.3049, val loss: 1.4972, val acc: 0.3430  (best train acc: 0.4483, best val acc: 0.4651)\n",
      "[Epoch: 13740] train loss: 1.4973, train acc: 0.3226, val loss: 1.4933, val acc: 0.3653  (best train acc: 0.4483, best val acc: 0.4651)\n",
      "[Epoch: 13760] train loss: 1.4910, train acc: 0.3154, val loss: 1.4896, val acc: 0.3373  (best train acc: 0.4483, best val acc: 0.4651)\n",
      "[Epoch: 13780] train loss: 1.4899, train acc: 0.3349, val loss: 1.4872, val acc: 0.3551  (best train acc: 0.4483, best val acc: 0.4651)\n",
      "[Epoch: 13800] train loss: 1.4874, train acc: 0.3433, val loss: 1.4845, val acc: 0.3582  (best train acc: 0.4483, best val acc: 0.4651)\n",
      "[Epoch: 13820] train loss: 1.4850, train acc: 0.3496, val loss: 1.4815, val acc: 0.3619  (best train acc: 0.4483, best val acc: 0.4651)\n",
      "[Epoch: 13840] train loss: 1.4805, train acc: 0.3543, val loss: 1.4780, val acc: 0.3693  (best train acc: 0.4483, best val acc: 0.4651)\n",
      "[Epoch: 13860] train loss: 1.4754, train acc: 0.3681, val loss: 1.4747, val acc: 0.3794  (best train acc: 0.4483, best val acc: 0.4651)\n",
      "[Epoch: 13880] train loss: 1.4727, train acc: 0.3736, val loss: 1.4698, val acc: 0.3808  (best train acc: 0.4483, best val acc: 0.4651)\n",
      "[Epoch: 13900] train loss: 1.4687, train acc: 0.3734, val loss: 1.4650, val acc: 0.3777  (best train acc: 0.4483, best val acc: 0.4651)\n",
      "[Epoch: 13920] train loss: 1.4623, train acc: 0.3830, val loss: 1.4598, val acc: 0.3821  (best train acc: 0.4483, best val acc: 0.4651)\n",
      "[Epoch: 13940] train loss: 1.4580, train acc: 0.3856, val loss: 1.4536, val acc: 0.3885  (best train acc: 0.4483, best val acc: 0.4651)\n",
      "[Epoch: 13960] train loss: 1.4516, train acc: 0.3904, val loss: 1.4462, val acc: 0.3875  (best train acc: 0.4483, best val acc: 0.4651)\n",
      "[Epoch: 13980] train loss: 1.4443, train acc: 0.3827, val loss: 1.4381, val acc: 0.3788  (best train acc: 0.4483, best val acc: 0.4651)\n",
      "[Epoch: 14000] train loss: 1.4381, train acc: 0.3906, val loss: 1.4289, val acc: 0.3798  (best train acc: 0.4483, best val acc: 0.4651)\n",
      "[Epoch: 14020] train loss: 1.4279, train acc: 0.3994, val loss: 1.4193, val acc: 0.3791  (best train acc: 0.4483, best val acc: 0.4651)\n",
      "[Epoch: 14040] train loss: 1.4183, train acc: 0.4003, val loss: 1.4080, val acc: 0.3825  (best train acc: 0.4483, best val acc: 0.4651)\n",
      "[Epoch: 14060] train loss: 1.4132, train acc: 0.3957, val loss: 1.3989, val acc: 0.3973  (best train acc: 0.4483, best val acc: 0.4651)\n",
      "[Epoch: 14080] train loss: 1.4072, train acc: 0.4104, val loss: 1.3931, val acc: 0.4013  (best train acc: 0.4483, best val acc: 0.4651)\n",
      "[Epoch: 14100] train loss: 1.4009, train acc: 0.4151, val loss: 1.3892, val acc: 0.4040  (best train acc: 0.4483, best val acc: 0.4651)\n",
      "[Epoch: 14120] train loss: 1.4018, train acc: 0.4119, val loss: 1.3866, val acc: 0.4098  (best train acc: 0.4483, best val acc: 0.4651)\n",
      "[Epoch: 14140] train loss: 1.3973, train acc: 0.4187, val loss: 1.3850, val acc: 0.4111  (best train acc: 0.4483, best val acc: 0.4651)\n",
      "[Epoch: 14160] train loss: 1.3996, train acc: 0.4204, val loss: 1.3838, val acc: 0.4132  (best train acc: 0.4483, best val acc: 0.4651)\n",
      "[Epoch: 14180] train loss: 1.3970, train acc: 0.4212, val loss: 1.3833, val acc: 0.4118  (best train acc: 0.4483, best val acc: 0.4651)\n",
      "[Epoch: 14200] train loss: 1.3995, train acc: 0.4127, val loss: 1.3826, val acc: 0.4105  (best train acc: 0.4483, best val acc: 0.4651)\n",
      "[Epoch: 14220] train loss: 1.3945, train acc: 0.4199, val loss: 1.3859, val acc: 0.4017  (best train acc: 0.4483, best val acc: 0.4651)\n",
      "[Epoch: 14240] train loss: 1.3970, train acc: 0.4194, val loss: 1.3822, val acc: 0.4108  (best train acc: 0.4483, best val acc: 0.4651)\n",
      "[Epoch: 14260] train loss: 1.3988, train acc: 0.4226, val loss: 1.3818, val acc: 0.4142  (best train acc: 0.4483, best val acc: 0.4651)\n",
      "[Epoch: 14280] train loss: 1.3955, train acc: 0.4214, val loss: 1.3813, val acc: 0.4128  (best train acc: 0.4483, best val acc: 0.4651)\n",
      "[Epoch: 14300] train loss: 1.3981, train acc: 0.4212, val loss: 1.3811, val acc: 0.4132  (best train acc: 0.4483, best val acc: 0.4651)\n",
      "[Epoch: 14320] train loss: 1.3957, train acc: 0.4260, val loss: 1.3809, val acc: 0.4128  (best train acc: 0.4483, best val acc: 0.4651)\n",
      "[Epoch: 14340] train loss: 1.3966, train acc: 0.4190, val loss: 1.3805, val acc: 0.4128  (best train acc: 0.4483, best val acc: 0.4651)\n",
      "[Epoch: 14360] train loss: 1.3951, train acc: 0.4240, val loss: 1.3804, val acc: 0.4115  (best train acc: 0.4483, best val acc: 0.4651)\n",
      "[Epoch: 14380] train loss: 1.3939, train acc: 0.4226, val loss: 1.3800, val acc: 0.4128  (best train acc: 0.4483, best val acc: 0.4651)\n",
      "[Epoch: 14400] train loss: 1.3913, train acc: 0.4237, val loss: 1.3799, val acc: 0.4121  (best train acc: 0.4483, best val acc: 0.4651)\n",
      "[Epoch: 14420] train loss: 1.3989, train acc: 0.4257, val loss: 1.3796, val acc: 0.4142  (best train acc: 0.4483, best val acc: 0.4651)\n",
      "[Epoch: 14440] train loss: 1.3928, train acc: 0.4185, val loss: 1.3796, val acc: 0.4172  (best train acc: 0.4483, best val acc: 0.4651)\n",
      "[Epoch: 14460] train loss: 1.3935, train acc: 0.4200, val loss: 1.3794, val acc: 0.4152  (best train acc: 0.4483, best val acc: 0.4651)\n",
      "[Epoch: 14480] train loss: 1.3963, train acc: 0.4229, val loss: 1.3793, val acc: 0.4145  (best train acc: 0.4483, best val acc: 0.4651)\n",
      "[Epoch: 14500] train loss: 1.3905, train acc: 0.4244, val loss: 1.3791, val acc: 0.4165  (best train acc: 0.4483, best val acc: 0.4651)\n",
      "[Epoch: 14520] train loss: 1.3950, train acc: 0.4238, val loss: 1.3789, val acc: 0.4148  (best train acc: 0.4483, best val acc: 0.4651)\n",
      "[Epoch: 14540] train loss: 1.3938, train acc: 0.4224, val loss: 1.3788, val acc: 0.4135  (best train acc: 0.4483, best val acc: 0.4651)\n",
      "[Epoch: 14560] train loss: 1.3933, train acc: 0.4237, val loss: 1.3786, val acc: 0.4121  (best train acc: 0.4483, best val acc: 0.4651)\n",
      "[Epoch: 14580] train loss: 1.3961, train acc: 0.4237, val loss: 1.3785, val acc: 0.4165  (best train acc: 0.4483, best val acc: 0.4651)\n",
      "[Epoch: 14600] train loss: 1.3945, train acc: 0.4264, val loss: 1.3785, val acc: 0.4142  (best train acc: 0.4483, best val acc: 0.4651)\n",
      "[Epoch: 14620] train loss: 1.3927, train acc: 0.4226, val loss: 1.3783, val acc: 0.4094  (best train acc: 0.4483, best val acc: 0.4651)\n",
      "[Epoch: 14640] train loss: 1.3880, train acc: 0.4210, val loss: 1.3782, val acc: 0.4115  (best train acc: 0.4483, best val acc: 0.4651)\n",
      "[Epoch: 14660] train loss: 1.3943, train acc: 0.4185, val loss: 1.3781, val acc: 0.4132  (best train acc: 0.4483, best val acc: 0.4651)\n",
      "[Epoch: 14680] train loss: 1.3922, train acc: 0.4240, val loss: 1.3781, val acc: 0.4105  (best train acc: 0.4483, best val acc: 0.4651)\n",
      "[Epoch: 14700] train loss: 1.3886, train acc: 0.4218, val loss: 1.3779, val acc: 0.4138  (best train acc: 0.4483, best val acc: 0.4651)\n",
      "[Epoch: 14720] train loss: 1.3928, train acc: 0.4255, val loss: 1.3778, val acc: 0.4138  (best train acc: 0.4483, best val acc: 0.4651)\n",
      "[Epoch: 14740] train loss: 1.3891, train acc: 0.4239, val loss: 1.3776, val acc: 0.4179  (best train acc: 0.4483, best val acc: 0.4651)\n",
      "[Epoch: 14760] train loss: 1.3911, train acc: 0.4246, val loss: 1.3775, val acc: 0.4152  (best train acc: 0.4483, best val acc: 0.4651)\n",
      "[Epoch: 14780] train loss: 1.3891, train acc: 0.4235, val loss: 1.3773, val acc: 0.4132  (best train acc: 0.4483, best val acc: 0.4651)\n",
      "[Epoch: 14800] train loss: 1.3907, train acc: 0.4254, val loss: 1.3773, val acc: 0.4175  (best train acc: 0.4483, best val acc: 0.4651)\n",
      "[Epoch: 14820] train loss: 1.3887, train acc: 0.4238, val loss: 1.3773, val acc: 0.4179  (best train acc: 0.4483, best val acc: 0.4651)\n",
      "[Epoch: 14840] train loss: 1.3895, train acc: 0.4216, val loss: 1.3770, val acc: 0.4118  (best train acc: 0.4483, best val acc: 0.4651)\n",
      "[Epoch: 14860] train loss: 1.3893, train acc: 0.4237, val loss: 1.3767, val acc: 0.4152  (best train acc: 0.4483, best val acc: 0.4651)\n",
      "[Epoch: 14880] train loss: 1.3873, train acc: 0.4242, val loss: 1.3765, val acc: 0.4145  (best train acc: 0.4483, best val acc: 0.4651)\n",
      "[Epoch: 14900] train loss: 1.3879, train acc: 0.4245, val loss: 1.3764, val acc: 0.4155  (best train acc: 0.4483, best val acc: 0.4651)\n",
      "[Epoch: 14920] train loss: 1.3893, train acc: 0.4230, val loss: 1.3763, val acc: 0.4142  (best train acc: 0.4483, best val acc: 0.4651)\n",
      "[Epoch: 14940] train loss: 1.3888, train acc: 0.4247, val loss: 1.3761, val acc: 0.4148  (best train acc: 0.4483, best val acc: 0.4651)\n",
      "[Epoch: 14960] train loss: 1.3878, train acc: 0.4240, val loss: 1.3761, val acc: 0.4142  (best train acc: 0.4483, best val acc: 0.4651)\n",
      "[Epoch: 14980] train loss: 1.3929, train acc: 0.4188, val loss: 1.3760, val acc: 0.4138  (best train acc: 0.4483, best val acc: 0.4651)\n",
      "[Epoch: 15000] train loss: 1.3908, train acc: 0.4242, val loss: 1.3759, val acc: 0.4142  (best train acc: 0.4483, best val acc: 0.4651)\n",
      "[Epoch: 15020] train loss: 1.3912, train acc: 0.4211, val loss: 1.3758, val acc: 0.4155  (best train acc: 0.4483, best val acc: 0.4651)\n",
      "[Epoch: 15040] train loss: 1.3861, train acc: 0.4212, val loss: 1.3757, val acc: 0.4145  (best train acc: 0.4483, best val acc: 0.4651)\n",
      "[Epoch: 15060] train loss: 1.3873, train acc: 0.4289, val loss: 1.3756, val acc: 0.4162  (best train acc: 0.4483, best val acc: 0.4651)\n",
      "[Epoch: 15080] train loss: 1.3874, train acc: 0.4226, val loss: 1.3755, val acc: 0.4135  (best train acc: 0.4483, best val acc: 0.4651)\n",
      "[Epoch: 15100] train loss: 1.3863, train acc: 0.4203, val loss: 1.3755, val acc: 0.4142  (best train acc: 0.4483, best val acc: 0.4651)\n",
      "[Epoch: 15120] train loss: 1.3893, train acc: 0.4253, val loss: 1.3754, val acc: 0.4148  (best train acc: 0.4483, best val acc: 0.4651)\n",
      "[Epoch: 15140] train loss: 1.3854, train acc: 0.4231, val loss: 1.3752, val acc: 0.4159  (best train acc: 0.4483, best val acc: 0.4651)\n",
      "[Epoch: 15160] train loss: 1.3870, train acc: 0.4242, val loss: 1.3751, val acc: 0.4159  (best train acc: 0.4483, best val acc: 0.4651)\n",
      "[Epoch: 15180] train loss: 1.3839, train acc: 0.4229, val loss: 1.3748, val acc: 0.4175  (best train acc: 0.4483, best val acc: 0.4651)\n",
      "[Epoch: 15200] train loss: 1.3858, train acc: 0.4223, val loss: 1.3748, val acc: 0.4162  (best train acc: 0.4483, best val acc: 0.4651)\n",
      "[Epoch: 15220] train loss: 1.3854, train acc: 0.4200, val loss: 1.3749, val acc: 0.4196  (best train acc: 0.4483, best val acc: 0.4651)\n",
      "[Epoch: 15240] train loss: 1.3864, train acc: 0.4237, val loss: 1.3746, val acc: 0.4165  (best train acc: 0.4483, best val acc: 0.4651)\n",
      "[Epoch: 15260] train loss: 1.3791, train acc: 0.4269, val loss: 1.3748, val acc: 0.4125  (best train acc: 0.4483, best val acc: 0.4651)\n",
      "[Epoch: 15280] train loss: 1.3833, train acc: 0.4268, val loss: 1.3744, val acc: 0.4155  (best train acc: 0.4483, best val acc: 0.4651)\n",
      "[Epoch: 15300] train loss: 1.3855, train acc: 0.4237, val loss: 1.3744, val acc: 0.4172  (best train acc: 0.4483, best val acc: 0.4651)\n",
      "[Epoch: 15320] train loss: 1.3827, train acc: 0.4266, val loss: 1.3743, val acc: 0.4148  (best train acc: 0.4483, best val acc: 0.4651)\n",
      "[Epoch: 15340] train loss: 1.3825, train acc: 0.4245, val loss: 1.3742, val acc: 0.4175  (best train acc: 0.4483, best val acc: 0.4651)\n",
      "[Epoch: 15360] train loss: 1.3815, train acc: 0.4266, val loss: 1.3743, val acc: 0.4162  (best train acc: 0.4483, best val acc: 0.4651)\n",
      "[Epoch: 15380] train loss: 1.3858, train acc: 0.4257, val loss: 1.3741, val acc: 0.4165  (best train acc: 0.4483, best val acc: 0.4651)\n",
      "[Epoch: 15400] train loss: 1.3848, train acc: 0.4234, val loss: 1.3736, val acc: 0.4155  (best train acc: 0.4483, best val acc: 0.4651)\n",
      "[Epoch: 15420] train loss: 1.3832, train acc: 0.4182, val loss: 1.3734, val acc: 0.4162  (best train acc: 0.4483, best val acc: 0.4651)\n",
      "[Epoch: 15440] train loss: 1.3780, train acc: 0.4232, val loss: 1.3733, val acc: 0.4148  (best train acc: 0.4483, best val acc: 0.4651)\n",
      "[Epoch: 15460] train loss: 1.3824, train acc: 0.4276, val loss: 1.3729, val acc: 0.4165  (best train acc: 0.4483, best val acc: 0.4651)\n",
      "[Epoch: 15480] train loss: 1.3825, train acc: 0.4246, val loss: 1.3726, val acc: 0.4142  (best train acc: 0.4483, best val acc: 0.4651)\n",
      "[Epoch: 15500] train loss: 1.3815, train acc: 0.4318, val loss: 1.3723, val acc: 0.4152  (best train acc: 0.4483, best val acc: 0.4651)\n",
      "[Epoch: 15520] train loss: 1.3796, train acc: 0.4227, val loss: 1.3722, val acc: 0.4145  (best train acc: 0.4483, best val acc: 0.4651)\n",
      "[Epoch: 15540] train loss: 1.3807, train acc: 0.4263, val loss: 1.3716, val acc: 0.4145  (best train acc: 0.4483, best val acc: 0.4651)\n",
      "[Epoch: 15560] train loss: 1.3780, train acc: 0.4299, val loss: 1.3686, val acc: 0.4253  (best train acc: 0.4483, best val acc: 0.4651)\n",
      "[Epoch: 15580] train loss: 1.3731, train acc: 0.4347, val loss: 1.3664, val acc: 0.4256  (best train acc: 0.4483, best val acc: 0.4651)\n",
      "[Epoch: 15600] train loss: 1.3753, train acc: 0.4331, val loss: 1.3654, val acc: 0.4317  (best train acc: 0.4483, best val acc: 0.4651)\n",
      "[Epoch: 15620] train loss: 1.3748, train acc: 0.4336, val loss: 1.3650, val acc: 0.4331  (best train acc: 0.4483, best val acc: 0.4651)\n",
      "[Epoch: 15640] train loss: 1.3747, train acc: 0.4377, val loss: 1.3646, val acc: 0.4250  (best train acc: 0.4483, best val acc: 0.4651)\n",
      "[Epoch: 15660] train loss: 1.3752, train acc: 0.4320, val loss: 1.3639, val acc: 0.4347  (best train acc: 0.4483, best val acc: 0.4651)\n",
      "[Epoch: 15680] train loss: 1.3743, train acc: 0.4323, val loss: 1.3635, val acc: 0.4358  (best train acc: 0.4483, best val acc: 0.4651)\n",
      "[Epoch: 15700] train loss: 1.3736, train acc: 0.4335, val loss: 1.3631, val acc: 0.4354  (best train acc: 0.4483, best val acc: 0.4651)\n",
      "[Epoch: 15720] train loss: 1.3675, train acc: 0.4380, val loss: 1.3624, val acc: 0.4368  (best train acc: 0.4483, best val acc: 0.4651)\n",
      "[Epoch: 15740] train loss: 1.3721, train acc: 0.4331, val loss: 1.3622, val acc: 0.4337  (best train acc: 0.4483, best val acc: 0.4651)\n",
      "[Epoch: 15760] train loss: 1.3729, train acc: 0.4371, val loss: 1.3615, val acc: 0.4307  (best train acc: 0.4483, best val acc: 0.4651)\n",
      "[Epoch: 15780] train loss: 1.3730, train acc: 0.4388, val loss: 1.3609, val acc: 0.4317  (best train acc: 0.4483, best val acc: 0.4651)\n",
      "[Epoch: 15800] train loss: 1.3700, train acc: 0.4385, val loss: 1.3613, val acc: 0.4246  (best train acc: 0.4483, best val acc: 0.4651)\n",
      "[Epoch: 15820] train loss: 1.3677, train acc: 0.4403, val loss: 1.3594, val acc: 0.4307  (best train acc: 0.4483, best val acc: 0.4651)\n",
      "[Epoch: 15840] train loss: 1.3667, train acc: 0.4404, val loss: 1.3583, val acc: 0.4364  (best train acc: 0.4483, best val acc: 0.4651)\n",
      "[Epoch: 15860] train loss: 1.3716, train acc: 0.4309, val loss: 1.3577, val acc: 0.4452  (best train acc: 0.4483, best val acc: 0.4651)\n",
      "[Epoch: 15880] train loss: 1.3641, train acc: 0.4418, val loss: 1.3564, val acc: 0.4368  (best train acc: 0.4483, best val acc: 0.4651)\n",
      "[Epoch: 15900] train loss: 1.3671, train acc: 0.4345, val loss: 1.3559, val acc: 0.4469  (best train acc: 0.4483, best val acc: 0.4651)\n",
      "[Epoch: 15920] train loss: 1.3662, train acc: 0.4359, val loss: 1.3543, val acc: 0.4388  (best train acc: 0.4483, best val acc: 0.4651)\n",
      "[Epoch: 15940] train loss: 1.3616, train acc: 0.4406, val loss: 1.3533, val acc: 0.4337  (best train acc: 0.4483, best val acc: 0.4651)\n",
      "[Epoch: 15960] train loss: 1.3685, train acc: 0.4394, val loss: 1.3523, val acc: 0.4354  (best train acc: 0.4483, best val acc: 0.4651)\n",
      "[Epoch: 15980] train loss: 1.3675, train acc: 0.4376, val loss: 1.3530, val acc: 0.4266  (best train acc: 0.4483, best val acc: 0.4651)\n",
      "[Epoch: 16000] train loss: 1.3676, train acc: 0.4324, val loss: 1.3509, val acc: 0.4469  (best train acc: 0.4483, best val acc: 0.4651)\n",
      "[Epoch: 16020] train loss: 1.3609, train acc: 0.4380, val loss: 1.3498, val acc: 0.4344  (best train acc: 0.4483, best val acc: 0.4651)\n",
      "[Epoch: 16040] train loss: 1.3679, train acc: 0.4376, val loss: 1.3484, val acc: 0.4472  (best train acc: 0.4483, best val acc: 0.4651)\n",
      "[Epoch: 16060] train loss: 1.3577, train acc: 0.4410, val loss: 1.3470, val acc: 0.4374  (best train acc: 0.4483, best val acc: 0.4651)\n",
      "[Epoch: 16080] train loss: 1.3612, train acc: 0.4408, val loss: 1.3487, val acc: 0.4277  (best train acc: 0.4483, best val acc: 0.4651)\n",
      "[Epoch: 16100] train loss: 1.3608, train acc: 0.4427, val loss: 1.3464, val acc: 0.4516  (best train acc: 0.4483, best val acc: 0.4651)\n",
      "[Epoch: 16120] train loss: 1.3564, train acc: 0.4432, val loss: 1.3465, val acc: 0.4293  (best train acc: 0.4483, best val acc: 0.4651)\n",
      "[Epoch: 16140] train loss: 1.3595, train acc: 0.4333, val loss: 1.3440, val acc: 0.4449  (best train acc: 0.4483, best val acc: 0.4651)\n",
      "[Epoch: 16160] train loss: 1.3598, train acc: 0.4429, val loss: 1.3431, val acc: 0.4418  (best train acc: 0.4483, best val acc: 0.4651)\n",
      "[Epoch: 16180] train loss: 1.3568, train acc: 0.4411, val loss: 1.3429, val acc: 0.4492  (best train acc: 0.4483, best val acc: 0.4651)\n",
      "[Epoch: 16200] train loss: 1.3583, train acc: 0.4370, val loss: 1.3418, val acc: 0.4445  (best train acc: 0.4483, best val acc: 0.4651)\n",
      "[Epoch: 16220] train loss: 1.3542, train acc: 0.4456, val loss: 1.3419, val acc: 0.4341  (best train acc: 0.4483, best val acc: 0.4651)\n",
      "[Epoch: 16240] train loss: 1.3565, train acc: 0.4330, val loss: 1.3447, val acc: 0.4492  (best train acc: 0.4483, best val acc: 0.4651)\n",
      "[Epoch: 16260] train loss: 1.3506, train acc: 0.4453, val loss: 1.3393, val acc: 0.4509  (best train acc: 0.4483, best val acc: 0.4651)\n",
      "[Epoch: 16280] train loss: 1.3480, train acc: 0.4453, val loss: 1.3373, val acc: 0.4422  (best train acc: 0.4492, best val acc: 0.4651)\n",
      "[Epoch: 16300] train loss: 1.3485, train acc: 0.4481, val loss: 1.3406, val acc: 0.4503  (best train acc: 0.4500, best val acc: 0.4651)\n",
      "[Epoch: 16320] train loss: 1.3462, train acc: 0.4443, val loss: 1.3346, val acc: 0.4476  (best train acc: 0.4500, best val acc: 0.4651)\n",
      "[Epoch: 16340] train loss: 1.3416, train acc: 0.4479, val loss: 1.3334, val acc: 0.4401  (best train acc: 0.4500, best val acc: 0.4651)\n",
      "[Epoch: 16360] train loss: 1.3420, train acc: 0.4488, val loss: 1.3305, val acc: 0.4503  (best train acc: 0.4500, best val acc: 0.4651)\n",
      "[Epoch: 16380] train loss: 1.3401, train acc: 0.4476, val loss: 1.3303, val acc: 0.4388  (best train acc: 0.4502, best val acc: 0.4651)\n",
      "[Epoch: 16400] train loss: 1.3398, train acc: 0.4504, val loss: 1.3272, val acc: 0.4418  (best train acc: 0.4511, best val acc: 0.4651)\n",
      "[Epoch: 16420] train loss: 1.3357, train acc: 0.4477, val loss: 1.3255, val acc: 0.4513  (best train acc: 0.4524, best val acc: 0.4651)\n",
      "[Epoch: 16440] train loss: 1.3380, train acc: 0.4469, val loss: 1.3292, val acc: 0.4388  (best train acc: 0.4524, best val acc: 0.4651)\n",
      "[Epoch: 16460] train loss: 1.3333, train acc: 0.4505, val loss: 1.3212, val acc: 0.4476  (best train acc: 0.4543, best val acc: 0.4651)\n",
      "[Epoch: 16480] train loss: 1.3334, train acc: 0.4538, val loss: 1.3213, val acc: 0.4398  (best train acc: 0.4543, best val acc: 0.4651)\n",
      "[Epoch: 16500] train loss: 1.3312, train acc: 0.4533, val loss: 1.3177, val acc: 0.4418  (best train acc: 0.4558, best val acc: 0.4651)\n",
      "[Epoch: 16520] train loss: 1.3276, train acc: 0.4532, val loss: 1.3158, val acc: 0.4459  (best train acc: 0.4558, best val acc: 0.4651)\n",
      "[Epoch: 16540] train loss: 1.3343, train acc: 0.4486, val loss: 1.3156, val acc: 0.4418  (best train acc: 0.4558, best val acc: 0.4651)\n",
      "[Epoch: 16560] train loss: 1.3266, train acc: 0.4524, val loss: 1.3154, val acc: 0.4408  (best train acc: 0.4558, best val acc: 0.4651)\n",
      "[Epoch: 16580] train loss: 1.3267, train acc: 0.4534, val loss: 1.3142, val acc: 0.4408  (best train acc: 0.4558, best val acc: 0.4651)\n",
      "[Epoch: 16600] train loss: 1.3254, train acc: 0.4461, val loss: 1.3121, val acc: 0.4519  (best train acc: 0.4558, best val acc: 0.4651)\n",
      "[Epoch: 16620] train loss: 1.3204, train acc: 0.4505, val loss: 1.3103, val acc: 0.4374  (best train acc: 0.4558, best val acc: 0.4651)\n",
      "[Epoch: 16640] train loss: 1.3200, train acc: 0.4557, val loss: 1.3070, val acc: 0.4422  (best train acc: 0.4558, best val acc: 0.4651)\n",
      "[Epoch: 16660] train loss: 1.3192, train acc: 0.4492, val loss: 1.3034, val acc: 0.4378  (best train acc: 0.4558, best val acc: 0.4651)\n",
      "[Epoch: 16680] train loss: 1.3098, train acc: 0.4540, val loss: 1.3002, val acc: 0.4415  (best train acc: 0.4565, best val acc: 0.4651)\n",
      "[Epoch: 16700] train loss: 1.3130, train acc: 0.4490, val loss: 1.3048, val acc: 0.4567  (best train acc: 0.4568, best val acc: 0.4651)\n",
      "[Epoch: 16720] train loss: 1.3106, train acc: 0.4554, val loss: 1.2959, val acc: 0.4469  (best train acc: 0.4568, best val acc: 0.4651)\n",
      "[Epoch: 16740] train loss: 1.3055, train acc: 0.4566, val loss: 1.2937, val acc: 0.4418  (best train acc: 0.4575, best val acc: 0.4651)\n",
      "[Epoch: 16760] train loss: 1.3064, train acc: 0.4500, val loss: 1.2946, val acc: 0.4577  (best train acc: 0.4575, best val acc: 0.4651)\n",
      "[Epoch: 16780] train loss: 1.3028, train acc: 0.4529, val loss: 1.2925, val acc: 0.4428  (best train acc: 0.4575, best val acc: 0.4651)\n",
      "[Epoch: 16800] train loss: 1.3040, train acc: 0.4532, val loss: 1.2928, val acc: 0.4445  (best train acc: 0.4575, best val acc: 0.4651)\n",
      "[Epoch: 16820] train loss: 1.3051, train acc: 0.4536, val loss: 1.2924, val acc: 0.4449  (best train acc: 0.4575, best val acc: 0.4651)\n",
      "[Epoch: 16840] train loss: 1.3021, train acc: 0.4554, val loss: 1.2907, val acc: 0.4610  (best train acc: 0.4575, best val acc: 0.4651)\n",
      "[Epoch: 16860] train loss: 1.3018, train acc: 0.4550, val loss: 1.2960, val acc: 0.4418  (best train acc: 0.4575, best val acc: 0.4651)\n",
      "[Epoch: 16880] train loss: 1.3027, train acc: 0.4499, val loss: 1.2885, val acc: 0.4590  (best train acc: 0.4579, best val acc: 0.4651)\n",
      "[Epoch: 16900] train loss: 1.2977, train acc: 0.4576, val loss: 1.2869, val acc: 0.4455  (best train acc: 0.4587, best val acc: 0.4651)\n",
      "[Epoch: 16920] train loss: 1.2955, train acc: 0.4592, val loss: 1.2861, val acc: 0.4486  (best train acc: 0.4599, best val acc: 0.4651)\n",
      "[Epoch: 16940] train loss: 1.2972, train acc: 0.4584, val loss: 1.2853, val acc: 0.4452  (best train acc: 0.4599, best val acc: 0.4651)\n",
      "[Epoch: 16960] train loss: 1.2951, train acc: 0.4571, val loss: 1.2846, val acc: 0.4479  (best train acc: 0.4631, best val acc: 0.4651)\n",
      "[Epoch: 16980] train loss: 1.2965, train acc: 0.4571, val loss: 1.2849, val acc: 0.4597  (best train acc: 0.4631, best val acc: 0.4651)\n",
      "[Epoch: 17000] train loss: 1.2966, train acc: 0.4544, val loss: 1.2831, val acc: 0.4573  (best train acc: 0.4631, best val acc: 0.4651)\n",
      "[Epoch: 17020] train loss: 1.3094, train acc: 0.4422, val loss: 1.2834, val acc: 0.4597  (best train acc: 0.4631, best val acc: 0.4651)\n",
      "[Epoch: 17040] train loss: 1.2960, train acc: 0.4534, val loss: 1.2826, val acc: 0.4526  (best train acc: 0.4631, best val acc: 0.4651)\n",
      "[Epoch: 17060] train loss: 1.2958, train acc: 0.4581, val loss: 1.2818, val acc: 0.4536  (best train acc: 0.4631, best val acc: 0.4651)\n",
      "[Epoch: 17080] train loss: 1.2962, train acc: 0.4568, val loss: 1.2874, val acc: 0.4432  (best train acc: 0.4631, best val acc: 0.4651)\n",
      "[Epoch: 17100] train loss: 1.2910, train acc: 0.4628, val loss: 1.2823, val acc: 0.4617  (best train acc: 0.4631, best val acc: 0.4651)\n",
      "[Epoch: 17120] train loss: 1.2941, train acc: 0.4544, val loss: 1.2806, val acc: 0.4550  (best train acc: 0.4631, best val acc: 0.4651)\n",
      "[Epoch: 17140] train loss: 1.2928, train acc: 0.4574, val loss: 1.2802, val acc: 0.4543  (best train acc: 0.4631, best val acc: 0.4651)\n",
      "[Epoch: 17160] train loss: 1.2946, train acc: 0.4563, val loss: 1.2807, val acc: 0.4637  (best train acc: 0.4642, best val acc: 0.4651)\n",
      "[Epoch: 17180] train loss: 1.2978, train acc: 0.4547, val loss: 1.2866, val acc: 0.4445  (best train acc: 0.4642, best val acc: 0.4651)\n",
      "[Epoch: 17200] train loss: 1.2950, train acc: 0.4546, val loss: 1.2790, val acc: 0.4560  (best train acc: 0.4670, best val acc: 0.4651)\n",
      "[Epoch: 17220] train loss: 1.2914, train acc: 0.4608, val loss: 1.2856, val acc: 0.4459  (best train acc: 0.4670, best val acc: 0.4651)\n",
      "[Epoch: 17240] train loss: 1.2953, train acc: 0.4557, val loss: 1.2815, val acc: 0.4499  (best train acc: 0.4670, best val acc: 0.4651)\n",
      "[Epoch: 17260] train loss: 1.2888, train acc: 0.4642, val loss: 1.2789, val acc: 0.4637  (best train acc: 0.4670, best val acc: 0.4651)\n",
      "[Epoch: 17280] train loss: 1.2892, train acc: 0.4615, val loss: 1.2789, val acc: 0.4523  (best train acc: 0.4670, best val acc: 0.4651)\n",
      "[Epoch: 17300] train loss: 1.2863, train acc: 0.4589, val loss: 1.2784, val acc: 0.4627  (best train acc: 0.4670, best val acc: 0.4651)\n",
      "[Epoch: 17320] train loss: 1.2901, train acc: 0.4537, val loss: 1.2782, val acc: 0.4648  (best train acc: 0.4670, best val acc: 0.4651)\n",
      "[Epoch: 17340] train loss: 1.2897, train acc: 0.4602, val loss: 1.2788, val acc: 0.4648  (best train acc: 0.4670, best val acc: 0.4651)\n",
      "[Epoch: 17360] train loss: 1.2856, train acc: 0.4607, val loss: 1.2779, val acc: 0.4519  (best train acc: 0.4670, best val acc: 0.4651)\n",
      "[Epoch: 17380] train loss: 1.2912, train acc: 0.4623, val loss: 1.2779, val acc: 0.4550  (best train acc: 0.4670, best val acc: 0.4664)\n",
      "[Epoch: 17400] train loss: 1.2901, train acc: 0.4628, val loss: 1.2778, val acc: 0.4567  (best train acc: 0.4670, best val acc: 0.4664)\n",
      "[Epoch: 17420] train loss: 1.2909, train acc: 0.4635, val loss: 1.2778, val acc: 0.4513  (best train acc: 0.4670, best val acc: 0.4664)\n",
      "[Epoch: 17440] train loss: 1.2909, train acc: 0.4576, val loss: 1.2841, val acc: 0.4459  (best train acc: 0.4670, best val acc: 0.4664)\n",
      "[Epoch: 17460] train loss: 1.2881, train acc: 0.4631, val loss: 1.2782, val acc: 0.4523  (best train acc: 0.4670, best val acc: 0.4671)\n",
      "[Epoch: 17480] train loss: 1.2890, train acc: 0.4595, val loss: 1.2764, val acc: 0.4486  (best train acc: 0.4677, best val acc: 0.4671)\n",
      "[Epoch: 17500] train loss: 1.2899, train acc: 0.4620, val loss: 1.2754, val acc: 0.4526  (best train acc: 0.4677, best val acc: 0.4671)\n",
      "[Epoch: 17520] train loss: 1.2826, train acc: 0.4633, val loss: 1.2754, val acc: 0.4516  (best train acc: 0.4677, best val acc: 0.4691)\n",
      "[Epoch: 17540] train loss: 1.2845, train acc: 0.4628, val loss: 1.2755, val acc: 0.4681  (best train acc: 0.4677, best val acc: 0.4691)\n",
      "[Epoch: 17560] train loss: 1.2869, train acc: 0.4622, val loss: 1.2748, val acc: 0.4664  (best train acc: 0.4677, best val acc: 0.4702)\n",
      "[Epoch: 17580] train loss: 1.2848, train acc: 0.4628, val loss: 1.2735, val acc: 0.4651  (best train acc: 0.4677, best val acc: 0.4702)\n",
      "[Epoch: 17600] train loss: 1.2811, train acc: 0.4610, val loss: 1.2736, val acc: 0.4654  (best train acc: 0.4677, best val acc: 0.4702)\n",
      "[Epoch: 17620] train loss: 1.2835, train acc: 0.4625, val loss: 1.2779, val acc: 0.4496  (best train acc: 0.4677, best val acc: 0.4702)\n",
      "[Epoch: 17640] train loss: 1.2801, train acc: 0.4572, val loss: 1.2730, val acc: 0.4648  (best train acc: 0.4677, best val acc: 0.4712)\n",
      "[Epoch: 17660] train loss: 1.2847, train acc: 0.4601, val loss: 1.2728, val acc: 0.4627  (best train acc: 0.4677, best val acc: 0.4712)\n",
      "[Epoch: 17680] train loss: 1.2837, train acc: 0.4634, val loss: 1.2734, val acc: 0.4577  (best train acc: 0.4677, best val acc: 0.4712)\n",
      "[Epoch: 17700] train loss: 1.2933, train acc: 0.4547, val loss: 1.2834, val acc: 0.4519  (best train acc: 0.4677, best val acc: 0.4712)\n",
      "[Epoch: 17720] train loss: 1.2830, train acc: 0.4605, val loss: 1.2720, val acc: 0.4668  (best train acc: 0.4685, best val acc: 0.4712)\n",
      "[Epoch: 17740] train loss: 1.2792, train acc: 0.4666, val loss: 1.2719, val acc: 0.4570  (best train acc: 0.4685, best val acc: 0.4712)\n",
      "[Epoch: 17760] train loss: 1.2850, train acc: 0.4492, val loss: 1.2756, val acc: 0.4664  (best train acc: 0.4685, best val acc: 0.4712)\n",
      "[Epoch: 17780] train loss: 1.2815, train acc: 0.4665, val loss: 1.2725, val acc: 0.4540  (best train acc: 0.4685, best val acc: 0.4718)\n",
      "[Epoch: 17800] train loss: 1.2837, train acc: 0.4639, val loss: 1.2764, val acc: 0.4546  (best train acc: 0.4685, best val acc: 0.4718)\n",
      "[Epoch: 17820] train loss: 1.2885, train acc: 0.4571, val loss: 1.2750, val acc: 0.4702  (best train acc: 0.4689, best val acc: 0.4718)\n",
      "[Epoch: 17840] train loss: 1.2811, train acc: 0.4654, val loss: 1.2731, val acc: 0.4550  (best train acc: 0.4689, best val acc: 0.4718)\n",
      "[Epoch: 17860] train loss: 1.2793, train acc: 0.4654, val loss: 1.2749, val acc: 0.4540  (best train acc: 0.4689, best val acc: 0.4718)\n",
      "[Epoch: 17880] train loss: 1.2829, train acc: 0.4629, val loss: 1.2707, val acc: 0.4654  (best train acc: 0.4689, best val acc: 0.4728)\n",
      "[Epoch: 17900] train loss: 1.2799, train acc: 0.4672, val loss: 1.2707, val acc: 0.4634  (best train acc: 0.4689, best val acc: 0.4728)\n",
      "[Epoch: 17920] train loss: 1.2845, train acc: 0.4537, val loss: 1.2713, val acc: 0.4695  (best train acc: 0.4689, best val acc: 0.4728)\n",
      "[Epoch: 17940] train loss: 1.2834, train acc: 0.4675, val loss: 1.2769, val acc: 0.4553  (best train acc: 0.4689, best val acc: 0.4728)\n",
      "[Epoch: 17960] train loss: 1.2777, train acc: 0.4677, val loss: 1.2721, val acc: 0.4546  (best train acc: 0.4689, best val acc: 0.4728)\n",
      "[Epoch: 17980] train loss: 1.2802, train acc: 0.4650, val loss: 1.2693, val acc: 0.4671  (best train acc: 0.4704, best val acc: 0.4739)\n",
      "[Epoch: 18000] train loss: 1.2802, train acc: 0.4630, val loss: 1.2697, val acc: 0.4607  (best train acc: 0.4704, best val acc: 0.4739)\n",
      "[Epoch: 18020] train loss: 1.2862, train acc: 0.4545, val loss: 1.2731, val acc: 0.4722  (best train acc: 0.4711, best val acc: 0.4739)\n",
      "[Epoch: 18040] train loss: 1.2911, train acc: 0.4507, val loss: 1.2736, val acc: 0.4698  (best train acc: 0.4711, best val acc: 0.4739)\n",
      "[Epoch: 18060] train loss: 1.2805, train acc: 0.4645, val loss: 1.2740, val acc: 0.4546  (best train acc: 0.4718, best val acc: 0.4739)\n",
      "[Epoch: 18080] train loss: 1.2748, train acc: 0.4665, val loss: 1.2726, val acc: 0.4550  (best train acc: 0.4718, best val acc: 0.4739)\n",
      "[Epoch: 18100] train loss: 1.2755, train acc: 0.4694, val loss: 1.2720, val acc: 0.4577  (best train acc: 0.4718, best val acc: 0.4739)\n",
      "[Epoch: 18120] train loss: 1.2809, train acc: 0.4633, val loss: 1.2737, val acc: 0.4573  (best train acc: 0.4718, best val acc: 0.4739)\n",
      "[Epoch: 18140] train loss: 1.2771, train acc: 0.4683, val loss: 1.2685, val acc: 0.4610  (best train acc: 0.4725, best val acc: 0.4739)\n",
      "[Epoch: 18160] train loss: 1.2802, train acc: 0.4646, val loss: 1.2702, val acc: 0.4563  (best train acc: 0.4725, best val acc: 0.4739)\n",
      "[Epoch: 18180] train loss: 1.2818, train acc: 0.4703, val loss: 1.2714, val acc: 0.4567  (best train acc: 0.4725, best val acc: 0.4739)\n",
      "[Epoch: 18200] train loss: 1.2816, train acc: 0.4655, val loss: 1.2704, val acc: 0.4577  (best train acc: 0.4725, best val acc: 0.4739)\n",
      "[Epoch: 18220] train loss: 1.2834, train acc: 0.4601, val loss: 1.2785, val acc: 0.4590  (best train acc: 0.4725, best val acc: 0.4739)\n",
      "[Epoch: 18240] train loss: 1.2777, train acc: 0.4616, val loss: 1.2669, val acc: 0.4678  (best train acc: 0.4725, best val acc: 0.4739)\n",
      "[Epoch: 18260] train loss: 1.2790, train acc: 0.4623, val loss: 1.2732, val acc: 0.4587  (best train acc: 0.4725, best val acc: 0.4752)\n",
      "[Epoch: 18280] train loss: 1.2783, train acc: 0.4560, val loss: 1.2695, val acc: 0.4688  (best train acc: 0.4725, best val acc: 0.4752)\n",
      "[Epoch: 18300] train loss: 1.2780, train acc: 0.4702, val loss: 1.2730, val acc: 0.4597  (best train acc: 0.4725, best val acc: 0.4752)\n",
      "[Epoch: 18320] train loss: 1.2791, train acc: 0.4604, val loss: 1.2666, val acc: 0.4725  (best train acc: 0.4729, best val acc: 0.4759)\n",
      "[Epoch: 18340] train loss: 1.2768, train acc: 0.4652, val loss: 1.2657, val acc: 0.4675  (best train acc: 0.4729, best val acc: 0.4759)\n",
      "[Epoch: 18360] train loss: 1.2802, train acc: 0.4651, val loss: 1.2702, val acc: 0.4610  (best train acc: 0.4729, best val acc: 0.4759)\n",
      "[Epoch: 18380] train loss: 1.2783, train acc: 0.4686, val loss: 1.2653, val acc: 0.4654  (best train acc: 0.4729, best val acc: 0.4759)\n",
      "[Epoch: 18400] train loss: 1.2757, train acc: 0.4667, val loss: 1.2656, val acc: 0.4654  (best train acc: 0.4729, best val acc: 0.4759)\n",
      "[Epoch: 18420] train loss: 1.2785, train acc: 0.4618, val loss: 1.2651, val acc: 0.4658  (best train acc: 0.4729, best val acc: 0.4759)\n",
      "[Epoch: 18440] train loss: 1.2737, train acc: 0.4673, val loss: 1.2675, val acc: 0.4597  (best train acc: 0.4729, best val acc: 0.4759)\n",
      "[Epoch: 18460] train loss: 1.2765, train acc: 0.4628, val loss: 1.2653, val acc: 0.4718  (best train acc: 0.4729, best val acc: 0.4759)\n",
      "[Epoch: 18480] train loss: 1.2767, train acc: 0.4658, val loss: 1.2711, val acc: 0.4594  (best train acc: 0.4729, best val acc: 0.4759)\n",
      "[Epoch: 18500] train loss: 1.2778, train acc: 0.4674, val loss: 1.2672, val acc: 0.4621  (best train acc: 0.4740, best val acc: 0.4759)\n",
      "[Epoch: 18520] train loss: 1.2774, train acc: 0.4667, val loss: 1.2640, val acc: 0.4735  (best train acc: 0.4740, best val acc: 0.4759)\n",
      "[Epoch: 18540] train loss: 1.2782, train acc: 0.4665, val loss: 1.2643, val acc: 0.4671  (best train acc: 0.4740, best val acc: 0.4759)\n",
      "[Epoch: 18560] train loss: 1.2741, train acc: 0.4705, val loss: 1.2635, val acc: 0.4691  (best train acc: 0.4740, best val acc: 0.4759)\n",
      "[Epoch: 18580] train loss: 1.2728, train acc: 0.4670, val loss: 1.2650, val acc: 0.4631  (best train acc: 0.4740, best val acc: 0.4759)\n",
      "[Epoch: 18600] train loss: 1.2805, train acc: 0.4624, val loss: 1.2689, val acc: 0.4631  (best train acc: 0.4740, best val acc: 0.4759)\n",
      "[Epoch: 18620] train loss: 1.2750, train acc: 0.4698, val loss: 1.2643, val acc: 0.4658  (best train acc: 0.4740, best val acc: 0.4759)\n",
      "[Epoch: 18640] train loss: 1.2723, train acc: 0.4681, val loss: 1.2644, val acc: 0.4624  (best train acc: 0.4740, best val acc: 0.4759)\n",
      "[Epoch: 18660] train loss: 1.2838, train acc: 0.4597, val loss: 1.2673, val acc: 0.4607  (best train acc: 0.4740, best val acc: 0.4759)\n",
      "[Epoch: 18680] train loss: 1.2743, train acc: 0.4670, val loss: 1.2635, val acc: 0.4627  (best train acc: 0.4740, best val acc: 0.4766)\n",
      "[Epoch: 18700] train loss: 1.2813, train acc: 0.4639, val loss: 1.2751, val acc: 0.4533  (best train acc: 0.4740, best val acc: 0.4766)\n",
      "[Epoch: 18720] train loss: 1.2725, train acc: 0.4639, val loss: 1.2624, val acc: 0.4651  (best train acc: 0.4740, best val acc: 0.4766)\n",
      "[Epoch: 18740] train loss: 1.2715, train acc: 0.4692, val loss: 1.2631, val acc: 0.4728  (best train acc: 0.4740, best val acc: 0.4766)\n",
      "[Epoch: 18760] train loss: 1.2708, train acc: 0.4710, val loss: 1.2633, val acc: 0.4631  (best train acc: 0.4740, best val acc: 0.4766)\n",
      "[Epoch: 18780] train loss: 1.2725, train acc: 0.4680, val loss: 1.2642, val acc: 0.4742  (best train acc: 0.4740, best val acc: 0.4782)\n",
      "[Epoch: 18800] train loss: 1.2749, train acc: 0.4663, val loss: 1.2653, val acc: 0.4614  (best train acc: 0.4740, best val acc: 0.4782)\n",
      "[Epoch: 18820] train loss: 1.2696, train acc: 0.4724, val loss: 1.2621, val acc: 0.4668  (best train acc: 0.4740, best val acc: 0.4782)\n",
      "[Epoch: 18840] train loss: 1.2742, train acc: 0.4680, val loss: 1.2622, val acc: 0.4668  (best train acc: 0.4740, best val acc: 0.4803)\n",
      "[Epoch: 18860] train loss: 1.2793, train acc: 0.4644, val loss: 1.2651, val acc: 0.4637  (best train acc: 0.4740, best val acc: 0.4803)\n",
      "[Epoch: 18880] train loss: 1.2746, train acc: 0.4665, val loss: 1.2614, val acc: 0.4654  (best train acc: 0.4740, best val acc: 0.4803)\n",
      "[Epoch: 18900] train loss: 1.2757, train acc: 0.4637, val loss: 1.2668, val acc: 0.4587  (best train acc: 0.4740, best val acc: 0.4803)\n",
      "[Epoch: 18920] train loss: 1.2707, train acc: 0.4701, val loss: 1.2623, val acc: 0.4664  (best train acc: 0.4740, best val acc: 0.4803)\n",
      "[Epoch: 18940] train loss: 1.2730, train acc: 0.4646, val loss: 1.2616, val acc: 0.4752  (best train acc: 0.4740, best val acc: 0.4803)\n",
      "[Epoch: 18960] train loss: 1.2762, train acc: 0.4614, val loss: 1.2627, val acc: 0.4675  (best train acc: 0.4740, best val acc: 0.4813)\n",
      "[Epoch: 18980] train loss: 1.2717, train acc: 0.4696, val loss: 1.2611, val acc: 0.4648  (best train acc: 0.4740, best val acc: 0.4813)\n",
      "[Epoch: 19000] train loss: 1.2725, train acc: 0.4656, val loss: 1.2623, val acc: 0.4762  (best train acc: 0.4740, best val acc: 0.4813)\n",
      "[Epoch: 19020] train loss: 1.2736, train acc: 0.4669, val loss: 1.2616, val acc: 0.4772  (best train acc: 0.4740, best val acc: 0.4813)\n",
      "[Epoch: 19040] train loss: 1.2747, train acc: 0.4628, val loss: 1.2605, val acc: 0.4695  (best train acc: 0.4740, best val acc: 0.4813)\n",
      "[Epoch: 19060] train loss: 1.2694, train acc: 0.4696, val loss: 1.2609, val acc: 0.4742  (best train acc: 0.4740, best val acc: 0.4813)\n",
      "[Epoch: 19080] train loss: 1.2731, train acc: 0.4683, val loss: 1.2644, val acc: 0.4627  (best train acc: 0.4740, best val acc: 0.4813)\n",
      "[Epoch: 19100] train loss: 1.2735, train acc: 0.4701, val loss: 1.2653, val acc: 0.4607  (best train acc: 0.4740, best val acc: 0.4813)\n",
      "[Epoch: 19120] train loss: 1.2764, train acc: 0.4626, val loss: 1.2620, val acc: 0.4752  (best train acc: 0.4740, best val acc: 0.4813)\n",
      "[Epoch: 19140] train loss: 1.2758, train acc: 0.4646, val loss: 1.2711, val acc: 0.4567  (best train acc: 0.4740, best val acc: 0.4813)\n",
      "[Epoch: 19160] train loss: 1.2698, train acc: 0.4706, val loss: 1.2626, val acc: 0.4631  (best train acc: 0.4740, best val acc: 0.4813)\n",
      "[Epoch: 19180] train loss: 1.2744, train acc: 0.4696, val loss: 1.2592, val acc: 0.4695  (best train acc: 0.4740, best val acc: 0.4813)\n",
      "[Epoch: 19200] train loss: 1.2694, train acc: 0.4703, val loss: 1.2631, val acc: 0.4597  (best train acc: 0.4740, best val acc: 0.4813)\n",
      "[Epoch: 19220] train loss: 1.2719, train acc: 0.4704, val loss: 1.2634, val acc: 0.4587  (best train acc: 0.4740, best val acc: 0.4816)\n",
      "[Epoch: 19240] train loss: 1.2652, train acc: 0.4759, val loss: 1.2618, val acc: 0.4600  (best train acc: 0.4759, best val acc: 0.4816)\n",
      "[Epoch: 19260] train loss: 1.2688, train acc: 0.4701, val loss: 1.2588, val acc: 0.4688  (best train acc: 0.4759, best val acc: 0.4816)\n",
      "[Epoch: 19280] train loss: 1.2675, train acc: 0.4691, val loss: 1.2592, val acc: 0.4766  (best train acc: 0.4759, best val acc: 0.4816)\n",
      "[Epoch: 19300] train loss: 1.2673, train acc: 0.4712, val loss: 1.2595, val acc: 0.4678  (best train acc: 0.4759, best val acc: 0.4816)\n",
      "[Epoch: 19320] train loss: 1.2703, train acc: 0.4723, val loss: 1.2593, val acc: 0.4688  (best train acc: 0.4759, best val acc: 0.4816)\n",
      "[Epoch: 19340] train loss: 1.2700, train acc: 0.4662, val loss: 1.2623, val acc: 0.4607  (best train acc: 0.4759, best val acc: 0.4816)\n",
      "[Epoch: 19360] train loss: 1.2780, train acc: 0.4641, val loss: 1.2672, val acc: 0.4577  (best train acc: 0.4759, best val acc: 0.4816)\n",
      "[Epoch: 19380] train loss: 1.2758, train acc: 0.4610, val loss: 1.2606, val acc: 0.4782  (best train acc: 0.4759, best val acc: 0.4816)\n",
      "[Epoch: 19400] train loss: 1.2695, train acc: 0.4704, val loss: 1.2595, val acc: 0.4634  (best train acc: 0.4759, best val acc: 0.4816)\n",
      "[Epoch: 19420] train loss: 1.2653, train acc: 0.4748, val loss: 1.2584, val acc: 0.4661  (best train acc: 0.4759, best val acc: 0.4816)\n",
      "[Epoch: 19440] train loss: 1.2671, train acc: 0.4742, val loss: 1.2607, val acc: 0.4813  (best train acc: 0.4759, best val acc: 0.4830)\n",
      "[Epoch: 19460] train loss: 1.2727, train acc: 0.4696, val loss: 1.2654, val acc: 0.4577  (best train acc: 0.4759, best val acc: 0.4830)\n",
      "[Epoch: 19480] train loss: 1.2685, train acc: 0.4730, val loss: 1.2586, val acc: 0.4664  (best train acc: 0.4759, best val acc: 0.4830)\n",
      "[Epoch: 19500] train loss: 1.2661, train acc: 0.4718, val loss: 1.2584, val acc: 0.4664  (best train acc: 0.4759, best val acc: 0.4830)\n",
      "[Epoch: 19520] train loss: 1.2724, train acc: 0.4667, val loss: 1.2611, val acc: 0.4627  (best train acc: 0.4759, best val acc: 0.4830)\n",
      "[Epoch: 19540] train loss: 1.2692, train acc: 0.4665, val loss: 1.2631, val acc: 0.4762  (best train acc: 0.4759, best val acc: 0.4830)\n",
      "[Epoch: 19560] train loss: 1.2755, train acc: 0.4664, val loss: 1.2650, val acc: 0.4627  (best train acc: 0.4759, best val acc: 0.4830)\n",
      "[Epoch: 19580] train loss: 1.2685, train acc: 0.4675, val loss: 1.2623, val acc: 0.4637  (best train acc: 0.4759, best val acc: 0.4830)\n",
      "[Epoch: 19600] train loss: 1.2655, train acc: 0.4710, val loss: 1.2580, val acc: 0.4678  (best train acc: 0.4759, best val acc: 0.4830)\n",
      "[Epoch: 19620] train loss: 1.2654, train acc: 0.4711, val loss: 1.2575, val acc: 0.4705  (best train acc: 0.4759, best val acc: 0.4830)\n",
      "[Epoch: 19640] train loss: 1.2674, train acc: 0.4725, val loss: 1.2572, val acc: 0.4722  (best train acc: 0.4759, best val acc: 0.4830)\n",
      "[Epoch: 19660] train loss: 1.2789, train acc: 0.4598, val loss: 1.2631, val acc: 0.4769  (best train acc: 0.4759, best val acc: 0.4830)\n",
      "[Epoch: 19680] train loss: 1.2747, train acc: 0.4618, val loss: 1.2644, val acc: 0.4759  (best train acc: 0.4759, best val acc: 0.4830)\n",
      "[Epoch: 19700] train loss: 1.2680, train acc: 0.4696, val loss: 1.2593, val acc: 0.4624  (best train acc: 0.4759, best val acc: 0.4830)\n",
      "[Epoch: 19720] train loss: 1.2686, train acc: 0.4720, val loss: 1.2596, val acc: 0.4597  (best train acc: 0.4759, best val acc: 0.4830)\n",
      "[Epoch: 19740] train loss: 1.2641, train acc: 0.4708, val loss: 1.2573, val acc: 0.4675  (best train acc: 0.4759, best val acc: 0.4830)\n",
      "[Epoch: 19760] train loss: 1.2657, train acc: 0.4690, val loss: 1.2584, val acc: 0.4614  (best train acc: 0.4759, best val acc: 0.4830)\n",
      "[Epoch: 19780] train loss: 1.2735, train acc: 0.4687, val loss: 1.2571, val acc: 0.4715  (best train acc: 0.4761, best val acc: 0.4830)\n",
      "[Epoch: 19800] train loss: 1.2677, train acc: 0.4676, val loss: 1.2588, val acc: 0.4627  (best train acc: 0.4761, best val acc: 0.4830)\n",
      "[Epoch: 19820] train loss: 1.2704, train acc: 0.4684, val loss: 1.2567, val acc: 0.4715  (best train acc: 0.4761, best val acc: 0.4830)\n",
      "[Epoch: 19840] train loss: 1.2718, train acc: 0.4677, val loss: 1.2593, val acc: 0.4610  (best train acc: 0.4779, best val acc: 0.4830)\n",
      "[Epoch: 19860] train loss: 1.2722, train acc: 0.4686, val loss: 1.2642, val acc: 0.4617  (best train acc: 0.4779, best val acc: 0.4830)\n",
      "[Epoch: 19880] train loss: 1.2667, train acc: 0.4725, val loss: 1.2584, val acc: 0.4664  (best train acc: 0.4779, best val acc: 0.4830)\n",
      "[Epoch: 19900] train loss: 1.2704, train acc: 0.4652, val loss: 1.2601, val acc: 0.4793  (best train acc: 0.4779, best val acc: 0.4830)\n",
      "[Epoch: 19920] train loss: 1.2659, train acc: 0.4709, val loss: 1.2562, val acc: 0.4813  (best train acc: 0.4779, best val acc: 0.4830)\n",
      "[Epoch: 19940] train loss: 1.2680, train acc: 0.4670, val loss: 1.2569, val acc: 0.4799  (best train acc: 0.4779, best val acc: 0.4830)\n",
      "[Epoch: 19960] train loss: 1.2672, train acc: 0.4668, val loss: 1.2567, val acc: 0.4796  (best train acc: 0.4779, best val acc: 0.4830)\n",
      "[Epoch: 19980] train loss: 1.2691, train acc: 0.4733, val loss: 1.2668, val acc: 0.4583  (best train acc: 0.4779, best val acc: 0.4836)\n",
      "[Epoch: 20000] train loss: 1.2719, train acc: 0.4683, val loss: 1.2688, val acc: 0.4614  (best train acc: 0.4779, best val acc: 0.4836)\n",
      "[Epoch: 20020] train loss: 1.2709, train acc: 0.4733, val loss: 1.2592, val acc: 0.4617  (best train acc: 0.4779, best val acc: 0.4836)\n",
      "[Epoch: 20040] train loss: 1.2692, train acc: 0.4675, val loss: 1.2581, val acc: 0.4786  (best train acc: 0.4779, best val acc: 0.4836)\n",
      "[Epoch: 20060] train loss: 1.2630, train acc: 0.4738, val loss: 1.2572, val acc: 0.4668  (best train acc: 0.4779, best val acc: 0.4836)\n",
      "[Epoch: 20080] train loss: 1.2654, train acc: 0.4700, val loss: 1.2559, val acc: 0.4799  (best train acc: 0.4779, best val acc: 0.4847)\n",
      "[Epoch: 20100] train loss: 1.2636, train acc: 0.4712, val loss: 1.2555, val acc: 0.4806  (best train acc: 0.4779, best val acc: 0.4847)\n",
      "[Epoch: 20120] train loss: 1.2712, train acc: 0.4636, val loss: 1.2696, val acc: 0.4621  (best train acc: 0.4779, best val acc: 0.4847)\n",
      "[Epoch: 20140] train loss: 1.2763, train acc: 0.4619, val loss: 1.2620, val acc: 0.4850  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 20160] train loss: 1.2765, train acc: 0.4608, val loss: 1.2640, val acc: 0.4610  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 20180] train loss: 1.2700, train acc: 0.4675, val loss: 1.2601, val acc: 0.4823  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 20200] train loss: 1.3544, train acc: 0.4323, val loss: 1.5183, val acc: 0.4037  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 20220] train loss: 1.5894, train acc: 0.1748, val loss: 1.5333, val acc: 0.3481  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 20240] train loss: 1.4785, train acc: 0.3188, val loss: 1.4704, val acc: 0.3309  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 20260] train loss: 1.4373, train acc: 0.3328, val loss: 1.4292, val acc: 0.3379  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 20280] train loss: 1.4117, train acc: 0.3290, val loss: 1.4090, val acc: 0.3379  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 20300] train loss: 1.4026, train acc: 0.3370, val loss: 1.3981, val acc: 0.3379  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 20320] train loss: 1.3947, train acc: 0.3777, val loss: 1.3934, val acc: 0.3963  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 20340] train loss: 1.3906, train acc: 0.3978, val loss: 1.3910, val acc: 0.3828  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 20360] train loss: 1.3901, train acc: 0.3963, val loss: 1.3899, val acc: 0.3798  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 20380] train loss: 1.3877, train acc: 0.4023, val loss: 1.3895, val acc: 0.3818  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 20400] train loss: 1.3846, train acc: 0.4038, val loss: 1.3885, val acc: 0.3818  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 20420] train loss: 1.3736, train acc: 0.4047, val loss: 1.3668, val acc: 0.3919  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 20440] train loss: 1.3689, train acc: 0.4067, val loss: 1.3636, val acc: 0.3939  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 20460] train loss: 1.3670, train acc: 0.4051, val loss: 1.3621, val acc: 0.3929  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 20480] train loss: 1.3619, train acc: 0.4023, val loss: 1.3604, val acc: 0.3943  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 20500] train loss: 1.3656, train acc: 0.4035, val loss: 1.3601, val acc: 0.3946  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 20520] train loss: 1.3656, train acc: 0.4062, val loss: 1.3595, val acc: 0.3956  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 20540] train loss: 1.3651, train acc: 0.4045, val loss: 1.3591, val acc: 0.3993  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 20560] train loss: 1.3641, train acc: 0.4043, val loss: 1.3591, val acc: 0.3936  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 20580] train loss: 1.3610, train acc: 0.4015, val loss: 1.3586, val acc: 0.3960  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 20600] train loss: 1.3635, train acc: 0.4066, val loss: 1.3589, val acc: 0.3960  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 20620] train loss: 1.3625, train acc: 0.4055, val loss: 1.3585, val acc: 0.3960  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 20640] train loss: 1.3630, train acc: 0.4074, val loss: 1.3584, val acc: 0.3983  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 20660] train loss: 1.3638, train acc: 0.4057, val loss: 1.3579, val acc: 0.3963  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 20680] train loss: 1.3618, train acc: 0.4058, val loss: 1.3580, val acc: 0.3963  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 20700] train loss: 1.3624, train acc: 0.4085, val loss: 1.3585, val acc: 0.3976  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 20720] train loss: 1.3617, train acc: 0.4079, val loss: 1.3580, val acc: 0.3976  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 20740] train loss: 1.3623, train acc: 0.4091, val loss: 1.3578, val acc: 0.3990  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 20760] train loss: 1.3607, train acc: 0.4085, val loss: 1.3575, val acc: 0.3976  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 20780] train loss: 1.3617, train acc: 0.4060, val loss: 1.3573, val acc: 0.3973  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 20800] train loss: 1.3605, train acc: 0.4052, val loss: 1.3573, val acc: 0.3970  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 20820] train loss: 1.3641, train acc: 0.4065, val loss: 1.3573, val acc: 0.4007  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 20840] train loss: 1.3633, train acc: 0.4047, val loss: 1.3571, val acc: 0.3990  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 20860] train loss: 1.3600, train acc: 0.4104, val loss: 1.3573, val acc: 0.3973  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 20880] train loss: 1.3587, train acc: 0.4086, val loss: 1.3566, val acc: 0.4010  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 20900] train loss: 1.3593, train acc: 0.4095, val loss: 1.3570, val acc: 0.3997  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 20920] train loss: 1.3599, train acc: 0.4093, val loss: 1.3563, val acc: 0.3983  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 20940] train loss: 1.3598, train acc: 0.4104, val loss: 1.3561, val acc: 0.4051  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 20960] train loss: 1.3591, train acc: 0.4091, val loss: 1.3557, val acc: 0.4024  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 20980] train loss: 1.3567, train acc: 0.4101, val loss: 1.3553, val acc: 0.4010  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 21000] train loss: 1.3621, train acc: 0.4072, val loss: 1.3549, val acc: 0.4007  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 21020] train loss: 1.3587, train acc: 0.4104, val loss: 1.3550, val acc: 0.4030  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 21040] train loss: 1.3595, train acc: 0.4133, val loss: 1.3550, val acc: 0.4013  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 21060] train loss: 1.3558, train acc: 0.4106, val loss: 1.3548, val acc: 0.4017  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 21080] train loss: 1.3594, train acc: 0.4076, val loss: 1.3544, val acc: 0.4037  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 21100] train loss: 1.3620, train acc: 0.4091, val loss: 1.3544, val acc: 0.4040  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 21120] train loss: 1.3586, train acc: 0.4115, val loss: 1.3588, val acc: 0.4030  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 21140] train loss: 1.3597, train acc: 0.4114, val loss: 1.3556, val acc: 0.4047  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 21160] train loss: 1.3585, train acc: 0.4119, val loss: 1.3554, val acc: 0.4061  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 21180] train loss: 1.3591, train acc: 0.4113, val loss: 1.3553, val acc: 0.4027  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 21200] train loss: 1.3601, train acc: 0.4079, val loss: 1.3550, val acc: 0.4027  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 21220] train loss: 1.3640, train acc: 0.4097, val loss: 1.3550, val acc: 0.4040  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 21240] train loss: 1.3615, train acc: 0.4094, val loss: 1.3550, val acc: 0.4037  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 21260] train loss: 1.3612, train acc: 0.4085, val loss: 1.3548, val acc: 0.4027  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 21280] train loss: 1.3613, train acc: 0.4074, val loss: 1.3548, val acc: 0.4013  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 21300] train loss: 1.3596, train acc: 0.4098, val loss: 1.3544, val acc: 0.4040  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 21320] train loss: 1.3588, train acc: 0.4145, val loss: 1.3545, val acc: 0.4044  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 21340] train loss: 1.3584, train acc: 0.4110, val loss: 1.3545, val acc: 0.4030  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 21360] train loss: 1.3588, train acc: 0.4089, val loss: 1.3545, val acc: 0.4047  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 21380] train loss: 1.3573, train acc: 0.4145, val loss: 1.3542, val acc: 0.4024  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 21400] train loss: 1.3568, train acc: 0.4124, val loss: 1.3541, val acc: 0.4044  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 21420] train loss: 1.3564, train acc: 0.4099, val loss: 1.3542, val acc: 0.4037  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 21440] train loss: 1.3601, train acc: 0.4106, val loss: 1.3541, val acc: 0.4047  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 21460] train loss: 1.3576, train acc: 0.4135, val loss: 1.3541, val acc: 0.4040  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 21480] train loss: 1.3585, train acc: 0.4126, val loss: 1.3543, val acc: 0.4064  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 21500] train loss: 1.3534, train acc: 0.4093, val loss: 1.3541, val acc: 0.4040  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 21520] train loss: 1.3595, train acc: 0.4082, val loss: 1.3540, val acc: 0.4044  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 21540] train loss: 1.3581, train acc: 0.4099, val loss: 1.3537, val acc: 0.4027  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 21560] train loss: 1.3570, train acc: 0.4088, val loss: 1.3536, val acc: 0.4030  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 21580] train loss: 1.3576, train acc: 0.4143, val loss: 1.3532, val acc: 0.4024  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 21600] train loss: 1.3571, train acc: 0.4111, val loss: 1.3533, val acc: 0.4034  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 21620] train loss: 1.3572, train acc: 0.4134, val loss: 1.3534, val acc: 0.4044  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 21640] train loss: 1.3575, train acc: 0.4112, val loss: 1.3532, val acc: 0.4037  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 21660] train loss: 1.3574, train acc: 0.4102, val loss: 1.3532, val acc: 0.4047  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 21680] train loss: 1.3576, train acc: 0.4121, val loss: 1.3530, val acc: 0.4044  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 21700] train loss: 1.3561, train acc: 0.4121, val loss: 1.3531, val acc: 0.4057  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 21720] train loss: 1.3576, train acc: 0.4104, val loss: 1.3528, val acc: 0.4064  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 21740] train loss: 1.3580, train acc: 0.4137, val loss: 1.3526, val acc: 0.4027  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 21760] train loss: 1.3561, train acc: 0.4080, val loss: 1.3525, val acc: 0.4020  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 21780] train loss: 1.3549, train acc: 0.4116, val loss: 1.3528, val acc: 0.4047  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 21800] train loss: 1.3590, train acc: 0.4098, val loss: 1.3527, val acc: 0.4071  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 21820] train loss: 1.3561, train acc: 0.4148, val loss: 1.3526, val acc: 0.4074  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 21840] train loss: 1.3555, train acc: 0.4106, val loss: 1.3524, val acc: 0.4017  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 21860] train loss: 1.3565, train acc: 0.4119, val loss: 1.3520, val acc: 0.4051  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 21880] train loss: 1.3574, train acc: 0.4114, val loss: 1.3520, val acc: 0.4057  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 21900] train loss: 1.3578, train acc: 0.4138, val loss: 1.3517, val acc: 0.4027  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 21920] train loss: 1.3544, train acc: 0.4119, val loss: 1.3515, val acc: 0.4061  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 21940] train loss: 1.3553, train acc: 0.4124, val loss: 1.3514, val acc: 0.4017  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 21960] train loss: 1.3574, train acc: 0.4135, val loss: 1.3516, val acc: 0.4007  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 21980] train loss: 1.3584, train acc: 0.4104, val loss: 1.3515, val acc: 0.4030  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 22000] train loss: 1.3558, train acc: 0.4106, val loss: 1.3523, val acc: 0.4057  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 22020] train loss: 1.3544, train acc: 0.4124, val loss: 1.3513, val acc: 0.4017  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 22040] train loss: 1.3528, train acc: 0.4096, val loss: 1.3510, val acc: 0.4051  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 22060] train loss: 1.3553, train acc: 0.4160, val loss: 1.3511, val acc: 0.4044  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 22080] train loss: 1.3581, train acc: 0.4083, val loss: 1.3511, val acc: 0.4013  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 22100] train loss: 1.3556, train acc: 0.4101, val loss: 1.3509, val acc: 0.4044  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 22120] train loss: 1.3529, train acc: 0.4144, val loss: 1.3508, val acc: 0.4034  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 22140] train loss: 1.3557, train acc: 0.4147, val loss: 1.3506, val acc: 0.4024  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 22160] train loss: 1.3531, train acc: 0.4114, val loss: 1.3510, val acc: 0.4054  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 22180] train loss: 1.3538, train acc: 0.4145, val loss: 1.3508, val acc: 0.4034  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 22200] train loss: 1.3549, train acc: 0.4122, val loss: 1.3507, val acc: 0.4051  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 22220] train loss: 1.3551, train acc: 0.4114, val loss: 1.3505, val acc: 0.4044  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 22240] train loss: 1.3557, train acc: 0.4108, val loss: 1.3504, val acc: 0.4040  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 22260] train loss: 1.3569, train acc: 0.4151, val loss: 1.3506, val acc: 0.4037  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 22280] train loss: 1.3553, train acc: 0.4135, val loss: 1.3506, val acc: 0.4037  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 22300] train loss: 1.3545, train acc: 0.4110, val loss: 1.3503, val acc: 0.4051  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 22320] train loss: 1.3551, train acc: 0.4159, val loss: 1.3503, val acc: 0.4040  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 22340] train loss: 1.3568, train acc: 0.4119, val loss: 1.3501, val acc: 0.4020  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 22360] train loss: 1.3555, train acc: 0.4115, val loss: 1.3502, val acc: 0.4074  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 22380] train loss: 1.3556, train acc: 0.4129, val loss: 1.3501, val acc: 0.4054  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 22400] train loss: 1.3532, train acc: 0.4156, val loss: 1.3502, val acc: 0.4057  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 22420] train loss: 1.3572, train acc: 0.4083, val loss: 1.3501, val acc: 0.4024  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 22440] train loss: 1.3558, train acc: 0.4126, val loss: 1.3505, val acc: 0.4000  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 22460] train loss: 1.3564, train acc: 0.4152, val loss: 1.3500, val acc: 0.4078  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 22480] train loss: 1.3532, train acc: 0.4161, val loss: 1.3497, val acc: 0.4078  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 22500] train loss: 1.3548, train acc: 0.4117, val loss: 1.3499, val acc: 0.4051  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 22520] train loss: 1.3527, train acc: 0.4119, val loss: 1.3500, val acc: 0.4074  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 22540] train loss: 1.3540, train acc: 0.4140, val loss: 1.3498, val acc: 0.4040  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 22560] train loss: 1.3558, train acc: 0.4159, val loss: 1.3503, val acc: 0.4064  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 22580] train loss: 1.3515, train acc: 0.4166, val loss: 1.3497, val acc: 0.4064  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 22600] train loss: 1.3548, train acc: 0.4128, val loss: 1.3498, val acc: 0.4054  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 22620] train loss: 1.3535, train acc: 0.4129, val loss: 1.3498, val acc: 0.4101  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 22640] train loss: 1.3554, train acc: 0.4148, val loss: 1.3494, val acc: 0.4081  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 22660] train loss: 1.3554, train acc: 0.4104, val loss: 1.3495, val acc: 0.4088  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 22680] train loss: 1.3532, train acc: 0.4122, val loss: 1.3495, val acc: 0.4108  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 22700] train loss: 1.3536, train acc: 0.4126, val loss: 1.3493, val acc: 0.4054  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 22720] train loss: 1.3540, train acc: 0.4173, val loss: 1.3494, val acc: 0.4024  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 22740] train loss: 1.3533, train acc: 0.4126, val loss: 1.3487, val acc: 0.4094  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 22760] train loss: 1.3521, train acc: 0.4133, val loss: 1.3502, val acc: 0.4067  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 22780] train loss: 1.3557, train acc: 0.4137, val loss: 1.3489, val acc: 0.4057  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 22800] train loss: 1.3530, train acc: 0.4140, val loss: 1.3487, val acc: 0.4081  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 22820] train loss: 1.3532, train acc: 0.4141, val loss: 1.3489, val acc: 0.4098  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 22840] train loss: 1.3532, train acc: 0.4138, val loss: 1.3485, val acc: 0.4101  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 22860] train loss: 1.3551, train acc: 0.4150, val loss: 1.3486, val acc: 0.4074  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 22880] train loss: 1.3529, train acc: 0.4144, val loss: 1.3495, val acc: 0.4145  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 22900] train loss: 1.3536, train acc: 0.4173, val loss: 1.3487, val acc: 0.4105  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 22920] train loss: 1.3505, train acc: 0.4154, val loss: 1.3486, val acc: 0.4101  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 22940] train loss: 1.3529, train acc: 0.4142, val loss: 1.3482, val acc: 0.4081  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 22960] train loss: 1.3548, train acc: 0.4103, val loss: 1.3485, val acc: 0.4024  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 22980] train loss: 1.3521, train acc: 0.4172, val loss: 1.3490, val acc: 0.4047  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 23000] train loss: 1.3536, train acc: 0.4193, val loss: 1.3480, val acc: 0.4037  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 23020] train loss: 1.3540, train acc: 0.4151, val loss: 1.3495, val acc: 0.4138  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 23040] train loss: 1.3533, train acc: 0.4137, val loss: 1.3488, val acc: 0.4105  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 23060] train loss: 1.3530, train acc: 0.4096, val loss: 1.3486, val acc: 0.4132  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 23080] train loss: 1.3514, train acc: 0.4144, val loss: 1.3478, val acc: 0.4094  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 23100] train loss: 1.3512, train acc: 0.4126, val loss: 1.3477, val acc: 0.4081  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 23120] train loss: 1.3520, train acc: 0.4182, val loss: 1.3481, val acc: 0.4091  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 23140] train loss: 1.3519, train acc: 0.4135, val loss: 1.3475, val acc: 0.4054  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 23160] train loss: 1.3544, train acc: 0.4166, val loss: 1.3482, val acc: 0.4040  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 23180] train loss: 1.3527, train acc: 0.4164, val loss: 1.3478, val acc: 0.4098  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 23200] train loss: 1.3508, train acc: 0.4123, val loss: 1.3474, val acc: 0.4088  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 23220] train loss: 1.3497, train acc: 0.4156, val loss: 1.3473, val acc: 0.4078  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 23240] train loss: 1.3529, train acc: 0.4127, val loss: 1.3470, val acc: 0.4098  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 23260] train loss: 1.3525, train acc: 0.4152, val loss: 1.3477, val acc: 0.4044  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 23280] train loss: 1.3521, train acc: 0.4136, val loss: 1.3475, val acc: 0.4044  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 23300] train loss: 1.3546, train acc: 0.4169, val loss: 1.3468, val acc: 0.4078  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 23320] train loss: 1.3530, train acc: 0.4134, val loss: 1.3468, val acc: 0.4047  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 23340] train loss: 1.3498, train acc: 0.4142, val loss: 1.3470, val acc: 0.4098  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 23360] train loss: 1.3517, train acc: 0.4155, val loss: 1.3478, val acc: 0.4078  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 23380] train loss: 1.3524, train acc: 0.4130, val loss: 1.3477, val acc: 0.4118  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 23400] train loss: 1.3543, train acc: 0.4120, val loss: 1.3485, val acc: 0.4078  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 23420] train loss: 1.3472, train acc: 0.4195, val loss: 1.3466, val acc: 0.4098  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 23440] train loss: 1.3498, train acc: 0.4134, val loss: 1.3461, val acc: 0.4044  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 23460] train loss: 1.3524, train acc: 0.4153, val loss: 1.3463, val acc: 0.4078  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 23480] train loss: 1.3524, train acc: 0.4148, val loss: 1.3467, val acc: 0.4051  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 23500] train loss: 1.3481, train acc: 0.4148, val loss: 1.3466, val acc: 0.4040  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 23520] train loss: 1.3486, train acc: 0.4158, val loss: 1.3462, val acc: 0.4074  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 23540] train loss: 1.3523, train acc: 0.4134, val loss: 1.3463, val acc: 0.4024  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 23560] train loss: 1.3483, train acc: 0.4127, val loss: 1.3467, val acc: 0.4125  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 23580] train loss: 1.3493, train acc: 0.4147, val loss: 1.3478, val acc: 0.4088  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 23600] train loss: 1.3529, train acc: 0.4136, val loss: 1.3454, val acc: 0.4091  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 23620] train loss: 1.3483, train acc: 0.4174, val loss: 1.3458, val acc: 0.4098  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 23640] train loss: 1.3516, train acc: 0.4137, val loss: 1.3453, val acc: 0.4115  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 23660] train loss: 1.3537, train acc: 0.4126, val loss: 1.3457, val acc: 0.4105  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 23680] train loss: 1.3604, train acc: 0.4122, val loss: 1.3521, val acc: 0.4067  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 23700] train loss: 1.3500, train acc: 0.4140, val loss: 1.3447, val acc: 0.4118  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 23720] train loss: 1.3507, train acc: 0.4124, val loss: 1.3438, val acc: 0.4057  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 23740] train loss: 1.3470, train acc: 0.4139, val loss: 1.3434, val acc: 0.4098  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 23760] train loss: 1.3456, train acc: 0.4173, val loss: 1.3433, val acc: 0.4108  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 23780] train loss: 1.3488, train acc: 0.4138, val loss: 1.3426, val acc: 0.4057  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 23800] train loss: 1.3494, train acc: 0.4149, val loss: 1.3424, val acc: 0.4061  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 23820] train loss: 1.3466, train acc: 0.4138, val loss: 1.3412, val acc: 0.4057  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 23840] train loss: 1.3464, train acc: 0.4088, val loss: 1.3416, val acc: 0.4057  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 23860] train loss: 1.3450, train acc: 0.4114, val loss: 1.3419, val acc: 0.4091  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 23880] train loss: 1.3433, train acc: 0.4131, val loss: 1.3378, val acc: 0.4074  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 23900] train loss: 1.3419, train acc: 0.4112, val loss: 1.3373, val acc: 0.4118  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 23920] train loss: 1.3400, train acc: 0.4131, val loss: 1.3358, val acc: 0.4132  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 23940] train loss: 1.3381, train acc: 0.4149, val loss: 1.3356, val acc: 0.4108  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 23960] train loss: 1.3372, train acc: 0.4078, val loss: 1.3340, val acc: 0.4121  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 23980] train loss: 1.3374, train acc: 0.4176, val loss: 1.3333, val acc: 0.4121  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 24000] train loss: 1.3353, train acc: 0.4180, val loss: 1.3317, val acc: 0.4162  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 24020] train loss: 1.3382, train acc: 0.4208, val loss: 1.3312, val acc: 0.4172  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 24040] train loss: 1.3333, train acc: 0.4160, val loss: 1.3307, val acc: 0.4155  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 24060] train loss: 1.3341, train acc: 0.4171, val loss: 1.3301, val acc: 0.4138  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 24080] train loss: 1.3341, train acc: 0.4154, val loss: 1.3299, val acc: 0.4132  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 24100] train loss: 1.3351, train acc: 0.4101, val loss: 1.3293, val acc: 0.4175  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 24120] train loss: 1.3355, train acc: 0.4130, val loss: 1.3293, val acc: 0.4202  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 24140] train loss: 1.3233, train acc: 0.4198, val loss: 1.3158, val acc: 0.4331  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 24160] train loss: 1.5308, train acc: 0.3085, val loss: 1.5486, val acc: 0.2563  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 24180] train loss: 1.5299, train acc: 0.2387, val loss: 1.4882, val acc: 0.3113  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 24200] train loss: 1.5191, train acc: 0.2649, val loss: 1.4912, val acc: 0.2867  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 24220] train loss: 1.4999, train acc: 0.3078, val loss: 1.4840, val acc: 0.3096  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 24240] train loss: 1.5038, train acc: 0.2943, val loss: 1.4787, val acc: 0.3221  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 24260] train loss: 1.4994, train acc: 0.2842, val loss: 1.4788, val acc: 0.3376  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 24280] train loss: 1.5016, train acc: 0.3030, val loss: 1.4747, val acc: 0.3518  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 24300] train loss: 1.5003, train acc: 0.3162, val loss: 1.4748, val acc: 0.3110  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 24320] train loss: 1.4919, train acc: 0.2713, val loss: 1.4706, val acc: 0.3130  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 24340] train loss: 1.4872, train acc: 0.3028, val loss: 1.4690, val acc: 0.3238  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 24360] train loss: 1.4815, train acc: 0.3059, val loss: 1.4623, val acc: 0.3052  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 24380] train loss: 1.4799, train acc: 0.3259, val loss: 1.4573, val acc: 0.3336  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 24400] train loss: 1.4754, train acc: 0.3248, val loss: 1.4514, val acc: 0.3413  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 24420] train loss: 1.4698, train acc: 0.3180, val loss: 1.4474, val acc: 0.3551  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 24440] train loss: 1.4710, train acc: 0.3101, val loss: 1.4380, val acc: 0.3447  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 24460] train loss: 1.4550, train acc: 0.3400, val loss: 1.4274, val acc: 0.3484  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 24480] train loss: 1.4467, train acc: 0.3119, val loss: 1.4177, val acc: 0.3676  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 24500] train loss: 1.4446, train acc: 0.3227, val loss: 1.4137, val acc: 0.3680  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 24520] train loss: 1.4398, train acc: 0.3527, val loss: 1.4144, val acc: 0.3690  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 24540] train loss: 1.4414, train acc: 0.3605, val loss: 1.4149, val acc: 0.3572  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 24560] train loss: 1.4416, train acc: 0.3254, val loss: 1.4139, val acc: 0.3491  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 24580] train loss: 1.4355, train acc: 0.3603, val loss: 1.4111, val acc: 0.3447  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 24600] train loss: 1.4364, train acc: 0.3401, val loss: 1.4105, val acc: 0.3717  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 24620] train loss: 1.4340, train acc: 0.3391, val loss: 1.4090, val acc: 0.3558  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 24640] train loss: 1.4350, train acc: 0.3322, val loss: 1.4097, val acc: 0.3555  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 24660] train loss: 1.4343, train acc: 0.3584, val loss: 1.4067, val acc: 0.3730  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 24680] train loss: 1.4282, train acc: 0.3473, val loss: 1.4042, val acc: 0.3875  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 24700] train loss: 1.4241, train acc: 0.3482, val loss: 1.4016, val acc: 0.3774  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 24720] train loss: 1.4178, train acc: 0.3777, val loss: 1.3889, val acc: 0.3801  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 24740] train loss: 1.4059, train acc: 0.3816, val loss: 1.3796, val acc: 0.3909  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 24760] train loss: 1.3982, train acc: 0.3910, val loss: 1.3754, val acc: 0.3892  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 24780] train loss: 1.3974, train acc: 0.3907, val loss: 1.3742, val acc: 0.3855  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 24800] train loss: 1.3974, train acc: 0.3882, val loss: 1.3718, val acc: 0.3983  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 24820] train loss: 1.4320, train acc: 0.3677, val loss: 1.4026, val acc: 0.3818  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 24840] train loss: 1.4021, train acc: 0.3874, val loss: 1.3717, val acc: 0.4010  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 24860] train loss: 1.4002, train acc: 0.3862, val loss: 1.3713, val acc: 0.3939  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 24880] train loss: 1.3939, train acc: 0.3872, val loss: 1.3697, val acc: 0.4000  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 24900] train loss: 1.3940, train acc: 0.3861, val loss: 1.3679, val acc: 0.3980  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 24920] train loss: 1.3919, train acc: 0.3865, val loss: 1.3671, val acc: 0.3949  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 24940] train loss: 1.3901, train acc: 0.3815, val loss: 1.3659, val acc: 0.3997  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 24960] train loss: 1.3922, train acc: 0.3893, val loss: 1.3670, val acc: 0.4037  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 24980] train loss: 1.3911, train acc: 0.3840, val loss: 1.3666, val acc: 0.4044  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 25000] train loss: 1.3852, train acc: 0.3906, val loss: 1.3652, val acc: 0.3973  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 25020] train loss: 1.3853, train acc: 0.3900, val loss: 1.3652, val acc: 0.4061  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 25040] train loss: 1.3849, train acc: 0.3914, val loss: 1.3648, val acc: 0.4037  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 25060] train loss: 1.3861, train acc: 0.3887, val loss: 1.3640, val acc: 0.3980  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 25080] train loss: 1.3835, train acc: 0.3967, val loss: 1.3647, val acc: 0.4152  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 25100] train loss: 1.3846, train acc: 0.3924, val loss: 1.3636, val acc: 0.4081  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 25120] train loss: 1.3853, train acc: 0.3953, val loss: 1.3630, val acc: 0.4084  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 25140] train loss: 1.3802, train acc: 0.4000, val loss: 1.3617, val acc: 0.4074  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 25160] train loss: 1.3833, train acc: 0.3957, val loss: 1.3619, val acc: 0.4010  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 25180] train loss: 1.3802, train acc: 0.3951, val loss: 1.3608, val acc: 0.4078  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 25200] train loss: 1.3842, train acc: 0.3960, val loss: 1.3599, val acc: 0.3980  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 25220] train loss: 1.3794, train acc: 0.4023, val loss: 1.3590, val acc: 0.4088  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 25240] train loss: 1.3803, train acc: 0.3941, val loss: 1.3575, val acc: 0.4013  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 25260] train loss: 1.3830, train acc: 0.3952, val loss: 1.3576, val acc: 0.4094  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 25280] train loss: 1.3762, train acc: 0.3943, val loss: 1.3568, val acc: 0.4101  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 25300] train loss: 1.3808, train acc: 0.3975, val loss: 1.3564, val acc: 0.4108  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 25320] train loss: 1.3782, train acc: 0.3984, val loss: 1.3571, val acc: 0.4125  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 25340] train loss: 1.3777, train acc: 0.3970, val loss: 1.3560, val acc: 0.4105  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 25360] train loss: 1.3776, train acc: 0.3972, val loss: 1.3566, val acc: 0.4078  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 25380] train loss: 1.3779, train acc: 0.3944, val loss: 1.3556, val acc: 0.4165  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 25400] train loss: 1.3763, train acc: 0.4036, val loss: 1.3546, val acc: 0.4084  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 25420] train loss: 1.3749, train acc: 0.3970, val loss: 1.3542, val acc: 0.4037  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 25440] train loss: 1.3730, train acc: 0.3999, val loss: 1.3542, val acc: 0.4121  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 25460] train loss: 1.3771, train acc: 0.3956, val loss: 1.3535, val acc: 0.4091  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 25480] train loss: 1.3745, train acc: 0.4004, val loss: 1.3531, val acc: 0.4155  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 25500] train loss: 1.3738, train acc: 0.3970, val loss: 1.3535, val acc: 0.4132  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 25520] train loss: 1.3732, train acc: 0.3999, val loss: 1.3513, val acc: 0.4145  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 25540] train loss: 1.3745, train acc: 0.3999, val loss: 1.3526, val acc: 0.4159  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 25560] train loss: 1.3718, train acc: 0.4035, val loss: 1.3518, val acc: 0.4098  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 25580] train loss: 1.3687, train acc: 0.4006, val loss: 1.3522, val acc: 0.4142  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 25600] train loss: 1.3688, train acc: 0.4041, val loss: 1.3502, val acc: 0.4145  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 25620] train loss: 1.3730, train acc: 0.4018, val loss: 1.3516, val acc: 0.4179  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 25640] train loss: 1.3702, train acc: 0.4069, val loss: 1.3487, val acc: 0.4101  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 25660] train loss: 1.3701, train acc: 0.4089, val loss: 1.3478, val acc: 0.4165  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 25680] train loss: 1.3716, train acc: 0.4044, val loss: 1.3492, val acc: 0.4175  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 25700] train loss: 1.3684, train acc: 0.4036, val loss: 1.3484, val acc: 0.4091  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 25720] train loss: 1.3696, train acc: 0.4080, val loss: 1.3475, val acc: 0.4185  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 25740] train loss: 1.3694, train acc: 0.4025, val loss: 1.3477, val acc: 0.4145  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 25760] train loss: 1.3694, train acc: 0.4035, val loss: 1.3460, val acc: 0.4135  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 25780] train loss: 1.3689, train acc: 0.4073, val loss: 1.3454, val acc: 0.4152  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 25800] train loss: 1.3636, train acc: 0.4109, val loss: 1.3449, val acc: 0.4088  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 25820] train loss: 1.3685, train acc: 0.4031, val loss: 1.3455, val acc: 0.4199  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 25840] train loss: 1.3698, train acc: 0.4035, val loss: 1.3432, val acc: 0.4179  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 25860] train loss: 1.3704, train acc: 0.4025, val loss: 1.3471, val acc: 0.4071  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 25880] train loss: 1.7564, train acc: 0.2371, val loss: 1.6011, val acc: 0.2604  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 25900] train loss: 1.4698, train acc: 0.3233, val loss: 1.4672, val acc: 0.2840  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 25920] train loss: 1.4537, train acc: 0.3237, val loss: 1.4395, val acc: 0.3373  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 25940] train loss: 1.4494, train acc: 0.3670, val loss: 1.4347, val acc: 0.3781  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 25960] train loss: 1.4440, train acc: 0.3699, val loss: 1.4303, val acc: 0.3656  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 25980] train loss: 1.4374, train acc: 0.3694, val loss: 1.4258, val acc: 0.3686  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 26000] train loss: 1.4432, train acc: 0.3639, val loss: 1.4235, val acc: 0.3656  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 26020] train loss: 1.4375, train acc: 0.3695, val loss: 1.4196, val acc: 0.3747  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 26040] train loss: 1.4317, train acc: 0.3717, val loss: 1.4186, val acc: 0.3754  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 26060] train loss: 1.4343, train acc: 0.3774, val loss: 1.4160, val acc: 0.3791  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 26080] train loss: 1.4290, train acc: 0.3810, val loss: 1.4149, val acc: 0.3889  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 26100] train loss: 1.4302, train acc: 0.3759, val loss: 1.4132, val acc: 0.3892  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 26120] train loss: 1.4285, train acc: 0.3748, val loss: 1.4121, val acc: 0.3858  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 26140] train loss: 1.4257, train acc: 0.3778, val loss: 1.4112, val acc: 0.3801  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 26160] train loss: 1.4215, train acc: 0.3785, val loss: 1.4096, val acc: 0.3841  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 26180] train loss: 1.4234, train acc: 0.3785, val loss: 1.4105, val acc: 0.3798  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 26200] train loss: 1.4212, train acc: 0.3790, val loss: 1.4097, val acc: 0.3831  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 26220] train loss: 1.4243, train acc: 0.3796, val loss: 1.4091, val acc: 0.3841  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 26240] train loss: 1.4214, train acc: 0.3837, val loss: 1.4078, val acc: 0.3922  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 26260] train loss: 1.4207, train acc: 0.3805, val loss: 1.4075, val acc: 0.3862  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 26280] train loss: 1.4193, train acc: 0.3852, val loss: 1.4071, val acc: 0.3868  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 26300] train loss: 1.4167, train acc: 0.3890, val loss: 1.4059, val acc: 0.3929  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 26320] train loss: 1.4189, train acc: 0.3845, val loss: 1.4063, val acc: 0.3858  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 26340] train loss: 1.4180, train acc: 0.3879, val loss: 1.4057, val acc: 0.3889  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 26360] train loss: 1.4197, train acc: 0.3795, val loss: 1.4058, val acc: 0.3845  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 26380] train loss: 1.4170, train acc: 0.3843, val loss: 1.4048, val acc: 0.3841  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 26400] train loss: 1.4152, train acc: 0.3864, val loss: 1.4040, val acc: 0.3902  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 26420] train loss: 1.4169, train acc: 0.3874, val loss: 1.4045, val acc: 0.3895  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 26440] train loss: 1.4112, train acc: 0.3888, val loss: 1.4036, val acc: 0.3909  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 26460] train loss: 1.4201, train acc: 0.3867, val loss: 1.4064, val acc: 0.3895  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 26480] train loss: 1.4140, train acc: 0.3905, val loss: 1.4031, val acc: 0.3933  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 26500] train loss: 1.4149, train acc: 0.3908, val loss: 1.4031, val acc: 0.3895  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 26520] train loss: 1.4122, train acc: 0.3898, val loss: 1.4030, val acc: 0.3801  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 26540] train loss: 1.4122, train acc: 0.3941, val loss: 1.4021, val acc: 0.3889  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 26560] train loss: 1.4117, train acc: 0.3880, val loss: 1.4013, val acc: 0.3811  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 26580] train loss: 1.4110, train acc: 0.3924, val loss: 1.4011, val acc: 0.3993  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 26600] train loss: 1.4092, train acc: 0.3933, val loss: 1.4012, val acc: 0.3831  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 26620] train loss: 1.4127, train acc: 0.3663, val loss: 1.4018, val acc: 0.4010  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 26640] train loss: 1.4122, train acc: 0.3928, val loss: 1.4005, val acc: 0.3946  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 26660] train loss: 1.4079, train acc: 0.3954, val loss: 1.4001, val acc: 0.3845  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 26680] train loss: 1.4101, train acc: 0.3970, val loss: 1.4019, val acc: 0.3889  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 26700] train loss: 1.4143, train acc: 0.3595, val loss: 1.4013, val acc: 0.4000  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 26720] train loss: 1.4077, train acc: 0.3963, val loss: 1.3998, val acc: 0.3855  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 26740] train loss: 1.4083, train acc: 0.3972, val loss: 1.3999, val acc: 0.3761  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 26760] train loss: 1.4093, train acc: 0.3894, val loss: 1.4009, val acc: 0.4013  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 26780] train loss: 1.4071, train acc: 0.3981, val loss: 1.3991, val acc: 0.3889  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 26800] train loss: 1.4078, train acc: 0.3967, val loss: 1.3988, val acc: 0.3831  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 26820] train loss: 1.4122, train acc: 0.3918, val loss: 1.4022, val acc: 0.3501  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 26840] train loss: 1.4031, train acc: 0.3984, val loss: 1.4001, val acc: 0.3966  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 26860] train loss: 1.4085, train acc: 0.3923, val loss: 1.4001, val acc: 0.4054  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 26880] train loss: 1.4059, train acc: 0.3984, val loss: 1.3980, val acc: 0.4024  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 26900] train loss: 1.4095, train acc: 0.3961, val loss: 1.3977, val acc: 0.3953  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 26920] train loss: 1.4079, train acc: 0.3990, val loss: 1.3989, val acc: 0.4017  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 26940] train loss: 1.4044, train acc: 0.4057, val loss: 1.3989, val acc: 0.3949  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 26960] train loss: 1.4087, train acc: 0.3934, val loss: 1.3983, val acc: 0.3960  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 26980] train loss: 1.4053, train acc: 0.4020, val loss: 1.3985, val acc: 0.3949  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 27000] train loss: 1.4079, train acc: 0.3992, val loss: 1.3984, val acc: 0.4027  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 27020] train loss: 1.4037, train acc: 0.3994, val loss: 1.3977, val acc: 0.4027  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 27040] train loss: 1.4041, train acc: 0.4053, val loss: 1.3985, val acc: 0.3963  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 27060] train loss: 1.4077, train acc: 0.4053, val loss: 1.3981, val acc: 0.3966  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 27080] train loss: 1.4059, train acc: 0.4036, val loss: 1.3975, val acc: 0.4030  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 27100] train loss: 1.4052, train acc: 0.4025, val loss: 1.3984, val acc: 0.3976  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 27120] train loss: 1.4070, train acc: 0.3989, val loss: 1.3974, val acc: 0.4010  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 27140] train loss: 1.4059, train acc: 0.4010, val loss: 1.3969, val acc: 0.3922  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 27160] train loss: 1.4068, train acc: 0.3919, val loss: 1.3966, val acc: 0.3976  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 27180] train loss: 1.4044, train acc: 0.3905, val loss: 1.3969, val acc: 0.4030  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 27200] train loss: 1.4047, train acc: 0.3677, val loss: 1.3981, val acc: 0.3953  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 27220] train loss: 1.4069, train acc: 0.3579, val loss: 1.3965, val acc: 0.4061  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 27240] train loss: 1.4066, train acc: 0.3905, val loss: 1.3971, val acc: 0.3966  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 27260] train loss: 1.4039, train acc: 0.3920, val loss: 1.3974, val acc: 0.4037  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 27280] train loss: 1.4040, train acc: 0.3932, val loss: 1.3962, val acc: 0.4020  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 27300] train loss: 1.4054, train acc: 0.3930, val loss: 1.3958, val acc: 0.3899  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 27320] train loss: 1.4019, train acc: 0.3937, val loss: 1.3955, val acc: 0.3949  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 27340] train loss: 1.4054, train acc: 0.3951, val loss: 1.3957, val acc: 0.3922  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 27360] train loss: 1.4045, train acc: 0.3989, val loss: 1.3964, val acc: 0.3949  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 27380] train loss: 1.4004, train acc: 0.3940, val loss: 1.3959, val acc: 0.3875  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 27400] train loss: 1.4029, train acc: 0.3952, val loss: 1.3951, val acc: 0.4003  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 27420] train loss: 1.4069, train acc: 0.3914, val loss: 1.3954, val acc: 0.3983  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 27440] train loss: 1.4065, train acc: 0.3889, val loss: 1.3969, val acc: 0.4071  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 27460] train loss: 1.4026, train acc: 0.3957, val loss: 1.3981, val acc: 0.3973  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 27480] train loss: 1.4054, train acc: 0.3927, val loss: 1.3964, val acc: 0.3953  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 27500] train loss: 1.4043, train acc: 0.3663, val loss: 1.3954, val acc: 0.4030  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 27520] train loss: 1.4034, train acc: 0.3887, val loss: 1.3973, val acc: 0.3946  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 27540] train loss: 1.4048, train acc: 0.3847, val loss: 1.3948, val acc: 0.3949  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 27560] train loss: 1.4041, train acc: 0.3855, val loss: 1.3953, val acc: 0.3902  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 27580] train loss: 1.4019, train acc: 0.3827, val loss: 1.3950, val acc: 0.3781  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 27600] train loss: 1.4025, train acc: 0.3908, val loss: 1.3970, val acc: 0.3936  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 27620] train loss: 1.3999, train acc: 0.3942, val loss: 1.3949, val acc: 0.4061  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 27640] train loss: 1.4032, train acc: 0.3932, val loss: 1.3968, val acc: 0.3619  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 27660] train loss: 1.4023, train acc: 0.3856, val loss: 1.3943, val acc: 0.3960  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 27680] train loss: 1.4031, train acc: 0.3825, val loss: 1.3952, val acc: 0.3889  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 27700] train loss: 1.4030, train acc: 0.3935, val loss: 1.3952, val acc: 0.3929  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 27720] train loss: 1.4001, train acc: 0.3901, val loss: 1.3934, val acc: 0.4007  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 27740] train loss: 1.4002, train acc: 0.3912, val loss: 1.3937, val acc: 0.3963  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 27760] train loss: 1.4027, train acc: 0.3916, val loss: 1.3929, val acc: 0.3916  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 27780] train loss: 1.3996, train acc: 0.3921, val loss: 1.3936, val acc: 0.3993  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 27800] train loss: 1.4015, train acc: 0.3943, val loss: 1.3942, val acc: 0.3916  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 27820] train loss: 1.4012, train acc: 0.3898, val loss: 1.3938, val acc: 0.3916  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 27840] train loss: 1.4022, train acc: 0.3924, val loss: 1.3946, val acc: 0.3943  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 27860] train loss: 1.4033, train acc: 0.3905, val loss: 1.3934, val acc: 0.4020  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 27880] train loss: 1.4008, train acc: 0.3924, val loss: 1.3929, val acc: 0.3963  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 27900] train loss: 1.4015, train acc: 0.3590, val loss: 1.3938, val acc: 0.3976  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 27920] train loss: 1.4073, train acc: 0.3557, val loss: 1.3952, val acc: 0.3815  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 27940] train loss: 1.4028, train acc: 0.3842, val loss: 1.3929, val acc: 0.4030  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 27960] train loss: 1.3996, train acc: 0.3935, val loss: 1.3944, val acc: 0.4010  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 27980] train loss: 1.3991, train acc: 0.3871, val loss: 1.3936, val acc: 0.3980  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 28000] train loss: 1.4013, train acc: 0.3921, val loss: 1.3925, val acc: 0.4027  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 28020] train loss: 1.3997, train acc: 0.3899, val loss: 1.3926, val acc: 0.3895  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 28040] train loss: 1.4015, train acc: 0.3827, val loss: 1.3934, val acc: 0.3899  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 28060] train loss: 1.4011, train acc: 0.3882, val loss: 1.3929, val acc: 0.3872  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 28080] train loss: 1.3987, train acc: 0.3876, val loss: 1.3922, val acc: 0.3909  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 28100] train loss: 1.4002, train acc: 0.3852, val loss: 1.3958, val acc: 0.3636  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 28120] train loss: 1.3980, train acc: 0.3848, val loss: 1.3917, val acc: 0.3929  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 28140] train loss: 1.4009, train acc: 0.3897, val loss: 1.3927, val acc: 0.3882  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 28160] train loss: 1.4010, train acc: 0.3882, val loss: 1.3927, val acc: 0.3906  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 28180] train loss: 1.3964, train acc: 0.3895, val loss: 1.3920, val acc: 0.3953  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 28200] train loss: 1.4021, train acc: 0.3587, val loss: 1.3943, val acc: 0.3939  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 28220] train loss: 1.3989, train acc: 0.3919, val loss: 1.3928, val acc: 0.3980  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 28240] train loss: 1.4005, train acc: 0.3881, val loss: 1.3919, val acc: 0.3882  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 28260] train loss: 1.3973, train acc: 0.3895, val loss: 1.3935, val acc: 0.4017  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 28280] train loss: 1.4003, train acc: 0.3864, val loss: 1.3917, val acc: 0.3970  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 28300] train loss: 1.3972, train acc: 0.3890, val loss: 1.3921, val acc: 0.3838  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 28320] train loss: 1.3991, train acc: 0.3871, val loss: 1.3912, val acc: 0.3922  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 28340] train loss: 1.3981, train acc: 0.3858, val loss: 1.3924, val acc: 0.3831  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 28360] train loss: 1.3963, train acc: 0.3922, val loss: 1.3913, val acc: 0.3906  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 28380] train loss: 1.4005, train acc: 0.3908, val loss: 1.3949, val acc: 0.3902  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 28400] train loss: 1.3990, train acc: 0.3868, val loss: 1.3919, val acc: 0.3879  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 28420] train loss: 1.4028, train acc: 0.3888, val loss: 1.3925, val acc: 0.3862  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 28440] train loss: 1.4020, train acc: 0.3591, val loss: 1.3956, val acc: 0.3612  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 28460] train loss: 1.3985, train acc: 0.3890, val loss: 1.3930, val acc: 0.3926  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 28480] train loss: 1.4045, train acc: 0.3770, val loss: 1.3932, val acc: 0.3582  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 28500] train loss: 1.3987, train acc: 0.3832, val loss: 1.3908, val acc: 0.3960  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 28520] train loss: 1.3987, train acc: 0.3785, val loss: 1.3910, val acc: 0.3960  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 28540] train loss: 1.3972, train acc: 0.3918, val loss: 1.3926, val acc: 0.3956  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 28560] train loss: 1.3969, train acc: 0.3936, val loss: 1.3913, val acc: 0.3916  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 28580] train loss: 1.3974, train acc: 0.3877, val loss: 1.3931, val acc: 0.3875  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 28600] train loss: 1.3979, train acc: 0.3857, val loss: 1.3905, val acc: 0.3855  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 28620] train loss: 1.3960, train acc: 0.3933, val loss: 1.3908, val acc: 0.3862  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 28640] train loss: 1.4058, train acc: 0.3753, val loss: 1.3915, val acc: 0.3835  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 28660] train loss: 1.3969, train acc: 0.3873, val loss: 1.3911, val acc: 0.3933  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 28680] train loss: 1.3960, train acc: 0.3917, val loss: 1.3913, val acc: 0.3953  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 28700] train loss: 1.4044, train acc: 0.3751, val loss: 1.3957, val acc: 0.4027  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 28720] train loss: 1.4000, train acc: 0.3902, val loss: 1.3909, val acc: 0.3872  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 28740] train loss: 1.3976, train acc: 0.3822, val loss: 1.3905, val acc: 0.3966  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 28760] train loss: 1.3984, train acc: 0.3871, val loss: 1.3915, val acc: 0.3895  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 28780] train loss: 1.3957, train acc: 0.3911, val loss: 1.3901, val acc: 0.3929  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 28800] train loss: 1.4005, train acc: 0.3742, val loss: 1.3915, val acc: 0.4044  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 28820] train loss: 1.3980, train acc: 0.3892, val loss: 1.3921, val acc: 0.4020  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 28840] train loss: 1.3990, train acc: 0.3908, val loss: 1.3903, val acc: 0.3966  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 28860] train loss: 1.3987, train acc: 0.3835, val loss: 1.3908, val acc: 0.3936  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 28880] train loss: 1.3975, train acc: 0.3894, val loss: 1.3899, val acc: 0.3929  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 28900] train loss: 1.3981, train acc: 0.3911, val loss: 1.3898, val acc: 0.3990  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 28920] train loss: 1.3979, train acc: 0.3895, val loss: 1.3913, val acc: 0.3831  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 28940] train loss: 1.4041, train acc: 0.3852, val loss: 1.3901, val acc: 0.3906  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 28960] train loss: 1.3991, train acc: 0.3853, val loss: 1.3903, val acc: 0.3987  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 28980] train loss: 1.3971, train acc: 0.3932, val loss: 1.3904, val acc: 0.3946  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 29000] train loss: 1.3974, train acc: 0.3845, val loss: 1.3902, val acc: 0.3993  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 29020] train loss: 1.3967, train acc: 0.3818, val loss: 1.3898, val acc: 0.3845  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 29040] train loss: 1.3978, train acc: 0.3854, val loss: 1.3902, val acc: 0.3993  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 29060] train loss: 1.3975, train acc: 0.3948, val loss: 1.3894, val acc: 0.3953  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 29080] train loss: 1.4006, train acc: 0.3730, val loss: 1.3911, val acc: 0.3949  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 29100] train loss: 1.3999, train acc: 0.3532, val loss: 1.3895, val acc: 0.4027  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 29120] train loss: 1.3995, train acc: 0.3806, val loss: 1.3896, val acc: 0.3926  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 29140] train loss: 1.3984, train acc: 0.3861, val loss: 1.3928, val acc: 0.3919  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 29160] train loss: 1.3998, train acc: 0.3822, val loss: 1.3895, val acc: 0.3831  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 29180] train loss: 1.3980, train acc: 0.3780, val loss: 1.3931, val acc: 0.3602  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 29200] train loss: 1.3980, train acc: 0.3913, val loss: 1.3907, val acc: 0.3960  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 29220] train loss: 1.3991, train acc: 0.3806, val loss: 1.3908, val acc: 0.3858  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 29240] train loss: 1.3968, train acc: 0.3866, val loss: 1.3907, val acc: 0.3825  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 29260] train loss: 1.3982, train acc: 0.3836, val loss: 1.3901, val acc: 0.3895  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 29280] train loss: 1.3979, train acc: 0.3790, val loss: 1.3905, val acc: 0.3909  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 29300] train loss: 1.3950, train acc: 0.3864, val loss: 1.3892, val acc: 0.4007  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 29320] train loss: 1.3995, train acc: 0.3905, val loss: 1.3895, val acc: 0.3906  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 29340] train loss: 1.3971, train acc: 0.3916, val loss: 1.3890, val acc: 0.3956  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 29360] train loss: 1.3962, train acc: 0.3889, val loss: 1.3893, val acc: 0.3858  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 29380] train loss: 1.3984, train acc: 0.3945, val loss: 1.3900, val acc: 0.3980  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 29400] train loss: 1.3967, train acc: 0.3891, val loss: 1.3915, val acc: 0.4057  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 29420] train loss: 1.3981, train acc: 0.3897, val loss: 1.3900, val acc: 0.3919  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 29440] train loss: 1.3996, train acc: 0.3872, val loss: 1.3912, val acc: 0.3997  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 29460] train loss: 1.3970, train acc: 0.3877, val loss: 1.3903, val acc: 0.3909  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 29480] train loss: 1.3973, train acc: 0.3842, val loss: 1.3900, val acc: 0.3848  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 29500] train loss: 1.3982, train acc: 0.3646, val loss: 1.3897, val acc: 0.3939  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 29520] train loss: 1.3948, train acc: 0.3896, val loss: 1.3891, val acc: 0.3943  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 29540] train loss: 1.3964, train acc: 0.3929, val loss: 1.3889, val acc: 0.3953  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 29560] train loss: 1.3974, train acc: 0.3840, val loss: 1.3898, val acc: 0.3875  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 29580] train loss: 1.3977, train acc: 0.3865, val loss: 1.3916, val acc: 0.3825  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 29600] train loss: 1.3954, train acc: 0.3918, val loss: 1.3899, val acc: 0.3936  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 29620] train loss: 1.3982, train acc: 0.3843, val loss: 1.3896, val acc: 0.3852  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 29640] train loss: 1.3971, train acc: 0.3798, val loss: 1.3889, val acc: 0.3862  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 29660] train loss: 1.3975, train acc: 0.3828, val loss: 1.3892, val acc: 0.3987  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 29680] train loss: 1.4003, train acc: 0.3825, val loss: 1.3934, val acc: 0.3595  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 29700] train loss: 1.3966, train acc: 0.3803, val loss: 1.3899, val acc: 0.3879  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 29720] train loss: 1.3957, train acc: 0.3817, val loss: 1.3901, val acc: 0.3929  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 29740] train loss: 1.3966, train acc: 0.3848, val loss: 1.3890, val acc: 0.3879  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 29760] train loss: 1.4009, train acc: 0.3751, val loss: 1.3941, val acc: 0.3609  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 29780] train loss: 1.3967, train acc: 0.3890, val loss: 1.3903, val acc: 0.3798  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 29800] train loss: 1.3949, train acc: 0.3888, val loss: 1.3892, val acc: 0.3936  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 29820] train loss: 1.4000, train acc: 0.3828, val loss: 1.3885, val acc: 0.3902  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 29840] train loss: 1.3964, train acc: 0.3829, val loss: 1.3897, val acc: 0.3980  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 29860] train loss: 1.3950, train acc: 0.3838, val loss: 1.3890, val acc: 0.3889  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 29880] train loss: 1.3977, train acc: 0.3870, val loss: 1.3906, val acc: 0.3825  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 29900] train loss: 1.3928, train acc: 0.3930, val loss: 1.3925, val acc: 0.3798  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 29920] train loss: 1.3995, train acc: 0.3895, val loss: 1.3893, val acc: 0.3916  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 29940] train loss: 1.3974, train acc: 0.3860, val loss: 1.3892, val acc: 0.3919  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 29960] train loss: 1.3957, train acc: 0.3882, val loss: 1.3897, val acc: 0.3943  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 29980] train loss: 1.3972, train acc: 0.3891, val loss: 1.3887, val acc: 0.3919  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 30000] train loss: 1.3948, train acc: 0.3887, val loss: 1.3896, val acc: 0.3909  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 30020] train loss: 1.3954, train acc: 0.3869, val loss: 1.3893, val acc: 0.4000  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 30040] train loss: 1.3976, train acc: 0.3790, val loss: 1.3894, val acc: 0.3956  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 30060] train loss: 1.3988, train acc: 0.3883, val loss: 1.3901, val acc: 0.3946  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 30080] train loss: 1.3959, train acc: 0.3874, val loss: 1.3888, val acc: 0.3949  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 30100] train loss: 1.3967, train acc: 0.3929, val loss: 1.3894, val acc: 0.3889  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 30120] train loss: 1.4003, train acc: 0.3523, val loss: 1.3894, val acc: 0.4078  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 30140] train loss: 1.3969, train acc: 0.3790, val loss: 1.3885, val acc: 0.3922  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 30160] train loss: 1.3963, train acc: 0.3901, val loss: 1.3899, val acc: 0.3980  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 30180] train loss: 1.3970, train acc: 0.3840, val loss: 1.3886, val acc: 0.3933  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 30200] train loss: 1.3983, train acc: 0.3884, val loss: 1.3904, val acc: 0.4007  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 30220] train loss: 1.3983, train acc: 0.3833, val loss: 1.3883, val acc: 0.3879  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 30240] train loss: 1.3967, train acc: 0.3840, val loss: 1.3884, val acc: 0.3841  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 30260] train loss: 1.3951, train acc: 0.3946, val loss: 1.3886, val acc: 0.3879  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 30280] train loss: 1.3935, train acc: 0.3885, val loss: 1.3910, val acc: 0.3875  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 30300] train loss: 1.3959, train acc: 0.3845, val loss: 1.3910, val acc: 0.3983  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 30320] train loss: 1.3982, train acc: 0.3910, val loss: 1.3890, val acc: 0.3960  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 30340] train loss: 1.4022, train acc: 0.3746, val loss: 1.3916, val acc: 0.4040  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 30360] train loss: 1.3960, train acc: 0.3673, val loss: 1.3932, val acc: 0.4047  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 30380] train loss: 1.3980, train acc: 0.3895, val loss: 1.3892, val acc: 0.3801  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 30400] train loss: 1.3928, train acc: 0.3890, val loss: 1.3918, val acc: 0.3858  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 30420] train loss: 1.3956, train acc: 0.3862, val loss: 1.3887, val acc: 0.3889  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 30440] train loss: 1.3984, train acc: 0.3754, val loss: 1.3913, val acc: 0.3632  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 30460] train loss: 1.3946, train acc: 0.3882, val loss: 1.3906, val acc: 0.3919  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 30480] train loss: 1.3969, train acc: 0.3856, val loss: 1.3893, val acc: 0.3848  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 30500] train loss: 1.3953, train acc: 0.3941, val loss: 1.3894, val acc: 0.3906  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 30520] train loss: 1.3926, train acc: 0.3944, val loss: 1.3884, val acc: 0.3875  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 30540] train loss: 1.3938, train acc: 0.3915, val loss: 1.3891, val acc: 0.3953  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 30560] train loss: 1.3950, train acc: 0.3920, val loss: 1.3891, val acc: 0.3963  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 30580] train loss: 1.3952, train acc: 0.3875, val loss: 1.3882, val acc: 0.3865  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 30600] train loss: 1.3979, train acc: 0.3606, val loss: 1.3916, val acc: 0.3761  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 30620] train loss: 1.3942, train acc: 0.3907, val loss: 1.3908, val acc: 0.4078  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 30640] train loss: 1.3937, train acc: 0.3922, val loss: 1.3909, val acc: 0.3774  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 30660] train loss: 1.3975, train acc: 0.3917, val loss: 1.3897, val acc: 0.4044  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 30680] train loss: 1.3973, train acc: 0.3583, val loss: 1.3915, val acc: 0.3848  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 30700] train loss: 1.3944, train acc: 0.3946, val loss: 1.3898, val acc: 0.3980  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 30720] train loss: 1.4001, train acc: 0.3837, val loss: 1.3899, val acc: 0.3963  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 30740] train loss: 1.3952, train acc: 0.3870, val loss: 1.3886, val acc: 0.4000  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 30760] train loss: 1.3960, train acc: 0.3916, val loss: 1.3887, val acc: 0.3956  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 30780] train loss: 1.3947, train acc: 0.3893, val loss: 1.3886, val acc: 0.3953  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 30800] train loss: 1.3978, train acc: 0.3858, val loss: 1.3900, val acc: 0.3922  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 30820] train loss: 1.3957, train acc: 0.3955, val loss: 1.3893, val acc: 0.3902  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 30840] train loss: 1.3977, train acc: 0.3849, val loss: 1.3884, val acc: 0.3889  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 30860] train loss: 1.3982, train acc: 0.3846, val loss: 1.3909, val acc: 0.4067  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 30880] train loss: 1.3920, train acc: 0.3952, val loss: 1.3883, val acc: 0.3909  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 30900] train loss: 1.3941, train acc: 0.3903, val loss: 1.3885, val acc: 0.3889  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 30920] train loss: 1.3953, train acc: 0.3868, val loss: 1.3915, val acc: 0.3936  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 30940] train loss: 1.3946, train acc: 0.3899, val loss: 1.3887, val acc: 0.4034  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 30960] train loss: 1.3911, train acc: 0.3900, val loss: 1.3885, val acc: 0.3970  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 30980] train loss: 1.3943, train acc: 0.3892, val loss: 1.3880, val acc: 0.3973  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 31000] train loss: 1.3922, train acc: 0.3949, val loss: 1.3886, val acc: 0.4027  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 31020] train loss: 1.3946, train acc: 0.3897, val loss: 1.3879, val acc: 0.3848  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 31040] train loss: 1.3930, train acc: 0.3835, val loss: 1.3893, val acc: 0.4020  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 31060] train loss: 1.3953, train acc: 0.3909, val loss: 1.3906, val acc: 0.3875  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 31080] train loss: 1.3939, train acc: 0.3861, val loss: 1.3878, val acc: 0.3804  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 31100] train loss: 1.3928, train acc: 0.3888, val loss: 1.3885, val acc: 0.4024  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 31120] train loss: 1.3927, train acc: 0.3870, val loss: 1.3888, val acc: 0.4024  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 31140] train loss: 1.3983, train acc: 0.3907, val loss: 1.3906, val acc: 0.4121  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 31160] train loss: 1.3938, train acc: 0.3947, val loss: 1.3890, val acc: 0.4000  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 31180] train loss: 1.3984, train acc: 0.3657, val loss: 1.3897, val acc: 0.3875  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 31200] train loss: 1.3953, train acc: 0.3830, val loss: 1.3993, val acc: 0.4105  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 31220] train loss: 1.3945, train acc: 0.3832, val loss: 1.3902, val acc: 0.3963  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 31240] train loss: 1.3962, train acc: 0.3901, val loss: 1.3880, val acc: 0.3855  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 31260] train loss: 1.3961, train acc: 0.3918, val loss: 1.3879, val acc: 0.3916  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 31280] train loss: 1.3932, train acc: 0.3879, val loss: 1.3873, val acc: 0.3926  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 31300] train loss: 1.3950, train acc: 0.3957, val loss: 1.3897, val acc: 0.4024  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 31320] train loss: 1.3912, train acc: 0.3948, val loss: 1.3887, val acc: 0.4105  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 31340] train loss: 1.3931, train acc: 0.3967, val loss: 1.3890, val acc: 0.3895  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 31360] train loss: 1.3966, train acc: 0.3954, val loss: 1.3877, val acc: 0.3868  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 31380] train loss: 1.3941, train acc: 0.3982, val loss: 1.3876, val acc: 0.3912  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 31400] train loss: 1.3967, train acc: 0.3797, val loss: 1.3900, val acc: 0.3882  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 31420] train loss: 1.3949, train acc: 0.3936, val loss: 1.3885, val acc: 0.3970  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 31440] train loss: 1.3946, train acc: 0.3882, val loss: 1.3896, val acc: 0.3929  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 31460] train loss: 1.3971, train acc: 0.3748, val loss: 1.3890, val acc: 0.4010  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 31480] train loss: 1.3946, train acc: 0.3918, val loss: 1.3872, val acc: 0.3929  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 31500] train loss: 1.3959, train acc: 0.3819, val loss: 1.3882, val acc: 0.4037  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 31520] train loss: 1.3963, train acc: 0.3971, val loss: 1.3884, val acc: 0.4007  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 31540] train loss: 1.3931, train acc: 0.3872, val loss: 1.3917, val acc: 0.3599  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 31560] train loss: 1.3911, train acc: 0.3944, val loss: 1.3885, val acc: 0.3875  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 31580] train loss: 1.3939, train acc: 0.3846, val loss: 1.3881, val acc: 0.3936  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 31600] train loss: 1.4048, train acc: 0.3651, val loss: 1.3894, val acc: 0.3855  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 31620] train loss: 1.3939, train acc: 0.3881, val loss: 1.3893, val acc: 0.3754  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 31640] train loss: 1.3973, train acc: 0.3871, val loss: 1.3888, val acc: 0.4027  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 31660] train loss: 1.3964, train acc: 0.3866, val loss: 1.3888, val acc: 0.3909  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 31680] train loss: 1.3935, train acc: 0.3964, val loss: 1.3882, val acc: 0.3909  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 31700] train loss: 1.3973, train acc: 0.3758, val loss: 1.3887, val acc: 0.3855  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 31720] train loss: 1.3938, train acc: 0.3916, val loss: 1.3884, val acc: 0.4037  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 31740] train loss: 1.3955, train acc: 0.3911, val loss: 1.3905, val acc: 0.3909  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 31760] train loss: 1.3940, train acc: 0.3900, val loss: 1.3880, val acc: 0.4054  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 31780] train loss: 1.3966, train acc: 0.3889, val loss: 1.3896, val acc: 0.4094  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 31800] train loss: 1.3942, train acc: 0.3958, val loss: 1.3878, val acc: 0.3895  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 31820] train loss: 1.3966, train acc: 0.3867, val loss: 1.3900, val acc: 0.4067  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 31840] train loss: 1.3928, train acc: 0.3960, val loss: 1.3872, val acc: 0.4047  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 31860] train loss: 1.3962, train acc: 0.3949, val loss: 1.3875, val acc: 0.4081  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 31880] train loss: 1.3953, train acc: 0.3925, val loss: 1.3888, val acc: 0.3922  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 31900] train loss: 1.3934, train acc: 0.3851, val loss: 1.3867, val acc: 0.3983  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 31920] train loss: 1.3931, train acc: 0.4010, val loss: 1.3898, val acc: 0.3595  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 31940] train loss: 1.3948, train acc: 0.3967, val loss: 1.3884, val acc: 0.4064  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 31960] train loss: 1.3964, train acc: 0.3910, val loss: 1.3923, val acc: 0.3639  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 31980] train loss: 1.4178, train acc: 0.3292, val loss: 1.3915, val acc: 0.3858  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 32000] train loss: 1.3929, train acc: 0.3963, val loss: 1.3871, val acc: 0.3953  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 32020] train loss: 1.3972, train acc: 0.3947, val loss: 1.3886, val acc: 0.3970  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 32040] train loss: 1.3958, train acc: 0.3897, val loss: 1.3889, val acc: 0.3993  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 32060] train loss: 1.3947, train acc: 0.3898, val loss: 1.3880, val acc: 0.3892  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 32080] train loss: 1.3929, train acc: 0.3920, val loss: 1.3884, val acc: 0.4148  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 32100] train loss: 1.4022, train acc: 0.3594, val loss: 1.3930, val acc: 0.3750  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 32120] train loss: 1.3947, train acc: 0.3942, val loss: 1.3871, val acc: 0.3946  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 32140] train loss: 1.3968, train acc: 0.3941, val loss: 1.3884, val acc: 0.4142  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 32160] train loss: 1.3934, train acc: 0.3912, val loss: 1.3869, val acc: 0.3922  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 32180] train loss: 1.3941, train acc: 0.3895, val loss: 1.3887, val acc: 0.4182  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 32200] train loss: 1.3971, train acc: 0.3929, val loss: 1.3872, val acc: 0.3990  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 32220] train loss: 1.3934, train acc: 0.3867, val loss: 1.3881, val acc: 0.3852  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 32240] train loss: 1.3961, train acc: 0.3876, val loss: 1.3892, val acc: 0.3987  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 32260] train loss: 1.3936, train acc: 0.3908, val loss: 1.3872, val acc: 0.3912  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 32280] train loss: 1.3935, train acc: 0.3971, val loss: 1.3870, val acc: 0.4047  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 32300] train loss: 1.3943, train acc: 0.3624, val loss: 1.3865, val acc: 0.4030  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 32320] train loss: 1.3926, train acc: 0.3990, val loss: 1.3858, val acc: 0.4081  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 32340] train loss: 1.3935, train acc: 0.3959, val loss: 1.3872, val acc: 0.4010  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 32360] train loss: 1.3912, train acc: 0.4013, val loss: 1.3930, val acc: 0.4189  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 32380] train loss: 1.3935, train acc: 0.4042, val loss: 1.3904, val acc: 0.3757  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 32400] train loss: 1.3936, train acc: 0.3974, val loss: 1.3859, val acc: 0.4142  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 32420] train loss: 1.3950, train acc: 0.3939, val loss: 1.3850, val acc: 0.3997  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 32440] train loss: 1.3928, train acc: 0.4028, val loss: 1.3848, val acc: 0.4098  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 32460] train loss: 1.3921, train acc: 0.3897, val loss: 1.3849, val acc: 0.4084  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 32480] train loss: 1.3918, train acc: 0.3976, val loss: 1.3845, val acc: 0.4125  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 32500] train loss: 1.3887, train acc: 0.3897, val loss: 1.3851, val acc: 0.4024  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 32520] train loss: 1.3924, train acc: 0.3988, val loss: 1.3846, val acc: 0.4226  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 32540] train loss: 1.3926, train acc: 0.3985, val loss: 1.3854, val acc: 0.4159  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 32560] train loss: 1.3902, train acc: 0.4053, val loss: 1.3842, val acc: 0.4246  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 32580] train loss: 1.3958, train acc: 0.3932, val loss: 1.3829, val acc: 0.4152  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 32600] train loss: 1.3909, train acc: 0.3942, val loss: 1.3831, val acc: 0.4044  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 32620] train loss: 1.3944, train acc: 0.3890, val loss: 1.3838, val acc: 0.4051  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 32640] train loss: 1.3935, train acc: 0.4038, val loss: 1.3851, val acc: 0.4047  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 32660] train loss: 1.3885, train acc: 0.4076, val loss: 1.3826, val acc: 0.4108  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 32680] train loss: 1.3900, train acc: 0.4061, val loss: 1.3833, val acc: 0.4061  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 32700] train loss: 1.3920, train acc: 0.3999, val loss: 1.3853, val acc: 0.4138  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 32720] train loss: 1.3919, train acc: 0.3951, val loss: 1.3831, val acc: 0.4115  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 32740] train loss: 1.4146, train acc: 0.3777, val loss: 1.4523, val acc: 0.2931  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 32760] train loss: 1.3983, train acc: 0.4036, val loss: 1.3908, val acc: 0.3912  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 32780] train loss: 1.3941, train acc: 0.3971, val loss: 1.3837, val acc: 0.3953  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 32800] train loss: 1.3920, train acc: 0.4060, val loss: 1.3829, val acc: 0.4074  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 32820] train loss: 1.3895, train acc: 0.4058, val loss: 1.3821, val acc: 0.4128  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 32840] train loss: 1.3878, train acc: 0.4049, val loss: 1.3822, val acc: 0.4185  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 32860] train loss: 1.3913, train acc: 0.4017, val loss: 1.3824, val acc: 0.4101  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 32880] train loss: 1.3938, train acc: 0.3900, val loss: 1.3840, val acc: 0.4239  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 32900] train loss: 1.3927, train acc: 0.4020, val loss: 1.3836, val acc: 0.4132  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 32920] train loss: 1.3924, train acc: 0.3920, val loss: 1.3828, val acc: 0.4199  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 32940] train loss: 1.3901, train acc: 0.4097, val loss: 1.3826, val acc: 0.4071  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 32960] train loss: 1.3901, train acc: 0.4031, val loss: 1.3821, val acc: 0.4132  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 32980] train loss: 1.3919, train acc: 0.4081, val loss: 1.3822, val acc: 0.4094  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 33000] train loss: 1.3940, train acc: 0.3924, val loss: 1.3831, val acc: 0.4078  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 33020] train loss: 1.3929, train acc: 0.3957, val loss: 1.3829, val acc: 0.4199  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 33040] train loss: 1.3941, train acc: 0.4082, val loss: 1.3819, val acc: 0.4115  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 33060] train loss: 1.3918, train acc: 0.4067, val loss: 1.3816, val acc: 0.4226  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 33080] train loss: 1.3872, train acc: 0.4090, val loss: 1.3849, val acc: 0.4239  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 33100] train loss: 1.3912, train acc: 0.4027, val loss: 1.3829, val acc: 0.4320  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 33120] train loss: 1.3879, train acc: 0.4043, val loss: 1.3826, val acc: 0.4280  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 33140] train loss: 1.3872, train acc: 0.4075, val loss: 1.3818, val acc: 0.4101  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 33160] train loss: 1.3915, train acc: 0.4036, val loss: 1.3851, val acc: 0.4260  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 33180] train loss: 1.3926, train acc: 0.3994, val loss: 1.3814, val acc: 0.4067  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 33200] train loss: 1.4068, train acc: 0.3907, val loss: 1.3897, val acc: 0.3916  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 33220] train loss: 1.3952, train acc: 0.3914, val loss: 1.3835, val acc: 0.4260  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 33240] train loss: 1.3945, train acc: 0.3975, val loss: 1.3818, val acc: 0.4162  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 33260] train loss: 1.3888, train acc: 0.4077, val loss: 1.3837, val acc: 0.4007  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 33280] train loss: 1.3909, train acc: 0.3979, val loss: 1.3812, val acc: 0.4165  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 33300] train loss: 1.3916, train acc: 0.4096, val loss: 1.3814, val acc: 0.4067  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 33320] train loss: 1.3893, train acc: 0.3994, val loss: 1.3812, val acc: 0.4145  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 33340] train loss: 1.3922, train acc: 0.4046, val loss: 1.3805, val acc: 0.4118  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 33360] train loss: 1.3903, train acc: 0.4051, val loss: 1.3808, val acc: 0.4297  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 33380] train loss: 1.4028, train acc: 0.3899, val loss: 1.3926, val acc: 0.3953  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 33400] train loss: 1.3892, train acc: 0.4070, val loss: 1.3802, val acc: 0.4189  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 33420] train loss: 1.3894, train acc: 0.3925, val loss: 1.3803, val acc: 0.4246  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 33440] train loss: 1.3908, train acc: 0.4010, val loss: 1.3814, val acc: 0.4155  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 33460] train loss: 1.3877, train acc: 0.3973, val loss: 1.3806, val acc: 0.4182  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 33480] train loss: 1.3923, train acc: 0.3891, val loss: 1.3809, val acc: 0.4199  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 33500] train loss: 1.3898, train acc: 0.4119, val loss: 1.3819, val acc: 0.4047  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 33520] train loss: 1.3874, train acc: 0.4105, val loss: 1.3806, val acc: 0.4159  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 33540] train loss: 1.3875, train acc: 0.4103, val loss: 1.3805, val acc: 0.4273  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 33560] train loss: 1.3915, train acc: 0.4093, val loss: 1.3791, val acc: 0.4169  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 33580] train loss: 1.3919, train acc: 0.4093, val loss: 1.3803, val acc: 0.4212  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 33600] train loss: 1.3881, train acc: 0.4071, val loss: 1.3821, val acc: 0.4175  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 33620] train loss: 1.3906, train acc: 0.4111, val loss: 1.3822, val acc: 0.4128  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 33640] train loss: 1.3885, train acc: 0.4041, val loss: 1.3796, val acc: 0.4172  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 33660] train loss: 1.3860, train acc: 0.4033, val loss: 1.3796, val acc: 0.4165  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 33680] train loss: 1.3872, train acc: 0.4053, val loss: 1.3797, val acc: 0.4206  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 33700] train loss: 1.3914, train acc: 0.4016, val loss: 1.3808, val acc: 0.4216  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 33720] train loss: 1.3894, train acc: 0.4055, val loss: 1.3811, val acc: 0.4216  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 33740] train loss: 1.4010, train acc: 0.4011, val loss: 1.3814, val acc: 0.4105  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 33760] train loss: 1.4129, train acc: 0.3908, val loss: 1.3873, val acc: 0.3987  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 33780] train loss: 1.3878, train acc: 0.4106, val loss: 1.3795, val acc: 0.4135  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 33800] train loss: 1.3876, train acc: 0.4076, val loss: 1.3804, val acc: 0.4108  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 33820] train loss: 1.3914, train acc: 0.4126, val loss: 1.3814, val acc: 0.4061  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 33840] train loss: 1.3902, train acc: 0.3962, val loss: 1.3817, val acc: 0.4142  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 33860] train loss: 1.3901, train acc: 0.4002, val loss: 1.3807, val acc: 0.4179  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 33880] train loss: 1.3880, train acc: 0.4034, val loss: 1.3788, val acc: 0.4206  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 33900] train loss: 1.3893, train acc: 0.4064, val loss: 1.3795, val acc: 0.4300  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 33920] train loss: 1.3906, train acc: 0.4075, val loss: 1.3794, val acc: 0.4115  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 33940] train loss: 1.3895, train acc: 0.3937, val loss: 1.3837, val acc: 0.4256  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 33960] train loss: 1.3880, train acc: 0.4068, val loss: 1.3789, val acc: 0.4145  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 33980] train loss: 1.3905, train acc: 0.4040, val loss: 1.3824, val acc: 0.4013  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 34000] train loss: 1.3930, train acc: 0.3944, val loss: 1.3813, val acc: 0.4064  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 34020] train loss: 1.3896, train acc: 0.4027, val loss: 1.3858, val acc: 0.4317  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 34040] train loss: 1.3872, train acc: 0.4082, val loss: 1.3789, val acc: 0.4169  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 34060] train loss: 1.3868, train acc: 0.4096, val loss: 1.3790, val acc: 0.4162  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 34080] train loss: 1.3960, train acc: 0.4067, val loss: 1.3842, val acc: 0.4081  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 34100] train loss: 1.3895, train acc: 0.4070, val loss: 1.3796, val acc: 0.4098  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 34120] train loss: 1.3908, train acc: 0.4018, val loss: 1.3793, val acc: 0.4263  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 34140] train loss: 1.3850, train acc: 0.4136, val loss: 1.3788, val acc: 0.4236  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 34160] train loss: 1.3882, train acc: 0.4074, val loss: 1.3848, val acc: 0.4223  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 34180] train loss: 1.3873, train acc: 0.4076, val loss: 1.3815, val acc: 0.4088  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 34200] train loss: 1.3891, train acc: 0.4077, val loss: 1.3781, val acc: 0.4165  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 34220] train loss: 1.3917, train acc: 0.4036, val loss: 1.3813, val acc: 0.4206  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 34240] train loss: 1.3896, train acc: 0.4063, val loss: 1.3776, val acc: 0.4138  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 34260] train loss: 1.3867, train acc: 0.4067, val loss: 1.3771, val acc: 0.4256  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 34280] train loss: 1.3886, train acc: 0.4051, val loss: 1.3823, val acc: 0.4175  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 34300] train loss: 1.3888, train acc: 0.4053, val loss: 1.3803, val acc: 0.4202  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 34320] train loss: 1.3904, train acc: 0.4002, val loss: 1.3853, val acc: 0.4179  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 34340] train loss: 1.4056, train acc: 0.3934, val loss: 1.3823, val acc: 0.3966  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 34360] train loss: 1.3867, train acc: 0.4123, val loss: 1.3790, val acc: 0.4341  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 34380] train loss: 1.3919, train acc: 0.3994, val loss: 1.3772, val acc: 0.4223  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 34400] train loss: 1.3854, train acc: 0.4129, val loss: 1.3772, val acc: 0.4192  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 34420] train loss: 1.3901, train acc: 0.4036, val loss: 1.3776, val acc: 0.4179  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 34440] train loss: 1.3882, train acc: 0.4119, val loss: 1.3771, val acc: 0.4233  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 34460] train loss: 1.3874, train acc: 0.4104, val loss: 1.3780, val acc: 0.4233  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 34480] train loss: 1.3872, train acc: 0.4095, val loss: 1.3767, val acc: 0.4253  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 34500] train loss: 1.3864, train acc: 0.4113, val loss: 1.3779, val acc: 0.4341  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 34520] train loss: 1.3905, train acc: 0.4077, val loss: 1.3772, val acc: 0.4226  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 34540] train loss: 1.3918, train acc: 0.3963, val loss: 1.3785, val acc: 0.4121  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 34560] train loss: 1.3867, train acc: 0.4017, val loss: 1.3769, val acc: 0.4206  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 34580] train loss: 1.3856, train acc: 0.4051, val loss: 1.3772, val acc: 0.4256  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 34600] train loss: 1.3854, train acc: 0.4027, val loss: 1.3775, val acc: 0.4226  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 34620] train loss: 1.3934, train acc: 0.4028, val loss: 1.3802, val acc: 0.4152  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 34640] train loss: 1.4219, train acc: 0.3780, val loss: 1.3864, val acc: 0.3892  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 34660] train loss: 1.3924, train acc: 0.4031, val loss: 1.3855, val acc: 0.4189  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 34680] train loss: 1.3894, train acc: 0.4060, val loss: 1.3766, val acc: 0.4179  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 34700] train loss: 1.3861, train acc: 0.4074, val loss: 1.3771, val acc: 0.4243  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 34720] train loss: 1.3858, train acc: 0.4093, val loss: 1.3759, val acc: 0.4219  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 34740] train loss: 1.3862, train acc: 0.4105, val loss: 1.3769, val acc: 0.4125  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 34760] train loss: 1.3878, train acc: 0.4083, val loss: 1.3768, val acc: 0.4310  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 34780] train loss: 1.3891, train acc: 0.4067, val loss: 1.3765, val acc: 0.4304  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 34800] train loss: 1.3867, train acc: 0.4077, val loss: 1.3758, val acc: 0.4300  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 34820] train loss: 1.4214, train acc: 0.3313, val loss: 1.4221, val acc: 0.3578  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 34840] train loss: 1.4073, train acc: 0.3907, val loss: 1.3729, val acc: 0.4229  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 34860] train loss: 1.4343, train acc: 0.3511, val loss: 1.4145, val acc: 0.3467  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 34880] train loss: 1.4012, train acc: 0.3792, val loss: 1.3605, val acc: 0.4067  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 34900] train loss: 1.3998, train acc: 0.3937, val loss: 1.3520, val acc: 0.4260  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 34920] train loss: 1.3928, train acc: 0.4002, val loss: 1.3507, val acc: 0.4364  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 34940] train loss: 1.3854, train acc: 0.4016, val loss: 1.3466, val acc: 0.4310  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 34960] train loss: 1.3878, train acc: 0.4043, val loss: 1.3435, val acc: 0.4341  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 34980] train loss: 1.3826, train acc: 0.4051, val loss: 1.3422, val acc: 0.4307  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 35000] train loss: 1.3821, train acc: 0.4080, val loss: 1.3406, val acc: 0.4337  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 35020] train loss: 1.3833, train acc: 0.4046, val loss: 1.3397, val acc: 0.4341  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 35040] train loss: 1.3802, train acc: 0.4041, val loss: 1.3388, val acc: 0.4310  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 35060] train loss: 1.3769, train acc: 0.4236, val loss: 1.3407, val acc: 0.4297  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 35080] train loss: 1.3734, train acc: 0.4208, val loss: 1.3383, val acc: 0.4243  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 35100] train loss: 1.3749, train acc: 0.4219, val loss: 1.3372, val acc: 0.4263  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 35120] train loss: 1.3721, train acc: 0.4191, val loss: 1.3379, val acc: 0.4246  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 35140] train loss: 1.3713, train acc: 0.4215, val loss: 1.3385, val acc: 0.4253  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 35160] train loss: 1.3689, train acc: 0.4188, val loss: 1.3360, val acc: 0.4253  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 35180] train loss: 1.3799, train acc: 0.4158, val loss: 1.3474, val acc: 0.4233  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 35200] train loss: 1.3763, train acc: 0.4190, val loss: 1.3440, val acc: 0.4226  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 35220] train loss: 1.3716, train acc: 0.4180, val loss: 1.3406, val acc: 0.4253  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 35240] train loss: 1.3695, train acc: 0.4190, val loss: 1.3372, val acc: 0.4253  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 35260] train loss: 1.3650, train acc: 0.4226, val loss: 1.3379, val acc: 0.4239  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 35280] train loss: 1.3642, train acc: 0.4234, val loss: 1.3375, val acc: 0.4280  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 35300] train loss: 1.3696, train acc: 0.4196, val loss: 1.3367, val acc: 0.4297  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 35320] train loss: 1.3636, train acc: 0.4180, val loss: 1.3403, val acc: 0.4256  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 35340] train loss: 1.3659, train acc: 0.4200, val loss: 1.3364, val acc: 0.4243  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 35360] train loss: 1.3654, train acc: 0.4199, val loss: 1.3364, val acc: 0.4192  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 35380] train loss: 1.3614, train acc: 0.4202, val loss: 1.3355, val acc: 0.4256  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 35400] train loss: 1.3623, train acc: 0.4224, val loss: 1.3361, val acc: 0.4277  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 35420] train loss: 1.3621, train acc: 0.4209, val loss: 1.3361, val acc: 0.4253  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 35440] train loss: 1.3639, train acc: 0.4244, val loss: 1.3357, val acc: 0.4256  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 35460] train loss: 1.3637, train acc: 0.4214, val loss: 1.3356, val acc: 0.4219  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 35480] train loss: 1.3595, train acc: 0.4213, val loss: 1.3346, val acc: 0.4212  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 35500] train loss: 1.3615, train acc: 0.4245, val loss: 1.3351, val acc: 0.4219  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 35520] train loss: 1.3608, train acc: 0.4245, val loss: 1.3348, val acc: 0.4229  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 35540] train loss: 1.3604, train acc: 0.4214, val loss: 1.3346, val acc: 0.4280  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 35560] train loss: 1.3587, train acc: 0.4276, val loss: 1.3352, val acc: 0.4216  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 35580] train loss: 1.3618, train acc: 0.4255, val loss: 1.3344, val acc: 0.4226  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 35600] train loss: 1.3614, train acc: 0.4221, val loss: 1.3377, val acc: 0.4334  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 35620] train loss: 1.3573, train acc: 0.4237, val loss: 1.3351, val acc: 0.4317  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 35640] train loss: 1.3608, train acc: 0.4230, val loss: 1.3356, val acc: 0.4290  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 35660] train loss: 1.3577, train acc: 0.4247, val loss: 1.3358, val acc: 0.4256  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 35680] train loss: 1.3569, train acc: 0.4227, val loss: 1.3347, val acc: 0.4253  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 35700] train loss: 1.3566, train acc: 0.4199, val loss: 1.3353, val acc: 0.4239  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 35720] train loss: 1.3586, train acc: 0.4307, val loss: 1.3345, val acc: 0.4239  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 35740] train loss: 1.3572, train acc: 0.4204, val loss: 1.3337, val acc: 0.4266  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 35760] train loss: 1.3586, train acc: 0.4207, val loss: 1.3343, val acc: 0.4327  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 35780] train loss: 1.3569, train acc: 0.4237, val loss: 1.3336, val acc: 0.4320  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 35800] train loss: 1.3583, train acc: 0.4192, val loss: 1.3335, val acc: 0.4256  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 35820] train loss: 1.3564, train acc: 0.4197, val loss: 1.3340, val acc: 0.4246  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 35840] train loss: 1.3585, train acc: 0.4273, val loss: 1.3341, val acc: 0.4263  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 35860] train loss: 1.3526, train acc: 0.4234, val loss: 1.3339, val acc: 0.4256  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 35880] train loss: 1.3539, train acc: 0.4224, val loss: 1.3343, val acc: 0.4314  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 35900] train loss: 1.3578, train acc: 0.4244, val loss: 1.3333, val acc: 0.4283  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 35920] train loss: 1.3558, train acc: 0.4255, val loss: 1.3332, val acc: 0.4283  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 35940] train loss: 1.3564, train acc: 0.4276, val loss: 1.3338, val acc: 0.4253  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 35960] train loss: 1.3566, train acc: 0.4241, val loss: 1.3335, val acc: 0.4273  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 35980] train loss: 1.3543, train acc: 0.4271, val loss: 1.3337, val acc: 0.4239  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 36000] train loss: 1.3550, train acc: 0.4286, val loss: 1.3346, val acc: 0.4206  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 36020] train loss: 1.3592, train acc: 0.4161, val loss: 1.3331, val acc: 0.4273  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 36040] train loss: 1.3591, train acc: 0.4182, val loss: 1.3330, val acc: 0.4310  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 36060] train loss: 1.3570, train acc: 0.4308, val loss: 1.3341, val acc: 0.4307  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 36080] train loss: 1.3530, train acc: 0.4217, val loss: 1.3339, val acc: 0.4304  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 36100] train loss: 1.3562, train acc: 0.4294, val loss: 1.3344, val acc: 0.4300  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 36120] train loss: 1.3573, train acc: 0.4242, val loss: 1.3378, val acc: 0.4145  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 36140] train loss: 1.3583, train acc: 0.4150, val loss: 1.3337, val acc: 0.4246  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 36160] train loss: 1.3572, train acc: 0.4233, val loss: 1.3329, val acc: 0.4310  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 36180] train loss: 1.3552, train acc: 0.4251, val loss: 1.3327, val acc: 0.4304  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 36200] train loss: 1.3575, train acc: 0.4153, val loss: 1.3367, val acc: 0.4327  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 36220] train loss: 1.3553, train acc: 0.4263, val loss: 1.3328, val acc: 0.4229  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 36240] train loss: 1.3531, train acc: 0.4349, val loss: 1.3320, val acc: 0.4266  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 36260] train loss: 1.3531, train acc: 0.4205, val loss: 1.3339, val acc: 0.4256  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 36280] train loss: 1.3540, train acc: 0.4238, val loss: 1.3326, val acc: 0.4280  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 36300] train loss: 1.3556, train acc: 0.4174, val loss: 1.3330, val acc: 0.4266  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 36320] train loss: 1.3550, train acc: 0.4265, val loss: 1.3326, val acc: 0.4290  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 36340] train loss: 1.3577, train acc: 0.4158, val loss: 1.3336, val acc: 0.4320  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 36360] train loss: 1.3550, train acc: 0.4179, val loss: 1.3326, val acc: 0.4300  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 36380] train loss: 1.3567, train acc: 0.4239, val loss: 1.3358, val acc: 0.4297  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 36400] train loss: 1.3561, train acc: 0.4189, val loss: 1.3316, val acc: 0.4327  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 36420] train loss: 1.3554, train acc: 0.4236, val loss: 1.3319, val acc: 0.4351  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 36440] train loss: 1.3545, train acc: 0.4207, val loss: 1.3332, val acc: 0.4202  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 36460] train loss: 1.3527, train acc: 0.4152, val loss: 1.3324, val acc: 0.4283  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 36480] train loss: 1.3579, train acc: 0.4331, val loss: 1.3323, val acc: 0.4229  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 36500] train loss: 1.3547, train acc: 0.4215, val loss: 1.3369, val acc: 0.4327  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 36520] train loss: 1.3520, train acc: 0.4213, val loss: 1.3320, val acc: 0.4223  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 36540] train loss: 1.3530, train acc: 0.4312, val loss: 1.3328, val acc: 0.4304  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 36560] train loss: 1.3525, train acc: 0.4240, val loss: 1.3316, val acc: 0.4331  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 36580] train loss: 1.3553, train acc: 0.4159, val loss: 1.3315, val acc: 0.4246  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 36600] train loss: 1.3534, train acc: 0.4189, val loss: 1.3314, val acc: 0.4324  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 36620] train loss: 1.3510, train acc: 0.4192, val loss: 1.3309, val acc: 0.4250  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 36640] train loss: 1.3549, train acc: 0.4147, val loss: 1.3316, val acc: 0.4293  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 36660] train loss: 1.3542, train acc: 0.4217, val loss: 1.3305, val acc: 0.4270  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 36680] train loss: 1.3593, train acc: 0.4287, val loss: 1.3335, val acc: 0.4202  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 36700] train loss: 1.3544, train acc: 0.4205, val loss: 1.3305, val acc: 0.4337  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 36720] train loss: 1.3502, train acc: 0.4270, val loss: 1.3303, val acc: 0.4290  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 36740] train loss: 1.3525, train acc: 0.4234, val loss: 1.3299, val acc: 0.4239  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 36760] train loss: 1.3543, train acc: 0.4164, val loss: 1.3293, val acc: 0.4347  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 36780] train loss: 1.3569, train acc: 0.4171, val loss: 1.3298, val acc: 0.4334  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 36800] train loss: 1.3545, train acc: 0.4271, val loss: 1.3331, val acc: 0.4159  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 36820] train loss: 1.3521, train acc: 0.4217, val loss: 1.3314, val acc: 0.4273  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 36840] train loss: 1.3532, train acc: 0.4203, val loss: 1.3296, val acc: 0.4250  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 36860] train loss: 1.3645, train acc: 0.4100, val loss: 1.3306, val acc: 0.4358  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 36880] train loss: 1.3530, train acc: 0.4251, val loss: 1.3326, val acc: 0.4128  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 36900] train loss: 1.3573, train acc: 0.4166, val loss: 1.3309, val acc: 0.4314  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 36920] train loss: 1.3527, train acc: 0.4204, val loss: 1.3299, val acc: 0.4334  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 36940] train loss: 1.3546, train acc: 0.4168, val loss: 1.3315, val acc: 0.4317  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 36960] train loss: 1.3524, train acc: 0.4196, val loss: 1.3296, val acc: 0.4341  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 36980] train loss: 1.3521, train acc: 0.4190, val loss: 1.3288, val acc: 0.4327  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 37000] train loss: 1.3542, train acc: 0.4221, val loss: 1.3299, val acc: 0.4290  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 37020] train loss: 1.3497, train acc: 0.4224, val loss: 1.3311, val acc: 0.4270  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 37040] train loss: 1.3553, train acc: 0.4276, val loss: 1.3354, val acc: 0.4159  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 37060] train loss: 1.3532, train acc: 0.4169, val loss: 1.3311, val acc: 0.4280  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 37080] train loss: 1.3523, train acc: 0.4174, val loss: 1.3294, val acc: 0.4320  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 37100] train loss: 1.3528, train acc: 0.4252, val loss: 1.3296, val acc: 0.4256  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 37120] train loss: 1.3548, train acc: 0.4117, val loss: 1.3310, val acc: 0.4277  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 37140] train loss: 1.3524, train acc: 0.4142, val loss: 1.3329, val acc: 0.4263  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 37160] train loss: 1.3522, train acc: 0.4250, val loss: 1.3312, val acc: 0.4223  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 37180] train loss: 1.3503, train acc: 0.4263, val loss: 1.3351, val acc: 0.4115  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 37200] train loss: 1.3523, train acc: 0.4301, val loss: 1.3307, val acc: 0.4263  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 37220] train loss: 1.3477, train acc: 0.4230, val loss: 1.3296, val acc: 0.4239  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 37240] train loss: 1.3504, train acc: 0.4187, val loss: 1.3343, val acc: 0.4317  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 37260] train loss: 1.3518, train acc: 0.4219, val loss: 1.3318, val acc: 0.4344  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 37280] train loss: 1.3509, train acc: 0.4165, val loss: 1.3349, val acc: 0.4351  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 37300] train loss: 1.3512, train acc: 0.4216, val loss: 1.3333, val acc: 0.4229  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 37320] train loss: 1.3510, train acc: 0.4268, val loss: 1.3295, val acc: 0.4209  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 37340] train loss: 1.3486, train acc: 0.4229, val loss: 1.3290, val acc: 0.4239  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 37360] train loss: 1.3499, train acc: 0.4197, val loss: 1.3286, val acc: 0.4293  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 37380] train loss: 1.3530, train acc: 0.4151, val loss: 1.3300, val acc: 0.4202  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 37400] train loss: 1.3506, train acc: 0.4223, val loss: 1.3308, val acc: 0.4273  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 37420] train loss: 1.3530, train acc: 0.4286, val loss: 1.3286, val acc: 0.4304  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 37440] train loss: 1.3508, train acc: 0.4326, val loss: 1.3312, val acc: 0.4324  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 37460] train loss: 1.3521, train acc: 0.4191, val loss: 1.3297, val acc: 0.4314  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 37480] train loss: 1.3501, train acc: 0.4262, val loss: 1.3309, val acc: 0.4283  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 37500] train loss: 1.3509, train acc: 0.4213, val loss: 1.3287, val acc: 0.4341  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 37520] train loss: 1.3505, train acc: 0.4251, val loss: 1.3281, val acc: 0.4293  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 37540] train loss: 1.3528, train acc: 0.4193, val loss: 1.3293, val acc: 0.4327  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 37560] train loss: 1.3479, train acc: 0.4189, val loss: 1.3289, val acc: 0.4304  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 37580] train loss: 1.3548, train acc: 0.4178, val loss: 1.3312, val acc: 0.4317  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 37600] train loss: 1.3552, train acc: 0.4128, val loss: 1.3291, val acc: 0.4216  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 37620] train loss: 1.3527, train acc: 0.4223, val loss: 1.3311, val acc: 0.4105  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 37640] train loss: 1.3496, train acc: 0.4098, val loss: 1.3283, val acc: 0.4314  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 37660] train loss: 1.3566, train acc: 0.4229, val loss: 1.3341, val acc: 0.4314  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 37680] train loss: 1.3494, train acc: 0.4240, val loss: 1.3274, val acc: 0.4256  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 37700] train loss: 1.3492, train acc: 0.4239, val loss: 1.3280, val acc: 0.4347  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 37720] train loss: 1.3493, train acc: 0.4264, val loss: 1.3296, val acc: 0.4347  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 37740] train loss: 1.3545, train acc: 0.4203, val loss: 1.3307, val acc: 0.4324  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 37760] train loss: 1.3548, train acc: 0.4193, val loss: 1.3339, val acc: 0.4310  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 37780] train loss: 1.3514, train acc: 0.4149, val loss: 1.3318, val acc: 0.4304  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 37800] train loss: 1.3562, train acc: 0.4127, val loss: 1.3295, val acc: 0.4331  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 37820] train loss: 1.3500, train acc: 0.4153, val loss: 1.3296, val acc: 0.4277  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 37840] train loss: 1.3480, train acc: 0.4230, val loss: 1.3288, val acc: 0.4273  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 37860] train loss: 1.3499, train acc: 0.4218, val loss: 1.3310, val acc: 0.4260  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 37880] train loss: 1.3570, train acc: 0.4176, val loss: 1.3292, val acc: 0.4317  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 37900] train loss: 1.3528, train acc: 0.4239, val loss: 1.3278, val acc: 0.4219  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 37920] train loss: 1.3497, train acc: 0.4216, val loss: 1.3284, val acc: 0.4354  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 37940] train loss: 1.3563, train acc: 0.4174, val loss: 1.3286, val acc: 0.4297  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 37960] train loss: 1.3542, train acc: 0.4296, val loss: 1.3334, val acc: 0.4334  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 37980] train loss: 1.3526, train acc: 0.4211, val loss: 1.3275, val acc: 0.4310  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 38000] train loss: 1.3550, train acc: 0.4318, val loss: 1.3322, val acc: 0.4361  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 38020] train loss: 1.3495, train acc: 0.4224, val loss: 1.3270, val acc: 0.4280  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 38040] train loss: 1.3502, train acc: 0.4271, val loss: 1.3291, val acc: 0.4341  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 38060] train loss: 1.3518, train acc: 0.4329, val loss: 1.3333, val acc: 0.4297  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 38080] train loss: 1.3486, train acc: 0.4203, val loss: 1.3319, val acc: 0.4101  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 38100] train loss: 1.3652, train acc: 0.4226, val loss: 1.3404, val acc: 0.4047  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 38120] train loss: 1.3524, train acc: 0.4208, val loss: 1.3314, val acc: 0.4277  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 38140] train loss: 1.3538, train acc: 0.4140, val loss: 1.3310, val acc: 0.4354  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 38160] train loss: 1.3507, train acc: 0.4226, val loss: 1.3286, val acc: 0.4320  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 38180] train loss: 1.3498, train acc: 0.4235, val loss: 1.3283, val acc: 0.4351  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 38200] train loss: 1.3515, train acc: 0.4294, val loss: 1.3318, val acc: 0.4189  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 38220] train loss: 1.3506, train acc: 0.4225, val loss: 1.3283, val acc: 0.4317  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 38240] train loss: 1.3487, train acc: 0.4156, val loss: 1.3283, val acc: 0.4293  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 38260] train loss: 1.3529, train acc: 0.4237, val loss: 1.3318, val acc: 0.4347  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 38280] train loss: 1.3531, train acc: 0.4245, val loss: 1.3306, val acc: 0.4317  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 38300] train loss: 1.3496, train acc: 0.4297, val loss: 1.3284, val acc: 0.4256  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 38320] train loss: 1.3514, train acc: 0.4186, val loss: 1.3282, val acc: 0.4297  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 38340] train loss: 1.3489, train acc: 0.4208, val loss: 1.3287, val acc: 0.4277  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 38360] train loss: 1.3514, train acc: 0.4229, val loss: 1.3280, val acc: 0.4307  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 38380] train loss: 1.3495, train acc: 0.4182, val loss: 1.3330, val acc: 0.4287  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 38400] train loss: 1.3513, train acc: 0.4226, val loss: 1.3289, val acc: 0.4270  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 38420] train loss: 1.3495, train acc: 0.4327, val loss: 1.3311, val acc: 0.4111  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 38440] train loss: 1.3493, train acc: 0.4242, val loss: 1.3288, val acc: 0.4307  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 38460] train loss: 1.3514, train acc: 0.4291, val loss: 1.3278, val acc: 0.4266  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 38480] train loss: 1.3488, train acc: 0.4173, val loss: 1.3272, val acc: 0.4304  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 38500] train loss: 1.3498, train acc: 0.4205, val loss: 1.3284, val acc: 0.4341  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 38520] train loss: 1.3487, train acc: 0.4182, val loss: 1.3286, val acc: 0.4287  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 38540] train loss: 1.3492, train acc: 0.4271, val loss: 1.3282, val acc: 0.4256  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 38560] train loss: 1.3483, train acc: 0.4247, val loss: 1.3272, val acc: 0.4223  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 38580] train loss: 1.3475, train acc: 0.4243, val loss: 1.3285, val acc: 0.4334  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 38600] train loss: 1.3455, train acc: 0.4292, val loss: 1.3282, val acc: 0.4243  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 38620] train loss: 1.3498, train acc: 0.4153, val loss: 1.3319, val acc: 0.4223  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 38640] train loss: 1.3514, train acc: 0.4166, val loss: 1.3396, val acc: 0.4084  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 38660] train loss: 1.3504, train acc: 0.4195, val loss: 1.3278, val acc: 0.4256  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 38680] train loss: 1.3472, train acc: 0.4259, val loss: 1.3288, val acc: 0.4115  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 38700] train loss: 1.3495, train acc: 0.4245, val loss: 1.3275, val acc: 0.4351  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 38720] train loss: 1.3528, train acc: 0.4255, val loss: 1.3271, val acc: 0.4310  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 38740] train loss: 1.3491, train acc: 0.4189, val loss: 1.3301, val acc: 0.4344  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 38760] train loss: 1.3524, train acc: 0.4218, val loss: 1.3442, val acc: 0.4128  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 38780] train loss: 1.3511, train acc: 0.4215, val loss: 1.3284, val acc: 0.4300  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 38800] train loss: 1.3501, train acc: 0.4258, val loss: 1.3281, val acc: 0.4229  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 38820] train loss: 1.3480, train acc: 0.4191, val loss: 1.3287, val acc: 0.4287  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 38840] train loss: 1.3447, train acc: 0.4237, val loss: 1.3268, val acc: 0.4364  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 38860] train loss: 1.3484, train acc: 0.4336, val loss: 1.3279, val acc: 0.4307  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 38880] train loss: 1.3478, train acc: 0.4248, val loss: 1.3286, val acc: 0.4226  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 38900] train loss: 1.3502, train acc: 0.4130, val loss: 1.3288, val acc: 0.4280  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 38920] train loss: 1.3475, train acc: 0.4216, val loss: 1.3272, val acc: 0.4243  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 38940] train loss: 1.3500, train acc: 0.4296, val loss: 1.3270, val acc: 0.4260  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 38960] train loss: 1.3482, train acc: 0.4127, val loss: 1.3315, val acc: 0.4233  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 38980] train loss: 1.3494, train acc: 0.4336, val loss: 1.3320, val acc: 0.4283  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 39000] train loss: 1.3486, train acc: 0.4177, val loss: 1.3273, val acc: 0.4297  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 39020] train loss: 1.3464, train acc: 0.4234, val loss: 1.3272, val acc: 0.4236  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 39040] train loss: 1.3475, train acc: 0.4264, val loss: 1.3269, val acc: 0.4287  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 39060] train loss: 1.3543, train acc: 0.4287, val loss: 1.3330, val acc: 0.4081  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 39080] train loss: 1.3487, train acc: 0.4246, val loss: 1.3283, val acc: 0.4307  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 39100] train loss: 1.3475, train acc: 0.4203, val loss: 1.3265, val acc: 0.4233  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 39120] train loss: 1.3464, train acc: 0.4164, val loss: 1.3265, val acc: 0.4243  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 39140] train loss: 1.3532, train acc: 0.4171, val loss: 1.3299, val acc: 0.4256  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 39160] train loss: 1.3452, train acc: 0.4237, val loss: 1.3272, val acc: 0.4297  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 39180] train loss: 1.3500, train acc: 0.4179, val loss: 1.3270, val acc: 0.4358  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 39200] train loss: 1.3464, train acc: 0.4287, val loss: 1.3265, val acc: 0.4341  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 39220] train loss: 1.3467, train acc: 0.4206, val loss: 1.3289, val acc: 0.4277  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 39240] train loss: 1.3500, train acc: 0.4164, val loss: 1.3275, val acc: 0.4243  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 39260] train loss: 1.3529, train acc: 0.4226, val loss: 1.3316, val acc: 0.4094  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 39280] train loss: 1.3517, train acc: 0.4320, val loss: 1.3289, val acc: 0.4297  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 39300] train loss: 1.3497, train acc: 0.4229, val loss: 1.3291, val acc: 0.4253  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 39320] train loss: 1.3473, train acc: 0.4350, val loss: 1.3258, val acc: 0.4263  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 39340] train loss: 1.3497, train acc: 0.4272, val loss: 1.3300, val acc: 0.4081  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 39360] train loss: 1.3536, train acc: 0.4177, val loss: 1.3273, val acc: 0.4280  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 39380] train loss: 1.3485, train acc: 0.4205, val loss: 1.3280, val acc: 0.4287  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 39400] train loss: 1.3491, train acc: 0.4229, val loss: 1.3326, val acc: 0.4270  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 39420] train loss: 1.3502, train acc: 0.4242, val loss: 1.3280, val acc: 0.4341  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 39440] train loss: 1.3475, train acc: 0.4178, val loss: 1.3270, val acc: 0.4253  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 39460] train loss: 1.3496, train acc: 0.4306, val loss: 1.3258, val acc: 0.4300  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 39480] train loss: 1.3484, train acc: 0.4191, val loss: 1.3260, val acc: 0.4283  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 39500] train loss: 1.3506, train acc: 0.4184, val loss: 1.3294, val acc: 0.4300  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 39520] train loss: 1.3490, train acc: 0.4202, val loss: 1.3264, val acc: 0.4341  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 39540] train loss: 1.3494, train acc: 0.4135, val loss: 1.3272, val acc: 0.4307  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 39560] train loss: 1.3470, train acc: 0.4209, val loss: 1.3286, val acc: 0.4233  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 39580] train loss: 1.3594, train acc: 0.4166, val loss: 1.3330, val acc: 0.4223  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 39600] train loss: 1.3490, train acc: 0.4238, val loss: 1.3280, val acc: 0.4287  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 39620] train loss: 1.3557, train acc: 0.4228, val loss: 1.3324, val acc: 0.4196  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 39640] train loss: 1.3483, train acc: 0.4185, val loss: 1.3290, val acc: 0.4351  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 39660] train loss: 1.3498, train acc: 0.4167, val loss: 1.3293, val acc: 0.4236  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 39680] train loss: 1.3502, train acc: 0.4153, val loss: 1.3280, val acc: 0.4310  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 39700] train loss: 1.3515, train acc: 0.4291, val loss: 1.3293, val acc: 0.4277  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 39720] train loss: 1.3460, train acc: 0.4171, val loss: 1.3328, val acc: 0.4125  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 39740] train loss: 1.3519, train acc: 0.4146, val loss: 1.3273, val acc: 0.4371  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 39760] train loss: 1.3481, train acc: 0.4164, val loss: 1.3273, val acc: 0.4347  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 39780] train loss: 1.3456, train acc: 0.4218, val loss: 1.3307, val acc: 0.4270  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 39800] train loss: 1.3498, train acc: 0.4255, val loss: 1.3284, val acc: 0.4273  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 39820] train loss: 1.3453, train acc: 0.4239, val loss: 1.3274, val acc: 0.4290  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 39840] train loss: 1.3523, train acc: 0.4276, val loss: 1.3314, val acc: 0.4212  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 39860] train loss: 1.3511, train acc: 0.4248, val loss: 1.3383, val acc: 0.4192  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 39880] train loss: 1.3488, train acc: 0.4188, val loss: 1.3286, val acc: 0.4290  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 39900] train loss: 1.3459, train acc: 0.4166, val loss: 1.3255, val acc: 0.4310  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 39920] train loss: 1.3450, train acc: 0.4259, val loss: 1.3276, val acc: 0.4304  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 39940] train loss: 1.3501, train acc: 0.4212, val loss: 1.3273, val acc: 0.4317  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 39960] train loss: 1.3481, train acc: 0.4202, val loss: 1.3270, val acc: 0.4253  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 39980] train loss: 1.3497, train acc: 0.4200, val loss: 1.3294, val acc: 0.4206  (best train acc: 0.4779, best val acc: 0.4850)\n",
      "[Epoch: 40000] train loss: 1.3485, train acc: 0.4247, val loss: 1.3272, val acc: 0.4300  (best train acc: 0.4779, best val acc: 0.4850)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtAAAAGpCAYAAACkkgEIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAABjv0lEQVR4nO3dd5gUVdYG8PdOjgxMIg4z5JxkyAiSBVTMomvWdXVVdE2LrhmMq666pnUNq35rDmsAA2JAkCw5SYYhx4GByX2/PzpM567qrtTd7+95eOiurq66U9NTferWuecKKSWIiIiIiEiZBLMbQEREREQUTRhAExERERGpwACaiIiIiEgFBtBERERERCowgCYiIiIiUiHJ7AaolZ+fL0tKSsxuBhERERHFuKVLlx6UUhZ4L4+6ALqkpARLliwxuxlEREREFOOEENv9LWcKBxERERGRCgygiYiIiIhUYABNRERERKQCA2giIiIiIhUYQBMRERERqcAAmoiIiIhIBQbQREREREQqMIAmIiIiIlKBATQRERERkQoMoImIiIiIVGAATURERESkAgNoIiIiIiIVGEATEREREanAAJqIiIiISAUG0EREMchmkzheVWt2M4iIYhIDaCKiGPTIzHXo8eB3qKypB2APqKWUJreKiCg2MIAmIooyVbX1eH/RDo+AeMuBChw5UYP9x6qw+2glXp+7FQDw8FdrcPhEDdreMxNt7p6J0c/8jPJK9kwTEUVCRFuPRGlpqVyyZInZzSAiMs30r9bitblb8eplfZGXlYLzXp6vehvbHp+oQ8uIiGKLEGKplLLUe3mSGY0hIiJPk16Yi1ZNMvDiH04BAPyy8QB2H63ERf1au9aRUkJK4GBFNQDguneWmtJWIqJ4xwCaiMhAx6tq8dS3GzC8UwF+2nAAb8/fjm2PT8SKsnKsKCvHjKkzkJKYgJp6GwDgr5+sMrnFRETkjQE0EZEBpJQ47+Vf8duOowCAt+Zvd71WMnWGx7rO4JmIiKyJATQRkQHG/GMONu2vMLsZRESkAQbQREQ62H+sCgu2HsYP6/YBAINnIqIYwgCaiCgMNXU27DtWhaLcDADAzsMncc9nqzBtUnfkZaWg/6OzTW4hERHphQE0EVEY7v98Nd5fvBPL7x+DxhkpePCLNfhl40Gc9tRPZjeNiIh0xgCaiCgMP/9+AACw62glbBKYvX6/yS0iIiKjMIAmIlJp0/7j2FNeBQCY+Pxck1tDRERG41TeREQKHDlRg5KpM/DST5sw+pk5ZjeHiIhMxACaiEiBXUcrAQBPfrPB5Jao0zY/0+wmEBHFHAbQREQKzFq7z+wmhOXaU9ua3QQiopjDAJqIKAQpJZ6bvdHsZni4oG8rReulJDWc5v91WV+9mkNEFFcYQBMRBbCnvBI/bdiP8c/9YnZTfPz9gl4ezy8d2Nrvep2bZePJ83ti5YNjMa5bMxRkp2J4xwIjmkhEFLNYhYOIyE11XT1SEhOwsqwck16cZ3Zz/LrvjK4AgHP7tMSny3YBAO4c1xnTz+6BT38rw20frkD3lo3QuVkjdG6Wje4tc1zvzc9KRXIi+06IiCLBAJqIyKGqth6d7/sGhdmp2H+82uzmeFj38OnoM+07VNXacM3QNgCAJ8/vic+W74KUDeslOYLj4rxMPOXVSw0AAgAgfZYTEZFyDKCJiBymvLcMACwXPANAekoiZk45FUu3H3EtS0pMQFZqEo5X1bmWDWybCwC4anCJ3+0IoWsziYjiAgNoIiLYBwp+Z/FKG20LstC2IMtzoVdncmF2GrY9PjHodiQ7oImIIsJEOCIiAP/5dZvh+3zs3B5499oBIde7akhJyHWU9iwLwQQOIqJIMYAmorhWb5P4Yf0+PPTlWl33c2FpQ9m5vMwUAMD47s0wuH1+yPemJydq1g4B5nAQEUWKKRxEFNeu/7+lEU+S0rRRKvYdC5433a1FDn64vR02HziBMV2berz25pX9UFlbjw6FWfhs2S689NNmj9eD9RgPbp+Hb9fsQ4qKyhqSORxERBFhAE1Eurj0tYWYu+kg2hZk4ofbTzO7OQFpMcPgwntGo2TqDADAtLO741BFNZ793nPiFZuU/nOYAYzoXOh63LNVYwBA1+aNcOnAYvy4YT+udVTd8Oe5yX1QdqQSaQp7qZnCQUQUOQbQRKSLuZsOAgC2HDhhckt8ffpbGR78Yg3untBFs20u/tto2KRE00ZpKK+sxepd5Xjs3J54fvZGvLNgO+ptysLW1rkZAIBz+rTEJQNa45IB/idIcUpLTkT7Qt+gPBABDiIkIooUA2gi0lx5Za3ZTQioqrYet324AgBw96erwt5ORkoiTtbUu54XZKe6HuekJ+O1K/oBAAody9NTlPUQd23RCHPuHIGi3PSw2xYU69gREUWMATQRaW7wY7PNboJfu49WYvDjP2iyLSntvcT9SnKDrven4e2QlZaEyf2C9yS7a52XEWnzgmIHNBFRZBhAE5Fmautt6PC3r81uhl+7jlZiiEbBs9M/Luodcp2UpARcNSRwDrPR7CkcDKGJiCLBAJqINFNZWx96JYPV1Nlw+RsLsWDLYU23+/h5PTTdnlGYwUFEFDnWgSaiiK3dfQzvLdoBaTO7JQ2klDh6sgYd7/1as+C5R8sc1+NJvVtqsk2jMX4mIooce6CJKGITnv8FQGSD8rT20Jdrw5pd8OU/nIIvV+7GzFV7fV47o2dzrNpVjl6tcvy8M3owg4OIKDIMoIkoIlbMp62tt4U9Nff4Hs0xvkdzV11nADitUwGmTeqO5jlpaJKZgvNPaRVkC9YmhIDkMEIiioCUEm/M24YLS1shOy1Z9fsPVlQjQQjkOmZljUZM4SCiiLw5b5vZTfCxYe/xsN5XHKD6xX+u6o+i3AwkJSbgwtIiJCREbyJE9LaciKzi+3X7Me2rtXjoy7WQUvp0pByvqkVFdR0AYOfhk/j0tzIAwN7yKnyweAdKp3+PU6bNgk1hfXwrYg80EYXll40H0LNlYzz81Vqzm+Lj933hBdAfXz9Y45ZYkwVvGhBRGJyBq9BpdPCxqlrsP1aNLQcqUFVnw1m9WgAA/vj2EgDA1oMn0ObumZjQoxluHtkBCUKgKDcdPR78DgCw7L4xGPOPn1FVa0Of1k3wx7eXYNP+Ctf2294zEy9c0gePzliHo5W1aNk4HZ/fNATVtTakpyQiLTkRR0/WIC05UfFsq0ZhAE1EIb23aAeKczMwuH0+AKDsyElc9voiRe9NSzb+Rtec3w+E9T73yVCcBrQJXuc52gjBAJooVlzz1hL8sH4/tj0+EQBworoO/R/5Hq9f2Q8D2+Zh//Eq5Gemol5KlFfWIj/Lfo5buOUQ3pq/DS9cfArW7z2O5ESBDk2zAQCrysoxe/0+3Dq6Iy54eT42uHVITOjeDP/8YZPr+dLtRwAAM1ft9TtupM+0Wa7HI576ye/PcNO7y1yPN+6vwLsLd2D6jHUAgBX3j0Xvh2ehR8scfHnz0HAOkW4YQBNRSM7Bgc6T9L3/W634vVmpxp9m/rd8d8TbuP+MrpbsXY+UAHOgiaLNtoMnUJSbgURH+lhNnQ37jlXhh/X7XevsP16F/o/YJ7Ga/OoCnN+3FT5eWoY7x3XCtoMn8NHSMmyYfjpSkxJx0asLAAAV59W5BoFfMqA1Hj2nB858YS4A4IyeLTyCZwC4/aMV+FyD82swzuAZAC56dT4AYNWucl33GQ4G0HHivwu3o0NhNvrHWG8a6etYVS2SExp6kHcePolTn/xR1TaM7u08WFGtyXa6NG+kyXYsh0nQRFHh0ZnrcFavFshMTcKIp37CZQOL0b9NLs7s1QK3frDMo8f34S/X4o15Wz3e//FSe97xrLX7sHznUQBAXb3EnN/3udbZ6BYgv7twByb3K3I9f/q7DT5t0jt49rY+zPEsRmAAHSf+9pm9x9DZg6jUieo6dHvgW1w7tA3uPaOrHk0ji3nim/V4+afNSBCA9/gOtcGznk7W1CEjxfMUVl5Zi9Lp3yt6v/Nvwb3aRrxgCgeRtW3afxyvztmCV+dsweguhQCAdxZsxzsLtuPm95b5rO8dPLtzBs8AUFVb78pfBuDRgw0AZ70wz/X469W+KRnUgAF0HDgUQY9ctwe+BQC8NncrA+gYt2zHEew6WomXf9oMwDd4DpcesdqKnUcx6cV5ePzcHpiz8YDf3LtgbhvT0fX4+Yv7YIqfL6RYJaDP74SIwne8qhZLtx/BaZ0Kse9YlavTC7BXvNDK+Od+8Xj+4o+bNdt2vGEAHQeufHOx2U2gKHDOS7/qsl01daIXbDmEtvmZyMtKxTOzNuDT33Zh/t2jIKXEuj3HcclrC1Bvkzi9WzMAwFSFE7dM6NHMFWT3b5OLPwxo7XrtjB7NMXPlHtx/ZnxcIHIQIZH5pJSorK133UW79f3lmL1+P769dRjGPTtHt/3uP65NihsxgI4LVky+J3LnnUaRnChQWy/9vgYAHzly+5QY370ZXvpDX2w9eAKN0pKQl+VZaSMhQeCVy/qG0eooxgCayFRt7p4JAPjHRb1wTp9WmO1IpdAzeCZtcSKVGDfx+V9Cr0RxqbquHou3HUa9TeLsF+eFfkOYwonVnMFzpB44sytevtQeHLdx9GzHO1bhILKOv3ywAlsOVIRekSyHPdAxbs3uY2Y3gSzq4S/X4r8Ld2i+3aaNUrHvWDUGtc3D/C2HTE0XGOtI9YhErIWa8ZbC8cP6faissWFiz+ZmN4XIr5FP/2x2E6LCgePVfmv1m4UBNFGcWrtHn4urGVNOxdrdx9C/TS7u+GhFyElNvluj30jvlo3Tddt2tNJpwjJL+nH9flz9H3vFgYk91VUgIiJr2VteZakAmikcRHFKr7sT+VmpGNaxAGnJia5Zr4K57p2lurRDK7EYb0ZLB/ShimqUTJ2B/1uwXdVgVACw2SSu+g8HUBPFilqbzewmeGAPdIya/Op8LNhyOOz3V9XW69ZDSdZQU2fMyShY2LP14AlD2hCJaAk2lRIQqoNRPdls9imGm2SmAACOnqxB74dnISUpwfUZvfd/q5GUIDC+e3Os2VOOZTuOYtfRStw6qgPqbBJn/nMuLh1YjDN6NndNR7xs5xHTfiYi0l5SgrW6MxhAx6hIgmcA+OsnKw2fcYiMcbKmDoMf/8GQfYkQRYd/3XxQl/1+f9swtIgwfSNWUx2EMOeiYMm2wyjJz3TdlVi6/TBa52bi9blb8crPm3HdsLY4o2dzj6mK3c3ZeMCnbOG7bjn8z83eiOdmb8Q1Q9ugJD8T96mYbp6IrC+RATRZ3Wu/bGHwHMOufWsJjp6sNWRfIkQCxFu/btNlvznpKT6zFKploU7amHD+K/PROjcD71zTH4kJAue9PB9pyQmoqrUHys5Z1x4IUI9b6WQ5r88NPCMbkVW432EhZVISrZV1bK3WkCVMn7HO7CaQjjZHWDLpysElqtYPFof+vi/8tiy5d7Tr8eK/jfZ4LTuNfQPB6HlxMOf3Axj6xA+oqq0HYJ81cs1uey36HYdPYvjff8LQJ+xTwjuDZ3cPfblWv8YRWURpcROzmxB1six2XmcATYpYLfeIwhdpjeVAPYT+2Eumqd9fniMf1tsnNwxC69wMjO3aFPlZqVj0t1H47M+DUZCdiiYZychJT8a2xyciLTlR9T69xW4Kh4g4haOypt6Vv75+7zF8tGQnAKCiug6Xv7EIZUcq8d6iHSiZOgOTXpyHic/PjXCPRMaqt0nU1fte4D0yYy1u+3C5qm2dqK7DP2dvxOJtDamVUvo/z908sr3qtsaL5jnWqqpkrXCeLKtXUWOzm0AakFLi8ImaiLYhVESWamPQa4a2waTeLdCzVWOs3lWONvmZuOz1hXj03B7o3KwRAGDOXSNc6xdmp6EwOw2AvRdaTdvildojJKVEbb3EVyt347nZG/HBdYNw6pM/oLZeonlOGvaUVwGw5yfe9uEK1/vYk0zRqq7ehvNemY8VO49i2+P28of/XbgdwzsW4N+/2FOEHj2nB9KSE/HN6j24/v9+83j/c5N7Y1Lvlth9tBLzNx/C7R85/i5mwbU9CYl2BVk4dMJzvNLtYzvh9rGdXDOwOtf3NyMrmYsBNClipVH7FL7lO48avk81n5x7JnRxDRTp3jIHAPDpn4coem+SxvlxzhrSozoXarpdS1Dw91x25CQe+3o9WjVOx7/mbHEtH/jYbNdjZ/AMwCN4jjdlR06iVZMMs5tBKkkp8fPvB7Dt4Amc1qkQ//l1G/7jNS5jVVk5XpmzGTNW7kHr3Ibfcef7vsFlA4vxzoLtPtu95f3l2LS/Av/8YVOQfSPo1eyvU0fiUEVDZ8fzF/fBlPeWKf7ZSH8MoGPQwYrqsN/LQDm2bD90Al+v3ovrh7cDAMxau8/Q/aud9c5Ko6yLcjOw7L4xaJyRbHZTNBWqCsc3q/egfWE2Rj/D2dGU+GLFbkx5bxluG9MRlw4sRm6A9COyhqraenzyWxme+Ho97hzXCfd9vsb+QoA7Jme+0JB+tOPwSY/X/AXPToGCZymlK43K+2y34oGxrsctGqd7VBIa27Up+hY3Qf82uXj5p80B9xupPwxorcsMtZH6x0W9zG6CDwbQMejT38oieO8uDVtCZjvv5fk4WFGNTfsr8JcxHfGSjideADi1Q77H82hPqWgSg8GQgP+Lmme+24Dng/SYkSebTcImJZbtsNebfmbW73jxx03YMH28yS0jd4u3HcaJ6jqc1sl+J+np7za40jBcwbOB3lu0E5cMaA1IQLjdNJv1l2HISQ98sZ6WnIhPbhgMAGiSkYwOTbNx1ZsNEwUN71hgr3ITJKgHgDl3jsCwv/8Y8LXWeRlYtascK8vKFf08F5a2wodL7DFHTnoyyis9Kzz1apWDFQq3FUxmhFWV9MBBhDHo0Znrw36vK1eLop6U0nU34uOlZRhiUO1nsrZAFzUMntX5839/Q/u/fY03521zLatmWTLLqKqtx5T3luGCV+bjyjcXY/3eY7DZpOF34bz9tGE/pJRYtO2wx3wNzgmAlLhuWDuM6FSILY9OAAC0zs3AW1f3x7Szu+PdawcAsPdY3zamo897G6UnYcP003H98HaugNypdZ49RaVnK3v63B8GtMYDZ3bFpN4tXOskJzacP97740BcPqgEANC2IBMrHhiLPq0be2yza4tG+OrmoZh2dneP5TOnnOrxvGWAuv3tCjLt+02yXrhqvRaRavU2CZtNoqbOhnfmb9N8+969ihQdFm6NbDIdNc7p09KwfVHkpFsSx9PfbYiKAUp9i5tgWMcC1/PJ/Yp81vH+8u7lCAT08M0aZXWpyRyd7/sGX6xomM/g9Gd/Qdt7ZmLboZNB3qW/Xzcf0myAbUKCwJc3DcVXU4a6lg1un4/P/jwYz07ujTFdmwIA2ubbg9CL+xehcUYKUpMSMXV8Z/QtboKX/nAKAODGEe1c27ju1Hbo1qIRbh/bCVcNaYPnJvfB6d2aAQDundhQhWlQuzx0a9EIU0a2x38dgfud4zp5tDE/KxXdW+bgsoHFHsszUxNd+1xy72jMmzrS5+eb2KM5Prp+MO4c1wnDOxT4vG426/WJk2rt7pmJAW1y0buoscdgH4pvk19dYNi+zurdAp8tY/pPNHBP4Vi6/XDQgU5GWv3QOMzbdBA9Wua4Zsqc1LuFa1In794yAHh/8U6P569dXoq+0793PS/Jz9Tk9rERpJRoc/dM3DyyPW4f2wl19TbNB8aS+Sqq63wGKkaih5+LxD6t7TWmnbN+TuzZHLeP7eSzHgBM6NEc66ed7lH6s3VeBmZ49RA7L7oLs1Px7h8HuCaqEkLgNrdtD26Xj1ZN0lF2pNJnX38Z3RFfrtyNiqo6NMtJw+1jOuGaoW0Djht40RXcW7O0HwPoGLFw62Fs1+HKenz3ZqiortN8uxTdzunTEku3H8HtYzti474K1aXRALgm1yBjuQ/sPO/l+brtp3VuBnYcPolXLj0FX63cg69W7kFRbjq+vmUYKmvqUZCdijfnbcVDX67FzSPbIys1CeO6NfMYyHxWL3sA/fCkbor2mZeVihY5adhdXoWbR7ZHekrk9cDdbTt4Ah8vLcOXKwPP1FpdV4/UJPX7tTl+7H/+sMl1UfP383viglLfnnbyVG+TSBAN6UnzNx8yuUXWUJCdiqX3jkaTjOBjOZTUzb/htPaYv/kQBrbNCzk2ZO5fR+K3HUdw7ku/etw1umV0B9wyuoPHuoGCZzXzDZiFAXSU23+8oYzU3mNVQdZU7w8DWuP2sZ1wy/vLIp54gYx12wfLdd3+Py7q7fH8pw37AagbNHjPp6u0bBIpFvnAzhtHtENiQgLeXbgdBytq8NrlpchISUSX5o1wzVuLcdfpnTGwbZ5r/XHdmqFpozRc3L8IWalJyEq1f/VcNaQNrhrSxrN1Qrhq3wL2+t4F2amK2/b5TUOx9eAJ9G+Ti3qbxAs/bEJbRx5luHYePomDFdU456VfQ64bTiGjgxXVKHXrOXe68+OVOL9vK0hpv11P/rW7ZyZGdi7E61eUos3dM81ujqXkZSn/2wmmd1FjrHxwnOL1T2ndxOPvWInXryjFgePVmNCzORqlWb/6EQPoKNf/kdmhVwrTI+f0iOj9S7YdRlWtDUOZQ224T6MgnWKLYyY7MofNpj7S+/GO09AoLcn1pexvkJK/ut1CCNx3Rng9SmqCZ+f6zvckJggMapsXcefCqU/6r1rgT0IYlWf8Bc9Od328Eh8tLVMdjMS6mjobjlXVutIUfli/n8FzlBvVpanZTVBF1wQrIcTpQogNQohNQoipQdbrJ4SoF0Kcr2d7SLnuLRtF9P7Vu8px/ivzcenrCzVqEZlFyRd3OHcojlcxNchMgx73f/F9xaBizJgyFJ/fOATTz+6OVQ+ORVpyAib0aIaSvAzNerS08NmfB+P/rhkQdJ1IKylW1darWl/ryo0fLbWXCHOWy4tnNpvEa79sQUV1Hf7ywfKgFx5EetOtB1oIkQjgRQBjAJQBWCyE+EJKudbPek8A+FavtsQaKSW+X7cfQ9rnhV45TIPbRdZrfMY/54ZeiWIObzJHBwlg3zH/Ey49NKmh3FSvosYAgPXTrFnb2DlYqlerHLQvVF4GTI1nZv2uy3bVOuelX/HutQMwuH1s39FbvascT3+3Aa9eXopkr0GUP27Yj+kz1mH5zqOYsWoPAGD2OnPL0lH80jOFoz+ATVLKLQAghHgfwCQA3vVbbgbwCYB+OrYlpjhvU53Vq0WINcM3NMZP0rFsy4EKTbZz04j2GNVF2TTWKY4vugyNB22R9gL1kBZkp+I/V0Xnafjzm4YGfT2cvOSq2npsPlCBV3WsbFRRXYf1e44pXv+S1xbivjO64pqhbUKvHCWOVdXika/WYUTnApzevTlu/3AFNuw7jo37KtC1RcOd0CMnanC3Y9zEVyv3uJZf89YSw9tMBOgbQLcE4F5jqAyAx702IURLAOcAGIkgAbQQ4joA1wFA69atNW9otHKvcakF95xIWwRTeoeTW0naufZtbb5Q7hjnv+yRP4Pa5uF2x1TGFH1uGtFe1e87uoR3X6Tzfd9o3I4GdfU2lB2pxGlP/aT6vdO+Wou+xU3QrUUjnx5as23afxxNG6UhO8gAsKMna9D74VmY0KMZpozqgP/M24YPluzEB0s8SxJOeP4XfH/bcGSlJqFZThpufPc37D/u/65JtOrcLBvr9x43uxkUJj0DaH9nLe/I6lkAf5VS1gcbvS+lfBXAqwBQWlrK6EwnH7tNAV6UmxH2dt5dtEOL5lCYthwwfnBeQoLAzaM6hF6RLOn2sb6DAePVwi2HcFEENdSV9D38/dsNEdXsP/vFeUhKEFj90DgkJghFgfTKsqNolJaMkvzIKpIA9vrhzXLSPWaP23esCqOfmQPAc9zEsh1HkJSQgDNf8Ezrm7lqL2auCj4Zzehnfo64rVb2/nUD0fvhWWY3g8KkZwBdBsC9gGUrAN5dpqUA3ncEz/kAJggh6qSU/9OxXRTAr5sOuh63K8jyeE1Nh3Ss9RKQ/qZN6uYaTU/6k15/0GrKD0Yjpaev6rr6iIJnpRZti3yW0Dqb9Ogl//nO01Ccl4n298zE+B7N0aEwCwcrqnH3+C5IT0nEWS/MAwBsfWyC6/f92bIy3P3pKjw3uQ/GOWaaC2T30UrXBDdO08/ujnv/txp9i5tgZOeGdK+SqTNwxaBivDV/e8Q/Z6yaMqoDGoeoz0zWpmcAvRhAByFEGwC7AEwGcIn7ClJKVyKXEOI/AL5i8Gye7Yf9T8Si9stV7ah1ij/en5HLBpWY05A4FNuhsi+lp69N+4+7elDDcdWQErw5b5uidZftOBr2fgIZ/vefXJPkfOmW3peXmeoxxflPGw5ghCPY/csHKwAAf3pnKcZ3b4avV+/FpN4t8NzkPpBSorK2Hu8u3IHDJ2rw0k+bffZ57/9WAwCWbj+Cpds9q4QweA7OXwlIii66BdBSyjohxE2wV9dIBPCGlHKNEOJ6x+uv6LXvWKZnfrFWJ/W5Gw+GXoni2v8taPhybZ6TZmJLiICPluzEnR+vVPWeTY+Mx5yNB9CycQYapSfh09+U1V5frEHvcyD+7hT+43vPKiJ3fbIS1bX1OOZVRvLr1fZ0is+X78bU8Z0x6DHP3maKzOguhfh+3X6zm0Ea0nUiFSnlTAAzvZb5DZyllFfq2ZZY8ZmFJ8j495wteGTmOrObQVFg+oyGz0ltPYc1kLnUBs8AkJSYgJGd1U38MOG5X7BWRdUNPRxQkGLH4DlyX9401CPvO4Jx+WRR1hrCSyH9+xf9Sio5uQ8MUWrX0UoGzxSWm0e2N7sJcW362d1DrxTlvHO+jVZeWYvpX601PXgm4/RolePxXVpTbzOxNaQHBtBRxoiSN2O7qZ9Oc8jj7LGg8JypYz1z8uXMCT6ldWMMbZ8f86UHtcz5vnJwSVjv6//I93ht7lYNW0JW06W57+y9H/xpoOvxjgBjjCh6MYCOIr9sPGDIfib3U1drex17VSyjui78AZyf3zhEw5Yol5vJkehmsEntp52OdQ+e1S3kOtJPzY/qOvY+WtH1w9spWq9vcZOgr699eBy+uMn3/JmY0PAHxhSO2MMA2uKW7TiCPeWVAIDLXl8UcL2rhpRots8mmYGL4Psz/rlfNNs3RWb6V+Gn0TinbabYJ6U9rcH9C56Ca1+YFXolP0qmztC4JRSprs0bYeWDY3GXgsmDLu7fGp/cMDjg6/dM6IyMlCSPWtz/c3RGcE6x2KbrIEKKzIBHv8e+Y8pqKt8yqoOrhNKZvVp4lDFSKyfdN4DmeSA6vLOApaMoOOFIaqiXEglx0AWtxY/44iWnYGLP5pFviAy37fGJPhcxQgCNgsyW6M6793lU50LMXt9QTeO6Yb692L0dnRHuVbP83Zmg6MYeaAtTGjwDnreHIv2+SPT6xgm2vUMVnDQlFnRsGl7vWjjMHtBFdjYb4iKAjtRbV/cPO3iu5cAxS3L/2H98/SCP1/K8UsrOO6Wlx/OhHfJ1axdFFwbQMSIlKQErHxyL3+4bE3GPS5KCaWGdVu9m/nMsUDIVsFbqeF/TEmxSIl4yOCK5ZhvesSDs957/yvzwd0y6EW7dQqUluRjUNs/1/J4JXTzX9fpCbVugvLMhL4vjO2IZUzhiwJ9Pa4fM1IZfZfMc9WXowlXHHpaYYGRH5K4jlcbtjPySkJAScZEDLUyae3Hmqj1YsfOoKfsmu6njO/td7n2+S3DrP0hNbnjy+hWlPu/NTElUvP+0JOXrUvRhD3QMuOt0z5PEbWM64v4zuhqyb96itA4lv4t5U0f6XW5kkHHaUz+5Hi+4e5Rh+yU7Z/Dw+74KVocIIjlR4OkLeoX9/r98sFy7xlBY3DuWgnE//znTmvKzUjCqi29JVzWdDe7r7i2vUv5GigoMoGNQSlKCYbV17/lstSH7odAOVdSEXCfQJDlmpcI24zTepth+yF6T9of18TG1cDgDuJ69qA/O69sqrP19uHhn0IsTfzWDSZ0UBWlngW6wBDvdZTmCbiVpPw+eqbyjijOuxh4G0FFOzR9wMJU14dUPPnwidNBGxohklLfzC+UsTmoSF+Kp5znci8NwhwVICdz1SfCpwXtHUcnIywdZb6KdDdNPx4C2uSHXCzhI1nugvNvTri3sFzeBz6YNK185pE3Q/XvnT1NsYQ50lPrwT4NQWtwECQEusdUGUzWhvlBZOSG2OU70GSry+4hiQaBxHIHKnPUqaoyMZN+/E6vGSrmZKRF1dEzo0Rxvz9e3PGZBdioOHG+o6HT1kDaY2LMZznvZ/yDMVIW5xeH0QCc53tTYTzlXIncMoKPQ+9cNRP82Ia6+Vca7Ikhvi1W/GMiT+xeQWs5fsZHXSUPbsxyUGfj37GnVrnKP523yM3Hr6A4Y1C7P7/qhZuy02riQXq1y8OOG8GexNeLj4n3e6VWUg77Fwb/jlPTuBlqnZZP0gOs1zkjBtEndMNJP/rN93ZC7DSkOxu7GBaZwRJm2+ZkY2Nb/iT0SzvzZtOTIPhIMisyzaX+F2U1Q5ZFzupvdhLgVb0F0sAtD75euH94Wk3q3DPv2+zVvLQnrfd4uKi3SZDuRtkHva+rXryhFapLn945W9ckDbeeJ83p6PPde67JBJYHHi2jQLqZ2xAYG0FHm3T8OVLSe2pPeP2b9DgCoqg2/9yQ7NQnXDPXMCYs0ICflIimv7DyfG3leL87LNG5n5CGeJlAJ9aN6T+wTaQfyoq2HFawV+o+1dV5GZA3RQGKiQJqfdBWtNEpLCrvShZJPcHKi/7WyvKpzqKusEfnfDnugYwOjG4sKdBJWWrVA7a14LXoZnp3c25j7feSXLYL8C/7a4gt/3w28/2wi+Tuyokh/ml6tcjRphz9698QqrXNuxAVlK7e0EbNqk5O2GEBb1IX/ahg88eVNQwEAfVo31m1/SRpcErdqksHTgomORDBQyKhbiu/M32bIfigwARFXPdBA8CCy3uvWjVUCaK1+Rc0ahV8qUiC8c4PaoNv7LoDz89kzyHaUfGW1L1Q2a6CanzDcX0tpcZPIN0KWwgA6CvRolYPfp4/HR38apPg9+W5TiF4dotQOEPmMZO/9cSA6NcuOaBsUmf8t3x32e406n9/3+RqD9kRBxdEXeKjevo+Xlnk871cSujyaGs9f3MdnmZExelFu+Kkg4QbxLQLkD3tzfu14Hw7h9b8/SgL7zs2U1dsOd3IUNdx/xjj684tpDKCjREpSApJUFCZ1X7dF49A9EK1DnGRDne8HKqjJSfpat+dY2O+Nsw7JuMcczAYfeQXQjTPCK18WKFD3V1tdSQCt1W3+SP62w22D0g6ZQEGwlQfZhXtM3H/n5/RpqVFryEwMoOOAv0Ea3j5YvDPga0pOF1Y+4VFoR07Wmt0EMlDcpXCo6PKNZDBuKFqkyhkp/EloFAbQEexXTdPGdg31Hah8a1r0QN84on14GyFLYQBNAIBdRys12Y53IG2RdELy46PrG1KCnHmg/H3FgeiK4SIX5Oe91k/JOTXBtlqL/jYav9w1Ak0bpQZd71+X9dWtDWooTYHwpvQCzbmad965qy59WHu3W3bfGNfjly/ti/XTTg/ZDqNkpnIKjljAAJo8/PHU0PnSZC1lR0765HEq0bd1w6AW5/dHnlvuvFYimWKcSE/fr9vns0zPi8jczBQU5WZgcv/WQdcryE61RFpVuAPXlQfQ9vW8j7mySVKCv94ks+FclpgQvByfc4BfozT9Alv3CzML/GpJA7wMIg+Bpq9ViicG4134ynzsLq9S/T6PLyDH45GdC/HST5uDjn5XtQ9NtkJai7c7DWp+XCOOTagAsyArNar/dib2bIZPfgt9UR/oZ1SWAaLdEbpuWFtkpCZhcj9lk9e8f91AHFWZ9hZnf3JxgQE0edh7TH0g5o4nCeMdrAivfJ17L4/3V1G05WqScgINdwX+fFo7cxtjAH+f5Kraetzx0Qq/6+txF0aNT24YhKLcDI8L3NSkBFTXhTfDixk1hzNTlIUWzp/RpwqHwU0WQuCygcWK1w9nNmA9U4PIHEzhIA/xNriI/OOg0PhQEqezQQ7/+4/4auUen+XbHp+oS35qTrrnnb1glT78zdA5/+5RAde/KcSAtEhSqMKN+ZS+zRnc+6RwKAj6zTpFhT2I0O1n5Ok1NjCAJg+XDfJ/Fa70RMrzQnTyHfzJ3hKKXfuOVRu6vzl3jcCiexqC4GD5uImOv0X3IDI3M3Cv+CUDgudTD2mXr7SZhms47XiebwIdn9FdmmL1Q+P0bVQI4fboM2iOPQygCQBw80h7L0bbfN/eDzW9ke6rDmqbx5QOA2gxSM81cQFP8nEh7q6PDPh59wVJf8tJT0ZhkBkBtzw6AQvuHoUnzuvhGvzW0Wtiqu9vG+73vaF+NPcppAN5+oJeCkq9Kaf0NNKthb3Kh/fEK0mJ9i2c37eVx/KMlERkOe4QRNupyqMHOupaT/4wgCYADakbkc5I6K5XUWPNtkX6cuUiGhRYvXKpNcp0xSMhREPQFQff41ZNR5p/90ic06clbhrRHgkJAs1y0nBRv4be5L7uUz8j8LTUoe4WZSsYGH5KcRO8enmpglark5ESuKf9o+sH4bnJ9lkanYG0k/M3dtnAYrz3x4Gu5R6z+fn5tY7oVBBuUxXTIoWDYgMDaAIAPDd7IwDrftmQvrx7RPT+HAzvqP8XHZGR1P7JNM9Jxz8u6o07xnXy+7qzp9XfXUF3oQKzlCRjvuaLcn17ugONqXn0nB7oV5Lryjfv2sJ/1R8hRMCKHG0LPC8ohrTPw1MX9FLRYmOxnGfsYQBNmuJVdnTy/p7TOwea12kmi7O/UyN+XD0GYC+/fwxmTDnV9fz0bs181gln6vFLB3rmTWvR8k9uGKx43ZK8DI/n3kGyew1n94v5O8Z2dD2+bUxHeDNighJNfs08/8UEBtCkqTj7Xo4ZvPMQX5y9YfHwWzfqZ9Sj8mPjjBSku6VBPHFeT9x/RldsfWyCa5mSFA1vE3u08Hge6M+/WU7gvG1vhdkN6zpLAQY8JML7qX3BOX1a4oPrBqKdW++yM63w1A75HhVKkhMT0NmRJ969ZSM8fm7PoIMztRJu/jI7l2IPA2jSlPeUrBSd9Aio3Xu1WS7RPEdP1qC2nn+nWjPiIjQnIxlXD23js6+Prh+kaju9FY5PcVb/SFcZmBZkOYJpt2bmu9XX9t7exJ7N0a+kCW4b0xEDvGosn9K6MW4b0xHPXNg74P7+fn4vFOVmBHxdS2HnQGvbDLIATqRCmvK59c+zhu6i5ZrF5tZOo/IyydcvGw+6HsdLIG1EWUajrwl/vvM0V+90v5JcrHpwLHo8+B0Ae1WPhVsP+9SfdvIeLO7sVe3cLBvr9x73Wb+ytj6sNiYnJuCG09rh1A75mLlqD/5vwQ7cOKId+rT2HCCZk56Mj673nwIihMCUUR38vhYt5z7Aaypv9h/EBAbQpIjSARA2t8myeJKIHkZMPFhvi6Jvuzjx2twtIesIRzujzkNG31XxnnAlOy0Z/768FJ2bZSMhQWBQu4ae3LFdm+K7tfsAAH8a3hYpSQn4+PpBOP+V+QAajtH/bhyCzvd947Ovx87tgbs/XRWyTU+c1wM2CTRKT8KUke1xZq8W6NDUnmZRWpyLC/oW6VKdyf3QP3VBLyzbcUTzfbj2Feb7Ug1ILyFjMYCmkNScMJjCEZ2M+O5nAG09lTXh9SySLyv0F4wJUMv5r+M7Y2VZOXq0ysHtY+xVP0pLcvHW1f0x7au1aOqoUe2eQ/zjHae5Ho/qUqho/+5l+G4b61ldJCUpQfPguSGXv+Hon9+3lU/9aCsIdDeAohcDaAJgH6BRUV0X8XYYIllX/za5AV9zHwCkl437fW8Nk7l4vaud33Ts9YxUu4IsLLjHdzrw4R0LMNxrgpZNj4xHghBIcLstVZidhucm98aQ9vkonf49AODi/kW4dGAxJj4/F1cEmMFWb87Pr5Gd/1rsywoXWxQ5BtAEwH4i0uKPuklG4ClnyVzNgsyEdu3QNrrv/6wX5um+D1InXu4YGfFTVtXaQq8UBZIS/Y9PmNS7JQB77vX+49XoV2K/IN/0yHhNJ+BSw5xPL8NfsuNIHgJgvxWmxShypaO7yXhfrNgd8DXnF2CvosY4u3cLPHl+T7/r5Wel6tI2Mkc8ZNV4n9UOn6jxu55ZQWC0Kc7LdAXPgD3gNqsMpnNgnpF712ImQpYNjQ3sgSYA2vVA+2yXSR1RJTkxAc86ptf1hzFGbDGiOoXVvD53iyH7efGSU1CcZ0xptXjl/PQamsJhkW2Q+RhAEwBHAK3xXzVPEkTWFjcpHO69fwadmSb2bG7Ifggw8tsm/N7j+PhbiydM4YgDSv7cJWTQLxZ/37PlJ2vDbxRpRovTcoLCrmV+BcSWePh9egc8evVW8q58fCjWYMIWflZiAwNoAuAIkAP8UQf6Y1+zp1y39pB+Whs0Y1cgGSmsh2oVtnhIgvby+z59qsHESWe+tURpFQ6KDUzhIAD285Da/FZ/PdbMeTae2jzWr6YMNfXuwQUWrNEar+IlfnY/L327Zp9O+yCjuXKgTW2FMhykGnsYQBMAexAmhLobErwStwa1QVCjtGQ0SvMs6m/kr/KaoW0N3BsFc06flmY3QXdKP9vTz+6uaztIe2YMgg03B7qjY0ZGwLg8fNIXUzjigJK/93AGEfpb3Xs6W97WjA5GXgylJvO0YxVn9W5hdhMso11BltlNIJUaqnAwICXj8ZssDgQqjO9OQpsgKtltXzynxR6WcKJo5LyQ33esKuA6pcVNDGoNacU1E6G5zVCEnUmxhwF0HGjZOD3kOlIGr8LhD6/6Y4lxVTiq62JjxrZYEBd/wW4/5PKdRwOuprQSDVlPNHwVuaebREN7KTQG0AQgdA+0v6tnngTix1VDSgAAozoXRrytungZuUaWc7yqTrdt83RovGgatB49LSWlGEBb3MX9iwzZT/DbS/6/GviFETtCXQy1amIvfZeuQQm6gmxOB07mmLlqj9lNIA01pHDw24iMxwDa4i4dWGzIfuw90OpOQlW1vBUfTQa3ywv4WqjfvJaj3bNSWfzHKipr681ugiGcH99FWw+b2xDSlDShDrQemmQkh16JLIcBtMV5V7XQjZSqr+EDTQP878tL8e2tw+ybjbBZpJ0uzRtFvA329MSWtbuPmd0E3bl/Ziuq/adwnNUr8mokPNdRMOO6NfO7/M0r+2HGlFMNbg1pgV1BFmdUAB1OFY5AheHHdG0KABArGGxFCw4IjU/8tdv1KmpsdhMoDNlp9hAmGiYpaRFgMP8IDcaVkDkYQFucUecFKQ3s7aaoFe6gHTMmPCCi2Pb6lf3w9ao9AYNTq+JXbWxgCofFGdUzaAsjhYMxUXQJ9vvS+1PGwhvWFA8pOc5T6KtzNgdcJ1GLw+D2B8bp6o3RsnE6rj01+mY2jYe/u3jAANrijOyB5lVx/FL6uw/3xM8eaGuKp9rHj85cH/C19oXZAV8Lx9AO+Zpuj2ILv2tjAwNoi6utNybwsO8l8F81wx+KBHugrUmTntcoEOoCrmMzDabxdouKzuzJKdKJYh0DaIurNyjykFIGvCrm1XLs0/uWYjRNeBBPomHwVaSU/ISF2WmR78gtSI+nnn2ieMUA2uKM/IJTnQOtICjirXvrCPb7alOQqe+++TGwJP5atJldEwBWlJVrsh2KPUW5noMceXkVG1iFw+ISDbrE0SMHmj3X1hIsiM3UYIbBcPdN5jk/Tga7Bfv4FeVmGNYOij/f3zYcBVmcfTUWMYC2OKOqcEhI1bfxExkhkwJVtTY8OnOd2c0gPzJSYv8rQAigosr/BCoA8JcxHQ1sDcWyr285FeWVtR7L2hdqkF9PlhT7Z88o1zxHg9w8BcLpgS7Itl9V//X0zjq0iGKC4zP1zoLt5raD4tbJmnocDzADIQDkpHMaZdKG0tleOXFVbGAAbVFjuzbFjsMnDeshCmcmwvlbDgEAdh45qX2DSHOBpl4nimVfrdxjdhOIKAZxEKFFGR3q2KtwBClj5yf4WrjlMABg0dbDurWLtJOUEPjPXWmPCDtOiIiIGEBbmpG3eaQMPDI4klawz9M4oQq2JMVL0V9S5M+ntTO7CURxiWfi2MAAmgA4Uzj8/1mfqKnD+r3Hcbyq1us9wcNjniSMJYTAd38Z5rHMvQxiJCUFmf0Re3g3gYgofAygCYAjhSPAa/M22XOd/z1ni9/X+T1sHWlJnuXo7pnQRZPtypBzVVK00XvyHCKiWMYAmgAoG0TI6Zitzd+vzz2tI5JeZOd72WtJseTTPw82uwkUh3gejQ0MoC3K6Fvm2w+dxOETNUHX8U7Z4G1987mnZQjhe2JOcFsQyUnbuReWX4od/FUCp7RuYnYTiChKMYC2MKO/337ZeDDo6wyYraeytt712N8tea1mgnf1QKt+ozb7J+0xfiYiCl/IAFoIcZMQgpfpFFYsxKDbWLmZKR7PNe8xVrm5TfsrtN0/EVGU45282KCkB7oZgMVCiA+FEKcL/uZjUkF2Ki7u3zqs9wb8RPCjYqjJ/YuQmZqEbY9PdC1zT+GIKAc6zK7kel5BWRf/PomIwhYygJZS3gugA4DXAVwJYKMQ4lEhBIuI6srYwENKGfJ2v3csxNjIfO6/g2EdCnxeT9QoSashhYNBFxERkaKvV2kfqbTX8a8OQBMAHwshntSxbXHPyA4im/TsrfQn3F5I0o/7b8Tfr0/rG0bstIwd/FUSEYVPSQ70FCHEUgBPApgHoIeU8gYAfQGcF+K9pwshNgghNgkhpvp5fZIQYqUQYrkQYokQYmiYPwdFqN4WugearOfz5buCvh7qokitSLf21tX9NWkHRY4XQ0RE4UtSsE4+gHOllNvdF0opbUKIMwK9SQiRCOBFAGMAlMGeR/2FlHKt22qzAXwhpZRCiJ4APgTQWe0PYRUlU2cAADY+Mh7JWt07N4hNSiSEzOHwv5i39c2z52iV67G/YFm7Khza3H3gJ4WIiGKBkihvJoDDzidCiGwhxAAAkFKuC/K+/gA2SSm3SClrALwPYJL7ClLKCtnwzZyJGCl69dePV5rdBNWkohQO7+cx8euKau6/shVlR31eT9Qogr6wtAidm2Xj0oHFqt53orrO4zl7Pa2DF75EROFTEkC/DMC9FtUJx7JQWgLY6fa8zLHMgxDiHCHEegAzAFztb0NCiOscKR5LDhw4oGDX5vp0WfDb6koYPUDPpmAQoVr8etaf+zE+WFGt234KG6Xhm1uHoUXjdFXvW7ztiMdzBm3m+8MAe7UdXswQEYVPSQAt3HqJIaW0QVnqh7/Ts09YKKX8TErZGcDZAKb525CU8lUpZamUsrSgwLfSgBVU19WHXkklYwcRytA90AGi+nj9It647zhKps7Ar5uCT0CjJ/ffyIQezX1ed/+djuxcaECLgstMTTS7CXEv2tLLiIisSMmZdItjIGGy498tALYoeF8ZgCK3560A7A60spRyDoB2Qoh8Bdu2nHfmbw+9koXZZOiKDSxj52nBlkMAgBmr9vh9vabOhnkGBteJfnOgG5YV52ca1pZA+nDqZMuI0+teIiJNKAmgrwcwGMAu2IPiAQCuU/C+xQA6CCHaCCFSAEwG8IX7CkKI9s6JWYQQpwBIAXBIefOtY095VeiVLExRHWhjmhI9QlxwPP71evzhtYX4ds1etLtnJlbvKte+CR7N0W8QIcWeeL1zRESkBSUTqeyXUk6WUhZKKZtKKS+RUu5X8L46ADcB+BbAOgAfSinXCCGuF0Jc71jtPACrhRDLYa/YcZHUari/wfRI4TCSvYyduh5oCu6NeVsBAM9+vxH1NqnqLsXDX67FoMdmh1wvVB1o98oqjJcI0K6iSrT78iZWTSWi8IXMZRZCpAG4BkA3AGnO5VJKvwP+3EkpZ8JexcN92Stuj58A8ISK9lrW5v0nNN2e0V9x9olUgq/jXXUjKVF4/B/wfVJqPqGHJSgMRNbtOWZfXcVv1Rl8q+HvCLsvY9hEQMPnICb/JlXo0SrH7CYQURRTksLxDoBmAMYB+Bn2XObjejYqGs3f4pl5snzn0Yi3qUfFgsoa355yZ49UyDrQXs7t0woAcPvYTn5fj/PvZx96dPyFSuFolpPms4yIiIgioySAbi+lvA/ACSnlWwAmAuihb7Oi39kvzjO7CX4t2OqbYm5zBHZqZ61LTrJ/fBqlKSnKEruUHjZdeoDddu6vHV2bN9Jjr0RERHFNSQBd6/j/qBCiO4AcACW6tYh05S/Wszl7oEMEgm/O26Ziq7FPbUBsduppfP6WyJvzc8g7RETG+vGO05h7H0OUdB2+KoRoAuBe2KtoZAG4T9dWkW4Dffzd5ncG0PGeE6mFkqkzUJyXgfHdfWsyK7H5QAVGPf1zWO9VeweB4hsntSEyVhsLlBIl7QTtgRZCJAA4JqU8IqWcI6Vs66jG8S+D2hfVIg2C9YiH/M5uE2YKR7yO5v/59wM4XlUbMPzYfugkXvl5s89yCQkpJeZuPBjw2E1+dYGiNny/dh+qaj3z2Vs18Z0lkBdF5E3NYFYiIvIvaADtmHXwJoPaEnNq6m1mN0ERpSkcgYSK0WIpzt5bXoUr3liEW95f7vNaRXVd8DdL4MMlO3Hp6wvx4ZKd+HhpGZ7+boPHKtW1ocshLt95FNe+vQTTvlrrsTw/K9Xv+rmZKSG3SfGH11ZEROFTkgM9SwhxhxCiSAiR6/yne8tiQG299SJHf1+a9TZnAK3tN2os3iKudAS4Ww5U+LwWKviVAP76ySoAQNmRStzx0Qr884dNAIDaehvOfnEejlV5BuFrdvtOvlJeaR+WsOPwSUUBd4pj6mYGTATE1gUtEZFZlATQVwO4EcAcAEsd/5bo2aho8878bX6X/3P2RmMbooC/oNZVhUNlF3Q8fg8HS1uxqTgg3pvZW17lt/Shd5qGexsOVtTgX3O2BN1PqN/oHWM7IpHTFcaV79ftA8BBpUREkVAyE2EbP//aGtG4aHHf52tcj68cXOJ6HCq4CcbI4FRGmsKhYVuihXtu8f8t2IG/f7se5738a9D3hJMzvvXgSQDApv3HXTW8nVtxTtASiZtGdsDmRydEvB2KHvuOVQNQd8FHRESeQgbQQojL/f0zonHR6J4JXTTblh6Bqb/b+OHWgY537pPnvPjjZuw4fDLo+tLjsWf0Eii2Xr2rHNV19Rj9zBzc9O5vYbWTg8aIiIi0paSMXT+3x2kARgH4DcDburQoyqUkKcmKMU8kdaB9xHFctvXgCWw9qG769kBB8htztwb83CQIgTpHLv2vm30nwVEjFnPSKXy8XiYiCl/IAFpKebP7cyFEDuzTexOisJSb3x5o5XWg//DaAvzx1LZITkzAa3O3KHpflB2hoCL5WdzfW3ak0vX4Ya9qGp7vaXiX8zAv3HJY8T4ZJFEgR07UmN0EIkPNnHIqcjKSzW4GxYhw5mA+CaCD1g2JVuOf+8XsJqjirxdSTR3oeZsOYd4mZT2hDN4C+3z5bkXrvTlvG+4Y2wkAcLKmHjab9FtjmkitFWVHzW4CkaG6tmhkdhMohoQMoIUQX6Kh8ywBQFcAH+rZqGiyfu9xXbZrZMd2Qxk74/YZTfpOm4U/DW+L64a1i2g7X65QFjQHU8eRX6SReE7pGdaxwOwmEFGUU9ID/ZTb4zoA26WUZTq1J6qtfXgcAHvd3XAnUfnzf5di5qq9GN6xIOIu3JaN07HraKXHMv+DCB0BdJgRdKx/DR86UYNHZ67HdcPa4WR16LrLWnverRzivM0HVb8/2rKMiPT29tX9zW4CEUU5JQH0DgB7pJRVACCESBdClEgpt+nasiiUkWI/nN/+ZRhGPPVTWNuYuWqvZu2ZfftwV3Ds5C9NI9ypvOPRbR8uN3yf7uUQDx6vVvVe9/x0/nrJHauzECn37h8HYNbafWY3gyxEScmIjwC4d6fWO5ZRAG3yM12PzRxkmJac6ArqnRZv8x2AFqoKR3ZqOKnysaHeK2Vi437fGQiNdOfHK03dPxFRPBrcLh8PnNnN7GaQhSgJoJOklK7h2o7HKfo1KbbsLq8K6316hd1vztvmsyxkHegQPZehejajrlKJG/Y4UKyK4j/LiIzp2tTsJhBRDFASQB8QQpzlfCKEmARAfSJmjPtbgAlULn1tYdjb1OOOe6Kf33hDGTtt9xULGQPh5rJbSZzGSUR+jevWzOwmEFEMUHJv/noA/xVCvOB4XgaAMxF6GdQuz+9ytZNt6C0pwTeCbpjK23/IG689VQAwe11DD/SJ6joTWxI5sy9oivMyTG4BuWNOPBFR+JRMpLIZwEAhRBYAIaXUp25blCtxy3v2tv9YFXIykpGalGhgi/zzEz/D2ckaKICuCBE4xnI5LPd6zd0e+NbElhBpK14vjLs2Zy1gIopcyBQOIcSjQojGUsoKKeVxIUQTIcR0IxoXTbKCDLTr/+hsnPPirwa2JrBkPzkczhQOf+kdRFqJ14CNrOO+M7pyMg0i0oSSkGm8lPKo84mU8giACbq1KMoUZKfi4v5FIddbu+cY1u05pni7eg28S/TTy6xmKu9wjH7m55C92KSfKweXAACy08ydwpYpA9YSj9cz5/RpaXYTiChGKAmgE4UQqc4nQoh0AKlB1o8r9TaJRD/13/zdJvznDxt9lgVjVMChdx3obYdO4j/ztuqybS2t2V2OkqkzfCafiXY3jmiPbY9PRHqK+SlEZCFxGEHnZrKAFBFpQ0kA/X8AZgshrhFCXA1gFoC39W1W9Kirt/kdmDfZT6/0LxvNL17i7zszVB3oUAJWvxPuj63f/fjfhTsAAD+s329yS4iIiMjKQgbQUsonAUwH0AVANwDTpJRP6N0wK6qqrcfXq/Z4LDtWVYfyylpF7z9eZc00hpB1oMP0zZqGWRW3HLBWNZJgBIDX525FydQZZjeFSD/Wv6YlIrIsRcPGpJTfSCnvAHA/gAIhRFxGFo/NXIcb/vsbFm21z+ZXU2cvX/HdGt/pt3u1amxk0xTzl1utVx3o1bsacr6PVSm7yDDTnN8PuB5P+2qtiS0hMkAcpnAQEWlFSRWOFCHE2UKIDwHsATAKwCu6t8yCyo7Yc2OPOXqc9x2zzzJYWVvvs26vosZ+t7FgyyF0uvdrHD1Z4/d1d3p0EPlN4bAFrwOthaHt83Xbtlacv18iIiKiYAIG0EKIMUKINwBsBXA+gHcAHJZSXiWl/NKoBlqJd/DpDKBtKnpyXv5pM6rrbFi246hm7YqUs/3+BkNqZe4m8/O/lXr5p81mN4FId5Jd0EREYQvWA/0tgHYAhkopL3UEzdE/r3EEvNMf6tVEzg7OILXMrEoPfpqsVwpHo7SG2tiz1u4Lsqa1xFoVDiIiItJWsAC6L4AFAL4XQswSQlwDgHWw3NSHUavZWeHhvv+t9nnNrMDt09/KAABHT4aXpxwFBTaIyAsntiEiCl/AAFpKuUxK+VcpZTsADwLoAyBFCPG1EOI6oxpoRc6AMTPF3sNaWtzE73pL7h2tarvbDzZUqtAr5cHfd+aHS+wB9DOzftdtX5Gmh3yxYjfK/QT4VbX1ivLJQ1FaSYWIiIhIaRWOeVLKmwC0BPAsgEF6NsqqXOXeHMHgxv0VAIARnQv9rp+flYr/XNVP8fbde7Sl1Kd2sncayqqyctfj1CT95vKOZGbFbQdPYMp7y3DLB8t8Xrvo1QXo/fAslEydgcmvzle97U37j6PHA9/i+dnqJrkhIiKi+JUUepUGUkob7LnR3+rTHGtz5Qo7nv9v2S4AwOx1+3DjiPZ+33NaJ//BtT8r3YJZvXiHsWe+MNf1uENhVljbFIHqhUi/D1WrqrNXOdlztMpjuc0msWLnUdfzBVsOo/xkLXIyQk9Z/Z95W7Fq1zF84khfeX2u9WdKJNISU6+IiMKnKoCOd86ZBGvr7eGgMy0hM1Wbw/jfBds12U64JvRorun23INmLfIt3asG7C2vwter9/iss6LsKIZ1LAi5rQe/ZJ1nozFesxbmQBMRhY8BtEKVNQ21njNT7WMpLxnQGj//fgA3nNZOk33sLq8KvVKYJvVugc+X7/ZYtm7PMY/nVp0p0dnD/fu+CteygY/N9ruuv5rc7jYfqEBGCsfCEjF+JiIKn6KkVyFEohCihRCitfOf3g2zkuq6enS5/xvX87RkewCW5OyBTgl+HfLJDYP1a5xC/nr/bnnfM6d4++GT4W07QNdi85y0sLYXTPcHvvU786PTtoPBpwwf9fTPGPTYD1o3i4iIiOKIkpkIbwawD8AsADMc/77SuV2W4kzZcHLe+lQ6AUnfAFU6QtFlJkK3H+XwCc/KE2nJ+g0ijMS4Z+e4HldU12Hqp6sCrvvY1+sBADsOnXTNsEhEviIZ2EtEFO+UpHDcAqCTlPKQ3o2xKpvPF439uXMiFb0G49RpGAA6p+l2zyM+WFHtsU5GsrVSGzbsPY6VZUd9lh8+Ebxs3e/7jmPsP+YgLzMFE3s2x8OTuuvUQlJDj6oyREREZlASQO8EoH95CAuTXvMvNvRAew4mVOv3fcfRsWk2auttfidWCRUoquJoYrBOJyWD7/xuOnQRDr9Kps7weD65XxEeP68nAPvFiXvPsxpj/2F/36ETNXh7/nb0LW6C0V2aulJviIiIiCKhJIDeAuAnIcQMAK4uSynlM7q1ymJ8e6A9lycq6Fl74rwe+OsnnqkHX67YjSmjOmDO7wfw/uKdPu+JcO4Rv4IF0MmJ+qVwTO5X5Hp8orrONfuhu/cX78SNI9qjKDcD57/yq2b7vuX95QCApo1SNdsmERERxS8lAfQOx78Ux7+44x1AO581pHCEjnQv6tcanyzdhUXbDruW/fOHTfj3L1vQp8h/jrSet7w/X77LZ1lSon77c/bSV9bUo9sDgcuIn/rkj7q1Yd+x6tArERFFIJNVfojiQsgAWkr5kBENsTLvVORwUzhevvQU9J3+vceyqlob5m8xPr18xU7frJykhPB6oANOpOLGeQyfna3tdOFEFB4OIdRHTnroiZyIKPoFjJiEEM86/v9SCPGF9z/DWmgB3qPVnc9tjtxoJSkcAJCXlYptj09UvF8tO6C9g9y9xyp91tG6PrLncbM//tfPWzTdB0UPDiG0Fhbh0AcHyxLFh2A90O84/n/KiIZYmU8PtOP/t+ZvAwDU2bxGGWpEz9PwzFW+tZTDnVFRyfeFTofIst7740Bc/O8F+PBPg3Dhv+ab3RxLYLxmLYzziIjCFzBiklIudfz/s3HNsSafHGjH05Vl9jSIkzXBZ7+zAuGqwuE/jBnUNk/zfbrvKdBAzFg1qF0efp8+HilJCZh2dne/VVaIiIgoOimZSKWDEOJjIcRaIcQW5z8jGmcVWWme1xnSqy8tNUmf6hUJGnYRObcUKIx977qBYW9bSXAcL+Fz52bZmDnlVABAiuNzcdnAYjObZBns8LSWOLumJSLSlJLI700ALwOoAzACwNtoSO+IC43SvAaFOL54WjVJBwA01WHKakDjHGi3OtCb9ldot2EoHUQYH9/W39w6DF1bNDK7GZZ09dA2ZjeBiIhIE0oC6HQp5WwAQkq5XUr5IICR+jbL2pyh4IWl9trGmSnh5Q6bZfQzkWXlzLlzhMdz7x55v+IjfjbMKa0b491rB5jdDFW6NM82uwlERESaUBL5VQkhEgBsFELcBGAXgEJ9m2Vt3mXs1E54kpuZomiWQT06bfceq/JZNmVUB1XbaJ2XoWxFt/bHSw+0nnLSk/HGlf1gkxL9SnLNbg5FOf5F6oODM4nig5IA+lYAGQCmAJgGexrHFTq2yfKcPa42FROpuPvtvjE+01j7s1HDVItAaRaz/jIM7QuzNNtPIPyyjkz7wix8f9tws5tBRCG0bJxudhOIyABBUziEEIkALpRSVkgpy6SUV0kpz5NSLjCofZbU0AOtz3TbeggU43domq3qAuDFS07xWRaoc9mzCofiXUSVEZ0K8O2tw3Bun5b45a4Rod8Q16LkjyVe8K6Qpga2td8Vumlke5NbQkRGCDaRSpKUsh5AX8HK8H7ZpFQ8C6HZtPoNTuzZPKx9O1M4mjXSZ8ClUT66fhDSkxsmnGmSkYJOzbLxzEW9UZSrMLUlDNHxKSMipRNrEVF0C9YDvcjx/zIAnwshLhNCnOv8Z0DbLMvZb2OTnHUKCN2RlSCE66Cd2iHf5/WL+7fWpB1GlIsrapKB1OTIyha+e+0A5GWmKFrXWZ/7+uHtItqnWa4aUmJ2EyiAaLn4jxZKqhERUexQEgnkAjgEe+WNMwCc6fg/brmm8pYy4hSOi0qLMLhdHk7tkI+mjVI1aF0g5p3cBYIPIpw2qRuSNPgyf+DMrhFvI5geLXPQLCcNhdnh/55Ki5tgcPt8JCUq+3kzU+293dlp0VXpxalJhrILBTKOM0f3ltEdTW5JbFFUjYiIYkawALpQCHEbgNUAVjn+X+P4P66nVXP1QNtkxJOdXDW0BO/+cSDeuWYAbhxhbO7cUxf00mQ7gb44pKtKiQjYS90mPxNJiQkRDZBrm5+J168oRVKiPhPaOGWk2IPZt69uKB+Xk5EcaHUP08/uDgBIdrTRp7Z4HNDiIoki57woi/Z0Ksvix5woLgSLOBIBZDn+Zbs9dv6LX26DCCPNd0t2C/q6NDd2Ao7eRTmG7Mc9B9o9jr7/jK5455r+AICS/EyseGAsLixthQ8csyK2zc/0u71/XtzH4/ldp3fCqC5NtW+4l8sHlQAAmuWkYcujE3D/GV1x17jOit57QWkrjO5SiGmOQPo/V/cP+Z5RncOvFjmiU0HY79VLz1bGfN6IiIj0Fuy+8B4p5cOGtSSKuMrYSRnx4DznbIYAdK3t66+d7Qv1ndjCGSwnCOG3j9p7Zrqc9GQ8eX4vrN5VHnCbr1zaF32Lm2jXSBXcB1AmJAhVM+ulJiXitSv6uZ4rKXXVrWUO1u4+pq6RDv+6rBQna+rCeq9eOF7AWphyoC0WNSGKL8F6oPltF4DzRCmlREKEt6VTvNIOUpP0SUPQ+pepJt84QTSkczi9FaQH1jXtuOO59zEpyE7F17ecitMs2MsaqSsHlzQ8kRJD29sHEZYE6I0PJCUpAY2Zf0x+cLCbvnh8ieJDsGhtlGGtiDLOWLBeRp4D7d0rp1cvhtadf1cNaeh9tdmCr+ueAy2lvfd1eMfAwa8zP7hTU3sPuRC+g+i6NG/kCqxjpedn2+MT8eBZ3TyWXTG4BIvuGYWOTTkNNmmDPc9ERJELGEBLKQ8b2ZBo4l7GLtIA2lt9FEaDtSEiaPccaCWKcjPw4Z8G4ZmLeqF5ThoeO7eHq5ybu2mTuuOi0iJD8p+NdH7fVgDsnzMhBAqjeLAX++Ksiz2lRETh07dsQYxypiNIDcrYecvP0ue2u3f8eue4Tpptu6bOfwDt3GdCgoBNAkdP1mD7oROKttm/TS4yUpIw/+5ROKdPK9dgS/fjXdgoDU+c3xMpOqS9lBY3wR9Pbehl12MA3Jiu/gP/YseELGouOkZGMOCQ4hN7oomIwscAOgxbD9qDQJtN+x7oj68fjLvHK6vsEAktS+aFqh7iHEQ44qmfsGT7EdSHMa/3Q5O64ZqhbQwLFBMTBP42sSHP+8M/DdJ8H4HKuqn5SH176zB0bpaNpzUqSUixjz3PRESRYwAdhse+Xo+SqTMcOdDabrsoN8N1Cz9a5KT7r2ns7OFyDiI8crIWALD3WJXqfeRnpeK+M7rqXuvZqXmOZ9pEmtv03VpxD5S3PDrB53UlHdCdmmXjm1uHoYnCmQ2NxsIbFC/Yn08UXxhAR8Bmi7wKhz9a92oDwG1jzJt1TASZSEUvBRHMFggADzvqNespM6VhYKT75yjJla4S/dEnS9eRVXQ1qM4+P/JE8YEBdAQ+XbYr7CBnxf1jseahcX5f0yMoz8vSc5rw4EJN5a2HSGY2BIyZKfDiAa0BAB2bes5LdMWgElwxqBjXn9ZO9zZQ/IrC8coR+dPwtmY3gYhiCAPoCIUb6+ZkJCMzNdg8NrEjQQhX3rje3riyFGf1ahEwrSSQUZ0L8ctdI3RqlX/JCfY/P+9BkOkpiXhoUndkxcnng8gIut/RibMLEqJ4xwA6QnqclKvr6jXfprtXLu2r6/adXFU4BLCnXH3eczhGdm6K5x1Tff/jol5oX6hs1vm0lEQUOapfGC3eegLJGphqQEQUPgbQEdLjS0iPAWvuTu/eTNftezMrD/acPq3wkZ/qGb38lKS7ekiJAS0isg5euGmMFyREcYX3iCOUqEO+coZOAfQtozogI0Xf4NxdQx1ow3apyAd/GoTO930DAHjyvJ64oLSVK8hvm5/pqjlNFIvY86wTXpAQxRUG0BHSI4VDrx7bv5hUicPK1SQu7Ffk8fyHO07zeL7ob6NwolqflBoLHxYiChP/rIniAwPoMCQI+zTegD7BrlVPwOf2aRnW+6wSQOdlpuDQiRpVgWthdhqQrV+bAN5KJyIiija6BtBCiNMBPAcgEcBrUsrHvV7/A4C/Op5WALhBSrlCzzZpISkhATX19umr9bjbb5F408fTF4Y3251Vfpwfbj8NG/YdR2pSIj65YTCaZOhfqo7IquLtwi3QzJ9EROHQLYAWQiQCeBHAGABlABYLIb6QUq51W20rgOFSyiNCiPEAXgUwQK82aSUhAYDjrr4e+bKxNvmEmT9OY7cgOScjGf3b5AIA+hY3MatJcSU7jTe5yBpGdWlqdhOIKIbo+e3WH8AmKeUWABBCvA9gEgBXAC2l/NVt/QUAomIO66SEBACOHugYC3b1YGYKhxACax8ep8tgTwrNiAlpiIJxptwZdQqIs459orilZ7mBlgB2uj0vcywL5BoAX/t7QQhxnRBiiRBiyYEDBzRsYnjcT8R6zBoYLU7vFrwcnnTcI/YOoHMzU3Rrkz8ZKUlITTKu+ohSzsleurUwZophInfxcu1v2B29ODmeRGSnZw+0v9OJ34tzIcQI2APoof5el1K+Cnt6B0pLS02/wI/noNndK5cpm5DF+3j9OnWkHs2JOkW5Gfjsz4PRpXnsBtDOnv/OzXQeiUmqxVsONBGRlvQMoMsAuNcIawVgt/dKQoieAF4DMF5KeUjH9mjGfTBKnWMwIQUm3b6pv7p5qO4TxUSTPq1jOxd7fPdmuH54O9wwvJ3ZTaE4x+sFItKSnikciwF0EEK0EUKkAJgM4Av3FYQQrQF8CuAyKeXvOrZFUwlC4M5xnQAA9bb4OS2Heyt0/d7jrsfFeeZMl03mSEpMwNTxnZHDiidkEsPuF8bPVwERQcceaCllnRDiJgDfwl7G7g0p5RohxPWO118BcD+APAAvOYKzOillqV5t0kpigsAHi+3p3SvKyk1uTXTJ5qAyIophTPAjig+61piSUs4EMNNr2Stuj68FcK2ebdBDghDYcfik2c0gIgqbZJcpEVHY9EzhiFlJiexjUIJfz0RktgfO6oaUxASWHCUiTXGWgzBwRisiinYiTpINLhtYjMsGFhu2P3YcEMUH9kCHYXjHQkP2M7KzMfvRS2oSP15EFCfi43qEiBwY4aj078tLcc+EzrrvZ8X9Y/HKpcrqLFvVW1f3x19GdzS7GUTkB3OgiYjCxwBapaLcdCQlJuDeiV103U9ORjJSorwHtzgvE7eM7uB63jY/08TWEBFg4Mx8REQxLLojNBMN7ZBvdhOizkuXnmJ2E4iI9MEOfaK4wgA6TDZOQKhaI9aAJqIYx/59ovjAADpMNsnuBrUSWb2EiIiIYgAD6DAxgFYvgbmXRBTj+M1AFB8YQIcpIyURAHBWrxYmtyR6sAeaiGIWT29EcYUTqYSpfWE2Xru8FIPb55ndlKjBGRyJrIM30YiIwscAOgKjuzY1uwlRhTM4EpmPf4VERJFjCodK7LUJH3OgiYiIKBYwgCbDsAeaiIiIYgEDaDIMBxESma84LwNAw0Bo0gjvThLFFeZAk2E4hTCR+f5+QS+ce0ortC3IMrspRERRiz3QpLt+JU3MbgIROWSlJmEMB0Brj/0DRHGFPdCku3euGYCK6jqzm0FERESkCQbQpLu05ESkJTPfkoiIiGIDUziIiIiIiFRgAE1EREREpAIDaCIiIiIiFRhAExERRSiRZTqJ4goDaCIiogg9fWEvXDm4BP1Kcs1uChEZgFU4iIiIItSicToePKub2c0gIoOwB5qIiIiISAUG0EREREREKjCAJiIiIiJSgQE0EREREZEKDKCJiIiIiFRgAK2SlGa3gIiIiIjMxACaiIiIiEgFBtBERERERCowgCYiIiIiUoEBNBERERGRCgygiYiIiIhUYABNRERERKQCA2giIiIiIhUYQBMRERERqcAAmoiIiIhIBQbQREREREQqMIAmIiIiIlKBATQRERERkQoMoImIiIiIVGAATURERESkAgNoIiIiIiIVGEATEREREanAAJqIiIiISAUG0EREREREKjCAVklCmt0EIiIiIjIRA2giIiIiIhUYQBMRERERqcAAmoiIiIhIBQbQREREREQqMIAmIiIiIlKBATQRERERkQoMoImIiIiIVGAATURERESkAgNoIiIiIiIVGEATEREREanAAJqIiIiISAUG0EREREREKjCAJiIiIiJSgQE0EREREZEKDKCJiIiIiFRgAE1EREREpAIDaCIiIiIiFRhAqySl2S0gIiIiIjMxgCYiIiIiUoEBNBERERGRCgygiYiIiIhUYABNRERERKQCA2giIiIiIhV0DaCFEKcLITYIITYJIab6eb2zEGK+EKJaCHGHnm0hIiIiItJCkl4bFkIkAngRwBgAZQAWCyG+kFKudVvtMIApAM7Wqx1ERERERFrSswe6P4BNUsotUsoaAO8DmOS+gpRyv5RyMYBaHdtBRERERKQZPQPolgB2uj0vcyxTTQhxnRBiiRBiyYEDBzRpHBERERFROPQMoIWfZWHN4yelfFVKWSqlLC0oKIiwWURERERE4dMzgC4DUOT2vBWA3Truj4iIiIhId3oG0IsBdBBCtBFCpACYDOALHfdHRERERKQ73apwSCnrhBA3AfgWQCKAN6SUa4QQ1ztef0UI0QzAEgCNANiEELcC6CqlPKZXu4iIiIiIIqFbAA0AUsqZAGZ6LXvF7fFe2FM7iIiIiIiiAmciJCIiIiJSgQE0EREREZEKDKCJiIiIiFRgAE1EREREpAIDaCIiIiIiFRhAExERERGpwACaiIiIiEgFBtBERERERCowgCYiIiIiUoEBNBERERGRCgygiYiIiIhUYABNRERERKQCA2giIiIiIhUYQBMRERERqZBkdgPI+r66eSjW7z1udjOIiIiILIEBNIXUvWUOurfMMbsZRERERJbAFA4iIiIiIhUYQBMRERERqcAAmoiIiIhIBQbQREREREQqMIAmIiIiIlKBATQRERERkQoMoFWS0uwWEBEREZGZGEATEREREanAAJqIiIiISAUG0EREREREKjCAJiIiIiJSgQE0EREREZEKDKCJiIiIiFRgAE1EREREpAIDaCIiIiIiFRhAq9S+MMvsJhARERGRiZLMbkC0WH7/GAghkJ6SaHZTiIiIiMhEDKAVapyRYnYTiIiIiMgCmMJBRERERKQCA2giIiIiIhUYQBMRERERqcAAmoiIiIhIBQbQREREREQqMIAmIiIiIlKBATQRERERkQoMoImIiIiIVGAATURERESkAgNoIiIiIiIVGEATEREREanAAJqIiIiISAUG0EREREREKjCAJiIiIiJSgQE0EREREZEKQkppdhtUEUIcALDdpN3nAzho0r6jEY+XOjxe6vB4qcPjpQ6Plzo8XurweKlj5vEqllIWeC+MugDaTEKIJVLKUrPbES14vNTh8VKHx0sdHi91eLzU4fFSh8dLHSseL6ZwEBERERGpwACaiIiIiEgFBtDqvGp2A6IMj5c6PF7q8Hipw+OlDo+XOjxe6vB4qWO548UcaCIiIiIiFdgDTURERESkAgNoIiIiIiIVGEArIIQ4XQixQQixSQgx1ez2mEkIsU0IsUoIsVwIscSxLFcIMUsIsdHxfxO39e92HLcNQohxbsv7OrazSQjxvBBCmPHzaE0I8YYQYr8QYrXbMs2OjxAiVQjxgWP5QiFEiaE/oMYCHK8HhRC7HJ+x5UKICW6vxfvxKhJC/CiEWCeEWCOEuMWxnJ8xP4IcL37G/BBCpAkhFgkhVjiO10OO5fx8+RHkePHzFYQQIlEIsUwI8ZXjeXR+vqSU/BfkH4BEAJsBtAWQAmAFgK5mt8vE47ENQL7XsicBTHU8ngrgCcfjro7jlQqgjeM4JjpeWwRgEAAB4GsA483+2TQ6PsMAnAJgtR7HB8CfAbzieDwZwAdm/8w6HK8HAdzhZ10eL6A5gFMcj7MB/O44LvyMqTte/Iz5P14CQJbjcTKAhQAG8vOl+njx8xX8uN0G4F0AXzmeR+Xniz3QofUHsElKuUVKWQPgfQCTTG6T1UwC8Jbj8VsAznZb/r6UslpKuRXAJgD9hRDNATSSUs6X9k/5227viWpSyjkADnst1vL4uG/rYwCjnFfe0SjA8QqEx0vKPVLK3xyPjwNYB6Al+BnzK8jxCiTej5eUUlY4niY7/knw8+VXkOMVSFwfLwAQQrQCMBHAa26Lo/LzxQA6tJYAdro9L0PwE3CskwC+E0IsFUJc51jWVEq5B7B/YQEodCwPdOxaOh57L49VWh4f13uklHUAygHk6dZy89wkhFgp7Ckeztt5PF5uHLcm+8De68XPWAhexwvgZ8wvx+315QD2A5glpeTnK4gAxwvg5yuQZwHcBcDmtiwqP18MoEPzd+USz7X/hkgpTwEwHsCNQohhQdYNdOx4TO3COT7xcOxeBtAOQG8AewA87VjO4+UghMgC8AmAW6WUx4Kt6mdZ3B0zP8eLn7EApJT1UsreAFrB3tvXPcjqPF7+jxc/X34IIc4AsF9KuVTpW/wss8zxYgAdWhmAIrfnrQDsNqktppNS7nb8vx/AZ7CnuOxz3FKB4//9jtUDHbsyx2Pv5bFKy+Pjeo8QIglADpSnQEQFKeU+x5eSDcC/Yf+MATxeAAAhRDLsweB/pZSfOhbzMxaAv+PFz1hoUsqjAH4CcDr4+QrJ/Xjx8xXQEABnCSG2wZ4OO1II8X+I0s8XA+jQFgPoIIRoI4RIgT0p/QuT22QKIUSmECLb+RjAWACrYT8eVzhWuwLA547HXwCY7BgV2wZABwCLHLdojgshBjpyky53e08s0vL4uG/rfAA/OHLAYobzROpwDuyfMYDHC46f73UA66SUz7i9xM+YH4GOFz9j/gkhCoQQjR2P0wGMBrAe/Hz5Feh48fPln5TybillKyllCeyx1A9SyksRrZ8vaYERmVb/B2AC7KO3NwP4m9ntMfE4tIV9ROwKAGucxwL2/KLZADY6/s91e8/fHMdtA9wqbQAohf2kshnAC3DMihnt/wC8B/stu1rYr4Sv0fL4AEgD8BHsgykWAWhr9s+sw/F6B8AqACthPxk25/Fy/ZxDYb8duRLAcse/CfyMqT5e/Iz5P149ASxzHJfVAO53LOfnS93x4ucr9LE7DQ1VOKLy88WpvImIiIiIVGAKBxERERGRCgygiYiIiIhUYABNRERERKQCA2giIiIiIhUYQBMRERERqcAAmogoiggh6oUQy93+TdVw2yVCiNWh1yQiim9JZjeAiIhUqZT2qYOJiMgk7IEmIooBQohtQognhBCLHP/aO5YXCyFmCyFWOv5v7VjeVAjxmRBihePfYMemEoUQ/xZCrBFCfOeYYY2IiNwwgCYiii7pXikcF7m9dkxK2R/2mbmedSx7AcDbUsqeAP4L4HnH8ucB/Cyl7AXgFNhnFwXs0+W+KKXsBuAogPN0/WmIiKIQZyIkIooiQogKKWWWn+XbAIyUUm4RQiQD2CulzBNCHIR9KuFax/I9Usp8IcQBAK2klNVu2ygBMEtK2cHx/K8AkqWU0w340YiIogZ7oImIYocM8DjQOv5Uuz2uB8fKEBH5YABNRBQ7LnL7f77j8a8AJjse/wHAXMfj2QBuAAAhRKIQopFRjSQiinbsWSAiii7pQojlbs+/kVI6S9mlCiEWwt45crFj2RQAbwgh7gRwAMBVjuW3AHhVCHEN7D3NNwDYo3fjiYhiAXOgiYhigCMHulRKedDsthARxTqmcBARERERqcAeaCIiIiIiFdgDTURERESkAgNoIiIiIiIVGEATEREREanAAJqIiIiISAUG0EREREREKvw/iQzVggIsP7cAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 864x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       154\n",
      "           1       0.49      0.65      0.56       703\n",
      "           2       0.34      0.12      0.18       702\n",
      "           3       0.38      0.49      0.42       703\n",
      "           4       0.60      0.75      0.66       702\n",
      "\n",
      "    accuracy                           0.47      2964\n",
      "   macro avg       0.36      0.40      0.36      2964\n",
      "weighted avg       0.43      0.47      0.43      2964\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pmaur\\miniconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\pmaur\\miniconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\pmaur\\miniconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\pmaur\\miniconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\pmaur\\miniconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\pmaur\\miniconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 504x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 504x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "gnn_gat = GNN7L_GAT(data_with_nedbit).to(device)\n",
    "pred = train(gnn_gat, data_with_nedbit.to(device), 40000, cm_title='GAT7L_multiclass_16HC', classes=['P', 'LP', 'WN', 'LN', 'RN'], weight_decay=0.0005, layers=7)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GCN"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f5a94ad06564d2a9dfa6d74e42d4b72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40001 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch: 0000] train loss: 521.6893, train acc: 0.2363, val loss: 497.6612, val acc: 0.2371  (best train acc: 0.2363, best val acc: 0.2371)\n",
      "[Epoch: 0020] train loss: 303.4787, train acc: 0.2441, val loss: 271.1812, val acc: 0.2371  (best train acc: 0.2454, best val acc: 0.2371)\n",
      "[Epoch: 0040] train loss: 187.8087, train acc: 0.2502, val loss: 129.9039, val acc: 0.2368  (best train acc: 0.2838, best val acc: 0.2374)\n",
      "[Epoch: 0060] train loss: 149.6211, train acc: 0.2571, val loss: 97.2607, val acc: 0.2354  (best train acc: 0.2866, best val acc: 0.2374)\n",
      "[Epoch: 0080] train loss: 122.2620, train acc: 0.2716, val loss: 71.9313, val acc: 0.2381  (best train acc: 0.2932, best val acc: 0.2405)\n",
      "[Epoch: 0100] train loss: 96.3673, train acc: 0.2647, val loss: 43.9820, val acc: 0.2388  (best train acc: 0.2932, best val acc: 0.2411)\n",
      "[Epoch: 0120] train loss: 84.3531, train acc: 0.2574, val loss: 29.1298, val acc: 0.2125  (best train acc: 0.3096, best val acc: 0.2411)\n",
      "[Epoch: 0140] train loss: 74.4655, train acc: 0.2445, val loss: 23.5781, val acc: 0.2371  (best train acc: 0.3096, best val acc: 0.2411)\n",
      "[Epoch: 0160] train loss: 74.5443, train acc: 0.2382, val loss: 22.1579, val acc: 0.2381  (best train acc: 0.3096, best val acc: 0.2411)\n",
      "[Epoch: 0180] train loss: 69.1214, train acc: 0.2546, val loss: 21.6345, val acc: 0.2307  (best train acc: 0.3096, best val acc: 0.2411)\n",
      "[Epoch: 0200] train loss: 66.8270, train acc: 0.2565, val loss: 22.7413, val acc: 0.2378  (best train acc: 0.3096, best val acc: 0.2411)\n",
      "[Epoch: 0220] train loss: 61.1072, train acc: 0.2705, val loss: 19.8312, val acc: 0.2374  (best train acc: 0.3096, best val acc: 0.2411)\n",
      "[Epoch: 0240] train loss: 59.8434, train acc: 0.2434, val loss: 18.6985, val acc: 0.2347  (best train acc: 0.3096, best val acc: 0.2411)\n",
      "[Epoch: 0260] train loss: 57.5261, train acc: 0.2447, val loss: 16.2831, val acc: 0.2253  (best train acc: 0.3096, best val acc: 0.2411)\n",
      "[Epoch: 0280] train loss: 52.9120, train acc: 0.2618, val loss: 16.4127, val acc: 0.2320  (best train acc: 0.3096, best val acc: 0.2411)\n",
      "[Epoch: 0300] train loss: 50.6839, train acc: 0.2624, val loss: 15.9777, val acc: 0.2270  (best train acc: 0.3096, best val acc: 0.2411)\n",
      "[Epoch: 0320] train loss: 48.7689, train acc: 0.2616, val loss: 16.5236, val acc: 0.2364  (best train acc: 0.3096, best val acc: 0.2411)\n",
      "[Epoch: 0340] train loss: 45.1207, train acc: 0.2676, val loss: 13.8216, val acc: 0.2148  (best train acc: 0.3096, best val acc: 0.2411)\n",
      "[Epoch: 0360] train loss: 44.3972, train acc: 0.2682, val loss: 12.8026, val acc: 0.2155  (best train acc: 0.3096, best val acc: 0.2411)\n",
      "[Epoch: 0380] train loss: 41.5768, train acc: 0.2613, val loss: 14.2797, val acc: 0.2371  (best train acc: 0.3096, best val acc: 0.2411)\n",
      "[Epoch: 0400] train loss: 37.6327, train acc: 0.2613, val loss: 14.0163, val acc: 0.2344  (best train acc: 0.3096, best val acc: 0.2411)\n",
      "[Epoch: 0420] train loss: 37.0732, train acc: 0.2590, val loss: 11.6450, val acc: 0.2384  (best train acc: 0.3096, best val acc: 0.2411)\n",
      "[Epoch: 0440] train loss: 32.4704, train acc: 0.2493, val loss: 10.4589, val acc: 0.2381  (best train acc: 0.3096, best val acc: 0.2415)\n",
      "[Epoch: 0460] train loss: 31.4749, train acc: 0.2723, val loss: 9.4559, val acc: 0.2358  (best train acc: 0.3096, best val acc: 0.2415)\n",
      "[Epoch: 0480] train loss: 29.4379, train acc: 0.2583, val loss: 9.2828, val acc: 0.2243  (best train acc: 0.3096, best val acc: 0.2415)\n",
      "[Epoch: 0500] train loss: 28.1568, train acc: 0.2462, val loss: 7.7678, val acc: 0.2206  (best train acc: 0.3096, best val acc: 0.2415)\n",
      "[Epoch: 0520] train loss: 25.4896, train acc: 0.2615, val loss: 8.5651, val acc: 0.2351  (best train acc: 0.3096, best val acc: 0.2415)\n",
      "[Epoch: 0540] train loss: 22.6117, train acc: 0.2594, val loss: 8.0106, val acc: 0.2169  (best train acc: 0.3096, best val acc: 0.2415)\n",
      "[Epoch: 0560] train loss: 22.5651, train acc: 0.2432, val loss: 6.8236, val acc: 0.2297  (best train acc: 0.3096, best val acc: 0.2415)\n",
      "[Epoch: 0580] train loss: 19.8102, train acc: 0.2555, val loss: 6.1205, val acc: 0.2331  (best train acc: 0.3096, best val acc: 0.2415)\n",
      "[Epoch: 0600] train loss: 18.4234, train acc: 0.2524, val loss: 6.8843, val acc: 0.2378  (best train acc: 0.3096, best val acc: 0.2415)\n",
      "[Epoch: 0620] train loss: 16.7251, train acc: 0.2678, val loss: 6.0147, val acc: 0.2293  (best train acc: 0.3096, best val acc: 0.2415)\n",
      "[Epoch: 0640] train loss: 15.2681, train acc: 0.2582, val loss: 5.3725, val acc: 0.2388  (best train acc: 0.3096, best val acc: 0.2499)\n",
      "[Epoch: 0660] train loss: 14.1354, train acc: 0.2472, val loss: 5.4554, val acc: 0.2418  (best train acc: 0.3096, best val acc: 0.2553)\n",
      "[Epoch: 0680] train loss: 12.7263, train acc: 0.2767, val loss: 4.8278, val acc: 0.2432  (best train acc: 0.3096, best val acc: 0.2671)\n",
      "[Epoch: 0700] train loss: 10.9462, train acc: 0.2695, val loss: 4.2653, val acc: 0.2550  (best train acc: 0.3096, best val acc: 0.2671)\n",
      "[Epoch: 0720] train loss: 11.2808, train acc: 0.2763, val loss: 4.6651, val acc: 0.2540  (best train acc: 0.3096, best val acc: 0.2671)\n",
      "[Epoch: 0740] train loss: 9.1958, train acc: 0.2714, val loss: 4.1388, val acc: 0.2567  (best train acc: 0.3096, best val acc: 0.2678)\n",
      "[Epoch: 0760] train loss: 8.9693, train acc: 0.2346, val loss: 3.4398, val acc: 0.2691  (best train acc: 0.3096, best val acc: 0.2691)\n",
      "[Epoch: 0780] train loss: 7.6770, train acc: 0.2720, val loss: 3.3326, val acc: 0.2627  (best train acc: 0.3096, best val acc: 0.2782)\n",
      "[Epoch: 0800] train loss: 6.6431, train acc: 0.2766, val loss: 3.1979, val acc: 0.2722  (best train acc: 0.3096, best val acc: 0.2820)\n",
      "[Epoch: 0820] train loss: 5.9901, train acc: 0.2869, val loss: 2.6161, val acc: 0.2968  (best train acc: 0.3096, best val acc: 0.2968)\n",
      "[Epoch: 0840] train loss: 5.2180, train acc: 0.2978, val loss: 2.2843, val acc: 0.3113  (best train acc: 0.3195, best val acc: 0.3207)\n",
      "[Epoch: 0860] train loss: 4.6716, train acc: 0.2889, val loss: 2.3029, val acc: 0.3454  (best train acc: 0.3235, best val acc: 0.3548)\n",
      "[Epoch: 0880] train loss: 4.2056, train acc: 0.3112, val loss: 2.0363, val acc: 0.3386  (best train acc: 0.3321, best val acc: 0.3575)\n",
      "[Epoch: 0900] train loss: 3.4918, train acc: 0.3417, val loss: 1.8883, val acc: 0.3295  (best train acc: 0.3417, best val acc: 0.3653)\n",
      "[Epoch: 0920] train loss: 3.2287, train acc: 0.3376, val loss: 1.8114, val acc: 0.3882  (best train acc: 0.3529, best val acc: 0.3882)\n",
      "[Epoch: 0940] train loss: 2.8381, train acc: 0.3616, val loss: 1.7654, val acc: 0.3754  (best train acc: 0.3730, best val acc: 0.4148)\n",
      "[Epoch: 0960] train loss: 2.3858, train acc: 0.3906, val loss: 1.5657, val acc: 0.4199  (best train acc: 0.3933, best val acc: 0.4253)\n",
      "[Epoch: 0980] train loss: 2.3580, train acc: 0.3778, val loss: 1.5712, val acc: 0.4098  (best train acc: 0.4189, best val acc: 0.4405)\n",
      "[Epoch: 1000] train loss: 2.0420, train acc: 0.4116, val loss: 1.5119, val acc: 0.4358  (best train acc: 0.4189, best val acc: 0.4503)\n",
      "[Epoch: 1020] train loss: 1.8403, train acc: 0.4093, val loss: 1.4660, val acc: 0.4361  (best train acc: 0.4190, best val acc: 0.4503)\n",
      "[Epoch: 1040] train loss: 1.7766, train acc: 0.4083, val loss: 1.4425, val acc: 0.4465  (best train acc: 0.4227, best val acc: 0.4503)\n",
      "[Epoch: 1060] train loss: 1.6306, train acc: 0.4323, val loss: 1.3988, val acc: 0.4546  (best train acc: 0.4323, best val acc: 0.4556)\n",
      "[Epoch: 1080] train loss: 1.5207, train acc: 0.4303, val loss: 1.3710, val acc: 0.4476  (best train acc: 0.4363, best val acc: 0.4556)\n",
      "[Epoch: 1100] train loss: 1.4763, train acc: 0.4218, val loss: 1.3425, val acc: 0.4499  (best train acc: 0.4431, best val acc: 0.4556)\n",
      "[Epoch: 1120] train loss: 1.3985, train acc: 0.4471, val loss: 1.3267, val acc: 0.4486  (best train acc: 0.4471, best val acc: 0.4556)\n",
      "[Epoch: 1140] train loss: 1.3904, train acc: 0.4453, val loss: 1.3196, val acc: 0.4627  (best train acc: 0.4575, best val acc: 0.4627)\n",
      "[Epoch: 1160] train loss: 1.3497, train acc: 0.4589, val loss: 1.3133, val acc: 0.4675  (best train acc: 0.4618, best val acc: 0.4725)\n",
      "[Epoch: 1180] train loss: 1.3474, train acc: 0.4537, val loss: 1.3090, val acc: 0.4691  (best train acc: 0.4636, best val acc: 0.4759)\n",
      "[Epoch: 1200] train loss: 1.3316, train acc: 0.4594, val loss: 1.3057, val acc: 0.4691  (best train acc: 0.4771, best val acc: 0.4759)\n",
      "[Epoch: 1220] train loss: 1.3213, train acc: 0.4715, val loss: 1.3021, val acc: 0.4742  (best train acc: 0.4784, best val acc: 0.4762)\n",
      "[Epoch: 1240] train loss: 1.3206, train acc: 0.4652, val loss: 1.2987, val acc: 0.4789  (best train acc: 0.4784, best val acc: 0.4799)\n",
      "[Epoch: 1260] train loss: 1.3104, train acc: 0.4687, val loss: 1.2958, val acc: 0.4749  (best train acc: 0.4795, best val acc: 0.4820)\n",
      "[Epoch: 1280] train loss: 1.3107, train acc: 0.4579, val loss: 1.2931, val acc: 0.4806  (best train acc: 0.4795, best val acc: 0.4820)\n",
      "[Epoch: 1300] train loss: 1.2981, train acc: 0.4800, val loss: 1.2898, val acc: 0.4766  (best train acc: 0.4803, best val acc: 0.4820)\n",
      "[Epoch: 1320] train loss: 1.3028, train acc: 0.4678, val loss: 1.2869, val acc: 0.4806  (best train acc: 0.4815, best val acc: 0.4820)\n",
      "[Epoch: 1340] train loss: 1.2923, train acc: 0.4759, val loss: 1.2848, val acc: 0.4813  (best train acc: 0.4822, best val acc: 0.4857)\n",
      "[Epoch: 1360] train loss: 1.2922, train acc: 0.4790, val loss: 1.2814, val acc: 0.4860  (best train acc: 0.4840, best val acc: 0.4904)\n",
      "[Epoch: 1380] train loss: 1.2913, train acc: 0.4825, val loss: 1.2789, val acc: 0.4890  (best train acc: 0.4866, best val acc: 0.4917)\n",
      "[Epoch: 1400] train loss: 1.2911, train acc: 0.4817, val loss: 1.2764, val acc: 0.4938  (best train acc: 0.4931, best val acc: 0.4938)\n",
      "[Epoch: 1420] train loss: 1.2815, train acc: 0.4843, val loss: 1.2741, val acc: 0.4901  (best train acc: 0.4931, best val acc: 0.4938)\n",
      "[Epoch: 1440] train loss: 1.2771, train acc: 0.4900, val loss: 1.2717, val acc: 0.4927  (best train acc: 0.4931, best val acc: 0.4938)\n",
      "[Epoch: 1460] train loss: 1.2792, train acc: 0.4875, val loss: 1.2690, val acc: 0.4948  (best train acc: 0.4931, best val acc: 0.4958)\n",
      "[Epoch: 1480] train loss: 1.2765, train acc: 0.4894, val loss: 1.2667, val acc: 0.4948  (best train acc: 0.4931, best val acc: 0.4958)\n",
      "[Epoch: 1500] train loss: 1.2826, train acc: 0.4851, val loss: 1.2646, val acc: 0.4951  (best train acc: 0.4939, best val acc: 0.4975)\n",
      "[Epoch: 1520] train loss: 1.2786, train acc: 0.4862, val loss: 1.2623, val acc: 0.4975  (best train acc: 0.4996, best val acc: 0.4988)\n",
      "[Epoch: 1540] train loss: 1.2726, train acc: 0.4925, val loss: 1.2602, val acc: 0.4998  (best train acc: 0.4996, best val acc: 0.5002)\n",
      "[Epoch: 1560] train loss: 1.2658, train acc: 0.4951, val loss: 1.2581, val acc: 0.5039  (best train acc: 0.4996, best val acc: 0.5046)\n",
      "[Epoch: 1580] train loss: 1.2706, train acc: 0.4881, val loss: 1.2559, val acc: 0.4978  (best train acc: 0.4996, best val acc: 0.5046)\n",
      "[Epoch: 1600] train loss: 1.2652, train acc: 0.4967, val loss: 1.2543, val acc: 0.5025  (best train acc: 0.4996, best val acc: 0.5073)\n",
      "[Epoch: 1620] train loss: 1.2610, train acc: 0.5001, val loss: 1.2522, val acc: 0.5035  (best train acc: 0.5001, best val acc: 0.5076)\n",
      "[Epoch: 1640] train loss: 1.2653, train acc: 0.4931, val loss: 1.2503, val acc: 0.5049  (best train acc: 0.5009, best val acc: 0.5093)\n",
      "[Epoch: 1660] train loss: 1.2595, train acc: 0.5022, val loss: 1.2484, val acc: 0.5086  (best train acc: 0.5022, best val acc: 0.5099)\n",
      "[Epoch: 1680] train loss: 1.2616, train acc: 0.4992, val loss: 1.2467, val acc: 0.5079  (best train acc: 0.5022, best val acc: 0.5110)\n",
      "[Epoch: 1700] train loss: 1.2529, train acc: 0.4961, val loss: 1.2445, val acc: 0.5093  (best train acc: 0.5072, best val acc: 0.5110)\n",
      "[Epoch: 1720] train loss: 1.2579, train acc: 0.4913, val loss: 1.2430, val acc: 0.5089  (best train acc: 0.5072, best val acc: 0.5133)\n",
      "[Epoch: 1740] train loss: 1.2568, train acc: 0.4980, val loss: 1.2409, val acc: 0.5126  (best train acc: 0.5072, best val acc: 0.5153)\n",
      "[Epoch: 1760] train loss: 1.2613, train acc: 0.4964, val loss: 1.2396, val acc: 0.5147  (best train acc: 0.5072, best val acc: 0.5157)\n",
      "[Epoch: 1780] train loss: 1.2486, train acc: 0.5044, val loss: 1.2380, val acc: 0.5130  (best train acc: 0.5072, best val acc: 0.5157)\n",
      "[Epoch: 1800] train loss: 1.2467, train acc: 0.5032, val loss: 1.2361, val acc: 0.5137  (best train acc: 0.5072, best val acc: 0.5157)\n",
      "[Epoch: 1820] train loss: 1.2510, train acc: 0.4937, val loss: 1.2342, val acc: 0.5167  (best train acc: 0.5072, best val acc: 0.5191)\n",
      "[Epoch: 1840] train loss: 1.2455, train acc: 0.5008, val loss: 1.2326, val acc: 0.5157  (best train acc: 0.5080, best val acc: 0.5191)\n",
      "[Epoch: 1860] train loss: 1.2452, train acc: 0.5046, val loss: 1.2318, val acc: 0.5170  (best train acc: 0.5085, best val acc: 0.5241)\n",
      "[Epoch: 1880] train loss: 1.2419, train acc: 0.5017, val loss: 1.2295, val acc: 0.5157  (best train acc: 0.5085, best val acc: 0.5241)\n",
      "[Epoch: 1900] train loss: 1.2346, train acc: 0.5044, val loss: 1.2280, val acc: 0.5140  (best train acc: 0.5087, best val acc: 0.5241)\n",
      "[Epoch: 1920] train loss: 1.2422, train acc: 0.4978, val loss: 1.2264, val acc: 0.5174  (best train acc: 0.5124, best val acc: 0.5241)\n",
      "[Epoch: 1940] train loss: 1.2470, train acc: 0.5002, val loss: 1.2247, val acc: 0.5191  (best train acc: 0.5124, best val acc: 0.5241)\n",
      "[Epoch: 1960] train loss: 1.2376, train acc: 0.5007, val loss: 1.2233, val acc: 0.5170  (best train acc: 0.5124, best val acc: 0.5241)\n",
      "[Epoch: 1980] train loss: 1.2459, train acc: 0.4993, val loss: 1.2219, val acc: 0.5177  (best train acc: 0.5124, best val acc: 0.5241)\n",
      "[Epoch: 2000] train loss: 1.2423, train acc: 0.4986, val loss: 1.2204, val acc: 0.5184  (best train acc: 0.5124, best val acc: 0.5241)\n",
      "[Epoch: 2020] train loss: 1.2379, train acc: 0.4989, val loss: 1.2190, val acc: 0.5160  (best train acc: 0.5124, best val acc: 0.5241)\n",
      "[Epoch: 2040] train loss: 1.2307, train acc: 0.5043, val loss: 1.2173, val acc: 0.5184  (best train acc: 0.5130, best val acc: 0.5241)\n",
      "[Epoch: 2060] train loss: 1.2425, train acc: 0.5002, val loss: 1.2166, val acc: 0.5191  (best train acc: 0.5130, best val acc: 0.5241)\n",
      "[Epoch: 2080] train loss: 1.2147, train acc: 0.5163, val loss: 1.2144, val acc: 0.5224  (best train acc: 0.5163, best val acc: 0.5241)\n",
      "[Epoch: 2100] train loss: 1.2271, train acc: 0.5065, val loss: 1.2135, val acc: 0.5197  (best train acc: 0.5163, best val acc: 0.5241)\n",
      "[Epoch: 2120] train loss: 1.2306, train acc: 0.5012, val loss: 1.2120, val acc: 0.5191  (best train acc: 0.5163, best val acc: 0.5241)\n",
      "[Epoch: 2140] train loss: 1.2331, train acc: 0.4971, val loss: 1.2108, val acc: 0.5231  (best train acc: 0.5163, best val acc: 0.5241)\n",
      "[Epoch: 2160] train loss: 1.2122, train acc: 0.5137, val loss: 1.2091, val acc: 0.5224  (best train acc: 0.5163, best val acc: 0.5241)\n",
      "[Epoch: 2180] train loss: 1.2289, train acc: 0.5062, val loss: 1.2083, val acc: 0.5218  (best train acc: 0.5163, best val acc: 0.5258)\n",
      "[Epoch: 2200] train loss: 1.2122, train acc: 0.5042, val loss: 1.2069, val acc: 0.5221  (best train acc: 0.5163, best val acc: 0.5258)\n",
      "[Epoch: 2220] train loss: 1.2294, train acc: 0.5013, val loss: 1.2061, val acc: 0.5231  (best train acc: 0.5163, best val acc: 0.5258)\n",
      "[Epoch: 2240] train loss: 1.2129, train acc: 0.5067, val loss: 1.2045, val acc: 0.5204  (best train acc: 0.5163, best val acc: 0.5258)\n",
      "[Epoch: 2260] train loss: 1.2145, train acc: 0.5093, val loss: 1.2031, val acc: 0.5238  (best train acc: 0.5163, best val acc: 0.5258)\n",
      "[Epoch: 2280] train loss: 1.2179, train acc: 0.5091, val loss: 1.2020, val acc: 0.5224  (best train acc: 0.5163, best val acc: 0.5258)\n",
      "[Epoch: 2300] train loss: 1.2108, train acc: 0.5126, val loss: 1.2011, val acc: 0.5218  (best train acc: 0.5163, best val acc: 0.5258)\n",
      "[Epoch: 2320] train loss: 1.2187, train acc: 0.5045, val loss: 1.1995, val acc: 0.5221  (best train acc: 0.5163, best val acc: 0.5258)\n",
      "[Epoch: 2340] train loss: 1.2151, train acc: 0.5095, val loss: 1.1985, val acc: 0.5221  (best train acc: 0.5165, best val acc: 0.5258)\n",
      "[Epoch: 2360] train loss: 1.2140, train acc: 0.5024, val loss: 1.1973, val acc: 0.5221  (best train acc: 0.5174, best val acc: 0.5258)\n",
      "[Epoch: 2380] train loss: 1.2198, train acc: 0.5053, val loss: 1.1964, val acc: 0.5218  (best train acc: 0.5174, best val acc: 0.5258)\n",
      "[Epoch: 2400] train loss: 1.2114, train acc: 0.5051, val loss: 1.1949, val acc: 0.5221  (best train acc: 0.5174, best val acc: 0.5258)\n",
      "[Epoch: 2420] train loss: 1.2088, train acc: 0.5045, val loss: 1.1940, val acc: 0.5234  (best train acc: 0.5216, best val acc: 0.5258)\n",
      "[Epoch: 2440] train loss: 1.2197, train acc: 0.5045, val loss: 1.1926, val acc: 0.5258  (best train acc: 0.5216, best val acc: 0.5258)\n",
      "[Epoch: 2460] train loss: 1.2089, train acc: 0.5063, val loss: 1.1916, val acc: 0.5221  (best train acc: 0.5216, best val acc: 0.5258)\n",
      "[Epoch: 2480] train loss: 1.2165, train acc: 0.5093, val loss: 1.1908, val acc: 0.5224  (best train acc: 0.5216, best val acc: 0.5258)\n",
      "[Epoch: 2500] train loss: 1.2061, train acc: 0.5120, val loss: 1.1900, val acc: 0.5214  (best train acc: 0.5216, best val acc: 0.5265)\n",
      "[Epoch: 2520] train loss: 1.2131, train acc: 0.4999, val loss: 1.1887, val acc: 0.5234  (best train acc: 0.5216, best val acc: 0.5265)\n",
      "[Epoch: 2540] train loss: 1.2118, train acc: 0.5059, val loss: 1.1878, val acc: 0.5221  (best train acc: 0.5216, best val acc: 0.5265)\n",
      "[Epoch: 2560] train loss: 1.1891, train acc: 0.5181, val loss: 1.1866, val acc: 0.5228  (best train acc: 0.5216, best val acc: 0.5265)\n",
      "[Epoch: 2580] train loss: 1.2016, train acc: 0.5091, val loss: 1.1854, val acc: 0.5245  (best train acc: 0.5216, best val acc: 0.5265)\n",
      "[Epoch: 2600] train loss: 1.1996, train acc: 0.5149, val loss: 1.1845, val acc: 0.5224  (best train acc: 0.5216, best val acc: 0.5265)\n",
      "[Epoch: 2620] train loss: 1.2082, train acc: 0.5091, val loss: 1.1833, val acc: 0.5241  (best train acc: 0.5216, best val acc: 0.5265)\n",
      "[Epoch: 2640] train loss: 1.2091, train acc: 0.5103, val loss: 1.1824, val acc: 0.5228  (best train acc: 0.5216, best val acc: 0.5282)\n",
      "[Epoch: 2660] train loss: 1.2111, train acc: 0.5101, val loss: 1.1814, val acc: 0.5234  (best train acc: 0.5216, best val acc: 0.5282)\n",
      "[Epoch: 2680] train loss: 1.2042, train acc: 0.5069, val loss: 1.1805, val acc: 0.5231  (best train acc: 0.5216, best val acc: 0.5282)\n",
      "[Epoch: 2700] train loss: 1.1963, train acc: 0.5134, val loss: 1.1795, val acc: 0.5255  (best train acc: 0.5216, best val acc: 0.5282)\n",
      "[Epoch: 2720] train loss: 1.1958, train acc: 0.5154, val loss: 1.1793, val acc: 0.5234  (best train acc: 0.5216, best val acc: 0.5282)\n",
      "[Epoch: 2740] train loss: 1.1964, train acc: 0.5105, val loss: 1.1779, val acc: 0.5224  (best train acc: 0.5216, best val acc: 0.5282)\n",
      "[Epoch: 2760] train loss: 1.2069, train acc: 0.5011, val loss: 1.1766, val acc: 0.5255  (best train acc: 0.5216, best val acc: 0.5282)\n",
      "[Epoch: 2780] train loss: 1.1987, train acc: 0.5100, val loss: 1.1758, val acc: 0.5261  (best train acc: 0.5216, best val acc: 0.5282)\n",
      "[Epoch: 2800] train loss: 1.2037, train acc: 0.5106, val loss: 1.1750, val acc: 0.5238  (best train acc: 0.5216, best val acc: 0.5282)\n",
      "[Epoch: 2820] train loss: 1.1942, train acc: 0.5093, val loss: 1.1746, val acc: 0.5258  (best train acc: 0.5216, best val acc: 0.5292)\n",
      "[Epoch: 2840] train loss: 1.1945, train acc: 0.5161, val loss: 1.1734, val acc: 0.5238  (best train acc: 0.5216, best val acc: 0.5292)\n",
      "[Epoch: 2860] train loss: 1.1926, train acc: 0.5127, val loss: 1.1724, val acc: 0.5255  (best train acc: 0.5236, best val acc: 0.5292)\n",
      "[Epoch: 2880] train loss: 1.1892, train acc: 0.5157, val loss: 1.1714, val acc: 0.5251  (best train acc: 0.5236, best val acc: 0.5292)\n",
      "[Epoch: 2900] train loss: 1.1918, train acc: 0.5121, val loss: 1.1707, val acc: 0.5231  (best train acc: 0.5236, best val acc: 0.5292)\n",
      "[Epoch: 2920] train loss: 1.1972, train acc: 0.5097, val loss: 1.1697, val acc: 0.5245  (best train acc: 0.5236, best val acc: 0.5292)\n",
      "[Epoch: 2940] train loss: 1.1873, train acc: 0.5170, val loss: 1.1688, val acc: 0.5258  (best train acc: 0.5236, best val acc: 0.5292)\n",
      "[Epoch: 2960] train loss: 1.1854, train acc: 0.5166, val loss: 1.1683, val acc: 0.5255  (best train acc: 0.5236, best val acc: 0.5292)\n",
      "[Epoch: 2980] train loss: 1.1844, train acc: 0.5155, val loss: 1.1671, val acc: 0.5322  (best train acc: 0.5236, best val acc: 0.5322)\n",
      "[Epoch: 3000] train loss: 1.1695, train acc: 0.5210, val loss: 1.1646, val acc: 0.5285  (best train acc: 0.5245, best val acc: 0.5356)\n",
      "[Epoch: 3020] train loss: 1.1732, train acc: 0.5190, val loss: 1.1646, val acc: 0.5285  (best train acc: 0.5249, best val acc: 0.5356)\n",
      "[Epoch: 3040] train loss: 1.1723, train acc: 0.5166, val loss: 1.1610, val acc: 0.5332  (best train acc: 0.5309, best val acc: 0.5356)\n",
      "[Epoch: 3060] train loss: 1.1645, train acc: 0.5249, val loss: 1.1565, val acc: 0.5305  (best train acc: 0.5326, best val acc: 0.5356)\n",
      "[Epoch: 3080] train loss: 1.1566, train acc: 0.5243, val loss: 1.1547, val acc: 0.5312  (best train acc: 0.5326, best val acc: 0.5356)\n",
      "[Epoch: 3100] train loss: 1.1568, train acc: 0.5276, val loss: 1.1535, val acc: 0.5302  (best train acc: 0.5326, best val acc: 0.5366)\n",
      "[Epoch: 3120] train loss: 1.1553, train acc: 0.5226, val loss: 1.1504, val acc: 0.5332  (best train acc: 0.5328, best val acc: 0.5366)\n",
      "[Epoch: 3140] train loss: 1.1652, train acc: 0.5196, val loss: 1.1485, val acc: 0.5315  (best train acc: 0.5328, best val acc: 0.5366)\n",
      "[Epoch: 3160] train loss: 1.1540, train acc: 0.5295, val loss: 1.1467, val acc: 0.5309  (best train acc: 0.5336, best val acc: 0.5366)\n",
      "[Epoch: 3180] train loss: 1.1487, train acc: 0.5289, val loss: 1.1452, val acc: 0.5312  (best train acc: 0.5338, best val acc: 0.5366)\n",
      "[Epoch: 3200] train loss: 1.1555, train acc: 0.5267, val loss: 1.1507, val acc: 0.5339  (best train acc: 0.5365, best val acc: 0.5366)\n",
      "[Epoch: 3220] train loss: 1.1504, train acc: 0.5304, val loss: 1.1433, val acc: 0.5349  (best train acc: 0.5365, best val acc: 0.5366)\n",
      "[Epoch: 3240] train loss: 1.1403, train acc: 0.5364, val loss: 1.1396, val acc: 0.5369  (best train acc: 0.5365, best val acc: 0.5369)\n",
      "[Epoch: 3260] train loss: 1.1431, train acc: 0.5255, val loss: 1.1391, val acc: 0.5295  (best train acc: 0.5365, best val acc: 0.5383)\n",
      "[Epoch: 3280] train loss: 1.1400, train acc: 0.5360, val loss: 1.1362, val acc: 0.5369  (best train acc: 0.5365, best val acc: 0.5410)\n",
      "[Epoch: 3300] train loss: 1.1404, train acc: 0.5323, val loss: 1.1343, val acc: 0.5359  (best train acc: 0.5365, best val acc: 0.5410)\n",
      "[Epoch: 3320] train loss: 1.1370, train acc: 0.5388, val loss: 1.1327, val acc: 0.5336  (best train acc: 0.5388, best val acc: 0.5410)\n",
      "[Epoch: 3340] train loss: 1.1364, train acc: 0.5308, val loss: 1.1309, val acc: 0.5383  (best train acc: 0.5388, best val acc: 0.5410)\n",
      "[Epoch: 3360] train loss: 1.1311, train acc: 0.5369, val loss: 1.1322, val acc: 0.5373  (best train acc: 0.5406, best val acc: 0.5410)\n",
      "[Epoch: 3380] train loss: 1.1451, train acc: 0.5223, val loss: 1.1280, val acc: 0.5393  (best train acc: 0.5417, best val acc: 0.5413)\n",
      "[Epoch: 3400] train loss: 1.1385, train acc: 0.5369, val loss: 1.1260, val acc: 0.5376  (best train acc: 0.5417, best val acc: 0.5420)\n",
      "[Epoch: 3420] train loss: 1.1258, train acc: 0.5383, val loss: 1.1287, val acc: 0.5396  (best train acc: 0.5417, best val acc: 0.5420)\n",
      "[Epoch: 3440] train loss: 1.1324, train acc: 0.5375, val loss: 1.1247, val acc: 0.5325  (best train acc: 0.5429, best val acc: 0.5420)\n",
      "[Epoch: 3460] train loss: 1.1293, train acc: 0.5414, val loss: 1.1221, val acc: 0.5406  (best train acc: 0.5429, best val acc: 0.5420)\n",
      "[Epoch: 3480] train loss: 1.1304, train acc: 0.5363, val loss: 1.1244, val acc: 0.5383  (best train acc: 0.5429, best val acc: 0.5423)\n",
      "[Epoch: 3500] train loss: 1.1271, train acc: 0.5401, val loss: 1.1181, val acc: 0.5400  (best train acc: 0.5429, best val acc: 0.5440)\n",
      "[Epoch: 3520] train loss: 1.1243, train acc: 0.5377, val loss: 1.1161, val acc: 0.5413  (best train acc: 0.5429, best val acc: 0.5444)\n",
      "[Epoch: 3540] train loss: 1.1300, train acc: 0.5340, val loss: 1.1269, val acc: 0.5427  (best train acc: 0.5451, best val acc: 0.5444)\n",
      "[Epoch: 3560] train loss: 1.1281, train acc: 0.5384, val loss: 1.1140, val acc: 0.5403  (best train acc: 0.5475, best val acc: 0.5474)\n",
      "[Epoch: 3580] train loss: 1.1312, train acc: 0.5312, val loss: 1.1121, val acc: 0.5427  (best train acc: 0.5475, best val acc: 0.5474)\n",
      "[Epoch: 3600] train loss: 1.1331, train acc: 0.5282, val loss: 1.1114, val acc: 0.5467  (best train acc: 0.5475, best val acc: 0.5474)\n",
      "[Epoch: 3620] train loss: 1.1146, train acc: 0.5401, val loss: 1.1094, val acc: 0.5444  (best train acc: 0.5491, best val acc: 0.5491)\n",
      "[Epoch: 3640] train loss: 1.1231, train acc: 0.5369, val loss: 1.1072, val acc: 0.5430  (best train acc: 0.5491, best val acc: 0.5491)\n",
      "[Epoch: 3660] train loss: 1.1185, train acc: 0.5351, val loss: 1.1059, val acc: 0.5470  (best train acc: 0.5492, best val acc: 0.5508)\n",
      "[Epoch: 3680] train loss: 1.1126, train acc: 0.5458, val loss: 1.1066, val acc: 0.5467  (best train acc: 0.5492, best val acc: 0.5518)\n",
      "[Epoch: 3700] train loss: 1.1241, train acc: 0.5430, val loss: 1.1020, val acc: 0.5484  (best train acc: 0.5499, best val acc: 0.5531)\n",
      "[Epoch: 3720] train loss: 1.1123, train acc: 0.5349, val loss: 1.1010, val acc: 0.5464  (best train acc: 0.5499, best val acc: 0.5531)\n",
      "[Epoch: 3740] train loss: 1.1193, train acc: 0.5410, val loss: 1.1038, val acc: 0.5464  (best train acc: 0.5525, best val acc: 0.5531)\n",
      "[Epoch: 3760] train loss: 1.1079, train acc: 0.5416, val loss: 1.1010, val acc: 0.5464  (best train acc: 0.5526, best val acc: 0.5531)\n",
      "[Epoch: 3780] train loss: 1.1085, train acc: 0.5424, val loss: 1.0962, val acc: 0.5518  (best train acc: 0.5526, best val acc: 0.5538)\n",
      "[Epoch: 3800] train loss: 1.1204, train acc: 0.5385, val loss: 1.0963, val acc: 0.5494  (best train acc: 0.5526, best val acc: 0.5538)\n",
      "[Epoch: 3820] train loss: 1.1156, train acc: 0.5398, val loss: 1.0926, val acc: 0.5511  (best train acc: 0.5541, best val acc: 0.5548)\n",
      "[Epoch: 3840] train loss: 1.1093, train acc: 0.5335, val loss: 1.0931, val acc: 0.5575  (best train acc: 0.5580, best val acc: 0.5575)\n",
      "[Epoch: 3860] train loss: 1.1054, train acc: 0.5394, val loss: 1.0894, val acc: 0.5531  (best train acc: 0.5580, best val acc: 0.5575)\n",
      "[Epoch: 3880] train loss: 1.0969, train acc: 0.5445, val loss: 1.0881, val acc: 0.5521  (best train acc: 0.5580, best val acc: 0.5575)\n",
      "[Epoch: 3900] train loss: 1.0993, train acc: 0.5533, val loss: 1.0863, val acc: 0.5562  (best train acc: 0.5580, best val acc: 0.5575)\n",
      "[Epoch: 3920] train loss: 1.1093, train acc: 0.5435, val loss: 1.0858, val acc: 0.5541  (best train acc: 0.5580, best val acc: 0.5589)\n",
      "[Epoch: 3940] train loss: 1.1006, train acc: 0.5497, val loss: 1.0830, val acc: 0.5541  (best train acc: 0.5601, best val acc: 0.5589)\n",
      "[Epoch: 3960] train loss: 1.0968, train acc: 0.5532, val loss: 1.0819, val acc: 0.5548  (best train acc: 0.5601, best val acc: 0.5589)\n",
      "[Epoch: 3980] train loss: 1.1022, train acc: 0.5471, val loss: 1.0833, val acc: 0.5524  (best train acc: 0.5603, best val acc: 0.5599)\n",
      "[Epoch: 4000] train loss: 1.0758, train acc: 0.5625, val loss: 1.0906, val acc: 0.5582  (best train acc: 0.5625, best val acc: 0.5599)\n",
      "[Epoch: 4020] train loss: 1.0826, train acc: 0.5583, val loss: 1.0770, val acc: 0.5558  (best train acc: 0.5625, best val acc: 0.5626)\n",
      "[Epoch: 4040] train loss: 1.1033, train acc: 0.5359, val loss: 1.0890, val acc: 0.5592  (best train acc: 0.5625, best val acc: 0.5626)\n",
      "[Epoch: 4060] train loss: 1.0947, train acc: 0.5516, val loss: 1.0797, val acc: 0.5619  (best train acc: 0.5625, best val acc: 0.5626)\n",
      "[Epoch: 4080] train loss: 1.0847, train acc: 0.5602, val loss: 1.0726, val acc: 0.5592  (best train acc: 0.5637, best val acc: 0.5626)\n",
      "[Epoch: 4100] train loss: 1.0736, train acc: 0.5638, val loss: 1.0729, val acc: 0.5626  (best train acc: 0.5638, best val acc: 0.5646)\n",
      "[Epoch: 4120] train loss: 1.0903, train acc: 0.5504, val loss: 1.0704, val acc: 0.5595  (best train acc: 0.5654, best val acc: 0.5666)\n",
      "[Epoch: 4140] train loss: 1.0932, train acc: 0.5534, val loss: 1.0721, val acc: 0.5551  (best train acc: 0.5679, best val acc: 0.5673)\n",
      "[Epoch: 4160] train loss: 1.0993, train acc: 0.5514, val loss: 1.0700, val acc: 0.5663  (best train acc: 0.5679, best val acc: 0.5690)\n",
      "[Epoch: 4180] train loss: 1.0866, train acc: 0.5520, val loss: 1.0654, val acc: 0.5605  (best train acc: 0.5688, best val acc: 0.5690)\n",
      "[Epoch: 4200] train loss: 1.0685, train acc: 0.5602, val loss: 1.0706, val acc: 0.5693  (best train acc: 0.5755, best val acc: 0.5693)\n",
      "[Epoch: 4220] train loss: 1.0854, train acc: 0.5634, val loss: 1.0623, val acc: 0.5629  (best train acc: 0.5755, best val acc: 0.5703)\n",
      "[Epoch: 4240] train loss: 1.0726, train acc: 0.5585, val loss: 1.0645, val acc: 0.5673  (best train acc: 0.5755, best val acc: 0.5703)\n",
      "[Epoch: 4260] train loss: 1.0789, train acc: 0.5560, val loss: 1.0629, val acc: 0.5612  (best train acc: 0.5755, best val acc: 0.5703)\n",
      "[Epoch: 4280] train loss: 1.0726, train acc: 0.5585, val loss: 1.0632, val acc: 0.5555  (best train acc: 0.5755, best val acc: 0.5703)\n",
      "[Epoch: 4300] train loss: 1.0836, train acc: 0.5586, val loss: 1.0570, val acc: 0.5663  (best train acc: 0.5755, best val acc: 0.5734)\n",
      "[Epoch: 4320] train loss: 1.0919, train acc: 0.5570, val loss: 1.0586, val acc: 0.5703  (best train acc: 0.5755, best val acc: 0.5744)\n",
      "[Epoch: 4340] train loss: 1.0820, train acc: 0.5491, val loss: 1.0568, val acc: 0.5707  (best train acc: 0.5755, best val acc: 0.5744)\n",
      "[Epoch: 4360] train loss: 1.0683, train acc: 0.5657, val loss: 1.0588, val acc: 0.5754  (best train acc: 0.5755, best val acc: 0.5754)\n",
      "[Epoch: 4380] train loss: 1.0782, train acc: 0.5550, val loss: 1.0535, val acc: 0.5653  (best train acc: 0.5755, best val acc: 0.5754)\n",
      "[Epoch: 4400] train loss: 1.0836, train acc: 0.5449, val loss: 1.0498, val acc: 0.5683  (best train acc: 0.5767, best val acc: 0.5771)\n",
      "[Epoch: 4420] train loss: 1.0686, train acc: 0.5556, val loss: 1.0489, val acc: 0.5669  (best train acc: 0.5767, best val acc: 0.5771)\n",
      "[Epoch: 4440] train loss: 1.0690, train acc: 0.5623, val loss: 1.0512, val acc: 0.5700  (best train acc: 0.5767, best val acc: 0.5771)\n",
      "[Epoch: 4460] train loss: 1.0719, train acc: 0.5643, val loss: 1.0462, val acc: 0.5649  (best train acc: 0.5776, best val acc: 0.5771)\n",
      "[Epoch: 4480] train loss: 1.0644, train acc: 0.5669, val loss: 1.0491, val acc: 0.5744  (best train acc: 0.5776, best val acc: 0.5777)\n",
      "[Epoch: 4500] train loss: 1.0740, train acc: 0.5502, val loss: 1.0439, val acc: 0.5673  (best train acc: 0.5776, best val acc: 0.5777)\n",
      "[Epoch: 4520] train loss: 1.0666, train acc: 0.5602, val loss: 1.0477, val acc: 0.5754  (best train acc: 0.5776, best val acc: 0.5777)\n",
      "[Epoch: 4540] train loss: 1.0487, train acc: 0.5753, val loss: 1.0411, val acc: 0.5686  (best train acc: 0.5778, best val acc: 0.5801)\n",
      "[Epoch: 4560] train loss: 1.0577, train acc: 0.5706, val loss: 1.0397, val acc: 0.5696  (best train acc: 0.5778, best val acc: 0.5801)\n",
      "[Epoch: 4580] train loss: 1.0794, train acc: 0.5597, val loss: 1.0413, val acc: 0.5744  (best train acc: 0.5789, best val acc: 0.5801)\n",
      "[Epoch: 4600] train loss: 1.0456, train acc: 0.5641, val loss: 1.0373, val acc: 0.5690  (best train acc: 0.5789, best val acc: 0.5801)\n",
      "[Epoch: 4620] train loss: 1.0449, train acc: 0.5653, val loss: 1.0401, val acc: 0.5757  (best train acc: 0.5789, best val acc: 0.5801)\n",
      "[Epoch: 4640] train loss: 1.0659, train acc: 0.5563, val loss: 1.0487, val acc: 0.5750  (best train acc: 0.5789, best val acc: 0.5801)\n",
      "[Epoch: 4660] train loss: 1.0671, train acc: 0.5509, val loss: 1.0350, val acc: 0.5737  (best train acc: 0.5860, best val acc: 0.5818)\n",
      "[Epoch: 4680] train loss: 1.0662, train acc: 0.5599, val loss: 1.0346, val acc: 0.5734  (best train acc: 0.5860, best val acc: 0.5818)\n",
      "[Epoch: 4700] train loss: 1.0641, train acc: 0.5638, val loss: 1.0322, val acc: 0.5764  (best train acc: 0.5860, best val acc: 0.5825)\n",
      "[Epoch: 4720] train loss: 1.0516, train acc: 0.5724, val loss: 1.0427, val acc: 0.5575  (best train acc: 0.5860, best val acc: 0.5825)\n",
      "[Epoch: 4740] train loss: 1.0356, train acc: 0.5731, val loss: 1.0330, val acc: 0.5791  (best train acc: 0.5860, best val acc: 0.5825)\n",
      "[Epoch: 4760] train loss: 1.0672, train acc: 0.5664, val loss: 1.0374, val acc: 0.5622  (best train acc: 0.5860, best val acc: 0.5825)\n",
      "[Epoch: 4780] train loss: 1.0544, train acc: 0.5565, val loss: 1.0331, val acc: 0.5828  (best train acc: 0.5860, best val acc: 0.5828)\n",
      "[Epoch: 4800] train loss: 1.0633, train acc: 0.5504, val loss: 1.0372, val acc: 0.5798  (best train acc: 0.5860, best val acc: 0.5828)\n",
      "[Epoch: 4820] train loss: 1.0373, train acc: 0.5720, val loss: 1.0358, val acc: 0.5821  (best train acc: 0.5860, best val acc: 0.5828)\n",
      "[Epoch: 4840] train loss: 1.0630, train acc: 0.5758, val loss: 1.0264, val acc: 0.5791  (best train acc: 0.5860, best val acc: 0.5841)\n",
      "[Epoch: 4860] train loss: 1.0409, train acc: 0.5719, val loss: 1.0314, val acc: 0.5815  (best train acc: 0.5860, best val acc: 0.5841)\n",
      "[Epoch: 4880] train loss: 1.0586, train acc: 0.5722, val loss: 1.0277, val acc: 0.5868  (best train acc: 0.5889, best val acc: 0.5868)\n",
      "[Epoch: 4900] train loss: 1.0454, train acc: 0.5807, val loss: 1.0218, val acc: 0.5777  (best train acc: 0.5889, best val acc: 0.5868)\n",
      "[Epoch: 4920] train loss: 1.0424, train acc: 0.5692, val loss: 1.0301, val acc: 0.5828  (best train acc: 0.5889, best val acc: 0.5868)\n",
      "[Epoch: 4940] train loss: 1.0596, train acc: 0.5647, val loss: 1.0228, val acc: 0.5740  (best train acc: 0.5889, best val acc: 0.5868)\n",
      "[Epoch: 4960] train loss: 1.0459, train acc: 0.5703, val loss: 1.0191, val acc: 0.5801  (best train acc: 0.5889, best val acc: 0.5868)\n",
      "[Epoch: 4980] train loss: 1.0341, train acc: 0.5765, val loss: 1.0168, val acc: 0.5815  (best train acc: 0.5889, best val acc: 0.5868)\n",
      "[Epoch: 5000] train loss: 1.0517, train acc: 0.5779, val loss: 1.0242, val acc: 0.5690  (best train acc: 0.5889, best val acc: 0.5875)\n",
      "[Epoch: 5020] train loss: 1.0213, train acc: 0.5849, val loss: 1.0232, val acc: 0.5838  (best train acc: 0.5889, best val acc: 0.5885)\n",
      "[Epoch: 5040] train loss: 1.0553, train acc: 0.5498, val loss: 1.0149, val acc: 0.5838  (best train acc: 0.5889, best val acc: 0.5885)\n",
      "[Epoch: 5060] train loss: 1.0285, train acc: 0.5820, val loss: 1.0211, val acc: 0.5845  (best train acc: 0.5889, best val acc: 0.5885)\n",
      "[Epoch: 5080] train loss: 1.0303, train acc: 0.5808, val loss: 1.0119, val acc: 0.5815  (best train acc: 0.5889, best val acc: 0.5885)\n",
      "[Epoch: 5100] train loss: 1.0323, train acc: 0.5758, val loss: 1.0098, val acc: 0.5828  (best train acc: 0.5889, best val acc: 0.5885)\n",
      "[Epoch: 5120] train loss: 1.0304, train acc: 0.5714, val loss: 1.0145, val acc: 0.5848  (best train acc: 0.5913, best val acc: 0.5885)\n",
      "[Epoch: 5140] train loss: 1.0497, train acc: 0.5700, val loss: 1.0104, val acc: 0.5801  (best train acc: 0.5915, best val acc: 0.5889)\n",
      "[Epoch: 5160] train loss: 1.0438, train acc: 0.5695, val loss: 1.0157, val acc: 0.5713  (best train acc: 0.5915, best val acc: 0.5889)\n",
      "[Epoch: 5180] train loss: 1.0325, train acc: 0.5754, val loss: 1.0046, val acc: 0.5835  (best train acc: 0.5915, best val acc: 0.5892)\n",
      "[Epoch: 5200] train loss: 1.0186, train acc: 0.5835, val loss: 1.0074, val acc: 0.5777  (best train acc: 0.5915, best val acc: 0.5906)\n",
      "[Epoch: 5220] train loss: 1.0280, train acc: 0.5764, val loss: 1.0037, val acc: 0.5902  (best train acc: 0.5915, best val acc: 0.5919)\n",
      "[Epoch: 5240] train loss: 1.0317, train acc: 0.5784, val loss: 1.0035, val acc: 0.5912  (best train acc: 0.5915, best val acc: 0.5919)\n",
      "[Epoch: 5260] train loss: 1.0156, train acc: 0.5766, val loss: 1.0079, val acc: 0.5784  (best train acc: 0.5915, best val acc: 0.5922)\n",
      "[Epoch: 5280] train loss: 1.0246, train acc: 0.5804, val loss: 1.0003, val acc: 0.5889  (best train acc: 0.5915, best val acc: 0.5922)\n",
      "[Epoch: 5300] train loss: 1.0540, train acc: 0.5613, val loss: 1.0001, val acc: 0.5916  (best train acc: 0.5915, best val acc: 0.5939)\n",
      "[Epoch: 5320] train loss: 1.0489, train acc: 0.5662, val loss: 1.0179, val acc: 0.5632  (best train acc: 0.5915, best val acc: 0.5939)\n",
      "[Epoch: 5340] train loss: 1.0190, train acc: 0.5920, val loss: 0.9965, val acc: 0.5858  (best train acc: 0.5920, best val acc: 0.5939)\n",
      "[Epoch: 5360] train loss: 1.0252, train acc: 0.5765, val loss: 1.0415, val acc: 0.5653  (best train acc: 0.5920, best val acc: 0.5953)\n",
      "[Epoch: 5380] train loss: 1.0090, train acc: 0.5812, val loss: 0.9971, val acc: 0.5902  (best train acc: 0.5920, best val acc: 0.5953)\n",
      "[Epoch: 5400] train loss: 1.0484, train acc: 0.5755, val loss: 1.0053, val acc: 0.5750  (best train acc: 0.5928, best val acc: 0.5963)\n",
      "[Epoch: 5420] train loss: 1.0264, train acc: 0.5883, val loss: 0.9957, val acc: 0.5960  (best train acc: 0.5928, best val acc: 0.5963)\n",
      "[Epoch: 5440] train loss: 1.0073, train acc: 0.5817, val loss: 0.9931, val acc: 0.5943  (best train acc: 0.5930, best val acc: 0.5963)\n",
      "[Epoch: 5460] train loss: 1.0157, train acc: 0.5860, val loss: 0.9944, val acc: 0.5919  (best train acc: 0.5930, best val acc: 0.5963)\n",
      "[Epoch: 5480] train loss: 1.0292, train acc: 0.5783, val loss: 0.9920, val acc: 0.5808  (best train acc: 0.6005, best val acc: 0.5980)\n",
      "[Epoch: 5500] train loss: 1.0143, train acc: 0.5878, val loss: 0.9890, val acc: 0.5852  (best train acc: 0.6005, best val acc: 0.5980)\n",
      "[Epoch: 5520] train loss: 1.0181, train acc: 0.5809, val loss: 1.0070, val acc: 0.5703  (best train acc: 0.6005, best val acc: 0.5980)\n",
      "[Epoch: 5540] train loss: 1.0290, train acc: 0.5693, val loss: 1.0191, val acc: 0.5744  (best train acc: 0.6005, best val acc: 0.5980)\n",
      "[Epoch: 5560] train loss: 1.0538, train acc: 0.5640, val loss: 1.0002, val acc: 0.5744  (best train acc: 0.6005, best val acc: 0.5987)\n",
      "[Epoch: 5580] train loss: 1.0302, train acc: 0.5723, val loss: 1.0031, val acc: 0.5895  (best train acc: 0.6005, best val acc: 0.5987)\n",
      "[Epoch: 5600] train loss: 1.0196, train acc: 0.5763, val loss: 0.9949, val acc: 0.5872  (best train acc: 0.6005, best val acc: 0.5987)\n",
      "[Epoch: 5620] train loss: 1.0436, train acc: 0.5703, val loss: 0.9879, val acc: 0.5882  (best train acc: 0.6005, best val acc: 0.5990)\n",
      "[Epoch: 5640] train loss: 1.0361, train acc: 0.5748, val loss: 0.9943, val acc: 0.5788  (best train acc: 0.6005, best val acc: 0.6007)\n",
      "[Epoch: 5660] train loss: 0.9945, train acc: 0.5905, val loss: 0.9886, val acc: 0.5970  (best train acc: 0.6005, best val acc: 0.6007)\n",
      "[Epoch: 5680] train loss: 1.0326, train acc: 0.5691, val loss: 0.9857, val acc: 0.5976  (best train acc: 0.6005, best val acc: 0.6020)\n",
      "[Epoch: 5700] train loss: 1.0019, train acc: 0.5837, val loss: 0.9837, val acc: 0.5943  (best train acc: 0.6006, best val acc: 0.6020)\n",
      "[Epoch: 5720] train loss: 0.9989, train acc: 0.5841, val loss: 0.9881, val acc: 0.5973  (best train acc: 0.6006, best val acc: 0.6024)\n",
      "[Epoch: 5740] train loss: 1.0240, train acc: 0.5759, val loss: 0.9804, val acc: 0.5922  (best train acc: 0.6006, best val acc: 0.6024)\n",
      "[Epoch: 5760] train loss: 1.0011, train acc: 0.5882, val loss: 0.9875, val acc: 0.5828  (best train acc: 0.6006, best val acc: 0.6024)\n",
      "[Epoch: 5780] train loss: 1.0332, train acc: 0.5809, val loss: 1.0102, val acc: 0.5642  (best train acc: 0.6006, best val acc: 0.6024)\n",
      "[Epoch: 5800] train loss: 1.0446, train acc: 0.5620, val loss: 0.9814, val acc: 0.6003  (best train acc: 0.6006, best val acc: 0.6024)\n",
      "[Epoch: 5820] train loss: 1.0238, train acc: 0.5844, val loss: 0.9806, val acc: 0.5882  (best train acc: 0.6006, best val acc: 0.6030)\n",
      "[Epoch: 5840] train loss: 1.0400, train acc: 0.5588, val loss: 0.9817, val acc: 0.5997  (best train acc: 0.6006, best val acc: 0.6030)\n",
      "[Epoch: 5860] train loss: 1.0091, train acc: 0.5932, val loss: 0.9797, val acc: 0.6000  (best train acc: 0.6006, best val acc: 0.6030)\n",
      "[Epoch: 5880] train loss: 1.0461, train acc: 0.5689, val loss: 0.9807, val acc: 0.5852  (best train acc: 0.6011, best val acc: 0.6030)\n",
      "[Epoch: 5900] train loss: 1.0106, train acc: 0.5805, val loss: 0.9824, val acc: 0.5956  (best train acc: 0.6011, best val acc: 0.6051)\n",
      "[Epoch: 5920] train loss: 1.0255, train acc: 0.5741, val loss: 0.9748, val acc: 0.5926  (best train acc: 0.6011, best val acc: 0.6051)\n",
      "[Epoch: 5940] train loss: 1.0129, train acc: 0.5846, val loss: 0.9888, val acc: 0.5808  (best train acc: 0.6011, best val acc: 0.6051)\n",
      "[Epoch: 5960] train loss: 0.9902, train acc: 0.5964, val loss: 0.9927, val acc: 0.5892  (best train acc: 0.6014, best val acc: 0.6051)\n",
      "[Epoch: 5980] train loss: 1.0450, train acc: 0.5722, val loss: 1.0025, val acc: 0.5680  (best train acc: 0.6014, best val acc: 0.6051)\n",
      "[Epoch: 6000] train loss: 1.0120, train acc: 0.5886, val loss: 0.9819, val acc: 0.5852  (best train acc: 0.6015, best val acc: 0.6051)\n",
      "[Epoch: 6020] train loss: 1.0118, train acc: 0.5859, val loss: 0.9768, val acc: 0.5895  (best train acc: 0.6066, best val acc: 0.6051)\n",
      "[Epoch: 6040] train loss: 1.0327, train acc: 0.5729, val loss: 0.9748, val acc: 0.5916  (best train acc: 0.6066, best val acc: 0.6054)\n",
      "[Epoch: 6060] train loss: 1.0241, train acc: 0.5795, val loss: 0.9767, val acc: 0.6030  (best train acc: 0.6066, best val acc: 0.6054)\n",
      "[Epoch: 6080] train loss: 0.9934, train acc: 0.5969, val loss: 0.9709, val acc: 0.5960  (best train acc: 0.6066, best val acc: 0.6054)\n",
      "[Epoch: 6100] train loss: 0.9919, train acc: 0.5872, val loss: 0.9741, val acc: 0.5889  (best train acc: 0.6066, best val acc: 0.6054)\n",
      "[Epoch: 6120] train loss: 0.9957, train acc: 0.5926, val loss: 0.9730, val acc: 0.6020  (best train acc: 0.6066, best val acc: 0.6054)\n",
      "[Epoch: 6140] train loss: 1.0011, train acc: 0.5912, val loss: 0.9717, val acc: 0.6051  (best train acc: 0.6066, best val acc: 0.6057)\n",
      "[Epoch: 6160] train loss: 1.0217, train acc: 0.5826, val loss: 0.9721, val acc: 0.6037  (best train acc: 0.6066, best val acc: 0.6057)\n",
      "[Epoch: 6180] train loss: 0.9911, train acc: 0.5909, val loss: 0.9776, val acc: 0.5862  (best train acc: 0.6066, best val acc: 0.6057)\n",
      "[Epoch: 6200] train loss: 1.0113, train acc: 0.5930, val loss: 0.9693, val acc: 0.5980  (best train acc: 0.6066, best val acc: 0.6057)\n",
      "[Epoch: 6220] train loss: 1.0375, train acc: 0.5697, val loss: 1.0016, val acc: 0.5700  (best train acc: 0.6066, best val acc: 0.6057)\n",
      "[Epoch: 6240] train loss: 0.9889, train acc: 0.6002, val loss: 0.9675, val acc: 0.5983  (best train acc: 0.6066, best val acc: 0.6078)\n",
      "[Epoch: 6260] train loss: 0.9655, train acc: 0.6087, val loss: 0.9660, val acc: 0.5963  (best train acc: 0.6087, best val acc: 0.6078)\n",
      "[Epoch: 6280] train loss: 0.9814, train acc: 0.5956, val loss: 0.9731, val acc: 0.6013  (best train acc: 0.6087, best val acc: 0.6084)\n",
      "[Epoch: 6300] train loss: 0.9857, train acc: 0.5954, val loss: 1.0089, val acc: 0.5767  (best train acc: 0.6087, best val acc: 0.6084)\n",
      "[Epoch: 6320] train loss: 1.0192, train acc: 0.5797, val loss: 0.9646, val acc: 0.6054  (best train acc: 0.6087, best val acc: 0.6094)\n",
      "[Epoch: 6340] train loss: 1.0026, train acc: 0.5863, val loss: 0.9611, val acc: 0.6054  (best train acc: 0.6116, best val acc: 0.6094)\n",
      "[Epoch: 6360] train loss: 0.9986, train acc: 0.5888, val loss: 0.9607, val acc: 0.6027  (best train acc: 0.6116, best val acc: 0.6094)\n",
      "[Epoch: 6380] train loss: 1.0040, train acc: 0.5823, val loss: 0.9643, val acc: 0.6108  (best train acc: 0.6116, best val acc: 0.6108)\n",
      "[Epoch: 6400] train loss: 1.0170, train acc: 0.5764, val loss: 0.9684, val acc: 0.5909  (best train acc: 0.6116, best val acc: 0.6108)\n",
      "[Epoch: 6420] train loss: 0.9859, train acc: 0.5983, val loss: 0.9616, val acc: 0.6088  (best train acc: 0.6116, best val acc: 0.6108)\n",
      "[Epoch: 6440] train loss: 0.9777, train acc: 0.5961, val loss: 0.9592, val acc: 0.6040  (best train acc: 0.6116, best val acc: 0.6115)\n",
      "[Epoch: 6460] train loss: 1.0040, train acc: 0.5915, val loss: 0.9615, val acc: 0.5976  (best train acc: 0.6116, best val acc: 0.6115)\n",
      "[Epoch: 6480] train loss: 1.0002, train acc: 0.5846, val loss: 0.9620, val acc: 0.5973  (best train acc: 0.6116, best val acc: 0.6115)\n",
      "[Epoch: 6500] train loss: 1.0124, train acc: 0.5847, val loss: 0.9602, val acc: 0.5997  (best train acc: 0.6116, best val acc: 0.6115)\n",
      "[Epoch: 6520] train loss: 0.9877, train acc: 0.5935, val loss: 0.9565, val acc: 0.6071  (best train acc: 0.6116, best val acc: 0.6118)\n",
      "[Epoch: 6540] train loss: 1.0080, train acc: 0.5899, val loss: 0.9589, val acc: 0.5990  (best train acc: 0.6116, best val acc: 0.6118)\n",
      "[Epoch: 6560] train loss: 0.9852, train acc: 0.5904, val loss: 0.9587, val acc: 0.6101  (best train acc: 0.6116, best val acc: 0.6152)\n",
      "[Epoch: 6580] train loss: 0.9969, train acc: 0.5860, val loss: 0.9592, val acc: 0.6121  (best train acc: 0.6116, best val acc: 0.6155)\n",
      "[Epoch: 6600] train loss: 0.9808, train acc: 0.5915, val loss: 0.9658, val acc: 0.5895  (best train acc: 0.6116, best val acc: 0.6155)\n",
      "[Epoch: 6620] train loss: 1.0211, train acc: 0.5755, val loss: 0.9555, val acc: 0.6071  (best train acc: 0.6116, best val acc: 0.6155)\n",
      "[Epoch: 6640] train loss: 1.0111, train acc: 0.5847, val loss: 0.9530, val acc: 0.6084  (best train acc: 0.6116, best val acc: 0.6155)\n",
      "[Epoch: 6660] train loss: 0.9821, train acc: 0.6018, val loss: 0.9626, val acc: 0.5902  (best train acc: 0.6116, best val acc: 0.6155)\n",
      "[Epoch: 6680] train loss: 0.9736, train acc: 0.6006, val loss: 0.9530, val acc: 0.6142  (best train acc: 0.6116, best val acc: 0.6155)\n",
      "[Epoch: 6700] train loss: 0.9902, train acc: 0.5915, val loss: 0.9563, val acc: 0.6142  (best train acc: 0.6116, best val acc: 0.6155)\n",
      "[Epoch: 6720] train loss: 0.9782, train acc: 0.5877, val loss: 0.9584, val acc: 0.6101  (best train acc: 0.6116, best val acc: 0.6155)\n",
      "[Epoch: 6740] train loss: 0.9809, train acc: 0.5990, val loss: 0.9533, val acc: 0.6105  (best train acc: 0.6116, best val acc: 0.6155)\n",
      "[Epoch: 6760] train loss: 0.9593, train acc: 0.6054, val loss: 0.9506, val acc: 0.6040  (best train acc: 0.6116, best val acc: 0.6155)\n",
      "[Epoch: 6780] train loss: 0.9806, train acc: 0.5977, val loss: 0.9550, val acc: 0.5976  (best train acc: 0.6116, best val acc: 0.6155)\n",
      "[Epoch: 6800] train loss: 0.9974, train acc: 0.5802, val loss: 0.9492, val acc: 0.6128  (best train acc: 0.6116, best val acc: 0.6155)\n",
      "[Epoch: 6820] train loss: 1.0097, train acc: 0.5864, val loss: 0.9604, val acc: 0.5889  (best train acc: 0.6116, best val acc: 0.6162)\n",
      "[Epoch: 6840] train loss: 0.9625, train acc: 0.6077, val loss: 0.9655, val acc: 0.5953  (best train acc: 0.6116, best val acc: 0.6162)\n",
      "[Epoch: 6860] train loss: 0.9609, train acc: 0.6100, val loss: 0.9471, val acc: 0.6067  (best train acc: 0.6147, best val acc: 0.6169)\n",
      "[Epoch: 6880] train loss: 0.9795, train acc: 0.5947, val loss: 0.9470, val acc: 0.6132  (best train acc: 0.6147, best val acc: 0.6169)\n",
      "[Epoch: 6900] train loss: 0.9710, train acc: 0.5964, val loss: 0.9465, val acc: 0.6152  (best train acc: 0.6147, best val acc: 0.6169)\n",
      "[Epoch: 6920] train loss: 0.9713, train acc: 0.6016, val loss: 0.9509, val acc: 0.6135  (best train acc: 0.6147, best val acc: 0.6169)\n",
      "[Epoch: 6940] train loss: 0.9813, train acc: 0.5964, val loss: 0.9456, val acc: 0.6098  (best train acc: 0.6147, best val acc: 0.6169)\n",
      "[Epoch: 6960] train loss: 0.9711, train acc: 0.5995, val loss: 0.9452, val acc: 0.6138  (best train acc: 0.6147, best val acc: 0.6169)\n",
      "[Epoch: 6980] train loss: 0.9674, train acc: 0.5970, val loss: 0.9479, val acc: 0.6165  (best train acc: 0.6147, best val acc: 0.6172)\n",
      "[Epoch: 7000] train loss: 0.9776, train acc: 0.6003, val loss: 0.9559, val acc: 0.6091  (best train acc: 0.6147, best val acc: 0.6172)\n",
      "[Epoch: 7020] train loss: 0.9641, train acc: 0.6006, val loss: 0.9428, val acc: 0.6128  (best train acc: 0.6147, best val acc: 0.6172)\n",
      "[Epoch: 7040] train loss: 0.9976, train acc: 0.5808, val loss: 0.9786, val acc: 0.5862  (best train acc: 0.6147, best val acc: 0.6172)\n",
      "[Epoch: 7060] train loss: 0.9799, train acc: 0.5930, val loss: 0.9454, val acc: 0.6078  (best train acc: 0.6147, best val acc: 0.6172)\n",
      "[Epoch: 7080] train loss: 0.9736, train acc: 0.6001, val loss: 0.9457, val acc: 0.6155  (best train acc: 0.6147, best val acc: 0.6175)\n",
      "[Epoch: 7100] train loss: 0.9891, train acc: 0.5959, val loss: 0.9537, val acc: 0.5912  (best train acc: 0.6147, best val acc: 0.6175)\n",
      "[Epoch: 7120] train loss: 0.9706, train acc: 0.6007, val loss: 0.9416, val acc: 0.6152  (best train acc: 0.6147, best val acc: 0.6175)\n",
      "[Epoch: 7140] train loss: 0.9743, train acc: 0.5980, val loss: 0.9500, val acc: 0.6138  (best train acc: 0.6147, best val acc: 0.6175)\n",
      "[Epoch: 7160] train loss: 0.9578, train acc: 0.6067, val loss: 0.9416, val acc: 0.6081  (best train acc: 0.6147, best val acc: 0.6185)\n",
      "[Epoch: 7180] train loss: 0.9507, train acc: 0.6116, val loss: 0.9406, val acc: 0.6101  (best train acc: 0.6147, best val acc: 0.6192)\n",
      "[Epoch: 7200] train loss: 0.9461, train acc: 0.6172, val loss: 0.9398, val acc: 0.6199  (best train acc: 0.6172, best val acc: 0.6199)\n",
      "[Epoch: 7220] train loss: 0.9955, train acc: 0.5883, val loss: 0.9391, val acc: 0.6155  (best train acc: 0.6172, best val acc: 0.6199)\n",
      "[Epoch: 7240] train loss: 0.9833, train acc: 0.5909, val loss: 0.9405, val acc: 0.6108  (best train acc: 0.6172, best val acc: 0.6199)\n",
      "[Epoch: 7260] train loss: 0.9885, train acc: 0.5955, val loss: 0.9489, val acc: 0.5983  (best train acc: 0.6172, best val acc: 0.6199)\n",
      "[Epoch: 7280] train loss: 0.9553, train acc: 0.6073, val loss: 0.9361, val acc: 0.6125  (best train acc: 0.6172, best val acc: 0.6202)\n",
      "[Epoch: 7300] train loss: 0.9563, train acc: 0.6057, val loss: 0.9365, val acc: 0.6142  (best train acc: 0.6172, best val acc: 0.6202)\n",
      "[Epoch: 7320] train loss: 0.9776, train acc: 0.5958, val loss: 0.9461, val acc: 0.6138  (best train acc: 0.6172, best val acc: 0.6212)\n",
      "[Epoch: 7340] train loss: 1.0010, train acc: 0.5812, val loss: 0.9364, val acc: 0.6125  (best train acc: 0.6172, best val acc: 0.6212)\n",
      "[Epoch: 7360] train loss: 0.9952, train acc: 0.5735, val loss: 0.9370, val acc: 0.6132  (best train acc: 0.6172, best val acc: 0.6212)\n",
      "[Epoch: 7380] train loss: 0.9526, train acc: 0.6065, val loss: 0.9379, val acc: 0.6074  (best train acc: 0.6172, best val acc: 0.6212)\n",
      "[Epoch: 7400] train loss: 0.9626, train acc: 0.6019, val loss: 0.9369, val acc: 0.6111  (best train acc: 0.6176, best val acc: 0.6233)\n",
      "[Epoch: 7420] train loss: 0.9761, train acc: 0.5915, val loss: 0.9367, val acc: 0.6199  (best train acc: 0.6176, best val acc: 0.6233)\n",
      "[Epoch: 7440] train loss: 0.9503, train acc: 0.6080, val loss: 0.9316, val acc: 0.6152  (best train acc: 0.6176, best val acc: 0.6233)\n",
      "[Epoch: 7460] train loss: 0.9558, train acc: 0.6061, val loss: 0.9355, val acc: 0.6101  (best train acc: 0.6176, best val acc: 0.6233)\n",
      "[Epoch: 7480] train loss: 0.9555, train acc: 0.6035, val loss: 0.9514, val acc: 0.5997  (best train acc: 0.6176, best val acc: 0.6239)\n",
      "[Epoch: 7500] train loss: 0.9484, train acc: 0.6129, val loss: 0.9453, val acc: 0.6098  (best train acc: 0.6176, best val acc: 0.6239)\n",
      "[Epoch: 7520] train loss: 0.9865, train acc: 0.5847, val loss: 0.9378, val acc: 0.6101  (best train acc: 0.6176, best val acc: 0.6239)\n",
      "[Epoch: 7540] train loss: 0.9744, train acc: 0.6000, val loss: 0.9386, val acc: 0.5997  (best train acc: 0.6176, best val acc: 0.6239)\n",
      "[Epoch: 7560] train loss: 0.9537, train acc: 0.6127, val loss: 0.9418, val acc: 0.6111  (best train acc: 0.6176, best val acc: 0.6239)\n",
      "[Epoch: 7580] train loss: 0.9733, train acc: 0.5993, val loss: 0.9298, val acc: 0.6169  (best train acc: 0.6176, best val acc: 0.6239)\n",
      "[Epoch: 7600] train loss: 0.9626, train acc: 0.6072, val loss: 0.9296, val acc: 0.6182  (best train acc: 0.6176, best val acc: 0.6239)\n",
      "[Epoch: 7620] train loss: 1.0005, train acc: 0.5797, val loss: 0.9311, val acc: 0.6159  (best train acc: 0.6197, best val acc: 0.6239)\n",
      "[Epoch: 7640] train loss: 0.9629, train acc: 0.6064, val loss: 0.9415, val acc: 0.6094  (best train acc: 0.6197, best val acc: 0.6239)\n",
      "[Epoch: 7660] train loss: 0.9490, train acc: 0.6053, val loss: 0.9417, val acc: 0.6078  (best train acc: 0.6197, best val acc: 0.6239)\n",
      "[Epoch: 7680] train loss: 0.9463, train acc: 0.6095, val loss: 0.9284, val acc: 0.6196  (best train acc: 0.6197, best val acc: 0.6239)\n",
      "[Epoch: 7700] train loss: 0.9698, train acc: 0.6003, val loss: 0.9669, val acc: 0.5912  (best train acc: 0.6197, best val acc: 0.6256)\n",
      "[Epoch: 7720] train loss: 0.9462, train acc: 0.6069, val loss: 0.9359, val acc: 0.6179  (best train acc: 0.6197, best val acc: 0.6256)\n",
      "[Epoch: 7740] train loss: 0.9693, train acc: 0.5990, val loss: 0.9324, val acc: 0.6209  (best train acc: 0.6197, best val acc: 0.6256)\n",
      "[Epoch: 7760] train loss: 0.9768, train acc: 0.5971, val loss: 0.9263, val acc: 0.6179  (best train acc: 0.6197, best val acc: 0.6256)\n",
      "[Epoch: 7780] train loss: 0.9637, train acc: 0.5993, val loss: 0.9521, val acc: 0.6040  (best train acc: 0.6197, best val acc: 0.6256)\n",
      "[Epoch: 7800] train loss: 0.9706, train acc: 0.6001, val loss: 0.9302, val acc: 0.6118  (best train acc: 0.6197, best val acc: 0.6256)\n",
      "[Epoch: 7820] train loss: 0.9773, train acc: 0.5956, val loss: 0.9253, val acc: 0.6229  (best train acc: 0.6218, best val acc: 0.6256)\n",
      "[Epoch: 7840] train loss: 0.9522, train acc: 0.6034, val loss: 0.9760, val acc: 0.5855  (best train acc: 0.6233, best val acc: 0.6256)\n",
      "[Epoch: 7860] train loss: 0.9320, train acc: 0.6173, val loss: 0.9269, val acc: 0.6169  (best train acc: 0.6233, best val acc: 0.6256)\n",
      "[Epoch: 7880] train loss: 0.9508, train acc: 0.6068, val loss: 0.9240, val acc: 0.6219  (best train acc: 0.6233, best val acc: 0.6256)\n",
      "[Epoch: 7900] train loss: 0.9571, train acc: 0.6066, val loss: 0.9251, val acc: 0.6256  (best train acc: 0.6233, best val acc: 0.6263)\n",
      "[Epoch: 7920] train loss: 0.9466, train acc: 0.6131, val loss: 0.9330, val acc: 0.6169  (best train acc: 0.6233, best val acc: 0.6263)\n",
      "[Epoch: 7940] train loss: 0.9507, train acc: 0.6085, val loss: 0.9373, val acc: 0.6121  (best train acc: 0.6233, best val acc: 0.6263)\n",
      "[Epoch: 7960] train loss: 0.9566, train acc: 0.6023, val loss: 0.9400, val acc: 0.5973  (best train acc: 0.6233, best val acc: 0.6263)\n",
      "[Epoch: 7980] train loss: 0.9601, train acc: 0.6041, val loss: 0.9258, val acc: 0.6253  (best train acc: 0.6233, best val acc: 0.6263)\n",
      "[Epoch: 8000] train loss: 0.9302, train acc: 0.6199, val loss: 0.9220, val acc: 0.6266  (best train acc: 0.6233, best val acc: 0.6283)\n",
      "[Epoch: 8020] train loss: 0.9787, train acc: 0.5995, val loss: 0.9216, val acc: 0.6209  (best train acc: 0.6233, best val acc: 0.6293)\n",
      "[Epoch: 8040] train loss: 0.9865, train acc: 0.5897, val loss: 0.9205, val acc: 0.6236  (best train acc: 0.6233, best val acc: 0.6293)\n",
      "[Epoch: 8060] train loss: 0.9433, train acc: 0.6113, val loss: 0.9206, val acc: 0.6260  (best train acc: 0.6233, best val acc: 0.6293)\n",
      "[Epoch: 8080] train loss: 0.9544, train acc: 0.6064, val loss: 0.9249, val acc: 0.6236  (best train acc: 0.6233, best val acc: 0.6293)\n",
      "[Epoch: 8100] train loss: 0.9414, train acc: 0.6132, val loss: 0.9199, val acc: 0.6206  (best train acc: 0.6233, best val acc: 0.6293)\n",
      "[Epoch: 8120] train loss: 0.9536, train acc: 0.6053, val loss: 0.9201, val acc: 0.6179  (best train acc: 0.6233, best val acc: 0.6293)\n",
      "[Epoch: 8140] train loss: 0.9489, train acc: 0.6090, val loss: 0.9198, val acc: 0.6290  (best train acc: 0.6262, best val acc: 0.6293)\n",
      "[Epoch: 8160] train loss: 0.9496, train acc: 0.6079, val loss: 0.9175, val acc: 0.6253  (best train acc: 0.6262, best val acc: 0.6300)\n",
      "[Epoch: 8180] train loss: 0.9816, train acc: 0.5861, val loss: 0.9189, val acc: 0.6260  (best train acc: 0.6262, best val acc: 0.6300)\n",
      "[Epoch: 8200] train loss: 0.9739, train acc: 0.5997, val loss: 0.9401, val acc: 0.6051  (best train acc: 0.6262, best val acc: 0.6300)\n",
      "[Epoch: 8220] train loss: 0.9655, train acc: 0.6001, val loss: 0.9405, val acc: 0.6020  (best train acc: 0.6262, best val acc: 0.6300)\n",
      "[Epoch: 8240] train loss: 0.9506, train acc: 0.6112, val loss: 0.9171, val acc: 0.6253  (best train acc: 0.6262, best val acc: 0.6300)\n",
      "[Epoch: 8260] train loss: 0.9473, train acc: 0.6188, val loss: 0.9176, val acc: 0.6307  (best train acc: 0.6262, best val acc: 0.6307)\n",
      "[Epoch: 8280] train loss: 0.9382, train acc: 0.6145, val loss: 0.9290, val acc: 0.6148  (best train acc: 0.6262, best val acc: 0.6317)\n",
      "[Epoch: 8300] train loss: 0.9694, train acc: 0.5951, val loss: 0.9290, val acc: 0.6148  (best train acc: 0.6262, best val acc: 0.6317)\n",
      "[Epoch: 8320] train loss: 0.9658, train acc: 0.5965, val loss: 0.9399, val acc: 0.6027  (best train acc: 0.6262, best val acc: 0.6317)\n",
      "[Epoch: 8340] train loss: 0.9605, train acc: 0.6061, val loss: 0.9183, val acc: 0.6219  (best train acc: 0.6262, best val acc: 0.6317)\n",
      "[Epoch: 8360] train loss: 0.9424, train acc: 0.6156, val loss: 0.9150, val acc: 0.6280  (best train acc: 0.6262, best val acc: 0.6317)\n",
      "[Epoch: 8380] train loss: 0.9843, train acc: 0.5844, val loss: 0.9274, val acc: 0.6054  (best train acc: 0.6262, best val acc: 0.6324)\n",
      "[Epoch: 8400] train loss: 0.9555, train acc: 0.6037, val loss: 0.9292, val acc: 0.6115  (best train acc: 0.6262, best val acc: 0.6324)\n",
      "[Epoch: 8420] train loss: 0.9501, train acc: 0.6029, val loss: 0.9328, val acc: 0.5997  (best train acc: 0.6262, best val acc: 0.6324)\n",
      "[Epoch: 8440] train loss: 0.9538, train acc: 0.6119, val loss: 0.9221, val acc: 0.6239  (best train acc: 0.6262, best val acc: 0.6324)\n",
      "[Epoch: 8460] train loss: 0.9628, train acc: 0.6015, val loss: 0.9174, val acc: 0.6277  (best train acc: 0.6262, best val acc: 0.6324)\n",
      "[Epoch: 8480] train loss: 0.9580, train acc: 0.6031, val loss: 0.9133, val acc: 0.6219  (best train acc: 0.6262, best val acc: 0.6324)\n",
      "[Epoch: 8500] train loss: 0.9454, train acc: 0.6076, val loss: 0.9225, val acc: 0.6074  (best train acc: 0.6262, best val acc: 0.6341)\n",
      "[Epoch: 8520] train loss: 0.9500, train acc: 0.6095, val loss: 0.9113, val acc: 0.6304  (best train acc: 0.6276, best val acc: 0.6341)\n",
      "[Epoch: 8540] train loss: 0.9566, train acc: 0.6037, val loss: 0.9109, val acc: 0.6320  (best train acc: 0.6276, best val acc: 0.6341)\n",
      "[Epoch: 8560] train loss: 0.9589, train acc: 0.6032, val loss: 0.9125, val acc: 0.6226  (best train acc: 0.6276, best val acc: 0.6341)\n",
      "[Epoch: 8580] train loss: 0.9814, train acc: 0.5986, val loss: 0.9093, val acc: 0.6307  (best train acc: 0.6293, best val acc: 0.6341)\n",
      "[Epoch: 8600] train loss: 0.9498, train acc: 0.5991, val loss: 0.9263, val acc: 0.6108  (best train acc: 0.6293, best val acc: 0.6341)\n",
      "[Epoch: 8620] train loss: 0.9508, train acc: 0.6073, val loss: 0.9100, val acc: 0.6273  (best train acc: 0.6293, best val acc: 0.6341)\n",
      "[Epoch: 8640] train loss: 0.9215, train acc: 0.6265, val loss: 0.9136, val acc: 0.6307  (best train acc: 0.6293, best val acc: 0.6341)\n",
      "[Epoch: 8660] train loss: 0.9608, train acc: 0.5957, val loss: 0.9240, val acc: 0.6040  (best train acc: 0.6293, best val acc: 0.6347)\n",
      "[Epoch: 8680] train loss: 0.9642, train acc: 0.5974, val loss: 0.9497, val acc: 0.5936  (best train acc: 0.6293, best val acc: 0.6347)\n",
      "[Epoch: 8700] train loss: 0.9413, train acc: 0.6168, val loss: 0.9184, val acc: 0.6253  (best train acc: 0.6293, best val acc: 0.6347)\n",
      "[Epoch: 8720] train loss: 0.9586, train acc: 0.5976, val loss: 0.9093, val acc: 0.6256  (best train acc: 0.6293, best val acc: 0.6347)\n",
      "[Epoch: 8740] train loss: 0.9579, train acc: 0.6012, val loss: 0.9098, val acc: 0.6223  (best train acc: 0.6293, best val acc: 0.6347)\n",
      "[Epoch: 8760] train loss: 0.9345, train acc: 0.6148, val loss: 0.9257, val acc: 0.6145  (best train acc: 0.6293, best val acc: 0.6347)\n",
      "[Epoch: 8780] train loss: 0.9525, train acc: 0.6155, val loss: 0.9111, val acc: 0.6206  (best train acc: 0.6293, best val acc: 0.6347)\n",
      "[Epoch: 8800] train loss: 0.9876, train acc: 0.5817, val loss: 0.9093, val acc: 0.6290  (best train acc: 0.6293, best val acc: 0.6347)\n",
      "[Epoch: 8820] train loss: 0.9549, train acc: 0.5999, val loss: 0.9099, val acc: 0.6314  (best train acc: 0.6293, best val acc: 0.6347)\n",
      "[Epoch: 8840] train loss: 0.9529, train acc: 0.6097, val loss: 0.9081, val acc: 0.6344  (best train acc: 0.6293, best val acc: 0.6347)\n",
      "[Epoch: 8860] train loss: 0.9293, train acc: 0.6182, val loss: 0.9212, val acc: 0.6084  (best train acc: 0.6293, best val acc: 0.6364)\n",
      "[Epoch: 8880] train loss: 0.9514, train acc: 0.6150, val loss: 0.9043, val acc: 0.6310  (best train acc: 0.6293, best val acc: 0.6364)\n",
      "[Epoch: 8900] train loss: 0.9631, train acc: 0.6058, val loss: 0.9088, val acc: 0.6310  (best train acc: 0.6293, best val acc: 0.6364)\n",
      "[Epoch: 8920] train loss: 0.9558, train acc: 0.5997, val loss: 0.9108, val acc: 0.6169  (best train acc: 0.6293, best val acc: 0.6364)\n",
      "[Epoch: 8940] train loss: 0.9647, train acc: 0.6074, val loss: 0.9063, val acc: 0.6307  (best train acc: 0.6293, best val acc: 0.6364)\n",
      "[Epoch: 8960] train loss: 0.9671, train acc: 0.5969, val loss: 0.9044, val acc: 0.6293  (best train acc: 0.6293, best val acc: 0.6368)\n",
      "[Epoch: 8980] train loss: 0.9482, train acc: 0.6016, val loss: 0.9093, val acc: 0.6212  (best train acc: 0.6293, best val acc: 0.6368)\n",
      "[Epoch: 9000] train loss: 0.9763, train acc: 0.5922, val loss: 0.9031, val acc: 0.6317  (best train acc: 0.6293, best val acc: 0.6368)\n",
      "[Epoch: 9020] train loss: 0.9526, train acc: 0.6045, val loss: 0.9011, val acc: 0.6341  (best train acc: 0.6293, best val acc: 0.6368)\n",
      "[Epoch: 9040] train loss: 0.9453, train acc: 0.6082, val loss: 0.9016, val acc: 0.6337  (best train acc: 0.6293, best val acc: 0.6368)\n",
      "[Epoch: 9060] train loss: 0.9530, train acc: 0.6095, val loss: 0.9065, val acc: 0.6300  (best train acc: 0.6293, best val acc: 0.6368)\n",
      "[Epoch: 9080] train loss: 0.9441, train acc: 0.6082, val loss: 0.9044, val acc: 0.6253  (best train acc: 0.6293, best val acc: 0.6368)\n",
      "[Epoch: 9100] train loss: 0.9449, train acc: 0.6165, val loss: 0.9020, val acc: 0.6368  (best train acc: 0.6293, best val acc: 0.6371)\n",
      "[Epoch: 9120] train loss: 0.9292, train acc: 0.6191, val loss: 0.9205, val acc: 0.6159  (best train acc: 0.6293, best val acc: 0.6371)\n",
      "[Epoch: 9140] train loss: 0.9334, train acc: 0.6150, val loss: 0.9010, val acc: 0.6327  (best train acc: 0.6293, best val acc: 0.6371)\n",
      "[Epoch: 9160] train loss: 0.9344, train acc: 0.6152, val loss: 0.9039, val acc: 0.6253  (best train acc: 0.6294, best val acc: 0.6371)\n",
      "[Epoch: 9180] train loss: 0.9527, train acc: 0.6066, val loss: 0.9021, val acc: 0.6300  (best train acc: 0.6294, best val acc: 0.6374)\n",
      "[Epoch: 9200] train loss: 0.9585, train acc: 0.5928, val loss: 0.8998, val acc: 0.6337  (best train acc: 0.6294, best val acc: 0.6378)\n",
      "[Epoch: 9220] train loss: 0.9287, train acc: 0.6186, val loss: 0.8986, val acc: 0.6354  (best train acc: 0.6294, best val acc: 0.6381)\n",
      "[Epoch: 9240] train loss: 0.9177, train acc: 0.6270, val loss: 0.9069, val acc: 0.6179  (best train acc: 0.6294, best val acc: 0.6381)\n",
      "[Epoch: 9260] train loss: 0.9551, train acc: 0.6104, val loss: 0.9050, val acc: 0.6202  (best train acc: 0.6294, best val acc: 0.6395)\n",
      "[Epoch: 9280] train loss: 0.9293, train acc: 0.6144, val loss: 0.9008, val acc: 0.6327  (best train acc: 0.6294, best val acc: 0.6395)\n",
      "[Epoch: 9300] train loss: 0.9170, train acc: 0.6200, val loss: 0.9357, val acc: 0.6020  (best train acc: 0.6294, best val acc: 0.6395)\n",
      "[Epoch: 9320] train loss: 0.9378, train acc: 0.6095, val loss: 0.8979, val acc: 0.6364  (best train acc: 0.6294, best val acc: 0.6395)\n",
      "[Epoch: 9340] train loss: 0.9272, train acc: 0.6199, val loss: 0.8999, val acc: 0.6253  (best train acc: 0.6294, best val acc: 0.6395)\n",
      "[Epoch: 9360] train loss: 0.9779, train acc: 0.5932, val loss: 0.9030, val acc: 0.6239  (best train acc: 0.6301, best val acc: 0.6395)\n",
      "[Epoch: 9380] train loss: 0.9494, train acc: 0.6134, val loss: 0.9170, val acc: 0.6182  (best train acc: 0.6301, best val acc: 0.6395)\n",
      "[Epoch: 9400] train loss: 0.9397, train acc: 0.6057, val loss: 0.9069, val acc: 0.6307  (best train acc: 0.6301, best val acc: 0.6395)\n",
      "[Epoch: 9420] train loss: 0.9709, train acc: 0.5962, val loss: 0.9310, val acc: 0.5980  (best train acc: 0.6301, best val acc: 0.6395)\n",
      "[Epoch: 9440] train loss: 0.9140, train acc: 0.6220, val loss: 0.9130, val acc: 0.6243  (best train acc: 0.6301, best val acc: 0.6398)\n",
      "[Epoch: 9460] train loss: 0.9542, train acc: 0.6048, val loss: 0.9076, val acc: 0.6152  (best train acc: 0.6301, best val acc: 0.6398)\n",
      "[Epoch: 9480] train loss: 0.9336, train acc: 0.6190, val loss: 0.8972, val acc: 0.6384  (best train acc: 0.6301, best val acc: 0.6398)\n",
      "[Epoch: 9500] train loss: 0.9339, train acc: 0.6079, val loss: 0.8948, val acc: 0.6371  (best train acc: 0.6301, best val acc: 0.6398)\n",
      "[Epoch: 9520] train loss: 0.9544, train acc: 0.6038, val loss: 0.8936, val acc: 0.6374  (best train acc: 0.6301, best val acc: 0.6401)\n",
      "[Epoch: 9540] train loss: 0.9426, train acc: 0.6069, val loss: 0.8949, val acc: 0.6374  (best train acc: 0.6301, best val acc: 0.6401)\n",
      "[Epoch: 9560] train loss: 0.9482, train acc: 0.6095, val loss: 0.9003, val acc: 0.6300  (best train acc: 0.6301, best val acc: 0.6401)\n",
      "[Epoch: 9580] train loss: 0.9313, train acc: 0.6161, val loss: 0.8937, val acc: 0.6310  (best train acc: 0.6301, best val acc: 0.6401)\n",
      "[Epoch: 9600] train loss: 0.9810, train acc: 0.5940, val loss: 0.8987, val acc: 0.6246  (best train acc: 0.6301, best val acc: 0.6401)\n",
      "[Epoch: 9620] train loss: 0.9386, train acc: 0.6195, val loss: 0.9245, val acc: 0.6105  (best train acc: 0.6301, best val acc: 0.6401)\n",
      "[Epoch: 9640] train loss: 0.9185, train acc: 0.6238, val loss: 0.8970, val acc: 0.6381  (best train acc: 0.6301, best val acc: 0.6408)\n",
      "[Epoch: 9660] train loss: 0.9339, train acc: 0.6087, val loss: 0.8947, val acc: 0.6378  (best train acc: 0.6301, best val acc: 0.6408)\n",
      "[Epoch: 9680] train loss: 0.9478, train acc: 0.6100, val loss: 0.9173, val acc: 0.6067  (best train acc: 0.6301, best val acc: 0.6408)\n",
      "[Epoch: 9700] train loss: 0.9472, train acc: 0.6077, val loss: 0.8976, val acc: 0.6391  (best train acc: 0.6301, best val acc: 0.6408)\n",
      "[Epoch: 9720] train loss: 0.9281, train acc: 0.6122, val loss: 0.9039, val acc: 0.6283  (best train acc: 0.6301, best val acc: 0.6408)\n",
      "[Epoch: 9740] train loss: 0.9247, train acc: 0.6142, val loss: 0.9035, val acc: 0.6233  (best train acc: 0.6301, best val acc: 0.6408)\n",
      "[Epoch: 9760] train loss: 0.9221, train acc: 0.6207, val loss: 0.9002, val acc: 0.6260  (best train acc: 0.6301, best val acc: 0.6408)\n",
      "[Epoch: 9780] train loss: 0.9277, train acc: 0.6179, val loss: 0.8905, val acc: 0.6374  (best train acc: 0.6311, best val acc: 0.6408)\n",
      "[Epoch: 9800] train loss: 0.9405, train acc: 0.6105, val loss: 0.8913, val acc: 0.6310  (best train acc: 0.6311, best val acc: 0.6408)\n",
      "[Epoch: 9820] train loss: 0.9272, train acc: 0.6162, val loss: 0.8909, val acc: 0.6378  (best train acc: 0.6319, best val acc: 0.6408)\n",
      "[Epoch: 9840] train loss: 0.9438, train acc: 0.6050, val loss: 0.8912, val acc: 0.6324  (best train acc: 0.6331, best val acc: 0.6411)\n",
      "[Epoch: 9860] train loss: 0.9622, train acc: 0.6001, val loss: 0.8932, val acc: 0.6290  (best train acc: 0.6331, best val acc: 0.6411)\n",
      "[Epoch: 9880] train loss: 0.9557, train acc: 0.6053, val loss: 0.9271, val acc: 0.6027  (best train acc: 0.6331, best val acc: 0.6418)\n",
      "[Epoch: 9900] train loss: 0.9621, train acc: 0.6025, val loss: 0.8951, val acc: 0.6405  (best train acc: 0.6331, best val acc: 0.6418)\n",
      "[Epoch: 9920] train loss: 0.9269, train acc: 0.6183, val loss: 0.8901, val acc: 0.6398  (best train acc: 0.6331, best val acc: 0.6418)\n",
      "[Epoch: 9940] train loss: 0.9145, train acc: 0.6259, val loss: 0.9014, val acc: 0.6293  (best train acc: 0.6331, best val acc: 0.6418)\n",
      "[Epoch: 9960] train loss: 0.9477, train acc: 0.6050, val loss: 0.8974, val acc: 0.6253  (best train acc: 0.6331, best val acc: 0.6418)\n",
      "[Epoch: 9980] train loss: 0.9414, train acc: 0.6151, val loss: 0.8912, val acc: 0.6290  (best train acc: 0.6331, best val acc: 0.6418)\n",
      "[Epoch: 10000] train loss: 0.9399, train acc: 0.6093, val loss: 0.8948, val acc: 0.6253  (best train acc: 0.6331, best val acc: 0.6418)\n",
      "[Epoch: 10020] train loss: 0.9396, train acc: 0.6176, val loss: 0.8972, val acc: 0.6337  (best train acc: 0.6340, best val acc: 0.6425)\n",
      "[Epoch: 10040] train loss: 0.9366, train acc: 0.6102, val loss: 0.8897, val acc: 0.6395  (best train acc: 0.6340, best val acc: 0.6425)\n",
      "[Epoch: 10060] train loss: 0.9358, train acc: 0.6099, val loss: 0.8882, val acc: 0.6374  (best train acc: 0.6340, best val acc: 0.6425)\n",
      "[Epoch: 10080] train loss: 0.9263, train acc: 0.6149, val loss: 0.9127, val acc: 0.6159  (best train acc: 0.6340, best val acc: 0.6425)\n",
      "[Epoch: 10100] train loss: 0.9297, train acc: 0.6131, val loss: 0.8949, val acc: 0.6378  (best train acc: 0.6340, best val acc: 0.6425)\n",
      "[Epoch: 10120] train loss: 0.9696, train acc: 0.5994, val loss: 0.9220, val acc: 0.6024  (best train acc: 0.6340, best val acc: 0.6425)\n",
      "[Epoch: 10140] train loss: 0.9505, train acc: 0.6023, val loss: 0.8894, val acc: 0.6428  (best train acc: 0.6340, best val acc: 0.6449)\n",
      "[Epoch: 10160] train loss: 0.9048, train acc: 0.6303, val loss: 0.8878, val acc: 0.6374  (best train acc: 0.6340, best val acc: 0.6449)\n",
      "[Epoch: 10180] train loss: 0.9284, train acc: 0.6157, val loss: 0.8918, val acc: 0.6277  (best train acc: 0.6344, best val acc: 0.6449)\n",
      "[Epoch: 10200] train loss: 0.9454, train acc: 0.6053, val loss: 0.8881, val acc: 0.6324  (best train acc: 0.6344, best val acc: 0.6449)\n",
      "[Epoch: 10220] train loss: 0.9469, train acc: 0.6097, val loss: 0.9057, val acc: 0.6293  (best train acc: 0.6344, best val acc: 0.6449)\n",
      "[Epoch: 10240] train loss: 0.9201, train acc: 0.6264, val loss: 0.8907, val acc: 0.6378  (best train acc: 0.6344, best val acc: 0.6449)\n",
      "[Epoch: 10260] train loss: 0.9343, train acc: 0.6083, val loss: 0.8934, val acc: 0.6347  (best train acc: 0.6344, best val acc: 0.6449)\n",
      "[Epoch: 10280] train loss: 0.9275, train acc: 0.6100, val loss: 0.9113, val acc: 0.6142  (best train acc: 0.6344, best val acc: 0.6449)\n",
      "[Epoch: 10300] train loss: 0.9347, train acc: 0.6139, val loss: 0.8903, val acc: 0.6405  (best train acc: 0.6344, best val acc: 0.6449)\n",
      "[Epoch: 10320] train loss: 0.9296, train acc: 0.6118, val loss: 0.8886, val acc: 0.6324  (best train acc: 0.6348, best val acc: 0.6449)\n",
      "[Epoch: 10340] train loss: 0.9202, train acc: 0.6266, val loss: 0.8916, val acc: 0.6273  (best train acc: 0.6391, best val acc: 0.6449)\n",
      "[Epoch: 10360] train loss: 0.9412, train acc: 0.6050, val loss: 0.8867, val acc: 0.6341  (best train acc: 0.6391, best val acc: 0.6449)\n",
      "[Epoch: 10380] train loss: 0.9410, train acc: 0.6037, val loss: 0.9127, val acc: 0.6135  (best train acc: 0.6391, best val acc: 0.6449)\n",
      "[Epoch: 10400] train loss: 0.9078, train acc: 0.6257, val loss: 0.8831, val acc: 0.6398  (best train acc: 0.6391, best val acc: 0.6455)\n",
      "[Epoch: 10420] train loss: 0.9366, train acc: 0.6096, val loss: 0.8920, val acc: 0.6341  (best train acc: 0.6391, best val acc: 0.6455)\n",
      "[Epoch: 10440] train loss: 0.9673, train acc: 0.5958, val loss: 0.9051, val acc: 0.6162  (best train acc: 0.6391, best val acc: 0.6455)\n",
      "[Epoch: 10460] train loss: 0.9132, train acc: 0.6243, val loss: 0.8829, val acc: 0.6371  (best train acc: 0.6391, best val acc: 0.6455)\n",
      "[Epoch: 10480] train loss: 0.9357, train acc: 0.6162, val loss: 0.8903, val acc: 0.6283  (best train acc: 0.6397, best val acc: 0.6455)\n",
      "[Epoch: 10500] train loss: 0.9442, train acc: 0.6066, val loss: 0.9085, val acc: 0.6155  (best train acc: 0.6397, best val acc: 0.6455)\n",
      "[Epoch: 10520] train loss: 0.9147, train acc: 0.6214, val loss: 0.8837, val acc: 0.6432  (best train acc: 0.6397, best val acc: 0.6455)\n",
      "[Epoch: 10540] train loss: 0.9185, train acc: 0.6257, val loss: 0.8809, val acc: 0.6378  (best train acc: 0.6397, best val acc: 0.6455)\n",
      "[Epoch: 10560] train loss: 0.9174, train acc: 0.6237, val loss: 0.8828, val acc: 0.6381  (best train acc: 0.6397, best val acc: 0.6455)\n",
      "[Epoch: 10580] train loss: 0.9475, train acc: 0.6013, val loss: 0.8855, val acc: 0.6422  (best train acc: 0.6397, best val acc: 0.6455)\n",
      "[Epoch: 10600] train loss: 0.9420, train acc: 0.6105, val loss: 0.9272, val acc: 0.6027  (best train acc: 0.6397, best val acc: 0.6455)\n",
      "[Epoch: 10620] train loss: 0.9379, train acc: 0.6072, val loss: 0.8918, val acc: 0.6263  (best train acc: 0.6397, best val acc: 0.6455)\n",
      "[Epoch: 10640] train loss: 0.9172, train acc: 0.6223, val loss: 0.8810, val acc: 0.6388  (best train acc: 0.6397, best val acc: 0.6455)\n",
      "[Epoch: 10660] train loss: 0.9249, train acc: 0.6179, val loss: 0.8911, val acc: 0.6381  (best train acc: 0.6397, best val acc: 0.6455)\n",
      "[Epoch: 10680] train loss: 0.9249, train acc: 0.6174, val loss: 0.9039, val acc: 0.6223  (best train acc: 0.6397, best val acc: 0.6455)\n",
      "[Epoch: 10700] train loss: 0.9444, train acc: 0.6124, val loss: 0.9409, val acc: 0.5990  (best train acc: 0.6397, best val acc: 0.6455)\n",
      "[Epoch: 10720] train loss: 0.9887, train acc: 0.5867, val loss: 0.8960, val acc: 0.6364  (best train acc: 0.6397, best val acc: 0.6455)\n",
      "[Epoch: 10740] train loss: 0.9235, train acc: 0.6179, val loss: 0.8841, val acc: 0.6344  (best train acc: 0.6397, best val acc: 0.6455)\n",
      "[Epoch: 10760] train loss: 0.9222, train acc: 0.6195, val loss: 0.8838, val acc: 0.6425  (best train acc: 0.6397, best val acc: 0.6455)\n",
      "[Epoch: 10780] train loss: 0.9461, train acc: 0.6067, val loss: 0.8996, val acc: 0.6192  (best train acc: 0.6397, best val acc: 0.6455)\n",
      "[Epoch: 10800] train loss: 0.9119, train acc: 0.6242, val loss: 0.8916, val acc: 0.6374  (best train acc: 0.6397, best val acc: 0.6455)\n",
      "[Epoch: 10820] train loss: 0.9472, train acc: 0.5964, val loss: 0.8863, val acc: 0.6347  (best train acc: 0.6397, best val acc: 0.6455)\n",
      "[Epoch: 10840] train loss: 0.9196, train acc: 0.6151, val loss: 0.8821, val acc: 0.6391  (best train acc: 0.6397, best val acc: 0.6455)\n",
      "[Epoch: 10860] train loss: 0.9477, train acc: 0.6053, val loss: 0.8811, val acc: 0.6384  (best train acc: 0.6397, best val acc: 0.6455)\n",
      "[Epoch: 10880] train loss: 0.9493, train acc: 0.6036, val loss: 0.8915, val acc: 0.6331  (best train acc: 0.6418, best val acc: 0.6455)\n",
      "[Epoch: 10900] train loss: 0.9354, train acc: 0.6096, val loss: 0.8831, val acc: 0.6368  (best train acc: 0.6418, best val acc: 0.6455)\n",
      "[Epoch: 10920] train loss: 0.9398, train acc: 0.6048, val loss: 0.8792, val acc: 0.6401  (best train acc: 0.6418, best val acc: 0.6455)\n",
      "[Epoch: 10940] train loss: 0.9350, train acc: 0.6033, val loss: 0.8994, val acc: 0.6223  (best train acc: 0.6418, best val acc: 0.6455)\n",
      "[Epoch: 10960] train loss: 0.8985, train acc: 0.6324, val loss: 0.8784, val acc: 0.6418  (best train acc: 0.6418, best val acc: 0.6455)\n",
      "[Epoch: 10980] train loss: 0.9048, train acc: 0.6231, val loss: 0.8774, val acc: 0.6411  (best train acc: 0.6418, best val acc: 0.6455)\n",
      "[Epoch: 11000] train loss: 0.9238, train acc: 0.6241, val loss: 0.8769, val acc: 0.6374  (best train acc: 0.6458, best val acc: 0.6455)\n",
      "[Epoch: 11020] train loss: 0.9227, train acc: 0.6164, val loss: 0.8893, val acc: 0.6344  (best train acc: 0.6458, best val acc: 0.6455)\n",
      "[Epoch: 11040] train loss: 0.9283, train acc: 0.6177, val loss: 0.8764, val acc: 0.6398  (best train acc: 0.6458, best val acc: 0.6455)\n",
      "[Epoch: 11060] train loss: 0.9278, train acc: 0.6131, val loss: 0.8777, val acc: 0.6391  (best train acc: 0.6458, best val acc: 0.6465)\n",
      "[Epoch: 11080] train loss: 0.9091, train acc: 0.6240, val loss: 0.8821, val acc: 0.6422  (best train acc: 0.6458, best val acc: 0.6465)\n",
      "[Epoch: 11100] train loss: 0.9246, train acc: 0.6169, val loss: 0.8769, val acc: 0.6432  (best train acc: 0.6458, best val acc: 0.6465)\n",
      "[Epoch: 11120] train loss: 0.9176, train acc: 0.6257, val loss: 0.8834, val acc: 0.6337  (best train acc: 0.6458, best val acc: 0.6465)\n",
      "[Epoch: 11140] train loss: 1.0061, train acc: 0.5681, val loss: 0.8855, val acc: 0.6347  (best train acc: 0.6458, best val acc: 0.6465)\n",
      "[Epoch: 11160] train loss: 0.8893, train acc: 0.6332, val loss: 0.8751, val acc: 0.6425  (best train acc: 0.6458, best val acc: 0.6465)\n",
      "[Epoch: 11180] train loss: 0.9171, train acc: 0.6254, val loss: 0.8797, val acc: 0.6374  (best train acc: 0.6458, best val acc: 0.6465)\n",
      "[Epoch: 11200] train loss: 0.8998, train acc: 0.6245, val loss: 0.8845, val acc: 0.6401  (best train acc: 0.6458, best val acc: 0.6469)\n",
      "[Epoch: 11220] train loss: 0.9184, train acc: 0.6178, val loss: 0.8909, val acc: 0.6344  (best train acc: 0.6458, best val acc: 0.6469)\n",
      "[Epoch: 11240] train loss: 0.9247, train acc: 0.6218, val loss: 0.8772, val acc: 0.6388  (best train acc: 0.6458, best val acc: 0.6469)\n",
      "[Epoch: 11260] train loss: 0.8971, train acc: 0.6323, val loss: 0.8737, val acc: 0.6398  (best train acc: 0.6458, best val acc: 0.6469)\n",
      "[Epoch: 11280] train loss: 0.8961, train acc: 0.6335, val loss: 0.8789, val acc: 0.6445  (best train acc: 0.6458, best val acc: 0.6469)\n",
      "[Epoch: 11300] train loss: 0.9226, train acc: 0.6188, val loss: 0.9096, val acc: 0.6145  (best train acc: 0.6458, best val acc: 0.6479)\n",
      "[Epoch: 11320] train loss: 0.9295, train acc: 0.6160, val loss: 0.8756, val acc: 0.6449  (best train acc: 0.6458, best val acc: 0.6479)\n",
      "[Epoch: 11340] train loss: 0.8970, train acc: 0.6364, val loss: 0.9056, val acc: 0.6236  (best train acc: 0.6458, best val acc: 0.6479)\n",
      "[Epoch: 11360] train loss: 0.9264, train acc: 0.6160, val loss: 0.8991, val acc: 0.6243  (best train acc: 0.6458, best val acc: 0.6479)\n",
      "[Epoch: 11380] train loss: 0.9185, train acc: 0.6233, val loss: 0.8759, val acc: 0.6422  (best train acc: 0.6458, best val acc: 0.6479)\n",
      "[Epoch: 11400] train loss: 0.8904, train acc: 0.6387, val loss: 0.8966, val acc: 0.6229  (best train acc: 0.6458, best val acc: 0.6479)\n",
      "[Epoch: 11420] train loss: 0.9294, train acc: 0.6178, val loss: 0.8741, val acc: 0.6438  (best train acc: 0.6458, best val acc: 0.6479)\n",
      "[Epoch: 11440] train loss: 0.9151, train acc: 0.6290, val loss: 0.8723, val acc: 0.6435  (best train acc: 0.6458, best val acc: 0.6479)\n",
      "[Epoch: 11460] train loss: 0.8955, train acc: 0.6296, val loss: 0.9048, val acc: 0.6138  (best train acc: 0.6458, best val acc: 0.6479)\n",
      "[Epoch: 11480] train loss: 0.9490, train acc: 0.6069, val loss: 0.8770, val acc: 0.6455  (best train acc: 0.6458, best val acc: 0.6479)\n",
      "[Epoch: 11500] train loss: 0.9143, train acc: 0.6319, val loss: 0.8742, val acc: 0.6425  (best train acc: 0.6458, best val acc: 0.6479)\n",
      "[Epoch: 11520] train loss: 0.9023, train acc: 0.6277, val loss: 0.8841, val acc: 0.6297  (best train acc: 0.6458, best val acc: 0.6499)\n",
      "[Epoch: 11540] train loss: 0.9044, train acc: 0.6313, val loss: 0.8824, val acc: 0.6334  (best train acc: 0.6458, best val acc: 0.6499)\n",
      "[Epoch: 11560] train loss: 0.9307, train acc: 0.6127, val loss: 0.8847, val acc: 0.6263  (best train acc: 0.6458, best val acc: 0.6499)\n",
      "[Epoch: 11580] train loss: 0.9325, train acc: 0.6121, val loss: 0.8797, val acc: 0.6449  (best train acc: 0.6458, best val acc: 0.6499)\n",
      "[Epoch: 11600] train loss: 0.9253, train acc: 0.6251, val loss: 0.8841, val acc: 0.6297  (best train acc: 0.6458, best val acc: 0.6499)\n",
      "[Epoch: 11620] train loss: 0.9378, train acc: 0.6058, val loss: 0.8761, val acc: 0.6425  (best train acc: 0.6458, best val acc: 0.6499)\n",
      "[Epoch: 11640] train loss: 0.9427, train acc: 0.6106, val loss: 0.8778, val acc: 0.6391  (best train acc: 0.6458, best val acc: 0.6499)\n",
      "[Epoch: 11660] train loss: 0.9345, train acc: 0.6225, val loss: 0.8728, val acc: 0.6469  (best train acc: 0.6458, best val acc: 0.6499)\n",
      "[Epoch: 11680] train loss: 0.9079, train acc: 0.6292, val loss: 0.8700, val acc: 0.6418  (best train acc: 0.6458, best val acc: 0.6499)\n",
      "[Epoch: 11700] train loss: 0.9106, train acc: 0.6264, val loss: 0.8721, val acc: 0.6422  (best train acc: 0.6458, best val acc: 0.6499)\n",
      "[Epoch: 11720] train loss: 0.9108, train acc: 0.6217, val loss: 0.8818, val acc: 0.6438  (best train acc: 0.6458, best val acc: 0.6499)\n",
      "[Epoch: 11740] train loss: 0.9339, train acc: 0.6143, val loss: 0.8746, val acc: 0.6415  (best train acc: 0.6458, best val acc: 0.6499)\n",
      "[Epoch: 11760] train loss: 0.8920, train acc: 0.6313, val loss: 0.8778, val acc: 0.6364  (best train acc: 0.6458, best val acc: 0.6499)\n",
      "[Epoch: 11780] train loss: 0.9234, train acc: 0.6306, val loss: 0.8896, val acc: 0.6260  (best train acc: 0.6458, best val acc: 0.6499)\n",
      "[Epoch: 11800] train loss: 0.9025, train acc: 0.6343, val loss: 0.8847, val acc: 0.6411  (best train acc: 0.6458, best val acc: 0.6499)\n",
      "[Epoch: 11820] train loss: 0.9848, train acc: 0.5831, val loss: 0.8895, val acc: 0.6300  (best train acc: 0.6458, best val acc: 0.6499)\n",
      "[Epoch: 11840] train loss: 0.9683, train acc: 0.5937, val loss: 0.8785, val acc: 0.6462  (best train acc: 0.6458, best val acc: 0.6499)\n",
      "[Epoch: 11860] train loss: 0.9129, train acc: 0.6165, val loss: 0.8809, val acc: 0.6381  (best train acc: 0.6458, best val acc: 0.6499)\n",
      "[Epoch: 11880] train loss: 0.8928, train acc: 0.6374, val loss: 0.8755, val acc: 0.6449  (best train acc: 0.6458, best val acc: 0.6499)\n",
      "[Epoch: 11900] train loss: 0.9228, train acc: 0.6280, val loss: 0.8730, val acc: 0.6418  (best train acc: 0.6458, best val acc: 0.6499)\n",
      "[Epoch: 11920] train loss: 0.9414, train acc: 0.6182, val loss: 0.8766, val acc: 0.6482  (best train acc: 0.6458, best val acc: 0.6499)\n",
      "[Epoch: 11940] train loss: 0.9124, train acc: 0.6248, val loss: 0.8722, val acc: 0.6442  (best train acc: 0.6458, best val acc: 0.6499)\n",
      "[Epoch: 11960] train loss: 0.9175, train acc: 0.6254, val loss: 0.8885, val acc: 0.6300  (best train acc: 0.6458, best val acc: 0.6499)\n",
      "[Epoch: 11980] train loss: 0.9053, train acc: 0.6322, val loss: 0.8682, val acc: 0.6432  (best train acc: 0.6458, best val acc: 0.6499)\n",
      "[Epoch: 12000] train loss: 0.9083, train acc: 0.6272, val loss: 0.8817, val acc: 0.6432  (best train acc: 0.6465, best val acc: 0.6499)\n",
      "[Epoch: 12020] train loss: 0.9483, train acc: 0.6041, val loss: 0.8718, val acc: 0.6452  (best train acc: 0.6465, best val acc: 0.6499)\n",
      "[Epoch: 12040] train loss: 0.9388, train acc: 0.6152, val loss: 0.8774, val acc: 0.6364  (best train acc: 0.6465, best val acc: 0.6499)\n",
      "[Epoch: 12060] train loss: 0.9335, train acc: 0.6177, val loss: 0.8772, val acc: 0.6388  (best train acc: 0.6465, best val acc: 0.6499)\n",
      "[Epoch: 12080] train loss: 0.9153, train acc: 0.6216, val loss: 0.8686, val acc: 0.6442  (best train acc: 0.6465, best val acc: 0.6499)\n",
      "[Epoch: 12100] train loss: 0.9091, train acc: 0.6262, val loss: 0.8951, val acc: 0.6270  (best train acc: 0.6502, best val acc: 0.6499)\n",
      "[Epoch: 12120] train loss: 0.9309, train acc: 0.6149, val loss: 0.8694, val acc: 0.6438  (best train acc: 0.6502, best val acc: 0.6499)\n",
      "[Epoch: 12140] train loss: 0.8888, train acc: 0.6377, val loss: 0.8741, val acc: 0.6459  (best train acc: 0.6502, best val acc: 0.6519)\n",
      "[Epoch: 12160] train loss: 0.9150, train acc: 0.6219, val loss: 0.8671, val acc: 0.6438  (best train acc: 0.6502, best val acc: 0.6519)\n",
      "[Epoch: 12180] train loss: 0.9288, train acc: 0.6096, val loss: 0.8773, val acc: 0.6381  (best train acc: 0.6502, best val acc: 0.6519)\n",
      "[Epoch: 12200] train loss: 0.8955, train acc: 0.6361, val loss: 0.8794, val acc: 0.6425  (best train acc: 0.6502, best val acc: 0.6519)\n",
      "[Epoch: 12220] train loss: 0.8893, train acc: 0.6350, val loss: 0.8661, val acc: 0.6442  (best train acc: 0.6502, best val acc: 0.6519)\n",
      "[Epoch: 12240] train loss: 0.8919, train acc: 0.6356, val loss: 0.8735, val acc: 0.6476  (best train acc: 0.6502, best val acc: 0.6519)\n",
      "[Epoch: 12260] train loss: 0.9340, train acc: 0.6090, val loss: 0.8686, val acc: 0.6519  (best train acc: 0.6502, best val acc: 0.6519)\n",
      "[Epoch: 12280] train loss: 0.9444, train acc: 0.6107, val loss: 0.8694, val acc: 0.6506  (best train acc: 0.6502, best val acc: 0.6519)\n",
      "[Epoch: 12300] train loss: 0.8931, train acc: 0.6351, val loss: 0.8696, val acc: 0.6445  (best train acc: 0.6502, best val acc: 0.6519)\n",
      "[Epoch: 12320] train loss: 0.9079, train acc: 0.6215, val loss: 0.8976, val acc: 0.6273  (best train acc: 0.6502, best val acc: 0.6519)\n",
      "[Epoch: 12340] train loss: 0.9197, train acc: 0.6178, val loss: 0.8707, val acc: 0.6486  (best train acc: 0.6502, best val acc: 0.6519)\n",
      "[Epoch: 12360] train loss: 0.8855, train acc: 0.6410, val loss: 0.8663, val acc: 0.6455  (best train acc: 0.6502, best val acc: 0.6519)\n",
      "[Epoch: 12380] train loss: 0.9002, train acc: 0.6242, val loss: 0.8663, val acc: 0.6449  (best train acc: 0.6502, best val acc: 0.6519)\n",
      "[Epoch: 12400] train loss: 0.9011, train acc: 0.6266, val loss: 0.8776, val acc: 0.6455  (best train acc: 0.6502, best val acc: 0.6519)\n",
      "[Epoch: 12420] train loss: 0.9041, train acc: 0.6296, val loss: 0.8679, val acc: 0.6465  (best train acc: 0.6502, best val acc: 0.6519)\n",
      "[Epoch: 12440] train loss: 0.9497, train acc: 0.5955, val loss: 0.9135, val acc: 0.6196  (best train acc: 0.6502, best val acc: 0.6519)\n",
      "[Epoch: 12460] train loss: 0.9396, train acc: 0.6079, val loss: 0.8740, val acc: 0.6452  (best train acc: 0.6502, best val acc: 0.6519)\n",
      "[Epoch: 12480] train loss: 0.9189, train acc: 0.6184, val loss: 0.8751, val acc: 0.6486  (best train acc: 0.6502, best val acc: 0.6519)\n",
      "[Epoch: 12500] train loss: 0.9027, train acc: 0.6191, val loss: 0.8760, val acc: 0.6455  (best train acc: 0.6502, best val acc: 0.6519)\n",
      "[Epoch: 12520] train loss: 0.9039, train acc: 0.6297, val loss: 0.8685, val acc: 0.6435  (best train acc: 0.6502, best val acc: 0.6519)\n",
      "[Epoch: 12540] train loss: 0.8942, train acc: 0.6408, val loss: 0.8745, val acc: 0.6445  (best train acc: 0.6502, best val acc: 0.6519)\n",
      "[Epoch: 12560] train loss: 0.9008, train acc: 0.6247, val loss: 0.8820, val acc: 0.6354  (best train acc: 0.6517, best val acc: 0.6536)\n",
      "[Epoch: 12580] train loss: 0.8961, train acc: 0.6325, val loss: 0.8668, val acc: 0.6442  (best train acc: 0.6517, best val acc: 0.6577)\n",
      "[Epoch: 12600] train loss: 0.9275, train acc: 0.6164, val loss: 0.8657, val acc: 0.6503  (best train acc: 0.6517, best val acc: 0.6577)\n",
      "[Epoch: 12620] train loss: 0.9086, train acc: 0.6216, val loss: 0.8757, val acc: 0.6469  (best train acc: 0.6517, best val acc: 0.6577)\n",
      "[Epoch: 12640] train loss: 0.9238, train acc: 0.6150, val loss: 0.8708, val acc: 0.6509  (best train acc: 0.6517, best val acc: 0.6577)\n",
      "[Epoch: 12660] train loss: 0.9102, train acc: 0.6248, val loss: 0.8738, val acc: 0.6415  (best train acc: 0.6517, best val acc: 0.6577)\n",
      "[Epoch: 12680] train loss: 0.8945, train acc: 0.6290, val loss: 0.8684, val acc: 0.6428  (best train acc: 0.6517, best val acc: 0.6577)\n",
      "[Epoch: 12700] train loss: 0.9281, train acc: 0.6118, val loss: 0.8699, val acc: 0.6499  (best train acc: 0.6517, best val acc: 0.6577)\n",
      "[Epoch: 12720] train loss: 0.8941, train acc: 0.6322, val loss: 0.8636, val acc: 0.6496  (best train acc: 0.6517, best val acc: 0.6577)\n",
      "[Epoch: 12740] train loss: 0.9154, train acc: 0.6189, val loss: 0.8921, val acc: 0.6293  (best train acc: 0.6517, best val acc: 0.6577)\n",
      "[Epoch: 12760] train loss: 0.8805, train acc: 0.6418, val loss: 0.8691, val acc: 0.6469  (best train acc: 0.6517, best val acc: 0.6577)\n",
      "[Epoch: 12780] train loss: 0.9270, train acc: 0.6174, val loss: 0.8750, val acc: 0.6395  (best train acc: 0.6517, best val acc: 0.6577)\n",
      "[Epoch: 12800] train loss: 0.9179, train acc: 0.6245, val loss: 0.8662, val acc: 0.6462  (best train acc: 0.6517, best val acc: 0.6577)\n",
      "[Epoch: 12820] train loss: 0.9124, train acc: 0.6243, val loss: 0.8615, val acc: 0.6479  (best train acc: 0.6517, best val acc: 0.6577)\n",
      "[Epoch: 12840] train loss: 0.9015, train acc: 0.6250, val loss: 0.8618, val acc: 0.6462  (best train acc: 0.6517, best val acc: 0.6577)\n",
      "[Epoch: 12860] train loss: 0.8842, train acc: 0.6351, val loss: 0.8687, val acc: 0.6486  (best train acc: 0.6517, best val acc: 0.6577)\n",
      "[Epoch: 12880] train loss: 0.9215, train acc: 0.6207, val loss: 0.8618, val acc: 0.6459  (best train acc: 0.6517, best val acc: 0.6577)\n",
      "[Epoch: 12900] train loss: 0.9631, train acc: 0.5956, val loss: 0.8658, val acc: 0.6503  (best train acc: 0.6517, best val acc: 0.6577)\n",
      "[Epoch: 12920] train loss: 0.9224, train acc: 0.6213, val loss: 0.8628, val acc: 0.6556  (best train acc: 0.6517, best val acc: 0.6577)\n",
      "[Epoch: 12940] train loss: 0.8697, train acc: 0.6480, val loss: 0.8729, val acc: 0.6422  (best train acc: 0.6517, best val acc: 0.6577)\n",
      "[Epoch: 12960] train loss: 0.8834, train acc: 0.6420, val loss: 0.8965, val acc: 0.6283  (best train acc: 0.6517, best val acc: 0.6577)\n",
      "[Epoch: 12980] train loss: 0.9148, train acc: 0.6196, val loss: 0.8754, val acc: 0.6476  (best train acc: 0.6517, best val acc: 0.6577)\n",
      "[Epoch: 13000] train loss: 0.8971, train acc: 0.6299, val loss: 0.8658, val acc: 0.6509  (best train acc: 0.6517, best val acc: 0.6577)\n",
      "[Epoch: 13020] train loss: 0.9130, train acc: 0.6283, val loss: 0.9016, val acc: 0.6223  (best train acc: 0.6517, best val acc: 0.6577)\n",
      "[Epoch: 13040] train loss: 0.9016, train acc: 0.6286, val loss: 0.8639, val acc: 0.6523  (best train acc: 0.6517, best val acc: 0.6577)\n",
      "[Epoch: 13060] train loss: 0.9174, train acc: 0.6215, val loss: 0.8739, val acc: 0.6415  (best train acc: 0.6517, best val acc: 0.6577)\n",
      "[Epoch: 13080] train loss: 0.9293, train acc: 0.6188, val loss: 0.8625, val acc: 0.6536  (best train acc: 0.6517, best val acc: 0.6577)\n",
      "[Epoch: 13100] train loss: 0.9615, train acc: 0.5993, val loss: 0.8646, val acc: 0.6580  (best train acc: 0.6558, best val acc: 0.6580)\n",
      "[Epoch: 13120] train loss: 0.8958, train acc: 0.6337, val loss: 0.8741, val acc: 0.6469  (best train acc: 0.6558, best val acc: 0.6580)\n",
      "[Epoch: 13140] train loss: 0.9186, train acc: 0.6222, val loss: 0.8771, val acc: 0.6384  (best train acc: 0.6558, best val acc: 0.6580)\n",
      "[Epoch: 13160] train loss: 0.9019, train acc: 0.6310, val loss: 0.8674, val acc: 0.6536  (best train acc: 0.6558, best val acc: 0.6580)\n",
      "[Epoch: 13180] train loss: 0.8886, train acc: 0.6394, val loss: 0.8626, val acc: 0.6496  (best train acc: 0.6558, best val acc: 0.6580)\n",
      "[Epoch: 13200] train loss: 0.9126, train acc: 0.6266, val loss: 0.9062, val acc: 0.6094  (best train acc: 0.6558, best val acc: 0.6580)\n",
      "[Epoch: 13220] train loss: 0.8905, train acc: 0.6327, val loss: 0.8602, val acc: 0.6516  (best train acc: 0.6558, best val acc: 0.6580)\n",
      "[Epoch: 13240] train loss: 0.8970, train acc: 0.6313, val loss: 0.8614, val acc: 0.6519  (best train acc: 0.6558, best val acc: 0.6590)\n",
      "[Epoch: 13260] train loss: 0.9020, train acc: 0.6386, val loss: 0.8639, val acc: 0.6503  (best train acc: 0.6558, best val acc: 0.6590)\n",
      "[Epoch: 13280] train loss: 0.9064, train acc: 0.6240, val loss: 0.8685, val acc: 0.6492  (best train acc: 0.6558, best val acc: 0.6590)\n",
      "[Epoch: 13300] train loss: 0.8932, train acc: 0.6340, val loss: 0.8801, val acc: 0.6358  (best train acc: 0.6558, best val acc: 0.6590)\n",
      "[Epoch: 13320] train loss: 0.9062, train acc: 0.6332, val loss: 0.8611, val acc: 0.6519  (best train acc: 0.6558, best val acc: 0.6590)\n",
      "[Epoch: 13340] train loss: 0.9357, train acc: 0.6110, val loss: 0.8742, val acc: 0.6442  (best train acc: 0.6558, best val acc: 0.6590)\n",
      "[Epoch: 13360] train loss: 0.9129, train acc: 0.6189, val loss: 0.8600, val acc: 0.6556  (best train acc: 0.6558, best val acc: 0.6590)\n",
      "[Epoch: 13380] train loss: 0.9021, train acc: 0.6320, val loss: 0.8702, val acc: 0.6438  (best train acc: 0.6558, best val acc: 0.6590)\n",
      "[Epoch: 13400] train loss: 0.9390, train acc: 0.6139, val loss: 0.8711, val acc: 0.6438  (best train acc: 0.6558, best val acc: 0.6590)\n",
      "[Epoch: 13420] train loss: 0.8891, train acc: 0.6284, val loss: 0.8587, val acc: 0.6577  (best train acc: 0.6558, best val acc: 0.6597)\n",
      "[Epoch: 13440] train loss: 0.9213, train acc: 0.6236, val loss: 0.8823, val acc: 0.6364  (best train acc: 0.6558, best val acc: 0.6597)\n",
      "[Epoch: 13460] train loss: 0.8997, train acc: 0.6297, val loss: 0.8637, val acc: 0.6482  (best train acc: 0.6558, best val acc: 0.6597)\n",
      "[Epoch: 13480] train loss: 0.9234, train acc: 0.6221, val loss: 0.8644, val acc: 0.6509  (best train acc: 0.6558, best val acc: 0.6604)\n",
      "[Epoch: 13500] train loss: 0.8923, train acc: 0.6339, val loss: 0.8594, val acc: 0.6617  (best train acc: 0.6558, best val acc: 0.6617)\n",
      "[Epoch: 13520] train loss: 0.9083, train acc: 0.6243, val loss: 0.8582, val acc: 0.6533  (best train acc: 0.6558, best val acc: 0.6617)\n",
      "[Epoch: 13540] train loss: 0.8910, train acc: 0.6311, val loss: 0.8732, val acc: 0.6455  (best train acc: 0.6558, best val acc: 0.6617)\n",
      "[Epoch: 13560] train loss: 0.9034, train acc: 0.6356, val loss: 0.8613, val acc: 0.6530  (best train acc: 0.6558, best val acc: 0.6617)\n",
      "[Epoch: 13580] train loss: 0.8950, train acc: 0.6373, val loss: 0.9132, val acc: 0.6094  (best train acc: 0.6558, best val acc: 0.6617)\n",
      "[Epoch: 13600] train loss: 0.9834, train acc: 0.5823, val loss: 0.8945, val acc: 0.6314  (best train acc: 0.6558, best val acc: 0.6617)\n",
      "[Epoch: 13620] train loss: 0.9234, train acc: 0.6210, val loss: 0.8616, val acc: 0.6550  (best train acc: 0.6558, best val acc: 0.6617)\n",
      "[Epoch: 13640] train loss: 0.9036, train acc: 0.6238, val loss: 0.8675, val acc: 0.6482  (best train acc: 0.6558, best val acc: 0.6617)\n",
      "[Epoch: 13660] train loss: 0.9019, train acc: 0.6322, val loss: 0.8591, val acc: 0.6546  (best train acc: 0.6558, best val acc: 0.6617)\n",
      "[Epoch: 13680] train loss: 0.9117, train acc: 0.6288, val loss: 0.8579, val acc: 0.6519  (best train acc: 0.6558, best val acc: 0.6617)\n",
      "[Epoch: 13700] train loss: 0.9064, train acc: 0.6337, val loss: 0.8615, val acc: 0.6530  (best train acc: 0.6558, best val acc: 0.6617)\n",
      "[Epoch: 13720] train loss: 0.9081, train acc: 0.6248, val loss: 0.8587, val acc: 0.6530  (best train acc: 0.6558, best val acc: 0.6617)\n",
      "[Epoch: 13740] train loss: 0.9060, train acc: 0.6199, val loss: 0.8599, val acc: 0.6516  (best train acc: 0.6558, best val acc: 0.6617)\n",
      "[Epoch: 13760] train loss: 0.8867, train acc: 0.6380, val loss: 0.8621, val acc: 0.6499  (best train acc: 0.6558, best val acc: 0.6617)\n",
      "[Epoch: 13780] train loss: 0.9235, train acc: 0.6259, val loss: 0.8568, val acc: 0.6536  (best train acc: 0.6558, best val acc: 0.6617)\n",
      "[Epoch: 13800] train loss: 0.9007, train acc: 0.6377, val loss: 0.8592, val acc: 0.6499  (best train acc: 0.6558, best val acc: 0.6617)\n",
      "[Epoch: 13820] train loss: 0.8832, train acc: 0.6379, val loss: 0.8589, val acc: 0.6516  (best train acc: 0.6558, best val acc: 0.6617)\n",
      "[Epoch: 13840] train loss: 0.9423, train acc: 0.6096, val loss: 0.8666, val acc: 0.6462  (best train acc: 0.6558, best val acc: 0.6617)\n",
      "[Epoch: 13860] train loss: 0.9142, train acc: 0.6236, val loss: 0.8561, val acc: 0.6556  (best train acc: 0.6558, best val acc: 0.6617)\n",
      "[Epoch: 13880] train loss: 0.9625, train acc: 0.5926, val loss: 0.8618, val acc: 0.6526  (best train acc: 0.6558, best val acc: 0.6617)\n",
      "[Epoch: 13900] train loss: 0.8850, train acc: 0.6393, val loss: 0.8570, val acc: 0.6513  (best train acc: 0.6558, best val acc: 0.6617)\n",
      "[Epoch: 13920] train loss: 0.8730, train acc: 0.6473, val loss: 0.8576, val acc: 0.6540  (best train acc: 0.6558, best val acc: 0.6617)\n",
      "[Epoch: 13940] train loss: 0.9227, train acc: 0.6140, val loss: 0.8575, val acc: 0.6550  (best train acc: 0.6558, best val acc: 0.6617)\n",
      "[Epoch: 13960] train loss: 0.8798, train acc: 0.6381, val loss: 0.8582, val acc: 0.6519  (best train acc: 0.6558, best val acc: 0.6617)\n",
      "[Epoch: 13980] train loss: 0.8971, train acc: 0.6280, val loss: 0.8627, val acc: 0.6516  (best train acc: 0.6558, best val acc: 0.6617)\n",
      "[Epoch: 14000] train loss: 0.8882, train acc: 0.6391, val loss: 0.8590, val acc: 0.6526  (best train acc: 0.6558, best val acc: 0.6617)\n",
      "[Epoch: 14020] train loss: 0.9267, train acc: 0.6155, val loss: 0.8913, val acc: 0.6236  (best train acc: 0.6558, best val acc: 0.6617)\n",
      "[Epoch: 14040] train loss: 0.9207, train acc: 0.6184, val loss: 0.8555, val acc: 0.6567  (best train acc: 0.6558, best val acc: 0.6617)\n",
      "[Epoch: 14060] train loss: 0.9041, train acc: 0.6327, val loss: 0.8547, val acc: 0.6577  (best train acc: 0.6558, best val acc: 0.6617)\n",
      "[Epoch: 14080] train loss: 0.8964, train acc: 0.6330, val loss: 0.8559, val acc: 0.6516  (best train acc: 0.6558, best val acc: 0.6617)\n",
      "[Epoch: 14100] train loss: 0.8974, train acc: 0.6306, val loss: 0.8803, val acc: 0.6391  (best train acc: 0.6558, best val acc: 0.6617)\n",
      "[Epoch: 14120] train loss: 0.8838, train acc: 0.6335, val loss: 0.8671, val acc: 0.6452  (best train acc: 0.6558, best val acc: 0.6617)\n",
      "[Epoch: 14140] train loss: 0.9357, train acc: 0.6175, val loss: 0.8588, val acc: 0.6516  (best train acc: 0.6558, best val acc: 0.6617)\n",
      "[Epoch: 14160] train loss: 0.9013, train acc: 0.6277, val loss: 0.8651, val acc: 0.6462  (best train acc: 0.6558, best val acc: 0.6617)\n",
      "[Epoch: 14180] train loss: 0.9196, train acc: 0.6250, val loss: 0.8773, val acc: 0.6411  (best train acc: 0.6558, best val acc: 0.6617)\n",
      "[Epoch: 14200] train loss: 0.8774, train acc: 0.6422, val loss: 0.8617, val acc: 0.6550  (best train acc: 0.6558, best val acc: 0.6617)\n",
      "[Epoch: 14220] train loss: 0.8960, train acc: 0.6319, val loss: 0.8544, val acc: 0.6563  (best train acc: 0.6558, best val acc: 0.6621)\n",
      "[Epoch: 14240] train loss: 0.9119, train acc: 0.6274, val loss: 0.8538, val acc: 0.6610  (best train acc: 0.6558, best val acc: 0.6621)\n",
      "[Epoch: 14260] train loss: 0.9152, train acc: 0.6236, val loss: 0.8599, val acc: 0.6509  (best train acc: 0.6558, best val acc: 0.6621)\n",
      "[Epoch: 14280] train loss: 0.8951, train acc: 0.6335, val loss: 0.8532, val acc: 0.6604  (best train acc: 0.6558, best val acc: 0.6627)\n",
      "[Epoch: 14300] train loss: 0.8810, train acc: 0.6460, val loss: 0.8612, val acc: 0.6563  (best train acc: 0.6558, best val acc: 0.6627)\n",
      "[Epoch: 14320] train loss: 0.9250, train acc: 0.6248, val loss: 0.8541, val acc: 0.6526  (best train acc: 0.6558, best val acc: 0.6627)\n",
      "[Epoch: 14340] train loss: 0.9321, train acc: 0.6165, val loss: 0.8536, val acc: 0.6536  (best train acc: 0.6558, best val acc: 0.6627)\n",
      "[Epoch: 14360] train loss: 0.8883, train acc: 0.6365, val loss: 0.8663, val acc: 0.6489  (best train acc: 0.6558, best val acc: 0.6637)\n",
      "[Epoch: 14380] train loss: 0.8852, train acc: 0.6379, val loss: 0.8688, val acc: 0.6476  (best train acc: 0.6558, best val acc: 0.6637)\n",
      "[Epoch: 14400] train loss: 0.9044, train acc: 0.6285, val loss: 0.9339, val acc: 0.6145  (best train acc: 0.6558, best val acc: 0.6637)\n",
      "[Epoch: 14420] train loss: 1.0250, train acc: 0.5664, val loss: 0.8626, val acc: 0.6476  (best train acc: 0.6558, best val acc: 0.6637)\n",
      "[Epoch: 14440] train loss: 0.9139, train acc: 0.6189, val loss: 0.8733, val acc: 0.6388  (best train acc: 0.6558, best val acc: 0.6637)\n",
      "[Epoch: 14460] train loss: 0.9095, train acc: 0.6345, val loss: 0.8807, val acc: 0.6374  (best train acc: 0.6558, best val acc: 0.6637)\n",
      "[Epoch: 14480] train loss: 0.9286, train acc: 0.6259, val loss: 0.8583, val acc: 0.6540  (best train acc: 0.6560, best val acc: 0.6637)\n",
      "[Epoch: 14500] train loss: 0.8941, train acc: 0.6317, val loss: 0.8586, val acc: 0.6546  (best train acc: 0.6560, best val acc: 0.6637)\n",
      "[Epoch: 14520] train loss: 0.8925, train acc: 0.6344, val loss: 0.8587, val acc: 0.6560  (best train acc: 0.6560, best val acc: 0.6637)\n",
      "[Epoch: 14540] train loss: 0.9181, train acc: 0.6250, val loss: 0.8601, val acc: 0.6556  (best train acc: 0.6560, best val acc: 0.6637)\n",
      "[Epoch: 14560] train loss: 0.9269, train acc: 0.6176, val loss: 0.8536, val acc: 0.6553  (best train acc: 0.6560, best val acc: 0.6644)\n",
      "[Epoch: 14580] train loss: 0.9027, train acc: 0.6243, val loss: 0.8755, val acc: 0.6405  (best train acc: 0.6560, best val acc: 0.6644)\n",
      "[Epoch: 14600] train loss: 0.9176, train acc: 0.6230, val loss: 0.8549, val acc: 0.6482  (best train acc: 0.6560, best val acc: 0.6654)\n",
      "[Epoch: 14620] train loss: 0.9298, train acc: 0.6140, val loss: 0.8532, val acc: 0.6577  (best train acc: 0.6560, best val acc: 0.6654)\n",
      "[Epoch: 14640] train loss: 0.8815, train acc: 0.6450, val loss: 0.8618, val acc: 0.6553  (best train acc: 0.6560, best val acc: 0.6654)\n",
      "[Epoch: 14660] train loss: 0.8734, train acc: 0.6419, val loss: 0.8607, val acc: 0.6492  (best train acc: 0.6560, best val acc: 0.6654)\n",
      "[Epoch: 14680] train loss: 0.9130, train acc: 0.6288, val loss: 0.9194, val acc: 0.6179  (best train acc: 0.6560, best val acc: 0.6654)\n",
      "[Epoch: 14700] train loss: 0.9104, train acc: 0.6267, val loss: 0.8869, val acc: 0.6270  (best train acc: 0.6560, best val acc: 0.6654)\n",
      "[Epoch: 14720] train loss: 0.9020, train acc: 0.6331, val loss: 0.8621, val acc: 0.6506  (best train acc: 0.6560, best val acc: 0.6654)\n",
      "[Epoch: 14740] train loss: 0.9057, train acc: 0.6271, val loss: 0.8506, val acc: 0.6573  (best train acc: 0.6560, best val acc: 0.6654)\n",
      "[Epoch: 14760] train loss: 0.9307, train acc: 0.6139, val loss: 0.8590, val acc: 0.6506  (best train acc: 0.6606, best val acc: 0.6654)\n",
      "[Epoch: 14780] train loss: 0.8861, train acc: 0.6409, val loss: 0.8513, val acc: 0.6563  (best train acc: 0.6606, best val acc: 0.6654)\n",
      "[Epoch: 14800] train loss: 0.9171, train acc: 0.6212, val loss: 0.8523, val acc: 0.6580  (best train acc: 0.6606, best val acc: 0.6654)\n",
      "[Epoch: 14820] train loss: 0.8744, train acc: 0.6417, val loss: 0.8626, val acc: 0.6523  (best train acc: 0.6606, best val acc: 0.6654)\n",
      "[Epoch: 14840] train loss: 0.8931, train acc: 0.6285, val loss: 0.8648, val acc: 0.6509  (best train acc: 0.6620, best val acc: 0.6654)\n",
      "[Epoch: 14860] train loss: 0.8805, train acc: 0.6413, val loss: 0.8539, val acc: 0.6513  (best train acc: 0.6620, best val acc: 0.6654)\n",
      "[Epoch: 14880] train loss: 0.9123, train acc: 0.6250, val loss: 0.8545, val acc: 0.6634  (best train acc: 0.6620, best val acc: 0.6654)\n",
      "[Epoch: 14900] train loss: 0.8748, train acc: 0.6418, val loss: 0.8617, val acc: 0.6513  (best train acc: 0.6620, best val acc: 0.6654)\n",
      "[Epoch: 14920] train loss: 0.8945, train acc: 0.6347, val loss: 0.8636, val acc: 0.6479  (best train acc: 0.6620, best val acc: 0.6654)\n",
      "[Epoch: 14940] train loss: 0.8974, train acc: 0.6319, val loss: 0.8495, val acc: 0.6583  (best train acc: 0.6620, best val acc: 0.6654)\n",
      "[Epoch: 14960] train loss: 0.8927, train acc: 0.6368, val loss: 0.8644, val acc: 0.6482  (best train acc: 0.6620, best val acc: 0.6654)\n",
      "[Epoch: 14980] train loss: 0.9028, train acc: 0.6290, val loss: 0.8526, val acc: 0.6587  (best train acc: 0.6620, best val acc: 0.6654)\n",
      "[Epoch: 15000] train loss: 0.9171, train acc: 0.6197, val loss: 0.8509, val acc: 0.6587  (best train acc: 0.6620, best val acc: 0.6654)\n",
      "[Epoch: 15020] train loss: 0.9159, train acc: 0.6197, val loss: 0.8630, val acc: 0.6509  (best train acc: 0.6620, best val acc: 0.6654)\n",
      "[Epoch: 15040] train loss: 0.8842, train acc: 0.6345, val loss: 0.8675, val acc: 0.6489  (best train acc: 0.6620, best val acc: 0.6654)\n",
      "[Epoch: 15060] train loss: 0.9045, train acc: 0.6272, val loss: 0.8525, val acc: 0.6556  (best train acc: 0.6620, best val acc: 0.6654)\n",
      "[Epoch: 15080] train loss: 0.9331, train acc: 0.6175, val loss: 0.8636, val acc: 0.6459  (best train acc: 0.6620, best val acc: 0.6654)\n",
      "[Epoch: 15100] train loss: 0.8908, train acc: 0.6359, val loss: 0.8758, val acc: 0.6337  (best train acc: 0.6620, best val acc: 0.6654)\n",
      "[Epoch: 15120] train loss: 0.9427, train acc: 0.6091, val loss: 0.8505, val acc: 0.6597  (best train acc: 0.6620, best val acc: 0.6654)\n",
      "[Epoch: 15140] train loss: 0.8919, train acc: 0.6326, val loss: 0.8572, val acc: 0.6563  (best train acc: 0.6620, best val acc: 0.6654)\n",
      "[Epoch: 15160] train loss: 0.9213, train acc: 0.6194, val loss: 0.8738, val acc: 0.6381  (best train acc: 0.6620, best val acc: 0.6654)\n",
      "[Epoch: 15180] train loss: 0.9013, train acc: 0.6340, val loss: 0.8546, val acc: 0.6604  (best train acc: 0.6620, best val acc: 0.6654)\n",
      "[Epoch: 15200] train loss: 0.8779, train acc: 0.6422, val loss: 0.8484, val acc: 0.6556  (best train acc: 0.6620, best val acc: 0.6654)\n",
      "[Epoch: 15220] train loss: 0.8802, train acc: 0.6433, val loss: 0.8485, val acc: 0.6594  (best train acc: 0.6620, best val acc: 0.6668)\n",
      "[Epoch: 15240] train loss: 0.8900, train acc: 0.6401, val loss: 0.8554, val acc: 0.6523  (best train acc: 0.6620, best val acc: 0.6668)\n",
      "[Epoch: 15260] train loss: 0.8715, train acc: 0.6484, val loss: 0.8487, val acc: 0.6580  (best train acc: 0.6620, best val acc: 0.6668)\n",
      "[Epoch: 15280] train loss: 0.9031, train acc: 0.6212, val loss: 0.8724, val acc: 0.6384  (best train acc: 0.6620, best val acc: 0.6668)\n",
      "[Epoch: 15300] train loss: 0.8984, train acc: 0.6337, val loss: 0.8498, val acc: 0.6556  (best train acc: 0.6620, best val acc: 0.6668)\n",
      "[Epoch: 15320] train loss: 0.8643, train acc: 0.6448, val loss: 0.8533, val acc: 0.6556  (best train acc: 0.6620, best val acc: 0.6668)\n",
      "[Epoch: 15340] train loss: 0.9126, train acc: 0.6194, val loss: 0.8533, val acc: 0.6627  (best train acc: 0.6620, best val acc: 0.6668)\n",
      "[Epoch: 15360] train loss: 0.8938, train acc: 0.6305, val loss: 0.8553, val acc: 0.6580  (best train acc: 0.6620, best val acc: 0.6668)\n",
      "[Epoch: 15380] train loss: 0.9085, train acc: 0.6303, val loss: 0.8521, val acc: 0.6536  (best train acc: 0.6620, best val acc: 0.6668)\n",
      "[Epoch: 15400] train loss: 0.8613, train acc: 0.6507, val loss: 0.8510, val acc: 0.6617  (best train acc: 0.6620, best val acc: 0.6668)\n",
      "[Epoch: 15420] train loss: 0.8856, train acc: 0.6332, val loss: 0.8525, val acc: 0.6597  (best train acc: 0.6620, best val acc: 0.6668)\n",
      "[Epoch: 15440] train loss: 0.9056, train acc: 0.6230, val loss: 0.8580, val acc: 0.6516  (best train acc: 0.6620, best val acc: 0.6668)\n",
      "[Epoch: 15460] train loss: 0.8990, train acc: 0.6243, val loss: 0.8580, val acc: 0.6503  (best train acc: 0.6620, best val acc: 0.6668)\n",
      "[Epoch: 15480] train loss: 0.9194, train acc: 0.6189, val loss: 0.8498, val acc: 0.6543  (best train acc: 0.6620, best val acc: 0.6668)\n",
      "[Epoch: 15500] train loss: 0.8772, train acc: 0.6382, val loss: 0.8476, val acc: 0.6567  (best train acc: 0.6620, best val acc: 0.6668)\n",
      "[Epoch: 15520] train loss: 0.9020, train acc: 0.6323, val loss: 0.9111, val acc: 0.6263  (best train acc: 0.6620, best val acc: 0.6668)\n",
      "[Epoch: 15540] train loss: 0.8714, train acc: 0.6429, val loss: 0.8597, val acc: 0.6590  (best train acc: 0.6620, best val acc: 0.6668)\n",
      "[Epoch: 15560] train loss: 0.9344, train acc: 0.6060, val loss: 0.8529, val acc: 0.6621  (best train acc: 0.6620, best val acc: 0.6668)\n",
      "[Epoch: 15580] train loss: 0.8900, train acc: 0.6364, val loss: 0.8544, val acc: 0.6634  (best train acc: 0.6620, best val acc: 0.6668)\n",
      "[Epoch: 15600] train loss: 0.8643, train acc: 0.6483, val loss: 0.8530, val acc: 0.6631  (best train acc: 0.6620, best val acc: 0.6668)\n",
      "[Epoch: 15620] train loss: 0.8614, train acc: 0.6528, val loss: 0.8463, val acc: 0.6627  (best train acc: 0.6620, best val acc: 0.6668)\n",
      "[Epoch: 15640] train loss: 0.9161, train acc: 0.6204, val loss: 0.8524, val acc: 0.6543  (best train acc: 0.6620, best val acc: 0.6668)\n",
      "[Epoch: 15660] train loss: 0.9244, train acc: 0.6185, val loss: 0.8567, val acc: 0.6509  (best train acc: 0.6620, best val acc: 0.6668)\n",
      "[Epoch: 15680] train loss: 0.8884, train acc: 0.6382, val loss: 0.8462, val acc: 0.6617  (best train acc: 0.6627, best val acc: 0.6668)\n",
      "[Epoch: 15700] train loss: 0.8935, train acc: 0.6409, val loss: 0.8463, val acc: 0.6587  (best train acc: 0.6627, best val acc: 0.6668)\n",
      "[Epoch: 15720] train loss: 0.9130, train acc: 0.6271, val loss: 0.8478, val acc: 0.6597  (best train acc: 0.6627, best val acc: 0.6668)\n",
      "[Epoch: 15740] train loss: 0.8758, train acc: 0.6401, val loss: 0.8701, val acc: 0.6418  (best train acc: 0.6627, best val acc: 0.6668)\n",
      "[Epoch: 15760] train loss: 0.9002, train acc: 0.6247, val loss: 0.8519, val acc: 0.6634  (best train acc: 0.6627, best val acc: 0.6668)\n",
      "[Epoch: 15780] train loss: 0.8625, train acc: 0.6531, val loss: 0.8452, val acc: 0.6610  (best train acc: 0.6627, best val acc: 0.6668)\n",
      "[Epoch: 15800] train loss: 0.9091, train acc: 0.6170, val loss: 0.8795, val acc: 0.6391  (best train acc: 0.6627, best val acc: 0.6668)\n",
      "[Epoch: 15820] train loss: 0.9044, train acc: 0.6262, val loss: 0.8866, val acc: 0.6327  (best train acc: 0.6627, best val acc: 0.6668)\n",
      "[Epoch: 15840] train loss: 0.9441, train acc: 0.6067, val loss: 0.8550, val acc: 0.6567  (best train acc: 0.6627, best val acc: 0.6668)\n",
      "[Epoch: 15860] train loss: 0.8823, train acc: 0.6385, val loss: 0.8745, val acc: 0.6398  (best train acc: 0.6627, best val acc: 0.6668)\n",
      "[Epoch: 15880] train loss: 0.9341, train acc: 0.6077, val loss: 0.8649, val acc: 0.6486  (best train acc: 0.6627, best val acc: 0.6668)\n",
      "[Epoch: 15900] train loss: 0.9012, train acc: 0.6316, val loss: 0.8504, val acc: 0.6570  (best train acc: 0.6627, best val acc: 0.6668)\n",
      "[Epoch: 15920] train loss: 0.8590, train acc: 0.6552, val loss: 0.8612, val acc: 0.6496  (best train acc: 0.6627, best val acc: 0.6668)\n",
      "[Epoch: 15940] train loss: 0.8798, train acc: 0.6445, val loss: 0.8448, val acc: 0.6637  (best train acc: 0.6627, best val acc: 0.6671)\n",
      "[Epoch: 15960] train loss: 0.9353, train acc: 0.6086, val loss: 0.8498, val acc: 0.6513  (best train acc: 0.6627, best val acc: 0.6671)\n",
      "[Epoch: 15980] train loss: 0.9352, train acc: 0.6142, val loss: 0.8820, val acc: 0.6391  (best train acc: 0.6627, best val acc: 0.6671)\n",
      "[Epoch: 16000] train loss: 0.9049, train acc: 0.6326, val loss: 0.8487, val acc: 0.6631  (best train acc: 0.6627, best val acc: 0.6678)\n",
      "[Epoch: 16020] train loss: 0.8909, train acc: 0.6378, val loss: 0.8535, val acc: 0.6604  (best train acc: 0.6627, best val acc: 0.6678)\n",
      "[Epoch: 16040] train loss: 0.8749, train acc: 0.6440, val loss: 0.8466, val acc: 0.6648  (best train acc: 0.6627, best val acc: 0.6678)\n",
      "[Epoch: 16060] train loss: 0.9617, train acc: 0.5967, val loss: 0.8479, val acc: 0.6594  (best train acc: 0.6627, best val acc: 0.6681)\n",
      "[Epoch: 16080] train loss: 0.8956, train acc: 0.6386, val loss: 0.8480, val acc: 0.6641  (best train acc: 0.6627, best val acc: 0.6681)\n",
      "[Epoch: 16100] train loss: 0.8799, train acc: 0.6429, val loss: 0.8495, val acc: 0.6654  (best train acc: 0.6627, best val acc: 0.6681)\n",
      "[Epoch: 16120] train loss: 0.9021, train acc: 0.6325, val loss: 0.8464, val acc: 0.6597  (best train acc: 0.6627, best val acc: 0.6681)\n",
      "[Epoch: 16140] train loss: 0.8610, train acc: 0.6516, val loss: 0.8479, val acc: 0.6648  (best train acc: 0.6627, best val acc: 0.6681)\n",
      "[Epoch: 16160] train loss: 0.8901, train acc: 0.6379, val loss: 0.8445, val acc: 0.6540  (best train acc: 0.6627, best val acc: 0.6681)\n",
      "[Epoch: 16180] train loss: 0.9000, train acc: 0.6329, val loss: 0.8488, val acc: 0.6526  (best train acc: 0.6627, best val acc: 0.6681)\n",
      "[Epoch: 16200] train loss: 0.8819, train acc: 0.6414, val loss: 0.8434, val acc: 0.6610  (best train acc: 0.6627, best val acc: 0.6681)\n",
      "[Epoch: 16220] train loss: 0.8964, train acc: 0.6267, val loss: 0.8433, val acc: 0.6617  (best train acc: 0.6627, best val acc: 0.6681)\n",
      "[Epoch: 16240] train loss: 0.9180, train acc: 0.6148, val loss: 0.8439, val acc: 0.6546  (best train acc: 0.6627, best val acc: 0.6681)\n",
      "[Epoch: 16260] train loss: 0.9069, train acc: 0.6276, val loss: 0.8486, val acc: 0.6634  (best train acc: 0.6627, best val acc: 0.6681)\n",
      "[Epoch: 16280] train loss: 0.9074, train acc: 0.6258, val loss: 0.8432, val acc: 0.6614  (best train acc: 0.6627, best val acc: 0.6681)\n",
      "[Epoch: 16300] train loss: 0.8577, train acc: 0.6515, val loss: 0.8501, val acc: 0.6624  (best train acc: 0.6627, best val acc: 0.6681)\n",
      "[Epoch: 16320] train loss: 0.9079, train acc: 0.6270, val loss: 0.8447, val acc: 0.6671  (best train acc: 0.6627, best val acc: 0.6681)\n",
      "[Epoch: 16340] train loss: 0.8708, train acc: 0.6471, val loss: 0.8603, val acc: 0.6472  (best train acc: 0.6627, best val acc: 0.6681)\n",
      "[Epoch: 16360] train loss: 0.9107, train acc: 0.6183, val loss: 0.8459, val acc: 0.6587  (best train acc: 0.6627, best val acc: 0.6681)\n",
      "[Epoch: 16380] train loss: 0.9141, train acc: 0.6207, val loss: 0.8417, val acc: 0.6637  (best train acc: 0.6640, best val acc: 0.6681)\n",
      "[Epoch: 16400] train loss: 0.8751, train acc: 0.6452, val loss: 0.8512, val acc: 0.6556  (best train acc: 0.6640, best val acc: 0.6681)\n",
      "[Epoch: 16420] train loss: 0.8856, train acc: 0.6299, val loss: 0.8496, val acc: 0.6590  (best train acc: 0.6640, best val acc: 0.6681)\n",
      "[Epoch: 16440] train loss: 0.8705, train acc: 0.6498, val loss: 0.8563, val acc: 0.6556  (best train acc: 0.6640, best val acc: 0.6688)\n",
      "[Epoch: 16460] train loss: 0.9213, train acc: 0.6145, val loss: 0.8530, val acc: 0.6570  (best train acc: 0.6640, best val acc: 0.6688)\n",
      "[Epoch: 16480] train loss: 0.9846, train acc: 0.5814, val loss: 0.8501, val acc: 0.6523  (best train acc: 0.6640, best val acc: 0.6688)\n",
      "[Epoch: 16500] train loss: 0.8859, train acc: 0.6454, val loss: 0.8467, val acc: 0.6597  (best train acc: 0.6640, best val acc: 0.6688)\n",
      "[Epoch: 16520] train loss: 0.9573, train acc: 0.5981, val loss: 0.8466, val acc: 0.6651  (best train acc: 0.6640, best val acc: 0.6688)\n",
      "[Epoch: 16540] train loss: 0.9049, train acc: 0.6245, val loss: 0.8479, val acc: 0.6691  (best train acc: 0.6640, best val acc: 0.6691)\n",
      "[Epoch: 16560] train loss: 0.8966, train acc: 0.6395, val loss: 0.8691, val acc: 0.6472  (best train acc: 0.6640, best val acc: 0.6691)\n",
      "[Epoch: 16580] train loss: 0.9091, train acc: 0.6241, val loss: 0.8423, val acc: 0.6597  (best train acc: 0.6640, best val acc: 0.6691)\n",
      "[Epoch: 16600] train loss: 0.9062, train acc: 0.6334, val loss: 0.8445, val acc: 0.6570  (best train acc: 0.6640, best val acc: 0.6691)\n",
      "[Epoch: 16620] train loss: 0.9007, train acc: 0.6366, val loss: 0.8485, val acc: 0.6631  (best train acc: 0.6640, best val acc: 0.6691)\n",
      "[Epoch: 16640] train loss: 0.9228, train acc: 0.6215, val loss: 0.8587, val acc: 0.6482  (best train acc: 0.6640, best val acc: 0.6691)\n",
      "[Epoch: 16660] train loss: 0.8802, train acc: 0.6382, val loss: 0.8546, val acc: 0.6546  (best train acc: 0.6640, best val acc: 0.6691)\n",
      "[Epoch: 16680] train loss: 0.9036, train acc: 0.6270, val loss: 0.8593, val acc: 0.6523  (best train acc: 0.6640, best val acc: 0.6691)\n",
      "[Epoch: 16700] train loss: 0.9297, train acc: 0.6152, val loss: 0.8671, val acc: 0.6432  (best train acc: 0.6640, best val acc: 0.6691)\n",
      "[Epoch: 16720] train loss: 0.9216, train acc: 0.6194, val loss: 0.8493, val acc: 0.6634  (best train acc: 0.6640, best val acc: 0.6691)\n",
      "[Epoch: 16740] train loss: 0.8677, train acc: 0.6507, val loss: 0.8441, val acc: 0.6661  (best train acc: 0.6640, best val acc: 0.6712)\n",
      "[Epoch: 16760] train loss: 0.8946, train acc: 0.6299, val loss: 0.8455, val acc: 0.6580  (best train acc: 0.6640, best val acc: 0.6712)\n",
      "[Epoch: 16780] train loss: 0.9129, train acc: 0.6233, val loss: 0.8420, val acc: 0.6627  (best train acc: 0.6640, best val acc: 0.6712)\n",
      "[Epoch: 16800] train loss: 0.9132, train acc: 0.6228, val loss: 0.8458, val acc: 0.6661  (best train acc: 0.6640, best val acc: 0.6712)\n",
      "[Epoch: 16820] train loss: 0.9459, train acc: 0.6093, val loss: 0.8757, val acc: 0.6428  (best train acc: 0.6640, best val acc: 0.6712)\n",
      "[Epoch: 16840] train loss: 0.8973, train acc: 0.6283, val loss: 0.8445, val acc: 0.6644  (best train acc: 0.6640, best val acc: 0.6712)\n",
      "[Epoch: 16860] train loss: 0.8738, train acc: 0.6399, val loss: 0.8496, val acc: 0.6671  (best train acc: 0.6640, best val acc: 0.6712)\n",
      "[Epoch: 16880] train loss: 0.8906, train acc: 0.6341, val loss: 0.8450, val acc: 0.6567  (best train acc: 0.6640, best val acc: 0.6712)\n",
      "[Epoch: 16900] train loss: 0.9151, train acc: 0.6204, val loss: 0.8657, val acc: 0.6513  (best train acc: 0.6640, best val acc: 0.6712)\n",
      "[Epoch: 16920] train loss: 0.9014, train acc: 0.6262, val loss: 0.8428, val acc: 0.6658  (best train acc: 0.6640, best val acc: 0.6712)\n",
      "[Epoch: 16940] train loss: 0.8959, train acc: 0.6350, val loss: 0.8429, val acc: 0.6563  (best train acc: 0.6640, best val acc: 0.6712)\n",
      "[Epoch: 16960] train loss: 0.8718, train acc: 0.6403, val loss: 0.8417, val acc: 0.6654  (best train acc: 0.6640, best val acc: 0.6712)\n",
      "[Epoch: 16980] train loss: 0.9202, train acc: 0.6180, val loss: 0.8452, val acc: 0.6661  (best train acc: 0.6640, best val acc: 0.6712)\n",
      "[Epoch: 17000] train loss: 0.8855, train acc: 0.6363, val loss: 0.8811, val acc: 0.6398  (best train acc: 0.6640, best val acc: 0.6712)\n",
      "[Epoch: 17020] train loss: 0.8454, train acc: 0.6618, val loss: 0.8426, val acc: 0.6651  (best train acc: 0.6640, best val acc: 0.6712)\n",
      "[Epoch: 17040] train loss: 0.8963, train acc: 0.6328, val loss: 0.8393, val acc: 0.6648  (best train acc: 0.6640, best val acc: 0.6712)\n",
      "[Epoch: 17060] train loss: 0.8861, train acc: 0.6342, val loss: 0.8392, val acc: 0.6600  (best train acc: 0.6640, best val acc: 0.6712)\n",
      "[Epoch: 17080] train loss: 0.9193, train acc: 0.6163, val loss: 0.8477, val acc: 0.6648  (best train acc: 0.6640, best val acc: 0.6712)\n",
      "[Epoch: 17100] train loss: 0.9047, train acc: 0.6317, val loss: 0.8526, val acc: 0.6580  (best train acc: 0.6640, best val acc: 0.6712)\n",
      "[Epoch: 17120] train loss: 0.8840, train acc: 0.6478, val loss: 0.8412, val acc: 0.6614  (best train acc: 0.6640, best val acc: 0.6712)\n",
      "[Epoch: 17140] train loss: 0.8987, train acc: 0.6342, val loss: 0.8464, val acc: 0.6590  (best train acc: 0.6640, best val acc: 0.6712)\n",
      "[Epoch: 17160] train loss: 0.9107, train acc: 0.6211, val loss: 0.8601, val acc: 0.6489  (best train acc: 0.6640, best val acc: 0.6712)\n",
      "[Epoch: 17180] train loss: 0.8998, train acc: 0.6328, val loss: 0.8602, val acc: 0.6438  (best train acc: 0.6640, best val acc: 0.6712)\n",
      "[Epoch: 17200] train loss: 0.8881, train acc: 0.6331, val loss: 0.8469, val acc: 0.6702  (best train acc: 0.6640, best val acc: 0.6712)\n",
      "[Epoch: 17220] train loss: 0.9008, train acc: 0.6347, val loss: 0.8474, val acc: 0.6540  (best train acc: 0.6640, best val acc: 0.6712)\n",
      "[Epoch: 17240] train loss: 0.9121, train acc: 0.6241, val loss: 0.8404, val acc: 0.6664  (best train acc: 0.6640, best val acc: 0.6712)\n",
      "[Epoch: 17260] train loss: 0.9118, train acc: 0.6230, val loss: 0.8413, val acc: 0.6681  (best train acc: 0.6640, best val acc: 0.6712)\n",
      "[Epoch: 17280] train loss: 0.9376, train acc: 0.6049, val loss: 0.8412, val acc: 0.6614  (best train acc: 0.6640, best val acc: 0.6712)\n",
      "[Epoch: 17300] train loss: 0.9227, train acc: 0.6136, val loss: 0.8411, val acc: 0.6651  (best train acc: 0.6640, best val acc: 0.6712)\n",
      "[Epoch: 17320] train loss: 0.9198, train acc: 0.6192, val loss: 0.8479, val acc: 0.6543  (best train acc: 0.6640, best val acc: 0.6712)\n",
      "[Epoch: 17340] train loss: 0.8938, train acc: 0.6434, val loss: 0.8480, val acc: 0.6607  (best train acc: 0.6640, best val acc: 0.6712)\n",
      "[Epoch: 17360] train loss: 0.8864, train acc: 0.6358, val loss: 0.8649, val acc: 0.6455  (best train acc: 0.6640, best val acc: 0.6712)\n",
      "[Epoch: 17380] train loss: 0.8914, train acc: 0.6345, val loss: 0.8404, val acc: 0.6661  (best train acc: 0.6640, best val acc: 0.6712)\n",
      "[Epoch: 17400] train loss: 0.9064, train acc: 0.6224, val loss: 0.8586, val acc: 0.6479  (best train acc: 0.6640, best val acc: 0.6712)\n",
      "[Epoch: 17420] train loss: 0.8726, train acc: 0.6507, val loss: 0.8448, val acc: 0.6651  (best train acc: 0.6640, best val acc: 0.6712)\n",
      "[Epoch: 17440] train loss: 0.8942, train acc: 0.6315, val loss: 0.8405, val acc: 0.6702  (best train acc: 0.6640, best val acc: 0.6712)\n",
      "[Epoch: 17460] train loss: 0.9125, train acc: 0.6207, val loss: 0.8426, val acc: 0.6654  (best train acc: 0.6640, best val acc: 0.6712)\n",
      "[Epoch: 17480] train loss: 0.8943, train acc: 0.6313, val loss: 0.8469, val acc: 0.6664  (best train acc: 0.6640, best val acc: 0.6712)\n",
      "[Epoch: 17500] train loss: 0.8917, train acc: 0.6311, val loss: 0.8586, val acc: 0.6543  (best train acc: 0.6640, best val acc: 0.6712)\n",
      "[Epoch: 17520] train loss: 0.8864, train acc: 0.6304, val loss: 0.8555, val acc: 0.6546  (best train acc: 0.6640, best val acc: 0.6712)\n",
      "[Epoch: 17540] train loss: 0.8516, train acc: 0.6595, val loss: 0.8411, val acc: 0.6617  (best train acc: 0.6640, best val acc: 0.6712)\n",
      "[Epoch: 17560] train loss: 0.9038, train acc: 0.6312, val loss: 0.8525, val acc: 0.6543  (best train acc: 0.6640, best val acc: 0.6712)\n",
      "[Epoch: 17580] train loss: 0.8792, train acc: 0.6356, val loss: 0.8469, val acc: 0.6678  (best train acc: 0.6640, best val acc: 0.6712)\n",
      "[Epoch: 17600] train loss: 0.8894, train acc: 0.6293, val loss: 0.8430, val acc: 0.6712  (best train acc: 0.6661, best val acc: 0.6712)\n",
      "[Epoch: 17620] train loss: 0.8576, train acc: 0.6575, val loss: 0.8412, val acc: 0.6685  (best train acc: 0.6661, best val acc: 0.6712)\n",
      "[Epoch: 17640] train loss: 0.8821, train acc: 0.6392, val loss: 0.8780, val acc: 0.6361  (best train acc: 0.6661, best val acc: 0.6712)\n",
      "[Epoch: 17660] train loss: 0.8747, train acc: 0.6475, val loss: 0.8637, val acc: 0.6482  (best train acc: 0.6661, best val acc: 0.6712)\n",
      "[Epoch: 17680] train loss: 0.8587, train acc: 0.6533, val loss: 0.8422, val acc: 0.6658  (best train acc: 0.6661, best val acc: 0.6712)\n",
      "[Epoch: 17700] train loss: 0.8750, train acc: 0.6403, val loss: 0.8392, val acc: 0.6637  (best train acc: 0.6661, best val acc: 0.6712)\n",
      "[Epoch: 17720] train loss: 0.8608, train acc: 0.6517, val loss: 0.8369, val acc: 0.6712  (best train acc: 0.6661, best val acc: 0.6712)\n",
      "[Epoch: 17740] train loss: 0.8782, train acc: 0.6442, val loss: 0.8653, val acc: 0.6452  (best train acc: 0.6661, best val acc: 0.6712)\n",
      "[Epoch: 17760] train loss: 0.8781, train acc: 0.6413, val loss: 0.8498, val acc: 0.6526  (best train acc: 0.6661, best val acc: 0.6712)\n",
      "[Epoch: 17780] train loss: 0.9277, train acc: 0.6150, val loss: 0.8492, val acc: 0.6567  (best train acc: 0.6661, best val acc: 0.6712)\n",
      "[Epoch: 17800] train loss: 0.9196, train acc: 0.6136, val loss: 0.8417, val acc: 0.6590  (best train acc: 0.6661, best val acc: 0.6712)\n",
      "[Epoch: 17820] train loss: 0.9110, train acc: 0.6271, val loss: 0.8382, val acc: 0.6624  (best train acc: 0.6661, best val acc: 0.6712)\n",
      "[Epoch: 17840] train loss: 0.8978, train acc: 0.6327, val loss: 0.8442, val acc: 0.6641  (best train acc: 0.6661, best val acc: 0.6722)\n",
      "[Epoch: 17860] train loss: 0.8810, train acc: 0.6382, val loss: 0.8554, val acc: 0.6509  (best train acc: 0.6661, best val acc: 0.6722)\n",
      "[Epoch: 17880] train loss: 0.9426, train acc: 0.6028, val loss: 0.8442, val acc: 0.6567  (best train acc: 0.6661, best val acc: 0.6722)\n",
      "[Epoch: 17900] train loss: 0.8911, train acc: 0.6402, val loss: 0.8556, val acc: 0.6506  (best train acc: 0.6661, best val acc: 0.6722)\n",
      "[Epoch: 17920] train loss: 0.9191, train acc: 0.6254, val loss: 0.8602, val acc: 0.6496  (best train acc: 0.6661, best val acc: 0.6722)\n",
      "[Epoch: 17940] train loss: 0.8703, train acc: 0.6476, val loss: 0.8640, val acc: 0.6459  (best train acc: 0.6661, best val acc: 0.6722)\n",
      "[Epoch: 17960] train loss: 0.9216, train acc: 0.6172, val loss: 0.8565, val acc: 0.6519  (best train acc: 0.6661, best val acc: 0.6722)\n",
      "[Epoch: 17980] train loss: 0.9197, train acc: 0.6209, val loss: 0.8625, val acc: 0.6482  (best train acc: 0.6661, best val acc: 0.6722)\n",
      "[Epoch: 18000] train loss: 0.8809, train acc: 0.6457, val loss: 0.8376, val acc: 0.6718  (best train acc: 0.6661, best val acc: 0.6745)\n",
      "[Epoch: 18020] train loss: 0.9019, train acc: 0.6287, val loss: 0.8436, val acc: 0.6654  (best train acc: 0.6661, best val acc: 0.6745)\n",
      "[Epoch: 18040] train loss: 0.8621, train acc: 0.6471, val loss: 0.8360, val acc: 0.6698  (best train acc: 0.6661, best val acc: 0.6745)\n",
      "[Epoch: 18060] train loss: 0.8677, train acc: 0.6471, val loss: 0.8382, val acc: 0.6698  (best train acc: 0.6666, best val acc: 0.6745)\n",
      "[Epoch: 18080] train loss: 0.8556, train acc: 0.6549, val loss: 0.8357, val acc: 0.6685  (best train acc: 0.6666, best val acc: 0.6745)\n",
      "[Epoch: 18100] train loss: 0.9172, train acc: 0.6211, val loss: 0.8486, val acc: 0.6664  (best train acc: 0.6666, best val acc: 0.6745)\n",
      "[Epoch: 18120] train loss: 0.8907, train acc: 0.6353, val loss: 0.8409, val acc: 0.6621  (best train acc: 0.6666, best val acc: 0.6745)\n",
      "[Epoch: 18140] train loss: 0.9072, train acc: 0.6224, val loss: 0.8610, val acc: 0.6476  (best train acc: 0.6666, best val acc: 0.6745)\n",
      "[Epoch: 18160] train loss: 0.9250, train acc: 0.6195, val loss: 0.8620, val acc: 0.6516  (best train acc: 0.6666, best val acc: 0.6745)\n",
      "[Epoch: 18180] train loss: 0.9029, train acc: 0.6322, val loss: 0.8337, val acc: 0.6641  (best train acc: 0.6666, best val acc: 0.6745)\n",
      "[Epoch: 18200] train loss: 0.8759, train acc: 0.6453, val loss: 0.8549, val acc: 0.6563  (best train acc: 0.6666, best val acc: 0.6745)\n",
      "[Epoch: 18220] train loss: 0.8502, train acc: 0.6583, val loss: 0.8687, val acc: 0.6432  (best train acc: 0.6666, best val acc: 0.6745)\n",
      "[Epoch: 18240] train loss: 0.8730, train acc: 0.6496, val loss: 0.8469, val acc: 0.6519  (best train acc: 0.6666, best val acc: 0.6745)\n",
      "[Epoch: 18260] train loss: 0.9115, train acc: 0.6212, val loss: 0.8607, val acc: 0.6482  (best train acc: 0.6666, best val acc: 0.6745)\n",
      "[Epoch: 18280] train loss: 0.8614, train acc: 0.6510, val loss: 0.8375, val acc: 0.6583  (best train acc: 0.6666, best val acc: 0.6745)\n",
      "[Epoch: 18300] train loss: 0.8988, train acc: 0.6379, val loss: 0.8353, val acc: 0.6648  (best train acc: 0.6666, best val acc: 0.6745)\n",
      "[Epoch: 18320] train loss: 0.8908, train acc: 0.6330, val loss: 0.8330, val acc: 0.6637  (best train acc: 0.6666, best val acc: 0.6745)\n",
      "[Epoch: 18340] train loss: 0.8619, train acc: 0.6491, val loss: 0.8351, val acc: 0.6661  (best train acc: 0.6666, best val acc: 0.6745)\n",
      "[Epoch: 18360] train loss: 0.8512, train acc: 0.6524, val loss: 0.8350, val acc: 0.6728  (best train acc: 0.6666, best val acc: 0.6745)\n",
      "[Epoch: 18380] train loss: 0.8829, train acc: 0.6385, val loss: 0.8383, val acc: 0.6695  (best train acc: 0.6666, best val acc: 0.6745)\n",
      "[Epoch: 18400] train loss: 0.8650, train acc: 0.6486, val loss: 0.8404, val acc: 0.6678  (best train acc: 0.6666, best val acc: 0.6745)\n",
      "[Epoch: 18420] train loss: 0.8862, train acc: 0.6317, val loss: 0.8445, val acc: 0.6685  (best train acc: 0.6666, best val acc: 0.6745)\n",
      "[Epoch: 18440] train loss: 0.8972, train acc: 0.6303, val loss: 0.8421, val acc: 0.6715  (best train acc: 0.6666, best val acc: 0.6745)\n",
      "[Epoch: 18460] train loss: 0.8936, train acc: 0.6327, val loss: 0.8336, val acc: 0.6715  (best train acc: 0.6666, best val acc: 0.6745)\n",
      "[Epoch: 18480] train loss: 0.9032, train acc: 0.6270, val loss: 0.8545, val acc: 0.6523  (best train acc: 0.6666, best val acc: 0.6745)\n",
      "[Epoch: 18500] train loss: 0.8569, train acc: 0.6556, val loss: 0.8511, val acc: 0.6553  (best train acc: 0.6666, best val acc: 0.6745)\n",
      "[Epoch: 18520] train loss: 0.8899, train acc: 0.6366, val loss: 0.8399, val acc: 0.6705  (best train acc: 0.6666, best val acc: 0.6745)\n",
      "[Epoch: 18540] train loss: 0.8900, train acc: 0.6337, val loss: 0.8435, val acc: 0.6607  (best train acc: 0.6666, best val acc: 0.6745)\n",
      "[Epoch: 18560] train loss: 0.9179, train acc: 0.6184, val loss: 0.9154, val acc: 0.6273  (best train acc: 0.6666, best val acc: 0.6745)\n",
      "[Epoch: 18580] train loss: 0.9563, train acc: 0.6019, val loss: 0.8493, val acc: 0.6583  (best train acc: 0.6666, best val acc: 0.6745)\n",
      "[Epoch: 18600] train loss: 0.9085, train acc: 0.6235, val loss: 0.8408, val acc: 0.6604  (best train acc: 0.6666, best val acc: 0.6745)\n",
      "[Epoch: 18620] train loss: 0.8909, train acc: 0.6314, val loss: 0.8442, val acc: 0.6651  (best train acc: 0.6666, best val acc: 0.6745)\n",
      "[Epoch: 18640] train loss: 0.8497, train acc: 0.6558, val loss: 0.8528, val acc: 0.6644  (best train acc: 0.6666, best val acc: 0.6745)\n",
      "[Epoch: 18660] train loss: 0.8891, train acc: 0.6356, val loss: 0.8540, val acc: 0.6496  (best train acc: 0.6666, best val acc: 0.6745)\n",
      "[Epoch: 18680] train loss: 0.8791, train acc: 0.6472, val loss: 0.8346, val acc: 0.6725  (best train acc: 0.6666, best val acc: 0.6745)\n",
      "[Epoch: 18700] train loss: 0.8822, train acc: 0.6392, val loss: 0.8410, val acc: 0.6600  (best train acc: 0.6666, best val acc: 0.6745)\n",
      "[Epoch: 18720] train loss: 0.8532, train acc: 0.6531, val loss: 0.8852, val acc: 0.6226  (best train acc: 0.6666, best val acc: 0.6745)\n",
      "[Epoch: 18740] train loss: 0.9082, train acc: 0.6199, val loss: 0.8366, val acc: 0.6654  (best train acc: 0.6666, best val acc: 0.6745)\n",
      "[Epoch: 18760] train loss: 0.8484, train acc: 0.6598, val loss: 0.8472, val acc: 0.6594  (best train acc: 0.6666, best val acc: 0.6745)\n",
      "[Epoch: 18780] train loss: 0.9374, train acc: 0.6119, val loss: 0.8361, val acc: 0.6678  (best train acc: 0.6666, best val acc: 0.6745)\n",
      "[Epoch: 18800] train loss: 0.9227, train acc: 0.6197, val loss: 0.8373, val acc: 0.6742  (best train acc: 0.6666, best val acc: 0.6745)\n",
      "[Epoch: 18820] train loss: 0.8700, train acc: 0.6469, val loss: 0.8473, val acc: 0.6637  (best train acc: 0.6666, best val acc: 0.6745)\n",
      "[Epoch: 18840] train loss: 0.8976, train acc: 0.6283, val loss: 0.8348, val acc: 0.6735  (best train acc: 0.6666, best val acc: 0.6752)\n",
      "[Epoch: 18860] train loss: 0.8429, train acc: 0.6567, val loss: 0.8767, val acc: 0.6320  (best train acc: 0.6666, best val acc: 0.6752)\n",
      "[Epoch: 18880] train loss: 0.8944, train acc: 0.6283, val loss: 0.8394, val acc: 0.6702  (best train acc: 0.6666, best val acc: 0.6779)\n",
      "[Epoch: 18900] train loss: 0.8596, train acc: 0.6507, val loss: 0.8337, val acc: 0.6671  (best train acc: 0.6666, best val acc: 0.6779)\n",
      "[Epoch: 18920] train loss: 0.9107, train acc: 0.6169, val loss: 0.8314, val acc: 0.6722  (best train acc: 0.6666, best val acc: 0.6779)\n",
      "[Epoch: 18940] train loss: 0.9002, train acc: 0.6247, val loss: 0.8433, val acc: 0.6570  (best train acc: 0.6666, best val acc: 0.6779)\n",
      "[Epoch: 18960] train loss: 0.8434, train acc: 0.6552, val loss: 0.8440, val acc: 0.6610  (best train acc: 0.6666, best val acc: 0.6779)\n",
      "[Epoch: 18980] train loss: 0.8800, train acc: 0.6441, val loss: 0.8391, val acc: 0.6705  (best train acc: 0.6689, best val acc: 0.6779)\n",
      "[Epoch: 19000] train loss: 0.8970, train acc: 0.6288, val loss: 0.8376, val acc: 0.6661  (best train acc: 0.6689, best val acc: 0.6779)\n",
      "[Epoch: 19020] train loss: 0.8509, train acc: 0.6598, val loss: 0.8343, val acc: 0.6641  (best train acc: 0.6689, best val acc: 0.6779)\n",
      "[Epoch: 19040] train loss: 0.8927, train acc: 0.6352, val loss: 0.8341, val acc: 0.6739  (best train acc: 0.6689, best val acc: 0.6779)\n",
      "[Epoch: 19060] train loss: 0.9256, train acc: 0.6134, val loss: 0.8478, val acc: 0.6644  (best train acc: 0.6689, best val acc: 0.6779)\n",
      "[Epoch: 19080] train loss: 0.9130, train acc: 0.6352, val loss: 0.8822, val acc: 0.6432  (best train acc: 0.6689, best val acc: 0.6779)\n",
      "[Epoch: 19100] train loss: 0.9122, train acc: 0.6280, val loss: 0.8474, val acc: 0.6651  (best train acc: 0.6689, best val acc: 0.6779)\n",
      "[Epoch: 19120] train loss: 0.8695, train acc: 0.6536, val loss: 0.8423, val acc: 0.6695  (best train acc: 0.6689, best val acc: 0.6779)\n",
      "[Epoch: 19140] train loss: 0.8713, train acc: 0.6482, val loss: 0.8437, val acc: 0.6580  (best train acc: 0.6689, best val acc: 0.6779)\n",
      "[Epoch: 19160] train loss: 0.8809, train acc: 0.6366, val loss: 0.8391, val acc: 0.6627  (best train acc: 0.6689, best val acc: 0.6779)\n",
      "[Epoch: 19180] train loss: 0.8798, train acc: 0.6386, val loss: 0.8423, val acc: 0.6678  (best train acc: 0.6689, best val acc: 0.6779)\n",
      "[Epoch: 19200] train loss: 0.8781, train acc: 0.6418, val loss: 0.8490, val acc: 0.6567  (best train acc: 0.6689, best val acc: 0.6779)\n",
      "[Epoch: 19220] train loss: 0.8746, train acc: 0.6426, val loss: 0.8402, val acc: 0.6728  (best train acc: 0.6689, best val acc: 0.6779)\n",
      "[Epoch: 19240] train loss: 0.9077, train acc: 0.6309, val loss: 0.8334, val acc: 0.6745  (best train acc: 0.6689, best val acc: 0.6779)\n",
      "[Epoch: 19260] train loss: 0.8888, train acc: 0.6433, val loss: 0.8604, val acc: 0.6530  (best train acc: 0.6689, best val acc: 0.6782)\n",
      "[Epoch: 19280] train loss: 0.8441, train acc: 0.6633, val loss: 0.8403, val acc: 0.6604  (best train acc: 0.6689, best val acc: 0.6782)\n",
      "[Epoch: 19300] train loss: 0.8599, train acc: 0.6562, val loss: 0.8968, val acc: 0.6169  (best train acc: 0.6689, best val acc: 0.6782)\n",
      "[Epoch: 19320] train loss: 0.8511, train acc: 0.6517, val loss: 0.8366, val acc: 0.6742  (best train acc: 0.6689, best val acc: 0.6782)\n",
      "[Epoch: 19340] train loss: 0.8782, train acc: 0.6395, val loss: 0.8347, val acc: 0.6722  (best train acc: 0.6689, best val acc: 0.6782)\n",
      "[Epoch: 19360] train loss: 0.8646, train acc: 0.6471, val loss: 0.8472, val acc: 0.6624  (best train acc: 0.6689, best val acc: 0.6782)\n",
      "[Epoch: 19380] train loss: 0.8549, train acc: 0.6561, val loss: 0.8463, val acc: 0.6560  (best train acc: 0.6724, best val acc: 0.6782)\n",
      "[Epoch: 19400] train loss: 0.8961, train acc: 0.6384, val loss: 0.8489, val acc: 0.6631  (best train acc: 0.6724, best val acc: 0.6782)\n",
      "[Epoch: 19420] train loss: 0.8649, train acc: 0.6442, val loss: 0.8462, val acc: 0.6621  (best train acc: 0.6724, best val acc: 0.6782)\n",
      "[Epoch: 19440] train loss: 0.8529, train acc: 0.6549, val loss: 0.8322, val acc: 0.6739  (best train acc: 0.6724, best val acc: 0.6782)\n",
      "[Epoch: 19460] train loss: 0.9275, train acc: 0.6134, val loss: 0.8360, val acc: 0.6739  (best train acc: 0.6724, best val acc: 0.6782)\n",
      "[Epoch: 19480] train loss: 0.8689, train acc: 0.6464, val loss: 0.8315, val acc: 0.6675  (best train acc: 0.6724, best val acc: 0.6782)\n",
      "[Epoch: 19500] train loss: 0.8994, train acc: 0.6254, val loss: 0.8323, val acc: 0.6681  (best train acc: 0.6724, best val acc: 0.6782)\n",
      "[Epoch: 19520] train loss: 0.8642, train acc: 0.6546, val loss: 0.8605, val acc: 0.6408  (best train acc: 0.6724, best val acc: 0.6782)\n",
      "[Epoch: 19540] train loss: 0.8577, train acc: 0.6468, val loss: 0.8401, val acc: 0.6695  (best train acc: 0.6724, best val acc: 0.6782)\n",
      "[Epoch: 19560] train loss: 0.8935, train acc: 0.6311, val loss: 0.8364, val acc: 0.6691  (best train acc: 0.6724, best val acc: 0.6782)\n",
      "[Epoch: 19580] train loss: 0.8502, train acc: 0.6581, val loss: 0.8340, val acc: 0.6681  (best train acc: 0.6724, best val acc: 0.6782)\n",
      "[Epoch: 19600] train loss: 0.8729, train acc: 0.6416, val loss: 0.8320, val acc: 0.6712  (best train acc: 0.6724, best val acc: 0.6782)\n",
      "[Epoch: 19620] train loss: 0.8473, train acc: 0.6572, val loss: 0.8422, val acc: 0.6702  (best train acc: 0.6724, best val acc: 0.6782)\n",
      "[Epoch: 19640] train loss: 0.8346, train acc: 0.6629, val loss: 0.8294, val acc: 0.6695  (best train acc: 0.6724, best val acc: 0.6782)\n",
      "[Epoch: 19660] train loss: 0.8759, train acc: 0.6465, val loss: 0.8531, val acc: 0.6580  (best train acc: 0.6724, best val acc: 0.6782)\n",
      "[Epoch: 19680] train loss: 0.8961, train acc: 0.6291, val loss: 0.8342, val acc: 0.6644  (best train acc: 0.6724, best val acc: 0.6782)\n",
      "[Epoch: 19700] train loss: 0.8823, train acc: 0.6411, val loss: 0.8355, val acc: 0.6624  (best train acc: 0.6724, best val acc: 0.6782)\n",
      "[Epoch: 19720] train loss: 0.8542, train acc: 0.6539, val loss: 0.8376, val acc: 0.6735  (best train acc: 0.6724, best val acc: 0.6782)\n",
      "[Epoch: 19740] train loss: 0.8936, train acc: 0.6329, val loss: 0.8346, val acc: 0.6772  (best train acc: 0.6724, best val acc: 0.6782)\n",
      "[Epoch: 19760] train loss: 0.8490, train acc: 0.6608, val loss: 0.8496, val acc: 0.6567  (best train acc: 0.6724, best val acc: 0.6782)\n",
      "[Epoch: 19780] train loss: 0.8650, train acc: 0.6501, val loss: 0.9015, val acc: 0.6121  (best train acc: 0.6724, best val acc: 0.6782)\n",
      "[Epoch: 19800] train loss: 0.8978, train acc: 0.6314, val loss: 0.8461, val acc: 0.6546  (best train acc: 0.6724, best val acc: 0.6782)\n",
      "[Epoch: 19820] train loss: 0.8409, train acc: 0.6617, val loss: 0.9217, val acc: 0.5946  (best train acc: 0.6724, best val acc: 0.6782)\n",
      "[Epoch: 19840] train loss: 0.9300, train acc: 0.6129, val loss: 0.8576, val acc: 0.6506  (best train acc: 0.6724, best val acc: 0.6782)\n",
      "[Epoch: 19860] train loss: 0.8888, train acc: 0.6393, val loss: 0.8555, val acc: 0.6526  (best train acc: 0.6724, best val acc: 0.6782)\n",
      "[Epoch: 19880] train loss: 0.8868, train acc: 0.6403, val loss: 0.8389, val acc: 0.6634  (best train acc: 0.6724, best val acc: 0.6789)\n",
      "[Epoch: 19900] train loss: 0.8931, train acc: 0.6328, val loss: 0.8336, val acc: 0.6759  (best train acc: 0.6724, best val acc: 0.6789)\n",
      "[Epoch: 19920] train loss: 0.8805, train acc: 0.6392, val loss: 0.8312, val acc: 0.6725  (best train acc: 0.6724, best val acc: 0.6789)\n",
      "[Epoch: 19940] train loss: 0.9059, train acc: 0.6217, val loss: 0.8488, val acc: 0.6600  (best train acc: 0.6724, best val acc: 0.6789)\n",
      "[Epoch: 19960] train loss: 0.9235, train acc: 0.6170, val loss: 0.8558, val acc: 0.6516  (best train acc: 0.6737, best val acc: 0.6789)\n",
      "[Epoch: 19980] train loss: 0.8593, train acc: 0.6535, val loss: 0.8464, val acc: 0.6631  (best train acc: 0.6737, best val acc: 0.6789)\n",
      "[Epoch: 20000] train loss: 0.8558, train acc: 0.6546, val loss: 0.8489, val acc: 0.6661  (best train acc: 0.6737, best val acc: 0.6789)\n",
      "[Epoch: 20020] train loss: 0.8930, train acc: 0.6392, val loss: 0.8303, val acc: 0.6766  (best train acc: 0.6737, best val acc: 0.6789)\n",
      "[Epoch: 20040] train loss: 0.8444, train acc: 0.6551, val loss: 0.8421, val acc: 0.6604  (best train acc: 0.6737, best val acc: 0.6789)\n",
      "[Epoch: 20060] train loss: 0.8857, train acc: 0.6354, val loss: 0.8364, val acc: 0.6641  (best train acc: 0.6737, best val acc: 0.6789)\n",
      "[Epoch: 20080] train loss: 0.9083, train acc: 0.6220, val loss: 0.8474, val acc: 0.6546  (best train acc: 0.6737, best val acc: 0.6803)\n",
      "[Epoch: 20100] train loss: 0.8644, train acc: 0.6515, val loss: 0.8339, val acc: 0.6759  (best train acc: 0.6737, best val acc: 0.6803)\n",
      "[Epoch: 20120] train loss: 0.8684, train acc: 0.6424, val loss: 0.8340, val acc: 0.6735  (best train acc: 0.6737, best val acc: 0.6803)\n",
      "[Epoch: 20140] train loss: 0.8644, train acc: 0.6455, val loss: 0.8283, val acc: 0.6712  (best train acc: 0.6737, best val acc: 0.6803)\n",
      "[Epoch: 20160] train loss: 0.8479, train acc: 0.6515, val loss: 0.8276, val acc: 0.6725  (best train acc: 0.6737, best val acc: 0.6803)\n",
      "[Epoch: 20180] train loss: 0.8984, train acc: 0.6275, val loss: 0.8356, val acc: 0.6634  (best train acc: 0.6737, best val acc: 0.6803)\n",
      "[Epoch: 20200] train loss: 0.8750, train acc: 0.6379, val loss: 0.8294, val acc: 0.6712  (best train acc: 0.6737, best val acc: 0.6803)\n",
      "[Epoch: 20220] train loss: 0.8965, train acc: 0.6275, val loss: 0.8323, val acc: 0.6728  (best train acc: 0.6737, best val acc: 0.6803)\n",
      "[Epoch: 20240] train loss: 0.9071, train acc: 0.6260, val loss: 0.8301, val acc: 0.6745  (best train acc: 0.6737, best val acc: 0.6803)\n",
      "[Epoch: 20260] train loss: 0.8697, train acc: 0.6466, val loss: 0.8424, val acc: 0.6664  (best train acc: 0.6737, best val acc: 0.6803)\n",
      "[Epoch: 20280] train loss: 0.8779, train acc: 0.6398, val loss: 0.8405, val acc: 0.6675  (best train acc: 0.6781, best val acc: 0.6803)\n",
      "[Epoch: 20300] train loss: 0.8945, train acc: 0.6184, val loss: 0.8431, val acc: 0.6617  (best train acc: 0.6781, best val acc: 0.6803)\n",
      "[Epoch: 20320] train loss: 0.9061, train acc: 0.6246, val loss: 0.8549, val acc: 0.6513  (best train acc: 0.6781, best val acc: 0.6803)\n",
      "[Epoch: 20340] train loss: 0.9003, train acc: 0.6261, val loss: 0.8356, val acc: 0.6749  (best train acc: 0.6781, best val acc: 0.6803)\n",
      "[Epoch: 20360] train loss: 0.8620, train acc: 0.6499, val loss: 0.8310, val acc: 0.6688  (best train acc: 0.6781, best val acc: 0.6803)\n",
      "[Epoch: 20380] train loss: 0.9137, train acc: 0.6242, val loss: 0.8321, val acc: 0.6688  (best train acc: 0.6781, best val acc: 0.6806)\n",
      "[Epoch: 20400] train loss: 0.8522, train acc: 0.6540, val loss: 0.8464, val acc: 0.6648  (best train acc: 0.6781, best val acc: 0.6806)\n",
      "[Epoch: 20420] train loss: 0.9078, train acc: 0.6212, val loss: 0.8400, val acc: 0.6648  (best train acc: 0.6781, best val acc: 0.6806)\n",
      "[Epoch: 20440] train loss: 0.8816, train acc: 0.6365, val loss: 0.8352, val acc: 0.6735  (best train acc: 0.6781, best val acc: 0.6806)\n",
      "[Epoch: 20460] train loss: 0.8688, train acc: 0.6445, val loss: 0.8303, val acc: 0.6702  (best train acc: 0.6781, best val acc: 0.6806)\n",
      "[Epoch: 20480] train loss: 0.8871, train acc: 0.6366, val loss: 0.8856, val acc: 0.6411  (best train acc: 0.6781, best val acc: 0.6806)\n",
      "[Epoch: 20500] train loss: 0.8772, train acc: 0.6418, val loss: 0.8293, val acc: 0.6749  (best train acc: 0.6781, best val acc: 0.6806)\n",
      "[Epoch: 20520] train loss: 0.8713, train acc: 0.6464, val loss: 0.8287, val acc: 0.6745  (best train acc: 0.6781, best val acc: 0.6806)\n",
      "[Epoch: 20540] train loss: 0.8434, train acc: 0.6616, val loss: 0.8339, val acc: 0.6769  (best train acc: 0.6781, best val acc: 0.6806)\n",
      "[Epoch: 20560] train loss: 0.8489, train acc: 0.6643, val loss: 0.8278, val acc: 0.6728  (best train acc: 0.6781, best val acc: 0.6806)\n",
      "[Epoch: 20580] train loss: 0.8730, train acc: 0.6403, val loss: 0.8303, val acc: 0.6752  (best train acc: 0.6781, best val acc: 0.6806)\n",
      "[Epoch: 20600] train loss: 0.8887, train acc: 0.6390, val loss: 0.8392, val acc: 0.6691  (best train acc: 0.6781, best val acc: 0.6806)\n",
      "[Epoch: 20620] train loss: 0.8647, train acc: 0.6428, val loss: 0.8410, val acc: 0.6550  (best train acc: 0.6781, best val acc: 0.6806)\n",
      "[Epoch: 20640] train loss: 0.9291, train acc: 0.6095, val loss: 0.8330, val acc: 0.6712  (best train acc: 0.6781, best val acc: 0.6806)\n",
      "[Epoch: 20660] train loss: 0.9083, train acc: 0.6210, val loss: 0.8761, val acc: 0.6307  (best train acc: 0.6781, best val acc: 0.6806)\n",
      "[Epoch: 20680] train loss: 0.8398, train acc: 0.6668, val loss: 0.8319, val acc: 0.6668  (best train acc: 0.6781, best val acc: 0.6806)\n",
      "[Epoch: 20700] train loss: 0.8972, train acc: 0.6278, val loss: 0.8313, val acc: 0.6762  (best train acc: 0.6781, best val acc: 0.6806)\n",
      "[Epoch: 20720] train loss: 0.9022, train acc: 0.6238, val loss: 0.8441, val acc: 0.6567  (best train acc: 0.6781, best val acc: 0.6806)\n",
      "[Epoch: 20740] train loss: 0.8755, train acc: 0.6388, val loss: 0.8285, val acc: 0.6786  (best train acc: 0.6781, best val acc: 0.6806)\n",
      "[Epoch: 20760] train loss: 0.8743, train acc: 0.6444, val loss: 0.8445, val acc: 0.6590  (best train acc: 0.6781, best val acc: 0.6806)\n",
      "[Epoch: 20780] train loss: 0.8743, train acc: 0.6434, val loss: 0.8328, val acc: 0.6648  (best train acc: 0.6781, best val acc: 0.6806)\n",
      "[Epoch: 20800] train loss: 0.8942, train acc: 0.6323, val loss: 0.8402, val acc: 0.6580  (best train acc: 0.6781, best val acc: 0.6806)\n",
      "[Epoch: 20820] train loss: 0.8892, train acc: 0.6288, val loss: 0.8461, val acc: 0.6610  (best train acc: 0.6781, best val acc: 0.6806)\n",
      "[Epoch: 20840] train loss: 0.8820, train acc: 0.6374, val loss: 0.8268, val acc: 0.6739  (best train acc: 0.6781, best val acc: 0.6806)\n",
      "[Epoch: 20860] train loss: 0.8776, train acc: 0.6369, val loss: 0.8338, val acc: 0.6658  (best train acc: 0.6781, best val acc: 0.6816)\n",
      "[Epoch: 20880] train loss: 0.9263, train acc: 0.6169, val loss: 0.8311, val acc: 0.6685  (best train acc: 0.6781, best val acc: 0.6816)\n",
      "[Epoch: 20900] train loss: 0.9794, train acc: 0.5821, val loss: 0.8344, val acc: 0.6728  (best train acc: 0.6781, best val acc: 0.6816)\n",
      "[Epoch: 20920] train loss: 0.8706, train acc: 0.6490, val loss: 0.8400, val acc: 0.6702  (best train acc: 0.6781, best val acc: 0.6816)\n",
      "[Epoch: 20940] train loss: 0.8450, train acc: 0.6591, val loss: 0.8322, val acc: 0.6702  (best train acc: 0.6781, best val acc: 0.6816)\n",
      "[Epoch: 20960] train loss: 0.8709, train acc: 0.6424, val loss: 0.8350, val acc: 0.6631  (best train acc: 0.6781, best val acc: 0.6816)\n",
      "[Epoch: 20980] train loss: 0.8999, train acc: 0.6233, val loss: 0.8331, val acc: 0.6739  (best train acc: 0.6781, best val acc: 0.6816)\n",
      "[Epoch: 21000] train loss: 0.8676, train acc: 0.6484, val loss: 0.8332, val acc: 0.6766  (best train acc: 0.6781, best val acc: 0.6816)\n",
      "[Epoch: 21020] train loss: 0.9191, train acc: 0.6183, val loss: 0.8462, val acc: 0.6644  (best train acc: 0.6781, best val acc: 0.6816)\n",
      "[Epoch: 21040] train loss: 0.9192, train acc: 0.6151, val loss: 0.8278, val acc: 0.6782  (best train acc: 0.6781, best val acc: 0.6816)\n",
      "[Epoch: 21060] train loss: 0.8789, train acc: 0.6397, val loss: 0.8531, val acc: 0.6540  (best train acc: 0.6781, best val acc: 0.6816)\n",
      "[Epoch: 21080] train loss: 0.8604, train acc: 0.6461, val loss: 0.8277, val acc: 0.6762  (best train acc: 0.6781, best val acc: 0.6816)\n",
      "[Epoch: 21100] train loss: 0.8834, train acc: 0.6345, val loss: 0.8624, val acc: 0.6435  (best train acc: 0.6781, best val acc: 0.6816)\n",
      "[Epoch: 21120] train loss: 0.8564, train acc: 0.6544, val loss: 0.8289, val acc: 0.6742  (best train acc: 0.6781, best val acc: 0.6816)\n",
      "[Epoch: 21140] train loss: 0.8963, train acc: 0.6329, val loss: 0.8301, val acc: 0.6651  (best train acc: 0.6781, best val acc: 0.6816)\n",
      "[Epoch: 21160] train loss: 0.8937, train acc: 0.6241, val loss: 0.8295, val acc: 0.6769  (best train acc: 0.6781, best val acc: 0.6816)\n",
      "[Epoch: 21180] train loss: 0.8652, train acc: 0.6507, val loss: 0.8305, val acc: 0.6712  (best train acc: 0.6781, best val acc: 0.6816)\n",
      "[Epoch: 21200] train loss: 0.9274, train acc: 0.6129, val loss: 0.8290, val acc: 0.6769  (best train acc: 0.6781, best val acc: 0.6816)\n",
      "[Epoch: 21220] train loss: 0.8760, train acc: 0.6420, val loss: 0.8320, val acc: 0.6782  (best train acc: 0.6781, best val acc: 0.6816)\n",
      "[Epoch: 21240] train loss: 0.8946, train acc: 0.6290, val loss: 0.8269, val acc: 0.6749  (best train acc: 0.6781, best val acc: 0.6816)\n",
      "[Epoch: 21260] train loss: 0.8877, train acc: 0.6334, val loss: 0.8533, val acc: 0.6526  (best train acc: 0.6781, best val acc: 0.6816)\n",
      "[Epoch: 21280] train loss: 0.8590, train acc: 0.6481, val loss: 0.8270, val acc: 0.6809  (best train acc: 0.6781, best val acc: 0.6823)\n",
      "[Epoch: 21300] train loss: 0.8813, train acc: 0.6392, val loss: 0.8341, val acc: 0.6728  (best train acc: 0.6781, best val acc: 0.6823)\n",
      "[Epoch: 21320] train loss: 0.8540, train acc: 0.6552, val loss: 0.8277, val acc: 0.6796  (best train acc: 0.6781, best val acc: 0.6823)\n",
      "[Epoch: 21340] train loss: 0.8623, train acc: 0.6478, val loss: 0.8249, val acc: 0.6732  (best train acc: 0.6781, best val acc: 0.6823)\n",
      "[Epoch: 21360] train loss: 0.8573, train acc: 0.6514, val loss: 0.8541, val acc: 0.6556  (best train acc: 0.6781, best val acc: 0.6823)\n",
      "[Epoch: 21380] train loss: 0.9236, train acc: 0.6145, val loss: 0.8315, val acc: 0.6772  (best train acc: 0.6781, best val acc: 0.6823)\n",
      "[Epoch: 21400] train loss: 0.8632, train acc: 0.6455, val loss: 0.8458, val acc: 0.6563  (best train acc: 0.6781, best val acc: 0.6823)\n",
      "[Epoch: 21420] train loss: 0.8714, train acc: 0.6471, val loss: 0.8253, val acc: 0.6705  (best train acc: 0.6781, best val acc: 0.6823)\n",
      "[Epoch: 21440] train loss: 0.8749, train acc: 0.6349, val loss: 0.8256, val acc: 0.6722  (best train acc: 0.6781, best val acc: 0.6823)\n",
      "[Epoch: 21460] train loss: 0.8525, train acc: 0.6538, val loss: 0.8247, val acc: 0.6732  (best train acc: 0.6781, best val acc: 0.6823)\n",
      "[Epoch: 21480] train loss: 0.8919, train acc: 0.6396, val loss: 0.8670, val acc: 0.6526  (best train acc: 0.6781, best val acc: 0.6823)\n",
      "[Epoch: 21500] train loss: 0.9486, train acc: 0.6071, val loss: 0.8808, val acc: 0.6479  (best train acc: 0.6781, best val acc: 0.6823)\n",
      "[Epoch: 21520] train loss: 0.8984, train acc: 0.6328, val loss: 0.8409, val acc: 0.6728  (best train acc: 0.6781, best val acc: 0.6823)\n",
      "[Epoch: 21540] train loss: 0.9173, train acc: 0.6236, val loss: 0.8444, val acc: 0.6641  (best train acc: 0.6781, best val acc: 0.6823)\n",
      "[Epoch: 21560] train loss: 0.8868, train acc: 0.6350, val loss: 0.8411, val acc: 0.6668  (best train acc: 0.6781, best val acc: 0.6823)\n",
      "[Epoch: 21580] train loss: 0.9515, train acc: 0.6064, val loss: 0.8460, val acc: 0.6641  (best train acc: 0.6781, best val acc: 0.6823)\n",
      "[Epoch: 21600] train loss: 0.8753, train acc: 0.6429, val loss: 0.8562, val acc: 0.6543  (best train acc: 0.6781, best val acc: 0.6823)\n",
      "[Epoch: 21620] train loss: 0.8851, train acc: 0.6388, val loss: 0.8248, val acc: 0.6732  (best train acc: 0.6781, best val acc: 0.6823)\n",
      "[Epoch: 21640] train loss: 0.8611, train acc: 0.6516, val loss: 0.8389, val acc: 0.6648  (best train acc: 0.6781, best val acc: 0.6823)\n",
      "[Epoch: 21660] train loss: 0.8696, train acc: 0.6502, val loss: 0.8454, val acc: 0.6583  (best train acc: 0.6781, best val acc: 0.6823)\n",
      "[Epoch: 21680] train loss: 0.9105, train acc: 0.6013, val loss: 0.8345, val acc: 0.6712  (best train acc: 0.6781, best val acc: 0.6823)\n",
      "[Epoch: 21700] train loss: 0.9077, train acc: 0.6271, val loss: 0.8319, val acc: 0.6702  (best train acc: 0.6781, best val acc: 0.6823)\n",
      "[Epoch: 21720] train loss: 0.8562, train acc: 0.6554, val loss: 0.8578, val acc: 0.6526  (best train acc: 0.6781, best val acc: 0.6823)\n",
      "[Epoch: 21740] train loss: 0.8491, train acc: 0.6576, val loss: 0.8287, val acc: 0.6769  (best train acc: 0.6781, best val acc: 0.6823)\n",
      "[Epoch: 21760] train loss: 0.8815, train acc: 0.6427, val loss: 0.8302, val acc: 0.6712  (best train acc: 0.6781, best val acc: 0.6823)\n",
      "[Epoch: 21780] train loss: 0.8958, train acc: 0.6337, val loss: 0.8426, val acc: 0.6641  (best train acc: 0.6781, best val acc: 0.6823)\n",
      "[Epoch: 21800] train loss: 0.8780, train acc: 0.6451, val loss: 0.8489, val acc: 0.6543  (best train acc: 0.6781, best val acc: 0.6823)\n",
      "[Epoch: 21820] train loss: 0.8463, train acc: 0.6611, val loss: 0.8260, val acc: 0.6728  (best train acc: 0.6781, best val acc: 0.6823)\n",
      "[Epoch: 21840] train loss: 0.8628, train acc: 0.6431, val loss: 0.8265, val acc: 0.6681  (best train acc: 0.6781, best val acc: 0.6823)\n",
      "[Epoch: 21860] train loss: 0.8435, train acc: 0.6645, val loss: 0.8390, val acc: 0.6648  (best train acc: 0.6781, best val acc: 0.6823)\n",
      "[Epoch: 21880] train loss: 0.8881, train acc: 0.6400, val loss: 0.8246, val acc: 0.6745  (best train acc: 0.6781, best val acc: 0.6823)\n",
      "[Epoch: 21900] train loss: 0.8660, train acc: 0.6396, val loss: 0.8267, val acc: 0.6725  (best train acc: 0.6781, best val acc: 0.6823)\n",
      "[Epoch: 21920] train loss: 0.8523, train acc: 0.6573, val loss: 0.8918, val acc: 0.6159  (best train acc: 0.6781, best val acc: 0.6823)\n",
      "[Epoch: 21940] train loss: 0.8943, train acc: 0.6324, val loss: 0.8515, val acc: 0.6553  (best train acc: 0.6781, best val acc: 0.6823)\n",
      "[Epoch: 21960] train loss: 0.8952, train acc: 0.6309, val loss: 0.8354, val acc: 0.6654  (best train acc: 0.6781, best val acc: 0.6823)\n",
      "[Epoch: 21980] train loss: 0.9177, train acc: 0.6169, val loss: 0.8545, val acc: 0.6503  (best train acc: 0.6781, best val acc: 0.6823)\n",
      "[Epoch: 22000] train loss: 0.8676, train acc: 0.6446, val loss: 0.8278, val acc: 0.6766  (best train acc: 0.6781, best val acc: 0.6823)\n",
      "[Epoch: 22020] train loss: 0.8821, train acc: 0.6361, val loss: 0.8258, val acc: 0.6779  (best train acc: 0.6781, best val acc: 0.6823)\n",
      "[Epoch: 22040] train loss: 0.8766, train acc: 0.6412, val loss: 0.8577, val acc: 0.6489  (best train acc: 0.6781, best val acc: 0.6823)\n",
      "[Epoch: 22060] train loss: 0.8355, train acc: 0.6652, val loss: 0.8248, val acc: 0.6772  (best train acc: 0.6781, best val acc: 0.6823)\n",
      "[Epoch: 22080] train loss: 0.8665, train acc: 0.6485, val loss: 0.8266, val acc: 0.6793  (best train acc: 0.6781, best val acc: 0.6823)\n",
      "[Epoch: 22100] train loss: 0.8476, train acc: 0.6600, val loss: 0.8304, val acc: 0.6779  (best train acc: 0.6781, best val acc: 0.6823)\n",
      "[Epoch: 22120] train loss: 0.9199, train acc: 0.6152, val loss: 0.8384, val acc: 0.6681  (best train acc: 0.6781, best val acc: 0.6823)\n",
      "[Epoch: 22140] train loss: 0.9148, train acc: 0.6123, val loss: 0.8312, val acc: 0.6668  (best train acc: 0.6781, best val acc: 0.6823)\n",
      "[Epoch: 22160] train loss: 0.8926, train acc: 0.6295, val loss: 0.8411, val acc: 0.6634  (best train acc: 0.6781, best val acc: 0.6823)\n",
      "[Epoch: 22180] train loss: 0.8600, train acc: 0.6481, val loss: 0.8403, val acc: 0.6641  (best train acc: 0.6781, best val acc: 0.6823)\n",
      "[Epoch: 22200] train loss: 0.8729, train acc: 0.6528, val loss: 0.8369, val acc: 0.6718  (best train acc: 0.6781, best val acc: 0.6823)\n",
      "[Epoch: 22220] train loss: 0.8856, train acc: 0.6422, val loss: 0.8366, val acc: 0.6705  (best train acc: 0.6781, best val acc: 0.6823)\n",
      "[Epoch: 22240] train loss: 0.8813, train acc: 0.6421, val loss: 0.8316, val acc: 0.6742  (best train acc: 0.6781, best val acc: 0.6823)\n",
      "[Epoch: 22260] train loss: 0.9283, train acc: 0.6120, val loss: 0.8498, val acc: 0.6600  (best train acc: 0.6781, best val acc: 0.6823)\n",
      "[Epoch: 22280] train loss: 0.8718, train acc: 0.6476, val loss: 0.8388, val acc: 0.6725  (best train acc: 0.6781, best val acc: 0.6823)\n",
      "[Epoch: 22300] train loss: 0.8874, train acc: 0.6403, val loss: 0.8526, val acc: 0.6567  (best train acc: 0.6781, best val acc: 0.6823)\n",
      "[Epoch: 22320] train loss: 0.8728, train acc: 0.6478, val loss: 0.8724, val acc: 0.6418  (best train acc: 0.6781, best val acc: 0.6823)\n",
      "[Epoch: 22340] train loss: 0.8882, train acc: 0.6366, val loss: 0.8330, val acc: 0.6718  (best train acc: 0.6781, best val acc: 0.6823)\n",
      "[Epoch: 22360] train loss: 0.8687, train acc: 0.6475, val loss: 0.8325, val acc: 0.6749  (best train acc: 0.6781, best val acc: 0.6823)\n",
      "[Epoch: 22380] train loss: 0.8809, train acc: 0.6410, val loss: 0.8235, val acc: 0.6769  (best train acc: 0.6781, best val acc: 0.6823)\n",
      "[Epoch: 22400] train loss: 0.8669, train acc: 0.6523, val loss: 0.8228, val acc: 0.6830  (best train acc: 0.6781, best val acc: 0.6830)\n",
      "[Epoch: 22420] train loss: 0.8943, train acc: 0.6298, val loss: 0.8228, val acc: 0.6789  (best train acc: 0.6781, best val acc: 0.6830)\n",
      "[Epoch: 22440] train loss: 0.8982, train acc: 0.6230, val loss: 0.8283, val acc: 0.6671  (best train acc: 0.6781, best val acc: 0.6830)\n",
      "[Epoch: 22460] train loss: 0.8503, train acc: 0.6541, val loss: 0.8253, val acc: 0.6698  (best train acc: 0.6781, best val acc: 0.6830)\n",
      "[Epoch: 22480] train loss: 0.8740, train acc: 0.6391, val loss: 0.8249, val acc: 0.6735  (best train acc: 0.6781, best val acc: 0.6830)\n",
      "[Epoch: 22500] train loss: 0.9295, train acc: 0.6118, val loss: 0.8272, val acc: 0.6762  (best train acc: 0.6781, best val acc: 0.6830)\n",
      "[Epoch: 22520] train loss: 0.8578, train acc: 0.6586, val loss: 0.8244, val acc: 0.6813  (best train acc: 0.6781, best val acc: 0.6830)\n",
      "[Epoch: 22540] train loss: 0.8941, train acc: 0.6364, val loss: 0.8588, val acc: 0.6445  (best train acc: 0.6781, best val acc: 0.6840)\n",
      "[Epoch: 22560] train loss: 0.9098, train acc: 0.6305, val loss: 0.8429, val acc: 0.6631  (best train acc: 0.6781, best val acc: 0.6840)\n",
      "[Epoch: 22580] train loss: 0.9054, train acc: 0.6235, val loss: 0.8245, val acc: 0.6803  (best train acc: 0.6781, best val acc: 0.6840)\n",
      "[Epoch: 22600] train loss: 0.8473, train acc: 0.6556, val loss: 0.8239, val acc: 0.6799  (best train acc: 0.6781, best val acc: 0.6840)\n",
      "[Epoch: 22620] train loss: 0.8564, train acc: 0.6499, val loss: 0.8297, val acc: 0.6675  (best train acc: 0.6781, best val acc: 0.6847)\n",
      "[Epoch: 22640] train loss: 0.8652, train acc: 0.6491, val loss: 0.8484, val acc: 0.6560  (best train acc: 0.6781, best val acc: 0.6847)\n",
      "[Epoch: 22660] train loss: 0.8316, train acc: 0.6661, val loss: 0.8233, val acc: 0.6793  (best train acc: 0.6781, best val acc: 0.6847)\n",
      "[Epoch: 22680] train loss: 0.8696, train acc: 0.6434, val loss: 0.8350, val acc: 0.6678  (best train acc: 0.6781, best val acc: 0.6847)\n",
      "[Epoch: 22700] train loss: 0.9138, train acc: 0.6183, val loss: 0.8275, val acc: 0.6668  (best train acc: 0.6781, best val acc: 0.6847)\n",
      "[Epoch: 22720] train loss: 0.8471, train acc: 0.6525, val loss: 0.8215, val acc: 0.6803  (best train acc: 0.6781, best val acc: 0.6847)\n",
      "[Epoch: 22740] train loss: 0.9024, train acc: 0.6267, val loss: 0.8652, val acc: 0.6378  (best train acc: 0.6781, best val acc: 0.6847)\n",
      "[Epoch: 22760] train loss: 0.8519, train acc: 0.6609, val loss: 0.8325, val acc: 0.6735  (best train acc: 0.6781, best val acc: 0.6847)\n",
      "[Epoch: 22780] train loss: 0.8641, train acc: 0.6425, val loss: 0.8299, val acc: 0.6762  (best train acc: 0.6781, best val acc: 0.6847)\n",
      "[Epoch: 22800] train loss: 0.8988, train acc: 0.6262, val loss: 0.8288, val acc: 0.6685  (best train acc: 0.6781, best val acc: 0.6847)\n",
      "[Epoch: 22820] train loss: 0.8844, train acc: 0.6432, val loss: 0.8369, val acc: 0.6654  (best train acc: 0.6781, best val acc: 0.6847)\n",
      "[Epoch: 22840] train loss: 0.8861, train acc: 0.6423, val loss: 0.8407, val acc: 0.6634  (best train acc: 0.6781, best val acc: 0.6847)\n",
      "[Epoch: 22860] train loss: 0.8518, train acc: 0.6572, val loss: 0.8288, val acc: 0.6769  (best train acc: 0.6781, best val acc: 0.6847)\n",
      "[Epoch: 22880] train loss: 0.8731, train acc: 0.6430, val loss: 0.8263, val acc: 0.6678  (best train acc: 0.6781, best val acc: 0.6847)\n",
      "[Epoch: 22900] train loss: 0.8637, train acc: 0.6450, val loss: 0.8211, val acc: 0.6826  (best train acc: 0.6781, best val acc: 0.6847)\n",
      "[Epoch: 22920] train loss: 0.8447, train acc: 0.6585, val loss: 0.8235, val acc: 0.6718  (best train acc: 0.6781, best val acc: 0.6847)\n",
      "[Epoch: 22940] train loss: 0.8777, train acc: 0.6298, val loss: 0.8640, val acc: 0.6567  (best train acc: 0.6781, best val acc: 0.6847)\n",
      "[Epoch: 22960] train loss: 0.8578, train acc: 0.6609, val loss: 0.8991, val acc: 0.6020  (best train acc: 0.6781, best val acc: 0.6847)\n",
      "[Epoch: 22980] train loss: 0.8916, train acc: 0.6408, val loss: 0.8246, val acc: 0.6786  (best train acc: 0.6781, best val acc: 0.6847)\n",
      "[Epoch: 23000] train loss: 0.8650, train acc: 0.6527, val loss: 0.8783, val acc: 0.6253  (best train acc: 0.6781, best val acc: 0.6847)\n",
      "[Epoch: 23020] train loss: 0.8653, train acc: 0.6526, val loss: 0.8331, val acc: 0.6685  (best train acc: 0.6781, best val acc: 0.6847)\n",
      "[Epoch: 23040] train loss: 0.8528, train acc: 0.6542, val loss: 0.8278, val acc: 0.6728  (best train acc: 0.6781, best val acc: 0.6847)\n",
      "[Epoch: 23060] train loss: 0.8687, train acc: 0.6488, val loss: 0.8226, val acc: 0.6779  (best train acc: 0.6781, best val acc: 0.6847)\n",
      "[Epoch: 23080] train loss: 0.8576, train acc: 0.6619, val loss: 0.8248, val acc: 0.6820  (best train acc: 0.6781, best val acc: 0.6847)\n",
      "[Epoch: 23100] train loss: 0.8592, train acc: 0.6503, val loss: 0.8213, val acc: 0.6830  (best train acc: 0.6781, best val acc: 0.6850)\n",
      "[Epoch: 23120] train loss: 0.8387, train acc: 0.6645, val loss: 0.8264, val acc: 0.6779  (best train acc: 0.6781, best val acc: 0.6850)\n",
      "[Epoch: 23140] train loss: 0.8671, train acc: 0.6487, val loss: 0.8427, val acc: 0.6580  (best train acc: 0.6781, best val acc: 0.6850)\n",
      "[Epoch: 23160] train loss: 0.8954, train acc: 0.6426, val loss: 0.8319, val acc: 0.6681  (best train acc: 0.6781, best val acc: 0.6850)\n",
      "[Epoch: 23180] train loss: 0.9001, train acc: 0.6297, val loss: 0.8226, val acc: 0.6749  (best train acc: 0.6781, best val acc: 0.6850)\n",
      "[Epoch: 23200] train loss: 0.8645, train acc: 0.6524, val loss: 0.8202, val acc: 0.6796  (best train acc: 0.6781, best val acc: 0.6850)\n",
      "[Epoch: 23220] train loss: 0.8769, train acc: 0.6457, val loss: 0.8521, val acc: 0.6496  (best train acc: 0.6781, best val acc: 0.6850)\n",
      "[Epoch: 23240] train loss: 0.8433, train acc: 0.6612, val loss: 0.8245, val acc: 0.6779  (best train acc: 0.6781, best val acc: 0.6850)\n",
      "[Epoch: 23260] train loss: 0.8397, train acc: 0.6682, val loss: 0.8213, val acc: 0.6776  (best train acc: 0.6804, best val acc: 0.6850)\n",
      "[Epoch: 23280] train loss: 0.8993, train acc: 0.6283, val loss: 0.8279, val acc: 0.6749  (best train acc: 0.6804, best val acc: 0.6850)\n",
      "[Epoch: 23300] train loss: 0.8262, train acc: 0.6719, val loss: 0.8241, val acc: 0.6793  (best train acc: 0.6804, best val acc: 0.6850)\n",
      "[Epoch: 23320] train loss: 0.8730, train acc: 0.6463, val loss: 0.8208, val acc: 0.6782  (best train acc: 0.6804, best val acc: 0.6850)\n",
      "[Epoch: 23340] train loss: 0.8522, train acc: 0.6551, val loss: 0.8226, val acc: 0.6789  (best train acc: 0.6804, best val acc: 0.6850)\n",
      "[Epoch: 23360] train loss: 0.8771, train acc: 0.6417, val loss: 0.8317, val acc: 0.6715  (best train acc: 0.6804, best val acc: 0.6850)\n",
      "[Epoch: 23380] train loss: 0.8806, train acc: 0.6411, val loss: 0.8489, val acc: 0.6567  (best train acc: 0.6804, best val acc: 0.6850)\n",
      "[Epoch: 23400] train loss: 0.8409, train acc: 0.6618, val loss: 0.8219, val acc: 0.6799  (best train acc: 0.6804, best val acc: 0.6850)\n",
      "[Epoch: 23420] train loss: 0.8826, train acc: 0.6335, val loss: 0.8276, val acc: 0.6766  (best train acc: 0.6804, best val acc: 0.6850)\n",
      "[Epoch: 23440] train loss: 0.8785, train acc: 0.6432, val loss: 0.8195, val acc: 0.6796  (best train acc: 0.6804, best val acc: 0.6853)\n",
      "[Epoch: 23460] train loss: 0.8859, train acc: 0.6394, val loss: 0.8256, val acc: 0.6799  (best train acc: 0.6804, best val acc: 0.6853)\n",
      "[Epoch: 23480] train loss: 0.8311, train acc: 0.6674, val loss: 0.8209, val acc: 0.6796  (best train acc: 0.6804, best val acc: 0.6853)\n",
      "[Epoch: 23500] train loss: 0.8848, train acc: 0.6389, val loss: 0.8212, val acc: 0.6762  (best train acc: 0.6804, best val acc: 0.6853)\n",
      "[Epoch: 23520] train loss: 0.8766, train acc: 0.6352, val loss: 0.8281, val acc: 0.6702  (best train acc: 0.6804, best val acc: 0.6853)\n",
      "[Epoch: 23540] train loss: 0.8708, train acc: 0.6437, val loss: 0.8344, val acc: 0.6661  (best train acc: 0.6804, best val acc: 0.6853)\n",
      "[Epoch: 23560] train loss: 0.8324, train acc: 0.6671, val loss: 0.9222, val acc: 0.5993  (best train acc: 0.6804, best val acc: 0.6853)\n",
      "[Epoch: 23580] train loss: 0.9112, train acc: 0.6212, val loss: 0.8248, val acc: 0.6759  (best train acc: 0.6804, best val acc: 0.6853)\n",
      "[Epoch: 23600] train loss: 0.9101, train acc: 0.6220, val loss: 0.8261, val acc: 0.6708  (best train acc: 0.6804, best val acc: 0.6853)\n",
      "[Epoch: 23620] train loss: 0.8906, train acc: 0.6264, val loss: 0.8222, val acc: 0.6786  (best train acc: 0.6804, best val acc: 0.6853)\n",
      "[Epoch: 23640] train loss: 0.8326, train acc: 0.6733, val loss: 0.8210, val acc: 0.6809  (best train acc: 0.6804, best val acc: 0.6853)\n",
      "[Epoch: 23660] train loss: 0.8683, train acc: 0.6506, val loss: 0.8244, val acc: 0.6752  (best train acc: 0.6804, best val acc: 0.6853)\n",
      "[Epoch: 23680] train loss: 0.8816, train acc: 0.6365, val loss: 0.8474, val acc: 0.6604  (best train acc: 0.6804, best val acc: 0.6853)\n",
      "[Epoch: 23700] train loss: 0.8594, train acc: 0.6515, val loss: 0.8201, val acc: 0.6772  (best train acc: 0.6838, best val acc: 0.6853)\n",
      "[Epoch: 23720] train loss: 0.8712, train acc: 0.6459, val loss: 0.8509, val acc: 0.6573  (best train acc: 0.6838, best val acc: 0.6853)\n",
      "[Epoch: 23740] train loss: 0.8634, train acc: 0.6407, val loss: 0.8208, val acc: 0.6718  (best train acc: 0.6838, best val acc: 0.6853)\n",
      "[Epoch: 23760] train loss: 0.8547, train acc: 0.6491, val loss: 0.8276, val acc: 0.6722  (best train acc: 0.6838, best val acc: 0.6853)\n",
      "[Epoch: 23780] train loss: 0.8894, train acc: 0.6345, val loss: 0.8317, val acc: 0.6664  (best train acc: 0.6838, best val acc: 0.6853)\n",
      "[Epoch: 23800] train loss: 0.8714, train acc: 0.6488, val loss: 0.8196, val acc: 0.6766  (best train acc: 0.6838, best val acc: 0.6853)\n",
      "[Epoch: 23820] train loss: 0.8340, train acc: 0.6708, val loss: 0.8250, val acc: 0.6695  (best train acc: 0.6838, best val acc: 0.6853)\n",
      "[Epoch: 23840] train loss: 0.8634, train acc: 0.6457, val loss: 0.8183, val acc: 0.6786  (best train acc: 0.6838, best val acc: 0.6853)\n",
      "[Epoch: 23860] train loss: 0.8573, train acc: 0.6491, val loss: 0.8198, val acc: 0.6779  (best train acc: 0.6838, best val acc: 0.6853)\n",
      "[Epoch: 23880] train loss: 0.8580, train acc: 0.6557, val loss: 0.8385, val acc: 0.6688  (best train acc: 0.6838, best val acc: 0.6853)\n",
      "[Epoch: 23900] train loss: 0.8705, train acc: 0.6558, val loss: 0.8268, val acc: 0.6732  (best train acc: 0.6838, best val acc: 0.6853)\n",
      "[Epoch: 23920] train loss: 0.9394, train acc: 0.6123, val loss: 0.8274, val acc: 0.6766  (best train acc: 0.6838, best val acc: 0.6853)\n",
      "[Epoch: 23940] train loss: 0.8918, train acc: 0.6326, val loss: 0.8392, val acc: 0.6614  (best train acc: 0.6838, best val acc: 0.6853)\n",
      "[Epoch: 23960] train loss: 0.8692, train acc: 0.6426, val loss: 0.8188, val acc: 0.6782  (best train acc: 0.6838, best val acc: 0.6853)\n",
      "[Epoch: 23980] train loss: 0.8650, train acc: 0.6435, val loss: 0.8246, val acc: 0.6705  (best train acc: 0.6838, best val acc: 0.6853)\n",
      "[Epoch: 24000] train loss: 0.8764, train acc: 0.6424, val loss: 0.8550, val acc: 0.6546  (best train acc: 0.6838, best val acc: 0.6853)\n",
      "[Epoch: 24020] train loss: 0.8839, train acc: 0.6365, val loss: 0.8232, val acc: 0.6806  (best train acc: 0.6838, best val acc: 0.6853)\n",
      "[Epoch: 24040] train loss: 0.8812, train acc: 0.6364, val loss: 0.8256, val acc: 0.6833  (best train acc: 0.6838, best val acc: 0.6853)\n",
      "[Epoch: 24060] train loss: 0.8525, train acc: 0.6485, val loss: 0.8189, val acc: 0.6755  (best train acc: 0.6838, best val acc: 0.6853)\n",
      "[Epoch: 24080] train loss: 0.8669, train acc: 0.6494, val loss: 0.8386, val acc: 0.6661  (best train acc: 0.6838, best val acc: 0.6857)\n",
      "[Epoch: 24100] train loss: 0.8507, train acc: 0.6588, val loss: 0.8202, val acc: 0.6786  (best train acc: 0.6838, best val acc: 0.6857)\n",
      "[Epoch: 24120] train loss: 0.8296, train acc: 0.6607, val loss: 0.8912, val acc: 0.6128  (best train acc: 0.6838, best val acc: 0.6857)\n",
      "[Epoch: 24140] train loss: 0.8230, train acc: 0.6770, val loss: 0.8221, val acc: 0.6715  (best train acc: 0.6838, best val acc: 0.6857)\n",
      "[Epoch: 24160] train loss: 0.8755, train acc: 0.6408, val loss: 0.8416, val acc: 0.6614  (best train acc: 0.6838, best val acc: 0.6857)\n",
      "[Epoch: 24180] train loss: 0.8636, train acc: 0.6453, val loss: 0.8175, val acc: 0.6803  (best train acc: 0.6838, best val acc: 0.6857)\n",
      "[Epoch: 24200] train loss: 0.8753, train acc: 0.6485, val loss: 0.8211, val acc: 0.6718  (best train acc: 0.6838, best val acc: 0.6857)\n",
      "[Epoch: 24220] train loss: 0.8897, train acc: 0.6392, val loss: 0.8273, val acc: 0.6685  (best train acc: 0.6838, best val acc: 0.6857)\n",
      "[Epoch: 24240] train loss: 0.8247, train acc: 0.6760, val loss: 0.8551, val acc: 0.6570  (best train acc: 0.6838, best val acc: 0.6857)\n",
      "[Epoch: 24260] train loss: 0.8819, train acc: 0.6303, val loss: 0.8274, val acc: 0.6708  (best train acc: 0.6838, best val acc: 0.6857)\n",
      "[Epoch: 24280] train loss: 0.8401, train acc: 0.6604, val loss: 0.8353, val acc: 0.6705  (best train acc: 0.6838, best val acc: 0.6857)\n",
      "[Epoch: 24300] train loss: 0.9008, train acc: 0.6295, val loss: 0.8290, val acc: 0.6745  (best train acc: 0.6838, best val acc: 0.6857)\n",
      "[Epoch: 24320] train loss: 0.8863, train acc: 0.6413, val loss: 0.8303, val acc: 0.6752  (best train acc: 0.6838, best val acc: 0.6857)\n",
      "[Epoch: 24340] train loss: 0.9059, train acc: 0.6243, val loss: 0.8546, val acc: 0.6610  (best train acc: 0.6838, best val acc: 0.6857)\n",
      "[Epoch: 24360] train loss: 0.8932, train acc: 0.6339, val loss: 0.8575, val acc: 0.6577  (best train acc: 0.6838, best val acc: 0.6857)\n",
      "[Epoch: 24380] train loss: 0.8811, train acc: 0.6455, val loss: 0.8325, val acc: 0.6695  (best train acc: 0.6838, best val acc: 0.6857)\n",
      "[Epoch: 24400] train loss: 0.8502, train acc: 0.6594, val loss: 0.8272, val acc: 0.6806  (best train acc: 0.6838, best val acc: 0.6857)\n",
      "[Epoch: 24420] train loss: 0.8389, train acc: 0.6580, val loss: 0.8658, val acc: 0.6331  (best train acc: 0.6838, best val acc: 0.6857)\n",
      "[Epoch: 24440] train loss: 0.8594, train acc: 0.6523, val loss: 0.8383, val acc: 0.6651  (best train acc: 0.6838, best val acc: 0.6857)\n",
      "[Epoch: 24460] train loss: 0.8592, train acc: 0.6536, val loss: 0.8269, val acc: 0.6776  (best train acc: 0.6838, best val acc: 0.6857)\n",
      "[Epoch: 24480] train loss: 0.8636, train acc: 0.6575, val loss: 0.8316, val acc: 0.6681  (best train acc: 0.6838, best val acc: 0.6857)\n",
      "[Epoch: 24500] train loss: 0.8434, train acc: 0.6603, val loss: 0.8177, val acc: 0.6809  (best train acc: 0.6838, best val acc: 0.6857)\n",
      "[Epoch: 24520] train loss: 0.8825, train acc: 0.6366, val loss: 0.8254, val acc: 0.6813  (best train acc: 0.6838, best val acc: 0.6857)\n",
      "[Epoch: 24540] train loss: 0.8582, train acc: 0.6510, val loss: 0.8272, val acc: 0.6718  (best train acc: 0.6838, best val acc: 0.6857)\n",
      "[Epoch: 24560] train loss: 0.8871, train acc: 0.6350, val loss: 0.8354, val acc: 0.6644  (best train acc: 0.6838, best val acc: 0.6857)\n",
      "[Epoch: 24580] train loss: 0.8384, train acc: 0.6601, val loss: 0.8315, val acc: 0.6681  (best train acc: 0.6838, best val acc: 0.6857)\n",
      "[Epoch: 24600] train loss: 0.8521, train acc: 0.6552, val loss: 0.8250, val acc: 0.6702  (best train acc: 0.6838, best val acc: 0.6857)\n",
      "[Epoch: 24620] train loss: 0.8569, train acc: 0.6540, val loss: 0.8506, val acc: 0.6533  (best train acc: 0.6838, best val acc: 0.6867)\n",
      "[Epoch: 24640] train loss: 0.8195, train acc: 0.6699, val loss: 0.8155, val acc: 0.6850  (best train acc: 0.6838, best val acc: 0.6867)\n",
      "[Epoch: 24660] train loss: 0.8475, train acc: 0.6604, val loss: 0.8322, val acc: 0.6688  (best train acc: 0.6838, best val acc: 0.6867)\n",
      "[Epoch: 24680] train loss: 0.8688, train acc: 0.6520, val loss: 0.8201, val acc: 0.6725  (best train acc: 0.6849, best val acc: 0.6867)\n",
      "[Epoch: 24700] train loss: 0.8883, train acc: 0.6316, val loss: 0.8188, val acc: 0.6762  (best train acc: 0.6849, best val acc: 0.6867)\n",
      "[Epoch: 24720] train loss: 0.8580, train acc: 0.6560, val loss: 0.8228, val acc: 0.6739  (best train acc: 0.6849, best val acc: 0.6867)\n",
      "[Epoch: 24740] train loss: 0.8650, train acc: 0.6436, val loss: 0.8356, val acc: 0.6651  (best train acc: 0.6849, best val acc: 0.6867)\n",
      "[Epoch: 24760] train loss: 0.8603, train acc: 0.6562, val loss: 0.8536, val acc: 0.6597  (best train acc: 0.6849, best val acc: 0.6870)\n",
      "[Epoch: 24780] train loss: 0.8975, train acc: 0.6343, val loss: 0.8390, val acc: 0.6634  (best train acc: 0.6849, best val acc: 0.6870)\n",
      "[Epoch: 24800] train loss: 0.8847, train acc: 0.6324, val loss: 0.8225, val acc: 0.6745  (best train acc: 0.6849, best val acc: 0.6870)\n",
      "[Epoch: 24820] train loss: 0.8626, train acc: 0.6514, val loss: 0.8229, val acc: 0.6786  (best train acc: 0.6849, best val acc: 0.6870)\n",
      "[Epoch: 24840] train loss: 0.8555, train acc: 0.6570, val loss: 0.8203, val acc: 0.6816  (best train acc: 0.6849, best val acc: 0.6870)\n",
      "[Epoch: 24860] train loss: 0.8652, train acc: 0.6541, val loss: 0.8166, val acc: 0.6847  (best train acc: 0.6849, best val acc: 0.6870)\n",
      "[Epoch: 24880] train loss: 0.8707, train acc: 0.6526, val loss: 0.8317, val acc: 0.6671  (best train acc: 0.6849, best val acc: 0.6870)\n",
      "[Epoch: 24900] train loss: 0.8581, train acc: 0.6497, val loss: 0.8177, val acc: 0.6766  (best train acc: 0.6849, best val acc: 0.6870)\n",
      "[Epoch: 24920] train loss: 0.8601, train acc: 0.6481, val loss: 0.8176, val acc: 0.6786  (best train acc: 0.6849, best val acc: 0.6870)\n",
      "[Epoch: 24940] train loss: 0.8382, train acc: 0.6562, val loss: 0.8155, val acc: 0.6806  (best train acc: 0.6849, best val acc: 0.6870)\n",
      "[Epoch: 24960] train loss: 0.8890, train acc: 0.6407, val loss: 0.8333, val acc: 0.6624  (best train acc: 0.6857, best val acc: 0.6870)\n",
      "[Epoch: 24980] train loss: 0.8468, train acc: 0.6546, val loss: 0.8137, val acc: 0.6803  (best train acc: 0.6857, best val acc: 0.6870)\n",
      "[Epoch: 25000] train loss: 0.8704, train acc: 0.6499, val loss: 0.8441, val acc: 0.6567  (best train acc: 0.6857, best val acc: 0.6870)\n",
      "[Epoch: 25020] train loss: 0.8753, train acc: 0.6443, val loss: 0.8369, val acc: 0.6722  (best train acc: 0.6857, best val acc: 0.6870)\n",
      "[Epoch: 25040] train loss: 0.8826, train acc: 0.6358, val loss: 0.8523, val acc: 0.6543  (best train acc: 0.6857, best val acc: 0.6870)\n",
      "[Epoch: 25060] train loss: 0.8894, train acc: 0.6328, val loss: 0.8828, val acc: 0.6428  (best train acc: 0.6857, best val acc: 0.6870)\n",
      "[Epoch: 25080] train loss: 0.8863, train acc: 0.6449, val loss: 0.8472, val acc: 0.6607  (best train acc: 0.6857, best val acc: 0.6870)\n",
      "[Epoch: 25100] train loss: 0.8430, train acc: 0.6762, val loss: 0.8286, val acc: 0.6826  (best train acc: 0.6857, best val acc: 0.6870)\n",
      "[Epoch: 25120] train loss: 0.8453, train acc: 0.6688, val loss: 0.8292, val acc: 0.6793  (best train acc: 0.6857, best val acc: 0.6870)\n",
      "[Epoch: 25140] train loss: 0.8899, train acc: 0.6425, val loss: 0.8624, val acc: 0.6550  (best train acc: 0.6857, best val acc: 0.6870)\n",
      "[Epoch: 25160] train loss: 0.8919, train acc: 0.6346, val loss: 0.8291, val acc: 0.6732  (best train acc: 0.6857, best val acc: 0.6870)\n",
      "[Epoch: 25180] train loss: 0.8353, train acc: 0.6709, val loss: 0.8245, val acc: 0.6769  (best train acc: 0.6857, best val acc: 0.6870)\n",
      "[Epoch: 25200] train loss: 0.8716, train acc: 0.6511, val loss: 0.8618, val acc: 0.6536  (best train acc: 0.6857, best val acc: 0.6870)\n",
      "[Epoch: 25220] train loss: 0.8615, train acc: 0.6499, val loss: 0.8155, val acc: 0.6843  (best train acc: 0.6857, best val acc: 0.6870)\n",
      "[Epoch: 25240] train loss: 0.9001, train acc: 0.6290, val loss: 0.8153, val acc: 0.6867  (best train acc: 0.6857, best val acc: 0.6870)\n",
      "[Epoch: 25260] train loss: 0.8659, train acc: 0.6526, val loss: 0.8158, val acc: 0.6813  (best train acc: 0.6857, best val acc: 0.6870)\n",
      "[Epoch: 25280] train loss: 0.8742, train acc: 0.6437, val loss: 0.8313, val acc: 0.6698  (best train acc: 0.6857, best val acc: 0.6870)\n",
      "[Epoch: 25300] train loss: 0.8821, train acc: 0.6382, val loss: 0.8368, val acc: 0.6671  (best train acc: 0.6857, best val acc: 0.6870)\n",
      "[Epoch: 25320] train loss: 0.8945, train acc: 0.6326, val loss: 0.8284, val acc: 0.6708  (best train acc: 0.6857, best val acc: 0.6870)\n",
      "[Epoch: 25340] train loss: 0.8311, train acc: 0.6680, val loss: 0.8184, val acc: 0.6799  (best train acc: 0.6857, best val acc: 0.6870)\n",
      "[Epoch: 25360] train loss: 0.8768, train acc: 0.6457, val loss: 0.8195, val acc: 0.6796  (best train acc: 0.6857, best val acc: 0.6870)\n",
      "[Epoch: 25380] train loss: 0.8337, train acc: 0.6677, val loss: 0.8191, val acc: 0.6752  (best train acc: 0.6857, best val acc: 0.6870)\n",
      "[Epoch: 25400] train loss: 0.8672, train acc: 0.6526, val loss: 0.8522, val acc: 0.6617  (best train acc: 0.6857, best val acc: 0.6870)\n",
      "[Epoch: 25420] train loss: 0.8969, train acc: 0.6260, val loss: 0.8235, val acc: 0.6803  (best train acc: 0.6857, best val acc: 0.6870)\n",
      "[Epoch: 25440] train loss: 0.8714, train acc: 0.6440, val loss: 0.8191, val acc: 0.6796  (best train acc: 0.6857, best val acc: 0.6870)\n",
      "[Epoch: 25460] train loss: 0.8483, train acc: 0.6476, val loss: 0.8168, val acc: 0.6752  (best train acc: 0.6857, best val acc: 0.6870)\n",
      "[Epoch: 25480] train loss: 0.8898, train acc: 0.6374, val loss: 0.8602, val acc: 0.6425  (best train acc: 0.6857, best val acc: 0.6870)\n",
      "[Epoch: 25500] train loss: 0.9222, train acc: 0.6029, val loss: 0.8750, val acc: 0.6452  (best train acc: 0.6857, best val acc: 0.6870)\n",
      "[Epoch: 25520] train loss: 0.8804, train acc: 0.6418, val loss: 0.8312, val acc: 0.6708  (best train acc: 0.6857, best val acc: 0.6870)\n",
      "[Epoch: 25540] train loss: 0.8695, train acc: 0.6522, val loss: 0.8161, val acc: 0.6803  (best train acc: 0.6857, best val acc: 0.6870)\n",
      "[Epoch: 25560] train loss: 0.8266, train acc: 0.6671, val loss: 0.8153, val acc: 0.6786  (best train acc: 0.6857, best val acc: 0.6870)\n",
      "[Epoch: 25580] train loss: 0.8404, train acc: 0.6575, val loss: 0.8144, val acc: 0.6782  (best train acc: 0.6857, best val acc: 0.6870)\n",
      "[Epoch: 25600] train loss: 0.8525, train acc: 0.6608, val loss: 0.8190, val acc: 0.6789  (best train acc: 0.6857, best val acc: 0.6870)\n",
      "[Epoch: 25620] train loss: 0.8470, train acc: 0.6542, val loss: 0.8188, val acc: 0.6702  (best train acc: 0.6857, best val acc: 0.6870)\n",
      "[Epoch: 25640] train loss: 0.8751, train acc: 0.6444, val loss: 0.8181, val acc: 0.6830  (best train acc: 0.6857, best val acc: 0.6870)\n",
      "[Epoch: 25660] train loss: 0.8241, train acc: 0.6711, val loss: 0.8279, val acc: 0.6691  (best train acc: 0.6857, best val acc: 0.6870)\n",
      "[Epoch: 25680] train loss: 0.8607, train acc: 0.6489, val loss: 0.8225, val acc: 0.6766  (best train acc: 0.6857, best val acc: 0.6870)\n",
      "[Epoch: 25700] train loss: 0.9426, train acc: 0.5981, val loss: 0.8231, val acc: 0.6752  (best train acc: 0.6857, best val acc: 0.6870)\n",
      "[Epoch: 25720] train loss: 0.8944, train acc: 0.6392, val loss: 0.8316, val acc: 0.6702  (best train acc: 0.6857, best val acc: 0.6870)\n",
      "[Epoch: 25740] train loss: 0.8681, train acc: 0.6396, val loss: 0.8187, val acc: 0.6806  (best train acc: 0.6857, best val acc: 0.6870)\n",
      "[Epoch: 25760] train loss: 0.8654, train acc: 0.6513, val loss: 0.8236, val acc: 0.6752  (best train acc: 0.6857, best val acc: 0.6870)\n",
      "[Epoch: 25780] train loss: 0.9173, train acc: 0.6236, val loss: 0.8201, val acc: 0.6833  (best train acc: 0.6857, best val acc: 0.6870)\n",
      "[Epoch: 25800] train loss: 0.8231, train acc: 0.6836, val loss: 0.8232, val acc: 0.6803  (best train acc: 0.6857, best val acc: 0.6870)\n",
      "[Epoch: 25820] train loss: 0.8619, train acc: 0.6467, val loss: 0.8213, val acc: 0.6772  (best train acc: 0.6857, best val acc: 0.6870)\n",
      "[Epoch: 25840] train loss: 0.8387, train acc: 0.6690, val loss: 0.8573, val acc: 0.6594  (best train acc: 0.6857, best val acc: 0.6870)\n",
      "[Epoch: 25860] train loss: 0.9563, train acc: 0.5881, val loss: 0.8270, val acc: 0.6749  (best train acc: 0.6857, best val acc: 0.6870)\n",
      "[Epoch: 25880] train loss: 0.8771, train acc: 0.6339, val loss: 0.8296, val acc: 0.6705  (best train acc: 0.6857, best val acc: 0.6870)\n",
      "[Epoch: 25900] train loss: 0.8375, train acc: 0.6606, val loss: 0.8344, val acc: 0.6661  (best train acc: 0.6857, best val acc: 0.6870)\n",
      "[Epoch: 25920] train loss: 0.8718, train acc: 0.6486, val loss: 0.8289, val acc: 0.6702  (best train acc: 0.6857, best val acc: 0.6870)\n",
      "[Epoch: 25940] train loss: 0.8375, train acc: 0.6664, val loss: 0.8240, val acc: 0.6745  (best train acc: 0.6857, best val acc: 0.6870)\n",
      "[Epoch: 25960] train loss: 0.8594, train acc: 0.6484, val loss: 0.8148, val acc: 0.6803  (best train acc: 0.6857, best val acc: 0.6870)\n",
      "[Epoch: 25980] train loss: 0.8703, train acc: 0.6431, val loss: 0.8139, val acc: 0.6853  (best train acc: 0.6857, best val acc: 0.6870)\n",
      "[Epoch: 26000] train loss: 0.8489, train acc: 0.6560, val loss: 0.8581, val acc: 0.6469  (best train acc: 0.6857, best val acc: 0.6870)\n",
      "[Epoch: 26020] train loss: 0.9002, train acc: 0.6331, val loss: 0.8162, val acc: 0.6779  (best train acc: 0.6857, best val acc: 0.6870)\n",
      "[Epoch: 26040] train loss: 0.8929, train acc: 0.6335, val loss: 0.8218, val acc: 0.6813  (best train acc: 0.6857, best val acc: 0.6870)\n",
      "[Epoch: 26060] train loss: 0.8906, train acc: 0.6318, val loss: 0.8312, val acc: 0.6688  (best train acc: 0.6857, best val acc: 0.6870)\n",
      "[Epoch: 26080] train loss: 0.8710, train acc: 0.6424, val loss: 0.8157, val acc: 0.6816  (best train acc: 0.6857, best val acc: 0.6870)\n",
      "[Epoch: 26100] train loss: 0.8619, train acc: 0.6498, val loss: 0.8151, val acc: 0.6799  (best train acc: 0.6857, best val acc: 0.6870)\n",
      "[Epoch: 26120] train loss: 0.8532, train acc: 0.6568, val loss: 0.8177, val acc: 0.6749  (best train acc: 0.6857, best val acc: 0.6870)\n",
      "[Epoch: 26140] train loss: 0.8943, train acc: 0.6301, val loss: 0.8122, val acc: 0.6836  (best train acc: 0.6857, best val acc: 0.6870)\n",
      "[Epoch: 26160] train loss: 0.8322, train acc: 0.6620, val loss: 0.8113, val acc: 0.6826  (best train acc: 0.6857, best val acc: 0.6870)\n",
      "[Epoch: 26180] train loss: 0.9198, train acc: 0.6115, val loss: 0.8343, val acc: 0.6695  (best train acc: 0.6857, best val acc: 0.6870)\n",
      "[Epoch: 26200] train loss: 0.8598, train acc: 0.6514, val loss: 0.8159, val acc: 0.6782  (best train acc: 0.6857, best val acc: 0.6870)\n",
      "[Epoch: 26220] train loss: 0.8890, train acc: 0.6343, val loss: 0.8141, val acc: 0.6796  (best train acc: 0.6857, best val acc: 0.6874)\n",
      "[Epoch: 26240] train loss: 0.8261, train acc: 0.6712, val loss: 0.8455, val acc: 0.6600  (best train acc: 0.6857, best val acc: 0.6874)\n",
      "[Epoch: 26260] train loss: 0.8392, train acc: 0.6627, val loss: 0.8169, val acc: 0.6772  (best train acc: 0.6857, best val acc: 0.6874)\n",
      "[Epoch: 26280] train loss: 0.9130, train acc: 0.6234, val loss: 0.8314, val acc: 0.6651  (best train acc: 0.6857, best val acc: 0.6874)\n",
      "[Epoch: 26300] train loss: 0.9393, train acc: 0.6110, val loss: 0.8292, val acc: 0.6732  (best train acc: 0.6857, best val acc: 0.6874)\n",
      "[Epoch: 26320] train loss: 0.8842, train acc: 0.6379, val loss: 0.8228, val acc: 0.6718  (best train acc: 0.6857, best val acc: 0.6874)\n",
      "[Epoch: 26340] train loss: 0.8609, train acc: 0.6505, val loss: 0.8132, val acc: 0.6816  (best train acc: 0.6857, best val acc: 0.6874)\n",
      "[Epoch: 26360] train loss: 0.8125, train acc: 0.6807, val loss: 0.8713, val acc: 0.6256  (best train acc: 0.6857, best val acc: 0.6874)\n",
      "[Epoch: 26380] train loss: 0.8391, train acc: 0.6692, val loss: 0.8162, val acc: 0.6816  (best train acc: 0.6857, best val acc: 0.6874)\n",
      "[Epoch: 26400] train loss: 0.8689, train acc: 0.6440, val loss: 0.8147, val acc: 0.6796  (best train acc: 0.6857, best val acc: 0.6874)\n",
      "[Epoch: 26420] train loss: 0.8323, train acc: 0.6609, val loss: 0.8426, val acc: 0.6604  (best train acc: 0.6857, best val acc: 0.6874)\n",
      "[Epoch: 26440] train loss: 0.8694, train acc: 0.6416, val loss: 0.8348, val acc: 0.6634  (best train acc: 0.6857, best val acc: 0.6874)\n",
      "[Epoch: 26460] train loss: 0.8977, train acc: 0.6110, val loss: 0.8174, val acc: 0.6793  (best train acc: 0.6857, best val acc: 0.6874)\n",
      "[Epoch: 26480] train loss: 0.8722, train acc: 0.6418, val loss: 0.8199, val acc: 0.6762  (best train acc: 0.6857, best val acc: 0.6874)\n",
      "[Epoch: 26500] train loss: 0.8890, train acc: 0.6327, val loss: 0.8264, val acc: 0.6715  (best train acc: 0.6857, best val acc: 0.6874)\n",
      "[Epoch: 26520] train loss: 0.9062, train acc: 0.6219, val loss: 0.8149, val acc: 0.6803  (best train acc: 0.6857, best val acc: 0.6874)\n",
      "[Epoch: 26540] train loss: 0.8928, train acc: 0.6277, val loss: 0.8152, val acc: 0.6826  (best train acc: 0.6857, best val acc: 0.6874)\n",
      "[Epoch: 26560] train loss: 0.8868, train acc: 0.6352, val loss: 0.8296, val acc: 0.6712  (best train acc: 0.6857, best val acc: 0.6874)\n",
      "[Epoch: 26580] train loss: 0.8445, train acc: 0.6562, val loss: 0.8187, val acc: 0.6745  (best train acc: 0.6857, best val acc: 0.6874)\n",
      "[Epoch: 26600] train loss: 0.9066, train acc: 0.6246, val loss: 0.8240, val acc: 0.6715  (best train acc: 0.6857, best val acc: 0.6874)\n",
      "[Epoch: 26620] train loss: 0.8814, train acc: 0.6358, val loss: 0.8340, val acc: 0.6661  (best train acc: 0.6857, best val acc: 0.6874)\n",
      "[Epoch: 26640] train loss: 0.8646, train acc: 0.6476, val loss: 0.8341, val acc: 0.6695  (best train acc: 0.6857, best val acc: 0.6874)\n",
      "[Epoch: 26660] train loss: 0.8709, train acc: 0.6428, val loss: 0.8137, val acc: 0.6840  (best train acc: 0.6857, best val acc: 0.6874)\n",
      "[Epoch: 26680] train loss: 0.8836, train acc: 0.6432, val loss: 0.8266, val acc: 0.6728  (best train acc: 0.6857, best val acc: 0.6874)\n",
      "[Epoch: 26700] train loss: 0.9248, train acc: 0.6170, val loss: 0.8175, val acc: 0.6789  (best train acc: 0.6857, best val acc: 0.6874)\n",
      "[Epoch: 26720] train loss: 0.8635, train acc: 0.6546, val loss: 0.8110, val acc: 0.6803  (best train acc: 0.6857, best val acc: 0.6874)\n",
      "[Epoch: 26740] train loss: 0.8830, train acc: 0.6334, val loss: 0.8129, val acc: 0.6853  (best train acc: 0.6857, best val acc: 0.6874)\n",
      "[Epoch: 26760] train loss: 0.8662, train acc: 0.6502, val loss: 0.8317, val acc: 0.6705  (best train acc: 0.6857, best val acc: 0.6874)\n",
      "[Epoch: 26780] train loss: 0.8420, train acc: 0.6604, val loss: 0.8144, val acc: 0.6782  (best train acc: 0.6857, best val acc: 0.6884)\n",
      "[Epoch: 26800] train loss: 0.8847, train acc: 0.6332, val loss: 0.8141, val acc: 0.6826  (best train acc: 0.6857, best val acc: 0.6884)\n",
      "[Epoch: 26820] train loss: 0.8734, train acc: 0.6464, val loss: 0.8210, val acc: 0.6803  (best train acc: 0.6857, best val acc: 0.6884)\n",
      "[Epoch: 26840] train loss: 0.8639, train acc: 0.6626, val loss: 0.8475, val acc: 0.6624  (best train acc: 0.6857, best val acc: 0.6884)\n",
      "[Epoch: 26860] train loss: 0.8918, train acc: 0.6249, val loss: 0.8184, val acc: 0.6830  (best train acc: 0.6857, best val acc: 0.6884)\n",
      "[Epoch: 26880] train loss: 0.8687, train acc: 0.6512, val loss: 0.8130, val acc: 0.6786  (best train acc: 0.6857, best val acc: 0.6884)\n",
      "[Epoch: 26900] train loss: 0.8555, train acc: 0.6502, val loss: 0.8215, val acc: 0.6789  (best train acc: 0.6857, best val acc: 0.6884)\n",
      "[Epoch: 26920] train loss: 0.8800, train acc: 0.6424, val loss: 0.8451, val acc: 0.6631  (best train acc: 0.6857, best val acc: 0.6884)\n",
      "[Epoch: 26940] train loss: 0.8540, train acc: 0.6575, val loss: 0.8157, val acc: 0.6853  (best train acc: 0.6857, best val acc: 0.6884)\n",
      "[Epoch: 26960] train loss: 0.8436, train acc: 0.6565, val loss: 0.8333, val acc: 0.6712  (best train acc: 0.6857, best val acc: 0.6884)\n",
      "[Epoch: 26980] train loss: 0.8953, train acc: 0.6316, val loss: 0.8188, val acc: 0.6793  (best train acc: 0.6857, best val acc: 0.6884)\n",
      "[Epoch: 27000] train loss: 0.8606, train acc: 0.6487, val loss: 0.8292, val acc: 0.6691  (best train acc: 0.6857, best val acc: 0.6884)\n",
      "[Epoch: 27020] train loss: 0.8376, train acc: 0.6614, val loss: 0.8150, val acc: 0.6799  (best train acc: 0.6857, best val acc: 0.6884)\n",
      "[Epoch: 27040] train loss: 0.8777, train acc: 0.6365, val loss: 0.8332, val acc: 0.6671  (best train acc: 0.6857, best val acc: 0.6884)\n",
      "[Epoch: 27060] train loss: 0.8945, train acc: 0.6303, val loss: 0.8231, val acc: 0.6759  (best train acc: 0.6857, best val acc: 0.6884)\n",
      "[Epoch: 27080] train loss: 0.8302, train acc: 0.6679, val loss: 0.8292, val acc: 0.6708  (best train acc: 0.6857, best val acc: 0.6884)\n",
      "[Epoch: 27100] train loss: 0.8555, train acc: 0.6556, val loss: 0.8127, val acc: 0.6809  (best train acc: 0.6857, best val acc: 0.6884)\n",
      "[Epoch: 27120] train loss: 0.8719, train acc: 0.6396, val loss: 0.8607, val acc: 0.6573  (best train acc: 0.6857, best val acc: 0.6884)\n",
      "[Epoch: 27140] train loss: 0.8133, train acc: 0.6757, val loss: 0.8241, val acc: 0.6749  (best train acc: 0.6857, best val acc: 0.6884)\n",
      "[Epoch: 27160] train loss: 0.8682, train acc: 0.6431, val loss: 0.8306, val acc: 0.6661  (best train acc: 0.6857, best val acc: 0.6884)\n",
      "[Epoch: 27180] train loss: 0.8734, train acc: 0.6366, val loss: 0.8304, val acc: 0.6739  (best train acc: 0.6857, best val acc: 0.6884)\n",
      "[Epoch: 27200] train loss: 0.8591, train acc: 0.6473, val loss: 0.8161, val acc: 0.6816  (best train acc: 0.6857, best val acc: 0.6884)\n",
      "[Epoch: 27220] train loss: 0.9053, train acc: 0.6254, val loss: 0.8332, val acc: 0.6685  (best train acc: 0.6857, best val acc: 0.6884)\n",
      "[Epoch: 27240] train loss: 0.8345, train acc: 0.6667, val loss: 0.8162, val acc: 0.6857  (best train acc: 0.6857, best val acc: 0.6884)\n",
      "[Epoch: 27260] train loss: 0.8679, train acc: 0.6497, val loss: 0.8293, val acc: 0.6702  (best train acc: 0.6857, best val acc: 0.6884)\n",
      "[Epoch: 27280] train loss: 0.8644, train acc: 0.6429, val loss: 0.8105, val acc: 0.6860  (best train acc: 0.6857, best val acc: 0.6884)\n",
      "[Epoch: 27300] train loss: 0.9278, train acc: 0.6099, val loss: 0.8203, val acc: 0.6809  (best train acc: 0.6857, best val acc: 0.6884)\n",
      "[Epoch: 27320] train loss: 0.9149, train acc: 0.6175, val loss: 0.8411, val acc: 0.6658  (best train acc: 0.6857, best val acc: 0.6884)\n",
      "[Epoch: 27340] train loss: 0.8459, train acc: 0.6672, val loss: 0.8332, val acc: 0.6698  (best train acc: 0.6857, best val acc: 0.6884)\n",
      "[Epoch: 27360] train loss: 0.8664, train acc: 0.6502, val loss: 0.8111, val acc: 0.6853  (best train acc: 0.6857, best val acc: 0.6884)\n",
      "[Epoch: 27380] train loss: 0.8866, train acc: 0.6456, val loss: 0.8114, val acc: 0.6813  (best train acc: 0.6857, best val acc: 0.6884)\n",
      "[Epoch: 27400] train loss: 0.8870, train acc: 0.6320, val loss: 0.8147, val acc: 0.6836  (best train acc: 0.6857, best val acc: 0.6884)\n",
      "[Epoch: 27420] train loss: 0.8667, train acc: 0.6481, val loss: 0.8436, val acc: 0.6675  (best train acc: 0.6857, best val acc: 0.6884)\n",
      "[Epoch: 27440] train loss: 0.8580, train acc: 0.6465, val loss: 0.8143, val acc: 0.6857  (best train acc: 0.6857, best val acc: 0.6884)\n",
      "[Epoch: 27460] train loss: 0.8563, train acc: 0.6504, val loss: 0.8228, val acc: 0.6776  (best train acc: 0.6857, best val acc: 0.6884)\n",
      "[Epoch: 27480] train loss: 0.8247, train acc: 0.6709, val loss: 0.8113, val acc: 0.6853  (best train acc: 0.6857, best val acc: 0.6884)\n",
      "[Epoch: 27500] train loss: 0.8622, train acc: 0.6383, val loss: 0.8103, val acc: 0.6863  (best train acc: 0.6857, best val acc: 0.6884)\n",
      "[Epoch: 27520] train loss: 0.8450, train acc: 0.6564, val loss: 0.8151, val acc: 0.6762  (best train acc: 0.6857, best val acc: 0.6884)\n",
      "[Epoch: 27540] train loss: 0.8608, train acc: 0.6456, val loss: 0.8130, val acc: 0.6816  (best train acc: 0.6857, best val acc: 0.6884)\n",
      "[Epoch: 27560] train loss: 0.8453, train acc: 0.6562, val loss: 0.8098, val acc: 0.6833  (best train acc: 0.6857, best val acc: 0.6884)\n",
      "[Epoch: 27580] train loss: 0.8786, train acc: 0.6404, val loss: 0.8104, val acc: 0.6799  (best train acc: 0.6857, best val acc: 0.6894)\n",
      "[Epoch: 27600] train loss: 0.8968, train acc: 0.6310, val loss: 0.8083, val acc: 0.6850  (best train acc: 0.6857, best val acc: 0.6894)\n",
      "[Epoch: 27620] train loss: 0.8885, train acc: 0.6364, val loss: 0.8120, val acc: 0.6820  (best train acc: 0.6857, best val acc: 0.6894)\n",
      "[Epoch: 27640] train loss: 0.8993, train acc: 0.6272, val loss: 0.8218, val acc: 0.6732  (best train acc: 0.6857, best val acc: 0.6894)\n",
      "[Epoch: 27660] train loss: 0.8497, train acc: 0.6511, val loss: 0.8140, val acc: 0.6796  (best train acc: 0.6857, best val acc: 0.6894)\n",
      "[Epoch: 27680] train loss: 0.8244, train acc: 0.6736, val loss: 0.8110, val acc: 0.6813  (best train acc: 0.6857, best val acc: 0.6894)\n",
      "[Epoch: 27700] train loss: 0.8579, train acc: 0.6472, val loss: 0.8091, val acc: 0.6867  (best train acc: 0.6857, best val acc: 0.6894)\n",
      "[Epoch: 27720] train loss: 0.8255, train acc: 0.6698, val loss: 0.8236, val acc: 0.6772  (best train acc: 0.6857, best val acc: 0.6894)\n",
      "[Epoch: 27740] train loss: 0.9173, train acc: 0.6188, val loss: 0.8485, val acc: 0.6550  (best train acc: 0.6857, best val acc: 0.6894)\n",
      "[Epoch: 27760] train loss: 0.8684, train acc: 0.6418, val loss: 0.8244, val acc: 0.6739  (best train acc: 0.6857, best val acc: 0.6894)\n",
      "[Epoch: 27780] train loss: 0.8359, train acc: 0.6645, val loss: 0.8103, val acc: 0.6833  (best train acc: 0.6857, best val acc: 0.6894)\n",
      "[Epoch: 27800] train loss: 0.8275, train acc: 0.6752, val loss: 0.8282, val acc: 0.6745  (best train acc: 0.6857, best val acc: 0.6894)\n",
      "[Epoch: 27820] train loss: 0.8543, train acc: 0.6566, val loss: 0.8269, val acc: 0.6732  (best train acc: 0.6857, best val acc: 0.6894)\n",
      "[Epoch: 27840] train loss: 0.8964, train acc: 0.6285, val loss: 0.8216, val acc: 0.6769  (best train acc: 0.6857, best val acc: 0.6894)\n",
      "[Epoch: 27860] train loss: 0.9072, train acc: 0.6189, val loss: 0.8133, val acc: 0.6813  (best train acc: 0.6857, best val acc: 0.6894)\n",
      "[Epoch: 27880] train loss: 0.8464, train acc: 0.6517, val loss: 0.8202, val acc: 0.6742  (best train acc: 0.6857, best val acc: 0.6894)\n",
      "[Epoch: 27900] train loss: 0.8373, train acc: 0.6604, val loss: 0.8110, val acc: 0.6840  (best train acc: 0.6857, best val acc: 0.6894)\n",
      "[Epoch: 27920] train loss: 0.8839, train acc: 0.6345, val loss: 0.8592, val acc: 0.6570  (best train acc: 0.6857, best val acc: 0.6894)\n",
      "[Epoch: 27940] train loss: 0.8802, train acc: 0.6416, val loss: 0.8505, val acc: 0.6590  (best train acc: 0.6857, best val acc: 0.6894)\n",
      "[Epoch: 27960] train loss: 0.8718, train acc: 0.6470, val loss: 0.8088, val acc: 0.6860  (best train acc: 0.6857, best val acc: 0.6894)\n",
      "[Epoch: 27980] train loss: 0.8690, train acc: 0.6460, val loss: 0.8167, val acc: 0.6816  (best train acc: 0.6857, best val acc: 0.6894)\n",
      "[Epoch: 28000] train loss: 0.8575, train acc: 0.6569, val loss: 0.8107, val acc: 0.6840  (best train acc: 0.6857, best val acc: 0.6894)\n",
      "[Epoch: 28020] train loss: 0.8732, train acc: 0.6508, val loss: 0.8094, val acc: 0.6860  (best train acc: 0.6857, best val acc: 0.6894)\n",
      "[Epoch: 28040] train loss: 0.8484, train acc: 0.6585, val loss: 0.8233, val acc: 0.6742  (best train acc: 0.6857, best val acc: 0.6894)\n",
      "[Epoch: 28060] train loss: 0.8576, train acc: 0.6500, val loss: 0.8086, val acc: 0.6823  (best train acc: 0.6857, best val acc: 0.6894)\n",
      "[Epoch: 28080] train loss: 0.8385, train acc: 0.6702, val loss: 0.8281, val acc: 0.6769  (best train acc: 0.6857, best val acc: 0.6894)\n",
      "[Epoch: 28100] train loss: 0.8431, train acc: 0.6604, val loss: 0.8144, val acc: 0.6850  (best train acc: 0.6857, best val acc: 0.6894)\n",
      "[Epoch: 28120] train loss: 0.8856, train acc: 0.6369, val loss: 0.8112, val acc: 0.6830  (best train acc: 0.6857, best val acc: 0.6894)\n",
      "[Epoch: 28140] train loss: 0.8817, train acc: 0.6442, val loss: 0.8093, val acc: 0.6840  (best train acc: 0.6857, best val acc: 0.6894)\n",
      "[Epoch: 28160] train loss: 0.8718, train acc: 0.6330, val loss: 0.8149, val acc: 0.6816  (best train acc: 0.6857, best val acc: 0.6894)\n",
      "[Epoch: 28180] train loss: 0.8287, train acc: 0.6680, val loss: 0.8101, val acc: 0.6857  (best train acc: 0.6857, best val acc: 0.6894)\n",
      "[Epoch: 28200] train loss: 0.8228, train acc: 0.6742, val loss: 0.8226, val acc: 0.6742  (best train acc: 0.6857, best val acc: 0.6894)\n",
      "[Epoch: 28220] train loss: 0.8714, train acc: 0.6450, val loss: 0.8111, val acc: 0.6816  (best train acc: 0.6857, best val acc: 0.6894)\n",
      "[Epoch: 28240] train loss: 0.8324, train acc: 0.6611, val loss: 0.8224, val acc: 0.6762  (best train acc: 0.6857, best val acc: 0.6894)\n",
      "[Epoch: 28260] train loss: 0.8870, train acc: 0.6261, val loss: 0.8148, val acc: 0.6857  (best train acc: 0.6857, best val acc: 0.6894)\n",
      "[Epoch: 28280] train loss: 0.8693, train acc: 0.6465, val loss: 0.8748, val acc: 0.6216  (best train acc: 0.6857, best val acc: 0.6894)\n",
      "[Epoch: 28300] train loss: 0.8587, train acc: 0.6531, val loss: 0.8137, val acc: 0.6809  (best train acc: 0.6857, best val acc: 0.6894)\n",
      "[Epoch: 28320] train loss: 0.8502, train acc: 0.6609, val loss: 0.8160, val acc: 0.6772  (best train acc: 0.6857, best val acc: 0.6894)\n",
      "[Epoch: 28340] train loss: 0.8508, train acc: 0.6510, val loss: 0.8112, val acc: 0.6833  (best train acc: 0.6857, best val acc: 0.6894)\n",
      "[Epoch: 28360] train loss: 0.8821, train acc: 0.6389, val loss: 0.8104, val acc: 0.6786  (best train acc: 0.6857, best val acc: 0.6897)\n",
      "[Epoch: 28380] train loss: 0.8664, train acc: 0.6469, val loss: 0.8100, val acc: 0.6843  (best train acc: 0.6857, best val acc: 0.6897)\n",
      "[Epoch: 28400] train loss: 0.9566, train acc: 0.5789, val loss: 0.8263, val acc: 0.6735  (best train acc: 0.6857, best val acc: 0.6897)\n",
      "[Epoch: 28420] train loss: 0.8516, train acc: 0.6523, val loss: 0.8148, val acc: 0.6809  (best train acc: 0.6857, best val acc: 0.6897)\n",
      "[Epoch: 28440] train loss: 0.8700, train acc: 0.6523, val loss: 0.8728, val acc: 0.6486  (best train acc: 0.6857, best val acc: 0.6897)\n",
      "[Epoch: 28460] train loss: 0.8595, train acc: 0.6538, val loss: 0.8515, val acc: 0.6627  (best train acc: 0.6857, best val acc: 0.6897)\n",
      "[Epoch: 28480] train loss: 0.8302, train acc: 0.6674, val loss: 0.8319, val acc: 0.6685  (best train acc: 0.6857, best val acc: 0.6897)\n",
      "[Epoch: 28500] train loss: 0.8473, train acc: 0.6663, val loss: 0.8253, val acc: 0.6712  (best train acc: 0.6857, best val acc: 0.6897)\n",
      "[Epoch: 28520] train loss: 0.8379, train acc: 0.6618, val loss: 0.8113, val acc: 0.6877  (best train acc: 0.6857, best val acc: 0.6897)\n",
      "[Epoch: 28540] train loss: 0.8559, train acc: 0.6596, val loss: 0.8137, val acc: 0.6826  (best train acc: 0.6857, best val acc: 0.6897)\n",
      "[Epoch: 28560] train loss: 0.8459, train acc: 0.6608, val loss: 0.8088, val acc: 0.6850  (best train acc: 0.6857, best val acc: 0.6897)\n",
      "[Epoch: 28580] train loss: 0.8392, train acc: 0.6616, val loss: 0.8210, val acc: 0.6769  (best train acc: 0.6857, best val acc: 0.6897)\n",
      "[Epoch: 28600] train loss: 0.8425, train acc: 0.6585, val loss: 0.8370, val acc: 0.6648  (best train acc: 0.6857, best val acc: 0.6897)\n",
      "[Epoch: 28620] train loss: 0.8620, train acc: 0.6479, val loss: 0.8136, val acc: 0.6809  (best train acc: 0.6857, best val acc: 0.6897)\n",
      "[Epoch: 28640] train loss: 0.8404, train acc: 0.6639, val loss: 0.8441, val acc: 0.6580  (best train acc: 0.6857, best val acc: 0.6897)\n",
      "[Epoch: 28660] train loss: 0.8129, train acc: 0.6743, val loss: 0.8376, val acc: 0.6637  (best train acc: 0.6857, best val acc: 0.6897)\n",
      "[Epoch: 28680] train loss: 0.8396, train acc: 0.6613, val loss: 0.8205, val acc: 0.6749  (best train acc: 0.6857, best val acc: 0.6897)\n",
      "[Epoch: 28700] train loss: 0.8643, train acc: 0.6585, val loss: 0.8208, val acc: 0.6722  (best train acc: 0.6857, best val acc: 0.6897)\n",
      "[Epoch: 28720] train loss: 0.8272, train acc: 0.6723, val loss: 0.8072, val acc: 0.6853  (best train acc: 0.6857, best val acc: 0.6901)\n",
      "[Epoch: 28740] train loss: 0.8669, train acc: 0.6537, val loss: 0.8353, val acc: 0.6675  (best train acc: 0.6857, best val acc: 0.6901)\n",
      "[Epoch: 28760] train loss: 0.8452, train acc: 0.6582, val loss: 0.8073, val acc: 0.6870  (best train acc: 0.6857, best val acc: 0.6901)\n",
      "[Epoch: 28780] train loss: 0.8400, train acc: 0.6616, val loss: 0.8057, val acc: 0.6884  (best train acc: 0.6857, best val acc: 0.6901)\n",
      "[Epoch: 28800] train loss: 0.8651, train acc: 0.6520, val loss: 0.8172, val acc: 0.6782  (best train acc: 0.6880, best val acc: 0.6901)\n",
      "[Epoch: 28820] train loss: 0.8286, train acc: 0.6676, val loss: 0.8075, val acc: 0.6853  (best train acc: 0.6880, best val acc: 0.6901)\n",
      "[Epoch: 28840] train loss: 0.8504, train acc: 0.6629, val loss: 0.8266, val acc: 0.6732  (best train acc: 0.6880, best val acc: 0.6901)\n",
      "[Epoch: 28860] train loss: 0.8733, train acc: 0.6421, val loss: 0.8071, val acc: 0.6857  (best train acc: 0.6880, best val acc: 0.6901)\n",
      "[Epoch: 28880] train loss: 0.8167, train acc: 0.6763, val loss: 0.8060, val acc: 0.6863  (best train acc: 0.6880, best val acc: 0.6901)\n",
      "[Epoch: 28900] train loss: 0.8562, train acc: 0.6535, val loss: 0.8070, val acc: 0.6840  (best train acc: 0.6880, best val acc: 0.6901)\n",
      "[Epoch: 28920] train loss: 0.8409, train acc: 0.6623, val loss: 0.8302, val acc: 0.6715  (best train acc: 0.6880, best val acc: 0.6901)\n",
      "[Epoch: 28940] train loss: 0.8538, train acc: 0.6525, val loss: 0.8081, val acc: 0.6830  (best train acc: 0.6880, best val acc: 0.6901)\n",
      "[Epoch: 28960] train loss: 0.8229, train acc: 0.6729, val loss: 0.8075, val acc: 0.6870  (best train acc: 0.6880, best val acc: 0.6911)\n",
      "[Epoch: 28980] train loss: 0.8544, train acc: 0.6505, val loss: 0.8212, val acc: 0.6752  (best train acc: 0.6880, best val acc: 0.6911)\n",
      "[Epoch: 29000] train loss: 0.8843, train acc: 0.6289, val loss: 0.8078, val acc: 0.6853  (best train acc: 0.6880, best val acc: 0.6911)\n",
      "[Epoch: 29020] train loss: 0.8668, train acc: 0.6487, val loss: 0.8139, val acc: 0.6820  (best train acc: 0.6880, best val acc: 0.6911)\n",
      "[Epoch: 29040] train loss: 0.8575, train acc: 0.6447, val loss: 0.8072, val acc: 0.6857  (best train acc: 0.6880, best val acc: 0.6911)\n",
      "[Epoch: 29060] train loss: 0.8206, train acc: 0.6755, val loss: 0.8719, val acc: 0.6260  (best train acc: 0.6880, best val acc: 0.6911)\n",
      "[Epoch: 29080] train loss: 0.8334, train acc: 0.6705, val loss: 0.8168, val acc: 0.6769  (best train acc: 0.6880, best val acc: 0.6911)\n",
      "[Epoch: 29100] train loss: 0.8773, train acc: 0.6392, val loss: 0.8282, val acc: 0.6762  (best train acc: 0.6880, best val acc: 0.6911)\n",
      "[Epoch: 29120] train loss: 0.8648, train acc: 0.6493, val loss: 0.8100, val acc: 0.6836  (best train acc: 0.6880, best val acc: 0.6911)\n",
      "[Epoch: 29140] train loss: 0.8299, train acc: 0.6671, val loss: 0.8092, val acc: 0.6867  (best train acc: 0.6880, best val acc: 0.6911)\n",
      "[Epoch: 29160] train loss: 0.8318, train acc: 0.6654, val loss: 0.8194, val acc: 0.6742  (best train acc: 0.6880, best val acc: 0.6911)\n",
      "[Epoch: 29180] train loss: 0.8998, train acc: 0.6226, val loss: 0.8211, val acc: 0.6769  (best train acc: 0.6880, best val acc: 0.6911)\n",
      "[Epoch: 29200] train loss: 0.8650, train acc: 0.6508, val loss: 0.8175, val acc: 0.6745  (best train acc: 0.6880, best val acc: 0.6911)\n",
      "[Epoch: 29220] train loss: 0.8719, train acc: 0.6575, val loss: 0.8328, val acc: 0.6793  (best train acc: 0.6880, best val acc: 0.6911)\n",
      "[Epoch: 29240] train loss: 0.8877, train acc: 0.6459, val loss: 0.8291, val acc: 0.6857  (best train acc: 0.6880, best val acc: 0.6911)\n",
      "[Epoch: 29260] train loss: 0.9100, train acc: 0.6283, val loss: 0.8253, val acc: 0.6850  (best train acc: 0.6880, best val acc: 0.6911)\n",
      "[Epoch: 29280] train loss: 0.8453, train acc: 0.6696, val loss: 0.8217, val acc: 0.6867  (best train acc: 0.6880, best val acc: 0.6911)\n",
      "[Epoch: 29300] train loss: 0.9011, train acc: 0.6333, val loss: 0.8239, val acc: 0.6786  (best train acc: 0.6880, best val acc: 0.6911)\n",
      "[Epoch: 29320] train loss: 0.8955, train acc: 0.6400, val loss: 0.8145, val acc: 0.6779  (best train acc: 0.6880, best val acc: 0.6911)\n",
      "[Epoch: 29340] train loss: 0.8358, train acc: 0.6642, val loss: 0.8076, val acc: 0.6877  (best train acc: 0.6880, best val acc: 0.6911)\n",
      "[Epoch: 29360] train loss: 0.8189, train acc: 0.6748, val loss: 0.8067, val acc: 0.6894  (best train acc: 0.6880, best val acc: 0.6911)\n",
      "[Epoch: 29380] train loss: 0.8810, train acc: 0.6298, val loss: 0.8053, val acc: 0.6887  (best train acc: 0.6880, best val acc: 0.6911)\n",
      "[Epoch: 29400] train loss: 0.8376, train acc: 0.6617, val loss: 0.8075, val acc: 0.6890  (best train acc: 0.6880, best val acc: 0.6911)\n",
      "[Epoch: 29420] train loss: 0.8367, train acc: 0.6624, val loss: 0.8112, val acc: 0.6806  (best train acc: 0.6880, best val acc: 0.6911)\n",
      "[Epoch: 29440] train loss: 0.8759, train acc: 0.6486, val loss: 0.8091, val acc: 0.6840  (best train acc: 0.6880, best val acc: 0.6911)\n",
      "[Epoch: 29460] train loss: 0.8422, train acc: 0.6578, val loss: 0.8191, val acc: 0.6833  (best train acc: 0.6880, best val acc: 0.6911)\n",
      "[Epoch: 29480] train loss: 0.8362, train acc: 0.6658, val loss: 0.8147, val acc: 0.6863  (best train acc: 0.6880, best val acc: 0.6911)\n",
      "[Epoch: 29500] train loss: 0.8420, train acc: 0.6687, val loss: 0.8087, val acc: 0.6890  (best train acc: 0.6880, best val acc: 0.6911)\n",
      "[Epoch: 29520] train loss: 0.8450, train acc: 0.6565, val loss: 0.8110, val acc: 0.6843  (best train acc: 0.6880, best val acc: 0.6911)\n",
      "[Epoch: 29540] train loss: 0.9051, train acc: 0.6286, val loss: 0.8185, val acc: 0.6779  (best train acc: 0.6880, best val acc: 0.6911)\n",
      "[Epoch: 29560] train loss: 0.8759, train acc: 0.6468, val loss: 0.8106, val acc: 0.6840  (best train acc: 0.6880, best val acc: 0.6911)\n",
      "[Epoch: 29580] train loss: 0.8783, train acc: 0.6372, val loss: 0.8160, val acc: 0.6820  (best train acc: 0.6880, best val acc: 0.6911)\n",
      "[Epoch: 29600] train loss: 0.8544, train acc: 0.6462, val loss: 0.8219, val acc: 0.6712  (best train acc: 0.6880, best val acc: 0.6911)\n",
      "[Epoch: 29620] train loss: 0.8832, train acc: 0.6203, val loss: 0.8136, val acc: 0.6860  (best train acc: 0.6880, best val acc: 0.6911)\n",
      "[Epoch: 29640] train loss: 0.8644, train acc: 0.6520, val loss: 0.8048, val acc: 0.6877  (best train acc: 0.6880, best val acc: 0.6911)\n",
      "[Epoch: 29660] train loss: 0.8193, train acc: 0.6752, val loss: 0.8066, val acc: 0.6887  (best train acc: 0.6880, best val acc: 0.6914)\n",
      "[Epoch: 29680] train loss: 0.8482, train acc: 0.6593, val loss: 0.8139, val acc: 0.6809  (best train acc: 0.6880, best val acc: 0.6914)\n",
      "[Epoch: 29700] train loss: 0.8439, train acc: 0.6574, val loss: 0.8373, val acc: 0.6658  (best train acc: 0.6880, best val acc: 0.6914)\n",
      "[Epoch: 29720] train loss: 0.8854, train acc: 0.6359, val loss: 0.8074, val acc: 0.6917  (best train acc: 0.6880, best val acc: 0.6917)\n",
      "[Epoch: 29740] train loss: 0.8541, train acc: 0.6535, val loss: 0.8086, val acc: 0.6826  (best train acc: 0.6880, best val acc: 0.6924)\n",
      "[Epoch: 29760] train loss: 0.8822, train acc: 0.6357, val loss: 0.8160, val acc: 0.6823  (best train acc: 0.6880, best val acc: 0.6924)\n",
      "[Epoch: 29780] train loss: 0.8451, train acc: 0.6640, val loss: 0.8049, val acc: 0.6850  (best train acc: 0.6880, best val acc: 0.6924)\n",
      "[Epoch: 29800] train loss: 0.8898, train acc: 0.6329, val loss: 0.8278, val acc: 0.6722  (best train acc: 0.6880, best val acc: 0.6924)\n",
      "[Epoch: 29820] train loss: 0.8453, train acc: 0.6658, val loss: 0.8530, val acc: 0.6577  (best train acc: 0.6880, best val acc: 0.6924)\n",
      "[Epoch: 29840] train loss: 0.8556, train acc: 0.6502, val loss: 0.8152, val acc: 0.6793  (best train acc: 0.6880, best val acc: 0.6924)\n",
      "[Epoch: 29860] train loss: 0.8814, train acc: 0.6406, val loss: 0.8128, val acc: 0.6826  (best train acc: 0.6880, best val acc: 0.6924)\n",
      "[Epoch: 29880] train loss: 0.8507, train acc: 0.6519, val loss: 0.8284, val acc: 0.6712  (best train acc: 0.6880, best val acc: 0.6924)\n",
      "[Epoch: 29900] train loss: 0.8349, train acc: 0.6778, val loss: 0.8138, val acc: 0.6867  (best train acc: 0.6880, best val acc: 0.6924)\n",
      "[Epoch: 29920] train loss: 0.8962, train acc: 0.6354, val loss: 0.8535, val acc: 0.6519  (best train acc: 0.6880, best val acc: 0.6924)\n",
      "[Epoch: 29940] train loss: 0.8844, train acc: 0.6375, val loss: 0.8322, val acc: 0.6715  (best train acc: 0.6880, best val acc: 0.6924)\n",
      "[Epoch: 29960] train loss: 0.8794, train acc: 0.6358, val loss: 0.8132, val acc: 0.6813  (best train acc: 0.6880, best val acc: 0.6924)\n",
      "[Epoch: 29980] train loss: 0.8506, train acc: 0.6586, val loss: 0.8078, val acc: 0.6863  (best train acc: 0.6880, best val acc: 0.6924)\n",
      "[Epoch: 30000] train loss: 0.8349, train acc: 0.6674, val loss: 0.8401, val acc: 0.6634  (best train acc: 0.6880, best val acc: 0.6924)\n",
      "[Epoch: 30020] train loss: 0.8451, train acc: 0.6604, val loss: 0.8099, val acc: 0.6863  (best train acc: 0.6880, best val acc: 0.6924)\n",
      "[Epoch: 30040] train loss: 0.8544, train acc: 0.6583, val loss: 0.8100, val acc: 0.6877  (best train acc: 0.6880, best val acc: 0.6924)\n",
      "[Epoch: 30060] train loss: 0.8423, train acc: 0.6556, val loss: 0.8045, val acc: 0.6917  (best train acc: 0.6880, best val acc: 0.6924)\n",
      "[Epoch: 30080] train loss: 0.8717, train acc: 0.6416, val loss: 0.8080, val acc: 0.6874  (best train acc: 0.6880, best val acc: 0.6924)\n",
      "[Epoch: 30100] train loss: 0.8496, train acc: 0.6555, val loss: 0.8119, val acc: 0.6863  (best train acc: 0.6880, best val acc: 0.6927)\n",
      "[Epoch: 30120] train loss: 0.8534, train acc: 0.6546, val loss: 0.8061, val acc: 0.6897  (best train acc: 0.6880, best val acc: 0.6927)\n",
      "[Epoch: 30140] train loss: 0.8811, train acc: 0.6381, val loss: 0.8097, val acc: 0.6877  (best train acc: 0.6880, best val acc: 0.6927)\n",
      "[Epoch: 30160] train loss: 0.9328, train acc: 0.6034, val loss: 0.8761, val acc: 0.6496  (best train acc: 0.6880, best val acc: 0.6927)\n",
      "[Epoch: 30180] train loss: 0.8717, train acc: 0.6471, val loss: 0.8270, val acc: 0.6695  (best train acc: 0.6880, best val acc: 0.6927)\n",
      "[Epoch: 30200] train loss: 0.8701, train acc: 0.6395, val loss: 0.8206, val acc: 0.6813  (best train acc: 0.6880, best val acc: 0.6931)\n",
      "[Epoch: 30220] train loss: 0.8138, train acc: 0.6823, val loss: 0.8104, val acc: 0.6874  (best train acc: 0.6880, best val acc: 0.6931)\n",
      "[Epoch: 30240] train loss: 0.8234, train acc: 0.6714, val loss: 0.8073, val acc: 0.6884  (best train acc: 0.6880, best val acc: 0.6931)\n",
      "[Epoch: 30260] train loss: 0.8544, train acc: 0.6459, val loss: 0.8113, val acc: 0.6874  (best train acc: 0.6880, best val acc: 0.6931)\n",
      "[Epoch: 30280] train loss: 0.8457, train acc: 0.6570, val loss: 0.8386, val acc: 0.6607  (best train acc: 0.6880, best val acc: 0.6931)\n",
      "[Epoch: 30300] train loss: 0.8355, train acc: 0.6696, val loss: 0.8324, val acc: 0.6715  (best train acc: 0.6880, best val acc: 0.6931)\n",
      "[Epoch: 30320] train loss: 0.8384, train acc: 0.6643, val loss: 0.8086, val acc: 0.6907  (best train acc: 0.6880, best val acc: 0.6941)\n",
      "[Epoch: 30340] train loss: 0.8659, train acc: 0.6555, val loss: 0.8185, val acc: 0.6803  (best train acc: 0.6880, best val acc: 0.6941)\n",
      "[Epoch: 30360] train loss: 0.8446, train acc: 0.6653, val loss: 0.8641, val acc: 0.6543  (best train acc: 0.6880, best val acc: 0.6941)\n",
      "[Epoch: 30380] train loss: 0.8518, train acc: 0.6534, val loss: 0.8073, val acc: 0.6857  (best train acc: 0.6880, best val acc: 0.6941)\n",
      "[Epoch: 30400] train loss: 0.8756, train acc: 0.6493, val loss: 0.8177, val acc: 0.6826  (best train acc: 0.6880, best val acc: 0.6941)\n",
      "[Epoch: 30420] train loss: 0.8470, train acc: 0.6592, val loss: 0.8080, val acc: 0.6887  (best train acc: 0.6880, best val acc: 0.6941)\n",
      "[Epoch: 30440] train loss: 0.8442, train acc: 0.6661, val loss: 0.8275, val acc: 0.6752  (best train acc: 0.6880, best val acc: 0.6941)\n",
      "[Epoch: 30460] train loss: 0.8522, train acc: 0.6583, val loss: 0.8086, val acc: 0.6867  (best train acc: 0.6880, best val acc: 0.6941)\n",
      "[Epoch: 30480] train loss: 0.8437, train acc: 0.6726, val loss: 0.8541, val acc: 0.6661  (best train acc: 0.6880, best val acc: 0.6941)\n",
      "[Epoch: 30500] train loss: 0.8542, train acc: 0.6564, val loss: 0.8118, val acc: 0.6843  (best train acc: 0.6880, best val acc: 0.6941)\n",
      "[Epoch: 30520] train loss: 0.8887, train acc: 0.6403, val loss: 0.8257, val acc: 0.6712  (best train acc: 0.6880, best val acc: 0.6941)\n",
      "[Epoch: 30540] train loss: 0.8475, train acc: 0.6653, val loss: 0.8539, val acc: 0.6587  (best train acc: 0.6880, best val acc: 0.6941)\n",
      "[Epoch: 30560] train loss: 0.8831, train acc: 0.6398, val loss: 0.8497, val acc: 0.6499  (best train acc: 0.6880, best val acc: 0.6941)\n",
      "[Epoch: 30580] train loss: 0.8214, train acc: 0.6792, val loss: 0.8367, val acc: 0.6702  (best train acc: 0.6880, best val acc: 0.6941)\n",
      "[Epoch: 30600] train loss: 0.9027, train acc: 0.6273, val loss: 0.8098, val acc: 0.6850  (best train acc: 0.6880, best val acc: 0.6941)\n",
      "[Epoch: 30620] train loss: 0.8432, train acc: 0.6590, val loss: 0.8102, val acc: 0.6860  (best train acc: 0.6880, best val acc: 0.6941)\n",
      "[Epoch: 30640] train loss: 0.8161, train acc: 0.6802, val loss: 0.8121, val acc: 0.6840  (best train acc: 0.6880, best val acc: 0.6941)\n",
      "[Epoch: 30660] train loss: 0.8553, train acc: 0.6546, val loss: 0.8137, val acc: 0.6826  (best train acc: 0.6880, best val acc: 0.6941)\n",
      "[Epoch: 30680] train loss: 0.8758, train acc: 0.6525, val loss: 0.8175, val acc: 0.6769  (best train acc: 0.6880, best val acc: 0.6941)\n",
      "[Epoch: 30700] train loss: 0.8279, train acc: 0.6685, val loss: 0.8038, val acc: 0.6840  (best train acc: 0.6880, best val acc: 0.6941)\n",
      "[Epoch: 30720] train loss: 0.8132, train acc: 0.6752, val loss: 0.8055, val acc: 0.6911  (best train acc: 0.6880, best val acc: 0.6941)\n",
      "[Epoch: 30740] train loss: 0.8762, train acc: 0.6403, val loss: 0.8114, val acc: 0.6786  (best train acc: 0.6880, best val acc: 0.6941)\n",
      "[Epoch: 30760] train loss: 0.9212, train acc: 0.6153, val loss: 0.8301, val acc: 0.6745  (best train acc: 0.6880, best val acc: 0.6941)\n",
      "[Epoch: 30780] train loss: 0.8472, train acc: 0.6516, val loss: 0.8060, val acc: 0.6850  (best train acc: 0.6880, best val acc: 0.6941)\n",
      "[Epoch: 30800] train loss: 0.8536, train acc: 0.6582, val loss: 0.8027, val acc: 0.6874  (best train acc: 0.6880, best val acc: 0.6941)\n",
      "[Epoch: 30820] train loss: 0.8167, train acc: 0.6760, val loss: 0.8195, val acc: 0.6755  (best train acc: 0.6880, best val acc: 0.6941)\n",
      "[Epoch: 30840] train loss: 0.8183, train acc: 0.6733, val loss: 0.8508, val acc: 0.6509  (best train acc: 0.6880, best val acc: 0.6941)\n",
      "[Epoch: 30860] train loss: 0.8628, train acc: 0.6544, val loss: 0.8163, val acc: 0.6847  (best train acc: 0.6880, best val acc: 0.6941)\n",
      "[Epoch: 30880] train loss: 0.8386, train acc: 0.6653, val loss: 0.8121, val acc: 0.6884  (best train acc: 0.6880, best val acc: 0.6941)\n",
      "[Epoch: 30900] train loss: 0.9213, train acc: 0.6254, val loss: 0.8822, val acc: 0.6391  (best train acc: 0.6880, best val acc: 0.6941)\n",
      "[Epoch: 30920] train loss: 0.8652, train acc: 0.6475, val loss: 0.8287, val acc: 0.6735  (best train acc: 0.6880, best val acc: 0.6941)\n",
      "[Epoch: 30940] train loss: 0.8716, train acc: 0.6460, val loss: 0.8163, val acc: 0.6796  (best train acc: 0.6880, best val acc: 0.6941)\n",
      "[Epoch: 30960] train loss: 0.8798, train acc: 0.6551, val loss: 0.8334, val acc: 0.6826  (best train acc: 0.6880, best val acc: 0.6941)\n",
      "[Epoch: 30980] train loss: 0.8600, train acc: 0.6561, val loss: 0.8271, val acc: 0.6796  (best train acc: 0.6880, best val acc: 0.6941)\n",
      "[Epoch: 31000] train loss: 0.8534, train acc: 0.6601, val loss: 0.8224, val acc: 0.6813  (best train acc: 0.6880, best val acc: 0.6941)\n",
      "[Epoch: 31020] train loss: 0.8635, train acc: 0.6555, val loss: 0.8223, val acc: 0.6806  (best train acc: 0.6880, best val acc: 0.6941)\n",
      "[Epoch: 31040] train loss: 0.8118, train acc: 0.6824, val loss: 0.8196, val acc: 0.6796  (best train acc: 0.6880, best val acc: 0.6941)\n",
      "[Epoch: 31060] train loss: 0.8231, train acc: 0.6681, val loss: 0.8028, val acc: 0.6890  (best train acc: 0.6880, best val acc: 0.6941)\n",
      "[Epoch: 31080] train loss: 0.8396, train acc: 0.6580, val loss: 0.8202, val acc: 0.6752  (best train acc: 0.6880, best val acc: 0.6941)\n",
      "[Epoch: 31100] train loss: 0.8198, train acc: 0.6716, val loss: 0.8123, val acc: 0.6823  (best train acc: 0.6880, best val acc: 0.6941)\n",
      "[Epoch: 31120] train loss: 0.8411, train acc: 0.6640, val loss: 0.8080, val acc: 0.6826  (best train acc: 0.6880, best val acc: 0.6941)\n",
      "[Epoch: 31140] train loss: 0.8072, train acc: 0.6822, val loss: 0.8146, val acc: 0.6769  (best train acc: 0.6885, best val acc: 0.6941)\n",
      "[Epoch: 31160] train loss: 0.8617, train acc: 0.6493, val loss: 0.8540, val acc: 0.6395  (best train acc: 0.6885, best val acc: 0.6941)\n",
      "[Epoch: 31180] train loss: 0.8616, train acc: 0.6425, val loss: 0.8101, val acc: 0.6843  (best train acc: 0.6885, best val acc: 0.6941)\n",
      "[Epoch: 31200] train loss: 0.8914, train acc: 0.6315, val loss: 0.8119, val acc: 0.6840  (best train acc: 0.6885, best val acc: 0.6941)\n",
      "[Epoch: 31220] train loss: 0.8701, train acc: 0.6404, val loss: 0.8199, val acc: 0.6772  (best train acc: 0.6885, best val acc: 0.6941)\n",
      "[Epoch: 31240] train loss: 0.8551, train acc: 0.6489, val loss: 0.8054, val acc: 0.6847  (best train acc: 0.6885, best val acc: 0.6941)\n",
      "[Epoch: 31260] train loss: 0.8497, train acc: 0.6530, val loss: 0.8116, val acc: 0.6853  (best train acc: 0.6885, best val acc: 0.6941)\n",
      "[Epoch: 31280] train loss: 0.8555, train acc: 0.6560, val loss: 0.8047, val acc: 0.6884  (best train acc: 0.6885, best val acc: 0.6941)\n",
      "[Epoch: 31300] train loss: 0.8714, train acc: 0.6475, val loss: 0.8239, val acc: 0.6732  (best train acc: 0.6885, best val acc: 0.6941)\n",
      "[Epoch: 31320] train loss: 0.8335, train acc: 0.6643, val loss: 0.8124, val acc: 0.6843  (best train acc: 0.6885, best val acc: 0.6941)\n",
      "[Epoch: 31340] train loss: 0.8392, train acc: 0.6635, val loss: 0.8046, val acc: 0.6863  (best train acc: 0.6885, best val acc: 0.6941)\n",
      "[Epoch: 31360] train loss: 0.8945, train acc: 0.6292, val loss: 0.8021, val acc: 0.6897  (best train acc: 0.6885, best val acc: 0.6941)\n",
      "[Epoch: 31380] train loss: 0.8265, train acc: 0.6720, val loss: 0.8202, val acc: 0.6739  (best train acc: 0.6885, best val acc: 0.6941)\n",
      "[Epoch: 31400] train loss: 0.8946, train acc: 0.6343, val loss: 0.8263, val acc: 0.6712  (best train acc: 0.6885, best val acc: 0.6941)\n",
      "[Epoch: 31420] train loss: 0.8632, train acc: 0.6476, val loss: 0.8143, val acc: 0.6836  (best train acc: 0.6885, best val acc: 0.6941)\n",
      "[Epoch: 31440] train loss: 0.8386, train acc: 0.6660, val loss: 0.8043, val acc: 0.6897  (best train acc: 0.6885, best val acc: 0.6941)\n",
      "[Epoch: 31460] train loss: 0.8656, train acc: 0.6500, val loss: 0.8201, val acc: 0.6766  (best train acc: 0.6885, best val acc: 0.6941)\n",
      "[Epoch: 31480] train loss: 0.8442, train acc: 0.6559, val loss: 0.8076, val acc: 0.6860  (best train acc: 0.6885, best val acc: 0.6941)\n",
      "[Epoch: 31500] train loss: 0.8391, train acc: 0.6543, val loss: 0.8256, val acc: 0.6695  (best train acc: 0.6917, best val acc: 0.6941)\n",
      "[Epoch: 31520] train loss: 0.8396, train acc: 0.6568, val loss: 0.8070, val acc: 0.6840  (best train acc: 0.6917, best val acc: 0.6941)\n",
      "[Epoch: 31540] train loss: 0.8545, train acc: 0.6585, val loss: 0.8033, val acc: 0.6911  (best train acc: 0.6917, best val acc: 0.6941)\n",
      "[Epoch: 31560] train loss: 0.8866, train acc: 0.6408, val loss: 0.8238, val acc: 0.6739  (best train acc: 0.6917, best val acc: 0.6941)\n",
      "[Epoch: 31580] train loss: 0.8190, train acc: 0.6776, val loss: 0.8029, val acc: 0.6914  (best train acc: 0.6917, best val acc: 0.6941)\n",
      "[Epoch: 31600] train loss: 0.8512, train acc: 0.6474, val loss: 0.8022, val acc: 0.6914  (best train acc: 0.6917, best val acc: 0.6941)\n",
      "[Epoch: 31620] train loss: 0.8595, train acc: 0.6476, val loss: 0.8060, val acc: 0.6860  (best train acc: 0.6917, best val acc: 0.6941)\n",
      "[Epoch: 31640] train loss: 0.8463, train acc: 0.6485, val loss: 0.8046, val acc: 0.6884  (best train acc: 0.6917, best val acc: 0.6941)\n",
      "[Epoch: 31660] train loss: 0.8814, train acc: 0.6405, val loss: 0.8209, val acc: 0.6772  (best train acc: 0.6917, best val acc: 0.6941)\n",
      "[Epoch: 31680] train loss: 0.8393, train acc: 0.6655, val loss: 0.8133, val acc: 0.6759  (best train acc: 0.6917, best val acc: 0.6941)\n",
      "[Epoch: 31700] train loss: 0.8563, train acc: 0.6557, val loss: 0.8156, val acc: 0.6796  (best train acc: 0.6917, best val acc: 0.6941)\n",
      "[Epoch: 31720] train loss: 0.8721, train acc: 0.6469, val loss: 0.8039, val acc: 0.6894  (best train acc: 0.6917, best val acc: 0.6941)\n",
      "[Epoch: 31740] train loss: 0.8798, train acc: 0.6374, val loss: 0.8218, val acc: 0.6772  (best train acc: 0.6917, best val acc: 0.6941)\n",
      "[Epoch: 31760] train loss: 0.9014, train acc: 0.6335, val loss: 0.8031, val acc: 0.6867  (best train acc: 0.6917, best val acc: 0.6941)\n",
      "[Epoch: 31780] train loss: 0.8653, train acc: 0.6419, val loss: 0.8248, val acc: 0.6759  (best train acc: 0.6917, best val acc: 0.6941)\n",
      "[Epoch: 31800] train loss: 0.8636, train acc: 0.6551, val loss: 0.8213, val acc: 0.6769  (best train acc: 0.6917, best val acc: 0.6941)\n",
      "[Epoch: 31820] train loss: 0.8243, train acc: 0.6674, val loss: 0.8216, val acc: 0.6728  (best train acc: 0.6917, best val acc: 0.6941)\n",
      "[Epoch: 31840] train loss: 0.8218, train acc: 0.6682, val loss: 0.8164, val acc: 0.6803  (best train acc: 0.6917, best val acc: 0.6941)\n",
      "[Epoch: 31860] train loss: 0.8622, train acc: 0.6541, val loss: 0.8021, val acc: 0.6951  (best train acc: 0.6917, best val acc: 0.6951)\n",
      "[Epoch: 31880] train loss: 0.8422, train acc: 0.6614, val loss: 0.8611, val acc: 0.6351  (best train acc: 0.6917, best val acc: 0.6965)\n",
      "[Epoch: 31900] train loss: 0.8572, train acc: 0.6542, val loss: 0.8372, val acc: 0.6712  (best train acc: 0.6917, best val acc: 0.6965)\n",
      "[Epoch: 31920] train loss: 0.8493, train acc: 0.6515, val loss: 0.8260, val acc: 0.6735  (best train acc: 0.6919, best val acc: 0.6965)\n",
      "[Epoch: 31940] train loss: 0.8521, train acc: 0.6544, val loss: 0.8216, val acc: 0.6796  (best train acc: 0.6919, best val acc: 0.6965)\n",
      "[Epoch: 31960] train loss: 1.0011, train acc: 0.5410, val loss: 0.8524, val acc: 0.6688  (best train acc: 0.6919, best val acc: 0.6965)\n",
      "[Epoch: 31980] train loss: 0.8695, train acc: 0.6471, val loss: 0.8362, val acc: 0.6691  (best train acc: 0.6919, best val acc: 0.6965)\n",
      "[Epoch: 32000] train loss: 0.8849, train acc: 0.6418, val loss: 0.8217, val acc: 0.6799  (best train acc: 0.6919, best val acc: 0.6965)\n",
      "[Epoch: 32020] train loss: 0.8715, train acc: 0.6547, val loss: 0.8279, val acc: 0.6752  (best train acc: 0.6919, best val acc: 0.6965)\n",
      "[Epoch: 32040] train loss: 0.8265, train acc: 0.6794, val loss: 0.8116, val acc: 0.6904  (best train acc: 0.6919, best val acc: 0.6965)\n",
      "[Epoch: 32060] train loss: 0.8850, train acc: 0.6434, val loss: 0.8110, val acc: 0.6874  (best train acc: 0.6919, best val acc: 0.6965)\n",
      "[Epoch: 32080] train loss: 0.8806, train acc: 0.6455, val loss: 0.8162, val acc: 0.6867  (best train acc: 0.6919, best val acc: 0.6965)\n",
      "[Epoch: 32100] train loss: 0.8628, train acc: 0.6499, val loss: 0.8137, val acc: 0.6860  (best train acc: 0.6919, best val acc: 0.6965)\n",
      "[Epoch: 32120] train loss: 0.8515, train acc: 0.6596, val loss: 0.8082, val acc: 0.6877  (best train acc: 0.6919, best val acc: 0.6965)\n",
      "[Epoch: 32140] train loss: 0.8625, train acc: 0.6564, val loss: 0.8159, val acc: 0.6813  (best train acc: 0.6919, best val acc: 0.6965)\n",
      "[Epoch: 32160] train loss: 0.8372, train acc: 0.6630, val loss: 0.8300, val acc: 0.6648  (best train acc: 0.6919, best val acc: 0.6965)\n",
      "[Epoch: 32180] train loss: 0.9181, train acc: 0.6219, val loss: 0.8045, val acc: 0.6914  (best train acc: 0.6919, best val acc: 0.6965)\n",
      "[Epoch: 32200] train loss: 0.8685, train acc: 0.6495, val loss: 0.8048, val acc: 0.6887  (best train acc: 0.6919, best val acc: 0.6965)\n",
      "[Epoch: 32220] train loss: 0.8709, train acc: 0.6436, val loss: 0.8403, val acc: 0.6661  (best train acc: 0.6919, best val acc: 0.6965)\n",
      "[Epoch: 32240] train loss: 0.8271, train acc: 0.6690, val loss: 0.8034, val acc: 0.6863  (best train acc: 0.6919, best val acc: 0.6965)\n",
      "[Epoch: 32260] train loss: 0.8641, train acc: 0.6595, val loss: 0.8163, val acc: 0.6752  (best train acc: 0.6919, best val acc: 0.6965)\n",
      "[Epoch: 32280] train loss: 0.8541, train acc: 0.6608, val loss: 0.8009, val acc: 0.6914  (best train acc: 0.6919, best val acc: 0.6965)\n",
      "[Epoch: 32300] train loss: 0.8274, train acc: 0.6638, val loss: 0.8195, val acc: 0.6749  (best train acc: 0.6919, best val acc: 0.6965)\n",
      "[Epoch: 32320] train loss: 0.8279, train acc: 0.6693, val loss: 0.8054, val acc: 0.6887  (best train acc: 0.6919, best val acc: 0.6965)\n",
      "[Epoch: 32340] train loss: 0.8391, train acc: 0.6729, val loss: 0.8002, val acc: 0.6921  (best train acc: 0.6919, best val acc: 0.6965)\n",
      "[Epoch: 32360] train loss: 0.8777, train acc: 0.6485, val loss: 0.8624, val acc: 0.6536  (best train acc: 0.6919, best val acc: 0.6965)\n",
      "[Epoch: 32380] train loss: 0.8520, train acc: 0.6572, val loss: 0.8358, val acc: 0.6668  (best train acc: 0.6919, best val acc: 0.6965)\n",
      "[Epoch: 32400] train loss: 0.8208, train acc: 0.6806, val loss: 0.8421, val acc: 0.6678  (best train acc: 0.6919, best val acc: 0.6965)\n",
      "[Epoch: 32420] train loss: 0.8645, train acc: 0.6493, val loss: 0.8119, val acc: 0.6840  (best train acc: 0.6919, best val acc: 0.6965)\n",
      "[Epoch: 32440] train loss: 0.8612, train acc: 0.6593, val loss: 0.8660, val acc: 0.6503  (best train acc: 0.6919, best val acc: 0.6965)\n",
      "[Epoch: 32460] train loss: 0.8656, train acc: 0.6489, val loss: 0.8097, val acc: 0.6847  (best train acc: 0.6919, best val acc: 0.6965)\n",
      "[Epoch: 32480] train loss: 0.8516, train acc: 0.6598, val loss: 0.8019, val acc: 0.6938  (best train acc: 0.6919, best val acc: 0.6965)\n",
      "[Epoch: 32500] train loss: 0.8531, train acc: 0.6512, val loss: 0.8131, val acc: 0.6826  (best train acc: 0.6919, best val acc: 0.6965)\n",
      "[Epoch: 32520] train loss: 0.9678, train acc: 0.5909, val loss: 0.8829, val acc: 0.6432  (best train acc: 0.6919, best val acc: 0.6965)\n",
      "[Epoch: 32540] train loss: 0.8899, train acc: 0.6345, val loss: 0.8080, val acc: 0.6894  (best train acc: 0.6919, best val acc: 0.6965)\n",
      "[Epoch: 32560] train loss: 0.8333, train acc: 0.6739, val loss: 0.8185, val acc: 0.6843  (best train acc: 0.6919, best val acc: 0.6965)\n",
      "[Epoch: 32580] train loss: 0.8718, train acc: 0.6475, val loss: 0.8061, val acc: 0.6870  (best train acc: 0.6919, best val acc: 0.6965)\n",
      "[Epoch: 32600] train loss: 0.8740, train acc: 0.6549, val loss: 0.8079, val acc: 0.6863  (best train acc: 0.6919, best val acc: 0.6965)\n",
      "[Epoch: 32620] train loss: 0.8743, train acc: 0.6350, val loss: 0.8051, val acc: 0.6884  (best train acc: 0.6919, best val acc: 0.6965)\n",
      "[Epoch: 32640] train loss: 0.8778, train acc: 0.6460, val loss: 0.8025, val acc: 0.6911  (best train acc: 0.6919, best val acc: 0.6965)\n",
      "[Epoch: 32660] train loss: 0.8778, train acc: 0.6388, val loss: 0.8085, val acc: 0.6809  (best train acc: 0.6919, best val acc: 0.6965)\n",
      "[Epoch: 32680] train loss: 0.9066, train acc: 0.6277, val loss: 0.8237, val acc: 0.6762  (best train acc: 0.6919, best val acc: 0.6965)\n",
      "[Epoch: 32700] train loss: 0.8453, train acc: 0.6615, val loss: 0.8097, val acc: 0.6853  (best train acc: 0.6919, best val acc: 0.6965)\n",
      "[Epoch: 32720] train loss: 0.7977, train acc: 0.6871, val loss: 0.8197, val acc: 0.6762  (best train acc: 0.6919, best val acc: 0.6965)\n",
      "[Epoch: 32740] train loss: 0.8425, train acc: 0.6716, val loss: 0.8937, val acc: 0.6364  (best train acc: 0.6919, best val acc: 0.6965)\n",
      "[Epoch: 32760] train loss: 0.8322, train acc: 0.6669, val loss: 0.8388, val acc: 0.6614  (best train acc: 0.6919, best val acc: 0.6965)\n",
      "[Epoch: 32780] train loss: 0.8472, train acc: 0.6626, val loss: 0.8091, val acc: 0.6813  (best train acc: 0.6919, best val acc: 0.6965)\n",
      "[Epoch: 32800] train loss: 0.8377, train acc: 0.6624, val loss: 0.8268, val acc: 0.6749  (best train acc: 0.6919, best val acc: 0.6965)\n",
      "[Epoch: 32820] train loss: 0.9527, train acc: 0.5964, val loss: 0.8635, val acc: 0.6563  (best train acc: 0.6919, best val acc: 0.6965)\n",
      "[Epoch: 32840] train loss: 0.8681, train acc: 0.6464, val loss: 0.8043, val acc: 0.6863  (best train acc: 0.6919, best val acc: 0.6965)\n",
      "[Epoch: 32860] train loss: 0.9209, train acc: 0.6133, val loss: 0.8067, val acc: 0.6877  (best train acc: 0.6919, best val acc: 0.6965)\n",
      "[Epoch: 32880] train loss: 0.8676, train acc: 0.6375, val loss: 0.8121, val acc: 0.6826  (best train acc: 0.6919, best val acc: 0.6965)\n",
      "[Epoch: 32900] train loss: 0.8753, train acc: 0.6257, val loss: 0.8110, val acc: 0.6847  (best train acc: 0.6919, best val acc: 0.6965)\n",
      "[Epoch: 32920] train loss: 0.8508, train acc: 0.6580, val loss: 0.8062, val acc: 0.6826  (best train acc: 0.6919, best val acc: 0.6965)\n",
      "[Epoch: 32940] train loss: 0.8356, train acc: 0.6627, val loss: 0.8066, val acc: 0.6887  (best train acc: 0.6919, best val acc: 0.6965)\n",
      "[Epoch: 32960] train loss: 0.8170, train acc: 0.6762, val loss: 0.7999, val acc: 0.6941  (best train acc: 0.6919, best val acc: 0.6965)\n",
      "[Epoch: 32980] train loss: 0.9296, train acc: 0.6123, val loss: 0.8220, val acc: 0.6752  (best train acc: 0.6919, best val acc: 0.6965)\n",
      "[Epoch: 33000] train loss: 0.8323, train acc: 0.6605, val loss: 0.8116, val acc: 0.6809  (best train acc: 0.6919, best val acc: 0.6965)\n",
      "[Epoch: 33020] train loss: 0.8567, train acc: 0.6560, val loss: 0.8043, val acc: 0.6850  (best train acc: 0.6919, best val acc: 0.6968)\n",
      "[Epoch: 33040] train loss: 0.8667, train acc: 0.6466, val loss: 0.8050, val acc: 0.6863  (best train acc: 0.6935, best val acc: 0.6968)\n",
      "[Epoch: 33060] train loss: 0.8511, train acc: 0.6558, val loss: 0.8014, val acc: 0.6924  (best train acc: 0.6935, best val acc: 0.6968)\n",
      "[Epoch: 33080] train loss: 0.8509, train acc: 0.6486, val loss: 0.8051, val acc: 0.6850  (best train acc: 0.6935, best val acc: 0.6968)\n",
      "[Epoch: 33100] train loss: 0.8326, train acc: 0.6712, val loss: 0.8534, val acc: 0.6438  (best train acc: 0.6935, best val acc: 0.6968)\n",
      "[Epoch: 33120] train loss: 0.8661, train acc: 0.6512, val loss: 0.8036, val acc: 0.6914  (best train acc: 0.6935, best val acc: 0.6968)\n",
      "[Epoch: 33140] train loss: 0.8485, train acc: 0.6542, val loss: 0.8081, val acc: 0.6877  (best train acc: 0.6935, best val acc: 0.6968)\n",
      "[Epoch: 33160] train loss: 0.8684, train acc: 0.6439, val loss: 0.8034, val acc: 0.6941  (best train acc: 0.6935, best val acc: 0.6968)\n",
      "[Epoch: 33180] train loss: 0.8799, train acc: 0.6388, val loss: 0.8054, val acc: 0.6890  (best train acc: 0.6935, best val acc: 0.6968)\n",
      "[Epoch: 33200] train loss: 0.8260, train acc: 0.6674, val loss: 0.8608, val acc: 0.6368  (best train acc: 0.6935, best val acc: 0.6968)\n",
      "[Epoch: 33220] train loss: 0.8646, train acc: 0.6469, val loss: 0.8005, val acc: 0.6927  (best train acc: 0.6935, best val acc: 0.6968)\n",
      "[Epoch: 33240] train loss: 0.9226, train acc: 0.6173, val loss: 0.8224, val acc: 0.6755  (best train acc: 0.6935, best val acc: 0.6968)\n",
      "[Epoch: 33260] train loss: 0.8275, train acc: 0.6693, val loss: 0.8003, val acc: 0.6901  (best train acc: 0.6935, best val acc: 0.6968)\n",
      "[Epoch: 33280] train loss: 0.8306, train acc: 0.6666, val loss: 0.8016, val acc: 0.6951  (best train acc: 0.6935, best val acc: 0.6968)\n",
      "[Epoch: 33300] train loss: 0.8379, train acc: 0.6584, val loss: 0.8050, val acc: 0.6921  (best train acc: 0.6935, best val acc: 0.6968)\n",
      "[Epoch: 33320] train loss: 0.8899, train acc: 0.6352, val loss: 0.8040, val acc: 0.6880  (best train acc: 0.6935, best val acc: 0.6968)\n",
      "[Epoch: 33340] train loss: 0.8631, train acc: 0.6517, val loss: 0.8146, val acc: 0.6789  (best train acc: 0.6935, best val acc: 0.6968)\n",
      "[Epoch: 33360] train loss: 0.8589, train acc: 0.6546, val loss: 0.8184, val acc: 0.6752  (best train acc: 0.6935, best val acc: 0.6968)\n",
      "[Epoch: 33380] train loss: 0.8752, train acc: 0.6375, val loss: 0.8064, val acc: 0.6874  (best train acc: 0.6935, best val acc: 0.6968)\n",
      "[Epoch: 33400] train loss: 0.8885, train acc: 0.6339, val loss: 0.8240, val acc: 0.6759  (best train acc: 0.6935, best val acc: 0.6968)\n",
      "[Epoch: 33420] train loss: 0.9117, train acc: 0.6246, val loss: 0.8409, val acc: 0.6759  (best train acc: 0.6935, best val acc: 0.6968)\n",
      "[Epoch: 33440] train loss: 0.8455, train acc: 0.6635, val loss: 0.8238, val acc: 0.6850  (best train acc: 0.6935, best val acc: 0.6968)\n",
      "[Epoch: 33460] train loss: 0.8768, train acc: 0.6517, val loss: 0.8195, val acc: 0.6907  (best train acc: 0.6935, best val acc: 0.6968)\n",
      "[Epoch: 33480] train loss: 0.8559, train acc: 0.6550, val loss: 0.8250, val acc: 0.6786  (best train acc: 0.6935, best val acc: 0.6968)\n",
      "[Epoch: 33500] train loss: 0.8586, train acc: 0.6515, val loss: 0.8047, val acc: 0.6931  (best train acc: 0.6935, best val acc: 0.6968)\n",
      "[Epoch: 33520] train loss: 0.8567, train acc: 0.6418, val loss: 0.8131, val acc: 0.6803  (best train acc: 0.6935, best val acc: 0.6968)\n",
      "[Epoch: 33540] train loss: 0.8830, train acc: 0.6426, val loss: 0.8155, val acc: 0.6853  (best train acc: 0.6935, best val acc: 0.6968)\n",
      "[Epoch: 33560] train loss: 0.8399, train acc: 0.6601, val loss: 0.7992, val acc: 0.6914  (best train acc: 0.6935, best val acc: 0.6968)\n",
      "[Epoch: 33580] train loss: 0.8640, train acc: 0.6524, val loss: 0.7994, val acc: 0.6907  (best train acc: 0.6935, best val acc: 0.6968)\n",
      "[Epoch: 33600] train loss: 0.8785, train acc: 0.6437, val loss: 0.8063, val acc: 0.6917  (best train acc: 0.6935, best val acc: 0.6968)\n",
      "[Epoch: 33620] train loss: 0.8348, train acc: 0.6669, val loss: 0.8135, val acc: 0.6853  (best train acc: 0.6935, best val acc: 0.6968)\n",
      "[Epoch: 33640] train loss: 0.8434, train acc: 0.6632, val loss: 0.8155, val acc: 0.6823  (best train acc: 0.6935, best val acc: 0.6968)\n",
      "[Epoch: 33660] train loss: 0.8418, train acc: 0.6628, val loss: 0.8346, val acc: 0.6671  (best train acc: 0.6935, best val acc: 0.6968)\n",
      "[Epoch: 33680] train loss: 0.8504, train acc: 0.6613, val loss: 0.8260, val acc: 0.6722  (best train acc: 0.6935, best val acc: 0.6968)\n",
      "[Epoch: 33700] train loss: 0.8168, train acc: 0.6747, val loss: 0.8001, val acc: 0.6904  (best train acc: 0.6935, best val acc: 0.6968)\n",
      "[Epoch: 33720] train loss: 0.8066, train acc: 0.6788, val loss: 0.8004, val acc: 0.6941  (best train acc: 0.6935, best val acc: 0.6968)\n",
      "[Epoch: 33740] train loss: 0.8601, train acc: 0.6549, val loss: 0.7988, val acc: 0.6941  (best train acc: 0.6935, best val acc: 0.6971)\n",
      "[Epoch: 33760] train loss: 0.8643, train acc: 0.6504, val loss: 0.8114, val acc: 0.6833  (best train acc: 0.6935, best val acc: 0.6971)\n",
      "[Epoch: 33780] train loss: 0.8153, train acc: 0.6776, val loss: 0.7994, val acc: 0.6978  (best train acc: 0.6935, best val acc: 0.6978)\n",
      "[Epoch: 33800] train loss: 0.8609, train acc: 0.6542, val loss: 0.8011, val acc: 0.6880  (best train acc: 0.6935, best val acc: 0.6978)\n",
      "[Epoch: 33820] train loss: 0.8576, train acc: 0.6478, val loss: 0.8032, val acc: 0.6914  (best train acc: 0.6935, best val acc: 0.6978)\n",
      "[Epoch: 33840] train loss: 0.8406, train acc: 0.6656, val loss: 0.8037, val acc: 0.6914  (best train acc: 0.6935, best val acc: 0.6978)\n",
      "[Epoch: 33860] train loss: 0.8278, train acc: 0.6658, val loss: 0.8158, val acc: 0.6776  (best train acc: 0.6935, best val acc: 0.6978)\n",
      "[Epoch: 33880] train loss: 0.8674, train acc: 0.6464, val loss: 0.8118, val acc: 0.6813  (best train acc: 0.6935, best val acc: 0.6978)\n",
      "[Epoch: 33900] train loss: 0.8738, train acc: 0.6451, val loss: 0.8270, val acc: 0.6766  (best train acc: 0.6935, best val acc: 0.6978)\n",
      "[Epoch: 33920] train loss: 0.8479, train acc: 0.6622, val loss: 0.8124, val acc: 0.6803  (best train acc: 0.6935, best val acc: 0.6978)\n",
      "[Epoch: 33940] train loss: 0.8402, train acc: 0.6607, val loss: 0.8071, val acc: 0.6860  (best train acc: 0.6935, best val acc: 0.6978)\n",
      "[Epoch: 33960] train loss: 0.9053, train acc: 0.6272, val loss: 0.8209, val acc: 0.6779  (best train acc: 0.6935, best val acc: 0.6978)\n",
      "[Epoch: 33980] train loss: 0.8754, train acc: 0.6392, val loss: 0.8057, val acc: 0.6901  (best train acc: 0.6935, best val acc: 0.6978)\n",
      "[Epoch: 34000] train loss: 0.8823, train acc: 0.6367, val loss: 0.8016, val acc: 0.6954  (best train acc: 0.6935, best val acc: 0.6978)\n",
      "[Epoch: 34020] train loss: 0.9272, train acc: 0.5928, val loss: 0.8094, val acc: 0.6830  (best train acc: 0.6935, best val acc: 0.6978)\n",
      "[Epoch: 34040] train loss: 0.8607, train acc: 0.6650, val loss: 0.8091, val acc: 0.6897  (best train acc: 0.6935, best val acc: 0.6978)\n",
      "[Epoch: 34060] train loss: 0.8805, train acc: 0.6470, val loss: 0.8706, val acc: 0.6550  (best train acc: 0.6935, best val acc: 0.6978)\n",
      "[Epoch: 34080] train loss: 0.8264, train acc: 0.6739, val loss: 0.8215, val acc: 0.6813  (best train acc: 0.6935, best val acc: 0.6978)\n",
      "[Epoch: 34100] train loss: 0.8639, train acc: 0.6431, val loss: 0.8287, val acc: 0.6732  (best train acc: 0.6935, best val acc: 0.6978)\n",
      "[Epoch: 34120] train loss: 0.8646, train acc: 0.6462, val loss: 0.8195, val acc: 0.6782  (best train acc: 0.6935, best val acc: 0.6978)\n",
      "[Epoch: 34140] train loss: 0.8973, train acc: 0.6324, val loss: 0.8083, val acc: 0.6850  (best train acc: 0.6935, best val acc: 0.6992)\n",
      "[Epoch: 34160] train loss: 0.8817, train acc: 0.6370, val loss: 0.8130, val acc: 0.6847  (best train acc: 0.6935, best val acc: 0.6992)\n",
      "[Epoch: 34180] train loss: 0.8214, train acc: 0.6705, val loss: 0.8333, val acc: 0.6658  (best train acc: 0.6935, best val acc: 0.6992)\n",
      "[Epoch: 34200] train loss: 0.8679, train acc: 0.6449, val loss: 0.8041, val acc: 0.6894  (best train acc: 0.6935, best val acc: 0.6992)\n",
      "[Epoch: 34220] train loss: 0.8301, train acc: 0.6636, val loss: 0.8035, val acc: 0.6884  (best train acc: 0.6935, best val acc: 0.6992)\n",
      "[Epoch: 34240] train loss: 0.8101, train acc: 0.6752, val loss: 0.8010, val acc: 0.6965  (best train acc: 0.6935, best val acc: 0.6995)\n",
      "[Epoch: 34260] train loss: 0.8149, train acc: 0.6796, val loss: 0.8235, val acc: 0.6705  (best train acc: 0.6935, best val acc: 0.6995)\n",
      "[Epoch: 34280] train loss: 0.8786, train acc: 0.6392, val loss: 0.8176, val acc: 0.6752  (best train acc: 0.6935, best val acc: 0.6995)\n",
      "[Epoch: 34300] train loss: 0.8862, train acc: 0.6257, val loss: 0.8091, val acc: 0.6853  (best train acc: 0.6935, best val acc: 0.6995)\n",
      "[Epoch: 34320] train loss: 0.8206, train acc: 0.6722, val loss: 0.8164, val acc: 0.6772  (best train acc: 0.6935, best val acc: 0.6995)\n",
      "[Epoch: 34340] train loss: 0.8258, train acc: 0.6726, val loss: 0.8395, val acc: 0.6594  (best train acc: 0.6935, best val acc: 0.6995)\n",
      "[Epoch: 34360] train loss: 0.8372, train acc: 0.6605, val loss: 0.8076, val acc: 0.6884  (best train acc: 0.6935, best val acc: 0.6995)\n",
      "[Epoch: 34380] train loss: 0.8364, train acc: 0.6656, val loss: 0.8494, val acc: 0.6425  (best train acc: 0.6935, best val acc: 0.6995)\n",
      "[Epoch: 34400] train loss: 0.8705, train acc: 0.6503, val loss: 0.8150, val acc: 0.6786  (best train acc: 0.6935, best val acc: 0.6995)\n",
      "[Epoch: 34420] train loss: 0.8614, train acc: 0.6549, val loss: 0.8205, val acc: 0.6745  (best train acc: 0.6935, best val acc: 0.6995)\n",
      "[Epoch: 34440] train loss: 0.8570, train acc: 0.6592, val loss: 0.8337, val acc: 0.6732  (best train acc: 0.6935, best val acc: 0.6995)\n",
      "[Epoch: 34460] train loss: 0.8501, train acc: 0.6594, val loss: 0.8110, val acc: 0.6897  (best train acc: 0.6935, best val acc: 0.6995)\n",
      "[Epoch: 34480] train loss: 0.8263, train acc: 0.6773, val loss: 0.8118, val acc: 0.6917  (best train acc: 0.6935, best val acc: 0.6995)\n",
      "[Epoch: 34500] train loss: 0.8542, train acc: 0.6579, val loss: 0.8267, val acc: 0.6718  (best train acc: 0.6935, best val acc: 0.6995)\n",
      "[Epoch: 34520] train loss: 0.8263, train acc: 0.6700, val loss: 0.8018, val acc: 0.6901  (best train acc: 0.6935, best val acc: 0.6995)\n",
      "[Epoch: 34540] train loss: 0.9441, train acc: 0.6045, val loss: 0.8223, val acc: 0.6752  (best train acc: 0.6935, best val acc: 0.6995)\n",
      "[Epoch: 34560] train loss: 0.8652, train acc: 0.6413, val loss: 0.8002, val acc: 0.6911  (best train acc: 0.6935, best val acc: 0.6995)\n",
      "[Epoch: 34580] train loss: 0.8623, train acc: 0.6395, val loss: 0.8107, val acc: 0.6901  (best train acc: 0.6935, best val acc: 0.6995)\n",
      "[Epoch: 34600] train loss: 0.8553, train acc: 0.6531, val loss: 0.8006, val acc: 0.6914  (best train acc: 0.6935, best val acc: 0.6995)\n",
      "[Epoch: 34620] train loss: 0.8280, train acc: 0.6711, val loss: 0.8128, val acc: 0.6830  (best train acc: 0.6946, best val acc: 0.6995)\n",
      "[Epoch: 34640] train loss: 0.8405, train acc: 0.6615, val loss: 0.8411, val acc: 0.6644  (best train acc: 0.6946, best val acc: 0.6995)\n",
      "[Epoch: 34660] train loss: 0.8435, train acc: 0.6598, val loss: 0.8461, val acc: 0.6533  (best train acc: 0.6946, best val acc: 0.6995)\n",
      "[Epoch: 34680] train loss: 0.8686, train acc: 0.6431, val loss: 0.7995, val acc: 0.6931  (best train acc: 0.6946, best val acc: 0.6995)\n",
      "[Epoch: 34700] train loss: 0.8072, train acc: 0.6816, val loss: 0.8031, val acc: 0.6911  (best train acc: 0.6946, best val acc: 0.6995)\n",
      "[Epoch: 34720] train loss: 0.8919, train acc: 0.6408, val loss: 0.7983, val acc: 0.6941  (best train acc: 0.6946, best val acc: 0.6995)\n",
      "[Epoch: 34740] train loss: 0.8463, train acc: 0.6619, val loss: 0.8011, val acc: 0.6921  (best train acc: 0.6946, best val acc: 0.6995)\n",
      "[Epoch: 34760] train loss: 0.8464, train acc: 0.6575, val loss: 0.8009, val acc: 0.6948  (best train acc: 0.6946, best val acc: 0.6995)\n",
      "[Epoch: 34780] train loss: 0.8676, train acc: 0.6505, val loss: 0.7983, val acc: 0.6954  (best train acc: 0.6946, best val acc: 0.6995)\n",
      "[Epoch: 34800] train loss: 0.8705, train acc: 0.6418, val loss: 0.7976, val acc: 0.6975  (best train acc: 0.6946, best val acc: 0.6995)\n",
      "[Epoch: 34820] train loss: 0.8174, train acc: 0.6720, val loss: 0.8010, val acc: 0.6904  (best train acc: 0.6946, best val acc: 0.6995)\n",
      "[Epoch: 34840] train loss: 0.8108, train acc: 0.6815, val loss: 0.8235, val acc: 0.6742  (best train acc: 0.6946, best val acc: 0.6995)\n",
      "[Epoch: 34860] train loss: 0.8666, train acc: 0.6464, val loss: 0.8318, val acc: 0.6745  (best train acc: 0.6946, best val acc: 0.6995)\n",
      "[Epoch: 34880] train loss: 0.8396, train acc: 0.6619, val loss: 0.8049, val acc: 0.6874  (best train acc: 0.6946, best val acc: 0.6995)\n",
      "[Epoch: 34900] train loss: 0.8505, train acc: 0.6617, val loss: 0.7968, val acc: 0.6914  (best train acc: 0.6946, best val acc: 0.6995)\n",
      "[Epoch: 34920] train loss: 0.8385, train acc: 0.6663, val loss: 0.7980, val acc: 0.6968  (best train acc: 0.6946, best val acc: 0.6995)\n",
      "[Epoch: 34940] train loss: 0.8756, train acc: 0.6437, val loss: 0.7992, val acc: 0.6897  (best train acc: 0.6946, best val acc: 0.6995)\n",
      "[Epoch: 34960] train loss: 0.8697, train acc: 0.6483, val loss: 0.8257, val acc: 0.6749  (best train acc: 0.6946, best val acc: 0.6995)\n",
      "[Epoch: 34980] train loss: 0.8459, train acc: 0.6537, val loss: 0.7994, val acc: 0.6948  (best train acc: 0.6946, best val acc: 0.6995)\n",
      "[Epoch: 35000] train loss: 0.8559, train acc: 0.6545, val loss: 0.8312, val acc: 0.6739  (best train acc: 0.6946, best val acc: 0.6995)\n",
      "[Epoch: 35020] train loss: 0.8472, train acc: 0.6650, val loss: 0.8115, val acc: 0.6877  (best train acc: 0.6946, best val acc: 0.6995)\n",
      "[Epoch: 35040] train loss: 0.8849, train acc: 0.6390, val loss: 0.8319, val acc: 0.6789  (best train acc: 0.6946, best val acc: 0.6995)\n",
      "[Epoch: 35060] train loss: 0.8723, train acc: 0.6460, val loss: 0.8043, val acc: 0.6948  (best train acc: 0.6946, best val acc: 0.6995)\n",
      "[Epoch: 35080] train loss: 0.8754, train acc: 0.6457, val loss: 0.8004, val acc: 0.6904  (best train acc: 0.6946, best val acc: 0.6995)\n",
      "[Epoch: 35100] train loss: 0.8352, train acc: 0.6713, val loss: 0.8187, val acc: 0.6766  (best train acc: 0.6946, best val acc: 0.6995)\n",
      "[Epoch: 35120] train loss: 0.8745, train acc: 0.6404, val loss: 0.8194, val acc: 0.6799  (best train acc: 0.6946, best val acc: 0.6995)\n",
      "[Epoch: 35140] train loss: 0.8274, train acc: 0.6661, val loss: 0.8054, val acc: 0.6847  (best train acc: 0.6946, best val acc: 0.6995)\n",
      "[Epoch: 35160] train loss: 0.8546, train acc: 0.6578, val loss: 0.7981, val acc: 0.6948  (best train acc: 0.6946, best val acc: 0.6995)\n",
      "[Epoch: 35180] train loss: 0.8087, train acc: 0.6778, val loss: 0.7998, val acc: 0.6954  (best train acc: 0.6946, best val acc: 0.6995)\n",
      "[Epoch: 35200] train loss: 0.8039, train acc: 0.6862, val loss: 0.8552, val acc: 0.6401  (best train acc: 0.6946, best val acc: 0.6995)\n",
      "[Epoch: 35220] train loss: 0.8876, train acc: 0.6288, val loss: 0.8150, val acc: 0.6833  (best train acc: 0.6946, best val acc: 0.6995)\n",
      "[Epoch: 35240] train loss: 0.8906, train acc: 0.6293, val loss: 0.8414, val acc: 0.6671  (best train acc: 0.6946, best val acc: 0.6995)\n",
      "[Epoch: 35260] train loss: 0.8887, train acc: 0.6337, val loss: 0.8115, val acc: 0.6863  (best train acc: 0.6946, best val acc: 0.6995)\n",
      "[Epoch: 35280] train loss: 0.8098, train acc: 0.6836, val loss: 0.8048, val acc: 0.6894  (best train acc: 0.6946, best val acc: 0.6995)\n",
      "[Epoch: 35300] train loss: 0.8704, train acc: 0.6405, val loss: 0.7978, val acc: 0.6958  (best train acc: 0.6946, best val acc: 0.6995)\n",
      "[Epoch: 35320] train loss: 0.8354, train acc: 0.6686, val loss: 0.8080, val acc: 0.6867  (best train acc: 0.6946, best val acc: 0.6995)\n",
      "[Epoch: 35340] train loss: 0.9071, train acc: 0.6361, val loss: 0.8026, val acc: 0.6901  (best train acc: 0.6946, best val acc: 0.6995)\n",
      "[Epoch: 35360] train loss: 0.8535, train acc: 0.6664, val loss: 0.7965, val acc: 0.6941  (best train acc: 0.6946, best val acc: 0.6995)\n",
      "[Epoch: 35380] train loss: 0.8865, train acc: 0.6304, val loss: 0.7989, val acc: 0.6948  (best train acc: 0.6946, best val acc: 0.6995)\n",
      "[Epoch: 35400] train loss: 0.8972, train acc: 0.6253, val loss: 0.7987, val acc: 0.6894  (best train acc: 0.6946, best val acc: 0.6995)\n",
      "[Epoch: 35420] train loss: 0.8372, train acc: 0.6541, val loss: 0.8197, val acc: 0.6769  (best train acc: 0.6946, best val acc: 0.6995)\n",
      "[Epoch: 35440] train loss: 0.8434, train acc: 0.6669, val loss: 0.7993, val acc: 0.6924  (best train acc: 0.6946, best val acc: 0.7005)\n",
      "[Epoch: 35460] train loss: 0.8572, train acc: 0.6504, val loss: 0.7999, val acc: 0.6894  (best train acc: 0.6946, best val acc: 0.7005)\n",
      "[Epoch: 35480] train loss: 0.8253, train acc: 0.6669, val loss: 0.7983, val acc: 0.6924  (best train acc: 0.6946, best val acc: 0.7005)\n",
      "[Epoch: 35500] train loss: 0.8782, train acc: 0.6356, val loss: 0.8151, val acc: 0.6793  (best train acc: 0.6946, best val acc: 0.7005)\n",
      "[Epoch: 35520] train loss: 0.8580, train acc: 0.6573, val loss: 0.8128, val acc: 0.6793  (best train acc: 0.6946, best val acc: 0.7005)\n",
      "[Epoch: 35540] train loss: 0.8399, train acc: 0.6580, val loss: 0.8078, val acc: 0.6857  (best train acc: 0.6946, best val acc: 0.7005)\n",
      "[Epoch: 35560] train loss: 0.8361, train acc: 0.6682, val loss: 0.8123, val acc: 0.6840  (best train acc: 0.6946, best val acc: 0.7005)\n",
      "[Epoch: 35580] train loss: 0.8401, train acc: 0.6622, val loss: 0.8034, val acc: 0.6921  (best train acc: 0.6946, best val acc: 0.7005)\n",
      "[Epoch: 35600] train loss: 0.8277, train acc: 0.6705, val loss: 0.8086, val acc: 0.6904  (best train acc: 0.6946, best val acc: 0.7005)\n",
      "[Epoch: 35620] train loss: 0.8519, train acc: 0.6584, val loss: 0.8040, val acc: 0.6880  (best train acc: 0.6946, best val acc: 0.7005)\n",
      "[Epoch: 35640] train loss: 0.8367, train acc: 0.6656, val loss: 0.7988, val acc: 0.6938  (best train acc: 0.6946, best val acc: 0.7005)\n",
      "[Epoch: 35660] train loss: 0.8649, train acc: 0.6507, val loss: 0.8040, val acc: 0.6877  (best train acc: 0.6946, best val acc: 0.7005)\n",
      "[Epoch: 35680] train loss: 0.8065, train acc: 0.6830, val loss: 0.7962, val acc: 0.6931  (best train acc: 0.6946, best val acc: 0.7005)\n",
      "[Epoch: 35700] train loss: 0.8716, train acc: 0.6456, val loss: 0.8003, val acc: 0.6874  (best train acc: 0.6946, best val acc: 0.7005)\n",
      "[Epoch: 35720] train loss: 0.8120, train acc: 0.6720, val loss: 0.7984, val acc: 0.6958  (best train acc: 0.6946, best val acc: 0.7005)\n",
      "[Epoch: 35740] train loss: 0.8695, train acc: 0.6517, val loss: 0.7976, val acc: 0.6961  (best train acc: 0.6946, best val acc: 0.7005)\n",
      "[Epoch: 35760] train loss: 0.8602, train acc: 0.6538, val loss: 0.7989, val acc: 0.6938  (best train acc: 0.6946, best val acc: 0.7005)\n",
      "[Epoch: 35780] train loss: 0.8262, train acc: 0.6718, val loss: 0.8135, val acc: 0.6782  (best train acc: 0.6946, best val acc: 0.7005)\n",
      "[Epoch: 35800] train loss: 0.8828, train acc: 0.6343, val loss: 0.8016, val acc: 0.6931  (best train acc: 0.6946, best val acc: 0.7005)\n",
      "[Epoch: 35820] train loss: 0.9145, train acc: 0.6188, val loss: 0.8949, val acc: 0.6401  (best train acc: 0.6946, best val acc: 0.7005)\n",
      "[Epoch: 35840] train loss: 0.8471, train acc: 0.6550, val loss: 0.8148, val acc: 0.6850  (best train acc: 0.6946, best val acc: 0.7005)\n",
      "[Epoch: 35860] train loss: 0.8299, train acc: 0.6703, val loss: 0.8007, val acc: 0.6951  (best train acc: 0.6946, best val acc: 0.7015)\n",
      "[Epoch: 35880] train loss: 0.8528, train acc: 0.6542, val loss: 0.8024, val acc: 0.6880  (best train acc: 0.6946, best val acc: 0.7015)\n",
      "[Epoch: 35900] train loss: 0.8604, train acc: 0.6529, val loss: 0.7991, val acc: 0.6924  (best train acc: 0.6946, best val acc: 0.7015)\n",
      "[Epoch: 35920] train loss: 0.8760, train acc: 0.6373, val loss: 0.8060, val acc: 0.6850  (best train acc: 0.6946, best val acc: 0.7015)\n",
      "[Epoch: 35940] train loss: 0.8693, train acc: 0.6440, val loss: 0.8072, val acc: 0.6809  (best train acc: 0.6946, best val acc: 0.7015)\n",
      "[Epoch: 35960] train loss: 0.8821, train acc: 0.6324, val loss: 0.7963, val acc: 0.6938  (best train acc: 0.6946, best val acc: 0.7015)\n",
      "[Epoch: 35980] train loss: 0.8388, train acc: 0.6615, val loss: 0.8070, val acc: 0.6833  (best train acc: 0.6946, best val acc: 0.7015)\n",
      "[Epoch: 36000] train loss: 0.8211, train acc: 0.6677, val loss: 0.8114, val acc: 0.6806  (best train acc: 0.6946, best val acc: 0.7015)\n",
      "[Epoch: 36020] train loss: 0.8227, train acc: 0.6766, val loss: 0.8041, val acc: 0.6853  (best train acc: 0.6946, best val acc: 0.7015)\n",
      "[Epoch: 36040] train loss: 0.8358, train acc: 0.6687, val loss: 0.8207, val acc: 0.6755  (best train acc: 0.6946, best val acc: 0.7015)\n",
      "[Epoch: 36060] train loss: 0.8276, train acc: 0.6653, val loss: 0.7994, val acc: 0.6934  (best train acc: 0.6946, best val acc: 0.7015)\n",
      "[Epoch: 36080] train loss: 0.9028, train acc: 0.6254, val loss: 0.8113, val acc: 0.6820  (best train acc: 0.6946, best val acc: 0.7015)\n",
      "[Epoch: 36100] train loss: 0.8188, train acc: 0.6751, val loss: 0.8359, val acc: 0.6644  (best train acc: 0.6946, best val acc: 0.7015)\n",
      "[Epoch: 36120] train loss: 0.8257, train acc: 0.6724, val loss: 0.8018, val acc: 0.6948  (best train acc: 0.6946, best val acc: 0.7015)\n",
      "[Epoch: 36140] train loss: 0.8382, train acc: 0.6649, val loss: 0.8023, val acc: 0.6924  (best train acc: 0.6946, best val acc: 0.7015)\n",
      "[Epoch: 36160] train loss: 0.8105, train acc: 0.6830, val loss: 0.8162, val acc: 0.6803  (best train acc: 0.6946, best val acc: 0.7015)\n",
      "[Epoch: 36180] train loss: 0.8332, train acc: 0.6674, val loss: 0.8057, val acc: 0.6870  (best train acc: 0.6946, best val acc: 0.7015)\n",
      "[Epoch: 36200] train loss: 0.8216, train acc: 0.6690, val loss: 0.8202, val acc: 0.6769  (best train acc: 0.6946, best val acc: 0.7015)\n",
      "[Epoch: 36220] train loss: 0.8490, train acc: 0.6608, val loss: 0.8012, val acc: 0.6954  (best train acc: 0.6946, best val acc: 0.7015)\n",
      "[Epoch: 36240] train loss: 0.8464, train acc: 0.6623, val loss: 0.7999, val acc: 0.6958  (best train acc: 0.6946, best val acc: 0.7015)\n",
      "[Epoch: 36260] train loss: 0.8758, train acc: 0.6378, val loss: 0.8047, val acc: 0.6853  (best train acc: 0.6946, best val acc: 0.7015)\n",
      "[Epoch: 36280] train loss: 0.8606, train acc: 0.6525, val loss: 0.8034, val acc: 0.6857  (best train acc: 0.6946, best val acc: 0.7015)\n",
      "[Epoch: 36300] train loss: 0.8532, train acc: 0.6578, val loss: 0.8029, val acc: 0.6887  (best train acc: 0.6946, best val acc: 0.7015)\n",
      "[Epoch: 36320] train loss: 0.8469, train acc: 0.6580, val loss: 0.7955, val acc: 0.6985  (best train acc: 0.6946, best val acc: 0.7015)\n",
      "[Epoch: 36340] train loss: 0.8856, train acc: 0.6384, val loss: 0.8311, val acc: 0.6607  (best train acc: 0.6946, best val acc: 0.7015)\n",
      "[Epoch: 36360] train loss: 0.8417, train acc: 0.6613, val loss: 0.8053, val acc: 0.6901  (best train acc: 0.6946, best val acc: 0.7015)\n",
      "[Epoch: 36380] train loss: 0.8659, train acc: 0.6502, val loss: 0.8064, val acc: 0.6847  (best train acc: 0.6946, best val acc: 0.7015)\n",
      "[Epoch: 36400] train loss: 0.8532, train acc: 0.6543, val loss: 0.7997, val acc: 0.6958  (best train acc: 0.6946, best val acc: 0.7015)\n",
      "[Epoch: 36420] train loss: 0.8469, train acc: 0.6553, val loss: 0.8102, val acc: 0.6836  (best train acc: 0.6946, best val acc: 0.7015)\n",
      "[Epoch: 36440] train loss: 0.8621, train acc: 0.6548, val loss: 0.8056, val acc: 0.6863  (best train acc: 0.6946, best val acc: 0.7015)\n",
      "[Epoch: 36460] train loss: 0.8608, train acc: 0.6524, val loss: 0.8154, val acc: 0.6806  (best train acc: 0.6946, best val acc: 0.7015)\n",
      "[Epoch: 36480] train loss: 0.7945, train acc: 0.6984, val loss: 0.7971, val acc: 0.6965  (best train acc: 0.6984, best val acc: 0.7015)\n",
      "[Epoch: 36500] train loss: 0.8228, train acc: 0.6736, val loss: 0.7974, val acc: 0.6944  (best train acc: 0.6984, best val acc: 0.7015)\n",
      "[Epoch: 36520] train loss: 0.8486, train acc: 0.6568, val loss: 0.8232, val acc: 0.6745  (best train acc: 0.6984, best val acc: 0.7015)\n",
      "[Epoch: 36540] train loss: 0.8896, train acc: 0.6408, val loss: 0.8247, val acc: 0.6715  (best train acc: 0.6984, best val acc: 0.7015)\n",
      "[Epoch: 36560] train loss: 0.8337, train acc: 0.6616, val loss: 0.8081, val acc: 0.6884  (best train acc: 0.6984, best val acc: 0.7015)\n",
      "[Epoch: 36580] train loss: 0.8743, train acc: 0.6432, val loss: 0.8083, val acc: 0.6870  (best train acc: 0.6984, best val acc: 0.7015)\n",
      "[Epoch: 36600] train loss: 0.9756, train acc: 0.5743, val loss: 0.8205, val acc: 0.6755  (best train acc: 0.6984, best val acc: 0.7015)\n",
      "[Epoch: 36620] train loss: 0.8709, train acc: 0.6426, val loss: 0.8046, val acc: 0.6870  (best train acc: 0.6984, best val acc: 0.7015)\n",
      "[Epoch: 36640] train loss: 0.8110, train acc: 0.6805, val loss: 0.7983, val acc: 0.6914  (best train acc: 0.6984, best val acc: 0.7015)\n",
      "[Epoch: 36660] train loss: 0.8701, train acc: 0.6474, val loss: 0.7964, val acc: 0.6911  (best train acc: 0.6984, best val acc: 0.7015)\n",
      "[Epoch: 36680] train loss: 0.8449, train acc: 0.6618, val loss: 0.7958, val acc: 0.6958  (best train acc: 0.6984, best val acc: 0.7015)\n",
      "[Epoch: 36700] train loss: 0.8425, train acc: 0.6563, val loss: 0.7955, val acc: 0.6944  (best train acc: 0.6984, best val acc: 0.7015)\n",
      "[Epoch: 36720] train loss: 0.8400, train acc: 0.6690, val loss: 0.8032, val acc: 0.6917  (best train acc: 0.6984, best val acc: 0.7015)\n",
      "[Epoch: 36740] train loss: 0.8544, train acc: 0.6575, val loss: 0.8066, val acc: 0.6867  (best train acc: 0.6984, best val acc: 0.7015)\n",
      "[Epoch: 36760] train loss: 0.8582, train acc: 0.6541, val loss: 0.8056, val acc: 0.6860  (best train acc: 0.6984, best val acc: 0.7015)\n",
      "[Epoch: 36780] train loss: 0.8714, train acc: 0.6423, val loss: 0.8329, val acc: 0.6702  (best train acc: 0.6984, best val acc: 0.7015)\n",
      "[Epoch: 36800] train loss: 0.8581, train acc: 0.6529, val loss: 0.8011, val acc: 0.6907  (best train acc: 0.6984, best val acc: 0.7015)\n",
      "[Epoch: 36820] train loss: 0.8354, train acc: 0.6679, val loss: 0.8226, val acc: 0.6752  (best train acc: 0.6984, best val acc: 0.7015)\n",
      "[Epoch: 36840] train loss: 0.8470, train acc: 0.6556, val loss: 0.7965, val acc: 0.6981  (best train acc: 0.6984, best val acc: 0.7015)\n",
      "[Epoch: 36860] train loss: 0.8242, train acc: 0.6693, val loss: 0.8003, val acc: 0.6944  (best train acc: 0.6984, best val acc: 0.7015)\n",
      "[Epoch: 36880] train loss: 0.8310, train acc: 0.6661, val loss: 0.7966, val acc: 0.6971  (best train acc: 0.6984, best val acc: 0.7015)\n",
      "[Epoch: 36900] train loss: 0.7972, train acc: 0.6853, val loss: 0.8165, val acc: 0.6803  (best train acc: 0.6984, best val acc: 0.7015)\n",
      "[Epoch: 36920] train loss: 0.8607, train acc: 0.6514, val loss: 0.8037, val acc: 0.6874  (best train acc: 0.6984, best val acc: 0.7015)\n",
      "[Epoch: 36940] train loss: 0.8321, train acc: 0.6650, val loss: 0.8100, val acc: 0.6847  (best train acc: 0.6984, best val acc: 0.7015)\n",
      "[Epoch: 36960] train loss: 0.8182, train acc: 0.6770, val loss: 0.8172, val acc: 0.6772  (best train acc: 0.6984, best val acc: 0.7015)\n",
      "[Epoch: 36980] train loss: 0.8876, train acc: 0.6319, val loss: 0.8014, val acc: 0.6894  (best train acc: 0.6984, best val acc: 0.7015)\n",
      "[Epoch: 37000] train loss: 0.8371, train acc: 0.6682, val loss: 0.7981, val acc: 0.6981  (best train acc: 0.6984, best val acc: 0.7015)\n",
      "[Epoch: 37020] train loss: 0.8612, train acc: 0.6422, val loss: 0.8072, val acc: 0.6863  (best train acc: 0.6984, best val acc: 0.7015)\n",
      "[Epoch: 37040] train loss: 0.8146, train acc: 0.6721, val loss: 0.8094, val acc: 0.6803  (best train acc: 0.6984, best val acc: 0.7015)\n",
      "[Epoch: 37060] train loss: 0.8342, train acc: 0.6634, val loss: 0.8000, val acc: 0.6927  (best train acc: 0.6984, best val acc: 0.7015)\n",
      "[Epoch: 37080] train loss: 0.8482, train acc: 0.6543, val loss: 0.8035, val acc: 0.6867  (best train acc: 0.6984, best val acc: 0.7015)\n",
      "[Epoch: 37100] train loss: 0.8272, train acc: 0.6690, val loss: 0.8533, val acc: 0.6428  (best train acc: 0.6984, best val acc: 0.7015)\n",
      "[Epoch: 37120] train loss: 0.8139, train acc: 0.6725, val loss: 0.8034, val acc: 0.6850  (best train acc: 0.6984, best val acc: 0.7015)\n",
      "[Epoch: 37140] train loss: 0.8310, train acc: 0.6694, val loss: 0.8052, val acc: 0.6870  (best train acc: 0.6984, best val acc: 0.7015)\n",
      "[Epoch: 37160] train loss: 0.8347, train acc: 0.6683, val loss: 0.7967, val acc: 0.6927  (best train acc: 0.6984, best val acc: 0.7015)\n",
      "[Epoch: 37180] train loss: 0.8843, train acc: 0.6376, val loss: 0.8473, val acc: 0.6624  (best train acc: 0.6984, best val acc: 0.7015)\n",
      "[Epoch: 37200] train loss: 0.8977, train acc: 0.6343, val loss: 0.8109, val acc: 0.6833  (best train acc: 0.6984, best val acc: 0.7015)\n",
      "[Epoch: 37220] train loss: 0.8897, train acc: 0.6434, val loss: 0.8237, val acc: 0.6779  (best train acc: 0.6984, best val acc: 0.7015)\n",
      "[Epoch: 37240] train loss: 0.8450, train acc: 0.6632, val loss: 0.8104, val acc: 0.6843  (best train acc: 0.6984, best val acc: 0.7015)\n",
      "[Epoch: 37260] train loss: 0.8927, train acc: 0.6279, val loss: 0.7973, val acc: 0.6951  (best train acc: 0.6984, best val acc: 0.7015)\n",
      "[Epoch: 37280] train loss: 0.8926, train acc: 0.6368, val loss: 0.7955, val acc: 0.6911  (best train acc: 0.6984, best val acc: 0.7015)\n",
      "[Epoch: 37300] train loss: 0.9508, train acc: 0.5978, val loss: 0.7968, val acc: 0.6958  (best train acc: 0.6984, best val acc: 0.7015)\n",
      "[Epoch: 37320] train loss: 0.8308, train acc: 0.6686, val loss: 0.7954, val acc: 0.6968  (best train acc: 0.6984, best val acc: 0.7015)\n",
      "[Epoch: 37340] train loss: 0.8404, train acc: 0.6629, val loss: 0.7969, val acc: 0.6914  (best train acc: 0.6984, best val acc: 0.7015)\n",
      "[Epoch: 37360] train loss: 0.8437, train acc: 0.6559, val loss: 0.7944, val acc: 0.6897  (best train acc: 0.6984, best val acc: 0.7015)\n",
      "[Epoch: 37380] train loss: 0.8065, train acc: 0.6801, val loss: 0.7936, val acc: 0.6978  (best train acc: 0.6984, best val acc: 0.7019)\n",
      "[Epoch: 37400] train loss: 0.8609, train acc: 0.6526, val loss: 0.7950, val acc: 0.6971  (best train acc: 0.6984, best val acc: 0.7019)\n",
      "[Epoch: 37420] train loss: 0.8440, train acc: 0.6622, val loss: 0.8352, val acc: 0.6621  (best train acc: 0.6984, best val acc: 0.7019)\n",
      "[Epoch: 37440] train loss: 0.8052, train acc: 0.6887, val loss: 0.8130, val acc: 0.6826  (best train acc: 0.6984, best val acc: 0.7019)\n",
      "[Epoch: 37460] train loss: 0.8590, train acc: 0.6431, val loss: 0.8355, val acc: 0.6691  (best train acc: 0.6984, best val acc: 0.7019)\n",
      "[Epoch: 37480] train loss: 0.8627, train acc: 0.6486, val loss: 0.8012, val acc: 0.6904  (best train acc: 0.6984, best val acc: 0.7019)\n",
      "[Epoch: 37500] train loss: 0.8838, train acc: 0.6361, val loss: 0.8338, val acc: 0.6708  (best train acc: 0.6984, best val acc: 0.7019)\n",
      "[Epoch: 37520] train loss: 0.8646, train acc: 0.6525, val loss: 0.8029, val acc: 0.6927  (best train acc: 0.6984, best val acc: 0.7019)\n",
      "[Epoch: 37540] train loss: 0.8533, train acc: 0.6566, val loss: 0.7960, val acc: 0.6941  (best train acc: 0.6984, best val acc: 0.7019)\n",
      "[Epoch: 37560] train loss: 0.8647, train acc: 0.6468, val loss: 0.7970, val acc: 0.6998  (best train acc: 0.6984, best val acc: 0.7019)\n",
      "[Epoch: 37580] train loss: 0.8941, train acc: 0.6343, val loss: 0.7994, val acc: 0.6880  (best train acc: 0.6984, best val acc: 0.7019)\n",
      "[Epoch: 37600] train loss: 0.8028, train acc: 0.6867, val loss: 0.8256, val acc: 0.6712  (best train acc: 0.6984, best val acc: 0.7019)\n",
      "[Epoch: 37620] train loss: 0.8566, train acc: 0.6544, val loss: 0.8066, val acc: 0.6874  (best train acc: 0.6984, best val acc: 0.7019)\n",
      "[Epoch: 37640] train loss: 0.9018, train acc: 0.6220, val loss: 0.8027, val acc: 0.6887  (best train acc: 0.6984, best val acc: 0.7019)\n",
      "[Epoch: 37660] train loss: 0.8075, train acc: 0.6792, val loss: 0.8005, val acc: 0.6907  (best train acc: 0.6984, best val acc: 0.7019)\n",
      "[Epoch: 37680] train loss: 0.8796, train acc: 0.6424, val loss: 0.7961, val acc: 0.6958  (best train acc: 0.6984, best val acc: 0.7019)\n",
      "[Epoch: 37700] train loss: 0.8753, train acc: 0.6281, val loss: 0.7959, val acc: 0.6934  (best train acc: 0.6984, best val acc: 0.7019)\n",
      "[Epoch: 37720] train loss: 0.8307, train acc: 0.6672, val loss: 0.7987, val acc: 0.6924  (best train acc: 0.6984, best val acc: 0.7025)\n",
      "[Epoch: 37740] train loss: 0.8693, train acc: 0.6437, val loss: 0.7950, val acc: 0.6971  (best train acc: 0.6984, best val acc: 0.7025)\n",
      "[Epoch: 37760] train loss: 0.8717, train acc: 0.6395, val loss: 0.7951, val acc: 0.6934  (best train acc: 0.6984, best val acc: 0.7025)\n",
      "[Epoch: 37780] train loss: 0.8172, train acc: 0.6752, val loss: 0.8080, val acc: 0.6840  (best train acc: 0.6984, best val acc: 0.7025)\n",
      "[Epoch: 37800] train loss: 0.8251, train acc: 0.6689, val loss: 0.7990, val acc: 0.6911  (best train acc: 0.6984, best val acc: 0.7025)\n",
      "[Epoch: 37820] train loss: 0.8473, train acc: 0.6523, val loss: 0.7947, val acc: 0.6948  (best train acc: 0.6984, best val acc: 0.7025)\n",
      "[Epoch: 37840] train loss: 0.8461, train acc: 0.6546, val loss: 0.7962, val acc: 0.6971  (best train acc: 0.6984, best val acc: 0.7025)\n",
      "[Epoch: 37860] train loss: 0.8659, train acc: 0.6512, val loss: 0.7952, val acc: 0.6941  (best train acc: 0.6984, best val acc: 0.7025)\n",
      "[Epoch: 37880] train loss: 0.8241, train acc: 0.6729, val loss: 0.8159, val acc: 0.6813  (best train acc: 0.6984, best val acc: 0.7025)\n",
      "[Epoch: 37900] train loss: 0.8167, train acc: 0.6776, val loss: 0.8027, val acc: 0.6840  (best train acc: 0.6984, best val acc: 0.7025)\n",
      "[Epoch: 37920] train loss: 0.8573, train acc: 0.6619, val loss: 0.8033, val acc: 0.6874  (best train acc: 0.6984, best val acc: 0.7025)\n",
      "[Epoch: 37940] train loss: 0.8382, train acc: 0.6651, val loss: 0.8044, val acc: 0.6904  (best train acc: 0.6984, best val acc: 0.7025)\n",
      "[Epoch: 37960] train loss: 0.8149, train acc: 0.6745, val loss: 0.7994, val acc: 0.6931  (best train acc: 0.6984, best val acc: 0.7025)\n",
      "[Epoch: 37980] train loss: 0.8015, train acc: 0.6847, val loss: 0.8167, val acc: 0.6752  (best train acc: 0.6984, best val acc: 0.7025)\n",
      "[Epoch: 38000] train loss: 0.8501, train acc: 0.6593, val loss: 0.8275, val acc: 0.6705  (best train acc: 0.6984, best val acc: 0.7025)\n",
      "[Epoch: 38020] train loss: 0.8286, train acc: 0.6737, val loss: 0.7998, val acc: 0.6944  (best train acc: 0.6984, best val acc: 0.7025)\n",
      "[Epoch: 38040] train loss: 0.8509, train acc: 0.6568, val loss: 0.7955, val acc: 0.6954  (best train acc: 0.6984, best val acc: 0.7025)\n",
      "[Epoch: 38060] train loss: 0.8354, train acc: 0.6708, val loss: 0.8067, val acc: 0.6853  (best train acc: 0.6984, best val acc: 0.7025)\n",
      "[Epoch: 38080] train loss: 0.8280, train acc: 0.6721, val loss: 0.7965, val acc: 0.6992  (best train acc: 0.6984, best val acc: 0.7025)\n",
      "[Epoch: 38100] train loss: 0.8800, train acc: 0.6467, val loss: 0.8022, val acc: 0.6894  (best train acc: 0.6984, best val acc: 0.7025)\n",
      "[Epoch: 38120] train loss: 0.8667, train acc: 0.6460, val loss: 0.8037, val acc: 0.6880  (best train acc: 0.6984, best val acc: 0.7025)\n",
      "[Epoch: 38140] train loss: 0.8448, train acc: 0.6644, val loss: 0.8031, val acc: 0.6874  (best train acc: 0.6984, best val acc: 0.7025)\n",
      "[Epoch: 38160] train loss: 0.8561, train acc: 0.6479, val loss: 0.7940, val acc: 0.6938  (best train acc: 0.6984, best val acc: 0.7025)\n",
      "[Epoch: 38180] train loss: 0.8530, train acc: 0.6517, val loss: 0.8304, val acc: 0.6712  (best train acc: 0.6984, best val acc: 0.7025)\n",
      "[Epoch: 38200] train loss: 0.8628, train acc: 0.6533, val loss: 0.8167, val acc: 0.6823  (best train acc: 0.6984, best val acc: 0.7025)\n",
      "[Epoch: 38220] train loss: 0.8707, train acc: 0.6494, val loss: 0.8021, val acc: 0.6944  (best train acc: 0.6984, best val acc: 0.7025)\n",
      "[Epoch: 38240] train loss: 0.8997, train acc: 0.6330, val loss: 0.7984, val acc: 0.6914  (best train acc: 0.6984, best val acc: 0.7025)\n",
      "[Epoch: 38260] train loss: 0.8698, train acc: 0.6421, val loss: 0.8104, val acc: 0.6806  (best train acc: 0.6984, best val acc: 0.7025)\n",
      "[Epoch: 38280] train loss: 0.8220, train acc: 0.6776, val loss: 0.8297, val acc: 0.6742  (best train acc: 0.6984, best val acc: 0.7025)\n",
      "[Epoch: 38300] train loss: 0.9367, train acc: 0.6025, val loss: 0.8169, val acc: 0.6806  (best train acc: 0.6984, best val acc: 0.7025)\n",
      "[Epoch: 38320] train loss: 0.8614, train acc: 0.6554, val loss: 0.8206, val acc: 0.6813  (best train acc: 0.6984, best val acc: 0.7025)\n",
      "[Epoch: 38340] train loss: 0.8742, train acc: 0.6416, val loss: 0.8113, val acc: 0.6796  (best train acc: 0.6984, best val acc: 0.7025)\n",
      "[Epoch: 38360] train loss: 0.8515, train acc: 0.6583, val loss: 0.7958, val acc: 0.6938  (best train acc: 0.6984, best val acc: 0.7025)\n",
      "[Epoch: 38380] train loss: 0.8589, train acc: 0.6529, val loss: 0.8074, val acc: 0.6863  (best train acc: 0.6984, best val acc: 0.7025)\n",
      "[Epoch: 38400] train loss: 0.9754, train acc: 0.5693, val loss: 0.8326, val acc: 0.6654  (best train acc: 0.6984, best val acc: 0.7025)\n",
      "[Epoch: 38420] train loss: 0.8910, train acc: 0.6442, val loss: 0.8467, val acc: 0.6637  (best train acc: 0.6984, best val acc: 0.7025)\n",
      "[Epoch: 38440] train loss: 0.8286, train acc: 0.6698, val loss: 0.8254, val acc: 0.6749  (best train acc: 0.6984, best val acc: 0.7025)\n",
      "[Epoch: 38460] train loss: 0.8456, train acc: 0.6663, val loss: 0.8130, val acc: 0.6843  (best train acc: 0.6984, best val acc: 0.7025)\n",
      "[Epoch: 38480] train loss: 0.8456, train acc: 0.6611, val loss: 0.7975, val acc: 0.6985  (best train acc: 0.6984, best val acc: 0.7025)\n",
      "[Epoch: 38500] train loss: 0.8010, train acc: 0.6951, val loss: 0.8772, val acc: 0.6165  (best train acc: 0.6984, best val acc: 0.7025)\n",
      "[Epoch: 38520] train loss: 0.8109, train acc: 0.6784, val loss: 0.8108, val acc: 0.6840  (best train acc: 0.6984, best val acc: 0.7025)\n",
      "[Epoch: 38540] train loss: 0.8327, train acc: 0.6672, val loss: 0.7988, val acc: 0.6971  (best train acc: 0.6984, best val acc: 0.7025)\n",
      "[Epoch: 38560] train loss: 0.8154, train acc: 0.6836, val loss: 0.7974, val acc: 0.6971  (best train acc: 0.6984, best val acc: 0.7025)\n",
      "[Epoch: 38580] train loss: 0.8911, train acc: 0.6299, val loss: 0.8438, val acc: 0.6530  (best train acc: 0.6984, best val acc: 0.7025)\n",
      "[Epoch: 38600] train loss: 0.8398, train acc: 0.6584, val loss: 0.8621, val acc: 0.6597  (best train acc: 0.6984, best val acc: 0.7025)\n",
      "[Epoch: 38620] train loss: 0.8559, train acc: 0.6592, val loss: 0.8091, val acc: 0.6830  (best train acc: 0.6984, best val acc: 0.7025)\n",
      "[Epoch: 38640] train loss: 0.8419, train acc: 0.6649, val loss: 0.7998, val acc: 0.6934  (best train acc: 0.6984, best val acc: 0.7025)\n",
      "[Epoch: 38660] train loss: 0.8362, train acc: 0.6554, val loss: 0.8186, val acc: 0.6806  (best train acc: 0.6984, best val acc: 0.7025)\n",
      "[Epoch: 38680] train loss: 0.8282, train acc: 0.6703, val loss: 0.7956, val acc: 0.6958  (best train acc: 0.6984, best val acc: 0.7025)\n",
      "[Epoch: 38700] train loss: 0.8639, train acc: 0.6538, val loss: 0.7969, val acc: 0.6948  (best train acc: 0.6984, best val acc: 0.7025)\n",
      "[Epoch: 38720] train loss: 0.8788, train acc: 0.6404, val loss: 0.7953, val acc: 0.6931  (best train acc: 0.6984, best val acc: 0.7025)\n",
      "[Epoch: 38740] train loss: 0.8634, train acc: 0.6493, val loss: 0.8193, val acc: 0.6772  (best train acc: 0.6984, best val acc: 0.7025)\n",
      "[Epoch: 38760] train loss: 0.8637, train acc: 0.6588, val loss: 0.7989, val acc: 0.6965  (best train acc: 0.6984, best val acc: 0.7025)\n",
      "[Epoch: 38780] train loss: 0.8366, train acc: 0.6673, val loss: 0.7957, val acc: 0.6961  (best train acc: 0.6984, best val acc: 0.7025)\n",
      "[Epoch: 38800] train loss: 0.8121, train acc: 0.6820, val loss: 0.8137, val acc: 0.6809  (best train acc: 0.6984, best val acc: 0.7025)\n",
      "[Epoch: 38820] train loss: 0.7943, train acc: 0.6904, val loss: 0.8384, val acc: 0.6604  (best train acc: 0.6984, best val acc: 0.7025)\n",
      "[Epoch: 38840] train loss: 0.8313, train acc: 0.6713, val loss: 0.8079, val acc: 0.6830  (best train acc: 0.6984, best val acc: 0.7025)\n",
      "[Epoch: 38860] train loss: 0.8540, train acc: 0.6536, val loss: 0.7968, val acc: 0.6975  (best train acc: 0.6984, best val acc: 0.7029)\n",
      "[Epoch: 38880] train loss: 0.8494, train acc: 0.6567, val loss: 0.7943, val acc: 0.6954  (best train acc: 0.6984, best val acc: 0.7029)\n",
      "[Epoch: 38900] train loss: 0.8466, train acc: 0.6530, val loss: 0.7976, val acc: 0.6890  (best train acc: 0.6984, best val acc: 0.7029)\n",
      "[Epoch: 38920] train loss: 0.8812, train acc: 0.6348, val loss: 0.8043, val acc: 0.6843  (best train acc: 0.6984, best val acc: 0.7029)\n",
      "[Epoch: 38940] train loss: 0.8762, train acc: 0.6397, val loss: 0.8010, val acc: 0.6951  (best train acc: 0.6984, best val acc: 0.7029)\n",
      "[Epoch: 38960] train loss: 0.8246, train acc: 0.6815, val loss: 0.8057, val acc: 0.6863  (best train acc: 0.6984, best val acc: 0.7029)\n",
      "[Epoch: 38980] train loss: 0.8507, train acc: 0.6578, val loss: 0.8067, val acc: 0.6890  (best train acc: 0.6984, best val acc: 0.7042)\n",
      "[Epoch: 39000] train loss: 0.8960, train acc: 0.6340, val loss: 0.8248, val acc: 0.6702  (best train acc: 0.6984, best val acc: 0.7042)\n",
      "[Epoch: 39020] train loss: 0.8655, train acc: 0.6473, val loss: 0.8178, val acc: 0.6836  (best train acc: 0.6984, best val acc: 0.7042)\n",
      "[Epoch: 39040] train loss: 0.8511, train acc: 0.6632, val loss: 0.8095, val acc: 0.6823  (best train acc: 0.6984, best val acc: 0.7042)\n",
      "[Epoch: 39060] train loss: 0.8415, train acc: 0.6624, val loss: 0.8347, val acc: 0.6678  (best train acc: 0.6984, best val acc: 0.7042)\n",
      "[Epoch: 39080] train loss: 0.8461, train acc: 0.6642, val loss: 0.8124, val acc: 0.6836  (best train acc: 0.6984, best val acc: 0.7042)\n",
      "[Epoch: 39100] train loss: 0.8780, train acc: 0.6469, val loss: 0.7963, val acc: 0.6921  (best train acc: 0.6984, best val acc: 0.7042)\n",
      "[Epoch: 39120] train loss: 0.9008, train acc: 0.6359, val loss: 0.7969, val acc: 0.6944  (best train acc: 0.6984, best val acc: 0.7042)\n",
      "[Epoch: 39140] train loss: 0.9343, train acc: 0.6117, val loss: 0.8595, val acc: 0.6344  (best train acc: 0.6984, best val acc: 0.7042)\n",
      "[Epoch: 39160] train loss: 0.9029, train acc: 0.6382, val loss: 0.8294, val acc: 0.6705  (best train acc: 0.6984, best val acc: 0.7042)\n",
      "[Epoch: 39180] train loss: 0.8828, train acc: 0.6349, val loss: 0.8279, val acc: 0.6772  (best train acc: 0.6984, best val acc: 0.7042)\n",
      "[Epoch: 39200] train loss: 0.8403, train acc: 0.6643, val loss: 0.7961, val acc: 0.6938  (best train acc: 0.6984, best val acc: 0.7042)\n",
      "[Epoch: 39220] train loss: 0.8494, train acc: 0.6580, val loss: 0.7974, val acc: 0.6975  (best train acc: 0.6984, best val acc: 0.7042)\n",
      "[Epoch: 39240] train loss: 0.8338, train acc: 0.6664, val loss: 0.8178, val acc: 0.6782  (best train acc: 0.6984, best val acc: 0.7042)\n",
      "[Epoch: 39260] train loss: 0.8418, train acc: 0.6485, val loss: 0.8010, val acc: 0.6897  (best train acc: 0.6996, best val acc: 0.7042)\n",
      "[Epoch: 39280] train loss: 0.8689, train acc: 0.6385, val loss: 0.7963, val acc: 0.6927  (best train acc: 0.6996, best val acc: 0.7042)\n",
      "[Epoch: 39300] train loss: 0.8142, train acc: 0.6771, val loss: 0.7967, val acc: 0.6954  (best train acc: 0.6996, best val acc: 0.7042)\n",
      "[Epoch: 39320] train loss: 0.8147, train acc: 0.6740, val loss: 0.7989, val acc: 0.6931  (best train acc: 0.6996, best val acc: 0.7042)\n",
      "[Epoch: 39340] train loss: 0.8629, train acc: 0.6538, val loss: 0.8122, val acc: 0.6793  (best train acc: 0.6996, best val acc: 0.7042)\n",
      "[Epoch: 39360] train loss: 0.8612, train acc: 0.6490, val loss: 0.7963, val acc: 0.6971  (best train acc: 0.6996, best val acc: 0.7042)\n",
      "[Epoch: 39380] train loss: 0.8368, train acc: 0.6615, val loss: 0.7926, val acc: 0.6931  (best train acc: 0.6996, best val acc: 0.7042)\n",
      "[Epoch: 39400] train loss: 0.8385, train acc: 0.6657, val loss: 0.7927, val acc: 0.6961  (best train acc: 0.6996, best val acc: 0.7042)\n",
      "[Epoch: 39420] train loss: 0.8034, train acc: 0.6833, val loss: 0.7981, val acc: 0.6931  (best train acc: 0.6996, best val acc: 0.7042)\n",
      "[Epoch: 39440] train loss: 0.8848, train acc: 0.6271, val loss: 0.7940, val acc: 0.6965  (best train acc: 0.6996, best val acc: 0.7042)\n",
      "[Epoch: 39460] train loss: 0.8053, train acc: 0.6838, val loss: 0.7949, val acc: 0.6978  (best train acc: 0.6996, best val acc: 0.7042)\n",
      "[Epoch: 39480] train loss: 0.8391, train acc: 0.6622, val loss: 0.7998, val acc: 0.6921  (best train acc: 0.6996, best val acc: 0.7042)\n",
      "[Epoch: 39500] train loss: 0.8426, train acc: 0.6548, val loss: 0.8019, val acc: 0.6924  (best train acc: 0.6996, best val acc: 0.7042)\n",
      "[Epoch: 39520] train loss: 0.8778, train acc: 0.6483, val loss: 0.8065, val acc: 0.6887  (best train acc: 0.6996, best val acc: 0.7042)\n",
      "[Epoch: 39540] train loss: 0.8187, train acc: 0.6718, val loss: 0.8572, val acc: 0.6445  (best train acc: 0.6996, best val acc: 0.7042)\n",
      "[Epoch: 39560] train loss: 0.9242, train acc: 0.6109, val loss: 0.8609, val acc: 0.6540  (best train acc: 0.6996, best val acc: 0.7042)\n",
      "[Epoch: 39580] train loss: 0.8501, train acc: 0.6559, val loss: 0.7964, val acc: 0.6965  (best train acc: 0.6996, best val acc: 0.7042)\n",
      "[Epoch: 39600] train loss: 0.8415, train acc: 0.6542, val loss: 0.7922, val acc: 0.6981  (best train acc: 0.6996, best val acc: 0.7042)\n",
      "[Epoch: 39620] train loss: 0.8374, train acc: 0.6645, val loss: 0.8169, val acc: 0.6762  (best train acc: 0.6996, best val acc: 0.7042)\n",
      "[Epoch: 39640] train loss: 0.8670, train acc: 0.6457, val loss: 0.8129, val acc: 0.6809  (best train acc: 0.6996, best val acc: 0.7042)\n",
      "[Epoch: 39660] train loss: 0.8572, train acc: 0.6581, val loss: 0.7935, val acc: 0.6981  (best train acc: 0.6996, best val acc: 0.7042)\n",
      "[Epoch: 39680] train loss: 0.8328, train acc: 0.6639, val loss: 0.8034, val acc: 0.6890  (best train acc: 0.6996, best val acc: 0.7042)\n",
      "[Epoch: 39700] train loss: 0.8408, train acc: 0.6606, val loss: 0.7917, val acc: 0.6998  (best train acc: 0.6996, best val acc: 0.7042)\n",
      "[Epoch: 39720] train loss: 0.8289, train acc: 0.6729, val loss: 0.7970, val acc: 0.6944  (best train acc: 0.6996, best val acc: 0.7042)\n",
      "[Epoch: 39740] train loss: 0.8340, train acc: 0.6624, val loss: 0.8302, val acc: 0.6715  (best train acc: 0.6996, best val acc: 0.7042)\n",
      "[Epoch: 39760] train loss: 0.8597, train acc: 0.6553, val loss: 0.8363, val acc: 0.6637  (best train acc: 0.6996, best val acc: 0.7042)\n",
      "[Epoch: 39780] train loss: 0.8288, train acc: 0.6691, val loss: 0.8278, val acc: 0.6718  (best train acc: 0.6996, best val acc: 0.7042)\n",
      "[Epoch: 39800] train loss: 0.8717, train acc: 0.6421, val loss: 0.8018, val acc: 0.6901  (best train acc: 0.6996, best val acc: 0.7042)\n",
      "[Epoch: 39820] train loss: 0.8339, train acc: 0.6624, val loss: 0.8135, val acc: 0.6809  (best train acc: 0.6996, best val acc: 0.7042)\n",
      "[Epoch: 39840] train loss: 0.8279, train acc: 0.6718, val loss: 0.7992, val acc: 0.6914  (best train acc: 0.6996, best val acc: 0.7042)\n",
      "[Epoch: 39860] train loss: 0.8769, train acc: 0.6397, val loss: 0.7938, val acc: 0.6954  (best train acc: 0.6996, best val acc: 0.7042)\n",
      "[Epoch: 39880] train loss: 0.8383, train acc: 0.6629, val loss: 0.8199, val acc: 0.6786  (best train acc: 0.6996, best val acc: 0.7042)\n",
      "[Epoch: 39900] train loss: 0.8268, train acc: 0.6817, val loss: 0.8171, val acc: 0.6877  (best train acc: 0.6996, best val acc: 0.7042)\n",
      "[Epoch: 39920] train loss: 0.8241, train acc: 0.6693, val loss: 0.8248, val acc: 0.6739  (best train acc: 0.6996, best val acc: 0.7042)\n",
      "[Epoch: 39940] train loss: 0.8362, train acc: 0.6665, val loss: 0.8003, val acc: 0.6921  (best train acc: 0.6996, best val acc: 0.7042)\n",
      "[Epoch: 39960] train loss: 0.8145, train acc: 0.6791, val loss: 0.7935, val acc: 0.6992  (best train acc: 0.6996, best val acc: 0.7042)\n",
      "[Epoch: 39980] train loss: 0.8298, train acc: 0.6694, val loss: 0.8157, val acc: 0.6772  (best train acc: 0.6996, best val acc: 0.7042)\n",
      "[Epoch: 40000] train loss: 0.8119, train acc: 0.6727, val loss: 0.8245, val acc: 0.6745  (best train acc: 0.6996, best val acc: 0.7042)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pmaur\\miniconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\pmaur\\miniconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\pmaur\\miniconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\pmaur\\miniconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\pmaur\\miniconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\pmaur\\miniconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       154\n",
      "           1       0.71      0.79      0.75       703\n",
      "           2       0.55      0.54      0.55       702\n",
      "           3       0.65      0.76      0.70       703\n",
      "           4       0.93      0.90      0.92       702\n",
      "\n",
      "    accuracy                           0.71      2964\n",
      "   macro avg       0.57      0.60      0.58      2964\n",
      "weighted avg       0.67      0.71      0.69      2964\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 504x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 504x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "gnn = GNN2L_GCN(data_with_nedbit).to(device)\n",
    "pred = train(gnn, data_with_nedbit.to(device), 40000, cm_title='GAT7L_multiclass_16HC', classes=['P', 'LP', 'WN', 'LN', 'RN'], weight_decay=0.0005, arch='GCN', layers=2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fd3f8a02e634e069160a5f4101abed0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40001 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch: 0000] train loss: 344.9328, train acc: 0.2349, val loss: 317.4842, val acc: 0.2371  (best train acc: 0.2349, best val acc: 0.2371)\n",
      "[Epoch: 0020] train loss: 100.4811, train acc: 0.2469, val loss: 78.7473, val acc: 0.2371  (best train acc: 0.2543, best val acc: 0.2371)\n",
      "[Epoch: 0040] train loss: 42.4615, train acc: 0.2673, val loss: 25.7609, val acc: 0.2368  (best train acc: 0.2788, best val acc: 0.2371)\n",
      "[Epoch: 0060] train loss: 14.8375, train acc: 0.1629, val loss: 6.3664, val acc: 0.0563  (best train acc: 0.2788, best val acc: 0.2401)\n",
      "[Epoch: 0080] train loss: 1.7562, train acc: 0.2304, val loss: 1.6517, val acc: 0.2685  (best train acc: 0.2788, best val acc: 0.2685)\n",
      "[Epoch: 0100] train loss: 1.6069, train acc: 0.2373, val loss: 1.6067, val acc: 0.2374  (best train acc: 0.3012, best val acc: 0.2685)\n",
      "[Epoch: 0120] train loss: 1.6034, train acc: 0.2371, val loss: 1.6032, val acc: 0.2371  (best train acc: 0.3012, best val acc: 0.2685)\n",
      "[Epoch: 0140] train loss: 1.5999, train acc: 0.2371, val loss: 1.5998, val acc: 0.2371  (best train acc: 0.3012, best val acc: 0.2685)\n",
      "[Epoch: 0160] train loss: 1.5966, train acc: 0.2371, val loss: 1.5965, val acc: 0.2371  (best train acc: 0.3012, best val acc: 0.2685)\n",
      "[Epoch: 0180] train loss: 1.5935, train acc: 0.2371, val loss: 1.5933, val acc: 0.2371  (best train acc: 0.3012, best val acc: 0.2685)\n",
      "[Epoch: 0200] train loss: 1.5904, train acc: 0.2371, val loss: 1.5903, val acc: 0.2371  (best train acc: 0.3012, best val acc: 0.2685)\n",
      "[Epoch: 0220] train loss: 1.5875, train acc: 0.2371, val loss: 1.5874, val acc: 0.2371  (best train acc: 0.3012, best val acc: 0.2685)\n",
      "[Epoch: 0240] train loss: 1.5847, train acc: 0.2371, val loss: 1.5846, val acc: 0.2371  (best train acc: 0.3012, best val acc: 0.2685)\n",
      "[Epoch: 0260] train loss: 1.5820, train acc: 0.2371, val loss: 1.5819, val acc: 0.2371  (best train acc: 0.3012, best val acc: 0.2685)\n",
      "[Epoch: 0280] train loss: 1.5794, train acc: 0.2371, val loss: 1.5793, val acc: 0.2371  (best train acc: 0.3012, best val acc: 0.2685)\n",
      "[Epoch: 0300] train loss: 1.5769, train acc: 0.2371, val loss: 1.5768, val acc: 0.2371  (best train acc: 0.3012, best val acc: 0.2685)\n",
      "[Epoch: 0320] train loss: 1.5744, train acc: 0.2371, val loss: 1.5744, val acc: 0.2371  (best train acc: 0.3012, best val acc: 0.2685)\n",
      "[Epoch: 0340] train loss: 1.5721, train acc: 0.2371, val loss: 1.5720, val acc: 0.2371  (best train acc: 0.3012, best val acc: 0.2685)\n",
      "[Epoch: 0360] train loss: 1.5699, train acc: 0.2371, val loss: 1.5698, val acc: 0.2371  (best train acc: 0.3012, best val acc: 0.2685)\n",
      "[Epoch: 0380] train loss: 1.5678, train acc: 0.2371, val loss: 1.5677, val acc: 0.2371  (best train acc: 0.3012, best val acc: 0.2685)\n",
      "[Epoch: 0400] train loss: 1.5657, train acc: 0.2371, val loss: 1.5657, val acc: 0.2371  (best train acc: 0.3012, best val acc: 0.2685)\n",
      "[Epoch: 0420] train loss: 1.5637, train acc: 0.2371, val loss: 1.5637, val acc: 0.2371  (best train acc: 0.3012, best val acc: 0.2685)\n",
      "[Epoch: 0440] train loss: 1.5618, train acc: 0.2371, val loss: 1.5618, val acc: 0.2371  (best train acc: 0.3012, best val acc: 0.2685)\n",
      "[Epoch: 0460] train loss: 1.5600, train acc: 0.2371, val loss: 1.5600, val acc: 0.2371  (best train acc: 0.3012, best val acc: 0.2685)\n",
      "[Epoch: 0480] train loss: 1.5583, train acc: 0.2370, val loss: 1.5582, val acc: 0.2371  (best train acc: 0.3012, best val acc: 0.2685)\n",
      "[Epoch: 0500] train loss: 1.5566, train acc: 0.2370, val loss: 1.5566, val acc: 0.2371  (best train acc: 0.3012, best val acc: 0.2685)\n",
      "[Epoch: 0520] train loss: 1.5550, train acc: 0.2369, val loss: 1.5550, val acc: 0.2371  (best train acc: 0.3012, best val acc: 0.2685)\n",
      "[Epoch: 0540] train loss: 1.5535, train acc: 0.2370, val loss: 1.5535, val acc: 0.2371  (best train acc: 0.3012, best val acc: 0.2685)\n",
      "[Epoch: 0560] train loss: 1.5520, train acc: 0.2370, val loss: 1.5520, val acc: 0.2371  (best train acc: 0.3012, best val acc: 0.2685)\n",
      "[Epoch: 0580] train loss: 1.5506, train acc: 0.2370, val loss: 1.5505, val acc: 0.2371  (best train acc: 0.3012, best val acc: 0.2685)\n",
      "[Epoch: 0600] train loss: 1.5492, train acc: 0.2370, val loss: 1.5492, val acc: 0.2371  (best train acc: 0.3012, best val acc: 0.2685)\n",
      "[Epoch: 0620] train loss: 1.5479, train acc: 0.2370, val loss: 1.5479, val acc: 0.2371  (best train acc: 0.3012, best val acc: 0.2685)\n",
      "[Epoch: 0640] train loss: 1.5466, train acc: 0.2370, val loss: 1.5466, val acc: 0.2371  (best train acc: 0.3012, best val acc: 0.2685)\n",
      "[Epoch: 0660] train loss: 1.5454, train acc: 0.2370, val loss: 1.5454, val acc: 0.2371  (best train acc: 0.3012, best val acc: 0.2685)\n",
      "[Epoch: 0680] train loss: 1.5442, train acc: 0.2370, val loss: 1.5442, val acc: 0.2371  (best train acc: 0.3012, best val acc: 0.2685)\n",
      "[Epoch: 0700] train loss: 1.5431, train acc: 0.2370, val loss: 1.5431, val acc: 0.2371  (best train acc: 0.3012, best val acc: 0.2685)\n",
      "[Epoch: 0720] train loss: 1.5420, train acc: 0.2370, val loss: 1.5420, val acc: 0.2371  (best train acc: 0.3012, best val acc: 0.2685)\n",
      "[Epoch: 0740] train loss: 1.5410, train acc: 0.2370, val loss: 1.5410, val acc: 0.2371  (best train acc: 0.3012, best val acc: 0.2685)\n",
      "[Epoch: 0760] train loss: 1.5400, train acc: 0.2370, val loss: 1.5400, val acc: 0.2371  (best train acc: 0.3012, best val acc: 0.2685)\n",
      "[Epoch: 0780] train loss: 1.5390, train acc: 0.2370, val loss: 1.5391, val acc: 0.2371  (best train acc: 0.3012, best val acc: 0.2685)\n",
      "[Epoch: 0800] train loss: 1.5381, train acc: 0.2370, val loss: 1.5381, val acc: 0.2371  (best train acc: 0.3012, best val acc: 0.2685)\n",
      "[Epoch: 0820] train loss: 1.5372, train acc: 0.2370, val loss: 1.5373, val acc: 0.2371  (best train acc: 0.3012, best val acc: 0.2685)\n",
      "[Epoch: 0840] train loss: 1.5364, train acc: 0.2370, val loss: 1.5364, val acc: 0.2371  (best train acc: 0.3012, best val acc: 0.2685)\n",
      "[Epoch: 0860] train loss: 1.5356, train acc: 0.2370, val loss: 1.5356, val acc: 0.2371  (best train acc: 0.3012, best val acc: 0.2685)\n",
      "[Epoch: 0880] train loss: 1.5348, train acc: 0.2370, val loss: 1.5349, val acc: 0.2371  (best train acc: 0.3012, best val acc: 0.2685)\n",
      "[Epoch: 0900] train loss: 1.5341, train acc: 0.2370, val loss: 1.5341, val acc: 0.2371  (best train acc: 0.3012, best val acc: 0.2685)\n",
      "[Epoch: 0920] train loss: 1.5334, train acc: 0.2370, val loss: 1.5334, val acc: 0.2371  (best train acc: 0.3012, best val acc: 0.2685)\n",
      "[Epoch: 0940] train loss: 1.5327, train acc: 0.2370, val loss: 1.5327, val acc: 0.2371  (best train acc: 0.3012, best val acc: 0.2685)\n",
      "[Epoch: 0960] train loss: 1.5320, train acc: 0.2370, val loss: 1.5321, val acc: 0.2371  (best train acc: 0.3012, best val acc: 0.2685)\n",
      "[Epoch: 0980] train loss: 1.5314, train acc: 0.2370, val loss: 1.5315, val acc: 0.2371  (best train acc: 0.3012, best val acc: 0.2685)\n",
      "[Epoch: 1000] train loss: 1.5308, train acc: 0.2370, val loss: 1.5309, val acc: 0.2371  (best train acc: 0.3012, best val acc: 0.2685)\n",
      "[Epoch: 1020] train loss: 1.5302, train acc: 0.2370, val loss: 1.5303, val acc: 0.2371  (best train acc: 0.3012, best val acc: 0.2685)\n",
      "[Epoch: 1040] train loss: 1.5297, train acc: 0.2370, val loss: 1.5297, val acc: 0.2371  (best train acc: 0.3012, best val acc: 0.2685)\n",
      "[Epoch: 1060] train loss: 1.5291, train acc: 0.2370, val loss: 1.5292, val acc: 0.2371  (best train acc: 0.3012, best val acc: 0.2685)\n",
      "[Epoch: 1080] train loss: 1.5286, train acc: 0.2370, val loss: 1.5287, val acc: 0.2371  (best train acc: 0.3012, best val acc: 0.2685)\n",
      "[Epoch: 1100] train loss: 1.5282, train acc: 0.2370, val loss: 1.5282, val acc: 0.2371  (best train acc: 0.3012, best val acc: 0.2685)\n",
      "[Epoch: 1120] train loss: 1.5277, train acc: 0.2370, val loss: 1.5278, val acc: 0.2371  (best train acc: 0.3012, best val acc: 0.2685)\n",
      "[Epoch: 1140] train loss: 1.5272, train acc: 0.2370, val loss: 1.5273, val acc: 0.2371  (best train acc: 0.3012, best val acc: 0.2685)\n",
      "[Epoch: 1160] train loss: 1.5268, train acc: 0.2370, val loss: 1.5269, val acc: 0.2371  (best train acc: 0.3012, best val acc: 0.2685)\n",
      "[Epoch: 1180] train loss: 1.5264, train acc: 0.2370, val loss: 1.5265, val acc: 0.2371  (best train acc: 0.3012, best val acc: 0.2685)\n",
      "[Epoch: 1200] train loss: 1.5260, train acc: 0.2370, val loss: 1.5261, val acc: 0.2371  (best train acc: 0.3012, best val acc: 0.2685)\n",
      "[Epoch: 1220] train loss: 1.5257, train acc: 0.2370, val loss: 1.5257, val acc: 0.2371  (best train acc: 0.3012, best val acc: 0.2685)\n",
      "[Epoch: 1240] train loss: 1.5253, train acc: 0.2370, val loss: 1.5254, val acc: 0.2371  (best train acc: 0.3012, best val acc: 0.2685)\n",
      "[Epoch: 1260] train loss: 1.5249, train acc: 0.2370, val loss: 1.5250, val acc: 0.2371  (best train acc: 0.3012, best val acc: 0.2685)\n",
      "[Epoch: 1280] train loss: 1.5246, train acc: 0.2370, val loss: 1.5247, val acc: 0.2371  (best train acc: 0.3012, best val acc: 0.2685)\n",
      "[Epoch: 1300] train loss: 1.5243, train acc: 0.2370, val loss: 1.5244, val acc: 0.2371  (best train acc: 0.3012, best val acc: 0.2685)\n",
      "[Epoch: 1320] train loss: 1.5240, train acc: 0.2370, val loss: 1.5241, val acc: 0.2371  (best train acc: 0.3012, best val acc: 0.2685)\n",
      "[Epoch: 1340] train loss: 1.5237, train acc: 0.2370, val loss: 1.5238, val acc: 0.2371  (best train acc: 0.3012, best val acc: 0.2685)\n",
      "[Epoch: 1360] train loss: 1.5235, train acc: 0.2370, val loss: 1.5236, val acc: 0.2371  (best train acc: 0.3012, best val acc: 0.2685)\n",
      "[Epoch: 1380] train loss: 1.5232, train acc: 0.2370, val loss: 1.5233, val acc: 0.2371  (best train acc: 0.3012, best val acc: 0.2685)\n",
      "[Epoch: 1400] train loss: 1.5230, train acc: 0.2370, val loss: 1.5231, val acc: 0.2371  (best train acc: 0.3012, best val acc: 0.2685)\n",
      "[Epoch: 1420] train loss: 1.5227, train acc: 0.2370, val loss: 1.5228, val acc: 0.2371  (best train acc: 0.3012, best val acc: 0.2685)\n",
      "[Epoch: 1440] train loss: 1.5225, train acc: 0.2370, val loss: 1.5226, val acc: 0.2371  (best train acc: 0.3012, best val acc: 0.2685)\n",
      "[Epoch: 1460] train loss: 1.5223, train acc: 0.2370, val loss: 1.5224, val acc: 0.2371  (best train acc: 0.3012, best val acc: 0.2685)\n",
      "[Epoch: 1480] train loss: 1.5221, train acc: 0.2370, val loss: 1.5222, val acc: 0.2371  (best train acc: 0.3012, best val acc: 0.2685)\n",
      "[Epoch: 1500] train loss: 1.5219, train acc: 0.2370, val loss: 1.5220, val acc: 0.2371  (best train acc: 0.3012, best val acc: 0.2685)\n",
      "[Epoch: 1520] train loss: 1.5217, train acc: 0.2370, val loss: 1.5218, val acc: 0.2371  (best train acc: 0.3012, best val acc: 0.2685)\n",
      "[Epoch: 1540] train loss: 1.5215, train acc: 0.2370, val loss: 1.5216, val acc: 0.2371  (best train acc: 0.3012, best val acc: 0.2685)\n",
      "[Epoch: 1560] train loss: 1.5213, train acc: 0.2370, val loss: 1.5215, val acc: 0.2371  (best train acc: 0.3012, best val acc: 0.2685)\n",
      "[Epoch: 1580] train loss: 1.5212, train acc: 0.2370, val loss: 1.5213, val acc: 0.2371  (best train acc: 0.3012, best val acc: 0.2685)\n",
      "[Epoch: 1600] train loss: 1.5210, train acc: 0.2370, val loss: 1.5211, val acc: 0.2371  (best train acc: 0.3012, best val acc: 0.2685)\n",
      "[Epoch: 1620] train loss: 1.5209, train acc: 0.2370, val loss: 1.5210, val acc: 0.2371  (best train acc: 0.3012, best val acc: 0.2685)\n",
      "[Epoch: 1640] train loss: 1.5207, train acc: 0.2370, val loss: 1.5209, val acc: 0.2371  (best train acc: 0.3012, best val acc: 0.2685)\n",
      "[Epoch: 1660] train loss: 1.5206, train acc: 0.2370, val loss: 1.5207, val acc: 0.2371  (best train acc: 0.3012, best val acc: 0.2685)\n",
      "[Epoch: 1680] train loss: 1.5205, train acc: 0.2370, val loss: 1.5206, val acc: 0.2371  (best train acc: 0.3012, best val acc: 0.2685)\n",
      "[Epoch: 1700] train loss: 1.5204, train acc: 0.2370, val loss: 1.5205, val acc: 0.2371  (best train acc: 0.3012, best val acc: 0.2685)\n",
      "[Epoch: 1720] train loss: 1.5203, train acc: 0.2370, val loss: 1.5204, val acc: 0.2371  (best train acc: 0.3012, best val acc: 0.2685)\n",
      "[Epoch: 1740] train loss: 1.5201, train acc: 0.2370, val loss: 1.5203, val acc: 0.2371  (best train acc: 0.3012, best val acc: 0.2685)\n",
      "[Epoch: 1760] train loss: 1.5200, train acc: 0.2370, val loss: 1.5202, val acc: 0.2371  (best train acc: 0.3012, best val acc: 0.2685)\n",
      "[Epoch: 1780] train loss: 1.5200, train acc: 0.2370, val loss: 1.5201, val acc: 0.2371  (best train acc: 0.3012, best val acc: 0.2685)\n",
      "[Epoch: 1800] train loss: 1.5199, train acc: 0.2370, val loss: 1.5200, val acc: 0.2371  (best train acc: 0.3012, best val acc: 0.2685)\n",
      "[Epoch: 1820] train loss: 1.5198, train acc: 0.2370, val loss: 1.5199, val acc: 0.2371  (best train acc: 0.3012, best val acc: 0.2685)\n",
      "[Epoch: 1840] train loss: 1.5197, train acc: 0.2370, val loss: 1.5198, val acc: 0.2371  (best train acc: 0.3012, best val acc: 0.2685)\n",
      "[Epoch: 1860] train loss: 1.5196, train acc: 0.2370, val loss: 1.5197, val acc: 0.2371  (best train acc: 0.3012, best val acc: 0.2685)\n",
      "[Epoch: 1880] train loss: 1.5195, train acc: 0.2370, val loss: 1.5197, val acc: 0.2371  (best train acc: 0.3012, best val acc: 0.2685)\n",
      "[Epoch: 1900] train loss: 1.5195, train acc: 0.2370, val loss: 1.5196, val acc: 0.2371  (best train acc: 0.3012, best val acc: 0.2685)\n",
      "[Epoch: 1920] train loss: 1.5194, train acc: 0.2370, val loss: 1.5195, val acc: 0.2371  (best train acc: 0.3012, best val acc: 0.2685)\n",
      "[Epoch: 1940] train loss: 1.5193, train acc: 0.2370, val loss: 1.5195, val acc: 0.2371  (best train acc: 0.3012, best val acc: 0.2685)\n",
      "[Epoch: 1960] train loss: 1.5193, train acc: 0.2370, val loss: 1.5194, val acc: 0.2371  (best train acc: 0.3012, best val acc: 0.2685)\n",
      "[Epoch: 1980] train loss: 1.5192, train acc: 0.2370, val loss: 1.5193, val acc: 0.2371  (best train acc: 0.3012, best val acc: 0.2685)\n",
      "[Epoch: 2000] train loss: 1.5192, train acc: 0.2370, val loss: 1.5193, val acc: 0.2371  (best train acc: 0.3012, best val acc: 0.2685)\n",
      "[Epoch: 2020] train loss: 1.5191, train acc: 0.2370, val loss: 1.5192, val acc: 0.2371  (best train acc: 0.3012, best val acc: 0.2685)\n",
      "[Epoch: 2040] train loss: 1.5191, train acc: 0.2370, val loss: 1.5192, val acc: 0.2371  (best train acc: 0.3012, best val acc: 0.2685)\n",
      "[Epoch: 2060] train loss: 1.5190, train acc: 0.2370, val loss: 1.5192, val acc: 0.2371  (best train acc: 0.3012, best val acc: 0.2685)\n",
      "[Epoch: 2080] train loss: 1.5190, train acc: 0.2370, val loss: 1.5191, val acc: 0.2371  (best train acc: 0.3012, best val acc: 0.2685)\n",
      "[Epoch: 2100] train loss: 1.5189, train acc: 0.2370, val loss: 1.5191, val acc: 0.2371  (best train acc: 0.3012, best val acc: 0.2685)\n",
      "[Epoch: 2120] train loss: 1.5189, train acc: 0.2370, val loss: 1.5190, val acc: 0.2371  (best train acc: 0.3012, best val acc: 0.2685)\n",
      "[Epoch: 2140] train loss: 1.5189, train acc: 0.2370, val loss: 1.5190, val acc: 0.2371  (best train acc: 0.3012, best val acc: 0.2685)\n",
      "[Epoch: 2160] train loss: 1.5188, train acc: 0.2370, val loss: 1.5190, val acc: 0.2371  (best train acc: 0.3012, best val acc: 0.2685)\n",
      "[Epoch: 2180] train loss: 1.5188, train acc: 0.2370, val loss: 1.5189, val acc: 0.2371  (best train acc: 0.3012, best val acc: 0.2685)\n",
      "[Epoch: 2200] train loss: 1.5188, train acc: 0.2370, val loss: 1.5189, val acc: 0.2371  (best train acc: 0.3012, best val acc: 0.2685)\n",
      "[Epoch: 2220] train loss: 1.5187, train acc: 0.2370, val loss: 1.5189, val acc: 0.2371  (best train acc: 0.3012, best val acc: 0.2685)\n",
      "[Epoch: 2240] train loss: 1.5187, train acc: 0.2370, val loss: 1.5189, val acc: 0.2371  (best train acc: 0.3012, best val acc: 0.2685)\n",
      "[Epoch: 2260] train loss: 1.5187, train acc: 0.2370, val loss: 1.5188, val acc: 0.2371  (best train acc: 0.3012, best val acc: 0.2685)\n",
      "[Epoch: 2280] train loss: 1.5187, train acc: 0.2370, val loss: 1.5188, val acc: 0.2371  (best train acc: 0.3012, best val acc: 0.2685)\n",
      "[Epoch: 2300] train loss: 1.5186, train acc: 0.2370, val loss: 1.5188, val acc: 0.2371  (best train acc: 0.3012, best val acc: 0.2685)\n",
      "[Epoch: 2320] train loss: 1.5186, train acc: 0.2370, val loss: 1.5188, val acc: 0.2371  (best train acc: 0.3012, best val acc: 0.2685)\n",
      "[Epoch: 2340] train loss: 1.5186, train acc: 0.2370, val loss: 1.5187, val acc: 0.2371  (best train acc: 0.3012, best val acc: 0.2685)\n",
      "[Epoch: 2360] train loss: 1.5186, train acc: 0.2370, val loss: 1.5187, val acc: 0.2371  (best train acc: 0.3012, best val acc: 0.2685)\n",
      "[Epoch: 2380] train loss: 1.5186, train acc: 0.2370, val loss: 1.5187, val acc: 0.2371  (best train acc: 0.3012, best val acc: 0.2685)\n",
      "[Epoch: 2400] train loss: 1.5185, train acc: 0.2370, val loss: 1.5187, val acc: 0.2371  (best train acc: 0.3012, best val acc: 0.2685)\n",
      "[Epoch: 2420] train loss: 1.5185, train acc: 0.2370, val loss: 1.5187, val acc: 0.2371  (best train acc: 0.3012, best val acc: 0.2685)\n",
      "[Epoch: 2440] train loss: 1.5185, train acc: 0.2370, val loss: 1.5187, val acc: 0.2371  (best train acc: 0.3012, best val acc: 0.2685)\n",
      "[Epoch: 2460] train loss: 1.5185, train acc: 0.2370, val loss: 1.5187, val acc: 0.2371  (best train acc: 0.3012, best val acc: 0.2685)\n",
      "[Epoch: 2480] train loss: 1.5185, train acc: 0.2370, val loss: 1.5186, val acc: 0.2371  (best train acc: 0.3012, best val acc: 0.2685)\n",
      "[Epoch: 2500] train loss: 1.5185, train acc: 0.2370, val loss: 1.5186, val acc: 0.2371  (best train acc: 0.3012, best val acc: 0.2685)\n",
      "[Epoch: 2520] train loss: 1.5185, train acc: 0.2370, val loss: 1.5186, val acc: 0.2371  (best train acc: 0.3012, best val acc: 0.2685)\n",
      "[Epoch: 2540] train loss: 1.5185, train acc: 0.2370, val loss: 1.5186, val acc: 0.2371  (best train acc: 0.3012, best val acc: 0.2685)\n",
      "[Epoch: 2560] train loss: 1.5185, train acc: 0.2370, val loss: 1.5186, val acc: 0.2371  (best train acc: 0.3012, best val acc: 0.2685)\n",
      "[Epoch: 2580] train loss: 1.5184, train acc: 0.2370, val loss: 1.5186, val acc: 0.2371  (best train acc: 0.3012, best val acc: 0.2685)\n",
      "[Epoch: 2600] train loss: 1.5184, train acc: 0.2370, val loss: 1.5186, val acc: 0.2371  (best train acc: 0.3012, best val acc: 0.2685)\n",
      "[Epoch: 2620] train loss: 1.5184, train acc: 0.2370, val loss: 1.5186, val acc: 0.2371  (best train acc: 0.3012, best val acc: 0.2685)\n",
      "[Epoch: 2640] train loss: 1.5184, train acc: 0.2370, val loss: 1.5186, val acc: 0.2371  (best train acc: 0.3012, best val acc: 0.2685)\n",
      "[Epoch: 2660] train loss: 1.5184, train acc: 0.2370, val loss: 1.5186, val acc: 0.2371  (best train acc: 0.3012, best val acc: 0.2685)\n",
      "[Epoch: 2680] train loss: 1.5184, train acc: 0.2370, val loss: 1.5186, val acc: 0.2371  (best train acc: 0.3012, best val acc: 0.2685)\n",
      "[Epoch: 2700] train loss: 1.5184, train acc: 0.2370, val loss: 1.5186, val acc: 0.2371  (best train acc: 0.3012, best val acc: 0.2685)\n",
      "[Epoch: 2720] train loss: 1.5184, train acc: 0.2370, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.3012, best val acc: 0.2685)\n",
      "[Epoch: 2740] train loss: 1.5184, train acc: 0.2370, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.3012, best val acc: 0.2685)\n",
      "[Epoch: 2760] train loss: 1.5184, train acc: 0.2370, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.3012, best val acc: 0.2685)\n",
      "[Epoch: 2780] train loss: 1.5184, train acc: 0.2370, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.3012, best val acc: 0.2685)\n",
      "[Epoch: 2800] train loss: 1.5184, train acc: 0.2370, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.3012, best val acc: 0.2685)\n",
      "[Epoch: 2820] train loss: 1.5184, train acc: 0.2370, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.3012, best val acc: 0.2685)\n",
      "[Epoch: 2840] train loss: 1.5184, train acc: 0.2370, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.3012, best val acc: 0.2685)\n",
      "[Epoch: 2860] train loss: 1.5184, train acc: 0.2370, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.3012, best val acc: 0.2685)\n",
      "[Epoch: 2880] train loss: 1.5184, train acc: 0.2370, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.3012, best val acc: 0.2685)\n",
      "[Epoch: 2900] train loss: 1.5184, train acc: 0.2370, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.3012, best val acc: 0.2685)\n",
      "[Epoch: 2920] train loss: 1.5184, train acc: 0.2370, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.3012, best val acc: 0.2685)\n",
      "[Epoch: 2940] train loss: 1.5184, train acc: 0.2370, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.3012, best val acc: 0.2685)\n",
      "[Epoch: 2960] train loss: 1.4478, train acc: 0.2630, val loss: 1.4494, val acc: 0.2368  (best train acc: 0.3012, best val acc: 0.2685)\n",
      "[Epoch: 2980] train loss: 1.4265, train acc: 0.2595, val loss: 1.4276, val acc: 0.2425  (best train acc: 0.3012, best val acc: 0.2685)\n",
      "[Epoch: 3000] train loss: 1.4072, train acc: 0.2919, val loss: 1.4115, val acc: 0.2536  (best train acc: 0.3012, best val acc: 0.2685)\n",
      "[Epoch: 3020] train loss: 1.4002, train acc: 0.3281, val loss: 1.3992, val acc: 0.3167  (best train acc: 0.3281, best val acc: 0.3167)\n",
      "[Epoch: 3040] train loss: 1.3854, train acc: 0.3385, val loss: 1.3881, val acc: 0.3582  (best train acc: 0.3451, best val acc: 0.3582)\n",
      "[Epoch: 3060] train loss: 1.3754, train acc: 0.3738, val loss: 1.3784, val acc: 0.3663  (best train acc: 0.3874, best val acc: 0.3734)\n",
      "[Epoch: 3080] train loss: 1.3672, train acc: 0.3982, val loss: 1.3726, val acc: 0.4162  (best train acc: 0.4109, best val acc: 0.4411)\n",
      "[Epoch: 3100] train loss: 1.3604, train acc: 0.4138, val loss: 1.3623, val acc: 0.4125  (best train acc: 0.4138, best val acc: 0.4435)\n",
      "[Epoch: 3120] train loss: 1.3527, train acc: 0.4089, val loss: 1.3552, val acc: 0.4293  (best train acc: 0.4138, best val acc: 0.4435)\n",
      "[Epoch: 3140] train loss: 1.3478, train acc: 0.4239, val loss: 1.3484, val acc: 0.4445  (best train acc: 0.4239, best val acc: 0.4469)\n",
      "[Epoch: 3160] train loss: 1.3458, train acc: 0.4274, val loss: 1.3424, val acc: 0.4482  (best train acc: 0.4495, best val acc: 0.4536)\n",
      "[Epoch: 3180] train loss: 1.3395, train acc: 0.4238, val loss: 1.3355, val acc: 0.4462  (best train acc: 0.4495, best val acc: 0.4536)\n",
      "[Epoch: 3200] train loss: 1.3291, train acc: 0.4397, val loss: 1.3299, val acc: 0.4543  (best train acc: 0.4588, best val acc: 0.4583)\n",
      "[Epoch: 3220] train loss: 1.3200, train acc: 0.4547, val loss: 1.3267, val acc: 0.4769  (best train acc: 0.4683, best val acc: 0.4826)\n",
      "[Epoch: 3240] train loss: 1.3165, train acc: 0.4500, val loss: 1.3194, val acc: 0.4614  (best train acc: 0.4697, best val acc: 0.4843)\n",
      "[Epoch: 3260] train loss: 1.3134, train acc: 0.4638, val loss: 1.3137, val acc: 0.4526  (best train acc: 0.4753, best val acc: 0.4877)\n",
      "[Epoch: 3280] train loss: 1.3085, train acc: 0.4613, val loss: 1.3094, val acc: 0.4492  (best train acc: 0.4803, best val acc: 0.4877)\n",
      "[Epoch: 3300] train loss: 1.3064, train acc: 0.4585, val loss: 1.3068, val acc: 0.4917  (best train acc: 0.4873, best val acc: 0.4917)\n",
      "[Epoch: 3320] train loss: 1.3066, train acc: 0.4902, val loss: 1.3001, val acc: 0.4786  (best train acc: 0.4998, best val acc: 0.5012)\n",
      "[Epoch: 3340] train loss: 1.2954, train acc: 0.4689, val loss: 1.2970, val acc: 0.4870  (best train acc: 0.4998, best val acc: 0.5012)\n",
      "[Epoch: 3360] train loss: 1.2893, train acc: 0.4760, val loss: 1.2915, val acc: 0.4624  (best train acc: 0.4998, best val acc: 0.5012)\n",
      "[Epoch: 3380] train loss: 1.2860, train acc: 0.4566, val loss: 1.2873, val acc: 0.4675  (best train acc: 0.4998, best val acc: 0.5012)\n",
      "[Epoch: 3400] train loss: 1.2868, train acc: 0.4977, val loss: 1.2882, val acc: 0.4958  (best train acc: 0.4998, best val acc: 0.5015)\n",
      "[Epoch: 3420] train loss: 1.2807, train acc: 0.4954, val loss: 1.2819, val acc: 0.4597  (best train acc: 0.5004, best val acc: 0.5025)\n",
      "[Epoch: 3440] train loss: 1.2799, train acc: 0.4662, val loss: 1.2779, val acc: 0.4948  (best train acc: 0.5004, best val acc: 0.5025)\n",
      "[Epoch: 3460] train loss: 1.2738, train acc: 0.4853, val loss: 1.2763, val acc: 0.4938  (best train acc: 0.5004, best val acc: 0.5025)\n",
      "[Epoch: 3480] train loss: 1.2795, train acc: 0.4506, val loss: 1.2716, val acc: 0.4931  (best train acc: 0.5004, best val acc: 0.5025)\n",
      "[Epoch: 3500] train loss: 1.2762, train acc: 0.4494, val loss: 1.2725, val acc: 0.4975  (best train acc: 0.5004, best val acc: 0.5025)\n",
      "[Epoch: 3520] train loss: 1.2708, train acc: 0.4717, val loss: 1.2670, val acc: 0.4954  (best train acc: 0.5004, best val acc: 0.5025)\n",
      "[Epoch: 3540] train loss: 1.2653, train acc: 0.4858, val loss: 1.2629, val acc: 0.4978  (best train acc: 0.5004, best val acc: 0.5025)\n",
      "[Epoch: 3560] train loss: 1.2557, train acc: 0.4802, val loss: 1.2621, val acc: 0.5002  (best train acc: 0.5004, best val acc: 0.5032)\n",
      "[Epoch: 3580] train loss: 1.2558, train acc: 0.4991, val loss: 1.2590, val acc: 0.4668  (best train acc: 0.5004, best val acc: 0.5049)\n",
      "[Epoch: 3600] train loss: 1.2697, train acc: 0.4543, val loss: 1.2560, val acc: 0.4965  (best train acc: 0.5004, best val acc: 0.5049)\n",
      "[Epoch: 3620] train loss: 1.2551, train acc: 0.4854, val loss: 1.2562, val acc: 0.4607  (best train acc: 0.5017, best val acc: 0.5049)\n",
      "[Epoch: 3640] train loss: 1.2443, train acc: 0.4947, val loss: 1.2491, val acc: 0.4958  (best train acc: 0.5017, best val acc: 0.5049)\n",
      "[Epoch: 3660] train loss: 1.2421, train acc: 0.5029, val loss: 1.2505, val acc: 0.4661  (best train acc: 0.5029, best val acc: 0.5049)\n",
      "[Epoch: 3680] train loss: 1.2475, train acc: 0.5052, val loss: 1.2442, val acc: 0.4880  (best train acc: 0.5052, best val acc: 0.5059)\n",
      "[Epoch: 3700] train loss: 1.2530, train acc: 0.4608, val loss: 1.2430, val acc: 0.4954  (best train acc: 0.5052, best val acc: 0.5059)\n",
      "[Epoch: 3720] train loss: 1.2436, train acc: 0.4915, val loss: 1.2573, val acc: 0.5029  (best train acc: 0.5052, best val acc: 0.5059)\n",
      "[Epoch: 3740] train loss: 1.2527, train acc: 0.4931, val loss: 1.2386, val acc: 0.4927  (best train acc: 0.5052, best val acc: 0.5059)\n",
      "[Epoch: 3760] train loss: 1.2377, train acc: 0.4751, val loss: 1.2372, val acc: 0.4951  (best train acc: 0.5052, best val acc: 0.5059)\n",
      "[Epoch: 3780] train loss: 1.2447, train acc: 0.4641, val loss: 1.2396, val acc: 0.4988  (best train acc: 0.5052, best val acc: 0.5059)\n",
      "[Epoch: 3800] train loss: 1.2351, train acc: 0.4957, val loss: 1.2379, val acc: 0.5019  (best train acc: 0.5052, best val acc: 0.5062)\n",
      "[Epoch: 3820] train loss: 1.2437, train acc: 0.4757, val loss: 1.2355, val acc: 0.4948  (best train acc: 0.5052, best val acc: 0.5062)\n",
      "[Epoch: 3840] train loss: 1.2392, train acc: 0.4766, val loss: 1.2333, val acc: 0.4981  (best train acc: 0.5052, best val acc: 0.5062)\n",
      "[Epoch: 3860] train loss: 1.2397, train acc: 0.4980, val loss: 1.2322, val acc: 0.4995  (best train acc: 0.5052, best val acc: 0.5062)\n",
      "[Epoch: 3880] train loss: 1.2334, train acc: 0.4765, val loss: 1.2297, val acc: 0.4847  (best train acc: 0.5058, best val acc: 0.5062)\n",
      "[Epoch: 3900] train loss: 1.2282, train acc: 0.5023, val loss: 1.2268, val acc: 0.4941  (best train acc: 0.5058, best val acc: 0.5066)\n",
      "[Epoch: 3920] train loss: 1.2383, train acc: 0.4931, val loss: 1.2262, val acc: 0.4944  (best train acc: 0.5058, best val acc: 0.5066)\n",
      "[Epoch: 3940] train loss: 1.2295, train acc: 0.4860, val loss: 1.2251, val acc: 0.4971  (best train acc: 0.5058, best val acc: 0.5066)\n",
      "[Epoch: 3960] train loss: 1.2338, train acc: 0.4745, val loss: 1.2252, val acc: 0.4992  (best train acc: 0.5058, best val acc: 0.5066)\n",
      "[Epoch: 3980] train loss: 1.2202, train acc: 0.4866, val loss: 1.2258, val acc: 0.5029  (best train acc: 0.5058, best val acc: 0.5066)\n",
      "[Epoch: 4000] train loss: 1.2200, train acc: 0.4840, val loss: 1.2209, val acc: 0.4988  (best train acc: 0.5058, best val acc: 0.5066)\n",
      "[Epoch: 4020] train loss: 1.2337, train acc: 0.4923, val loss: 1.2261, val acc: 0.5062  (best train acc: 0.5085, best val acc: 0.5066)\n",
      "[Epoch: 4040] train loss: 1.2165, train acc: 0.5037, val loss: 1.2195, val acc: 0.4904  (best train acc: 0.5085, best val acc: 0.5066)\n",
      "[Epoch: 4060] train loss: 1.2215, train acc: 0.5026, val loss: 1.2200, val acc: 0.5019  (best train acc: 0.5085, best val acc: 0.5066)\n",
      "[Epoch: 4080] train loss: 1.2258, train acc: 0.4831, val loss: 1.2174, val acc: 0.4901  (best train acc: 0.5089, best val acc: 0.5076)\n",
      "[Epoch: 4100] train loss: 1.2228, train acc: 0.4860, val loss: 1.2299, val acc: 0.5029  (best train acc: 0.5089, best val acc: 0.5076)\n",
      "[Epoch: 4120] train loss: 1.2280, train acc: 0.4748, val loss: 1.2173, val acc: 0.4988  (best train acc: 0.5089, best val acc: 0.5079)\n",
      "[Epoch: 4140] train loss: 1.2179, train acc: 0.5009, val loss: 1.2135, val acc: 0.4938  (best train acc: 0.5089, best val acc: 0.5079)\n",
      "[Epoch: 4160] train loss: 1.2249, train acc: 0.4985, val loss: 1.2133, val acc: 0.4985  (best train acc: 0.5089, best val acc: 0.5079)\n",
      "[Epoch: 4180] train loss: 1.2136, train acc: 0.4982, val loss: 1.2200, val acc: 0.4722  (best train acc: 0.5089, best val acc: 0.5079)\n",
      "[Epoch: 4200] train loss: 1.2149, train acc: 0.4816, val loss: 1.2107, val acc: 0.5012  (best train acc: 0.5089, best val acc: 0.5079)\n",
      "[Epoch: 4220] train loss: 1.2118, train acc: 0.4946, val loss: 1.2174, val acc: 0.5056  (best train acc: 0.5089, best val acc: 0.5089)\n",
      "[Epoch: 4240] train loss: 1.2096, train acc: 0.4931, val loss: 1.2149, val acc: 0.4833  (best train acc: 0.5089, best val acc: 0.5089)\n",
      "[Epoch: 4260] train loss: 1.2190, train acc: 0.4853, val loss: 1.2094, val acc: 0.4985  (best train acc: 0.5089, best val acc: 0.5089)\n",
      "[Epoch: 4280] train loss: 1.2088, train acc: 0.4900, val loss: 1.2117, val acc: 0.4874  (best train acc: 0.5089, best val acc: 0.5089)\n",
      "[Epoch: 4300] train loss: 1.2071, train acc: 0.4973, val loss: 1.2118, val acc: 0.5049  (best train acc: 0.5089, best val acc: 0.5089)\n",
      "[Epoch: 4320] train loss: 1.2063, train acc: 0.4878, val loss: 1.2119, val acc: 0.5062  (best train acc: 0.5089, best val acc: 0.5089)\n",
      "[Epoch: 4340] train loss: 1.2128, train acc: 0.4939, val loss: 1.2067, val acc: 0.4927  (best train acc: 0.5089, best val acc: 0.5089)\n",
      "[Epoch: 4360] train loss: 1.2155, train acc: 0.4885, val loss: 1.2070, val acc: 0.5059  (best train acc: 0.5089, best val acc: 0.5089)\n",
      "[Epoch: 4380] train loss: 1.2192, train acc: 0.4907, val loss: 1.2035, val acc: 0.4978  (best train acc: 0.5089, best val acc: 0.5089)\n",
      "[Epoch: 4400] train loss: 1.2109, train acc: 0.4986, val loss: 1.2061, val acc: 0.4921  (best train acc: 0.5089, best val acc: 0.5089)\n",
      "[Epoch: 4420] train loss: 1.2149, train acc: 0.4913, val loss: 1.2046, val acc: 0.4995  (best train acc: 0.5089, best val acc: 0.5089)\n",
      "[Epoch: 4440] train loss: 1.2082, train acc: 0.4918, val loss: 1.2025, val acc: 0.5005  (best train acc: 0.5089, best val acc: 0.5096)\n",
      "[Epoch: 4460] train loss: 1.2170, train acc: 0.4899, val loss: 1.2035, val acc: 0.4931  (best train acc: 0.5089, best val acc: 0.5096)\n",
      "[Epoch: 4480] train loss: 1.2117, train acc: 0.4948, val loss: 1.2015, val acc: 0.4944  (best train acc: 0.5089, best val acc: 0.5096)\n",
      "[Epoch: 4500] train loss: 1.2051, train acc: 0.4988, val loss: 1.2046, val acc: 0.5029  (best train acc: 0.5089, best val acc: 0.5096)\n",
      "[Epoch: 4520] train loss: 1.2123, train acc: 0.4929, val loss: 1.2035, val acc: 0.4863  (best train acc: 0.5089, best val acc: 0.5096)\n",
      "[Epoch: 4540] train loss: 1.2151, train acc: 0.4890, val loss: 1.2020, val acc: 0.4954  (best train acc: 0.5089, best val acc: 0.5096)\n",
      "[Epoch: 4560] train loss: 1.2175, train acc: 0.4876, val loss: 1.2017, val acc: 0.4938  (best train acc: 0.5089, best val acc: 0.5096)\n",
      "[Epoch: 4580] train loss: 1.2088, train acc: 0.4894, val loss: 1.2032, val acc: 0.4985  (best train acc: 0.5089, best val acc: 0.5096)\n",
      "[Epoch: 4600] train loss: 1.2142, train acc: 0.4920, val loss: 1.2016, val acc: 0.4985  (best train acc: 0.5089, best val acc: 0.5096)\n",
      "[Epoch: 4620] train loss: 1.1977, train acc: 0.4932, val loss: 1.2018, val acc: 0.4985  (best train acc: 0.5089, best val acc: 0.5096)\n",
      "[Epoch: 4640] train loss: 1.2044, train acc: 0.4876, val loss: 1.1992, val acc: 0.4961  (best train acc: 0.5089, best val acc: 0.5096)\n",
      "[Epoch: 4660] train loss: 1.2074, train acc: 0.4887, val loss: 1.1988, val acc: 0.4971  (best train acc: 0.5089, best val acc: 0.5096)\n",
      "[Epoch: 4680] train loss: 1.2042, train acc: 0.4924, val loss: 1.1983, val acc: 0.4971  (best train acc: 0.5089, best val acc: 0.5096)\n",
      "[Epoch: 4700] train loss: 1.1977, train acc: 0.4905, val loss: 1.1990, val acc: 0.4988  (best train acc: 0.5089, best val acc: 0.5096)\n",
      "[Epoch: 4720] train loss: 1.2070, train acc: 0.4993, val loss: 1.1980, val acc: 0.5005  (best train acc: 0.5089, best val acc: 0.5096)\n",
      "[Epoch: 4740] train loss: 1.2043, train acc: 0.4916, val loss: 1.2009, val acc: 0.5002  (best train acc: 0.5089, best val acc: 0.5096)\n",
      "[Epoch: 4760] train loss: 1.2067, train acc: 0.4961, val loss: 1.1964, val acc: 0.4971  (best train acc: 0.5089, best val acc: 0.5096)\n",
      "[Epoch: 4780] train loss: 1.2062, train acc: 0.4869, val loss: 1.2029, val acc: 0.5066  (best train acc: 0.5089, best val acc: 0.5096)\n",
      "[Epoch: 4800] train loss: 1.1911, train acc: 0.4958, val loss: 1.1964, val acc: 0.4901  (best train acc: 0.5089, best val acc: 0.5096)\n",
      "[Epoch: 4820] train loss: 1.1929, train acc: 0.4991, val loss: 1.2037, val acc: 0.5056  (best train acc: 0.5089, best val acc: 0.5096)\n",
      "[Epoch: 4840] train loss: 1.2075, train acc: 0.4902, val loss: 1.1947, val acc: 0.4968  (best train acc: 0.5089, best val acc: 0.5096)\n",
      "[Epoch: 4860] train loss: 1.1973, train acc: 0.5009, val loss: 1.1950, val acc: 0.5012  (best train acc: 0.5089, best val acc: 0.5096)\n",
      "[Epoch: 4880] train loss: 1.1860, train acc: 0.4993, val loss: 1.1960, val acc: 0.4971  (best train acc: 0.5089, best val acc: 0.5096)\n",
      "[Epoch: 4900] train loss: 1.2029, train acc: 0.4939, val loss: 1.1936, val acc: 0.5002  (best train acc: 0.5089, best val acc: 0.5096)\n",
      "[Epoch: 4920] train loss: 1.1988, train acc: 0.4911, val loss: 1.1961, val acc: 0.5015  (best train acc: 0.5089, best val acc: 0.5096)\n",
      "[Epoch: 4940] train loss: 1.2034, train acc: 0.4960, val loss: 1.1944, val acc: 0.4988  (best train acc: 0.5089, best val acc: 0.5096)\n",
      "[Epoch: 4960] train loss: 1.1969, train acc: 0.4918, val loss: 1.1931, val acc: 0.4998  (best train acc: 0.5089, best val acc: 0.5096)\n",
      "[Epoch: 4980] train loss: 1.2037, train acc: 0.4949, val loss: 1.1924, val acc: 0.4921  (best train acc: 0.5089, best val acc: 0.5096)\n",
      "[Epoch: 5000] train loss: 1.2046, train acc: 0.4961, val loss: 1.1952, val acc: 0.5022  (best train acc: 0.5089, best val acc: 0.5096)\n",
      "[Epoch: 5020] train loss: 1.2053, train acc: 0.4889, val loss: 1.1925, val acc: 0.5008  (best train acc: 0.5089, best val acc: 0.5096)\n",
      "[Epoch: 5040] train loss: 1.2045, train acc: 0.4907, val loss: 1.1928, val acc: 0.4988  (best train acc: 0.5089, best val acc: 0.5096)\n",
      "[Epoch: 5060] train loss: 1.2152, train acc: 0.4882, val loss: 1.1908, val acc: 0.5019  (best train acc: 0.5089, best val acc: 0.5096)\n",
      "[Epoch: 5080] train loss: 1.1982, train acc: 0.4995, val loss: 1.1905, val acc: 0.5008  (best train acc: 0.5089, best val acc: 0.5096)\n",
      "[Epoch: 5100] train loss: 1.1943, train acc: 0.4913, val loss: 1.1906, val acc: 0.4995  (best train acc: 0.5089, best val acc: 0.5096)\n",
      "[Epoch: 5120] train loss: 1.1988, train acc: 0.4946, val loss: 1.1947, val acc: 0.5039  (best train acc: 0.5089, best val acc: 0.5096)\n",
      "[Epoch: 5140] train loss: 1.1968, train acc: 0.4883, val loss: 1.1899, val acc: 0.4944  (best train acc: 0.5089, best val acc: 0.5096)\n",
      "[Epoch: 5160] train loss: 1.2052, train acc: 0.4946, val loss: 1.1899, val acc: 0.5012  (best train acc: 0.5089, best val acc: 0.5096)\n",
      "[Epoch: 5180] train loss: 1.1896, train acc: 0.4997, val loss: 1.1926, val acc: 0.5029  (best train acc: 0.5089, best val acc: 0.5096)\n",
      "[Epoch: 5200] train loss: 1.1981, train acc: 0.4975, val loss: 1.1907, val acc: 0.4978  (best train acc: 0.5089, best val acc: 0.5096)\n",
      "[Epoch: 5220] train loss: 1.1980, train acc: 0.4910, val loss: 1.1908, val acc: 0.4897  (best train acc: 0.5089, best val acc: 0.5096)\n",
      "[Epoch: 5240] train loss: 1.1930, train acc: 0.4928, val loss: 1.1904, val acc: 0.4985  (best train acc: 0.5089, best val acc: 0.5096)\n",
      "[Epoch: 5260] train loss: 1.1958, train acc: 0.4886, val loss: 1.1919, val acc: 0.5002  (best train acc: 0.5089, best val acc: 0.5096)\n",
      "[Epoch: 5280] train loss: 1.1983, train acc: 0.4963, val loss: 1.1897, val acc: 0.4992  (best train acc: 0.5089, best val acc: 0.5096)\n",
      "[Epoch: 5300] train loss: 1.2034, train acc: 0.4952, val loss: 1.1908, val acc: 0.4998  (best train acc: 0.5089, best val acc: 0.5096)\n",
      "[Epoch: 5320] train loss: 1.2007, train acc: 0.4843, val loss: 1.1882, val acc: 0.4921  (best train acc: 0.5089, best val acc: 0.5096)\n",
      "[Epoch: 5340] train loss: 1.2020, train acc: 0.4954, val loss: 1.1873, val acc: 0.5002  (best train acc: 0.5089, best val acc: 0.5096)\n",
      "[Epoch: 5360] train loss: 1.2118, train acc: 0.4930, val loss: 1.1908, val acc: 0.5019  (best train acc: 0.5089, best val acc: 0.5096)\n",
      "[Epoch: 5380] train loss: 1.2030, train acc: 0.4956, val loss: 1.1901, val acc: 0.5008  (best train acc: 0.5089, best val acc: 0.5096)\n",
      "[Epoch: 5400] train loss: 1.1906, train acc: 0.4937, val loss: 1.1868, val acc: 0.4985  (best train acc: 0.5089, best val acc: 0.5096)\n",
      "[Epoch: 5420] train loss: 1.1890, train acc: 0.4971, val loss: 1.1865, val acc: 0.5025  (best train acc: 0.5089, best val acc: 0.5096)\n",
      "[Epoch: 5440] train loss: 1.1983, train acc: 0.4999, val loss: 1.1882, val acc: 0.4995  (best train acc: 0.5089, best val acc: 0.5096)\n",
      "[Epoch: 5460] train loss: 1.1902, train acc: 0.4975, val loss: 1.1869, val acc: 0.4998  (best train acc: 0.5089, best val acc: 0.5096)\n",
      "[Epoch: 5480] train loss: 1.1934, train acc: 0.4944, val loss: 1.1924, val acc: 0.5073  (best train acc: 0.5089, best val acc: 0.5096)\n",
      "[Epoch: 5500] train loss: 1.1986, train acc: 0.4911, val loss: 1.1863, val acc: 0.4941  (best train acc: 0.5089, best val acc: 0.5096)\n",
      "[Epoch: 5520] train loss: 1.1983, train acc: 0.5003, val loss: 1.1892, val acc: 0.5008  (best train acc: 0.5089, best val acc: 0.5096)\n",
      "[Epoch: 5540] train loss: 1.2053, train acc: 0.4977, val loss: 1.1853, val acc: 0.4998  (best train acc: 0.5089, best val acc: 0.5096)\n",
      "[Epoch: 5560] train loss: 1.1850, train acc: 0.5004, val loss: 1.1980, val acc: 0.5059  (best train acc: 0.5089, best val acc: 0.5096)\n",
      "[Epoch: 5580] train loss: 1.1935, train acc: 0.4906, val loss: 1.1848, val acc: 0.4992  (best train acc: 0.5089, best val acc: 0.5096)\n",
      "[Epoch: 5600] train loss: 1.2047, train acc: 0.4895, val loss: 1.1847, val acc: 0.5019  (best train acc: 0.5089, best val acc: 0.5096)\n",
      "[Epoch: 5620] train loss: 1.2029, train acc: 0.4965, val loss: 1.1860, val acc: 0.5008  (best train acc: 0.5089, best val acc: 0.5096)\n",
      "[Epoch: 5640] train loss: 1.1995, train acc: 0.4876, val loss: 1.1857, val acc: 0.4927  (best train acc: 0.5089, best val acc: 0.5096)\n",
      "[Epoch: 5660] train loss: 1.2022, train acc: 0.4928, val loss: 1.1857, val acc: 0.4992  (best train acc: 0.5089, best val acc: 0.5096)\n",
      "[Epoch: 5680] train loss: 1.1830, train acc: 0.4947, val loss: 1.1841, val acc: 0.5005  (best train acc: 0.5089, best val acc: 0.5096)\n",
      "[Epoch: 5700] train loss: 1.2079, train acc: 0.4934, val loss: 1.1844, val acc: 0.4998  (best train acc: 0.5089, best val acc: 0.5096)\n",
      "[Epoch: 5720] train loss: 1.1974, train acc: 0.4976, val loss: 1.1857, val acc: 0.4988  (best train acc: 0.5089, best val acc: 0.5096)\n",
      "[Epoch: 5740] train loss: 1.1972, train acc: 0.4991, val loss: 1.1836, val acc: 0.5015  (best train acc: 0.5089, best val acc: 0.5096)\n",
      "[Epoch: 5760] train loss: 1.1952, train acc: 0.4980, val loss: 1.1858, val acc: 0.5005  (best train acc: 0.5089, best val acc: 0.5096)\n",
      "[Epoch: 5780] train loss: 1.1916, train acc: 0.4905, val loss: 1.1833, val acc: 0.5008  (best train acc: 0.5089, best val acc: 0.5096)\n",
      "[Epoch: 5800] train loss: 1.1812, train acc: 0.5004, val loss: 1.1853, val acc: 0.4931  (best train acc: 0.5089, best val acc: 0.5096)\n",
      "[Epoch: 5820] train loss: 1.1942, train acc: 0.4981, val loss: 1.1883, val acc: 0.5049  (best train acc: 0.5089, best val acc: 0.5096)\n",
      "[Epoch: 5840] train loss: 1.1910, train acc: 0.4939, val loss: 1.1914, val acc: 0.5083  (best train acc: 0.5089, best val acc: 0.5096)\n",
      "[Epoch: 5860] train loss: 1.1912, train acc: 0.4903, val loss: 1.1827, val acc: 0.5035  (best train acc: 0.5089, best val acc: 0.5096)\n",
      "[Epoch: 5880] train loss: 1.1913, train acc: 0.4948, val loss: 1.1828, val acc: 0.4998  (best train acc: 0.5089, best val acc: 0.5096)\n",
      "[Epoch: 5900] train loss: 1.1956, train acc: 0.4942, val loss: 1.1829, val acc: 0.4992  (best train acc: 0.5089, best val acc: 0.5096)\n",
      "[Epoch: 5920] train loss: 1.1951, train acc: 0.4977, val loss: 1.1824, val acc: 0.5039  (best train acc: 0.5089, best val acc: 0.5096)\n",
      "[Epoch: 5940] train loss: 1.2098, train acc: 0.4901, val loss: 1.1824, val acc: 0.4992  (best train acc: 0.5089, best val acc: 0.5096)\n",
      "[Epoch: 5960] train loss: 1.1978, train acc: 0.4936, val loss: 1.1864, val acc: 0.5059  (best train acc: 0.5089, best val acc: 0.5096)\n",
      "[Epoch: 5980] train loss: 1.1966, train acc: 0.4935, val loss: 1.1823, val acc: 0.4992  (best train acc: 0.5089, best val acc: 0.5096)\n",
      "[Epoch: 6000] train loss: 1.1896, train acc: 0.4928, val loss: 1.1835, val acc: 0.5019  (best train acc: 0.5089, best val acc: 0.5096)\n",
      "[Epoch: 6020] train loss: 1.1941, train acc: 0.4916, val loss: 1.1824, val acc: 0.4981  (best train acc: 0.5089, best val acc: 0.5096)\n",
      "[Epoch: 6040] train loss: 1.2010, train acc: 0.4962, val loss: 1.1820, val acc: 0.5008  (best train acc: 0.5089, best val acc: 0.5096)\n",
      "[Epoch: 6060] train loss: 1.1889, train acc: 0.5040, val loss: 1.1850, val acc: 0.5046  (best train acc: 0.5089, best val acc: 0.5096)\n",
      "[Epoch: 6080] train loss: 1.1809, train acc: 0.4955, val loss: 1.1816, val acc: 0.5025  (best train acc: 0.5089, best val acc: 0.5096)\n",
      "[Epoch: 6100] train loss: 1.1943, train acc: 0.4944, val loss: 1.1828, val acc: 0.4941  (best train acc: 0.5089, best val acc: 0.5096)\n",
      "[Epoch: 6120] train loss: 1.1999, train acc: 0.4928, val loss: 1.1815, val acc: 0.5005  (best train acc: 0.5089, best val acc: 0.5096)\n",
      "[Epoch: 6140] train loss: 1.1783, train acc: 0.5035, val loss: 1.1822, val acc: 0.5022  (best train acc: 0.5089, best val acc: 0.5096)\n",
      "[Epoch: 6160] train loss: 1.1963, train acc: 0.4973, val loss: 1.1815, val acc: 0.4992  (best train acc: 0.5089, best val acc: 0.5096)\n",
      "[Epoch: 6180] train loss: 1.2011, train acc: 0.4935, val loss: 1.1851, val acc: 0.5056  (best train acc: 0.5089, best val acc: 0.5096)\n",
      "[Epoch: 6200] train loss: 1.1914, train acc: 0.4927, val loss: 1.1837, val acc: 0.5012  (best train acc: 0.5089, best val acc: 0.5096)\n",
      "[Epoch: 6220] train loss: 1.1996, train acc: 0.4897, val loss: 1.1834, val acc: 0.5019  (best train acc: 0.5089, best val acc: 0.5096)\n",
      "[Epoch: 6240] train loss: 1.1924, train acc: 0.4941, val loss: 1.1849, val acc: 0.5049  (best train acc: 0.5089, best val acc: 0.5096)\n",
      "[Epoch: 6260] train loss: 1.1947, train acc: 0.4939, val loss: 1.1810, val acc: 0.5012  (best train acc: 0.5089, best val acc: 0.5096)\n",
      "[Epoch: 6280] train loss: 1.1936, train acc: 0.4960, val loss: 1.1825, val acc: 0.5008  (best train acc: 0.5089, best val acc: 0.5096)\n",
      "[Epoch: 6300] train loss: 1.1997, train acc: 0.4950, val loss: 1.1876, val acc: 0.5076  (best train acc: 0.5089, best val acc: 0.5096)\n",
      "[Epoch: 6320] train loss: 1.1840, train acc: 0.4991, val loss: 1.1815, val acc: 0.5025  (best train acc: 0.5089, best val acc: 0.5096)\n",
      "[Epoch: 6340] train loss: 1.1814, train acc: 0.4978, val loss: 1.1806, val acc: 0.5005  (best train acc: 0.5089, best val acc: 0.5096)\n",
      "[Epoch: 6360] train loss: 1.1974, train acc: 0.4980, val loss: 1.1828, val acc: 0.5019  (best train acc: 0.5089, best val acc: 0.5096)\n",
      "[Epoch: 6380] train loss: 1.1955, train acc: 0.4910, val loss: 1.1815, val acc: 0.4958  (best train acc: 0.5089, best val acc: 0.5096)\n",
      "[Epoch: 6400] train loss: 1.1964, train acc: 0.4973, val loss: 1.1806, val acc: 0.5019  (best train acc: 0.5089, best val acc: 0.5096)\n",
      "[Epoch: 6420] train loss: 1.1892, train acc: 0.5022, val loss: 1.1804, val acc: 0.5002  (best train acc: 0.5089, best val acc: 0.5096)\n",
      "[Epoch: 6440] train loss: 1.1846, train acc: 0.5020, val loss: 1.1803, val acc: 0.5015  (best train acc: 0.5089, best val acc: 0.5096)\n",
      "[Epoch: 6460] train loss: 1.1829, train acc: 0.4980, val loss: 1.1813, val acc: 0.5022  (best train acc: 0.5089, best val acc: 0.5096)\n",
      "[Epoch: 6480] train loss: 1.1867, train acc: 0.4967, val loss: 1.1873, val acc: 0.5059  (best train acc: 0.5089, best val acc: 0.5096)\n",
      "[Epoch: 6500] train loss: 1.2137, train acc: 0.4840, val loss: 1.1847, val acc: 0.5052  (best train acc: 0.5089, best val acc: 0.5096)\n",
      "[Epoch: 6520] train loss: 1.1872, train acc: 0.4963, val loss: 1.1807, val acc: 0.4995  (best train acc: 0.5089, best val acc: 0.5096)\n",
      "[Epoch: 6540] train loss: 1.1899, train acc: 0.4903, val loss: 1.1800, val acc: 0.5008  (best train acc: 0.5089, best val acc: 0.5096)\n",
      "[Epoch: 6560] train loss: 1.1825, train acc: 0.4986, val loss: 1.1800, val acc: 0.5025  (best train acc: 0.5089, best val acc: 0.5096)\n",
      "[Epoch: 6580] train loss: 1.1930, train acc: 0.4947, val loss: 1.1807, val acc: 0.4971  (best train acc: 0.5089, best val acc: 0.5096)\n",
      "[Epoch: 6600] train loss: 1.1853, train acc: 0.4968, val loss: 1.1816, val acc: 0.5012  (best train acc: 0.5089, best val acc: 0.5096)\n",
      "[Epoch: 6620] train loss: 1.1826, train acc: 0.5068, val loss: 1.1811, val acc: 0.5022  (best train acc: 0.5089, best val acc: 0.5096)\n",
      "[Epoch: 6640] train loss: 1.2010, train acc: 0.4926, val loss: 1.1819, val acc: 0.5035  (best train acc: 0.5089, best val acc: 0.5096)\n",
      "[Epoch: 6660] train loss: 1.1896, train acc: 0.4954, val loss: 1.1869, val acc: 0.5079  (best train acc: 0.5089, best val acc: 0.5096)\n",
      "[Epoch: 6680] train loss: 1.1904, train acc: 0.4993, val loss: 1.1836, val acc: 0.5046  (best train acc: 0.5089, best val acc: 0.5096)\n",
      "[Epoch: 6700] train loss: 1.1883, train acc: 0.4915, val loss: 1.1857, val acc: 0.5052  (best train acc: 0.5089, best val acc: 0.5096)\n",
      "[Epoch: 6720] train loss: 1.1930, train acc: 0.4928, val loss: 1.1796, val acc: 0.5029  (best train acc: 0.5089, best val acc: 0.5096)\n",
      "[Epoch: 6740] train loss: 1.1712, train acc: 0.5025, val loss: 1.1795, val acc: 0.5005  (best train acc: 0.5089, best val acc: 0.5096)\n",
      "[Epoch: 6760] train loss: 1.1992, train acc: 0.4987, val loss: 1.1829, val acc: 0.5052  (best train acc: 0.5089, best val acc: 0.5096)\n",
      "[Epoch: 6780] train loss: 1.1878, train acc: 0.4975, val loss: 1.1823, val acc: 0.5042  (best train acc: 0.5089, best val acc: 0.5096)\n",
      "[Epoch: 6800] train loss: 1.1966, train acc: 0.4956, val loss: 1.1794, val acc: 0.4995  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 6820] train loss: 1.2000, train acc: 0.4829, val loss: 1.1801, val acc: 0.5025  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 6840] train loss: 1.1975, train acc: 0.4946, val loss: 1.1799, val acc: 0.5012  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 6860] train loss: 1.1900, train acc: 0.4903, val loss: 1.1809, val acc: 0.5012  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 6880] train loss: 1.1872, train acc: 0.4978, val loss: 1.1795, val acc: 0.4998  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 6900] train loss: 1.2162, train acc: 0.4885, val loss: 1.1797, val acc: 0.5012  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 6920] train loss: 1.1966, train acc: 0.4923, val loss: 1.1790, val acc: 0.5012  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 6940] train loss: 1.1837, train acc: 0.4995, val loss: 1.1797, val acc: 0.4992  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 6960] train loss: 1.2008, train acc: 0.4955, val loss: 1.1790, val acc: 0.5008  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 6980] train loss: 1.1991, train acc: 0.4905, val loss: 1.1818, val acc: 0.5059  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 7000] train loss: 1.1897, train acc: 0.4965, val loss: 1.1812, val acc: 0.4954  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 7020] train loss: 1.1985, train acc: 0.4970, val loss: 1.1802, val acc: 0.5019  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 7040] train loss: 1.1871, train acc: 0.5080, val loss: 1.1900, val acc: 0.5083  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 7060] train loss: 1.1895, train acc: 0.4908, val loss: 1.1788, val acc: 0.5025  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 7080] train loss: 1.1750, train acc: 0.4991, val loss: 1.1831, val acc: 0.5052  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 7100] train loss: 1.2130, train acc: 0.4903, val loss: 1.1809, val acc: 0.4954  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 7120] train loss: 1.2058, train acc: 0.4884, val loss: 1.1787, val acc: 0.5019  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 7140] train loss: 1.1846, train acc: 0.4991, val loss: 1.1926, val acc: 0.5062  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 7160] train loss: 1.1833, train acc: 0.4970, val loss: 1.1793, val acc: 0.5008  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 7180] train loss: 1.2020, train acc: 0.4886, val loss: 1.1813, val acc: 0.5056  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 7200] train loss: 1.1867, train acc: 0.4971, val loss: 1.1800, val acc: 0.5032  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 7220] train loss: 1.1966, train acc: 0.4900, val loss: 1.1786, val acc: 0.5025  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 7240] train loss: 1.1967, train acc: 0.5009, val loss: 1.1804, val acc: 0.5025  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 7260] train loss: 1.1904, train acc: 0.4941, val loss: 1.1795, val acc: 0.4971  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 7280] train loss: 1.2054, train acc: 0.4896, val loss: 1.1833, val acc: 0.5066  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 7300] train loss: 1.1946, train acc: 0.4949, val loss: 1.1805, val acc: 0.5052  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 7320] train loss: 1.1956, train acc: 0.4973, val loss: 1.1908, val acc: 0.5039  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 7340] train loss: 1.1876, train acc: 0.4915, val loss: 1.1842, val acc: 0.4870  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 7360] train loss: 1.1799, train acc: 0.4996, val loss: 1.1850, val acc: 0.5076  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 7380] train loss: 1.1926, train acc: 0.4888, val loss: 1.1785, val acc: 0.5025  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 7400] train loss: 1.1842, train acc: 0.5005, val loss: 1.1787, val acc: 0.5005  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 7420] train loss: 1.1996, train acc: 0.4926, val loss: 1.1791, val acc: 0.5042  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 7440] train loss: 1.2041, train acc: 0.4940, val loss: 1.1783, val acc: 0.5005  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 7460] train loss: 1.1996, train acc: 0.4926, val loss: 1.1783, val acc: 0.5002  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 7480] train loss: 1.2076, train acc: 0.4941, val loss: 1.1790, val acc: 0.5015  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 7500] train loss: 1.2028, train acc: 0.4793, val loss: 1.1797, val acc: 0.4958  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 7520] train loss: 1.1913, train acc: 0.5028, val loss: 1.1809, val acc: 0.5066  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 7540] train loss: 1.1924, train acc: 0.4970, val loss: 1.1802, val acc: 0.5039  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 7560] train loss: 1.1887, train acc: 0.4978, val loss: 1.1790, val acc: 0.5008  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 7580] train loss: 1.1999, train acc: 0.4895, val loss: 1.1782, val acc: 0.5029  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 7600] train loss: 1.1937, train acc: 0.4970, val loss: 1.1915, val acc: 0.5062  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 7620] train loss: 1.1918, train acc: 0.4986, val loss: 1.1813, val acc: 0.5052  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 7640] train loss: 1.1937, train acc: 0.4849, val loss: 1.1790, val acc: 0.5029  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 7660] train loss: 1.1970, train acc: 0.4977, val loss: 1.1794, val acc: 0.5022  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 7680] train loss: 1.1861, train acc: 0.4983, val loss: 1.1781, val acc: 0.5005  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 7700] train loss: 1.1932, train acc: 0.4899, val loss: 1.1781, val acc: 0.5015  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 7720] train loss: 1.1810, train acc: 0.4991, val loss: 1.1797, val acc: 0.5019  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 7740] train loss: 1.1885, train acc: 0.4965, val loss: 1.1806, val acc: 0.5046  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 7760] train loss: 1.1946, train acc: 0.4967, val loss: 1.1805, val acc: 0.4951  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 7780] train loss: 1.1874, train acc: 0.5001, val loss: 1.1797, val acc: 0.5042  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 7800] train loss: 1.1930, train acc: 0.4980, val loss: 1.1783, val acc: 0.4988  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 7820] train loss: 1.1917, train acc: 0.4965, val loss: 1.1837, val acc: 0.5059  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 7840] train loss: 1.2023, train acc: 0.4905, val loss: 1.1779, val acc: 0.5008  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 7860] train loss: 1.1878, train acc: 0.4978, val loss: 1.1780, val acc: 0.4998  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 7880] train loss: 1.1827, train acc: 0.5043, val loss: 1.1799, val acc: 0.5046  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 7900] train loss: 1.1959, train acc: 0.4977, val loss: 1.1836, val acc: 0.5059  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 7920] train loss: 1.1821, train acc: 0.4957, val loss: 1.1794, val acc: 0.5029  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 7940] train loss: 1.2063, train acc: 0.4934, val loss: 1.1808, val acc: 0.5046  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 7960] train loss: 1.1948, train acc: 0.4946, val loss: 1.1784, val acc: 0.4995  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 7980] train loss: 1.1973, train acc: 0.4918, val loss: 1.1793, val acc: 0.4958  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 8000] train loss: 1.1708, train acc: 0.4983, val loss: 1.1778, val acc: 0.5019  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 8020] train loss: 1.2126, train acc: 0.4888, val loss: 1.1778, val acc: 0.5029  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 8040] train loss: 1.1863, train acc: 0.4957, val loss: 1.1784, val acc: 0.4992  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 8060] train loss: 1.1780, train acc: 0.4973, val loss: 1.1781, val acc: 0.5012  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 8080] train loss: 1.1973, train acc: 0.4949, val loss: 1.1794, val acc: 0.5049  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 8100] train loss: 1.2105, train acc: 0.4889, val loss: 1.1822, val acc: 0.5056  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 8120] train loss: 1.1958, train acc: 0.4987, val loss: 1.1779, val acc: 0.4988  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 8140] train loss: 1.1997, train acc: 0.4886, val loss: 1.1821, val acc: 0.5073  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 8160] train loss: 1.1990, train acc: 0.4918, val loss: 1.1792, val acc: 0.5032  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 8180] train loss: 1.1983, train acc: 0.4904, val loss: 1.1787, val acc: 0.5029  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 8200] train loss: 1.1907, train acc: 0.4954, val loss: 1.1790, val acc: 0.5035  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 8220] train loss: 1.1879, train acc: 0.4936, val loss: 1.1798, val acc: 0.5049  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 8240] train loss: 1.1829, train acc: 0.4930, val loss: 1.1779, val acc: 0.5002  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 8260] train loss: 1.1913, train acc: 0.4881, val loss: 1.1813, val acc: 0.4931  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 8280] train loss: 1.1868, train acc: 0.4999, val loss: 1.1915, val acc: 0.5049  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 8300] train loss: 1.1787, train acc: 0.5000, val loss: 1.1791, val acc: 0.5032  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 8320] train loss: 1.1886, train acc: 0.4918, val loss: 1.1784, val acc: 0.5025  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 8340] train loss: 1.1785, train acc: 0.5015, val loss: 1.1784, val acc: 0.4988  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 8360] train loss: 1.1823, train acc: 0.4887, val loss: 1.1789, val acc: 0.5032  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 8380] train loss: 1.2070, train acc: 0.4860, val loss: 1.1781, val acc: 0.5002  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 8400] train loss: 1.1824, train acc: 0.4970, val loss: 1.1817, val acc: 0.5066  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 8420] train loss: 1.1831, train acc: 0.5002, val loss: 1.1814, val acc: 0.5062  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 8440] train loss: 1.2213, train acc: 0.4863, val loss: 1.1822, val acc: 0.5042  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 8460] train loss: 1.1893, train acc: 0.4948, val loss: 1.1782, val acc: 0.5012  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 8480] train loss: 1.1969, train acc: 0.4939, val loss: 1.1777, val acc: 0.5005  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 8500] train loss: 1.2020, train acc: 0.4907, val loss: 1.1777, val acc: 0.4998  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 8520] train loss: 1.1974, train acc: 0.4929, val loss: 1.1839, val acc: 0.5056  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 8540] train loss: 1.1951, train acc: 0.4935, val loss: 1.1777, val acc: 0.5012  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 8560] train loss: 1.1962, train acc: 0.4956, val loss: 1.1835, val acc: 0.5059  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 8580] train loss: 1.1792, train acc: 0.4977, val loss: 1.1778, val acc: 0.4988  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 8600] train loss: 1.1977, train acc: 0.4929, val loss: 1.1813, val acc: 0.5056  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 8620] train loss: 1.1866, train acc: 0.4980, val loss: 1.1825, val acc: 0.5069  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 8640] train loss: 1.1916, train acc: 0.4943, val loss: 1.1782, val acc: 0.5025  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 8660] train loss: 1.2080, train acc: 0.4928, val loss: 1.1778, val acc: 0.5002  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 8680] train loss: 1.1993, train acc: 0.4927, val loss: 1.1828, val acc: 0.5062  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 8700] train loss: 1.2002, train acc: 0.4908, val loss: 1.1777, val acc: 0.5025  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 8720] train loss: 1.1903, train acc: 0.4967, val loss: 1.1785, val acc: 0.5046  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 8740] train loss: 1.1930, train acc: 0.4986, val loss: 1.1819, val acc: 0.5073  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 8760] train loss: 1.1874, train acc: 0.4941, val loss: 1.1776, val acc: 0.5012  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 8780] train loss: 1.1981, train acc: 0.4961, val loss: 1.1780, val acc: 0.5015  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 8800] train loss: 1.1973, train acc: 0.4862, val loss: 1.1776, val acc: 0.5025  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 8820] train loss: 1.1948, train acc: 0.4939, val loss: 1.1776, val acc: 0.5008  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 8840] train loss: 1.1686, train acc: 0.5048, val loss: 1.1780, val acc: 0.4985  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 8860] train loss: 1.1879, train acc: 0.4983, val loss: 1.1794, val acc: 0.5039  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 8880] train loss: 1.1977, train acc: 0.4969, val loss: 1.1785, val acc: 0.5025  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 8900] train loss: 1.1978, train acc: 0.4887, val loss: 1.1776, val acc: 0.5012  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 8920] train loss: 1.1946, train acc: 0.4939, val loss: 1.1817, val acc: 0.5062  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 8940] train loss: 1.1913, train acc: 0.4931, val loss: 1.1786, val acc: 0.4971  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 8960] train loss: 1.1868, train acc: 0.5034, val loss: 1.1798, val acc: 0.5056  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 8980] train loss: 1.1837, train acc: 0.4971, val loss: 1.1792, val acc: 0.5049  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 9000] train loss: 1.2074, train acc: 0.4852, val loss: 1.1776, val acc: 0.5005  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 9020] train loss: 1.1941, train acc: 0.4945, val loss: 1.1776, val acc: 0.5015  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 9040] train loss: 1.2004, train acc: 0.4990, val loss: 1.1870, val acc: 0.5073  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 9060] train loss: 1.1789, train acc: 0.4966, val loss: 1.1781, val acc: 0.5012  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 9080] train loss: 1.1906, train acc: 0.4986, val loss: 1.1785, val acc: 0.5005  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 9100] train loss: 1.1866, train acc: 0.4941, val loss: 1.1824, val acc: 0.5056  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 9120] train loss: 1.2035, train acc: 0.4909, val loss: 1.1778, val acc: 0.5012  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 9140] train loss: 1.1956, train acc: 0.4966, val loss: 1.1780, val acc: 0.5012  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 9160] train loss: 1.1792, train acc: 0.4956, val loss: 1.1819, val acc: 0.5076  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 9180] train loss: 1.1853, train acc: 0.5004, val loss: 1.1799, val acc: 0.4948  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 9200] train loss: 1.2185, train acc: 0.4775, val loss: 1.1920, val acc: 0.5042  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 9220] train loss: 1.1950, train acc: 0.4979, val loss: 1.1843, val acc: 0.5062  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 9240] train loss: 1.1832, train acc: 0.4978, val loss: 1.1775, val acc: 0.5008  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 9260] train loss: 1.1916, train acc: 0.4941, val loss: 1.1782, val acc: 0.5012  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 9280] train loss: 1.2032, train acc: 0.4914, val loss: 1.1796, val acc: 0.5049  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 9300] train loss: 1.1795, train acc: 0.4978, val loss: 1.1800, val acc: 0.5052  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 9320] train loss: 1.2064, train acc: 0.4951, val loss: 1.1802, val acc: 0.5052  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 9340] train loss: 1.1745, train acc: 0.5031, val loss: 1.1775, val acc: 0.5015  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 9360] train loss: 1.1925, train acc: 0.4870, val loss: 1.1776, val acc: 0.5019  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 9380] train loss: 1.1997, train acc: 0.4949, val loss: 1.1793, val acc: 0.5032  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 9400] train loss: 1.1916, train acc: 0.4923, val loss: 1.1779, val acc: 0.5002  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 9420] train loss: 1.1740, train acc: 0.5044, val loss: 1.1909, val acc: 0.5056  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 9440] train loss: 1.1910, train acc: 0.4910, val loss: 1.1778, val acc: 0.4985  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 9460] train loss: 1.1808, train acc: 0.5027, val loss: 1.1777, val acc: 0.5002  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 9480] train loss: 1.1866, train acc: 0.5005, val loss: 1.1806, val acc: 0.5035  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 9500] train loss: 1.1998, train acc: 0.4984, val loss: 1.1777, val acc: 0.5012  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 9520] train loss: 1.1767, train acc: 0.4965, val loss: 1.1781, val acc: 0.4998  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 9540] train loss: 1.1952, train acc: 0.4993, val loss: 1.1821, val acc: 0.5076  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 9560] train loss: 1.1990, train acc: 0.4907, val loss: 1.1776, val acc: 0.5025  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 9580] train loss: 1.1815, train acc: 0.4973, val loss: 1.1790, val acc: 0.5029  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 9600] train loss: 1.1975, train acc: 0.4957, val loss: 1.1823, val acc: 0.5059  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 9620] train loss: 1.1860, train acc: 0.5020, val loss: 1.1794, val acc: 0.5056  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 9640] train loss: 1.1806, train acc: 0.4986, val loss: 1.1792, val acc: 0.5022  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 9660] train loss: 1.1854, train acc: 0.5029, val loss: 1.1810, val acc: 0.5066  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 9680] train loss: 1.1717, train acc: 0.5040, val loss: 1.1781, val acc: 0.5005  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 9700] train loss: 1.1972, train acc: 0.4996, val loss: 1.1799, val acc: 0.5052  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 9720] train loss: 1.1952, train acc: 0.4903, val loss: 1.1783, val acc: 0.5012  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 9740] train loss: 1.1968, train acc: 0.4980, val loss: 1.1814, val acc: 0.5046  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 9760] train loss: 1.1933, train acc: 0.4952, val loss: 1.1795, val acc: 0.5049  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 9780] train loss: 1.1942, train acc: 0.4940, val loss: 1.1886, val acc: 0.5059  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 9800] train loss: 1.1799, train acc: 0.4979, val loss: 1.1778, val acc: 0.5022  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 9820] train loss: 1.1837, train acc: 0.5009, val loss: 1.1835, val acc: 0.5062  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 9840] train loss: 1.1962, train acc: 0.4920, val loss: 1.1812, val acc: 0.4934  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 9860] train loss: 1.1973, train acc: 0.4916, val loss: 1.1778, val acc: 0.5012  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 9880] train loss: 1.1902, train acc: 0.4937, val loss: 1.1783, val acc: 0.5025  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 9900] train loss: 1.1801, train acc: 0.5015, val loss: 1.1785, val acc: 0.5015  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 9920] train loss: 1.1908, train acc: 0.4933, val loss: 1.1827, val acc: 0.5056  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 9940] train loss: 1.1910, train acc: 0.4989, val loss: 1.1793, val acc: 0.5039  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 9960] train loss: 1.1950, train acc: 0.4879, val loss: 1.1783, val acc: 0.4985  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 9980] train loss: 1.2057, train acc: 0.4897, val loss: 1.1793, val acc: 0.4944  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 10000] train loss: 1.1901, train acc: 0.4992, val loss: 1.1775, val acc: 0.5002  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 10020] train loss: 1.1882, train acc: 0.4928, val loss: 1.1777, val acc: 0.5002  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 10040] train loss: 1.2002, train acc: 0.4908, val loss: 1.1775, val acc: 0.4992  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 10060] train loss: 1.2016, train acc: 0.4898, val loss: 1.1806, val acc: 0.5062  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 10080] train loss: 1.1736, train acc: 0.4983, val loss: 1.1776, val acc: 0.5008  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 10100] train loss: 1.2032, train acc: 0.4869, val loss: 1.1783, val acc: 0.5039  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 10120] train loss: 1.2034, train acc: 0.4930, val loss: 1.1781, val acc: 0.5012  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 10140] train loss: 1.2054, train acc: 0.4941, val loss: 1.1809, val acc: 0.5066  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 10160] train loss: 1.1905, train acc: 0.4926, val loss: 1.1779, val acc: 0.4992  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 10180] train loss: 1.2080, train acc: 0.4912, val loss: 1.1779, val acc: 0.4995  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 10200] train loss: 1.1898, train acc: 0.4986, val loss: 1.1777, val acc: 0.5015  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 10220] train loss: 1.1923, train acc: 0.4933, val loss: 1.1785, val acc: 0.5025  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 10240] train loss: 1.1792, train acc: 0.5012, val loss: 1.1786, val acc: 0.5039  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 10260] train loss: 1.1870, train acc: 0.4979, val loss: 1.1779, val acc: 0.4998  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 10280] train loss: 1.1953, train acc: 0.4978, val loss: 1.1821, val acc: 0.5046  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 10300] train loss: 1.1884, train acc: 0.5000, val loss: 1.1811, val acc: 0.5059  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 10320] train loss: 1.1717, train acc: 0.4943, val loss: 1.1776, val acc: 0.5015  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 10340] train loss: 1.1901, train acc: 0.4949, val loss: 1.1847, val acc: 0.5066  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 10360] train loss: 1.1774, train acc: 0.5038, val loss: 1.1802, val acc: 0.5062  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 10380] train loss: 1.2083, train acc: 0.4938, val loss: 1.1793, val acc: 0.5032  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 10400] train loss: 1.1782, train acc: 0.5007, val loss: 1.1806, val acc: 0.5059  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 10420] train loss: 1.1881, train acc: 0.5002, val loss: 1.1795, val acc: 0.5049  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 10440] train loss: 1.1736, train acc: 0.5027, val loss: 1.1775, val acc: 0.4995  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 10460] train loss: 1.1736, train acc: 0.4978, val loss: 1.1776, val acc: 0.5015  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 10480] train loss: 1.2053, train acc: 0.4913, val loss: 1.1800, val acc: 0.4951  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 10500] train loss: 1.2049, train acc: 0.4815, val loss: 1.1775, val acc: 0.5015  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 10520] train loss: 1.1870, train acc: 0.4939, val loss: 1.1818, val acc: 0.5066  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 10540] train loss: 1.2104, train acc: 0.4945, val loss: 1.1877, val acc: 0.5056  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 10560] train loss: 1.1890, train acc: 0.5017, val loss: 1.1800, val acc: 0.5056  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 10580] train loss: 1.1904, train acc: 0.4937, val loss: 1.1775, val acc: 0.4995  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 10600] train loss: 1.1926, train acc: 0.4966, val loss: 1.1777, val acc: 0.5008  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 10620] train loss: 1.1709, train acc: 0.5059, val loss: 1.1797, val acc: 0.5035  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 10640] train loss: 1.2003, train acc: 0.4901, val loss: 1.1781, val acc: 0.5025  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 10660] train loss: 1.1966, train acc: 0.4863, val loss: 1.1792, val acc: 0.5029  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 10680] train loss: 1.1984, train acc: 0.4962, val loss: 1.1802, val acc: 0.5049  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 10700] train loss: 1.1904, train acc: 0.4944, val loss: 1.1783, val acc: 0.5005  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 10720] train loss: 1.2007, train acc: 0.4954, val loss: 1.1809, val acc: 0.5069  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 10740] train loss: 1.2026, train acc: 0.4871, val loss: 1.1785, val acc: 0.5022  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 10760] train loss: 1.2024, train acc: 0.4891, val loss: 1.1777, val acc: 0.4985  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 10780] train loss: 1.1882, train acc: 0.5022, val loss: 1.1899, val acc: 0.5046  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 10800] train loss: 1.1964, train acc: 0.4950, val loss: 1.1778, val acc: 0.5002  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 10820] train loss: 1.1862, train acc: 0.4927, val loss: 1.1789, val acc: 0.4954  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 10840] train loss: 1.2003, train acc: 0.4886, val loss: 1.1782, val acc: 0.5015  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 10860] train loss: 1.2020, train acc: 0.4879, val loss: 1.1786, val acc: 0.4975  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 10880] train loss: 1.1987, train acc: 0.4962, val loss: 1.1777, val acc: 0.5012  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 10900] train loss: 1.1965, train acc: 0.4921, val loss: 1.1797, val acc: 0.4951  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 10920] train loss: 1.1814, train acc: 0.4936, val loss: 1.1785, val acc: 0.5022  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 10940] train loss: 1.1966, train acc: 0.4948, val loss: 1.1779, val acc: 0.4995  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 10960] train loss: 1.2008, train acc: 0.4868, val loss: 1.1778, val acc: 0.5005  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 10980] train loss: 1.1933, train acc: 0.4943, val loss: 1.1775, val acc: 0.5005  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 11000] train loss: 1.2022, train acc: 0.4889, val loss: 1.1790, val acc: 0.5046  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 11020] train loss: 1.1953, train acc: 0.4939, val loss: 1.1798, val acc: 0.5062  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 11040] train loss: 1.1869, train acc: 0.4952, val loss: 1.1776, val acc: 0.5015  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 11060] train loss: 1.2049, train acc: 0.4881, val loss: 1.1780, val acc: 0.5012  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 11080] train loss: 1.1955, train acc: 0.4975, val loss: 1.1780, val acc: 0.4992  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 11100] train loss: 1.2121, train acc: 0.4871, val loss: 1.1781, val acc: 0.5012  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 11120] train loss: 1.1984, train acc: 0.4978, val loss: 1.1795, val acc: 0.5029  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 11140] train loss: 1.2002, train acc: 0.4919, val loss: 1.1776, val acc: 0.5019  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 11160] train loss: 1.1933, train acc: 0.4924, val loss: 1.1782, val acc: 0.5015  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 11180] train loss: 1.2024, train acc: 0.4930, val loss: 1.1792, val acc: 0.5052  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 11200] train loss: 1.1872, train acc: 0.4923, val loss: 1.1810, val acc: 0.5056  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 11220] train loss: 1.1878, train acc: 0.4980, val loss: 1.1830, val acc: 0.5069  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 11240] train loss: 1.1936, train acc: 0.4990, val loss: 1.1791, val acc: 0.5022  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 11260] train loss: 1.1831, train acc: 0.4993, val loss: 1.1815, val acc: 0.5066  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 11280] train loss: 1.1923, train acc: 0.4978, val loss: 1.1796, val acc: 0.5052  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 11300] train loss: 1.1843, train acc: 0.5036, val loss: 1.1830, val acc: 0.5062  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 11320] train loss: 1.2175, train acc: 0.4880, val loss: 1.1779, val acc: 0.5019  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 11340] train loss: 1.1715, train acc: 0.5094, val loss: 1.1791, val acc: 0.4958  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 11360] train loss: 1.5277, train acc: 0.3020, val loss: 1.4828, val acc: 0.3835  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 11380] train loss: 1.1993, train acc: 0.4951, val loss: 1.2033, val acc: 0.4880  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 11400] train loss: 1.1867, train acc: 0.4854, val loss: 1.1825, val acc: 0.4944  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 11420] train loss: 1.1996, train acc: 0.5004, val loss: 1.1837, val acc: 0.5079  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 11440] train loss: 1.1939, train acc: 0.5002, val loss: 1.1829, val acc: 0.5022  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 11460] train loss: 1.1900, train acc: 0.5015, val loss: 1.1857, val acc: 0.5042  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 11480] train loss: 1.1761, train acc: 0.5009, val loss: 1.1843, val acc: 0.5029  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 11500] train loss: 1.1856, train acc: 0.4966, val loss: 1.1827, val acc: 0.5042  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 11520] train loss: 1.1965, train acc: 0.4949, val loss: 1.1826, val acc: 0.5005  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 11540] train loss: 1.1822, train acc: 0.5070, val loss: 1.1832, val acc: 0.4995  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 11560] train loss: 1.1825, train acc: 0.4988, val loss: 1.1825, val acc: 0.5019  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 11580] train loss: 1.1871, train acc: 0.5030, val loss: 1.1826, val acc: 0.5032  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 11600] train loss: 1.1734, train acc: 0.4995, val loss: 1.1822, val acc: 0.5029  (best train acc: 0.5110, best val acc: 0.5096)\n",
      "[Epoch: 11620] train loss: 1.1879, train acc: 0.4986, val loss: 1.1813, val acc: 0.5019  (best train acc: 0.5115, best val acc: 0.5096)\n",
      "[Epoch: 11640] train loss: 1.1754, train acc: 0.5059, val loss: 1.1813, val acc: 0.4988  (best train acc: 0.5115, best val acc: 0.5096)\n",
      "[Epoch: 11660] train loss: 1.1717, train acc: 0.5027, val loss: 1.1822, val acc: 0.5042  (best train acc: 0.5115, best val acc: 0.5096)\n",
      "[Epoch: 11680] train loss: 1.1822, train acc: 0.5027, val loss: 1.1807, val acc: 0.5005  (best train acc: 0.5115, best val acc: 0.5096)\n",
      "[Epoch: 11700] train loss: 1.1849, train acc: 0.4983, val loss: 1.1801, val acc: 0.4985  (best train acc: 0.5115, best val acc: 0.5096)\n",
      "[Epoch: 11720] train loss: 1.1851, train acc: 0.5025, val loss: 1.1815, val acc: 0.4965  (best train acc: 0.5115, best val acc: 0.5096)\n",
      "[Epoch: 11740] train loss: 1.1883, train acc: 0.5027, val loss: 1.1816, val acc: 0.5052  (best train acc: 0.5115, best val acc: 0.5096)\n",
      "[Epoch: 11760] train loss: 1.1899, train acc: 0.4954, val loss: 1.1802, val acc: 0.5008  (best train acc: 0.5115, best val acc: 0.5096)\n",
      "[Epoch: 11780] train loss: 1.1779, train acc: 0.5040, val loss: 1.1808, val acc: 0.5005  (best train acc: 0.5115, best val acc: 0.5096)\n",
      "[Epoch: 11800] train loss: 1.1960, train acc: 0.4939, val loss: 1.1810, val acc: 0.5022  (best train acc: 0.5115, best val acc: 0.5096)\n",
      "[Epoch: 11820] train loss: 1.1854, train acc: 0.4999, val loss: 1.1823, val acc: 0.5073  (best train acc: 0.5115, best val acc: 0.5096)\n",
      "[Epoch: 11840] train loss: 1.1727, train acc: 0.5033, val loss: 1.1827, val acc: 0.5056  (best train acc: 0.5115, best val acc: 0.5096)\n",
      "[Epoch: 11860] train loss: 1.1849, train acc: 0.4995, val loss: 1.1805, val acc: 0.4975  (best train acc: 0.5115, best val acc: 0.5096)\n",
      "[Epoch: 11880] train loss: 1.1727, train acc: 0.5059, val loss: 1.1819, val acc: 0.5062  (best train acc: 0.5115, best val acc: 0.5096)\n",
      "[Epoch: 11900] train loss: 1.1871, train acc: 0.4972, val loss: 1.1796, val acc: 0.5005  (best train acc: 0.5115, best val acc: 0.5096)\n",
      "[Epoch: 11920] train loss: 1.1831, train acc: 0.4954, val loss: 1.1794, val acc: 0.4988  (best train acc: 0.5115, best val acc: 0.5096)\n",
      "[Epoch: 11940] train loss: 1.1893, train acc: 0.4980, val loss: 1.1799, val acc: 0.5005  (best train acc: 0.5115, best val acc: 0.5096)\n",
      "[Epoch: 11960] train loss: 1.1794, train acc: 0.5004, val loss: 1.1799, val acc: 0.5002  (best train acc: 0.5115, best val acc: 0.5096)\n",
      "[Epoch: 11980] train loss: 1.1717, train acc: 0.5020, val loss: 1.1794, val acc: 0.5019  (best train acc: 0.5115, best val acc: 0.5096)\n",
      "[Epoch: 12000] train loss: 1.1777, train acc: 0.4969, val loss: 1.1795, val acc: 0.4998  (best train acc: 0.5115, best val acc: 0.5096)\n",
      "[Epoch: 12020] train loss: 1.1811, train acc: 0.5023, val loss: 1.1795, val acc: 0.4995  (best train acc: 0.5115, best val acc: 0.5096)\n",
      "[Epoch: 12040] train loss: 1.1741, train acc: 0.4986, val loss: 1.1801, val acc: 0.4978  (best train acc: 0.5115, best val acc: 0.5096)\n",
      "[Epoch: 12060] train loss: 1.1944, train acc: 0.4995, val loss: 1.1800, val acc: 0.5008  (best train acc: 0.5115, best val acc: 0.5096)\n",
      "[Epoch: 12080] train loss: 1.1854, train acc: 0.5003, val loss: 1.1802, val acc: 0.5052  (best train acc: 0.5115, best val acc: 0.5096)\n",
      "[Epoch: 12100] train loss: 1.1754, train acc: 0.5058, val loss: 1.1788, val acc: 0.5002  (best train acc: 0.5115, best val acc: 0.5096)\n",
      "[Epoch: 12120] train loss: 1.2049, train acc: 0.4905, val loss: 1.1835, val acc: 0.4917  (best train acc: 0.5115, best val acc: 0.5096)\n",
      "[Epoch: 12140] train loss: 1.1892, train acc: 0.4897, val loss: 1.1789, val acc: 0.4998  (best train acc: 0.5115, best val acc: 0.5096)\n",
      "[Epoch: 12160] train loss: 1.1844, train acc: 0.4935, val loss: 1.1798, val acc: 0.5005  (best train acc: 0.5115, best val acc: 0.5096)\n",
      "[Epoch: 12180] train loss: 1.1826, train acc: 0.5032, val loss: 1.1787, val acc: 0.5005  (best train acc: 0.5115, best val acc: 0.5096)\n",
      "[Epoch: 12200] train loss: 1.1865, train acc: 0.4965, val loss: 1.1782, val acc: 0.5005  (best train acc: 0.5115, best val acc: 0.5096)\n",
      "[Epoch: 12220] train loss: 1.1878, train acc: 0.5028, val loss: 1.1815, val acc: 0.5056  (best train acc: 0.5115, best val acc: 0.5096)\n",
      "[Epoch: 12240] train loss: 1.1876, train acc: 0.5000, val loss: 1.1783, val acc: 0.5002  (best train acc: 0.5115, best val acc: 0.5096)\n",
      "[Epoch: 12260] train loss: 1.1725, train acc: 0.5030, val loss: 1.1789, val acc: 0.4981  (best train acc: 0.5115, best val acc: 0.5096)\n",
      "[Epoch: 12280] train loss: 1.1914, train acc: 0.4963, val loss: 1.1789, val acc: 0.5002  (best train acc: 0.5115, best val acc: 0.5096)\n",
      "[Epoch: 12300] train loss: 1.1689, train acc: 0.5045, val loss: 1.1789, val acc: 0.5008  (best train acc: 0.5115, best val acc: 0.5096)\n",
      "[Epoch: 12320] train loss: 1.1800, train acc: 0.5010, val loss: 1.1811, val acc: 0.5079  (best train acc: 0.5115, best val acc: 0.5096)\n",
      "[Epoch: 12340] train loss: 1.1905, train acc: 0.4936, val loss: 1.1781, val acc: 0.4995  (best train acc: 0.5115, best val acc: 0.5096)\n",
      "[Epoch: 12360] train loss: 1.1828, train acc: 0.5038, val loss: 1.1798, val acc: 0.5042  (best train acc: 0.5115, best val acc: 0.5096)\n",
      "[Epoch: 12380] train loss: 1.1821, train acc: 0.4952, val loss: 1.1780, val acc: 0.5012  (best train acc: 0.5115, best val acc: 0.5096)\n",
      "[Epoch: 12400] train loss: 1.1812, train acc: 0.5008, val loss: 1.1791, val acc: 0.5046  (best train acc: 0.5115, best val acc: 0.5096)\n",
      "[Epoch: 12420] train loss: 1.1925, train acc: 0.4938, val loss: 1.1786, val acc: 0.5002  (best train acc: 0.5115, best val acc: 0.5096)\n",
      "[Epoch: 12440] train loss: 1.1831, train acc: 0.4962, val loss: 1.1786, val acc: 0.5025  (best train acc: 0.5115, best val acc: 0.5096)\n",
      "[Epoch: 12460] train loss: 1.1757, train acc: 0.5046, val loss: 1.1792, val acc: 0.5019  (best train acc: 0.5115, best val acc: 0.5096)\n",
      "[Epoch: 12480] train loss: 1.1850, train acc: 0.4978, val loss: 1.1784, val acc: 0.5042  (best train acc: 0.5115, best val acc: 0.5096)\n",
      "[Epoch: 12500] train loss: 1.1617, train acc: 0.5093, val loss: 1.1805, val acc: 0.5056  (best train acc: 0.5115, best val acc: 0.5096)\n",
      "[Epoch: 12520] train loss: 1.1793, train acc: 0.4964, val loss: 1.1776, val acc: 0.5019  (best train acc: 0.5115, best val acc: 0.5096)\n",
      "[Epoch: 12540] train loss: 1.1762, train acc: 0.5052, val loss: 1.1783, val acc: 0.5022  (best train acc: 0.5115, best val acc: 0.5096)\n",
      "[Epoch: 12560] train loss: 1.1750, train acc: 0.5022, val loss: 1.1775, val acc: 0.4998  (best train acc: 0.5115, best val acc: 0.5096)\n",
      "[Epoch: 12580] train loss: 1.1725, train acc: 0.5024, val loss: 1.1787, val acc: 0.5025  (best train acc: 0.5115, best val acc: 0.5096)\n",
      "[Epoch: 12600] train loss: 1.1732, train acc: 0.5046, val loss: 1.1805, val acc: 0.5059  (best train acc: 0.5115, best val acc: 0.5096)\n",
      "[Epoch: 12620] train loss: 1.1833, train acc: 0.4986, val loss: 1.1778, val acc: 0.4975  (best train acc: 0.5115, best val acc: 0.5096)\n",
      "[Epoch: 12640] train loss: 1.1816, train acc: 0.4980, val loss: 1.1788, val acc: 0.5012  (best train acc: 0.5115, best val acc: 0.5096)\n",
      "[Epoch: 12660] train loss: 1.1841, train acc: 0.4993, val loss: 1.1780, val acc: 0.4988  (best train acc: 0.5115, best val acc: 0.5096)\n",
      "[Epoch: 12680] train loss: 1.1681, train acc: 0.5038, val loss: 1.1778, val acc: 0.5012  (best train acc: 0.5115, best val acc: 0.5096)\n",
      "[Epoch: 12700] train loss: 1.1918, train acc: 0.4931, val loss: 1.1784, val acc: 0.4944  (best train acc: 0.5115, best val acc: 0.5096)\n",
      "[Epoch: 12720] train loss: 1.1951, train acc: 0.4952, val loss: 1.1795, val acc: 0.5035  (best train acc: 0.5115, best val acc: 0.5096)\n",
      "[Epoch: 12740] train loss: 1.1722, train acc: 0.5009, val loss: 1.1809, val acc: 0.5042  (best train acc: 0.5115, best val acc: 0.5096)\n",
      "[Epoch: 12760] train loss: 1.1833, train acc: 0.4915, val loss: 1.1775, val acc: 0.4985  (best train acc: 0.5115, best val acc: 0.5096)\n",
      "[Epoch: 12780] train loss: 1.1852, train acc: 0.5003, val loss: 1.1816, val acc: 0.5019  (best train acc: 0.5115, best val acc: 0.5096)\n",
      "[Epoch: 12800] train loss: 1.1835, train acc: 0.4928, val loss: 1.1779, val acc: 0.5002  (best train acc: 0.5115, best val acc: 0.5096)\n",
      "[Epoch: 12820] train loss: 1.1800, train acc: 0.5054, val loss: 1.1789, val acc: 0.4961  (best train acc: 0.5115, best val acc: 0.5096)\n",
      "[Epoch: 12840] train loss: 1.1842, train acc: 0.4946, val loss: 1.1835, val acc: 0.4998  (best train acc: 0.5115, best val acc: 0.5096)\n",
      "[Epoch: 12860] train loss: 1.1754, train acc: 0.5017, val loss: 1.1777, val acc: 0.4985  (best train acc: 0.5115, best val acc: 0.5096)\n",
      "[Epoch: 12880] train loss: 1.1790, train acc: 0.5037, val loss: 1.1775, val acc: 0.5005  (best train acc: 0.5115, best val acc: 0.5096)\n",
      "[Epoch: 12900] train loss: 1.1902, train acc: 0.4941, val loss: 1.1784, val acc: 0.5039  (best train acc: 0.5115, best val acc: 0.5096)\n",
      "[Epoch: 12920] train loss: 1.1794, train acc: 0.4990, val loss: 1.1772, val acc: 0.4992  (best train acc: 0.5115, best val acc: 0.5096)\n",
      "[Epoch: 12940] train loss: 1.1879, train acc: 0.5015, val loss: 1.1787, val acc: 0.5032  (best train acc: 0.5115, best val acc: 0.5096)\n",
      "[Epoch: 12960] train loss: 1.1894, train acc: 0.5013, val loss: 1.1797, val acc: 0.5032  (best train acc: 0.5134, best val acc: 0.5096)\n",
      "[Epoch: 12980] train loss: 1.1658, train acc: 0.5027, val loss: 1.1777, val acc: 0.4992  (best train acc: 0.5134, best val acc: 0.5096)\n",
      "[Epoch: 13000] train loss: 1.1895, train acc: 0.4938, val loss: 1.1781, val acc: 0.5008  (best train acc: 0.5134, best val acc: 0.5096)\n",
      "[Epoch: 13020] train loss: 1.1728, train acc: 0.5008, val loss: 1.1781, val acc: 0.5019  (best train acc: 0.5134, best val acc: 0.5096)\n",
      "[Epoch: 13040] train loss: 1.1958, train acc: 0.4894, val loss: 1.1779, val acc: 0.5025  (best train acc: 0.5134, best val acc: 0.5096)\n",
      "[Epoch: 13060] train loss: 1.1776, train acc: 0.5034, val loss: 1.1781, val acc: 0.4998  (best train acc: 0.5134, best val acc: 0.5096)\n",
      "[Epoch: 13080] train loss: 1.1784, train acc: 0.4967, val loss: 1.1783, val acc: 0.5008  (best train acc: 0.5134, best val acc: 0.5096)\n",
      "[Epoch: 13100] train loss: 1.1897, train acc: 0.4912, val loss: 1.1780, val acc: 0.4988  (best train acc: 0.5134, best val acc: 0.5096)\n",
      "[Epoch: 13120] train loss: 1.1938, train acc: 0.5001, val loss: 1.1786, val acc: 0.5042  (best train acc: 0.5134, best val acc: 0.5096)\n",
      "[Epoch: 13140] train loss: 1.2006, train acc: 0.4902, val loss: 1.1784, val acc: 0.5025  (best train acc: 0.5134, best val acc: 0.5096)\n",
      "[Epoch: 13160] train loss: 1.1884, train acc: 0.4973, val loss: 1.1772, val acc: 0.4992  (best train acc: 0.5134, best val acc: 0.5096)\n",
      "[Epoch: 13180] train loss: 1.1917, train acc: 0.4941, val loss: 1.1774, val acc: 0.4981  (best train acc: 0.5134, best val acc: 0.5096)\n",
      "[Epoch: 13200] train loss: 1.1805, train acc: 0.5013, val loss: 1.1773, val acc: 0.4995  (best train acc: 0.5134, best val acc: 0.5096)\n",
      "[Epoch: 13220] train loss: 1.1671, train acc: 0.5067, val loss: 1.1782, val acc: 0.5008  (best train acc: 0.5134, best val acc: 0.5096)\n",
      "[Epoch: 13240] train loss: 1.1870, train acc: 0.4913, val loss: 1.1770, val acc: 0.5008  (best train acc: 0.5134, best val acc: 0.5096)\n",
      "[Epoch: 13260] train loss: 1.1706, train acc: 0.5049, val loss: 1.1787, val acc: 0.5015  (best train acc: 0.5134, best val acc: 0.5096)\n",
      "[Epoch: 13280] train loss: 1.1803, train acc: 0.4978, val loss: 1.1789, val acc: 0.5032  (best train acc: 0.5134, best val acc: 0.5096)\n",
      "[Epoch: 13300] train loss: 1.1859, train acc: 0.4977, val loss: 1.1772, val acc: 0.5005  (best train acc: 0.5134, best val acc: 0.5096)\n",
      "[Epoch: 13320] train loss: 1.1709, train acc: 0.5018, val loss: 1.1771, val acc: 0.5005  (best train acc: 0.5134, best val acc: 0.5096)\n",
      "[Epoch: 13340] train loss: 1.1865, train acc: 0.4960, val loss: 1.1765, val acc: 0.5005  (best train acc: 0.5134, best val acc: 0.5096)\n",
      "[Epoch: 13360] train loss: 1.1891, train acc: 0.4988, val loss: 1.1772, val acc: 0.5025  (best train acc: 0.5134, best val acc: 0.5096)\n",
      "[Epoch: 13380] train loss: 1.1843, train acc: 0.4982, val loss: 1.1791, val acc: 0.5019  (best train acc: 0.5134, best val acc: 0.5096)\n",
      "[Epoch: 13400] train loss: 1.1760, train acc: 0.4939, val loss: 1.1780, val acc: 0.4971  (best train acc: 0.5134, best val acc: 0.5096)\n",
      "[Epoch: 13420] train loss: 1.1775, train acc: 0.5083, val loss: 1.1777, val acc: 0.5046  (best train acc: 0.5134, best val acc: 0.5096)\n",
      "[Epoch: 13440] train loss: 1.1908, train acc: 0.4985, val loss: 1.1821, val acc: 0.5069  (best train acc: 0.5134, best val acc: 0.5096)\n",
      "[Epoch: 13460] train loss: 1.1938, train acc: 0.4928, val loss: 1.1797, val acc: 0.5029  (best train acc: 0.5134, best val acc: 0.5096)\n",
      "[Epoch: 13480] train loss: 1.1710, train acc: 0.5064, val loss: 1.1792, val acc: 0.4934  (best train acc: 0.5134, best val acc: 0.5096)\n",
      "[Epoch: 13500] train loss: 1.1869, train acc: 0.4993, val loss: 1.1789, val acc: 0.5062  (best train acc: 0.5134, best val acc: 0.5096)\n",
      "[Epoch: 13520] train loss: 1.1849, train acc: 0.4977, val loss: 1.1785, val acc: 0.5022  (best train acc: 0.5134, best val acc: 0.5096)\n",
      "[Epoch: 13540] train loss: 1.1957, train acc: 0.4925, val loss: 1.1789, val acc: 0.5039  (best train acc: 0.5134, best val acc: 0.5096)\n",
      "[Epoch: 13560] train loss: 1.1896, train acc: 0.4923, val loss: 1.1774, val acc: 0.4985  (best train acc: 0.5134, best val acc: 0.5096)\n",
      "[Epoch: 13580] train loss: 1.1907, train acc: 0.4957, val loss: 1.1786, val acc: 0.5059  (best train acc: 0.5134, best val acc: 0.5096)\n",
      "[Epoch: 13600] train loss: 1.1827, train acc: 0.4960, val loss: 1.1775, val acc: 0.5005  (best train acc: 0.5134, best val acc: 0.5096)\n",
      "[Epoch: 13620] train loss: 1.1793, train acc: 0.5009, val loss: 1.1779, val acc: 0.4975  (best train acc: 0.5134, best val acc: 0.5096)\n",
      "[Epoch: 13640] train loss: 1.1841, train acc: 0.4962, val loss: 1.1768, val acc: 0.4995  (best train acc: 0.5134, best val acc: 0.5096)\n",
      "[Epoch: 13660] train loss: 1.1901, train acc: 0.4944, val loss: 1.1782, val acc: 0.4951  (best train acc: 0.5134, best val acc: 0.5096)\n",
      "[Epoch: 13680] train loss: 1.1770, train acc: 0.5017, val loss: 1.1783, val acc: 0.5046  (best train acc: 0.5134, best val acc: 0.5096)\n",
      "[Epoch: 13700] train loss: 1.1714, train acc: 0.5061, val loss: 1.1807, val acc: 0.5052  (best train acc: 0.5134, best val acc: 0.5096)\n",
      "[Epoch: 13720] train loss: 1.1827, train acc: 0.4957, val loss: 1.1784, val acc: 0.4968  (best train acc: 0.5134, best val acc: 0.5096)\n",
      "[Epoch: 13740] train loss: 1.1758, train acc: 0.5007, val loss: 1.1777, val acc: 0.4995  (best train acc: 0.5134, best val acc: 0.5096)\n",
      "[Epoch: 13760] train loss: 1.1807, train acc: 0.5001, val loss: 1.1841, val acc: 0.5019  (best train acc: 0.5134, best val acc: 0.5096)\n",
      "[Epoch: 13780] train loss: 1.1734, train acc: 0.5008, val loss: 1.1779, val acc: 0.4958  (best train acc: 0.5134, best val acc: 0.5096)\n",
      "[Epoch: 13800] train loss: 1.1885, train acc: 0.4942, val loss: 1.1791, val acc: 0.5059  (best train acc: 0.5134, best val acc: 0.5096)\n",
      "[Epoch: 13820] train loss: 1.2005, train acc: 0.4967, val loss: 1.1775, val acc: 0.5022  (best train acc: 0.5134, best val acc: 0.5096)\n",
      "[Epoch: 13840] train loss: 1.1902, train acc: 0.4986, val loss: 1.1786, val acc: 0.5049  (best train acc: 0.5134, best val acc: 0.5096)\n",
      "[Epoch: 13860] train loss: 1.1700, train acc: 0.5020, val loss: 1.1779, val acc: 0.5025  (best train acc: 0.5134, best val acc: 0.5096)\n",
      "[Epoch: 13880] train loss: 1.1843, train acc: 0.4997, val loss: 1.1803, val acc: 0.5052  (best train acc: 0.5134, best val acc: 0.5096)\n",
      "[Epoch: 13900] train loss: 1.1767, train acc: 0.4980, val loss: 1.1781, val acc: 0.4981  (best train acc: 0.5134, best val acc: 0.5096)\n",
      "[Epoch: 13920] train loss: 1.1856, train acc: 0.5004, val loss: 1.1799, val acc: 0.5042  (best train acc: 0.5134, best val acc: 0.5096)\n",
      "[Epoch: 13940] train loss: 1.1796, train acc: 0.5004, val loss: 1.1770, val acc: 0.4992  (best train acc: 0.5134, best val acc: 0.5096)\n",
      "[Epoch: 13960] train loss: 1.1710, train acc: 0.5046, val loss: 1.1773, val acc: 0.5002  (best train acc: 0.5134, best val acc: 0.5096)\n",
      "[Epoch: 13980] train loss: 1.1812, train acc: 0.5029, val loss: 1.1771, val acc: 0.5008  (best train acc: 0.5134, best val acc: 0.5096)\n",
      "[Epoch: 14000] train loss: 1.1903, train acc: 0.4960, val loss: 1.1769, val acc: 0.4992  (best train acc: 0.5134, best val acc: 0.5096)\n",
      "[Epoch: 14020] train loss: 1.1810, train acc: 0.4979, val loss: 1.1805, val acc: 0.5069  (best train acc: 0.5134, best val acc: 0.5096)\n",
      "[Epoch: 14040] train loss: 1.1795, train acc: 0.4970, val loss: 1.1771, val acc: 0.4998  (best train acc: 0.5134, best val acc: 0.5096)\n",
      "[Epoch: 14060] train loss: 1.1817, train acc: 0.4991, val loss: 1.1781, val acc: 0.5025  (best train acc: 0.5134, best val acc: 0.5096)\n",
      "[Epoch: 14080] train loss: 1.1899, train acc: 0.4939, val loss: 1.1773, val acc: 0.5005  (best train acc: 0.5134, best val acc: 0.5096)\n",
      "[Epoch: 14100] train loss: 1.1796, train acc: 0.4968, val loss: 1.1784, val acc: 0.5022  (best train acc: 0.5134, best val acc: 0.5096)\n",
      "[Epoch: 14120] train loss: 1.1782, train acc: 0.5030, val loss: 1.1780, val acc: 0.5032  (best train acc: 0.5134, best val acc: 0.5096)\n",
      "[Epoch: 14140] train loss: 1.1739, train acc: 0.5044, val loss: 1.1784, val acc: 0.5015  (best train acc: 0.5134, best val acc: 0.5096)\n",
      "[Epoch: 14160] train loss: 1.1790, train acc: 0.5014, val loss: 1.1779, val acc: 0.5035  (best train acc: 0.5134, best val acc: 0.5096)\n",
      "[Epoch: 14180] train loss: 1.1864, train acc: 0.4925, val loss: 1.1776, val acc: 0.5005  (best train acc: 0.5134, best val acc: 0.5096)\n",
      "[Epoch: 14200] train loss: 1.1972, train acc: 0.4966, val loss: 1.1778, val acc: 0.5046  (best train acc: 0.5134, best val acc: 0.5096)\n",
      "[Epoch: 14220] train loss: 1.1772, train acc: 0.4999, val loss: 1.1773, val acc: 0.4992  (best train acc: 0.5134, best val acc: 0.5096)\n",
      "[Epoch: 14240] train loss: 1.1844, train acc: 0.4917, val loss: 1.1770, val acc: 0.4985  (best train acc: 0.5134, best val acc: 0.5096)\n",
      "[Epoch: 14260] train loss: 1.1772, train acc: 0.5037, val loss: 1.1795, val acc: 0.5042  (best train acc: 0.5134, best val acc: 0.5096)\n",
      "[Epoch: 14280] train loss: 1.1721, train acc: 0.5021, val loss: 1.1776, val acc: 0.4965  (best train acc: 0.5134, best val acc: 0.5096)\n",
      "[Epoch: 14300] train loss: 1.1853, train acc: 0.4993, val loss: 1.1782, val acc: 0.5042  (best train acc: 0.5134, best val acc: 0.5096)\n",
      "[Epoch: 14320] train loss: 1.1737, train acc: 0.5004, val loss: 1.1781, val acc: 0.5015  (best train acc: 0.5134, best val acc: 0.5096)\n",
      "[Epoch: 14340] train loss: 1.1831, train acc: 0.5009, val loss: 1.1794, val acc: 0.5046  (best train acc: 0.5134, best val acc: 0.5096)\n",
      "[Epoch: 14360] train loss: 1.1831, train acc: 0.4949, val loss: 1.1776, val acc: 0.4965  (best train acc: 0.5134, best val acc: 0.5096)\n",
      "[Epoch: 14380] train loss: 1.1877, train acc: 0.5020, val loss: 1.1774, val acc: 0.4992  (best train acc: 0.5134, best val acc: 0.5096)\n",
      "[Epoch: 14400] train loss: 1.1743, train acc: 0.5030, val loss: 1.1774, val acc: 0.4981  (best train acc: 0.5134, best val acc: 0.5096)\n",
      "[Epoch: 14420] train loss: 1.1813, train acc: 0.5036, val loss: 1.1789, val acc: 0.5052  (best train acc: 0.5134, best val acc: 0.5096)\n",
      "[Epoch: 14440] train loss: 1.1958, train acc: 0.4957, val loss: 1.1775, val acc: 0.5012  (best train acc: 0.5134, best val acc: 0.5096)\n",
      "[Epoch: 14460] train loss: 1.1803, train acc: 0.4984, val loss: 1.1773, val acc: 0.4992  (best train acc: 0.5134, best val acc: 0.5096)\n",
      "[Epoch: 14480] train loss: 1.1861, train acc: 0.4993, val loss: 1.1778, val acc: 0.5029  (best train acc: 0.5134, best val acc: 0.5096)\n",
      "[Epoch: 14500] train loss: 1.1761, train acc: 0.5012, val loss: 1.1797, val acc: 0.5032  (best train acc: 0.5134, best val acc: 0.5096)\n",
      "[Epoch: 14520] train loss: 1.1865, train acc: 0.4957, val loss: 1.1770, val acc: 0.4971  (best train acc: 0.5134, best val acc: 0.5096)\n",
      "[Epoch: 14540] train loss: 1.1858, train acc: 0.5012, val loss: 1.1789, val acc: 0.5046  (best train acc: 0.5134, best val acc: 0.5096)\n",
      "[Epoch: 14560] train loss: 1.1858, train acc: 0.4924, val loss: 1.1768, val acc: 0.5012  (best train acc: 0.5134, best val acc: 0.5096)\n",
      "[Epoch: 14580] train loss: 1.1792, train acc: 0.4977, val loss: 1.1768, val acc: 0.4985  (best train acc: 0.5134, best val acc: 0.5096)\n",
      "[Epoch: 14600] train loss: 1.1808, train acc: 0.5016, val loss: 1.1770, val acc: 0.4988  (best train acc: 0.5134, best val acc: 0.5096)\n",
      "[Epoch: 14620] train loss: 1.1776, train acc: 0.5009, val loss: 1.1782, val acc: 0.5019  (best train acc: 0.5134, best val acc: 0.5096)\n",
      "[Epoch: 14640] train loss: 1.1933, train acc: 0.4957, val loss: 1.1773, val acc: 0.4992  (best train acc: 0.5134, best val acc: 0.5096)\n",
      "[Epoch: 14660] train loss: 1.1988, train acc: 0.4936, val loss: 1.1771, val acc: 0.4981  (best train acc: 0.5134, best val acc: 0.5096)\n",
      "[Epoch: 14680] train loss: 1.1846, train acc: 0.4993, val loss: 1.1789, val acc: 0.5073  (best train acc: 0.5134, best val acc: 0.5096)\n",
      "[Epoch: 14700] train loss: 1.1691, train acc: 0.5001, val loss: 1.1865, val acc: 0.5002  (best train acc: 0.5134, best val acc: 0.5096)\n",
      "[Epoch: 14720] train loss: 1.1760, train acc: 0.4999, val loss: 1.1789, val acc: 0.4934  (best train acc: 0.5134, best val acc: 0.5096)\n",
      "[Epoch: 14740] train loss: 1.1764, train acc: 0.5024, val loss: 1.1789, val acc: 0.5042  (best train acc: 0.5134, best val acc: 0.5096)\n",
      "[Epoch: 14760] train loss: 1.1674, train acc: 0.5037, val loss: 1.1798, val acc: 0.5059  (best train acc: 0.5134, best val acc: 0.5096)\n",
      "[Epoch: 14780] train loss: 1.1795, train acc: 0.4983, val loss: 1.1817, val acc: 0.5059  (best train acc: 0.5134, best val acc: 0.5096)\n",
      "[Epoch: 14800] train loss: 1.1782, train acc: 0.5043, val loss: 1.1786, val acc: 0.4968  (best train acc: 0.5134, best val acc: 0.5096)\n",
      "[Epoch: 14820] train loss: 1.1769, train acc: 0.5015, val loss: 1.1792, val acc: 0.5062  (best train acc: 0.5134, best val acc: 0.5096)\n",
      "[Epoch: 14840] train loss: 1.1768, train acc: 0.5002, val loss: 1.1788, val acc: 0.5042  (best train acc: 0.5134, best val acc: 0.5096)\n",
      "[Epoch: 14860] train loss: 1.1737, train acc: 0.4978, val loss: 1.1810, val acc: 0.4914  (best train acc: 0.5134, best val acc: 0.5096)\n",
      "[Epoch: 14880] train loss: 1.1783, train acc: 0.5007, val loss: 1.1764, val acc: 0.4995  (best train acc: 0.5134, best val acc: 0.5096)\n",
      "[Epoch: 14900] train loss: 1.1710, train acc: 0.5019, val loss: 1.1773, val acc: 0.5022  (best train acc: 0.5134, best val acc: 0.5096)\n",
      "[Epoch: 14920] train loss: 1.1787, train acc: 0.4970, val loss: 1.1769, val acc: 0.5062  (best train acc: 0.5134, best val acc: 0.5096)\n",
      "[Epoch: 14940] train loss: 1.1763, train acc: 0.5097, val loss: 1.1758, val acc: 0.4981  (best train acc: 0.5134, best val acc: 0.5096)\n",
      "[Epoch: 14960] train loss: 1.1702, train acc: 0.5017, val loss: 1.1766, val acc: 0.4938  (best train acc: 0.5134, best val acc: 0.5096)\n",
      "[Epoch: 14980] train loss: 1.1752, train acc: 0.5048, val loss: 1.1717, val acc: 0.4988  (best train acc: 0.5134, best val acc: 0.5096)\n",
      "[Epoch: 15000] train loss: 1.1754, train acc: 0.5001, val loss: 1.1699, val acc: 0.5049  (best train acc: 0.5134, best val acc: 0.5096)\n",
      "[Epoch: 15020] train loss: 1.1742, train acc: 0.4978, val loss: 1.1761, val acc: 0.5052  (best train acc: 0.5134, best val acc: 0.5116)\n",
      "[Epoch: 15040] train loss: 1.1621, train acc: 0.5113, val loss: 1.1696, val acc: 0.5029  (best train acc: 0.5134, best val acc: 0.5147)\n",
      "[Epoch: 15060] train loss: 1.1626, train acc: 0.5070, val loss: 1.1724, val acc: 0.5025  (best train acc: 0.5134, best val acc: 0.5147)\n",
      "[Epoch: 15080] train loss: 1.1676, train acc: 0.5007, val loss: 1.1704, val acc: 0.5046  (best train acc: 0.5134, best val acc: 0.5147)\n",
      "[Epoch: 15100] train loss: 1.1764, train acc: 0.5006, val loss: 1.1733, val acc: 0.5069  (best train acc: 0.5134, best val acc: 0.5147)\n",
      "[Epoch: 15120] train loss: 1.1736, train acc: 0.4967, val loss: 1.1682, val acc: 0.5008  (best train acc: 0.5134, best val acc: 0.5147)\n",
      "[Epoch: 15140] train loss: 1.1670, train acc: 0.5095, val loss: 1.1691, val acc: 0.5093  (best train acc: 0.5134, best val acc: 0.5147)\n",
      "[Epoch: 15160] train loss: 1.1740, train acc: 0.4957, val loss: 1.1676, val acc: 0.5019  (best train acc: 0.5134, best val acc: 0.5147)\n",
      "[Epoch: 15180] train loss: 1.1759, train acc: 0.5064, val loss: 1.1675, val acc: 0.4995  (best train acc: 0.5134, best val acc: 0.5147)\n",
      "[Epoch: 15200] train loss: 1.1682, train acc: 0.5040, val loss: 1.1677, val acc: 0.5073  (best train acc: 0.5134, best val acc: 0.5147)\n",
      "[Epoch: 15220] train loss: 1.1678, train acc: 0.5018, val loss: 1.1737, val acc: 0.5079  (best train acc: 0.5134, best val acc: 0.5147)\n",
      "[Epoch: 15240] train loss: 1.1634, train acc: 0.5166, val loss: 1.1681, val acc: 0.5059  (best train acc: 0.5166, best val acc: 0.5147)\n",
      "[Epoch: 15260] train loss: 1.1759, train acc: 0.4931, val loss: 1.1669, val acc: 0.5052  (best train acc: 0.5166, best val acc: 0.5147)\n",
      "[Epoch: 15280] train loss: 1.1736, train acc: 0.5014, val loss: 1.1668, val acc: 0.5035  (best train acc: 0.5166, best val acc: 0.5147)\n",
      "[Epoch: 15300] train loss: 1.1856, train acc: 0.4952, val loss: 1.1679, val acc: 0.5120  (best train acc: 0.5166, best val acc: 0.5150)\n",
      "[Epoch: 15320] train loss: 1.1714, train acc: 0.5037, val loss: 1.1666, val acc: 0.5019  (best train acc: 0.5166, best val acc: 0.5150)\n",
      "[Epoch: 15340] train loss: 1.1638, train acc: 0.5090, val loss: 1.1676, val acc: 0.5062  (best train acc: 0.5166, best val acc: 0.5150)\n",
      "[Epoch: 15360] train loss: 1.1673, train acc: 0.5037, val loss: 1.1692, val acc: 0.5049  (best train acc: 0.5166, best val acc: 0.5150)\n",
      "[Epoch: 15380] train loss: 1.1770, train acc: 0.5013, val loss: 1.1696, val acc: 0.5099  (best train acc: 0.5166, best val acc: 0.5150)\n",
      "[Epoch: 15400] train loss: 1.1660, train acc: 0.5090, val loss: 1.1738, val acc: 0.5032  (best train acc: 0.5166, best val acc: 0.5150)\n",
      "[Epoch: 15420] train loss: 1.1718, train acc: 0.5021, val loss: 1.1666, val acc: 0.5039  (best train acc: 0.5179, best val acc: 0.5150)\n",
      "[Epoch: 15440] train loss: 1.1651, train acc: 0.5020, val loss: 1.1669, val acc: 0.5056  (best train acc: 0.5179, best val acc: 0.5150)\n",
      "[Epoch: 15460] train loss: 1.1607, train acc: 0.5075, val loss: 1.1670, val acc: 0.5076  (best train acc: 0.5179, best val acc: 0.5150)\n",
      "[Epoch: 15480] train loss: 1.1652, train acc: 0.5048, val loss: 1.1660, val acc: 0.5022  (best train acc: 0.5179, best val acc: 0.5150)\n",
      "[Epoch: 15500] train loss: 1.1727, train acc: 0.5093, val loss: 1.1683, val acc: 0.5083  (best train acc: 0.5179, best val acc: 0.5150)\n",
      "[Epoch: 15520] train loss: 1.1614, train acc: 0.5060, val loss: 1.1658, val acc: 0.5046  (best train acc: 0.5179, best val acc: 0.5150)\n",
      "[Epoch: 15540] train loss: 1.1745, train acc: 0.5030, val loss: 1.1649, val acc: 0.5029  (best train acc: 0.5179, best val acc: 0.5150)\n",
      "[Epoch: 15560] train loss: 1.1651, train acc: 0.5032, val loss: 1.1690, val acc: 0.5123  (best train acc: 0.5179, best val acc: 0.5150)\n",
      "[Epoch: 15580] train loss: 1.1698, train acc: 0.4973, val loss: 1.1670, val acc: 0.4965  (best train acc: 0.5186, best val acc: 0.5157)\n",
      "[Epoch: 15600] train loss: 1.1779, train acc: 0.5015, val loss: 1.1664, val acc: 0.5039  (best train acc: 0.5186, best val acc: 0.5157)\n",
      "[Epoch: 15620] train loss: 1.1571, train acc: 0.5201, val loss: 1.1695, val acc: 0.5113  (best train acc: 0.5201, best val acc: 0.5157)\n",
      "[Epoch: 15640] train loss: 1.1583, train acc: 0.5084, val loss: 1.1646, val acc: 0.5035  (best train acc: 0.5201, best val acc: 0.5157)\n",
      "[Epoch: 15660] train loss: 1.1757, train acc: 0.5035, val loss: 1.1646, val acc: 0.5079  (best train acc: 0.5201, best val acc: 0.5157)\n",
      "[Epoch: 15680] train loss: 1.1607, train acc: 0.5148, val loss: 1.1643, val acc: 0.5052  (best train acc: 0.5201, best val acc: 0.5157)\n",
      "[Epoch: 15700] train loss: 1.1740, train acc: 0.5077, val loss: 1.1670, val acc: 0.5130  (best train acc: 0.5201, best val acc: 0.5157)\n",
      "[Epoch: 15720] train loss: 1.1677, train acc: 0.5064, val loss: 1.1737, val acc: 0.5086  (best train acc: 0.5201, best val acc: 0.5174)\n",
      "[Epoch: 15740] train loss: 1.1745, train acc: 0.5003, val loss: 1.1644, val acc: 0.5066  (best train acc: 0.5201, best val acc: 0.5174)\n",
      "[Epoch: 15760] train loss: 1.1814, train acc: 0.4988, val loss: 1.1641, val acc: 0.5076  (best train acc: 0.5201, best val acc: 0.5174)\n",
      "[Epoch: 15780] train loss: 1.1692, train acc: 0.5107, val loss: 1.1651, val acc: 0.5113  (best train acc: 0.5201, best val acc: 0.5174)\n",
      "[Epoch: 15800] train loss: 1.1649, train acc: 0.5097, val loss: 1.1648, val acc: 0.5089  (best train acc: 0.5201, best val acc: 0.5174)\n",
      "[Epoch: 15820] train loss: 1.1731, train acc: 0.4987, val loss: 1.1676, val acc: 0.5126  (best train acc: 0.5201, best val acc: 0.5174)\n",
      "[Epoch: 15840] train loss: 1.1700, train acc: 0.5030, val loss: 1.1658, val acc: 0.5137  (best train acc: 0.5201, best val acc: 0.5174)\n",
      "[Epoch: 15860] train loss: 1.1694, train acc: 0.5009, val loss: 1.1639, val acc: 0.5049  (best train acc: 0.5201, best val acc: 0.5174)\n",
      "[Epoch: 15880] train loss: 1.1690, train acc: 0.5056, val loss: 1.1647, val acc: 0.5096  (best train acc: 0.5201, best val acc: 0.5174)\n",
      "[Epoch: 15900] train loss: 1.1586, train acc: 0.5153, val loss: 1.1649, val acc: 0.5096  (best train acc: 0.5201, best val acc: 0.5174)\n",
      "[Epoch: 15920] train loss: 1.1742, train acc: 0.4987, val loss: 1.1639, val acc: 0.5039  (best train acc: 0.5201, best val acc: 0.5174)\n",
      "[Epoch: 15940] train loss: 1.1668, train acc: 0.5063, val loss: 1.1647, val acc: 0.5002  (best train acc: 0.5201, best val acc: 0.5174)\n",
      "[Epoch: 15960] train loss: 1.1667, train acc: 0.5086, val loss: 1.1645, val acc: 0.5083  (best train acc: 0.5201, best val acc: 0.5174)\n",
      "[Epoch: 15980] train loss: 1.1575, train acc: 0.5168, val loss: 1.1645, val acc: 0.5113  (best train acc: 0.5201, best val acc: 0.5174)\n",
      "[Epoch: 16000] train loss: 1.1679, train acc: 0.5013, val loss: 1.1653, val acc: 0.5140  (best train acc: 0.5201, best val acc: 0.5174)\n",
      "[Epoch: 16020] train loss: 1.1722, train acc: 0.5017, val loss: 1.1642, val acc: 0.5110  (best train acc: 0.5201, best val acc: 0.5174)\n",
      "[Epoch: 16040] train loss: 1.1711, train acc: 0.5029, val loss: 1.1631, val acc: 0.5089  (best train acc: 0.5201, best val acc: 0.5174)\n",
      "[Epoch: 16060] train loss: 1.1703, train acc: 0.5077, val loss: 1.1628, val acc: 0.5083  (best train acc: 0.5201, best val acc: 0.5174)\n",
      "[Epoch: 16080] train loss: 1.1708, train acc: 0.5067, val loss: 1.1680, val acc: 0.5143  (best train acc: 0.5201, best val acc: 0.5174)\n",
      "[Epoch: 16100] train loss: 1.1582, train acc: 0.5168, val loss: 1.1685, val acc: 0.5130  (best train acc: 0.5201, best val acc: 0.5174)\n",
      "[Epoch: 16120] train loss: 1.1632, train acc: 0.5100, val loss: 1.1631, val acc: 0.5093  (best train acc: 0.5201, best val acc: 0.5174)\n",
      "[Epoch: 16140] train loss: 1.1898, train acc: 0.5001, val loss: 1.1703, val acc: 0.5150  (best train acc: 0.5201, best val acc: 0.5174)\n",
      "[Epoch: 16160] train loss: 1.1748, train acc: 0.5102, val loss: 1.1640, val acc: 0.5130  (best train acc: 0.5201, best val acc: 0.5177)\n",
      "[Epoch: 16180] train loss: 1.1715, train acc: 0.5016, val loss: 1.1695, val acc: 0.5140  (best train acc: 0.5201, best val acc: 0.5177)\n",
      "[Epoch: 16200] train loss: 1.1744, train acc: 0.4978, val loss: 1.1639, val acc: 0.5150  (best train acc: 0.5201, best val acc: 0.5177)\n",
      "[Epoch: 16220] train loss: 1.1654, train acc: 0.5093, val loss: 1.1649, val acc: 0.5029  (best train acc: 0.5201, best val acc: 0.5177)\n",
      "[Epoch: 16240] train loss: 1.1657, train acc: 0.5090, val loss: 1.1633, val acc: 0.5073  (best train acc: 0.5201, best val acc: 0.5177)\n",
      "[Epoch: 16260] train loss: 1.1705, train acc: 0.5024, val loss: 1.1673, val acc: 0.5147  (best train acc: 0.5201, best val acc: 0.5177)\n",
      "[Epoch: 16280] train loss: 1.1609, train acc: 0.5090, val loss: 1.1632, val acc: 0.5079  (best train acc: 0.5201, best val acc: 0.5187)\n",
      "[Epoch: 16300] train loss: 1.1681, train acc: 0.5111, val loss: 1.1664, val acc: 0.5184  (best train acc: 0.5201, best val acc: 0.5187)\n",
      "[Epoch: 16320] train loss: 1.1807, train acc: 0.4985, val loss: 1.1628, val acc: 0.5069  (best train acc: 0.5203, best val acc: 0.5187)\n",
      "[Epoch: 16340] train loss: 1.1687, train acc: 0.4993, val loss: 1.1626, val acc: 0.5076  (best train acc: 0.5203, best val acc: 0.5187)\n",
      "[Epoch: 16360] train loss: 1.1700, train acc: 0.4996, val loss: 1.1623, val acc: 0.5093  (best train acc: 0.5203, best val acc: 0.5187)\n",
      "[Epoch: 16380] train loss: 1.1662, train acc: 0.5051, val loss: 1.1630, val acc: 0.5096  (best train acc: 0.5203, best val acc: 0.5187)\n",
      "[Epoch: 16400] train loss: 1.1684, train acc: 0.5144, val loss: 1.1667, val acc: 0.5177  (best train acc: 0.5203, best val acc: 0.5187)\n",
      "[Epoch: 16420] train loss: 1.1677, train acc: 0.5103, val loss: 1.1621, val acc: 0.5086  (best train acc: 0.5203, best val acc: 0.5187)\n",
      "[Epoch: 16440] train loss: 1.1636, train acc: 0.5051, val loss: 1.1625, val acc: 0.5083  (best train acc: 0.5203, best val acc: 0.5187)\n",
      "[Epoch: 16460] train loss: 1.1717, train acc: 0.5011, val loss: 1.1616, val acc: 0.5083  (best train acc: 0.5203, best val acc: 0.5187)\n",
      "[Epoch: 16480] train loss: 1.1755, train acc: 0.5038, val loss: 1.1679, val acc: 0.5143  (best train acc: 0.5203, best val acc: 0.5187)\n",
      "[Epoch: 16500] train loss: 1.1652, train acc: 0.5142, val loss: 1.1621, val acc: 0.5106  (best train acc: 0.5203, best val acc: 0.5187)\n",
      "[Epoch: 16520] train loss: 1.1597, train acc: 0.5161, val loss: 1.1621, val acc: 0.5079  (best train acc: 0.5203, best val acc: 0.5187)\n",
      "[Epoch: 16540] train loss: 1.1613, train acc: 0.5114, val loss: 1.1660, val acc: 0.5123  (best train acc: 0.5203, best val acc: 0.5187)\n",
      "[Epoch: 16560] train loss: 1.1737, train acc: 0.5099, val loss: 1.1685, val acc: 0.5093  (best train acc: 0.5203, best val acc: 0.5187)\n",
      "[Epoch: 16580] train loss: 1.1647, train acc: 0.5110, val loss: 1.1627, val acc: 0.5083  (best train acc: 0.5203, best val acc: 0.5187)\n",
      "[Epoch: 16600] train loss: 1.1675, train acc: 0.5014, val loss: 1.1618, val acc: 0.5076  (best train acc: 0.5203, best val acc: 0.5187)\n",
      "[Epoch: 16620] train loss: 1.1752, train acc: 0.4973, val loss: 1.1642, val acc: 0.5153  (best train acc: 0.5226, best val acc: 0.5187)\n",
      "[Epoch: 16640] train loss: 1.1723, train acc: 0.5004, val loss: 1.1627, val acc: 0.5049  (best train acc: 0.5226, best val acc: 0.5187)\n",
      "[Epoch: 16660] train loss: 1.1706, train acc: 0.5057, val loss: 1.1613, val acc: 0.5086  (best train acc: 0.5226, best val acc: 0.5187)\n",
      "[Epoch: 16680] train loss: 1.1616, train acc: 0.5118, val loss: 1.1608, val acc: 0.5089  (best train acc: 0.5226, best val acc: 0.5187)\n",
      "[Epoch: 16700] train loss: 1.1571, train acc: 0.5188, val loss: 1.1616, val acc: 0.5103  (best train acc: 0.5226, best val acc: 0.5187)\n",
      "[Epoch: 16720] train loss: 1.1657, train acc: 0.5124, val loss: 1.1628, val acc: 0.5099  (best train acc: 0.5226, best val acc: 0.5187)\n",
      "[Epoch: 16740] train loss: 1.1604, train acc: 0.5073, val loss: 1.1638, val acc: 0.5153  (best train acc: 0.5226, best val acc: 0.5187)\n",
      "[Epoch: 16760] train loss: 1.1643, train acc: 0.5111, val loss: 1.1640, val acc: 0.5164  (best train acc: 0.5226, best val acc: 0.5187)\n",
      "[Epoch: 16780] train loss: 1.1674, train acc: 0.5074, val loss: 1.1612, val acc: 0.5079  (best train acc: 0.5226, best val acc: 0.5187)\n",
      "[Epoch: 16800] train loss: 1.1591, train acc: 0.5133, val loss: 1.1630, val acc: 0.5103  (best train acc: 0.5233, best val acc: 0.5187)\n",
      "[Epoch: 16820] train loss: 1.1584, train acc: 0.5046, val loss: 1.1647, val acc: 0.5157  (best train acc: 0.5233, best val acc: 0.5187)\n",
      "[Epoch: 16840] train loss: 1.1721, train acc: 0.5069, val loss: 1.1605, val acc: 0.5099  (best train acc: 0.5233, best val acc: 0.5187)\n",
      "[Epoch: 16860] train loss: 1.1612, train acc: 0.5123, val loss: 1.1622, val acc: 0.5110  (best train acc: 0.5233, best val acc: 0.5187)\n",
      "[Epoch: 16880] train loss: 1.1625, train acc: 0.5081, val loss: 1.1681, val acc: 0.5110  (best train acc: 0.5233, best val acc: 0.5187)\n",
      "[Epoch: 16900] train loss: 1.1576, train acc: 0.5143, val loss: 1.1602, val acc: 0.5093  (best train acc: 0.5233, best val acc: 0.5187)\n",
      "[Epoch: 16920] train loss: 1.1610, train acc: 0.5077, val loss: 1.1615, val acc: 0.5120  (best train acc: 0.5233, best val acc: 0.5187)\n",
      "[Epoch: 16940] train loss: 1.1569, train acc: 0.5035, val loss: 1.1610, val acc: 0.5120  (best train acc: 0.5233, best val acc: 0.5187)\n",
      "[Epoch: 16960] train loss: 1.1703, train acc: 0.4984, val loss: 1.1616, val acc: 0.5113  (best train acc: 0.5233, best val acc: 0.5187)\n",
      "[Epoch: 16980] train loss: 1.1550, train acc: 0.5200, val loss: 1.1662, val acc: 0.5177  (best train acc: 0.5233, best val acc: 0.5187)\n",
      "[Epoch: 17000] train loss: 1.1613, train acc: 0.5144, val loss: 1.1622, val acc: 0.5123  (best train acc: 0.5233, best val acc: 0.5187)\n",
      "[Epoch: 17020] train loss: 1.1588, train acc: 0.5106, val loss: 1.1631, val acc: 0.5143  (best train acc: 0.5233, best val acc: 0.5187)\n",
      "[Epoch: 17040] train loss: 1.1638, train acc: 0.5037, val loss: 1.1603, val acc: 0.5083  (best train acc: 0.5233, best val acc: 0.5187)\n",
      "[Epoch: 17060] train loss: 1.1641, train acc: 0.5086, val loss: 1.1657, val acc: 0.5170  (best train acc: 0.5233, best val acc: 0.5187)\n",
      "[Epoch: 17080] train loss: 1.1656, train acc: 0.5159, val loss: 1.1607, val acc: 0.5096  (best train acc: 0.5233, best val acc: 0.5187)\n",
      "[Epoch: 17100] train loss: 1.1753, train acc: 0.5051, val loss: 1.1619, val acc: 0.5062  (best train acc: 0.5233, best val acc: 0.5187)\n",
      "[Epoch: 17120] train loss: 1.1575, train acc: 0.5095, val loss: 1.1627, val acc: 0.5143  (best train acc: 0.5233, best val acc: 0.5187)\n",
      "[Epoch: 17140] train loss: 1.1813, train acc: 0.5019, val loss: 1.1603, val acc: 0.5096  (best train acc: 0.5233, best val acc: 0.5211)\n",
      "[Epoch: 17160] train loss: 1.1688, train acc: 0.5129, val loss: 1.1638, val acc: 0.5160  (best train acc: 0.5233, best val acc: 0.5211)\n",
      "[Epoch: 17180] train loss: 1.1627, train acc: 0.5122, val loss: 1.1613, val acc: 0.5133  (best train acc: 0.5233, best val acc: 0.5211)\n",
      "[Epoch: 17200] train loss: 1.1713, train acc: 0.5084, val loss: 1.1614, val acc: 0.5174  (best train acc: 0.5233, best val acc: 0.5211)\n",
      "[Epoch: 17220] train loss: 1.1576, train acc: 0.5142, val loss: 1.1628, val acc: 0.5177  (best train acc: 0.5233, best val acc: 0.5211)\n",
      "[Epoch: 17240] train loss: 1.1681, train acc: 0.5160, val loss: 1.1600, val acc: 0.5116  (best train acc: 0.5233, best val acc: 0.5211)\n",
      "[Epoch: 17260] train loss: 1.1667, train acc: 0.5115, val loss: 1.1618, val acc: 0.5133  (best train acc: 0.5233, best val acc: 0.5211)\n",
      "[Epoch: 17280] train loss: 1.1780, train acc: 0.4859, val loss: 1.1617, val acc: 0.5177  (best train acc: 0.5233, best val acc: 0.5211)\n",
      "[Epoch: 17300] train loss: 1.1687, train acc: 0.5138, val loss: 1.1601, val acc: 0.5120  (best train acc: 0.5233, best val acc: 0.5221)\n",
      "[Epoch: 17320] train loss: 1.1598, train acc: 0.5109, val loss: 1.1603, val acc: 0.5116  (best train acc: 0.5233, best val acc: 0.5221)\n",
      "[Epoch: 17340] train loss: 1.1610, train acc: 0.5038, val loss: 1.1604, val acc: 0.5120  (best train acc: 0.5233, best val acc: 0.5221)\n",
      "[Epoch: 17360] train loss: 1.1642, train acc: 0.5111, val loss: 1.1600, val acc: 0.5116  (best train acc: 0.5233, best val acc: 0.5221)\n",
      "[Epoch: 17380] train loss: 1.1639, train acc: 0.5141, val loss: 1.1627, val acc: 0.5096  (best train acc: 0.5233, best val acc: 0.5221)\n",
      "[Epoch: 17400] train loss: 1.1712, train acc: 0.5005, val loss: 1.1594, val acc: 0.5106  (best train acc: 0.5248, best val acc: 0.5221)\n",
      "[Epoch: 17420] train loss: 1.1616, train acc: 0.5084, val loss: 1.1593, val acc: 0.5106  (best train acc: 0.5248, best val acc: 0.5221)\n",
      "[Epoch: 17440] train loss: 1.1620, train acc: 0.5168, val loss: 1.1601, val acc: 0.5123  (best train acc: 0.5248, best val acc: 0.5221)\n",
      "[Epoch: 17460] train loss: 1.1740, train acc: 0.4961, val loss: 1.1606, val acc: 0.5143  (best train acc: 0.5248, best val acc: 0.5221)\n",
      "[Epoch: 17480] train loss: 1.1577, train acc: 0.5053, val loss: 1.1621, val acc: 0.5083  (best train acc: 0.5270, best val acc: 0.5221)\n",
      "[Epoch: 17500] train loss: 1.1582, train acc: 0.5121, val loss: 1.1633, val acc: 0.5143  (best train acc: 0.5270, best val acc: 0.5221)\n",
      "[Epoch: 17520] train loss: 1.1734, train acc: 0.5002, val loss: 1.1595, val acc: 0.5116  (best train acc: 0.5270, best val acc: 0.5221)\n",
      "[Epoch: 17540] train loss: 1.1705, train acc: 0.5084, val loss: 1.1591, val acc: 0.5113  (best train acc: 0.5270, best val acc: 0.5221)\n",
      "[Epoch: 17560] train loss: 1.1548, train acc: 0.5147, val loss: 1.1621, val acc: 0.5147  (best train acc: 0.5270, best val acc: 0.5221)\n",
      "[Epoch: 17580] train loss: 1.1667, train acc: 0.5035, val loss: 1.1604, val acc: 0.5170  (best train acc: 0.5270, best val acc: 0.5221)\n",
      "[Epoch: 17600] train loss: 1.1608, train acc: 0.5140, val loss: 1.1626, val acc: 0.5002  (best train acc: 0.5270, best val acc: 0.5221)\n",
      "[Epoch: 17620] train loss: 1.1707, train acc: 0.5087, val loss: 1.1638, val acc: 0.5174  (best train acc: 0.5270, best val acc: 0.5221)\n",
      "[Epoch: 17640] train loss: 1.1615, train acc: 0.5117, val loss: 1.1585, val acc: 0.5137  (best train acc: 0.5270, best val acc: 0.5221)\n",
      "[Epoch: 17660] train loss: 1.1655, train acc: 0.5116, val loss: 1.1589, val acc: 0.5153  (best train acc: 0.5270, best val acc: 0.5221)\n",
      "[Epoch: 17680] train loss: 1.1577, train acc: 0.5185, val loss: 1.1578, val acc: 0.5126  (best train acc: 0.5270, best val acc: 0.5221)\n",
      "[Epoch: 17700] train loss: 1.1691, train acc: 0.4959, val loss: 1.1584, val acc: 0.5096  (best train acc: 0.5270, best val acc: 0.5221)\n",
      "[Epoch: 17720] train loss: 1.1652, train acc: 0.5126, val loss: 1.1564, val acc: 0.5133  (best train acc: 0.5270, best val acc: 0.5221)\n",
      "[Epoch: 17740] train loss: 1.1493, train acc: 0.5181, val loss: 1.1569, val acc: 0.5160  (best train acc: 0.5270, best val acc: 0.5221)\n",
      "[Epoch: 17760] train loss: 1.1499, train acc: 0.5194, val loss: 1.1549, val acc: 0.5137  (best train acc: 0.5270, best val acc: 0.5221)\n",
      "[Epoch: 17780] train loss: 1.1578, train acc: 0.5172, val loss: 1.1557, val acc: 0.5073  (best train acc: 0.5270, best val acc: 0.5221)\n",
      "[Epoch: 17800] train loss: 1.1556, train acc: 0.5132, val loss: 1.1521, val acc: 0.5126  (best train acc: 0.5270, best val acc: 0.5221)\n",
      "[Epoch: 17820] train loss: 1.1619, train acc: 0.5107, val loss: 1.1511, val acc: 0.5153  (best train acc: 0.5270, best val acc: 0.5221)\n",
      "[Epoch: 17840] train loss: 1.1569, train acc: 0.5095, val loss: 1.1529, val acc: 0.5191  (best train acc: 0.5270, best val acc: 0.5221)\n",
      "[Epoch: 17860] train loss: 1.1569, train acc: 0.4977, val loss: 1.1502, val acc: 0.5201  (best train acc: 0.5270, best val acc: 0.5221)\n",
      "[Epoch: 17880] train loss: 1.1505, train acc: 0.5160, val loss: 1.1465, val acc: 0.5164  (best train acc: 0.5270, best val acc: 0.5234)\n",
      "[Epoch: 17900] train loss: 1.1597, train acc: 0.5106, val loss: 1.1465, val acc: 0.5214  (best train acc: 0.5270, best val acc: 0.5234)\n",
      "[Epoch: 17920] train loss: 1.1467, train acc: 0.5160, val loss: 1.1510, val acc: 0.5167  (best train acc: 0.5270, best val acc: 0.5234)\n",
      "[Epoch: 17940] train loss: 1.1498, train acc: 0.5103, val loss: 1.1501, val acc: 0.5180  (best train acc: 0.5270, best val acc: 0.5248)\n",
      "[Epoch: 17960] train loss: 1.1489, train acc: 0.5168, val loss: 1.1436, val acc: 0.5177  (best train acc: 0.5338, best val acc: 0.5248)\n",
      "[Epoch: 17980] train loss: 1.1430, train acc: 0.5169, val loss: 1.1432, val acc: 0.5174  (best train acc: 0.5338, best val acc: 0.5248)\n",
      "[Epoch: 18000] train loss: 1.1462, train acc: 0.5247, val loss: 1.1429, val acc: 0.5180  (best train acc: 0.5338, best val acc: 0.5248)\n",
      "[Epoch: 18020] train loss: 1.1396, train acc: 0.5154, val loss: 1.1465, val acc: 0.5194  (best train acc: 0.5338, best val acc: 0.5248)\n",
      "[Epoch: 18040] train loss: 1.1479, train acc: 0.5054, val loss: 1.1410, val acc: 0.5174  (best train acc: 0.5338, best val acc: 0.5248)\n",
      "[Epoch: 18060] train loss: 1.1465, train acc: 0.5216, val loss: 1.1402, val acc: 0.5167  (best train acc: 0.5338, best val acc: 0.5248)\n",
      "[Epoch: 18080] train loss: 1.1550, train acc: 0.4973, val loss: 1.1398, val acc: 0.5180  (best train acc: 0.5338, best val acc: 0.5248)\n",
      "[Epoch: 18100] train loss: 1.1482, train acc: 0.5138, val loss: 1.1399, val acc: 0.5218  (best train acc: 0.5338, best val acc: 0.5248)\n",
      "[Epoch: 18120] train loss: 1.1457, train acc: 0.5200, val loss: 1.1497, val acc: 0.4938  (best train acc: 0.5338, best val acc: 0.5272)\n",
      "[Epoch: 18140] train loss: 1.1473, train acc: 0.5171, val loss: 1.1400, val acc: 0.5187  (best train acc: 0.5338, best val acc: 0.5272)\n",
      "[Epoch: 18160] train loss: 1.1443, train acc: 0.5165, val loss: 1.1463, val acc: 0.5035  (best train acc: 0.5338, best val acc: 0.5272)\n",
      "[Epoch: 18180] train loss: 1.1384, train acc: 0.5265, val loss: 1.1421, val acc: 0.5187  (best train acc: 0.5338, best val acc: 0.5272)\n",
      "[Epoch: 18200] train loss: 1.1472, train acc: 0.5210, val loss: 1.1408, val acc: 0.5211  (best train acc: 0.5338, best val acc: 0.5272)\n",
      "[Epoch: 18220] train loss: 1.1430, train acc: 0.5210, val loss: 1.1383, val acc: 0.5177  (best train acc: 0.5338, best val acc: 0.5272)\n",
      "[Epoch: 18240] train loss: 1.1483, train acc: 0.5147, val loss: 1.1379, val acc: 0.5218  (best train acc: 0.5338, best val acc: 0.5272)\n",
      "[Epoch: 18260] train loss: 1.1474, train acc: 0.5187, val loss: 1.1390, val acc: 0.5218  (best train acc: 0.5338, best val acc: 0.5272)\n",
      "[Epoch: 18280] train loss: 1.1442, train acc: 0.5212, val loss: 1.1374, val acc: 0.5180  (best train acc: 0.5338, best val acc: 0.5272)\n",
      "[Epoch: 18300] train loss: 1.1376, train acc: 0.5208, val loss: 1.1392, val acc: 0.5218  (best train acc: 0.5338, best val acc: 0.5275)\n",
      "[Epoch: 18320] train loss: 1.1374, train acc: 0.5212, val loss: 1.1486, val acc: 0.5187  (best train acc: 0.5338, best val acc: 0.5275)\n",
      "[Epoch: 18340] train loss: 1.1373, train acc: 0.5261, val loss: 1.1459, val acc: 0.5191  (best train acc: 0.5338, best val acc: 0.5275)\n",
      "[Epoch: 18360] train loss: 1.1457, train acc: 0.5203, val loss: 1.1443, val acc: 0.5167  (best train acc: 0.5338, best val acc: 0.5275)\n",
      "[Epoch: 18380] train loss: 1.1545, train acc: 0.5131, val loss: 1.1366, val acc: 0.5194  (best train acc: 0.5338, best val acc: 0.5275)\n",
      "[Epoch: 18400] train loss: 1.1441, train acc: 0.5201, val loss: 1.1372, val acc: 0.5157  (best train acc: 0.5338, best val acc: 0.5275)\n",
      "[Epoch: 18420] train loss: 1.1407, train acc: 0.5215, val loss: 1.1388, val acc: 0.5177  (best train acc: 0.5338, best val acc: 0.5275)\n",
      "[Epoch: 18440] train loss: 1.1583, train acc: 0.5035, val loss: 1.1376, val acc: 0.5231  (best train acc: 0.5338, best val acc: 0.5275)\n",
      "[Epoch: 18460] train loss: 1.1400, train acc: 0.5178, val loss: 1.1412, val acc: 0.5234  (best train acc: 0.5338, best val acc: 0.5275)\n",
      "[Epoch: 18480] train loss: 1.1511, train acc: 0.5193, val loss: 1.1369, val acc: 0.5133  (best train acc: 0.5338, best val acc: 0.5275)\n",
      "[Epoch: 18500] train loss: 1.1478, train acc: 0.5170, val loss: 1.1362, val acc: 0.5204  (best train acc: 0.5338, best val acc: 0.5278)\n",
      "[Epoch: 18520] train loss: 1.1548, train acc: 0.5124, val loss: 1.1357, val acc: 0.5224  (best train acc: 0.5338, best val acc: 0.5295)\n",
      "[Epoch: 18540] train loss: 1.1515, train acc: 0.5167, val loss: 1.1386, val acc: 0.5150  (best train acc: 0.5338, best val acc: 0.5295)\n",
      "[Epoch: 18560] train loss: 1.1327, train acc: 0.5267, val loss: 1.1409, val acc: 0.5211  (best train acc: 0.5338, best val acc: 0.5295)\n",
      "[Epoch: 18580] train loss: 1.1433, train acc: 0.5205, val loss: 1.1351, val acc: 0.5197  (best train acc: 0.5338, best val acc: 0.5295)\n",
      "[Epoch: 18600] train loss: 1.1355, train acc: 0.5237, val loss: 1.1343, val acc: 0.5194  (best train acc: 0.5338, best val acc: 0.5295)\n",
      "[Epoch: 18620] train loss: 1.1368, train acc: 0.5160, val loss: 1.1356, val acc: 0.5218  (best train acc: 0.5338, best val acc: 0.5295)\n",
      "[Epoch: 18640] train loss: 1.1470, train acc: 0.5064, val loss: 1.1336, val acc: 0.5207  (best train acc: 0.5338, best val acc: 0.5295)\n",
      "[Epoch: 18660] train loss: 1.1501, train acc: 0.5187, val loss: 1.1370, val acc: 0.5170  (best train acc: 0.5338, best val acc: 0.5295)\n",
      "[Epoch: 18680] train loss: 1.1379, train acc: 0.5219, val loss: 1.1338, val acc: 0.5218  (best train acc: 0.5338, best val acc: 0.5295)\n",
      "[Epoch: 18700] train loss: 1.1431, train acc: 0.5214, val loss: 1.1377, val acc: 0.5126  (best train acc: 0.5338, best val acc: 0.5295)\n",
      "[Epoch: 18720] train loss: 1.1474, train acc: 0.5171, val loss: 1.1344, val acc: 0.5224  (best train acc: 0.5338, best val acc: 0.5295)\n",
      "[Epoch: 18740] train loss: 1.1393, train acc: 0.5191, val loss: 1.1382, val acc: 0.5228  (best train acc: 0.5338, best val acc: 0.5295)\n",
      "[Epoch: 18760] train loss: 1.1335, train acc: 0.5187, val loss: 1.1340, val acc: 0.5211  (best train acc: 0.5338, best val acc: 0.5295)\n",
      "[Epoch: 18780] train loss: 1.1341, train acc: 0.5234, val loss: 1.1341, val acc: 0.5177  (best train acc: 0.5338, best val acc: 0.5295)\n",
      "[Epoch: 18800] train loss: 1.1498, train acc: 0.5202, val loss: 1.1342, val acc: 0.5224  (best train acc: 0.5338, best val acc: 0.5295)\n",
      "[Epoch: 18820] train loss: 1.1372, train acc: 0.5244, val loss: 1.1332, val acc: 0.5207  (best train acc: 0.5338, best val acc: 0.5295)\n",
      "[Epoch: 18840] train loss: 1.1363, train acc: 0.5218, val loss: 1.1371, val acc: 0.5214  (best train acc: 0.5338, best val acc: 0.5295)\n",
      "[Epoch: 18860] train loss: 1.1429, train acc: 0.5189, val loss: 1.1352, val acc: 0.5241  (best train acc: 0.5338, best val acc: 0.5295)\n",
      "[Epoch: 18880] train loss: 1.1523, train acc: 0.5166, val loss: 1.1323, val acc: 0.5204  (best train acc: 0.5338, best val acc: 0.5295)\n",
      "[Epoch: 18900] train loss: 1.1447, train acc: 0.5215, val loss: 1.1324, val acc: 0.5231  (best train acc: 0.5338, best val acc: 0.5295)\n",
      "[Epoch: 18920] train loss: 1.1407, train acc: 0.5237, val loss: 1.1388, val acc: 0.5110  (best train acc: 0.5338, best val acc: 0.5295)\n",
      "[Epoch: 18940] train loss: 1.1297, train acc: 0.5247, val loss: 1.1362, val acc: 0.5174  (best train acc: 0.5338, best val acc: 0.5295)\n",
      "[Epoch: 18960] train loss: 1.1425, train acc: 0.5242, val loss: 1.1323, val acc: 0.5245  (best train acc: 0.5338, best val acc: 0.5295)\n",
      "[Epoch: 18980] train loss: 1.1366, train acc: 0.5229, val loss: 1.1317, val acc: 0.5214  (best train acc: 0.5338, best val acc: 0.5295)\n",
      "[Epoch: 19000] train loss: 1.1363, train acc: 0.5218, val loss: 1.1324, val acc: 0.5234  (best train acc: 0.5338, best val acc: 0.5295)\n",
      "[Epoch: 19020] train loss: 1.1441, train acc: 0.5137, val loss: 1.1323, val acc: 0.5248  (best train acc: 0.5338, best val acc: 0.5295)\n",
      "[Epoch: 19040] train loss: 1.1484, train acc: 0.5123, val loss: 1.1360, val acc: 0.5258  (best train acc: 0.5338, best val acc: 0.5295)\n",
      "[Epoch: 19060] train loss: 1.1467, train acc: 0.5155, val loss: 1.1462, val acc: 0.5130  (best train acc: 0.5338, best val acc: 0.5295)\n",
      "[Epoch: 19080] train loss: 1.1422, train acc: 0.5186, val loss: 1.1326, val acc: 0.5245  (best train acc: 0.5338, best val acc: 0.5295)\n",
      "[Epoch: 19100] train loss: 1.1439, train acc: 0.5221, val loss: 1.1321, val acc: 0.5255  (best train acc: 0.5338, best val acc: 0.5295)\n",
      "[Epoch: 19120] train loss: 1.1431, train acc: 0.5113, val loss: 1.1309, val acc: 0.5204  (best train acc: 0.5340, best val acc: 0.5295)\n",
      "[Epoch: 19140] train loss: 1.1350, train acc: 0.5236, val loss: 1.1304, val acc: 0.5207  (best train acc: 0.5340, best val acc: 0.5295)\n",
      "[Epoch: 19160] train loss: 1.1376, train acc: 0.5149, val loss: 1.1331, val acc: 0.5197  (best train acc: 0.5340, best val acc: 0.5295)\n",
      "[Epoch: 19180] train loss: 1.1375, train acc: 0.5181, val loss: 1.1302, val acc: 0.5218  (best train acc: 0.5340, best val acc: 0.5295)\n",
      "[Epoch: 19200] train loss: 1.1485, train acc: 0.5149, val loss: 1.1303, val acc: 0.5197  (best train acc: 0.5340, best val acc: 0.5295)\n",
      "[Epoch: 19220] train loss: 1.1450, train acc: 0.5234, val loss: 1.1313, val acc: 0.5231  (best train acc: 0.5340, best val acc: 0.5298)\n",
      "[Epoch: 19240] train loss: 1.1476, train acc: 0.5219, val loss: 1.1313, val acc: 0.5251  (best train acc: 0.5340, best val acc: 0.5298)\n",
      "[Epoch: 19260] train loss: 1.1381, train acc: 0.5234, val loss: 1.1375, val acc: 0.5069  (best train acc: 0.5340, best val acc: 0.5298)\n",
      "[Epoch: 19280] train loss: 1.1415, train acc: 0.5234, val loss: 1.1317, val acc: 0.5201  (best train acc: 0.5340, best val acc: 0.5298)\n",
      "[Epoch: 19300] train loss: 1.1373, train acc: 0.5262, val loss: 1.1350, val acc: 0.5160  (best train acc: 0.5340, best val acc: 0.5298)\n",
      "[Epoch: 19320] train loss: 1.1570, train acc: 0.5102, val loss: 1.1380, val acc: 0.5204  (best train acc: 0.5340, best val acc: 0.5298)\n",
      "[Epoch: 19340] train loss: 1.1405, train acc: 0.5137, val loss: 1.1373, val acc: 0.5207  (best train acc: 0.5340, best val acc: 0.5298)\n",
      "[Epoch: 19360] train loss: 1.1406, train acc: 0.5181, val loss: 1.1326, val acc: 0.5170  (best train acc: 0.5340, best val acc: 0.5298)\n",
      "[Epoch: 19380] train loss: 1.1409, train acc: 0.5199, val loss: 1.1294, val acc: 0.5218  (best train acc: 0.5340, best val acc: 0.5298)\n",
      "[Epoch: 19400] train loss: 1.1338, train acc: 0.5249, val loss: 1.1312, val acc: 0.5204  (best train acc: 0.5340, best val acc: 0.5298)\n",
      "[Epoch: 19420] train loss: 1.1325, train acc: 0.5215, val loss: 1.1292, val acc: 0.5197  (best train acc: 0.5340, best val acc: 0.5298)\n",
      "[Epoch: 19440] train loss: 1.1338, train acc: 0.5269, val loss: 1.1289, val acc: 0.5207  (best train acc: 0.5340, best val acc: 0.5298)\n",
      "[Epoch: 19460] train loss: 1.1444, train acc: 0.5232, val loss: 1.1299, val acc: 0.5234  (best train acc: 0.5340, best val acc: 0.5298)\n",
      "[Epoch: 19480] train loss: 1.1375, train acc: 0.5246, val loss: 1.1293, val acc: 0.5255  (best train acc: 0.5340, best val acc: 0.5298)\n",
      "[Epoch: 19500] train loss: 1.1310, train acc: 0.5284, val loss: 1.1333, val acc: 0.5288  (best train acc: 0.5340, best val acc: 0.5298)\n",
      "[Epoch: 19520] train loss: 1.1416, train acc: 0.5233, val loss: 1.1284, val acc: 0.5214  (best train acc: 0.5340, best val acc: 0.5298)\n",
      "[Epoch: 19540] train loss: 1.1364, train acc: 0.5257, val loss: 1.1378, val acc: 0.5211  (best train acc: 0.5340, best val acc: 0.5298)\n",
      "[Epoch: 19560] train loss: 1.1459, train acc: 0.5195, val loss: 1.1306, val acc: 0.5255  (best train acc: 0.5340, best val acc: 0.5298)\n",
      "[Epoch: 19580] train loss: 1.1433, train acc: 0.5206, val loss: 1.1294, val acc: 0.5268  (best train acc: 0.5340, best val acc: 0.5298)\n",
      "[Epoch: 19600] train loss: 1.1298, train acc: 0.5265, val loss: 1.1295, val acc: 0.5268  (best train acc: 0.5340, best val acc: 0.5298)\n",
      "[Epoch: 19620] train loss: 1.1346, train acc: 0.5252, val loss: 1.1318, val acc: 0.5288  (best train acc: 0.5340, best val acc: 0.5298)\n",
      "[Epoch: 19640] train loss: 1.1406, train acc: 0.5186, val loss: 1.1285, val acc: 0.5214  (best train acc: 0.5340, best val acc: 0.5298)\n",
      "[Epoch: 19660] train loss: 1.1405, train acc: 0.5250, val loss: 1.1298, val acc: 0.5238  (best train acc: 0.5340, best val acc: 0.5298)\n",
      "[Epoch: 19680] train loss: 1.1436, train acc: 0.5176, val loss: 1.1283, val acc: 0.5231  (best train acc: 0.5340, best val acc: 0.5298)\n",
      "[Epoch: 19700] train loss: 1.1427, train acc: 0.5205, val loss: 1.1286, val acc: 0.5278  (best train acc: 0.5340, best val acc: 0.5298)\n",
      "[Epoch: 19720] train loss: 1.1305, train acc: 0.5252, val loss: 1.1288, val acc: 0.5261  (best train acc: 0.5340, best val acc: 0.5298)\n",
      "[Epoch: 19740] train loss: 1.1460, train acc: 0.5200, val loss: 1.1327, val acc: 0.5265  (best train acc: 0.5340, best val acc: 0.5298)\n",
      "[Epoch: 19760] train loss: 1.1510, train acc: 0.5176, val loss: 1.1278, val acc: 0.5228  (best train acc: 0.5340, best val acc: 0.5298)\n",
      "[Epoch: 19780] train loss: 1.1313, train acc: 0.5307, val loss: 1.1284, val acc: 0.5245  (best train acc: 0.5340, best val acc: 0.5298)\n",
      "[Epoch: 19800] train loss: 1.1334, train acc: 0.5314, val loss: 1.1350, val acc: 0.5258  (best train acc: 0.5340, best val acc: 0.5298)\n",
      "[Epoch: 19820] train loss: 1.1404, train acc: 0.5126, val loss: 1.1275, val acc: 0.5238  (best train acc: 0.5340, best val acc: 0.5298)\n",
      "[Epoch: 19840] train loss: 1.1358, train acc: 0.5221, val loss: 1.1289, val acc: 0.5218  (best train acc: 0.5340, best val acc: 0.5302)\n",
      "[Epoch: 19860] train loss: 1.1274, train acc: 0.5271, val loss: 1.1306, val acc: 0.5261  (best train acc: 0.5340, best val acc: 0.5302)\n",
      "[Epoch: 19880] train loss: 1.1445, train acc: 0.5220, val loss: 1.1287, val acc: 0.5265  (best train acc: 0.5340, best val acc: 0.5302)\n",
      "[Epoch: 19900] train loss: 1.1481, train acc: 0.5222, val loss: 1.1267, val acc: 0.5234  (best train acc: 0.5340, best val acc: 0.5302)\n",
      "[Epoch: 19920] train loss: 1.1642, train acc: 0.5065, val loss: 1.1335, val acc: 0.5160  (best train acc: 0.5340, best val acc: 0.5302)\n",
      "[Epoch: 19940] train loss: 1.1273, train acc: 0.5325, val loss: 1.1292, val acc: 0.5204  (best train acc: 0.5340, best val acc: 0.5302)\n",
      "[Epoch: 19960] train loss: 1.1462, train acc: 0.5209, val loss: 1.1266, val acc: 0.5218  (best train acc: 0.5340, best val acc: 0.5302)\n",
      "[Epoch: 19980] train loss: 1.1392, train acc: 0.5166, val loss: 1.1268, val acc: 0.5258  (best train acc: 0.5340, best val acc: 0.5302)\n",
      "[Epoch: 20000] train loss: 1.1486, train acc: 0.5204, val loss: 1.1275, val acc: 0.5258  (best train acc: 0.5340, best val acc: 0.5305)\n",
      "[Epoch: 20020] train loss: 1.1484, train acc: 0.5194, val loss: 1.1301, val acc: 0.5292  (best train acc: 0.5340, best val acc: 0.5336)\n",
      "[Epoch: 20040] train loss: 1.1332, train acc: 0.5262, val loss: 1.1280, val acc: 0.5258  (best train acc: 0.5340, best val acc: 0.5336)\n",
      "[Epoch: 20060] train loss: 1.1419, train acc: 0.5228, val loss: 1.1324, val acc: 0.5103  (best train acc: 0.5340, best val acc: 0.5336)\n",
      "[Epoch: 20080] train loss: 1.1279, train acc: 0.5270, val loss: 1.1366, val acc: 0.5191  (best train acc: 0.5340, best val acc: 0.5336)\n",
      "[Epoch: 20100] train loss: 1.1360, train acc: 0.5184, val loss: 1.1271, val acc: 0.5255  (best train acc: 0.5340, best val acc: 0.5336)\n",
      "[Epoch: 20120] train loss: 1.1330, train acc: 0.5275, val loss: 1.1288, val acc: 0.5238  (best train acc: 0.5340, best val acc: 0.5336)\n",
      "[Epoch: 20140] train loss: 1.1287, train acc: 0.5306, val loss: 1.1260, val acc: 0.5218  (best train acc: 0.5346, best val acc: 0.5336)\n",
      "[Epoch: 20160] train loss: 1.1377, train acc: 0.5157, val loss: 1.1284, val acc: 0.5265  (best train acc: 0.5346, best val acc: 0.5336)\n",
      "[Epoch: 20180] train loss: 1.1363, train acc: 0.5244, val loss: 1.1266, val acc: 0.5251  (best train acc: 0.5346, best val acc: 0.5336)\n",
      "[Epoch: 20200] train loss: 1.1377, train acc: 0.5238, val loss: 1.1282, val acc: 0.5241  (best train acc: 0.5346, best val acc: 0.5336)\n",
      "[Epoch: 20220] train loss: 1.1502, train acc: 0.5217, val loss: 1.1285, val acc: 0.5245  (best train acc: 0.5346, best val acc: 0.5336)\n",
      "[Epoch: 20240] train loss: 1.1580, train acc: 0.5057, val loss: 1.1297, val acc: 0.5288  (best train acc: 0.5346, best val acc: 0.5336)\n",
      "[Epoch: 20260] train loss: 1.1278, train acc: 0.5285, val loss: 1.1254, val acc: 0.5224  (best train acc: 0.5346, best val acc: 0.5336)\n",
      "[Epoch: 20280] train loss: 1.1320, train acc: 0.5275, val loss: 1.1325, val acc: 0.5096  (best train acc: 0.5346, best val acc: 0.5336)\n",
      "[Epoch: 20300] train loss: 1.1352, train acc: 0.5260, val loss: 1.1261, val acc: 0.5272  (best train acc: 0.5349, best val acc: 0.5336)\n",
      "[Epoch: 20320] train loss: 1.1528, train acc: 0.5163, val loss: 1.1266, val acc: 0.5278  (best train acc: 0.5349, best val acc: 0.5336)\n",
      "[Epoch: 20340] train loss: 1.1266, train acc: 0.5264, val loss: 1.1303, val acc: 0.5170  (best train acc: 0.5349, best val acc: 0.5336)\n",
      "[Epoch: 20360] train loss: 1.1354, train acc: 0.5233, val loss: 1.1253, val acc: 0.5214  (best train acc: 0.5350, best val acc: 0.5336)\n",
      "[Epoch: 20380] train loss: 1.1390, train acc: 0.5164, val loss: 1.1278, val acc: 0.5265  (best train acc: 0.5350, best val acc: 0.5336)\n",
      "[Epoch: 20400] train loss: 1.1426, train acc: 0.5207, val loss: 1.1257, val acc: 0.5272  (best train acc: 0.5350, best val acc: 0.5336)\n",
      "[Epoch: 20420] train loss: 1.1267, train acc: 0.5268, val loss: 1.1259, val acc: 0.5241  (best train acc: 0.5350, best val acc: 0.5336)\n",
      "[Epoch: 20440] train loss: 1.1377, train acc: 0.5206, val loss: 1.1246, val acc: 0.5272  (best train acc: 0.5350, best val acc: 0.5336)\n",
      "[Epoch: 20460] train loss: 1.1299, train acc: 0.5246, val loss: 1.1390, val acc: 0.5042  (best train acc: 0.5350, best val acc: 0.5336)\n",
      "[Epoch: 20480] train loss: 1.1440, train acc: 0.5202, val loss: 1.1288, val acc: 0.5295  (best train acc: 0.5350, best val acc: 0.5336)\n",
      "[Epoch: 20500] train loss: 1.1353, train acc: 0.5218, val loss: 1.1245, val acc: 0.5268  (best train acc: 0.5350, best val acc: 0.5336)\n",
      "[Epoch: 20520] train loss: 1.1375, train acc: 0.5226, val loss: 1.1242, val acc: 0.5245  (best train acc: 0.5372, best val acc: 0.5336)\n",
      "[Epoch: 20540] train loss: 1.1331, train acc: 0.5257, val loss: 1.1241, val acc: 0.5218  (best train acc: 0.5372, best val acc: 0.5336)\n",
      "[Epoch: 20560] train loss: 1.1339, train acc: 0.5248, val loss: 1.1235, val acc: 0.5224  (best train acc: 0.5372, best val acc: 0.5336)\n",
      "[Epoch: 20580] train loss: 1.1349, train acc: 0.5227, val loss: 1.1295, val acc: 0.5285  (best train acc: 0.5372, best val acc: 0.5336)\n",
      "[Epoch: 20600] train loss: 1.1397, train acc: 0.5172, val loss: 1.1284, val acc: 0.5282  (best train acc: 0.5372, best val acc: 0.5336)\n",
      "[Epoch: 20620] train loss: 1.1323, train acc: 0.5194, val loss: 1.1264, val acc: 0.5218  (best train acc: 0.5372, best val acc: 0.5336)\n",
      "[Epoch: 20640] train loss: 1.1298, train acc: 0.5288, val loss: 1.1246, val acc: 0.5238  (best train acc: 0.5372, best val acc: 0.5336)\n",
      "[Epoch: 20660] train loss: 1.1266, train acc: 0.5273, val loss: 1.1255, val acc: 0.5285  (best train acc: 0.5372, best val acc: 0.5336)\n",
      "[Epoch: 20680] train loss: 1.1269, train acc: 0.5280, val loss: 1.1263, val acc: 0.5207  (best train acc: 0.5372, best val acc: 0.5336)\n",
      "[Epoch: 20700] train loss: 1.1479, train acc: 0.5200, val loss: 1.1253, val acc: 0.5319  (best train acc: 0.5372, best val acc: 0.5336)\n",
      "[Epoch: 20720] train loss: 1.1329, train acc: 0.5275, val loss: 1.1249, val acc: 0.5261  (best train acc: 0.5372, best val acc: 0.5336)\n",
      "[Epoch: 20740] train loss: 1.1197, train acc: 0.5315, val loss: 1.1224, val acc: 0.5282  (best train acc: 0.5372, best val acc: 0.5336)\n",
      "[Epoch: 20760] train loss: 1.1371, train acc: 0.5241, val loss: 1.1224, val acc: 0.5255  (best train acc: 0.5372, best val acc: 0.5336)\n",
      "[Epoch: 20780] train loss: 1.1358, train acc: 0.5209, val loss: 1.1252, val acc: 0.5319  (best train acc: 0.5372, best val acc: 0.5336)\n",
      "[Epoch: 20800] train loss: 1.1414, train acc: 0.5290, val loss: 1.1250, val acc: 0.5295  (best train acc: 0.5372, best val acc: 0.5336)\n",
      "[Epoch: 20820] train loss: 1.1377, train acc: 0.5267, val loss: 1.1274, val acc: 0.5305  (best train acc: 0.5372, best val acc: 0.5336)\n",
      "[Epoch: 20840] train loss: 1.1461, train acc: 0.5234, val loss: 1.1267, val acc: 0.5231  (best train acc: 0.5372, best val acc: 0.5336)\n",
      "[Epoch: 20860] train loss: 1.1299, train acc: 0.5234, val loss: 1.1228, val acc: 0.5258  (best train acc: 0.5372, best val acc: 0.5336)\n",
      "[Epoch: 20880] train loss: 1.1243, train acc: 0.5297, val loss: 1.1227, val acc: 0.5258  (best train acc: 0.5372, best val acc: 0.5336)\n",
      "[Epoch: 20900] train loss: 1.1323, train acc: 0.5288, val loss: 1.1248, val acc: 0.5288  (best train acc: 0.5372, best val acc: 0.5336)\n",
      "[Epoch: 20920] train loss: 1.1279, train acc: 0.5276, val loss: 1.1215, val acc: 0.5255  (best train acc: 0.5372, best val acc: 0.5336)\n",
      "[Epoch: 20940] train loss: 1.1279, train acc: 0.5298, val loss: 1.1241, val acc: 0.5241  (best train acc: 0.5372, best val acc: 0.5336)\n",
      "[Epoch: 20960] train loss: 1.1363, train acc: 0.5226, val loss: 1.1220, val acc: 0.5282  (best train acc: 0.5372, best val acc: 0.5336)\n",
      "[Epoch: 20980] train loss: 1.1265, train acc: 0.5297, val loss: 1.1332, val acc: 0.5137  (best train acc: 0.5372, best val acc: 0.5342)\n",
      "[Epoch: 21000] train loss: 1.1328, train acc: 0.5290, val loss: 1.1226, val acc: 0.5241  (best train acc: 0.5372, best val acc: 0.5342)\n",
      "[Epoch: 21020] train loss: 1.1294, train acc: 0.5256, val loss: 1.1259, val acc: 0.5251  (best train acc: 0.5372, best val acc: 0.5342)\n",
      "[Epoch: 21040] train loss: 1.1196, train acc: 0.5315, val loss: 1.1236, val acc: 0.5241  (best train acc: 0.5372, best val acc: 0.5342)\n",
      "[Epoch: 21060] train loss: 1.1367, train acc: 0.5285, val loss: 1.1229, val acc: 0.5255  (best train acc: 0.5372, best val acc: 0.5346)\n",
      "[Epoch: 21080] train loss: 1.1432, train acc: 0.5207, val loss: 1.1225, val acc: 0.5275  (best train acc: 0.5372, best val acc: 0.5346)\n",
      "[Epoch: 21100] train loss: 1.1396, train acc: 0.5188, val loss: 1.1230, val acc: 0.5298  (best train acc: 0.5372, best val acc: 0.5346)\n",
      "[Epoch: 21120] train loss: 1.1360, train acc: 0.5260, val loss: 1.1221, val acc: 0.5255  (best train acc: 0.5372, best val acc: 0.5346)\n",
      "[Epoch: 21140] train loss: 1.1312, train acc: 0.5335, val loss: 1.1214, val acc: 0.5285  (best train acc: 0.5372, best val acc: 0.5346)\n",
      "[Epoch: 21160] train loss: 1.1236, train acc: 0.5343, val loss: 1.1206, val acc: 0.5255  (best train acc: 0.5372, best val acc: 0.5346)\n",
      "[Epoch: 21180] train loss: 1.1243, train acc: 0.5276, val loss: 1.1203, val acc: 0.5302  (best train acc: 0.5372, best val acc: 0.5346)\n",
      "[Epoch: 21200] train loss: 1.1374, train acc: 0.5254, val loss: 1.1289, val acc: 0.5305  (best train acc: 0.5372, best val acc: 0.5346)\n",
      "[Epoch: 21220] train loss: 1.1378, train acc: 0.5252, val loss: 1.1249, val acc: 0.5268  (best train acc: 0.5372, best val acc: 0.5346)\n",
      "[Epoch: 21240] train loss: 1.1213, train acc: 0.5295, val loss: 1.1224, val acc: 0.5272  (best train acc: 0.5372, best val acc: 0.5346)\n",
      "[Epoch: 21260] train loss: 1.1218, train acc: 0.5283, val loss: 1.1244, val acc: 0.5231  (best train acc: 0.5372, best val acc: 0.5352)\n",
      "[Epoch: 21280] train loss: 1.1290, train acc: 0.5264, val loss: 1.1202, val acc: 0.5261  (best train acc: 0.5372, best val acc: 0.5352)\n",
      "[Epoch: 21300] train loss: 1.1208, train acc: 0.5303, val loss: 1.1199, val acc: 0.5298  (best train acc: 0.5372, best val acc: 0.5352)\n",
      "[Epoch: 21320] train loss: 1.1315, train acc: 0.5239, val loss: 1.1231, val acc: 0.5339  (best train acc: 0.5372, best val acc: 0.5366)\n",
      "[Epoch: 21340] train loss: 1.1294, train acc: 0.5285, val loss: 1.1230, val acc: 0.5282  (best train acc: 0.5372, best val acc: 0.5366)\n",
      "[Epoch: 21360] train loss: 1.1254, train acc: 0.5286, val loss: 1.1252, val acc: 0.5275  (best train acc: 0.5381, best val acc: 0.5366)\n",
      "[Epoch: 21380] train loss: 1.1310, train acc: 0.5227, val loss: 1.1260, val acc: 0.5288  (best train acc: 0.5381, best val acc: 0.5366)\n",
      "[Epoch: 21400] train loss: 1.1237, train acc: 0.5320, val loss: 1.1222, val acc: 0.5298  (best train acc: 0.5381, best val acc: 0.5369)\n",
      "[Epoch: 21420] train loss: 1.1194, train acc: 0.5338, val loss: 1.1263, val acc: 0.5194  (best train acc: 0.5381, best val acc: 0.5369)\n",
      "[Epoch: 21440] train loss: 1.1408, train acc: 0.5209, val loss: 1.1205, val acc: 0.5285  (best train acc: 0.5381, best val acc: 0.5369)\n",
      "[Epoch: 21460] train loss: 1.1351, train acc: 0.5210, val loss: 1.1286, val acc: 0.5285  (best train acc: 0.5381, best val acc: 0.5369)\n",
      "[Epoch: 21480] train loss: 1.1345, train acc: 0.5325, val loss: 1.1199, val acc: 0.5268  (best train acc: 0.5381, best val acc: 0.5369)\n",
      "[Epoch: 21500] train loss: 1.1230, train acc: 0.5351, val loss: 1.1188, val acc: 0.5322  (best train acc: 0.5381, best val acc: 0.5369)\n",
      "[Epoch: 21520] train loss: 1.1261, train acc: 0.5258, val loss: 1.1202, val acc: 0.5339  (best train acc: 0.5381, best val acc: 0.5369)\n",
      "[Epoch: 21540] train loss: 1.1402, train acc: 0.5202, val loss: 1.1279, val acc: 0.5278  (best train acc: 0.5381, best val acc: 0.5369)\n",
      "[Epoch: 21560] train loss: 1.1276, train acc: 0.5301, val loss: 1.1235, val acc: 0.5315  (best train acc: 0.5381, best val acc: 0.5369)\n",
      "[Epoch: 21580] train loss: 1.1249, train acc: 0.5326, val loss: 1.1184, val acc: 0.5352  (best train acc: 0.5381, best val acc: 0.5369)\n",
      "[Epoch: 21600] train loss: 1.1298, train acc: 0.5236, val loss: 1.1222, val acc: 0.5342  (best train acc: 0.5381, best val acc: 0.5369)\n",
      "[Epoch: 21620] train loss: 1.1346, train acc: 0.5309, val loss: 1.1204, val acc: 0.5329  (best train acc: 0.5381, best val acc: 0.5383)\n",
      "[Epoch: 21640] train loss: 1.1265, train acc: 0.5306, val loss: 1.1187, val acc: 0.5339  (best train acc: 0.5381, best val acc: 0.5390)\n",
      "[Epoch: 21660] train loss: 1.1224, train acc: 0.5283, val loss: 1.1208, val acc: 0.5292  (best train acc: 0.5381, best val acc: 0.5390)\n",
      "[Epoch: 21680] train loss: 1.1233, train acc: 0.5289, val loss: 1.1174, val acc: 0.5305  (best train acc: 0.5381, best val acc: 0.5390)\n",
      "[Epoch: 21700] train loss: 1.1195, train acc: 0.5330, val loss: 1.1174, val acc: 0.5329  (best train acc: 0.5381, best val acc: 0.5390)\n",
      "[Epoch: 21720] train loss: 1.1219, train acc: 0.5296, val loss: 1.1208, val acc: 0.5302  (best train acc: 0.5381, best val acc: 0.5390)\n",
      "[Epoch: 21740] train loss: 1.1299, train acc: 0.5249, val loss: 1.1185, val acc: 0.5336  (best train acc: 0.5381, best val acc: 0.5390)\n",
      "[Epoch: 21760] train loss: 1.1365, train acc: 0.5299, val loss: 1.1187, val acc: 0.5295  (best train acc: 0.5381, best val acc: 0.5393)\n",
      "[Epoch: 21780] train loss: 1.1231, train acc: 0.5320, val loss: 1.1180, val acc: 0.5315  (best train acc: 0.5381, best val acc: 0.5393)\n",
      "[Epoch: 21800] train loss: 1.1296, train acc: 0.5263, val loss: 1.1172, val acc: 0.5302  (best train acc: 0.5403, best val acc: 0.5393)\n",
      "[Epoch: 21820] train loss: 1.1235, train acc: 0.5262, val loss: 1.1188, val acc: 0.5346  (best train acc: 0.5403, best val acc: 0.5396)\n",
      "[Epoch: 21840] train loss: 1.1220, train acc: 0.5240, val loss: 1.1157, val acc: 0.5322  (best train acc: 0.5403, best val acc: 0.5403)\n",
      "[Epoch: 21860] train loss: 1.1171, train acc: 0.5340, val loss: 1.1170, val acc: 0.5265  (best train acc: 0.5403, best val acc: 0.5403)\n",
      "[Epoch: 21880] train loss: 1.1176, train acc: 0.5367, val loss: 1.1213, val acc: 0.5309  (best train acc: 0.5403, best val acc: 0.5403)\n",
      "[Epoch: 21900] train loss: 1.1166, train acc: 0.5379, val loss: 1.1162, val acc: 0.5302  (best train acc: 0.5403, best val acc: 0.5403)\n",
      "[Epoch: 21920] train loss: 1.1231, train acc: 0.5217, val loss: 1.1152, val acc: 0.5366  (best train acc: 0.5403, best val acc: 0.5403)\n",
      "[Epoch: 21940] train loss: 1.1075, train acc: 0.5303, val loss: 1.1176, val acc: 0.5363  (best train acc: 0.5403, best val acc: 0.5403)\n",
      "[Epoch: 21960] train loss: 1.1277, train acc: 0.5298, val loss: 1.1188, val acc: 0.5309  (best train acc: 0.5403, best val acc: 0.5403)\n",
      "[Epoch: 21980] train loss: 1.1212, train acc: 0.5288, val loss: 1.1175, val acc: 0.5255  (best train acc: 0.5403, best val acc: 0.5403)\n",
      "[Epoch: 22000] train loss: 1.1292, train acc: 0.5201, val loss: 1.1161, val acc: 0.5292  (best train acc: 0.5403, best val acc: 0.5403)\n",
      "[Epoch: 22020] train loss: 1.1222, train acc: 0.5301, val loss: 1.1183, val acc: 0.5241  (best train acc: 0.5403, best val acc: 0.5403)\n",
      "[Epoch: 22040] train loss: 1.1156, train acc: 0.5349, val loss: 1.1243, val acc: 0.5272  (best train acc: 0.5403, best val acc: 0.5403)\n",
      "[Epoch: 22060] train loss: 1.1143, train acc: 0.5257, val loss: 1.1211, val acc: 0.5234  (best train acc: 0.5403, best val acc: 0.5403)\n",
      "[Epoch: 22080] train loss: 1.1168, train acc: 0.5316, val loss: 1.1177, val acc: 0.5332  (best train acc: 0.5403, best val acc: 0.5403)\n",
      "[Epoch: 22100] train loss: 1.1174, train acc: 0.5280, val loss: 1.1143, val acc: 0.5315  (best train acc: 0.5403, best val acc: 0.5403)\n",
      "[Epoch: 22120] train loss: 1.1314, train acc: 0.5247, val loss: 1.1154, val acc: 0.5315  (best train acc: 0.5421, best val acc: 0.5403)\n",
      "[Epoch: 22140] train loss: 1.1222, train acc: 0.5368, val loss: 1.1194, val acc: 0.5325  (best train acc: 0.5421, best val acc: 0.5403)\n",
      "[Epoch: 22160] train loss: 1.1279, train acc: 0.5265, val loss: 1.1148, val acc: 0.5376  (best train acc: 0.5421, best val acc: 0.5406)\n",
      "[Epoch: 22180] train loss: 1.1181, train acc: 0.5343, val loss: 1.1173, val acc: 0.5258  (best train acc: 0.5421, best val acc: 0.5410)\n",
      "[Epoch: 22200] train loss: 1.1196, train acc: 0.5361, val loss: 1.1168, val acc: 0.5285  (best train acc: 0.5421, best val acc: 0.5423)\n",
      "[Epoch: 22220] train loss: 1.1248, train acc: 0.5330, val loss: 1.1149, val acc: 0.5292  (best train acc: 0.5421, best val acc: 0.5423)\n",
      "[Epoch: 22240] train loss: 1.1125, train acc: 0.5303, val loss: 1.1142, val acc: 0.5305  (best train acc: 0.5421, best val acc: 0.5423)\n",
      "[Epoch: 22260] train loss: 1.1165, train acc: 0.5313, val loss: 1.1166, val acc: 0.5349  (best train acc: 0.5421, best val acc: 0.5423)\n",
      "[Epoch: 22280] train loss: 1.1323, train acc: 0.5288, val loss: 1.1155, val acc: 0.5325  (best train acc: 0.5421, best val acc: 0.5423)\n",
      "[Epoch: 22300] train loss: 1.1300, train acc: 0.5249, val loss: 1.1145, val acc: 0.5342  (best train acc: 0.5421, best val acc: 0.5423)\n",
      "[Epoch: 22320] train loss: 1.1257, train acc: 0.5245, val loss: 1.1134, val acc: 0.5282  (best train acc: 0.5421, best val acc: 0.5423)\n",
      "[Epoch: 22340] train loss: 1.1166, train acc: 0.5289, val loss: 1.1123, val acc: 0.5366  (best train acc: 0.5421, best val acc: 0.5423)\n",
      "[Epoch: 22360] train loss: 1.1224, train acc: 0.5204, val loss: 1.1133, val acc: 0.5352  (best train acc: 0.5421, best val acc: 0.5423)\n",
      "[Epoch: 22380] train loss: 1.1138, train acc: 0.5347, val loss: 1.1204, val acc: 0.5339  (best train acc: 0.5421, best val acc: 0.5423)\n",
      "[Epoch: 22400] train loss: 1.1292, train acc: 0.5228, val loss: 1.1156, val acc: 0.5322  (best train acc: 0.5421, best val acc: 0.5423)\n",
      "[Epoch: 22420] train loss: 1.1270, train acc: 0.5346, val loss: 1.1145, val acc: 0.5336  (best train acc: 0.5421, best val acc: 0.5423)\n",
      "[Epoch: 22440] train loss: 1.1244, train acc: 0.5281, val loss: 1.1146, val acc: 0.5349  (best train acc: 0.5421, best val acc: 0.5423)\n",
      "[Epoch: 22460] train loss: 1.1184, train acc: 0.5286, val loss: 1.1128, val acc: 0.5376  (best train acc: 0.5421, best val acc: 0.5427)\n",
      "[Epoch: 22480] train loss: 1.1220, train acc: 0.5214, val loss: 1.1124, val acc: 0.5342  (best train acc: 0.5421, best val acc: 0.5427)\n",
      "[Epoch: 22500] train loss: 1.1289, train acc: 0.5228, val loss: 1.1149, val acc: 0.5312  (best train acc: 0.5421, best val acc: 0.5427)\n",
      "[Epoch: 22520] train loss: 1.1123, train acc: 0.5382, val loss: 1.1182, val acc: 0.5383  (best train acc: 0.5421, best val acc: 0.5427)\n",
      "[Epoch: 22540] train loss: 1.1153, train acc: 0.5359, val loss: 1.1171, val acc: 0.5346  (best train acc: 0.5438, best val acc: 0.5427)\n",
      "[Epoch: 22560] train loss: 1.1159, train acc: 0.5320, val loss: 1.1121, val acc: 0.5373  (best train acc: 0.5438, best val acc: 0.5427)\n",
      "[Epoch: 22580] train loss: 1.1275, train acc: 0.5287, val loss: 1.1124, val acc: 0.5369  (best train acc: 0.5438, best val acc: 0.5427)\n",
      "[Epoch: 22600] train loss: 1.1131, train acc: 0.5363, val loss: 1.1119, val acc: 0.5393  (best train acc: 0.5438, best val acc: 0.5427)\n",
      "[Epoch: 22620] train loss: 1.1251, train acc: 0.5317, val loss: 1.1171, val acc: 0.5400  (best train acc: 0.5438, best val acc: 0.5427)\n",
      "[Epoch: 22640] train loss: 1.1202, train acc: 0.5336, val loss: 1.1118, val acc: 0.5305  (best train acc: 0.5442, best val acc: 0.5427)\n",
      "[Epoch: 22660] train loss: 1.1283, train acc: 0.5258, val loss: 1.1169, val acc: 0.5373  (best train acc: 0.5442, best val acc: 0.5427)\n",
      "[Epoch: 22680] train loss: 1.1243, train acc: 0.5312, val loss: 1.1112, val acc: 0.5390  (best train acc: 0.5442, best val acc: 0.5427)\n",
      "[Epoch: 22700] train loss: 1.1133, train acc: 0.5318, val loss: 1.1131, val acc: 0.5342  (best train acc: 0.5442, best val acc: 0.5427)\n",
      "[Epoch: 22720] train loss: 1.1235, train acc: 0.5262, val loss: 1.1130, val acc: 0.5305  (best train acc: 0.5442, best val acc: 0.5427)\n",
      "[Epoch: 22740] train loss: 1.1180, train acc: 0.5346, val loss: 1.1151, val acc: 0.5339  (best train acc: 0.5442, best val acc: 0.5433)\n",
      "[Epoch: 22760] train loss: 1.1180, train acc: 0.5330, val loss: 1.1131, val acc: 0.5366  (best train acc: 0.5442, best val acc: 0.5433)\n",
      "[Epoch: 22780] train loss: 1.1118, train acc: 0.5370, val loss: 1.1108, val acc: 0.5366  (best train acc: 0.5442, best val acc: 0.5433)\n",
      "[Epoch: 22800] train loss: 1.1108, train acc: 0.5354, val loss: 1.1102, val acc: 0.5379  (best train acc: 0.5442, best val acc: 0.5433)\n",
      "[Epoch: 22820] train loss: 1.1223, train acc: 0.5242, val loss: 1.1130, val acc: 0.5325  (best train acc: 0.5442, best val acc: 0.5433)\n",
      "[Epoch: 22840] train loss: 1.1254, train acc: 0.5299, val loss: 1.1189, val acc: 0.5309  (best train acc: 0.5442, best val acc: 0.5433)\n",
      "[Epoch: 22860] train loss: 1.1180, train acc: 0.5273, val loss: 1.1099, val acc: 0.5359  (best train acc: 0.5442, best val acc: 0.5433)\n",
      "[Epoch: 22880] train loss: 1.1090, train acc: 0.5362, val loss: 1.1163, val acc: 0.5295  (best train acc: 0.5442, best val acc: 0.5433)\n",
      "[Epoch: 22900] train loss: 1.1209, train acc: 0.5317, val loss: 1.1141, val acc: 0.5396  (best train acc: 0.5446, best val acc: 0.5450)\n",
      "[Epoch: 22920] train loss: 1.1265, train acc: 0.5280, val loss: 1.1101, val acc: 0.5342  (best train acc: 0.5446, best val acc: 0.5450)\n",
      "[Epoch: 22940] train loss: 1.1231, train acc: 0.5265, val loss: 1.1142, val acc: 0.5366  (best train acc: 0.5446, best val acc: 0.5450)\n",
      "[Epoch: 22960] train loss: 1.1150, train acc: 0.5392, val loss: 1.1106, val acc: 0.5386  (best train acc: 0.5446, best val acc: 0.5450)\n",
      "[Epoch: 22980] train loss: 1.1248, train acc: 0.5299, val loss: 1.1283, val acc: 0.5106  (best train acc: 0.5446, best val acc: 0.5450)\n",
      "[Epoch: 23000] train loss: 1.1170, train acc: 0.5295, val loss: 1.1151, val acc: 0.5359  (best train acc: 0.5446, best val acc: 0.5450)\n",
      "[Epoch: 23020] train loss: 1.1253, train acc: 0.5300, val loss: 1.1130, val acc: 0.5410  (best train acc: 0.5446, best val acc: 0.5450)\n",
      "[Epoch: 23040] train loss: 1.1194, train acc: 0.5333, val loss: 1.1125, val acc: 0.5234  (best train acc: 0.5446, best val acc: 0.5450)\n",
      "[Epoch: 23060] train loss: 1.1236, train acc: 0.5328, val loss: 1.1097, val acc: 0.5359  (best train acc: 0.5446, best val acc: 0.5450)\n",
      "[Epoch: 23080] train loss: 1.1014, train acc: 0.5369, val loss: 1.1137, val acc: 0.5410  (best train acc: 0.5446, best val acc: 0.5450)\n",
      "[Epoch: 23100] train loss: 1.1236, train acc: 0.5280, val loss: 1.1089, val acc: 0.5376  (best train acc: 0.5446, best val acc: 0.5450)\n",
      "[Epoch: 23120] train loss: 1.1152, train acc: 0.5360, val loss: 1.1108, val acc: 0.5325  (best train acc: 0.5455, best val acc: 0.5450)\n",
      "[Epoch: 23140] train loss: 1.1167, train acc: 0.5305, val loss: 1.1099, val acc: 0.5396  (best train acc: 0.5455, best val acc: 0.5450)\n",
      "[Epoch: 23160] train loss: 1.1341, train acc: 0.5213, val loss: 1.1095, val acc: 0.5352  (best train acc: 0.5455, best val acc: 0.5450)\n",
      "[Epoch: 23180] train loss: 1.1343, train acc: 0.5288, val loss: 1.1125, val acc: 0.5285  (best train acc: 0.5455, best val acc: 0.5450)\n",
      "[Epoch: 23200] train loss: 1.1259, train acc: 0.5234, val loss: 1.1098, val acc: 0.5302  (best train acc: 0.5455, best val acc: 0.5450)\n",
      "[Epoch: 23220] train loss: 1.1240, train acc: 0.5323, val loss: 1.1137, val acc: 0.5248  (best train acc: 0.5455, best val acc: 0.5450)\n",
      "[Epoch: 23240] train loss: 1.1062, train acc: 0.5366, val loss: 1.1121, val acc: 0.5251  (best train acc: 0.5455, best val acc: 0.5450)\n",
      "[Epoch: 23260] train loss: 1.1169, train acc: 0.5302, val loss: 1.1112, val acc: 0.5349  (best train acc: 0.5455, best val acc: 0.5450)\n",
      "[Epoch: 23280] train loss: 1.1222, train acc: 0.5275, val loss: 1.1189, val acc: 0.5322  (best train acc: 0.5455, best val acc: 0.5450)\n",
      "[Epoch: 23300] train loss: 1.1019, train acc: 0.5386, val loss: 1.1146, val acc: 0.5346  (best train acc: 0.5455, best val acc: 0.5450)\n",
      "[Epoch: 23320] train loss: 1.1108, train acc: 0.5319, val loss: 1.1125, val acc: 0.5420  (best train acc: 0.5455, best val acc: 0.5450)\n",
      "[Epoch: 23340] train loss: 1.1158, train acc: 0.5374, val loss: 1.1113, val acc: 0.5393  (best train acc: 0.5455, best val acc: 0.5450)\n",
      "[Epoch: 23360] train loss: 1.1169, train acc: 0.5402, val loss: 1.1089, val acc: 0.5379  (best train acc: 0.5455, best val acc: 0.5454)\n",
      "[Epoch: 23380] train loss: 1.1184, train acc: 0.5337, val loss: 1.1165, val acc: 0.5285  (best train acc: 0.5455, best val acc: 0.5454)\n",
      "[Epoch: 23400] train loss: 1.1201, train acc: 0.5325, val loss: 1.1089, val acc: 0.5376  (best train acc: 0.5455, best val acc: 0.5454)\n",
      "[Epoch: 23420] train loss: 1.1213, train acc: 0.5372, val loss: 1.1111, val acc: 0.5349  (best train acc: 0.5455, best val acc: 0.5454)\n",
      "[Epoch: 23440] train loss: 1.1112, train acc: 0.5340, val loss: 1.1084, val acc: 0.5400  (best train acc: 0.5455, best val acc: 0.5454)\n",
      "[Epoch: 23460] train loss: 1.1242, train acc: 0.5299, val loss: 1.1083, val acc: 0.5383  (best train acc: 0.5455, best val acc: 0.5454)\n",
      "[Epoch: 23480] train loss: 1.1127, train acc: 0.5373, val loss: 1.1077, val acc: 0.5403  (best train acc: 0.5455, best val acc: 0.5454)\n",
      "[Epoch: 23500] train loss: 1.1262, train acc: 0.5245, val loss: 1.1082, val acc: 0.5403  (best train acc: 0.5455, best val acc: 0.5454)\n",
      "[Epoch: 23520] train loss: 1.1169, train acc: 0.5309, val loss: 1.1090, val acc: 0.5393  (best train acc: 0.5459, best val acc: 0.5454)\n",
      "[Epoch: 23540] train loss: 1.1319, train acc: 0.5307, val loss: 1.1100, val acc: 0.5444  (best train acc: 0.5459, best val acc: 0.5454)\n",
      "[Epoch: 23560] train loss: 1.1143, train acc: 0.5331, val loss: 1.1076, val acc: 0.5396  (best train acc: 0.5459, best val acc: 0.5454)\n",
      "[Epoch: 23580] train loss: 1.1232, train acc: 0.5277, val loss: 1.1103, val acc: 0.5450  (best train acc: 0.5459, best val acc: 0.5454)\n",
      "[Epoch: 23600] train loss: 1.1272, train acc: 0.5253, val loss: 1.1085, val acc: 0.5393  (best train acc: 0.5459, best val acc: 0.5460)\n",
      "[Epoch: 23620] train loss: 1.1007, train acc: 0.5382, val loss: 1.1136, val acc: 0.5197  (best train acc: 0.5459, best val acc: 0.5460)\n",
      "[Epoch: 23640] train loss: 1.1123, train acc: 0.5263, val loss: 1.1104, val acc: 0.5427  (best train acc: 0.5459, best val acc: 0.5460)\n",
      "[Epoch: 23660] train loss: 1.1211, train acc: 0.5344, val loss: 1.1065, val acc: 0.5373  (best train acc: 0.5459, best val acc: 0.5460)\n",
      "[Epoch: 23680] train loss: 1.0968, train acc: 0.5414, val loss: 1.1065, val acc: 0.5363  (best train acc: 0.5459, best val acc: 0.5460)\n",
      "[Epoch: 23700] train loss: 1.1284, train acc: 0.5320, val loss: 1.1071, val acc: 0.5312  (best train acc: 0.5459, best val acc: 0.5460)\n",
      "[Epoch: 23720] train loss: 1.1169, train acc: 0.5318, val loss: 1.1055, val acc: 0.5427  (best train acc: 0.5459, best val acc: 0.5460)\n",
      "[Epoch: 23740] train loss: 1.1091, train acc: 0.5424, val loss: 1.1092, val acc: 0.5369  (best train acc: 0.5459, best val acc: 0.5460)\n",
      "[Epoch: 23760] train loss: 1.1035, train acc: 0.5368, val loss: 1.1058, val acc: 0.5406  (best train acc: 0.5459, best val acc: 0.5460)\n",
      "[Epoch: 23780] train loss: 1.1194, train acc: 0.5335, val loss: 1.1079, val acc: 0.5413  (best train acc: 0.5459, best val acc: 0.5460)\n",
      "[Epoch: 23800] train loss: 1.1191, train acc: 0.5307, val loss: 1.1063, val acc: 0.5403  (best train acc: 0.5459, best val acc: 0.5460)\n",
      "[Epoch: 23820] train loss: 1.1297, train acc: 0.5228, val loss: 1.1090, val acc: 0.5258  (best train acc: 0.5459, best val acc: 0.5460)\n",
      "[Epoch: 23840] train loss: 1.1231, train acc: 0.5294, val loss: 1.1089, val acc: 0.5261  (best train acc: 0.5459, best val acc: 0.5460)\n",
      "[Epoch: 23860] train loss: 1.1024, train acc: 0.5374, val loss: 1.1078, val acc: 0.5268  (best train acc: 0.5459, best val acc: 0.5460)\n",
      "[Epoch: 23880] train loss: 1.1247, train acc: 0.5309, val loss: 1.1116, val acc: 0.5285  (best train acc: 0.5471, best val acc: 0.5460)\n",
      "[Epoch: 23900] train loss: 1.1087, train acc: 0.5418, val loss: 1.1067, val acc: 0.5406  (best train acc: 0.5471, best val acc: 0.5460)\n",
      "[Epoch: 23920] train loss: 1.1159, train acc: 0.5219, val loss: 1.1083, val acc: 0.5379  (best train acc: 0.5471, best val acc: 0.5460)\n",
      "[Epoch: 23940] train loss: 1.1163, train acc: 0.5301, val loss: 1.1060, val acc: 0.5403  (best train acc: 0.5471, best val acc: 0.5460)\n",
      "[Epoch: 23960] train loss: 1.1122, train acc: 0.5322, val loss: 1.1059, val acc: 0.5369  (best train acc: 0.5471, best val acc: 0.5460)\n",
      "[Epoch: 23980] train loss: 1.0990, train acc: 0.5405, val loss: 1.1048, val acc: 0.5390  (best train acc: 0.5471, best val acc: 0.5460)\n",
      "[Epoch: 24000] train loss: 1.1231, train acc: 0.5264, val loss: 1.1052, val acc: 0.5410  (best train acc: 0.5500, best val acc: 0.5460)\n",
      "[Epoch: 24020] train loss: 1.1293, train acc: 0.5276, val loss: 1.1045, val acc: 0.5403  (best train acc: 0.5500, best val acc: 0.5460)\n",
      "[Epoch: 24040] train loss: 1.1087, train acc: 0.5275, val loss: 1.1077, val acc: 0.5325  (best train acc: 0.5500, best val acc: 0.5460)\n",
      "[Epoch: 24060] train loss: 1.1166, train acc: 0.5325, val loss: 1.1086, val acc: 0.5460  (best train acc: 0.5500, best val acc: 0.5460)\n",
      "[Epoch: 24080] train loss: 1.0987, train acc: 0.5432, val loss: 1.1075, val acc: 0.5396  (best train acc: 0.5500, best val acc: 0.5460)\n",
      "[Epoch: 24100] train loss: 1.1116, train acc: 0.5359, val loss: 1.1051, val acc: 0.5403  (best train acc: 0.5500, best val acc: 0.5460)\n",
      "[Epoch: 24120] train loss: 1.1128, train acc: 0.5326, val loss: 1.1073, val acc: 0.5285  (best train acc: 0.5500, best val acc: 0.5460)\n",
      "[Epoch: 24140] train loss: 1.1117, train acc: 0.5401, val loss: 1.1082, val acc: 0.5342  (best train acc: 0.5500, best val acc: 0.5460)\n",
      "[Epoch: 24160] train loss: 1.1225, train acc: 0.5305, val loss: 1.1055, val acc: 0.5332  (best train acc: 0.5500, best val acc: 0.5460)\n",
      "[Epoch: 24180] train loss: 1.1107, train acc: 0.5266, val loss: 1.1057, val acc: 0.5309  (best train acc: 0.5500, best val acc: 0.5460)\n",
      "[Epoch: 24200] train loss: 1.1192, train acc: 0.5335, val loss: 1.1179, val acc: 0.5315  (best train acc: 0.5500, best val acc: 0.5460)\n",
      "[Epoch: 24220] train loss: 1.0969, train acc: 0.5439, val loss: 1.1101, val acc: 0.5288  (best train acc: 0.5500, best val acc: 0.5460)\n",
      "[Epoch: 24240] train loss: 1.1008, train acc: 0.5325, val loss: 1.1041, val acc: 0.5396  (best train acc: 0.5500, best val acc: 0.5464)\n",
      "[Epoch: 24260] train loss: 1.1189, train acc: 0.5202, val loss: 1.1042, val acc: 0.5312  (best train acc: 0.5500, best val acc: 0.5464)\n",
      "[Epoch: 24280] train loss: 1.1072, train acc: 0.5388, val loss: 1.1030, val acc: 0.5379  (best train acc: 0.5500, best val acc: 0.5474)\n",
      "[Epoch: 24300] train loss: 1.1159, train acc: 0.5314, val loss: 1.1046, val acc: 0.5410  (best train acc: 0.5500, best val acc: 0.5474)\n",
      "[Epoch: 24320] train loss: 1.1217, train acc: 0.5311, val loss: 1.1052, val acc: 0.5403  (best train acc: 0.5500, best val acc: 0.5474)\n",
      "[Epoch: 24340] train loss: 1.0936, train acc: 0.5411, val loss: 1.1056, val acc: 0.5386  (best train acc: 0.5500, best val acc: 0.5474)\n",
      "[Epoch: 24360] train loss: 1.1101, train acc: 0.5322, val loss: 1.1036, val acc: 0.5396  (best train acc: 0.5500, best val acc: 0.5474)\n",
      "[Epoch: 24380] train loss: 1.1365, train acc: 0.5147, val loss: 1.1103, val acc: 0.5403  (best train acc: 0.5500, best val acc: 0.5477)\n",
      "[Epoch: 24400] train loss: 1.1136, train acc: 0.5229, val loss: 1.1109, val acc: 0.5363  (best train acc: 0.5500, best val acc: 0.5477)\n",
      "[Epoch: 24420] train loss: 1.1065, train acc: 0.5434, val loss: 1.1039, val acc: 0.5363  (best train acc: 0.5500, best val acc: 0.5477)\n",
      "[Epoch: 24440] train loss: 1.1088, train acc: 0.5301, val loss: 1.1043, val acc: 0.5410  (best train acc: 0.5500, best val acc: 0.5477)\n",
      "[Epoch: 24460] train loss: 1.1015, train acc: 0.5369, val loss: 1.1109, val acc: 0.5349  (best train acc: 0.5500, best val acc: 0.5477)\n",
      "[Epoch: 24480] train loss: 1.1150, train acc: 0.5369, val loss: 1.1036, val acc: 0.5406  (best train acc: 0.5500, best val acc: 0.5477)\n",
      "[Epoch: 24500] train loss: 1.1193, train acc: 0.5297, val loss: 1.1126, val acc: 0.5352  (best train acc: 0.5500, best val acc: 0.5477)\n",
      "[Epoch: 24520] train loss: 1.1013, train acc: 0.5454, val loss: 1.1028, val acc: 0.5366  (best train acc: 0.5500, best val acc: 0.5477)\n",
      "[Epoch: 24540] train loss: 1.1199, train acc: 0.5312, val loss: 1.1201, val acc: 0.5379  (best train acc: 0.5500, best val acc: 0.5477)\n",
      "[Epoch: 24560] train loss: 1.0999, train acc: 0.5409, val loss: 1.1080, val acc: 0.5430  (best train acc: 0.5500, best val acc: 0.5477)\n",
      "[Epoch: 24580] train loss: 1.1116, train acc: 0.5299, val loss: 1.1023, val acc: 0.5403  (best train acc: 0.5500, best val acc: 0.5477)\n",
      "[Epoch: 24600] train loss: 1.1106, train acc: 0.5328, val loss: 1.1018, val acc: 0.5403  (best train acc: 0.5500, best val acc: 0.5477)\n",
      "[Epoch: 24620] train loss: 1.1209, train acc: 0.5364, val loss: 1.1087, val acc: 0.5396  (best train acc: 0.5500, best val acc: 0.5497)\n",
      "[Epoch: 24640] train loss: 1.1143, train acc: 0.5377, val loss: 1.1030, val acc: 0.5393  (best train acc: 0.5500, best val acc: 0.5497)\n",
      "[Epoch: 24660] train loss: 1.1005, train acc: 0.5393, val loss: 1.1041, val acc: 0.5359  (best train acc: 0.5500, best val acc: 0.5497)\n",
      "[Epoch: 24680] train loss: 1.1302, train acc: 0.5207, val loss: 1.1026, val acc: 0.5369  (best train acc: 0.5500, best val acc: 0.5497)\n",
      "[Epoch: 24700] train loss: 1.0989, train acc: 0.5438, val loss: 1.1016, val acc: 0.5373  (best train acc: 0.5500, best val acc: 0.5497)\n",
      "[Epoch: 24720] train loss: 1.1182, train acc: 0.5337, val loss: 1.1022, val acc: 0.5352  (best train acc: 0.5500, best val acc: 0.5497)\n",
      "[Epoch: 24740] train loss: 1.1311, train acc: 0.5269, val loss: 1.1013, val acc: 0.5403  (best train acc: 0.5500, best val acc: 0.5511)\n",
      "[Epoch: 24760] train loss: 1.1211, train acc: 0.5226, val loss: 1.1071, val acc: 0.5423  (best train acc: 0.5500, best val acc: 0.5511)\n",
      "[Epoch: 24780] train loss: 1.1003, train acc: 0.5398, val loss: 1.1012, val acc: 0.5363  (best train acc: 0.5500, best val acc: 0.5511)\n",
      "[Epoch: 24800] train loss: 1.1100, train acc: 0.5317, val loss: 1.1018, val acc: 0.5359  (best train acc: 0.5500, best val acc: 0.5511)\n",
      "[Epoch: 24820] train loss: 1.1033, train acc: 0.5398, val loss: 1.1107, val acc: 0.5268  (best train acc: 0.5500, best val acc: 0.5511)\n",
      "[Epoch: 24840] train loss: 1.1157, train acc: 0.5301, val loss: 1.1056, val acc: 0.5322  (best train acc: 0.5500, best val acc: 0.5511)\n",
      "[Epoch: 24860] train loss: 1.1093, train acc: 0.5356, val loss: 1.1029, val acc: 0.5427  (best train acc: 0.5500, best val acc: 0.5511)\n",
      "[Epoch: 24880] train loss: 1.1019, train acc: 0.5429, val loss: 1.1015, val acc: 0.5440  (best train acc: 0.5500, best val acc: 0.5511)\n",
      "[Epoch: 24900] train loss: 1.1055, train acc: 0.5369, val loss: 1.1016, val acc: 0.5369  (best train acc: 0.5500, best val acc: 0.5518)\n",
      "[Epoch: 24920] train loss: 1.0998, train acc: 0.5387, val loss: 1.1014, val acc: 0.5400  (best train acc: 0.5500, best val acc: 0.5518)\n",
      "[Epoch: 24940] train loss: 1.1105, train acc: 0.5417, val loss: 1.1027, val acc: 0.5373  (best train acc: 0.5500, best val acc: 0.5518)\n",
      "[Epoch: 24960] train loss: 1.1084, train acc: 0.5395, val loss: 1.1035, val acc: 0.5437  (best train acc: 0.5500, best val acc: 0.5518)\n",
      "[Epoch: 24980] train loss: 1.1138, train acc: 0.5320, val loss: 1.1000, val acc: 0.5396  (best train acc: 0.5500, best val acc: 0.5518)\n",
      "[Epoch: 25000] train loss: 1.1095, train acc: 0.5380, val loss: 1.1022, val acc: 0.5322  (best train acc: 0.5500, best val acc: 0.5518)\n",
      "[Epoch: 25020] train loss: 1.1089, train acc: 0.5348, val loss: 1.1021, val acc: 0.5474  (best train acc: 0.5500, best val acc: 0.5518)\n",
      "[Epoch: 25040] train loss: 1.1017, train acc: 0.5433, val loss: 1.1040, val acc: 0.5447  (best train acc: 0.5500, best val acc: 0.5518)\n",
      "[Epoch: 25060] train loss: 1.0942, train acc: 0.5417, val loss: 1.1003, val acc: 0.5366  (best train acc: 0.5500, best val acc: 0.5518)\n",
      "[Epoch: 25080] train loss: 1.1066, train acc: 0.5296, val loss: 1.1011, val acc: 0.5393  (best train acc: 0.5528, best val acc: 0.5518)\n",
      "[Epoch: 25100] train loss: 1.1017, train acc: 0.5343, val loss: 1.1060, val acc: 0.5379  (best train acc: 0.5528, best val acc: 0.5518)\n",
      "[Epoch: 25120] train loss: 1.1263, train acc: 0.5369, val loss: 1.1031, val acc: 0.5352  (best train acc: 0.5528, best val acc: 0.5518)\n",
      "[Epoch: 25140] train loss: 1.1128, train acc: 0.5320, val loss: 1.1015, val acc: 0.5390  (best train acc: 0.5528, best val acc: 0.5518)\n",
      "[Epoch: 25160] train loss: 1.1227, train acc: 0.5303, val loss: 1.1029, val acc: 0.5376  (best train acc: 0.5528, best val acc: 0.5518)\n",
      "[Epoch: 25180] train loss: 1.1074, train acc: 0.5337, val loss: 1.0999, val acc: 0.5356  (best train acc: 0.5528, best val acc: 0.5518)\n",
      "[Epoch: 25200] train loss: 1.1084, train acc: 0.5414, val loss: 1.1012, val acc: 0.5379  (best train acc: 0.5528, best val acc: 0.5518)\n",
      "[Epoch: 25220] train loss: 1.0964, train acc: 0.5398, val loss: 1.1003, val acc: 0.5406  (best train acc: 0.5528, best val acc: 0.5518)\n",
      "[Epoch: 25240] train loss: 1.1213, train acc: 0.5297, val loss: 1.1013, val acc: 0.5383  (best train acc: 0.5528, best val acc: 0.5518)\n",
      "[Epoch: 25260] train loss: 1.1145, train acc: 0.5284, val loss: 1.1000, val acc: 0.5417  (best train acc: 0.5528, best val acc: 0.5518)\n",
      "[Epoch: 25280] train loss: 1.1201, train acc: 0.5314, val loss: 1.1027, val acc: 0.5474  (best train acc: 0.5528, best val acc: 0.5518)\n",
      "[Epoch: 25300] train loss: 1.1099, train acc: 0.5386, val loss: 1.0999, val acc: 0.5420  (best train acc: 0.5528, best val acc: 0.5518)\n",
      "[Epoch: 25320] train loss: 1.1049, train acc: 0.5383, val loss: 1.1025, val acc: 0.5369  (best train acc: 0.5528, best val acc: 0.5518)\n",
      "[Epoch: 25340] train loss: 1.1238, train acc: 0.5349, val loss: 1.0995, val acc: 0.5383  (best train acc: 0.5528, best val acc: 0.5518)\n",
      "[Epoch: 25360] train loss: 1.0995, train acc: 0.5413, val loss: 1.1050, val acc: 0.5322  (best train acc: 0.5528, best val acc: 0.5518)\n",
      "[Epoch: 25380] train loss: 1.1133, train acc: 0.5271, val loss: 1.0980, val acc: 0.5440  (best train acc: 0.5528, best val acc: 0.5518)\n",
      "[Epoch: 25400] train loss: 1.1094, train acc: 0.5312, val loss: 1.1028, val acc: 0.5508  (best train acc: 0.5528, best val acc: 0.5518)\n",
      "[Epoch: 25420] train loss: 1.1070, train acc: 0.5383, val loss: 1.1022, val acc: 0.5319  (best train acc: 0.5528, best val acc: 0.5518)\n",
      "[Epoch: 25440] train loss: 1.1055, train acc: 0.5367, val loss: 1.0988, val acc: 0.5390  (best train acc: 0.5528, best val acc: 0.5518)\n",
      "[Epoch: 25460] train loss: 1.0984, train acc: 0.5421, val loss: 1.1010, val acc: 0.5410  (best train acc: 0.5528, best val acc: 0.5518)\n",
      "[Epoch: 25480] train loss: 1.1091, train acc: 0.5344, val loss: 1.0992, val acc: 0.5440  (best train acc: 0.5528, best val acc: 0.5518)\n",
      "[Epoch: 25500] train loss: 1.1217, train acc: 0.5324, val loss: 1.1030, val acc: 0.5484  (best train acc: 0.5528, best val acc: 0.5518)\n",
      "[Epoch: 25520] train loss: 1.1168, train acc: 0.5284, val loss: 1.0995, val acc: 0.5393  (best train acc: 0.5528, best val acc: 0.5518)\n",
      "[Epoch: 25540] train loss: 1.1048, train acc: 0.5406, val loss: 1.1020, val acc: 0.5339  (best train acc: 0.5528, best val acc: 0.5518)\n",
      "[Epoch: 25560] train loss: 1.1166, train acc: 0.5288, val loss: 1.0993, val acc: 0.5504  (best train acc: 0.5528, best val acc: 0.5518)\n",
      "[Epoch: 25580] train loss: 1.0985, train acc: 0.5395, val loss: 1.0985, val acc: 0.5356  (best train acc: 0.5528, best val acc: 0.5518)\n",
      "[Epoch: 25600] train loss: 1.1052, train acc: 0.5275, val loss: 1.1023, val acc: 0.5305  (best train acc: 0.5528, best val acc: 0.5518)\n",
      "[Epoch: 25620] train loss: 1.1001, train acc: 0.5330, val loss: 1.0984, val acc: 0.5386  (best train acc: 0.5528, best val acc: 0.5518)\n",
      "[Epoch: 25640] train loss: 1.1102, train acc: 0.5395, val loss: 1.0982, val acc: 0.5369  (best train acc: 0.5528, best val acc: 0.5518)\n",
      "[Epoch: 25660] train loss: 1.1016, train acc: 0.5432, val loss: 1.0966, val acc: 0.5403  (best train acc: 0.5528, best val acc: 0.5518)\n",
      "[Epoch: 25680] train loss: 1.1018, train acc: 0.5366, val loss: 1.1059, val acc: 0.5261  (best train acc: 0.5528, best val acc: 0.5518)\n",
      "[Epoch: 25700] train loss: 1.1153, train acc: 0.5246, val loss: 1.1082, val acc: 0.5214  (best train acc: 0.5528, best val acc: 0.5518)\n",
      "[Epoch: 25720] train loss: 1.1061, train acc: 0.5416, val loss: 1.0981, val acc: 0.5430  (best train acc: 0.5528, best val acc: 0.5518)\n",
      "[Epoch: 25740] train loss: 1.1108, train acc: 0.5283, val loss: 1.0998, val acc: 0.5433  (best train acc: 0.5528, best val acc: 0.5518)\n",
      "[Epoch: 25760] train loss: 1.0959, train acc: 0.5437, val loss: 1.0998, val acc: 0.5420  (best train acc: 0.5528, best val acc: 0.5518)\n",
      "[Epoch: 25780] train loss: 1.1187, train acc: 0.5314, val loss: 1.0968, val acc: 0.5393  (best train acc: 0.5528, best val acc: 0.5518)\n",
      "[Epoch: 25800] train loss: 1.0975, train acc: 0.5420, val loss: 1.1120, val acc: 0.5248  (best train acc: 0.5528, best val acc: 0.5518)\n",
      "[Epoch: 25820] train loss: 1.1241, train acc: 0.5281, val loss: 1.1060, val acc: 0.5413  (best train acc: 0.5528, best val acc: 0.5518)\n",
      "[Epoch: 25840] train loss: 1.0856, train acc: 0.5476, val loss: 1.1301, val acc: 0.5383  (best train acc: 0.5528, best val acc: 0.5518)\n",
      "[Epoch: 25860] train loss: 1.1030, train acc: 0.5309, val loss: 1.0975, val acc: 0.5423  (best train acc: 0.5528, best val acc: 0.5518)\n",
      "[Epoch: 25880] train loss: 1.1259, train acc: 0.5261, val loss: 1.0977, val acc: 0.5454  (best train acc: 0.5529, best val acc: 0.5518)\n",
      "[Epoch: 25900] train loss: 1.1083, train acc: 0.5411, val loss: 1.0998, val acc: 0.5322  (best train acc: 0.5529, best val acc: 0.5518)\n",
      "[Epoch: 25920] train loss: 1.1215, train acc: 0.5305, val loss: 1.0974, val acc: 0.5454  (best train acc: 0.5529, best val acc: 0.5518)\n",
      "[Epoch: 25940] train loss: 1.1140, train acc: 0.5357, val loss: 1.1044, val acc: 0.5312  (best train acc: 0.5529, best val acc: 0.5518)\n",
      "[Epoch: 25960] train loss: 1.0916, train acc: 0.5425, val loss: 1.0959, val acc: 0.5390  (best train acc: 0.5529, best val acc: 0.5518)\n",
      "[Epoch: 25980] train loss: 1.1055, train acc: 0.5317, val loss: 1.0976, val acc: 0.5450  (best train acc: 0.5529, best val acc: 0.5518)\n",
      "[Epoch: 26000] train loss: 1.1166, train acc: 0.5337, val loss: 1.0956, val acc: 0.5417  (best train acc: 0.5529, best val acc: 0.5518)\n",
      "[Epoch: 26020] train loss: 1.1196, train acc: 0.5267, val loss: 1.0984, val acc: 0.5467  (best train acc: 0.5529, best val acc: 0.5518)\n",
      "[Epoch: 26040] train loss: 1.1122, train acc: 0.5364, val loss: 1.0984, val acc: 0.5464  (best train acc: 0.5529, best val acc: 0.5518)\n",
      "[Epoch: 26060] train loss: 1.1132, train acc: 0.5358, val loss: 1.0992, val acc: 0.5410  (best train acc: 0.5529, best val acc: 0.5518)\n",
      "[Epoch: 26080] train loss: 1.0961, train acc: 0.5374, val loss: 1.0994, val acc: 0.5487  (best train acc: 0.5529, best val acc: 0.5518)\n",
      "[Epoch: 26100] train loss: 1.0997, train acc: 0.5376, val loss: 1.0972, val acc: 0.5433  (best train acc: 0.5529, best val acc: 0.5518)\n",
      "[Epoch: 26120] train loss: 1.1112, train acc: 0.5361, val loss: 1.0948, val acc: 0.5430  (best train acc: 0.5529, best val acc: 0.5518)\n",
      "[Epoch: 26140] train loss: 1.1003, train acc: 0.5420, val loss: 1.1001, val acc: 0.5275  (best train acc: 0.5529, best val acc: 0.5518)\n",
      "[Epoch: 26160] train loss: 1.1115, train acc: 0.5377, val loss: 1.1083, val acc: 0.5309  (best train acc: 0.5529, best val acc: 0.5518)\n",
      "[Epoch: 26180] train loss: 1.1018, train acc: 0.5358, val loss: 1.0995, val acc: 0.5342  (best train acc: 0.5529, best val acc: 0.5518)\n",
      "[Epoch: 26200] train loss: 1.1062, train acc: 0.5343, val loss: 1.0988, val acc: 0.5460  (best train acc: 0.5529, best val acc: 0.5518)\n",
      "[Epoch: 26220] train loss: 1.1071, train acc: 0.5338, val loss: 1.0953, val acc: 0.5383  (best train acc: 0.5529, best val acc: 0.5518)\n",
      "[Epoch: 26240] train loss: 1.1093, train acc: 0.5336, val loss: 1.1047, val acc: 0.5410  (best train acc: 0.5530, best val acc: 0.5518)\n",
      "[Epoch: 26260] train loss: 1.1106, train acc: 0.5378, val loss: 1.0949, val acc: 0.5400  (best train acc: 0.5530, best val acc: 0.5518)\n",
      "[Epoch: 26280] train loss: 1.1061, train acc: 0.5304, val loss: 1.0949, val acc: 0.5457  (best train acc: 0.5530, best val acc: 0.5518)\n",
      "[Epoch: 26300] train loss: 1.0959, train acc: 0.5401, val loss: 1.0938, val acc: 0.5440  (best train acc: 0.5530, best val acc: 0.5518)\n",
      "[Epoch: 26320] train loss: 1.1138, train acc: 0.5313, val loss: 1.0946, val acc: 0.5423  (best train acc: 0.5530, best val acc: 0.5518)\n",
      "[Epoch: 26340] train loss: 1.0973, train acc: 0.5433, val loss: 1.0959, val acc: 0.5457  (best train acc: 0.5530, best val acc: 0.5518)\n",
      "[Epoch: 26360] train loss: 1.1102, train acc: 0.5377, val loss: 1.0963, val acc: 0.5363  (best train acc: 0.5530, best val acc: 0.5518)\n",
      "[Epoch: 26380] train loss: 1.1246, train acc: 0.5330, val loss: 1.1233, val acc: 0.5406  (best train acc: 0.5530, best val acc: 0.5518)\n",
      "[Epoch: 26400] train loss: 1.1093, train acc: 0.5424, val loss: 1.1150, val acc: 0.5238  (best train acc: 0.5530, best val acc: 0.5518)\n",
      "[Epoch: 26420] train loss: 1.1255, train acc: 0.5276, val loss: 1.0994, val acc: 0.5403  (best train acc: 0.5530, best val acc: 0.5518)\n",
      "[Epoch: 26440] train loss: 1.1061, train acc: 0.5429, val loss: 1.0952, val acc: 0.5390  (best train acc: 0.5530, best val acc: 0.5518)\n",
      "[Epoch: 26460] train loss: 1.1071, train acc: 0.5398, val loss: 1.0942, val acc: 0.5420  (best train acc: 0.5530, best val acc: 0.5518)\n",
      "[Epoch: 26480] train loss: 1.1139, train acc: 0.5333, val loss: 1.0956, val acc: 0.5474  (best train acc: 0.5530, best val acc: 0.5518)\n",
      "[Epoch: 26500] train loss: 1.0988, train acc: 0.5462, val loss: 1.0955, val acc: 0.5440  (best train acc: 0.5530, best val acc: 0.5518)\n",
      "[Epoch: 26520] train loss: 1.1052, train acc: 0.5358, val loss: 1.0972, val acc: 0.5491  (best train acc: 0.5530, best val acc: 0.5518)\n",
      "[Epoch: 26540] train loss: 1.1210, train acc: 0.5303, val loss: 1.0955, val acc: 0.5349  (best train acc: 0.5530, best val acc: 0.5518)\n",
      "[Epoch: 26560] train loss: 1.1024, train acc: 0.5414, val loss: 1.0968, val acc: 0.5491  (best train acc: 0.5530, best val acc: 0.5518)\n",
      "[Epoch: 26580] train loss: 1.0981, train acc: 0.5402, val loss: 1.0973, val acc: 0.5396  (best train acc: 0.5530, best val acc: 0.5518)\n",
      "[Epoch: 26600] train loss: 1.1080, train acc: 0.5355, val loss: 1.0949, val acc: 0.5386  (best train acc: 0.5530, best val acc: 0.5518)\n",
      "[Epoch: 26620] train loss: 1.1416, train acc: 0.5176, val loss: 1.1130, val acc: 0.5447  (best train acc: 0.5530, best val acc: 0.5518)\n",
      "[Epoch: 26640] train loss: 1.1293, train acc: 0.5268, val loss: 1.1046, val acc: 0.5379  (best train acc: 0.5530, best val acc: 0.5518)\n",
      "[Epoch: 26660] train loss: 1.0998, train acc: 0.5374, val loss: 1.0935, val acc: 0.5444  (best train acc: 0.5530, best val acc: 0.5518)\n",
      "[Epoch: 26680] train loss: 1.0921, train acc: 0.5406, val loss: 1.0954, val acc: 0.5410  (best train acc: 0.5530, best val acc: 0.5518)\n",
      "[Epoch: 26700] train loss: 1.1052, train acc: 0.5338, val loss: 1.0973, val acc: 0.5403  (best train acc: 0.5530, best val acc: 0.5518)\n",
      "[Epoch: 26720] train loss: 1.1175, train acc: 0.5420, val loss: 1.0982, val acc: 0.5356  (best train acc: 0.5530, best val acc: 0.5518)\n",
      "[Epoch: 26740] train loss: 1.0967, train acc: 0.5419, val loss: 1.0946, val acc: 0.5430  (best train acc: 0.5530, best val acc: 0.5518)\n",
      "[Epoch: 26760] train loss: 1.1223, train acc: 0.5327, val loss: 1.0994, val acc: 0.5346  (best train acc: 0.5530, best val acc: 0.5518)\n",
      "[Epoch: 26780] train loss: 1.0987, train acc: 0.5396, val loss: 1.0929, val acc: 0.5427  (best train acc: 0.5557, best val acc: 0.5518)\n",
      "[Epoch: 26800] train loss: 1.1056, train acc: 0.5317, val loss: 1.1089, val acc: 0.5484  (best train acc: 0.5557, best val acc: 0.5518)\n",
      "[Epoch: 26820] train loss: 1.1153, train acc: 0.5267, val loss: 1.0942, val acc: 0.5356  (best train acc: 0.5557, best val acc: 0.5518)\n",
      "[Epoch: 26840] train loss: 1.0997, train acc: 0.5402, val loss: 1.0927, val acc: 0.5383  (best train acc: 0.5557, best val acc: 0.5518)\n",
      "[Epoch: 26860] train loss: 1.1045, train acc: 0.5421, val loss: 1.0922, val acc: 0.5427  (best train acc: 0.5557, best val acc: 0.5518)\n",
      "[Epoch: 26880] train loss: 1.1101, train acc: 0.5411, val loss: 1.0953, val acc: 0.5400  (best train acc: 0.5557, best val acc: 0.5518)\n",
      "[Epoch: 26900] train loss: 1.0758, train acc: 0.5508, val loss: 1.1059, val acc: 0.5201  (best train acc: 0.5557, best val acc: 0.5518)\n",
      "[Epoch: 26920] train loss: 1.0980, train acc: 0.5406, val loss: 1.0950, val acc: 0.5403  (best train acc: 0.5557, best val acc: 0.5518)\n",
      "[Epoch: 26940] train loss: 1.0942, train acc: 0.5401, val loss: 1.0919, val acc: 0.5433  (best train acc: 0.5557, best val acc: 0.5518)\n",
      "[Epoch: 26960] train loss: 1.1108, train acc: 0.5341, val loss: 1.0939, val acc: 0.5369  (best train acc: 0.5557, best val acc: 0.5518)\n",
      "[Epoch: 26980] train loss: 1.1031, train acc: 0.5407, val loss: 1.0942, val acc: 0.5400  (best train acc: 0.5557, best val acc: 0.5518)\n",
      "[Epoch: 27000] train loss: 1.1222, train acc: 0.5367, val loss: 1.1022, val acc: 0.5487  (best train acc: 0.5557, best val acc: 0.5518)\n",
      "[Epoch: 27020] train loss: 1.1009, train acc: 0.5367, val loss: 1.1028, val acc: 0.5393  (best train acc: 0.5557, best val acc: 0.5518)\n",
      "[Epoch: 27040] train loss: 1.1243, train acc: 0.5295, val loss: 1.0934, val acc: 0.5444  (best train acc: 0.5557, best val acc: 0.5518)\n",
      "[Epoch: 27060] train loss: 1.1147, train acc: 0.5294, val loss: 1.0968, val acc: 0.5417  (best train acc: 0.5557, best val acc: 0.5518)\n",
      "[Epoch: 27080] train loss: 1.0945, train acc: 0.5441, val loss: 1.0946, val acc: 0.5413  (best train acc: 0.5557, best val acc: 0.5518)\n",
      "[Epoch: 27100] train loss: 1.0937, train acc: 0.5518, val loss: 1.0957, val acc: 0.5447  (best train acc: 0.5583, best val acc: 0.5518)\n",
      "[Epoch: 27120] train loss: 1.1029, train acc: 0.5370, val loss: 1.0947, val acc: 0.5487  (best train acc: 0.5583, best val acc: 0.5518)\n",
      "[Epoch: 27140] train loss: 1.1060, train acc: 0.5335, val loss: 1.0926, val acc: 0.5383  (best train acc: 0.5583, best val acc: 0.5518)\n",
      "[Epoch: 27160] train loss: 1.1214, train acc: 0.5328, val loss: 1.0927, val acc: 0.5406  (best train acc: 0.5583, best val acc: 0.5518)\n",
      "[Epoch: 27180] train loss: 1.1078, train acc: 0.5313, val loss: 1.0909, val acc: 0.5420  (best train acc: 0.5583, best val acc: 0.5518)\n",
      "[Epoch: 27200] train loss: 1.1235, train acc: 0.5192, val loss: 1.0964, val acc: 0.5504  (best train acc: 0.5583, best val acc: 0.5518)\n",
      "[Epoch: 27220] train loss: 1.0929, train acc: 0.5395, val loss: 1.0924, val acc: 0.5396  (best train acc: 0.5583, best val acc: 0.5518)\n",
      "[Epoch: 27240] train loss: 1.1111, train acc: 0.5361, val loss: 1.0902, val acc: 0.5440  (best train acc: 0.5583, best val acc: 0.5518)\n",
      "[Epoch: 27260] train loss: 1.1102, train acc: 0.5301, val loss: 1.0923, val acc: 0.5356  (best train acc: 0.5583, best val acc: 0.5518)\n",
      "[Epoch: 27280] train loss: 1.0993, train acc: 0.5408, val loss: 1.0934, val acc: 0.5484  (best train acc: 0.5583, best val acc: 0.5518)\n",
      "[Epoch: 27300] train loss: 1.1153, train acc: 0.5415, val loss: 1.0926, val acc: 0.5332  (best train acc: 0.5583, best val acc: 0.5518)\n",
      "[Epoch: 27320] train loss: 1.1112, train acc: 0.5361, val loss: 1.1037, val acc: 0.5474  (best train acc: 0.5583, best val acc: 0.5518)\n",
      "[Epoch: 27340] train loss: 1.1185, train acc: 0.5312, val loss: 1.1049, val acc: 0.5315  (best train acc: 0.5583, best val acc: 0.5518)\n",
      "[Epoch: 27360] train loss: 1.1065, train acc: 0.5422, val loss: 1.0957, val acc: 0.5460  (best train acc: 0.5583, best val acc: 0.5518)\n",
      "[Epoch: 27380] train loss: 1.1129, train acc: 0.5438, val loss: 1.1031, val acc: 0.5460  (best train acc: 0.5583, best val acc: 0.5518)\n",
      "[Epoch: 27400] train loss: 1.1034, train acc: 0.5407, val loss: 1.0930, val acc: 0.5417  (best train acc: 0.5583, best val acc: 0.5518)\n",
      "[Epoch: 27420] train loss: 1.1241, train acc: 0.5212, val loss: 1.0966, val acc: 0.5497  (best train acc: 0.5583, best val acc: 0.5518)\n",
      "[Epoch: 27440] train loss: 1.1220, train acc: 0.5264, val loss: 1.0907, val acc: 0.5413  (best train acc: 0.5583, best val acc: 0.5518)\n",
      "[Epoch: 27460] train loss: 1.0873, train acc: 0.5401, val loss: 1.0921, val acc: 0.5413  (best train acc: 0.5583, best val acc: 0.5524)\n",
      "[Epoch: 27480] train loss: 1.0995, train acc: 0.5422, val loss: 1.0942, val acc: 0.5474  (best train acc: 0.5583, best val acc: 0.5524)\n",
      "[Epoch: 27500] train loss: 1.0987, train acc: 0.5390, val loss: 1.0914, val acc: 0.5491  (best train acc: 0.5583, best val acc: 0.5524)\n",
      "[Epoch: 27520] train loss: 1.0936, train acc: 0.5419, val loss: 1.0895, val acc: 0.5457  (best train acc: 0.5583, best val acc: 0.5524)\n",
      "[Epoch: 27540] train loss: 1.1144, train acc: 0.5356, val loss: 1.0900, val acc: 0.5423  (best train acc: 0.5583, best val acc: 0.5524)\n",
      "[Epoch: 27560] train loss: 1.1010, train acc: 0.5391, val loss: 1.0885, val acc: 0.5470  (best train acc: 0.5583, best val acc: 0.5524)\n",
      "[Epoch: 27580] train loss: 1.1209, train acc: 0.5350, val loss: 1.0887, val acc: 0.5440  (best train acc: 0.5583, best val acc: 0.5524)\n",
      "[Epoch: 27600] train loss: 1.0990, train acc: 0.5447, val loss: 1.0930, val acc: 0.5396  (best train acc: 0.5583, best val acc: 0.5524)\n",
      "[Epoch: 27620] train loss: 1.0871, train acc: 0.5492, val loss: 1.0923, val acc: 0.5474  (best train acc: 0.5583, best val acc: 0.5524)\n",
      "[Epoch: 27640] train loss: 1.1025, train acc: 0.5398, val loss: 1.0933, val acc: 0.5514  (best train acc: 0.5583, best val acc: 0.5524)\n",
      "[Epoch: 27660] train loss: 1.1145, train acc: 0.5312, val loss: 1.0890, val acc: 0.5393  (best train acc: 0.5583, best val acc: 0.5524)\n",
      "[Epoch: 27680] train loss: 1.1027, train acc: 0.5348, val loss: 1.0910, val acc: 0.5400  (best train acc: 0.5583, best val acc: 0.5524)\n",
      "[Epoch: 27700] train loss: 1.0925, train acc: 0.5370, val loss: 1.0886, val acc: 0.5403  (best train acc: 0.5583, best val acc: 0.5524)\n",
      "[Epoch: 27720] train loss: 1.0884, train acc: 0.5432, val loss: 1.0961, val acc: 0.5524  (best train acc: 0.5583, best val acc: 0.5524)\n",
      "[Epoch: 27740] train loss: 1.1087, train acc: 0.5394, val loss: 1.0947, val acc: 0.5497  (best train acc: 0.5583, best val acc: 0.5524)\n",
      "[Epoch: 27760] train loss: 1.1122, train acc: 0.5378, val loss: 1.0877, val acc: 0.5430  (best train acc: 0.5583, best val acc: 0.5524)\n",
      "[Epoch: 27780] train loss: 1.1027, train acc: 0.5411, val loss: 1.0957, val acc: 0.5508  (best train acc: 0.5583, best val acc: 0.5524)\n",
      "[Epoch: 27800] train loss: 1.0964, train acc: 0.5388, val loss: 1.0878, val acc: 0.5454  (best train acc: 0.5583, best val acc: 0.5524)\n",
      "[Epoch: 27820] train loss: 1.0946, train acc: 0.5461, val loss: 1.0900, val acc: 0.5427  (best train acc: 0.5583, best val acc: 0.5524)\n",
      "[Epoch: 27840] train loss: 1.0837, train acc: 0.5458, val loss: 1.0875, val acc: 0.5447  (best train acc: 0.5583, best val acc: 0.5524)\n",
      "[Epoch: 27860] train loss: 1.1045, train acc: 0.5375, val loss: 1.0924, val acc: 0.5417  (best train acc: 0.5583, best val acc: 0.5524)\n",
      "[Epoch: 27880] train loss: 1.0936, train acc: 0.5418, val loss: 1.0880, val acc: 0.5487  (best train acc: 0.5583, best val acc: 0.5524)\n",
      "[Epoch: 27900] train loss: 1.1201, train acc: 0.5250, val loss: 1.0868, val acc: 0.5444  (best train acc: 0.5583, best val acc: 0.5524)\n",
      "[Epoch: 27920] train loss: 1.1149, train acc: 0.5393, val loss: 1.0879, val acc: 0.5444  (best train acc: 0.5583, best val acc: 0.5524)\n",
      "[Epoch: 27940] train loss: 1.1038, train acc: 0.5376, val loss: 1.0888, val acc: 0.5447  (best train acc: 0.5583, best val acc: 0.5524)\n",
      "[Epoch: 27960] train loss: 1.0994, train acc: 0.5451, val loss: 1.0865, val acc: 0.5457  (best train acc: 0.5583, best val acc: 0.5524)\n",
      "[Epoch: 27980] train loss: 1.0912, train acc: 0.5501, val loss: 1.0912, val acc: 0.5417  (best train acc: 0.5583, best val acc: 0.5524)\n",
      "[Epoch: 28000] train loss: 1.1003, train acc: 0.5322, val loss: 1.0883, val acc: 0.5433  (best train acc: 0.5583, best val acc: 0.5524)\n",
      "[Epoch: 28020] train loss: 1.1008, train acc: 0.5388, val loss: 1.0871, val acc: 0.5376  (best train acc: 0.5583, best val acc: 0.5524)\n",
      "[Epoch: 28040] train loss: 1.0962, train acc: 0.5448, val loss: 1.0883, val acc: 0.5433  (best train acc: 0.5583, best val acc: 0.5524)\n",
      "[Epoch: 28060] train loss: 1.0980, train acc: 0.5401, val loss: 1.0925, val acc: 0.5410  (best train acc: 0.5583, best val acc: 0.5524)\n",
      "[Epoch: 28080] train loss: 1.0864, train acc: 0.5445, val loss: 1.0922, val acc: 0.5518  (best train acc: 0.5583, best val acc: 0.5524)\n",
      "[Epoch: 28100] train loss: 1.0932, train acc: 0.5457, val loss: 1.0953, val acc: 0.5393  (best train acc: 0.5583, best val acc: 0.5524)\n",
      "[Epoch: 28120] train loss: 1.1261, train acc: 0.5287, val loss: 1.0890, val acc: 0.5504  (best train acc: 0.5583, best val acc: 0.5524)\n",
      "[Epoch: 28140] train loss: 1.1085, train acc: 0.5386, val loss: 1.0855, val acc: 0.5464  (best train acc: 0.5583, best val acc: 0.5531)\n",
      "[Epoch: 28160] train loss: 1.1153, train acc: 0.5349, val loss: 1.0913, val acc: 0.5521  (best train acc: 0.5583, best val acc: 0.5531)\n",
      "[Epoch: 28180] train loss: 1.0940, train acc: 0.5435, val loss: 1.0875, val acc: 0.5444  (best train acc: 0.5583, best val acc: 0.5531)\n",
      "[Epoch: 28200] train loss: 1.0865, train acc: 0.5468, val loss: 1.1015, val acc: 0.5433  (best train acc: 0.5583, best val acc: 0.5531)\n",
      "[Epoch: 28220] train loss: 1.1046, train acc: 0.5439, val loss: 1.0841, val acc: 0.5491  (best train acc: 0.5583, best val acc: 0.5531)\n",
      "[Epoch: 28240] train loss: 1.1063, train acc: 0.5375, val loss: 1.0871, val acc: 0.5484  (best train acc: 0.5583, best val acc: 0.5531)\n",
      "[Epoch: 28260] train loss: 1.0996, train acc: 0.5340, val loss: 1.0834, val acc: 0.5470  (best train acc: 0.5583, best val acc: 0.5531)\n",
      "[Epoch: 28280] train loss: 1.1076, train acc: 0.5289, val loss: 1.0918, val acc: 0.5417  (best train acc: 0.5583, best val acc: 0.5548)\n",
      "[Epoch: 28300] train loss: 1.0813, train acc: 0.5492, val loss: 1.0873, val acc: 0.5457  (best train acc: 0.5583, best val acc: 0.5548)\n",
      "[Epoch: 28320] train loss: 1.1094, train acc: 0.5300, val loss: 1.0901, val acc: 0.5497  (best train acc: 0.5619, best val acc: 0.5548)\n",
      "[Epoch: 28340] train loss: 1.0898, train acc: 0.5453, val loss: 1.0847, val acc: 0.5481  (best train acc: 0.5619, best val acc: 0.5548)\n",
      "[Epoch: 28360] train loss: 1.0967, train acc: 0.5351, val loss: 1.0834, val acc: 0.5470  (best train acc: 0.5619, best val acc: 0.5548)\n",
      "[Epoch: 28380] train loss: 1.1100, train acc: 0.5369, val loss: 1.0917, val acc: 0.5528  (best train acc: 0.5619, best val acc: 0.5548)\n",
      "[Epoch: 28400] train loss: 1.0934, train acc: 0.5513, val loss: 1.0908, val acc: 0.5555  (best train acc: 0.5619, best val acc: 0.5555)\n",
      "[Epoch: 28420] train loss: 1.1076, train acc: 0.5412, val loss: 1.0832, val acc: 0.5501  (best train acc: 0.5619, best val acc: 0.5555)\n",
      "[Epoch: 28440] train loss: 1.1024, train acc: 0.5417, val loss: 1.0827, val acc: 0.5474  (best train acc: 0.5619, best val acc: 0.5555)\n",
      "[Epoch: 28460] train loss: 1.0835, train acc: 0.5458, val loss: 1.0910, val acc: 0.5518  (best train acc: 0.5619, best val acc: 0.5555)\n",
      "[Epoch: 28480] train loss: 1.0869, train acc: 0.5373, val loss: 1.0895, val acc: 0.5417  (best train acc: 0.5619, best val acc: 0.5555)\n",
      "[Epoch: 28500] train loss: 1.1163, train acc: 0.5346, val loss: 1.0826, val acc: 0.5491  (best train acc: 0.5619, best val acc: 0.5565)\n",
      "[Epoch: 28520] train loss: 1.0940, train acc: 0.5390, val loss: 1.0828, val acc: 0.5484  (best train acc: 0.5619, best val acc: 0.5565)\n",
      "[Epoch: 28540] train loss: 1.1012, train acc: 0.5374, val loss: 1.0819, val acc: 0.5467  (best train acc: 0.5619, best val acc: 0.5565)\n",
      "[Epoch: 28560] train loss: 1.1074, train acc: 0.5376, val loss: 1.0813, val acc: 0.5464  (best train acc: 0.5619, best val acc: 0.5565)\n",
      "[Epoch: 28580] train loss: 1.0894, train acc: 0.5473, val loss: 1.0815, val acc: 0.5470  (best train acc: 0.5619, best val acc: 0.5565)\n",
      "[Epoch: 28600] train loss: 1.1116, train acc: 0.5405, val loss: 1.0928, val acc: 0.5508  (best train acc: 0.5619, best val acc: 0.5565)\n",
      "[Epoch: 28620] train loss: 1.0979, train acc: 0.5490, val loss: 1.0853, val acc: 0.5514  (best train acc: 0.5619, best val acc: 0.5565)\n",
      "[Epoch: 28640] train loss: 1.1167, train acc: 0.5343, val loss: 1.1179, val acc: 0.5282  (best train acc: 0.5619, best val acc: 0.5565)\n",
      "[Epoch: 28660] train loss: 1.0947, train acc: 0.5525, val loss: 1.0813, val acc: 0.5484  (best train acc: 0.5619, best val acc: 0.5565)\n",
      "[Epoch: 28680] train loss: 1.0890, train acc: 0.5487, val loss: 1.0808, val acc: 0.5481  (best train acc: 0.5621, best val acc: 0.5565)\n",
      "[Epoch: 28700] train loss: 1.0966, train acc: 0.5399, val loss: 1.0874, val acc: 0.5393  (best train acc: 0.5621, best val acc: 0.5565)\n",
      "[Epoch: 28720] train loss: 1.1069, train acc: 0.5406, val loss: 1.0871, val acc: 0.5433  (best train acc: 0.5621, best val acc: 0.5565)\n",
      "[Epoch: 28740] train loss: 1.0969, train acc: 0.5481, val loss: 1.0810, val acc: 0.5477  (best train acc: 0.5621, best val acc: 0.5572)\n",
      "[Epoch: 28760] train loss: 1.0854, train acc: 0.5502, val loss: 1.0804, val acc: 0.5524  (best train acc: 0.5621, best val acc: 0.5572)\n",
      "[Epoch: 28780] train loss: 1.1032, train acc: 0.5395, val loss: 1.0799, val acc: 0.5541  (best train acc: 0.5630, best val acc: 0.5572)\n",
      "[Epoch: 28800] train loss: 1.1007, train acc: 0.5411, val loss: 1.0819, val acc: 0.5481  (best train acc: 0.5630, best val acc: 0.5572)\n",
      "[Epoch: 28820] train loss: 1.0867, train acc: 0.5412, val loss: 1.0940, val acc: 0.5363  (best train acc: 0.5630, best val acc: 0.5572)\n",
      "[Epoch: 28840] train loss: 1.0764, train acc: 0.5572, val loss: 1.0839, val acc: 0.5501  (best train acc: 0.5630, best val acc: 0.5572)\n",
      "[Epoch: 28860] train loss: 1.0925, train acc: 0.5557, val loss: 1.0834, val acc: 0.5501  (best train acc: 0.5630, best val acc: 0.5572)\n",
      "[Epoch: 28880] train loss: 1.1028, train acc: 0.5424, val loss: 1.0827, val acc: 0.5541  (best train acc: 0.5630, best val acc: 0.5572)\n",
      "[Epoch: 28900] train loss: 1.0742, train acc: 0.5565, val loss: 1.0847, val acc: 0.5454  (best train acc: 0.5630, best val acc: 0.5572)\n",
      "[Epoch: 28920] train loss: 1.0865, train acc: 0.5481, val loss: 1.1046, val acc: 0.5221  (best train acc: 0.5630, best val acc: 0.5572)\n",
      "[Epoch: 28940] train loss: 1.0868, train acc: 0.5517, val loss: 1.0796, val acc: 0.5514  (best train acc: 0.5630, best val acc: 0.5572)\n",
      "[Epoch: 28960] train loss: 1.0835, train acc: 0.5461, val loss: 1.0795, val acc: 0.5393  (best train acc: 0.5630, best val acc: 0.5572)\n",
      "[Epoch: 28980] train loss: 1.0922, train acc: 0.5497, val loss: 1.1080, val acc: 0.5285  (best train acc: 0.5630, best val acc: 0.5572)\n",
      "[Epoch: 29000] train loss: 1.0936, train acc: 0.5525, val loss: 1.1005, val acc: 0.5437  (best train acc: 0.5630, best val acc: 0.5572)\n",
      "[Epoch: 29020] train loss: 1.0751, train acc: 0.5554, val loss: 1.0795, val acc: 0.5524  (best train acc: 0.5630, best val acc: 0.5572)\n",
      "[Epoch: 29040] train loss: 1.1114, train acc: 0.5384, val loss: 1.0782, val acc: 0.5511  (best train acc: 0.5630, best val acc: 0.5575)\n",
      "[Epoch: 29060] train loss: 1.1067, train acc: 0.5299, val loss: 1.1153, val acc: 0.5140  (best train acc: 0.5630, best val acc: 0.5575)\n",
      "[Epoch: 29080] train loss: 1.1020, train acc: 0.5420, val loss: 1.1275, val acc: 0.5089  (best train acc: 0.5630, best val acc: 0.5575)\n",
      "[Epoch: 29100] train loss: 1.0921, train acc: 0.5435, val loss: 1.0809, val acc: 0.5528  (best train acc: 0.5630, best val acc: 0.5585)\n",
      "[Epoch: 29120] train loss: 1.0774, train acc: 0.5563, val loss: 1.0801, val acc: 0.5545  (best train acc: 0.5630, best val acc: 0.5585)\n",
      "[Epoch: 29140] train loss: 1.1143, train acc: 0.5395, val loss: 1.0790, val acc: 0.5524  (best train acc: 0.5630, best val acc: 0.5585)\n",
      "[Epoch: 29160] train loss: 1.0992, train acc: 0.5438, val loss: 1.0787, val acc: 0.5437  (best train acc: 0.5630, best val acc: 0.5585)\n",
      "[Epoch: 29180] train loss: 1.0842, train acc: 0.5511, val loss: 1.0805, val acc: 0.5535  (best train acc: 0.5630, best val acc: 0.5585)\n",
      "[Epoch: 29200] train loss: 1.0981, train acc: 0.5390, val loss: 1.0804, val acc: 0.5420  (best train acc: 0.5630, best val acc: 0.5585)\n",
      "[Epoch: 29220] train loss: 1.1021, train acc: 0.5351, val loss: 1.0798, val acc: 0.5521  (best train acc: 0.5630, best val acc: 0.5585)\n",
      "[Epoch: 29240] train loss: 1.0770, train acc: 0.5443, val loss: 1.0781, val acc: 0.5437  (best train acc: 0.5630, best val acc: 0.5585)\n",
      "[Epoch: 29260] train loss: 1.0925, train acc: 0.5323, val loss: 1.0840, val acc: 0.5514  (best train acc: 0.5630, best val acc: 0.5585)\n",
      "[Epoch: 29280] train loss: 1.1175, train acc: 0.5388, val loss: 1.0961, val acc: 0.5457  (best train acc: 0.5630, best val acc: 0.5585)\n",
      "[Epoch: 29300] train loss: 1.0994, train acc: 0.5371, val loss: 1.0936, val acc: 0.5423  (best train acc: 0.5630, best val acc: 0.5585)\n",
      "[Epoch: 29320] train loss: 1.1196, train acc: 0.5388, val loss: 1.0888, val acc: 0.5464  (best train acc: 0.5630, best val acc: 0.5585)\n",
      "[Epoch: 29340] train loss: 1.0990, train acc: 0.5440, val loss: 1.0923, val acc: 0.5386  (best train acc: 0.5630, best val acc: 0.5585)\n",
      "[Epoch: 29360] train loss: 1.0814, train acc: 0.5417, val loss: 1.0803, val acc: 0.5430  (best train acc: 0.5630, best val acc: 0.5585)\n",
      "[Epoch: 29380] train loss: 1.0753, train acc: 0.5581, val loss: 1.0794, val acc: 0.5531  (best train acc: 0.5630, best val acc: 0.5585)\n",
      "[Epoch: 29400] train loss: 1.1277, train acc: 0.5240, val loss: 1.0951, val acc: 0.5444  (best train acc: 0.5630, best val acc: 0.5585)\n",
      "[Epoch: 29420] train loss: 1.0921, train acc: 0.5448, val loss: 1.0927, val acc: 0.5366  (best train acc: 0.5630, best val acc: 0.5585)\n",
      "[Epoch: 29440] train loss: 1.1011, train acc: 0.5510, val loss: 1.0885, val acc: 0.5390  (best train acc: 0.5630, best val acc: 0.5585)\n",
      "[Epoch: 29460] train loss: 1.0864, train acc: 0.5427, val loss: 1.0785, val acc: 0.5396  (best train acc: 0.5630, best val acc: 0.5585)\n",
      "[Epoch: 29480] train loss: 1.0828, train acc: 0.5475, val loss: 1.0809, val acc: 0.5450  (best train acc: 0.5630, best val acc: 0.5585)\n",
      "[Epoch: 29500] train loss: 1.0916, train acc: 0.5493, val loss: 1.0769, val acc: 0.5528  (best train acc: 0.5630, best val acc: 0.5585)\n",
      "[Epoch: 29520] train loss: 1.1075, train acc: 0.5328, val loss: 1.0774, val acc: 0.5433  (best train acc: 0.5630, best val acc: 0.5585)\n",
      "[Epoch: 29540] train loss: 1.0821, train acc: 0.5502, val loss: 1.0757, val acc: 0.5521  (best train acc: 0.5630, best val acc: 0.5585)\n",
      "[Epoch: 29560] train loss: 1.0740, train acc: 0.5581, val loss: 1.0748, val acc: 0.5491  (best train acc: 0.5630, best val acc: 0.5585)\n",
      "[Epoch: 29580] train loss: 1.0930, train acc: 0.5417, val loss: 1.0789, val acc: 0.5511  (best train acc: 0.5630, best val acc: 0.5585)\n",
      "[Epoch: 29600] train loss: 1.0844, train acc: 0.5502, val loss: 1.0750, val acc: 0.5538  (best train acc: 0.5630, best val acc: 0.5585)\n",
      "[Epoch: 29620] train loss: 1.0785, train acc: 0.5530, val loss: 1.0905, val acc: 0.5427  (best train acc: 0.5632, best val acc: 0.5585)\n",
      "[Epoch: 29640] train loss: 1.1067, train acc: 0.5354, val loss: 1.0830, val acc: 0.5450  (best train acc: 0.5632, best val acc: 0.5585)\n",
      "[Epoch: 29660] train loss: 1.0901, train acc: 0.5476, val loss: 1.0866, val acc: 0.5359  (best train acc: 0.5632, best val acc: 0.5585)\n",
      "[Epoch: 29680] train loss: 1.1310, train acc: 0.5338, val loss: 1.0802, val acc: 0.5457  (best train acc: 0.5632, best val acc: 0.5589)\n",
      "[Epoch: 29700] train loss: 1.0783, train acc: 0.5532, val loss: 1.0792, val acc: 0.5430  (best train acc: 0.5632, best val acc: 0.5589)\n",
      "[Epoch: 29720] train loss: 1.1034, train acc: 0.5396, val loss: 1.0771, val acc: 0.5477  (best train acc: 0.5632, best val acc: 0.5589)\n",
      "[Epoch: 29740] train loss: 1.1098, train acc: 0.5365, val loss: 1.0786, val acc: 0.5514  (best train acc: 0.5632, best val acc: 0.5589)\n",
      "[Epoch: 29760] train loss: 1.0948, train acc: 0.5495, val loss: 1.0764, val acc: 0.5562  (best train acc: 0.5632, best val acc: 0.5592)\n",
      "[Epoch: 29780] train loss: 1.0692, train acc: 0.5616, val loss: 1.0755, val acc: 0.5562  (best train acc: 0.5632, best val acc: 0.5592)\n",
      "[Epoch: 29800] train loss: 1.0871, train acc: 0.5483, val loss: 1.0745, val acc: 0.5497  (best train acc: 0.5632, best val acc: 0.5592)\n",
      "[Epoch: 29820] train loss: 1.1000, train acc: 0.5457, val loss: 1.0761, val acc: 0.5420  (best train acc: 0.5655, best val acc: 0.5592)\n",
      "[Epoch: 29840] train loss: 1.0790, train acc: 0.5488, val loss: 1.0749, val acc: 0.5491  (best train acc: 0.5655, best val acc: 0.5592)\n",
      "[Epoch: 29860] train loss: 1.0944, train acc: 0.5505, val loss: 1.0760, val acc: 0.5508  (best train acc: 0.5655, best val acc: 0.5592)\n",
      "[Epoch: 29880] train loss: 1.0904, train acc: 0.5440, val loss: 1.0834, val acc: 0.5467  (best train acc: 0.5655, best val acc: 0.5592)\n",
      "[Epoch: 29900] train loss: 1.0834, train acc: 0.5536, val loss: 1.0793, val acc: 0.5518  (best train acc: 0.5655, best val acc: 0.5592)\n",
      "[Epoch: 29920] train loss: 1.1129, train acc: 0.5317, val loss: 1.0732, val acc: 0.5558  (best train acc: 0.5655, best val acc: 0.5626)\n",
      "[Epoch: 29940] train loss: 1.0629, train acc: 0.5617, val loss: 1.0722, val acc: 0.5551  (best train acc: 0.5655, best val acc: 0.5626)\n",
      "[Epoch: 29960] train loss: 1.0971, train acc: 0.5398, val loss: 1.0722, val acc: 0.5555  (best train acc: 0.5655, best val acc: 0.5626)\n",
      "[Epoch: 29980] train loss: 1.0778, train acc: 0.5585, val loss: 1.0746, val acc: 0.5524  (best train acc: 0.5655, best val acc: 0.5626)\n",
      "[Epoch: 30000] train loss: 1.0768, train acc: 0.5514, val loss: 1.0754, val acc: 0.5589  (best train acc: 0.5655, best val acc: 0.5626)\n",
      "[Epoch: 30020] train loss: 1.0856, train acc: 0.5536, val loss: 1.0817, val acc: 0.5545  (best train acc: 0.5655, best val acc: 0.5626)\n",
      "[Epoch: 30040] train loss: 1.0735, train acc: 0.5583, val loss: 1.0943, val acc: 0.5376  (best train acc: 0.5655, best val acc: 0.5626)\n",
      "[Epoch: 30060] train loss: 1.0798, train acc: 0.5495, val loss: 1.0724, val acc: 0.5444  (best train acc: 0.5655, best val acc: 0.5626)\n",
      "[Epoch: 30080] train loss: 1.0825, train acc: 0.5481, val loss: 1.0705, val acc: 0.5562  (best train acc: 0.5655, best val acc: 0.5626)\n",
      "[Epoch: 30100] train loss: 1.0862, train acc: 0.5407, val loss: 1.0770, val acc: 0.5494  (best train acc: 0.5655, best val acc: 0.5626)\n",
      "[Epoch: 30120] train loss: 1.0657, train acc: 0.5549, val loss: 1.0769, val acc: 0.5535  (best train acc: 0.5655, best val acc: 0.5626)\n",
      "[Epoch: 30140] train loss: 1.1046, train acc: 0.5346, val loss: 1.0718, val acc: 0.5440  (best train acc: 0.5655, best val acc: 0.5626)\n",
      "[Epoch: 30160] train loss: 1.0841, train acc: 0.5524, val loss: 1.0788, val acc: 0.5464  (best train acc: 0.5655, best val acc: 0.5626)\n",
      "[Epoch: 30180] train loss: 1.0878, train acc: 0.5539, val loss: 1.0708, val acc: 0.5521  (best train acc: 0.5655, best val acc: 0.5626)\n",
      "[Epoch: 30200] train loss: 1.0819, train acc: 0.5467, val loss: 1.0693, val acc: 0.5484  (best train acc: 0.5679, best val acc: 0.5626)\n",
      "[Epoch: 30220] train loss: 1.0892, train acc: 0.5419, val loss: 1.0730, val acc: 0.5487  (best train acc: 0.5679, best val acc: 0.5626)\n",
      "[Epoch: 30240] train loss: 1.1161, train acc: 0.5354, val loss: 1.0970, val acc: 0.5369  (best train acc: 0.5679, best val acc: 0.5632)\n",
      "[Epoch: 30260] train loss: 1.0851, train acc: 0.5502, val loss: 1.1188, val acc: 0.5180  (best train acc: 0.5679, best val acc: 0.5632)\n",
      "[Epoch: 30280] train loss: 1.1040, train acc: 0.5546, val loss: 1.0756, val acc: 0.5444  (best train acc: 0.5679, best val acc: 0.5632)\n",
      "[Epoch: 30300] train loss: 1.0818, train acc: 0.5541, val loss: 1.0738, val acc: 0.5450  (best train acc: 0.5679, best val acc: 0.5632)\n",
      "[Epoch: 30320] train loss: 1.0912, train acc: 0.5458, val loss: 1.0702, val acc: 0.5538  (best train acc: 0.5679, best val acc: 0.5632)\n",
      "[Epoch: 30340] train loss: 1.1115, train acc: 0.5303, val loss: 1.0725, val acc: 0.5528  (best train acc: 0.5679, best val acc: 0.5632)\n",
      "[Epoch: 30360] train loss: 1.1482, train acc: 0.5266, val loss: 1.1587, val acc: 0.5167  (best train acc: 0.5679, best val acc: 0.5632)\n",
      "[Epoch: 30380] train loss: 1.1177, train acc: 0.5388, val loss: 1.0797, val acc: 0.5477  (best train acc: 0.5679, best val acc: 0.5632)\n",
      "[Epoch: 30400] train loss: 1.0774, train acc: 0.5487, val loss: 1.0732, val acc: 0.5497  (best train acc: 0.5679, best val acc: 0.5632)\n",
      "[Epoch: 30420] train loss: 1.0995, train acc: 0.5474, val loss: 1.0718, val acc: 0.5437  (best train acc: 0.5679, best val acc: 0.5632)\n",
      "[Epoch: 30440] train loss: 1.0955, train acc: 0.5517, val loss: 1.0745, val acc: 0.5558  (best train acc: 0.5679, best val acc: 0.5632)\n",
      "[Epoch: 30460] train loss: 1.0814, train acc: 0.5526, val loss: 1.0768, val acc: 0.5460  (best train acc: 0.5679, best val acc: 0.5632)\n",
      "[Epoch: 30480] train loss: 1.0743, train acc: 0.5550, val loss: 1.0771, val acc: 0.5464  (best train acc: 0.5679, best val acc: 0.5632)\n",
      "[Epoch: 30500] train loss: 1.0999, train acc: 0.5424, val loss: 1.0714, val acc: 0.5501  (best train acc: 0.5679, best val acc: 0.5632)\n",
      "[Epoch: 30520] train loss: 1.0930, train acc: 0.5472, val loss: 1.0697, val acc: 0.5548  (best train acc: 0.5679, best val acc: 0.5632)\n",
      "[Epoch: 30540] train loss: 1.0879, train acc: 0.5440, val loss: 1.0699, val acc: 0.5528  (best train acc: 0.5679, best val acc: 0.5632)\n",
      "[Epoch: 30560] train loss: 1.0764, train acc: 0.5603, val loss: 1.0700, val acc: 0.5555  (best train acc: 0.5679, best val acc: 0.5632)\n",
      "[Epoch: 30580] train loss: 1.0744, train acc: 0.5588, val loss: 1.0698, val acc: 0.5565  (best train acc: 0.5679, best val acc: 0.5632)\n",
      "[Epoch: 30600] train loss: 1.0850, train acc: 0.5540, val loss: 1.0678, val acc: 0.5538  (best train acc: 0.5679, best val acc: 0.5632)\n",
      "[Epoch: 30620] train loss: 1.1464, train acc: 0.5324, val loss: 1.0982, val acc: 0.5477  (best train acc: 0.5679, best val acc: 0.5632)\n",
      "[Epoch: 30640] train loss: 1.0830, train acc: 0.5441, val loss: 1.0683, val acc: 0.5524  (best train acc: 0.5679, best val acc: 0.5632)\n",
      "[Epoch: 30660] train loss: 1.0858, train acc: 0.5382, val loss: 1.0694, val acc: 0.5491  (best train acc: 0.5679, best val acc: 0.5632)\n",
      "[Epoch: 30680] train loss: 1.0981, train acc: 0.5484, val loss: 1.0721, val acc: 0.5477  (best train acc: 0.5679, best val acc: 0.5632)\n",
      "[Epoch: 30700] train loss: 1.0666, train acc: 0.5580, val loss: 1.0817, val acc: 0.5437  (best train acc: 0.5679, best val acc: 0.5632)\n",
      "[Epoch: 30720] train loss: 1.0797, train acc: 0.5377, val loss: 1.0675, val acc: 0.5565  (best train acc: 0.5679, best val acc: 0.5636)\n",
      "[Epoch: 30740] train loss: 1.0796, train acc: 0.5477, val loss: 1.0835, val acc: 0.5369  (best train acc: 0.5679, best val acc: 0.5636)\n",
      "[Epoch: 30760] train loss: 1.0962, train acc: 0.5283, val loss: 1.0840, val acc: 0.5481  (best train acc: 0.5679, best val acc: 0.5636)\n",
      "[Epoch: 30780] train loss: 1.1177, train acc: 0.5335, val loss: 1.0700, val acc: 0.5491  (best train acc: 0.5679, best val acc: 0.5636)\n",
      "[Epoch: 30800] train loss: 1.0969, train acc: 0.5367, val loss: 1.0682, val acc: 0.5629  (best train acc: 0.5679, best val acc: 0.5636)\n",
      "[Epoch: 30820] train loss: 1.0939, train acc: 0.5412, val loss: 1.0741, val acc: 0.5474  (best train acc: 0.5679, best val acc: 0.5636)\n",
      "[Epoch: 30840] train loss: 1.0973, train acc: 0.5417, val loss: 1.0703, val acc: 0.5599  (best train acc: 0.5679, best val acc: 0.5636)\n",
      "[Epoch: 30860] train loss: 1.0751, train acc: 0.5602, val loss: 1.0676, val acc: 0.5585  (best train acc: 0.5679, best val acc: 0.5636)\n",
      "[Epoch: 30880] train loss: 1.0840, train acc: 0.5495, val loss: 1.0807, val acc: 0.5437  (best train acc: 0.5679, best val acc: 0.5636)\n",
      "[Epoch: 30900] train loss: 1.1093, train acc: 0.5310, val loss: 1.0907, val acc: 0.5386  (best train acc: 0.5679, best val acc: 0.5636)\n",
      "[Epoch: 30920] train loss: 1.0761, train acc: 0.5620, val loss: 1.0768, val acc: 0.5477  (best train acc: 0.5679, best val acc: 0.5636)\n",
      "[Epoch: 30940] train loss: 1.0901, train acc: 0.5542, val loss: 1.0673, val acc: 0.5551  (best train acc: 0.5679, best val acc: 0.5636)\n",
      "[Epoch: 30960] train loss: 1.1149, train acc: 0.5368, val loss: 1.1084, val acc: 0.5255  (best train acc: 0.5679, best val acc: 0.5636)\n",
      "[Epoch: 30980] train loss: 1.0796, train acc: 0.5416, val loss: 1.0783, val acc: 0.5332  (best train acc: 0.5679, best val acc: 0.5636)\n",
      "[Epoch: 31000] train loss: 1.0896, train acc: 0.5430, val loss: 1.0723, val acc: 0.5437  (best train acc: 0.5679, best val acc: 0.5636)\n",
      "[Epoch: 31020] train loss: 1.0826, train acc: 0.5481, val loss: 1.0684, val acc: 0.5541  (best train acc: 0.5679, best val acc: 0.5639)\n",
      "[Epoch: 31040] train loss: 1.0575, train acc: 0.5656, val loss: 1.0680, val acc: 0.5501  (best train acc: 0.5679, best val acc: 0.5639)\n",
      "[Epoch: 31060] train loss: 1.0910, train acc: 0.5457, val loss: 1.0686, val acc: 0.5528  (best train acc: 0.5679, best val acc: 0.5639)\n",
      "[Epoch: 31080] train loss: 1.1090, train acc: 0.5424, val loss: 1.0784, val acc: 0.5521  (best train acc: 0.5679, best val acc: 0.5639)\n",
      "[Epoch: 31100] train loss: 1.1131, train acc: 0.5470, val loss: 1.0884, val acc: 0.5437  (best train acc: 0.5679, best val acc: 0.5639)\n",
      "[Epoch: 31120] train loss: 1.1197, train acc: 0.5338, val loss: 1.0826, val acc: 0.5430  (best train acc: 0.5679, best val acc: 0.5639)\n",
      "[Epoch: 31140] train loss: 1.1028, train acc: 0.5477, val loss: 1.0789, val acc: 0.5528  (best train acc: 0.5679, best val acc: 0.5639)\n",
      "[Epoch: 31160] train loss: 1.0752, train acc: 0.5545, val loss: 1.0705, val acc: 0.5565  (best train acc: 0.5679, best val acc: 0.5639)\n",
      "[Epoch: 31180] train loss: 1.0784, train acc: 0.5473, val loss: 1.0688, val acc: 0.5447  (best train acc: 0.5679, best val acc: 0.5639)\n",
      "[Epoch: 31200] train loss: 1.0860, train acc: 0.5434, val loss: 1.0782, val acc: 0.5484  (best train acc: 0.5679, best val acc: 0.5639)\n",
      "[Epoch: 31220] train loss: 1.0962, train acc: 0.5328, val loss: 1.0911, val acc: 0.5511  (best train acc: 0.5679, best val acc: 0.5639)\n",
      "[Epoch: 31240] train loss: 1.0986, train acc: 0.5546, val loss: 1.0664, val acc: 0.5565  (best train acc: 0.5679, best val acc: 0.5639)\n",
      "[Epoch: 31260] train loss: 1.0808, train acc: 0.5477, val loss: 1.0739, val acc: 0.5437  (best train acc: 0.5679, best val acc: 0.5639)\n",
      "[Epoch: 31280] train loss: 1.0945, train acc: 0.5519, val loss: 1.0667, val acc: 0.5514  (best train acc: 0.5679, best val acc: 0.5639)\n",
      "[Epoch: 31300] train loss: 1.0705, train acc: 0.5495, val loss: 1.0669, val acc: 0.5508  (best train acc: 0.5679, best val acc: 0.5639)\n",
      "[Epoch: 31320] train loss: 1.0791, train acc: 0.5546, val loss: 1.0652, val acc: 0.5582  (best train acc: 0.5679, best val acc: 0.5639)\n",
      "[Epoch: 31340] train loss: 1.1046, train acc: 0.5481, val loss: 1.0760, val acc: 0.5511  (best train acc: 0.5679, best val acc: 0.5639)\n",
      "[Epoch: 31360] train loss: 1.0958, train acc: 0.5452, val loss: 1.0755, val acc: 0.5524  (best train acc: 0.5679, best val acc: 0.5639)\n",
      "[Epoch: 31380] train loss: 1.0738, train acc: 0.5572, val loss: 1.0656, val acc: 0.5582  (best train acc: 0.5679, best val acc: 0.5639)\n",
      "[Epoch: 31400] train loss: 1.0884, train acc: 0.5479, val loss: 1.0708, val acc: 0.5541  (best train acc: 0.5679, best val acc: 0.5639)\n",
      "[Epoch: 31420] train loss: 1.0694, train acc: 0.5522, val loss: 1.0693, val acc: 0.5592  (best train acc: 0.5679, best val acc: 0.5639)\n",
      "[Epoch: 31440] train loss: 1.1002, train acc: 0.5359, val loss: 1.0734, val acc: 0.5538  (best train acc: 0.5679, best val acc: 0.5639)\n",
      "[Epoch: 31460] train loss: 1.0969, train acc: 0.5472, val loss: 1.0763, val acc: 0.5437  (best train acc: 0.5679, best val acc: 0.5639)\n",
      "[Epoch: 31480] train loss: 1.0889, train acc: 0.5510, val loss: 1.0649, val acc: 0.5548  (best train acc: 0.5679, best val acc: 0.5639)\n",
      "[Epoch: 31500] train loss: 1.0951, train acc: 0.5462, val loss: 1.0647, val acc: 0.5555  (best train acc: 0.5679, best val acc: 0.5639)\n",
      "[Epoch: 31520] train loss: 1.0935, train acc: 0.5293, val loss: 1.0654, val acc: 0.5551  (best train acc: 0.5684, best val acc: 0.5639)\n",
      "[Epoch: 31540] train loss: 1.0826, train acc: 0.5578, val loss: 1.0844, val acc: 0.5497  (best train acc: 0.5684, best val acc: 0.5639)\n",
      "[Epoch: 31560] train loss: 1.0819, train acc: 0.5462, val loss: 1.0654, val acc: 0.5548  (best train acc: 0.5684, best val acc: 0.5639)\n",
      "[Epoch: 31580] train loss: 1.1175, train acc: 0.5160, val loss: 1.0938, val acc: 0.5474  (best train acc: 0.5688, best val acc: 0.5639)\n",
      "[Epoch: 31600] train loss: 1.1244, train acc: 0.5101, val loss: 1.0730, val acc: 0.5406  (best train acc: 0.5706, best val acc: 0.5639)\n",
      "[Epoch: 31620] train loss: 1.1051, train acc: 0.5308, val loss: 1.0673, val acc: 0.5558  (best train acc: 0.5706, best val acc: 0.5639)\n",
      "[Epoch: 31640] train loss: 1.0655, train acc: 0.5538, val loss: 1.0655, val acc: 0.5558  (best train acc: 0.5706, best val acc: 0.5639)\n",
      "[Epoch: 31660] train loss: 1.0619, train acc: 0.5614, val loss: 1.0648, val acc: 0.5595  (best train acc: 0.5706, best val acc: 0.5639)\n",
      "[Epoch: 31680] train loss: 1.0642, train acc: 0.5631, val loss: 1.0644, val acc: 0.5622  (best train acc: 0.5706, best val acc: 0.5639)\n",
      "[Epoch: 31700] train loss: 1.0936, train acc: 0.5423, val loss: 1.0644, val acc: 0.5578  (best train acc: 0.5706, best val acc: 0.5639)\n",
      "[Epoch: 31720] train loss: 1.1519, train acc: 0.5281, val loss: 1.1422, val acc: 0.5285  (best train acc: 0.5706, best val acc: 0.5639)\n",
      "[Epoch: 31740] train loss: 1.1328, train acc: 0.5211, val loss: 1.0726, val acc: 0.5444  (best train acc: 0.5706, best val acc: 0.5639)\n",
      "[Epoch: 31760] train loss: 1.0700, train acc: 0.5626, val loss: 1.0770, val acc: 0.5511  (best train acc: 0.5706, best val acc: 0.5639)\n",
      "[Epoch: 31780] train loss: 1.0762, train acc: 0.5525, val loss: 1.0752, val acc: 0.5518  (best train acc: 0.5706, best val acc: 0.5639)\n",
      "[Epoch: 31800] train loss: 1.0852, train acc: 0.5534, val loss: 1.0675, val acc: 0.5562  (best train acc: 0.5706, best val acc: 0.5639)\n",
      "[Epoch: 31820] train loss: 1.0917, train acc: 0.5394, val loss: 1.0649, val acc: 0.5616  (best train acc: 0.5706, best val acc: 0.5639)\n",
      "[Epoch: 31840] train loss: 1.0667, train acc: 0.5615, val loss: 1.0674, val acc: 0.5524  (best train acc: 0.5706, best val acc: 0.5639)\n",
      "[Epoch: 31860] train loss: 1.0661, train acc: 0.5625, val loss: 1.0704, val acc: 0.5551  (best train acc: 0.5706, best val acc: 0.5639)\n",
      "[Epoch: 31880] train loss: 1.0802, train acc: 0.5531, val loss: 1.0702, val acc: 0.5511  (best train acc: 0.5706, best val acc: 0.5639)\n",
      "[Epoch: 31900] train loss: 1.0877, train acc: 0.5483, val loss: 1.0684, val acc: 0.5548  (best train acc: 0.5706, best val acc: 0.5639)\n",
      "[Epoch: 31920] train loss: 1.1004, train acc: 0.5437, val loss: 1.0749, val acc: 0.5501  (best train acc: 0.5706, best val acc: 0.5639)\n",
      "[Epoch: 31940] train loss: 1.0659, train acc: 0.5607, val loss: 1.0666, val acc: 0.5585  (best train acc: 0.5706, best val acc: 0.5646)\n",
      "[Epoch: 31960] train loss: 1.0934, train acc: 0.5436, val loss: 1.0673, val acc: 0.5572  (best train acc: 0.5706, best val acc: 0.5646)\n",
      "[Epoch: 31980] train loss: 1.0637, train acc: 0.5670, val loss: 1.0722, val acc: 0.5548  (best train acc: 0.5706, best val acc: 0.5646)\n",
      "[Epoch: 32000] train loss: 1.0756, train acc: 0.5547, val loss: 1.0625, val acc: 0.5582  (best train acc: 0.5706, best val acc: 0.5656)\n",
      "[Epoch: 32020] train loss: 1.0812, train acc: 0.5502, val loss: 1.0627, val acc: 0.5616  (best train acc: 0.5706, best val acc: 0.5656)\n",
      "[Epoch: 32040] train loss: 1.0885, train acc: 0.5422, val loss: 1.0625, val acc: 0.5622  (best train acc: 0.5706, best val acc: 0.5656)\n",
      "[Epoch: 32060] train loss: 1.0861, train acc: 0.5536, val loss: 1.0627, val acc: 0.5562  (best train acc: 0.5706, best val acc: 0.5656)\n",
      "[Epoch: 32080] train loss: 1.0844, train acc: 0.5418, val loss: 1.0640, val acc: 0.5582  (best train acc: 0.5706, best val acc: 0.5656)\n",
      "[Epoch: 32100] train loss: 1.0795, train acc: 0.5408, val loss: 1.0689, val acc: 0.5565  (best train acc: 0.5706, best val acc: 0.5656)\n",
      "[Epoch: 32120] train loss: 1.0785, train acc: 0.5510, val loss: 1.0653, val acc: 0.5605  (best train acc: 0.5706, best val acc: 0.5656)\n",
      "[Epoch: 32140] train loss: 1.0661, train acc: 0.5531, val loss: 1.0638, val acc: 0.5582  (best train acc: 0.5706, best val acc: 0.5659)\n",
      "[Epoch: 32160] train loss: 1.0754, train acc: 0.5539, val loss: 1.0625, val acc: 0.5626  (best train acc: 0.5706, best val acc: 0.5659)\n",
      "[Epoch: 32180] train loss: 1.0770, train acc: 0.5570, val loss: 1.0613, val acc: 0.5619  (best train acc: 0.5706, best val acc: 0.5659)\n",
      "[Epoch: 32200] train loss: 1.0665, train acc: 0.5523, val loss: 1.0672, val acc: 0.5585  (best train acc: 0.5706, best val acc: 0.5659)\n",
      "[Epoch: 32220] train loss: 1.0840, train acc: 0.5516, val loss: 1.0675, val acc: 0.5592  (best train acc: 0.5706, best val acc: 0.5659)\n",
      "[Epoch: 32240] train loss: 1.0938, train acc: 0.5477, val loss: 1.0968, val acc: 0.5484  (best train acc: 0.5706, best val acc: 0.5659)\n",
      "[Epoch: 32260] train loss: 1.0995, train acc: 0.5367, val loss: 1.0616, val acc: 0.5568  (best train acc: 0.5706, best val acc: 0.5673)\n",
      "[Epoch: 32280] train loss: 1.0924, train acc: 0.5473, val loss: 1.0622, val acc: 0.5612  (best train acc: 0.5706, best val acc: 0.5673)\n",
      "[Epoch: 32300] train loss: 1.0649, train acc: 0.5603, val loss: 1.0647, val acc: 0.5599  (best train acc: 0.5706, best val acc: 0.5673)\n",
      "[Epoch: 32320] train loss: 1.1814, train acc: 0.4934, val loss: 1.1443, val acc: 0.5376  (best train acc: 0.5706, best val acc: 0.5673)\n",
      "[Epoch: 32340] train loss: 1.1051, train acc: 0.5412, val loss: 1.0809, val acc: 0.5528  (best train acc: 0.5706, best val acc: 0.5673)\n",
      "[Epoch: 32360] train loss: 1.1068, train acc: 0.5294, val loss: 1.0742, val acc: 0.5605  (best train acc: 0.5706, best val acc: 0.5673)\n",
      "[Epoch: 32380] train loss: 1.0962, train acc: 0.5534, val loss: 1.0652, val acc: 0.5582  (best train acc: 0.5706, best val acc: 0.5673)\n",
      "[Epoch: 32400] train loss: 1.0883, train acc: 0.5444, val loss: 1.0764, val acc: 0.5538  (best train acc: 0.5706, best val acc: 0.5676)\n",
      "[Epoch: 32420] train loss: 1.0859, train acc: 0.5515, val loss: 1.0615, val acc: 0.5575  (best train acc: 0.5706, best val acc: 0.5676)\n",
      "[Epoch: 32440] train loss: 1.0775, train acc: 0.5458, val loss: 1.0665, val acc: 0.5521  (best train acc: 0.5706, best val acc: 0.5676)\n",
      "[Epoch: 32460] train loss: 1.1164, train acc: 0.5199, val loss: 1.1127, val acc: 0.5467  (best train acc: 0.5706, best val acc: 0.5676)\n",
      "[Epoch: 32480] train loss: 1.0819, train acc: 0.5578, val loss: 1.0736, val acc: 0.5545  (best train acc: 0.5706, best val acc: 0.5676)\n",
      "[Epoch: 32500] train loss: 1.0668, train acc: 0.5568, val loss: 1.0830, val acc: 0.5545  (best train acc: 0.5706, best val acc: 0.5676)\n",
      "[Epoch: 32520] train loss: 1.0886, train acc: 0.5576, val loss: 1.0776, val acc: 0.5511  (best train acc: 0.5706, best val acc: 0.5676)\n",
      "[Epoch: 32540] train loss: 1.0774, train acc: 0.5545, val loss: 1.0727, val acc: 0.5524  (best train acc: 0.5706, best val acc: 0.5676)\n",
      "[Epoch: 32560] train loss: 1.1007, train acc: 0.5400, val loss: 1.0739, val acc: 0.5565  (best train acc: 0.5706, best val acc: 0.5676)\n",
      "[Epoch: 32580] train loss: 1.0961, train acc: 0.5269, val loss: 1.0841, val acc: 0.5359  (best train acc: 0.5706, best val acc: 0.5676)\n",
      "[Epoch: 32600] train loss: 1.0644, train acc: 0.5563, val loss: 1.0755, val acc: 0.5450  (best train acc: 0.5706, best val acc: 0.5676)\n",
      "[Epoch: 32620] train loss: 1.0771, train acc: 0.5392, val loss: 1.0637, val acc: 0.5565  (best train acc: 0.5706, best val acc: 0.5676)\n",
      "[Epoch: 32640] train loss: 1.0691, train acc: 0.5626, val loss: 1.0764, val acc: 0.5562  (best train acc: 0.5706, best val acc: 0.5676)\n",
      "[Epoch: 32660] train loss: 1.0883, train acc: 0.5479, val loss: 1.0679, val acc: 0.5508  (best train acc: 0.5706, best val acc: 0.5676)\n",
      "[Epoch: 32680] train loss: 1.0750, train acc: 0.5519, val loss: 1.0638, val acc: 0.5582  (best train acc: 0.5706, best val acc: 0.5676)\n",
      "[Epoch: 32700] train loss: 1.0841, train acc: 0.5390, val loss: 1.0639, val acc: 0.5575  (best train acc: 0.5706, best val acc: 0.5676)\n",
      "[Epoch: 32720] train loss: 1.0690, train acc: 0.5526, val loss: 1.0613, val acc: 0.5642  (best train acc: 0.5706, best val acc: 0.5676)\n",
      "[Epoch: 32740] train loss: 1.0896, train acc: 0.5492, val loss: 1.0609, val acc: 0.5585  (best train acc: 0.5706, best val acc: 0.5676)\n",
      "[Epoch: 32760] train loss: 1.0695, train acc: 0.5642, val loss: 1.0647, val acc: 0.5619  (best train acc: 0.5706, best val acc: 0.5676)\n",
      "[Epoch: 32780] train loss: 1.0852, train acc: 0.5477, val loss: 1.0613, val acc: 0.5605  (best train acc: 0.5706, best val acc: 0.5676)\n",
      "[Epoch: 32800] train loss: 1.0749, train acc: 0.5607, val loss: 1.0667, val acc: 0.5585  (best train acc: 0.5706, best val acc: 0.5676)\n",
      "[Epoch: 32820] train loss: 1.0733, train acc: 0.5521, val loss: 1.0663, val acc: 0.5619  (best train acc: 0.5706, best val acc: 0.5676)\n",
      "[Epoch: 32840] train loss: 1.1143, train acc: 0.5403, val loss: 1.0613, val acc: 0.5659  (best train acc: 0.5706, best val acc: 0.5690)\n",
      "[Epoch: 32860] train loss: 1.0624, train acc: 0.5594, val loss: 1.0615, val acc: 0.5575  (best train acc: 0.5706, best val acc: 0.5690)\n",
      "[Epoch: 32880] train loss: 1.0613, train acc: 0.5599, val loss: 1.0608, val acc: 0.5629  (best train acc: 0.5706, best val acc: 0.5690)\n",
      "[Epoch: 32900] train loss: 1.0764, train acc: 0.5496, val loss: 1.0776, val acc: 0.5562  (best train acc: 0.5708, best val acc: 0.5690)\n",
      "[Epoch: 32920] train loss: 1.0816, train acc: 0.5534, val loss: 1.1071, val acc: 0.5207  (best train acc: 0.5708, best val acc: 0.5690)\n",
      "[Epoch: 32940] train loss: 1.0692, train acc: 0.5546, val loss: 1.0832, val acc: 0.5531  (best train acc: 0.5708, best val acc: 0.5690)\n",
      "[Epoch: 32960] train loss: 1.0709, train acc: 0.5527, val loss: 1.0716, val acc: 0.5491  (best train acc: 0.5708, best val acc: 0.5690)\n",
      "[Epoch: 32980] train loss: 1.0766, train acc: 0.5496, val loss: 1.0654, val acc: 0.5497  (best train acc: 0.5717, best val acc: 0.5690)\n",
      "[Epoch: 33000] train loss: 1.0835, train acc: 0.5498, val loss: 1.0853, val acc: 0.5423  (best train acc: 0.5717, best val acc: 0.5690)\n",
      "[Epoch: 33020] train loss: 1.1144, train acc: 0.5408, val loss: 1.0762, val acc: 0.5487  (best train acc: 0.5717, best val acc: 0.5690)\n",
      "[Epoch: 33040] train loss: 1.1227, train acc: 0.5375, val loss: 1.0640, val acc: 0.5616  (best train acc: 0.5717, best val acc: 0.5690)\n",
      "[Epoch: 33060] train loss: 1.0759, train acc: 0.5501, val loss: 1.0610, val acc: 0.5568  (best train acc: 0.5717, best val acc: 0.5690)\n",
      "[Epoch: 33080] train loss: 1.0678, train acc: 0.5494, val loss: 1.0806, val acc: 0.5521  (best train acc: 0.5717, best val acc: 0.5690)\n",
      "[Epoch: 33100] train loss: 1.1021, train acc: 0.5419, val loss: 1.0665, val acc: 0.5622  (best train acc: 0.5717, best val acc: 0.5690)\n",
      "[Epoch: 33120] train loss: 1.0792, train acc: 0.5547, val loss: 1.0678, val acc: 0.5578  (best train acc: 0.5717, best val acc: 0.5690)\n",
      "[Epoch: 33140] train loss: 1.1018, train acc: 0.5445, val loss: 1.0753, val acc: 0.5511  (best train acc: 0.5717, best val acc: 0.5690)\n",
      "[Epoch: 33160] train loss: 1.1032, train acc: 0.5243, val loss: 1.0751, val acc: 0.5548  (best train acc: 0.5717, best val acc: 0.5690)\n",
      "[Epoch: 33180] train loss: 1.0740, train acc: 0.5582, val loss: 1.0610, val acc: 0.5602  (best train acc: 0.5717, best val acc: 0.5690)\n",
      "[Epoch: 33200] train loss: 1.0658, train acc: 0.5522, val loss: 1.0650, val acc: 0.5582  (best train acc: 0.5717, best val acc: 0.5690)\n",
      "[Epoch: 33220] train loss: 1.0655, train acc: 0.5626, val loss: 1.0596, val acc: 0.5649  (best train acc: 0.5717, best val acc: 0.5690)\n",
      "[Epoch: 33240] train loss: 1.0909, train acc: 0.5374, val loss: 1.0594, val acc: 0.5612  (best train acc: 0.5717, best val acc: 0.5690)\n",
      "[Epoch: 33260] train loss: 1.0792, train acc: 0.5521, val loss: 1.0599, val acc: 0.5659  (best train acc: 0.5717, best val acc: 0.5690)\n",
      "[Epoch: 33280] train loss: 1.0995, train acc: 0.5398, val loss: 1.0740, val acc: 0.5531  (best train acc: 0.5717, best val acc: 0.5690)\n",
      "[Epoch: 33300] train loss: 1.0878, train acc: 0.5551, val loss: 1.0630, val acc: 0.5616  (best train acc: 0.5717, best val acc: 0.5690)\n",
      "[Epoch: 33320] train loss: 1.0579, train acc: 0.5587, val loss: 1.0849, val acc: 0.5447  (best train acc: 0.5717, best val acc: 0.5690)\n",
      "[Epoch: 33340] train loss: 1.0784, train acc: 0.5512, val loss: 1.0690, val acc: 0.5595  (best train acc: 0.5717, best val acc: 0.5690)\n",
      "[Epoch: 33360] train loss: 1.0859, train acc: 0.5523, val loss: 1.0601, val acc: 0.5589  (best train acc: 0.5717, best val acc: 0.5690)\n",
      "[Epoch: 33380] train loss: 1.1124, train acc: 0.5431, val loss: 1.0933, val acc: 0.5460  (best train acc: 0.5717, best val acc: 0.5690)\n",
      "[Epoch: 33400] train loss: 1.1001, train acc: 0.5440, val loss: 1.0690, val acc: 0.5521  (best train acc: 0.5717, best val acc: 0.5690)\n",
      "[Epoch: 33420] train loss: 1.0970, train acc: 0.5380, val loss: 1.0677, val acc: 0.5551  (best train acc: 0.5717, best val acc: 0.5690)\n",
      "[Epoch: 33440] train loss: 1.1055, train acc: 0.5368, val loss: 1.0941, val acc: 0.5514  (best train acc: 0.5717, best val acc: 0.5690)\n",
      "[Epoch: 33460] train loss: 1.0814, train acc: 0.5488, val loss: 1.0605, val acc: 0.5602  (best train acc: 0.5717, best val acc: 0.5690)\n",
      "[Epoch: 33480] train loss: 1.0858, train acc: 0.5436, val loss: 1.0609, val acc: 0.5622  (best train acc: 0.5717, best val acc: 0.5690)\n",
      "[Epoch: 33500] train loss: 1.0762, train acc: 0.5555, val loss: 1.0641, val acc: 0.5605  (best train acc: 0.5717, best val acc: 0.5690)\n",
      "[Epoch: 33520] train loss: 1.0779, train acc: 0.5493, val loss: 1.0611, val acc: 0.5592  (best train acc: 0.5717, best val acc: 0.5690)\n",
      "[Epoch: 33540] train loss: 1.0990, train acc: 0.5440, val loss: 1.0588, val acc: 0.5649  (best train acc: 0.5717, best val acc: 0.5690)\n",
      "[Epoch: 33560] train loss: 1.0767, train acc: 0.5491, val loss: 1.0718, val acc: 0.5575  (best train acc: 0.5717, best val acc: 0.5690)\n",
      "[Epoch: 33580] train loss: 1.1076, train acc: 0.5286, val loss: 1.0622, val acc: 0.5626  (best train acc: 0.5717, best val acc: 0.5690)\n",
      "[Epoch: 33600] train loss: 1.0658, train acc: 0.5626, val loss: 1.0592, val acc: 0.5646  (best train acc: 0.5717, best val acc: 0.5690)\n",
      "[Epoch: 33620] train loss: 1.0891, train acc: 0.5392, val loss: 1.0596, val acc: 0.5636  (best train acc: 0.5717, best val acc: 0.5690)\n",
      "[Epoch: 33640] train loss: 1.0886, train acc: 0.5515, val loss: 1.0627, val acc: 0.5602  (best train acc: 0.5717, best val acc: 0.5690)\n",
      "[Epoch: 33660] train loss: 1.0779, train acc: 0.5508, val loss: 1.0651, val acc: 0.5605  (best train acc: 0.5717, best val acc: 0.5690)\n",
      "[Epoch: 33680] train loss: 1.0821, train acc: 0.5585, val loss: 1.0589, val acc: 0.5636  (best train acc: 0.5717, best val acc: 0.5690)\n",
      "[Epoch: 33700] train loss: 1.0677, train acc: 0.5518, val loss: 1.0626, val acc: 0.5612  (best train acc: 0.5717, best val acc: 0.5690)\n",
      "[Epoch: 33720] train loss: 1.0901, train acc: 0.5539, val loss: 1.0602, val acc: 0.5632  (best train acc: 0.5717, best val acc: 0.5690)\n",
      "[Epoch: 33740] train loss: 1.0887, train acc: 0.5380, val loss: 1.0588, val acc: 0.5626  (best train acc: 0.5717, best val acc: 0.5690)\n",
      "[Epoch: 33760] train loss: 1.0956, train acc: 0.5416, val loss: 1.0576, val acc: 0.5609  (best train acc: 0.5717, best val acc: 0.5690)\n",
      "[Epoch: 33780] train loss: 1.0880, train acc: 0.5534, val loss: 1.0586, val acc: 0.5612  (best train acc: 0.5717, best val acc: 0.5690)\n",
      "[Epoch: 33800] train loss: 1.0677, train acc: 0.5583, val loss: 1.0579, val acc: 0.5619  (best train acc: 0.5717, best val acc: 0.5690)\n",
      "[Epoch: 33820] train loss: 1.0957, train acc: 0.5493, val loss: 1.0817, val acc: 0.5474  (best train acc: 0.5717, best val acc: 0.5690)\n",
      "[Epoch: 33840] train loss: 1.0767, train acc: 0.5452, val loss: 1.0591, val acc: 0.5632  (best train acc: 0.5717, best val acc: 0.5690)\n",
      "[Epoch: 33860] train loss: 1.0914, train acc: 0.5374, val loss: 1.0587, val acc: 0.5572  (best train acc: 0.5717, best val acc: 0.5696)\n",
      "[Epoch: 33880] train loss: 1.1215, train acc: 0.5421, val loss: 1.0575, val acc: 0.5602  (best train acc: 0.5717, best val acc: 0.5696)\n",
      "[Epoch: 33900] train loss: 1.0798, train acc: 0.5452, val loss: 1.0591, val acc: 0.5649  (best train acc: 0.5717, best val acc: 0.5700)\n",
      "[Epoch: 33920] train loss: 1.0774, train acc: 0.5608, val loss: 1.0712, val acc: 0.5558  (best train acc: 0.5717, best val acc: 0.5700)\n",
      "[Epoch: 33940] train loss: 1.0768, train acc: 0.5518, val loss: 1.0574, val acc: 0.5663  (best train acc: 0.5717, best val acc: 0.5700)\n",
      "[Epoch: 33960] train loss: 1.0664, train acc: 0.5573, val loss: 1.0649, val acc: 0.5599  (best train acc: 0.5717, best val acc: 0.5700)\n",
      "[Epoch: 33980] train loss: 1.0606, train acc: 0.5566, val loss: 1.0768, val acc: 0.5541  (best train acc: 0.5717, best val acc: 0.5700)\n",
      "[Epoch: 34000] train loss: 1.0898, train acc: 0.5483, val loss: 1.0578, val acc: 0.5578  (best train acc: 0.5717, best val acc: 0.5700)\n",
      "[Epoch: 34020] train loss: 1.0992, train acc: 0.5404, val loss: 1.0669, val acc: 0.5562  (best train acc: 0.5717, best val acc: 0.5700)\n",
      "[Epoch: 34040] train loss: 1.0946, train acc: 0.5517, val loss: 1.0667, val acc: 0.5595  (best train acc: 0.5717, best val acc: 0.5700)\n",
      "[Epoch: 34060] train loss: 1.0609, train acc: 0.5622, val loss: 1.0753, val acc: 0.5514  (best train acc: 0.5717, best val acc: 0.5700)\n",
      "[Epoch: 34080] train loss: 1.0882, train acc: 0.5475, val loss: 1.0656, val acc: 0.5602  (best train acc: 0.5717, best val acc: 0.5700)\n",
      "[Epoch: 34100] train loss: 1.0798, train acc: 0.5560, val loss: 1.0865, val acc: 0.5410  (best train acc: 0.5717, best val acc: 0.5700)\n",
      "[Epoch: 34120] train loss: 1.0938, train acc: 0.5359, val loss: 1.0724, val acc: 0.5575  (best train acc: 0.5717, best val acc: 0.5700)\n",
      "[Epoch: 34140] train loss: 1.0966, train acc: 0.5374, val loss: 1.0604, val acc: 0.5642  (best train acc: 0.5717, best val acc: 0.5700)\n",
      "[Epoch: 34160] train loss: 1.0832, train acc: 0.5416, val loss: 1.0698, val acc: 0.5545  (best train acc: 0.5717, best val acc: 0.5700)\n",
      "[Epoch: 34180] train loss: 1.1164, train acc: 0.5362, val loss: 1.0644, val acc: 0.5541  (best train acc: 0.5717, best val acc: 0.5700)\n",
      "[Epoch: 34200] train loss: 1.0699, train acc: 0.5551, val loss: 1.0610, val acc: 0.5572  (best train acc: 0.5717, best val acc: 0.5700)\n",
      "[Epoch: 34220] train loss: 1.0892, train acc: 0.5472, val loss: 1.0868, val acc: 0.5427  (best train acc: 0.5717, best val acc: 0.5700)\n",
      "[Epoch: 34240] train loss: 1.0911, train acc: 0.5513, val loss: 1.0572, val acc: 0.5626  (best train acc: 0.5717, best val acc: 0.5700)\n",
      "[Epoch: 34260] train loss: 1.0803, train acc: 0.5574, val loss: 1.0586, val acc: 0.5653  (best train acc: 0.5717, best val acc: 0.5700)\n",
      "[Epoch: 34280] train loss: 1.0818, train acc: 0.5510, val loss: 1.0591, val acc: 0.5646  (best train acc: 0.5717, best val acc: 0.5700)\n",
      "[Epoch: 34300] train loss: 1.0829, train acc: 0.5516, val loss: 1.0619, val acc: 0.5592  (best train acc: 0.5717, best val acc: 0.5700)\n",
      "[Epoch: 34320] train loss: 1.0813, train acc: 0.5484, val loss: 1.0633, val acc: 0.5616  (best train acc: 0.5717, best val acc: 0.5700)\n",
      "[Epoch: 34340] train loss: 1.0781, train acc: 0.5547, val loss: 1.0987, val acc: 0.5312  (best train acc: 0.5717, best val acc: 0.5700)\n",
      "[Epoch: 34360] train loss: 1.0977, train acc: 0.5334, val loss: 1.0667, val acc: 0.5558  (best train acc: 0.5717, best val acc: 0.5700)\n",
      "[Epoch: 34380] train loss: 1.0848, train acc: 0.5429, val loss: 1.0902, val acc: 0.5268  (best train acc: 0.5717, best val acc: 0.5700)\n",
      "[Epoch: 34400] train loss: 1.0938, train acc: 0.5368, val loss: 1.0787, val acc: 0.5410  (best train acc: 0.5717, best val acc: 0.5700)\n",
      "[Epoch: 34420] train loss: 1.0826, train acc: 0.5491, val loss: 1.0783, val acc: 0.5470  (best train acc: 0.5717, best val acc: 0.5700)\n",
      "[Epoch: 34440] train loss: 1.0837, train acc: 0.5574, val loss: 1.0598, val acc: 0.5619  (best train acc: 0.5717, best val acc: 0.5720)\n",
      "[Epoch: 34460] train loss: 1.0680, train acc: 0.5592, val loss: 1.0705, val acc: 0.5602  (best train acc: 0.5717, best val acc: 0.5720)\n",
      "[Epoch: 34480] train loss: 1.1225, train acc: 0.5383, val loss: 1.0565, val acc: 0.5646  (best train acc: 0.5717, best val acc: 0.5720)\n",
      "[Epoch: 34500] train loss: 1.0646, train acc: 0.5578, val loss: 1.0566, val acc: 0.5616  (best train acc: 0.5717, best val acc: 0.5720)\n",
      "[Epoch: 34520] train loss: 1.0710, train acc: 0.5528, val loss: 1.0590, val acc: 0.5582  (best train acc: 0.5717, best val acc: 0.5720)\n",
      "[Epoch: 34540] train loss: 1.0685, train acc: 0.5581, val loss: 1.0562, val acc: 0.5659  (best train acc: 0.5717, best val acc: 0.5720)\n",
      "[Epoch: 34560] train loss: 1.0820, train acc: 0.5468, val loss: 1.0589, val acc: 0.5622  (best train acc: 0.5717, best val acc: 0.5720)\n",
      "[Epoch: 34580] train loss: 1.0979, train acc: 0.5453, val loss: 1.0591, val acc: 0.5629  (best train acc: 0.5717, best val acc: 0.5720)\n",
      "[Epoch: 34600] train loss: 1.0825, train acc: 0.5534, val loss: 1.0622, val acc: 0.5575  (best train acc: 0.5717, best val acc: 0.5720)\n",
      "[Epoch: 34620] train loss: 1.0817, train acc: 0.5478, val loss: 1.0612, val acc: 0.5649  (best train acc: 0.5717, best val acc: 0.5720)\n",
      "[Epoch: 34640] train loss: 1.0745, train acc: 0.5489, val loss: 1.0627, val acc: 0.5642  (best train acc: 0.5717, best val acc: 0.5720)\n",
      "[Epoch: 34660] train loss: 1.0569, train acc: 0.5719, val loss: 1.0568, val acc: 0.5646  (best train acc: 0.5719, best val acc: 0.5720)\n",
      "[Epoch: 34680] train loss: 1.0789, train acc: 0.5515, val loss: 1.0673, val acc: 0.5562  (best train acc: 0.5719, best val acc: 0.5720)\n",
      "[Epoch: 34700] train loss: 1.0873, train acc: 0.5469, val loss: 1.0566, val acc: 0.5622  (best train acc: 0.5719, best val acc: 0.5720)\n",
      "[Epoch: 34720] train loss: 1.0696, train acc: 0.5601, val loss: 1.0738, val acc: 0.5535  (best train acc: 0.5719, best val acc: 0.5720)\n",
      "[Epoch: 34740] train loss: 1.0966, train acc: 0.5446, val loss: 1.0583, val acc: 0.5656  (best train acc: 0.5719, best val acc: 0.5720)\n",
      "[Epoch: 34760] train loss: 1.1231, train acc: 0.5409, val loss: 1.0667, val acc: 0.5619  (best train acc: 0.5719, best val acc: 0.5720)\n",
      "[Epoch: 34780] train loss: 1.0658, train acc: 0.5533, val loss: 1.0694, val acc: 0.5602  (best train acc: 0.5719, best val acc: 0.5720)\n",
      "[Epoch: 34800] train loss: 1.0851, train acc: 0.5469, val loss: 1.0619, val acc: 0.5609  (best train acc: 0.5719, best val acc: 0.5720)\n",
      "[Epoch: 34820] train loss: 1.0870, train acc: 0.5452, val loss: 1.0708, val acc: 0.5558  (best train acc: 0.5719, best val acc: 0.5720)\n",
      "[Epoch: 34840] train loss: 1.0692, train acc: 0.5533, val loss: 1.0550, val acc: 0.5629  (best train acc: 0.5719, best val acc: 0.5720)\n",
      "[Epoch: 34860] train loss: 1.0829, train acc: 0.5463, val loss: 1.0570, val acc: 0.5592  (best train acc: 0.5719, best val acc: 0.5720)\n",
      "[Epoch: 34880] train loss: 1.0756, train acc: 0.5560, val loss: 1.0553, val acc: 0.5636  (best train acc: 0.5719, best val acc: 0.5720)\n",
      "[Epoch: 34900] train loss: 1.0900, train acc: 0.5470, val loss: 1.0560, val acc: 0.5629  (best train acc: 0.5719, best val acc: 0.5720)\n",
      "[Epoch: 34920] train loss: 1.0786, train acc: 0.5583, val loss: 1.0551, val acc: 0.5649  (best train acc: 0.5719, best val acc: 0.5720)\n",
      "[Epoch: 34940] train loss: 1.0890, train acc: 0.5423, val loss: 1.0545, val acc: 0.5653  (best train acc: 0.5719, best val acc: 0.5720)\n",
      "[Epoch: 34960] train loss: 1.0799, train acc: 0.5531, val loss: 1.0564, val acc: 0.5700  (best train acc: 0.5719, best val acc: 0.5720)\n",
      "[Epoch: 34980] train loss: 1.0689, train acc: 0.5537, val loss: 1.0550, val acc: 0.5626  (best train acc: 0.5719, best val acc: 0.5720)\n",
      "[Epoch: 35000] train loss: 1.0812, train acc: 0.5535, val loss: 1.0538, val acc: 0.5669  (best train acc: 0.5719, best val acc: 0.5720)\n",
      "[Epoch: 35020] train loss: 1.0499, train acc: 0.5711, val loss: 1.0552, val acc: 0.5669  (best train acc: 0.5719, best val acc: 0.5720)\n",
      "[Epoch: 35040] train loss: 1.0801, train acc: 0.5510, val loss: 1.0539, val acc: 0.5666  (best train acc: 0.5719, best val acc: 0.5720)\n",
      "[Epoch: 35060] train loss: 1.1151, train acc: 0.5360, val loss: 1.0550, val acc: 0.5683  (best train acc: 0.5719, best val acc: 0.5720)\n",
      "[Epoch: 35080] train loss: 1.0731, train acc: 0.5573, val loss: 1.0546, val acc: 0.5646  (best train acc: 0.5719, best val acc: 0.5720)\n",
      "[Epoch: 35100] train loss: 1.0950, train acc: 0.5471, val loss: 1.0577, val acc: 0.5639  (best train acc: 0.5719, best val acc: 0.5720)\n",
      "[Epoch: 35120] train loss: 1.0855, train acc: 0.5468, val loss: 1.0539, val acc: 0.5690  (best train acc: 0.5719, best val acc: 0.5720)\n",
      "[Epoch: 35140] train loss: 1.1198, train acc: 0.5205, val loss: 1.0574, val acc: 0.5565  (best train acc: 0.5719, best val acc: 0.5720)\n",
      "[Epoch: 35160] train loss: 1.1142, train acc: 0.5298, val loss: 1.0956, val acc: 0.5413  (best train acc: 0.5719, best val acc: 0.5720)\n",
      "[Epoch: 35180] train loss: 1.0923, train acc: 0.5462, val loss: 1.0732, val acc: 0.5487  (best train acc: 0.5719, best val acc: 0.5720)\n",
      "[Epoch: 35200] train loss: 1.0917, train acc: 0.5448, val loss: 1.0711, val acc: 0.5575  (best train acc: 0.5719, best val acc: 0.5720)\n",
      "[Epoch: 35220] train loss: 1.0940, train acc: 0.5468, val loss: 1.0707, val acc: 0.5494  (best train acc: 0.5719, best val acc: 0.5720)\n",
      "[Epoch: 35240] train loss: 1.0862, train acc: 0.5507, val loss: 1.0703, val acc: 0.5524  (best train acc: 0.5719, best val acc: 0.5720)\n",
      "[Epoch: 35260] train loss: 1.0901, train acc: 0.5459, val loss: 1.0690, val acc: 0.5531  (best train acc: 0.5719, best val acc: 0.5720)\n",
      "[Epoch: 35280] train loss: 1.0632, train acc: 0.5609, val loss: 1.0672, val acc: 0.5555  (best train acc: 0.5719, best val acc: 0.5720)\n",
      "[Epoch: 35300] train loss: 1.0822, train acc: 0.5536, val loss: 1.0648, val acc: 0.5619  (best train acc: 0.5719, best val acc: 0.5720)\n",
      "[Epoch: 35320] train loss: 1.1104, train acc: 0.5406, val loss: 1.0603, val acc: 0.5653  (best train acc: 0.5719, best val acc: 0.5720)\n",
      "[Epoch: 35340] train loss: 1.1157, train acc: 0.5317, val loss: 1.0649, val acc: 0.5555  (best train acc: 0.5719, best val acc: 0.5720)\n",
      "[Epoch: 35360] train loss: 1.0690, train acc: 0.5552, val loss: 1.0635, val acc: 0.5558  (best train acc: 0.5719, best val acc: 0.5720)\n",
      "[Epoch: 35380] train loss: 1.0966, train acc: 0.5374, val loss: 1.0559, val acc: 0.5666  (best train acc: 0.5719, best val acc: 0.5720)\n",
      "[Epoch: 35400] train loss: 1.0955, train acc: 0.5489, val loss: 1.0543, val acc: 0.5622  (best train acc: 0.5719, best val acc: 0.5720)\n",
      "[Epoch: 35420] train loss: 1.0864, train acc: 0.5518, val loss: 1.0591, val acc: 0.5619  (best train acc: 0.5719, best val acc: 0.5720)\n",
      "[Epoch: 35440] train loss: 1.0964, train acc: 0.5448, val loss: 1.0587, val acc: 0.5612  (best train acc: 0.5719, best val acc: 0.5720)\n",
      "[Epoch: 35460] train loss: 1.0824, train acc: 0.5528, val loss: 1.0579, val acc: 0.5575  (best train acc: 0.5719, best val acc: 0.5720)\n",
      "[Epoch: 35480] train loss: 1.0682, train acc: 0.5548, val loss: 1.1127, val acc: 0.5218  (best train acc: 0.5719, best val acc: 0.5720)\n",
      "[Epoch: 35500] train loss: 1.0685, train acc: 0.5441, val loss: 1.0596, val acc: 0.5595  (best train acc: 0.5719, best val acc: 0.5720)\n",
      "[Epoch: 35520] train loss: 1.0654, train acc: 0.5529, val loss: 1.0619, val acc: 0.5612  (best train acc: 0.5719, best val acc: 0.5720)\n",
      "[Epoch: 35540] train loss: 1.0653, train acc: 0.5531, val loss: 1.0704, val acc: 0.5555  (best train acc: 0.5719, best val acc: 0.5720)\n",
      "[Epoch: 35560] train loss: 1.0643, train acc: 0.5531, val loss: 1.0548, val acc: 0.5646  (best train acc: 0.5719, best val acc: 0.5720)\n",
      "[Epoch: 35580] train loss: 1.0911, train acc: 0.5471, val loss: 1.0770, val acc: 0.5518  (best train acc: 0.5719, best val acc: 0.5720)\n",
      "[Epoch: 35600] train loss: 1.1175, train acc: 0.5337, val loss: 1.0578, val acc: 0.5548  (best train acc: 0.5719, best val acc: 0.5720)\n",
      "[Epoch: 35620] train loss: 1.0783, train acc: 0.5502, val loss: 1.1132, val acc: 0.5433  (best train acc: 0.5719, best val acc: 0.5720)\n",
      "[Epoch: 35640] train loss: 1.1171, train acc: 0.5391, val loss: 1.0692, val acc: 0.5578  (best train acc: 0.5719, best val acc: 0.5720)\n",
      "[Epoch: 35660] train loss: 1.0715, train acc: 0.5505, val loss: 1.0590, val acc: 0.5639  (best train acc: 0.5719, best val acc: 0.5720)\n",
      "[Epoch: 35680] train loss: 1.0813, train acc: 0.5538, val loss: 1.0582, val acc: 0.5575  (best train acc: 0.5719, best val acc: 0.5720)\n",
      "[Epoch: 35700] train loss: 1.0809, train acc: 0.5521, val loss: 1.0718, val acc: 0.5541  (best train acc: 0.5719, best val acc: 0.5720)\n",
      "[Epoch: 35720] train loss: 1.0828, train acc: 0.5530, val loss: 1.0578, val acc: 0.5632  (best train acc: 0.5719, best val acc: 0.5720)\n",
      "[Epoch: 35740] train loss: 1.0983, train acc: 0.5497, val loss: 1.0552, val acc: 0.5663  (best train acc: 0.5719, best val acc: 0.5720)\n",
      "[Epoch: 35760] train loss: 1.0811, train acc: 0.5562, val loss: 1.0564, val acc: 0.5629  (best train acc: 0.5719, best val acc: 0.5720)\n",
      "[Epoch: 35780] train loss: 1.1097, train acc: 0.5403, val loss: 1.0658, val acc: 0.5558  (best train acc: 0.5719, best val acc: 0.5720)\n",
      "[Epoch: 35800] train loss: 1.0609, train acc: 0.5533, val loss: 1.0575, val acc: 0.5622  (best train acc: 0.5719, best val acc: 0.5720)\n",
      "[Epoch: 35820] train loss: 1.1052, train acc: 0.5440, val loss: 1.0556, val acc: 0.5639  (best train acc: 0.5719, best val acc: 0.5720)\n",
      "[Epoch: 35840] train loss: 1.0868, train acc: 0.5551, val loss: 1.0900, val acc: 0.5342  (best train acc: 0.5719, best val acc: 0.5720)\n",
      "[Epoch: 35860] train loss: 1.0626, train acc: 0.5604, val loss: 1.0543, val acc: 0.5642  (best train acc: 0.5719, best val acc: 0.5720)\n",
      "[Epoch: 35880] train loss: 1.0911, train acc: 0.5393, val loss: 1.0707, val acc: 0.5518  (best train acc: 0.5719, best val acc: 0.5720)\n",
      "[Epoch: 35900] train loss: 1.0548, train acc: 0.5617, val loss: 1.0546, val acc: 0.5642  (best train acc: 0.5719, best val acc: 0.5720)\n",
      "[Epoch: 35920] train loss: 1.0666, train acc: 0.5543, val loss: 1.0535, val acc: 0.5649  (best train acc: 0.5719, best val acc: 0.5720)\n",
      "[Epoch: 35940] train loss: 1.0629, train acc: 0.5614, val loss: 1.0562, val acc: 0.5619  (best train acc: 0.5719, best val acc: 0.5720)\n",
      "[Epoch: 35960] train loss: 1.0851, train acc: 0.5450, val loss: 1.0582, val acc: 0.5595  (best train acc: 0.5719, best val acc: 0.5720)\n",
      "[Epoch: 35980] train loss: 1.0588, train acc: 0.5630, val loss: 1.0714, val acc: 0.5524  (best train acc: 0.5719, best val acc: 0.5720)\n",
      "[Epoch: 36000] train loss: 1.0659, train acc: 0.5597, val loss: 1.0718, val acc: 0.5555  (best train acc: 0.5719, best val acc: 0.5720)\n",
      "[Epoch: 36020] train loss: 1.0916, train acc: 0.5448, val loss: 1.0534, val acc: 0.5703  (best train acc: 0.5719, best val acc: 0.5720)\n",
      "[Epoch: 36040] train loss: 1.0845, train acc: 0.5429, val loss: 1.0732, val acc: 0.5484  (best train acc: 0.5719, best val acc: 0.5720)\n",
      "[Epoch: 36060] train loss: 1.0889, train acc: 0.5504, val loss: 1.0640, val acc: 0.5555  (best train acc: 0.5719, best val acc: 0.5720)\n",
      "[Epoch: 36080] train loss: 1.0645, train acc: 0.5592, val loss: 1.0530, val acc: 0.5642  (best train acc: 0.5719, best val acc: 0.5720)\n",
      "[Epoch: 36100] train loss: 1.0782, train acc: 0.5572, val loss: 1.0531, val acc: 0.5632  (best train acc: 0.5719, best val acc: 0.5720)\n",
      "[Epoch: 36120] train loss: 1.0548, train acc: 0.5594, val loss: 1.0803, val acc: 0.5474  (best train acc: 0.5719, best val acc: 0.5720)\n",
      "[Epoch: 36140] train loss: 1.1001, train acc: 0.5468, val loss: 1.0914, val acc: 0.5497  (best train acc: 0.5719, best val acc: 0.5720)\n",
      "[Epoch: 36160] train loss: 1.0813, train acc: 0.5487, val loss: 1.0541, val acc: 0.5609  (best train acc: 0.5719, best val acc: 0.5720)\n",
      "[Epoch: 36180] train loss: 1.0921, train acc: 0.5444, val loss: 1.0622, val acc: 0.5626  (best train acc: 0.5719, best val acc: 0.5720)\n",
      "[Epoch: 36200] train loss: 1.0591, train acc: 0.5639, val loss: 1.0533, val acc: 0.5636  (best train acc: 0.5719, best val acc: 0.5720)\n",
      "[Epoch: 36220] train loss: 1.0668, train acc: 0.5588, val loss: 1.0544, val acc: 0.5629  (best train acc: 0.5719, best val acc: 0.5720)\n",
      "[Epoch: 36240] train loss: 1.0945, train acc: 0.5369, val loss: 1.0549, val acc: 0.5669  (best train acc: 0.5719, best val acc: 0.5720)\n",
      "[Epoch: 36260] train loss: 1.0769, train acc: 0.5588, val loss: 1.0567, val acc: 0.5605  (best train acc: 0.5719, best val acc: 0.5720)\n",
      "[Epoch: 36280] train loss: 1.1245, train acc: 0.5164, val loss: 1.0531, val acc: 0.5676  (best train acc: 0.5719, best val acc: 0.5720)\n",
      "[Epoch: 36300] train loss: 1.0535, train acc: 0.5647, val loss: 1.0541, val acc: 0.5636  (best train acc: 0.5719, best val acc: 0.5720)\n",
      "[Epoch: 36320] train loss: 1.0641, train acc: 0.5599, val loss: 1.0621, val acc: 0.5589  (best train acc: 0.5744, best val acc: 0.5720)\n",
      "[Epoch: 36340] train loss: 1.0660, train acc: 0.5634, val loss: 1.0530, val acc: 0.5609  (best train acc: 0.5744, best val acc: 0.5720)\n",
      "[Epoch: 36360] train loss: 1.0625, train acc: 0.5645, val loss: 1.0717, val acc: 0.5541  (best train acc: 0.5744, best val acc: 0.5720)\n",
      "[Epoch: 36380] train loss: 1.0941, train acc: 0.5468, val loss: 1.0581, val acc: 0.5612  (best train acc: 0.5744, best val acc: 0.5720)\n",
      "[Epoch: 36400] train loss: 1.0723, train acc: 0.5562, val loss: 1.0525, val acc: 0.5663  (best train acc: 0.5744, best val acc: 0.5720)\n",
      "[Epoch: 36420] train loss: 1.0738, train acc: 0.5513, val loss: 1.0586, val acc: 0.5609  (best train acc: 0.5744, best val acc: 0.5720)\n",
      "[Epoch: 36440] train loss: 1.0826, train acc: 0.5577, val loss: 1.0577, val acc: 0.5632  (best train acc: 0.5744, best val acc: 0.5720)\n",
      "[Epoch: 36460] train loss: 1.0605, train acc: 0.5606, val loss: 1.0540, val acc: 0.5629  (best train acc: 0.5744, best val acc: 0.5720)\n",
      "[Epoch: 36480] train loss: 1.0816, train acc: 0.5533, val loss: 1.0558, val acc: 0.5653  (best train acc: 0.5744, best val acc: 0.5720)\n",
      "[Epoch: 36500] train loss: 1.0614, train acc: 0.5599, val loss: 1.0549, val acc: 0.5639  (best train acc: 0.5744, best val acc: 0.5720)\n",
      "[Epoch: 36520] train loss: 1.0732, train acc: 0.5528, val loss: 1.0523, val acc: 0.5636  (best train acc: 0.5744, best val acc: 0.5720)\n",
      "[Epoch: 36540] train loss: 1.1784, train acc: 0.4867, val loss: 1.0904, val acc: 0.5487  (best train acc: 0.5744, best val acc: 0.5720)\n",
      "[Epoch: 36560] train loss: 1.1057, train acc: 0.5475, val loss: 1.0543, val acc: 0.5619  (best train acc: 0.5744, best val acc: 0.5720)\n",
      "[Epoch: 36580] train loss: 1.1091, train acc: 0.5263, val loss: 1.0908, val acc: 0.5278  (best train acc: 0.5744, best val acc: 0.5720)\n",
      "[Epoch: 36600] train loss: 1.0969, train acc: 0.5355, val loss: 1.0592, val acc: 0.5555  (best train acc: 0.5744, best val acc: 0.5720)\n",
      "[Epoch: 36620] train loss: 1.0806, train acc: 0.5521, val loss: 1.0556, val acc: 0.5582  (best train acc: 0.5744, best val acc: 0.5720)\n",
      "[Epoch: 36640] train loss: 1.0640, train acc: 0.5611, val loss: 1.0558, val acc: 0.5616  (best train acc: 0.5744, best val acc: 0.5720)\n",
      "[Epoch: 36660] train loss: 1.0771, train acc: 0.5574, val loss: 1.0635, val acc: 0.5616  (best train acc: 0.5744, best val acc: 0.5720)\n",
      "[Epoch: 36680] train loss: 1.0737, train acc: 0.5569, val loss: 1.0539, val acc: 0.5592  (best train acc: 0.5744, best val acc: 0.5720)\n",
      "[Epoch: 36700] train loss: 1.0824, train acc: 0.5554, val loss: 1.0541, val acc: 0.5632  (best train acc: 0.5744, best val acc: 0.5720)\n",
      "[Epoch: 36720] train loss: 1.0696, train acc: 0.5539, val loss: 1.0558, val acc: 0.5619  (best train acc: 0.5744, best val acc: 0.5720)\n",
      "[Epoch: 36740] train loss: 1.0605, train acc: 0.5616, val loss: 1.0528, val acc: 0.5629  (best train acc: 0.5744, best val acc: 0.5720)\n",
      "[Epoch: 36760] train loss: 1.0702, train acc: 0.5530, val loss: 1.0602, val acc: 0.5629  (best train acc: 0.5744, best val acc: 0.5720)\n",
      "[Epoch: 36780] train loss: 1.0797, train acc: 0.5463, val loss: 1.0534, val acc: 0.5646  (best train acc: 0.5744, best val acc: 0.5720)\n",
      "[Epoch: 36800] train loss: 1.0680, train acc: 0.5609, val loss: 1.0520, val acc: 0.5669  (best train acc: 0.5744, best val acc: 0.5720)\n",
      "[Epoch: 36820] train loss: 1.0674, train acc: 0.5596, val loss: 1.0529, val acc: 0.5626  (best train acc: 0.5744, best val acc: 0.5720)\n",
      "[Epoch: 36840] train loss: 1.0707, train acc: 0.5547, val loss: 1.0517, val acc: 0.5646  (best train acc: 0.5744, best val acc: 0.5720)\n",
      "[Epoch: 36860] train loss: 1.0566, train acc: 0.5615, val loss: 1.0516, val acc: 0.5683  (best train acc: 0.5744, best val acc: 0.5720)\n",
      "[Epoch: 36880] train loss: 1.0785, train acc: 0.5585, val loss: 1.0595, val acc: 0.5622  (best train acc: 0.5744, best val acc: 0.5720)\n",
      "[Epoch: 36900] train loss: 1.0429, train acc: 0.5658, val loss: 1.0598, val acc: 0.5649  (best train acc: 0.5744, best val acc: 0.5720)\n",
      "[Epoch: 36920] train loss: 1.0659, train acc: 0.5591, val loss: 1.0512, val acc: 0.5680  (best train acc: 0.5744, best val acc: 0.5720)\n",
      "[Epoch: 36940] train loss: 1.0823, train acc: 0.5523, val loss: 1.0545, val acc: 0.5646  (best train acc: 0.5744, best val acc: 0.5720)\n",
      "[Epoch: 36960] train loss: 1.0833, train acc: 0.5484, val loss: 1.0668, val acc: 0.5595  (best train acc: 0.5744, best val acc: 0.5720)\n",
      "[Epoch: 36980] train loss: 1.0723, train acc: 0.5491, val loss: 1.0539, val acc: 0.5595  (best train acc: 0.5744, best val acc: 0.5734)\n",
      "[Epoch: 37000] train loss: 1.0877, train acc: 0.5572, val loss: 1.0597, val acc: 0.5589  (best train acc: 0.5744, best val acc: 0.5734)\n",
      "[Epoch: 37020] train loss: 1.0741, train acc: 0.5521, val loss: 1.0804, val acc: 0.5535  (best train acc: 0.5744, best val acc: 0.5734)\n",
      "[Epoch: 37040] train loss: 1.0743, train acc: 0.5537, val loss: 1.0586, val acc: 0.5639  (best train acc: 0.5744, best val acc: 0.5734)\n",
      "[Epoch: 37060] train loss: 1.0970, train acc: 0.5368, val loss: 1.0584, val acc: 0.5518  (best train acc: 0.5744, best val acc: 0.5734)\n",
      "[Epoch: 37080] train loss: 1.0782, train acc: 0.5619, val loss: 1.0627, val acc: 0.5575  (best train acc: 0.5744, best val acc: 0.5734)\n",
      "[Epoch: 37100] train loss: 1.0815, train acc: 0.5515, val loss: 1.1022, val acc: 0.5251  (best train acc: 0.5744, best val acc: 0.5734)\n",
      "[Epoch: 37120] train loss: 1.0844, train acc: 0.5549, val loss: 1.0580, val acc: 0.5622  (best train acc: 0.5744, best val acc: 0.5734)\n",
      "[Epoch: 37140] train loss: 1.0954, train acc: 0.5260, val loss: 1.0522, val acc: 0.5642  (best train acc: 0.5744, best val acc: 0.5734)\n",
      "[Epoch: 37160] train loss: 1.0943, train acc: 0.5509, val loss: 1.0744, val acc: 0.5518  (best train acc: 0.5744, best val acc: 0.5734)\n",
      "[Epoch: 37180] train loss: 1.0582, train acc: 0.5693, val loss: 1.0543, val acc: 0.5642  (best train acc: 0.5744, best val acc: 0.5734)\n",
      "[Epoch: 37200] train loss: 1.0879, train acc: 0.5473, val loss: 1.0512, val acc: 0.5663  (best train acc: 0.5744, best val acc: 0.5734)\n",
      "[Epoch: 37220] train loss: 1.0574, train acc: 0.5627, val loss: 1.0542, val acc: 0.5568  (best train acc: 0.5744, best val acc: 0.5734)\n",
      "[Epoch: 37240] train loss: 1.0726, train acc: 0.5604, val loss: 1.0535, val acc: 0.5659  (best train acc: 0.5744, best val acc: 0.5734)\n",
      "[Epoch: 37260] train loss: 1.0837, train acc: 0.5440, val loss: 1.0544, val acc: 0.5619  (best train acc: 0.5744, best val acc: 0.5734)\n",
      "[Epoch: 37280] train loss: 1.1106, train acc: 0.5240, val loss: 1.0522, val acc: 0.5642  (best train acc: 0.5744, best val acc: 0.5734)\n",
      "[Epoch: 37300] train loss: 1.0626, train acc: 0.5640, val loss: 1.0545, val acc: 0.5609  (best train acc: 0.5744, best val acc: 0.5734)\n",
      "[Epoch: 37320] train loss: 1.0925, train acc: 0.5435, val loss: 1.0533, val acc: 0.5612  (best train acc: 0.5744, best val acc: 0.5734)\n",
      "[Epoch: 37340] train loss: 1.0718, train acc: 0.5538, val loss: 1.0574, val acc: 0.5636  (best train acc: 0.5744, best val acc: 0.5734)\n",
      "[Epoch: 37360] train loss: 1.0874, train acc: 0.5423, val loss: 1.0655, val acc: 0.5572  (best train acc: 0.5744, best val acc: 0.5734)\n",
      "[Epoch: 37380] train loss: 1.0750, train acc: 0.5509, val loss: 1.0508, val acc: 0.5653  (best train acc: 0.5744, best val acc: 0.5734)\n",
      "[Epoch: 37400] train loss: 1.0908, train acc: 0.5518, val loss: 1.1030, val acc: 0.5454  (best train acc: 0.5744, best val acc: 0.5734)\n",
      "[Epoch: 37420] train loss: 1.0680, train acc: 0.5529, val loss: 1.0525, val acc: 0.5649  (best train acc: 0.5744, best val acc: 0.5734)\n",
      "[Epoch: 37440] train loss: 1.0753, train acc: 0.5537, val loss: 1.0522, val acc: 0.5686  (best train acc: 0.5756, best val acc: 0.5734)\n",
      "[Epoch: 37460] train loss: 1.0954, train acc: 0.5421, val loss: 1.0527, val acc: 0.5609  (best train acc: 0.5756, best val acc: 0.5734)\n",
      "[Epoch: 37480] train loss: 1.0748, train acc: 0.5565, val loss: 1.0831, val acc: 0.5457  (best train acc: 0.5756, best val acc: 0.5734)\n",
      "[Epoch: 37500] train loss: 1.0819, train acc: 0.5369, val loss: 1.0516, val acc: 0.5663  (best train acc: 0.5756, best val acc: 0.5734)\n",
      "[Epoch: 37520] train loss: 1.0739, train acc: 0.5564, val loss: 1.0565, val acc: 0.5642  (best train acc: 0.5756, best val acc: 0.5734)\n",
      "[Epoch: 37540] train loss: 1.0901, train acc: 0.5444, val loss: 1.0662, val acc: 0.5599  (best train acc: 0.5756, best val acc: 0.5734)\n",
      "[Epoch: 37560] train loss: 1.0761, train acc: 0.5575, val loss: 1.0756, val acc: 0.5501  (best train acc: 0.5756, best val acc: 0.5734)\n",
      "[Epoch: 37580] train loss: 1.0725, train acc: 0.5524, val loss: 1.0521, val acc: 0.5676  (best train acc: 0.5756, best val acc: 0.5734)\n",
      "[Epoch: 37600] train loss: 1.0934, train acc: 0.5483, val loss: 1.0507, val acc: 0.5683  (best train acc: 0.5756, best val acc: 0.5734)\n",
      "[Epoch: 37620] train loss: 1.1019, train acc: 0.5399, val loss: 1.0567, val acc: 0.5578  (best train acc: 0.5756, best val acc: 0.5734)\n",
      "[Epoch: 37640] train loss: 1.0560, train acc: 0.5670, val loss: 1.0511, val acc: 0.5659  (best train acc: 0.5756, best val acc: 0.5734)\n",
      "[Epoch: 37660] train loss: 1.0579, train acc: 0.5623, val loss: 1.0524, val acc: 0.5663  (best train acc: 0.5756, best val acc: 0.5734)\n",
      "[Epoch: 37680] train loss: 1.0908, train acc: 0.5421, val loss: 1.0512, val acc: 0.5642  (best train acc: 0.5756, best val acc: 0.5734)\n",
      "[Epoch: 37700] train loss: 1.0759, train acc: 0.5486, val loss: 1.0541, val acc: 0.5585  (best train acc: 0.5756, best val acc: 0.5734)\n",
      "[Epoch: 37720] train loss: 1.1128, train acc: 0.5429, val loss: 1.0676, val acc: 0.5562  (best train acc: 0.5756, best val acc: 0.5734)\n",
      "[Epoch: 37740] train loss: 1.2133, train acc: 0.5025, val loss: 1.1301, val acc: 0.5278  (best train acc: 0.5756, best val acc: 0.5734)\n",
      "[Epoch: 37760] train loss: 1.0560, train acc: 0.5640, val loss: 1.0685, val acc: 0.5622  (best train acc: 0.5756, best val acc: 0.5734)\n",
      "[Epoch: 37780] train loss: 1.0676, train acc: 0.5583, val loss: 1.0583, val acc: 0.5616  (best train acc: 0.5756, best val acc: 0.5734)\n",
      "[Epoch: 37800] train loss: 1.0643, train acc: 0.5648, val loss: 1.0546, val acc: 0.5632  (best train acc: 0.5756, best val acc: 0.5734)\n",
      "[Epoch: 37820] train loss: 1.0559, train acc: 0.5588, val loss: 1.0522, val acc: 0.5626  (best train acc: 0.5756, best val acc: 0.5734)\n",
      "[Epoch: 37840] train loss: 1.0627, train acc: 0.5610, val loss: 1.0556, val acc: 0.5659  (best train acc: 0.5756, best val acc: 0.5734)\n",
      "[Epoch: 37860] train loss: 1.0644, train acc: 0.5518, val loss: 1.0514, val acc: 0.5632  (best train acc: 0.5756, best val acc: 0.5734)\n",
      "[Epoch: 37880] train loss: 1.0753, train acc: 0.5551, val loss: 1.0966, val acc: 0.5298  (best train acc: 0.5756, best val acc: 0.5734)\n",
      "[Epoch: 37900] train loss: 1.0842, train acc: 0.5466, val loss: 1.1630, val acc: 0.4951  (best train acc: 0.5756, best val acc: 0.5734)\n",
      "[Epoch: 37920] train loss: 1.0934, train acc: 0.5506, val loss: 1.0520, val acc: 0.5642  (best train acc: 0.5756, best val acc: 0.5734)\n",
      "[Epoch: 37940] train loss: 1.0797, train acc: 0.5460, val loss: 1.0576, val acc: 0.5592  (best train acc: 0.5756, best val acc: 0.5734)\n",
      "[Epoch: 37960] train loss: 1.0612, train acc: 0.5578, val loss: 1.0544, val acc: 0.5629  (best train acc: 0.5756, best val acc: 0.5734)\n",
      "[Epoch: 37980] train loss: 1.0596, train acc: 0.5571, val loss: 1.0516, val acc: 0.5629  (best train acc: 0.5756, best val acc: 0.5734)\n",
      "[Epoch: 38000] train loss: 1.0633, train acc: 0.5597, val loss: 1.0522, val acc: 0.5636  (best train acc: 0.5756, best val acc: 0.5734)\n",
      "[Epoch: 38020] train loss: 1.0661, train acc: 0.5509, val loss: 1.0511, val acc: 0.5669  (best train acc: 0.5756, best val acc: 0.5734)\n",
      "[Epoch: 38040] train loss: 1.0490, train acc: 0.5683, val loss: 1.0521, val acc: 0.5646  (best train acc: 0.5756, best val acc: 0.5734)\n",
      "[Epoch: 38060] train loss: 1.0714, train acc: 0.5583, val loss: 1.0750, val acc: 0.5521  (best train acc: 0.5756, best val acc: 0.5734)\n",
      "[Epoch: 38080] train loss: 1.0940, train acc: 0.5384, val loss: 1.0519, val acc: 0.5659  (best train acc: 0.5756, best val acc: 0.5734)\n",
      "[Epoch: 38100] train loss: 1.0937, train acc: 0.5407, val loss: 1.0633, val acc: 0.5595  (best train acc: 0.5756, best val acc: 0.5734)\n",
      "[Epoch: 38120] train loss: 1.0794, train acc: 0.5515, val loss: 1.0519, val acc: 0.5629  (best train acc: 0.5756, best val acc: 0.5734)\n",
      "[Epoch: 38140] train loss: 1.0904, train acc: 0.5401, val loss: 1.0530, val acc: 0.5636  (best train acc: 0.5756, best val acc: 0.5734)\n",
      "[Epoch: 38160] train loss: 1.0622, train acc: 0.5581, val loss: 1.0548, val acc: 0.5642  (best train acc: 0.5756, best val acc: 0.5734)\n",
      "[Epoch: 38180] train loss: 1.0713, train acc: 0.5630, val loss: 1.0548, val acc: 0.5619  (best train acc: 0.5756, best val acc: 0.5734)\n",
      "[Epoch: 38200] train loss: 1.0975, train acc: 0.5309, val loss: 1.0534, val acc: 0.5632  (best train acc: 0.5756, best val acc: 0.5734)\n",
      "[Epoch: 38220] train loss: 1.0880, train acc: 0.5499, val loss: 1.0525, val acc: 0.5663  (best train acc: 0.5756, best val acc: 0.5734)\n",
      "[Epoch: 38240] train loss: 1.0540, train acc: 0.5529, val loss: 1.0610, val acc: 0.5656  (best train acc: 0.5756, best val acc: 0.5734)\n",
      "[Epoch: 38260] train loss: 1.0789, train acc: 0.5387, val loss: 1.0736, val acc: 0.5524  (best train acc: 0.5756, best val acc: 0.5734)\n",
      "[Epoch: 38280] train loss: 1.1292, train acc: 0.5337, val loss: 1.0971, val acc: 0.5298  (best train acc: 0.5756, best val acc: 0.5734)\n",
      "[Epoch: 38300] train loss: 1.0817, train acc: 0.5529, val loss: 1.0572, val acc: 0.5636  (best train acc: 0.5756, best val acc: 0.5734)\n",
      "[Epoch: 38320] train loss: 1.0741, train acc: 0.5536, val loss: 1.0613, val acc: 0.5639  (best train acc: 0.5756, best val acc: 0.5734)\n",
      "[Epoch: 38340] train loss: 1.0871, train acc: 0.5483, val loss: 1.0654, val acc: 0.5605  (best train acc: 0.5756, best val acc: 0.5734)\n",
      "[Epoch: 38360] train loss: 1.0655, train acc: 0.5596, val loss: 1.0521, val acc: 0.5649  (best train acc: 0.5756, best val acc: 0.5734)\n",
      "[Epoch: 38380] train loss: 1.0800, train acc: 0.5569, val loss: 1.0648, val acc: 0.5626  (best train acc: 0.5756, best val acc: 0.5734)\n",
      "[Epoch: 38400] train loss: 1.0935, train acc: 0.5304, val loss: 1.0567, val acc: 0.5636  (best train acc: 0.5756, best val acc: 0.5734)\n",
      "[Epoch: 38420] train loss: 1.0740, train acc: 0.5427, val loss: 1.0519, val acc: 0.5642  (best train acc: 0.5756, best val acc: 0.5734)\n",
      "[Epoch: 38440] train loss: 1.0862, train acc: 0.5551, val loss: 1.0628, val acc: 0.5589  (best train acc: 0.5756, best val acc: 0.5734)\n",
      "[Epoch: 38460] train loss: 1.0819, train acc: 0.5523, val loss: 1.0870, val acc: 0.5508  (best train acc: 0.5756, best val acc: 0.5734)\n",
      "[Epoch: 38480] train loss: 1.0589, train acc: 0.5714, val loss: 1.0522, val acc: 0.5669  (best train acc: 0.5756, best val acc: 0.5734)\n",
      "[Epoch: 38500] train loss: 1.0671, train acc: 0.5492, val loss: 1.0584, val acc: 0.5612  (best train acc: 0.5756, best val acc: 0.5734)\n",
      "[Epoch: 38520] train loss: 1.0625, train acc: 0.5629, val loss: 1.0534, val acc: 0.5555  (best train acc: 0.5756, best val acc: 0.5734)\n",
      "[Epoch: 38540] train loss: 1.0783, train acc: 0.5437, val loss: 1.0586, val acc: 0.5595  (best train acc: 0.5756, best val acc: 0.5734)\n",
      "[Epoch: 38560] train loss: 1.0750, train acc: 0.5578, val loss: 1.0510, val acc: 0.5639  (best train acc: 0.5756, best val acc: 0.5734)\n",
      "[Epoch: 38580] train loss: 1.0841, train acc: 0.5429, val loss: 1.0494, val acc: 0.5686  (best train acc: 0.5756, best val acc: 0.5734)\n",
      "[Epoch: 38600] train loss: 1.0873, train acc: 0.5449, val loss: 1.0496, val acc: 0.5639  (best train acc: 0.5756, best val acc: 0.5734)\n",
      "[Epoch: 38620] train loss: 1.0975, train acc: 0.5444, val loss: 1.0497, val acc: 0.5642  (best train acc: 0.5756, best val acc: 0.5734)\n",
      "[Epoch: 38640] train loss: 1.0847, train acc: 0.5380, val loss: 1.0545, val acc: 0.5642  (best train acc: 0.5756, best val acc: 0.5734)\n",
      "[Epoch: 38660] train loss: 1.0737, train acc: 0.5569, val loss: 1.0628, val acc: 0.5595  (best train acc: 0.5756, best val acc: 0.5734)\n",
      "[Epoch: 38680] train loss: 1.1893, train acc: 0.4821, val loss: 1.1069, val acc: 0.5447  (best train acc: 0.5756, best val acc: 0.5734)\n",
      "[Epoch: 38700] train loss: 1.0581, train acc: 0.5678, val loss: 1.0741, val acc: 0.5545  (best train acc: 0.5756, best val acc: 0.5734)\n",
      "[Epoch: 38720] train loss: 1.0956, train acc: 0.5297, val loss: 1.0548, val acc: 0.5626  (best train acc: 0.5756, best val acc: 0.5734)\n",
      "[Epoch: 38740] train loss: 1.0704, train acc: 0.5492, val loss: 1.0639, val acc: 0.5626  (best train acc: 0.5756, best val acc: 0.5734)\n",
      "[Epoch: 38760] train loss: 1.0858, train acc: 0.5491, val loss: 1.0621, val acc: 0.5582  (best train acc: 0.5756, best val acc: 0.5734)\n",
      "[Epoch: 38780] train loss: 1.0463, train acc: 0.5668, val loss: 1.0496, val acc: 0.5680  (best train acc: 0.5756, best val acc: 0.5734)\n",
      "[Epoch: 38800] train loss: 1.0590, train acc: 0.5607, val loss: 1.0670, val acc: 0.5582  (best train acc: 0.5756, best val acc: 0.5734)\n",
      "[Epoch: 38820] train loss: 1.0813, train acc: 0.5571, val loss: 1.0579, val acc: 0.5626  (best train acc: 0.5756, best val acc: 0.5734)\n",
      "[Epoch: 38840] train loss: 1.0692, train acc: 0.5583, val loss: 1.0517, val acc: 0.5666  (best train acc: 0.5756, best val acc: 0.5734)\n",
      "[Epoch: 38860] train loss: 1.0831, train acc: 0.5401, val loss: 1.0562, val acc: 0.5632  (best train acc: 0.5756, best val acc: 0.5734)\n",
      "[Epoch: 38880] train loss: 1.0692, train acc: 0.5557, val loss: 1.0677, val acc: 0.5575  (best train acc: 0.5756, best val acc: 0.5734)\n",
      "[Epoch: 38900] train loss: 1.0933, train acc: 0.5308, val loss: 1.0855, val acc: 0.5504  (best train acc: 0.5756, best val acc: 0.5734)\n",
      "[Epoch: 38920] train loss: 1.0881, train acc: 0.5535, val loss: 1.0700, val acc: 0.5592  (best train acc: 0.5756, best val acc: 0.5734)\n",
      "[Epoch: 38940] train loss: 1.0543, train acc: 0.5674, val loss: 1.0523, val acc: 0.5642  (best train acc: 0.5756, best val acc: 0.5734)\n",
      "[Epoch: 38960] train loss: 1.0657, train acc: 0.5572, val loss: 1.0505, val acc: 0.5663  (best train acc: 0.5756, best val acc: 0.5734)\n",
      "[Epoch: 38980] train loss: 1.0873, train acc: 0.5393, val loss: 1.0813, val acc: 0.5454  (best train acc: 0.5756, best val acc: 0.5734)\n",
      "[Epoch: 39000] train loss: 1.0664, train acc: 0.5670, val loss: 1.0663, val acc: 0.5589  (best train acc: 0.5756, best val acc: 0.5734)\n",
      "[Epoch: 39020] train loss: 1.0772, train acc: 0.5557, val loss: 1.1672, val acc: 0.5261  (best train acc: 0.5756, best val acc: 0.5734)\n",
      "[Epoch: 39040] train loss: 1.0970, train acc: 0.5291, val loss: 1.0528, val acc: 0.5683  (best train acc: 0.5756, best val acc: 0.5734)\n",
      "[Epoch: 39060] train loss: 1.0721, train acc: 0.5498, val loss: 1.0677, val acc: 0.5538  (best train acc: 0.5756, best val acc: 0.5734)\n",
      "[Epoch: 39080] train loss: 1.0560, train acc: 0.5573, val loss: 1.0511, val acc: 0.5669  (best train acc: 0.5756, best val acc: 0.5734)\n",
      "[Epoch: 39100] train loss: 1.0773, train acc: 0.5521, val loss: 1.0520, val acc: 0.5649  (best train acc: 0.5756, best val acc: 0.5734)\n",
      "[Epoch: 39120] train loss: 1.0743, train acc: 0.5578, val loss: 1.0633, val acc: 0.5642  (best train acc: 0.5756, best val acc: 0.5734)\n",
      "[Epoch: 39140] train loss: 1.0512, train acc: 0.5681, val loss: 1.0512, val acc: 0.5659  (best train acc: 0.5756, best val acc: 0.5734)\n",
      "[Epoch: 39160] train loss: 1.0856, train acc: 0.5476, val loss: 1.0594, val acc: 0.5642  (best train acc: 0.5756, best val acc: 0.5734)\n",
      "[Epoch: 39180] train loss: 1.0856, train acc: 0.5457, val loss: 1.0559, val acc: 0.5592  (best train acc: 0.5756, best val acc: 0.5734)\n",
      "[Epoch: 39200] train loss: 1.0625, train acc: 0.5607, val loss: 1.0534, val acc: 0.5626  (best train acc: 0.5756, best val acc: 0.5734)\n",
      "[Epoch: 39220] train loss: 1.0625, train acc: 0.5466, val loss: 1.0502, val acc: 0.5592  (best train acc: 0.5756, best val acc: 0.5734)\n",
      "[Epoch: 39240] train loss: 1.0705, train acc: 0.5602, val loss: 1.0557, val acc: 0.5602  (best train acc: 0.5756, best val acc: 0.5734)\n",
      "[Epoch: 39260] train loss: 1.0634, train acc: 0.5622, val loss: 1.0518, val acc: 0.5622  (best train acc: 0.5756, best val acc: 0.5734)\n",
      "[Epoch: 39280] train loss: 1.0947, train acc: 0.5301, val loss: 1.0503, val acc: 0.5626  (best train acc: 0.5756, best val acc: 0.5734)\n",
      "[Epoch: 39300] train loss: 1.0862, train acc: 0.5545, val loss: 1.0527, val acc: 0.5629  (best train acc: 0.5756, best val acc: 0.5734)\n",
      "[Epoch: 39320] train loss: 1.0856, train acc: 0.5458, val loss: 1.0542, val acc: 0.5656  (best train acc: 0.5756, best val acc: 0.5734)\n",
      "[Epoch: 39340] train loss: 1.0739, train acc: 0.5487, val loss: 1.0518, val acc: 0.5622  (best train acc: 0.5756, best val acc: 0.5734)\n",
      "[Epoch: 39360] train loss: 1.0731, train acc: 0.5581, val loss: 1.0569, val acc: 0.5642  (best train acc: 0.5756, best val acc: 0.5734)\n",
      "[Epoch: 39380] train loss: 1.0658, train acc: 0.5541, val loss: 1.0566, val acc: 0.5572  (best train acc: 0.5756, best val acc: 0.5734)\n",
      "[Epoch: 39400] train loss: 1.0699, train acc: 0.5539, val loss: 1.0490, val acc: 0.5683  (best train acc: 0.5756, best val acc: 0.5734)\n",
      "[Epoch: 39420] train loss: 1.0866, train acc: 0.5475, val loss: 1.0491, val acc: 0.5632  (best train acc: 0.5756, best val acc: 0.5734)\n",
      "[Epoch: 39440] train loss: 1.0606, train acc: 0.5589, val loss: 1.0514, val acc: 0.5612  (best train acc: 0.5756, best val acc: 0.5734)\n",
      "[Epoch: 39460] train loss: 1.0763, train acc: 0.5557, val loss: 1.0543, val acc: 0.5626  (best train acc: 0.5756, best val acc: 0.5734)\n",
      "[Epoch: 39480] train loss: 1.0840, train acc: 0.5444, val loss: 1.0504, val acc: 0.5629  (best train acc: 0.5756, best val acc: 0.5734)\n",
      "[Epoch: 39500] train loss: 1.0730, train acc: 0.5429, val loss: 1.0576, val acc: 0.5649  (best train acc: 0.5756, best val acc: 0.5734)\n",
      "[Epoch: 39520] train loss: 1.0605, train acc: 0.5571, val loss: 1.0486, val acc: 0.5676  (best train acc: 0.5756, best val acc: 0.5734)\n",
      "[Epoch: 39540] train loss: 1.0904, train acc: 0.5350, val loss: 1.0498, val acc: 0.5632  (best train acc: 0.5756, best val acc: 0.5734)\n",
      "[Epoch: 39560] train loss: 1.0700, train acc: 0.5541, val loss: 1.0497, val acc: 0.5653  (best train acc: 0.5756, best val acc: 0.5734)\n",
      "[Epoch: 39580] train loss: 1.0578, train acc: 0.5560, val loss: 1.0488, val acc: 0.5653  (best train acc: 0.5756, best val acc: 0.5734)\n",
      "[Epoch: 39600] train loss: 1.0572, train acc: 0.5625, val loss: 1.0506, val acc: 0.5626  (best train acc: 0.5756, best val acc: 0.5734)\n",
      "[Epoch: 39620] train loss: 1.0762, train acc: 0.5443, val loss: 1.0487, val acc: 0.5619  (best train acc: 0.5756, best val acc: 0.5734)\n",
      "[Epoch: 39640] train loss: 1.0602, train acc: 0.5646, val loss: 1.0524, val acc: 0.5646  (best train acc: 0.5756, best val acc: 0.5734)\n",
      "[Epoch: 39660] train loss: 1.0835, train acc: 0.5518, val loss: 1.0652, val acc: 0.5565  (best train acc: 0.5756, best val acc: 0.5734)\n",
      "[Epoch: 39680] train loss: 1.0750, train acc: 0.5596, val loss: 1.0892, val acc: 0.5501  (best train acc: 0.5756, best val acc: 0.5734)\n",
      "[Epoch: 39700] train loss: 1.0588, train acc: 0.5578, val loss: 1.0487, val acc: 0.5673  (best train acc: 0.5756, best val acc: 0.5734)\n",
      "[Epoch: 39720] train loss: 1.0824, train acc: 0.5556, val loss: 1.0559, val acc: 0.5663  (best train acc: 0.5756, best val acc: 0.5734)\n",
      "[Epoch: 39740] train loss: 1.1210, train acc: 0.5449, val loss: 1.0689, val acc: 0.5470  (best train acc: 0.5756, best val acc: 0.5734)\n",
      "[Epoch: 39760] train loss: 1.0710, train acc: 0.5501, val loss: 1.0518, val acc: 0.5669  (best train acc: 0.5756, best val acc: 0.5734)\n",
      "[Epoch: 39780] train loss: 1.0844, train acc: 0.5562, val loss: 1.0602, val acc: 0.5605  (best train acc: 0.5756, best val acc: 0.5734)\n",
      "[Epoch: 39800] train loss: 1.0755, train acc: 0.5481, val loss: 1.0485, val acc: 0.5626  (best train acc: 0.5756, best val acc: 0.5734)\n",
      "[Epoch: 39820] train loss: 1.0779, train acc: 0.5513, val loss: 1.0607, val acc: 0.5582  (best train acc: 0.5756, best val acc: 0.5734)\n",
      "[Epoch: 39840] train loss: 1.0986, train acc: 0.5447, val loss: 1.0517, val acc: 0.5673  (best train acc: 0.5756, best val acc: 0.5734)\n",
      "[Epoch: 39860] train loss: 1.0974, train acc: 0.5272, val loss: 1.0525, val acc: 0.5669  (best train acc: 0.5756, best val acc: 0.5734)\n",
      "[Epoch: 39880] train loss: 1.0605, train acc: 0.5588, val loss: 1.0511, val acc: 0.5602  (best train acc: 0.5756, best val acc: 0.5734)\n",
      "[Epoch: 39900] train loss: 1.0807, train acc: 0.5495, val loss: 1.0659, val acc: 0.5565  (best train acc: 0.5756, best val acc: 0.5734)\n",
      "[Epoch: 39920] train loss: 1.0755, train acc: 0.5563, val loss: 1.0525, val acc: 0.5646  (best train acc: 0.5756, best val acc: 0.5734)\n",
      "[Epoch: 39940] train loss: 1.1011, train acc: 0.5226, val loss: 1.0630, val acc: 0.5562  (best train acc: 0.5756, best val acc: 0.5734)\n",
      "[Epoch: 39960] train loss: 1.0709, train acc: 0.5591, val loss: 1.0561, val acc: 0.5639  (best train acc: 0.5756, best val acc: 0.5734)\n",
      "[Epoch: 39980] train loss: 1.0706, train acc: 0.5548, val loss: 1.0503, val acc: 0.5690  (best train acc: 0.5756, best val acc: 0.5734)\n",
      "[Epoch: 40000] train loss: 1.0625, train acc: 0.5586, val loss: 1.0574, val acc: 0.5646  (best train acc: 0.5756, best val acc: 0.5734)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pmaur\\miniconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\pmaur\\miniconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\pmaur\\miniconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\pmaur\\miniconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\pmaur\\miniconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\pmaur\\miniconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       154\n",
      "           1       0.67      0.65      0.66       703\n",
      "           2       0.40      0.36      0.38       702\n",
      "           3       0.49      0.55      0.52       703\n",
      "           4       0.70      0.86      0.77       702\n",
      "\n",
      "    accuracy                           0.57      2964\n",
      "   macro avg       0.45      0.48      0.46      2964\n",
      "weighted avg       0.53      0.57      0.55      2964\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 504x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 504x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "gnn = GNN4L_GCN(data_with_nedbit).to(device)\n",
    "pred = train(gnn, data_with_nedbit.to(device), 40000, cm_title='GAT7L_multiclass_16HC', classes=['P', 'LP', 'WN', 'LN', 'RN'], weight_decay=0.0005, arch='GCN', layers=4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c58c6b3a0d5749dc8a7f76938f829fbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40001 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch: 0000] train loss: 237.0473, train acc: 0.2353, val loss: 207.7576, val acc: 0.2371  (best train acc: 0.2353, best val acc: 0.2371)\n",
      "[Epoch: 0020] train loss: 27.0236, train acc: 0.2393, val loss: 21.1935, val acc: 0.2347  (best train acc: 0.2641, best val acc: 0.2371)\n",
      "[Epoch: 0040] train loss: 8.2772, train acc: 0.2454, val loss: 4.8833, val acc: 0.2368  (best train acc: 0.2965, best val acc: 0.3821)\n",
      "[Epoch: 0060] train loss: 4.4072, train acc: 0.2439, val loss: 1.8764, val acc: 0.2371  (best train acc: 0.2965, best val acc: 0.3821)\n",
      "[Epoch: 0080] train loss: 3.4530, train acc: 0.2520, val loss: 1.5906, val acc: 0.2368  (best train acc: 0.2965, best val acc: 0.3821)\n",
      "[Epoch: 0100] train loss: 2.8259, train acc: 0.2629, val loss: 1.4837, val acc: 0.2401  (best train acc: 0.2965, best val acc: 0.3821)\n",
      "[Epoch: 0120] train loss: 2.5470, train acc: 0.2571, val loss: 1.4918, val acc: 0.2371  (best train acc: 0.2965, best val acc: 0.3821)\n",
      "[Epoch: 0140] train loss: 2.2677, train acc: 0.2712, val loss: 1.4869, val acc: 0.2411  (best train acc: 0.2965, best val acc: 0.3821)\n",
      "[Epoch: 0160] train loss: 2.0076, train acc: 0.2786, val loss: 1.4621, val acc: 0.2438  (best train acc: 0.2983, best val acc: 0.3821)\n",
      "[Epoch: 0180] train loss: 1.9157, train acc: 0.2650, val loss: 1.4644, val acc: 0.2408  (best train acc: 0.2992, best val acc: 0.3821)\n",
      "[Epoch: 0200] train loss: 1.7279, train acc: 0.3056, val loss: 1.4490, val acc: 0.2543  (best train acc: 0.3056, best val acc: 0.3821)\n",
      "[Epoch: 0220] train loss: 1.6593, train acc: 0.2938, val loss: 1.4559, val acc: 0.2428  (best train acc: 0.3107, best val acc: 0.3821)\n",
      "[Epoch: 0240] train loss: 1.5707, train acc: 0.3005, val loss: 1.4462, val acc: 0.3504  (best train acc: 0.3129, best val acc: 0.3855)\n",
      "[Epoch: 0260] train loss: 1.5262, train acc: 0.3156, val loss: 1.4395, val acc: 0.3454  (best train acc: 0.3214, best val acc: 0.3855)\n",
      "[Epoch: 0280] train loss: 1.5128, train acc: 0.3109, val loss: 1.4435, val acc: 0.3619  (best train acc: 0.3284, best val acc: 0.3872)\n",
      "[Epoch: 0300] train loss: 1.4882, train acc: 0.3230, val loss: 1.4337, val acc: 0.3835  (best train acc: 0.3378, best val acc: 0.4010)\n",
      "[Epoch: 0320] train loss: 1.4627, train acc: 0.3247, val loss: 1.4299, val acc: 0.3882  (best train acc: 0.3378, best val acc: 0.4010)\n",
      "[Epoch: 0340] train loss: 1.4611, train acc: 0.3302, val loss: 1.4260, val acc: 0.3922  (best train acc: 0.3471, best val acc: 0.4010)\n",
      "[Epoch: 0360] train loss: 1.4464, train acc: 0.3429, val loss: 1.4220, val acc: 0.3919  (best train acc: 0.3530, best val acc: 0.4024)\n",
      "[Epoch: 0380] train loss: 1.4331, train acc: 0.3583, val loss: 1.4180, val acc: 0.4007  (best train acc: 0.3625, best val acc: 0.4024)\n",
      "[Epoch: 0400] train loss: 1.4311, train acc: 0.3436, val loss: 1.4141, val acc: 0.3895  (best train acc: 0.3658, best val acc: 0.4074)\n",
      "[Epoch: 0420] train loss: 1.4288, train acc: 0.3639, val loss: 1.4114, val acc: 0.3781  (best train acc: 0.3771, best val acc: 0.4145)\n",
      "[Epoch: 0440] train loss: 1.4117, train acc: 0.3676, val loss: 1.4083, val acc: 0.3808  (best train acc: 0.3881, best val acc: 0.4341)\n",
      "[Epoch: 0460] train loss: 1.4095, train acc: 0.3759, val loss: 1.4004, val acc: 0.3983  (best train acc: 0.3973, best val acc: 0.4341)\n",
      "[Epoch: 0480] train loss: 1.4050, train acc: 0.3833, val loss: 1.3957, val acc: 0.4024  (best train acc: 0.4070, best val acc: 0.4506)\n",
      "[Epoch: 0500] train loss: 1.4098, train acc: 0.3702, val loss: 1.3913, val acc: 0.4138  (best train acc: 0.4070, best val acc: 0.4506)\n",
      "[Epoch: 0520] train loss: 1.3925, train acc: 0.4015, val loss: 1.3869, val acc: 0.4219  (best train acc: 0.4070, best val acc: 0.4506)\n",
      "[Epoch: 0540] train loss: 1.3848, train acc: 0.4015, val loss: 1.3831, val acc: 0.4182  (best train acc: 0.4070, best val acc: 0.4506)\n",
      "[Epoch: 0560] train loss: 1.3908, train acc: 0.3858, val loss: 1.3799, val acc: 0.4175  (best train acc: 0.4114, best val acc: 0.4506)\n",
      "[Epoch: 0580] train loss: 1.3863, train acc: 0.3774, val loss: 1.3758, val acc: 0.4202  (best train acc: 0.4229, best val acc: 0.4506)\n",
      "[Epoch: 0600] train loss: 1.3781, train acc: 0.4161, val loss: 1.3710, val acc: 0.4290  (best train acc: 0.4238, best val acc: 0.4641)\n",
      "[Epoch: 0620] train loss: 1.3819, train acc: 0.3937, val loss: 1.3676, val acc: 0.4216  (best train acc: 0.4284, best val acc: 0.4641)\n",
      "[Epoch: 0640] train loss: 1.3757, train acc: 0.4117, val loss: 1.3646, val acc: 0.4445  (best train acc: 0.4322, best val acc: 0.4658)\n",
      "[Epoch: 0660] train loss: 1.3661, train acc: 0.4247, val loss: 1.3597, val acc: 0.4293  (best train acc: 0.4328, best val acc: 0.4658)\n",
      "[Epoch: 0680] train loss: 1.3673, train acc: 0.4100, val loss: 1.3564, val acc: 0.4543  (best train acc: 0.4328, best val acc: 0.4658)\n",
      "[Epoch: 0700] train loss: 1.3660, train acc: 0.4345, val loss: 1.3522, val acc: 0.4388  (best train acc: 0.4425, best val acc: 0.4725)\n",
      "[Epoch: 0720] train loss: 1.3533, train acc: 0.4291, val loss: 1.3486, val acc: 0.4418  (best train acc: 0.4490, best val acc: 0.4725)\n",
      "[Epoch: 0740] train loss: 1.3568, train acc: 0.4174, val loss: 1.3456, val acc: 0.4398  (best train acc: 0.4492, best val acc: 0.4725)\n",
      "[Epoch: 0760] train loss: 1.3576, train acc: 0.4388, val loss: 1.3427, val acc: 0.4641  (best train acc: 0.4492, best val acc: 0.4725)\n",
      "[Epoch: 0780] train loss: 1.3483, train acc: 0.4215, val loss: 1.3391, val acc: 0.4398  (best train acc: 0.4521, best val acc: 0.4725)\n",
      "[Epoch: 0800] train loss: 1.3469, train acc: 0.4443, val loss: 1.3355, val acc: 0.4513  (best train acc: 0.4537, best val acc: 0.4725)\n",
      "[Epoch: 0820] train loss: 1.3385, train acc: 0.4372, val loss: 1.3334, val acc: 0.4685  (best train acc: 0.4580, best val acc: 0.4725)\n",
      "[Epoch: 0840] train loss: 1.3328, train acc: 0.4571, val loss: 1.3294, val acc: 0.4624  (best train acc: 0.4580, best val acc: 0.4725)\n",
      "[Epoch: 0860] train loss: 1.3381, train acc: 0.4554, val loss: 1.3291, val acc: 0.4749  (best train acc: 0.4599, best val acc: 0.4749)\n",
      "[Epoch: 0880] train loss: 1.3301, train acc: 0.4318, val loss: 1.3231, val acc: 0.4658  (best train acc: 0.4743, best val acc: 0.4904)\n",
      "[Epoch: 0900] train loss: 1.3264, train acc: 0.4318, val loss: 1.3202, val acc: 0.4533  (best train acc: 0.4743, best val acc: 0.4904)\n",
      "[Epoch: 0920] train loss: 1.3174, train acc: 0.4579, val loss: 1.3181, val acc: 0.4482  (best train acc: 0.4743, best val acc: 0.4904)\n",
      "[Epoch: 0940] train loss: 1.3190, train acc: 0.4603, val loss: 1.3149, val acc: 0.4715  (best train acc: 0.4743, best val acc: 0.4904)\n",
      "[Epoch: 0960] train loss: 1.3159, train acc: 0.4535, val loss: 1.3155, val acc: 0.4904  (best train acc: 0.4743, best val acc: 0.4904)\n",
      "[Epoch: 0980] train loss: 1.3126, train acc: 0.4554, val loss: 1.3089, val acc: 0.4698  (best train acc: 0.4743, best val acc: 0.4904)\n",
      "[Epoch: 1000] train loss: 1.3195, train acc: 0.4580, val loss: 1.3063, val acc: 0.4695  (best train acc: 0.4743, best val acc: 0.4904)\n",
      "[Epoch: 1020] train loss: 1.2995, train acc: 0.4680, val loss: 1.3042, val acc: 0.4705  (best train acc: 0.4777, best val acc: 0.4904)\n",
      "[Epoch: 1040] train loss: 1.3150, train acc: 0.4459, val loss: 1.3038, val acc: 0.4533  (best train acc: 0.4777, best val acc: 0.4904)\n",
      "[Epoch: 1060] train loss: 1.3073, train acc: 0.4735, val loss: 1.2990, val acc: 0.4718  (best train acc: 0.4777, best val acc: 0.4921)\n",
      "[Epoch: 1080] train loss: 1.3061, train acc: 0.4412, val loss: 1.2982, val acc: 0.4472  (best train acc: 0.4777, best val acc: 0.4921)\n",
      "[Epoch: 1100] train loss: 1.2986, train acc: 0.4680, val loss: 1.2934, val acc: 0.4708  (best train acc: 0.4834, best val acc: 0.4921)\n",
      "[Epoch: 1120] train loss: 1.2917, train acc: 0.4626, val loss: 1.2916, val acc: 0.4809  (best train acc: 0.4834, best val acc: 0.4921)\n",
      "[Epoch: 1140] train loss: 1.2875, train acc: 0.4709, val loss: 1.2876, val acc: 0.4698  (best train acc: 0.4834, best val acc: 0.4921)\n",
      "[Epoch: 1160] train loss: 1.2872, train acc: 0.4646, val loss: 1.2843, val acc: 0.4610  (best train acc: 0.4910, best val acc: 0.4958)\n",
      "[Epoch: 1180] train loss: 1.2853, train acc: 0.4651, val loss: 1.2807, val acc: 0.4840  (best train acc: 0.4910, best val acc: 0.4958)\n",
      "[Epoch: 1200] train loss: 1.2741, train acc: 0.4801, val loss: 1.2792, val acc: 0.4580  (best train acc: 0.4924, best val acc: 0.4961)\n",
      "[Epoch: 1220] train loss: 1.2761, train acc: 0.4750, val loss: 1.2749, val acc: 0.4779  (best train acc: 0.4970, best val acc: 0.4965)\n",
      "[Epoch: 1240] train loss: 1.2765, train acc: 0.4661, val loss: 1.2746, val acc: 0.4853  (best train acc: 0.5016, best val acc: 0.4985)\n",
      "[Epoch: 1260] train loss: 1.2716, train acc: 0.4799, val loss: 1.2721, val acc: 0.4823  (best train acc: 0.5033, best val acc: 0.4985)\n",
      "[Epoch: 1280] train loss: 1.2690, train acc: 0.4782, val loss: 1.2677, val acc: 0.4904  (best train acc: 0.5033, best val acc: 0.4985)\n",
      "[Epoch: 1300] train loss: 1.2636, train acc: 0.5025, val loss: 1.2658, val acc: 0.4968  (best train acc: 0.5033, best val acc: 0.5019)\n",
      "[Epoch: 1320] train loss: 1.2638, train acc: 0.4975, val loss: 1.2638, val acc: 0.4911  (best train acc: 0.5033, best val acc: 0.5052)\n",
      "[Epoch: 1340] train loss: 1.2615, train acc: 0.5098, val loss: 1.2612, val acc: 0.4884  (best train acc: 0.5098, best val acc: 0.5059)\n",
      "[Epoch: 1360] train loss: 1.2574, train acc: 0.4939, val loss: 1.2607, val acc: 0.4728  (best train acc: 0.5098, best val acc: 0.5059)\n",
      "[Epoch: 1380] train loss: 1.2563, train acc: 0.4902, val loss: 1.2571, val acc: 0.4901  (best train acc: 0.5098, best val acc: 0.5059)\n",
      "[Epoch: 1400] train loss: 1.2531, train acc: 0.5167, val loss: 1.2594, val acc: 0.4597  (best train acc: 0.5167, best val acc: 0.5079)\n",
      "[Epoch: 1420] train loss: 1.2519, train acc: 0.5040, val loss: 1.2571, val acc: 0.4793  (best train acc: 0.5167, best val acc: 0.5079)\n",
      "[Epoch: 1440] train loss: 1.2549, train acc: 0.4978, val loss: 1.2553, val acc: 0.4809  (best train acc: 0.5167, best val acc: 0.5113)\n",
      "[Epoch: 1460] train loss: 1.2525, train acc: 0.4910, val loss: 1.2553, val acc: 0.4796  (best train acc: 0.5167, best val acc: 0.5113)\n",
      "[Epoch: 1480] train loss: 1.2515, train acc: 0.4759, val loss: 1.2500, val acc: 0.4870  (best train acc: 0.5167, best val acc: 0.5113)\n",
      "[Epoch: 1500] train loss: 1.2438, train acc: 0.4923, val loss: 1.2480, val acc: 0.4897  (best train acc: 0.5167, best val acc: 0.5113)\n",
      "[Epoch: 1520] train loss: 1.2427, train acc: 0.4973, val loss: 1.2471, val acc: 0.4978  (best train acc: 0.5167, best val acc: 0.5113)\n",
      "[Epoch: 1540] train loss: 1.2391, train acc: 0.4891, val loss: 1.2445, val acc: 0.4941  (best train acc: 0.5167, best val acc: 0.5113)\n",
      "[Epoch: 1560] train loss: 1.2469, train acc: 0.4947, val loss: 1.2459, val acc: 0.5103  (best train acc: 0.5167, best val acc: 0.5113)\n",
      "[Epoch: 1580] train loss: 1.2420, train acc: 0.5036, val loss: 1.2415, val acc: 0.4934  (best train acc: 0.5167, best val acc: 0.5113)\n",
      "[Epoch: 1600] train loss: 1.2362, train acc: 0.5056, val loss: 1.2398, val acc: 0.4934  (best train acc: 0.5167, best val acc: 0.5113)\n",
      "[Epoch: 1620] train loss: 1.2388, train acc: 0.4954, val loss: 1.2390, val acc: 0.4988  (best train acc: 0.5167, best val acc: 0.5113)\n",
      "[Epoch: 1640] train loss: 1.2303, train acc: 0.5098, val loss: 1.2379, val acc: 0.5025  (best train acc: 0.5167, best val acc: 0.5113)\n",
      "[Epoch: 1660] train loss: 1.2390, train acc: 0.4929, val loss: 1.2361, val acc: 0.4944  (best train acc: 0.5167, best val acc: 0.5113)\n",
      "[Epoch: 1680] train loss: 1.2313, train acc: 0.5020, val loss: 1.2342, val acc: 0.4931  (best train acc: 0.5167, best val acc: 0.5113)\n",
      "[Epoch: 1700] train loss: 1.2375, train acc: 0.4722, val loss: 1.2429, val acc: 0.5133  (best train acc: 0.5167, best val acc: 0.5133)\n",
      "[Epoch: 1720] train loss: 1.2354, train acc: 0.5066, val loss: 1.2324, val acc: 0.4921  (best train acc: 0.5167, best val acc: 0.5133)\n",
      "[Epoch: 1740] train loss: 1.2299, train acc: 0.5074, val loss: 1.2314, val acc: 0.4911  (best train acc: 0.5167, best val acc: 0.5133)\n",
      "[Epoch: 1760] train loss: 1.2295, train acc: 0.5048, val loss: 1.2296, val acc: 0.5022  (best train acc: 0.5167, best val acc: 0.5133)\n",
      "[Epoch: 1780] train loss: 1.2254, train acc: 0.4954, val loss: 1.2289, val acc: 0.5035  (best train acc: 0.5167, best val acc: 0.5133)\n",
      "[Epoch: 1800] train loss: 1.2264, train acc: 0.5051, val loss: 1.2279, val acc: 0.4965  (best train acc: 0.5167, best val acc: 0.5133)\n",
      "[Epoch: 1820] train loss: 1.2250, train acc: 0.5116, val loss: 1.2268, val acc: 0.5083  (best train acc: 0.5176, best val acc: 0.5133)\n",
      "[Epoch: 1840] train loss: 1.2264, train acc: 0.5083, val loss: 1.2285, val acc: 0.4813  (best train acc: 0.5176, best val acc: 0.5133)\n",
      "[Epoch: 1860] train loss: 1.2239, train acc: 0.5007, val loss: 1.2243, val acc: 0.4914  (best train acc: 0.5176, best val acc: 0.5140)\n",
      "[Epoch: 1880] train loss: 1.2217, train acc: 0.5059, val loss: 1.2238, val acc: 0.4921  (best train acc: 0.5212, best val acc: 0.5140)\n",
      "[Epoch: 1900] train loss: 1.2253, train acc: 0.4985, val loss: 1.2241, val acc: 0.5147  (best train acc: 0.5212, best val acc: 0.5147)\n",
      "[Epoch: 1920] train loss: 1.2258, train acc: 0.5122, val loss: 1.2224, val acc: 0.5073  (best train acc: 0.5212, best val acc: 0.5147)\n",
      "[Epoch: 1940] train loss: 1.2176, train acc: 0.5062, val loss: 1.2285, val acc: 0.4766  (best train acc: 0.5212, best val acc: 0.5147)\n",
      "[Epoch: 1960] train loss: 1.2128, train acc: 0.5094, val loss: 1.2202, val acc: 0.5066  (best train acc: 0.5212, best val acc: 0.5147)\n",
      "[Epoch: 1980] train loss: 1.2196, train acc: 0.5062, val loss: 1.2177, val acc: 0.5046  (best train acc: 0.5212, best val acc: 0.5147)\n",
      "[Epoch: 2000] train loss: 1.2172, train acc: 0.5013, val loss: 1.2181, val acc: 0.5042  (best train acc: 0.5212, best val acc: 0.5147)\n",
      "[Epoch: 2020] train loss: 1.2134, train acc: 0.5074, val loss: 1.2219, val acc: 0.5113  (best train acc: 0.5219, best val acc: 0.5147)\n",
      "[Epoch: 2040] train loss: 1.2183, train acc: 0.5039, val loss: 1.2156, val acc: 0.4968  (best train acc: 0.5219, best val acc: 0.5147)\n",
      "[Epoch: 2060] train loss: 1.2090, train acc: 0.4950, val loss: 1.2155, val acc: 0.5113  (best train acc: 0.5219, best val acc: 0.5153)\n",
      "[Epoch: 2080] train loss: 1.2033, train acc: 0.5100, val loss: 1.2138, val acc: 0.5035  (best train acc: 0.5219, best val acc: 0.5153)\n",
      "[Epoch: 2100] train loss: 1.2146, train acc: 0.5058, val loss: 1.2132, val acc: 0.5089  (best train acc: 0.5230, best val acc: 0.5153)\n",
      "[Epoch: 2120] train loss: 1.2147, train acc: 0.5100, val loss: 1.2135, val acc: 0.4941  (best train acc: 0.5230, best val acc: 0.5153)\n",
      "[Epoch: 2140] train loss: 1.2147, train acc: 0.5006, val loss: 1.2135, val acc: 0.5116  (best train acc: 0.5230, best val acc: 0.5153)\n",
      "[Epoch: 2160] train loss: 1.2125, train acc: 0.4995, val loss: 1.2123, val acc: 0.4924  (best train acc: 0.5230, best val acc: 0.5153)\n",
      "[Epoch: 2180] train loss: 1.2055, train acc: 0.5197, val loss: 1.2112, val acc: 0.4961  (best train acc: 0.5230, best val acc: 0.5153)\n",
      "[Epoch: 2200] train loss: 1.2093, train acc: 0.5046, val loss: 1.2097, val acc: 0.5079  (best train acc: 0.5230, best val acc: 0.5153)\n",
      "[Epoch: 2220] train loss: 1.2078, train acc: 0.5036, val loss: 1.2089, val acc: 0.5046  (best train acc: 0.5230, best val acc: 0.5153)\n",
      "[Epoch: 2240] train loss: 1.2013, train acc: 0.5174, val loss: 1.2076, val acc: 0.5069  (best train acc: 0.5230, best val acc: 0.5153)\n",
      "[Epoch: 2260] train loss: 1.2075, train acc: 0.5062, val loss: 1.2081, val acc: 0.5066  (best train acc: 0.5230, best val acc: 0.5153)\n",
      "[Epoch: 2280] train loss: 1.2013, train acc: 0.5075, val loss: 1.2081, val acc: 0.5076  (best train acc: 0.5230, best val acc: 0.5153)\n",
      "[Epoch: 2300] train loss: 1.2050, train acc: 0.5091, val loss: 1.2114, val acc: 0.5106  (best train acc: 0.5230, best val acc: 0.5153)\n",
      "[Epoch: 2320] train loss: 1.2079, train acc: 0.5130, val loss: 1.2051, val acc: 0.5062  (best train acc: 0.5230, best val acc: 0.5153)\n",
      "[Epoch: 2340] train loss: 1.2016, train acc: 0.5158, val loss: 1.2051, val acc: 0.5022  (best train acc: 0.5230, best val acc: 0.5153)\n",
      "[Epoch: 2360] train loss: 1.2007, train acc: 0.5061, val loss: 1.2047, val acc: 0.5025  (best train acc: 0.5230, best val acc: 0.5160)\n",
      "[Epoch: 2380] train loss: 1.2058, train acc: 0.5012, val loss: 1.2037, val acc: 0.5039  (best train acc: 0.5230, best val acc: 0.5160)\n",
      "[Epoch: 2400] train loss: 1.2168, train acc: 0.5088, val loss: 1.2087, val acc: 0.4877  (best train acc: 0.5230, best val acc: 0.5160)\n",
      "[Epoch: 2420] train loss: 1.2089, train acc: 0.4842, val loss: 1.2047, val acc: 0.4958  (best train acc: 0.5230, best val acc: 0.5160)\n",
      "[Epoch: 2440] train loss: 1.1946, train acc: 0.5091, val loss: 1.2032, val acc: 0.5096  (best train acc: 0.5230, best val acc: 0.5228)\n",
      "[Epoch: 2460] train loss: 1.1975, train acc: 0.5147, val loss: 1.2022, val acc: 0.5069  (best train acc: 0.5230, best val acc: 0.5228)\n",
      "[Epoch: 2480] train loss: 1.2052, train acc: 0.5131, val loss: 1.2029, val acc: 0.4998  (best train acc: 0.5230, best val acc: 0.5228)\n",
      "[Epoch: 2500] train loss: 1.1982, train acc: 0.5106, val loss: 1.2015, val acc: 0.5029  (best train acc: 0.5230, best val acc: 0.5228)\n",
      "[Epoch: 2520] train loss: 1.1956, train acc: 0.5181, val loss: 1.2022, val acc: 0.4958  (best train acc: 0.5230, best val acc: 0.5228)\n",
      "[Epoch: 2540] train loss: 1.1975, train acc: 0.5106, val loss: 1.1996, val acc: 0.5069  (best train acc: 0.5230, best val acc: 0.5228)\n",
      "[Epoch: 2560] train loss: 1.2010, train acc: 0.5025, val loss: 1.2001, val acc: 0.5150  (best train acc: 0.5244, best val acc: 0.5228)\n",
      "[Epoch: 2580] train loss: 1.1994, train acc: 0.5067, val loss: 1.1987, val acc: 0.5110  (best train acc: 0.5244, best val acc: 0.5228)\n",
      "[Epoch: 2600] train loss: 1.2043, train acc: 0.5001, val loss: 1.1980, val acc: 0.5120  (best train acc: 0.5244, best val acc: 0.5228)\n",
      "[Epoch: 2620] train loss: 1.1935, train acc: 0.5129, val loss: 1.1987, val acc: 0.5035  (best train acc: 0.5244, best val acc: 0.5228)\n",
      "[Epoch: 2640] train loss: 1.2017, train acc: 0.4975, val loss: 1.1986, val acc: 0.5120  (best train acc: 0.5244, best val acc: 0.5228)\n",
      "[Epoch: 2660] train loss: 1.2017, train acc: 0.5153, val loss: 1.1974, val acc: 0.5039  (best train acc: 0.5244, best val acc: 0.5228)\n",
      "[Epoch: 2680] train loss: 1.1951, train acc: 0.4988, val loss: 1.1989, val acc: 0.5019  (best train acc: 0.5244, best val acc: 0.5228)\n",
      "[Epoch: 2700] train loss: 1.2086, train acc: 0.4868, val loss: 1.1972, val acc: 0.5143  (best train acc: 0.5244, best val acc: 0.5228)\n",
      "[Epoch: 2720] train loss: 1.1946, train acc: 0.5041, val loss: 1.2037, val acc: 0.5076  (best train acc: 0.5244, best val acc: 0.5228)\n",
      "[Epoch: 2740] train loss: 1.2160, train acc: 0.4985, val loss: 1.2134, val acc: 0.4671  (best train acc: 0.5244, best val acc: 0.5228)\n",
      "[Epoch: 2760] train loss: 1.2065, train acc: 0.5013, val loss: 1.2096, val acc: 0.4877  (best train acc: 0.5244, best val acc: 0.5228)\n",
      "[Epoch: 2780] train loss: 1.1921, train acc: 0.5133, val loss: 1.1967, val acc: 0.4998  (best train acc: 0.5244, best val acc: 0.5228)\n",
      "[Epoch: 2800] train loss: 1.1923, train acc: 0.5206, val loss: 1.1958, val acc: 0.5160  (best train acc: 0.5244, best val acc: 0.5228)\n",
      "[Epoch: 2820] train loss: 1.1940, train acc: 0.5119, val loss: 1.1948, val acc: 0.5046  (best train acc: 0.5244, best val acc: 0.5228)\n",
      "[Epoch: 2840] train loss: 1.1902, train acc: 0.5155, val loss: 1.1939, val acc: 0.5069  (best train acc: 0.5244, best val acc: 0.5228)\n",
      "[Epoch: 2860] train loss: 1.1941, train acc: 0.4960, val loss: 1.1950, val acc: 0.5160  (best train acc: 0.5244, best val acc: 0.5228)\n",
      "[Epoch: 2880] train loss: 1.1945, train acc: 0.5038, val loss: 1.2013, val acc: 0.4897  (best train acc: 0.5244, best val acc: 0.5228)\n",
      "[Epoch: 2900] train loss: 1.1960, train acc: 0.5160, val loss: 1.1940, val acc: 0.5059  (best train acc: 0.5244, best val acc: 0.5228)\n",
      "[Epoch: 2920] train loss: 1.1953, train acc: 0.4925, val loss: 1.1972, val acc: 0.4968  (best train acc: 0.5244, best val acc: 0.5228)\n",
      "[Epoch: 2940] train loss: 1.1908, train acc: 0.5180, val loss: 1.1931, val acc: 0.5042  (best train acc: 0.5244, best val acc: 0.5228)\n",
      "[Epoch: 2960] train loss: 1.1968, train acc: 0.5200, val loss: 1.1936, val acc: 0.5056  (best train acc: 0.5244, best val acc: 0.5228)\n",
      "[Epoch: 2980] train loss: 1.1937, train acc: 0.5124, val loss: 1.2028, val acc: 0.5049  (best train acc: 0.5244, best val acc: 0.5228)\n",
      "[Epoch: 3000] train loss: 1.1863, train acc: 0.5022, val loss: 1.1947, val acc: 0.5228  (best train acc: 0.5244, best val acc: 0.5228)\n",
      "[Epoch: 3020] train loss: 1.1876, train acc: 0.5102, val loss: 1.1916, val acc: 0.5076  (best train acc: 0.5244, best val acc: 0.5228)\n",
      "[Epoch: 3040] train loss: 1.1982, train acc: 0.4927, val loss: 1.1937, val acc: 0.5005  (best train acc: 0.5254, best val acc: 0.5228)\n",
      "[Epoch: 3060] train loss: 1.1943, train acc: 0.4983, val loss: 1.1937, val acc: 0.5140  (best train acc: 0.5254, best val acc: 0.5228)\n",
      "[Epoch: 3080] train loss: 1.1929, train acc: 0.5044, val loss: 1.1913, val acc: 0.5086  (best train acc: 0.5254, best val acc: 0.5228)\n",
      "[Epoch: 3100] train loss: 1.1896, train acc: 0.5086, val loss: 1.1938, val acc: 0.5137  (best train acc: 0.5254, best val acc: 0.5228)\n",
      "[Epoch: 3120] train loss: 1.1886, train acc: 0.5140, val loss: 1.1909, val acc: 0.5147  (best train acc: 0.5254, best val acc: 0.5228)\n",
      "[Epoch: 3140] train loss: 1.1873, train acc: 0.5103, val loss: 1.1955, val acc: 0.5056  (best train acc: 0.5254, best val acc: 0.5245)\n",
      "[Epoch: 3160] train loss: 1.1852, train acc: 0.5182, val loss: 1.1898, val acc: 0.5046  (best train acc: 0.5272, best val acc: 0.5245)\n",
      "[Epoch: 3180] train loss: 1.1820, train acc: 0.5103, val loss: 1.1905, val acc: 0.5228  (best train acc: 0.5272, best val acc: 0.5245)\n",
      "[Epoch: 3200] train loss: 1.1936, train acc: 0.5123, val loss: 1.1893, val acc: 0.5177  (best train acc: 0.5272, best val acc: 0.5245)\n",
      "[Epoch: 3220] train loss: 1.1835, train acc: 0.5096, val loss: 1.1884, val acc: 0.5147  (best train acc: 0.5272, best val acc: 0.5245)\n",
      "[Epoch: 3240] train loss: 1.1796, train acc: 0.5170, val loss: 1.1884, val acc: 0.5035  (best train acc: 0.5272, best val acc: 0.5245)\n",
      "[Epoch: 3260] train loss: 1.1771, train acc: 0.5213, val loss: 1.1897, val acc: 0.5069  (best train acc: 0.5272, best val acc: 0.5245)\n",
      "[Epoch: 3280] train loss: 1.1893, train acc: 0.5072, val loss: 1.1996, val acc: 0.5076  (best train acc: 0.5272, best val acc: 0.5245)\n",
      "[Epoch: 3300] train loss: 1.1825, train acc: 0.5185, val loss: 1.1886, val acc: 0.5204  (best train acc: 0.5272, best val acc: 0.5245)\n",
      "[Epoch: 3320] train loss: 1.1868, train acc: 0.5090, val loss: 1.1955, val acc: 0.5079  (best train acc: 0.5321, best val acc: 0.5265)\n",
      "[Epoch: 3340] train loss: 1.1913, train acc: 0.5127, val loss: 1.1926, val acc: 0.5029  (best train acc: 0.5321, best val acc: 0.5265)\n",
      "[Epoch: 3360] train loss: 1.1969, train acc: 0.4956, val loss: 1.1916, val acc: 0.5099  (best train acc: 0.5321, best val acc: 0.5265)\n",
      "[Epoch: 3380] train loss: 1.1853, train acc: 0.5143, val loss: 1.1872, val acc: 0.5076  (best train acc: 0.5321, best val acc: 0.5265)\n",
      "[Epoch: 3400] train loss: 1.1831, train acc: 0.5147, val loss: 1.1872, val acc: 0.5126  (best train acc: 0.5321, best val acc: 0.5265)\n",
      "[Epoch: 3420] train loss: 1.1840, train acc: 0.5147, val loss: 1.1863, val acc: 0.5120  (best train acc: 0.5321, best val acc: 0.5265)\n",
      "[Epoch: 3440] train loss: 1.1846, train acc: 0.5170, val loss: 1.1880, val acc: 0.5076  (best train acc: 0.5321, best val acc: 0.5265)\n",
      "[Epoch: 3460] train loss: 1.1843, train acc: 0.5139, val loss: 1.1869, val acc: 0.5113  (best train acc: 0.5321, best val acc: 0.5265)\n",
      "[Epoch: 3480] train loss: 1.1802, train acc: 0.5202, val loss: 1.1873, val acc: 0.5069  (best train acc: 0.5321, best val acc: 0.5265)\n",
      "[Epoch: 3500] train loss: 1.1836, train acc: 0.5153, val loss: 1.1852, val acc: 0.5174  (best train acc: 0.5321, best val acc: 0.5265)\n",
      "[Epoch: 3520] train loss: 1.1812, train acc: 0.5153, val loss: 1.1878, val acc: 0.5191  (best train acc: 0.5321, best val acc: 0.5265)\n",
      "[Epoch: 3540] train loss: 1.1749, train acc: 0.5202, val loss: 1.1858, val acc: 0.5160  (best train acc: 0.5321, best val acc: 0.5265)\n",
      "[Epoch: 3560] train loss: 1.1790, train acc: 0.5166, val loss: 1.1844, val acc: 0.5160  (best train acc: 0.5321, best val acc: 0.5265)\n",
      "[Epoch: 3580] train loss: 1.1822, train acc: 0.5056, val loss: 1.1847, val acc: 0.5180  (best train acc: 0.5321, best val acc: 0.5265)\n",
      "[Epoch: 3600] train loss: 1.1792, train acc: 0.5139, val loss: 1.1847, val acc: 0.5130  (best train acc: 0.5321, best val acc: 0.5265)\n",
      "[Epoch: 3620] train loss: 1.1871, train acc: 0.5033, val loss: 1.1886, val acc: 0.5029  (best train acc: 0.5321, best val acc: 0.5265)\n",
      "[Epoch: 3640] train loss: 1.1788, train acc: 0.5194, val loss: 1.1839, val acc: 0.5099  (best train acc: 0.5321, best val acc: 0.5265)\n",
      "[Epoch: 3660] train loss: 1.1915, train acc: 0.5106, val loss: 1.1868, val acc: 0.5113  (best train acc: 0.5321, best val acc: 0.5265)\n",
      "[Epoch: 3680] train loss: 1.1758, train acc: 0.5187, val loss: 1.1851, val acc: 0.5110  (best train acc: 0.5321, best val acc: 0.5265)\n",
      "[Epoch: 3700] train loss: 1.1854, train acc: 0.5114, val loss: 1.1847, val acc: 0.5116  (best train acc: 0.5321, best val acc: 0.5265)\n",
      "[Epoch: 3720] train loss: 1.1783, train acc: 0.5208, val loss: 1.1837, val acc: 0.5153  (best train acc: 0.5321, best val acc: 0.5265)\n",
      "[Epoch: 3740] train loss: 1.1800, train acc: 0.5084, val loss: 1.1879, val acc: 0.5005  (best train acc: 0.5321, best val acc: 0.5268)\n",
      "[Epoch: 3760] train loss: 1.1830, train acc: 0.5022, val loss: 1.1828, val acc: 0.5180  (best train acc: 0.5321, best val acc: 0.5268)\n",
      "[Epoch: 3780] train loss: 1.1756, train acc: 0.5234, val loss: 1.1852, val acc: 0.5234  (best train acc: 0.5321, best val acc: 0.5268)\n",
      "[Epoch: 3800] train loss: 1.1763, train acc: 0.5195, val loss: 1.1823, val acc: 0.5143  (best train acc: 0.5321, best val acc: 0.5268)\n",
      "[Epoch: 3820] train loss: 1.1860, train acc: 0.5171, val loss: 1.1827, val acc: 0.5126  (best train acc: 0.5321, best val acc: 0.5268)\n",
      "[Epoch: 3840] train loss: 1.1724, train acc: 0.5187, val loss: 1.1820, val acc: 0.5170  (best train acc: 0.5321, best val acc: 0.5268)\n",
      "[Epoch: 3860] train loss: 1.1789, train acc: 0.5184, val loss: 1.1830, val acc: 0.5073  (best train acc: 0.5321, best val acc: 0.5268)\n",
      "[Epoch: 3880] train loss: 1.1835, train acc: 0.5142, val loss: 1.1825, val acc: 0.5164  (best train acc: 0.5321, best val acc: 0.5268)\n",
      "[Epoch: 3900] train loss: 1.1859, train acc: 0.5141, val loss: 1.1855, val acc: 0.5032  (best train acc: 0.5321, best val acc: 0.5268)\n",
      "[Epoch: 3920] train loss: 1.1913, train acc: 0.5014, val loss: 1.1824, val acc: 0.5116  (best train acc: 0.5321, best val acc: 0.5278)\n",
      "[Epoch: 3940] train loss: 1.1885, train acc: 0.5162, val loss: 1.1817, val acc: 0.5180  (best train acc: 0.5321, best val acc: 0.5278)\n",
      "[Epoch: 3960] train loss: 1.1771, train acc: 0.5250, val loss: 1.1829, val acc: 0.5207  (best train acc: 0.5321, best val acc: 0.5278)\n",
      "[Epoch: 3980] train loss: 1.1796, train acc: 0.5215, val loss: 1.1814, val acc: 0.5234  (best train acc: 0.5321, best val acc: 0.5278)\n",
      "[Epoch: 4000] train loss: 1.1839, train acc: 0.5107, val loss: 1.1808, val acc: 0.5133  (best train acc: 0.5321, best val acc: 0.5278)\n",
      "[Epoch: 4020] train loss: 1.1826, train acc: 0.5049, val loss: 1.1840, val acc: 0.5245  (best train acc: 0.5321, best val acc: 0.5278)\n",
      "[Epoch: 4040] train loss: 1.1778, train acc: 0.5080, val loss: 1.1825, val acc: 0.5099  (best train acc: 0.5321, best val acc: 0.5278)\n",
      "[Epoch: 4060] train loss: 1.1709, train acc: 0.5242, val loss: 1.1806, val acc: 0.5207  (best train acc: 0.5321, best val acc: 0.5278)\n",
      "[Epoch: 4080] train loss: 1.1715, train acc: 0.5202, val loss: 1.1800, val acc: 0.5201  (best train acc: 0.5321, best val acc: 0.5278)\n",
      "[Epoch: 4100] train loss: 1.1888, train acc: 0.5132, val loss: 1.1901, val acc: 0.4914  (best train acc: 0.5321, best val acc: 0.5278)\n",
      "[Epoch: 4120] train loss: 1.1803, train acc: 0.5082, val loss: 1.1799, val acc: 0.5140  (best train acc: 0.5321, best val acc: 0.5278)\n",
      "[Epoch: 4140] train loss: 1.1852, train acc: 0.5132, val loss: 1.1796, val acc: 0.5204  (best train acc: 0.5321, best val acc: 0.5278)\n",
      "[Epoch: 4160] train loss: 1.1711, train acc: 0.5141, val loss: 1.1820, val acc: 0.5231  (best train acc: 0.5321, best val acc: 0.5278)\n",
      "[Epoch: 4180] train loss: 1.1769, train acc: 0.5201, val loss: 1.1848, val acc: 0.5062  (best train acc: 0.5321, best val acc: 0.5278)\n",
      "[Epoch: 4200] train loss: 1.1747, train acc: 0.5133, val loss: 1.1809, val acc: 0.5218  (best train acc: 0.5321, best val acc: 0.5278)\n",
      "[Epoch: 4220] train loss: 1.1768, train acc: 0.5158, val loss: 1.1796, val acc: 0.5231  (best train acc: 0.5321, best val acc: 0.5278)\n",
      "[Epoch: 4240] train loss: 1.1727, train acc: 0.5197, val loss: 1.1794, val acc: 0.5218  (best train acc: 0.5321, best val acc: 0.5278)\n",
      "[Epoch: 4260] train loss: 1.1752, train acc: 0.5128, val loss: 1.1791, val acc: 0.5123  (best train acc: 0.5321, best val acc: 0.5278)\n",
      "[Epoch: 4280] train loss: 1.1739, train acc: 0.5139, val loss: 1.1791, val acc: 0.5218  (best train acc: 0.5321, best val acc: 0.5278)\n",
      "[Epoch: 4300] train loss: 1.1738, train acc: 0.5193, val loss: 1.1789, val acc: 0.5126  (best train acc: 0.5321, best val acc: 0.5278)\n",
      "[Epoch: 4320] train loss: 1.1731, train acc: 0.5204, val loss: 1.1792, val acc: 0.5170  (best train acc: 0.5321, best val acc: 0.5278)\n",
      "[Epoch: 4340] train loss: 1.1861, train acc: 0.5161, val loss: 1.1786, val acc: 0.5224  (best train acc: 0.5321, best val acc: 0.5278)\n",
      "[Epoch: 4360] train loss: 1.1736, train acc: 0.5212, val loss: 1.1796, val acc: 0.5258  (best train acc: 0.5321, best val acc: 0.5278)\n",
      "[Epoch: 4380] train loss: 1.1770, train acc: 0.5163, val loss: 1.1797, val acc: 0.5204  (best train acc: 0.5321, best val acc: 0.5278)\n",
      "[Epoch: 4400] train loss: 1.1685, train acc: 0.5226, val loss: 1.1777, val acc: 0.5197  (best train acc: 0.5321, best val acc: 0.5278)\n",
      "[Epoch: 4420] train loss: 1.1701, train acc: 0.5223, val loss: 1.1840, val acc: 0.5008  (best train acc: 0.5321, best val acc: 0.5285)\n",
      "[Epoch: 4440] train loss: 1.1785, train acc: 0.5153, val loss: 1.1814, val acc: 0.5147  (best train acc: 0.5321, best val acc: 0.5285)\n",
      "[Epoch: 4460] train loss: 1.1754, train acc: 0.5234, val loss: 1.1783, val acc: 0.5180  (best train acc: 0.5321, best val acc: 0.5285)\n",
      "[Epoch: 4480] train loss: 1.1705, train acc: 0.5088, val loss: 1.1791, val acc: 0.5218  (best train acc: 0.5321, best val acc: 0.5285)\n",
      "[Epoch: 4500] train loss: 1.1782, train acc: 0.5180, val loss: 1.1928, val acc: 0.5029  (best train acc: 0.5321, best val acc: 0.5285)\n",
      "[Epoch: 4520] train loss: 1.1739, train acc: 0.5141, val loss: 1.1778, val acc: 0.5187  (best train acc: 0.5321, best val acc: 0.5285)\n",
      "[Epoch: 4540] train loss: 1.1727, train acc: 0.5213, val loss: 1.1786, val acc: 0.5218  (best train acc: 0.5321, best val acc: 0.5285)\n",
      "[Epoch: 4560] train loss: 1.1706, train acc: 0.5170, val loss: 1.1766, val acc: 0.5218  (best train acc: 0.5321, best val acc: 0.5285)\n",
      "[Epoch: 4580] train loss: 1.1756, train acc: 0.5059, val loss: 1.1764, val acc: 0.5255  (best train acc: 0.5321, best val acc: 0.5285)\n",
      "[Epoch: 4600] train loss: 1.1667, train acc: 0.5233, val loss: 1.1771, val acc: 0.5265  (best train acc: 0.5321, best val acc: 0.5285)\n",
      "[Epoch: 4620] train loss: 1.1702, train acc: 0.5250, val loss: 1.1778, val acc: 0.5140  (best train acc: 0.5321, best val acc: 0.5285)\n",
      "[Epoch: 4640] train loss: 1.1760, train acc: 0.5184, val loss: 1.1763, val acc: 0.5207  (best train acc: 0.5321, best val acc: 0.5285)\n",
      "[Epoch: 4660] train loss: 1.1721, train acc: 0.5244, val loss: 1.1797, val acc: 0.5147  (best train acc: 0.5321, best val acc: 0.5285)\n",
      "[Epoch: 4680] train loss: 1.1702, train acc: 0.5262, val loss: 1.1757, val acc: 0.5164  (best train acc: 0.5321, best val acc: 0.5285)\n",
      "[Epoch: 4700] train loss: 1.1692, train acc: 0.5130, val loss: 1.1758, val acc: 0.5170  (best train acc: 0.5321, best val acc: 0.5285)\n",
      "[Epoch: 4720] train loss: 1.1703, train acc: 0.5144, val loss: 1.1774, val acc: 0.5201  (best train acc: 0.5321, best val acc: 0.5285)\n",
      "[Epoch: 4740] train loss: 1.1704, train acc: 0.5239, val loss: 1.1748, val acc: 0.5251  (best train acc: 0.5321, best val acc: 0.5285)\n",
      "[Epoch: 4760] train loss: 1.1671, train acc: 0.5259, val loss: 1.1782, val acc: 0.5180  (best train acc: 0.5321, best val acc: 0.5285)\n",
      "[Epoch: 4780] train loss: 1.1759, train acc: 0.5175, val loss: 1.1759, val acc: 0.5255  (best train acc: 0.5321, best val acc: 0.5285)\n",
      "[Epoch: 4800] train loss: 1.1711, train acc: 0.5167, val loss: 1.1755, val acc: 0.5174  (best train acc: 0.5321, best val acc: 0.5285)\n",
      "[Epoch: 4820] train loss: 1.1700, train acc: 0.5254, val loss: 1.1742, val acc: 0.5245  (best train acc: 0.5321, best val acc: 0.5285)\n",
      "[Epoch: 4840] train loss: 1.1767, train acc: 0.5206, val loss: 1.1777, val acc: 0.5234  (best train acc: 0.5321, best val acc: 0.5285)\n",
      "[Epoch: 4860] train loss: 1.1697, train acc: 0.5205, val loss: 1.1750, val acc: 0.5194  (best train acc: 0.5321, best val acc: 0.5285)\n",
      "[Epoch: 4880] train loss: 1.1666, train acc: 0.5247, val loss: 1.1812, val acc: 0.5042  (best train acc: 0.5321, best val acc: 0.5285)\n",
      "[Epoch: 4900] train loss: 1.1717, train acc: 0.5054, val loss: 1.1738, val acc: 0.5201  (best train acc: 0.5321, best val acc: 0.5285)\n",
      "[Epoch: 4920] train loss: 1.1620, train acc: 0.5277, val loss: 1.1746, val acc: 0.5224  (best train acc: 0.5321, best val acc: 0.5285)\n",
      "[Epoch: 4940] train loss: 1.1709, train acc: 0.5189, val loss: 1.1757, val acc: 0.5140  (best train acc: 0.5321, best val acc: 0.5285)\n",
      "[Epoch: 4960] train loss: 1.1647, train acc: 0.5219, val loss: 1.1738, val acc: 0.5218  (best train acc: 0.5321, best val acc: 0.5285)\n",
      "[Epoch: 4980] train loss: 1.1686, train acc: 0.5231, val loss: 1.1751, val acc: 0.5292  (best train acc: 0.5321, best val acc: 0.5292)\n",
      "[Epoch: 5000] train loss: 1.1712, train acc: 0.5109, val loss: 1.1738, val acc: 0.5207  (best train acc: 0.5321, best val acc: 0.5292)\n",
      "[Epoch: 5020] train loss: 1.1867, train acc: 0.5121, val loss: 1.1725, val acc: 0.5211  (best train acc: 0.5321, best val acc: 0.5292)\n",
      "[Epoch: 5040] train loss: 1.1762, train acc: 0.5222, val loss: 1.1741, val acc: 0.5214  (best train acc: 0.5321, best val acc: 0.5292)\n",
      "[Epoch: 5060] train loss: 1.1668, train acc: 0.5133, val loss: 1.1746, val acc: 0.5157  (best train acc: 0.5321, best val acc: 0.5292)\n",
      "[Epoch: 5080] train loss: 1.1699, train acc: 0.5114, val loss: 1.1741, val acc: 0.5238  (best train acc: 0.5321, best val acc: 0.5292)\n",
      "[Epoch: 5100] train loss: 1.1665, train acc: 0.5223, val loss: 1.1724, val acc: 0.5228  (best train acc: 0.5321, best val acc: 0.5292)\n",
      "[Epoch: 5120] train loss: 1.1612, train acc: 0.5248, val loss: 1.1722, val acc: 0.5218  (best train acc: 0.5321, best val acc: 0.5292)\n",
      "[Epoch: 5140] train loss: 1.1664, train acc: 0.5215, val loss: 1.1720, val acc: 0.5221  (best train acc: 0.5321, best val acc: 0.5292)\n",
      "[Epoch: 5160] train loss: 1.1631, train acc: 0.5226, val loss: 1.1735, val acc: 0.5160  (best train acc: 0.5321, best val acc: 0.5292)\n",
      "[Epoch: 5180] train loss: 1.1718, train acc: 0.5160, val loss: 1.1722, val acc: 0.5177  (best train acc: 0.5321, best val acc: 0.5292)\n",
      "[Epoch: 5200] train loss: 1.1661, train acc: 0.5236, val loss: 1.1717, val acc: 0.5251  (best train acc: 0.5321, best val acc: 0.5292)\n",
      "[Epoch: 5220] train loss: 1.1701, train acc: 0.5228, val loss: 1.1714, val acc: 0.5221  (best train acc: 0.5321, best val acc: 0.5292)\n",
      "[Epoch: 5240] train loss: 1.1676, train acc: 0.5241, val loss: 1.1713, val acc: 0.5207  (best train acc: 0.5321, best val acc: 0.5292)\n",
      "[Epoch: 5260] train loss: 1.1671, train acc: 0.5268, val loss: 1.1747, val acc: 0.5238  (best train acc: 0.5321, best val acc: 0.5292)\n",
      "[Epoch: 5280] train loss: 1.1635, train acc: 0.5153, val loss: 1.1704, val acc: 0.5228  (best train acc: 0.5321, best val acc: 0.5292)\n",
      "[Epoch: 5300] train loss: 1.1686, train acc: 0.5219, val loss: 1.1723, val acc: 0.5164  (best train acc: 0.5321, best val acc: 0.5292)\n",
      "[Epoch: 5320] train loss: 1.1686, train acc: 0.5160, val loss: 1.1724, val acc: 0.5221  (best train acc: 0.5321, best val acc: 0.5292)\n",
      "[Epoch: 5340] train loss: 1.1651, train acc: 0.5211, val loss: 1.1694, val acc: 0.5231  (best train acc: 0.5321, best val acc: 0.5292)\n",
      "[Epoch: 5360] train loss: 1.1547, train acc: 0.5226, val loss: 1.1713, val acc: 0.5170  (best train acc: 0.5321, best val acc: 0.5292)\n",
      "[Epoch: 5380] train loss: 1.1648, train acc: 0.5241, val loss: 1.1735, val acc: 0.5211  (best train acc: 0.5349, best val acc: 0.5292)\n",
      "[Epoch: 5400] train loss: 1.1687, train acc: 0.5155, val loss: 1.1727, val acc: 0.5241  (best train acc: 0.5349, best val acc: 0.5292)\n",
      "[Epoch: 5420] train loss: 1.1737, train acc: 0.5153, val loss: 1.1716, val acc: 0.5153  (best train acc: 0.5349, best val acc: 0.5292)\n",
      "[Epoch: 5440] train loss: 1.1630, train acc: 0.5215, val loss: 1.1681, val acc: 0.5255  (best train acc: 0.5349, best val acc: 0.5292)\n",
      "[Epoch: 5460] train loss: 1.1648, train acc: 0.5262, val loss: 1.1690, val acc: 0.5234  (best train acc: 0.5349, best val acc: 0.5292)\n",
      "[Epoch: 5480] train loss: 1.1636, train acc: 0.5241, val loss: 1.1715, val acc: 0.5201  (best train acc: 0.5349, best val acc: 0.5305)\n",
      "[Epoch: 5500] train loss: 1.1601, train acc: 0.5270, val loss: 1.1674, val acc: 0.5255  (best train acc: 0.5349, best val acc: 0.5305)\n",
      "[Epoch: 5520] train loss: 1.1669, train acc: 0.5220, val loss: 1.1690, val acc: 0.5207  (best train acc: 0.5349, best val acc: 0.5305)\n",
      "[Epoch: 5540] train loss: 1.1646, train acc: 0.5181, val loss: 1.1731, val acc: 0.5089  (best train acc: 0.5349, best val acc: 0.5305)\n",
      "[Epoch: 5560] train loss: 1.1615, train acc: 0.5213, val loss: 1.1680, val acc: 0.5258  (best train acc: 0.5349, best val acc: 0.5305)\n",
      "[Epoch: 5580] train loss: 1.1620, train acc: 0.5205, val loss: 1.1708, val acc: 0.5231  (best train acc: 0.5349, best val acc: 0.5305)\n",
      "[Epoch: 5600] train loss: 1.1598, train acc: 0.5277, val loss: 1.1664, val acc: 0.5258  (best train acc: 0.5349, best val acc: 0.5305)\n",
      "[Epoch: 5620] train loss: 1.1611, train acc: 0.5234, val loss: 1.1670, val acc: 0.5174  (best train acc: 0.5349, best val acc: 0.5305)\n",
      "[Epoch: 5640] train loss: 1.1644, train acc: 0.5283, val loss: 1.1655, val acc: 0.5234  (best train acc: 0.5349, best val acc: 0.5305)\n",
      "[Epoch: 5660] train loss: 1.1574, train acc: 0.5289, val loss: 1.1669, val acc: 0.5268  (best train acc: 0.5349, best val acc: 0.5305)\n",
      "[Epoch: 5680] train loss: 1.1563, train acc: 0.5314, val loss: 1.1661, val acc: 0.5214  (best train acc: 0.5349, best val acc: 0.5305)\n",
      "[Epoch: 5700] train loss: 1.1657, train acc: 0.5168, val loss: 1.1656, val acc: 0.5238  (best train acc: 0.5349, best val acc: 0.5305)\n",
      "[Epoch: 5720] train loss: 1.1606, train acc: 0.5228, val loss: 1.1677, val acc: 0.5211  (best train acc: 0.5349, best val acc: 0.5305)\n",
      "[Epoch: 5740] train loss: 1.1528, train acc: 0.5309, val loss: 1.1658, val acc: 0.5214  (best train acc: 0.5349, best val acc: 0.5305)\n",
      "[Epoch: 5760] train loss: 1.1626, train acc: 0.5147, val loss: 1.1683, val acc: 0.5191  (best train acc: 0.5349, best val acc: 0.5305)\n",
      "[Epoch: 5780] train loss: 1.1607, train acc: 0.5231, val loss: 1.1643, val acc: 0.5248  (best train acc: 0.5349, best val acc: 0.5305)\n",
      "[Epoch: 5800] train loss: 1.1622, train acc: 0.5292, val loss: 1.1717, val acc: 0.5201  (best train acc: 0.5349, best val acc: 0.5305)\n",
      "[Epoch: 5820] train loss: 1.1601, train acc: 0.5222, val loss: 1.1644, val acc: 0.5231  (best train acc: 0.5349, best val acc: 0.5305)\n",
      "[Epoch: 5840] train loss: 1.1704, train acc: 0.5175, val loss: 1.1651, val acc: 0.5221  (best train acc: 0.5349, best val acc: 0.5312)\n",
      "[Epoch: 5860] train loss: 1.1725, train acc: 0.5032, val loss: 1.1648, val acc: 0.5197  (best train acc: 0.5349, best val acc: 0.5312)\n",
      "[Epoch: 5880] train loss: 1.1657, train acc: 0.5194, val loss: 1.1644, val acc: 0.5207  (best train acc: 0.5349, best val acc: 0.5312)\n",
      "[Epoch: 5900] train loss: 1.1522, train acc: 0.5343, val loss: 1.1631, val acc: 0.5272  (best train acc: 0.5349, best val acc: 0.5312)\n",
      "[Epoch: 5920] train loss: 1.1540, train acc: 0.5250, val loss: 1.1629, val acc: 0.5285  (best train acc: 0.5349, best val acc: 0.5312)\n",
      "[Epoch: 5940] train loss: 1.1635, train acc: 0.5248, val loss: 1.1629, val acc: 0.5248  (best train acc: 0.5349, best val acc: 0.5312)\n",
      "[Epoch: 5960] train loss: 1.1630, train acc: 0.5241, val loss: 1.1649, val acc: 0.5164  (best train acc: 0.5349, best val acc: 0.5312)\n",
      "[Epoch: 5980] train loss: 1.1651, train acc: 0.5205, val loss: 1.1639, val acc: 0.5295  (best train acc: 0.5349, best val acc: 0.5312)\n",
      "[Epoch: 6000] train loss: 1.1627, train acc: 0.5213, val loss: 1.1635, val acc: 0.5204  (best train acc: 0.5349, best val acc: 0.5312)\n",
      "[Epoch: 6020] train loss: 1.1581, train acc: 0.5285, val loss: 1.1617, val acc: 0.5268  (best train acc: 0.5349, best val acc: 0.5312)\n",
      "[Epoch: 6040] train loss: 1.1580, train acc: 0.5293, val loss: 1.1628, val acc: 0.5272  (best train acc: 0.5349, best val acc: 0.5312)\n",
      "[Epoch: 6060] train loss: 1.1555, train acc: 0.5226, val loss: 1.1628, val acc: 0.5194  (best train acc: 0.5349, best val acc: 0.5312)\n",
      "[Epoch: 6080] train loss: 1.1544, train acc: 0.5222, val loss: 1.1640, val acc: 0.5197  (best train acc: 0.5349, best val acc: 0.5312)\n",
      "[Epoch: 6100] train loss: 1.1540, train acc: 0.5285, val loss: 1.1612, val acc: 0.5218  (best train acc: 0.5349, best val acc: 0.5312)\n",
      "[Epoch: 6120] train loss: 1.1559, train acc: 0.5275, val loss: 1.1610, val acc: 0.5295  (best train acc: 0.5349, best val acc: 0.5312)\n",
      "[Epoch: 6140] train loss: 1.1551, train acc: 0.5310, val loss: 1.1607, val acc: 0.5251  (best train acc: 0.5349, best val acc: 0.5312)\n",
      "[Epoch: 6160] train loss: 1.1494, train acc: 0.5254, val loss: 1.1619, val acc: 0.5245  (best train acc: 0.5349, best val acc: 0.5312)\n",
      "[Epoch: 6180] train loss: 1.1547, train acc: 0.5281, val loss: 1.1624, val acc: 0.5224  (best train acc: 0.5349, best val acc: 0.5312)\n",
      "[Epoch: 6200] train loss: 1.1567, train acc: 0.5171, val loss: 1.1613, val acc: 0.5272  (best train acc: 0.5349, best val acc: 0.5312)\n",
      "[Epoch: 6220] train loss: 1.1587, train acc: 0.5230, val loss: 1.1607, val acc: 0.5194  (best train acc: 0.5349, best val acc: 0.5312)\n",
      "[Epoch: 6240] train loss: 1.1548, train acc: 0.5273, val loss: 1.1631, val acc: 0.5194  (best train acc: 0.5349, best val acc: 0.5312)\n",
      "[Epoch: 6260] train loss: 1.1579, train acc: 0.5215, val loss: 1.1601, val acc: 0.5282  (best train acc: 0.5349, best val acc: 0.5312)\n",
      "[Epoch: 6280] train loss: 1.1617, train acc: 0.5250, val loss: 1.1594, val acc: 0.5241  (best train acc: 0.5349, best val acc: 0.5312)\n",
      "[Epoch: 6300] train loss: 1.1648, train acc: 0.5185, val loss: 1.1590, val acc: 0.5248  (best train acc: 0.5349, best val acc: 0.5312)\n",
      "[Epoch: 6320] train loss: 1.1542, train acc: 0.5269, val loss: 1.1590, val acc: 0.5309  (best train acc: 0.5349, best val acc: 0.5312)\n",
      "[Epoch: 6340] train loss: 1.1590, train acc: 0.5304, val loss: 1.1595, val acc: 0.5234  (best train acc: 0.5349, best val acc: 0.5312)\n",
      "[Epoch: 6360] train loss: 1.1535, train acc: 0.5234, val loss: 1.1591, val acc: 0.5258  (best train acc: 0.5349, best val acc: 0.5312)\n",
      "[Epoch: 6380] train loss: 1.1487, train acc: 0.5276, val loss: 1.1619, val acc: 0.5177  (best train acc: 0.5349, best val acc: 0.5312)\n",
      "[Epoch: 6400] train loss: 1.1459, train acc: 0.5293, val loss: 1.1585, val acc: 0.5218  (best train acc: 0.5349, best val acc: 0.5312)\n",
      "[Epoch: 6420] train loss: 1.1599, train acc: 0.5218, val loss: 1.1589, val acc: 0.5258  (best train acc: 0.5349, best val acc: 0.5312)\n",
      "[Epoch: 6440] train loss: 1.1497, train acc: 0.5267, val loss: 1.1592, val acc: 0.5272  (best train acc: 0.5349, best val acc: 0.5312)\n",
      "[Epoch: 6460] train loss: 1.1580, train acc: 0.5218, val loss: 1.1601, val acc: 0.5245  (best train acc: 0.5349, best val acc: 0.5312)\n",
      "[Epoch: 6480] train loss: 1.1561, train acc: 0.5266, val loss: 1.1593, val acc: 0.5298  (best train acc: 0.5349, best val acc: 0.5312)\n",
      "[Epoch: 6500] train loss: 1.1596, train acc: 0.5260, val loss: 1.1617, val acc: 0.5170  (best train acc: 0.5349, best val acc: 0.5312)\n",
      "[Epoch: 6520] train loss: 1.1574, train acc: 0.5257, val loss: 1.1570, val acc: 0.5221  (best train acc: 0.5349, best val acc: 0.5312)\n",
      "[Epoch: 6540] train loss: 1.1489, train acc: 0.5306, val loss: 1.1568, val acc: 0.5275  (best train acc: 0.5349, best val acc: 0.5312)\n",
      "[Epoch: 6560] train loss: 1.1479, train acc: 0.5291, val loss: 1.1551, val acc: 0.5265  (best train acc: 0.5351, best val acc: 0.5312)\n",
      "[Epoch: 6580] train loss: 1.1485, train acc: 0.5337, val loss: 1.1543, val acc: 0.5258  (best train acc: 0.5351, best val acc: 0.5312)\n",
      "[Epoch: 6600] train loss: 1.1596, train acc: 0.5256, val loss: 1.1556, val acc: 0.5194  (best train acc: 0.5351, best val acc: 0.5312)\n",
      "[Epoch: 6620] train loss: 1.1434, train acc: 0.5333, val loss: 1.1531, val acc: 0.5288  (best train acc: 0.5357, best val acc: 0.5312)\n",
      "[Epoch: 6640] train loss: 1.1415, train acc: 0.5307, val loss: 1.1515, val acc: 0.5285  (best train acc: 0.5363, best val acc: 0.5312)\n",
      "[Epoch: 6660] train loss: 1.1528, train acc: 0.5283, val loss: 1.1521, val acc: 0.5285  (best train acc: 0.5363, best val acc: 0.5312)\n",
      "[Epoch: 6680] train loss: 1.1545, train acc: 0.5228, val loss: 1.1506, val acc: 0.5298  (best train acc: 0.5363, best val acc: 0.5312)\n",
      "[Epoch: 6700] train loss: 1.1558, train acc: 0.5273, val loss: 1.1504, val acc: 0.5255  (best train acc: 0.5363, best val acc: 0.5312)\n",
      "[Epoch: 6720] train loss: 1.1440, train acc: 0.5242, val loss: 1.1497, val acc: 0.5272  (best train acc: 0.5363, best val acc: 0.5312)\n",
      "[Epoch: 6740] train loss: 1.1521, train acc: 0.5247, val loss: 1.1495, val acc: 0.5292  (best train acc: 0.5363, best val acc: 0.5312)\n",
      "[Epoch: 6760] train loss: 1.1538, train acc: 0.5179, val loss: 1.1484, val acc: 0.5258  (best train acc: 0.5367, best val acc: 0.5312)\n",
      "[Epoch: 6780] train loss: 1.1410, train acc: 0.5286, val loss: 1.1486, val acc: 0.5234  (best train acc: 0.5367, best val acc: 0.5312)\n",
      "[Epoch: 6800] train loss: 1.1392, train acc: 0.5274, val loss: 1.1467, val acc: 0.5282  (best train acc: 0.5367, best val acc: 0.5312)\n",
      "[Epoch: 6820] train loss: 1.1475, train acc: 0.5218, val loss: 1.1479, val acc: 0.5268  (best train acc: 0.5367, best val acc: 0.5312)\n",
      "[Epoch: 6840] train loss: 1.1464, train acc: 0.5254, val loss: 1.1475, val acc: 0.5245  (best train acc: 0.5367, best val acc: 0.5312)\n",
      "[Epoch: 6860] train loss: 1.1488, train acc: 0.5197, val loss: 1.1475, val acc: 0.5261  (best train acc: 0.5367, best val acc: 0.5312)\n",
      "[Epoch: 6880] train loss: 1.1473, train acc: 0.5275, val loss: 1.1478, val acc: 0.5207  (best train acc: 0.5367, best val acc: 0.5312)\n",
      "[Epoch: 6900] train loss: 1.1447, train acc: 0.5283, val loss: 1.1521, val acc: 0.5123  (best train acc: 0.5367, best val acc: 0.5312)\n",
      "[Epoch: 6920] train loss: 1.1385, train acc: 0.5244, val loss: 1.1447, val acc: 0.5272  (best train acc: 0.5367, best val acc: 0.5312)\n",
      "[Epoch: 6940] train loss: 1.1519, train acc: 0.5194, val loss: 1.1455, val acc: 0.5292  (best train acc: 0.5367, best val acc: 0.5312)\n",
      "[Epoch: 6960] train loss: 1.1442, train acc: 0.5275, val loss: 1.1464, val acc: 0.5305  (best train acc: 0.5367, best val acc: 0.5312)\n",
      "[Epoch: 6980] train loss: 1.1362, train acc: 0.5301, val loss: 1.1449, val acc: 0.5231  (best train acc: 0.5380, best val acc: 0.5322)\n",
      "[Epoch: 7000] train loss: 1.1518, train acc: 0.5249, val loss: 1.1430, val acc: 0.5322  (best train acc: 0.5380, best val acc: 0.5322)\n",
      "[Epoch: 7020] train loss: 1.1404, train acc: 0.5316, val loss: 1.1436, val acc: 0.5275  (best train acc: 0.5380, best val acc: 0.5322)\n",
      "[Epoch: 7040] train loss: 1.1418, train acc: 0.5298, val loss: 1.1442, val acc: 0.5295  (best train acc: 0.5380, best val acc: 0.5322)\n",
      "[Epoch: 7060] train loss: 1.1480, train acc: 0.5262, val loss: 1.1437, val acc: 0.5278  (best train acc: 0.5380, best val acc: 0.5322)\n",
      "[Epoch: 7080] train loss: 1.1398, train acc: 0.5328, val loss: 1.1440, val acc: 0.5261  (best train acc: 0.5380, best val acc: 0.5322)\n",
      "[Epoch: 7100] train loss: 1.1402, train acc: 0.5286, val loss: 1.1408, val acc: 0.5278  (best train acc: 0.5380, best val acc: 0.5346)\n",
      "[Epoch: 7120] train loss: 1.1427, train acc: 0.5308, val loss: 1.1438, val acc: 0.5272  (best train acc: 0.5380, best val acc: 0.5346)\n",
      "[Epoch: 7140] train loss: 1.1440, train acc: 0.5212, val loss: 1.1406, val acc: 0.5245  (best train acc: 0.5380, best val acc: 0.5346)\n",
      "[Epoch: 7160] train loss: 1.1437, train acc: 0.5309, val loss: 1.1461, val acc: 0.5187  (best train acc: 0.5380, best val acc: 0.5346)\n",
      "[Epoch: 7180] train loss: 1.1350, train acc: 0.5261, val loss: 1.1420, val acc: 0.5255  (best train acc: 0.5380, best val acc: 0.5346)\n",
      "[Epoch: 7200] train loss: 1.1404, train acc: 0.5289, val loss: 1.1406, val acc: 0.5339  (best train acc: 0.5380, best val acc: 0.5346)\n",
      "[Epoch: 7220] train loss: 1.1402, train acc: 0.5260, val loss: 1.1391, val acc: 0.5302  (best train acc: 0.5380, best val acc: 0.5346)\n",
      "[Epoch: 7240] train loss: 1.1364, train acc: 0.5322, val loss: 1.1418, val acc: 0.5241  (best train acc: 0.5380, best val acc: 0.5346)\n",
      "[Epoch: 7260] train loss: 1.1334, train acc: 0.5338, val loss: 1.1406, val acc: 0.5234  (best train acc: 0.5380, best val acc: 0.5346)\n",
      "[Epoch: 7280] train loss: 1.1341, train acc: 0.5279, val loss: 1.1413, val acc: 0.5275  (best train acc: 0.5380, best val acc: 0.5346)\n",
      "[Epoch: 7300] train loss: 1.1358, train acc: 0.5300, val loss: 1.1384, val acc: 0.5258  (best train acc: 0.5380, best val acc: 0.5346)\n",
      "[Epoch: 7320] train loss: 1.1305, train acc: 0.5300, val loss: 1.1395, val acc: 0.5245  (best train acc: 0.5380, best val acc: 0.5346)\n",
      "[Epoch: 7340] train loss: 1.1461, train acc: 0.5253, val loss: 1.1377, val acc: 0.5288  (best train acc: 0.5380, best val acc: 0.5346)\n",
      "[Epoch: 7360] train loss: 1.1438, train acc: 0.5262, val loss: 1.1389, val acc: 0.5309  (best train acc: 0.5380, best val acc: 0.5346)\n",
      "[Epoch: 7380] train loss: 1.1397, train acc: 0.5249, val loss: 1.1384, val acc: 0.5292  (best train acc: 0.5380, best val acc: 0.5356)\n",
      "[Epoch: 7400] train loss: 1.1447, train acc: 0.5215, val loss: 1.1413, val acc: 0.5282  (best train acc: 0.5380, best val acc: 0.5356)\n",
      "[Epoch: 7420] train loss: 1.1504, train acc: 0.5208, val loss: 1.1420, val acc: 0.5228  (best train acc: 0.5380, best val acc: 0.5356)\n",
      "[Epoch: 7440] train loss: 1.1395, train acc: 0.5300, val loss: 1.1391, val acc: 0.5218  (best train acc: 0.5380, best val acc: 0.5356)\n",
      "[Epoch: 7460] train loss: 1.1303, train acc: 0.5278, val loss: 1.1396, val acc: 0.5282  (best train acc: 0.5380, best val acc: 0.5356)\n",
      "[Epoch: 7480] train loss: 1.1388, train acc: 0.5271, val loss: 1.1364, val acc: 0.5309  (best train acc: 0.5380, best val acc: 0.5356)\n",
      "[Epoch: 7500] train loss: 1.1395, train acc: 0.5278, val loss: 1.1403, val acc: 0.5191  (best train acc: 0.5392, best val acc: 0.5356)\n",
      "[Epoch: 7520] train loss: 1.1368, train acc: 0.5218, val loss: 1.1373, val acc: 0.5234  (best train acc: 0.5392, best val acc: 0.5356)\n",
      "[Epoch: 7540] train loss: 1.1402, train acc: 0.5330, val loss: 1.1375, val acc: 0.5298  (best train acc: 0.5392, best val acc: 0.5356)\n",
      "[Epoch: 7560] train loss: 1.1515, train acc: 0.5195, val loss: 1.1386, val acc: 0.5265  (best train acc: 0.5392, best val acc: 0.5356)\n",
      "[Epoch: 7580] train loss: 1.1333, train acc: 0.5298, val loss: 1.1366, val acc: 0.5319  (best train acc: 0.5392, best val acc: 0.5356)\n",
      "[Epoch: 7600] train loss: 1.1363, train acc: 0.5288, val loss: 1.1373, val acc: 0.5339  (best train acc: 0.5392, best val acc: 0.5356)\n",
      "[Epoch: 7620] train loss: 1.1356, train acc: 0.5225, val loss: 1.1350, val acc: 0.5309  (best train acc: 0.5392, best val acc: 0.5356)\n",
      "[Epoch: 7640] train loss: 1.1355, train acc: 0.5160, val loss: 1.1424, val acc: 0.5241  (best train acc: 0.5392, best val acc: 0.5356)\n",
      "[Epoch: 7660] train loss: 1.1460, train acc: 0.5229, val loss: 1.1353, val acc: 0.5248  (best train acc: 0.5392, best val acc: 0.5356)\n",
      "[Epoch: 7680] train loss: 1.1387, train acc: 0.5307, val loss: 1.1358, val acc: 0.5295  (best train acc: 0.5392, best val acc: 0.5359)\n",
      "[Epoch: 7700] train loss: 1.1418, train acc: 0.5185, val loss: 1.1331, val acc: 0.5336  (best train acc: 0.5392, best val acc: 0.5359)\n",
      "[Epoch: 7720] train loss: 1.1348, train acc: 0.5291, val loss: 1.1400, val acc: 0.5174  (best train acc: 0.5392, best val acc: 0.5359)\n",
      "[Epoch: 7740] train loss: 1.1286, train acc: 0.5306, val loss: 1.1339, val acc: 0.5272  (best train acc: 0.5392, best val acc: 0.5366)\n",
      "[Epoch: 7760] train loss: 1.1426, train acc: 0.5226, val loss: 1.1400, val acc: 0.5312  (best train acc: 0.5392, best val acc: 0.5366)\n",
      "[Epoch: 7780] train loss: 1.1376, train acc: 0.5267, val loss: 1.1346, val acc: 0.5339  (best train acc: 0.5392, best val acc: 0.5366)\n",
      "[Epoch: 7800] train loss: 1.1318, train acc: 0.5250, val loss: 1.1332, val acc: 0.5292  (best train acc: 0.5392, best val acc: 0.5366)\n",
      "[Epoch: 7820] train loss: 1.1319, train acc: 0.5324, val loss: 1.1332, val acc: 0.5255  (best train acc: 0.5392, best val acc: 0.5366)\n",
      "[Epoch: 7840] train loss: 1.1329, train acc: 0.5265, val loss: 1.1422, val acc: 0.5184  (best train acc: 0.5392, best val acc: 0.5366)\n",
      "[Epoch: 7860] train loss: 1.1345, train acc: 0.5356, val loss: 1.1353, val acc: 0.5278  (best train acc: 0.5392, best val acc: 0.5366)\n",
      "[Epoch: 7880] train loss: 1.1488, train acc: 0.5170, val loss: 1.1350, val acc: 0.5295  (best train acc: 0.5392, best val acc: 0.5366)\n",
      "[Epoch: 7900] train loss: 1.1457, train acc: 0.5183, val loss: 1.1336, val acc: 0.5298  (best train acc: 0.5392, best val acc: 0.5366)\n",
      "[Epoch: 7920] train loss: 1.1373, train acc: 0.5263, val loss: 1.1331, val acc: 0.5363  (best train acc: 0.5392, best val acc: 0.5366)\n",
      "[Epoch: 7940] train loss: 1.1387, train acc: 0.5255, val loss: 1.1348, val acc: 0.5336  (best train acc: 0.5392, best val acc: 0.5366)\n",
      "[Epoch: 7960] train loss: 1.1339, train acc: 0.5241, val loss: 1.1321, val acc: 0.5352  (best train acc: 0.5392, best val acc: 0.5366)\n",
      "[Epoch: 7980] train loss: 1.1449, train acc: 0.5184, val loss: 1.1375, val acc: 0.5224  (best train acc: 0.5392, best val acc: 0.5366)\n",
      "[Epoch: 8000] train loss: 1.1309, train acc: 0.5273, val loss: 1.1331, val acc: 0.5265  (best train acc: 0.5392, best val acc: 0.5366)\n",
      "[Epoch: 8020] train loss: 1.1312, train acc: 0.5299, val loss: 1.1344, val acc: 0.5234  (best train acc: 0.5392, best val acc: 0.5366)\n",
      "[Epoch: 8040] train loss: 1.1388, train acc: 0.5247, val loss: 1.1339, val acc: 0.5342  (best train acc: 0.5392, best val acc: 0.5366)\n",
      "[Epoch: 8060] train loss: 1.1340, train acc: 0.5270, val loss: 1.1321, val acc: 0.5265  (best train acc: 0.5392, best val acc: 0.5366)\n",
      "[Epoch: 8080] train loss: 1.1314, train acc: 0.5275, val loss: 1.1300, val acc: 0.5315  (best train acc: 0.5392, best val acc: 0.5366)\n",
      "[Epoch: 8100] train loss: 1.1281, train acc: 0.5308, val loss: 1.1307, val acc: 0.5315  (best train acc: 0.5392, best val acc: 0.5366)\n",
      "[Epoch: 8120] train loss: 1.1298, train acc: 0.5298, val loss: 1.1303, val acc: 0.5302  (best train acc: 0.5392, best val acc: 0.5366)\n",
      "[Epoch: 8140] train loss: 1.1308, train acc: 0.5308, val loss: 1.1351, val acc: 0.5211  (best train acc: 0.5392, best val acc: 0.5366)\n",
      "[Epoch: 8160] train loss: 1.1304, train acc: 0.5349, val loss: 1.1297, val acc: 0.5319  (best train acc: 0.5403, best val acc: 0.5366)\n",
      "[Epoch: 8180] train loss: 1.1265, train acc: 0.5290, val loss: 1.1273, val acc: 0.5346  (best train acc: 0.5403, best val acc: 0.5369)\n",
      "[Epoch: 8200] train loss: 1.1288, train acc: 0.5294, val loss: 1.1288, val acc: 0.5288  (best train acc: 0.5403, best val acc: 0.5369)\n",
      "[Epoch: 8220] train loss: 1.1370, train acc: 0.5253, val loss: 1.1298, val acc: 0.5336  (best train acc: 0.5403, best val acc: 0.5376)\n",
      "[Epoch: 8240] train loss: 1.1272, train acc: 0.5298, val loss: 1.1283, val acc: 0.5363  (best train acc: 0.5403, best val acc: 0.5376)\n",
      "[Epoch: 8260] train loss: 1.1324, train acc: 0.5290, val loss: 1.1301, val acc: 0.5322  (best train acc: 0.5403, best val acc: 0.5376)\n",
      "[Epoch: 8280] train loss: 1.1255, train acc: 0.5304, val loss: 1.1281, val acc: 0.5275  (best train acc: 0.5403, best val acc: 0.5376)\n",
      "[Epoch: 8300] train loss: 1.1324, train acc: 0.5267, val loss: 1.1279, val acc: 0.5295  (best train acc: 0.5403, best val acc: 0.5376)\n",
      "[Epoch: 8320] train loss: 1.1299, train acc: 0.5316, val loss: 1.1266, val acc: 0.5339  (best train acc: 0.5403, best val acc: 0.5376)\n",
      "[Epoch: 8340] train loss: 1.1341, train acc: 0.5270, val loss: 1.1291, val acc: 0.5282  (best train acc: 0.5403, best val acc: 0.5379)\n",
      "[Epoch: 8360] train loss: 1.1304, train acc: 0.5323, val loss: 1.1277, val acc: 0.5315  (best train acc: 0.5403, best val acc: 0.5379)\n",
      "[Epoch: 8380] train loss: 1.1304, train acc: 0.5277, val loss: 1.1297, val acc: 0.5221  (best train acc: 0.5403, best val acc: 0.5379)\n",
      "[Epoch: 8400] train loss: 1.1308, train acc: 0.5278, val loss: 1.1256, val acc: 0.5315  (best train acc: 0.5403, best val acc: 0.5379)\n",
      "[Epoch: 8420] train loss: 1.1275, train acc: 0.5312, val loss: 1.1323, val acc: 0.5298  (best train acc: 0.5403, best val acc: 0.5379)\n",
      "[Epoch: 8440] train loss: 1.1244, train acc: 0.5307, val loss: 1.1271, val acc: 0.5315  (best train acc: 0.5403, best val acc: 0.5379)\n",
      "[Epoch: 8460] train loss: 1.1376, train acc: 0.5259, val loss: 1.1244, val acc: 0.5342  (best train acc: 0.5403, best val acc: 0.5379)\n",
      "[Epoch: 8480] train loss: 1.1261, train acc: 0.5314, val loss: 1.1261, val acc: 0.5309  (best train acc: 0.5403, best val acc: 0.5379)\n",
      "[Epoch: 8500] train loss: 1.1305, train acc: 0.5309, val loss: 1.1243, val acc: 0.5295  (best train acc: 0.5403, best val acc: 0.5379)\n",
      "[Epoch: 8520] train loss: 1.1252, train acc: 0.5328, val loss: 1.1250, val acc: 0.5329  (best train acc: 0.5403, best val acc: 0.5379)\n",
      "[Epoch: 8540] train loss: 1.1343, train acc: 0.5249, val loss: 1.1235, val acc: 0.5366  (best train acc: 0.5403, best val acc: 0.5379)\n",
      "[Epoch: 8560] train loss: 1.1301, train acc: 0.5251, val loss: 1.1309, val acc: 0.5164  (best train acc: 0.5403, best val acc: 0.5379)\n",
      "[Epoch: 8580] train loss: 1.1174, train acc: 0.5329, val loss: 1.1321, val acc: 0.5177  (best train acc: 0.5403, best val acc: 0.5379)\n",
      "[Epoch: 8600] train loss: 1.1304, train acc: 0.5293, val loss: 1.1266, val acc: 0.5366  (best train acc: 0.5403, best val acc: 0.5379)\n",
      "[Epoch: 8620] train loss: 1.1217, train acc: 0.5330, val loss: 1.1290, val acc: 0.5346  (best train acc: 0.5403, best val acc: 0.5379)\n",
      "[Epoch: 8640] train loss: 1.1257, train acc: 0.5337, val loss: 1.1233, val acc: 0.5319  (best train acc: 0.5403, best val acc: 0.5379)\n",
      "[Epoch: 8660] train loss: 1.1205, train acc: 0.5388, val loss: 1.1272, val acc: 0.5342  (best train acc: 0.5403, best val acc: 0.5390)\n",
      "[Epoch: 8680] train loss: 1.1231, train acc: 0.5374, val loss: 1.1255, val acc: 0.5332  (best train acc: 0.5403, best val acc: 0.5393)\n",
      "[Epoch: 8700] train loss: 1.1162, train acc: 0.5370, val loss: 1.1235, val acc: 0.5265  (best train acc: 0.5403, best val acc: 0.5396)\n",
      "[Epoch: 8720] train loss: 1.1168, train acc: 0.5358, val loss: 1.1213, val acc: 0.5352  (best train acc: 0.5403, best val acc: 0.5396)\n",
      "[Epoch: 8740] train loss: 1.1199, train acc: 0.5348, val loss: 1.1224, val acc: 0.5339  (best train acc: 0.5403, best val acc: 0.5396)\n",
      "[Epoch: 8760] train loss: 1.1243, train acc: 0.5282, val loss: 1.1250, val acc: 0.5319  (best train acc: 0.5403, best val acc: 0.5396)\n",
      "[Epoch: 8780] train loss: 1.1340, train acc: 0.5234, val loss: 1.1247, val acc: 0.5319  (best train acc: 0.5403, best val acc: 0.5403)\n",
      "[Epoch: 8800] train loss: 1.1274, train acc: 0.5313, val loss: 1.1241, val acc: 0.5245  (best train acc: 0.5405, best val acc: 0.5403)\n",
      "[Epoch: 8820] train loss: 1.1210, train acc: 0.5314, val loss: 1.1320, val acc: 0.5231  (best train acc: 0.5405, best val acc: 0.5403)\n",
      "[Epoch: 8840] train loss: 1.1208, train acc: 0.5307, val loss: 1.1206, val acc: 0.5336  (best train acc: 0.5405, best val acc: 0.5403)\n",
      "[Epoch: 8860] train loss: 1.1382, train acc: 0.5278, val loss: 1.1282, val acc: 0.5218  (best train acc: 0.5405, best val acc: 0.5403)\n",
      "[Epoch: 8880] train loss: 1.1331, train acc: 0.5275, val loss: 1.1243, val acc: 0.5298  (best train acc: 0.5405, best val acc: 0.5403)\n",
      "[Epoch: 8900] train loss: 1.1344, train acc: 0.5312, val loss: 1.1230, val acc: 0.5325  (best train acc: 0.5409, best val acc: 0.5403)\n",
      "[Epoch: 8920] train loss: 1.1200, train acc: 0.5356, val loss: 1.1201, val acc: 0.5332  (best train acc: 0.5409, best val acc: 0.5403)\n",
      "[Epoch: 8940] train loss: 1.1306, train acc: 0.5256, val loss: 1.1212, val acc: 0.5295  (best train acc: 0.5409, best val acc: 0.5403)\n",
      "[Epoch: 8960] train loss: 1.1374, train acc: 0.5221, val loss: 1.1193, val acc: 0.5369  (best train acc: 0.5409, best val acc: 0.5403)\n",
      "[Epoch: 8980] train loss: 1.1324, train acc: 0.5299, val loss: 1.1204, val acc: 0.5359  (best train acc: 0.5409, best val acc: 0.5403)\n",
      "[Epoch: 9000] train loss: 1.1305, train acc: 0.5259, val loss: 1.1194, val acc: 0.5342  (best train acc: 0.5409, best val acc: 0.5406)\n",
      "[Epoch: 9020] train loss: 1.1280, train acc: 0.5325, val loss: 1.1187, val acc: 0.5322  (best train acc: 0.5409, best val acc: 0.5406)\n",
      "[Epoch: 9040] train loss: 1.1291, train acc: 0.5265, val loss: 1.1185, val acc: 0.5383  (best train acc: 0.5409, best val acc: 0.5406)\n",
      "[Epoch: 9060] train loss: 1.1278, train acc: 0.5299, val loss: 1.1261, val acc: 0.5322  (best train acc: 0.5409, best val acc: 0.5406)\n",
      "[Epoch: 9080] train loss: 1.1339, train acc: 0.5267, val loss: 1.1269, val acc: 0.5285  (best train acc: 0.5409, best val acc: 0.5406)\n",
      "[Epoch: 9100] train loss: 1.1304, train acc: 0.5262, val loss: 1.1231, val acc: 0.5228  (best train acc: 0.5409, best val acc: 0.5406)\n",
      "[Epoch: 9120] train loss: 1.1327, train acc: 0.5275, val loss: 1.1212, val acc: 0.5305  (best train acc: 0.5409, best val acc: 0.5406)\n",
      "[Epoch: 9140] train loss: 1.1220, train acc: 0.5332, val loss: 1.1194, val acc: 0.5325  (best train acc: 0.5412, best val acc: 0.5406)\n",
      "[Epoch: 9160] train loss: 1.1334, train acc: 0.5296, val loss: 1.1233, val acc: 0.5295  (best train acc: 0.5412, best val acc: 0.5406)\n",
      "[Epoch: 9180] train loss: 1.1176, train acc: 0.5356, val loss: 1.1168, val acc: 0.5325  (best train acc: 0.5412, best val acc: 0.5406)\n",
      "[Epoch: 9200] train loss: 1.1247, train acc: 0.5351, val loss: 1.1211, val acc: 0.5298  (best train acc: 0.5412, best val acc: 0.5406)\n",
      "[Epoch: 9220] train loss: 1.1272, train acc: 0.5275, val loss: 1.1171, val acc: 0.5342  (best train acc: 0.5412, best val acc: 0.5406)\n",
      "[Epoch: 9240] train loss: 1.1182, train acc: 0.5314, val loss: 1.1160, val acc: 0.5352  (best train acc: 0.5412, best val acc: 0.5406)\n",
      "[Epoch: 9260] train loss: 1.1190, train acc: 0.5322, val loss: 1.1167, val acc: 0.5295  (best train acc: 0.5412, best val acc: 0.5406)\n",
      "[Epoch: 9280] train loss: 1.1110, train acc: 0.5399, val loss: 1.1186, val acc: 0.5336  (best train acc: 0.5412, best val acc: 0.5406)\n",
      "[Epoch: 9300] train loss: 1.1213, train acc: 0.5317, val loss: 1.1162, val acc: 0.5373  (best train acc: 0.5412, best val acc: 0.5406)\n",
      "[Epoch: 9320] train loss: 1.1352, train acc: 0.5231, val loss: 1.1247, val acc: 0.5160  (best train acc: 0.5412, best val acc: 0.5406)\n",
      "[Epoch: 9340] train loss: 1.1241, train acc: 0.5302, val loss: 1.1165, val acc: 0.5366  (best train acc: 0.5412, best val acc: 0.5406)\n",
      "[Epoch: 9360] train loss: 1.1169, train acc: 0.5328, val loss: 1.1194, val acc: 0.5352  (best train acc: 0.5412, best val acc: 0.5406)\n",
      "[Epoch: 9380] train loss: 1.1213, train acc: 0.5316, val loss: 1.1200, val acc: 0.5295  (best train acc: 0.5412, best val acc: 0.5406)\n",
      "[Epoch: 9400] train loss: 1.1173, train acc: 0.5362, val loss: 1.1145, val acc: 0.5322  (best train acc: 0.5412, best val acc: 0.5406)\n",
      "[Epoch: 9420] train loss: 1.1214, train acc: 0.5287, val loss: 1.1172, val acc: 0.5309  (best train acc: 0.5412, best val acc: 0.5406)\n",
      "[Epoch: 9440] train loss: 1.1260, train acc: 0.5267, val loss: 1.1190, val acc: 0.5312  (best train acc: 0.5412, best val acc: 0.5406)\n",
      "[Epoch: 9460] train loss: 1.1188, train acc: 0.5371, val loss: 1.1133, val acc: 0.5363  (best train acc: 0.5412, best val acc: 0.5410)\n",
      "[Epoch: 9480] train loss: 1.1157, train acc: 0.5344, val loss: 1.1198, val acc: 0.5339  (best train acc: 0.5412, best val acc: 0.5410)\n",
      "[Epoch: 9500] train loss: 1.1124, train acc: 0.5335, val loss: 1.1149, val acc: 0.5319  (best train acc: 0.5412, best val acc: 0.5410)\n",
      "[Epoch: 9520] train loss: 1.1064, train acc: 0.5399, val loss: 1.1188, val acc: 0.5261  (best train acc: 0.5412, best val acc: 0.5410)\n",
      "[Epoch: 9540] train loss: 1.1237, train acc: 0.5315, val loss: 1.1145, val acc: 0.5363  (best train acc: 0.5412, best val acc: 0.5410)\n",
      "[Epoch: 9560] train loss: 1.1201, train acc: 0.5322, val loss: 1.1135, val acc: 0.5366  (best train acc: 0.5412, best val acc: 0.5410)\n",
      "[Epoch: 9580] train loss: 1.1247, train acc: 0.5261, val loss: 1.1147, val acc: 0.5285  (best train acc: 0.5412, best val acc: 0.5413)\n",
      "[Epoch: 9600] train loss: 1.1270, train acc: 0.5239, val loss: 1.1169, val acc: 0.5211  (best train acc: 0.5414, best val acc: 0.5413)\n",
      "[Epoch: 9620] train loss: 1.1202, train acc: 0.5313, val loss: 1.1120, val acc: 0.5336  (best train acc: 0.5414, best val acc: 0.5413)\n",
      "[Epoch: 9640] train loss: 1.1255, train acc: 0.5335, val loss: 1.1137, val acc: 0.5366  (best train acc: 0.5414, best val acc: 0.5413)\n",
      "[Epoch: 9660] train loss: 1.1138, train acc: 0.5397, val loss: 1.1166, val acc: 0.5356  (best train acc: 0.5414, best val acc: 0.5413)\n",
      "[Epoch: 9680] train loss: 1.1238, train acc: 0.5290, val loss: 1.1129, val acc: 0.5329  (best train acc: 0.5421, best val acc: 0.5413)\n",
      "[Epoch: 9700] train loss: 1.1159, train acc: 0.5318, val loss: 1.1144, val acc: 0.5282  (best train acc: 0.5421, best val acc: 0.5413)\n",
      "[Epoch: 9720] train loss: 1.1227, train acc: 0.5313, val loss: 1.1139, val acc: 0.5339  (best train acc: 0.5421, best val acc: 0.5413)\n",
      "[Epoch: 9740] train loss: 1.1056, train acc: 0.5385, val loss: 1.1120, val acc: 0.5302  (best train acc: 0.5424, best val acc: 0.5413)\n",
      "[Epoch: 9760] train loss: 1.1125, train acc: 0.5341, val loss: 1.1109, val acc: 0.5366  (best train acc: 0.5424, best val acc: 0.5413)\n",
      "[Epoch: 9780] train loss: 1.1090, train acc: 0.5341, val loss: 1.1154, val acc: 0.5295  (best train acc: 0.5440, best val acc: 0.5413)\n",
      "[Epoch: 9800] train loss: 1.1277, train acc: 0.5289, val loss: 1.1116, val acc: 0.5373  (best train acc: 0.5440, best val acc: 0.5413)\n",
      "[Epoch: 9820] train loss: 1.1325, train acc: 0.5205, val loss: 1.1274, val acc: 0.5143  (best train acc: 0.5440, best val acc: 0.5413)\n",
      "[Epoch: 9840] train loss: 1.1216, train acc: 0.5301, val loss: 1.1150, val acc: 0.5359  (best train acc: 0.5440, best val acc: 0.5413)\n",
      "[Epoch: 9860] train loss: 1.1124, train acc: 0.5382, val loss: 1.1130, val acc: 0.5349  (best train acc: 0.5440, best val acc: 0.5413)\n",
      "[Epoch: 9880] train loss: 1.1125, train acc: 0.5389, val loss: 1.1109, val acc: 0.5359  (best train acc: 0.5440, best val acc: 0.5413)\n",
      "[Epoch: 9900] train loss: 1.1189, train acc: 0.5322, val loss: 1.1109, val acc: 0.5386  (best train acc: 0.5440, best val acc: 0.5413)\n",
      "[Epoch: 9920] train loss: 1.1128, train acc: 0.5354, val loss: 1.1119, val acc: 0.5346  (best train acc: 0.5440, best val acc: 0.5427)\n",
      "[Epoch: 9940] train loss: 1.1086, train acc: 0.5374, val loss: 1.1137, val acc: 0.5346  (best train acc: 0.5440, best val acc: 0.5427)\n",
      "[Epoch: 9960] train loss: 1.1071, train acc: 0.5374, val loss: 1.1094, val acc: 0.5369  (best train acc: 0.5440, best val acc: 0.5427)\n",
      "[Epoch: 9980] train loss: 1.1153, train acc: 0.5314, val loss: 1.1087, val acc: 0.5379  (best train acc: 0.5440, best val acc: 0.5427)\n",
      "[Epoch: 10000] train loss: 1.1059, train acc: 0.5382, val loss: 1.1099, val acc: 0.5356  (best train acc: 0.5440, best val acc: 0.5427)\n",
      "[Epoch: 10020] train loss: 1.1143, train acc: 0.5305, val loss: 1.1086, val acc: 0.5403  (best train acc: 0.5440, best val acc: 0.5433)\n",
      "[Epoch: 10040] train loss: 1.1206, train acc: 0.5330, val loss: 1.1087, val acc: 0.5309  (best train acc: 0.5440, best val acc: 0.5433)\n",
      "[Epoch: 10060] train loss: 1.1146, train acc: 0.5316, val loss: 1.1072, val acc: 0.5356  (best train acc: 0.5440, best val acc: 0.5433)\n",
      "[Epoch: 10080] train loss: 1.1129, train acc: 0.5329, val loss: 1.1085, val acc: 0.5339  (best train acc: 0.5440, best val acc: 0.5433)\n",
      "[Epoch: 10100] train loss: 1.1150, train acc: 0.5387, val loss: 1.1084, val acc: 0.5396  (best train acc: 0.5440, best val acc: 0.5433)\n",
      "[Epoch: 10120] train loss: 1.1119, train acc: 0.5352, val loss: 1.1093, val acc: 0.5352  (best train acc: 0.5440, best val acc: 0.5433)\n",
      "[Epoch: 10140] train loss: 1.1273, train acc: 0.5264, val loss: 1.1084, val acc: 0.5342  (best train acc: 0.5440, best val acc: 0.5433)\n",
      "[Epoch: 10160] train loss: 1.1120, train acc: 0.5369, val loss: 1.1163, val acc: 0.5359  (best train acc: 0.5453, best val acc: 0.5433)\n",
      "[Epoch: 10180] train loss: 1.1123, train acc: 0.5376, val loss: 1.1093, val acc: 0.5322  (best train acc: 0.5453, best val acc: 0.5433)\n",
      "[Epoch: 10200] train loss: 1.1022, train acc: 0.5391, val loss: 1.1071, val acc: 0.5319  (best train acc: 0.5473, best val acc: 0.5433)\n",
      "[Epoch: 10220] train loss: 1.1084, train acc: 0.5439, val loss: 1.1069, val acc: 0.5346  (best train acc: 0.5473, best val acc: 0.5433)\n",
      "[Epoch: 10240] train loss: 1.1026, train acc: 0.5440, val loss: 1.1088, val acc: 0.5255  (best train acc: 0.5473, best val acc: 0.5433)\n",
      "[Epoch: 10260] train loss: 1.1083, train acc: 0.5335, val loss: 1.1072, val acc: 0.5346  (best train acc: 0.5473, best val acc: 0.5433)\n",
      "[Epoch: 10280] train loss: 1.1171, train acc: 0.5337, val loss: 1.1098, val acc: 0.5272  (best train acc: 0.5473, best val acc: 0.5433)\n",
      "[Epoch: 10300] train loss: 1.1190, train acc: 0.5288, val loss: 1.1057, val acc: 0.5349  (best train acc: 0.5473, best val acc: 0.5433)\n",
      "[Epoch: 10320] train loss: 1.1076, train acc: 0.5388, val loss: 1.1078, val acc: 0.5309  (best train acc: 0.5473, best val acc: 0.5433)\n",
      "[Epoch: 10340] train loss: 1.1132, train acc: 0.5342, val loss: 1.1075, val acc: 0.5363  (best train acc: 0.5473, best val acc: 0.5433)\n",
      "[Epoch: 10360] train loss: 1.1257, train acc: 0.5255, val loss: 1.1054, val acc: 0.5383  (best train acc: 0.5473, best val acc: 0.5433)\n",
      "[Epoch: 10380] train loss: 1.0997, train acc: 0.5418, val loss: 1.1058, val acc: 0.5369  (best train acc: 0.5473, best val acc: 0.5433)\n",
      "[Epoch: 10400] train loss: 1.1149, train acc: 0.5310, val loss: 1.1114, val acc: 0.5288  (best train acc: 0.5473, best val acc: 0.5433)\n",
      "[Epoch: 10420] train loss: 1.1234, train acc: 0.5269, val loss: 1.1035, val acc: 0.5376  (best train acc: 0.5473, best val acc: 0.5433)\n",
      "[Epoch: 10440] train loss: 1.0972, train acc: 0.5429, val loss: 1.1089, val acc: 0.5379  (best train acc: 0.5473, best val acc: 0.5433)\n",
      "[Epoch: 10460] train loss: 1.1126, train acc: 0.5371, val loss: 1.1046, val acc: 0.5383  (best train acc: 0.5473, best val acc: 0.5433)\n",
      "[Epoch: 10480] train loss: 1.1110, train acc: 0.5356, val loss: 1.1046, val acc: 0.5413  (best train acc: 0.5473, best val acc: 0.5433)\n",
      "[Epoch: 10500] train loss: 1.1055, train acc: 0.5375, val loss: 1.1055, val acc: 0.5352  (best train acc: 0.5473, best val acc: 0.5433)\n",
      "[Epoch: 10520] train loss: 1.1153, train acc: 0.5334, val loss: 1.1059, val acc: 0.5376  (best train acc: 0.5473, best val acc: 0.5433)\n",
      "[Epoch: 10540] train loss: 1.1167, train acc: 0.5263, val loss: 1.1090, val acc: 0.5342  (best train acc: 0.5473, best val acc: 0.5433)\n",
      "[Epoch: 10560] train loss: 1.1180, train acc: 0.5357, val loss: 1.1050, val acc: 0.5305  (best train acc: 0.5473, best val acc: 0.5433)\n",
      "[Epoch: 10580] train loss: 1.1157, train acc: 0.5283, val loss: 1.1086, val acc: 0.5268  (best train acc: 0.5473, best val acc: 0.5433)\n",
      "[Epoch: 10600] train loss: 1.0993, train acc: 0.5372, val loss: 1.1048, val acc: 0.5329  (best train acc: 0.5475, best val acc: 0.5433)\n",
      "[Epoch: 10620] train loss: 1.1203, train acc: 0.5273, val loss: 1.1066, val acc: 0.5255  (best train acc: 0.5475, best val acc: 0.5433)\n",
      "[Epoch: 10640] train loss: 1.1130, train acc: 0.5397, val loss: 1.1055, val acc: 0.5417  (best train acc: 0.5475, best val acc: 0.5433)\n",
      "[Epoch: 10660] train loss: 1.0971, train acc: 0.5420, val loss: 1.1033, val acc: 0.5329  (best train acc: 0.5475, best val acc: 0.5433)\n",
      "[Epoch: 10680] train loss: 1.1061, train acc: 0.5393, val loss: 1.1017, val acc: 0.5376  (best train acc: 0.5475, best val acc: 0.5433)\n",
      "[Epoch: 10700] train loss: 1.1044, train acc: 0.5354, val loss: 1.1032, val acc: 0.5403  (best train acc: 0.5475, best val acc: 0.5433)\n",
      "[Epoch: 10720] train loss: 1.1122, train acc: 0.5377, val loss: 1.1034, val acc: 0.5413  (best train acc: 0.5475, best val acc: 0.5444)\n",
      "[Epoch: 10740] train loss: 1.1130, train acc: 0.5364, val loss: 1.1019, val acc: 0.5420  (best train acc: 0.5475, best val acc: 0.5444)\n",
      "[Epoch: 10760] train loss: 1.1167, train acc: 0.5348, val loss: 1.1009, val acc: 0.5379  (best train acc: 0.5475, best val acc: 0.5450)\n",
      "[Epoch: 10780] train loss: 1.1034, train acc: 0.5400, val loss: 1.1038, val acc: 0.5410  (best train acc: 0.5475, best val acc: 0.5450)\n",
      "[Epoch: 10800] train loss: 1.1108, train acc: 0.5320, val loss: 1.1009, val acc: 0.5339  (best train acc: 0.5475, best val acc: 0.5450)\n",
      "[Epoch: 10820] train loss: 1.1107, train acc: 0.5410, val loss: 1.1062, val acc: 0.5332  (best train acc: 0.5475, best val acc: 0.5450)\n",
      "[Epoch: 10840] train loss: 1.1045, train acc: 0.5415, val loss: 1.1043, val acc: 0.5302  (best train acc: 0.5475, best val acc: 0.5450)\n",
      "[Epoch: 10860] train loss: 1.1157, train acc: 0.5320, val loss: 1.1008, val acc: 0.5359  (best train acc: 0.5475, best val acc: 0.5450)\n",
      "[Epoch: 10880] train loss: 1.1157, train acc: 0.5317, val loss: 1.1019, val acc: 0.5396  (best train acc: 0.5475, best val acc: 0.5450)\n",
      "[Epoch: 10900] train loss: 1.1201, train acc: 0.5293, val loss: 1.1055, val acc: 0.5359  (best train acc: 0.5475, best val acc: 0.5450)\n",
      "[Epoch: 10920] train loss: 1.1024, train acc: 0.5393, val loss: 1.1021, val acc: 0.5423  (best train acc: 0.5475, best val acc: 0.5450)\n",
      "[Epoch: 10940] train loss: 1.1007, train acc: 0.5456, val loss: 1.1037, val acc: 0.5325  (best train acc: 0.5475, best val acc: 0.5450)\n",
      "[Epoch: 10960] train loss: 1.0947, train acc: 0.5397, val loss: 1.1009, val acc: 0.5427  (best train acc: 0.5475, best val acc: 0.5450)\n",
      "[Epoch: 10980] train loss: 1.1045, train acc: 0.5358, val loss: 1.0998, val acc: 0.5437  (best train acc: 0.5475, best val acc: 0.5450)\n",
      "[Epoch: 11000] train loss: 1.0961, train acc: 0.5408, val loss: 1.1000, val acc: 0.5417  (best train acc: 0.5475, best val acc: 0.5450)\n",
      "[Epoch: 11020] train loss: 1.1010, train acc: 0.5367, val loss: 1.1005, val acc: 0.5420  (best train acc: 0.5475, best val acc: 0.5450)\n",
      "[Epoch: 11040] train loss: 1.1234, train acc: 0.5314, val loss: 1.1043, val acc: 0.5312  (best train acc: 0.5475, best val acc: 0.5450)\n",
      "[Epoch: 11060] train loss: 1.1094, train acc: 0.5338, val loss: 1.1036, val acc: 0.5268  (best train acc: 0.5475, best val acc: 0.5450)\n",
      "[Epoch: 11080] train loss: 1.1174, train acc: 0.5314, val loss: 1.1001, val acc: 0.5437  (best train acc: 0.5475, best val acc: 0.5457)\n",
      "[Epoch: 11100] train loss: 1.1077, train acc: 0.5387, val loss: 1.1053, val acc: 0.5292  (best train acc: 0.5475, best val acc: 0.5457)\n",
      "[Epoch: 11120] train loss: 1.1088, train acc: 0.5346, val loss: 1.0991, val acc: 0.5406  (best train acc: 0.5475, best val acc: 0.5467)\n",
      "[Epoch: 11140] train loss: 1.1074, train acc: 0.5346, val loss: 1.1008, val acc: 0.5440  (best train acc: 0.5475, best val acc: 0.5467)\n",
      "[Epoch: 11160] train loss: 1.1048, train acc: 0.5369, val loss: 1.0995, val acc: 0.5464  (best train acc: 0.5475, best val acc: 0.5467)\n",
      "[Epoch: 11180] train loss: 1.0960, train acc: 0.5344, val loss: 1.0987, val acc: 0.5393  (best train acc: 0.5475, best val acc: 0.5467)\n",
      "[Epoch: 11200] train loss: 1.1013, train acc: 0.5377, val loss: 1.0992, val acc: 0.5423  (best train acc: 0.5475, best val acc: 0.5467)\n",
      "[Epoch: 11220] train loss: 1.0960, train acc: 0.5414, val loss: 1.0982, val acc: 0.5403  (best train acc: 0.5481, best val acc: 0.5467)\n",
      "[Epoch: 11240] train loss: 1.1035, train acc: 0.5337, val loss: 1.0996, val acc: 0.5369  (best train acc: 0.5481, best val acc: 0.5467)\n",
      "[Epoch: 11260] train loss: 1.1062, train acc: 0.5346, val loss: 1.1022, val acc: 0.5332  (best train acc: 0.5481, best val acc: 0.5467)\n",
      "[Epoch: 11280] train loss: 1.1058, train acc: 0.5364, val loss: 1.0986, val acc: 0.5390  (best train acc: 0.5481, best val acc: 0.5467)\n",
      "[Epoch: 11300] train loss: 1.1014, train acc: 0.5367, val loss: 1.0985, val acc: 0.5400  (best train acc: 0.5481, best val acc: 0.5467)\n",
      "[Epoch: 11320] train loss: 1.0965, train acc: 0.5427, val loss: 1.0987, val acc: 0.5400  (best train acc: 0.5481, best val acc: 0.5467)\n",
      "[Epoch: 11340] train loss: 1.0989, train acc: 0.5424, val loss: 1.0975, val acc: 0.5427  (best train acc: 0.5481, best val acc: 0.5467)\n",
      "[Epoch: 11360] train loss: 1.0960, train acc: 0.5426, val loss: 1.1018, val acc: 0.5410  (best train acc: 0.5481, best val acc: 0.5467)\n",
      "[Epoch: 11380] train loss: 1.1079, train acc: 0.5377, val loss: 1.1007, val acc: 0.5420  (best train acc: 0.5481, best val acc: 0.5467)\n",
      "[Epoch: 11400] train loss: 1.1086, train acc: 0.5309, val loss: 1.0982, val acc: 0.5427  (best train acc: 0.5481, best val acc: 0.5474)\n",
      "[Epoch: 11420] train loss: 1.1047, train acc: 0.5356, val loss: 1.0963, val acc: 0.5423  (best train acc: 0.5481, best val acc: 0.5474)\n",
      "[Epoch: 11440] train loss: 1.1032, train acc: 0.5364, val loss: 1.0985, val acc: 0.5410  (best train acc: 0.5481, best val acc: 0.5474)\n",
      "[Epoch: 11460] train loss: 1.1008, train acc: 0.5378, val loss: 1.0971, val acc: 0.5433  (best train acc: 0.5481, best val acc: 0.5474)\n",
      "[Epoch: 11480] train loss: 1.1030, train acc: 0.5374, val loss: 1.0963, val acc: 0.5433  (best train acc: 0.5481, best val acc: 0.5474)\n",
      "[Epoch: 11500] train loss: 1.1075, train acc: 0.5251, val loss: 1.0953, val acc: 0.5423  (best train acc: 0.5481, best val acc: 0.5474)\n",
      "[Epoch: 11520] train loss: 1.1203, train acc: 0.5274, val loss: 1.0979, val acc: 0.5373  (best train acc: 0.5481, best val acc: 0.5474)\n",
      "[Epoch: 11540] train loss: 1.1031, train acc: 0.5411, val loss: 1.0991, val acc: 0.5295  (best train acc: 0.5481, best val acc: 0.5474)\n",
      "[Epoch: 11560] train loss: 1.1055, train acc: 0.5350, val loss: 1.0956, val acc: 0.5417  (best train acc: 0.5481, best val acc: 0.5474)\n",
      "[Epoch: 11580] train loss: 1.1073, train acc: 0.5354, val loss: 1.0962, val acc: 0.5390  (best train acc: 0.5481, best val acc: 0.5474)\n",
      "[Epoch: 11600] train loss: 1.1054, train acc: 0.5382, val loss: 1.1017, val acc: 0.5325  (best train acc: 0.5481, best val acc: 0.5474)\n",
      "[Epoch: 11620] train loss: 1.1142, train acc: 0.5316, val loss: 1.1019, val acc: 0.5430  (best train acc: 0.5492, best val acc: 0.5474)\n",
      "[Epoch: 11640] train loss: 1.1183, train acc: 0.5260, val loss: 1.0961, val acc: 0.5447  (best train acc: 0.5492, best val acc: 0.5474)\n",
      "[Epoch: 11660] train loss: 1.1058, train acc: 0.5375, val loss: 1.0966, val acc: 0.5433  (best train acc: 0.5492, best val acc: 0.5474)\n",
      "[Epoch: 11680] train loss: 1.1156, train acc: 0.5294, val loss: 1.0973, val acc: 0.5393  (best train acc: 0.5492, best val acc: 0.5474)\n",
      "[Epoch: 11700] train loss: 1.0918, train acc: 0.5417, val loss: 1.0945, val acc: 0.5413  (best train acc: 0.5492, best val acc: 0.5474)\n",
      "[Epoch: 11720] train loss: 1.1004, train acc: 0.5407, val loss: 1.0965, val acc: 0.5427  (best train acc: 0.5492, best val acc: 0.5474)\n",
      "[Epoch: 11740] train loss: 1.1096, train acc: 0.5332, val loss: 1.0949, val acc: 0.5403  (best train acc: 0.5492, best val acc: 0.5474)\n",
      "[Epoch: 11760] train loss: 1.0963, train acc: 0.5414, val loss: 1.0953, val acc: 0.5440  (best train acc: 0.5492, best val acc: 0.5474)\n",
      "[Epoch: 11780] train loss: 1.1085, train acc: 0.5388, val loss: 1.0959, val acc: 0.5430  (best train acc: 0.5492, best val acc: 0.5474)\n",
      "[Epoch: 11800] train loss: 1.0954, train acc: 0.5416, val loss: 1.0940, val acc: 0.5417  (best train acc: 0.5492, best val acc: 0.5474)\n",
      "[Epoch: 11820] train loss: 1.0962, train acc: 0.5437, val loss: 1.0951, val acc: 0.5430  (best train acc: 0.5492, best val acc: 0.5474)\n",
      "[Epoch: 11840] train loss: 1.1133, train acc: 0.5295, val loss: 1.0962, val acc: 0.5444  (best train acc: 0.5494, best val acc: 0.5474)\n",
      "[Epoch: 11860] train loss: 1.1097, train acc: 0.5343, val loss: 1.0964, val acc: 0.5376  (best train acc: 0.5494, best val acc: 0.5474)\n",
      "[Epoch: 11880] train loss: 1.0903, train acc: 0.5450, val loss: 1.0934, val acc: 0.5427  (best train acc: 0.5494, best val acc: 0.5474)\n",
      "[Epoch: 11900] train loss: 1.1151, train acc: 0.5298, val loss: 1.0976, val acc: 0.5427  (best train acc: 0.5494, best val acc: 0.5474)\n",
      "[Epoch: 11920] train loss: 1.0965, train acc: 0.5408, val loss: 1.0957, val acc: 0.5396  (best train acc: 0.5494, best val acc: 0.5474)\n",
      "[Epoch: 11940] train loss: 1.0960, train acc: 0.5417, val loss: 1.1090, val acc: 0.5349  (best train acc: 0.5494, best val acc: 0.5474)\n",
      "[Epoch: 11960] train loss: 1.0961, train acc: 0.5401, val loss: 1.0935, val acc: 0.5400  (best train acc: 0.5494, best val acc: 0.5474)\n",
      "[Epoch: 11980] train loss: 1.0973, train acc: 0.5416, val loss: 1.0926, val acc: 0.5423  (best train acc: 0.5494, best val acc: 0.5474)\n",
      "[Epoch: 12000] train loss: 1.0968, train acc: 0.5368, val loss: 1.0940, val acc: 0.5437  (best train acc: 0.5494, best val acc: 0.5474)\n",
      "[Epoch: 12020] train loss: 1.0944, train acc: 0.5362, val loss: 1.0966, val acc: 0.5342  (best train acc: 0.5494, best val acc: 0.5474)\n",
      "[Epoch: 12040] train loss: 1.0951, train acc: 0.5423, val loss: 1.0963, val acc: 0.5454  (best train acc: 0.5494, best val acc: 0.5474)\n",
      "[Epoch: 12060] train loss: 1.0999, train acc: 0.5424, val loss: 1.0930, val acc: 0.5423  (best train acc: 0.5494, best val acc: 0.5474)\n",
      "[Epoch: 12080] train loss: 1.1049, train acc: 0.5363, val loss: 1.1004, val acc: 0.5346  (best train acc: 0.5494, best val acc: 0.5474)\n",
      "[Epoch: 12100] train loss: 1.1202, train acc: 0.5272, val loss: 1.0949, val acc: 0.5403  (best train acc: 0.5494, best val acc: 0.5474)\n",
      "[Epoch: 12120] train loss: 1.0910, train acc: 0.5416, val loss: 1.0988, val acc: 0.5379  (best train acc: 0.5494, best val acc: 0.5474)\n",
      "[Epoch: 12140] train loss: 1.0956, train acc: 0.5434, val loss: 1.0930, val acc: 0.5413  (best train acc: 0.5494, best val acc: 0.5474)\n",
      "[Epoch: 12160] train loss: 1.0977, train acc: 0.5377, val loss: 1.0948, val acc: 0.5430  (best train acc: 0.5494, best val acc: 0.5474)\n",
      "[Epoch: 12180] train loss: 1.0933, train acc: 0.5442, val loss: 1.0948, val acc: 0.5363  (best train acc: 0.5494, best val acc: 0.5474)\n",
      "[Epoch: 12200] train loss: 1.1085, train acc: 0.5312, val loss: 1.0935, val acc: 0.5406  (best train acc: 0.5494, best val acc: 0.5487)\n",
      "[Epoch: 12220] train loss: 1.1066, train acc: 0.5340, val loss: 1.0940, val acc: 0.5366  (best train acc: 0.5494, best val acc: 0.5487)\n",
      "[Epoch: 12240] train loss: 1.0992, train acc: 0.5404, val loss: 1.0941, val acc: 0.5379  (best train acc: 0.5494, best val acc: 0.5487)\n",
      "[Epoch: 12260] train loss: 1.1109, train acc: 0.5348, val loss: 1.0937, val acc: 0.5423  (best train acc: 0.5512, best val acc: 0.5487)\n",
      "[Epoch: 12280] train loss: 1.0929, train acc: 0.5410, val loss: 1.0916, val acc: 0.5420  (best train acc: 0.5512, best val acc: 0.5487)\n",
      "[Epoch: 12300] train loss: 1.1144, train acc: 0.5317, val loss: 1.0906, val acc: 0.5396  (best train acc: 0.5512, best val acc: 0.5487)\n",
      "[Epoch: 12320] train loss: 1.0939, train acc: 0.5408, val loss: 1.0926, val acc: 0.5444  (best train acc: 0.5512, best val acc: 0.5487)\n",
      "[Epoch: 12340] train loss: 1.0916, train acc: 0.5405, val loss: 1.0936, val acc: 0.5420  (best train acc: 0.5512, best val acc: 0.5487)\n",
      "[Epoch: 12360] train loss: 1.1014, train acc: 0.5369, val loss: 1.0908, val acc: 0.5427  (best train acc: 0.5512, best val acc: 0.5487)\n",
      "[Epoch: 12380] train loss: 1.1021, train acc: 0.5347, val loss: 1.0900, val acc: 0.5393  (best train acc: 0.5515, best val acc: 0.5487)\n",
      "[Epoch: 12400] train loss: 1.0986, train acc: 0.5373, val loss: 1.0939, val acc: 0.5454  (best train acc: 0.5515, best val acc: 0.5487)\n",
      "[Epoch: 12420] train loss: 1.1024, train acc: 0.5429, val loss: 1.0912, val acc: 0.5427  (best train acc: 0.5515, best val acc: 0.5487)\n",
      "[Epoch: 12440] train loss: 1.0937, train acc: 0.5424, val loss: 1.0908, val acc: 0.5457  (best train acc: 0.5515, best val acc: 0.5487)\n",
      "[Epoch: 12460] train loss: 1.0998, train acc: 0.5396, val loss: 1.0907, val acc: 0.5440  (best train acc: 0.5515, best val acc: 0.5487)\n",
      "[Epoch: 12480] train loss: 1.0962, train acc: 0.5360, val loss: 1.0935, val acc: 0.5444  (best train acc: 0.5515, best val acc: 0.5487)\n",
      "[Epoch: 12500] train loss: 1.0963, train acc: 0.5388, val loss: 1.0926, val acc: 0.5406  (best train acc: 0.5515, best val acc: 0.5487)\n",
      "[Epoch: 12520] train loss: 1.0819, train acc: 0.5489, val loss: 1.0927, val acc: 0.5410  (best train acc: 0.5515, best val acc: 0.5487)\n",
      "[Epoch: 12540] train loss: 1.0959, train acc: 0.5385, val loss: 1.0894, val acc: 0.5420  (best train acc: 0.5515, best val acc: 0.5487)\n",
      "[Epoch: 12560] train loss: 1.0952, train acc: 0.5384, val loss: 1.0901, val acc: 0.5430  (best train acc: 0.5515, best val acc: 0.5487)\n",
      "[Epoch: 12580] train loss: 1.0918, train acc: 0.5413, val loss: 1.0905, val acc: 0.5447  (best train acc: 0.5515, best val acc: 0.5487)\n",
      "[Epoch: 12600] train loss: 1.0892, train acc: 0.5424, val loss: 1.0948, val acc: 0.5427  (best train acc: 0.5515, best val acc: 0.5487)\n",
      "[Epoch: 12620] train loss: 1.0830, train acc: 0.5497, val loss: 1.0895, val acc: 0.5413  (best train acc: 0.5515, best val acc: 0.5487)\n",
      "[Epoch: 12640] train loss: 1.0844, train acc: 0.5448, val loss: 1.0893, val acc: 0.5427  (best train acc: 0.5515, best val acc: 0.5487)\n",
      "[Epoch: 12660] train loss: 1.1094, train acc: 0.5317, val loss: 1.0935, val acc: 0.5440  (best train acc: 0.5515, best val acc: 0.5487)\n",
      "[Epoch: 12680] train loss: 1.1060, train acc: 0.5356, val loss: 1.0893, val acc: 0.5427  (best train acc: 0.5515, best val acc: 0.5487)\n",
      "[Epoch: 12700] train loss: 1.0990, train acc: 0.5421, val loss: 1.0899, val acc: 0.5406  (best train acc: 0.5515, best val acc: 0.5487)\n",
      "[Epoch: 12720] train loss: 1.0971, train acc: 0.5408, val loss: 1.0930, val acc: 0.5420  (best train acc: 0.5515, best val acc: 0.5487)\n",
      "[Epoch: 12740] train loss: 1.0784, train acc: 0.5455, val loss: 1.0874, val acc: 0.5433  (best train acc: 0.5515, best val acc: 0.5487)\n",
      "[Epoch: 12760] train loss: 1.0902, train acc: 0.5421, val loss: 1.0911, val acc: 0.5427  (best train acc: 0.5515, best val acc: 0.5487)\n",
      "[Epoch: 12780] train loss: 1.0862, train acc: 0.5466, val loss: 1.0902, val acc: 0.5423  (best train acc: 0.5515, best val acc: 0.5487)\n",
      "[Epoch: 12800] train loss: 1.0948, train acc: 0.5336, val loss: 1.0898, val acc: 0.5400  (best train acc: 0.5515, best val acc: 0.5487)\n",
      "[Epoch: 12820] train loss: 1.0918, train acc: 0.5419, val loss: 1.0888, val acc: 0.5460  (best train acc: 0.5515, best val acc: 0.5487)\n",
      "[Epoch: 12840] train loss: 1.1017, train acc: 0.5364, val loss: 1.0895, val acc: 0.5460  (best train acc: 0.5515, best val acc: 0.5487)\n",
      "[Epoch: 12860] train loss: 1.0973, train acc: 0.5410, val loss: 1.0885, val acc: 0.5467  (best train acc: 0.5515, best val acc: 0.5487)\n",
      "[Epoch: 12880] train loss: 1.0916, train acc: 0.5400, val loss: 1.0891, val acc: 0.5467  (best train acc: 0.5515, best val acc: 0.5487)\n",
      "[Epoch: 12900] train loss: 1.1003, train acc: 0.5403, val loss: 1.0875, val acc: 0.5430  (best train acc: 0.5515, best val acc: 0.5487)\n",
      "[Epoch: 12920] train loss: 1.0836, train acc: 0.5476, val loss: 1.0884, val acc: 0.5413  (best train acc: 0.5515, best val acc: 0.5487)\n",
      "[Epoch: 12940] train loss: 1.0926, train acc: 0.5376, val loss: 1.0872, val acc: 0.5433  (best train acc: 0.5515, best val acc: 0.5487)\n",
      "[Epoch: 12960] train loss: 1.0917, train acc: 0.5407, val loss: 1.0879, val acc: 0.5437  (best train acc: 0.5515, best val acc: 0.5487)\n",
      "[Epoch: 12980] train loss: 1.0948, train acc: 0.5421, val loss: 1.0884, val acc: 0.5474  (best train acc: 0.5517, best val acc: 0.5497)\n",
      "[Epoch: 13000] train loss: 1.0893, train acc: 0.5455, val loss: 1.0873, val acc: 0.5423  (best train acc: 0.5517, best val acc: 0.5497)\n",
      "[Epoch: 13020] train loss: 1.0951, train acc: 0.5380, val loss: 1.0891, val acc: 0.5444  (best train acc: 0.5517, best val acc: 0.5497)\n",
      "[Epoch: 13040] train loss: 1.0855, train acc: 0.5472, val loss: 1.0872, val acc: 0.5366  (best train acc: 0.5523, best val acc: 0.5497)\n",
      "[Epoch: 13060] train loss: 1.0808, train acc: 0.5429, val loss: 1.0866, val acc: 0.5437  (best train acc: 0.5523, best val acc: 0.5497)\n",
      "[Epoch: 13080] train loss: 1.0853, train acc: 0.5461, val loss: 1.0963, val acc: 0.5400  (best train acc: 0.5523, best val acc: 0.5497)\n",
      "[Epoch: 13100] train loss: 1.0845, train acc: 0.5480, val loss: 1.0925, val acc: 0.5420  (best train acc: 0.5523, best val acc: 0.5497)\n",
      "[Epoch: 13120] train loss: 1.0988, train acc: 0.5390, val loss: 1.0867, val acc: 0.5417  (best train acc: 0.5523, best val acc: 0.5497)\n",
      "[Epoch: 13140] train loss: 1.0893, train acc: 0.5427, val loss: 1.0865, val acc: 0.5440  (best train acc: 0.5523, best val acc: 0.5497)\n",
      "[Epoch: 13160] train loss: 1.0866, train acc: 0.5432, val loss: 1.0856, val acc: 0.5454  (best train acc: 0.5523, best val acc: 0.5497)\n",
      "[Epoch: 13180] train loss: 1.0980, train acc: 0.5422, val loss: 1.0861, val acc: 0.5437  (best train acc: 0.5529, best val acc: 0.5501)\n",
      "[Epoch: 13200] train loss: 1.1066, train acc: 0.5353, val loss: 1.0852, val acc: 0.5460  (best train acc: 0.5529, best val acc: 0.5501)\n",
      "[Epoch: 13220] train loss: 1.0978, train acc: 0.5434, val loss: 1.0913, val acc: 0.5440  (best train acc: 0.5529, best val acc: 0.5501)\n",
      "[Epoch: 13240] train loss: 1.0946, train acc: 0.5410, val loss: 1.0929, val acc: 0.5379  (best train acc: 0.5529, best val acc: 0.5511)\n",
      "[Epoch: 13260] train loss: 1.0951, train acc: 0.5432, val loss: 1.0963, val acc: 0.5444  (best train acc: 0.5529, best val acc: 0.5524)\n",
      "[Epoch: 13280] train loss: 1.0952, train acc: 0.5442, val loss: 1.0878, val acc: 0.5420  (best train acc: 0.5529, best val acc: 0.5524)\n",
      "[Epoch: 13300] train loss: 1.1005, train acc: 0.5408, val loss: 1.0870, val acc: 0.5467  (best train acc: 0.5529, best val acc: 0.5524)\n",
      "[Epoch: 13320] train loss: 1.0867, train acc: 0.5497, val loss: 1.0877, val acc: 0.5464  (best train acc: 0.5529, best val acc: 0.5524)\n",
      "[Epoch: 13340] train loss: 1.0852, train acc: 0.5466, val loss: 1.0849, val acc: 0.5427  (best train acc: 0.5529, best val acc: 0.5524)\n",
      "[Epoch: 13360] train loss: 1.0905, train acc: 0.5485, val loss: 1.0871, val acc: 0.5447  (best train acc: 0.5529, best val acc: 0.5524)\n",
      "[Epoch: 13380] train loss: 1.0914, train acc: 0.5403, val loss: 1.0910, val acc: 0.5369  (best train acc: 0.5529, best val acc: 0.5524)\n",
      "[Epoch: 13400] train loss: 1.0917, train acc: 0.5481, val loss: 1.0857, val acc: 0.5467  (best train acc: 0.5529, best val acc: 0.5524)\n",
      "[Epoch: 13420] train loss: 1.0914, train acc: 0.5436, val loss: 1.0837, val acc: 0.5423  (best train acc: 0.5529, best val acc: 0.5524)\n",
      "[Epoch: 13440] train loss: 1.0797, train acc: 0.5505, val loss: 1.0850, val acc: 0.5481  (best train acc: 0.5529, best val acc: 0.5524)\n",
      "[Epoch: 13460] train loss: 1.0819, train acc: 0.5446, val loss: 1.0848, val acc: 0.5494  (best train acc: 0.5573, best val acc: 0.5524)\n",
      "[Epoch: 13480] train loss: 1.0924, train acc: 0.5420, val loss: 1.0832, val acc: 0.5467  (best train acc: 0.5573, best val acc: 0.5524)\n",
      "[Epoch: 13500] train loss: 1.0833, train acc: 0.5447, val loss: 1.0869, val acc: 0.5440  (best train acc: 0.5573, best val acc: 0.5524)\n",
      "[Epoch: 13520] train loss: 1.0891, train acc: 0.5414, val loss: 1.0841, val acc: 0.5470  (best train acc: 0.5573, best val acc: 0.5524)\n",
      "[Epoch: 13540] train loss: 1.0793, train acc: 0.5494, val loss: 1.0827, val acc: 0.5427  (best train acc: 0.5573, best val acc: 0.5524)\n",
      "[Epoch: 13560] train loss: 1.0813, train acc: 0.5473, val loss: 1.0835, val acc: 0.5457  (best train acc: 0.5573, best val acc: 0.5524)\n",
      "[Epoch: 13580] train loss: 1.0938, train acc: 0.5388, val loss: 1.0845, val acc: 0.5420  (best train acc: 0.5573, best val acc: 0.5524)\n",
      "[Epoch: 13600] train loss: 1.0977, train acc: 0.5390, val loss: 1.0873, val acc: 0.5450  (best train acc: 0.5573, best val acc: 0.5524)\n",
      "[Epoch: 13620] train loss: 1.0856, train acc: 0.5466, val loss: 1.0829, val acc: 0.5433  (best train acc: 0.5573, best val acc: 0.5524)\n",
      "[Epoch: 13640] train loss: 1.0740, train acc: 0.5565, val loss: 1.0825, val acc: 0.5423  (best train acc: 0.5573, best val acc: 0.5524)\n",
      "[Epoch: 13660] train loss: 1.0953, train acc: 0.5340, val loss: 1.0828, val acc: 0.5454  (best train acc: 0.5573, best val acc: 0.5524)\n",
      "[Epoch: 13680] train loss: 1.0820, train acc: 0.5495, val loss: 1.0827, val acc: 0.5420  (best train acc: 0.5573, best val acc: 0.5524)\n",
      "[Epoch: 13700] train loss: 1.0868, train acc: 0.5460, val loss: 1.0898, val acc: 0.5444  (best train acc: 0.5573, best val acc: 0.5524)\n",
      "[Epoch: 13720] train loss: 1.0949, train acc: 0.5395, val loss: 1.0815, val acc: 0.5474  (best train acc: 0.5573, best val acc: 0.5524)\n",
      "[Epoch: 13740] train loss: 1.0925, train acc: 0.5421, val loss: 1.0834, val acc: 0.5427  (best train acc: 0.5573, best val acc: 0.5524)\n",
      "[Epoch: 13760] train loss: 1.0885, train acc: 0.5434, val loss: 1.0857, val acc: 0.5406  (best train acc: 0.5573, best val acc: 0.5524)\n",
      "[Epoch: 13780] train loss: 1.0974, train acc: 0.5388, val loss: 1.0829, val acc: 0.5447  (best train acc: 0.5573, best val acc: 0.5524)\n",
      "[Epoch: 13800] train loss: 1.0885, train acc: 0.5431, val loss: 1.0817, val acc: 0.5460  (best train acc: 0.5573, best val acc: 0.5524)\n",
      "[Epoch: 13820] train loss: 1.0886, train acc: 0.5435, val loss: 1.0812, val acc: 0.5474  (best train acc: 0.5573, best val acc: 0.5524)\n",
      "[Epoch: 13840] train loss: 1.0971, train acc: 0.5354, val loss: 1.0918, val acc: 0.5373  (best train acc: 0.5573, best val acc: 0.5524)\n",
      "[Epoch: 13860] train loss: 1.0978, train acc: 0.5377, val loss: 1.0900, val acc: 0.5386  (best train acc: 0.5573, best val acc: 0.5524)\n",
      "[Epoch: 13880] train loss: 1.0839, train acc: 0.5427, val loss: 1.0853, val acc: 0.5413  (best train acc: 0.5573, best val acc: 0.5524)\n",
      "[Epoch: 13900] train loss: 1.0841, train acc: 0.5434, val loss: 1.0810, val acc: 0.5494  (best train acc: 0.5573, best val acc: 0.5524)\n",
      "[Epoch: 13920] train loss: 1.0807, train acc: 0.5480, val loss: 1.0804, val acc: 0.5447  (best train acc: 0.5573, best val acc: 0.5524)\n",
      "[Epoch: 13940] train loss: 1.0664, train acc: 0.5579, val loss: 1.0804, val acc: 0.5518  (best train acc: 0.5579, best val acc: 0.5524)\n",
      "[Epoch: 13960] train loss: 1.0745, train acc: 0.5495, val loss: 1.0807, val acc: 0.5477  (best train acc: 0.5579, best val acc: 0.5524)\n",
      "[Epoch: 13980] train loss: 1.0835, train acc: 0.5440, val loss: 1.0862, val acc: 0.5437  (best train acc: 0.5583, best val acc: 0.5524)\n",
      "[Epoch: 14000] train loss: 1.0884, train acc: 0.5442, val loss: 1.0807, val acc: 0.5444  (best train acc: 0.5583, best val acc: 0.5524)\n",
      "[Epoch: 14020] train loss: 1.0842, train acc: 0.5416, val loss: 1.0805, val acc: 0.5427  (best train acc: 0.5583, best val acc: 0.5524)\n",
      "[Epoch: 14040] train loss: 1.0822, train acc: 0.5445, val loss: 1.0824, val acc: 0.5477  (best train acc: 0.5583, best val acc: 0.5524)\n",
      "[Epoch: 14060] train loss: 1.0748, train acc: 0.5514, val loss: 1.0797, val acc: 0.5497  (best train acc: 0.5583, best val acc: 0.5524)\n",
      "[Epoch: 14080] train loss: 1.0789, train acc: 0.5475, val loss: 1.0819, val acc: 0.5454  (best train acc: 0.5583, best val acc: 0.5524)\n",
      "[Epoch: 14100] train loss: 1.0785, train acc: 0.5521, val loss: 1.0799, val acc: 0.5444  (best train acc: 0.5583, best val acc: 0.5524)\n",
      "[Epoch: 14120] train loss: 1.0940, train acc: 0.5432, val loss: 1.0844, val acc: 0.5450  (best train acc: 0.5583, best val acc: 0.5524)\n",
      "[Epoch: 14140] train loss: 1.0864, train acc: 0.5455, val loss: 1.0804, val acc: 0.5497  (best train acc: 0.5583, best val acc: 0.5524)\n",
      "[Epoch: 14160] train loss: 1.0854, train acc: 0.5474, val loss: 1.0812, val acc: 0.5450  (best train acc: 0.5583, best val acc: 0.5524)\n",
      "[Epoch: 14180] train loss: 1.0838, train acc: 0.5432, val loss: 1.0880, val acc: 0.5501  (best train acc: 0.5583, best val acc: 0.5524)\n",
      "[Epoch: 14200] train loss: 1.0992, train acc: 0.5418, val loss: 1.0850, val acc: 0.5379  (best train acc: 0.5583, best val acc: 0.5524)\n",
      "[Epoch: 14220] train loss: 1.0875, train acc: 0.5471, val loss: 1.0797, val acc: 0.5447  (best train acc: 0.5583, best val acc: 0.5524)\n",
      "[Epoch: 14240] train loss: 1.0699, train acc: 0.5515, val loss: 1.0797, val acc: 0.5467  (best train acc: 0.5583, best val acc: 0.5524)\n",
      "[Epoch: 14260] train loss: 1.0886, train acc: 0.5448, val loss: 1.0804, val acc: 0.5460  (best train acc: 0.5583, best val acc: 0.5528)\n",
      "[Epoch: 14280] train loss: 1.0949, train acc: 0.5377, val loss: 1.0791, val acc: 0.5430  (best train acc: 0.5583, best val acc: 0.5535)\n",
      "[Epoch: 14300] train loss: 1.0822, train acc: 0.5463, val loss: 1.0792, val acc: 0.5470  (best train acc: 0.5583, best val acc: 0.5535)\n",
      "[Epoch: 14320] train loss: 1.0836, train acc: 0.5429, val loss: 1.0854, val acc: 0.5501  (best train acc: 0.5583, best val acc: 0.5535)\n",
      "[Epoch: 14340] train loss: 1.0980, train acc: 0.5398, val loss: 1.0813, val acc: 0.5481  (best train acc: 0.5583, best val acc: 0.5535)\n",
      "[Epoch: 14360] train loss: 1.0975, train acc: 0.5404, val loss: 1.0836, val acc: 0.5474  (best train acc: 0.5583, best val acc: 0.5535)\n",
      "[Epoch: 14380] train loss: 1.0842, train acc: 0.5428, val loss: 1.0791, val acc: 0.5508  (best train acc: 0.5583, best val acc: 0.5535)\n",
      "[Epoch: 14400] train loss: 1.0765, train acc: 0.5452, val loss: 1.0778, val acc: 0.5467  (best train acc: 0.5583, best val acc: 0.5535)\n",
      "[Epoch: 14420] train loss: 1.0784, train acc: 0.5438, val loss: 1.0778, val acc: 0.5487  (best train acc: 0.5583, best val acc: 0.5535)\n",
      "[Epoch: 14440] train loss: 1.0877, train acc: 0.5424, val loss: 1.0823, val acc: 0.5457  (best train acc: 0.5583, best val acc: 0.5535)\n",
      "[Epoch: 14460] train loss: 1.0898, train acc: 0.5420, val loss: 1.0785, val acc: 0.5511  (best train acc: 0.5583, best val acc: 0.5535)\n",
      "[Epoch: 14480] train loss: 1.0837, train acc: 0.5494, val loss: 1.0801, val acc: 0.5474  (best train acc: 0.5583, best val acc: 0.5535)\n",
      "[Epoch: 14500] train loss: 1.0743, train acc: 0.5508, val loss: 1.0786, val acc: 0.5460  (best train acc: 0.5583, best val acc: 0.5535)\n",
      "[Epoch: 14520] train loss: 1.0822, train acc: 0.5484, val loss: 1.0794, val acc: 0.5474  (best train acc: 0.5583, best val acc: 0.5535)\n",
      "[Epoch: 14540] train loss: 1.0897, train acc: 0.5445, val loss: 1.0798, val acc: 0.5514  (best train acc: 0.5583, best val acc: 0.5535)\n",
      "[Epoch: 14560] train loss: 1.0918, train acc: 0.5441, val loss: 1.0784, val acc: 0.5518  (best train acc: 0.5583, best val acc: 0.5535)\n",
      "[Epoch: 14580] train loss: 1.0888, train acc: 0.5475, val loss: 1.0834, val acc: 0.5410  (best train acc: 0.5583, best val acc: 0.5541)\n",
      "[Epoch: 14600] train loss: 1.0784, train acc: 0.5468, val loss: 1.0816, val acc: 0.5410  (best train acc: 0.5602, best val acc: 0.5541)\n",
      "[Epoch: 14620] train loss: 1.0712, train acc: 0.5518, val loss: 1.0775, val acc: 0.5497  (best train acc: 0.5602, best val acc: 0.5541)\n",
      "[Epoch: 14640] train loss: 1.0779, train acc: 0.5469, val loss: 1.0769, val acc: 0.5518  (best train acc: 0.5602, best val acc: 0.5541)\n",
      "[Epoch: 14660] train loss: 1.0926, train acc: 0.5434, val loss: 1.0782, val acc: 0.5481  (best train acc: 0.5602, best val acc: 0.5541)\n",
      "[Epoch: 14680] train loss: 1.0817, train acc: 0.5481, val loss: 1.0768, val acc: 0.5457  (best train acc: 0.5633, best val acc: 0.5541)\n",
      "[Epoch: 14700] train loss: 1.0745, train acc: 0.5500, val loss: 1.0779, val acc: 0.5501  (best train acc: 0.5633, best val acc: 0.5545)\n",
      "[Epoch: 14720] train loss: 1.0665, train acc: 0.5565, val loss: 1.0763, val acc: 0.5508  (best train acc: 0.5633, best val acc: 0.5545)\n",
      "[Epoch: 14740] train loss: 1.0815, train acc: 0.5488, val loss: 1.0937, val acc: 0.5460  (best train acc: 0.5633, best val acc: 0.5545)\n",
      "[Epoch: 14760] train loss: 1.0764, train acc: 0.5466, val loss: 1.0761, val acc: 0.5460  (best train acc: 0.5633, best val acc: 0.5545)\n",
      "[Epoch: 14780] train loss: 1.0890, train acc: 0.5398, val loss: 1.0796, val acc: 0.5535  (best train acc: 0.5633, best val acc: 0.5545)\n",
      "[Epoch: 14800] train loss: 1.0871, train acc: 0.5416, val loss: 1.0769, val acc: 0.5467  (best train acc: 0.5633, best val acc: 0.5545)\n",
      "[Epoch: 14820] train loss: 1.0992, train acc: 0.5351, val loss: 1.0804, val acc: 0.5528  (best train acc: 0.5633, best val acc: 0.5565)\n",
      "[Epoch: 14840] train loss: 1.0716, train acc: 0.5487, val loss: 1.0799, val acc: 0.5474  (best train acc: 0.5633, best val acc: 0.5565)\n",
      "[Epoch: 14860] train loss: 1.0781, train acc: 0.5443, val loss: 1.0766, val acc: 0.5511  (best train acc: 0.5633, best val acc: 0.5565)\n",
      "[Epoch: 14880] train loss: 1.0725, train acc: 0.5471, val loss: 1.0754, val acc: 0.5518  (best train acc: 0.5633, best val acc: 0.5565)\n",
      "[Epoch: 14900] train loss: 1.0767, train acc: 0.5505, val loss: 1.0800, val acc: 0.5420  (best train acc: 0.5633, best val acc: 0.5565)\n",
      "[Epoch: 14920] train loss: 1.0870, train acc: 0.5458, val loss: 1.0780, val acc: 0.5514  (best train acc: 0.5633, best val acc: 0.5565)\n",
      "[Epoch: 14940] train loss: 1.1046, train acc: 0.5445, val loss: 1.0788, val acc: 0.5518  (best train acc: 0.5633, best val acc: 0.5565)\n",
      "[Epoch: 14960] train loss: 1.0778, train acc: 0.5518, val loss: 1.0761, val acc: 0.5538  (best train acc: 0.5633, best val acc: 0.5565)\n",
      "[Epoch: 14980] train loss: 1.0706, train acc: 0.5500, val loss: 1.0769, val acc: 0.5518  (best train acc: 0.5633, best val acc: 0.5565)\n",
      "[Epoch: 15000] train loss: 1.0737, train acc: 0.5510, val loss: 1.0752, val acc: 0.5521  (best train acc: 0.5633, best val acc: 0.5565)\n",
      "[Epoch: 15020] train loss: 1.0650, train acc: 0.5553, val loss: 1.0767, val acc: 0.5524  (best train acc: 0.5633, best val acc: 0.5565)\n",
      "[Epoch: 15040] train loss: 1.0910, train acc: 0.5419, val loss: 1.0773, val acc: 0.5504  (best train acc: 0.5633, best val acc: 0.5565)\n",
      "[Epoch: 15060] train loss: 1.0928, train acc: 0.5380, val loss: 1.0738, val acc: 0.5477  (best train acc: 0.5633, best val acc: 0.5565)\n",
      "[Epoch: 15080] train loss: 1.0834, train acc: 0.5495, val loss: 1.0814, val acc: 0.5477  (best train acc: 0.5633, best val acc: 0.5565)\n",
      "[Epoch: 15100] train loss: 1.0818, train acc: 0.5427, val loss: 1.0745, val acc: 0.5497  (best train acc: 0.5633, best val acc: 0.5565)\n",
      "[Epoch: 15120] train loss: 1.0763, train acc: 0.5516, val loss: 1.0746, val acc: 0.5467  (best train acc: 0.5633, best val acc: 0.5565)\n",
      "[Epoch: 15140] train loss: 1.0863, train acc: 0.5444, val loss: 1.0796, val acc: 0.5440  (best train acc: 0.5633, best val acc: 0.5565)\n",
      "[Epoch: 15160] train loss: 1.0682, train acc: 0.5566, val loss: 1.0748, val acc: 0.5457  (best train acc: 0.5633, best val acc: 0.5565)\n",
      "[Epoch: 15180] train loss: 1.0824, train acc: 0.5450, val loss: 1.0798, val acc: 0.5497  (best train acc: 0.5633, best val acc: 0.5565)\n",
      "[Epoch: 15200] train loss: 1.0803, train acc: 0.5464, val loss: 1.0829, val acc: 0.5440  (best train acc: 0.5633, best val acc: 0.5565)\n",
      "[Epoch: 15220] train loss: 1.0867, train acc: 0.5422, val loss: 1.0859, val acc: 0.5504  (best train acc: 0.5633, best val acc: 0.5565)\n",
      "[Epoch: 15240] train loss: 1.0793, train acc: 0.5503, val loss: 1.0767, val acc: 0.5477  (best train acc: 0.5633, best val acc: 0.5565)\n",
      "[Epoch: 15260] train loss: 1.0705, train acc: 0.5540, val loss: 1.0745, val acc: 0.5454  (best train acc: 0.5633, best val acc: 0.5565)\n",
      "[Epoch: 15280] train loss: 1.0760, train acc: 0.5492, val loss: 1.0761, val acc: 0.5501  (best train acc: 0.5633, best val acc: 0.5565)\n",
      "[Epoch: 15300] train loss: 1.0826, train acc: 0.5440, val loss: 1.0847, val acc: 0.5420  (best train acc: 0.5633, best val acc: 0.5565)\n",
      "[Epoch: 15320] train loss: 1.0779, train acc: 0.5484, val loss: 1.0774, val acc: 0.5494  (best train acc: 0.5633, best val acc: 0.5565)\n",
      "[Epoch: 15340] train loss: 1.0890, train acc: 0.5382, val loss: 1.0807, val acc: 0.5487  (best train acc: 0.5633, best val acc: 0.5565)\n",
      "[Epoch: 15360] train loss: 1.0834, train acc: 0.5433, val loss: 1.0745, val acc: 0.5487  (best train acc: 0.5633, best val acc: 0.5568)\n",
      "[Epoch: 15380] train loss: 1.0798, train acc: 0.5488, val loss: 1.0729, val acc: 0.5521  (best train acc: 0.5633, best val acc: 0.5568)\n",
      "[Epoch: 15400] train loss: 1.0921, train acc: 0.5402, val loss: 1.0761, val acc: 0.5467  (best train acc: 0.5633, best val acc: 0.5568)\n",
      "[Epoch: 15420] train loss: 1.0892, train acc: 0.5415, val loss: 1.0807, val acc: 0.5484  (best train acc: 0.5633, best val acc: 0.5568)\n",
      "[Epoch: 15440] train loss: 1.0763, train acc: 0.5484, val loss: 1.0760, val acc: 0.5562  (best train acc: 0.5633, best val acc: 0.5568)\n",
      "[Epoch: 15460] train loss: 1.0776, train acc: 0.5454, val loss: 1.0767, val acc: 0.5535  (best train acc: 0.5633, best val acc: 0.5568)\n",
      "[Epoch: 15480] train loss: 1.0853, train acc: 0.5442, val loss: 1.0731, val acc: 0.5524  (best train acc: 0.5633, best val acc: 0.5568)\n",
      "[Epoch: 15500] train loss: 1.0680, train acc: 0.5557, val loss: 1.0731, val acc: 0.5524  (best train acc: 0.5633, best val acc: 0.5568)\n",
      "[Epoch: 15520] train loss: 1.0767, train acc: 0.5466, val loss: 1.0739, val acc: 0.5477  (best train acc: 0.5633, best val acc: 0.5568)\n",
      "[Epoch: 15540] train loss: 1.0832, train acc: 0.5456, val loss: 1.0732, val acc: 0.5467  (best train acc: 0.5651, best val acc: 0.5568)\n",
      "[Epoch: 15560] train loss: 1.0660, train acc: 0.5523, val loss: 1.0733, val acc: 0.5541  (best train acc: 0.5651, best val acc: 0.5568)\n",
      "[Epoch: 15580] train loss: 1.0671, train acc: 0.5500, val loss: 1.0767, val acc: 0.5444  (best train acc: 0.5651, best val acc: 0.5578)\n",
      "[Epoch: 15600] train loss: 1.0846, train acc: 0.5470, val loss: 1.0747, val acc: 0.5524  (best train acc: 0.5651, best val acc: 0.5578)\n",
      "[Epoch: 15620] train loss: 1.0679, train acc: 0.5555, val loss: 1.0746, val acc: 0.5535  (best train acc: 0.5651, best val acc: 0.5578)\n",
      "[Epoch: 15640] train loss: 1.0733, train acc: 0.5494, val loss: 1.0720, val acc: 0.5518  (best train acc: 0.5651, best val acc: 0.5578)\n",
      "[Epoch: 15660] train loss: 1.0589, train acc: 0.5579, val loss: 1.0720, val acc: 0.5487  (best train acc: 0.5651, best val acc: 0.5578)\n",
      "[Epoch: 15680] train loss: 1.0837, train acc: 0.5444, val loss: 1.0749, val acc: 0.5474  (best train acc: 0.5651, best val acc: 0.5585)\n",
      "[Epoch: 15700] train loss: 1.0535, train acc: 0.5619, val loss: 1.0740, val acc: 0.5511  (best train acc: 0.5651, best val acc: 0.5585)\n",
      "[Epoch: 15720] train loss: 1.0756, train acc: 0.5503, val loss: 1.0714, val acc: 0.5558  (best train acc: 0.5651, best val acc: 0.5585)\n",
      "[Epoch: 15740] train loss: 1.0667, train acc: 0.5568, val loss: 1.0745, val acc: 0.5514  (best train acc: 0.5651, best val acc: 0.5585)\n",
      "[Epoch: 15760] train loss: 1.0759, train acc: 0.5439, val loss: 1.0721, val acc: 0.5508  (best train acc: 0.5651, best val acc: 0.5585)\n",
      "[Epoch: 15780] train loss: 1.0726, train acc: 0.5522, val loss: 1.0719, val acc: 0.5551  (best train acc: 0.5651, best val acc: 0.5585)\n",
      "[Epoch: 15800] train loss: 1.0748, train acc: 0.5498, val loss: 1.0711, val acc: 0.5535  (best train acc: 0.5651, best val acc: 0.5585)\n",
      "[Epoch: 15820] train loss: 1.0874, train acc: 0.5430, val loss: 1.0746, val acc: 0.5491  (best train acc: 0.5651, best val acc: 0.5585)\n",
      "[Epoch: 15840] train loss: 1.0695, train acc: 0.5497, val loss: 1.0721, val acc: 0.5508  (best train acc: 0.5651, best val acc: 0.5585)\n",
      "[Epoch: 15860] train loss: 1.0750, train acc: 0.5565, val loss: 1.0733, val acc: 0.5548  (best train acc: 0.5651, best val acc: 0.5585)\n",
      "[Epoch: 15880] train loss: 1.0694, train acc: 0.5518, val loss: 1.0714, val acc: 0.5497  (best train acc: 0.5651, best val acc: 0.5585)\n",
      "[Epoch: 15900] train loss: 1.0927, train acc: 0.5393, val loss: 1.0703, val acc: 0.5524  (best train acc: 0.5651, best val acc: 0.5585)\n",
      "[Epoch: 15920] train loss: 1.0834, train acc: 0.5456, val loss: 1.0762, val acc: 0.5511  (best train acc: 0.5651, best val acc: 0.5585)\n",
      "[Epoch: 15940] train loss: 1.0845, train acc: 0.5437, val loss: 1.0742, val acc: 0.5444  (best train acc: 0.5651, best val acc: 0.5585)\n",
      "[Epoch: 15960] train loss: 1.0734, train acc: 0.5484, val loss: 1.0708, val acc: 0.5508  (best train acc: 0.5651, best val acc: 0.5585)\n",
      "[Epoch: 15980] train loss: 1.0676, train acc: 0.5513, val loss: 1.0706, val acc: 0.5548  (best train acc: 0.5651, best val acc: 0.5585)\n",
      "[Epoch: 16000] train loss: 1.0754, train acc: 0.5506, val loss: 1.0730, val acc: 0.5494  (best train acc: 0.5651, best val acc: 0.5585)\n",
      "[Epoch: 16020] train loss: 1.0626, train acc: 0.5581, val loss: 1.0710, val acc: 0.5514  (best train acc: 0.5651, best val acc: 0.5585)\n",
      "[Epoch: 16040] train loss: 1.0568, train acc: 0.5573, val loss: 1.0718, val acc: 0.5508  (best train acc: 0.5651, best val acc: 0.5599)\n",
      "[Epoch: 16060] train loss: 1.0787, train acc: 0.5398, val loss: 1.0716, val acc: 0.5497  (best train acc: 0.5651, best val acc: 0.5599)\n",
      "[Epoch: 16080] train loss: 1.0700, train acc: 0.5547, val loss: 1.0692, val acc: 0.5511  (best train acc: 0.5651, best val acc: 0.5599)\n",
      "[Epoch: 16100] train loss: 1.0869, train acc: 0.5347, val loss: 1.0745, val acc: 0.5541  (best train acc: 0.5651, best val acc: 0.5599)\n",
      "[Epoch: 16120] train loss: 1.0748, train acc: 0.5544, val loss: 1.0764, val acc: 0.5521  (best train acc: 0.5651, best val acc: 0.5599)\n",
      "[Epoch: 16140] train loss: 1.0782, train acc: 0.5490, val loss: 1.0726, val acc: 0.5541  (best train acc: 0.5651, best val acc: 0.5599)\n",
      "[Epoch: 16160] train loss: 1.0724, train acc: 0.5510, val loss: 1.0775, val acc: 0.5504  (best train acc: 0.5651, best val acc: 0.5599)\n",
      "[Epoch: 16180] train loss: 1.0749, train acc: 0.5520, val loss: 1.0727, val acc: 0.5562  (best train acc: 0.5651, best val acc: 0.5599)\n",
      "[Epoch: 16200] train loss: 1.0669, train acc: 0.5569, val loss: 1.0705, val acc: 0.5501  (best train acc: 0.5651, best val acc: 0.5599)\n",
      "[Epoch: 16220] train loss: 1.0543, train acc: 0.5570, val loss: 1.0730, val acc: 0.5592  (best train acc: 0.5651, best val acc: 0.5599)\n",
      "[Epoch: 16240] train loss: 1.0816, train acc: 0.5445, val loss: 1.0725, val acc: 0.5538  (best train acc: 0.5651, best val acc: 0.5616)\n",
      "[Epoch: 16260] train loss: 1.0577, train acc: 0.5529, val loss: 1.0694, val acc: 0.5535  (best train acc: 0.5651, best val acc: 0.5616)\n",
      "[Epoch: 16280] train loss: 1.0654, train acc: 0.5502, val loss: 1.0697, val acc: 0.5538  (best train acc: 0.5651, best val acc: 0.5616)\n",
      "[Epoch: 16300] train loss: 1.0716, train acc: 0.5498, val loss: 1.0719, val acc: 0.5545  (best train acc: 0.5651, best val acc: 0.5616)\n",
      "[Epoch: 16320] train loss: 1.0731, train acc: 0.5502, val loss: 1.0716, val acc: 0.5504  (best train acc: 0.5651, best val acc: 0.5616)\n",
      "[Epoch: 16340] train loss: 1.0812, train acc: 0.5448, val loss: 1.0685, val acc: 0.5531  (best train acc: 0.5651, best val acc: 0.5616)\n",
      "[Epoch: 16360] train loss: 1.0867, train acc: 0.5445, val loss: 1.0715, val acc: 0.5511  (best train acc: 0.5651, best val acc: 0.5616)\n",
      "[Epoch: 16380] train loss: 1.0985, train acc: 0.5377, val loss: 1.0833, val acc: 0.5444  (best train acc: 0.5651, best val acc: 0.5616)\n",
      "[Epoch: 16400] train loss: 1.0809, train acc: 0.5488, val loss: 1.0778, val acc: 0.5531  (best train acc: 0.5651, best val acc: 0.5616)\n",
      "[Epoch: 16420] train loss: 1.0822, train acc: 0.5426, val loss: 1.0682, val acc: 0.5538  (best train acc: 0.5651, best val acc: 0.5616)\n",
      "[Epoch: 16440] train loss: 1.0642, train acc: 0.5549, val loss: 1.0689, val acc: 0.5562  (best train acc: 0.5651, best val acc: 0.5616)\n",
      "[Epoch: 16460] train loss: 1.0726, train acc: 0.5512, val loss: 1.0688, val acc: 0.5514  (best train acc: 0.5651, best val acc: 0.5616)\n",
      "[Epoch: 16480] train loss: 1.0703, train acc: 0.5573, val loss: 1.0684, val acc: 0.5545  (best train acc: 0.5651, best val acc: 0.5616)\n",
      "[Epoch: 16500] train loss: 1.0613, train acc: 0.5559, val loss: 1.0672, val acc: 0.5531  (best train acc: 0.5651, best val acc: 0.5616)\n",
      "[Epoch: 16520] train loss: 1.0926, train acc: 0.5390, val loss: 1.0688, val acc: 0.5481  (best train acc: 0.5651, best val acc: 0.5616)\n",
      "[Epoch: 16540] train loss: 1.0819, train acc: 0.5423, val loss: 1.0690, val acc: 0.5494  (best train acc: 0.5651, best val acc: 0.5616)\n",
      "[Epoch: 16560] train loss: 1.0580, train acc: 0.5570, val loss: 1.0678, val acc: 0.5568  (best train acc: 0.5651, best val acc: 0.5616)\n",
      "[Epoch: 16580] train loss: 1.0774, train acc: 0.5447, val loss: 1.0705, val acc: 0.5514  (best train acc: 0.5651, best val acc: 0.5616)\n",
      "[Epoch: 16600] train loss: 1.0706, train acc: 0.5502, val loss: 1.0682, val acc: 0.5524  (best train acc: 0.5651, best val acc: 0.5616)\n",
      "[Epoch: 16620] train loss: 1.0688, train acc: 0.5512, val loss: 1.0724, val acc: 0.5504  (best train acc: 0.5651, best val acc: 0.5616)\n",
      "[Epoch: 16640] train loss: 1.0535, train acc: 0.5576, val loss: 1.0675, val acc: 0.5524  (best train acc: 0.5651, best val acc: 0.5616)\n",
      "[Epoch: 16660] train loss: 1.0524, train acc: 0.5583, val loss: 1.0688, val acc: 0.5501  (best train acc: 0.5651, best val acc: 0.5616)\n",
      "[Epoch: 16680] train loss: 1.0769, train acc: 0.5475, val loss: 1.0667, val acc: 0.5501  (best train acc: 0.5651, best val acc: 0.5616)\n",
      "[Epoch: 16700] train loss: 1.0869, train acc: 0.5416, val loss: 1.0678, val acc: 0.5545  (best train acc: 0.5651, best val acc: 0.5616)\n",
      "[Epoch: 16720] train loss: 1.0780, train acc: 0.5476, val loss: 1.0666, val acc: 0.5565  (best train acc: 0.5651, best val acc: 0.5616)\n",
      "[Epoch: 16740] train loss: 1.0639, train acc: 0.5478, val loss: 1.0664, val acc: 0.5497  (best train acc: 0.5651, best val acc: 0.5616)\n",
      "[Epoch: 16760] train loss: 1.0751, train acc: 0.5489, val loss: 1.0685, val acc: 0.5518  (best train acc: 0.5651, best val acc: 0.5616)\n",
      "[Epoch: 16780] train loss: 1.0912, train acc: 0.5385, val loss: 1.0667, val acc: 0.5514  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 16800] train loss: 1.0682, train acc: 0.5510, val loss: 1.0739, val acc: 0.5568  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 16820] train loss: 1.0765, train acc: 0.5464, val loss: 1.0679, val acc: 0.5568  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 16840] train loss: 1.0700, train acc: 0.5509, val loss: 1.0677, val acc: 0.5565  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 16860] train loss: 1.3824, train acc: 0.4693, val loss: 1.4550, val acc: 0.4040  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 16880] train loss: 1.1577, train acc: 0.5098, val loss: 1.1828, val acc: 0.4948  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 16900] train loss: 1.1557, train acc: 0.5205, val loss: 1.1410, val acc: 0.5325  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 16920] train loss: 1.1343, train acc: 0.5289, val loss: 1.1333, val acc: 0.5298  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 16940] train loss: 1.1337, train acc: 0.5276, val loss: 1.1316, val acc: 0.5329  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 16960] train loss: 1.1276, train acc: 0.5333, val loss: 1.1309, val acc: 0.5342  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 16980] train loss: 1.1389, train acc: 0.5260, val loss: 1.1314, val acc: 0.5309  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 17000] train loss: 1.1398, train acc: 0.5277, val loss: 1.1306, val acc: 0.5319  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 17020] train loss: 1.1486, train acc: 0.5253, val loss: 1.1305, val acc: 0.5336  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 17040] train loss: 1.1292, train acc: 0.5304, val loss: 1.1301, val acc: 0.5315  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 17060] train loss: 1.1308, train acc: 0.5322, val loss: 1.1316, val acc: 0.5332  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 17080] train loss: 1.1230, train acc: 0.5359, val loss: 1.1303, val acc: 0.5298  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 17100] train loss: 1.1424, train acc: 0.5275, val loss: 1.1324, val acc: 0.5322  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 17120] train loss: 1.1394, train acc: 0.5330, val loss: 1.1312, val acc: 0.5336  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 17140] train loss: 1.1488, train acc: 0.5244, val loss: 1.1296, val acc: 0.5298  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 17160] train loss: 1.1368, train acc: 0.5291, val loss: 1.1296, val acc: 0.5292  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 17180] train loss: 1.1512, train acc: 0.5242, val loss: 1.1296, val acc: 0.5302  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 17200] train loss: 1.1268, train acc: 0.5364, val loss: 1.1329, val acc: 0.5245  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 17220] train loss: 1.1660, train acc: 0.5171, val loss: 1.1338, val acc: 0.5305  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 17240] train loss: 1.1384, train acc: 0.5308, val loss: 1.1298, val acc: 0.5315  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 17260] train loss: 1.1425, train acc: 0.5301, val loss: 1.1291, val acc: 0.5309  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 17280] train loss: 1.1268, train acc: 0.5351, val loss: 1.1310, val acc: 0.5336  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 17300] train loss: 1.1242, train acc: 0.5360, val loss: 1.1400, val acc: 0.5302  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 17320] train loss: 1.1339, train acc: 0.5263, val loss: 1.1293, val acc: 0.5325  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 17340] train loss: 1.1537, train acc: 0.5198, val loss: 1.1289, val acc: 0.5322  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 17360] train loss: 1.1407, train acc: 0.5226, val loss: 1.1290, val acc: 0.5305  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 17380] train loss: 1.1287, train acc: 0.5312, val loss: 1.1300, val acc: 0.5346  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 17400] train loss: 1.1485, train acc: 0.5239, val loss: 1.1288, val acc: 0.5315  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 17420] train loss: 1.1265, train acc: 0.5348, val loss: 1.1294, val acc: 0.5315  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 17440] train loss: 1.1400, train acc: 0.5359, val loss: 1.1317, val acc: 0.5342  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 17460] train loss: 1.1545, train acc: 0.5192, val loss: 1.1291, val acc: 0.5319  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 17480] train loss: 1.1419, train acc: 0.5250, val loss: 1.1311, val acc: 0.5251  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 17500] train loss: 1.1393, train acc: 0.5282, val loss: 1.1285, val acc: 0.5312  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 17520] train loss: 1.1402, train acc: 0.5307, val loss: 1.1336, val acc: 0.5325  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 17540] train loss: 1.1319, train acc: 0.5309, val loss: 1.1285, val acc: 0.5325  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 17560] train loss: 1.1269, train acc: 0.5333, val loss: 1.1310, val acc: 0.5352  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 17580] train loss: 1.1307, train acc: 0.5320, val loss: 1.1294, val acc: 0.5329  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 17600] train loss: 1.1504, train acc: 0.5255, val loss: 1.1317, val acc: 0.5336  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 17620] train loss: 1.1391, train acc: 0.5302, val loss: 1.1283, val acc: 0.5332  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 17640] train loss: 1.1258, train acc: 0.5333, val loss: 1.1282, val acc: 0.5325  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 17660] train loss: 1.1372, train acc: 0.5253, val loss: 1.1282, val acc: 0.5305  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 17680] train loss: 1.1316, train acc: 0.5331, val loss: 1.1280, val acc: 0.5319  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 17700] train loss: 1.1347, train acc: 0.5289, val loss: 1.1280, val acc: 0.5315  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 17720] train loss: 1.1427, train acc: 0.5250, val loss: 1.1287, val acc: 0.5268  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 17740] train loss: 1.1479, train acc: 0.5282, val loss: 1.1278, val acc: 0.5319  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 17760] train loss: 1.1182, train acc: 0.5379, val loss: 1.1288, val acc: 0.5342  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 17780] train loss: 1.1206, train acc: 0.5331, val loss: 1.1281, val acc: 0.5315  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 17800] train loss: 1.1285, train acc: 0.5305, val loss: 1.1276, val acc: 0.5312  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 17820] train loss: 1.1304, train acc: 0.5277, val loss: 1.1287, val acc: 0.5349  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 17840] train loss: 1.1290, train acc: 0.5341, val loss: 1.1278, val acc: 0.5329  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 17860] train loss: 1.1336, train acc: 0.5341, val loss: 1.1277, val acc: 0.5346  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 17880] train loss: 1.1516, train acc: 0.5196, val loss: 1.1278, val acc: 0.5349  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 17900] train loss: 1.1455, train acc: 0.5276, val loss: 1.1380, val acc: 0.5325  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 17920] train loss: 1.1343, train acc: 0.5324, val loss: 1.1280, val acc: 0.5292  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 17940] train loss: 1.1325, train acc: 0.5288, val loss: 1.1282, val acc: 0.5295  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 17960] train loss: 1.1475, train acc: 0.5237, val loss: 1.1272, val acc: 0.5312  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 17980] train loss: 1.1447, train acc: 0.5240, val loss: 1.1275, val acc: 0.5342  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 18000] train loss: 1.1387, train acc: 0.5286, val loss: 1.1288, val acc: 0.5285  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 18020] train loss: 1.1341, train acc: 0.5328, val loss: 1.1270, val acc: 0.5322  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 18040] train loss: 1.1376, train acc: 0.5299, val loss: 1.1310, val acc: 0.5204  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 18060] train loss: 1.1456, train acc: 0.5261, val loss: 1.1292, val acc: 0.5366  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 18080] train loss: 1.1291, train acc: 0.5295, val loss: 1.1269, val acc: 0.5336  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 18100] train loss: 1.1410, train acc: 0.5215, val loss: 1.1269, val acc: 0.5315  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 18120] train loss: 1.1420, train acc: 0.5257, val loss: 1.1268, val acc: 0.5309  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 18140] train loss: 1.1357, train acc: 0.5294, val loss: 1.1288, val acc: 0.5356  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 18160] train loss: 1.1500, train acc: 0.5237, val loss: 1.1269, val acc: 0.5309  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 18180] train loss: 1.1180, train acc: 0.5329, val loss: 1.1270, val acc: 0.5302  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 18200] train loss: 1.1470, train acc: 0.5276, val loss: 1.1344, val acc: 0.5366  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 18220] train loss: 1.1408, train acc: 0.5300, val loss: 1.1279, val acc: 0.5342  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 18240] train loss: 1.1369, train acc: 0.5299, val loss: 1.1267, val acc: 0.5363  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 18260] train loss: 1.1262, train acc: 0.5348, val loss: 1.1262, val acc: 0.5319  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 18280] train loss: 1.1249, train acc: 0.5401, val loss: 1.1271, val acc: 0.5349  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 18300] train loss: 1.1277, train acc: 0.5406, val loss: 1.1262, val acc: 0.5356  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 18320] train loss: 1.1309, train acc: 0.5367, val loss: 1.1322, val acc: 0.5346  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 18340] train loss: 1.1364, train acc: 0.5275, val loss: 1.1259, val acc: 0.5336  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 18360] train loss: 1.1385, train acc: 0.5282, val loss: 1.1258, val acc: 0.5329  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 18380] train loss: 1.1220, train acc: 0.5400, val loss: 1.1293, val acc: 0.5224  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 18400] train loss: 1.1253, train acc: 0.5351, val loss: 1.1263, val acc: 0.5315  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 18420] train loss: 1.1284, train acc: 0.5299, val loss: 1.1283, val acc: 0.5379  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 18440] train loss: 1.1508, train acc: 0.5236, val loss: 1.1264, val acc: 0.5336  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 18460] train loss: 1.1418, train acc: 0.5234, val loss: 1.1261, val acc: 0.5319  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 18480] train loss: 1.1466, train acc: 0.5191, val loss: 1.1255, val acc: 0.5359  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 18500] train loss: 1.1199, train acc: 0.5382, val loss: 1.1255, val acc: 0.5305  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 18520] train loss: 1.1431, train acc: 0.5281, val loss: 1.1255, val acc: 0.5305  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 18540] train loss: 1.1324, train acc: 0.5325, val loss: 1.1300, val acc: 0.5342  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 18560] train loss: 1.1462, train acc: 0.5278, val loss: 1.1252, val acc: 0.5352  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 18580] train loss: 1.1288, train acc: 0.5346, val loss: 1.1277, val acc: 0.5248  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 18600] train loss: 1.1296, train acc: 0.5320, val loss: 1.1254, val acc: 0.5312  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 18620] train loss: 1.1244, train acc: 0.5416, val loss: 1.1251, val acc: 0.5346  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 18640] train loss: 1.1338, train acc: 0.5325, val loss: 1.1253, val acc: 0.5336  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 18660] train loss: 1.1281, train acc: 0.5312, val loss: 1.1253, val acc: 0.5366  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 18680] train loss: 1.1192, train acc: 0.5342, val loss: 1.1256, val acc: 0.5329  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 18700] train loss: 1.1246, train acc: 0.5331, val loss: 1.1263, val acc: 0.5366  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 18720] train loss: 1.1271, train acc: 0.5343, val loss: 1.1256, val acc: 0.5363  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 18740] train loss: 1.1278, train acc: 0.5331, val loss: 1.1268, val acc: 0.5379  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 18760] train loss: 1.1292, train acc: 0.5364, val loss: 1.1245, val acc: 0.5322  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 18780] train loss: 1.1297, train acc: 0.5310, val loss: 1.1274, val acc: 0.5359  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 18800] train loss: 1.1227, train acc: 0.5356, val loss: 1.1244, val acc: 0.5342  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 18820] train loss: 1.1394, train acc: 0.5234, val loss: 1.1260, val acc: 0.5282  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 18840] train loss: 1.1366, train acc: 0.5306, val loss: 1.1248, val acc: 0.5346  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 18860] train loss: 1.1270, train acc: 0.5312, val loss: 1.1243, val acc: 0.5346  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 18880] train loss: 1.1294, train acc: 0.5375, val loss: 1.1245, val acc: 0.5322  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 18900] train loss: 1.1297, train acc: 0.5316, val loss: 1.1291, val acc: 0.5349  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 18920] train loss: 1.1297, train acc: 0.5305, val loss: 1.1244, val acc: 0.5359  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 18940] train loss: 1.1341, train acc: 0.5320, val loss: 1.1243, val acc: 0.5352  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 18960] train loss: 1.1255, train acc: 0.5312, val loss: 1.1241, val acc: 0.5336  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 18980] train loss: 1.1359, train acc: 0.5279, val loss: 1.1243, val acc: 0.5332  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 19000] train loss: 1.1254, train acc: 0.5346, val loss: 1.1342, val acc: 0.5359  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 19020] train loss: 1.1377, train acc: 0.5328, val loss: 1.1259, val acc: 0.5390  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 19040] train loss: 1.1339, train acc: 0.5300, val loss: 1.1252, val acc: 0.5288  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 19060] train loss: 1.1482, train acc: 0.5301, val loss: 1.1242, val acc: 0.5336  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 19080] train loss: 1.1345, train acc: 0.5329, val loss: 1.1241, val acc: 0.5305  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 19100] train loss: 1.1236, train acc: 0.5404, val loss: 1.1238, val acc: 0.5369  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 19120] train loss: 1.1273, train acc: 0.5331, val loss: 1.1248, val acc: 0.5319  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 19140] train loss: 1.1353, train acc: 0.5292, val loss: 1.1241, val acc: 0.5319  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 19160] train loss: 1.1468, train acc: 0.5299, val loss: 1.1241, val acc: 0.5369  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 19180] train loss: 1.1238, train acc: 0.5375, val loss: 1.1256, val acc: 0.5379  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 19200] train loss: 1.1232, train acc: 0.5306, val loss: 1.1236, val acc: 0.5329  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 19220] train loss: 1.1367, train acc: 0.5302, val loss: 1.1268, val acc: 0.5376  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 19240] train loss: 1.1421, train acc: 0.5283, val loss: 1.1371, val acc: 0.5336  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 19260] train loss: 1.1402, train acc: 0.5211, val loss: 1.1238, val acc: 0.5366  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 19280] train loss: 1.1357, train acc: 0.5278, val loss: 1.1236, val acc: 0.5322  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 19300] train loss: 1.1360, train acc: 0.5291, val loss: 1.1269, val acc: 0.5228  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 19320] train loss: 1.1249, train acc: 0.5332, val loss: 1.1234, val acc: 0.5346  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 19340] train loss: 1.1230, train acc: 0.5404, val loss: 1.1256, val acc: 0.5379  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 19360] train loss: 1.1235, train acc: 0.5369, val loss: 1.1236, val acc: 0.5319  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 19380] train loss: 1.1447, train acc: 0.5260, val loss: 1.1235, val acc: 0.5356  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 19400] train loss: 1.1268, train acc: 0.5344, val loss: 1.1268, val acc: 0.5359  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 19420] train loss: 1.1373, train acc: 0.5274, val loss: 1.1242, val acc: 0.5295  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 19440] train loss: 1.1332, train acc: 0.5314, val loss: 1.1234, val acc: 0.5352  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 19460] train loss: 1.1348, train acc: 0.5278, val loss: 1.1233, val acc: 0.5366  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 19480] train loss: 1.1209, train acc: 0.5369, val loss: 1.1230, val acc: 0.5342  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 19500] train loss: 1.1499, train acc: 0.5278, val loss: 1.1274, val acc: 0.5356  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 19520] train loss: 1.1237, train acc: 0.5358, val loss: 1.1227, val acc: 0.5359  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 19540] train loss: 1.1236, train acc: 0.5374, val loss: 1.1230, val acc: 0.5339  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 19560] train loss: 1.1265, train acc: 0.5382, val loss: 1.1229, val acc: 0.5359  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 19580] train loss: 1.1198, train acc: 0.5386, val loss: 1.1245, val acc: 0.5369  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 19600] train loss: 1.1324, train acc: 0.5290, val loss: 1.1233, val acc: 0.5369  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 19620] train loss: 1.1135, train acc: 0.5389, val loss: 1.1228, val acc: 0.5352  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 19640] train loss: 1.1390, train acc: 0.5291, val loss: 1.1223, val acc: 0.5352  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 19660] train loss: 1.1245, train acc: 0.5413, val loss: 1.1277, val acc: 0.5349  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 19680] train loss: 1.1324, train acc: 0.5337, val loss: 1.1233, val acc: 0.5373  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 19700] train loss: 1.1371, train acc: 0.5303, val loss: 1.1223, val acc: 0.5366  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 19720] train loss: 1.1264, train acc: 0.5343, val loss: 1.1225, val acc: 0.5366  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 19740] train loss: 1.1217, train acc: 0.5335, val loss: 1.1234, val acc: 0.5305  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 19760] train loss: 1.1244, train acc: 0.5385, val loss: 1.1222, val acc: 0.5369  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 19780] train loss: 1.1403, train acc: 0.5271, val loss: 1.1239, val acc: 0.5359  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 19800] train loss: 1.1149, train acc: 0.5451, val loss: 1.1229, val acc: 0.5278  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 19820] train loss: 1.1441, train acc: 0.5257, val loss: 1.1225, val acc: 0.5359  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 19840] train loss: 1.1325, train acc: 0.5289, val loss: 1.1217, val acc: 0.5363  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 19860] train loss: 1.1448, train acc: 0.5225, val loss: 1.1238, val acc: 0.5383  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 19880] train loss: 1.1273, train acc: 0.5380, val loss: 1.1225, val acc: 0.5285  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 19900] train loss: 1.1061, train acc: 0.5463, val loss: 1.1237, val acc: 0.5255  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 19920] train loss: 1.1239, train acc: 0.5345, val loss: 1.1215, val acc: 0.5319  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 19940] train loss: 1.1275, train acc: 0.5307, val loss: 1.1255, val acc: 0.5359  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 19960] train loss: 1.1132, train acc: 0.5367, val loss: 1.1240, val acc: 0.5251  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 19980] train loss: 1.1314, train acc: 0.5269, val loss: 1.1225, val acc: 0.5282  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 20000] train loss: 1.1427, train acc: 0.5254, val loss: 1.1229, val acc: 0.5369  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 20020] train loss: 1.1372, train acc: 0.5291, val loss: 1.1218, val acc: 0.5373  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 20040] train loss: 1.1278, train acc: 0.5335, val loss: 1.1217, val acc: 0.5342  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 20060] train loss: 1.1472, train acc: 0.5271, val loss: 1.1214, val acc: 0.5312  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 20080] train loss: 1.1323, train acc: 0.5268, val loss: 1.1249, val acc: 0.5349  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 20100] train loss: 1.1307, train acc: 0.5338, val loss: 1.1217, val acc: 0.5379  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 20120] train loss: 1.1461, train acc: 0.5208, val loss: 1.1212, val acc: 0.5329  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 20140] train loss: 1.1006, train acc: 0.5491, val loss: 1.1223, val acc: 0.5285  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 20160] train loss: 1.1295, train acc: 0.5299, val loss: 1.1207, val acc: 0.5346  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 20180] train loss: 1.1222, train acc: 0.5374, val loss: 1.1208, val acc: 0.5363  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 20200] train loss: 1.1296, train acc: 0.5296, val loss: 1.1210, val acc: 0.5342  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 20220] train loss: 1.1225, train acc: 0.5368, val loss: 1.1212, val acc: 0.5356  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 20240] train loss: 1.1214, train acc: 0.5356, val loss: 1.1230, val acc: 0.5258  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 20260] train loss: 1.1474, train acc: 0.5270, val loss: 1.1211, val acc: 0.5329  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 20280] train loss: 1.1204, train acc: 0.5337, val loss: 1.1208, val acc: 0.5342  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 20300] train loss: 1.1268, train acc: 0.5338, val loss: 1.1251, val acc: 0.5231  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 20320] train loss: 1.1126, train acc: 0.5414, val loss: 1.1208, val acc: 0.5336  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 20340] train loss: 1.1392, train acc: 0.5254, val loss: 1.1208, val acc: 0.5356  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 20360] train loss: 1.1087, train acc: 0.5457, val loss: 1.1215, val acc: 0.5366  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 20380] train loss: 1.1395, train acc: 0.5232, val loss: 1.1201, val acc: 0.5356  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 20400] train loss: 1.1099, train acc: 0.5442, val loss: 1.1206, val acc: 0.5336  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 20420] train loss: 1.1325, train acc: 0.5290, val loss: 1.1201, val acc: 0.5325  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 20440] train loss: 1.1407, train acc: 0.5260, val loss: 1.1199, val acc: 0.5339  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 20460] train loss: 1.1231, train acc: 0.5372, val loss: 1.1205, val acc: 0.5332  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 20480] train loss: 1.1277, train acc: 0.5282, val loss: 1.1219, val acc: 0.5298  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 20500] train loss: 1.1121, train acc: 0.5372, val loss: 1.1200, val acc: 0.5312  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 20520] train loss: 1.1295, train acc: 0.5312, val loss: 1.1217, val acc: 0.5383  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 20540] train loss: 1.1273, train acc: 0.5393, val loss: 1.1219, val acc: 0.5373  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 20560] train loss: 1.1155, train acc: 0.5380, val loss: 1.1199, val acc: 0.5332  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 20580] train loss: 1.1331, train acc: 0.5337, val loss: 1.1195, val acc: 0.5352  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 20600] train loss: 1.1347, train acc: 0.5278, val loss: 1.1235, val acc: 0.5373  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 20620] train loss: 1.1428, train acc: 0.5209, val loss: 1.1194, val acc: 0.5356  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 20640] train loss: 1.1562, train acc: 0.5175, val loss: 1.1204, val acc: 0.5369  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 20660] train loss: 1.1140, train acc: 0.5450, val loss: 1.1219, val acc: 0.5379  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 20680] train loss: 1.1394, train acc: 0.5304, val loss: 1.1209, val acc: 0.5400  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 20700] train loss: 1.1121, train acc: 0.5367, val loss: 1.1198, val acc: 0.5329  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 20720] train loss: 1.1386, train acc: 0.5228, val loss: 1.1200, val acc: 0.5359  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 20740] train loss: 1.1281, train acc: 0.5346, val loss: 1.1215, val acc: 0.5261  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 20760] train loss: 1.1386, train acc: 0.5276, val loss: 1.1198, val acc: 0.5356  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 20780] train loss: 1.1305, train acc: 0.5289, val loss: 1.1202, val acc: 0.5403  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 20800] train loss: 1.1224, train acc: 0.5328, val loss: 1.1210, val acc: 0.5298  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 20820] train loss: 1.1169, train acc: 0.5368, val loss: 1.1189, val acc: 0.5352  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 20840] train loss: 1.1259, train acc: 0.5359, val loss: 1.1190, val acc: 0.5356  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 20860] train loss: 1.1256, train acc: 0.5359, val loss: 1.1189, val acc: 0.5356  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 20880] train loss: 1.1178, train acc: 0.5371, val loss: 1.1191, val acc: 0.5342  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 20900] train loss: 1.1365, train acc: 0.5296, val loss: 1.1201, val acc: 0.5393  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 20920] train loss: 1.1452, train acc: 0.5229, val loss: 1.1227, val acc: 0.5376  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 20940] train loss: 1.1154, train acc: 0.5418, val loss: 1.1246, val acc: 0.5356  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 20960] train loss: 1.1209, train acc: 0.5337, val loss: 1.1194, val acc: 0.5369  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 20980] train loss: 1.1281, train acc: 0.5322, val loss: 1.1229, val acc: 0.5234  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 21000] train loss: 1.1042, train acc: 0.5407, val loss: 1.1210, val acc: 0.5251  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 21020] train loss: 1.1112, train acc: 0.5395, val loss: 1.1192, val acc: 0.5342  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 21040] train loss: 1.1256, train acc: 0.5323, val loss: 1.1199, val acc: 0.5403  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 21060] train loss: 1.1075, train acc: 0.5437, val loss: 1.1187, val acc: 0.5376  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 21080] train loss: 1.1298, train acc: 0.5300, val loss: 1.1200, val acc: 0.5396  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 21100] train loss: 1.1210, train acc: 0.5312, val loss: 1.1199, val acc: 0.5390  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 21120] train loss: 1.1258, train acc: 0.5310, val loss: 1.1182, val acc: 0.5346  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 21140] train loss: 1.1462, train acc: 0.5267, val loss: 1.1186, val acc: 0.5309  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 21160] train loss: 1.1141, train acc: 0.5374, val loss: 1.1181, val acc: 0.5366  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 21180] train loss: 1.1423, train acc: 0.5262, val loss: 1.1180, val acc: 0.5349  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 21200] train loss: 1.1435, train acc: 0.5215, val loss: 1.1196, val acc: 0.5390  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 21220] train loss: 1.1265, train acc: 0.5286, val loss: 1.1181, val acc: 0.5352  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 21240] train loss: 1.1327, train acc: 0.5296, val loss: 1.1178, val acc: 0.5359  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 21260] train loss: 1.1377, train acc: 0.5267, val loss: 1.1176, val acc: 0.5352  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 21280] train loss: 1.1224, train acc: 0.5385, val loss: 1.1202, val acc: 0.5379  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 21300] train loss: 1.1360, train acc: 0.5257, val loss: 1.1176, val acc: 0.5356  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 21320] train loss: 1.1160, train acc: 0.5370, val loss: 1.1195, val acc: 0.5379  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 21340] train loss: 1.1413, train acc: 0.5233, val loss: 1.1190, val acc: 0.5393  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 21360] train loss: 1.1151, train acc: 0.5430, val loss: 1.1172, val acc: 0.5356  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 21380] train loss: 1.1248, train acc: 0.5360, val loss: 1.1185, val acc: 0.5393  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 21400] train loss: 1.1125, train acc: 0.5417, val loss: 1.1174, val acc: 0.5359  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 21420] train loss: 1.1225, train acc: 0.5221, val loss: 1.1176, val acc: 0.5342  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 21440] train loss: 1.1274, train acc: 0.5338, val loss: 1.1183, val acc: 0.5309  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 21460] train loss: 1.1252, train acc: 0.5306, val loss: 1.1171, val acc: 0.5352  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 21480] train loss: 1.1215, train acc: 0.5341, val loss: 1.1173, val acc: 0.5363  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 21500] train loss: 1.1295, train acc: 0.5331, val loss: 1.1174, val acc: 0.5315  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 21520] train loss: 1.1156, train acc: 0.5424, val loss: 1.1169, val acc: 0.5339  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 21540] train loss: 1.1256, train acc: 0.5321, val loss: 1.1167, val acc: 0.5373  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 21560] train loss: 1.1170, train acc: 0.5388, val loss: 1.1170, val acc: 0.5383  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 21580] train loss: 1.1061, train acc: 0.5455, val loss: 1.1167, val acc: 0.5376  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 21600] train loss: 1.1361, train acc: 0.5256, val loss: 1.1169, val acc: 0.5369  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 21620] train loss: 1.1174, train acc: 0.5374, val loss: 1.1173, val acc: 0.5302  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 21640] train loss: 1.1245, train acc: 0.5347, val loss: 1.1213, val acc: 0.5376  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 21660] train loss: 1.1331, train acc: 0.5293, val loss: 1.1179, val acc: 0.5295  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 21680] train loss: 1.1239, train acc: 0.5359, val loss: 1.1165, val acc: 0.5329  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 21700] train loss: 1.1575, train acc: 0.5200, val loss: 1.1177, val acc: 0.5329  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 21720] train loss: 1.1382, train acc: 0.5296, val loss: 1.1165, val acc: 0.5336  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 21740] train loss: 1.1216, train acc: 0.5357, val loss: 1.1160, val acc: 0.5356  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 21760] train loss: 1.1404, train acc: 0.5257, val loss: 1.1163, val acc: 0.5373  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 21780] train loss: 1.1127, train acc: 0.5364, val loss: 1.1163, val acc: 0.5325  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 21800] train loss: 1.1300, train acc: 0.5320, val loss: 1.1159, val acc: 0.5336  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 21820] train loss: 1.1247, train acc: 0.5326, val loss: 1.1175, val acc: 0.5400  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 21840] train loss: 1.1202, train acc: 0.5326, val loss: 1.1159, val acc: 0.5346  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 21860] train loss: 1.1272, train acc: 0.5300, val loss: 1.1172, val acc: 0.5403  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 21880] train loss: 1.1170, train acc: 0.5402, val loss: 1.1195, val acc: 0.5379  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 21900] train loss: 1.1227, train acc: 0.5333, val loss: 1.1154, val acc: 0.5336  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 21920] train loss: 1.1062, train acc: 0.5419, val loss: 1.1160, val acc: 0.5336  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 21940] train loss: 1.1250, train acc: 0.5332, val loss: 1.1164, val acc: 0.5309  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 21960] train loss: 1.1154, train acc: 0.5309, val loss: 1.1167, val acc: 0.5352  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 21980] train loss: 1.1276, train acc: 0.5324, val loss: 1.1162, val acc: 0.5359  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 22000] train loss: 1.1176, train acc: 0.5376, val loss: 1.1165, val acc: 0.5356  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 22020] train loss: 1.1226, train acc: 0.5361, val loss: 1.1238, val acc: 0.5352  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 22040] train loss: 1.1380, train acc: 0.5261, val loss: 1.1158, val acc: 0.5376  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 22060] train loss: 1.1402, train acc: 0.5273, val loss: 1.1157, val acc: 0.5383  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 22080] train loss: 1.1263, train acc: 0.5296, val loss: 1.1159, val acc: 0.5379  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 22100] train loss: 1.1248, train acc: 0.5323, val loss: 1.1170, val acc: 0.5383  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 22120] train loss: 1.1238, train acc: 0.5368, val loss: 1.1144, val acc: 0.5352  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 22140] train loss: 1.1022, train acc: 0.5424, val loss: 1.1143, val acc: 0.5339  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 22160] train loss: 1.1020, train acc: 0.5440, val loss: 1.1169, val acc: 0.5393  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 22180] train loss: 1.1185, train acc: 0.5345, val loss: 1.1143, val acc: 0.5363  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 22200] train loss: 1.1223, train acc: 0.5393, val loss: 1.1152, val acc: 0.5376  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 22220] train loss: 1.1220, train acc: 0.5330, val loss: 1.1156, val acc: 0.5352  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 22240] train loss: 1.1370, train acc: 0.5265, val loss: 1.1157, val acc: 0.5285  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 22260] train loss: 1.1284, train acc: 0.5294, val loss: 1.1166, val acc: 0.5406  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 22280] train loss: 1.1210, train acc: 0.5321, val loss: 1.1159, val acc: 0.5379  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 22300] train loss: 1.1335, train acc: 0.5292, val loss: 1.1168, val acc: 0.5396  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 22320] train loss: 1.1423, train acc: 0.5239, val loss: 1.1164, val acc: 0.5393  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 22340] train loss: 1.1138, train acc: 0.5380, val loss: 1.1147, val acc: 0.5325  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 22360] train loss: 1.1224, train acc: 0.5322, val loss: 1.1143, val acc: 0.5302  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 22380] train loss: 1.1097, train acc: 0.5414, val loss: 1.1149, val acc: 0.5356  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 22400] train loss: 1.1249, train acc: 0.5327, val loss: 1.1136, val acc: 0.5376  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 22420] train loss: 1.1137, train acc: 0.5390, val loss: 1.1176, val acc: 0.5390  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 22440] train loss: 1.1231, train acc: 0.5283, val loss: 1.1143, val acc: 0.5346  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 22460] train loss: 1.1251, train acc: 0.5338, val loss: 1.1142, val acc: 0.5329  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 22480] train loss: 1.1313, train acc: 0.5286, val loss: 1.1160, val acc: 0.5406  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 22500] train loss: 1.1494, train acc: 0.5224, val loss: 1.1134, val acc: 0.5352  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 22520] train loss: 1.1258, train acc: 0.5358, val loss: 1.1132, val acc: 0.5352  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 22540] train loss: 1.1126, train acc: 0.5416, val loss: 1.1180, val acc: 0.5403  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 22560] train loss: 1.1089, train acc: 0.5336, val loss: 1.1130, val acc: 0.5325  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 22580] train loss: 1.1263, train acc: 0.5323, val loss: 1.1150, val acc: 0.5366  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 22600] train loss: 1.1334, train acc: 0.5275, val loss: 1.1163, val acc: 0.5268  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 22620] train loss: 1.1229, train acc: 0.5343, val loss: 1.1128, val acc: 0.5359  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 22640] train loss: 1.1099, train acc: 0.5447, val loss: 1.1127, val acc: 0.5376  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 22660] train loss: 1.1012, train acc: 0.5434, val loss: 1.1141, val acc: 0.5403  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 22680] train loss: 1.1075, train acc: 0.5389, val loss: 1.1135, val acc: 0.5390  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 22700] train loss: 1.1114, train acc: 0.5346, val loss: 1.1142, val acc: 0.5386  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 22720] train loss: 1.1115, train acc: 0.5356, val loss: 1.1131, val acc: 0.5366  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 22740] train loss: 1.1127, train acc: 0.5451, val loss: 1.1137, val acc: 0.5390  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 22760] train loss: 1.1132, train acc: 0.5371, val loss: 1.1170, val acc: 0.5383  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 22780] train loss: 1.1348, train acc: 0.5270, val loss: 1.1141, val acc: 0.5373  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 22800] train loss: 1.1215, train acc: 0.5331, val loss: 1.1157, val acc: 0.5376  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 22820] train loss: 1.1121, train acc: 0.5390, val loss: 1.1132, val acc: 0.5396  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 22840] train loss: 1.1364, train acc: 0.5278, val loss: 1.1156, val acc: 0.5396  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 22860] train loss: 1.1310, train acc: 0.5262, val loss: 1.1127, val acc: 0.5386  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 22880] train loss: 1.1227, train acc: 0.5342, val loss: 1.1135, val acc: 0.5386  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 22900] train loss: 1.1141, train acc: 0.5382, val loss: 1.1146, val acc: 0.5410  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 22920] train loss: 1.1423, train acc: 0.5253, val loss: 1.1130, val acc: 0.5379  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 22940] train loss: 1.1266, train acc: 0.5328, val loss: 1.1118, val acc: 0.5379  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 22960] train loss: 1.1344, train acc: 0.5307, val loss: 1.1120, val acc: 0.5329  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 22980] train loss: 1.1110, train acc: 0.5373, val loss: 1.1114, val acc: 0.5356  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 23000] train loss: 1.1202, train acc: 0.5317, val loss: 1.1112, val acc: 0.5349  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 23020] train loss: 1.1260, train acc: 0.5331, val loss: 1.1130, val acc: 0.5298  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 23040] train loss: 1.1251, train acc: 0.5308, val loss: 1.1122, val acc: 0.5369  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 23060] train loss: 1.1251, train acc: 0.5360, val loss: 1.1154, val acc: 0.5386  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 23080] train loss: 1.1166, train acc: 0.5320, val loss: 1.1110, val acc: 0.5359  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 23100] train loss: 1.1289, train acc: 0.5270, val loss: 1.1107, val acc: 0.5346  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 23120] train loss: 1.1283, train acc: 0.5325, val loss: 1.1108, val acc: 0.5379  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 23140] train loss: 1.1270, train acc: 0.5331, val loss: 1.1108, val acc: 0.5336  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 23160] train loss: 1.1249, train acc: 0.5325, val loss: 1.1108, val acc: 0.5383  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 23180] train loss: 1.1090, train acc: 0.5419, val loss: 1.1146, val acc: 0.5403  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 23200] train loss: 1.1133, train acc: 0.5364, val loss: 1.1149, val acc: 0.5376  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 23220] train loss: 1.1310, train acc: 0.5276, val loss: 1.1118, val acc: 0.5413  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 23240] train loss: 1.1344, train acc: 0.5306, val loss: 1.1111, val acc: 0.5376  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 23260] train loss: 1.1070, train acc: 0.5462, val loss: 1.1143, val acc: 0.5332  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 23280] train loss: 1.1238, train acc: 0.5324, val loss: 1.1103, val acc: 0.5342  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 23300] train loss: 1.1136, train acc: 0.5395, val loss: 1.1103, val acc: 0.5359  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 23320] train loss: 1.1183, train acc: 0.5372, val loss: 1.1098, val acc: 0.5349  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 23340] train loss: 1.1079, train acc: 0.5419, val loss: 1.1115, val acc: 0.5282  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 23360] train loss: 1.1073, train acc: 0.5364, val loss: 1.1093, val acc: 0.5376  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 23380] train loss: 1.1153, train acc: 0.5337, val loss: 1.1093, val acc: 0.5349  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 23400] train loss: 1.1260, train acc: 0.5330, val loss: 1.1098, val acc: 0.5379  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 23420] train loss: 1.1197, train acc: 0.5364, val loss: 1.1090, val acc: 0.5346  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 23440] train loss: 1.1080, train acc: 0.5350, val loss: 1.1091, val acc: 0.5373  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 23460] train loss: 1.1177, train acc: 0.5360, val loss: 1.1088, val acc: 0.5329  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 23480] train loss: 1.1190, train acc: 0.5371, val loss: 1.1089, val acc: 0.5383  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 23500] train loss: 1.1218, train acc: 0.5314, val loss: 1.1089, val acc: 0.5359  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 23520] train loss: 1.1001, train acc: 0.5416, val loss: 1.1091, val acc: 0.5369  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 23540] train loss: 1.1110, train acc: 0.5421, val loss: 1.1086, val acc: 0.5349  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 23560] train loss: 1.1169, train acc: 0.5386, val loss: 1.1084, val acc: 0.5363  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 23580] train loss: 1.1175, train acc: 0.5333, val loss: 1.1086, val acc: 0.5369  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 23600] train loss: 1.1065, train acc: 0.5415, val loss: 1.1115, val acc: 0.5403  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 23620] train loss: 1.1124, train acc: 0.5369, val loss: 1.1106, val acc: 0.5356  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 23640] train loss: 1.1219, train acc: 0.5307, val loss: 1.1099, val acc: 0.5315  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 23660] train loss: 1.1113, train acc: 0.5342, val loss: 1.1083, val acc: 0.5339  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 23680] train loss: 1.1194, train acc: 0.5314, val loss: 1.1078, val acc: 0.5339  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 23700] train loss: 1.1132, train acc: 0.5320, val loss: 1.1086, val acc: 0.5363  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 23720] train loss: 1.1167, train acc: 0.5317, val loss: 1.1096, val acc: 0.5386  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 23740] train loss: 1.1064, train acc: 0.5419, val loss: 1.1094, val acc: 0.5349  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 23760] train loss: 1.1289, train acc: 0.5287, val loss: 1.1096, val acc: 0.5417  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 23780] train loss: 1.1302, train acc: 0.5249, val loss: 1.1108, val acc: 0.5400  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 23800] train loss: 1.1251, train acc: 0.5310, val loss: 1.1077, val acc: 0.5390  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 23820] train loss: 1.1245, train acc: 0.5297, val loss: 1.1098, val acc: 0.5417  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 23840] train loss: 1.1059, train acc: 0.5382, val loss: 1.1079, val acc: 0.5332  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 23860] train loss: 1.1167, train acc: 0.5335, val loss: 1.1086, val acc: 0.5386  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 23880] train loss: 1.1133, train acc: 0.5374, val loss: 1.1103, val acc: 0.5403  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 23900] train loss: 1.1145, train acc: 0.5373, val loss: 1.1066, val acc: 0.5329  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 23920] train loss: 1.1068, train acc: 0.5416, val loss: 1.1091, val acc: 0.5410  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 23940] train loss: 1.1194, train acc: 0.5318, val loss: 1.1061, val acc: 0.5359  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 23960] train loss: 1.1201, train acc: 0.5330, val loss: 1.1065, val acc: 0.5352  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 23980] train loss: 1.1102, train acc: 0.5414, val loss: 1.1097, val acc: 0.5379  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 24000] train loss: 1.1236, train acc: 0.5327, val loss: 1.1062, val acc: 0.5373  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 24020] train loss: 1.1001, train acc: 0.5399, val loss: 1.1059, val acc: 0.5366  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 24040] train loss: 1.1182, train acc: 0.5320, val loss: 1.1066, val acc: 0.5298  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 24060] train loss: 1.1065, train acc: 0.5398, val loss: 1.1051, val acc: 0.5403  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 24080] train loss: 1.1164, train acc: 0.5361, val loss: 1.1049, val acc: 0.5339  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 24100] train loss: 1.1237, train acc: 0.5328, val loss: 1.1055, val acc: 0.5339  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 24120] train loss: 1.1174, train acc: 0.5309, val loss: 1.1075, val acc: 0.5329  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 24140] train loss: 1.1136, train acc: 0.5344, val loss: 1.1062, val acc: 0.5440  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 24160] train loss: 1.1224, train acc: 0.5294, val loss: 1.1051, val acc: 0.5349  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 24180] train loss: 1.1140, train acc: 0.5324, val loss: 1.1056, val acc: 0.5298  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 24200] train loss: 1.1115, train acc: 0.5369, val loss: 1.1051, val acc: 0.5390  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 24220] train loss: 1.0991, train acc: 0.5452, val loss: 1.1071, val acc: 0.5383  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 24240] train loss: 1.1162, train acc: 0.5335, val loss: 1.1050, val acc: 0.5329  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 24260] train loss: 1.1189, train acc: 0.5328, val loss: 1.1107, val acc: 0.5413  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 24280] train loss: 1.1299, train acc: 0.5266, val loss: 1.1043, val acc: 0.5366  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 24300] train loss: 1.1156, train acc: 0.5354, val loss: 1.1036, val acc: 0.5403  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 24320] train loss: 1.1046, train acc: 0.5360, val loss: 1.1047, val acc: 0.5417  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 24340] train loss: 1.1159, train acc: 0.5345, val loss: 1.1031, val acc: 0.5363  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 24360] train loss: 1.1204, train acc: 0.5319, val loss: 1.1040, val acc: 0.5379  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 24380] train loss: 1.1201, train acc: 0.5341, val loss: 1.1045, val acc: 0.5417  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 24400] train loss: 1.1304, train acc: 0.5225, val loss: 1.1061, val acc: 0.5396  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 24420] train loss: 1.1008, train acc: 0.5406, val loss: 1.1068, val acc: 0.5413  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 24440] train loss: 1.1097, train acc: 0.5370, val loss: 1.1039, val acc: 0.5366  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 24460] train loss: 1.0906, train acc: 0.5501, val loss: 1.1028, val acc: 0.5359  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 24480] train loss: 1.1047, train acc: 0.5386, val loss: 1.1030, val acc: 0.5376  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 24500] train loss: 1.1102, train acc: 0.5367, val loss: 1.1066, val acc: 0.5349  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 24520] train loss: 1.1058, train acc: 0.5389, val loss: 1.1036, val acc: 0.5396  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 24540] train loss: 1.1119, train acc: 0.5415, val loss: 1.1021, val acc: 0.5406  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 24560] train loss: 1.1199, train acc: 0.5329, val loss: 1.1016, val acc: 0.5373  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 24580] train loss: 1.1223, train acc: 0.5296, val loss: 1.1014, val acc: 0.5400  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 24600] train loss: 1.1040, train acc: 0.5351, val loss: 1.1021, val acc: 0.5352  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 24620] train loss: 1.1094, train acc: 0.5314, val loss: 1.1011, val acc: 0.5393  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 24640] train loss: 1.1165, train acc: 0.5333, val loss: 1.1046, val acc: 0.5390  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 24660] train loss: 1.1133, train acc: 0.5369, val loss: 1.1017, val acc: 0.5400  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 24680] train loss: 1.1175, train acc: 0.5301, val loss: 1.1009, val acc: 0.5396  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 24700] train loss: 1.1144, train acc: 0.5326, val loss: 1.1018, val acc: 0.5379  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 24720] train loss: 1.0962, train acc: 0.5479, val loss: 1.1021, val acc: 0.5363  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 24740] train loss: 1.1169, train acc: 0.5319, val loss: 1.1021, val acc: 0.5400  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 24760] train loss: 1.0978, train acc: 0.5457, val loss: 1.1015, val acc: 0.5390  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 24780] train loss: 1.1013, train acc: 0.5421, val loss: 1.1004, val acc: 0.5352  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 24800] train loss: 1.0997, train acc: 0.5421, val loss: 1.1051, val acc: 0.5359  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 24820] train loss: 1.0881, train acc: 0.5510, val loss: 1.1003, val acc: 0.5363  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 24840] train loss: 1.1049, train acc: 0.5381, val loss: 1.1043, val acc: 0.5396  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 24860] train loss: 1.1030, train acc: 0.5405, val loss: 1.1015, val acc: 0.5417  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 24880] train loss: 1.1160, train acc: 0.5340, val loss: 1.1006, val acc: 0.5366  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 24900] train loss: 1.1095, train acc: 0.5406, val loss: 1.1010, val acc: 0.5406  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 24920] train loss: 1.1236, train acc: 0.5267, val loss: 1.1058, val acc: 0.5383  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 24940] train loss: 1.0948, train acc: 0.5428, val loss: 1.1014, val acc: 0.5373  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 24960] train loss: 1.1183, train acc: 0.5325, val loss: 1.1024, val acc: 0.5383  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 24980] train loss: 1.1179, train acc: 0.5329, val loss: 1.1031, val acc: 0.5386  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 25000] train loss: 1.1106, train acc: 0.5468, val loss: 1.1060, val acc: 0.5356  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 25020] train loss: 1.0909, train acc: 0.5469, val loss: 1.1015, val acc: 0.5366  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 25040] train loss: 1.1224, train acc: 0.5286, val loss: 1.1034, val acc: 0.5295  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 25060] train loss: 1.1128, train acc: 0.5351, val loss: 1.0999, val acc: 0.5383  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 25080] train loss: 1.1057, train acc: 0.5430, val loss: 1.0997, val acc: 0.5406  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 25100] train loss: 1.1092, train acc: 0.5381, val loss: 1.1006, val acc: 0.5363  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 25120] train loss: 1.0979, train acc: 0.5481, val loss: 1.0990, val acc: 0.5342  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 25140] train loss: 1.1037, train acc: 0.5401, val loss: 1.0985, val acc: 0.5403  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 25160] train loss: 1.0989, train acc: 0.5405, val loss: 1.0987, val acc: 0.5403  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 25180] train loss: 1.1090, train acc: 0.5376, val loss: 1.0984, val acc: 0.5339  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 25200] train loss: 1.1055, train acc: 0.5427, val loss: 1.0988, val acc: 0.5400  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 25220] train loss: 1.0919, train acc: 0.5481, val loss: 1.0999, val acc: 0.5410  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 25240] train loss: 1.1239, train acc: 0.5291, val loss: 1.1000, val acc: 0.5406  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 25260] train loss: 1.1048, train acc: 0.5405, val loss: 1.0980, val acc: 0.5386  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 25280] train loss: 1.1099, train acc: 0.5372, val loss: 1.0997, val acc: 0.5363  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 25300] train loss: 1.1045, train acc: 0.5429, val loss: 1.0980, val acc: 0.5406  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 25320] train loss: 1.1092, train acc: 0.5313, val loss: 1.0979, val acc: 0.5383  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 25340] train loss: 1.0955, train acc: 0.5388, val loss: 1.0976, val acc: 0.5366  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 25360] train loss: 1.1208, train acc: 0.5320, val loss: 1.0987, val acc: 0.5420  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 25380] train loss: 1.1253, train acc: 0.5312, val loss: 1.1006, val acc: 0.5423  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 25400] train loss: 1.1166, train acc: 0.5328, val loss: 1.0993, val acc: 0.5413  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 25420] train loss: 1.1108, train acc: 0.5324, val loss: 1.0985, val acc: 0.5363  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 25440] train loss: 1.0875, train acc: 0.5510, val loss: 1.0989, val acc: 0.5356  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 25460] train loss: 1.1015, train acc: 0.5458, val loss: 1.0971, val acc: 0.5359  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 25480] train loss: 1.1098, train acc: 0.5367, val loss: 1.0981, val acc: 0.5386  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 25500] train loss: 1.1013, train acc: 0.5446, val loss: 1.0975, val acc: 0.5356  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 25520] train loss: 1.1109, train acc: 0.5341, val loss: 1.0968, val acc: 0.5379  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 25540] train loss: 1.1181, train acc: 0.5337, val loss: 1.0988, val acc: 0.5437  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 25560] train loss: 1.1209, train acc: 0.5333, val loss: 1.0993, val acc: 0.5447  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 25580] train loss: 1.1124, train acc: 0.5339, val loss: 1.0977, val acc: 0.5403  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 25600] train loss: 1.0976, train acc: 0.5384, val loss: 1.0966, val acc: 0.5413  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 25620] train loss: 1.1232, train acc: 0.5305, val loss: 1.0969, val acc: 0.5383  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 25640] train loss: 1.1077, train acc: 0.5395, val loss: 1.0968, val acc: 0.5386  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 25660] train loss: 1.0918, train acc: 0.5439, val loss: 1.0987, val acc: 0.5430  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 25680] train loss: 1.1194, train acc: 0.5346, val loss: 1.0962, val acc: 0.5413  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 25700] train loss: 1.1014, train acc: 0.5372, val loss: 1.0998, val acc: 0.5450  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 25720] train loss: 1.1015, train acc: 0.5413, val loss: 1.0973, val acc: 0.5376  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 25740] train loss: 1.0987, train acc: 0.5416, val loss: 1.0970, val acc: 0.5437  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 25760] train loss: 1.1146, train acc: 0.5315, val loss: 1.0986, val acc: 0.5386  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 25780] train loss: 1.0839, train acc: 0.5497, val loss: 1.0955, val acc: 0.5457  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 25800] train loss: 1.1009, train acc: 0.5450, val loss: 1.0975, val acc: 0.5447  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 25820] train loss: 1.0984, train acc: 0.5427, val loss: 1.0964, val acc: 0.5477  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 25840] train loss: 1.1150, train acc: 0.5329, val loss: 1.0972, val acc: 0.5420  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 25860] train loss: 1.0822, train acc: 0.5517, val loss: 1.0969, val acc: 0.5457  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 25880] train loss: 1.0967, train acc: 0.5385, val loss: 1.0950, val acc: 0.5410  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 25900] train loss: 1.1288, train acc: 0.5322, val loss: 1.0953, val acc: 0.5437  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 25920] train loss: 1.0806, train acc: 0.5495, val loss: 1.0942, val acc: 0.5460  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 25940] train loss: 1.1146, train acc: 0.5328, val loss: 1.0956, val acc: 0.5437  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 25960] train loss: 1.0977, train acc: 0.5466, val loss: 1.0946, val acc: 0.5423  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 25980] train loss: 1.0996, train acc: 0.5412, val loss: 1.0939, val acc: 0.5437  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 26000] train loss: 1.1044, train acc: 0.5398, val loss: 1.0941, val acc: 0.5437  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 26020] train loss: 1.1166, train acc: 0.5315, val loss: 1.0960, val acc: 0.5430  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 26040] train loss: 1.1054, train acc: 0.5429, val loss: 1.0997, val acc: 0.5444  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 26060] train loss: 1.1087, train acc: 0.5365, val loss: 1.0938, val acc: 0.5460  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 26080] train loss: 1.1103, train acc: 0.5380, val loss: 1.0931, val acc: 0.5464  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 26100] train loss: 1.1041, train acc: 0.5468, val loss: 1.0927, val acc: 0.5457  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 26120] train loss: 1.1211, train acc: 0.5281, val loss: 1.0935, val acc: 0.5427  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 26140] train loss: 1.1255, train acc: 0.5301, val loss: 1.0935, val acc: 0.5447  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 26160] train loss: 1.0916, train acc: 0.5539, val loss: 1.0949, val acc: 0.5403  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 26180] train loss: 1.1165, train acc: 0.5382, val loss: 1.0926, val acc: 0.5444  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 26200] train loss: 1.0939, train acc: 0.5455, val loss: 1.0945, val acc: 0.5460  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 26220] train loss: 1.1108, train acc: 0.5311, val loss: 1.0922, val acc: 0.5460  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 26240] train loss: 1.1059, train acc: 0.5348, val loss: 1.0917, val acc: 0.5440  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 26260] train loss: 1.0938, train acc: 0.5395, val loss: 1.0943, val acc: 0.5440  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 26280] train loss: 1.0908, train acc: 0.5452, val loss: 1.0942, val acc: 0.5474  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 26300] train loss: 1.1039, train acc: 0.5396, val loss: 1.0962, val acc: 0.5447  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 26320] train loss: 1.1033, train acc: 0.5427, val loss: 1.0924, val acc: 0.5450  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 26340] train loss: 1.0801, train acc: 0.5466, val loss: 1.0969, val acc: 0.5430  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 26360] train loss: 1.1026, train acc: 0.5395, val loss: 1.0937, val acc: 0.5464  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 26380] train loss: 1.0983, train acc: 0.5421, val loss: 1.0929, val acc: 0.5410  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 26400] train loss: 1.0998, train acc: 0.5385, val loss: 1.0916, val acc: 0.5447  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 26420] train loss: 1.1038, train acc: 0.5382, val loss: 1.0928, val acc: 0.5427  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 26440] train loss: 1.0940, train acc: 0.5421, val loss: 1.0981, val acc: 0.5396  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 26460] train loss: 1.1071, train acc: 0.5405, val loss: 1.0918, val acc: 0.5427  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 26480] train loss: 1.0923, train acc: 0.5530, val loss: 1.0921, val acc: 0.5383  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 26500] train loss: 1.1052, train acc: 0.5375, val loss: 1.0920, val acc: 0.5400  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 26520] train loss: 1.1033, train acc: 0.5385, val loss: 1.0911, val acc: 0.5457  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 26540] train loss: 1.0841, train acc: 0.5484, val loss: 1.0912, val acc: 0.5437  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 26560] train loss: 1.0975, train acc: 0.5461, val loss: 1.0912, val acc: 0.5437  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 26580] train loss: 1.1014, train acc: 0.5375, val loss: 1.0920, val acc: 0.5423  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 26600] train loss: 1.1040, train acc: 0.5391, val loss: 1.0930, val acc: 0.5474  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 26620] train loss: 1.1018, train acc: 0.5401, val loss: 1.0930, val acc: 0.5454  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 26640] train loss: 1.1047, train acc: 0.5418, val loss: 1.0910, val acc: 0.5430  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 26660] train loss: 1.0909, train acc: 0.5472, val loss: 1.0921, val acc: 0.5440  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 26680] train loss: 1.1223, train acc: 0.5307, val loss: 1.0915, val acc: 0.5417  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 26700] train loss: 1.0866, train acc: 0.5458, val loss: 1.0910, val acc: 0.5447  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 26720] train loss: 1.0975, train acc: 0.5375, val loss: 1.0919, val acc: 0.5433  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 26740] train loss: 1.1105, train acc: 0.5369, val loss: 1.0948, val acc: 0.5511  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 26760] train loss: 1.1026, train acc: 0.5424, val loss: 1.0919, val acc: 0.5413  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 26780] train loss: 1.1098, train acc: 0.5381, val loss: 1.0915, val acc: 0.5474  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 26800] train loss: 1.1162, train acc: 0.5362, val loss: 1.0904, val acc: 0.5457  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 26820] train loss: 1.1023, train acc: 0.5357, val loss: 1.0895, val acc: 0.5467  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 26840] train loss: 1.0996, train acc: 0.5419, val loss: 1.0905, val acc: 0.5433  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 26860] train loss: 1.1212, train acc: 0.5333, val loss: 1.0897, val acc: 0.5477  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 26880] train loss: 1.0891, train acc: 0.5453, val loss: 1.0898, val acc: 0.5457  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 26900] train loss: 1.1070, train acc: 0.5369, val loss: 1.0892, val acc: 0.5474  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 26920] train loss: 1.1132, train acc: 0.5373, val loss: 1.0905, val acc: 0.5477  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 26940] train loss: 1.1006, train acc: 0.5442, val loss: 1.0909, val acc: 0.5447  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 26960] train loss: 1.0840, train acc: 0.5472, val loss: 1.0940, val acc: 0.5413  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 26980] train loss: 1.1015, train acc: 0.5378, val loss: 1.0904, val acc: 0.5450  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 27000] train loss: 1.1119, train acc: 0.5406, val loss: 1.0926, val acc: 0.5467  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 27020] train loss: 1.0952, train acc: 0.5432, val loss: 1.0899, val acc: 0.5460  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 27040] train loss: 1.1204, train acc: 0.5313, val loss: 1.0909, val acc: 0.5450  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 27060] train loss: 1.0859, train acc: 0.5476, val loss: 1.0915, val acc: 0.5460  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 27080] train loss: 1.1115, train acc: 0.5369, val loss: 1.0889, val acc: 0.5467  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 27100] train loss: 1.0986, train acc: 0.5495, val loss: 1.0901, val acc: 0.5464  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 27120] train loss: 1.0868, train acc: 0.5518, val loss: 1.0895, val acc: 0.5460  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 27140] train loss: 1.1096, train acc: 0.5372, val loss: 1.0902, val acc: 0.5464  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 27160] train loss: 1.1101, train acc: 0.5397, val loss: 1.0887, val acc: 0.5460  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 27180] train loss: 1.0888, train acc: 0.5432, val loss: 1.0892, val acc: 0.5457  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 27200] train loss: 1.0958, train acc: 0.5412, val loss: 1.0902, val acc: 0.5454  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 27220] train loss: 1.1112, train acc: 0.5336, val loss: 1.0887, val acc: 0.5470  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 27240] train loss: 1.0907, train acc: 0.5483, val loss: 1.0900, val acc: 0.5470  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 27260] train loss: 1.0978, train acc: 0.5424, val loss: 1.0909, val acc: 0.5460  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 27280] train loss: 1.1039, train acc: 0.5394, val loss: 1.0887, val acc: 0.5457  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 27300] train loss: 1.0920, train acc: 0.5429, val loss: 1.0881, val acc: 0.5467  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 27320] train loss: 1.0903, train acc: 0.5432, val loss: 1.0890, val acc: 0.5450  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 27340] train loss: 1.1080, train acc: 0.5367, val loss: 1.0898, val acc: 0.5444  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 27360] train loss: 1.0927, train acc: 0.5413, val loss: 1.0900, val acc: 0.5444  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 27380] train loss: 1.0996, train acc: 0.5454, val loss: 1.0876, val acc: 0.5454  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 27400] train loss: 1.1001, train acc: 0.5408, val loss: 1.0881, val acc: 0.5454  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 27420] train loss: 1.0967, train acc: 0.5468, val loss: 1.0927, val acc: 0.5447  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 27440] train loss: 1.0796, train acc: 0.5497, val loss: 1.0926, val acc: 0.5454  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 27460] train loss: 1.1096, train acc: 0.5375, val loss: 1.0871, val acc: 0.5450  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 27480] train loss: 1.1019, train acc: 0.5406, val loss: 1.0892, val acc: 0.5450  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 27500] train loss: 1.0973, train acc: 0.5423, val loss: 1.0901, val acc: 0.5491  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 27520] train loss: 1.1074, train acc: 0.5384, val loss: 1.0896, val acc: 0.5491  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 27540] train loss: 1.0892, train acc: 0.5471, val loss: 1.0901, val acc: 0.5464  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 27560] train loss: 1.0849, train acc: 0.5505, val loss: 1.0876, val acc: 0.5454  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 27580] train loss: 1.0926, train acc: 0.5438, val loss: 1.0875, val acc: 0.5460  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 27600] train loss: 1.0926, train acc: 0.5395, val loss: 1.0877, val acc: 0.5474  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 27620] train loss: 1.0881, train acc: 0.5402, val loss: 1.0892, val acc: 0.5504  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 27640] train loss: 1.1046, train acc: 0.5300, val loss: 1.0873, val acc: 0.5454  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 27660] train loss: 1.1003, train acc: 0.5420, val loss: 1.0909, val acc: 0.5417  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 27680] train loss: 1.0957, train acc: 0.5489, val loss: 1.0876, val acc: 0.5457  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 27700] train loss: 1.0967, train acc: 0.5361, val loss: 1.0869, val acc: 0.5460  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 27720] train loss: 1.0746, train acc: 0.5568, val loss: 1.0863, val acc: 0.5423  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 27740] train loss: 1.0933, train acc: 0.5429, val loss: 1.0891, val acc: 0.5430  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 27760] train loss: 1.0987, train acc: 0.5404, val loss: 1.0863, val acc: 0.5440  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 27780] train loss: 1.0843, train acc: 0.5483, val loss: 1.0873, val acc: 0.5501  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 27800] train loss: 1.0887, train acc: 0.5443, val loss: 1.0868, val acc: 0.5437  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 27820] train loss: 1.0886, train acc: 0.5479, val loss: 1.0863, val acc: 0.5450  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 27840] train loss: 1.0766, train acc: 0.5540, val loss: 1.0865, val acc: 0.5467  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 27860] train loss: 1.0745, train acc: 0.5541, val loss: 1.0879, val acc: 0.5454  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 27880] train loss: 1.0918, train acc: 0.5474, val loss: 1.0864, val acc: 0.5447  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 27900] train loss: 1.1008, train acc: 0.5401, val loss: 1.0868, val acc: 0.5467  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 27920] train loss: 1.0910, train acc: 0.5422, val loss: 1.0892, val acc: 0.5457  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 27940] train loss: 1.1044, train acc: 0.5401, val loss: 1.0861, val acc: 0.5437  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 27960] train loss: 1.1034, train acc: 0.5335, val loss: 1.0876, val acc: 0.5447  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 27980] train loss: 1.0811, train acc: 0.5536, val loss: 1.0888, val acc: 0.5494  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 28000] train loss: 1.0680, train acc: 0.5536, val loss: 1.0869, val acc: 0.5464  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 28020] train loss: 1.0879, train acc: 0.5458, val loss: 1.0856, val acc: 0.5460  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 28040] train loss: 1.0893, train acc: 0.5474, val loss: 1.0895, val acc: 0.5444  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 28060] train loss: 1.0873, train acc: 0.5469, val loss: 1.0875, val acc: 0.5481  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 28080] train loss: 1.0879, train acc: 0.5497, val loss: 1.0858, val acc: 0.5433  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 28100] train loss: 1.0951, train acc: 0.5442, val loss: 1.0889, val acc: 0.5491  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 28120] train loss: 1.0830, train acc: 0.5489, val loss: 1.0875, val acc: 0.5460  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 28140] train loss: 1.0809, train acc: 0.5473, val loss: 1.0900, val acc: 0.5423  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 28160] train loss: 1.1232, train acc: 0.5301, val loss: 1.0856, val acc: 0.5460  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 28180] train loss: 1.0958, train acc: 0.5389, val loss: 1.0876, val acc: 0.5484  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 28200] train loss: 1.0987, train acc: 0.5435, val loss: 1.0868, val acc: 0.5390  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 28220] train loss: 1.0819, train acc: 0.5460, val loss: 1.0852, val acc: 0.5440  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 28240] train loss: 1.1029, train acc: 0.5341, val loss: 1.0873, val acc: 0.5491  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 28260] train loss: 1.1050, train acc: 0.5344, val loss: 1.0859, val acc: 0.5423  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 28280] train loss: 1.0931, train acc: 0.5458, val loss: 1.0887, val acc: 0.5464  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 28300] train loss: 1.1004, train acc: 0.5361, val loss: 1.0851, val acc: 0.5464  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 28320] train loss: 1.0999, train acc: 0.5351, val loss: 1.0880, val acc: 0.5447  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 28340] train loss: 1.0963, train acc: 0.5380, val loss: 1.0856, val acc: 0.5474  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 28360] train loss: 1.0987, train acc: 0.5417, val loss: 1.0886, val acc: 0.5474  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 28380] train loss: 1.0964, train acc: 0.5417, val loss: 1.0850, val acc: 0.5450  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 28400] train loss: 1.0715, train acc: 0.5547, val loss: 1.0853, val acc: 0.5440  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 28420] train loss: 1.1118, train acc: 0.5348, val loss: 1.0850, val acc: 0.5433  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 28440] train loss: 1.0965, train acc: 0.5436, val loss: 1.0881, val acc: 0.5444  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 28460] train loss: 1.1081, train acc: 0.5408, val loss: 1.0847, val acc: 0.5440  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 28480] train loss: 1.1065, train acc: 0.5367, val loss: 1.0844, val acc: 0.5464  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 28500] train loss: 1.0904, train acc: 0.5442, val loss: 1.0872, val acc: 0.5484  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 28520] train loss: 1.1054, train acc: 0.5370, val loss: 1.0868, val acc: 0.5467  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 28540] train loss: 1.0962, train acc: 0.5383, val loss: 1.0869, val acc: 0.5454  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 28560] train loss: 1.0863, train acc: 0.5453, val loss: 1.0851, val acc: 0.5444  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 28580] train loss: 1.0980, train acc: 0.5398, val loss: 1.0847, val acc: 0.5447  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 28600] train loss: 1.1002, train acc: 0.5430, val loss: 1.0873, val acc: 0.5511  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 28620] train loss: 1.0903, train acc: 0.5426, val loss: 1.0851, val acc: 0.5454  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 28640] train loss: 1.0961, train acc: 0.5383, val loss: 1.0993, val acc: 0.5437  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 28660] train loss: 1.0943, train acc: 0.5440, val loss: 1.0895, val acc: 0.5514  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 28680] train loss: 1.0877, train acc: 0.5460, val loss: 1.0850, val acc: 0.5464  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 28700] train loss: 1.0849, train acc: 0.5489, val loss: 1.0846, val acc: 0.5460  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 28720] train loss: 1.0774, train acc: 0.5511, val loss: 1.0846, val acc: 0.5460  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 28740] train loss: 1.0931, train acc: 0.5413, val loss: 1.0868, val acc: 0.5474  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 28760] train loss: 1.0730, train acc: 0.5548, val loss: 1.0854, val acc: 0.5454  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 28780] train loss: 1.0795, train acc: 0.5488, val loss: 1.0844, val acc: 0.5430  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 28800] train loss: 1.0979, train acc: 0.5406, val loss: 1.0840, val acc: 0.5454  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 28820] train loss: 1.1029, train acc: 0.5328, val loss: 1.0840, val acc: 0.5460  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 28840] train loss: 1.0835, train acc: 0.5463, val loss: 1.0868, val acc: 0.5491  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 28860] train loss: 1.0808, train acc: 0.5505, val loss: 1.0856, val acc: 0.5494  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 28880] train loss: 1.1041, train acc: 0.5386, val loss: 1.0853, val acc: 0.5417  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 28900] train loss: 1.0910, train acc: 0.5429, val loss: 1.0875, val acc: 0.5477  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 28920] train loss: 1.1044, train acc: 0.5352, val loss: 1.0851, val acc: 0.5470  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 28940] train loss: 1.1062, train acc: 0.5335, val loss: 1.0864, val acc: 0.5477  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 28960] train loss: 1.0923, train acc: 0.5408, val loss: 1.0856, val acc: 0.5464  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 28980] train loss: 1.0855, train acc: 0.5468, val loss: 1.0840, val acc: 0.5464  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 29000] train loss: 1.0908, train acc: 0.5422, val loss: 1.0862, val acc: 0.5497  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 29020] train loss: 1.0930, train acc: 0.5376, val loss: 1.0858, val acc: 0.5491  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 29040] train loss: 1.0966, train acc: 0.5424, val loss: 1.0849, val acc: 0.5437  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 29060] train loss: 1.0905, train acc: 0.5447, val loss: 1.0842, val acc: 0.5444  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 29080] train loss: 1.1170, train acc: 0.5287, val loss: 1.0834, val acc: 0.5467  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 29100] train loss: 1.0704, train acc: 0.5524, val loss: 1.0834, val acc: 0.5444  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 29120] train loss: 1.0793, train acc: 0.5450, val loss: 1.0834, val acc: 0.5447  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 29140] train loss: 1.1007, train acc: 0.5354, val loss: 1.0854, val acc: 0.5444  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 29160] train loss: 1.0763, train acc: 0.5458, val loss: 1.0845, val acc: 0.5454  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 29180] train loss: 1.0746, train acc: 0.5497, val loss: 1.0863, val acc: 0.5447  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 29200] train loss: 1.0839, train acc: 0.5489, val loss: 1.0845, val acc: 0.5467  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 29220] train loss: 1.0874, train acc: 0.5451, val loss: 1.0833, val acc: 0.5450  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 29240] train loss: 1.0899, train acc: 0.5444, val loss: 1.0843, val acc: 0.5457  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 29260] train loss: 1.0964, train acc: 0.5415, val loss: 1.0829, val acc: 0.5447  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 29280] train loss: 1.0898, train acc: 0.5400, val loss: 1.0836, val acc: 0.5444  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 29300] train loss: 1.0900, train acc: 0.5507, val loss: 1.0868, val acc: 0.5504  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 29320] train loss: 1.0870, train acc: 0.5492, val loss: 1.0879, val acc: 0.5484  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 29340] train loss: 1.0998, train acc: 0.5356, val loss: 1.0831, val acc: 0.5447  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 29360] train loss: 1.0791, train acc: 0.5461, val loss: 1.0837, val acc: 0.5474  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 29380] train loss: 1.1000, train acc: 0.5407, val loss: 1.0835, val acc: 0.5467  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 29400] train loss: 1.0937, train acc: 0.5427, val loss: 1.0845, val acc: 0.5467  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 29420] train loss: 1.0852, train acc: 0.5455, val loss: 1.0833, val acc: 0.5464  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 29440] train loss: 1.0870, train acc: 0.5468, val loss: 1.0834, val acc: 0.5454  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 29460] train loss: 1.0814, train acc: 0.5463, val loss: 1.0891, val acc: 0.5477  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 29480] train loss: 1.0703, train acc: 0.5520, val loss: 1.0834, val acc: 0.5430  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 29500] train loss: 1.1125, train acc: 0.5299, val loss: 1.0833, val acc: 0.5497  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 29520] train loss: 1.0933, train acc: 0.5423, val loss: 1.0832, val acc: 0.5450  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 29540] train loss: 1.0869, train acc: 0.5435, val loss: 1.0827, val acc: 0.5447  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 29560] train loss: 1.0906, train acc: 0.5371, val loss: 1.0899, val acc: 0.5474  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 29580] train loss: 1.1065, train acc: 0.5367, val loss: 1.0845, val acc: 0.5460  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 29600] train loss: 1.0872, train acc: 0.5466, val loss: 1.0904, val acc: 0.5491  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 29620] train loss: 1.0936, train acc: 0.5436, val loss: 1.0849, val acc: 0.5481  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 29640] train loss: 1.0842, train acc: 0.5437, val loss: 1.0849, val acc: 0.5518  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 29660] train loss: 1.1021, train acc: 0.5426, val loss: 1.0857, val acc: 0.5514  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 29680] train loss: 1.1029, train acc: 0.5369, val loss: 1.0832, val acc: 0.5470  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 29700] train loss: 1.1128, train acc: 0.5341, val loss: 1.0841, val acc: 0.5481  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 29720] train loss: 1.0772, train acc: 0.5505, val loss: 1.0827, val acc: 0.5460  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 29740] train loss: 1.0708, train acc: 0.5536, val loss: 1.0865, val acc: 0.5514  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 29760] train loss: 1.0873, train acc: 0.5401, val loss: 1.0826, val acc: 0.5477  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 29780] train loss: 1.0927, train acc: 0.5379, val loss: 1.0889, val acc: 0.5457  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 29800] train loss: 1.1054, train acc: 0.5400, val loss: 1.0853, val acc: 0.5504  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 29820] train loss: 1.0750, train acc: 0.5515, val loss: 1.0825, val acc: 0.5481  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 29840] train loss: 1.0992, train acc: 0.5406, val loss: 1.0847, val acc: 0.5454  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 29860] train loss: 1.0824, train acc: 0.5466, val loss: 1.0849, val acc: 0.5508  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 29880] train loss: 1.0895, train acc: 0.5462, val loss: 1.0817, val acc: 0.5457  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 29900] train loss: 1.0812, train acc: 0.5448, val loss: 1.0819, val acc: 0.5460  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 29920] train loss: 1.0740, train acc: 0.5517, val loss: 1.0825, val acc: 0.5464  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 29940] train loss: 1.0815, train acc: 0.5531, val loss: 1.0822, val acc: 0.5470  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 29960] train loss: 1.1139, train acc: 0.5289, val loss: 1.0825, val acc: 0.5484  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 29980] train loss: 1.1040, train acc: 0.5324, val loss: 1.0842, val acc: 0.5491  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 30000] train loss: 1.1090, train acc: 0.5310, val loss: 1.0839, val acc: 0.5491  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 30020] train loss: 1.0729, train acc: 0.5512, val loss: 1.0826, val acc: 0.5457  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 30040] train loss: 1.0966, train acc: 0.5381, val loss: 1.0839, val acc: 0.5481  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 30060] train loss: 1.0888, train acc: 0.5442, val loss: 1.0832, val acc: 0.5487  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 30080] train loss: 1.1073, train acc: 0.5318, val loss: 1.0828, val acc: 0.5504  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 30100] train loss: 1.0851, train acc: 0.5427, val loss: 1.0835, val acc: 0.5521  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 30120] train loss: 1.0866, train acc: 0.5439, val loss: 1.0846, val acc: 0.5433  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 30140] train loss: 1.0729, train acc: 0.5486, val loss: 1.0812, val acc: 0.5464  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 30160] train loss: 1.1117, train acc: 0.5296, val loss: 1.0836, val acc: 0.5528  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 30180] train loss: 1.0868, train acc: 0.5468, val loss: 1.0809, val acc: 0.5474  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 30200] train loss: 1.0944, train acc: 0.5424, val loss: 1.0809, val acc: 0.5477  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 30220] train loss: 1.0796, train acc: 0.5505, val loss: 1.0823, val acc: 0.5487  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 30240] train loss: 1.0998, train acc: 0.5373, val loss: 1.0858, val acc: 0.5538  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 30260] train loss: 1.0860, train acc: 0.5415, val loss: 1.0804, val acc: 0.5467  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 30280] train loss: 1.0901, train acc: 0.5447, val loss: 1.0816, val acc: 0.5470  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 30300] train loss: 1.0959, train acc: 0.5398, val loss: 1.0826, val acc: 0.5491  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 30320] train loss: 1.0858, train acc: 0.5479, val loss: 1.0806, val acc: 0.5494  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 30340] train loss: 1.0855, train acc: 0.5485, val loss: 1.0822, val acc: 0.5464  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 30360] train loss: 1.0791, train acc: 0.5535, val loss: 1.0827, val acc: 0.5524  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 30380] train loss: 1.0942, train acc: 0.5436, val loss: 1.0831, val acc: 0.5514  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 30400] train loss: 1.1004, train acc: 0.5338, val loss: 1.0846, val acc: 0.5481  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 30420] train loss: 1.1079, train acc: 0.5307, val loss: 1.0806, val acc: 0.5481  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 30440] train loss: 1.0950, train acc: 0.5424, val loss: 1.0818, val acc: 0.5460  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 30460] train loss: 1.0818, train acc: 0.5458, val loss: 1.0813, val acc: 0.5400  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 30480] train loss: 1.0871, train acc: 0.5430, val loss: 1.0857, val acc: 0.5528  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 30500] train loss: 1.0986, train acc: 0.5437, val loss: 1.0821, val acc: 0.5501  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 30520] train loss: 1.0795, train acc: 0.5450, val loss: 1.0807, val acc: 0.5457  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 30540] train loss: 1.0914, train acc: 0.5455, val loss: 1.0801, val acc: 0.5487  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 30560] train loss: 1.0829, train acc: 0.5512, val loss: 1.0807, val acc: 0.5457  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 30580] train loss: 1.0950, train acc: 0.5412, val loss: 1.0822, val acc: 0.5504  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 30600] train loss: 1.0867, train acc: 0.5495, val loss: 1.0846, val acc: 0.5481  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 30620] train loss: 1.0911, train acc: 0.5416, val loss: 1.0802, val acc: 0.5470  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 30640] train loss: 1.0739, train acc: 0.5480, val loss: 1.0797, val acc: 0.5464  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 30660] train loss: 1.0860, train acc: 0.5391, val loss: 1.0806, val acc: 0.5470  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 30680] train loss: 1.0683, train acc: 0.5536, val loss: 1.0803, val acc: 0.5501  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 30700] train loss: 1.0869, train acc: 0.5463, val loss: 1.0797, val acc: 0.5487  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 30720] train loss: 1.0806, train acc: 0.5463, val loss: 1.0802, val acc: 0.5504  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 30740] train loss: 1.0847, train acc: 0.5452, val loss: 1.0813, val acc: 0.5447  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 30760] train loss: 1.0835, train acc: 0.5432, val loss: 1.0805, val acc: 0.5494  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 30780] train loss: 1.0723, train acc: 0.5505, val loss: 1.0796, val acc: 0.5454  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 30800] train loss: 1.0681, train acc: 0.5533, val loss: 1.0829, val acc: 0.5528  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 30820] train loss: 1.1066, train acc: 0.5361, val loss: 1.0803, val acc: 0.5467  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 30840] train loss: 1.0942, train acc: 0.5414, val loss: 1.0809, val acc: 0.5501  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 30860] train loss: 1.0976, train acc: 0.5376, val loss: 1.0812, val acc: 0.5467  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 30880] train loss: 1.0976, train acc: 0.5411, val loss: 1.0797, val acc: 0.5487  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 30900] train loss: 1.0895, train acc: 0.5415, val loss: 1.0792, val acc: 0.5477  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 30920] train loss: 1.0925, train acc: 0.5458, val loss: 1.0806, val acc: 0.5538  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 30940] train loss: 1.0981, train acc: 0.5434, val loss: 1.0813, val acc: 0.5497  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 30960] train loss: 1.1094, train acc: 0.5337, val loss: 1.0795, val acc: 0.5487  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 30980] train loss: 1.0773, train acc: 0.5523, val loss: 1.0786, val acc: 0.5481  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 31000] train loss: 1.0845, train acc: 0.5467, val loss: 1.0792, val acc: 0.5481  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 31020] train loss: 1.0754, train acc: 0.5486, val loss: 1.0844, val acc: 0.5487  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 31040] train loss: 1.0776, train acc: 0.5490, val loss: 1.0842, val acc: 0.5541  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 31060] train loss: 1.0937, train acc: 0.5443, val loss: 1.0821, val acc: 0.5535  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 31080] train loss: 1.0777, train acc: 0.5482, val loss: 1.0789, val acc: 0.5511  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 31100] train loss: 1.0935, train acc: 0.5421, val loss: 1.0792, val acc: 0.5481  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 31120] train loss: 1.0889, train acc: 0.5464, val loss: 1.0787, val acc: 0.5481  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 31140] train loss: 1.0883, train acc: 0.5434, val loss: 1.0802, val acc: 0.5481  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 31160] train loss: 1.1085, train acc: 0.5363, val loss: 1.0820, val acc: 0.5433  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 31180] train loss: 1.0916, train acc: 0.5405, val loss: 1.0803, val acc: 0.5491  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 31200] train loss: 1.0844, train acc: 0.5437, val loss: 1.0845, val acc: 0.5464  (best train acc: 0.5651, best val acc: 0.5622)\n",
      "[Epoch: 31220] train loss: 1.0814, train acc: 0.5474, val loss: 1.0797, val acc: 0.5487  (best train acc: 0.5652, best val acc: 0.5622)\n",
      "[Epoch: 31240] train loss: 1.0780, train acc: 0.5483, val loss: 1.0846, val acc: 0.5457  (best train acc: 0.5652, best val acc: 0.5622)\n",
      "[Epoch: 31260] train loss: 1.0848, train acc: 0.5431, val loss: 1.0804, val acc: 0.5497  (best train acc: 0.5652, best val acc: 0.5622)\n",
      "[Epoch: 31280] train loss: 1.0898, train acc: 0.5429, val loss: 1.0788, val acc: 0.5477  (best train acc: 0.5652, best val acc: 0.5622)\n",
      "[Epoch: 31300] train loss: 1.0911, train acc: 0.5470, val loss: 1.0798, val acc: 0.5501  (best train acc: 0.5652, best val acc: 0.5622)\n",
      "[Epoch: 31320] train loss: 1.0888, train acc: 0.5437, val loss: 1.0791, val acc: 0.5491  (best train acc: 0.5652, best val acc: 0.5622)\n",
      "[Epoch: 31340] train loss: 1.0789, train acc: 0.5541, val loss: 1.0796, val acc: 0.5497  (best train acc: 0.5652, best val acc: 0.5622)\n",
      "[Epoch: 31360] train loss: 1.0861, train acc: 0.5468, val loss: 1.0787, val acc: 0.5454  (best train acc: 0.5652, best val acc: 0.5622)\n",
      "[Epoch: 31380] train loss: 1.0806, train acc: 0.5479, val loss: 1.0798, val acc: 0.5514  (best train acc: 0.5652, best val acc: 0.5622)\n",
      "[Epoch: 31400] train loss: 1.1047, train acc: 0.5399, val loss: 1.0797, val acc: 0.5481  (best train acc: 0.5652, best val acc: 0.5622)\n",
      "[Epoch: 31420] train loss: 1.0735, train acc: 0.5544, val loss: 1.0809, val acc: 0.5518  (best train acc: 0.5652, best val acc: 0.5622)\n",
      "[Epoch: 31440] train loss: 1.0824, train acc: 0.5471, val loss: 1.0795, val acc: 0.5494  (best train acc: 0.5652, best val acc: 0.5622)\n",
      "[Epoch: 31460] train loss: 1.0940, train acc: 0.5478, val loss: 1.0790, val acc: 0.5481  (best train acc: 0.5652, best val acc: 0.5622)\n",
      "[Epoch: 31480] train loss: 1.1019, train acc: 0.5335, val loss: 1.0814, val acc: 0.5508  (best train acc: 0.5652, best val acc: 0.5622)\n",
      "[Epoch: 31500] train loss: 1.1031, train acc: 0.5376, val loss: 1.0780, val acc: 0.5460  (best train acc: 0.5652, best val acc: 0.5622)\n",
      "[Epoch: 31520] train loss: 1.0823, train acc: 0.5471, val loss: 1.0785, val acc: 0.5497  (best train acc: 0.5652, best val acc: 0.5622)\n",
      "[Epoch: 31540] train loss: 1.1065, train acc: 0.5380, val loss: 1.0834, val acc: 0.5572  (best train acc: 0.5652, best val acc: 0.5622)\n",
      "[Epoch: 31560] train loss: 1.0665, train acc: 0.5555, val loss: 1.0853, val acc: 0.5555  (best train acc: 0.5652, best val acc: 0.5622)\n",
      "[Epoch: 31580] train loss: 1.0871, train acc: 0.5478, val loss: 1.0788, val acc: 0.5487  (best train acc: 0.5652, best val acc: 0.5622)\n",
      "[Epoch: 31600] train loss: 1.0652, train acc: 0.5545, val loss: 1.0848, val acc: 0.5531  (best train acc: 0.5652, best val acc: 0.5622)\n",
      "[Epoch: 31620] train loss: 1.0921, train acc: 0.5410, val loss: 1.0783, val acc: 0.5491  (best train acc: 0.5652, best val acc: 0.5622)\n",
      "[Epoch: 31640] train loss: 1.0757, train acc: 0.5495, val loss: 1.0790, val acc: 0.5437  (best train acc: 0.5652, best val acc: 0.5622)\n",
      "[Epoch: 31660] train loss: 1.0830, train acc: 0.5430, val loss: 1.0804, val acc: 0.5474  (best train acc: 0.5652, best val acc: 0.5622)\n",
      "[Epoch: 31680] train loss: 1.0824, train acc: 0.5480, val loss: 1.0783, val acc: 0.5454  (best train acc: 0.5652, best val acc: 0.5622)\n",
      "[Epoch: 31700] train loss: 1.0946, train acc: 0.5433, val loss: 1.0802, val acc: 0.5484  (best train acc: 0.5652, best val acc: 0.5622)\n",
      "[Epoch: 31720] train loss: 1.0734, train acc: 0.5552, val loss: 1.0825, val acc: 0.5481  (best train acc: 0.5652, best val acc: 0.5622)\n",
      "[Epoch: 31740] train loss: 1.0886, train acc: 0.5500, val loss: 1.0779, val acc: 0.5484  (best train acc: 0.5652, best val acc: 0.5622)\n",
      "[Epoch: 31760] train loss: 1.0662, train acc: 0.5543, val loss: 1.0783, val acc: 0.5467  (best train acc: 0.5652, best val acc: 0.5622)\n",
      "[Epoch: 31780] train loss: 1.0879, train acc: 0.5482, val loss: 1.0774, val acc: 0.5460  (best train acc: 0.5652, best val acc: 0.5622)\n",
      "[Epoch: 31800] train loss: 1.1010, train acc: 0.5385, val loss: 1.0776, val acc: 0.5521  (best train acc: 0.5652, best val acc: 0.5622)\n",
      "[Epoch: 31820] train loss: 1.0774, train acc: 0.5459, val loss: 1.0770, val acc: 0.5497  (best train acc: 0.5652, best val acc: 0.5622)\n",
      "[Epoch: 31840] train loss: 1.0748, train acc: 0.5486, val loss: 1.0775, val acc: 0.5504  (best train acc: 0.5652, best val acc: 0.5622)\n",
      "[Epoch: 31860] train loss: 1.0941, train acc: 0.5419, val loss: 1.0784, val acc: 0.5494  (best train acc: 0.5652, best val acc: 0.5622)\n",
      "[Epoch: 31880] train loss: 1.0949, train acc: 0.5432, val loss: 1.0777, val acc: 0.5467  (best train acc: 0.5652, best val acc: 0.5622)\n",
      "[Epoch: 31900] train loss: 1.0950, train acc: 0.5405, val loss: 1.0772, val acc: 0.5477  (best train acc: 0.5652, best val acc: 0.5622)\n",
      "[Epoch: 31920] train loss: 1.0756, train acc: 0.5469, val loss: 1.0776, val acc: 0.5484  (best train acc: 0.5652, best val acc: 0.5622)\n",
      "[Epoch: 31940] train loss: 1.1046, train acc: 0.5377, val loss: 1.0786, val acc: 0.5481  (best train acc: 0.5652, best val acc: 0.5622)\n",
      "[Epoch: 31960] train loss: 1.0829, train acc: 0.5468, val loss: 1.0777, val acc: 0.5504  (best train acc: 0.5652, best val acc: 0.5622)\n",
      "[Epoch: 31980] train loss: 1.0953, train acc: 0.5437, val loss: 1.0768, val acc: 0.5491  (best train acc: 0.5652, best val acc: 0.5622)\n",
      "[Epoch: 32000] train loss: 1.0955, train acc: 0.5401, val loss: 1.0796, val acc: 0.5444  (best train acc: 0.5652, best val acc: 0.5622)\n",
      "[Epoch: 32020] train loss: 1.0680, train acc: 0.5541, val loss: 1.0773, val acc: 0.5494  (best train acc: 0.5652, best val acc: 0.5622)\n",
      "[Epoch: 32040] train loss: 1.0880, train acc: 0.5464, val loss: 1.0810, val acc: 0.5497  (best train acc: 0.5652, best val acc: 0.5622)\n",
      "[Epoch: 32060] train loss: 1.0795, train acc: 0.5481, val loss: 1.0777, val acc: 0.5501  (best train acc: 0.5652, best val acc: 0.5622)\n",
      "[Epoch: 32080] train loss: 1.0579, train acc: 0.5620, val loss: 1.0774, val acc: 0.5481  (best train acc: 0.5652, best val acc: 0.5622)\n",
      "[Epoch: 32100] train loss: 1.0886, train acc: 0.5447, val loss: 1.0853, val acc: 0.5406  (best train acc: 0.5652, best val acc: 0.5622)\n",
      "[Epoch: 32120] train loss: 1.0757, train acc: 0.5497, val loss: 1.0800, val acc: 0.5481  (best train acc: 0.5652, best val acc: 0.5622)\n",
      "[Epoch: 32140] train loss: 1.0829, train acc: 0.5455, val loss: 1.0793, val acc: 0.5521  (best train acc: 0.5652, best val acc: 0.5622)\n",
      "[Epoch: 32160] train loss: 1.0852, train acc: 0.5479, val loss: 1.0765, val acc: 0.5470  (best train acc: 0.5652, best val acc: 0.5622)\n",
      "[Epoch: 32180] train loss: 1.0927, train acc: 0.5427, val loss: 1.0800, val acc: 0.5511  (best train acc: 0.5652, best val acc: 0.5622)\n",
      "[Epoch: 32200] train loss: 1.0929, train acc: 0.5450, val loss: 1.0773, val acc: 0.5477  (best train acc: 0.5660, best val acc: 0.5622)\n",
      "[Epoch: 32220] train loss: 1.0891, train acc: 0.5468, val loss: 1.0776, val acc: 0.5420  (best train acc: 0.5660, best val acc: 0.5622)\n",
      "[Epoch: 32240] train loss: 1.0874, train acc: 0.5468, val loss: 1.0757, val acc: 0.5454  (best train acc: 0.5660, best val acc: 0.5622)\n",
      "[Epoch: 32260] train loss: 1.1043, train acc: 0.5385, val loss: 1.0808, val acc: 0.5477  (best train acc: 0.5660, best val acc: 0.5622)\n",
      "[Epoch: 32280] train loss: 1.0967, train acc: 0.5464, val loss: 1.0759, val acc: 0.5481  (best train acc: 0.5660, best val acc: 0.5622)\n",
      "[Epoch: 32300] train loss: 1.0880, train acc: 0.5462, val loss: 1.0813, val acc: 0.5494  (best train acc: 0.5660, best val acc: 0.5622)\n",
      "[Epoch: 32320] train loss: 1.1039, train acc: 0.5361, val loss: 1.0769, val acc: 0.5494  (best train acc: 0.5660, best val acc: 0.5622)\n",
      "[Epoch: 32340] train loss: 1.0770, train acc: 0.5510, val loss: 1.0802, val acc: 0.5460  (best train acc: 0.5660, best val acc: 0.5622)\n",
      "[Epoch: 32360] train loss: 1.0661, train acc: 0.5573, val loss: 1.0773, val acc: 0.5460  (best train acc: 0.5660, best val acc: 0.5622)\n",
      "[Epoch: 32380] train loss: 1.0622, train acc: 0.5589, val loss: 1.0769, val acc: 0.5467  (best train acc: 0.5660, best val acc: 0.5622)\n",
      "[Epoch: 32400] train loss: 1.0888, train acc: 0.5514, val loss: 1.0796, val acc: 0.5524  (best train acc: 0.5660, best val acc: 0.5622)\n",
      "[Epoch: 32420] train loss: 1.0680, train acc: 0.5570, val loss: 1.0769, val acc: 0.5538  (best train acc: 0.5660, best val acc: 0.5622)\n",
      "[Epoch: 32440] train loss: 1.0922, train acc: 0.5450, val loss: 1.0770, val acc: 0.5474  (best train acc: 0.5660, best val acc: 0.5622)\n",
      "[Epoch: 32460] train loss: 1.0806, train acc: 0.5489, val loss: 1.0774, val acc: 0.5487  (best train acc: 0.5660, best val acc: 0.5622)\n",
      "[Epoch: 32480] train loss: 1.0662, train acc: 0.5521, val loss: 1.0792, val acc: 0.5548  (best train acc: 0.5660, best val acc: 0.5622)\n",
      "[Epoch: 32500] train loss: 1.0922, train acc: 0.5427, val loss: 1.0764, val acc: 0.5467  (best train acc: 0.5660, best val acc: 0.5622)\n",
      "[Epoch: 32520] train loss: 1.0640, train acc: 0.5559, val loss: 1.0790, val acc: 0.5444  (best train acc: 0.5660, best val acc: 0.5622)\n",
      "[Epoch: 32540] train loss: 1.1048, train acc: 0.5360, val loss: 1.0755, val acc: 0.5457  (best train acc: 0.5660, best val acc: 0.5622)\n",
      "[Epoch: 32560] train loss: 1.0952, train acc: 0.5424, val loss: 1.0761, val acc: 0.5555  (best train acc: 0.5660, best val acc: 0.5622)\n",
      "[Epoch: 32580] train loss: 1.0859, train acc: 0.5438, val loss: 1.0773, val acc: 0.5470  (best train acc: 0.5660, best val acc: 0.5622)\n",
      "[Epoch: 32600] train loss: 1.0605, train acc: 0.5598, val loss: 1.0753, val acc: 0.5477  (best train acc: 0.5660, best val acc: 0.5622)\n",
      "[Epoch: 32620] train loss: 1.0713, train acc: 0.5527, val loss: 1.0759, val acc: 0.5511  (best train acc: 0.5660, best val acc: 0.5622)\n",
      "[Epoch: 32640] train loss: 1.0702, train acc: 0.5508, val loss: 1.0750, val acc: 0.5524  (best train acc: 0.5660, best val acc: 0.5622)\n",
      "[Epoch: 32660] train loss: 1.0644, train acc: 0.5519, val loss: 1.0748, val acc: 0.5518  (best train acc: 0.5660, best val acc: 0.5622)\n",
      "[Epoch: 32680] train loss: 1.0865, train acc: 0.5437, val loss: 1.0743, val acc: 0.5477  (best train acc: 0.5660, best val acc: 0.5622)\n",
      "[Epoch: 32700] train loss: 1.0816, train acc: 0.5463, val loss: 1.0760, val acc: 0.5508  (best train acc: 0.5660, best val acc: 0.5622)\n",
      "[Epoch: 32720] train loss: 1.0863, train acc: 0.5406, val loss: 1.0744, val acc: 0.5474  (best train acc: 0.5660, best val acc: 0.5622)\n",
      "[Epoch: 32740] train loss: 1.0843, train acc: 0.5465, val loss: 1.0745, val acc: 0.5464  (best train acc: 0.5660, best val acc: 0.5622)\n",
      "[Epoch: 32760] train loss: 1.0733, train acc: 0.5510, val loss: 1.0744, val acc: 0.5511  (best train acc: 0.5660, best val acc: 0.5622)\n",
      "[Epoch: 32780] train loss: 1.0820, train acc: 0.5470, val loss: 1.0742, val acc: 0.5528  (best train acc: 0.5660, best val acc: 0.5622)\n",
      "[Epoch: 32800] train loss: 1.0705, train acc: 0.5531, val loss: 1.0739, val acc: 0.5504  (best train acc: 0.5660, best val acc: 0.5622)\n",
      "[Epoch: 32820] train loss: 1.0762, train acc: 0.5521, val loss: 1.0748, val acc: 0.5484  (best train acc: 0.5660, best val acc: 0.5622)\n",
      "[Epoch: 32840] train loss: 1.0957, train acc: 0.5409, val loss: 1.0743, val acc: 0.5487  (best train acc: 0.5660, best val acc: 0.5622)\n",
      "[Epoch: 32860] train loss: 1.1018, train acc: 0.5361, val loss: 1.0749, val acc: 0.5481  (best train acc: 0.5660, best val acc: 0.5622)\n",
      "[Epoch: 32880] train loss: 1.0894, train acc: 0.5463, val loss: 1.0743, val acc: 0.5481  (best train acc: 0.5660, best val acc: 0.5622)\n",
      "[Epoch: 32900] train loss: 1.0916, train acc: 0.5436, val loss: 1.0745, val acc: 0.5497  (best train acc: 0.5660, best val acc: 0.5622)\n",
      "[Epoch: 32920] train loss: 1.0794, train acc: 0.5510, val loss: 1.0747, val acc: 0.5521  (best train acc: 0.5660, best val acc: 0.5622)\n",
      "[Epoch: 32940] train loss: 1.1038, train acc: 0.5330, val loss: 1.0830, val acc: 0.5420  (best train acc: 0.5660, best val acc: 0.5622)\n",
      "[Epoch: 32960] train loss: 1.0841, train acc: 0.5487, val loss: 1.0749, val acc: 0.5497  (best train acc: 0.5660, best val acc: 0.5622)\n",
      "[Epoch: 32980] train loss: 1.0906, train acc: 0.5411, val loss: 1.0746, val acc: 0.5491  (best train acc: 0.5660, best val acc: 0.5622)\n",
      "[Epoch: 33000] train loss: 1.0810, train acc: 0.5459, val loss: 1.0741, val acc: 0.5501  (best train acc: 0.5660, best val acc: 0.5622)\n",
      "[Epoch: 33020] train loss: 1.0899, train acc: 0.5423, val loss: 1.0746, val acc: 0.5484  (best train acc: 0.5660, best val acc: 0.5622)\n",
      "[Epoch: 33040] train loss: 1.0607, train acc: 0.5565, val loss: 1.0750, val acc: 0.5487  (best train acc: 0.5660, best val acc: 0.5622)\n",
      "[Epoch: 33060] train loss: 1.0871, train acc: 0.5453, val loss: 1.0765, val acc: 0.5491  (best train acc: 0.5660, best val acc: 0.5622)\n",
      "[Epoch: 33080] train loss: 1.0688, train acc: 0.5520, val loss: 1.0766, val acc: 0.5514  (best train acc: 0.5660, best val acc: 0.5622)\n",
      "[Epoch: 33100] train loss: 1.0949, train acc: 0.5455, val loss: 1.0744, val acc: 0.5558  (best train acc: 0.5660, best val acc: 0.5622)\n",
      "[Epoch: 33120] train loss: 1.0914, train acc: 0.5419, val loss: 1.0737, val acc: 0.5535  (best train acc: 0.5660, best val acc: 0.5622)\n",
      "[Epoch: 33140] train loss: 1.0691, train acc: 0.5546, val loss: 1.0754, val acc: 0.5474  (best train acc: 0.5660, best val acc: 0.5622)\n",
      "[Epoch: 33160] train loss: 1.1052, train acc: 0.5338, val loss: 1.0731, val acc: 0.5491  (best train acc: 0.5660, best val acc: 0.5622)\n",
      "[Epoch: 33180] train loss: 1.0789, train acc: 0.5478, val loss: 1.0728, val acc: 0.5474  (best train acc: 0.5660, best val acc: 0.5622)\n",
      "[Epoch: 33200] train loss: 1.0931, train acc: 0.5411, val loss: 1.0779, val acc: 0.5582  (best train acc: 0.5660, best val acc: 0.5622)\n",
      "[Epoch: 33220] train loss: 1.0802, train acc: 0.5497, val loss: 1.0727, val acc: 0.5477  (best train acc: 0.5660, best val acc: 0.5622)\n",
      "[Epoch: 33240] train loss: 1.0924, train acc: 0.5395, val loss: 1.0728, val acc: 0.5474  (best train acc: 0.5660, best val acc: 0.5622)\n",
      "[Epoch: 33260] train loss: 1.1107, train acc: 0.5402, val loss: 1.0750, val acc: 0.5575  (best train acc: 0.5660, best val acc: 0.5622)\n",
      "[Epoch: 33280] train loss: 1.0754, train acc: 0.5466, val loss: 1.0736, val acc: 0.5555  (best train acc: 0.5660, best val acc: 0.5622)\n",
      "[Epoch: 33300] train loss: 1.0622, train acc: 0.5609, val loss: 1.0725, val acc: 0.5497  (best train acc: 0.5660, best val acc: 0.5622)\n",
      "[Epoch: 33320] train loss: 1.0697, train acc: 0.5561, val loss: 1.0721, val acc: 0.5487  (best train acc: 0.5660, best val acc: 0.5622)\n",
      "[Epoch: 33340] train loss: 1.0995, train acc: 0.5399, val loss: 1.0745, val acc: 0.5521  (best train acc: 0.5660, best val acc: 0.5622)\n",
      "[Epoch: 33360] train loss: 1.0684, train acc: 0.5547, val loss: 1.0722, val acc: 0.5487  (best train acc: 0.5660, best val acc: 0.5622)\n",
      "[Epoch: 33380] train loss: 1.0915, train acc: 0.5408, val loss: 1.0729, val acc: 0.5518  (best train acc: 0.5660, best val acc: 0.5622)\n",
      "[Epoch: 33400] train loss: 1.0712, train acc: 0.5550, val loss: 1.0720, val acc: 0.5467  (best train acc: 0.5660, best val acc: 0.5622)\n",
      "[Epoch: 33420] train loss: 1.0834, train acc: 0.5459, val loss: 1.0762, val acc: 0.5487  (best train acc: 0.5660, best val acc: 0.5622)\n",
      "[Epoch: 33440] train loss: 1.0878, train acc: 0.5428, val loss: 1.0723, val acc: 0.5528  (best train acc: 0.5660, best val acc: 0.5622)\n",
      "[Epoch: 33460] train loss: 1.0860, train acc: 0.5431, val loss: 1.0730, val acc: 0.5484  (best train acc: 0.5660, best val acc: 0.5622)\n",
      "[Epoch: 33480] train loss: 1.0866, train acc: 0.5427, val loss: 1.0743, val acc: 0.5484  (best train acc: 0.5660, best val acc: 0.5622)\n",
      "[Epoch: 33500] train loss: 1.0724, train acc: 0.5521, val loss: 1.0722, val acc: 0.5514  (best train acc: 0.5660, best val acc: 0.5622)\n",
      "[Epoch: 33520] train loss: 1.0717, train acc: 0.5521, val loss: 1.0720, val acc: 0.5494  (best train acc: 0.5660, best val acc: 0.5622)\n",
      "[Epoch: 33540] train loss: 1.0672, train acc: 0.5575, val loss: 1.0722, val acc: 0.5541  (best train acc: 0.5660, best val acc: 0.5622)\n",
      "[Epoch: 33560] train loss: 1.0765, train acc: 0.5443, val loss: 1.0724, val acc: 0.5501  (best train acc: 0.5660, best val acc: 0.5622)\n",
      "[Epoch: 33580] train loss: 1.0934, train acc: 0.5434, val loss: 1.0733, val acc: 0.5535  (best train acc: 0.5660, best val acc: 0.5622)\n",
      "[Epoch: 33600] train loss: 1.0639, train acc: 0.5551, val loss: 1.0744, val acc: 0.5575  (best train acc: 0.5660, best val acc: 0.5622)\n",
      "[Epoch: 33620] train loss: 1.0576, train acc: 0.5586, val loss: 1.0729, val acc: 0.5481  (best train acc: 0.5660, best val acc: 0.5622)\n",
      "[Epoch: 33640] train loss: 1.0932, train acc: 0.5392, val loss: 1.0713, val acc: 0.5521  (best train acc: 0.5660, best val acc: 0.5622)\n",
      "[Epoch: 33660] train loss: 1.0705, train acc: 0.5515, val loss: 1.0721, val acc: 0.5528  (best train acc: 0.5660, best val acc: 0.5622)\n",
      "[Epoch: 33680] train loss: 1.0699, train acc: 0.5515, val loss: 1.0766, val acc: 0.5491  (best train acc: 0.5660, best val acc: 0.5622)\n",
      "[Epoch: 33700] train loss: 1.0648, train acc: 0.5561, val loss: 1.0728, val acc: 0.5491  (best train acc: 0.5660, best val acc: 0.5622)\n",
      "[Epoch: 33720] train loss: 1.0620, train acc: 0.5573, val loss: 1.0733, val acc: 0.5551  (best train acc: 0.5660, best val acc: 0.5622)\n",
      "[Epoch: 33740] train loss: 1.0799, train acc: 0.5495, val loss: 1.0727, val acc: 0.5562  (best train acc: 0.5660, best val acc: 0.5622)\n",
      "[Epoch: 33760] train loss: 1.0925, train acc: 0.5446, val loss: 1.0711, val acc: 0.5531  (best train acc: 0.5660, best val acc: 0.5622)\n",
      "[Epoch: 33780] train loss: 1.0786, train acc: 0.5512, val loss: 1.0706, val acc: 0.5538  (best train acc: 0.5660, best val acc: 0.5622)\n",
      "[Epoch: 33800] train loss: 1.1034, train acc: 0.5351, val loss: 1.0704, val acc: 0.5487  (best train acc: 0.5660, best val acc: 0.5622)\n",
      "[Epoch: 33820] train loss: 1.0676, train acc: 0.5507, val loss: 1.0721, val acc: 0.5508  (best train acc: 0.5660, best val acc: 0.5622)\n",
      "[Epoch: 33840] train loss: 1.0736, train acc: 0.5463, val loss: 1.0710, val acc: 0.5568  (best train acc: 0.5660, best val acc: 0.5622)\n",
      "[Epoch: 33860] train loss: 1.0809, train acc: 0.5487, val loss: 1.0724, val acc: 0.5531  (best train acc: 0.5660, best val acc: 0.5622)\n",
      "[Epoch: 33880] train loss: 1.0938, train acc: 0.5415, val loss: 1.0728, val acc: 0.5481  (best train acc: 0.5660, best val acc: 0.5622)\n",
      "[Epoch: 33900] train loss: 1.0793, train acc: 0.5517, val loss: 1.0705, val acc: 0.5481  (best train acc: 0.5660, best val acc: 0.5622)\n",
      "[Epoch: 33920] train loss: 1.0854, train acc: 0.5424, val loss: 1.0709, val acc: 0.5548  (best train acc: 0.5660, best val acc: 0.5622)\n",
      "[Epoch: 33940] train loss: 1.0746, train acc: 0.5510, val loss: 1.0696, val acc: 0.5514  (best train acc: 0.5660, best val acc: 0.5622)\n",
      "[Epoch: 33960] train loss: 1.0659, train acc: 0.5544, val loss: 1.0733, val acc: 0.5578  (best train acc: 0.5660, best val acc: 0.5622)\n",
      "[Epoch: 33980] train loss: 1.0696, train acc: 0.5463, val loss: 1.0720, val acc: 0.5494  (best train acc: 0.5660, best val acc: 0.5622)\n",
      "[Epoch: 34000] train loss: 1.0787, train acc: 0.5458, val loss: 1.0705, val acc: 0.5511  (best train acc: 0.5660, best val acc: 0.5622)\n",
      "[Epoch: 34020] train loss: 1.0799, train acc: 0.5440, val loss: 1.0711, val acc: 0.5555  (best train acc: 0.5660, best val acc: 0.5622)\n",
      "[Epoch: 34040] train loss: 1.0696, train acc: 0.5551, val loss: 1.0722, val acc: 0.5541  (best train acc: 0.5660, best val acc: 0.5622)\n",
      "[Epoch: 34060] train loss: 1.0854, train acc: 0.5440, val loss: 1.0695, val acc: 0.5487  (best train acc: 0.5660, best val acc: 0.5622)\n",
      "[Epoch: 34080] train loss: 1.0716, train acc: 0.5557, val loss: 1.0742, val acc: 0.5565  (best train acc: 0.5660, best val acc: 0.5622)\n",
      "[Epoch: 34100] train loss: 1.0675, train acc: 0.5563, val loss: 1.0699, val acc: 0.5504  (best train acc: 0.5660, best val acc: 0.5622)\n",
      "[Epoch: 34120] train loss: 1.0605, train acc: 0.5568, val loss: 1.0698, val acc: 0.5555  (best train acc: 0.5660, best val acc: 0.5622)\n",
      "[Epoch: 34140] train loss: 1.0715, train acc: 0.5463, val loss: 1.0726, val acc: 0.5551  (best train acc: 0.5660, best val acc: 0.5622)\n",
      "[Epoch: 34160] train loss: 1.0884, train acc: 0.5432, val loss: 1.0695, val acc: 0.5487  (best train acc: 0.5660, best val acc: 0.5622)\n",
      "[Epoch: 34180] train loss: 1.0834, train acc: 0.5458, val loss: 1.0689, val acc: 0.5535  (best train acc: 0.5660, best val acc: 0.5622)\n",
      "[Epoch: 34200] train loss: 1.0693, train acc: 0.5510, val loss: 1.0701, val acc: 0.5551  (best train acc: 0.5661, best val acc: 0.5622)\n",
      "[Epoch: 34220] train loss: 1.0768, train acc: 0.5454, val loss: 1.0734, val acc: 0.5565  (best train acc: 0.5661, best val acc: 0.5622)\n",
      "[Epoch: 34240] train loss: 1.0817, train acc: 0.5424, val loss: 1.0715, val acc: 0.5470  (best train acc: 0.5661, best val acc: 0.5622)\n",
      "[Epoch: 34260] train loss: 1.0847, train acc: 0.5423, val loss: 1.0699, val acc: 0.5481  (best train acc: 0.5661, best val acc: 0.5622)\n",
      "[Epoch: 34280] train loss: 1.0698, train acc: 0.5484, val loss: 1.0689, val acc: 0.5504  (best train acc: 0.5661, best val acc: 0.5622)\n",
      "[Epoch: 34300] train loss: 1.0588, train acc: 0.5622, val loss: 1.0695, val acc: 0.5535  (best train acc: 0.5661, best val acc: 0.5622)\n",
      "[Epoch: 34320] train loss: 1.0644, train acc: 0.5544, val loss: 1.0719, val acc: 0.5562  (best train acc: 0.5661, best val acc: 0.5622)\n",
      "[Epoch: 34340] train loss: 1.0738, train acc: 0.5499, val loss: 1.0718, val acc: 0.5535  (best train acc: 0.5661, best val acc: 0.5622)\n",
      "[Epoch: 34360] train loss: 1.0731, train acc: 0.5472, val loss: 1.0684, val acc: 0.5494  (best train acc: 0.5661, best val acc: 0.5622)\n",
      "[Epoch: 34380] train loss: 1.0819, train acc: 0.5509, val loss: 1.0683, val acc: 0.5518  (best train acc: 0.5661, best val acc: 0.5622)\n",
      "[Epoch: 34400] train loss: 1.0919, train acc: 0.5410, val loss: 1.0683, val acc: 0.5524  (best train acc: 0.5661, best val acc: 0.5622)\n",
      "[Epoch: 34420] train loss: 1.0801, train acc: 0.5504, val loss: 1.0682, val acc: 0.5487  (best train acc: 0.5661, best val acc: 0.5622)\n",
      "[Epoch: 34440] train loss: 1.0665, train acc: 0.5526, val loss: 1.0693, val acc: 0.5585  (best train acc: 0.5661, best val acc: 0.5622)\n",
      "[Epoch: 34460] train loss: 1.0585, train acc: 0.5583, val loss: 1.0677, val acc: 0.5487  (best train acc: 0.5661, best val acc: 0.5622)\n",
      "[Epoch: 34480] train loss: 1.0912, train acc: 0.5437, val loss: 1.0688, val acc: 0.5504  (best train acc: 0.5661, best val acc: 0.5622)\n",
      "[Epoch: 34500] train loss: 1.0774, train acc: 0.5485, val loss: 1.0686, val acc: 0.5551  (best train acc: 0.5661, best val acc: 0.5622)\n",
      "[Epoch: 34520] train loss: 1.0728, train acc: 0.5447, val loss: 1.0678, val acc: 0.5535  (best train acc: 0.5661, best val acc: 0.5622)\n",
      "[Epoch: 34540] train loss: 1.0792, train acc: 0.5490, val loss: 1.0671, val acc: 0.5524  (best train acc: 0.5661, best val acc: 0.5622)\n",
      "[Epoch: 34560] train loss: 1.0845, train acc: 0.5446, val loss: 1.0679, val acc: 0.5555  (best train acc: 0.5661, best val acc: 0.5622)\n",
      "[Epoch: 34580] train loss: 1.0819, train acc: 0.5463, val loss: 1.0752, val acc: 0.5390  (best train acc: 0.5661, best val acc: 0.5622)\n",
      "[Epoch: 34600] train loss: 1.0673, train acc: 0.5555, val loss: 1.0678, val acc: 0.5545  (best train acc: 0.5675, best val acc: 0.5622)\n",
      "[Epoch: 34620] train loss: 1.1057, train acc: 0.5332, val loss: 1.0689, val acc: 0.5460  (best train acc: 0.5675, best val acc: 0.5622)\n",
      "[Epoch: 34640] train loss: 1.0622, train acc: 0.5579, val loss: 1.0730, val acc: 0.5545  (best train acc: 0.5689, best val acc: 0.5622)\n",
      "[Epoch: 34660] train loss: 1.0778, train acc: 0.5475, val loss: 1.0703, val acc: 0.5501  (best train acc: 0.5689, best val acc: 0.5622)\n",
      "[Epoch: 34680] train loss: 1.0593, train acc: 0.5575, val loss: 1.0689, val acc: 0.5508  (best train acc: 0.5689, best val acc: 0.5622)\n",
      "[Epoch: 34700] train loss: 1.0531, train acc: 0.5571, val loss: 1.0675, val acc: 0.5568  (best train acc: 0.5689, best val acc: 0.5622)\n",
      "[Epoch: 34720] train loss: 1.0684, train acc: 0.5547, val loss: 1.0685, val acc: 0.5518  (best train acc: 0.5689, best val acc: 0.5622)\n",
      "[Epoch: 34740] train loss: 1.0686, train acc: 0.5555, val loss: 1.0677, val acc: 0.5555  (best train acc: 0.5689, best val acc: 0.5622)\n",
      "[Epoch: 34760] train loss: 1.1037, train acc: 0.5352, val loss: 1.0673, val acc: 0.5511  (best train acc: 0.5689, best val acc: 0.5622)\n",
      "[Epoch: 34780] train loss: 1.0647, train acc: 0.5568, val loss: 1.0717, val acc: 0.5427  (best train acc: 0.5689, best val acc: 0.5622)\n",
      "[Epoch: 34800] train loss: 1.0715, train acc: 0.5481, val loss: 1.0678, val acc: 0.5551  (best train acc: 0.5689, best val acc: 0.5622)\n",
      "[Epoch: 34820] train loss: 1.0720, train acc: 0.5463, val loss: 1.0688, val acc: 0.5568  (best train acc: 0.5689, best val acc: 0.5622)\n",
      "[Epoch: 34840] train loss: 1.0765, train acc: 0.5504, val loss: 1.0703, val acc: 0.5575  (best train acc: 0.5689, best val acc: 0.5622)\n",
      "[Epoch: 34860] train loss: 1.0673, train acc: 0.5575, val loss: 1.0666, val acc: 0.5504  (best train acc: 0.5689, best val acc: 0.5622)\n",
      "[Epoch: 34880] train loss: 1.0807, train acc: 0.5436, val loss: 1.0662, val acc: 0.5548  (best train acc: 0.5689, best val acc: 0.5622)\n",
      "[Epoch: 34900] train loss: 1.0609, train acc: 0.5560, val loss: 1.0682, val acc: 0.5494  (best train acc: 0.5689, best val acc: 0.5622)\n",
      "[Epoch: 34920] train loss: 1.0583, train acc: 0.5579, val loss: 1.0682, val acc: 0.5548  (best train acc: 0.5689, best val acc: 0.5622)\n",
      "[Epoch: 34940] train loss: 1.0611, train acc: 0.5541, val loss: 1.0665, val acc: 0.5521  (best train acc: 0.5689, best val acc: 0.5622)\n",
      "[Epoch: 34960] train loss: 1.0879, train acc: 0.5402, val loss: 1.0661, val acc: 0.5551  (best train acc: 0.5689, best val acc: 0.5622)\n",
      "[Epoch: 34980] train loss: 1.0958, train acc: 0.5381, val loss: 1.0668, val acc: 0.5524  (best train acc: 0.5695, best val acc: 0.5622)\n",
      "[Epoch: 35000] train loss: 1.0660, train acc: 0.5574, val loss: 1.0701, val acc: 0.5551  (best train acc: 0.5695, best val acc: 0.5622)\n",
      "[Epoch: 35020] train loss: 1.0785, train acc: 0.5492, val loss: 1.0671, val acc: 0.5585  (best train acc: 0.5695, best val acc: 0.5622)\n",
      "[Epoch: 35040] train loss: 1.0716, train acc: 0.5538, val loss: 1.0697, val acc: 0.5460  (best train acc: 0.5695, best val acc: 0.5622)\n",
      "[Epoch: 35060] train loss: 1.0600, train acc: 0.5622, val loss: 1.0684, val acc: 0.5578  (best train acc: 0.5695, best val acc: 0.5622)\n",
      "[Epoch: 35080] train loss: 1.0746, train acc: 0.5528, val loss: 1.0703, val acc: 0.5592  (best train acc: 0.5695, best val acc: 0.5622)\n",
      "[Epoch: 35100] train loss: 1.0803, train acc: 0.5455, val loss: 1.0683, val acc: 0.5481  (best train acc: 0.5695, best val acc: 0.5622)\n",
      "[Epoch: 35120] train loss: 1.0656, train acc: 0.5545, val loss: 1.0701, val acc: 0.5562  (best train acc: 0.5695, best val acc: 0.5622)\n",
      "[Epoch: 35140] train loss: 1.0566, train acc: 0.5560, val loss: 1.0663, val acc: 0.5524  (best train acc: 0.5695, best val acc: 0.5622)\n",
      "[Epoch: 35160] train loss: 1.0723, train acc: 0.5512, val loss: 1.0658, val acc: 0.5491  (best train acc: 0.5695, best val acc: 0.5622)\n",
      "[Epoch: 35180] train loss: 1.0685, train acc: 0.5480, val loss: 1.0666, val acc: 0.5528  (best train acc: 0.5695, best val acc: 0.5622)\n",
      "[Epoch: 35200] train loss: 1.0885, train acc: 0.5409, val loss: 1.0695, val acc: 0.5528  (best train acc: 0.5695, best val acc: 0.5622)\n",
      "[Epoch: 35220] train loss: 1.0823, train acc: 0.5470, val loss: 1.0702, val acc: 0.5599  (best train acc: 0.5695, best val acc: 0.5622)\n",
      "[Epoch: 35240] train loss: 1.0604, train acc: 0.5528, val loss: 1.0650, val acc: 0.5514  (best train acc: 0.5695, best val acc: 0.5622)\n",
      "[Epoch: 35260] train loss: 1.0650, train acc: 0.5517, val loss: 1.0659, val acc: 0.5535  (best train acc: 0.5695, best val acc: 0.5622)\n",
      "[Epoch: 35280] train loss: 1.0603, train acc: 0.5544, val loss: 1.0652, val acc: 0.5528  (best train acc: 0.5695, best val acc: 0.5622)\n",
      "[Epoch: 35300] train loss: 1.0918, train acc: 0.5382, val loss: 1.0658, val acc: 0.5521  (best train acc: 0.5695, best val acc: 0.5622)\n",
      "[Epoch: 35320] train loss: 1.0905, train acc: 0.5399, val loss: 1.0659, val acc: 0.5541  (best train acc: 0.5695, best val acc: 0.5622)\n",
      "[Epoch: 35340] train loss: 1.0784, train acc: 0.5479, val loss: 1.0664, val acc: 0.5504  (best train acc: 0.5695, best val acc: 0.5622)\n",
      "[Epoch: 35360] train loss: 1.0642, train acc: 0.5500, val loss: 1.0658, val acc: 0.5568  (best train acc: 0.5695, best val acc: 0.5622)\n",
      "[Epoch: 35380] train loss: 1.0470, train acc: 0.5648, val loss: 1.0690, val acc: 0.5524  (best train acc: 0.5695, best val acc: 0.5622)\n",
      "[Epoch: 35400] train loss: 1.0938, train acc: 0.5429, val loss: 1.0792, val acc: 0.5450  (best train acc: 0.5695, best val acc: 0.5622)\n",
      "[Epoch: 35420] train loss: 1.0613, train acc: 0.5535, val loss: 1.0676, val acc: 0.5524  (best train acc: 0.5695, best val acc: 0.5622)\n",
      "[Epoch: 35440] train loss: 1.0799, train acc: 0.5455, val loss: 1.0658, val acc: 0.5491  (best train acc: 0.5695, best val acc: 0.5622)\n",
      "[Epoch: 35460] train loss: 1.0850, train acc: 0.5421, val loss: 1.0682, val acc: 0.5497  (best train acc: 0.5695, best val acc: 0.5622)\n",
      "[Epoch: 35480] train loss: 1.0827, train acc: 0.5461, val loss: 1.0669, val acc: 0.5524  (best train acc: 0.5695, best val acc: 0.5622)\n",
      "[Epoch: 35500] train loss: 1.0656, train acc: 0.5517, val loss: 1.0652, val acc: 0.5531  (best train acc: 0.5695, best val acc: 0.5622)\n",
      "[Epoch: 35520] train loss: 1.0646, train acc: 0.5594, val loss: 1.0655, val acc: 0.5555  (best train acc: 0.5695, best val acc: 0.5622)\n",
      "[Epoch: 35540] train loss: 1.0831, train acc: 0.5432, val loss: 1.0646, val acc: 0.5535  (best train acc: 0.5695, best val acc: 0.5622)\n",
      "[Epoch: 35560] train loss: 1.0836, train acc: 0.5380, val loss: 1.0651, val acc: 0.5497  (best train acc: 0.5695, best val acc: 0.5622)\n",
      "[Epoch: 35580] train loss: 1.0795, train acc: 0.5458, val loss: 1.0664, val acc: 0.5572  (best train acc: 0.5695, best val acc: 0.5622)\n",
      "[Epoch: 35600] train loss: 1.0857, train acc: 0.5429, val loss: 1.0649, val acc: 0.5572  (best train acc: 0.5695, best val acc: 0.5622)\n",
      "[Epoch: 35620] train loss: 1.0624, train acc: 0.5509, val loss: 1.0643, val acc: 0.5545  (best train acc: 0.5695, best val acc: 0.5622)\n",
      "[Epoch: 35640] train loss: 1.0580, train acc: 0.5565, val loss: 1.0653, val acc: 0.5521  (best train acc: 0.5695, best val acc: 0.5622)\n",
      "[Epoch: 35660] train loss: 1.0825, train acc: 0.5476, val loss: 1.0662, val acc: 0.5538  (best train acc: 0.5695, best val acc: 0.5622)\n",
      "[Epoch: 35680] train loss: 1.0628, train acc: 0.5558, val loss: 1.0645, val acc: 0.5565  (best train acc: 0.5695, best val acc: 0.5622)\n",
      "[Epoch: 35700] train loss: 1.0823, train acc: 0.5403, val loss: 1.0685, val acc: 0.5497  (best train acc: 0.5695, best val acc: 0.5622)\n",
      "[Epoch: 35720] train loss: 1.0830, train acc: 0.5440, val loss: 1.0665, val acc: 0.5558  (best train acc: 0.5695, best val acc: 0.5622)\n",
      "[Epoch: 35740] train loss: 1.0883, train acc: 0.5435, val loss: 1.0642, val acc: 0.5521  (best train acc: 0.5695, best val acc: 0.5622)\n",
      "[Epoch: 35760] train loss: 1.0552, train acc: 0.5630, val loss: 1.0647, val acc: 0.5551  (best train acc: 0.5695, best val acc: 0.5622)\n",
      "[Epoch: 35780] train loss: 1.0700, train acc: 0.5496, val loss: 1.0640, val acc: 0.5504  (best train acc: 0.5695, best val acc: 0.5622)\n",
      "[Epoch: 35800] train loss: 1.0831, train acc: 0.5431, val loss: 1.0675, val acc: 0.5538  (best train acc: 0.5695, best val acc: 0.5622)\n",
      "[Epoch: 35820] train loss: 1.0849, train acc: 0.5409, val loss: 1.0674, val acc: 0.5497  (best train acc: 0.5695, best val acc: 0.5622)\n",
      "[Epoch: 35840] train loss: 1.0796, train acc: 0.5428, val loss: 1.0650, val acc: 0.5551  (best train acc: 0.5695, best val acc: 0.5622)\n",
      "[Epoch: 35860] train loss: 1.0887, train acc: 0.5391, val loss: 1.0647, val acc: 0.5521  (best train acc: 0.5695, best val acc: 0.5622)\n",
      "[Epoch: 35880] train loss: 1.0881, train acc: 0.5411, val loss: 1.0650, val acc: 0.5514  (best train acc: 0.5695, best val acc: 0.5622)\n",
      "[Epoch: 35900] train loss: 1.0607, train acc: 0.5567, val loss: 1.0658, val acc: 0.5518  (best train acc: 0.5695, best val acc: 0.5622)\n",
      "[Epoch: 35920] train loss: 1.0689, train acc: 0.5512, val loss: 1.0641, val acc: 0.5504  (best train acc: 0.5695, best val acc: 0.5626)\n",
      "[Epoch: 35940] train loss: 1.0698, train acc: 0.5531, val loss: 1.0634, val acc: 0.5508  (best train acc: 0.5695, best val acc: 0.5626)\n",
      "[Epoch: 35960] train loss: 1.0568, train acc: 0.5539, val loss: 1.0670, val acc: 0.5575  (best train acc: 0.5695, best val acc: 0.5626)\n",
      "[Epoch: 35980] train loss: 1.0566, train acc: 0.5534, val loss: 1.0632, val acc: 0.5548  (best train acc: 0.5695, best val acc: 0.5626)\n",
      "[Epoch: 36000] train loss: 1.0555, train acc: 0.5578, val loss: 1.0640, val acc: 0.5592  (best train acc: 0.5695, best val acc: 0.5626)\n",
      "[Epoch: 36020] train loss: 1.0662, train acc: 0.5505, val loss: 1.0636, val acc: 0.5545  (best train acc: 0.5695, best val acc: 0.5626)\n",
      "[Epoch: 36040] train loss: 1.0739, train acc: 0.5478, val loss: 1.0666, val acc: 0.5494  (best train acc: 0.5695, best val acc: 0.5626)\n",
      "[Epoch: 36060] train loss: 1.0720, train acc: 0.5522, val loss: 1.0674, val acc: 0.5518  (best train acc: 0.5695, best val acc: 0.5626)\n",
      "[Epoch: 36080] train loss: 1.0540, train acc: 0.5628, val loss: 1.0642, val acc: 0.5558  (best train acc: 0.5695, best val acc: 0.5626)\n",
      "[Epoch: 36100] train loss: 1.0649, train acc: 0.5568, val loss: 1.0640, val acc: 0.5494  (best train acc: 0.5695, best val acc: 0.5626)\n",
      "[Epoch: 36120] train loss: 1.0874, train acc: 0.5380, val loss: 1.0645, val acc: 0.5558  (best train acc: 0.5695, best val acc: 0.5626)\n",
      "[Epoch: 36140] train loss: 1.0672, train acc: 0.5525, val loss: 1.0624, val acc: 0.5524  (best train acc: 0.5695, best val acc: 0.5626)\n",
      "[Epoch: 36160] train loss: 1.0767, train acc: 0.5507, val loss: 1.0623, val acc: 0.5524  (best train acc: 0.5695, best val acc: 0.5626)\n",
      "[Epoch: 36180] train loss: 1.0691, train acc: 0.5544, val loss: 1.0627, val acc: 0.5541  (best train acc: 0.5695, best val acc: 0.5626)\n",
      "[Epoch: 36200] train loss: 1.0804, train acc: 0.5478, val loss: 1.0629, val acc: 0.5568  (best train acc: 0.5695, best val acc: 0.5626)\n",
      "[Epoch: 36220] train loss: 1.0596, train acc: 0.5594, val loss: 1.0622, val acc: 0.5511  (best train acc: 0.5695, best val acc: 0.5626)\n",
      "[Epoch: 36240] train loss: 1.0563, train acc: 0.5628, val loss: 1.0639, val acc: 0.5612  (best train acc: 0.5695, best val acc: 0.5626)\n",
      "[Epoch: 36260] train loss: 1.0750, train acc: 0.5484, val loss: 1.0638, val acc: 0.5504  (best train acc: 0.5695, best val acc: 0.5629)\n",
      "[Epoch: 36280] train loss: 1.0722, train acc: 0.5470, val loss: 1.0742, val acc: 0.5413  (best train acc: 0.5695, best val acc: 0.5629)\n",
      "[Epoch: 36300] train loss: 1.0715, train acc: 0.5508, val loss: 1.0636, val acc: 0.5568  (best train acc: 0.5695, best val acc: 0.5629)\n",
      "[Epoch: 36320] train loss: 1.0761, train acc: 0.5505, val loss: 1.0650, val acc: 0.5575  (best train acc: 0.5695, best val acc: 0.5629)\n",
      "[Epoch: 36340] train loss: 1.0556, train acc: 0.5572, val loss: 1.0625, val acc: 0.5555  (best train acc: 0.5695, best val acc: 0.5629)\n",
      "[Epoch: 36360] train loss: 1.0640, train acc: 0.5557, val loss: 1.0635, val acc: 0.5551  (best train acc: 0.5695, best val acc: 0.5629)\n",
      "[Epoch: 36380] train loss: 1.0686, train acc: 0.5510, val loss: 1.0628, val acc: 0.5535  (best train acc: 0.5695, best val acc: 0.5629)\n",
      "[Epoch: 36400] train loss: 1.0654, train acc: 0.5530, val loss: 1.0638, val acc: 0.5558  (best train acc: 0.5695, best val acc: 0.5629)\n",
      "[Epoch: 36420] train loss: 1.0648, train acc: 0.5547, val loss: 1.0665, val acc: 0.5551  (best train acc: 0.5695, best val acc: 0.5629)\n",
      "[Epoch: 36440] train loss: 1.0810, train acc: 0.5476, val loss: 1.0639, val acc: 0.5538  (best train acc: 0.5695, best val acc: 0.5629)\n",
      "[Epoch: 36460] train loss: 1.0670, train acc: 0.5523, val loss: 1.0623, val acc: 0.5535  (best train acc: 0.5695, best val acc: 0.5629)\n",
      "[Epoch: 36480] train loss: 1.0699, train acc: 0.5507, val loss: 1.0634, val acc: 0.5508  (best train acc: 0.5695, best val acc: 0.5629)\n",
      "[Epoch: 36500] train loss: 1.0646, train acc: 0.5535, val loss: 1.0633, val acc: 0.5582  (best train acc: 0.5695, best val acc: 0.5629)\n",
      "[Epoch: 36520] train loss: 1.0530, train acc: 0.5588, val loss: 1.0633, val acc: 0.5585  (best train acc: 0.5695, best val acc: 0.5629)\n",
      "[Epoch: 36540] train loss: 1.0668, train acc: 0.5499, val loss: 1.0622, val acc: 0.5555  (best train acc: 0.5695, best val acc: 0.5629)\n",
      "[Epoch: 36560] train loss: 1.0643, train acc: 0.5543, val loss: 1.0641, val acc: 0.5589  (best train acc: 0.5695, best val acc: 0.5629)\n",
      "[Epoch: 36580] train loss: 1.0662, train acc: 0.5484, val loss: 1.0639, val acc: 0.5558  (best train acc: 0.5695, best val acc: 0.5629)\n",
      "[Epoch: 36600] train loss: 1.0848, train acc: 0.5434, val loss: 1.0629, val acc: 0.5535  (best train acc: 0.5695, best val acc: 0.5629)\n",
      "[Epoch: 36620] train loss: 1.0714, train acc: 0.5544, val loss: 1.0627, val acc: 0.5578  (best train acc: 0.5695, best val acc: 0.5629)\n",
      "[Epoch: 36640] train loss: 1.0745, train acc: 0.5506, val loss: 1.0627, val acc: 0.5562  (best train acc: 0.5695, best val acc: 0.5629)\n",
      "[Epoch: 36660] train loss: 1.0671, train acc: 0.5498, val loss: 1.0627, val acc: 0.5545  (best train acc: 0.5695, best val acc: 0.5629)\n",
      "[Epoch: 36680] train loss: 1.0780, train acc: 0.5471, val loss: 1.0625, val acc: 0.5531  (best train acc: 0.5695, best val acc: 0.5629)\n",
      "[Epoch: 36700] train loss: 1.0727, train acc: 0.5510, val loss: 1.0634, val acc: 0.5592  (best train acc: 0.5695, best val acc: 0.5629)\n",
      "[Epoch: 36720] train loss: 1.0930, train acc: 0.5399, val loss: 1.0623, val acc: 0.5531  (best train acc: 0.5695, best val acc: 0.5629)\n",
      "[Epoch: 36740] train loss: 1.0794, train acc: 0.5457, val loss: 1.0651, val acc: 0.5531  (best train acc: 0.5695, best val acc: 0.5629)\n",
      "[Epoch: 36760] train loss: 1.0701, train acc: 0.5492, val loss: 1.0622, val acc: 0.5541  (best train acc: 0.5695, best val acc: 0.5629)\n",
      "[Epoch: 36780] train loss: 1.0631, train acc: 0.5556, val loss: 1.0638, val acc: 0.5548  (best train acc: 0.5695, best val acc: 0.5629)\n",
      "[Epoch: 36800] train loss: 1.0453, train acc: 0.5632, val loss: 1.0618, val acc: 0.5548  (best train acc: 0.5695, best val acc: 0.5629)\n",
      "[Epoch: 36820] train loss: 1.0440, train acc: 0.5690, val loss: 1.0656, val acc: 0.5551  (best train acc: 0.5695, best val acc: 0.5629)\n",
      "[Epoch: 36840] train loss: 1.0844, train acc: 0.5442, val loss: 1.0622, val acc: 0.5582  (best train acc: 0.5695, best val acc: 0.5629)\n",
      "[Epoch: 36860] train loss: 1.0576, train acc: 0.5519, val loss: 1.0720, val acc: 0.5491  (best train acc: 0.5695, best val acc: 0.5629)\n",
      "[Epoch: 36880] train loss: 1.0818, train acc: 0.5455, val loss: 1.0668, val acc: 0.5464  (best train acc: 0.5695, best val acc: 0.5629)\n",
      "[Epoch: 36900] train loss: 1.0615, train acc: 0.5568, val loss: 1.0656, val acc: 0.5548  (best train acc: 0.5695, best val acc: 0.5629)\n",
      "[Epoch: 36920] train loss: 1.0793, train acc: 0.5500, val loss: 1.0679, val acc: 0.5548  (best train acc: 0.5695, best val acc: 0.5629)\n",
      "[Epoch: 36940] train loss: 1.0487, train acc: 0.5639, val loss: 1.0634, val acc: 0.5511  (best train acc: 0.5695, best val acc: 0.5629)\n",
      "[Epoch: 36960] train loss: 1.0923, train acc: 0.5333, val loss: 1.0610, val acc: 0.5568  (best train acc: 0.5695, best val acc: 0.5629)\n",
      "[Epoch: 36980] train loss: 1.0638, train acc: 0.5555, val loss: 1.0616, val acc: 0.5555  (best train acc: 0.5695, best val acc: 0.5629)\n",
      "[Epoch: 37000] train loss: 1.0809, train acc: 0.5450, val loss: 1.0627, val acc: 0.5599  (best train acc: 0.5695, best val acc: 0.5629)\n",
      "[Epoch: 37020] train loss: 1.0913, train acc: 0.5402, val loss: 1.0637, val acc: 0.5501  (best train acc: 0.5695, best val acc: 0.5629)\n",
      "[Epoch: 37040] train loss: 1.0796, train acc: 0.5424, val loss: 1.0683, val acc: 0.5531  (best train acc: 0.5695, best val acc: 0.5629)\n",
      "[Epoch: 37060] train loss: 1.0864, train acc: 0.5460, val loss: 1.0646, val acc: 0.5501  (best train acc: 0.5696, best val acc: 0.5629)\n",
      "[Epoch: 37080] train loss: 1.0500, train acc: 0.5626, val loss: 1.0619, val acc: 0.5572  (best train acc: 0.5696, best val acc: 0.5629)\n",
      "[Epoch: 37100] train loss: 1.0715, train acc: 0.5473, val loss: 1.0683, val acc: 0.5487  (best train acc: 0.5696, best val acc: 0.5629)\n",
      "[Epoch: 37120] train loss: 1.0627, train acc: 0.5583, val loss: 1.0618, val acc: 0.5558  (best train acc: 0.5696, best val acc: 0.5629)\n",
      "[Epoch: 37140] train loss: 1.0733, train acc: 0.5543, val loss: 1.0603, val acc: 0.5568  (best train acc: 0.5696, best val acc: 0.5629)\n",
      "[Epoch: 37160] train loss: 1.0820, train acc: 0.5427, val loss: 1.0626, val acc: 0.5595  (best train acc: 0.5696, best val acc: 0.5629)\n",
      "[Epoch: 37180] train loss: 1.0623, train acc: 0.5510, val loss: 1.0619, val acc: 0.5589  (best train acc: 0.5696, best val acc: 0.5629)\n",
      "[Epoch: 37200] train loss: 1.0885, train acc: 0.5438, val loss: 1.0611, val acc: 0.5528  (best train acc: 0.5696, best val acc: 0.5629)\n",
      "[Epoch: 37220] train loss: 1.0794, train acc: 0.5482, val loss: 1.0706, val acc: 0.5454  (best train acc: 0.5696, best val acc: 0.5629)\n",
      "[Epoch: 37240] train loss: 1.0624, train acc: 0.5565, val loss: 1.0675, val acc: 0.5626  (best train acc: 0.5696, best val acc: 0.5629)\n",
      "[Epoch: 37260] train loss: 1.0530, train acc: 0.5596, val loss: 1.0617, val acc: 0.5562  (best train acc: 0.5696, best val acc: 0.5629)\n",
      "[Epoch: 37280] train loss: 1.0775, train acc: 0.5464, val loss: 1.0612, val acc: 0.5575  (best train acc: 0.5696, best val acc: 0.5629)\n",
      "[Epoch: 37300] train loss: 1.0672, train acc: 0.5544, val loss: 1.0625, val acc: 0.5589  (best train acc: 0.5696, best val acc: 0.5629)\n",
      "[Epoch: 37320] train loss: 1.0563, train acc: 0.5639, val loss: 1.0627, val acc: 0.5524  (best train acc: 0.5696, best val acc: 0.5629)\n",
      "[Epoch: 37340] train loss: 1.0749, train acc: 0.5499, val loss: 1.0642, val acc: 0.5602  (best train acc: 0.5696, best val acc: 0.5629)\n",
      "[Epoch: 37360] train loss: 1.0826, train acc: 0.5398, val loss: 1.0645, val acc: 0.5514  (best train acc: 0.5696, best val acc: 0.5629)\n",
      "[Epoch: 37380] train loss: 1.0665, train acc: 0.5488, val loss: 1.0623, val acc: 0.5585  (best train acc: 0.5696, best val acc: 0.5629)\n",
      "[Epoch: 37400] train loss: 1.0704, train acc: 0.5509, val loss: 1.0620, val acc: 0.5578  (best train acc: 0.5696, best val acc: 0.5629)\n",
      "[Epoch: 37420] train loss: 1.0731, train acc: 0.5460, val loss: 1.0607, val acc: 0.5562  (best train acc: 0.5696, best val acc: 0.5629)\n",
      "[Epoch: 37440] train loss: 1.0641, train acc: 0.5556, val loss: 1.0617, val acc: 0.5565  (best train acc: 0.5696, best val acc: 0.5636)\n",
      "[Epoch: 37460] train loss: 1.0621, train acc: 0.5531, val loss: 1.0628, val acc: 0.5518  (best train acc: 0.5696, best val acc: 0.5636)\n",
      "[Epoch: 37480] train loss: 1.0749, train acc: 0.5463, val loss: 1.0632, val acc: 0.5535  (best train acc: 0.5696, best val acc: 0.5636)\n",
      "[Epoch: 37500] train loss: 1.0686, train acc: 0.5507, val loss: 1.0631, val acc: 0.5562  (best train acc: 0.5702, best val acc: 0.5636)\n",
      "[Epoch: 37520] train loss: 1.0516, train acc: 0.5578, val loss: 1.0616, val acc: 0.5518  (best train acc: 0.5702, best val acc: 0.5642)\n",
      "[Epoch: 37540] train loss: 1.0780, train acc: 0.5471, val loss: 1.0598, val acc: 0.5545  (best train acc: 0.5702, best val acc: 0.5642)\n",
      "[Epoch: 37560] train loss: 1.0772, train acc: 0.5500, val loss: 1.0610, val acc: 0.5568  (best train acc: 0.5702, best val acc: 0.5642)\n",
      "[Epoch: 37580] train loss: 1.0542, train acc: 0.5606, val loss: 1.0697, val acc: 0.5467  (best train acc: 0.5702, best val acc: 0.5642)\n",
      "[Epoch: 37600] train loss: 1.0606, train acc: 0.5521, val loss: 1.0604, val acc: 0.5538  (best train acc: 0.5702, best val acc: 0.5646)\n",
      "[Epoch: 37620] train loss: 1.0559, train acc: 0.5569, val loss: 1.0609, val acc: 0.5599  (best train acc: 0.5702, best val acc: 0.5646)\n",
      "[Epoch: 37640] train loss: 1.0790, train acc: 0.5514, val loss: 1.0602, val acc: 0.5538  (best train acc: 0.5702, best val acc: 0.5646)\n",
      "[Epoch: 37660] train loss: 1.0713, train acc: 0.5510, val loss: 1.0607, val acc: 0.5568  (best train acc: 0.5702, best val acc: 0.5646)\n",
      "[Epoch: 37680] train loss: 1.0570, train acc: 0.5519, val loss: 1.0638, val acc: 0.5585  (best train acc: 0.5702, best val acc: 0.5646)\n",
      "[Epoch: 37700] train loss: 1.0752, train acc: 0.5447, val loss: 1.0600, val acc: 0.5494  (best train acc: 0.5702, best val acc: 0.5646)\n",
      "[Epoch: 37720] train loss: 1.0658, train acc: 0.5558, val loss: 1.0603, val acc: 0.5582  (best train acc: 0.5702, best val acc: 0.5646)\n",
      "[Epoch: 37740] train loss: 1.0709, train acc: 0.5494, val loss: 1.0611, val acc: 0.5572  (best train acc: 0.5702, best val acc: 0.5646)\n",
      "[Epoch: 37760] train loss: 1.0587, train acc: 0.5530, val loss: 1.0595, val acc: 0.5555  (best train acc: 0.5702, best val acc: 0.5646)\n",
      "[Epoch: 37780] train loss: 1.0894, train acc: 0.5432, val loss: 1.0597, val acc: 0.5568  (best train acc: 0.5702, best val acc: 0.5646)\n",
      "[Epoch: 37800] train loss: 1.0505, train acc: 0.5566, val loss: 1.0631, val acc: 0.5568  (best train acc: 0.5702, best val acc: 0.5646)\n",
      "[Epoch: 37820] train loss: 1.0688, train acc: 0.5528, val loss: 1.0601, val acc: 0.5565  (best train acc: 0.5702, best val acc: 0.5646)\n",
      "[Epoch: 37840] train loss: 1.0673, train acc: 0.5508, val loss: 1.0638, val acc: 0.5578  (best train acc: 0.5702, best val acc: 0.5646)\n",
      "[Epoch: 37860] train loss: 1.0621, train acc: 0.5523, val loss: 1.0613, val acc: 0.5592  (best train acc: 0.5702, best val acc: 0.5646)\n",
      "[Epoch: 37880] train loss: 1.0907, train acc: 0.5372, val loss: 1.0590, val acc: 0.5562  (best train acc: 0.5702, best val acc: 0.5646)\n",
      "[Epoch: 37900] train loss: 1.0623, train acc: 0.5516, val loss: 1.0593, val acc: 0.5538  (best train acc: 0.5702, best val acc: 0.5646)\n",
      "[Epoch: 37920] train loss: 1.0676, train acc: 0.5547, val loss: 1.0596, val acc: 0.5551  (best train acc: 0.5702, best val acc: 0.5646)\n",
      "[Epoch: 37940] train loss: 1.0539, train acc: 0.5608, val loss: 1.0596, val acc: 0.5575  (best train acc: 0.5702, best val acc: 0.5646)\n",
      "[Epoch: 37960] train loss: 1.0811, train acc: 0.5476, val loss: 1.0594, val acc: 0.5548  (best train acc: 0.5702, best val acc: 0.5646)\n",
      "[Epoch: 37980] train loss: 1.0608, train acc: 0.5587, val loss: 1.0606, val acc: 0.5538  (best train acc: 0.5702, best val acc: 0.5646)\n",
      "[Epoch: 38000] train loss: 1.0636, train acc: 0.5496, val loss: 1.0614, val acc: 0.5545  (best train acc: 0.5702, best val acc: 0.5646)\n",
      "[Epoch: 38020] train loss: 1.0803, train acc: 0.5460, val loss: 1.0588, val acc: 0.5568  (best train acc: 0.5702, best val acc: 0.5646)\n",
      "[Epoch: 38040] train loss: 1.0861, train acc: 0.5388, val loss: 1.0599, val acc: 0.5528  (best train acc: 0.5702, best val acc: 0.5646)\n",
      "[Epoch: 38060] train loss: 1.0690, train acc: 0.5505, val loss: 1.0602, val acc: 0.5575  (best train acc: 0.5702, best val acc: 0.5646)\n",
      "[Epoch: 38080] train loss: 1.0549, train acc: 0.5580, val loss: 1.0599, val acc: 0.5551  (best train acc: 0.5702, best val acc: 0.5646)\n",
      "[Epoch: 38100] train loss: 1.0572, train acc: 0.5533, val loss: 1.0618, val acc: 0.5609  (best train acc: 0.5702, best val acc: 0.5646)\n",
      "[Epoch: 38120] train loss: 1.6144, train acc: 0.4493, val loss: 1.6182, val acc: 0.3946  (best train acc: 0.5702, best val acc: 0.5646)\n",
      "[Epoch: 38140] train loss: 1.5364, train acc: 0.4028, val loss: 1.4953, val acc: 0.4182  (best train acc: 0.5702, best val acc: 0.5646)\n",
      "[Epoch: 38160] train loss: 1.1512, train acc: 0.5186, val loss: 1.1566, val acc: 0.5288  (best train acc: 0.5702, best val acc: 0.5646)\n",
      "[Epoch: 38180] train loss: 1.1194, train acc: 0.5376, val loss: 1.1185, val acc: 0.5349  (best train acc: 0.5702, best val acc: 0.5646)\n",
      "[Epoch: 38200] train loss: 1.1129, train acc: 0.5397, val loss: 1.1163, val acc: 0.5339  (best train acc: 0.5702, best val acc: 0.5646)\n",
      "[Epoch: 38220] train loss: 1.1066, train acc: 0.5488, val loss: 1.1149, val acc: 0.5376  (best train acc: 0.5702, best val acc: 0.5646)\n",
      "[Epoch: 38240] train loss: 1.1285, train acc: 0.5335, val loss: 1.1150, val acc: 0.5396  (best train acc: 0.5702, best val acc: 0.5646)\n",
      "[Epoch: 38260] train loss: 1.1355, train acc: 0.5314, val loss: 1.1136, val acc: 0.5379  (best train acc: 0.5702, best val acc: 0.5646)\n",
      "[Epoch: 38280] train loss: 1.1174, train acc: 0.5441, val loss: 1.1129, val acc: 0.5369  (best train acc: 0.5702, best val acc: 0.5646)\n",
      "[Epoch: 38300] train loss: 1.1202, train acc: 0.5400, val loss: 1.1141, val acc: 0.5406  (best train acc: 0.5702, best val acc: 0.5646)\n",
      "[Epoch: 38320] train loss: 1.1100, train acc: 0.5419, val loss: 1.1114, val acc: 0.5369  (best train acc: 0.5702, best val acc: 0.5646)\n",
      "[Epoch: 38340] train loss: 1.1214, train acc: 0.5381, val loss: 1.1109, val acc: 0.5369  (best train acc: 0.5702, best val acc: 0.5646)\n",
      "[Epoch: 38360] train loss: 1.1282, train acc: 0.5283, val loss: 1.1100, val acc: 0.5352  (best train acc: 0.5702, best val acc: 0.5646)\n",
      "[Epoch: 38380] train loss: 1.1245, train acc: 0.5312, val loss: 1.1093, val acc: 0.5363  (best train acc: 0.5702, best val acc: 0.5646)\n",
      "[Epoch: 38400] train loss: 1.1189, train acc: 0.5380, val loss: 1.1089, val acc: 0.5363  (best train acc: 0.5702, best val acc: 0.5646)\n",
      "[Epoch: 38420] train loss: 1.1179, train acc: 0.5364, val loss: 1.1085, val acc: 0.5363  (best train acc: 0.5702, best val acc: 0.5646)\n",
      "[Epoch: 38440] train loss: 1.1141, train acc: 0.5418, val loss: 1.1081, val acc: 0.5403  (best train acc: 0.5702, best val acc: 0.5646)\n",
      "[Epoch: 38460] train loss: 1.1211, train acc: 0.5381, val loss: 1.1079, val acc: 0.5373  (best train acc: 0.5702, best val acc: 0.5646)\n",
      "[Epoch: 38480] train loss: 1.1151, train acc: 0.5377, val loss: 1.1078, val acc: 0.5369  (best train acc: 0.5702, best val acc: 0.5646)\n",
      "[Epoch: 38500] train loss: 1.1124, train acc: 0.5406, val loss: 1.1074, val acc: 0.5369  (best train acc: 0.5702, best val acc: 0.5646)\n",
      "[Epoch: 38520] train loss: 1.1089, train acc: 0.5416, val loss: 1.1087, val acc: 0.5406  (best train acc: 0.5702, best val acc: 0.5646)\n",
      "[Epoch: 38540] train loss: 1.0982, train acc: 0.5433, val loss: 1.1068, val acc: 0.5386  (best train acc: 0.5702, best val acc: 0.5646)\n",
      "[Epoch: 38560] train loss: 1.1250, train acc: 0.5321, val loss: 1.1073, val acc: 0.5383  (best train acc: 0.5702, best val acc: 0.5646)\n",
      "[Epoch: 38580] train loss: 1.1055, train acc: 0.5440, val loss: 1.1096, val acc: 0.5457  (best train acc: 0.5702, best val acc: 0.5646)\n",
      "[Epoch: 38600] train loss: 1.1158, train acc: 0.5408, val loss: 1.1061, val acc: 0.5363  (best train acc: 0.5702, best val acc: 0.5646)\n",
      "[Epoch: 38620] train loss: 1.1225, train acc: 0.5386, val loss: 1.1061, val acc: 0.5373  (best train acc: 0.5702, best val acc: 0.5646)\n",
      "[Epoch: 38640] train loss: 1.0933, train acc: 0.5485, val loss: 1.1055, val acc: 0.5393  (best train acc: 0.5702, best val acc: 0.5646)\n",
      "[Epoch: 38660] train loss: 1.0961, train acc: 0.5443, val loss: 1.1062, val acc: 0.5386  (best train acc: 0.5702, best val acc: 0.5646)\n",
      "[Epoch: 38680] train loss: 1.1143, train acc: 0.5414, val loss: 1.1063, val acc: 0.5393  (best train acc: 0.5702, best val acc: 0.5646)\n",
      "[Epoch: 38700] train loss: 1.1359, train acc: 0.5244, val loss: 1.1056, val acc: 0.5400  (best train acc: 0.5702, best val acc: 0.5646)\n",
      "[Epoch: 38720] train loss: 1.1057, train acc: 0.5446, val loss: 1.1050, val acc: 0.5376  (best train acc: 0.5702, best val acc: 0.5646)\n",
      "[Epoch: 38740] train loss: 1.0991, train acc: 0.5492, val loss: 1.1045, val acc: 0.5383  (best train acc: 0.5702, best val acc: 0.5646)\n",
      "[Epoch: 38760] train loss: 1.1057, train acc: 0.5401, val loss: 1.1055, val acc: 0.5390  (best train acc: 0.5702, best val acc: 0.5646)\n",
      "[Epoch: 38780] train loss: 1.1304, train acc: 0.5314, val loss: 1.1048, val acc: 0.5400  (best train acc: 0.5702, best val acc: 0.5646)\n",
      "[Epoch: 38800] train loss: 1.0929, train acc: 0.5471, val loss: 1.1041, val acc: 0.5363  (best train acc: 0.5702, best val acc: 0.5646)\n",
      "[Epoch: 38820] train loss: 1.0861, train acc: 0.5439, val loss: 1.1037, val acc: 0.5400  (best train acc: 0.5702, best val acc: 0.5646)\n",
      "[Epoch: 38840] train loss: 1.0852, train acc: 0.5503, val loss: 1.1040, val acc: 0.5376  (best train acc: 0.5702, best val acc: 0.5646)\n",
      "[Epoch: 38860] train loss: 1.1022, train acc: 0.5410, val loss: 1.1036, val acc: 0.5383  (best train acc: 0.5702, best val acc: 0.5646)\n",
      "[Epoch: 38880] train loss: 1.1250, train acc: 0.5314, val loss: 1.1034, val acc: 0.5406  (best train acc: 0.5702, best val acc: 0.5646)\n",
      "[Epoch: 38900] train loss: 1.1085, train acc: 0.5349, val loss: 1.1033, val acc: 0.5383  (best train acc: 0.5702, best val acc: 0.5646)\n",
      "[Epoch: 38920] train loss: 1.1027, train acc: 0.5424, val loss: 1.1035, val acc: 0.5393  (best train acc: 0.5702, best val acc: 0.5646)\n",
      "[Epoch: 38940] train loss: 1.1089, train acc: 0.5448, val loss: 1.1030, val acc: 0.5376  (best train acc: 0.5702, best val acc: 0.5646)\n",
      "[Epoch: 38960] train loss: 1.1073, train acc: 0.5352, val loss: 1.1029, val acc: 0.5383  (best train acc: 0.5702, best val acc: 0.5646)\n",
      "[Epoch: 38980] train loss: 1.1163, train acc: 0.5340, val loss: 1.1024, val acc: 0.5400  (best train acc: 0.5702, best val acc: 0.5646)\n",
      "[Epoch: 39000] train loss: 1.0990, train acc: 0.5473, val loss: 1.1031, val acc: 0.5400  (best train acc: 0.5702, best val acc: 0.5646)\n",
      "[Epoch: 39020] train loss: 1.1122, train acc: 0.5383, val loss: 1.1039, val acc: 0.5403  (best train acc: 0.5702, best val acc: 0.5646)\n",
      "[Epoch: 39040] train loss: 1.0958, train acc: 0.5510, val loss: 1.1023, val acc: 0.5390  (best train acc: 0.5702, best val acc: 0.5646)\n",
      "[Epoch: 39060] train loss: 1.0942, train acc: 0.5472, val loss: 1.1054, val acc: 0.5437  (best train acc: 0.5702, best val acc: 0.5646)\n",
      "[Epoch: 39080] train loss: 1.1135, train acc: 0.5421, val loss: 1.1017, val acc: 0.5383  (best train acc: 0.5702, best val acc: 0.5646)\n",
      "[Epoch: 39100] train loss: 1.1333, train acc: 0.5278, val loss: 1.1014, val acc: 0.5406  (best train acc: 0.5702, best val acc: 0.5646)\n",
      "[Epoch: 39120] train loss: 1.1096, train acc: 0.5401, val loss: 1.1017, val acc: 0.5410  (best train acc: 0.5702, best val acc: 0.5646)\n",
      "[Epoch: 39140] train loss: 1.1053, train acc: 0.5363, val loss: 1.1009, val acc: 0.5373  (best train acc: 0.5702, best val acc: 0.5646)\n",
      "[Epoch: 39160] train loss: 1.0987, train acc: 0.5461, val loss: 1.1010, val acc: 0.5373  (best train acc: 0.5702, best val acc: 0.5646)\n",
      "[Epoch: 39180] train loss: 1.0886, train acc: 0.5463, val loss: 1.1015, val acc: 0.5417  (best train acc: 0.5702, best val acc: 0.5646)\n",
      "[Epoch: 39200] train loss: 1.0851, train acc: 0.5487, val loss: 1.1005, val acc: 0.5379  (best train acc: 0.5702, best val acc: 0.5646)\n",
      "[Epoch: 39220] train loss: 1.0912, train acc: 0.5521, val loss: 1.1009, val acc: 0.5396  (best train acc: 0.5702, best val acc: 0.5646)\n",
      "[Epoch: 39240] train loss: 1.0969, train acc: 0.5505, val loss: 1.1006, val acc: 0.5417  (best train acc: 0.5702, best val acc: 0.5646)\n",
      "[Epoch: 39260] train loss: 1.1011, train acc: 0.5370, val loss: 1.0998, val acc: 0.5369  (best train acc: 0.5702, best val acc: 0.5646)\n",
      "[Epoch: 39280] train loss: 1.1042, train acc: 0.5342, val loss: 1.1040, val acc: 0.5322  (best train acc: 0.5702, best val acc: 0.5646)\n",
      "[Epoch: 39300] train loss: 1.1039, train acc: 0.5469, val loss: 1.1003, val acc: 0.5430  (best train acc: 0.5702, best val acc: 0.5646)\n",
      "[Epoch: 39320] train loss: 1.1013, train acc: 0.5463, val loss: 1.0989, val acc: 0.5427  (best train acc: 0.5702, best val acc: 0.5646)\n",
      "[Epoch: 39340] train loss: 1.0789, train acc: 0.5510, val loss: 1.0996, val acc: 0.5440  (best train acc: 0.5702, best val acc: 0.5646)\n",
      "[Epoch: 39360] train loss: 1.0864, train acc: 0.5437, val loss: 1.0992, val acc: 0.5342  (best train acc: 0.5702, best val acc: 0.5646)\n",
      "[Epoch: 39380] train loss: 1.0960, train acc: 0.5481, val loss: 1.0975, val acc: 0.5423  (best train acc: 0.5702, best val acc: 0.5646)\n",
      "[Epoch: 39400] train loss: 1.0824, train acc: 0.5605, val loss: 1.0990, val acc: 0.5433  (best train acc: 0.5702, best val acc: 0.5646)\n",
      "[Epoch: 39420] train loss: 1.1050, train acc: 0.5401, val loss: 1.0966, val acc: 0.5369  (best train acc: 0.5702, best val acc: 0.5646)\n",
      "[Epoch: 39440] train loss: 1.1089, train acc: 0.5354, val loss: 1.0959, val acc: 0.5406  (best train acc: 0.5702, best val acc: 0.5646)\n",
      "[Epoch: 39460] train loss: 1.1191, train acc: 0.5397, val loss: 1.0975, val acc: 0.5437  (best train acc: 0.5702, best val acc: 0.5646)\n",
      "[Epoch: 39480] train loss: 1.1196, train acc: 0.5341, val loss: 1.0954, val acc: 0.5406  (best train acc: 0.5702, best val acc: 0.5646)\n",
      "[Epoch: 39500] train loss: 1.1120, train acc: 0.5355, val loss: 1.0951, val acc: 0.5420  (best train acc: 0.5702, best val acc: 0.5646)\n",
      "[Epoch: 39520] train loss: 1.0874, train acc: 0.5487, val loss: 1.0949, val acc: 0.5430  (best train acc: 0.5702, best val acc: 0.5646)\n",
      "[Epoch: 39540] train loss: 1.0935, train acc: 0.5426, val loss: 1.0942, val acc: 0.5406  (best train acc: 0.5702, best val acc: 0.5646)\n",
      "[Epoch: 39560] train loss: 1.0995, train acc: 0.5431, val loss: 1.0939, val acc: 0.5396  (best train acc: 0.5702, best val acc: 0.5646)\n",
      "[Epoch: 39580] train loss: 1.0826, train acc: 0.5461, val loss: 1.0934, val acc: 0.5363  (best train acc: 0.5702, best val acc: 0.5646)\n",
      "[Epoch: 39600] train loss: 1.1103, train acc: 0.5391, val loss: 1.0938, val acc: 0.5430  (best train acc: 0.5702, best val acc: 0.5646)\n",
      "[Epoch: 39620] train loss: 1.0995, train acc: 0.5455, val loss: 1.0935, val acc: 0.5433  (best train acc: 0.5702, best val acc: 0.5646)\n",
      "[Epoch: 39640] train loss: 1.1071, train acc: 0.5369, val loss: 1.0937, val acc: 0.5447  (best train acc: 0.5702, best val acc: 0.5646)\n",
      "[Epoch: 39660] train loss: 1.0912, train acc: 0.5452, val loss: 1.0936, val acc: 0.5359  (best train acc: 0.5702, best val acc: 0.5646)\n",
      "[Epoch: 39680] train loss: 1.0926, train acc: 0.5445, val loss: 1.0922, val acc: 0.5396  (best train acc: 0.5702, best val acc: 0.5646)\n",
      "[Epoch: 39700] train loss: 1.0833, train acc: 0.5484, val loss: 1.0919, val acc: 0.5383  (best train acc: 0.5702, best val acc: 0.5646)\n",
      "[Epoch: 39720] train loss: 1.1054, train acc: 0.5377, val loss: 1.0917, val acc: 0.5393  (best train acc: 0.5702, best val acc: 0.5646)\n",
      "[Epoch: 39740] train loss: 1.1213, train acc: 0.5323, val loss: 1.0922, val acc: 0.5427  (best train acc: 0.5702, best val acc: 0.5646)\n",
      "[Epoch: 39760] train loss: 1.0850, train acc: 0.5486, val loss: 1.0923, val acc: 0.5437  (best train acc: 0.5702, best val acc: 0.5646)\n",
      "[Epoch: 39780] train loss: 1.0994, train acc: 0.5471, val loss: 1.0911, val acc: 0.5376  (best train acc: 0.5702, best val acc: 0.5646)\n",
      "[Epoch: 39800] train loss: 1.0992, train acc: 0.5435, val loss: 1.0949, val acc: 0.5467  (best train acc: 0.5702, best val acc: 0.5646)\n",
      "[Epoch: 39820] train loss: 1.0888, train acc: 0.5403, val loss: 1.0908, val acc: 0.5390  (best train acc: 0.5702, best val acc: 0.5646)\n",
      "[Epoch: 39840] train loss: 1.1101, train acc: 0.5403, val loss: 1.0907, val acc: 0.5383  (best train acc: 0.5702, best val acc: 0.5646)\n",
      "[Epoch: 39860] train loss: 1.0964, train acc: 0.5501, val loss: 1.0918, val acc: 0.5440  (best train acc: 0.5702, best val acc: 0.5646)\n",
      "[Epoch: 39880] train loss: 1.1030, train acc: 0.5338, val loss: 1.0901, val acc: 0.5420  (best train acc: 0.5702, best val acc: 0.5646)\n",
      "[Epoch: 39900] train loss: 1.0936, train acc: 0.5427, val loss: 1.0900, val acc: 0.5383  (best train acc: 0.5702, best val acc: 0.5646)\n",
      "[Epoch: 39920] train loss: 1.1058, train acc: 0.5309, val loss: 1.0901, val acc: 0.5359  (best train acc: 0.5702, best val acc: 0.5646)\n",
      "[Epoch: 39940] train loss: 1.1124, train acc: 0.5359, val loss: 1.0910, val acc: 0.5464  (best train acc: 0.5702, best val acc: 0.5646)\n",
      "[Epoch: 39960] train loss: 1.0940, train acc: 0.5450, val loss: 1.0909, val acc: 0.5373  (best train acc: 0.5702, best val acc: 0.5646)\n",
      "[Epoch: 39980] train loss: 1.0896, train acc: 0.5510, val loss: 1.0901, val acc: 0.5454  (best train acc: 0.5702, best val acc: 0.5646)\n",
      "[Epoch: 40000] train loss: 1.1155, train acc: 0.5318, val loss: 1.0892, val acc: 0.5396  (best train acc: 0.5702, best val acc: 0.5646)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pmaur\\miniconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\pmaur\\miniconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\pmaur\\miniconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\pmaur\\miniconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\pmaur\\miniconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\pmaur\\miniconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       154\n",
      "           1       0.66      0.63      0.64       703\n",
      "           2       0.39      0.34      0.37       702\n",
      "           3       0.47      0.57      0.52       703\n",
      "           4       0.69      0.81      0.74       702\n",
      "\n",
      "    accuracy                           0.56      2964\n",
      "   macro avg       0.44      0.47      0.45      2964\n",
      "weighted avg       0.52      0.56      0.54      2964\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 504x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 504x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "gnn = GNN7L_GCN(data_with_nedbit).to(device)\n",
    "pred = train(gnn, data_with_nedbit.to(device), 40000, cm_title='GAT7L_multiclass_16HC', classes=['P', 'LP', 'WN', 'LN', 'RN'], weight_decay=0.0005, arch='GCN', layers=7)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SAGE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "775880f73d214fa18b49715c8b52c275",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40001 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch: 0000] train loss: 522.3055, train acc: 0.2729, val loss: 169.1959, val acc: 0.2395  (best train acc: 0.2729, best val acc: 0.2395)\n",
      "[Epoch: 0020] train loss: 114.5587, train acc: 0.2838, val loss: 27.3256, val acc: 0.3761  (best train acc: 0.3060, best val acc: 0.3761)\n",
      "[Epoch: 0040] train loss: 15.7130, train acc: 0.4803, val loss: 6.1319, val acc: 0.5450  (best train acc: 0.4852, best val acc: 0.5484)\n",
      "[Epoch: 0060] train loss: 8.9353, train acc: 0.4276, val loss: 3.7480, val acc: 0.5322  (best train acc: 0.4852, best val acc: 0.5484)\n",
      "[Epoch: 0080] train loss: 4.2486, train acc: 0.3789, val loss: 2.5231, val acc: 0.4604  (best train acc: 0.4852, best val acc: 0.5686)\n",
      "[Epoch: 0100] train loss: 1.9839, train acc: 0.2461, val loss: 1.7723, val acc: 0.2462  (best train acc: 0.4852, best val acc: 0.5686)\n",
      "[Epoch: 0120] train loss: 1.6881, train acc: 0.2533, val loss: 1.6578, val acc: 0.2631  (best train acc: 0.4852, best val acc: 0.5686)\n",
      "[Epoch: 0140] train loss: 1.6405, train acc: 0.2513, val loss: 1.6365, val acc: 0.2624  (best train acc: 0.4852, best val acc: 0.5686)\n",
      "[Epoch: 0160] train loss: 1.6170, train acc: 0.2509, val loss: 1.6216, val acc: 0.2621  (best train acc: 0.4852, best val acc: 0.5686)\n",
      "[Epoch: 0180] train loss: 1.6251, train acc: 0.2504, val loss: 1.6110, val acc: 0.2637  (best train acc: 0.4852, best val acc: 0.5686)\n",
      "[Epoch: 0200] train loss: 1.6109, train acc: 0.2515, val loss: 1.6038, val acc: 0.2637  (best train acc: 0.4852, best val acc: 0.5686)\n",
      "[Epoch: 0220] train loss: 1.5976, train acc: 0.2520, val loss: 1.5967, val acc: 0.2634  (best train acc: 0.4852, best val acc: 0.5686)\n",
      "[Epoch: 0240] train loss: 1.6036, train acc: 0.2496, val loss: 1.5895, val acc: 0.2627  (best train acc: 0.4852, best val acc: 0.5686)\n",
      "[Epoch: 0260] train loss: 1.5770, train acc: 0.2496, val loss: 1.5839, val acc: 0.2621  (best train acc: 0.4852, best val acc: 0.5686)\n",
      "[Epoch: 0280] train loss: 1.5807, train acc: 0.2504, val loss: 1.5794, val acc: 0.2621  (best train acc: 0.4852, best val acc: 0.5686)\n",
      "[Epoch: 0300] train loss: 1.5885, train acc: 0.2496, val loss: 1.5739, val acc: 0.2607  (best train acc: 0.4852, best val acc: 0.5686)\n",
      "[Epoch: 0320] train loss: 1.5844, train acc: 0.2504, val loss: 1.5690, val acc: 0.2604  (best train acc: 0.4852, best val acc: 0.5686)\n",
      "[Epoch: 0340] train loss: 1.5680, train acc: 0.2502, val loss: 1.5628, val acc: 0.2600  (best train acc: 0.4852, best val acc: 0.5686)\n",
      "[Epoch: 0360] train loss: 1.5559, train acc: 0.2490, val loss: 1.5589, val acc: 0.2604  (best train acc: 0.4852, best val acc: 0.5686)\n",
      "[Epoch: 0380] train loss: 1.5414, train acc: 0.2496, val loss: 1.5544, val acc: 0.2604  (best train acc: 0.4852, best val acc: 0.5686)\n",
      "[Epoch: 0400] train loss: 1.5586, train acc: 0.2492, val loss: 1.5511, val acc: 0.2594  (best train acc: 0.4852, best val acc: 0.5686)\n",
      "[Epoch: 0420] train loss: 1.5504, train acc: 0.2495, val loss: 1.5451, val acc: 0.2600  (best train acc: 0.4852, best val acc: 0.5686)\n",
      "[Epoch: 0440] train loss: 1.5493, train acc: 0.2486, val loss: 1.5403, val acc: 0.2597  (best train acc: 0.4852, best val acc: 0.5686)\n",
      "[Epoch: 0460] train loss: 1.5408, train acc: 0.2493, val loss: 1.5372, val acc: 0.2597  (best train acc: 0.4852, best val acc: 0.5686)\n",
      "[Epoch: 0480] train loss: 1.5384, train acc: 0.2496, val loss: 1.5352, val acc: 0.2597  (best train acc: 0.4852, best val acc: 0.5686)\n",
      "[Epoch: 0500] train loss: 1.5305, train acc: 0.2498, val loss: 1.5321, val acc: 0.2597  (best train acc: 0.4852, best val acc: 0.5686)\n",
      "[Epoch: 0520] train loss: 1.5303, train acc: 0.2486, val loss: 1.5296, val acc: 0.2600  (best train acc: 0.4852, best val acc: 0.5686)\n",
      "[Epoch: 0540] train loss: 1.5389, train acc: 0.2495, val loss: 1.5272, val acc: 0.2604  (best train acc: 0.4852, best val acc: 0.5686)\n",
      "[Epoch: 0560] train loss: 1.5271, train acc: 0.2491, val loss: 1.5248, val acc: 0.2600  (best train acc: 0.4852, best val acc: 0.5686)\n",
      "[Epoch: 0580] train loss: 1.5252, train acc: 0.2505, val loss: 1.5233, val acc: 0.2604  (best train acc: 0.4852, best val acc: 0.5686)\n",
      "[Epoch: 0600] train loss: 1.5284, train acc: 0.2495, val loss: 1.5214, val acc: 0.2604  (best train acc: 0.4852, best val acc: 0.5686)\n",
      "[Epoch: 0620] train loss: 1.5302, train acc: 0.2495, val loss: 1.5201, val acc: 0.2607  (best train acc: 0.4852, best val acc: 0.5686)\n",
      "[Epoch: 0640] train loss: 1.5228, train acc: 0.2494, val loss: 1.5187, val acc: 0.2607  (best train acc: 0.4852, best val acc: 0.5686)\n",
      "[Epoch: 0660] train loss: 1.5237, train acc: 0.2493, val loss: 1.5170, val acc: 0.2607  (best train acc: 0.4852, best val acc: 0.5686)\n",
      "[Epoch: 0680] train loss: 1.5268, train acc: 0.2504, val loss: 1.5157, val acc: 0.2607  (best train acc: 0.4852, best val acc: 0.5686)\n",
      "[Epoch: 0700] train loss: 1.5195, train acc: 0.2504, val loss: 1.5147, val acc: 0.2607  (best train acc: 0.4852, best val acc: 0.5686)\n",
      "[Epoch: 0720] train loss: 1.5177, train acc: 0.2499, val loss: 1.5136, val acc: 0.2607  (best train acc: 0.4852, best val acc: 0.5686)\n",
      "[Epoch: 0740] train loss: 1.5217, train acc: 0.2488, val loss: 1.5123, val acc: 0.2607  (best train acc: 0.4852, best val acc: 0.5686)\n",
      "[Epoch: 0760] train loss: 1.5182, train acc: 0.2497, val loss: 1.5110, val acc: 0.2607  (best train acc: 0.4852, best val acc: 0.5686)\n",
      "[Epoch: 0780] train loss: 1.5169, train acc: 0.2497, val loss: 1.5110, val acc: 0.2607  (best train acc: 0.4852, best val acc: 0.5686)\n",
      "[Epoch: 0800] train loss: 1.5153, train acc: 0.2494, val loss: 1.5099, val acc: 0.2607  (best train acc: 0.4852, best val acc: 0.5686)\n",
      "[Epoch: 0820] train loss: 1.5167, train acc: 0.2492, val loss: 1.5090, val acc: 0.2607  (best train acc: 0.4852, best val acc: 0.5686)\n",
      "[Epoch: 0840] train loss: 1.5146, train acc: 0.2493, val loss: 1.5078, val acc: 0.2607  (best train acc: 0.4852, best val acc: 0.5686)\n",
      "[Epoch: 0860] train loss: 1.5133, train acc: 0.2507, val loss: 1.5073, val acc: 0.2607  (best train acc: 0.4852, best val acc: 0.5686)\n",
      "[Epoch: 0880] train loss: 1.5085, train acc: 0.2493, val loss: 1.5068, val acc: 0.2607  (best train acc: 0.4852, best val acc: 0.5686)\n",
      "[Epoch: 0900] train loss: 1.5076, train acc: 0.2499, val loss: 1.5067, val acc: 0.2607  (best train acc: 0.4852, best val acc: 0.5686)\n",
      "[Epoch: 0920] train loss: 1.5125, train acc: 0.2504, val loss: 1.5058, val acc: 0.2607  (best train acc: 0.4852, best val acc: 0.5686)\n",
      "[Epoch: 0940] train loss: 1.5121, train acc: 0.2491, val loss: 1.5052, val acc: 0.2610  (best train acc: 0.4852, best val acc: 0.5686)\n",
      "[Epoch: 0960] train loss: 1.5089, train acc: 0.2501, val loss: 1.5048, val acc: 0.2610  (best train acc: 0.4852, best val acc: 0.5686)\n",
      "[Epoch: 0980] train loss: 1.5079, train acc: 0.2502, val loss: 1.5041, val acc: 0.2607  (best train acc: 0.4852, best val acc: 0.5686)\n",
      "[Epoch: 1000] train loss: 1.5116, train acc: 0.2509, val loss: 1.5035, val acc: 0.2641  (best train acc: 0.4852, best val acc: 0.5686)\n",
      "[Epoch: 1020] train loss: 1.5094, train acc: 0.2512, val loss: 1.5026, val acc: 0.2658  (best train acc: 0.4852, best val acc: 0.5686)\n",
      "[Epoch: 1040] train loss: 1.5091, train acc: 0.2505, val loss: 1.5016, val acc: 0.2644  (best train acc: 0.4852, best val acc: 0.5686)\n",
      "[Epoch: 1060] train loss: 1.5078, train acc: 0.2504, val loss: 1.5009, val acc: 0.2641  (best train acc: 0.4852, best val acc: 0.5686)\n",
      "[Epoch: 1080] train loss: 1.5080, train acc: 0.2533, val loss: 1.4937, val acc: 0.2671  (best train acc: 0.4852, best val acc: 0.5686)\n",
      "[Epoch: 1100] train loss: 1.5033, train acc: 0.2523, val loss: 1.4947, val acc: 0.2668  (best train acc: 0.4852, best val acc: 0.5686)\n",
      "[Epoch: 1120] train loss: 1.5042, train acc: 0.2515, val loss: 1.4909, val acc: 0.2681  (best train acc: 0.4852, best val acc: 0.5686)\n",
      "[Epoch: 1140] train loss: 1.5065, train acc: 0.2520, val loss: 1.4875, val acc: 0.2688  (best train acc: 0.4852, best val acc: 0.5686)\n",
      "[Epoch: 1160] train loss: 1.5034, train acc: 0.2552, val loss: 1.4871, val acc: 0.2708  (best train acc: 0.4852, best val acc: 0.5686)\n",
      "[Epoch: 1180] train loss: 1.5022, train acc: 0.2530, val loss: 1.4869, val acc: 0.2715  (best train acc: 0.4852, best val acc: 0.5686)\n",
      "[Epoch: 1200] train loss: 1.5026, train acc: 0.2521, val loss: 1.4861, val acc: 0.2702  (best train acc: 0.4852, best val acc: 0.5686)\n",
      "[Epoch: 1220] train loss: 1.4972, train acc: 0.2551, val loss: 1.4847, val acc: 0.2705  (best train acc: 0.4852, best val acc: 0.5686)\n",
      "[Epoch: 1240] train loss: 1.4976, train acc: 0.2546, val loss: 1.4848, val acc: 0.2715  (best train acc: 0.4852, best val acc: 0.5686)\n",
      "[Epoch: 1260] train loss: 1.4989, train acc: 0.2551, val loss: 1.4840, val acc: 0.2715  (best train acc: 0.4852, best val acc: 0.5686)\n",
      "[Epoch: 1280] train loss: 1.4951, train acc: 0.2552, val loss: 1.4833, val acc: 0.2705  (best train acc: 0.4852, best val acc: 0.5686)\n",
      "[Epoch: 1300] train loss: 1.4992, train acc: 0.2540, val loss: 1.4834, val acc: 0.2705  (best train acc: 0.4852, best val acc: 0.5686)\n",
      "[Epoch: 1320] train loss: 1.5007, train acc: 0.2550, val loss: 1.4824, val acc: 0.2755  (best train acc: 0.4852, best val acc: 0.5686)\n",
      "[Epoch: 1340] train loss: 1.4996, train acc: 0.2566, val loss: 1.4815, val acc: 0.2752  (best train acc: 0.4852, best val acc: 0.5686)\n",
      "[Epoch: 1360] train loss: 1.4942, train acc: 0.2580, val loss: 1.4809, val acc: 0.2742  (best train acc: 0.4852, best val acc: 0.5686)\n",
      "[Epoch: 1380] train loss: 1.4925, train acc: 0.2578, val loss: 1.4806, val acc: 0.2769  (best train acc: 0.4852, best val acc: 0.5686)\n",
      "[Epoch: 1400] train loss: 1.4974, train acc: 0.2564, val loss: 1.4793, val acc: 0.2755  (best train acc: 0.4852, best val acc: 0.5686)\n",
      "[Epoch: 1420] train loss: 1.4944, train acc: 0.2580, val loss: 1.4792, val acc: 0.2769  (best train acc: 0.4852, best val acc: 0.5686)\n",
      "[Epoch: 1440] train loss: 1.4929, train acc: 0.2588, val loss: 1.4781, val acc: 0.2782  (best train acc: 0.4852, best val acc: 0.5686)\n",
      "[Epoch: 1460] train loss: 1.4945, train acc: 0.2579, val loss: 1.4769, val acc: 0.2789  (best train acc: 0.4852, best val acc: 0.5686)\n",
      "[Epoch: 1480] train loss: 1.4972, train acc: 0.2578, val loss: 1.4762, val acc: 0.2820  (best train acc: 0.4852, best val acc: 0.5686)\n",
      "[Epoch: 1500] train loss: 1.4915, train acc: 0.2622, val loss: 1.4746, val acc: 0.2843  (best train acc: 0.4852, best val acc: 0.5686)\n",
      "[Epoch: 1520] train loss: 1.4931, train acc: 0.2604, val loss: 1.4734, val acc: 0.2793  (best train acc: 0.4852, best val acc: 0.5686)\n",
      "[Epoch: 1540] train loss: 1.4866, train acc: 0.2637, val loss: 1.4719, val acc: 0.2840  (best train acc: 0.4852, best val acc: 0.5686)\n",
      "[Epoch: 1560] train loss: 1.4884, train acc: 0.2632, val loss: 1.4702, val acc: 0.2857  (best train acc: 0.4852, best val acc: 0.5686)\n",
      "[Epoch: 1580] train loss: 1.4880, train acc: 0.2663, val loss: 1.4679, val acc: 0.2870  (best train acc: 0.4852, best val acc: 0.5686)\n",
      "[Epoch: 1600] train loss: 1.4929, train acc: 0.2641, val loss: 1.4684, val acc: 0.2850  (best train acc: 0.4852, best val acc: 0.5686)\n",
      "[Epoch: 1620] train loss: 1.4673, train acc: 0.2807, val loss: 1.4229, val acc: 0.3234  (best train acc: 0.4852, best val acc: 0.5686)\n",
      "[Epoch: 1640] train loss: 1.4594, train acc: 0.2835, val loss: 1.4108, val acc: 0.3238  (best train acc: 0.4852, best val acc: 0.5686)\n",
      "[Epoch: 1660] train loss: 1.4552, train acc: 0.2847, val loss: 1.4067, val acc: 0.3292  (best train acc: 0.4852, best val acc: 0.5686)\n",
      "[Epoch: 1680] train loss: 1.4522, train acc: 0.2890, val loss: 1.3874, val acc: 0.3379  (best train acc: 0.4852, best val acc: 0.5686)\n",
      "[Epoch: 1700] train loss: 1.4517, train acc: 0.2900, val loss: 1.3933, val acc: 0.3359  (best train acc: 0.4852, best val acc: 0.5686)\n",
      "[Epoch: 1720] train loss: 1.4516, train acc: 0.2931, val loss: 1.3767, val acc: 0.3454  (best train acc: 0.4852, best val acc: 0.5686)\n",
      "[Epoch: 1740] train loss: 1.4498, train acc: 0.2939, val loss: 1.3845, val acc: 0.3406  (best train acc: 0.4852, best val acc: 0.5686)\n",
      "[Epoch: 1760] train loss: 1.4514, train acc: 0.2917, val loss: 1.3686, val acc: 0.3494  (best train acc: 0.4852, best val acc: 0.5686)\n",
      "[Epoch: 1780] train loss: 1.4408, train acc: 0.2941, val loss: 1.3660, val acc: 0.3487  (best train acc: 0.4852, best val acc: 0.5686)\n",
      "[Epoch: 1800] train loss: 1.4377, train acc: 0.2957, val loss: 1.3670, val acc: 0.3491  (best train acc: 0.4852, best val acc: 0.5686)\n",
      "[Epoch: 1820] train loss: 1.4404, train acc: 0.2940, val loss: 1.3679, val acc: 0.3447  (best train acc: 0.4852, best val acc: 0.5686)\n",
      "[Epoch: 1840] train loss: 1.4390, train acc: 0.2934, val loss: 1.3602, val acc: 0.3501  (best train acc: 0.4852, best val acc: 0.5686)\n",
      "[Epoch: 1860] train loss: 1.4507, train acc: 0.2953, val loss: 1.3537, val acc: 0.3558  (best train acc: 0.4852, best val acc: 0.5686)\n",
      "[Epoch: 1880] train loss: 1.4422, train acc: 0.2946, val loss: 1.3643, val acc: 0.3497  (best train acc: 0.4852, best val acc: 0.5686)\n",
      "[Epoch: 1900] train loss: 1.4420, train acc: 0.2944, val loss: 1.3596, val acc: 0.3494  (best train acc: 0.4852, best val acc: 0.5686)\n",
      "[Epoch: 1920] train loss: 1.4376, train acc: 0.2956, val loss: 1.3608, val acc: 0.3481  (best train acc: 0.4852, best val acc: 0.5686)\n",
      "[Epoch: 1940] train loss: 1.4316, train acc: 0.2955, val loss: 1.3594, val acc: 0.3494  (best train acc: 0.4852, best val acc: 0.5686)\n",
      "[Epoch: 1960] train loss: 1.4347, train acc: 0.2954, val loss: 1.3603, val acc: 0.3477  (best train acc: 0.4852, best val acc: 0.5686)\n",
      "[Epoch: 1980] train loss: 1.4422, train acc: 0.2920, val loss: 1.3657, val acc: 0.3460  (best train acc: 0.4852, best val acc: 0.5686)\n",
      "[Epoch: 2000] train loss: 1.4337, train acc: 0.2971, val loss: 1.3551, val acc: 0.3518  (best train acc: 0.4852, best val acc: 0.5686)\n",
      "[Epoch: 2020] train loss: 1.4391, train acc: 0.2976, val loss: 1.3626, val acc: 0.3474  (best train acc: 0.4852, best val acc: 0.5686)\n",
      "[Epoch: 2040] train loss: 1.4312, train acc: 0.2991, val loss: 1.3564, val acc: 0.3511  (best train acc: 0.4852, best val acc: 0.5686)\n",
      "[Epoch: 2060] train loss: 1.4354, train acc: 0.2984, val loss: 1.3597, val acc: 0.3504  (best train acc: 0.4852, best val acc: 0.5686)\n",
      "[Epoch: 2080] train loss: 1.4377, train acc: 0.2979, val loss: 1.3536, val acc: 0.3528  (best train acc: 0.4852, best val acc: 0.5686)\n",
      "[Epoch: 2100] train loss: 1.4350, train acc: 0.2971, val loss: 1.3553, val acc: 0.3508  (best train acc: 0.4852, best val acc: 0.5686)\n",
      "[Epoch: 2120] train loss: 1.4360, train acc: 0.2968, val loss: 1.3556, val acc: 0.3521  (best train acc: 0.4852, best val acc: 0.5686)\n",
      "[Epoch: 2140] train loss: 1.4274, train acc: 0.2986, val loss: 1.3607, val acc: 0.3511  (best train acc: 0.4852, best val acc: 0.5686)\n",
      "[Epoch: 2160] train loss: 1.4343, train acc: 0.2991, val loss: 1.3584, val acc: 0.3504  (best train acc: 0.4852, best val acc: 0.5686)\n",
      "[Epoch: 2180] train loss: 1.4370, train acc: 0.2953, val loss: 1.3592, val acc: 0.3494  (best train acc: 0.4852, best val acc: 0.5686)\n",
      "[Epoch: 2200] train loss: 1.4361, train acc: 0.2955, val loss: 1.3567, val acc: 0.3508  (best train acc: 0.4852, best val acc: 0.5686)\n",
      "[Epoch: 2220] train loss: 1.4265, train acc: 0.3007, val loss: 1.3576, val acc: 0.3518  (best train acc: 0.4852, best val acc: 0.5686)\n",
      "[Epoch: 2240] train loss: 1.4370, train acc: 0.2946, val loss: 1.3596, val acc: 0.3508  (best train acc: 0.4852, best val acc: 0.5686)\n",
      "[Epoch: 2260] train loss: 1.4262, train acc: 0.3034, val loss: 1.3545, val acc: 0.3555  (best train acc: 0.4852, best val acc: 0.5686)\n",
      "[Epoch: 2280] train loss: 1.4242, train acc: 0.3065, val loss: 1.3485, val acc: 0.3616  (best train acc: 0.4852, best val acc: 0.5686)\n",
      "[Epoch: 2300] train loss: 1.2335, train acc: 0.3955, val loss: 1.0921, val acc: 0.4924  (best train acc: 0.4852, best val acc: 0.5686)\n",
      "[Epoch: 2320] train loss: 1.2124, train acc: 0.4137, val loss: 1.0527, val acc: 0.5298  (best train acc: 0.4852, best val acc: 0.5686)\n",
      "[Epoch: 2340] train loss: 1.1851, train acc: 0.4336, val loss: 0.9828, val acc: 0.6202  (best train acc: 0.4852, best val acc: 0.6202)\n",
      "[Epoch: 2360] train loss: 1.1554, train acc: 0.4852, val loss: 0.9360, val acc: 0.6405  (best train acc: 0.4941, best val acc: 0.6897)\n",
      "[Epoch: 2380] train loss: 1.1478, train acc: 0.4866, val loss: 0.9334, val acc: 0.6917  (best train acc: 0.4941, best val acc: 0.6995)\n",
      "[Epoch: 2400] train loss: 1.1558, train acc: 0.4787, val loss: 0.9494, val acc: 0.6857  (best train acc: 0.4941, best val acc: 0.6995)\n",
      "[Epoch: 2420] train loss: 1.1545, train acc: 0.4815, val loss: 0.9311, val acc: 0.6948  (best train acc: 0.4941, best val acc: 0.6995)\n",
      "[Epoch: 2440] train loss: 1.1508, train acc: 0.4864, val loss: 0.9311, val acc: 0.6944  (best train acc: 0.4957, best val acc: 0.7005)\n",
      "[Epoch: 2460] train loss: 1.1511, train acc: 0.4949, val loss: 0.9327, val acc: 0.7093  (best train acc: 0.4983, best val acc: 0.7093)\n",
      "[Epoch: 2480] train loss: 1.1640, train acc: 0.5014, val loss: 0.9204, val acc: 0.6816  (best train acc: 0.5067, best val acc: 0.7153)\n",
      "[Epoch: 2500] train loss: 1.1420, train acc: 0.4989, val loss: 0.9248, val acc: 0.6944  (best train acc: 0.5067, best val acc: 0.7153)\n",
      "[Epoch: 2520] train loss: 1.1467, train acc: 0.4864, val loss: 0.9307, val acc: 0.6857  (best train acc: 0.5067, best val acc: 0.7167)\n",
      "[Epoch: 2540] train loss: 1.1403, train acc: 0.4991, val loss: 0.9333, val acc: 0.6826  (best train acc: 0.5067, best val acc: 0.7191)\n",
      "[Epoch: 2560] train loss: 1.1453, train acc: 0.4832, val loss: 0.9190, val acc: 0.6897  (best train acc: 0.5067, best val acc: 0.7191)\n",
      "[Epoch: 2580] train loss: 1.1410, train acc: 0.4884, val loss: 0.9244, val acc: 0.6998  (best train acc: 0.5071, best val acc: 0.7191)\n",
      "[Epoch: 2600] train loss: 1.1347, train acc: 0.4915, val loss: 0.9203, val acc: 0.6826  (best train acc: 0.5071, best val acc: 0.7191)\n",
      "[Epoch: 2620] train loss: 1.1193, train acc: 0.4993, val loss: 0.9181, val acc: 0.7046  (best train acc: 0.5071, best val acc: 0.7191)\n",
      "[Epoch: 2640] train loss: 1.1299, train acc: 0.5013, val loss: 0.9083, val acc: 0.7191  (best train acc: 0.5071, best val acc: 0.7191)\n",
      "[Epoch: 2660] train loss: 1.1276, train acc: 0.4990, val loss: 0.9111, val acc: 0.6998  (best train acc: 0.5071, best val acc: 0.7191)\n",
      "[Epoch: 2680] train loss: 1.1314, train acc: 0.4884, val loss: 0.9213, val acc: 0.7022  (best train acc: 0.5088, best val acc: 0.7268)\n",
      "[Epoch: 2700] train loss: 1.1188, train acc: 0.5038, val loss: 0.9039, val acc: 0.7106  (best train acc: 0.5093, best val acc: 0.7268)\n",
      "[Epoch: 2720] train loss: 1.1200, train acc: 0.5027, val loss: 0.9009, val acc: 0.7164  (best train acc: 0.5093, best val acc: 0.7268)\n",
      "[Epoch: 2740] train loss: 1.1270, train acc: 0.4979, val loss: 0.9078, val acc: 0.7170  (best train acc: 0.5093, best val acc: 0.7268)\n",
      "[Epoch: 2760] train loss: 1.1204, train acc: 0.4994, val loss: 0.8975, val acc: 0.7187  (best train acc: 0.5093, best val acc: 0.7268)\n",
      "[Epoch: 2780] train loss: 1.1306, train acc: 0.4949, val loss: 0.8994, val acc: 0.7201  (best train acc: 0.5112, best val acc: 0.7305)\n",
      "[Epoch: 2800] train loss: 1.1156, train acc: 0.5114, val loss: 0.8886, val acc: 0.7248  (best train acc: 0.5182, best val acc: 0.7305)\n",
      "[Epoch: 2820] train loss: 1.1215, train acc: 0.5037, val loss: 0.8986, val acc: 0.7238  (best train acc: 0.5207, best val acc: 0.7346)\n",
      "[Epoch: 2840] train loss: 1.1262, train acc: 0.5030, val loss: 0.8984, val acc: 0.7201  (best train acc: 0.5207, best val acc: 0.7346)\n",
      "[Epoch: 2860] train loss: 1.1128, train acc: 0.5114, val loss: 0.8913, val acc: 0.7241  (best train acc: 0.5207, best val acc: 0.7346)\n",
      "[Epoch: 2880] train loss: 1.1185, train acc: 0.5045, val loss: 0.8909, val acc: 0.7346  (best train acc: 0.5207, best val acc: 0.7376)\n",
      "[Epoch: 2900] train loss: 1.0986, train acc: 0.5445, val loss: 0.8831, val acc: 0.7207  (best train acc: 0.5445, best val acc: 0.7376)\n",
      "[Epoch: 2920] train loss: 1.0118, train acc: 0.5799, val loss: 0.8329, val acc: 0.7197  (best train acc: 0.5799, best val acc: 0.7376)\n",
      "[Epoch: 2940] train loss: 0.9745, train acc: 0.6136, val loss: 0.8154, val acc: 0.7390  (best train acc: 0.6136, best val acc: 0.7592)\n",
      "[Epoch: 2960] train loss: 0.9718, train acc: 0.6136, val loss: 0.7986, val acc: 0.7514  (best train acc: 0.6185, best val acc: 0.7737)\n",
      "[Epoch: 2980] train loss: 0.9550, train acc: 0.6279, val loss: 0.7824, val acc: 0.7686  (best train acc: 0.6279, best val acc: 0.7784)\n",
      "[Epoch: 3000] train loss: 0.9549, train acc: 0.6263, val loss: 0.7784, val acc: 0.7707  (best train acc: 0.6288, best val acc: 0.7784)\n",
      "[Epoch: 3020] train loss: 0.9551, train acc: 0.6298, val loss: 0.7864, val acc: 0.7602  (best train acc: 0.6323, best val acc: 0.7791)\n",
      "[Epoch: 3040] train loss: 0.9557, train acc: 0.6282, val loss: 0.7689, val acc: 0.7649  (best train acc: 0.6330, best val acc: 0.7858)\n",
      "[Epoch: 3060] train loss: 0.9499, train acc: 0.6296, val loss: 0.7671, val acc: 0.7804  (best train acc: 0.6371, best val acc: 0.7858)\n",
      "[Epoch: 3080] train loss: 0.9469, train acc: 0.6329, val loss: 0.7635, val acc: 0.7666  (best train acc: 0.6371, best val acc: 0.7858)\n",
      "[Epoch: 3100] train loss: 0.9421, train acc: 0.6318, val loss: 0.7553, val acc: 0.7821  (best train acc: 0.6385, best val acc: 0.7933)\n",
      "[Epoch: 3120] train loss: 0.9421, train acc: 0.6348, val loss: 0.7511, val acc: 0.7815  (best train acc: 0.6484, best val acc: 0.8010)\n",
      "[Epoch: 3140] train loss: 0.9528, train acc: 0.6369, val loss: 0.7531, val acc: 0.7868  (best train acc: 0.6484, best val acc: 0.8024)\n",
      "[Epoch: 3160] train loss: 0.9460, train acc: 0.6330, val loss: 0.7415, val acc: 0.7990  (best train acc: 0.6484, best val acc: 0.8024)\n",
      "[Epoch: 3180] train loss: 0.9332, train acc: 0.6431, val loss: 0.7459, val acc: 0.7784  (best train acc: 0.6484, best val acc: 0.8024)\n",
      "[Epoch: 3200] train loss: 0.9349, train acc: 0.6465, val loss: 0.7334, val acc: 0.7960  (best train acc: 0.6484, best val acc: 0.8074)\n",
      "[Epoch: 3220] train loss: 0.9315, train acc: 0.6472, val loss: 0.7332, val acc: 0.8098  (best train acc: 0.6570, best val acc: 0.8098)\n",
      "[Epoch: 3240] train loss: 0.9266, train acc: 0.6473, val loss: 0.7357, val acc: 0.8020  (best train acc: 0.6570, best val acc: 0.8118)\n",
      "[Epoch: 3260] train loss: 0.9253, train acc: 0.6520, val loss: 0.7272, val acc: 0.8027  (best train acc: 0.6583, best val acc: 0.8125)\n",
      "[Epoch: 3280] train loss: 0.9301, train acc: 0.6466, val loss: 0.7302, val acc: 0.8057  (best train acc: 0.6583, best val acc: 0.8145)\n",
      "[Epoch: 3300] train loss: 0.9222, train acc: 0.6521, val loss: 0.7253, val acc: 0.8037  (best train acc: 0.6583, best val acc: 0.8175)\n",
      "[Epoch: 3320] train loss: 0.9074, train acc: 0.6643, val loss: 0.7227, val acc: 0.8047  (best train acc: 0.6643, best val acc: 0.8175)\n",
      "[Epoch: 3340] train loss: 0.9223, train acc: 0.6611, val loss: 0.7140, val acc: 0.8256  (best train acc: 0.6643, best val acc: 0.8256)\n",
      "[Epoch: 3360] train loss: 0.9227, train acc: 0.6565, val loss: 0.7086, val acc: 0.8216  (best train acc: 0.6664, best val acc: 0.8287)\n",
      "[Epoch: 3380] train loss: 0.9242, train acc: 0.6549, val loss: 0.7061, val acc: 0.8283  (best train acc: 0.6679, best val acc: 0.8287)\n",
      "[Epoch: 3400] train loss: 0.9279, train acc: 0.6607, val loss: 0.7006, val acc: 0.8310  (best train acc: 0.6687, best val acc: 0.8320)\n",
      "[Epoch: 3420] train loss: 0.9180, train acc: 0.6538, val loss: 0.7125, val acc: 0.8010  (best train acc: 0.6687, best val acc: 0.8320)\n",
      "[Epoch: 3440] train loss: 0.9094, train acc: 0.6661, val loss: 0.7053, val acc: 0.8344  (best train acc: 0.6687, best val acc: 0.8344)\n",
      "[Epoch: 3460] train loss: 0.9085, train acc: 0.6648, val loss: 0.7061, val acc: 0.8297  (best train acc: 0.6752, best val acc: 0.8361)\n",
      "[Epoch: 3480] train loss: 0.9164, train acc: 0.6592, val loss: 0.6957, val acc: 0.8226  (best train acc: 0.6752, best val acc: 0.8395)\n",
      "[Epoch: 3500] train loss: 0.9179, train acc: 0.6564, val loss: 0.6979, val acc: 0.8253  (best train acc: 0.6752, best val acc: 0.8459)\n",
      "[Epoch: 3520] train loss: 0.9158, train acc: 0.6625, val loss: 0.7022, val acc: 0.8297  (best train acc: 0.6752, best val acc: 0.8459)\n",
      "[Epoch: 3540] train loss: 0.9033, train acc: 0.6736, val loss: 0.6887, val acc: 0.8388  (best train acc: 0.6776, best val acc: 0.8472)\n",
      "[Epoch: 3560] train loss: 0.9038, train acc: 0.6761, val loss: 0.6819, val acc: 0.8401  (best train acc: 0.6776, best val acc: 0.8482)\n",
      "[Epoch: 3580] train loss: 0.9048, train acc: 0.6682, val loss: 0.6993, val acc: 0.8260  (best train acc: 0.6796, best val acc: 0.8482)\n",
      "[Epoch: 3600] train loss: 0.9105, train acc: 0.6742, val loss: 0.6949, val acc: 0.8398  (best train acc: 0.6796, best val acc: 0.8570)\n",
      "[Epoch: 3620] train loss: 0.9080, train acc: 0.6768, val loss: 0.6918, val acc: 0.8435  (best train acc: 0.6845, best val acc: 0.8570)\n",
      "[Epoch: 3640] train loss: 0.9053, train acc: 0.6657, val loss: 0.6916, val acc: 0.8334  (best train acc: 0.6845, best val acc: 0.8570)\n",
      "[Epoch: 3660] train loss: 0.9046, train acc: 0.6750, val loss: 0.6897, val acc: 0.8401  (best train acc: 0.6845, best val acc: 0.8570)\n",
      "[Epoch: 3680] train loss: 0.9005, train acc: 0.6774, val loss: 0.6852, val acc: 0.8391  (best train acc: 0.6861, best val acc: 0.8570)\n",
      "[Epoch: 3700] train loss: 0.9028, train acc: 0.6702, val loss: 0.7038, val acc: 0.8229  (best train acc: 0.6861, best val acc: 0.8607)\n",
      "[Epoch: 3720] train loss: 0.9012, train acc: 0.6888, val loss: 0.6708, val acc: 0.8577  (best train acc: 0.6888, best val acc: 0.8607)\n",
      "[Epoch: 3740] train loss: 0.9035, train acc: 0.6809, val loss: 0.6683, val acc: 0.8560  (best train acc: 0.6888, best val acc: 0.8607)\n",
      "[Epoch: 3760] train loss: 0.9079, train acc: 0.6765, val loss: 0.6724, val acc: 0.8560  (best train acc: 0.6888, best val acc: 0.8634)\n",
      "[Epoch: 3780] train loss: 0.8943, train acc: 0.6835, val loss: 0.6850, val acc: 0.8418  (best train acc: 0.6888, best val acc: 0.8634)\n",
      "[Epoch: 3800] train loss: 0.8930, train acc: 0.6821, val loss: 0.6797, val acc: 0.8556  (best train acc: 0.6894, best val acc: 0.8644)\n",
      "[Epoch: 3820] train loss: 0.9089, train acc: 0.6834, val loss: 0.6689, val acc: 0.8358  (best train acc: 0.6894, best val acc: 0.8644)\n",
      "[Epoch: 3840] train loss: 0.8950, train acc: 0.6849, val loss: 0.6572, val acc: 0.8496  (best train acc: 0.6921, best val acc: 0.8644)\n",
      "[Epoch: 3860] train loss: 0.9107, train acc: 0.6656, val loss: 0.6738, val acc: 0.8509  (best train acc: 0.6935, best val acc: 0.8644)\n",
      "[Epoch: 3880] train loss: 0.8935, train acc: 0.6862, val loss: 0.6607, val acc: 0.8546  (best train acc: 0.6935, best val acc: 0.8678)\n",
      "[Epoch: 3900] train loss: 0.9009, train acc: 0.6768, val loss: 0.6707, val acc: 0.8604  (best train acc: 0.6941, best val acc: 0.8678)\n",
      "[Epoch: 3920] train loss: 0.8819, train acc: 0.6878, val loss: 0.6587, val acc: 0.8637  (best train acc: 0.6941, best val acc: 0.8678)\n",
      "[Epoch: 3940] train loss: 0.8864, train acc: 0.6900, val loss: 0.6604, val acc: 0.8627  (best train acc: 0.6961, best val acc: 0.8678)\n",
      "[Epoch: 3960] train loss: 0.8756, train acc: 0.6997, val loss: 0.6476, val acc: 0.8678  (best train acc: 0.6997, best val acc: 0.8678)\n",
      "[Epoch: 3980] train loss: 0.8846, train acc: 0.6940, val loss: 0.6564, val acc: 0.8462  (best train acc: 0.6997, best val acc: 0.8685)\n",
      "[Epoch: 4000] train loss: 0.8877, train acc: 0.6959, val loss: 0.6589, val acc: 0.8519  (best train acc: 0.6997, best val acc: 0.8685)\n",
      "[Epoch: 4020] train loss: 0.8985, train acc: 0.6896, val loss: 0.6619, val acc: 0.8374  (best train acc: 0.7015, best val acc: 0.8691)\n",
      "[Epoch: 4040] train loss: 0.8947, train acc: 0.6869, val loss: 0.6704, val acc: 0.8607  (best train acc: 0.7015, best val acc: 0.8691)\n",
      "[Epoch: 4060] train loss: 0.8818, train acc: 0.6982, val loss: 0.6679, val acc: 0.8712  (best train acc: 0.7015, best val acc: 0.8782)\n",
      "[Epoch: 4080] train loss: 0.8850, train acc: 0.7023, val loss: 0.6538, val acc: 0.8735  (best train acc: 0.7035, best val acc: 0.8782)\n",
      "[Epoch: 4100] train loss: 0.8860, train acc: 0.6963, val loss: 0.6606, val acc: 0.8742  (best train acc: 0.7035, best val acc: 0.8782)\n",
      "[Epoch: 4120] train loss: 0.8906, train acc: 0.6969, val loss: 0.6508, val acc: 0.8752  (best train acc: 0.7076, best val acc: 0.8786)\n",
      "[Epoch: 4140] train loss: 0.8877, train acc: 0.6990, val loss: 0.6551, val acc: 0.8681  (best train acc: 0.7076, best val acc: 0.8786)\n",
      "[Epoch: 4160] train loss: 0.8859, train acc: 0.6953, val loss: 0.6546, val acc: 0.8691  (best train acc: 0.7076, best val acc: 0.8786)\n",
      "[Epoch: 4180] train loss: 0.8721, train acc: 0.6961, val loss: 0.6551, val acc: 0.8691  (best train acc: 0.7107, best val acc: 0.8786)\n",
      "[Epoch: 4200] train loss: 0.8772, train acc: 0.6842, val loss: 0.6706, val acc: 0.8556  (best train acc: 0.7107, best val acc: 0.8806)\n",
      "[Epoch: 4220] train loss: 0.8825, train acc: 0.7056, val loss: 0.6332, val acc: 0.8708  (best train acc: 0.7107, best val acc: 0.8826)\n",
      "[Epoch: 4240] train loss: 0.8781, train acc: 0.7049, val loss: 0.6530, val acc: 0.8826  (best train acc: 0.7107, best val acc: 0.8826)\n",
      "[Epoch: 4260] train loss: 0.8807, train acc: 0.6974, val loss: 0.6465, val acc: 0.8776  (best train acc: 0.7126, best val acc: 0.8826)\n",
      "[Epoch: 4280] train loss: 0.8810, train acc: 0.6998, val loss: 0.6404, val acc: 0.8779  (best train acc: 0.7126, best val acc: 0.8826)\n",
      "[Epoch: 4300] train loss: 0.8762, train acc: 0.7064, val loss: 0.6309, val acc: 0.8776  (best train acc: 0.7126, best val acc: 0.8826)\n",
      "[Epoch: 4320] train loss: 0.8696, train acc: 0.7107, val loss: 0.6480, val acc: 0.8624  (best train acc: 0.7126, best val acc: 0.8826)\n",
      "[Epoch: 4340] train loss: 0.8894, train acc: 0.6768, val loss: 0.6710, val acc: 0.8563  (best train acc: 0.7126, best val acc: 0.8826)\n",
      "[Epoch: 4360] train loss: 0.9029, train acc: 0.6859, val loss: 0.6388, val acc: 0.8388  (best train acc: 0.7126, best val acc: 0.8826)\n",
      "[Epoch: 4380] train loss: 0.9035, train acc: 0.6431, val loss: 0.6880, val acc: 0.8003  (best train acc: 0.7126, best val acc: 0.8826)\n",
      "[Epoch: 4400] train loss: 0.8858, train acc: 0.6678, val loss: 0.6395, val acc: 0.8587  (best train acc: 0.7126, best val acc: 0.8826)\n",
      "[Epoch: 4420] train loss: 0.8836, train acc: 0.6767, val loss: 0.6522, val acc: 0.8533  (best train acc: 0.7126, best val acc: 0.8826)\n",
      "[Epoch: 4440] train loss: 0.8756, train acc: 0.6302, val loss: 0.6797, val acc: 0.6890  (best train acc: 0.7126, best val acc: 0.8826)\n",
      "[Epoch: 4460] train loss: 0.8377, train acc: 0.6894, val loss: 0.5928, val acc: 0.8681  (best train acc: 0.7126, best val acc: 0.8826)\n",
      "[Epoch: 4480] train loss: 0.8286, train acc: 0.6800, val loss: 0.6190, val acc: 0.8206  (best train acc: 0.7126, best val acc: 0.8826)\n",
      "[Epoch: 4500] train loss: 0.7946, train acc: 0.7235, val loss: 0.5926, val acc: 0.8739  (best train acc: 0.7298, best val acc: 0.8826)\n",
      "[Epoch: 4520] train loss: 0.7838, train acc: 0.7319, val loss: 0.5911, val acc: 0.8712  (best train acc: 0.7373, best val acc: 0.8826)\n",
      "[Epoch: 4540] train loss: 0.7937, train acc: 0.7280, val loss: 0.5895, val acc: 0.8772  (best train acc: 0.7428, best val acc: 0.8840)\n",
      "[Epoch: 4560] train loss: 0.7840, train acc: 0.7332, val loss: 0.5808, val acc: 0.8809  (best train acc: 0.7428, best val acc: 0.8847)\n",
      "[Epoch: 4580] train loss: 0.7907, train acc: 0.7385, val loss: 0.5829, val acc: 0.8833  (best train acc: 0.7436, best val acc: 0.8867)\n",
      "[Epoch: 4600] train loss: 0.7841, train acc: 0.7321, val loss: 0.5865, val acc: 0.8843  (best train acc: 0.7436, best val acc: 0.8867)\n",
      "[Epoch: 4620] train loss: 0.7845, train acc: 0.7399, val loss: 0.5735, val acc: 0.8745  (best train acc: 0.7436, best val acc: 0.8887)\n",
      "[Epoch: 4640] train loss: 0.7880, train acc: 0.7369, val loss: 0.5741, val acc: 0.8826  (best train acc: 0.7439, best val acc: 0.8887)\n",
      "[Epoch: 4660] train loss: 0.7913, train acc: 0.7355, val loss: 0.5749, val acc: 0.8830  (best train acc: 0.7454, best val acc: 0.8887)\n",
      "[Epoch: 4680] train loss: 0.7903, train acc: 0.7351, val loss: 0.5654, val acc: 0.8853  (best train acc: 0.7454, best val acc: 0.8887)\n",
      "[Epoch: 4700] train loss: 0.7789, train acc: 0.7327, val loss: 0.5803, val acc: 0.8782  (best train acc: 0.7463, best val acc: 0.8890)\n",
      "[Epoch: 4720] train loss: 0.7764, train acc: 0.7421, val loss: 0.5617, val acc: 0.8806  (best train acc: 0.7463, best val acc: 0.8897)\n",
      "[Epoch: 4740] train loss: 0.7766, train acc: 0.7406, val loss: 0.5621, val acc: 0.8860  (best train acc: 0.7463, best val acc: 0.8897)\n",
      "[Epoch: 4760] train loss: 0.7888, train acc: 0.7361, val loss: 0.5667, val acc: 0.8836  (best train acc: 0.7463, best val acc: 0.8897)\n",
      "[Epoch: 4780] train loss: 0.7789, train acc: 0.7430, val loss: 0.5644, val acc: 0.8870  (best train acc: 0.7476, best val acc: 0.8897)\n",
      "[Epoch: 4800] train loss: 0.7721, train acc: 0.7462, val loss: 0.5630, val acc: 0.8840  (best train acc: 0.7476, best val acc: 0.8897)\n",
      "[Epoch: 4820] train loss: 0.7866, train acc: 0.7359, val loss: 0.5647, val acc: 0.8850  (best train acc: 0.7476, best val acc: 0.8897)\n",
      "[Epoch: 4840] train loss: 0.7714, train acc: 0.7421, val loss: 0.5590, val acc: 0.8867  (best train acc: 0.7501, best val acc: 0.8897)\n",
      "[Epoch: 4860] train loss: 0.7804, train acc: 0.7439, val loss: 0.5583, val acc: 0.8853  (best train acc: 0.7501, best val acc: 0.8897)\n",
      "[Epoch: 4880] train loss: 0.7883, train acc: 0.7420, val loss: 0.5606, val acc: 0.8867  (best train acc: 0.7509, best val acc: 0.8897)\n",
      "[Epoch: 4900] train loss: 0.8139, train acc: 0.7169, val loss: 0.5959, val acc: 0.8644  (best train acc: 0.7509, best val acc: 0.8897)\n",
      "[Epoch: 4920] train loss: 0.8123, train acc: 0.7083, val loss: 0.5841, val acc: 0.8577  (best train acc: 0.7509, best val acc: 0.8897)\n",
      "[Epoch: 4940] train loss: 0.7773, train acc: 0.7390, val loss: 0.5586, val acc: 0.8836  (best train acc: 0.7509, best val acc: 0.8907)\n",
      "[Epoch: 4960] train loss: 0.7712, train acc: 0.7462, val loss: 0.5515, val acc: 0.8874  (best train acc: 0.7509, best val acc: 0.8907)\n",
      "[Epoch: 4980] train loss: 0.7645, train acc: 0.7423, val loss: 0.5509, val acc: 0.8833  (best train acc: 0.7509, best val acc: 0.8907)\n",
      "[Epoch: 5000] train loss: 0.7671, train acc: 0.7450, val loss: 0.5514, val acc: 0.8847  (best train acc: 0.7535, best val acc: 0.8907)\n",
      "[Epoch: 5020] train loss: 0.7862, train acc: 0.7315, val loss: 0.5487, val acc: 0.8884  (best train acc: 0.7535, best val acc: 0.8907)\n",
      "[Epoch: 5040] train loss: 0.7640, train acc: 0.7517, val loss: 0.5412, val acc: 0.8850  (best train acc: 0.7535, best val acc: 0.8907)\n",
      "[Epoch: 5060] train loss: 0.7781, train acc: 0.7426, val loss: 0.5446, val acc: 0.8823  (best train acc: 0.7535, best val acc: 0.8917)\n",
      "[Epoch: 5080] train loss: 0.7795, train acc: 0.7430, val loss: 0.5475, val acc: 0.8793  (best train acc: 0.7535, best val acc: 0.8917)\n",
      "[Epoch: 5100] train loss: 0.7729, train acc: 0.7440, val loss: 0.5527, val acc: 0.8847  (best train acc: 0.7535, best val acc: 0.8917)\n",
      "[Epoch: 5120] train loss: 0.7681, train acc: 0.7488, val loss: 0.5391, val acc: 0.8867  (best train acc: 0.7535, best val acc: 0.8917)\n",
      "[Epoch: 5140] train loss: 0.7918, train acc: 0.7389, val loss: 0.5450, val acc: 0.8863  (best train acc: 0.7535, best val acc: 0.8917)\n",
      "[Epoch: 5160] train loss: 0.7834, train acc: 0.7184, val loss: 0.5453, val acc: 0.8749  (best train acc: 0.7535, best val acc: 0.8917)\n",
      "[Epoch: 5180] train loss: 0.7660, train acc: 0.7487, val loss: 0.5384, val acc: 0.8860  (best train acc: 0.7535, best val acc: 0.8917)\n",
      "[Epoch: 5200] train loss: 0.7758, train acc: 0.7436, val loss: 0.5386, val acc: 0.8853  (best train acc: 0.7544, best val acc: 0.8917)\n",
      "[Epoch: 5220] train loss: 0.7709, train acc: 0.7491, val loss: 0.5460, val acc: 0.8874  (best train acc: 0.7544, best val acc: 0.8917)\n",
      "[Epoch: 5240] train loss: 0.7672, train acc: 0.7501, val loss: 0.5373, val acc: 0.8857  (best train acc: 0.7544, best val acc: 0.8917)\n",
      "[Epoch: 5260] train loss: 0.7838, train acc: 0.7354, val loss: 0.5453, val acc: 0.8782  (best train acc: 0.7578, best val acc: 0.8917)\n",
      "[Epoch: 5280] train loss: 0.7779, train acc: 0.7467, val loss: 0.5390, val acc: 0.8830  (best train acc: 0.7578, best val acc: 0.8917)\n",
      "[Epoch: 5300] train loss: 0.7652, train acc: 0.7470, val loss: 0.5453, val acc: 0.8843  (best train acc: 0.7578, best val acc: 0.8917)\n",
      "[Epoch: 5320] train loss: 0.7702, train acc: 0.7459, val loss: 0.5377, val acc: 0.8793  (best train acc: 0.7614, best val acc: 0.8917)\n",
      "[Epoch: 5340] train loss: 0.7551, train acc: 0.7573, val loss: 0.5320, val acc: 0.8867  (best train acc: 0.7614, best val acc: 0.8917)\n",
      "[Epoch: 5360] train loss: 0.7766, train acc: 0.7423, val loss: 0.5497, val acc: 0.8823  (best train acc: 0.7614, best val acc: 0.8917)\n",
      "[Epoch: 5380] train loss: 0.7786, train acc: 0.7392, val loss: 0.5334, val acc: 0.8863  (best train acc: 0.7614, best val acc: 0.8917)\n",
      "[Epoch: 5400] train loss: 0.7679, train acc: 0.7495, val loss: 0.5420, val acc: 0.8735  (best train acc: 0.7614, best val acc: 0.8917)\n",
      "[Epoch: 5420] train loss: 0.7553, train acc: 0.7510, val loss: 0.5549, val acc: 0.8806  (best train acc: 0.7614, best val acc: 0.8917)\n",
      "[Epoch: 5440] train loss: 0.7649, train acc: 0.7452, val loss: 0.5272, val acc: 0.8762  (best train acc: 0.7614, best val acc: 0.8917)\n",
      "[Epoch: 5460] train loss: 0.7432, train acc: 0.7553, val loss: 0.5282, val acc: 0.8836  (best train acc: 0.7614, best val acc: 0.8917)\n",
      "[Epoch: 5480] train loss: 0.7255, train acc: 0.7634, val loss: 0.5258, val acc: 0.8880  (best train acc: 0.7634, best val acc: 0.8924)\n",
      "[Epoch: 5500] train loss: 0.7364, train acc: 0.7586, val loss: 0.5141, val acc: 0.8847  (best train acc: 0.7668, best val acc: 0.8924)\n",
      "[Epoch: 5520] train loss: 0.7225, train acc: 0.7593, val loss: 0.5195, val acc: 0.8897  (best train acc: 0.7668, best val acc: 0.8924)\n",
      "[Epoch: 5540] train loss: 0.7278, train acc: 0.7574, val loss: 0.5114, val acc: 0.8877  (best train acc: 0.7682, best val acc: 0.8924)\n",
      "[Epoch: 5560] train loss: 0.7320, train acc: 0.7533, val loss: 0.5311, val acc: 0.8897  (best train acc: 0.7682, best val acc: 0.8941)\n",
      "[Epoch: 5580] train loss: 0.7345, train acc: 0.7540, val loss: 0.5179, val acc: 0.8836  (best train acc: 0.7682, best val acc: 0.8941)\n",
      "[Epoch: 5600] train loss: 0.7172, train acc: 0.7623, val loss: 0.5157, val acc: 0.8874  (best train acc: 0.7682, best val acc: 0.8941)\n",
      "[Epoch: 5620] train loss: 0.7373, train acc: 0.7595, val loss: 0.5160, val acc: 0.8732  (best train acc: 0.7702, best val acc: 0.8941)\n",
      "[Epoch: 5640] train loss: 0.7143, train acc: 0.7623, val loss: 0.5153, val acc: 0.8924  (best train acc: 0.7702, best val acc: 0.8941)\n",
      "[Epoch: 5660] train loss: 0.7301, train acc: 0.7626, val loss: 0.5173, val acc: 0.8914  (best train acc: 0.7702, best val acc: 0.8941)\n",
      "[Epoch: 5680] train loss: 0.7220, train acc: 0.7554, val loss: 0.5239, val acc: 0.8894  (best train acc: 0.7702, best val acc: 0.8941)\n",
      "[Epoch: 5700] train loss: 0.7265, train acc: 0.7577, val loss: 0.5199, val acc: 0.8887  (best train acc: 0.7702, best val acc: 0.8941)\n",
      "[Epoch: 5720] train loss: 0.7205, train acc: 0.7616, val loss: 0.5097, val acc: 0.8880  (best train acc: 0.7702, best val acc: 0.8941)\n",
      "[Epoch: 5740] train loss: 0.7121, train acc: 0.7676, val loss: 0.5133, val acc: 0.8894  (best train acc: 0.7702, best val acc: 0.8941)\n",
      "[Epoch: 5760] train loss: 0.7189, train acc: 0.7653, val loss: 0.4987, val acc: 0.8880  (best train acc: 0.7702, best val acc: 0.8941)\n",
      "[Epoch: 5780] train loss: 0.7148, train acc: 0.7608, val loss: 0.5157, val acc: 0.8931  (best train acc: 0.7702, best val acc: 0.8941)\n",
      "[Epoch: 5800] train loss: 0.7242, train acc: 0.7621, val loss: 0.5077, val acc: 0.8850  (best train acc: 0.7719, best val acc: 0.8941)\n",
      "[Epoch: 5820] train loss: 0.7239, train acc: 0.7577, val loss: 0.5012, val acc: 0.8934  (best train acc: 0.7719, best val acc: 0.8941)\n",
      "[Epoch: 5840] train loss: 0.7093, train acc: 0.7636, val loss: 0.5180, val acc: 0.8853  (best train acc: 0.7719, best val acc: 0.8941)\n",
      "[Epoch: 5860] train loss: 0.7124, train acc: 0.7628, val loss: 0.5027, val acc: 0.8860  (best train acc: 0.7719, best val acc: 0.8941)\n",
      "[Epoch: 5880] train loss: 0.7241, train acc: 0.7478, val loss: 0.5399, val acc: 0.8803  (best train acc: 0.7719, best val acc: 0.8941)\n",
      "[Epoch: 5900] train loss: 0.7412, train acc: 0.7494, val loss: 0.5048, val acc: 0.8867  (best train acc: 0.7719, best val acc: 0.8941)\n",
      "[Epoch: 5920] train loss: 0.7368, train acc: 0.7559, val loss: 0.5065, val acc: 0.8843  (best train acc: 0.7719, best val acc: 0.8941)\n",
      "[Epoch: 5940] train loss: 0.7014, train acc: 0.7727, val loss: 0.5026, val acc: 0.8843  (best train acc: 0.7727, best val acc: 0.8958)\n",
      "[Epoch: 5960] train loss: 0.7102, train acc: 0.7681, val loss: 0.5006, val acc: 0.8884  (best train acc: 0.7727, best val acc: 0.8958)\n",
      "[Epoch: 5980] train loss: 0.7224, train acc: 0.7674, val loss: 0.5072, val acc: 0.8897  (best train acc: 0.7744, best val acc: 0.8958)\n",
      "[Epoch: 6000] train loss: 0.7183, train acc: 0.7634, val loss: 0.5016, val acc: 0.8914  (best train acc: 0.7744, best val acc: 0.8958)\n",
      "[Epoch: 6020] train loss: 0.7223, train acc: 0.7670, val loss: 0.5063, val acc: 0.8813  (best train acc: 0.7744, best val acc: 0.8958)\n",
      "[Epoch: 6040] train loss: 0.7135, train acc: 0.7686, val loss: 0.5114, val acc: 0.8938  (best train acc: 0.7744, best val acc: 0.8958)\n",
      "[Epoch: 6060] train loss: 0.7193, train acc: 0.7679, val loss: 0.5006, val acc: 0.8890  (best train acc: 0.7744, best val acc: 0.8958)\n",
      "[Epoch: 6080] train loss: 0.7140, train acc: 0.7670, val loss: 0.4999, val acc: 0.8901  (best train acc: 0.7744, best val acc: 0.8958)\n",
      "[Epoch: 6100] train loss: 0.7055, train acc: 0.7711, val loss: 0.4996, val acc: 0.8938  (best train acc: 0.7744, best val acc: 0.8958)\n",
      "[Epoch: 6120] train loss: 0.7128, train acc: 0.7650, val loss: 0.5045, val acc: 0.8907  (best train acc: 0.7761, best val acc: 0.8958)\n",
      "[Epoch: 6140] train loss: 0.7248, train acc: 0.7602, val loss: 0.5005, val acc: 0.8917  (best train acc: 0.7761, best val acc: 0.8958)\n",
      "[Epoch: 6160] train loss: 0.7283, train acc: 0.7543, val loss: 0.4917, val acc: 0.8847  (best train acc: 0.7761, best val acc: 0.8958)\n",
      "[Epoch: 6180] train loss: 0.7177, train acc: 0.7628, val loss: 0.4896, val acc: 0.8948  (best train acc: 0.7761, best val acc: 0.8958)\n",
      "[Epoch: 6200] train loss: 0.7251, train acc: 0.7671, val loss: 0.5165, val acc: 0.8938  (best train acc: 0.7761, best val acc: 0.8961)\n",
      "[Epoch: 6220] train loss: 0.7198, train acc: 0.7621, val loss: 0.5061, val acc: 0.8931  (best train acc: 0.7761, best val acc: 0.8961)\n",
      "[Epoch: 6240] train loss: 0.7041, train acc: 0.7694, val loss: 0.5000, val acc: 0.8880  (best train acc: 0.7761, best val acc: 0.8961)\n",
      "[Epoch: 6260] train loss: 0.7172, train acc: 0.7669, val loss: 0.4931, val acc: 0.8890  (best train acc: 0.7761, best val acc: 0.8961)\n",
      "[Epoch: 6280] train loss: 0.7083, train acc: 0.7702, val loss: 0.4961, val acc: 0.8921  (best train acc: 0.7761, best val acc: 0.8961)\n",
      "[Epoch: 6300] train loss: 0.7116, train acc: 0.7697, val loss: 0.4910, val acc: 0.8941  (best train acc: 0.7761, best val acc: 0.8961)\n",
      "[Epoch: 6320] train loss: 0.7146, train acc: 0.7698, val loss: 0.4967, val acc: 0.8911  (best train acc: 0.7761, best val acc: 0.8961)\n",
      "[Epoch: 6340] train loss: 0.7133, train acc: 0.7611, val loss: 0.4984, val acc: 0.8924  (best train acc: 0.7761, best val acc: 0.8961)\n",
      "[Epoch: 6360] train loss: 0.7146, train acc: 0.7645, val loss: 0.5123, val acc: 0.8870  (best train acc: 0.7761, best val acc: 0.8961)\n",
      "[Epoch: 6380] train loss: 0.7065, train acc: 0.7687, val loss: 0.4893, val acc: 0.8931  (best train acc: 0.7761, best val acc: 0.8961)\n",
      "[Epoch: 6400] train loss: 0.7102, train acc: 0.7681, val loss: 0.5011, val acc: 0.8951  (best train acc: 0.7769, best val acc: 0.8961)\n",
      "[Epoch: 6420] train loss: 0.7095, train acc: 0.7639, val loss: 0.5014, val acc: 0.8924  (best train acc: 0.7769, best val acc: 0.8961)\n",
      "[Epoch: 6440] train loss: 0.7149, train acc: 0.7650, val loss: 0.4993, val acc: 0.8894  (best train acc: 0.7769, best val acc: 0.8968)\n",
      "[Epoch: 6460] train loss: 0.7119, train acc: 0.7677, val loss: 0.5063, val acc: 0.8904  (best train acc: 0.7769, best val acc: 0.8968)\n",
      "[Epoch: 6480] train loss: 0.7725, train acc: 0.6989, val loss: 0.4772, val acc: 0.8877  (best train acc: 0.7769, best val acc: 0.8968)\n",
      "[Epoch: 6500] train loss: 0.7223, train acc: 0.7556, val loss: 0.5156, val acc: 0.8759  (best train acc: 0.7769, best val acc: 0.8968)\n",
      "[Epoch: 6520] train loss: 0.7045, train acc: 0.7674, val loss: 0.5107, val acc: 0.8823  (best train acc: 0.7769, best val acc: 0.8968)\n",
      "[Epoch: 6540] train loss: 0.7087, train acc: 0.7728, val loss: 0.5031, val acc: 0.8958  (best train acc: 0.7769, best val acc: 0.8968)\n",
      "[Epoch: 6560] train loss: 0.7109, train acc: 0.7689, val loss: 0.5039, val acc: 0.8914  (best train acc: 0.7769, best val acc: 0.8968)\n",
      "[Epoch: 6580] train loss: 0.7071, train acc: 0.7721, val loss: 0.4933, val acc: 0.8951  (best train acc: 0.7769, best val acc: 0.8968)\n",
      "[Epoch: 6600] train loss: 0.7059, train acc: 0.7687, val loss: 0.4994, val acc: 0.8921  (best train acc: 0.7789, best val acc: 0.8975)\n",
      "[Epoch: 6620] train loss: 0.6975, train acc: 0.7759, val loss: 0.4894, val acc: 0.8914  (best train acc: 0.7789, best val acc: 0.8975)\n",
      "[Epoch: 6640] train loss: 0.6969, train acc: 0.7740, val loss: 0.4906, val acc: 0.8924  (best train acc: 0.7799, best val acc: 0.8975)\n",
      "[Epoch: 6660] train loss: 0.7133, train acc: 0.7741, val loss: 0.5020, val acc: 0.8405  (best train acc: 0.7843, best val acc: 0.8975)\n",
      "[Epoch: 6680] train loss: 0.7612, train acc: 0.7130, val loss: 0.5328, val acc: 0.8614  (best train acc: 0.7843, best val acc: 0.8975)\n",
      "[Epoch: 6700] train loss: 0.7132, train acc: 0.7372, val loss: 0.4913, val acc: 0.8874  (best train acc: 0.7843, best val acc: 0.8975)\n",
      "[Epoch: 6720] train loss: 0.7123, train acc: 0.7538, val loss: 0.4973, val acc: 0.8904  (best train acc: 0.7843, best val acc: 0.8975)\n",
      "[Epoch: 6740] train loss: 0.6985, train acc: 0.7566, val loss: 0.4896, val acc: 0.8917  (best train acc: 0.7843, best val acc: 0.8975)\n",
      "[Epoch: 6760] train loss: 0.6852, train acc: 0.7559, val loss: 0.4839, val acc: 0.8931  (best train acc: 0.7843, best val acc: 0.8975)\n",
      "[Epoch: 6780] train loss: 0.6905, train acc: 0.7650, val loss: 0.4722, val acc: 0.8890  (best train acc: 0.7843, best val acc: 0.8975)\n",
      "[Epoch: 6800] train loss: 0.7134, train acc: 0.7650, val loss: 0.4907, val acc: 0.8685  (best train acc: 0.7843, best val acc: 0.8975)\n",
      "[Epoch: 6820] train loss: 0.6988, train acc: 0.7639, val loss: 0.4887, val acc: 0.8894  (best train acc: 0.7843, best val acc: 0.8975)\n",
      "[Epoch: 6840] train loss: 0.6856, train acc: 0.7718, val loss: 0.4988, val acc: 0.8934  (best train acc: 0.7843, best val acc: 0.8975)\n",
      "[Epoch: 6860] train loss: 0.6839, train acc: 0.7761, val loss: 0.4854, val acc: 0.8927  (best train acc: 0.7843, best val acc: 0.8975)\n",
      "[Epoch: 6880] train loss: 0.6774, train acc: 0.7795, val loss: 0.4754, val acc: 0.8911  (best train acc: 0.7861, best val acc: 0.8975)\n",
      "[Epoch: 6900] train loss: 0.6625, train acc: 0.7920, val loss: 0.4775, val acc: 0.8951  (best train acc: 0.8017, best val acc: 0.8975)\n",
      "[Epoch: 6920] train loss: 0.6565, train acc: 0.8010, val loss: 0.4727, val acc: 0.8944  (best train acc: 0.8065, best val acc: 0.8975)\n",
      "[Epoch: 6940] train loss: 0.6520, train acc: 0.8013, val loss: 0.4831, val acc: 0.8948  (best train acc: 0.8068, best val acc: 0.8975)\n",
      "[Epoch: 6960] train loss: 0.6526, train acc: 0.8039, val loss: 0.4659, val acc: 0.8938  (best train acc: 0.8090, best val acc: 0.8975)\n",
      "[Epoch: 6980] train loss: 0.6531, train acc: 0.8055, val loss: 0.4827, val acc: 0.8786  (best train acc: 0.8125, best val acc: 0.8975)\n",
      "[Epoch: 7000] train loss: 0.6584, train acc: 0.7893, val loss: 0.4794, val acc: 0.8961  (best train acc: 0.8125, best val acc: 0.8975)\n",
      "[Epoch: 7020] train loss: 0.6504, train acc: 0.8053, val loss: 0.4616, val acc: 0.8948  (best train acc: 0.8125, best val acc: 0.8975)\n",
      "[Epoch: 7040] train loss: 0.6620, train acc: 0.8005, val loss: 0.4791, val acc: 0.8897  (best train acc: 0.8125, best val acc: 0.8975)\n",
      "[Epoch: 7060] train loss: 0.6507, train acc: 0.8046, val loss: 0.4845, val acc: 0.8954  (best train acc: 0.8125, best val acc: 0.8975)\n",
      "[Epoch: 7080] train loss: 0.6514, train acc: 0.7964, val loss: 0.4710, val acc: 0.8897  (best train acc: 0.8125, best val acc: 0.8975)\n",
      "[Epoch: 7100] train loss: 0.6668, train acc: 0.8028, val loss: 0.4693, val acc: 0.8921  (best train acc: 0.8125, best val acc: 0.8975)\n",
      "[Epoch: 7120] train loss: 0.6645, train acc: 0.7951, val loss: 0.4724, val acc: 0.8938  (best train acc: 0.8125, best val acc: 0.8975)\n",
      "[Epoch: 7140] train loss: 0.6415, train acc: 0.8075, val loss: 0.4717, val acc: 0.8954  (best train acc: 0.8125, best val acc: 0.8975)\n",
      "[Epoch: 7160] train loss: 0.6544, train acc: 0.8047, val loss: 0.4670, val acc: 0.8931  (best train acc: 0.8125, best val acc: 0.8995)\n",
      "[Epoch: 7180] train loss: 0.6581, train acc: 0.7926, val loss: 0.5030, val acc: 0.8776  (best train acc: 0.8125, best val acc: 0.8995)\n",
      "[Epoch: 7200] train loss: 0.6576, train acc: 0.8001, val loss: 0.4684, val acc: 0.8874  (best train acc: 0.8125, best val acc: 0.8995)\n",
      "[Epoch: 7220] train loss: 0.6585, train acc: 0.8003, val loss: 0.4958, val acc: 0.8894  (best train acc: 0.8125, best val acc: 0.8995)\n",
      "[Epoch: 7240] train loss: 0.6660, train acc: 0.7851, val loss: 0.4725, val acc: 0.8867  (best train acc: 0.8125, best val acc: 0.8995)\n",
      "[Epoch: 7260] train loss: 0.6620, train acc: 0.8024, val loss: 0.4708, val acc: 0.8927  (best train acc: 0.8135, best val acc: 0.8995)\n",
      "[Epoch: 7280] train loss: 0.6491, train acc: 0.8051, val loss: 0.4715, val acc: 0.8813  (best train acc: 0.8135, best val acc: 0.8995)\n",
      "[Epoch: 7300] train loss: 0.6576, train acc: 0.7892, val loss: 0.4781, val acc: 0.8931  (best train acc: 0.8135, best val acc: 0.8995)\n",
      "[Epoch: 7320] train loss: 0.6384, train acc: 0.8040, val loss: 0.4641, val acc: 0.8921  (best train acc: 0.8135, best val acc: 0.8995)\n",
      "[Epoch: 7340] train loss: 0.6566, train acc: 0.7968, val loss: 0.4723, val acc: 0.8958  (best train acc: 0.8143, best val acc: 0.8995)\n",
      "[Epoch: 7360] train loss: 0.6491, train acc: 0.8044, val loss: 0.4763, val acc: 0.8836  (best train acc: 0.8146, best val acc: 0.8995)\n",
      "[Epoch: 7380] train loss: 0.6386, train acc: 0.8085, val loss: 0.4608, val acc: 0.8938  (best train acc: 0.8147, best val acc: 0.8995)\n",
      "[Epoch: 7400] train loss: 0.6416, train acc: 0.8104, val loss: 0.4612, val acc: 0.8907  (best train acc: 0.8147, best val acc: 0.8995)\n",
      "[Epoch: 7420] train loss: 0.6483, train acc: 0.8023, val loss: 0.4590, val acc: 0.8917  (best train acc: 0.8147, best val acc: 0.8995)\n",
      "[Epoch: 7440] train loss: 0.6410, train acc: 0.8088, val loss: 0.4563, val acc: 0.8938  (best train acc: 0.8147, best val acc: 0.8995)\n",
      "[Epoch: 7460] train loss: 0.6429, train acc: 0.8066, val loss: 0.4858, val acc: 0.8924  (best train acc: 0.8147, best val acc: 0.8995)\n",
      "[Epoch: 7480] train loss: 0.7077, train acc: 0.7765, val loss: 0.4579, val acc: 0.8806  (best train acc: 0.8147, best val acc: 0.8995)\n",
      "[Epoch: 7500] train loss: 0.6816, train acc: 0.7693, val loss: 0.4862, val acc: 0.8897  (best train acc: 0.8147, best val acc: 0.8995)\n",
      "[Epoch: 7520] train loss: 0.6571, train acc: 0.8006, val loss: 0.4600, val acc: 0.8958  (best train acc: 0.8147, best val acc: 0.8995)\n",
      "[Epoch: 7540] train loss: 0.6403, train acc: 0.8094, val loss: 0.4608, val acc: 0.8944  (best train acc: 0.8147, best val acc: 0.8995)\n",
      "[Epoch: 7560] train loss: 0.6469, train acc: 0.8080, val loss: 0.4596, val acc: 0.8917  (best train acc: 0.8147, best val acc: 0.8995)\n",
      "[Epoch: 7580] train loss: 0.6456, train acc: 0.8128, val loss: 0.4692, val acc: 0.8863  (best train acc: 0.8166, best val acc: 0.8995)\n",
      "[Epoch: 7600] train loss: 0.6390, train acc: 0.8124, val loss: 0.4608, val acc: 0.8907  (best train acc: 0.8166, best val acc: 0.8995)\n",
      "[Epoch: 7620] train loss: 0.6253, train acc: 0.8115, val loss: 0.4618, val acc: 0.8954  (best train acc: 0.8166, best val acc: 0.8995)\n",
      "[Epoch: 7640] train loss: 0.7131, train acc: 0.7658, val loss: 0.4456, val acc: 0.8971  (best train acc: 0.8166, best val acc: 0.8995)\n",
      "[Epoch: 7660] train loss: 0.6429, train acc: 0.8011, val loss: 0.4483, val acc: 0.8944  (best train acc: 0.8166, best val acc: 0.8995)\n",
      "[Epoch: 7680] train loss: 0.6288, train acc: 0.8103, val loss: 0.4540, val acc: 0.8965  (best train acc: 0.8166, best val acc: 0.8995)\n",
      "[Epoch: 7700] train loss: 0.6415, train acc: 0.8093, val loss: 0.4602, val acc: 0.8901  (best train acc: 0.8170, best val acc: 0.8995)\n",
      "[Epoch: 7720] train loss: 0.6496, train acc: 0.8029, val loss: 0.4596, val acc: 0.8938  (best train acc: 0.8177, best val acc: 0.8995)\n",
      "[Epoch: 7740] train loss: 0.6538, train acc: 0.8005, val loss: 0.4531, val acc: 0.8924  (best train acc: 0.8177, best val acc: 0.8995)\n",
      "[Epoch: 7760] train loss: 0.6383, train acc: 0.8055, val loss: 0.4501, val acc: 0.8931  (best train acc: 0.8177, best val acc: 0.8995)\n",
      "[Epoch: 7780] train loss: 0.6457, train acc: 0.8050, val loss: 0.4627, val acc: 0.8944  (best train acc: 0.8177, best val acc: 0.8995)\n",
      "[Epoch: 7800] train loss: 0.6309, train acc: 0.8125, val loss: 0.4544, val acc: 0.8921  (best train acc: 0.8180, best val acc: 0.8995)\n",
      "[Epoch: 7820] train loss: 0.6474, train acc: 0.7911, val loss: 0.4690, val acc: 0.8796  (best train acc: 0.8180, best val acc: 0.8995)\n",
      "[Epoch: 7840] train loss: 0.6279, train acc: 0.8096, val loss: 0.4647, val acc: 0.8897  (best train acc: 0.8180, best val acc: 0.8995)\n",
      "[Epoch: 7860] train loss: 0.6295, train acc: 0.8081, val loss: 0.4640, val acc: 0.8874  (best train acc: 0.8190, best val acc: 0.8995)\n",
      "[Epoch: 7880] train loss: 0.6303, train acc: 0.8078, val loss: 0.4562, val acc: 0.8927  (best train acc: 0.8190, best val acc: 0.8995)\n",
      "[Epoch: 7900] train loss: 0.6235, train acc: 0.8118, val loss: 0.4495, val acc: 0.8911  (best train acc: 0.8200, best val acc: 0.8995)\n",
      "[Epoch: 7920] train loss: 0.6493, train acc: 0.8018, val loss: 0.4547, val acc: 0.8853  (best train acc: 0.8200, best val acc: 0.8995)\n",
      "[Epoch: 7940] train loss: 0.6379, train acc: 0.8146, val loss: 0.4508, val acc: 0.8914  (best train acc: 0.8204, best val acc: 0.8995)\n",
      "[Epoch: 7960] train loss: 0.6264, train acc: 0.8070, val loss: 0.4731, val acc: 0.8840  (best train acc: 0.8220, best val acc: 0.8995)\n",
      "[Epoch: 7980] train loss: 0.6283, train acc: 0.8081, val loss: 0.4716, val acc: 0.8739  (best train acc: 0.8220, best val acc: 0.8995)\n",
      "[Epoch: 8000] train loss: 0.6320, train acc: 0.8118, val loss: 0.4510, val acc: 0.8863  (best train acc: 0.8220, best val acc: 0.8995)\n",
      "[Epoch: 8020] train loss: 0.6272, train acc: 0.8065, val loss: 0.4486, val acc: 0.8971  (best train acc: 0.8220, best val acc: 0.8995)\n",
      "[Epoch: 8040] train loss: 0.6382, train acc: 0.8128, val loss: 0.4488, val acc: 0.8894  (best train acc: 0.8220, best val acc: 0.8995)\n",
      "[Epoch: 8060] train loss: 0.6138, train acc: 0.8145, val loss: 0.4459, val acc: 0.8927  (best train acc: 0.8220, best val acc: 0.8995)\n",
      "[Epoch: 8080] train loss: 0.6201, train acc: 0.8118, val loss: 0.4462, val acc: 0.8998  (best train acc: 0.8220, best val acc: 0.8998)\n",
      "[Epoch: 8100] train loss: 0.6177, train acc: 0.8132, val loss: 0.4682, val acc: 0.8782  (best train acc: 0.8220, best val acc: 0.9002)\n",
      "[Epoch: 8120] train loss: 0.6562, train acc: 0.7826, val loss: 0.4801, val acc: 0.8938  (best train acc: 0.8220, best val acc: 0.9002)\n",
      "[Epoch: 8140] train loss: 0.6127, train acc: 0.8161, val loss: 0.4488, val acc: 0.8961  (best train acc: 0.8220, best val acc: 0.9002)\n",
      "[Epoch: 8160] train loss: 0.6569, train acc: 0.7929, val loss: 0.4291, val acc: 0.8921  (best train acc: 0.8233, best val acc: 0.9002)\n",
      "[Epoch: 8180] train loss: 0.6319, train acc: 0.8013, val loss: 0.4502, val acc: 0.8877  (best train acc: 0.8233, best val acc: 0.9002)\n",
      "[Epoch: 8200] train loss: 0.6108, train acc: 0.8142, val loss: 0.4462, val acc: 0.8954  (best train acc: 0.8233, best val acc: 0.9002)\n",
      "[Epoch: 8220] train loss: 0.6123, train acc: 0.8181, val loss: 0.4546, val acc: 0.8897  (best train acc: 0.8233, best val acc: 0.9002)\n",
      "[Epoch: 8240] train loss: 0.6064, train acc: 0.8233, val loss: 0.4401, val acc: 0.8965  (best train acc: 0.8233, best val acc: 0.9002)\n",
      "[Epoch: 8260] train loss: 0.6044, train acc: 0.8185, val loss: 0.4443, val acc: 0.8981  (best train acc: 0.8235, best val acc: 0.9005)\n",
      "[Epoch: 8280] train loss: 0.6272, train acc: 0.8130, val loss: 0.4548, val acc: 0.8847  (best train acc: 0.8264, best val acc: 0.9005)\n",
      "[Epoch: 8300] train loss: 0.6445, train acc: 0.8036, val loss: 0.4692, val acc: 0.8715  (best train acc: 0.8264, best val acc: 0.9005)\n",
      "[Epoch: 8320] train loss: 0.6078, train acc: 0.8194, val loss: 0.4515, val acc: 0.8921  (best train acc: 0.8264, best val acc: 0.9005)\n",
      "[Epoch: 8340] train loss: 0.6165, train acc: 0.8123, val loss: 0.4454, val acc: 0.8938  (best train acc: 0.8264, best val acc: 0.9005)\n",
      "[Epoch: 8360] train loss: 0.5998, train acc: 0.8190, val loss: 0.4440, val acc: 0.8978  (best train acc: 0.8264, best val acc: 0.9005)\n",
      "[Epoch: 8380] train loss: 0.5953, train acc: 0.8215, val loss: 0.4433, val acc: 0.8948  (best train acc: 0.8264, best val acc: 0.9008)\n",
      "[Epoch: 8400] train loss: 0.6014, train acc: 0.8232, val loss: 0.4414, val acc: 0.8961  (best train acc: 0.8264, best val acc: 0.9008)\n",
      "[Epoch: 8420] train loss: 0.6115, train acc: 0.8156, val loss: 0.4454, val acc: 0.8985  (best train acc: 0.8264, best val acc: 0.9008)\n",
      "[Epoch: 8440] train loss: 0.6182, train acc: 0.8148, val loss: 0.4442, val acc: 0.8958  (best train acc: 0.8267, best val acc: 0.9008)\n",
      "[Epoch: 8460] train loss: 0.6216, train acc: 0.8154, val loss: 0.4488, val acc: 0.8934  (best train acc: 0.8267, best val acc: 0.9008)\n",
      "[Epoch: 8480] train loss: 0.6153, train acc: 0.8166, val loss: 0.4429, val acc: 0.8796  (best train acc: 0.8267, best val acc: 0.9008)\n",
      "[Epoch: 8500] train loss: 0.6077, train acc: 0.8240, val loss: 0.4320, val acc: 0.8968  (best train acc: 0.8267, best val acc: 0.9008)\n",
      "[Epoch: 8520] train loss: 0.6324, train acc: 0.8117, val loss: 0.4603, val acc: 0.8715  (best train acc: 0.8267, best val acc: 0.9008)\n",
      "[Epoch: 8540] train loss: 0.6100, train acc: 0.8216, val loss: 0.4463, val acc: 0.8998  (best train acc: 0.8267, best val acc: 0.9008)\n",
      "[Epoch: 8560] train loss: 0.6205, train acc: 0.8120, val loss: 0.5096, val acc: 0.8806  (best train acc: 0.8267, best val acc: 0.9008)\n",
      "[Epoch: 8580] train loss: 0.6238, train acc: 0.8074, val loss: 0.4379, val acc: 0.8975  (best train acc: 0.8267, best val acc: 0.9008)\n",
      "[Epoch: 8600] train loss: 0.5967, train acc: 0.8200, val loss: 0.4362, val acc: 0.8961  (best train acc: 0.8267, best val acc: 0.9008)\n",
      "[Epoch: 8620] train loss: 0.6040, train acc: 0.8242, val loss: 0.4509, val acc: 0.8954  (best train acc: 0.8267, best val acc: 0.9008)\n",
      "[Epoch: 8640] train loss: 0.6148, train acc: 0.8180, val loss: 0.4232, val acc: 0.8951  (best train acc: 0.8269, best val acc: 0.9008)\n",
      "[Epoch: 8660] train loss: 0.6031, train acc: 0.8231, val loss: 0.4264, val acc: 0.8995  (best train acc: 0.8269, best val acc: 0.9008)\n",
      "[Epoch: 8680] train loss: 0.6045, train acc: 0.8155, val loss: 0.4411, val acc: 0.8917  (best train acc: 0.8269, best val acc: 0.9008)\n",
      "[Epoch: 8700] train loss: 0.6116, train acc: 0.8042, val loss: 0.4268, val acc: 0.8907  (best train acc: 0.8269, best val acc: 0.9008)\n",
      "[Epoch: 8720] train loss: 0.6113, train acc: 0.8169, val loss: 0.4386, val acc: 0.8981  (best train acc: 0.8269, best val acc: 0.9008)\n",
      "[Epoch: 8740] train loss: 0.5998, train acc: 0.8225, val loss: 0.4506, val acc: 0.8880  (best train acc: 0.8291, best val acc: 0.9008)\n",
      "[Epoch: 8760] train loss: 0.6169, train acc: 0.8178, val loss: 0.4514, val acc: 0.8867  (best train acc: 0.8291, best val acc: 0.9008)\n",
      "[Epoch: 8780] train loss: 0.6266, train acc: 0.8079, val loss: 0.4238, val acc: 0.8901  (best train acc: 0.8291, best val acc: 0.9008)\n",
      "[Epoch: 8800] train loss: 0.6169, train acc: 0.8159, val loss: 0.4310, val acc: 0.8961  (best train acc: 0.8291, best val acc: 0.9008)\n",
      "[Epoch: 8820] train loss: 0.6320, train acc: 0.8133, val loss: 0.4340, val acc: 0.8924  (best train acc: 0.8291, best val acc: 0.9008)\n",
      "[Epoch: 8840] train loss: 0.6086, train acc: 0.8146, val loss: 0.4419, val acc: 0.8904  (best train acc: 0.8291, best val acc: 0.9008)\n",
      "[Epoch: 8860] train loss: 0.6026, train acc: 0.8141, val loss: 0.4355, val acc: 0.8968  (best train acc: 0.8291, best val acc: 0.9008)\n",
      "[Epoch: 8880] train loss: 0.5941, train acc: 0.8227, val loss: 0.4370, val acc: 0.8948  (best train acc: 0.8291, best val acc: 0.9008)\n",
      "[Epoch: 8900] train loss: 0.6025, train acc: 0.8217, val loss: 0.4357, val acc: 0.8958  (best train acc: 0.8291, best val acc: 0.9008)\n",
      "[Epoch: 8920] train loss: 0.5859, train acc: 0.8240, val loss: 0.4422, val acc: 0.8992  (best train acc: 0.8303, best val acc: 0.9012)\n",
      "[Epoch: 8940] train loss: 0.5841, train acc: 0.8186, val loss: 0.4424, val acc: 0.8985  (best train acc: 0.8303, best val acc: 0.9012)\n",
      "[Epoch: 8960] train loss: 0.5938, train acc: 0.8187, val loss: 0.4339, val acc: 0.8998  (best train acc: 0.8303, best val acc: 0.9012)\n",
      "[Epoch: 8980] train loss: 0.5715, train acc: 0.8210, val loss: 0.4204, val acc: 0.9002  (best train acc: 0.8303, best val acc: 0.9015)\n",
      "[Epoch: 9000] train loss: 0.5695, train acc: 0.8254, val loss: 0.4195, val acc: 0.9005  (best train acc: 0.8303, best val acc: 0.9029)\n",
      "[Epoch: 9020] train loss: 0.5832, train acc: 0.8210, val loss: 0.4694, val acc: 0.8708  (best train acc: 0.8303, best val acc: 0.9029)\n",
      "[Epoch: 9040] train loss: 0.5845, train acc: 0.8210, val loss: 0.4482, val acc: 0.8995  (best train acc: 0.8303, best val acc: 0.9039)\n",
      "[Epoch: 9060] train loss: 0.5788, train acc: 0.8263, val loss: 0.4223, val acc: 0.8958  (best train acc: 0.8303, best val acc: 0.9039)\n",
      "[Epoch: 9080] train loss: 0.6024, train acc: 0.8161, val loss: 0.4146, val acc: 0.8981  (best train acc: 0.8307, best val acc: 0.9039)\n",
      "[Epoch: 9100] train loss: 0.5706, train acc: 0.8181, val loss: 0.4323, val acc: 0.9029  (best train acc: 0.8307, best val acc: 0.9039)\n",
      "[Epoch: 9120] train loss: 0.5707, train acc: 0.8269, val loss: 0.4142, val acc: 0.9025  (best train acc: 0.8307, best val acc: 0.9039)\n",
      "[Epoch: 9140] train loss: 0.5676, train acc: 0.8239, val loss: 0.4397, val acc: 0.8981  (best train acc: 0.8309, best val acc: 0.9039)\n",
      "[Epoch: 9160] train loss: 0.5682, train acc: 0.8255, val loss: 0.4311, val acc: 0.8971  (best train acc: 0.8343, best val acc: 0.9039)\n",
      "[Epoch: 9180] train loss: 0.5696, train acc: 0.8252, val loss: 0.4176, val acc: 0.8954  (best train acc: 0.8343, best val acc: 0.9042)\n",
      "[Epoch: 9200] train loss: 0.5684, train acc: 0.8216, val loss: 0.4178, val acc: 0.8971  (best train acc: 0.8343, best val acc: 0.9042)\n",
      "[Epoch: 9220] train loss: 0.5704, train acc: 0.8189, val loss: 0.4153, val acc: 0.8944  (best train acc: 0.8343, best val acc: 0.9046)\n",
      "[Epoch: 9240] train loss: 0.5606, train acc: 0.8284, val loss: 0.4167, val acc: 0.8924  (best train acc: 0.8343, best val acc: 0.9046)\n",
      "[Epoch: 9260] train loss: 0.5551, train acc: 0.8321, val loss: 0.4137, val acc: 0.9019  (best train acc: 0.8343, best val acc: 0.9046)\n",
      "[Epoch: 9280] train loss: 0.6127, train acc: 0.7530, val loss: 0.4077, val acc: 0.9029  (best train acc: 0.8343, best val acc: 0.9046)\n",
      "[Epoch: 9300] train loss: 0.5628, train acc: 0.8274, val loss: 0.4088, val acc: 0.8995  (best train acc: 0.8343, best val acc: 0.9046)\n",
      "[Epoch: 9320] train loss: 0.5521, train acc: 0.8287, val loss: 0.4153, val acc: 0.8995  (best train acc: 0.8344, best val acc: 0.9046)\n",
      "[Epoch: 9340] train loss: 0.5575, train acc: 0.8297, val loss: 0.4106, val acc: 0.9012  (best train acc: 0.8374, best val acc: 0.9046)\n",
      "[Epoch: 9360] train loss: 0.5410, train acc: 0.8297, val loss: 0.4039, val acc: 0.8985  (best train acc: 0.8374, best val acc: 0.9046)\n",
      "[Epoch: 9380] train loss: 0.5588, train acc: 0.8171, val loss: 0.4258, val acc: 0.8998  (best train acc: 0.8374, best val acc: 0.9046)\n",
      "[Epoch: 9400] train loss: 0.5523, train acc: 0.8232, val loss: 0.4063, val acc: 0.9008  (best train acc: 0.8374, best val acc: 0.9046)\n",
      "[Epoch: 9420] train loss: 0.5489, train acc: 0.8272, val loss: 0.4078, val acc: 0.9029  (best train acc: 0.8374, best val acc: 0.9046)\n",
      "[Epoch: 9440] train loss: 0.5579, train acc: 0.8216, val loss: 0.4111, val acc: 0.8975  (best train acc: 0.8374, best val acc: 0.9046)\n",
      "[Epoch: 9460] train loss: 0.7133, train acc: 0.7843, val loss: 0.4377, val acc: 0.8675  (best train acc: 0.8374, best val acc: 0.9046)\n",
      "[Epoch: 9480] train loss: 0.5850, train acc: 0.8190, val loss: 0.4325, val acc: 0.8887  (best train acc: 0.8374, best val acc: 0.9046)\n",
      "[Epoch: 9500] train loss: 0.5648, train acc: 0.8228, val loss: 0.4197, val acc: 0.8965  (best train acc: 0.8374, best val acc: 0.9046)\n",
      "[Epoch: 9520] train loss: 0.5596, train acc: 0.8213, val loss: 0.4287, val acc: 0.8877  (best train acc: 0.8374, best val acc: 0.9046)\n",
      "[Epoch: 9540] train loss: 0.5602, train acc: 0.8203, val loss: 0.4190, val acc: 0.8938  (best train acc: 0.8374, best val acc: 0.9046)\n",
      "[Epoch: 9560] train loss: 0.5478, train acc: 0.8284, val loss: 0.4220, val acc: 0.8985  (best train acc: 0.8374, best val acc: 0.9046)\n",
      "[Epoch: 9580] train loss: 0.5533, train acc: 0.8282, val loss: 0.4157, val acc: 0.8968  (best train acc: 0.8374, best val acc: 0.9046)\n",
      "[Epoch: 9600] train loss: 0.5488, train acc: 0.8317, val loss: 0.4168, val acc: 0.8931  (best train acc: 0.8374, best val acc: 0.9046)\n",
      "[Epoch: 9620] train loss: 0.5478, train acc: 0.8313, val loss: 0.4052, val acc: 0.8988  (best train acc: 0.8374, best val acc: 0.9046)\n",
      "[Epoch: 9640] train loss: 0.5501, train acc: 0.8245, val loss: 0.4114, val acc: 0.8992  (best train acc: 0.8374, best val acc: 0.9046)\n",
      "[Epoch: 9660] train loss: 0.5475, train acc: 0.8307, val loss: 0.4153, val acc: 0.8995  (best train acc: 0.8374, best val acc: 0.9046)\n",
      "[Epoch: 9680] train loss: 0.5476, train acc: 0.8311, val loss: 0.4156, val acc: 0.8971  (best train acc: 0.8395, best val acc: 0.9046)\n",
      "[Epoch: 9700] train loss: 0.5553, train acc: 0.8253, val loss: 0.4105, val acc: 0.9012  (best train acc: 0.8395, best val acc: 0.9046)\n",
      "[Epoch: 9720] train loss: 0.5531, train acc: 0.8269, val loss: 0.4232, val acc: 0.8968  (best train acc: 0.8395, best val acc: 0.9046)\n",
      "[Epoch: 9740] train loss: 0.5538, train acc: 0.8262, val loss: 0.4155, val acc: 0.8954  (best train acc: 0.8395, best val acc: 0.9046)\n",
      "[Epoch: 9760] train loss: 0.5372, train acc: 0.8333, val loss: 0.4038, val acc: 0.9002  (best train acc: 0.8395, best val acc: 0.9046)\n",
      "[Epoch: 9780] train loss: 0.5637, train acc: 0.8180, val loss: 0.4174, val acc: 0.9002  (best train acc: 0.8395, best val acc: 0.9046)\n",
      "[Epoch: 9800] train loss: 0.5536, train acc: 0.8318, val loss: 0.4271, val acc: 0.8965  (best train acc: 0.8395, best val acc: 0.9046)\n",
      "[Epoch: 9820] train loss: 0.5570, train acc: 0.8234, val loss: 0.4171, val acc: 0.8884  (best train acc: 0.8395, best val acc: 0.9046)\n",
      "[Epoch: 9840] train loss: 0.5398, train acc: 0.8292, val loss: 0.4028, val acc: 0.9005  (best train acc: 0.8395, best val acc: 0.9046)\n",
      "[Epoch: 9860] train loss: 0.5470, train acc: 0.8262, val loss: 0.4126, val acc: 0.9022  (best train acc: 0.8395, best val acc: 0.9046)\n",
      "[Epoch: 9880] train loss: 0.5471, train acc: 0.8304, val loss: 0.4138, val acc: 0.9002  (best train acc: 0.8395, best val acc: 0.9046)\n",
      "[Epoch: 9900] train loss: 0.5420, train acc: 0.8317, val loss: 0.4165, val acc: 0.8985  (best train acc: 0.8395, best val acc: 0.9046)\n",
      "[Epoch: 9920] train loss: 0.5407, train acc: 0.8311, val loss: 0.4065, val acc: 0.8975  (best train acc: 0.8395, best val acc: 0.9046)\n",
      "[Epoch: 9940] train loss: 0.5492, train acc: 0.8323, val loss: 0.4044, val acc: 0.8978  (best train acc: 0.8395, best val acc: 0.9046)\n",
      "[Epoch: 9960] train loss: 0.5470, train acc: 0.8274, val loss: 0.4233, val acc: 0.8934  (best train acc: 0.8395, best val acc: 0.9046)\n",
      "[Epoch: 9980] train loss: 0.5439, train acc: 0.8300, val loss: 0.4100, val acc: 0.8988  (best train acc: 0.8395, best val acc: 0.9046)\n",
      "[Epoch: 10000] train loss: 0.5483, train acc: 0.8271, val loss: 0.4118, val acc: 0.9039  (best train acc: 0.8395, best val acc: 0.9046)\n",
      "[Epoch: 10020] train loss: 0.5467, train acc: 0.8318, val loss: 0.4053, val acc: 0.8931  (best train acc: 0.8395, best val acc: 0.9046)\n",
      "[Epoch: 10040] train loss: 0.5433, train acc: 0.8266, val loss: 0.3994, val acc: 0.8975  (best train acc: 0.8395, best val acc: 0.9046)\n",
      "[Epoch: 10060] train loss: 0.5504, train acc: 0.8249, val loss: 0.4010, val acc: 0.9015  (best train acc: 0.8395, best val acc: 0.9046)\n",
      "[Epoch: 10080] train loss: 0.5497, train acc: 0.8232, val loss: 0.4147, val acc: 0.8985  (best train acc: 0.8395, best val acc: 0.9046)\n",
      "[Epoch: 10100] train loss: 0.5494, train acc: 0.8330, val loss: 0.4059, val acc: 0.9035  (best train acc: 0.8395, best val acc: 0.9046)\n",
      "[Epoch: 10120] train loss: 0.5498, train acc: 0.8286, val loss: 0.3984, val acc: 0.8998  (best train acc: 0.8395, best val acc: 0.9046)\n",
      "[Epoch: 10140] train loss: 0.5468, train acc: 0.8274, val loss: 0.4222, val acc: 0.9032  (best train acc: 0.8395, best val acc: 0.9052)\n",
      "[Epoch: 10160] train loss: 0.5531, train acc: 0.8274, val loss: 0.4240, val acc: 0.9005  (best train acc: 0.8395, best val acc: 0.9052)\n",
      "[Epoch: 10180] train loss: 0.5383, train acc: 0.8326, val loss: 0.4120, val acc: 0.9015  (best train acc: 0.8395, best val acc: 0.9052)\n",
      "[Epoch: 10200] train loss: 0.5425, train acc: 0.8294, val loss: 0.4098, val acc: 0.9029  (best train acc: 0.8395, best val acc: 0.9059)\n",
      "[Epoch: 10220] train loss: 0.5421, train acc: 0.8281, val loss: 0.4186, val acc: 0.8978  (best train acc: 0.8395, best val acc: 0.9062)\n",
      "[Epoch: 10240] train loss: 0.5585, train acc: 0.8243, val loss: 0.3988, val acc: 0.9052  (best train acc: 0.8395, best val acc: 0.9062)\n",
      "[Epoch: 10260] train loss: 0.5427, train acc: 0.8255, val loss: 0.3958, val acc: 0.9039  (best train acc: 0.8395, best val acc: 0.9066)\n",
      "[Epoch: 10280] train loss: 0.5517, train acc: 0.8232, val loss: 0.4125, val acc: 0.9005  (best train acc: 0.8395, best val acc: 0.9066)\n",
      "[Epoch: 10300] train loss: 0.5423, train acc: 0.8355, val loss: 0.3952, val acc: 0.8988  (best train acc: 0.8395, best val acc: 0.9066)\n",
      "[Epoch: 10320] train loss: 0.5386, train acc: 0.8372, val loss: 0.4119, val acc: 0.8914  (best train acc: 0.8395, best val acc: 0.9066)\n",
      "[Epoch: 10340] train loss: 0.5460, train acc: 0.8334, val loss: 0.3959, val acc: 0.8911  (best train acc: 0.8395, best val acc: 0.9066)\n",
      "[Epoch: 10360] train loss: 0.5441, train acc: 0.8304, val loss: 0.4009, val acc: 0.8992  (best train acc: 0.8395, best val acc: 0.9066)\n",
      "[Epoch: 10380] train loss: 0.5403, train acc: 0.8322, val loss: 0.3941, val acc: 0.9002  (best train acc: 0.8395, best val acc: 0.9066)\n",
      "[Epoch: 10400] train loss: 0.5299, train acc: 0.8309, val loss: 0.3992, val acc: 0.8998  (best train acc: 0.8395, best val acc: 0.9066)\n",
      "[Epoch: 10420] train loss: 0.5399, train acc: 0.8351, val loss: 0.3968, val acc: 0.9056  (best train acc: 0.8395, best val acc: 0.9066)\n",
      "[Epoch: 10440] train loss: 0.5425, train acc: 0.8342, val loss: 0.3997, val acc: 0.8958  (best train acc: 0.8395, best val acc: 0.9066)\n",
      "[Epoch: 10460] train loss: 0.5364, train acc: 0.8252, val loss: 0.3946, val acc: 0.8971  (best train acc: 0.8395, best val acc: 0.9066)\n",
      "[Epoch: 10480] train loss: 0.5484, train acc: 0.8266, val loss: 0.4091, val acc: 0.8948  (best train acc: 0.8395, best val acc: 0.9066)\n",
      "[Epoch: 10500] train loss: 0.5482, train acc: 0.8302, val loss: 0.3908, val acc: 0.8934  (best train acc: 0.8395, best val acc: 0.9066)\n",
      "[Epoch: 10520] train loss: 0.5384, train acc: 0.8301, val loss: 0.4072, val acc: 0.9005  (best train acc: 0.8395, best val acc: 0.9066)\n",
      "[Epoch: 10540] train loss: 0.5443, train acc: 0.8297, val loss: 0.3980, val acc: 0.8998  (best train acc: 0.8395, best val acc: 0.9066)\n",
      "[Epoch: 10560] train loss: 0.5336, train acc: 0.8390, val loss: 0.3955, val acc: 0.8981  (best train acc: 0.8395, best val acc: 0.9066)\n",
      "[Epoch: 10580] train loss: 0.5281, train acc: 0.8362, val loss: 0.3931, val acc: 0.8978  (best train acc: 0.8395, best val acc: 0.9066)\n",
      "[Epoch: 10600] train loss: 0.5326, train acc: 0.8266, val loss: 0.3999, val acc: 0.9008  (best train acc: 0.8395, best val acc: 0.9066)\n",
      "[Epoch: 10620] train loss: 0.5296, train acc: 0.8326, val loss: 0.3981, val acc: 0.9042  (best train acc: 0.8395, best val acc: 0.9066)\n",
      "[Epoch: 10640] train loss: 0.5226, train acc: 0.8372, val loss: 0.3929, val acc: 0.9029  (best train acc: 0.8395, best val acc: 0.9066)\n",
      "[Epoch: 10660] train loss: 0.5347, train acc: 0.8226, val loss: 0.3969, val acc: 0.9032  (best train acc: 0.8395, best val acc: 0.9066)\n",
      "[Epoch: 10680] train loss: 0.5310, train acc: 0.8282, val loss: 0.4073, val acc: 0.8981  (best train acc: 0.8395, best val acc: 0.9066)\n",
      "[Epoch: 10700] train loss: 0.5334, train acc: 0.8345, val loss: 0.3971, val acc: 0.9019  (best train acc: 0.8395, best val acc: 0.9066)\n",
      "[Epoch: 10720] train loss: 0.5278, train acc: 0.8381, val loss: 0.3893, val acc: 0.8995  (best train acc: 0.8409, best val acc: 0.9066)\n",
      "[Epoch: 10740] train loss: 0.5273, train acc: 0.8330, val loss: 0.3947, val acc: 0.8988  (best train acc: 0.8409, best val acc: 0.9066)\n",
      "[Epoch: 10760] train loss: 0.5138, train acc: 0.8349, val loss: 0.3935, val acc: 0.9029  (best train acc: 0.8409, best val acc: 0.9066)\n",
      "[Epoch: 10780] train loss: 0.5206, train acc: 0.8328, val loss: 0.4351, val acc: 0.8941  (best train acc: 0.8409, best val acc: 0.9066)\n",
      "[Epoch: 10800] train loss: 0.5265, train acc: 0.8360, val loss: 0.3996, val acc: 0.9059  (best train acc: 0.8409, best val acc: 0.9066)\n",
      "[Epoch: 10820] train loss: 0.5253, train acc: 0.8324, val loss: 0.3967, val acc: 0.9039  (best train acc: 0.8409, best val acc: 0.9066)\n",
      "[Epoch: 10840] train loss: 0.5199, train acc: 0.8363, val loss: 0.3898, val acc: 0.9042  (best train acc: 0.8409, best val acc: 0.9066)\n",
      "[Epoch: 10860] train loss: 0.5132, train acc: 0.8403, val loss: 0.3863, val acc: 0.9032  (best train acc: 0.8414, best val acc: 0.9066)\n",
      "[Epoch: 10880] train loss: 0.5306, train acc: 0.8300, val loss: 0.4043, val acc: 0.9029  (best train acc: 0.8414, best val acc: 0.9066)\n",
      "[Epoch: 10900] train loss: 0.5151, train acc: 0.8345, val loss: 0.3916, val acc: 0.9015  (best train acc: 0.8414, best val acc: 0.9066)\n",
      "[Epoch: 10920] train loss: 0.5219, train acc: 0.8386, val loss: 0.3982, val acc: 0.8971  (best train acc: 0.8414, best val acc: 0.9066)\n",
      "[Epoch: 10940] train loss: 0.5391, train acc: 0.8277, val loss: 0.4017, val acc: 0.8948  (best train acc: 0.8417, best val acc: 0.9066)\n",
      "[Epoch: 10960] train loss: 0.5240, train acc: 0.8393, val loss: 0.3917, val acc: 0.8985  (best train acc: 0.8417, best val acc: 0.9066)\n",
      "[Epoch: 10980] train loss: 0.5140, train acc: 0.8375, val loss: 0.3922, val acc: 0.8995  (best train acc: 0.8417, best val acc: 0.9066)\n",
      "[Epoch: 11000] train loss: 0.5226, train acc: 0.8358, val loss: 0.3905, val acc: 0.9032  (best train acc: 0.8417, best val acc: 0.9066)\n",
      "[Epoch: 11020] train loss: 0.5242, train acc: 0.8343, val loss: 0.3876, val acc: 0.9025  (best train acc: 0.8417, best val acc: 0.9066)\n",
      "[Epoch: 11040] train loss: 0.5161, train acc: 0.8375, val loss: 0.3899, val acc: 0.9032  (best train acc: 0.8417, best val acc: 0.9066)\n",
      "[Epoch: 11060] train loss: 0.5238, train acc: 0.8279, val loss: 0.3958, val acc: 0.8961  (best train acc: 0.8417, best val acc: 0.9066)\n",
      "[Epoch: 11080] train loss: 0.5172, train acc: 0.8392, val loss: 0.3866, val acc: 0.9008  (best train acc: 0.8417, best val acc: 0.9066)\n",
      "[Epoch: 11100] train loss: 0.5370, train acc: 0.8345, val loss: 0.3982, val acc: 0.8958  (best train acc: 0.8417, best val acc: 0.9066)\n",
      "[Epoch: 11120] train loss: 0.5262, train acc: 0.8334, val loss: 0.4092, val acc: 0.8954  (best train acc: 0.8417, best val acc: 0.9066)\n",
      "[Epoch: 11140] train loss: 0.5294, train acc: 0.8332, val loss: 0.4149, val acc: 0.8988  (best train acc: 0.8422, best val acc: 0.9066)\n",
      "[Epoch: 11160] train loss: 0.5282, train acc: 0.8336, val loss: 0.3901, val acc: 0.9015  (best train acc: 0.8422, best val acc: 0.9066)\n",
      "[Epoch: 11180] train loss: 0.5163, train acc: 0.8360, val loss: 0.3866, val acc: 0.9042  (best train acc: 0.8422, best val acc: 0.9066)\n",
      "[Epoch: 11200] train loss: 0.5174, train acc: 0.8343, val loss: 0.3869, val acc: 0.9005  (best train acc: 0.8422, best val acc: 0.9066)\n",
      "[Epoch: 11220] train loss: 0.5090, train acc: 0.8388, val loss: 0.3790, val acc: 0.9015  (best train acc: 0.8423, best val acc: 0.9066)\n",
      "[Epoch: 11240] train loss: 0.5236, train acc: 0.8282, val loss: 0.3984, val acc: 0.8975  (best train acc: 0.8423, best val acc: 0.9066)\n",
      "[Epoch: 11260] train loss: 0.5152, train acc: 0.8364, val loss: 0.3787, val acc: 0.9025  (best train acc: 0.8428, best val acc: 0.9066)\n",
      "[Epoch: 11280] train loss: 0.5100, train acc: 0.8391, val loss: 0.3848, val acc: 0.8998  (best train acc: 0.8428, best val acc: 0.9066)\n",
      "[Epoch: 11300] train loss: 0.5146, train acc: 0.8392, val loss: 0.3972, val acc: 0.9022  (best train acc: 0.8428, best val acc: 0.9066)\n",
      "[Epoch: 11320] train loss: 0.5287, train acc: 0.8268, val loss: 0.4001, val acc: 0.9002  (best train acc: 0.8428, best val acc: 0.9066)\n",
      "[Epoch: 11340] train loss: 0.5239, train acc: 0.8328, val loss: 0.3950, val acc: 0.9032  (best train acc: 0.8428, best val acc: 0.9066)\n",
      "[Epoch: 11360] train loss: 0.5215, train acc: 0.8330, val loss: 0.3891, val acc: 0.9012  (best train acc: 0.8428, best val acc: 0.9066)\n",
      "[Epoch: 11380] train loss: 0.5221, train acc: 0.8302, val loss: 0.3914, val acc: 0.8998  (best train acc: 0.8428, best val acc: 0.9066)\n",
      "[Epoch: 11400] train loss: 0.5081, train acc: 0.8370, val loss: 0.3838, val acc: 0.8992  (best train acc: 0.8428, best val acc: 0.9066)\n",
      "[Epoch: 11420] train loss: 0.5408, train acc: 0.8167, val loss: 0.4048, val acc: 0.8884  (best train acc: 0.8446, best val acc: 0.9066)\n",
      "[Epoch: 11440] train loss: 0.5171, train acc: 0.8318, val loss: 0.3808, val acc: 0.9042  (best train acc: 0.8446, best val acc: 0.9066)\n",
      "[Epoch: 11460] train loss: 0.5235, train acc: 0.8266, val loss: 0.3870, val acc: 0.8971  (best train acc: 0.8446, best val acc: 0.9066)\n",
      "[Epoch: 11480] train loss: 0.5190, train acc: 0.8344, val loss: 0.3963, val acc: 0.9025  (best train acc: 0.8446, best val acc: 0.9066)\n",
      "[Epoch: 11500] train loss: 0.5175, train acc: 0.8412, val loss: 0.3813, val acc: 0.8954  (best train acc: 0.8446, best val acc: 0.9066)\n",
      "[Epoch: 11520] train loss: 0.4936, train acc: 0.8409, val loss: 0.3711, val acc: 0.8958  (best train acc: 0.8481, best val acc: 0.9066)\n",
      "[Epoch: 11540] train loss: 0.4915, train acc: 0.8441, val loss: 0.3711, val acc: 0.9022  (best train acc: 0.8481, best val acc: 0.9066)\n",
      "[Epoch: 11560] train loss: 0.4822, train acc: 0.8425, val loss: 0.3718, val acc: 0.9049  (best train acc: 0.8481, best val acc: 0.9066)\n",
      "[Epoch: 11580] train loss: 0.4952, train acc: 0.8320, val loss: 0.4332, val acc: 0.8739  (best train acc: 0.8511, best val acc: 0.9066)\n",
      "[Epoch: 11600] train loss: 0.4733, train acc: 0.8447, val loss: 0.3742, val acc: 0.9019  (best train acc: 0.8511, best val acc: 0.9066)\n",
      "[Epoch: 11620] train loss: 0.4757, train acc: 0.8425, val loss: 0.3740, val acc: 0.9046  (best train acc: 0.8515, best val acc: 0.9066)\n",
      "[Epoch: 11640] train loss: 0.5061, train acc: 0.8433, val loss: 0.3797, val acc: 0.8877  (best train acc: 0.8515, best val acc: 0.9073)\n",
      "[Epoch: 11660] train loss: 0.4867, train acc: 0.8458, val loss: 0.3690, val acc: 0.9022  (best train acc: 0.8515, best val acc: 0.9079)\n",
      "[Epoch: 11680] train loss: 0.4863, train acc: 0.8417, val loss: 0.3637, val acc: 0.9025  (best train acc: 0.8545, best val acc: 0.9079)\n",
      "[Epoch: 11700] train loss: 0.4709, train acc: 0.8456, val loss: 0.3542, val acc: 0.9035  (best train acc: 0.8545, best val acc: 0.9079)\n",
      "[Epoch: 11720] train loss: 0.4907, train acc: 0.8419, val loss: 0.3561, val acc: 0.9029  (best train acc: 0.8554, best val acc: 0.9079)\n",
      "[Epoch: 11740] train loss: 0.4815, train acc: 0.8446, val loss: 0.3575, val acc: 0.9049  (best train acc: 0.8554, best val acc: 0.9079)\n",
      "[Epoch: 11760] train loss: 0.4675, train acc: 0.8478, val loss: 0.3746, val acc: 0.9039  (best train acc: 0.8554, best val acc: 0.9079)\n",
      "[Epoch: 11780] train loss: 0.4612, train acc: 0.8551, val loss: 0.3634, val acc: 0.9052  (best train acc: 0.8554, best val acc: 0.9079)\n",
      "[Epoch: 11800] train loss: 0.4658, train acc: 0.8496, val loss: 0.3626, val acc: 0.9049  (best train acc: 0.8555, best val acc: 0.9079)\n",
      "[Epoch: 11820] train loss: 0.4543, train acc: 0.8540, val loss: 0.3591, val acc: 0.9035  (best train acc: 0.8574, best val acc: 0.9079)\n",
      "[Epoch: 11840] train loss: 0.4651, train acc: 0.8517, val loss: 0.3544, val acc: 0.9046  (best train acc: 0.8574, best val acc: 0.9079)\n",
      "[Epoch: 11860] train loss: 0.4621, train acc: 0.8514, val loss: 0.3701, val acc: 0.9039  (best train acc: 0.8605, best val acc: 0.9079)\n",
      "[Epoch: 11880] train loss: 0.4703, train acc: 0.8458, val loss: 0.3664, val acc: 0.9079  (best train acc: 0.8605, best val acc: 0.9079)\n",
      "[Epoch: 11900] train loss: 0.4633, train acc: 0.8514, val loss: 0.3764, val acc: 0.9039  (best train acc: 0.8605, best val acc: 0.9079)\n",
      "[Epoch: 11920] train loss: 0.4754, train acc: 0.8521, val loss: 0.3540, val acc: 0.9059  (best train acc: 0.8605, best val acc: 0.9079)\n",
      "[Epoch: 11940] train loss: 0.4545, train acc: 0.8545, val loss: 0.3538, val acc: 0.9056  (best train acc: 0.8605, best val acc: 0.9079)\n",
      "[Epoch: 11960] train loss: 0.4556, train acc: 0.8535, val loss: 0.3615, val acc: 0.9039  (best train acc: 0.8605, best val acc: 0.9079)\n",
      "[Epoch: 11980] train loss: 0.4663, train acc: 0.8540, val loss: 0.3558, val acc: 0.8988  (best train acc: 0.8605, best val acc: 0.9079)\n",
      "[Epoch: 12000] train loss: 0.4561, train acc: 0.8522, val loss: 0.3555, val acc: 0.9029  (best train acc: 0.8605, best val acc: 0.9079)\n",
      "[Epoch: 12020] train loss: 0.4559, train acc: 0.8519, val loss: 0.3580, val acc: 0.9002  (best train acc: 0.8605, best val acc: 0.9079)\n",
      "[Epoch: 12040] train loss: 0.4676, train acc: 0.8475, val loss: 0.3535, val acc: 0.9056  (best train acc: 0.8605, best val acc: 0.9079)\n",
      "[Epoch: 12060] train loss: 0.4583, train acc: 0.8543, val loss: 0.3507, val acc: 0.9062  (best train acc: 0.8605, best val acc: 0.9079)\n",
      "[Epoch: 12080] train loss: 0.4580, train acc: 0.8498, val loss: 0.3892, val acc: 0.8971  (best train acc: 0.8605, best val acc: 0.9079)\n",
      "[Epoch: 12100] train loss: 0.4694, train acc: 0.8510, val loss: 0.3535, val acc: 0.9002  (best train acc: 0.8605, best val acc: 0.9079)\n",
      "[Epoch: 12120] train loss: 0.4574, train acc: 0.8550, val loss: 0.3476, val acc: 0.9042  (best train acc: 0.8605, best val acc: 0.9079)\n",
      "[Epoch: 12140] train loss: 0.4563, train acc: 0.8532, val loss: 0.3503, val acc: 0.9015  (best train acc: 0.8605, best val acc: 0.9079)\n",
      "[Epoch: 12160] train loss: 0.4662, train acc: 0.8541, val loss: 0.3566, val acc: 0.9035  (best train acc: 0.8610, best val acc: 0.9079)\n",
      "[Epoch: 12180] train loss: 0.4586, train acc: 0.8535, val loss: 0.3450, val acc: 0.9032  (best train acc: 0.8646, best val acc: 0.9079)\n",
      "[Epoch: 12200] train loss: 0.4532, train acc: 0.8521, val loss: 0.3409, val acc: 0.9035  (best train acc: 0.8646, best val acc: 0.9079)\n",
      "[Epoch: 12220] train loss: 0.4541, train acc: 0.8562, val loss: 0.3485, val acc: 0.8992  (best train acc: 0.8646, best val acc: 0.9079)\n",
      "[Epoch: 12240] train loss: 0.4708, train acc: 0.8555, val loss: 0.3463, val acc: 0.8968  (best train acc: 0.8646, best val acc: 0.9079)\n",
      "[Epoch: 12260] train loss: 0.4544, train acc: 0.8491, val loss: 0.3539, val acc: 0.8998  (best train acc: 0.8646, best val acc: 0.9079)\n",
      "[Epoch: 12280] train loss: 0.4577, train acc: 0.8540, val loss: 0.3497, val acc: 0.9015  (best train acc: 0.8646, best val acc: 0.9079)\n",
      "[Epoch: 12300] train loss: 0.4651, train acc: 0.8525, val loss: 0.3487, val acc: 0.9019  (best train acc: 0.8646, best val acc: 0.9079)\n",
      "[Epoch: 12320] train loss: 0.4681, train acc: 0.8488, val loss: 0.3434, val acc: 0.9015  (best train acc: 0.8646, best val acc: 0.9079)\n",
      "[Epoch: 12340] train loss: 0.4672, train acc: 0.8569, val loss: 0.3491, val acc: 0.8988  (best train acc: 0.8646, best val acc: 0.9079)\n",
      "[Epoch: 12360] train loss: 0.4563, train acc: 0.8544, val loss: 0.3483, val acc: 0.9032  (best train acc: 0.8646, best val acc: 0.9079)\n",
      "[Epoch: 12380] train loss: 0.4615, train acc: 0.8532, val loss: 0.3569, val acc: 0.9049  (best train acc: 0.8646, best val acc: 0.9079)\n",
      "[Epoch: 12400] train loss: 0.4608, train acc: 0.8580, val loss: 0.3624, val acc: 0.9025  (best train acc: 0.8646, best val acc: 0.9079)\n",
      "[Epoch: 12420] train loss: 0.4515, train acc: 0.8583, val loss: 0.3501, val acc: 0.9002  (best train acc: 0.8646, best val acc: 0.9079)\n",
      "[Epoch: 12440] train loss: 0.4533, train acc: 0.8522, val loss: 0.3714, val acc: 0.9022  (best train acc: 0.8646, best val acc: 0.9079)\n",
      "[Epoch: 12460] train loss: 0.4568, train acc: 0.8532, val loss: 0.3494, val acc: 0.9019  (best train acc: 0.8646, best val acc: 0.9079)\n",
      "[Epoch: 12480] train loss: 0.4528, train acc: 0.8553, val loss: 0.3669, val acc: 0.9015  (best train acc: 0.8646, best val acc: 0.9079)\n",
      "[Epoch: 12500] train loss: 0.4594, train acc: 0.8515, val loss: 0.3687, val acc: 0.9046  (best train acc: 0.8646, best val acc: 0.9079)\n",
      "[Epoch: 12520] train loss: 0.4538, train acc: 0.8594, val loss: 0.3486, val acc: 0.9012  (best train acc: 0.8646, best val acc: 0.9079)\n",
      "[Epoch: 12540] train loss: 0.4563, train acc: 0.8574, val loss: 0.3628, val acc: 0.9029  (best train acc: 0.8646, best val acc: 0.9079)\n",
      "[Epoch: 12560] train loss: 0.4572, train acc: 0.8564, val loss: 0.3445, val acc: 0.9025  (best train acc: 0.8646, best val acc: 0.9079)\n",
      "[Epoch: 12580] train loss: 0.4610, train acc: 0.8485, val loss: 0.3542, val acc: 0.9039  (best train acc: 0.8646, best val acc: 0.9079)\n",
      "[Epoch: 12600] train loss: 0.4596, train acc: 0.8529, val loss: 0.3566, val acc: 0.9008  (best train acc: 0.8646, best val acc: 0.9079)\n",
      "[Epoch: 12620] train loss: 0.4643, train acc: 0.8567, val loss: 0.3623, val acc: 0.8992  (best train acc: 0.8648, best val acc: 0.9079)\n",
      "[Epoch: 12640] train loss: 0.4626, train acc: 0.8538, val loss: 0.3658, val acc: 0.9012  (best train acc: 0.8648, best val acc: 0.9079)\n",
      "[Epoch: 12660] train loss: 0.4553, train acc: 0.8577, val loss: 0.3540, val acc: 0.8981  (best train acc: 0.8648, best val acc: 0.9079)\n",
      "[Epoch: 12680] train loss: 0.4528, train acc: 0.8625, val loss: 0.3509, val acc: 0.9019  (best train acc: 0.8648, best val acc: 0.9079)\n",
      "[Epoch: 12700] train loss: 0.4627, train acc: 0.8522, val loss: 0.3642, val acc: 0.9046  (best train acc: 0.8648, best val acc: 0.9079)\n",
      "[Epoch: 12720] train loss: 0.4748, train acc: 0.8449, val loss: 0.3529, val acc: 0.9029  (best train acc: 0.8648, best val acc: 0.9079)\n",
      "[Epoch: 12740] train loss: 0.4490, train acc: 0.8571, val loss: 0.3458, val acc: 0.9025  (best train acc: 0.8648, best val acc: 0.9079)\n",
      "[Epoch: 12760] train loss: 0.4496, train acc: 0.8575, val loss: 0.3582, val acc: 0.9025  (best train acc: 0.8648, best val acc: 0.9079)\n",
      "[Epoch: 12780] train loss: 0.4521, train acc: 0.8553, val loss: 0.3537, val acc: 0.9019  (best train acc: 0.8648, best val acc: 0.9079)\n",
      "[Epoch: 12800] train loss: 0.4543, train acc: 0.8526, val loss: 0.3567, val acc: 0.9019  (best train acc: 0.8648, best val acc: 0.9079)\n",
      "[Epoch: 12820] train loss: 0.4513, train acc: 0.8564, val loss: 0.3572, val acc: 0.9019  (best train acc: 0.8648, best val acc: 0.9079)\n",
      "[Epoch: 12840] train loss: 0.4624, train acc: 0.8494, val loss: 0.3519, val acc: 0.9022  (best train acc: 0.8648, best val acc: 0.9079)\n",
      "[Epoch: 12860] train loss: 0.5058, train acc: 0.8455, val loss: 0.3557, val acc: 0.8887  (best train acc: 0.8648, best val acc: 0.9079)\n",
      "[Epoch: 12880] train loss: 0.4584, train acc: 0.8573, val loss: 0.3543, val acc: 0.9029  (best train acc: 0.8648, best val acc: 0.9079)\n",
      "[Epoch: 12900] train loss: 0.4477, train acc: 0.8578, val loss: 0.3456, val acc: 0.9022  (best train acc: 0.8648, best val acc: 0.9079)\n",
      "[Epoch: 12920] train loss: 0.4479, train acc: 0.8589, val loss: 0.3514, val acc: 0.9042  (best train acc: 0.8648, best val acc: 0.9079)\n",
      "[Epoch: 12940] train loss: 0.4462, train acc: 0.8610, val loss: 0.3425, val acc: 0.9025  (best train acc: 0.8648, best val acc: 0.9079)\n",
      "[Epoch: 12960] train loss: 0.4385, train acc: 0.8610, val loss: 0.3460, val acc: 0.9022  (best train acc: 0.8648, best val acc: 0.9079)\n",
      "[Epoch: 12980] train loss: 0.4473, train acc: 0.8595, val loss: 0.3466, val acc: 0.8992  (best train acc: 0.8648, best val acc: 0.9079)\n",
      "[Epoch: 13000] train loss: 0.4661, train acc: 0.8475, val loss: 0.3546, val acc: 0.8958  (best train acc: 0.8648, best val acc: 0.9079)\n",
      "[Epoch: 13020] train loss: 0.4411, train acc: 0.8589, val loss: 0.3463, val acc: 0.9008  (best train acc: 0.8648, best val acc: 0.9079)\n",
      "[Epoch: 13040] train loss: 0.4430, train acc: 0.8590, val loss: 0.3429, val acc: 0.9025  (best train acc: 0.8648, best val acc: 0.9079)\n",
      "[Epoch: 13060] train loss: 0.4589, train acc: 0.8559, val loss: 0.3474, val acc: 0.9039  (best train acc: 0.8648, best val acc: 0.9079)\n",
      "[Epoch: 13080] train loss: 0.4607, train acc: 0.8545, val loss: 0.3455, val acc: 0.9049  (best train acc: 0.8648, best val acc: 0.9079)\n",
      "[Epoch: 13100] train loss: 0.4551, train acc: 0.8546, val loss: 0.3717, val acc: 0.8948  (best train acc: 0.8648, best val acc: 0.9079)\n",
      "[Epoch: 13120] train loss: 0.4500, train acc: 0.8570, val loss: 0.3392, val acc: 0.9015  (best train acc: 0.8648, best val acc: 0.9079)\n",
      "[Epoch: 13140] train loss: 0.4459, train acc: 0.8556, val loss: 0.3502, val acc: 0.9062  (best train acc: 0.8648, best val acc: 0.9079)\n",
      "[Epoch: 13160] train loss: 0.4424, train acc: 0.8561, val loss: 0.3358, val acc: 0.9029  (best train acc: 0.8648, best val acc: 0.9079)\n",
      "[Epoch: 13180] train loss: 0.4480, train acc: 0.8588, val loss: 0.3518, val acc: 0.9019  (best train acc: 0.8648, best val acc: 0.9079)\n",
      "[Epoch: 13200] train loss: 0.4511, train acc: 0.8577, val loss: 0.3423, val acc: 0.9025  (best train acc: 0.8648, best val acc: 0.9079)\n",
      "[Epoch: 13220] train loss: 0.4536, train acc: 0.8527, val loss: 0.3665, val acc: 0.8998  (best train acc: 0.8648, best val acc: 0.9079)\n",
      "[Epoch: 13240] train loss: 0.4412, train acc: 0.8597, val loss: 0.3469, val acc: 0.9022  (best train acc: 0.8648, best val acc: 0.9079)\n",
      "[Epoch: 13260] train loss: 0.4424, train acc: 0.8617, val loss: 0.3405, val acc: 0.9035  (best train acc: 0.8648, best val acc: 0.9079)\n",
      "[Epoch: 13280] train loss: 0.4564, train acc: 0.8574, val loss: 0.3538, val acc: 0.9035  (best train acc: 0.8654, best val acc: 0.9079)\n",
      "[Epoch: 13300] train loss: 0.4614, train acc: 0.8475, val loss: 0.3665, val acc: 0.9002  (best train acc: 0.8654, best val acc: 0.9079)\n",
      "[Epoch: 13320] train loss: 0.4532, train acc: 0.8588, val loss: 0.3457, val acc: 0.8992  (best train acc: 0.8654, best val acc: 0.9079)\n",
      "[Epoch: 13340] train loss: 0.4471, train acc: 0.8642, val loss: 0.3418, val acc: 0.9002  (best train acc: 0.8654, best val acc: 0.9079)\n",
      "[Epoch: 13360] train loss: 0.4409, train acc: 0.8606, val loss: 0.3559, val acc: 0.9029  (best train acc: 0.8654, best val acc: 0.9079)\n",
      "[Epoch: 13380] train loss: 0.4494, train acc: 0.8610, val loss: 0.3391, val acc: 0.9042  (best train acc: 0.8654, best val acc: 0.9079)\n",
      "[Epoch: 13400] train loss: 0.4465, train acc: 0.8600, val loss: 0.3437, val acc: 0.9015  (best train acc: 0.8654, best val acc: 0.9079)\n",
      "[Epoch: 13420] train loss: 0.4446, train acc: 0.8616, val loss: 0.3451, val acc: 0.9046  (best train acc: 0.8654, best val acc: 0.9079)\n",
      "[Epoch: 13440] train loss: 0.4753, train acc: 0.8510, val loss: 0.3359, val acc: 0.9025  (best train acc: 0.8654, best val acc: 0.9079)\n",
      "[Epoch: 13460] train loss: 0.4570, train acc: 0.8540, val loss: 0.3693, val acc: 0.8992  (best train acc: 0.8654, best val acc: 0.9079)\n",
      "[Epoch: 13480] train loss: 0.4477, train acc: 0.8622, val loss: 0.3408, val acc: 0.9032  (best train acc: 0.8654, best val acc: 0.9079)\n",
      "[Epoch: 13500] train loss: 0.4499, train acc: 0.8572, val loss: 0.3427, val acc: 0.9039  (best train acc: 0.8654, best val acc: 0.9079)\n",
      "[Epoch: 13520] train loss: 0.4478, train acc: 0.8560, val loss: 0.3549, val acc: 0.9035  (best train acc: 0.8654, best val acc: 0.9079)\n",
      "[Epoch: 13540] train loss: 0.4537, train acc: 0.8565, val loss: 0.3498, val acc: 0.9008  (best train acc: 0.8654, best val acc: 0.9079)\n",
      "[Epoch: 13560] train loss: 0.4427, train acc: 0.8599, val loss: 0.3462, val acc: 0.9046  (best train acc: 0.8654, best val acc: 0.9079)\n",
      "[Epoch: 13580] train loss: 0.4472, train acc: 0.8601, val loss: 0.3423, val acc: 0.9022  (best train acc: 0.8654, best val acc: 0.9079)\n",
      "[Epoch: 13600] train loss: 0.4437, train acc: 0.8592, val loss: 0.3427, val acc: 0.9019  (best train acc: 0.8657, best val acc: 0.9079)\n",
      "[Epoch: 13620] train loss: 0.4526, train acc: 0.8561, val loss: 0.3480, val acc: 0.9035  (best train acc: 0.8657, best val acc: 0.9079)\n",
      "[Epoch: 13640] train loss: 0.4429, train acc: 0.8642, val loss: 0.3468, val acc: 0.9012  (best train acc: 0.8657, best val acc: 0.9079)\n",
      "[Epoch: 13660] train loss: 0.4364, train acc: 0.8610, val loss: 0.3376, val acc: 0.9025  (best train acc: 0.8657, best val acc: 0.9079)\n",
      "[Epoch: 13680] train loss: 0.4381, train acc: 0.8621, val loss: 0.3405, val acc: 0.9039  (best train acc: 0.8657, best val acc: 0.9079)\n",
      "[Epoch: 13700] train loss: 0.4385, train acc: 0.8603, val loss: 0.3375, val acc: 0.9029  (best train acc: 0.8657, best val acc: 0.9079)\n",
      "[Epoch: 13720] train loss: 0.4414, train acc: 0.8600, val loss: 0.3451, val acc: 0.8978  (best train acc: 0.8657, best val acc: 0.9079)\n",
      "[Epoch: 13740] train loss: 0.4368, train acc: 0.8631, val loss: 0.3413, val acc: 0.9015  (best train acc: 0.8657, best val acc: 0.9079)\n",
      "[Epoch: 13760] train loss: 0.4698, train acc: 0.8431, val loss: 0.3481, val acc: 0.9039  (best train acc: 0.8657, best val acc: 0.9079)\n",
      "[Epoch: 13780] train loss: 0.4491, train acc: 0.8572, val loss: 0.3508, val acc: 0.8968  (best train acc: 0.8657, best val acc: 0.9079)\n",
      "[Epoch: 13800] train loss: 0.4450, train acc: 0.8595, val loss: 0.3365, val acc: 0.9032  (best train acc: 0.8657, best val acc: 0.9079)\n",
      "[Epoch: 13820] train loss: 0.4374, train acc: 0.8615, val loss: 0.3452, val acc: 0.9049  (best train acc: 0.8663, best val acc: 0.9079)\n",
      "[Epoch: 13840] train loss: 0.4521, train acc: 0.8534, val loss: 0.3690, val acc: 0.8988  (best train acc: 0.8663, best val acc: 0.9079)\n",
      "[Epoch: 13860] train loss: 0.4436, train acc: 0.8552, val loss: 0.3408, val acc: 0.9052  (best train acc: 0.8663, best val acc: 0.9079)\n",
      "[Epoch: 13880] train loss: 0.4452, train acc: 0.8587, val loss: 0.3479, val acc: 0.8978  (best train acc: 0.8680, best val acc: 0.9079)\n",
      "[Epoch: 13900] train loss: 0.4424, train acc: 0.8596, val loss: 0.3447, val acc: 0.9029  (best train acc: 0.8680, best val acc: 0.9079)\n",
      "[Epoch: 13920] train loss: 0.4585, train acc: 0.8472, val loss: 0.3663, val acc: 0.8948  (best train acc: 0.8680, best val acc: 0.9079)\n",
      "[Epoch: 13940] train loss: 0.4443, train acc: 0.8594, val loss: 0.3468, val acc: 0.8988  (best train acc: 0.8680, best val acc: 0.9079)\n",
      "[Epoch: 13960] train loss: 0.4468, train acc: 0.8589, val loss: 0.3400, val acc: 0.9025  (best train acc: 0.8680, best val acc: 0.9079)\n",
      "[Epoch: 13980] train loss: 0.4458, train acc: 0.8630, val loss: 0.3475, val acc: 0.9029  (best train acc: 0.8680, best val acc: 0.9079)\n",
      "[Epoch: 14000] train loss: 0.4314, train acc: 0.8584, val loss: 0.3564, val acc: 0.9025  (best train acc: 0.8680, best val acc: 0.9079)\n",
      "[Epoch: 14020] train loss: 0.4362, train acc: 0.8636, val loss: 0.3455, val acc: 0.9039  (best train acc: 0.8680, best val acc: 0.9079)\n",
      "[Epoch: 14040] train loss: 0.4393, train acc: 0.8567, val loss: 0.3504, val acc: 0.9046  (best train acc: 0.8680, best val acc: 0.9079)\n",
      "[Epoch: 14060] train loss: 0.4344, train acc: 0.8586, val loss: 0.3381, val acc: 0.9012  (best train acc: 0.8680, best val acc: 0.9079)\n",
      "[Epoch: 14080] train loss: 0.4416, train acc: 0.8591, val loss: 0.3524, val acc: 0.9025  (best train acc: 0.8680, best val acc: 0.9079)\n",
      "[Epoch: 14100] train loss: 0.4402, train acc: 0.8596, val loss: 0.3464, val acc: 0.8998  (best train acc: 0.8680, best val acc: 0.9079)\n",
      "[Epoch: 14120] train loss: 0.4608, train acc: 0.8528, val loss: 0.3520, val acc: 0.9008  (best train acc: 0.8680, best val acc: 0.9079)\n",
      "[Epoch: 14140] train loss: 0.4491, train acc: 0.8594, val loss: 0.3769, val acc: 0.9015  (best train acc: 0.8680, best val acc: 0.9079)\n",
      "[Epoch: 14160] train loss: 0.4320, train acc: 0.8624, val loss: 0.3297, val acc: 0.9046  (best train acc: 0.8680, best val acc: 0.9079)\n",
      "[Epoch: 14180] train loss: 0.4261, train acc: 0.8571, val loss: 0.3179, val acc: 0.8961  (best train acc: 0.8680, best val acc: 0.9079)\n",
      "[Epoch: 14200] train loss: 0.4133, train acc: 0.8480, val loss: 0.3111, val acc: 0.9022  (best train acc: 0.8680, best val acc: 0.9079)\n",
      "[Epoch: 14220] train loss: 0.4584, train acc: 0.8447, val loss: 0.3166, val acc: 0.8934  (best train acc: 0.8680, best val acc: 0.9079)\n",
      "[Epoch: 14240] train loss: 0.4538, train acc: 0.8445, val loss: 0.3157, val acc: 0.9042  (best train acc: 0.8680, best val acc: 0.9079)\n",
      "[Epoch: 14260] train loss: 0.3996, train acc: 0.8502, val loss: 0.3349, val acc: 0.8745  (best train acc: 0.8680, best val acc: 0.9079)\n",
      "[Epoch: 14280] train loss: 0.4114, train acc: 0.8422, val loss: 0.3116, val acc: 0.8968  (best train acc: 0.8680, best val acc: 0.9079)\n",
      "[Epoch: 14300] train loss: 0.3987, train acc: 0.8451, val loss: 0.3204, val acc: 0.9052  (best train acc: 0.8680, best val acc: 0.9079)\n",
      "[Epoch: 14320] train loss: 0.4262, train acc: 0.8549, val loss: 0.3229, val acc: 0.8934  (best train acc: 0.8680, best val acc: 0.9079)\n",
      "[Epoch: 14340] train loss: 0.4161, train acc: 0.8587, val loss: 0.3038, val acc: 0.9025  (best train acc: 0.8680, best val acc: 0.9079)\n",
      "[Epoch: 14360] train loss: 0.4356, train acc: 0.8540, val loss: 0.3038, val acc: 0.8934  (best train acc: 0.8680, best val acc: 0.9079)\n",
      "[Epoch: 14380] train loss: 0.4132, train acc: 0.8534, val loss: 0.3098, val acc: 0.9049  (best train acc: 0.8680, best val acc: 0.9079)\n",
      "[Epoch: 14400] train loss: 0.4287, train acc: 0.8337, val loss: 0.3197, val acc: 0.8965  (best train acc: 0.8680, best val acc: 0.9079)\n",
      "[Epoch: 14420] train loss: 0.3810, train acc: 0.8603, val loss: 0.3032, val acc: 0.9019  (best train acc: 0.8680, best val acc: 0.9079)\n",
      "[Epoch: 14440] train loss: 0.3987, train acc: 0.8511, val loss: 0.3081, val acc: 0.9032  (best train acc: 0.8680, best val acc: 0.9079)\n",
      "[Epoch: 14460] train loss: 0.3974, train acc: 0.8520, val loss: 0.3046, val acc: 0.8992  (best train acc: 0.8680, best val acc: 0.9079)\n",
      "[Epoch: 14480] train loss: 0.4005, train acc: 0.8609, val loss: 0.3000, val acc: 0.8958  (best train acc: 0.8680, best val acc: 0.9079)\n",
      "[Epoch: 14500] train loss: 0.4118, train acc: 0.8545, val loss: 0.3196, val acc: 0.9019  (best train acc: 0.8680, best val acc: 0.9079)\n",
      "[Epoch: 14520] train loss: 0.4266, train acc: 0.8499, val loss: 0.3012, val acc: 0.8992  (best train acc: 0.8680, best val acc: 0.9079)\n",
      "[Epoch: 14540] train loss: 0.4223, train acc: 0.8410, val loss: 0.3222, val acc: 0.8921  (best train acc: 0.8680, best val acc: 0.9079)\n",
      "[Epoch: 14560] train loss: 0.4138, train acc: 0.8545, val loss: 0.3043, val acc: 0.8931  (best train acc: 0.8680, best val acc: 0.9079)\n",
      "[Epoch: 14580] train loss: 0.4215, train acc: 0.8420, val loss: 0.3059, val acc: 0.9005  (best train acc: 0.8680, best val acc: 0.9079)\n",
      "[Epoch: 14600] train loss: 0.3784, train acc: 0.8540, val loss: 0.3058, val acc: 0.9062  (best train acc: 0.8680, best val acc: 0.9079)\n",
      "[Epoch: 14620] train loss: 0.4185, train acc: 0.8509, val loss: 0.3101, val acc: 0.9035  (best train acc: 0.8680, best val acc: 0.9079)\n",
      "[Epoch: 14640] train loss: 0.3974, train acc: 0.8480, val loss: 0.3117, val acc: 0.8971  (best train acc: 0.8680, best val acc: 0.9079)\n",
      "[Epoch: 14660] train loss: 0.3885, train acc: 0.8524, val loss: 0.2894, val acc: 0.9012  (best train acc: 0.8680, best val acc: 0.9079)\n",
      "[Epoch: 14680] train loss: 0.3784, train acc: 0.8571, val loss: 0.3044, val acc: 0.9039  (best train acc: 0.8680, best val acc: 0.9079)\n",
      "[Epoch: 14700] train loss: 0.4081, train acc: 0.8459, val loss: 0.3028, val acc: 0.9035  (best train acc: 0.8680, best val acc: 0.9079)\n",
      "[Epoch: 14720] train loss: 0.4086, train acc: 0.8556, val loss: 0.3044, val acc: 0.8978  (best train acc: 0.8680, best val acc: 0.9079)\n",
      "[Epoch: 14740] train loss: 0.4150, train acc: 0.8447, val loss: 0.3052, val acc: 0.8965  (best train acc: 0.8680, best val acc: 0.9079)\n",
      "[Epoch: 14760] train loss: 0.3760, train acc: 0.8556, val loss: 0.3092, val acc: 0.8978  (best train acc: 0.8680, best val acc: 0.9079)\n",
      "[Epoch: 14780] train loss: 0.4569, train acc: 0.8394, val loss: 0.3062, val acc: 0.8877  (best train acc: 0.8680, best val acc: 0.9079)\n",
      "[Epoch: 14800] train loss: 0.3857, train acc: 0.8547, val loss: 0.3079, val acc: 0.9032  (best train acc: 0.8680, best val acc: 0.9079)\n",
      "[Epoch: 14820] train loss: 0.4304, train acc: 0.8548, val loss: 0.3079, val acc: 0.9042  (best train acc: 0.8680, best val acc: 0.9079)\n",
      "[Epoch: 14840] train loss: 0.4020, train acc: 0.8545, val loss: 0.3131, val acc: 0.9052  (best train acc: 0.8680, best val acc: 0.9079)\n",
      "[Epoch: 14860] train loss: 0.4082, train acc: 0.8483, val loss: 0.3037, val acc: 0.8951  (best train acc: 0.8680, best val acc: 0.9079)\n",
      "[Epoch: 14880] train loss: 0.4158, train acc: 0.8508, val loss: 0.2987, val acc: 0.9032  (best train acc: 0.8680, best val acc: 0.9079)\n",
      "[Epoch: 14900] train loss: 0.4050, train acc: 0.8459, val loss: 0.3276, val acc: 0.8921  (best train acc: 0.8680, best val acc: 0.9079)\n",
      "[Epoch: 14920] train loss: 0.4271, train acc: 0.8612, val loss: 0.3105, val acc: 0.9029  (best train acc: 0.8680, best val acc: 0.9079)\n",
      "[Epoch: 14940] train loss: 0.3936, train acc: 0.8523, val loss: 0.2948, val acc: 0.9059  (best train acc: 0.8680, best val acc: 0.9079)\n",
      "[Epoch: 14960] train loss: 0.4085, train acc: 0.8511, val loss: 0.3042, val acc: 0.9049  (best train acc: 0.8680, best val acc: 0.9079)\n",
      "[Epoch: 14980] train loss: 0.3931, train acc: 0.8527, val loss: 0.3645, val acc: 0.8806  (best train acc: 0.8680, best val acc: 0.9079)\n",
      "[Epoch: 15000] train loss: 0.4224, train acc: 0.8358, val loss: 0.3376, val acc: 0.8948  (best train acc: 0.8680, best val acc: 0.9079)\n",
      "[Epoch: 15020] train loss: 0.4217, train acc: 0.8395, val loss: 0.3045, val acc: 0.9022  (best train acc: 0.8680, best val acc: 0.9079)\n",
      "[Epoch: 15040] train loss: 0.4564, train acc: 0.8415, val loss: 0.3254, val acc: 0.8995  (best train acc: 0.8680, best val acc: 0.9079)\n",
      "[Epoch: 15060] train loss: 0.4183, train acc: 0.8435, val loss: 0.3037, val acc: 0.9012  (best train acc: 0.8680, best val acc: 0.9079)\n",
      "[Epoch: 15080] train loss: 0.4372, train acc: 0.8396, val loss: 0.2980, val acc: 0.8948  (best train acc: 0.8680, best val acc: 0.9079)\n",
      "[Epoch: 15100] train loss: 0.4136, train acc: 0.8494, val loss: 0.3007, val acc: 0.8975  (best train acc: 0.8680, best val acc: 0.9079)\n",
      "[Epoch: 15120] train loss: 0.4133, train acc: 0.8577, val loss: 0.2976, val acc: 0.9059  (best train acc: 0.8680, best val acc: 0.9079)\n",
      "[Epoch: 15140] train loss: 0.3819, train acc: 0.8585, val loss: 0.3055, val acc: 0.9042  (best train acc: 0.8680, best val acc: 0.9079)\n",
      "[Epoch: 15160] train loss: 0.3887, train acc: 0.8485, val loss: 0.3123, val acc: 0.8914  (best train acc: 0.8680, best val acc: 0.9079)\n",
      "[Epoch: 15180] train loss: 0.4353, train acc: 0.8407, val loss: 0.3070, val acc: 0.8978  (best train acc: 0.8680, best val acc: 0.9079)\n",
      "[Epoch: 15200] train loss: 0.3769, train acc: 0.8566, val loss: 0.3123, val acc: 0.9062  (best train acc: 0.8680, best val acc: 0.9079)\n",
      "[Epoch: 15220] train loss: 0.3825, train acc: 0.8531, val loss: 0.2983, val acc: 0.9039  (best train acc: 0.8680, best val acc: 0.9079)\n",
      "[Epoch: 15240] train loss: 0.3835, train acc: 0.8369, val loss: 0.3048, val acc: 0.9049  (best train acc: 0.8680, best val acc: 0.9079)\n",
      "[Epoch: 15260] train loss: 0.3944, train acc: 0.8525, val loss: 0.2969, val acc: 0.9029  (best train acc: 0.8680, best val acc: 0.9079)\n",
      "[Epoch: 15280] train loss: 0.3976, train acc: 0.8530, val loss: 0.3032, val acc: 0.9056  (best train acc: 0.8680, best val acc: 0.9079)\n",
      "[Epoch: 15300] train loss: 0.3925, train acc: 0.8464, val loss: 0.3286, val acc: 0.8897  (best train acc: 0.8680, best val acc: 0.9079)\n",
      "[Epoch: 15320] train loss: 0.4185, train acc: 0.8616, val loss: 0.3111, val acc: 0.8941  (best train acc: 0.8680, best val acc: 0.9079)\n",
      "[Epoch: 15340] train loss: 0.3781, train acc: 0.8551, val loss: 0.3154, val acc: 0.9056  (best train acc: 0.8680, best val acc: 0.9079)\n",
      "[Epoch: 15360] train loss: 0.3812, train acc: 0.8549, val loss: 0.2993, val acc: 0.9049  (best train acc: 0.8680, best val acc: 0.9079)\n",
      "[Epoch: 15380] train loss: 0.3950, train acc: 0.8520, val loss: 0.2958, val acc: 0.9032  (best train acc: 0.8680, best val acc: 0.9079)\n",
      "[Epoch: 15400] train loss: 0.3665, train acc: 0.8545, val loss: 0.3062, val acc: 0.8981  (best train acc: 0.8680, best val acc: 0.9079)\n",
      "[Epoch: 15420] train loss: 0.3854, train acc: 0.8501, val loss: 0.3239, val acc: 0.8907  (best train acc: 0.8680, best val acc: 0.9079)\n",
      "[Epoch: 15440] train loss: 0.3881, train acc: 0.8474, val loss: 0.3046, val acc: 0.8941  (best train acc: 0.8680, best val acc: 0.9083)\n",
      "[Epoch: 15460] train loss: 0.3869, train acc: 0.8494, val loss: 0.3067, val acc: 0.9049  (best train acc: 0.8680, best val acc: 0.9083)\n",
      "[Epoch: 15480] train loss: 0.4427, train acc: 0.8148, val loss: 0.3010, val acc: 0.9025  (best train acc: 0.8680, best val acc: 0.9083)\n",
      "[Epoch: 15500] train loss: 0.3773, train acc: 0.8571, val loss: 0.3012, val acc: 0.9039  (best train acc: 0.8680, best val acc: 0.9083)\n",
      "[Epoch: 15520] train loss: 0.4005, train acc: 0.8519, val loss: 0.3165, val acc: 0.9019  (best train acc: 0.8680, best val acc: 0.9083)\n",
      "[Epoch: 15540] train loss: 0.4152, train acc: 0.8527, val loss: 0.3023, val acc: 0.9059  (best train acc: 0.8680, best val acc: 0.9083)\n",
      "[Epoch: 15560] train loss: 0.3868, train acc: 0.8535, val loss: 0.2949, val acc: 0.9029  (best train acc: 0.8680, best val acc: 0.9083)\n",
      "[Epoch: 15580] train loss: 0.3970, train acc: 0.8537, val loss: 0.3100, val acc: 0.9039  (best train acc: 0.8680, best val acc: 0.9083)\n",
      "[Epoch: 15600] train loss: 0.3880, train acc: 0.8515, val loss: 0.3147, val acc: 0.8985  (best train acc: 0.8680, best val acc: 0.9083)\n",
      "[Epoch: 15620] train loss: 0.3855, train acc: 0.8530, val loss: 0.3123, val acc: 0.8941  (best train acc: 0.8680, best val acc: 0.9083)\n",
      "[Epoch: 15640] train loss: 0.3843, train acc: 0.8552, val loss: 0.2947, val acc: 0.9039  (best train acc: 0.8680, best val acc: 0.9083)\n",
      "[Epoch: 15660] train loss: 0.3775, train acc: 0.8547, val loss: 0.2930, val acc: 0.8992  (best train acc: 0.8680, best val acc: 0.9083)\n",
      "[Epoch: 15680] train loss: 0.3944, train acc: 0.8566, val loss: 0.2954, val acc: 0.9032  (best train acc: 0.8680, best val acc: 0.9083)\n",
      "[Epoch: 15700] train loss: 0.3958, train acc: 0.8516, val loss: 0.2992, val acc: 0.9046  (best train acc: 0.8680, best val acc: 0.9083)\n",
      "[Epoch: 15720] train loss: 0.4071, train acc: 0.8475, val loss: 0.2952, val acc: 0.9052  (best train acc: 0.8680, best val acc: 0.9083)\n",
      "[Epoch: 15740] train loss: 0.3892, train acc: 0.8391, val loss: 0.2991, val acc: 0.9049  (best train acc: 0.8680, best val acc: 0.9083)\n",
      "[Epoch: 15760] train loss: 0.4276, train acc: 0.8367, val loss: 0.2972, val acc: 0.9035  (best train acc: 0.8680, best val acc: 0.9083)\n",
      "[Epoch: 15780] train loss: 0.3868, train acc: 0.8620, val loss: 0.2982, val acc: 0.8988  (best train acc: 0.8680, best val acc: 0.9083)\n",
      "[Epoch: 15800] train loss: 0.3997, train acc: 0.8483, val loss: 0.3137, val acc: 0.9035  (best train acc: 0.8680, best val acc: 0.9083)\n",
      "[Epoch: 15820] train loss: 0.3805, train acc: 0.8519, val loss: 0.3212, val acc: 0.8995  (best train acc: 0.8680, best val acc: 0.9083)\n",
      "[Epoch: 15840] train loss: 0.3790, train acc: 0.8605, val loss: 0.3174, val acc: 0.9025  (best train acc: 0.8680, best val acc: 0.9083)\n",
      "[Epoch: 15860] train loss: 0.3846, train acc: 0.8459, val loss: 0.2949, val acc: 0.9049  (best train acc: 0.8680, best val acc: 0.9083)\n",
      "[Epoch: 15880] train loss: 0.3889, train acc: 0.8545, val loss: 0.3023, val acc: 0.9052  (best train acc: 0.8680, best val acc: 0.9083)\n",
      "[Epoch: 15900] train loss: 0.4159, train acc: 0.8395, val loss: 0.3006, val acc: 0.9008  (best train acc: 0.8680, best val acc: 0.9083)\n",
      "[Epoch: 15920] train loss: 0.3999, train acc: 0.8398, val loss: 0.3002, val acc: 0.8931  (best train acc: 0.8680, best val acc: 0.9083)\n",
      "[Epoch: 15940] train loss: 0.3712, train acc: 0.8565, val loss: 0.3135, val acc: 0.9025  (best train acc: 0.8680, best val acc: 0.9083)\n",
      "[Epoch: 15960] train loss: 0.4166, train acc: 0.8451, val loss: 0.3036, val acc: 0.8998  (best train acc: 0.8680, best val acc: 0.9083)\n",
      "[Epoch: 15980] train loss: 0.3888, train acc: 0.8422, val loss: 0.2913, val acc: 0.8995  (best train acc: 0.8680, best val acc: 0.9083)\n",
      "[Epoch: 16000] train loss: 0.3951, train acc: 0.8537, val loss: 0.3094, val acc: 0.9025  (best train acc: 0.8680, best val acc: 0.9083)\n",
      "[Epoch: 16020] train loss: 0.3959, train acc: 0.8449, val loss: 0.3168, val acc: 0.8911  (best train acc: 0.8680, best val acc: 0.9083)\n",
      "[Epoch: 16040] train loss: 0.3906, train acc: 0.8523, val loss: 0.3141, val acc: 0.9025  (best train acc: 0.8680, best val acc: 0.9083)\n",
      "[Epoch: 16060] train loss: 0.3997, train acc: 0.8510, val loss: 0.3015, val acc: 0.9032  (best train acc: 0.8680, best val acc: 0.9083)\n",
      "[Epoch: 16080] train loss: 0.4110, train acc: 0.8521, val loss: 0.2946, val acc: 0.9056  (best train acc: 0.8680, best val acc: 0.9083)\n",
      "[Epoch: 16100] train loss: 0.4025, train acc: 0.8441, val loss: 0.3047, val acc: 0.9019  (best train acc: 0.8680, best val acc: 0.9083)\n",
      "[Epoch: 16120] train loss: 0.4047, train acc: 0.8508, val loss: 0.3214, val acc: 0.8954  (best train acc: 0.8680, best val acc: 0.9083)\n",
      "[Epoch: 16140] train loss: 0.3965, train acc: 0.8448, val loss: 0.3011, val acc: 0.9029  (best train acc: 0.8680, best val acc: 0.9083)\n",
      "[Epoch: 16160] train loss: 0.3903, train acc: 0.8610, val loss: 0.2921, val acc: 0.9059  (best train acc: 0.8680, best val acc: 0.9083)\n",
      "[Epoch: 16180] train loss: 0.3952, train acc: 0.8594, val loss: 0.3083, val acc: 0.8931  (best train acc: 0.8680, best val acc: 0.9083)\n",
      "[Epoch: 16200] train loss: 0.3747, train acc: 0.8536, val loss: 0.2933, val acc: 0.8992  (best train acc: 0.8680, best val acc: 0.9083)\n",
      "[Epoch: 16220] train loss: 0.3722, train acc: 0.8456, val loss: 0.2994, val acc: 0.9005  (best train acc: 0.8680, best val acc: 0.9083)\n",
      "[Epoch: 16240] train loss: 0.4015, train acc: 0.8517, val loss: 0.2963, val acc: 0.8975  (best train acc: 0.8680, best val acc: 0.9083)\n",
      "[Epoch: 16260] train loss: 0.3849, train acc: 0.8554, val loss: 0.3011, val acc: 0.8995  (best train acc: 0.8680, best val acc: 0.9083)\n",
      "[Epoch: 16280] train loss: 0.3970, train acc: 0.8509, val loss: 0.2963, val acc: 0.9073  (best train acc: 0.8680, best val acc: 0.9083)\n",
      "[Epoch: 16300] train loss: 0.4146, train acc: 0.8514, val loss: 0.3050, val acc: 0.9039  (best train acc: 0.8680, best val acc: 0.9083)\n",
      "[Epoch: 16320] train loss: 0.3901, train acc: 0.8446, val loss: 0.2961, val acc: 0.8958  (best train acc: 0.8680, best val acc: 0.9083)\n",
      "[Epoch: 16340] train loss: 0.3806, train acc: 0.8477, val loss: 0.3007, val acc: 0.9019  (best train acc: 0.8680, best val acc: 0.9083)\n",
      "[Epoch: 16360] train loss: 0.3811, train acc: 0.8566, val loss: 0.2892, val acc: 0.9029  (best train acc: 0.8680, best val acc: 0.9083)\n",
      "[Epoch: 16380] train loss: 0.3832, train acc: 0.8412, val loss: 0.3055, val acc: 0.8951  (best train acc: 0.8680, best val acc: 0.9083)\n",
      "[Epoch: 16400] train loss: 0.3886, train acc: 0.8600, val loss: 0.2880, val acc: 0.9039  (best train acc: 0.8680, best val acc: 0.9083)\n",
      "[Epoch: 16420] train loss: 0.3888, train acc: 0.8507, val loss: 0.2889, val acc: 0.9056  (best train acc: 0.8680, best val acc: 0.9083)\n",
      "[Epoch: 16440] train loss: 0.4251, train acc: 0.8343, val loss: 0.2905, val acc: 0.8971  (best train acc: 0.8680, best val acc: 0.9083)\n",
      "[Epoch: 16460] train loss: 0.4117, train acc: 0.8316, val loss: 0.3221, val acc: 0.9005  (best train acc: 0.8680, best val acc: 0.9083)\n",
      "[Epoch: 16480] train loss: 0.4166, train acc: 0.8360, val loss: 0.3186, val acc: 0.9008  (best train acc: 0.8680, best val acc: 0.9083)\n",
      "[Epoch: 16500] train loss: 0.4266, train acc: 0.8378, val loss: 0.3144, val acc: 0.9032  (best train acc: 0.8680, best val acc: 0.9083)\n",
      "[Epoch: 16520] train loss: 0.3831, train acc: 0.8508, val loss: 0.2933, val acc: 0.9019  (best train acc: 0.8680, best val acc: 0.9083)\n",
      "[Epoch: 16540] train loss: 0.3594, train acc: 0.8527, val loss: 0.2954, val acc: 0.9025  (best train acc: 0.8680, best val acc: 0.9083)\n",
      "[Epoch: 16560] train loss: 0.3674, train acc: 0.8542, val loss: 0.3199, val acc: 0.8968  (best train acc: 0.8680, best val acc: 0.9083)\n",
      "[Epoch: 16580] train loss: 0.4130, train acc: 0.8453, val loss: 0.2893, val acc: 0.9002  (best train acc: 0.8680, best val acc: 0.9083)\n",
      "[Epoch: 16600] train loss: 0.3808, train acc: 0.8550, val loss: 0.2872, val acc: 0.9022  (best train acc: 0.8680, best val acc: 0.9083)\n",
      "[Epoch: 16620] train loss: 0.3862, train acc: 0.8511, val loss: 0.2928, val acc: 0.9012  (best train acc: 0.8680, best val acc: 0.9083)\n",
      "[Epoch: 16640] train loss: 0.3898, train acc: 0.8408, val loss: 0.2919, val acc: 0.9046  (best train acc: 0.8680, best val acc: 0.9083)\n",
      "[Epoch: 16660] train loss: 0.3867, train acc: 0.8548, val loss: 0.2991, val acc: 0.9035  (best train acc: 0.8680, best val acc: 0.9083)\n",
      "[Epoch: 16680] train loss: 0.3914, train acc: 0.8514, val loss: 0.3070, val acc: 0.9008  (best train acc: 0.8680, best val acc: 0.9083)\n",
      "[Epoch: 16700] train loss: 0.3846, train acc: 0.8446, val loss: 0.3207, val acc: 0.9022  (best train acc: 0.8680, best val acc: 0.9083)\n",
      "[Epoch: 16720] train loss: 0.3938, train acc: 0.8439, val loss: 0.2992, val acc: 0.8985  (best train acc: 0.8680, best val acc: 0.9083)\n",
      "[Epoch: 16740] train loss: 0.4047, train acc: 0.8475, val loss: 0.3011, val acc: 0.8874  (best train acc: 0.8680, best val acc: 0.9083)\n",
      "[Epoch: 16760] train loss: 0.3866, train acc: 0.8440, val loss: 0.3107, val acc: 0.9046  (best train acc: 0.8680, best val acc: 0.9083)\n",
      "[Epoch: 16780] train loss: 0.3635, train acc: 0.8561, val loss: 0.2924, val acc: 0.9015  (best train acc: 0.8680, best val acc: 0.9083)\n",
      "[Epoch: 16800] train loss: 0.3916, train acc: 0.8441, val loss: 0.3087, val acc: 0.9049  (best train acc: 0.8680, best val acc: 0.9083)\n",
      "[Epoch: 16820] train loss: 0.4095, train acc: 0.8410, val loss: 0.2855, val acc: 0.8988  (best train acc: 0.8680, best val acc: 0.9083)\n",
      "[Epoch: 16840] train loss: 0.3828, train acc: 0.8378, val loss: 0.3072, val acc: 0.8944  (best train acc: 0.8680, best val acc: 0.9083)\n",
      "[Epoch: 16860] train loss: 0.3746, train acc: 0.8553, val loss: 0.3008, val acc: 0.9039  (best train acc: 0.8680, best val acc: 0.9083)\n",
      "[Epoch: 16880] train loss: 0.4151, train acc: 0.8404, val loss: 0.3027, val acc: 0.8917  (best train acc: 0.8680, best val acc: 0.9083)\n",
      "[Epoch: 16900] train loss: 0.3663, train acc: 0.8559, val loss: 0.3144, val acc: 0.8951  (best train acc: 0.8680, best val acc: 0.9083)\n",
      "[Epoch: 16920] train loss: 0.3852, train acc: 0.8402, val loss: 0.2967, val acc: 0.9035  (best train acc: 0.8680, best val acc: 0.9083)\n",
      "[Epoch: 16940] train loss: 0.4060, train acc: 0.8411, val loss: 0.3059, val acc: 0.9073  (best train acc: 0.8680, best val acc: 0.9083)\n",
      "[Epoch: 16960] train loss: 0.3911, train acc: 0.8536, val loss: 0.2857, val acc: 0.9002  (best train acc: 0.8680, best val acc: 0.9083)\n",
      "[Epoch: 16980] train loss: 0.3805, train acc: 0.8428, val loss: 0.3118, val acc: 0.8860  (best train acc: 0.8680, best val acc: 0.9083)\n",
      "[Epoch: 17000] train loss: 0.3780, train acc: 0.8516, val loss: 0.2926, val acc: 0.9022  (best train acc: 0.8680, best val acc: 0.9083)\n",
      "[Epoch: 17020] train loss: 0.3623, train acc: 0.8556, val loss: 0.3039, val acc: 0.8975  (best train acc: 0.8680, best val acc: 0.9083)\n",
      "[Epoch: 17040] train loss: 0.3725, train acc: 0.8461, val loss: 0.3109, val acc: 0.9049  (best train acc: 0.8680, best val acc: 0.9083)\n",
      "[Epoch: 17060] train loss: 0.3815, train acc: 0.8537, val loss: 0.2946, val acc: 0.9042  (best train acc: 0.8680, best val acc: 0.9083)\n",
      "[Epoch: 17080] train loss: 0.3826, train acc: 0.8515, val loss: 0.2782, val acc: 0.9015  (best train acc: 0.8680, best val acc: 0.9083)\n",
      "[Epoch: 17100] train loss: 0.4057, train acc: 0.8303, val loss: 0.2880, val acc: 0.9025  (best train acc: 0.8680, best val acc: 0.9083)\n",
      "[Epoch: 17120] train loss: 0.3899, train acc: 0.8583, val loss: 0.2948, val acc: 0.8948  (best train acc: 0.8680, best val acc: 0.9083)\n",
      "[Epoch: 17140] train loss: 0.3705, train acc: 0.8579, val loss: 0.3159, val acc: 0.8975  (best train acc: 0.8680, best val acc: 0.9083)\n",
      "[Epoch: 17160] train loss: 0.3817, train acc: 0.8591, val loss: 0.2850, val acc: 0.9046  (best train acc: 0.8680, best val acc: 0.9083)\n",
      "[Epoch: 17180] train loss: 0.3373, train acc: 0.8572, val loss: 0.3303, val acc: 0.8985  (best train acc: 0.8680, best val acc: 0.9083)\n",
      "[Epoch: 17200] train loss: 0.3586, train acc: 0.8556, val loss: 0.2948, val acc: 0.9029  (best train acc: 0.8680, best val acc: 0.9083)\n",
      "[Epoch: 17220] train loss: 0.3690, train acc: 0.8631, val loss: 0.2751, val acc: 0.9059  (best train acc: 0.8680, best val acc: 0.9083)\n",
      "[Epoch: 17240] train loss: 0.3607, train acc: 0.8560, val loss: 0.2893, val acc: 0.9039  (best train acc: 0.8680, best val acc: 0.9083)\n",
      "[Epoch: 17260] train loss: 0.3602, train acc: 0.8592, val loss: 0.2784, val acc: 0.9052  (best train acc: 0.8680, best val acc: 0.9083)\n",
      "[Epoch: 17280] train loss: 0.3447, train acc: 0.8543, val loss: 0.2874, val acc: 0.9039  (best train acc: 0.8680, best val acc: 0.9083)\n",
      "[Epoch: 17300] train loss: 0.3509, train acc: 0.8560, val loss: 0.2968, val acc: 0.9039  (best train acc: 0.8680, best val acc: 0.9083)\n",
      "[Epoch: 17320] train loss: 0.3738, train acc: 0.8526, val loss: 0.2991, val acc: 0.9083  (best train acc: 0.8680, best val acc: 0.9083)\n",
      "[Epoch: 17340] train loss: 0.3534, train acc: 0.8576, val loss: 0.3109, val acc: 0.9015  (best train acc: 0.8680, best val acc: 0.9083)\n",
      "[Epoch: 17360] train loss: 0.3496, train acc: 0.8582, val loss: 0.3004, val acc: 0.9052  (best train acc: 0.8680, best val acc: 0.9083)\n",
      "[Epoch: 17380] train loss: 0.3494, train acc: 0.8571, val loss: 0.2976, val acc: 0.9019  (best train acc: 0.8680, best val acc: 0.9083)\n",
      "[Epoch: 17400] train loss: 0.3307, train acc: 0.8603, val loss: 0.2873, val acc: 0.9066  (best train acc: 0.8680, best val acc: 0.9083)\n",
      "[Epoch: 17420] train loss: 0.3526, train acc: 0.8524, val loss: 0.2933, val acc: 0.9062  (best train acc: 0.8680, best val acc: 0.9083)\n",
      "[Epoch: 17440] train loss: 0.3517, train acc: 0.8605, val loss: 0.2989, val acc: 0.9066  (best train acc: 0.8680, best val acc: 0.9089)\n",
      "[Epoch: 17460] train loss: 0.3473, train acc: 0.8566, val loss: 0.2865, val acc: 0.9039  (best train acc: 0.8680, best val acc: 0.9089)\n",
      "[Epoch: 17480] train loss: 0.3530, train acc: 0.8540, val loss: 0.2782, val acc: 0.9025  (best train acc: 0.8680, best val acc: 0.9089)\n",
      "[Epoch: 17500] train loss: 0.3316, train acc: 0.8621, val loss: 0.3011, val acc: 0.9015  (best train acc: 0.8680, best val acc: 0.9089)\n",
      "[Epoch: 17520] train loss: 0.3595, train acc: 0.8527, val loss: 0.2914, val acc: 0.9056  (best train acc: 0.8680, best val acc: 0.9089)\n",
      "[Epoch: 17540] train loss: 0.3833, train acc: 0.8511, val loss: 0.2925, val acc: 0.9029  (best train acc: 0.8680, best val acc: 0.9089)\n",
      "[Epoch: 17560] train loss: 0.3716, train acc: 0.8585, val loss: 0.2778, val acc: 0.9022  (best train acc: 0.8680, best val acc: 0.9089)\n",
      "[Epoch: 17580] train loss: 0.3527, train acc: 0.8550, val loss: 0.2961, val acc: 0.9046  (best train acc: 0.8680, best val acc: 0.9089)\n",
      "[Epoch: 17600] train loss: 0.3449, train acc: 0.8539, val loss: 0.3217, val acc: 0.8985  (best train acc: 0.8680, best val acc: 0.9089)\n",
      "[Epoch: 17620] train loss: 0.3569, train acc: 0.8624, val loss: 0.2804, val acc: 0.9069  (best train acc: 0.8680, best val acc: 0.9089)\n",
      "[Epoch: 17640] train loss: 0.3473, train acc: 0.8591, val loss: 0.2779, val acc: 0.9002  (best train acc: 0.8683, best val acc: 0.9089)\n",
      "[Epoch: 17660] train loss: 0.3416, train acc: 0.8621, val loss: 0.2839, val acc: 0.9073  (best train acc: 0.8683, best val acc: 0.9096)\n",
      "[Epoch: 17680] train loss: 0.3522, train acc: 0.8644, val loss: 0.2846, val acc: 0.9008  (best train acc: 0.8683, best val acc: 0.9096)\n",
      "[Epoch: 17700] train loss: 0.3547, train acc: 0.8561, val loss: 0.3064, val acc: 0.9005  (best train acc: 0.8683, best val acc: 0.9096)\n",
      "[Epoch: 17720] train loss: 0.3687, train acc: 0.8559, val loss: 0.2819, val acc: 0.9039  (best train acc: 0.8683, best val acc: 0.9096)\n",
      "[Epoch: 17740] train loss: 0.3547, train acc: 0.8530, val loss: 0.2906, val acc: 0.8992  (best train acc: 0.8683, best val acc: 0.9096)\n",
      "[Epoch: 17760] train loss: 0.3930, train acc: 0.8485, val loss: 0.2980, val acc: 0.9049  (best train acc: 0.8683, best val acc: 0.9096)\n",
      "[Epoch: 17780] train loss: 0.3412, train acc: 0.8623, val loss: 0.3020, val acc: 0.9042  (best train acc: 0.8683, best val acc: 0.9096)\n",
      "[Epoch: 17800] train loss: 0.3418, train acc: 0.8610, val loss: 0.3224, val acc: 0.9008  (best train acc: 0.8683, best val acc: 0.9096)\n",
      "[Epoch: 17820] train loss: 0.3351, train acc: 0.8627, val loss: 0.2928, val acc: 0.9039  (best train acc: 0.8683, best val acc: 0.9096)\n",
      "[Epoch: 17840] train loss: 0.3399, train acc: 0.8614, val loss: 0.3026, val acc: 0.9002  (best train acc: 0.8683, best val acc: 0.9096)\n",
      "[Epoch: 17860] train loss: 0.3615, train acc: 0.8604, val loss: 0.2885, val acc: 0.9042  (best train acc: 0.8683, best val acc: 0.9096)\n",
      "[Epoch: 17880] train loss: 0.3580, train acc: 0.8540, val loss: 0.3090, val acc: 0.9019  (best train acc: 0.8683, best val acc: 0.9096)\n",
      "[Epoch: 17900] train loss: 0.3535, train acc: 0.8585, val loss: 0.2913, val acc: 0.9035  (best train acc: 0.8683, best val acc: 0.9096)\n",
      "[Epoch: 17920] train loss: 0.3273, train acc: 0.8653, val loss: 0.2950, val acc: 0.9035  (best train acc: 0.8683, best val acc: 0.9096)\n",
      "[Epoch: 17940] train loss: 0.3486, train acc: 0.8581, val loss: 0.2978, val acc: 0.9076  (best train acc: 0.8683, best val acc: 0.9096)\n",
      "[Epoch: 17960] train loss: 0.3692, train acc: 0.8590, val loss: 0.2879, val acc: 0.9062  (best train acc: 0.8683, best val acc: 0.9096)\n",
      "[Epoch: 17980] train loss: 0.3781, train acc: 0.8605, val loss: 0.2817, val acc: 0.9025  (best train acc: 0.8683, best val acc: 0.9096)\n",
      "[Epoch: 18000] train loss: 0.3444, train acc: 0.8587, val loss: 0.2848, val acc: 0.9022  (best train acc: 0.8683, best val acc: 0.9096)\n",
      "[Epoch: 18020] train loss: 0.3741, train acc: 0.8418, val loss: 0.2830, val acc: 0.8927  (best train acc: 0.8683, best val acc: 0.9096)\n",
      "[Epoch: 18040] train loss: 0.3521, train acc: 0.8614, val loss: 0.2891, val acc: 0.9059  (best train acc: 0.8683, best val acc: 0.9096)\n",
      "[Epoch: 18060] train loss: 0.3484, train acc: 0.8621, val loss: 0.2977, val acc: 0.9039  (best train acc: 0.8683, best val acc: 0.9096)\n",
      "[Epoch: 18080] train loss: 0.3473, train acc: 0.8561, val loss: 0.2834, val acc: 0.9035  (best train acc: 0.8703, best val acc: 0.9096)\n",
      "[Epoch: 18100] train loss: 0.3768, train acc: 0.8644, val loss: 0.2775, val acc: 0.9029  (best train acc: 0.8703, best val acc: 0.9096)\n",
      "[Epoch: 18120] train loss: 0.3548, train acc: 0.8578, val loss: 0.2879, val acc: 0.9039  (best train acc: 0.8703, best val acc: 0.9096)\n",
      "[Epoch: 18140] train loss: 0.3447, train acc: 0.8561, val loss: 0.3039, val acc: 0.9039  (best train acc: 0.8703, best val acc: 0.9096)\n",
      "[Epoch: 18160] train loss: 0.3479, train acc: 0.8633, val loss: 0.2816, val acc: 0.9012  (best train acc: 0.8703, best val acc: 0.9096)\n",
      "[Epoch: 18180] train loss: 0.3523, train acc: 0.8462, val loss: 0.2963, val acc: 0.8992  (best train acc: 0.8703, best val acc: 0.9096)\n",
      "[Epoch: 18200] train loss: 0.3453, train acc: 0.8564, val loss: 0.3015, val acc: 0.9005  (best train acc: 0.8703, best val acc: 0.9096)\n",
      "[Epoch: 18220] train loss: 0.3423, train acc: 0.8584, val loss: 0.3083, val acc: 0.8988  (best train acc: 0.8703, best val acc: 0.9096)\n",
      "[Epoch: 18240] train loss: 0.3505, train acc: 0.8543, val loss: 0.2878, val acc: 0.9025  (best train acc: 0.8703, best val acc: 0.9096)\n",
      "[Epoch: 18260] train loss: 0.3851, train acc: 0.8444, val loss: 0.3046, val acc: 0.9046  (best train acc: 0.8703, best val acc: 0.9096)\n",
      "[Epoch: 18280] train loss: 0.3578, train acc: 0.8616, val loss: 0.3063, val acc: 0.9042  (best train acc: 0.8703, best val acc: 0.9096)\n",
      "[Epoch: 18300] train loss: 0.3321, train acc: 0.8628, val loss: 0.3027, val acc: 0.9012  (best train acc: 0.8703, best val acc: 0.9096)\n",
      "[Epoch: 18320] train loss: 0.3484, train acc: 0.8583, val loss: 0.3081, val acc: 0.8968  (best train acc: 0.8703, best val acc: 0.9096)\n",
      "[Epoch: 18340] train loss: 0.3606, train acc: 0.8593, val loss: 0.2909, val acc: 0.9056  (best train acc: 0.8703, best val acc: 0.9096)\n",
      "[Epoch: 18360] train loss: 0.3379, train acc: 0.8612, val loss: 0.2862, val acc: 0.9046  (best train acc: 0.8703, best val acc: 0.9096)\n",
      "[Epoch: 18380] train loss: 0.3698, train acc: 0.8569, val loss: 0.3481, val acc: 0.8897  (best train acc: 0.8703, best val acc: 0.9096)\n",
      "[Epoch: 18400] train loss: 0.3662, train acc: 0.8580, val loss: 0.2950, val acc: 0.9076  (best train acc: 0.8703, best val acc: 0.9096)\n",
      "[Epoch: 18420] train loss: 0.3415, train acc: 0.8581, val loss: 0.3045, val acc: 0.9032  (best train acc: 0.8703, best val acc: 0.9096)\n",
      "[Epoch: 18440] train loss: 0.3427, train acc: 0.8633, val loss: 0.2958, val acc: 0.9025  (best train acc: 0.8703, best val acc: 0.9096)\n",
      "[Epoch: 18460] train loss: 0.3458, train acc: 0.8574, val loss: 0.3254, val acc: 0.8965  (best train acc: 0.8703, best val acc: 0.9096)\n",
      "[Epoch: 18480] train loss: 0.3680, train acc: 0.8586, val loss: 0.2817, val acc: 0.9035  (best train acc: 0.8703, best val acc: 0.9096)\n",
      "[Epoch: 18500] train loss: 0.3510, train acc: 0.8582, val loss: 0.3072, val acc: 0.9039  (best train acc: 0.8703, best val acc: 0.9096)\n",
      "[Epoch: 18520] train loss: 0.3473, train acc: 0.8584, val loss: 0.2854, val acc: 0.9062  (best train acc: 0.8703, best val acc: 0.9096)\n",
      "[Epoch: 18540] train loss: 0.3351, train acc: 0.8651, val loss: 0.2868, val acc: 0.9039  (best train acc: 0.8703, best val acc: 0.9096)\n",
      "[Epoch: 18560] train loss: 0.3362, train acc: 0.8574, val loss: 0.2950, val acc: 0.9022  (best train acc: 0.8707, best val acc: 0.9096)\n",
      "[Epoch: 18580] train loss: 0.3263, train acc: 0.8638, val loss: 0.3118, val acc: 0.8998  (best train acc: 0.8707, best val acc: 0.9096)\n",
      "[Epoch: 18600] train loss: 0.3293, train acc: 0.8639, val loss: 0.3108, val acc: 0.8995  (best train acc: 0.8707, best val acc: 0.9096)\n",
      "[Epoch: 18620] train loss: 0.4020, train acc: 0.8526, val loss: 0.2786, val acc: 0.9022  (best train acc: 0.8707, best val acc: 0.9096)\n",
      "[Epoch: 18640] train loss: 0.3521, train acc: 0.8504, val loss: 0.3136, val acc: 0.8941  (best train acc: 0.8707, best val acc: 0.9096)\n",
      "[Epoch: 18660] train loss: 0.3415, train acc: 0.8623, val loss: 0.2856, val acc: 0.9059  (best train acc: 0.8707, best val acc: 0.9096)\n",
      "[Epoch: 18680] train loss: 0.3220, train acc: 0.8666, val loss: 0.3002, val acc: 0.9046  (best train acc: 0.8707, best val acc: 0.9096)\n",
      "[Epoch: 18700] train loss: 0.3323, train acc: 0.8592, val loss: 0.2984, val acc: 0.8995  (best train acc: 0.8707, best val acc: 0.9096)\n",
      "[Epoch: 18720] train loss: 0.3487, train acc: 0.8638, val loss: 0.2999, val acc: 0.8954  (best train acc: 0.8707, best val acc: 0.9096)\n",
      "[Epoch: 18740] train loss: 0.3379, train acc: 0.8618, val loss: 0.3117, val acc: 0.8978  (best train acc: 0.8707, best val acc: 0.9096)\n",
      "[Epoch: 18760] train loss: 0.3628, train acc: 0.8571, val loss: 0.2787, val acc: 0.9015  (best train acc: 0.8707, best val acc: 0.9096)\n",
      "[Epoch: 18780] train loss: 0.3532, train acc: 0.8592, val loss: 0.2989, val acc: 0.9062  (best train acc: 0.8707, best val acc: 0.9096)\n",
      "[Epoch: 18800] train loss: 0.3394, train acc: 0.8611, val loss: 0.3385, val acc: 0.8867  (best train acc: 0.8707, best val acc: 0.9096)\n",
      "[Epoch: 18820] train loss: 0.3430, train acc: 0.8569, val loss: 0.3059, val acc: 0.9032  (best train acc: 0.8707, best val acc: 0.9096)\n",
      "[Epoch: 18840] train loss: 0.3463, train acc: 0.8589, val loss: 0.3026, val acc: 0.9015  (best train acc: 0.8707, best val acc: 0.9096)\n",
      "[Epoch: 18860] train loss: 0.3560, train acc: 0.8623, val loss: 0.2775, val acc: 0.9022  (best train acc: 0.8707, best val acc: 0.9096)\n",
      "[Epoch: 18880] train loss: 0.3707, train acc: 0.8400, val loss: 0.2983, val acc: 0.9029  (best train acc: 0.8707, best val acc: 0.9096)\n",
      "[Epoch: 18900] train loss: 0.3616, train acc: 0.8565, val loss: 0.2999, val acc: 0.8995  (best train acc: 0.8707, best val acc: 0.9096)\n",
      "[Epoch: 18920] train loss: 0.3552, train acc: 0.8561, val loss: 0.3024, val acc: 0.8961  (best train acc: 0.8707, best val acc: 0.9096)\n",
      "[Epoch: 18940] train loss: 0.3439, train acc: 0.8605, val loss: 0.2899, val acc: 0.8961  (best train acc: 0.8707, best val acc: 0.9096)\n",
      "[Epoch: 18960] train loss: 0.3398, train acc: 0.8608, val loss: 0.3054, val acc: 0.8998  (best train acc: 0.8707, best val acc: 0.9096)\n",
      "[Epoch: 18980] train loss: 0.3474, train acc: 0.8643, val loss: 0.2891, val acc: 0.9019  (best train acc: 0.8707, best val acc: 0.9096)\n",
      "[Epoch: 19000] train loss: 0.3451, train acc: 0.8595, val loss: 0.3058, val acc: 0.9052  (best train acc: 0.8707, best val acc: 0.9096)\n",
      "[Epoch: 19020] train loss: 0.3364, train acc: 0.8652, val loss: 0.3056, val acc: 0.9012  (best train acc: 0.8707, best val acc: 0.9096)\n",
      "[Epoch: 19040] train loss: 0.3343, train acc: 0.8611, val loss: 0.3052, val acc: 0.9049  (best train acc: 0.8707, best val acc: 0.9096)\n",
      "[Epoch: 19060] train loss: 0.3407, train acc: 0.8644, val loss: 0.3014, val acc: 0.9035  (best train acc: 0.8707, best val acc: 0.9096)\n",
      "[Epoch: 19080] train loss: 0.3573, train acc: 0.8568, val loss: 0.2776, val acc: 0.9042  (best train acc: 0.8707, best val acc: 0.9096)\n",
      "[Epoch: 19100] train loss: 0.3383, train acc: 0.8601, val loss: 0.3037, val acc: 0.9022  (best train acc: 0.8707, best val acc: 0.9096)\n",
      "[Epoch: 19120] train loss: 0.3430, train acc: 0.8565, val loss: 0.3009, val acc: 0.8958  (best train acc: 0.8707, best val acc: 0.9096)\n",
      "[Epoch: 19140] train loss: 0.3359, train acc: 0.8599, val loss: 0.2838, val acc: 0.9059  (best train acc: 0.8707, best val acc: 0.9096)\n",
      "[Epoch: 19160] train loss: 0.3897, train acc: 0.8479, val loss: 0.3151, val acc: 0.9025  (best train acc: 0.8707, best val acc: 0.9096)\n",
      "[Epoch: 19180] train loss: 0.3602, train acc: 0.8571, val loss: 0.2968, val acc: 0.9073  (best train acc: 0.8707, best val acc: 0.9096)\n",
      "[Epoch: 19200] train loss: 0.3631, train acc: 0.8584, val loss: 0.2848, val acc: 0.9035  (best train acc: 0.8707, best val acc: 0.9096)\n",
      "[Epoch: 19220] train loss: 0.3634, train acc: 0.8595, val loss: 0.2974, val acc: 0.8975  (best train acc: 0.8707, best val acc: 0.9096)\n",
      "[Epoch: 19240] train loss: 0.3390, train acc: 0.8596, val loss: 0.2914, val acc: 0.9046  (best train acc: 0.8707, best val acc: 0.9096)\n",
      "[Epoch: 19260] train loss: 0.3349, train acc: 0.8530, val loss: 0.3231, val acc: 0.8968  (best train acc: 0.8707, best val acc: 0.9096)\n",
      "[Epoch: 19280] train loss: 0.3512, train acc: 0.8550, val loss: 0.3048, val acc: 0.9029  (best train acc: 0.8707, best val acc: 0.9096)\n",
      "[Epoch: 19300] train loss: 0.3354, train acc: 0.8650, val loss: 0.2982, val acc: 0.9002  (best train acc: 0.8707, best val acc: 0.9096)\n",
      "[Epoch: 19320] train loss: 0.3400, train acc: 0.8618, val loss: 0.3119, val acc: 0.8971  (best train acc: 0.8707, best val acc: 0.9096)\n",
      "[Epoch: 19340] train loss: 0.3488, train acc: 0.8568, val loss: 0.3034, val acc: 0.9039  (best train acc: 0.8707, best val acc: 0.9096)\n",
      "[Epoch: 19360] train loss: 0.3201, train acc: 0.8623, val loss: 0.3077, val acc: 0.9066  (best train acc: 0.8707, best val acc: 0.9096)\n",
      "[Epoch: 19380] train loss: 0.3257, train acc: 0.8627, val loss: 0.3068, val acc: 0.8995  (best train acc: 0.8707, best val acc: 0.9096)\n",
      "[Epoch: 19400] train loss: 0.3231, train acc: 0.8634, val loss: 0.3023, val acc: 0.9025  (best train acc: 0.8707, best val acc: 0.9096)\n",
      "[Epoch: 19420] train loss: 0.3684, train acc: 0.8584, val loss: 0.3142, val acc: 0.8971  (best train acc: 0.8713, best val acc: 0.9096)\n",
      "[Epoch: 19440] train loss: 0.3418, train acc: 0.8610, val loss: 0.3161, val acc: 0.9022  (best train acc: 0.8713, best val acc: 0.9096)\n",
      "[Epoch: 19460] train loss: 0.3431, train acc: 0.8582, val loss: 0.2994, val acc: 0.9039  (best train acc: 0.8713, best val acc: 0.9130)\n",
      "[Epoch: 19480] train loss: 0.3393, train acc: 0.8559, val loss: 0.3261, val acc: 0.8941  (best train acc: 0.8713, best val acc: 0.9130)\n",
      "[Epoch: 19500] train loss: 0.3409, train acc: 0.8556, val loss: 0.3226, val acc: 0.8985  (best train acc: 0.8713, best val acc: 0.9130)\n",
      "[Epoch: 19520] train loss: 0.3331, train acc: 0.8633, val loss: 0.3324, val acc: 0.8934  (best train acc: 0.8713, best val acc: 0.9130)\n",
      "[Epoch: 19540] train loss: 0.3401, train acc: 0.8603, val loss: 0.3219, val acc: 0.8961  (best train acc: 0.8713, best val acc: 0.9130)\n",
      "[Epoch: 19560] train loss: 0.3472, train acc: 0.8594, val loss: 0.2988, val acc: 0.9059  (best train acc: 0.8713, best val acc: 0.9130)\n",
      "[Epoch: 19580] train loss: 0.3554, train acc: 0.8603, val loss: 0.3403, val acc: 0.8924  (best train acc: 0.8713, best val acc: 0.9130)\n",
      "[Epoch: 19600] train loss: 0.3534, train acc: 0.8613, val loss: 0.3072, val acc: 0.9052  (best train acc: 0.8713, best val acc: 0.9130)\n",
      "[Epoch: 19620] train loss: 0.3463, train acc: 0.8623, val loss: 0.2912, val acc: 0.9069  (best train acc: 0.8713, best val acc: 0.9130)\n",
      "[Epoch: 19640] train loss: 0.3439, train acc: 0.8599, val loss: 0.2979, val acc: 0.9035  (best train acc: 0.8713, best val acc: 0.9130)\n",
      "[Epoch: 19660] train loss: 0.3532, train acc: 0.8537, val loss: 0.2933, val acc: 0.9056  (best train acc: 0.8713, best val acc: 0.9130)\n",
      "[Epoch: 19680] train loss: 0.3166, train acc: 0.8644, val loss: 0.3155, val acc: 0.8981  (best train acc: 0.8713, best val acc: 0.9130)\n",
      "[Epoch: 19700] train loss: 0.3424, train acc: 0.8631, val loss: 0.2958, val acc: 0.9066  (best train acc: 0.8713, best val acc: 0.9130)\n",
      "[Epoch: 19720] train loss: 0.3630, train acc: 0.8472, val loss: 0.3160, val acc: 0.9019  (best train acc: 0.8713, best val acc: 0.9130)\n",
      "[Epoch: 19740] train loss: 0.3608, train acc: 0.8583, val loss: 0.3050, val acc: 0.9069  (best train acc: 0.8713, best val acc: 0.9130)\n",
      "[Epoch: 19760] train loss: 0.3436, train acc: 0.8625, val loss: 0.3057, val acc: 0.9073  (best train acc: 0.8713, best val acc: 0.9130)\n",
      "[Epoch: 19780] train loss: 0.3213, train acc: 0.8646, val loss: 0.2950, val acc: 0.9032  (best train acc: 0.8713, best val acc: 0.9130)\n",
      "[Epoch: 19800] train loss: 0.3352, train acc: 0.8644, val loss: 0.3253, val acc: 0.9032  (best train acc: 0.8713, best val acc: 0.9130)\n",
      "[Epoch: 19820] train loss: 0.3198, train acc: 0.8681, val loss: 0.3503, val acc: 0.8890  (best train acc: 0.8713, best val acc: 0.9130)\n",
      "[Epoch: 19840] train loss: 0.3609, train acc: 0.8608, val loss: 0.2866, val acc: 0.9032  (best train acc: 0.8713, best val acc: 0.9157)\n",
      "[Epoch: 19860] train loss: 0.3411, train acc: 0.8686, val loss: 0.2802, val acc: 0.9076  (best train acc: 0.8713, best val acc: 0.9157)\n",
      "[Epoch: 19880] train loss: 0.3418, train acc: 0.8616, val loss: 0.3015, val acc: 0.9076  (best train acc: 0.8713, best val acc: 0.9157)\n",
      "[Epoch: 19900] train loss: 0.3388, train acc: 0.8588, val loss: 0.2994, val acc: 0.9073  (best train acc: 0.8713, best val acc: 0.9157)\n",
      "[Epoch: 19920] train loss: 0.3402, train acc: 0.8632, val loss: 0.3076, val acc: 0.9056  (best train acc: 0.8713, best val acc: 0.9157)\n",
      "[Epoch: 19940] train loss: 0.3361, train acc: 0.8670, val loss: 0.2971, val acc: 0.9049  (best train acc: 0.8713, best val acc: 0.9157)\n",
      "[Epoch: 19960] train loss: 0.3709, train acc: 0.8529, val loss: 0.2807, val acc: 0.9076  (best train acc: 0.8713, best val acc: 0.9157)\n",
      "[Epoch: 19980] train loss: 0.3355, train acc: 0.8615, val loss: 0.2935, val acc: 0.9019  (best train acc: 0.8713, best val acc: 0.9157)\n",
      "[Epoch: 20000] train loss: 0.3334, train acc: 0.8717, val loss: 0.3251, val acc: 0.8995  (best train acc: 0.8717, best val acc: 0.9157)\n",
      "[Epoch: 20020] train loss: 0.3701, train acc: 0.8346, val loss: 0.3014, val acc: 0.9022  (best train acc: 0.8731, best val acc: 0.9157)\n",
      "[Epoch: 20040] train loss: 0.3664, train acc: 0.8592, val loss: 0.2961, val acc: 0.9083  (best train acc: 0.8731, best val acc: 0.9157)\n",
      "[Epoch: 20060] train loss: 0.3218, train acc: 0.8673, val loss: 0.3068, val acc: 0.9089  (best train acc: 0.8731, best val acc: 0.9157)\n",
      "[Epoch: 20080] train loss: 0.3305, train acc: 0.8554, val loss: 0.3004, val acc: 0.9052  (best train acc: 0.8731, best val acc: 0.9157)\n",
      "[Epoch: 20100] train loss: 0.3452, train acc: 0.8501, val loss: 0.2927, val acc: 0.8998  (best train acc: 0.8731, best val acc: 0.9157)\n",
      "[Epoch: 20120] train loss: 0.3263, train acc: 0.8602, val loss: 0.3066, val acc: 0.8992  (best train acc: 0.8731, best val acc: 0.9157)\n",
      "[Epoch: 20140] train loss: 0.3332, train acc: 0.8642, val loss: 0.2867, val acc: 0.9096  (best train acc: 0.8731, best val acc: 0.9157)\n",
      "[Epoch: 20160] train loss: 0.3296, train acc: 0.8656, val loss: 0.3057, val acc: 0.9106  (best train acc: 0.8731, best val acc: 0.9157)\n",
      "[Epoch: 20180] train loss: 0.3326, train acc: 0.8641, val loss: 0.2942, val acc: 0.9066  (best train acc: 0.8731, best val acc: 0.9177)\n",
      "[Epoch: 20200] train loss: 0.3453, train acc: 0.8553, val loss: 0.3064, val acc: 0.8992  (best train acc: 0.8731, best val acc: 0.9177)\n",
      "[Epoch: 20220] train loss: 0.3377, train acc: 0.8650, val loss: 0.2995, val acc: 0.9042  (best train acc: 0.8731, best val acc: 0.9177)\n",
      "[Epoch: 20240] train loss: 0.3462, train acc: 0.8601, val loss: 0.3082, val acc: 0.9015  (best train acc: 0.8731, best val acc: 0.9177)\n",
      "[Epoch: 20260] train loss: 0.3228, train acc: 0.8640, val loss: 0.3011, val acc: 0.9025  (best train acc: 0.8736, best val acc: 0.9177)\n",
      "[Epoch: 20280] train loss: 0.3096, train acc: 0.8688, val loss: 0.3152, val acc: 0.9079  (best train acc: 0.8747, best val acc: 0.9177)\n",
      "[Epoch: 20300] train loss: 0.4054, train acc: 0.8295, val loss: 0.3321, val acc: 0.8998  (best train acc: 0.8747, best val acc: 0.9177)\n",
      "[Epoch: 20320] train loss: 0.3711, train acc: 0.8468, val loss: 0.3102, val acc: 0.8985  (best train acc: 0.8747, best val acc: 0.9177)\n",
      "[Epoch: 20340] train loss: 0.3306, train acc: 0.8674, val loss: 0.3071, val acc: 0.9049  (best train acc: 0.8747, best val acc: 0.9177)\n",
      "[Epoch: 20360] train loss: 0.3621, train acc: 0.8488, val loss: 0.3866, val acc: 0.8782  (best train acc: 0.8747, best val acc: 0.9177)\n",
      "[Epoch: 20380] train loss: 0.3972, train acc: 0.8167, val loss: 0.3184, val acc: 0.9042  (best train acc: 0.8747, best val acc: 0.9177)\n",
      "[Epoch: 20400] train loss: 0.3382, train acc: 0.8621, val loss: 0.3061, val acc: 0.9079  (best train acc: 0.8747, best val acc: 0.9180)\n",
      "[Epoch: 20420] train loss: 0.3356, train acc: 0.8641, val loss: 0.2856, val acc: 0.9130  (best train acc: 0.8747, best val acc: 0.9194)\n",
      "[Epoch: 20440] train loss: 0.3516, train acc: 0.8594, val loss: 0.2964, val acc: 0.9110  (best train acc: 0.8747, best val acc: 0.9194)\n",
      "[Epoch: 20460] train loss: 0.3558, train acc: 0.8564, val loss: 0.2947, val acc: 0.9076  (best train acc: 0.8747, best val acc: 0.9194)\n",
      "[Epoch: 20480] train loss: 0.3415, train acc: 0.8647, val loss: 0.2984, val acc: 0.9113  (best train acc: 0.8747, best val acc: 0.9194)\n",
      "[Epoch: 20500] train loss: 0.3423, train acc: 0.8608, val loss: 0.2864, val acc: 0.9103  (best train acc: 0.8747, best val acc: 0.9194)\n",
      "[Epoch: 20520] train loss: 0.3362, train acc: 0.8655, val loss: 0.2820, val acc: 0.9133  (best train acc: 0.8753, best val acc: 0.9194)\n",
      "[Epoch: 20540] train loss: 0.3110, train acc: 0.8680, val loss: 0.2924, val acc: 0.9130  (best train acc: 0.8753, best val acc: 0.9194)\n",
      "[Epoch: 20560] train loss: 0.3513, train acc: 0.8586, val loss: 0.2967, val acc: 0.9120  (best train acc: 0.8753, best val acc: 0.9194)\n",
      "[Epoch: 20580] train loss: 0.3574, train acc: 0.8544, val loss: 0.3188, val acc: 0.9022  (best train acc: 0.8753, best val acc: 0.9201)\n",
      "[Epoch: 20600] train loss: 0.4131, train acc: 0.8197, val loss: 0.3088, val acc: 0.8880  (best train acc: 0.8754, best val acc: 0.9201)\n",
      "[Epoch: 20620] train loss: 0.3777, train acc: 0.8492, val loss: 0.2731, val acc: 0.9103  (best train acc: 0.8754, best val acc: 0.9201)\n",
      "[Epoch: 20640] train loss: 0.3382, train acc: 0.8600, val loss: 0.3038, val acc: 0.9096  (best train acc: 0.8754, best val acc: 0.9201)\n",
      "[Epoch: 20660] train loss: 0.3260, train acc: 0.8629, val loss: 0.3219, val acc: 0.9039  (best train acc: 0.8754, best val acc: 0.9201)\n",
      "[Epoch: 20680] train loss: 0.3481, train acc: 0.8626, val loss: 0.3020, val acc: 0.9089  (best train acc: 0.8754, best val acc: 0.9201)\n",
      "[Epoch: 20700] train loss: 0.3262, train acc: 0.8737, val loss: 0.3333, val acc: 0.9106  (best train acc: 0.8754, best val acc: 0.9201)\n",
      "[Epoch: 20720] train loss: 0.3347, train acc: 0.8652, val loss: 0.2916, val acc: 0.9099  (best train acc: 0.8754, best val acc: 0.9201)\n",
      "[Epoch: 20740] train loss: 0.3288, train acc: 0.8698, val loss: 0.3042, val acc: 0.9116  (best train acc: 0.8754, best val acc: 0.9201)\n",
      "[Epoch: 20760] train loss: 0.3390, train acc: 0.8624, val loss: 0.3206, val acc: 0.9089  (best train acc: 0.8754, best val acc: 0.9201)\n",
      "[Epoch: 20780] train loss: 0.3273, train acc: 0.8663, val loss: 0.2967, val acc: 0.9079  (best train acc: 0.8754, best val acc: 0.9201)\n",
      "[Epoch: 20800] train loss: 0.3316, train acc: 0.8626, val loss: 0.3130, val acc: 0.9073  (best train acc: 0.8754, best val acc: 0.9201)\n",
      "[Epoch: 20820] train loss: 0.3405, train acc: 0.8618, val loss: 0.3060, val acc: 0.9049  (best train acc: 0.8754, best val acc: 0.9201)\n",
      "[Epoch: 20840] train loss: 0.3545, train acc: 0.8559, val loss: 0.2897, val acc: 0.9137  (best train acc: 0.8754, best val acc: 0.9201)\n",
      "[Epoch: 20860] train loss: 0.3153, train acc: 0.8677, val loss: 0.3038, val acc: 0.9130  (best train acc: 0.8754, best val acc: 0.9201)\n",
      "[Epoch: 20880] train loss: 0.3518, train acc: 0.8631, val loss: 0.3246, val acc: 0.9099  (best train acc: 0.8754, best val acc: 0.9201)\n",
      "[Epoch: 20900] train loss: 0.3448, train acc: 0.8626, val loss: 0.2933, val acc: 0.9120  (best train acc: 0.8754, best val acc: 0.9201)\n",
      "[Epoch: 20920] train loss: 0.3466, train acc: 0.8625, val loss: 0.2882, val acc: 0.9076  (best train acc: 0.8754, best val acc: 0.9201)\n",
      "[Epoch: 20940] train loss: 0.3583, train acc: 0.8576, val loss: 0.3048, val acc: 0.9073  (best train acc: 0.8754, best val acc: 0.9201)\n",
      "[Epoch: 20960] train loss: 0.3410, train acc: 0.8673, val loss: 0.2950, val acc: 0.9167  (best train acc: 0.8754, best val acc: 0.9201)\n",
      "[Epoch: 20980] train loss: 0.3270, train acc: 0.8668, val loss: 0.2985, val acc: 0.9086  (best train acc: 0.8754, best val acc: 0.9204)\n",
      "[Epoch: 21000] train loss: 0.3538, train acc: 0.8593, val loss: 0.3001, val acc: 0.9096  (best train acc: 0.8754, best val acc: 0.9204)\n",
      "[Epoch: 21020] train loss: 0.3301, train acc: 0.8644, val loss: 0.3174, val acc: 0.9076  (best train acc: 0.8754, best val acc: 0.9204)\n",
      "[Epoch: 21040] train loss: 0.3279, train acc: 0.8690, val loss: 0.3088, val acc: 0.9062  (best train acc: 0.8754, best val acc: 0.9204)\n",
      "[Epoch: 21060] train loss: 0.3503, train acc: 0.8513, val loss: 0.3036, val acc: 0.9113  (best train acc: 0.8754, best val acc: 0.9204)\n",
      "[Epoch: 21080] train loss: 0.3623, train acc: 0.8520, val loss: 0.3072, val acc: 0.9019  (best train acc: 0.8754, best val acc: 0.9204)\n",
      "[Epoch: 21100] train loss: 0.3658, train acc: 0.8537, val loss: 0.2911, val acc: 0.9123  (best train acc: 0.8754, best val acc: 0.9204)\n",
      "[Epoch: 21120] train loss: 0.3287, train acc: 0.8682, val loss: 0.3218, val acc: 0.9069  (best train acc: 0.8754, best val acc: 0.9204)\n",
      "[Epoch: 21140] train loss: 0.4044, train acc: 0.8352, val loss: 0.3508, val acc: 0.8897  (best train acc: 0.8754, best val acc: 0.9204)\n",
      "[Epoch: 21160] train loss: 0.3693, train acc: 0.8504, val loss: 0.3386, val acc: 0.8914  (best train acc: 0.8754, best val acc: 0.9204)\n",
      "[Epoch: 21180] train loss: 0.3714, train acc: 0.8485, val loss: 0.3306, val acc: 0.9049  (best train acc: 0.8754, best val acc: 0.9204)\n",
      "[Epoch: 21200] train loss: 0.3436, train acc: 0.8647, val loss: 0.3064, val acc: 0.9197  (best train acc: 0.8754, best val acc: 0.9204)\n",
      "[Epoch: 21220] train loss: 0.3893, train acc: 0.8375, val loss: 0.3413, val acc: 0.9056  (best train acc: 0.8754, best val acc: 0.9204)\n",
      "[Epoch: 21240] train loss: 0.3866, train acc: 0.8493, val loss: 0.2797, val acc: 0.9015  (best train acc: 0.8754, best val acc: 0.9204)\n",
      "[Epoch: 21260] train loss: 0.3393, train acc: 0.8707, val loss: 0.3389, val acc: 0.9059  (best train acc: 0.8754, best val acc: 0.9204)\n",
      "[Epoch: 21280] train loss: 0.3314, train acc: 0.8628, val loss: 0.3152, val acc: 0.9177  (best train acc: 0.8754, best val acc: 0.9204)\n",
      "[Epoch: 21300] train loss: 0.3149, train acc: 0.8670, val loss: 0.2832, val acc: 0.9140  (best train acc: 0.8754, best val acc: 0.9204)\n",
      "[Epoch: 21320] train loss: 0.3366, train acc: 0.8657, val loss: 0.3114, val acc: 0.9126  (best train acc: 0.8754, best val acc: 0.9218)\n",
      "[Epoch: 21340] train loss: 0.3260, train acc: 0.8669, val loss: 0.2984, val acc: 0.9184  (best train acc: 0.8754, best val acc: 0.9218)\n",
      "[Epoch: 21360] train loss: 0.3550, train acc: 0.8571, val loss: 0.2922, val acc: 0.9133  (best train acc: 0.8754, best val acc: 0.9221)\n",
      "[Epoch: 21380] train loss: 0.3298, train acc: 0.8626, val loss: 0.3022, val acc: 0.9157  (best train acc: 0.8754, best val acc: 0.9251)\n",
      "[Epoch: 21400] train loss: 0.3326, train acc: 0.8620, val loss: 0.2922, val acc: 0.9147  (best train acc: 0.8754, best val acc: 0.9251)\n",
      "[Epoch: 21420] train loss: 0.3262, train acc: 0.8655, val loss: 0.3098, val acc: 0.9184  (best train acc: 0.8754, best val acc: 0.9251)\n",
      "[Epoch: 21440] train loss: 0.3714, train acc: 0.8540, val loss: 0.3392, val acc: 0.8985  (best train acc: 0.8754, best val acc: 0.9251)\n",
      "[Epoch: 21460] train loss: 0.3379, train acc: 0.8623, val loss: 0.2922, val acc: 0.9180  (best train acc: 0.8754, best val acc: 0.9251)\n",
      "[Epoch: 21480] train loss: 0.3389, train acc: 0.8640, val loss: 0.3076, val acc: 0.9174  (best train acc: 0.8754, best val acc: 0.9251)\n",
      "[Epoch: 21500] train loss: 0.3232, train acc: 0.8685, val loss: 0.3107, val acc: 0.9103  (best train acc: 0.8754, best val acc: 0.9251)\n",
      "[Epoch: 21520] train loss: 0.3532, train acc: 0.8543, val loss: 0.3830, val acc: 0.8971  (best train acc: 0.8783, best val acc: 0.9251)\n",
      "[Epoch: 21540] train loss: 0.3406, train acc: 0.8601, val loss: 0.3036, val acc: 0.9126  (best train acc: 0.8783, best val acc: 0.9251)\n",
      "[Epoch: 21560] train loss: 0.3618, train acc: 0.8550, val loss: 0.2999, val acc: 0.9123  (best train acc: 0.8783, best val acc: 0.9251)\n",
      "[Epoch: 21580] train loss: 0.3381, train acc: 0.8708, val loss: 0.2979, val acc: 0.9221  (best train acc: 0.8788, best val acc: 0.9251)\n",
      "[Epoch: 21600] train loss: 0.3478, train acc: 0.8622, val loss: 0.2999, val acc: 0.9069  (best train acc: 0.8788, best val acc: 0.9251)\n",
      "[Epoch: 21620] train loss: 0.3496, train acc: 0.8656, val loss: 0.3184, val acc: 0.9130  (best train acc: 0.8788, best val acc: 0.9251)\n",
      "[Epoch: 21640] train loss: 0.3401, train acc: 0.8638, val loss: 0.2974, val acc: 0.9221  (best train acc: 0.8788, best val acc: 0.9251)\n",
      "[Epoch: 21660] train loss: 0.3382, train acc: 0.8694, val loss: 0.2891, val acc: 0.9194  (best train acc: 0.8788, best val acc: 0.9251)\n",
      "[Epoch: 21680] train loss: 0.3264, train acc: 0.8697, val loss: 0.2968, val acc: 0.9167  (best train acc: 0.8788, best val acc: 0.9251)\n",
      "[Epoch: 21700] train loss: 0.3332, train acc: 0.8629, val loss: 0.2996, val acc: 0.9187  (best train acc: 0.8788, best val acc: 0.9251)\n",
      "[Epoch: 21720] train loss: 0.3702, train acc: 0.8537, val loss: 0.3134, val acc: 0.9160  (best train acc: 0.8788, best val acc: 0.9265)\n",
      "[Epoch: 21740] train loss: 0.3315, train acc: 0.8616, val loss: 0.3354, val acc: 0.9099  (best train acc: 0.8788, best val acc: 0.9265)\n",
      "[Epoch: 21760] train loss: 0.3896, train acc: 0.8439, val loss: 0.3111, val acc: 0.9079  (best train acc: 0.8788, best val acc: 0.9265)\n",
      "[Epoch: 21780] train loss: 0.3307, train acc: 0.8664, val loss: 0.3129, val acc: 0.9137  (best train acc: 0.8788, best val acc: 0.9265)\n",
      "[Epoch: 21800] train loss: 0.3350, train acc: 0.8667, val loss: 0.3078, val acc: 0.9255  (best train acc: 0.8788, best val acc: 0.9265)\n",
      "[Epoch: 21820] train loss: 0.3305, train acc: 0.8753, val loss: 0.2887, val acc: 0.9177  (best train acc: 0.8788, best val acc: 0.9265)\n",
      "[Epoch: 21840] train loss: 0.3263, train acc: 0.8686, val loss: 0.3126, val acc: 0.9153  (best train acc: 0.8788, best val acc: 0.9265)\n",
      "[Epoch: 21860] train loss: 0.3286, train acc: 0.8653, val loss: 0.3757, val acc: 0.8938  (best train acc: 0.8788, best val acc: 0.9265)\n",
      "[Epoch: 21880] train loss: 0.3379, train acc: 0.8676, val loss: 0.3056, val acc: 0.9187  (best train acc: 0.8788, best val acc: 0.9265)\n",
      "[Epoch: 21900] train loss: 0.3065, train acc: 0.8775, val loss: 0.3160, val acc: 0.9201  (best train acc: 0.8788, best val acc: 0.9265)\n",
      "[Epoch: 21920] train loss: 0.3261, train acc: 0.8692, val loss: 0.3003, val acc: 0.9218  (best train acc: 0.8788, best val acc: 0.9265)\n",
      "[Epoch: 21940] train loss: 0.3624, train acc: 0.8530, val loss: 0.3086, val acc: 0.9137  (best train acc: 0.8788, best val acc: 0.9265)\n",
      "[Epoch: 21960] train loss: 0.3599, train acc: 0.8647, val loss: 0.2755, val acc: 0.9167  (best train acc: 0.8788, best val acc: 0.9265)\n",
      "[Epoch: 21980] train loss: 0.3158, train acc: 0.8720, val loss: 0.3014, val acc: 0.9197  (best train acc: 0.8788, best val acc: 0.9275)\n",
      "[Epoch: 22000] train loss: 0.3317, train acc: 0.8673, val loss: 0.2933, val acc: 0.9157  (best train acc: 0.8800, best val acc: 0.9275)\n",
      "[Epoch: 22020] train loss: 0.3160, train acc: 0.8728, val loss: 0.3002, val acc: 0.9258  (best train acc: 0.8800, best val acc: 0.9275)\n",
      "[Epoch: 22040] train loss: 0.3430, train acc: 0.8626, val loss: 0.3036, val acc: 0.9137  (best train acc: 0.8844, best val acc: 0.9275)\n",
      "[Epoch: 22060] train loss: 0.3178, train acc: 0.8706, val loss: 0.2850, val acc: 0.9255  (best train acc: 0.8844, best val acc: 0.9275)\n",
      "[Epoch: 22080] train loss: 0.3293, train acc: 0.8590, val loss: 0.3401, val acc: 0.9022  (best train acc: 0.8844, best val acc: 0.9275)\n",
      "[Epoch: 22100] train loss: 0.3328, train acc: 0.8698, val loss: 0.2923, val acc: 0.9221  (best train acc: 0.8844, best val acc: 0.9275)\n",
      "[Epoch: 22120] train loss: 0.3098, train acc: 0.8723, val loss: 0.3098, val acc: 0.9272  (best train acc: 0.8844, best val acc: 0.9298)\n",
      "[Epoch: 22140] train loss: 0.3075, train acc: 0.8697, val loss: 0.3106, val acc: 0.9130  (best train acc: 0.8844, best val acc: 0.9298)\n",
      "[Epoch: 22160] train loss: 0.3404, train acc: 0.8681, val loss: 0.2921, val acc: 0.9245  (best train acc: 0.8844, best val acc: 0.9298)\n",
      "[Epoch: 22180] train loss: 0.3389, train acc: 0.8708, val loss: 0.2975, val acc: 0.9261  (best train acc: 0.8850, best val acc: 0.9298)\n",
      "[Epoch: 22200] train loss: 0.3382, train acc: 0.8699, val loss: 0.2825, val acc: 0.9211  (best train acc: 0.8850, best val acc: 0.9298)\n",
      "[Epoch: 22220] train loss: 0.3340, train acc: 0.8654, val loss: 0.3065, val acc: 0.9241  (best train acc: 0.8850, best val acc: 0.9298)\n",
      "[Epoch: 22240] train loss: 0.3380, train acc: 0.8514, val loss: 0.3053, val acc: 0.9201  (best train acc: 0.8850, best val acc: 0.9298)\n",
      "[Epoch: 22260] train loss: 0.3064, train acc: 0.8683, val loss: 0.2965, val acc: 0.9221  (best train acc: 0.8850, best val acc: 0.9298)\n",
      "[Epoch: 22280] train loss: 0.3253, train acc: 0.8701, val loss: 0.2994, val acc: 0.9187  (best train acc: 0.8850, best val acc: 0.9298)\n",
      "[Epoch: 22300] train loss: 0.3805, train acc: 0.8506, val loss: 0.2995, val acc: 0.9187  (best train acc: 0.8850, best val acc: 0.9298)\n",
      "[Epoch: 22320] train loss: 0.3418, train acc: 0.8509, val loss: 0.2792, val acc: 0.9052  (best train acc: 0.8850, best val acc: 0.9298)\n",
      "[Epoch: 22340] train loss: 0.3205, train acc: 0.8733, val loss: 0.3097, val acc: 0.9025  (best train acc: 0.8850, best val acc: 0.9298)\n",
      "[Epoch: 22360] train loss: 0.3239, train acc: 0.8623, val loss: 0.2822, val acc: 0.9137  (best train acc: 0.8850, best val acc: 0.9298)\n",
      "[Epoch: 22380] train loss: 0.3060, train acc: 0.8728, val loss: 0.2774, val acc: 0.9241  (best train acc: 0.8850, best val acc: 0.9298)\n",
      "[Epoch: 22400] train loss: 0.3441, train acc: 0.8587, val loss: 0.3045, val acc: 0.9130  (best train acc: 0.8850, best val acc: 0.9298)\n",
      "[Epoch: 22420] train loss: 0.3309, train acc: 0.8609, val loss: 0.2796, val acc: 0.9137  (best train acc: 0.8850, best val acc: 0.9298)\n",
      "[Epoch: 22440] train loss: 0.3589, train acc: 0.8508, val loss: 0.3023, val acc: 0.9116  (best train acc: 0.8850, best val acc: 0.9298)\n",
      "[Epoch: 22460] train loss: 0.2980, train acc: 0.8737, val loss: 0.2668, val acc: 0.9096  (best train acc: 0.8850, best val acc: 0.9298)\n",
      "[Epoch: 22480] train loss: 0.3131, train acc: 0.8699, val loss: 0.2961, val acc: 0.9180  (best train acc: 0.8850, best val acc: 0.9298)\n",
      "[Epoch: 22500] train loss: 0.3207, train acc: 0.8618, val loss: 0.2594, val acc: 0.9194  (best train acc: 0.8850, best val acc: 0.9298)\n",
      "[Epoch: 22520] train loss: 0.3211, train acc: 0.8623, val loss: 0.2535, val acc: 0.9218  (best train acc: 0.8850, best val acc: 0.9298)\n",
      "[Epoch: 22540] train loss: 0.3112, train acc: 0.8728, val loss: 0.2876, val acc: 0.9187  (best train acc: 0.8850, best val acc: 0.9298)\n",
      "[Epoch: 22560] train loss: 0.3417, train acc: 0.8459, val loss: 0.2924, val acc: 0.9211  (best train acc: 0.8850, best val acc: 0.9298)\n",
      "[Epoch: 22580] train loss: 0.3299, train acc: 0.8570, val loss: 0.2880, val acc: 0.9143  (best train acc: 0.8850, best val acc: 0.9298)\n",
      "[Epoch: 22600] train loss: 0.2943, train acc: 0.8845, val loss: 0.2673, val acc: 0.9056  (best train acc: 0.8850, best val acc: 0.9298)\n",
      "[Epoch: 22620] train loss: 0.3171, train acc: 0.8676, val loss: 0.3079, val acc: 0.9164  (best train acc: 0.8850, best val acc: 0.9298)\n",
      "[Epoch: 22640] train loss: 0.4026, train acc: 0.8083, val loss: 0.3103, val acc: 0.8968  (best train acc: 0.8850, best val acc: 0.9298)\n",
      "[Epoch: 22660] train loss: 0.3444, train acc: 0.8472, val loss: 0.2855, val acc: 0.9116  (best train acc: 0.8850, best val acc: 0.9298)\n",
      "[Epoch: 22680] train loss: 0.3140, train acc: 0.8702, val loss: 0.2631, val acc: 0.9052  (best train acc: 0.8850, best val acc: 0.9298)\n",
      "[Epoch: 22700] train loss: 0.3884, train acc: 0.8334, val loss: 0.2606, val acc: 0.9160  (best train acc: 0.8850, best val acc: 0.9298)\n",
      "[Epoch: 22720] train loss: 0.3004, train acc: 0.8679, val loss: 0.2457, val acc: 0.9288  (best train acc: 0.8850, best val acc: 0.9315)\n",
      "[Epoch: 22740] train loss: 0.3052, train acc: 0.8829, val loss: 0.2725, val acc: 0.9201  (best train acc: 0.8850, best val acc: 0.9315)\n",
      "[Epoch: 22760] train loss: 0.3685, train acc: 0.8496, val loss: 0.2639, val acc: 0.9221  (best train acc: 0.8850, best val acc: 0.9315)\n",
      "[Epoch: 22780] train loss: 0.3029, train acc: 0.8776, val loss: 0.2731, val acc: 0.9255  (best train acc: 0.8850, best val acc: 0.9319)\n",
      "[Epoch: 22800] train loss: 0.3471, train acc: 0.8460, val loss: 0.2507, val acc: 0.9140  (best train acc: 0.8850, best val acc: 0.9322)\n",
      "[Epoch: 22820] train loss: 0.3667, train acc: 0.8446, val loss: 0.2351, val acc: 0.9342  (best train acc: 0.8850, best val acc: 0.9342)\n",
      "[Epoch: 22840] train loss: 0.3474, train acc: 0.8590, val loss: 0.2581, val acc: 0.9234  (best train acc: 0.8850, best val acc: 0.9342)\n",
      "[Epoch: 22860] train loss: 0.3306, train acc: 0.8597, val loss: 0.2687, val acc: 0.9224  (best train acc: 0.8850, best val acc: 0.9342)\n",
      "[Epoch: 22880] train loss: 0.3026, train acc: 0.8673, val loss: 0.2815, val acc: 0.9170  (best train acc: 0.8850, best val acc: 0.9342)\n",
      "[Epoch: 22900] train loss: 0.3254, train acc: 0.8669, val loss: 0.2504, val acc: 0.9258  (best train acc: 0.8850, best val acc: 0.9342)\n",
      "[Epoch: 22920] train loss: 0.3000, train acc: 0.8636, val loss: 0.2836, val acc: 0.9292  (best train acc: 0.8850, best val acc: 0.9342)\n",
      "[Epoch: 22940] train loss: 0.3222, train acc: 0.8681, val loss: 0.2406, val acc: 0.9285  (best train acc: 0.8850, best val acc: 0.9373)\n",
      "[Epoch: 22960] train loss: 0.3337, train acc: 0.8562, val loss: 0.2546, val acc: 0.9285  (best train acc: 0.8850, best val acc: 0.9373)\n",
      "[Epoch: 22980] train loss: 0.3164, train acc: 0.8613, val loss: 0.2627, val acc: 0.9251  (best train acc: 0.8850, best val acc: 0.9373)\n",
      "[Epoch: 23000] train loss: 0.3758, train acc: 0.8250, val loss: 0.2692, val acc: 0.9191  (best train acc: 0.8850, best val acc: 0.9373)\n",
      "[Epoch: 23020] train loss: 0.3360, train acc: 0.8536, val loss: 0.2762, val acc: 0.9238  (best train acc: 0.8850, best val acc: 0.9373)\n",
      "[Epoch: 23040] train loss: 0.3125, train acc: 0.8640, val loss: 0.2651, val acc: 0.9224  (best train acc: 0.8850, best val acc: 0.9373)\n",
      "[Epoch: 23060] train loss: 0.2926, train acc: 0.8720, val loss: 0.2519, val acc: 0.9319  (best train acc: 0.8850, best val acc: 0.9373)\n",
      "[Epoch: 23080] train loss: 0.3371, train acc: 0.8544, val loss: 0.2602, val acc: 0.9305  (best train acc: 0.8850, best val acc: 0.9373)\n",
      "[Epoch: 23100] train loss: 0.3218, train acc: 0.8693, val loss: 0.2689, val acc: 0.9329  (best train acc: 0.8850, best val acc: 0.9373)\n",
      "[Epoch: 23120] train loss: 0.3076, train acc: 0.8738, val loss: 0.2530, val acc: 0.9325  (best train acc: 0.8850, best val acc: 0.9373)\n",
      "[Epoch: 23140] train loss: 0.2992, train acc: 0.8720, val loss: 0.2622, val acc: 0.9379  (best train acc: 0.8850, best val acc: 0.9379)\n",
      "[Epoch: 23160] train loss: 0.3309, train acc: 0.8666, val loss: 0.2584, val acc: 0.9342  (best train acc: 0.8850, best val acc: 0.9383)\n",
      "[Epoch: 23180] train loss: 0.3275, train acc: 0.8619, val loss: 0.2601, val acc: 0.9268  (best train acc: 0.8850, best val acc: 0.9400)\n",
      "[Epoch: 23200] train loss: 0.3633, train acc: 0.8266, val loss: 0.2492, val acc: 0.9214  (best train acc: 0.8850, best val acc: 0.9400)\n",
      "[Epoch: 23220] train loss: 0.3063, train acc: 0.8694, val loss: 0.2539, val acc: 0.9272  (best train acc: 0.8850, best val acc: 0.9400)\n",
      "[Epoch: 23240] train loss: 0.3363, train acc: 0.8449, val loss: 0.2732, val acc: 0.9305  (best train acc: 0.8850, best val acc: 0.9400)\n",
      "[Epoch: 23260] train loss: 0.3159, train acc: 0.8657, val loss: 0.2639, val acc: 0.9285  (best train acc: 0.8850, best val acc: 0.9400)\n",
      "[Epoch: 23280] train loss: 0.3209, train acc: 0.8634, val loss: 0.2758, val acc: 0.9228  (best train acc: 0.8850, best val acc: 0.9400)\n",
      "[Epoch: 23300] train loss: 0.2923, train acc: 0.8740, val loss: 0.2593, val acc: 0.9325  (best train acc: 0.8850, best val acc: 0.9400)\n",
      "[Epoch: 23320] train loss: 0.3009, train acc: 0.8731, val loss: 0.2699, val acc: 0.9265  (best train acc: 0.8850, best val acc: 0.9400)\n",
      "[Epoch: 23340] train loss: 0.3502, train acc: 0.8446, val loss: 0.2513, val acc: 0.9332  (best train acc: 0.8850, best val acc: 0.9400)\n",
      "[Epoch: 23360] train loss: 0.3076, train acc: 0.8699, val loss: 0.2665, val acc: 0.9272  (best train acc: 0.8850, best val acc: 0.9400)\n",
      "[Epoch: 23380] train loss: 0.3254, train acc: 0.8623, val loss: 0.2575, val acc: 0.9386  (best train acc: 0.8850, best val acc: 0.9400)\n",
      "[Epoch: 23400] train loss: 0.3117, train acc: 0.8699, val loss: 0.2706, val acc: 0.9302  (best train acc: 0.8850, best val acc: 0.9400)\n",
      "[Epoch: 23420] train loss: 0.3221, train acc: 0.8691, val loss: 0.2613, val acc: 0.9191  (best train acc: 0.8850, best val acc: 0.9400)\n",
      "[Epoch: 23440] train loss: 0.3401, train acc: 0.8470, val loss: 0.2920, val acc: 0.9042  (best train acc: 0.8850, best val acc: 0.9400)\n",
      "[Epoch: 23460] train loss: 0.3102, train acc: 0.8680, val loss: 0.2606, val acc: 0.9410  (best train acc: 0.8850, best val acc: 0.9410)\n",
      "[Epoch: 23480] train loss: 0.3016, train acc: 0.8718, val loss: 0.2513, val acc: 0.9406  (best train acc: 0.8850, best val acc: 0.9440)\n",
      "[Epoch: 23500] train loss: 0.3275, train acc: 0.8619, val loss: 0.2627, val acc: 0.9265  (best train acc: 0.8850, best val acc: 0.9440)\n",
      "[Epoch: 23520] train loss: 0.3060, train acc: 0.8700, val loss: 0.2561, val acc: 0.9363  (best train acc: 0.8850, best val acc: 0.9440)\n",
      "[Epoch: 23540] train loss: 0.2933, train acc: 0.8695, val loss: 0.2650, val acc: 0.9315  (best train acc: 0.8850, best val acc: 0.9440)\n",
      "[Epoch: 23560] train loss: 0.3369, train acc: 0.8607, val loss: 0.2675, val acc: 0.9248  (best train acc: 0.8850, best val acc: 0.9440)\n",
      "[Epoch: 23580] train loss: 0.2976, train acc: 0.8790, val loss: 0.2456, val acc: 0.9376  (best train acc: 0.8850, best val acc: 0.9440)\n",
      "[Epoch: 23600] train loss: 0.3390, train acc: 0.8597, val loss: 0.2797, val acc: 0.9336  (best train acc: 0.8850, best val acc: 0.9440)\n",
      "[Epoch: 23620] train loss: 0.3255, train acc: 0.8652, val loss: 0.2663, val acc: 0.9390  (best train acc: 0.8850, best val acc: 0.9440)\n",
      "[Epoch: 23640] train loss: 0.3184, train acc: 0.8629, val loss: 0.2396, val acc: 0.9298  (best train acc: 0.8861, best val acc: 0.9440)\n",
      "[Epoch: 23660] train loss: 0.2856, train acc: 0.8799, val loss: 0.2779, val acc: 0.9339  (best train acc: 0.8861, best val acc: 0.9440)\n",
      "[Epoch: 23680] train loss: 0.3005, train acc: 0.8756, val loss: 0.2574, val acc: 0.9245  (best train acc: 0.8861, best val acc: 0.9440)\n",
      "[Epoch: 23700] train loss: 0.2965, train acc: 0.8726, val loss: 0.2561, val acc: 0.9356  (best train acc: 0.8861, best val acc: 0.9440)\n",
      "[Epoch: 23720] train loss: 0.3204, train acc: 0.8597, val loss: 0.2597, val acc: 0.9356  (best train acc: 0.8861, best val acc: 0.9440)\n",
      "[Epoch: 23740] train loss: 0.3066, train acc: 0.8724, val loss: 0.2499, val acc: 0.9265  (best train acc: 0.8861, best val acc: 0.9440)\n",
      "[Epoch: 23760] train loss: 0.3466, train acc: 0.8520, val loss: 0.2766, val acc: 0.9228  (best train acc: 0.8861, best val acc: 0.9440)\n",
      "[Epoch: 23780] train loss: 0.3215, train acc: 0.8664, val loss: 0.2639, val acc: 0.9298  (best train acc: 0.8861, best val acc: 0.9440)\n",
      "[Epoch: 23800] train loss: 0.3033, train acc: 0.8723, val loss: 0.2502, val acc: 0.9309  (best train acc: 0.8861, best val acc: 0.9440)\n",
      "[Epoch: 23820] train loss: 0.3250, train acc: 0.8431, val loss: 0.2615, val acc: 0.9076  (best train acc: 0.8861, best val acc: 0.9440)\n",
      "[Epoch: 23840] train loss: 0.2872, train acc: 0.8729, val loss: 0.2430, val acc: 0.9352  (best train acc: 0.8861, best val acc: 0.9440)\n",
      "[Epoch: 23860] train loss: 0.3388, train acc: 0.8500, val loss: 0.2541, val acc: 0.9359  (best train acc: 0.8861, best val acc: 0.9440)\n",
      "[Epoch: 23880] train loss: 0.3317, train acc: 0.8539, val loss: 0.2378, val acc: 0.9228  (best train acc: 0.8861, best val acc: 0.9440)\n",
      "[Epoch: 23900] train loss: 0.3552, train acc: 0.8439, val loss: 0.2480, val acc: 0.9261  (best train acc: 0.8861, best val acc: 0.9440)\n",
      "[Epoch: 23920] train loss: 0.3075, train acc: 0.8685, val loss: 0.2504, val acc: 0.9272  (best train acc: 0.8861, best val acc: 0.9440)\n",
      "[Epoch: 23940] train loss: 0.3179, train acc: 0.8570, val loss: 0.2697, val acc: 0.9211  (best train acc: 0.8861, best val acc: 0.9440)\n",
      "[Epoch: 23960] train loss: 0.3268, train acc: 0.8704, val loss: 0.2473, val acc: 0.9312  (best train acc: 0.8861, best val acc: 0.9440)\n",
      "[Epoch: 23980] train loss: 0.3340, train acc: 0.8581, val loss: 0.2890, val acc: 0.9241  (best train acc: 0.8861, best val acc: 0.9440)\n",
      "[Epoch: 24000] train loss: 0.3564, train acc: 0.8382, val loss: 0.2423, val acc: 0.9305  (best train acc: 0.8861, best val acc: 0.9440)\n",
      "[Epoch: 24020] train loss: 0.3235, train acc: 0.8583, val loss: 0.2588, val acc: 0.9396  (best train acc: 0.8861, best val acc: 0.9440)\n",
      "[Epoch: 24040] train loss: 0.2950, train acc: 0.8631, val loss: 0.2627, val acc: 0.9157  (best train acc: 0.8895, best val acc: 0.9440)\n",
      "[Epoch: 24060] train loss: 0.3013, train acc: 0.8676, val loss: 0.2609, val acc: 0.9265  (best train acc: 0.8895, best val acc: 0.9440)\n",
      "[Epoch: 24080] train loss: 0.3185, train acc: 0.8663, val loss: 0.2616, val acc: 0.9356  (best train acc: 0.8895, best val acc: 0.9447)\n",
      "[Epoch: 24100] train loss: 0.2936, train acc: 0.8752, val loss: 0.2707, val acc: 0.9285  (best train acc: 0.8895, best val acc: 0.9447)\n",
      "[Epoch: 24120] train loss: 0.2948, train acc: 0.8733, val loss: 0.2680, val acc: 0.9298  (best train acc: 0.8904, best val acc: 0.9447)\n",
      "[Epoch: 24140] train loss: 0.2988, train acc: 0.8780, val loss: 0.2556, val acc: 0.9393  (best train acc: 0.8904, best val acc: 0.9447)\n",
      "[Epoch: 24160] train loss: 0.3221, train acc: 0.8514, val loss: 0.2701, val acc: 0.9228  (best train acc: 0.8904, best val acc: 0.9447)\n",
      "[Epoch: 24180] train loss: 0.3129, train acc: 0.8677, val loss: 0.2641, val acc: 0.8938  (best train acc: 0.8904, best val acc: 0.9447)\n",
      "[Epoch: 24200] train loss: 0.3195, train acc: 0.8633, val loss: 0.2716, val acc: 0.9207  (best train acc: 0.8904, best val acc: 0.9447)\n",
      "[Epoch: 24220] train loss: 0.3363, train acc: 0.8559, val loss: 0.2686, val acc: 0.9073  (best train acc: 0.8904, best val acc: 0.9447)\n",
      "[Epoch: 24240] train loss: 0.3305, train acc: 0.8547, val loss: 0.2562, val acc: 0.9352  (best train acc: 0.8904, best val acc: 0.9447)\n",
      "[Epoch: 24260] train loss: 0.2971, train acc: 0.8734, val loss: 0.2684, val acc: 0.9427  (best train acc: 0.8904, best val acc: 0.9454)\n",
      "[Epoch: 24280] train loss: 0.3125, train acc: 0.8704, val loss: 0.2605, val acc: 0.9329  (best train acc: 0.8904, best val acc: 0.9457)\n",
      "[Epoch: 24300] train loss: 0.2907, train acc: 0.8791, val loss: 0.2500, val acc: 0.9282  (best train acc: 0.8904, best val acc: 0.9457)\n",
      "[Epoch: 24320] train loss: 0.3080, train acc: 0.8725, val loss: 0.2850, val acc: 0.9282  (best train acc: 0.8904, best val acc: 0.9457)\n",
      "[Epoch: 24340] train loss: 0.3195, train acc: 0.8665, val loss: 0.2678, val acc: 0.9383  (best train acc: 0.8904, best val acc: 0.9457)\n",
      "[Epoch: 24360] train loss: 0.3053, train acc: 0.8799, val loss: 0.2667, val acc: 0.9369  (best train acc: 0.8904, best val acc: 0.9457)\n",
      "[Epoch: 24380] train loss: 0.3231, train acc: 0.8635, val loss: 0.2691, val acc: 0.9309  (best train acc: 0.8904, best val acc: 0.9460)\n",
      "[Epoch: 24400] train loss: 0.2986, train acc: 0.8799, val loss: 0.2704, val acc: 0.9265  (best train acc: 0.8904, best val acc: 0.9460)\n",
      "[Epoch: 24420] train loss: 0.3326, train acc: 0.8562, val loss: 0.2693, val acc: 0.9218  (best train acc: 0.8904, best val acc: 0.9460)\n",
      "[Epoch: 24440] train loss: 0.3525, train acc: 0.8453, val loss: 0.2614, val acc: 0.9332  (best train acc: 0.8904, best val acc: 0.9460)\n",
      "[Epoch: 24460] train loss: 0.3488, train acc: 0.8455, val loss: 0.2542, val acc: 0.9298  (best train acc: 0.8904, best val acc: 0.9460)\n",
      "[Epoch: 24480] train loss: 0.3229, train acc: 0.8647, val loss: 0.2624, val acc: 0.9170  (best train acc: 0.8904, best val acc: 0.9460)\n",
      "[Epoch: 24500] train loss: 0.2943, train acc: 0.8732, val loss: 0.2558, val acc: 0.9228  (best train acc: 0.8904, best val acc: 0.9460)\n",
      "[Epoch: 24520] train loss: 0.3121, train acc: 0.8699, val loss: 0.2580, val acc: 0.9214  (best train acc: 0.8904, best val acc: 0.9460)\n",
      "[Epoch: 24540] train loss: 0.3947, train acc: 0.8261, val loss: 0.3013, val acc: 0.9059  (best train acc: 0.8904, best val acc: 0.9460)\n",
      "[Epoch: 24560] train loss: 0.4517, train acc: 0.7809, val loss: 0.2828, val acc: 0.9187  (best train acc: 0.8904, best val acc: 0.9460)\n",
      "[Epoch: 24580] train loss: 0.3117, train acc: 0.8738, val loss: 0.2318, val acc: 0.9248  (best train acc: 0.8904, best val acc: 0.9460)\n",
      "[Epoch: 24600] train loss: 0.3283, train acc: 0.8634, val loss: 0.2744, val acc: 0.9126  (best train acc: 0.8904, best val acc: 0.9460)\n",
      "[Epoch: 24620] train loss: 0.2998, train acc: 0.8717, val loss: 0.2404, val acc: 0.9332  (best train acc: 0.8904, best val acc: 0.9460)\n",
      "[Epoch: 24640] train loss: 0.3017, train acc: 0.8643, val loss: 0.2699, val acc: 0.9106  (best train acc: 0.8904, best val acc: 0.9460)\n",
      "[Epoch: 24660] train loss: 0.2981, train acc: 0.8726, val loss: 0.2532, val acc: 0.9427  (best train acc: 0.8904, best val acc: 0.9460)\n",
      "[Epoch: 24680] train loss: 0.3131, train acc: 0.8713, val loss: 0.2492, val acc: 0.9352  (best train acc: 0.8904, best val acc: 0.9460)\n",
      "[Epoch: 24700] train loss: 0.2946, train acc: 0.8787, val loss: 0.2473, val acc: 0.9359  (best train acc: 0.8904, best val acc: 0.9460)\n",
      "[Epoch: 24720] train loss: 0.3020, train acc: 0.8766, val loss: 0.2440, val acc: 0.9369  (best train acc: 0.8904, best val acc: 0.9460)\n",
      "[Epoch: 24740] train loss: 0.3004, train acc: 0.8806, val loss: 0.2293, val acc: 0.9363  (best train acc: 0.8904, best val acc: 0.9460)\n",
      "[Epoch: 24760] train loss: 0.2984, train acc: 0.8676, val loss: 0.2522, val acc: 0.9319  (best train acc: 0.8904, best val acc: 0.9460)\n",
      "[Epoch: 24780] train loss: 0.3357, train acc: 0.8492, val loss: 0.2458, val acc: 0.9437  (best train acc: 0.8904, best val acc: 0.9460)\n",
      "[Epoch: 24800] train loss: 0.3037, train acc: 0.8793, val loss: 0.2369, val acc: 0.9393  (best train acc: 0.8904, best val acc: 0.9460)\n",
      "[Epoch: 24820] train loss: 0.2817, train acc: 0.8778, val loss: 0.2524, val acc: 0.9356  (best train acc: 0.8904, best val acc: 0.9460)\n",
      "[Epoch: 24840] train loss: 0.3085, train acc: 0.8718, val loss: 0.2626, val acc: 0.9285  (best train acc: 0.8904, best val acc: 0.9460)\n",
      "[Epoch: 24860] train loss: 0.2917, train acc: 0.8738, val loss: 0.2638, val acc: 0.9248  (best train acc: 0.8904, best val acc: 0.9460)\n",
      "[Epoch: 24880] train loss: 0.3459, train acc: 0.8526, val loss: 0.2355, val acc: 0.9339  (best train acc: 0.8904, best val acc: 0.9460)\n",
      "[Epoch: 24900] train loss: 0.2954, train acc: 0.8805, val loss: 0.2689, val acc: 0.9325  (best train acc: 0.8904, best val acc: 0.9460)\n",
      "[Epoch: 24920] train loss: 0.3291, train acc: 0.8584, val loss: 0.2504, val acc: 0.9332  (best train acc: 0.8904, best val acc: 0.9460)\n",
      "[Epoch: 24940] train loss: 0.3028, train acc: 0.8697, val loss: 0.2505, val acc: 0.9379  (best train acc: 0.8904, best val acc: 0.9460)\n",
      "[Epoch: 24960] train loss: 0.3577, train acc: 0.8460, val loss: 0.2522, val acc: 0.9329  (best train acc: 0.8904, best val acc: 0.9460)\n",
      "[Epoch: 24980] train loss: 0.3009, train acc: 0.8725, val loss: 0.2493, val acc: 0.9356  (best train acc: 0.8904, best val acc: 0.9460)\n",
      "[Epoch: 25000] train loss: 0.3229, train acc: 0.8569, val loss: 0.2386, val acc: 0.9214  (best train acc: 0.8904, best val acc: 0.9460)\n",
      "[Epoch: 25020] train loss: 0.2957, train acc: 0.8788, val loss: 0.2477, val acc: 0.9282  (best train acc: 0.8904, best val acc: 0.9460)\n",
      "[Epoch: 25040] train loss: 0.3214, train acc: 0.8590, val loss: 0.2728, val acc: 0.9282  (best train acc: 0.8904, best val acc: 0.9460)\n",
      "[Epoch: 25060] train loss: 0.3355, train acc: 0.8550, val loss: 0.2563, val acc: 0.9403  (best train acc: 0.8904, best val acc: 0.9460)\n",
      "[Epoch: 25080] train loss: 0.3146, train acc: 0.8657, val loss: 0.2644, val acc: 0.9319  (best train acc: 0.8904, best val acc: 0.9460)\n",
      "[Epoch: 25100] train loss: 0.3051, train acc: 0.8713, val loss: 0.2372, val acc: 0.9346  (best train acc: 0.8904, best val acc: 0.9460)\n",
      "[Epoch: 25120] train loss: 0.3140, train acc: 0.8671, val loss: 0.2392, val acc: 0.9339  (best train acc: 0.8904, best val acc: 0.9460)\n",
      "[Epoch: 25140] train loss: 0.2917, train acc: 0.8745, val loss: 0.2621, val acc: 0.9272  (best train acc: 0.8904, best val acc: 0.9460)\n",
      "[Epoch: 25160] train loss: 0.2871, train acc: 0.8783, val loss: 0.2690, val acc: 0.9194  (best train acc: 0.8904, best val acc: 0.9460)\n",
      "[Epoch: 25180] train loss: 0.3145, train acc: 0.8637, val loss: 0.2332, val acc: 0.9272  (best train acc: 0.8904, best val acc: 0.9460)\n",
      "[Epoch: 25200] train loss: 0.2955, train acc: 0.8714, val loss: 0.2588, val acc: 0.9339  (best train acc: 0.8904, best val acc: 0.9460)\n",
      "[Epoch: 25220] train loss: 0.3127, train acc: 0.8610, val loss: 0.2417, val acc: 0.9352  (best train acc: 0.8904, best val acc: 0.9460)\n",
      "[Epoch: 25240] train loss: 0.3210, train acc: 0.8673, val loss: 0.2342, val acc: 0.9346  (best train acc: 0.8904, best val acc: 0.9460)\n",
      "[Epoch: 25260] train loss: 0.3343, train acc: 0.8550, val loss: 0.2484, val acc: 0.9359  (best train acc: 0.8904, best val acc: 0.9460)\n",
      "[Epoch: 25280] train loss: 0.2962, train acc: 0.8788, val loss: 0.2385, val acc: 0.9312  (best train acc: 0.8904, best val acc: 0.9460)\n",
      "[Epoch: 25300] train loss: 0.3114, train acc: 0.8673, val loss: 0.2293, val acc: 0.9325  (best train acc: 0.8904, best val acc: 0.9460)\n",
      "[Epoch: 25320] train loss: 0.3386, train acc: 0.8496, val loss: 0.2534, val acc: 0.9164  (best train acc: 0.8904, best val acc: 0.9460)\n",
      "[Epoch: 25340] train loss: 0.3257, train acc: 0.8571, val loss: 0.2483, val acc: 0.9319  (best train acc: 0.8904, best val acc: 0.9460)\n",
      "[Epoch: 25360] train loss: 0.3156, train acc: 0.8710, val loss: 0.2407, val acc: 0.9369  (best train acc: 0.8904, best val acc: 0.9460)\n",
      "[Epoch: 25380] train loss: 0.3201, train acc: 0.8642, val loss: 0.2457, val acc: 0.9245  (best train acc: 0.8904, best val acc: 0.9460)\n",
      "[Epoch: 25400] train loss: 0.3506, train acc: 0.8592, val loss: 0.2602, val acc: 0.9126  (best train acc: 0.8904, best val acc: 0.9460)\n",
      "[Epoch: 25420] train loss: 0.3286, train acc: 0.8624, val loss: 0.2613, val acc: 0.9346  (best train acc: 0.8904, best val acc: 0.9460)\n",
      "[Epoch: 25440] train loss: 0.3466, train acc: 0.8344, val loss: 0.2573, val acc: 0.9342  (best train acc: 0.8904, best val acc: 0.9460)\n",
      "[Epoch: 25460] train loss: 0.2927, train acc: 0.8709, val loss: 0.2645, val acc: 0.9241  (best train acc: 0.8904, best val acc: 0.9460)\n",
      "[Epoch: 25480] train loss: 0.3192, train acc: 0.8735, val loss: 0.2463, val acc: 0.9265  (best train acc: 0.8904, best val acc: 0.9460)\n",
      "[Epoch: 25500] train loss: 0.3506, train acc: 0.8488, val loss: 0.2484, val acc: 0.9221  (best train acc: 0.8904, best val acc: 0.9460)\n",
      "[Epoch: 25520] train loss: 0.3030, train acc: 0.8790, val loss: 0.2557, val acc: 0.9403  (best train acc: 0.8904, best val acc: 0.9460)\n",
      "[Epoch: 25540] train loss: 0.2956, train acc: 0.8724, val loss: 0.2509, val acc: 0.9352  (best train acc: 0.8904, best val acc: 0.9460)\n",
      "[Epoch: 25560] train loss: 0.2845, train acc: 0.8749, val loss: 0.2554, val acc: 0.9285  (best train acc: 0.8904, best val acc: 0.9460)\n",
      "[Epoch: 25580] train loss: 0.2905, train acc: 0.8813, val loss: 0.2439, val acc: 0.9363  (best train acc: 0.8909, best val acc: 0.9460)\n",
      "[Epoch: 25600] train loss: 0.2737, train acc: 0.8856, val loss: 0.2475, val acc: 0.9309  (best train acc: 0.8909, best val acc: 0.9460)\n",
      "[Epoch: 25620] train loss: 0.3664, train acc: 0.8307, val loss: 0.2583, val acc: 0.9309  (best train acc: 0.8909, best val acc: 0.9460)\n",
      "[Epoch: 25640] train loss: 0.3271, train acc: 0.8523, val loss: 0.2521, val acc: 0.9295  (best train acc: 0.8909, best val acc: 0.9460)\n",
      "[Epoch: 25660] train loss: 0.2808, train acc: 0.8823, val loss: 0.2609, val acc: 0.9332  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 25680] train loss: 0.2863, train acc: 0.8804, val loss: 0.2775, val acc: 0.9305  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 25700] train loss: 0.2964, train acc: 0.8763, val loss: 0.2530, val acc: 0.9369  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 25720] train loss: 0.2803, train acc: 0.8797, val loss: 0.2624, val acc: 0.9228  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 25740] train loss: 0.3175, train acc: 0.8649, val loss: 0.2468, val acc: 0.9379  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 25760] train loss: 0.2782, train acc: 0.8854, val loss: 0.2564, val acc: 0.9298  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 25780] train loss: 0.3448, train acc: 0.8561, val loss: 0.3025, val acc: 0.9272  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 25800] train loss: 0.3555, train acc: 0.8378, val loss: 0.2302, val acc: 0.9282  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 25820] train loss: 0.3069, train acc: 0.8723, val loss: 0.2393, val acc: 0.9383  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 25840] train loss: 0.3235, train acc: 0.8678, val loss: 0.2627, val acc: 0.9295  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 25860] train loss: 0.3079, train acc: 0.8809, val loss: 0.2533, val acc: 0.9319  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 25880] train loss: 0.3578, train acc: 0.8469, val loss: 0.2925, val acc: 0.9116  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 25900] train loss: 0.3394, train acc: 0.8502, val loss: 0.2432, val acc: 0.9295  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 25920] train loss: 0.3014, train acc: 0.8693, val loss: 0.2564, val acc: 0.9339  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 25940] train loss: 0.3032, train acc: 0.8689, val loss: 0.2513, val acc: 0.9298  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 25960] train loss: 0.2795, train acc: 0.8810, val loss: 0.2556, val acc: 0.9305  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 25980] train loss: 0.2842, train acc: 0.8812, val loss: 0.2541, val acc: 0.9400  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 26000] train loss: 0.3366, train acc: 0.8604, val loss: 0.2731, val acc: 0.9315  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 26020] train loss: 0.2920, train acc: 0.8773, val loss: 0.2440, val acc: 0.9369  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 26040] train loss: 0.3041, train acc: 0.8772, val loss: 0.2414, val acc: 0.9363  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 26060] train loss: 0.3017, train acc: 0.8782, val loss: 0.2505, val acc: 0.9393  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 26080] train loss: 0.3566, train acc: 0.8460, val loss: 0.2383, val acc: 0.9379  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 26100] train loss: 0.2916, train acc: 0.8762, val loss: 0.2321, val acc: 0.9383  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 26120] train loss: 0.3070, train acc: 0.8741, val loss: 0.2235, val acc: 0.9393  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 26140] train loss: 0.3117, train acc: 0.8681, val loss: 0.2680, val acc: 0.9352  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 26160] train loss: 0.3547, train acc: 0.8458, val loss: 0.2552, val acc: 0.9309  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 26180] train loss: 0.3959, train acc: 0.8363, val loss: 0.2745, val acc: 0.9143  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 26200] train loss: 0.3474, train acc: 0.8545, val loss: 0.2428, val acc: 0.9346  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 26220] train loss: 0.3037, train acc: 0.8741, val loss: 0.2542, val acc: 0.9373  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 26240] train loss: 0.2999, train acc: 0.8726, val loss: 0.2539, val acc: 0.9390  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 26260] train loss: 0.3250, train acc: 0.8625, val loss: 0.2430, val acc: 0.9400  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 26280] train loss: 0.3298, train acc: 0.8638, val loss: 0.2499, val acc: 0.9366  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 26300] train loss: 0.3012, train acc: 0.8706, val loss: 0.2478, val acc: 0.9339  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 26320] train loss: 0.2983, train acc: 0.8690, val loss: 0.2561, val acc: 0.9214  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 26340] train loss: 0.3095, train acc: 0.8670, val loss: 0.2737, val acc: 0.9298  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 26360] train loss: 0.2870, train acc: 0.8762, val loss: 0.2743, val acc: 0.9298  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 26380] train loss: 0.3225, train acc: 0.8676, val loss: 0.2437, val acc: 0.9329  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 26400] train loss: 0.3129, train acc: 0.8701, val loss: 0.2757, val acc: 0.9164  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 26420] train loss: 0.2883, train acc: 0.8787, val loss: 0.2579, val acc: 0.9332  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 26440] train loss: 0.2814, train acc: 0.8854, val loss: 0.2383, val acc: 0.9373  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 26460] train loss: 0.2942, train acc: 0.8798, val loss: 0.2523, val acc: 0.9366  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 26480] train loss: 0.3350, train acc: 0.8657, val loss: 0.2786, val acc: 0.9251  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 26500] train loss: 0.3235, train acc: 0.8582, val loss: 0.2558, val acc: 0.9390  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 26520] train loss: 0.3231, train acc: 0.8630, val loss: 0.2269, val acc: 0.9278  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 26540] train loss: 0.3429, train acc: 0.8642, val loss: 0.2884, val acc: 0.9228  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 26560] train loss: 0.3454, train acc: 0.8508, val loss: 0.2791, val acc: 0.9336  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 26580] train loss: 0.3087, train acc: 0.8744, val loss: 0.2482, val acc: 0.9349  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 26600] train loss: 0.3154, train acc: 0.8728, val loss: 0.2869, val acc: 0.9298  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 26620] train loss: 0.3360, train acc: 0.8434, val loss: 0.2614, val acc: 0.9272  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 26640] train loss: 0.3065, train acc: 0.8728, val loss: 0.2202, val acc: 0.9390  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 26660] train loss: 0.3154, train acc: 0.8775, val loss: 0.2455, val acc: 0.9275  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 26680] train loss: 0.3223, train acc: 0.8586, val loss: 0.2600, val acc: 0.9332  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 26700] train loss: 0.3074, train acc: 0.8743, val loss: 0.2532, val acc: 0.9221  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 26720] train loss: 0.3174, train acc: 0.8629, val loss: 0.2156, val acc: 0.9363  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 26740] train loss: 0.3267, train acc: 0.8612, val loss: 0.2513, val acc: 0.9363  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 26760] train loss: 0.3107, train acc: 0.8704, val loss: 0.2841, val acc: 0.9248  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 26780] train loss: 0.3004, train acc: 0.8735, val loss: 0.2444, val acc: 0.9261  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 26800] train loss: 0.3316, train acc: 0.8655, val loss: 0.2827, val acc: 0.9298  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 26820] train loss: 0.3657, train acc: 0.8404, val loss: 0.2484, val acc: 0.9305  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 26840] train loss: 0.3374, train acc: 0.8621, val loss: 0.2577, val acc: 0.9332  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 26860] train loss: 0.2846, train acc: 0.8736, val loss: 0.2562, val acc: 0.9174  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 26880] train loss: 0.3017, train acc: 0.8743, val loss: 0.2442, val acc: 0.9396  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 26900] train loss: 0.2874, train acc: 0.8809, val loss: 0.2523, val acc: 0.9305  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 26920] train loss: 0.3337, train acc: 0.8547, val loss: 0.2420, val acc: 0.9396  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 26940] train loss: 0.2961, train acc: 0.8749, val loss: 0.2451, val acc: 0.9406  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 26960] train loss: 0.2955, train acc: 0.8694, val loss: 0.2669, val acc: 0.9285  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 26980] train loss: 0.3264, train acc: 0.8626, val loss: 0.2446, val acc: 0.9359  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 27000] train loss: 0.3148, train acc: 0.8712, val loss: 0.2504, val acc: 0.9336  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 27020] train loss: 0.2950, train acc: 0.8746, val loss: 0.2648, val acc: 0.9356  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 27040] train loss: 0.3655, train acc: 0.8546, val loss: 0.2713, val acc: 0.9238  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 27060] train loss: 0.3352, train acc: 0.8587, val loss: 0.2596, val acc: 0.9255  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 27080] train loss: 0.2932, train acc: 0.8763, val loss: 0.2465, val acc: 0.9400  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 27100] train loss: 0.3404, train acc: 0.8626, val loss: 0.3066, val acc: 0.9130  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 27120] train loss: 0.3232, train acc: 0.8660, val loss: 0.2646, val acc: 0.9315  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 27140] train loss: 0.3526, train acc: 0.8443, val loss: 0.2419, val acc: 0.9312  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 27160] train loss: 0.2845, train acc: 0.8832, val loss: 0.2508, val acc: 0.9349  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 27180] train loss: 0.3133, train acc: 0.8717, val loss: 0.2440, val acc: 0.9376  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 27200] train loss: 0.3508, train acc: 0.8494, val loss: 0.2386, val acc: 0.9292  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 27220] train loss: 0.3033, train acc: 0.8698, val loss: 0.2989, val acc: 0.9201  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 27240] train loss: 0.2825, train acc: 0.8887, val loss: 0.2424, val acc: 0.9379  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 27260] train loss: 0.3140, train acc: 0.8679, val loss: 0.2496, val acc: 0.9369  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 27280] train loss: 0.3330, train acc: 0.8543, val loss: 0.2582, val acc: 0.9245  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 27300] train loss: 0.3129, train acc: 0.8645, val loss: 0.2471, val acc: 0.9305  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 27320] train loss: 0.2800, train acc: 0.8845, val loss: 0.2636, val acc: 0.9336  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 27340] train loss: 0.2936, train acc: 0.8736, val loss: 0.2462, val acc: 0.9352  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 27360] train loss: 0.2893, train acc: 0.8790, val loss: 0.2911, val acc: 0.9207  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 27380] train loss: 0.3388, train acc: 0.8553, val loss: 0.2556, val acc: 0.9288  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 27400] train loss: 0.3056, train acc: 0.8655, val loss: 0.2475, val acc: 0.9403  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 27420] train loss: 0.3049, train acc: 0.8718, val loss: 0.2737, val acc: 0.9295  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 27440] train loss: 0.3020, train acc: 0.8716, val loss: 0.2317, val acc: 0.9373  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 27460] train loss: 0.4103, train acc: 0.8001, val loss: 0.2481, val acc: 0.8934  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 27480] train loss: 0.3020, train acc: 0.8699, val loss: 0.2430, val acc: 0.9234  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 27500] train loss: 0.2857, train acc: 0.8710, val loss: 0.2465, val acc: 0.9359  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 27520] train loss: 0.2735, train acc: 0.8827, val loss: 0.2708, val acc: 0.9207  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 27540] train loss: 0.2700, train acc: 0.8864, val loss: 0.2348, val acc: 0.9406  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 27560] train loss: 0.3226, train acc: 0.8514, val loss: 0.2289, val acc: 0.9379  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 27580] train loss: 0.3138, train acc: 0.8633, val loss: 0.2460, val acc: 0.9325  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 27600] train loss: 0.2803, train acc: 0.8871, val loss: 0.2357, val acc: 0.9386  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 27620] train loss: 0.3048, train acc: 0.8730, val loss: 0.2203, val acc: 0.9390  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 27640] train loss: 0.2608, train acc: 0.8903, val loss: 0.2360, val acc: 0.9369  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 27660] train loss: 0.2819, train acc: 0.8785, val loss: 0.2355, val acc: 0.9336  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 27680] train loss: 0.3233, train acc: 0.8679, val loss: 0.2500, val acc: 0.9302  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 27700] train loss: 0.3157, train acc: 0.8511, val loss: 0.2586, val acc: 0.9224  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 27720] train loss: 0.4181, train acc: 0.8380, val loss: 0.2464, val acc: 0.9295  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 27740] train loss: 0.2777, train acc: 0.8761, val loss: 0.2591, val acc: 0.9319  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 27760] train loss: 0.2696, train acc: 0.8858, val loss: 0.2399, val acc: 0.9302  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 27780] train loss: 0.2903, train acc: 0.8762, val loss: 0.2304, val acc: 0.9413  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 27800] train loss: 0.2677, train acc: 0.8836, val loss: 0.2371, val acc: 0.9211  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 27820] train loss: 0.2913, train acc: 0.8736, val loss: 0.2460, val acc: 0.9255  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 27840] train loss: 0.2807, train acc: 0.8774, val loss: 0.2553, val acc: 0.9272  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 27860] train loss: 0.2779, train acc: 0.8765, val loss: 0.2337, val acc: 0.9218  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 27880] train loss: 0.2920, train acc: 0.8740, val loss: 0.2407, val acc: 0.9349  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 27900] train loss: 0.2867, train acc: 0.8789, val loss: 0.2479, val acc: 0.9329  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 27920] train loss: 0.2959, train acc: 0.8720, val loss: 0.2386, val acc: 0.9265  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 27940] train loss: 0.3000, train acc: 0.8702, val loss: 0.2346, val acc: 0.9319  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 27960] train loss: 0.2663, train acc: 0.8821, val loss: 0.2514, val acc: 0.9295  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 27980] train loss: 0.2890, train acc: 0.8757, val loss: 0.2382, val acc: 0.9336  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 28000] train loss: 0.2928, train acc: 0.8678, val loss: 0.2367, val acc: 0.9298  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 28020] train loss: 0.3052, train acc: 0.8711, val loss: 0.2459, val acc: 0.9295  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 28040] train loss: 0.3518, train acc: 0.8542, val loss: 0.2412, val acc: 0.9106  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 28060] train loss: 0.2859, train acc: 0.8858, val loss: 0.2376, val acc: 0.9126  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 28080] train loss: 0.2938, train acc: 0.8709, val loss: 0.2773, val acc: 0.9231  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 28100] train loss: 0.3069, train acc: 0.8685, val loss: 0.2354, val acc: 0.9329  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 28120] train loss: 0.2696, train acc: 0.8871, val loss: 0.2326, val acc: 0.9366  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 28140] train loss: 0.3087, train acc: 0.8629, val loss: 0.2282, val acc: 0.9298  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 28160] train loss: 0.2842, train acc: 0.8768, val loss: 0.2433, val acc: 0.9282  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 28180] train loss: 0.2750, train acc: 0.8819, val loss: 0.2845, val acc: 0.9315  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 28200] train loss: 0.2703, train acc: 0.8858, val loss: 0.2219, val acc: 0.9245  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 28220] train loss: 0.2837, train acc: 0.8742, val loss: 0.2244, val acc: 0.9329  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 28240] train loss: 0.3157, train acc: 0.8675, val loss: 0.2943, val acc: 0.9029  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 28260] train loss: 0.2862, train acc: 0.8750, val loss: 0.2348, val acc: 0.9268  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 28280] train loss: 0.2754, train acc: 0.8844, val loss: 0.2230, val acc: 0.9383  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 28300] train loss: 0.2725, train acc: 0.8835, val loss: 0.2861, val acc: 0.9272  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 28320] train loss: 0.2827, train acc: 0.8785, val loss: 0.2531, val acc: 0.9356  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 28340] train loss: 0.2757, train acc: 0.8871, val loss: 0.2252, val acc: 0.9363  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 28360] train loss: 0.2894, train acc: 0.8754, val loss: 0.2691, val acc: 0.9234  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 28380] train loss: 0.2757, train acc: 0.8775, val loss: 0.2346, val acc: 0.9393  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 28400] train loss: 0.2792, train acc: 0.8827, val loss: 0.2332, val acc: 0.9376  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 28420] train loss: 0.2856, train acc: 0.8815, val loss: 0.2299, val acc: 0.9403  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 28440] train loss: 0.2783, train acc: 0.8767, val loss: 0.2489, val acc: 0.9288  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 28460] train loss: 0.2732, train acc: 0.8772, val loss: 0.2465, val acc: 0.9248  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 28480] train loss: 0.3202, train acc: 0.8632, val loss: 0.2276, val acc: 0.9346  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 28500] train loss: 0.2784, train acc: 0.8861, val loss: 0.2154, val acc: 0.9349  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 28520] train loss: 0.3242, train acc: 0.8583, val loss: 0.2114, val acc: 0.9363  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 28540] train loss: 0.2752, train acc: 0.8864, val loss: 0.2421, val acc: 0.9231  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 28560] train loss: 0.2936, train acc: 0.8780, val loss: 0.2845, val acc: 0.9245  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 28580] train loss: 0.3117, train acc: 0.8683, val loss: 0.2288, val acc: 0.9315  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 28600] train loss: 0.3166, train acc: 0.8475, val loss: 0.2223, val acc: 0.9346  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 28620] train loss: 0.3099, train acc: 0.8613, val loss: 0.2648, val acc: 0.9039  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 28640] train loss: 0.2926, train acc: 0.8678, val loss: 0.2441, val acc: 0.9177  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 28660] train loss: 0.3103, train acc: 0.8741, val loss: 0.2657, val acc: 0.9268  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 28680] train loss: 0.2731, train acc: 0.8843, val loss: 0.2227, val acc: 0.9359  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 28700] train loss: 0.3070, train acc: 0.8686, val loss: 0.2797, val acc: 0.9204  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 28720] train loss: 0.2865, train acc: 0.8810, val loss: 0.2185, val acc: 0.9356  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 28740] train loss: 0.2775, train acc: 0.8850, val loss: 0.2391, val acc: 0.9336  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 28760] train loss: 0.2571, train acc: 0.8922, val loss: 0.2367, val acc: 0.9379  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 28780] train loss: 0.2853, train acc: 0.8802, val loss: 0.2182, val acc: 0.9386  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 28800] train loss: 0.3132, train acc: 0.8725, val loss: 0.2228, val acc: 0.9396  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 28820] train loss: 0.2963, train acc: 0.8704, val loss: 0.2477, val acc: 0.9241  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 28840] train loss: 0.2869, train acc: 0.8801, val loss: 0.2435, val acc: 0.9261  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 28860] train loss: 0.2946, train acc: 0.8778, val loss: 0.2339, val acc: 0.9319  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 28880] train loss: 0.2769, train acc: 0.8780, val loss: 0.2486, val acc: 0.9288  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 28900] train loss: 0.3246, train acc: 0.8692, val loss: 0.2254, val acc: 0.9325  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 28920] train loss: 0.2888, train acc: 0.8796, val loss: 0.2390, val acc: 0.9342  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 28940] train loss: 0.2649, train acc: 0.8819, val loss: 0.2243, val acc: 0.9346  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 28960] train loss: 0.2729, train acc: 0.8822, val loss: 0.2589, val acc: 0.9278  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 28980] train loss: 0.2953, train acc: 0.8814, val loss: 0.2374, val acc: 0.9336  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 29000] train loss: 0.2819, train acc: 0.8730, val loss: 0.2543, val acc: 0.9228  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 29020] train loss: 0.2807, train acc: 0.8845, val loss: 0.2381, val acc: 0.9346  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 29040] train loss: 0.2937, train acc: 0.8632, val loss: 0.2562, val acc: 0.9059  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 29060] train loss: 0.2688, train acc: 0.8892, val loss: 0.2249, val acc: 0.9319  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 29080] train loss: 0.2857, train acc: 0.8803, val loss: 0.2206, val acc: 0.9245  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 29100] train loss: 0.5188, train acc: 0.8083, val loss: 0.2522, val acc: 0.9046  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 29120] train loss: 0.4610, train acc: 0.8068, val loss: 0.2341, val acc: 0.9126  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 29140] train loss: 0.3414, train acc: 0.8485, val loss: 0.2566, val acc: 0.9170  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 29160] train loss: 0.3249, train acc: 0.8561, val loss: 0.2362, val acc: 0.9369  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 29180] train loss: 0.3103, train acc: 0.8738, val loss: 0.2166, val acc: 0.9339  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 29200] train loss: 0.2851, train acc: 0.8777, val loss: 0.2349, val acc: 0.9336  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 29220] train loss: 0.2930, train acc: 0.8731, val loss: 0.2351, val acc: 0.9356  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 29240] train loss: 0.2720, train acc: 0.8831, val loss: 0.2352, val acc: 0.9363  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 29260] train loss: 0.3269, train acc: 0.8574, val loss: 0.2325, val acc: 0.9285  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 29280] train loss: 0.3068, train acc: 0.8741, val loss: 0.2271, val acc: 0.9352  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 29300] train loss: 0.2994, train acc: 0.8781, val loss: 0.2367, val acc: 0.9366  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 29320] train loss: 0.3038, train acc: 0.8767, val loss: 0.2288, val acc: 0.9346  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 29340] train loss: 0.3052, train acc: 0.8682, val loss: 0.2530, val acc: 0.9352  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 29360] train loss: 0.2914, train acc: 0.8710, val loss: 0.2700, val acc: 0.9305  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 29380] train loss: 0.2925, train acc: 0.8751, val loss: 0.2266, val acc: 0.9366  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 29400] train loss: 0.2856, train acc: 0.8693, val loss: 0.2657, val acc: 0.9197  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 29420] train loss: 0.3005, train acc: 0.8832, val loss: 0.2318, val acc: 0.9025  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 29440] train loss: 0.2880, train acc: 0.8772, val loss: 0.2463, val acc: 0.9238  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 29460] train loss: 0.3110, train acc: 0.8563, val loss: 0.2321, val acc: 0.9275  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 29480] train loss: 0.2722, train acc: 0.8834, val loss: 0.2334, val acc: 0.9275  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 29500] train loss: 0.2761, train acc: 0.8837, val loss: 0.2308, val acc: 0.9352  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 29520] train loss: 0.2895, train acc: 0.8694, val loss: 0.2367, val acc: 0.9312  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 29540] train loss: 0.2683, train acc: 0.8900, val loss: 0.2402, val acc: 0.9292  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 29560] train loss: 0.2939, train acc: 0.8722, val loss: 0.2377, val acc: 0.9363  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 29580] train loss: 0.2726, train acc: 0.8848, val loss: 0.2557, val acc: 0.9342  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 29600] train loss: 0.2897, train acc: 0.8801, val loss: 0.2195, val acc: 0.9248  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 29620] train loss: 0.2937, train acc: 0.8778, val loss: 0.2492, val acc: 0.9032  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 29640] train loss: 0.3042, train acc: 0.8632, val loss: 0.2235, val acc: 0.9349  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 29660] train loss: 0.2831, train acc: 0.8785, val loss: 0.2289, val acc: 0.9258  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 29680] train loss: 0.2997, train acc: 0.8720, val loss: 0.2424, val acc: 0.9302  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 29700] train loss: 0.2858, train acc: 0.8764, val loss: 0.2438, val acc: 0.9359  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 29720] train loss: 0.3291, train acc: 0.8640, val loss: 0.2567, val acc: 0.9329  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 29740] train loss: 0.3379, train acc: 0.8516, val loss: 0.2471, val acc: 0.9329  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 29760] train loss: 0.2766, train acc: 0.8860, val loss: 0.2238, val acc: 0.9123  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 29780] train loss: 0.3543, train acc: 0.8388, val loss: 0.2409, val acc: 0.9062  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 29800] train loss: 0.2737, train acc: 0.8813, val loss: 0.2878, val acc: 0.9123  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 29820] train loss: 0.2926, train acc: 0.8819, val loss: 0.2497, val acc: 0.9268  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 29840] train loss: 0.2610, train acc: 0.8841, val loss: 0.2366, val acc: 0.9295  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 29860] train loss: 0.2942, train acc: 0.8748, val loss: 0.2797, val acc: 0.9285  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 29880] train loss: 0.2934, train acc: 0.8772, val loss: 0.2268, val acc: 0.9309  (best train acc: 0.8962, best val acc: 0.9460)\n",
      "[Epoch: 29900] train loss: 0.3553, train acc: 0.8284, val loss: 0.2347, val acc: 0.9275  (best train acc: 0.8966, best val acc: 0.9460)\n",
      "[Epoch: 29920] train loss: 0.2830, train acc: 0.8840, val loss: 0.2364, val acc: 0.9143  (best train acc: 0.8981, best val acc: 0.9460)\n",
      "[Epoch: 29940] train loss: 0.2733, train acc: 0.8835, val loss: 0.2157, val acc: 0.9366  (best train acc: 0.8981, best val acc: 0.9460)\n",
      "[Epoch: 29960] train loss: 0.3352, train acc: 0.8524, val loss: 0.2571, val acc: 0.9325  (best train acc: 0.8981, best val acc: 0.9460)\n",
      "[Epoch: 29980] train loss: 0.2845, train acc: 0.8746, val loss: 0.2435, val acc: 0.9160  (best train acc: 0.8981, best val acc: 0.9460)\n",
      "[Epoch: 30000] train loss: 0.2698, train acc: 0.8832, val loss: 0.2386, val acc: 0.9275  (best train acc: 0.8981, best val acc: 0.9460)\n",
      "[Epoch: 30020] train loss: 0.2751, train acc: 0.8799, val loss: 0.2318, val acc: 0.9373  (best train acc: 0.8981, best val acc: 0.9460)\n",
      "[Epoch: 30040] train loss: 0.2802, train acc: 0.8830, val loss: 0.2462, val acc: 0.9305  (best train acc: 0.8981, best val acc: 0.9460)\n",
      "[Epoch: 30060] train loss: 0.2795, train acc: 0.8746, val loss: 0.2586, val acc: 0.9275  (best train acc: 0.8981, best val acc: 0.9460)\n",
      "[Epoch: 30080] train loss: 0.3013, train acc: 0.8615, val loss: 0.2233, val acc: 0.9396  (best train acc: 0.8981, best val acc: 0.9460)\n",
      "[Epoch: 30100] train loss: 0.2791, train acc: 0.8822, val loss: 0.2164, val acc: 0.9359  (best train acc: 0.8981, best val acc: 0.9460)\n",
      "[Epoch: 30120] train loss: 0.2990, train acc: 0.8733, val loss: 0.2304, val acc: 0.9325  (best train acc: 0.8981, best val acc: 0.9460)\n",
      "[Epoch: 30140] train loss: 0.3068, train acc: 0.8669, val loss: 0.2363, val acc: 0.9352  (best train acc: 0.8981, best val acc: 0.9460)\n",
      "[Epoch: 30160] train loss: 0.3088, train acc: 0.8754, val loss: 0.2144, val acc: 0.9376  (best train acc: 0.8981, best val acc: 0.9460)\n",
      "[Epoch: 30180] train loss: 0.2729, train acc: 0.8865, val loss: 0.2295, val acc: 0.9359  (best train acc: 0.8981, best val acc: 0.9460)\n",
      "[Epoch: 30200] train loss: 0.2913, train acc: 0.8796, val loss: 0.2145, val acc: 0.9359  (best train acc: 0.8981, best val acc: 0.9460)\n",
      "[Epoch: 30220] train loss: 0.2773, train acc: 0.8767, val loss: 0.2399, val acc: 0.9251  (best train acc: 0.8981, best val acc: 0.9460)\n",
      "[Epoch: 30240] train loss: 0.3339, train acc: 0.8655, val loss: 0.2377, val acc: 0.9285  (best train acc: 0.8981, best val acc: 0.9460)\n",
      "[Epoch: 30260] train loss: 0.2984, train acc: 0.8759, val loss: 0.2689, val acc: 0.9265  (best train acc: 0.8981, best val acc: 0.9460)\n",
      "[Epoch: 30280] train loss: 0.2661, train acc: 0.8820, val loss: 0.2447, val acc: 0.9184  (best train acc: 0.8981, best val acc: 0.9460)\n",
      "[Epoch: 30300] train loss: 0.3371, train acc: 0.8613, val loss: 0.3806, val acc: 0.9025  (best train acc: 0.8981, best val acc: 0.9460)\n",
      "[Epoch: 30320] train loss: 0.2910, train acc: 0.8754, val loss: 0.2326, val acc: 0.9130  (best train acc: 0.8981, best val acc: 0.9460)\n",
      "[Epoch: 30340] train loss: 0.2784, train acc: 0.8823, val loss: 0.2302, val acc: 0.9309  (best train acc: 0.8981, best val acc: 0.9460)\n",
      "[Epoch: 30360] train loss: 0.2945, train acc: 0.8787, val loss: 0.2585, val acc: 0.9356  (best train acc: 0.8981, best val acc: 0.9460)\n",
      "[Epoch: 30380] train loss: 0.2718, train acc: 0.8805, val loss: 0.2550, val acc: 0.9359  (best train acc: 0.8981, best val acc: 0.9460)\n",
      "[Epoch: 30400] train loss: 0.2884, train acc: 0.8781, val loss: 0.2584, val acc: 0.9221  (best train acc: 0.8981, best val acc: 0.9460)\n",
      "[Epoch: 30420] train loss: 0.2924, train acc: 0.8754, val loss: 0.2490, val acc: 0.9309  (best train acc: 0.8981, best val acc: 0.9460)\n",
      "[Epoch: 30440] train loss: 0.2776, train acc: 0.8842, val loss: 0.2284, val acc: 0.9305  (best train acc: 0.8981, best val acc: 0.9460)\n",
      "[Epoch: 30460] train loss: 0.2729, train acc: 0.8754, val loss: 0.2490, val acc: 0.9207  (best train acc: 0.8981, best val acc: 0.9460)\n",
      "[Epoch: 30480] train loss: 0.2971, train acc: 0.8720, val loss: 0.2380, val acc: 0.9359  (best train acc: 0.8981, best val acc: 0.9460)\n",
      "[Epoch: 30500] train loss: 0.2801, train acc: 0.8808, val loss: 0.2852, val acc: 0.9143  (best train acc: 0.8981, best val acc: 0.9460)\n",
      "[Epoch: 30520] train loss: 0.3223, train acc: 0.8664, val loss: 0.2302, val acc: 0.9329  (best train acc: 0.8981, best val acc: 0.9460)\n",
      "[Epoch: 30540] train loss: 0.2955, train acc: 0.8767, val loss: 0.2323, val acc: 0.9386  (best train acc: 0.8981, best val acc: 0.9460)\n",
      "[Epoch: 30560] train loss: 0.3027, train acc: 0.8719, val loss: 0.2411, val acc: 0.9336  (best train acc: 0.8981, best val acc: 0.9460)\n",
      "[Epoch: 30580] train loss: 0.2749, train acc: 0.8829, val loss: 0.2441, val acc: 0.9265  (best train acc: 0.8981, best val acc: 0.9460)\n",
      "[Epoch: 30600] train loss: 0.2880, train acc: 0.8825, val loss: 0.2589, val acc: 0.9221  (best train acc: 0.8981, best val acc: 0.9460)\n",
      "[Epoch: 30620] train loss: 0.3256, train acc: 0.8555, val loss: 0.2471, val acc: 0.9234  (best train acc: 0.8981, best val acc: 0.9460)\n",
      "[Epoch: 30640] train loss: 0.3036, train acc: 0.8678, val loss: 0.2411, val acc: 0.9231  (best train acc: 0.8981, best val acc: 0.9460)\n",
      "[Epoch: 30660] train loss: 0.2774, train acc: 0.8816, val loss: 0.2631, val acc: 0.9315  (best train acc: 0.8981, best val acc: 0.9460)\n",
      "[Epoch: 30680] train loss: 0.2768, train acc: 0.8840, val loss: 0.2246, val acc: 0.9197  (best train acc: 0.8981, best val acc: 0.9460)\n",
      "[Epoch: 30700] train loss: 0.3224, train acc: 0.8670, val loss: 0.2085, val acc: 0.9410  (best train acc: 0.8981, best val acc: 0.9460)\n",
      "[Epoch: 30720] train loss: 0.3428, train acc: 0.8356, val loss: 0.2225, val acc: 0.9251  (best train acc: 0.8981, best val acc: 0.9460)\n",
      "[Epoch: 30740] train loss: 0.2830, train acc: 0.8836, val loss: 0.2592, val acc: 0.9207  (best train acc: 0.8981, best val acc: 0.9460)\n",
      "[Epoch: 30760] train loss: 0.2846, train acc: 0.8819, val loss: 0.2316, val acc: 0.9292  (best train acc: 0.8981, best val acc: 0.9460)\n",
      "[Epoch: 30780] train loss: 0.2772, train acc: 0.8902, val loss: 0.2205, val acc: 0.9339  (best train acc: 0.8981, best val acc: 0.9460)\n",
      "[Epoch: 30800] train loss: 0.3127, train acc: 0.8593, val loss: 0.2303, val acc: 0.9099  (best train acc: 0.8981, best val acc: 0.9460)\n",
      "[Epoch: 30820] train loss: 0.3181, train acc: 0.8663, val loss: 0.2305, val acc: 0.9278  (best train acc: 0.8981, best val acc: 0.9460)\n",
      "[Epoch: 30840] train loss: 0.3036, train acc: 0.8644, val loss: 0.2178, val acc: 0.9322  (best train acc: 0.8981, best val acc: 0.9460)\n",
      "[Epoch: 30860] train loss: 0.2951, train acc: 0.8704, val loss: 0.2421, val acc: 0.9295  (best train acc: 0.8981, best val acc: 0.9460)\n",
      "[Epoch: 30880] train loss: 0.2716, train acc: 0.8824, val loss: 0.2437, val acc: 0.9302  (best train acc: 0.8981, best val acc: 0.9460)\n",
      "[Epoch: 30900] train loss: 0.3034, train acc: 0.8753, val loss: 0.2201, val acc: 0.9386  (best train acc: 0.8981, best val acc: 0.9460)\n",
      "[Epoch: 30920] train loss: 0.3464, train acc: 0.8595, val loss: 0.2297, val acc: 0.9147  (best train acc: 0.8981, best val acc: 0.9460)\n",
      "[Epoch: 30940] train loss: 0.3532, train acc: 0.8582, val loss: 0.2260, val acc: 0.9255  (best train acc: 0.8981, best val acc: 0.9460)\n",
      "[Epoch: 30960] train loss: 0.2962, train acc: 0.8746, val loss: 0.2264, val acc: 0.9342  (best train acc: 0.8981, best val acc: 0.9460)\n",
      "[Epoch: 30980] train loss: 0.3191, train acc: 0.8728, val loss: 0.2364, val acc: 0.9329  (best train acc: 0.8981, best val acc: 0.9460)\n",
      "[Epoch: 31000] train loss: 0.2956, train acc: 0.8752, val loss: 0.2232, val acc: 0.9325  (best train acc: 0.8981, best val acc: 0.9460)\n",
      "[Epoch: 31020] train loss: 0.2879, train acc: 0.8772, val loss: 0.2352, val acc: 0.9379  (best train acc: 0.8981, best val acc: 0.9460)\n",
      "[Epoch: 31040] train loss: 0.2858, train acc: 0.8762, val loss: 0.2277, val acc: 0.9133  (best train acc: 0.8981, best val acc: 0.9460)\n",
      "[Epoch: 31060] train loss: 0.3100, train acc: 0.8697, val loss: 0.2293, val acc: 0.9288  (best train acc: 0.8981, best val acc: 0.9460)\n",
      "[Epoch: 31080] train loss: 0.2747, train acc: 0.8849, val loss: 0.2317, val acc: 0.9211  (best train acc: 0.8981, best val acc: 0.9460)\n",
      "[Epoch: 31100] train loss: 0.2805, train acc: 0.8789, val loss: 0.2498, val acc: 0.9325  (best train acc: 0.8981, best val acc: 0.9460)\n",
      "[Epoch: 31120] train loss: 0.2767, train acc: 0.8793, val loss: 0.2453, val acc: 0.9332  (best train acc: 0.8981, best val acc: 0.9460)\n",
      "[Epoch: 31140] train loss: 0.2632, train acc: 0.8899, val loss: 0.2206, val acc: 0.9288  (best train acc: 0.8981, best val acc: 0.9460)\n",
      "[Epoch: 31160] train loss: 0.2831, train acc: 0.8804, val loss: 0.2124, val acc: 0.9393  (best train acc: 0.8981, best val acc: 0.9460)\n",
      "[Epoch: 31180] train loss: 0.2933, train acc: 0.8744, val loss: 0.2636, val acc: 0.9268  (best train acc: 0.8981, best val acc: 0.9460)\n",
      "[Epoch: 31200] train loss: 0.2921, train acc: 0.8843, val loss: 0.2589, val acc: 0.9248  (best train acc: 0.8981, best val acc: 0.9460)\n",
      "[Epoch: 31220] train loss: 0.2709, train acc: 0.8857, val loss: 0.2386, val acc: 0.9278  (best train acc: 0.8981, best val acc: 0.9460)\n",
      "[Epoch: 31240] train loss: 0.3238, train acc: 0.8592, val loss: 0.3115, val acc: 0.9133  (best train acc: 0.8981, best val acc: 0.9460)\n",
      "[Epoch: 31260] train loss: 0.2840, train acc: 0.8884, val loss: 0.2258, val acc: 0.9295  (best train acc: 0.8981, best val acc: 0.9460)\n",
      "[Epoch: 31280] train loss: 0.2727, train acc: 0.8817, val loss: 0.2511, val acc: 0.9275  (best train acc: 0.8981, best val acc: 0.9460)\n",
      "[Epoch: 31300] train loss: 0.3077, train acc: 0.8644, val loss: 0.2442, val acc: 0.9234  (best train acc: 0.8981, best val acc: 0.9460)\n",
      "[Epoch: 31320] train loss: 0.2600, train acc: 0.8889, val loss: 0.2296, val acc: 0.9120  (best train acc: 0.8981, best val acc: 0.9460)\n",
      "[Epoch: 31340] train loss: 0.3227, train acc: 0.8689, val loss: 0.2257, val acc: 0.9295  (best train acc: 0.8981, best val acc: 0.9460)\n",
      "[Epoch: 31360] train loss: 0.2699, train acc: 0.8809, val loss: 0.2472, val acc: 0.9211  (best train acc: 0.8981, best val acc: 0.9460)\n",
      "[Epoch: 31380] train loss: 0.2707, train acc: 0.8819, val loss: 0.2452, val acc: 0.9282  (best train acc: 0.8981, best val acc: 0.9460)\n",
      "[Epoch: 31400] train loss: 0.2911, train acc: 0.8811, val loss: 0.2259, val acc: 0.9336  (best train acc: 0.8981, best val acc: 0.9460)\n",
      "[Epoch: 31420] train loss: 0.2751, train acc: 0.8832, val loss: 0.2257, val acc: 0.9180  (best train acc: 0.8981, best val acc: 0.9460)\n",
      "[Epoch: 31440] train loss: 0.2908, train acc: 0.8777, val loss: 0.2306, val acc: 0.9325  (best train acc: 0.8981, best val acc: 0.9460)\n",
      "[Epoch: 31460] train loss: 0.2948, train acc: 0.8697, val loss: 0.2386, val acc: 0.8954  (best train acc: 0.8981, best val acc: 0.9460)\n",
      "[Epoch: 31480] train loss: 0.3503, train acc: 0.8474, val loss: 0.2605, val acc: 0.8998  (best train acc: 0.8981, best val acc: 0.9460)\n",
      "[Epoch: 31500] train loss: 0.2887, train acc: 0.8691, val loss: 0.2517, val acc: 0.9234  (best train acc: 0.8981, best val acc: 0.9460)\n",
      "[Epoch: 31520] train loss: 0.3078, train acc: 0.8716, val loss: 0.2365, val acc: 0.9282  (best train acc: 0.8981, best val acc: 0.9460)\n",
      "[Epoch: 31540] train loss: 0.2885, train acc: 0.8785, val loss: 0.2311, val acc: 0.9258  (best train acc: 0.8981, best val acc: 0.9460)\n",
      "[Epoch: 31560] train loss: 0.2884, train acc: 0.8770, val loss: 0.2461, val acc: 0.9356  (best train acc: 0.8981, best val acc: 0.9460)\n",
      "[Epoch: 31580] train loss: 0.2684, train acc: 0.8848, val loss: 0.2424, val acc: 0.9342  (best train acc: 0.8981, best val acc: 0.9460)\n",
      "[Epoch: 31600] train loss: 0.2881, train acc: 0.8754, val loss: 0.2149, val acc: 0.9393  (best train acc: 0.8981, best val acc: 0.9460)\n",
      "[Epoch: 31620] train loss: 0.2947, train acc: 0.8775, val loss: 0.2091, val acc: 0.9282  (best train acc: 0.8981, best val acc: 0.9460)\n",
      "[Epoch: 31640] train loss: 0.2652, train acc: 0.8806, val loss: 0.2360, val acc: 0.9315  (best train acc: 0.8981, best val acc: 0.9460)\n",
      "[Epoch: 31660] train loss: 0.3311, train acc: 0.8623, val loss: 0.2418, val acc: 0.9285  (best train acc: 0.8981, best val acc: 0.9460)\n",
      "[Epoch: 31680] train loss: 0.3034, train acc: 0.8783, val loss: 0.3360, val acc: 0.9201  (best train acc: 0.8981, best val acc: 0.9460)\n",
      "[Epoch: 31700] train loss: 0.2820, train acc: 0.8799, val loss: 0.2588, val acc: 0.9325  (best train acc: 0.8981, best val acc: 0.9460)\n",
      "[Epoch: 31720] train loss: 0.3110, train acc: 0.8599, val loss: 0.2557, val acc: 0.9275  (best train acc: 0.8981, best val acc: 0.9460)\n",
      "[Epoch: 31740] train loss: 0.2686, train acc: 0.8899, val loss: 0.2232, val acc: 0.9359  (best train acc: 0.8981, best val acc: 0.9460)\n",
      "[Epoch: 31760] train loss: 0.2767, train acc: 0.8785, val loss: 0.2381, val acc: 0.9315  (best train acc: 0.8981, best val acc: 0.9460)\n",
      "[Epoch: 31780] train loss: 0.3057, train acc: 0.8717, val loss: 0.2105, val acc: 0.9336  (best train acc: 0.8981, best val acc: 0.9460)\n",
      "[Epoch: 31800] train loss: 0.2906, train acc: 0.8742, val loss: 0.2204, val acc: 0.9339  (best train acc: 0.8981, best val acc: 0.9460)\n",
      "[Epoch: 31820] train loss: 0.2947, train acc: 0.8748, val loss: 0.2763, val acc: 0.9349  (best train acc: 0.8981, best val acc: 0.9460)\n",
      "[Epoch: 31840] train loss: 0.3318, train acc: 0.8631, val loss: 0.2301, val acc: 0.9315  (best train acc: 0.8981, best val acc: 0.9460)\n",
      "[Epoch: 31860] train loss: 0.2694, train acc: 0.8881, val loss: 0.2483, val acc: 0.9329  (best train acc: 0.8981, best val acc: 0.9460)\n",
      "[Epoch: 31880] train loss: 0.2809, train acc: 0.8763, val loss: 0.2505, val acc: 0.9214  (best train acc: 0.8981, best val acc: 0.9460)\n",
      "[Epoch: 31900] train loss: 0.2752, train acc: 0.8886, val loss: 0.2443, val acc: 0.9390  (best train acc: 0.8981, best val acc: 0.9460)\n",
      "[Epoch: 31920] train loss: 0.2694, train acc: 0.8882, val loss: 0.2356, val acc: 0.9309  (best train acc: 0.8981, best val acc: 0.9460)\n",
      "[Epoch: 31940] train loss: 0.3052, train acc: 0.8706, val loss: 0.2361, val acc: 0.9322  (best train acc: 0.8981, best val acc: 0.9460)\n",
      "[Epoch: 31960] train loss: 0.3000, train acc: 0.8622, val loss: 0.2414, val acc: 0.9383  (best train acc: 0.8981, best val acc: 0.9460)\n",
      "[Epoch: 31980] train loss: 0.3027, train acc: 0.8638, val loss: 0.2387, val acc: 0.9315  (best train acc: 0.8981, best val acc: 0.9460)\n",
      "[Epoch: 32000] train loss: 0.3599, train acc: 0.8402, val loss: 0.2708, val acc: 0.8904  (best train acc: 0.8981, best val acc: 0.9460)\n",
      "[Epoch: 32020] train loss: 0.2989, train acc: 0.8728, val loss: 0.2449, val acc: 0.9103  (best train acc: 0.8981, best val acc: 0.9460)\n",
      "[Epoch: 32040] train loss: 0.2847, train acc: 0.8679, val loss: 0.2649, val acc: 0.9093  (best train acc: 0.8981, best val acc: 0.9460)\n",
      "[Epoch: 32060] train loss: 0.3202, train acc: 0.8587, val loss: 0.2162, val acc: 0.9309  (best train acc: 0.8981, best val acc: 0.9460)\n",
      "[Epoch: 32080] train loss: 0.3168, train acc: 0.8568, val loss: 0.2578, val acc: 0.9184  (best train acc: 0.8981, best val acc: 0.9460)\n",
      "[Epoch: 32100] train loss: 0.2966, train acc: 0.8671, val loss: 0.2318, val acc: 0.9029  (best train acc: 0.8981, best val acc: 0.9460)\n",
      "[Epoch: 32120] train loss: 0.3162, train acc: 0.8797, val loss: 0.2318, val acc: 0.9329  (best train acc: 0.8981, best val acc: 0.9460)\n",
      "[Epoch: 32140] train loss: 0.3339, train acc: 0.8412, val loss: 0.2154, val acc: 0.9302  (best train acc: 0.8981, best val acc: 0.9460)\n",
      "[Epoch: 32160] train loss: 0.3078, train acc: 0.8623, val loss: 0.2247, val acc: 0.9197  (best train acc: 0.8981, best val acc: 0.9460)\n",
      "[Epoch: 32180] train loss: 0.2788, train acc: 0.8805, val loss: 0.2453, val acc: 0.9248  (best train acc: 0.8981, best val acc: 0.9460)\n",
      "[Epoch: 32200] train loss: 0.2926, train acc: 0.8715, val loss: 0.2234, val acc: 0.9359  (best train acc: 0.8981, best val acc: 0.9460)\n",
      "[Epoch: 32220] train loss: 0.3091, train acc: 0.8603, val loss: 0.2195, val acc: 0.9363  (best train acc: 0.8981, best val acc: 0.9460)\n",
      "[Epoch: 32240] train loss: 0.2889, train acc: 0.8811, val loss: 0.2463, val acc: 0.9352  (best train acc: 0.8981, best val acc: 0.9460)\n",
      "[Epoch: 32260] train loss: 0.2900, train acc: 0.8777, val loss: 0.2370, val acc: 0.9346  (best train acc: 0.8981, best val acc: 0.9460)\n",
      "[Epoch: 32280] train loss: 0.2978, train acc: 0.8711, val loss: 0.2613, val acc: 0.9346  (best train acc: 0.8981, best val acc: 0.9460)\n",
      "[Epoch: 32300] train loss: 0.2929, train acc: 0.8707, val loss: 0.2233, val acc: 0.9241  (best train acc: 0.8981, best val acc: 0.9460)\n",
      "[Epoch: 32320] train loss: 0.2763, train acc: 0.8835, val loss: 0.2409, val acc: 0.9207  (best train acc: 0.8981, best val acc: 0.9460)\n",
      "[Epoch: 32340] train loss: 0.3281, train acc: 0.8540, val loss: 0.2321, val acc: 0.9373  (best train acc: 0.8981, best val acc: 0.9460)\n",
      "[Epoch: 32360] train loss: 0.2712, train acc: 0.8852, val loss: 0.2495, val acc: 0.9332  (best train acc: 0.8981, best val acc: 0.9460)\n",
      "[Epoch: 32380] train loss: 0.2890, train acc: 0.8770, val loss: 0.2486, val acc: 0.9251  (best train acc: 0.8981, best val acc: 0.9460)\n",
      "[Epoch: 32400] train loss: 0.3040, train acc: 0.8690, val loss: 0.2711, val acc: 0.9251  (best train acc: 0.8981, best val acc: 0.9460)\n",
      "[Epoch: 32420] train loss: 0.2894, train acc: 0.8764, val loss: 0.2601, val acc: 0.9295  (best train acc: 0.8981, best val acc: 0.9460)\n",
      "[Epoch: 32440] train loss: 0.2847, train acc: 0.8877, val loss: 0.2200, val acc: 0.9275  (best train acc: 0.8981, best val acc: 0.9460)\n",
      "[Epoch: 32460] train loss: 0.3350, train acc: 0.8610, val loss: 0.2275, val acc: 0.9356  (best train acc: 0.8981, best val acc: 0.9460)\n",
      "[Epoch: 32480] train loss: 0.2835, train acc: 0.8832, val loss: 0.2736, val acc: 0.9285  (best train acc: 0.8981, best val acc: 0.9460)\n",
      "[Epoch: 32500] train loss: 0.2800, train acc: 0.8805, val loss: 0.2649, val acc: 0.9315  (best train acc: 0.8981, best val acc: 0.9460)\n",
      "[Epoch: 32520] train loss: 0.3256, train acc: 0.8639, val loss: 0.2630, val acc: 0.9126  (best train acc: 0.8981, best val acc: 0.9460)\n",
      "[Epoch: 32540] train loss: 0.2679, train acc: 0.8919, val loss: 0.2533, val acc: 0.9278  (best train acc: 0.8981, best val acc: 0.9460)\n",
      "[Epoch: 32560] train loss: 0.2685, train acc: 0.8812, val loss: 0.2388, val acc: 0.9177  (best train acc: 0.8981, best val acc: 0.9460)\n",
      "[Epoch: 32580] train loss: 0.2664, train acc: 0.8845, val loss: 0.2169, val acc: 0.9319  (best train acc: 0.8981, best val acc: 0.9460)\n",
      "[Epoch: 32600] train loss: 0.3006, train acc: 0.8720, val loss: 0.2440, val acc: 0.9332  (best train acc: 0.8981, best val acc: 0.9460)\n",
      "[Epoch: 32620] train loss: 0.3021, train acc: 0.8631, val loss: 0.2207, val acc: 0.9285  (best train acc: 0.8981, best val acc: 0.9460)\n",
      "[Epoch: 32640] train loss: 0.3031, train acc: 0.8640, val loss: 0.2101, val acc: 0.9309  (best train acc: 0.8981, best val acc: 0.9460)\n",
      "[Epoch: 32660] train loss: 0.2925, train acc: 0.8729, val loss: 0.2349, val acc: 0.9336  (best train acc: 0.8981, best val acc: 0.9460)\n",
      "[Epoch: 32680] train loss: 0.3028, train acc: 0.8765, val loss: 0.2386, val acc: 0.9319  (best train acc: 0.8981, best val acc: 0.9460)\n",
      "[Epoch: 32700] train loss: 0.2950, train acc: 0.8799, val loss: 0.2265, val acc: 0.9329  (best train acc: 0.8981, best val acc: 0.9460)\n",
      "[Epoch: 32720] train loss: 0.2768, train acc: 0.8820, val loss: 0.2388, val acc: 0.9302  (best train acc: 0.8981, best val acc: 0.9460)\n",
      "[Epoch: 32740] train loss: 0.2715, train acc: 0.8882, val loss: 0.2388, val acc: 0.9147  (best train acc: 0.8981, best val acc: 0.9460)\n",
      "[Epoch: 32760] train loss: 0.2630, train acc: 0.8876, val loss: 0.2584, val acc: 0.9120  (best train acc: 0.8981, best val acc: 0.9460)\n",
      "[Epoch: 32780] train loss: 0.2754, train acc: 0.8851, val loss: 0.2301, val acc: 0.9349  (best train acc: 0.8981, best val acc: 0.9460)\n",
      "[Epoch: 32800] train loss: 0.2721, train acc: 0.8849, val loss: 0.2403, val acc: 0.9356  (best train acc: 0.8981, best val acc: 0.9460)\n",
      "[Epoch: 32820] train loss: 0.2819, train acc: 0.8780, val loss: 0.2569, val acc: 0.9231  (best train acc: 0.8981, best val acc: 0.9460)\n",
      "[Epoch: 32840] train loss: 0.3200, train acc: 0.8670, val loss: 0.2075, val acc: 0.9339  (best train acc: 0.8981, best val acc: 0.9460)\n",
      "[Epoch: 32860] train loss: 0.2930, train acc: 0.8759, val loss: 0.2408, val acc: 0.9234  (best train acc: 0.8981, best val acc: 0.9460)\n",
      "[Epoch: 32880] train loss: 0.2921, train acc: 0.8803, val loss: 0.2430, val acc: 0.9349  (best train acc: 0.8984, best val acc: 0.9460)\n",
      "[Epoch: 32900] train loss: 0.3065, train acc: 0.8736, val loss: 0.2018, val acc: 0.9352  (best train acc: 0.8984, best val acc: 0.9460)\n",
      "[Epoch: 32920] train loss: 0.2763, train acc: 0.8897, val loss: 0.2409, val acc: 0.9393  (best train acc: 0.8984, best val acc: 0.9460)\n",
      "[Epoch: 32940] train loss: 0.3322, train acc: 0.8548, val loss: 0.2238, val acc: 0.9386  (best train acc: 0.8984, best val acc: 0.9460)\n",
      "[Epoch: 32960] train loss: 0.2858, train acc: 0.8776, val loss: 0.2298, val acc: 0.9373  (best train acc: 0.8984, best val acc: 0.9460)\n",
      "[Epoch: 32980] train loss: 0.3230, train acc: 0.8590, val loss: 0.2217, val acc: 0.9312  (best train acc: 0.8984, best val acc: 0.9460)\n",
      "[Epoch: 33000] train loss: 0.2566, train acc: 0.8947, val loss: 0.2471, val acc: 0.9167  (best train acc: 0.8984, best val acc: 0.9460)\n",
      "[Epoch: 33020] train loss: 0.2649, train acc: 0.8897, val loss: 0.2289, val acc: 0.9366  (best train acc: 0.8984, best val acc: 0.9460)\n",
      "[Epoch: 33040] train loss: 0.2757, train acc: 0.8851, val loss: 0.2666, val acc: 0.9272  (best train acc: 0.8984, best val acc: 0.9460)\n",
      "[Epoch: 33060] train loss: 0.3008, train acc: 0.8746, val loss: 0.2935, val acc: 0.9228  (best train acc: 0.8984, best val acc: 0.9460)\n",
      "[Epoch: 33080] train loss: 0.4956, train acc: 0.7690, val loss: 0.5260, val acc: 0.7781  (best train acc: 0.8984, best val acc: 0.9460)\n",
      "[Epoch: 33100] train loss: 0.3939, train acc: 0.8131, val loss: 0.2757, val acc: 0.8901  (best train acc: 0.8984, best val acc: 0.9460)\n",
      "[Epoch: 33120] train loss: 0.3710, train acc: 0.8239, val loss: 0.2793, val acc: 0.9130  (best train acc: 0.8984, best val acc: 0.9460)\n",
      "[Epoch: 33140] train loss: 0.3216, train acc: 0.8644, val loss: 0.2509, val acc: 0.9150  (best train acc: 0.8984, best val acc: 0.9460)\n",
      "[Epoch: 33160] train loss: 0.3572, train acc: 0.8289, val loss: 0.2412, val acc: 0.9153  (best train acc: 0.8984, best val acc: 0.9460)\n",
      "[Epoch: 33180] train loss: 0.3274, train acc: 0.8568, val loss: 0.2419, val acc: 0.9049  (best train acc: 0.8984, best val acc: 0.9460)\n",
      "[Epoch: 33200] train loss: 0.2881, train acc: 0.8715, val loss: 0.2523, val acc: 0.9285  (best train acc: 0.8984, best val acc: 0.9460)\n",
      "[Epoch: 33220] train loss: 0.3012, train acc: 0.8710, val loss: 0.2593, val acc: 0.9177  (best train acc: 0.8984, best val acc: 0.9460)\n",
      "[Epoch: 33240] train loss: 0.2899, train acc: 0.8887, val loss: 0.2341, val acc: 0.9140  (best train acc: 0.8984, best val acc: 0.9460)\n",
      "[Epoch: 33260] train loss: 0.3071, train acc: 0.8660, val loss: 0.2337, val acc: 0.9356  (best train acc: 0.8984, best val acc: 0.9460)\n",
      "[Epoch: 33280] train loss: 0.3276, train acc: 0.8566, val loss: 0.2716, val acc: 0.9251  (best train acc: 0.8984, best val acc: 0.9460)\n",
      "[Epoch: 33300] train loss: 0.3363, train acc: 0.8468, val loss: 0.2654, val acc: 0.8985  (best train acc: 0.8984, best val acc: 0.9460)\n",
      "[Epoch: 33320] train loss: 0.2768, train acc: 0.8832, val loss: 0.2643, val acc: 0.9069  (best train acc: 0.8984, best val acc: 0.9460)\n",
      "[Epoch: 33340] train loss: 0.2710, train acc: 0.8850, val loss: 0.2357, val acc: 0.9298  (best train acc: 0.8984, best val acc: 0.9460)\n",
      "[Epoch: 33360] train loss: 0.3111, train acc: 0.8699, val loss: 0.2248, val acc: 0.9393  (best train acc: 0.8984, best val acc: 0.9460)\n",
      "[Epoch: 33380] train loss: 0.3138, train acc: 0.8542, val loss: 0.2310, val acc: 0.9116  (best train acc: 0.8984, best val acc: 0.9460)\n",
      "[Epoch: 33400] train loss: 0.2798, train acc: 0.8789, val loss: 0.2294, val acc: 0.9383  (best train acc: 0.8984, best val acc: 0.9460)\n",
      "[Epoch: 33420] train loss: 0.2883, train acc: 0.8797, val loss: 0.2364, val acc: 0.9339  (best train acc: 0.8984, best val acc: 0.9460)\n",
      "[Epoch: 33440] train loss: 0.3073, train acc: 0.8710, val loss: 0.2249, val acc: 0.9417  (best train acc: 0.8984, best val acc: 0.9460)\n",
      "[Epoch: 33460] train loss: 0.2781, train acc: 0.8809, val loss: 0.2299, val acc: 0.9390  (best train acc: 0.8984, best val acc: 0.9460)\n",
      "[Epoch: 33480] train loss: 0.2875, train acc: 0.8783, val loss: 0.2737, val acc: 0.9207  (best train acc: 0.8984, best val acc: 0.9460)\n",
      "[Epoch: 33500] train loss: 0.2913, train acc: 0.8715, val loss: 0.2400, val acc: 0.9322  (best train acc: 0.8984, best val acc: 0.9460)\n",
      "[Epoch: 33520] train loss: 0.3290, train acc: 0.8554, val loss: 0.2311, val acc: 0.9349  (best train acc: 0.8984, best val acc: 0.9460)\n",
      "[Epoch: 33540] train loss: 0.2645, train acc: 0.8918, val loss: 0.2235, val acc: 0.9356  (best train acc: 0.8984, best val acc: 0.9460)\n",
      "[Epoch: 33560] train loss: 0.2799, train acc: 0.8779, val loss: 0.2335, val acc: 0.9305  (best train acc: 0.8984, best val acc: 0.9460)\n",
      "[Epoch: 33580] train loss: 0.2971, train acc: 0.8732, val loss: 0.2287, val acc: 0.9363  (best train acc: 0.8984, best val acc: 0.9460)\n",
      "[Epoch: 33600] train loss: 0.2679, train acc: 0.8923, val loss: 0.2458, val acc: 0.9248  (best train acc: 0.8984, best val acc: 0.9460)\n",
      "[Epoch: 33620] train loss: 0.2732, train acc: 0.8796, val loss: 0.2364, val acc: 0.9305  (best train acc: 0.8984, best val acc: 0.9460)\n",
      "[Epoch: 33640] train loss: 0.3316, train acc: 0.8667, val loss: 0.2187, val acc: 0.9346  (best train acc: 0.8985, best val acc: 0.9460)\n",
      "[Epoch: 33660] train loss: 0.2681, train acc: 0.8926, val loss: 0.2372, val acc: 0.9140  (best train acc: 0.8985, best val acc: 0.9460)\n",
      "[Epoch: 33680] train loss: 0.2802, train acc: 0.8856, val loss: 0.2123, val acc: 0.9332  (best train acc: 0.8985, best val acc: 0.9460)\n",
      "[Epoch: 33700] train loss: 0.2928, train acc: 0.8736, val loss: 0.2487, val acc: 0.9315  (best train acc: 0.8985, best val acc: 0.9460)\n",
      "[Epoch: 33720] train loss: 0.2979, train acc: 0.8778, val loss: 0.2250, val acc: 0.9349  (best train acc: 0.8985, best val acc: 0.9460)\n",
      "[Epoch: 33740] train loss: 0.3205, train acc: 0.8566, val loss: 0.2632, val acc: 0.9396  (best train acc: 0.8985, best val acc: 0.9460)\n",
      "[Epoch: 33760] train loss: 0.2728, train acc: 0.8846, val loss: 0.2140, val acc: 0.9211  (best train acc: 0.8985, best val acc: 0.9460)\n",
      "[Epoch: 33780] train loss: 0.3185, train acc: 0.8629, val loss: 0.2364, val acc: 0.9302  (best train acc: 0.8985, best val acc: 0.9460)\n",
      "[Epoch: 33800] train loss: 0.2856, train acc: 0.8774, val loss: 0.2308, val acc: 0.9376  (best train acc: 0.8985, best val acc: 0.9460)\n",
      "[Epoch: 33820] train loss: 0.2707, train acc: 0.8791, val loss: 0.2488, val acc: 0.9234  (best train acc: 0.8985, best val acc: 0.9460)\n",
      "[Epoch: 33840] train loss: 0.3087, train acc: 0.8670, val loss: 0.2821, val acc: 0.9204  (best train acc: 0.8985, best val acc: 0.9460)\n",
      "[Epoch: 33860] train loss: 0.2764, train acc: 0.8798, val loss: 0.2305, val acc: 0.9231  (best train acc: 0.8985, best val acc: 0.9460)\n",
      "[Epoch: 33880] train loss: 0.2725, train acc: 0.8845, val loss: 0.2583, val acc: 0.9325  (best train acc: 0.8985, best val acc: 0.9460)\n",
      "[Epoch: 33900] train loss: 0.2890, train acc: 0.8819, val loss: 0.2098, val acc: 0.9231  (best train acc: 0.8985, best val acc: 0.9460)\n",
      "[Epoch: 33920] train loss: 0.2843, train acc: 0.8814, val loss: 0.2133, val acc: 0.9356  (best train acc: 0.8985, best val acc: 0.9460)\n",
      "[Epoch: 33940] train loss: 0.3174, train acc: 0.8576, val loss: 0.2220, val acc: 0.9285  (best train acc: 0.8985, best val acc: 0.9460)\n",
      "[Epoch: 33960] train loss: 0.3075, train acc: 0.8683, val loss: 0.2376, val acc: 0.9376  (best train acc: 0.8985, best val acc: 0.9460)\n",
      "[Epoch: 33980] train loss: 0.3031, train acc: 0.8676, val loss: 0.2352, val acc: 0.9275  (best train acc: 0.8985, best val acc: 0.9460)\n",
      "[Epoch: 34000] train loss: 0.2945, train acc: 0.8761, val loss: 0.2310, val acc: 0.9255  (best train acc: 0.8985, best val acc: 0.9460)\n",
      "[Epoch: 34020] train loss: 0.3305, train acc: 0.8636, val loss: 0.2379, val acc: 0.9366  (best train acc: 0.8985, best val acc: 0.9460)\n",
      "[Epoch: 34040] train loss: 0.3000, train acc: 0.8763, val loss: 0.2486, val acc: 0.9379  (best train acc: 0.8985, best val acc: 0.9460)\n",
      "[Epoch: 34060] train loss: 0.2672, train acc: 0.8885, val loss: 0.2090, val acc: 0.9218  (best train acc: 0.8985, best val acc: 0.9460)\n",
      "[Epoch: 34080] train loss: 0.2882, train acc: 0.8737, val loss: 0.2309, val acc: 0.9400  (best train acc: 0.8985, best val acc: 0.9460)\n",
      "[Epoch: 34100] train loss: 0.2922, train acc: 0.8850, val loss: 0.2582, val acc: 0.9329  (best train acc: 0.8985, best val acc: 0.9460)\n",
      "[Epoch: 34120] train loss: 0.2998, train acc: 0.8730, val loss: 0.2290, val acc: 0.9379  (best train acc: 0.8985, best val acc: 0.9460)\n",
      "[Epoch: 34140] train loss: 0.2788, train acc: 0.8840, val loss: 0.2368, val acc: 0.9322  (best train acc: 0.8985, best val acc: 0.9460)\n",
      "[Epoch: 34160] train loss: 0.2926, train acc: 0.8724, val loss: 0.2580, val acc: 0.9241  (best train acc: 0.8985, best val acc: 0.9460)\n",
      "[Epoch: 34180] train loss: 0.3071, train acc: 0.8704, val loss: 0.2169, val acc: 0.9282  (best train acc: 0.8985, best val acc: 0.9460)\n",
      "[Epoch: 34200] train loss: 0.2652, train acc: 0.8872, val loss: 0.2637, val acc: 0.9329  (best train acc: 0.8985, best val acc: 0.9460)\n",
      "[Epoch: 34220] train loss: 0.2732, train acc: 0.8835, val loss: 0.2246, val acc: 0.9383  (best train acc: 0.8985, best val acc: 0.9460)\n",
      "[Epoch: 34240] train loss: 0.2667, train acc: 0.8869, val loss: 0.2365, val acc: 0.9356  (best train acc: 0.8985, best val acc: 0.9460)\n",
      "[Epoch: 34260] train loss: 0.3136, train acc: 0.8621, val loss: 0.2197, val acc: 0.9410  (best train acc: 0.8985, best val acc: 0.9460)\n",
      "[Epoch: 34280] train loss: 0.3106, train acc: 0.8608, val loss: 0.2615, val acc: 0.9305  (best train acc: 0.8985, best val acc: 0.9460)\n",
      "[Epoch: 34300] train loss: 0.2668, train acc: 0.8908, val loss: 0.2380, val acc: 0.9332  (best train acc: 0.8985, best val acc: 0.9460)\n",
      "[Epoch: 34320] train loss: 0.2817, train acc: 0.8848, val loss: 0.2250, val acc: 0.9272  (best train acc: 0.8985, best val acc: 0.9460)\n",
      "[Epoch: 34340] train loss: 0.2935, train acc: 0.8832, val loss: 0.2431, val acc: 0.9204  (best train acc: 0.8985, best val acc: 0.9460)\n",
      "[Epoch: 34360] train loss: 0.3010, train acc: 0.8686, val loss: 0.2252, val acc: 0.9363  (best train acc: 0.8985, best val acc: 0.9460)\n",
      "[Epoch: 34380] train loss: 0.2875, train acc: 0.8769, val loss: 0.2398, val acc: 0.9359  (best train acc: 0.8985, best val acc: 0.9460)\n",
      "[Epoch: 34400] train loss: 0.3085, train acc: 0.8558, val loss: 0.2374, val acc: 0.9342  (best train acc: 0.8985, best val acc: 0.9460)\n",
      "[Epoch: 34420] train loss: 0.3108, train acc: 0.8698, val loss: 0.2346, val acc: 0.9265  (best train acc: 0.8985, best val acc: 0.9460)\n",
      "[Epoch: 34440] train loss: 0.2719, train acc: 0.8866, val loss: 0.2140, val acc: 0.9393  (best train acc: 0.8985, best val acc: 0.9460)\n",
      "[Epoch: 34460] train loss: 0.2704, train acc: 0.8879, val loss: 0.2410, val acc: 0.9359  (best train acc: 0.8985, best val acc: 0.9460)\n",
      "[Epoch: 34480] train loss: 0.3154, train acc: 0.8553, val loss: 0.2370, val acc: 0.9238  (best train acc: 0.8985, best val acc: 0.9460)\n",
      "[Epoch: 34500] train loss: 0.3220, train acc: 0.8657, val loss: 0.2327, val acc: 0.9369  (best train acc: 0.8985, best val acc: 0.9460)\n",
      "[Epoch: 34520] train loss: 0.2618, train acc: 0.8827, val loss: 0.2390, val acc: 0.9160  (best train acc: 0.8985, best val acc: 0.9460)\n",
      "[Epoch: 34540] train loss: 0.5816, train acc: 0.8501, val loss: 0.5356, val acc: 0.8631  (best train acc: 0.8985, best val acc: 0.9460)\n",
      "[Epoch: 34560] train loss: 0.3693, train acc: 0.8412, val loss: 0.2905, val acc: 0.9147  (best train acc: 0.8985, best val acc: 0.9460)\n",
      "[Epoch: 34580] train loss: 0.3218, train acc: 0.8535, val loss: 0.2233, val acc: 0.9305  (best train acc: 0.8985, best val acc: 0.9460)\n",
      "[Epoch: 34600] train loss: 0.3121, train acc: 0.8631, val loss: 0.2661, val acc: 0.9265  (best train acc: 0.8985, best val acc: 0.9460)\n",
      "[Epoch: 34620] train loss: 0.2725, train acc: 0.8836, val loss: 0.2702, val acc: 0.9329  (best train acc: 0.8985, best val acc: 0.9460)\n",
      "[Epoch: 34640] train loss: 0.3036, train acc: 0.8684, val loss: 0.2257, val acc: 0.9379  (best train acc: 0.8985, best val acc: 0.9460)\n",
      "[Epoch: 34660] train loss: 0.2949, train acc: 0.8798, val loss: 0.2405, val acc: 0.9356  (best train acc: 0.8985, best val acc: 0.9460)\n",
      "[Epoch: 34680] train loss: 0.2970, train acc: 0.8760, val loss: 0.2461, val acc: 0.9400  (best train acc: 0.8985, best val acc: 0.9460)\n",
      "[Epoch: 34700] train loss: 0.2757, train acc: 0.8871, val loss: 0.2440, val acc: 0.9359  (best train acc: 0.8985, best val acc: 0.9460)\n",
      "[Epoch: 34720] train loss: 0.2705, train acc: 0.8850, val loss: 0.2286, val acc: 0.9383  (best train acc: 0.8985, best val acc: 0.9460)\n",
      "[Epoch: 34740] train loss: 0.3119, train acc: 0.8680, val loss: 0.2234, val acc: 0.9292  (best train acc: 0.8985, best val acc: 0.9460)\n",
      "[Epoch: 34760] train loss: 0.2810, train acc: 0.8819, val loss: 0.2656, val acc: 0.9325  (best train acc: 0.8985, best val acc: 0.9460)\n",
      "[Epoch: 34780] train loss: 0.3003, train acc: 0.8689, val loss: 0.2528, val acc: 0.9322  (best train acc: 0.8985, best val acc: 0.9460)\n",
      "[Epoch: 34800] train loss: 0.2593, train acc: 0.8882, val loss: 0.2438, val acc: 0.9393  (best train acc: 0.8985, best val acc: 0.9460)\n",
      "[Epoch: 34820] train loss: 0.2964, train acc: 0.8790, val loss: 0.2467, val acc: 0.9400  (best train acc: 0.8985, best val acc: 0.9460)\n",
      "[Epoch: 34840] train loss: 0.2572, train acc: 0.8951, val loss: 0.2198, val acc: 0.9245  (best train acc: 0.8985, best val acc: 0.9460)\n",
      "[Epoch: 34860] train loss: 0.3226, train acc: 0.8581, val loss: 0.2401, val acc: 0.9336  (best train acc: 0.8985, best val acc: 0.9460)\n",
      "[Epoch: 34880] train loss: 0.2980, train acc: 0.8773, val loss: 0.2187, val acc: 0.9329  (best train acc: 0.8985, best val acc: 0.9460)\n",
      "[Epoch: 34900] train loss: 0.2809, train acc: 0.8820, val loss: 0.2467, val acc: 0.9312  (best train acc: 0.8985, best val acc: 0.9460)\n",
      "[Epoch: 34920] train loss: 0.2641, train acc: 0.8874, val loss: 0.2415, val acc: 0.9298  (best train acc: 0.8985, best val acc: 0.9460)\n",
      "[Epoch: 34940] train loss: 0.2708, train acc: 0.8856, val loss: 0.2336, val acc: 0.9373  (best train acc: 0.8985, best val acc: 0.9460)\n",
      "[Epoch: 34960] train loss: 0.2689, train acc: 0.8867, val loss: 0.2583, val acc: 0.9251  (best train acc: 0.8985, best val acc: 0.9460)\n",
      "[Epoch: 34980] train loss: 0.3572, train acc: 0.8276, val loss: 0.2197, val acc: 0.9288  (best train acc: 0.8985, best val acc: 0.9460)\n",
      "[Epoch: 35000] train loss: 0.2937, train acc: 0.8772, val loss: 0.2085, val acc: 0.9228  (best train acc: 0.8985, best val acc: 0.9460)\n",
      "[Epoch: 35020] train loss: 0.3111, train acc: 0.8617, val loss: 0.2537, val acc: 0.9214  (best train acc: 0.8985, best val acc: 0.9460)\n",
      "[Epoch: 35040] train loss: 0.3018, train acc: 0.8690, val loss: 0.2703, val acc: 0.9049  (best train acc: 0.8985, best val acc: 0.9460)\n",
      "[Epoch: 35060] train loss: 0.2690, train acc: 0.8867, val loss: 0.2639, val acc: 0.9336  (best train acc: 0.8985, best val acc: 0.9460)\n",
      "[Epoch: 35080] train loss: 0.2890, train acc: 0.8697, val loss: 0.2463, val acc: 0.9241  (best train acc: 0.8990, best val acc: 0.9460)\n",
      "[Epoch: 35100] train loss: 0.3249, train acc: 0.8616, val loss: 0.2387, val acc: 0.9292  (best train acc: 0.8990, best val acc: 0.9460)\n",
      "[Epoch: 35120] train loss: 0.3208, train acc: 0.8706, val loss: 0.2112, val acc: 0.9322  (best train acc: 0.8990, best val acc: 0.9460)\n",
      "[Epoch: 35140] train loss: 0.2659, train acc: 0.8916, val loss: 0.2341, val acc: 0.9363  (best train acc: 0.8990, best val acc: 0.9460)\n",
      "[Epoch: 35160] train loss: 0.2601, train acc: 0.8876, val loss: 0.2594, val acc: 0.9255  (best train acc: 0.8990, best val acc: 0.9460)\n",
      "[Epoch: 35180] train loss: 0.2988, train acc: 0.8741, val loss: 0.2578, val acc: 0.9329  (best train acc: 0.8990, best val acc: 0.9460)\n",
      "[Epoch: 35200] train loss: 0.2808, train acc: 0.8796, val loss: 0.2249, val acc: 0.9231  (best train acc: 0.8990, best val acc: 0.9460)\n",
      "[Epoch: 35220] train loss: 0.2692, train acc: 0.8888, val loss: 0.2535, val acc: 0.9363  (best train acc: 0.8990, best val acc: 0.9460)\n",
      "[Epoch: 35240] train loss: 0.2706, train acc: 0.8822, val loss: 0.2190, val acc: 0.9295  (best train acc: 0.8990, best val acc: 0.9460)\n",
      "[Epoch: 35260] train loss: 0.2684, train acc: 0.8842, val loss: 0.2746, val acc: 0.9083  (best train acc: 0.8990, best val acc: 0.9460)\n",
      "[Epoch: 35280] train loss: 0.2999, train acc: 0.8703, val loss: 0.2435, val acc: 0.9245  (best train acc: 0.8990, best val acc: 0.9460)\n",
      "[Epoch: 35300] train loss: 0.3236, train acc: 0.8673, val loss: 0.2149, val acc: 0.9359  (best train acc: 0.8990, best val acc: 0.9460)\n",
      "[Epoch: 35320] train loss: 0.2835, train acc: 0.8916, val loss: 0.2246, val acc: 0.9336  (best train acc: 0.8990, best val acc: 0.9460)\n",
      "[Epoch: 35340] train loss: 0.2713, train acc: 0.8871, val loss: 0.2185, val acc: 0.9305  (best train acc: 0.8990, best val acc: 0.9460)\n",
      "[Epoch: 35360] train loss: 0.2741, train acc: 0.8777, val loss: 0.2284, val acc: 0.9325  (best train acc: 0.8990, best val acc: 0.9460)\n",
      "[Epoch: 35380] train loss: 0.2570, train acc: 0.8892, val loss: 0.2449, val acc: 0.9272  (best train acc: 0.8990, best val acc: 0.9460)\n",
      "[Epoch: 35400] train loss: 0.2753, train acc: 0.8857, val loss: 0.2973, val acc: 0.9211  (best train acc: 0.8990, best val acc: 0.9460)\n",
      "[Epoch: 35420] train loss: 0.2652, train acc: 0.8816, val loss: 0.2382, val acc: 0.9278  (best train acc: 0.9004, best val acc: 0.9460)\n",
      "[Epoch: 35440] train loss: 0.2741, train acc: 0.8751, val loss: 0.2561, val acc: 0.9160  (best train acc: 0.9004, best val acc: 0.9460)\n",
      "[Epoch: 35460] train loss: 0.2930, train acc: 0.8754, val loss: 0.2161, val acc: 0.9234  (best train acc: 0.9004, best val acc: 0.9460)\n",
      "[Epoch: 35480] train loss: 0.2705, train acc: 0.8859, val loss: 0.2509, val acc: 0.9356  (best train acc: 0.9004, best val acc: 0.9460)\n",
      "[Epoch: 35500] train loss: 0.2759, train acc: 0.8775, val loss: 0.2510, val acc: 0.9265  (best train acc: 0.9004, best val acc: 0.9460)\n",
      "[Epoch: 35520] train loss: 0.3072, train acc: 0.8722, val loss: 0.2249, val acc: 0.9332  (best train acc: 0.9004, best val acc: 0.9460)\n",
      "[Epoch: 35540] train loss: 0.2868, train acc: 0.8801, val loss: 0.2305, val acc: 0.9373  (best train acc: 0.9004, best val acc: 0.9460)\n",
      "[Epoch: 35560] train loss: 0.3005, train acc: 0.8848, val loss: 0.2227, val acc: 0.9356  (best train acc: 0.9004, best val acc: 0.9460)\n",
      "[Epoch: 35580] train loss: 0.3254, train acc: 0.8762, val loss: 0.2015, val acc: 0.9386  (best train acc: 0.9004, best val acc: 0.9460)\n",
      "[Epoch: 35600] train loss: 0.2734, train acc: 0.8809, val loss: 0.2274, val acc: 0.9221  (best train acc: 0.9004, best val acc: 0.9460)\n",
      "[Epoch: 35620] train loss: 0.2895, train acc: 0.8762, val loss: 0.2152, val acc: 0.9342  (best train acc: 0.9004, best val acc: 0.9460)\n",
      "[Epoch: 35640] train loss: 0.2731, train acc: 0.8838, val loss: 0.2393, val acc: 0.9272  (best train acc: 0.9004, best val acc: 0.9460)\n",
      "[Epoch: 35660] train loss: 0.2692, train acc: 0.8869, val loss: 0.2366, val acc: 0.9234  (best train acc: 0.9004, best val acc: 0.9460)\n",
      "[Epoch: 35680] train loss: 0.2669, train acc: 0.8858, val loss: 0.2461, val acc: 0.9298  (best train acc: 0.9004, best val acc: 0.9460)\n",
      "[Epoch: 35700] train loss: 0.2796, train acc: 0.8721, val loss: 0.2369, val acc: 0.9390  (best train acc: 0.9004, best val acc: 0.9460)\n",
      "[Epoch: 35720] train loss: 0.2991, train acc: 0.8798, val loss: 0.2309, val acc: 0.9379  (best train acc: 0.9004, best val acc: 0.9460)\n",
      "[Epoch: 35740] train loss: 0.2698, train acc: 0.8839, val loss: 0.2098, val acc: 0.9325  (best train acc: 0.9004, best val acc: 0.9460)\n",
      "[Epoch: 35760] train loss: 0.2581, train acc: 0.8909, val loss: 0.2349, val acc: 0.9356  (best train acc: 0.9004, best val acc: 0.9460)\n",
      "[Epoch: 35780] train loss: 0.2803, train acc: 0.8794, val loss: 0.2272, val acc: 0.9325  (best train acc: 0.9004, best val acc: 0.9460)\n",
      "[Epoch: 35800] train loss: 0.2767, train acc: 0.8824, val loss: 0.2488, val acc: 0.9309  (best train acc: 0.9004, best val acc: 0.9460)\n",
      "[Epoch: 35820] train loss: 0.3011, train acc: 0.8782, val loss: 0.2679, val acc: 0.9251  (best train acc: 0.9004, best val acc: 0.9460)\n",
      "[Epoch: 35840] train loss: 0.2639, train acc: 0.8887, val loss: 0.2504, val acc: 0.9288  (best train acc: 0.9004, best val acc: 0.9460)\n",
      "[Epoch: 35860] train loss: 0.2880, train acc: 0.8809, val loss: 0.2403, val acc: 0.9369  (best train acc: 0.9025, best val acc: 0.9460)\n",
      "[Epoch: 35880] train loss: 0.2751, train acc: 0.8849, val loss: 0.2101, val acc: 0.9396  (best train acc: 0.9025, best val acc: 0.9460)\n",
      "[Epoch: 35900] train loss: 0.2673, train acc: 0.8841, val loss: 0.2427, val acc: 0.9214  (best train acc: 0.9025, best val acc: 0.9460)\n",
      "[Epoch: 35920] train loss: 0.2660, train acc: 0.8876, val loss: 0.2266, val acc: 0.9356  (best train acc: 0.9025, best val acc: 0.9460)\n",
      "[Epoch: 35940] train loss: 0.2671, train acc: 0.8857, val loss: 0.2656, val acc: 0.9349  (best train acc: 0.9025, best val acc: 0.9460)\n",
      "[Epoch: 35960] train loss: 0.2794, train acc: 0.8868, val loss: 0.2420, val acc: 0.9292  (best train acc: 0.9025, best val acc: 0.9460)\n",
      "[Epoch: 35980] train loss: 0.2841, train acc: 0.8721, val loss: 0.2363, val acc: 0.9319  (best train acc: 0.9025, best val acc: 0.9460)\n",
      "[Epoch: 36000] train loss: 0.2546, train acc: 0.8909, val loss: 0.2268, val acc: 0.9282  (best train acc: 0.9025, best val acc: 0.9460)\n",
      "[Epoch: 36020] train loss: 0.2683, train acc: 0.8884, val loss: 0.2891, val acc: 0.9268  (best train acc: 0.9025, best val acc: 0.9460)\n",
      "[Epoch: 36040] train loss: 0.2870, train acc: 0.8789, val loss: 0.2291, val acc: 0.9312  (best train acc: 0.9025, best val acc: 0.9460)\n",
      "[Epoch: 36060] train loss: 0.2742, train acc: 0.8873, val loss: 0.2422, val acc: 0.9366  (best train acc: 0.9025, best val acc: 0.9460)\n",
      "[Epoch: 36080] train loss: 0.2836, train acc: 0.8751, val loss: 0.2615, val acc: 0.9292  (best train acc: 0.9025, best val acc: 0.9460)\n",
      "[Epoch: 36100] train loss: 0.2960, train acc: 0.8725, val loss: 0.2496, val acc: 0.9332  (best train acc: 0.9025, best val acc: 0.9460)\n",
      "[Epoch: 36120] train loss: 0.2653, train acc: 0.8781, val loss: 0.2471, val acc: 0.9298  (best train acc: 0.9025, best val acc: 0.9460)\n",
      "[Epoch: 36140] train loss: 0.2754, train acc: 0.8861, val loss: 0.2232, val acc: 0.9282  (best train acc: 0.9025, best val acc: 0.9460)\n",
      "[Epoch: 36160] train loss: 0.2795, train acc: 0.8856, val loss: 0.2737, val acc: 0.9325  (best train acc: 0.9025, best val acc: 0.9460)\n",
      "[Epoch: 36180] train loss: 0.2947, train acc: 0.8780, val loss: 0.2479, val acc: 0.9288  (best train acc: 0.9025, best val acc: 0.9460)\n",
      "[Epoch: 36200] train loss: 0.2897, train acc: 0.8858, val loss: 0.2316, val acc: 0.9255  (best train acc: 0.9025, best val acc: 0.9460)\n",
      "[Epoch: 36220] train loss: 0.2919, train acc: 0.8748, val loss: 0.1998, val acc: 0.9383  (best train acc: 0.9025, best val acc: 0.9460)\n",
      "[Epoch: 36240] train loss: 0.2676, train acc: 0.8846, val loss: 0.2761, val acc: 0.9248  (best train acc: 0.9025, best val acc: 0.9460)\n",
      "[Epoch: 36260] train loss: 0.2893, train acc: 0.8809, val loss: 0.2332, val acc: 0.9322  (best train acc: 0.9025, best val acc: 0.9460)\n",
      "[Epoch: 36280] train loss: 0.2675, train acc: 0.8897, val loss: 0.2090, val acc: 0.9336  (best train acc: 0.9025, best val acc: 0.9460)\n",
      "[Epoch: 36300] train loss: 0.2639, train acc: 0.8874, val loss: 0.2222, val acc: 0.9336  (best train acc: 0.9025, best val acc: 0.9460)\n",
      "[Epoch: 36320] train loss: 0.2882, train acc: 0.8822, val loss: 0.2411, val acc: 0.9342  (best train acc: 0.9025, best val acc: 0.9460)\n",
      "[Epoch: 36340] train loss: 0.2588, train acc: 0.8866, val loss: 0.2461, val acc: 0.9278  (best train acc: 0.9025, best val acc: 0.9460)\n",
      "[Epoch: 36360] train loss: 0.2583, train acc: 0.8849, val loss: 0.2610, val acc: 0.9231  (best train acc: 0.9025, best val acc: 0.9460)\n",
      "[Epoch: 36380] train loss: 0.2881, train acc: 0.8777, val loss: 0.2194, val acc: 0.9275  (best train acc: 0.9025, best val acc: 0.9460)\n",
      "[Epoch: 36400] train loss: 0.2889, train acc: 0.8801, val loss: 0.2474, val acc: 0.9255  (best train acc: 0.9025, best val acc: 0.9460)\n",
      "[Epoch: 36420] train loss: 0.2885, train acc: 0.8726, val loss: 0.2165, val acc: 0.9295  (best train acc: 0.9025, best val acc: 0.9460)\n",
      "[Epoch: 36440] train loss: 0.2849, train acc: 0.8859, val loss: 0.2282, val acc: 0.9312  (best train acc: 0.9025, best val acc: 0.9460)\n",
      "[Epoch: 36460] train loss: 0.2708, train acc: 0.8810, val loss: 0.2441, val acc: 0.9329  (best train acc: 0.9025, best val acc: 0.9460)\n",
      "[Epoch: 36480] train loss: 0.2772, train acc: 0.8827, val loss: 0.2422, val acc: 0.9349  (best train acc: 0.9025, best val acc: 0.9460)\n",
      "[Epoch: 36500] train loss: 0.2980, train acc: 0.8775, val loss: 0.2281, val acc: 0.9251  (best train acc: 0.9025, best val acc: 0.9460)\n",
      "[Epoch: 36520] train loss: 0.2510, train acc: 0.8931, val loss: 0.2384, val acc: 0.9329  (best train acc: 0.9025, best val acc: 0.9460)\n",
      "[Epoch: 36540] train loss: 0.2724, train acc: 0.8848, val loss: 0.2812, val acc: 0.9187  (best train acc: 0.9025, best val acc: 0.9460)\n",
      "[Epoch: 36560] train loss: 0.2633, train acc: 0.8897, val loss: 0.2213, val acc: 0.9285  (best train acc: 0.9025, best val acc: 0.9460)\n",
      "[Epoch: 36580] train loss: 0.2604, train acc: 0.8896, val loss: 0.2278, val acc: 0.9383  (best train acc: 0.9025, best val acc: 0.9460)\n",
      "[Epoch: 36600] train loss: 0.2964, train acc: 0.8803, val loss: 0.2397, val acc: 0.9238  (best train acc: 0.9025, best val acc: 0.9460)\n",
      "[Epoch: 36620] train loss: 0.2714, train acc: 0.8861, val loss: 0.2296, val acc: 0.9292  (best train acc: 0.9025, best val acc: 0.9460)\n",
      "[Epoch: 36640] train loss: 0.2646, train acc: 0.8874, val loss: 0.2198, val acc: 0.9342  (best train acc: 0.9025, best val acc: 0.9460)\n",
      "[Epoch: 36660] train loss: 0.3072, train acc: 0.8605, val loss: 0.2329, val acc: 0.9025  (best train acc: 0.9025, best val acc: 0.9460)\n",
      "[Epoch: 36680] train loss: 0.2700, train acc: 0.8874, val loss: 0.2267, val acc: 0.9214  (best train acc: 0.9025, best val acc: 0.9460)\n",
      "[Epoch: 36700] train loss: 0.2690, train acc: 0.8857, val loss: 0.2219, val acc: 0.9336  (best train acc: 0.9025, best val acc: 0.9460)\n",
      "[Epoch: 36720] train loss: 0.2678, train acc: 0.8850, val loss: 0.2626, val acc: 0.9288  (best train acc: 0.9025, best val acc: 0.9460)\n",
      "[Epoch: 36740] train loss: 0.2912, train acc: 0.8810, val loss: 0.2169, val acc: 0.9248  (best train acc: 0.9025, best val acc: 0.9460)\n",
      "[Epoch: 36760] train loss: 0.2515, train acc: 0.8944, val loss: 0.2248, val acc: 0.9363  (best train acc: 0.9025, best val acc: 0.9460)\n",
      "[Epoch: 36780] train loss: 0.2860, train acc: 0.8869, val loss: 0.2221, val acc: 0.9369  (best train acc: 0.9025, best val acc: 0.9460)\n",
      "[Epoch: 36800] train loss: 0.2885, train acc: 0.8755, val loss: 0.2306, val acc: 0.9288  (best train acc: 0.9025, best val acc: 0.9460)\n",
      "[Epoch: 36820] train loss: 0.2514, train acc: 0.8965, val loss: 0.2171, val acc: 0.9231  (best train acc: 0.9025, best val acc: 0.9460)\n",
      "[Epoch: 36840] train loss: 0.2717, train acc: 0.8885, val loss: 0.2418, val acc: 0.9288  (best train acc: 0.9025, best val acc: 0.9460)\n",
      "[Epoch: 36860] train loss: 0.2718, train acc: 0.8878, val loss: 0.2341, val acc: 0.9352  (best train acc: 0.9025, best val acc: 0.9460)\n",
      "[Epoch: 36880] train loss: 0.2548, train acc: 0.8905, val loss: 0.2457, val acc: 0.9258  (best train acc: 0.9025, best val acc: 0.9460)\n",
      "[Epoch: 36900] train loss: 0.2581, train acc: 0.8939, val loss: 0.2557, val acc: 0.9346  (best train acc: 0.9025, best val acc: 0.9460)\n",
      "[Epoch: 36920] train loss: 0.2774, train acc: 0.8851, val loss: 0.2046, val acc: 0.9309  (best train acc: 0.9025, best val acc: 0.9460)\n",
      "[Epoch: 36940] train loss: 0.2887, train acc: 0.8707, val loss: 0.2128, val acc: 0.9319  (best train acc: 0.9025, best val acc: 0.9460)\n",
      "[Epoch: 36960] train loss: 0.2869, train acc: 0.8791, val loss: 0.2393, val acc: 0.9309  (best train acc: 0.9025, best val acc: 0.9460)\n",
      "[Epoch: 36980] train loss: 0.2685, train acc: 0.8890, val loss: 0.2453, val acc: 0.9356  (best train acc: 0.9025, best val acc: 0.9460)\n",
      "[Epoch: 37000] train loss: 0.2591, train acc: 0.8895, val loss: 0.1909, val acc: 0.9400  (best train acc: 0.9025, best val acc: 0.9460)\n",
      "[Epoch: 37020] train loss: 0.2517, train acc: 0.8979, val loss: 0.2167, val acc: 0.9329  (best train acc: 0.9025, best val acc: 0.9460)\n",
      "[Epoch: 37040] train loss: 0.2514, train acc: 0.8884, val loss: 0.2062, val acc: 0.9359  (best train acc: 0.9025, best val acc: 0.9460)\n",
      "[Epoch: 37060] train loss: 0.2550, train acc: 0.8894, val loss: 0.2115, val acc: 0.9346  (best train acc: 0.9025, best val acc: 0.9460)\n",
      "[Epoch: 37080] train loss: 0.2589, train acc: 0.8971, val loss: 0.2399, val acc: 0.9369  (best train acc: 0.9025, best val acc: 0.9460)\n",
      "[Epoch: 37100] train loss: 0.2561, train acc: 0.8933, val loss: 0.2228, val acc: 0.9379  (best train acc: 0.9025, best val acc: 0.9460)\n",
      "[Epoch: 37120] train loss: 0.2344, train acc: 0.9010, val loss: 0.2202, val acc: 0.9288  (best train acc: 0.9025, best val acc: 0.9460)\n",
      "[Epoch: 37140] train loss: 0.2484, train acc: 0.8957, val loss: 0.2088, val acc: 0.9356  (best train acc: 0.9025, best val acc: 0.9460)\n",
      "[Epoch: 37160] train loss: 0.2564, train acc: 0.8853, val loss: 0.2346, val acc: 0.9322  (best train acc: 0.9025, best val acc: 0.9460)\n",
      "[Epoch: 37180] train loss: 0.2650, train acc: 0.8958, val loss: 0.2276, val acc: 0.9393  (best train acc: 0.9036, best val acc: 0.9460)\n",
      "[Epoch: 37200] train loss: 0.2981, train acc: 0.8780, val loss: 0.2056, val acc: 0.9336  (best train acc: 0.9036, best val acc: 0.9460)\n",
      "[Epoch: 37220] train loss: 0.2607, train acc: 0.8965, val loss: 0.2123, val acc: 0.9359  (best train acc: 0.9067, best val acc: 0.9460)\n",
      "[Epoch: 37240] train loss: 0.2448, train acc: 0.8963, val loss: 0.2164, val acc: 0.9417  (best train acc: 0.9067, best val acc: 0.9460)\n",
      "[Epoch: 37260] train loss: 0.2517, train acc: 0.8968, val loss: 0.2289, val acc: 0.9366  (best train acc: 0.9067, best val acc: 0.9460)\n",
      "[Epoch: 37280] train loss: 0.2426, train acc: 0.8997, val loss: 0.2041, val acc: 0.9457  (best train acc: 0.9068, best val acc: 0.9460)\n",
      "[Epoch: 37300] train loss: 0.2482, train acc: 0.8977, val loss: 0.2166, val acc: 0.9413  (best train acc: 0.9080, best val acc: 0.9460)\n",
      "[Epoch: 37320] train loss: 0.2577, train acc: 0.8920, val loss: 0.2101, val acc: 0.9417  (best train acc: 0.9082, best val acc: 0.9460)\n",
      "[Epoch: 37340] train loss: 0.2841, train acc: 0.8924, val loss: 0.2166, val acc: 0.9400  (best train acc: 0.9120, best val acc: 0.9460)\n",
      "[Epoch: 37360] train loss: 0.2458, train acc: 0.9012, val loss: 0.2013, val acc: 0.9396  (best train acc: 0.9120, best val acc: 0.9460)\n",
      "[Epoch: 37380] train loss: 0.3039, train acc: 0.8727, val loss: 0.2877, val acc: 0.9255  (best train acc: 0.9120, best val acc: 0.9460)\n",
      "[Epoch: 37400] train loss: 0.2563, train acc: 0.8967, val loss: 0.2182, val acc: 0.9352  (best train acc: 0.9120, best val acc: 0.9460)\n",
      "[Epoch: 37420] train loss: 0.2755, train acc: 0.8924, val loss: 0.1877, val acc: 0.9406  (best train acc: 0.9120, best val acc: 0.9460)\n",
      "[Epoch: 37440] train loss: 0.2585, train acc: 0.8952, val loss: 0.2704, val acc: 0.9261  (best train acc: 0.9124, best val acc: 0.9460)\n",
      "[Epoch: 37460] train loss: 0.2637, train acc: 0.8942, val loss: 0.1888, val acc: 0.9423  (best train acc: 0.9124, best val acc: 0.9460)\n",
      "[Epoch: 37480] train loss: 0.2436, train acc: 0.9083, val loss: 0.2125, val acc: 0.9386  (best train acc: 0.9124, best val acc: 0.9460)\n",
      "[Epoch: 37500] train loss: 0.2405, train acc: 0.9050, val loss: 0.2255, val acc: 0.9423  (best train acc: 0.9124, best val acc: 0.9464)\n",
      "[Epoch: 37520] train loss: 0.2224, train acc: 0.9123, val loss: 0.2160, val acc: 0.9386  (best train acc: 0.9151, best val acc: 0.9467)\n",
      "[Epoch: 37540] train loss: 0.2276, train acc: 0.9108, val loss: 0.1951, val acc: 0.9410  (best train acc: 0.9151, best val acc: 0.9467)\n",
      "[Epoch: 37560] train loss: 0.2322, train acc: 0.9033, val loss: 0.1907, val acc: 0.9423  (best train acc: 0.9151, best val acc: 0.9467)\n",
      "[Epoch: 37580] train loss: 0.2805, train acc: 0.8814, val loss: 0.2161, val acc: 0.9396  (best train acc: 0.9151, best val acc: 0.9467)\n",
      "[Epoch: 37600] train loss: 0.2284, train acc: 0.9103, val loss: 0.2106, val acc: 0.9403  (best train acc: 0.9151, best val acc: 0.9470)\n",
      "[Epoch: 37620] train loss: 0.2403, train acc: 0.9044, val loss: 0.2112, val acc: 0.9410  (best train acc: 0.9153, best val acc: 0.9470)\n",
      "[Epoch: 37640] train loss: 0.2824, train acc: 0.8895, val loss: 0.2007, val acc: 0.9423  (best train acc: 0.9153, best val acc: 0.9470)\n",
      "[Epoch: 37660] train loss: 0.2317, train acc: 0.9072, val loss: 0.2128, val acc: 0.9417  (best train acc: 0.9153, best val acc: 0.9470)\n",
      "[Epoch: 37680] train loss: 0.2487, train acc: 0.8991, val loss: 0.1980, val acc: 0.9433  (best train acc: 0.9153, best val acc: 0.9470)\n",
      "[Epoch: 37700] train loss: 0.2250, train acc: 0.9036, val loss: 0.2041, val acc: 0.9349  (best train acc: 0.9153, best val acc: 0.9470)\n",
      "[Epoch: 37720] train loss: 0.2259, train acc: 0.9082, val loss: 0.2405, val acc: 0.9329  (best train acc: 0.9179, best val acc: 0.9470)\n",
      "[Epoch: 37740] train loss: 0.3027, train acc: 0.8686, val loss: 0.1805, val acc: 0.9423  (best train acc: 0.9179, best val acc: 0.9470)\n",
      "[Epoch: 37760] train loss: 0.2380, train acc: 0.9109, val loss: 0.1910, val acc: 0.9423  (best train acc: 0.9179, best val acc: 0.9474)\n",
      "[Epoch: 37780] train loss: 0.2438, train acc: 0.9164, val loss: 0.1949, val acc: 0.9454  (best train acc: 0.9179, best val acc: 0.9474)\n",
      "[Epoch: 37800] train loss: 0.2300, train acc: 0.9114, val loss: 0.1992, val acc: 0.9410  (best train acc: 0.9179, best val acc: 0.9474)\n",
      "[Epoch: 37820] train loss: 0.2394, train acc: 0.9000, val loss: 0.1844, val acc: 0.9383  (best train acc: 0.9179, best val acc: 0.9474)\n",
      "[Epoch: 37840] train loss: 0.2511, train acc: 0.9020, val loss: 0.1958, val acc: 0.9379  (best train acc: 0.9179, best val acc: 0.9474)\n",
      "[Epoch: 37860] train loss: 0.2436, train acc: 0.9078, val loss: 0.2250, val acc: 0.9437  (best train acc: 0.9179, best val acc: 0.9474)\n",
      "[Epoch: 37880] train loss: 0.2277, train acc: 0.9057, val loss: 0.2270, val acc: 0.9393  (best train acc: 0.9179, best val acc: 0.9474)\n",
      "[Epoch: 37900] train loss: 0.2708, train acc: 0.8890, val loss: 0.2026, val acc: 0.9413  (best train acc: 0.9179, best val acc: 0.9474)\n",
      "[Epoch: 37920] train loss: 0.2392, train acc: 0.9079, val loss: 0.2055, val acc: 0.9410  (best train acc: 0.9179, best val acc: 0.9474)\n",
      "[Epoch: 37940] train loss: 0.2579, train acc: 0.8859, val loss: 0.2246, val acc: 0.9447  (best train acc: 0.9179, best val acc: 0.9474)\n",
      "[Epoch: 37960] train loss: 0.2420, train acc: 0.9065, val loss: 0.1958, val acc: 0.9322  (best train acc: 0.9179, best val acc: 0.9474)\n",
      "[Epoch: 37980] train loss: 0.2451, train acc: 0.8998, val loss: 0.1852, val acc: 0.9460  (best train acc: 0.9179, best val acc: 0.9474)\n",
      "[Epoch: 38000] train loss: 0.2655, train acc: 0.8845, val loss: 0.2021, val acc: 0.9373  (best train acc: 0.9179, best val acc: 0.9474)\n",
      "[Epoch: 38020] train loss: 0.2335, train acc: 0.9078, val loss: 0.1900, val acc: 0.9447  (best train acc: 0.9179, best val acc: 0.9474)\n",
      "[Epoch: 38040] train loss: 0.2442, train acc: 0.9012, val loss: 0.2149, val acc: 0.9413  (best train acc: 0.9179, best val acc: 0.9474)\n",
      "[Epoch: 38060] train loss: 0.2476, train acc: 0.8973, val loss: 0.2139, val acc: 0.9423  (best train acc: 0.9184, best val acc: 0.9474)\n",
      "[Epoch: 38080] train loss: 0.2523, train acc: 0.9004, val loss: 0.1925, val acc: 0.9420  (best train acc: 0.9184, best val acc: 0.9474)\n",
      "[Epoch: 38100] train loss: 0.2502, train acc: 0.8970, val loss: 0.1988, val acc: 0.9410  (best train acc: 0.9184, best val acc: 0.9474)\n",
      "[Epoch: 38120] train loss: 0.2702, train acc: 0.8934, val loss: 0.1966, val acc: 0.9430  (best train acc: 0.9184, best val acc: 0.9474)\n",
      "[Epoch: 38140] train loss: 0.2261, train acc: 0.9075, val loss: 0.1936, val acc: 0.9417  (best train acc: 0.9184, best val acc: 0.9474)\n",
      "[Epoch: 38160] train loss: 0.2330, train acc: 0.9104, val loss: 0.2045, val acc: 0.9437  (best train acc: 0.9184, best val acc: 0.9474)\n",
      "[Epoch: 38180] train loss: 0.2328, train acc: 0.9062, val loss: 0.2168, val acc: 0.9403  (best train acc: 0.9203, best val acc: 0.9474)\n",
      "[Epoch: 38200] train loss: 0.2418, train acc: 0.9083, val loss: 0.2310, val acc: 0.9433  (best train acc: 0.9203, best val acc: 0.9474)\n",
      "[Epoch: 38220] train loss: 0.2370, train acc: 0.9045, val loss: 0.2001, val acc: 0.9420  (best train acc: 0.9203, best val acc: 0.9474)\n",
      "[Epoch: 38240] train loss: 0.2404, train acc: 0.9035, val loss: 0.2491, val acc: 0.9457  (best train acc: 0.9203, best val acc: 0.9474)\n",
      "[Epoch: 38260] train loss: 0.2799, train acc: 0.8853, val loss: 0.1929, val acc: 0.9393  (best train acc: 0.9203, best val acc: 0.9474)\n",
      "[Epoch: 38280] train loss: 0.2549, train acc: 0.9035, val loss: 0.1817, val acc: 0.9423  (best train acc: 0.9203, best val acc: 0.9474)\n",
      "[Epoch: 38300] train loss: 0.2442, train acc: 0.9041, val loss: 0.2285, val acc: 0.9423  (best train acc: 0.9203, best val acc: 0.9474)\n",
      "[Epoch: 38320] train loss: 0.2220, train acc: 0.9130, val loss: 0.1965, val acc: 0.9444  (best train acc: 0.9203, best val acc: 0.9474)\n",
      "[Epoch: 38340] train loss: 0.2251, train acc: 0.9091, val loss: 0.1962, val acc: 0.9447  (best train acc: 0.9203, best val acc: 0.9474)\n",
      "[Epoch: 38360] train loss: 0.2789, train acc: 0.8856, val loss: 0.1890, val acc: 0.9430  (best train acc: 0.9203, best val acc: 0.9474)\n",
      "[Epoch: 38380] train loss: 0.2385, train acc: 0.9046, val loss: 0.2572, val acc: 0.9336  (best train acc: 0.9203, best val acc: 0.9474)\n",
      "[Epoch: 38400] train loss: 0.2675, train acc: 0.8961, val loss: 0.2067, val acc: 0.9433  (best train acc: 0.9203, best val acc: 0.9474)\n",
      "[Epoch: 38420] train loss: 0.2180, train acc: 0.9113, val loss: 0.2001, val acc: 0.9423  (best train acc: 0.9203, best val acc: 0.9474)\n",
      "[Epoch: 38440] train loss: 0.2687, train acc: 0.8823, val loss: 0.1999, val acc: 0.9437  (best train acc: 0.9203, best val acc: 0.9474)\n",
      "[Epoch: 38460] train loss: 0.2334, train acc: 0.9119, val loss: 0.1821, val acc: 0.9447  (best train acc: 0.9203, best val acc: 0.9474)\n",
      "[Epoch: 38480] train loss: 0.2450, train acc: 0.8981, val loss: 0.1921, val acc: 0.9423  (best train acc: 0.9203, best val acc: 0.9474)\n",
      "[Epoch: 38500] train loss: 0.2579, train acc: 0.9017, val loss: 0.2006, val acc: 0.9437  (best train acc: 0.9203, best val acc: 0.9474)\n",
      "[Epoch: 38520] train loss: 0.2721, train acc: 0.8984, val loss: 0.1847, val acc: 0.9474  (best train acc: 0.9203, best val acc: 0.9474)\n",
      "[Epoch: 38540] train loss: 0.2329, train acc: 0.9092, val loss: 0.2034, val acc: 0.9427  (best train acc: 0.9203, best val acc: 0.9474)\n",
      "[Epoch: 38560] train loss: 0.2370, train acc: 0.9057, val loss: 0.2426, val acc: 0.9292  (best train acc: 0.9203, best val acc: 0.9477)\n",
      "[Epoch: 38580] train loss: 0.2625, train acc: 0.8905, val loss: 0.2594, val acc: 0.9282  (best train acc: 0.9203, best val acc: 0.9477)\n",
      "[Epoch: 38600] train loss: 0.2378, train acc: 0.9085, val loss: 0.2338, val acc: 0.9366  (best train acc: 0.9203, best val acc: 0.9477)\n",
      "[Epoch: 38620] train loss: 0.2408, train acc: 0.9036, val loss: 0.1877, val acc: 0.9423  (best train acc: 0.9203, best val acc: 0.9477)\n",
      "[Epoch: 38640] train loss: 0.2775, train acc: 0.8998, val loss: 0.2131, val acc: 0.9467  (best train acc: 0.9203, best val acc: 0.9477)\n",
      "[Epoch: 38660] train loss: 0.2351, train acc: 0.9074, val loss: 0.2270, val acc: 0.9376  (best train acc: 0.9203, best val acc: 0.9477)\n",
      "[Epoch: 38680] train loss: 0.2305, train acc: 0.9060, val loss: 0.2002, val acc: 0.9437  (best train acc: 0.9203, best val acc: 0.9477)\n",
      "[Epoch: 38700] train loss: 0.2403, train acc: 0.9077, val loss: 0.2052, val acc: 0.9454  (best train acc: 0.9203, best val acc: 0.9477)\n",
      "[Epoch: 38720] train loss: 0.2320, train acc: 0.9088, val loss: 0.1885, val acc: 0.9440  (best train acc: 0.9203, best val acc: 0.9477)\n",
      "[Epoch: 38740] train loss: 0.2295, train acc: 0.9127, val loss: 0.2314, val acc: 0.9437  (best train acc: 0.9203, best val acc: 0.9477)\n",
      "[Epoch: 38760] train loss: 0.2378, train acc: 0.9116, val loss: 0.1873, val acc: 0.9447  (best train acc: 0.9203, best val acc: 0.9477)\n",
      "[Epoch: 38780] train loss: 0.2369, train acc: 0.9065, val loss: 0.2012, val acc: 0.9413  (best train acc: 0.9203, best val acc: 0.9477)\n",
      "[Epoch: 38800] train loss: 0.2374, train acc: 0.9075, val loss: 0.2162, val acc: 0.9430  (best train acc: 0.9203, best val acc: 0.9477)\n",
      "[Epoch: 38820] train loss: 0.2309, train acc: 0.9106, val loss: 0.2513, val acc: 0.9325  (best train acc: 0.9203, best val acc: 0.9477)\n",
      "[Epoch: 38840] train loss: 0.2447, train acc: 0.9038, val loss: 0.2380, val acc: 0.9393  (best train acc: 0.9203, best val acc: 0.9477)\n",
      "[Epoch: 38860] train loss: 0.2439, train acc: 0.9017, val loss: 0.2028, val acc: 0.9336  (best train acc: 0.9203, best val acc: 0.9477)\n",
      "[Epoch: 38880] train loss: 0.2374, train acc: 0.8990, val loss: 0.1951, val acc: 0.9400  (best train acc: 0.9203, best val acc: 0.9477)\n",
      "[Epoch: 38900] train loss: 0.2162, train acc: 0.9122, val loss: 0.2360, val acc: 0.9423  (best train acc: 0.9203, best val acc: 0.9477)\n",
      "[Epoch: 38920] train loss: 0.2247, train acc: 0.9144, val loss: 0.2109, val acc: 0.9454  (best train acc: 0.9203, best val acc: 0.9477)\n",
      "[Epoch: 38940] train loss: 0.2659, train acc: 0.9012, val loss: 0.2017, val acc: 0.9457  (best train acc: 0.9203, best val acc: 0.9477)\n",
      "[Epoch: 38960] train loss: 0.2523, train acc: 0.8977, val loss: 0.1940, val acc: 0.9440  (best train acc: 0.9203, best val acc: 0.9477)\n",
      "[Epoch: 38980] train loss: 0.2860, train acc: 0.8880, val loss: 0.1856, val acc: 0.9423  (best train acc: 0.9203, best val acc: 0.9477)\n",
      "[Epoch: 39000] train loss: 0.2265, train acc: 0.9091, val loss: 0.1994, val acc: 0.9467  (best train acc: 0.9203, best val acc: 0.9477)\n",
      "[Epoch: 39020] train loss: 0.2608, train acc: 0.8957, val loss: 0.2028, val acc: 0.9423  (best train acc: 0.9203, best val acc: 0.9477)\n",
      "[Epoch: 39040] train loss: 0.2382, train acc: 0.9086, val loss: 0.2015, val acc: 0.9454  (best train acc: 0.9203, best val acc: 0.9477)\n",
      "[Epoch: 39060] train loss: 0.2269, train acc: 0.9089, val loss: 0.2144, val acc: 0.9417  (best train acc: 0.9203, best val acc: 0.9477)\n",
      "[Epoch: 39080] train loss: 0.2471, train acc: 0.9049, val loss: 0.2055, val acc: 0.9410  (best train acc: 0.9203, best val acc: 0.9477)\n",
      "[Epoch: 39100] train loss: 0.2467, train acc: 0.9001, val loss: 0.2194, val acc: 0.9437  (best train acc: 0.9203, best val acc: 0.9477)\n",
      "[Epoch: 39120] train loss: 0.2372, train acc: 0.9126, val loss: 0.2088, val acc: 0.9423  (best train acc: 0.9203, best val acc: 0.9477)\n",
      "[Epoch: 39140] train loss: 0.2497, train acc: 0.9054, val loss: 0.2237, val acc: 0.9383  (best train acc: 0.9203, best val acc: 0.9477)\n",
      "[Epoch: 39160] train loss: 0.2528, train acc: 0.8967, val loss: 0.2096, val acc: 0.9437  (best train acc: 0.9203, best val acc: 0.9477)\n",
      "[Epoch: 39180] train loss: 0.2426, train acc: 0.9012, val loss: 0.2166, val acc: 0.9413  (best train acc: 0.9203, best val acc: 0.9477)\n",
      "[Epoch: 39200] train loss: 0.2459, train acc: 0.9004, val loss: 0.2110, val acc: 0.9406  (best train acc: 0.9203, best val acc: 0.9477)\n",
      "[Epoch: 39220] train loss: 0.2353, train acc: 0.9082, val loss: 0.2264, val acc: 0.9400  (best train acc: 0.9203, best val acc: 0.9477)\n",
      "[Epoch: 39240] train loss: 0.2348, train acc: 0.9131, val loss: 0.2108, val acc: 0.9437  (best train acc: 0.9203, best val acc: 0.9477)\n",
      "[Epoch: 39260] train loss: 0.2511, train acc: 0.9044, val loss: 0.2604, val acc: 0.9302  (best train acc: 0.9203, best val acc: 0.9477)\n",
      "[Epoch: 39280] train loss: 0.2664, train acc: 0.9009, val loss: 0.2039, val acc: 0.9440  (best train acc: 0.9203, best val acc: 0.9477)\n",
      "[Epoch: 39300] train loss: 0.2727, train acc: 0.9024, val loss: 0.2364, val acc: 0.9437  (best train acc: 0.9203, best val acc: 0.9477)\n",
      "[Epoch: 39320] train loss: 0.2499, train acc: 0.8984, val loss: 0.2491, val acc: 0.9315  (best train acc: 0.9203, best val acc: 0.9477)\n",
      "[Epoch: 39340] train loss: 0.2330, train acc: 0.9037, val loss: 0.1962, val acc: 0.9433  (best train acc: 0.9203, best val acc: 0.9477)\n",
      "[Epoch: 39360] train loss: 0.2276, train acc: 0.9134, val loss: 0.2271, val acc: 0.9400  (best train acc: 0.9203, best val acc: 0.9477)\n",
      "[Epoch: 39380] train loss: 0.2382, train acc: 0.9081, val loss: 0.2417, val acc: 0.9393  (best train acc: 0.9203, best val acc: 0.9477)\n",
      "[Epoch: 39400] train loss: 0.2194, train acc: 0.9159, val loss: 0.2130, val acc: 0.9403  (best train acc: 0.9203, best val acc: 0.9477)\n",
      "[Epoch: 39420] train loss: 0.2391, train acc: 0.8997, val loss: 0.2263, val acc: 0.9400  (best train acc: 0.9203, best val acc: 0.9477)\n",
      "[Epoch: 39440] train loss: 0.2242, train acc: 0.9152, val loss: 0.2273, val acc: 0.9400  (best train acc: 0.9203, best val acc: 0.9477)\n",
      "[Epoch: 39460] train loss: 0.2293, train acc: 0.9105, val loss: 0.2098, val acc: 0.9430  (best train acc: 0.9203, best val acc: 0.9477)\n",
      "[Epoch: 39480] train loss: 0.2368, train acc: 0.9004, val loss: 0.2156, val acc: 0.9295  (best train acc: 0.9203, best val acc: 0.9477)\n",
      "[Epoch: 39500] train loss: 0.2461, train acc: 0.8926, val loss: 0.1955, val acc: 0.9427  (best train acc: 0.9203, best val acc: 0.9477)\n",
      "[Epoch: 39520] train loss: 0.2132, train acc: 0.9190, val loss: 0.1895, val acc: 0.9417  (best train acc: 0.9203, best val acc: 0.9477)\n",
      "[Epoch: 39540] train loss: 0.2701, train acc: 0.8957, val loss: 0.2059, val acc: 0.9403  (best train acc: 0.9203, best val acc: 0.9477)\n",
      "[Epoch: 39560] train loss: 0.2744, train acc: 0.8986, val loss: 0.1792, val acc: 0.9447  (best train acc: 0.9203, best val acc: 0.9477)\n",
      "[Epoch: 39580] train loss: 0.2473, train acc: 0.9069, val loss: 0.2289, val acc: 0.9440  (best train acc: 0.9203, best val acc: 0.9477)\n",
      "[Epoch: 39600] train loss: 0.2720, train acc: 0.8931, val loss: 0.2152, val acc: 0.9440  (best train acc: 0.9203, best val acc: 0.9477)\n",
      "[Epoch: 39620] train loss: 0.2340, train acc: 0.9075, val loss: 0.2273, val acc: 0.9390  (best train acc: 0.9203, best val acc: 0.9477)\n",
      "[Epoch: 39640] train loss: 0.2464, train acc: 0.9098, val loss: 0.1964, val acc: 0.9444  (best train acc: 0.9203, best val acc: 0.9477)\n",
      "[Epoch: 39660] train loss: 0.2993, train acc: 0.8811, val loss: 0.2539, val acc: 0.9194  (best train acc: 0.9203, best val acc: 0.9477)\n",
      "[Epoch: 39680] train loss: 0.2557, train acc: 0.9002, val loss: 0.2053, val acc: 0.9376  (best train acc: 0.9203, best val acc: 0.9477)\n",
      "[Epoch: 39700] train loss: 0.2269, train acc: 0.9087, val loss: 0.1962, val acc: 0.9464  (best train acc: 0.9203, best val acc: 0.9477)\n",
      "[Epoch: 39720] train loss: 0.2320, train acc: 0.9092, val loss: 0.2147, val acc: 0.9420  (best train acc: 0.9203, best val acc: 0.9477)\n",
      "[Epoch: 39740] train loss: 0.2417, train acc: 0.9093, val loss: 0.2038, val acc: 0.9437  (best train acc: 0.9203, best val acc: 0.9477)\n",
      "[Epoch: 39760] train loss: 0.2256, train acc: 0.9108, val loss: 0.2027, val acc: 0.9430  (best train acc: 0.9203, best val acc: 0.9477)\n",
      "[Epoch: 39780] train loss: 0.2273, train acc: 0.9088, val loss: 0.2019, val acc: 0.9413  (best train acc: 0.9203, best val acc: 0.9477)\n",
      "[Epoch: 39800] train loss: 0.2418, train acc: 0.9088, val loss: 0.2373, val acc: 0.9393  (best train acc: 0.9203, best val acc: 0.9477)\n",
      "[Epoch: 39820] train loss: 0.2257, train acc: 0.9111, val loss: 0.2381, val acc: 0.9413  (best train acc: 0.9203, best val acc: 0.9477)\n",
      "[Epoch: 39840] train loss: 0.2315, train acc: 0.9023, val loss: 0.2241, val acc: 0.9460  (best train acc: 0.9203, best val acc: 0.9477)\n",
      "[Epoch: 39860] train loss: 0.2251, train acc: 0.9113, val loss: 0.2036, val acc: 0.9454  (best train acc: 0.9203, best val acc: 0.9477)\n",
      "[Epoch: 39880] train loss: 0.2281, train acc: 0.9106, val loss: 0.2037, val acc: 0.9400  (best train acc: 0.9203, best val acc: 0.9477)\n",
      "[Epoch: 39900] train loss: 0.2536, train acc: 0.9020, val loss: 0.2027, val acc: 0.9447  (best train acc: 0.9203, best val acc: 0.9477)\n",
      "[Epoch: 39920] train loss: 0.2735, train acc: 0.8929, val loss: 0.1947, val acc: 0.9437  (best train acc: 0.9203, best val acc: 0.9477)\n",
      "[Epoch: 39940] train loss: 0.2511, train acc: 0.9028, val loss: 0.2206, val acc: 0.9433  (best train acc: 0.9203, best val acc: 0.9477)\n",
      "[Epoch: 39960] train loss: 0.2500, train acc: 0.9025, val loss: 0.2323, val acc: 0.9336  (best train acc: 0.9203, best val acc: 0.9477)\n",
      "[Epoch: 39980] train loss: 0.2388, train acc: 0.9084, val loss: 0.2078, val acc: 0.9363  (best train acc: 0.9203, best val acc: 0.9477)\n",
      "[Epoch: 40000] train loss: 0.2320, train acc: 0.9053, val loss: 0.2037, val acc: 0.9447  (best train acc: 0.9203, best val acc: 0.9477)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.90      0.91       154\n",
      "           1       0.99      0.93      0.96       703\n",
      "           2       0.90      0.92      0.91       702\n",
      "           3       0.92      0.95      0.93       703\n",
      "           4       0.99      1.00      0.99       702\n",
      "\n",
      "    accuracy                           0.95      2964\n",
      "   macro avg       0.94      0.94      0.94      2964\n",
      "weighted avg       0.95      0.95      0.95      2964\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 504x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 504x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "gnn = GNN2L_SAGE(data_with_nedbit).to(device)\n",
    "pred = train(gnn, data_with_nedbit.to(device), 40000, cm_title='GAT7L_multiclass_16HC', classes=['P', 'LP', 'WN', 'LN', 'RN'], weight_decay=0.0005, arch='SAGE', layers=2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e84776d73dc40d79f81edb7ff30ca81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40001 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch: 0000] train loss: 1155.6880, train acc: 0.2380, val loss: 622.2990, val acc: 0.2371  (best train acc: 0.2380, best val acc: 0.2371)\n",
      "[Epoch: 0020] train loss: 200.4646, train acc: 0.2274, val loss: 82.8014, val acc: 0.1481  (best train acc: 0.2412, best val acc: 0.2513)\n",
      "[Epoch: 0040] train loss: 63.6169, train acc: 0.2865, val loss: 22.5064, val acc: 0.2691  (best train acc: 0.3133, best val acc: 0.3069)\n",
      "[Epoch: 0060] train loss: 6.3391, train acc: 0.3372, val loss: 3.5343, val acc: 0.3551  (best train acc: 0.3372, best val acc: 0.3551)\n",
      "[Epoch: 0080] train loss: 1.6867, train acc: 0.2350, val loss: 1.7271, val acc: 0.2334  (best train acc: 0.3989, best val acc: 0.4283)\n",
      "[Epoch: 0100] train loss: 1.6161, train acc: 0.2366, val loss: 1.6474, val acc: 0.2351  (best train acc: 0.3989, best val acc: 0.4283)\n",
      "[Epoch: 0120] train loss: 1.5931, train acc: 0.2369, val loss: 1.6063, val acc: 0.2351  (best train acc: 0.3989, best val acc: 0.4283)\n",
      "[Epoch: 0140] train loss: 1.5863, train acc: 0.2368, val loss: 1.5778, val acc: 0.2358  (best train acc: 0.3989, best val acc: 0.4283)\n",
      "[Epoch: 0160] train loss: 1.5780, train acc: 0.2368, val loss: 1.5646, val acc: 0.2368  (best train acc: 0.3989, best val acc: 0.4283)\n",
      "[Epoch: 0180] train loss: 1.5725, train acc: 0.2371, val loss: 1.5581, val acc: 0.2371  (best train acc: 0.3989, best val acc: 0.4283)\n",
      "[Epoch: 0200] train loss: 1.5668, train acc: 0.2375, val loss: 1.5530, val acc: 0.2368  (best train acc: 0.3989, best val acc: 0.4283)\n",
      "[Epoch: 0220] train loss: 1.5664, train acc: 0.2369, val loss: 1.5489, val acc: 0.2371  (best train acc: 0.3989, best val acc: 0.4283)\n",
      "[Epoch: 0240] train loss: 1.5626, train acc: 0.2368, val loss: 1.5462, val acc: 0.2368  (best train acc: 0.3989, best val acc: 0.4283)\n",
      "[Epoch: 0260] train loss: 1.5605, train acc: 0.2368, val loss: 1.5439, val acc: 0.2368  (best train acc: 0.3989, best val acc: 0.4283)\n",
      "[Epoch: 0280] train loss: 1.5541, train acc: 0.2454, val loss: 1.5418, val acc: 0.2543  (best train acc: 0.3989, best val acc: 0.4283)\n",
      "[Epoch: 0300] train loss: 1.5533, train acc: 0.2440, val loss: 1.5394, val acc: 0.2543  (best train acc: 0.3989, best val acc: 0.4283)\n",
      "[Epoch: 0320] train loss: 1.5496, train acc: 0.2451, val loss: 1.5373, val acc: 0.2543  (best train acc: 0.3989, best val acc: 0.4283)\n",
      "[Epoch: 0340] train loss: 1.5466, train acc: 0.2449, val loss: 1.5354, val acc: 0.2543  (best train acc: 0.3989, best val acc: 0.4283)\n",
      "[Epoch: 0360] train loss: 1.5453, train acc: 0.2454, val loss: 1.5336, val acc: 0.2540  (best train acc: 0.3989, best val acc: 0.4283)\n",
      "[Epoch: 0380] train loss: 1.5427, train acc: 0.2449, val loss: 1.5318, val acc: 0.2540  (best train acc: 0.3989, best val acc: 0.4283)\n",
      "[Epoch: 0400] train loss: 1.5424, train acc: 0.2443, val loss: 1.5300, val acc: 0.2540  (best train acc: 0.3989, best val acc: 0.4283)\n",
      "[Epoch: 0420] train loss: 1.5387, train acc: 0.2452, val loss: 1.5285, val acc: 0.2540  (best train acc: 0.3989, best val acc: 0.4283)\n",
      "[Epoch: 0440] train loss: 1.5374, train acc: 0.2448, val loss: 1.5271, val acc: 0.2536  (best train acc: 0.3989, best val acc: 0.4283)\n",
      "[Epoch: 0460] train loss: 1.5354, train acc: 0.2453, val loss: 1.5254, val acc: 0.2536  (best train acc: 0.3989, best val acc: 0.4283)\n",
      "[Epoch: 0480] train loss: 1.5351, train acc: 0.2444, val loss: 1.5241, val acc: 0.2536  (best train acc: 0.3989, best val acc: 0.4283)\n",
      "[Epoch: 0500] train loss: 1.5335, train acc: 0.2446, val loss: 1.5231, val acc: 0.2536  (best train acc: 0.3989, best val acc: 0.4283)\n",
      "[Epoch: 0520] train loss: 1.5319, train acc: 0.2447, val loss: 1.5219, val acc: 0.2536  (best train acc: 0.3989, best val acc: 0.4283)\n",
      "[Epoch: 0540] train loss: 1.5304, train acc: 0.2452, val loss: 1.5204, val acc: 0.2536  (best train acc: 0.3989, best val acc: 0.4283)\n",
      "[Epoch: 0560] train loss: 1.5291, train acc: 0.2449, val loss: 1.5195, val acc: 0.2536  (best train acc: 0.3989, best val acc: 0.4283)\n",
      "[Epoch: 0580] train loss: 1.5301, train acc: 0.2434, val loss: 1.5183, val acc: 0.2540  (best train acc: 0.3989, best val acc: 0.4283)\n",
      "[Epoch: 0600] train loss: 1.5278, train acc: 0.2446, val loss: 1.5174, val acc: 0.2536  (best train acc: 0.3989, best val acc: 0.4283)\n",
      "[Epoch: 0620] train loss: 1.5267, train acc: 0.2447, val loss: 1.5162, val acc: 0.2536  (best train acc: 0.3989, best val acc: 0.4283)\n",
      "[Epoch: 0640] train loss: 1.5258, train acc: 0.2439, val loss: 1.5150, val acc: 0.2533  (best train acc: 0.3989, best val acc: 0.4283)\n",
      "[Epoch: 0660] train loss: 1.5251, train acc: 0.2438, val loss: 1.5141, val acc: 0.2533  (best train acc: 0.3989, best val acc: 0.4283)\n",
      "[Epoch: 0680] train loss: 1.5250, train acc: 0.2431, val loss: 1.5132, val acc: 0.2533  (best train acc: 0.3989, best val acc: 0.4283)\n",
      "[Epoch: 0700] train loss: 1.5237, train acc: 0.2434, val loss: 1.5123, val acc: 0.2533  (best train acc: 0.3989, best val acc: 0.4283)\n",
      "[Epoch: 0720] train loss: 1.5224, train acc: 0.2439, val loss: 1.5117, val acc: 0.2533  (best train acc: 0.3989, best val acc: 0.4283)\n",
      "[Epoch: 0740] train loss: 1.5212, train acc: 0.2441, val loss: 1.5108, val acc: 0.2533  (best train acc: 0.3989, best val acc: 0.4283)\n",
      "[Epoch: 0760] train loss: 1.5172, train acc: 0.2516, val loss: 1.4842, val acc: 0.2772  (best train acc: 0.3989, best val acc: 0.4283)\n",
      "[Epoch: 0780] train loss: 1.5135, train acc: 0.2532, val loss: 1.4802, val acc: 0.2782  (best train acc: 0.3989, best val acc: 0.4283)\n",
      "[Epoch: 0800] train loss: 1.5104, train acc: 0.2535, val loss: 1.4820, val acc: 0.2779  (best train acc: 0.3989, best val acc: 0.4283)\n",
      "[Epoch: 0820] train loss: 1.5106, train acc: 0.2525, val loss: 1.4804, val acc: 0.2782  (best train acc: 0.3989, best val acc: 0.4283)\n",
      "[Epoch: 0840] train loss: 1.5091, train acc: 0.2525, val loss: 1.4789, val acc: 0.2776  (best train acc: 0.3989, best val acc: 0.4283)\n",
      "[Epoch: 0860] train loss: 1.5078, train acc: 0.2521, val loss: 1.4779, val acc: 0.2782  (best train acc: 0.3989, best val acc: 0.4283)\n",
      "[Epoch: 0880] train loss: 1.5032, train acc: 0.2546, val loss: 1.4780, val acc: 0.2779  (best train acc: 0.3989, best val acc: 0.4283)\n",
      "[Epoch: 0900] train loss: 1.5034, train acc: 0.2538, val loss: 1.4758, val acc: 0.2779  (best train acc: 0.3989, best val acc: 0.4283)\n",
      "[Epoch: 0920] train loss: 1.5030, train acc: 0.2537, val loss: 1.4677, val acc: 0.2833  (best train acc: 0.3989, best val acc: 0.4283)\n",
      "[Epoch: 0940] train loss: 1.4999, train acc: 0.2551, val loss: 1.4680, val acc: 0.2836  (best train acc: 0.3989, best val acc: 0.4283)\n",
      "[Epoch: 0960] train loss: 1.4988, train acc: 0.2551, val loss: 1.4684, val acc: 0.2840  (best train acc: 0.3989, best val acc: 0.4283)\n",
      "[Epoch: 0980] train loss: 1.4990, train acc: 0.2541, val loss: 1.4651, val acc: 0.2843  (best train acc: 0.3989, best val acc: 0.4283)\n",
      "[Epoch: 1000] train loss: 1.4961, train acc: 0.2554, val loss: 1.4636, val acc: 0.2843  (best train acc: 0.3989, best val acc: 0.4283)\n",
      "[Epoch: 1020] train loss: 1.4961, train acc: 0.2553, val loss: 1.4620, val acc: 0.2850  (best train acc: 0.3989, best val acc: 0.4283)\n",
      "[Epoch: 1040] train loss: 1.4923, train acc: 0.2571, val loss: 1.4581, val acc: 0.2850  (best train acc: 0.3989, best val acc: 0.4283)\n",
      "[Epoch: 1060] train loss: 1.4933, train acc: 0.2556, val loss: 1.4567, val acc: 0.2843  (best train acc: 0.3989, best val acc: 0.4283)\n",
      "[Epoch: 1080] train loss: 1.4928, train acc: 0.2548, val loss: 1.4556, val acc: 0.2847  (best train acc: 0.3989, best val acc: 0.4283)\n",
      "[Epoch: 1100] train loss: 1.4937, train acc: 0.2535, val loss: 1.4547, val acc: 0.2850  (best train acc: 0.3989, best val acc: 0.4283)\n",
      "[Epoch: 1120] train loss: 1.4899, train acc: 0.2560, val loss: 1.4503, val acc: 0.2870  (best train acc: 0.3989, best val acc: 0.4283)\n",
      "[Epoch: 1140] train loss: 1.4855, train acc: 0.2585, val loss: 1.4472, val acc: 0.2870  (best train acc: 0.3989, best val acc: 0.4283)\n",
      "[Epoch: 1160] train loss: 1.4879, train acc: 0.2551, val loss: 1.4471, val acc: 0.2874  (best train acc: 0.3989, best val acc: 0.4283)\n",
      "[Epoch: 1180] train loss: 1.4869, train acc: 0.2558, val loss: 1.4452, val acc: 0.2887  (best train acc: 0.3989, best val acc: 0.4283)\n",
      "[Epoch: 1200] train loss: 1.4848, train acc: 0.2561, val loss: 1.4405, val acc: 0.2890  (best train acc: 0.3989, best val acc: 0.4283)\n",
      "[Epoch: 1220] train loss: 1.4785, train acc: 0.2603, val loss: 1.4328, val acc: 0.2890  (best train acc: 0.3989, best val acc: 0.4283)\n",
      "[Epoch: 1240] train loss: 1.4771, train acc: 0.2592, val loss: 1.4310, val acc: 0.2897  (best train acc: 0.3989, best val acc: 0.4283)\n",
      "[Epoch: 1260] train loss: 1.4793, train acc: 0.2579, val loss: 1.4200, val acc: 0.2907  (best train acc: 0.3989, best val acc: 0.4283)\n",
      "[Epoch: 1280] train loss: 1.4680, train acc: 0.2600, val loss: 1.4072, val acc: 0.2904  (best train acc: 0.3989, best val acc: 0.4283)\n",
      "[Epoch: 1300] train loss: 1.3830, train acc: 0.3205, val loss: 1.2253, val acc: 0.4040  (best train acc: 0.3989, best val acc: 0.4283)\n",
      "[Epoch: 1320] train loss: 1.3477, train acc: 0.3483, val loss: 1.2045, val acc: 0.3737  (best train acc: 0.3989, best val acc: 0.4425)\n",
      "[Epoch: 1340] train loss: 1.3440, train acc: 0.3535, val loss: 1.1850, val acc: 0.4391  (best train acc: 0.3989, best val acc: 0.4476)\n",
      "[Epoch: 1360] train loss: 1.3300, train acc: 0.3511, val loss: 1.1700, val acc: 0.4411  (best train acc: 0.3989, best val acc: 0.4476)\n",
      "[Epoch: 1380] train loss: 1.3244, train acc: 0.3529, val loss: 1.1655, val acc: 0.4637  (best train acc: 0.3989, best val acc: 0.4637)\n",
      "[Epoch: 1400] train loss: 1.3259, train acc: 0.3683, val loss: 1.1640, val acc: 0.4651  (best train acc: 0.3989, best val acc: 0.4658)\n",
      "[Epoch: 1420] train loss: 1.3319, train acc: 0.3641, val loss: 1.1629, val acc: 0.4668  (best train acc: 0.3989, best val acc: 0.4668)\n",
      "[Epoch: 1440] train loss: 1.3181, train acc: 0.3685, val loss: 1.1585, val acc: 0.4648  (best train acc: 0.3989, best val acc: 0.4668)\n",
      "[Epoch: 1460] train loss: 1.3190, train acc: 0.3685, val loss: 1.1584, val acc: 0.4678  (best train acc: 0.3989, best val acc: 0.4685)\n",
      "[Epoch: 1480] train loss: 1.3192, train acc: 0.3689, val loss: 1.1604, val acc: 0.4671  (best train acc: 0.3989, best val acc: 0.4698)\n",
      "[Epoch: 1500] train loss: 1.3166, train acc: 0.3669, val loss: 1.1572, val acc: 0.4685  (best train acc: 0.3989, best val acc: 0.4698)\n",
      "[Epoch: 1520] train loss: 1.3082, train acc: 0.3681, val loss: 1.1556, val acc: 0.4678  (best train acc: 0.3989, best val acc: 0.4698)\n",
      "[Epoch: 1540] train loss: 1.3093, train acc: 0.3697, val loss: 1.1544, val acc: 0.4681  (best train acc: 0.3989, best val acc: 0.4698)\n",
      "[Epoch: 1560] train loss: 1.3161, train acc: 0.3697, val loss: 1.1536, val acc: 0.4698  (best train acc: 0.3989, best val acc: 0.4712)\n",
      "[Epoch: 1580] train loss: 1.3124, train acc: 0.3671, val loss: 1.1541, val acc: 0.4685  (best train acc: 0.3989, best val acc: 0.4712)\n",
      "[Epoch: 1600] train loss: 1.3135, train acc: 0.3642, val loss: 1.1494, val acc: 0.4698  (best train acc: 0.3989, best val acc: 0.4712)\n",
      "[Epoch: 1620] train loss: 1.3064, train acc: 0.3686, val loss: 1.1510, val acc: 0.4671  (best train acc: 0.3989, best val acc: 0.4712)\n",
      "[Epoch: 1640] train loss: 1.3103, train acc: 0.3668, val loss: 1.1461, val acc: 0.4654  (best train acc: 0.3989, best val acc: 0.4712)\n",
      "[Epoch: 1660] train loss: 1.3113, train acc: 0.3660, val loss: 1.1535, val acc: 0.4671  (best train acc: 0.3989, best val acc: 0.4712)\n",
      "[Epoch: 1680] train loss: 1.3081, train acc: 0.3695, val loss: 1.1457, val acc: 0.4695  (best train acc: 0.3989, best val acc: 0.4712)\n",
      "[Epoch: 1700] train loss: 1.3070, train acc: 0.3678, val loss: 1.1457, val acc: 0.4685  (best train acc: 0.3989, best val acc: 0.4712)\n",
      "[Epoch: 1720] train loss: 1.3016, train acc: 0.3710, val loss: 1.1466, val acc: 0.4540  (best train acc: 0.3989, best val acc: 0.4712)\n",
      "[Epoch: 1740] train loss: 1.3127, train acc: 0.3623, val loss: 1.1477, val acc: 0.4631  (best train acc: 0.3989, best val acc: 0.4722)\n",
      "[Epoch: 1760] train loss: 1.3091, train acc: 0.3685, val loss: 1.1452, val acc: 0.4705  (best train acc: 0.3989, best val acc: 0.4722)\n",
      "[Epoch: 1780] train loss: 1.3076, train acc: 0.3658, val loss: 1.1434, val acc: 0.4685  (best train acc: 0.3989, best val acc: 0.4722)\n",
      "[Epoch: 1800] train loss: 1.3056, train acc: 0.3696, val loss: 1.1424, val acc: 0.4695  (best train acc: 0.3989, best val acc: 0.4722)\n",
      "[Epoch: 1820] train loss: 1.3066, train acc: 0.3632, val loss: 1.1407, val acc: 0.4668  (best train acc: 0.3989, best val acc: 0.4722)\n",
      "[Epoch: 1840] train loss: 1.3046, train acc: 0.3669, val loss: 1.1434, val acc: 0.4688  (best train acc: 0.3989, best val acc: 0.4722)\n",
      "[Epoch: 1860] train loss: 1.3047, train acc: 0.3689, val loss: 1.1462, val acc: 0.4668  (best train acc: 0.3989, best val acc: 0.4722)\n",
      "[Epoch: 1880] train loss: 1.2965, train acc: 0.3712, val loss: 1.1409, val acc: 0.4705  (best train acc: 0.3989, best val acc: 0.4752)\n",
      "[Epoch: 1900] train loss: 1.2961, train acc: 0.3724, val loss: 1.1375, val acc: 0.4712  (best train acc: 0.3989, best val acc: 0.4752)\n",
      "[Epoch: 1920] train loss: 1.2953, train acc: 0.3707, val loss: 1.1433, val acc: 0.4695  (best train acc: 0.3989, best val acc: 0.4752)\n",
      "[Epoch: 1940] train loss: 1.3034, train acc: 0.3668, val loss: 1.1416, val acc: 0.4681  (best train acc: 0.3989, best val acc: 0.4752)\n",
      "[Epoch: 1960] train loss: 1.3005, train acc: 0.3663, val loss: 1.1387, val acc: 0.4691  (best train acc: 0.3989, best val acc: 0.4752)\n",
      "[Epoch: 1980] train loss: 1.3011, train acc: 0.3707, val loss: 1.1397, val acc: 0.4698  (best train acc: 0.3989, best val acc: 0.4752)\n",
      "[Epoch: 2000] train loss: 1.3006, train acc: 0.3635, val loss: 1.1408, val acc: 0.4715  (best train acc: 0.3989, best val acc: 0.4752)\n",
      "[Epoch: 2020] train loss: 1.3107, train acc: 0.3647, val loss: 1.1423, val acc: 0.4702  (best train acc: 0.3989, best val acc: 0.4752)\n",
      "[Epoch: 2040] train loss: 1.2917, train acc: 0.3709, val loss: 1.1395, val acc: 0.4728  (best train acc: 0.3989, best val acc: 0.4752)\n",
      "[Epoch: 2060] train loss: 1.3031, train acc: 0.3635, val loss: 1.1398, val acc: 0.4728  (best train acc: 0.3989, best val acc: 0.4752)\n",
      "[Epoch: 2080] train loss: 1.3006, train acc: 0.3673, val loss: 1.1412, val acc: 0.4715  (best train acc: 0.3989, best val acc: 0.4752)\n",
      "[Epoch: 2100] train loss: 1.2986, train acc: 0.3681, val loss: 1.1386, val acc: 0.4732  (best train acc: 0.3989, best val acc: 0.4752)\n",
      "[Epoch: 2120] train loss: 1.3011, train acc: 0.3694, val loss: 1.1393, val acc: 0.4722  (best train acc: 0.3989, best val acc: 0.4752)\n",
      "[Epoch: 2140] train loss: 1.3041, train acc: 0.3645, val loss: 1.1428, val acc: 0.4728  (best train acc: 0.3989, best val acc: 0.4752)\n",
      "[Epoch: 2160] train loss: 1.3023, train acc: 0.3670, val loss: 1.1420, val acc: 0.4745  (best train acc: 0.3989, best val acc: 0.4759)\n",
      "[Epoch: 2180] train loss: 1.2951, train acc: 0.3672, val loss: 1.1450, val acc: 0.4745  (best train acc: 0.3989, best val acc: 0.4762)\n",
      "[Epoch: 2200] train loss: 1.2978, train acc: 0.3673, val loss: 1.1412, val acc: 0.4769  (best train acc: 0.3989, best val acc: 0.4772)\n",
      "[Epoch: 2220] train loss: 1.3026, train acc: 0.3631, val loss: 1.1410, val acc: 0.4769  (best train acc: 0.3989, best val acc: 0.4779)\n",
      "[Epoch: 2240] train loss: 1.3024, train acc: 0.3697, val loss: 1.1420, val acc: 0.4769  (best train acc: 0.3989, best val acc: 0.4779)\n",
      "[Epoch: 2260] train loss: 1.2964, train acc: 0.3708, val loss: 1.1429, val acc: 0.4769  (best train acc: 0.3989, best val acc: 0.4779)\n",
      "[Epoch: 2280] train loss: 1.2958, train acc: 0.3673, val loss: 1.1426, val acc: 0.4759  (best train acc: 0.3989, best val acc: 0.4782)\n",
      "[Epoch: 2300] train loss: 1.2965, train acc: 0.3672, val loss: 1.1415, val acc: 0.4759  (best train acc: 0.3989, best val acc: 0.4782)\n",
      "[Epoch: 2320] train loss: 1.2929, train acc: 0.3715, val loss: 1.1429, val acc: 0.4762  (best train acc: 0.3989, best val acc: 0.4782)\n",
      "[Epoch: 2340] train loss: 1.2862, train acc: 0.3723, val loss: 1.1416, val acc: 0.4759  (best train acc: 0.3989, best val acc: 0.4786)\n",
      "[Epoch: 2360] train loss: 1.2938, train acc: 0.3725, val loss: 1.1418, val acc: 0.4782  (best train acc: 0.3989, best val acc: 0.4786)\n",
      "[Epoch: 2380] train loss: 1.2903, train acc: 0.3727, val loss: 1.1421, val acc: 0.4749  (best train acc: 0.3989, best val acc: 0.4786)\n",
      "[Epoch: 2400] train loss: 1.2834, train acc: 0.3788, val loss: 1.1419, val acc: 0.4782  (best train acc: 0.3989, best val acc: 0.4786)\n",
      "[Epoch: 2420] train loss: 1.2995, train acc: 0.3658, val loss: 1.1398, val acc: 0.4759  (best train acc: 0.3989, best val acc: 0.4789)\n",
      "[Epoch: 2440] train loss: 1.2900, train acc: 0.3741, val loss: 1.1413, val acc: 0.4745  (best train acc: 0.3989, best val acc: 0.4806)\n",
      "[Epoch: 2460] train loss: 1.2934, train acc: 0.3736, val loss: 1.1403, val acc: 0.4762  (best train acc: 0.3989, best val acc: 0.4806)\n",
      "[Epoch: 2480] train loss: 1.2899, train acc: 0.3733, val loss: 1.1395, val acc: 0.4779  (best train acc: 0.3989, best val acc: 0.4806)\n",
      "[Epoch: 2500] train loss: 1.2914, train acc: 0.3691, val loss: 1.1417, val acc: 0.4755  (best train acc: 0.3989, best val acc: 0.4809)\n",
      "[Epoch: 2520] train loss: 1.2972, train acc: 0.3696, val loss: 1.1392, val acc: 0.4745  (best train acc: 0.3989, best val acc: 0.4809)\n",
      "[Epoch: 2540] train loss: 1.2955, train acc: 0.3700, val loss: 1.1407, val acc: 0.4769  (best train acc: 0.3989, best val acc: 0.4809)\n",
      "[Epoch: 2560] train loss: 1.2934, train acc: 0.3716, val loss: 1.1415, val acc: 0.4786  (best train acc: 0.3989, best val acc: 0.4809)\n",
      "[Epoch: 2580] train loss: 1.2985, train acc: 0.3681, val loss: 1.1396, val acc: 0.4769  (best train acc: 0.3989, best val acc: 0.4809)\n",
      "[Epoch: 2600] train loss: 1.2950, train acc: 0.3679, val loss: 1.1392, val acc: 0.4766  (best train acc: 0.3989, best val acc: 0.4809)\n",
      "[Epoch: 2620] train loss: 1.2969, train acc: 0.3683, val loss: 1.1417, val acc: 0.4739  (best train acc: 0.3989, best val acc: 0.4813)\n",
      "[Epoch: 2640] train loss: 1.2927, train acc: 0.3694, val loss: 1.1401, val acc: 0.4776  (best train acc: 0.3989, best val acc: 0.4813)\n",
      "[Epoch: 2660] train loss: 1.2897, train acc: 0.3739, val loss: 1.1388, val acc: 0.4745  (best train acc: 0.3989, best val acc: 0.4813)\n",
      "[Epoch: 2680] train loss: 1.2931, train acc: 0.3689, val loss: 1.1412, val acc: 0.4782  (best train acc: 0.3989, best val acc: 0.4813)\n",
      "[Epoch: 2700] train loss: 1.2977, train acc: 0.3642, val loss: 1.1408, val acc: 0.4725  (best train acc: 0.3989, best val acc: 0.4813)\n",
      "[Epoch: 2720] train loss: 1.2959, train acc: 0.3655, val loss: 1.1402, val acc: 0.4782  (best train acc: 0.3989, best val acc: 0.4813)\n",
      "[Epoch: 2740] train loss: 1.2923, train acc: 0.3704, val loss: 1.1401, val acc: 0.4793  (best train acc: 0.3989, best val acc: 0.4813)\n",
      "[Epoch: 2760] train loss: 1.2882, train acc: 0.3715, val loss: 1.1384, val acc: 0.4755  (best train acc: 0.3989, best val acc: 0.4813)\n",
      "[Epoch: 2780] train loss: 1.3002, train acc: 0.3663, val loss: 1.1392, val acc: 0.4762  (best train acc: 0.3989, best val acc: 0.4813)\n",
      "[Epoch: 2800] train loss: 1.2859, train acc: 0.3736, val loss: 1.1401, val acc: 0.4766  (best train acc: 0.3989, best val acc: 0.4813)\n",
      "[Epoch: 2820] train loss: 1.2968, train acc: 0.3683, val loss: 1.1399, val acc: 0.4752  (best train acc: 0.3989, best val acc: 0.4813)\n",
      "[Epoch: 2840] train loss: 1.2973, train acc: 0.3688, val loss: 1.1391, val acc: 0.4759  (best train acc: 0.3989, best val acc: 0.4813)\n",
      "[Epoch: 2860] train loss: 1.2911, train acc: 0.3712, val loss: 1.1383, val acc: 0.4766  (best train acc: 0.3989, best val acc: 0.4813)\n",
      "[Epoch: 2880] train loss: 1.2920, train acc: 0.3713, val loss: 1.1391, val acc: 0.4762  (best train acc: 0.3989, best val acc: 0.4813)\n",
      "[Epoch: 2900] train loss: 1.2938, train acc: 0.3702, val loss: 1.1393, val acc: 0.4755  (best train acc: 0.3989, best val acc: 0.4813)\n",
      "[Epoch: 2920] train loss: 1.2909, train acc: 0.3727, val loss: 1.1360, val acc: 0.4779  (best train acc: 0.3989, best val acc: 0.4813)\n",
      "[Epoch: 2940] train loss: 1.2856, train acc: 0.3706, val loss: 1.1356, val acc: 0.4762  (best train acc: 0.3989, best val acc: 0.4813)\n",
      "[Epoch: 2960] train loss: 1.2942, train acc: 0.3651, val loss: 1.1354, val acc: 0.4769  (best train acc: 0.3989, best val acc: 0.4813)\n",
      "[Epoch: 2980] train loss: 1.2855, train acc: 0.3759, val loss: 1.1288, val acc: 0.4782  (best train acc: 0.3989, best val acc: 0.4813)\n",
      "[Epoch: 3000] train loss: 1.2905, train acc: 0.3687, val loss: 1.1400, val acc: 0.4745  (best train acc: 0.3989, best val acc: 0.4813)\n",
      "[Epoch: 3020] train loss: 1.2879, train acc: 0.3738, val loss: 1.1341, val acc: 0.4732  (best train acc: 0.3989, best val acc: 0.4813)\n",
      "[Epoch: 3040] train loss: 1.2776, train acc: 0.3767, val loss: 1.1342, val acc: 0.4725  (best train acc: 0.3989, best val acc: 0.4813)\n",
      "[Epoch: 3060] train loss: 1.2842, train acc: 0.3762, val loss: 1.1335, val acc: 0.4745  (best train acc: 0.3989, best val acc: 0.4813)\n",
      "[Epoch: 3080] train loss: 1.2883, train acc: 0.3702, val loss: 1.1349, val acc: 0.4745  (best train acc: 0.3989, best val acc: 0.4813)\n",
      "[Epoch: 3100] train loss: 1.2902, train acc: 0.3712, val loss: 1.1320, val acc: 0.4715  (best train acc: 0.3989, best val acc: 0.4813)\n",
      "[Epoch: 3120] train loss: 1.2858, train acc: 0.3696, val loss: 1.1351, val acc: 0.4739  (best train acc: 0.3989, best val acc: 0.4813)\n",
      "[Epoch: 3140] train loss: 1.2961, train acc: 0.3667, val loss: 1.1312, val acc: 0.4725  (best train acc: 0.3989, best val acc: 0.4813)\n",
      "[Epoch: 3160] train loss: 1.2836, train acc: 0.3765, val loss: 1.1297, val acc: 0.4725  (best train acc: 0.3989, best val acc: 0.4813)\n",
      "[Epoch: 3180] train loss: 1.2839, train acc: 0.3743, val loss: 1.1379, val acc: 0.4766  (best train acc: 0.3989, best val acc: 0.4813)\n",
      "[Epoch: 3200] train loss: 1.2923, train acc: 0.3686, val loss: 1.1361, val acc: 0.4755  (best train acc: 0.3989, best val acc: 0.4813)\n",
      "[Epoch: 3220] train loss: 1.2873, train acc: 0.3746, val loss: 1.1299, val acc: 0.4725  (best train acc: 0.3989, best val acc: 0.4813)\n",
      "[Epoch: 3240] train loss: 1.2921, train acc: 0.3717, val loss: 1.1339, val acc: 0.4766  (best train acc: 0.3989, best val acc: 0.4813)\n",
      "[Epoch: 3260] train loss: 1.2856, train acc: 0.3707, val loss: 1.1299, val acc: 0.4755  (best train acc: 0.3989, best val acc: 0.4813)\n",
      "[Epoch: 3280] train loss: 1.2926, train acc: 0.3681, val loss: 1.1363, val acc: 0.4722  (best train acc: 0.3989, best val acc: 0.4813)\n",
      "[Epoch: 3300] train loss: 1.2860, train acc: 0.3751, val loss: 1.1313, val acc: 0.4749  (best train acc: 0.3989, best val acc: 0.4813)\n",
      "[Epoch: 3320] train loss: 1.2938, train acc: 0.3730, val loss: 1.1329, val acc: 0.4752  (best train acc: 0.3989, best val acc: 0.4813)\n",
      "[Epoch: 3340] train loss: 1.2933, train acc: 0.3683, val loss: 1.1308, val acc: 0.4715  (best train acc: 0.3989, best val acc: 0.4813)\n",
      "[Epoch: 3360] train loss: 1.2881, train acc: 0.3711, val loss: 1.1336, val acc: 0.4732  (best train acc: 0.3989, best val acc: 0.4813)\n",
      "[Epoch: 3380] train loss: 1.2853, train acc: 0.3716, val loss: 1.1311, val acc: 0.4718  (best train acc: 0.3989, best val acc: 0.4813)\n",
      "[Epoch: 3400] train loss: 1.2888, train acc: 0.3676, val loss: 1.1301, val acc: 0.4725  (best train acc: 0.3989, best val acc: 0.4813)\n",
      "[Epoch: 3420] train loss: 1.2791, train acc: 0.3749, val loss: 1.1336, val acc: 0.4742  (best train acc: 0.3989, best val acc: 0.4813)\n",
      "[Epoch: 3440] train loss: 1.2817, train acc: 0.3749, val loss: 1.1351, val acc: 0.4728  (best train acc: 0.3989, best val acc: 0.4813)\n",
      "[Epoch: 3460] train loss: 1.2845, train acc: 0.3732, val loss: 1.1320, val acc: 0.4718  (best train acc: 0.3989, best val acc: 0.4813)\n",
      "[Epoch: 3480] train loss: 1.2891, train acc: 0.3697, val loss: 1.1320, val acc: 0.4749  (best train acc: 0.3989, best val acc: 0.4813)\n",
      "[Epoch: 3500] train loss: 1.2894, train acc: 0.3701, val loss: 1.1325, val acc: 0.4735  (best train acc: 0.3989, best val acc: 0.4813)\n",
      "[Epoch: 3520] train loss: 1.2870, train acc: 0.3754, val loss: 1.1309, val acc: 0.4732  (best train acc: 0.3989, best val acc: 0.4813)\n",
      "[Epoch: 3540] train loss: 1.2871, train acc: 0.3723, val loss: 1.1303, val acc: 0.4732  (best train acc: 0.3989, best val acc: 0.4813)\n",
      "[Epoch: 3560] train loss: 1.2862, train acc: 0.3691, val loss: 1.1307, val acc: 0.4725  (best train acc: 0.3989, best val acc: 0.4813)\n",
      "[Epoch: 3580] train loss: 1.2881, train acc: 0.3699, val loss: 1.1322, val acc: 0.4712  (best train acc: 0.3989, best val acc: 0.4813)\n",
      "[Epoch: 3600] train loss: 1.2847, train acc: 0.3729, val loss: 1.1342, val acc: 0.4809  (best train acc: 0.3989, best val acc: 0.4813)\n",
      "[Epoch: 3620] train loss: 1.2892, train acc: 0.3728, val loss: 1.1316, val acc: 0.4725  (best train acc: 0.3989, best val acc: 0.4813)\n",
      "[Epoch: 3640] train loss: 1.2863, train acc: 0.3707, val loss: 1.1316, val acc: 0.4725  (best train acc: 0.3989, best val acc: 0.4813)\n",
      "[Epoch: 3660] train loss: 1.2895, train acc: 0.3718, val loss: 1.1323, val acc: 0.4715  (best train acc: 0.3989, best val acc: 0.4813)\n",
      "[Epoch: 3680] train loss: 1.2858, train acc: 0.3705, val loss: 1.1326, val acc: 0.4742  (best train acc: 0.3989, best val acc: 0.4813)\n",
      "[Epoch: 3700] train loss: 1.2912, train acc: 0.3684, val loss: 1.1321, val acc: 0.4725  (best train acc: 0.3989, best val acc: 0.4813)\n",
      "[Epoch: 3720] train loss: 1.2826, train acc: 0.3780, val loss: 1.1321, val acc: 0.4722  (best train acc: 0.3989, best val acc: 0.4813)\n",
      "[Epoch: 3740] train loss: 1.2889, train acc: 0.3691, val loss: 1.1310, val acc: 0.4735  (best train acc: 0.3989, best val acc: 0.4813)\n",
      "[Epoch: 3760] train loss: 1.2824, train acc: 0.3777, val loss: 1.1313, val acc: 0.4759  (best train acc: 0.3989, best val acc: 0.4813)\n",
      "[Epoch: 3780] train loss: 1.2862, train acc: 0.3725, val loss: 1.1326, val acc: 0.4712  (best train acc: 0.3989, best val acc: 0.4813)\n",
      "[Epoch: 3800] train loss: 1.2849, train acc: 0.3749, val loss: 1.1319, val acc: 0.4739  (best train acc: 0.3989, best val acc: 0.4813)\n",
      "[Epoch: 3820] train loss: 1.2856, train acc: 0.3704, val loss: 1.1305, val acc: 0.4725  (best train acc: 0.3989, best val acc: 0.4813)\n",
      "[Epoch: 3840] train loss: 1.2875, train acc: 0.3741, val loss: 1.1314, val acc: 0.4749  (best train acc: 0.3989, best val acc: 0.4813)\n",
      "[Epoch: 3860] train loss: 1.2880, train acc: 0.3708, val loss: 1.1294, val acc: 0.4712  (best train acc: 0.3989, best val acc: 0.4813)\n",
      "[Epoch: 3880] train loss: 1.2909, train acc: 0.3695, val loss: 1.1330, val acc: 0.4759  (best train acc: 0.3989, best val acc: 0.4813)\n",
      "[Epoch: 3900] train loss: 1.2899, train acc: 0.3698, val loss: 1.1288, val acc: 0.4739  (best train acc: 0.3989, best val acc: 0.4813)\n",
      "[Epoch: 3920] train loss: 1.2888, train acc: 0.3725, val loss: 1.1348, val acc: 0.4735  (best train acc: 0.3989, best val acc: 0.4813)\n",
      "[Epoch: 3940] train loss: 1.2889, train acc: 0.3704, val loss: 1.1321, val acc: 0.4739  (best train acc: 0.3989, best val acc: 0.4813)\n",
      "[Epoch: 3960] train loss: 1.2879, train acc: 0.3710, val loss: 1.1315, val acc: 0.4722  (best train acc: 0.3989, best val acc: 0.4813)\n",
      "[Epoch: 3980] train loss: 1.2900, train acc: 0.3690, val loss: 1.1339, val acc: 0.4766  (best train acc: 0.3989, best val acc: 0.4813)\n",
      "[Epoch: 4000] train loss: 1.2796, train acc: 0.3754, val loss: 1.1288, val acc: 0.4735  (best train acc: 0.3989, best val acc: 0.4813)\n",
      "[Epoch: 4020] train loss: 1.2898, train acc: 0.3730, val loss: 1.1313, val acc: 0.4762  (best train acc: 0.3989, best val acc: 0.4813)\n",
      "[Epoch: 4040] train loss: 1.2845, train acc: 0.3713, val loss: 1.1301, val acc: 0.4749  (best train acc: 0.3989, best val acc: 0.4813)\n",
      "[Epoch: 4060] train loss: 1.2841, train acc: 0.3723, val loss: 1.1313, val acc: 0.4766  (best train acc: 0.3989, best val acc: 0.4813)\n",
      "[Epoch: 4080] train loss: 1.2782, train acc: 0.3781, val loss: 1.1328, val acc: 0.4739  (best train acc: 0.3989, best val acc: 0.4813)\n",
      "[Epoch: 4100] train loss: 1.2921, train acc: 0.3683, val loss: 1.1320, val acc: 0.4755  (best train acc: 0.3989, best val acc: 0.4813)\n",
      "[Epoch: 4120] train loss: 1.2871, train acc: 0.3691, val loss: 1.1324, val acc: 0.4728  (best train acc: 0.3989, best val acc: 0.4813)\n",
      "[Epoch: 4140] train loss: 1.2792, train acc: 0.3757, val loss: 1.1315, val acc: 0.4752  (best train acc: 0.3989, best val acc: 0.4813)\n",
      "[Epoch: 4160] train loss: 1.2917, train acc: 0.3693, val loss: 1.1326, val acc: 0.4745  (best train acc: 0.3989, best val acc: 0.4813)\n",
      "[Epoch: 4180] train loss: 1.2872, train acc: 0.3728, val loss: 1.1286, val acc: 0.4732  (best train acc: 0.3989, best val acc: 0.4813)\n",
      "[Epoch: 4200] train loss: 1.2811, train acc: 0.3749, val loss: 1.1305, val acc: 0.4745  (best train acc: 0.3989, best val acc: 0.4813)\n",
      "[Epoch: 4220] train loss: 1.2832, train acc: 0.3773, val loss: 1.1328, val acc: 0.4755  (best train acc: 0.3989, best val acc: 0.4813)\n",
      "[Epoch: 4240] train loss: 1.2921, train acc: 0.3704, val loss: 1.1288, val acc: 0.4789  (best train acc: 0.3989, best val acc: 0.4813)\n",
      "[Epoch: 4260] train loss: 1.2799, train acc: 0.3738, val loss: 1.1312, val acc: 0.4755  (best train acc: 0.3989, best val acc: 0.4813)\n",
      "[Epoch: 4280] train loss: 1.2882, train acc: 0.3749, val loss: 1.1323, val acc: 0.4742  (best train acc: 0.3989, best val acc: 0.4813)\n",
      "[Epoch: 4300] train loss: 1.2809, train acc: 0.3751, val loss: 1.1336, val acc: 0.4759  (best train acc: 0.3989, best val acc: 0.4813)\n",
      "[Epoch: 4320] train loss: 1.2824, train acc: 0.3726, val loss: 1.1314, val acc: 0.4776  (best train acc: 0.3989, best val acc: 0.4813)\n",
      "[Epoch: 4340] train loss: 1.2840, train acc: 0.3736, val loss: 1.1296, val acc: 0.4705  (best train acc: 0.3989, best val acc: 0.4813)\n",
      "[Epoch: 4360] train loss: 1.2892, train acc: 0.3705, val loss: 1.1304, val acc: 0.4782  (best train acc: 0.3989, best val acc: 0.4813)\n",
      "[Epoch: 4380] train loss: 1.2856, train acc: 0.3710, val loss: 1.1308, val acc: 0.4793  (best train acc: 0.3989, best val acc: 0.4813)\n",
      "[Epoch: 4400] train loss: 1.2882, train acc: 0.3730, val loss: 1.1315, val acc: 0.4739  (best train acc: 0.3989, best val acc: 0.4813)\n",
      "[Epoch: 4420] train loss: 1.2886, train acc: 0.3726, val loss: 1.1286, val acc: 0.4789  (best train acc: 0.3989, best val acc: 0.4820)\n",
      "[Epoch: 4440] train loss: 1.2829, train acc: 0.3716, val loss: 1.1326, val acc: 0.4725  (best train acc: 0.3989, best val acc: 0.4820)\n",
      "[Epoch: 4460] train loss: 1.2945, train acc: 0.3695, val loss: 1.1304, val acc: 0.4732  (best train acc: 0.3989, best val acc: 0.4820)\n",
      "[Epoch: 4480] train loss: 1.2774, train acc: 0.3782, val loss: 1.1288, val acc: 0.4755  (best train acc: 0.3989, best val acc: 0.4820)\n",
      "[Epoch: 4500] train loss: 1.2871, train acc: 0.3699, val loss: 1.1297, val acc: 0.4793  (best train acc: 0.3989, best val acc: 0.4820)\n",
      "[Epoch: 4520] train loss: 1.2822, train acc: 0.3720, val loss: 1.1310, val acc: 0.4776  (best train acc: 0.3989, best val acc: 0.4820)\n",
      "[Epoch: 4540] train loss: 1.2882, train acc: 0.3700, val loss: 1.1294, val acc: 0.4793  (best train acc: 0.3989, best val acc: 0.4820)\n",
      "[Epoch: 4560] train loss: 1.2900, train acc: 0.3725, val loss: 1.1329, val acc: 0.4786  (best train acc: 0.3989, best val acc: 0.4820)\n",
      "[Epoch: 4580] train loss: 1.2797, train acc: 0.3754, val loss: 1.1305, val acc: 0.4755  (best train acc: 0.3989, best val acc: 0.4820)\n",
      "[Epoch: 4600] train loss: 1.2852, train acc: 0.3727, val loss: 1.1303, val acc: 0.4718  (best train acc: 0.3989, best val acc: 0.4820)\n",
      "[Epoch: 4620] train loss: 1.2872, train acc: 0.3701, val loss: 1.1316, val acc: 0.4759  (best train acc: 0.3989, best val acc: 0.4820)\n",
      "[Epoch: 4640] train loss: 1.2905, train acc: 0.3729, val loss: 1.1315, val acc: 0.4813  (best train acc: 0.3989, best val acc: 0.4820)\n",
      "[Epoch: 4660] train loss: 1.2913, train acc: 0.3704, val loss: 1.1303, val acc: 0.4745  (best train acc: 0.3989, best val acc: 0.4820)\n",
      "[Epoch: 4680] train loss: 1.2829, train acc: 0.3723, val loss: 1.1293, val acc: 0.4786  (best train acc: 0.3989, best val acc: 0.4820)\n",
      "[Epoch: 4700] train loss: 1.2894, train acc: 0.3694, val loss: 1.1309, val acc: 0.4769  (best train acc: 0.3989, best val acc: 0.4820)\n",
      "[Epoch: 4720] train loss: 1.2829, train acc: 0.3732, val loss: 1.1322, val acc: 0.4752  (best train acc: 0.3989, best val acc: 0.4820)\n",
      "[Epoch: 4740] train loss: 1.2828, train acc: 0.3726, val loss: 1.1330, val acc: 0.4755  (best train acc: 0.3989, best val acc: 0.4820)\n",
      "[Epoch: 4760] train loss: 1.2861, train acc: 0.3764, val loss: 1.1284, val acc: 0.4803  (best train acc: 0.3989, best val acc: 0.4820)\n",
      "[Epoch: 4780] train loss: 1.2847, train acc: 0.3720, val loss: 1.1278, val acc: 0.4766  (best train acc: 0.3989, best val acc: 0.4820)\n",
      "[Epoch: 4800] train loss: 1.2892, train acc: 0.3688, val loss: 1.1312, val acc: 0.4735  (best train acc: 0.3989, best val acc: 0.4840)\n",
      "[Epoch: 4820] train loss: 1.2848, train acc: 0.3772, val loss: 1.1302, val acc: 0.4735  (best train acc: 0.3989, best val acc: 0.4840)\n",
      "[Epoch: 4840] train loss: 1.2789, train acc: 0.3747, val loss: 1.1283, val acc: 0.4766  (best train acc: 0.3989, best val acc: 0.4840)\n",
      "[Epoch: 4860] train loss: 1.2949, train acc: 0.3665, val loss: 1.1315, val acc: 0.4766  (best train acc: 0.3989, best val acc: 0.4840)\n",
      "[Epoch: 4880] train loss: 1.2817, train acc: 0.3740, val loss: 1.1314, val acc: 0.4786  (best train acc: 0.3989, best val acc: 0.4840)\n",
      "[Epoch: 4900] train loss: 1.2856, train acc: 0.3710, val loss: 1.1313, val acc: 0.4752  (best train acc: 0.3989, best val acc: 0.4840)\n",
      "[Epoch: 4920] train loss: 1.2801, train acc: 0.3746, val loss: 1.1307, val acc: 0.4779  (best train acc: 0.3989, best val acc: 0.4840)\n",
      "[Epoch: 4940] train loss: 1.2840, train acc: 0.3720, val loss: 1.1299, val acc: 0.4735  (best train acc: 0.3989, best val acc: 0.4840)\n",
      "[Epoch: 4960] train loss: 1.2817, train acc: 0.3714, val loss: 1.1288, val acc: 0.4766  (best train acc: 0.3989, best val acc: 0.4840)\n",
      "[Epoch: 4980] train loss: 1.2836, train acc: 0.3736, val loss: 1.1310, val acc: 0.4830  (best train acc: 0.3989, best val acc: 0.4840)\n",
      "[Epoch: 5000] train loss: 1.2827, train acc: 0.3719, val loss: 1.1291, val acc: 0.4796  (best train acc: 0.3989, best val acc: 0.4840)\n",
      "[Epoch: 5020] train loss: 1.2828, train acc: 0.3751, val loss: 1.1282, val acc: 0.4796  (best train acc: 0.3989, best val acc: 0.4840)\n",
      "[Epoch: 5040] train loss: 1.2890, train acc: 0.3706, val loss: 1.1303, val acc: 0.4766  (best train acc: 0.3989, best val acc: 0.4840)\n",
      "[Epoch: 5060] train loss: 1.2879, train acc: 0.3704, val loss: 1.1285, val acc: 0.4749  (best train acc: 0.3989, best val acc: 0.4840)\n",
      "[Epoch: 5080] train loss: 1.2875, train acc: 0.3719, val loss: 1.1296, val acc: 0.4759  (best train acc: 0.3989, best val acc: 0.4840)\n",
      "[Epoch: 5100] train loss: 1.2925, train acc: 0.3696, val loss: 1.1312, val acc: 0.4728  (best train acc: 0.3989, best val acc: 0.4840)\n",
      "[Epoch: 5120] train loss: 1.2754, train acc: 0.3823, val loss: 1.1305, val acc: 0.4793  (best train acc: 0.3989, best val acc: 0.4840)\n",
      "[Epoch: 5140] train loss: 1.2875, train acc: 0.3675, val loss: 1.1305, val acc: 0.4742  (best train acc: 0.3989, best val acc: 0.4840)\n",
      "[Epoch: 5160] train loss: 1.2877, train acc: 0.3679, val loss: 1.1319, val acc: 0.4752  (best train acc: 0.3989, best val acc: 0.4840)\n",
      "[Epoch: 5180] train loss: 1.2822, train acc: 0.3741, val loss: 1.1302, val acc: 0.4755  (best train acc: 0.3989, best val acc: 0.4840)\n",
      "[Epoch: 5200] train loss: 1.2810, train acc: 0.3749, val loss: 1.1286, val acc: 0.4766  (best train acc: 0.3989, best val acc: 0.4840)\n",
      "[Epoch: 5220] train loss: 1.2826, train acc: 0.3745, val loss: 1.1288, val acc: 0.4809  (best train acc: 0.3989, best val acc: 0.4840)\n",
      "[Epoch: 5240] train loss: 1.2840, train acc: 0.3723, val loss: 1.1302, val acc: 0.4782  (best train acc: 0.3989, best val acc: 0.4840)\n",
      "[Epoch: 5260] train loss: 1.2848, train acc: 0.3721, val loss: 1.1324, val acc: 0.4796  (best train acc: 0.3989, best val acc: 0.4840)\n",
      "[Epoch: 5280] train loss: 1.2839, train acc: 0.3717, val loss: 1.1294, val acc: 0.4799  (best train acc: 0.3989, best val acc: 0.4840)\n",
      "[Epoch: 5300] train loss: 1.2842, train acc: 0.3723, val loss: 1.1293, val acc: 0.4752  (best train acc: 0.3989, best val acc: 0.4840)\n",
      "[Epoch: 5320] train loss: 1.2815, train acc: 0.3741, val loss: 1.1273, val acc: 0.4755  (best train acc: 0.3989, best val acc: 0.4840)\n",
      "[Epoch: 5340] train loss: 1.2828, train acc: 0.3757, val loss: 1.1279, val acc: 0.4766  (best train acc: 0.3989, best val acc: 0.4840)\n",
      "[Epoch: 5360] train loss: 1.2860, train acc: 0.3722, val loss: 1.1297, val acc: 0.4739  (best train acc: 0.3989, best val acc: 0.4840)\n",
      "[Epoch: 5380] train loss: 1.2818, train acc: 0.3735, val loss: 1.1302, val acc: 0.4843  (best train acc: 0.3989, best val acc: 0.4853)\n",
      "[Epoch: 5400] train loss: 1.2838, train acc: 0.3709, val loss: 1.1310, val acc: 0.4735  (best train acc: 0.3989, best val acc: 0.4853)\n",
      "[Epoch: 5420] train loss: 1.2837, train acc: 0.3747, val loss: 1.1279, val acc: 0.4745  (best train acc: 0.3989, best val acc: 0.4853)\n",
      "[Epoch: 5440] train loss: 1.2859, train acc: 0.3702, val loss: 1.1284, val acc: 0.4739  (best train acc: 0.3989, best val acc: 0.4853)\n",
      "[Epoch: 5460] train loss: 1.2940, train acc: 0.3690, val loss: 1.1298, val acc: 0.4739  (best train acc: 0.3989, best val acc: 0.4853)\n",
      "[Epoch: 5480] train loss: 1.2809, train acc: 0.3783, val loss: 1.1328, val acc: 0.4779  (best train acc: 0.3989, best val acc: 0.4853)\n",
      "[Epoch: 5500] train loss: 1.2863, train acc: 0.3705, val loss: 1.1297, val acc: 0.4772  (best train acc: 0.3989, best val acc: 0.4853)\n",
      "[Epoch: 5520] train loss: 1.2805, train acc: 0.3744, val loss: 1.1277, val acc: 0.4779  (best train acc: 0.3989, best val acc: 0.4853)\n",
      "[Epoch: 5540] train loss: 1.2759, train acc: 0.3774, val loss: 1.1264, val acc: 0.4766  (best train acc: 0.3989, best val acc: 0.4853)\n",
      "[Epoch: 5560] train loss: 1.2797, train acc: 0.3759, val loss: 1.1310, val acc: 0.4752  (best train acc: 0.3989, best val acc: 0.4853)\n",
      "[Epoch: 5580] train loss: 1.2869, train acc: 0.3704, val loss: 1.1317, val acc: 0.4749  (best train acc: 0.3989, best val acc: 0.4853)\n",
      "[Epoch: 5600] train loss: 1.2882, train acc: 0.3704, val loss: 1.1313, val acc: 0.4766  (best train acc: 0.3989, best val acc: 0.4853)\n",
      "[Epoch: 5620] train loss: 1.2904, train acc: 0.3681, val loss: 1.1315, val acc: 0.4796  (best train acc: 0.3989, best val acc: 0.4853)\n",
      "[Epoch: 5640] train loss: 1.2858, train acc: 0.3733, val loss: 1.1284, val acc: 0.4772  (best train acc: 0.3989, best val acc: 0.4853)\n",
      "[Epoch: 5660] train loss: 1.2934, train acc: 0.3667, val loss: 1.1285, val acc: 0.4749  (best train acc: 0.3989, best val acc: 0.4853)\n",
      "[Epoch: 5680] train loss: 1.2864, train acc: 0.3718, val loss: 1.1275, val acc: 0.4772  (best train acc: 0.3989, best val acc: 0.4853)\n",
      "[Epoch: 5700] train loss: 1.2910, train acc: 0.3686, val loss: 1.1287, val acc: 0.4782  (best train acc: 0.3989, best val acc: 0.4853)\n",
      "[Epoch: 5720] train loss: 1.2825, train acc: 0.3746, val loss: 1.1269, val acc: 0.4796  (best train acc: 0.3989, best val acc: 0.4853)\n",
      "[Epoch: 5740] train loss: 1.2855, train acc: 0.3712, val loss: 1.1288, val acc: 0.4776  (best train acc: 0.3989, best val acc: 0.4853)\n",
      "[Epoch: 5760] train loss: 1.2826, train acc: 0.3746, val loss: 1.1293, val acc: 0.4776  (best train acc: 0.3989, best val acc: 0.4853)\n",
      "[Epoch: 5780] train loss: 1.2893, train acc: 0.3708, val loss: 1.1300, val acc: 0.4809  (best train acc: 0.3989, best val acc: 0.4853)\n",
      "[Epoch: 5800] train loss: 1.2733, train acc: 0.3814, val loss: 1.1295, val acc: 0.4772  (best train acc: 0.3989, best val acc: 0.4853)\n",
      "[Epoch: 5820] train loss: 1.2853, train acc: 0.3717, val loss: 1.1288, val acc: 0.4769  (best train acc: 0.3989, best val acc: 0.4853)\n",
      "[Epoch: 5840] train loss: 1.2825, train acc: 0.3739, val loss: 1.1254, val acc: 0.4759  (best train acc: 0.3989, best val acc: 0.4853)\n",
      "[Epoch: 5860] train loss: 1.2821, train acc: 0.3725, val loss: 1.1274, val acc: 0.4772  (best train acc: 0.3989, best val acc: 0.4853)\n",
      "[Epoch: 5880] train loss: 1.2921, train acc: 0.3683, val loss: 1.1285, val acc: 0.4830  (best train acc: 0.3989, best val acc: 0.4853)\n",
      "[Epoch: 5900] train loss: 1.2767, train acc: 0.3746, val loss: 1.1275, val acc: 0.4759  (best train acc: 0.3989, best val acc: 0.4853)\n",
      "[Epoch: 5920] train loss: 1.2779, train acc: 0.3737, val loss: 1.1262, val acc: 0.4786  (best train acc: 0.3989, best val acc: 0.4853)\n",
      "[Epoch: 5940] train loss: 1.2835, train acc: 0.3728, val loss: 1.1290, val acc: 0.4752  (best train acc: 0.3989, best val acc: 0.4853)\n",
      "[Epoch: 5960] train loss: 1.2892, train acc: 0.3713, val loss: 1.1299, val acc: 0.4759  (best train acc: 0.3989, best val acc: 0.4853)\n",
      "[Epoch: 5980] train loss: 1.2792, train acc: 0.3733, val loss: 1.1271, val acc: 0.4755  (best train acc: 0.3989, best val acc: 0.4853)\n",
      "[Epoch: 6000] train loss: 1.2923, train acc: 0.3683, val loss: 1.1288, val acc: 0.4755  (best train acc: 0.3989, best val acc: 0.4853)\n",
      "[Epoch: 6020] train loss: 1.2827, train acc: 0.3709, val loss: 1.1266, val acc: 0.4779  (best train acc: 0.3989, best val acc: 0.4853)\n",
      "[Epoch: 6040] train loss: 1.2852, train acc: 0.3723, val loss: 1.1311, val acc: 0.4762  (best train acc: 0.3989, best val acc: 0.4853)\n",
      "[Epoch: 6060] train loss: 1.2852, train acc: 0.3736, val loss: 1.1303, val acc: 0.4752  (best train acc: 0.3989, best val acc: 0.4853)\n",
      "[Epoch: 6080] train loss: 1.2853, train acc: 0.3742, val loss: 1.1280, val acc: 0.4796  (best train acc: 0.3989, best val acc: 0.4853)\n",
      "[Epoch: 6100] train loss: 1.2816, train acc: 0.3708, val loss: 1.1258, val acc: 0.4772  (best train acc: 0.3989, best val acc: 0.4853)\n",
      "[Epoch: 6120] train loss: 1.2802, train acc: 0.3759, val loss: 1.1301, val acc: 0.4745  (best train acc: 0.3989, best val acc: 0.4853)\n",
      "[Epoch: 6140] train loss: 1.2816, train acc: 0.3703, val loss: 1.1292, val acc: 0.4749  (best train acc: 0.3989, best val acc: 0.4853)\n",
      "[Epoch: 6160] train loss: 1.2788, train acc: 0.3747, val loss: 1.1277, val acc: 0.4826  (best train acc: 0.3989, best val acc: 0.4853)\n",
      "[Epoch: 6180] train loss: 1.2821, train acc: 0.3749, val loss: 1.1259, val acc: 0.4813  (best train acc: 0.3989, best val acc: 0.4853)\n",
      "[Epoch: 6200] train loss: 1.2849, train acc: 0.3741, val loss: 1.1313, val acc: 0.4755  (best train acc: 0.3989, best val acc: 0.4853)\n",
      "[Epoch: 6220] train loss: 1.2791, train acc: 0.3757, val loss: 1.1263, val acc: 0.4809  (best train acc: 0.3989, best val acc: 0.4853)\n",
      "[Epoch: 6240] train loss: 1.2855, train acc: 0.3712, val loss: 1.1285, val acc: 0.4755  (best train acc: 0.3989, best val acc: 0.4853)\n",
      "[Epoch: 6260] train loss: 1.2847, train acc: 0.3709, val loss: 1.1268, val acc: 0.4823  (best train acc: 0.3989, best val acc: 0.4853)\n",
      "[Epoch: 6280] train loss: 1.2880, train acc: 0.3711, val loss: 1.1259, val acc: 0.4820  (best train acc: 0.3989, best val acc: 0.4853)\n",
      "[Epoch: 6300] train loss: 1.2860, train acc: 0.3702, val loss: 1.1298, val acc: 0.4806  (best train acc: 0.3989, best val acc: 0.4853)\n",
      "[Epoch: 6320] train loss: 1.2905, train acc: 0.3686, val loss: 1.1279, val acc: 0.4813  (best train acc: 0.3989, best val acc: 0.4853)\n",
      "[Epoch: 6340] train loss: 1.2857, train acc: 0.3750, val loss: 1.1275, val acc: 0.4776  (best train acc: 0.3989, best val acc: 0.4853)\n",
      "[Epoch: 6360] train loss: 1.2768, train acc: 0.3760, val loss: 1.1280, val acc: 0.4796  (best train acc: 0.3989, best val acc: 0.4874)\n",
      "[Epoch: 6380] train loss: 1.2846, train acc: 0.3744, val loss: 1.1299, val acc: 0.4772  (best train acc: 0.3989, best val acc: 0.4874)\n",
      "[Epoch: 6400] train loss: 1.2852, train acc: 0.3712, val loss: 1.1311, val acc: 0.4749  (best train acc: 0.3989, best val acc: 0.4874)\n",
      "[Epoch: 6420] train loss: 1.2805, train acc: 0.3757, val loss: 1.1273, val acc: 0.4745  (best train acc: 0.3989, best val acc: 0.4874)\n",
      "[Epoch: 6440] train loss: 1.2815, train acc: 0.3728, val loss: 1.1296, val acc: 0.4759  (best train acc: 0.3989, best val acc: 0.4874)\n",
      "[Epoch: 6460] train loss: 1.2806, train acc: 0.3721, val loss: 1.1295, val acc: 0.4752  (best train acc: 0.3989, best val acc: 0.4874)\n",
      "[Epoch: 6480] train loss: 1.2779, train acc: 0.3764, val loss: 1.1280, val acc: 0.4782  (best train acc: 0.3989, best val acc: 0.4874)\n",
      "[Epoch: 6500] train loss: 1.2895, train acc: 0.3697, val loss: 1.1270, val acc: 0.4755  (best train acc: 0.3989, best val acc: 0.4874)\n",
      "[Epoch: 6520] train loss: 1.2819, train acc: 0.3701, val loss: 1.1254, val acc: 0.4759  (best train acc: 0.3989, best val acc: 0.4874)\n",
      "[Epoch: 6540] train loss: 1.2873, train acc: 0.3684, val loss: 1.1253, val acc: 0.4806  (best train acc: 0.3989, best val acc: 0.4874)\n",
      "[Epoch: 6560] train loss: 1.2845, train acc: 0.3737, val loss: 1.1238, val acc: 0.4803  (best train acc: 0.3989, best val acc: 0.4874)\n",
      "[Epoch: 6580] train loss: 1.2845, train acc: 0.3731, val loss: 1.1299, val acc: 0.4759  (best train acc: 0.3989, best val acc: 0.4874)\n",
      "[Epoch: 6600] train loss: 1.2866, train acc: 0.3725, val loss: 1.1295, val acc: 0.4766  (best train acc: 0.3989, best val acc: 0.4874)\n",
      "[Epoch: 6620] train loss: 1.2830, train acc: 0.3757, val loss: 1.1318, val acc: 0.4759  (best train acc: 0.3989, best val acc: 0.4874)\n",
      "[Epoch: 6640] train loss: 1.2879, train acc: 0.3665, val loss: 1.1296, val acc: 0.4772  (best train acc: 0.3989, best val acc: 0.4874)\n",
      "[Epoch: 6660] train loss: 1.2760, train acc: 0.3722, val loss: 1.1367, val acc: 0.3906  (best train acc: 0.3989, best val acc: 0.4874)\n",
      "[Epoch: 6680] train loss: 1.2816, train acc: 0.3752, val loss: 1.1367, val acc: 0.4735  (best train acc: 0.3989, best val acc: 0.4884)\n",
      "[Epoch: 6700] train loss: 1.2922, train acc: 0.3644, val loss: 1.1293, val acc: 0.4742  (best train acc: 0.3989, best val acc: 0.4884)\n",
      "[Epoch: 6720] train loss: 1.2772, train acc: 0.3796, val loss: 1.1301, val acc: 0.4766  (best train acc: 0.3989, best val acc: 0.4884)\n",
      "[Epoch: 6740] train loss: 1.2795, train acc: 0.3750, val loss: 1.1318, val acc: 0.4762  (best train acc: 0.3989, best val acc: 0.4884)\n",
      "[Epoch: 6760] train loss: 1.2843, train acc: 0.3720, val loss: 1.1304, val acc: 0.4769  (best train acc: 0.3989, best val acc: 0.4884)\n",
      "[Epoch: 6780] train loss: 1.2762, train acc: 0.3761, val loss: 1.1295, val acc: 0.4752  (best train acc: 0.3989, best val acc: 0.4884)\n",
      "[Epoch: 6800] train loss: 1.2838, train acc: 0.3705, val loss: 1.1312, val acc: 0.4739  (best train acc: 0.3989, best val acc: 0.4884)\n",
      "[Epoch: 6820] train loss: 1.2842, train acc: 0.3715, val loss: 1.1307, val acc: 0.4742  (best train acc: 0.3989, best val acc: 0.4884)\n",
      "[Epoch: 6840] train loss: 1.2912, train acc: 0.3694, val loss: 1.1270, val acc: 0.4762  (best train acc: 0.3989, best val acc: 0.4884)\n",
      "[Epoch: 6860] train loss: 1.2786, train acc: 0.3783, val loss: 1.1275, val acc: 0.4769  (best train acc: 0.3989, best val acc: 0.4884)\n",
      "[Epoch: 6880] train loss: 1.2761, train acc: 0.3766, val loss: 1.1296, val acc: 0.4735  (best train acc: 0.3989, best val acc: 0.4884)\n",
      "[Epoch: 6900] train loss: 1.2849, train acc: 0.3733, val loss: 1.1303, val acc: 0.4728  (best train acc: 0.3989, best val acc: 0.4884)\n",
      "[Epoch: 6920] train loss: 1.2811, train acc: 0.3730, val loss: 1.1282, val acc: 0.4766  (best train acc: 0.3989, best val acc: 0.4884)\n",
      "[Epoch: 6940] train loss: 1.2843, train acc: 0.3753, val loss: 1.1288, val acc: 0.4745  (best train acc: 0.3989, best val acc: 0.4884)\n",
      "[Epoch: 6960] train loss: 1.2830, train acc: 0.3713, val loss: 1.1281, val acc: 0.4725  (best train acc: 0.3989, best val acc: 0.4884)\n",
      "[Epoch: 6980] train loss: 1.2849, train acc: 0.3723, val loss: 1.1293, val acc: 0.4745  (best train acc: 0.3989, best val acc: 0.4884)\n",
      "[Epoch: 7000] train loss: 1.2836, train acc: 0.3738, val loss: 1.1277, val acc: 0.4759  (best train acc: 0.3989, best val acc: 0.4884)\n",
      "[Epoch: 7020] train loss: 1.2854, train acc: 0.3713, val loss: 1.1270, val acc: 0.4826  (best train acc: 0.3989, best val acc: 0.4884)\n",
      "[Epoch: 7040] train loss: 1.2884, train acc: 0.3699, val loss: 1.1280, val acc: 0.4762  (best train acc: 0.3989, best val acc: 0.4884)\n",
      "[Epoch: 7060] train loss: 1.2872, train acc: 0.3696, val loss: 1.1292, val acc: 0.4762  (best train acc: 0.3989, best val acc: 0.4884)\n",
      "[Epoch: 7080] train loss: 1.2847, train acc: 0.3731, val loss: 1.1315, val acc: 0.4752  (best train acc: 0.3989, best val acc: 0.4884)\n",
      "[Epoch: 7100] train loss: 1.2868, train acc: 0.3700, val loss: 1.1276, val acc: 0.4752  (best train acc: 0.3989, best val acc: 0.4884)\n",
      "[Epoch: 7120] train loss: 1.2857, train acc: 0.3705, val loss: 1.1305, val acc: 0.4755  (best train acc: 0.3989, best val acc: 0.4884)\n",
      "[Epoch: 7140] train loss: 1.2804, train acc: 0.3713, val loss: 1.1299, val acc: 0.4755  (best train acc: 0.3989, best val acc: 0.4884)\n",
      "[Epoch: 7160] train loss: 1.2838, train acc: 0.3694, val loss: 1.1299, val acc: 0.4745  (best train acc: 0.3989, best val acc: 0.4884)\n",
      "[Epoch: 7180] train loss: 1.2901, train acc: 0.3645, val loss: 1.1265, val acc: 0.4749  (best train acc: 0.3989, best val acc: 0.4884)\n",
      "[Epoch: 7200] train loss: 1.2876, train acc: 0.3704, val loss: 1.1300, val acc: 0.4762  (best train acc: 0.3989, best val acc: 0.4884)\n",
      "[Epoch: 7220] train loss: 1.2858, train acc: 0.3700, val loss: 1.1260, val acc: 0.4732  (best train acc: 0.3989, best val acc: 0.4884)\n",
      "[Epoch: 7240] train loss: 1.2849, train acc: 0.3706, val loss: 1.1275, val acc: 0.4755  (best train acc: 0.3989, best val acc: 0.4884)\n",
      "[Epoch: 7260] train loss: 1.2817, train acc: 0.3742, val loss: 1.1291, val acc: 0.4752  (best train acc: 0.3989, best val acc: 0.4884)\n",
      "[Epoch: 7280] train loss: 1.2849, train acc: 0.3735, val loss: 1.1271, val acc: 0.4749  (best train acc: 0.3989, best val acc: 0.4884)\n",
      "[Epoch: 7300] train loss: 1.2775, train acc: 0.3732, val loss: 1.1296, val acc: 0.4749  (best train acc: 0.3989, best val acc: 0.4884)\n",
      "[Epoch: 7320] train loss: 1.2901, train acc: 0.3720, val loss: 1.1273, val acc: 0.4749  (best train acc: 0.3989, best val acc: 0.4884)\n",
      "[Epoch: 7340] train loss: 1.2822, train acc: 0.3724, val loss: 1.1239, val acc: 0.4809  (best train acc: 0.3989, best val acc: 0.4884)\n",
      "[Epoch: 7360] train loss: 1.2759, train acc: 0.3776, val loss: 1.1315, val acc: 0.4759  (best train acc: 0.3989, best val acc: 0.4884)\n",
      "[Epoch: 7380] train loss: 1.2856, train acc: 0.3711, val loss: 1.1287, val acc: 0.4752  (best train acc: 0.3989, best val acc: 0.4884)\n",
      "[Epoch: 7400] train loss: 1.2824, train acc: 0.3709, val loss: 1.1295, val acc: 0.4820  (best train acc: 0.3989, best val acc: 0.4884)\n",
      "[Epoch: 7420] train loss: 1.2799, train acc: 0.3753, val loss: 1.1293, val acc: 0.4799  (best train acc: 0.3989, best val acc: 0.4884)\n",
      "[Epoch: 7440] train loss: 1.2822, train acc: 0.3738, val loss: 1.1303, val acc: 0.4789  (best train acc: 0.3989, best val acc: 0.4884)\n",
      "[Epoch: 7460] train loss: 1.2778, train acc: 0.3752, val loss: 1.1262, val acc: 0.4725  (best train acc: 0.3989, best val acc: 0.4884)\n",
      "[Epoch: 7480] train loss: 1.2794, train acc: 0.3743, val loss: 1.1256, val acc: 0.4742  (best train acc: 0.3989, best val acc: 0.4884)\n",
      "[Epoch: 7500] train loss: 1.2862, train acc: 0.3691, val loss: 1.1307, val acc: 0.4728  (best train acc: 0.3989, best val acc: 0.4884)\n",
      "[Epoch: 7520] train loss: 1.2849, train acc: 0.3716, val loss: 1.1271, val acc: 0.4752  (best train acc: 0.3989, best val acc: 0.4884)\n",
      "[Epoch: 7540] train loss: 1.2791, train acc: 0.3755, val loss: 1.1276, val acc: 0.4772  (best train acc: 0.3989, best val acc: 0.4884)\n",
      "[Epoch: 7560] train loss: 1.2829, train acc: 0.3734, val loss: 1.1280, val acc: 0.4752  (best train acc: 0.3989, best val acc: 0.4884)\n",
      "[Epoch: 7580] train loss: 1.2816, train acc: 0.3747, val loss: 1.1259, val acc: 0.4735  (best train acc: 0.3989, best val acc: 0.4884)\n",
      "[Epoch: 7600] train loss: 1.2755, train acc: 0.3740, val loss: 1.1288, val acc: 0.4826  (best train acc: 0.3989, best val acc: 0.4884)\n",
      "[Epoch: 7620] train loss: 1.2776, train acc: 0.3757, val loss: 1.1308, val acc: 0.4752  (best train acc: 0.3989, best val acc: 0.4884)\n",
      "[Epoch: 7640] train loss: 1.2862, train acc: 0.3698, val loss: 1.1269, val acc: 0.4732  (best train acc: 0.3989, best val acc: 0.4884)\n",
      "[Epoch: 7660] train loss: 1.2842, train acc: 0.3676, val loss: 1.1295, val acc: 0.4742  (best train acc: 0.3989, best val acc: 0.4884)\n",
      "[Epoch: 7680] train loss: 1.2799, train acc: 0.3730, val loss: 1.1268, val acc: 0.4742  (best train acc: 0.3989, best val acc: 0.4884)\n",
      "[Epoch: 7700] train loss: 1.2767, train acc: 0.3746, val loss: 1.1297, val acc: 0.4739  (best train acc: 0.3989, best val acc: 0.4884)\n",
      "[Epoch: 7720] train loss: 1.2843, train acc: 0.3700, val loss: 1.1247, val acc: 0.4745  (best train acc: 0.3989, best val acc: 0.4884)\n",
      "[Epoch: 7740] train loss: 1.2869, train acc: 0.3693, val loss: 1.1218, val acc: 0.4755  (best train acc: 0.3989, best val acc: 0.4884)\n",
      "[Epoch: 7760] train loss: 1.2915, train acc: 0.3678, val loss: 1.1222, val acc: 0.4766  (best train acc: 0.3989, best val acc: 0.4884)\n",
      "[Epoch: 7780] train loss: 1.2873, train acc: 0.3668, val loss: 1.1273, val acc: 0.4718  (best train acc: 0.3989, best val acc: 0.4884)\n",
      "[Epoch: 7800] train loss: 1.2820, train acc: 0.3741, val loss: 1.1263, val acc: 0.4735  (best train acc: 0.3989, best val acc: 0.4884)\n",
      "[Epoch: 7820] train loss: 1.2860, train acc: 0.3715, val loss: 1.1302, val acc: 0.4749  (best train acc: 0.3989, best val acc: 0.4884)\n",
      "[Epoch: 7840] train loss: 1.2869, train acc: 0.3684, val loss: 1.1257, val acc: 0.4776  (best train acc: 0.3989, best val acc: 0.4884)\n",
      "[Epoch: 7860] train loss: 1.2887, train acc: 0.3699, val loss: 1.1283, val acc: 0.4735  (best train acc: 0.3989, best val acc: 0.4884)\n",
      "[Epoch: 7880] train loss: 1.2735, train acc: 0.3780, val loss: 1.1266, val acc: 0.4752  (best train acc: 0.3989, best val acc: 0.4884)\n",
      "[Epoch: 7900] train loss: 1.2807, train acc: 0.3704, val loss: 1.1240, val acc: 0.4799  (best train acc: 0.3989, best val acc: 0.4884)\n",
      "[Epoch: 7920] train loss: 1.2829, train acc: 0.3723, val loss: 1.1285, val acc: 0.4755  (best train acc: 0.3989, best val acc: 0.4884)\n",
      "[Epoch: 7940] train loss: 1.2802, train acc: 0.3763, val loss: 1.1239, val acc: 0.4728  (best train acc: 0.3989, best val acc: 0.4884)\n",
      "[Epoch: 7960] train loss: 1.2781, train acc: 0.3748, val loss: 1.1288, val acc: 0.4793  (best train acc: 0.3989, best val acc: 0.4884)\n",
      "[Epoch: 7980] train loss: 1.2810, train acc: 0.3720, val loss: 1.1256, val acc: 0.4739  (best train acc: 0.3989, best val acc: 0.4884)\n",
      "[Epoch: 8000] train loss: 1.2798, train acc: 0.3735, val loss: 1.1270, val acc: 0.4755  (best train acc: 0.3989, best val acc: 0.4884)\n",
      "[Epoch: 8020] train loss: 1.2760, train acc: 0.3772, val loss: 1.1307, val acc: 0.4796  (best train acc: 0.3989, best val acc: 0.4884)\n",
      "[Epoch: 8040] train loss: 1.2795, train acc: 0.3727, val loss: 1.1244, val acc: 0.4759  (best train acc: 0.3989, best val acc: 0.4884)\n",
      "[Epoch: 8060] train loss: 1.2827, train acc: 0.3717, val loss: 1.1263, val acc: 0.4755  (best train acc: 0.3989, best val acc: 0.4884)\n",
      "[Epoch: 8080] train loss: 1.2806, train acc: 0.3765, val loss: 1.1233, val acc: 0.4776  (best train acc: 0.3989, best val acc: 0.4884)\n",
      "[Epoch: 8100] train loss: 1.2835, train acc: 0.3741, val loss: 1.1303, val acc: 0.4793  (best train acc: 0.3989, best val acc: 0.4884)\n",
      "[Epoch: 8120] train loss: 1.2809, train acc: 0.3767, val loss: 1.1263, val acc: 0.4745  (best train acc: 0.3989, best val acc: 0.4884)\n",
      "[Epoch: 8140] train loss: 1.2863, train acc: 0.3699, val loss: 1.1276, val acc: 0.4745  (best train acc: 0.3989, best val acc: 0.4884)\n",
      "[Epoch: 8160] train loss: 1.2742, train acc: 0.3760, val loss: 1.1242, val acc: 0.4745  (best train acc: 0.3989, best val acc: 0.4884)\n",
      "[Epoch: 8180] train loss: 1.2813, train acc: 0.3749, val loss: 1.1266, val acc: 0.4755  (best train acc: 0.3989, best val acc: 0.4884)\n",
      "[Epoch: 8200] train loss: 1.2844, train acc: 0.3730, val loss: 1.1222, val acc: 0.4779  (best train acc: 0.3989, best val acc: 0.4884)\n",
      "[Epoch: 8220] train loss: 1.2825, train acc: 0.3717, val loss: 1.1257, val acc: 0.4739  (best train acc: 0.3989, best val acc: 0.4884)\n",
      "[Epoch: 8240] train loss: 1.2802, train acc: 0.3736, val loss: 1.1257, val acc: 0.4820  (best train acc: 0.3989, best val acc: 0.4884)\n",
      "[Epoch: 8260] train loss: 1.2817, train acc: 0.3724, val loss: 1.1256, val acc: 0.4786  (best train acc: 0.3989, best val acc: 0.4884)\n",
      "[Epoch: 8280] train loss: 1.2813, train acc: 0.3707, val loss: 1.1266, val acc: 0.4755  (best train acc: 0.3989, best val acc: 0.4884)\n",
      "[Epoch: 8300] train loss: 1.2817, train acc: 0.3707, val loss: 1.1233, val acc: 0.4749  (best train acc: 0.3989, best val acc: 0.4884)\n",
      "[Epoch: 8320] train loss: 1.2868, train acc: 0.3724, val loss: 1.1241, val acc: 0.4749  (best train acc: 0.3989, best val acc: 0.4884)\n",
      "[Epoch: 8340] train loss: 1.2808, train acc: 0.3754, val loss: 1.1234, val acc: 0.4769  (best train acc: 0.3989, best val acc: 0.4884)\n",
      "[Epoch: 8360] train loss: 1.2813, train acc: 0.3736, val loss: 1.1262, val acc: 0.4759  (best train acc: 0.3989, best val acc: 0.4884)\n",
      "[Epoch: 8380] train loss: 1.2819, train acc: 0.3706, val loss: 1.1294, val acc: 0.4742  (best train acc: 0.3989, best val acc: 0.4884)\n",
      "[Epoch: 8400] train loss: 1.2775, train acc: 0.3782, val loss: 1.1292, val acc: 0.4766  (best train acc: 0.3989, best val acc: 0.4884)\n",
      "[Epoch: 8420] train loss: 1.2865, train acc: 0.3709, val loss: 1.1238, val acc: 0.4786  (best train acc: 0.3989, best val acc: 0.4884)\n",
      "[Epoch: 8440] train loss: 1.2774, train acc: 0.3753, val loss: 1.1244, val acc: 0.4742  (best train acc: 0.3989, best val acc: 0.4884)\n",
      "[Epoch: 8460] train loss: 1.2850, train acc: 0.3730, val loss: 1.1259, val acc: 0.4749  (best train acc: 0.3989, best val acc: 0.4884)\n",
      "[Epoch: 8480] train loss: 1.2804, train acc: 0.3777, val loss: 1.1284, val acc: 0.4769  (best train acc: 0.3989, best val acc: 0.4884)\n",
      "[Epoch: 8500] train loss: 1.2814, train acc: 0.3748, val loss: 1.1283, val acc: 0.4772  (best train acc: 0.3989, best val acc: 0.4884)\n",
      "[Epoch: 8520] train loss: 1.2844, train acc: 0.3732, val loss: 1.1347, val acc: 0.4776  (best train acc: 0.3989, best val acc: 0.4884)\n",
      "[Epoch: 8540] train loss: 1.2883, train acc: 0.3681, val loss: 1.1318, val acc: 0.4803  (best train acc: 0.3989, best val acc: 0.4884)\n",
      "[Epoch: 8560] train loss: 1.2826, train acc: 0.3733, val loss: 1.1271, val acc: 0.4782  (best train acc: 0.3989, best val acc: 0.4884)\n",
      "[Epoch: 8580] train loss: 1.2784, train acc: 0.3757, val loss: 1.1250, val acc: 0.4786  (best train acc: 0.3989, best val acc: 0.4884)\n",
      "[Epoch: 8600] train loss: 1.2715, train acc: 0.3815, val loss: 1.1264, val acc: 0.4779  (best train acc: 0.3989, best val acc: 0.4884)\n",
      "[Epoch: 8620] train loss: 1.2801, train acc: 0.3734, val loss: 1.1273, val acc: 0.4759  (best train acc: 0.3989, best val acc: 0.4884)\n",
      "[Epoch: 8640] train loss: 1.2823, train acc: 0.3730, val loss: 1.1253, val acc: 0.4796  (best train acc: 0.3989, best val acc: 0.4884)\n",
      "[Epoch: 8660] train loss: 1.2762, train acc: 0.3769, val loss: 1.1279, val acc: 0.4755  (best train acc: 0.3989, best val acc: 0.4884)\n",
      "[Epoch: 8680] train loss: 1.2867, train acc: 0.3709, val loss: 1.1251, val acc: 0.4799  (best train acc: 0.3989, best val acc: 0.4884)\n",
      "[Epoch: 8700] train loss: 1.2829, train acc: 0.3723, val loss: 1.1219, val acc: 0.4772  (best train acc: 0.3989, best val acc: 0.4884)\n",
      "[Epoch: 8720] train loss: 1.2803, train acc: 0.3706, val loss: 1.1250, val acc: 0.4769  (best train acc: 0.3989, best val acc: 0.4884)\n",
      "[Epoch: 8740] train loss: 1.2766, train acc: 0.3759, val loss: 1.1230, val acc: 0.4752  (best train acc: 0.3989, best val acc: 0.4884)\n",
      "[Epoch: 8760] train loss: 1.2842, train acc: 0.3697, val loss: 1.1275, val acc: 0.4762  (best train acc: 0.3989, best val acc: 0.4884)\n",
      "[Epoch: 8780] train loss: 1.2766, train acc: 0.3788, val loss: 1.1282, val acc: 0.4786  (best train acc: 0.3989, best val acc: 0.4884)\n",
      "[Epoch: 8800] train loss: 1.2841, train acc: 0.3733, val loss: 1.1268, val acc: 0.4769  (best train acc: 0.3989, best val acc: 0.4884)\n",
      "[Epoch: 8820] train loss: 1.2926, train acc: 0.3710, val loss: 1.1235, val acc: 0.4789  (best train acc: 0.3989, best val acc: 0.4884)\n",
      "[Epoch: 8840] train loss: 1.2855, train acc: 0.3664, val loss: 1.1197, val acc: 0.4786  (best train acc: 0.3989, best val acc: 0.4884)\n",
      "[Epoch: 8860] train loss: 1.2837, train acc: 0.3678, val loss: 1.1318, val acc: 0.4796  (best train acc: 0.3989, best val acc: 0.4884)\n",
      "[Epoch: 8880] train loss: 1.2797, train acc: 0.3721, val loss: 1.1287, val acc: 0.4755  (best train acc: 0.3989, best val acc: 0.4884)\n",
      "[Epoch: 8900] train loss: 1.2813, train acc: 0.3725, val loss: 1.1290, val acc: 0.4766  (best train acc: 0.3989, best val acc: 0.4884)\n",
      "[Epoch: 8920] train loss: 1.2864, train acc: 0.3704, val loss: 1.1264, val acc: 0.4749  (best train acc: 0.3989, best val acc: 0.4884)\n",
      "[Epoch: 8940] train loss: 1.2759, train acc: 0.3735, val loss: 1.1240, val acc: 0.4752  (best train acc: 0.3989, best val acc: 0.4884)\n",
      "[Epoch: 8960] train loss: 1.2760, train acc: 0.3741, val loss: 1.1238, val acc: 0.4749  (best train acc: 0.3989, best val acc: 0.4884)\n",
      "[Epoch: 8980] train loss: 1.2791, train acc: 0.3697, val loss: 1.1264, val acc: 0.4769  (best train acc: 0.3989, best val acc: 0.4884)\n",
      "[Epoch: 9000] train loss: 1.2847, train acc: 0.3766, val loss: 1.1332, val acc: 0.4843  (best train acc: 0.3989, best val acc: 0.4884)\n",
      "[Epoch: 9020] train loss: 1.2919, train acc: 0.3680, val loss: 1.1261, val acc: 0.4769  (best train acc: 0.3989, best val acc: 0.4924)\n",
      "[Epoch: 9040] train loss: 1.2844, train acc: 0.3736, val loss: 1.1211, val acc: 0.4820  (best train acc: 0.3989, best val acc: 0.4924)\n",
      "[Epoch: 9060] train loss: 1.2860, train acc: 0.3712, val loss: 1.1276, val acc: 0.4803  (best train acc: 0.3989, best val acc: 0.4924)\n",
      "[Epoch: 9080] train loss: 1.2822, train acc: 0.3703, val loss: 1.1288, val acc: 0.4762  (best train acc: 0.3989, best val acc: 0.4924)\n",
      "[Epoch: 9100] train loss: 1.2829, train acc: 0.3706, val loss: 1.1209, val acc: 0.4766  (best train acc: 0.3989, best val acc: 0.4924)\n",
      "[Epoch: 9120] train loss: 1.2759, train acc: 0.3780, val loss: 1.1237, val acc: 0.4762  (best train acc: 0.3989, best val acc: 0.4924)\n",
      "[Epoch: 9140] train loss: 1.2721, train acc: 0.3770, val loss: 1.1226, val acc: 0.4749  (best train acc: 0.3989, best val acc: 0.4924)\n",
      "[Epoch: 9160] train loss: 1.2791, train acc: 0.3690, val loss: 1.1206, val acc: 0.4793  (best train acc: 0.3989, best val acc: 0.4924)\n",
      "[Epoch: 9180] train loss: 1.2873, train acc: 0.3717, val loss: 1.1234, val acc: 0.4755  (best train acc: 0.3989, best val acc: 0.4924)\n",
      "[Epoch: 9200] train loss: 1.2864, train acc: 0.3726, val loss: 1.1232, val acc: 0.4745  (best train acc: 0.3989, best val acc: 0.4924)\n",
      "[Epoch: 9220] train loss: 1.2823, train acc: 0.3695, val loss: 1.1223, val acc: 0.4755  (best train acc: 0.3989, best val acc: 0.4924)\n",
      "[Epoch: 9240] train loss: 1.2815, train acc: 0.3713, val loss: 1.1228, val acc: 0.4755  (best train acc: 0.3989, best val acc: 0.4924)\n",
      "[Epoch: 9260] train loss: 1.2770, train acc: 0.3721, val loss: 1.1235, val acc: 0.4776  (best train acc: 0.3989, best val acc: 0.4924)\n",
      "[Epoch: 9280] train loss: 1.2759, train acc: 0.3758, val loss: 1.1282, val acc: 0.4816  (best train acc: 0.3989, best val acc: 0.4924)\n",
      "[Epoch: 9300] train loss: 1.2709, train acc: 0.3777, val loss: 1.1212, val acc: 0.4809  (best train acc: 0.3989, best val acc: 0.4924)\n",
      "[Epoch: 9320] train loss: 1.2828, train acc: 0.3705, val loss: 1.1252, val acc: 0.4779  (best train acc: 0.3989, best val acc: 0.4924)\n",
      "[Epoch: 9340] train loss: 1.2773, train acc: 0.3753, val loss: 1.1260, val acc: 0.4772  (best train acc: 0.3989, best val acc: 0.4924)\n",
      "[Epoch: 9360] train loss: 1.2803, train acc: 0.3730, val loss: 1.1262, val acc: 0.4769  (best train acc: 0.3989, best val acc: 0.4924)\n",
      "[Epoch: 9380] train loss: 1.2880, train acc: 0.3696, val loss: 1.1269, val acc: 0.4762  (best train acc: 0.3989, best val acc: 0.4924)\n",
      "[Epoch: 9400] train loss: 1.2798, train acc: 0.3723, val loss: 1.1349, val acc: 0.4803  (best train acc: 0.3989, best val acc: 0.4924)\n",
      "[Epoch: 9420] train loss: 1.2843, train acc: 0.3739, val loss: 1.1220, val acc: 0.4789  (best train acc: 0.3989, best val acc: 0.4924)\n",
      "[Epoch: 9440] train loss: 1.2805, train acc: 0.3723, val loss: 1.1252, val acc: 0.4799  (best train acc: 0.3989, best val acc: 0.4924)\n",
      "[Epoch: 9460] train loss: 1.2809, train acc: 0.3751, val loss: 1.1227, val acc: 0.4762  (best train acc: 0.3989, best val acc: 0.4924)\n",
      "[Epoch: 9480] train loss: 1.2803, train acc: 0.3713, val loss: 1.1251, val acc: 0.4786  (best train acc: 0.3989, best val acc: 0.4924)\n",
      "[Epoch: 9500] train loss: 1.2751, train acc: 0.3743, val loss: 1.1249, val acc: 0.4749  (best train acc: 0.3989, best val acc: 0.4924)\n",
      "[Epoch: 9520] train loss: 1.2880, train acc: 0.3694, val loss: 1.1224, val acc: 0.4799  (best train acc: 0.3989, best val acc: 0.4924)\n",
      "[Epoch: 9540] train loss: 1.2831, train acc: 0.3687, val loss: 1.1232, val acc: 0.4813  (best train acc: 0.3989, best val acc: 0.4924)\n",
      "[Epoch: 9560] train loss: 1.2836, train acc: 0.3728, val loss: 1.1254, val acc: 0.4786  (best train acc: 0.3989, best val acc: 0.4924)\n",
      "[Epoch: 9580] train loss: 1.2886, train acc: 0.3644, val loss: 1.1189, val acc: 0.4776  (best train acc: 0.3989, best val acc: 0.4924)\n",
      "[Epoch: 9600] train loss: 1.2821, train acc: 0.3733, val loss: 1.1209, val acc: 0.4766  (best train acc: 0.3989, best val acc: 0.4924)\n",
      "[Epoch: 9620] train loss: 1.2851, train acc: 0.3749, val loss: 1.1161, val acc: 0.4776  (best train acc: 0.3989, best val acc: 0.4924)\n",
      "[Epoch: 9640] train loss: 1.2783, train acc: 0.3736, val loss: 1.1253, val acc: 0.4779  (best train acc: 0.3989, best val acc: 0.4924)\n",
      "[Epoch: 9660] train loss: 1.2813, train acc: 0.3732, val loss: 1.1290, val acc: 0.4836  (best train acc: 0.3989, best val acc: 0.4924)\n",
      "[Epoch: 9680] train loss: 1.2809, train acc: 0.3717, val loss: 1.1253, val acc: 0.4769  (best train acc: 0.3989, best val acc: 0.4924)\n",
      "[Epoch: 9700] train loss: 1.2827, train acc: 0.3785, val loss: 1.1241, val acc: 0.4772  (best train acc: 0.3989, best val acc: 0.4924)\n",
      "[Epoch: 9720] train loss: 1.2788, train acc: 0.3753, val loss: 1.1297, val acc: 0.4809  (best train acc: 0.3989, best val acc: 0.4924)\n",
      "[Epoch: 9740] train loss: 1.2866, train acc: 0.3685, val loss: 1.1247, val acc: 0.4759  (best train acc: 0.3989, best val acc: 0.4924)\n",
      "[Epoch: 9760] train loss: 1.2821, train acc: 0.3726, val loss: 1.1215, val acc: 0.4769  (best train acc: 0.3989, best val acc: 0.4924)\n",
      "[Epoch: 9780] train loss: 1.2848, train acc: 0.3715, val loss: 1.1257, val acc: 0.4776  (best train acc: 0.3989, best val acc: 0.4924)\n",
      "[Epoch: 9800] train loss: 1.2783, train acc: 0.3771, val loss: 1.1244, val acc: 0.4803  (best train acc: 0.3989, best val acc: 0.4924)\n",
      "[Epoch: 9820] train loss: 1.2809, train acc: 0.3742, val loss: 1.1305, val acc: 0.4809  (best train acc: 0.3989, best val acc: 0.4924)\n",
      "[Epoch: 9840] train loss: 1.2821, train acc: 0.3716, val loss: 1.1242, val acc: 0.4755  (best train acc: 0.3989, best val acc: 0.4924)\n",
      "[Epoch: 9860] train loss: 1.2743, train acc: 0.3784, val loss: 1.1265, val acc: 0.4803  (best train acc: 0.3989, best val acc: 0.4924)\n",
      "[Epoch: 9880] train loss: 1.2721, train acc: 0.3767, val loss: 1.1244, val acc: 0.4776  (best train acc: 0.3989, best val acc: 0.4924)\n",
      "[Epoch: 9900] train loss: 1.2825, train acc: 0.3724, val loss: 1.1231, val acc: 0.4786  (best train acc: 0.3989, best val acc: 0.4924)\n",
      "[Epoch: 9920] train loss: 1.2773, train acc: 0.3754, val loss: 1.1244, val acc: 0.4789  (best train acc: 0.3989, best val acc: 0.4924)\n",
      "[Epoch: 9940] train loss: 1.2834, train acc: 0.3745, val loss: 1.1220, val acc: 0.4779  (best train acc: 0.3989, best val acc: 0.4924)\n",
      "[Epoch: 9960] train loss: 1.2861, train acc: 0.3704, val loss: 1.1201, val acc: 0.4745  (best train acc: 0.3989, best val acc: 0.4924)\n",
      "[Epoch: 9980] train loss: 1.2783, train acc: 0.3778, val loss: 1.1216, val acc: 0.4776  (best train acc: 0.3989, best val acc: 0.4924)\n",
      "[Epoch: 10000] train loss: 1.2813, train acc: 0.3714, val loss: 1.1319, val acc: 0.4820  (best train acc: 0.3989, best val acc: 0.4924)\n",
      "[Epoch: 10020] train loss: 1.2855, train acc: 0.3657, val loss: 1.1238, val acc: 0.4782  (best train acc: 0.3989, best val acc: 0.4924)\n",
      "[Epoch: 10040] train loss: 1.2883, train acc: 0.3725, val loss: 1.1219, val acc: 0.4809  (best train acc: 0.3989, best val acc: 0.4924)\n",
      "[Epoch: 10060] train loss: 1.2764, train acc: 0.3736, val loss: 1.1293, val acc: 0.4833  (best train acc: 0.3989, best val acc: 0.4924)\n",
      "[Epoch: 10080] train loss: 1.2760, train acc: 0.3783, val loss: 1.1186, val acc: 0.4786  (best train acc: 0.3989, best val acc: 0.4924)\n",
      "[Epoch: 10100] train loss: 1.2821, train acc: 0.3697, val loss: 1.1154, val acc: 0.4766  (best train acc: 0.3989, best val acc: 0.4924)\n",
      "[Epoch: 10120] train loss: 1.2891, train acc: 0.3647, val loss: 1.1214, val acc: 0.4813  (best train acc: 0.3989, best val acc: 0.4924)\n",
      "[Epoch: 10140] train loss: 1.2813, train acc: 0.3754, val loss: 1.1270, val acc: 0.4799  (best train acc: 0.3989, best val acc: 0.4924)\n",
      "[Epoch: 10160] train loss: 1.2769, train acc: 0.3738, val loss: 1.1231, val acc: 0.4776  (best train acc: 0.3989, best val acc: 0.4924)\n",
      "[Epoch: 10180] train loss: 1.2757, train acc: 0.3759, val loss: 1.1254, val acc: 0.4793  (best train acc: 0.3989, best val acc: 0.4924)\n",
      "[Epoch: 10200] train loss: 1.2730, train acc: 0.3743, val loss: 1.1199, val acc: 0.4739  (best train acc: 0.3989, best val acc: 0.4924)\n",
      "[Epoch: 10220] train loss: 1.2812, train acc: 0.3719, val loss: 1.1264, val acc: 0.4820  (best train acc: 0.3989, best val acc: 0.4924)\n",
      "[Epoch: 10240] train loss: 1.2805, train acc: 0.3759, val loss: 1.1207, val acc: 0.4752  (best train acc: 0.3989, best val acc: 0.4924)\n",
      "[Epoch: 10260] train loss: 1.2796, train acc: 0.3747, val loss: 1.1252, val acc: 0.4809  (best train acc: 0.3989, best val acc: 0.4924)\n",
      "[Epoch: 10280] train loss: 1.2729, train acc: 0.3803, val loss: 1.1275, val acc: 0.4803  (best train acc: 0.3989, best val acc: 0.4924)\n",
      "[Epoch: 10300] train loss: 1.2839, train acc: 0.3649, val loss: 1.1193, val acc: 0.4776  (best train acc: 0.3989, best val acc: 0.4924)\n",
      "[Epoch: 10320] train loss: 1.2855, train acc: 0.3719, val loss: 1.1180, val acc: 0.4796  (best train acc: 0.3989, best val acc: 0.4924)\n",
      "[Epoch: 10340] train loss: 1.2848, train acc: 0.3697, val loss: 1.1165, val acc: 0.4766  (best train acc: 0.3989, best val acc: 0.4924)\n",
      "[Epoch: 10360] train loss: 1.2876, train acc: 0.3737, val loss: 1.1221, val acc: 0.4786  (best train acc: 0.3989, best val acc: 0.4924)\n",
      "[Epoch: 10380] train loss: 1.2781, train acc: 0.3721, val loss: 1.1225, val acc: 0.4799  (best train acc: 0.3989, best val acc: 0.4924)\n",
      "[Epoch: 10400] train loss: 1.2815, train acc: 0.3758, val loss: 1.1193, val acc: 0.4776  (best train acc: 0.3989, best val acc: 0.4924)\n",
      "[Epoch: 10420] train loss: 1.2804, train acc: 0.3743, val loss: 1.1235, val acc: 0.4820  (best train acc: 0.3989, best val acc: 0.4924)\n",
      "[Epoch: 10440] train loss: 1.2687, train acc: 0.3804, val loss: 1.1211, val acc: 0.4786  (best train acc: 0.3989, best val acc: 0.4924)\n",
      "[Epoch: 10460] train loss: 1.2574, train acc: 0.3902, val loss: 1.1074, val acc: 0.4826  (best train acc: 0.3989, best val acc: 0.4924)\n",
      "[Epoch: 10480] train loss: 1.2554, train acc: 0.3848, val loss: 1.1081, val acc: 0.4782  (best train acc: 0.4022, best val acc: 0.4924)\n",
      "[Epoch: 10500] train loss: 1.2369, train acc: 0.4009, val loss: 1.0652, val acc: 0.4931  (best train acc: 0.4022, best val acc: 0.4978)\n",
      "[Epoch: 10520] train loss: 1.5375, train acc: 0.2370, val loss: 1.5370, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 10540] train loss: 1.5271, train acc: 0.2370, val loss: 1.5269, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 10560] train loss: 1.5225, train acc: 0.2370, val loss: 1.5224, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 10580] train loss: 1.5207, train acc: 0.2370, val loss: 1.5208, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 10600] train loss: 1.5198, train acc: 0.2371, val loss: 1.5199, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 10620] train loss: 1.5193, train acc: 0.2371, val loss: 1.5194, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 10640] train loss: 1.5190, train acc: 0.2371, val loss: 1.5191, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 10660] train loss: 1.5188, train acc: 0.2371, val loss: 1.5189, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 10680] train loss: 1.5186, train acc: 0.2371, val loss: 1.5188, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 10700] train loss: 1.5185, train acc: 0.2371, val loss: 1.5187, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 10720] train loss: 1.5185, train acc: 0.2371, val loss: 1.5186, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 10740] train loss: 1.5184, train acc: 0.2371, val loss: 1.5186, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 10760] train loss: 1.5184, train acc: 0.2371, val loss: 1.5186, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 10780] train loss: 1.5184, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 10800] train loss: 1.5184, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 10820] train loss: 1.5184, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 10840] train loss: 1.5184, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 10860] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 10880] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 10900] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 10920] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 10940] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 10960] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 10980] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 11000] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 11020] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 11040] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 11060] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 11080] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 11100] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 11120] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 11140] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 11160] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 11180] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 11200] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 11220] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 11240] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 11260] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 11280] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 11300] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 11320] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 11340] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 11360] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 11380] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 11400] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 11420] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 11440] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2368  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 11460] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2368  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 11480] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 11500] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 11520] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 11540] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 11560] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 11580] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 11600] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 11620] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 11640] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2368  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 11660] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2368  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 11680] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2368  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 11700] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 11720] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 11740] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 11760] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 11780] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 11800] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 11820] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 11840] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 11860] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 11880] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 11900] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 11920] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 11940] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 11960] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 11980] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2368  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 12000] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2368  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 12020] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2368  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 12040] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 12060] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 12080] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 12100] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 12120] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 12140] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 12160] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 12180] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2368  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 12200] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2368  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 12220] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 12240] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 12260] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 12280] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 12300] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 12320] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 12340] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 12360] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 12380] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 12400] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 12420] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 12440] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 12460] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2368  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 12480] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2368  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 12500] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 12520] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 12540] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 12560] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 12580] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 12600] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 12620] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 12640] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 12660] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 12680] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 12700] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 12720] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 12740] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 12760] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 12780] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 12800] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 12820] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 12840] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 12860] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 12880] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2368  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 12900] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 12920] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 12940] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 12960] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2368  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 12980] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 13000] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 13020] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 13040] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 13060] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 13080] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2368  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 13100] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2368  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 13120] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 13140] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 13160] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 13180] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2368  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 13200] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 13220] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 13240] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 13260] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2368  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 13280] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2368  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 13300] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 13320] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 13340] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2368  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 13360] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 13380] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 13400] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 13420] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 13440] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 13460] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 13480] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 13500] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 13520] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 13540] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2368  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 13560] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2368  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 13580] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 13600] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 13620] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 13640] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2368  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 13660] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 13680] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2368  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 13700] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 13720] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 13740] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2368  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 13760] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2368  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 13780] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2368  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 13800] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2368  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 13820] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2368  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 13840] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2368  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 13860] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2368  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 13880] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2368  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 13900] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 13920] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2368  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 13940] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 13960] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2368  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 13980] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 14000] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 14020] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2368  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 14040] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 14060] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2368  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 14080] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 14100] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 14120] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 14140] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2368  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 14160] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 14180] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2368  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 14200] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 14220] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 14240] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2368  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 14260] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2368  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 14280] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 14300] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2368  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 14320] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 14340] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 14360] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2368  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 14380] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 14400] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2368  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 14420] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 14440] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 14460] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 14480] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 14500] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 14520] train loss: 1.5183, train acc: 0.2371, val loss: 1.5186, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 14540] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2368  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 14560] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 14580] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 14600] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 14620] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 14640] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2368  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 14660] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 14680] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 14700] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 14720] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 14740] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 14760] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 14780] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 14800] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 14820] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 14840] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 14860] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 14880] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 14900] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 14920] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 14940] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 14960] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 14980] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2368  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 15000] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 15020] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2368  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 15040] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 15060] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2368  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 15080] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 15100] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2368  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 15120] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 15140] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 15160] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2368  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 15180] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 15200] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2368  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 15220] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 15240] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 15260] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2368  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 15280] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2368  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 15300] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2368  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 15320] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2368  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 15340] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2368  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 15360] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2368  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 15380] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2368  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 15400] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2368  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 15420] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2368  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 15440] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2368  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 15460] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2368  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 15480] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2368  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 15500] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2368  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 15520] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2368  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 15540] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2368  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 15560] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2368  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 15580] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2368  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 15600] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2368  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 15620] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2368  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 15640] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2368  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 15660] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 15680] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 15700] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2368  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 15720] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 15740] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 15760] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2368  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 15780] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2368  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 15800] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 15820] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 15840] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 15860] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2368  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 15880] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 15900] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 15920] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2368  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 15940] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 15960] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 15980] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2368  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 16000] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 16020] train loss: 1.5183, train acc: 0.2371, val loss: 1.5186, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 16040] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2368  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 16060] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2368  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 16080] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 16100] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2368  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 16120] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2368  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 16140] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 16160] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2368  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 16180] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2368  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 16200] train loss: 1.5184, train acc: 0.2371, val loss: 1.5186, val acc: 0.2368  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 16220] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 16240] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2368  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 16260] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2368  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 16280] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2368  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 16300] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 16320] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 16340] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 16360] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2368  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 16380] train loss: 1.5183, train acc: 0.2371, val loss: 1.5186, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 16400] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 16420] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2368  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 16440] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 16460] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 16480] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 16500] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 16520] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 16540] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 16560] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 16580] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 16600] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 16620] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 16640] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2368  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 16660] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 16680] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 16700] train loss: 1.5183, train acc: 0.2371, val loss: 1.5186, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 16720] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2368  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 16740] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 16760] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 16780] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2368  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 16800] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 16820] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2368  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 16840] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 16860] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 16880] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 16900] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 16920] train loss: 1.5183, train acc: 0.2371, val loss: 1.5188, val acc: 0.2368  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 16940] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2368  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 16960] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 16980] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 17000] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2368  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 17020] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2368  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 17040] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2368  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 17060] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2368  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 17080] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2368  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 17100] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2368  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 17120] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2368  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 17140] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2368  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 17160] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2368  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 17180] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2368  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 17200] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2368  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 17220] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2368  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 17240] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2368  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 17260] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2368  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 17280] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2368  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 17300] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2368  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 17320] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2368  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 17340] train loss: 1.5183, train acc: 0.2371, val loss: 1.5186, val acc: 0.2368  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 17360] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2368  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 17380] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2368  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 17400] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 17420] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2368  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 17440] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2368  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 17460] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2368  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 17480] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 17500] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 17520] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 17540] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2368  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 17560] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2368  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 17580] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2368  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 17600] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2368  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 17620] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2368  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 17640] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2368  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 17660] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2368  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 17680] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2368  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 17700] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 17720] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 17740] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2368  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 17760] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 17780] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2368  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 17800] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2368  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 17820] train loss: 1.5183, train acc: 0.2371, val loss: 1.5186, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 17840] train loss: 1.5183, train acc: 0.2371, val loss: 1.5186, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 17860] train loss: 1.5183, train acc: 0.2371, val loss: 1.5186, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 17880] train loss: 1.5183, train acc: 0.2371, val loss: 1.5191, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 17900] train loss: 1.5181, train acc: 0.2372, val loss: 1.5192, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 17920] train loss: 1.5183, train acc: 0.2371, val loss: 1.5188, val acc: 0.2368  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 17940] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 17960] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 17980] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2368  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 18000] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 18020] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 18040] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2368  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 18060] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 18080] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 18100] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 18120] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 18140] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 18160] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 18180] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 18200] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 18220] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 18240] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 18260] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 18280] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 18300] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 18320] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 18340] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 18360] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 18380] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 18400] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 18420] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 18440] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 18460] train loss: 1.5183, train acc: 0.2371, val loss: 1.5186, val acc: 0.2368  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 18480] train loss: 1.5184, train acc: 0.2371, val loss: 1.5186, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 18500] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 18520] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2368  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 18540] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 18560] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 18580] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2368  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 18600] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 18620] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 18640] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 18660] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2368  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 18680] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 18700] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 18720] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 18740] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 18760] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 18780] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 18800] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 18820] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 18840] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 18860] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 18880] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 18900] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 18920] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 18940] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 18960] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 18980] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 19000] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 19020] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 19040] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 19060] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 19080] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 19100] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 19120] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2368  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 19140] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 19160] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 19180] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 19200] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 19220] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 19240] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 19260] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 19280] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 19300] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 19320] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 19340] train loss: 1.5183, train acc: 0.2371, val loss: 1.5193, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 19360] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2368  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 19380] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 19400] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 19420] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2368  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 19440] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 19460] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 19480] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 19500] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 19520] train loss: 1.5209, train acc: 0.2469, val loss: 1.5109, val acc: 0.2526  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 19540] train loss: 1.5185, train acc: 0.2371, val loss: 1.5186, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 19560] train loss: 1.5184, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 19580] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 19600] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 19620] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 19640] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 19660] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 19680] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 19700] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 19720] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 19740] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 19760] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 19780] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 19800] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 19820] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 19840] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 19860] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 19880] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 19900] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 19920] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 19940] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 19960] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 19980] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 20000] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 20020] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 20040] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 20060] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 20080] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 20100] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 20120] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 20140] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 20160] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 20180] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 20200] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 20220] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 20240] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 20260] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 20280] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 20300] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 20320] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 20340] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 20360] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 20380] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 20400] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 20420] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 20440] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 20460] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 20480] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 20500] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 20520] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 20540] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 20560] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 20580] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 20600] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 20620] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 20640] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 20660] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 20680] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 20700] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 20720] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 20740] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 20760] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 20780] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 20800] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 20820] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 20840] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 20860] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 20880] train loss: 1.5183, train acc: 0.2371, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 20900] train loss: 1.6053, train acc: 0.2337, val loss: 1.5185, val acc: 0.2371  (best train acc: 0.4067, best val acc: 0.5305)\n",
      "[Epoch: 20920] train loss: 1.2564, train acc: 0.4273, val loss: 0.9345, val acc: 0.5875  (best train acc: 0.4968, best val acc: 0.5993)\n",
      "[Epoch: 20940] train loss: 1.0837, train acc: 0.5169, val loss: 0.8587, val acc: 0.6843  (best train acc: 0.5195, best val acc: 0.6894)\n",
      "[Epoch: 20960] train loss: 1.1375, train acc: 0.4822, val loss: 0.8372, val acc: 0.6260  (best train acc: 0.5799, best val acc: 0.7484)\n",
      "[Epoch: 20980] train loss: 1.0394, train acc: 0.5391, val loss: 0.7767, val acc: 0.7676  (best train acc: 0.6032, best val acc: 0.7696)\n",
      "[Epoch: 21000] train loss: 0.9357, train acc: 0.5578, val loss: 0.7541, val acc: 0.7696  (best train acc: 0.6134, best val acc: 0.8020)\n",
      "[Epoch: 21020] train loss: 0.9150, train acc: 0.5868, val loss: 0.7451, val acc: 0.8098  (best train acc: 0.6134, best val acc: 0.8121)\n",
      "[Epoch: 21040] train loss: 0.8829, train acc: 0.5943, val loss: 0.7602, val acc: 0.8236  (best train acc: 0.6134, best val acc: 0.8236)\n",
      "[Epoch: 21060] train loss: 0.9025, train acc: 0.5690, val loss: 0.7407, val acc: 0.8128  (best train acc: 0.6134, best val acc: 0.8334)\n",
      "[Epoch: 21080] train loss: 0.9446, train acc: 0.5361, val loss: 0.7374, val acc: 0.7575  (best train acc: 0.6134, best val acc: 0.8337)\n",
      "[Epoch: 21100] train loss: 1.0015, train acc: 0.5149, val loss: 0.7467, val acc: 0.8003  (best train acc: 0.6156, best val acc: 0.8398)\n",
      "[Epoch: 21120] train loss: 0.9428, train acc: 0.5918, val loss: 0.7591, val acc: 0.8159  (best train acc: 0.6223, best val acc: 0.8398)\n",
      "[Epoch: 21140] train loss: 0.8625, train acc: 0.6246, val loss: 0.7885, val acc: 0.8138  (best train acc: 0.6246, best val acc: 0.8438)\n",
      "[Epoch: 21160] train loss: 1.0316, train acc: 0.4965, val loss: 0.7291, val acc: 0.7086  (best train acc: 0.6246, best val acc: 0.8438)\n",
      "[Epoch: 21180] train loss: 1.0004, train acc: 0.5403, val loss: 0.7572, val acc: 0.8179  (best train acc: 0.6246, best val acc: 0.8438)\n",
      "[Epoch: 21200] train loss: 0.9739, train acc: 0.5300, val loss: 0.7178, val acc: 0.8223  (best train acc: 0.6246, best val acc: 0.8442)\n",
      "[Epoch: 21220] train loss: 0.9941, train acc: 0.5497, val loss: 0.7225, val acc: 0.8344  (best train acc: 0.6319, best val acc: 0.8455)\n",
      "[Epoch: 21240] train loss: 0.9381, train acc: 0.5222, val loss: 0.7116, val acc: 0.7565  (best train acc: 0.6319, best val acc: 0.8455)\n",
      "[Epoch: 21260] train loss: 0.9874, train acc: 0.5505, val loss: 0.7383, val acc: 0.8233  (best train acc: 0.6577, best val acc: 0.8455)\n",
      "[Epoch: 21280] train loss: 0.9199, train acc: 0.5911, val loss: 0.6975, val acc: 0.8334  (best train acc: 0.6577, best val acc: 0.8455)\n",
      "[Epoch: 21300] train loss: 0.9498, train acc: 0.5902, val loss: 0.7008, val acc: 0.8337  (best train acc: 0.6577, best val acc: 0.8469)\n",
      "[Epoch: 21320] train loss: 0.8785, train acc: 0.5889, val loss: 0.7137, val acc: 0.8358  (best train acc: 0.6594, best val acc: 0.8469)\n",
      "[Epoch: 21340] train loss: 0.8786, train acc: 0.5970, val loss: 0.6921, val acc: 0.8411  (best train acc: 0.6594, best val acc: 0.8479)\n",
      "[Epoch: 21360] train loss: 0.8199, train acc: 0.6457, val loss: 0.6911, val acc: 0.8418  (best train acc: 0.6594, best val acc: 0.8479)\n",
      "[Epoch: 21380] train loss: 0.8025, train acc: 0.6283, val loss: 0.7028, val acc: 0.8438  (best train acc: 0.6594, best val acc: 0.8479)\n",
      "[Epoch: 21400] train loss: 0.9585, train acc: 0.5813, val loss: 0.6842, val acc: 0.8037  (best train acc: 0.6594, best val acc: 0.8486)\n",
      "[Epoch: 21420] train loss: 0.9632, train acc: 0.5483, val loss: 0.6990, val acc: 0.7953  (best train acc: 0.6736, best val acc: 0.8486)\n",
      "[Epoch: 21440] train loss: 0.8729, train acc: 0.6139, val loss: 0.6849, val acc: 0.8044  (best train acc: 0.6736, best val acc: 0.8486)\n",
      "[Epoch: 21460] train loss: 0.9256, train acc: 0.5720, val loss: 0.6775, val acc: 0.8078  (best train acc: 0.6736, best val acc: 0.8486)\n",
      "[Epoch: 21480] train loss: 0.8773, train acc: 0.6178, val loss: 0.7158, val acc: 0.7730  (best train acc: 0.6736, best val acc: 0.8486)\n",
      "[Epoch: 21500] train loss: 0.9617, train acc: 0.5462, val loss: 0.6848, val acc: 0.7929  (best train acc: 0.6736, best val acc: 0.8486)\n",
      "[Epoch: 21520] train loss: 0.9425, train acc: 0.5756, val loss: 0.6906, val acc: 0.8024  (best train acc: 0.6888, best val acc: 0.8486)\n",
      "[Epoch: 21540] train loss: 0.9214, train acc: 0.6071, val loss: 0.6877, val acc: 0.8189  (best train acc: 0.6888, best val acc: 0.8486)\n",
      "[Epoch: 21560] train loss: 0.8006, train acc: 0.6986, val loss: 0.7129, val acc: 0.7649  (best train acc: 0.6986, best val acc: 0.8486)\n",
      "[Epoch: 21580] train loss: 0.8623, train acc: 0.6393, val loss: 0.6862, val acc: 0.8047  (best train acc: 0.6986, best val acc: 0.8486)\n",
      "[Epoch: 21600] train loss: 0.9346, train acc: 0.5842, val loss: 0.7304, val acc: 0.7423  (best train acc: 0.6986, best val acc: 0.8486)\n",
      "[Epoch: 21620] train loss: 0.8032, train acc: 0.6781, val loss: 0.6911, val acc: 0.7946  (best train acc: 0.6986, best val acc: 0.8486)\n",
      "[Epoch: 21640] train loss: 0.8724, train acc: 0.6171, val loss: 0.6748, val acc: 0.8091  (best train acc: 0.6986, best val acc: 0.8486)\n",
      "[Epoch: 21660] train loss: 0.9179, train acc: 0.6090, val loss: 0.6752, val acc: 0.8135  (best train acc: 0.6986, best val acc: 0.8486)\n",
      "[Epoch: 21680] train loss: 0.7954, train acc: 0.6700, val loss: 0.6816, val acc: 0.8054  (best train acc: 0.7022, best val acc: 0.8486)\n",
      "[Epoch: 21700] train loss: 0.8128, train acc: 0.6616, val loss: 0.6811, val acc: 0.8084  (best train acc: 0.7022, best val acc: 0.8486)\n",
      "[Epoch: 21720] train loss: 0.8453, train acc: 0.6272, val loss: 0.6656, val acc: 0.8209  (best train acc: 0.7022, best val acc: 0.8486)\n",
      "[Epoch: 21740] train loss: 0.7964, train acc: 0.6598, val loss: 0.6657, val acc: 0.8212  (best train acc: 0.7022, best val acc: 0.8486)\n",
      "[Epoch: 21760] train loss: 0.8687, train acc: 0.6287, val loss: 0.6791, val acc: 0.8064  (best train acc: 0.7022, best val acc: 0.8486)\n",
      "[Epoch: 21780] train loss: 0.8982, train acc: 0.6272, val loss: 0.6711, val acc: 0.8121  (best train acc: 0.7022, best val acc: 0.8486)\n",
      "[Epoch: 21800] train loss: 0.9090, train acc: 0.5960, val loss: 0.6612, val acc: 0.8337  (best train acc: 0.7022, best val acc: 0.8486)\n",
      "[Epoch: 21820] train loss: 0.9121, train acc: 0.6015, val loss: 0.6814, val acc: 0.8115  (best train acc: 0.7022, best val acc: 0.8486)\n",
      "[Epoch: 21840] train loss: 0.8971, train acc: 0.6267, val loss: 0.6851, val acc: 0.8064  (best train acc: 0.7022, best val acc: 0.8486)\n",
      "[Epoch: 21860] train loss: 0.8613, train acc: 0.6330, val loss: 0.6548, val acc: 0.8324  (best train acc: 0.7022, best val acc: 0.8486)\n",
      "[Epoch: 21880] train loss: 0.9562, train acc: 0.5954, val loss: 0.6856, val acc: 0.8202  (best train acc: 0.7022, best val acc: 0.8486)\n",
      "[Epoch: 21900] train loss: 0.9119, train acc: 0.6220, val loss: 0.6710, val acc: 0.8250  (best train acc: 0.7022, best val acc: 0.8486)\n",
      "[Epoch: 21920] train loss: 0.8364, train acc: 0.6290, val loss: 0.6532, val acc: 0.8121  (best train acc: 0.7022, best val acc: 0.8486)\n",
      "[Epoch: 21940] train loss: 0.8637, train acc: 0.6518, val loss: 0.6643, val acc: 0.8415  (best train acc: 0.7022, best val acc: 0.8486)\n",
      "[Epoch: 21960] train loss: 0.9137, train acc: 0.5789, val loss: 0.6760, val acc: 0.8384  (best train acc: 0.7022, best val acc: 0.8486)\n",
      "[Epoch: 21980] train loss: 0.9127, train acc: 0.5730, val loss: 0.6637, val acc: 0.8428  (best train acc: 0.7212, best val acc: 0.8486)\n",
      "[Epoch: 22000] train loss: 0.8303, train acc: 0.6757, val loss: 0.6942, val acc: 0.7912  (best train acc: 0.7212, best val acc: 0.8486)\n",
      "[Epoch: 22020] train loss: 0.9360, train acc: 0.5910, val loss: 0.6777, val acc: 0.8445  (best train acc: 0.7212, best val acc: 0.8540)\n",
      "[Epoch: 22040] train loss: 0.8445, train acc: 0.6076, val loss: 0.6842, val acc: 0.7980  (best train acc: 0.7212, best val acc: 0.8540)\n",
      "[Epoch: 22060] train loss: 0.8378, train acc: 0.6165, val loss: 0.6913, val acc: 0.7936  (best train acc: 0.7212, best val acc: 0.8540)\n",
      "[Epoch: 22080] train loss: 0.8515, train acc: 0.6324, val loss: 0.6965, val acc: 0.8135  (best train acc: 0.7212, best val acc: 0.8540)\n",
      "[Epoch: 22100] train loss: 0.8163, train acc: 0.6862, val loss: 0.7185, val acc: 0.6563  (best train acc: 0.7216, best val acc: 0.8540)\n",
      "[Epoch: 22120] train loss: 0.7909, train acc: 0.6751, val loss: 0.6893, val acc: 0.8273  (best train acc: 0.7216, best val acc: 0.8540)\n",
      "[Epoch: 22140] train loss: 0.8338, train acc: 0.6610, val loss: 0.7104, val acc: 0.7757  (best train acc: 0.7216, best val acc: 0.8540)\n",
      "[Epoch: 22160] train loss: 0.8292, train acc: 0.6603, val loss: 0.7143, val acc: 0.6462  (best train acc: 0.7216, best val acc: 0.8540)\n",
      "[Epoch: 22180] train loss: 0.8420, train acc: 0.6585, val loss: 0.6935, val acc: 0.7841  (best train acc: 0.7216, best val acc: 0.8540)\n",
      "[Epoch: 22200] train loss: 0.7801, train acc: 0.7218, val loss: 0.7285, val acc: 0.6277  (best train acc: 0.7218, best val acc: 0.8540)\n",
      "[Epoch: 22220] train loss: 0.8616, train acc: 0.6276, val loss: 0.6745, val acc: 0.8398  (best train acc: 0.7300, best val acc: 0.8540)\n",
      "[Epoch: 22240] train loss: 0.8867, train acc: 0.6147, val loss: 0.7027, val acc: 0.6688  (best train acc: 0.7300, best val acc: 0.8540)\n",
      "[Epoch: 22260] train loss: 0.9006, train acc: 0.6040, val loss: 0.6789, val acc: 0.8351  (best train acc: 0.7300, best val acc: 0.8540)\n",
      "[Epoch: 22280] train loss: 0.8748, train acc: 0.6592, val loss: 0.7174, val acc: 0.6597  (best train acc: 0.7444, best val acc: 0.8540)\n",
      "[Epoch: 22300] train loss: 0.9098, train acc: 0.6118, val loss: 0.6969, val acc: 0.7008  (best train acc: 0.7444, best val acc: 0.8540)\n",
      "[Epoch: 22320] train loss: 0.8451, train acc: 0.6451, val loss: 0.6818, val acc: 0.8226  (best train acc: 0.7444, best val acc: 0.8540)\n",
      "[Epoch: 22340] train loss: 0.8887, train acc: 0.6122, val loss: 0.6989, val acc: 0.7403  (best train acc: 0.7444, best val acc: 0.8540)\n",
      "[Epoch: 22360] train loss: 0.9050, train acc: 0.6247, val loss: 0.6858, val acc: 0.7639  (best train acc: 0.7444, best val acc: 0.8540)\n",
      "[Epoch: 22380] train loss: 0.8988, train acc: 0.6092, val loss: 0.6994, val acc: 0.7241  (best train acc: 0.7444, best val acc: 0.8540)\n",
      "[Epoch: 22400] train loss: 0.8512, train acc: 0.6559, val loss: 0.6874, val acc: 0.7504  (best train acc: 0.7444, best val acc: 0.8540)\n",
      "[Epoch: 22420] train loss: 0.8467, train acc: 0.6154, val loss: 0.6965, val acc: 0.6455  (best train acc: 0.7444, best val acc: 0.8540)\n",
      "[Epoch: 22440] train loss: 0.9163, train acc: 0.6277, val loss: 0.6968, val acc: 0.7629  (best train acc: 0.7444, best val acc: 0.8540)\n",
      "[Epoch: 22460] train loss: 0.7809, train acc: 0.6646, val loss: 0.6900, val acc: 0.7632  (best train acc: 0.7444, best val acc: 0.8540)\n",
      "[Epoch: 22480] train loss: 0.8140, train acc: 0.6668, val loss: 0.6859, val acc: 0.7595  (best train acc: 0.7444, best val acc: 0.8540)\n",
      "[Epoch: 22500] train loss: 0.8218, train acc: 0.6704, val loss: 0.7390, val acc: 0.5798  (best train acc: 0.7444, best val acc: 0.8540)\n",
      "[Epoch: 22520] train loss: 0.8413, train acc: 0.6470, val loss: 0.6678, val acc: 0.7889  (best train acc: 0.7444, best val acc: 0.8540)\n",
      "[Epoch: 22540] train loss: 0.8720, train acc: 0.5928, val loss: 0.6618, val acc: 0.7710  (best train acc: 0.7444, best val acc: 0.8540)\n",
      "[Epoch: 22560] train loss: 0.8142, train acc: 0.6797, val loss: 0.7049, val acc: 0.6907  (best train acc: 0.7447, best val acc: 0.8540)\n",
      "[Epoch: 22580] train loss: 0.7984, train acc: 0.6767, val loss: 0.6818, val acc: 0.7497  (best train acc: 0.7447, best val acc: 0.8540)\n",
      "[Epoch: 22600] train loss: 0.8378, train acc: 0.6453, val loss: 0.6785, val acc: 0.7575  (best train acc: 0.7447, best val acc: 0.8540)\n",
      "[Epoch: 22620] train loss: 0.8238, train acc: 0.6548, val loss: 0.6707, val acc: 0.7919  (best train acc: 0.7447, best val acc: 0.8540)\n",
      "[Epoch: 22640] train loss: 0.8179, train acc: 0.6527, val loss: 0.6715, val acc: 0.7922  (best train acc: 0.7447, best val acc: 0.8540)\n",
      "[Epoch: 22660] train loss: 0.8353, train acc: 0.6329, val loss: 0.6898, val acc: 0.6708  (best train acc: 0.7483, best val acc: 0.8540)\n",
      "[Epoch: 22680] train loss: 0.7505, train acc: 0.7009, val loss: 0.6945, val acc: 0.6337  (best train acc: 0.7483, best val acc: 0.8540)\n",
      "[Epoch: 22700] train loss: 0.8790, train acc: 0.6259, val loss: 0.6599, val acc: 0.7976  (best train acc: 0.7483, best val acc: 0.8540)\n",
      "[Epoch: 22720] train loss: 0.8930, train acc: 0.6366, val loss: 0.6652, val acc: 0.7845  (best train acc: 0.7483, best val acc: 0.8540)\n",
      "[Epoch: 22740] train loss: 0.8326, train acc: 0.6592, val loss: 0.6771, val acc: 0.7565  (best train acc: 0.7483, best val acc: 0.8540)\n",
      "[Epoch: 22760] train loss: 0.8530, train acc: 0.6204, val loss: 0.6412, val acc: 0.8064  (best train acc: 0.7483, best val acc: 0.8540)\n",
      "[Epoch: 22780] train loss: 0.8416, train acc: 0.6597, val loss: 0.6962, val acc: 0.6226  (best train acc: 0.7483, best val acc: 0.8540)\n",
      "[Epoch: 22800] train loss: 0.8248, train acc: 0.6515, val loss: 0.6770, val acc: 0.7501  (best train acc: 0.7483, best val acc: 0.8540)\n",
      "[Epoch: 22820] train loss: 0.8030, train acc: 0.7141, val loss: 0.7179, val acc: 0.5821  (best train acc: 0.7483, best val acc: 0.8540)\n",
      "[Epoch: 22840] train loss: 0.9512, train acc: 0.6280, val loss: 0.7048, val acc: 0.6000  (best train acc: 0.7483, best val acc: 0.8540)\n",
      "[Epoch: 22860] train loss: 0.8184, train acc: 0.6739, val loss: 0.6626, val acc: 0.7734  (best train acc: 0.7540, best val acc: 0.8540)\n",
      "[Epoch: 22880] train loss: 0.8446, train acc: 0.6627, val loss: 0.6685, val acc: 0.7720  (best train acc: 0.7540, best val acc: 0.8540)\n",
      "[Epoch: 22900] train loss: 0.8658, train acc: 0.6422, val loss: 0.6636, val acc: 0.7616  (best train acc: 0.7540, best val acc: 0.8540)\n",
      "[Epoch: 22920] train loss: 0.8542, train acc: 0.6195, val loss: 0.6428, val acc: 0.8145  (best train acc: 0.7709, best val acc: 0.8540)\n",
      "[Epoch: 22940] train loss: 0.8118, train acc: 0.6605, val loss: 0.6668, val acc: 0.7514  (best train acc: 0.7709, best val acc: 0.8540)\n",
      "[Epoch: 22960] train loss: 0.7640, train acc: 0.6905, val loss: 0.7058, val acc: 0.5949  (best train acc: 0.7709, best val acc: 0.8540)\n",
      "[Epoch: 22980] train loss: 0.7762, train acc: 0.7131, val loss: 0.6654, val acc: 0.7599  (best train acc: 0.7709, best val acc: 0.8540)\n",
      "[Epoch: 23000] train loss: 0.7719, train acc: 0.6767, val loss: 0.6443, val acc: 0.8024  (best train acc: 0.7709, best val acc: 0.8540)\n",
      "[Epoch: 23020] train loss: 0.8965, train acc: 0.6436, val loss: 0.6623, val acc: 0.7680  (best train acc: 0.7709, best val acc: 0.8540)\n",
      "[Epoch: 23040] train loss: 0.7878, train acc: 0.6966, val loss: 0.6844, val acc: 0.6199  (best train acc: 0.7709, best val acc: 0.8540)\n",
      "[Epoch: 23060] train loss: 0.7531, train acc: 0.6846, val loss: 0.6377, val acc: 0.8054  (best train acc: 0.7709, best val acc: 0.8540)\n",
      "[Epoch: 23080] train loss: 0.8342, train acc: 0.6858, val loss: 0.6748, val acc: 0.7201  (best train acc: 0.7709, best val acc: 0.8540)\n",
      "[Epoch: 23100] train loss: 0.7188, train acc: 0.7217, val loss: 0.6508, val acc: 0.7916  (best train acc: 0.7709, best val acc: 0.8540)\n",
      "[Epoch: 23120] train loss: 0.9029, train acc: 0.6413, val loss: 0.6610, val acc: 0.7531  (best train acc: 0.7709, best val acc: 0.8540)\n",
      "[Epoch: 23140] train loss: 0.9086, train acc: 0.6445, val loss: 0.6557, val acc: 0.7484  (best train acc: 0.7709, best val acc: 0.8540)\n",
      "[Epoch: 23160] train loss: 0.9325, train acc: 0.6263, val loss: 0.6510, val acc: 0.7278  (best train acc: 0.7709, best val acc: 0.8540)\n",
      "[Epoch: 23180] train loss: 0.6462, train acc: 0.7803, val loss: 0.5618, val acc: 0.8071  (best train acc: 0.7813, best val acc: 0.8540)\n",
      "[Epoch: 23200] train loss: 0.7322, train acc: 0.7123, val loss: 0.5242, val acc: 0.8371  (best train acc: 0.8068, best val acc: 0.8540)\n",
      "[Epoch: 23220] train loss: 0.6175, train acc: 0.7712, val loss: 0.5326, val acc: 0.8570  (best train acc: 0.8068, best val acc: 0.8617)\n",
      "[Epoch: 23240] train loss: 0.7539, train acc: 0.7027, val loss: 0.5158, val acc: 0.8300  (best train acc: 0.8068, best val acc: 0.8617)\n",
      "[Epoch: 23260] train loss: 0.8710, train acc: 0.6496, val loss: 0.5262, val acc: 0.8432  (best train acc: 0.8068, best val acc: 0.8617)\n",
      "[Epoch: 23280] train loss: 0.6187, train acc: 0.7930, val loss: 0.5417, val acc: 0.8199  (best train acc: 0.8068, best val acc: 0.8661)\n",
      "[Epoch: 23300] train loss: 0.7049, train acc: 0.7438, val loss: 0.5149, val acc: 0.8344  (best train acc: 0.8068, best val acc: 0.8661)\n",
      "[Epoch: 23320] train loss: 0.6240, train acc: 0.7676, val loss: 0.4941, val acc: 0.8509  (best train acc: 0.8068, best val acc: 0.8661)\n",
      "[Epoch: 23340] train loss: 0.6227, train acc: 0.8104, val loss: 0.5251, val acc: 0.8540  (best train acc: 0.8104, best val acc: 0.8661)\n",
      "[Epoch: 23360] train loss: 0.7297, train acc: 0.7336, val loss: 0.4883, val acc: 0.8580  (best train acc: 0.8104, best val acc: 0.8661)\n",
      "[Epoch: 23380] train loss: 0.8321, train acc: 0.6878, val loss: 0.5001, val acc: 0.8583  (best train acc: 0.8142, best val acc: 0.8661)\n",
      "[Epoch: 23400] train loss: 0.6295, train acc: 0.7827, val loss: 0.5069, val acc: 0.8499  (best train acc: 0.8142, best val acc: 0.8661)\n",
      "[Epoch: 23420] train loss: 0.7252, train acc: 0.7450, val loss: 0.5036, val acc: 0.8513  (best train acc: 0.8142, best val acc: 0.8661)\n",
      "[Epoch: 23440] train loss: 0.6790, train acc: 0.7452, val loss: 0.4916, val acc: 0.8614  (best train acc: 0.8142, best val acc: 0.8661)\n",
      "[Epoch: 23460] train loss: 0.5822, train acc: 0.7867, val loss: 0.5113, val acc: 0.8563  (best train acc: 0.8142, best val acc: 0.8661)\n",
      "[Epoch: 23480] train loss: 0.5864, train acc: 0.7967, val loss: 0.5053, val acc: 0.8567  (best train acc: 0.8167, best val acc: 0.8661)\n",
      "[Epoch: 23500] train loss: 0.6278, train acc: 0.7669, val loss: 0.4797, val acc: 0.8526  (best train acc: 0.8219, best val acc: 0.8681)\n",
      "[Epoch: 23520] train loss: 0.6025, train acc: 0.7933, val loss: 0.4937, val acc: 0.8395  (best train acc: 0.8219, best val acc: 0.8681)\n",
      "[Epoch: 23540] train loss: 0.6698, train acc: 0.7332, val loss: 0.4872, val acc: 0.8381  (best train acc: 0.8219, best val acc: 0.8681)\n",
      "[Epoch: 23560] train loss: 0.6245, train acc: 0.7615, val loss: 0.4827, val acc: 0.8550  (best train acc: 0.8219, best val acc: 0.8681)\n",
      "[Epoch: 23580] train loss: 0.6222, train acc: 0.7718, val loss: 0.4773, val acc: 0.8479  (best train acc: 0.8219, best val acc: 0.8728)\n",
      "[Epoch: 23600] train loss: 0.5547, train acc: 0.8006, val loss: 0.4795, val acc: 0.8492  (best train acc: 0.8266, best val acc: 0.8728)\n",
      "[Epoch: 23620] train loss: 0.6982, train acc: 0.7057, val loss: 0.4864, val acc: 0.8496  (best train acc: 0.8266, best val acc: 0.8728)\n",
      "[Epoch: 23640] train loss: 0.6218, train acc: 0.7676, val loss: 0.4940, val acc: 0.8546  (best train acc: 0.8280, best val acc: 0.8759)\n",
      "[Epoch: 23660] train loss: 0.5695, train acc: 0.7857, val loss: 0.4881, val acc: 0.8462  (best train acc: 0.8280, best val acc: 0.8759)\n",
      "[Epoch: 23680] train loss: 0.6494, train acc: 0.7715, val loss: 0.4817, val acc: 0.8604  (best train acc: 0.8427, best val acc: 0.8759)\n",
      "[Epoch: 23700] train loss: 0.5044, train acc: 0.8216, val loss: 0.4625, val acc: 0.8681  (best train acc: 0.8427, best val acc: 0.8759)\n",
      "[Epoch: 23720] train loss: 0.5397, train acc: 0.8111, val loss: 0.4754, val acc: 0.8550  (best train acc: 0.8427, best val acc: 0.8759)\n",
      "[Epoch: 23740] train loss: 0.6351, train acc: 0.7775, val loss: 0.4604, val acc: 0.8688  (best train acc: 0.8427, best val acc: 0.8759)\n",
      "[Epoch: 23760] train loss: 0.5380, train acc: 0.8250, val loss: 0.5019, val acc: 0.8513  (best train acc: 0.8427, best val acc: 0.8759)\n",
      "[Epoch: 23780] train loss: 0.7825, train acc: 0.7038, val loss: 0.4800, val acc: 0.8715  (best train acc: 0.8427, best val acc: 0.8759)\n",
      "[Epoch: 23800] train loss: 0.6389, train acc: 0.7853, val loss: 0.4780, val acc: 0.8499  (best train acc: 0.8427, best val acc: 0.8759)\n",
      "[Epoch: 23820] train loss: 0.5701, train acc: 0.7925, val loss: 0.4971, val acc: 0.8361  (best train acc: 0.8427, best val acc: 0.8759)\n",
      "[Epoch: 23840] train loss: 0.5349, train acc: 0.8185, val loss: 0.4660, val acc: 0.8492  (best train acc: 0.8427, best val acc: 0.8823)\n",
      "[Epoch: 23860] train loss: 0.4936, train acc: 0.8321, val loss: 0.4673, val acc: 0.8718  (best train acc: 0.8427, best val acc: 0.8823)\n",
      "[Epoch: 23880] train loss: 0.6497, train acc: 0.7762, val loss: 0.4583, val acc: 0.8695  (best train acc: 0.8427, best val acc: 0.8823)\n",
      "[Epoch: 23900] train loss: 0.6365, train acc: 0.7650, val loss: 0.4665, val acc: 0.8702  (best train acc: 0.8427, best val acc: 0.8823)\n",
      "[Epoch: 23920] train loss: 0.5485, train acc: 0.8065, val loss: 0.4740, val acc: 0.8594  (best train acc: 0.8427, best val acc: 0.8823)\n",
      "[Epoch: 23940] train loss: 0.5808, train acc: 0.7788, val loss: 0.4695, val acc: 0.8722  (best train acc: 0.8427, best val acc: 0.8823)\n",
      "[Epoch: 23960] train loss: 0.5493, train acc: 0.8117, val loss: 0.4555, val acc: 0.8718  (best train acc: 0.8427, best val acc: 0.8823)\n",
      "[Epoch: 23980] train loss: 0.6372, train acc: 0.7551, val loss: 0.4737, val acc: 0.8708  (best train acc: 0.8427, best val acc: 0.8823)\n",
      "[Epoch: 24000] train loss: 0.5844, train acc: 0.7856, val loss: 0.4519, val acc: 0.8577  (best train acc: 0.8427, best val acc: 0.8823)\n",
      "[Epoch: 24020] train loss: 0.5231, train acc: 0.8222, val loss: 0.4325, val acc: 0.8691  (best train acc: 0.8427, best val acc: 0.8823)\n",
      "[Epoch: 24040] train loss: 0.6254, train acc: 0.7642, val loss: 0.4324, val acc: 0.8759  (best train acc: 0.8427, best val acc: 0.8823)\n",
      "[Epoch: 24060] train loss: 0.7769, train acc: 0.7066, val loss: 0.4336, val acc: 0.8796  (best train acc: 0.8427, best val acc: 0.8823)\n",
      "[Epoch: 24080] train loss: 0.5893, train acc: 0.7839, val loss: 0.4406, val acc: 0.8685  (best train acc: 0.8427, best val acc: 0.8823)\n",
      "[Epoch: 24100] train loss: 0.5418, train acc: 0.8175, val loss: 0.4477, val acc: 0.8621  (best train acc: 0.8427, best val acc: 0.8823)\n",
      "[Epoch: 24120] train loss: 0.5305, train acc: 0.8086, val loss: 0.4384, val acc: 0.8718  (best train acc: 0.8427, best val acc: 0.8823)\n",
      "[Epoch: 24140] train loss: 0.6424, train acc: 0.7760, val loss: 0.4448, val acc: 0.8651  (best train acc: 0.8427, best val acc: 0.8823)\n",
      "[Epoch: 24160] train loss: 0.6689, train acc: 0.7625, val loss: 0.4358, val acc: 0.8705  (best train acc: 0.8427, best val acc: 0.8823)\n",
      "[Epoch: 24180] train loss: 0.6213, train acc: 0.7765, val loss: 0.4440, val acc: 0.8702  (best train acc: 0.8427, best val acc: 0.8823)\n",
      "[Epoch: 24200] train loss: 0.5884, train acc: 0.7732, val loss: 0.4652, val acc: 0.8452  (best train acc: 0.8427, best val acc: 0.8823)\n",
      "[Epoch: 24220] train loss: 0.5651, train acc: 0.7843, val loss: 0.4517, val acc: 0.8722  (best train acc: 0.8427, best val acc: 0.8823)\n",
      "[Epoch: 24240] train loss: 0.5570, train acc: 0.8156, val loss: 0.4672, val acc: 0.8503  (best train acc: 0.8496, best val acc: 0.8823)\n",
      "[Epoch: 24260] train loss: 0.5509, train acc: 0.8073, val loss: 0.4505, val acc: 0.8627  (best train acc: 0.8496, best val acc: 0.8823)\n",
      "[Epoch: 24280] train loss: 0.5379, train acc: 0.8211, val loss: 0.4441, val acc: 0.8583  (best train acc: 0.8496, best val acc: 0.8823)\n",
      "[Epoch: 24300] train loss: 0.5320, train acc: 0.7967, val loss: 0.4376, val acc: 0.8614  (best train acc: 0.8496, best val acc: 0.8823)\n",
      "[Epoch: 24320] train loss: 0.5192, train acc: 0.8193, val loss: 0.4715, val acc: 0.8550  (best train acc: 0.8496, best val acc: 0.8823)\n",
      "[Epoch: 24340] train loss: 0.5129, train acc: 0.8206, val loss: 0.4223, val acc: 0.8648  (best train acc: 0.8496, best val acc: 0.8823)\n",
      "[Epoch: 24360] train loss: 0.5557, train acc: 0.7997, val loss: 0.4341, val acc: 0.8725  (best train acc: 0.8496, best val acc: 0.8823)\n",
      "[Epoch: 24380] train loss: 0.6368, train acc: 0.7752, val loss: 0.4389, val acc: 0.8637  (best train acc: 0.8496, best val acc: 0.8823)\n",
      "[Epoch: 24400] train loss: 0.5757, train acc: 0.7921, val loss: 0.4259, val acc: 0.8776  (best train acc: 0.8496, best val acc: 0.8823)\n",
      "[Epoch: 24420] train loss: 0.5570, train acc: 0.8016, val loss: 0.4532, val acc: 0.8651  (best train acc: 0.8496, best val acc: 0.8823)\n",
      "[Epoch: 24440] train loss: 0.4791, train acc: 0.8344, val loss: 0.4554, val acc: 0.8401  (best train acc: 0.8496, best val acc: 0.8823)\n",
      "[Epoch: 24460] train loss: 0.5733, train acc: 0.7911, val loss: 0.4530, val acc: 0.8739  (best train acc: 0.8496, best val acc: 0.8823)\n",
      "[Epoch: 24480] train loss: 0.5574, train acc: 0.7927, val loss: 0.4441, val acc: 0.8627  (best train acc: 0.8496, best val acc: 0.8823)\n",
      "[Epoch: 24500] train loss: 0.4919, train acc: 0.8305, val loss: 0.4787, val acc: 0.8513  (best train acc: 0.8496, best val acc: 0.8823)\n",
      "[Epoch: 24520] train loss: 0.7770, train acc: 0.6964, val loss: 0.4233, val acc: 0.8752  (best train acc: 0.8496, best val acc: 0.8823)\n",
      "[Epoch: 24540] train loss: 0.5598, train acc: 0.7813, val loss: 0.4300, val acc: 0.8715  (best train acc: 0.8496, best val acc: 0.8823)\n",
      "[Epoch: 24560] train loss: 0.5140, train acc: 0.8263, val loss: 0.4677, val acc: 0.8587  (best train acc: 0.8496, best val acc: 0.8823)\n",
      "[Epoch: 24580] train loss: 0.6334, train acc: 0.7634, val loss: 0.4267, val acc: 0.8708  (best train acc: 0.8496, best val acc: 0.8823)\n",
      "[Epoch: 24600] train loss: 0.5332, train acc: 0.8167, val loss: 0.4311, val acc: 0.8607  (best train acc: 0.8496, best val acc: 0.8823)\n",
      "[Epoch: 24620] train loss: 0.5551, train acc: 0.8091, val loss: 0.4280, val acc: 0.8678  (best train acc: 0.8496, best val acc: 0.8823)\n",
      "[Epoch: 24640] train loss: 0.5288, train acc: 0.8009, val loss: 0.4778, val acc: 0.8476  (best train acc: 0.8496, best val acc: 0.8823)\n",
      "[Epoch: 24660] train loss: 0.5197, train acc: 0.8075, val loss: 0.4080, val acc: 0.8779  (best train acc: 0.8496, best val acc: 0.8823)\n",
      "[Epoch: 24680] train loss: 0.5994, train acc: 0.7663, val loss: 0.4217, val acc: 0.8621  (best train acc: 0.8496, best val acc: 0.8823)\n",
      "[Epoch: 24700] train loss: 0.6158, train acc: 0.7716, val loss: 0.4607, val acc: 0.8725  (best train acc: 0.8508, best val acc: 0.8823)\n",
      "[Epoch: 24720] train loss: 0.5850, train acc: 0.7772, val loss: 0.4472, val acc: 0.8732  (best train acc: 0.8508, best val acc: 0.8823)\n",
      "[Epoch: 24740] train loss: 0.4877, train acc: 0.8276, val loss: 0.4422, val acc: 0.8415  (best train acc: 0.8508, best val acc: 0.8823)\n",
      "[Epoch: 24760] train loss: 0.5517, train acc: 0.8051, val loss: 0.4362, val acc: 0.8651  (best train acc: 0.8508, best val acc: 0.8823)\n",
      "[Epoch: 24780] train loss: 0.5947, train acc: 0.7817, val loss: 0.4113, val acc: 0.8809  (best train acc: 0.8508, best val acc: 0.8823)\n",
      "[Epoch: 24800] train loss: 0.5791, train acc: 0.7934, val loss: 0.4299, val acc: 0.8735  (best train acc: 0.8508, best val acc: 0.8823)\n",
      "[Epoch: 24820] train loss: 0.5163, train acc: 0.8229, val loss: 0.4400, val acc: 0.8718  (best train acc: 0.8508, best val acc: 0.8823)\n",
      "[Epoch: 24840] train loss: 0.6454, train acc: 0.7405, val loss: 0.3973, val acc: 0.8786  (best train acc: 0.8533, best val acc: 0.8823)\n",
      "[Epoch: 24860] train loss: 0.5178, train acc: 0.7992, val loss: 0.4314, val acc: 0.8691  (best train acc: 0.8533, best val acc: 0.8823)\n",
      "[Epoch: 24880] train loss: 0.5283, train acc: 0.8115, val loss: 0.4464, val acc: 0.8664  (best train acc: 0.8533, best val acc: 0.8823)\n",
      "[Epoch: 24900] train loss: 0.5049, train acc: 0.8321, val loss: 0.4546, val acc: 0.8516  (best train acc: 0.8533, best val acc: 0.8823)\n",
      "[Epoch: 24920] train loss: 0.5407, train acc: 0.8020, val loss: 0.4483, val acc: 0.8499  (best train acc: 0.8533, best val acc: 0.8823)\n",
      "[Epoch: 24940] train loss: 0.6773, train acc: 0.7583, val loss: 0.4316, val acc: 0.8671  (best train acc: 0.8533, best val acc: 0.8823)\n",
      "[Epoch: 24960] train loss: 0.5974, train acc: 0.7606, val loss: 0.4166, val acc: 0.8712  (best train acc: 0.8533, best val acc: 0.8823)\n",
      "[Epoch: 24980] train loss: 0.5039, train acc: 0.8235, val loss: 0.4163, val acc: 0.8762  (best train acc: 0.8533, best val acc: 0.8823)\n",
      "[Epoch: 25000] train loss: 0.5280, train acc: 0.8190, val loss: 0.4841, val acc: 0.8476  (best train acc: 0.8533, best val acc: 0.8823)\n",
      "[Epoch: 25020] train loss: 0.6412, train acc: 0.7630, val loss: 0.4402, val acc: 0.8681  (best train acc: 0.8533, best val acc: 0.8823)\n",
      "[Epoch: 25040] train loss: 0.4995, train acc: 0.8278, val loss: 0.4267, val acc: 0.8728  (best train acc: 0.8533, best val acc: 0.8823)\n",
      "[Epoch: 25060] train loss: 0.6020, train acc: 0.7853, val loss: 0.4086, val acc: 0.8732  (best train acc: 0.8533, best val acc: 0.8823)\n",
      "[Epoch: 25080] train loss: 0.6221, train acc: 0.7825, val loss: 0.4495, val acc: 0.8671  (best train acc: 0.8533, best val acc: 0.8823)\n",
      "[Epoch: 25100] train loss: 0.6327, train acc: 0.7809, val loss: 0.3950, val acc: 0.8786  (best train acc: 0.8533, best val acc: 0.8823)\n",
      "[Epoch: 25120] train loss: 0.6271, train acc: 0.7701, val loss: 0.4259, val acc: 0.8752  (best train acc: 0.8533, best val acc: 0.8823)\n",
      "[Epoch: 25140] train loss: 0.7924, train acc: 0.7063, val loss: 0.3978, val acc: 0.8806  (best train acc: 0.8533, best val acc: 0.8823)\n",
      "[Epoch: 25160] train loss: 0.5183, train acc: 0.8328, val loss: 0.4128, val acc: 0.8728  (best train acc: 0.8533, best val acc: 0.8830)\n",
      "[Epoch: 25180] train loss: 0.6221, train acc: 0.7636, val loss: 0.4000, val acc: 0.8779  (best train acc: 0.8533, best val acc: 0.8830)\n",
      "[Epoch: 25200] train loss: 0.5106, train acc: 0.8310, val loss: 0.4399, val acc: 0.8749  (best train acc: 0.8533, best val acc: 0.8830)\n",
      "[Epoch: 25220] train loss: 0.6230, train acc: 0.7553, val loss: 0.4356, val acc: 0.8718  (best train acc: 0.8533, best val acc: 0.8830)\n",
      "[Epoch: 25240] train loss: 0.6498, train acc: 0.7501, val loss: 0.4097, val acc: 0.8715  (best train acc: 0.8533, best val acc: 0.8830)\n",
      "[Epoch: 25260] train loss: 0.5048, train acc: 0.8313, val loss: 0.4295, val acc: 0.8691  (best train acc: 0.8548, best val acc: 0.8830)\n",
      "[Epoch: 25280] train loss: 0.5270, train acc: 0.8133, val loss: 0.4320, val acc: 0.8506  (best train acc: 0.8548, best val acc: 0.8840)\n",
      "[Epoch: 25300] train loss: 0.5381, train acc: 0.8120, val loss: 0.4534, val acc: 0.8661  (best train acc: 0.8548, best val acc: 0.8840)\n",
      "[Epoch: 25320] train loss: 0.6325, train acc: 0.7714, val loss: 0.4243, val acc: 0.8772  (best train acc: 0.8548, best val acc: 0.8840)\n",
      "[Epoch: 25340] train loss: 0.5995, train acc: 0.7668, val loss: 0.3932, val acc: 0.8789  (best train acc: 0.8548, best val acc: 0.8840)\n",
      "[Epoch: 25360] train loss: 0.5902, train acc: 0.7835, val loss: 0.4162, val acc: 0.8745  (best train acc: 0.8548, best val acc: 0.8840)\n",
      "[Epoch: 25380] train loss: 0.5213, train acc: 0.8094, val loss: 0.3945, val acc: 0.8762  (best train acc: 0.8548, best val acc: 0.8840)\n",
      "[Epoch: 25400] train loss: 0.5355, train acc: 0.8195, val loss: 0.4119, val acc: 0.8755  (best train acc: 0.8548, best val acc: 0.8840)\n",
      "[Epoch: 25420] train loss: 0.5064, train acc: 0.8251, val loss: 0.4558, val acc: 0.8513  (best train acc: 0.8548, best val acc: 0.8840)\n",
      "[Epoch: 25440] train loss: 0.5143, train acc: 0.8352, val loss: 0.4051, val acc: 0.8735  (best train acc: 0.8548, best val acc: 0.8840)\n",
      "[Epoch: 25460] train loss: 0.6126, train acc: 0.7769, val loss: 0.4098, val acc: 0.8675  (best train acc: 0.8548, best val acc: 0.8840)\n",
      "[Epoch: 25480] train loss: 0.5677, train acc: 0.8060, val loss: 0.4520, val acc: 0.8556  (best train acc: 0.8548, best val acc: 0.8840)\n",
      "[Epoch: 25500] train loss: 0.4584, train acc: 0.8469, val loss: 0.4227, val acc: 0.8526  (best train acc: 0.8548, best val acc: 0.8840)\n",
      "[Epoch: 25520] train loss: 0.5104, train acc: 0.8157, val loss: 0.4538, val acc: 0.8556  (best train acc: 0.8548, best val acc: 0.8840)\n",
      "[Epoch: 25540] train loss: 0.6353, train acc: 0.7599, val loss: 0.4189, val acc: 0.8752  (best train acc: 0.8548, best val acc: 0.8840)\n",
      "[Epoch: 25560] train loss: 0.5153, train acc: 0.8306, val loss: 0.4109, val acc: 0.8749  (best train acc: 0.8548, best val acc: 0.8840)\n",
      "[Epoch: 25580] train loss: 0.5540, train acc: 0.8163, val loss: 0.4328, val acc: 0.8691  (best train acc: 0.8548, best val acc: 0.8840)\n",
      "[Epoch: 25600] train loss: 0.7531, train acc: 0.7013, val loss: 0.3954, val acc: 0.8779  (best train acc: 0.8548, best val acc: 0.8840)\n",
      "[Epoch: 25620] train loss: 0.6775, train acc: 0.7473, val loss: 0.4096, val acc: 0.8755  (best train acc: 0.8548, best val acc: 0.8840)\n",
      "[Epoch: 25640] train loss: 0.5606, train acc: 0.7968, val loss: 0.4090, val acc: 0.8732  (best train acc: 0.8548, best val acc: 0.8840)\n",
      "[Epoch: 25660] train loss: 0.7593, train acc: 0.7070, val loss: 0.4108, val acc: 0.8749  (best train acc: 0.8548, best val acc: 0.8840)\n",
      "[Epoch: 25680] train loss: 0.5229, train acc: 0.8123, val loss: 0.3959, val acc: 0.8772  (best train acc: 0.8548, best val acc: 0.8840)\n",
      "[Epoch: 25700] train loss: 0.6166, train acc: 0.7731, val loss: 0.3972, val acc: 0.8776  (best train acc: 0.8548, best val acc: 0.8840)\n",
      "[Epoch: 25720] train loss: 0.6294, train acc: 0.7722, val loss: 0.4288, val acc: 0.8739  (best train acc: 0.8548, best val acc: 0.8840)\n",
      "[Epoch: 25740] train loss: 0.5576, train acc: 0.8036, val loss: 0.4406, val acc: 0.8752  (best train acc: 0.8548, best val acc: 0.8840)\n",
      "[Epoch: 25760] train loss: 0.5185, train acc: 0.8300, val loss: 0.4033, val acc: 0.8681  (best train acc: 0.8548, best val acc: 0.8840)\n",
      "[Epoch: 25780] train loss: 0.5642, train acc: 0.7940, val loss: 0.4035, val acc: 0.8752  (best train acc: 0.8548, best val acc: 0.8840)\n",
      "[Epoch: 25800] train loss: 0.5091, train acc: 0.8239, val loss: 0.4476, val acc: 0.8627  (best train acc: 0.8548, best val acc: 0.8840)\n",
      "[Epoch: 25820] train loss: 0.7285, train acc: 0.7223, val loss: 0.4315, val acc: 0.8786  (best train acc: 0.8548, best val acc: 0.8840)\n",
      "[Epoch: 25840] train loss: 0.6121, train acc: 0.7626, val loss: 0.3993, val acc: 0.8489  (best train acc: 0.8548, best val acc: 0.8840)\n",
      "[Epoch: 25860] train loss: 0.4618, train acc: 0.8414, val loss: 0.4347, val acc: 0.8634  (best train acc: 0.8548, best val acc: 0.8840)\n",
      "[Epoch: 25880] train loss: 0.5129, train acc: 0.8162, val loss: 0.4153, val acc: 0.8752  (best train acc: 0.8548, best val acc: 0.8840)\n",
      "[Epoch: 25900] train loss: 0.6693, train acc: 0.7412, val loss: 0.3846, val acc: 0.8796  (best train acc: 0.8548, best val acc: 0.8840)\n",
      "[Epoch: 25920] train loss: 0.5032, train acc: 0.8334, val loss: 0.4021, val acc: 0.8712  (best train acc: 0.8548, best val acc: 0.8840)\n",
      "[Epoch: 25940] train loss: 0.5341, train acc: 0.7992, val loss: 0.4167, val acc: 0.8749  (best train acc: 0.8548, best val acc: 0.8840)\n",
      "[Epoch: 25960] train loss: 0.7370, train acc: 0.7123, val loss: 0.4299, val acc: 0.8762  (best train acc: 0.8548, best val acc: 0.8840)\n",
      "[Epoch: 25980] train loss: 0.7725, train acc: 0.7172, val loss: 0.3924, val acc: 0.8627  (best train acc: 0.8548, best val acc: 0.8840)\n",
      "[Epoch: 26000] train loss: 0.6268, train acc: 0.7683, val loss: 0.3881, val acc: 0.8752  (best train acc: 0.8548, best val acc: 0.8840)\n",
      "[Epoch: 26020] train loss: 0.6587, train acc: 0.7446, val loss: 0.4278, val acc: 0.8769  (best train acc: 0.8554, best val acc: 0.8840)\n",
      "[Epoch: 26040] train loss: 0.6711, train acc: 0.7494, val loss: 0.4250, val acc: 0.8759  (best train acc: 0.8554, best val acc: 0.8840)\n",
      "[Epoch: 26060] train loss: 0.6028, train acc: 0.7792, val loss: 0.3888, val acc: 0.8823  (best train acc: 0.8554, best val acc: 0.8840)\n",
      "[Epoch: 26080] train loss: 0.5013, train acc: 0.8307, val loss: 0.4106, val acc: 0.8705  (best train acc: 0.8554, best val acc: 0.8840)\n",
      "[Epoch: 26100] train loss: 0.4997, train acc: 0.8367, val loss: 0.4228, val acc: 0.8681  (best train acc: 0.8554, best val acc: 0.8840)\n",
      "[Epoch: 26120] train loss: 0.4876, train acc: 0.8234, val loss: 0.4039, val acc: 0.8752  (best train acc: 0.8554, best val acc: 0.8840)\n",
      "[Epoch: 26140] train loss: 0.5619, train acc: 0.8156, val loss: 0.4423, val acc: 0.8600  (best train acc: 0.8554, best val acc: 0.8840)\n",
      "[Epoch: 26160] train loss: 0.4989, train acc: 0.8061, val loss: 0.4487, val acc: 0.8583  (best train acc: 0.8554, best val acc: 0.8840)\n",
      "[Epoch: 26180] train loss: 0.5588, train acc: 0.7964, val loss: 0.4080, val acc: 0.8786  (best train acc: 0.8554, best val acc: 0.8840)\n",
      "[Epoch: 26200] train loss: 0.5464, train acc: 0.8086, val loss: 0.4057, val acc: 0.8702  (best train acc: 0.8554, best val acc: 0.8840)\n",
      "[Epoch: 26220] train loss: 0.6262, train acc: 0.7611, val loss: 0.3885, val acc: 0.8739  (best train acc: 0.8554, best val acc: 0.8840)\n",
      "[Epoch: 26240] train loss: 0.6451, train acc: 0.7612, val loss: 0.4311, val acc: 0.8708  (best train acc: 0.8554, best val acc: 0.8840)\n",
      "[Epoch: 26260] train loss: 0.4723, train acc: 0.8411, val loss: 0.4241, val acc: 0.8766  (best train acc: 0.8554, best val acc: 0.8840)\n",
      "[Epoch: 26280] train loss: 0.6102, train acc: 0.7783, val loss: 0.3750, val acc: 0.8668  (best train acc: 0.8554, best val acc: 0.8840)\n",
      "[Epoch: 26300] train loss: 0.6463, train acc: 0.7701, val loss: 0.4163, val acc: 0.8752  (best train acc: 0.8554, best val acc: 0.8840)\n",
      "[Epoch: 26320] train loss: 0.5670, train acc: 0.7974, val loss: 0.4276, val acc: 0.8708  (best train acc: 0.8554, best val acc: 0.8840)\n",
      "[Epoch: 26340] train loss: 0.5123, train acc: 0.8207, val loss: 0.4418, val acc: 0.8567  (best train acc: 0.8554, best val acc: 0.8840)\n",
      "[Epoch: 26360] train loss: 0.6422, train acc: 0.7684, val loss: 0.4773, val acc: 0.8465  (best train acc: 0.8554, best val acc: 0.8860)\n",
      "[Epoch: 26380] train loss: 0.6653, train acc: 0.7663, val loss: 0.4324, val acc: 0.8637  (best train acc: 0.8554, best val acc: 0.8860)\n",
      "[Epoch: 26400] train loss: 0.5233, train acc: 0.8129, val loss: 0.4273, val acc: 0.8688  (best train acc: 0.8554, best val acc: 0.8860)\n",
      "[Epoch: 26420] train loss: 0.5746, train acc: 0.7854, val loss: 0.4109, val acc: 0.8530  (best train acc: 0.8554, best val acc: 0.8860)\n",
      "[Epoch: 26440] train loss: 0.5053, train acc: 0.8323, val loss: 0.4450, val acc: 0.8604  (best train acc: 0.8554, best val acc: 0.8860)\n",
      "[Epoch: 26460] train loss: 0.6851, train acc: 0.7460, val loss: 0.4384, val acc: 0.8762  (best train acc: 0.8554, best val acc: 0.8860)\n",
      "[Epoch: 26480] train loss: 0.5447, train acc: 0.8167, val loss: 0.4222, val acc: 0.8725  (best train acc: 0.8554, best val acc: 0.8860)\n",
      "[Epoch: 26500] train loss: 0.5429, train acc: 0.8155, val loss: 0.3974, val acc: 0.8759  (best train acc: 0.8554, best val acc: 0.8860)\n",
      "[Epoch: 26520] train loss: 0.5883, train acc: 0.7888, val loss: 0.4204, val acc: 0.8715  (best train acc: 0.8554, best val acc: 0.8860)\n",
      "[Epoch: 26540] train loss: 0.4821, train acc: 0.8405, val loss: 0.4233, val acc: 0.8688  (best train acc: 0.8554, best val acc: 0.8860)\n",
      "[Epoch: 26560] train loss: 0.6885, train acc: 0.7426, val loss: 0.3827, val acc: 0.8813  (best train acc: 0.8554, best val acc: 0.8860)\n",
      "[Epoch: 26580] train loss: 0.4782, train acc: 0.8437, val loss: 0.4122, val acc: 0.8688  (best train acc: 0.8554, best val acc: 0.8860)\n",
      "[Epoch: 26600] train loss: 0.5379, train acc: 0.7940, val loss: 0.4347, val acc: 0.8661  (best train acc: 0.8554, best val acc: 0.8860)\n",
      "[Epoch: 26620] train loss: 0.6545, train acc: 0.7622, val loss: 0.3941, val acc: 0.8762  (best train acc: 0.8554, best val acc: 0.8860)\n",
      "[Epoch: 26640] train loss: 0.5467, train acc: 0.7979, val loss: 0.4089, val acc: 0.8759  (best train acc: 0.8647, best val acc: 0.8860)\n",
      "[Epoch: 26660] train loss: 0.5118, train acc: 0.7990, val loss: 0.3926, val acc: 0.8759  (best train acc: 0.8647, best val acc: 0.8860)\n",
      "[Epoch: 26680] train loss: 0.5463, train acc: 0.7957, val loss: 0.4307, val acc: 0.8668  (best train acc: 0.8647, best val acc: 0.8860)\n",
      "[Epoch: 26700] train loss: 0.5794, train acc: 0.7951, val loss: 0.4215, val acc: 0.8745  (best train acc: 0.8647, best val acc: 0.8863)\n",
      "[Epoch: 26720] train loss: 0.5934, train acc: 0.7792, val loss: 0.3863, val acc: 0.8705  (best train acc: 0.8647, best val acc: 0.8863)\n",
      "[Epoch: 26740] train loss: 0.4577, train acc: 0.8360, val loss: 0.4195, val acc: 0.8745  (best train acc: 0.8647, best val acc: 0.8863)\n",
      "[Epoch: 26760] train loss: 0.5532, train acc: 0.8089, val loss: 0.4054, val acc: 0.8732  (best train acc: 0.8647, best val acc: 0.8863)\n",
      "[Epoch: 26780] train loss: 0.5030, train acc: 0.8136, val loss: 0.3764, val acc: 0.8786  (best train acc: 0.8647, best val acc: 0.8863)\n",
      "[Epoch: 26800] train loss: 0.6921, train acc: 0.7337, val loss: 0.3959, val acc: 0.8766  (best train acc: 0.8647, best val acc: 0.8863)\n",
      "[Epoch: 26820] train loss: 0.6428, train acc: 0.7462, val loss: 0.4064, val acc: 0.8799  (best train acc: 0.8647, best val acc: 0.8863)\n",
      "[Epoch: 26840] train loss: 0.5970, train acc: 0.7988, val loss: 0.3942, val acc: 0.8705  (best train acc: 0.8647, best val acc: 0.8863)\n",
      "[Epoch: 26860] train loss: 0.5832, train acc: 0.8056, val loss: 0.3988, val acc: 0.8698  (best train acc: 0.8647, best val acc: 0.8863)\n",
      "[Epoch: 26880] train loss: 0.7384, train acc: 0.7268, val loss: 0.4040, val acc: 0.8573  (best train acc: 0.8647, best val acc: 0.8863)\n",
      "[Epoch: 26900] train loss: 0.6106, train acc: 0.7852, val loss: 0.4157, val acc: 0.8759  (best train acc: 0.8647, best val acc: 0.8863)\n",
      "[Epoch: 26920] train loss: 0.4978, train acc: 0.8314, val loss: 0.4065, val acc: 0.8732  (best train acc: 0.8647, best val acc: 0.8863)\n",
      "[Epoch: 26940] train loss: 0.5261, train acc: 0.8198, val loss: 0.3806, val acc: 0.8853  (best train acc: 0.8647, best val acc: 0.8863)\n",
      "[Epoch: 26960] train loss: 0.5649, train acc: 0.8010, val loss: 0.4008, val acc: 0.8772  (best train acc: 0.8647, best val acc: 0.8863)\n",
      "[Epoch: 26980] train loss: 0.5543, train acc: 0.7880, val loss: 0.3836, val acc: 0.8728  (best train acc: 0.8647, best val acc: 0.8863)\n",
      "[Epoch: 27000] train loss: 0.5426, train acc: 0.7991, val loss: 0.4137, val acc: 0.8735  (best train acc: 0.8647, best val acc: 0.8863)\n",
      "[Epoch: 27020] train loss: 0.6042, train acc: 0.7790, val loss: 0.3873, val acc: 0.8776  (best train acc: 0.8647, best val acc: 0.8863)\n",
      "[Epoch: 27040] train loss: 0.5237, train acc: 0.8054, val loss: 0.4200, val acc: 0.8621  (best train acc: 0.8647, best val acc: 0.8863)\n",
      "[Epoch: 27060] train loss: 0.5850, train acc: 0.7814, val loss: 0.4237, val acc: 0.8695  (best train acc: 0.8647, best val acc: 0.8863)\n",
      "[Epoch: 27080] train loss: 0.5338, train acc: 0.7961, val loss: 0.3928, val acc: 0.8776  (best train acc: 0.8647, best val acc: 0.8863)\n",
      "[Epoch: 27100] train loss: 0.5090, train acc: 0.8023, val loss: 0.3870, val acc: 0.8712  (best train acc: 0.8647, best val acc: 0.8863)\n",
      "[Epoch: 27120] train loss: 0.5811, train acc: 0.7942, val loss: 0.4296, val acc: 0.8772  (best train acc: 0.8647, best val acc: 0.8863)\n",
      "[Epoch: 27140] train loss: 0.5043, train acc: 0.8305, val loss: 0.3963, val acc: 0.8772  (best train acc: 0.8647, best val acc: 0.8863)\n",
      "[Epoch: 27160] train loss: 0.6319, train acc: 0.7651, val loss: 0.3883, val acc: 0.8772  (best train acc: 0.8647, best val acc: 0.8863)\n",
      "[Epoch: 27180] train loss: 0.4846, train acc: 0.8260, val loss: 0.4246, val acc: 0.8786  (best train acc: 0.8647, best val acc: 0.8863)\n",
      "[Epoch: 27200] train loss: 0.4550, train acc: 0.8297, val loss: 0.4115, val acc: 0.8702  (best train acc: 0.8647, best val acc: 0.8863)\n",
      "[Epoch: 27220] train loss: 0.6584, train acc: 0.7588, val loss: 0.3844, val acc: 0.8830  (best train acc: 0.8647, best val acc: 0.8863)\n",
      "[Epoch: 27240] train loss: 0.6059, train acc: 0.7750, val loss: 0.3811, val acc: 0.8786  (best train acc: 0.8647, best val acc: 0.8863)\n",
      "[Epoch: 27260] train loss: 0.6069, train acc: 0.7702, val loss: 0.3903, val acc: 0.8702  (best train acc: 0.8647, best val acc: 0.8863)\n",
      "[Epoch: 27280] train loss: 0.4946, train acc: 0.8383, val loss: 0.3845, val acc: 0.8779  (best train acc: 0.8647, best val acc: 0.8863)\n",
      "[Epoch: 27300] train loss: 0.5247, train acc: 0.8094, val loss: 0.4687, val acc: 0.8452  (best train acc: 0.8647, best val acc: 0.8863)\n",
      "[Epoch: 27320] train loss: 0.5926, train acc: 0.7880, val loss: 0.3863, val acc: 0.8796  (best train acc: 0.8647, best val acc: 0.8863)\n",
      "[Epoch: 27340] train loss: 0.4802, train acc: 0.8221, val loss: 0.4076, val acc: 0.8617  (best train acc: 0.8647, best val acc: 0.8863)\n",
      "[Epoch: 27360] train loss: 0.5822, train acc: 0.7979, val loss: 0.3897, val acc: 0.8779  (best train acc: 0.8647, best val acc: 0.8863)\n",
      "[Epoch: 27380] train loss: 0.5257, train acc: 0.8200, val loss: 0.3951, val acc: 0.8772  (best train acc: 0.8647, best val acc: 0.8863)\n",
      "[Epoch: 27400] train loss: 0.5948, train acc: 0.7801, val loss: 0.4052, val acc: 0.8772  (best train acc: 0.8647, best val acc: 0.8863)\n",
      "[Epoch: 27420] train loss: 0.4457, train acc: 0.8566, val loss: 0.4338, val acc: 0.8671  (best train acc: 0.8647, best val acc: 0.8877)\n",
      "[Epoch: 27440] train loss: 0.5575, train acc: 0.7955, val loss: 0.4104, val acc: 0.8745  (best train acc: 0.8647, best val acc: 0.8877)\n",
      "[Epoch: 27460] train loss: 0.6119, train acc: 0.7916, val loss: 0.4233, val acc: 0.8755  (best train acc: 0.8647, best val acc: 0.8877)\n",
      "[Epoch: 27480] train loss: 0.6514, train acc: 0.7613, val loss: 0.4051, val acc: 0.8634  (best train acc: 0.8647, best val acc: 0.8877)\n",
      "[Epoch: 27500] train loss: 0.5644, train acc: 0.7928, val loss: 0.4138, val acc: 0.8658  (best train acc: 0.8647, best val acc: 0.8877)\n",
      "[Epoch: 27520] train loss: 0.7260, train acc: 0.7204, val loss: 0.4174, val acc: 0.8688  (best train acc: 0.8647, best val acc: 0.8877)\n",
      "[Epoch: 27540] train loss: 0.4750, train acc: 0.8433, val loss: 0.4158, val acc: 0.8708  (best train acc: 0.8647, best val acc: 0.8877)\n",
      "[Epoch: 27560] train loss: 0.6184, train acc: 0.7669, val loss: 0.3804, val acc: 0.8735  (best train acc: 0.8647, best val acc: 0.8877)\n",
      "[Epoch: 27580] train loss: 0.5330, train acc: 0.8086, val loss: 0.4302, val acc: 0.8664  (best train acc: 0.8647, best val acc: 0.8877)\n",
      "[Epoch: 27600] train loss: 0.5794, train acc: 0.7838, val loss: 0.4163, val acc: 0.8722  (best train acc: 0.8647, best val acc: 0.8877)\n",
      "[Epoch: 27620] train loss: 0.5267, train acc: 0.8250, val loss: 0.3768, val acc: 0.8789  (best train acc: 0.8647, best val acc: 0.8877)\n",
      "[Epoch: 27640] train loss: 0.5774, train acc: 0.7779, val loss: 0.4388, val acc: 0.8546  (best train acc: 0.8647, best val acc: 0.8877)\n",
      "[Epoch: 27660] train loss: 0.4621, train acc: 0.8545, val loss: 0.4484, val acc: 0.8540  (best train acc: 0.8647, best val acc: 0.8877)\n",
      "[Epoch: 27680] train loss: 0.5344, train acc: 0.8227, val loss: 0.3930, val acc: 0.8789  (best train acc: 0.8647, best val acc: 0.8877)\n",
      "[Epoch: 27700] train loss: 0.5306, train acc: 0.8044, val loss: 0.3670, val acc: 0.8675  (best train acc: 0.8647, best val acc: 0.8877)\n",
      "[Epoch: 27720] train loss: 0.5209, train acc: 0.8109, val loss: 0.4384, val acc: 0.8627  (best train acc: 0.8647, best val acc: 0.8877)\n",
      "[Epoch: 27740] train loss: 0.5228, train acc: 0.7958, val loss: 0.3884, val acc: 0.8820  (best train acc: 0.8647, best val acc: 0.8877)\n",
      "[Epoch: 27760] train loss: 0.5572, train acc: 0.7927, val loss: 0.4410, val acc: 0.8604  (best train acc: 0.8647, best val acc: 0.8877)\n",
      "[Epoch: 27780] train loss: 0.5491, train acc: 0.7961, val loss: 0.4094, val acc: 0.8735  (best train acc: 0.8647, best val acc: 0.8877)\n",
      "[Epoch: 27800] train loss: 0.4986, train acc: 0.8150, val loss: 0.3870, val acc: 0.8789  (best train acc: 0.8647, best val acc: 0.8877)\n",
      "[Epoch: 27820] train loss: 0.4883, train acc: 0.8323, val loss: 0.4125, val acc: 0.8661  (best train acc: 0.8647, best val acc: 0.8877)\n",
      "[Epoch: 27840] train loss: 0.4731, train acc: 0.8366, val loss: 0.3998, val acc: 0.8799  (best train acc: 0.8647, best val acc: 0.8877)\n",
      "[Epoch: 27860] train loss: 0.5262, train acc: 0.8120, val loss: 0.4159, val acc: 0.8742  (best train acc: 0.8647, best val acc: 0.8877)\n",
      "[Epoch: 27880] train loss: 0.5127, train acc: 0.8298, val loss: 0.3970, val acc: 0.8648  (best train acc: 0.8647, best val acc: 0.8880)\n",
      "[Epoch: 27900] train loss: 0.4838, train acc: 0.8151, val loss: 0.3831, val acc: 0.8782  (best train acc: 0.8647, best val acc: 0.8880)\n",
      "[Epoch: 27920] train loss: 0.4951, train acc: 0.8377, val loss: 0.4001, val acc: 0.8782  (best train acc: 0.8647, best val acc: 0.8880)\n",
      "[Epoch: 27940] train loss: 0.6018, train acc: 0.7792, val loss: 0.3799, val acc: 0.8600  (best train acc: 0.8647, best val acc: 0.8880)\n",
      "[Epoch: 27960] train loss: 0.4904, train acc: 0.8297, val loss: 0.3955, val acc: 0.8688  (best train acc: 0.8647, best val acc: 0.8880)\n",
      "[Epoch: 27980] train loss: 0.6090, train acc: 0.7689, val loss: 0.3964, val acc: 0.8749  (best train acc: 0.8647, best val acc: 0.8880)\n",
      "[Epoch: 28000] train loss: 0.4951, train acc: 0.8300, val loss: 0.4014, val acc: 0.8739  (best train acc: 0.8647, best val acc: 0.8880)\n",
      "[Epoch: 28020] train loss: 0.6089, train acc: 0.7934, val loss: 0.4132, val acc: 0.8732  (best train acc: 0.8647, best val acc: 0.8880)\n",
      "[Epoch: 28040] train loss: 0.6911, train acc: 0.7354, val loss: 0.3701, val acc: 0.8816  (best train acc: 0.8647, best val acc: 0.8880)\n",
      "[Epoch: 28060] train loss: 0.5118, train acc: 0.8141, val loss: 0.3935, val acc: 0.8813  (best train acc: 0.8647, best val acc: 0.8880)\n",
      "[Epoch: 28080] train loss: 0.5274, train acc: 0.8044, val loss: 0.4144, val acc: 0.8749  (best train acc: 0.8647, best val acc: 0.8880)\n",
      "[Epoch: 28100] train loss: 0.5651, train acc: 0.7852, val loss: 0.4066, val acc: 0.8681  (best train acc: 0.8647, best val acc: 0.8880)\n",
      "[Epoch: 28120] train loss: 0.6144, train acc: 0.7694, val loss: 0.4121, val acc: 0.8796  (best train acc: 0.8647, best val acc: 0.8880)\n",
      "[Epoch: 28140] train loss: 0.6510, train acc: 0.7650, val loss: 0.4073, val acc: 0.8762  (best train acc: 0.8647, best val acc: 0.8880)\n",
      "[Epoch: 28160] train loss: 0.6021, train acc: 0.7674, val loss: 0.4051, val acc: 0.8614  (best train acc: 0.8647, best val acc: 0.8880)\n",
      "[Epoch: 28180] train loss: 0.5032, train acc: 0.8247, val loss: 0.3782, val acc: 0.8786  (best train acc: 0.8647, best val acc: 0.8880)\n",
      "[Epoch: 28200] train loss: 0.7176, train acc: 0.7297, val loss: 0.3974, val acc: 0.8769  (best train acc: 0.8647, best val acc: 0.8880)\n",
      "[Epoch: 28220] train loss: 0.5686, train acc: 0.7981, val loss: 0.3891, val acc: 0.8816  (best train acc: 0.8647, best val acc: 0.8880)\n",
      "[Epoch: 28240] train loss: 0.6709, train acc: 0.7634, val loss: 0.3958, val acc: 0.8644  (best train acc: 0.8647, best val acc: 0.8880)\n",
      "[Epoch: 28260] train loss: 0.6843, train acc: 0.7389, val loss: 0.3995, val acc: 0.8749  (best train acc: 0.8647, best val acc: 0.8880)\n",
      "[Epoch: 28280] train loss: 0.7313, train acc: 0.7278, val loss: 0.3979, val acc: 0.8769  (best train acc: 0.8647, best val acc: 0.8880)\n",
      "[Epoch: 28300] train loss: 0.5516, train acc: 0.7910, val loss: 0.3845, val acc: 0.8776  (best train acc: 0.8647, best val acc: 0.8880)\n",
      "[Epoch: 28320] train loss: 0.4778, train acc: 0.8264, val loss: 0.4263, val acc: 0.8523  (best train acc: 0.8647, best val acc: 0.8880)\n",
      "[Epoch: 28340] train loss: 0.5068, train acc: 0.8387, val loss: 0.4503, val acc: 0.8634  (best train acc: 0.8647, best val acc: 0.8880)\n",
      "[Epoch: 28360] train loss: 0.5122, train acc: 0.8336, val loss: 0.4234, val acc: 0.8728  (best train acc: 0.8647, best val acc: 0.8880)\n",
      "[Epoch: 28380] train loss: 0.5643, train acc: 0.8035, val loss: 0.4186, val acc: 0.8641  (best train acc: 0.8647, best val acc: 0.8880)\n",
      "[Epoch: 28400] train loss: 0.6118, train acc: 0.7668, val loss: 0.3772, val acc: 0.8776  (best train acc: 0.8647, best val acc: 0.8880)\n",
      "[Epoch: 28420] train loss: 0.5105, train acc: 0.8084, val loss: 0.3691, val acc: 0.8836  (best train acc: 0.8647, best val acc: 0.8880)\n",
      "[Epoch: 28440] train loss: 0.6443, train acc: 0.7680, val loss: 0.4237, val acc: 0.8755  (best train acc: 0.8647, best val acc: 0.8880)\n",
      "[Epoch: 28460] train loss: 0.5275, train acc: 0.8036, val loss: 0.3821, val acc: 0.8793  (best train acc: 0.8647, best val acc: 0.8880)\n",
      "[Epoch: 28480] train loss: 0.4878, train acc: 0.8381, val loss: 0.4399, val acc: 0.8617  (best train acc: 0.8647, best val acc: 0.8880)\n",
      "[Epoch: 28500] train loss: 0.6970, train acc: 0.7394, val loss: 0.3778, val acc: 0.8843  (best train acc: 0.8647, best val acc: 0.8880)\n",
      "[Epoch: 28520] train loss: 0.5801, train acc: 0.7845, val loss: 0.3639, val acc: 0.8843  (best train acc: 0.8647, best val acc: 0.8880)\n",
      "[Epoch: 28540] train loss: 0.5191, train acc: 0.8036, val loss: 0.3855, val acc: 0.8695  (best train acc: 0.8647, best val acc: 0.8880)\n",
      "[Epoch: 28560] train loss: 0.4457, train acc: 0.8513, val loss: 0.4077, val acc: 0.8715  (best train acc: 0.8647, best val acc: 0.8880)\n",
      "[Epoch: 28580] train loss: 0.5999, train acc: 0.7761, val loss: 0.4100, val acc: 0.8786  (best train acc: 0.8647, best val acc: 0.8880)\n",
      "[Epoch: 28600] train loss: 0.6526, train acc: 0.7827, val loss: 0.3991, val acc: 0.8718  (best train acc: 0.8647, best val acc: 0.8880)\n",
      "[Epoch: 28620] train loss: 0.4540, train acc: 0.8459, val loss: 0.3917, val acc: 0.8782  (best train acc: 0.8647, best val acc: 0.8880)\n",
      "[Epoch: 28640] train loss: 0.6617, train acc: 0.7611, val loss: 0.3998, val acc: 0.8793  (best train acc: 0.8647, best val acc: 0.8880)\n",
      "[Epoch: 28660] train loss: 0.4843, train acc: 0.8120, val loss: 0.3766, val acc: 0.8718  (best train acc: 0.8647, best val acc: 0.8880)\n",
      "[Epoch: 28680] train loss: 0.5281, train acc: 0.8093, val loss: 0.3944, val acc: 0.8766  (best train acc: 0.8647, best val acc: 0.8880)\n",
      "[Epoch: 28700] train loss: 0.5036, train acc: 0.8297, val loss: 0.4169, val acc: 0.8631  (best train acc: 0.8647, best val acc: 0.8880)\n",
      "[Epoch: 28720] train loss: 0.6113, train acc: 0.7820, val loss: 0.4263, val acc: 0.8708  (best train acc: 0.8647, best val acc: 0.8880)\n",
      "[Epoch: 28740] train loss: 0.5246, train acc: 0.8115, val loss: 0.3819, val acc: 0.8678  (best train acc: 0.8647, best val acc: 0.8880)\n",
      "[Epoch: 28760] train loss: 0.4948, train acc: 0.8253, val loss: 0.4338, val acc: 0.8526  (best train acc: 0.8647, best val acc: 0.8890)\n",
      "[Epoch: 28780] train loss: 0.4901, train acc: 0.8282, val loss: 0.4094, val acc: 0.8786  (best train acc: 0.8668, best val acc: 0.8890)\n",
      "[Epoch: 28800] train loss: 0.4809, train acc: 0.8295, val loss: 0.4089, val acc: 0.8728  (best train acc: 0.8668, best val acc: 0.8890)\n",
      "[Epoch: 28820] train loss: 0.5381, train acc: 0.8256, val loss: 0.4229, val acc: 0.8708  (best train acc: 0.8668, best val acc: 0.8890)\n",
      "[Epoch: 28840] train loss: 0.5870, train acc: 0.7958, val loss: 0.3858, val acc: 0.8759  (best train acc: 0.8668, best val acc: 0.8890)\n",
      "[Epoch: 28860] train loss: 0.5731, train acc: 0.7830, val loss: 0.4027, val acc: 0.8718  (best train acc: 0.8668, best val acc: 0.8890)\n",
      "[Epoch: 28880] train loss: 0.6014, train acc: 0.7619, val loss: 0.3898, val acc: 0.8823  (best train acc: 0.8668, best val acc: 0.8890)\n",
      "[Epoch: 28900] train loss: 0.6015, train acc: 0.7816, val loss: 0.3993, val acc: 0.8732  (best train acc: 0.8668, best val acc: 0.8890)\n",
      "[Epoch: 28920] train loss: 0.5376, train acc: 0.8091, val loss: 0.4151, val acc: 0.8803  (best train acc: 0.8668, best val acc: 0.8911)\n",
      "[Epoch: 28940] train loss: 0.6066, train acc: 0.7723, val loss: 0.4111, val acc: 0.8809  (best train acc: 0.8668, best val acc: 0.8911)\n",
      "[Epoch: 28960] train loss: 0.5537, train acc: 0.8027, val loss: 0.3989, val acc: 0.8705  (best train acc: 0.8668, best val acc: 0.8911)\n",
      "[Epoch: 28980] train loss: 0.6408, train acc: 0.7530, val loss: 0.4016, val acc: 0.8742  (best train acc: 0.8668, best val acc: 0.8911)\n",
      "[Epoch: 29000] train loss: 0.5536, train acc: 0.7989, val loss: 0.3888, val acc: 0.8735  (best train acc: 0.8668, best val acc: 0.8911)\n",
      "[Epoch: 29020] train loss: 0.4807, train acc: 0.8364, val loss: 0.3956, val acc: 0.8739  (best train acc: 0.8668, best val acc: 0.8911)\n",
      "[Epoch: 29040] train loss: 0.4847, train acc: 0.8510, val loss: 0.4248, val acc: 0.8691  (best train acc: 0.8668, best val acc: 0.8911)\n",
      "[Epoch: 29060] train loss: 0.4987, train acc: 0.8299, val loss: 0.4355, val acc: 0.8546  (best train acc: 0.8668, best val acc: 0.8911)\n",
      "[Epoch: 29080] train loss: 0.5698, train acc: 0.8030, val loss: 0.3912, val acc: 0.8793  (best train acc: 0.8668, best val acc: 0.8911)\n",
      "[Epoch: 29100] train loss: 0.5082, train acc: 0.8110, val loss: 0.3745, val acc: 0.8728  (best train acc: 0.8668, best val acc: 0.8911)\n",
      "[Epoch: 29120] train loss: 0.5597, train acc: 0.8134, val loss: 0.3672, val acc: 0.8718  (best train acc: 0.8668, best val acc: 0.8911)\n",
      "[Epoch: 29140] train loss: 0.4865, train acc: 0.8336, val loss: 0.4220, val acc: 0.8715  (best train acc: 0.8668, best val acc: 0.8911)\n",
      "[Epoch: 29160] train loss: 0.4813, train acc: 0.8396, val loss: 0.3761, val acc: 0.8688  (best train acc: 0.8668, best val acc: 0.8911)\n",
      "[Epoch: 29180] train loss: 0.5481, train acc: 0.8085, val loss: 0.3754, val acc: 0.8742  (best train acc: 0.8668, best val acc: 0.8911)\n",
      "[Epoch: 29200] train loss: 0.6428, train acc: 0.7724, val loss: 0.4215, val acc: 0.8725  (best train acc: 0.8668, best val acc: 0.8911)\n",
      "[Epoch: 29220] train loss: 0.5614, train acc: 0.8067, val loss: 0.3936, val acc: 0.8772  (best train acc: 0.8668, best val acc: 0.8911)\n",
      "[Epoch: 29240] train loss: 0.4641, train acc: 0.8349, val loss: 0.3901, val acc: 0.8698  (best train acc: 0.8668, best val acc: 0.8911)\n",
      "[Epoch: 29260] train loss: 0.6917, train acc: 0.7426, val loss: 0.4316, val acc: 0.8577  (best train acc: 0.8668, best val acc: 0.8911)\n",
      "[Epoch: 29280] train loss: 0.4625, train acc: 0.8467, val loss: 0.3940, val acc: 0.8782  (best train acc: 0.8668, best val acc: 0.8911)\n",
      "[Epoch: 29300] train loss: 0.6278, train acc: 0.7846, val loss: 0.3849, val acc: 0.8816  (best train acc: 0.8668, best val acc: 0.8911)\n",
      "[Epoch: 29320] train loss: 0.5702, train acc: 0.7900, val loss: 0.3624, val acc: 0.8766  (best train acc: 0.8668, best val acc: 0.8911)\n",
      "[Epoch: 29340] train loss: 0.5296, train acc: 0.8149, val loss: 0.4283, val acc: 0.8688  (best train acc: 0.8668, best val acc: 0.8911)\n",
      "[Epoch: 29360] train loss: 0.4814, train acc: 0.8368, val loss: 0.3938, val acc: 0.8813  (best train acc: 0.8668, best val acc: 0.8911)\n",
      "[Epoch: 29380] train loss: 0.6266, train acc: 0.7564, val loss: 0.3720, val acc: 0.8820  (best train acc: 0.8668, best val acc: 0.8911)\n",
      "[Epoch: 29400] train loss: 0.5426, train acc: 0.8009, val loss: 0.3974, val acc: 0.8830  (best train acc: 0.8668, best val acc: 0.8911)\n",
      "[Epoch: 29420] train loss: 0.5514, train acc: 0.8109, val loss: 0.3972, val acc: 0.8752  (best train acc: 0.8668, best val acc: 0.8911)\n",
      "[Epoch: 29440] train loss: 0.4990, train acc: 0.8220, val loss: 0.4028, val acc: 0.8671  (best train acc: 0.8668, best val acc: 0.8911)\n",
      "[Epoch: 29460] train loss: 0.5391, train acc: 0.8088, val loss: 0.3956, val acc: 0.8803  (best train acc: 0.8668, best val acc: 0.8911)\n",
      "[Epoch: 29480] train loss: 0.6188, train acc: 0.7778, val loss: 0.4070, val acc: 0.8698  (best train acc: 0.8668, best val acc: 0.8911)\n",
      "[Epoch: 29500] train loss: 0.7210, train acc: 0.7243, val loss: 0.4039, val acc: 0.8847  (best train acc: 0.8668, best val acc: 0.8911)\n",
      "[Epoch: 29520] train loss: 0.4871, train acc: 0.8324, val loss: 0.4260, val acc: 0.8648  (best train acc: 0.8668, best val acc: 0.8911)\n",
      "[Epoch: 29540] train loss: 0.5236, train acc: 0.8106, val loss: 0.3716, val acc: 0.8826  (best train acc: 0.8668, best val acc: 0.8911)\n",
      "[Epoch: 29560] train loss: 0.4572, train acc: 0.8299, val loss: 0.3742, val acc: 0.8799  (best train acc: 0.8668, best val acc: 0.8911)\n",
      "[Epoch: 29580] train loss: 0.6068, train acc: 0.7619, val loss: 0.3601, val acc: 0.8901  (best train acc: 0.8668, best val acc: 0.8911)\n",
      "[Epoch: 29600] train loss: 0.4739, train acc: 0.8297, val loss: 0.3696, val acc: 0.8880  (best train acc: 0.8668, best val acc: 0.8911)\n",
      "[Epoch: 29620] train loss: 0.4800, train acc: 0.8292, val loss: 0.3868, val acc: 0.8806  (best train acc: 0.8668, best val acc: 0.8911)\n",
      "[Epoch: 29640] train loss: 0.5854, train acc: 0.7810, val loss: 0.3788, val acc: 0.8806  (best train acc: 0.8668, best val acc: 0.8911)\n",
      "[Epoch: 29660] train loss: 0.4904, train acc: 0.8133, val loss: 0.3965, val acc: 0.8793  (best train acc: 0.8668, best val acc: 0.8911)\n",
      "[Epoch: 29680] train loss: 0.4932, train acc: 0.8159, val loss: 0.4028, val acc: 0.8803  (best train acc: 0.8668, best val acc: 0.8911)\n",
      "[Epoch: 29700] train loss: 0.7221, train acc: 0.7235, val loss: 0.3709, val acc: 0.8809  (best train acc: 0.8668, best val acc: 0.8911)\n",
      "[Epoch: 29720] train loss: 0.6536, train acc: 0.7550, val loss: 0.4269, val acc: 0.8621  (best train acc: 0.8717, best val acc: 0.8911)\n",
      "[Epoch: 29740] train loss: 0.5111, train acc: 0.8216, val loss: 0.3799, val acc: 0.8870  (best train acc: 0.8717, best val acc: 0.8911)\n",
      "[Epoch: 29760] train loss: 0.4814, train acc: 0.8217, val loss: 0.3771, val acc: 0.8752  (best train acc: 0.8717, best val acc: 0.8911)\n",
      "[Epoch: 29780] train loss: 0.6017, train acc: 0.7893, val loss: 0.4041, val acc: 0.8803  (best train acc: 0.8717, best val acc: 0.8911)\n",
      "[Epoch: 29800] train loss: 0.5073, train acc: 0.8103, val loss: 0.3970, val acc: 0.8820  (best train acc: 0.8717, best val acc: 0.8911)\n",
      "[Epoch: 29820] train loss: 0.5562, train acc: 0.8058, val loss: 0.3718, val acc: 0.8820  (best train acc: 0.8717, best val acc: 0.8911)\n",
      "[Epoch: 29840] train loss: 0.6413, train acc: 0.7544, val loss: 0.3818, val acc: 0.8860  (best train acc: 0.8717, best val acc: 0.8911)\n",
      "[Epoch: 29860] train loss: 0.6175, train acc: 0.7897, val loss: 0.4247, val acc: 0.8583  (best train acc: 0.8717, best val acc: 0.8911)\n",
      "[Epoch: 29880] train loss: 0.5711, train acc: 0.7744, val loss: 0.3632, val acc: 0.8921  (best train acc: 0.8717, best val acc: 0.8921)\n",
      "[Epoch: 29900] train loss: 0.6989, train acc: 0.7458, val loss: 0.3774, val acc: 0.8840  (best train acc: 0.8717, best val acc: 0.8921)\n",
      "[Epoch: 29920] train loss: 0.5287, train acc: 0.7996, val loss: 0.3720, val acc: 0.8853  (best train acc: 0.8717, best val acc: 0.8921)\n",
      "[Epoch: 29940] train loss: 0.5997, train acc: 0.7854, val loss: 0.4250, val acc: 0.8681  (best train acc: 0.8717, best val acc: 0.8921)\n",
      "[Epoch: 29960] train loss: 0.5243, train acc: 0.8279, val loss: 0.3937, val acc: 0.8833  (best train acc: 0.8717, best val acc: 0.8921)\n",
      "[Epoch: 29980] train loss: 0.5712, train acc: 0.8052, val loss: 0.3643, val acc: 0.8597  (best train acc: 0.8717, best val acc: 0.8921)\n",
      "[Epoch: 30000] train loss: 0.4960, train acc: 0.8308, val loss: 0.3805, val acc: 0.8631  (best train acc: 0.8717, best val acc: 0.8938)\n",
      "[Epoch: 30020] train loss: 0.5244, train acc: 0.8324, val loss: 0.3953, val acc: 0.8732  (best train acc: 0.8717, best val acc: 0.8938)\n",
      "[Epoch: 30040] train loss: 0.5202, train acc: 0.8153, val loss: 0.3801, val acc: 0.8779  (best train acc: 0.8717, best val acc: 0.8951)\n",
      "[Epoch: 30060] train loss: 0.5496, train acc: 0.8141, val loss: 0.3731, val acc: 0.8833  (best train acc: 0.8717, best val acc: 0.8981)\n",
      "[Epoch: 30080] train loss: 0.4834, train acc: 0.8179, val loss: 0.4193, val acc: 0.8614  (best train acc: 0.8717, best val acc: 0.8981)\n",
      "[Epoch: 30100] train loss: 0.5244, train acc: 0.8031, val loss: 0.3671, val acc: 0.8880  (best train acc: 0.8717, best val acc: 0.8981)\n",
      "[Epoch: 30120] train loss: 0.4801, train acc: 0.8469, val loss: 0.4105, val acc: 0.8631  (best train acc: 0.8717, best val acc: 0.8981)\n",
      "[Epoch: 30140] train loss: 0.5507, train acc: 0.8119, val loss: 0.4091, val acc: 0.8732  (best train acc: 0.8717, best val acc: 0.8981)\n",
      "[Epoch: 30160] train loss: 0.4842, train acc: 0.8509, val loss: 0.4049, val acc: 0.8823  (best train acc: 0.8717, best val acc: 0.8981)\n",
      "[Epoch: 30180] train loss: 0.5672, train acc: 0.7918, val loss: 0.3683, val acc: 0.8820  (best train acc: 0.8717, best val acc: 0.8981)\n",
      "[Epoch: 30200] train loss: 0.5974, train acc: 0.7924, val loss: 0.3956, val acc: 0.8762  (best train acc: 0.8717, best val acc: 0.8981)\n",
      "[Epoch: 30220] train loss: 0.5966, train acc: 0.7799, val loss: 0.4038, val acc: 0.8776  (best train acc: 0.8717, best val acc: 0.8985)\n",
      "[Epoch: 30240] train loss: 0.5803, train acc: 0.7764, val loss: 0.3821, val acc: 0.8809  (best train acc: 0.8717, best val acc: 0.8985)\n",
      "[Epoch: 30260] train loss: 0.5609, train acc: 0.8028, val loss: 0.4501, val acc: 0.8280  (best train acc: 0.8717, best val acc: 0.8985)\n",
      "[Epoch: 30280] train loss: 0.5233, train acc: 0.8232, val loss: 0.3588, val acc: 0.9012  (best train acc: 0.8717, best val acc: 0.9012)\n",
      "[Epoch: 30300] train loss: 0.5440, train acc: 0.7919, val loss: 0.3714, val acc: 0.8728  (best train acc: 0.8717, best val acc: 0.9012)\n",
      "[Epoch: 30320] train loss: 0.5487, train acc: 0.7916, val loss: 0.4184, val acc: 0.8371  (best train acc: 0.8717, best val acc: 0.9012)\n",
      "[Epoch: 30340] train loss: 0.4774, train acc: 0.8279, val loss: 0.4557, val acc: 0.8415  (best train acc: 0.8717, best val acc: 0.9012)\n",
      "[Epoch: 30360] train loss: 0.5865, train acc: 0.7796, val loss: 0.4013, val acc: 0.8843  (best train acc: 0.8717, best val acc: 0.9012)\n",
      "[Epoch: 30380] train loss: 0.5780, train acc: 0.7980, val loss: 0.3814, val acc: 0.8880  (best train acc: 0.8717, best val acc: 0.9012)\n",
      "[Epoch: 30400] train loss: 0.6657, train acc: 0.7552, val loss: 0.3704, val acc: 0.8948  (best train acc: 0.8717, best val acc: 0.9012)\n",
      "[Epoch: 30420] train loss: 0.5244, train acc: 0.8180, val loss: 0.4460, val acc: 0.8405  (best train acc: 0.8717, best val acc: 0.9012)\n",
      "[Epoch: 30440] train loss: 0.4946, train acc: 0.8419, val loss: 0.4051, val acc: 0.8836  (best train acc: 0.8717, best val acc: 0.9012)\n",
      "[Epoch: 30460] train loss: 0.5083, train acc: 0.8134, val loss: 0.3825, val acc: 0.8887  (best train acc: 0.8717, best val acc: 0.9012)\n",
      "[Epoch: 30480] train loss: 0.5689, train acc: 0.7880, val loss: 0.4158, val acc: 0.8752  (best train acc: 0.8717, best val acc: 0.9012)\n",
      "[Epoch: 30500] train loss: 0.5557, train acc: 0.8085, val loss: 0.3644, val acc: 0.8965  (best train acc: 0.8717, best val acc: 0.9012)\n",
      "[Epoch: 30520] train loss: 0.5216, train acc: 0.8205, val loss: 0.4062, val acc: 0.8850  (best train acc: 0.8717, best val acc: 0.9012)\n",
      "[Epoch: 30540] train loss: 0.5018, train acc: 0.8240, val loss: 0.3783, val acc: 0.8971  (best train acc: 0.8717, best val acc: 0.9012)\n",
      "[Epoch: 30560] train loss: 0.6511, train acc: 0.7603, val loss: 0.3941, val acc: 0.8749  (best train acc: 0.8717, best val acc: 0.9012)\n",
      "[Epoch: 30580] train loss: 0.5230, train acc: 0.8239, val loss: 0.3903, val acc: 0.8917  (best train acc: 0.8717, best val acc: 0.9012)\n",
      "[Epoch: 30600] train loss: 0.5669, train acc: 0.7974, val loss: 0.3741, val acc: 0.8782  (best train acc: 0.8717, best val acc: 0.9022)\n",
      "[Epoch: 30620] train loss: 0.5007, train acc: 0.8283, val loss: 0.4260, val acc: 0.8661  (best train acc: 0.8717, best val acc: 0.9022)\n",
      "[Epoch: 30640] train loss: 0.4847, train acc: 0.8311, val loss: 0.3738, val acc: 0.8927  (best train acc: 0.8717, best val acc: 0.9022)\n",
      "[Epoch: 30660] train loss: 0.5045, train acc: 0.8191, val loss: 0.3670, val acc: 0.8934  (best train acc: 0.8717, best val acc: 0.9022)\n",
      "[Epoch: 30680] train loss: 0.4854, train acc: 0.8363, val loss: 0.3736, val acc: 0.8813  (best train acc: 0.8717, best val acc: 0.9022)\n",
      "[Epoch: 30700] train loss: 0.7069, train acc: 0.7198, val loss: 0.3852, val acc: 0.8776  (best train acc: 0.8717, best val acc: 0.9022)\n",
      "[Epoch: 30720] train loss: 0.5118, train acc: 0.8231, val loss: 0.3793, val acc: 0.8877  (best train acc: 0.8717, best val acc: 0.9022)\n",
      "[Epoch: 30740] train loss: 0.6437, train acc: 0.7572, val loss: 0.3968, val acc: 0.8917  (best train acc: 0.8717, best val acc: 0.9022)\n",
      "[Epoch: 30760] train loss: 0.4566, train acc: 0.8468, val loss: 0.3682, val acc: 0.8938  (best train acc: 0.8717, best val acc: 0.9022)\n",
      "[Epoch: 30780] train loss: 0.6054, train acc: 0.7781, val loss: 0.4191, val acc: 0.8587  (best train acc: 0.8717, best val acc: 0.9022)\n",
      "[Epoch: 30800] train loss: 0.4722, train acc: 0.8488, val loss: 0.4178, val acc: 0.8577  (best train acc: 0.8717, best val acc: 0.9022)\n",
      "[Epoch: 30820] train loss: 0.5329, train acc: 0.7901, val loss: 0.4534, val acc: 0.8438  (best train acc: 0.8717, best val acc: 0.9022)\n",
      "[Epoch: 30840] train loss: 0.5543, train acc: 0.8084, val loss: 0.3961, val acc: 0.8648  (best train acc: 0.8717, best val acc: 0.9022)\n",
      "[Epoch: 30860] train loss: 0.4682, train acc: 0.8394, val loss: 0.3964, val acc: 0.8877  (best train acc: 0.8717, best val acc: 0.9022)\n",
      "[Epoch: 30880] train loss: 0.4612, train acc: 0.8509, val loss: 0.3700, val acc: 0.8816  (best train acc: 0.8717, best val acc: 0.9022)\n",
      "[Epoch: 30900] train loss: 0.4948, train acc: 0.8261, val loss: 0.4146, val acc: 0.8624  (best train acc: 0.8717, best val acc: 0.9022)\n",
      "[Epoch: 30920] train loss: 0.5941, train acc: 0.7846, val loss: 0.3755, val acc: 0.8809  (best train acc: 0.8717, best val acc: 0.9022)\n",
      "[Epoch: 30940] train loss: 0.5127, train acc: 0.8310, val loss: 0.3582, val acc: 0.8995  (best train acc: 0.8717, best val acc: 0.9022)\n",
      "[Epoch: 30960] train loss: 0.5675, train acc: 0.8045, val loss: 0.3658, val acc: 0.8816  (best train acc: 0.8717, best val acc: 0.9022)\n",
      "[Epoch: 30980] train loss: 0.5334, train acc: 0.8143, val loss: 0.3745, val acc: 0.8897  (best train acc: 0.8717, best val acc: 0.9022)\n",
      "[Epoch: 31000] train loss: 0.4709, train acc: 0.8647, val loss: 0.3718, val acc: 0.8914  (best train acc: 0.8717, best val acc: 0.9022)\n",
      "[Epoch: 31020] train loss: 0.6901, train acc: 0.7358, val loss: 0.3941, val acc: 0.8911  (best train acc: 0.8717, best val acc: 0.9022)\n",
      "[Epoch: 31040] train loss: 0.4830, train acc: 0.8228, val loss: 0.3774, val acc: 0.8725  (best train acc: 0.8717, best val acc: 0.9022)\n",
      "[Epoch: 31060] train loss: 0.6762, train acc: 0.7470, val loss: 0.3887, val acc: 0.8745  (best train acc: 0.8717, best val acc: 0.9022)\n",
      "[Epoch: 31080] train loss: 0.5141, train acc: 0.8121, val loss: 0.3951, val acc: 0.8776  (best train acc: 0.8717, best val acc: 0.9022)\n",
      "[Epoch: 31100] train loss: 0.4822, train acc: 0.8334, val loss: 0.4321, val acc: 0.8509  (best train acc: 0.8717, best val acc: 0.9022)\n",
      "[Epoch: 31120] train loss: 0.6126, train acc: 0.7744, val loss: 0.3966, val acc: 0.8793  (best train acc: 0.8717, best val acc: 0.9022)\n",
      "[Epoch: 31140] train loss: 0.6095, train acc: 0.7913, val loss: 0.3739, val acc: 0.8931  (best train acc: 0.8717, best val acc: 0.9022)\n",
      "[Epoch: 31160] train loss: 0.4800, train acc: 0.8386, val loss: 0.3768, val acc: 0.8877  (best train acc: 0.8717, best val acc: 0.9022)\n",
      "[Epoch: 31180] train loss: 0.4988, train acc: 0.8224, val loss: 0.3623, val acc: 0.8857  (best train acc: 0.8717, best val acc: 0.9022)\n",
      "[Epoch: 31200] train loss: 0.4628, train acc: 0.8360, val loss: 0.3702, val acc: 0.8901  (best train acc: 0.8717, best val acc: 0.9022)\n",
      "[Epoch: 31220] train loss: 0.5310, train acc: 0.8156, val loss: 0.4210, val acc: 0.8543  (best train acc: 0.8717, best val acc: 0.9022)\n",
      "[Epoch: 31240] train loss: 0.5476, train acc: 0.8007, val loss: 0.3880, val acc: 0.8735  (best train acc: 0.8717, best val acc: 0.9022)\n",
      "[Epoch: 31260] train loss: 0.4929, train acc: 0.8097, val loss: 0.3598, val acc: 0.8826  (best train acc: 0.8717, best val acc: 0.9022)\n",
      "[Epoch: 31280] train loss: 0.4688, train acc: 0.8271, val loss: 0.3716, val acc: 0.8718  (best train acc: 0.8717, best val acc: 0.9022)\n",
      "[Epoch: 31300] train loss: 0.5138, train acc: 0.8104, val loss: 0.4735, val acc: 0.8334  (best train acc: 0.8717, best val acc: 0.9022)\n",
      "[Epoch: 31320] train loss: 0.4609, train acc: 0.8342, val loss: 0.3813, val acc: 0.8766  (best train acc: 0.8717, best val acc: 0.9022)\n",
      "[Epoch: 31340] train loss: 0.4692, train acc: 0.8241, val loss: 0.4082, val acc: 0.8691  (best train acc: 0.8717, best val acc: 0.9029)\n",
      "[Epoch: 31360] train loss: 0.5615, train acc: 0.8016, val loss: 0.3555, val acc: 0.8897  (best train acc: 0.8717, best val acc: 0.9029)\n",
      "[Epoch: 31380] train loss: 0.4829, train acc: 0.8232, val loss: 0.4236, val acc: 0.8614  (best train acc: 0.8717, best val acc: 0.9029)\n",
      "[Epoch: 31400] train loss: 0.7122, train acc: 0.7162, val loss: 0.3683, val acc: 0.8857  (best train acc: 0.8717, best val acc: 0.9029)\n",
      "[Epoch: 31420] train loss: 0.6051, train acc: 0.7804, val loss: 0.3826, val acc: 0.8847  (best train acc: 0.8717, best val acc: 0.9029)\n",
      "[Epoch: 31440] train loss: 0.6165, train acc: 0.7724, val loss: 0.3765, val acc: 0.8789  (best train acc: 0.8717, best val acc: 0.9029)\n",
      "[Epoch: 31460] train loss: 0.4804, train acc: 0.8279, val loss: 0.3712, val acc: 0.8718  (best train acc: 0.8717, best val acc: 0.9029)\n",
      "[Epoch: 31480] train loss: 0.4866, train acc: 0.8347, val loss: 0.4169, val acc: 0.8604  (best train acc: 0.8717, best val acc: 0.9029)\n",
      "[Epoch: 31500] train loss: 0.7445, train acc: 0.7263, val loss: 0.4146, val acc: 0.8580  (best train acc: 0.8717, best val acc: 0.9029)\n",
      "[Epoch: 31520] train loss: 0.5516, train acc: 0.8099, val loss: 0.3703, val acc: 0.8799  (best train acc: 0.8717, best val acc: 0.9029)\n",
      "[Epoch: 31540] train loss: 0.5068, train acc: 0.8145, val loss: 0.4383, val acc: 0.8317  (best train acc: 0.8717, best val acc: 0.9029)\n",
      "[Epoch: 31560] train loss: 0.5198, train acc: 0.8167, val loss: 0.3778, val acc: 0.8863  (best train acc: 0.8717, best val acc: 0.9029)\n",
      "[Epoch: 31580] train loss: 0.5777, train acc: 0.7856, val loss: 0.3463, val acc: 0.8951  (best train acc: 0.8717, best val acc: 0.9029)\n",
      "[Epoch: 31600] train loss: 0.5694, train acc: 0.7942, val loss: 0.3498, val acc: 0.8958  (best train acc: 0.8717, best val acc: 0.9029)\n",
      "[Epoch: 31620] train loss: 0.4866, train acc: 0.8588, val loss: 0.4461, val acc: 0.8354  (best train acc: 0.8717, best val acc: 0.9029)\n",
      "[Epoch: 31640] train loss: 0.4711, train acc: 0.8412, val loss: 0.3910, val acc: 0.8708  (best train acc: 0.8720, best val acc: 0.9029)\n",
      "[Epoch: 31660] train loss: 0.4656, train acc: 0.8294, val loss: 0.3889, val acc: 0.8712  (best train acc: 0.8720, best val acc: 0.9029)\n",
      "[Epoch: 31680] train loss: 0.4578, train acc: 0.8389, val loss: 0.4044, val acc: 0.8654  (best train acc: 0.8720, best val acc: 0.9029)\n",
      "[Epoch: 31700] train loss: 0.5330, train acc: 0.8037, val loss: 0.3484, val acc: 0.8978  (best train acc: 0.8720, best val acc: 0.9029)\n",
      "[Epoch: 31720] train loss: 0.5135, train acc: 0.8182, val loss: 0.3642, val acc: 0.8860  (best train acc: 0.8720, best val acc: 0.9029)\n",
      "[Epoch: 31740] train loss: 0.5861, train acc: 0.7892, val loss: 0.4146, val acc: 0.8573  (best train acc: 0.8720, best val acc: 0.9029)\n",
      "[Epoch: 31760] train loss: 0.5788, train acc: 0.7817, val loss: 0.3610, val acc: 0.8685  (best train acc: 0.8720, best val acc: 0.9029)\n",
      "[Epoch: 31780] train loss: 0.5454, train acc: 0.8131, val loss: 0.3857, val acc: 0.8816  (best train acc: 0.8720, best val acc: 0.9029)\n",
      "[Epoch: 31800] train loss: 0.4757, train acc: 0.8490, val loss: 0.3798, val acc: 0.8739  (best train acc: 0.8720, best val acc: 0.9029)\n",
      "[Epoch: 31820] train loss: 0.5783, train acc: 0.7749, val loss: 0.3489, val acc: 0.8874  (best train acc: 0.8720, best val acc: 0.9029)\n",
      "[Epoch: 31840] train loss: 0.5598, train acc: 0.7911, val loss: 0.3918, val acc: 0.8745  (best train acc: 0.8720, best val acc: 0.9029)\n",
      "[Epoch: 31860] train loss: 0.5732, train acc: 0.7942, val loss: 0.3693, val acc: 0.8958  (best train acc: 0.8720, best val acc: 0.9029)\n",
      "[Epoch: 31880] train loss: 0.5029, train acc: 0.8167, val loss: 0.3670, val acc: 0.8755  (best train acc: 0.8720, best val acc: 0.9029)\n",
      "[Epoch: 31900] train loss: 0.4538, train acc: 0.8422, val loss: 0.3881, val acc: 0.8850  (best train acc: 0.8720, best val acc: 0.9029)\n",
      "[Epoch: 31920] train loss: 0.4514, train acc: 0.8567, val loss: 0.3800, val acc: 0.8944  (best train acc: 0.8720, best val acc: 0.9039)\n",
      "[Epoch: 31940] train loss: 0.5168, train acc: 0.8035, val loss: 0.3823, val acc: 0.8745  (best train acc: 0.8720, best val acc: 0.9039)\n",
      "[Epoch: 31960] train loss: 0.4480, train acc: 0.8428, val loss: 0.3850, val acc: 0.8722  (best train acc: 0.8720, best val acc: 0.9039)\n",
      "[Epoch: 31980] train loss: 0.5066, train acc: 0.8358, val loss: 0.3549, val acc: 0.8901  (best train acc: 0.8720, best val acc: 0.9039)\n",
      "[Epoch: 32000] train loss: 0.6034, train acc: 0.7983, val loss: 0.3951, val acc: 0.8755  (best train acc: 0.8720, best val acc: 0.9039)\n",
      "[Epoch: 32020] train loss: 0.5235, train acc: 0.8180, val loss: 0.3458, val acc: 0.8762  (best train acc: 0.8720, best val acc: 0.9039)\n",
      "[Epoch: 32040] train loss: 0.5136, train acc: 0.8020, val loss: 0.4151, val acc: 0.8570  (best train acc: 0.8720, best val acc: 0.9039)\n",
      "[Epoch: 32060] train loss: 0.4901, train acc: 0.8344, val loss: 0.3985, val acc: 0.8627  (best train acc: 0.8720, best val acc: 0.9039)\n",
      "[Epoch: 32080] train loss: 0.6092, train acc: 0.7775, val loss: 0.3553, val acc: 0.8954  (best train acc: 0.8720, best val acc: 0.9039)\n",
      "[Epoch: 32100] train loss: 0.5289, train acc: 0.8027, val loss: 0.3676, val acc: 0.8847  (best train acc: 0.8720, best val acc: 0.9039)\n",
      "[Epoch: 32120] train loss: 0.5305, train acc: 0.8000, val loss: 0.4150, val acc: 0.8486  (best train acc: 0.8720, best val acc: 0.9039)\n",
      "[Epoch: 32140] train loss: 0.4949, train acc: 0.8435, val loss: 0.3924, val acc: 0.8823  (best train acc: 0.8720, best val acc: 0.9039)\n",
      "[Epoch: 32160] train loss: 0.4455, train acc: 0.8556, val loss: 0.4129, val acc: 0.8634  (best train acc: 0.8720, best val acc: 0.9039)\n",
      "[Epoch: 32180] train loss: 0.5077, train acc: 0.8188, val loss: 0.3765, val acc: 0.8931  (best train acc: 0.8720, best val acc: 0.9039)\n",
      "[Epoch: 32200] train loss: 0.6246, train acc: 0.7790, val loss: 0.3486, val acc: 0.9005  (best train acc: 0.8720, best val acc: 0.9039)\n",
      "[Epoch: 32220] train loss: 0.4967, train acc: 0.8315, val loss: 0.3603, val acc: 0.8938  (best train acc: 0.8720, best val acc: 0.9039)\n",
      "[Epoch: 32240] train loss: 0.6219, train acc: 0.7740, val loss: 0.3743, val acc: 0.8907  (best train acc: 0.8720, best val acc: 0.9039)\n",
      "[Epoch: 32260] train loss: 0.6022, train acc: 0.7798, val loss: 0.3725, val acc: 0.8867  (best train acc: 0.8720, best val acc: 0.9039)\n",
      "[Epoch: 32280] train loss: 0.4761, train acc: 0.8250, val loss: 0.3586, val acc: 0.8836  (best train acc: 0.8720, best val acc: 0.9039)\n",
      "[Epoch: 32300] train loss: 0.6021, train acc: 0.7922, val loss: 0.3715, val acc: 0.8762  (best train acc: 0.8720, best val acc: 0.9039)\n",
      "[Epoch: 32320] train loss: 0.6212, train acc: 0.7707, val loss: 0.3699, val acc: 0.8786  (best train acc: 0.8720, best val acc: 0.9039)\n",
      "[Epoch: 32340] train loss: 0.5830, train acc: 0.7742, val loss: 0.3592, val acc: 0.8725  (best train acc: 0.8720, best val acc: 0.9039)\n",
      "[Epoch: 32360] train loss: 0.4442, train acc: 0.8539, val loss: 0.4343, val acc: 0.8530  (best train acc: 0.8720, best val acc: 0.9039)\n",
      "[Epoch: 32380] train loss: 0.6555, train acc: 0.7563, val loss: 0.3811, val acc: 0.8809  (best train acc: 0.8720, best val acc: 0.9039)\n",
      "[Epoch: 32400] train loss: 0.5724, train acc: 0.7962, val loss: 0.3580, val acc: 0.8944  (best train acc: 0.8720, best val acc: 0.9039)\n",
      "[Epoch: 32420] train loss: 0.4508, train acc: 0.8449, val loss: 0.3685, val acc: 0.8890  (best train acc: 0.8720, best val acc: 0.9039)\n",
      "[Epoch: 32440] train loss: 0.4770, train acc: 0.8348, val loss: 0.3795, val acc: 0.8691  (best train acc: 0.8720, best val acc: 0.9039)\n",
      "[Epoch: 32460] train loss: 0.4978, train acc: 0.8142, val loss: 0.3699, val acc: 0.8708  (best train acc: 0.8720, best val acc: 0.9039)\n",
      "[Epoch: 32480] train loss: 0.5588, train acc: 0.7928, val loss: 0.4059, val acc: 0.8759  (best train acc: 0.8720, best val acc: 0.9039)\n",
      "[Epoch: 32500] train loss: 0.4614, train acc: 0.8365, val loss: 0.3668, val acc: 0.8901  (best train acc: 0.8720, best val acc: 0.9039)\n",
      "[Epoch: 32520] train loss: 0.5172, train acc: 0.8177, val loss: 0.3453, val acc: 0.8988  (best train acc: 0.8720, best val acc: 0.9039)\n",
      "[Epoch: 32540] train loss: 0.4727, train acc: 0.8450, val loss: 0.3574, val acc: 0.8938  (best train acc: 0.8720, best val acc: 0.9039)\n",
      "[Epoch: 32560] train loss: 0.5078, train acc: 0.8208, val loss: 0.3564, val acc: 0.8796  (best train acc: 0.8720, best val acc: 0.9039)\n",
      "[Epoch: 32580] train loss: 0.4679, train acc: 0.8403, val loss: 0.3529, val acc: 0.8941  (best train acc: 0.8720, best val acc: 0.9039)\n",
      "[Epoch: 32600] train loss: 0.4774, train acc: 0.8345, val loss: 0.3533, val acc: 0.8867  (best train acc: 0.8720, best val acc: 0.9039)\n",
      "[Epoch: 32620] train loss: 0.5219, train acc: 0.8232, val loss: 0.3723, val acc: 0.8776  (best train acc: 0.8720, best val acc: 0.9039)\n",
      "[Epoch: 32640] train loss: 0.5247, train acc: 0.8170, val loss: 0.3811, val acc: 0.8688  (best train acc: 0.8720, best val acc: 0.9039)\n",
      "[Epoch: 32660] train loss: 0.5752, train acc: 0.7964, val loss: 0.3603, val acc: 0.8914  (best train acc: 0.8720, best val acc: 0.9039)\n",
      "[Epoch: 32680] train loss: 0.4747, train acc: 0.8400, val loss: 0.3893, val acc: 0.8698  (best train acc: 0.8720, best val acc: 0.9039)\n",
      "[Epoch: 32700] train loss: 0.5989, train acc: 0.7940, val loss: 0.3942, val acc: 0.8762  (best train acc: 0.8720, best val acc: 0.9039)\n",
      "[Epoch: 32720] train loss: 0.4703, train acc: 0.8323, val loss: 0.3559, val acc: 0.8836  (best train acc: 0.8720, best val acc: 0.9039)\n",
      "[Epoch: 32740] train loss: 0.6079, train acc: 0.7856, val loss: 0.3712, val acc: 0.8850  (best train acc: 0.8720, best val acc: 0.9039)\n",
      "[Epoch: 32760] train loss: 0.5490, train acc: 0.8148, val loss: 0.3500, val acc: 0.8847  (best train acc: 0.8720, best val acc: 0.9039)\n",
      "[Epoch: 32780] train loss: 0.6094, train acc: 0.7839, val loss: 0.3421, val acc: 0.8965  (best train acc: 0.8720, best val acc: 0.9039)\n",
      "[Epoch: 32800] train loss: 0.5111, train acc: 0.8146, val loss: 0.3452, val acc: 0.8894  (best train acc: 0.8720, best val acc: 0.9042)\n",
      "[Epoch: 32820] train loss: 0.4982, train acc: 0.8284, val loss: 0.3362, val acc: 0.9056  (best train acc: 0.8720, best val acc: 0.9056)\n",
      "[Epoch: 32840] train loss: 0.5348, train acc: 0.8169, val loss: 0.3534, val acc: 0.8927  (best train acc: 0.8720, best val acc: 0.9056)\n",
      "[Epoch: 32860] train loss: 0.5146, train acc: 0.8197, val loss: 0.3872, val acc: 0.8884  (best train acc: 0.8720, best val acc: 0.9056)\n",
      "[Epoch: 32880] train loss: 0.4815, train acc: 0.8376, val loss: 0.3557, val acc: 0.8847  (best train acc: 0.8720, best val acc: 0.9056)\n",
      "[Epoch: 32900] train loss: 0.5476, train acc: 0.7993, val loss: 0.3807, val acc: 0.8691  (best train acc: 0.8720, best val acc: 0.9056)\n",
      "[Epoch: 32920] train loss: 0.4399, train acc: 0.8576, val loss: 0.4130, val acc: 0.8486  (best train acc: 0.8720, best val acc: 0.9056)\n",
      "[Epoch: 32940] train loss: 0.5765, train acc: 0.7843, val loss: 0.3327, val acc: 0.8954  (best train acc: 0.8720, best val acc: 0.9056)\n",
      "[Epoch: 32960] train loss: 0.5493, train acc: 0.8124, val loss: 0.3814, val acc: 0.8847  (best train acc: 0.8720, best val acc: 0.9056)\n",
      "[Epoch: 32980] train loss: 0.5554, train acc: 0.8010, val loss: 0.3348, val acc: 0.8971  (best train acc: 0.8720, best val acc: 0.9056)\n",
      "[Epoch: 33000] train loss: 0.5880, train acc: 0.7866, val loss: 0.4042, val acc: 0.8654  (best train acc: 0.8720, best val acc: 0.9056)\n",
      "[Epoch: 33020] train loss: 0.5395, train acc: 0.8055, val loss: 0.3597, val acc: 0.8901  (best train acc: 0.8720, best val acc: 0.9056)\n",
      "[Epoch: 33040] train loss: 0.5116, train acc: 0.8134, val loss: 0.3870, val acc: 0.8772  (best train acc: 0.8720, best val acc: 0.9056)\n",
      "[Epoch: 33060] train loss: 0.6491, train acc: 0.7681, val loss: 0.3797, val acc: 0.8931  (best train acc: 0.8720, best val acc: 0.9056)\n",
      "[Epoch: 33080] train loss: 0.4699, train acc: 0.8493, val loss: 0.3684, val acc: 0.8843  (best train acc: 0.8720, best val acc: 0.9056)\n",
      "[Epoch: 33100] train loss: 0.6199, train acc: 0.7752, val loss: 0.3526, val acc: 0.8907  (best train acc: 0.8720, best val acc: 0.9056)\n",
      "[Epoch: 33120] train loss: 0.4659, train acc: 0.8484, val loss: 0.3401, val acc: 0.8921  (best train acc: 0.8720, best val acc: 0.9056)\n",
      "[Epoch: 33140] train loss: 0.4710, train acc: 0.8353, val loss: 0.3713, val acc: 0.8816  (best train acc: 0.8720, best val acc: 0.9056)\n",
      "[Epoch: 33160] train loss: 0.4559, train acc: 0.8386, val loss: 0.3919, val acc: 0.8705  (best train acc: 0.8720, best val acc: 0.9056)\n",
      "[Epoch: 33180] train loss: 0.4748, train acc: 0.8437, val loss: 0.3609, val acc: 0.8782  (best train acc: 0.8720, best val acc: 0.9056)\n",
      "[Epoch: 33200] train loss: 0.5427, train acc: 0.8082, val loss: 0.3779, val acc: 0.8965  (best train acc: 0.8720, best val acc: 0.9056)\n",
      "[Epoch: 33220] train loss: 0.5282, train acc: 0.8104, val loss: 0.3424, val acc: 0.8975  (best train acc: 0.8720, best val acc: 0.9056)\n",
      "[Epoch: 33240] train loss: 0.4721, train acc: 0.8388, val loss: 0.3294, val acc: 0.8904  (best train acc: 0.8720, best val acc: 0.9056)\n",
      "[Epoch: 33260] train loss: 0.5867, train acc: 0.7871, val loss: 0.3929, val acc: 0.8681  (best train acc: 0.8720, best val acc: 0.9056)\n",
      "[Epoch: 33280] train loss: 0.5753, train acc: 0.7942, val loss: 0.3925, val acc: 0.8749  (best train acc: 0.8720, best val acc: 0.9056)\n",
      "[Epoch: 33300] train loss: 0.5265, train acc: 0.8192, val loss: 0.3714, val acc: 0.8536  (best train acc: 0.8720, best val acc: 0.9056)\n",
      "[Epoch: 33320] train loss: 0.5508, train acc: 0.7926, val loss: 0.3495, val acc: 0.8857  (best train acc: 0.8720, best val acc: 0.9056)\n",
      "[Epoch: 33340] train loss: 0.5947, train acc: 0.7836, val loss: 0.3697, val acc: 0.8793  (best train acc: 0.8720, best val acc: 0.9056)\n",
      "[Epoch: 33360] train loss: 0.5703, train acc: 0.7931, val loss: 0.3851, val acc: 0.8894  (best train acc: 0.8720, best val acc: 0.9056)\n",
      "[Epoch: 33380] train loss: 0.5000, train acc: 0.8274, val loss: 0.3392, val acc: 0.8981  (best train acc: 0.8720, best val acc: 0.9056)\n",
      "[Epoch: 33400] train loss: 0.5900, train acc: 0.7846, val loss: 0.3656, val acc: 0.8958  (best train acc: 0.8720, best val acc: 0.9056)\n",
      "[Epoch: 33420] train loss: 0.4442, train acc: 0.8586, val loss: 0.3794, val acc: 0.8769  (best train acc: 0.8720, best val acc: 0.9056)\n",
      "[Epoch: 33440] train loss: 0.4495, train acc: 0.8391, val loss: 0.3958, val acc: 0.8708  (best train acc: 0.8720, best val acc: 0.9056)\n",
      "[Epoch: 33460] train loss: 0.5980, train acc: 0.7843, val loss: 0.3390, val acc: 0.8985  (best train acc: 0.8720, best val acc: 0.9056)\n",
      "[Epoch: 33480] train loss: 0.5678, train acc: 0.7937, val loss: 0.3441, val acc: 0.8887  (best train acc: 0.8720, best val acc: 0.9056)\n",
      "[Epoch: 33500] train loss: 0.6096, train acc: 0.7811, val loss: 0.3643, val acc: 0.8894  (best train acc: 0.8720, best val acc: 0.9056)\n",
      "[Epoch: 33520] train loss: 0.4548, train acc: 0.8517, val loss: 0.3944, val acc: 0.8691  (best train acc: 0.8720, best val acc: 0.9062)\n",
      "[Epoch: 33540] train loss: 0.6639, train acc: 0.7460, val loss: 0.3389, val acc: 0.8978  (best train acc: 0.8720, best val acc: 0.9062)\n",
      "[Epoch: 33560] train loss: 0.4843, train acc: 0.8276, val loss: 0.3606, val acc: 0.8793  (best train acc: 0.8720, best val acc: 0.9062)\n",
      "[Epoch: 33580] train loss: 0.4884, train acc: 0.8417, val loss: 0.3974, val acc: 0.8712  (best train acc: 0.8720, best val acc: 0.9062)\n",
      "[Epoch: 33600] train loss: 0.5044, train acc: 0.8287, val loss: 0.4112, val acc: 0.8509  (best train acc: 0.8720, best val acc: 0.9062)\n",
      "[Epoch: 33620] train loss: 0.5923, train acc: 0.7848, val loss: 0.4103, val acc: 0.8874  (best train acc: 0.8720, best val acc: 0.9062)\n",
      "[Epoch: 33640] train loss: 0.6135, train acc: 0.7700, val loss: 0.4013, val acc: 0.8560  (best train acc: 0.8720, best val acc: 0.9062)\n",
      "[Epoch: 33660] train loss: 0.5356, train acc: 0.7926, val loss: 0.3929, val acc: 0.8654  (best train acc: 0.8720, best val acc: 0.9062)\n",
      "[Epoch: 33680] train loss: 0.5090, train acc: 0.8310, val loss: 0.3590, val acc: 0.8924  (best train acc: 0.8720, best val acc: 0.9062)\n",
      "[Epoch: 33700] train loss: 0.4497, train acc: 0.8350, val loss: 0.3686, val acc: 0.8769  (best train acc: 0.8720, best val acc: 0.9062)\n",
      "[Epoch: 33720] train loss: 0.5023, train acc: 0.8313, val loss: 0.3594, val acc: 0.8793  (best train acc: 0.8720, best val acc: 0.9062)\n",
      "[Epoch: 33740] train loss: 0.5389, train acc: 0.7946, val loss: 0.3223, val acc: 0.8931  (best train acc: 0.8720, best val acc: 0.9062)\n",
      "[Epoch: 33760] train loss: 0.5160, train acc: 0.8050, val loss: 0.4372, val acc: 0.8061  (best train acc: 0.8720, best val acc: 0.9062)\n",
      "[Epoch: 33780] train loss: 0.4728, train acc: 0.8325, val loss: 0.3840, val acc: 0.8718  (best train acc: 0.8720, best val acc: 0.9062)\n",
      "[Epoch: 33800] train loss: 0.4501, train acc: 0.8546, val loss: 0.3685, val acc: 0.8830  (best train acc: 0.8720, best val acc: 0.9062)\n",
      "[Epoch: 33820] train loss: 0.6308, train acc: 0.7668, val loss: 0.3531, val acc: 0.8850  (best train acc: 0.8720, best val acc: 0.9062)\n",
      "[Epoch: 33840] train loss: 0.5090, train acc: 0.8102, val loss: 0.3769, val acc: 0.8671  (best train acc: 0.8720, best val acc: 0.9062)\n",
      "[Epoch: 33860] train loss: 0.4776, train acc: 0.8187, val loss: 0.3377, val acc: 0.8914  (best train acc: 0.8720, best val acc: 0.9062)\n",
      "[Epoch: 33880] train loss: 0.5592, train acc: 0.8034, val loss: 0.3433, val acc: 0.8894  (best train acc: 0.8720, best val acc: 0.9062)\n",
      "[Epoch: 33900] train loss: 0.5812, train acc: 0.7861, val loss: 0.3669, val acc: 0.8857  (best train acc: 0.8720, best val acc: 0.9062)\n",
      "[Epoch: 33920] train loss: 0.4709, train acc: 0.8326, val loss: 0.3733, val acc: 0.8762  (best train acc: 0.8720, best val acc: 0.9062)\n",
      "[Epoch: 33940] train loss: 0.5241, train acc: 0.8179, val loss: 0.3605, val acc: 0.8816  (best train acc: 0.8720, best val acc: 0.9062)\n",
      "[Epoch: 33960] train loss: 0.6427, train acc: 0.7535, val loss: 0.3670, val acc: 0.8897  (best train acc: 0.8720, best val acc: 0.9062)\n",
      "[Epoch: 33980] train loss: 0.5837, train acc: 0.7820, val loss: 0.3332, val acc: 0.8954  (best train acc: 0.8720, best val acc: 0.9062)\n",
      "[Epoch: 34000] train loss: 0.4829, train acc: 0.8415, val loss: 0.3601, val acc: 0.8870  (best train acc: 0.8720, best val acc: 0.9062)\n",
      "[Epoch: 34020] train loss: 0.6912, train acc: 0.7467, val loss: 0.3340, val acc: 0.9042  (best train acc: 0.8720, best val acc: 0.9062)\n",
      "[Epoch: 34040] train loss: 0.4789, train acc: 0.8408, val loss: 0.3604, val acc: 0.8722  (best train acc: 0.8720, best val acc: 0.9062)\n",
      "[Epoch: 34060] train loss: 0.6445, train acc: 0.7587, val loss: 0.3414, val acc: 0.8968  (best train acc: 0.8720, best val acc: 0.9062)\n",
      "[Epoch: 34080] train loss: 0.6415, train acc: 0.7693, val loss: 0.3448, val acc: 0.8843  (best train acc: 0.8720, best val acc: 0.9062)\n",
      "[Epoch: 34100] train loss: 0.6384, train acc: 0.7825, val loss: 0.3225, val acc: 0.8877  (best train acc: 0.8720, best val acc: 0.9062)\n",
      "[Epoch: 34120] train loss: 0.5982, train acc: 0.7704, val loss: 0.4077, val acc: 0.8641  (best train acc: 0.8720, best val acc: 0.9062)\n",
      "[Epoch: 34140] train loss: 0.6591, train acc: 0.7601, val loss: 0.3682, val acc: 0.8833  (best train acc: 0.8720, best val acc: 0.9062)\n",
      "[Epoch: 34160] train loss: 0.5597, train acc: 0.7973, val loss: 0.3713, val acc: 0.8688  (best train acc: 0.8720, best val acc: 0.9062)\n",
      "[Epoch: 34180] train loss: 0.5524, train acc: 0.7922, val loss: 0.3472, val acc: 0.8911  (best train acc: 0.8720, best val acc: 0.9062)\n",
      "[Epoch: 34200] train loss: 0.4376, train acc: 0.8562, val loss: 0.3693, val acc: 0.8877  (best train acc: 0.8720, best val acc: 0.9062)\n",
      "[Epoch: 34220] train loss: 0.4994, train acc: 0.8281, val loss: 0.3979, val acc: 0.8850  (best train acc: 0.8720, best val acc: 0.9062)\n",
      "[Epoch: 34240] train loss: 0.5743, train acc: 0.8021, val loss: 0.3546, val acc: 0.8877  (best train acc: 0.8720, best val acc: 0.9062)\n",
      "[Epoch: 34260] train loss: 0.7452, train acc: 0.7157, val loss: 0.3536, val acc: 0.8941  (best train acc: 0.8720, best val acc: 0.9062)\n",
      "[Epoch: 34280] train loss: 0.5888, train acc: 0.7973, val loss: 0.3620, val acc: 0.8793  (best train acc: 0.8720, best val acc: 0.9062)\n",
      "[Epoch: 34300] train loss: 0.5067, train acc: 0.8210, val loss: 0.3391, val acc: 0.8951  (best train acc: 0.8720, best val acc: 0.9062)\n",
      "[Epoch: 34320] train loss: 0.4694, train acc: 0.8509, val loss: 0.3737, val acc: 0.8914  (best train acc: 0.8720, best val acc: 0.9062)\n",
      "[Epoch: 34340] train loss: 0.4920, train acc: 0.8402, val loss: 0.3595, val acc: 0.8904  (best train acc: 0.8720, best val acc: 0.9062)\n",
      "[Epoch: 34360] train loss: 0.6663, train acc: 0.7392, val loss: 0.3331, val acc: 0.8941  (best train acc: 0.8720, best val acc: 0.9062)\n",
      "[Epoch: 34380] train loss: 0.6430, train acc: 0.7557, val loss: 0.3535, val acc: 0.8961  (best train acc: 0.8720, best val acc: 0.9062)\n",
      "[Epoch: 34400] train loss: 0.5364, train acc: 0.8170, val loss: 0.3514, val acc: 0.8901  (best train acc: 0.8720, best val acc: 0.9062)\n",
      "[Epoch: 34420] train loss: 0.5914, train acc: 0.7900, val loss: 0.3540, val acc: 0.8911  (best train acc: 0.8720, best val acc: 0.9062)\n",
      "[Epoch: 34440] train loss: 0.5035, train acc: 0.8277, val loss: 0.3508, val acc: 0.8826  (best train acc: 0.8720, best val acc: 0.9062)\n",
      "[Epoch: 34460] train loss: 0.6291, train acc: 0.7739, val loss: 0.3760, val acc: 0.8911  (best train acc: 0.8720, best val acc: 0.9062)\n",
      "[Epoch: 34480] train loss: 0.5279, train acc: 0.8302, val loss: 0.3656, val acc: 0.8860  (best train acc: 0.8720, best val acc: 0.9062)\n",
      "[Epoch: 34500] train loss: 0.4941, train acc: 0.8261, val loss: 0.3468, val acc: 0.8867  (best train acc: 0.8720, best val acc: 0.9062)\n",
      "[Epoch: 34520] train loss: 0.6951, train acc: 0.7370, val loss: 0.3491, val acc: 0.8830  (best train acc: 0.8720, best val acc: 0.9062)\n",
      "[Epoch: 34540] train loss: 0.4637, train acc: 0.8446, val loss: 0.3479, val acc: 0.8931  (best train acc: 0.8720, best val acc: 0.9062)\n",
      "[Epoch: 34560] train loss: 0.5701, train acc: 0.7933, val loss: 0.3606, val acc: 0.8796  (best train acc: 0.8720, best val acc: 0.9062)\n",
      "[Epoch: 34580] train loss: 0.4851, train acc: 0.8362, val loss: 0.3633, val acc: 0.8867  (best train acc: 0.8720, best val acc: 0.9062)\n",
      "[Epoch: 34600] train loss: 0.5485, train acc: 0.8061, val loss: 0.3481, val acc: 0.8840  (best train acc: 0.8720, best val acc: 0.9062)\n",
      "[Epoch: 34620] train loss: 0.4907, train acc: 0.8230, val loss: 0.3650, val acc: 0.8759  (best train acc: 0.8720, best val acc: 0.9062)\n",
      "[Epoch: 34640] train loss: 0.6008, train acc: 0.7728, val loss: 0.3711, val acc: 0.8951  (best train acc: 0.8720, best val acc: 0.9062)\n",
      "[Epoch: 34660] train loss: 0.4566, train acc: 0.8394, val loss: 0.3678, val acc: 0.8803  (best train acc: 0.8720, best val acc: 0.9062)\n",
      "[Epoch: 34680] train loss: 0.4227, train acc: 0.8581, val loss: 0.3837, val acc: 0.8755  (best train acc: 0.8720, best val acc: 0.9062)\n",
      "[Epoch: 34700] train loss: 0.6311, train acc: 0.7702, val loss: 0.3631, val acc: 0.8820  (best train acc: 0.8720, best val acc: 0.9062)\n",
      "[Epoch: 34720] train loss: 0.5476, train acc: 0.7911, val loss: 0.4235, val acc: 0.8540  (best train acc: 0.8720, best val acc: 0.9062)\n",
      "[Epoch: 34740] train loss: 0.4342, train acc: 0.8604, val loss: 0.3686, val acc: 0.8752  (best train acc: 0.8720, best val acc: 0.9062)\n",
      "[Epoch: 34760] train loss: 0.5858, train acc: 0.7862, val loss: 0.3562, val acc: 0.8796  (best train acc: 0.8720, best val acc: 0.9062)\n",
      "[Epoch: 34780] train loss: 0.4614, train acc: 0.8463, val loss: 0.3383, val acc: 0.9049  (best train acc: 0.8720, best val acc: 0.9062)\n",
      "[Epoch: 34800] train loss: 0.4931, train acc: 0.8175, val loss: 0.3576, val acc: 0.8833  (best train acc: 0.8720, best val acc: 0.9062)\n",
      "[Epoch: 34820] train loss: 0.4691, train acc: 0.8493, val loss: 0.3695, val acc: 0.8914  (best train acc: 0.8720, best val acc: 0.9062)\n",
      "[Epoch: 34840] train loss: 0.6030, train acc: 0.7838, val loss: 0.3683, val acc: 0.8904  (best train acc: 0.8720, best val acc: 0.9062)\n",
      "[Epoch: 34860] train loss: 0.4825, train acc: 0.8145, val loss: 0.3639, val acc: 0.8880  (best train acc: 0.8720, best val acc: 0.9062)\n",
      "[Epoch: 34880] train loss: 0.4878, train acc: 0.8344, val loss: 0.3588, val acc: 0.8863  (best train acc: 0.8720, best val acc: 0.9062)\n",
      "[Epoch: 34900] train loss: 0.5505, train acc: 0.7823, val loss: 0.3396, val acc: 0.8880  (best train acc: 0.8720, best val acc: 0.9062)\n",
      "[Epoch: 34920] train loss: 0.6017, train acc: 0.7710, val loss: 0.3587, val acc: 0.8874  (best train acc: 0.8720, best val acc: 0.9062)\n",
      "[Epoch: 34940] train loss: 0.4696, train acc: 0.8376, val loss: 0.3400, val acc: 0.8901  (best train acc: 0.8720, best val acc: 0.9062)\n",
      "[Epoch: 34960] train loss: 0.5158, train acc: 0.8134, val loss: 0.3919, val acc: 0.8695  (best train acc: 0.8720, best val acc: 0.9062)\n",
      "[Epoch: 34980] train loss: 0.5538, train acc: 0.8030, val loss: 0.3551, val acc: 0.8806  (best train acc: 0.8720, best val acc: 0.9062)\n",
      "[Epoch: 35000] train loss: 0.5586, train acc: 0.7937, val loss: 0.3840, val acc: 0.8749  (best train acc: 0.8720, best val acc: 0.9062)\n",
      "[Epoch: 35020] train loss: 0.4287, train acc: 0.8488, val loss: 0.4189, val acc: 0.8405  (best train acc: 0.8720, best val acc: 0.9062)\n",
      "[Epoch: 35040] train loss: 0.4787, train acc: 0.8347, val loss: 0.3707, val acc: 0.8755  (best train acc: 0.8720, best val acc: 0.9062)\n",
      "[Epoch: 35060] train loss: 0.5034, train acc: 0.8194, val loss: 0.3643, val acc: 0.8759  (best train acc: 0.8720, best val acc: 0.9062)\n",
      "[Epoch: 35080] train loss: 0.4825, train acc: 0.8391, val loss: 0.3547, val acc: 0.8809  (best train acc: 0.8720, best val acc: 0.9062)\n",
      "[Epoch: 35100] train loss: 0.4821, train acc: 0.8202, val loss: 0.3925, val acc: 0.8702  (best train acc: 0.8720, best val acc: 0.9062)\n",
      "[Epoch: 35120] train loss: 0.4702, train acc: 0.8206, val loss: 0.3875, val acc: 0.8681  (best train acc: 0.8720, best val acc: 0.9062)\n",
      "[Epoch: 35140] train loss: 0.4683, train acc: 0.8454, val loss: 0.3597, val acc: 0.8833  (best train acc: 0.8720, best val acc: 0.9062)\n",
      "[Epoch: 35160] train loss: 0.5560, train acc: 0.8036, val loss: 0.3516, val acc: 0.8897  (best train acc: 0.8720, best val acc: 0.9062)\n",
      "[Epoch: 35180] train loss: 0.5661, train acc: 0.7966, val loss: 0.4065, val acc: 0.8637  (best train acc: 0.8720, best val acc: 0.9062)\n",
      "[Epoch: 35200] train loss: 0.5832, train acc: 0.7963, val loss: 0.3777, val acc: 0.8857  (best train acc: 0.8720, best val acc: 0.9062)\n",
      "[Epoch: 35220] train loss: 0.4786, train acc: 0.8097, val loss: 0.3558, val acc: 0.8927  (best train acc: 0.8720, best val acc: 0.9062)\n",
      "[Epoch: 35240] train loss: 0.4696, train acc: 0.8434, val loss: 0.3635, val acc: 0.8826  (best train acc: 0.8720, best val acc: 0.9062)\n",
      "[Epoch: 35260] train loss: 0.5059, train acc: 0.8164, val loss: 0.3935, val acc: 0.8745  (best train acc: 0.8720, best val acc: 0.9062)\n",
      "[Epoch: 35280] train loss: 0.6255, train acc: 0.7664, val loss: 0.3857, val acc: 0.8766  (best train acc: 0.8720, best val acc: 0.9062)\n",
      "[Epoch: 35300] train loss: 0.5618, train acc: 0.8011, val loss: 0.3642, val acc: 0.8840  (best train acc: 0.8720, best val acc: 0.9062)\n",
      "[Epoch: 35320] train loss: 0.4674, train acc: 0.8458, val loss: 0.3366, val acc: 0.8826  (best train acc: 0.8720, best val acc: 0.9062)\n",
      "[Epoch: 35340] train loss: 0.5925, train acc: 0.7891, val loss: 0.3810, val acc: 0.8617  (best train acc: 0.8720, best val acc: 0.9062)\n",
      "[Epoch: 35360] train loss: 0.5725, train acc: 0.7923, val loss: 0.3923, val acc: 0.8813  (best train acc: 0.8720, best val acc: 0.9062)\n",
      "[Epoch: 35380] train loss: 0.5082, train acc: 0.8244, val loss: 0.4752, val acc: 0.8246  (best train acc: 0.8720, best val acc: 0.9062)\n",
      "[Epoch: 35400] train loss: 0.5908, train acc: 0.7786, val loss: 0.4031, val acc: 0.8702  (best train acc: 0.8720, best val acc: 0.9062)\n",
      "[Epoch: 35420] train loss: 0.5329, train acc: 0.8122, val loss: 0.3862, val acc: 0.8759  (best train acc: 0.8720, best val acc: 0.9062)\n",
      "[Epoch: 35440] train loss: 0.5018, train acc: 0.8242, val loss: 0.3714, val acc: 0.8911  (best train acc: 0.8720, best val acc: 0.9062)\n",
      "[Epoch: 35460] train loss: 0.4927, train acc: 0.8289, val loss: 0.3635, val acc: 0.8894  (best train acc: 0.8720, best val acc: 0.9062)\n",
      "[Epoch: 35480] train loss: 0.5526, train acc: 0.7919, val loss: 0.3609, val acc: 0.8870  (best train acc: 0.8720, best val acc: 0.9062)\n",
      "[Epoch: 35500] train loss: 0.4633, train acc: 0.8485, val loss: 0.3705, val acc: 0.8759  (best train acc: 0.8720, best val acc: 0.9062)\n",
      "[Epoch: 35520] train loss: 0.4736, train acc: 0.8379, val loss: 0.3639, val acc: 0.8992  (best train acc: 0.8720, best val acc: 0.9062)\n",
      "[Epoch: 35540] train loss: 0.6831, train acc: 0.7398, val loss: 0.3628, val acc: 0.8941  (best train acc: 0.8720, best val acc: 0.9062)\n",
      "[Epoch: 35560] train loss: 0.5512, train acc: 0.8102, val loss: 0.3866, val acc: 0.8863  (best train acc: 0.8720, best val acc: 0.9062)\n",
      "[Epoch: 35580] train loss: 0.4680, train acc: 0.8214, val loss: 0.3627, val acc: 0.8813  (best train acc: 0.8720, best val acc: 0.9062)\n",
      "[Epoch: 35600] train loss: 0.4678, train acc: 0.8427, val loss: 0.3383, val acc: 0.8954  (best train acc: 0.8720, best val acc: 0.9062)\n",
      "[Epoch: 35620] train loss: 0.4695, train acc: 0.8216, val loss: 0.3483, val acc: 0.8874  (best train acc: 0.8720, best val acc: 0.9062)\n",
      "[Epoch: 35640] train loss: 0.5598, train acc: 0.7940, val loss: 0.3745, val acc: 0.8779  (best train acc: 0.8720, best val acc: 0.9062)\n",
      "[Epoch: 35660] train loss: 0.4597, train acc: 0.8439, val loss: 0.3522, val acc: 0.8809  (best train acc: 0.8720, best val acc: 0.9062)\n",
      "[Epoch: 35680] train loss: 0.4588, train acc: 0.8220, val loss: 0.3354, val acc: 0.8992  (best train acc: 0.8720, best val acc: 0.9062)\n",
      "[Epoch: 35700] train loss: 0.5285, train acc: 0.8148, val loss: 0.3424, val acc: 0.8857  (best train acc: 0.8720, best val acc: 0.9062)\n",
      "[Epoch: 35720] train loss: 0.4308, train acc: 0.8664, val loss: 0.3702, val acc: 0.8793  (best train acc: 0.8720, best val acc: 0.9062)\n",
      "[Epoch: 35740] train loss: 0.4286, train acc: 0.8630, val loss: 0.3925, val acc: 0.8782  (best train acc: 0.8720, best val acc: 0.9062)\n",
      "[Epoch: 35760] train loss: 0.4784, train acc: 0.8439, val loss: 0.3453, val acc: 0.9019  (best train acc: 0.8720, best val acc: 0.9062)\n",
      "[Epoch: 35780] train loss: 0.4893, train acc: 0.8370, val loss: 0.3380, val acc: 0.8965  (best train acc: 0.8720, best val acc: 0.9062)\n",
      "[Epoch: 35800] train loss: 0.4959, train acc: 0.8204, val loss: 0.3659, val acc: 0.8813  (best train acc: 0.8720, best val acc: 0.9062)\n",
      "[Epoch: 35820] train loss: 0.6253, train acc: 0.7726, val loss: 0.3583, val acc: 0.8948  (best train acc: 0.8720, best val acc: 0.9062)\n",
      "[Epoch: 35840] train loss: 0.5950, train acc: 0.7838, val loss: 0.3514, val acc: 0.8901  (best train acc: 0.8720, best val acc: 0.9062)\n",
      "[Epoch: 35860] train loss: 0.5595, train acc: 0.8159, val loss: 0.3943, val acc: 0.8806  (best train acc: 0.8720, best val acc: 0.9062)\n",
      "[Epoch: 35880] train loss: 0.5002, train acc: 0.8222, val loss: 0.3507, val acc: 0.8904  (best train acc: 0.8720, best val acc: 0.9062)\n",
      "[Epoch: 35900] train loss: 0.6544, train acc: 0.7389, val loss: 0.3630, val acc: 0.8975  (best train acc: 0.8720, best val acc: 0.9062)\n",
      "[Epoch: 35920] train loss: 0.4653, train acc: 0.8254, val loss: 0.3697, val acc: 0.8739  (best train acc: 0.8720, best val acc: 0.9062)\n",
      "[Epoch: 35940] train loss: 0.4578, train acc: 0.8527, val loss: 0.3911, val acc: 0.8637  (best train acc: 0.8720, best val acc: 0.9062)\n",
      "[Epoch: 35960] train loss: 0.4579, train acc: 0.8412, val loss: 0.3604, val acc: 0.8914  (best train acc: 0.8720, best val acc: 0.9062)\n",
      "[Epoch: 35980] train loss: 0.4911, train acc: 0.8392, val loss: 0.3725, val acc: 0.8884  (best train acc: 0.8720, best val acc: 0.9062)\n",
      "[Epoch: 36000] train loss: 0.4542, train acc: 0.8339, val loss: 0.3363, val acc: 0.9008  (best train acc: 0.8720, best val acc: 0.9062)\n",
      "[Epoch: 36020] train loss: 0.4617, train acc: 0.8514, val loss: 0.3687, val acc: 0.8938  (best train acc: 0.8720, best val acc: 0.9062)\n",
      "[Epoch: 36040] train loss: 0.4658, train acc: 0.8347, val loss: 0.3509, val acc: 0.8816  (best train acc: 0.8720, best val acc: 0.9062)\n",
      "[Epoch: 36060] train loss: 0.5037, train acc: 0.8110, val loss: 0.3701, val acc: 0.8762  (best train acc: 0.8720, best val acc: 0.9062)\n",
      "[Epoch: 36080] train loss: 0.5200, train acc: 0.8097, val loss: 0.3478, val acc: 0.9005  (best train acc: 0.8720, best val acc: 0.9062)\n",
      "[Epoch: 36100] train loss: 0.5827, train acc: 0.7848, val loss: 0.3545, val acc: 0.8887  (best train acc: 0.8720, best val acc: 0.9073)\n",
      "[Epoch: 36120] train loss: 0.5555, train acc: 0.8010, val loss: 0.3597, val acc: 0.8820  (best train acc: 0.8720, best val acc: 0.9073)\n",
      "[Epoch: 36140] train loss: 0.4872, train acc: 0.8337, val loss: 0.3809, val acc: 0.8840  (best train acc: 0.8720, best val acc: 0.9073)\n",
      "[Epoch: 36160] train loss: 0.4457, train acc: 0.8536, val loss: 0.3370, val acc: 0.8965  (best train acc: 0.8720, best val acc: 0.9073)\n",
      "[Epoch: 36180] train loss: 0.4567, train acc: 0.8540, val loss: 0.3674, val acc: 0.8907  (best train acc: 0.8720, best val acc: 0.9073)\n",
      "[Epoch: 36200] train loss: 0.4206, train acc: 0.8645, val loss: 0.3611, val acc: 0.8890  (best train acc: 0.8720, best val acc: 0.9073)\n",
      "[Epoch: 36220] train loss: 0.4505, train acc: 0.8417, val loss: 0.3749, val acc: 0.8735  (best train acc: 0.8720, best val acc: 0.9073)\n",
      "[Epoch: 36240] train loss: 0.7180, train acc: 0.7097, val loss: 0.3838, val acc: 0.8840  (best train acc: 0.8720, best val acc: 0.9073)\n",
      "[Epoch: 36260] train loss: 0.5614, train acc: 0.7928, val loss: 0.4009, val acc: 0.8624  (best train acc: 0.8720, best val acc: 0.9073)\n",
      "[Epoch: 36280] train loss: 0.4411, train acc: 0.8495, val loss: 0.3552, val acc: 0.8860  (best train acc: 0.8720, best val acc: 0.9073)\n",
      "[Epoch: 36300] train loss: 0.5366, train acc: 0.8248, val loss: 0.3809, val acc: 0.8728  (best train acc: 0.8720, best val acc: 0.9073)\n",
      "[Epoch: 36320] train loss: 0.6362, train acc: 0.7606, val loss: 0.3822, val acc: 0.8732  (best train acc: 0.8720, best val acc: 0.9073)\n",
      "[Epoch: 36340] train loss: 0.4418, train acc: 0.8357, val loss: 0.3646, val acc: 0.8755  (best train acc: 0.8720, best val acc: 0.9073)\n",
      "[Epoch: 36360] train loss: 0.4892, train acc: 0.8305, val loss: 0.3575, val acc: 0.8833  (best train acc: 0.8720, best val acc: 0.9073)\n",
      "[Epoch: 36380] train loss: 0.5260, train acc: 0.8097, val loss: 0.3478, val acc: 0.8826  (best train acc: 0.8720, best val acc: 0.9073)\n",
      "[Epoch: 36400] train loss: 0.4923, train acc: 0.8365, val loss: 0.3773, val acc: 0.8742  (best train acc: 0.8720, best val acc: 0.9073)\n",
      "[Epoch: 36420] train loss: 0.4685, train acc: 0.8263, val loss: 0.3417, val acc: 0.8833  (best train acc: 0.8720, best val acc: 0.9073)\n",
      "[Epoch: 36440] train loss: 0.5740, train acc: 0.7810, val loss: 0.3590, val acc: 0.8877  (best train acc: 0.8720, best val acc: 0.9073)\n",
      "[Epoch: 36460] train loss: 0.5332, train acc: 0.7971, val loss: 0.3583, val acc: 0.9015  (best train acc: 0.8720, best val acc: 0.9073)\n",
      "[Epoch: 36480] train loss: 0.5730, train acc: 0.7967, val loss: 0.3431, val acc: 0.8867  (best train acc: 0.8720, best val acc: 0.9073)\n",
      "[Epoch: 36500] train loss: 0.4886, train acc: 0.8297, val loss: 0.3847, val acc: 0.8782  (best train acc: 0.8720, best val acc: 0.9073)\n",
      "[Epoch: 36520] train loss: 0.5081, train acc: 0.8299, val loss: 0.3439, val acc: 0.8887  (best train acc: 0.8720, best val acc: 0.9073)\n",
      "[Epoch: 36540] train loss: 0.4952, train acc: 0.8216, val loss: 0.3478, val acc: 0.8934  (best train acc: 0.8720, best val acc: 0.9073)\n",
      "[Epoch: 36560] train loss: 0.4787, train acc: 0.8290, val loss: 0.3775, val acc: 0.8755  (best train acc: 0.8720, best val acc: 0.9073)\n",
      "[Epoch: 36580] train loss: 0.5620, train acc: 0.7867, val loss: 0.3465, val acc: 0.8745  (best train acc: 0.8720, best val acc: 0.9073)\n",
      "[Epoch: 36600] train loss: 0.4867, train acc: 0.8343, val loss: 0.3592, val acc: 0.8749  (best train acc: 0.8720, best val acc: 0.9073)\n",
      "[Epoch: 36620] train loss: 0.5757, train acc: 0.7916, val loss: 0.3910, val acc: 0.8698  (best train acc: 0.8720, best val acc: 0.9073)\n",
      "[Epoch: 36640] train loss: 0.4224, train acc: 0.8611, val loss: 0.3414, val acc: 0.8894  (best train acc: 0.8720, best val acc: 0.9073)\n",
      "[Epoch: 36660] train loss: 0.4779, train acc: 0.8278, val loss: 0.3345, val acc: 0.8988  (best train acc: 0.8720, best val acc: 0.9073)\n",
      "[Epoch: 36680] train loss: 0.6227, train acc: 0.7580, val loss: 0.3311, val acc: 0.8981  (best train acc: 0.8720, best val acc: 0.9073)\n",
      "[Epoch: 36700] train loss: 0.6022, train acc: 0.7866, val loss: 0.3643, val acc: 0.8890  (best train acc: 0.8720, best val acc: 0.9073)\n",
      "[Epoch: 36720] train loss: 0.4634, train acc: 0.8466, val loss: 0.3827, val acc: 0.8641  (best train acc: 0.8720, best val acc: 0.9073)\n",
      "[Epoch: 36740] train loss: 0.5078, train acc: 0.8211, val loss: 0.3672, val acc: 0.8772  (best train acc: 0.8720, best val acc: 0.9073)\n",
      "[Epoch: 36760] train loss: 0.4554, train acc: 0.8441, val loss: 0.3760, val acc: 0.8651  (best train acc: 0.8720, best val acc: 0.9073)\n",
      "[Epoch: 36780] train loss: 0.4887, train acc: 0.8255, val loss: 0.3487, val acc: 0.8901  (best train acc: 0.8720, best val acc: 0.9073)\n",
      "[Epoch: 36800] train loss: 0.4952, train acc: 0.8282, val loss: 0.4001, val acc: 0.8621  (best train acc: 0.8720, best val acc: 0.9073)\n",
      "[Epoch: 36820] train loss: 0.4861, train acc: 0.8381, val loss: 0.3765, val acc: 0.8840  (best train acc: 0.8720, best val acc: 0.9073)\n",
      "[Epoch: 36840] train loss: 0.5976, train acc: 0.7689, val loss: 0.3499, val acc: 0.9022  (best train acc: 0.8775, best val acc: 0.9073)\n",
      "[Epoch: 36860] train loss: 0.5408, train acc: 0.8154, val loss: 0.3319, val acc: 0.8975  (best train acc: 0.8775, best val acc: 0.9073)\n",
      "[Epoch: 36880] train loss: 0.5027, train acc: 0.8148, val loss: 0.3466, val acc: 0.8850  (best train acc: 0.8775, best val acc: 0.9073)\n",
      "[Epoch: 36900] train loss: 0.5627, train acc: 0.7849, val loss: 0.3444, val acc: 0.8961  (best train acc: 0.8775, best val acc: 0.9073)\n",
      "[Epoch: 36920] train loss: 0.4372, train acc: 0.8615, val loss: 0.3413, val acc: 0.8954  (best train acc: 0.8775, best val acc: 0.9073)\n",
      "[Epoch: 36940] train loss: 0.5412, train acc: 0.8013, val loss: 0.3322, val acc: 0.8884  (best train acc: 0.8775, best val acc: 0.9073)\n",
      "[Epoch: 36960] train loss: 0.4599, train acc: 0.8404, val loss: 0.4061, val acc: 0.8563  (best train acc: 0.8775, best val acc: 0.9073)\n",
      "[Epoch: 36980] train loss: 0.4594, train acc: 0.8340, val loss: 0.3365, val acc: 0.8944  (best train acc: 0.8775, best val acc: 0.9073)\n",
      "[Epoch: 37000] train loss: 0.7199, train acc: 0.7240, val loss: 0.3292, val acc: 0.8769  (best train acc: 0.8775, best val acc: 0.9073)\n",
      "[Epoch: 37020] train loss: 0.4953, train acc: 0.8227, val loss: 0.3721, val acc: 0.8853  (best train acc: 0.8775, best val acc: 0.9073)\n",
      "[Epoch: 37040] train loss: 0.5332, train acc: 0.8066, val loss: 0.3495, val acc: 0.8860  (best train acc: 0.8775, best val acc: 0.9073)\n",
      "[Epoch: 37060] train loss: 0.4740, train acc: 0.8194, val loss: 0.3950, val acc: 0.8637  (best train acc: 0.8775, best val acc: 0.9073)\n",
      "[Epoch: 37080] train loss: 0.5886, train acc: 0.7799, val loss: 0.3836, val acc: 0.8762  (best train acc: 0.8775, best val acc: 0.9073)\n",
      "[Epoch: 37100] train loss: 0.5658, train acc: 0.7936, val loss: 0.3288, val acc: 0.8954  (best train acc: 0.8775, best val acc: 0.9099)\n",
      "[Epoch: 37120] train loss: 0.5591, train acc: 0.8059, val loss: 0.3824, val acc: 0.8732  (best train acc: 0.8775, best val acc: 0.9099)\n",
      "[Epoch: 37140] train loss: 0.6920, train acc: 0.7222, val loss: 0.3509, val acc: 0.8988  (best train acc: 0.8775, best val acc: 0.9099)\n",
      "[Epoch: 37160] train loss: 0.5639, train acc: 0.7922, val loss: 0.3673, val acc: 0.8867  (best train acc: 0.8775, best val acc: 0.9099)\n",
      "[Epoch: 37180] train loss: 0.4419, train acc: 0.8590, val loss: 0.3346, val acc: 0.8938  (best train acc: 0.8775, best val acc: 0.9099)\n",
      "[Epoch: 37200] train loss: 0.5524, train acc: 0.8211, val loss: 0.3730, val acc: 0.8803  (best train acc: 0.8775, best val acc: 0.9099)\n",
      "[Epoch: 37220] train loss: 0.5392, train acc: 0.7877, val loss: 0.3265, val acc: 0.8877  (best train acc: 0.8775, best val acc: 0.9099)\n",
      "[Epoch: 37240] train loss: 0.5037, train acc: 0.8339, val loss: 0.3530, val acc: 0.8786  (best train acc: 0.8775, best val acc: 0.9099)\n",
      "[Epoch: 37260] train loss: 0.4982, train acc: 0.8125, val loss: 0.3397, val acc: 0.8961  (best train acc: 0.8775, best val acc: 0.9099)\n",
      "[Epoch: 37280] train loss: 0.4768, train acc: 0.8271, val loss: 0.3513, val acc: 0.8833  (best train acc: 0.8775, best val acc: 0.9099)\n",
      "[Epoch: 37300] train loss: 0.4722, train acc: 0.8269, val loss: 0.3553, val acc: 0.8823  (best train acc: 0.8775, best val acc: 0.9099)\n",
      "[Epoch: 37320] train loss: 0.5004, train acc: 0.8244, val loss: 0.3460, val acc: 0.8934  (best train acc: 0.8775, best val acc: 0.9099)\n",
      "[Epoch: 37340] train loss: 0.5375, train acc: 0.8067, val loss: 0.3827, val acc: 0.8631  (best train acc: 0.8775, best val acc: 0.9099)\n",
      "[Epoch: 37360] train loss: 0.4556, train acc: 0.8475, val loss: 0.3407, val acc: 0.9002  (best train acc: 0.8775, best val acc: 0.9099)\n",
      "[Epoch: 37380] train loss: 0.4703, train acc: 0.8438, val loss: 0.3693, val acc: 0.8826  (best train acc: 0.8775, best val acc: 0.9099)\n",
      "[Epoch: 37400] train loss: 0.4525, train acc: 0.8358, val loss: 0.3771, val acc: 0.8820  (best train acc: 0.8775, best val acc: 0.9099)\n",
      "[Epoch: 37420] train loss: 0.4833, train acc: 0.8241, val loss: 0.3379, val acc: 0.9025  (best train acc: 0.8775, best val acc: 0.9099)\n",
      "[Epoch: 37440] train loss: 0.4443, train acc: 0.8436, val loss: 0.3503, val acc: 0.8847  (best train acc: 0.8775, best val acc: 0.9099)\n",
      "[Epoch: 37460] train loss: 0.4373, train acc: 0.8472, val loss: 0.3510, val acc: 0.8890  (best train acc: 0.8775, best val acc: 0.9099)\n",
      "[Epoch: 37480] train loss: 0.4244, train acc: 0.8506, val loss: 0.4010, val acc: 0.8718  (best train acc: 0.8775, best val acc: 0.9099)\n",
      "[Epoch: 37500] train loss: 0.4984, train acc: 0.7954, val loss: 0.3975, val acc: 0.8664  (best train acc: 0.8775, best val acc: 0.9099)\n",
      "[Epoch: 37520] train loss: 0.5711, train acc: 0.7865, val loss: 0.3630, val acc: 0.8901  (best train acc: 0.8775, best val acc: 0.9099)\n",
      "[Epoch: 37540] train loss: 0.4678, train acc: 0.8443, val loss: 0.3318, val acc: 0.8992  (best train acc: 0.8775, best val acc: 0.9099)\n",
      "[Epoch: 37560] train loss: 0.6524, train acc: 0.7584, val loss: 0.4405, val acc: 0.8479  (best train acc: 0.8775, best val acc: 0.9099)\n",
      "[Epoch: 37580] train loss: 0.7519, train acc: 0.7079, val loss: 0.3777, val acc: 0.8631  (best train acc: 0.8775, best val acc: 0.9099)\n",
      "[Epoch: 37600] train loss: 0.4637, train acc: 0.8459, val loss: 0.3591, val acc: 0.8782  (best train acc: 0.8775, best val acc: 0.9099)\n",
      "[Epoch: 37620] train loss: 0.4802, train acc: 0.8220, val loss: 0.3403, val acc: 0.8884  (best train acc: 0.8775, best val acc: 0.9099)\n",
      "[Epoch: 37640] train loss: 0.5078, train acc: 0.8171, val loss: 0.3539, val acc: 0.8803  (best train acc: 0.8775, best val acc: 0.9099)\n",
      "[Epoch: 37660] train loss: 0.4995, train acc: 0.8094, val loss: 0.3248, val acc: 0.9005  (best train acc: 0.8775, best val acc: 0.9099)\n",
      "[Epoch: 37680] train loss: 0.5766, train acc: 0.7971, val loss: 0.3316, val acc: 0.9022  (best train acc: 0.8775, best val acc: 0.9099)\n",
      "[Epoch: 37700] train loss: 0.4315, train acc: 0.8589, val loss: 0.3486, val acc: 0.8850  (best train acc: 0.8775, best val acc: 0.9099)\n",
      "[Epoch: 37720] train loss: 0.5402, train acc: 0.8076, val loss: 0.3601, val acc: 0.8850  (best train acc: 0.8775, best val acc: 0.9099)\n",
      "[Epoch: 37740] train loss: 0.5574, train acc: 0.8034, val loss: 0.3569, val acc: 0.8857  (best train acc: 0.8775, best val acc: 0.9099)\n",
      "[Epoch: 37760] train loss: 0.5128, train acc: 0.8229, val loss: 0.3660, val acc: 0.8857  (best train acc: 0.8775, best val acc: 0.9099)\n",
      "[Epoch: 37780] train loss: 0.4352, train acc: 0.8619, val loss: 0.3658, val acc: 0.8880  (best train acc: 0.8775, best val acc: 0.9099)\n",
      "[Epoch: 37800] train loss: 0.4395, train acc: 0.8566, val loss: 0.3437, val acc: 0.8901  (best train acc: 0.8775, best val acc: 0.9099)\n",
      "[Epoch: 37820] train loss: 0.4440, train acc: 0.8446, val loss: 0.3235, val acc: 0.8985  (best train acc: 0.8775, best val acc: 0.9099)\n",
      "[Epoch: 37840] train loss: 0.5333, train acc: 0.8199, val loss: 0.3521, val acc: 0.8911  (best train acc: 0.8775, best val acc: 0.9099)\n",
      "[Epoch: 37860] train loss: 0.4858, train acc: 0.8370, val loss: 0.3448, val acc: 0.8884  (best train acc: 0.8775, best val acc: 0.9099)\n",
      "[Epoch: 37880] train loss: 0.4891, train acc: 0.8378, val loss: 0.3526, val acc: 0.8880  (best train acc: 0.8775, best val acc: 0.9099)\n",
      "[Epoch: 37900] train loss: 0.4974, train acc: 0.8235, val loss: 0.3385, val acc: 0.8921  (best train acc: 0.8775, best val acc: 0.9099)\n",
      "[Epoch: 37920] train loss: 0.5094, train acc: 0.8376, val loss: 0.3439, val acc: 0.8870  (best train acc: 0.8775, best val acc: 0.9099)\n",
      "[Epoch: 37940] train loss: 0.4518, train acc: 0.8461, val loss: 0.3594, val acc: 0.8782  (best train acc: 0.8775, best val acc: 0.9099)\n",
      "[Epoch: 37960] train loss: 0.4422, train acc: 0.8483, val loss: 0.3521, val acc: 0.8772  (best train acc: 0.8775, best val acc: 0.9099)\n",
      "[Epoch: 37980] train loss: 0.7538, train acc: 0.7016, val loss: 0.3511, val acc: 0.8897  (best train acc: 0.8775, best val acc: 0.9099)\n",
      "[Epoch: 38000] train loss: 0.5780, train acc: 0.8054, val loss: 0.4171, val acc: 0.8550  (best train acc: 0.8775, best val acc: 0.9099)\n",
      "[Epoch: 38020] train loss: 0.6086, train acc: 0.7687, val loss: 0.3480, val acc: 0.8897  (best train acc: 0.8775, best val acc: 0.9099)\n",
      "[Epoch: 38040] train loss: 0.5192, train acc: 0.8238, val loss: 0.3803, val acc: 0.8496  (best train acc: 0.8775, best val acc: 0.9099)\n",
      "[Epoch: 38060] train loss: 0.6105, train acc: 0.7890, val loss: 0.3646, val acc: 0.8847  (best train acc: 0.8775, best val acc: 0.9099)\n",
      "[Epoch: 38080] train loss: 0.4871, train acc: 0.8327, val loss: 0.4016, val acc: 0.8688  (best train acc: 0.8775, best val acc: 0.9099)\n",
      "[Epoch: 38100] train loss: 0.5360, train acc: 0.8198, val loss: 0.3924, val acc: 0.8658  (best train acc: 0.8775, best val acc: 0.9099)\n",
      "[Epoch: 38120] train loss: 0.5457, train acc: 0.8088, val loss: 0.3278, val acc: 0.8927  (best train acc: 0.8775, best val acc: 0.9099)\n",
      "[Epoch: 38140] train loss: 0.5156, train acc: 0.8263, val loss: 0.3958, val acc: 0.8681  (best train acc: 0.8775, best val acc: 0.9099)\n",
      "[Epoch: 38160] train loss: 0.5802, train acc: 0.8021, val loss: 0.3567, val acc: 0.8850  (best train acc: 0.8775, best val acc: 0.9099)\n",
      "[Epoch: 38180] train loss: 0.4249, train acc: 0.8610, val loss: 0.3512, val acc: 0.8850  (best train acc: 0.8775, best val acc: 0.9099)\n",
      "[Epoch: 38200] train loss: 0.4630, train acc: 0.8409, val loss: 0.3559, val acc: 0.9015  (best train acc: 0.8775, best val acc: 0.9099)\n",
      "[Epoch: 38220] train loss: 0.5205, train acc: 0.8099, val loss: 0.3393, val acc: 0.8884  (best train acc: 0.8775, best val acc: 0.9099)\n",
      "[Epoch: 38240] train loss: 0.4657, train acc: 0.8308, val loss: 0.3299, val acc: 0.8924  (best train acc: 0.8775, best val acc: 0.9099)\n",
      "[Epoch: 38260] train loss: 0.6109, train acc: 0.7747, val loss: 0.3595, val acc: 0.8820  (best train acc: 0.8775, best val acc: 0.9099)\n",
      "[Epoch: 38280] train loss: 0.4826, train acc: 0.8273, val loss: 0.3271, val acc: 0.8877  (best train acc: 0.8775, best val acc: 0.9099)\n",
      "[Epoch: 38300] train loss: 0.4701, train acc: 0.8339, val loss: 0.3495, val acc: 0.8934  (best train acc: 0.8775, best val acc: 0.9099)\n",
      "[Epoch: 38320] train loss: 0.4261, train acc: 0.8637, val loss: 0.3738, val acc: 0.8867  (best train acc: 0.8775, best val acc: 0.9099)\n",
      "[Epoch: 38340] train loss: 0.8217, train acc: 0.6791, val loss: 0.3310, val acc: 0.9062  (best train acc: 0.8775, best val acc: 0.9099)\n",
      "[Epoch: 38360] train loss: 0.5808, train acc: 0.7839, val loss: 0.4169, val acc: 0.8489  (best train acc: 0.8775, best val acc: 0.9099)\n",
      "[Epoch: 38380] train loss: 0.4817, train acc: 0.8207, val loss: 0.3666, val acc: 0.8833  (best train acc: 0.8775, best val acc: 0.9099)\n",
      "[Epoch: 38400] train loss: 0.6116, train acc: 0.7867, val loss: 0.3582, val acc: 0.8917  (best train acc: 0.8775, best val acc: 0.9099)\n",
      "[Epoch: 38420] train loss: 0.4515, train acc: 0.8446, val loss: 0.3506, val acc: 0.8884  (best train acc: 0.8775, best val acc: 0.9099)\n",
      "[Epoch: 38440] train loss: 0.4777, train acc: 0.8469, val loss: 0.4035, val acc: 0.8637  (best train acc: 0.8775, best val acc: 0.9099)\n",
      "[Epoch: 38460] train loss: 0.4867, train acc: 0.8137, val loss: 0.3501, val acc: 0.8934  (best train acc: 0.8775, best val acc: 0.9099)\n",
      "[Epoch: 38480] train loss: 0.5628, train acc: 0.7966, val loss: 0.3505, val acc: 0.8776  (best train acc: 0.8775, best val acc: 0.9099)\n",
      "[Epoch: 38500] train loss: 0.4328, train acc: 0.8587, val loss: 0.4066, val acc: 0.8577  (best train acc: 0.8775, best val acc: 0.9099)\n",
      "[Epoch: 38520] train loss: 0.5776, train acc: 0.7884, val loss: 0.3276, val acc: 0.9069  (best train acc: 0.8775, best val acc: 0.9099)\n",
      "[Epoch: 38540] train loss: 0.5453, train acc: 0.8002, val loss: 0.3203, val acc: 0.9035  (best train acc: 0.8775, best val acc: 0.9099)\n",
      "[Epoch: 38560] train loss: 0.5995, train acc: 0.7744, val loss: 0.3429, val acc: 0.8917  (best train acc: 0.8775, best val acc: 0.9099)\n",
      "[Epoch: 38580] train loss: 0.5192, train acc: 0.8137, val loss: 0.3310, val acc: 0.8998  (best train acc: 0.8775, best val acc: 0.9099)\n",
      "[Epoch: 38600] train loss: 0.4903, train acc: 0.8290, val loss: 0.3629, val acc: 0.8776  (best train acc: 0.8775, best val acc: 0.9099)\n",
      "[Epoch: 38620] train loss: 0.5133, train acc: 0.8295, val loss: 0.3659, val acc: 0.8965  (best train acc: 0.8775, best val acc: 0.9099)\n",
      "[Epoch: 38640] train loss: 0.4680, train acc: 0.8362, val loss: 0.3437, val acc: 0.8884  (best train acc: 0.8775, best val acc: 0.9099)\n",
      "[Epoch: 38660] train loss: 0.4853, train acc: 0.8411, val loss: 0.3775, val acc: 0.8843  (best train acc: 0.8775, best val acc: 0.9099)\n",
      "[Epoch: 38680] train loss: 0.6027, train acc: 0.7713, val loss: 0.3590, val acc: 0.8924  (best train acc: 0.8775, best val acc: 0.9099)\n",
      "[Epoch: 38700] train loss: 0.4878, train acc: 0.8188, val loss: 0.3384, val acc: 0.8961  (best train acc: 0.8775, best val acc: 0.9099)\n",
      "[Epoch: 38720] train loss: 0.5696, train acc: 0.8019, val loss: 0.3409, val acc: 0.8850  (best train acc: 0.8775, best val acc: 0.9099)\n",
      "[Epoch: 38740] train loss: 0.5527, train acc: 0.8043, val loss: 0.3595, val acc: 0.8782  (best train acc: 0.8775, best val acc: 0.9099)\n",
      "[Epoch: 38760] train loss: 0.6441, train acc: 0.7514, val loss: 0.3586, val acc: 0.8992  (best train acc: 0.8775, best val acc: 0.9099)\n",
      "[Epoch: 38780] train loss: 0.4570, train acc: 0.8421, val loss: 0.4154, val acc: 0.8428  (best train acc: 0.8775, best val acc: 0.9099)\n",
      "[Epoch: 38800] train loss: 0.4604, train acc: 0.8259, val loss: 0.3831, val acc: 0.8725  (best train acc: 0.8775, best val acc: 0.9099)\n",
      "[Epoch: 38820] train loss: 0.5635, train acc: 0.7900, val loss: 0.3385, val acc: 0.8934  (best train acc: 0.8775, best val acc: 0.9099)\n",
      "[Epoch: 38840] train loss: 0.5741, train acc: 0.8082, val loss: 0.3791, val acc: 0.8789  (best train acc: 0.8775, best val acc: 0.9099)\n",
      "[Epoch: 38860] train loss: 0.4485, train acc: 0.8420, val loss: 0.3756, val acc: 0.8668  (best train acc: 0.8775, best val acc: 0.9099)\n",
      "[Epoch: 38880] train loss: 0.4880, train acc: 0.8292, val loss: 0.3494, val acc: 0.8863  (best train acc: 0.8775, best val acc: 0.9099)\n",
      "[Epoch: 38900] train loss: 0.5938, train acc: 0.7915, val loss: 0.3329, val acc: 0.8894  (best train acc: 0.8775, best val acc: 0.9099)\n",
      "[Epoch: 38920] train loss: 0.4489, train acc: 0.8512, val loss: 0.3632, val acc: 0.8786  (best train acc: 0.8775, best val acc: 0.9099)\n",
      "[Epoch: 38940] train loss: 0.5490, train acc: 0.8144, val loss: 0.3440, val acc: 0.8897  (best train acc: 0.8775, best val acc: 0.9099)\n",
      "[Epoch: 38960] train loss: 0.6334, train acc: 0.7785, val loss: 0.3504, val acc: 0.8874  (best train acc: 0.8775, best val acc: 0.9099)\n",
      "[Epoch: 38980] train loss: 0.5698, train acc: 0.7786, val loss: 0.3945, val acc: 0.8675  (best train acc: 0.8775, best val acc: 0.9099)\n",
      "[Epoch: 39000] train loss: 0.4991, train acc: 0.8227, val loss: 0.3660, val acc: 0.8735  (best train acc: 0.8775, best val acc: 0.9099)\n",
      "[Epoch: 39020] train loss: 0.4453, train acc: 0.8523, val loss: 0.3515, val acc: 0.8772  (best train acc: 0.8775, best val acc: 0.9099)\n",
      "[Epoch: 39040] train loss: 0.6962, train acc: 0.7264, val loss: 0.3500, val acc: 0.8978  (best train acc: 0.8775, best val acc: 0.9099)\n",
      "[Epoch: 39060] train loss: 0.4634, train acc: 0.8271, val loss: 0.3659, val acc: 0.8735  (best train acc: 0.8775, best val acc: 0.9099)\n",
      "[Epoch: 39080] train loss: 0.4886, train acc: 0.8298, val loss: 0.3596, val acc: 0.8884  (best train acc: 0.8775, best val acc: 0.9099)\n",
      "[Epoch: 39100] train loss: 0.4537, train acc: 0.8550, val loss: 0.3931, val acc: 0.8782  (best train acc: 0.8775, best val acc: 0.9099)\n",
      "[Epoch: 39120] train loss: 0.5485, train acc: 0.8140, val loss: 0.3628, val acc: 0.8796  (best train acc: 0.8775, best val acc: 0.9099)\n",
      "[Epoch: 39140] train loss: 0.6808, train acc: 0.7301, val loss: 0.3558, val acc: 0.8998  (best train acc: 0.8775, best val acc: 0.9099)\n",
      "[Epoch: 39160] train loss: 0.4856, train acc: 0.8341, val loss: 0.3544, val acc: 0.8853  (best train acc: 0.8775, best val acc: 0.9099)\n",
      "[Epoch: 39180] train loss: 0.4499, train acc: 0.8483, val loss: 0.3455, val acc: 0.8907  (best train acc: 0.8775, best val acc: 0.9099)\n",
      "[Epoch: 39200] train loss: 0.4723, train acc: 0.8432, val loss: 0.3449, val acc: 0.8823  (best train acc: 0.8775, best val acc: 0.9099)\n",
      "[Epoch: 39220] train loss: 0.4198, train acc: 0.8556, val loss: 0.3347, val acc: 0.8877  (best train acc: 0.8775, best val acc: 0.9099)\n",
      "[Epoch: 39240] train loss: 0.6066, train acc: 0.7843, val loss: 0.3295, val acc: 0.8904  (best train acc: 0.8809, best val acc: 0.9099)\n",
      "[Epoch: 39260] train loss: 0.4326, train acc: 0.8640, val loss: 0.3809, val acc: 0.8725  (best train acc: 0.8809, best val acc: 0.9099)\n",
      "[Epoch: 39280] train loss: 0.5851, train acc: 0.7825, val loss: 0.3156, val acc: 0.8833  (best train acc: 0.8809, best val acc: 0.9099)\n",
      "[Epoch: 39300] train loss: 0.6063, train acc: 0.7840, val loss: 0.3396, val acc: 0.8874  (best train acc: 0.8809, best val acc: 0.9099)\n",
      "[Epoch: 39320] train loss: 0.5277, train acc: 0.8155, val loss: 0.3761, val acc: 0.8685  (best train acc: 0.8809, best val acc: 0.9099)\n",
      "[Epoch: 39340] train loss: 0.5531, train acc: 0.8099, val loss: 0.3414, val acc: 0.8914  (best train acc: 0.8809, best val acc: 0.9099)\n",
      "[Epoch: 39360] train loss: 0.5511, train acc: 0.7958, val loss: 0.3744, val acc: 0.8870  (best train acc: 0.8809, best val acc: 0.9099)\n",
      "[Epoch: 39380] train loss: 0.4482, train acc: 0.8424, val loss: 0.3668, val acc: 0.8887  (best train acc: 0.8809, best val acc: 0.9099)\n",
      "[Epoch: 39400] train loss: 0.7330, train acc: 0.7283, val loss: 0.3163, val acc: 0.8921  (best train acc: 0.8809, best val acc: 0.9099)\n",
      "[Epoch: 39420] train loss: 0.6033, train acc: 0.7812, val loss: 0.3405, val acc: 0.8917  (best train acc: 0.8809, best val acc: 0.9099)\n",
      "[Epoch: 39440] train loss: 0.4218, train acc: 0.8662, val loss: 0.3457, val acc: 0.8894  (best train acc: 0.8809, best val acc: 0.9099)\n",
      "[Epoch: 39460] train loss: 0.6325, train acc: 0.7679, val loss: 0.3438, val acc: 0.8826  (best train acc: 0.8809, best val acc: 0.9099)\n",
      "[Epoch: 39480] train loss: 0.4429, train acc: 0.8485, val loss: 0.4119, val acc: 0.8556  (best train acc: 0.8809, best val acc: 0.9099)\n",
      "[Epoch: 39500] train loss: 0.4489, train acc: 0.8500, val loss: 0.3698, val acc: 0.8745  (best train acc: 0.8809, best val acc: 0.9099)\n",
      "[Epoch: 39520] train loss: 0.5147, train acc: 0.8041, val loss: 0.3566, val acc: 0.8998  (best train acc: 0.8809, best val acc: 0.9099)\n",
      "[Epoch: 39540] train loss: 0.4578, train acc: 0.8351, val loss: 0.3469, val acc: 0.8951  (best train acc: 0.8809, best val acc: 0.9099)\n",
      "[Epoch: 39560] train loss: 0.5173, train acc: 0.8173, val loss: 0.3248, val acc: 0.9022  (best train acc: 0.8809, best val acc: 0.9099)\n",
      "[Epoch: 39580] train loss: 0.4919, train acc: 0.8102, val loss: 0.3686, val acc: 0.8860  (best train acc: 0.8809, best val acc: 0.9099)\n",
      "[Epoch: 39600] train loss: 0.4500, train acc: 0.8409, val loss: 0.3743, val acc: 0.8820  (best train acc: 0.8809, best val acc: 0.9099)\n",
      "[Epoch: 39620] train loss: 0.5175, train acc: 0.8105, val loss: 0.3598, val acc: 0.8806  (best train acc: 0.8809, best val acc: 0.9099)\n",
      "[Epoch: 39640] train loss: 0.5954, train acc: 0.7749, val loss: 0.3596, val acc: 0.8968  (best train acc: 0.8809, best val acc: 0.9099)\n",
      "[Epoch: 39660] train loss: 0.5435, train acc: 0.8047, val loss: 0.3306, val acc: 0.8995  (best train acc: 0.8809, best val acc: 0.9099)\n",
      "[Epoch: 39680] train loss: 0.5384, train acc: 0.8071, val loss: 0.3716, val acc: 0.8749  (best train acc: 0.8809, best val acc: 0.9099)\n",
      "[Epoch: 39700] train loss: 0.4579, train acc: 0.8599, val loss: 0.3561, val acc: 0.8830  (best train acc: 0.8809, best val acc: 0.9099)\n",
      "[Epoch: 39720] train loss: 0.4992, train acc: 0.8127, val loss: 0.3219, val acc: 0.8772  (best train acc: 0.8809, best val acc: 0.9099)\n",
      "[Epoch: 39740] train loss: 0.4771, train acc: 0.8302, val loss: 0.3199, val acc: 0.9049  (best train acc: 0.8809, best val acc: 0.9099)\n",
      "[Epoch: 39760] train loss: 0.5251, train acc: 0.8207, val loss: 0.3556, val acc: 0.8840  (best train acc: 0.8809, best val acc: 0.9099)\n",
      "[Epoch: 39780] train loss: 0.4695, train acc: 0.8240, val loss: 0.3623, val acc: 0.8823  (best train acc: 0.8809, best val acc: 0.9099)\n",
      "[Epoch: 39800] train loss: 0.4653, train acc: 0.8323, val loss: 0.3286, val acc: 0.8907  (best train acc: 0.8809, best val acc: 0.9099)\n",
      "[Epoch: 39820] train loss: 0.5273, train acc: 0.8167, val loss: 0.3402, val acc: 0.8921  (best train acc: 0.8809, best val acc: 0.9099)\n",
      "[Epoch: 39840] train loss: 0.4848, train acc: 0.8086, val loss: 0.3600, val acc: 0.8948  (best train acc: 0.8809, best val acc: 0.9099)\n",
      "[Epoch: 39860] train loss: 0.4916, train acc: 0.8173, val loss: 0.3238, val acc: 0.8857  (best train acc: 0.8809, best val acc: 0.9099)\n",
      "[Epoch: 39880] train loss: 0.5270, train acc: 0.8189, val loss: 0.3225, val acc: 0.8961  (best train acc: 0.8809, best val acc: 0.9099)\n",
      "[Epoch: 39900] train loss: 0.4387, train acc: 0.8581, val loss: 0.3592, val acc: 0.8954  (best train acc: 0.8809, best val acc: 0.9099)\n",
      "[Epoch: 39920] train loss: 0.4722, train acc: 0.8418, val loss: 0.4318, val acc: 0.8432  (best train acc: 0.8809, best val acc: 0.9099)\n",
      "[Epoch: 39940] train loss: 0.4954, train acc: 0.8323, val loss: 0.3770, val acc: 0.8705  (best train acc: 0.8809, best val acc: 0.9099)\n",
      "[Epoch: 39960] train loss: 0.7293, train acc: 0.7238, val loss: 0.3350, val acc: 0.9012  (best train acc: 0.8809, best val acc: 0.9099)\n",
      "[Epoch: 39980] train loss: 0.6069, train acc: 0.7788, val loss: 0.3248, val acc: 0.8938  (best train acc: 0.8809, best val acc: 0.9099)\n",
      "[Epoch: 40000] train loss: 0.4661, train acc: 0.8397, val loss: 0.3774, val acc: 0.8772  (best train acc: 0.8809, best val acc: 0.9099)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.78      0.86       154\n",
      "           1       0.99      0.79      0.88       703\n",
      "           2       0.77      0.79      0.78       702\n",
      "           3       0.80      0.96      0.87       703\n",
      "           4       0.98      1.00      0.99       702\n",
      "\n",
      "    accuracy                           0.88      2964\n",
      "   macro avg       0.90      0.86      0.88      2964\n",
      "weighted avg       0.89      0.88      0.88      2964\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 504x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 504x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "gnn = GNN4L_SAGE(data_with_nedbit).to(device)\n",
    "pred = train(gnn, data_with_nedbit.to(device), 40000, cm_title='GAT7L_multiclass_16HC', classes=['P', 'LP', 'WN', 'LN', 'RN'], weight_decay=0.0005, arch='SAGE', layers=4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92f00b8d70f8485ea16b1c3f2a4a67d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40001 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch: 0000] train loss: 317.7693, train acc: 0.0871, val loss: 127.3152, val acc: 0.0516  (best train acc: 0.0871, best val acc: 0.0516)\n",
      "[Epoch: 0020] train loss: 41.8326, train acc: 0.2444, val loss: 14.5331, val acc: 0.2725  (best train acc: 0.2566, best val acc: 0.2944)\n",
      "[Epoch: 0040] train loss: 9.7900, train acc: 0.2871, val loss: 2.8893, val acc: 0.3194  (best train acc: 0.2871, best val acc: 0.3528)\n",
      "[Epoch: 0060] train loss: 4.5343, train acc: 0.3376, val loss: 2.0810, val acc: 0.4057  (best train acc: 0.3519, best val acc: 0.4165)\n",
      "[Epoch: 0080] train loss: 2.9139, train acc: 0.4010, val loss: 1.5627, val acc: 0.4624  (best train acc: 0.4010, best val acc: 0.4752)\n",
      "[Epoch: 0100] train loss: 2.0139, train acc: 0.4328, val loss: 1.2677, val acc: 0.5346  (best train acc: 0.4427, best val acc: 0.5346)\n",
      "[Epoch: 0120] train loss: 1.4094, train acc: 0.5042, val loss: 1.2275, val acc: 0.5214  (best train acc: 0.5042, best val acc: 0.5379)\n",
      "[Epoch: 0140] train loss: 1.4703, train acc: 0.4216, val loss: 1.2307, val acc: 0.5568  (best train acc: 0.5042, best val acc: 0.5599)\n",
      "[Epoch: 0160] train loss: 1.3552, train acc: 0.4764, val loss: 1.2154, val acc: 0.5723  (best train acc: 0.5166, best val acc: 0.5734)\n",
      "[Epoch: 0180] train loss: 1.3309, train acc: 0.4870, val loss: 1.1937, val acc: 0.5717  (best train acc: 0.5166, best val acc: 0.5734)\n",
      "[Epoch: 0200] train loss: 1.2609, train acc: 0.5246, val loss: 1.1875, val acc: 0.5744  (best train acc: 0.5246, best val acc: 0.5811)\n",
      "[Epoch: 0220] train loss: 1.2639, train acc: 0.5185, val loss: 1.1753, val acc: 0.5754  (best train acc: 0.5256, best val acc: 0.5841)\n",
      "[Epoch: 0240] train loss: 1.3458, train acc: 0.4495, val loss: 1.1706, val acc: 0.5838  (best train acc: 0.5256, best val acc: 0.5845)\n",
      "[Epoch: 0260] train loss: 1.2196, train acc: 0.5252, val loss: 1.1559, val acc: 0.5818  (best train acc: 0.5287, best val acc: 0.5845)\n",
      "[Epoch: 0280] train loss: 1.2748, train acc: 0.4920, val loss: 1.1415, val acc: 0.5855  (best train acc: 0.5287, best val acc: 0.5879)\n",
      "[Epoch: 0300] train loss: 1.2652, train acc: 0.4883, val loss: 1.1355, val acc: 0.5838  (best train acc: 0.5287, best val acc: 0.5892)\n",
      "[Epoch: 0320] train loss: 1.2354, train acc: 0.5058, val loss: 1.1322, val acc: 0.5858  (best train acc: 0.5287, best val acc: 0.5892)\n",
      "[Epoch: 0340] train loss: 1.2340, train acc: 0.4963, val loss: 1.1199, val acc: 0.5774  (best train acc: 0.5287, best val acc: 0.5919)\n",
      "[Epoch: 0360] train loss: 1.2421, train acc: 0.4836, val loss: 1.1248, val acc: 0.5993  (best train acc: 0.5287, best val acc: 0.6007)\n",
      "[Epoch: 0380] train loss: 1.2069, train acc: 0.4928, val loss: 1.1021, val acc: 0.5973  (best train acc: 0.5287, best val acc: 0.6155)\n",
      "[Epoch: 0400] train loss: 1.1963, train acc: 0.5075, val loss: 1.0918, val acc: 0.6010  (best train acc: 0.5287, best val acc: 0.6226)\n",
      "[Epoch: 0420] train loss: 1.1760, train acc: 0.5132, val loss: 1.0774, val acc: 0.6057  (best train acc: 0.5413, best val acc: 0.6287)\n",
      "[Epoch: 0440] train loss: 1.1306, train acc: 0.5279, val loss: 1.0724, val acc: 0.6314  (best train acc: 0.5413, best val acc: 0.6344)\n",
      "[Epoch: 0460] train loss: 1.2152, train acc: 0.4758, val loss: 1.0515, val acc: 0.6304  (best train acc: 0.5413, best val acc: 0.6489)\n",
      "[Epoch: 0480] train loss: 1.1579, train acc: 0.5198, val loss: 1.0443, val acc: 0.6422  (best train acc: 0.5413, best val acc: 0.6594)\n",
      "[Epoch: 0500] train loss: 1.1008, train acc: 0.5528, val loss: 1.0452, val acc: 0.6678  (best train acc: 0.5528, best val acc: 0.6678)\n",
      "[Epoch: 0520] train loss: 1.2051, train acc: 0.4861, val loss: 1.0345, val acc: 0.6671  (best train acc: 0.5528, best val acc: 0.6769)\n",
      "[Epoch: 0540] train loss: 1.1125, train acc: 0.5093, val loss: 1.0078, val acc: 0.6725  (best train acc: 0.5660, best val acc: 0.6877)\n",
      "[Epoch: 0560] train loss: 1.1193, train acc: 0.5312, val loss: 1.0131, val acc: 0.6884  (best train acc: 0.5701, best val acc: 0.7056)\n",
      "[Epoch: 0580] train loss: 1.1224, train acc: 0.5275, val loss: 1.0157, val acc: 0.7052  (best train acc: 0.5701, best val acc: 0.7056)\n",
      "[Epoch: 0600] train loss: 1.0968, train acc: 0.5281, val loss: 0.9875, val acc: 0.6820  (best train acc: 0.5701, best val acc: 0.7113)\n",
      "[Epoch: 0620] train loss: 1.1587, train acc: 0.4922, val loss: 0.9787, val acc: 0.7012  (best train acc: 0.5701, best val acc: 0.7150)\n",
      "[Epoch: 0640] train loss: 1.0668, train acc: 0.5411, val loss: 0.9745, val acc: 0.7116  (best train acc: 0.5701, best val acc: 0.7204)\n",
      "[Epoch: 0660] train loss: 1.0739, train acc: 0.5472, val loss: 0.9614, val acc: 0.7241  (best train acc: 0.5794, best val acc: 0.7349)\n",
      "[Epoch: 0680] train loss: 1.1118, train acc: 0.5105, val loss: 0.9393, val acc: 0.7130  (best train acc: 0.5842, best val acc: 0.7460)\n",
      "[Epoch: 0700] train loss: 1.0124, train acc: 0.5733, val loss: 0.9424, val acc: 0.7383  (best train acc: 0.5842, best val acc: 0.7474)\n",
      "[Epoch: 0720] train loss: 1.1054, train acc: 0.5262, val loss: 0.9420, val acc: 0.7616  (best train acc: 0.5842, best val acc: 0.7616)\n",
      "[Epoch: 0740] train loss: 0.9933, train acc: 0.5846, val loss: 0.9218, val acc: 0.7642  (best train acc: 0.5932, best val acc: 0.7642)\n",
      "[Epoch: 0760] train loss: 1.0144, train acc: 0.5903, val loss: 0.8955, val acc: 0.7939  (best train acc: 0.6210, best val acc: 0.7963)\n",
      "[Epoch: 0780] train loss: 0.9808, train acc: 0.5873, val loss: 0.8374, val acc: 0.7815  (best train acc: 0.6414, best val acc: 0.8142)\n",
      "[Epoch: 0800] train loss: 0.9603, train acc: 0.6113, val loss: 0.8153, val acc: 0.8192  (best train acc: 0.6560, best val acc: 0.8277)\n",
      "[Epoch: 0820] train loss: 0.8853, train acc: 0.6380, val loss: 0.8023, val acc: 0.8290  (best train acc: 0.6560, best val acc: 0.8290)\n",
      "[Epoch: 0840] train loss: 1.0656, train acc: 0.5414, val loss: 0.7834, val acc: 0.8226  (best train acc: 0.6751, best val acc: 0.8371)\n",
      "[Epoch: 0860] train loss: 0.9197, train acc: 0.6217, val loss: 0.7675, val acc: 0.8428  (best train acc: 0.6751, best val acc: 0.8435)\n",
      "[Epoch: 0880] train loss: 0.8999, train acc: 0.6399, val loss: 0.7605, val acc: 0.8422  (best train acc: 0.6789, best val acc: 0.8459)\n",
      "[Epoch: 0900] train loss: 0.8459, train acc: 0.6788, val loss: 0.7536, val acc: 0.8418  (best train acc: 0.6789, best val acc: 0.8519)\n",
      "[Epoch: 0920] train loss: 0.9149, train acc: 0.6377, val loss: 0.7456, val acc: 0.8277  (best train acc: 0.6789, best val acc: 0.8519)\n",
      "[Epoch: 0940] train loss: 0.9169, train acc: 0.6073, val loss: 0.7350, val acc: 0.8411  (best train acc: 0.6789, best val acc: 0.8540)\n",
      "[Epoch: 0960] train loss: 0.9547, train acc: 0.6039, val loss: 0.7283, val acc: 0.8573  (best train acc: 0.6789, best val acc: 0.8587)\n",
      "[Epoch: 0980] train loss: 0.8189, train acc: 0.6926, val loss: 0.7302, val acc: 0.8388  (best train acc: 0.6979, best val acc: 0.8614)\n",
      "[Epoch: 1000] train loss: 0.8108, train acc: 0.6768, val loss: 0.7148, val acc: 0.8567  (best train acc: 0.7076, best val acc: 0.8627)\n",
      "[Epoch: 1020] train loss: 0.9615, train acc: 0.5952, val loss: 0.7021, val acc: 0.8624  (best train acc: 0.7076, best val acc: 0.8637)\n",
      "[Epoch: 1040] train loss: 0.8469, train acc: 0.6534, val loss: 0.6990, val acc: 0.8664  (best train acc: 0.7076, best val acc: 0.8664)\n",
      "[Epoch: 1060] train loss: 0.8821, train acc: 0.6410, val loss: 0.6988, val acc: 0.8654  (best train acc: 0.7076, best val acc: 0.8664)\n",
      "[Epoch: 1080] train loss: 0.8405, train acc: 0.6756, val loss: 0.6968, val acc: 0.8540  (best train acc: 0.7076, best val acc: 0.8688)\n",
      "[Epoch: 1100] train loss: 0.9511, train acc: 0.6109, val loss: 0.6839, val acc: 0.8654  (best train acc: 0.7076, best val acc: 0.8702)\n",
      "[Epoch: 1120] train loss: 0.8287, train acc: 0.6701, val loss: 0.6833, val acc: 0.8688  (best train acc: 0.7076, best val acc: 0.8708)\n",
      "[Epoch: 1140] train loss: 0.8457, train acc: 0.6755, val loss: 0.6836, val acc: 0.8600  (best train acc: 0.7127, best val acc: 0.8708)\n",
      "[Epoch: 1160] train loss: 0.8049, train acc: 0.6793, val loss: 0.6784, val acc: 0.8610  (best train acc: 0.7127, best val acc: 0.8708)\n",
      "[Epoch: 1180] train loss: 0.8787, train acc: 0.6399, val loss: 0.6707, val acc: 0.8681  (best train acc: 0.7143, best val acc: 0.8728)\n",
      "[Epoch: 1200] train loss: 0.8920, train acc: 0.6328, val loss: 0.6724, val acc: 0.8712  (best train acc: 0.7214, best val acc: 0.8728)\n",
      "[Epoch: 1220] train loss: 0.9522, train acc: 0.5972, val loss: 0.6615, val acc: 0.8715  (best train acc: 0.7292, best val acc: 0.8728)\n",
      "[Epoch: 1240] train loss: 0.8622, train acc: 0.6641, val loss: 0.6690, val acc: 0.8567  (best train acc: 0.7292, best val acc: 0.8728)\n",
      "[Epoch: 1260] train loss: 0.7927, train acc: 0.6709, val loss: 0.6577, val acc: 0.8637  (best train acc: 0.7292, best val acc: 0.8735)\n",
      "[Epoch: 1280] train loss: 0.7706, train acc: 0.7130, val loss: 0.6523, val acc: 0.8644  (best train acc: 0.7292, best val acc: 0.8735)\n",
      "[Epoch: 1300] train loss: 0.8228, train acc: 0.6743, val loss: 0.6531, val acc: 0.8681  (best train acc: 0.7292, best val acc: 0.8735)\n",
      "[Epoch: 1320] train loss: 0.7618, train acc: 0.7063, val loss: 0.6502, val acc: 0.8695  (best train acc: 0.7292, best val acc: 0.8735)\n",
      "[Epoch: 1340] train loss: 0.7421, train acc: 0.7165, val loss: 0.6505, val acc: 0.8644  (best train acc: 0.7292, best val acc: 0.8739)\n",
      "[Epoch: 1360] train loss: 0.8319, train acc: 0.6773, val loss: 0.6425, val acc: 0.8641  (best train acc: 0.7358, best val acc: 0.8739)\n",
      "[Epoch: 1380] train loss: 0.8856, train acc: 0.6397, val loss: 0.6385, val acc: 0.8530  (best train acc: 0.7378, best val acc: 0.8739)\n",
      "[Epoch: 1400] train loss: 0.7273, train acc: 0.7156, val loss: 0.6394, val acc: 0.8644  (best train acc: 0.7456, best val acc: 0.8742)\n",
      "[Epoch: 1420] train loss: 0.8026, train acc: 0.6859, val loss: 0.6307, val acc: 0.8702  (best train acc: 0.7456, best val acc: 0.8742)\n",
      "[Epoch: 1440] train loss: 0.7489, train acc: 0.6823, val loss: 0.6315, val acc: 0.8668  (best train acc: 0.7456, best val acc: 0.8742)\n",
      "[Epoch: 1460] train loss: 0.8144, train acc: 0.6718, val loss: 0.6291, val acc: 0.8698  (best train acc: 0.7456, best val acc: 0.8742)\n",
      "[Epoch: 1480] train loss: 0.8345, train acc: 0.6643, val loss: 0.6259, val acc: 0.8695  (best train acc: 0.7456, best val acc: 0.8742)\n",
      "[Epoch: 1500] train loss: 0.8831, train acc: 0.6460, val loss: 0.6272, val acc: 0.8664  (best train acc: 0.7456, best val acc: 0.8745)\n",
      "[Epoch: 1520] train loss: 0.8209, train acc: 0.6655, val loss: 0.6256, val acc: 0.8688  (best train acc: 0.7456, best val acc: 0.8745)\n",
      "[Epoch: 1540] train loss: 0.8379, train acc: 0.6567, val loss: 0.6315, val acc: 0.8607  (best train acc: 0.7456, best val acc: 0.8745)\n",
      "[Epoch: 1560] train loss: 0.7366, train acc: 0.7014, val loss: 0.6250, val acc: 0.8691  (best train acc: 0.7456, best val acc: 0.8745)\n",
      "[Epoch: 1580] train loss: 0.7661, train acc: 0.6949, val loss: 0.6196, val acc: 0.8705  (best train acc: 0.7456, best val acc: 0.8745)\n",
      "[Epoch: 1600] train loss: 0.7197, train acc: 0.6982, val loss: 0.6229, val acc: 0.8600  (best train acc: 0.7456, best val acc: 0.8745)\n",
      "[Epoch: 1620] train loss: 0.8156, train acc: 0.6687, val loss: 0.6133, val acc: 0.8678  (best train acc: 0.7456, best val acc: 0.8745)\n",
      "[Epoch: 1640] train loss: 0.8142, train acc: 0.6725, val loss: 0.6151, val acc: 0.8732  (best train acc: 0.7456, best val acc: 0.8745)\n",
      "[Epoch: 1660] train loss: 0.7999, train acc: 0.6839, val loss: 0.6132, val acc: 0.8661  (best train acc: 0.7456, best val acc: 0.8745)\n",
      "[Epoch: 1680] train loss: 0.7206, train acc: 0.7205, val loss: 0.6098, val acc: 0.8617  (best train acc: 0.7456, best val acc: 0.8745)\n",
      "[Epoch: 1700] train loss: 0.8138, train acc: 0.6438, val loss: 0.6040, val acc: 0.8614  (best train acc: 0.7456, best val acc: 0.8745)\n",
      "[Epoch: 1720] train loss: 0.7801, train acc: 0.6807, val loss: 0.6365, val acc: 0.8459  (best train acc: 0.7456, best val acc: 0.8745)\n",
      "[Epoch: 1740] train loss: 0.7638, train acc: 0.6971, val loss: 0.5995, val acc: 0.8617  (best train acc: 0.7590, best val acc: 0.8752)\n",
      "[Epoch: 1760] train loss: 0.7308, train acc: 0.7034, val loss: 0.6115, val acc: 0.8641  (best train acc: 0.7590, best val acc: 0.8752)\n",
      "[Epoch: 1780] train loss: 0.7894, train acc: 0.6932, val loss: 0.6009, val acc: 0.8658  (best train acc: 0.7590, best val acc: 0.8752)\n",
      "[Epoch: 1800] train loss: 0.9067, train acc: 0.6215, val loss: 0.6146, val acc: 0.8358  (best train acc: 0.7590, best val acc: 0.8752)\n",
      "[Epoch: 1820] train loss: 0.7003, train acc: 0.7159, val loss: 0.5979, val acc: 0.8648  (best train acc: 0.7590, best val acc: 0.8752)\n",
      "[Epoch: 1840] train loss: 0.8845, train acc: 0.6277, val loss: 0.5918, val acc: 0.8621  (best train acc: 0.7590, best val acc: 0.8752)\n",
      "[Epoch: 1860] train loss: 0.8763, train acc: 0.6312, val loss: 0.5918, val acc: 0.8627  (best train acc: 0.7590, best val acc: 0.8752)\n",
      "[Epoch: 1880] train loss: 0.7282, train acc: 0.7145, val loss: 0.6042, val acc: 0.8725  (best train acc: 0.7590, best val acc: 0.8752)\n",
      "[Epoch: 1900] train loss: 0.6877, train acc: 0.7255, val loss: 0.6285, val acc: 0.8678  (best train acc: 0.7590, best val acc: 0.8779)\n",
      "[Epoch: 1920] train loss: 0.7610, train acc: 0.6872, val loss: 0.6218, val acc: 0.8546  (best train acc: 0.7590, best val acc: 0.8779)\n",
      "[Epoch: 1940] train loss: 0.7487, train acc: 0.7001, val loss: 0.5927, val acc: 0.8617  (best train acc: 0.7590, best val acc: 0.8779)\n",
      "[Epoch: 1960] train loss: 0.7001, train acc: 0.7252, val loss: 0.6067, val acc: 0.8486  (best train acc: 0.7861, best val acc: 0.8779)\n",
      "[Epoch: 1980] train loss: 0.6953, train acc: 0.7253, val loss: 0.6090, val acc: 0.8395  (best train acc: 0.7861, best val acc: 0.8779)\n",
      "[Epoch: 2000] train loss: 0.7722, train acc: 0.6735, val loss: 0.5900, val acc: 0.8621  (best train acc: 0.7861, best val acc: 0.8779)\n",
      "[Epoch: 2020] train loss: 0.8046, train acc: 0.6796, val loss: 0.6208, val acc: 0.8243  (best train acc: 0.7861, best val acc: 0.8779)\n",
      "[Epoch: 2040] train loss: 0.6617, train acc: 0.7333, val loss: 0.5892, val acc: 0.8610  (best train acc: 0.7861, best val acc: 0.8779)\n",
      "[Epoch: 2060] train loss: 0.7289, train acc: 0.7079, val loss: 0.5843, val acc: 0.8563  (best train acc: 0.7861, best val acc: 0.8779)\n",
      "[Epoch: 2080] train loss: 0.8353, train acc: 0.6415, val loss: 0.5761, val acc: 0.8641  (best train acc: 0.7861, best val acc: 0.8779)\n",
      "[Epoch: 2100] train loss: 0.6557, train acc: 0.7337, val loss: 0.5567, val acc: 0.8570  (best train acc: 0.7861, best val acc: 0.8779)\n",
      "[Epoch: 2120] train loss: 0.6977, train acc: 0.7227, val loss: 0.5658, val acc: 0.8455  (best train acc: 0.7861, best val acc: 0.8779)\n",
      "[Epoch: 2140] train loss: 0.7158, train acc: 0.6925, val loss: 0.5736, val acc: 0.8580  (best train acc: 0.7861, best val acc: 0.8779)\n",
      "[Epoch: 2160] train loss: 0.7253, train acc: 0.7096, val loss: 0.6059, val acc: 0.8509  (best train acc: 0.7861, best val acc: 0.8779)\n",
      "[Epoch: 2180] train loss: 0.8612, train acc: 0.6422, val loss: 0.5960, val acc: 0.8533  (best train acc: 0.7861, best val acc: 0.8779)\n",
      "[Epoch: 2200] train loss: 0.6768, train acc: 0.7355, val loss: 0.5759, val acc: 0.8705  (best train acc: 0.7861, best val acc: 0.8779)\n",
      "[Epoch: 2220] train loss: 0.6457, train acc: 0.7374, val loss: 0.5764, val acc: 0.8637  (best train acc: 0.7861, best val acc: 0.8779)\n",
      "[Epoch: 2240] train loss: 0.7320, train acc: 0.7099, val loss: 0.5831, val acc: 0.8570  (best train acc: 0.7861, best val acc: 0.8779)\n",
      "[Epoch: 2260] train loss: 0.8343, train acc: 0.6568, val loss: 0.5703, val acc: 0.8732  (best train acc: 0.7861, best val acc: 0.8779)\n",
      "[Epoch: 2280] train loss: 0.6127, train acc: 0.7762, val loss: 0.6175, val acc: 0.8266  (best train acc: 0.7861, best val acc: 0.8779)\n",
      "[Epoch: 2300] train loss: 0.6938, train acc: 0.7159, val loss: 0.5749, val acc: 0.8712  (best train acc: 0.7861, best val acc: 0.8779)\n",
      "[Epoch: 2320] train loss: 0.7322, train acc: 0.7152, val loss: 0.5853, val acc: 0.8641  (best train acc: 0.7861, best val acc: 0.8779)\n",
      "[Epoch: 2340] train loss: 0.7800, train acc: 0.6837, val loss: 0.5679, val acc: 0.8705  (best train acc: 0.7861, best val acc: 0.8779)\n",
      "[Epoch: 2360] train loss: 0.7589, train acc: 0.6820, val loss: 0.5600, val acc: 0.8712  (best train acc: 0.7861, best val acc: 0.8779)\n",
      "[Epoch: 2380] train loss: 0.6950, train acc: 0.7177, val loss: 0.5690, val acc: 0.8698  (best train acc: 0.7861, best val acc: 0.8779)\n",
      "[Epoch: 2400] train loss: 0.7400, train acc: 0.7043, val loss: 0.5706, val acc: 0.8702  (best train acc: 0.7861, best val acc: 0.8779)\n",
      "[Epoch: 2420] train loss: 0.7166, train acc: 0.7318, val loss: 0.5891, val acc: 0.8465  (best train acc: 0.7861, best val acc: 0.8779)\n",
      "[Epoch: 2440] train loss: 0.7165, train acc: 0.6998, val loss: 0.5719, val acc: 0.8533  (best train acc: 0.7877, best val acc: 0.8779)\n",
      "[Epoch: 2460] train loss: 0.7172, train acc: 0.7313, val loss: 0.5758, val acc: 0.8661  (best train acc: 0.7877, best val acc: 0.8779)\n",
      "[Epoch: 2480] train loss: 0.7364, train acc: 0.6961, val loss: 0.5602, val acc: 0.8681  (best train acc: 0.7877, best val acc: 0.8779)\n",
      "[Epoch: 2500] train loss: 0.7674, train acc: 0.6807, val loss: 0.5746, val acc: 0.8556  (best train acc: 0.7877, best val acc: 0.8779)\n",
      "[Epoch: 2520] train loss: 0.6902, train acc: 0.7246, val loss: 0.5842, val acc: 0.8496  (best train acc: 0.7886, best val acc: 0.8779)\n",
      "[Epoch: 2540] train loss: 0.8642, train acc: 0.6392, val loss: 0.5855, val acc: 0.8513  (best train acc: 0.7886, best val acc: 0.8779)\n",
      "[Epoch: 2560] train loss: 0.7688, train acc: 0.6861, val loss: 0.5670, val acc: 0.8594  (best train acc: 0.7886, best val acc: 0.8779)\n",
      "[Epoch: 2580] train loss: 0.7546, train acc: 0.7008, val loss: 0.5750, val acc: 0.8627  (best train acc: 0.7886, best val acc: 0.8779)\n",
      "[Epoch: 2600] train loss: 0.6159, train acc: 0.7785, val loss: 0.5730, val acc: 0.8597  (best train acc: 0.7886, best val acc: 0.8779)\n",
      "[Epoch: 2620] train loss: 0.7899, train acc: 0.6678, val loss: 0.6141, val acc: 0.8229  (best train acc: 0.7886, best val acc: 0.8779)\n",
      "[Epoch: 2640] train loss: 0.7401, train acc: 0.7285, val loss: 0.5779, val acc: 0.8567  (best train acc: 0.7886, best val acc: 0.8779)\n",
      "[Epoch: 2660] train loss: 0.6799, train acc: 0.7241, val loss: 0.5625, val acc: 0.8600  (best train acc: 0.7985, best val acc: 0.8779)\n",
      "[Epoch: 2680] train loss: 0.7104, train acc: 0.7262, val loss: 0.5672, val acc: 0.8678  (best train acc: 0.7985, best val acc: 0.8779)\n",
      "[Epoch: 2700] train loss: 0.7271, train acc: 0.7000, val loss: 0.5630, val acc: 0.8644  (best train acc: 0.8050, best val acc: 0.8779)\n",
      "[Epoch: 2720] train loss: 0.8681, train acc: 0.6352, val loss: 0.5668, val acc: 0.8685  (best train acc: 0.8050, best val acc: 0.8779)\n",
      "[Epoch: 2740] train loss: 0.7480, train acc: 0.7056, val loss: 0.5731, val acc: 0.8553  (best train acc: 0.8050, best val acc: 0.8779)\n",
      "[Epoch: 2760] train loss: 0.7169, train acc: 0.7134, val loss: 0.5643, val acc: 0.8725  (best train acc: 0.8050, best val acc: 0.8779)\n",
      "[Epoch: 2780] train loss: 0.7362, train acc: 0.7126, val loss: 0.5698, val acc: 0.8482  (best train acc: 0.8050, best val acc: 0.8779)\n",
      "[Epoch: 2800] train loss: 0.7084, train acc: 0.7426, val loss: 0.5790, val acc: 0.8347  (best train acc: 0.8050, best val acc: 0.8779)\n",
      "[Epoch: 2820] train loss: 0.7148, train acc: 0.7205, val loss: 0.5613, val acc: 0.8651  (best train acc: 0.8050, best val acc: 0.8779)\n",
      "[Epoch: 2840] train loss: 0.7129, train acc: 0.7368, val loss: 0.5789, val acc: 0.8540  (best train acc: 0.8050, best val acc: 0.8779)\n",
      "[Epoch: 2860] train loss: 0.8402, train acc: 0.6635, val loss: 0.5688, val acc: 0.8695  (best train acc: 0.8050, best val acc: 0.8779)\n",
      "[Epoch: 2880] train loss: 0.7433, train acc: 0.7205, val loss: 0.5634, val acc: 0.8698  (best train acc: 0.8050, best val acc: 0.8779)\n",
      "[Epoch: 2900] train loss: 0.8397, train acc: 0.6348, val loss: 0.5737, val acc: 0.8445  (best train acc: 0.8050, best val acc: 0.8779)\n",
      "[Epoch: 2920] train loss: 0.6036, train acc: 0.7932, val loss: 0.5813, val acc: 0.8519  (best train acc: 0.8050, best val acc: 0.8779)\n",
      "[Epoch: 2940] train loss: 0.7824, train acc: 0.6772, val loss: 0.5739, val acc: 0.8587  (best train acc: 0.8050, best val acc: 0.8779)\n",
      "[Epoch: 2960] train loss: 0.7913, train acc: 0.6700, val loss: 0.5681, val acc: 0.8499  (best train acc: 0.8050, best val acc: 0.8779)\n",
      "[Epoch: 2980] train loss: 0.8122, train acc: 0.6649, val loss: 0.5571, val acc: 0.8722  (best train acc: 0.8050, best val acc: 0.8779)\n",
      "[Epoch: 3000] train loss: 0.6199, train acc: 0.7737, val loss: 0.5593, val acc: 0.8641  (best train acc: 0.8050, best val acc: 0.8779)\n",
      "[Epoch: 3020] train loss: 0.7660, train acc: 0.7055, val loss: 0.5918, val acc: 0.8368  (best train acc: 0.8050, best val acc: 0.8779)\n",
      "[Epoch: 3040] train loss: 0.7835, train acc: 0.6859, val loss: 0.5743, val acc: 0.8570  (best train acc: 0.8050, best val acc: 0.8782)\n",
      "[Epoch: 3060] train loss: 0.5954, train acc: 0.7810, val loss: 0.5770, val acc: 0.8465  (best train acc: 0.8064, best val acc: 0.8782)\n",
      "[Epoch: 3080] train loss: 0.6384, train acc: 0.7465, val loss: 0.6014, val acc: 0.8132  (best train acc: 0.8064, best val acc: 0.8786)\n",
      "[Epoch: 3100] train loss: 0.6792, train acc: 0.7376, val loss: 0.5638, val acc: 0.8702  (best train acc: 0.8064, best val acc: 0.8786)\n",
      "[Epoch: 3120] train loss: 0.7205, train acc: 0.7226, val loss: 0.5751, val acc: 0.8533  (best train acc: 0.8064, best val acc: 0.8786)\n",
      "[Epoch: 3140] train loss: 0.7874, train acc: 0.6706, val loss: 0.5651, val acc: 0.8637  (best train acc: 0.8064, best val acc: 0.8786)\n",
      "[Epoch: 3160] train loss: 0.7193, train acc: 0.7076, val loss: 0.5767, val acc: 0.8583  (best train acc: 0.8064, best val acc: 0.8786)\n",
      "[Epoch: 3180] train loss: 0.6355, train acc: 0.7567, val loss: 0.5618, val acc: 0.8651  (best train acc: 0.8064, best val acc: 0.8786)\n",
      "[Epoch: 3200] train loss: 0.6860, train acc: 0.7514, val loss: 0.5538, val acc: 0.8675  (best train acc: 0.8064, best val acc: 0.8786)\n",
      "[Epoch: 3220] train loss: 0.7048, train acc: 0.7332, val loss: 0.5855, val acc: 0.8324  (best train acc: 0.8064, best val acc: 0.8786)\n",
      "[Epoch: 3240] train loss: 0.5943, train acc: 0.8013, val loss: 0.5713, val acc: 0.8530  (best train acc: 0.8064, best val acc: 0.8786)\n",
      "[Epoch: 3260] train loss: 0.7070, train acc: 0.7131, val loss: 0.5646, val acc: 0.8594  (best train acc: 0.8064, best val acc: 0.8786)\n",
      "[Epoch: 3280] train loss: 0.8963, train acc: 0.6129, val loss: 0.5701, val acc: 0.8590  (best train acc: 0.8064, best val acc: 0.8786)\n",
      "[Epoch: 3300] train loss: 0.6831, train acc: 0.7188, val loss: 0.5525, val acc: 0.8621  (best train acc: 0.8074, best val acc: 0.8786)\n",
      "[Epoch: 3320] train loss: 0.7897, train acc: 0.6825, val loss: 0.5707, val acc: 0.8563  (best train acc: 0.8074, best val acc: 0.8786)\n",
      "[Epoch: 3340] train loss: 0.7850, train acc: 0.6625, val loss: 0.5523, val acc: 0.8577  (best train acc: 0.8074, best val acc: 0.8786)\n",
      "[Epoch: 3360] train loss: 0.7836, train acc: 0.6847, val loss: 0.5817, val acc: 0.8432  (best train acc: 0.8074, best val acc: 0.8786)\n",
      "[Epoch: 3380] train loss: 0.8083, train acc: 0.6662, val loss: 0.5510, val acc: 0.8739  (best train acc: 0.8074, best val acc: 0.8786)\n",
      "[Epoch: 3400] train loss: 0.6240, train acc: 0.7764, val loss: 0.5812, val acc: 0.8398  (best train acc: 0.8074, best val acc: 0.8786)\n",
      "[Epoch: 3420] train loss: 0.7847, train acc: 0.6749, val loss: 0.5552, val acc: 0.8641  (best train acc: 0.8074, best val acc: 0.8786)\n",
      "[Epoch: 3440] train loss: 0.7175, train acc: 0.7226, val loss: 0.5557, val acc: 0.8604  (best train acc: 0.8074, best val acc: 0.8786)\n",
      "[Epoch: 3460] train loss: 0.7923, train acc: 0.6755, val loss: 0.5635, val acc: 0.8583  (best train acc: 0.8074, best val acc: 0.8786)\n",
      "[Epoch: 3480] train loss: 0.6550, train acc: 0.7593, val loss: 0.5955, val acc: 0.8229  (best train acc: 0.8074, best val acc: 0.8786)\n",
      "[Epoch: 3500] train loss: 0.7963, train acc: 0.6777, val loss: 0.5499, val acc: 0.8705  (best train acc: 0.8074, best val acc: 0.8786)\n",
      "[Epoch: 3520] train loss: 0.7490, train acc: 0.6843, val loss: 0.5641, val acc: 0.8492  (best train acc: 0.8074, best val acc: 0.8786)\n",
      "[Epoch: 3540] train loss: 0.6622, train acc: 0.7290, val loss: 0.5612, val acc: 0.8590  (best train acc: 0.8074, best val acc: 0.8786)\n",
      "[Epoch: 3560] train loss: 0.8251, train acc: 0.6530, val loss: 0.5457, val acc: 0.8712  (best train acc: 0.8074, best val acc: 0.8786)\n",
      "[Epoch: 3580] train loss: 0.6651, train acc: 0.7421, val loss: 0.5565, val acc: 0.8695  (best train acc: 0.8074, best val acc: 0.8786)\n",
      "[Epoch: 3600] train loss: 0.5934, train acc: 0.7893, val loss: 0.5719, val acc: 0.8550  (best train acc: 0.8074, best val acc: 0.8786)\n",
      "[Epoch: 3620] train loss: 0.8525, train acc: 0.6455, val loss: 0.5688, val acc: 0.8472  (best train acc: 0.8074, best val acc: 0.8786)\n",
      "[Epoch: 3640] train loss: 0.7418, train acc: 0.7003, val loss: 0.5535, val acc: 0.8597  (best train acc: 0.8074, best val acc: 0.8786)\n",
      "[Epoch: 3660] train loss: 0.6993, train acc: 0.7307, val loss: 0.5680, val acc: 0.8533  (best train acc: 0.8074, best val acc: 0.8786)\n",
      "[Epoch: 3680] train loss: 0.6243, train acc: 0.7731, val loss: 0.5635, val acc: 0.8583  (best train acc: 0.8074, best val acc: 0.8786)\n",
      "[Epoch: 3700] train loss: 0.7045, train acc: 0.7353, val loss: 0.5786, val acc: 0.8253  (best train acc: 0.8074, best val acc: 0.8786)\n",
      "[Epoch: 3720] train loss: 0.7968, train acc: 0.6772, val loss: 0.5592, val acc: 0.8590  (best train acc: 0.8074, best val acc: 0.8786)\n",
      "[Epoch: 3740] train loss: 0.7941, train acc: 0.6791, val loss: 0.5547, val acc: 0.8614  (best train acc: 0.8074, best val acc: 0.8786)\n",
      "[Epoch: 3760] train loss: 0.8319, train acc: 0.6550, val loss: 0.5577, val acc: 0.8567  (best train acc: 0.8074, best val acc: 0.8786)\n",
      "[Epoch: 3780] train loss: 0.7365, train acc: 0.7102, val loss: 0.5520, val acc: 0.8607  (best train acc: 0.8074, best val acc: 0.8786)\n",
      "[Epoch: 3800] train loss: 0.7337, train acc: 0.7007, val loss: 0.5523, val acc: 0.8617  (best train acc: 0.8074, best val acc: 0.8786)\n",
      "[Epoch: 3820] train loss: 0.6837, train acc: 0.7445, val loss: 0.5763, val acc: 0.8327  (best train acc: 0.8074, best val acc: 0.8786)\n",
      "[Epoch: 3840] train loss: 0.7439, train acc: 0.6844, val loss: 0.5515, val acc: 0.8668  (best train acc: 0.8074, best val acc: 0.8786)\n",
      "[Epoch: 3860] train loss: 0.6025, train acc: 0.7928, val loss: 0.5836, val acc: 0.8347  (best train acc: 0.8074, best val acc: 0.8786)\n",
      "[Epoch: 3880] train loss: 0.7934, train acc: 0.6703, val loss: 0.5463, val acc: 0.8715  (best train acc: 0.8074, best val acc: 0.8786)\n",
      "[Epoch: 3900] train loss: 0.7190, train acc: 0.7122, val loss: 0.5607, val acc: 0.8577  (best train acc: 0.8074, best val acc: 0.8786)\n",
      "[Epoch: 3920] train loss: 0.7291, train acc: 0.6910, val loss: 0.5563, val acc: 0.8661  (best train acc: 0.8074, best val acc: 0.8786)\n",
      "[Epoch: 3940] train loss: 0.6673, train acc: 0.7162, val loss: 0.5456, val acc: 0.8597  (best train acc: 0.8074, best val acc: 0.8786)\n",
      "[Epoch: 3960] train loss: 0.7464, train acc: 0.7155, val loss: 0.5905, val acc: 0.8283  (best train acc: 0.8074, best val acc: 0.8786)\n",
      "[Epoch: 3980] train loss: 0.7245, train acc: 0.7211, val loss: 0.5577, val acc: 0.8580  (best train acc: 0.8074, best val acc: 0.8786)\n",
      "[Epoch: 4000] train loss: 0.6970, train acc: 0.7303, val loss: 0.5506, val acc: 0.8604  (best train acc: 0.8074, best val acc: 0.8786)\n",
      "[Epoch: 4020] train loss: 0.7852, train acc: 0.6737, val loss: 0.5607, val acc: 0.8556  (best train acc: 0.8074, best val acc: 0.8786)\n",
      "[Epoch: 4040] train loss: 0.6918, train acc: 0.7212, val loss: 0.5659, val acc: 0.8391  (best train acc: 0.8074, best val acc: 0.8786)\n",
      "[Epoch: 4060] train loss: 0.7679, train acc: 0.6795, val loss: 0.5434, val acc: 0.8718  (best train acc: 0.8074, best val acc: 0.8786)\n",
      "[Epoch: 4080] train loss: 0.8171, train acc: 0.6800, val loss: 0.5541, val acc: 0.8560  (best train acc: 0.8074, best val acc: 0.8786)\n",
      "[Epoch: 4100] train loss: 0.7043, train acc: 0.7296, val loss: 0.5625, val acc: 0.8553  (best train acc: 0.8074, best val acc: 0.8786)\n",
      "[Epoch: 4120] train loss: 0.7911, train acc: 0.6807, val loss: 0.5565, val acc: 0.8560  (best train acc: 0.8074, best val acc: 0.8786)\n",
      "[Epoch: 4140] train loss: 0.6960, train acc: 0.7337, val loss: 0.5624, val acc: 0.8550  (best train acc: 0.8074, best val acc: 0.8786)\n",
      "[Epoch: 4160] train loss: 0.7562, train acc: 0.6973, val loss: 0.5539, val acc: 0.8583  (best train acc: 0.8074, best val acc: 0.8786)\n",
      "[Epoch: 4180] train loss: 0.6990, train acc: 0.7322, val loss: 0.5520, val acc: 0.8523  (best train acc: 0.8074, best val acc: 0.8786)\n",
      "[Epoch: 4200] train loss: 0.7072, train acc: 0.7087, val loss: 0.5592, val acc: 0.8553  (best train acc: 0.8074, best val acc: 0.8786)\n",
      "[Epoch: 4220] train loss: 0.7502, train acc: 0.6934, val loss: 0.5613, val acc: 0.8546  (best train acc: 0.8074, best val acc: 0.8786)\n",
      "[Epoch: 4240] train loss: 0.5948, train acc: 0.7915, val loss: 0.5589, val acc: 0.8563  (best train acc: 0.8074, best val acc: 0.8786)\n",
      "[Epoch: 4260] train loss: 0.6242, train acc: 0.7654, val loss: 0.5495, val acc: 0.8580  (best train acc: 0.8074, best val acc: 0.8786)\n",
      "[Epoch: 4280] train loss: 0.8086, train acc: 0.6791, val loss: 0.5667, val acc: 0.8422  (best train acc: 0.8074, best val acc: 0.8786)\n",
      "[Epoch: 4300] train loss: 0.7023, train acc: 0.7290, val loss: 0.5643, val acc: 0.8543  (best train acc: 0.8074, best val acc: 0.8786)\n",
      "[Epoch: 4320] train loss: 0.6832, train acc: 0.7360, val loss: 0.5743, val acc: 0.8290  (best train acc: 0.8074, best val acc: 0.8786)\n",
      "[Epoch: 4340] train loss: 0.7891, train acc: 0.6650, val loss: 0.5438, val acc: 0.8732  (best train acc: 0.8074, best val acc: 0.8786)\n",
      "[Epoch: 4360] train loss: 0.7123, train acc: 0.7008, val loss: 0.5441, val acc: 0.8560  (best train acc: 0.8074, best val acc: 0.8786)\n",
      "[Epoch: 4380] train loss: 0.6071, train acc: 0.7930, val loss: 0.5751, val acc: 0.8344  (best train acc: 0.8074, best val acc: 0.8786)\n",
      "[Epoch: 4400] train loss: 0.7013, train acc: 0.7306, val loss: 0.5420, val acc: 0.8614  (best train acc: 0.8074, best val acc: 0.8786)\n",
      "[Epoch: 4420] train loss: 0.8181, train acc: 0.6496, val loss: 0.5423, val acc: 0.8621  (best train acc: 0.8074, best val acc: 0.8786)\n",
      "[Epoch: 4440] train loss: 0.7362, train acc: 0.7003, val loss: 0.5492, val acc: 0.8658  (best train acc: 0.8074, best val acc: 0.8786)\n",
      "[Epoch: 4460] train loss: 0.6943, train acc: 0.7356, val loss: 0.5656, val acc: 0.8583  (best train acc: 0.8074, best val acc: 0.8786)\n",
      "[Epoch: 4480] train loss: 0.8052, train acc: 0.6510, val loss: 0.5737, val acc: 0.8530  (best train acc: 0.8074, best val acc: 0.8786)\n",
      "[Epoch: 4500] train loss: 0.6964, train acc: 0.7119, val loss: 0.5389, val acc: 0.8722  (best train acc: 0.8074, best val acc: 0.8786)\n",
      "[Epoch: 4520] train loss: 0.6858, train acc: 0.6991, val loss: 0.5464, val acc: 0.8664  (best train acc: 0.8074, best val acc: 0.8786)\n",
      "[Epoch: 4540] train loss: 0.8552, train acc: 0.6355, val loss: 0.5554, val acc: 0.8573  (best train acc: 0.8074, best val acc: 0.8786)\n",
      "[Epoch: 4560] train loss: 0.8422, train acc: 0.6405, val loss: 0.5463, val acc: 0.8664  (best train acc: 0.8074, best val acc: 0.8786)\n",
      "[Epoch: 4580] train loss: 0.6968, train acc: 0.7293, val loss: 0.5994, val acc: 0.8236  (best train acc: 0.8074, best val acc: 0.8786)\n",
      "[Epoch: 4600] train loss: 0.7055, train acc: 0.7145, val loss: 0.5489, val acc: 0.8583  (best train acc: 0.8074, best val acc: 0.8786)\n",
      "[Epoch: 4620] train loss: 0.6110, train acc: 0.7697, val loss: 0.5701, val acc: 0.8331  (best train acc: 0.8074, best val acc: 0.8786)\n",
      "[Epoch: 4640] train loss: 0.6761, train acc: 0.7229, val loss: 0.5492, val acc: 0.8567  (best train acc: 0.8074, best val acc: 0.8786)\n",
      "[Epoch: 4660] train loss: 0.6549, train acc: 0.7385, val loss: 0.5588, val acc: 0.8472  (best train acc: 0.8074, best val acc: 0.8786)\n",
      "[Epoch: 4680] train loss: 0.8087, train acc: 0.6436, val loss: 0.5460, val acc: 0.8627  (best train acc: 0.8074, best val acc: 0.8786)\n",
      "[Epoch: 4700] train loss: 0.6426, train acc: 0.7489, val loss: 0.5476, val acc: 0.8695  (best train acc: 0.8074, best val acc: 0.8786)\n",
      "[Epoch: 4720] train loss: 0.6541, train acc: 0.7462, val loss: 0.5446, val acc: 0.8688  (best train acc: 0.8074, best val acc: 0.8793)\n",
      "[Epoch: 4740] train loss: 0.6473, train acc: 0.7493, val loss: 0.5529, val acc: 0.8597  (best train acc: 0.8074, best val acc: 0.8793)\n",
      "[Epoch: 4760] train loss: 0.8049, train acc: 0.6468, val loss: 0.5335, val acc: 0.8755  (best train acc: 0.8074, best val acc: 0.8793)\n",
      "[Epoch: 4780] train loss: 0.7476, train acc: 0.6893, val loss: 0.5512, val acc: 0.8610  (best train acc: 0.8074, best val acc: 0.8793)\n",
      "[Epoch: 4800] train loss: 0.6873, train acc: 0.7188, val loss: 0.5601, val acc: 0.8543  (best train acc: 0.8074, best val acc: 0.8793)\n",
      "[Epoch: 4820] train loss: 0.7117, train acc: 0.7110, val loss: 0.5491, val acc: 0.8583  (best train acc: 0.8074, best val acc: 0.8793)\n",
      "[Epoch: 4840] train loss: 0.7994, train acc: 0.6482, val loss: 0.5321, val acc: 0.8739  (best train acc: 0.8074, best val acc: 0.8793)\n",
      "[Epoch: 4860] train loss: 0.6895, train acc: 0.7350, val loss: 0.5531, val acc: 0.8594  (best train acc: 0.8074, best val acc: 0.8793)\n",
      "[Epoch: 4880] train loss: 0.8376, train acc: 0.6425, val loss: 0.5627, val acc: 0.8445  (best train acc: 0.8074, best val acc: 0.8793)\n",
      "[Epoch: 4900] train loss: 0.7562, train acc: 0.6737, val loss: 0.5448, val acc: 0.8637  (best train acc: 0.8074, best val acc: 0.8793)\n",
      "[Epoch: 4920] train loss: 0.7353, train acc: 0.7028, val loss: 0.5607, val acc: 0.8435  (best train acc: 0.8074, best val acc: 0.8793)\n",
      "[Epoch: 4940] train loss: 0.6864, train acc: 0.7274, val loss: 0.5527, val acc: 0.8546  (best train acc: 0.8074, best val acc: 0.8793)\n",
      "[Epoch: 4960] train loss: 0.6836, train acc: 0.7224, val loss: 0.5493, val acc: 0.8577  (best train acc: 0.8074, best val acc: 0.8793)\n",
      "[Epoch: 4980] train loss: 0.8126, train acc: 0.6642, val loss: 0.5541, val acc: 0.8533  (best train acc: 0.8074, best val acc: 0.8793)\n",
      "[Epoch: 5000] train loss: 0.8131, train acc: 0.6604, val loss: 0.5666, val acc: 0.8354  (best train acc: 0.8074, best val acc: 0.8793)\n",
      "[Epoch: 5020] train loss: 0.8125, train acc: 0.6318, val loss: 0.5425, val acc: 0.8651  (best train acc: 0.8074, best val acc: 0.8793)\n",
      "[Epoch: 5040] train loss: 0.6750, train acc: 0.7453, val loss: 0.5675, val acc: 0.8334  (best train acc: 0.8074, best val acc: 0.8793)\n",
      "[Epoch: 5060] train loss: 0.7167, train acc: 0.7063, val loss: 0.5459, val acc: 0.8712  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 5080] train loss: 0.7609, train acc: 0.6589, val loss: 0.5514, val acc: 0.8631  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 5100] train loss: 0.7585, train acc: 0.6889, val loss: 0.5428, val acc: 0.8755  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 5120] train loss: 0.6794, train acc: 0.7202, val loss: 0.5626, val acc: 0.8479  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 5140] train loss: 0.6874, train acc: 0.7337, val loss: 0.5395, val acc: 0.8708  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 5160] train loss: 0.7946, train acc: 0.6560, val loss: 0.5392, val acc: 0.8739  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 5180] train loss: 0.6888, train acc: 0.7178, val loss: 0.5418, val acc: 0.8691  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 5200] train loss: 0.7099, train acc: 0.7287, val loss: 0.5544, val acc: 0.8556  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 5220] train loss: 0.7015, train acc: 0.7321, val loss: 0.5772, val acc: 0.8293  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 5240] train loss: 0.7645, train acc: 0.6893, val loss: 0.5546, val acc: 0.8556  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 5260] train loss: 0.6644, train acc: 0.7298, val loss: 0.5376, val acc: 0.8671  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 5280] train loss: 0.8596, train acc: 0.6387, val loss: 0.5654, val acc: 0.8405  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 5300] train loss: 0.6816, train acc: 0.7259, val loss: 0.5654, val acc: 0.8540  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 5320] train loss: 0.8520, train acc: 0.6276, val loss: 0.5533, val acc: 0.8577  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 5340] train loss: 0.6686, train acc: 0.7258, val loss: 0.5426, val acc: 0.8664  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 5360] train loss: 0.8495, train acc: 0.6475, val loss: 0.5568, val acc: 0.8530  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 5380] train loss: 0.6387, train acc: 0.7413, val loss: 0.5319, val acc: 0.8742  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 5400] train loss: 0.6791, train acc: 0.7062, val loss: 0.5310, val acc: 0.8712  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 5420] train loss: 0.7187, train acc: 0.7133, val loss: 0.5826, val acc: 0.8398  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 5440] train loss: 0.5924, train acc: 0.7775, val loss: 0.5562, val acc: 0.8560  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 5460] train loss: 0.7957, train acc: 0.6650, val loss: 0.5461, val acc: 0.8637  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 5480] train loss: 0.7912, train acc: 0.6623, val loss: 0.5438, val acc: 0.8718  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 5500] train loss: 0.7758, train acc: 0.6742, val loss: 0.5408, val acc: 0.8661  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 5520] train loss: 0.7495, train acc: 0.7036, val loss: 0.5515, val acc: 0.8587  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 5540] train loss: 0.6000, train acc: 0.7797, val loss: 0.5786, val acc: 0.8371  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 5560] train loss: 0.6510, train acc: 0.7315, val loss: 0.5620, val acc: 0.8371  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 5580] train loss: 0.6766, train acc: 0.7392, val loss: 0.5660, val acc: 0.8476  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 5600] train loss: 0.7741, train acc: 0.6671, val loss: 0.5838, val acc: 0.8418  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 5620] train loss: 0.6717, train acc: 0.7371, val loss: 0.5490, val acc: 0.8496  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 5640] train loss: 0.6315, train acc: 0.7575, val loss: 0.5580, val acc: 0.8573  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 5660] train loss: 0.5732, train acc: 0.7990, val loss: 0.5636, val acc: 0.8476  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 5680] train loss: 0.7026, train acc: 0.7248, val loss: 0.5408, val acc: 0.8678  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 5700] train loss: 0.7060, train acc: 0.7183, val loss: 0.5522, val acc: 0.8570  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 5720] train loss: 0.6148, train acc: 0.7689, val loss: 0.5800, val acc: 0.8300  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 5740] train loss: 0.6327, train acc: 0.7629, val loss: 0.5639, val acc: 0.8519  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 5760] train loss: 0.7974, train acc: 0.6581, val loss: 0.5492, val acc: 0.8604  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 5780] train loss: 0.8471, train acc: 0.6225, val loss: 0.5417, val acc: 0.8702  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 5800] train loss: 0.7128, train acc: 0.6999, val loss: 0.5437, val acc: 0.8610  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 5820] train loss: 0.7163, train acc: 0.7154, val loss: 0.5436, val acc: 0.8715  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 5840] train loss: 0.7697, train acc: 0.6710, val loss: 0.5389, val acc: 0.8742  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 5860] train loss: 0.6407, train acc: 0.7160, val loss: 0.5324, val acc: 0.8739  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 5880] train loss: 0.6690, train acc: 0.7355, val loss: 0.5548, val acc: 0.8540  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 5900] train loss: 0.6808, train acc: 0.7204, val loss: 0.5537, val acc: 0.8590  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 5920] train loss: 0.7930, train acc: 0.6726, val loss: 0.5439, val acc: 0.8600  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 5940] train loss: 0.7106, train acc: 0.7259, val loss: 0.5668, val acc: 0.8408  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 5960] train loss: 0.6480, train acc: 0.7452, val loss: 0.5585, val acc: 0.8432  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 5980] train loss: 0.8128, train acc: 0.6647, val loss: 0.5554, val acc: 0.8492  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 6000] train loss: 0.6484, train acc: 0.7458, val loss: 0.5458, val acc: 0.8590  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 6020] train loss: 0.7060, train acc: 0.7139, val loss: 0.5707, val acc: 0.8368  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 6040] train loss: 0.6638, train acc: 0.7282, val loss: 0.5557, val acc: 0.8533  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 6060] train loss: 0.7067, train acc: 0.7248, val loss: 0.5577, val acc: 0.8442  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 6080] train loss: 0.6156, train acc: 0.7679, val loss: 0.5730, val acc: 0.8408  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 6100] train loss: 0.7371, train acc: 0.6818, val loss: 0.5468, val acc: 0.8590  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 6120] train loss: 0.6399, train acc: 0.7368, val loss: 0.5381, val acc: 0.8702  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 6140] train loss: 0.7159, train acc: 0.7072, val loss: 0.5501, val acc: 0.8503  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 6160] train loss: 0.8397, train acc: 0.6222, val loss: 0.5490, val acc: 0.8570  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 6180] train loss: 0.7281, train acc: 0.7071, val loss: 0.5449, val acc: 0.8577  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 6200] train loss: 0.5842, train acc: 0.7828, val loss: 0.5588, val acc: 0.8459  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 6220] train loss: 0.7181, train acc: 0.7180, val loss: 0.5639, val acc: 0.8489  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 6240] train loss: 0.8366, train acc: 0.6389, val loss: 0.5401, val acc: 0.8654  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 6260] train loss: 0.7659, train acc: 0.6776, val loss: 0.5416, val acc: 0.8688  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 6280] train loss: 0.6527, train acc: 0.7306, val loss: 0.5575, val acc: 0.8455  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 6300] train loss: 0.6414, train acc: 0.7356, val loss: 0.5690, val acc: 0.8418  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 6320] train loss: 0.6832, train acc: 0.7031, val loss: 0.5520, val acc: 0.8509  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 6340] train loss: 0.6767, train acc: 0.7319, val loss: 0.5769, val acc: 0.8327  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 6360] train loss: 0.7209, train acc: 0.7050, val loss: 0.5509, val acc: 0.8556  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 6380] train loss: 0.6543, train acc: 0.7249, val loss: 0.5586, val acc: 0.8573  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 6400] train loss: 0.8147, train acc: 0.6321, val loss: 0.5308, val acc: 0.8725  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 6420] train loss: 0.6274, train acc: 0.7493, val loss: 0.5614, val acc: 0.8489  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 6440] train loss: 0.7426, train acc: 0.6829, val loss: 0.5627, val acc: 0.8391  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 6460] train loss: 0.6019, train acc: 0.7677, val loss: 0.5410, val acc: 0.8590  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 6480] train loss: 0.7991, train acc: 0.6499, val loss: 0.5505, val acc: 0.8587  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 6500] train loss: 0.7027, train acc: 0.7044, val loss: 0.5473, val acc: 0.8661  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 6520] train loss: 0.8189, train acc: 0.6575, val loss: 0.5534, val acc: 0.8506  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 6540] train loss: 0.7249, train acc: 0.6858, val loss: 0.5502, val acc: 0.8506  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 6560] train loss: 0.8450, train acc: 0.6429, val loss: 0.5490, val acc: 0.8516  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 6580] train loss: 0.7027, train acc: 0.6867, val loss: 0.5670, val acc: 0.8587  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 6600] train loss: 0.5861, train acc: 0.7874, val loss: 0.5730, val acc: 0.8344  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 6620] train loss: 0.6906, train acc: 0.6805, val loss: 0.5364, val acc: 0.8722  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 6640] train loss: 0.5877, train acc: 0.7739, val loss: 0.5488, val acc: 0.8567  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 6660] train loss: 0.5621, train acc: 0.7974, val loss: 0.5611, val acc: 0.8462  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 6680] train loss: 0.6225, train acc: 0.7590, val loss: 0.5513, val acc: 0.8526  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 6700] train loss: 0.6763, train acc: 0.7131, val loss: 0.5508, val acc: 0.8563  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 6720] train loss: 0.7860, train acc: 0.6464, val loss: 0.5433, val acc: 0.8627  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 6740] train loss: 0.7227, train acc: 0.6966, val loss: 0.5805, val acc: 0.8405  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 6760] train loss: 0.8303, train acc: 0.6395, val loss: 0.5392, val acc: 0.8617  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 6780] train loss: 0.7026, train acc: 0.7032, val loss: 0.5582, val acc: 0.8361  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 6800] train loss: 0.8091, train acc: 0.6512, val loss: 0.5481, val acc: 0.8587  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 6820] train loss: 0.6642, train acc: 0.7112, val loss: 0.5611, val acc: 0.8368  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 6840] train loss: 0.6454, train acc: 0.7377, val loss: 0.5459, val acc: 0.8627  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 6860] train loss: 0.7525, train acc: 0.6884, val loss: 0.5648, val acc: 0.8523  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 6880] train loss: 0.6939, train acc: 0.7112, val loss: 0.5315, val acc: 0.8651  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 6900] train loss: 0.6139, train acc: 0.7609, val loss: 0.5526, val acc: 0.8543  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 6920] train loss: 0.7806, train acc: 0.6589, val loss: 0.5393, val acc: 0.8648  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 6940] train loss: 0.6792, train acc: 0.7345, val loss: 0.5498, val acc: 0.8489  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 6960] train loss: 0.6731, train acc: 0.7324, val loss: 0.5552, val acc: 0.8479  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 6980] train loss: 0.7929, train acc: 0.6379, val loss: 0.5539, val acc: 0.8509  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 7000] train loss: 0.6579, train acc: 0.7274, val loss: 0.5643, val acc: 0.8388  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 7020] train loss: 0.7396, train acc: 0.6921, val loss: 0.5450, val acc: 0.8533  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 7040] train loss: 0.7788, train acc: 0.6609, val loss: 0.5658, val acc: 0.8492  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 7060] train loss: 0.7393, train acc: 0.6807, val loss: 0.5355, val acc: 0.8617  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 7080] train loss: 0.7439, train acc: 0.7076, val loss: 0.5979, val acc: 0.8152  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 7100] train loss: 0.8637, train acc: 0.5870, val loss: 0.6154, val acc: 0.8121  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 7120] train loss: 0.6414, train acc: 0.7368, val loss: 0.5947, val acc: 0.8428  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 7140] train loss: 0.6858, train acc: 0.6957, val loss: 0.5875, val acc: 0.8283  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 7160] train loss: 0.7757, train acc: 0.6761, val loss: 0.5954, val acc: 0.8324  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 7180] train loss: 0.8156, train acc: 0.6599, val loss: 0.5585, val acc: 0.8496  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 7200] train loss: 0.5798, train acc: 0.7900, val loss: 0.6097, val acc: 0.8101  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 7220] train loss: 0.7589, train acc: 0.6574, val loss: 0.5548, val acc: 0.8634  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 7240] train loss: 0.7305, train acc: 0.6979, val loss: 0.5643, val acc: 0.8425  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 7260] train loss: 0.7041, train acc: 0.7029, val loss: 0.5550, val acc: 0.8358  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 7280] train loss: 0.6582, train acc: 0.7215, val loss: 0.5824, val acc: 0.8175  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 7300] train loss: 0.6563, train acc: 0.7233, val loss: 0.5834, val acc: 0.8381  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 7320] train loss: 0.6803, train acc: 0.7047, val loss: 0.5646, val acc: 0.8472  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 7340] train loss: 0.6878, train acc: 0.7071, val loss: 0.5608, val acc: 0.8442  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 7360] train loss: 0.6467, train acc: 0.7259, val loss: 0.5898, val acc: 0.8010  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 7380] train loss: 0.6832, train acc: 0.7150, val loss: 0.5652, val acc: 0.8331  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 7400] train loss: 0.6446, train acc: 0.7368, val loss: 0.5909, val acc: 0.8206  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 7420] train loss: 0.6922, train acc: 0.7272, val loss: 0.5761, val acc: 0.8472  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 7440] train loss: 0.7802, train acc: 0.6570, val loss: 0.5617, val acc: 0.8428  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 7460] train loss: 0.7957, train acc: 0.6537, val loss: 0.5644, val acc: 0.8442  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 7480] train loss: 0.8171, train acc: 0.6453, val loss: 0.5590, val acc: 0.8462  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 7500] train loss: 0.7259, train acc: 0.7079, val loss: 0.5564, val acc: 0.8432  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 7520] train loss: 0.7085, train acc: 0.7136, val loss: 0.5867, val acc: 0.8226  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 7540] train loss: 0.6263, train acc: 0.7287, val loss: 0.5695, val acc: 0.8438  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 7560] train loss: 0.6531, train acc: 0.7427, val loss: 0.5802, val acc: 0.8368  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 7580] train loss: 0.6025, train acc: 0.7501, val loss: 0.5586, val acc: 0.8465  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 7600] train loss: 0.8498, train acc: 0.6100, val loss: 0.5342, val acc: 0.8661  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 7620] train loss: 0.6666, train acc: 0.7062, val loss: 0.5449, val acc: 0.8469  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 7640] train loss: 0.6521, train acc: 0.7144, val loss: 0.5422, val acc: 0.8526  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 7660] train loss: 0.7753, train acc: 0.6750, val loss: 0.5467, val acc: 0.8492  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 7680] train loss: 0.6898, train acc: 0.7214, val loss: 0.5666, val acc: 0.8344  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 7700] train loss: 0.6910, train acc: 0.7107, val loss: 0.5675, val acc: 0.8273  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 7720] train loss: 0.6753, train acc: 0.7275, val loss: 0.5391, val acc: 0.8378  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 7740] train loss: 0.7617, train acc: 0.6643, val loss: 0.5274, val acc: 0.8702  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 7760] train loss: 0.6066, train acc: 0.7628, val loss: 0.5359, val acc: 0.8452  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 7780] train loss: 0.7153, train acc: 0.7060, val loss: 0.5250, val acc: 0.8688  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 7800] train loss: 0.6695, train acc: 0.7186, val loss: 0.5559, val acc: 0.8364  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 7820] train loss: 0.6889, train acc: 0.7153, val loss: 0.5411, val acc: 0.8398  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 7840] train loss: 0.7380, train acc: 0.6938, val loss: 0.5405, val acc: 0.8553  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 7860] train loss: 0.6921, train acc: 0.6971, val loss: 0.5297, val acc: 0.8503  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 7880] train loss: 0.6556, train acc: 0.7258, val loss: 0.5410, val acc: 0.8526  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 7900] train loss: 0.6029, train acc: 0.7601, val loss: 0.5352, val acc: 0.8540  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 7920] train loss: 0.6805, train acc: 0.7142, val loss: 0.5536, val acc: 0.8263  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 7940] train loss: 0.6969, train acc: 0.7222, val loss: 0.5335, val acc: 0.8452  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 7960] train loss: 0.7599, train acc: 0.6786, val loss: 0.5446, val acc: 0.8374  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 7980] train loss: 0.6593, train acc: 0.7271, val loss: 0.5256, val acc: 0.8695  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 8000] train loss: 0.5637, train acc: 0.7824, val loss: 0.5460, val acc: 0.8455  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 8020] train loss: 0.5967, train acc: 0.7627, val loss: 0.5443, val acc: 0.8489  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 8040] train loss: 0.6241, train acc: 0.7379, val loss: 0.5557, val acc: 0.8219  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 8060] train loss: 0.6604, train acc: 0.7158, val loss: 0.5342, val acc: 0.8570  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 8080] train loss: 0.6065, train acc: 0.7671, val loss: 0.5304, val acc: 0.8530  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 8100] train loss: 0.5970, train acc: 0.7660, val loss: 0.5372, val acc: 0.8560  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 8120] train loss: 0.7820, train acc: 0.6583, val loss: 0.5187, val acc: 0.8637  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 8140] train loss: 0.6815, train acc: 0.7248, val loss: 0.5552, val acc: 0.8465  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 8160] train loss: 0.7481, train acc: 0.6875, val loss: 0.5362, val acc: 0.8567  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 8180] train loss: 0.7451, train acc: 0.6564, val loss: 0.5331, val acc: 0.8641  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 8200] train loss: 0.5488, train acc: 0.8072, val loss: 0.5585, val acc: 0.8216  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 8220] train loss: 0.6805, train acc: 0.7151, val loss: 0.5483, val acc: 0.8516  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 8240] train loss: 0.7365, train acc: 0.6876, val loss: 0.5211, val acc: 0.8540  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 8260] train loss: 0.6559, train acc: 0.7337, val loss: 0.5255, val acc: 0.8661  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 8280] train loss: 0.5886, train acc: 0.7537, val loss: 0.5289, val acc: 0.8543  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 8300] train loss: 0.5757, train acc: 0.7790, val loss: 0.5479, val acc: 0.8293  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 8320] train loss: 0.6286, train acc: 0.7487, val loss: 0.5482, val acc: 0.8459  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 8340] train loss: 0.8004, train acc: 0.6401, val loss: 0.5421, val acc: 0.8469  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 8360] train loss: 0.6867, train acc: 0.7305, val loss: 0.5569, val acc: 0.8209  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 8380] train loss: 0.7000, train acc: 0.7126, val loss: 0.5382, val acc: 0.8354  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 8400] train loss: 0.6565, train acc: 0.7422, val loss: 0.5396, val acc: 0.8516  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 8420] train loss: 0.7749, train acc: 0.6706, val loss: 0.5273, val acc: 0.8641  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 8440] train loss: 0.6469, train acc: 0.7244, val loss: 0.5462, val acc: 0.8344  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 8460] train loss: 0.5877, train acc: 0.7776, val loss: 0.5530, val acc: 0.8347  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 8480] train loss: 0.7126, train acc: 0.6927, val loss: 0.5194, val acc: 0.8671  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 8500] train loss: 0.7449, train acc: 0.6979, val loss: 0.5308, val acc: 0.8580  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 8520] train loss: 0.6749, train acc: 0.7091, val loss: 0.5449, val acc: 0.8489  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 8540] train loss: 0.6464, train acc: 0.7328, val loss: 0.5324, val acc: 0.8472  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 8560] train loss: 0.6989, train acc: 0.7041, val loss: 0.5241, val acc: 0.8664  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 8580] train loss: 0.6642, train acc: 0.7191, val loss: 0.5720, val acc: 0.8145  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 8600] train loss: 0.7077, train acc: 0.7013, val loss: 0.5692, val acc: 0.8199  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 8620] train loss: 0.6749, train acc: 0.7190, val loss: 0.5546, val acc: 0.8273  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 8640] train loss: 0.6603, train acc: 0.7206, val loss: 0.5352, val acc: 0.8499  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 8660] train loss: 0.6876, train acc: 0.7157, val loss: 0.5423, val acc: 0.8411  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 8680] train loss: 0.6808, train acc: 0.7084, val loss: 0.5319, val acc: 0.8401  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 8700] train loss: 0.6725, train acc: 0.7351, val loss: 0.5314, val acc: 0.8664  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 8720] train loss: 0.7412, train acc: 0.6984, val loss: 0.5568, val acc: 0.8216  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 8740] train loss: 0.8543, train acc: 0.6217, val loss: 0.5451, val acc: 0.8358  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 8760] train loss: 0.5670, train acc: 0.7890, val loss: 0.5390, val acc: 0.8422  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 8780] train loss: 0.8048, train acc: 0.6518, val loss: 0.5180, val acc: 0.8658  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 8800] train loss: 0.6082, train acc: 0.7689, val loss: 0.5575, val acc: 0.8209  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 8820] train loss: 0.5895, train acc: 0.7699, val loss: 0.5725, val acc: 0.8216  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 8840] train loss: 0.7852, train acc: 0.6593, val loss: 0.5367, val acc: 0.8597  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 8860] train loss: 0.7062, train acc: 0.6877, val loss: 0.5388, val acc: 0.8530  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 8880] train loss: 0.7093, train acc: 0.7066, val loss: 0.5217, val acc: 0.8577  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 8900] train loss: 0.7772, train acc: 0.6532, val loss: 0.5358, val acc: 0.8489  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 8920] train loss: 0.5577, train acc: 0.7871, val loss: 0.5384, val acc: 0.8452  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 8940] train loss: 0.5822, train acc: 0.7664, val loss: 0.5326, val acc: 0.8519  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 8960] train loss: 0.6783, train acc: 0.7284, val loss: 0.5402, val acc: 0.8415  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 8980] train loss: 0.7074, train acc: 0.6944, val loss: 0.5185, val acc: 0.8637  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 9000] train loss: 0.6549, train acc: 0.7289, val loss: 0.5250, val acc: 0.8597  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 9020] train loss: 0.5759, train acc: 0.7804, val loss: 0.5280, val acc: 0.8567  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 9040] train loss: 0.6087, train acc: 0.7591, val loss: 0.5328, val acc: 0.8516  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 9060] train loss: 0.8554, train acc: 0.6208, val loss: 0.5314, val acc: 0.8580  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 9080] train loss: 0.7393, train acc: 0.6712, val loss: 0.5199, val acc: 0.8644  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 9100] train loss: 0.6661, train acc: 0.7319, val loss: 0.5679, val acc: 0.8169  (best train acc: 0.8074, best val acc: 0.8796)\n",
      "[Epoch: 9120] train loss: 0.5438, train acc: 0.8080, val loss: 0.5758, val acc: 0.8145  (best train acc: 0.8080, best val acc: 0.8796)\n",
      "[Epoch: 9140] train loss: 0.6659, train acc: 0.7139, val loss: 0.5723, val acc: 0.8287  (best train acc: 0.8080, best val acc: 0.8796)\n",
      "[Epoch: 9160] train loss: 0.6384, train acc: 0.7412, val loss: 0.5420, val acc: 0.8354  (best train acc: 0.8080, best val acc: 0.8796)\n",
      "[Epoch: 9180] train loss: 0.6342, train acc: 0.7319, val loss: 0.5378, val acc: 0.8418  (best train acc: 0.8080, best val acc: 0.8796)\n",
      "[Epoch: 9200] train loss: 0.6708, train acc: 0.7165, val loss: 0.5579, val acc: 0.8317  (best train acc: 0.8080, best val acc: 0.8796)\n",
      "[Epoch: 9220] train loss: 0.6077, train acc: 0.7507, val loss: 0.5273, val acc: 0.8486  (best train acc: 0.8081, best val acc: 0.8796)\n",
      "[Epoch: 9240] train loss: 0.6788, train acc: 0.7252, val loss: 0.5574, val acc: 0.8492  (best train acc: 0.8081, best val acc: 0.8796)\n",
      "[Epoch: 9260] train loss: 0.6777, train acc: 0.7029, val loss: 0.5109, val acc: 0.8664  (best train acc: 0.8081, best val acc: 0.8796)\n",
      "[Epoch: 9280] train loss: 0.6753, train acc: 0.7277, val loss: 0.5294, val acc: 0.8499  (best train acc: 0.8081, best val acc: 0.8796)\n",
      "[Epoch: 9300] train loss: 0.7110, train acc: 0.7061, val loss: 0.5201, val acc: 0.8644  (best train acc: 0.8081, best val acc: 0.8796)\n",
      "[Epoch: 9320] train loss: 0.5921, train acc: 0.7473, val loss: 0.5267, val acc: 0.8449  (best train acc: 0.8081, best val acc: 0.8796)\n",
      "[Epoch: 9340] train loss: 0.6719, train acc: 0.7122, val loss: 0.5463, val acc: 0.8415  (best train acc: 0.8081, best val acc: 0.8796)\n",
      "[Epoch: 9360] train loss: 0.5727, train acc: 0.7817, val loss: 0.5730, val acc: 0.8212  (best train acc: 0.8081, best val acc: 0.8796)\n",
      "[Epoch: 9380] train loss: 0.6581, train acc: 0.7214, val loss: 0.5644, val acc: 0.8266  (best train acc: 0.8081, best val acc: 0.8796)\n",
      "[Epoch: 9400] train loss: 0.5694, train acc: 0.7818, val loss: 0.5402, val acc: 0.8381  (best train acc: 0.8081, best val acc: 0.8796)\n",
      "[Epoch: 9420] train loss: 0.6608, train acc: 0.7393, val loss: 0.5318, val acc: 0.8482  (best train acc: 0.8081, best val acc: 0.8796)\n",
      "[Epoch: 9440] train loss: 0.6833, train acc: 0.7050, val loss: 0.5158, val acc: 0.8675  (best train acc: 0.8081, best val acc: 0.8796)\n",
      "[Epoch: 9460] train loss: 0.5588, train acc: 0.7871, val loss: 0.5375, val acc: 0.8358  (best train acc: 0.8081, best val acc: 0.8796)\n",
      "[Epoch: 9480] train loss: 0.6911, train acc: 0.7014, val loss: 0.5413, val acc: 0.8496  (best train acc: 0.8081, best val acc: 0.8796)\n",
      "[Epoch: 9500] train loss: 0.7089, train acc: 0.7041, val loss: 0.5686, val acc: 0.8185  (best train acc: 0.8081, best val acc: 0.8796)\n",
      "[Epoch: 9520] train loss: 0.7462, train acc: 0.6771, val loss: 0.5089, val acc: 0.8624  (best train acc: 0.8081, best val acc: 0.8796)\n",
      "[Epoch: 9540] train loss: 0.6610, train acc: 0.7399, val loss: 0.5544, val acc: 0.8209  (best train acc: 0.8081, best val acc: 0.8796)\n",
      "[Epoch: 9560] train loss: 0.7507, train acc: 0.6813, val loss: 0.5226, val acc: 0.8614  (best train acc: 0.8081, best val acc: 0.8796)\n",
      "[Epoch: 9580] train loss: 0.6686, train acc: 0.7178, val loss: 0.5222, val acc: 0.8533  (best train acc: 0.8081, best val acc: 0.8796)\n",
      "[Epoch: 9600] train loss: 0.6098, train acc: 0.7757, val loss: 0.5564, val acc: 0.8229  (best train acc: 0.8081, best val acc: 0.8796)\n",
      "[Epoch: 9620] train loss: 0.6405, train acc: 0.7316, val loss: 0.5563, val acc: 0.8172  (best train acc: 0.8081, best val acc: 0.8796)\n",
      "[Epoch: 9640] train loss: 0.7075, train acc: 0.7066, val loss: 0.5127, val acc: 0.8664  (best train acc: 0.8081, best val acc: 0.8796)\n",
      "[Epoch: 9660] train loss: 0.6806, train acc: 0.6944, val loss: 0.5109, val acc: 0.8577  (best train acc: 0.8081, best val acc: 0.8796)\n",
      "[Epoch: 9680] train loss: 0.7222, train acc: 0.6903, val loss: 0.5091, val acc: 0.8735  (best train acc: 0.8081, best val acc: 0.8796)\n",
      "[Epoch: 9700] train loss: 0.5721, train acc: 0.7667, val loss: 0.5232, val acc: 0.8577  (best train acc: 0.8081, best val acc: 0.8796)\n",
      "[Epoch: 9720] train loss: 0.6614, train acc: 0.7272, val loss: 0.5213, val acc: 0.8607  (best train acc: 0.8081, best val acc: 0.8796)\n",
      "[Epoch: 9740] train loss: 0.5615, train acc: 0.7839, val loss: 0.5214, val acc: 0.8661  (best train acc: 0.8081, best val acc: 0.8796)\n",
      "[Epoch: 9760] train loss: 0.7316, train acc: 0.6841, val loss: 0.5685, val acc: 0.8216  (best train acc: 0.8081, best val acc: 0.8796)\n",
      "[Epoch: 9780] train loss: 0.7135, train acc: 0.6929, val loss: 0.6162, val acc: 0.8280  (best train acc: 0.8081, best val acc: 0.8796)\n",
      "[Epoch: 9800] train loss: 0.7027, train acc: 0.6865, val loss: 0.5103, val acc: 0.8708  (best train acc: 0.8081, best val acc: 0.8796)\n",
      "[Epoch: 9820] train loss: 0.6145, train acc: 0.7483, val loss: 0.5427, val acc: 0.8273  (best train acc: 0.8081, best val acc: 0.8796)\n",
      "[Epoch: 9840] train loss: 0.6761, train acc: 0.7266, val loss: 0.5322, val acc: 0.8405  (best train acc: 0.8081, best val acc: 0.8796)\n",
      "[Epoch: 9860] train loss: 0.6641, train acc: 0.7301, val loss: 0.5172, val acc: 0.8617  (best train acc: 0.8081, best val acc: 0.8796)\n",
      "[Epoch: 9880] train loss: 0.6796, train acc: 0.7228, val loss: 0.5286, val acc: 0.8543  (best train acc: 0.8081, best val acc: 0.8796)\n",
      "[Epoch: 9900] train loss: 0.8066, train acc: 0.6483, val loss: 0.5258, val acc: 0.8712  (best train acc: 0.8081, best val acc: 0.8796)\n",
      "[Epoch: 9920] train loss: 0.6115, train acc: 0.7633, val loss: 0.5695, val acc: 0.8196  (best train acc: 0.8081, best val acc: 0.8796)\n",
      "[Epoch: 9940] train loss: 0.6364, train acc: 0.7334, val loss: 0.5160, val acc: 0.8482  (best train acc: 0.8141, best val acc: 0.8796)\n",
      "[Epoch: 9960] train loss: 0.8189, train acc: 0.6389, val loss: 0.5159, val acc: 0.8698  (best train acc: 0.8141, best val acc: 0.8796)\n",
      "[Epoch: 9980] train loss: 0.6665, train acc: 0.7316, val loss: 0.5445, val acc: 0.8229  (best train acc: 0.8141, best val acc: 0.8796)\n",
      "[Epoch: 10000] train loss: 0.6097, train acc: 0.7426, val loss: 0.5178, val acc: 0.8550  (best train acc: 0.8141, best val acc: 0.8796)\n",
      "[Epoch: 10020] train loss: 0.6731, train acc: 0.7204, val loss: 0.5426, val acc: 0.8250  (best train acc: 0.8141, best val acc: 0.8796)\n",
      "[Epoch: 10040] train loss: 0.7744, train acc: 0.6757, val loss: 0.5180, val acc: 0.8597  (best train acc: 0.8141, best val acc: 0.8796)\n",
      "[Epoch: 10060] train loss: 0.6896, train acc: 0.7089, val loss: 0.5108, val acc: 0.8661  (best train acc: 0.8141, best val acc: 0.8796)\n",
      "[Epoch: 10080] train loss: 0.6378, train acc: 0.7347, val loss: 0.5171, val acc: 0.8550  (best train acc: 0.8141, best val acc: 0.8796)\n",
      "[Epoch: 10100] train loss: 0.8184, train acc: 0.6548, val loss: 0.6279, val acc: 0.8428  (best train acc: 0.8141, best val acc: 0.8796)\n",
      "[Epoch: 10120] train loss: 0.7419, train acc: 0.6844, val loss: 0.6419, val acc: 0.8034  (best train acc: 0.8141, best val acc: 0.8796)\n",
      "[Epoch: 10140] train loss: 0.8282, train acc: 0.6072, val loss: 0.5682, val acc: 0.8472  (best train acc: 0.8141, best val acc: 0.8796)\n",
      "[Epoch: 10160] train loss: 0.6989, train acc: 0.6791, val loss: 0.6373, val acc: 0.7734  (best train acc: 0.8141, best val acc: 0.8796)\n",
      "[Epoch: 10180] train loss: 0.7220, train acc: 0.6879, val loss: 0.6486, val acc: 0.7376  (best train acc: 0.8141, best val acc: 0.8796)\n",
      "[Epoch: 10200] train loss: 0.7095, train acc: 0.6897, val loss: 0.5965, val acc: 0.8067  (best train acc: 0.8141, best val acc: 0.8796)\n",
      "[Epoch: 10220] train loss: 0.6960, train acc: 0.6903, val loss: 0.5556, val acc: 0.8418  (best train acc: 0.8141, best val acc: 0.8796)\n",
      "[Epoch: 10240] train loss: 0.6889, train acc: 0.6882, val loss: 0.5607, val acc: 0.8297  (best train acc: 0.8141, best val acc: 0.8796)\n",
      "[Epoch: 10260] train loss: 0.7172, train acc: 0.6837, val loss: 0.5597, val acc: 0.8361  (best train acc: 0.8141, best val acc: 0.8796)\n",
      "[Epoch: 10280] train loss: 0.7386, train acc: 0.6893, val loss: 0.5719, val acc: 0.8476  (best train acc: 0.8141, best val acc: 0.8796)\n",
      "[Epoch: 10300] train loss: 0.7020, train acc: 0.7061, val loss: 0.5601, val acc: 0.8331  (best train acc: 0.8141, best val acc: 0.8796)\n",
      "[Epoch: 10320] train loss: 0.7025, train acc: 0.6944, val loss: 0.5703, val acc: 0.8263  (best train acc: 0.8141, best val acc: 0.8796)\n",
      "[Epoch: 10340] train loss: 0.6548, train acc: 0.7241, val loss: 0.5709, val acc: 0.8185  (best train acc: 0.8141, best val acc: 0.8796)\n",
      "[Epoch: 10360] train loss: 0.6382, train acc: 0.7415, val loss: 0.5818, val acc: 0.8334  (best train acc: 0.8141, best val acc: 0.8796)\n",
      "[Epoch: 10380] train loss: 0.6285, train acc: 0.7183, val loss: 0.5556, val acc: 0.8388  (best train acc: 0.8141, best val acc: 0.8796)\n",
      "[Epoch: 10400] train loss: 0.6783, train acc: 0.7278, val loss: 0.5754, val acc: 0.8290  (best train acc: 0.8141, best val acc: 0.8796)\n",
      "[Epoch: 10420] train loss: 0.6613, train acc: 0.7462, val loss: 0.5661, val acc: 0.8246  (best train acc: 0.8141, best val acc: 0.8796)\n",
      "[Epoch: 10440] train loss: 0.6011, train acc: 0.7589, val loss: 0.5788, val acc: 0.8169  (best train acc: 0.8141, best val acc: 0.8796)\n",
      "[Epoch: 10460] train loss: 0.7107, train acc: 0.6951, val loss: 0.5431, val acc: 0.8506  (best train acc: 0.8141, best val acc: 0.8796)\n",
      "[Epoch: 10480] train loss: 0.6073, train acc: 0.7600, val loss: 0.5531, val acc: 0.8263  (best train acc: 0.8141, best val acc: 0.8796)\n",
      "[Epoch: 10500] train loss: 0.6816, train acc: 0.7204, val loss: 0.5476, val acc: 0.8496  (best train acc: 0.8141, best val acc: 0.8796)\n",
      "[Epoch: 10520] train loss: 0.6537, train acc: 0.7532, val loss: 0.5720, val acc: 0.8125  (best train acc: 0.8141, best val acc: 0.8796)\n",
      "[Epoch: 10540] train loss: 0.6399, train acc: 0.7331, val loss: 0.5646, val acc: 0.8469  (best train acc: 0.8141, best val acc: 0.8796)\n",
      "[Epoch: 10560] train loss: 0.7112, train acc: 0.7000, val loss: 0.5603, val acc: 0.8327  (best train acc: 0.8141, best val acc: 0.8796)\n",
      "[Epoch: 10580] train loss: 0.8157, train acc: 0.6432, val loss: 0.5388, val acc: 0.8435  (best train acc: 0.8141, best val acc: 0.8796)\n",
      "[Epoch: 10600] train loss: 0.6458, train acc: 0.7503, val loss: 0.5594, val acc: 0.8260  (best train acc: 0.8141, best val acc: 0.8796)\n",
      "[Epoch: 10620] train loss: 0.7516, train acc: 0.6859, val loss: 0.5607, val acc: 0.8159  (best train acc: 0.8141, best val acc: 0.8796)\n",
      "[Epoch: 10640] train loss: 0.7098, train acc: 0.6899, val loss: 0.5548, val acc: 0.8486  (best train acc: 0.8141, best val acc: 0.8796)\n",
      "[Epoch: 10660] train loss: 0.6770, train acc: 0.7176, val loss: 0.5803, val acc: 0.8094  (best train acc: 0.8141, best val acc: 0.8796)\n",
      "[Epoch: 10680] train loss: 0.7826, train acc: 0.6510, val loss: 0.5289, val acc: 0.8462  (best train acc: 0.8141, best val acc: 0.8796)\n",
      "[Epoch: 10700] train loss: 0.5945, train acc: 0.7518, val loss: 0.5490, val acc: 0.8253  (best train acc: 0.8141, best val acc: 0.8796)\n",
      "[Epoch: 10720] train loss: 0.6234, train acc: 0.7433, val loss: 0.5376, val acc: 0.8438  (best train acc: 0.8141, best val acc: 0.8796)\n",
      "[Epoch: 10740] train loss: 0.7998, train acc: 0.6525, val loss: 0.5251, val acc: 0.8509  (best train acc: 0.8141, best val acc: 0.8796)\n",
      "[Epoch: 10760] train loss: 0.6584, train acc: 0.7288, val loss: 0.5535, val acc: 0.8172  (best train acc: 0.8141, best val acc: 0.8796)\n",
      "[Epoch: 10780] train loss: 0.5951, train acc: 0.7616, val loss: 0.5433, val acc: 0.8256  (best train acc: 0.8141, best val acc: 0.8796)\n",
      "[Epoch: 10800] train loss: 0.6299, train acc: 0.7414, val loss: 0.5227, val acc: 0.8526  (best train acc: 0.8141, best val acc: 0.8796)\n",
      "[Epoch: 10820] train loss: 0.7661, train acc: 0.6664, val loss: 0.5096, val acc: 0.8634  (best train acc: 0.8141, best val acc: 0.8796)\n",
      "[Epoch: 10840] train loss: 0.6628, train acc: 0.7376, val loss: 0.5287, val acc: 0.8449  (best train acc: 0.8141, best val acc: 0.8796)\n",
      "[Epoch: 10860] train loss: 0.6129, train acc: 0.7363, val loss: 0.5171, val acc: 0.8546  (best train acc: 0.8141, best val acc: 0.8796)\n",
      "[Epoch: 10880] train loss: 0.7981, train acc: 0.6486, val loss: 0.5132, val acc: 0.8624  (best train acc: 0.8141, best val acc: 0.8796)\n",
      "[Epoch: 10900] train loss: 0.6934, train acc: 0.7128, val loss: 0.5300, val acc: 0.8422  (best train acc: 0.8141, best val acc: 0.8796)\n",
      "[Epoch: 10920] train loss: 0.6411, train acc: 0.7230, val loss: 0.5425, val acc: 0.8405  (best train acc: 0.8141, best val acc: 0.8796)\n",
      "[Epoch: 10940] train loss: 0.6931, train acc: 0.6889, val loss: 0.5148, val acc: 0.8627  (best train acc: 0.8141, best val acc: 0.8796)\n",
      "[Epoch: 10960] train loss: 0.6171, train acc: 0.7388, val loss: 0.5359, val acc: 0.8398  (best train acc: 0.8141, best val acc: 0.8796)\n",
      "[Epoch: 10980] train loss: 0.6966, train acc: 0.7093, val loss: 0.5191, val acc: 0.8577  (best train acc: 0.8141, best val acc: 0.8796)\n",
      "[Epoch: 11000] train loss: 0.6671, train acc: 0.7305, val loss: 0.5274, val acc: 0.8455  (best train acc: 0.8141, best val acc: 0.8796)\n",
      "[Epoch: 11020] train loss: 0.7132, train acc: 0.6951, val loss: 0.5242, val acc: 0.8526  (best train acc: 0.8141, best val acc: 0.8796)\n",
      "[Epoch: 11040] train loss: 0.6971, train acc: 0.6930, val loss: 0.5143, val acc: 0.8594  (best train acc: 0.8141, best val acc: 0.8796)\n",
      "[Epoch: 11060] train loss: 0.5857, train acc: 0.7656, val loss: 0.5303, val acc: 0.8368  (best train acc: 0.8141, best val acc: 0.8796)\n",
      "[Epoch: 11080] train loss: 0.6802, train acc: 0.7217, val loss: 0.5408, val acc: 0.8331  (best train acc: 0.8141, best val acc: 0.8796)\n",
      "[Epoch: 11100] train loss: 0.6841, train acc: 0.6912, val loss: 0.5483, val acc: 0.8388  (best train acc: 0.8141, best val acc: 0.8796)\n",
      "[Epoch: 11120] train loss: 0.6346, train acc: 0.7385, val loss: 0.5557, val acc: 0.8263  (best train acc: 0.8141, best val acc: 0.8796)\n",
      "[Epoch: 11140] train loss: 0.6341, train acc: 0.7492, val loss: 0.5403, val acc: 0.8290  (best train acc: 0.8141, best val acc: 0.8796)\n",
      "[Epoch: 11160] train loss: 0.6670, train acc: 0.7251, val loss: 0.5381, val acc: 0.8300  (best train acc: 0.8141, best val acc: 0.8796)\n",
      "[Epoch: 11180] train loss: 0.8446, train acc: 0.6346, val loss: 0.5623, val acc: 0.8260  (best train acc: 0.8141, best val acc: 0.8796)\n",
      "[Epoch: 11200] train loss: 0.6988, train acc: 0.7200, val loss: 0.5133, val acc: 0.8523  (best train acc: 0.8141, best val acc: 0.8796)\n",
      "[Epoch: 11220] train loss: 0.7718, train acc: 0.6532, val loss: 0.5139, val acc: 0.8749  (best train acc: 0.8141, best val acc: 0.8796)\n",
      "[Epoch: 11240] train loss: 0.7008, train acc: 0.7122, val loss: 0.5601, val acc: 0.8159  (best train acc: 0.8141, best val acc: 0.8796)\n",
      "[Epoch: 11260] train loss: 0.7694, train acc: 0.6716, val loss: 0.5685, val acc: 0.8185  (best train acc: 0.8141, best val acc: 0.8796)\n",
      "[Epoch: 11280] train loss: 0.7157, train acc: 0.6855, val loss: 0.5609, val acc: 0.8304  (best train acc: 0.8141, best val acc: 0.8796)\n",
      "[Epoch: 11300] train loss: 0.5942, train acc: 0.7508, val loss: 0.5296, val acc: 0.8304  (best train acc: 0.8141, best val acc: 0.8796)\n",
      "[Epoch: 11320] train loss: 0.6389, train acc: 0.7233, val loss: 0.5511, val acc: 0.8594  (best train acc: 0.8141, best val acc: 0.8796)\n",
      "[Epoch: 11340] train loss: 0.7939, train acc: 0.6632, val loss: 0.5469, val acc: 0.8088  (best train acc: 0.8141, best val acc: 0.8796)\n",
      "[Epoch: 11360] train loss: 0.6757, train acc: 0.7179, val loss: 0.4977, val acc: 0.8503  (best train acc: 0.8141, best val acc: 0.8796)\n",
      "[Epoch: 11380] train loss: 0.6480, train acc: 0.7442, val loss: 0.4879, val acc: 0.8287  (best train acc: 0.8141, best val acc: 0.8796)\n",
      "[Epoch: 11400] train loss: 0.5864, train acc: 0.7519, val loss: 0.4607, val acc: 0.8573  (best train acc: 0.8141, best val acc: 0.8796)\n",
      "[Epoch: 11420] train loss: 0.5382, train acc: 0.7852, val loss: 0.4896, val acc: 0.8310  (best train acc: 0.8151, best val acc: 0.8796)\n",
      "[Epoch: 11440] train loss: 0.6447, train acc: 0.7488, val loss: 0.4574, val acc: 0.8698  (best train acc: 0.8156, best val acc: 0.8796)\n",
      "[Epoch: 11460] train loss: 0.5332, train acc: 0.7856, val loss: 0.4497, val acc: 0.8702  (best train acc: 0.8172, best val acc: 0.8796)\n",
      "[Epoch: 11480] train loss: 0.4974, train acc: 0.8237, val loss: 0.4693, val acc: 0.8685  (best train acc: 0.8237, best val acc: 0.8796)\n",
      "[Epoch: 11500] train loss: 0.6076, train acc: 0.7444, val loss: 0.4869, val acc: 0.8354  (best train acc: 0.8239, best val acc: 0.8796)\n",
      "[Epoch: 11520] train loss: 0.5046, train acc: 0.8151, val loss: 0.4478, val acc: 0.8718  (best train acc: 0.8335, best val acc: 0.8796)\n",
      "[Epoch: 11540] train loss: 0.6118, train acc: 0.7548, val loss: 0.4528, val acc: 0.8610  (best train acc: 0.8335, best val acc: 0.8796)\n",
      "[Epoch: 11560] train loss: 0.5129, train acc: 0.8039, val loss: 0.4716, val acc: 0.8459  (best train acc: 0.8335, best val acc: 0.8796)\n",
      "[Epoch: 11580] train loss: 0.4793, train acc: 0.8276, val loss: 0.4404, val acc: 0.8691  (best train acc: 0.8335, best val acc: 0.8796)\n",
      "[Epoch: 11600] train loss: 0.5790, train acc: 0.7612, val loss: 0.4441, val acc: 0.8732  (best train acc: 0.8357, best val acc: 0.8796)\n",
      "[Epoch: 11620] train loss: 0.5915, train acc: 0.7720, val loss: 0.4373, val acc: 0.8722  (best train acc: 0.8357, best val acc: 0.8796)\n",
      "[Epoch: 11640] train loss: 0.5195, train acc: 0.7881, val loss: 0.4411, val acc: 0.8685  (best train acc: 0.8357, best val acc: 0.8796)\n",
      "[Epoch: 11660] train loss: 0.5652, train acc: 0.7896, val loss: 0.4896, val acc: 0.8317  (best train acc: 0.8357, best val acc: 0.8796)\n",
      "[Epoch: 11680] train loss: 0.5126, train acc: 0.8025, val loss: 0.4311, val acc: 0.8722  (best train acc: 0.8357, best val acc: 0.8796)\n",
      "[Epoch: 11700] train loss: 0.6017, train acc: 0.7726, val loss: 0.4557, val acc: 0.8644  (best train acc: 0.8357, best val acc: 0.8799)\n",
      "[Epoch: 11720] train loss: 0.4906, train acc: 0.8188, val loss: 0.4295, val acc: 0.8816  (best train acc: 0.8357, best val acc: 0.8820)\n",
      "[Epoch: 11740] train loss: 0.5109, train acc: 0.8067, val loss: 0.4431, val acc: 0.8681  (best train acc: 0.8357, best val acc: 0.8820)\n",
      "[Epoch: 11760] train loss: 0.6175, train acc: 0.7404, val loss: 0.4244, val acc: 0.8766  (best train acc: 0.8357, best val acc: 0.8820)\n",
      "[Epoch: 11780] train loss: 0.4812, train acc: 0.8286, val loss: 0.4360, val acc: 0.8745  (best train acc: 0.8357, best val acc: 0.8826)\n",
      "[Epoch: 11800] train loss: 0.5741, train acc: 0.7813, val loss: 0.4269, val acc: 0.8820  (best train acc: 0.8357, best val acc: 0.8833)\n",
      "[Epoch: 11820] train loss: 0.5039, train acc: 0.8147, val loss: 0.4135, val acc: 0.8776  (best train acc: 0.8357, best val acc: 0.8833)\n",
      "[Epoch: 11840] train loss: 0.6986, train acc: 0.7119, val loss: 0.4417, val acc: 0.8577  (best train acc: 0.8357, best val acc: 0.8833)\n",
      "[Epoch: 11860] train loss: 0.4984, train acc: 0.8104, val loss: 0.4592, val acc: 0.8641  (best train acc: 0.8357, best val acc: 0.8833)\n",
      "[Epoch: 11880] train loss: 0.5159, train acc: 0.8023, val loss: 0.4555, val acc: 0.8587  (best train acc: 0.8357, best val acc: 0.8833)\n",
      "[Epoch: 11900] train loss: 0.6068, train acc: 0.7535, val loss: 0.4224, val acc: 0.8755  (best train acc: 0.8357, best val acc: 0.8833)\n",
      "[Epoch: 11920] train loss: 0.5468, train acc: 0.7882, val loss: 0.4335, val acc: 0.8728  (best train acc: 0.8357, best val acc: 0.8833)\n",
      "[Epoch: 11940] train loss: 0.5846, train acc: 0.7711, val loss: 0.4297, val acc: 0.8715  (best train acc: 0.8357, best val acc: 0.8833)\n",
      "[Epoch: 11960] train loss: 0.6130, train acc: 0.7658, val loss: 0.4231, val acc: 0.8708  (best train acc: 0.8357, best val acc: 0.8833)\n",
      "[Epoch: 11980] train loss: 0.5274, train acc: 0.7862, val loss: 0.4429, val acc: 0.8624  (best train acc: 0.8357, best val acc: 0.8833)\n",
      "[Epoch: 12000] train loss: 0.4964, train acc: 0.8068, val loss: 0.4499, val acc: 0.8610  (best train acc: 0.8357, best val acc: 0.8833)\n",
      "[Epoch: 12020] train loss: 0.5382, train acc: 0.7955, val loss: 0.4395, val acc: 0.8782  (best train acc: 0.8357, best val acc: 0.8833)\n",
      "[Epoch: 12040] train loss: 0.5148, train acc: 0.8002, val loss: 0.4243, val acc: 0.8742  (best train acc: 0.8357, best val acc: 0.8833)\n",
      "[Epoch: 12060] train loss: 0.5613, train acc: 0.7669, val loss: 0.4345, val acc: 0.8698  (best train acc: 0.8357, best val acc: 0.8833)\n",
      "[Epoch: 12080] train loss: 0.6016, train acc: 0.7744, val loss: 0.4295, val acc: 0.8715  (best train acc: 0.8357, best val acc: 0.8833)\n",
      "[Epoch: 12100] train loss: 0.4930, train acc: 0.8219, val loss: 0.4303, val acc: 0.8769  (best train acc: 0.8357, best val acc: 0.8833)\n",
      "[Epoch: 12120] train loss: 0.5310, train acc: 0.7914, val loss: 0.4313, val acc: 0.8752  (best train acc: 0.8357, best val acc: 0.8833)\n",
      "[Epoch: 12140] train loss: 0.5100, train acc: 0.7996, val loss: 0.4311, val acc: 0.8769  (best train acc: 0.8357, best val acc: 0.8833)\n",
      "[Epoch: 12160] train loss: 0.5901, train acc: 0.7710, val loss: 0.4348, val acc: 0.8685  (best train acc: 0.8357, best val acc: 0.8833)\n",
      "[Epoch: 12180] train loss: 0.6066, train acc: 0.7509, val loss: 0.4528, val acc: 0.8594  (best train acc: 0.8357, best val acc: 0.8833)\n",
      "[Epoch: 12200] train loss: 0.6332, train acc: 0.7513, val loss: 0.4315, val acc: 0.8752  (best train acc: 0.8357, best val acc: 0.8833)\n",
      "[Epoch: 12220] train loss: 0.5619, train acc: 0.7953, val loss: 0.4357, val acc: 0.8675  (best train acc: 0.8390, best val acc: 0.8833)\n",
      "[Epoch: 12240] train loss: 0.5529, train acc: 0.7971, val loss: 0.4464, val acc: 0.8658  (best train acc: 0.8390, best val acc: 0.8833)\n",
      "[Epoch: 12260] train loss: 0.5766, train acc: 0.7955, val loss: 0.4197, val acc: 0.8745  (best train acc: 0.8470, best val acc: 0.8833)\n",
      "[Epoch: 12280] train loss: 0.6473, train acc: 0.7516, val loss: 0.4493, val acc: 0.8614  (best train acc: 0.8470, best val acc: 0.8833)\n",
      "[Epoch: 12300] train loss: 0.6111, train acc: 0.7595, val loss: 0.4593, val acc: 0.8465  (best train acc: 0.8470, best val acc: 0.8833)\n",
      "[Epoch: 12320] train loss: 0.5693, train acc: 0.7713, val loss: 0.4223, val acc: 0.8725  (best train acc: 0.8470, best val acc: 0.8833)\n",
      "[Epoch: 12340] train loss: 0.5540, train acc: 0.7993, val loss: 0.4226, val acc: 0.8752  (best train acc: 0.8470, best val acc: 0.8833)\n",
      "[Epoch: 12360] train loss: 0.6002, train acc: 0.7642, val loss: 0.4210, val acc: 0.8755  (best train acc: 0.8470, best val acc: 0.8833)\n",
      "[Epoch: 12380] train loss: 0.4585, train acc: 0.8361, val loss: 0.4311, val acc: 0.8648  (best train acc: 0.8474, best val acc: 0.8833)\n",
      "[Epoch: 12400] train loss: 0.4885, train acc: 0.8329, val loss: 0.4264, val acc: 0.8749  (best train acc: 0.8474, best val acc: 0.8833)\n",
      "[Epoch: 12420] train loss: 0.6142, train acc: 0.7645, val loss: 0.4164, val acc: 0.8742  (best train acc: 0.8474, best val acc: 0.8833)\n",
      "[Epoch: 12440] train loss: 0.5024, train acc: 0.8198, val loss: 0.4304, val acc: 0.8742  (best train acc: 0.8474, best val acc: 0.8833)\n",
      "[Epoch: 12460] train loss: 0.5387, train acc: 0.7942, val loss: 0.4263, val acc: 0.8755  (best train acc: 0.8474, best val acc: 0.8833)\n",
      "[Epoch: 12480] train loss: 0.4781, train acc: 0.8374, val loss: 0.4334, val acc: 0.8722  (best train acc: 0.8474, best val acc: 0.8833)\n",
      "[Epoch: 12500] train loss: 0.4823, train acc: 0.8345, val loss: 0.4339, val acc: 0.8634  (best train acc: 0.8474, best val acc: 0.8833)\n",
      "[Epoch: 12520] train loss: 0.5549, train acc: 0.7874, val loss: 0.4233, val acc: 0.8772  (best train acc: 0.8474, best val acc: 0.8833)\n",
      "[Epoch: 12540] train loss: 0.4806, train acc: 0.8331, val loss: 0.4385, val acc: 0.8627  (best train acc: 0.8484, best val acc: 0.8833)\n",
      "[Epoch: 12560] train loss: 0.5371, train acc: 0.7997, val loss: 0.4154, val acc: 0.8742  (best train acc: 0.8484, best val acc: 0.8833)\n",
      "[Epoch: 12580] train loss: 0.4656, train acc: 0.8404, val loss: 0.4306, val acc: 0.8705  (best train acc: 0.8484, best val acc: 0.8833)\n",
      "[Epoch: 12600] train loss: 0.4807, train acc: 0.8340, val loss: 0.4324, val acc: 0.8755  (best train acc: 0.8484, best val acc: 0.8833)\n",
      "[Epoch: 12620] train loss: 0.5590, train acc: 0.7829, val loss: 0.4658, val acc: 0.8351  (best train acc: 0.8484, best val acc: 0.8847)\n",
      "[Epoch: 12640] train loss: 0.5091, train acc: 0.8198, val loss: 0.4177, val acc: 0.8809  (best train acc: 0.8484, best val acc: 0.8847)\n",
      "[Epoch: 12660] train loss: 0.4953, train acc: 0.8196, val loss: 0.4254, val acc: 0.8661  (best train acc: 0.8484, best val acc: 0.8847)\n",
      "[Epoch: 12680] train loss: 0.5783, train acc: 0.7856, val loss: 0.4126, val acc: 0.8745  (best train acc: 0.8484, best val acc: 0.8847)\n",
      "[Epoch: 12700] train loss: 0.4557, train acc: 0.8440, val loss: 0.4258, val acc: 0.8732  (best train acc: 0.8484, best val acc: 0.8853)\n",
      "[Epoch: 12720] train loss: 0.4786, train acc: 0.8334, val loss: 0.4246, val acc: 0.8671  (best train acc: 0.8484, best val acc: 0.8853)\n",
      "[Epoch: 12740] train loss: 0.4908, train acc: 0.8276, val loss: 0.4208, val acc: 0.8833  (best train acc: 0.8484, best val acc: 0.8863)\n",
      "[Epoch: 12760] train loss: 0.5777, train acc: 0.7780, val loss: 0.4180, val acc: 0.8836  (best train acc: 0.8529, best val acc: 0.8863)\n",
      "[Epoch: 12780] train loss: 0.4791, train acc: 0.8309, val loss: 0.4550, val acc: 0.8452  (best train acc: 0.8546, best val acc: 0.8863)\n",
      "[Epoch: 12800] train loss: 0.4918, train acc: 0.8281, val loss: 0.4150, val acc: 0.8789  (best train acc: 0.8546, best val acc: 0.8863)\n",
      "[Epoch: 12820] train loss: 0.5930, train acc: 0.7896, val loss: 0.4196, val acc: 0.8796  (best train acc: 0.8546, best val acc: 0.8863)\n",
      "[Epoch: 12840] train loss: 0.6561, train acc: 0.7363, val loss: 0.4064, val acc: 0.8840  (best train acc: 0.8546, best val acc: 0.8863)\n",
      "[Epoch: 12860] train loss: 0.5218, train acc: 0.8041, val loss: 0.4101, val acc: 0.8809  (best train acc: 0.8546, best val acc: 0.8863)\n",
      "[Epoch: 12880] train loss: 0.5175, train acc: 0.8024, val loss: 0.4241, val acc: 0.8715  (best train acc: 0.8550, best val acc: 0.8870)\n",
      "[Epoch: 12900] train loss: 0.5321, train acc: 0.7910, val loss: 0.4341, val acc: 0.8675  (best train acc: 0.8550, best val acc: 0.8870)\n",
      "[Epoch: 12920] train loss: 0.4950, train acc: 0.8241, val loss: 0.4052, val acc: 0.8766  (best train acc: 0.8576, best val acc: 0.8870)\n",
      "[Epoch: 12940] train loss: 0.5377, train acc: 0.8112, val loss: 0.4419, val acc: 0.8695  (best train acc: 0.8576, best val acc: 0.8870)\n",
      "[Epoch: 12960] train loss: 0.5884, train acc: 0.7897, val loss: 0.4215, val acc: 0.8762  (best train acc: 0.8576, best val acc: 0.8870)\n",
      "[Epoch: 12980] train loss: 0.6442, train acc: 0.7432, val loss: 0.4379, val acc: 0.8755  (best train acc: 0.8576, best val acc: 0.8897)\n",
      "[Epoch: 13000] train loss: 0.4799, train acc: 0.8263, val loss: 0.4060, val acc: 0.8826  (best train acc: 0.8576, best val acc: 0.8897)\n",
      "[Epoch: 13020] train loss: 0.4580, train acc: 0.8458, val loss: 0.4110, val acc: 0.8718  (best train acc: 0.8576, best val acc: 0.8897)\n",
      "[Epoch: 13040] train loss: 0.6018, train acc: 0.7702, val loss: 0.4058, val acc: 0.8813  (best train acc: 0.8576, best val acc: 0.8897)\n",
      "[Epoch: 13060] train loss: 0.4898, train acc: 0.8305, val loss: 0.4230, val acc: 0.8809  (best train acc: 0.8576, best val acc: 0.8897)\n",
      "[Epoch: 13080] train loss: 0.6280, train acc: 0.7522, val loss: 0.4075, val acc: 0.8779  (best train acc: 0.8576, best val acc: 0.8897)\n",
      "[Epoch: 13100] train loss: 0.4878, train acc: 0.8203, val loss: 0.4002, val acc: 0.8843  (best train acc: 0.8576, best val acc: 0.8897)\n",
      "[Epoch: 13120] train loss: 0.4987, train acc: 0.8141, val loss: 0.4006, val acc: 0.8809  (best train acc: 0.8576, best val acc: 0.8897)\n",
      "[Epoch: 13140] train loss: 0.4722, train acc: 0.8452, val loss: 0.4228, val acc: 0.8843  (best train acc: 0.8576, best val acc: 0.8897)\n",
      "[Epoch: 13160] train loss: 0.4890, train acc: 0.8396, val loss: 0.4113, val acc: 0.8836  (best train acc: 0.8601, best val acc: 0.8897)\n",
      "[Epoch: 13180] train loss: 0.4788, train acc: 0.8303, val loss: 0.4047, val acc: 0.8702  (best train acc: 0.8601, best val acc: 0.8897)\n",
      "[Epoch: 13200] train loss: 0.5554, train acc: 0.7804, val loss: 0.4028, val acc: 0.8830  (best train acc: 0.8601, best val acc: 0.8897)\n",
      "[Epoch: 13220] train loss: 0.5911, train acc: 0.7846, val loss: 0.4117, val acc: 0.8820  (best train acc: 0.8601, best val acc: 0.8897)\n",
      "[Epoch: 13240] train loss: 0.6136, train acc: 0.7576, val loss: 0.4015, val acc: 0.8742  (best train acc: 0.8601, best val acc: 0.8897)\n",
      "[Epoch: 13260] train loss: 0.5346, train acc: 0.8146, val loss: 0.4052, val acc: 0.8843  (best train acc: 0.8601, best val acc: 0.8897)\n",
      "[Epoch: 13280] train loss: 0.4803, train acc: 0.8291, val loss: 0.4035, val acc: 0.8749  (best train acc: 0.8601, best val acc: 0.8897)\n",
      "[Epoch: 13300] train loss: 0.4580, train acc: 0.8336, val loss: 0.4037, val acc: 0.8782  (best train acc: 0.8601, best val acc: 0.8897)\n",
      "[Epoch: 13320] train loss: 0.4958, train acc: 0.8156, val loss: 0.3911, val acc: 0.8809  (best train acc: 0.8601, best val acc: 0.8897)\n",
      "[Epoch: 13340] train loss: 0.4138, train acc: 0.8608, val loss: 0.4134, val acc: 0.8648  (best train acc: 0.8608, best val acc: 0.8897)\n",
      "[Epoch: 13360] train loss: 0.6904, train acc: 0.7142, val loss: 0.3887, val acc: 0.8836  (best train acc: 0.8608, best val acc: 0.8897)\n",
      "[Epoch: 13380] train loss: 0.4605, train acc: 0.8346, val loss: 0.3981, val acc: 0.8695  (best train acc: 0.8608, best val acc: 0.8897)\n",
      "[Epoch: 13400] train loss: 0.6388, train acc: 0.7526, val loss: 0.3842, val acc: 0.8853  (best train acc: 0.8608, best val acc: 0.8897)\n",
      "[Epoch: 13420] train loss: 0.5954, train acc: 0.7770, val loss: 0.3971, val acc: 0.8782  (best train acc: 0.8608, best val acc: 0.8897)\n",
      "[Epoch: 13440] train loss: 0.5462, train acc: 0.7911, val loss: 0.4138, val acc: 0.8789  (best train acc: 0.8608, best val acc: 0.8897)\n",
      "[Epoch: 13460] train loss: 0.4869, train acc: 0.8305, val loss: 0.3997, val acc: 0.8664  (best train acc: 0.8608, best val acc: 0.8897)\n",
      "[Epoch: 13480] train loss: 0.4628, train acc: 0.8445, val loss: 0.3905, val acc: 0.8782  (best train acc: 0.8608, best val acc: 0.8897)\n",
      "[Epoch: 13500] train loss: 0.4982, train acc: 0.8258, val loss: 0.4014, val acc: 0.8836  (best train acc: 0.8608, best val acc: 0.8897)\n",
      "[Epoch: 13520] train loss: 0.4654, train acc: 0.8331, val loss: 0.4054, val acc: 0.8826  (best train acc: 0.8608, best val acc: 0.8897)\n",
      "[Epoch: 13540] train loss: 0.4946, train acc: 0.8088, val loss: 0.4140, val acc: 0.8702  (best train acc: 0.8608, best val acc: 0.8897)\n",
      "[Epoch: 13560] train loss: 0.4745, train acc: 0.8266, val loss: 0.3886, val acc: 0.8826  (best train acc: 0.8608, best val acc: 0.8897)\n",
      "[Epoch: 13580] train loss: 0.4078, train acc: 0.8620, val loss: 0.3930, val acc: 0.8782  (best train acc: 0.8620, best val acc: 0.8897)\n",
      "[Epoch: 13600] train loss: 0.4527, train acc: 0.8377, val loss: 0.4005, val acc: 0.8823  (best train acc: 0.8620, best val acc: 0.8897)\n",
      "[Epoch: 13620] train loss: 0.4701, train acc: 0.8223, val loss: 0.3940, val acc: 0.8826  (best train acc: 0.8620, best val acc: 0.8897)\n",
      "[Epoch: 13640] train loss: 0.4747, train acc: 0.8347, val loss: 0.4002, val acc: 0.8813  (best train acc: 0.8658, best val acc: 0.8897)\n",
      "[Epoch: 13660] train loss: 0.5514, train acc: 0.7854, val loss: 0.4135, val acc: 0.8820  (best train acc: 0.8658, best val acc: 0.8897)\n",
      "[Epoch: 13680] train loss: 0.4654, train acc: 0.8399, val loss: 0.4069, val acc: 0.8739  (best train acc: 0.8658, best val acc: 0.8897)\n",
      "[Epoch: 13700] train loss: 0.5086, train acc: 0.8234, val loss: 0.3901, val acc: 0.8840  (best train acc: 0.8658, best val acc: 0.8897)\n",
      "[Epoch: 13720] train loss: 0.5477, train acc: 0.7987, val loss: 0.3845, val acc: 0.8833  (best train acc: 0.8658, best val acc: 0.8897)\n",
      "[Epoch: 13740] train loss: 0.4847, train acc: 0.8269, val loss: 0.3995, val acc: 0.8766  (best train acc: 0.8658, best val acc: 0.8897)\n",
      "[Epoch: 13760] train loss: 0.4804, train acc: 0.8263, val loss: 0.4005, val acc: 0.8708  (best train acc: 0.8658, best val acc: 0.8897)\n",
      "[Epoch: 13780] train loss: 0.4191, train acc: 0.8630, val loss: 0.3904, val acc: 0.8843  (best train acc: 0.8658, best val acc: 0.8897)\n",
      "[Epoch: 13800] train loss: 0.4691, train acc: 0.8346, val loss: 0.3862, val acc: 0.8782  (best train acc: 0.8658, best val acc: 0.8897)\n",
      "[Epoch: 13820] train loss: 0.4337, train acc: 0.8534, val loss: 0.4129, val acc: 0.8560  (best train acc: 0.8658, best val acc: 0.8897)\n",
      "[Epoch: 13840] train loss: 0.5654, train acc: 0.7929, val loss: 0.4008, val acc: 0.8830  (best train acc: 0.8658, best val acc: 0.8897)\n",
      "[Epoch: 13860] train loss: 0.4647, train acc: 0.8341, val loss: 0.3979, val acc: 0.8806  (best train acc: 0.8658, best val acc: 0.8897)\n",
      "[Epoch: 13880] train loss: 0.4477, train acc: 0.8319, val loss: 0.3804, val acc: 0.8803  (best train acc: 0.8658, best val acc: 0.8897)\n",
      "[Epoch: 13900] train loss: 0.5556, train acc: 0.7900, val loss: 0.4029, val acc: 0.8658  (best train acc: 0.8658, best val acc: 0.8897)\n",
      "[Epoch: 13920] train loss: 0.4210, train acc: 0.8548, val loss: 0.4143, val acc: 0.8570  (best train acc: 0.8658, best val acc: 0.8897)\n",
      "[Epoch: 13940] train loss: 0.4473, train acc: 0.8445, val loss: 0.3960, val acc: 0.8739  (best train acc: 0.8658, best val acc: 0.8897)\n",
      "[Epoch: 13960] train loss: 0.4232, train acc: 0.8646, val loss: 0.3856, val acc: 0.8799  (best train acc: 0.8658, best val acc: 0.8897)\n",
      "[Epoch: 13980] train loss: 0.4974, train acc: 0.8177, val loss: 0.3897, val acc: 0.8860  (best train acc: 0.8658, best val acc: 0.8897)\n",
      "[Epoch: 14000] train loss: 0.4551, train acc: 0.8450, val loss: 0.3887, val acc: 0.8813  (best train acc: 0.8658, best val acc: 0.8897)\n",
      "[Epoch: 14020] train loss: 0.5365, train acc: 0.7906, val loss: 0.3862, val acc: 0.8840  (best train acc: 0.8658, best val acc: 0.8897)\n",
      "[Epoch: 14040] train loss: 0.5496, train acc: 0.7890, val loss: 0.3875, val acc: 0.8850  (best train acc: 0.8658, best val acc: 0.8897)\n",
      "[Epoch: 14060] train loss: 0.4223, train acc: 0.8447, val loss: 0.3938, val acc: 0.8755  (best train acc: 0.8677, best val acc: 0.8897)\n",
      "[Epoch: 14080] train loss: 0.4644, train acc: 0.8438, val loss: 0.3928, val acc: 0.8813  (best train acc: 0.8677, best val acc: 0.8921)\n",
      "[Epoch: 14100] train loss: 0.5032, train acc: 0.8089, val loss: 0.4213, val acc: 0.8779  (best train acc: 0.8677, best val acc: 0.8921)\n",
      "[Epoch: 14120] train loss: 0.4324, train acc: 0.8464, val loss: 0.3728, val acc: 0.8880  (best train acc: 0.8677, best val acc: 0.8921)\n",
      "[Epoch: 14140] train loss: 0.4358, train acc: 0.8428, val loss: 0.3893, val acc: 0.8772  (best train acc: 0.8677, best val acc: 0.8921)\n",
      "[Epoch: 14160] train loss: 0.5202, train acc: 0.7973, val loss: 0.3867, val acc: 0.8904  (best train acc: 0.8677, best val acc: 0.8921)\n",
      "[Epoch: 14180] train loss: 0.5517, train acc: 0.7846, val loss: 0.3824, val acc: 0.8894  (best train acc: 0.8677, best val acc: 0.8921)\n",
      "[Epoch: 14200] train loss: 0.5394, train acc: 0.7804, val loss: 0.3772, val acc: 0.8911  (best train acc: 0.8677, best val acc: 0.8921)\n",
      "[Epoch: 14220] train loss: 0.5743, train acc: 0.7866, val loss: 0.3846, val acc: 0.8887  (best train acc: 0.8677, best val acc: 0.8921)\n",
      "[Epoch: 14240] train loss: 0.4599, train acc: 0.8284, val loss: 0.3871, val acc: 0.8820  (best train acc: 0.8677, best val acc: 0.8921)\n",
      "[Epoch: 14260] train loss: 0.4277, train acc: 0.8550, val loss: 0.3957, val acc: 0.8749  (best train acc: 0.8677, best val acc: 0.8921)\n",
      "[Epoch: 14280] train loss: 0.5291, train acc: 0.8050, val loss: 0.3803, val acc: 0.8853  (best train acc: 0.8677, best val acc: 0.8921)\n",
      "[Epoch: 14300] train loss: 0.4397, train acc: 0.8388, val loss: 0.3742, val acc: 0.8894  (best train acc: 0.8677, best val acc: 0.8921)\n",
      "[Epoch: 14320] train loss: 0.5073, train acc: 0.8151, val loss: 0.3712, val acc: 0.8823  (best train acc: 0.8677, best val acc: 0.8921)\n",
      "[Epoch: 14340] train loss: 0.4119, train acc: 0.8569, val loss: 0.3902, val acc: 0.8769  (best train acc: 0.8677, best val acc: 0.8921)\n",
      "[Epoch: 14360] train loss: 0.6322, train acc: 0.7572, val loss: 0.3796, val acc: 0.8867  (best train acc: 0.8677, best val acc: 0.8921)\n",
      "[Epoch: 14380] train loss: 0.5445, train acc: 0.7979, val loss: 0.3892, val acc: 0.8847  (best train acc: 0.8681, best val acc: 0.8921)\n",
      "[Epoch: 14400] train loss: 0.4609, train acc: 0.8342, val loss: 0.3875, val acc: 0.8813  (best train acc: 0.8681, best val acc: 0.8921)\n",
      "[Epoch: 14420] train loss: 0.6700, train acc: 0.7210, val loss: 0.3793, val acc: 0.8863  (best train acc: 0.8681, best val acc: 0.8921)\n",
      "[Epoch: 14440] train loss: 0.4503, train acc: 0.8323, val loss: 0.3866, val acc: 0.8749  (best train acc: 0.8681, best val acc: 0.8921)\n",
      "[Epoch: 14460] train loss: 0.5339, train acc: 0.8075, val loss: 0.3810, val acc: 0.8782  (best train acc: 0.8681, best val acc: 0.8921)\n",
      "[Epoch: 14480] train loss: 0.5493, train acc: 0.8010, val loss: 0.3735, val acc: 0.8874  (best train acc: 0.8730, best val acc: 0.8924)\n",
      "[Epoch: 14500] train loss: 0.4609, train acc: 0.8261, val loss: 0.3807, val acc: 0.8840  (best train acc: 0.8730, best val acc: 0.8924)\n",
      "[Epoch: 14520] train loss: 0.4197, train acc: 0.8489, val loss: 0.4181, val acc: 0.8695  (best train acc: 0.8730, best val acc: 0.8927)\n",
      "[Epoch: 14540] train loss: 0.5573, train acc: 0.7943, val loss: 0.3706, val acc: 0.8884  (best train acc: 0.8730, best val acc: 0.8927)\n",
      "[Epoch: 14560] train loss: 0.6427, train acc: 0.7331, val loss: 0.3853, val acc: 0.8870  (best train acc: 0.8730, best val acc: 0.8927)\n",
      "[Epoch: 14580] train loss: 0.4182, train acc: 0.8469, val loss: 0.3735, val acc: 0.8911  (best train acc: 0.8730, best val acc: 0.8934)\n",
      "[Epoch: 14600] train loss: 0.6537, train acc: 0.7538, val loss: 0.3812, val acc: 0.8897  (best train acc: 0.8730, best val acc: 0.8938)\n",
      "[Epoch: 14620] train loss: 0.5254, train acc: 0.7949, val loss: 0.3840, val acc: 0.8901  (best train acc: 0.8730, best val acc: 0.8938)\n",
      "[Epoch: 14640] train loss: 0.6343, train acc: 0.7499, val loss: 0.3799, val acc: 0.8853  (best train acc: 0.8730, best val acc: 0.8938)\n",
      "[Epoch: 14660] train loss: 0.4685, train acc: 0.8287, val loss: 0.3805, val acc: 0.8901  (best train acc: 0.8730, best val acc: 0.8938)\n",
      "[Epoch: 14680] train loss: 0.4212, train acc: 0.8496, val loss: 0.3919, val acc: 0.8840  (best train acc: 0.8730, best val acc: 0.8951)\n",
      "[Epoch: 14700] train loss: 0.4663, train acc: 0.8309, val loss: 0.3848, val acc: 0.8752  (best train acc: 0.8730, best val acc: 0.8965)\n",
      "[Epoch: 14720] train loss: 0.3902, train acc: 0.8697, val loss: 0.3778, val acc: 0.8836  (best train acc: 0.8730, best val acc: 0.8965)\n",
      "[Epoch: 14740] train loss: 0.4725, train acc: 0.8193, val loss: 0.3648, val acc: 0.8901  (best train acc: 0.8730, best val acc: 0.8968)\n",
      "[Epoch: 14760] train loss: 0.4839, train acc: 0.8239, val loss: 0.3639, val acc: 0.8887  (best train acc: 0.8730, best val acc: 0.8968)\n",
      "[Epoch: 14780] train loss: 0.4194, train acc: 0.8431, val loss: 0.3693, val acc: 0.8820  (best train acc: 0.8730, best val acc: 0.8968)\n",
      "[Epoch: 14800] train loss: 0.4041, train acc: 0.8510, val loss: 0.3704, val acc: 0.8820  (best train acc: 0.8730, best val acc: 0.8968)\n",
      "[Epoch: 14820] train loss: 0.4359, train acc: 0.8515, val loss: 0.3569, val acc: 0.8894  (best train acc: 0.8733, best val acc: 0.8968)\n",
      "[Epoch: 14840] train loss: 0.5564, train acc: 0.7974, val loss: 0.3667, val acc: 0.8934  (best train acc: 0.8733, best val acc: 0.8968)\n",
      "[Epoch: 14860] train loss: 0.4393, train acc: 0.8400, val loss: 0.3742, val acc: 0.8836  (best train acc: 0.8733, best val acc: 0.8968)\n",
      "[Epoch: 14880] train loss: 0.5363, train acc: 0.7896, val loss: 0.3661, val acc: 0.8874  (best train acc: 0.8733, best val acc: 0.8981)\n",
      "[Epoch: 14900] train loss: 0.4292, train acc: 0.8452, val loss: 0.3675, val acc: 0.8934  (best train acc: 0.8733, best val acc: 0.8988)\n",
      "[Epoch: 14920] train loss: 0.4240, train acc: 0.8466, val loss: 0.3814, val acc: 0.8755  (best train acc: 0.8733, best val acc: 0.8988)\n",
      "[Epoch: 14940] train loss: 0.4769, train acc: 0.8266, val loss: 0.3717, val acc: 0.8823  (best train acc: 0.8733, best val acc: 0.8995)\n",
      "[Epoch: 14960] train loss: 0.5404, train acc: 0.7941, val loss: 0.3561, val acc: 0.8927  (best train acc: 0.8733, best val acc: 0.8995)\n",
      "[Epoch: 14980] train loss: 0.4418, train acc: 0.8284, val loss: 0.3642, val acc: 0.8860  (best train acc: 0.8733, best val acc: 0.8995)\n",
      "[Epoch: 15000] train loss: 0.4088, train acc: 0.8632, val loss: 0.3566, val acc: 0.9002  (best train acc: 0.8733, best val acc: 0.9002)\n",
      "[Epoch: 15020] train loss: 0.5653, train acc: 0.7832, val loss: 0.3541, val acc: 0.8958  (best train acc: 0.8757, best val acc: 0.9002)\n",
      "[Epoch: 15040] train loss: 0.4049, train acc: 0.8477, val loss: 0.3563, val acc: 0.8867  (best train acc: 0.8757, best val acc: 0.9002)\n",
      "[Epoch: 15060] train loss: 0.4207, train acc: 0.8509, val loss: 0.3548, val acc: 0.8985  (best train acc: 0.8757, best val acc: 0.9002)\n",
      "[Epoch: 15080] train loss: 0.3899, train acc: 0.8624, val loss: 0.3575, val acc: 0.8995  (best train acc: 0.8757, best val acc: 0.9008)\n",
      "[Epoch: 15100] train loss: 0.5736, train acc: 0.7816, val loss: 0.3680, val acc: 0.8934  (best train acc: 0.8757, best val acc: 0.9008)\n",
      "[Epoch: 15120] train loss: 0.4612, train acc: 0.8241, val loss: 0.3745, val acc: 0.8843  (best train acc: 0.8757, best val acc: 0.9008)\n",
      "[Epoch: 15140] train loss: 0.5127, train acc: 0.8040, val loss: 0.3660, val acc: 0.8968  (best train acc: 0.8757, best val acc: 0.9015)\n",
      "[Epoch: 15160] train loss: 0.4128, train acc: 0.8527, val loss: 0.3529, val acc: 0.8941  (best train acc: 0.8757, best val acc: 0.9015)\n",
      "[Epoch: 15180] train loss: 0.3990, train acc: 0.8624, val loss: 0.3558, val acc: 0.8961  (best train acc: 0.8757, best val acc: 0.9015)\n",
      "[Epoch: 15200] train loss: 0.5220, train acc: 0.7971, val loss: 0.3461, val acc: 0.8924  (best train acc: 0.8757, best val acc: 0.9015)\n",
      "[Epoch: 15220] train loss: 0.4020, train acc: 0.8603, val loss: 0.3881, val acc: 0.8782  (best train acc: 0.8757, best val acc: 0.9019)\n",
      "[Epoch: 15240] train loss: 0.4294, train acc: 0.8415, val loss: 0.3524, val acc: 0.8944  (best train acc: 0.8757, best val acc: 0.9019)\n",
      "[Epoch: 15260] train loss: 0.4729, train acc: 0.8321, val loss: 0.3537, val acc: 0.8917  (best train acc: 0.8757, best val acc: 0.9019)\n",
      "[Epoch: 15280] train loss: 0.4000, train acc: 0.8599, val loss: 0.3439, val acc: 0.8981  (best train acc: 0.8757, best val acc: 0.9019)\n",
      "[Epoch: 15300] train loss: 0.3862, train acc: 0.8712, val loss: 0.3557, val acc: 0.8887  (best train acc: 0.8757, best val acc: 0.9019)\n",
      "[Epoch: 15320] train loss: 0.4466, train acc: 0.8343, val loss: 0.3459, val acc: 0.8938  (best train acc: 0.8757, best val acc: 0.9019)\n",
      "[Epoch: 15340] train loss: 0.4911, train acc: 0.8078, val loss: 0.3690, val acc: 0.8803  (best train acc: 0.8757, best val acc: 0.9019)\n",
      "[Epoch: 15360] train loss: 0.3896, train acc: 0.8762, val loss: 0.3678, val acc: 0.8830  (best train acc: 0.8762, best val acc: 0.9019)\n",
      "[Epoch: 15380] train loss: 0.4119, train acc: 0.8512, val loss: 0.3467, val acc: 0.8948  (best train acc: 0.8762, best val acc: 0.9022)\n",
      "[Epoch: 15400] train loss: 0.5252, train acc: 0.8021, val loss: 0.3479, val acc: 0.8961  (best train acc: 0.8762, best val acc: 0.9022)\n",
      "[Epoch: 15420] train loss: 0.5187, train acc: 0.8015, val loss: 0.3595, val acc: 0.8961  (best train acc: 0.8762, best val acc: 0.9022)\n",
      "[Epoch: 15440] train loss: 0.4131, train acc: 0.8648, val loss: 0.3637, val acc: 0.8816  (best train acc: 0.8767, best val acc: 0.9022)\n",
      "[Epoch: 15460] train loss: 0.4523, train acc: 0.8415, val loss: 0.3704, val acc: 0.8860  (best train acc: 0.8767, best val acc: 0.9022)\n",
      "[Epoch: 15480] train loss: 0.6312, train acc: 0.7478, val loss: 0.3621, val acc: 0.8894  (best train acc: 0.8770, best val acc: 0.9022)\n",
      "[Epoch: 15500] train loss: 0.5001, train acc: 0.8057, val loss: 0.3554, val acc: 0.8897  (best train acc: 0.8774, best val acc: 0.9022)\n",
      "[Epoch: 15520] train loss: 0.3737, train acc: 0.8720, val loss: 0.3516, val acc: 0.8938  (best train acc: 0.8774, best val acc: 0.9022)\n",
      "[Epoch: 15540] train loss: 0.5241, train acc: 0.8063, val loss: 0.3592, val acc: 0.8948  (best train acc: 0.8774, best val acc: 0.9022)\n",
      "[Epoch: 15560] train loss: 0.4035, train acc: 0.8661, val loss: 0.3594, val acc: 0.8833  (best train acc: 0.8784, best val acc: 0.9029)\n",
      "[Epoch: 15580] train loss: 0.3778, train acc: 0.8719, val loss: 0.3402, val acc: 0.8978  (best train acc: 0.8784, best val acc: 0.9029)\n",
      "[Epoch: 15600] train loss: 0.4406, train acc: 0.8446, val loss: 0.3533, val acc: 0.8992  (best train acc: 0.8784, best val acc: 0.9029)\n",
      "[Epoch: 15620] train loss: 0.5181, train acc: 0.8008, val loss: 0.3536, val acc: 0.8948  (best train acc: 0.8784, best val acc: 0.9029)\n",
      "[Epoch: 15640] train loss: 0.4154, train acc: 0.8616, val loss: 0.3537, val acc: 0.8961  (best train acc: 0.8784, best val acc: 0.9029)\n",
      "[Epoch: 15660] train loss: 0.4540, train acc: 0.8399, val loss: 0.3525, val acc: 0.8975  (best train acc: 0.8784, best val acc: 0.9029)\n",
      "[Epoch: 15680] train loss: 0.5919, train acc: 0.7651, val loss: 0.3476, val acc: 0.8988  (best train acc: 0.8784, best val acc: 0.9029)\n",
      "[Epoch: 15700] train loss: 0.4424, train acc: 0.8352, val loss: 0.3406, val acc: 0.8975  (best train acc: 0.8784, best val acc: 0.9029)\n",
      "[Epoch: 15720] train loss: 0.4552, train acc: 0.8277, val loss: 0.3756, val acc: 0.8796  (best train acc: 0.8784, best val acc: 0.9049)\n",
      "[Epoch: 15740] train loss: 0.4575, train acc: 0.8353, val loss: 0.3695, val acc: 0.8766  (best train acc: 0.8784, best val acc: 0.9062)\n",
      "[Epoch: 15760] train loss: 0.4941, train acc: 0.8179, val loss: 0.3469, val acc: 0.8971  (best train acc: 0.8784, best val acc: 0.9062)\n",
      "[Epoch: 15780] train loss: 0.4287, train acc: 0.8462, val loss: 0.3490, val acc: 0.8995  (best train acc: 0.8784, best val acc: 0.9062)\n",
      "[Epoch: 15800] train loss: 0.4170, train acc: 0.8552, val loss: 0.3411, val acc: 0.9002  (best train acc: 0.8822, best val acc: 0.9062)\n",
      "[Epoch: 15820] train loss: 0.5710, train acc: 0.7761, val loss: 0.3387, val acc: 0.9008  (best train acc: 0.8822, best val acc: 0.9062)\n",
      "[Epoch: 15840] train loss: 0.5362, train acc: 0.8073, val loss: 0.3603, val acc: 0.8948  (best train acc: 0.8822, best val acc: 0.9062)\n",
      "[Epoch: 15860] train loss: 0.5546, train acc: 0.7771, val loss: 0.3607, val acc: 0.8981  (best train acc: 0.8822, best val acc: 0.9062)\n",
      "[Epoch: 15880] train loss: 0.3677, train acc: 0.8806, val loss: 0.3537, val acc: 0.8924  (best train acc: 0.8822, best val acc: 0.9062)\n",
      "[Epoch: 15900] train loss: 0.5549, train acc: 0.7823, val loss: 0.3474, val acc: 0.9019  (best train acc: 0.8822, best val acc: 0.9062)\n",
      "[Epoch: 15920] train loss: 0.4470, train acc: 0.8412, val loss: 0.3496, val acc: 0.8998  (best train acc: 0.8822, best val acc: 0.9062)\n",
      "[Epoch: 15940] train loss: 0.4402, train acc: 0.8405, val loss: 0.3574, val acc: 0.8847  (best train acc: 0.8822, best val acc: 0.9062)\n",
      "[Epoch: 15960] train loss: 0.5770, train acc: 0.7879, val loss: 0.3501, val acc: 0.9012  (best train acc: 0.8822, best val acc: 0.9062)\n",
      "[Epoch: 15980] train loss: 0.4234, train acc: 0.8422, val loss: 0.3626, val acc: 0.8907  (best train acc: 0.8822, best val acc: 0.9062)\n",
      "[Epoch: 16000] train loss: 0.6359, train acc: 0.7494, val loss: 0.3525, val acc: 0.8927  (best train acc: 0.8822, best val acc: 0.9062)\n",
      "[Epoch: 16020] train loss: 0.4040, train acc: 0.8641, val loss: 0.3737, val acc: 0.8779  (best train acc: 0.8822, best val acc: 0.9062)\n",
      "[Epoch: 16040] train loss: 0.4052, train acc: 0.8556, val loss: 0.3557, val acc: 0.8897  (best train acc: 0.8822, best val acc: 0.9062)\n",
      "[Epoch: 16060] train loss: 0.3964, train acc: 0.8540, val loss: 0.3512, val acc: 0.8890  (best train acc: 0.8822, best val acc: 0.9062)\n",
      "[Epoch: 16080] train loss: 0.4726, train acc: 0.8189, val loss: 0.3574, val acc: 0.8874  (best train acc: 0.8822, best val acc: 0.9066)\n",
      "[Epoch: 16100] train loss: 0.4572, train acc: 0.8357, val loss: 0.3466, val acc: 0.8951  (best train acc: 0.8822, best val acc: 0.9066)\n",
      "[Epoch: 16120] train loss: 0.5235, train acc: 0.8028, val loss: 0.3445, val acc: 0.8992  (best train acc: 0.8822, best val acc: 0.9066)\n",
      "[Epoch: 16140] train loss: 0.3872, train acc: 0.8697, val loss: 0.3594, val acc: 0.8887  (best train acc: 0.8822, best val acc: 0.9066)\n",
      "[Epoch: 16160] train loss: 0.4235, train acc: 0.8560, val loss: 0.3463, val acc: 0.8951  (best train acc: 0.8822, best val acc: 0.9066)\n",
      "[Epoch: 16180] train loss: 0.4893, train acc: 0.8288, val loss: 0.3555, val acc: 0.8907  (best train acc: 0.8822, best val acc: 0.9066)\n",
      "[Epoch: 16200] train loss: 0.4591, train acc: 0.8295, val loss: 0.3948, val acc: 0.8604  (best train acc: 0.8822, best val acc: 0.9093)\n",
      "[Epoch: 16220] train loss: 0.5874, train acc: 0.7596, val loss: 0.3432, val acc: 0.8978  (best train acc: 0.8822, best val acc: 0.9093)\n",
      "[Epoch: 16240] train loss: 0.4722, train acc: 0.8240, val loss: 0.3497, val acc: 0.8978  (best train acc: 0.8822, best val acc: 0.9093)\n",
      "[Epoch: 16260] train loss: 0.5106, train acc: 0.8040, val loss: 0.3384, val acc: 0.8998  (best train acc: 0.8822, best val acc: 0.9093)\n",
      "[Epoch: 16280] train loss: 0.3925, train acc: 0.8678, val loss: 0.3588, val acc: 0.8857  (best train acc: 0.8822, best val acc: 0.9093)\n",
      "[Epoch: 16300] train loss: 0.4093, train acc: 0.8636, val loss: 0.3497, val acc: 0.8924  (best train acc: 0.8822, best val acc: 0.9093)\n",
      "[Epoch: 16320] train loss: 0.4211, train acc: 0.8486, val loss: 0.3284, val acc: 0.9012  (best train acc: 0.8822, best val acc: 0.9093)\n",
      "[Epoch: 16340] train loss: 0.4399, train acc: 0.8351, val loss: 0.3463, val acc: 0.8992  (best train acc: 0.8822, best val acc: 0.9093)\n",
      "[Epoch: 16360] train loss: 0.4088, train acc: 0.8508, val loss: 0.3393, val acc: 0.8975  (best train acc: 0.8822, best val acc: 0.9093)\n",
      "[Epoch: 16380] train loss: 0.3688, train acc: 0.8703, val loss: 0.3476, val acc: 0.8880  (best train acc: 0.8822, best val acc: 0.9093)\n",
      "[Epoch: 16400] train loss: 0.5390, train acc: 0.7833, val loss: 0.3524, val acc: 0.8877  (best train acc: 0.8822, best val acc: 0.9093)\n",
      "[Epoch: 16420] train loss: 0.5466, train acc: 0.7995, val loss: 0.3390, val acc: 0.8944  (best train acc: 0.8822, best val acc: 0.9093)\n",
      "[Epoch: 16440] train loss: 0.4061, train acc: 0.8526, val loss: 0.3357, val acc: 0.9032  (best train acc: 0.8822, best val acc: 0.9093)\n",
      "[Epoch: 16460] train loss: 0.4819, train acc: 0.8305, val loss: 0.3262, val acc: 0.8968  (best train acc: 0.8822, best val acc: 0.9093)\n",
      "[Epoch: 16480] train loss: 0.5359, train acc: 0.7917, val loss: 0.3433, val acc: 0.8951  (best train acc: 0.8822, best val acc: 0.9093)\n",
      "[Epoch: 16500] train loss: 0.4145, train acc: 0.8465, val loss: 0.3232, val acc: 0.8934  (best train acc: 0.8822, best val acc: 0.9093)\n",
      "[Epoch: 16520] train loss: 0.4814, train acc: 0.8159, val loss: 0.3162, val acc: 0.9096  (best train acc: 0.8822, best val acc: 0.9096)\n",
      "[Epoch: 16540] train loss: 0.4198, train acc: 0.8447, val loss: 0.3171, val acc: 0.8978  (best train acc: 0.8822, best val acc: 0.9096)\n",
      "[Epoch: 16560] train loss: 0.4795, train acc: 0.8078, val loss: 0.3029, val acc: 0.9076  (best train acc: 0.8822, best val acc: 0.9116)\n",
      "[Epoch: 16580] train loss: 0.4185, train acc: 0.8495, val loss: 0.3169, val acc: 0.9005  (best train acc: 0.8822, best val acc: 0.9116)\n",
      "[Epoch: 16600] train loss: 0.4040, train acc: 0.8496, val loss: 0.3110, val acc: 0.9042  (best train acc: 0.8822, best val acc: 0.9116)\n",
      "[Epoch: 16620] train loss: 0.3577, train acc: 0.8752, val loss: 0.3276, val acc: 0.8816  (best train acc: 0.8822, best val acc: 0.9116)\n",
      "[Epoch: 16640] train loss: 0.4223, train acc: 0.8403, val loss: 0.3246, val acc: 0.8745  (best train acc: 0.8822, best val acc: 0.9116)\n",
      "[Epoch: 16660] train loss: 0.3301, train acc: 0.8838, val loss: 0.3017, val acc: 0.9076  (best train acc: 0.8838, best val acc: 0.9177)\n",
      "[Epoch: 16680] train loss: 0.3911, train acc: 0.8516, val loss: 0.2937, val acc: 0.9089  (best train acc: 0.8838, best val acc: 0.9177)\n",
      "[Epoch: 16700] train loss: 0.4377, train acc: 0.8289, val loss: 0.2947, val acc: 0.9083  (best train acc: 0.8838, best val acc: 0.9177)\n",
      "[Epoch: 16720] train loss: 0.3485, train acc: 0.8772, val loss: 0.2932, val acc: 0.9096  (best train acc: 0.8838, best val acc: 0.9177)\n",
      "[Epoch: 16740] train loss: 0.3531, train acc: 0.8760, val loss: 0.2996, val acc: 0.9110  (best train acc: 0.8838, best val acc: 0.9177)\n",
      "[Epoch: 16760] train loss: 0.3293, train acc: 0.8801, val loss: 0.3005, val acc: 0.9002  (best train acc: 0.8838, best val acc: 0.9177)\n",
      "[Epoch: 16780] train loss: 0.4136, train acc: 0.8431, val loss: 0.2922, val acc: 0.9167  (best train acc: 0.8838, best val acc: 0.9187)\n",
      "[Epoch: 16800] train loss: 0.4081, train acc: 0.8501, val loss: 0.2821, val acc: 0.9150  (best train acc: 0.8868, best val acc: 0.9187)\n",
      "[Epoch: 16820] train loss: 0.5119, train acc: 0.8023, val loss: 0.2995, val acc: 0.9123  (best train acc: 0.8868, best val acc: 0.9187)\n",
      "[Epoch: 16840] train loss: 0.3803, train acc: 0.8660, val loss: 0.2988, val acc: 0.9187  (best train acc: 0.8868, best val acc: 0.9187)\n",
      "[Epoch: 16860] train loss: 0.3720, train acc: 0.8630, val loss: 0.2829, val acc: 0.9137  (best train acc: 0.8868, best val acc: 0.9191)\n",
      "[Epoch: 16880] train loss: 0.3334, train acc: 0.8788, val loss: 0.2924, val acc: 0.9035  (best train acc: 0.8868, best val acc: 0.9218)\n",
      "[Epoch: 16900] train loss: 0.3921, train acc: 0.8502, val loss: 0.3023, val acc: 0.8890  (best train acc: 0.8880, best val acc: 0.9218)\n",
      "[Epoch: 16920] train loss: 0.3770, train acc: 0.8616, val loss: 0.2921, val acc: 0.9197  (best train acc: 0.8880, best val acc: 0.9218)\n",
      "[Epoch: 16940] train loss: 0.4769, train acc: 0.8182, val loss: 0.2950, val acc: 0.9204  (best train acc: 0.8880, best val acc: 0.9218)\n",
      "[Epoch: 16960] train loss: 0.3632, train acc: 0.8687, val loss: 0.2839, val acc: 0.9245  (best train acc: 0.8880, best val acc: 0.9251)\n",
      "[Epoch: 16980] train loss: 0.3475, train acc: 0.8762, val loss: 0.2875, val acc: 0.9201  (best train acc: 0.8880, best val acc: 0.9261)\n",
      "[Epoch: 17000] train loss: 0.3528, train acc: 0.8722, val loss: 0.2898, val acc: 0.9177  (best train acc: 0.8880, best val acc: 0.9261)\n",
      "[Epoch: 17020] train loss: 0.3374, train acc: 0.8771, val loss: 0.2868, val acc: 0.9059  (best train acc: 0.8880, best val acc: 0.9261)\n",
      "[Epoch: 17040] train loss: 0.4193, train acc: 0.8477, val loss: 0.2797, val acc: 0.9106  (best train acc: 0.8880, best val acc: 0.9261)\n",
      "[Epoch: 17060] train loss: 0.4208, train acc: 0.8433, val loss: 0.2864, val acc: 0.9228  (best train acc: 0.8880, best val acc: 0.9268)\n",
      "[Epoch: 17080] train loss: 0.3923, train acc: 0.8516, val loss: 0.2808, val acc: 0.9258  (best train acc: 0.8880, best val acc: 0.9275)\n",
      "[Epoch: 17100] train loss: 0.3625, train acc: 0.8672, val loss: 0.2713, val acc: 0.9261  (best train acc: 0.8887, best val acc: 0.9288)\n",
      "[Epoch: 17120] train loss: 0.3893, train acc: 0.8509, val loss: 0.2756, val acc: 0.9123  (best train acc: 0.8887, best val acc: 0.9288)\n",
      "[Epoch: 17140] train loss: 0.3540, train acc: 0.8691, val loss: 0.2737, val acc: 0.9258  (best train acc: 0.8887, best val acc: 0.9322)\n",
      "[Epoch: 17160] train loss: 0.3072, train acc: 0.8903, val loss: 0.2643, val acc: 0.9255  (best train acc: 0.8903, best val acc: 0.9325)\n",
      "[Epoch: 17180] train loss: 0.3740, train acc: 0.8663, val loss: 0.2536, val acc: 0.9268  (best train acc: 0.8939, best val acc: 0.9342)\n",
      "[Epoch: 17200] train loss: 0.3825, train acc: 0.8511, val loss: 0.2519, val acc: 0.9346  (best train acc: 0.8954, best val acc: 0.9376)\n",
      "[Epoch: 17220] train loss: 0.3533, train acc: 0.8632, val loss: 0.2561, val acc: 0.9379  (best train acc: 0.8954, best val acc: 0.9430)\n",
      "[Epoch: 17240] train loss: 0.3245, train acc: 0.8897, val loss: 0.2350, val acc: 0.9369  (best train acc: 0.8954, best val acc: 0.9430)\n",
      "[Epoch: 17260] train loss: 0.3310, train acc: 0.8817, val loss: 0.2543, val acc: 0.9376  (best train acc: 0.8954, best val acc: 0.9430)\n",
      "[Epoch: 17280] train loss: 0.3169, train acc: 0.8831, val loss: 0.2482, val acc: 0.9214  (best train acc: 0.8954, best val acc: 0.9430)\n",
      "[Epoch: 17300] train loss: 0.3582, train acc: 0.8626, val loss: 0.2216, val acc: 0.9447  (best train acc: 0.8991, best val acc: 0.9447)\n",
      "[Epoch: 17320] train loss: 0.3345, train acc: 0.8723, val loss: 0.2162, val acc: 0.9396  (best train acc: 0.9096, best val acc: 0.9470)\n",
      "[Epoch: 17340] train loss: 0.2664, train acc: 0.9032, val loss: 0.2096, val acc: 0.9393  (best train acc: 0.9096, best val acc: 0.9511)\n",
      "[Epoch: 17360] train loss: 0.3363, train acc: 0.8772, val loss: 0.2114, val acc: 0.9450  (best train acc: 0.9096, best val acc: 0.9511)\n",
      "[Epoch: 17380] train loss: 0.4002, train acc: 0.8553, val loss: 0.2197, val acc: 0.9481  (best train acc: 0.9096, best val acc: 0.9511)\n",
      "[Epoch: 17400] train loss: 0.2757, train acc: 0.9003, val loss: 0.2182, val acc: 0.9467  (best train acc: 0.9096, best val acc: 0.9511)\n",
      "[Epoch: 17420] train loss: 0.3028, train acc: 0.8846, val loss: 0.2188, val acc: 0.9373  (best train acc: 0.9096, best val acc: 0.9511)\n",
      "[Epoch: 17440] train loss: 0.2660, train acc: 0.9091, val loss: 0.2041, val acc: 0.9494  (best train acc: 0.9157, best val acc: 0.9511)\n",
      "[Epoch: 17460] train loss: 0.2617, train acc: 0.9096, val loss: 0.2082, val acc: 0.9487  (best train acc: 0.9157, best val acc: 0.9511)\n",
      "[Epoch: 17480] train loss: 0.3204, train acc: 0.8843, val loss: 0.2137, val acc: 0.9467  (best train acc: 0.9157, best val acc: 0.9511)\n",
      "[Epoch: 17500] train loss: 0.3247, train acc: 0.8816, val loss: 0.2170, val acc: 0.9444  (best train acc: 0.9157, best val acc: 0.9511)\n",
      "[Epoch: 17520] train loss: 0.2570, train acc: 0.9125, val loss: 0.2057, val acc: 0.9403  (best train acc: 0.9157, best val acc: 0.9511)\n",
      "[Epoch: 17540] train loss: 0.2623, train acc: 0.9119, val loss: 0.2113, val acc: 0.9417  (best train acc: 0.9158, best val acc: 0.9511)\n",
      "[Epoch: 17560] train loss: 0.2770, train acc: 0.9058, val loss: 0.2039, val acc: 0.9454  (best train acc: 0.9158, best val acc: 0.9514)\n",
      "[Epoch: 17580] train loss: 0.2789, train acc: 0.9048, val loss: 0.2099, val acc: 0.9396  (best train acc: 0.9211, best val acc: 0.9514)\n",
      "[Epoch: 17600] train loss: 0.2754, train acc: 0.9012, val loss: 0.2207, val acc: 0.9417  (best train acc: 0.9211, best val acc: 0.9514)\n",
      "[Epoch: 17620] train loss: 0.2713, train acc: 0.9015, val loss: 0.1975, val acc: 0.9467  (best train acc: 0.9211, best val acc: 0.9514)\n",
      "[Epoch: 17640] train loss: 0.2564, train acc: 0.9085, val loss: 0.2006, val acc: 0.9474  (best train acc: 0.9211, best val acc: 0.9514)\n",
      "[Epoch: 17660] train loss: 0.2681, train acc: 0.9046, val loss: 0.2097, val acc: 0.9403  (best train acc: 0.9211, best val acc: 0.9514)\n",
      "[Epoch: 17680] train loss: 0.2730, train acc: 0.8938, val loss: 0.1917, val acc: 0.9484  (best train acc: 0.9211, best val acc: 0.9514)\n",
      "[Epoch: 17700] train loss: 0.3475, train acc: 0.8778, val loss: 0.1985, val acc: 0.9440  (best train acc: 0.9211, best val acc: 0.9528)\n",
      "[Epoch: 17720] train loss: 0.2815, train acc: 0.8931, val loss: 0.2067, val acc: 0.9444  (best train acc: 0.9211, best val acc: 0.9528)\n",
      "[Epoch: 17740] train loss: 0.2557, train acc: 0.9110, val loss: 0.1916, val acc: 0.9501  (best train acc: 0.9211, best val acc: 0.9528)\n",
      "[Epoch: 17760] train loss: 0.2587, train acc: 0.9096, val loss: 0.2026, val acc: 0.9494  (best train acc: 0.9211, best val acc: 0.9528)\n",
      "[Epoch: 17780] train loss: 0.2664, train acc: 0.9101, val loss: 0.2011, val acc: 0.9477  (best train acc: 0.9211, best val acc: 0.9538)\n",
      "[Epoch: 17800] train loss: 0.2624, train acc: 0.9110, val loss: 0.1943, val acc: 0.9477  (best train acc: 0.9211, best val acc: 0.9538)\n",
      "[Epoch: 17820] train loss: 0.2975, train acc: 0.8832, val loss: 0.1956, val acc: 0.9450  (best train acc: 0.9222, best val acc: 0.9538)\n",
      "[Epoch: 17840] train loss: 0.2436, train acc: 0.9148, val loss: 0.1862, val acc: 0.9501  (best train acc: 0.9222, best val acc: 0.9538)\n",
      "[Epoch: 17860] train loss: 0.2735, train acc: 0.9030, val loss: 0.2062, val acc: 0.9454  (best train acc: 0.9222, best val acc: 0.9538)\n",
      "[Epoch: 17880] train loss: 0.2433, train acc: 0.9158, val loss: 0.2017, val acc: 0.9420  (best train acc: 0.9222, best val acc: 0.9545)\n",
      "[Epoch: 17900] train loss: 0.2477, train acc: 0.9128, val loss: 0.1980, val acc: 0.9477  (best train acc: 0.9222, best val acc: 0.9545)\n",
      "[Epoch: 17920] train loss: 0.2686, train acc: 0.9062, val loss: 0.1978, val acc: 0.9437  (best train acc: 0.9222, best val acc: 0.9545)\n",
      "[Epoch: 17940] train loss: 0.2920, train acc: 0.8944, val loss: 0.2109, val acc: 0.9336  (best train acc: 0.9222, best val acc: 0.9545)\n",
      "[Epoch: 17960] train loss: 0.2718, train acc: 0.9068, val loss: 0.2081, val acc: 0.9464  (best train acc: 0.9260, best val acc: 0.9545)\n",
      "[Epoch: 17980] train loss: 0.4186, train acc: 0.8540, val loss: 0.2106, val acc: 0.9470  (best train acc: 0.9260, best val acc: 0.9545)\n",
      "[Epoch: 18000] train loss: 0.2563, train acc: 0.9091, val loss: 0.1929, val acc: 0.9491  (best train acc: 0.9260, best val acc: 0.9545)\n",
      "[Epoch: 18020] train loss: 0.2533, train acc: 0.9143, val loss: 0.2101, val acc: 0.9403  (best train acc: 0.9260, best val acc: 0.9545)\n",
      "[Epoch: 18040] train loss: 0.2445, train acc: 0.9122, val loss: 0.1919, val acc: 0.9524  (best train acc: 0.9260, best val acc: 0.9545)\n",
      "[Epoch: 18060] train loss: 0.2392, train acc: 0.9177, val loss: 0.1954, val acc: 0.9501  (best train acc: 0.9260, best val acc: 0.9545)\n",
      "[Epoch: 18080] train loss: 0.3104, train acc: 0.8858, val loss: 0.1986, val acc: 0.9454  (best train acc: 0.9260, best val acc: 0.9545)\n",
      "[Epoch: 18100] train loss: 0.2678, train acc: 0.9024, val loss: 0.2085, val acc: 0.9352  (best train acc: 0.9260, best val acc: 0.9545)\n",
      "[Epoch: 18120] train loss: 0.2857, train acc: 0.8958, val loss: 0.1930, val acc: 0.9487  (best train acc: 0.9260, best val acc: 0.9545)\n",
      "[Epoch: 18140] train loss: 0.2349, train acc: 0.9195, val loss: 0.2007, val acc: 0.9467  (best train acc: 0.9260, best val acc: 0.9545)\n",
      "[Epoch: 18160] train loss: 0.2827, train acc: 0.8871, val loss: 0.1976, val acc: 0.9484  (best train acc: 0.9260, best val acc: 0.9545)\n",
      "[Epoch: 18180] train loss: 0.2414, train acc: 0.9157, val loss: 0.2014, val acc: 0.9474  (best train acc: 0.9260, best val acc: 0.9545)\n",
      "[Epoch: 18200] train loss: 0.2192, train acc: 0.9219, val loss: 0.1959, val acc: 0.9454  (best train acc: 0.9260, best val acc: 0.9545)\n",
      "[Epoch: 18220] train loss: 0.2600, train acc: 0.9058, val loss: 0.2064, val acc: 0.9487  (best train acc: 0.9260, best val acc: 0.9545)\n",
      "[Epoch: 18240] train loss: 0.2850, train acc: 0.8957, val loss: 0.1980, val acc: 0.9497  (best train acc: 0.9260, best val acc: 0.9545)\n",
      "[Epoch: 18260] train loss: 0.2417, train acc: 0.9157, val loss: 0.2063, val acc: 0.9484  (best train acc: 0.9260, best val acc: 0.9545)\n",
      "[Epoch: 18280] train loss: 0.2345, train acc: 0.9156, val loss: 0.2069, val acc: 0.9393  (best train acc: 0.9260, best val acc: 0.9545)\n",
      "[Epoch: 18300] train loss: 0.2624, train acc: 0.8995, val loss: 0.1963, val acc: 0.9470  (best train acc: 0.9263, best val acc: 0.9545)\n",
      "[Epoch: 18320] train loss: 0.2558, train acc: 0.9088, val loss: 0.2046, val acc: 0.9454  (best train acc: 0.9263, best val acc: 0.9545)\n",
      "[Epoch: 18340] train loss: 0.2746, train acc: 0.9045, val loss: 0.2202, val acc: 0.9278  (best train acc: 0.9263, best val acc: 0.9545)\n",
      "[Epoch: 18360] train loss: 0.2498, train acc: 0.9109, val loss: 0.1886, val acc: 0.9521  (best train acc: 0.9263, best val acc: 0.9545)\n",
      "[Epoch: 18380] train loss: 0.2035, train acc: 0.9299, val loss: 0.1891, val acc: 0.9440  (best train acc: 0.9299, best val acc: 0.9551)\n",
      "[Epoch: 18400] train loss: 0.2373, train acc: 0.9130, val loss: 0.2011, val acc: 0.9504  (best train acc: 0.9299, best val acc: 0.9551)\n",
      "[Epoch: 18420] train loss: 0.2404, train acc: 0.9171, val loss: 0.2095, val acc: 0.9504  (best train acc: 0.9299, best val acc: 0.9551)\n",
      "[Epoch: 18440] train loss: 0.2242, train acc: 0.9218, val loss: 0.1880, val acc: 0.9508  (best train acc: 0.9299, best val acc: 0.9551)\n",
      "[Epoch: 18460] train loss: 0.2206, train acc: 0.9207, val loss: 0.1949, val acc: 0.9413  (best train acc: 0.9299, best val acc: 0.9551)\n",
      "[Epoch: 18480] train loss: 0.2161, train acc: 0.9283, val loss: 0.1947, val acc: 0.9450  (best train acc: 0.9299, best val acc: 0.9551)\n",
      "[Epoch: 18500] train loss: 0.2661, train acc: 0.9017, val loss: 0.2130, val acc: 0.9477  (best train acc: 0.9299, best val acc: 0.9551)\n",
      "[Epoch: 18520] train loss: 0.2875, train acc: 0.8879, val loss: 0.2014, val acc: 0.9474  (best train acc: 0.9299, best val acc: 0.9551)\n",
      "[Epoch: 18540] train loss: 0.2219, train acc: 0.9233, val loss: 0.1915, val acc: 0.9467  (best train acc: 0.9299, best val acc: 0.9551)\n",
      "[Epoch: 18560] train loss: 0.2518, train acc: 0.9102, val loss: 0.2000, val acc: 0.9497  (best train acc: 0.9299, best val acc: 0.9551)\n",
      "[Epoch: 18580] train loss: 0.2409, train acc: 0.9144, val loss: 0.1850, val acc: 0.9484  (best train acc: 0.9302, best val acc: 0.9551)\n",
      "[Epoch: 18600] train loss: 0.2189, train acc: 0.9231, val loss: 0.1921, val acc: 0.9481  (best train acc: 0.9307, best val acc: 0.9551)\n",
      "[Epoch: 18620] train loss: 0.2163, train acc: 0.9258, val loss: 0.1998, val acc: 0.9433  (best train acc: 0.9307, best val acc: 0.9551)\n",
      "[Epoch: 18640] train loss: 0.2252, train acc: 0.9199, val loss: 0.1907, val acc: 0.9494  (best train acc: 0.9307, best val acc: 0.9551)\n",
      "[Epoch: 18660] train loss: 0.2354, train acc: 0.9106, val loss: 0.1955, val acc: 0.9491  (best train acc: 0.9324, best val acc: 0.9551)\n",
      "[Epoch: 18680] train loss: 0.2212, train acc: 0.9213, val loss: 0.1909, val acc: 0.9460  (best train acc: 0.9324, best val acc: 0.9551)\n",
      "[Epoch: 18700] train loss: 0.2143, train acc: 0.9239, val loss: 0.1952, val acc: 0.9501  (best train acc: 0.9324, best val acc: 0.9551)\n",
      "[Epoch: 18720] train loss: 0.2504, train acc: 0.9156, val loss: 0.2256, val acc: 0.9460  (best train acc: 0.9324, best val acc: 0.9551)\n",
      "[Epoch: 18740] train loss: 0.2202, train acc: 0.9245, val loss: 0.1979, val acc: 0.9467  (best train acc: 0.9324, best val acc: 0.9551)\n",
      "[Epoch: 18760] train loss: 0.2596, train acc: 0.9098, val loss: 0.1979, val acc: 0.9487  (best train acc: 0.9324, best val acc: 0.9551)\n",
      "[Epoch: 18780] train loss: 0.2177, train acc: 0.9202, val loss: 0.1896, val acc: 0.9460  (best train acc: 0.9324, best val acc: 0.9551)\n",
      "[Epoch: 18800] train loss: 0.2246, train acc: 0.9198, val loss: 0.2060, val acc: 0.9413  (best train acc: 0.9324, best val acc: 0.9551)\n",
      "[Epoch: 18820] train loss: 0.2243, train acc: 0.9217, val loss: 0.1794, val acc: 0.9511  (best train acc: 0.9324, best val acc: 0.9551)\n",
      "[Epoch: 18840] train loss: 0.2559, train acc: 0.9094, val loss: 0.1944, val acc: 0.9497  (best train acc: 0.9324, best val acc: 0.9551)\n",
      "[Epoch: 18860] train loss: 0.2375, train acc: 0.9140, val loss: 0.1910, val acc: 0.9521  (best train acc: 0.9334, best val acc: 0.9551)\n",
      "[Epoch: 18880] train loss: 0.2188, train acc: 0.9239, val loss: 0.1992, val acc: 0.9454  (best train acc: 0.9334, best val acc: 0.9551)\n",
      "[Epoch: 18900] train loss: 0.2154, train acc: 0.9298, val loss: 0.2010, val acc: 0.9514  (best train acc: 0.9334, best val acc: 0.9551)\n",
      "[Epoch: 18920] train loss: 0.3225, train acc: 0.8903, val loss: 0.2138, val acc: 0.9477  (best train acc: 0.9334, best val acc: 0.9551)\n",
      "[Epoch: 18940] train loss: 0.2324, train acc: 0.9223, val loss: 0.2015, val acc: 0.9521  (best train acc: 0.9334, best val acc: 0.9551)\n",
      "[Epoch: 18960] train loss: 0.2293, train acc: 0.9147, val loss: 0.1845, val acc: 0.9484  (best train acc: 0.9334, best val acc: 0.9551)\n",
      "[Epoch: 18980] train loss: 0.2123, train acc: 0.9252, val loss: 0.1951, val acc: 0.9521  (best train acc: 0.9334, best val acc: 0.9551)\n",
      "[Epoch: 19000] train loss: 0.2248, train acc: 0.9244, val loss: 0.1975, val acc: 0.9504  (best train acc: 0.9368, best val acc: 0.9551)\n",
      "[Epoch: 19020] train loss: 0.2112, train acc: 0.9226, val loss: 0.1920, val acc: 0.9477  (best train acc: 0.9368, best val acc: 0.9551)\n",
      "[Epoch: 19040] train loss: 0.2631, train acc: 0.9106, val loss: 0.1935, val acc: 0.9501  (best train acc: 0.9368, best val acc: 0.9568)\n",
      "[Epoch: 19060] train loss: 0.2752, train acc: 0.9018, val loss: 0.1999, val acc: 0.9524  (best train acc: 0.9368, best val acc: 0.9568)\n",
      "[Epoch: 19080] train loss: 0.2236, train acc: 0.9158, val loss: 0.2091, val acc: 0.9504  (best train acc: 0.9368, best val acc: 0.9568)\n",
      "[Epoch: 19100] train loss: 0.2524, train acc: 0.9059, val loss: 0.1915, val acc: 0.9518  (best train acc: 0.9368, best val acc: 0.9568)\n",
      "[Epoch: 19120] train loss: 0.2138, train acc: 0.9289, val loss: 0.2006, val acc: 0.9430  (best train acc: 0.9368, best val acc: 0.9568)\n",
      "[Epoch: 19140] train loss: 0.2295, train acc: 0.9226, val loss: 0.1926, val acc: 0.9521  (best train acc: 0.9368, best val acc: 0.9568)\n",
      "[Epoch: 19160] train loss: 0.2114, train acc: 0.9265, val loss: 0.1894, val acc: 0.9521  (best train acc: 0.9368, best val acc: 0.9568)\n",
      "[Epoch: 19180] train loss: 0.2171, train acc: 0.9237, val loss: 0.2014, val acc: 0.9386  (best train acc: 0.9368, best val acc: 0.9568)\n",
      "[Epoch: 19200] train loss: 0.2416, train acc: 0.9218, val loss: 0.2025, val acc: 0.9494  (best train acc: 0.9368, best val acc: 0.9568)\n",
      "[Epoch: 19220] train loss: 0.2076, train acc: 0.9289, val loss: 0.1959, val acc: 0.9562  (best train acc: 0.9368, best val acc: 0.9568)\n",
      "[Epoch: 19240] train loss: 0.1911, train acc: 0.9384, val loss: 0.1990, val acc: 0.9508  (best train acc: 0.9384, best val acc: 0.9568)\n",
      "[Epoch: 19260] train loss: 0.2085, train acc: 0.9289, val loss: 0.1959, val acc: 0.9501  (best train acc: 0.9384, best val acc: 0.9568)\n",
      "[Epoch: 19280] train loss: 0.2143, train acc: 0.9218, val loss: 0.2097, val acc: 0.9406  (best train acc: 0.9384, best val acc: 0.9568)\n",
      "[Epoch: 19300] train loss: 0.3338, train acc: 0.8819, val loss: 0.1931, val acc: 0.9524  (best train acc: 0.9384, best val acc: 0.9568)\n",
      "[Epoch: 19320] train loss: 0.2038, train acc: 0.9336, val loss: 0.2027, val acc: 0.9528  (best train acc: 0.9384, best val acc: 0.9568)\n",
      "[Epoch: 19340] train loss: 0.2430, train acc: 0.9074, val loss: 0.2034, val acc: 0.9511  (best train acc: 0.9384, best val acc: 0.9568)\n",
      "[Epoch: 19360] train loss: 0.2102, train acc: 0.9264, val loss: 0.2001, val acc: 0.9477  (best train acc: 0.9384, best val acc: 0.9568)\n",
      "[Epoch: 19380] train loss: 0.2443, train acc: 0.9121, val loss: 0.2026, val acc: 0.9518  (best train acc: 0.9384, best val acc: 0.9568)\n",
      "[Epoch: 19400] train loss: 0.2271, train acc: 0.9250, val loss: 0.2091, val acc: 0.9454  (best train acc: 0.9384, best val acc: 0.9568)\n",
      "[Epoch: 19420] train loss: 0.2275, train acc: 0.9250, val loss: 0.1880, val acc: 0.9545  (best train acc: 0.9384, best val acc: 0.9568)\n",
      "[Epoch: 19440] train loss: 0.2257, train acc: 0.9244, val loss: 0.2017, val acc: 0.9545  (best train acc: 0.9384, best val acc: 0.9568)\n",
      "[Epoch: 19460] train loss: 0.1978, train acc: 0.9286, val loss: 0.1926, val acc: 0.9511  (best train acc: 0.9384, best val acc: 0.9568)\n",
      "[Epoch: 19480] train loss: 0.2182, train acc: 0.9194, val loss: 0.2007, val acc: 0.9531  (best train acc: 0.9384, best val acc: 0.9568)\n",
      "[Epoch: 19500] train loss: 0.2288, train acc: 0.9225, val loss: 0.1982, val acc: 0.9514  (best train acc: 0.9384, best val acc: 0.9568)\n",
      "[Epoch: 19520] train loss: 0.2314, train acc: 0.9160, val loss: 0.2018, val acc: 0.9521  (best train acc: 0.9384, best val acc: 0.9568)\n",
      "[Epoch: 19540] train loss: 0.2285, train acc: 0.9173, val loss: 0.1932, val acc: 0.9484  (best train acc: 0.9384, best val acc: 0.9568)\n",
      "[Epoch: 19560] train loss: 0.2455, train acc: 0.9093, val loss: 0.2058, val acc: 0.9545  (best train acc: 0.9384, best val acc: 0.9568)\n",
      "[Epoch: 19580] train loss: 0.2480, train acc: 0.9137, val loss: 0.2138, val acc: 0.9464  (best train acc: 0.9384, best val acc: 0.9568)\n",
      "[Epoch: 19600] train loss: 0.2072, train acc: 0.9269, val loss: 0.2012, val acc: 0.9504  (best train acc: 0.9384, best val acc: 0.9568)\n",
      "[Epoch: 19620] train loss: 0.2304, train acc: 0.9164, val loss: 0.1953, val acc: 0.9501  (best train acc: 0.9384, best val acc: 0.9568)\n",
      "[Epoch: 19640] train loss: 0.3519, train acc: 0.8746, val loss: 0.2164, val acc: 0.9457  (best train acc: 0.9384, best val acc: 0.9568)\n",
      "[Epoch: 19660] train loss: 0.2153, train acc: 0.9252, val loss: 0.1847, val acc: 0.9494  (best train acc: 0.9384, best val acc: 0.9568)\n",
      "[Epoch: 19680] train loss: 0.2517, train acc: 0.9022, val loss: 0.1922, val acc: 0.9470  (best train acc: 0.9384, best val acc: 0.9568)\n",
      "[Epoch: 19700] train loss: 0.3131, train acc: 0.8895, val loss: 0.1915, val acc: 0.9484  (best train acc: 0.9384, best val acc: 0.9568)\n",
      "[Epoch: 19720] train loss: 0.2971, train acc: 0.8864, val loss: 0.2010, val acc: 0.9464  (best train acc: 0.9384, best val acc: 0.9568)\n",
      "[Epoch: 19740] train loss: 0.2530, train acc: 0.9010, val loss: 0.1980, val acc: 0.9467  (best train acc: 0.9384, best val acc: 0.9568)\n",
      "[Epoch: 19760] train loss: 0.1976, train acc: 0.9344, val loss: 0.1994, val acc: 0.9514  (best train acc: 0.9384, best val acc: 0.9568)\n",
      "[Epoch: 19780] train loss: 0.2755, train acc: 0.8995, val loss: 0.1873, val acc: 0.9521  (best train acc: 0.9384, best val acc: 0.9568)\n",
      "[Epoch: 19800] train loss: 0.2280, train acc: 0.9195, val loss: 0.2006, val acc: 0.9501  (best train acc: 0.9384, best val acc: 0.9568)\n",
      "[Epoch: 19820] train loss: 0.2336, train acc: 0.9139, val loss: 0.1816, val acc: 0.9501  (best train acc: 0.9384, best val acc: 0.9568)\n",
      "[Epoch: 19840] train loss: 0.2185, train acc: 0.9232, val loss: 0.1861, val acc: 0.9551  (best train acc: 0.9384, best val acc: 0.9568)\n",
      "[Epoch: 19860] train loss: 0.2259, train acc: 0.9213, val loss: 0.2038, val acc: 0.9514  (best train acc: 0.9384, best val acc: 0.9568)\n",
      "[Epoch: 19880] train loss: 0.2016, train acc: 0.9278, val loss: 0.1958, val acc: 0.9474  (best train acc: 0.9384, best val acc: 0.9568)\n",
      "[Epoch: 19900] train loss: 0.2609, train acc: 0.9093, val loss: 0.2019, val acc: 0.9457  (best train acc: 0.9384, best val acc: 0.9568)\n",
      "[Epoch: 19920] train loss: 0.2144, train acc: 0.9250, val loss: 0.1985, val acc: 0.9487  (best train acc: 0.9384, best val acc: 0.9568)\n",
      "[Epoch: 19940] train loss: 0.2246, train acc: 0.9163, val loss: 0.2010, val acc: 0.9508  (best train acc: 0.9384, best val acc: 0.9568)\n",
      "[Epoch: 19960] train loss: 0.1938, train acc: 0.9332, val loss: 0.2068, val acc: 0.9457  (best train acc: 0.9384, best val acc: 0.9568)\n",
      "[Epoch: 19980] train loss: 0.1959, train acc: 0.9377, val loss: 0.1939, val acc: 0.9535  (best train acc: 0.9384, best val acc: 0.9575)\n",
      "[Epoch: 20000] train loss: 0.2266, train acc: 0.9164, val loss: 0.1943, val acc: 0.9578  (best train acc: 0.9384, best val acc: 0.9578)\n",
      "[Epoch: 20020] train loss: 0.1968, train acc: 0.9323, val loss: 0.1883, val acc: 0.9551  (best train acc: 0.9384, best val acc: 0.9578)\n",
      "[Epoch: 20040] train loss: 0.2249, train acc: 0.9252, val loss: 0.1968, val acc: 0.9511  (best train acc: 0.9384, best val acc: 0.9578)\n",
      "[Epoch: 20060] train loss: 0.2249, train acc: 0.9245, val loss: 0.2015, val acc: 0.9524  (best train acc: 0.9384, best val acc: 0.9578)\n",
      "[Epoch: 20080] train loss: 0.2355, train acc: 0.9161, val loss: 0.2149, val acc: 0.9447  (best train acc: 0.9384, best val acc: 0.9578)\n",
      "[Epoch: 20100] train loss: 0.2026, train acc: 0.9257, val loss: 0.1892, val acc: 0.9551  (best train acc: 0.9384, best val acc: 0.9578)\n",
      "[Epoch: 20120] train loss: 0.2269, train acc: 0.9218, val loss: 0.1915, val acc: 0.9521  (best train acc: 0.9384, best val acc: 0.9578)\n",
      "[Epoch: 20140] train loss: 0.2129, train acc: 0.9248, val loss: 0.1942, val acc: 0.9548  (best train acc: 0.9384, best val acc: 0.9578)\n",
      "[Epoch: 20160] train loss: 0.2360, train acc: 0.9158, val loss: 0.1996, val acc: 0.9497  (best train acc: 0.9384, best val acc: 0.9578)\n",
      "[Epoch: 20180] train loss: 0.1996, train acc: 0.9299, val loss: 0.2033, val acc: 0.9541  (best train acc: 0.9384, best val acc: 0.9578)\n",
      "[Epoch: 20200] train loss: 0.2248, train acc: 0.9233, val loss: 0.1956, val acc: 0.9572  (best train acc: 0.9384, best val acc: 0.9578)\n",
      "[Epoch: 20220] train loss: 0.2174, train acc: 0.9251, val loss: 0.2000, val acc: 0.9518  (best train acc: 0.9384, best val acc: 0.9578)\n",
      "[Epoch: 20240] train loss: 0.2307, train acc: 0.9218, val loss: 0.2050, val acc: 0.9494  (best train acc: 0.9384, best val acc: 0.9578)\n",
      "[Epoch: 20260] train loss: 0.2258, train acc: 0.9218, val loss: 0.1961, val acc: 0.9535  (best train acc: 0.9384, best val acc: 0.9578)\n",
      "[Epoch: 20280] train loss: 0.2303, train acc: 0.9137, val loss: 0.2191, val acc: 0.9491  (best train acc: 0.9384, best val acc: 0.9578)\n",
      "[Epoch: 20300] train loss: 0.2343, train acc: 0.9145, val loss: 0.2029, val acc: 0.9511  (best train acc: 0.9384, best val acc: 0.9578)\n",
      "[Epoch: 20320] train loss: 0.2116, train acc: 0.9292, val loss: 0.1922, val acc: 0.9474  (best train acc: 0.9384, best val acc: 0.9578)\n",
      "[Epoch: 20340] train loss: 0.2329, train acc: 0.9215, val loss: 0.1910, val acc: 0.9531  (best train acc: 0.9384, best val acc: 0.9578)\n",
      "[Epoch: 20360] train loss: 0.2172, train acc: 0.9279, val loss: 0.2164, val acc: 0.9487  (best train acc: 0.9384, best val acc: 0.9578)\n",
      "[Epoch: 20380] train loss: 0.2044, train acc: 0.9318, val loss: 0.1841, val acc: 0.9582  (best train acc: 0.9384, best val acc: 0.9582)\n",
      "[Epoch: 20400] train loss: 0.2109, train acc: 0.9280, val loss: 0.2007, val acc: 0.9514  (best train acc: 0.9384, best val acc: 0.9582)\n",
      "[Epoch: 20420] train loss: 0.2330, train acc: 0.9109, val loss: 0.1945, val acc: 0.9568  (best train acc: 0.9384, best val acc: 0.9582)\n",
      "[Epoch: 20440] train loss: 0.1951, train acc: 0.9319, val loss: 0.1964, val acc: 0.9497  (best train acc: 0.9384, best val acc: 0.9582)\n",
      "[Epoch: 20460] train loss: 0.3141, train acc: 0.8778, val loss: 0.2034, val acc: 0.9518  (best train acc: 0.9384, best val acc: 0.9582)\n",
      "[Epoch: 20480] train loss: 0.1995, train acc: 0.9321, val loss: 0.2102, val acc: 0.9481  (best train acc: 0.9384, best val acc: 0.9582)\n",
      "[Epoch: 20500] train loss: 0.2258, train acc: 0.9241, val loss: 0.2277, val acc: 0.9511  (best train acc: 0.9384, best val acc: 0.9582)\n",
      "[Epoch: 20520] train loss: 0.2334, train acc: 0.9127, val loss: 0.1952, val acc: 0.9450  (best train acc: 0.9384, best val acc: 0.9582)\n",
      "[Epoch: 20540] train loss: 0.2141, train acc: 0.9229, val loss: 0.1994, val acc: 0.9518  (best train acc: 0.9384, best val acc: 0.9582)\n",
      "[Epoch: 20560] train loss: 0.2181, train acc: 0.9239, val loss: 0.1929, val acc: 0.9562  (best train acc: 0.9384, best val acc: 0.9582)\n",
      "[Epoch: 20580] train loss: 0.2283, train acc: 0.9184, val loss: 0.1989, val acc: 0.9582  (best train acc: 0.9384, best val acc: 0.9592)\n",
      "[Epoch: 20600] train loss: 0.2107, train acc: 0.9254, val loss: 0.1998, val acc: 0.9521  (best train acc: 0.9384, best val acc: 0.9592)\n",
      "[Epoch: 20620] train loss: 0.2056, train acc: 0.9336, val loss: 0.1945, val acc: 0.9514  (best train acc: 0.9402, best val acc: 0.9592)\n",
      "[Epoch: 20640] train loss: 0.2241, train acc: 0.9146, val loss: 0.2160, val acc: 0.9481  (best train acc: 0.9402, best val acc: 0.9592)\n",
      "[Epoch: 20660] train loss: 0.2085, train acc: 0.9271, val loss: 0.2081, val acc: 0.9497  (best train acc: 0.9402, best val acc: 0.9592)\n",
      "[Epoch: 20680] train loss: 0.1937, train acc: 0.9377, val loss: 0.2035, val acc: 0.9474  (best train acc: 0.9402, best val acc: 0.9592)\n",
      "[Epoch: 20700] train loss: 0.2142, train acc: 0.9310, val loss: 0.2146, val acc: 0.9535  (best train acc: 0.9402, best val acc: 0.9592)\n",
      "[Epoch: 20720] train loss: 0.2091, train acc: 0.9211, val loss: 0.1931, val acc: 0.9521  (best train acc: 0.9402, best val acc: 0.9592)\n",
      "[Epoch: 20740] train loss: 0.2048, train acc: 0.9278, val loss: 0.1935, val acc: 0.9531  (best train acc: 0.9402, best val acc: 0.9592)\n",
      "[Epoch: 20760] train loss: 0.2378, train acc: 0.9223, val loss: 0.1985, val acc: 0.9548  (best train acc: 0.9402, best val acc: 0.9592)\n",
      "[Epoch: 20780] train loss: 0.2251, train acc: 0.9155, val loss: 0.2056, val acc: 0.9508  (best train acc: 0.9402, best val acc: 0.9592)\n",
      "[Epoch: 20800] train loss: 0.2307, train acc: 0.9190, val loss: 0.2021, val acc: 0.9494  (best train acc: 0.9402, best val acc: 0.9592)\n",
      "[Epoch: 20820] train loss: 0.2067, train acc: 0.9289, val loss: 0.1898, val acc: 0.9538  (best train acc: 0.9402, best val acc: 0.9592)\n",
      "[Epoch: 20840] train loss: 0.2026, train acc: 0.9268, val loss: 0.1933, val acc: 0.9562  (best train acc: 0.9402, best val acc: 0.9592)\n",
      "[Epoch: 20860] train loss: 0.2168, train acc: 0.9192, val loss: 0.1982, val acc: 0.9545  (best train acc: 0.9402, best val acc: 0.9592)\n",
      "[Epoch: 20880] train loss: 0.2029, train acc: 0.9325, val loss: 0.2047, val acc: 0.9484  (best train acc: 0.9402, best val acc: 0.9592)\n",
      "[Epoch: 20900] train loss: 0.2002, train acc: 0.9343, val loss: 0.1866, val acc: 0.9585  (best train acc: 0.9402, best val acc: 0.9592)\n",
      "[Epoch: 20920] train loss: 0.2193, train acc: 0.9211, val loss: 0.2093, val acc: 0.9521  (best train acc: 0.9402, best val acc: 0.9592)\n",
      "[Epoch: 20940] train loss: 0.2259, train acc: 0.9208, val loss: 0.2026, val acc: 0.9470  (best train acc: 0.9402, best val acc: 0.9592)\n",
      "[Epoch: 20960] train loss: 0.2016, train acc: 0.9338, val loss: 0.2035, val acc: 0.9555  (best train acc: 0.9402, best val acc: 0.9592)\n",
      "[Epoch: 20980] train loss: 0.1969, train acc: 0.9341, val loss: 0.2113, val acc: 0.9558  (best train acc: 0.9402, best val acc: 0.9609)\n",
      "[Epoch: 21000] train loss: 0.2333, train acc: 0.9114, val loss: 0.2135, val acc: 0.9548  (best train acc: 0.9402, best val acc: 0.9609)\n",
      "[Epoch: 21020] train loss: 0.2303, train acc: 0.9207, val loss: 0.2042, val acc: 0.9548  (best train acc: 0.9402, best val acc: 0.9609)\n",
      "[Epoch: 21040] train loss: 0.2159, train acc: 0.9264, val loss: 0.1987, val acc: 0.9562  (best train acc: 0.9402, best val acc: 0.9609)\n",
      "[Epoch: 21060] train loss: 0.2449, train acc: 0.9149, val loss: 0.2088, val acc: 0.9474  (best train acc: 0.9402, best val acc: 0.9609)\n",
      "[Epoch: 21080] train loss: 0.2128, train acc: 0.9260, val loss: 0.2108, val acc: 0.9501  (best train acc: 0.9402, best val acc: 0.9609)\n",
      "[Epoch: 21100] train loss: 0.2375, train acc: 0.9077, val loss: 0.2064, val acc: 0.9514  (best train acc: 0.9402, best val acc: 0.9609)\n",
      "[Epoch: 21120] train loss: 0.2203, train acc: 0.9229, val loss: 0.2001, val acc: 0.9551  (best train acc: 0.9402, best val acc: 0.9609)\n",
      "[Epoch: 21140] train loss: 0.2385, train acc: 0.9134, val loss: 0.1928, val acc: 0.9568  (best train acc: 0.9402, best val acc: 0.9609)\n",
      "[Epoch: 21160] train loss: 0.2100, train acc: 0.9247, val loss: 0.2070, val acc: 0.9592  (best train acc: 0.9402, best val acc: 0.9609)\n",
      "[Epoch: 21180] train loss: 0.2320, train acc: 0.9223, val loss: 0.1997, val acc: 0.9524  (best train acc: 0.9402, best val acc: 0.9609)\n",
      "[Epoch: 21200] train loss: 0.2227, train acc: 0.9203, val loss: 0.1952, val acc: 0.9535  (best train acc: 0.9402, best val acc: 0.9609)\n",
      "[Epoch: 21220] train loss: 0.1906, train acc: 0.9378, val loss: 0.1956, val acc: 0.9568  (best train acc: 0.9402, best val acc: 0.9609)\n",
      "[Epoch: 21240] train loss: 0.2120, train acc: 0.9275, val loss: 0.1984, val acc: 0.9535  (best train acc: 0.9416, best val acc: 0.9609)\n",
      "[Epoch: 21260] train loss: 0.2071, train acc: 0.9298, val loss: 0.2011, val acc: 0.9538  (best train acc: 0.9416, best val acc: 0.9609)\n",
      "[Epoch: 21280] train loss: 0.1935, train acc: 0.9367, val loss: 0.1963, val acc: 0.9528  (best train acc: 0.9416, best val acc: 0.9609)\n",
      "[Epoch: 21300] train loss: 0.2239, train acc: 0.9227, val loss: 0.2030, val acc: 0.9545  (best train acc: 0.9416, best val acc: 0.9612)\n",
      "[Epoch: 21320] train loss: 0.1974, train acc: 0.9283, val loss: 0.2037, val acc: 0.9551  (best train acc: 0.9416, best val acc: 0.9612)\n",
      "[Epoch: 21340] train loss: 0.2446, train acc: 0.9145, val loss: 0.2055, val acc: 0.9531  (best train acc: 0.9416, best val acc: 0.9612)\n",
      "[Epoch: 21360] train loss: 0.2153, train acc: 0.9281, val loss: 0.1939, val acc: 0.9541  (best train acc: 0.9416, best val acc: 0.9612)\n",
      "[Epoch: 21380] train loss: 0.2179, train acc: 0.9174, val loss: 0.2092, val acc: 0.9514  (best train acc: 0.9416, best val acc: 0.9612)\n",
      "[Epoch: 21400] train loss: 0.2070, train acc: 0.9276, val loss: 0.1821, val acc: 0.9562  (best train acc: 0.9416, best val acc: 0.9612)\n",
      "[Epoch: 21420] train loss: 0.2139, train acc: 0.9263, val loss: 0.2060, val acc: 0.9562  (best train acc: 0.9416, best val acc: 0.9612)\n",
      "[Epoch: 21440] train loss: 0.2025, train acc: 0.9292, val loss: 0.1997, val acc: 0.9609  (best train acc: 0.9416, best val acc: 0.9612)\n",
      "[Epoch: 21460] train loss: 0.1987, train acc: 0.9319, val loss: 0.1913, val acc: 0.9585  (best train acc: 0.9416, best val acc: 0.9612)\n",
      "[Epoch: 21480] train loss: 0.2256, train acc: 0.9164, val loss: 0.1881, val acc: 0.9514  (best train acc: 0.9416, best val acc: 0.9612)\n",
      "[Epoch: 21500] train loss: 0.2041, train acc: 0.9259, val loss: 0.1900, val acc: 0.9535  (best train acc: 0.9416, best val acc: 0.9612)\n",
      "[Epoch: 21520] train loss: 0.1977, train acc: 0.9334, val loss: 0.1892, val acc: 0.9575  (best train acc: 0.9416, best val acc: 0.9612)\n",
      "[Epoch: 21540] train loss: 0.2029, train acc: 0.9298, val loss: 0.1880, val acc: 0.9504  (best train acc: 0.9416, best val acc: 0.9612)\n",
      "[Epoch: 21560] train loss: 0.2148, train acc: 0.9255, val loss: 0.1925, val acc: 0.9444  (best train acc: 0.9416, best val acc: 0.9612)\n",
      "[Epoch: 21580] train loss: 0.2033, train acc: 0.9344, val loss: 0.1935, val acc: 0.9562  (best train acc: 0.9416, best val acc: 0.9612)\n",
      "[Epoch: 21600] train loss: 0.2112, train acc: 0.9271, val loss: 0.1865, val acc: 0.9578  (best train acc: 0.9416, best val acc: 0.9612)\n",
      "[Epoch: 21620] train loss: 0.1973, train acc: 0.9343, val loss: 0.1970, val acc: 0.9572  (best train acc: 0.9416, best val acc: 0.9612)\n",
      "[Epoch: 21640] train loss: 0.2811, train acc: 0.9022, val loss: 0.2075, val acc: 0.9497  (best train acc: 0.9416, best val acc: 0.9612)\n",
      "[Epoch: 21660] train loss: 0.2515, train acc: 0.9008, val loss: 0.1957, val acc: 0.9605  (best train acc: 0.9416, best val acc: 0.9612)\n",
      "[Epoch: 21680] train loss: 0.1927, train acc: 0.9338, val loss: 0.2008, val acc: 0.9541  (best train acc: 0.9416, best val acc: 0.9612)\n",
      "[Epoch: 21700] train loss: 0.2137, train acc: 0.9284, val loss: 0.1998, val acc: 0.9470  (best train acc: 0.9416, best val acc: 0.9612)\n",
      "[Epoch: 21720] train loss: 0.2146, train acc: 0.9233, val loss: 0.2010, val acc: 0.9558  (best train acc: 0.9416, best val acc: 0.9612)\n",
      "[Epoch: 21740] train loss: 0.2112, train acc: 0.9250, val loss: 0.2012, val acc: 0.9572  (best train acc: 0.9416, best val acc: 0.9612)\n",
      "[Epoch: 21760] train loss: 0.2121, train acc: 0.9268, val loss: 0.1939, val acc: 0.9528  (best train acc: 0.9416, best val acc: 0.9612)\n",
      "[Epoch: 21780] train loss: 0.2195, train acc: 0.9260, val loss: 0.2095, val acc: 0.9504  (best train acc: 0.9416, best val acc: 0.9612)\n",
      "[Epoch: 21800] train loss: 0.2135, train acc: 0.9307, val loss: 0.2130, val acc: 0.9511  (best train acc: 0.9416, best val acc: 0.9612)\n",
      "[Epoch: 21820] train loss: 0.2039, train acc: 0.9262, val loss: 0.2002, val acc: 0.9484  (best train acc: 0.9416, best val acc: 0.9612)\n",
      "[Epoch: 21840] train loss: 0.2166, train acc: 0.9238, val loss: 0.1897, val acc: 0.9578  (best train acc: 0.9416, best val acc: 0.9612)\n",
      "[Epoch: 21860] train loss: 0.1870, train acc: 0.9378, val loss: 0.1959, val acc: 0.9589  (best train acc: 0.9416, best val acc: 0.9612)\n",
      "[Epoch: 21880] train loss: 0.1924, train acc: 0.9303, val loss: 0.2012, val acc: 0.9514  (best train acc: 0.9416, best val acc: 0.9612)\n",
      "[Epoch: 21900] train loss: 0.1957, train acc: 0.9284, val loss: 0.1955, val acc: 0.9565  (best train acc: 0.9416, best val acc: 0.9616)\n",
      "[Epoch: 21920] train loss: 0.1904, train acc: 0.9340, val loss: 0.1963, val acc: 0.9521  (best train acc: 0.9416, best val acc: 0.9616)\n",
      "[Epoch: 21940] train loss: 0.1897, train acc: 0.9309, val loss: 0.1937, val acc: 0.9541  (best train acc: 0.9416, best val acc: 0.9616)\n",
      "[Epoch: 21960] train loss: 0.1999, train acc: 0.9224, val loss: 0.2031, val acc: 0.9518  (best train acc: 0.9416, best val acc: 0.9616)\n",
      "[Epoch: 21980] train loss: 0.2331, train acc: 0.9109, val loss: 0.2087, val acc: 0.9524  (best train acc: 0.9416, best val acc: 0.9616)\n",
      "[Epoch: 22000] train loss: 0.2235, train acc: 0.9237, val loss: 0.1981, val acc: 0.9558  (best train acc: 0.9416, best val acc: 0.9616)\n",
      "[Epoch: 22020] train loss: 0.2283, train acc: 0.9180, val loss: 0.2224, val acc: 0.9403  (best train acc: 0.9416, best val acc: 0.9616)\n",
      "[Epoch: 22040] train loss: 0.2159, train acc: 0.9174, val loss: 0.2271, val acc: 0.9376  (best train acc: 0.9416, best val acc: 0.9616)\n",
      "[Epoch: 22060] train loss: 0.2008, train acc: 0.9266, val loss: 0.1873, val acc: 0.9599  (best train acc: 0.9416, best val acc: 0.9616)\n",
      "[Epoch: 22080] train loss: 0.2286, train acc: 0.9190, val loss: 0.2014, val acc: 0.9575  (best train acc: 0.9416, best val acc: 0.9616)\n",
      "[Epoch: 22100] train loss: 0.2563, train acc: 0.9088, val loss: 0.1892, val acc: 0.9565  (best train acc: 0.9416, best val acc: 0.9616)\n",
      "[Epoch: 22120] train loss: 0.2478, train acc: 0.9168, val loss: 0.2053, val acc: 0.9568  (best train acc: 0.9416, best val acc: 0.9616)\n",
      "[Epoch: 22140] train loss: 0.2165, train acc: 0.9250, val loss: 0.1985, val acc: 0.9592  (best train acc: 0.9416, best val acc: 0.9616)\n",
      "[Epoch: 22160] train loss: 0.1913, train acc: 0.9372, val loss: 0.2015, val acc: 0.9592  (best train acc: 0.9416, best val acc: 0.9616)\n",
      "[Epoch: 22180] train loss: 0.1977, train acc: 0.9323, val loss: 0.2040, val acc: 0.9599  (best train acc: 0.9416, best val acc: 0.9616)\n",
      "[Epoch: 22200] train loss: 0.2225, train acc: 0.9296, val loss: 0.1937, val acc: 0.9568  (best train acc: 0.9416, best val acc: 0.9616)\n",
      "[Epoch: 22220] train loss: 0.2100, train acc: 0.9273, val loss: 0.1976, val acc: 0.9545  (best train acc: 0.9416, best val acc: 0.9616)\n",
      "[Epoch: 22240] train loss: 0.2114, train acc: 0.9239, val loss: 0.1987, val acc: 0.9497  (best train acc: 0.9416, best val acc: 0.9616)\n",
      "[Epoch: 22260] train loss: 0.2797, train acc: 0.8974, val loss: 0.2018, val acc: 0.9501  (best train acc: 0.9416, best val acc: 0.9616)\n",
      "[Epoch: 22280] train loss: 0.2192, train acc: 0.9175, val loss: 0.1885, val acc: 0.9548  (best train acc: 0.9416, best val acc: 0.9616)\n",
      "[Epoch: 22300] train loss: 0.1973, train acc: 0.9305, val loss: 0.1856, val acc: 0.9582  (best train acc: 0.9416, best val acc: 0.9616)\n",
      "[Epoch: 22320] train loss: 0.1989, train acc: 0.9307, val loss: 0.2032, val acc: 0.9558  (best train acc: 0.9416, best val acc: 0.9616)\n",
      "[Epoch: 22340] train loss: 0.2009, train acc: 0.9299, val loss: 0.2049, val acc: 0.9555  (best train acc: 0.9416, best val acc: 0.9616)\n",
      "[Epoch: 22360] train loss: 0.1818, train acc: 0.9332, val loss: 0.1903, val acc: 0.9589  (best train acc: 0.9418, best val acc: 0.9616)\n",
      "[Epoch: 22380] train loss: 0.2062, train acc: 0.9320, val loss: 0.2063, val acc: 0.9562  (best train acc: 0.9418, best val acc: 0.9616)\n",
      "[Epoch: 22400] train loss: 0.2017, train acc: 0.9315, val loss: 0.2046, val acc: 0.9582  (best train acc: 0.9418, best val acc: 0.9616)\n",
      "[Epoch: 22420] train loss: 0.1951, train acc: 0.9336, val loss: 0.2226, val acc: 0.9474  (best train acc: 0.9418, best val acc: 0.9616)\n",
      "[Epoch: 22440] train loss: 0.2110, train acc: 0.9299, val loss: 0.1984, val acc: 0.9575  (best train acc: 0.9418, best val acc: 0.9616)\n",
      "[Epoch: 22460] train loss: 0.1981, train acc: 0.9313, val loss: 0.1895, val acc: 0.9589  (best train acc: 0.9418, best val acc: 0.9616)\n",
      "[Epoch: 22480] train loss: 0.3111, train acc: 0.8898, val loss: 0.2090, val acc: 0.9548  (best train acc: 0.9418, best val acc: 0.9616)\n",
      "[Epoch: 22500] train loss: 0.2253, train acc: 0.9202, val loss: 0.2083, val acc: 0.9541  (best train acc: 0.9418, best val acc: 0.9616)\n",
      "[Epoch: 22520] train loss: 0.2250, train acc: 0.9205, val loss: 0.2048, val acc: 0.9575  (best train acc: 0.9418, best val acc: 0.9616)\n",
      "[Epoch: 22540] train loss: 0.2337, train acc: 0.9094, val loss: 0.2016, val acc: 0.9578  (best train acc: 0.9418, best val acc: 0.9616)\n",
      "[Epoch: 22560] train loss: 0.2011, train acc: 0.9297, val loss: 0.2001, val acc: 0.9612  (best train acc: 0.9418, best val acc: 0.9616)\n",
      "[Epoch: 22580] train loss: 0.2230, train acc: 0.9119, val loss: 0.2256, val acc: 0.9440  (best train acc: 0.9418, best val acc: 0.9619)\n",
      "[Epoch: 22600] train loss: 0.2335, train acc: 0.9174, val loss: 0.1957, val acc: 0.9609  (best train acc: 0.9418, best val acc: 0.9619)\n",
      "[Epoch: 22620] train loss: 0.2074, train acc: 0.9265, val loss: 0.2056, val acc: 0.9568  (best train acc: 0.9418, best val acc: 0.9619)\n",
      "[Epoch: 22640] train loss: 0.2197, train acc: 0.9183, val loss: 0.1959, val acc: 0.9551  (best train acc: 0.9418, best val acc: 0.9619)\n",
      "[Epoch: 22660] train loss: 0.2036, train acc: 0.9301, val loss: 0.2052, val acc: 0.9578  (best train acc: 0.9418, best val acc: 0.9619)\n",
      "[Epoch: 22680] train loss: 0.1931, train acc: 0.9332, val loss: 0.2208, val acc: 0.9518  (best train acc: 0.9418, best val acc: 0.9619)\n",
      "[Epoch: 22700] train loss: 0.2414, train acc: 0.9216, val loss: 0.2193, val acc: 0.9599  (best train acc: 0.9418, best val acc: 0.9619)\n",
      "[Epoch: 22720] train loss: 0.2066, train acc: 0.9344, val loss: 0.1812, val acc: 0.9562  (best train acc: 0.9418, best val acc: 0.9619)\n",
      "[Epoch: 22740] train loss: 0.1938, train acc: 0.9315, val loss: 0.2004, val acc: 0.9545  (best train acc: 0.9418, best val acc: 0.9619)\n",
      "[Epoch: 22760] train loss: 0.1968, train acc: 0.9352, val loss: 0.2022, val acc: 0.9592  (best train acc: 0.9418, best val acc: 0.9619)\n",
      "[Epoch: 22780] train loss: 0.1853, train acc: 0.9378, val loss: 0.2018, val acc: 0.9585  (best train acc: 0.9418, best val acc: 0.9619)\n",
      "[Epoch: 22800] train loss: 0.2364, train acc: 0.9224, val loss: 0.1950, val acc: 0.9551  (best train acc: 0.9418, best val acc: 0.9619)\n",
      "[Epoch: 22820] train loss: 0.1949, train acc: 0.9364, val loss: 0.2026, val acc: 0.9582  (best train acc: 0.9418, best val acc: 0.9619)\n",
      "[Epoch: 22840] train loss: 0.2049, train acc: 0.9226, val loss: 0.2233, val acc: 0.9541  (best train acc: 0.9418, best val acc: 0.9619)\n",
      "[Epoch: 22860] train loss: 0.2168, train acc: 0.9226, val loss: 0.2012, val acc: 0.9558  (best train acc: 0.9418, best val acc: 0.9619)\n",
      "[Epoch: 22880] train loss: 0.2058, train acc: 0.9282, val loss: 0.1924, val acc: 0.9572  (best train acc: 0.9418, best val acc: 0.9619)\n",
      "[Epoch: 22900] train loss: 0.1923, train acc: 0.9349, val loss: 0.2106, val acc: 0.9551  (best train acc: 0.9418, best val acc: 0.9619)\n",
      "[Epoch: 22920] train loss: 0.2800, train acc: 0.9051, val loss: 0.2145, val acc: 0.9582  (best train acc: 0.9418, best val acc: 0.9619)\n",
      "[Epoch: 22940] train loss: 0.2112, train acc: 0.9242, val loss: 0.1895, val acc: 0.9568  (best train acc: 0.9418, best val acc: 0.9619)\n",
      "[Epoch: 22960] train loss: 0.1959, train acc: 0.9337, val loss: 0.1950, val acc: 0.9605  (best train acc: 0.9418, best val acc: 0.9619)\n",
      "[Epoch: 22980] train loss: 0.2421, train acc: 0.9111, val loss: 0.1921, val acc: 0.9568  (best train acc: 0.9418, best val acc: 0.9619)\n",
      "[Epoch: 23000] train loss: 0.2060, train acc: 0.9273, val loss: 0.1987, val acc: 0.9545  (best train acc: 0.9418, best val acc: 0.9619)\n",
      "[Epoch: 23020] train loss: 0.2031, train acc: 0.9310, val loss: 0.2004, val acc: 0.9572  (best train acc: 0.9418, best val acc: 0.9619)\n",
      "[Epoch: 23040] train loss: 0.2151, train acc: 0.9278, val loss: 0.2046, val acc: 0.9555  (best train acc: 0.9418, best val acc: 0.9619)\n",
      "[Epoch: 23060] train loss: 0.1872, train acc: 0.9362, val loss: 0.1981, val acc: 0.9595  (best train acc: 0.9418, best val acc: 0.9619)\n",
      "[Epoch: 23080] train loss: 0.2015, train acc: 0.9332, val loss: 0.1979, val acc: 0.9589  (best train acc: 0.9418, best val acc: 0.9619)\n",
      "[Epoch: 23100] train loss: 0.1912, train acc: 0.9319, val loss: 0.1986, val acc: 0.9589  (best train acc: 0.9418, best val acc: 0.9619)\n",
      "[Epoch: 23120] train loss: 0.2060, train acc: 0.9289, val loss: 0.2034, val acc: 0.9575  (best train acc: 0.9418, best val acc: 0.9619)\n",
      "[Epoch: 23140] train loss: 0.1973, train acc: 0.9275, val loss: 0.2177, val acc: 0.9497  (best train acc: 0.9418, best val acc: 0.9619)\n",
      "[Epoch: 23160] train loss: 0.2319, train acc: 0.9212, val loss: 0.2099, val acc: 0.9572  (best train acc: 0.9418, best val acc: 0.9619)\n",
      "[Epoch: 23180] train loss: 0.2010, train acc: 0.9289, val loss: 0.2109, val acc: 0.9582  (best train acc: 0.9418, best val acc: 0.9619)\n",
      "[Epoch: 23200] train loss: 0.3353, train acc: 0.8862, val loss: 0.2099, val acc: 0.9562  (best train acc: 0.9418, best val acc: 0.9619)\n",
      "[Epoch: 23220] train loss: 0.1907, train acc: 0.9374, val loss: 0.1968, val acc: 0.9575  (best train acc: 0.9418, best val acc: 0.9619)\n",
      "[Epoch: 23240] train loss: 0.2232, train acc: 0.9231, val loss: 0.2154, val acc: 0.9487  (best train acc: 0.9418, best val acc: 0.9619)\n",
      "[Epoch: 23260] train loss: 0.2846, train acc: 0.8958, val loss: 0.1843, val acc: 0.9548  (best train acc: 0.9418, best val acc: 0.9619)\n",
      "[Epoch: 23280] train loss: 0.1961, train acc: 0.9341, val loss: 0.1955, val acc: 0.9565  (best train acc: 0.9418, best val acc: 0.9619)\n",
      "[Epoch: 23300] train loss: 0.2443, train acc: 0.9041, val loss: 0.1960, val acc: 0.9558  (best train acc: 0.9418, best val acc: 0.9619)\n",
      "[Epoch: 23320] train loss: 0.2028, train acc: 0.9277, val loss: 0.2013, val acc: 0.9555  (best train acc: 0.9418, best val acc: 0.9619)\n",
      "[Epoch: 23340] train loss: 0.2015, train acc: 0.9277, val loss: 0.1880, val acc: 0.9575  (best train acc: 0.9418, best val acc: 0.9619)\n",
      "[Epoch: 23360] train loss: 0.1888, train acc: 0.9346, val loss: 0.1996, val acc: 0.9518  (best train acc: 0.9418, best val acc: 0.9619)\n",
      "[Epoch: 23380] train loss: 0.2182, train acc: 0.9179, val loss: 0.1925, val acc: 0.9578  (best train acc: 0.9418, best val acc: 0.9619)\n",
      "[Epoch: 23400] train loss: 0.2095, train acc: 0.9235, val loss: 0.1917, val acc: 0.9578  (best train acc: 0.9418, best val acc: 0.9619)\n",
      "[Epoch: 23420] train loss: 0.1973, train acc: 0.9353, val loss: 0.1906, val acc: 0.9589  (best train acc: 0.9418, best val acc: 0.9619)\n",
      "[Epoch: 23440] train loss: 0.2042, train acc: 0.9289, val loss: 0.1904, val acc: 0.9589  (best train acc: 0.9418, best val acc: 0.9636)\n",
      "[Epoch: 23460] train loss: 0.1993, train acc: 0.9241, val loss: 0.1970, val acc: 0.9504  (best train acc: 0.9418, best val acc: 0.9636)\n",
      "[Epoch: 23480] train loss: 0.2324, train acc: 0.9210, val loss: 0.1970, val acc: 0.9548  (best train acc: 0.9418, best val acc: 0.9636)\n",
      "[Epoch: 23500] train loss: 0.2005, train acc: 0.9359, val loss: 0.1935, val acc: 0.9578  (best train acc: 0.9418, best val acc: 0.9636)\n",
      "[Epoch: 23520] train loss: 0.2121, train acc: 0.9242, val loss: 0.1910, val acc: 0.9562  (best train acc: 0.9418, best val acc: 0.9636)\n",
      "[Epoch: 23540] train loss: 0.2059, train acc: 0.9252, val loss: 0.2086, val acc: 0.9558  (best train acc: 0.9418, best val acc: 0.9636)\n",
      "[Epoch: 23560] train loss: 0.1911, train acc: 0.9373, val loss: 0.1992, val acc: 0.9599  (best train acc: 0.9418, best val acc: 0.9636)\n",
      "[Epoch: 23580] train loss: 0.2322, train acc: 0.9080, val loss: 0.2003, val acc: 0.9467  (best train acc: 0.9418, best val acc: 0.9636)\n",
      "[Epoch: 23600] train loss: 0.2530, train acc: 0.9028, val loss: 0.1943, val acc: 0.9555  (best train acc: 0.9418, best val acc: 0.9636)\n",
      "[Epoch: 23620] train loss: 0.2026, train acc: 0.9276, val loss: 0.1841, val acc: 0.9578  (best train acc: 0.9418, best val acc: 0.9636)\n",
      "[Epoch: 23640] train loss: 0.2503, train acc: 0.9098, val loss: 0.2116, val acc: 0.9511  (best train acc: 0.9418, best val acc: 0.9636)\n",
      "[Epoch: 23660] train loss: 0.2506, train acc: 0.9059, val loss: 0.1944, val acc: 0.9497  (best train acc: 0.9418, best val acc: 0.9636)\n",
      "[Epoch: 23680] train loss: 0.1949, train acc: 0.9342, val loss: 0.1773, val acc: 0.9589  (best train acc: 0.9418, best val acc: 0.9636)\n",
      "[Epoch: 23700] train loss: 0.2155, train acc: 0.9193, val loss: 0.1956, val acc: 0.9521  (best train acc: 0.9418, best val acc: 0.9636)\n",
      "[Epoch: 23720] train loss: 0.2326, train acc: 0.9140, val loss: 0.2093, val acc: 0.9518  (best train acc: 0.9418, best val acc: 0.9636)\n",
      "[Epoch: 23740] train loss: 0.2118, train acc: 0.9301, val loss: 0.2018, val acc: 0.9504  (best train acc: 0.9418, best val acc: 0.9636)\n",
      "[Epoch: 23760] train loss: 0.2212, train acc: 0.9187, val loss: 0.2127, val acc: 0.9447  (best train acc: 0.9418, best val acc: 0.9636)\n",
      "[Epoch: 23780] train loss: 0.2233, train acc: 0.9130, val loss: 0.2111, val acc: 0.9528  (best train acc: 0.9418, best val acc: 0.9636)\n",
      "[Epoch: 23800] train loss: 0.2572, train acc: 0.9017, val loss: 0.2045, val acc: 0.9518  (best train acc: 0.9418, best val acc: 0.9636)\n",
      "[Epoch: 23820] train loss: 0.2198, train acc: 0.9183, val loss: 0.1994, val acc: 0.9575  (best train acc: 0.9418, best val acc: 0.9636)\n",
      "[Epoch: 23840] train loss: 0.2264, train acc: 0.9214, val loss: 0.2130, val acc: 0.9491  (best train acc: 0.9418, best val acc: 0.9636)\n",
      "[Epoch: 23860] train loss: 0.2103, train acc: 0.9273, val loss: 0.1846, val acc: 0.9582  (best train acc: 0.9418, best val acc: 0.9636)\n",
      "[Epoch: 23880] train loss: 0.2118, train acc: 0.9260, val loss: 0.1922, val acc: 0.9531  (best train acc: 0.9418, best val acc: 0.9636)\n",
      "[Epoch: 23900] train loss: 0.2090, train acc: 0.9261, val loss: 0.1973, val acc: 0.9589  (best train acc: 0.9418, best val acc: 0.9636)\n",
      "[Epoch: 23920] train loss: 0.1747, train acc: 0.9382, val loss: 0.1981, val acc: 0.9548  (best train acc: 0.9440, best val acc: 0.9636)\n",
      "[Epoch: 23940] train loss: 0.1770, train acc: 0.9378, val loss: 0.1854, val acc: 0.9582  (best train acc: 0.9440, best val acc: 0.9636)\n",
      "[Epoch: 23960] train loss: 0.1833, train acc: 0.9404, val loss: 0.2020, val acc: 0.9551  (best train acc: 0.9440, best val acc: 0.9636)\n",
      "[Epoch: 23980] train loss: 0.1862, train acc: 0.9353, val loss: 0.1827, val acc: 0.9609  (best train acc: 0.9440, best val acc: 0.9636)\n",
      "[Epoch: 24000] train loss: 0.1914, train acc: 0.9359, val loss: 0.1797, val acc: 0.9636  (best train acc: 0.9464, best val acc: 0.9636)\n",
      "[Epoch: 24020] train loss: 0.1713, train acc: 0.9390, val loss: 0.1881, val acc: 0.9599  (best train acc: 0.9464, best val acc: 0.9646)\n",
      "[Epoch: 24040] train loss: 0.1963, train acc: 0.9350, val loss: 0.1775, val acc: 0.9602  (best train acc: 0.9464, best val acc: 0.9646)\n",
      "[Epoch: 24060] train loss: 0.1760, train acc: 0.9390, val loss: 0.1845, val acc: 0.9555  (best train acc: 0.9464, best val acc: 0.9646)\n",
      "[Epoch: 24080] train loss: 0.2001, train acc: 0.9295, val loss: 0.1736, val acc: 0.9609  (best train acc: 0.9464, best val acc: 0.9646)\n",
      "[Epoch: 24100] train loss: 0.1781, train acc: 0.9385, val loss: 0.2075, val acc: 0.9457  (best train acc: 0.9464, best val acc: 0.9646)\n",
      "[Epoch: 24120] train loss: 0.1944, train acc: 0.9334, val loss: 0.1826, val acc: 0.9589  (best train acc: 0.9464, best val acc: 0.9646)\n",
      "[Epoch: 24140] train loss: 0.1833, train acc: 0.9347, val loss: 0.1980, val acc: 0.9572  (best train acc: 0.9464, best val acc: 0.9646)\n",
      "[Epoch: 24160] train loss: 0.1877, train acc: 0.9378, val loss: 0.1837, val acc: 0.9609  (best train acc: 0.9464, best val acc: 0.9646)\n",
      "[Epoch: 24180] train loss: 0.2021, train acc: 0.9237, val loss: 0.1840, val acc: 0.9551  (best train acc: 0.9464, best val acc: 0.9646)\n",
      "[Epoch: 24200] train loss: 0.1841, train acc: 0.9370, val loss: 0.1922, val acc: 0.9595  (best train acc: 0.9464, best val acc: 0.9653)\n",
      "[Epoch: 24220] train loss: 0.1872, train acc: 0.9317, val loss: 0.1805, val acc: 0.9629  (best train acc: 0.9464, best val acc: 0.9653)\n",
      "[Epoch: 24240] train loss: 0.1678, train acc: 0.9421, val loss: 0.1821, val acc: 0.9626  (best train acc: 0.9464, best val acc: 0.9653)\n",
      "[Epoch: 24260] train loss: 0.1654, train acc: 0.9430, val loss: 0.1777, val acc: 0.9616  (best train acc: 0.9464, best val acc: 0.9653)\n",
      "[Epoch: 24280] train loss: 0.1803, train acc: 0.9415, val loss: 0.1994, val acc: 0.9514  (best train acc: 0.9464, best val acc: 0.9653)\n",
      "[Epoch: 24300] train loss: 0.1836, train acc: 0.9346, val loss: 0.1867, val acc: 0.9609  (best train acc: 0.9464, best val acc: 0.9653)\n",
      "[Epoch: 24320] train loss: 0.1931, train acc: 0.9373, val loss: 0.1758, val acc: 0.9592  (best train acc: 0.9464, best val acc: 0.9653)\n",
      "[Epoch: 24340] train loss: 0.1841, train acc: 0.9383, val loss: 0.1736, val acc: 0.9612  (best train acc: 0.9464, best val acc: 0.9653)\n",
      "[Epoch: 24360] train loss: 0.2138, train acc: 0.9211, val loss: 0.1792, val acc: 0.9589  (best train acc: 0.9473, best val acc: 0.9653)\n",
      "[Epoch: 24380] train loss: 0.1905, train acc: 0.9332, val loss: 0.1795, val acc: 0.9562  (best train acc: 0.9473, best val acc: 0.9653)\n",
      "[Epoch: 24400] train loss: 0.2029, train acc: 0.9204, val loss: 0.1664, val acc: 0.9612  (best train acc: 0.9473, best val acc: 0.9653)\n",
      "[Epoch: 24420] train loss: 0.2027, train acc: 0.9273, val loss: 0.1728, val acc: 0.9629  (best train acc: 0.9473, best val acc: 0.9653)\n",
      "[Epoch: 24440] train loss: 0.1933, train acc: 0.9320, val loss: 0.1704, val acc: 0.9565  (best train acc: 0.9473, best val acc: 0.9653)\n",
      "[Epoch: 24460] train loss: 0.1827, train acc: 0.9389, val loss: 0.1711, val acc: 0.9578  (best train acc: 0.9473, best val acc: 0.9653)\n",
      "[Epoch: 24480] train loss: 0.1882, train acc: 0.9301, val loss: 0.1783, val acc: 0.9595  (best train acc: 0.9473, best val acc: 0.9653)\n",
      "[Epoch: 24500] train loss: 0.1907, train acc: 0.9357, val loss: 0.1898, val acc: 0.9582  (best train acc: 0.9473, best val acc: 0.9653)\n",
      "[Epoch: 24520] train loss: 0.1694, train acc: 0.9423, val loss: 0.1749, val acc: 0.9599  (best train acc: 0.9473, best val acc: 0.9653)\n",
      "[Epoch: 24540] train loss: 0.1734, train acc: 0.9419, val loss: 0.1803, val acc: 0.9592  (best train acc: 0.9473, best val acc: 0.9653)\n",
      "[Epoch: 24560] train loss: 0.1822, train acc: 0.9395, val loss: 0.1980, val acc: 0.9481  (best train acc: 0.9473, best val acc: 0.9653)\n",
      "[Epoch: 24580] train loss: 0.1985, train acc: 0.9310, val loss: 0.1817, val acc: 0.9616  (best train acc: 0.9473, best val acc: 0.9653)\n",
      "[Epoch: 24600] train loss: 0.1898, train acc: 0.9354, val loss: 0.1793, val acc: 0.9609  (best train acc: 0.9473, best val acc: 0.9653)\n",
      "[Epoch: 24620] train loss: 0.1751, train acc: 0.9376, val loss: 0.1790, val acc: 0.9558  (best train acc: 0.9474, best val acc: 0.9653)\n",
      "[Epoch: 24640] train loss: 0.1849, train acc: 0.9356, val loss: 0.1766, val acc: 0.9575  (best train acc: 0.9474, best val acc: 0.9653)\n",
      "[Epoch: 24660] train loss: 0.1637, train acc: 0.9432, val loss: 0.1659, val acc: 0.9602  (best train acc: 0.9474, best val acc: 0.9653)\n",
      "[Epoch: 24680] train loss: 0.2191, train acc: 0.9167, val loss: 0.2129, val acc: 0.9484  (best train acc: 0.9474, best val acc: 0.9653)\n",
      "[Epoch: 24700] train loss: 0.2013, train acc: 0.9306, val loss: 0.1802, val acc: 0.9568  (best train acc: 0.9474, best val acc: 0.9653)\n",
      "[Epoch: 24720] train loss: 0.1764, train acc: 0.9397, val loss: 0.1700, val acc: 0.9585  (best train acc: 0.9474, best val acc: 0.9653)\n",
      "[Epoch: 24740] train loss: 0.1806, train acc: 0.9323, val loss: 0.1774, val acc: 0.9562  (best train acc: 0.9474, best val acc: 0.9653)\n",
      "[Epoch: 24760] train loss: 0.1878, train acc: 0.9320, val loss: 0.1732, val acc: 0.9568  (best train acc: 0.9474, best val acc: 0.9653)\n",
      "[Epoch: 24780] train loss: 0.1984, train acc: 0.9320, val loss: 0.1818, val acc: 0.9562  (best train acc: 0.9474, best val acc: 0.9653)\n",
      "[Epoch: 24800] train loss: 0.1816, train acc: 0.9411, val loss: 0.1591, val acc: 0.9602  (best train acc: 0.9474, best val acc: 0.9653)\n",
      "[Epoch: 24820] train loss: 0.1816, train acc: 0.9388, val loss: 0.1622, val acc: 0.9602  (best train acc: 0.9474, best val acc: 0.9653)\n",
      "[Epoch: 24840] train loss: 0.2020, train acc: 0.9265, val loss: 0.1802, val acc: 0.9528  (best train acc: 0.9474, best val acc: 0.9653)\n",
      "[Epoch: 24860] train loss: 0.2029, train acc: 0.9312, val loss: 0.1770, val acc: 0.9589  (best train acc: 0.9474, best val acc: 0.9653)\n",
      "[Epoch: 24880] train loss: 0.1904, train acc: 0.9367, val loss: 0.1803, val acc: 0.9609  (best train acc: 0.9474, best val acc: 0.9653)\n",
      "[Epoch: 24900] train loss: 0.1968, train acc: 0.9319, val loss: 0.1759, val acc: 0.9595  (best train acc: 0.9474, best val acc: 0.9653)\n",
      "[Epoch: 24920] train loss: 0.1715, train acc: 0.9406, val loss: 0.1834, val acc: 0.9612  (best train acc: 0.9477, best val acc: 0.9653)\n",
      "[Epoch: 24940] train loss: 0.2079, train acc: 0.9252, val loss: 0.1888, val acc: 0.9494  (best train acc: 0.9477, best val acc: 0.9653)\n",
      "[Epoch: 24960] train loss: 0.1945, train acc: 0.9305, val loss: 0.1892, val acc: 0.9504  (best train acc: 0.9477, best val acc: 0.9653)\n",
      "[Epoch: 24980] train loss: 0.1910, train acc: 0.9308, val loss: 0.1770, val acc: 0.9562  (best train acc: 0.9477, best val acc: 0.9653)\n",
      "[Epoch: 25000] train loss: 0.2324, train acc: 0.9224, val loss: 0.1697, val acc: 0.9585  (best train acc: 0.9477, best val acc: 0.9653)\n",
      "[Epoch: 25020] train loss: 0.2031, train acc: 0.9303, val loss: 0.1868, val acc: 0.9504  (best train acc: 0.9477, best val acc: 0.9653)\n",
      "[Epoch: 25040] train loss: 0.1816, train acc: 0.9345, val loss: 0.1801, val acc: 0.9518  (best train acc: 0.9477, best val acc: 0.9653)\n",
      "[Epoch: 25060] train loss: 0.1847, train acc: 0.9372, val loss: 0.1731, val acc: 0.9575  (best train acc: 0.9477, best val acc: 0.9653)\n",
      "[Epoch: 25080] train loss: 0.2192, train acc: 0.9265, val loss: 0.1831, val acc: 0.9551  (best train acc: 0.9477, best val acc: 0.9653)\n",
      "[Epoch: 25100] train loss: 0.1869, train acc: 0.9396, val loss: 0.1605, val acc: 0.9616  (best train acc: 0.9477, best val acc: 0.9653)\n",
      "[Epoch: 25120] train loss: 0.1717, train acc: 0.9430, val loss: 0.1888, val acc: 0.9575  (best train acc: 0.9477, best val acc: 0.9653)\n",
      "[Epoch: 25140] train loss: 0.1657, train acc: 0.9422, val loss: 0.1629, val acc: 0.9619  (best train acc: 0.9477, best val acc: 0.9653)\n",
      "[Epoch: 25160] train loss: 0.1629, train acc: 0.9471, val loss: 0.1576, val acc: 0.9602  (best train acc: 0.9477, best val acc: 0.9653)\n",
      "[Epoch: 25180] train loss: 0.1685, train acc: 0.9402, val loss: 0.1678, val acc: 0.9599  (best train acc: 0.9477, best val acc: 0.9653)\n",
      "[Epoch: 25200] train loss: 0.1871, train acc: 0.9321, val loss: 0.1762, val acc: 0.9589  (best train acc: 0.9477, best val acc: 0.9653)\n",
      "[Epoch: 25220] train loss: 0.1807, train acc: 0.9389, val loss: 0.1708, val acc: 0.9609  (best train acc: 0.9477, best val acc: 0.9653)\n",
      "[Epoch: 25240] train loss: 0.1788, train acc: 0.9367, val loss: 0.1759, val acc: 0.9602  (best train acc: 0.9477, best val acc: 0.9653)\n",
      "[Epoch: 25260] train loss: 0.1994, train acc: 0.9293, val loss: 0.1704, val acc: 0.9622  (best train acc: 0.9487, best val acc: 0.9653)\n",
      "[Epoch: 25280] train loss: 0.2052, train acc: 0.9260, val loss: 0.1989, val acc: 0.9400  (best train acc: 0.9487, best val acc: 0.9653)\n",
      "[Epoch: 25300] train loss: 0.1898, train acc: 0.9299, val loss: 0.1719, val acc: 0.9582  (best train acc: 0.9487, best val acc: 0.9653)\n",
      "[Epoch: 25320] train loss: 0.1992, train acc: 0.9319, val loss: 0.1630, val acc: 0.9599  (best train acc: 0.9487, best val acc: 0.9653)\n",
      "[Epoch: 25340] train loss: 0.1621, train acc: 0.9435, val loss: 0.1672, val acc: 0.9616  (best train acc: 0.9487, best val acc: 0.9653)\n",
      "[Epoch: 25360] train loss: 0.1854, train acc: 0.9365, val loss: 0.1736, val acc: 0.9555  (best train acc: 0.9487, best val acc: 0.9653)\n",
      "[Epoch: 25380] train loss: 0.1672, train acc: 0.9425, val loss: 0.1698, val acc: 0.9612  (best train acc: 0.9487, best val acc: 0.9653)\n",
      "[Epoch: 25400] train loss: 0.1788, train acc: 0.9376, val loss: 0.1827, val acc: 0.9545  (best train acc: 0.9487, best val acc: 0.9653)\n",
      "[Epoch: 25420] train loss: 0.2006, train acc: 0.9253, val loss: 0.1744, val acc: 0.9528  (best train acc: 0.9487, best val acc: 0.9653)\n",
      "[Epoch: 25440] train loss: 0.1981, train acc: 0.9315, val loss: 0.1700, val acc: 0.9609  (best train acc: 0.9487, best val acc: 0.9653)\n",
      "[Epoch: 25460] train loss: 0.1858, train acc: 0.9375, val loss: 0.1682, val acc: 0.9605  (best train acc: 0.9487, best val acc: 0.9653)\n",
      "[Epoch: 25480] train loss: 0.1696, train acc: 0.9422, val loss: 0.1590, val acc: 0.9612  (best train acc: 0.9487, best val acc: 0.9653)\n",
      "[Epoch: 25500] train loss: 0.1797, train acc: 0.9329, val loss: 0.1755, val acc: 0.9565  (best train acc: 0.9487, best val acc: 0.9653)\n",
      "[Epoch: 25520] train loss: 0.1690, train acc: 0.9445, val loss: 0.1988, val acc: 0.9518  (best train acc: 0.9487, best val acc: 0.9653)\n",
      "[Epoch: 25540] train loss: 0.2058, train acc: 0.9257, val loss: 0.1945, val acc: 0.9548  (best train acc: 0.9487, best val acc: 0.9653)\n",
      "[Epoch: 25560] train loss: 0.1703, train acc: 0.9412, val loss: 0.1708, val acc: 0.9585  (best train acc: 0.9487, best val acc: 0.9653)\n",
      "[Epoch: 25580] train loss: 0.1830, train acc: 0.9390, val loss: 0.1818, val acc: 0.9511  (best train acc: 0.9487, best val acc: 0.9653)\n",
      "[Epoch: 25600] train loss: 0.1687, train acc: 0.9415, val loss: 0.1712, val acc: 0.9629  (best train acc: 0.9498, best val acc: 0.9653)\n",
      "[Epoch: 25620] train loss: 0.1791, train acc: 0.9396, val loss: 0.1669, val acc: 0.9622  (best train acc: 0.9498, best val acc: 0.9653)\n",
      "[Epoch: 25640] train loss: 0.1689, train acc: 0.9431, val loss: 0.1961, val acc: 0.9508  (best train acc: 0.9498, best val acc: 0.9653)\n",
      "[Epoch: 25660] train loss: 0.1902, train acc: 0.9349, val loss: 0.1688, val acc: 0.9626  (best train acc: 0.9498, best val acc: 0.9653)\n",
      "[Epoch: 25680] train loss: 0.1860, train acc: 0.9380, val loss: 0.1757, val acc: 0.9619  (best train acc: 0.9498, best val acc: 0.9653)\n",
      "[Epoch: 25700] train loss: 0.1868, train acc: 0.9378, val loss: 0.1747, val acc: 0.9572  (best train acc: 0.9498, best val acc: 0.9653)\n",
      "[Epoch: 25720] train loss: 0.1685, train acc: 0.9418, val loss: 0.1755, val acc: 0.9599  (best train acc: 0.9498, best val acc: 0.9653)\n",
      "[Epoch: 25740] train loss: 0.1937, train acc: 0.9316, val loss: 0.1753, val acc: 0.9599  (best train acc: 0.9498, best val acc: 0.9653)\n",
      "[Epoch: 25760] train loss: 0.2138, train acc: 0.9252, val loss: 0.1936, val acc: 0.9504  (best train acc: 0.9498, best val acc: 0.9653)\n",
      "[Epoch: 25780] train loss: 0.2271, train acc: 0.9229, val loss: 0.1869, val acc: 0.9572  (best train acc: 0.9498, best val acc: 0.9653)\n",
      "[Epoch: 25800] train loss: 0.1721, train acc: 0.9413, val loss: 0.1836, val acc: 0.9535  (best train acc: 0.9498, best val acc: 0.9653)\n",
      "[Epoch: 25820] train loss: 0.1773, train acc: 0.9372, val loss: 0.1670, val acc: 0.9572  (best train acc: 0.9498, best val acc: 0.9653)\n",
      "[Epoch: 25840] train loss: 0.1804, train acc: 0.9357, val loss: 0.1739, val acc: 0.9612  (best train acc: 0.9498, best val acc: 0.9653)\n",
      "[Epoch: 25860] train loss: 0.1835, train acc: 0.9353, val loss: 0.1727, val acc: 0.9575  (best train acc: 0.9498, best val acc: 0.9653)\n",
      "[Epoch: 25880] train loss: 0.1836, train acc: 0.9344, val loss: 0.1696, val acc: 0.9575  (best train acc: 0.9498, best val acc: 0.9653)\n",
      "[Epoch: 25900] train loss: 0.1742, train acc: 0.9401, val loss: 0.1648, val acc: 0.9589  (best train acc: 0.9498, best val acc: 0.9653)\n",
      "[Epoch: 25920] train loss: 0.1812, train acc: 0.9339, val loss: 0.1585, val acc: 0.9605  (best train acc: 0.9498, best val acc: 0.9653)\n",
      "[Epoch: 25940] train loss: 0.2207, train acc: 0.9221, val loss: 0.1728, val acc: 0.9589  (best train acc: 0.9498, best val acc: 0.9653)\n",
      "[Epoch: 25960] train loss: 0.1996, train acc: 0.9344, val loss: 0.1719, val acc: 0.9531  (best train acc: 0.9498, best val acc: 0.9653)\n",
      "[Epoch: 25980] train loss: 0.2188, train acc: 0.9161, val loss: 0.1617, val acc: 0.9548  (best train acc: 0.9498, best val acc: 0.9653)\n",
      "[Epoch: 26000] train loss: 0.1741, train acc: 0.9394, val loss: 0.1638, val acc: 0.9558  (best train acc: 0.9498, best val acc: 0.9653)\n",
      "[Epoch: 26020] train loss: 0.1812, train acc: 0.9425, val loss: 0.1741, val acc: 0.9572  (best train acc: 0.9498, best val acc: 0.9653)\n",
      "[Epoch: 26040] train loss: 0.1666, train acc: 0.9430, val loss: 0.1641, val acc: 0.9612  (best train acc: 0.9498, best val acc: 0.9653)\n",
      "[Epoch: 26060] train loss: 0.2498, train acc: 0.9063, val loss: 0.1700, val acc: 0.9589  (best train acc: 0.9498, best val acc: 0.9653)\n",
      "[Epoch: 26080] train loss: 0.1930, train acc: 0.9318, val loss: 0.1590, val acc: 0.9592  (best train acc: 0.9498, best val acc: 0.9653)\n",
      "[Epoch: 26100] train loss: 0.2112, train acc: 0.9275, val loss: 0.1659, val acc: 0.9568  (best train acc: 0.9498, best val acc: 0.9653)\n",
      "[Epoch: 26120] train loss: 0.1744, train acc: 0.9421, val loss: 0.1550, val acc: 0.9605  (best train acc: 0.9498, best val acc: 0.9653)\n",
      "[Epoch: 26140] train loss: 0.2280, train acc: 0.9231, val loss: 0.1592, val acc: 0.9555  (best train acc: 0.9498, best val acc: 0.9653)\n",
      "[Epoch: 26160] train loss: 0.1803, train acc: 0.9354, val loss: 0.1546, val acc: 0.9578  (best train acc: 0.9498, best val acc: 0.9653)\n",
      "[Epoch: 26180] train loss: 0.1649, train acc: 0.9436, val loss: 0.1707, val acc: 0.9568  (best train acc: 0.9498, best val acc: 0.9653)\n",
      "[Epoch: 26200] train loss: 0.1842, train acc: 0.9322, val loss: 0.1588, val acc: 0.9605  (best train acc: 0.9498, best val acc: 0.9653)\n",
      "[Epoch: 26220] train loss: 0.1892, train acc: 0.9333, val loss: 0.1755, val acc: 0.9551  (best train acc: 0.9498, best val acc: 0.9653)\n",
      "[Epoch: 26240] train loss: 0.1712, train acc: 0.9375, val loss: 0.1731, val acc: 0.9578  (best train acc: 0.9498, best val acc: 0.9653)\n",
      "[Epoch: 26260] train loss: 0.1806, train acc: 0.9378, val loss: 0.1734, val acc: 0.9562  (best train acc: 0.9498, best val acc: 0.9653)\n",
      "[Epoch: 26280] train loss: 0.1825, train acc: 0.9396, val loss: 0.1731, val acc: 0.9575  (best train acc: 0.9498, best val acc: 0.9653)\n",
      "[Epoch: 26300] train loss: 0.1671, train acc: 0.9441, val loss: 0.1808, val acc: 0.9565  (best train acc: 0.9498, best val acc: 0.9653)\n",
      "[Epoch: 26320] train loss: 0.1693, train acc: 0.9428, val loss: 0.1756, val acc: 0.9578  (best train acc: 0.9498, best val acc: 0.9653)\n",
      "[Epoch: 26340] train loss: 0.1816, train acc: 0.9395, val loss: 0.1611, val acc: 0.9578  (best train acc: 0.9498, best val acc: 0.9653)\n",
      "[Epoch: 26360] train loss: 0.1790, train acc: 0.9378, val loss: 0.1544, val acc: 0.9582  (best train acc: 0.9498, best val acc: 0.9653)\n",
      "[Epoch: 26380] train loss: 0.1791, train acc: 0.9360, val loss: 0.1722, val acc: 0.9538  (best train acc: 0.9498, best val acc: 0.9653)\n",
      "[Epoch: 26400] train loss: 0.1967, train acc: 0.9275, val loss: 0.1715, val acc: 0.9514  (best train acc: 0.9498, best val acc: 0.9653)\n",
      "[Epoch: 26420] train loss: 0.1946, train acc: 0.9301, val loss: 0.1507, val acc: 0.9592  (best train acc: 0.9498, best val acc: 0.9653)\n",
      "[Epoch: 26440] train loss: 0.1809, train acc: 0.9389, val loss: 0.1689, val acc: 0.9487  (best train acc: 0.9498, best val acc: 0.9653)\n",
      "[Epoch: 26460] train loss: 0.1996, train acc: 0.9301, val loss: 0.1858, val acc: 0.9545  (best train acc: 0.9498, best val acc: 0.9653)\n",
      "[Epoch: 26480] train loss: 0.1608, train acc: 0.9430, val loss: 0.1762, val acc: 0.9578  (best train acc: 0.9498, best val acc: 0.9653)\n",
      "[Epoch: 26500] train loss: 0.2251, train acc: 0.9232, val loss: 0.1760, val acc: 0.9605  (best train acc: 0.9498, best val acc: 0.9653)\n",
      "[Epoch: 26520] train loss: 0.1844, train acc: 0.9365, val loss: 0.1809, val acc: 0.9541  (best train acc: 0.9498, best val acc: 0.9653)\n",
      "[Epoch: 26540] train loss: 0.1794, train acc: 0.9446, val loss: 0.1734, val acc: 0.9589  (best train acc: 0.9498, best val acc: 0.9653)\n",
      "[Epoch: 26560] train loss: 0.1736, train acc: 0.9432, val loss: 0.1834, val acc: 0.9541  (best train acc: 0.9498, best val acc: 0.9653)\n",
      "[Epoch: 26580] train loss: 0.2494, train acc: 0.9149, val loss: 0.1803, val acc: 0.9551  (best train acc: 0.9498, best val acc: 0.9653)\n",
      "[Epoch: 26600] train loss: 0.1717, train acc: 0.9451, val loss: 0.1611, val acc: 0.9578  (best train acc: 0.9498, best val acc: 0.9653)\n",
      "[Epoch: 26620] train loss: 0.1977, train acc: 0.9322, val loss: 0.1720, val acc: 0.9592  (best train acc: 0.9498, best val acc: 0.9653)\n",
      "[Epoch: 26640] train loss: 0.1711, train acc: 0.9406, val loss: 0.1683, val acc: 0.9595  (best train acc: 0.9498, best val acc: 0.9653)\n",
      "[Epoch: 26660] train loss: 0.1726, train acc: 0.9402, val loss: 0.1674, val acc: 0.9605  (best train acc: 0.9498, best val acc: 0.9653)\n",
      "[Epoch: 26680] train loss: 0.1682, train acc: 0.9396, val loss: 0.1853, val acc: 0.9504  (best train acc: 0.9498, best val acc: 0.9653)\n",
      "[Epoch: 26700] train loss: 0.1705, train acc: 0.9423, val loss: 0.1507, val acc: 0.9589  (best train acc: 0.9498, best val acc: 0.9653)\n",
      "[Epoch: 26720] train loss: 0.1718, train acc: 0.9447, val loss: 0.1583, val acc: 0.9589  (best train acc: 0.9498, best val acc: 0.9653)\n",
      "[Epoch: 26740] train loss: 0.1679, train acc: 0.9431, val loss: 0.1615, val acc: 0.9612  (best train acc: 0.9498, best val acc: 0.9653)\n",
      "[Epoch: 26760] train loss: 0.1695, train acc: 0.9410, val loss: 0.1686, val acc: 0.9589  (best train acc: 0.9498, best val acc: 0.9653)\n",
      "[Epoch: 26780] train loss: 0.1826, train acc: 0.9342, val loss: 0.1454, val acc: 0.9626  (best train acc: 0.9498, best val acc: 0.9653)\n",
      "[Epoch: 26800] train loss: 0.2200, train acc: 0.9190, val loss: 0.1532, val acc: 0.9612  (best train acc: 0.9498, best val acc: 0.9653)\n",
      "[Epoch: 26820] train loss: 0.1824, train acc: 0.9385, val loss: 0.1683, val acc: 0.9562  (best train acc: 0.9498, best val acc: 0.9653)\n",
      "[Epoch: 26840] train loss: 0.1751, train acc: 0.9351, val loss: 0.1616, val acc: 0.9555  (best train acc: 0.9498, best val acc: 0.9653)\n",
      "[Epoch: 26860] train loss: 0.1937, train acc: 0.9385, val loss: 0.1712, val acc: 0.9589  (best train acc: 0.9498, best val acc: 0.9653)\n",
      "[Epoch: 26880] train loss: 0.1738, train acc: 0.9425, val loss: 0.1816, val acc: 0.9545  (best train acc: 0.9498, best val acc: 0.9653)\n",
      "[Epoch: 26900] train loss: 0.1955, train acc: 0.9301, val loss: 0.1606, val acc: 0.9585  (best train acc: 0.9498, best val acc: 0.9653)\n",
      "[Epoch: 26920] train loss: 0.1866, train acc: 0.9336, val loss: 0.1857, val acc: 0.9497  (best train acc: 0.9498, best val acc: 0.9653)\n",
      "[Epoch: 26940] train loss: 0.1861, train acc: 0.9431, val loss: 0.1861, val acc: 0.9538  (best train acc: 0.9498, best val acc: 0.9653)\n",
      "[Epoch: 26960] train loss: 0.1993, train acc: 0.9300, val loss: 0.1738, val acc: 0.9585  (best train acc: 0.9498, best val acc: 0.9653)\n",
      "[Epoch: 26980] train loss: 0.1798, train acc: 0.9368, val loss: 0.1779, val acc: 0.9605  (best train acc: 0.9498, best val acc: 0.9653)\n",
      "[Epoch: 27000] train loss: 0.1966, train acc: 0.9276, val loss: 0.1992, val acc: 0.9464  (best train acc: 0.9498, best val acc: 0.9653)\n",
      "[Epoch: 27020] train loss: 0.1695, train acc: 0.9398, val loss: 0.1401, val acc: 0.9599  (best train acc: 0.9498, best val acc: 0.9653)\n",
      "[Epoch: 27040] train loss: 0.1875, train acc: 0.9325, val loss: 0.1683, val acc: 0.9572  (best train acc: 0.9498, best val acc: 0.9653)\n",
      "[Epoch: 27060] train loss: 0.1815, train acc: 0.9378, val loss: 0.1763, val acc: 0.9538  (best train acc: 0.9498, best val acc: 0.9653)\n",
      "[Epoch: 27080] train loss: 0.2084, train acc: 0.9289, val loss: 0.1692, val acc: 0.9568  (best train acc: 0.9498, best val acc: 0.9653)\n",
      "[Epoch: 27100] train loss: 0.1986, train acc: 0.9281, val loss: 0.1584, val acc: 0.9578  (best train acc: 0.9498, best val acc: 0.9653)\n",
      "[Epoch: 27120] train loss: 0.1860, train acc: 0.9371, val loss: 0.1489, val acc: 0.9602  (best train acc: 0.9498, best val acc: 0.9653)\n",
      "[Epoch: 27140] train loss: 0.2011, train acc: 0.9305, val loss: 0.1558, val acc: 0.9582  (best train acc: 0.9498, best val acc: 0.9653)\n",
      "[Epoch: 27160] train loss: 0.2318, train acc: 0.9145, val loss: 0.1722, val acc: 0.9578  (best train acc: 0.9498, best val acc: 0.9653)\n",
      "[Epoch: 27180] train loss: 0.1759, train acc: 0.9413, val loss: 0.1589, val acc: 0.9619  (best train acc: 0.9498, best val acc: 0.9653)\n",
      "[Epoch: 27200] train loss: 0.1848, train acc: 0.9362, val loss: 0.1647, val acc: 0.9535  (best train acc: 0.9498, best val acc: 0.9653)\n",
      "[Epoch: 27220] train loss: 0.1777, train acc: 0.9404, val loss: 0.1727, val acc: 0.9575  (best train acc: 0.9498, best val acc: 0.9653)\n",
      "[Epoch: 27240] train loss: 0.1780, train acc: 0.9371, val loss: 0.1621, val acc: 0.9612  (best train acc: 0.9498, best val acc: 0.9653)\n",
      "[Epoch: 27260] train loss: 0.1908, train acc: 0.9341, val loss: 0.1743, val acc: 0.9592  (best train acc: 0.9498, best val acc: 0.9653)\n",
      "[Epoch: 27280] train loss: 0.1662, train acc: 0.9411, val loss: 0.1765, val acc: 0.9602  (best train acc: 0.9498, best val acc: 0.9653)\n",
      "[Epoch: 27300] train loss: 0.1687, train acc: 0.9396, val loss: 0.1793, val acc: 0.9585  (best train acc: 0.9498, best val acc: 0.9653)\n",
      "[Epoch: 27320] train loss: 0.2040, train acc: 0.9245, val loss: 0.1931, val acc: 0.9514  (best train acc: 0.9498, best val acc: 0.9653)\n",
      "[Epoch: 27340] train loss: 0.1808, train acc: 0.9391, val loss: 0.1722, val acc: 0.9585  (best train acc: 0.9498, best val acc: 0.9653)\n",
      "[Epoch: 27360] train loss: 0.1609, train acc: 0.9447, val loss: 0.1858, val acc: 0.9558  (best train acc: 0.9498, best val acc: 0.9653)\n",
      "[Epoch: 27380] train loss: 0.2039, train acc: 0.9245, val loss: 0.2124, val acc: 0.9417  (best train acc: 0.9498, best val acc: 0.9653)\n",
      "[Epoch: 27400] train loss: 0.1902, train acc: 0.9336, val loss: 0.1656, val acc: 0.9605  (best train acc: 0.9498, best val acc: 0.9653)\n",
      "[Epoch: 27420] train loss: 0.1629, train acc: 0.9445, val loss: 0.1912, val acc: 0.9562  (best train acc: 0.9498, best val acc: 0.9653)\n",
      "[Epoch: 27440] train loss: 0.1844, train acc: 0.9361, val loss: 0.1667, val acc: 0.9609  (best train acc: 0.9498, best val acc: 0.9653)\n",
      "[Epoch: 27460] train loss: 0.1905, train acc: 0.9330, val loss: 0.1529, val acc: 0.9622  (best train acc: 0.9498, best val acc: 0.9653)\n",
      "[Epoch: 27480] train loss: 0.1739, train acc: 0.9374, val loss: 0.1667, val acc: 0.9592  (best train acc: 0.9498, best val acc: 0.9653)\n",
      "[Epoch: 27500] train loss: 0.1769, train acc: 0.9369, val loss: 0.1683, val acc: 0.9585  (best train acc: 0.9498, best val acc: 0.9653)\n",
      "[Epoch: 27520] train loss: 0.2473, train acc: 0.9087, val loss: 0.2312, val acc: 0.9346  (best train acc: 0.9498, best val acc: 0.9653)\n",
      "[Epoch: 27540] train loss: 0.1718, train acc: 0.9438, val loss: 0.1771, val acc: 0.9612  (best train acc: 0.9498, best val acc: 0.9653)\n",
      "[Epoch: 27560] train loss: 0.1962, train acc: 0.9375, val loss: 0.1700, val acc: 0.9595  (best train acc: 0.9498, best val acc: 0.9653)\n",
      "[Epoch: 27580] train loss: 0.1794, train acc: 0.9302, val loss: 0.1679, val acc: 0.9572  (best train acc: 0.9506, best val acc: 0.9653)\n",
      "[Epoch: 27600] train loss: 0.1647, train acc: 0.9460, val loss: 0.1653, val acc: 0.9609  (best train acc: 0.9506, best val acc: 0.9653)\n",
      "[Epoch: 27620] train loss: 0.1583, train acc: 0.9455, val loss: 0.1681, val acc: 0.9582  (best train acc: 0.9506, best val acc: 0.9653)\n",
      "[Epoch: 27640] train loss: 0.1943, train acc: 0.9343, val loss: 0.1770, val acc: 0.9599  (best train acc: 0.9506, best val acc: 0.9653)\n",
      "[Epoch: 27660] train loss: 0.1806, train acc: 0.9398, val loss: 0.1719, val acc: 0.9589  (best train acc: 0.9506, best val acc: 0.9653)\n",
      "[Epoch: 27680] train loss: 0.1777, train acc: 0.9373, val loss: 0.1743, val acc: 0.9599  (best train acc: 0.9506, best val acc: 0.9653)\n",
      "[Epoch: 27700] train loss: 0.1596, train acc: 0.9497, val loss: 0.1722, val acc: 0.9589  (best train acc: 0.9506, best val acc: 0.9653)\n",
      "[Epoch: 27720] train loss: 0.1712, train acc: 0.9446, val loss: 0.1782, val acc: 0.9589  (best train acc: 0.9506, best val acc: 0.9653)\n",
      "[Epoch: 27740] train loss: 0.1990, train acc: 0.9273, val loss: 0.1606, val acc: 0.9599  (best train acc: 0.9506, best val acc: 0.9653)\n",
      "[Epoch: 27760] train loss: 0.1811, train acc: 0.9355, val loss: 0.2028, val acc: 0.9467  (best train acc: 0.9506, best val acc: 0.9653)\n",
      "[Epoch: 27780] train loss: 0.1838, train acc: 0.9326, val loss: 0.1965, val acc: 0.9602  (best train acc: 0.9506, best val acc: 0.9653)\n",
      "[Epoch: 27800] train loss: 0.1941, train acc: 0.9305, val loss: 0.1853, val acc: 0.9572  (best train acc: 0.9506, best val acc: 0.9653)\n",
      "[Epoch: 27820] train loss: 0.2439, train acc: 0.9085, val loss: 0.1920, val acc: 0.9491  (best train acc: 0.9506, best val acc: 0.9653)\n",
      "[Epoch: 27840] train loss: 0.1903, train acc: 0.9384, val loss: 0.1956, val acc: 0.9565  (best train acc: 0.9506, best val acc: 0.9653)\n",
      "[Epoch: 27860] train loss: 0.1645, train acc: 0.9436, val loss: 0.1822, val acc: 0.9568  (best train acc: 0.9506, best val acc: 0.9653)\n",
      "[Epoch: 27880] train loss: 0.1796, train acc: 0.9346, val loss: 0.1813, val acc: 0.9592  (best train acc: 0.9506, best val acc: 0.9653)\n",
      "[Epoch: 27900] train loss: 0.1694, train acc: 0.9430, val loss: 0.1722, val acc: 0.9558  (best train acc: 0.9506, best val acc: 0.9653)\n",
      "[Epoch: 27920] train loss: 0.1708, train acc: 0.9409, val loss: 0.1660, val acc: 0.9622  (best train acc: 0.9506, best val acc: 0.9653)\n",
      "[Epoch: 27940] train loss: 0.1713, train acc: 0.9413, val loss: 0.1772, val acc: 0.9585  (best train acc: 0.9506, best val acc: 0.9653)\n",
      "[Epoch: 27960] train loss: 0.1717, train acc: 0.9398, val loss: 0.1568, val acc: 0.9595  (best train acc: 0.9506, best val acc: 0.9653)\n",
      "[Epoch: 27980] train loss: 0.1864, train acc: 0.9325, val loss: 0.1683, val acc: 0.9636  (best train acc: 0.9506, best val acc: 0.9653)\n",
      "[Epoch: 28000] train loss: 0.1871, train acc: 0.9364, val loss: 0.1722, val acc: 0.9575  (best train acc: 0.9506, best val acc: 0.9653)\n",
      "[Epoch: 28020] train loss: 0.1913, train acc: 0.9315, val loss: 0.1945, val acc: 0.9538  (best train acc: 0.9506, best val acc: 0.9653)\n",
      "[Epoch: 28040] train loss: 0.1778, train acc: 0.9348, val loss: 0.1618, val acc: 0.9642  (best train acc: 0.9506, best val acc: 0.9653)\n",
      "[Epoch: 28060] train loss: 0.1768, train acc: 0.9402, val loss: 0.1828, val acc: 0.9572  (best train acc: 0.9506, best val acc: 0.9653)\n",
      "[Epoch: 28080] train loss: 0.1920, train acc: 0.9312, val loss: 0.1676, val acc: 0.9629  (best train acc: 0.9506, best val acc: 0.9653)\n",
      "[Epoch: 28100] train loss: 0.1744, train acc: 0.9406, val loss: 0.1738, val acc: 0.9626  (best train acc: 0.9506, best val acc: 0.9653)\n",
      "[Epoch: 28120] train loss: 0.1591, train acc: 0.9456, val loss: 0.1759, val acc: 0.9589  (best train acc: 0.9506, best val acc: 0.9653)\n",
      "[Epoch: 28140] train loss: 0.1803, train acc: 0.9369, val loss: 0.1662, val acc: 0.9616  (best train acc: 0.9506, best val acc: 0.9653)\n",
      "[Epoch: 28160] train loss: 0.1686, train acc: 0.9419, val loss: 0.1764, val acc: 0.9599  (best train acc: 0.9506, best val acc: 0.9653)\n",
      "[Epoch: 28180] train loss: 0.1941, train acc: 0.9297, val loss: 0.1732, val acc: 0.9599  (best train acc: 0.9506, best val acc: 0.9653)\n",
      "[Epoch: 28200] train loss: 0.1999, train acc: 0.9252, val loss: 0.1762, val acc: 0.9592  (best train acc: 0.9506, best val acc: 0.9653)\n",
      "[Epoch: 28220] train loss: 0.1696, train acc: 0.9471, val loss: 0.1886, val acc: 0.9538  (best train acc: 0.9506, best val acc: 0.9653)\n",
      "[Epoch: 28240] train loss: 0.1677, train acc: 0.9472, val loss: 0.1810, val acc: 0.9602  (best train acc: 0.9506, best val acc: 0.9653)\n",
      "[Epoch: 28260] train loss: 0.1581, train acc: 0.9470, val loss: 0.1866, val acc: 0.9595  (best train acc: 0.9506, best val acc: 0.9653)\n",
      "[Epoch: 28280] train loss: 0.1756, train acc: 0.9424, val loss: 0.1858, val acc: 0.9595  (best train acc: 0.9506, best val acc: 0.9653)\n",
      "[Epoch: 28300] train loss: 0.1818, train acc: 0.9383, val loss: 0.1775, val acc: 0.9582  (best train acc: 0.9506, best val acc: 0.9653)\n",
      "[Epoch: 28320] train loss: 0.1813, train acc: 0.9367, val loss: 0.1919, val acc: 0.9548  (best train acc: 0.9506, best val acc: 0.9653)\n",
      "[Epoch: 28340] train loss: 0.1630, train acc: 0.9485, val loss: 0.1701, val acc: 0.9551  (best train acc: 0.9506, best val acc: 0.9653)\n",
      "[Epoch: 28360] train loss: 0.1753, train acc: 0.9377, val loss: 0.1767, val acc: 0.9578  (best train acc: 0.9506, best val acc: 0.9653)\n",
      "[Epoch: 28380] train loss: 0.1737, train acc: 0.9352, val loss: 0.1607, val acc: 0.9612  (best train acc: 0.9506, best val acc: 0.9653)\n",
      "[Epoch: 28400] train loss: 0.1891, train acc: 0.9328, val loss: 0.1733, val acc: 0.9551  (best train acc: 0.9506, best val acc: 0.9653)\n",
      "[Epoch: 28420] train loss: 0.1785, train acc: 0.9372, val loss: 0.1733, val acc: 0.9585  (best train acc: 0.9506, best val acc: 0.9653)\n",
      "[Epoch: 28440] train loss: 0.2475, train acc: 0.9187, val loss: 0.1936, val acc: 0.9531  (best train acc: 0.9506, best val acc: 0.9653)\n",
      "[Epoch: 28460] train loss: 0.1671, train acc: 0.9380, val loss: 0.1661, val acc: 0.9616  (best train acc: 0.9506, best val acc: 0.9653)\n",
      "[Epoch: 28480] train loss: 0.1857, train acc: 0.9335, val loss: 0.1857, val acc: 0.9558  (best train acc: 0.9506, best val acc: 0.9653)\n",
      "[Epoch: 28500] train loss: 0.2475, train acc: 0.9161, val loss: 0.1715, val acc: 0.9555  (best train acc: 0.9506, best val acc: 0.9653)\n",
      "[Epoch: 28520] train loss: 0.2015, train acc: 0.9252, val loss: 0.1786, val acc: 0.9595  (best train acc: 0.9506, best val acc: 0.9653)\n",
      "[Epoch: 28540] train loss: 0.1818, train acc: 0.9366, val loss: 0.1610, val acc: 0.9619  (best train acc: 0.9506, best val acc: 0.9653)\n",
      "[Epoch: 28560] train loss: 0.2042, train acc: 0.9302, val loss: 0.1823, val acc: 0.9585  (best train acc: 0.9506, best val acc: 0.9653)\n",
      "[Epoch: 28580] train loss: 0.1953, train acc: 0.9336, val loss: 0.1704, val acc: 0.9578  (best train acc: 0.9506, best val acc: 0.9653)\n",
      "[Epoch: 28600] train loss: 0.1723, train acc: 0.9419, val loss: 0.1781, val acc: 0.9589  (best train acc: 0.9506, best val acc: 0.9653)\n",
      "[Epoch: 28620] train loss: 0.2115, train acc: 0.9200, val loss: 0.1740, val acc: 0.9599  (best train acc: 0.9506, best val acc: 0.9653)\n",
      "[Epoch: 28640] train loss: 0.2205, train acc: 0.9192, val loss: 0.1854, val acc: 0.9599  (best train acc: 0.9506, best val acc: 0.9653)\n",
      "[Epoch: 28660] train loss: 0.1873, train acc: 0.9305, val loss: 0.1677, val acc: 0.9609  (best train acc: 0.9506, best val acc: 0.9653)\n",
      "[Epoch: 28680] train loss: 0.1925, train acc: 0.9279, val loss: 0.1769, val acc: 0.9578  (best train acc: 0.9506, best val acc: 0.9653)\n",
      "[Epoch: 28700] train loss: 0.1940, train acc: 0.9328, val loss: 0.1902, val acc: 0.9562  (best train acc: 0.9506, best val acc: 0.9653)\n",
      "[Epoch: 28720] train loss: 0.2114, train acc: 0.9330, val loss: 0.1667, val acc: 0.9602  (best train acc: 0.9506, best val acc: 0.9653)\n",
      "[Epoch: 28740] train loss: 0.1683, train acc: 0.9400, val loss: 0.1664, val acc: 0.9619  (best train acc: 0.9506, best val acc: 0.9653)\n",
      "[Epoch: 28760] train loss: 0.1676, train acc: 0.9396, val loss: 0.2058, val acc: 0.9454  (best train acc: 0.9506, best val acc: 0.9653)\n",
      "[Epoch: 28780] train loss: 0.1864, train acc: 0.9400, val loss: 0.1591, val acc: 0.9568  (best train acc: 0.9506, best val acc: 0.9653)\n",
      "[Epoch: 28800] train loss: 0.1625, train acc: 0.9425, val loss: 0.1752, val acc: 0.9629  (best train acc: 0.9506, best val acc: 0.9653)\n",
      "[Epoch: 28820] train loss: 0.1531, train acc: 0.9488, val loss: 0.1781, val acc: 0.9595  (best train acc: 0.9506, best val acc: 0.9653)\n",
      "[Epoch: 28840] train loss: 0.1870, train acc: 0.9343, val loss: 0.1776, val acc: 0.9585  (best train acc: 0.9506, best val acc: 0.9653)\n",
      "[Epoch: 28860] train loss: 0.1717, train acc: 0.9381, val loss: 0.1940, val acc: 0.9538  (best train acc: 0.9506, best val acc: 0.9653)\n",
      "[Epoch: 28880] train loss: 0.1752, train acc: 0.9356, val loss: 0.1636, val acc: 0.9605  (best train acc: 0.9506, best val acc: 0.9653)\n",
      "[Epoch: 28900] train loss: 0.1974, train acc: 0.9292, val loss: 0.1664, val acc: 0.9599  (best train acc: 0.9506, best val acc: 0.9653)\n",
      "[Epoch: 28920] train loss: 0.1906, train acc: 0.9339, val loss: 0.1798, val acc: 0.9599  (best train acc: 0.9506, best val acc: 0.9653)\n",
      "[Epoch: 28940] train loss: 0.1935, train acc: 0.9346, val loss: 0.1701, val acc: 0.9605  (best train acc: 0.9506, best val acc: 0.9653)\n",
      "[Epoch: 28960] train loss: 0.1825, train acc: 0.9425, val loss: 0.1790, val acc: 0.9578  (best train acc: 0.9506, best val acc: 0.9653)\n",
      "[Epoch: 28980] train loss: 0.1797, train acc: 0.9353, val loss: 0.1880, val acc: 0.9585  (best train acc: 0.9506, best val acc: 0.9653)\n",
      "[Epoch: 29000] train loss: 0.1838, train acc: 0.9352, val loss: 0.1780, val acc: 0.9609  (best train acc: 0.9506, best val acc: 0.9653)\n",
      "[Epoch: 29020] train loss: 0.2007, train acc: 0.9338, val loss: 0.1769, val acc: 0.9585  (best train acc: 0.9506, best val acc: 0.9653)\n",
      "[Epoch: 29040] train loss: 0.1717, train acc: 0.9371, val loss: 0.1641, val acc: 0.9626  (best train acc: 0.9506, best val acc: 0.9653)\n",
      "[Epoch: 29060] train loss: 0.1595, train acc: 0.9482, val loss: 0.1568, val acc: 0.9612  (best train acc: 0.9506, best val acc: 0.9653)\n",
      "[Epoch: 29080] train loss: 0.1771, train acc: 0.9424, val loss: 0.1951, val acc: 0.9555  (best train acc: 0.9506, best val acc: 0.9653)\n",
      "[Epoch: 29100] train loss: 0.1732, train acc: 0.9411, val loss: 0.1901, val acc: 0.9589  (best train acc: 0.9506, best val acc: 0.9653)\n",
      "[Epoch: 29120] train loss: 0.2049, train acc: 0.9243, val loss: 0.1637, val acc: 0.9578  (best train acc: 0.9506, best val acc: 0.9653)\n",
      "[Epoch: 29140] train loss: 0.1906, train acc: 0.9316, val loss: 0.1632, val acc: 0.9636  (best train acc: 0.9506, best val acc: 0.9653)\n",
      "[Epoch: 29160] train loss: 0.1945, train acc: 0.9273, val loss: 0.1673, val acc: 0.9545  (best train acc: 0.9506, best val acc: 0.9653)\n",
      "[Epoch: 29180] train loss: 0.1763, train acc: 0.9381, val loss: 0.1447, val acc: 0.9602  (best train acc: 0.9506, best val acc: 0.9653)\n",
      "[Epoch: 29200] train loss: 0.1742, train acc: 0.9403, val loss: 0.1570, val acc: 0.9602  (best train acc: 0.9506, best val acc: 0.9653)\n",
      "[Epoch: 29220] train loss: 0.1712, train acc: 0.9365, val loss: 0.1674, val acc: 0.9565  (best train acc: 0.9506, best val acc: 0.9653)\n",
      "[Epoch: 29240] train loss: 0.1996, train acc: 0.9315, val loss: 0.1809, val acc: 0.9582  (best train acc: 0.9506, best val acc: 0.9653)\n",
      "[Epoch: 29260] train loss: 0.2273, train acc: 0.9239, val loss: 0.1942, val acc: 0.9562  (best train acc: 0.9506, best val acc: 0.9653)\n",
      "[Epoch: 29280] train loss: 0.1684, train acc: 0.9435, val loss: 0.1659, val acc: 0.9609  (best train acc: 0.9506, best val acc: 0.9653)\n",
      "[Epoch: 29300] train loss: 0.1835, train acc: 0.9385, val loss: 0.1597, val acc: 0.9565  (best train acc: 0.9506, best val acc: 0.9653)\n",
      "[Epoch: 29320] train loss: 0.1796, train acc: 0.9398, val loss: 0.2064, val acc: 0.9548  (best train acc: 0.9506, best val acc: 0.9653)\n",
      "[Epoch: 29340] train loss: 0.1693, train acc: 0.9436, val loss: 0.1553, val acc: 0.9612  (best train acc: 0.9506, best val acc: 0.9653)\n",
      "[Epoch: 29360] train loss: 0.1750, train acc: 0.9461, val loss: 0.1837, val acc: 0.9551  (best train acc: 0.9506, best val acc: 0.9653)\n",
      "[Epoch: 29380] train loss: 0.1626, train acc: 0.9433, val loss: 0.1615, val acc: 0.9589  (best train acc: 0.9506, best val acc: 0.9653)\n",
      "[Epoch: 29400] train loss: 0.1694, train acc: 0.9398, val loss: 0.1848, val acc: 0.9565  (best train acc: 0.9506, best val acc: 0.9653)\n",
      "[Epoch: 29420] train loss: 0.1981, train acc: 0.9340, val loss: 0.1990, val acc: 0.9508  (best train acc: 0.9506, best val acc: 0.9653)\n",
      "[Epoch: 29440] train loss: 0.1788, train acc: 0.9409, val loss: 0.1763, val acc: 0.9541  (best train acc: 0.9506, best val acc: 0.9653)\n",
      "[Epoch: 29460] train loss: 0.1843, train acc: 0.9345, val loss: 0.1884, val acc: 0.9501  (best train acc: 0.9506, best val acc: 0.9653)\n",
      "[Epoch: 29480] train loss: 0.1782, train acc: 0.9383, val loss: 0.1510, val acc: 0.9639  (best train acc: 0.9506, best val acc: 0.9653)\n",
      "[Epoch: 29500] train loss: 0.1710, train acc: 0.9420, val loss: 0.1658, val acc: 0.9639  (best train acc: 0.9506, best val acc: 0.9653)\n",
      "[Epoch: 29520] train loss: 0.1797, train acc: 0.9346, val loss: 0.1697, val acc: 0.9575  (best train acc: 0.9506, best val acc: 0.9653)\n",
      "[Epoch: 29540] train loss: 0.1716, train acc: 0.9428, val loss: 0.1644, val acc: 0.9605  (best train acc: 0.9506, best val acc: 0.9653)\n",
      "[Epoch: 29560] train loss: 0.1633, train acc: 0.9470, val loss: 0.1696, val acc: 0.9602  (best train acc: 0.9506, best val acc: 0.9653)\n",
      "[Epoch: 29580] train loss: 0.1709, train acc: 0.9436, val loss: 0.1612, val acc: 0.9528  (best train acc: 0.9506, best val acc: 0.9653)\n",
      "[Epoch: 29600] train loss: 0.1616, train acc: 0.9461, val loss: 0.1810, val acc: 0.9521  (best train acc: 0.9507, best val acc: 0.9653)\n",
      "[Epoch: 29620] train loss: 0.1591, train acc: 0.9478, val loss: 0.1752, val acc: 0.9578  (best train acc: 0.9507, best val acc: 0.9653)\n",
      "[Epoch: 29640] train loss: 0.1789, train acc: 0.9399, val loss: 0.1594, val acc: 0.9589  (best train acc: 0.9507, best val acc: 0.9653)\n",
      "[Epoch: 29660] train loss: 0.1694, train acc: 0.9391, val loss: 0.1770, val acc: 0.9605  (best train acc: 0.9507, best val acc: 0.9653)\n",
      "[Epoch: 29680] train loss: 0.1654, train acc: 0.9432, val loss: 0.1651, val acc: 0.9622  (best train acc: 0.9507, best val acc: 0.9653)\n",
      "[Epoch: 29700] train loss: 0.1602, train acc: 0.9452, val loss: 0.1573, val acc: 0.9639  (best train acc: 0.9507, best val acc: 0.9653)\n",
      "[Epoch: 29720] train loss: 0.1824, train acc: 0.9419, val loss: 0.1618, val acc: 0.9592  (best train acc: 0.9507, best val acc: 0.9653)\n",
      "[Epoch: 29740] train loss: 0.1768, train acc: 0.9396, val loss: 0.1708, val acc: 0.9531  (best train acc: 0.9507, best val acc: 0.9653)\n",
      "[Epoch: 29760] train loss: 0.2114, train acc: 0.9303, val loss: 0.1647, val acc: 0.9626  (best train acc: 0.9507, best val acc: 0.9653)\n",
      "[Epoch: 29780] train loss: 0.1726, train acc: 0.9461, val loss: 0.1525, val acc: 0.9599  (best train acc: 0.9507, best val acc: 0.9653)\n",
      "[Epoch: 29800] train loss: 0.1816, train acc: 0.9373, val loss: 0.2035, val acc: 0.9508  (best train acc: 0.9507, best val acc: 0.9653)\n",
      "[Epoch: 29820] train loss: 0.1620, train acc: 0.9438, val loss: 0.1682, val acc: 0.9609  (best train acc: 0.9507, best val acc: 0.9653)\n",
      "[Epoch: 29840] train loss: 0.1532, train acc: 0.9486, val loss: 0.1482, val acc: 0.9555  (best train acc: 0.9507, best val acc: 0.9653)\n",
      "[Epoch: 29860] train loss: 0.2235, train acc: 0.9250, val loss: 0.1668, val acc: 0.9578  (best train acc: 0.9507, best val acc: 0.9653)\n",
      "[Epoch: 29880] train loss: 0.1670, train acc: 0.9414, val loss: 0.1592, val acc: 0.9568  (best train acc: 0.9507, best val acc: 0.9653)\n",
      "[Epoch: 29900] train loss: 0.1727, train acc: 0.9385, val loss: 0.1622, val acc: 0.9548  (best train acc: 0.9507, best val acc: 0.9653)\n",
      "[Epoch: 29920] train loss: 0.1622, train acc: 0.9406, val loss: 0.1450, val acc: 0.9595  (best train acc: 0.9507, best val acc: 0.9653)\n",
      "[Epoch: 29940] train loss: 0.1825, train acc: 0.9412, val loss: 0.1531, val acc: 0.9616  (best train acc: 0.9507, best val acc: 0.9653)\n",
      "[Epoch: 29960] train loss: 0.1781, train acc: 0.9388, val loss: 0.1759, val acc: 0.9585  (best train acc: 0.9507, best val acc: 0.9653)\n",
      "[Epoch: 29980] train loss: 0.1733, train acc: 0.9391, val loss: 0.1759, val acc: 0.9605  (best train acc: 0.9507, best val acc: 0.9653)\n",
      "[Epoch: 30000] train loss: 0.1843, train acc: 0.9375, val loss: 0.1645, val acc: 0.9582  (best train acc: 0.9507, best val acc: 0.9653)\n",
      "[Epoch: 30020] train loss: 0.2059, train acc: 0.9156, val loss: 0.1863, val acc: 0.9572  (best train acc: 0.9507, best val acc: 0.9653)\n",
      "[Epoch: 30040] train loss: 0.1787, train acc: 0.9354, val loss: 0.1712, val acc: 0.9626  (best train acc: 0.9507, best val acc: 0.9653)\n",
      "[Epoch: 30060] train loss: 0.1675, train acc: 0.9425, val loss: 0.1582, val acc: 0.9602  (best train acc: 0.9507, best val acc: 0.9653)\n",
      "[Epoch: 30080] train loss: 0.1639, train acc: 0.9464, val loss: 0.1664, val acc: 0.9568  (best train acc: 0.9507, best val acc: 0.9653)\n",
      "[Epoch: 30100] train loss: 0.1814, train acc: 0.9352, val loss: 0.1619, val acc: 0.9599  (best train acc: 0.9507, best val acc: 0.9653)\n",
      "[Epoch: 30120] train loss: 0.1676, train acc: 0.9430, val loss: 0.1505, val acc: 0.9548  (best train acc: 0.9507, best val acc: 0.9653)\n",
      "[Epoch: 30140] train loss: 0.1741, train acc: 0.9340, val loss: 0.1530, val acc: 0.9622  (best train acc: 0.9507, best val acc: 0.9653)\n",
      "[Epoch: 30160] train loss: 0.1775, train acc: 0.9326, val loss: 0.1582, val acc: 0.9582  (best train acc: 0.9507, best val acc: 0.9653)\n",
      "[Epoch: 30180] train loss: 0.1627, train acc: 0.9430, val loss: 0.1683, val acc: 0.9589  (best train acc: 0.9507, best val acc: 0.9653)\n",
      "[Epoch: 30200] train loss: 0.1763, train acc: 0.9354, val loss: 0.1468, val acc: 0.9626  (best train acc: 0.9507, best val acc: 0.9653)\n",
      "[Epoch: 30220] train loss: 0.2365, train acc: 0.9237, val loss: 0.1793, val acc: 0.9592  (best train acc: 0.9507, best val acc: 0.9653)\n",
      "[Epoch: 30240] train loss: 0.1693, train acc: 0.9432, val loss: 0.1718, val acc: 0.9595  (best train acc: 0.9507, best val acc: 0.9653)\n",
      "[Epoch: 30260] train loss: 0.1726, train acc: 0.9433, val loss: 0.1584, val acc: 0.9595  (best train acc: 0.9507, best val acc: 0.9653)\n",
      "[Epoch: 30280] train loss: 0.1665, train acc: 0.9419, val loss: 0.1539, val acc: 0.9629  (best train acc: 0.9507, best val acc: 0.9653)\n",
      "[Epoch: 30300] train loss: 0.1779, train acc: 0.9354, val loss: 0.1655, val acc: 0.9585  (best train acc: 0.9507, best val acc: 0.9653)\n",
      "[Epoch: 30320] train loss: 0.1673, train acc: 0.9418, val loss: 0.1578, val acc: 0.9592  (best train acc: 0.9507, best val acc: 0.9653)\n",
      "[Epoch: 30340] train loss: 0.1881, train acc: 0.9375, val loss: 0.1639, val acc: 0.9602  (best train acc: 0.9512, best val acc: 0.9653)\n",
      "[Epoch: 30360] train loss: 0.1711, train acc: 0.9444, val loss: 0.1530, val acc: 0.9592  (best train acc: 0.9512, best val acc: 0.9653)\n",
      "[Epoch: 30380] train loss: 0.1798, train acc: 0.9323, val loss: 0.1623, val acc: 0.9599  (best train acc: 0.9512, best val acc: 0.9653)\n",
      "[Epoch: 30400] train loss: 0.1633, train acc: 0.9423, val loss: 0.1618, val acc: 0.9565  (best train acc: 0.9512, best val acc: 0.9653)\n",
      "[Epoch: 30420] train loss: 0.1865, train acc: 0.9371, val loss: 0.1687, val acc: 0.9589  (best train acc: 0.9512, best val acc: 0.9653)\n",
      "[Epoch: 30440] train loss: 0.2192, train acc: 0.9279, val loss: 0.1722, val acc: 0.9572  (best train acc: 0.9512, best val acc: 0.9653)\n",
      "[Epoch: 30460] train loss: 0.1629, train acc: 0.9469, val loss: 0.1537, val acc: 0.9609  (best train acc: 0.9512, best val acc: 0.9653)\n",
      "[Epoch: 30480] train loss: 0.2045, train acc: 0.9310, val loss: 0.1583, val acc: 0.9619  (best train acc: 0.9523, best val acc: 0.9653)\n",
      "[Epoch: 30500] train loss: 0.1507, train acc: 0.9474, val loss: 0.1679, val acc: 0.9612  (best train acc: 0.9523, best val acc: 0.9653)\n",
      "[Epoch: 30520] train loss: 0.1684, train acc: 0.9424, val loss: 0.1411, val acc: 0.9622  (best train acc: 0.9523, best val acc: 0.9653)\n",
      "[Epoch: 30540] train loss: 0.1823, train acc: 0.9335, val loss: 0.1796, val acc: 0.9605  (best train acc: 0.9523, best val acc: 0.9653)\n",
      "[Epoch: 30560] train loss: 0.1660, train acc: 0.9414, val loss: 0.1623, val acc: 0.9619  (best train acc: 0.9523, best val acc: 0.9653)\n",
      "[Epoch: 30580] train loss: 0.1596, train acc: 0.9466, val loss: 0.1506, val acc: 0.9626  (best train acc: 0.9523, best val acc: 0.9653)\n",
      "[Epoch: 30600] train loss: 0.1707, train acc: 0.9390, val loss: 0.1632, val acc: 0.9605  (best train acc: 0.9523, best val acc: 0.9653)\n",
      "[Epoch: 30620] train loss: 0.1806, train acc: 0.9403, val loss: 0.1653, val acc: 0.9578  (best train acc: 0.9523, best val acc: 0.9653)\n",
      "[Epoch: 30640] train loss: 0.1889, train acc: 0.9381, val loss: 0.1787, val acc: 0.9524  (best train acc: 0.9523, best val acc: 0.9653)\n",
      "[Epoch: 30660] train loss: 0.1605, train acc: 0.9455, val loss: 0.1674, val acc: 0.9622  (best train acc: 0.9523, best val acc: 0.9653)\n",
      "[Epoch: 30680] train loss: 0.1705, train acc: 0.9406, val loss: 0.1583, val acc: 0.9589  (best train acc: 0.9523, best val acc: 0.9653)\n",
      "[Epoch: 30700] train loss: 0.1730, train acc: 0.9382, val loss: 0.1739, val acc: 0.9612  (best train acc: 0.9523, best val acc: 0.9653)\n",
      "[Epoch: 30720] train loss: 0.1661, train acc: 0.9435, val loss: 0.1622, val acc: 0.9629  (best train acc: 0.9523, best val acc: 0.9653)\n",
      "[Epoch: 30740] train loss: 0.1596, train acc: 0.9441, val loss: 0.1590, val acc: 0.9605  (best train acc: 0.9523, best val acc: 0.9653)\n",
      "[Epoch: 30760] train loss: 0.1789, train acc: 0.9342, val loss: 0.1604, val acc: 0.9602  (best train acc: 0.9523, best val acc: 0.9653)\n",
      "[Epoch: 30780] train loss: 0.2364, train acc: 0.9164, val loss: 0.1574, val acc: 0.9616  (best train acc: 0.9523, best val acc: 0.9653)\n",
      "[Epoch: 30800] train loss: 0.1717, train acc: 0.9412, val loss: 0.1529, val acc: 0.9622  (best train acc: 0.9523, best val acc: 0.9653)\n",
      "[Epoch: 30820] train loss: 0.1900, train acc: 0.9359, val loss: 0.1807, val acc: 0.9599  (best train acc: 0.9523, best val acc: 0.9653)\n",
      "[Epoch: 30840] train loss: 0.1934, train acc: 0.9307, val loss: 0.1616, val acc: 0.9626  (best train acc: 0.9523, best val acc: 0.9653)\n",
      "[Epoch: 30860] train loss: 0.1779, train acc: 0.9410, val loss: 0.1630, val acc: 0.9595  (best train acc: 0.9523, best val acc: 0.9653)\n",
      "[Epoch: 30880] train loss: 0.1784, train acc: 0.9371, val loss: 0.1848, val acc: 0.9568  (best train acc: 0.9523, best val acc: 0.9653)\n",
      "[Epoch: 30900] train loss: 0.1998, train acc: 0.9287, val loss: 0.1817, val acc: 0.9612  (best train acc: 0.9523, best val acc: 0.9653)\n",
      "[Epoch: 30920] train loss: 0.2722, train acc: 0.9140, val loss: 0.1632, val acc: 0.9602  (best train acc: 0.9523, best val acc: 0.9653)\n",
      "[Epoch: 30940] train loss: 0.1718, train acc: 0.9451, val loss: 0.1653, val acc: 0.9578  (best train acc: 0.9523, best val acc: 0.9653)\n",
      "[Epoch: 30960] train loss: 0.2157, train acc: 0.9208, val loss: 0.1762, val acc: 0.9558  (best train acc: 0.9523, best val acc: 0.9653)\n",
      "[Epoch: 30980] train loss: 0.1623, train acc: 0.9456, val loss: 0.1810, val acc: 0.9528  (best train acc: 0.9523, best val acc: 0.9653)\n",
      "[Epoch: 31000] train loss: 0.1632, train acc: 0.9389, val loss: 0.1487, val acc: 0.9595  (best train acc: 0.9523, best val acc: 0.9653)\n",
      "[Epoch: 31020] train loss: 0.1602, train acc: 0.9464, val loss: 0.1637, val acc: 0.9605  (best train acc: 0.9523, best val acc: 0.9653)\n",
      "[Epoch: 31040] train loss: 0.1666, train acc: 0.9426, val loss: 0.1636, val acc: 0.9622  (best train acc: 0.9523, best val acc: 0.9653)\n",
      "[Epoch: 31060] train loss: 0.1675, train acc: 0.9453, val loss: 0.1545, val acc: 0.9582  (best train acc: 0.9523, best val acc: 0.9653)\n",
      "[Epoch: 31080] train loss: 0.2292, train acc: 0.9145, val loss: 0.1836, val acc: 0.9538  (best train acc: 0.9523, best val acc: 0.9653)\n",
      "[Epoch: 31100] train loss: 0.1597, train acc: 0.9437, val loss: 0.1467, val acc: 0.9619  (best train acc: 0.9523, best val acc: 0.9653)\n",
      "[Epoch: 31120] train loss: 0.1673, train acc: 0.9412, val loss: 0.1668, val acc: 0.9595  (best train acc: 0.9523, best val acc: 0.9653)\n",
      "[Epoch: 31140] train loss: 0.1692, train acc: 0.9448, val loss: 0.1695, val acc: 0.9589  (best train acc: 0.9523, best val acc: 0.9653)\n",
      "[Epoch: 31160] train loss: 0.1817, train acc: 0.9395, val loss: 0.1969, val acc: 0.9514  (best train acc: 0.9523, best val acc: 0.9653)\n",
      "[Epoch: 31180] train loss: 0.1905, train acc: 0.9371, val loss: 0.1666, val acc: 0.9582  (best train acc: 0.9523, best val acc: 0.9653)\n",
      "[Epoch: 31200] train loss: 0.1660, train acc: 0.9373, val loss: 0.1752, val acc: 0.9626  (best train acc: 0.9523, best val acc: 0.9653)\n",
      "[Epoch: 31220] train loss: 0.1631, train acc: 0.9468, val loss: 0.1690, val acc: 0.9612  (best train acc: 0.9523, best val acc: 0.9653)\n",
      "[Epoch: 31240] train loss: 0.1777, train acc: 0.9359, val loss: 0.1721, val acc: 0.9602  (best train acc: 0.9523, best val acc: 0.9653)\n",
      "[Epoch: 31260] train loss: 0.1561, train acc: 0.9449, val loss: 0.1772, val acc: 0.9572  (best train acc: 0.9523, best val acc: 0.9653)\n",
      "[Epoch: 31280] train loss: 0.1670, train acc: 0.9447, val loss: 0.1549, val acc: 0.9639  (best train acc: 0.9523, best val acc: 0.9653)\n",
      "[Epoch: 31300] train loss: 0.1588, train acc: 0.9484, val loss: 0.1684, val acc: 0.9622  (best train acc: 0.9523, best val acc: 0.9653)\n",
      "[Epoch: 31320] train loss: 0.1552, train acc: 0.9456, val loss: 0.1530, val acc: 0.9595  (best train acc: 0.9523, best val acc: 0.9653)\n",
      "[Epoch: 31340] train loss: 0.1620, train acc: 0.9435, val loss: 0.1662, val acc: 0.9622  (best train acc: 0.9523, best val acc: 0.9653)\n",
      "[Epoch: 31360] train loss: 0.1826, train acc: 0.9365, val loss: 0.1862, val acc: 0.9562  (best train acc: 0.9527, best val acc: 0.9653)\n",
      "[Epoch: 31380] train loss: 0.1759, train acc: 0.9361, val loss: 0.1767, val acc: 0.9599  (best train acc: 0.9527, best val acc: 0.9653)\n",
      "[Epoch: 31400] train loss: 0.1495, train acc: 0.9511, val loss: 0.1632, val acc: 0.9639  (best train acc: 0.9527, best val acc: 0.9653)\n",
      "[Epoch: 31420] train loss: 0.2193, train acc: 0.9213, val loss: 0.1952, val acc: 0.9514  (best train acc: 0.9527, best val acc: 0.9653)\n",
      "[Epoch: 31440] train loss: 0.1617, train acc: 0.9435, val loss: 0.1712, val acc: 0.9605  (best train acc: 0.9527, best val acc: 0.9653)\n",
      "[Epoch: 31460] train loss: 0.1736, train acc: 0.9414, val loss: 0.1713, val acc: 0.9568  (best train acc: 0.9527, best val acc: 0.9653)\n",
      "[Epoch: 31480] train loss: 0.1652, train acc: 0.9464, val loss: 0.1739, val acc: 0.9602  (best train acc: 0.9527, best val acc: 0.9653)\n",
      "[Epoch: 31500] train loss: 0.1803, train acc: 0.9384, val loss: 0.1738, val acc: 0.9518  (best train acc: 0.9527, best val acc: 0.9653)\n",
      "[Epoch: 31520] train loss: 0.1794, train acc: 0.9373, val loss: 0.1647, val acc: 0.9622  (best train acc: 0.9527, best val acc: 0.9653)\n",
      "[Epoch: 31540] train loss: 0.1853, train acc: 0.9327, val loss: 0.1644, val acc: 0.9572  (best train acc: 0.9527, best val acc: 0.9653)\n",
      "[Epoch: 31560] train loss: 0.1570, train acc: 0.9459, val loss: 0.1820, val acc: 0.9568  (best train acc: 0.9527, best val acc: 0.9653)\n",
      "[Epoch: 31580] train loss: 0.1749, train acc: 0.9451, val loss: 0.1499, val acc: 0.9612  (best train acc: 0.9527, best val acc: 0.9653)\n",
      "[Epoch: 31600] train loss: 0.1812, train acc: 0.9368, val loss: 0.1512, val acc: 0.9616  (best train acc: 0.9527, best val acc: 0.9653)\n",
      "[Epoch: 31620] train loss: 0.1963, train acc: 0.9335, val loss: 0.1756, val acc: 0.9585  (best train acc: 0.9527, best val acc: 0.9653)\n",
      "[Epoch: 31640] train loss: 0.1682, train acc: 0.9418, val loss: 0.1771, val acc: 0.9568  (best train acc: 0.9527, best val acc: 0.9653)\n",
      "[Epoch: 31660] train loss: 0.1686, train acc: 0.9374, val loss: 0.1749, val acc: 0.9592  (best train acc: 0.9527, best val acc: 0.9653)\n",
      "[Epoch: 31680] train loss: 0.1639, train acc: 0.9432, val loss: 0.1607, val acc: 0.9605  (best train acc: 0.9527, best val acc: 0.9653)\n",
      "[Epoch: 31700] train loss: 0.1741, train acc: 0.9354, val loss: 0.1567, val acc: 0.9629  (best train acc: 0.9527, best val acc: 0.9653)\n",
      "[Epoch: 31720] train loss: 0.1782, train acc: 0.9358, val loss: 0.1659, val acc: 0.9565  (best train acc: 0.9527, best val acc: 0.9653)\n",
      "[Epoch: 31740] train loss: 0.1441, train acc: 0.9507, val loss: 0.1517, val acc: 0.9622  (best train acc: 0.9527, best val acc: 0.9653)\n",
      "[Epoch: 31760] train loss: 0.1774, train acc: 0.9401, val loss: 0.1907, val acc: 0.9497  (best train acc: 0.9527, best val acc: 0.9653)\n",
      "[Epoch: 31780] train loss: 0.1698, train acc: 0.9387, val loss: 0.1739, val acc: 0.9602  (best train acc: 0.9527, best val acc: 0.9653)\n",
      "[Epoch: 31800] train loss: 0.1540, train acc: 0.9523, val loss: 0.1751, val acc: 0.9616  (best train acc: 0.9527, best val acc: 0.9653)\n",
      "[Epoch: 31820] train loss: 0.1593, train acc: 0.9459, val loss: 0.1832, val acc: 0.9514  (best train acc: 0.9527, best val acc: 0.9653)\n",
      "[Epoch: 31840] train loss: 0.2215, train acc: 0.9262, val loss: 0.1579, val acc: 0.9622  (best train acc: 0.9527, best val acc: 0.9653)\n",
      "[Epoch: 31860] train loss: 0.2195, train acc: 0.9216, val loss: 0.1828, val acc: 0.9535  (best train acc: 0.9527, best val acc: 0.9653)\n",
      "[Epoch: 31880] train loss: 0.1784, train acc: 0.9349, val loss: 0.1637, val acc: 0.9568  (best train acc: 0.9527, best val acc: 0.9653)\n",
      "[Epoch: 31900] train loss: 0.1987, train acc: 0.9315, val loss: 0.1545, val acc: 0.9595  (best train acc: 0.9527, best val acc: 0.9653)\n",
      "[Epoch: 31920] train loss: 0.1761, train acc: 0.9423, val loss: 0.1546, val acc: 0.9595  (best train acc: 0.9527, best val acc: 0.9653)\n",
      "[Epoch: 31940] train loss: 0.1675, train acc: 0.9443, val loss: 0.1623, val acc: 0.9622  (best train acc: 0.9527, best val acc: 0.9653)\n",
      "[Epoch: 31960] train loss: 0.1642, train acc: 0.9427, val loss: 0.1728, val acc: 0.9609  (best train acc: 0.9527, best val acc: 0.9653)\n",
      "[Epoch: 31980] train loss: 0.1607, train acc: 0.9418, val loss: 0.1626, val acc: 0.9636  (best train acc: 0.9527, best val acc: 0.9653)\n",
      "[Epoch: 32000] train loss: 0.1893, train acc: 0.9378, val loss: 0.1759, val acc: 0.9605  (best train acc: 0.9527, best val acc: 0.9653)\n",
      "[Epoch: 32020] train loss: 0.1821, train acc: 0.9359, val loss: 0.1630, val acc: 0.9632  (best train acc: 0.9527, best val acc: 0.9653)\n",
      "[Epoch: 32040] train loss: 0.1638, train acc: 0.9435, val loss: 0.1606, val acc: 0.9592  (best train acc: 0.9527, best val acc: 0.9653)\n",
      "[Epoch: 32060] train loss: 0.1619, train acc: 0.9414, val loss: 0.1420, val acc: 0.9609  (best train acc: 0.9527, best val acc: 0.9653)\n",
      "[Epoch: 32080] train loss: 0.1814, train acc: 0.9395, val loss: 0.1870, val acc: 0.9616  (best train acc: 0.9527, best val acc: 0.9653)\n",
      "[Epoch: 32100] train loss: 0.1800, train acc: 0.9417, val loss: 0.1663, val acc: 0.9545  (best train acc: 0.9527, best val acc: 0.9653)\n",
      "[Epoch: 32120] train loss: 0.1742, train acc: 0.9400, val loss: 0.1574, val acc: 0.9629  (best train acc: 0.9527, best val acc: 0.9653)\n",
      "[Epoch: 32140] train loss: 0.1670, train acc: 0.9423, val loss: 0.1618, val acc: 0.9589  (best train acc: 0.9527, best val acc: 0.9653)\n",
      "[Epoch: 32160] train loss: 0.1638, train acc: 0.9446, val loss: 0.1597, val acc: 0.9612  (best train acc: 0.9527, best val acc: 0.9653)\n",
      "[Epoch: 32180] train loss: 0.1581, train acc: 0.9430, val loss: 0.1694, val acc: 0.9572  (best train acc: 0.9527, best val acc: 0.9653)\n",
      "[Epoch: 32200] train loss: 0.1641, train acc: 0.9411, val loss: 0.1606, val acc: 0.9626  (best train acc: 0.9527, best val acc: 0.9653)\n",
      "[Epoch: 32220] train loss: 0.1685, train acc: 0.9432, val loss: 0.1646, val acc: 0.9599  (best train acc: 0.9527, best val acc: 0.9653)\n",
      "[Epoch: 32240] train loss: 0.1699, train acc: 0.9417, val loss: 0.1716, val acc: 0.9619  (best train acc: 0.9527, best val acc: 0.9653)\n",
      "[Epoch: 32260] train loss: 0.1603, train acc: 0.9430, val loss: 0.1748, val acc: 0.9616  (best train acc: 0.9527, best val acc: 0.9653)\n",
      "[Epoch: 32280] train loss: 0.1534, train acc: 0.9496, val loss: 0.1664, val acc: 0.9629  (best train acc: 0.9527, best val acc: 0.9653)\n",
      "[Epoch: 32300] train loss: 0.2039, train acc: 0.9264, val loss: 0.1703, val acc: 0.9605  (best train acc: 0.9527, best val acc: 0.9653)\n",
      "[Epoch: 32320] train loss: 0.1630, train acc: 0.9454, val loss: 0.1666, val acc: 0.9636  (best train acc: 0.9530, best val acc: 0.9653)\n",
      "[Epoch: 32340] train loss: 0.1653, train acc: 0.9432, val loss: 0.1726, val acc: 0.9585  (best train acc: 0.9530, best val acc: 0.9653)\n",
      "[Epoch: 32360] train loss: 0.1635, train acc: 0.9436, val loss: 0.1706, val acc: 0.9555  (best train acc: 0.9530, best val acc: 0.9653)\n",
      "[Epoch: 32380] train loss: 0.2085, train acc: 0.9250, val loss: 0.1728, val acc: 0.9585  (best train acc: 0.9530, best val acc: 0.9653)\n",
      "[Epoch: 32400] train loss: 0.1833, train acc: 0.9345, val loss: 0.1640, val acc: 0.9632  (best train acc: 0.9530, best val acc: 0.9653)\n",
      "[Epoch: 32420] train loss: 0.2168, train acc: 0.9266, val loss: 0.1726, val acc: 0.9528  (best train acc: 0.9530, best val acc: 0.9653)\n",
      "[Epoch: 32440] train loss: 0.1753, train acc: 0.9408, val loss: 0.1637, val acc: 0.9622  (best train acc: 0.9530, best val acc: 0.9653)\n",
      "[Epoch: 32460] train loss: 0.1758, train acc: 0.9350, val loss: 0.1683, val acc: 0.9602  (best train acc: 0.9530, best val acc: 0.9653)\n",
      "[Epoch: 32480] train loss: 0.1478, train acc: 0.9491, val loss: 0.1808, val acc: 0.9541  (best train acc: 0.9530, best val acc: 0.9653)\n",
      "[Epoch: 32500] train loss: 0.1994, train acc: 0.9358, val loss: 0.1819, val acc: 0.9551  (best train acc: 0.9530, best val acc: 0.9653)\n",
      "[Epoch: 32520] train loss: 0.2022, train acc: 0.9289, val loss: 0.1902, val acc: 0.9545  (best train acc: 0.9530, best val acc: 0.9653)\n",
      "[Epoch: 32540] train loss: 0.1491, train acc: 0.9445, val loss: 0.1700, val acc: 0.9605  (best train acc: 0.9530, best val acc: 0.9653)\n",
      "[Epoch: 32560] train loss: 0.1644, train acc: 0.9448, val loss: 0.1572, val acc: 0.9605  (best train acc: 0.9530, best val acc: 0.9653)\n",
      "[Epoch: 32580] train loss: 0.1641, train acc: 0.9471, val loss: 0.1611, val acc: 0.9612  (best train acc: 0.9530, best val acc: 0.9653)\n",
      "[Epoch: 32600] train loss: 0.1576, train acc: 0.9445, val loss: 0.1518, val acc: 0.9599  (best train acc: 0.9530, best val acc: 0.9653)\n",
      "[Epoch: 32620] train loss: 0.1795, train acc: 0.9388, val loss: 0.1704, val acc: 0.9612  (best train acc: 0.9530, best val acc: 0.9653)\n",
      "[Epoch: 32640] train loss: 0.1746, train acc: 0.9387, val loss: 0.1883, val acc: 0.9575  (best train acc: 0.9530, best val acc: 0.9653)\n",
      "[Epoch: 32660] train loss: 0.1669, train acc: 0.9391, val loss: 0.1813, val acc: 0.9585  (best train acc: 0.9530, best val acc: 0.9653)\n",
      "[Epoch: 32680] train loss: 0.1769, train acc: 0.9448, val loss: 0.1601, val acc: 0.9616  (best train acc: 0.9530, best val acc: 0.9653)\n",
      "[Epoch: 32700] train loss: 0.1646, train acc: 0.9489, val loss: 0.1546, val acc: 0.9616  (best train acc: 0.9530, best val acc: 0.9653)\n",
      "[Epoch: 32720] train loss: 0.1751, train acc: 0.9353, val loss: 0.1672, val acc: 0.9619  (best train acc: 0.9530, best val acc: 0.9653)\n",
      "[Epoch: 32740] train loss: 0.1694, train acc: 0.9336, val loss: 0.1536, val acc: 0.9592  (best train acc: 0.9530, best val acc: 0.9653)\n",
      "[Epoch: 32760] train loss: 0.1886, train acc: 0.9298, val loss: 0.1521, val acc: 0.9616  (best train acc: 0.9530, best val acc: 0.9653)\n",
      "[Epoch: 32780] train loss: 0.2065, train acc: 0.9299, val loss: 0.1910, val acc: 0.9558  (best train acc: 0.9530, best val acc: 0.9653)\n",
      "[Epoch: 32800] train loss: 0.1574, train acc: 0.9452, val loss: 0.1622, val acc: 0.9599  (best train acc: 0.9530, best val acc: 0.9653)\n",
      "[Epoch: 32820] train loss: 0.1767, train acc: 0.9438, val loss: 0.1552, val acc: 0.9605  (best train acc: 0.9530, best val acc: 0.9653)\n",
      "[Epoch: 32840] train loss: 0.1742, train acc: 0.9405, val loss: 0.1763, val acc: 0.9572  (best train acc: 0.9530, best val acc: 0.9653)\n",
      "[Epoch: 32860] train loss: 0.2106, train acc: 0.9277, val loss: 0.1665, val acc: 0.9619  (best train acc: 0.9530, best val acc: 0.9653)\n",
      "[Epoch: 32880] train loss: 0.1833, train acc: 0.9301, val loss: 0.1636, val acc: 0.9609  (best train acc: 0.9530, best val acc: 0.9653)\n",
      "[Epoch: 32900] train loss: 0.1545, train acc: 0.9465, val loss: 0.1630, val acc: 0.9589  (best train acc: 0.9530, best val acc: 0.9653)\n",
      "[Epoch: 32920] train loss: 0.1663, train acc: 0.9456, val loss: 0.1577, val acc: 0.9626  (best train acc: 0.9530, best val acc: 0.9653)\n",
      "[Epoch: 32940] train loss: 0.2037, train acc: 0.9295, val loss: 0.1399, val acc: 0.9605  (best train acc: 0.9530, best val acc: 0.9653)\n",
      "[Epoch: 32960] train loss: 0.1633, train acc: 0.9481, val loss: 0.1661, val acc: 0.9619  (best train acc: 0.9530, best val acc: 0.9653)\n",
      "[Epoch: 32980] train loss: 0.1662, train acc: 0.9466, val loss: 0.1827, val acc: 0.9575  (best train acc: 0.9530, best val acc: 0.9653)\n",
      "[Epoch: 33000] train loss: 0.1741, train acc: 0.9398, val loss: 0.1537, val acc: 0.9619  (best train acc: 0.9530, best val acc: 0.9653)\n",
      "[Epoch: 33020] train loss: 0.1586, train acc: 0.9425, val loss: 0.1733, val acc: 0.9612  (best train acc: 0.9530, best val acc: 0.9653)\n",
      "[Epoch: 33040] train loss: 0.1702, train acc: 0.9402, val loss: 0.1633, val acc: 0.9605  (best train acc: 0.9530, best val acc: 0.9653)\n",
      "[Epoch: 33060] train loss: 0.1616, train acc: 0.9434, val loss: 0.1854, val acc: 0.9578  (best train acc: 0.9530, best val acc: 0.9653)\n",
      "[Epoch: 33080] train loss: 0.1723, train acc: 0.9434, val loss: 0.1624, val acc: 0.9609  (best train acc: 0.9530, best val acc: 0.9653)\n",
      "[Epoch: 33100] train loss: 0.1644, train acc: 0.9406, val loss: 0.1650, val acc: 0.9616  (best train acc: 0.9530, best val acc: 0.9653)\n",
      "[Epoch: 33120] train loss: 0.1589, train acc: 0.9469, val loss: 0.1653, val acc: 0.9595  (best train acc: 0.9530, best val acc: 0.9653)\n",
      "[Epoch: 33140] train loss: 0.1866, train acc: 0.9373, val loss: 0.1634, val acc: 0.9632  (best train acc: 0.9530, best val acc: 0.9653)\n",
      "[Epoch: 33160] train loss: 0.2025, train acc: 0.9286, val loss: 0.1756, val acc: 0.9616  (best train acc: 0.9530, best val acc: 0.9653)\n",
      "[Epoch: 33180] train loss: 0.1695, train acc: 0.9398, val loss: 0.1698, val acc: 0.9616  (best train acc: 0.9530, best val acc: 0.9653)\n",
      "[Epoch: 33200] train loss: 0.2512, train acc: 0.9169, val loss: 0.1847, val acc: 0.9605  (best train acc: 0.9530, best val acc: 0.9653)\n",
      "[Epoch: 33220] train loss: 0.1620, train acc: 0.9457, val loss: 0.1689, val acc: 0.9602  (best train acc: 0.9530, best val acc: 0.9653)\n",
      "[Epoch: 33240] train loss: 0.1549, train acc: 0.9461, val loss: 0.1758, val acc: 0.9599  (best train acc: 0.9530, best val acc: 0.9653)\n",
      "[Epoch: 33260] train loss: 0.1695, train acc: 0.9361, val loss: 0.1528, val acc: 0.9622  (best train acc: 0.9530, best val acc: 0.9653)\n",
      "[Epoch: 33280] train loss: 0.1651, train acc: 0.9451, val loss: 0.1566, val acc: 0.9599  (best train acc: 0.9530, best val acc: 0.9653)\n",
      "[Epoch: 33300] train loss: 0.1797, train acc: 0.9425, val loss: 0.1915, val acc: 0.9545  (best train acc: 0.9530, best val acc: 0.9653)\n",
      "[Epoch: 33320] train loss: 0.1781, train acc: 0.9378, val loss: 0.1522, val acc: 0.9595  (best train acc: 0.9530, best val acc: 0.9653)\n",
      "[Epoch: 33340] train loss: 0.2059, train acc: 0.9265, val loss: 0.1996, val acc: 0.9538  (best train acc: 0.9530, best val acc: 0.9653)\n",
      "[Epoch: 33360] train loss: 0.1445, train acc: 0.9502, val loss: 0.1531, val acc: 0.9619  (best train acc: 0.9530, best val acc: 0.9653)\n",
      "[Epoch: 33380] train loss: 0.1616, train acc: 0.9430, val loss: 0.1699, val acc: 0.9622  (best train acc: 0.9530, best val acc: 0.9653)\n",
      "[Epoch: 33400] train loss: 0.1763, train acc: 0.9430, val loss: 0.1724, val acc: 0.9582  (best train acc: 0.9530, best val acc: 0.9653)\n",
      "[Epoch: 33420] train loss: 0.2018, train acc: 0.9309, val loss: 0.1888, val acc: 0.9521  (best train acc: 0.9530, best val acc: 0.9653)\n",
      "[Epoch: 33440] train loss: 0.1573, train acc: 0.9477, val loss: 0.1872, val acc: 0.9555  (best train acc: 0.9530, best val acc: 0.9653)\n",
      "[Epoch: 33460] train loss: 0.1701, train acc: 0.9385, val loss: 0.1738, val acc: 0.9616  (best train acc: 0.9530, best val acc: 0.9653)\n",
      "[Epoch: 33480] train loss: 0.1803, train acc: 0.9383, val loss: 0.1873, val acc: 0.9589  (best train acc: 0.9530, best val acc: 0.9653)\n",
      "[Epoch: 33500] train loss: 0.1681, train acc: 0.9430, val loss: 0.1562, val acc: 0.9632  (best train acc: 0.9530, best val acc: 0.9653)\n",
      "[Epoch: 33520] train loss: 0.2011, train acc: 0.9315, val loss: 0.1572, val acc: 0.9592  (best train acc: 0.9530, best val acc: 0.9653)\n",
      "[Epoch: 33540] train loss: 0.2326, train acc: 0.9189, val loss: 0.1779, val acc: 0.9562  (best train acc: 0.9530, best val acc: 0.9653)\n",
      "[Epoch: 33560] train loss: 0.1877, train acc: 0.9338, val loss: 0.1531, val acc: 0.9612  (best train acc: 0.9530, best val acc: 0.9653)\n",
      "[Epoch: 33580] train loss: 0.1812, train acc: 0.9372, val loss: 0.1830, val acc: 0.9535  (best train acc: 0.9530, best val acc: 0.9653)\n",
      "[Epoch: 33600] train loss: 0.1732, train acc: 0.9361, val loss: 0.1663, val acc: 0.9602  (best train acc: 0.9530, best val acc: 0.9653)\n",
      "[Epoch: 33620] train loss: 0.1929, train acc: 0.9282, val loss: 0.2022, val acc: 0.9538  (best train acc: 0.9530, best val acc: 0.9653)\n",
      "[Epoch: 33640] train loss: 0.1720, train acc: 0.9459, val loss: 0.1960, val acc: 0.9528  (best train acc: 0.9530, best val acc: 0.9653)\n",
      "[Epoch: 33660] train loss: 0.1600, train acc: 0.9450, val loss: 0.1688, val acc: 0.9555  (best train acc: 0.9530, best val acc: 0.9653)\n",
      "[Epoch: 33680] train loss: 0.1872, train acc: 0.9354, val loss: 0.1612, val acc: 0.9619  (best train acc: 0.9530, best val acc: 0.9653)\n",
      "[Epoch: 33700] train loss: 0.1787, train acc: 0.9388, val loss: 0.1831, val acc: 0.9609  (best train acc: 0.9530, best val acc: 0.9653)\n",
      "[Epoch: 33720] train loss: 0.1657, train acc: 0.9422, val loss: 0.1708, val acc: 0.9616  (best train acc: 0.9530, best val acc: 0.9653)\n",
      "[Epoch: 33740] train loss: 0.1760, train acc: 0.9370, val loss: 0.1739, val acc: 0.9609  (best train acc: 0.9530, best val acc: 0.9653)\n",
      "[Epoch: 33760] train loss: 0.1474, train acc: 0.9491, val loss: 0.1526, val acc: 0.9626  (best train acc: 0.9530, best val acc: 0.9653)\n",
      "[Epoch: 33780] train loss: 0.1689, train acc: 0.9429, val loss: 0.1806, val acc: 0.9568  (best train acc: 0.9530, best val acc: 0.9653)\n",
      "[Epoch: 33800] train loss: 0.1445, train acc: 0.9506, val loss: 0.1630, val acc: 0.9642  (best train acc: 0.9538, best val acc: 0.9653)\n",
      "[Epoch: 33820] train loss: 0.1674, train acc: 0.9418, val loss: 0.1591, val acc: 0.9626  (best train acc: 0.9538, best val acc: 0.9653)\n",
      "[Epoch: 33840] train loss: 0.1898, train acc: 0.9276, val loss: 0.1730, val acc: 0.9599  (best train acc: 0.9538, best val acc: 0.9653)\n",
      "[Epoch: 33860] train loss: 0.2100, train acc: 0.9291, val loss: 0.1865, val acc: 0.9508  (best train acc: 0.9538, best val acc: 0.9653)\n",
      "[Epoch: 33880] train loss: 0.1772, train acc: 0.9383, val loss: 0.1829, val acc: 0.9555  (best train acc: 0.9538, best val acc: 0.9653)\n",
      "[Epoch: 33900] train loss: 0.1833, train acc: 0.9362, val loss: 0.1641, val acc: 0.9609  (best train acc: 0.9538, best val acc: 0.9653)\n",
      "[Epoch: 33920] train loss: 0.1596, train acc: 0.9470, val loss: 0.1455, val acc: 0.9605  (best train acc: 0.9538, best val acc: 0.9653)\n",
      "[Epoch: 33940] train loss: 0.1629, train acc: 0.9465, val loss: 0.1806, val acc: 0.9582  (best train acc: 0.9538, best val acc: 0.9653)\n",
      "[Epoch: 33960] train loss: 0.1842, train acc: 0.9357, val loss: 0.1585, val acc: 0.9599  (best train acc: 0.9538, best val acc: 0.9653)\n",
      "[Epoch: 33980] train loss: 0.1701, train acc: 0.9417, val loss: 0.1520, val acc: 0.9572  (best train acc: 0.9538, best val acc: 0.9653)\n",
      "[Epoch: 34000] train loss: 0.2116, train acc: 0.9260, val loss: 0.1566, val acc: 0.9605  (best train acc: 0.9538, best val acc: 0.9653)\n",
      "[Epoch: 34020] train loss: 0.1992, train acc: 0.9323, val loss: 0.1669, val acc: 0.9619  (best train acc: 0.9538, best val acc: 0.9653)\n",
      "[Epoch: 34040] train loss: 0.1587, train acc: 0.9447, val loss: 0.1536, val acc: 0.9626  (best train acc: 0.9538, best val acc: 0.9653)\n",
      "[Epoch: 34060] train loss: 0.1963, train acc: 0.9328, val loss: 0.1568, val acc: 0.9619  (best train acc: 0.9538, best val acc: 0.9653)\n",
      "[Epoch: 34080] train loss: 0.1614, train acc: 0.9446, val loss: 0.1660, val acc: 0.9612  (best train acc: 0.9538, best val acc: 0.9653)\n",
      "[Epoch: 34100] train loss: 0.1716, train acc: 0.9420, val loss: 0.1718, val acc: 0.9572  (best train acc: 0.9538, best val acc: 0.9653)\n",
      "[Epoch: 34120] train loss: 0.2058, train acc: 0.9371, val loss: 0.1774, val acc: 0.9575  (best train acc: 0.9538, best val acc: 0.9653)\n",
      "[Epoch: 34140] train loss: 0.1747, train acc: 0.9325, val loss: 0.1654, val acc: 0.9609  (best train acc: 0.9538, best val acc: 0.9653)\n",
      "[Epoch: 34160] train loss: 0.1608, train acc: 0.9450, val loss: 0.1627, val acc: 0.9578  (best train acc: 0.9538, best val acc: 0.9653)\n",
      "[Epoch: 34180] train loss: 0.1491, train acc: 0.9504, val loss: 0.1839, val acc: 0.9605  (best train acc: 0.9538, best val acc: 0.9653)\n",
      "[Epoch: 34200] train loss: 0.1600, train acc: 0.9440, val loss: 0.1833, val acc: 0.9572  (best train acc: 0.9538, best val acc: 0.9653)\n",
      "[Epoch: 34220] train loss: 0.1588, train acc: 0.9463, val loss: 0.1637, val acc: 0.9619  (best train acc: 0.9538, best val acc: 0.9653)\n",
      "[Epoch: 34240] train loss: 0.1561, train acc: 0.9521, val loss: 0.1622, val acc: 0.9619  (best train acc: 0.9538, best val acc: 0.9653)\n",
      "[Epoch: 34260] train loss: 0.1578, train acc: 0.9451, val loss: 0.1688, val acc: 0.9612  (best train acc: 0.9538, best val acc: 0.9653)\n",
      "[Epoch: 34280] train loss: 0.1932, train acc: 0.9366, val loss: 0.1816, val acc: 0.9589  (best train acc: 0.9538, best val acc: 0.9653)\n",
      "[Epoch: 34300] train loss: 0.1755, train acc: 0.9415, val loss: 0.1807, val acc: 0.9602  (best train acc: 0.9538, best val acc: 0.9653)\n",
      "[Epoch: 34320] train loss: 0.1586, train acc: 0.9455, val loss: 0.1608, val acc: 0.9619  (best train acc: 0.9538, best val acc: 0.9653)\n",
      "[Epoch: 34340] train loss: 0.2028, train acc: 0.9304, val loss: 0.1727, val acc: 0.9541  (best train acc: 0.9538, best val acc: 0.9653)\n",
      "[Epoch: 34360] train loss: 0.1534, train acc: 0.9490, val loss: 0.1568, val acc: 0.9609  (best train acc: 0.9538, best val acc: 0.9653)\n",
      "[Epoch: 34380] train loss: 0.1650, train acc: 0.9475, val loss: 0.1922, val acc: 0.9572  (best train acc: 0.9538, best val acc: 0.9653)\n",
      "[Epoch: 34400] train loss: 0.1849, train acc: 0.9305, val loss: 0.1859, val acc: 0.9605  (best train acc: 0.9538, best val acc: 0.9653)\n",
      "[Epoch: 34420] train loss: 0.1660, train acc: 0.9425, val loss: 0.1935, val acc: 0.9562  (best train acc: 0.9538, best val acc: 0.9653)\n",
      "[Epoch: 34440] train loss: 0.1685, train acc: 0.9422, val loss: 0.1661, val acc: 0.9605  (best train acc: 0.9538, best val acc: 0.9653)\n",
      "[Epoch: 34460] train loss: 0.1736, train acc: 0.9378, val loss: 0.1770, val acc: 0.9612  (best train acc: 0.9538, best val acc: 0.9653)\n",
      "[Epoch: 34480] train loss: 0.2136, train acc: 0.9341, val loss: 0.1699, val acc: 0.9582  (best train acc: 0.9538, best val acc: 0.9653)\n",
      "[Epoch: 34500] train loss: 0.1563, train acc: 0.9477, val loss: 0.1740, val acc: 0.9616  (best train acc: 0.9538, best val acc: 0.9653)\n",
      "[Epoch: 34520] train loss: 0.1629, train acc: 0.9459, val loss: 0.1563, val acc: 0.9619  (best train acc: 0.9538, best val acc: 0.9653)\n",
      "[Epoch: 34540] train loss: 0.1805, train acc: 0.9372, val loss: 0.2079, val acc: 0.9524  (best train acc: 0.9538, best val acc: 0.9653)\n",
      "[Epoch: 34560] train loss: 0.1743, train acc: 0.9355, val loss: 0.1978, val acc: 0.9467  (best train acc: 0.9538, best val acc: 0.9653)\n",
      "[Epoch: 34580] train loss: 0.1586, train acc: 0.9469, val loss: 0.1685, val acc: 0.9595  (best train acc: 0.9538, best val acc: 0.9653)\n",
      "[Epoch: 34600] train loss: 0.1559, train acc: 0.9498, val loss: 0.1794, val acc: 0.9595  (best train acc: 0.9538, best val acc: 0.9653)\n",
      "[Epoch: 34620] train loss: 0.1694, train acc: 0.9387, val loss: 0.2094, val acc: 0.9474  (best train acc: 0.9538, best val acc: 0.9653)\n",
      "[Epoch: 34640] train loss: 0.1555, train acc: 0.9492, val loss: 0.1995, val acc: 0.9508  (best train acc: 0.9538, best val acc: 0.9653)\n",
      "[Epoch: 34660] train loss: 0.1660, train acc: 0.9410, val loss: 0.1769, val acc: 0.9555  (best train acc: 0.9538, best val acc: 0.9653)\n",
      "[Epoch: 34680] train loss: 0.1625, train acc: 0.9426, val loss: 0.1991, val acc: 0.9504  (best train acc: 0.9538, best val acc: 0.9653)\n",
      "[Epoch: 34700] train loss: 0.1572, train acc: 0.9457, val loss: 0.1692, val acc: 0.9589  (best train acc: 0.9538, best val acc: 0.9653)\n",
      "[Epoch: 34720] train loss: 0.1528, train acc: 0.9500, val loss: 0.1937, val acc: 0.9565  (best train acc: 0.9545, best val acc: 0.9653)\n",
      "[Epoch: 34740] train loss: 0.1506, train acc: 0.9513, val loss: 0.1815, val acc: 0.9582  (best train acc: 0.9545, best val acc: 0.9653)\n",
      "[Epoch: 34760] train loss: 0.2031, train acc: 0.9310, val loss: 0.2125, val acc: 0.9457  (best train acc: 0.9545, best val acc: 0.9653)\n",
      "[Epoch: 34780] train loss: 0.1569, train acc: 0.9480, val loss: 0.1732, val acc: 0.9612  (best train acc: 0.9545, best val acc: 0.9653)\n",
      "[Epoch: 34800] train loss: 0.2254, train acc: 0.9271, val loss: 0.1864, val acc: 0.9609  (best train acc: 0.9545, best val acc: 0.9653)\n",
      "[Epoch: 34820] train loss: 0.1929, train acc: 0.9325, val loss: 0.1798, val acc: 0.9578  (best train acc: 0.9545, best val acc: 0.9653)\n",
      "[Epoch: 34840] train loss: 0.1750, train acc: 0.9409, val loss: 0.1792, val acc: 0.9602  (best train acc: 0.9545, best val acc: 0.9653)\n",
      "[Epoch: 34860] train loss: 0.1681, train acc: 0.9439, val loss: 0.1653, val acc: 0.9609  (best train acc: 0.9545, best val acc: 0.9653)\n",
      "[Epoch: 34880] train loss: 0.1693, train acc: 0.9391, val loss: 0.1720, val acc: 0.9575  (best train acc: 0.9545, best val acc: 0.9653)\n",
      "[Epoch: 34900] train loss: 0.1597, train acc: 0.9450, val loss: 0.1748, val acc: 0.9572  (best train acc: 0.9545, best val acc: 0.9653)\n",
      "[Epoch: 34920] train loss: 0.1690, train acc: 0.9421, val loss: 0.1795, val acc: 0.9612  (best train acc: 0.9545, best val acc: 0.9653)\n",
      "[Epoch: 34940] train loss: 0.1680, train acc: 0.9396, val loss: 0.1818, val acc: 0.9585  (best train acc: 0.9545, best val acc: 0.9653)\n",
      "[Epoch: 34960] train loss: 0.1652, train acc: 0.9430, val loss: 0.1824, val acc: 0.9562  (best train acc: 0.9545, best val acc: 0.9653)\n",
      "[Epoch: 34980] train loss: 0.1801, train acc: 0.9403, val loss: 0.1775, val acc: 0.9609  (best train acc: 0.9545, best val acc: 0.9653)\n",
      "[Epoch: 35000] train loss: 0.1589, train acc: 0.9469, val loss: 0.1692, val acc: 0.9636  (best train acc: 0.9545, best val acc: 0.9653)\n",
      "[Epoch: 35020] train loss: 0.1645, train acc: 0.9439, val loss: 0.1837, val acc: 0.9578  (best train acc: 0.9545, best val acc: 0.9653)\n",
      "[Epoch: 35040] train loss: 0.1420, train acc: 0.9510, val loss: 0.1571, val acc: 0.9622  (best train acc: 0.9545, best val acc: 0.9653)\n",
      "[Epoch: 35060] train loss: 0.1670, train acc: 0.9412, val loss: 0.1730, val acc: 0.9626  (best train acc: 0.9545, best val acc: 0.9653)\n",
      "[Epoch: 35080] train loss: 0.1680, train acc: 0.9421, val loss: 0.1711, val acc: 0.9629  (best train acc: 0.9545, best val acc: 0.9653)\n",
      "[Epoch: 35100] train loss: 0.1780, train acc: 0.9326, val loss: 0.1861, val acc: 0.9589  (best train acc: 0.9545, best val acc: 0.9653)\n",
      "[Epoch: 35120] train loss: 0.1666, train acc: 0.9462, val loss: 0.1952, val acc: 0.9565  (best train acc: 0.9545, best val acc: 0.9653)\n",
      "[Epoch: 35140] train loss: 0.1669, train acc: 0.9426, val loss: 0.1646, val acc: 0.9592  (best train acc: 0.9545, best val acc: 0.9653)\n",
      "[Epoch: 35160] train loss: 0.1545, train acc: 0.9474, val loss: 0.1702, val acc: 0.9629  (best train acc: 0.9545, best val acc: 0.9653)\n",
      "[Epoch: 35180] train loss: 0.1569, train acc: 0.9459, val loss: 0.1782, val acc: 0.9589  (best train acc: 0.9545, best val acc: 0.9653)\n",
      "[Epoch: 35200] train loss: 0.2001, train acc: 0.9323, val loss: 0.1893, val acc: 0.9602  (best train acc: 0.9545, best val acc: 0.9653)\n",
      "[Epoch: 35220] train loss: 0.2151, train acc: 0.9310, val loss: 0.1909, val acc: 0.9599  (best train acc: 0.9545, best val acc: 0.9653)\n",
      "[Epoch: 35240] train loss: 0.1573, train acc: 0.9469, val loss: 0.1875, val acc: 0.9575  (best train acc: 0.9545, best val acc: 0.9653)\n",
      "[Epoch: 35260] train loss: 0.2027, train acc: 0.9343, val loss: 0.1698, val acc: 0.9629  (best train acc: 0.9545, best val acc: 0.9653)\n",
      "[Epoch: 35280] train loss: 0.1604, train acc: 0.9474, val loss: 0.2225, val acc: 0.9508  (best train acc: 0.9545, best val acc: 0.9653)\n",
      "[Epoch: 35300] train loss: 0.2014, train acc: 0.9308, val loss: 0.1872, val acc: 0.9555  (best train acc: 0.9545, best val acc: 0.9653)\n",
      "[Epoch: 35320] train loss: 0.1514, train acc: 0.9492, val loss: 0.1696, val acc: 0.9605  (best train acc: 0.9545, best val acc: 0.9653)\n",
      "[Epoch: 35340] train loss: 0.1667, train acc: 0.9387, val loss: 0.1768, val acc: 0.9636  (best train acc: 0.9545, best val acc: 0.9653)\n",
      "[Epoch: 35360] train loss: 0.1649, train acc: 0.9456, val loss: 0.1805, val acc: 0.9619  (best train acc: 0.9545, best val acc: 0.9653)\n",
      "[Epoch: 35380] train loss: 0.1672, train acc: 0.9415, val loss: 0.1732, val acc: 0.9602  (best train acc: 0.9545, best val acc: 0.9653)\n",
      "[Epoch: 35400] train loss: 0.1590, train acc: 0.9520, val loss: 0.1844, val acc: 0.9609  (best train acc: 0.9545, best val acc: 0.9653)\n",
      "[Epoch: 35420] train loss: 0.1686, train acc: 0.9427, val loss: 0.1747, val acc: 0.9609  (best train acc: 0.9545, best val acc: 0.9653)\n",
      "[Epoch: 35440] train loss: 0.1581, train acc: 0.9455, val loss: 0.1771, val acc: 0.9626  (best train acc: 0.9545, best val acc: 0.9653)\n",
      "[Epoch: 35460] train loss: 0.1614, train acc: 0.9476, val loss: 0.1679, val acc: 0.9636  (best train acc: 0.9545, best val acc: 0.9653)\n",
      "[Epoch: 35480] train loss: 0.1999, train acc: 0.9362, val loss: 0.1872, val acc: 0.9592  (best train acc: 0.9545, best val acc: 0.9653)\n",
      "[Epoch: 35500] train loss: 0.1546, train acc: 0.9456, val loss: 0.2045, val acc: 0.9541  (best train acc: 0.9549, best val acc: 0.9653)\n",
      "[Epoch: 35520] train loss: 0.1543, train acc: 0.9462, val loss: 0.2019, val acc: 0.9568  (best train acc: 0.9549, best val acc: 0.9653)\n",
      "[Epoch: 35540] train loss: 0.1655, train acc: 0.9412, val loss: 0.1977, val acc: 0.9568  (best train acc: 0.9549, best val acc: 0.9653)\n",
      "[Epoch: 35560] train loss: 0.1487, train acc: 0.9481, val loss: 0.1759, val acc: 0.9592  (best train acc: 0.9549, best val acc: 0.9653)\n",
      "[Epoch: 35580] train loss: 0.1615, train acc: 0.9422, val loss: 0.1826, val acc: 0.9605  (best train acc: 0.9549, best val acc: 0.9653)\n",
      "[Epoch: 35600] train loss: 0.1653, train acc: 0.9481, val loss: 0.1754, val acc: 0.9595  (best train acc: 0.9549, best val acc: 0.9653)\n",
      "[Epoch: 35620] train loss: 0.1855, train acc: 0.9320, val loss: 0.1761, val acc: 0.9592  (best train acc: 0.9549, best val acc: 0.9653)\n",
      "[Epoch: 35640] train loss: 0.1466, train acc: 0.9505, val loss: 0.1799, val acc: 0.9572  (best train acc: 0.9549, best val acc: 0.9653)\n",
      "[Epoch: 35660] train loss: 0.1605, train acc: 0.9445, val loss: 0.1703, val acc: 0.9602  (best train acc: 0.9549, best val acc: 0.9653)\n",
      "[Epoch: 35680] train loss: 0.1603, train acc: 0.9451, val loss: 0.1736, val acc: 0.9619  (best train acc: 0.9549, best val acc: 0.9653)\n",
      "[Epoch: 35700] train loss: 0.1450, train acc: 0.9515, val loss: 0.1941, val acc: 0.9568  (best train acc: 0.9549, best val acc: 0.9653)\n",
      "[Epoch: 35720] train loss: 0.1862, train acc: 0.9356, val loss: 0.1830, val acc: 0.9602  (best train acc: 0.9549, best val acc: 0.9653)\n",
      "[Epoch: 35740] train loss: 0.1619, train acc: 0.9413, val loss: 0.1818, val acc: 0.9595  (best train acc: 0.9549, best val acc: 0.9653)\n",
      "[Epoch: 35760] train loss: 0.1566, train acc: 0.9456, val loss: 0.1881, val acc: 0.9555  (best train acc: 0.9563, best val acc: 0.9653)\n",
      "[Epoch: 35780] train loss: 0.1653, train acc: 0.9394, val loss: 0.1869, val acc: 0.9589  (best train acc: 0.9563, best val acc: 0.9653)\n",
      "[Epoch: 35800] train loss: 0.1514, train acc: 0.9453, val loss: 0.1528, val acc: 0.9612  (best train acc: 0.9563, best val acc: 0.9653)\n",
      "[Epoch: 35820] train loss: 0.1662, train acc: 0.9434, val loss: 0.1692, val acc: 0.9592  (best train acc: 0.9563, best val acc: 0.9653)\n",
      "[Epoch: 35840] train loss: 0.1370, train acc: 0.9525, val loss: 0.1851, val acc: 0.9562  (best train acc: 0.9563, best val acc: 0.9653)\n",
      "[Epoch: 35860] train loss: 0.1497, train acc: 0.9472, val loss: 0.1617, val acc: 0.9622  (best train acc: 0.9563, best val acc: 0.9653)\n",
      "[Epoch: 35880] train loss: 0.1528, train acc: 0.9497, val loss: 0.1753, val acc: 0.9612  (best train acc: 0.9563, best val acc: 0.9653)\n",
      "[Epoch: 35900] train loss: 0.1524, train acc: 0.9449, val loss: 0.1768, val acc: 0.9568  (best train acc: 0.9563, best val acc: 0.9653)\n",
      "[Epoch: 35920] train loss: 0.1566, train acc: 0.9430, val loss: 0.1686, val acc: 0.9599  (best train acc: 0.9563, best val acc: 0.9653)\n",
      "[Epoch: 35940] train loss: 0.1514, train acc: 0.9480, val loss: 0.1726, val acc: 0.9592  (best train acc: 0.9563, best val acc: 0.9653)\n",
      "[Epoch: 35960] train loss: 0.1735, train acc: 0.9412, val loss: 0.1868, val acc: 0.9558  (best train acc: 0.9563, best val acc: 0.9653)\n",
      "[Epoch: 35980] train loss: 0.1472, train acc: 0.9427, val loss: 0.1694, val acc: 0.9626  (best train acc: 0.9563, best val acc: 0.9653)\n",
      "[Epoch: 36000] train loss: 0.1572, train acc: 0.9412, val loss: 0.1941, val acc: 0.9494  (best train acc: 0.9563, best val acc: 0.9653)\n",
      "[Epoch: 36020] train loss: 0.1744, train acc: 0.9452, val loss: 0.1705, val acc: 0.9578  (best train acc: 0.9563, best val acc: 0.9653)\n",
      "[Epoch: 36040] train loss: 0.1685, train acc: 0.9391, val loss: 0.1737, val acc: 0.9595  (best train acc: 0.9563, best val acc: 0.9653)\n",
      "[Epoch: 36060] train loss: 0.1601, train acc: 0.9432, val loss: 0.1686, val acc: 0.9582  (best train acc: 0.9563, best val acc: 0.9653)\n",
      "[Epoch: 36080] train loss: 0.1626, train acc: 0.9430, val loss: 0.1515, val acc: 0.9639  (best train acc: 0.9563, best val acc: 0.9659)\n",
      "[Epoch: 36100] train loss: 0.1614, train acc: 0.9426, val loss: 0.1701, val acc: 0.9599  (best train acc: 0.9563, best val acc: 0.9659)\n",
      "[Epoch: 36120] train loss: 0.1515, train acc: 0.9440, val loss: 0.1771, val acc: 0.9572  (best train acc: 0.9563, best val acc: 0.9659)\n",
      "[Epoch: 36140] train loss: 0.1460, train acc: 0.9478, val loss: 0.1650, val acc: 0.9609  (best train acc: 0.9586, best val acc: 0.9659)\n",
      "[Epoch: 36160] train loss: 0.1315, train acc: 0.9580, val loss: 0.1852, val acc: 0.9568  (best train acc: 0.9586, best val acc: 0.9659)\n",
      "[Epoch: 36180] train loss: 0.2063, train acc: 0.9258, val loss: 0.1751, val acc: 0.9578  (best train acc: 0.9586, best val acc: 0.9659)\n",
      "[Epoch: 36200] train loss: 0.1527, train acc: 0.9462, val loss: 0.1944, val acc: 0.9535  (best train acc: 0.9586, best val acc: 0.9659)\n",
      "[Epoch: 36220] train loss: 0.1423, train acc: 0.9525, val loss: 0.1955, val acc: 0.9538  (best train acc: 0.9586, best val acc: 0.9659)\n",
      "[Epoch: 36240] train loss: 0.1390, train acc: 0.9527, val loss: 0.1882, val acc: 0.9562  (best train acc: 0.9586, best val acc: 0.9659)\n",
      "[Epoch: 36260] train loss: 0.1660, train acc: 0.9450, val loss: 0.2104, val acc: 0.9457  (best train acc: 0.9586, best val acc: 0.9659)\n",
      "[Epoch: 36280] train loss: 0.1366, train acc: 0.9526, val loss: 0.1665, val acc: 0.9592  (best train acc: 0.9586, best val acc: 0.9659)\n",
      "[Epoch: 36300] train loss: 0.1965, train acc: 0.9234, val loss: 0.1786, val acc: 0.9558  (best train acc: 0.9586, best val acc: 0.9659)\n",
      "[Epoch: 36320] train loss: 0.1522, train acc: 0.9491, val loss: 0.1895, val acc: 0.9494  (best train acc: 0.9592, best val acc: 0.9659)\n",
      "[Epoch: 36340] train loss: 0.1410, train acc: 0.9530, val loss: 0.1776, val acc: 0.9599  (best train acc: 0.9592, best val acc: 0.9659)\n",
      "[Epoch: 36360] train loss: 0.1438, train acc: 0.9486, val loss: 0.1652, val acc: 0.9609  (best train acc: 0.9592, best val acc: 0.9659)\n",
      "[Epoch: 36380] train loss: 0.1419, train acc: 0.9470, val loss: 0.1907, val acc: 0.9541  (best train acc: 0.9592, best val acc: 0.9659)\n",
      "[Epoch: 36400] train loss: 0.1457, train acc: 0.9501, val loss: 0.1830, val acc: 0.9565  (best train acc: 0.9592, best val acc: 0.9659)\n",
      "[Epoch: 36420] train loss: 0.1503, train acc: 0.9491, val loss: 0.1827, val acc: 0.9589  (best train acc: 0.9592, best val acc: 0.9659)\n",
      "[Epoch: 36440] train loss: 0.1946, train acc: 0.9374, val loss: 0.1918, val acc: 0.9555  (best train acc: 0.9592, best val acc: 0.9659)\n",
      "[Epoch: 36460] train loss: 0.1449, train acc: 0.9471, val loss: 0.1754, val acc: 0.9585  (best train acc: 0.9592, best val acc: 0.9659)\n",
      "[Epoch: 36480] train loss: 0.1433, train acc: 0.9542, val loss: 0.1861, val acc: 0.9572  (best train acc: 0.9592, best val acc: 0.9659)\n",
      "[Epoch: 36500] train loss: 0.1369, train acc: 0.9500, val loss: 0.1620, val acc: 0.9632  (best train acc: 0.9592, best val acc: 0.9659)\n",
      "[Epoch: 36520] train loss: 0.1309, train acc: 0.9563, val loss: 0.1683, val acc: 0.9605  (best train acc: 0.9592, best val acc: 0.9659)\n",
      "[Epoch: 36540] train loss: 0.1420, train acc: 0.9469, val loss: 0.1523, val acc: 0.9629  (best train acc: 0.9592, best val acc: 0.9659)\n",
      "[Epoch: 36560] train loss: 0.1895, train acc: 0.9372, val loss: 0.1538, val acc: 0.9609  (best train acc: 0.9592, best val acc: 0.9659)\n",
      "[Epoch: 36580] train loss: 0.1680, train acc: 0.9422, val loss: 0.1671, val acc: 0.9599  (best train acc: 0.9592, best val acc: 0.9659)\n",
      "[Epoch: 36600] train loss: 0.1477, train acc: 0.9488, val loss: 0.1809, val acc: 0.9575  (best train acc: 0.9592, best val acc: 0.9659)\n",
      "[Epoch: 36620] train loss: 0.1347, train acc: 0.9541, val loss: 0.1693, val acc: 0.9619  (best train acc: 0.9592, best val acc: 0.9659)\n",
      "[Epoch: 36640] train loss: 0.1427, train acc: 0.9534, val loss: 0.1559, val acc: 0.9622  (best train acc: 0.9592, best val acc: 0.9659)\n",
      "[Epoch: 36660] train loss: 0.1576, train acc: 0.9436, val loss: 0.1590, val acc: 0.9629  (best train acc: 0.9592, best val acc: 0.9659)\n",
      "[Epoch: 36680] train loss: 0.1342, train acc: 0.9523, val loss: 0.1638, val acc: 0.9602  (best train acc: 0.9592, best val acc: 0.9659)\n",
      "[Epoch: 36700] train loss: 0.1411, train acc: 0.9486, val loss: 0.1729, val acc: 0.9605  (best train acc: 0.9592, best val acc: 0.9659)\n",
      "[Epoch: 36720] train loss: 0.1399, train acc: 0.9526, val loss: 0.1595, val acc: 0.9616  (best train acc: 0.9592, best val acc: 0.9659)\n",
      "[Epoch: 36740] train loss: 0.1797, train acc: 0.9350, val loss: 0.1791, val acc: 0.9575  (best train acc: 0.9592, best val acc: 0.9659)\n",
      "[Epoch: 36760] train loss: 0.1342, train acc: 0.9536, val loss: 0.1697, val acc: 0.9558  (best train acc: 0.9592, best val acc: 0.9659)\n",
      "[Epoch: 36780] train loss: 0.1688, train acc: 0.9430, val loss: 0.1552, val acc: 0.9626  (best train acc: 0.9592, best val acc: 0.9659)\n",
      "[Epoch: 36800] train loss: 0.1490, train acc: 0.9484, val loss: 0.1687, val acc: 0.9629  (best train acc: 0.9592, best val acc: 0.9659)\n",
      "[Epoch: 36820] train loss: 0.1544, train acc: 0.9439, val loss: 0.1929, val acc: 0.9504  (best train acc: 0.9592, best val acc: 0.9659)\n",
      "[Epoch: 36840] train loss: 0.1522, train acc: 0.9498, val loss: 0.1668, val acc: 0.9639  (best train acc: 0.9592, best val acc: 0.9659)\n",
      "[Epoch: 36860] train loss: 0.1468, train acc: 0.9493, val loss: 0.1703, val acc: 0.9602  (best train acc: 0.9592, best val acc: 0.9659)\n",
      "[Epoch: 36880] train loss: 0.1512, train acc: 0.9451, val loss: 0.1865, val acc: 0.9599  (best train acc: 0.9592, best val acc: 0.9659)\n",
      "[Epoch: 36900] train loss: 0.1430, train acc: 0.9519, val loss: 0.1741, val acc: 0.9555  (best train acc: 0.9592, best val acc: 0.9659)\n",
      "[Epoch: 36920] train loss: 0.1658, train acc: 0.9385, val loss: 0.1947, val acc: 0.9572  (best train acc: 0.9592, best val acc: 0.9659)\n",
      "[Epoch: 36940] train loss: 0.1400, train acc: 0.9512, val loss: 0.1709, val acc: 0.9612  (best train acc: 0.9592, best val acc: 0.9659)\n",
      "[Epoch: 36960] train loss: 0.1540, train acc: 0.9502, val loss: 0.1681, val acc: 0.9609  (best train acc: 0.9592, best val acc: 0.9659)\n",
      "[Epoch: 36980] train loss: 0.1487, train acc: 0.9485, val loss: 0.1896, val acc: 0.9541  (best train acc: 0.9592, best val acc: 0.9659)\n",
      "[Epoch: 37000] train loss: 0.1769, train acc: 0.9305, val loss: 0.1996, val acc: 0.9497  (best train acc: 0.9592, best val acc: 0.9659)\n",
      "[Epoch: 37020] train loss: 0.1515, train acc: 0.9427, val loss: 0.1532, val acc: 0.9605  (best train acc: 0.9592, best val acc: 0.9659)\n",
      "[Epoch: 37040] train loss: 0.1498, train acc: 0.9489, val loss: 0.1683, val acc: 0.9602  (best train acc: 0.9592, best val acc: 0.9659)\n",
      "[Epoch: 37060] train loss: 0.1335, train acc: 0.9547, val loss: 0.1709, val acc: 0.9578  (best train acc: 0.9592, best val acc: 0.9659)\n",
      "[Epoch: 37080] train loss: 0.1346, train acc: 0.9552, val loss: 0.1638, val acc: 0.9612  (best train acc: 0.9592, best val acc: 0.9659)\n",
      "[Epoch: 37100] train loss: 0.1934, train acc: 0.9327, val loss: 0.1829, val acc: 0.9592  (best train acc: 0.9592, best val acc: 0.9659)\n",
      "[Epoch: 37120] train loss: 0.1421, train acc: 0.9534, val loss: 0.1704, val acc: 0.9626  (best train acc: 0.9592, best val acc: 0.9659)\n",
      "[Epoch: 37140] train loss: 0.1418, train acc: 0.9542, val loss: 0.1754, val acc: 0.9602  (best train acc: 0.9592, best val acc: 0.9659)\n",
      "[Epoch: 37160] train loss: 0.1318, train acc: 0.9525, val loss: 0.1685, val acc: 0.9602  (best train acc: 0.9592, best val acc: 0.9659)\n",
      "[Epoch: 37180] train loss: 0.1829, train acc: 0.9387, val loss: 0.1717, val acc: 0.9612  (best train acc: 0.9592, best val acc: 0.9659)\n",
      "[Epoch: 37200] train loss: 0.2046, train acc: 0.9329, val loss: 0.1551, val acc: 0.9605  (best train acc: 0.9592, best val acc: 0.9659)\n",
      "[Epoch: 37220] train loss: 0.1436, train acc: 0.9501, val loss: 0.1827, val acc: 0.9528  (best train acc: 0.9592, best val acc: 0.9659)\n",
      "[Epoch: 37240] train loss: 0.1442, train acc: 0.9517, val loss: 0.1691, val acc: 0.9589  (best train acc: 0.9592, best val acc: 0.9659)\n",
      "[Epoch: 37260] train loss: 0.1391, train acc: 0.9519, val loss: 0.1763, val acc: 0.9619  (best train acc: 0.9592, best val acc: 0.9659)\n",
      "[Epoch: 37280] train loss: 0.1372, train acc: 0.9523, val loss: 0.1668, val acc: 0.9629  (best train acc: 0.9592, best val acc: 0.9659)\n",
      "[Epoch: 37300] train loss: 0.1558, train acc: 0.9475, val loss: 0.1895, val acc: 0.9578  (best train acc: 0.9592, best val acc: 0.9659)\n",
      "[Epoch: 37320] train loss: 0.1519, train acc: 0.9483, val loss: 0.1816, val acc: 0.9612  (best train acc: 0.9592, best val acc: 0.9659)\n",
      "[Epoch: 37340] train loss: 0.1379, train acc: 0.9510, val loss: 0.1729, val acc: 0.9612  (best train acc: 0.9592, best val acc: 0.9659)\n",
      "[Epoch: 37360] train loss: 0.1483, train acc: 0.9481, val loss: 0.1764, val acc: 0.9572  (best train acc: 0.9592, best val acc: 0.9659)\n",
      "[Epoch: 37380] train loss: 0.1532, train acc: 0.9489, val loss: 0.1931, val acc: 0.9558  (best train acc: 0.9592, best val acc: 0.9659)\n",
      "[Epoch: 37400] train loss: 0.1459, train acc: 0.9498, val loss: 0.2173, val acc: 0.9413  (best train acc: 0.9592, best val acc: 0.9659)\n",
      "[Epoch: 37420] train loss: 0.1640, train acc: 0.9436, val loss: 0.1919, val acc: 0.9524  (best train acc: 0.9592, best val acc: 0.9659)\n",
      "[Epoch: 37440] train loss: 0.1449, train acc: 0.9491, val loss: 0.1776, val acc: 0.9595  (best train acc: 0.9592, best val acc: 0.9659)\n",
      "[Epoch: 37460] train loss: 0.1646, train acc: 0.9372, val loss: 0.1799, val acc: 0.9589  (best train acc: 0.9592, best val acc: 0.9659)\n",
      "[Epoch: 37480] train loss: 0.1331, train acc: 0.9552, val loss: 0.1555, val acc: 0.9629  (best train acc: 0.9604, best val acc: 0.9659)\n",
      "[Epoch: 37500] train loss: 0.1887, train acc: 0.9345, val loss: 0.1834, val acc: 0.9572  (best train acc: 0.9604, best val acc: 0.9659)\n",
      "[Epoch: 37520] train loss: 0.1597, train acc: 0.9404, val loss: 0.1538, val acc: 0.9595  (best train acc: 0.9604, best val acc: 0.9659)\n",
      "[Epoch: 37540] train loss: 0.1399, train acc: 0.9488, val loss: 0.1551, val acc: 0.9622  (best train acc: 0.9604, best val acc: 0.9659)\n",
      "[Epoch: 37560] train loss: 0.1361, train acc: 0.9498, val loss: 0.1593, val acc: 0.9602  (best train acc: 0.9604, best val acc: 0.9659)\n",
      "[Epoch: 37580] train loss: 0.1375, train acc: 0.9506, val loss: 0.1760, val acc: 0.9589  (best train acc: 0.9604, best val acc: 0.9659)\n",
      "[Epoch: 37600] train loss: 0.1259, train acc: 0.9572, val loss: 0.1709, val acc: 0.9589  (best train acc: 0.9604, best val acc: 0.9659)\n",
      "[Epoch: 37620] train loss: 0.1532, train acc: 0.9469, val loss: 0.1539, val acc: 0.9612  (best train acc: 0.9604, best val acc: 0.9659)\n",
      "[Epoch: 37640] train loss: 0.1347, train acc: 0.9524, val loss: 0.1995, val acc: 0.9494  (best train acc: 0.9604, best val acc: 0.9659)\n",
      "[Epoch: 37660] train loss: 0.1349, train acc: 0.9541, val loss: 0.1809, val acc: 0.9548  (best train acc: 0.9604, best val acc: 0.9659)\n",
      "[Epoch: 37680] train loss: 0.1376, train acc: 0.9529, val loss: 0.1511, val acc: 0.9616  (best train acc: 0.9604, best val acc: 0.9659)\n",
      "[Epoch: 37700] train loss: 0.1523, train acc: 0.9465, val loss: 0.1740, val acc: 0.9562  (best train acc: 0.9604, best val acc: 0.9659)\n",
      "[Epoch: 37720] train loss: 0.1500, train acc: 0.9471, val loss: 0.1741, val acc: 0.9605  (best train acc: 0.9604, best val acc: 0.9659)\n",
      "[Epoch: 37740] train loss: 0.1450, train acc: 0.9510, val loss: 0.1649, val acc: 0.9619  (best train acc: 0.9604, best val acc: 0.9659)\n",
      "[Epoch: 37760] train loss: 0.1453, train acc: 0.9479, val loss: 0.1592, val acc: 0.9622  (best train acc: 0.9604, best val acc: 0.9659)\n",
      "[Epoch: 37780] train loss: 0.1576, train acc: 0.9440, val loss: 0.1841, val acc: 0.9555  (best train acc: 0.9604, best val acc: 0.9659)\n",
      "[Epoch: 37800] train loss: 0.1417, train acc: 0.9498, val loss: 0.1649, val acc: 0.9646  (best train acc: 0.9604, best val acc: 0.9659)\n",
      "[Epoch: 37820] train loss: 0.1717, train acc: 0.9383, val loss: 0.1681, val acc: 0.9616  (best train acc: 0.9604, best val acc: 0.9659)\n",
      "[Epoch: 37840] train loss: 0.1504, train acc: 0.9460, val loss: 0.1640, val acc: 0.9599  (best train acc: 0.9604, best val acc: 0.9659)\n",
      "[Epoch: 37860] train loss: 0.1514, train acc: 0.9443, val loss: 0.1871, val acc: 0.9551  (best train acc: 0.9604, best val acc: 0.9659)\n",
      "[Epoch: 37880] train loss: 0.1361, train acc: 0.9547, val loss: 0.1670, val acc: 0.9612  (best train acc: 0.9604, best val acc: 0.9659)\n",
      "[Epoch: 37900] train loss: 0.1469, train acc: 0.9476, val loss: 0.1669, val acc: 0.9616  (best train acc: 0.9604, best val acc: 0.9659)\n",
      "[Epoch: 37920] train loss: 0.1667, train acc: 0.9429, val loss: 0.2051, val acc: 0.9460  (best train acc: 0.9604, best val acc: 0.9659)\n",
      "[Epoch: 37940] train loss: 0.1346, train acc: 0.9524, val loss: 0.1651, val acc: 0.9609  (best train acc: 0.9604, best val acc: 0.9659)\n",
      "[Epoch: 37960] train loss: 0.1419, train acc: 0.9513, val loss: 0.1680, val acc: 0.9589  (best train acc: 0.9604, best val acc: 0.9659)\n",
      "[Epoch: 37980] train loss: 0.1577, train acc: 0.9422, val loss: 0.1731, val acc: 0.9595  (best train acc: 0.9604, best val acc: 0.9659)\n",
      "[Epoch: 38000] train loss: 0.1482, train acc: 0.9512, val loss: 0.1580, val acc: 0.9619  (best train acc: 0.9604, best val acc: 0.9659)\n",
      "[Epoch: 38020] train loss: 0.1637, train acc: 0.9418, val loss: 0.2073, val acc: 0.9484  (best train acc: 0.9604, best val acc: 0.9659)\n",
      "[Epoch: 38040] train loss: 0.1244, train acc: 0.9570, val loss: 0.2137, val acc: 0.9538  (best train acc: 0.9604, best val acc: 0.9659)\n",
      "[Epoch: 38060] train loss: 0.1380, train acc: 0.9517, val loss: 0.1856, val acc: 0.9551  (best train acc: 0.9604, best val acc: 0.9659)\n",
      "[Epoch: 38080] train loss: 0.1438, train acc: 0.9487, val loss: 0.1596, val acc: 0.9632  (best train acc: 0.9604, best val acc: 0.9659)\n",
      "[Epoch: 38100] train loss: 0.1343, train acc: 0.9552, val loss: 0.1820, val acc: 0.9555  (best train acc: 0.9604, best val acc: 0.9659)\n",
      "[Epoch: 38120] train loss: 0.1442, train acc: 0.9516, val loss: 0.1773, val acc: 0.9585  (best train acc: 0.9604, best val acc: 0.9659)\n",
      "[Epoch: 38140] train loss: 0.1457, train acc: 0.9472, val loss: 0.1671, val acc: 0.9609  (best train acc: 0.9604, best val acc: 0.9659)\n",
      "[Epoch: 38160] train loss: 0.1567, train acc: 0.9411, val loss: 0.1804, val acc: 0.9599  (best train acc: 0.9604, best val acc: 0.9659)\n",
      "[Epoch: 38180] train loss: 0.1532, train acc: 0.9487, val loss: 0.1921, val acc: 0.9575  (best train acc: 0.9604, best val acc: 0.9659)\n",
      "[Epoch: 38200] train loss: 0.1454, train acc: 0.9482, val loss: 0.2357, val acc: 0.9417  (best train acc: 0.9604, best val acc: 0.9659)\n",
      "[Epoch: 38220] train loss: 0.1565, train acc: 0.9466, val loss: 0.1875, val acc: 0.9531  (best train acc: 0.9604, best val acc: 0.9659)\n",
      "[Epoch: 38240] train loss: 0.1565, train acc: 0.9384, val loss: 0.1634, val acc: 0.9605  (best train acc: 0.9604, best val acc: 0.9659)\n",
      "[Epoch: 38260] train loss: 0.1650, train acc: 0.9405, val loss: 0.1859, val acc: 0.9562  (best train acc: 0.9604, best val acc: 0.9659)\n",
      "[Epoch: 38280] train loss: 0.1482, train acc: 0.9500, val loss: 0.1780, val acc: 0.9582  (best train acc: 0.9604, best val acc: 0.9659)\n",
      "[Epoch: 38300] train loss: 0.1591, train acc: 0.9451, val loss: 0.1793, val acc: 0.9592  (best train acc: 0.9604, best val acc: 0.9659)\n",
      "[Epoch: 38320] train loss: 0.1402, train acc: 0.9511, val loss: 0.1757, val acc: 0.9578  (best train acc: 0.9604, best val acc: 0.9659)\n",
      "[Epoch: 38340] train loss: 0.1410, train acc: 0.9508, val loss: 0.1819, val acc: 0.9602  (best train acc: 0.9604, best val acc: 0.9659)\n",
      "[Epoch: 38360] train loss: 0.1474, train acc: 0.9476, val loss: 0.1844, val acc: 0.9578  (best train acc: 0.9604, best val acc: 0.9659)\n",
      "[Epoch: 38380] train loss: 0.2189, train acc: 0.9286, val loss: 0.1979, val acc: 0.9568  (best train acc: 0.9604, best val acc: 0.9659)\n",
      "[Epoch: 38400] train loss: 0.1480, train acc: 0.9470, val loss: 0.1970, val acc: 0.9548  (best train acc: 0.9604, best val acc: 0.9659)\n",
      "[Epoch: 38420] train loss: 0.1503, train acc: 0.9464, val loss: 0.1960, val acc: 0.9535  (best train acc: 0.9604, best val acc: 0.9659)\n",
      "[Epoch: 38440] train loss: 0.1408, train acc: 0.9492, val loss: 0.1631, val acc: 0.9592  (best train acc: 0.9604, best val acc: 0.9659)\n",
      "[Epoch: 38460] train loss: 0.1411, train acc: 0.9506, val loss: 0.1750, val acc: 0.9602  (best train acc: 0.9604, best val acc: 0.9659)\n",
      "[Epoch: 38480] train loss: 0.1517, train acc: 0.9446, val loss: 0.2005, val acc: 0.9521  (best train acc: 0.9604, best val acc: 0.9659)\n",
      "[Epoch: 38500] train loss: 0.2168, train acc: 0.9297, val loss: 0.1857, val acc: 0.9538  (best train acc: 0.9604, best val acc: 0.9659)\n",
      "[Epoch: 38520] train loss: 0.1725, train acc: 0.9417, val loss: 0.1837, val acc: 0.9629  (best train acc: 0.9604, best val acc: 0.9659)\n",
      "[Epoch: 38540] train loss: 0.1424, train acc: 0.9500, val loss: 0.2209, val acc: 0.9494  (best train acc: 0.9604, best val acc: 0.9659)\n",
      "[Epoch: 38560] train loss: 0.1496, train acc: 0.9478, val loss: 0.1604, val acc: 0.9595  (best train acc: 0.9604, best val acc: 0.9659)\n",
      "[Epoch: 38580] train loss: 0.1425, train acc: 0.9491, val loss: 0.1626, val acc: 0.9619  (best train acc: 0.9604, best val acc: 0.9659)\n",
      "[Epoch: 38600] train loss: 0.1425, train acc: 0.9515, val loss: 0.1746, val acc: 0.9616  (best train acc: 0.9604, best val acc: 0.9659)\n",
      "[Epoch: 38620] train loss: 0.1531, train acc: 0.9461, val loss: 0.1985, val acc: 0.9518  (best train acc: 0.9604, best val acc: 0.9659)\n",
      "[Epoch: 38640] train loss: 0.1542, train acc: 0.9462, val loss: 0.1767, val acc: 0.9616  (best train acc: 0.9604, best val acc: 0.9659)\n",
      "[Epoch: 38660] train loss: 0.1567, train acc: 0.9444, val loss: 0.1716, val acc: 0.9616  (best train acc: 0.9604, best val acc: 0.9659)\n",
      "[Epoch: 38680] train loss: 0.1357, train acc: 0.9548, val loss: 0.1780, val acc: 0.9605  (best train acc: 0.9604, best val acc: 0.9659)\n",
      "[Epoch: 38700] train loss: 0.1496, train acc: 0.9458, val loss: 0.2023, val acc: 0.9524  (best train acc: 0.9604, best val acc: 0.9659)\n",
      "[Epoch: 38720] train loss: 0.1458, train acc: 0.9513, val loss: 0.1721, val acc: 0.9619  (best train acc: 0.9604, best val acc: 0.9659)\n",
      "[Epoch: 38740] train loss: 0.1483, train acc: 0.9509, val loss: 0.1810, val acc: 0.9578  (best train acc: 0.9604, best val acc: 0.9659)\n",
      "[Epoch: 38760] train loss: 0.1422, train acc: 0.9564, val loss: 0.1668, val acc: 0.9609  (best train acc: 0.9604, best val acc: 0.9659)\n",
      "[Epoch: 38780] train loss: 0.1458, train acc: 0.9502, val loss: 0.1778, val acc: 0.9605  (best train acc: 0.9604, best val acc: 0.9659)\n",
      "[Epoch: 38800] train loss: 0.1698, train acc: 0.9409, val loss: 0.1719, val acc: 0.9622  (best train acc: 0.9604, best val acc: 0.9659)\n",
      "[Epoch: 38820] train loss: 0.1500, train acc: 0.9464, val loss: 0.1736, val acc: 0.9589  (best train acc: 0.9604, best val acc: 0.9659)\n",
      "[Epoch: 38840] train loss: 0.1771, train acc: 0.9340, val loss: 0.2126, val acc: 0.9514  (best train acc: 0.9604, best val acc: 0.9659)\n",
      "[Epoch: 38860] train loss: 0.1603, train acc: 0.9403, val loss: 0.2436, val acc: 0.9356  (best train acc: 0.9604, best val acc: 0.9659)\n",
      "[Epoch: 38880] train loss: 0.2055, train acc: 0.9352, val loss: 0.1672, val acc: 0.9599  (best train acc: 0.9604, best val acc: 0.9659)\n",
      "[Epoch: 38900] train loss: 0.2138, train acc: 0.9318, val loss: 0.1625, val acc: 0.9612  (best train acc: 0.9604, best val acc: 0.9659)\n",
      "[Epoch: 38920] train loss: 0.1413, train acc: 0.9534, val loss: 0.1644, val acc: 0.9602  (best train acc: 0.9604, best val acc: 0.9659)\n",
      "[Epoch: 38940] train loss: 0.1484, train acc: 0.9507, val loss: 0.1704, val acc: 0.9616  (best train acc: 0.9604, best val acc: 0.9659)\n",
      "[Epoch: 38960] train loss: 0.1436, train acc: 0.9469, val loss: 0.1594, val acc: 0.9626  (best train acc: 0.9604, best val acc: 0.9659)\n",
      "[Epoch: 38980] train loss: 0.1428, train acc: 0.9512, val loss: 0.1880, val acc: 0.9504  (best train acc: 0.9604, best val acc: 0.9659)\n",
      "[Epoch: 39000] train loss: 0.1669, train acc: 0.9365, val loss: 0.1683, val acc: 0.9595  (best train acc: 0.9604, best val acc: 0.9659)\n",
      "[Epoch: 39020] train loss: 0.1377, train acc: 0.9513, val loss: 0.1707, val acc: 0.9595  (best train acc: 0.9604, best val acc: 0.9659)\n",
      "[Epoch: 39040] train loss: 0.1493, train acc: 0.9504, val loss: 0.1685, val acc: 0.9612  (best train acc: 0.9604, best val acc: 0.9659)\n",
      "[Epoch: 39060] train loss: 0.1443, train acc: 0.9525, val loss: 0.1950, val acc: 0.9528  (best train acc: 0.9604, best val acc: 0.9659)\n",
      "[Epoch: 39080] train loss: 0.1389, train acc: 0.9539, val loss: 0.1760, val acc: 0.9562  (best train acc: 0.9604, best val acc: 0.9659)\n",
      "[Epoch: 39100] train loss: 0.1462, train acc: 0.9492, val loss: 0.1658, val acc: 0.9585  (best train acc: 0.9604, best val acc: 0.9659)\n",
      "[Epoch: 39120] train loss: 0.1388, train acc: 0.9524, val loss: 0.1497, val acc: 0.9626  (best train acc: 0.9604, best val acc: 0.9659)\n",
      "[Epoch: 39140] train loss: 0.1381, train acc: 0.9517, val loss: 0.1834, val acc: 0.9575  (best train acc: 0.9604, best val acc: 0.9659)\n",
      "[Epoch: 39160] train loss: 0.1429, train acc: 0.9500, val loss: 0.1668, val acc: 0.9592  (best train acc: 0.9604, best val acc: 0.9659)\n",
      "[Epoch: 39180] train loss: 0.1285, train acc: 0.9574, val loss: 0.1364, val acc: 0.9616  (best train acc: 0.9604, best val acc: 0.9659)\n",
      "[Epoch: 39200] train loss: 0.1299, train acc: 0.9530, val loss: 0.1590, val acc: 0.9592  (best train acc: 0.9604, best val acc: 0.9659)\n",
      "[Epoch: 39220] train loss: 0.1664, train acc: 0.9400, val loss: 0.1747, val acc: 0.9589  (best train acc: 0.9604, best val acc: 0.9659)\n",
      "[Epoch: 39240] train loss: 0.1325, train acc: 0.9556, val loss: 0.1577, val acc: 0.9619  (best train acc: 0.9604, best val acc: 0.9659)\n",
      "[Epoch: 39260] train loss: 0.1318, train acc: 0.9549, val loss: 0.1845, val acc: 0.9555  (best train acc: 0.9604, best val acc: 0.9659)\n",
      "[Epoch: 39280] train loss: 0.1290, train acc: 0.9563, val loss: 0.1561, val acc: 0.9626  (best train acc: 0.9624, best val acc: 0.9659)\n",
      "[Epoch: 39300] train loss: 0.1416, train acc: 0.9526, val loss: 0.1683, val acc: 0.9595  (best train acc: 0.9624, best val acc: 0.9659)\n",
      "[Epoch: 39320] train loss: 0.1452, train acc: 0.9473, val loss: 0.1580, val acc: 0.9622  (best train acc: 0.9624, best val acc: 0.9659)\n",
      "[Epoch: 39340] train loss: 0.1748, train acc: 0.9432, val loss: 0.1746, val acc: 0.9599  (best train acc: 0.9624, best val acc: 0.9659)\n",
      "[Epoch: 39360] train loss: 0.1298, train acc: 0.9542, val loss: 0.1581, val acc: 0.9609  (best train acc: 0.9624, best val acc: 0.9659)\n",
      "[Epoch: 39380] train loss: 0.1379, train acc: 0.9518, val loss: 0.1517, val acc: 0.9626  (best train acc: 0.9624, best val acc: 0.9659)\n",
      "[Epoch: 39400] train loss: 0.1129, train acc: 0.9609, val loss: 0.1527, val acc: 0.9602  (best train acc: 0.9624, best val acc: 0.9659)\n",
      "[Epoch: 39420] train loss: 0.1506, train acc: 0.9463, val loss: 0.1880, val acc: 0.9568  (best train acc: 0.9624, best val acc: 0.9659)\n",
      "[Epoch: 39440] train loss: 0.1516, train acc: 0.9453, val loss: 0.2042, val acc: 0.9535  (best train acc: 0.9624, best val acc: 0.9659)\n",
      "[Epoch: 39460] train loss: 0.1490, train acc: 0.9495, val loss: 0.1634, val acc: 0.9595  (best train acc: 0.9624, best val acc: 0.9659)\n",
      "[Epoch: 39480] train loss: 0.1493, train acc: 0.9474, val loss: 0.1569, val acc: 0.9609  (best train acc: 0.9624, best val acc: 0.9659)\n",
      "[Epoch: 39500] train loss: 0.1531, train acc: 0.9409, val loss: 0.1839, val acc: 0.9531  (best train acc: 0.9624, best val acc: 0.9659)\n",
      "[Epoch: 39520] train loss: 0.1706, train acc: 0.9415, val loss: 0.1610, val acc: 0.9595  (best train acc: 0.9624, best val acc: 0.9659)\n",
      "[Epoch: 39540] train loss: 0.1516, train acc: 0.9456, val loss: 0.1677, val acc: 0.9572  (best train acc: 0.9624, best val acc: 0.9659)\n",
      "[Epoch: 39560] train loss: 0.1303, train acc: 0.9529, val loss: 0.1815, val acc: 0.9562  (best train acc: 0.9624, best val acc: 0.9659)\n",
      "[Epoch: 39580] train loss: 0.1496, train acc: 0.9457, val loss: 0.1598, val acc: 0.9592  (best train acc: 0.9624, best val acc: 0.9659)\n",
      "[Epoch: 39600] train loss: 0.1348, train acc: 0.9533, val loss: 0.1591, val acc: 0.9609  (best train acc: 0.9624, best val acc: 0.9659)\n",
      "[Epoch: 39620] train loss: 0.1302, train acc: 0.9519, val loss: 0.1540, val acc: 0.9599  (best train acc: 0.9624, best val acc: 0.9659)\n",
      "[Epoch: 39640] train loss: 0.1567, train acc: 0.9422, val loss: 0.1596, val acc: 0.9629  (best train acc: 0.9624, best val acc: 0.9659)\n",
      "[Epoch: 39660] train loss: 0.1375, train acc: 0.9510, val loss: 0.1806, val acc: 0.9572  (best train acc: 0.9624, best val acc: 0.9659)\n",
      "[Epoch: 39680] train loss: 0.1253, train acc: 0.9596, val loss: 0.1874, val acc: 0.9568  (best train acc: 0.9624, best val acc: 0.9659)\n",
      "[Epoch: 39700] train loss: 0.1354, train acc: 0.9528, val loss: 0.1576, val acc: 0.9599  (best train acc: 0.9624, best val acc: 0.9659)\n",
      "[Epoch: 39720] train loss: 0.1150, train acc: 0.9589, val loss: 0.1694, val acc: 0.9568  (best train acc: 0.9624, best val acc: 0.9659)\n",
      "[Epoch: 39740] train loss: 0.1167, train acc: 0.9584, val loss: 0.1480, val acc: 0.9599  (best train acc: 0.9624, best val acc: 0.9659)\n",
      "[Epoch: 39760] train loss: 0.1257, train acc: 0.9539, val loss: 0.1391, val acc: 0.9616  (best train acc: 0.9624, best val acc: 0.9659)\n",
      "[Epoch: 39780] train loss: 0.1263, train acc: 0.9579, val loss: 0.1361, val acc: 0.9622  (best train acc: 0.9624, best val acc: 0.9659)\n",
      "[Epoch: 39800] train loss: 0.1245, train acc: 0.9592, val loss: 0.1358, val acc: 0.9605  (best train acc: 0.9624, best val acc: 0.9659)\n",
      "[Epoch: 39820] train loss: 0.1234, train acc: 0.9556, val loss: 0.1686, val acc: 0.9575  (best train acc: 0.9624, best val acc: 0.9659)\n",
      "[Epoch: 39840] train loss: 0.1166, train acc: 0.9573, val loss: 0.1549, val acc: 0.9619  (best train acc: 0.9626, best val acc: 0.9659)\n",
      "[Epoch: 39860] train loss: 0.1279, train acc: 0.9568, val loss: 0.1369, val acc: 0.9619  (best train acc: 0.9632, best val acc: 0.9659)\n",
      "[Epoch: 39880] train loss: 0.1215, train acc: 0.9586, val loss: 0.1333, val acc: 0.9602  (best train acc: 0.9632, best val acc: 0.9659)\n",
      "[Epoch: 39900] train loss: 0.1098, train acc: 0.9609, val loss: 0.1348, val acc: 0.9602  (best train acc: 0.9632, best val acc: 0.9659)\n",
      "[Epoch: 39920] train loss: 0.1387, train acc: 0.9442, val loss: 0.1499, val acc: 0.9605  (best train acc: 0.9632, best val acc: 0.9659)\n",
      "[Epoch: 39940] train loss: 0.1258, train acc: 0.9544, val loss: 0.1367, val acc: 0.9636  (best train acc: 0.9632, best val acc: 0.9659)\n",
      "[Epoch: 39960] train loss: 0.1317, train acc: 0.9500, val loss: 0.1580, val acc: 0.9599  (best train acc: 0.9632, best val acc: 0.9659)\n",
      "[Epoch: 39980] train loss: 0.1278, train acc: 0.9537, val loss: 0.1580, val acc: 0.9612  (best train acc: 0.9632, best val acc: 0.9659)\n",
      "[Epoch: 40000] train loss: 0.1305, train acc: 0.9545, val loss: 0.1630, val acc: 0.9585  (best train acc: 0.9632, best val acc: 0.9659)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtAAAAGrCAYAAADpWqADAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAABNmUlEQVR4nO3dd3hUVf7H8c9J7wUCARIg9N4jHUU6Yte1rt117a6urqi7dl1Wd939ubbF7qprd1fFggUrqKDSEaREqvReEpKc3x8zGWaSmeROyOSmvF/Pk4e5d+7c+eYy4icn33uOsdYKAAAAgDNRbhcAAAAA1CcEaAAAACAMBGgAAAAgDARoAAAAIAwEaAAAACAMBGgAAAAgDBEL0MaYp4wxm4wxC0M8b4wxDxpjlhtj5htj+keqFgAAAKCmxETw3M9IekjScyGenyipk/drkKRHvX9WKisry+bl5dVMhQAAAEAI33333RZrbbPy+yMWoK21nxtj8io55ARJz1nPSi5fG2MyjDEtrbUbKjtvXl6e5syZU5OlAgAAABUYY34Ott/NHugcSWv8ttd69wEAAAB1lpsB2gTZF3RdcWPMJcaYOcaYOZs3b45wWQAAAEBobgbotZJa+23nSlof7EBr7VRrbb61Nr9ZswptKAAAAECtcTNAvyXpXO9sHIMl7ayq/xkAAABwW8RuIjTG/EfSSElZxpi1km6TFCtJ1trHJL0r6RhJyyXtk3RBpGoBAAAAakokZ+E4s4rnraQrIvX+AAAAQCSwEiEAAAAQBgI0AAAAEAYCNAAAABAGAjQAAAAQBgI0AAAAEAYCNAAAABAGAjQAAAAQBgI0AAAAEIaILaQCAAAAVMeBgyU6WFKqklKrtIRYRUUZt0sKQIAGAABArXnmq1Wa/fN23XxMN/2yc78+WLRRQ9o31fJNezSme7aO/uunFV5TMGVS7RdaCVo4AAAA6pGSUqtX56zRwZJSrdm2T298v1aStGnXAb2/cIMkyVqr/UUlkjyjuXsKi2Wt1YeLN6qk1PrOtWNfkRav36W8ydN03/s/SpK27y3Suws2aOG6nfpw8UZJUmFxifYXlaik1Grn/oOSpG17i/T2vPW+cx0sKdUDHy7T3sJiSdLm3YXqffsHyps8TYXFnlr2Fhbr9rcXa9r8DRo25ROd8ugsTf18pS54ZrbueXdJ0PBc9j3XJcbaulVQVfLz8+2cOXPcLgMAACDiPlu2Wb1z0vX0zAJdNKydEuKidObUr/X96h0Bx3147ZE6+ZGZ2l1YrI7NU1RUXKrV2/YpMTZa+w+WVDhv5+wUXTumsy574fsqazh7UBu98M3qmvqWquXbW0areWpCrb+vMeY7a21+hf0EaAAA0BBYa3XgYKkS46Jr/Nzfr96upslxats0WSWlVkbStn1FapIUp5+37dOXy7dobLdstUgPDHlPfLFSHZqn6OguzfXSt54Q2rdNhto0SdKZU7/Wif1ydELfHM1asVXb9xXpr9OX6rkLByozKU63/m+hZizdXOPfS330+mVDNaBtZq2/LwEaAIB6bn9RiYpLS5WaEHtY59m+t0ipCTGKiQ7dyVlaalVUUqqEWE8Y3VNYrL2FxYqNjlKT5DhJUnFJqYr9frVurVRirfYWFmvemh0qtVaPf7FKr/52iCRpzs/bldc0SYXFpb7z3PvuEq3YvEc3TuiqlVv2KtoYXfFixVHRb28ZrbfmrteL36zWv84ZoLiYKE1570fddlwPtUhP0P/mrtM1L82VJN19Yk8t37RHX6/cqgFtM9UsNV6tM5P08IzlWrllr0Z0ytLors3VtmmyNu8uVLtmyToir4n+9N+Fenn2Gi29e4KslXbsP6gDB0s0b80O30jtlUd31EMzloe8bk2T43TqgFwd37eV5q/dqZveWCBJyk6L18ZdhWH8LcHff68Ypr6tM2r9fQnQAABUw7ertiknM1E5GYm+fS/PXq2XZ6/RG5cPq3D85t2F+nnrXuXnNan0vPPX7tCabfsVFxOlhNgojejUrMIxr8xZo/98u1rXj+uiAW0zNeqvn2r9zgOSpLjoKC2+c7xioqO0YvMe7T5QrOWb9mjb3kLd++6PyslI1Nju2UpLjNVXy7foylEddcHTs/WXU3rpxtc9oe7247rrqxVbtWbbPv34y26lxsfo1PxcbdpdqGnzN/jep3urNM1ds8NX119/1UfXvzrP8TU8rk+rgF7ZmvbEufm6+DmyQUP2xR+OVusmSbX+vgRoAECj8fGSjeqcner7H+62vUV64/u1euGb1Zp+7ZH6YfUOnfavWerbOkP/vcITgvcXlajbre/rjuN76Lyhedq5/6Cu+s8P+nyZ51fo064eru4t07Rh5wENnfKJJOm0/Fy9MmetZlw/Ul+v3Kp35q/XV8u3SpJ+e2R7/evzlZKk6Cij1y8bqpMf+Uqh7oUa1K6Jvlm1TbmZiVq7fb/SEmK060BxJC8TUG98/6exvt981CYCNACgwSsttbp/+lI9+ukKSVJuZqJapidodsF23zEXDMvT018V+LZf/M0gdWiWon9+8pOe/9rTo5qTkah1O/bXau0AQnNrGrtQAZpp7AAAjj3++Uot27g75PMHDpaouKRUkmd6rOWbAo+11urP7y3RTxt3q6TU6u1567WvyDPKumzjbn3y48aA4xev36UrXvheewuLtWzjbj348U8Bz6/bsV/Pzizwbb/xwzpfeJaktdv3B4RnSQHhWZLOevwbDbr3Y194LjsvAHddOKyd2yWExAg0ANQzJaVW367apiEdmtbYOW95c4Fe+Ga1CqZMUnFJqaKMqbDy14GDJer6p/clSY+e3V9Hd22uv36wVE98uUqf3TBSRkZH3j9D2Wnxevis/jr1sVmSpGV3T9Syjbu1cstedWiWrEkPfilJ6tYyTUs27KpQy2uXDvG91qkoIz10Vn9d7mBKLgB1V6fmKfpp0x5JnlHnvMnTdNagNrr3pF6u1EMLBwDUU8UlpfrT/xbp8pEd1LpJkh765Cf9dfoyvXDxIA3rmCXJM7JbXGoV6zerwvJNu/X6954R2RvGd9EVR3f0PVewZa8yk+L087a92rSrsMINWM1T43Vs71baV1SsPYXF+r8z+uml2at1y5sLa+ebBtDg9cpJV2FxiZZt3OPb99yFA3XuU99KqhurDxKgAaAe+mzZZsVEGZ39xDeSpDtP6KFb/7dIktS/TYZWb9uvm4/pqute8cyI8OCZ/XT1f36o9JzDOjb13egGAG4Z2qGperRK0+NfrPLt+/dFA7Vw3S498ulyLbh9vIvVeRCgAaCOs9bq96/O0xvfr9PzFw3Shc/MVpG3nxgA3Fad9qrK3HxMV104rJ027Dyg299apI9/3KQXfzNIQztk1dh7HK5QATrGjWIAAB7//WGdfvfyXKUnxmrn/oO+/b9+8hsXqwKAQElx0VXObR6O7/44Rk2S42SMUesmSfrLqb313MwCDW5Xc/d2RBKzcABALfho8caAgCxJswu26Xcvz5WkCs8BQDgGVhFu7z6xp6PzHNOrRdD9719zpCRp4R3j9eCZ/QKeu/OEHrphfBe9d82IoK99+vwjtOrPxwTsa5oSL2MO3aiclRKv68Z1qXDzcl3FCDQARNjGXQd08XNzNKJTlkpKrWau2KqU+BjtKWSRDADVc2zvlnrHu1qkJD1z4RHatrdIUz9fqedm/Vzh+ETvkuwdm6doeMcsvfjtahUVl6pP6wzN81tlsm/rDL274Bff9qReLfXw2f192ynxMTq+TyslxUYrKS5aQzsGtlv0yknXgnU7JUmPn5uvlZv36OiuzQOOyUqJr/43XkcwAg0ANejzZZu10Ps/j9Vb9ylv8jR9uNgzt/EXP23RzBWem/cIz0DDU9UosBOds1McHffPM/tp5b2HRnWT4mKUm5mkPx3bXVNO7qW0hBj1zk33PW+M9NM9E/X+NSN0+/E9lJ3mCbH3nNhTdxzf49D34G2huOP4HkqJj9HNk7oFff8x3bMrhGdJev7iQb7HIzpl6bdHdfBt//Cnsfpq8ih9esNIR99jXUaABoAadO5T3+rYf36p4pJSjfvHZ5KkP/6Xqd/QOCTHRevtK4fX6nse3aWZ3rtmhObdOq7Cc0fkZdbY+1w3tnOlz/9hQhc9c+ER+uyGkVp4x3j9bkyngOdP6Z8bsL38nokB21cc3UHTrz1SD53VP2D/u1eP0EuXDNZjvx6g+0/tLUka0y1bJshc7ZIUGx2lMwa20fzbxys57lCjgTGe52KiA6NfWkKszhua59vu2zpDK+49RucNzdPCO8YrJyOx0u+7vPTEWMVFB4+XmclxyslIVEp8/W+AIEADQA1YuXmPvl99aMW7jre8pwMHmUEDtePp849wuwRJ0tc3j1av3HTFxxyKF+EGsPL6t8nQpUd10Lju2UGfH9YxS91apik9KVZdslMDnmuZfnjv7e/q0YcC8V0n9NDNx3TV+UPzfL2/l4/sqKS4GLVtmqyU+Bj9bsyhwH3d2M7622l99NaVw3z7YqKjlJkU69vu3yZTnbNTlZF4aN/CO8are6s0DW7fVBN6tlBqgid4Om0TLjs+mMomYYs+zD7k247vrriYqJBBuiFouN8ZANSiUX/7TCc/MtPtMhDE5SM7VH1QNTx74UBHx7Vucngh7se7Jui0/NxKjzmyczNN6t1Svy83SpqbmaiJPVsoNtoTiE7pn6t5t47TqHI9qaEkxkbrxgldHR379AVHKDXBE/78g9PHvz+qwrEXDMur9Fz+YfWV3w7R5IldNfXcCjOJ6ZubR+ui4YeWe377quH68a4Jvu2kuGjf464tUgOC4f+d0VeSZwRbClw2+oqjPZ+Zd64arsd+PUBz/jgm4H1/PbitLjmyg24/voeuOLqjurVMC/p9XOm3eJEk9c7NCNjOyTz02WjdJEmSlOCteWLPFhVGast+KI+PPfR9dclODdk68pdTevseGwWG4rK/qyjvX9WwjjU3+8XZg9pq2d0T680NgdVR/8fQAQCoxNmD2+qRT1eE9ZrPbzhaR94/I+TzC24f5wsgVbl4eHslxkXrD6/NlyT9akCuXv1ubdBjH/v1AL08e7VaZSTqhW9WS5KijNGRnZvplTnBXyN5Rgwf9v7qf+oXK7X7QLHm3TpO6d4Rzvs/+FEPz1ihvKZJSk+KVd/WGfrkx02SpEuP6qDHPqt4fRbeMV6x0UbxMdHqnZvuW8ynvIIpk1RaagPCUlxMlFToaT9I8At7ZW47roee/qogYN/828fpuZkFyk5L0LgeLfTgxz9JUoWWA3/ZaQkB23ExgcfeMqmbzhnSVumJscrJSJS10nert6t5arzaNk3WCX1zfAsP+fcL3zC+q24Y7/nBoWdOusrznz0iXDOuH6ltewslSc9cMFCzV21Tz5x0X4BOS4jVe9eMULus5AqvLSwukSQl+H2fH1x7ZMj3ykyO03F9WunteetVvuQnz8vXuws2KDfT877PXjBQB0vq19ogbiJAA8BhuOKF7303CaJuiokyKpgySXmTp1V4bkj7ppq1suKqjG2aJlXYd9cJPfQn7yqQTsOz5Jn1YFjHLLVIS1B6Yqx6tEoLGaAn9GyhCT0904i9NHuNSkqtokzg6OF9p/ZWq/TE0HOFl2WgIBmvfIi6alRH/X5cF329cqvmrtmhjs1TtHyTZ1ll/9HPYR2z9OblQ1VSaoMupFF+pHFoxyy9PW+9o9H3uJgoffL7o5SWEKsrR3lGnmvqJtvUhFj1aBV4I90R5UZry66JVdXh8Ys/HH3YtbXLSvaF46yUeE3s1bLCMaFGtCf1bqUPF2/UdeMq78f2V9ZCk5kUF7C/VUaiLh7R3rcdEx2lmIo/6yAEAjQAHIZpCzZUfRBcFeqGpWlXD9eeA8WaNbXqZc0LpkySJPVrk6kmyXFVHH3IdWM7a5h3poIjOzcLedxvj2qv4nKjf6XeJtUoYzSo/aHQN657tjKS4vTFH47Wo5+tUEm515VtBfvteai+1zcvHypJ+ucny/XAh8uCHtOvjeeGvFV/PkYbdxVq8J8/Dvn93H9qb/1uTCdHP2gsvWtChRHdspaTygxsVzOLekR539ta6a4Te6p7iPAqHWqzcEtKfIyeOC+8fvfrxnZW39YZGtGp7qzu1xAQoAEADdIT5+arW6s0JXsDdL82Gfph9Q6N6JSlL37a4huZfOmSwTpj6teOzhns1/mSZ3qupRt3VzjPuUPahjzXSf1y9OYP6yRJY7tlV1jl7ZZjuunuaUtkjGeksizEl2ndJEn3ntSrwnnLgrd/KG2S7JmyLMMb/s8a1EafL9uscwa3DTj2qlEdQwboMsYYtUhP0FGdm2l8j+CLbiTERqtDs0PTsaXGx2h3iJHbYO0Q8TGe2TwSYgNbMv5xel/lZCZq576Dyq+hGTaap3quTXJ8jE7uX3mveX0UFxPl+60Gag4BGgDQII0pN2vD8xcN0pY9hcpOS9Auv5UfB+Y10TWjO+mcIW2Vf/dH1XqvzOQ436/lR3Zpph9W76h0dclld09UTJTR6m379N3P2yu0VkjSxSPaB/yK3anuLdM05+ftivEbgj5vSFulJsT4plLLSonXa5cNrfDacHp7nd5EKXn6dAu27NW0BRt8PzRUpVduxR9WTuyX4+i1V4/qqI7lZuQI5bpxndWxeUrIWT4Ox2G0SqOOI0ADAOq03MxErd2+/7DPkxwf4xuN9r+xLSrK6Frv7BUfXXek1u04EPC6yqYC85edlqBnLxyofm0ydPEzc/RtwbaQ04GV3ez2j9P76okvVqpv65qbr/jJ847Q0o27A77HmOgonZbfusbeI1ytMhLVKiNRQztm6Z4go+Y17bpxXRwfGx8TrV+5eG1QPzGNHQBUw9Y9hb4VBxFZr11acaS0Mk3D6FEur2PzVB3l7VV+1Lt88ec3HO349Ud1bqa0hFg9fm6+nr9oUJU9wK2bJOmOE3oe9ry7/tKTYmusPxiH5/QjWis3M1GnDmh4rSGNHSPQAFANA6r5q344c8P4LhrcvqmmL/7F0a/Bb5zQVX95/0dJniWOu1ZyI5hTE3u1rNB3XN4fJ3Xz3VznLz0pVsO5aavRy81M0pc3jnK7DEQAI9AAgBoXbFnlMuf7LRtcmQFtM3XTxG6VrphW5jK/xVJiY6LCminjcFw8or0GtK259gsA9QMBGgCqYK3Vf39Yp4MlnlXA/jZ9qcsV1b4RnbICVngrLybKBPQKpyeFbl24/fgeuta7zPGors01tMOhFdB+M6KdzjiiddCQnVnJOQGgNhGgAaASj322Qu1uele/e3muOt3yngbf+7H++clyt8uqEdeHWIzhyfPyNbJL4JzFx/VpFXRFuTKD2jfRk+Xmp/3ouopLOJf15l4zppNmXD9Sj5zdXy/+ZrCuHuVZ8jglPlZTTuntu9nPn//7r/rzMRWWty4bCWbiAwCRRoAG0ODtLyrR3jBXD/vf3HX6/SvzNOW9HwP2/7LrQIhXuOPmY7pW+7Vlq76V161lmm8+3PtP7a2bJnb1m/7M0xrxxLn5FV7XOTslYLtj88Dtri1S9cpvh/i222UlVwjlwVaDy06L12+PbK/n/KZNM8borEGBcyxbJ70eqFL5v8dIYYo31GfcRAigwet+2/uyVvrLKb10+hFtHL3mmpfmRraoGjK2ewtFR0WpU/MUzSnYpgf9RsfjY6JUWFzq+FxpCTHadaBYVtJxvVuqXdNk9cxJCzo3cO/W6br3pF7KzUzUuU99q965GcpICt13/Nuj2uumid1Cv3klacoYo5uO8bz2obP66dmZBZI8wVqSWqQlSPJbwZpgdlhevmSIVm3dG/H3mXPLGO0/WBLx9wEigQANoEEpKi7Vp0s3acG6nbpubGcZY3w3od34+gIdkddE7ZulaMaPm/TNqm06Z0hbFWzZq537D6pzdor2F5XquIe+dPebcGDm5FFatnG32mUl66Lh7SRJSzbsCjhmTPdsTZtfcanx0/Nbq1+bjAr7U+K9AdpaGWOCLmTh76xBnh9Gpl09XF28i1bMvXVs0Jv+TBWNFWcPaqOPl2zUmQMr/wHn2N6tdGzvVpIqLkvdKiNRP6zeoaQ4/td2ODKT45RZCzdhNk2Jj/h7AJHCvzIA6p1F63dq0fpdAQtDPDB9qf45Y3lAqBrQNlNtmiQFvHbU3z7TQ2f105Uv/iDJ0+Psr2yBC7esvPcYbd9XFHSavM7ZKVq2cY+kQwtT+CufW88fmhcQoONjonTzMd10nt8Ner1y0rXAO5912Uizk04I/0BctiS2pEpHoSuTnZagaVePCOs18d6/q07eloO/nNJbE3q0ULcamMIOACpDDzSAOmfG0k2at2ZHwL6XZ6/WV8u3SJImPfil/vDafO3cd1Azlm7S9r1FevCT5RWC3/lPz9aov31W4fxl4TmYojBaHmrKo2f315uXD1XBlEmKijJqmhKv5y8aVOG46ddWvCnPX9lS0qf0z9W9J/XSEXlNdJLf0sdnDmwTEJ4l6e2rhvsed2/lCZ7xsaH/11A2QpwS5Ca/yjhdzS8cZdfpYe+CJynxMTquT6safx8AKI8RaAB1zgVPz5Yk3yIWyzft0Y2vL5CkgJvQfvPvOfp21bbaL7CGTezVssK+qBAZ9ttbRismxJPje7TQ21cOD+hbPndIW735wzpHdfzj9L5auG6nmqcmhDzmurGddfXoToqNDm/85eIR7cI63ikWKwHgBkagAbjCWqt35q9XUXGpZvy4SRt27teqLXv1zFerfMeUllotXLdTYx44NIp82r9m+R43hPDs1JPneWa9aJ6aUOkiIb1y0wNu+uvXJlN/Ora7o/dIjo/RoPZNKz3GGBN2eJak+JjQU+ABQH3DCDSAiHr9u7VKiov2jbLuKyrWXe8sUcv0BD3w4bJKX/vAh8v00IyGMedy2Mq1o4zuln3Yp2R2CgCoGQRoABFTXFKq3786T5J0+3Hddfvbi9WpeYp+2rTH0es/XLwxkuW54sS+rfTfueurPK4sP7dvlqzHg8y5HA7mRwaAmkULB4DD9sGiX7SvqFglpVY3v7lABVs8c8je9c5i3zG3v+157DQ8S9LSjbtrtlAXjO0eOHL8wGl9ddnIDo5f3yItQR2aHd7CFmWr+qUmhF4KOzXMmwIBoDHjX0wAYVu/Y79Wb9unwe2bavH6Xfrtv7/Tyf1ydP6wPL34zWotWLtTb181XM/O+tntUl3z74sGakQnz3LYo/72qVZu9vxQ4XQsuCYHjX81IFd7DhTrnCFtgz4/4/qRSovALBmSdNPErlqx2fkPTQBQHxCgATj2y84Diok2GjrlE0meWTK27CmUJL3xwzq94Z3t4adNu32rxUG6/9Q+OuXRmb7tsnB89aiOFZaj9h3jjdo10bccEx2l3xzZPuTzZdPfRcJvj3I+2g4A9QUBGoAjB0tKNfjPHwfse+KLlbp72pIKxx44WKrb3lpUW6XVSVl+q6wNaJupnjlpWrgucKXAhLhotUgPPmVcS+/+AW2bRK5IAEC1EKABOPLKnDUV9gULz5B+PbhNhdXwnr9okJb+slvRUcY3ulyZjs1T9fHvj1Je08iNDgMAqoebCAH4bNp9QFv3FGremh1at2O/Zq3YqjOmzlJxSan2Fha7XV6d17apZ9nw3rkZFZ7LSIrzzbF8zuC26tg8Raf0z630fB2apSg6irnnAKCuYQQagM/Aez4Our/jLe/VciX106ReLfXIpyvUvdzoc3m5mUn66LrKl+UGANRdBGgAqCHH922lS0d2UFol08UBAOo/WjgAoAYRngGg4SNAA5AkfbZss9slAABQL9DCATQCf/zvArXPSlFiXLTG92ih1IQY3TNtia4a1VHb9xXpy5+2+FYKRPi6ZKc2iFUTAQDOEKCBBuzAwRL1uO0DlZQemjZt2vwNOnNgGz0zs0Db9hbprXnrXayw/vvvFcN042vz3S4DAFCLCNBAA7ZlT2FAeJakL5dv0ZfLt0iSSmpyvehGqm/rDN9jLicANA70QAMNWPnwXN60+RtqqZKGrSaW2wYA1B8EaKABK64iQKNmMPIMAI0LARpowP4962e3S2hUGIkGgMaBAA00YM/MLHC7BAAAGhwCNAAAABAGZuEAXLZm2z41S41XQmx0jZ73i59YGCUSpl97pDpnp+qB6UuVmRzndjkAABcQoAEXFZeUasR9MzSue7amnptfo+c+58lva/R88Chrc75uXBdX6wAAuIcWDsBFZfMwf7o0vNHiouJSXfzsHP0UYvW7nfsOHnZtCI4JNwAAEQ3QxpgJxpilxpjlxpjJQZ5PN8a8bYyZZ4xZZIy5IJL1NEY79x0MCFOPfrpCq7fuc7Gi2rFlT6EWrd9Z7dfvKyrWU1+uUmkV08D9sHq71mw7dD2XbNilvndOV97kafpw8cZKX7ts425t2HGgWvXNXbNDHy3ZqJvfXBD0+dvfXlSt86JqTFkHAIhYC4cxJlrSw5LGSlorabYx5i1r7WK/w66QtNhae5wxppmkpcaYF6y1RZGqq6Hasa9If52+VH+c1D2gl7bPndMlSZ2ap6hLi1S9M3+DXpq9Wp/dcHS13ueS5+Zo+uKNuvvEnvr14LYBz/3n29XavLtQV4/u5Ohcq7fu096iYnVrmVatWioz9oHPtH3fQRVMmVSt19/77hI9//Vq5WQmanyPFiGPO+mRmZKkFfceo+goo0c+XaEd3h9Y/jZ9qcZ2zw752nF//7xatTmxaz8j0JGSFFezveoAgPonkiPQAyUtt9au9AbilySdUO4YKynVGGMkpUjaJqk4gjU1WH+dvlTPf71ab3y/TpKnt9bfT5v26B3vqnP7i0qq/T7TvaOqd72zuMJzN72xQA98uEzvLtigkx/5qspzHXn/DE38vy8C9q3bsV95k6dp/Y79mr92hwq27A35+pe+Xa0Xv1kd9LntDloYrLV6dc6aoNdj537Px/DAQWfXqsPN74Zsp/Cc76D+76OfQq8MeJjzB//jo2V64ZtDcz5//OOmwzshKhjcvole/M0gtW6S5HYpAACXRTJA50ha47e91rvP30OSuklaL2mBpGustaVC2A4c9Fw2K6tVW/aq4y3v6a15612p5fIXvtf3q3do+qJfPDVZq+Wb9oTsy919wLO/pNRq2JRPJEmj/vapjn/oK43866ch32fyGwt085sLtGVPYZU1zV+7Q3mTp+n71du1bONuFRWXauaKrbrhtfm6a1rgDwOn/WuW3pkf/rU75dGZAdvLN+3R96u3S5Lufmex/v7RMn24+Jegry0qLg2rtebyF74LeO0/PvpJt7y5MOyaUbVFd4xXn9YZuv34HhraIcvtcgAAdUAkA3SwMbXyw2/jJc2V1EpSX0kPGWMq/D7fGHOJMWaOMWbO5s1MzRXMa9+t9T1evH6XJOn9hRuCHhtstbRX56zRc7MKfFOf3fXOYj03q0CSp8/382WB172wuFSbdlXev7t4g6eOp78q0JgHPlOfO6dr+94iPf75Slm/RtKy2SL+/uEy375gI7VFxaVat2N/0P1VKbtJ7/mvf9a4v3+uu95ZrD2FnlHmzbsPBXBrrb5dtS1kn+v3q7cH1B5QR0lpwHPFpVYnPzJT+4tK9Kr372fBup26+Nk5QWs+8v4ZFXquH/tshZ7+alWFY7fs8XQ5GRnZcv9ZHc5vGBCoT266kuNj9L8rhqlri9CtRo/8ur/OHtRGnZqn1mJ1AAC3RHIau7WSWvtt58oz0uzvAklTrCd1LDfGrJLUVVLA/FvW2qmSpkpSfn4+t/BEwA2vzfc9njl5lJ780hPazh2S5+vzvWpUx4DXHP/QV/r65tGSpBWb94Q8951+7R43vj5f0xdvVN82Gb59c9fskCR9tWJLpTXe9MYCvf79Wi24fZxSE2KDHmOt1S+VBPuy/uTvft6u4Z08o4kfLt6oGUs36eguzSt9/4+XbNRFz87RXSf00DlD8nTn2xXbWOYUbK+wb5VfG8rDM1ZIkn78ZVfw+sttT3nvR0nSBcPaBT2+1Fq9PPvQL3ryJk+r9HtAZHRolqJ7TurldhkAgFoSyRHo2ZI6GWPaGWPiJJ0h6a1yx6yWNFqSjDHZkrpIWhnBmhq8jbsOjaa+uyB4u4Dx/nLgnmmL9YfX5lUY1R3qbaMo75+fLA/Y9g+qo//2WdDXfLAosIbdBzyjvlWNGh8sCYySswu26fXvPaO4vW6fHvCc/5HPf/2zhvy5Yv3BBo2/XrnV9/iCp2frs2Whf7thrfUF4RWbPX8+VW5kuLC4NGh4Lz9CLHnaXEK9j+T5jYB/GO76p/e0Zts+FZeU6t53l/j2z/l5u279HzNuREpWSrzbJQAA6qCIjUBba4uNMVdK+kBStKSnrLWLjDGXep9/TNJdkp4xxiyQp+XjRmtt5cOQCLB9b1HArBsvfP2zzhrUptLXlLVwPP6FJwC+MmdtyGNv+1/1+2qNjB77bEXAvlne0DptQWB7yeB7P1bLjISg59l94KB+9disgH1/fu9QiHzyi1U6ZUCO1u84oK9XbQs47uRHvtIblw/zq8lj0+5CPf1VQcCx5z0VeuGRp78q0N3TloR8XgpverO12yu2ovh7eEbgDysHDpbq7fnrtWlXoZ6ZWeD8jXBYfpWf63YJAIA6KKIrEVpr35X0brl9j/k9Xi9pXCRrqM+stVq9bZ/aNk2u8Nyabft00iMzK9xAt3VvUYWR4sPx7Kyfqz6oEmX92OWVnz3jl10HlJEUvC2j/IizJP3rs0O/qJi3dodvNHhS75YBx32/eoe27S3yLVhSxsmNh5J059uLdWzvVgFtKM/MLNDtx/dw9HopvGBty/3pb8HanfppU+hWGdS8CT1bVn0QAKDRYSnvOuyprwp01zuL9faVw9UrN923f822fRpx34xqn9dIGnDXhzVQoYf/DYz+/v7RsqD7Q/nxl9DTwFXGPwxPm1/xxsn+ft/rfofT0pXZureowmiwJK2spOe7vLXba2bhmvcWBm/JAQAAtYulvOuw7372tCOs3hYYwC7zm8KsOtbvPKCte2tmrZriklJd/+q8GjlXdf0cxvRvM1dsrfqgch74sOIPAqNC9HwHc+nzwfudg/llZ/VWJgQAALWHAF1PfLNyq2+k92Bx3ZmIZEiIGw5RPSPum6FHPl0e1g8FAACgdtHCUYf5986ePvVrSdJxfVpq2abqtTpEgv8cyqgZ972/1O0SAABAJRiBrgeu+s+hFoBb/7sorJvSAFRPx+YpbpcAAKijCND1gP/idC/PWRP6QAA15tQBTGEHAAiOAF2HMdIMuCfIivcAAEgiQANAUEd1aeZ2CQCAOoqbCAGgnK8mj1JORqLbZQAA6ihGoAGgnKbJcW6XAACowwjQdczcNTs0c/kWt8sAGq2CKZOUEBvtdhkAgDqMFo465sSHv5IkfXjtkS5XAgAAgGAYga6jxv79cxVs3et2GUCjckp/pq4DAFSNAF2H/fhL3VlxEGgMrhndye0SAAD1AAEaAAAACAMB2kW/7DyggyWlbpcBAACAMBCgXbK/qESD//yxJr++wLdv+SZaNgAAAOo6ArRLDhwskSR9/ONG374xD3zuVjlAo3LVqI7q0zrD7TIAAPUUAdol1vvnjn0HtWj9Tu0rKna1HqAx+f24LvrfFcPcLgMAUE8RoF1y1zuLfY+nvPejCg/SCw3UtIIpkxwdl5vJst0AAOcI0C5584d1vsdf/LRFJz3ylYvVAHXbi78Z5Oi4wx1VNuawXg4AaCQI0HVEwdZ9bpcA1FlDO2SFfO7+U3urSXKcJCk1IfTiqqfls0gKAKBmsJR3LXt73nr9svOA22UA9d5dJ/TQ0I5Zap+VrNe+W6tvVm1TbHToMYHj++TolTlra7FCAEBDRYCuZVf95we3SwAahHOG5PkeTz0nXwvW7VRaYqx7BQEAGg1aOADUST1apTk+Nj0pVsM7ZQX0ML966ZAIVAUAAAEagEvuP7V3pc8fzg19xkhH5DVxfHyz1HhJUnQUdxECAKpGCwcAV6QmhNdu0So9QeuruH+gLP4mx4X3T9vUc/I1Y+kmtcpgOjsAQNUYgQZQJxkFjgZ/ceOoiL1Xs9R4nZbfOmLnBwA0LAToWjSnYJvbJQB1iK36ED/htFdYG965AQAIBwG6Fp362Cy3SwBqxL8vGhiw/fxFzhY6CUd1eqANK6EAAGoBAboW/Lx1LyNicN21YzqHdXxaJYuSjOjULGC7/ODwiE4VFz45sW8rvXfNCI3pli1J6tbS+Swb5c29dWzQ/fExnn/Szh+WV+1zAwBQFQJ0hC1Yu1NH3f+pnplZ4HYpaORiop2PzkZHGeVmJvm2j+rcTAVTJgUcc98pvX2zV/iP/PbOTVfz1ATPMX4zbXRolqJuLdP0xHn5KpgySW2bJldaQ2XVZiTFBd0fGx2lgimTdMP4rhXPF+SEKfExOrZ3y0rrAACgPGbhiLBVW/dKku54e7HLlQDONU0+FFB75qTp3pN7VTjmtCNa67Xv1mrz7sKAEejczMSgYfWiEe3CqqFVRqKuHNVJmUmRWxxl4R3jI3ZuAEDDxQh0hNG6gfropUsG+x5PObm3ckJM73b3ST11dJdm6tsmI2D/xJ4tJEn9Wh/an+RwarnstHg9dFY/3Xdqb43tnq38MOZzDiY5LvqwXg8AQHkE6AgrLC51uwQgbK0yEtWndbokKb2S5bE7Z6fq6QsGKj7mUEg1MhrdLVsFUyapU3Zqtd7/2N6twp4nOpTnLhqklukJ6p2bXiPnAwCAAB1h90xb4nYJgKTgvw35++l9Qh5/+/E99PaVw9W6SVLIY2rK21cO14sX1/xMHpI0oG2mZt00usYCOQAABOgI27n/oNslAD6vXzZUU88Z4Ns+qV9uQL+zv/iYaPWqxqht2UwY4eiVm672zVLCfh0AAG7gJkKggevQLFkrNu+VtZ7R2PJKa6hPf9ndE/XAh8t06VHtq/X6WO8sIW1qYcQbAIDDwQg00MB1rqIPuaTUeYC+oJL5leNiojR5YteQU8wFkxgbrZuP8Uw51zQlXlPPGaCp5+Q7fj0AAG5gBBpo4MqmlAsVk8MZgL7tuB667bgeYdfwqwG5QfcvuWtCwPa4Hi3CPjcAALWNAA00cJcc2UHJcTEhR4+DtXDU5OyLK+49psJKhdV11qA2emX2mmq//obxXZRWyawiAAA4QYAGGriU+Gjd/6vQs22E0cFRLdE1lZ4l3XtSL917UsVFXZy64uiONVYLAKDxIkBHUN7kaW6XAAR1zuC2vqW9a+omQgAAGgsCdITsLypxuwQgpLtO7Ol7HCxAB1uKGwAAeDALR4Tc+Pp8t0sAJFXdz+zfwuG74ZBBaQAAQiJAR8jSX3a7XQLgyO/HdZYkfXbDyGotggIAQGNDCwfQwFU1mHz5yI66fCQ31wEA4BTDTRHCjVmoz2yVsRsAgMaLAB0hP23a43YJgCQpJoxp5Iy4exAAgKoQoIEG7M8n91L7ZilulwEAQINCgAYasDMHtnG7BAAAGhwCdATsLSx2uwSgWga0zZQkRTERNAAAITELRwTMW7PD7RKAavnXOQO0asteJcRGu10KAAB1FiPQEVDCDByop5LjY9QzJ93tMgAAqNMI0BFQSn4GAABosAjQEVBKggYAAGiwCNARsGHnAbdLAAAAQIQQoCPg8S9Wul0CAAAAIoQAHQGrtux1uwQAAABECAEaAAAACAMBGqiD7julty4b2cHtMgAAQBAE6HoiITZKcdH8dTUWHZqnKInFTAAAqJNIZPXE5zccratHd5QkJRKsGgUmQwQAoG4iQNcTsXVw9PmV3w4J+Vz3lmkB26fl59b4+8fF1L1rUl7PnDT965wBVR4XG20cne/J8/IDtvOaJlWrLgAAUH11P4E0cvNuG6fnLxqkzOQ4ndgvR02T42o8jM754xj93xl9dXK/nLBe16FZcsjnXrnUE65T42MkeZaI/vPJvapfZBDRxlnodFNORqI6Nk+RJGWlxIU8bs4fxwZsGyMFWxF+dLfsgO27T6zZawoAAKpGgK7j0hNjNbxTliQpNzNJ3/1prH49uK3v+bm3jlVSnLOWjkHtmgTdn5USrxP65uiMgW0c11UwZZKapsT7tqeUC8cp8TEqmDJJ14zp5Pic064e7vhYSTpnSNuqDzoMvXLSD+v1Y7pl66+/6qMWaQmSpD9M6KoPfndk0GPTE2Mr7LN+TRxfTR6lGdePPKx6AABAzSBA10NZfsE1IylOSXExjl53Qt/wRpidOj2/tc4Y2Eb/vmhgyGOMKh8tHtSuiXq0ch5YOzVP0fF9WkmSurVM05w/jlHXFqmSpHPLBevzHAbtp88/ImA7Jd7ZdS1zwbA83XvSoR8kju3dUqkJsUr2/jBxWn5rdWmRqpmTRzk6n/8IdE5GotplhR7xBwAAtYcAXQ9lJpdvBXB2u5kN87a0Ds2SlRBb8SMyeWJX3+Pl90zUlFM8oXFEp2ZhnT+Y6dcGjtBGRwUP3vl5mb7HRp4fKk4d4GltyfaO+EqekfKrRh8aBS8/Uu7v6K7NFRPi/Qa0zQy6/w8Tuvge33ZcD5016NAoftsQ/cmtMhIDts84onXQ45z8bdWDLhYAABqcKgO0MeZKY0zw9ADX/HjXBH1+w9GOj//PbwYrNurQX7fTPup3rx6hG8Z7QmJWSpyW3DlBvz2yve/5mOgoGQcprqpDyp7vnJ0asH/5PRM177ZxAfsePzdftx3XwzdCW/bai4a30/J7JqppuR8wslLidecJPfTV5FFVtqkEq7NPbrpev2xohf3Xj+usy0d2DHmufm2q/s+mYMokTTmld/AngzVBAwAA1zkZgW4habYx5hVjzATjJC0h4hJio9XG4QwMR3VupiEdmupEv5sE+wcJdy3TEyrsa98sRb/yC9uJcdGVBuZvbh6txXeO920Hy4An9PW0XkQZT/tHZYwxSk+M9bVGHN+nlcZ2z1aC31R+ZeUYYxQTYraSc4fkKafcyG9VkuM979G6SfDrfOUo5/3dwVRWj5GUUEVv+xuXVwz1AAAg8qoM0NbaP0rqJOlJSedL+skYc68xpspl0ryBe6kxZrkxZnKIY0YaY+YaYxYZYz4Ls37oUJvDnSf00IfXBr9JTfJM+1YWWIONbbZukqTXLh2im/xaNAJV/bNTdlpC0J5sI2lU1+ZKjI3Wb0Z4RrB75WbohH6tqjyndCjM+gu3JaVMmxCBWJKGdczyPe6Vk6EHTutTYfaQ343ppHtO6lmt9y4z+5Yx+qCSvytjjC4c1k5nD2qjhXeMD3pM/zaZQf9G2tMrDQBARDm6S8paa40xv0j6RVKxpExJrxljPrTW/iHYa4wx0ZIeljRW0lp5RrHfstYu9jsmQ9IjkiZYa1cbY5of1nfTSD1/0SC9+cM6nTO4bZXtFFX9/iA/r4naZSXrz+/9qFFdvX8dh9FJUNav3TQlXtlpCVpy1wRJ0uuXDVXHZilas32fJKl1ZsVQe+GwdhX2BSulqhsUy+uVk67V2/YFfe7Rswfo+lfnadqCDZKkk/sfGn0/uX+OTuqXUyO93s1S4yt9vndOuqKijO45ydk0dYPaNVGUMZq1cqvnmm/Ze9g1AgCA4KoM0MaYqyWdJ2mLpCck3WCtPWiMiZL0k6SgAVrSQEnLrbUrved5SdIJkhb7HXOWpDestaslyVq7qbrfSGPWKTtVf5gQatRYmtCzhe/xmG7Zemn2GvVtnRHy+KYp8frm5tEBs31I1bth7eR+OYoy8s2YUabsprz0pHRNPWdA0FB663HdKz13qBZhp3UO7dBUg9s31bertunL5VskeVpU2nvnty5/ngdO6+vsxDUgKsTNjCGPN0a/H9dZpz42S5beaQAAIsrJCHSWpJOttT/777TWlhpjjq3kdTmS1vhtr5U0qNwxnSXFGmM+lZQq6f+stc85qKnBe+aCI2pkKecld04ImEljTPdsLb9nYshe4TL+M1mUrYLYtpLWh1CiokzAKG4w43q0qPR5SUpN8HxU/RcjKbs+5YNuVaPwZa0fZw1qo2N7t9KBgyXase9glTU41b5ZslZvDT7CHSlWlhk5AACoJU4C9LuStpVtGGNSJXW31n5jrV1SyeuC/e+8fCaMkTRA0mhJiZJmGWO+ttYuCziRMZdIukSS2rRxvthHfdW3dYZGdqmZbpbEIDeiVRWey8tMjtPj5+YrP8RUbpFQfi7no7s0132n9q4wki1V/KCVzQddlbLWj4TYaLVIP3Sdykag86rZS/zRtUdV63XVQmgGAKDWOQnQj0rq77e9N8i+YNZK8p9iIVfS+iDHbLHW7pW01xjzuaQ+kgICtLV2qqSpkpSfn9+gfj+d1zRJ3Vqm6b2Fv/j2hZr7OFwvXFx+wL/6xnbPrvqgGlIwZVKFfcYYnVZuxo5QrQpNUyrvL67KiX1zlNc0udI2F3/3nNRTnZofCu3htl8AAID6xUmANtYvqXhbN5y8brakTsaYdpLWSTpDnp5nf/+T9JD3fHHytHj83VHldVS4/adW0qO/HiBJyps8TZJnerea4D+jRDBdW6SquLQB/DxSrnfhcC+fMcbRHM5lzh4U2SXFy+veMk1jvD/QxHjn9vaf1g8AAESWkyC80nsj4aPe7cslrazqRdbaYmPMlZI+kBQt6Slr7SJjzKXe5x+z1i4xxrwvab6kUklPWGsXVucbqSsWrd912Oe4evThzS98/6m9lZ4YW+Vx7/8u9DRq9UG3lmnqlZOuW48NfrNhqFUFm6d6+ruDTY1XH7x7zQjf4/y2mbp6dCedM7itUhNiNLh9E/1xUncd+88vXawQAICGzUmAvlTSg5L+KM+A6cfy9iNXxVr7rjw91P77Hiu3fb+k+52crz4oLC4N6/gnzzvC9/jCYe2Unhh72NOk/aqKxUkaioTYaL191fAK+8sGpMvPIlJm8sSu6tEqTUd1Pvzp6NwWFWV03djOvu2XLhkiSfrLKb3Ux2ELCgAACE+VAdo7tdwZtVBLg/DlT1scH9s5O0Udm6f4tquatg3hCbXQSkJsdIP/IeP0Ixr+zbYAALjFyTzQCZIuktRDkm9uM2vthRGsq976+0fLqj7IK9wFQOBMfb+u5w/Nc7sEAABQCSfzmf1bUgtJ4yV9Js9sGrsjWVR95fQGQgJSZDVPjddZg9roqfOPqPrgOihUTzcAAKgbnATojtbaP0naa619VtIkSc7WF25kflizw9FxZwxs2O0DbouKMrr3pF7q0Srd7VKqhWnwAACo25wE6LIl2nYYY3pKSpeUF7GK6rGDYd5ACAAAgPrHySwcU40xmfLMwvGWpBRJf4poVfWUkwaOlukJvinm+rXJiGg9AAAAqHmVBmhjTJSkXdba7ZI+l9S+Vqqqhy7993f6bvX2kM+P7Z6tDxdv1FtXDlez1Hi9e/UIdWhevaWiAQAA4J5KA7R31cErJb1SS/XUW+8v+qXS5x8/Nz9gu3urtEiWAwAAgAhx0gP9oTHmemNMa2NMk7KviFdWjyxav9PtEgAAAFBLnPRAl833fIXfPivaOXw27S50uwQAAADUEicrEbarjULqs6omHevLksoAAAANhpOVCM8Ntt9a+1zNl9Pw3HF8D505kGWVAQAAGgonLRz+y7klSBot6XtJBGivyqava5Yar7gYJ63mAAAAqA+ctHBc5b9tjEmXZ3lveP3xzYVulwAAAIBaUp2h0X2SOtV0IfXZuh373S4BAAAAtcRJD/TbOtSlECWpu5gX2jHrZHlCAAAA1BtOeqD/6ve4WNLP1tq1EaoHAAAAqNOcBOjVkjZYaw9IkjEm0RiTZ60tiGhl9cRnyza7XQIAAABqkZMe6Fcllfptl3j3QdJ5T30b8rnWTRI1vGNWLVYDAACASHMSoGOstUVlG97HcZErqeH4+LqRSk+KdbsM1BMZfFYAAKgXnLRwbDbGHG+tfUuSjDEnSNoS2bIaBlPVEoWAn+nXHqmNO1kWHgCAus5JgL5U0gvGmIe822slBV2dEED1NU9NUPPUBLfLAAAAVaiyhcNau8JaO1ie6et6WGuHWmuXR760umvnvoMa/bdPtaewuNLjYqNZgRAAAKChqTLhGWPuNcZkWGv3WGt3G2MyjTF310ZxdVWfO6drxea96nnbB26XAgAAgFrmZIh0orV2R9mGtXa7pGMiVlEdt7+oxO0SAAAA4CInATraGBNftmGMSZQUX8nxDdovuw64XQIAAABc5OQmwuclfWyMeVqeJb0vlPRcRKsCAAAA6qgqA7S19j5jzHxJYyQZSXdZaxtt8+8905a4XQIAAABc5GQEWtba9yW9b4xJlnSSMWaatXZSZEurmz5astHtEgAAAOAiJ7NwxBljTjTGvCJpg6TRkh6LeGUAAABAHRRyBNoYM1bSmZLGS5oh6d+SBlprL6il2uq1Xw3IdbsEAAAAREBlLRwfSPpC0nBr7SpJMsb8X61U1QCkJca6XQIAAAAioLIAPUDSGZI+MsaslPSSpOhaqaqOWrJhl+NjrY1gIQAAAHBNyB5oa+0P1tobrbUdJN0uqZ+kOGPMe8aYS2qrwLrkxW9WOz6WVbwBAAAaJkcxz1r7lbX2Skk5kv4haUgkiwIAAADqKkfT2JWx1pbK0xvdKOeBnrVyq+Nj2zRNjmAlAAAAcAuNBmH4eetet0sAAACAywjQEdIhixFoAACAhshRC4cxJlpStv/x1lrnd9Q1Mn85pZeGdsxyuwwAAABEQJUB2hhzlaTbJG2UVOrdbSX1jmBd9dqAtplulwAAAIAIcTICfY2kLtZa53fQNXLMAQ0AANBwOemBXiNpZ6QLqQ8IxgAAAHAyAr1S0qfGmGmSCst2WmsfiFhV9ZwxblcAAACASHESoFd7v+K8XwAAAECjVWWAttbeURuF1AfFpc56OOJjoiNcCQAAANwSMkAbY/5hrf2dMeZteWbdCGCtPT6ildVTGUmxat0kye0yAAAAECGVjUD/2/vnX2ujkIZiUq+WbpcAAACACAoZoK2133n//Kz2yqn/oriDEAAAoEGrcho7Y0wnY8xrxpjFxpiVZV+1UVx9ct8pnnVlMpJiXa4EAAAAkeRkFo6n5VmJ8O+SjpZ0gSSGWf3cc1JPnTIgV/uKinXmoDZulwMAAIAIcrKQSqK19mNJxlr7s7X2dkmjIltW/XLGEW0UHWV0/rB2zMABAADQwDkZgT5gjImS9JMx5kpJ6yQ1j2xZ9QvD8QAAAI2HkxHo30lKknS1pAGSfi3pvAjWBAAAANRZlY5AG2OiJZ1mrb1B0h55+p9RDhNvAAAANB4hR6CNMTHW2hJJA4whIlaGywMAANB4VDYC/a2k/pJ+kPQ/Y8yrkvaWPWmtfSPCtQEAAAB1jpObCJtI2irPzBtWnnvmrCQCtKQ7T+jhdgkAAACoRZUF6ObGmOskLdSh4FzGRrSqeuSkfjlulwAAAIBaVFmAjpaUouCztBGgAQAA0ChVFqA3WGvvrLVKAAAAgHqgsnmgmVrCAWbgAAAAaFwqC9Cja60KAAAAoJ4IGaCttdtqs5D6KooBaAAAgEbFyVLeqERSnJOZAAEAANBQEKAPQ1oC4RkAAKCxIUAfhtPyW7tdAgAAAGoZAfowHNm5mdslAAAAoJYRoAEAAIAwEKAPA1NAAwAAND4E6MNgWGsGAACg0YlogDbGTDDGLDXGLDfGTK7kuCOMMSXGmFMjWQ8AAABwuCIWoI0x0ZIeljRRUndJZxpjuoc47i+SPohULZFCCwcAAEDjE8kR6IGSlltrV1priyS9JOmEIMddJel1SZsiWEtEkJ8BAAAan0gG6BxJa/y213r3+RhjciSdJOmxCNYRMW2zkt0uAQAAALUskkvpBRugteW2/yHpRmttiamkH8IYc4mkSySpTZs2NVXfYVl69wTFx0S7XQYAAABqWSQD9FpJ/kv15UpaX+6YfEkvecNzlqRjjDHF1tr/+h9krZ0qaaok5efnlw/hriA8AwAANE6RDNCzJXUyxrSTtE7SGZLO8j/AWtuu7LEx5hlJ75QPzwAAAEBdErEAba0tNsZcKc/sGtGSnrLWLjLGXOp9vl72PQMAAKBxi+QItKy170p6t9y+oMHZWnt+JGupSQVTJrldAgAAAFzCSoQAAABAGAjQAAAAQBgI0AAAAEAYCNAOlZbWidnzAAAA4DICtEMllgANAAAAArRjpQRoAAAAiADtWGmp2xUAAACgLiBAO0QLBwAAACQCtGO0cAAAAEAiQDvGLBwAAACQCNCOkZ8BAAAgEaAdKyFBAwAAQARox+iBBgAAgESAdmzHvoNulwAAAIA6gADt0L3vLnG7BAAAANQBBGiHFq3f5XYJAAAAqAMI0A5lpcS5XQIAAADqAAK0Q71z090uAQAAAHUAARoAAAAIAwHaISPjdgkAAACoAwjQAAAAQBgI0AAAAEAYCNAO7dhf5HYJAAAAqAMI0A59sGij2yUAAACgDiBAAwAAAGEgQAMAAABhIEADAAAAYSBAAwAAAGEgQAMAAABhIEADAAAAYSBAh2lSr5ZulwAAAAAXEaAdSo2PkSRlpyW4XAkAAADcRIAGAAAAwkCAdsi6XQAAAADqBAK0Q3sKi90uAQAAAHUAATpMyzfvcbsEAAAAuIgAHaZo43YFAAAAcBMBOkyds1PdLgEAAAAuIkCHaVTX5m6XAAAAABcRoMMUHUUPBwAAQGNGgA6TMQRoAACAxowAHSZGoAEAABo3AnSYyM8AAACNGwE6TDFRXDIAAIDGjDTo0On5rSVJ3VoyjR0AAEBjRoB2qLC4RBI3EQIAADR2BGiH/jt3vdslAAAAoA4gQAMAAABhIEADAAAAYSBAAwAAAGEgQAMAAABhIEADAAAAYSBAAwAAAGEgQAMAAABhIEADAAAAYSBAAwAAAGEgQAMAAABhIEADAAAAYSBAAwAAAGEgQAMAAABhIEADAAAAYSBAAwAAAGEgQAMAAABhIEADAAAAYSBAAwAAAGEgQDtgrXW7BAAAANQRBGgHyM8AAAAoQ4AGAAAAwkCAdoABaAAAAJQhQIfhlP65bpcAAAAAl0U0QBtjJhhjlhpjlhtjJgd5/mxjzHzv10xjTJ9I1lNdZTcRtm2a5HIlAAAAcFvEArQxJlrSw5ImSuou6UxjTPdyh62SdJS1trekuyRNjVQ9NcG4XQAAAABcF8kR6IGSlltrV1priyS9JOkE/wOstTOttdu9m19LqpM9EvRAAwAAoEwkA3SOpDV+22u9+0K5SNJ7EaznsBmGoAEAABq9mAieO1jcDDqYa4w5Wp4APTzE85dIukSS2rRpU1P1OcY80AAAACgTyRHotZJa+23nSlpf/iBjTG9JT0g6wVq7NdiJrLVTrbX51tr8Zs2aRaRYJwxD0AAAAI1eJAP0bEmdjDHtjDFxks6Q9Jb/AcaYNpLekHSOtXZZBGs5LJYuaAAAAHhFrIXDWltsjLlS0geSoiU9Za1dZIy51Pv8Y5JuldRU0iPe0d1ia21+pGoCAAAADlcke6BlrX1X0rvl9j3m9/hiSRdHsoaaUNYDTQcHAAAAWIkQAAAACAMBOgyGpVQAAAAaPQK0A0xjBwAAgDIE6DDQAw0AAAACtANMYwcAAIAyBOgwMAANAAAAArQDTGMHAACAMgRoAAAAIAwEaAfKOqCZxg4AAAAEaAAAACAMBGgHrLcJmh5oAAAAEKABAACAMBCgHWAWaAAAAJQhQAMAAABhIEA7cGgeaJqgAQAAGjsCtBNlAdrdKgAAAFAHEKABAACAMBCgHbBiGjsAAAB4EKABAACAMBCgHbD0QAMAAMCLAA0AAACEgQDtQNlCKkxjBwAAAAI0AAAAEAYCtAPWMgsHAAAAPAjQYSA/AwAAgADtgK36EAAAADQSBOhw0MMBAADQ6BGgHbAMQQMAAMCLAB0Gxp8BAABAgHbA0gUNAAAALwK0E2VLeTMEDQAA0OgRoAEAAIAwEKAd8C3lTRc0AABAo0eABgAAAMJAgHbA0gMNAAAALwJ0GMjPAAAAIEA7wDR2AAAAKEOADgMtHAAAACBAO8BS3gAAAChDgA4D09gBAAAgxu0C6oMmyXF68TeD1LFZitulAAAAwGUEaAcSYqM1tEOW22UAAACgDqCFAwAAAAgDARoAAAAIAwEaAAAACAMBGgAAAAgDARoAAAAIAwEaAAAACAMBGgAAAAgDARoAAAAIAwEaAAAACAMBGgAAAAgDARoAAAAIAwEaAAAACAMBGgAAAAgDARoAAAAIAwEaAAAACIOx1rpdQ1iMMZsl/ezS22dJ2uLSe9dHXK/wcL3Cw/UKD9crPFyv8HC9wsP1Co+b16uttbZZ+Z31LkC7yRgzx1qb73Yd9QXXKzxcr/BwvcLD9QoP1ys8XK/wcL3CUxevFy0cAAAAQBgI0AAAAEAYCNDhmep2AfUM1ys8XK/wcL3Cw/UKD9crPFyv8HC9wlPnrhc90AAAAEAYGIEGAAAAwkCAdsAYM8EYs9QYs9wYM9ntetxkjCkwxiwwxsw1xszx7mtijPnQGPOT989Mv+Nv8l63pcaY8X77B3jPs9wY86Axxrjx/dQ0Y8xTxphNxpiFfvtq7PoYY+KNMS97939jjMmr1W+whoW4XrcbY9Z5P2NzjTHH+D3X2K9Xa2PMDGPMEmPMImPMNd79fMaCqOR68RkLwhiTYIz51hgzz3u97vDu5/MVRCXXi89XJYwx0caYH4wx73i36+fny1rLVyVfkqIlrZDUXlKcpHmSurtdl4vXo0BSVrl990ma7H08WdJfvI+7e69XvKR23usY7X3uW0lDJBlJ70ma6Pb3VkPX50hJ/SUtjMT1kXS5pMe8j8+Q9LLb33MErtftkq4PcizXS2opqb/3caqkZd7rwmcsvOvFZyz49TKSUryPYyV9I2kwn6+wrxefr8qv23WSXpT0jne7Xn6+GIGu2kBJy621K621RZJeknSCyzXVNSdIetb7+FlJJ/rtf8laW2itXSVpuaSBxpiWktKstbOs51P+nN9r6jVr7eeStpXbXZPXx/9cr0kaXfaTd30U4nqFwvWydoO19nvv492SlkjKEZ+xoCq5XqE09utlrbV7vJux3i8rPl9BVXK9QmnU10uSjDG5kiZJesJvd738fBGgq5YjaY3f9lpV/g9wQ2clTTfGfGeMucS7L9tau0Hy/A9LUnPv/lDXLsf7uPz+hqomr4/vNdbaYkk7JTWNWOXuudIYM994WjzKfp3H9fLj/dVkP3lGvfiMVaHc9ZL4jAXl/fX6XEmbJH1oreXzVYkQ10vi8xXKPyT9QVKp3756+fkiQFct2E8ujXnqkmHW2v6SJkq6whhzZCXHhrp2XFOP6lyfxnDtHpXUQVJfSRsk/c27n+vlZYxJkfS6pN9Za3dVdmiQfY3umgW5XnzGQrDWllhr+0rKlWe0r2clh3O9gl8vPl9BGGOOlbTJWvud05cE2VdnrhcBumprJbX2286VtN6lWlxnrV3v/XOTpDflaXHZ6P2Virx/bvIeHurarfU+Lr+/oarJ6+N7jTEmRlK6nLdA1AvW2o3e/ymVSnpcns+YxPWSJBljYuUJgy9Ya9/w7uYzFkKw68VnrGrW2h2SPpU0QXy+quR/vfh8hTRM0vHGmAJ52mFHGWOeVz39fBGgqzZbUidjTDtjTJw8TelvuVyTK4wxycaY1LLHksZJWijP9TjPe9h5kv7nffyWpDO8d8W2k9RJ0rfeX9HsNsYM9vYmnev3moaoJq+P/7lOlfSJtweswSj7h9TrJHk+YxLXS97v70lJS6y1D/g9xWcsiFDXi89YcMaYZsaYDO/jREljJP0oPl9BhbpefL6Cs9beZK3NtdbmyZOlPrHW/lr19fNl68AdmXX9S9Ix8ty9vULSLW7X4+J1aC/PHbHzJC0quxby9Bd9LOkn759N/F5zi/e6LZXfTBuS8uX5R2WFpIfkXdSnvn9J+o88v7I7KM9PwhfV5PWRlCDpVXlupvhWUnu3v+cIXK9/S1ogab48/xi25Hr5vs/h8vw6cr6kud6vY/iMhX29+IwFv169Jf3gvS4LJd3q3c/nK7zrxeer6ms3Uodm4aiXny9WIgQAAADCQAsHAAAAEAYCNAAAABAGAjQAAAAQBgI0AAAAEAYCNAAAABAGAjQA1CPGmBJjzFy/r8k1eO48Y8zCqo8EgMYtxu0CAABh2W89SwcDAFzCCDQANADGmAJjzF+MMd96vzp697c1xnxsjJnv/bONd3+2MeZNY8w879dQ76mijTGPG2MWGWOme1dYAwD4IUADQP2SWK6F43S/53ZZawfKszLXP7z7HpL0nLW2t6QXJD3o3f+gpM+stX0k9ZdndVHJs1zuw9baHpJ2SDolot8NANRDrEQIAPWIMWaPtTYlyP4CSaOstSuNMbGSfrHWNjXGbJFnKeGD3v0brLVZxpjNknKttYV+58iT9KG1tpN3+0ZJsdbau2vhWwOAeoMRaABoOGyIx6GOCabQ73GJuFcGACogQANAw3G635+zvI9nSjrD+/hsSV96H38s6TJJMsZEG2PSaqtIAKjvGFkAgPol0Rgz12/7fWtt2VR28caYb+QZHDnTu+9qSU8ZY26QtFnSBd7910iaaoy5SJ6R5sskbYh08QDQENADDQANgLcHOt9au8XtWgCgoaOFAwAAAAgDI9AAAABAGBiBBgAAAMJAgAYAAADCQIAGAAAAwkCABgAAAMJAgAYAAADCQIAGAAAAwvD/Em2H21ONYnkAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 864x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99       154\n",
      "           1       0.96      0.98      0.97       703\n",
      "           2       0.92      0.91      0.91       702\n",
      "           3       0.94      0.93      0.94       703\n",
      "           4       0.99      1.00      0.99       702\n",
      "\n",
      "    accuracy                           0.96      2964\n",
      "   macro avg       0.96      0.96      0.96      2964\n",
      "weighted avg       0.96      0.96      0.96      2964\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 504x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 504x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "gnn = GNN7L_SAGE(data_with_nedbit).to(device)\n",
    "pred = train(gnn, data_with_nedbit.to(device), 40000, cm_title='GAT7L_multiclass_16HC', classes=['P', 'LP', 'WN', 'LN', 'RN'], weight_decay=0.0005, arch='SAGE', layers=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "marker": {
          "color": "rgb(51,161,253)"
         },
         "name": "Precision",
         "orientation": "h",
         "type": "bar",
         "x": [
          0.33,
          0.57,
          0.94,
          0.55,
          0.45,
          0.9,
          0.36,
          0.44,
          0.96,
          0.32
         ],
         "y": [
          "GAT-2L",
          "GCN-2L",
          "SAGE-2L",
          "GAT-4L",
          "GCN-4L",
          "SAGE-4L",
          "GAT-7L",
          "GCN-7L",
          "SAGE-7L",
          "AP-GCN"
         ]
        },
        {
         "marker": {
          "color": "rgb(253,202,64)"
         },
         "name": "Recall",
         "orientation": "h",
         "type": "bar",
         "x": [
          0.37,
          0.6,
          0.94,
          0.51,
          0.48,
          0.86,
          0.4,
          0.47,
          0.96,
          0.3
         ],
         "y": [
          "GAT-2L",
          "GCN-2L",
          "SAGE-2L",
          "GAT-4L",
          "GCN-4L",
          "SAGE-4L",
          "GAT-7L",
          "GCN-7L",
          "SAGE-7L",
          "AP-GCN"
         ]
        },
        {
         "marker": {
          "color": "rgb(247,152,36)"
         },
         "name": "F1-Score",
         "orientation": "h",
         "type": "bar",
         "x": [
          0.32,
          0.58,
          0.95,
          0.5,
          0.46,
          0.88,
          0.36,
          0.45,
          0.96,
          0.28
         ],
         "y": [
          "GAT-2L",
          "GCN-2L",
          "SAGE-2L",
          "GAT-4L",
          "GCN-4L",
          "SAGE-4L",
          "GAT-7L",
          "GCN-7L",
          "SAGE-7L",
          "AP-GCN"
         ]
        }
       ],
       "layout": {
        "barmode": "group",
        "height": 600,
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Metrics of every architecture and depth"
        },
        "width": 500,
        "xaxis": {
         "title": {
          "text": "Score"
         }
        },
        "yaxis": {
         "title": {
          "text": "Models"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "models = ['GAT-2L', 'GCN-2L', 'SAGE-2L', 'GAT-4L', 'GCN-4L', 'SAGE-4L', 'GAT-7L', 'GCN-7L', 'SAGE-7L', 'AP-GCN'] \n",
    "precisions = [0.33, 0.57, 0.94, 0.55, 0.45, 0.90, 0.36, 0.44, 0.96, 0.32]\n",
    "recalls = [0.37, 0.60, 0.94, 0.51, 0.48, 0.86, 0.40, 0.47, 0.96, 0.30]\n",
    "f1scores = [0.32, 0.58, 0.95, 0.50, 0.46, 0.88, 0.36, 0.45, 0.96, 0.28]\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Bar(\n",
    "    y=models,\n",
    "    x=precisions,\n",
    "    name='Precision',\n",
    "    marker_color='rgb(51,161,253)',\n",
    "    orientation='h'\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Bar(\n",
    "    y=models,\n",
    "    x=recalls,\n",
    "    name='Recall',\n",
    "    marker_color='rgb(253,202,64)',\n",
    "    orientation='h'\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Bar(\n",
    "    y=models,\n",
    "    x=f1scores,\n",
    "    name='F1-Score',\n",
    "    marker_color='rgb(247,152,36)',\n",
    "    orientation='h'\n",
    "))\n",
    "\n",
    "\n",
    "# Here we modify the tickangle of the xaxis, resulting in rotated labels.\n",
    "fig.update_layout(\n",
    "    barmode='group',\n",
    "    title='Metrics of every architecture and depth',\n",
    "    xaxis_title='Score',\n",
    "    yaxis_title='Models',\n",
    "    width=500,\n",
    "    height=600\n",
    "    )\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "marker": {
          "color": "rgb(79,18,113)"
         },
         "name": "Time",
         "orientation": "h",
         "type": "bar",
         "x": [
          18,
          8,
          15,
          41,
          16,
          15,
          72,
          29,
          27
         ],
         "y": [
          "GAT-2L",
          "GCN-2L",
          "SAGE-2L",
          "GAT-4L",
          "GCN-4L",
          "SAGE-4L",
          "GAT-7L",
          "GCN-7L",
          "SAGE-7L",
          "AP-GCN"
         ]
        }
       ],
       "layout": {
        "barmode": "group",
        "height": 600,
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Training times for each GNN and depth (in minutes)"
        },
        "width": 500,
        "xaxis": {
         "title": {
          "text": "Minutes"
         }
        },
        "yaxis": {
         "title": {
          "text": "Models"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_times = [18, 8, 15, 41, 16, 15, 72, 29, 27]\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Bar(\n",
    "    y=models,\n",
    "    x=training_times,\n",
    "    name='Time',\n",
    "    text=training_times,\n",
    "    marker_color='rgb(79,18,113)',\n",
    "    orientation='h',\n",
    "    textposition='outside'\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    barmode='group',\n",
    "    title='Training times for each GNN and depth (in minutes)',\n",
    "    xaxis_title='Minutes',\n",
    "    yaxis_title='Models',\n",
    "    width=500,\n",
    "    height=600\n",
    "    )\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "marker": {
          "color": "rgb(51,161,253)"
         },
         "name": "Precision",
         "orientation": "h",
         "type": "bar",
         "x": [
          0.96,
          1
         ],
         "y": [
          "Multiclass",
          "Binary"
         ]
        },
        {
         "marker": {
          "color": "rgb(253,202,64)"
         },
         "name": "Recall",
         "orientation": "h",
         "type": "bar",
         "x": [
          0.96,
          0.97
         ],
         "y": [
          "Multiclass",
          "Binary"
         ]
        },
        {
         "marker": {
          "color": "rgb(247,152,36)"
         },
         "name": "F1-Score",
         "orientation": "h",
         "type": "bar",
         "x": [
          0.96,
          0.99
         ],
         "y": [
          "Multiclass",
          "Binary"
         ]
        }
       ],
       "layout": {
        "barmode": "group",
        "height": 400,
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Metrics of SAGE-7L in binary and multiclass classification settings"
        },
        "width": 600,
        "xaxis": {
         "title": {
          "text": "Score"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "models = ['Multiclass', 'Binary'] \n",
    "precisions = [0.96, 1]\n",
    "recalls = [0.96, 0.97]\n",
    "f1scores = [0.96, 0.99]\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Bar(\n",
    "    y=models,\n",
    "    x=precisions,\n",
    "    name='Precision',\n",
    "    marker_color='rgb(51,161,253)',\n",
    "    orientation='h'\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Bar(\n",
    "    y=models,\n",
    "    x=recalls,\n",
    "    name='Recall',\n",
    "    marker_color='rgb(253,202,64)',\n",
    "    orientation='h'\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Bar(\n",
    "    y=models,\n",
    "    x=f1scores,\n",
    "    name='F1-Score',\n",
    "    marker_color='rgb(247,152,36)',\n",
    "    orientation='h'\n",
    "))\n",
    "\n",
    "\n",
    "# Here we modify the tickangle of the xaxis, resulting in rotated labels.\n",
    "fig.update_layout(\n",
    "    barmode='group',\n",
    "    title='Metrics of SAGE-7L in binary and multiclass classification settings',\n",
    "    xaxis_title='Score',\n",
    "    width=600,\n",
    "    height=400\n",
    "    )\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6aeea6fe202f5c7201d5940e4573c0a76b23e4e16f0e3784ac81597546f2b3b3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
