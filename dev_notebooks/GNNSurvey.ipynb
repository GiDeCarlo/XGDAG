{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Survey of different GNNs methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Methods in this notebook:\n",
    "- GCN\n",
    "- GAT\n",
    "- GraphConv\n",
    "- GraphSage\n",
    "- AP-GCN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Methods are compared on the multiclass classification based on the NeDBIT features from the [NIAPU paper](https://arxiv.org/pdf/2108.06158.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Packets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.11.0\n",
      "2.0.4\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "import networkx as nx\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric\n",
    "from torch_geometric.utils import from_networkx\n",
    "from torch_geometric.data import InMemoryDataset\n",
    "from torch_geometric.nn.conv import SAGEConv, GATv2Conv, GraphConv, GCNConv\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(torch.__version__)\n",
    "print(torch_geometric.__version__)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GNN models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN7L_GCN (nn.Module):\n",
    "    def __init__(self, data):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(data.num_features, 16, improved=True)\n",
    "        self.conv2 = GCNConv(16, 16, improved=True)\n",
    "        self.conv3 = GCNConv(16, 16, improved=True)\n",
    "        self.conv4 = GCNConv(16, 16, improved=True)\n",
    "        self.conv5 = GCNConv(16, 16, improved=True)\n",
    "        self.conv6 = GCNConv(16, 16, improved=True)\n",
    "        self.conv7 = GCNConv(16, int(data.num_classes), improved=True)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        x = F.relu(self.conv3(x, edge_index))\n",
    "        x = F.relu(self.conv4(x, edge_index))\n",
    "        x = F.relu(self.conv5(x, edge_index))\n",
    "        x = F.relu(self.conv6(x, edge_index))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv7(x, edge_index)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "class GNN7L_GAT (nn.Module):\n",
    "    def __init__(self, data):\n",
    "        super().__init__()\n",
    "        self.conv1 = GATv2Conv(data.num_features, 16)\n",
    "        self.conv2 = GATv2Conv(16, 16)\n",
    "        self.conv3 = GATv2Conv(16, 16)\n",
    "        self.conv4 = GATv2Conv(16, 16)\n",
    "        self.conv5 = GATv2Conv(16, 16)\n",
    "        self.conv6 = GATv2Conv(16, 16)\n",
    "        self.conv7 = GATv2Conv(16, int(data.num_classes), dropout=0.3)\n",
    "    \n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        \n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        x = F.relu(self.conv3(x, edge_index))\n",
    "        x = F.relu(self.conv4(x, edge_index))\n",
    "        x = F.relu(self.conv5(x, edge_index))\n",
    "        x = F.relu(self.conv6(x, edge_index))\n",
    "        x = self.conv7(x, edge_index)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "class GNN7L_GraphConv (nn.Module):\n",
    "    def __init__(self, data):\n",
    "        super().__init__()\n",
    "        self.conv1 = GraphConv(data.num_features, 16, aggr='mean')\n",
    "        self.conv2 = GraphConv(16, 16, aggr='mean')\n",
    "        self.conv3 = GraphConv(16, 16, aggr='mean')\n",
    "        self.conv4 = GraphConv(16, 16, aggr='mean')\n",
    "        self.conv5 = GraphConv(16, 16, aggr='mean')\n",
    "        self.conv6 = GraphConv(16, 16, aggr='mean')\n",
    "        self.conv7 = GraphConv(16, int(data.num_classes), aggr='mean')\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        x = F.relu(self.conv3(x, edge_index))\n",
    "        x = F.relu(self.conv4(x, edge_index))\n",
    "        x = F.relu(self.conv5(x, edge_index))\n",
    "        x = F.relu(self.conv6(x, edge_index))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv7(x, edge_index)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "class GNN7L_Sage (nn.Module):\n",
    "    def __init__(self, data):\n",
    "        super().__init__()\n",
    "        self.conv1 = SAGEConv(data.num_features, 16, aggr='max')\n",
    "        self.conv2 = SAGEConv(16, 16, aggr='max')\n",
    "        self.conv3 = SAGEConv(16, 16, aggr='max')\n",
    "        self.conv4 = SAGEConv(16, 16, aggr='max')\n",
    "        self.conv5 = SAGEConv(16, 16, aggr='max')\n",
    "        self.conv6 = SAGEConv(16, 16, aggr='max')\n",
    "        self.conv7 = SAGEConv(16, int(data.num_classes), aggr='max')\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        x = F.relu(self.conv3(x, edge_index))\n",
    "        x = F.relu(self.conv4(x, edge_index))\n",
    "        x = F.relu(self.conv5(x, edge_index))\n",
    "        x = F.relu(self.conv6(x, edge_index))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv7(x, edge_index)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Loader for PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset class: it allows to translate a vector (Graph, Attributes, Labels)\n",
    "# into a dataset compatible with the PyTorch models.\n",
    "# \n",
    "# Parameters:\n",
    "# - G: NetworkX graph\n",
    "# - Labels: of the nodes used for classification\n",
    "# - attributes: List of the nodes' attributes\n",
    "\n",
    "class MyDataset(InMemoryDataset):\n",
    "  def __init__(self, G, labels, attributes, num_classes=2):\n",
    "    super(MyDataset, self).__init__('.', None, None, None)\n",
    "\n",
    "    # import data from the networkx graph with the attributes of the nodes\n",
    "    data = from_networkx(G, attributes)\n",
    "      \n",
    "    y = torch.from_numpy(labels).type(torch.long)\n",
    "\n",
    "    data.x = data.x.float()\n",
    "    data.y = y.clone().detach()\n",
    "    data.num_classes = num_classes\n",
    "\n",
    "    # Using train_test_split function from sklearn to stratify train/test/val sets\n",
    "    indices = range(G.number_of_nodes())\n",
    "    # Stratified split of train/test/val sets. Returned indices are used to create the masks\n",
    "    X_train, X_test, y_train, y_test, train_idx, test_idx = train_test_split(data.x, data.y, indices, test_size=0.3, stratify=labels, random_state=42)\n",
    "    # To create validation set, test set is splitted in half\n",
    "    X_test, X_val, y_test, y_val, test_idx, val_idx = train_test_split(X_test, y_test, test_idx, test_size=0.5, stratify=y_test, random_state=42)\n",
    "\n",
    "    n_nodes = G.number_of_nodes()\n",
    "    train_mask  = torch.zeros(n_nodes, dtype=torch.bool)\n",
    "    test_mask   = torch.zeros(n_nodes, dtype=torch.bool)\n",
    "    val_mask    = torch.zeros(n_nodes, dtype=torch.bool)\n",
    "    \n",
    "    for idx in train_idx:\n",
    "      train_mask[idx] = True\n",
    "\n",
    "    for idx in test_idx:\n",
    "      test_mask[idx] = True\n",
    "    \n",
    "    for idx in val_idx:\n",
    "      val_mask[idx] = True\n",
    "\n",
    "    data['train_mask']  = train_mask\n",
    "    data['test_mask']   = test_mask\n",
    "    data['val_mask']    = val_mask\n",
    "\n",
    "    self.data, self.slices = self.collate([data])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saves as 'best model' the model with the lower training loss. Then the metrics are computed on the best model at the end of the training epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data, epochs, lr, weight_decay, classes, model_name):\n",
    "    model_names = ['GCN', 'GAT', 'GraphCONV', 'GraphSAGE']\n",
    "\n",
    "    if model_name not in model_names:\n",
    "        print('[ERR] No GNN model has been found for:', model_name)\n",
    "        return -1\n",
    "    \n",
    "    data = data.to(device)\n",
    "\n",
    "    title = model_name + '_' + str(epochs) + '_' + str(weight_decay).replace('.', '_')\n",
    "\n",
    "    model_path  = 'Models/' + title\n",
    "    image_path  = 'Images/' + title\n",
    "    report_path = 'Reports/' + title + '.csv'\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    # optimizer = torch.optim.RMSprop(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    train_mask  = data['train_mask']\n",
    "    test_mask   = data['test_mask']\n",
    "    val_mask    = data['val_mask']\n",
    "\n",
    "    labels = data.y\n",
    "    output = ''\n",
    "\n",
    "    # list to plot the train accuracy\n",
    "    train_acc_curve = []\n",
    "    train_lss_curve = []\n",
    "\n",
    "    best_train_acc  = 0\n",
    "    best_val_acc    = 0\n",
    "    best_train_lss  = 999\n",
    "    best_loss_epoch = 0\n",
    "\n",
    "    for e in tqdm(range(epochs+1)):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        logits      = model(data)\n",
    "        output      = logits.argmax(1)\n",
    "        #Â train_loss  = F.cross_entropy(logits[train_mask], labels[train_mask])\n",
    "        train_loss  = F.nll_loss(logits[train_mask], labels[train_mask])\n",
    "        train_acc   = (output[train_mask] == labels[train_mask]).float().mean()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Append train acc. to plot curve later\n",
    "        train_acc_curve.append(train_acc.item())\n",
    "        train_lss_curve.append(train_loss.item())\n",
    "\n",
    "        if train_acc > best_train_acc:\n",
    "            best_train_acc = train_acc\n",
    "\n",
    "        # Evaluation and test\n",
    "        model.eval()\n",
    "        logits      = model(data)\n",
    "        output      = logits.argmax(1)\n",
    "        val_loss    = F.nll_loss(logits[val_mask], labels[val_mask])\n",
    "        val_acc     = (output[val_mask] == labels[val_mask]).float().mean()\n",
    "\n",
    "        # Update best test/val acc.\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "        \n",
    "        # Save model with best train loss\n",
    "        if train_loss < best_train_lss:\n",
    "            best_train_lss = train_loss\n",
    "            best_loss_epoch = e\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "\n",
    "        if e % 20 == 0 or e == epochs:\n",
    "            print('[Epoch: {:04d}]'.format(e),\n",
    "            'train loss: {:.4f},'.format(train_loss.item()),\n",
    "            'train acc: {:.4f},'.format(train_acc.item()),\n",
    "            'val loss: {:.4f},'.format(val_loss.item()),\n",
    "            'val acc: {:.4f} '.format(val_acc.item()),\n",
    "            '(best train acc: {:.4f},'.format(best_train_acc.item()),\n",
    "            'best val acc: {:.4f},'.format(best_val_acc.item()),\n",
    "            'best train loss: {:.4f} '.format(best_train_lss),\n",
    "            '@ epoch', best_loss_epoch ,')')\n",
    "    \n",
    "    # Plot training accuracy curve\n",
    "    plt.figure(figsize = (12,7))\n",
    "    plt.plot(train_acc_curve)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Train Accuracy')\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize = (12,7))\n",
    "    plt.plot(train_lss_curve)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Train Loss')\n",
    "    plt.show()\n",
    "\n",
    "    # Load best model\n",
    "    loaded_model = None\n",
    "    if model_name == 'GCN':\n",
    "        loaded_model = GNN7L_GCN(data)\n",
    "    elif model_name == 'GAT':\n",
    "        loaded_model = GNN7L_GAT(data)\n",
    "    elif model_name == 'GraphCONV':\n",
    "        loaded_model = GNN7L_GraphConv(data)\n",
    "    else:\n",
    "        loaded_model = GNN7L_Sage(data)\n",
    "    \n",
    "    loaded_model = loaded_model.to(device)\n",
    "    loaded_model.load_state_dict(torch.load(model_path))\n",
    "    loaded_model.eval()\n",
    "    logits = loaded_model(data)\n",
    "    output = logits.argmax(1)\n",
    "\n",
    "    print(classification_report(labels[test_mask].to('cpu'), output[test_mask].to('cpu')))\n",
    "\n",
    "    class_report = classification_report(labels[test_mask].to('cpu'), output[test_mask].to('cpu'), output_dict=True)\n",
    "    classification_report_dataframe = pd.DataFrame(class_report)\n",
    "    classification_report_dataframe.to_csv(report_path)\n",
    "\n",
    "    #Confusion Matrix\n",
    "    norms = [None, \"true\"]\n",
    "    for norm in norms:\n",
    "        cm = confusion_matrix(labels[test_mask].to('cpu'), output[test_mask].to('cpu'), normalize=norm)\n",
    "\n",
    "        plt.figure(figsize=(7,7))\n",
    "        \n",
    "        if norm == \"true\":\n",
    "            sn.heatmap(cm, annot=True, fmt=\".3f\", linewidths=.5, square = True, cmap = 'BuPu', xticklabels = classes, yticklabels = classes)\n",
    "        else:\n",
    "            sn.heatmap(cm, annot=True, fmt=\".0f\", linewidths=.5, square = True, cmap = 'BuPu', xticklabels = classes, yticklabels = classes)\n",
    "        plt.title(model_name)\n",
    "        plt.ylabel('Actual label')\n",
    "        plt.xlabel('Predicted label')\n",
    "\n",
    "        if norm == None:\n",
    "            plt.savefig(image_path + '_notNorm.png')\n",
    "        else:\n",
    "            plt.savefig(image_path + '_Norm.png')\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataset from PPI graph and [NeDBIT features](https://arxiv.org/pdf/2108.06158.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "G = nx.read_gml('Graphs/graph_with_normalized_nedbit.gml')\n",
    "\n",
    "seed_genes          = pd.read_csv('Datasets/C0006142_Malignant_neoplasm_of_breast_seed_genes.txt', header=None, sep=' ')\n",
    "seed_genes.columns  = [\"name\", \"GDA Score\"]\n",
    "seeds_list          = seed_genes[\"name\"].values.tolist()\n",
    "\n",
    "nedbit_scores = pd.read_csv('Datasets/C0006142_Malignant_neoplasm_of_breast_features_Score.csv')\n",
    "\n",
    "# Remove seed genes\n",
    "nedbit_scores_not_seed = nedbit_scores[~nedbit_scores['name'].isin(seeds_list)]\n",
    "\n",
    "# Sort scores for quartile division\n",
    "nedbit_scores_not_seed = nedbit_scores_not_seed.sort_values(by = \"out\", ascending = False)\n",
    "pseudo_labels = pd.qcut(x = nedbit_scores_not_seed[\"out\"], q = 4, labels = [\"RN\", \"LN\", \"WN\", \"LP\"])\n",
    "\n",
    "nedbit_scores_not_seed['label'] = pseudo_labels\n",
    "\n",
    "nedbit_scores_seed = nedbit_scores[nedbit_scores['name'].isin(seeds_list)]\n",
    "nedbit_scores_seed = nedbit_scores_seed.assign(label = 'P')\n",
    "\n",
    "# Convert dataframe to dict for searching nodes and their labels\n",
    "not_seed_labels = dict(zip(nedbit_scores_not_seed['name'], nedbit_scores_not_seed['label']))\n",
    "seed_labels     = dict(zip(nedbit_scores_seed['name'], nedbit_scores_seed['label']))\n",
    "\n",
    "labels_dict = {'P':0, 'LP': 1, 'WN': 2, 'LN': 3, 'RN': 4}\n",
    "labels = []\n",
    "\n",
    "for node in G:\n",
    "    if node in not_seed_labels:\n",
    "        labels.append(labels_dict[not_seed_labels[node]])\n",
    "    else:\n",
    "        labels.append(labels_dict[seed_labels[node]])\n",
    "\n",
    "labels = np.asarray(labels)\n",
    "\n",
    "attributes = ['degree', 'ring', 'NetRank', 'NetShort', 'HeatDiff', 'InfoDiff']\n",
    "\n",
    "dataset_with_nedbit = MyDataset(G, labels, attributes, num_classes=5)\n",
    "data_with_nedbit = dataset_with_nedbit[0]\n",
    "\n",
    "plt.hist(labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr              = 0.001\n",
    "epochs          = 40000\n",
    "weight_decay    = 0.0005\n",
    "\n",
    "classes         = ['P', 'LP', 'WN', 'LN', 'RN']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b866e05f1f0a45698a34406367e57391",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40001 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch: 0000] train loss: 154.2524, train acc: 0.2447, val loss: 139.0947, val acc: 0.2371  (best train acc: 0.2447, best val acc: 0.2371, best train loss: 154.2524  @ epoch 0 )\n",
      "[Epoch: 0020] train loss: 28.8249, train acc: 0.2140, val loss: 24.1395, val acc: 0.2368  (best train acc: 0.2454, best val acc: 0.2371, best train loss: 28.8249  @ epoch 20 )\n",
      "[Epoch: 0040] train loss: 8.3824, train acc: 0.2384, val loss: 6.8963, val acc: 0.2371  (best train acc: 0.2572, best val acc: 0.2371, best train loss: 8.3824  @ epoch 40 )\n",
      "[Epoch: 0060] train loss: 3.5954, train acc: 0.2574, val loss: 2.8943, val acc: 0.2371  (best train acc: 0.2940, best val acc: 0.2371, best train loss: 3.5954  @ epoch 60 )\n",
      "[Epoch: 0080] train loss: 1.8750, train acc: 0.2728, val loss: 1.6885, val acc: 0.2371  (best train acc: 0.2998, best val acc: 0.2371, best train loss: 1.8346  @ epoch 78 )\n",
      "[Epoch: 0100] train loss: 1.5814, train acc: 0.2689, val loss: 1.5075, val acc: 0.2368  (best train acc: 0.3037, best val acc: 0.2371, best train loss: 1.5723  @ epoch 96 )\n",
      "[Epoch: 0120] train loss: 1.5305, train acc: 0.2953, val loss: 1.4758, val acc: 0.2398  (best train acc: 0.3040, best val acc: 0.2398, best train loss: 1.5214  @ epoch 115 )\n",
      "[Epoch: 0140] train loss: 1.5106, train acc: 0.2788, val loss: 1.4696, val acc: 0.2388  (best train acc: 0.3040, best val acc: 0.2398, best train loss: 1.5029  @ epoch 139 )\n",
      "[Epoch: 0160] train loss: 1.4955, train acc: 0.2879, val loss: 1.4619, val acc: 0.2428  (best train acc: 0.3040, best val acc: 0.2428, best train loss: 1.4894  @ epoch 159 )\n",
      "[Epoch: 0180] train loss: 1.4912, train acc: 0.2848, val loss: 1.4579, val acc: 0.2428  (best train acc: 0.3040, best val acc: 0.2442, best train loss: 1.4817  @ epoch 172 )\n",
      "[Epoch: 0200] train loss: 1.4834, train acc: 0.2955, val loss: 1.4540, val acc: 0.2492  (best train acc: 0.3040, best val acc: 0.2509, best train loss: 1.4756  @ epoch 190 )\n",
      "[Epoch: 0220] train loss: 1.4818, train acc: 0.3032, val loss: 1.4498, val acc: 0.2513  (best train acc: 0.3096, best val acc: 0.2523, best train loss: 1.4650  @ epoch 211 )\n",
      "[Epoch: 0240] train loss: 1.4664, train acc: 0.3118, val loss: 1.4473, val acc: 0.2513  (best train acc: 0.3156, best val acc: 0.2553, best train loss: 1.4648  @ epoch 231 )\n",
      "[Epoch: 0260] train loss: 1.4538, train acc: 0.3218, val loss: 1.4413, val acc: 0.2550  (best train acc: 0.3229, best val acc: 0.2914, best train loss: 1.4538  @ epoch 260 )\n",
      "[Epoch: 0280] train loss: 1.4552, train acc: 0.3190, val loss: 1.4374, val acc: 0.3255  (best train acc: 0.3285, best val acc: 0.3285, best train loss: 1.4509  @ epoch 267 )\n",
      "[Epoch: 0300] train loss: 1.4554, train acc: 0.3177, val loss: 1.4329, val acc: 0.3356  (best train acc: 0.3367, best val acc: 0.3356, best train loss: 1.4505  @ epoch 299 )\n",
      "[Epoch: 0320] train loss: 1.4491, train acc: 0.3331, val loss: 1.4283, val acc: 0.3454  (best train acc: 0.3433, best val acc: 0.3487, best train loss: 1.4375  @ epoch 315 )\n",
      "[Epoch: 0340] train loss: 1.4415, train acc: 0.3472, val loss: 1.4239, val acc: 0.3524  (best train acc: 0.3549, best val acc: 0.3673, best train loss: 1.4367  @ epoch 335 )\n",
      "[Epoch: 0360] train loss: 1.4321, train acc: 0.3486, val loss: 1.4199, val acc: 0.3545  (best train acc: 0.3568, best val acc: 0.3707, best train loss: 1.4295  @ epoch 358 )\n",
      "[Epoch: 0380] train loss: 1.4255, train acc: 0.3600, val loss: 1.4158, val acc: 0.3676  (best train acc: 0.3655, best val acc: 0.3970, best train loss: 1.4248  @ epoch 378 )\n",
      "[Epoch: 0400] train loss: 1.4218, train acc: 0.3663, val loss: 1.4100, val acc: 0.3855  (best train acc: 0.3751, best val acc: 0.3970, best train loss: 1.4212  @ epoch 395 )\n",
      "[Epoch: 0420] train loss: 1.4204, train acc: 0.3684, val loss: 1.4057, val acc: 0.3872  (best train acc: 0.3791, best val acc: 0.4020, best train loss: 1.4169  @ epoch 412 )\n",
      "[Epoch: 0440] train loss: 1.4101, train acc: 0.3691, val loss: 1.4003, val acc: 0.3906  (best train acc: 0.3819, best val acc: 0.4071, best train loss: 1.4101  @ epoch 440 )\n",
      "[Epoch: 0460] train loss: 1.4031, train acc: 0.3757, val loss: 1.3935, val acc: 0.4074  (best train acc: 0.3835, best val acc: 0.4125, best train loss: 1.4029  @ epoch 456 )\n",
      "[Epoch: 0480] train loss: 1.3980, train acc: 0.3913, val loss: 1.3865, val acc: 0.4105  (best train acc: 0.3942, best val acc: 0.4128, best train loss: 1.3948  @ epoch 478 )\n",
      "[Epoch: 0500] train loss: 1.3883, train acc: 0.4057, val loss: 1.3790, val acc: 0.4148  (best train acc: 0.4057, best val acc: 0.4229, best train loss: 1.3875  @ epoch 498 )\n",
      "[Epoch: 0520] train loss: 1.3758, train acc: 0.4030, val loss: 1.3713, val acc: 0.4202  (best train acc: 0.4125, best val acc: 0.4310, best train loss: 1.3758  @ epoch 520 )\n",
      "[Epoch: 0540] train loss: 1.3695, train acc: 0.4003, val loss: 1.3628, val acc: 0.4256  (best train acc: 0.4286, best val acc: 0.4492, best train loss: 1.3695  @ epoch 540 )\n",
      "[Epoch: 0560] train loss: 1.3608, train acc: 0.4142, val loss: 1.3507, val acc: 0.4435  (best train acc: 0.4291, best val acc: 0.4492, best train loss: 1.3576  @ epoch 558 )\n",
      "[Epoch: 0580] train loss: 1.3441, train acc: 0.4263, val loss: 1.3387, val acc: 0.4401  (best train acc: 0.4321, best val acc: 0.4492, best train loss: 1.3441  @ epoch 580 )\n",
      "[Epoch: 0600] train loss: 1.3398, train acc: 0.4273, val loss: 1.3265, val acc: 0.4492  (best train acc: 0.4346, best val acc: 0.4536, best train loss: 1.3389  @ epoch 599 )\n",
      "[Epoch: 0620] train loss: 1.3456, train acc: 0.4414, val loss: 1.3266, val acc: 0.4583  (best train acc: 0.4414, best val acc: 0.4621, best train loss: 1.3373  @ epoch 617 )\n",
      "[Epoch: 0640] train loss: 1.3300, train acc: 0.4354, val loss: 1.3110, val acc: 0.4533  (best train acc: 0.4493, best val acc: 0.4658, best train loss: 1.3275  @ epoch 639 )\n",
      "[Epoch: 0660] train loss: 1.3232, train acc: 0.4447, val loss: 1.3188, val acc: 0.4513  (best train acc: 0.4505, best val acc: 0.4708, best train loss: 1.3209  @ epoch 657 )\n",
      "[Epoch: 0680] train loss: 1.3185, train acc: 0.4250, val loss: 1.2998, val acc: 0.4320  (best train acc: 0.4555, best val acc: 0.4708, best train loss: 1.3185  @ epoch 680 )\n",
      "[Epoch: 0700] train loss: 1.3206, train acc: 0.4338, val loss: 1.2963, val acc: 0.4452  (best train acc: 0.4555, best val acc: 0.4708, best train loss: 1.3095  @ epoch 692 )\n",
      "[Epoch: 0720] train loss: 1.3075, train acc: 0.4354, val loss: 1.2910, val acc: 0.4459  (best train acc: 0.4572, best val acc: 0.4708, best train loss: 1.3075  @ epoch 720 )\n",
      "[Epoch: 0740] train loss: 1.3013, train acc: 0.4450, val loss: 1.2766, val acc: 0.4587  (best train acc: 0.4625, best val acc: 0.4715, best train loss: 1.2959  @ epoch 738 )\n",
      "[Epoch: 0760] train loss: 1.3443, train acc: 0.4131, val loss: 1.3360, val acc: 0.3875  (best train acc: 0.4625, best val acc: 0.4715, best train loss: 1.2959  @ epoch 738 )\n",
      "[Epoch: 0780] train loss: 1.3049, train acc: 0.4356, val loss: 1.2788, val acc: 0.4401  (best train acc: 0.4625, best val acc: 0.4715, best train loss: 1.2959  @ epoch 738 )\n",
      "[Epoch: 0800] train loss: 1.3016, train acc: 0.4586, val loss: 1.2745, val acc: 0.4594  (best train acc: 0.4625, best val acc: 0.4715, best train loss: 1.2934  @ epoch 796 )\n",
      "[Epoch: 0820] train loss: 1.3074, train acc: 0.4477, val loss: 1.2827, val acc: 0.4408  (best train acc: 0.4631, best val acc: 0.4752, best train loss: 1.2934  @ epoch 796 )\n",
      "[Epoch: 0840] train loss: 1.2836, train acc: 0.4545, val loss: 1.2660, val acc: 0.4506  (best train acc: 0.4631, best val acc: 0.4752, best train loss: 1.2836  @ epoch 840 )\n",
      "[Epoch: 0860] train loss: 1.2846, train acc: 0.4633, val loss: 1.2599, val acc: 0.4398  (best train acc: 0.4633, best val acc: 0.4752, best train loss: 1.2826  @ epoch 858 )\n",
      "[Epoch: 0880] train loss: 1.2828, train acc: 0.4529, val loss: 1.2554, val acc: 0.4516  (best train acc: 0.4633, best val acc: 0.4752, best train loss: 1.2810  @ epoch 879 )\n",
      "[Epoch: 0900] train loss: 1.2762, train acc: 0.4572, val loss: 1.2526, val acc: 0.4644  (best train acc: 0.4633, best val acc: 0.4752, best train loss: 1.2758  @ epoch 897 )\n",
      "[Epoch: 0920] train loss: 1.2741, train acc: 0.4578, val loss: 1.2495, val acc: 0.4634  (best train acc: 0.4633, best val acc: 0.4752, best train loss: 1.2741  @ epoch 920 )\n",
      "[Epoch: 0940] train loss: 1.2805, train acc: 0.4602, val loss: 1.2470, val acc: 0.4621  (best train acc: 0.4730, best val acc: 0.4752, best train loss: 1.2710  @ epoch 939 )\n",
      "[Epoch: 0960] train loss: 1.2759, train acc: 0.4592, val loss: 1.2472, val acc: 0.4550  (best train acc: 0.4730, best val acc: 0.4850, best train loss: 1.2710  @ epoch 939 )\n",
      "[Epoch: 0980] train loss: 1.2715, train acc: 0.4607, val loss: 1.2447, val acc: 0.4583  (best train acc: 0.4743, best val acc: 0.4857, best train loss: 1.2710  @ epoch 939 )\n",
      "[Epoch: 1000] train loss: 1.2749, train acc: 0.4626, val loss: 1.2479, val acc: 0.4654  (best train acc: 0.4743, best val acc: 0.4857, best train loss: 1.2670  @ epoch 997 )\n",
      "[Epoch: 1020] train loss: 1.2762, train acc: 0.4576, val loss: 1.2422, val acc: 0.4678  (best train acc: 0.4743, best val acc: 0.4857, best train loss: 1.2621  @ epoch 1014 )\n",
      "[Epoch: 1040] train loss: 1.2654, train acc: 0.4636, val loss: 1.2372, val acc: 0.4782  (best train acc: 0.4743, best val acc: 0.4867, best train loss: 1.2621  @ epoch 1014 )\n",
      "[Epoch: 1060] train loss: 1.2712, train acc: 0.4700, val loss: 1.2469, val acc: 0.4803  (best train acc: 0.4824, best val acc: 0.4944, best train loss: 1.2621  @ epoch 1014 )\n",
      "[Epoch: 1080] train loss: 1.2589, train acc: 0.4748, val loss: 1.2350, val acc: 0.4833  (best train acc: 0.4824, best val acc: 0.4944, best train loss: 1.2589  @ epoch 1080 )\n",
      "[Epoch: 1100] train loss: 1.2669, train acc: 0.4913, val loss: 1.2385, val acc: 0.4968  (best train acc: 0.4913, best val acc: 0.4968, best train loss: 1.2589  @ epoch 1080 )\n",
      "[Epoch: 1120] train loss: 1.2621, train acc: 0.4707, val loss: 1.2335, val acc: 0.4853  (best train acc: 0.4913, best val acc: 0.4968, best train loss: 1.2589  @ epoch 1080 )\n",
      "[Epoch: 1140] train loss: 1.2696, train acc: 0.4641, val loss: 1.2355, val acc: 0.4671  (best train acc: 0.4913, best val acc: 0.4968, best train loss: 1.2546  @ epoch 1124 )\n",
      "[Epoch: 1160] train loss: 1.2578, train acc: 0.4752, val loss: 1.2311, val acc: 0.4833  (best train acc: 0.4913, best val acc: 0.4968, best train loss: 1.2546  @ epoch 1124 )\n",
      "[Epoch: 1180] train loss: 1.2595, train acc: 0.4633, val loss: 1.2293, val acc: 0.4894  (best train acc: 0.4913, best val acc: 0.4968, best train loss: 1.2532  @ epoch 1162 )\n",
      "[Epoch: 1200] train loss: 1.2521, train acc: 0.4730, val loss: 1.2275, val acc: 0.4857  (best train acc: 0.4913, best val acc: 0.4968, best train loss: 1.2521  @ epoch 1200 )\n",
      "[Epoch: 1220] train loss: 1.3183, train acc: 0.4324, val loss: 1.3058, val acc: 0.4384  (best train acc: 0.4913, best val acc: 0.4968, best train loss: 1.2503  @ epoch 1210 )\n",
      "[Epoch: 1240] train loss: 1.2611, train acc: 0.4684, val loss: 1.2298, val acc: 0.4789  (best train acc: 0.4913, best val acc: 0.4988, best train loss: 1.2503  @ epoch 1210 )\n",
      "[Epoch: 1260] train loss: 1.2609, train acc: 0.4651, val loss: 1.2266, val acc: 0.4749  (best train acc: 0.4913, best val acc: 0.4988, best train loss: 1.2494  @ epoch 1254 )\n",
      "[Epoch: 1280] train loss: 1.2857, train acc: 0.4639, val loss: 1.2730, val acc: 0.4685  (best train acc: 0.4913, best val acc: 0.4988, best train loss: 1.2489  @ epoch 1261 )\n",
      "[Epoch: 1300] train loss: 1.2602, train acc: 0.4757, val loss: 1.2273, val acc: 0.4863  (best train acc: 0.4913, best val acc: 0.5005, best train loss: 1.2489  @ epoch 1261 )\n",
      "[Epoch: 1320] train loss: 1.2512, train acc: 0.4740, val loss: 1.2233, val acc: 0.4755  (best train acc: 0.4913, best val acc: 0.5005, best train loss: 1.2453  @ epoch 1317 )\n",
      "[Epoch: 1340] train loss: 1.2462, train acc: 0.4772, val loss: 1.2222, val acc: 0.4796  (best train acc: 0.4913, best val acc: 0.5005, best train loss: 1.2442  @ epoch 1338 )\n",
      "[Epoch: 1360] train loss: 1.2445, train acc: 0.4776, val loss: 1.2212, val acc: 0.4843  (best train acc: 0.4913, best val acc: 0.5005, best train loss: 1.2421  @ epoch 1346 )\n",
      "[Epoch: 1380] train loss: 1.2438, train acc: 0.4803, val loss: 1.2204, val acc: 0.4830  (best train acc: 0.4913, best val acc: 0.5005, best train loss: 1.2415  @ epoch 1374 )\n",
      "[Epoch: 1400] train loss: 1.2418, train acc: 0.4793, val loss: 1.2193, val acc: 0.4924  (best train acc: 0.4913, best val acc: 0.5005, best train loss: 1.2394  @ epoch 1384 )\n",
      "[Epoch: 1420] train loss: 1.2419, train acc: 0.4797, val loss: 1.2192, val acc: 0.4958  (best train acc: 0.4913, best val acc: 0.5005, best train loss: 1.2394  @ epoch 1384 )\n",
      "[Epoch: 1440] train loss: 1.2464, train acc: 0.4816, val loss: 1.2182, val acc: 0.4975  (best train acc: 0.4913, best val acc: 0.5005, best train loss: 1.2394  @ epoch 1384 )\n",
      "[Epoch: 1460] train loss: 1.2534, train acc: 0.4657, val loss: 1.2244, val acc: 0.4681  (best train acc: 0.4913, best val acc: 0.5005, best train loss: 1.2388  @ epoch 1458 )\n",
      "[Epoch: 1480] train loss: 1.2474, train acc: 0.4679, val loss: 1.2181, val acc: 0.4833  (best train acc: 0.4939, best val acc: 0.5029, best train loss: 1.2335  @ epoch 1477 )\n",
      "[Epoch: 1500] train loss: 1.2392, train acc: 0.4808, val loss: 1.2158, val acc: 0.4911  (best train acc: 0.4939, best val acc: 0.5029, best train loss: 1.2325  @ epoch 1490 )\n",
      "[Epoch: 1520] train loss: 1.2442, train acc: 0.4863, val loss: 1.2155, val acc: 0.4843  (best train acc: 0.4939, best val acc: 0.5029, best train loss: 1.2325  @ epoch 1490 )\n",
      "[Epoch: 1540] train loss: 1.2332, train acc: 0.4806, val loss: 1.2149, val acc: 0.4840  (best train acc: 0.4939, best val acc: 0.5029, best train loss: 1.2325  @ epoch 1490 )\n",
      "[Epoch: 1560] train loss: 1.2439, train acc: 0.4641, val loss: 1.2185, val acc: 0.4745  (best train acc: 0.4939, best val acc: 0.5029, best train loss: 1.2325  @ epoch 1490 )\n",
      "[Epoch: 1580] train loss: 1.2387, train acc: 0.4902, val loss: 1.2131, val acc: 0.4887  (best train acc: 0.4939, best val acc: 0.5029, best train loss: 1.2300  @ epoch 1571 )\n",
      "[Epoch: 1600] train loss: 1.2328, train acc: 0.4854, val loss: 1.2125, val acc: 0.4911  (best train acc: 0.4939, best val acc: 0.5029, best train loss: 1.2300  @ epoch 1571 )\n",
      "[Epoch: 1620] train loss: 1.2398, train acc: 0.4748, val loss: 1.2119, val acc: 0.4867  (best train acc: 0.4939, best val acc: 0.5029, best train loss: 1.2284  @ epoch 1611 )\n",
      "[Epoch: 1640] train loss: 1.2383, train acc: 0.4738, val loss: 1.2129, val acc: 0.4776  (best train acc: 0.4939, best val acc: 0.5029, best train loss: 1.2284  @ epoch 1611 )\n",
      "[Epoch: 1660] train loss: 1.2382, train acc: 0.4798, val loss: 1.2138, val acc: 0.4847  (best train acc: 0.4986, best val acc: 0.5029, best train loss: 1.2266  @ epoch 1642 )\n",
      "[Epoch: 1680] train loss: 1.2324, train acc: 0.4800, val loss: 1.2107, val acc: 0.4843  (best train acc: 0.4986, best val acc: 0.5029, best train loss: 1.2253  @ epoch 1673 )\n",
      "[Epoch: 1700] train loss: 1.2337, train acc: 0.4779, val loss: 1.2098, val acc: 0.4917  (best train acc: 0.4986, best val acc: 0.5029, best train loss: 1.2253  @ epoch 1673 )\n",
      "[Epoch: 1720] train loss: 1.2306, train acc: 0.4816, val loss: 1.2096, val acc: 0.4850  (best train acc: 0.4986, best val acc: 0.5029, best train loss: 1.2235  @ epoch 1706 )\n",
      "[Epoch: 1740] train loss: 1.2264, train acc: 0.4890, val loss: 1.2084, val acc: 0.4944  (best train acc: 0.4986, best val acc: 0.5029, best train loss: 1.2235  @ epoch 1706 )\n",
      "[Epoch: 1760] train loss: 1.2274, train acc: 0.4810, val loss: 1.2077, val acc: 0.4850  (best train acc: 0.4986, best val acc: 0.5029, best train loss: 1.2232  @ epoch 1757 )\n",
      "[Epoch: 1780] train loss: 1.2327, train acc: 0.4809, val loss: 1.2075, val acc: 0.4863  (best train acc: 0.4986, best val acc: 0.5029, best train loss: 1.2221  @ epoch 1764 )\n",
      "[Epoch: 1800] train loss: 1.2195, train acc: 0.4859, val loss: 1.2064, val acc: 0.4914  (best train acc: 0.4986, best val acc: 0.5029, best train loss: 1.2195  @ epoch 1800 )\n",
      "[Epoch: 1820] train loss: 1.2321, train acc: 0.4850, val loss: 1.2061, val acc: 0.4863  (best train acc: 0.4986, best val acc: 0.5029, best train loss: 1.2195  @ epoch 1800 )\n",
      "[Epoch: 1840] train loss: 1.2223, train acc: 0.4890, val loss: 1.2053, val acc: 0.4874  (best train acc: 0.4986, best val acc: 0.5029, best train loss: 1.2185  @ epoch 1824 )\n",
      "[Epoch: 1860] train loss: 1.2218, train acc: 0.4852, val loss: 1.2049, val acc: 0.4877  (best train acc: 0.4986, best val acc: 0.5029, best train loss: 1.2185  @ epoch 1824 )\n",
      "[Epoch: 1880] train loss: 1.2210, train acc: 0.4949, val loss: 1.2041, val acc: 0.4975  (best train acc: 0.4995, best val acc: 0.5029, best train loss: 1.2154  @ epoch 1874 )\n",
      "[Epoch: 1900] train loss: 1.2239, train acc: 0.4863, val loss: 1.2038, val acc: 0.4840  (best train acc: 0.4995, best val acc: 0.5029, best train loss: 1.2154  @ epoch 1874 )\n",
      "[Epoch: 1920] train loss: 1.2247, train acc: 0.4836, val loss: 1.2042, val acc: 0.4995  (best train acc: 0.4995, best val acc: 0.5029, best train loss: 1.2154  @ epoch 1874 )\n",
      "[Epoch: 1940] train loss: 1.2152, train acc: 0.4936, val loss: 1.2021, val acc: 0.4901  (best train acc: 0.4995, best val acc: 0.5029, best train loss: 1.2145  @ epoch 1928 )\n",
      "[Epoch: 1960] train loss: 1.2187, train acc: 0.4874, val loss: 1.2021, val acc: 0.4853  (best train acc: 0.4995, best val acc: 0.5029, best train loss: 1.2133  @ epoch 1957 )\n",
      "[Epoch: 1980] train loss: 1.2221, train acc: 0.4837, val loss: 1.2025, val acc: 0.4847  (best train acc: 0.4995, best val acc: 0.5029, best train loss: 1.2133  @ epoch 1957 )\n",
      "[Epoch: 2000] train loss: 1.2163, train acc: 0.4800, val loss: 1.2008, val acc: 0.4934  (best train acc: 0.4995, best val acc: 0.5029, best train loss: 1.2106  @ epoch 1981 )\n",
      "[Epoch: 2020] train loss: 1.2183, train acc: 0.4855, val loss: 1.1998, val acc: 0.4911  (best train acc: 0.4995, best val acc: 0.5029, best train loss: 1.2106  @ epoch 1981 )\n",
      "[Epoch: 2040] train loss: 1.2227, train acc: 0.4847, val loss: 1.2011, val acc: 0.4847  (best train acc: 0.4995, best val acc: 0.5029, best train loss: 1.2106  @ epoch 1981 )\n",
      "[Epoch: 2060] train loss: 1.2185, train acc: 0.4920, val loss: 1.2004, val acc: 0.4897  (best train acc: 0.4995, best val acc: 0.5029, best train loss: 1.2106  @ epoch 1981 )\n",
      "[Epoch: 2080] train loss: 1.2162, train acc: 0.4903, val loss: 1.1999, val acc: 0.4911  (best train acc: 0.4995, best val acc: 0.5029, best train loss: 1.2106  @ epoch 1981 )\n",
      "[Epoch: 2100] train loss: 1.2205, train acc: 0.4899, val loss: 1.1986, val acc: 0.4954  (best train acc: 0.4995, best val acc: 0.5029, best train loss: 1.2106  @ epoch 1981 )\n",
      "[Epoch: 2120] train loss: 1.2178, train acc: 0.4904, val loss: 1.1977, val acc: 0.4884  (best train acc: 0.4995, best val acc: 0.5029, best train loss: 1.2055  @ epoch 2112 )\n",
      "[Epoch: 2140] train loss: 1.2176, train acc: 0.4918, val loss: 1.1980, val acc: 0.4938  (best train acc: 0.4995, best val acc: 0.5029, best train loss: 1.2055  @ epoch 2112 )\n",
      "[Epoch: 2160] train loss: 1.2111, train acc: 0.4916, val loss: 1.1972, val acc: 0.4985  (best train acc: 0.4995, best val acc: 0.5073, best train loss: 1.2055  @ epoch 2112 )\n",
      "[Epoch: 2180] train loss: 1.2118, train acc: 0.4858, val loss: 1.1956, val acc: 0.4897  (best train acc: 0.4995, best val acc: 0.5073, best train loss: 1.2053  @ epoch 2179 )\n",
      "[Epoch: 2200] train loss: 1.2090, train acc: 0.4910, val loss: 1.1954, val acc: 0.4968  (best train acc: 0.4995, best val acc: 0.5073, best train loss: 1.2043  @ epoch 2199 )\n",
      "[Epoch: 2220] train loss: 1.2115, train acc: 0.4839, val loss: 1.1950, val acc: 0.4954  (best train acc: 0.4995, best val acc: 0.5073, best train loss: 1.2043  @ epoch 2199 )\n",
      "[Epoch: 2240] train loss: 1.2109, train acc: 0.4886, val loss: 1.1941, val acc: 0.4927  (best train acc: 0.4995, best val acc: 0.5073, best train loss: 1.2043  @ epoch 2199 )\n",
      "[Epoch: 2260] train loss: 1.2085, train acc: 0.4897, val loss: 1.1938, val acc: 0.4907  (best train acc: 0.4995, best val acc: 0.5073, best train loss: 1.2012  @ epoch 2244 )\n",
      "[Epoch: 2280] train loss: 1.2097, train acc: 0.4907, val loss: 1.1932, val acc: 0.4934  (best train acc: 0.4995, best val acc: 0.5073, best train loss: 1.2004  @ epoch 2266 )\n",
      "[Epoch: 2300] train loss: 1.2076, train acc: 0.4954, val loss: 1.1929, val acc: 0.4897  (best train acc: 0.4995, best val acc: 0.5073, best train loss: 1.2004  @ epoch 2266 )\n",
      "[Epoch: 2320] train loss: 1.2053, train acc: 0.4917, val loss: 1.1923, val acc: 0.4927  (best train acc: 0.4995, best val acc: 0.5073, best train loss: 1.2004  @ epoch 2266 )\n",
      "[Epoch: 2340] train loss: 1.2078, train acc: 0.4840, val loss: 1.1918, val acc: 0.4954  (best train acc: 0.5008, best val acc: 0.5073, best train loss: 1.2004  @ epoch 2266 )\n",
      "[Epoch: 2360] train loss: 1.2111, train acc: 0.4927, val loss: 1.1918, val acc: 0.4934  (best train acc: 0.5017, best val acc: 0.5073, best train loss: 1.2002  @ epoch 2341 )\n",
      "[Epoch: 2380] train loss: 1.2119, train acc: 0.4951, val loss: 1.1913, val acc: 0.4924  (best train acc: 0.5017, best val acc: 0.5073, best train loss: 1.1991  @ epoch 2363 )\n",
      "[Epoch: 2400] train loss: 1.2061, train acc: 0.4873, val loss: 1.1903, val acc: 0.4938  (best train acc: 0.5017, best val acc: 0.5073, best train loss: 1.1965  @ epoch 2382 )\n",
      "[Epoch: 2420] train loss: 1.2108, train acc: 0.4900, val loss: 1.1899, val acc: 0.4941  (best train acc: 0.5017, best val acc: 0.5073, best train loss: 1.1960  @ epoch 2418 )\n",
      "[Epoch: 2440] train loss: 1.2016, train acc: 0.4939, val loss: 1.1895, val acc: 0.4975  (best train acc: 0.5017, best val acc: 0.5073, best train loss: 1.1960  @ epoch 2418 )\n",
      "[Epoch: 2460] train loss: 1.2006, train acc: 0.4944, val loss: 1.1890, val acc: 0.4907  (best train acc: 0.5017, best val acc: 0.5073, best train loss: 1.1931  @ epoch 2450 )\n",
      "[Epoch: 2480] train loss: 1.1897, train acc: 0.4999, val loss: 1.1867, val acc: 0.5005  (best train acc: 0.5059, best val acc: 0.5073, best train loss: 1.1850  @ epoch 2477 )\n",
      "[Epoch: 2500] train loss: 1.2005, train acc: 0.4907, val loss: 1.1850, val acc: 0.4938  (best train acc: 0.5059, best val acc: 0.5073, best train loss: 1.1850  @ epoch 2477 )\n",
      "[Epoch: 2520] train loss: 1.1925, train acc: 0.4986, val loss: 1.1845, val acc: 0.4944  (best train acc: 0.5059, best val acc: 0.5073, best train loss: 1.1850  @ epoch 2477 )\n",
      "[Epoch: 2540] train loss: 1.1965, train acc: 0.4906, val loss: 1.1833, val acc: 0.4958  (best train acc: 0.5059, best val acc: 0.5073, best train loss: 1.1850  @ epoch 2477 )\n",
      "[Epoch: 2560] train loss: 1.1913, train acc: 0.5010, val loss: 1.1832, val acc: 0.4958  (best train acc: 0.5059, best val acc: 0.5073, best train loss: 1.1850  @ epoch 2477 )\n",
      "[Epoch: 2580] train loss: 1.1917, train acc: 0.4903, val loss: 1.1821, val acc: 0.4978  (best train acc: 0.5059, best val acc: 0.5073, best train loss: 1.1850  @ epoch 2477 )\n",
      "[Epoch: 2600] train loss: 1.1932, train acc: 0.4954, val loss: 1.1821, val acc: 0.4941  (best train acc: 0.5059, best val acc: 0.5073, best train loss: 1.1848  @ epoch 2591 )\n",
      "[Epoch: 2620] train loss: 1.1941, train acc: 0.4978, val loss: 1.1812, val acc: 0.4998  (best train acc: 0.5059, best val acc: 0.5073, best train loss: 1.1848  @ epoch 2591 )\n",
      "[Epoch: 2640] train loss: 1.1926, train acc: 0.4967, val loss: 1.1803, val acc: 0.4978  (best train acc: 0.5059, best val acc: 0.5073, best train loss: 1.1824  @ epoch 2621 )\n",
      "[Epoch: 2660] train loss: 1.1878, train acc: 0.4966, val loss: 1.1799, val acc: 0.4988  (best train acc: 0.5059, best val acc: 0.5073, best train loss: 1.1824  @ epoch 2621 )\n",
      "[Epoch: 2680] train loss: 1.1882, train acc: 0.4936, val loss: 1.1793, val acc: 0.4985  (best train acc: 0.5059, best val acc: 0.5073, best train loss: 1.1824  @ epoch 2621 )\n",
      "[Epoch: 2700] train loss: 1.1901, train acc: 0.4961, val loss: 1.1794, val acc: 0.4924  (best train acc: 0.5059, best val acc: 0.5073, best train loss: 1.1806  @ epoch 2683 )\n",
      "[Epoch: 2720] train loss: 1.1869, train acc: 0.5019, val loss: 1.1795, val acc: 0.4978  (best train acc: 0.5064, best val acc: 0.5073, best train loss: 1.1806  @ epoch 2683 )\n",
      "[Epoch: 2740] train loss: 1.1868, train acc: 0.4993, val loss: 1.1782, val acc: 0.5002  (best train acc: 0.5064, best val acc: 0.5073, best train loss: 1.1806  @ epoch 2683 )\n",
      "[Epoch: 2760] train loss: 1.1838, train acc: 0.4988, val loss: 1.1788, val acc: 0.5042  (best train acc: 0.5064, best val acc: 0.5073, best train loss: 1.1806  @ epoch 2683 )\n",
      "[Epoch: 2780] train loss: 1.1920, train acc: 0.4932, val loss: 1.1773, val acc: 0.4981  (best train acc: 0.5064, best val acc: 0.5073, best train loss: 1.1805  @ epoch 2767 )\n",
      "[Epoch: 2800] train loss: 1.1826, train acc: 0.4975, val loss: 1.1772, val acc: 0.5008  (best train acc: 0.5064, best val acc: 0.5073, best train loss: 1.1795  @ epoch 2788 )\n",
      "[Epoch: 2820] train loss: 1.1850, train acc: 0.4979, val loss: 1.1766, val acc: 0.5032  (best train acc: 0.5064, best val acc: 0.5073, best train loss: 1.1795  @ epoch 2788 )\n",
      "[Epoch: 2840] train loss: 1.1931, train acc: 0.4940, val loss: 1.1772, val acc: 0.5005  (best train acc: 0.5064, best val acc: 0.5073, best train loss: 1.1790  @ epoch 2831 )\n",
      "[Epoch: 2860] train loss: 1.1911, train acc: 0.4949, val loss: 1.1767, val acc: 0.5046  (best train acc: 0.5064, best val acc: 0.5073, best train loss: 1.1790  @ epoch 2831 )\n",
      "[Epoch: 2880] train loss: 1.1830, train acc: 0.4949, val loss: 1.1757, val acc: 0.4944  (best train acc: 0.5064, best val acc: 0.5073, best train loss: 1.1729  @ epoch 2865 )\n",
      "[Epoch: 2900] train loss: 1.1822, train acc: 0.4923, val loss: 1.1761, val acc: 0.4958  (best train acc: 0.5064, best val acc: 0.5073, best train loss: 1.1729  @ epoch 2865 )\n",
      "[Epoch: 2920] train loss: 1.1857, train acc: 0.5030, val loss: 1.1758, val acc: 0.4931  (best train acc: 0.5065, best val acc: 0.5073, best train loss: 1.1729  @ epoch 2865 )\n",
      "[Epoch: 2940] train loss: 1.1781, train acc: 0.5014, val loss: 1.1744, val acc: 0.5025  (best train acc: 0.5065, best val acc: 0.5073, best train loss: 1.1729  @ epoch 2865 )\n",
      "[Epoch: 2960] train loss: 1.1788, train acc: 0.5022, val loss: 1.1736, val acc: 0.5022  (best train acc: 0.5065, best val acc: 0.5073, best train loss: 1.1725  @ epoch 2953 )\n",
      "[Epoch: 2980] train loss: 1.1810, train acc: 0.4968, val loss: 1.1739, val acc: 0.5012  (best train acc: 0.5065, best val acc: 0.5073, best train loss: 1.1723  @ epoch 2965 )\n",
      "[Epoch: 3000] train loss: 1.1780, train acc: 0.5059, val loss: 1.1733, val acc: 0.5062  (best train acc: 0.5096, best val acc: 0.5073, best train loss: 1.1723  @ epoch 2965 )\n",
      "[Epoch: 3020] train loss: 1.1736, train acc: 0.5030, val loss: 1.1725, val acc: 0.5008  (best train acc: 0.5096, best val acc: 0.5079, best train loss: 1.1685  @ epoch 3013 )\n",
      "[Epoch: 3040] train loss: 1.1796, train acc: 0.4996, val loss: 1.1724, val acc: 0.5022  (best train acc: 0.5096, best val acc: 0.5079, best train loss: 1.1685  @ epoch 3013 )\n",
      "[Epoch: 3060] train loss: 1.1746, train acc: 0.5051, val loss: 1.1724, val acc: 0.5035  (best train acc: 0.5131, best val acc: 0.5113, best train loss: 1.1685  @ epoch 3013 )\n",
      "[Epoch: 3080] train loss: 1.1853, train acc: 0.5045, val loss: 1.1748, val acc: 0.4961  (best train acc: 0.5131, best val acc: 0.5116, best train loss: 1.1685  @ epoch 3013 )\n",
      "[Epoch: 3100] train loss: 1.1824, train acc: 0.4956, val loss: 1.1732, val acc: 0.5093  (best train acc: 0.5131, best val acc: 0.5130, best train loss: 1.1685  @ epoch 3013 )\n",
      "[Epoch: 3120] train loss: 1.1782, train acc: 0.4979, val loss: 1.1724, val acc: 0.5025  (best train acc: 0.5131, best val acc: 0.5130, best train loss: 1.1685  @ epoch 3013 )\n",
      "[Epoch: 3140] train loss: 1.1717, train acc: 0.5053, val loss: 1.1717, val acc: 0.5005  (best train acc: 0.5131, best val acc: 0.5130, best train loss: 1.1685  @ epoch 3013 )\n",
      "[Epoch: 3160] train loss: 1.1663, train acc: 0.5111, val loss: 1.1699, val acc: 0.5062  (best train acc: 0.5131, best val acc: 0.5130, best train loss: 1.1652  @ epoch 3145 )\n",
      "[Epoch: 3180] train loss: 1.1736, train acc: 0.4983, val loss: 1.1693, val acc: 0.5019  (best train acc: 0.5131, best val acc: 0.5130, best train loss: 1.1652  @ epoch 3145 )\n",
      "[Epoch: 3200] train loss: 1.1734, train acc: 0.5009, val loss: 1.1715, val acc: 0.5029  (best train acc: 0.5131, best val acc: 0.5130, best train loss: 1.1652  @ epoch 3145 )\n",
      "[Epoch: 3220] train loss: 1.1666, train acc: 0.5053, val loss: 1.1696, val acc: 0.5086  (best train acc: 0.5131, best val acc: 0.5130, best train loss: 1.1652  @ epoch 3145 )\n",
      "[Epoch: 3240] train loss: 1.1737, train acc: 0.5035, val loss: 1.1688, val acc: 0.5052  (best train acc: 0.5134, best val acc: 0.5130, best train loss: 1.1652  @ epoch 3145 )\n",
      "[Epoch: 3260] train loss: 1.1670, train acc: 0.5108, val loss: 1.1723, val acc: 0.5147  (best train acc: 0.5134, best val acc: 0.5147, best train loss: 1.1644  @ epoch 3246 )\n",
      "[Epoch: 3280] train loss: 1.1690, train acc: 0.5026, val loss: 1.1692, val acc: 0.4975  (best train acc: 0.5134, best val acc: 0.5147, best train loss: 1.1644  @ epoch 3246 )\n",
      "[Epoch: 3300] train loss: 1.1699, train acc: 0.5100, val loss: 1.1687, val acc: 0.5062  (best train acc: 0.5134, best val acc: 0.5147, best train loss: 1.1623  @ epoch 3299 )\n",
      "[Epoch: 3320] train loss: 1.1645, train acc: 0.5115, val loss: 1.1670, val acc: 0.5056  (best train acc: 0.5134, best val acc: 0.5147, best train loss: 1.1622  @ epoch 3317 )\n",
      "[Epoch: 3340] train loss: 1.1698, train acc: 0.5157, val loss: 1.1674, val acc: 0.5089  (best train acc: 0.5157, best val acc: 0.5147, best train loss: 1.1622  @ epoch 3317 )\n",
      "[Epoch: 3360] train loss: 1.1681, train acc: 0.5022, val loss: 1.1691, val acc: 0.5029  (best train acc: 0.5157, best val acc: 0.5147, best train loss: 1.1620  @ epoch 3343 )\n",
      "[Epoch: 3380] train loss: 1.1681, train acc: 0.5074, val loss: 1.1668, val acc: 0.5052  (best train acc: 0.5157, best val acc: 0.5147, best train loss: 1.1620  @ epoch 3343 )\n",
      "[Epoch: 3400] train loss: 1.1717, train acc: 0.5061, val loss: 1.1661, val acc: 0.5052  (best train acc: 0.5157, best val acc: 0.5147, best train loss: 1.1620  @ epoch 3343 )\n",
      "[Epoch: 3420] train loss: 1.1737, train acc: 0.5016, val loss: 1.1669, val acc: 0.5052  (best train acc: 0.5157, best val acc: 0.5147, best train loss: 1.1620  @ epoch 3343 )\n",
      "[Epoch: 3440] train loss: 1.1699, train acc: 0.5068, val loss: 1.1672, val acc: 0.5130  (best train acc: 0.5157, best val acc: 0.5147, best train loss: 1.1620  @ epoch 3343 )\n",
      "[Epoch: 3460] train loss: 1.1679, train acc: 0.5103, val loss: 1.1654, val acc: 0.5093  (best train acc: 0.5158, best val acc: 0.5147, best train loss: 1.1606  @ epoch 3452 )\n",
      "[Epoch: 3480] train loss: 1.1640, train acc: 0.5128, val loss: 1.1683, val acc: 0.5022  (best train acc: 0.5192, best val acc: 0.5147, best train loss: 1.1600  @ epoch 3472 )\n",
      "[Epoch: 3500] train loss: 1.1611, train acc: 0.5089, val loss: 1.1669, val acc: 0.5019  (best train acc: 0.5192, best val acc: 0.5150, best train loss: 1.1600  @ epoch 3472 )\n",
      "[Epoch: 3520] train loss: 1.1675, train acc: 0.5119, val loss: 1.1653, val acc: 0.5120  (best train acc: 0.5192, best val acc: 0.5150, best train loss: 1.1600  @ epoch 3472 )\n",
      "[Epoch: 3540] train loss: 1.1618, train acc: 0.5101, val loss: 1.1649, val acc: 0.5086  (best train acc: 0.5192, best val acc: 0.5150, best train loss: 1.1583  @ epoch 3535 )\n",
      "[Epoch: 3560] train loss: 1.1679, train acc: 0.5152, val loss: 1.1643, val acc: 0.5110  (best train acc: 0.5192, best val acc: 0.5150, best train loss: 1.1580  @ epoch 3553 )\n",
      "[Epoch: 3580] train loss: 1.1670, train acc: 0.5137, val loss: 1.1639, val acc: 0.5056  (best train acc: 0.5192, best val acc: 0.5150, best train loss: 1.1557  @ epoch 3562 )\n",
      "[Epoch: 3600] train loss: 1.1631, train acc: 0.5128, val loss: 1.1641, val acc: 0.5099  (best train acc: 0.5192, best val acc: 0.5157, best train loss: 1.1557  @ epoch 3562 )\n",
      "[Epoch: 3620] train loss: 1.1561, train acc: 0.5145, val loss: 1.1643, val acc: 0.5049  (best train acc: 0.5192, best val acc: 0.5157, best train loss: 1.1557  @ epoch 3562 )\n",
      "[Epoch: 3640] train loss: 1.1665, train acc: 0.5064, val loss: 1.1631, val acc: 0.5052  (best train acc: 0.5192, best val acc: 0.5157, best train loss: 1.1557  @ epoch 3562 )\n",
      "[Epoch: 3660] train loss: 1.1671, train acc: 0.5040, val loss: 1.1639, val acc: 0.5099  (best train acc: 0.5192, best val acc: 0.5157, best train loss: 1.1557  @ epoch 3562 )\n",
      "[Epoch: 3680] train loss: 1.1543, train acc: 0.5142, val loss: 1.1626, val acc: 0.5089  (best train acc: 0.5192, best val acc: 0.5157, best train loss: 1.1543  @ epoch 3680 )\n",
      "[Epoch: 3700] train loss: 1.1653, train acc: 0.4987, val loss: 1.1634, val acc: 0.5039  (best train acc: 0.5192, best val acc: 0.5157, best train loss: 1.1543  @ epoch 3680 )\n",
      "[Epoch: 3720] train loss: 1.1677, train acc: 0.5064, val loss: 1.1645, val acc: 0.5150  (best train acc: 0.5192, best val acc: 0.5157, best train loss: 1.1543  @ epoch 3680 )\n",
      "[Epoch: 3740] train loss: 1.1637, train acc: 0.5080, val loss: 1.1630, val acc: 0.5076  (best train acc: 0.5192, best val acc: 0.5174, best train loss: 1.1543  @ epoch 3680 )\n",
      "[Epoch: 3760] train loss: 1.1761, train acc: 0.4970, val loss: 1.1651, val acc: 0.5035  (best train acc: 0.5207, best val acc: 0.5174, best train loss: 1.1543  @ epoch 3680 )\n",
      "[Epoch: 3780] train loss: 1.1629, train acc: 0.5105, val loss: 1.1649, val acc: 0.5025  (best train acc: 0.5207, best val acc: 0.5174, best train loss: 1.1543  @ epoch 3680 )\n",
      "[Epoch: 3800] train loss: 1.1626, train acc: 0.5123, val loss: 1.1615, val acc: 0.5069  (best train acc: 0.5207, best val acc: 0.5174, best train loss: 1.1543  @ epoch 3680 )\n",
      "[Epoch: 3820] train loss: 1.1698, train acc: 0.5077, val loss: 1.1695, val acc: 0.5073  (best train acc: 0.5207, best val acc: 0.5174, best train loss: 1.1543  @ epoch 3680 )\n",
      "[Epoch: 3840] train loss: 1.1812, train acc: 0.5139, val loss: 1.1749, val acc: 0.5019  (best train acc: 0.5207, best val acc: 0.5174, best train loss: 1.1541  @ epoch 3839 )\n",
      "[Epoch: 3860] train loss: 1.2141, train acc: 0.4727, val loss: 1.1766, val acc: 0.5073  (best train acc: 0.5207, best val acc: 0.5174, best train loss: 1.1541  @ epoch 3839 )\n",
      "[Epoch: 3880] train loss: 1.1676, train acc: 0.5104, val loss: 1.1697, val acc: 0.5079  (best train acc: 0.5207, best val acc: 0.5174, best train loss: 1.1541  @ epoch 3839 )\n",
      "[Epoch: 3900] train loss: 1.1576, train acc: 0.5144, val loss: 1.1659, val acc: 0.5066  (best train acc: 0.5207, best val acc: 0.5174, best train loss: 1.1541  @ epoch 3839 )\n",
      "[Epoch: 3920] train loss: 1.1629, train acc: 0.5072, val loss: 1.1637, val acc: 0.5046  (best train acc: 0.5207, best val acc: 0.5174, best train loss: 1.1541  @ epoch 3839 )\n",
      "[Epoch: 3940] train loss: 1.1598, train acc: 0.5072, val loss: 1.1630, val acc: 0.5056  (best train acc: 0.5207, best val acc: 0.5174, best train loss: 1.1534  @ epoch 3925 )\n",
      "[Epoch: 3960] train loss: 1.1670, train acc: 0.5076, val loss: 1.1623, val acc: 0.5052  (best train acc: 0.5207, best val acc: 0.5174, best train loss: 1.1534  @ epoch 3925 )\n",
      "[Epoch: 3980] train loss: 1.1585, train acc: 0.5093, val loss: 1.1617, val acc: 0.5106  (best train acc: 0.5207, best val acc: 0.5174, best train loss: 1.1497  @ epoch 3973 )\n",
      "[Epoch: 4000] train loss: 1.1732, train acc: 0.5075, val loss: 1.1607, val acc: 0.5079  (best train acc: 0.5207, best val acc: 0.5174, best train loss: 1.1497  @ epoch 3973 )\n",
      "[Epoch: 4020] train loss: 1.2114, train acc: 0.5117, val loss: 1.2045, val acc: 0.5099  (best train acc: 0.5207, best val acc: 0.5174, best train loss: 1.1497  @ epoch 3973 )\n",
      "[Epoch: 4040] train loss: 1.1597, train acc: 0.5205, val loss: 1.1660, val acc: 0.5123  (best train acc: 0.5207, best val acc: 0.5174, best train loss: 1.1497  @ epoch 3973 )\n",
      "[Epoch: 4060] train loss: 1.2113, train acc: 0.4933, val loss: 1.2073, val acc: 0.5025  (best train acc: 0.5207, best val acc: 0.5174, best train loss: 1.1497  @ epoch 3973 )\n",
      "[Epoch: 4080] train loss: 1.1691, train acc: 0.5171, val loss: 1.1706, val acc: 0.5143  (best train acc: 0.5207, best val acc: 0.5194, best train loss: 1.1497  @ epoch 3973 )\n",
      "[Epoch: 4100] train loss: 1.1679, train acc: 0.5065, val loss: 1.1671, val acc: 0.5073  (best train acc: 0.5207, best val acc: 0.5194, best train loss: 1.1497  @ epoch 3973 )\n",
      "[Epoch: 4120] train loss: 1.1590, train acc: 0.5112, val loss: 1.1632, val acc: 0.5137  (best train acc: 0.5207, best val acc: 0.5194, best train loss: 1.1497  @ epoch 3973 )\n",
      "[Epoch: 4140] train loss: 1.1608, train acc: 0.5083, val loss: 1.1621, val acc: 0.5096  (best train acc: 0.5207, best val acc: 0.5194, best train loss: 1.1497  @ epoch 3973 )\n",
      "[Epoch: 4160] train loss: 1.1632, train acc: 0.5085, val loss: 1.1612, val acc: 0.5110  (best train acc: 0.5207, best val acc: 0.5194, best train loss: 1.1497  @ epoch 3973 )\n",
      "[Epoch: 4180] train loss: 1.1538, train acc: 0.5152, val loss: 1.1609, val acc: 0.5099  (best train acc: 0.5207, best val acc: 0.5194, best train loss: 1.1497  @ epoch 3973 )\n",
      "[Epoch: 4200] train loss: 1.1550, train acc: 0.5197, val loss: 1.1606, val acc: 0.5103  (best train acc: 0.5207, best val acc: 0.5194, best train loss: 1.1497  @ epoch 3973 )\n",
      "[Epoch: 4220] train loss: 1.1582, train acc: 0.5175, val loss: 1.1599, val acc: 0.5096  (best train acc: 0.5207, best val acc: 0.5194, best train loss: 1.1497  @ epoch 3973 )\n",
      "[Epoch: 4240] train loss: 1.1568, train acc: 0.5134, val loss: 1.1595, val acc: 0.5103  (best train acc: 0.5210, best val acc: 0.5194, best train loss: 1.1497  @ epoch 3973 )\n",
      "[Epoch: 4260] train loss: 1.1647, train acc: 0.5082, val loss: 1.1592, val acc: 0.5123  (best train acc: 0.5210, best val acc: 0.5194, best train loss: 1.1497  @ epoch 3973 )\n",
      "[Epoch: 4280] train loss: 1.2097, train acc: 0.4935, val loss: 1.2101, val acc: 0.4985  (best train acc: 0.5210, best val acc: 0.5194, best train loss: 1.1497  @ epoch 3973 )\n",
      "[Epoch: 4300] train loss: 1.1759, train acc: 0.5098, val loss: 1.1782, val acc: 0.5137  (best train acc: 0.5210, best val acc: 0.5194, best train loss: 1.1497  @ epoch 3973 )\n",
      "[Epoch: 4320] train loss: 1.1603, train acc: 0.5093, val loss: 1.1607, val acc: 0.5167  (best train acc: 0.5210, best val acc: 0.5194, best train loss: 1.1497  @ epoch 3973 )\n",
      "[Epoch: 4340] train loss: 1.1599, train acc: 0.5103, val loss: 1.1597, val acc: 0.5079  (best train acc: 0.5241, best val acc: 0.5194, best train loss: 1.1497  @ epoch 3973 )\n",
      "[Epoch: 4360] train loss: 1.1571, train acc: 0.5055, val loss: 1.1593, val acc: 0.5110  (best train acc: 0.5241, best val acc: 0.5194, best train loss: 1.1497  @ epoch 3973 )\n",
      "[Epoch: 4380] train loss: 1.1567, train acc: 0.5162, val loss: 1.1592, val acc: 0.5143  (best train acc: 0.5241, best val acc: 0.5201, best train loss: 1.1497  @ epoch 3973 )\n",
      "[Epoch: 4400] train loss: 1.1542, train acc: 0.5167, val loss: 1.1599, val acc: 0.5126  (best train acc: 0.5241, best val acc: 0.5201, best train loss: 1.1497  @ epoch 3973 )\n",
      "[Epoch: 4420] train loss: 1.1627, train acc: 0.5082, val loss: 1.1591, val acc: 0.5103  (best train acc: 0.5253, best val acc: 0.5201, best train loss: 1.1495  @ epoch 4407 )\n",
      "[Epoch: 4440] train loss: 1.1536, train acc: 0.5084, val loss: 1.1572, val acc: 0.5083  (best train acc: 0.5253, best val acc: 0.5201, best train loss: 1.1495  @ epoch 4407 )\n",
      "[Epoch: 4460] train loss: 1.1559, train acc: 0.5134, val loss: 1.1565, val acc: 0.5143  (best train acc: 0.5253, best val acc: 0.5201, best train loss: 1.1495  @ epoch 4407 )\n",
      "[Epoch: 4480] train loss: 1.1563, train acc: 0.5155, val loss: 1.1571, val acc: 0.5130  (best train acc: 0.5253, best val acc: 0.5201, best train loss: 1.1494  @ epoch 4478 )\n",
      "[Epoch: 4500] train loss: 1.1557, train acc: 0.5107, val loss: 1.1559, val acc: 0.5130  (best train acc: 0.5253, best val acc: 0.5201, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 4520] train loss: 1.1770, train acc: 0.5098, val loss: 1.1840, val acc: 0.5079  (best train acc: 0.5253, best val acc: 0.5201, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 4540] train loss: 1.2508, train acc: 0.4833, val loss: 1.2459, val acc: 0.4954  (best train acc: 0.5253, best val acc: 0.5201, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 4560] train loss: 1.6906, train acc: 0.4152, val loss: 1.5656, val acc: 0.3841  (best train acc: 0.5253, best val acc: 0.5201, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 4580] train loss: 1.2378, train acc: 0.4944, val loss: 1.2223, val acc: 0.5042  (best train acc: 0.5253, best val acc: 0.5201, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 4600] train loss: 1.2301, train acc: 0.4926, val loss: 1.2268, val acc: 0.4978  (best train acc: 0.5253, best val acc: 0.5201, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 4620] train loss: 1.2189, train acc: 0.4892, val loss: 1.2221, val acc: 0.4927  (best train acc: 0.5253, best val acc: 0.5201, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 4640] train loss: 1.2033, train acc: 0.5015, val loss: 1.2145, val acc: 0.4992  (best train acc: 0.5253, best val acc: 0.5201, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 4660] train loss: 1.2045, train acc: 0.5033, val loss: 1.2071, val acc: 0.4985  (best train acc: 0.5253, best val acc: 0.5201, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 4680] train loss: 1.1959, train acc: 0.4998, val loss: 1.2000, val acc: 0.5002  (best train acc: 0.5253, best val acc: 0.5201, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 4700] train loss: 1.1906, train acc: 0.5015, val loss: 1.1972, val acc: 0.4988  (best train acc: 0.5253, best val acc: 0.5201, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 4720] train loss: 1.1856, train acc: 0.5082, val loss: 1.1959, val acc: 0.5019  (best train acc: 0.5253, best val acc: 0.5201, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 4740] train loss: 1.1883, train acc: 0.5104, val loss: 1.1951, val acc: 0.5042  (best train acc: 0.5253, best val acc: 0.5201, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 4760] train loss: 1.1864, train acc: 0.5057, val loss: 1.1940, val acc: 0.4981  (best train acc: 0.5253, best val acc: 0.5201, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 4780] train loss: 1.1924, train acc: 0.5009, val loss: 1.1913, val acc: 0.5008  (best train acc: 0.5253, best val acc: 0.5201, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 4800] train loss: 1.1909, train acc: 0.5046, val loss: 1.1899, val acc: 0.5025  (best train acc: 0.5253, best val acc: 0.5201, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 4820] train loss: 1.1828, train acc: 0.5151, val loss: 1.1891, val acc: 0.5066  (best train acc: 0.5253, best val acc: 0.5201, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 4840] train loss: 1.1812, train acc: 0.5071, val loss: 1.1911, val acc: 0.4961  (best train acc: 0.5253, best val acc: 0.5201, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 4860] train loss: 1.1817, train acc: 0.5061, val loss: 1.1872, val acc: 0.5073  (best train acc: 0.5253, best val acc: 0.5201, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 4880] train loss: 1.1845, train acc: 0.5058, val loss: 1.1862, val acc: 0.5042  (best train acc: 0.5253, best val acc: 0.5201, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 4900] train loss: 1.1803, train acc: 0.5121, val loss: 1.1878, val acc: 0.5093  (best train acc: 0.5253, best val acc: 0.5201, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 4920] train loss: 1.1852, train acc: 0.5112, val loss: 1.1861, val acc: 0.4998  (best train acc: 0.5253, best val acc: 0.5201, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 4940] train loss: 1.1817, train acc: 0.5045, val loss: 1.1844, val acc: 0.5083  (best train acc: 0.5253, best val acc: 0.5201, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 4960] train loss: 1.1851, train acc: 0.5050, val loss: 1.1838, val acc: 0.5073  (best train acc: 0.5253, best val acc: 0.5201, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 4980] train loss: 1.1843, train acc: 0.5118, val loss: 1.1832, val acc: 0.5056  (best train acc: 0.5253, best val acc: 0.5201, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 5000] train loss: 1.1753, train acc: 0.5161, val loss: 1.1825, val acc: 0.5029  (best train acc: 0.5253, best val acc: 0.5201, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 5020] train loss: 1.1758, train acc: 0.5102, val loss: 1.1822, val acc: 0.5066  (best train acc: 0.5253, best val acc: 0.5201, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 5040] train loss: 1.1838, train acc: 0.5150, val loss: 1.1822, val acc: 0.5032  (best train acc: 0.5253, best val acc: 0.5201, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 5060] train loss: 1.1797, train acc: 0.5064, val loss: 1.1817, val acc: 0.5110  (best train acc: 0.5253, best val acc: 0.5201, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 5080] train loss: 1.1747, train acc: 0.5134, val loss: 1.1826, val acc: 0.5110  (best train acc: 0.5253, best val acc: 0.5201, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 5100] train loss: 1.1773, train acc: 0.5067, val loss: 1.1803, val acc: 0.5035  (best train acc: 0.5253, best val acc: 0.5201, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 5120] train loss: 1.1745, train acc: 0.5156, val loss: 1.1796, val acc: 0.5056  (best train acc: 0.5253, best val acc: 0.5201, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 5140] train loss: 1.1796, train acc: 0.5028, val loss: 1.1793, val acc: 0.5076  (best train acc: 0.5253, best val acc: 0.5201, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 5160] train loss: 1.1752, train acc: 0.5007, val loss: 1.1790, val acc: 0.5066  (best train acc: 0.5253, best val acc: 0.5201, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 5180] train loss: 1.1709, train acc: 0.5144, val loss: 1.1795, val acc: 0.5056  (best train acc: 0.5253, best val acc: 0.5201, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 5200] train loss: 1.1733, train acc: 0.5074, val loss: 1.1777, val acc: 0.5076  (best train acc: 0.5253, best val acc: 0.5201, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 5220] train loss: 1.1741, train acc: 0.5127, val loss: 1.1788, val acc: 0.5066  (best train acc: 0.5253, best val acc: 0.5201, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 5240] train loss: 1.1691, train acc: 0.5151, val loss: 1.1778, val acc: 0.5110  (best train acc: 0.5253, best val acc: 0.5201, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 5260] train loss: 1.1771, train acc: 0.5085, val loss: 1.1766, val acc: 0.5093  (best train acc: 0.5253, best val acc: 0.5201, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 5280] train loss: 1.1706, train acc: 0.5148, val loss: 1.1792, val acc: 0.5137  (best train acc: 0.5253, best val acc: 0.5201, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 5300] train loss: 1.1686, train acc: 0.5187, val loss: 1.1762, val acc: 0.5076  (best train acc: 0.5253, best val acc: 0.5201, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 5320] train loss: 1.1756, train acc: 0.5115, val loss: 1.1756, val acc: 0.5059  (best train acc: 0.5253, best val acc: 0.5201, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 5340] train loss: 1.1796, train acc: 0.5010, val loss: 1.1797, val acc: 0.5170  (best train acc: 0.5253, best val acc: 0.5201, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 5360] train loss: 1.1767, train acc: 0.5173, val loss: 1.1771, val acc: 0.5130  (best train acc: 0.5253, best val acc: 0.5201, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 5380] train loss: 1.1724, train acc: 0.5215, val loss: 1.1752, val acc: 0.5093  (best train acc: 0.5253, best val acc: 0.5201, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 5400] train loss: 1.1747, train acc: 0.5134, val loss: 1.1748, val acc: 0.5066  (best train acc: 0.5253, best val acc: 0.5201, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 5420] train loss: 1.1730, train acc: 0.5192, val loss: 1.1752, val acc: 0.5096  (best train acc: 0.5253, best val acc: 0.5201, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 5440] train loss: 1.1798, train acc: 0.5176, val loss: 1.1833, val acc: 0.5147  (best train acc: 0.5253, best val acc: 0.5201, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 5460] train loss: 1.1840, train acc: 0.5147, val loss: 1.1767, val acc: 0.5157  (best train acc: 0.5265, best val acc: 0.5201, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 5480] train loss: 1.1739, train acc: 0.5187, val loss: 1.1783, val acc: 0.5153  (best train acc: 0.5265, best val acc: 0.5201, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 5500] train loss: 1.1658, train acc: 0.5261, val loss: 1.1747, val acc: 0.5150  (best train acc: 0.5265, best val acc: 0.5201, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 5520] train loss: 1.1790, train acc: 0.5138, val loss: 1.1739, val acc: 0.5143  (best train acc: 0.5265, best val acc: 0.5201, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 5540] train loss: 1.1730, train acc: 0.5187, val loss: 1.1751, val acc: 0.5069  (best train acc: 0.5265, best val acc: 0.5201, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 5560] train loss: 1.1631, train acc: 0.5181, val loss: 1.1736, val acc: 0.5083  (best train acc: 0.5265, best val acc: 0.5201, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 5580] train loss: 1.1654, train acc: 0.5216, val loss: 1.1728, val acc: 0.5123  (best train acc: 0.5265, best val acc: 0.5201, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 5600] train loss: 1.1739, train acc: 0.5082, val loss: 1.1726, val acc: 0.5160  (best train acc: 0.5265, best val acc: 0.5201, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 5620] train loss: 1.1701, train acc: 0.5151, val loss: 1.1721, val acc: 0.5076  (best train acc: 0.5265, best val acc: 0.5201, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 5640] train loss: 1.1734, train acc: 0.5146, val loss: 1.1715, val acc: 0.5069  (best train acc: 0.5265, best val acc: 0.5201, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 5660] train loss: 1.1669, train acc: 0.5218, val loss: 1.1740, val acc: 0.5093  (best train acc: 0.5265, best val acc: 0.5201, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 5680] train loss: 1.1667, train acc: 0.5172, val loss: 1.1708, val acc: 0.5073  (best train acc: 0.5265, best val acc: 0.5201, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 5700] train loss: 1.1677, train acc: 0.5212, val loss: 1.1708, val acc: 0.5130  (best train acc: 0.5265, best val acc: 0.5204, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 5720] train loss: 1.1742, train acc: 0.5136, val loss: 1.1704, val acc: 0.5113  (best train acc: 0.5265, best val acc: 0.5204, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 5740] train loss: 1.1732, train acc: 0.5154, val loss: 1.1704, val acc: 0.5113  (best train acc: 0.5265, best val acc: 0.5204, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 5760] train loss: 1.1668, train acc: 0.5121, val loss: 1.1697, val acc: 0.5143  (best train acc: 0.5265, best val acc: 0.5204, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 5780] train loss: 1.1622, train acc: 0.5184, val loss: 1.1694, val acc: 0.5079  (best train acc: 0.5265, best val acc: 0.5204, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 5800] train loss: 1.1717, train acc: 0.5160, val loss: 1.1699, val acc: 0.5143  (best train acc: 0.5265, best val acc: 0.5204, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 5820] train loss: 1.1891, train acc: 0.5098, val loss: 1.1703, val acc: 0.5177  (best train acc: 0.5265, best val acc: 0.5204, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 5840] train loss: 1.1783, train acc: 0.5095, val loss: 1.1720, val acc: 0.5177  (best train acc: 0.5265, best val acc: 0.5204, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 5860] train loss: 1.1685, train acc: 0.5162, val loss: 1.1685, val acc: 0.5147  (best train acc: 0.5265, best val acc: 0.5204, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 5880] train loss: 1.1628, train acc: 0.5122, val loss: 1.1719, val acc: 0.5191  (best train acc: 0.5265, best val acc: 0.5211, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 5900] train loss: 1.1630, train acc: 0.5132, val loss: 1.1708, val acc: 0.5187  (best train acc: 0.5296, best val acc: 0.5211, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 5920] train loss: 1.1703, train acc: 0.5169, val loss: 1.1679, val acc: 0.5184  (best train acc: 0.5296, best val acc: 0.5214, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 5940] train loss: 1.1656, train acc: 0.5129, val loss: 1.1709, val acc: 0.5184  (best train acc: 0.5296, best val acc: 0.5214, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 5960] train loss: 1.1628, train acc: 0.5192, val loss: 1.1670, val acc: 0.5150  (best train acc: 0.5296, best val acc: 0.5214, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 5980] train loss: 1.1655, train acc: 0.5193, val loss: 1.1678, val acc: 0.5184  (best train acc: 0.5296, best val acc: 0.5214, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 6000] train loss: 1.1659, train acc: 0.5205, val loss: 1.1662, val acc: 0.5153  (best train acc: 0.5296, best val acc: 0.5214, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 6020] train loss: 1.1565, train acc: 0.5171, val loss: 1.1659, val acc: 0.5167  (best train acc: 0.5296, best val acc: 0.5214, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 6040] train loss: 1.1921, train acc: 0.5070, val loss: 1.2314, val acc: 0.4840  (best train acc: 0.5304, best val acc: 0.5221, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 6060] train loss: 1.1850, train acc: 0.5014, val loss: 1.1934, val acc: 0.4931  (best train acc: 0.5304, best val acc: 0.5221, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 6080] train loss: 1.1773, train acc: 0.5090, val loss: 1.1795, val acc: 0.5052  (best train acc: 0.5304, best val acc: 0.5221, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 6100] train loss: 1.1808, train acc: 0.5089, val loss: 1.1774, val acc: 0.5029  (best train acc: 0.5304, best val acc: 0.5221, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 6120] train loss: 1.1712, train acc: 0.5080, val loss: 1.1774, val acc: 0.5022  (best train acc: 0.5304, best val acc: 0.5221, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 6140] train loss: 1.1722, train acc: 0.5174, val loss: 1.1772, val acc: 0.5086  (best train acc: 0.5304, best val acc: 0.5221, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 6160] train loss: 1.1696, train acc: 0.5155, val loss: 1.1755, val acc: 0.5029  (best train acc: 0.5304, best val acc: 0.5221, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 6180] train loss: 1.1624, train acc: 0.5150, val loss: 1.1751, val acc: 0.5096  (best train acc: 0.5304, best val acc: 0.5221, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 6200] train loss: 1.1691, train acc: 0.5153, val loss: 1.1743, val acc: 0.5052  (best train acc: 0.5304, best val acc: 0.5221, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 6220] train loss: 1.1711, train acc: 0.5069, val loss: 1.1733, val acc: 0.5035  (best train acc: 0.5304, best val acc: 0.5221, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 6240] train loss: 1.1578, train acc: 0.5171, val loss: 1.1727, val acc: 0.5093  (best train acc: 0.5304, best val acc: 0.5221, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 6260] train loss: 1.1604, train acc: 0.5215, val loss: 1.1718, val acc: 0.5059  (best train acc: 0.5304, best val acc: 0.5221, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 6280] train loss: 1.1689, train acc: 0.5142, val loss: 1.1702, val acc: 0.5062  (best train acc: 0.5304, best val acc: 0.5221, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 6300] train loss: 1.1916, train acc: 0.5060, val loss: 1.1841, val acc: 0.5073  (best train acc: 0.5304, best val acc: 0.5221, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 6320] train loss: 1.1778, train acc: 0.5131, val loss: 1.1757, val acc: 0.5083  (best train acc: 0.5304, best val acc: 0.5221, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 6340] train loss: 1.1643, train acc: 0.5101, val loss: 1.1716, val acc: 0.5052  (best train acc: 0.5304, best val acc: 0.5221, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 6360] train loss: 1.1663, train acc: 0.5145, val loss: 1.1688, val acc: 0.5039  (best train acc: 0.5304, best val acc: 0.5221, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 6380] train loss: 1.1910, train acc: 0.5061, val loss: 1.1760, val acc: 0.5096  (best train acc: 0.5304, best val acc: 0.5221, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 6400] train loss: 1.1663, train acc: 0.5083, val loss: 1.1761, val acc: 0.4965  (best train acc: 0.5304, best val acc: 0.5221, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 6420] train loss: 1.1776, train acc: 0.5099, val loss: 1.1717, val acc: 0.5066  (best train acc: 0.5304, best val acc: 0.5221, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 6440] train loss: 1.1735, train acc: 0.5096, val loss: 1.1710, val acc: 0.5076  (best train acc: 0.5304, best val acc: 0.5221, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 6460] train loss: 1.1654, train acc: 0.5183, val loss: 1.1706, val acc: 0.5056  (best train acc: 0.5304, best val acc: 0.5221, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 6480] train loss: 1.1596, train acc: 0.5130, val loss: 1.1697, val acc: 0.5103  (best train acc: 0.5304, best val acc: 0.5221, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 6500] train loss: 1.1618, train acc: 0.5122, val loss: 1.1695, val acc: 0.5069  (best train acc: 0.5304, best val acc: 0.5221, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 6520] train loss: 1.1634, train acc: 0.5146, val loss: 1.1686, val acc: 0.5106  (best train acc: 0.5304, best val acc: 0.5221, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 6540] train loss: 1.1604, train acc: 0.5179, val loss: 1.1682, val acc: 0.5120  (best train acc: 0.5304, best val acc: 0.5221, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 6560] train loss: 1.1629, train acc: 0.5180, val loss: 1.1676, val acc: 0.5120  (best train acc: 0.5304, best val acc: 0.5221, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 6580] train loss: 1.1614, train acc: 0.5154, val loss: 1.1669, val acc: 0.5103  (best train acc: 0.5304, best val acc: 0.5221, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 6600] train loss: 1.1593, train acc: 0.5209, val loss: 1.1669, val acc: 0.5099  (best train acc: 0.5304, best val acc: 0.5221, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 6620] train loss: 1.1589, train acc: 0.5211, val loss: 1.1659, val acc: 0.5140  (best train acc: 0.5304, best val acc: 0.5221, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 6640] train loss: 1.1703, train acc: 0.5121, val loss: 1.1651, val acc: 0.5130  (best train acc: 0.5304, best val acc: 0.5221, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 6660] train loss: 1.1648, train acc: 0.5173, val loss: 1.1642, val acc: 0.5113  (best train acc: 0.5304, best val acc: 0.5221, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 6680] train loss: 1.1612, train acc: 0.5163, val loss: 1.1630, val acc: 0.5120  (best train acc: 0.5304, best val acc: 0.5221, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 6700] train loss: 1.1886, train acc: 0.4743, val loss: 1.2463, val acc: 0.4621  (best train acc: 0.5304, best val acc: 0.5221, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 6720] train loss: 1.1826, train acc: 0.5134, val loss: 1.1852, val acc: 0.4988  (best train acc: 0.5304, best val acc: 0.5221, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 6740] train loss: 1.1677, train acc: 0.5194, val loss: 1.1761, val acc: 0.5143  (best train acc: 0.5304, best val acc: 0.5221, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 6760] train loss: 1.1639, train acc: 0.5244, val loss: 1.1734, val acc: 0.5147  (best train acc: 0.5304, best val acc: 0.5221, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 6780] train loss: 1.1653, train acc: 0.5184, val loss: 1.1717, val acc: 0.5164  (best train acc: 0.5304, best val acc: 0.5221, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 6800] train loss: 1.1589, train acc: 0.5214, val loss: 1.1706, val acc: 0.5187  (best train acc: 0.5304, best val acc: 0.5221, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 6820] train loss: 1.1669, train acc: 0.5182, val loss: 1.1696, val acc: 0.5191  (best train acc: 0.5304, best val acc: 0.5221, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 6840] train loss: 1.1731, train acc: 0.5124, val loss: 1.1696, val acc: 0.5177  (best train acc: 0.5304, best val acc: 0.5221, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 6860] train loss: 1.1639, train acc: 0.5219, val loss: 1.1691, val acc: 0.5221  (best train acc: 0.5304, best val acc: 0.5228, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 6880] train loss: 1.1542, train acc: 0.5294, val loss: 1.1684, val acc: 0.5201  (best train acc: 0.5305, best val acc: 0.5228, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 6900] train loss: 1.1673, train acc: 0.5198, val loss: 1.1687, val acc: 0.5174  (best train acc: 0.5305, best val acc: 0.5228, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 6920] train loss: 1.1562, train acc: 0.5262, val loss: 1.1675, val acc: 0.5187  (best train acc: 0.5337, best val acc: 0.5245, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 6940] train loss: 1.1601, train acc: 0.5251, val loss: 1.1675, val acc: 0.5214  (best train acc: 0.5337, best val acc: 0.5245, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 6960] train loss: 1.1557, train acc: 0.5248, val loss: 1.1665, val acc: 0.5214  (best train acc: 0.5337, best val acc: 0.5245, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 6980] train loss: 1.1501, train acc: 0.5310, val loss: 1.1661, val acc: 0.5207  (best train acc: 0.5337, best val acc: 0.5245, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 7000] train loss: 1.1532, train acc: 0.5285, val loss: 1.1657, val acc: 0.5187  (best train acc: 0.5337, best val acc: 0.5245, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 7020] train loss: 1.1608, train acc: 0.5228, val loss: 1.1652, val acc: 0.5231  (best train acc: 0.5337, best val acc: 0.5245, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 7040] train loss: 1.1622, train acc: 0.5230, val loss: 1.1649, val acc: 0.5201  (best train acc: 0.5337, best val acc: 0.5245, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 7060] train loss: 1.1561, train acc: 0.5250, val loss: 1.1646, val acc: 0.5231  (best train acc: 0.5337, best val acc: 0.5245, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 7080] train loss: 1.1533, train acc: 0.5275, val loss: 1.1639, val acc: 0.5214  (best train acc: 0.5337, best val acc: 0.5251, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 7100] train loss: 1.1496, train acc: 0.5280, val loss: 1.1638, val acc: 0.5197  (best train acc: 0.5337, best val acc: 0.5251, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 7120] train loss: 1.1557, train acc: 0.5296, val loss: 1.1633, val acc: 0.5201  (best train acc: 0.5341, best val acc: 0.5251, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 7140] train loss: 1.1640, train acc: 0.5215, val loss: 1.1631, val acc: 0.5228  (best train acc: 0.5341, best val acc: 0.5258, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 7160] train loss: 1.1578, train acc: 0.5241, val loss: 1.1625, val acc: 0.5218  (best train acc: 0.5341, best val acc: 0.5258, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 7180] train loss: 1.1499, train acc: 0.5332, val loss: 1.1620, val acc: 0.5251  (best train acc: 0.5377, best val acc: 0.5258, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 7200] train loss: 1.1503, train acc: 0.5247, val loss: 1.1615, val acc: 0.5207  (best train acc: 0.5377, best val acc: 0.5258, best train loss: 1.1428  @ epoch 7193 )\n",
      "[Epoch: 7220] train loss: 1.1618, train acc: 0.5205, val loss: 1.1611, val acc: 0.5221  (best train acc: 0.5377, best val acc: 0.5265, best train loss: 1.1428  @ epoch 7193 )\n",
      "[Epoch: 7240] train loss: 1.1635, train acc: 0.5222, val loss: 1.1606, val acc: 0.5241  (best train acc: 0.5377, best val acc: 0.5265, best train loss: 1.1428  @ epoch 7193 )\n",
      "[Epoch: 7260] train loss: 1.1516, train acc: 0.5291, val loss: 1.1603, val acc: 0.5231  (best train acc: 0.5377, best val acc: 0.5265, best train loss: 1.1428  @ epoch 7193 )\n",
      "[Epoch: 7280] train loss: 1.1532, train acc: 0.5254, val loss: 1.1604, val acc: 0.5194  (best train acc: 0.5377, best val acc: 0.5265, best train loss: 1.1428  @ epoch 7193 )\n",
      "[Epoch: 7300] train loss: 1.1536, train acc: 0.5290, val loss: 1.1592, val acc: 0.5251  (best train acc: 0.5377, best val acc: 0.5265, best train loss: 1.1428  @ epoch 7193 )\n",
      "[Epoch: 7320] train loss: 1.1551, train acc: 0.5251, val loss: 1.1588, val acc: 0.5241  (best train acc: 0.5377, best val acc: 0.5265, best train loss: 1.1428  @ epoch 7193 )\n",
      "[Epoch: 7340] train loss: 1.1548, train acc: 0.5303, val loss: 1.1585, val acc: 0.5245  (best train acc: 0.5377, best val acc: 0.5265, best train loss: 1.1428  @ epoch 7193 )\n",
      "[Epoch: 7360] train loss: 1.1520, train acc: 0.5253, val loss: 1.1578, val acc: 0.5231  (best train acc: 0.5387, best val acc: 0.5275, best train loss: 1.1390  @ epoch 7356 )\n",
      "[Epoch: 7380] train loss: 1.1522, train acc: 0.5340, val loss: 1.1577, val acc: 0.5272  (best train acc: 0.5387, best val acc: 0.5278, best train loss: 1.1390  @ epoch 7356 )\n",
      "[Epoch: 7400] train loss: 1.1466, train acc: 0.5296, val loss: 1.1571, val acc: 0.5234  (best train acc: 0.5387, best val acc: 0.5278, best train loss: 1.1390  @ epoch 7356 )\n",
      "[Epoch: 7420] train loss: 1.1542, train acc: 0.5309, val loss: 1.1569, val acc: 0.5234  (best train acc: 0.5387, best val acc: 0.5288, best train loss: 1.1390  @ epoch 7356 )\n",
      "[Epoch: 7440] train loss: 1.1551, train acc: 0.5275, val loss: 1.1566, val acc: 0.5258  (best train acc: 0.5387, best val acc: 0.5288, best train loss: 1.1390  @ epoch 7356 )\n",
      "[Epoch: 7460] train loss: 1.1503, train acc: 0.5265, val loss: 1.1560, val acc: 0.5245  (best train acc: 0.5387, best val acc: 0.5288, best train loss: 1.1390  @ epoch 7356 )\n",
      "[Epoch: 7480] train loss: 1.1477, train acc: 0.5308, val loss: 1.1556, val acc: 0.5261  (best train acc: 0.5387, best val acc: 0.5288, best train loss: 1.1390  @ epoch 7356 )\n",
      "[Epoch: 7500] train loss: 1.1498, train acc: 0.5275, val loss: 1.1555, val acc: 0.5248  (best train acc: 0.5387, best val acc: 0.5298, best train loss: 1.1367  @ epoch 7498 )\n",
      "[Epoch: 7520] train loss: 1.1482, train acc: 0.5296, val loss: 1.1553, val acc: 0.5285  (best train acc: 0.5387, best val acc: 0.5298, best train loss: 1.1367  @ epoch 7498 )\n",
      "[Epoch: 7540] train loss: 1.1431, train acc: 0.5301, val loss: 1.1543, val acc: 0.5272  (best train acc: 0.5387, best val acc: 0.5309, best train loss: 1.1367  @ epoch 7498 )\n",
      "[Epoch: 7560] train loss: 1.1509, train acc: 0.5283, val loss: 1.1542, val acc: 0.5245  (best train acc: 0.5387, best val acc: 0.5309, best train loss: 1.1367  @ epoch 7498 )\n",
      "[Epoch: 7580] train loss: 1.1612, train acc: 0.5213, val loss: 1.1540, val acc: 0.5312  (best train acc: 0.5387, best val acc: 0.5319, best train loss: 1.1367  @ epoch 7498 )\n",
      "[Epoch: 7600] train loss: 1.1468, train acc: 0.5295, val loss: 1.1534, val acc: 0.5218  (best train acc: 0.5387, best val acc: 0.5319, best train loss: 1.1367  @ epoch 7498 )\n",
      "[Epoch: 7620] train loss: 1.1439, train acc: 0.5331, val loss: 1.1525, val acc: 0.5265  (best train acc: 0.5387, best val acc: 0.5319, best train loss: 1.1367  @ epoch 7498 )\n",
      "[Epoch: 7640] train loss: 1.1555, train acc: 0.5276, val loss: 1.1518, val acc: 0.5295  (best train acc: 0.5387, best val acc: 0.5325, best train loss: 1.1367  @ epoch 7498 )\n",
      "[Epoch: 7660] train loss: 1.1389, train acc: 0.5233, val loss: 1.1514, val acc: 0.5322  (best train acc: 0.5387, best val acc: 0.5325, best train loss: 1.1363  @ epoch 7644 )\n",
      "[Epoch: 7680] train loss: 1.1507, train acc: 0.5300, val loss: 1.1520, val acc: 0.5275  (best train acc: 0.5390, best val acc: 0.5325, best train loss: 1.1324  @ epoch 7672 )\n",
      "[Epoch: 7700] train loss: 1.1474, train acc: 0.5303, val loss: 1.1512, val acc: 0.5312  (best train acc: 0.5390, best val acc: 0.5336, best train loss: 1.1317  @ epoch 7696 )\n",
      "[Epoch: 7720] train loss: 1.1552, train acc: 0.5252, val loss: 1.1503, val acc: 0.5275  (best train acc: 0.5390, best val acc: 0.5339, best train loss: 1.1317  @ epoch 7696 )\n",
      "[Epoch: 7740] train loss: 1.1492, train acc: 0.5270, val loss: 1.1498, val acc: 0.5325  (best train acc: 0.5390, best val acc: 0.5346, best train loss: 1.1317  @ epoch 7696 )\n",
      "[Epoch: 7760] train loss: 1.1558, train acc: 0.5179, val loss: 1.1490, val acc: 0.5285  (best train acc: 0.5390, best val acc: 0.5346, best train loss: 1.1317  @ epoch 7696 )\n",
      "[Epoch: 7780] train loss: 1.1564, train acc: 0.5247, val loss: 1.1487, val acc: 0.5248  (best train acc: 0.5390, best val acc: 0.5346, best train loss: 1.1305  @ epoch 7777 )\n",
      "[Epoch: 7800] train loss: 1.1429, train acc: 0.5262, val loss: 1.1484, val acc: 0.5325  (best train acc: 0.5390, best val acc: 0.5346, best train loss: 1.1305  @ epoch 7777 )\n",
      "[Epoch: 7820] train loss: 1.1513, train acc: 0.5275, val loss: 1.1480, val acc: 0.5319  (best train acc: 0.5390, best val acc: 0.5346, best train loss: 1.1305  @ epoch 7777 )\n",
      "[Epoch: 7840] train loss: 1.1493, train acc: 0.5312, val loss: 1.1480, val acc: 0.5298  (best train acc: 0.5390, best val acc: 0.5346, best train loss: 1.1305  @ epoch 7777 )\n",
      "[Epoch: 7860] train loss: 1.1434, train acc: 0.5226, val loss: 1.1488, val acc: 0.5278  (best train acc: 0.5390, best val acc: 0.5346, best train loss: 1.1305  @ epoch 7777 )\n",
      "[Epoch: 7880] train loss: 1.1454, train acc: 0.5265, val loss: 1.1472, val acc: 0.5342  (best train acc: 0.5390, best val acc: 0.5346, best train loss: 1.1305  @ epoch 7777 )\n",
      "[Epoch: 7900] train loss: 1.1445, train acc: 0.5241, val loss: 1.1477, val acc: 0.5241  (best train acc: 0.5390, best val acc: 0.5346, best train loss: 1.1305  @ epoch 7777 )\n",
      "[Epoch: 7920] train loss: 1.1442, train acc: 0.5273, val loss: 1.1473, val acc: 0.5305  (best train acc: 0.5390, best val acc: 0.5346, best train loss: 1.1305  @ epoch 7777 )\n",
      "[Epoch: 7940] train loss: 1.1527, train acc: 0.5242, val loss: 1.1462, val acc: 0.5322  (best train acc: 0.5390, best val acc: 0.5349, best train loss: 1.1305  @ epoch 7777 )\n",
      "[Epoch: 7960] train loss: 1.1404, train acc: 0.5335, val loss: 1.1468, val acc: 0.5275  (best train acc: 0.5390, best val acc: 0.5349, best train loss: 1.1305  @ epoch 7777 )\n",
      "[Epoch: 7980] train loss: 1.1371, train acc: 0.5335, val loss: 1.1482, val acc: 0.5191  (best train acc: 0.5390, best val acc: 0.5349, best train loss: 1.1305  @ epoch 7777 )\n",
      "[Epoch: 8000] train loss: 1.1365, train acc: 0.5359, val loss: 1.1450, val acc: 0.5336  (best train acc: 0.5390, best val acc: 0.5349, best train loss: 1.1305  @ epoch 7777 )\n",
      "[Epoch: 8020] train loss: 1.1458, train acc: 0.5271, val loss: 1.1446, val acc: 0.5305  (best train acc: 0.5390, best val acc: 0.5349, best train loss: 1.1305  @ epoch 7777 )\n",
      "[Epoch: 8040] train loss: 1.1358, train acc: 0.5315, val loss: 1.1451, val acc: 0.5322  (best train acc: 0.5390, best val acc: 0.5349, best train loss: 1.1305  @ epoch 7777 )\n",
      "[Epoch: 8060] train loss: 1.1359, train acc: 0.5378, val loss: 1.1442, val acc: 0.5302  (best train acc: 0.5390, best val acc: 0.5349, best train loss: 1.1302  @ epoch 8046 )\n",
      "[Epoch: 8080] train loss: 1.1466, train acc: 0.5213, val loss: 1.1439, val acc: 0.5305  (best train acc: 0.5390, best val acc: 0.5349, best train loss: 1.1302  @ epoch 8046 )\n",
      "[Epoch: 8100] train loss: 1.1345, train acc: 0.5339, val loss: 1.1429, val acc: 0.5332  (best train acc: 0.5390, best val acc: 0.5349, best train loss: 1.1302  @ epoch 8046 )\n",
      "[Epoch: 8120] train loss: 1.1427, train acc: 0.5293, val loss: 1.1425, val acc: 0.5305  (best train acc: 0.5390, best val acc: 0.5349, best train loss: 1.1302  @ epoch 8046 )\n",
      "[Epoch: 8140] train loss: 1.1457, train acc: 0.5269, val loss: 1.1423, val acc: 0.5325  (best train acc: 0.5390, best val acc: 0.5349, best train loss: 1.1302  @ epoch 8046 )\n",
      "[Epoch: 8160] train loss: 1.1397, train acc: 0.5290, val loss: 1.1443, val acc: 0.5285  (best train acc: 0.5390, best val acc: 0.5349, best train loss: 1.1302  @ epoch 8046 )\n",
      "[Epoch: 8180] train loss: 1.1457, train acc: 0.5283, val loss: 1.1418, val acc: 0.5319  (best train acc: 0.5390, best val acc: 0.5349, best train loss: 1.1302  @ epoch 8046 )\n",
      "[Epoch: 8200] train loss: 1.1425, train acc: 0.5293, val loss: 1.1444, val acc: 0.5278  (best train acc: 0.5390, best val acc: 0.5349, best train loss: 1.1302  @ epoch 8046 )\n",
      "[Epoch: 8220] train loss: 1.1351, train acc: 0.5301, val loss: 1.1415, val acc: 0.5295  (best train acc: 0.5390, best val acc: 0.5349, best train loss: 1.1256  @ epoch 8208 )\n",
      "[Epoch: 8240] train loss: 1.1422, train acc: 0.5285, val loss: 1.1460, val acc: 0.5177  (best train acc: 0.5390, best val acc: 0.5349, best train loss: 1.1256  @ epoch 8208 )\n",
      "[Epoch: 8260] train loss: 1.1317, train acc: 0.5270, val loss: 1.1409, val acc: 0.5261  (best train acc: 0.5390, best val acc: 0.5349, best train loss: 1.1256  @ epoch 8208 )\n",
      "[Epoch: 8280] train loss: 1.1277, train acc: 0.5318, val loss: 1.1400, val acc: 0.5329  (best train acc: 0.5390, best val acc: 0.5349, best train loss: 1.1256  @ epoch 8208 )\n",
      "[Epoch: 8300] train loss: 1.1317, train acc: 0.5331, val loss: 1.1398, val acc: 0.5315  (best train acc: 0.5390, best val acc: 0.5349, best train loss: 1.1256  @ epoch 8208 )\n",
      "[Epoch: 8320] train loss: 1.1445, train acc: 0.5221, val loss: 1.1400, val acc: 0.5319  (best train acc: 0.5390, best val acc: 0.5349, best train loss: 1.1256  @ epoch 8208 )\n",
      "[Epoch: 8340] train loss: 1.1440, train acc: 0.5270, val loss: 1.1400, val acc: 0.5278  (best train acc: 0.5390, best val acc: 0.5349, best train loss: 1.1256  @ epoch 8208 )\n",
      "[Epoch: 8360] train loss: 1.1310, train acc: 0.5343, val loss: 1.1409, val acc: 0.5285  (best train acc: 0.5390, best val acc: 0.5349, best train loss: 1.1256  @ epoch 8208 )\n",
      "[Epoch: 8380] train loss: 1.1334, train acc: 0.5335, val loss: 1.1385, val acc: 0.5329  (best train acc: 0.5390, best val acc: 0.5349, best train loss: 1.1256  @ epoch 8208 )\n",
      "[Epoch: 8400] train loss: 1.1291, train acc: 0.5335, val loss: 1.1400, val acc: 0.5298  (best train acc: 0.5390, best val acc: 0.5349, best train loss: 1.1256  @ epoch 8208 )\n",
      "[Epoch: 8420] train loss: 1.1323, train acc: 0.5327, val loss: 1.1387, val acc: 0.5339  (best train acc: 0.5390, best val acc: 0.5349, best train loss: 1.1256  @ epoch 8208 )\n",
      "[Epoch: 8440] train loss: 1.1334, train acc: 0.5348, val loss: 1.1384, val acc: 0.5298  (best train acc: 0.5390, best val acc: 0.5349, best train loss: 1.1256  @ epoch 8208 )\n",
      "[Epoch: 8460] train loss: 1.1297, train acc: 0.5321, val loss: 1.1379, val acc: 0.5305  (best train acc: 0.5390, best val acc: 0.5349, best train loss: 1.1228  @ epoch 8445 )\n",
      "[Epoch: 8480] train loss: 1.1384, train acc: 0.5304, val loss: 1.1437, val acc: 0.5218  (best train acc: 0.5390, best val acc: 0.5349, best train loss: 1.1228  @ epoch 8445 )\n",
      "[Epoch: 8500] train loss: 1.1453, train acc: 0.5257, val loss: 1.1392, val acc: 0.5298  (best train acc: 0.5390, best val acc: 0.5349, best train loss: 1.1228  @ epoch 8445 )\n",
      "[Epoch: 8520] train loss: 1.1420, train acc: 0.5315, val loss: 1.1368, val acc: 0.5302  (best train acc: 0.5390, best val acc: 0.5349, best train loss: 1.1228  @ epoch 8445 )\n",
      "[Epoch: 8540] train loss: 1.1351, train acc: 0.5309, val loss: 1.1362, val acc: 0.5329  (best train acc: 0.5390, best val acc: 0.5349, best train loss: 1.1228  @ epoch 8445 )\n",
      "[Epoch: 8560] train loss: 1.1400, train acc: 0.5271, val loss: 1.1389, val acc: 0.5275  (best train acc: 0.5390, best val acc: 0.5349, best train loss: 1.1228  @ epoch 8445 )\n",
      "[Epoch: 8580] train loss: 1.1316, train acc: 0.5341, val loss: 1.1389, val acc: 0.5224  (best train acc: 0.5390, best val acc: 0.5349, best train loss: 1.1219  @ epoch 8564 )\n",
      "[Epoch: 8600] train loss: 1.1394, train acc: 0.5285, val loss: 1.1352, val acc: 0.5336  (best train acc: 0.5390, best val acc: 0.5349, best train loss: 1.1219  @ epoch 8564 )\n",
      "[Epoch: 8620] train loss: 1.1449, train acc: 0.5278, val loss: 1.1348, val acc: 0.5349  (best train acc: 0.5396, best val acc: 0.5349, best train loss: 1.1219  @ epoch 8564 )\n",
      "[Epoch: 8640] train loss: 1.1358, train acc: 0.5299, val loss: 1.1352, val acc: 0.5305  (best train acc: 0.5396, best val acc: 0.5349, best train loss: 1.1219  @ epoch 8564 )\n",
      "[Epoch: 8660] train loss: 1.1271, train acc: 0.5328, val loss: 1.1361, val acc: 0.5312  (best train acc: 0.5396, best val acc: 0.5349, best train loss: 1.1204  @ epoch 8646 )\n",
      "[Epoch: 8680] train loss: 1.1267, train acc: 0.5349, val loss: 1.1353, val acc: 0.5268  (best train acc: 0.5396, best val acc: 0.5349, best train loss: 1.1204  @ epoch 8646 )\n",
      "[Epoch: 8700] train loss: 1.1238, train acc: 0.5333, val loss: 1.1344, val acc: 0.5305  (best train acc: 0.5396, best val acc: 0.5349, best train loss: 1.1204  @ epoch 8646 )\n",
      "[Epoch: 8720] train loss: 1.1377, train acc: 0.5301, val loss: 1.1333, val acc: 0.5305  (best train acc: 0.5406, best val acc: 0.5349, best train loss: 1.1204  @ epoch 8646 )\n",
      "[Epoch: 8740] train loss: 1.1473, train acc: 0.5230, val loss: 1.1333, val acc: 0.5336  (best train acc: 0.5406, best val acc: 0.5349, best train loss: 1.1177  @ epoch 8734 )\n",
      "[Epoch: 8760] train loss: 1.1320, train acc: 0.5294, val loss: 1.1364, val acc: 0.5228  (best train acc: 0.5406, best val acc: 0.5349, best train loss: 1.1177  @ epoch 8734 )\n",
      "[Epoch: 8780] train loss: 1.1306, train acc: 0.5323, val loss: 1.1344, val acc: 0.5278  (best train acc: 0.5406, best val acc: 0.5352, best train loss: 1.1177  @ epoch 8734 )\n",
      "[Epoch: 8800] train loss: 1.1397, train acc: 0.5312, val loss: 1.1320, val acc: 0.5312  (best train acc: 0.5406, best val acc: 0.5352, best train loss: 1.1177  @ epoch 8734 )\n",
      "[Epoch: 8820] train loss: 1.1291, train acc: 0.5310, val loss: 1.1330, val acc: 0.5288  (best train acc: 0.5406, best val acc: 0.5352, best train loss: 1.1177  @ epoch 8734 )\n",
      "[Epoch: 8840] train loss: 1.1214, train acc: 0.5376, val loss: 1.1315, val acc: 0.5342  (best train acc: 0.5406, best val acc: 0.5359, best train loss: 1.1177  @ epoch 8734 )\n",
      "[Epoch: 8860] train loss: 1.1273, train acc: 0.5295, val loss: 1.1311, val acc: 0.5325  (best train acc: 0.5406, best val acc: 0.5359, best train loss: 1.1177  @ epoch 8734 )\n",
      "[Epoch: 8880] train loss: 1.1273, train acc: 0.5358, val loss: 1.1322, val acc: 0.5309  (best train acc: 0.5412, best val acc: 0.5359, best train loss: 1.1177  @ epoch 8734 )\n",
      "[Epoch: 8900] train loss: 1.1344, train acc: 0.5314, val loss: 1.1305, val acc: 0.5329  (best train acc: 0.5412, best val acc: 0.5359, best train loss: 1.1177  @ epoch 8734 )\n",
      "[Epoch: 8920] train loss: 1.1323, train acc: 0.5281, val loss: 1.1302, val acc: 0.5305  (best train acc: 0.5412, best val acc: 0.5359, best train loss: 1.1177  @ epoch 8734 )\n",
      "[Epoch: 8940] train loss: 1.1304, train acc: 0.5330, val loss: 1.1312, val acc: 0.5278  (best train acc: 0.5412, best val acc: 0.5359, best train loss: 1.1177  @ epoch 8734 )\n",
      "[Epoch: 8960] train loss: 1.1309, train acc: 0.5274, val loss: 1.1299, val acc: 0.5288  (best train acc: 0.5412, best val acc: 0.5359, best train loss: 1.1161  @ epoch 8945 )\n",
      "[Epoch: 8980] train loss: 1.1244, train acc: 0.5338, val loss: 1.1279, val acc: 0.5329  (best train acc: 0.5412, best val acc: 0.5359, best train loss: 1.1161  @ epoch 8945 )\n",
      "[Epoch: 9000] train loss: 1.1262, train acc: 0.5285, val loss: 1.1297, val acc: 0.5315  (best train acc: 0.5412, best val acc: 0.5359, best train loss: 1.1155  @ epoch 8988 )\n",
      "[Epoch: 9020] train loss: 1.1302, train acc: 0.5325, val loss: 1.1271, val acc: 0.5325  (best train acc: 0.5412, best val acc: 0.5359, best train loss: 1.1102  @ epoch 9018 )\n",
      "[Epoch: 9040] train loss: 1.1297, train acc: 0.5239, val loss: 1.1262, val acc: 0.5342  (best train acc: 0.5412, best val acc: 0.5359, best train loss: 1.1102  @ epoch 9018 )\n",
      "[Epoch: 9060] train loss: 1.1147, train acc: 0.5375, val loss: 1.1254, val acc: 0.5329  (best train acc: 0.5412, best val acc: 0.5373, best train loss: 1.1102  @ epoch 9018 )\n",
      "[Epoch: 9080] train loss: 1.1246, train acc: 0.5352, val loss: 1.1262, val acc: 0.5315  (best train acc: 0.5412, best val acc: 0.5373, best train loss: 1.1102  @ epoch 9018 )\n",
      "[Epoch: 9100] train loss: 1.1299, train acc: 0.5311, val loss: 1.1257, val acc: 0.5352  (best train acc: 0.5412, best val acc: 0.5373, best train loss: 1.1102  @ epoch 9018 )\n",
      "[Epoch: 9120] train loss: 1.1196, train acc: 0.5374, val loss: 1.1267, val acc: 0.5312  (best train acc: 0.5412, best val acc: 0.5390, best train loss: 1.1072  @ epoch 9114 )\n",
      "[Epoch: 9140] train loss: 1.1288, train acc: 0.5287, val loss: 1.1248, val acc: 0.5325  (best train acc: 0.5412, best val acc: 0.5390, best train loss: 1.1072  @ epoch 9114 )\n",
      "[Epoch: 9160] train loss: 1.1242, train acc: 0.5339, val loss: 1.1244, val acc: 0.5349  (best train acc: 0.5412, best val acc: 0.5390, best train loss: 1.1072  @ epoch 9114 )\n",
      "[Epoch: 9180] train loss: 1.1303, train acc: 0.5287, val loss: 1.1237, val acc: 0.5329  (best train acc: 0.5412, best val acc: 0.5390, best train loss: 1.1072  @ epoch 9114 )\n",
      "[Epoch: 9200] train loss: 1.1296, train acc: 0.5251, val loss: 1.1238, val acc: 0.5336  (best train acc: 0.5412, best val acc: 0.5390, best train loss: 1.1072  @ epoch 9114 )\n",
      "[Epoch: 9220] train loss: 1.1281, train acc: 0.5310, val loss: 1.1228, val acc: 0.5336  (best train acc: 0.5412, best val acc: 0.5390, best train loss: 1.1072  @ epoch 9114 )\n",
      "[Epoch: 9240] train loss: 1.1155, train acc: 0.5333, val loss: 1.1237, val acc: 0.5309  (best train acc: 0.5412, best val acc: 0.5390, best train loss: 1.1072  @ epoch 9114 )\n",
      "[Epoch: 9260] train loss: 1.1188, train acc: 0.5381, val loss: 1.1219, val acc: 0.5342  (best train acc: 0.5412, best val acc: 0.5390, best train loss: 1.1072  @ epoch 9114 )\n",
      "[Epoch: 9280] train loss: 1.1182, train acc: 0.5294, val loss: 1.1253, val acc: 0.5255  (best train acc: 0.5412, best val acc: 0.5390, best train loss: 1.1072  @ epoch 9114 )\n",
      "[Epoch: 9300] train loss: 1.1220, train acc: 0.5286, val loss: 1.1219, val acc: 0.5315  (best train acc: 0.5412, best val acc: 0.5390, best train loss: 1.1072  @ epoch 9114 )\n",
      "[Epoch: 9320] train loss: 1.1295, train acc: 0.5309, val loss: 1.1218, val acc: 0.5359  (best train acc: 0.5412, best val acc: 0.5390, best train loss: 1.1072  @ epoch 9114 )\n",
      "[Epoch: 9340] train loss: 1.1130, train acc: 0.5366, val loss: 1.1207, val acc: 0.5322  (best train acc: 0.5412, best val acc: 0.5390, best train loss: 1.1072  @ epoch 9114 )\n",
      "[Epoch: 9360] train loss: 1.1184, train acc: 0.5348, val loss: 1.1237, val acc: 0.5305  (best train acc: 0.5412, best val acc: 0.5390, best train loss: 1.1067  @ epoch 9348 )\n",
      "[Epoch: 9380] train loss: 1.1192, train acc: 0.5313, val loss: 1.1213, val acc: 0.5319  (best train acc: 0.5412, best val acc: 0.5390, best train loss: 1.1067  @ epoch 9348 )\n",
      "[Epoch: 9400] train loss: 1.1151, train acc: 0.5379, val loss: 1.1214, val acc: 0.5322  (best train acc: 0.5412, best val acc: 0.5390, best train loss: 1.1067  @ epoch 9348 )\n",
      "[Epoch: 9420] train loss: 1.1178, train acc: 0.5390, val loss: 1.1194, val acc: 0.5342  (best train acc: 0.5412, best val acc: 0.5390, best train loss: 1.1067  @ epoch 9348 )\n",
      "[Epoch: 9440] train loss: 1.1178, train acc: 0.5348, val loss: 1.1197, val acc: 0.5349  (best train acc: 0.5412, best val acc: 0.5390, best train loss: 1.1062  @ epoch 9433 )\n",
      "[Epoch: 9460] train loss: 1.1166, train acc: 0.5307, val loss: 1.1188, val acc: 0.5356  (best train acc: 0.5412, best val acc: 0.5390, best train loss: 1.1062  @ epoch 9433 )\n",
      "[Epoch: 9480] train loss: 1.1243, train acc: 0.5322, val loss: 1.1186, val acc: 0.5325  (best train acc: 0.5412, best val acc: 0.5390, best train loss: 1.1062  @ epoch 9433 )\n",
      "[Epoch: 9500] train loss: 1.1134, train acc: 0.5359, val loss: 1.1214, val acc: 0.5336  (best train acc: 0.5412, best val acc: 0.5390, best train loss: 1.1062  @ epoch 9433 )\n",
      "[Epoch: 9520] train loss: 1.1261, train acc: 0.5279, val loss: 1.1185, val acc: 0.5332  (best train acc: 0.5412, best val acc: 0.5390, best train loss: 1.1062  @ epoch 9433 )\n",
      "[Epoch: 9540] train loss: 1.1252, train acc: 0.5273, val loss: 1.1192, val acc: 0.5319  (best train acc: 0.5412, best val acc: 0.5390, best train loss: 1.1047  @ epoch 9528 )\n",
      "[Epoch: 9560] train loss: 1.1113, train acc: 0.5347, val loss: 1.1207, val acc: 0.5295  (best train acc: 0.5412, best val acc: 0.5390, best train loss: 1.1047  @ epoch 9528 )\n",
      "[Epoch: 9580] train loss: 1.1144, train acc: 0.5344, val loss: 1.1172, val acc: 0.5352  (best train acc: 0.5412, best val acc: 0.5390, best train loss: 1.1047  @ epoch 9528 )\n",
      "[Epoch: 9600] train loss: 1.1279, train acc: 0.5309, val loss: 1.1192, val acc: 0.5319  (best train acc: 0.5412, best val acc: 0.5390, best train loss: 1.1047  @ epoch 9528 )\n",
      "[Epoch: 9620] train loss: 1.1216, train acc: 0.5275, val loss: 1.1175, val acc: 0.5342  (best train acc: 0.5412, best val acc: 0.5390, best train loss: 1.1047  @ epoch 9528 )\n",
      "[Epoch: 9640] train loss: 1.1172, train acc: 0.5294, val loss: 1.1165, val acc: 0.5339  (best train acc: 0.5416, best val acc: 0.5393, best train loss: 1.1015  @ epoch 9629 )\n",
      "[Epoch: 9660] train loss: 1.1166, train acc: 0.5369, val loss: 1.1157, val acc: 0.5386  (best train acc: 0.5416, best val acc: 0.5393, best train loss: 1.1010  @ epoch 9650 )\n",
      "[Epoch: 9680] train loss: 1.1168, train acc: 0.5281, val loss: 1.1166, val acc: 0.5332  (best train acc: 0.5416, best val acc: 0.5393, best train loss: 1.1010  @ epoch 9650 )\n",
      "[Epoch: 9700] train loss: 1.1265, train acc: 0.5275, val loss: 1.1155, val acc: 0.5383  (best train acc: 0.5416, best val acc: 0.5393, best train loss: 1.1010  @ epoch 9650 )\n",
      "[Epoch: 9720] train loss: 1.1087, train acc: 0.5367, val loss: 1.1170, val acc: 0.5349  (best train acc: 0.5416, best val acc: 0.5393, best train loss: 1.1010  @ epoch 9650 )\n",
      "[Epoch: 9740] train loss: 1.1234, train acc: 0.5364, val loss: 1.1176, val acc: 0.5282  (best train acc: 0.5416, best val acc: 0.5393, best train loss: 1.1010  @ epoch 9650 )\n",
      "[Epoch: 9760] train loss: 1.1206, train acc: 0.5269, val loss: 1.1145, val acc: 0.5369  (best train acc: 0.5416, best val acc: 0.5396, best train loss: 1.1010  @ epoch 9650 )\n",
      "[Epoch: 9780] train loss: 1.1213, train acc: 0.5265, val loss: 1.1145, val acc: 0.5359  (best train acc: 0.5416, best val acc: 0.5400, best train loss: 1.1010  @ epoch 9650 )\n",
      "[Epoch: 9800] train loss: 1.1217, train acc: 0.5253, val loss: 1.1150, val acc: 0.5356  (best train acc: 0.5416, best val acc: 0.5406, best train loss: 1.1010  @ epoch 9650 )\n",
      "[Epoch: 9820] train loss: 1.1230, train acc: 0.5333, val loss: 1.1151, val acc: 0.5312  (best train acc: 0.5416, best val acc: 0.5406, best train loss: 1.1010  @ epoch 9650 )\n",
      "[Epoch: 9840] train loss: 1.1371, train acc: 0.5188, val loss: 1.1176, val acc: 0.5349  (best train acc: 0.5416, best val acc: 0.5406, best train loss: 1.1010  @ epoch 9650 )\n",
      "[Epoch: 9860] train loss: 1.1171, train acc: 0.5319, val loss: 1.1142, val acc: 0.5369  (best train acc: 0.5416, best val acc: 0.5406, best train loss: 1.1010  @ epoch 9650 )\n",
      "[Epoch: 9880] train loss: 1.1145, train acc: 0.5286, val loss: 1.1176, val acc: 0.5275  (best train acc: 0.5416, best val acc: 0.5406, best train loss: 1.1010  @ epoch 9650 )\n",
      "[Epoch: 9900] train loss: 1.1158, train acc: 0.5331, val loss: 1.1133, val acc: 0.5359  (best train acc: 0.5416, best val acc: 0.5406, best train loss: 1.1010  @ epoch 9650 )\n",
      "[Epoch: 9920] train loss: 1.1165, train acc: 0.5299, val loss: 1.1139, val acc: 0.5292  (best train acc: 0.5457, best val acc: 0.5406, best train loss: 1.1010  @ epoch 9650 )\n",
      "[Epoch: 9940] train loss: 1.1107, train acc: 0.5338, val loss: 1.1136, val acc: 0.5359  (best train acc: 0.5457, best val acc: 0.5406, best train loss: 1.1010  @ epoch 9650 )\n",
      "[Epoch: 9960] train loss: 1.1236, train acc: 0.5306, val loss: 1.1135, val acc: 0.5349  (best train acc: 0.5457, best val acc: 0.5406, best train loss: 1.1009  @ epoch 9944 )\n",
      "[Epoch: 9980] train loss: 1.1133, train acc: 0.5398, val loss: 1.1144, val acc: 0.5329  (best train acc: 0.5457, best val acc: 0.5406, best train loss: 1.1009  @ epoch 9944 )\n",
      "[Epoch: 10000] train loss: 1.1181, train acc: 0.5265, val loss: 1.1134, val acc: 0.5312  (best train acc: 0.5457, best val acc: 0.5406, best train loss: 1.0995  @ epoch 9992 )\n",
      "[Epoch: 10020] train loss: 1.1119, train acc: 0.5359, val loss: 1.1133, val acc: 0.5329  (best train acc: 0.5457, best val acc: 0.5406, best train loss: 1.0995  @ epoch 9992 )\n",
      "[Epoch: 10040] train loss: 1.1075, train acc: 0.5375, val loss: 1.1123, val acc: 0.5369  (best train acc: 0.5464, best val acc: 0.5413, best train loss: 1.0963  @ epoch 10024 )\n",
      "[Epoch: 10060] train loss: 1.1216, train acc: 0.5234, val loss: 1.1131, val acc: 0.5319  (best train acc: 0.5464, best val acc: 0.5413, best train loss: 1.0960  @ epoch 10058 )\n",
      "[Epoch: 10080] train loss: 1.1200, train acc: 0.5268, val loss: 1.1121, val acc: 0.5322  (best train acc: 0.5464, best val acc: 0.5413, best train loss: 1.0960  @ epoch 10058 )\n",
      "[Epoch: 10100] train loss: 1.1103, train acc: 0.5359, val loss: 1.1129, val acc: 0.5356  (best train acc: 0.5464, best val acc: 0.5413, best train loss: 1.0960  @ epoch 10058 )\n",
      "[Epoch: 10120] train loss: 1.1196, train acc: 0.5315, val loss: 1.1130, val acc: 0.5309  (best train acc: 0.5464, best val acc: 0.5413, best train loss: 1.0960  @ epoch 10058 )\n",
      "[Epoch: 10140] train loss: 1.0981, train acc: 0.5388, val loss: 1.1118, val acc: 0.5325  (best train acc: 0.5464, best val acc: 0.5413, best train loss: 1.0960  @ epoch 10058 )\n",
      "[Epoch: 10160] train loss: 1.1121, train acc: 0.5340, val loss: 1.1120, val acc: 0.5346  (best train acc: 0.5464, best val acc: 0.5413, best train loss: 1.0960  @ epoch 10058 )\n",
      "[Epoch: 10180] train loss: 1.1142, train acc: 0.5314, val loss: 1.1107, val acc: 0.5359  (best train acc: 0.5464, best val acc: 0.5430, best train loss: 1.0960  @ epoch 10058 )\n",
      "[Epoch: 10200] train loss: 1.1191, train acc: 0.5312, val loss: 1.1112, val acc: 0.5342  (best train acc: 0.5464, best val acc: 0.5430, best train loss: 1.0960  @ epoch 10058 )\n",
      "[Epoch: 10220] train loss: 1.1175, train acc: 0.5296, val loss: 1.1144, val acc: 0.5339  (best train acc: 0.5464, best val acc: 0.5430, best train loss: 1.0960  @ epoch 10058 )\n",
      "[Epoch: 10240] train loss: 1.1066, train acc: 0.5350, val loss: 1.1119, val acc: 0.5342  (best train acc: 0.5464, best val acc: 0.5430, best train loss: 1.0960  @ epoch 10058 )\n",
      "[Epoch: 10260] train loss: 1.1101, train acc: 0.5370, val loss: 1.1099, val acc: 0.5406  (best train acc: 0.5464, best val acc: 0.5430, best train loss: 1.0958  @ epoch 10250 )\n",
      "[Epoch: 10280] train loss: 1.1195, train acc: 0.5341, val loss: 1.1122, val acc: 0.5329  (best train acc: 0.5464, best val acc: 0.5430, best train loss: 1.0955  @ epoch 10276 )\n",
      "[Epoch: 10300] train loss: 1.1140, train acc: 0.5357, val loss: 1.1091, val acc: 0.5369  (best train acc: 0.5464, best val acc: 0.5430, best train loss: 1.0946  @ epoch 10292 )\n",
      "[Epoch: 10320] train loss: 1.1223, train acc: 0.5241, val loss: 1.1105, val acc: 0.5356  (best train acc: 0.5464, best val acc: 0.5430, best train loss: 1.0929  @ epoch 10315 )\n",
      "[Epoch: 10340] train loss: 1.1088, train acc: 0.5310, val loss: 1.1113, val acc: 0.5366  (best train acc: 0.5464, best val acc: 0.5430, best train loss: 1.0929  @ epoch 10315 )\n",
      "[Epoch: 10360] train loss: 1.1071, train acc: 0.5371, val loss: 1.1089, val acc: 0.5423  (best train acc: 0.5464, best val acc: 0.5444, best train loss: 1.0929  @ epoch 10315 )\n",
      "[Epoch: 10380] train loss: 1.1050, train acc: 0.5377, val loss: 1.1095, val acc: 0.5376  (best train acc: 0.5464, best val acc: 0.5444, best train loss: 1.0929  @ epoch 10315 )\n",
      "[Epoch: 10400] train loss: 1.1051, train acc: 0.5344, val loss: 1.1088, val acc: 0.5363  (best train acc: 0.5464, best val acc: 0.5444, best train loss: 1.0929  @ epoch 10315 )\n",
      "[Epoch: 10420] train loss: 1.1151, train acc: 0.5317, val loss: 1.1088, val acc: 0.5356  (best train acc: 0.5464, best val acc: 0.5444, best train loss: 1.0929  @ epoch 10315 )\n",
      "[Epoch: 10440] train loss: 1.1107, train acc: 0.5317, val loss: 1.1079, val acc: 0.5406  (best train acc: 0.5464, best val acc: 0.5444, best train loss: 1.0929  @ epoch 10315 )\n",
      "[Epoch: 10460] train loss: 1.1319, train acc: 0.5119, val loss: 1.1278, val acc: 0.5143  (best train acc: 0.5464, best val acc: 0.5444, best train loss: 1.0929  @ epoch 10315 )\n",
      "[Epoch: 10480] train loss: 1.1894, train acc: 0.4747, val loss: 1.1758, val acc: 0.4880  (best train acc: 0.5464, best val acc: 0.5444, best train loss: 1.0929  @ epoch 10315 )\n",
      "[Epoch: 10500] train loss: 1.1578, train acc: 0.5252, val loss: 1.1450, val acc: 0.5309  (best train acc: 0.5464, best val acc: 0.5444, best train loss: 1.0929  @ epoch 10315 )\n",
      "[Epoch: 10520] train loss: 1.1395, train acc: 0.5331, val loss: 1.1328, val acc: 0.5332  (best train acc: 0.5464, best val acc: 0.5444, best train loss: 1.0929  @ epoch 10315 )\n",
      "[Epoch: 10540] train loss: 1.1351, train acc: 0.5295, val loss: 1.1285, val acc: 0.5346  (best train acc: 0.5464, best val acc: 0.5444, best train loss: 1.0929  @ epoch 10315 )\n",
      "[Epoch: 10560] train loss: 1.1334, train acc: 0.5390, val loss: 1.1259, val acc: 0.5363  (best train acc: 0.5464, best val acc: 0.5444, best train loss: 1.0929  @ epoch 10315 )\n",
      "[Epoch: 10580] train loss: 1.1222, train acc: 0.5348, val loss: 1.1258, val acc: 0.5339  (best train acc: 0.5464, best val acc: 0.5444, best train loss: 1.0929  @ epoch 10315 )\n",
      "[Epoch: 10600] train loss: 1.1146, train acc: 0.5404, val loss: 1.1242, val acc: 0.5336  (best train acc: 0.5464, best val acc: 0.5444, best train loss: 1.0929  @ epoch 10315 )\n",
      "[Epoch: 10620] train loss: 1.1234, train acc: 0.5329, val loss: 1.1229, val acc: 0.5325  (best train acc: 0.5464, best val acc: 0.5444, best train loss: 1.0929  @ epoch 10315 )\n",
      "[Epoch: 10640] train loss: 1.1123, train acc: 0.5385, val loss: 1.1204, val acc: 0.5342  (best train acc: 0.5464, best val acc: 0.5444, best train loss: 1.0929  @ epoch 10315 )\n",
      "[Epoch: 10660] train loss: 1.1199, train acc: 0.5385, val loss: 1.1199, val acc: 0.5349  (best train acc: 0.5464, best val acc: 0.5444, best train loss: 1.0929  @ epoch 10315 )\n",
      "[Epoch: 10680] train loss: 1.1236, train acc: 0.5336, val loss: 1.1194, val acc: 0.5376  (best train acc: 0.5464, best val acc: 0.5444, best train loss: 1.0929  @ epoch 10315 )\n",
      "[Epoch: 10700] train loss: 1.1176, train acc: 0.5408, val loss: 1.1189, val acc: 0.5349  (best train acc: 0.5464, best val acc: 0.5444, best train loss: 1.0929  @ epoch 10315 )\n",
      "[Epoch: 10720] train loss: 1.1107, train acc: 0.5385, val loss: 1.1193, val acc: 0.5369  (best train acc: 0.5464, best val acc: 0.5444, best train loss: 1.0929  @ epoch 10315 )\n",
      "[Epoch: 10740] train loss: 1.1248, train acc: 0.5314, val loss: 1.1174, val acc: 0.5376  (best train acc: 0.5464, best val acc: 0.5444, best train loss: 1.0929  @ epoch 10315 )\n",
      "[Epoch: 10760] train loss: 1.1218, train acc: 0.5307, val loss: 1.1176, val acc: 0.5369  (best train acc: 0.5464, best val acc: 0.5444, best train loss: 1.0929  @ epoch 10315 )\n",
      "[Epoch: 10780] train loss: 1.1177, train acc: 0.5365, val loss: 1.1176, val acc: 0.5369  (best train acc: 0.5464, best val acc: 0.5444, best train loss: 1.0929  @ epoch 10315 )\n",
      "[Epoch: 10800] train loss: 1.1218, train acc: 0.5329, val loss: 1.1166, val acc: 0.5366  (best train acc: 0.5464, best val acc: 0.5444, best train loss: 1.0929  @ epoch 10315 )\n",
      "[Epoch: 10820] train loss: 1.1136, train acc: 0.5384, val loss: 1.1156, val acc: 0.5359  (best train acc: 0.5464, best val acc: 0.5444, best train loss: 1.0929  @ epoch 10315 )\n",
      "[Epoch: 10840] train loss: 1.1205, train acc: 0.5333, val loss: 1.1160, val acc: 0.5366  (best train acc: 0.5464, best val acc: 0.5444, best train loss: 1.0929  @ epoch 10315 )\n",
      "[Epoch: 10860] train loss: 1.1145, train acc: 0.5355, val loss: 1.1153, val acc: 0.5386  (best train acc: 0.5464, best val acc: 0.5444, best train loss: 1.0929  @ epoch 10315 )\n",
      "[Epoch: 10880] train loss: 1.1072, train acc: 0.5355, val loss: 1.1147, val acc: 0.5366  (best train acc: 0.5464, best val acc: 0.5444, best train loss: 1.0929  @ epoch 10315 )\n",
      "[Epoch: 10900] train loss: 1.1052, train acc: 0.5403, val loss: 1.1145, val acc: 0.5383  (best train acc: 0.5464, best val acc: 0.5444, best train loss: 1.0929  @ epoch 10315 )\n",
      "[Epoch: 10920] train loss: 1.1232, train acc: 0.5356, val loss: 1.1156, val acc: 0.5329  (best train acc: 0.5464, best val acc: 0.5444, best train loss: 1.0929  @ epoch 10315 )\n",
      "[Epoch: 10940] train loss: 1.1168, train acc: 0.5364, val loss: 1.1131, val acc: 0.5373  (best train acc: 0.5464, best val acc: 0.5444, best train loss: 1.0929  @ epoch 10315 )\n",
      "[Epoch: 10960] train loss: 1.1218, train acc: 0.5307, val loss: 1.1127, val acc: 0.5383  (best train acc: 0.5464, best val acc: 0.5444, best train loss: 1.0929  @ epoch 10315 )\n",
      "[Epoch: 10980] train loss: 1.0987, train acc: 0.5406, val loss: 1.1121, val acc: 0.5386  (best train acc: 0.5464, best val acc: 0.5444, best train loss: 1.0929  @ epoch 10315 )\n",
      "[Epoch: 11000] train loss: 1.1199, train acc: 0.5320, val loss: 1.1114, val acc: 0.5376  (best train acc: 0.5464, best val acc: 0.5444, best train loss: 1.0929  @ epoch 10315 )\n",
      "[Epoch: 11020] train loss: 1.0968, train acc: 0.5410, val loss: 1.1109, val acc: 0.5369  (best train acc: 0.5464, best val acc: 0.5444, best train loss: 1.0929  @ epoch 10315 )\n",
      "[Epoch: 11040] train loss: 1.1196, train acc: 0.5301, val loss: 1.1101, val acc: 0.5386  (best train acc: 0.5486, best val acc: 0.5444, best train loss: 1.0883  @ epoch 11028 )\n",
      "[Epoch: 11060] train loss: 1.1032, train acc: 0.5398, val loss: 1.1097, val acc: 0.5359  (best train acc: 0.5486, best val acc: 0.5444, best train loss: 1.0883  @ epoch 11028 )\n",
      "[Epoch: 11080] train loss: 1.1123, train acc: 0.5346, val loss: 1.1085, val acc: 0.5400  (best train acc: 0.5486, best val acc: 0.5444, best train loss: 1.0883  @ epoch 11028 )\n",
      "[Epoch: 11100] train loss: 1.1154, train acc: 0.5349, val loss: 1.1077, val acc: 0.5390  (best train acc: 0.5486, best val acc: 0.5444, best train loss: 1.0883  @ epoch 11028 )\n",
      "[Epoch: 11120] train loss: 1.1136, train acc: 0.5359, val loss: 1.1074, val acc: 0.5390  (best train acc: 0.5486, best val acc: 0.5444, best train loss: 1.0883  @ epoch 11028 )\n",
      "[Epoch: 11140] train loss: 1.1122, train acc: 0.5353, val loss: 1.1092, val acc: 0.5400  (best train acc: 0.5486, best val acc: 0.5444, best train loss: 1.0883  @ epoch 11028 )\n",
      "[Epoch: 11160] train loss: 1.1093, train acc: 0.5375, val loss: 1.1060, val acc: 0.5386  (best train acc: 0.5486, best val acc: 0.5444, best train loss: 1.0883  @ epoch 11028 )\n",
      "[Epoch: 11180] train loss: 1.0983, train acc: 0.5425, val loss: 1.1051, val acc: 0.5413  (best train acc: 0.5486, best val acc: 0.5444, best train loss: 1.0883  @ epoch 11028 )\n",
      "[Epoch: 11200] train loss: 1.0968, train acc: 0.5425, val loss: 1.1051, val acc: 0.5376  (best train acc: 0.5486, best val acc: 0.5444, best train loss: 1.0883  @ epoch 11028 )\n",
      "[Epoch: 11220] train loss: 1.1053, train acc: 0.5401, val loss: 1.1036, val acc: 0.5393  (best train acc: 0.5486, best val acc: 0.5444, best train loss: 1.0883  @ epoch 11028 )\n",
      "[Epoch: 11240] train loss: 1.1018, train acc: 0.5405, val loss: 1.1030, val acc: 0.5393  (best train acc: 0.5486, best val acc: 0.5444, best train loss: 1.0883  @ epoch 11028 )\n",
      "[Epoch: 11260] train loss: 1.0985, train acc: 0.5390, val loss: 1.1026, val acc: 0.5433  (best train acc: 0.5486, best val acc: 0.5444, best train loss: 1.0883  @ epoch 11028 )\n",
      "[Epoch: 11280] train loss: 1.0983, train acc: 0.5428, val loss: 1.1035, val acc: 0.5403  (best train acc: 0.5486, best val acc: 0.5444, best train loss: 1.0883  @ epoch 11028 )\n",
      "[Epoch: 11300] train loss: 1.0965, train acc: 0.5414, val loss: 1.1018, val acc: 0.5410  (best train acc: 0.5486, best val acc: 0.5444, best train loss: 1.0883  @ epoch 11028 )\n",
      "[Epoch: 11320] train loss: 1.0920, train acc: 0.5412, val loss: 1.1034, val acc: 0.5386  (best train acc: 0.5489, best val acc: 0.5444, best train loss: 1.0862  @ epoch 11314 )\n",
      "[Epoch: 11340] train loss: 1.0961, train acc: 0.5421, val loss: 1.1018, val acc: 0.5406  (best train acc: 0.5489, best val acc: 0.5444, best train loss: 1.0862  @ epoch 11314 )\n",
      "[Epoch: 11360] train loss: 1.1020, train acc: 0.5361, val loss: 1.1021, val acc: 0.5393  (best train acc: 0.5489, best val acc: 0.5444, best train loss: 1.0862  @ epoch 11314 )\n",
      "[Epoch: 11380] train loss: 1.1008, train acc: 0.5361, val loss: 1.1020, val acc: 0.5329  (best train acc: 0.5489, best val acc: 0.5444, best train loss: 1.0862  @ epoch 11314 )\n",
      "[Epoch: 11400] train loss: 1.1027, train acc: 0.5383, val loss: 1.1003, val acc: 0.5403  (best train acc: 0.5489, best val acc: 0.5444, best train loss: 1.0862  @ epoch 11314 )\n",
      "[Epoch: 11420] train loss: 1.1004, train acc: 0.5395, val loss: 1.1000, val acc: 0.5406  (best train acc: 0.5489, best val acc: 0.5444, best train loss: 1.0862  @ epoch 11314 )\n",
      "[Epoch: 11440] train loss: 1.1026, train acc: 0.5397, val loss: 1.0993, val acc: 0.5420  (best train acc: 0.5489, best val acc: 0.5444, best train loss: 1.0862  @ epoch 11314 )\n",
      "[Epoch: 11460] train loss: 1.1079, train acc: 0.5357, val loss: 1.0990, val acc: 0.5437  (best train acc: 0.5489, best val acc: 0.5444, best train loss: 1.0862  @ epoch 11314 )\n",
      "[Epoch: 11480] train loss: 1.1054, train acc: 0.5377, val loss: 1.1000, val acc: 0.5386  (best train acc: 0.5489, best val acc: 0.5444, best train loss: 1.0862  @ epoch 11314 )\n",
      "[Epoch: 11500] train loss: 1.0977, train acc: 0.5406, val loss: 1.0993, val acc: 0.5403  (best train acc: 0.5489, best val acc: 0.5444, best train loss: 1.0859  @ epoch 11490 )\n",
      "[Epoch: 11520] train loss: 1.0895, train acc: 0.5450, val loss: 1.0990, val acc: 0.5396  (best train acc: 0.5489, best val acc: 0.5444, best train loss: 1.0859  @ epoch 11490 )\n",
      "[Epoch: 11540] train loss: 1.0989, train acc: 0.5419, val loss: 1.0999, val acc: 0.5359  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0783  @ epoch 11529 )\n",
      "[Epoch: 11560] train loss: 1.0971, train acc: 0.5399, val loss: 1.0982, val acc: 0.5413  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0783  @ epoch 11529 )\n",
      "[Epoch: 11580] train loss: 1.1085, train acc: 0.5334, val loss: 1.0976, val acc: 0.5386  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0783  @ epoch 11529 )\n",
      "[Epoch: 11600] train loss: 1.1011, train acc: 0.5358, val loss: 1.0975, val acc: 0.5383  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0783  @ epoch 11529 )\n",
      "[Epoch: 11620] train loss: 1.0912, train acc: 0.5437, val loss: 1.0977, val acc: 0.5393  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0783  @ epoch 11529 )\n",
      "[Epoch: 11640] train loss: 1.0884, train acc: 0.5455, val loss: 1.0988, val acc: 0.5400  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0783  @ epoch 11529 )\n",
      "[Epoch: 11660] train loss: 1.1003, train acc: 0.5401, val loss: 1.0969, val acc: 0.5366  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0783  @ epoch 11529 )\n",
      "[Epoch: 11680] train loss: 1.0970, train acc: 0.5378, val loss: 1.0969, val acc: 0.5383  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0783  @ epoch 11529 )\n",
      "[Epoch: 11700] train loss: 1.0974, train acc: 0.5373, val loss: 1.0958, val acc: 0.5379  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0783  @ epoch 11529 )\n",
      "[Epoch: 11720] train loss: 1.0946, train acc: 0.5419, val loss: 1.0951, val acc: 0.5423  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0783  @ epoch 11529 )\n",
      "[Epoch: 11740] train loss: 1.0969, train acc: 0.5342, val loss: 1.0952, val acc: 0.5393  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0783  @ epoch 11529 )\n",
      "[Epoch: 11760] train loss: 1.0986, train acc: 0.5419, val loss: 1.0945, val acc: 0.5413  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0783  @ epoch 11529 )\n",
      "[Epoch: 11780] train loss: 1.0804, train acc: 0.5452, val loss: 1.0971, val acc: 0.5336  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0783  @ epoch 11529 )\n",
      "[Epoch: 11800] train loss: 1.1075, train acc: 0.5381, val loss: 1.0940, val acc: 0.5396  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0783  @ epoch 11529 )\n",
      "[Epoch: 11820] train loss: 1.0877, train acc: 0.5452, val loss: 1.0942, val acc: 0.5386  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 11840] train loss: 1.0907, train acc: 0.5356, val loss: 1.0932, val acc: 0.5396  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 11860] train loss: 1.0777, train acc: 0.5444, val loss: 1.0930, val acc: 0.5413  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 11880] train loss: 1.0869, train acc: 0.5468, val loss: 1.0928, val acc: 0.5383  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 11900] train loss: 1.0875, train acc: 0.5414, val loss: 1.0950, val acc: 0.5433  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 11920] train loss: 1.1019, train acc: 0.5390, val loss: 1.0911, val acc: 0.5410  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 11940] train loss: 1.0923, train acc: 0.5406, val loss: 1.0924, val acc: 0.5383  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 11960] train loss: 1.0928, train acc: 0.5427, val loss: 1.0942, val acc: 0.5433  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 11980] train loss: 1.8046, train acc: 0.3654, val loss: 1.5873, val acc: 0.4000  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 12000] train loss: 1.2954, train acc: 0.4187, val loss: 1.2855, val acc: 0.4226  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 12020] train loss: 1.2155, train acc: 0.4910, val loss: 1.2151, val acc: 0.4890  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 12040] train loss: 1.2031, train acc: 0.4983, val loss: 1.2071, val acc: 0.5019  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 12060] train loss: 1.2061, train acc: 0.4963, val loss: 1.2044, val acc: 0.4961  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 12080] train loss: 1.1913, train acc: 0.5034, val loss: 1.2025, val acc: 0.4924  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 12100] train loss: 1.1956, train acc: 0.4975, val loss: 1.1996, val acc: 0.4941  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 12120] train loss: 1.1940, train acc: 0.5020, val loss: 1.1977, val acc: 0.4988  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 12140] train loss: 1.1881, train acc: 0.4999, val loss: 1.1959, val acc: 0.4988  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 12160] train loss: 1.1856, train acc: 0.5001, val loss: 1.1941, val acc: 0.4917  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 12180] train loss: 1.1870, train acc: 0.4991, val loss: 1.1932, val acc: 0.4948  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 12200] train loss: 1.1894, train acc: 0.4978, val loss: 1.1915, val acc: 0.4924  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 12220] train loss: 1.1864, train acc: 0.4997, val loss: 1.1908, val acc: 0.4958  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 12240] train loss: 1.1840, train acc: 0.4981, val loss: 1.1895, val acc: 0.4924  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 12260] train loss: 1.1845, train acc: 0.5020, val loss: 1.1889, val acc: 0.4931  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 12280] train loss: 1.1863, train acc: 0.4978, val loss: 1.1876, val acc: 0.4921  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 12300] train loss: 1.1826, train acc: 0.4996, val loss: 1.1871, val acc: 0.4951  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 12320] train loss: 1.1752, train acc: 0.5034, val loss: 1.1863, val acc: 0.4954  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 12340] train loss: 1.1810, train acc: 0.4991, val loss: 1.1852, val acc: 0.4934  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 12360] train loss: 1.1811, train acc: 0.5018, val loss: 1.1840, val acc: 0.4931  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 12380] train loss: 1.1747, train acc: 0.5001, val loss: 1.1834, val acc: 0.4948  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 12400] train loss: 1.1741, train acc: 0.5033, val loss: 1.1825, val acc: 0.4931  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 12420] train loss: 1.1663, train acc: 0.5072, val loss: 1.1814, val acc: 0.4944  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 12440] train loss: 1.1749, train acc: 0.5076, val loss: 1.1804, val acc: 0.4944  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 12460] train loss: 1.1725, train acc: 0.5075, val loss: 1.1795, val acc: 0.4965  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 12480] train loss: 1.1786, train acc: 0.5014, val loss: 1.1783, val acc: 0.4975  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 12500] train loss: 1.1662, train acc: 0.5098, val loss: 1.1773, val acc: 0.5002  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 12520] train loss: 1.1613, train acc: 0.5124, val loss: 1.1762, val acc: 0.5042  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 12540] train loss: 1.1668, train acc: 0.5085, val loss: 1.1747, val acc: 0.5005  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 12560] train loss: 1.1681, train acc: 0.5095, val loss: 1.1735, val acc: 0.5056  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 12580] train loss: 1.1673, train acc: 0.5114, val loss: 1.1721, val acc: 0.5035  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 12600] train loss: 1.1614, train acc: 0.5131, val loss: 1.1710, val acc: 0.5096  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 12620] train loss: 1.1622, train acc: 0.5133, val loss: 1.1695, val acc: 0.5049  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 12640] train loss: 1.1682, train acc: 0.5114, val loss: 1.1679, val acc: 0.5086  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 12660] train loss: 1.1624, train acc: 0.5152, val loss: 1.1672, val acc: 0.5120  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 12680] train loss: 1.1678, train acc: 0.5116, val loss: 1.1651, val acc: 0.5126  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 12700] train loss: 1.1553, train acc: 0.5209, val loss: 1.1640, val acc: 0.5093  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 12720] train loss: 1.1547, train acc: 0.5228, val loss: 1.1625, val acc: 0.5194  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 12740] train loss: 1.1596, train acc: 0.5208, val loss: 1.1613, val acc: 0.5116  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 12760] train loss: 1.1625, train acc: 0.5194, val loss: 1.1594, val acc: 0.5143  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 12780] train loss: 1.1597, train acc: 0.5197, val loss: 1.1582, val acc: 0.5164  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 12800] train loss: 1.1546, train acc: 0.5199, val loss: 1.1570, val acc: 0.5167  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 12820] train loss: 1.1617, train acc: 0.5181, val loss: 1.1561, val acc: 0.5170  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 12840] train loss: 1.1522, train acc: 0.5245, val loss: 1.1551, val acc: 0.5174  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 12860] train loss: 1.1530, train acc: 0.5216, val loss: 1.1539, val acc: 0.5197  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 12880] train loss: 1.1491, train acc: 0.5247, val loss: 1.1533, val acc: 0.5234  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 12900] train loss: 1.1600, train acc: 0.5175, val loss: 1.1580, val acc: 0.5086  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 12920] train loss: 1.1496, train acc: 0.5249, val loss: 1.1513, val acc: 0.5238  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 12940] train loss: 1.1477, train acc: 0.5262, val loss: 1.1576, val acc: 0.5099  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 12960] train loss: 1.1449, train acc: 0.5248, val loss: 1.1497, val acc: 0.5251  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 12980] train loss: 1.1527, train acc: 0.5244, val loss: 1.1495, val acc: 0.5228  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 13000] train loss: 1.1577, train acc: 0.5134, val loss: 1.1503, val acc: 0.5164  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 13020] train loss: 1.1463, train acc: 0.5239, val loss: 1.1501, val acc: 0.5288  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 13040] train loss: 1.1482, train acc: 0.5238, val loss: 1.1462, val acc: 0.5268  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 13060] train loss: 1.1426, train acc: 0.5273, val loss: 1.1475, val acc: 0.5282  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 13080] train loss: 1.1497, train acc: 0.5230, val loss: 1.1478, val acc: 0.5258  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 13100] train loss: 1.1423, train acc: 0.5299, val loss: 1.1500, val acc: 0.5295  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 13120] train loss: 1.1461, train acc: 0.5244, val loss: 1.1442, val acc: 0.5288  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 13140] train loss: 1.1435, train acc: 0.5271, val loss: 1.1441, val acc: 0.5288  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 13160] train loss: 1.1446, train acc: 0.5221, val loss: 1.1439, val acc: 0.5258  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 13180] train loss: 1.1471, train acc: 0.5257, val loss: 1.1429, val acc: 0.5295  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 13200] train loss: 1.1427, train acc: 0.5221, val loss: 1.1432, val acc: 0.5282  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 13220] train loss: 1.1359, train acc: 0.5266, val loss: 1.1423, val acc: 0.5295  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 13240] train loss: 1.1393, train acc: 0.5244, val loss: 1.1430, val acc: 0.5261  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 13260] train loss: 1.1442, train acc: 0.5257, val loss: 1.1420, val acc: 0.5278  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 13280] train loss: 1.1419, train acc: 0.5271, val loss: 1.1415, val acc: 0.5288  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 13300] train loss: 1.1304, train acc: 0.5262, val loss: 1.1458, val acc: 0.5268  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 13320] train loss: 1.1410, train acc: 0.5265, val loss: 1.1411, val acc: 0.5292  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 13340] train loss: 1.1549, train acc: 0.5201, val loss: 1.1445, val acc: 0.5221  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 13360] train loss: 1.1364, train acc: 0.5270, val loss: 1.1412, val acc: 0.5282  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 13380] train loss: 1.1573, train acc: 0.5121, val loss: 1.1450, val acc: 0.5201  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 13400] train loss: 1.1468, train acc: 0.5224, val loss: 1.1427, val acc: 0.5282  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 13420] train loss: 1.1419, train acc: 0.5243, val loss: 1.1421, val acc: 0.5278  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 13440] train loss: 1.1457, train acc: 0.5267, val loss: 1.1398, val acc: 0.5309  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 13460] train loss: 1.1368, train acc: 0.5283, val loss: 1.1413, val acc: 0.5241  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 13480] train loss: 1.1411, train acc: 0.5280, val loss: 1.1407, val acc: 0.5278  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 13500] train loss: 1.1423, train acc: 0.5235, val loss: 1.1398, val acc: 0.5292  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 13520] train loss: 1.1431, train acc: 0.5231, val loss: 1.1392, val acc: 0.5315  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 13540] train loss: 1.1465, train acc: 0.5208, val loss: 1.1391, val acc: 0.5319  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 13560] train loss: 1.1361, train acc: 0.5296, val loss: 1.1419, val acc: 0.5305  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 13580] train loss: 1.1471, train acc: 0.5167, val loss: 1.1514, val acc: 0.5120  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 13600] train loss: 1.1340, train acc: 0.5268, val loss: 1.1449, val acc: 0.5174  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 13620] train loss: 1.1378, train acc: 0.5272, val loss: 1.1386, val acc: 0.5298  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 13640] train loss: 1.1419, train acc: 0.5253, val loss: 1.1425, val acc: 0.5224  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 13660] train loss: 1.1377, train acc: 0.5265, val loss: 1.1380, val acc: 0.5302  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 13680] train loss: 1.1370, train acc: 0.5296, val loss: 1.1408, val acc: 0.5278  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 13700] train loss: 1.1543, train acc: 0.5123, val loss: 1.1529, val acc: 0.5177  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 13720] train loss: 1.1342, train acc: 0.5285, val loss: 1.1388, val acc: 0.5309  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 13740] train loss: 1.1349, train acc: 0.5337, val loss: 1.1394, val acc: 0.5282  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 13760] train loss: 1.1300, train acc: 0.5292, val loss: 1.1370, val acc: 0.5305  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 13780] train loss: 1.1438, train acc: 0.5262, val loss: 1.1386, val acc: 0.5248  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 13800] train loss: 1.1296, train acc: 0.5294, val loss: 1.1376, val acc: 0.5278  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 13820] train loss: 1.1397, train acc: 0.5278, val loss: 1.1372, val acc: 0.5315  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 13840] train loss: 1.1373, train acc: 0.5314, val loss: 1.1368, val acc: 0.5309  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 13860] train loss: 1.1437, train acc: 0.5250, val loss: 1.1391, val acc: 0.5285  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 13880] train loss: 1.1368, train acc: 0.5270, val loss: 1.1399, val acc: 0.5302  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 13900] train loss: 1.1407, train acc: 0.5267, val loss: 1.1433, val acc: 0.5295  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 13920] train loss: 1.1398, train acc: 0.5275, val loss: 1.1367, val acc: 0.5305  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 13940] train loss: 1.1347, train acc: 0.5250, val loss: 1.1361, val acc: 0.5309  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 13960] train loss: 1.1347, train acc: 0.5282, val loss: 1.1432, val acc: 0.5278  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 13980] train loss: 1.1379, train acc: 0.5281, val loss: 1.1370, val acc: 0.5292  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 14000] train loss: 1.1334, train acc: 0.5289, val loss: 1.1398, val acc: 0.5241  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 14020] train loss: 1.1384, train acc: 0.5236, val loss: 1.1363, val acc: 0.5295  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 14040] train loss: 1.1393, train acc: 0.5197, val loss: 1.1353, val acc: 0.5309  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 14060] train loss: 1.1351, train acc: 0.5271, val loss: 1.1397, val acc: 0.5231  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 14080] train loss: 1.1364, train acc: 0.5260, val loss: 1.1448, val acc: 0.5197  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 14100] train loss: 1.1355, train acc: 0.5260, val loss: 1.1463, val acc: 0.5272  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 14120] train loss: 1.1351, train acc: 0.5312, val loss: 1.1366, val acc: 0.5288  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 14140] train loss: 1.1375, train acc: 0.5281, val loss: 1.1349, val acc: 0.5292  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 14160] train loss: 1.1436, train acc: 0.5223, val loss: 1.1374, val acc: 0.5241  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 14180] train loss: 1.1380, train acc: 0.5264, val loss: 1.1494, val acc: 0.5157  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 14200] train loss: 1.1385, train acc: 0.5270, val loss: 1.1358, val acc: 0.5315  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 14220] train loss: 1.1331, train acc: 0.5345, val loss: 1.1355, val acc: 0.5275  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 14240] train loss: 1.1357, train acc: 0.5270, val loss: 1.1354, val acc: 0.5275  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 14260] train loss: 1.1491, train acc: 0.5248, val loss: 1.1674, val acc: 0.5120  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 14280] train loss: 1.1399, train acc: 0.5248, val loss: 1.1377, val acc: 0.5272  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 14300] train loss: 1.1398, train acc: 0.5279, val loss: 1.1358, val acc: 0.5282  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 14320] train loss: 1.1467, train acc: 0.5174, val loss: 1.1460, val acc: 0.5255  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 14340] train loss: 1.1374, train acc: 0.5266, val loss: 1.1355, val acc: 0.5305  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 14360] train loss: 1.1283, train acc: 0.5301, val loss: 1.1343, val acc: 0.5305  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 14380] train loss: 1.1622, train acc: 0.4970, val loss: 1.1526, val acc: 0.5116  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 14400] train loss: 1.1498, train acc: 0.5194, val loss: 1.1483, val acc: 0.5197  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 14420] train loss: 1.1432, train acc: 0.5166, val loss: 1.1355, val acc: 0.5275  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 14440] train loss: 1.1411, train acc: 0.5312, val loss: 1.1355, val acc: 0.5312  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 14460] train loss: 1.1334, train acc: 0.5250, val loss: 1.1340, val acc: 0.5315  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 14480] train loss: 1.1413, train acc: 0.5243, val loss: 1.1334, val acc: 0.5319  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 14500] train loss: 1.1308, train acc: 0.5306, val loss: 1.1329, val acc: 0.5309  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 14520] train loss: 1.1320, train acc: 0.5318, val loss: 1.1345, val acc: 0.5275  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 14540] train loss: 1.1275, train acc: 0.5269, val loss: 1.1326, val acc: 0.5288  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 14560] train loss: 1.1339, train acc: 0.5278, val loss: 1.1328, val acc: 0.5329  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 14580] train loss: 1.1299, train acc: 0.5296, val loss: 1.1371, val acc: 0.5275  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 14600] train loss: 1.1379, train acc: 0.5254, val loss: 1.1330, val acc: 0.5285  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 14620] train loss: 1.1232, train acc: 0.5288, val loss: 1.1366, val acc: 0.5255  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 14640] train loss: 1.1379, train acc: 0.5231, val loss: 1.1349, val acc: 0.5312  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 14660] train loss: 1.1406, train acc: 0.5294, val loss: 1.1341, val acc: 0.5305  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 14680] train loss: 1.1369, train acc: 0.5272, val loss: 1.1330, val acc: 0.5285  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 14700] train loss: 1.1410, train acc: 0.5301, val loss: 1.1339, val acc: 0.5261  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 14720] train loss: 1.1344, train acc: 0.5268, val loss: 1.1312, val acc: 0.5319  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 14740] train loss: 1.1447, train acc: 0.5226, val loss: 1.1320, val acc: 0.5285  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 14760] train loss: 1.1257, train acc: 0.5312, val loss: 1.1343, val acc: 0.5282  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 14780] train loss: 1.1438, train acc: 0.5229, val loss: 1.1360, val acc: 0.5248  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 14800] train loss: 1.1355, train acc: 0.5263, val loss: 1.1314, val acc: 0.5302  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 14820] train loss: 1.1456, train acc: 0.5192, val loss: 1.1438, val acc: 0.5204  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 14840] train loss: 1.1355, train acc: 0.5246, val loss: 1.1411, val acc: 0.5170  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 14860] train loss: 1.1416, train acc: 0.5191, val loss: 1.1337, val acc: 0.5336  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 14880] train loss: 1.1276, train acc: 0.5315, val loss: 1.1305, val acc: 0.5309  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 14900] train loss: 1.1279, train acc: 0.5291, val loss: 1.1316, val acc: 0.5292  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 14920] train loss: 1.1318, train acc: 0.5251, val loss: 1.1312, val acc: 0.5288  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 14940] train loss: 1.1279, train acc: 0.5278, val loss: 1.1306, val acc: 0.5302  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 14960] train loss: 1.1232, train acc: 0.5316, val loss: 1.1316, val acc: 0.5302  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 14980] train loss: 1.1333, train acc: 0.5275, val loss: 1.1326, val acc: 0.5261  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 15000] train loss: 1.1318, train acc: 0.5301, val loss: 1.1296, val acc: 0.5339  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 15020] train loss: 1.1270, train acc: 0.5338, val loss: 1.1303, val acc: 0.5298  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 15040] train loss: 1.1384, train acc: 0.5234, val loss: 1.1317, val acc: 0.5322  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 15060] train loss: 1.1336, train acc: 0.5267, val loss: 1.1294, val acc: 0.5339  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 15080] train loss: 1.1544, train acc: 0.5073, val loss: 1.1629, val acc: 0.4938  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 15100] train loss: 1.1628, train acc: 0.5121, val loss: 1.1631, val acc: 0.5130  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 15120] train loss: 1.1351, train acc: 0.5303, val loss: 1.1383, val acc: 0.5258  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 15140] train loss: 1.1353, train acc: 0.5303, val loss: 1.1377, val acc: 0.5325  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 15160] train loss: 1.1351, train acc: 0.5270, val loss: 1.1379, val acc: 0.5255  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 15180] train loss: 1.1433, train acc: 0.5204, val loss: 1.1329, val acc: 0.5319  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 15200] train loss: 1.1347, train acc: 0.5307, val loss: 1.1350, val acc: 0.5329  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 15220] train loss: 1.1259, train acc: 0.5339, val loss: 1.1297, val acc: 0.5322  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 15240] train loss: 1.1256, train acc: 0.5332, val loss: 1.1301, val acc: 0.5336  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 15260] train loss: 1.1311, train acc: 0.5321, val loss: 1.1288, val acc: 0.5305  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 15280] train loss: 1.1325, train acc: 0.5254, val loss: 1.1287, val acc: 0.5302  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 15300] train loss: 1.1532, train acc: 0.5140, val loss: 1.1548, val acc: 0.5039  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 15320] train loss: 1.1304, train acc: 0.5278, val loss: 1.1352, val acc: 0.5292  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 15340] train loss: 1.1243, train acc: 0.5307, val loss: 1.1296, val acc: 0.5356  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 15360] train loss: 1.1355, train acc: 0.5275, val loss: 1.1345, val acc: 0.5224  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 15380] train loss: 1.1467, train acc: 0.5173, val loss: 1.1408, val acc: 0.5272  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 15400] train loss: 1.1313, train acc: 0.5281, val loss: 1.1302, val acc: 0.5332  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 15420] train loss: 1.1452, train acc: 0.5206, val loss: 1.1284, val acc: 0.5339  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 15440] train loss: 1.1334, train acc: 0.5348, val loss: 1.1298, val acc: 0.5322  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 15460] train loss: 1.1281, train acc: 0.5325, val loss: 1.1287, val acc: 0.5342  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 15480] train loss: 1.1307, train acc: 0.5344, val loss: 1.1292, val acc: 0.5288  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 15500] train loss: 1.1357, train acc: 0.5274, val loss: 1.1294, val acc: 0.5292  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 15520] train loss: 1.1313, train acc: 0.5270, val loss: 1.1271, val acc: 0.5339  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 15540] train loss: 1.1367, train acc: 0.5269, val loss: 1.1280, val acc: 0.5329  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 15560] train loss: 1.1311, train acc: 0.5243, val loss: 1.1317, val acc: 0.5241  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 15580] train loss: 1.1256, train acc: 0.5295, val loss: 1.1307, val acc: 0.5278  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 15600] train loss: 1.1275, train acc: 0.5250, val loss: 1.1349, val acc: 0.5349  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 15620] train loss: 1.1530, train acc: 0.5221, val loss: 1.1588, val acc: 0.5177  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 15640] train loss: 1.1337, train acc: 0.5281, val loss: 1.1302, val acc: 0.5228  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 15660] train loss: 1.1210, train acc: 0.5359, val loss: 1.1290, val acc: 0.5383  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 15680] train loss: 1.1312, train acc: 0.5253, val loss: 1.1286, val acc: 0.5339  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 15700] train loss: 1.1277, train acc: 0.5320, val loss: 1.1268, val acc: 0.5373  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 15720] train loss: 1.1211, train acc: 0.5324, val loss: 1.1262, val acc: 0.5366  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 15740] train loss: 1.1259, train acc: 0.5296, val loss: 1.1262, val acc: 0.5312  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 15760] train loss: 1.1190, train acc: 0.5345, val loss: 1.1307, val acc: 0.5255  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 15780] train loss: 1.1372, train acc: 0.5273, val loss: 1.1328, val acc: 0.5336  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 15800] train loss: 1.1270, train acc: 0.5272, val loss: 1.1348, val acc: 0.5201  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 15820] train loss: 1.1289, train acc: 0.5236, val loss: 1.1339, val acc: 0.5218  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 15840] train loss: 1.1218, train acc: 0.5324, val loss: 1.1276, val acc: 0.5325  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 15860] train loss: 1.1246, train acc: 0.5294, val loss: 1.1351, val acc: 0.5342  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 15880] train loss: 1.1259, train acc: 0.5338, val loss: 1.1254, val acc: 0.5325  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 15900] train loss: 1.1278, train acc: 0.5262, val loss: 1.1261, val acc: 0.5329  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 15920] train loss: 1.1310, train acc: 0.5316, val loss: 1.1248, val acc: 0.5363  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 15940] train loss: 1.1287, train acc: 0.5338, val loss: 1.1244, val acc: 0.5339  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 15960] train loss: 1.1289, train acc: 0.5292, val loss: 1.1277, val acc: 0.5288  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 15980] train loss: 1.1256, train acc: 0.5320, val loss: 1.1255, val acc: 0.5329  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 16000] train loss: 1.1300, train acc: 0.5288, val loss: 1.1238, val acc: 0.5352  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 16020] train loss: 1.1233, train acc: 0.5327, val loss: 1.1241, val acc: 0.5356  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 16040] train loss: 1.1215, train acc: 0.5348, val loss: 1.1238, val acc: 0.5352  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 16060] train loss: 1.1248, train acc: 0.5308, val loss: 1.1248, val acc: 0.5349  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 16080] train loss: 1.1460, train acc: 0.5134, val loss: 1.1496, val acc: 0.5089  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 16100] train loss: 1.1290, train acc: 0.5290, val loss: 1.1304, val acc: 0.5332  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 16120] train loss: 1.1747, train acc: 0.4871, val loss: 1.1783, val acc: 0.4887  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 16140] train loss: 1.1510, train acc: 0.5194, val loss: 1.1601, val acc: 0.5076  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 16160] train loss: 1.1519, train acc: 0.5187, val loss: 1.1546, val acc: 0.5174  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 16180] train loss: 1.1495, train acc: 0.5179, val loss: 1.1507, val acc: 0.5164  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 16200] train loss: 1.1477, train acc: 0.5207, val loss: 1.1481, val acc: 0.5170  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 16220] train loss: 1.1237, train acc: 0.5351, val loss: 1.1293, val acc: 0.5336  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 16240] train loss: 1.1279, train acc: 0.5336, val loss: 1.1270, val acc: 0.5268  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 16260] train loss: 1.1159, train acc: 0.5377, val loss: 1.1245, val acc: 0.5346  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 16280] train loss: 1.1245, train acc: 0.5317, val loss: 1.1331, val acc: 0.5268  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 16300] train loss: 1.1309, train acc: 0.5277, val loss: 1.1253, val acc: 0.5352  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 16320] train loss: 1.1359, train acc: 0.5185, val loss: 1.1255, val acc: 0.5386  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 16340] train loss: 1.1368, train acc: 0.5234, val loss: 1.1284, val acc: 0.5336  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 16360] train loss: 1.1310, train acc: 0.5282, val loss: 1.1239, val acc: 0.5359  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 16380] train loss: 1.1242, train acc: 0.5346, val loss: 1.1343, val acc: 0.5238  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 16400] train loss: 1.1258, train acc: 0.5294, val loss: 1.1325, val acc: 0.5238  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 16420] train loss: 1.1181, train acc: 0.5364, val loss: 1.1266, val acc: 0.5390  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 16440] train loss: 1.1267, train acc: 0.5343, val loss: 1.1257, val acc: 0.5329  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 16460] train loss: 1.1257, train acc: 0.5269, val loss: 1.1222, val acc: 0.5373  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 16480] train loss: 1.1520, train acc: 0.5207, val loss: 1.1478, val acc: 0.5207  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 16500] train loss: 1.1191, train acc: 0.5351, val loss: 1.1299, val acc: 0.5255  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 16520] train loss: 1.1395, train acc: 0.5257, val loss: 1.1415, val acc: 0.5160  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 16540] train loss: 1.1365, train acc: 0.5279, val loss: 1.1489, val acc: 0.5180  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 16560] train loss: 1.1205, train acc: 0.5348, val loss: 1.1269, val acc: 0.5336  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 16580] train loss: 1.1506, train acc: 0.5150, val loss: 1.1550, val acc: 0.5110  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 16600] train loss: 1.1522, train acc: 0.5189, val loss: 1.1451, val acc: 0.5180  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 16620] train loss: 1.1389, train acc: 0.5300, val loss: 1.1404, val acc: 0.5275  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 16640] train loss: 1.1311, train acc: 0.5281, val loss: 1.1230, val acc: 0.5342  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 16660] train loss: 1.1358, train acc: 0.5291, val loss: 1.1333, val acc: 0.5410  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 16680] train loss: 1.1246, train acc: 0.5338, val loss: 1.1239, val acc: 0.5369  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 16700] train loss: 1.1268, train acc: 0.5322, val loss: 1.1324, val acc: 0.5218  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 16720] train loss: 1.1352, train acc: 0.5277, val loss: 1.1220, val acc: 0.5373  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 16740] train loss: 1.1200, train acc: 0.5317, val loss: 1.1349, val acc: 0.5258  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 16760] train loss: 1.1240, train acc: 0.5361, val loss: 1.1241, val acc: 0.5329  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 16780] train loss: 1.1259, train acc: 0.5334, val loss: 1.1291, val acc: 0.5312  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 16800] train loss: 1.1204, train acc: 0.5349, val loss: 1.1221, val acc: 0.5359  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 16820] train loss: 1.1237, train acc: 0.5309, val loss: 1.1199, val acc: 0.5390  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 16840] train loss: 1.1297, train acc: 0.5330, val loss: 1.1251, val acc: 0.5373  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 16860] train loss: 1.1229, train acc: 0.5343, val loss: 1.1267, val acc: 0.5369  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 16880] train loss: 1.1259, train acc: 0.5270, val loss: 1.1335, val acc: 0.5218  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 16900] train loss: 1.1474, train acc: 0.5266, val loss: 1.1409, val acc: 0.5309  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 16920] train loss: 1.1299, train acc: 0.5316, val loss: 1.1203, val acc: 0.5363  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 16940] train loss: 1.1171, train acc: 0.5330, val loss: 1.1217, val acc: 0.5413  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 16960] train loss: 1.1145, train acc: 0.5346, val loss: 1.1254, val acc: 0.5390  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 16980] train loss: 1.1258, train acc: 0.5367, val loss: 1.1201, val acc: 0.5373  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 17000] train loss: 1.1254, train acc: 0.5296, val loss: 1.1194, val acc: 0.5386  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 17020] train loss: 1.1231, train acc: 0.5354, val loss: 1.1265, val acc: 0.5373  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 17040] train loss: 1.1255, train acc: 0.5309, val loss: 1.1235, val acc: 0.5329  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 17060] train loss: 1.1240, train acc: 0.5382, val loss: 1.1191, val acc: 0.5406  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 17080] train loss: 1.1170, train acc: 0.5415, val loss: 1.1220, val acc: 0.5413  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 17100] train loss: 1.1317, train acc: 0.5227, val loss: 1.1410, val acc: 0.5214  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 17120] train loss: 1.1277, train acc: 0.5339, val loss: 1.1302, val acc: 0.5366  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 17140] train loss: 1.1365, train acc: 0.5283, val loss: 1.1215, val acc: 0.5403  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 17160] train loss: 1.1239, train acc: 0.5364, val loss: 1.1305, val acc: 0.5292  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 17180] train loss: 1.1176, train acc: 0.5361, val loss: 1.1196, val acc: 0.5366  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 17200] train loss: 1.1291, train acc: 0.5336, val loss: 1.1345, val acc: 0.5268  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 17220] train loss: 1.1204, train acc: 0.5401, val loss: 1.1190, val acc: 0.5400  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 17240] train loss: 1.1110, train acc: 0.5396, val loss: 1.1188, val acc: 0.5332  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 17260] train loss: 1.1121, train acc: 0.5359, val loss: 1.1175, val acc: 0.5383  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 17280] train loss: 1.1289, train acc: 0.5254, val loss: 1.1456, val acc: 0.5150  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 17300] train loss: 1.1316, train acc: 0.5298, val loss: 1.1239, val acc: 0.5373  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 17320] train loss: 1.1215, train acc: 0.5382, val loss: 1.1249, val acc: 0.5322  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 17340] train loss: 1.1429, train acc: 0.5235, val loss: 1.1424, val acc: 0.5268  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 17360] train loss: 1.1241, train acc: 0.5346, val loss: 1.1376, val acc: 0.5268  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 17380] train loss: 1.1163, train acc: 0.5366, val loss: 1.1176, val acc: 0.5386  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 17400] train loss: 1.1227, train acc: 0.5286, val loss: 1.1205, val acc: 0.5339  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 17420] train loss: 1.1138, train acc: 0.5361, val loss: 1.1209, val acc: 0.5373  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 17440] train loss: 1.1223, train acc: 0.5354, val loss: 1.1203, val acc: 0.5346  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 17460] train loss: 1.1247, train acc: 0.5334, val loss: 1.1175, val acc: 0.5433  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 17480] train loss: 1.1281, train acc: 0.5304, val loss: 1.1162, val acc: 0.5400  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 17500] train loss: 1.1208, train acc: 0.5396, val loss: 1.1153, val acc: 0.5390  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 17520] train loss: 1.1196, train acc: 0.5390, val loss: 1.1147, val acc: 0.5423  (best train acc: 0.5536, best val acc: 0.5447, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 17540] train loss: 1.1166, train acc: 0.5372, val loss: 1.1163, val acc: 0.5349  (best train acc: 0.5536, best val acc: 0.5447, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 17560] train loss: 1.1083, train acc: 0.5377, val loss: 1.1224, val acc: 0.5410  (best train acc: 0.5536, best val acc: 0.5447, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 17580] train loss: 1.1250, train acc: 0.5298, val loss: 1.1149, val acc: 0.5417  (best train acc: 0.5536, best val acc: 0.5447, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 17600] train loss: 1.1162, train acc: 0.5309, val loss: 1.1173, val acc: 0.5373  (best train acc: 0.5536, best val acc: 0.5447, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 17620] train loss: 1.1259, train acc: 0.5329, val loss: 1.1675, val acc: 0.4968  (best train acc: 0.5536, best val acc: 0.5450, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 17640] train loss: 1.1601, train acc: 0.5187, val loss: 1.1544, val acc: 0.5224  (best train acc: 0.5536, best val acc: 0.5450, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 17660] train loss: 1.1248, train acc: 0.5319, val loss: 1.1443, val acc: 0.5245  (best train acc: 0.5536, best val acc: 0.5450, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 17680] train loss: 1.1329, train acc: 0.5353, val loss: 1.1390, val acc: 0.5298  (best train acc: 0.5536, best val acc: 0.5450, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 17700] train loss: 1.1257, train acc: 0.5310, val loss: 1.1368, val acc: 0.5332  (best train acc: 0.5536, best val acc: 0.5450, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 17720] train loss: 1.1341, train acc: 0.5316, val loss: 1.1354, val acc: 0.5309  (best train acc: 0.5536, best val acc: 0.5450, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 17740] train loss: 1.1304, train acc: 0.5274, val loss: 1.1331, val acc: 0.5339  (best train acc: 0.5536, best val acc: 0.5450, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 17760] train loss: 1.1253, train acc: 0.5362, val loss: 1.1309, val acc: 0.5342  (best train acc: 0.5536, best val acc: 0.5450, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 17780] train loss: 1.1247, train acc: 0.5296, val loss: 1.1297, val acc: 0.5309  (best train acc: 0.5536, best val acc: 0.5450, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 17800] train loss: 1.1289, train acc: 0.5365, val loss: 1.1292, val acc: 0.5332  (best train acc: 0.5536, best val acc: 0.5450, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 17820] train loss: 1.1128, train acc: 0.5381, val loss: 1.1277, val acc: 0.5336  (best train acc: 0.5536, best val acc: 0.5450, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 17840] train loss: 1.1305, train acc: 0.5330, val loss: 1.1269, val acc: 0.5332  (best train acc: 0.5536, best val acc: 0.5450, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 17860] train loss: 1.1310, train acc: 0.5359, val loss: 1.1262, val acc: 0.5342  (best train acc: 0.5536, best val acc: 0.5450, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 17880] train loss: 1.1194, train acc: 0.5381, val loss: 1.1256, val acc: 0.5373  (best train acc: 0.5536, best val acc: 0.5450, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 17900] train loss: 1.1344, train acc: 0.5269, val loss: 1.1252, val acc: 0.5386  (best train acc: 0.5536, best val acc: 0.5450, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 17920] train loss: 1.1233, train acc: 0.5364, val loss: 1.1242, val acc: 0.5383  (best train acc: 0.5536, best val acc: 0.5450, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 17940] train loss: 1.1267, train acc: 0.5372, val loss: 1.1240, val acc: 0.5400  (best train acc: 0.5536, best val acc: 0.5450, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 17960] train loss: 1.1263, train acc: 0.5389, val loss: 1.1242, val acc: 0.5352  (best train acc: 0.5536, best val acc: 0.5450, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 17980] train loss: 1.1170, train acc: 0.5354, val loss: 1.1229, val acc: 0.5373  (best train acc: 0.5536, best val acc: 0.5450, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 18000] train loss: 1.1298, train acc: 0.5368, val loss: 1.1223, val acc: 0.5386  (best train acc: 0.5536, best val acc: 0.5450, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 18020] train loss: 1.1235, train acc: 0.5382, val loss: 1.1216, val acc: 0.5390  (best train acc: 0.5536, best val acc: 0.5450, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 18040] train loss: 1.1176, train acc: 0.5349, val loss: 1.1221, val acc: 0.5349  (best train acc: 0.5536, best val acc: 0.5450, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 18060] train loss: 1.1153, train acc: 0.5417, val loss: 1.1212, val acc: 0.5369  (best train acc: 0.5536, best val acc: 0.5450, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 18080] train loss: 1.1120, train acc: 0.5441, val loss: 1.1200, val acc: 0.5417  (best train acc: 0.5536, best val acc: 0.5450, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 18100] train loss: 1.1259, train acc: 0.5369, val loss: 1.1191, val acc: 0.5396  (best train acc: 0.5536, best val acc: 0.5450, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 18120] train loss: 1.1193, train acc: 0.5409, val loss: 1.1198, val acc: 0.5386  (best train acc: 0.5536, best val acc: 0.5450, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 18140] train loss: 1.1281, train acc: 0.5392, val loss: 1.1189, val acc: 0.5390  (best train acc: 0.5536, best val acc: 0.5450, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 18160] train loss: 1.1092, train acc: 0.5445, val loss: 1.1186, val acc: 0.5420  (best train acc: 0.5536, best val acc: 0.5450, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 18180] train loss: 1.1238, train acc: 0.5376, val loss: 1.1179, val acc: 0.5427  (best train acc: 0.5536, best val acc: 0.5460, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 18200] train loss: 1.1289, train acc: 0.5374, val loss: 1.1183, val acc: 0.5379  (best train acc: 0.5536, best val acc: 0.5460, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 18220] train loss: 1.1150, train acc: 0.5403, val loss: 1.1170, val acc: 0.5430  (best train acc: 0.5536, best val acc: 0.5460, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 18240] train loss: 1.1152, train acc: 0.5406, val loss: 1.1167, val acc: 0.5423  (best train acc: 0.5536, best val acc: 0.5464, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 18260] train loss: 1.1110, train acc: 0.5476, val loss: 1.1166, val acc: 0.5450  (best train acc: 0.5536, best val acc: 0.5464, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 18280] train loss: 1.1123, train acc: 0.5432, val loss: 1.1153, val acc: 0.5460  (best train acc: 0.5536, best val acc: 0.5467, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 18300] train loss: 1.1221, train acc: 0.5382, val loss: 1.1168, val acc: 0.5454  (best train acc: 0.5536, best val acc: 0.5467, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 18320] train loss: 1.1116, train acc: 0.5428, val loss: 1.1152, val acc: 0.5427  (best train acc: 0.5536, best val acc: 0.5481, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 18340] train loss: 1.1148, train acc: 0.5440, val loss: 1.1172, val acc: 0.5390  (best train acc: 0.5536, best val acc: 0.5481, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 18360] train loss: 1.1126, train acc: 0.5400, val loss: 1.1153, val acc: 0.5457  (best train acc: 0.5536, best val acc: 0.5481, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 18380] train loss: 1.1174, train acc: 0.5335, val loss: 1.1137, val acc: 0.5450  (best train acc: 0.5536, best val acc: 0.5481, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 18400] train loss: 1.1122, train acc: 0.5415, val loss: 1.1141, val acc: 0.5470  (best train acc: 0.5536, best val acc: 0.5484, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 18420] train loss: 1.1157, train acc: 0.5386, val loss: 1.1128, val acc: 0.5437  (best train acc: 0.5536, best val acc: 0.5484, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 18440] train loss: 1.1111, train acc: 0.5378, val loss: 1.1130, val acc: 0.5423  (best train acc: 0.5536, best val acc: 0.5484, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 18460] train loss: 1.1146, train acc: 0.5383, val loss: 1.1122, val acc: 0.5467  (best train acc: 0.5536, best val acc: 0.5484, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 18480] train loss: 1.1219, train acc: 0.5414, val loss: 1.1134, val acc: 0.5417  (best train acc: 0.5536, best val acc: 0.5484, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 18500] train loss: 1.1183, train acc: 0.5416, val loss: 1.1126, val acc: 0.5427  (best train acc: 0.5536, best val acc: 0.5484, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 18520] train loss: 1.1116, train acc: 0.5416, val loss: 1.1120, val acc: 0.5444  (best train acc: 0.5536, best val acc: 0.5491, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 18540] train loss: 1.1041, train acc: 0.5404, val loss: 1.1109, val acc: 0.5470  (best train acc: 0.5536, best val acc: 0.5491, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 18560] train loss: 1.1193, train acc: 0.5375, val loss: 1.1106, val acc: 0.5427  (best train acc: 0.5536, best val acc: 0.5497, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 18580] train loss: 1.1121, train acc: 0.5386, val loss: 1.1130, val acc: 0.5444  (best train acc: 0.5536, best val acc: 0.5497, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 18600] train loss: 1.1145, train acc: 0.5440, val loss: 1.1104, val acc: 0.5470  (best train acc: 0.5536, best val acc: 0.5504, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 18620] train loss: 1.1052, train acc: 0.5438, val loss: 1.1099, val acc: 0.5450  (best train acc: 0.5536, best val acc: 0.5504, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 18640] train loss: 1.1124, train acc: 0.5421, val loss: 1.1135, val acc: 0.5410  (best train acc: 0.5536, best val acc: 0.5504, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 18660] train loss: 1.1151, train acc: 0.5432, val loss: 1.1104, val acc: 0.5400  (best train acc: 0.5536, best val acc: 0.5504, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 18680] train loss: 1.1166, train acc: 0.5393, val loss: 1.1105, val acc: 0.5417  (best train acc: 0.5536, best val acc: 0.5504, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 18700] train loss: 1.1128, train acc: 0.5378, val loss: 1.1093, val acc: 0.5464  (best train acc: 0.5536, best val acc: 0.5504, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 18720] train loss: 1.1051, train acc: 0.5489, val loss: 1.1086, val acc: 0.5447  (best train acc: 0.5536, best val acc: 0.5504, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 18740] train loss: 1.1119, train acc: 0.5432, val loss: 1.1093, val acc: 0.5464  (best train acc: 0.5536, best val acc: 0.5504, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 18760] train loss: 1.1101, train acc: 0.5397, val loss: 1.1083, val acc: 0.5464  (best train acc: 0.5536, best val acc: 0.5504, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 18780] train loss: 1.1049, train acc: 0.5452, val loss: 1.1078, val acc: 0.5474  (best train acc: 0.5536, best val acc: 0.5504, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 18800] train loss: 1.1143, train acc: 0.5417, val loss: 1.1087, val acc: 0.5457  (best train acc: 0.5536, best val acc: 0.5504, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 18820] train loss: 1.1293, train acc: 0.5354, val loss: 1.1073, val acc: 0.5494  (best train acc: 0.5543, best val acc: 0.5504, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 18840] train loss: 1.1135, train acc: 0.5420, val loss: 1.1076, val acc: 0.5454  (best train acc: 0.5543, best val acc: 0.5504, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 18860] train loss: 1.1168, train acc: 0.5332, val loss: 1.1092, val acc: 0.5406  (best train acc: 0.5543, best val acc: 0.5504, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 18880] train loss: 1.1152, train acc: 0.5399, val loss: 1.1090, val acc: 0.5491  (best train acc: 0.5543, best val acc: 0.5504, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 18900] train loss: 1.1165, train acc: 0.5358, val loss: 1.1096, val acc: 0.5406  (best train acc: 0.5543, best val acc: 0.5504, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 18920] train loss: 1.1090, train acc: 0.5400, val loss: 1.1150, val acc: 0.5366  (best train acc: 0.5543, best val acc: 0.5504, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 18940] train loss: 1.1044, train acc: 0.5440, val loss: 1.1080, val acc: 0.5491  (best train acc: 0.5543, best val acc: 0.5504, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 18960] train loss: 1.1021, train acc: 0.5445, val loss: 1.1063, val acc: 0.5444  (best train acc: 0.5543, best val acc: 0.5504, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 18980] train loss: 1.0962, train acc: 0.5490, val loss: 1.1059, val acc: 0.5437  (best train acc: 0.5543, best val acc: 0.5504, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 19000] train loss: 1.1119, train acc: 0.5434, val loss: 1.1052, val acc: 0.5467  (best train acc: 0.5543, best val acc: 0.5504, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 19020] train loss: 1.1001, train acc: 0.5437, val loss: 1.1059, val acc: 0.5447  (best train acc: 0.5543, best val acc: 0.5508, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 19040] train loss: 1.1100, train acc: 0.5415, val loss: 1.1112, val acc: 0.5454  (best train acc: 0.5543, best val acc: 0.5508, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 19060] train loss: 1.1023, train acc: 0.5479, val loss: 1.1062, val acc: 0.5474  (best train acc: 0.5549, best val acc: 0.5508, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 19080] train loss: 1.1123, train acc: 0.5360, val loss: 1.1128, val acc: 0.5454  (best train acc: 0.5549, best val acc: 0.5508, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 19100] train loss: 1.1114, train acc: 0.5435, val loss: 1.1041, val acc: 0.5511  (best train acc: 0.5549, best val acc: 0.5511, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 19120] train loss: 1.1193, train acc: 0.5372, val loss: 1.1062, val acc: 0.5447  (best train acc: 0.5549, best val acc: 0.5511, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 19140] train loss: 1.1203, train acc: 0.5364, val loss: 1.1045, val acc: 0.5481  (best train acc: 0.5549, best val acc: 0.5511, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 19160] train loss: 1.1037, train acc: 0.5453, val loss: 1.1051, val acc: 0.5470  (best train acc: 0.5549, best val acc: 0.5511, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 19180] train loss: 1.0910, train acc: 0.5535, val loss: 1.1037, val acc: 0.5457  (best train acc: 0.5549, best val acc: 0.5511, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 19200] train loss: 1.1111, train acc: 0.5456, val loss: 1.1068, val acc: 0.5393  (best train acc: 0.5549, best val acc: 0.5514, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 19220] train loss: 1.1062, train acc: 0.5408, val loss: 1.1068, val acc: 0.5481  (best train acc: 0.5549, best val acc: 0.5514, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 19240] train loss: 1.1183, train acc: 0.5392, val loss: 1.1048, val acc: 0.5477  (best train acc: 0.5549, best val acc: 0.5514, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 19260] train loss: 1.1007, train acc: 0.5461, val loss: 1.1032, val acc: 0.5481  (best train acc: 0.5549, best val acc: 0.5514, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 19280] train loss: 1.1165, train acc: 0.5427, val loss: 1.1047, val acc: 0.5450  (best train acc: 0.5549, best val acc: 0.5518, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 19300] train loss: 1.1021, train acc: 0.5455, val loss: 1.1026, val acc: 0.5467  (best train acc: 0.5549, best val acc: 0.5528, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 19320] train loss: 1.1160, train acc: 0.5380, val loss: 1.1034, val acc: 0.5494  (best train acc: 0.5549, best val acc: 0.5528, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 19340] train loss: 1.1150, train acc: 0.5378, val loss: 1.1024, val acc: 0.5504  (best train acc: 0.5549, best val acc: 0.5528, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 19360] train loss: 1.1036, train acc: 0.5440, val loss: 1.1014, val acc: 0.5484  (best train acc: 0.5549, best val acc: 0.5528, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 19380] train loss: 1.0997, train acc: 0.5455, val loss: 1.1063, val acc: 0.5450  (best train acc: 0.5549, best val acc: 0.5528, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 19400] train loss: 1.1049, train acc: 0.5428, val loss: 1.1023, val acc: 0.5491  (best train acc: 0.5549, best val acc: 0.5528, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 19420] train loss: 1.0997, train acc: 0.5411, val loss: 1.1043, val acc: 0.5470  (best train acc: 0.5549, best val acc: 0.5528, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 19440] train loss: 1.0962, train acc: 0.5489, val loss: 1.1003, val acc: 0.5508  (best train acc: 0.5549, best val acc: 0.5528, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 19460] train loss: 1.1064, train acc: 0.5471, val loss: 1.1038, val acc: 0.5423  (best train acc: 0.5549, best val acc: 0.5528, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 19480] train loss: 1.1120, train acc: 0.5408, val loss: 1.1075, val acc: 0.5417  (best train acc: 0.5549, best val acc: 0.5528, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 19500] train loss: 1.0982, train acc: 0.5514, val loss: 1.1008, val acc: 0.5501  (best train acc: 0.5549, best val acc: 0.5528, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 19520] train loss: 1.1162, train acc: 0.5368, val loss: 1.1049, val acc: 0.5440  (best train acc: 0.5553, best val acc: 0.5531, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 19540] train loss: 1.1043, train acc: 0.5403, val loss: 1.0993, val acc: 0.5504  (best train acc: 0.5553, best val acc: 0.5531, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 19560] train loss: 1.1047, train acc: 0.5414, val loss: 1.1004, val acc: 0.5481  (best train acc: 0.5555, best val acc: 0.5531, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 19580] train loss: 1.1066, train acc: 0.5419, val loss: 1.0996, val acc: 0.5497  (best train acc: 0.5555, best val acc: 0.5531, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 19600] train loss: 1.1163, train acc: 0.5391, val loss: 1.1040, val acc: 0.5417  (best train acc: 0.5555, best val acc: 0.5531, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 19620] train loss: 1.1022, train acc: 0.5416, val loss: 1.0986, val acc: 0.5470  (best train acc: 0.5555, best val acc: 0.5531, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 19640] train loss: 1.0998, train acc: 0.5457, val loss: 1.0986, val acc: 0.5501  (best train acc: 0.5555, best val acc: 0.5531, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 19660] train loss: 1.1207, train acc: 0.5363, val loss: 1.0975, val acc: 0.5481  (best train acc: 0.5555, best val acc: 0.5545, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 19680] train loss: 1.0862, train acc: 0.5527, val loss: 1.1010, val acc: 0.5487  (best train acc: 0.5555, best val acc: 0.5545, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 19700] train loss: 1.1128, train acc: 0.5445, val loss: 1.1087, val acc: 0.5470  (best train acc: 0.5555, best val acc: 0.5545, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 19720] train loss: 1.1100, train acc: 0.5476, val loss: 1.1059, val acc: 0.5467  (best train acc: 0.5555, best val acc: 0.5545, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 19740] train loss: 1.1032, train acc: 0.5447, val loss: 1.0984, val acc: 0.5484  (best train acc: 0.5555, best val acc: 0.5545, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 19760] train loss: 1.1026, train acc: 0.5510, val loss: 1.0983, val acc: 0.5508  (best train acc: 0.5555, best val acc: 0.5545, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 19780] train loss: 1.1117, train acc: 0.5455, val loss: 1.0997, val acc: 0.5474  (best train acc: 0.5555, best val acc: 0.5548, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 19800] train loss: 1.0966, train acc: 0.5506, val loss: 1.0995, val acc: 0.5467  (best train acc: 0.5555, best val acc: 0.5548, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 19820] train loss: 1.1106, train acc: 0.5491, val loss: 1.0973, val acc: 0.5521  (best train acc: 0.5555, best val acc: 0.5548, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 19840] train loss: 1.0954, train acc: 0.5460, val loss: 1.0987, val acc: 0.5518  (best train acc: 0.5555, best val acc: 0.5548, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 19860] train loss: 1.0946, train acc: 0.5463, val loss: 1.0991, val acc: 0.5464  (best train acc: 0.5577, best val acc: 0.5548, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 19880] train loss: 1.1118, train acc: 0.5377, val loss: 1.0971, val acc: 0.5504  (best train acc: 0.5577, best val acc: 0.5548, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 19900] train loss: 1.0905, train acc: 0.5561, val loss: 1.0985, val acc: 0.5460  (best train acc: 0.5577, best val acc: 0.5548, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 19920] train loss: 1.1119, train acc: 0.5410, val loss: 1.0962, val acc: 0.5481  (best train acc: 0.5577, best val acc: 0.5548, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 19940] train loss: 1.0988, train acc: 0.5453, val loss: 1.0968, val acc: 0.5511  (best train acc: 0.5577, best val acc: 0.5548, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 19960] train loss: 1.1033, train acc: 0.5382, val loss: 1.0988, val acc: 0.5504  (best train acc: 0.5577, best val acc: 0.5548, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 19980] train loss: 1.0997, train acc: 0.5431, val loss: 1.0990, val acc: 0.5464  (best train acc: 0.5577, best val acc: 0.5548, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 20000] train loss: 1.0985, train acc: 0.5474, val loss: 1.0952, val acc: 0.5501  (best train acc: 0.5577, best val acc: 0.5548, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 20020] train loss: 1.1156, train acc: 0.5345, val loss: 1.0963, val acc: 0.5511  (best train acc: 0.5577, best val acc: 0.5548, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 20040] train loss: 1.1082, train acc: 0.5441, val loss: 1.0955, val acc: 0.5518  (best train acc: 0.5577, best val acc: 0.5548, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 20060] train loss: 1.0994, train acc: 0.5469, val loss: 1.1009, val acc: 0.5477  (best train acc: 0.5577, best val acc: 0.5548, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 20080] train loss: 1.1003, train acc: 0.5477, val loss: 1.0980, val acc: 0.5504  (best train acc: 0.5577, best val acc: 0.5548, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 20100] train loss: 1.1065, train acc: 0.5370, val loss: 1.1128, val acc: 0.5423  (best train acc: 0.5577, best val acc: 0.5548, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 20120] train loss: 1.1215, train acc: 0.5338, val loss: 1.0989, val acc: 0.5413  (best train acc: 0.5577, best val acc: 0.5548, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 20140] train loss: 1.0918, train acc: 0.5479, val loss: 1.0954, val acc: 0.5474  (best train acc: 0.5577, best val acc: 0.5548, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 20160] train loss: 1.1230, train acc: 0.5334, val loss: 1.1068, val acc: 0.5447  (best train acc: 0.5577, best val acc: 0.5551, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 20180] train loss: 1.1056, train acc: 0.5444, val loss: 1.0965, val acc: 0.5467  (best train acc: 0.5577, best val acc: 0.5551, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 20200] train loss: 1.1117, train acc: 0.5418, val loss: 1.0985, val acc: 0.5440  (best train acc: 0.5577, best val acc: 0.5551, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 20220] train loss: 1.0964, train acc: 0.5496, val loss: 1.0950, val acc: 0.5497  (best train acc: 0.5577, best val acc: 0.5551, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 20240] train loss: 1.1099, train acc: 0.5419, val loss: 1.0925, val acc: 0.5508  (best train acc: 0.5577, best val acc: 0.5551, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 20260] train loss: 1.0999, train acc: 0.5478, val loss: 1.0932, val acc: 0.5504  (best train acc: 0.5577, best val acc: 0.5551, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 20280] train loss: 1.0964, train acc: 0.5464, val loss: 1.0928, val acc: 0.5484  (best train acc: 0.5577, best val acc: 0.5551, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 20300] train loss: 1.1071, train acc: 0.5412, val loss: 1.0933, val acc: 0.5491  (best train acc: 0.5577, best val acc: 0.5551, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 20320] train loss: 1.0979, train acc: 0.5429, val loss: 1.0942, val acc: 0.5501  (best train acc: 0.5577, best val acc: 0.5551, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 20340] train loss: 1.1058, train acc: 0.5407, val loss: 1.0959, val acc: 0.5477  (best train acc: 0.5577, best val acc: 0.5551, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 20360] train loss: 1.0953, train acc: 0.5535, val loss: 1.0942, val acc: 0.5464  (best train acc: 0.5577, best val acc: 0.5551, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 20380] train loss: 1.1017, train acc: 0.5439, val loss: 1.0945, val acc: 0.5514  (best train acc: 0.5577, best val acc: 0.5551, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 20400] train loss: 1.0945, train acc: 0.5508, val loss: 1.0965, val acc: 0.5484  (best train acc: 0.5577, best val acc: 0.5551, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 20420] train loss: 1.1081, train acc: 0.5403, val loss: 1.0925, val acc: 0.5484  (best train acc: 0.5577, best val acc: 0.5551, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 20440] train loss: 1.1037, train acc: 0.5462, val loss: 1.0932, val acc: 0.5518  (best train acc: 0.5577, best val acc: 0.5551, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 20460] train loss: 1.1125, train acc: 0.5382, val loss: 1.0927, val acc: 0.5497  (best train acc: 0.5577, best val acc: 0.5551, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 20480] train loss: 1.1018, train acc: 0.5393, val loss: 1.0906, val acc: 0.5504  (best train acc: 0.5577, best val acc: 0.5551, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 20500] train loss: 1.1053, train acc: 0.5438, val loss: 1.0900, val acc: 0.5508  (best train acc: 0.5577, best val acc: 0.5551, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 20520] train loss: 1.0905, train acc: 0.5487, val loss: 1.0900, val acc: 0.5501  (best train acc: 0.5577, best val acc: 0.5551, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 20540] train loss: 1.0881, train acc: 0.5434, val loss: 1.0949, val acc: 0.5437  (best train acc: 0.5577, best val acc: 0.5551, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 20560] train loss: 1.0856, train acc: 0.5505, val loss: 1.0928, val acc: 0.5487  (best train acc: 0.5577, best val acc: 0.5551, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 20580] train loss: 1.0935, train acc: 0.5516, val loss: 1.0895, val acc: 0.5497  (best train acc: 0.5577, best val acc: 0.5551, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 20600] train loss: 1.0931, train acc: 0.5488, val loss: 1.0900, val acc: 0.5531  (best train acc: 0.5577, best val acc: 0.5551, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 20620] train loss: 1.1152, train acc: 0.5364, val loss: 1.0940, val acc: 0.5464  (best train acc: 0.5577, best val acc: 0.5551, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 20640] train loss: 1.1057, train acc: 0.5448, val loss: 1.0899, val acc: 0.5562  (best train acc: 0.5577, best val acc: 0.5562, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 20660] train loss: 1.0736, train acc: 0.5628, val loss: 1.0929, val acc: 0.5474  (best train acc: 0.5628, best val acc: 0.5562, best train loss: 1.0736  @ epoch 20660 )\n",
      "[Epoch: 20680] train loss: 1.0832, train acc: 0.5455, val loss: 1.0901, val acc: 0.5494  (best train acc: 0.5628, best val acc: 0.5562, best train loss: 1.0736  @ epoch 20660 )\n",
      "[Epoch: 20700] train loss: 1.1060, train acc: 0.5450, val loss: 1.0942, val acc: 0.5477  (best train acc: 0.5628, best val acc: 0.5562, best train loss: 1.0736  @ epoch 20660 )\n",
      "[Epoch: 20720] train loss: 1.0957, train acc: 0.5461, val loss: 1.0938, val acc: 0.5481  (best train acc: 0.5628, best val acc: 0.5562, best train loss: 1.0736  @ epoch 20660 )\n",
      "[Epoch: 20740] train loss: 1.0930, train acc: 0.5471, val loss: 1.0938, val acc: 0.5487  (best train acc: 0.5628, best val acc: 0.5562, best train loss: 1.0736  @ epoch 20660 )\n",
      "[Epoch: 20760] train loss: 1.1041, train acc: 0.5440, val loss: 1.0882, val acc: 0.5497  (best train acc: 0.5628, best val acc: 0.5562, best train loss: 1.0736  @ epoch 20660 )\n",
      "[Epoch: 20780] train loss: 1.1002, train acc: 0.5463, val loss: 1.0874, val acc: 0.5501  (best train acc: 0.5628, best val acc: 0.5562, best train loss: 1.0736  @ epoch 20660 )\n",
      "[Epoch: 20800] train loss: 1.1013, train acc: 0.5440, val loss: 1.0871, val acc: 0.5494  (best train acc: 0.5628, best val acc: 0.5562, best train loss: 1.0736  @ epoch 20660 )\n",
      "[Epoch: 20820] train loss: 1.0930, train acc: 0.5497, val loss: 1.0877, val acc: 0.5497  (best train acc: 0.5628, best val acc: 0.5562, best train loss: 1.0736  @ epoch 20660 )\n",
      "[Epoch: 20840] train loss: 1.0903, train acc: 0.5530, val loss: 1.0875, val acc: 0.5494  (best train acc: 0.5628, best val acc: 0.5562, best train loss: 1.0736  @ epoch 20660 )\n",
      "[Epoch: 20860] train loss: 1.0905, train acc: 0.5513, val loss: 1.0879, val acc: 0.5521  (best train acc: 0.5628, best val acc: 0.5562, best train loss: 1.0736  @ epoch 20660 )\n",
      "[Epoch: 20880] train loss: 1.1108, train acc: 0.5378, val loss: 1.1123, val acc: 0.5366  (best train acc: 0.5628, best val acc: 0.5562, best train loss: 1.0736  @ epoch 20660 )\n",
      "[Epoch: 20900] train loss: 1.0945, train acc: 0.5503, val loss: 1.0925, val acc: 0.5501  (best train acc: 0.5628, best val acc: 0.5562, best train loss: 1.0736  @ epoch 20660 )\n",
      "[Epoch: 20920] train loss: 1.0908, train acc: 0.5496, val loss: 1.0945, val acc: 0.5494  (best train acc: 0.5628, best val acc: 0.5562, best train loss: 1.0736  @ epoch 20660 )\n",
      "[Epoch: 20940] train loss: 1.0965, train acc: 0.5456, val loss: 1.0886, val acc: 0.5491  (best train acc: 0.5628, best val acc: 0.5562, best train loss: 1.0736  @ epoch 20660 )\n",
      "[Epoch: 20960] train loss: 1.0915, train acc: 0.5450, val loss: 1.0949, val acc: 0.5467  (best train acc: 0.5628, best val acc: 0.5562, best train loss: 1.0736  @ epoch 20660 )\n",
      "[Epoch: 20980] train loss: 1.0817, train acc: 0.5534, val loss: 1.0879, val acc: 0.5491  (best train acc: 0.5628, best val acc: 0.5562, best train loss: 1.0736  @ epoch 20660 )\n",
      "[Epoch: 21000] train loss: 1.1155, train acc: 0.5356, val loss: 1.1044, val acc: 0.5454  (best train acc: 0.5628, best val acc: 0.5562, best train loss: 1.0736  @ epoch 20660 )\n",
      "[Epoch: 21020] train loss: 1.1057, train acc: 0.5477, val loss: 1.0904, val acc: 0.5504  (best train acc: 0.5628, best val acc: 0.5562, best train loss: 1.0736  @ epoch 20660 )\n",
      "[Epoch: 21040] train loss: 1.0854, train acc: 0.5526, val loss: 1.0872, val acc: 0.5508  (best train acc: 0.5628, best val acc: 0.5562, best train loss: 1.0736  @ epoch 20660 )\n",
      "[Epoch: 21060] train loss: 1.0897, train acc: 0.5490, val loss: 1.0869, val acc: 0.5514  (best train acc: 0.5628, best val acc: 0.5562, best train loss: 1.0736  @ epoch 20660 )\n",
      "[Epoch: 21080] train loss: 1.1003, train acc: 0.5471, val loss: 1.0876, val acc: 0.5551  (best train acc: 0.5628, best val acc: 0.5562, best train loss: 1.0736  @ epoch 20660 )\n",
      "[Epoch: 21100] train loss: 1.0983, train acc: 0.5463, val loss: 1.0857, val acc: 0.5528  (best train acc: 0.5628, best val acc: 0.5562, best train loss: 1.0736  @ epoch 20660 )\n",
      "[Epoch: 21120] train loss: 1.0861, train acc: 0.5497, val loss: 1.0848, val acc: 0.5535  (best train acc: 0.5628, best val acc: 0.5562, best train loss: 1.0736  @ epoch 20660 )\n",
      "[Epoch: 21140] train loss: 1.0926, train acc: 0.5427, val loss: 1.0998, val acc: 0.5457  (best train acc: 0.5628, best val acc: 0.5562, best train loss: 1.0736  @ epoch 20660 )\n",
      "[Epoch: 21160] train loss: 1.1080, train acc: 0.5452, val loss: 1.1088, val acc: 0.5403  (best train acc: 0.5628, best val acc: 0.5562, best train loss: 1.0736  @ epoch 20660 )\n",
      "[Epoch: 21180] train loss: 1.0909, train acc: 0.5525, val loss: 1.0892, val acc: 0.5484  (best train acc: 0.5628, best val acc: 0.5562, best train loss: 1.0736  @ epoch 20660 )\n",
      "[Epoch: 21200] train loss: 1.0916, train acc: 0.5497, val loss: 1.0860, val acc: 0.5521  (best train acc: 0.5628, best val acc: 0.5562, best train loss: 1.0736  @ epoch 20660 )\n",
      "[Epoch: 21220] train loss: 1.0788, train acc: 0.5580, val loss: 1.0848, val acc: 0.5494  (best train acc: 0.5628, best val acc: 0.5562, best train loss: 1.0736  @ epoch 20660 )\n",
      "[Epoch: 21240] train loss: 1.0766, train acc: 0.5541, val loss: 1.0867, val acc: 0.5501  (best train acc: 0.5628, best val acc: 0.5562, best train loss: 1.0736  @ epoch 20660 )\n",
      "[Epoch: 21260] train loss: 1.0852, train acc: 0.5536, val loss: 1.0861, val acc: 0.5518  (best train acc: 0.5628, best val acc: 0.5562, best train loss: 1.0736  @ epoch 20660 )\n",
      "[Epoch: 21280] train loss: 1.0931, train acc: 0.5452, val loss: 1.0856, val acc: 0.5521  (best train acc: 0.5628, best val acc: 0.5562, best train loss: 1.0736  @ epoch 20660 )\n",
      "[Epoch: 21300] train loss: 1.0880, train acc: 0.5519, val loss: 1.0835, val acc: 0.5508  (best train acc: 0.5628, best val acc: 0.5562, best train loss: 1.0736  @ epoch 20660 )\n",
      "[Epoch: 21320] train loss: 1.0953, train acc: 0.5445, val loss: 1.0904, val acc: 0.5494  (best train acc: 0.5628, best val acc: 0.5562, best train loss: 1.0736  @ epoch 20660 )\n",
      "[Epoch: 21340] train loss: 1.0956, train acc: 0.5453, val loss: 1.0835, val acc: 0.5555  (best train acc: 0.5628, best val acc: 0.5562, best train loss: 1.0736  @ epoch 20660 )\n",
      "[Epoch: 21360] train loss: 1.1042, train acc: 0.5405, val loss: 1.1070, val acc: 0.5427  (best train acc: 0.5628, best val acc: 0.5562, best train loss: 1.0736  @ epoch 20660 )\n",
      "[Epoch: 21380] train loss: 1.1010, train acc: 0.5448, val loss: 1.0862, val acc: 0.5474  (best train acc: 0.5628, best val acc: 0.5562, best train loss: 1.0736  @ epoch 20660 )\n",
      "[Epoch: 21400] train loss: 1.0864, train acc: 0.5479, val loss: 1.0849, val acc: 0.5531  (best train acc: 0.5628, best val acc: 0.5562, best train loss: 1.0736  @ epoch 20660 )\n",
      "[Epoch: 21420] train loss: 1.0945, train acc: 0.5492, val loss: 1.0841, val acc: 0.5508  (best train acc: 0.5628, best val acc: 0.5565, best train loss: 1.0736  @ epoch 20660 )\n",
      "[Epoch: 21440] train loss: 1.0828, train acc: 0.5541, val loss: 1.0839, val acc: 0.5538  (best train acc: 0.5628, best val acc: 0.5565, best train loss: 1.0736  @ epoch 20660 )\n",
      "[Epoch: 21460] train loss: 1.0948, train acc: 0.5438, val loss: 1.0844, val acc: 0.5484  (best train acc: 0.5628, best val acc: 0.5565, best train loss: 1.0736  @ epoch 20660 )\n",
      "[Epoch: 21480] train loss: 1.1002, train acc: 0.5495, val loss: 1.0870, val acc: 0.5508  (best train acc: 0.5628, best val acc: 0.5565, best train loss: 1.0736  @ epoch 20660 )\n",
      "[Epoch: 21500] train loss: 1.1126, train acc: 0.5402, val loss: 1.1047, val acc: 0.5420  (best train acc: 0.5628, best val acc: 0.5565, best train loss: 1.0695  @ epoch 21496 )\n",
      "[Epoch: 21520] train loss: 1.0967, train acc: 0.5508, val loss: 1.0939, val acc: 0.5504  (best train acc: 0.5628, best val acc: 0.5565, best train loss: 1.0695  @ epoch 21496 )\n",
      "[Epoch: 21540] train loss: 1.0832, train acc: 0.5541, val loss: 1.0985, val acc: 0.5460  (best train acc: 0.5628, best val acc: 0.5565, best train loss: 1.0695  @ epoch 21496 )\n",
      "[Epoch: 21560] train loss: 1.1132, train acc: 0.5396, val loss: 1.0873, val acc: 0.5487  (best train acc: 0.5628, best val acc: 0.5565, best train loss: 1.0695  @ epoch 21496 )\n",
      "[Epoch: 21580] train loss: 1.0930, train acc: 0.5515, val loss: 1.0876, val acc: 0.5511  (best train acc: 0.5628, best val acc: 0.5565, best train loss: 1.0695  @ epoch 21496 )\n",
      "[Epoch: 21600] train loss: 1.1070, train acc: 0.5385, val loss: 1.0860, val acc: 0.5524  (best train acc: 0.5628, best val acc: 0.5565, best train loss: 1.0695  @ epoch 21496 )\n",
      "[Epoch: 21620] train loss: 1.0950, train acc: 0.5517, val loss: 1.0896, val acc: 0.5460  (best train acc: 0.5628, best val acc: 0.5565, best train loss: 1.0695  @ epoch 21496 )\n",
      "[Epoch: 21640] train loss: 1.0942, train acc: 0.5497, val loss: 1.0928, val acc: 0.5467  (best train acc: 0.5628, best val acc: 0.5565, best train loss: 1.0695  @ epoch 21496 )\n",
      "[Epoch: 21660] train loss: 1.1026, train acc: 0.5460, val loss: 1.0888, val acc: 0.5545  (best train acc: 0.5628, best val acc: 0.5565, best train loss: 1.0695  @ epoch 21496 )\n",
      "[Epoch: 21680] train loss: 1.0780, train acc: 0.5535, val loss: 1.0835, val acc: 0.5535  (best train acc: 0.5628, best val acc: 0.5565, best train loss: 1.0695  @ epoch 21496 )\n",
      "[Epoch: 21700] train loss: 1.0974, train acc: 0.5453, val loss: 1.0839, val acc: 0.5524  (best train acc: 0.5628, best val acc: 0.5565, best train loss: 1.0695  @ epoch 21496 )\n",
      "[Epoch: 21720] train loss: 1.0852, train acc: 0.5481, val loss: 1.0846, val acc: 0.5524  (best train acc: 0.5628, best val acc: 0.5565, best train loss: 1.0695  @ epoch 21496 )\n",
      "[Epoch: 21740] train loss: 1.0911, train acc: 0.5489, val loss: 1.0823, val acc: 0.5524  (best train acc: 0.5628, best val acc: 0.5565, best train loss: 1.0695  @ epoch 21496 )\n",
      "[Epoch: 21760] train loss: 1.1077, train acc: 0.5357, val loss: 1.0874, val acc: 0.5454  (best train acc: 0.5628, best val acc: 0.5565, best train loss: 1.0695  @ epoch 21496 )\n",
      "[Epoch: 21780] train loss: 1.0945, train acc: 0.5440, val loss: 1.0865, val acc: 0.5508  (best train acc: 0.5628, best val acc: 0.5565, best train loss: 1.0695  @ epoch 21496 )\n",
      "[Epoch: 21800] train loss: 1.1018, train acc: 0.5448, val loss: 1.0831, val acc: 0.5508  (best train acc: 0.5628, best val acc: 0.5565, best train loss: 1.0695  @ epoch 21496 )\n",
      "[Epoch: 21820] train loss: 1.0800, train acc: 0.5532, val loss: 1.0835, val acc: 0.5524  (best train acc: 0.5628, best val acc: 0.5565, best train loss: 1.0695  @ epoch 21496 )\n",
      "[Epoch: 21840] train loss: 1.0810, train acc: 0.5571, val loss: 1.0828, val acc: 0.5491  (best train acc: 0.5628, best val acc: 0.5565, best train loss: 1.0680  @ epoch 21835 )\n",
      "[Epoch: 21860] train loss: 1.0871, train acc: 0.5510, val loss: 1.0823, val acc: 0.5514  (best train acc: 0.5628, best val acc: 0.5565, best train loss: 1.0680  @ epoch 21835 )\n",
      "[Epoch: 21880] train loss: 1.1009, train acc: 0.5398, val loss: 1.0818, val acc: 0.5477  (best train acc: 0.5628, best val acc: 0.5565, best train loss: 1.0680  @ epoch 21835 )\n",
      "[Epoch: 21900] train loss: 1.0766, train acc: 0.5560, val loss: 1.0810, val acc: 0.5548  (best train acc: 0.5628, best val acc: 0.5565, best train loss: 1.0680  @ epoch 21835 )\n",
      "[Epoch: 21920] train loss: 1.0846, train acc: 0.5547, val loss: 1.0837, val acc: 0.5518  (best train acc: 0.5628, best val acc: 0.5575, best train loss: 1.0680  @ epoch 21835 )\n",
      "[Epoch: 21940] train loss: 1.0829, train acc: 0.5517, val loss: 1.0814, val acc: 0.5558  (best train acc: 0.5652, best val acc: 0.5575, best train loss: 1.0680  @ epoch 21835 )\n",
      "[Epoch: 21960] train loss: 1.0823, train acc: 0.5523, val loss: 1.0807, val acc: 0.5545  (best train acc: 0.5652, best val acc: 0.5575, best train loss: 1.0680  @ epoch 21835 )\n",
      "[Epoch: 21980] train loss: 1.0835, train acc: 0.5513, val loss: 1.0792, val acc: 0.5531  (best train acc: 0.5652, best val acc: 0.5575, best train loss: 1.0680  @ epoch 21835 )\n",
      "[Epoch: 22000] train loss: 1.0908, train acc: 0.5476, val loss: 1.0841, val acc: 0.5524  (best train acc: 0.5652, best val acc: 0.5575, best train loss: 1.0680  @ epoch 21835 )\n",
      "[Epoch: 22020] train loss: 1.0771, train acc: 0.5508, val loss: 1.0788, val acc: 0.5481  (best train acc: 0.5652, best val acc: 0.5595, best train loss: 1.0680  @ epoch 21835 )\n",
      "[Epoch: 22040] train loss: 1.0800, train acc: 0.5544, val loss: 1.0810, val acc: 0.5548  (best train acc: 0.5652, best val acc: 0.5595, best train loss: 1.0680  @ epoch 21835 )\n",
      "[Epoch: 22060] train loss: 1.0935, train acc: 0.5447, val loss: 1.0782, val acc: 0.5511  (best train acc: 0.5652, best val acc: 0.5595, best train loss: 1.0674  @ epoch 22045 )\n",
      "[Epoch: 22080] train loss: 1.0926, train acc: 0.5499, val loss: 1.0805, val acc: 0.5551  (best train acc: 0.5652, best val acc: 0.5595, best train loss: 1.0613  @ epoch 22077 )\n",
      "[Epoch: 22100] train loss: 1.0870, train acc: 0.5515, val loss: 1.0817, val acc: 0.5521  (best train acc: 0.5652, best val acc: 0.5595, best train loss: 1.0613  @ epoch 22077 )\n",
      "[Epoch: 22120] train loss: 1.0798, train acc: 0.5555, val loss: 1.0822, val acc: 0.5531  (best train acc: 0.5652, best val acc: 0.5595, best train loss: 1.0613  @ epoch 22077 )\n",
      "[Epoch: 22140] train loss: 1.0799, train acc: 0.5556, val loss: 1.0775, val acc: 0.5578  (best train acc: 0.5652, best val acc: 0.5595, best train loss: 1.0613  @ epoch 22077 )\n",
      "[Epoch: 22160] train loss: 1.0750, train acc: 0.5591, val loss: 1.0783, val acc: 0.5545  (best train acc: 0.5652, best val acc: 0.5595, best train loss: 1.0613  @ epoch 22077 )\n",
      "[Epoch: 22180] train loss: 1.0849, train acc: 0.5494, val loss: 1.0772, val acc: 0.5558  (best train acc: 0.5652, best val acc: 0.5595, best train loss: 1.0613  @ epoch 22077 )\n",
      "[Epoch: 22200] train loss: 1.0817, train acc: 0.5543, val loss: 1.0769, val acc: 0.5578  (best train acc: 0.5652, best val acc: 0.5595, best train loss: 1.0613  @ epoch 22077 )\n",
      "[Epoch: 22220] train loss: 1.0840, train acc: 0.5476, val loss: 1.0788, val acc: 0.5508  (best train acc: 0.5652, best val acc: 0.5595, best train loss: 1.0613  @ epoch 22077 )\n",
      "[Epoch: 22240] train loss: 1.1000, train acc: 0.5465, val loss: 1.0821, val acc: 0.5501  (best train acc: 0.5652, best val acc: 0.5595, best train loss: 1.0613  @ epoch 22077 )\n",
      "[Epoch: 22260] train loss: 1.0831, train acc: 0.5541, val loss: 1.0807, val acc: 0.5535  (best train acc: 0.5652, best val acc: 0.5595, best train loss: 1.0613  @ epoch 22077 )\n",
      "[Epoch: 22280] train loss: 1.0664, train acc: 0.5558, val loss: 1.0788, val acc: 0.5528  (best train acc: 0.5675, best val acc: 0.5595, best train loss: 1.0597  @ epoch 22274 )\n",
      "[Epoch: 22300] train loss: 1.1660, train acc: 0.5220, val loss: 1.1538, val acc: 0.5228  (best train acc: 0.5675, best val acc: 0.5595, best train loss: 1.0597  @ epoch 22274 )\n",
      "[Epoch: 22320] train loss: 1.1056, train acc: 0.5437, val loss: 1.0907, val acc: 0.5417  (best train acc: 0.5675, best val acc: 0.5595, best train loss: 1.0597  @ epoch 22274 )\n",
      "[Epoch: 22340] train loss: 1.0802, train acc: 0.5537, val loss: 1.0799, val acc: 0.5508  (best train acc: 0.5675, best val acc: 0.5595, best train loss: 1.0597  @ epoch 22274 )\n",
      "[Epoch: 22360] train loss: 1.1053, train acc: 0.5410, val loss: 1.0785, val acc: 0.5508  (best train acc: 0.5675, best val acc: 0.5595, best train loss: 1.0597  @ epoch 22274 )\n",
      "[Epoch: 22380] train loss: 1.0821, train acc: 0.5485, val loss: 1.0782, val acc: 0.5511  (best train acc: 0.5675, best val acc: 0.5595, best train loss: 1.0597  @ epoch 22274 )\n",
      "[Epoch: 22400] train loss: 1.0788, train acc: 0.5591, val loss: 1.0757, val acc: 0.5562  (best train acc: 0.5675, best val acc: 0.5595, best train loss: 1.0597  @ epoch 22274 )\n",
      "[Epoch: 22420] train loss: 1.0931, train acc: 0.5476, val loss: 1.0922, val acc: 0.5437  (best train acc: 0.5675, best val acc: 0.5595, best train loss: 1.0597  @ epoch 22274 )\n",
      "[Epoch: 22440] train loss: 1.0792, train acc: 0.5505, val loss: 1.0779, val acc: 0.5555  (best train acc: 0.5675, best val acc: 0.5595, best train loss: 1.0597  @ epoch 22274 )\n",
      "[Epoch: 22460] train loss: 1.0870, train acc: 0.5532, val loss: 1.0754, val acc: 0.5551  (best train acc: 0.5675, best val acc: 0.5595, best train loss: 1.0597  @ epoch 22274 )\n",
      "[Epoch: 22480] train loss: 1.0854, train acc: 0.5512, val loss: 1.0739, val acc: 0.5538  (best train acc: 0.5675, best val acc: 0.5595, best train loss: 1.0597  @ epoch 22274 )\n",
      "[Epoch: 22500] train loss: 1.0811, train acc: 0.5514, val loss: 1.0747, val acc: 0.5582  (best train acc: 0.5675, best val acc: 0.5595, best train loss: 1.0597  @ epoch 22274 )\n",
      "[Epoch: 22520] train loss: 1.0813, train acc: 0.5484, val loss: 1.0998, val acc: 0.5497  (best train acc: 0.5675, best val acc: 0.5595, best train loss: 1.0597  @ epoch 22274 )\n",
      "[Epoch: 22540] train loss: 1.0863, train acc: 0.5422, val loss: 1.0762, val acc: 0.5555  (best train acc: 0.5675, best val acc: 0.5595, best train loss: 1.0591  @ epoch 22537 )\n",
      "[Epoch: 22560] train loss: 1.0842, train acc: 0.5480, val loss: 1.0744, val acc: 0.5545  (best train acc: 0.5675, best val acc: 0.5595, best train loss: 1.0591  @ epoch 22537 )\n",
      "[Epoch: 22580] train loss: 1.0933, train acc: 0.5446, val loss: 1.0752, val acc: 0.5555  (best train acc: 0.5675, best val acc: 0.5599, best train loss: 1.0591  @ epoch 22537 )\n",
      "[Epoch: 22600] train loss: 1.0877, train acc: 0.5505, val loss: 1.0726, val acc: 0.5524  (best train acc: 0.5675, best val acc: 0.5599, best train loss: 1.0591  @ epoch 22537 )\n",
      "[Epoch: 22620] train loss: 1.0867, train acc: 0.5468, val loss: 1.0726, val acc: 0.5545  (best train acc: 0.5675, best val acc: 0.5599, best train loss: 1.0544  @ epoch 22607 )\n",
      "[Epoch: 22640] train loss: 1.0679, train acc: 0.5565, val loss: 1.0718, val acc: 0.5538  (best train acc: 0.5675, best val acc: 0.5599, best train loss: 1.0544  @ epoch 22607 )\n",
      "[Epoch: 22660] train loss: 1.0717, train acc: 0.5552, val loss: 1.0713, val acc: 0.5555  (best train acc: 0.5675, best val acc: 0.5605, best train loss: 1.0544  @ epoch 22607 )\n",
      "[Epoch: 22680] train loss: 1.0702, train acc: 0.5561, val loss: 1.0732, val acc: 0.5545  (best train acc: 0.5675, best val acc: 0.5605, best train loss: 1.0544  @ epoch 22607 )\n",
      "[Epoch: 22700] train loss: 1.0571, train acc: 0.5655, val loss: 1.0713, val acc: 0.5518  (best train acc: 0.5675, best val acc: 0.5605, best train loss: 1.0544  @ epoch 22607 )\n",
      "[Epoch: 22720] train loss: 1.0838, train acc: 0.5560, val loss: 1.0721, val acc: 0.5551  (best train acc: 0.5675, best val acc: 0.5605, best train loss: 1.0544  @ epoch 22607 )\n",
      "[Epoch: 22740] train loss: 1.0850, train acc: 0.5505, val loss: 1.0765, val acc: 0.5582  (best train acc: 0.5675, best val acc: 0.5605, best train loss: 1.0544  @ epoch 22607 )\n",
      "[Epoch: 22760] train loss: 1.1268, train acc: 0.5338, val loss: 1.0985, val acc: 0.5460  (best train acc: 0.5675, best val acc: 0.5609, best train loss: 1.0544  @ epoch 22607 )\n",
      "[Epoch: 22780] train loss: 1.0709, train acc: 0.5559, val loss: 1.0780, val acc: 0.5572  (best train acc: 0.5675, best val acc: 0.5609, best train loss: 1.0544  @ epoch 22607 )\n",
      "[Epoch: 22800] train loss: 1.0826, train acc: 0.5438, val loss: 1.0849, val acc: 0.5531  (best train acc: 0.5675, best val acc: 0.5609, best train loss: 1.0544  @ epoch 22607 )\n",
      "[Epoch: 22820] train loss: 1.0757, train acc: 0.5552, val loss: 1.0750, val acc: 0.5555  (best train acc: 0.5675, best val acc: 0.5609, best train loss: 1.0544  @ epoch 22607 )\n",
      "[Epoch: 22840] train loss: 1.1376, train acc: 0.5322, val loss: 1.1295, val acc: 0.5197  (best train acc: 0.5675, best val acc: 0.5609, best train loss: 1.0544  @ epoch 22607 )\n",
      "[Epoch: 22860] train loss: 1.0867, train acc: 0.5487, val loss: 1.0749, val acc: 0.5558  (best train acc: 0.5675, best val acc: 0.5609, best train loss: 1.0544  @ epoch 22607 )\n",
      "[Epoch: 22880] train loss: 1.0925, train acc: 0.5486, val loss: 1.0732, val acc: 0.5501  (best train acc: 0.5675, best val acc: 0.5609, best train loss: 1.0544  @ epoch 22607 )\n",
      "[Epoch: 22900] train loss: 1.0974, train acc: 0.5474, val loss: 1.0922, val acc: 0.5494  (best train acc: 0.5675, best val acc: 0.5609, best train loss: 1.0544  @ epoch 22607 )\n",
      "[Epoch: 22920] train loss: 1.1167, train acc: 0.5362, val loss: 1.1058, val acc: 0.5379  (best train acc: 0.5675, best val acc: 0.5609, best train loss: 1.0544  @ epoch 22607 )\n",
      "[Epoch: 22940] train loss: 1.0759, train acc: 0.5533, val loss: 1.0800, val acc: 0.5514  (best train acc: 0.5675, best val acc: 0.5609, best train loss: 1.0544  @ epoch 22607 )\n",
      "[Epoch: 22960] train loss: 1.0939, train acc: 0.5395, val loss: 1.0739, val acc: 0.5511  (best train acc: 0.5675, best val acc: 0.5609, best train loss: 1.0544  @ epoch 22607 )\n",
      "[Epoch: 22980] train loss: 1.0801, train acc: 0.5522, val loss: 1.0836, val acc: 0.5548  (best train acc: 0.5675, best val acc: 0.5609, best train loss: 1.0544  @ epoch 22607 )\n",
      "[Epoch: 23000] train loss: 1.0833, train acc: 0.5553, val loss: 1.0787, val acc: 0.5568  (best train acc: 0.5675, best val acc: 0.5609, best train loss: 1.0544  @ epoch 22607 )\n",
      "[Epoch: 23020] train loss: 1.0737, train acc: 0.5510, val loss: 1.0739, val acc: 0.5555  (best train acc: 0.5675, best val acc: 0.5609, best train loss: 1.0544  @ epoch 22607 )\n",
      "[Epoch: 23040] train loss: 1.0825, train acc: 0.5471, val loss: 1.0721, val acc: 0.5558  (best train acc: 0.5675, best val acc: 0.5609, best train loss: 1.0544  @ epoch 22607 )\n",
      "[Epoch: 23060] train loss: 1.0834, train acc: 0.5511, val loss: 1.0755, val acc: 0.5531  (best train acc: 0.5675, best val acc: 0.5609, best train loss: 1.0544  @ epoch 22607 )\n",
      "[Epoch: 23080] train loss: 1.0964, train acc: 0.5449, val loss: 1.1052, val acc: 0.5447  (best train acc: 0.5675, best val acc: 0.5609, best train loss: 1.0544  @ epoch 22607 )\n",
      "[Epoch: 23100] train loss: 1.0724, train acc: 0.5591, val loss: 1.0757, val acc: 0.5541  (best train acc: 0.5675, best val acc: 0.5612, best train loss: 1.0544  @ epoch 22607 )\n",
      "[Epoch: 23120] train loss: 1.0696, train acc: 0.5568, val loss: 1.0751, val acc: 0.5568  (best train acc: 0.5675, best val acc: 0.5612, best train loss: 1.0544  @ epoch 22607 )\n",
      "[Epoch: 23140] train loss: 1.0977, train acc: 0.5471, val loss: 1.0765, val acc: 0.5578  (best train acc: 0.5675, best val acc: 0.5612, best train loss: 1.0544  @ epoch 22607 )\n",
      "[Epoch: 23160] train loss: 1.0736, train acc: 0.5502, val loss: 1.0783, val acc: 0.5538  (best train acc: 0.5675, best val acc: 0.5612, best train loss: 1.0544  @ epoch 22607 )\n",
      "[Epoch: 23180] train loss: 1.1149, train acc: 0.5355, val loss: 1.1188, val acc: 0.5379  (best train acc: 0.5675, best val acc: 0.5612, best train loss: 1.0544  @ epoch 22607 )\n",
      "[Epoch: 23200] train loss: 1.0853, train acc: 0.5502, val loss: 1.0730, val acc: 0.5562  (best train acc: 0.5675, best val acc: 0.5612, best train loss: 1.0544  @ epoch 22607 )\n",
      "[Epoch: 23220] train loss: 1.0766, train acc: 0.5520, val loss: 1.0707, val acc: 0.5558  (best train acc: 0.5675, best val acc: 0.5616, best train loss: 1.0544  @ epoch 22607 )\n",
      "[Epoch: 23240] train loss: 1.0845, train acc: 0.5462, val loss: 1.0757, val acc: 0.5565  (best train acc: 0.5675, best val acc: 0.5616, best train loss: 1.0544  @ epoch 22607 )\n",
      "[Epoch: 23260] train loss: 1.0778, train acc: 0.5536, val loss: 1.0733, val acc: 0.5578  (best train acc: 0.5675, best val acc: 0.5616, best train loss: 1.0544  @ epoch 22607 )\n",
      "[Epoch: 23280] train loss: 1.0676, train acc: 0.5588, val loss: 1.0692, val acc: 0.5545  (best train acc: 0.5675, best val acc: 0.5616, best train loss: 1.0542  @ epoch 23279 )\n",
      "[Epoch: 23300] train loss: 1.0844, train acc: 0.5451, val loss: 1.0695, val acc: 0.5565  (best train acc: 0.5675, best val acc: 0.5616, best train loss: 1.0542  @ epoch 23279 )\n",
      "[Epoch: 23320] train loss: 1.0761, train acc: 0.5578, val loss: 1.0712, val acc: 0.5599  (best train acc: 0.5675, best val acc: 0.5616, best train loss: 1.0542  @ epoch 23279 )\n",
      "[Epoch: 23340] train loss: 1.0679, train acc: 0.5535, val loss: 1.0685, val acc: 0.5555  (best train acc: 0.5675, best val acc: 0.5616, best train loss: 1.0542  @ epoch 23279 )\n",
      "[Epoch: 23360] train loss: 1.0885, train acc: 0.5479, val loss: 1.0872, val acc: 0.5538  (best train acc: 0.5675, best val acc: 0.5616, best train loss: 1.0542  @ epoch 23279 )\n",
      "[Epoch: 23380] train loss: 1.0967, train acc: 0.5523, val loss: 1.0788, val acc: 0.5494  (best train acc: 0.5675, best val acc: 0.5616, best train loss: 1.0542  @ epoch 23279 )\n",
      "[Epoch: 23400] train loss: 1.0655, train acc: 0.5615, val loss: 1.0724, val acc: 0.5599  (best train acc: 0.5675, best val acc: 0.5616, best train loss: 1.0542  @ epoch 23279 )\n",
      "[Epoch: 23420] train loss: 1.0735, train acc: 0.5500, val loss: 1.0677, val acc: 0.5568  (best train acc: 0.5675, best val acc: 0.5616, best train loss: 1.0542  @ epoch 23279 )\n",
      "[Epoch: 23440] train loss: 1.0664, train acc: 0.5600, val loss: 1.0689, val acc: 0.5551  (best train acc: 0.5675, best val acc: 0.5616, best train loss: 1.0542  @ epoch 23279 )\n",
      "[Epoch: 23460] train loss: 1.0713, train acc: 0.5534, val loss: 1.0693, val acc: 0.5572  (best train acc: 0.5675, best val acc: 0.5616, best train loss: 1.0542  @ epoch 23279 )\n",
      "[Epoch: 23480] train loss: 1.0889, train acc: 0.5461, val loss: 1.0784, val acc: 0.5501  (best train acc: 0.5675, best val acc: 0.5616, best train loss: 1.0542  @ epoch 23279 )\n",
      "[Epoch: 23500] train loss: 1.0723, train acc: 0.5506, val loss: 1.0706, val acc: 0.5572  (best train acc: 0.5675, best val acc: 0.5616, best train loss: 1.0542  @ epoch 23279 )\n",
      "[Epoch: 23520] train loss: 1.0832, train acc: 0.5416, val loss: 1.0672, val acc: 0.5535  (best train acc: 0.5675, best val acc: 0.5622, best train loss: 1.0542  @ epoch 23279 )\n",
      "[Epoch: 23540] train loss: 1.0588, train acc: 0.5591, val loss: 1.0657, val acc: 0.5562  (best train acc: 0.5675, best val acc: 0.5622, best train loss: 1.0542  @ epoch 23279 )\n",
      "[Epoch: 23560] train loss: 1.0652, train acc: 0.5592, val loss: 1.0723, val acc: 0.5592  (best train acc: 0.5675, best val acc: 0.5622, best train loss: 1.0542  @ epoch 23279 )\n",
      "[Epoch: 23580] train loss: 1.0696, train acc: 0.5544, val loss: 1.0709, val acc: 0.5605  (best train acc: 0.5675, best val acc: 0.5622, best train loss: 1.0542  @ epoch 23279 )\n",
      "[Epoch: 23600] train loss: 1.0987, train acc: 0.5429, val loss: 1.0691, val acc: 0.5609  (best train acc: 0.5675, best val acc: 0.5622, best train loss: 1.0534  @ epoch 23596 )\n",
      "[Epoch: 23620] train loss: 1.0642, train acc: 0.5586, val loss: 1.0675, val acc: 0.5605  (best train acc: 0.5675, best val acc: 0.5622, best train loss: 1.0534  @ epoch 23596 )\n",
      "[Epoch: 23640] train loss: 1.0663, train acc: 0.5588, val loss: 1.0669, val acc: 0.5589  (best train acc: 0.5675, best val acc: 0.5622, best train loss: 1.0534  @ epoch 23596 )\n",
      "[Epoch: 23660] train loss: 1.0656, train acc: 0.5544, val loss: 1.0633, val acc: 0.5575  (best train acc: 0.5675, best val acc: 0.5626, best train loss: 1.0534  @ epoch 23596 )\n",
      "[Epoch: 23680] train loss: 1.0641, train acc: 0.5556, val loss: 1.0635, val acc: 0.5616  (best train acc: 0.5675, best val acc: 0.5626, best train loss: 1.0534  @ epoch 23596 )\n",
      "[Epoch: 23700] train loss: 1.0932, train acc: 0.5424, val loss: 1.0608, val acc: 0.5575  (best train acc: 0.5675, best val acc: 0.5626, best train loss: 1.0528  @ epoch 23688 )\n",
      "[Epoch: 23720] train loss: 1.0686, train acc: 0.5526, val loss: 1.0668, val acc: 0.5589  (best train acc: 0.5675, best val acc: 0.5626, best train loss: 1.0528  @ epoch 23688 )\n",
      "[Epoch: 23740] train loss: 1.0670, train acc: 0.5557, val loss: 1.0678, val acc: 0.5629  (best train acc: 0.5675, best val acc: 0.5629, best train loss: 1.0528  @ epoch 23688 )\n",
      "[Epoch: 23760] train loss: 1.0638, train acc: 0.5541, val loss: 1.0632, val acc: 0.5605  (best train acc: 0.5675, best val acc: 0.5629, best train loss: 1.0528  @ epoch 23688 )\n",
      "[Epoch: 23780] train loss: 1.0724, train acc: 0.5547, val loss: 1.0692, val acc: 0.5568  (best train acc: 0.5675, best val acc: 0.5639, best train loss: 1.0528  @ epoch 23688 )\n",
      "[Epoch: 23800] train loss: 1.0661, train acc: 0.5549, val loss: 1.0619, val acc: 0.5592  (best train acc: 0.5675, best val acc: 0.5639, best train loss: 1.0528  @ epoch 23688 )\n",
      "[Epoch: 23820] train loss: 1.0620, train acc: 0.5591, val loss: 1.0607, val acc: 0.5595  (best train acc: 0.5675, best val acc: 0.5639, best train loss: 1.0528  @ epoch 23688 )\n",
      "[Epoch: 23840] train loss: 1.0837, train acc: 0.5494, val loss: 1.0651, val acc: 0.5572  (best train acc: 0.5675, best val acc: 0.5639, best train loss: 1.0528  @ epoch 23688 )\n",
      "[Epoch: 23860] train loss: 1.0862, train acc: 0.5421, val loss: 1.0731, val acc: 0.5558  (best train acc: 0.5678, best val acc: 0.5639, best train loss: 1.0528  @ epoch 23688 )\n",
      "[Epoch: 23880] train loss: 1.1033, train acc: 0.5453, val loss: 1.0751, val acc: 0.5582  (best train acc: 0.5678, best val acc: 0.5639, best train loss: 1.0528  @ epoch 23688 )\n",
      "[Epoch: 23900] train loss: 1.0820, train acc: 0.5495, val loss: 1.0707, val acc: 0.5551  (best train acc: 0.5678, best val acc: 0.5639, best train loss: 1.0528  @ epoch 23688 )\n",
      "[Epoch: 23920] train loss: 1.0779, train acc: 0.5496, val loss: 1.0701, val acc: 0.5578  (best train acc: 0.5678, best val acc: 0.5639, best train loss: 1.0528  @ epoch 23688 )\n",
      "[Epoch: 23940] train loss: 1.0782, train acc: 0.5571, val loss: 1.0645, val acc: 0.5592  (best train acc: 0.5678, best val acc: 0.5639, best train loss: 1.0517  @ epoch 23930 )\n",
      "[Epoch: 23960] train loss: 1.0658, train acc: 0.5531, val loss: 1.0611, val acc: 0.5605  (best train acc: 0.5693, best val acc: 0.5656, best train loss: 1.0447  @ epoch 23955 )\n",
      "[Epoch: 23980] train loss: 1.0696, train acc: 0.5481, val loss: 1.0663, val acc: 0.5592  (best train acc: 0.5693, best val acc: 0.5656, best train loss: 1.0447  @ epoch 23955 )\n",
      "[Epoch: 24000] train loss: 1.0616, train acc: 0.5564, val loss: 1.0582, val acc: 0.5629  (best train acc: 0.5693, best val acc: 0.5656, best train loss: 1.0447  @ epoch 23955 )\n",
      "[Epoch: 24020] train loss: 1.0632, train acc: 0.5599, val loss: 1.0597, val acc: 0.5619  (best train acc: 0.5693, best val acc: 0.5656, best train loss: 1.0447  @ epoch 23955 )\n",
      "[Epoch: 24040] train loss: 1.0698, train acc: 0.5546, val loss: 1.0581, val acc: 0.5629  (best train acc: 0.5693, best val acc: 0.5656, best train loss: 1.0447  @ epoch 23955 )\n",
      "[Epoch: 24060] train loss: 1.0625, train acc: 0.5632, val loss: 1.0714, val acc: 0.5592  (best train acc: 0.5693, best val acc: 0.5659, best train loss: 1.0447  @ epoch 23955 )\n",
      "[Epoch: 24080] train loss: 1.0810, train acc: 0.5552, val loss: 1.0819, val acc: 0.5565  (best train acc: 0.5693, best val acc: 0.5659, best train loss: 1.0447  @ epoch 23955 )\n",
      "[Epoch: 24100] train loss: 1.0665, train acc: 0.5581, val loss: 1.0729, val acc: 0.5582  (best train acc: 0.5693, best val acc: 0.5659, best train loss: 1.0447  @ epoch 23955 )\n",
      "[Epoch: 24120] train loss: 1.0772, train acc: 0.5620, val loss: 1.0666, val acc: 0.5609  (best train acc: 0.5693, best val acc: 0.5659, best train loss: 1.0447  @ epoch 23955 )\n",
      "[Epoch: 24140] train loss: 1.0736, train acc: 0.5546, val loss: 1.0706, val acc: 0.5558  (best train acc: 0.5693, best val acc: 0.5659, best train loss: 1.0447  @ epoch 23955 )\n",
      "[Epoch: 24160] train loss: 1.0811, train acc: 0.5460, val loss: 1.0648, val acc: 0.5609  (best train acc: 0.5693, best val acc: 0.5659, best train loss: 1.0447  @ epoch 23955 )\n",
      "[Epoch: 24180] train loss: 1.0832, train acc: 0.5481, val loss: 1.0656, val acc: 0.5585  (best train acc: 0.5693, best val acc: 0.5659, best train loss: 1.0447  @ epoch 23955 )\n",
      "[Epoch: 24200] train loss: 1.0807, train acc: 0.5479, val loss: 1.0680, val acc: 0.5602  (best train acc: 0.5693, best val acc: 0.5659, best train loss: 1.0447  @ epoch 23955 )\n",
      "[Epoch: 24220] train loss: 1.0828, train acc: 0.5552, val loss: 1.0696, val acc: 0.5538  (best train acc: 0.5693, best val acc: 0.5659, best train loss: 1.0447  @ epoch 23955 )\n",
      "[Epoch: 24240] train loss: 1.0805, train acc: 0.5522, val loss: 1.0702, val acc: 0.5595  (best train acc: 0.5693, best val acc: 0.5659, best train loss: 1.0447  @ epoch 23955 )\n",
      "[Epoch: 24260] train loss: 1.0842, train acc: 0.5492, val loss: 1.0671, val acc: 0.5565  (best train acc: 0.5693, best val acc: 0.5659, best train loss: 1.0447  @ epoch 23955 )\n",
      "[Epoch: 24280] train loss: 1.0940, train acc: 0.5476, val loss: 1.0663, val acc: 0.5558  (best train acc: 0.5693, best val acc: 0.5659, best train loss: 1.0447  @ epoch 23955 )\n",
      "[Epoch: 24300] train loss: 1.0749, train acc: 0.5566, val loss: 1.0633, val acc: 0.5609  (best train acc: 0.5693, best val acc: 0.5659, best train loss: 1.0447  @ epoch 23955 )\n",
      "[Epoch: 24320] train loss: 1.0553, train acc: 0.5578, val loss: 1.0622, val acc: 0.5642  (best train acc: 0.5693, best val acc: 0.5659, best train loss: 1.0447  @ epoch 23955 )\n",
      "[Epoch: 24340] train loss: 1.0793, train acc: 0.5521, val loss: 1.0655, val acc: 0.5589  (best train acc: 0.5693, best val acc: 0.5659, best train loss: 1.0447  @ epoch 23955 )\n",
      "[Epoch: 24360] train loss: 1.0620, train acc: 0.5606, val loss: 1.0667, val acc: 0.5632  (best train acc: 0.5693, best val acc: 0.5659, best train loss: 1.0447  @ epoch 23955 )\n",
      "[Epoch: 24380] train loss: 1.0748, train acc: 0.5607, val loss: 1.0609, val acc: 0.5619  (best train acc: 0.5693, best val acc: 0.5659, best train loss: 1.0447  @ epoch 23955 )\n",
      "[Epoch: 24400] train loss: 1.0857, train acc: 0.5474, val loss: 1.0653, val acc: 0.5585  (best train acc: 0.5693, best val acc: 0.5659, best train loss: 1.0447  @ epoch 23955 )\n",
      "[Epoch: 24420] train loss: 1.0614, train acc: 0.5622, val loss: 1.0611, val acc: 0.5612  (best train acc: 0.5693, best val acc: 0.5659, best train loss: 1.0447  @ epoch 23955 )\n",
      "[Epoch: 24440] train loss: 1.0701, train acc: 0.5524, val loss: 1.0648, val acc: 0.5619  (best train acc: 0.5693, best val acc: 0.5659, best train loss: 1.0447  @ epoch 23955 )\n",
      "[Epoch: 24460] train loss: 1.0747, train acc: 0.5522, val loss: 1.0675, val acc: 0.5589  (best train acc: 0.5693, best val acc: 0.5659, best train loss: 1.0447  @ epoch 23955 )\n",
      "[Epoch: 24480] train loss: 1.0754, train acc: 0.5486, val loss: 1.0647, val acc: 0.5582  (best train acc: 0.5693, best val acc: 0.5659, best train loss: 1.0447  @ epoch 23955 )\n",
      "[Epoch: 24500] train loss: 1.0860, train acc: 0.5523, val loss: 1.0648, val acc: 0.5609  (best train acc: 0.5693, best val acc: 0.5659, best train loss: 1.0447  @ epoch 23955 )\n",
      "[Epoch: 24520] train loss: 1.0739, train acc: 0.5528, val loss: 1.0631, val acc: 0.5629  (best train acc: 0.5693, best val acc: 0.5659, best train loss: 1.0447  @ epoch 23955 )\n",
      "[Epoch: 24540] train loss: 1.0674, train acc: 0.5567, val loss: 1.0669, val acc: 0.5592  (best train acc: 0.5693, best val acc: 0.5673, best train loss: 1.0447  @ epoch 23955 )\n",
      "[Epoch: 24560] train loss: 1.0728, train acc: 0.5518, val loss: 1.0619, val acc: 0.5656  (best train acc: 0.5693, best val acc: 0.5673, best train loss: 1.0447  @ epoch 23955 )\n",
      "[Epoch: 24580] train loss: 1.0780, train acc: 0.5523, val loss: 1.0596, val acc: 0.5612  (best train acc: 0.5693, best val acc: 0.5673, best train loss: 1.0447  @ epoch 23955 )\n",
      "[Epoch: 24600] train loss: 1.0725, train acc: 0.5547, val loss: 1.0604, val acc: 0.5609  (best train acc: 0.5693, best val acc: 0.5673, best train loss: 1.0447  @ epoch 23955 )\n",
      "[Epoch: 24620] train loss: 1.0845, train acc: 0.5454, val loss: 1.0662, val acc: 0.5602  (best train acc: 0.5693, best val acc: 0.5673, best train loss: 1.0447  @ epoch 23955 )\n",
      "[Epoch: 24640] train loss: 1.0791, train acc: 0.5549, val loss: 1.0590, val acc: 0.5629  (best train acc: 0.5693, best val acc: 0.5673, best train loss: 1.0447  @ epoch 23955 )\n",
      "[Epoch: 24660] train loss: 1.0698, train acc: 0.5612, val loss: 1.0593, val acc: 0.5609  (best train acc: 0.5693, best val acc: 0.5673, best train loss: 1.0447  @ epoch 23955 )\n",
      "[Epoch: 24680] train loss: 1.0644, train acc: 0.5561, val loss: 1.0674, val acc: 0.5595  (best train acc: 0.5693, best val acc: 0.5676, best train loss: 1.0447  @ epoch 23955 )\n",
      "[Epoch: 24700] train loss: 1.0590, train acc: 0.5665, val loss: 1.0614, val acc: 0.5632  (best train acc: 0.5693, best val acc: 0.5676, best train loss: 1.0447  @ epoch 23955 )\n",
      "[Epoch: 24720] train loss: 1.0722, train acc: 0.5597, val loss: 1.0660, val acc: 0.5582  (best train acc: 0.5693, best val acc: 0.5676, best train loss: 1.0447  @ epoch 23955 )\n",
      "[Epoch: 24740] train loss: 1.0619, train acc: 0.5578, val loss: 1.0683, val acc: 0.5599  (best train acc: 0.5693, best val acc: 0.5676, best train loss: 1.0447  @ epoch 23955 )\n",
      "[Epoch: 24760] train loss: 1.0588, train acc: 0.5636, val loss: 1.0643, val acc: 0.5626  (best train acc: 0.5693, best val acc: 0.5676, best train loss: 1.0447  @ epoch 23955 )\n",
      "[Epoch: 24780] train loss: 1.0738, train acc: 0.5468, val loss: 1.0650, val acc: 0.5646  (best train acc: 0.5693, best val acc: 0.5676, best train loss: 1.0447  @ epoch 23955 )\n",
      "[Epoch: 24800] train loss: 1.0784, train acc: 0.5466, val loss: 1.0627, val acc: 0.5659  (best train acc: 0.5693, best val acc: 0.5676, best train loss: 1.0447  @ epoch 23955 )\n",
      "[Epoch: 24820] train loss: 1.0709, train acc: 0.5536, val loss: 1.0646, val acc: 0.5592  (best train acc: 0.5697, best val acc: 0.5676, best train loss: 1.0447  @ epoch 23955 )\n",
      "[Epoch: 24840] train loss: 1.0592, train acc: 0.5599, val loss: 1.0603, val acc: 0.5659  (best train acc: 0.5697, best val acc: 0.5676, best train loss: 1.0447  @ epoch 23955 )\n",
      "[Epoch: 24860] train loss: 1.0650, train acc: 0.5600, val loss: 1.0597, val acc: 0.5659  (best train acc: 0.5720, best val acc: 0.5690, best train loss: 1.0447  @ epoch 23955 )\n",
      "[Epoch: 24880] train loss: 1.0840, train acc: 0.5452, val loss: 1.0565, val acc: 0.5666  (best train acc: 0.5720, best val acc: 0.5690, best train loss: 1.0447  @ epoch 23955 )\n",
      "[Epoch: 24900] train loss: 1.0582, train acc: 0.5634, val loss: 1.0548, val acc: 0.5683  (best train acc: 0.5720, best val acc: 0.5690, best train loss: 1.0447  @ epoch 23955 )\n",
      "[Epoch: 24920] train loss: 1.0779, train acc: 0.5503, val loss: 1.0585, val acc: 0.5639  (best train acc: 0.5720, best val acc: 0.5690, best train loss: 1.0447  @ epoch 23955 )\n",
      "[Epoch: 24940] train loss: 1.0534, train acc: 0.5615, val loss: 1.0529, val acc: 0.5629  (best train acc: 0.5720, best val acc: 0.5690, best train loss: 1.0447  @ epoch 23955 )\n",
      "[Epoch: 24960] train loss: 1.0831, train acc: 0.5547, val loss: 1.0574, val acc: 0.5669  (best train acc: 0.5720, best val acc: 0.5710, best train loss: 1.0447  @ epoch 23955 )\n",
      "[Epoch: 24980] train loss: 1.0680, train acc: 0.5568, val loss: 1.0550, val acc: 0.5693  (best train acc: 0.5720, best val acc: 0.5710, best train loss: 1.0447  @ epoch 23955 )\n",
      "[Epoch: 25000] train loss: 1.0634, train acc: 0.5575, val loss: 1.0588, val acc: 0.5690  (best train acc: 0.5720, best val acc: 0.5710, best train loss: 1.0447  @ epoch 23955 )\n",
      "[Epoch: 25020] train loss: 1.0727, train acc: 0.5507, val loss: 1.0558, val acc: 0.5626  (best train acc: 0.5720, best val acc: 0.5710, best train loss: 1.0447  @ epoch 23955 )\n",
      "[Epoch: 25040] train loss: 1.0784, train acc: 0.5472, val loss: 1.0582, val acc: 0.5663  (best train acc: 0.5722, best val acc: 0.5720, best train loss: 1.0447  @ epoch 23955 )\n",
      "[Epoch: 25060] train loss: 1.0541, train acc: 0.5596, val loss: 1.0531, val acc: 0.5666  (best train acc: 0.5722, best val acc: 0.5720, best train loss: 1.0447  @ epoch 23955 )\n",
      "[Epoch: 25080] train loss: 1.0532, train acc: 0.5589, val loss: 1.0639, val acc: 0.5659  (best train acc: 0.5729, best val acc: 0.5720, best train loss: 1.0418  @ epoch 25073 )\n",
      "[Epoch: 25100] train loss: 1.0577, train acc: 0.5601, val loss: 1.0533, val acc: 0.5676  (best train acc: 0.5729, best val acc: 0.5720, best train loss: 1.0418  @ epoch 25073 )\n",
      "[Epoch: 25120] train loss: 1.0830, train acc: 0.5510, val loss: 1.0575, val acc: 0.5680  (best train acc: 0.5729, best val acc: 0.5720, best train loss: 1.0418  @ epoch 25073 )\n",
      "[Epoch: 25140] train loss: 1.0634, train acc: 0.5562, val loss: 1.0523, val acc: 0.5659  (best train acc: 0.5729, best val acc: 0.5727, best train loss: 1.0418  @ epoch 25073 )\n",
      "[Epoch: 25160] train loss: 1.0616, train acc: 0.5598, val loss: 1.0564, val acc: 0.5659  (best train acc: 0.5735, best val acc: 0.5727, best train loss: 1.0418  @ epoch 25073 )\n",
      "[Epoch: 25180] train loss: 1.0551, train acc: 0.5568, val loss: 1.0569, val acc: 0.5622  (best train acc: 0.5735, best val acc: 0.5727, best train loss: 1.0418  @ epoch 25073 )\n",
      "[Epoch: 25200] train loss: 1.0587, train acc: 0.5607, val loss: 1.0592, val acc: 0.5612  (best train acc: 0.5735, best val acc: 0.5727, best train loss: 1.0418  @ epoch 25073 )\n",
      "[Epoch: 25220] train loss: 1.0608, train acc: 0.5625, val loss: 1.0523, val acc: 0.5673  (best train acc: 0.5735, best val acc: 0.5727, best train loss: 1.0385  @ epoch 25218 )\n",
      "[Epoch: 25240] train loss: 1.0803, train acc: 0.5463, val loss: 1.0520, val acc: 0.5666  (best train acc: 0.5735, best val acc: 0.5727, best train loss: 1.0385  @ epoch 25218 )\n",
      "[Epoch: 25260] train loss: 1.0637, train acc: 0.5525, val loss: 1.0517, val acc: 0.5659  (best train acc: 0.5735, best val acc: 0.5730, best train loss: 1.0385  @ epoch 25218 )\n",
      "[Epoch: 25280] train loss: 1.0616, train acc: 0.5557, val loss: 1.0520, val acc: 0.5686  (best train acc: 0.5735, best val acc: 0.5730, best train loss: 1.0385  @ epoch 25218 )\n",
      "[Epoch: 25300] train loss: 1.0698, train acc: 0.5500, val loss: 1.0483, val acc: 0.5639  (best train acc: 0.5735, best val acc: 0.5730, best train loss: 1.0385  @ epoch 25218 )\n",
      "[Epoch: 25320] train loss: 1.0448, train acc: 0.5636, val loss: 1.0515, val acc: 0.5656  (best train acc: 0.5735, best val acc: 0.5740, best train loss: 1.0385  @ epoch 25218 )\n",
      "[Epoch: 25340] train loss: 1.0614, train acc: 0.5536, val loss: 1.0465, val acc: 0.5683  (best train acc: 0.5735, best val acc: 0.5740, best train loss: 1.0385  @ epoch 25218 )\n",
      "[Epoch: 25360] train loss: 1.0641, train acc: 0.5522, val loss: 1.0482, val acc: 0.5632  (best train acc: 0.5735, best val acc: 0.5740, best train loss: 1.0385  @ epoch 25218 )\n",
      "[Epoch: 25380] train loss: 1.0536, train acc: 0.5646, val loss: 1.0504, val acc: 0.5690  (best train acc: 0.5735, best val acc: 0.5740, best train loss: 1.0385  @ epoch 25218 )\n",
      "[Epoch: 25400] train loss: 1.0612, train acc: 0.5585, val loss: 1.0521, val acc: 0.5683  (best train acc: 0.5735, best val acc: 0.5740, best train loss: 1.0385  @ epoch 25218 )\n",
      "[Epoch: 25420] train loss: 1.0591, train acc: 0.5625, val loss: 1.0604, val acc: 0.5636  (best train acc: 0.5735, best val acc: 0.5740, best train loss: 1.0385  @ epoch 25218 )\n",
      "[Epoch: 25440] train loss: 1.0503, train acc: 0.5701, val loss: 1.0480, val acc: 0.5629  (best train acc: 0.5735, best val acc: 0.5740, best train loss: 1.0385  @ epoch 25218 )\n",
      "[Epoch: 25460] train loss: 1.0721, train acc: 0.5528, val loss: 1.0484, val acc: 0.5710  (best train acc: 0.5735, best val acc: 0.5740, best train loss: 1.0385  @ epoch 25218 )\n",
      "[Epoch: 25480] train loss: 1.0648, train acc: 0.5565, val loss: 1.0484, val acc: 0.5669  (best train acc: 0.5735, best val acc: 0.5740, best train loss: 1.0385  @ epoch 25218 )\n",
      "[Epoch: 25500] train loss: 1.0659, train acc: 0.5586, val loss: 1.0449, val acc: 0.5690  (best train acc: 0.5760, best val acc: 0.5740, best train loss: 1.0318  @ epoch 25482 )\n",
      "[Epoch: 25520] train loss: 1.0705, train acc: 0.5524, val loss: 1.0473, val acc: 0.5700  (best train acc: 0.5760, best val acc: 0.5740, best train loss: 1.0318  @ epoch 25482 )\n",
      "[Epoch: 25540] train loss: 1.0477, train acc: 0.5653, val loss: 1.0467, val acc: 0.5680  (best train acc: 0.5760, best val acc: 0.5740, best train loss: 1.0318  @ epoch 25482 )\n",
      "[Epoch: 25560] train loss: 1.0576, train acc: 0.5494, val loss: 1.0415, val acc: 0.5693  (best train acc: 0.5760, best val acc: 0.5740, best train loss: 1.0318  @ epoch 25482 )\n",
      "[Epoch: 25580] train loss: 1.0540, train acc: 0.5612, val loss: 1.0474, val acc: 0.5703  (best train acc: 0.5760, best val acc: 0.5740, best train loss: 1.0318  @ epoch 25482 )\n",
      "[Epoch: 25600] train loss: 1.0560, train acc: 0.5578, val loss: 1.0539, val acc: 0.5730  (best train acc: 0.5760, best val acc: 0.5740, best train loss: 1.0318  @ epoch 25482 )\n",
      "[Epoch: 25620] train loss: 1.0708, train acc: 0.5511, val loss: 1.0404, val acc: 0.5707  (best train acc: 0.5760, best val acc: 0.5740, best train loss: 1.0318  @ epoch 25482 )\n",
      "[Epoch: 25640] train loss: 1.0561, train acc: 0.5612, val loss: 1.0457, val acc: 0.5727  (best train acc: 0.5760, best val acc: 0.5744, best train loss: 1.0318  @ epoch 25482 )\n",
      "[Epoch: 25660] train loss: 1.0486, train acc: 0.5586, val loss: 1.0427, val acc: 0.5696  (best train acc: 0.5760, best val acc: 0.5744, best train loss: 1.0318  @ epoch 25482 )\n",
      "[Epoch: 25680] train loss: 1.0691, train acc: 0.5558, val loss: 1.0417, val acc: 0.5713  (best train acc: 0.5760, best val acc: 0.5744, best train loss: 1.0318  @ epoch 25482 )\n",
      "[Epoch: 25700] train loss: 1.0560, train acc: 0.5620, val loss: 1.0460, val acc: 0.5669  (best train acc: 0.5760, best val acc: 0.5750, best train loss: 1.0318  @ epoch 25482 )\n",
      "[Epoch: 25720] train loss: 1.0453, train acc: 0.5605, val loss: 1.0472, val acc: 0.5723  (best train acc: 0.5760, best val acc: 0.5750, best train loss: 1.0318  @ epoch 25482 )\n",
      "[Epoch: 25740] train loss: 1.0395, train acc: 0.5635, val loss: 1.0417, val acc: 0.5713  (best train acc: 0.5760, best val acc: 0.5750, best train loss: 1.0318  @ epoch 25482 )\n",
      "[Epoch: 25760] train loss: 1.0489, train acc: 0.5546, val loss: 1.0433, val acc: 0.5730  (best train acc: 0.5760, best val acc: 0.5750, best train loss: 1.0318  @ epoch 25482 )\n",
      "[Epoch: 25780] train loss: 1.0542, train acc: 0.5594, val loss: 1.0447, val acc: 0.5690  (best train acc: 0.5760, best val acc: 0.5764, best train loss: 1.0318  @ epoch 25482 )\n",
      "[Epoch: 25800] train loss: 1.0778, train acc: 0.5512, val loss: 1.0624, val acc: 0.5696  (best train acc: 0.5760, best val acc: 0.5764, best train loss: 1.0318  @ epoch 25482 )\n",
      "[Epoch: 25820] train loss: 1.0758, train acc: 0.5434, val loss: 1.0593, val acc: 0.5582  (best train acc: 0.5760, best val acc: 0.5764, best train loss: 1.0318  @ epoch 25482 )\n",
      "[Epoch: 25840] train loss: 1.0678, train acc: 0.5549, val loss: 1.0562, val acc: 0.5642  (best train acc: 0.5760, best val acc: 0.5764, best train loss: 1.0318  @ epoch 25482 )\n",
      "[Epoch: 25860] train loss: 1.0676, train acc: 0.5541, val loss: 1.0490, val acc: 0.5720  (best train acc: 0.5760, best val acc: 0.5764, best train loss: 1.0318  @ epoch 25482 )\n",
      "[Epoch: 25880] train loss: 1.0656, train acc: 0.5518, val loss: 1.0428, val acc: 0.5700  (best train acc: 0.5760, best val acc: 0.5764, best train loss: 1.0318  @ epoch 25482 )\n",
      "[Epoch: 25900] train loss: 1.0408, train acc: 0.5711, val loss: 1.0460, val acc: 0.5713  (best train acc: 0.5760, best val acc: 0.5764, best train loss: 1.0318  @ epoch 25482 )\n",
      "[Epoch: 25920] train loss: 1.0714, train acc: 0.5583, val loss: 1.0420, val acc: 0.5744  (best train acc: 0.5760, best val acc: 0.5764, best train loss: 1.0318  @ epoch 25482 )\n",
      "[Epoch: 25940] train loss: 1.0648, train acc: 0.5519, val loss: 1.0441, val acc: 0.5696  (best train acc: 0.5760, best val acc: 0.5764, best train loss: 1.0318  @ epoch 25482 )\n",
      "[Epoch: 25960] train loss: 1.0494, train acc: 0.5643, val loss: 1.0460, val acc: 0.5713  (best train acc: 0.5760, best val acc: 0.5764, best train loss: 1.0318  @ epoch 25482 )\n",
      "[Epoch: 25980] train loss: 1.0564, train acc: 0.5580, val loss: 1.0428, val acc: 0.5744  (best train acc: 0.5760, best val acc: 0.5774, best train loss: 1.0318  @ epoch 25482 )\n",
      "[Epoch: 26000] train loss: 1.0455, train acc: 0.5649, val loss: 1.0427, val acc: 0.5737  (best train acc: 0.5760, best val acc: 0.5774, best train loss: 1.0318  @ epoch 25482 )\n",
      "[Epoch: 26020] train loss: 1.0580, train acc: 0.5620, val loss: 1.0431, val acc: 0.5690  (best train acc: 0.5760, best val acc: 0.5774, best train loss: 1.0318  @ epoch 25482 )\n",
      "[Epoch: 26040] train loss: 1.0574, train acc: 0.5648, val loss: 1.0510, val acc: 0.5680  (best train acc: 0.5760, best val acc: 0.5774, best train loss: 1.0273  @ epoch 26032 )\n",
      "[Epoch: 26060] train loss: 1.0557, train acc: 0.5653, val loss: 1.0442, val acc: 0.5713  (best train acc: 0.5760, best val acc: 0.5774, best train loss: 1.0273  @ epoch 26032 )\n",
      "[Epoch: 26080] train loss: 1.0607, train acc: 0.5520, val loss: 1.0444, val acc: 0.5686  (best train acc: 0.5760, best val acc: 0.5774, best train loss: 1.0273  @ epoch 26032 )\n",
      "[Epoch: 26100] train loss: 1.0526, train acc: 0.5637, val loss: 1.0449, val acc: 0.5703  (best train acc: 0.5760, best val acc: 0.5774, best train loss: 1.0273  @ epoch 26032 )\n",
      "[Epoch: 26120] train loss: 1.0584, train acc: 0.5594, val loss: 1.0403, val acc: 0.5720  (best train acc: 0.5760, best val acc: 0.5774, best train loss: 1.0273  @ epoch 26032 )\n",
      "[Epoch: 26140] train loss: 1.0493, train acc: 0.5633, val loss: 1.0393, val acc: 0.5720  (best train acc: 0.5760, best val acc: 0.5774, best train loss: 1.0273  @ epoch 26032 )\n",
      "[Epoch: 26160] train loss: 1.0554, train acc: 0.5609, val loss: 1.0407, val acc: 0.5720  (best train acc: 0.5760, best val acc: 0.5774, best train loss: 1.0273  @ epoch 26032 )\n",
      "[Epoch: 26180] train loss: 1.0458, train acc: 0.5624, val loss: 1.0403, val acc: 0.5713  (best train acc: 0.5760, best val acc: 0.5774, best train loss: 1.0273  @ epoch 26032 )\n",
      "[Epoch: 26200] train loss: 1.0616, train acc: 0.5513, val loss: 1.0446, val acc: 0.5717  (best train acc: 0.5763, best val acc: 0.5774, best train loss: 1.0273  @ epoch 26032 )\n",
      "[Epoch: 26220] train loss: 1.0531, train acc: 0.5609, val loss: 1.0431, val acc: 0.5727  (best train acc: 0.5763, best val acc: 0.5774, best train loss: 1.0273  @ epoch 26032 )\n",
      "[Epoch: 26240] train loss: 1.0588, train acc: 0.5567, val loss: 1.0391, val acc: 0.5720  (best train acc: 0.5763, best val acc: 0.5774, best train loss: 1.0273  @ epoch 26032 )\n",
      "[Epoch: 26260] train loss: 1.0415, train acc: 0.5689, val loss: 1.0412, val acc: 0.5767  (best train acc: 0.5763, best val acc: 0.5774, best train loss: 1.0273  @ epoch 26032 )\n",
      "[Epoch: 26280] train loss: 1.0467, train acc: 0.5662, val loss: 1.0446, val acc: 0.5710  (best train acc: 0.5763, best val acc: 0.5774, best train loss: 1.0273  @ epoch 26032 )\n",
      "[Epoch: 26300] train loss: 1.0536, train acc: 0.5588, val loss: 1.0390, val acc: 0.5723  (best train acc: 0.5763, best val acc: 0.5774, best train loss: 1.0273  @ epoch 26032 )\n",
      "[Epoch: 26320] train loss: 1.0442, train acc: 0.5635, val loss: 1.0455, val acc: 0.5723  (best train acc: 0.5768, best val acc: 0.5774, best train loss: 1.0261  @ epoch 26317 )\n",
      "[Epoch: 26340] train loss: 1.0410, train acc: 0.5678, val loss: 1.0389, val acc: 0.5703  (best train acc: 0.5768, best val acc: 0.5774, best train loss: 1.0261  @ epoch 26317 )\n",
      "[Epoch: 26360] train loss: 1.0341, train acc: 0.5681, val loss: 1.0415, val acc: 0.5740  (best train acc: 0.5768, best val acc: 0.5774, best train loss: 1.0261  @ epoch 26317 )\n",
      "[Epoch: 26380] train loss: 1.0380, train acc: 0.5693, val loss: 1.0382, val acc: 0.5710  (best train acc: 0.5768, best val acc: 0.5774, best train loss: 1.0261  @ epoch 26317 )\n",
      "[Epoch: 26400] train loss: 1.0536, train acc: 0.5568, val loss: 1.0377, val acc: 0.5734  (best train acc: 0.5768, best val acc: 0.5774, best train loss: 1.0261  @ epoch 26317 )\n",
      "[Epoch: 26420] train loss: 1.0626, train acc: 0.5473, val loss: 1.0404, val acc: 0.5713  (best train acc: 0.5768, best val acc: 0.5774, best train loss: 1.0261  @ epoch 26317 )\n",
      "[Epoch: 26440] train loss: 1.0540, train acc: 0.5630, val loss: 1.0371, val acc: 0.5720  (best train acc: 0.5768, best val acc: 0.5774, best train loss: 1.0261  @ epoch 26317 )\n",
      "[Epoch: 26460] train loss: 1.0511, train acc: 0.5625, val loss: 1.0412, val acc: 0.5754  (best train acc: 0.5768, best val acc: 0.5774, best train loss: 1.0261  @ epoch 26317 )\n",
      "[Epoch: 26480] train loss: 1.0348, train acc: 0.5702, val loss: 1.0394, val acc: 0.5730  (best train acc: 0.5768, best val acc: 0.5774, best train loss: 1.0261  @ epoch 26317 )\n",
      "[Epoch: 26500] train loss: 1.0595, train acc: 0.5612, val loss: 1.0456, val acc: 0.5683  (best train acc: 0.5768, best val acc: 0.5774, best train loss: 1.0261  @ epoch 26317 )\n",
      "[Epoch: 26520] train loss: 1.0952, train acc: 0.5431, val loss: 1.0923, val acc: 0.5440  (best train acc: 0.5768, best val acc: 0.5774, best train loss: 1.0261  @ epoch 26317 )\n",
      "[Epoch: 26540] train loss: 1.0569, train acc: 0.5566, val loss: 1.0410, val acc: 0.5727  (best train acc: 0.5768, best val acc: 0.5774, best train loss: 1.0261  @ epoch 26317 )\n",
      "[Epoch: 26560] train loss: 1.0528, train acc: 0.5670, val loss: 1.0385, val acc: 0.5700  (best train acc: 0.5768, best val acc: 0.5774, best train loss: 1.0261  @ epoch 26317 )\n",
      "[Epoch: 26580] train loss: 1.0462, train acc: 0.5682, val loss: 1.0411, val acc: 0.5737  (best train acc: 0.5768, best val acc: 0.5784, best train loss: 1.0261  @ epoch 26317 )\n",
      "[Epoch: 26600] train loss: 1.0630, train acc: 0.5525, val loss: 1.0394, val acc: 0.5717  (best train acc: 0.5768, best val acc: 0.5784, best train loss: 1.0261  @ epoch 26317 )\n",
      "[Epoch: 26620] train loss: 1.0507, train acc: 0.5625, val loss: 1.0383, val acc: 0.5754  (best train acc: 0.5768, best val acc: 0.5784, best train loss: 1.0261  @ epoch 26317 )\n",
      "[Epoch: 26640] train loss: 1.0712, train acc: 0.5608, val loss: 1.0434, val acc: 0.5730  (best train acc: 0.5768, best val acc: 0.5784, best train loss: 1.0261  @ epoch 26317 )\n",
      "[Epoch: 26660] train loss: 1.0393, train acc: 0.5654, val loss: 1.0339, val acc: 0.5717  (best train acc: 0.5768, best val acc: 0.5784, best train loss: 1.0261  @ epoch 26317 )\n",
      "[Epoch: 26680] train loss: 1.0514, train acc: 0.5667, val loss: 1.0364, val acc: 0.5747  (best train acc: 0.5768, best val acc: 0.5784, best train loss: 1.0261  @ epoch 26317 )\n",
      "[Epoch: 26700] train loss: 1.0597, train acc: 0.5513, val loss: 1.0435, val acc: 0.5696  (best train acc: 0.5768, best val acc: 0.5784, best train loss: 1.0261  @ epoch 26317 )\n",
      "[Epoch: 26720] train loss: 1.0521, train acc: 0.5651, val loss: 1.0377, val acc: 0.5720  (best train acc: 0.5768, best val acc: 0.5784, best train loss: 1.0261  @ epoch 26317 )\n",
      "[Epoch: 26740] train loss: 1.0490, train acc: 0.5587, val loss: 1.0397, val acc: 0.5747  (best train acc: 0.5768, best val acc: 0.5784, best train loss: 1.0261  @ epoch 26317 )\n",
      "[Epoch: 26760] train loss: 1.0580, train acc: 0.5614, val loss: 1.0392, val acc: 0.5727  (best train acc: 0.5768, best val acc: 0.5784, best train loss: 1.0261  @ epoch 26317 )\n",
      "[Epoch: 26780] train loss: 1.0489, train acc: 0.5567, val loss: 1.0389, val acc: 0.5713  (best train acc: 0.5768, best val acc: 0.5784, best train loss: 1.0261  @ epoch 26317 )\n",
      "[Epoch: 26800] train loss: 1.0423, train acc: 0.5685, val loss: 1.0369, val acc: 0.5723  (best train acc: 0.5768, best val acc: 0.5784, best train loss: 1.0261  @ epoch 26317 )\n",
      "[Epoch: 26820] train loss: 1.0594, train acc: 0.5480, val loss: 1.0364, val acc: 0.5720  (best train acc: 0.5774, best val acc: 0.5784, best train loss: 1.0261  @ epoch 26317 )\n",
      "[Epoch: 26840] train loss: 1.0614, train acc: 0.5515, val loss: 1.0689, val acc: 0.5568  (best train acc: 0.5774, best val acc: 0.5784, best train loss: 1.0261  @ epoch 26317 )\n",
      "[Epoch: 26860] train loss: 1.0618, train acc: 0.5618, val loss: 1.0479, val acc: 0.5723  (best train acc: 0.5774, best val acc: 0.5784, best train loss: 1.0261  @ epoch 26317 )\n",
      "[Epoch: 26880] train loss: 1.0485, train acc: 0.5592, val loss: 1.0424, val acc: 0.5727  (best train acc: 0.5774, best val acc: 0.5784, best train loss: 1.0261  @ epoch 26317 )\n",
      "[Epoch: 26900] train loss: 1.0479, train acc: 0.5616, val loss: 1.0366, val acc: 0.5727  (best train acc: 0.5774, best val acc: 0.5784, best train loss: 1.0261  @ epoch 26317 )\n",
      "[Epoch: 26920] train loss: 1.0524, train acc: 0.5570, val loss: 1.0412, val acc: 0.5727  (best train acc: 0.5774, best val acc: 0.5784, best train loss: 1.0261  @ epoch 26317 )\n",
      "[Epoch: 26940] train loss: 1.0663, train acc: 0.5492, val loss: 1.0410, val acc: 0.5740  (best train acc: 0.5774, best val acc: 0.5784, best train loss: 1.0261  @ epoch 26317 )\n",
      "[Epoch: 26960] train loss: 1.0623, train acc: 0.5636, val loss: 1.0366, val acc: 0.5707  (best train acc: 0.5774, best val acc: 0.5784, best train loss: 1.0261  @ epoch 26317 )\n",
      "[Epoch: 26980] train loss: 1.0531, train acc: 0.5565, val loss: 1.0341, val acc: 0.5703  (best train acc: 0.5774, best val acc: 0.5784, best train loss: 1.0261  @ epoch 26317 )\n",
      "[Epoch: 27000] train loss: 1.0447, train acc: 0.5648, val loss: 1.0409, val acc: 0.5717  (best train acc: 0.5774, best val acc: 0.5784, best train loss: 1.0261  @ epoch 26317 )\n",
      "[Epoch: 27020] train loss: 1.0419, train acc: 0.5664, val loss: 1.0384, val acc: 0.5696  (best train acc: 0.5774, best val acc: 0.5784, best train loss: 1.0261  @ epoch 26317 )\n",
      "[Epoch: 27040] train loss: 1.0558, train acc: 0.5674, val loss: 1.0398, val acc: 0.5713  (best train acc: 0.5774, best val acc: 0.5784, best train loss: 1.0261  @ epoch 26317 )\n",
      "[Epoch: 27060] train loss: 1.0628, train acc: 0.5560, val loss: 1.0362, val acc: 0.5713  (best train acc: 0.5774, best val acc: 0.5784, best train loss: 1.0261  @ epoch 26317 )\n",
      "[Epoch: 27080] train loss: 1.0427, train acc: 0.5651, val loss: 1.0354, val acc: 0.5737  (best train acc: 0.5774, best val acc: 0.5784, best train loss: 1.0261  @ epoch 26317 )\n",
      "[Epoch: 27100] train loss: 1.0598, train acc: 0.5709, val loss: 1.0714, val acc: 0.5589  (best train acc: 0.5774, best val acc: 0.5784, best train loss: 1.0261  @ epoch 26317 )\n",
      "[Epoch: 27120] train loss: 1.0492, train acc: 0.5608, val loss: 1.0516, val acc: 0.5676  (best train acc: 0.5774, best val acc: 0.5784, best train loss: 1.0261  @ epoch 26317 )\n",
      "[Epoch: 27140] train loss: 1.0414, train acc: 0.5688, val loss: 1.0415, val acc: 0.5683  (best train acc: 0.5774, best val acc: 0.5784, best train loss: 1.0261  @ epoch 26317 )\n",
      "[Epoch: 27160] train loss: 1.0469, train acc: 0.5695, val loss: 1.0357, val acc: 0.5720  (best train acc: 0.5774, best val acc: 0.5784, best train loss: 1.0261  @ epoch 26317 )\n",
      "[Epoch: 27180] train loss: 1.0545, train acc: 0.5531, val loss: 1.0358, val acc: 0.5713  (best train acc: 0.5774, best val acc: 0.5784, best train loss: 1.0206  @ epoch 27173 )\n",
      "[Epoch: 27200] train loss: 1.0361, train acc: 0.5688, val loss: 1.0404, val acc: 0.5747  (best train acc: 0.5774, best val acc: 0.5784, best train loss: 1.0206  @ epoch 27173 )\n",
      "[Epoch: 27220] train loss: 1.0628, train acc: 0.5592, val loss: 1.0366, val acc: 0.5723  (best train acc: 0.5774, best val acc: 0.5784, best train loss: 1.0206  @ epoch 27173 )\n",
      "[Epoch: 27240] train loss: 1.0383, train acc: 0.5732, val loss: 1.0347, val acc: 0.5703  (best train acc: 0.5774, best val acc: 0.5784, best train loss: 1.0206  @ epoch 27173 )\n",
      "[Epoch: 27260] train loss: 1.0571, train acc: 0.5630, val loss: 1.0337, val acc: 0.5750  (best train acc: 0.5774, best val acc: 0.5784, best train loss: 1.0206  @ epoch 27173 )\n",
      "[Epoch: 27280] train loss: 1.0315, train acc: 0.5703, val loss: 1.0347, val acc: 0.5757  (best train acc: 0.5774, best val acc: 0.5788, best train loss: 1.0206  @ epoch 27173 )\n",
      "[Epoch: 27300] train loss: 1.0470, train acc: 0.5585, val loss: 1.0338, val acc: 0.5761  (best train acc: 0.5776, best val acc: 0.5788, best train loss: 1.0206  @ epoch 27173 )\n",
      "[Epoch: 27320] train loss: 1.0504, train acc: 0.5622, val loss: 1.0379, val acc: 0.5703  (best train acc: 0.5776, best val acc: 0.5788, best train loss: 1.0206  @ epoch 27173 )\n",
      "[Epoch: 27340] train loss: 1.0481, train acc: 0.5658, val loss: 1.0522, val acc: 0.5663  (best train acc: 0.5776, best val acc: 0.5788, best train loss: 1.0206  @ epoch 27173 )\n",
      "[Epoch: 27360] train loss: 1.0634, train acc: 0.5646, val loss: 1.0468, val acc: 0.5642  (best train acc: 0.5776, best val acc: 0.5788, best train loss: 1.0206  @ epoch 27173 )\n",
      "[Epoch: 27380] train loss: 1.0399, train acc: 0.5695, val loss: 1.0391, val acc: 0.5747  (best train acc: 0.5776, best val acc: 0.5788, best train loss: 1.0206  @ epoch 27173 )\n",
      "[Epoch: 27400] train loss: 1.0385, train acc: 0.5673, val loss: 1.0319, val acc: 0.5744  (best train acc: 0.5776, best val acc: 0.5788, best train loss: 1.0206  @ epoch 27173 )\n",
      "[Epoch: 27420] train loss: 1.0400, train acc: 0.5649, val loss: 1.0374, val acc: 0.5713  (best train acc: 0.5776, best val acc: 0.5788, best train loss: 1.0206  @ epoch 27173 )\n",
      "[Epoch: 27440] train loss: 1.0580, train acc: 0.5546, val loss: 1.0313, val acc: 0.5720  (best train acc: 0.5776, best val acc: 0.5798, best train loss: 1.0206  @ epoch 27173 )\n",
      "[Epoch: 27460] train loss: 1.0391, train acc: 0.5677, val loss: 1.0376, val acc: 0.5747  (best train acc: 0.5776, best val acc: 0.5798, best train loss: 1.0206  @ epoch 27173 )\n",
      "[Epoch: 27480] train loss: 1.0342, train acc: 0.5675, val loss: 1.0340, val acc: 0.5740  (best train acc: 0.5776, best val acc: 0.5798, best train loss: 1.0206  @ epoch 27173 )\n",
      "[Epoch: 27500] train loss: 1.0486, train acc: 0.5630, val loss: 1.0360, val acc: 0.5666  (best train acc: 0.5776, best val acc: 0.5798, best train loss: 1.0206  @ epoch 27173 )\n",
      "[Epoch: 27520] train loss: 1.0477, train acc: 0.5610, val loss: 1.0391, val acc: 0.5707  (best train acc: 0.5776, best val acc: 0.5798, best train loss: 1.0206  @ epoch 27173 )\n",
      "[Epoch: 27540] train loss: 1.0423, train acc: 0.5636, val loss: 1.0349, val acc: 0.5737  (best train acc: 0.5776, best val acc: 0.5798, best train loss: 1.0206  @ epoch 27173 )\n",
      "[Epoch: 27560] train loss: 1.0366, train acc: 0.5656, val loss: 1.0307, val acc: 0.5713  (best train acc: 0.5776, best val acc: 0.5798, best train loss: 1.0206  @ epoch 27173 )\n",
      "[Epoch: 27580] train loss: 1.0481, train acc: 0.5616, val loss: 1.0348, val acc: 0.5740  (best train acc: 0.5776, best val acc: 0.5798, best train loss: 1.0206  @ epoch 27173 )\n",
      "[Epoch: 27600] train loss: 1.0292, train acc: 0.5722, val loss: 1.0336, val acc: 0.5727  (best train acc: 0.5776, best val acc: 0.5798, best train loss: 1.0206  @ epoch 27173 )\n",
      "[Epoch: 27620] train loss: 1.0402, train acc: 0.5611, val loss: 1.0307, val acc: 0.5740  (best train acc: 0.5776, best val acc: 0.5798, best train loss: 1.0206  @ epoch 27173 )\n",
      "[Epoch: 27640] train loss: 1.0479, train acc: 0.5568, val loss: 1.0391, val acc: 0.5771  (best train acc: 0.5776, best val acc: 0.5798, best train loss: 1.0206  @ epoch 27173 )\n",
      "[Epoch: 27660] train loss: 1.0440, train acc: 0.5622, val loss: 1.0323, val acc: 0.5777  (best train acc: 0.5782, best val acc: 0.5798, best train loss: 1.0206  @ epoch 27173 )\n",
      "[Epoch: 27680] train loss: 1.0569, train acc: 0.5552, val loss: 1.0480, val acc: 0.5720  (best train acc: 0.5782, best val acc: 0.5798, best train loss: 1.0206  @ epoch 27173 )\n",
      "[Epoch: 27700] train loss: 1.0406, train acc: 0.5663, val loss: 1.0334, val acc: 0.5767  (best train acc: 0.5782, best val acc: 0.5798, best train loss: 1.0206  @ epoch 27173 )\n",
      "[Epoch: 27720] train loss: 1.0402, train acc: 0.5668, val loss: 1.0381, val acc: 0.5700  (best train acc: 0.5782, best val acc: 0.5798, best train loss: 1.0195  @ epoch 27715 )\n",
      "[Epoch: 27740] train loss: 1.0382, train acc: 0.5682, val loss: 1.0304, val acc: 0.5754  (best train acc: 0.5782, best val acc: 0.5798, best train loss: 1.0195  @ epoch 27715 )\n",
      "[Epoch: 27760] train loss: 1.0432, train acc: 0.5721, val loss: 1.0312, val acc: 0.5707  (best train acc: 0.5782, best val acc: 0.5798, best train loss: 1.0195  @ epoch 27715 )\n",
      "[Epoch: 27780] train loss: 1.0367, train acc: 0.5677, val loss: 1.0318, val acc: 0.5727  (best train acc: 0.5787, best val acc: 0.5798, best train loss: 1.0195  @ epoch 27715 )\n",
      "[Epoch: 27800] train loss: 1.0400, train acc: 0.5634, val loss: 1.0313, val acc: 0.5690  (best train acc: 0.5787, best val acc: 0.5798, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 27820] train loss: 1.0182, train acc: 0.5776, val loss: 1.0310, val acc: 0.5717  (best train acc: 0.5787, best val acc: 0.5798, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 27840] train loss: 1.0454, train acc: 0.5603, val loss: 1.0304, val acc: 0.5750  (best train acc: 0.5787, best val acc: 0.5798, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 27860] train loss: 1.0356, train acc: 0.5703, val loss: 1.0301, val acc: 0.5757  (best train acc: 0.5787, best val acc: 0.5798, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 27880] train loss: 1.0330, train acc: 0.5656, val loss: 1.0301, val acc: 0.5727  (best train acc: 0.5787, best val acc: 0.5798, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 27900] train loss: 1.0461, train acc: 0.5575, val loss: 1.0299, val acc: 0.5750  (best train acc: 0.5787, best val acc: 0.5798, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 27920] train loss: 1.0236, train acc: 0.5677, val loss: 1.0299, val acc: 0.5771  (best train acc: 0.5787, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 27940] train loss: 1.0515, train acc: 0.5566, val loss: 1.3834, val acc: 0.4614  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 27960] train loss: 1.2066, train acc: 0.5132, val loss: 1.1752, val acc: 0.5325  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 27980] train loss: 1.1580, train acc: 0.5232, val loss: 1.1469, val acc: 0.5298  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 28000] train loss: 1.1634, train acc: 0.5260, val loss: 1.1444, val acc: 0.5312  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 28020] train loss: 1.1315, train acc: 0.5395, val loss: 1.1432, val acc: 0.5309  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 28040] train loss: 1.1472, train acc: 0.5319, val loss: 1.1428, val acc: 0.5305  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 28060] train loss: 1.1449, train acc: 0.5304, val loss: 1.1432, val acc: 0.5336  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 28080] train loss: 1.1558, train acc: 0.5260, val loss: 1.1422, val acc: 0.5315  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 28100] train loss: 1.1595, train acc: 0.5294, val loss: 1.1414, val acc: 0.5305  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 28120] train loss: 1.1413, train acc: 0.5325, val loss: 1.1408, val acc: 0.5309  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 28140] train loss: 1.1396, train acc: 0.5352, val loss: 1.1403, val acc: 0.5322  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 28160] train loss: 1.1341, train acc: 0.5353, val loss: 1.1414, val acc: 0.5352  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 28180] train loss: 1.1443, train acc: 0.5296, val loss: 1.1432, val acc: 0.5406  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 28200] train loss: 1.1599, train acc: 0.5280, val loss: 1.1407, val acc: 0.5376  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 28220] train loss: 1.1311, train acc: 0.5388, val loss: 1.1410, val acc: 0.5410  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 28240] train loss: 1.1408, train acc: 0.5357, val loss: 1.1383, val acc: 0.5325  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 28260] train loss: 1.1408, train acc: 0.5341, val loss: 1.1381, val acc: 0.5352  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 28280] train loss: 1.1341, train acc: 0.5341, val loss: 1.1376, val acc: 0.5339  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 28300] train loss: 1.1527, train acc: 0.5274, val loss: 1.1377, val acc: 0.5349  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 28320] train loss: 1.1486, train acc: 0.5291, val loss: 1.1370, val acc: 0.5336  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 28340] train loss: 1.1432, train acc: 0.5335, val loss: 1.1381, val acc: 0.5352  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 28360] train loss: 1.1358, train acc: 0.5396, val loss: 1.1361, val acc: 0.5336  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 28380] train loss: 1.1249, train acc: 0.5382, val loss: 1.1368, val acc: 0.5302  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 28400] train loss: 1.1253, train acc: 0.5412, val loss: 1.1351, val acc: 0.5315  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 28420] train loss: 1.1428, train acc: 0.5270, val loss: 1.1360, val acc: 0.5359  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 28440] train loss: 1.1337, train acc: 0.5364, val loss: 1.1347, val acc: 0.5356  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 28460] train loss: 1.1262, train acc: 0.5444, val loss: 1.1346, val acc: 0.5339  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 28480] train loss: 1.1285, train acc: 0.5388, val loss: 1.1358, val acc: 0.5356  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 28500] train loss: 1.1330, train acc: 0.5375, val loss: 1.1340, val acc: 0.5346  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 28520] train loss: 1.1370, train acc: 0.5303, val loss: 1.1328, val acc: 0.5346  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 28540] train loss: 1.1239, train acc: 0.5405, val loss: 1.1335, val acc: 0.5349  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 28560] train loss: 1.1366, train acc: 0.5301, val loss: 1.1325, val acc: 0.5379  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 28580] train loss: 1.1502, train acc: 0.5264, val loss: 1.1323, val acc: 0.5352  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 28600] train loss: 1.1370, train acc: 0.5326, val loss: 1.1323, val acc: 0.5349  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 28620] train loss: 1.1318, train acc: 0.5373, val loss: 1.1318, val acc: 0.5356  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 28640] train loss: 1.1400, train acc: 0.5320, val loss: 1.1311, val acc: 0.5352  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 28660] train loss: 1.1480, train acc: 0.5267, val loss: 1.1321, val acc: 0.5336  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 28680] train loss: 1.1346, train acc: 0.5320, val loss: 1.1306, val acc: 0.5373  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 28700] train loss: 1.1561, train acc: 0.5245, val loss: 1.1304, val acc: 0.5383  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 28720] train loss: 1.1224, train acc: 0.5417, val loss: 1.1301, val acc: 0.5366  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 28740] train loss: 1.1378, train acc: 0.5333, val loss: 1.1302, val acc: 0.5325  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 28760] train loss: 1.1328, train acc: 0.5366, val loss: 1.1303, val acc: 0.5339  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 28780] train loss: 1.1361, train acc: 0.5378, val loss: 1.1293, val acc: 0.5363  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 28800] train loss: 1.1473, train acc: 0.5329, val loss: 1.1304, val acc: 0.5332  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 28820] train loss: 1.1182, train acc: 0.5382, val loss: 1.1291, val acc: 0.5386  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 28840] train loss: 1.1389, train acc: 0.5346, val loss: 1.1295, val acc: 0.5352  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 28860] train loss: 1.1414, train acc: 0.5281, val loss: 1.1367, val acc: 0.5383  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 28880] train loss: 1.1244, train acc: 0.5367, val loss: 1.1286, val acc: 0.5336  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 28900] train loss: 1.1457, train acc: 0.5273, val loss: 1.1290, val acc: 0.5319  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 28920] train loss: 1.1392, train acc: 0.5302, val loss: 1.1309, val acc: 0.5329  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 28940] train loss: 1.1337, train acc: 0.5358, val loss: 1.1321, val acc: 0.5363  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 28960] train loss: 1.1206, train acc: 0.5382, val loss: 1.1272, val acc: 0.5393  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 28980] train loss: 1.1259, train acc: 0.5400, val loss: 1.1277, val acc: 0.5376  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 29000] train loss: 1.1333, train acc: 0.5302, val loss: 1.1267, val acc: 0.5369  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 29020] train loss: 1.1252, train acc: 0.5351, val loss: 1.1287, val acc: 0.5366  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 29040] train loss: 1.1176, train acc: 0.5416, val loss: 1.1315, val acc: 0.5336  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 29060] train loss: 1.1321, train acc: 0.5350, val loss: 1.1282, val acc: 0.5342  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 29080] train loss: 1.1325, train acc: 0.5319, val loss: 1.1305, val acc: 0.5278  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 29100] train loss: 1.1369, train acc: 0.5341, val loss: 1.1280, val acc: 0.5352  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 29120] train loss: 1.1031, train acc: 0.5480, val loss: 1.1263, val acc: 0.5369  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 29140] train loss: 1.1365, train acc: 0.5310, val loss: 1.1291, val acc: 0.5359  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 29160] train loss: 1.1251, train acc: 0.5356, val loss: 1.1257, val acc: 0.5383  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 29180] train loss: 1.1333, train acc: 0.5376, val loss: 1.1284, val acc: 0.5373  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 29200] train loss: 1.1247, train acc: 0.5330, val loss: 1.1256, val acc: 0.5373  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 29220] train loss: 1.1231, train acc: 0.5361, val loss: 1.1253, val acc: 0.5363  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 29240] train loss: 1.1332, train acc: 0.5446, val loss: 1.1269, val acc: 0.5379  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 29260] train loss: 1.1317, train acc: 0.5363, val loss: 1.1246, val acc: 0.5363  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 29280] train loss: 1.1355, train acc: 0.5337, val loss: 1.1328, val acc: 0.5295  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 29300] train loss: 1.1401, train acc: 0.5312, val loss: 1.1248, val acc: 0.5363  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 29320] train loss: 1.1490, train acc: 0.5291, val loss: 1.1268, val acc: 0.5332  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 29340] train loss: 1.1318, train acc: 0.5271, val loss: 1.1258, val acc: 0.5363  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 29360] train loss: 1.1551, train acc: 0.5201, val loss: 1.1240, val acc: 0.5379  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 29380] train loss: 1.1312, train acc: 0.5351, val loss: 1.1257, val acc: 0.5332  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 29400] train loss: 1.1499, train acc: 0.5321, val loss: 1.1238, val acc: 0.5376  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 29420] train loss: 1.1265, train acc: 0.5362, val loss: 1.1346, val acc: 0.5352  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 29440] train loss: 1.1443, train acc: 0.5298, val loss: 1.1255, val acc: 0.5393  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 29460] train loss: 1.1320, train acc: 0.5330, val loss: 1.1241, val acc: 0.5339  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 29480] train loss: 1.1158, train acc: 0.5367, val loss: 1.1258, val acc: 0.5336  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 29500] train loss: 1.1374, train acc: 0.5294, val loss: 1.1250, val acc: 0.5329  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 29520] train loss: 1.1355, train acc: 0.5258, val loss: 1.1229, val acc: 0.5386  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 29540] train loss: 1.1311, train acc: 0.5361, val loss: 1.1231, val acc: 0.5339  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 29560] train loss: 1.1135, train acc: 0.5443, val loss: 1.1266, val acc: 0.5393  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 29580] train loss: 1.1385, train acc: 0.5299, val loss: 1.1233, val acc: 0.5336  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 29600] train loss: 1.1461, train acc: 0.5264, val loss: 1.1238, val acc: 0.5390  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 29620] train loss: 1.1353, train acc: 0.5360, val loss: 1.1236, val acc: 0.5309  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 29640] train loss: 1.1171, train acc: 0.5380, val loss: 1.1235, val acc: 0.5339  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 29660] train loss: 1.1448, train acc: 0.5236, val loss: 1.1225, val acc: 0.5369  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 29680] train loss: 1.1370, train acc: 0.5341, val loss: 1.1219, val acc: 0.5349  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 29700] train loss: 1.1206, train acc: 0.5388, val loss: 1.1226, val acc: 0.5396  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 29720] train loss: 1.1483, train acc: 0.5207, val loss: 1.1228, val acc: 0.5386  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 29740] train loss: 1.1434, train acc: 0.5295, val loss: 1.1282, val acc: 0.5413  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 29760] train loss: 1.1297, train acc: 0.5380, val loss: 1.1235, val acc: 0.5339  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 29780] train loss: 1.1285, train acc: 0.5327, val loss: 1.1230, val acc: 0.5346  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 29800] train loss: 1.1248, train acc: 0.5391, val loss: 1.1227, val acc: 0.5379  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 29820] train loss: 1.1188, train acc: 0.5382, val loss: 1.1228, val acc: 0.5342  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 29840] train loss: 1.1325, train acc: 0.5320, val loss: 1.1231, val acc: 0.5390  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 29860] train loss: 1.1130, train acc: 0.5421, val loss: 1.1214, val acc: 0.5363  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 29880] train loss: 1.1427, train acc: 0.5297, val loss: 1.1192, val acc: 0.5363  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 29900] train loss: 1.1192, train acc: 0.5382, val loss: 1.1245, val acc: 0.5346  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 29920] train loss: 1.1253, train acc: 0.5343, val loss: 1.1182, val acc: 0.5406  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 29940] train loss: 1.1218, train acc: 0.5335, val loss: 1.1137, val acc: 0.5393  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 29960] train loss: 1.1132, train acc: 0.5380, val loss: 1.1139, val acc: 0.5359  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 29980] train loss: 1.1228, train acc: 0.5385, val loss: 1.1084, val acc: 0.5359  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 30000] train loss: 1.1132, train acc: 0.5415, val loss: 1.1108, val acc: 0.5376  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 30020] train loss: 1.1350, train acc: 0.5322, val loss: 1.1131, val acc: 0.5379  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 30040] train loss: 1.1176, train acc: 0.5377, val loss: 1.1074, val acc: 0.5396  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 30060] train loss: 1.1316, train acc: 0.5307, val loss: 1.1465, val acc: 0.5238  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 30080] train loss: 1.1167, train acc: 0.5348, val loss: 1.1160, val acc: 0.5359  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 30100] train loss: 1.0950, train acc: 0.5506, val loss: 1.1068, val acc: 0.5332  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 30120] train loss: 1.1442, train acc: 0.5211, val loss: 1.1375, val acc: 0.5275  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 30140] train loss: 1.1045, train acc: 0.5450, val loss: 1.1063, val acc: 0.5383  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 30160] train loss: 1.1073, train acc: 0.5427, val loss: 1.1018, val acc: 0.5420  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 30180] train loss: 1.1112, train acc: 0.5388, val loss: 1.0983, val acc: 0.5447  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 30200] train loss: 1.1038, train acc: 0.5436, val loss: 1.0976, val acc: 0.5457  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 30220] train loss: 1.0838, train acc: 0.5562, val loss: 1.0970, val acc: 0.5460  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 30240] train loss: 1.0960, train acc: 0.5450, val loss: 1.0945, val acc: 0.5481  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 30260] train loss: 1.1060, train acc: 0.5319, val loss: 1.1101, val acc: 0.5423  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 30280] train loss: 1.1111, train acc: 0.5346, val loss: 1.1064, val acc: 0.5491  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 30300] train loss: 1.1181, train acc: 0.5356, val loss: 1.0985, val acc: 0.5511  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 30320] train loss: 1.0851, train acc: 0.5579, val loss: 1.0940, val acc: 0.5531  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 30340] train loss: 1.0763, train acc: 0.5611, val loss: 1.0890, val acc: 0.5545  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 30360] train loss: 1.0872, train acc: 0.5504, val loss: 1.0941, val acc: 0.5497  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 30380] train loss: 1.0890, train acc: 0.5523, val loss: 1.1083, val acc: 0.5444  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 30400] train loss: 1.1514, train acc: 0.5312, val loss: 1.1404, val acc: 0.5369  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 30420] train loss: 1.1261, train acc: 0.5474, val loss: 1.1344, val acc: 0.5393  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 30440] train loss: 1.1446, train acc: 0.5350, val loss: 1.1340, val acc: 0.5427  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 30460] train loss: 1.1368, train acc: 0.5385, val loss: 1.1332, val acc: 0.5390  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 30480] train loss: 1.1342, train acc: 0.5362, val loss: 1.1337, val acc: 0.5329  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 30500] train loss: 1.1221, train acc: 0.5426, val loss: 1.1322, val acc: 0.5369  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 30520] train loss: 1.1240, train acc: 0.5416, val loss: 1.1316, val acc: 0.5410  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 30540] train loss: 1.1281, train acc: 0.5393, val loss: 1.1314, val acc: 0.5406  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 30560] train loss: 1.1394, train acc: 0.5317, val loss: 1.1309, val acc: 0.5420  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 30580] train loss: 1.1381, train acc: 0.5360, val loss: 1.1310, val acc: 0.5400  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 30600] train loss: 1.1433, train acc: 0.5320, val loss: 1.1305, val acc: 0.5403  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 30620] train loss: 1.1216, train acc: 0.5375, val loss: 1.1310, val acc: 0.5386  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 30640] train loss: 1.1236, train acc: 0.5438, val loss: 1.1299, val acc: 0.5420  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 30660] train loss: 1.1221, train acc: 0.5421, val loss: 1.1314, val acc: 0.5346  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 30680] train loss: 1.1228, train acc: 0.5422, val loss: 1.1298, val acc: 0.5420  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 30700] train loss: 1.1264, train acc: 0.5398, val loss: 1.1291, val acc: 0.5417  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 30720] train loss: 1.1211, train acc: 0.5408, val loss: 1.1290, val acc: 0.5413  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 30740] train loss: 1.1299, train acc: 0.5367, val loss: 1.1292, val acc: 0.5400  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 30760] train loss: 1.1217, train acc: 0.5400, val loss: 1.1293, val acc: 0.5406  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 30780] train loss: 1.1352, train acc: 0.5400, val loss: 1.1290, val acc: 0.5406  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 30800] train loss: 1.1198, train acc: 0.5458, val loss: 1.1284, val acc: 0.5423  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 30820] train loss: 1.1321, train acc: 0.5317, val loss: 1.1282, val acc: 0.5396  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 30840] train loss: 1.1346, train acc: 0.5349, val loss: 1.1285, val acc: 0.5393  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 30860] train loss: 1.1308, train acc: 0.5369, val loss: 1.1281, val acc: 0.5390  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 30880] train loss: 1.1270, train acc: 0.5395, val loss: 1.1287, val acc: 0.5379  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 30900] train loss: 1.1333, train acc: 0.5323, val loss: 1.1282, val acc: 0.5406  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 30920] train loss: 1.1176, train acc: 0.5402, val loss: 1.1282, val acc: 0.5406  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 30940] train loss: 1.1252, train acc: 0.5371, val loss: 1.1281, val acc: 0.5366  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 30960] train loss: 1.1317, train acc: 0.5396, val loss: 1.1294, val acc: 0.5383  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 30980] train loss: 1.1395, train acc: 0.5316, val loss: 1.1279, val acc: 0.5390  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 31000] train loss: 1.1396, train acc: 0.5352, val loss: 1.1276, val acc: 0.5400  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 31020] train loss: 1.1224, train acc: 0.5419, val loss: 1.1279, val acc: 0.5390  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 31040] train loss: 1.1217, train acc: 0.5374, val loss: 1.1276, val acc: 0.5403  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 31060] train loss: 1.1078, train acc: 0.5461, val loss: 1.1279, val acc: 0.5356  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 31080] train loss: 1.1287, train acc: 0.5392, val loss: 1.1277, val acc: 0.5386  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 31100] train loss: 1.1338, train acc: 0.5339, val loss: 1.1282, val acc: 0.5369  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 31120] train loss: 1.1201, train acc: 0.5393, val loss: 1.1274, val acc: 0.5400  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 31140] train loss: 1.1139, train acc: 0.5481, val loss: 1.1272, val acc: 0.5417  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 31160] train loss: 1.1208, train acc: 0.5439, val loss: 1.1275, val acc: 0.5363  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 31180] train loss: 1.1147, train acc: 0.5425, val loss: 1.1275, val acc: 0.5363  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 31200] train loss: 1.1166, train acc: 0.5448, val loss: 1.1274, val acc: 0.5400  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 31220] train loss: 1.1216, train acc: 0.5416, val loss: 1.1290, val acc: 0.5373  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 31240] train loss: 1.1345, train acc: 0.5346, val loss: 1.1272, val acc: 0.5403  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 31260] train loss: 1.1305, train acc: 0.5371, val loss: 1.1302, val acc: 0.5390  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 31280] train loss: 1.1201, train acc: 0.5410, val loss: 1.1268, val acc: 0.5383  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 31300] train loss: 1.1176, train acc: 0.5378, val loss: 1.1288, val acc: 0.5363  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 31320] train loss: 1.1243, train acc: 0.5405, val loss: 1.1269, val acc: 0.5410  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 31340] train loss: 1.1331, train acc: 0.5398, val loss: 1.1267, val acc: 0.5403  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 31360] train loss: 1.1451, train acc: 0.5276, val loss: 1.1272, val acc: 0.5373  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 31380] train loss: 1.1108, train acc: 0.5423, val loss: 1.1279, val acc: 0.5369  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 31400] train loss: 1.1130, train acc: 0.5476, val loss: 1.1264, val acc: 0.5433  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 31420] train loss: 1.1299, train acc: 0.5395, val loss: 1.1263, val acc: 0.5403  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 31440] train loss: 1.1110, train acc: 0.5442, val loss: 1.1262, val acc: 0.5403  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 31460] train loss: 1.1249, train acc: 0.5361, val loss: 1.1262, val acc: 0.5423  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 31480] train loss: 1.1231, train acc: 0.5364, val loss: 1.1259, val acc: 0.5400  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 31500] train loss: 1.1254, train acc: 0.5388, val loss: 1.1267, val acc: 0.5363  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 31520] train loss: 1.1419, train acc: 0.5297, val loss: 1.1270, val acc: 0.5373  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 31540] train loss: 1.1359, train acc: 0.5313, val loss: 1.1261, val acc: 0.5393  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 31560] train loss: 1.1389, train acc: 0.5336, val loss: 1.1259, val acc: 0.5373  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 31580] train loss: 1.1175, train acc: 0.5378, val loss: 1.1251, val acc: 0.5383  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 31600] train loss: 1.1298, train acc: 0.5363, val loss: 1.1260, val acc: 0.5366  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 31620] train loss: 1.1322, train acc: 0.5385, val loss: 1.1248, val acc: 0.5417  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 31640] train loss: 1.1466, train acc: 0.5275, val loss: 1.1267, val acc: 0.5329  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 31660] train loss: 1.1078, train acc: 0.5431, val loss: 1.1252, val acc: 0.5379  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 31680] train loss: 1.1224, train acc: 0.5388, val loss: 1.1243, val acc: 0.5400  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 31700] train loss: 1.1246, train acc: 0.5374, val loss: 1.1244, val acc: 0.5383  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 31720] train loss: 1.1329, train acc: 0.5401, val loss: 1.1244, val acc: 0.5390  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 31740] train loss: 1.1182, train acc: 0.5418, val loss: 1.1255, val acc: 0.5336  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 31760] train loss: 1.1130, train acc: 0.5433, val loss: 1.1253, val acc: 0.5373  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 31780] train loss: 1.1311, train acc: 0.5393, val loss: 1.1238, val acc: 0.5400  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 31800] train loss: 1.1256, train acc: 0.5380, val loss: 1.1245, val acc: 0.5363  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 31820] train loss: 1.1240, train acc: 0.5406, val loss: 1.1238, val acc: 0.5406  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 31840] train loss: 1.1164, train acc: 0.5408, val loss: 1.1237, val acc: 0.5400  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 31860] train loss: 1.1369, train acc: 0.5328, val loss: 1.1230, val acc: 0.5393  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 31880] train loss: 1.1254, train acc: 0.5356, val loss: 1.1174, val acc: 0.5406  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 31900] train loss: 1.1072, train acc: 0.5421, val loss: 1.1103, val acc: 0.5400  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 31920] train loss: 1.0980, train acc: 0.5437, val loss: 1.0986, val acc: 0.5447  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 31940] train loss: 1.1055, train acc: 0.5445, val loss: 1.0941, val acc: 0.5511  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 31960] train loss: 1.1159, train acc: 0.5314, val loss: 1.0950, val acc: 0.5477  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 31980] train loss: 1.1548, train acc: 0.5124, val loss: 1.1232, val acc: 0.5339  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 32000] train loss: 1.1178, train acc: 0.5381, val loss: 1.1169, val acc: 0.5413  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 32020] train loss: 1.0831, train acc: 0.5474, val loss: 1.0969, val acc: 0.5454  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 32040] train loss: 1.0932, train acc: 0.5455, val loss: 1.0916, val acc: 0.5464  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 32060] train loss: 1.0884, train acc: 0.5468, val loss: 1.0946, val acc: 0.5504  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 32080] train loss: 1.0871, train acc: 0.5532, val loss: 1.0898, val acc: 0.5568  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 32100] train loss: 1.0938, train acc: 0.5445, val loss: 1.0886, val acc: 0.5551  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 32120] train loss: 1.1038, train acc: 0.5442, val loss: 1.0884, val acc: 0.5575  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 32140] train loss: 1.1104, train acc: 0.5425, val loss: 1.1065, val acc: 0.5282  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 32160] train loss: 1.1393, train acc: 0.5221, val loss: 1.1182, val acc: 0.5420  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 32180] train loss: 1.0920, train acc: 0.5479, val loss: 1.0894, val acc: 0.5538  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 32200] train loss: 1.0942, train acc: 0.5457, val loss: 1.0866, val acc: 0.5535  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 32220] train loss: 1.0760, train acc: 0.5559, val loss: 1.0863, val acc: 0.5555  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 32240] train loss: 1.0952, train acc: 0.5455, val loss: 1.0850, val acc: 0.5565  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 32260] train loss: 1.0845, train acc: 0.5481, val loss: 1.0851, val acc: 0.5531  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 32280] train loss: 1.0974, train acc: 0.5464, val loss: 1.0847, val acc: 0.5558  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 32300] train loss: 1.0808, train acc: 0.5523, val loss: 1.0834, val acc: 0.5555  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 32320] train loss: 1.1036, train acc: 0.5409, val loss: 1.0832, val acc: 0.5555  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 32340] train loss: 1.1012, train acc: 0.5434, val loss: 1.0827, val acc: 0.5541  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 32360] train loss: 1.0895, train acc: 0.5464, val loss: 1.0832, val acc: 0.5555  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 32380] train loss: 1.0799, train acc: 0.5536, val loss: 1.0823, val acc: 0.5528  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 32400] train loss: 1.0791, train acc: 0.5523, val loss: 1.0816, val acc: 0.5541  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 32420] train loss: 1.0841, train acc: 0.5518, val loss: 1.0818, val acc: 0.5558  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 32440] train loss: 1.1012, train acc: 0.5448, val loss: 1.0840, val acc: 0.5541  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 32460] train loss: 1.0876, train acc: 0.5497, val loss: 1.0845, val acc: 0.5585  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 32480] train loss: 1.0656, train acc: 0.5598, val loss: 1.0817, val acc: 0.5545  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 32500] train loss: 1.0946, train acc: 0.5452, val loss: 1.0815, val acc: 0.5585  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 32520] train loss: 1.0831, train acc: 0.5534, val loss: 1.0813, val acc: 0.5535  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 32540] train loss: 1.0663, train acc: 0.5570, val loss: 1.0801, val acc: 0.5545  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 32560] train loss: 1.0820, train acc: 0.5535, val loss: 1.0836, val acc: 0.5555  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 32580] train loss: 1.0836, train acc: 0.5492, val loss: 1.0826, val acc: 0.5524  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 32600] train loss: 1.0891, train acc: 0.5507, val loss: 1.0810, val acc: 0.5585  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 32620] train loss: 1.0897, train acc: 0.5476, val loss: 1.0803, val acc: 0.5538  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 32640] train loss: 1.0876, train acc: 0.5518, val loss: 1.0798, val acc: 0.5578  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 32660] train loss: 1.0796, train acc: 0.5506, val loss: 1.0792, val acc: 0.5524  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 32680] train loss: 1.0825, train acc: 0.5543, val loss: 1.0801, val acc: 0.5565  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 32700] train loss: 1.0744, train acc: 0.5560, val loss: 1.0791, val acc: 0.5562  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 32720] train loss: 1.0853, train acc: 0.5496, val loss: 1.0790, val acc: 0.5565  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 32740] train loss: 1.0818, train acc: 0.5518, val loss: 1.0909, val acc: 0.5524  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 32760] train loss: 1.1024, train acc: 0.5474, val loss: 1.1213, val acc: 0.5363  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 32780] train loss: 1.1473, train acc: 0.5305, val loss: 1.1376, val acc: 0.5309  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 32800] train loss: 1.1281, train acc: 0.5422, val loss: 1.1309, val acc: 0.5420  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 32820] train loss: 1.1274, train acc: 0.5413, val loss: 1.1288, val acc: 0.5396  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 32840] train loss: 1.1330, train acc: 0.5356, val loss: 1.1291, val acc: 0.5400  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 32860] train loss: 1.1250, train acc: 0.5458, val loss: 1.1279, val acc: 0.5393  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 32880] train loss: 1.1285, train acc: 0.5401, val loss: 1.1262, val acc: 0.5417  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 32900] train loss: 1.1118, train acc: 0.5439, val loss: 1.1264, val acc: 0.5356  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 32920] train loss: 1.1232, train acc: 0.5416, val loss: 1.1253, val acc: 0.5417  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 32940] train loss: 1.1139, train acc: 0.5448, val loss: 1.1261, val acc: 0.5379  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 32960] train loss: 1.1234, train acc: 0.5463, val loss: 1.1245, val acc: 0.5417  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 32980] train loss: 1.1262, train acc: 0.5384, val loss: 1.1273, val acc: 0.5390  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 33000] train loss: 1.1240, train acc: 0.5440, val loss: 1.1241, val acc: 0.5410  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 33020] train loss: 1.1327, train acc: 0.5335, val loss: 1.1243, val acc: 0.5403  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 33040] train loss: 1.1171, train acc: 0.5424, val loss: 1.1242, val acc: 0.5376  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 33060] train loss: 1.1200, train acc: 0.5411, val loss: 1.1233, val acc: 0.5413  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 33080] train loss: 1.1176, train acc: 0.5403, val loss: 1.1234, val acc: 0.5393  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 33100] train loss: 1.1220, train acc: 0.5393, val loss: 1.1233, val acc: 0.5386  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 33120] train loss: 1.1179, train acc: 0.5430, val loss: 1.1259, val acc: 0.5383  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 33140] train loss: 1.1117, train acc: 0.5411, val loss: 1.1268, val acc: 0.5393  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 33160] train loss: 1.1150, train acc: 0.5438, val loss: 1.1226, val acc: 0.5430  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 33180] train loss: 1.1202, train acc: 0.5392, val loss: 1.1225, val acc: 0.5400  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 33200] train loss: 1.1216, train acc: 0.5425, val loss: 1.1223, val acc: 0.5430  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 33220] train loss: 1.1240, train acc: 0.5367, val loss: 1.1221, val acc: 0.5423  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 33240] train loss: 1.1072, train acc: 0.5467, val loss: 1.1283, val acc: 0.5406  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 33260] train loss: 1.1319, train acc: 0.5350, val loss: 1.1227, val acc: 0.5376  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 33280] train loss: 1.1013, train acc: 0.5471, val loss: 1.1056, val acc: 0.5423  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 33300] train loss: 1.1000, train acc: 0.5364, val loss: 1.0986, val acc: 0.5396  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 33320] train loss: 1.0673, train acc: 0.5623, val loss: 1.0880, val acc: 0.5454  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 33340] train loss: 1.0718, train acc: 0.5573, val loss: 1.0886, val acc: 0.5545  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 33360] train loss: 1.0993, train acc: 0.5398, val loss: 1.0853, val acc: 0.5518  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 33380] train loss: 1.0950, train acc: 0.5439, val loss: 1.0858, val acc: 0.5494  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 33400] train loss: 1.0949, train acc: 0.5456, val loss: 1.0995, val acc: 0.5346  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 33420] train loss: 1.0855, train acc: 0.5506, val loss: 1.1085, val acc: 0.5413  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 33440] train loss: 1.1013, train acc: 0.5425, val loss: 1.1008, val acc: 0.5535  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 33460] train loss: 1.0786, train acc: 0.5539, val loss: 1.0873, val acc: 0.5484  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 33480] train loss: 1.0748, train acc: 0.5592, val loss: 1.0864, val acc: 0.5501  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 33500] train loss: 1.0855, train acc: 0.5524, val loss: 1.0834, val acc: 0.5551  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 33520] train loss: 1.0804, train acc: 0.5502, val loss: 1.0835, val acc: 0.5568  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 33540] train loss: 1.0900, train acc: 0.5469, val loss: 1.0867, val acc: 0.5460  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 33560] train loss: 1.0821, train acc: 0.5531, val loss: 1.0828, val acc: 0.5562  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 33580] train loss: 1.0748, train acc: 0.5549, val loss: 1.0819, val acc: 0.5551  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 33600] train loss: 1.0927, train acc: 0.5430, val loss: 1.1053, val acc: 0.5319  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 33620] train loss: 1.1083, train acc: 0.5384, val loss: 1.1079, val acc: 0.5376  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 33640] train loss: 1.0974, train acc: 0.5417, val loss: 1.0842, val acc: 0.5535  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 33660] train loss: 1.0850, train acc: 0.5508, val loss: 1.0934, val acc: 0.5578  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 33680] train loss: 1.1203, train acc: 0.5482, val loss: 1.1309, val acc: 0.5396  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 33700] train loss: 1.1157, train acc: 0.5440, val loss: 1.1270, val acc: 0.5400  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 33720] train loss: 1.0971, train acc: 0.5424, val loss: 1.0883, val acc: 0.5551  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 33740] train loss: 1.0868, train acc: 0.5583, val loss: 1.0851, val acc: 0.5592  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 33760] train loss: 1.1115, train acc: 0.5387, val loss: 1.0833, val acc: 0.5558  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 33780] train loss: 1.0770, train acc: 0.5534, val loss: 1.0834, val acc: 0.5558  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 33800] train loss: 1.0815, train acc: 0.5527, val loss: 1.0812, val acc: 0.5578  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 33820] train loss: 1.0805, train acc: 0.5503, val loss: 1.0808, val acc: 0.5558  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 33840] train loss: 1.0783, train acc: 0.5510, val loss: 1.0809, val acc: 0.5592  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 33860] train loss: 1.0572, train acc: 0.5700, val loss: 1.0807, val acc: 0.5572  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 33880] train loss: 1.0777, train acc: 0.5586, val loss: 1.0824, val acc: 0.5575  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 33900] train loss: 1.0979, train acc: 0.5418, val loss: 1.0802, val acc: 0.5572  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 33920] train loss: 1.0773, train acc: 0.5576, val loss: 1.0808, val acc: 0.5589  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 33940] train loss: 1.0708, train acc: 0.5562, val loss: 1.0801, val acc: 0.5531  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 33960] train loss: 1.0554, train acc: 0.5635, val loss: 1.0806, val acc: 0.5558  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 33980] train loss: 1.0850, train acc: 0.5491, val loss: 1.0819, val acc: 0.5558  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 34000] train loss: 1.0828, train acc: 0.5505, val loss: 1.0801, val acc: 0.5568  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 34020] train loss: 1.0714, train acc: 0.5591, val loss: 1.0779, val acc: 0.5555  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 34040] train loss: 1.0784, train acc: 0.5523, val loss: 1.0778, val acc: 0.5538  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 34060] train loss: 1.0689, train acc: 0.5565, val loss: 1.0774, val acc: 0.5541  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 34080] train loss: 1.0852, train acc: 0.5512, val loss: 1.0791, val acc: 0.5514  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 34100] train loss: 1.0726, train acc: 0.5533, val loss: 1.0806, val acc: 0.5582  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 34120] train loss: 1.0863, train acc: 0.5489, val loss: 1.0790, val acc: 0.5541  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 34140] train loss: 1.0850, train acc: 0.5498, val loss: 1.0799, val acc: 0.5609  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 34160] train loss: 1.0695, train acc: 0.5539, val loss: 1.0775, val acc: 0.5589  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 34180] train loss: 1.0715, train acc: 0.5580, val loss: 1.0774, val acc: 0.5568  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 34200] train loss: 1.0718, train acc: 0.5541, val loss: 1.0768, val acc: 0.5582  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 34220] train loss: 1.0833, train acc: 0.5497, val loss: 1.0758, val acc: 0.5575  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 34240] train loss: 1.0839, train acc: 0.5510, val loss: 1.0756, val acc: 0.5568  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 34260] train loss: 1.1015, train acc: 0.5364, val loss: 1.1230, val acc: 0.5363  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 34280] train loss: 1.1967, train acc: 0.5058, val loss: 1.2577, val acc: 0.4715  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 34300] train loss: 1.1329, train acc: 0.5336, val loss: 1.1432, val acc: 0.5248  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 34320] train loss: 1.1336, train acc: 0.5402, val loss: 1.1365, val acc: 0.5403  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 34340] train loss: 1.1337, train acc: 0.5385, val loss: 1.1334, val acc: 0.5413  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 34360] train loss: 1.1204, train acc: 0.5461, val loss: 1.1332, val acc: 0.5403  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 34380] train loss: 1.1159, train acc: 0.5445, val loss: 1.1319, val acc: 0.5393  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 34400] train loss: 1.1312, train acc: 0.5366, val loss: 1.1303, val acc: 0.5369  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 34420] train loss: 1.1362, train acc: 0.5330, val loss: 1.1299, val acc: 0.5403  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 34440] train loss: 1.1096, train acc: 0.5495, val loss: 1.1293, val acc: 0.5383  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 34460] train loss: 1.1123, train acc: 0.5468, val loss: 1.1295, val acc: 0.5400  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 34480] train loss: 1.1246, train acc: 0.5447, val loss: 1.1301, val acc: 0.5376  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 34500] train loss: 1.1165, train acc: 0.5429, val loss: 1.1306, val acc: 0.5366  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 34520] train loss: 1.1154, train acc: 0.5446, val loss: 1.1290, val acc: 0.5406  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 34540] train loss: 1.1313, train acc: 0.5351, val loss: 1.1289, val acc: 0.5390  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 34560] train loss: 1.1272, train acc: 0.5378, val loss: 1.1288, val acc: 0.5410  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 34580] train loss: 1.1315, train acc: 0.5362, val loss: 1.1289, val acc: 0.5386  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 34600] train loss: 1.1461, train acc: 0.5336, val loss: 1.1289, val acc: 0.5400  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 34620] train loss: 1.1193, train acc: 0.5445, val loss: 1.1290, val acc: 0.5400  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 34640] train loss: 1.1468, train acc: 0.5301, val loss: 1.1288, val acc: 0.5393  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 34660] train loss: 1.1258, train acc: 0.5383, val loss: 1.1288, val acc: 0.5413  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 34680] train loss: 1.1161, train acc: 0.5474, val loss: 1.1291, val acc: 0.5352  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 34700] train loss: 1.1381, train acc: 0.5350, val loss: 1.1288, val acc: 0.5396  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 34720] train loss: 1.1343, train acc: 0.5369, val loss: 1.1286, val acc: 0.5390  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 34740] train loss: 1.1064, train acc: 0.5507, val loss: 1.1293, val acc: 0.5363  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 34760] train loss: 1.1230, train acc: 0.5424, val loss: 1.1285, val acc: 0.5400  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 34780] train loss: 1.1405, train acc: 0.5311, val loss: 1.1292, val acc: 0.5376  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 34800] train loss: 1.1314, train acc: 0.5382, val loss: 1.1285, val acc: 0.5420  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 34820] train loss: 1.1010, train acc: 0.5518, val loss: 1.1285, val acc: 0.5417  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 34840] train loss: 1.1201, train acc: 0.5406, val loss: 1.1283, val acc: 0.5396  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 34860] train loss: 1.1163, train acc: 0.5455, val loss: 1.1295, val acc: 0.5390  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 34880] train loss: 1.1404, train acc: 0.5335, val loss: 1.1296, val acc: 0.5376  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 34900] train loss: 1.1330, train acc: 0.5396, val loss: 1.1292, val acc: 0.5390  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 34920] train loss: 1.1201, train acc: 0.5427, val loss: 1.1284, val acc: 0.5410  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 34940] train loss: 1.1284, train acc: 0.5398, val loss: 1.1332, val acc: 0.5417  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 34960] train loss: 1.1093, train acc: 0.5484, val loss: 1.1303, val acc: 0.5390  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 34980] train loss: 1.1236, train acc: 0.5408, val loss: 1.1287, val acc: 0.5396  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 35000] train loss: 1.1176, train acc: 0.5392, val loss: 1.1306, val acc: 0.5396  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 35020] train loss: 1.1257, train acc: 0.5424, val loss: 1.1291, val acc: 0.5383  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 35040] train loss: 1.1373, train acc: 0.5346, val loss: 1.1295, val acc: 0.5379  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 35060] train loss: 1.1314, train acc: 0.5350, val loss: 1.1285, val acc: 0.5390  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 35080] train loss: 1.1128, train acc: 0.5433, val loss: 1.1282, val acc: 0.5406  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 35100] train loss: 1.1322, train acc: 0.5374, val loss: 1.1284, val acc: 0.5396  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 35120] train loss: 1.1179, train acc: 0.5432, val loss: 1.1287, val acc: 0.5406  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 35140] train loss: 1.1280, train acc: 0.5415, val loss: 1.1285, val acc: 0.5400  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 35160] train loss: 1.1235, train acc: 0.5436, val loss: 1.1293, val acc: 0.5386  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 35180] train loss: 1.1244, train acc: 0.5427, val loss: 1.1282, val acc: 0.5427  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 35200] train loss: 1.1190, train acc: 0.5474, val loss: 1.1281, val acc: 0.5390  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 35220] train loss: 1.1279, train acc: 0.5355, val loss: 1.1289, val acc: 0.5386  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 35240] train loss: 1.1245, train acc: 0.5442, val loss: 1.1298, val acc: 0.5383  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 35260] train loss: 1.1185, train acc: 0.5424, val loss: 1.1287, val acc: 0.5406  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 35280] train loss: 1.1163, train acc: 0.5452, val loss: 1.1281, val acc: 0.5400  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 35300] train loss: 1.1155, train acc: 0.5440, val loss: 1.1281, val acc: 0.5390  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 35320] train loss: 1.1289, train acc: 0.5343, val loss: 1.1284, val acc: 0.5383  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 35340] train loss: 1.1322, train acc: 0.5384, val loss: 1.1280, val acc: 0.5390  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 35360] train loss: 1.1265, train acc: 0.5402, val loss: 1.1293, val acc: 0.5332  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 35380] train loss: 1.1180, train acc: 0.5369, val loss: 1.1279, val acc: 0.5417  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 35400] train loss: 1.1153, train acc: 0.5505, val loss: 1.1289, val acc: 0.5359  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 35420] train loss: 1.1117, train acc: 0.5483, val loss: 1.1280, val acc: 0.5406  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 35440] train loss: 1.1253, train acc: 0.5382, val loss: 1.1280, val acc: 0.5352  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 35460] train loss: 1.1420, train acc: 0.5290, val loss: 1.1285, val acc: 0.5356  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 35480] train loss: 1.1221, train acc: 0.5404, val loss: 1.1284, val acc: 0.5403  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 35500] train loss: 1.1218, train acc: 0.5393, val loss: 1.1293, val acc: 0.5379  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 35520] train loss: 1.1222, train acc: 0.5411, val loss: 1.1279, val acc: 0.5390  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 35540] train loss: 1.1192, train acc: 0.5436, val loss: 1.1280, val acc: 0.5393  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 35560] train loss: 1.1256, train acc: 0.5434, val loss: 1.1284, val acc: 0.5396  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 35580] train loss: 1.1228, train acc: 0.5396, val loss: 1.1280, val acc: 0.5403  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 35600] train loss: 1.1287, train acc: 0.5388, val loss: 1.1281, val acc: 0.5396  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 35620] train loss: 1.1241, train acc: 0.5393, val loss: 1.1278, val acc: 0.5423  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 35640] train loss: 1.1206, train acc: 0.5452, val loss: 1.1279, val acc: 0.5417  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 35660] train loss: 1.1276, train acc: 0.5399, val loss: 1.1283, val acc: 0.5423  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 35680] train loss: 1.1305, train acc: 0.5390, val loss: 1.1280, val acc: 0.5413  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 35700] train loss: 1.1423, train acc: 0.5312, val loss: 1.1279, val acc: 0.5393  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 35720] train loss: 1.1247, train acc: 0.5403, val loss: 1.1284, val acc: 0.5386  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 35740] train loss: 1.1304, train acc: 0.5407, val loss: 1.1307, val acc: 0.5383  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 35760] train loss: 1.1156, train acc: 0.5479, val loss: 1.1278, val acc: 0.5373  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 35780] train loss: 1.1313, train acc: 0.5315, val loss: 1.1278, val acc: 0.5406  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 35800] train loss: 1.1349, train acc: 0.5359, val loss: 1.1285, val acc: 0.5386  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 35820] train loss: 1.1397, train acc: 0.5320, val loss: 1.1292, val acc: 0.5379  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 35840] train loss: 1.1223, train acc: 0.5382, val loss: 1.1278, val acc: 0.5423  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 35860] train loss: 1.1313, train acc: 0.5422, val loss: 1.1289, val acc: 0.5339  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 35880] train loss: 1.1313, train acc: 0.5359, val loss: 1.1277, val acc: 0.5423  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 35900] train loss: 1.1179, train acc: 0.5423, val loss: 1.1279, val acc: 0.5417  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 35920] train loss: 1.1182, train acc: 0.5432, val loss: 1.1290, val acc: 0.5376  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 35940] train loss: 1.1270, train acc: 0.5377, val loss: 1.1277, val acc: 0.5423  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 35960] train loss: 1.1323, train acc: 0.5371, val loss: 1.1283, val acc: 0.5390  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 35980] train loss: 1.1324, train acc: 0.5361, val loss: 1.1276, val acc: 0.5390  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 36000] train loss: 1.1146, train acc: 0.5475, val loss: 1.1280, val acc: 0.5390  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 36020] train loss: 1.1163, train acc: 0.5422, val loss: 1.1278, val acc: 0.5390  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 36040] train loss: 1.1331, train acc: 0.5330, val loss: 1.1278, val acc: 0.5379  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 36060] train loss: 1.1313, train acc: 0.5362, val loss: 1.1278, val acc: 0.5383  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 36080] train loss: 1.1227, train acc: 0.5436, val loss: 1.1295, val acc: 0.5359  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 36100] train loss: 1.1126, train acc: 0.5489, val loss: 1.1280, val acc: 0.5383  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 36120] train loss: 1.1166, train acc: 0.5460, val loss: 1.1284, val acc: 0.5379  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 36140] train loss: 1.1243, train acc: 0.5380, val loss: 1.1324, val acc: 0.5406  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 36160] train loss: 1.1276, train acc: 0.5387, val loss: 1.1290, val acc: 0.5400  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 36180] train loss: 1.1188, train acc: 0.5405, val loss: 1.1276, val acc: 0.5410  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 36200] train loss: 1.1277, train acc: 0.5334, val loss: 1.1281, val acc: 0.5386  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 36220] train loss: 1.1168, train acc: 0.5440, val loss: 1.1282, val acc: 0.5393  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 36240] train loss: 1.1156, train acc: 0.5424, val loss: 1.1278, val acc: 0.5400  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 36260] train loss: 1.1374, train acc: 0.5337, val loss: 1.1277, val acc: 0.5393  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 36280] train loss: 1.1192, train acc: 0.5416, val loss: 1.1279, val acc: 0.5413  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 36300] train loss: 1.1169, train acc: 0.5453, val loss: 1.1281, val acc: 0.5386  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 36320] train loss: 1.1223, train acc: 0.5434, val loss: 1.1289, val acc: 0.5400  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 36340] train loss: 1.1257, train acc: 0.5387, val loss: 1.1278, val acc: 0.5369  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 36360] train loss: 1.1241, train acc: 0.5419, val loss: 1.1280, val acc: 0.5386  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 36380] train loss: 1.1268, train acc: 0.5352, val loss: 1.1300, val acc: 0.5400  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 36400] train loss: 1.1193, train acc: 0.5452, val loss: 1.1279, val acc: 0.5433  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 36420] train loss: 1.1076, train acc: 0.5479, val loss: 1.1277, val acc: 0.5403  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 36440] train loss: 1.1215, train acc: 0.5461, val loss: 1.1279, val acc: 0.5376  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 36460] train loss: 1.1354, train acc: 0.5341, val loss: 1.1286, val acc: 0.5393  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 36480] train loss: 1.1273, train acc: 0.5406, val loss: 1.1286, val acc: 0.5393  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 36500] train loss: 1.1118, train acc: 0.5459, val loss: 1.1283, val acc: 0.5390  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 36520] train loss: 1.1273, train acc: 0.5369, val loss: 1.1288, val acc: 0.5356  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 36540] train loss: 1.1286, train acc: 0.5369, val loss: 1.1275, val acc: 0.5390  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 36560] train loss: 1.1163, train acc: 0.5439, val loss: 1.1279, val acc: 0.5393  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 36580] train loss: 1.1129, train acc: 0.5497, val loss: 1.1283, val acc: 0.5383  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 36600] train loss: 1.1236, train acc: 0.5390, val loss: 1.1284, val acc: 0.5373  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 36620] train loss: 1.1271, train acc: 0.5410, val loss: 1.1276, val acc: 0.5393  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 36640] train loss: 1.1353, train acc: 0.5376, val loss: 1.1277, val acc: 0.5386  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 36660] train loss: 1.1090, train acc: 0.5459, val loss: 1.1276, val acc: 0.5390  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 36680] train loss: 1.1260, train acc: 0.5401, val loss: 1.1288, val acc: 0.5336  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 36700] train loss: 1.1327, train acc: 0.5344, val loss: 1.1274, val acc: 0.5386  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 36720] train loss: 1.1388, train acc: 0.5324, val loss: 1.1273, val acc: 0.5410  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 36740] train loss: 1.1181, train acc: 0.5411, val loss: 1.1277, val acc: 0.5386  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 36760] train loss: 1.1288, train acc: 0.5376, val loss: 1.1292, val acc: 0.5383  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 36780] train loss: 1.1315, train acc: 0.5381, val loss: 1.1279, val acc: 0.5403  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 36800] train loss: 1.1150, train acc: 0.5442, val loss: 1.1280, val acc: 0.5383  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 36820] train loss: 1.1306, train acc: 0.5383, val loss: 1.1275, val acc: 0.5396  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 36840] train loss: 1.1299, train acc: 0.5368, val loss: 1.1275, val acc: 0.5386  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 36860] train loss: 1.1141, train acc: 0.5508, val loss: 1.1275, val acc: 0.5369  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 36880] train loss: 1.1054, train acc: 0.5547, val loss: 1.1277, val acc: 0.5376  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 36900] train loss: 1.1280, train acc: 0.5423, val loss: 1.1276, val acc: 0.5379  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 36920] train loss: 1.1248, train acc: 0.5414, val loss: 1.1275, val acc: 0.5379  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 36940] train loss: 1.1195, train acc: 0.5467, val loss: 1.1273, val acc: 0.5410  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 36960] train loss: 1.1078, train acc: 0.5496, val loss: 1.1277, val acc: 0.5369  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 36980] train loss: 1.1294, train acc: 0.5377, val loss: 1.1275, val acc: 0.5373  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 37000] train loss: 1.1050, train acc: 0.5489, val loss: 1.1298, val acc: 0.5325  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 37020] train loss: 1.1249, train acc: 0.5368, val loss: 1.1275, val acc: 0.5376  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 37040] train loss: 1.1215, train acc: 0.5400, val loss: 1.1271, val acc: 0.5427  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 37060] train loss: 1.1134, train acc: 0.5457, val loss: 1.1298, val acc: 0.5406  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 37080] train loss: 1.1153, train acc: 0.5458, val loss: 1.1275, val acc: 0.5386  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 37100] train loss: 1.1204, train acc: 0.5367, val loss: 1.1282, val acc: 0.5359  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 37120] train loss: 1.1210, train acc: 0.5437, val loss: 1.1300, val acc: 0.5393  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 37140] train loss: 1.1155, train acc: 0.5415, val loss: 1.1287, val acc: 0.5369  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 37160] train loss: 1.1245, train acc: 0.5412, val loss: 1.1281, val acc: 0.5369  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 37180] train loss: 1.1077, train acc: 0.5483, val loss: 1.1278, val acc: 0.5379  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 37200] train loss: 1.1375, train acc: 0.5355, val loss: 1.1281, val acc: 0.5386  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 37220] train loss: 1.1238, train acc: 0.5464, val loss: 1.1274, val acc: 0.5390  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 37240] train loss: 1.1086, train acc: 0.5491, val loss: 1.1278, val acc: 0.5373  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 37260] train loss: 1.1171, train acc: 0.5458, val loss: 1.1274, val acc: 0.5376  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 37280] train loss: 1.1092, train acc: 0.5463, val loss: 1.1276, val acc: 0.5383  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 37300] train loss: 1.1322, train acc: 0.5318, val loss: 1.1275, val acc: 0.5369  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 37320] train loss: 1.1297, train acc: 0.5360, val loss: 1.1284, val acc: 0.5366  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 37340] train loss: 1.1307, train acc: 0.5392, val loss: 1.1275, val acc: 0.5359  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 37360] train loss: 1.1226, train acc: 0.5409, val loss: 1.1272, val acc: 0.5390  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 37380] train loss: 1.1148, train acc: 0.5468, val loss: 1.1270, val acc: 0.5410  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 37400] train loss: 1.1199, train acc: 0.5431, val loss: 1.1272, val acc: 0.5413  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 37420] train loss: 1.1124, train acc: 0.5464, val loss: 1.1275, val acc: 0.5417  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 37440] train loss: 1.1203, train acc: 0.5443, val loss: 1.1298, val acc: 0.5400  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 37460] train loss: 1.1147, train acc: 0.5448, val loss: 1.1275, val acc: 0.5359  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 37480] train loss: 1.1176, train acc: 0.5428, val loss: 1.1272, val acc: 0.5383  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 37500] train loss: 1.1204, train acc: 0.5403, val loss: 1.1289, val acc: 0.5396  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 37520] train loss: 1.1261, train acc: 0.5432, val loss: 1.1275, val acc: 0.5386  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 37540] train loss: 1.1178, train acc: 0.5464, val loss: 1.1270, val acc: 0.5390  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 37560] train loss: 1.1256, train acc: 0.5437, val loss: 1.1267, val acc: 0.5420  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 37580] train loss: 1.1248, train acc: 0.5425, val loss: 1.1293, val acc: 0.5386  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 37600] train loss: 1.1179, train acc: 0.5403, val loss: 1.1285, val acc: 0.5379  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 37620] train loss: 1.1135, train acc: 0.5502, val loss: 1.1281, val acc: 0.5359  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 37640] train loss: 1.1348, train acc: 0.5337, val loss: 1.1270, val acc: 0.5430  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 37660] train loss: 1.1263, train acc: 0.5395, val loss: 1.1274, val acc: 0.5386  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 37680] train loss: 1.1372, train acc: 0.5318, val loss: 1.1276, val acc: 0.5393  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 37700] train loss: 1.1219, train acc: 0.5388, val loss: 1.1282, val acc: 0.5342  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 37720] train loss: 1.1370, train acc: 0.5348, val loss: 1.1272, val acc: 0.5410  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 37740] train loss: 1.1134, train acc: 0.5406, val loss: 1.1278, val acc: 0.5390  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 37760] train loss: 1.1234, train acc: 0.5401, val loss: 1.1276, val acc: 0.5366  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 37780] train loss: 1.1390, train acc: 0.5361, val loss: 1.1267, val acc: 0.5413  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 37800] train loss: 1.1193, train acc: 0.5395, val loss: 1.1284, val acc: 0.5390  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 37820] train loss: 1.1069, train acc: 0.5461, val loss: 1.1267, val acc: 0.5406  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 37840] train loss: 1.1082, train acc: 0.5496, val loss: 1.1269, val acc: 0.5417  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 37860] train loss: 1.1429, train acc: 0.5309, val loss: 1.1266, val acc: 0.5406  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 37880] train loss: 1.1138, train acc: 0.5463, val loss: 1.1265, val acc: 0.5383  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 37900] train loss: 1.1129, train acc: 0.5458, val loss: 1.1264, val acc: 0.5390  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 37920] train loss: 1.1209, train acc: 0.5419, val loss: 1.1271, val acc: 0.5379  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 37940] train loss: 1.1100, train acc: 0.5441, val loss: 1.1267, val acc: 0.5386  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 37960] train loss: 1.1226, train acc: 0.5414, val loss: 1.1269, val acc: 0.5359  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 37980] train loss: 1.1140, train acc: 0.5474, val loss: 1.1262, val acc: 0.5400  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 38000] train loss: 1.1291, train acc: 0.5385, val loss: 1.1280, val acc: 0.5390  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 38020] train loss: 1.1187, train acc: 0.5389, val loss: 1.1264, val acc: 0.5369  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 38040] train loss: 1.1195, train acc: 0.5419, val loss: 1.1268, val acc: 0.5390  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 38060] train loss: 1.1119, train acc: 0.5487, val loss: 1.1294, val acc: 0.5332  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 38080] train loss: 1.1110, train acc: 0.5445, val loss: 1.1261, val acc: 0.5410  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 38100] train loss: 1.1206, train acc: 0.5424, val loss: 1.1257, val acc: 0.5400  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 38120] train loss: 1.1238, train acc: 0.5383, val loss: 1.1264, val acc: 0.5406  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 38140] train loss: 1.1215, train acc: 0.5445, val loss: 1.1260, val acc: 0.5363  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 38160] train loss: 1.1226, train acc: 0.5431, val loss: 1.1255, val acc: 0.5390  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 38180] train loss: 1.1247, train acc: 0.5368, val loss: 1.1254, val acc: 0.5393  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 38200] train loss: 1.1149, train acc: 0.5467, val loss: 1.1269, val acc: 0.5376  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 38220] train loss: 1.1346, train acc: 0.5341, val loss: 1.1251, val acc: 0.5403  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 38240] train loss: 1.1081, train acc: 0.5471, val loss: 1.1262, val acc: 0.5383  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 38260] train loss: 1.1157, train acc: 0.5427, val loss: 1.1253, val acc: 0.5403  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 38280] train loss: 1.1193, train acc: 0.5415, val loss: 1.1249, val acc: 0.5400  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 38300] train loss: 1.1162, train acc: 0.5434, val loss: 1.1257, val acc: 0.5369  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 38320] train loss: 1.1285, train acc: 0.5371, val loss: 1.1266, val acc: 0.5400  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 38340] train loss: 1.1137, train acc: 0.5426, val loss: 1.1252, val acc: 0.5410  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 38360] train loss: 1.1133, train acc: 0.5427, val loss: 1.1262, val acc: 0.5393  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 38380] train loss: 1.1148, train acc: 0.5408, val loss: 1.1245, val acc: 0.5400  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 38400] train loss: 1.1080, train acc: 0.5468, val loss: 1.1243, val acc: 0.5400  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 38420] train loss: 1.1235, train acc: 0.5433, val loss: 1.1275, val acc: 0.5417  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 38440] train loss: 1.1215, train acc: 0.5412, val loss: 1.1243, val acc: 0.5359  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 38460] train loss: 1.1236, train acc: 0.5382, val loss: 1.1264, val acc: 0.5386  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 38480] train loss: 1.1252, train acc: 0.5389, val loss: 1.1249, val acc: 0.5390  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 38500] train loss: 1.1181, train acc: 0.5408, val loss: 1.1254, val acc: 0.5369  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 38520] train loss: 1.1124, train acc: 0.5463, val loss: 1.1245, val acc: 0.5369  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 38540] train loss: 1.1301, train acc: 0.5362, val loss: 1.1252, val acc: 0.5390  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 38560] train loss: 1.1090, train acc: 0.5484, val loss: 1.1243, val acc: 0.5373  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 38580] train loss: 1.0980, train acc: 0.5536, val loss: 1.1236, val acc: 0.5376  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 38600] train loss: 1.1308, train acc: 0.5388, val loss: 1.1233, val acc: 0.5413  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 38620] train loss: 1.1275, train acc: 0.5383, val loss: 1.1231, val acc: 0.5386  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 38640] train loss: 1.1170, train acc: 0.5427, val loss: 1.1269, val acc: 0.5403  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 38660] train loss: 1.1051, train acc: 0.5499, val loss: 1.1241, val acc: 0.5420  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 38680] train loss: 1.1136, train acc: 0.5459, val loss: 1.1227, val acc: 0.5379  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 38700] train loss: 1.1094, train acc: 0.5437, val loss: 1.1206, val acc: 0.5403  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 38720] train loss: 1.1229, train acc: 0.5376, val loss: 1.1229, val acc: 0.5403  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 38740] train loss: 1.1399, train acc: 0.5324, val loss: 1.1264, val acc: 0.5349  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 38760] train loss: 1.1286, train acc: 0.5394, val loss: 1.1192, val acc: 0.5403  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 38780] train loss: 1.1059, train acc: 0.5451, val loss: 1.1216, val acc: 0.5363  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 38800] train loss: 1.1344, train acc: 0.5334, val loss: 1.1186, val acc: 0.5420  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 38820] train loss: 1.1137, train acc: 0.5397, val loss: 1.1213, val acc: 0.5336  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 38840] train loss: 1.1171, train acc: 0.5404, val loss: 1.1174, val acc: 0.5393  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 38860] train loss: 1.1314, train acc: 0.5358, val loss: 1.1189, val acc: 0.5420  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 38880] train loss: 1.1012, train acc: 0.5441, val loss: 1.1205, val acc: 0.5430  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 38900] train loss: 1.1096, train acc: 0.5421, val loss: 1.1182, val acc: 0.5423  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 38920] train loss: 1.1077, train acc: 0.5454, val loss: 1.1154, val acc: 0.5417  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 38940] train loss: 1.1109, train acc: 0.5434, val loss: 1.1171, val acc: 0.5363  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 38960] train loss: 1.1109, train acc: 0.5420, val loss: 1.1149, val acc: 0.5410  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 38980] train loss: 1.0957, train acc: 0.5444, val loss: 1.1132, val acc: 0.5342  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 39000] train loss: 1.1106, train acc: 0.5394, val loss: 1.1124, val acc: 0.5413  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 39020] train loss: 1.1257, train acc: 0.5335, val loss: 1.1121, val acc: 0.5420  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 39040] train loss: 1.1263, train acc: 0.5338, val loss: 1.1109, val acc: 0.5417  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 39060] train loss: 1.0993, train acc: 0.5476, val loss: 1.1117, val acc: 0.5433  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 39080] train loss: 1.1188, train acc: 0.5404, val loss: 1.1123, val acc: 0.5423  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 39100] train loss: 1.1036, train acc: 0.5429, val loss: 1.1109, val acc: 0.5363  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 39120] train loss: 1.0994, train acc: 0.5470, val loss: 1.1077, val acc: 0.5403  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 39140] train loss: 1.1121, train acc: 0.5429, val loss: 1.1069, val acc: 0.5417  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 39160] train loss: 1.1106, train acc: 0.5439, val loss: 1.1061, val acc: 0.5410  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 39180] train loss: 1.1090, train acc: 0.5418, val loss: 1.1071, val acc: 0.5430  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 39200] train loss: 1.1083, train acc: 0.5411, val loss: 1.1045, val acc: 0.5390  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 39220] train loss: 1.1006, train acc: 0.5440, val loss: 1.1045, val acc: 0.5420  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 39240] train loss: 1.1070, train acc: 0.5416, val loss: 1.1044, val acc: 0.5444  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 39260] train loss: 1.1064, train acc: 0.5400, val loss: 1.1028, val acc: 0.5369  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 39280] train loss: 1.1139, train acc: 0.5367, val loss: 1.1035, val acc: 0.5379  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 39300] train loss: 1.0918, train acc: 0.5494, val loss: 1.1012, val acc: 0.5423  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 39320] train loss: 1.0966, train acc: 0.5476, val loss: 1.1024, val acc: 0.5447  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 39340] train loss: 1.1012, train acc: 0.5416, val loss: 1.0999, val acc: 0.5444  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 39360] train loss: 1.0949, train acc: 0.5430, val loss: 1.1000, val acc: 0.5433  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 39380] train loss: 1.0995, train acc: 0.5427, val loss: 1.0978, val acc: 0.5410  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 39400] train loss: 1.0999, train acc: 0.5426, val loss: 1.1032, val acc: 0.5447  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 39420] train loss: 1.1051, train acc: 0.5382, val loss: 1.1003, val acc: 0.5319  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 39440] train loss: 1.1061, train acc: 0.5388, val loss: 1.0961, val acc: 0.5447  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 39460] train loss: 1.0964, train acc: 0.5447, val loss: 1.0952, val acc: 0.5433  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 39480] train loss: 1.0855, train acc: 0.5508, val loss: 1.0957, val acc: 0.5420  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 39500] train loss: 1.0895, train acc: 0.5493, val loss: 1.0957, val acc: 0.5444  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 39520] train loss: 1.0995, train acc: 0.5446, val loss: 1.1016, val acc: 0.5470  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 39540] train loss: 1.1129, train acc: 0.5382, val loss: 1.0935, val acc: 0.5440  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 39560] train loss: 1.1010, train acc: 0.5432, val loss: 1.0966, val acc: 0.5325  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 39580] train loss: 1.0873, train acc: 0.5490, val loss: 1.0920, val acc: 0.5454  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 39600] train loss: 1.0767, train acc: 0.5542, val loss: 1.0940, val acc: 0.5474  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 39620] train loss: 1.1006, train acc: 0.5428, val loss: 1.0912, val acc: 0.5403  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 39640] train loss: 1.0723, train acc: 0.5675, val loss: 1.0941, val acc: 0.5400  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 39660] train loss: 1.0909, train acc: 0.5476, val loss: 1.0889, val acc: 0.5454  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 39680] train loss: 1.0809, train acc: 0.5581, val loss: 1.0880, val acc: 0.5474  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 39700] train loss: 1.0804, train acc: 0.5601, val loss: 1.0869, val acc: 0.5497  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 39720] train loss: 1.0822, train acc: 0.5496, val loss: 1.0869, val acc: 0.5521  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 39740] train loss: 1.0962, train acc: 0.5424, val loss: 1.0859, val acc: 0.5524  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 39760] train loss: 1.0743, train acc: 0.5545, val loss: 1.0869, val acc: 0.5551  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 39780] train loss: 1.0971, train acc: 0.5432, val loss: 1.0850, val acc: 0.5511  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 39800] train loss: 1.0906, train acc: 0.5461, val loss: 1.0841, val acc: 0.5511  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 39820] train loss: 1.0993, train acc: 0.5470, val loss: 1.0856, val acc: 0.5582  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 39840] train loss: 1.0795, train acc: 0.5506, val loss: 1.0838, val acc: 0.5548  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 39860] train loss: 1.0895, train acc: 0.5467, val loss: 1.0831, val acc: 0.5551  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 39880] train loss: 1.0891, train acc: 0.5519, val loss: 1.0824, val acc: 0.5541  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 39900] train loss: 1.0935, train acc: 0.5472, val loss: 1.0850, val acc: 0.5477  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 39920] train loss: 1.0827, train acc: 0.5518, val loss: 1.0811, val acc: 0.5521  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 39940] train loss: 1.0861, train acc: 0.5483, val loss: 1.0803, val acc: 0.5524  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 39960] train loss: 1.0839, train acc: 0.5487, val loss: 1.0800, val acc: 0.5524  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 39980] train loss: 1.0715, train acc: 0.5575, val loss: 1.0798, val acc: 0.5518  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 40000] train loss: 1.0776, train acc: 0.5513, val loss: 1.0785, val acc: 0.5545  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pmaur\\miniconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\pmaur\\miniconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\pmaur\\miniconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\pmaur\\miniconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\pmaur\\miniconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\pmaur\\miniconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       154\n",
      "           1       0.66      0.65      0.66       703\n",
      "           2       0.42      0.30      0.35       702\n",
      "           3       0.48      0.61      0.53       703\n",
      "           4       0.73      0.89      0.80       702\n",
      "\n",
      "    accuracy                           0.58      2964\n",
      "   macro avg       0.46      0.49      0.47      2964\n",
      "weighted avg       0.54      0.58      0.56      2964\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 504x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 504x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "gnn_model = GNN7L_GCN(data_with_nedbit).to(device)\n",
    "pred = train(gnn_model, data_with_nedbit, epochs, lr, weight_decay, classes, 'GCN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1eef5f4474b64f68b18e24fb01eb6122",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40001 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch: 0000] train loss: 185.8041, train acc: 0.0518, val loss: 156.9525, val acc: 0.2371  (best train acc: 0.0518, best val acc: 0.2371, best train loss: 185.8041  @ epoch 0 )\n",
      "[Epoch: 0020] train loss: 9.9967, train acc: 0.2379, val loss: 7.4373, val acc: 0.2354  (best train acc: 0.2450, best val acc: 0.2371, best train loss: 9.9967  @ epoch 20 )\n",
      "[Epoch: 0040] train loss: 3.1875, train acc: 0.2420, val loss: 2.8824, val acc: 0.2310  (best train acc: 0.2450, best val acc: 0.2371, best train loss: 3.1038  @ epoch 37 )\n",
      "[Epoch: 0060] train loss: 1.5522, train acc: 0.1844, val loss: 1.5360, val acc: 0.1922  (best train acc: 0.2450, best val acc: 0.2371, best train loss: 1.5522  @ epoch 60 )\n",
      "[Epoch: 0080] train loss: 1.5619, train acc: 0.1973, val loss: 1.5477, val acc: 0.1744  (best train acc: 0.2963, best val acc: 0.3170, best train loss: 1.5404  @ epoch 75 )\n",
      "[Epoch: 0100] train loss: 1.5269, train acc: 0.2240, val loss: 1.5189, val acc: 0.2351  (best train acc: 0.2963, best val acc: 0.3170, best train loss: 1.5246  @ epoch 97 )\n",
      "[Epoch: 0120] train loss: 1.5174, train acc: 0.3119, val loss: 1.5098, val acc: 0.3251  (best train acc: 0.3119, best val acc: 0.3278, best train loss: 1.5174  @ epoch 120 )\n",
      "[Epoch: 0140] train loss: 1.5115, train acc: 0.3102, val loss: 1.5039, val acc: 0.3201  (best train acc: 0.3165, best val acc: 0.3278, best train loss: 1.5115  @ epoch 140 )\n",
      "[Epoch: 0160] train loss: 1.5060, train acc: 0.3164, val loss: 1.4984, val acc: 0.3288  (best train acc: 0.3206, best val acc: 0.3302, best train loss: 1.5060  @ epoch 160 )\n",
      "[Epoch: 0180] train loss: 1.5003, train acc: 0.3251, val loss: 1.4936, val acc: 0.3312  (best train acc: 0.3276, best val acc: 0.3393, best train loss: 1.5003  @ epoch 180 )\n",
      "[Epoch: 0200] train loss: 1.4967, train acc: 0.3190, val loss: 1.4890, val acc: 0.3258  (best train acc: 0.3276, best val acc: 0.3393, best train loss: 1.4967  @ epoch 200 )\n",
      "[Epoch: 0220] train loss: 1.4935, train acc: 0.3239, val loss: 1.4844, val acc: 0.3295  (best train acc: 0.3276, best val acc: 0.3393, best train loss: 1.4933  @ epoch 219 )\n",
      "[Epoch: 0240] train loss: 1.4895, train acc: 0.3233, val loss: 1.4805, val acc: 0.3467  (best train acc: 0.3287, best val acc: 0.3467, best train loss: 1.4878  @ epoch 233 )\n",
      "[Epoch: 0260] train loss: 1.4864, train acc: 0.3230, val loss: 1.4768, val acc: 0.3315  (best train acc: 0.3376, best val acc: 0.3467, best train loss: 1.4859  @ epoch 259 )\n",
      "[Epoch: 0280] train loss: 1.4820, train acc: 0.3253, val loss: 1.4732, val acc: 0.3352  (best train acc: 0.3376, best val acc: 0.3467, best train loss: 1.4812  @ epoch 279 )\n",
      "[Epoch: 0300] train loss: 1.4795, train acc: 0.3259, val loss: 1.4694, val acc: 0.3342  (best train acc: 0.3376, best val acc: 0.3467, best train loss: 1.4786  @ epoch 295 )\n",
      "[Epoch: 0320] train loss: 1.4776, train acc: 0.3212, val loss: 1.4672, val acc: 0.3403  (best train acc: 0.3376, best val acc: 0.3467, best train loss: 1.4761  @ epoch 309 )\n",
      "[Epoch: 0340] train loss: 1.4755, train acc: 0.3221, val loss: 1.4650, val acc: 0.3319  (best train acc: 0.3376, best val acc: 0.3467, best train loss: 1.4741  @ epoch 336 )\n",
      "[Epoch: 0360] train loss: 1.4740, train acc: 0.3258, val loss: 1.4629, val acc: 0.3336  (best train acc: 0.3376, best val acc: 0.3467, best train loss: 1.4722  @ epoch 353 )\n",
      "[Epoch: 0380] train loss: 1.4709, train acc: 0.3213, val loss: 1.4610, val acc: 0.3309  (best train acc: 0.3376, best val acc: 0.3467, best train loss: 1.4688  @ epoch 378 )\n",
      "[Epoch: 0400] train loss: 1.4704, train acc: 0.3213, val loss: 1.4592, val acc: 0.3295  (best train acc: 0.3376, best val acc: 0.3467, best train loss: 1.4688  @ epoch 399 )\n",
      "[Epoch: 0420] train loss: 1.4688, train acc: 0.3211, val loss: 1.4572, val acc: 0.3298  (best train acc: 0.3376, best val acc: 0.3467, best train loss: 1.4671  @ epoch 413 )\n",
      "[Epoch: 0440] train loss: 1.4660, train acc: 0.3222, val loss: 1.4561, val acc: 0.3322  (best train acc: 0.3376, best val acc: 0.3467, best train loss: 1.4660  @ epoch 440 )\n",
      "[Epoch: 0460] train loss: 1.4675, train acc: 0.3240, val loss: 1.4541, val acc: 0.3403  (best train acc: 0.3376, best val acc: 0.3467, best train loss: 1.4644  @ epoch 459 )\n",
      "[Epoch: 0480] train loss: 1.4630, train acc: 0.3255, val loss: 1.4522, val acc: 0.3342  (best train acc: 0.3376, best val acc: 0.3508, best train loss: 1.4624  @ epoch 479 )\n",
      "[Epoch: 0500] train loss: 1.4619, train acc: 0.3271, val loss: 1.4507, val acc: 0.3379  (best train acc: 0.3376, best val acc: 0.3508, best train loss: 1.4613  @ epoch 495 )\n",
      "[Epoch: 0520] train loss: 1.4617, train acc: 0.3266, val loss: 1.4490, val acc: 0.3396  (best train acc: 0.3464, best val acc: 0.3602, best train loss: 1.4593  @ epoch 515 )\n",
      "[Epoch: 0540] train loss: 1.4612, train acc: 0.3321, val loss: 1.4483, val acc: 0.3470  (best train acc: 0.3464, best val acc: 0.3602, best train loss: 1.4582  @ epoch 536 )\n",
      "[Epoch: 0560] train loss: 1.4596, train acc: 0.3297, val loss: 1.4473, val acc: 0.3514  (best train acc: 0.3546, best val acc: 0.3680, best train loss: 1.4581  @ epoch 546 )\n",
      "[Epoch: 0580] train loss: 1.4582, train acc: 0.3355, val loss: 1.4460, val acc: 0.3535  (best train acc: 0.3598, best val acc: 0.3730, best train loss: 1.4557  @ epoch 573 )\n",
      "[Epoch: 0600] train loss: 1.4562, train acc: 0.3368, val loss: 1.4416, val acc: 0.3562  (best train acc: 0.3598, best val acc: 0.3730, best train loss: 1.4550  @ epoch 593 )\n",
      "[Epoch: 0620] train loss: 1.4493, train acc: 0.3728, val loss: 1.4365, val acc: 0.3788  (best train acc: 0.3728, best val acc: 0.3885, best train loss: 1.4464  @ epoch 617 )\n",
      "[Epoch: 0640] train loss: 1.4469, train acc: 0.3637, val loss: 1.4312, val acc: 0.3808  (best train acc: 0.3802, best val acc: 0.3909, best train loss: 1.4444  @ epoch 628 )\n",
      "[Epoch: 0660] train loss: 1.4460, train acc: 0.3779, val loss: 1.4315, val acc: 0.3862  (best train acc: 0.3810, best val acc: 0.4000, best train loss: 1.4408  @ epoch 658 )\n",
      "[Epoch: 0680] train loss: 1.4403, train acc: 0.3720, val loss: 1.4255, val acc: 0.3976  (best train acc: 0.3881, best val acc: 0.4017, best train loss: 1.4364  @ epoch 673 )\n",
      "[Epoch: 0700] train loss: 1.4381, train acc: 0.3950, val loss: 1.4208, val acc: 0.4044  (best train acc: 0.3955, best val acc: 0.4125, best train loss: 1.4359  @ epoch 698 )\n",
      "[Epoch: 0720] train loss: 1.4354, train acc: 0.3925, val loss: 1.4195, val acc: 0.4040  (best train acc: 0.3975, best val acc: 0.4125, best train loss: 1.4334  @ epoch 705 )\n",
      "[Epoch: 0740] train loss: 1.4313, train acc: 0.3800, val loss: 1.4173, val acc: 0.4007  (best train acc: 0.3975, best val acc: 0.4125, best train loss: 1.4308  @ epoch 738 )\n",
      "[Epoch: 0760] train loss: 1.4289, train acc: 0.3960, val loss: 1.4157, val acc: 0.4057  (best train acc: 0.4000, best val acc: 0.4125, best train loss: 1.4280  @ epoch 752 )\n",
      "[Epoch: 0780] train loss: 1.4275, train acc: 0.3910, val loss: 1.4132, val acc: 0.4044  (best train acc: 0.4000, best val acc: 0.4128, best train loss: 1.4261  @ epoch 774 )\n",
      "[Epoch: 0800] train loss: 1.4283, train acc: 0.3940, val loss: 1.4109, val acc: 0.4078  (best train acc: 0.4000, best val acc: 0.4128, best train loss: 1.4239  @ epoch 798 )\n",
      "[Epoch: 0820] train loss: 1.4232, train acc: 0.3946, val loss: 1.4095, val acc: 0.4111  (best train acc: 0.4000, best val acc: 0.4128, best train loss: 1.4232  @ epoch 820 )\n",
      "[Epoch: 0840] train loss: 1.4260, train acc: 0.3964, val loss: 1.4084, val acc: 0.4057  (best train acc: 0.4000, best val acc: 0.4138, best train loss: 1.4232  @ epoch 839 )\n",
      "[Epoch: 0860] train loss: 1.4233, train acc: 0.3979, val loss: 1.4071, val acc: 0.4081  (best train acc: 0.4009, best val acc: 0.4138, best train loss: 1.4209  @ epoch 847 )\n",
      "[Epoch: 0880] train loss: 1.4241, train acc: 0.3962, val loss: 1.4060, val acc: 0.4111  (best train acc: 0.4009, best val acc: 0.4138, best train loss: 1.4196  @ epoch 874 )\n",
      "[Epoch: 0900] train loss: 1.4215, train acc: 0.3980, val loss: 1.4045, val acc: 0.4091  (best train acc: 0.4034, best val acc: 0.4145, best train loss: 1.4175  @ epoch 885 )\n",
      "[Epoch: 0920] train loss: 1.4187, train acc: 0.4009, val loss: 1.4023, val acc: 0.4148  (best train acc: 0.4034, best val acc: 0.4172, best train loss: 1.4152  @ epoch 919 )\n",
      "[Epoch: 0940] train loss: 1.4151, train acc: 0.4015, val loss: 1.4007, val acc: 0.4155  (best train acc: 0.4034, best val acc: 0.4172, best train loss: 1.4149  @ epoch 924 )\n",
      "[Epoch: 0960] train loss: 1.4170, train acc: 0.3999, val loss: 1.3999, val acc: 0.4145  (best train acc: 0.4034, best val acc: 0.4172, best train loss: 1.4144  @ epoch 959 )\n",
      "[Epoch: 0980] train loss: 1.4170, train acc: 0.3990, val loss: 1.4011, val acc: 0.4159  (best train acc: 0.4034, best val acc: 0.4172, best train loss: 1.4144  @ epoch 959 )\n",
      "[Epoch: 1000] train loss: 1.4142, train acc: 0.3981, val loss: 1.3990, val acc: 0.4118  (best train acc: 0.4034, best val acc: 0.4172, best train loss: 1.4122  @ epoch 998 )\n",
      "[Epoch: 1020] train loss: 1.4148, train acc: 0.3967, val loss: 1.3981, val acc: 0.4138  (best train acc: 0.4034, best val acc: 0.4172, best train loss: 1.4117  @ epoch 1003 )\n",
      "[Epoch: 1040] train loss: 1.4148, train acc: 0.3963, val loss: 1.3974, val acc: 0.4098  (best train acc: 0.4034, best val acc: 0.4172, best train loss: 1.4117  @ epoch 1003 )\n",
      "[Epoch: 1060] train loss: 1.4158, train acc: 0.3969, val loss: 1.3973, val acc: 0.4101  (best train acc: 0.4034, best val acc: 0.4172, best train loss: 1.4117  @ epoch 1003 )\n",
      "[Epoch: 1080] train loss: 1.4115, train acc: 0.3987, val loss: 1.3957, val acc: 0.4128  (best train acc: 0.4034, best val acc: 0.4172, best train loss: 1.4106  @ epoch 1069 )\n",
      "[Epoch: 1100] train loss: 1.4146, train acc: 0.3987, val loss: 1.3940, val acc: 0.4135  (best train acc: 0.4041, best val acc: 0.4172, best train loss: 1.4097  @ epoch 1082 )\n",
      "[Epoch: 1120] train loss: 1.4120, train acc: 0.4011, val loss: 1.3927, val acc: 0.4172  (best train acc: 0.4041, best val acc: 0.4172, best train loss: 1.4090  @ epoch 1102 )\n",
      "[Epoch: 1140] train loss: 1.4120, train acc: 0.3997, val loss: 1.3913, val acc: 0.4182  (best train acc: 0.4041, best val acc: 0.4182, best train loss: 1.4082  @ epoch 1131 )\n",
      "[Epoch: 1160] train loss: 1.4080, train acc: 0.4055, val loss: 1.3919, val acc: 0.4182  (best train acc: 0.4055, best val acc: 0.4192, best train loss: 1.4062  @ epoch 1156 )\n",
      "[Epoch: 1180] train loss: 1.4093, train acc: 0.4016, val loss: 1.3899, val acc: 0.4152  (best train acc: 0.4055, best val acc: 0.4192, best train loss: 1.4061  @ epoch 1179 )\n",
      "[Epoch: 1200] train loss: 1.4109, train acc: 0.4002, val loss: 1.3903, val acc: 0.4125  (best train acc: 0.4055, best val acc: 0.4192, best train loss: 1.4052  @ epoch 1183 )\n",
      "[Epoch: 1220] train loss: 1.4070, train acc: 0.4025, val loss: 1.3884, val acc: 0.4155  (best train acc: 0.4055, best val acc: 0.4192, best train loss: 1.4049  @ epoch 1218 )\n",
      "[Epoch: 1240] train loss: 1.4078, train acc: 0.4033, val loss: 1.3869, val acc: 0.4175  (best train acc: 0.4055, best val acc: 0.4192, best train loss: 1.4043  @ epoch 1221 )\n",
      "[Epoch: 1260] train loss: 1.4051, train acc: 0.4012, val loss: 1.3868, val acc: 0.4182  (best train acc: 0.4058, best val acc: 0.4199, best train loss: 1.4030  @ epoch 1242 )\n",
      "[Epoch: 1280] train loss: 1.4083, train acc: 0.4001, val loss: 1.3882, val acc: 0.4172  (best train acc: 0.4070, best val acc: 0.4226, best train loss: 1.4030  @ epoch 1242 )\n",
      "[Epoch: 1300] train loss: 1.4090, train acc: 0.4040, val loss: 1.3849, val acc: 0.4179  (best train acc: 0.4070, best val acc: 0.4226, best train loss: 1.4016  @ epoch 1296 )\n",
      "[Epoch: 1320] train loss: 1.4075, train acc: 0.4024, val loss: 1.3842, val acc: 0.4202  (best train acc: 0.4070, best val acc: 0.4226, best train loss: 1.4016  @ epoch 1318 )\n",
      "[Epoch: 1340] train loss: 1.4039, train acc: 0.4064, val loss: 1.3841, val acc: 0.4172  (best train acc: 0.4070, best val acc: 0.4226, best train loss: 1.4016  @ epoch 1318 )\n",
      "[Epoch: 1360] train loss: 1.4014, train acc: 0.4059, val loss: 1.3833, val acc: 0.4185  (best train acc: 0.4070, best val acc: 0.4226, best train loss: 1.4007  @ epoch 1358 )\n",
      "[Epoch: 1380] train loss: 1.4035, train acc: 0.4039, val loss: 1.3829, val acc: 0.4206  (best train acc: 0.4070, best val acc: 0.4226, best train loss: 1.3997  @ epoch 1368 )\n",
      "[Epoch: 1400] train loss: 1.4057, train acc: 0.4025, val loss: 1.3817, val acc: 0.4202  (best train acc: 0.4083, best val acc: 0.4226, best train loss: 1.3976  @ epoch 1390 )\n",
      "[Epoch: 1420] train loss: 1.4029, train acc: 0.4017, val loss: 1.3817, val acc: 0.4165  (best train acc: 0.4083, best val acc: 0.4226, best train loss: 1.3976  @ epoch 1390 )\n",
      "[Epoch: 1440] train loss: 1.3986, train acc: 0.4051, val loss: 1.3811, val acc: 0.4202  (best train acc: 0.4083, best val acc: 0.4226, best train loss: 1.3976  @ epoch 1390 )\n",
      "[Epoch: 1460] train loss: 1.3993, train acc: 0.4071, val loss: 1.3808, val acc: 0.4196  (best train acc: 0.4083, best val acc: 0.4226, best train loss: 1.3973  @ epoch 1446 )\n",
      "[Epoch: 1480] train loss: 1.3985, train acc: 0.4056, val loss: 1.3797, val acc: 0.4189  (best train acc: 0.4083, best val acc: 0.4226, best train loss: 1.3973  @ epoch 1446 )\n",
      "[Epoch: 1500] train loss: 1.4014, train acc: 0.4042, val loss: 1.3797, val acc: 0.4169  (best train acc: 0.4083, best val acc: 0.4226, best train loss: 1.3973  @ epoch 1446 )\n",
      "[Epoch: 1520] train loss: 1.3984, train acc: 0.4063, val loss: 1.3784, val acc: 0.4172  (best train acc: 0.4083, best val acc: 0.4236, best train loss: 1.3951  @ epoch 1502 )\n",
      "[Epoch: 1540] train loss: 1.3987, train acc: 0.4061, val loss: 1.3784, val acc: 0.4212  (best train acc: 0.4083, best val acc: 0.4236, best train loss: 1.3951  @ epoch 1502 )\n",
      "[Epoch: 1560] train loss: 1.3978, train acc: 0.4039, val loss: 1.3779, val acc: 0.4223  (best train acc: 0.4083, best val acc: 0.4236, best train loss: 1.3951  @ epoch 1502 )\n",
      "[Epoch: 1580] train loss: 1.3981, train acc: 0.4054, val loss: 1.3789, val acc: 0.4182  (best train acc: 0.4083, best val acc: 0.4236, best train loss: 1.3951  @ epoch 1502 )\n",
      "[Epoch: 1600] train loss: 1.4021, train acc: 0.4048, val loss: 1.3801, val acc: 0.4169  (best train acc: 0.4083, best val acc: 0.4236, best train loss: 1.3951  @ epoch 1502 )\n",
      "[Epoch: 1620] train loss: 1.3971, train acc: 0.4044, val loss: 1.3778, val acc: 0.4179  (best train acc: 0.4083, best val acc: 0.4236, best train loss: 1.3951  @ epoch 1502 )\n",
      "[Epoch: 1640] train loss: 1.3989, train acc: 0.4035, val loss: 1.3777, val acc: 0.4185  (best train acc: 0.4083, best val acc: 0.4236, best train loss: 1.3939  @ epoch 1638 )\n",
      "[Epoch: 1660] train loss: 1.3999, train acc: 0.4065, val loss: 1.3770, val acc: 0.4216  (best train acc: 0.4083, best val acc: 0.4236, best train loss: 1.3939  @ epoch 1638 )\n",
      "[Epoch: 1680] train loss: 1.3990, train acc: 0.4032, val loss: 1.3766, val acc: 0.4169  (best train acc: 0.4083, best val acc: 0.4236, best train loss: 1.3939  @ epoch 1638 )\n",
      "[Epoch: 1700] train loss: 1.3974, train acc: 0.4040, val loss: 1.3780, val acc: 0.4179  (best train acc: 0.4083, best val acc: 0.4236, best train loss: 1.3939  @ epoch 1638 )\n",
      "[Epoch: 1720] train loss: 1.4111, train acc: 0.4007, val loss: 1.3823, val acc: 0.4111  (best train acc: 0.4083, best val acc: 0.4236, best train loss: 1.3921  @ epoch 1703 )\n",
      "[Epoch: 1740] train loss: 1.4035, train acc: 0.3996, val loss: 1.3803, val acc: 0.4105  (best train acc: 0.4083, best val acc: 0.4236, best train loss: 1.3921  @ epoch 1703 )\n",
      "[Epoch: 1760] train loss: 1.3973, train acc: 0.4055, val loss: 1.3783, val acc: 0.4064  (best train acc: 0.4083, best val acc: 0.4236, best train loss: 1.3921  @ epoch 1703 )\n",
      "[Epoch: 1780] train loss: 1.3951, train acc: 0.4014, val loss: 1.3760, val acc: 0.4152  (best train acc: 0.4083, best val acc: 0.4236, best train loss: 1.3921  @ epoch 1703 )\n",
      "[Epoch: 1800] train loss: 1.3942, train acc: 0.4043, val loss: 1.3794, val acc: 0.4152  (best train acc: 0.4083, best val acc: 0.4236, best train loss: 1.3921  @ epoch 1703 )\n",
      "[Epoch: 1820] train loss: 1.3942, train acc: 0.4068, val loss: 1.3769, val acc: 0.4229  (best train acc: 0.4083, best val acc: 0.4236, best train loss: 1.3921  @ epoch 1703 )\n",
      "[Epoch: 1840] train loss: 1.3920, train acc: 0.4060, val loss: 1.3721, val acc: 0.4219  (best train acc: 0.4083, best val acc: 0.4236, best train loss: 1.3920  @ epoch 1840 )\n",
      "[Epoch: 1860] train loss: 1.3958, train acc: 0.4049, val loss: 1.3706, val acc: 0.4223  (best train acc: 0.4083, best val acc: 0.4253, best train loss: 1.3920  @ epoch 1840 )\n",
      "[Epoch: 1880] train loss: 1.3953, train acc: 0.4040, val loss: 1.3704, val acc: 0.4233  (best train acc: 0.4085, best val acc: 0.4253, best train loss: 1.3903  @ epoch 1864 )\n",
      "[Epoch: 1900] train loss: 1.3938, train acc: 0.4048, val loss: 1.3760, val acc: 0.4115  (best train acc: 0.4087, best val acc: 0.4256, best train loss: 1.3903  @ epoch 1864 )\n",
      "[Epoch: 1920] train loss: 1.3959, train acc: 0.4054, val loss: 1.3737, val acc: 0.4165  (best train acc: 0.4087, best val acc: 0.4263, best train loss: 1.3903  @ epoch 1864 )\n",
      "[Epoch: 1940] train loss: 1.3939, train acc: 0.4080, val loss: 1.3734, val acc: 0.4223  (best train acc: 0.4087, best val acc: 0.4263, best train loss: 1.3903  @ epoch 1864 )\n",
      "[Epoch: 1960] train loss: 1.3967, train acc: 0.4030, val loss: 1.3775, val acc: 0.4115  (best train acc: 0.4091, best val acc: 0.4263, best train loss: 1.3903  @ epoch 1864 )\n",
      "[Epoch: 1980] train loss: 1.3938, train acc: 0.4080, val loss: 1.3748, val acc: 0.4236  (best train acc: 0.4091, best val acc: 0.4283, best train loss: 1.3903  @ epoch 1864 )\n",
      "[Epoch: 2000] train loss: 1.3927, train acc: 0.4072, val loss: 1.3747, val acc: 0.4108  (best train acc: 0.4091, best val acc: 0.4283, best train loss: 1.3903  @ epoch 1864 )\n",
      "[Epoch: 2020] train loss: 1.3960, train acc: 0.4029, val loss: 1.3697, val acc: 0.4236  (best train acc: 0.4093, best val acc: 0.4283, best train loss: 1.3901  @ epoch 2013 )\n",
      "[Epoch: 2040] train loss: 1.4019, train acc: 0.3997, val loss: 1.3697, val acc: 0.4256  (best train acc: 0.4093, best val acc: 0.4290, best train loss: 1.3901  @ epoch 2013 )\n",
      "[Epoch: 2060] train loss: 1.3919, train acc: 0.4029, val loss: 1.3693, val acc: 0.4239  (best train acc: 0.4093, best val acc: 0.4290, best train loss: 1.3901  @ epoch 2013 )\n",
      "[Epoch: 2080] train loss: 1.3934, train acc: 0.4045, val loss: 1.3684, val acc: 0.4253  (best train acc: 0.4093, best val acc: 0.4297, best train loss: 1.3893  @ epoch 2074 )\n",
      "[Epoch: 2100] train loss: 1.3914, train acc: 0.4054, val loss: 1.3664, val acc: 0.4236  (best train acc: 0.4093, best val acc: 0.4297, best train loss: 1.3871  @ epoch 2094 )\n",
      "[Epoch: 2120] train loss: 1.3962, train acc: 0.4041, val loss: 1.3728, val acc: 0.4175  (best train acc: 0.4093, best val acc: 0.4314, best train loss: 1.3866  @ epoch 2109 )\n",
      "[Epoch: 2140] train loss: 1.3936, train acc: 0.4076, val loss: 1.3694, val acc: 0.4270  (best train acc: 0.4093, best val acc: 0.4314, best train loss: 1.3866  @ epoch 2109 )\n",
      "[Epoch: 2160] train loss: 1.3935, train acc: 0.4066, val loss: 1.3697, val acc: 0.4162  (best train acc: 0.4093, best val acc: 0.4314, best train loss: 1.3866  @ epoch 2109 )\n",
      "[Epoch: 2180] train loss: 1.3893, train acc: 0.4078, val loss: 1.3674, val acc: 0.4219  (best train acc: 0.4093, best val acc: 0.4314, best train loss: 1.3866  @ epoch 2109 )\n",
      "[Epoch: 2200] train loss: 1.3881, train acc: 0.4067, val loss: 1.3691, val acc: 0.4216  (best train acc: 0.4096, best val acc: 0.4314, best train loss: 1.3866  @ epoch 2109 )\n",
      "[Epoch: 2220] train loss: 1.3883, train acc: 0.4062, val loss: 1.3756, val acc: 0.4155  (best train acc: 0.4105, best val acc: 0.4314, best train loss: 1.3866  @ epoch 2109 )\n",
      "[Epoch: 2240] train loss: 1.3909, train acc: 0.4060, val loss: 1.3667, val acc: 0.4162  (best train acc: 0.4105, best val acc: 0.4314, best train loss: 1.3866  @ epoch 2109 )\n",
      "[Epoch: 2260] train loss: 1.3875, train acc: 0.4111, val loss: 1.3704, val acc: 0.4145  (best train acc: 0.4111, best val acc: 0.4314, best train loss: 1.3866  @ epoch 2109 )\n",
      "[Epoch: 2280] train loss: 1.3929, train acc: 0.4035, val loss: 1.3669, val acc: 0.4280  (best train acc: 0.4111, best val acc: 0.4314, best train loss: 1.3866  @ epoch 2109 )\n",
      "[Epoch: 2300] train loss: 1.4187, train acc: 0.3942, val loss: 1.3728, val acc: 0.4260  (best train acc: 0.4111, best val acc: 0.4314, best train loss: 1.3866  @ epoch 2109 )\n",
      "[Epoch: 2320] train loss: 1.4625, train acc: 0.3377, val loss: 1.5198, val acc: 0.3575  (best train acc: 0.4111, best val acc: 0.4314, best train loss: 1.3866  @ epoch 2109 )\n",
      "[Epoch: 2340] train loss: 1.4487, train acc: 0.3816, val loss: 1.4067, val acc: 0.4000  (best train acc: 0.4111, best val acc: 0.4314, best train loss: 1.3866  @ epoch 2109 )\n",
      "[Epoch: 2360] train loss: 1.4179, train acc: 0.3458, val loss: 1.3972, val acc: 0.3960  (best train acc: 0.4111, best val acc: 0.4314, best train loss: 1.3866  @ epoch 2109 )\n",
      "[Epoch: 2380] train loss: 1.4084, train acc: 0.3942, val loss: 1.3915, val acc: 0.4061  (best train acc: 0.4111, best val acc: 0.4314, best train loss: 1.3866  @ epoch 2109 )\n",
      "[Epoch: 2400] train loss: 1.3996, train acc: 0.3937, val loss: 1.3861, val acc: 0.3902  (best train acc: 0.4111, best val acc: 0.4314, best train loss: 1.3866  @ epoch 2109 )\n",
      "[Epoch: 2420] train loss: 1.3978, train acc: 0.3884, val loss: 1.3845, val acc: 0.4034  (best train acc: 0.4111, best val acc: 0.4314, best train loss: 1.3866  @ epoch 2109 )\n",
      "[Epoch: 2440] train loss: 1.3984, train acc: 0.3926, val loss: 1.3837, val acc: 0.4047  (best train acc: 0.4111, best val acc: 0.4314, best train loss: 1.3866  @ epoch 2109 )\n",
      "[Epoch: 2460] train loss: 1.3980, train acc: 0.3931, val loss: 1.3825, val acc: 0.4044  (best train acc: 0.4111, best val acc: 0.4314, best train loss: 1.3866  @ epoch 2109 )\n",
      "[Epoch: 2480] train loss: 1.3986, train acc: 0.3952, val loss: 1.3820, val acc: 0.4040  (best train acc: 0.4111, best val acc: 0.4314, best train loss: 1.3866  @ epoch 2109 )\n",
      "[Epoch: 2500] train loss: 1.3977, train acc: 0.3951, val loss: 1.3816, val acc: 0.4047  (best train acc: 0.4111, best val acc: 0.4314, best train loss: 1.3866  @ epoch 2109 )\n",
      "[Epoch: 2520] train loss: 1.3995, train acc: 0.3957, val loss: 1.3813, val acc: 0.4054  (best train acc: 0.4111, best val acc: 0.4314, best train loss: 1.3866  @ epoch 2109 )\n",
      "[Epoch: 2540] train loss: 1.3965, train acc: 0.3961, val loss: 1.3811, val acc: 0.4067  (best train acc: 0.4111, best val acc: 0.4314, best train loss: 1.3866  @ epoch 2109 )\n",
      "[Epoch: 2560] train loss: 1.3964, train acc: 0.3930, val loss: 1.3801, val acc: 0.4074  (best train acc: 0.4111, best val acc: 0.4314, best train loss: 1.3866  @ epoch 2109 )\n",
      "[Epoch: 2580] train loss: 1.3961, train acc: 0.3916, val loss: 1.3804, val acc: 0.4064  (best train acc: 0.4111, best val acc: 0.4314, best train loss: 1.3866  @ epoch 2109 )\n",
      "[Epoch: 2600] train loss: 1.3952, train acc: 0.3924, val loss: 1.3802, val acc: 0.4088  (best train acc: 0.4111, best val acc: 0.4314, best train loss: 1.3866  @ epoch 2109 )\n",
      "[Epoch: 2620] train loss: 1.3960, train acc: 0.3979, val loss: 1.3788, val acc: 0.4088  (best train acc: 0.4111, best val acc: 0.4314, best train loss: 1.3866  @ epoch 2109 )\n",
      "[Epoch: 2640] train loss: 1.3966, train acc: 0.3942, val loss: 1.3784, val acc: 0.4088  (best train acc: 0.4111, best val acc: 0.4314, best train loss: 1.3866  @ epoch 2109 )\n",
      "[Epoch: 2660] train loss: 1.3928, train acc: 0.3989, val loss: 1.3785, val acc: 0.4088  (best train acc: 0.4111, best val acc: 0.4314, best train loss: 1.3866  @ epoch 2109 )\n",
      "[Epoch: 2680] train loss: 1.3926, train acc: 0.3995, val loss: 1.3779, val acc: 0.4088  (best train acc: 0.4111, best val acc: 0.4314, best train loss: 1.3866  @ epoch 2109 )\n",
      "[Epoch: 2700] train loss: 1.3932, train acc: 0.3968, val loss: 1.3778, val acc: 0.4081  (best train acc: 0.4111, best val acc: 0.4314, best train loss: 1.3866  @ epoch 2109 )\n",
      "[Epoch: 2720] train loss: 1.3938, train acc: 0.3963, val loss: 1.3768, val acc: 0.4105  (best train acc: 0.4111, best val acc: 0.4314, best train loss: 1.3866  @ epoch 2109 )\n",
      "[Epoch: 2740] train loss: 1.3926, train acc: 0.3947, val loss: 1.3772, val acc: 0.4101  (best train acc: 0.4111, best val acc: 0.4314, best train loss: 1.3866  @ epoch 2109 )\n",
      "[Epoch: 2760] train loss: 1.3900, train acc: 0.3973, val loss: 1.3762, val acc: 0.4078  (best train acc: 0.4111, best val acc: 0.4314, best train loss: 1.3866  @ epoch 2109 )\n",
      "[Epoch: 2780] train loss: 1.3938, train acc: 0.3980, val loss: 1.3769, val acc: 0.4108  (best train acc: 0.4111, best val acc: 0.4314, best train loss: 1.3866  @ epoch 2109 )\n",
      "[Epoch: 2800] train loss: 1.4038, train acc: 0.3788, val loss: 1.3799, val acc: 0.4067  (best train acc: 0.4111, best val acc: 0.4314, best train loss: 1.3866  @ epoch 2109 )\n",
      "[Epoch: 2820] train loss: 1.3941, train acc: 0.3989, val loss: 1.3760, val acc: 0.4111  (best train acc: 0.4111, best val acc: 0.4314, best train loss: 1.3866  @ epoch 2109 )\n",
      "[Epoch: 2840] train loss: 1.3902, train acc: 0.3994, val loss: 1.3815, val acc: 0.4084  (best train acc: 0.4111, best val acc: 0.4314, best train loss: 1.3866  @ epoch 2109 )\n",
      "[Epoch: 2860] train loss: 1.4060, train acc: 0.3540, val loss: 1.3861, val acc: 0.3933  (best train acc: 0.4111, best val acc: 0.4314, best train loss: 1.3866  @ epoch 2109 )\n",
      "[Epoch: 2880] train loss: 1.3923, train acc: 0.3975, val loss: 1.3761, val acc: 0.3882  (best train acc: 0.4111, best val acc: 0.4314, best train loss: 1.3866  @ epoch 2109 )\n",
      "[Epoch: 2900] train loss: 1.3860, train acc: 0.3959, val loss: 1.3738, val acc: 0.4125  (best train acc: 0.4111, best val acc: 0.4314, best train loss: 1.3833  @ epoch 2896 )\n",
      "[Epoch: 2920] train loss: 1.3823, train acc: 0.4025, val loss: 1.3735, val acc: 0.4071  (best train acc: 0.4111, best val acc: 0.4314, best train loss: 1.3823  @ epoch 2920 )\n",
      "[Epoch: 2940] train loss: 1.3910, train acc: 0.3892, val loss: 1.3827, val acc: 0.3700  (best train acc: 0.4111, best val acc: 0.4314, best train loss: 1.3823  @ epoch 2920 )\n",
      "[Epoch: 2960] train loss: 1.4066, train acc: 0.3790, val loss: 1.4049, val acc: 0.3838  (best train acc: 0.4111, best val acc: 0.4314, best train loss: 1.3823  @ epoch 2920 )\n",
      "[Epoch: 2980] train loss: 1.4017, train acc: 0.3596, val loss: 1.3881, val acc: 0.4135  (best train acc: 0.4111, best val acc: 0.4314, best train loss: 1.3823  @ epoch 2920 )\n",
      "[Epoch: 3000] train loss: 1.4209, train acc: 0.3786, val loss: 1.3835, val acc: 0.3592  (best train acc: 0.4111, best val acc: 0.4314, best train loss: 1.3823  @ epoch 2920 )\n",
      "[Epoch: 3020] train loss: 1.3961, train acc: 0.3930, val loss: 1.3802, val acc: 0.4007  (best train acc: 0.4111, best val acc: 0.4314, best train loss: 1.3823  @ epoch 2920 )\n",
      "[Epoch: 3040] train loss: 1.3858, train acc: 0.3857, val loss: 1.3784, val acc: 0.4094  (best train acc: 0.4111, best val acc: 0.4314, best train loss: 1.3823  @ epoch 2920 )\n",
      "[Epoch: 3060] train loss: 1.4005, train acc: 0.3932, val loss: 1.3767, val acc: 0.3875  (best train acc: 0.4111, best val acc: 0.4314, best train loss: 1.3823  @ epoch 2920 )\n",
      "[Epoch: 3080] train loss: 1.3998, train acc: 0.3887, val loss: 1.3749, val acc: 0.4061  (best train acc: 0.4111, best val acc: 0.4314, best train loss: 1.3823  @ epoch 2920 )\n",
      "[Epoch: 3100] train loss: 1.3889, train acc: 0.3971, val loss: 1.3746, val acc: 0.4088  (best train acc: 0.4111, best val acc: 0.4314, best train loss: 1.3823  @ epoch 2920 )\n",
      "[Epoch: 3120] train loss: 1.3833, train acc: 0.3991, val loss: 1.3746, val acc: 0.4081  (best train acc: 0.4111, best val acc: 0.4314, best train loss: 1.3822  @ epoch 3117 )\n",
      "[Epoch: 3140] train loss: 1.3857, train acc: 0.3974, val loss: 1.3729, val acc: 0.4135  (best train acc: 0.4111, best val acc: 0.4314, best train loss: 1.3822  @ epoch 3117 )\n",
      "[Epoch: 3160] train loss: 1.3954, train acc: 0.3931, val loss: 1.3719, val acc: 0.4084  (best train acc: 0.4111, best val acc: 0.4314, best train loss: 1.3822  @ epoch 3117 )\n",
      "[Epoch: 3180] train loss: 1.4183, train acc: 0.3652, val loss: 1.3863, val acc: 0.3663  (best train acc: 0.4148, best val acc: 0.4314, best train loss: 1.3729  @ epoch 3173 )\n",
      "[Epoch: 3200] train loss: 1.4148, train acc: 0.3490, val loss: 1.4120, val acc: 0.3963  (best train acc: 0.4148, best val acc: 0.4314, best train loss: 1.3729  @ epoch 3173 )\n",
      "[Epoch: 3220] train loss: 1.4101, train acc: 0.3867, val loss: 1.3880, val acc: 0.3710  (best train acc: 0.4148, best val acc: 0.4314, best train loss: 1.3729  @ epoch 3173 )\n",
      "[Epoch: 3240] train loss: 1.3972, train acc: 0.3928, val loss: 1.3791, val acc: 0.4081  (best train acc: 0.4148, best val acc: 0.4314, best train loss: 1.3729  @ epoch 3173 )\n",
      "[Epoch: 3260] train loss: 1.3955, train acc: 0.3908, val loss: 1.3821, val acc: 0.3761  (best train acc: 0.4148, best val acc: 0.4314, best train loss: 1.3729  @ epoch 3173 )\n",
      "[Epoch: 3280] train loss: 1.3892, train acc: 0.4017, val loss: 1.3779, val acc: 0.4037  (best train acc: 0.4148, best val acc: 0.4314, best train loss: 1.3729  @ epoch 3173 )\n",
      "[Epoch: 3300] train loss: 1.4104, train acc: 0.3352, val loss: 1.4649, val acc: 0.3868  (best train acc: 0.4148, best val acc: 0.4314, best train loss: 1.3729  @ epoch 3173 )\n",
      "[Epoch: 3320] train loss: 1.5121, train acc: 0.3334, val loss: 1.4296, val acc: 0.3906  (best train acc: 0.4148, best val acc: 0.4314, best train loss: 1.3729  @ epoch 3173 )\n",
      "[Epoch: 3340] train loss: 1.4563, train acc: 0.3561, val loss: 1.4155, val acc: 0.4064  (best train acc: 0.4148, best val acc: 0.4314, best train loss: 1.3729  @ epoch 3173 )\n",
      "[Epoch: 3360] train loss: 1.3915, train acc: 0.3892, val loss: 1.3832, val acc: 0.4209  (best train acc: 0.4148, best val acc: 0.4314, best train loss: 1.3729  @ epoch 3173 )\n",
      "[Epoch: 3380] train loss: 1.3899, train acc: 0.4017, val loss: 1.3648, val acc: 0.4202  (best train acc: 0.4148, best val acc: 0.4314, best train loss: 1.3729  @ epoch 3173 )\n",
      "[Epoch: 3400] train loss: 1.4100, train acc: 0.4044, val loss: 1.3902, val acc: 0.3872  (best train acc: 0.4148, best val acc: 0.4371, best train loss: 1.3729  @ epoch 3173 )\n",
      "[Epoch: 3420] train loss: 1.3816, train acc: 0.3884, val loss: 1.3594, val acc: 0.4152  (best train acc: 0.4148, best val acc: 0.4371, best train loss: 1.3729  @ epoch 3173 )\n",
      "[Epoch: 3440] train loss: 1.3806, train acc: 0.4062, val loss: 1.3563, val acc: 0.4266  (best train acc: 0.4218, best val acc: 0.4425, best train loss: 1.3726  @ epoch 3437 )\n",
      "[Epoch: 3460] train loss: 1.3744, train acc: 0.4021, val loss: 1.3618, val acc: 0.4266  (best train acc: 0.4218, best val acc: 0.4442, best train loss: 1.3716  @ epoch 3442 )\n",
      "[Epoch: 3480] train loss: 1.3738, train acc: 0.4059, val loss: 1.3560, val acc: 0.4236  (best train acc: 0.4218, best val acc: 0.4442, best train loss: 1.3716  @ epoch 3442 )\n",
      "[Epoch: 3500] train loss: 1.3810, train acc: 0.4145, val loss: 1.3585, val acc: 0.4132  (best train acc: 0.4218, best val acc: 0.4442, best train loss: 1.3709  @ epoch 3496 )\n",
      "[Epoch: 3520] train loss: 1.3712, train acc: 0.4088, val loss: 1.3540, val acc: 0.4243  (best train acc: 0.4218, best val acc: 0.4442, best train loss: 1.3682  @ epoch 3519 )\n",
      "[Epoch: 3540] train loss: 1.3820, train acc: 0.4030, val loss: 1.3590, val acc: 0.4405  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3519 )\n",
      "[Epoch: 3560] train loss: 1.4640, train acc: 0.3900, val loss: 1.4375, val acc: 0.3555  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3519 )\n",
      "[Epoch: 3580] train loss: 1.4139, train acc: 0.3670, val loss: 1.3880, val acc: 0.3636  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3519 )\n",
      "[Epoch: 3600] train loss: 1.4172, train acc: 0.3736, val loss: 1.3692, val acc: 0.4091  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3519 )\n",
      "[Epoch: 3620] train loss: 1.3880, train acc: 0.4064, val loss: 1.3674, val acc: 0.4145  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3519 )\n",
      "[Epoch: 3640] train loss: 1.3736, train acc: 0.4083, val loss: 1.3557, val acc: 0.4206  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3519 )\n",
      "[Epoch: 3660] train loss: 1.3682, train acc: 0.4108, val loss: 1.3555, val acc: 0.4206  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 3680] train loss: 1.3876, train acc: 0.4041, val loss: 1.3650, val acc: 0.4034  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 3700] train loss: 1.3872, train acc: 0.3488, val loss: 1.3638, val acc: 0.4054  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 3720] train loss: 1.7846, train acc: 0.3336, val loss: 1.6077, val acc: 0.3545  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 3740] train loss: 1.4842, train acc: 0.3422, val loss: 1.4705, val acc: 0.3639  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 3760] train loss: 1.4325, train acc: 0.3524, val loss: 1.4284, val acc: 0.3568  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 3780] train loss: 1.4112, train acc: 0.3686, val loss: 1.4067, val acc: 0.3673  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 3800] train loss: 1.3977, train acc: 0.3710, val loss: 1.3901, val acc: 0.3717  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 3820] train loss: 1.3992, train acc: 0.3650, val loss: 1.3892, val acc: 0.3693  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 3840] train loss: 1.4001, train acc: 0.3707, val loss: 1.3851, val acc: 0.3757  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 3860] train loss: 1.3815, train acc: 0.3727, val loss: 1.3801, val acc: 0.3727  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 3880] train loss: 1.4315, train acc: 0.3588, val loss: 1.4367, val acc: 0.3605  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 3900] train loss: 1.4066, train acc: 0.3567, val loss: 1.3950, val acc: 0.3642  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 3920] train loss: 1.4808, train acc: 0.3018, val loss: 1.6483, val acc: 0.3170  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 3940] train loss: 1.4950, train acc: 0.3514, val loss: 1.4583, val acc: 0.3578  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 3960] train loss: 1.4451, train acc: 0.3510, val loss: 1.4322, val acc: 0.3575  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 3980] train loss: 1.4378, train acc: 0.3508, val loss: 1.4272, val acc: 0.3555  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 4000] train loss: 1.4320, train acc: 0.3542, val loss: 1.4217, val acc: 0.3568  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 4020] train loss: 1.4304, train acc: 0.3544, val loss: 1.4202, val acc: 0.3562  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 4040] train loss: 1.4278, train acc: 0.3552, val loss: 1.4199, val acc: 0.3575  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 4060] train loss: 1.4258, train acc: 0.3568, val loss: 1.4191, val acc: 0.3589  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 4080] train loss: 1.4283, train acc: 0.3563, val loss: 1.4185, val acc: 0.3585  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 4100] train loss: 1.4249, train acc: 0.3564, val loss: 1.4184, val acc: 0.3589  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 4120] train loss: 1.4222, train acc: 0.3586, val loss: 1.4182, val acc: 0.3592  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 4140] train loss: 1.4241, train acc: 0.3572, val loss: 1.4179, val acc: 0.3589  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 4160] train loss: 1.4218, train acc: 0.3579, val loss: 1.4187, val acc: 0.3568  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 4180] train loss: 1.4229, train acc: 0.3578, val loss: 1.4172, val acc: 0.3592  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 4200] train loss: 1.4236, train acc: 0.3582, val loss: 1.4168, val acc: 0.3592  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 4220] train loss: 1.4232, train acc: 0.3583, val loss: 1.4165, val acc: 0.3589  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 4240] train loss: 1.4239, train acc: 0.3566, val loss: 1.4165, val acc: 0.3592  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 4260] train loss: 1.4239, train acc: 0.3580, val loss: 1.4161, val acc: 0.3592  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 4280] train loss: 1.4247, train acc: 0.3573, val loss: 1.4161, val acc: 0.3592  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 4300] train loss: 1.4170, train acc: 0.3605, val loss: 1.4156, val acc: 0.3592  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 4320] train loss: 1.4217, train acc: 0.3591, val loss: 1.4155, val acc: 0.3589  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 4340] train loss: 1.4214, train acc: 0.3590, val loss: 1.4150, val acc: 0.3585  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 4360] train loss: 1.4214, train acc: 0.3574, val loss: 1.4153, val acc: 0.3582  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 4380] train loss: 1.4202, train acc: 0.3576, val loss: 1.4144, val acc: 0.3582  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 4400] train loss: 1.4197, train acc: 0.3594, val loss: 1.4139, val acc: 0.3582  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 4420] train loss: 1.4219, train acc: 0.3573, val loss: 1.4137, val acc: 0.3578  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 4440] train loss: 1.4224, train acc: 0.3569, val loss: 1.4134, val acc: 0.3582  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 4460] train loss: 1.4201, train acc: 0.3584, val loss: 1.4130, val acc: 0.3582  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 4480] train loss: 1.4184, train acc: 0.3575, val loss: 1.4125, val acc: 0.3582  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 4500] train loss: 1.4212, train acc: 0.3589, val loss: 1.4118, val acc: 0.3585  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 4520] train loss: 1.4195, train acc: 0.3578, val loss: 1.4112, val acc: 0.3585  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 4540] train loss: 1.4178, train acc: 0.3572, val loss: 1.4106, val acc: 0.3585  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 4560] train loss: 1.4198, train acc: 0.3583, val loss: 1.4094, val acc: 0.3582  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 4580] train loss: 1.4139, train acc: 0.3587, val loss: 1.4089, val acc: 0.3578  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 4600] train loss: 1.4193, train acc: 0.3571, val loss: 1.4083, val acc: 0.3585  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 4620] train loss: 1.4163, train acc: 0.3592, val loss: 1.4083, val acc: 0.3578  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 4640] train loss: 1.4168, train acc: 0.3576, val loss: 1.4081, val acc: 0.3585  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 4660] train loss: 1.4146, train acc: 0.3599, val loss: 1.4078, val acc: 0.3578  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 4680] train loss: 1.4144, train acc: 0.3587, val loss: 1.4078, val acc: 0.3589  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 4700] train loss: 1.4155, train acc: 0.3583, val loss: 1.4078, val acc: 0.3582  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 4720] train loss: 1.4167, train acc: 0.3571, val loss: 1.4081, val acc: 0.3589  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 4740] train loss: 1.4158, train acc: 0.3587, val loss: 1.4080, val acc: 0.3589  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 4760] train loss: 1.4163, train acc: 0.3589, val loss: 1.4070, val acc: 0.3592  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 4780] train loss: 1.4152, train acc: 0.3585, val loss: 1.4069, val acc: 0.3595  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 4800] train loss: 1.4125, train acc: 0.3607, val loss: 1.4065, val acc: 0.3595  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 4820] train loss: 1.4135, train acc: 0.3600, val loss: 1.4058, val acc: 0.3592  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 4840] train loss: 1.4138, train acc: 0.3597, val loss: 1.4058, val acc: 0.3592  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 4860] train loss: 1.4122, train acc: 0.3612, val loss: 1.4060, val acc: 0.3595  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 4880] train loss: 1.4160, train acc: 0.3596, val loss: 1.4056, val acc: 0.3595  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 4900] train loss: 1.4109, train acc: 0.3599, val loss: 1.4052, val acc: 0.3592  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 4920] train loss: 1.4138, train acc: 0.3593, val loss: 1.4051, val acc: 0.3595  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 4940] train loss: 1.4125, train acc: 0.3595, val loss: 1.4050, val acc: 0.3595  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 4960] train loss: 1.4135, train acc: 0.3603, val loss: 1.4048, val acc: 0.3602  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 4980] train loss: 1.4122, train acc: 0.3592, val loss: 1.4044, val acc: 0.3602  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 5000] train loss: 1.4118, train acc: 0.3607, val loss: 1.4046, val acc: 0.3612  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 5020] train loss: 1.4102, train acc: 0.3613, val loss: 1.4039, val acc: 0.3605  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 5040] train loss: 1.4098, train acc: 0.3626, val loss: 1.4036, val acc: 0.3599  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 5060] train loss: 1.4128, train acc: 0.3601, val loss: 1.4044, val acc: 0.3622  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 5080] train loss: 1.4119, train acc: 0.3603, val loss: 1.4037, val acc: 0.3605  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 5100] train loss: 1.4112, train acc: 0.3616, val loss: 1.4028, val acc: 0.3612  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 5120] train loss: 1.4090, train acc: 0.3616, val loss: 1.4012, val acc: 0.3612  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 5140] train loss: 1.4085, train acc: 0.3621, val loss: 1.3996, val acc: 0.3636  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 5160] train loss: 1.4113, train acc: 0.3609, val loss: 1.3992, val acc: 0.3632  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 5180] train loss: 1.4119, train acc: 0.3589, val loss: 1.4033, val acc: 0.3629  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 5200] train loss: 1.4118, train acc: 0.3597, val loss: 1.4042, val acc: 0.3605  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 5220] train loss: 1.4098, train acc: 0.3592, val loss: 1.4028, val acc: 0.3619  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 5240] train loss: 1.4107, train acc: 0.3594, val loss: 1.3997, val acc: 0.3592  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 5260] train loss: 1.4088, train acc: 0.3577, val loss: 1.3962, val acc: 0.3589  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 5280] train loss: 1.4058, train acc: 0.3594, val loss: 1.3954, val acc: 0.3619  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 5300] train loss: 1.4048, train acc: 0.3586, val loss: 1.3961, val acc: 0.3592  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 5320] train loss: 1.4039, train acc: 0.3595, val loss: 1.3951, val acc: 0.3605  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 5340] train loss: 1.4040, train acc: 0.3583, val loss: 1.3946, val acc: 0.3609  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 5360] train loss: 1.4048, train acc: 0.3595, val loss: 1.3927, val acc: 0.3626  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 5380] train loss: 1.4060, train acc: 0.3576, val loss: 1.3932, val acc: 0.3639  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 5400] train loss: 1.4034, train acc: 0.3603, val loss: 1.3932, val acc: 0.3636  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 5420] train loss: 1.4037, train acc: 0.3589, val loss: 1.3937, val acc: 0.3626  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 5440] train loss: 1.4029, train acc: 0.3583, val loss: 1.3923, val acc: 0.3639  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 5460] train loss: 1.4039, train acc: 0.3586, val loss: 1.3928, val acc: 0.3632  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 5480] train loss: 1.4054, train acc: 0.3583, val loss: 1.3923, val acc: 0.3646  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 5500] train loss: 1.4010, train acc: 0.3599, val loss: 1.3926, val acc: 0.3656  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 5520] train loss: 1.4035, train acc: 0.3580, val loss: 1.3926, val acc: 0.3642  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 5540] train loss: 1.4042, train acc: 0.3599, val loss: 1.3920, val acc: 0.3642  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 5560] train loss: 1.4026, train acc: 0.3590, val loss: 1.3919, val acc: 0.3642  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 5580] train loss: 1.4041, train acc: 0.3579, val loss: 1.3916, val acc: 0.3649  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 5600] train loss: 1.4016, train acc: 0.3582, val loss: 1.3921, val acc: 0.3636  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 5620] train loss: 1.4034, train acc: 0.3597, val loss: 1.3917, val acc: 0.3642  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 5640] train loss: 1.4045, train acc: 0.3590, val loss: 1.3916, val acc: 0.3649  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 5660] train loss: 1.4030, train acc: 0.3582, val loss: 1.3916, val acc: 0.3649  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 5680] train loss: 1.4003, train acc: 0.3618, val loss: 1.3916, val acc: 0.3649  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 5700] train loss: 1.3987, train acc: 0.3621, val loss: 1.3912, val acc: 0.3656  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 5720] train loss: 1.4009, train acc: 0.3592, val loss: 1.3913, val acc: 0.3653  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 5740] train loss: 1.4015, train acc: 0.3607, val loss: 1.3910, val acc: 0.3653  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 5760] train loss: 1.4000, train acc: 0.3616, val loss: 1.3910, val acc: 0.3653  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 5780] train loss: 1.4022, train acc: 0.3577, val loss: 1.3917, val acc: 0.3646  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 5800] train loss: 1.4036, train acc: 0.3576, val loss: 1.3907, val acc: 0.3656  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 5820] train loss: 1.3995, train acc: 0.3611, val loss: 1.3909, val acc: 0.3629  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 5840] train loss: 1.4020, train acc: 0.3595, val loss: 1.3906, val acc: 0.3636  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 5860] train loss: 1.4022, train acc: 0.3606, val loss: 1.3906, val acc: 0.3622  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 5880] train loss: 1.4016, train acc: 0.3587, val loss: 1.3901, val acc: 0.3636  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 5900] train loss: 1.4016, train acc: 0.3587, val loss: 1.3899, val acc: 0.3629  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 5920] train loss: 1.4017, train acc: 0.3616, val loss: 1.3902, val acc: 0.3622  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 5940] train loss: 1.3970, train acc: 0.3616, val loss: 1.3903, val acc: 0.3622  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 5960] train loss: 1.3994, train acc: 0.3616, val loss: 1.3912, val acc: 0.3616  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 5980] train loss: 1.4011, train acc: 0.3598, val loss: 1.3902, val acc: 0.3636  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 6000] train loss: 1.3980, train acc: 0.3605, val loss: 1.3896, val acc: 0.3626  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 6020] train loss: 1.3991, train acc: 0.3591, val loss: 1.3897, val acc: 0.3639  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 6040] train loss: 1.3966, train acc: 0.3613, val loss: 1.3895, val acc: 0.3632  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 6060] train loss: 1.4002, train acc: 0.3613, val loss: 1.3899, val acc: 0.3626  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 6080] train loss: 1.3994, train acc: 0.3618, val loss: 1.3897, val acc: 0.3636  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 6100] train loss: 1.4015, train acc: 0.3602, val loss: 1.3896, val acc: 0.3646  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 6120] train loss: 1.3999, train acc: 0.3592, val loss: 1.3893, val acc: 0.3619  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 6140] train loss: 1.4021, train acc: 0.3603, val loss: 1.3894, val acc: 0.3639  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 6160] train loss: 1.4010, train acc: 0.3594, val loss: 1.3898, val acc: 0.3629  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 6180] train loss: 1.4026, train acc: 0.3593, val loss: 1.3910, val acc: 0.3656  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 6200] train loss: 1.4000, train acc: 0.3613, val loss: 1.3897, val acc: 0.3636  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 6220] train loss: 1.4018, train acc: 0.3590, val loss: 1.3896, val acc: 0.3622  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 6240] train loss: 1.3999, train acc: 0.3605, val loss: 1.3897, val acc: 0.3639  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 6260] train loss: 1.4010, train acc: 0.3584, val loss: 1.3894, val acc: 0.3619  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 6280] train loss: 1.3973, train acc: 0.3611, val loss: 1.3918, val acc: 0.3639  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 6300] train loss: 1.4389, train acc: 0.3584, val loss: 1.4924, val acc: 0.2934  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 6320] train loss: 1.4321, train acc: 0.3522, val loss: 1.4309, val acc: 0.3619  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 6340] train loss: 2.0324, train acc: 0.3394, val loss: 1.9294, val acc: 0.3595  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 6360] train loss: 1.4742, train acc: 0.3361, val loss: 1.4661, val acc: 0.3616  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 6380] train loss: 1.4507, train acc: 0.3456, val loss: 1.4416, val acc: 0.3589  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 6400] train loss: 1.4486, train acc: 0.3420, val loss: 1.4381, val acc: 0.3572  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 6420] train loss: 1.4451, train acc: 0.3442, val loss: 1.4371, val acc: 0.3565  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 6440] train loss: 1.4450, train acc: 0.3459, val loss: 1.4366, val acc: 0.3572  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 6460] train loss: 1.4455, train acc: 0.3451, val loss: 1.4363, val acc: 0.3572  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 6480] train loss: 1.4433, train acc: 0.3476, val loss: 1.4359, val acc: 0.3575  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 6500] train loss: 1.4446, train acc: 0.3462, val loss: 1.4355, val acc: 0.3575  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 6520] train loss: 1.4431, train acc: 0.3456, val loss: 1.4351, val acc: 0.3568  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 6540] train loss: 1.4439, train acc: 0.3462, val loss: 1.4347, val acc: 0.3562  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 6560] train loss: 1.4417, train acc: 0.3475, val loss: 1.4344, val acc: 0.3575  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 6580] train loss: 1.4424, train acc: 0.3469, val loss: 1.4342, val acc: 0.3568  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 6600] train loss: 1.4397, train acc: 0.3498, val loss: 1.4337, val acc: 0.3572  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 6620] train loss: 1.4419, train acc: 0.3467, val loss: 1.4334, val acc: 0.3568  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 6640] train loss: 1.4423, train acc: 0.3575, val loss: 1.4326, val acc: 0.3582  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 6660] train loss: 1.4412, train acc: 0.3610, val loss: 1.4324, val acc: 0.3602  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 6680] train loss: 1.4375, train acc: 0.3590, val loss: 1.4317, val acc: 0.3589  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 6700] train loss: 1.4373, train acc: 0.3614, val loss: 1.4311, val acc: 0.3602  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 6720] train loss: 1.4402, train acc: 0.3623, val loss: 1.4308, val acc: 0.3599  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 6740] train loss: 1.4354, train acc: 0.3622, val loss: 1.4300, val acc: 0.3605  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 6760] train loss: 1.4367, train acc: 0.3789, val loss: 1.4300, val acc: 0.3659  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 6780] train loss: 1.4363, train acc: 0.3762, val loss: 1.4293, val acc: 0.3616  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 6800] train loss: 1.4349, train acc: 0.3772, val loss: 1.4274, val acc: 0.3612  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 6820] train loss: 1.4294, train acc: 0.3776, val loss: 1.4243, val acc: 0.3639  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 6840] train loss: 1.4247, train acc: 0.3795, val loss: 1.4189, val acc: 0.3690  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 6860] train loss: 1.4207, train acc: 0.3813, val loss: 1.4047, val acc: 0.3825  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 6880] train loss: 1.4156, train acc: 0.3794, val loss: 1.4016, val acc: 0.3707  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 6900] train loss: 1.4130, train acc: 0.3805, val loss: 1.3963, val acc: 0.3784  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 6920] train loss: 1.4114, train acc: 0.3750, val loss: 1.3996, val acc: 0.3764  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 6940] train loss: 1.4115, train acc: 0.3710, val loss: 1.3996, val acc: 0.3754  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 6960] train loss: 1.4033, train acc: 0.3843, val loss: 1.3886, val acc: 0.3845  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 6980] train loss: 1.3993, train acc: 0.3766, val loss: 1.3790, val acc: 0.3791  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 7000] train loss: 1.4005, train acc: 0.3736, val loss: 1.3805, val acc: 0.3852  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 7020] train loss: 1.3930, train acc: 0.3795, val loss: 1.3776, val acc: 0.3821  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 7040] train loss: 1.3901, train acc: 0.3835, val loss: 1.3765, val acc: 0.3845  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 7060] train loss: 1.3891, train acc: 0.3852, val loss: 1.3696, val acc: 0.3936  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 7080] train loss: 1.5723, train acc: 0.3292, val loss: 1.6132, val acc: 0.3231  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 7100] train loss: 1.4637, train acc: 0.3357, val loss: 1.4664, val acc: 0.3147  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 7120] train loss: 1.4565, train acc: 0.3045, val loss: 1.4547, val acc: 0.2789  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 7140] train loss: 1.4559, train acc: 0.3496, val loss: 1.4519, val acc: 0.3231  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 7160] train loss: 1.4552, train acc: 0.3498, val loss: 1.4510, val acc: 0.3228  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 7180] train loss: 1.4535, train acc: 0.3499, val loss: 1.4505, val acc: 0.3228  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 7200] train loss: 1.4547, train acc: 0.3501, val loss: 1.4503, val acc: 0.3234  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 7220] train loss: 1.4507, train acc: 0.3527, val loss: 1.4501, val acc: 0.3234  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 7240] train loss: 1.4556, train acc: 0.3507, val loss: 1.4500, val acc: 0.3231  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 7260] train loss: 1.4514, train acc: 0.3541, val loss: 1.4501, val acc: 0.3231  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 7280] train loss: 1.4509, train acc: 0.3544, val loss: 1.4501, val acc: 0.3234  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 7300] train loss: 1.4514, train acc: 0.3542, val loss: 1.4500, val acc: 0.3238  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 7320] train loss: 1.4516, train acc: 0.3542, val loss: 1.4501, val acc: 0.3234  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 7340] train loss: 1.4519, train acc: 0.3536, val loss: 1.4500, val acc: 0.3238  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 7360] train loss: 1.4516, train acc: 0.3529, val loss: 1.4501, val acc: 0.3238  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 7380] train loss: 1.4526, train acc: 0.3549, val loss: 1.4501, val acc: 0.3238  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 7400] train loss: 1.4520, train acc: 0.3521, val loss: 1.4501, val acc: 0.3245  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 7420] train loss: 1.4525, train acc: 0.3540, val loss: 1.4501, val acc: 0.3234  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 7440] train loss: 1.4521, train acc: 0.3537, val loss: 1.4501, val acc: 0.3248  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 7460] train loss: 1.4528, train acc: 0.3563, val loss: 1.4501, val acc: 0.3248  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 7480] train loss: 1.4504, train acc: 0.3564, val loss: 1.4500, val acc: 0.3241  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 7500] train loss: 1.4507, train acc: 0.3574, val loss: 1.4500, val acc: 0.3245  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 7520] train loss: 1.4535, train acc: 0.3512, val loss: 1.4500, val acc: 0.3248  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 7540] train loss: 1.4542, train acc: 0.3541, val loss: 1.4500, val acc: 0.3245  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 7560] train loss: 1.4509, train acc: 0.3549, val loss: 1.4500, val acc: 0.3241  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 7580] train loss: 1.4532, train acc: 0.3522, val loss: 1.4500, val acc: 0.3241  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 7600] train loss: 1.4498, train acc: 0.3550, val loss: 1.4500, val acc: 0.3255  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 7620] train loss: 1.4520, train acc: 0.3532, val loss: 1.4499, val acc: 0.3245  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 7640] train loss: 1.4510, train acc: 0.3537, val loss: 1.4499, val acc: 0.3241  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 7660] train loss: 1.4512, train acc: 0.3541, val loss: 1.4499, val acc: 0.3245  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 7680] train loss: 1.4545, train acc: 0.3519, val loss: 1.4499, val acc: 0.3241  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 7700] train loss: 1.4516, train acc: 0.3524, val loss: 1.4498, val acc: 0.3241  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 7720] train loss: 1.4513, train acc: 0.3545, val loss: 1.4499, val acc: 0.3248  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 7740] train loss: 1.4512, train acc: 0.3545, val loss: 1.4500, val acc: 0.3241  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 7760] train loss: 1.4512, train acc: 0.3532, val loss: 1.4499, val acc: 0.3241  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 7780] train loss: 1.4509, train acc: 0.3529, val loss: 1.4499, val acc: 0.3245  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 7800] train loss: 1.4536, train acc: 0.3514, val loss: 1.4498, val acc: 0.3241  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 7820] train loss: 1.4551, train acc: 0.3513, val loss: 1.4498, val acc: 0.3238  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 7840] train loss: 1.4516, train acc: 0.3532, val loss: 1.4498, val acc: 0.3248  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 7860] train loss: 1.4503, train acc: 0.3536, val loss: 1.4498, val acc: 0.3251  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 7880] train loss: 1.4508, train acc: 0.3541, val loss: 1.4497, val acc: 0.3238  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 7900] train loss: 1.4498, train acc: 0.3540, val loss: 1.4498, val acc: 0.3248  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 7920] train loss: 1.4563, train acc: 0.3501, val loss: 1.4498, val acc: 0.3248  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 7940] train loss: 1.4507, train acc: 0.3523, val loss: 1.4497, val acc: 0.3251  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 7960] train loss: 1.4518, train acc: 0.3540, val loss: 1.4498, val acc: 0.3251  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 7980] train loss: 1.4517, train acc: 0.3554, val loss: 1.4497, val acc: 0.3248  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 8000] train loss: 1.4540, train acc: 0.3535, val loss: 1.4496, val acc: 0.3241  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 8020] train loss: 1.4516, train acc: 0.3527, val loss: 1.4497, val acc: 0.3248  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 8040] train loss: 1.4512, train acc: 0.3530, val loss: 1.4496, val acc: 0.3238  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 8060] train loss: 1.4513, train acc: 0.3529, val loss: 1.4496, val acc: 0.3248  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 8080] train loss: 1.4537, train acc: 0.3518, val loss: 1.4496, val acc: 0.3238  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 8100] train loss: 1.4513, train acc: 0.3550, val loss: 1.4495, val acc: 0.3251  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 8120] train loss: 1.4529, train acc: 0.3519, val loss: 1.4496, val acc: 0.3238  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 8140] train loss: 1.4517, train acc: 0.3534, val loss: 1.4495, val acc: 0.3241  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 8160] train loss: 1.4520, train acc: 0.3546, val loss: 1.4495, val acc: 0.3238  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 8180] train loss: 1.4512, train acc: 0.3522, val loss: 1.4494, val acc: 0.3251  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 8200] train loss: 1.4506, train acc: 0.3537, val loss: 1.4494, val acc: 0.3251  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 8220] train loss: 1.4480, train acc: 0.3545, val loss: 1.4495, val acc: 0.3251  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 8240] train loss: 1.4510, train acc: 0.3527, val loss: 1.4495, val acc: 0.3245  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 8260] train loss: 1.4531, train acc: 0.3539, val loss: 1.4494, val acc: 0.3251  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 8280] train loss: 1.4529, train acc: 0.3548, val loss: 1.4494, val acc: 0.3251  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 8300] train loss: 1.4515, train acc: 0.3527, val loss: 1.4494, val acc: 0.3251  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 8320] train loss: 1.4525, train acc: 0.3511, val loss: 1.4493, val acc: 0.3248  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 8340] train loss: 1.4487, train acc: 0.3546, val loss: 1.4493, val acc: 0.3251  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 8360] train loss: 1.4494, train acc: 0.3542, val loss: 1.4493, val acc: 0.3245  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 8380] train loss: 1.4523, train acc: 0.3546, val loss: 1.4493, val acc: 0.3251  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 8400] train loss: 1.4521, train acc: 0.3547, val loss: 1.4493, val acc: 0.3251  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 8420] train loss: 1.4512, train acc: 0.3527, val loss: 1.4493, val acc: 0.3251  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 8440] train loss: 1.4502, train acc: 0.3540, val loss: 1.4492, val acc: 0.3251  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 8460] train loss: 1.4523, train acc: 0.3570, val loss: 1.4492, val acc: 0.3251  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 8480] train loss: 1.4528, train acc: 0.3532, val loss: 1.4491, val acc: 0.3251  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 8500] train loss: 1.4493, train acc: 0.3529, val loss: 1.4492, val acc: 0.3251  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 8520] train loss: 1.4517, train acc: 0.3532, val loss: 1.4491, val acc: 0.3258  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 8540] train loss: 1.4490, train acc: 0.3535, val loss: 1.4491, val acc: 0.3251  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 8560] train loss: 1.4512, train acc: 0.3541, val loss: 1.4490, val acc: 0.3248  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 8580] train loss: 1.4524, train acc: 0.3531, val loss: 1.4490, val acc: 0.3248  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 8600] train loss: 1.4488, train acc: 0.3542, val loss: 1.4490, val acc: 0.3251  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 8620] train loss: 1.4497, train acc: 0.3532, val loss: 1.4489, val acc: 0.3251  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 8640] train loss: 1.4515, train acc: 0.3532, val loss: 1.4489, val acc: 0.3251  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 8660] train loss: 1.4494, train acc: 0.3535, val loss: 1.4488, val acc: 0.3261  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 8680] train loss: 1.4484, train acc: 0.3555, val loss: 1.4488, val acc: 0.3251  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 8700] train loss: 1.4507, train acc: 0.3548, val loss: 1.4488, val acc: 0.3255  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 8720] train loss: 1.4499, train acc: 0.3545, val loss: 1.4488, val acc: 0.3261  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 8740] train loss: 1.4520, train acc: 0.3507, val loss: 1.4487, val acc: 0.3261  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 8760] train loss: 1.4517, train acc: 0.3545, val loss: 1.4486, val acc: 0.3258  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 8780] train loss: 1.4509, train acc: 0.3538, val loss: 1.4486, val acc: 0.3261  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 8800] train loss: 1.4496, train acc: 0.3534, val loss: 1.4486, val acc: 0.3258  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 8820] train loss: 1.4521, train acc: 0.3531, val loss: 1.4485, val acc: 0.3265  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 8840] train loss: 1.4494, train acc: 0.3534, val loss: 1.4485, val acc: 0.3265  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 8860] train loss: 1.4507, train acc: 0.3544, val loss: 1.4483, val acc: 0.3255  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 8880] train loss: 1.4483, train acc: 0.3555, val loss: 1.4484, val acc: 0.3265  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 8900] train loss: 1.4483, train acc: 0.3545, val loss: 1.4483, val acc: 0.3261  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 8920] train loss: 1.4509, train acc: 0.3539, val loss: 1.4483, val acc: 0.3265  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 8940] train loss: 1.4492, train acc: 0.3550, val loss: 1.4482, val acc: 0.3261  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 8960] train loss: 1.4489, train acc: 0.3550, val loss: 1.4481, val acc: 0.3265  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 8980] train loss: 1.4502, train acc: 0.3541, val loss: 1.4480, val acc: 0.3265  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 9000] train loss: 1.4475, train acc: 0.3558, val loss: 1.4479, val acc: 0.3265  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 9020] train loss: 1.4503, train acc: 0.3523, val loss: 1.4479, val acc: 0.3261  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 9040] train loss: 1.4470, train acc: 0.3554, val loss: 1.4478, val acc: 0.3265  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 9060] train loss: 1.4470, train acc: 0.3536, val loss: 1.4478, val acc: 0.3265  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 9080] train loss: 1.4488, train acc: 0.3548, val loss: 1.4476, val acc: 0.3265  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 9100] train loss: 1.4480, train acc: 0.3545, val loss: 1.4474, val acc: 0.3275  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 9120] train loss: 1.4489, train acc: 0.3540, val loss: 1.4474, val acc: 0.3275  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 9140] train loss: 1.4484, train acc: 0.3551, val loss: 1.4473, val acc: 0.3282  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 9160] train loss: 1.4483, train acc: 0.3563, val loss: 1.4472, val acc: 0.3285  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 9180] train loss: 1.4477, train acc: 0.3554, val loss: 1.4470, val acc: 0.3282  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 9200] train loss: 1.4497, train acc: 0.3546, val loss: 1.4470, val acc: 0.3285  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 9220] train loss: 1.4477, train acc: 0.3557, val loss: 1.4469, val acc: 0.3292  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 9240] train loss: 1.4470, train acc: 0.3594, val loss: 1.4466, val acc: 0.3288  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 9260] train loss: 1.4460, train acc: 0.3576, val loss: 1.4465, val acc: 0.3288  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 9280] train loss: 1.4476, train acc: 0.3555, val loss: 1.4465, val acc: 0.3288  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 9300] train loss: 1.4477, train acc: 0.3561, val loss: 1.4461, val acc: 0.3288  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 9320] train loss: 1.4455, train acc: 0.3598, val loss: 1.4460, val acc: 0.3292  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 9340] train loss: 1.4453, train acc: 0.3556, val loss: 1.4459, val acc: 0.3288  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 9360] train loss: 1.4451, train acc: 0.3597, val loss: 1.4457, val acc: 0.3305  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 9380] train loss: 1.4443, train acc: 0.3561, val loss: 1.4454, val acc: 0.3298  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 9400] train loss: 1.4443, train acc: 0.3577, val loss: 1.4452, val acc: 0.3302  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 9420] train loss: 1.4439, train acc: 0.3615, val loss: 1.4450, val acc: 0.3305  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 9440] train loss: 1.4458, train acc: 0.3600, val loss: 1.4448, val acc: 0.3309  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 9460] train loss: 1.4434, train acc: 0.3596, val loss: 1.4449, val acc: 0.3312  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 9480] train loss: 1.4443, train acc: 0.3581, val loss: 1.4444, val acc: 0.3312  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 9500] train loss: 1.4434, train acc: 0.3605, val loss: 1.4440, val acc: 0.3339  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 9520] train loss: 1.4415, train acc: 0.3608, val loss: 1.4438, val acc: 0.3319  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 9540] train loss: 1.4424, train acc: 0.3634, val loss: 1.4434, val acc: 0.3336  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 9560] train loss: 1.4423, train acc: 0.3667, val loss: 1.4431, val acc: 0.3349  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 9580] train loss: 1.4422, train acc: 0.3662, val loss: 1.4428, val acc: 0.3366  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 9600] train loss: 1.4402, train acc: 0.3683, val loss: 1.4424, val acc: 0.3369  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 9620] train loss: 1.4401, train acc: 0.3707, val loss: 1.4421, val acc: 0.3393  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 9640] train loss: 1.4426, train acc: 0.3699, val loss: 1.4418, val acc: 0.3386  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 9660] train loss: 1.4410, train acc: 0.3726, val loss: 1.4414, val acc: 0.3403  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 9680] train loss: 1.4392, train acc: 0.3790, val loss: 1.4408, val acc: 0.3450  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 9700] train loss: 1.4369, train acc: 0.3784, val loss: 1.4405, val acc: 0.3457  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 9720] train loss: 1.4375, train acc: 0.3803, val loss: 1.4404, val acc: 0.3511  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 9740] train loss: 1.4389, train acc: 0.3822, val loss: 1.4397, val acc: 0.3491  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 9760] train loss: 1.4373, train acc: 0.3856, val loss: 1.4392, val acc: 0.3528  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 9780] train loss: 1.4353, train acc: 0.3877, val loss: 1.4384, val acc: 0.3568  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 9800] train loss: 1.4365, train acc: 0.3857, val loss: 1.4380, val acc: 0.3616  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 9820] train loss: 1.4330, train acc: 0.3929, val loss: 1.4380, val acc: 0.3642  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 9840] train loss: 1.4377, train acc: 0.3857, val loss: 1.4366, val acc: 0.3632  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 9860] train loss: 1.4342, train acc: 0.3904, val loss: 1.4360, val acc: 0.3669  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 9880] train loss: 1.4333, train acc: 0.3984, val loss: 1.4354, val acc: 0.3696  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 9900] train loss: 1.4337, train acc: 0.3941, val loss: 1.4355, val acc: 0.3717  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 9920] train loss: 1.4305, train acc: 0.3933, val loss: 1.4341, val acc: 0.3686  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 9940] train loss: 1.4307, train acc: 0.3948, val loss: 1.4331, val acc: 0.3710  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 9960] train loss: 1.4314, train acc: 0.3950, val loss: 1.4327, val acc: 0.3734  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 9980] train loss: 1.4271, train acc: 0.4011, val loss: 1.4328, val acc: 0.3723  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 10000] train loss: 1.4300, train acc: 0.3935, val loss: 1.4323, val acc: 0.3717  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 10020] train loss: 1.4274, train acc: 0.3934, val loss: 1.4305, val acc: 0.3727  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 10040] train loss: 1.4283, train acc: 0.3992, val loss: 1.4294, val acc: 0.3727  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 10060] train loss: 1.4275, train acc: 0.3989, val loss: 1.4295, val acc: 0.3744  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 10080] train loss: 1.4261, train acc: 0.3965, val loss: 1.4288, val acc: 0.3761  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 10100] train loss: 1.6022, train acc: 0.3176, val loss: 1.5298, val acc: 0.3352  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 10120] train loss: 1.4396, train acc: 0.3475, val loss: 1.4316, val acc: 0.3592  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 10140] train loss: 1.4368, train acc: 0.3510, val loss: 1.4291, val acc: 0.3612  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 10160] train loss: 1.4336, train acc: 0.3506, val loss: 1.4288, val acc: 0.3612  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 10180] train loss: 1.4341, train acc: 0.3535, val loss: 1.4254, val acc: 0.3612  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 10200] train loss: 1.4271, train acc: 0.3536, val loss: 1.4221, val acc: 0.3612  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 10220] train loss: 1.4289, train acc: 0.3537, val loss: 1.4206, val acc: 0.3612  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 10240] train loss: 1.4267, train acc: 0.3538, val loss: 1.4201, val acc: 0.3616  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 10260] train loss: 1.4274, train acc: 0.3520, val loss: 1.4190, val acc: 0.3619  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 10280] train loss: 1.4274, train acc: 0.3542, val loss: 1.4181, val acc: 0.3616  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 10300] train loss: 1.4254, train acc: 0.3543, val loss: 1.4169, val acc: 0.3616  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 10320] train loss: 1.4270, train acc: 0.3533, val loss: 1.4162, val acc: 0.3616  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 10340] train loss: 1.4246, train acc: 0.3527, val loss: 1.4155, val acc: 0.3612  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 10360] train loss: 1.4243, train acc: 0.3529, val loss: 1.4147, val acc: 0.3622  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 10380] train loss: 1.4261, train acc: 0.3529, val loss: 1.4140, val acc: 0.3622  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 10400] train loss: 1.4269, train acc: 0.3522, val loss: 1.4144, val acc: 0.3629  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 10420] train loss: 1.4242, train acc: 0.3532, val loss: 1.4130, val acc: 0.3619  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 10440] train loss: 1.4233, train acc: 0.3533, val loss: 1.4127, val acc: 0.3622  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 10460] train loss: 1.4187, train acc: 0.3555, val loss: 1.4121, val acc: 0.3622  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 10480] train loss: 1.4217, train acc: 0.3541, val loss: 1.4118, val acc: 0.3622  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 10500] train loss: 1.4200, train acc: 0.3535, val loss: 1.4115, val acc: 0.3622  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 10520] train loss: 1.4221, train acc: 0.3525, val loss: 1.4113, val acc: 0.3622  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 10540] train loss: 1.4213, train acc: 0.3540, val loss: 1.4110, val acc: 0.3622  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 10560] train loss: 1.4239, train acc: 0.3539, val loss: 1.4109, val acc: 0.3626  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 10580] train loss: 1.4204, train acc: 0.3551, val loss: 1.4112, val acc: 0.3629  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 10600] train loss: 1.4219, train acc: 0.3553, val loss: 1.4109, val acc: 0.3629  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 10620] train loss: 1.4184, train acc: 0.3565, val loss: 1.4103, val acc: 0.3626  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 10640] train loss: 1.4224, train acc: 0.3563, val loss: 1.4107, val acc: 0.3626  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 10660] train loss: 1.4221, train acc: 0.3561, val loss: 1.4105, val acc: 0.3616  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 10680] train loss: 1.4225, train acc: 0.3548, val loss: 1.4104, val acc: 0.3626  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 10700] train loss: 1.4216, train acc: 0.3561, val loss: 1.4101, val acc: 0.3626  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 10720] train loss: 1.4191, train acc: 0.3567, val loss: 1.4102, val acc: 0.3619  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 10740] train loss: 1.4255, train acc: 0.3540, val loss: 1.4098, val acc: 0.3626  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 10760] train loss: 1.4216, train acc: 0.3544, val loss: 1.4097, val acc: 0.3622  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 10780] train loss: 1.4201, train acc: 0.3559, val loss: 1.4098, val acc: 0.3619  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 10800] train loss: 1.4230, train acc: 0.3558, val loss: 1.4097, val acc: 0.3629  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 10820] train loss: 1.4208, train acc: 0.3551, val loss: 1.4104, val acc: 0.3619  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 10840] train loss: 1.4225, train acc: 0.3553, val loss: 1.4106, val acc: 0.3619  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 10860] train loss: 1.4205, train acc: 0.3571, val loss: 1.4095, val acc: 0.3622  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 10880] train loss: 1.4209, train acc: 0.3583, val loss: 1.4108, val acc: 0.3616  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 10900] train loss: 1.4196, train acc: 0.3565, val loss: 1.4121, val acc: 0.3619  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 10920] train loss: 1.4161, train acc: 0.3567, val loss: 1.4100, val acc: 0.3612  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 10940] train loss: 1.4206, train acc: 0.3570, val loss: 1.4096, val acc: 0.3622  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 10960] train loss: 1.4211, train acc: 0.3563, val loss: 1.4093, val acc: 0.3609  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 10980] train loss: 1.4171, train acc: 0.3568, val loss: 1.4093, val acc: 0.3619  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 11000] train loss: 1.4167, train acc: 0.3574, val loss: 1.4091, val acc: 0.3622  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 11020] train loss: 1.4177, train acc: 0.3587, val loss: 1.4090, val acc: 0.3622  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 11040] train loss: 1.4182, train acc: 0.3582, val loss: 1.4083, val acc: 0.3619  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 11060] train loss: 1.4213, train acc: 0.3723, val loss: 1.4088, val acc: 0.3605  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 11080] train loss: 1.4189, train acc: 0.3720, val loss: 1.4082, val acc: 0.3622  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 11100] train loss: 1.4204, train acc: 0.3702, val loss: 1.4083, val acc: 0.3619  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 11120] train loss: 1.4206, train acc: 0.3718, val loss: 1.4087, val acc: 0.3605  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 11140] train loss: 1.4189, train acc: 0.3757, val loss: 1.4087, val acc: 0.3609  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 11160] train loss: 1.4160, train acc: 0.3743, val loss: 1.4080, val acc: 0.3619  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 11180] train loss: 1.4195, train acc: 0.3728, val loss: 1.4082, val acc: 0.3619  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 11200] train loss: 1.4195, train acc: 0.3718, val loss: 1.4077, val acc: 0.3622  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 11220] train loss: 1.4173, train acc: 0.3738, val loss: 1.4074, val acc: 0.3622  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 11240] train loss: 1.4162, train acc: 0.3735, val loss: 1.4076, val acc: 0.3622  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 11260] train loss: 1.4168, train acc: 0.3724, val loss: 1.4076, val acc: 0.3616  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 11280] train loss: 1.4191, train acc: 0.3721, val loss: 1.4078, val acc: 0.3616  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 11300] train loss: 1.4188, train acc: 0.3710, val loss: 1.4074, val acc: 0.3612  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 11320] train loss: 1.4153, train acc: 0.3735, val loss: 1.4074, val acc: 0.3609  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 11340] train loss: 1.4150, train acc: 0.3743, val loss: 1.4068, val acc: 0.3619  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 11360] train loss: 1.4184, train acc: 0.3715, val loss: 1.4049, val acc: 0.3609  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 11380] train loss: 1.4117, train acc: 0.3725, val loss: 1.4036, val acc: 0.3609  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 11400] train loss: 1.4228, train acc: 0.3702, val loss: 1.4100, val acc: 0.3622  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 11420] train loss: 1.5429, train acc: 0.2592, val loss: 1.5362, val acc: 0.2388  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 11440] train loss: 1.5253, train acc: 0.2671, val loss: 1.5198, val acc: 0.2388  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 11460] train loss: 1.5241, train acc: 0.2269, val loss: 1.5186, val acc: 0.2293  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 11480] train loss: 1.5206, train acc: 0.2619, val loss: 1.5177, val acc: 0.2384  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 11500] train loss: 1.5195, train acc: 0.2701, val loss: 1.5166, val acc: 0.2648  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 11520] train loss: 1.5187, train acc: 0.2462, val loss: 1.5156, val acc: 0.2806  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 11540] train loss: 1.5173, train acc: 0.2497, val loss: 1.5155, val acc: 0.2712  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 11560] train loss: 1.5173, train acc: 0.2467, val loss: 1.5158, val acc: 0.2594  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 11580] train loss: 1.5162, train acc: 0.2582, val loss: 1.5150, val acc: 0.2573  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 11600] train loss: 1.5162, train acc: 0.2640, val loss: 1.5146, val acc: 0.2678  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 11620] train loss: 1.5158, train acc: 0.2731, val loss: 1.5140, val acc: 0.2735  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 11640] train loss: 1.5147, train acc: 0.2741, val loss: 1.5130, val acc: 0.2786  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 11660] train loss: 1.5140, train acc: 0.2795, val loss: 1.5122, val acc: 0.2799  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 11680] train loss: 1.5118, train acc: 0.2788, val loss: 1.5106, val acc: 0.2702  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 11700] train loss: 1.5100, train acc: 0.2880, val loss: 1.5082, val acc: 0.2816  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 11720] train loss: 1.5056, train acc: 0.2981, val loss: 1.5043, val acc: 0.2934  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 11740] train loss: 1.4984, train acc: 0.3167, val loss: 1.4973, val acc: 0.3029  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 11760] train loss: 1.4903, train acc: 0.3223, val loss: 1.4861, val acc: 0.3137  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 11780] train loss: 1.4819, train acc: 0.3188, val loss: 1.4739, val acc: 0.3147  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 11800] train loss: 1.4769, train acc: 0.3348, val loss: 1.4652, val acc: 0.3255  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 11820] train loss: 1.4731, train acc: 0.3308, val loss: 1.4598, val acc: 0.3292  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 11840] train loss: 1.4688, train acc: 0.3292, val loss: 1.4543, val acc: 0.3241  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 11860] train loss: 1.4654, train acc: 0.3298, val loss: 1.4506, val acc: 0.3282  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 11880] train loss: 1.4590, train acc: 0.3320, val loss: 1.4468, val acc: 0.3265  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 11900] train loss: 1.4559, train acc: 0.3277, val loss: 1.4438, val acc: 0.3221  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 11920] train loss: 1.4510, train acc: 0.3286, val loss: 1.4406, val acc: 0.3295  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 11940] train loss: 1.4497, train acc: 0.3307, val loss: 1.4378, val acc: 0.3231  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 11960] train loss: 1.4470, train acc: 0.3287, val loss: 1.4345, val acc: 0.3066  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 11980] train loss: 1.4422, train acc: 0.3470, val loss: 1.4306, val acc: 0.3491  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 12000] train loss: 1.4397, train acc: 0.3708, val loss: 1.4279, val acc: 0.3757  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 12020] train loss: 1.4390, train acc: 0.3812, val loss: 1.4260, val acc: 0.3838  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 12040] train loss: 1.4340, train acc: 0.3830, val loss: 1.4211, val acc: 0.3862  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 12060] train loss: 1.4310, train acc: 0.3570, val loss: 1.4188, val acc: 0.3841  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 12080] train loss: 1.4358, train acc: 0.3516, val loss: 1.4178, val acc: 0.3875  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 12100] train loss: 1.4342, train acc: 0.3538, val loss: 1.4151, val acc: 0.3862  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 12120] train loss: 1.4331, train acc: 0.3837, val loss: 1.4133, val acc: 0.3899  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 12140] train loss: 1.4294, train acc: 0.3847, val loss: 1.4109, val acc: 0.3895  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 12160] train loss: 1.4301, train acc: 0.3817, val loss: 1.4092, val acc: 0.3906  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 12180] train loss: 1.4288, train acc: 0.3795, val loss: 1.4081, val acc: 0.3909  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 12200] train loss: 1.4275, train acc: 0.3808, val loss: 1.4086, val acc: 0.3919  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 12220] train loss: 1.4278, train acc: 0.3833, val loss: 1.4064, val acc: 0.3906  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 12240] train loss: 1.4243, train acc: 0.3799, val loss: 1.4049, val acc: 0.3895  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 12260] train loss: 1.4257, train acc: 0.3810, val loss: 1.4037, val acc: 0.3909  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 12280] train loss: 1.4263, train acc: 0.3791, val loss: 1.4012, val acc: 0.3865  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 12300] train loss: 1.4190, train acc: 0.3742, val loss: 1.3982, val acc: 0.3892  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 12320] train loss: 1.4169, train acc: 0.3817, val loss: 1.3957, val acc: 0.3899  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 12340] train loss: 1.4135, train acc: 0.3827, val loss: 1.3923, val acc: 0.3892  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 12360] train loss: 1.4131, train acc: 0.3842, val loss: 1.3894, val acc: 0.3926  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 12380] train loss: 1.4144, train acc: 0.3852, val loss: 1.3874, val acc: 0.3946  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 12400] train loss: 1.4108, train acc: 0.3834, val loss: 1.3855, val acc: 0.3933  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 12420] train loss: 1.4089, train acc: 0.3910, val loss: 1.3849, val acc: 0.3973  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 12440] train loss: 1.4116, train acc: 0.3879, val loss: 1.3836, val acc: 0.3946  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 12460] train loss: 1.4068, train acc: 0.3854, val loss: 1.3826, val acc: 0.3970  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 12480] train loss: 1.4071, train acc: 0.3905, val loss: 1.3818, val acc: 0.3966  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 12500] train loss: 1.4046, train acc: 0.3926, val loss: 1.3806, val acc: 0.3949  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 12520] train loss: 1.4055, train acc: 0.3873, val loss: 1.3803, val acc: 0.3939  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 12540] train loss: 1.4040, train acc: 0.3901, val loss: 1.3797, val acc: 0.3926  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 12560] train loss: 1.4052, train acc: 0.3871, val loss: 1.3787, val acc: 0.3983  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 12580] train loss: 1.4038, train acc: 0.3942, val loss: 1.3790, val acc: 0.4000  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 12600] train loss: 1.4067, train acc: 0.3913, val loss: 1.3788, val acc: 0.3997  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 12620] train loss: 1.4021, train acc: 0.3853, val loss: 1.3778, val acc: 0.4000  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 12640] train loss: 1.4040, train acc: 0.3869, val loss: 1.3776, val acc: 0.4000  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 12660] train loss: 1.4024, train acc: 0.3922, val loss: 1.3806, val acc: 0.4044  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 12680] train loss: 1.4027, train acc: 0.3919, val loss: 1.3768, val acc: 0.4024  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 12700] train loss: 1.4043, train acc: 0.3895, val loss: 1.3774, val acc: 0.4037  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 12720] train loss: 1.4031, train acc: 0.3913, val loss: 1.3769, val acc: 0.4067  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 12740] train loss: 1.4056, train acc: 0.3883, val loss: 1.3774, val acc: 0.4044  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 12760] train loss: 1.4036, train acc: 0.3885, val loss: 1.3775, val acc: 0.4064  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 12780] train loss: 1.3981, train acc: 0.3904, val loss: 1.3762, val acc: 0.4067  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 12800] train loss: 1.4025, train acc: 0.3889, val loss: 1.3775, val acc: 0.4047  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 12820] train loss: 1.4035, train acc: 0.3937, val loss: 1.3775, val acc: 0.4064  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 12840] train loss: 1.4027, train acc: 0.3902, val loss: 1.3771, val acc: 0.4057  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 12860] train loss: 1.4000, train acc: 0.3872, val loss: 1.3770, val acc: 0.4037  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 12880] train loss: 1.4042, train acc: 0.3911, val loss: 1.3763, val acc: 0.4047  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 12900] train loss: 1.4008, train acc: 0.3913, val loss: 1.3772, val acc: 0.4071  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 12920] train loss: 1.3998, train acc: 0.3913, val loss: 1.3765, val acc: 0.4071  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 12940] train loss: 1.4005, train acc: 0.3890, val loss: 1.3765, val acc: 0.4047  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 12960] train loss: 1.4034, train acc: 0.3929, val loss: 1.3763, val acc: 0.4064  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 12980] train loss: 1.4024, train acc: 0.3881, val loss: 1.3766, val acc: 0.4037  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 13000] train loss: 1.4017, train acc: 0.3929, val loss: 1.3767, val acc: 0.4074  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 13020] train loss: 1.4043, train acc: 0.3931, val loss: 1.3764, val acc: 0.4054  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 13040] train loss: 1.4021, train acc: 0.3916, val loss: 1.3766, val acc: 0.4044  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 13060] train loss: 1.3974, train acc: 0.3933, val loss: 1.3757, val acc: 0.4047  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 13080] train loss: 1.4038, train acc: 0.3902, val loss: 1.3759, val acc: 0.4044  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 13100] train loss: 1.3985, train acc: 0.3905, val loss: 1.3758, val acc: 0.4054  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 13120] train loss: 1.4010, train acc: 0.3903, val loss: 1.3760, val acc: 0.4061  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 13140] train loss: 1.4051, train acc: 0.3903, val loss: 1.3754, val acc: 0.4047  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 13160] train loss: 1.4024, train acc: 0.3915, val loss: 1.3759, val acc: 0.4027  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 13180] train loss: 1.4038, train acc: 0.3916, val loss: 1.3756, val acc: 0.4054  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 13200] train loss: 1.4035, train acc: 0.3903, val loss: 1.3755, val acc: 0.4057  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 13220] train loss: 1.4024, train acc: 0.3926, val loss: 1.3757, val acc: 0.4044  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 13240] train loss: 1.4017, train acc: 0.3941, val loss: 1.3759, val acc: 0.4047  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 13260] train loss: 1.3994, train acc: 0.3942, val loss: 1.3760, val acc: 0.4044  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 13280] train loss: 1.3994, train acc: 0.3926, val loss: 1.3758, val acc: 0.4047  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 13300] train loss: 1.3963, train acc: 0.3927, val loss: 1.3752, val acc: 0.4037  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 13320] train loss: 1.4012, train acc: 0.3913, val loss: 1.3750, val acc: 0.4051  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 13340] train loss: 1.4009, train acc: 0.3929, val loss: 1.3755, val acc: 0.4071  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 13360] train loss: 1.3999, train acc: 0.3936, val loss: 1.3758, val acc: 0.4054  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 13380] train loss: 1.4007, train acc: 0.3938, val loss: 1.3751, val acc: 0.4044  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 13400] train loss: 1.4025, train acc: 0.3947, val loss: 1.3755, val acc: 0.4057  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 13420] train loss: 1.3981, train acc: 0.3937, val loss: 1.3748, val acc: 0.4078  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 13440] train loss: 1.4003, train acc: 0.3918, val loss: 1.3751, val acc: 0.4054  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 13460] train loss: 1.4004, train acc: 0.3926, val loss: 1.3745, val acc: 0.4078  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 13480] train loss: 1.3981, train acc: 0.3900, val loss: 1.3743, val acc: 0.4074  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 13500] train loss: 1.3998, train acc: 0.3921, val loss: 1.3750, val acc: 0.4057  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 13520] train loss: 1.4007, train acc: 0.3926, val loss: 1.3744, val acc: 0.4061  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 13540] train loss: 1.4028, train acc: 0.3935, val loss: 1.3745, val acc: 0.4071  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 13560] train loss: 1.4026, train acc: 0.3930, val loss: 1.3743, val acc: 0.4054  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 13580] train loss: 1.3976, train acc: 0.3942, val loss: 1.3746, val acc: 0.4057  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 13600] train loss: 1.4018, train acc: 0.3939, val loss: 1.3746, val acc: 0.4034  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 13620] train loss: 1.4045, train acc: 0.3869, val loss: 1.3748, val acc: 0.4027  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 13640] train loss: 1.3995, train acc: 0.3879, val loss: 1.3750, val acc: 0.4061  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 13660] train loss: 1.4025, train acc: 0.3936, val loss: 1.3752, val acc: 0.4057  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 13680] train loss: 1.3992, train acc: 0.3935, val loss: 1.3755, val acc: 0.4054  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 13700] train loss: 1.3983, train acc: 0.3913, val loss: 1.3744, val acc: 0.4078  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 13720] train loss: 1.4004, train acc: 0.3930, val loss: 1.3747, val acc: 0.4081  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 13740] train loss: 1.3974, train acc: 0.3965, val loss: 1.3738, val acc: 0.4067  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 13760] train loss: 1.4013, train acc: 0.3951, val loss: 1.3749, val acc: 0.4074  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 13780] train loss: 1.4001, train acc: 0.3955, val loss: 1.3745, val acc: 0.4084  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 13800] train loss: 1.4017, train acc: 0.3918, val loss: 1.3745, val acc: 0.4074  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 13820] train loss: 1.4014, train acc: 0.3952, val loss: 1.3751, val acc: 0.4071  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 13840] train loss: 1.4016, train acc: 0.3960, val loss: 1.3747, val acc: 0.4078  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 13860] train loss: 1.4025, train acc: 0.3930, val loss: 1.3752, val acc: 0.4071  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 13880] train loss: 1.3975, train acc: 0.3943, val loss: 1.3747, val acc: 0.4084  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 13900] train loss: 1.3980, train acc: 0.3953, val loss: 1.3747, val acc: 0.4088  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 13920] train loss: 1.4017, train acc: 0.3922, val loss: 1.3742, val acc: 0.4081  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 13940] train loss: 1.4032, train acc: 0.3909, val loss: 1.3753, val acc: 0.4054  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 13960] train loss: 1.3990, train acc: 0.3907, val loss: 1.3741, val acc: 0.4061  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 13980] train loss: 1.4010, train acc: 0.3950, val loss: 1.3757, val acc: 0.4084  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 14000] train loss: 1.4006, train acc: 0.3966, val loss: 1.3745, val acc: 0.4111  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 14020] train loss: 1.3994, train acc: 0.3960, val loss: 1.3738, val acc: 0.4094  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 14040] train loss: 1.3979, train acc: 0.3976, val loss: 1.3742, val acc: 0.4051  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 14060] train loss: 1.4003, train acc: 0.3926, val loss: 1.3748, val acc: 0.3987  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 14080] train loss: 1.4049, train acc: 0.3916, val loss: 1.3741, val acc: 0.4128  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 14100] train loss: 1.4004, train acc: 0.3974, val loss: 1.3738, val acc: 0.4135  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 14120] train loss: 1.3991, train acc: 0.3924, val loss: 1.3737, val acc: 0.4098  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 14140] train loss: 1.3989, train acc: 0.3954, val loss: 1.3745, val acc: 0.4115  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 14160] train loss: 1.4013, train acc: 0.3912, val loss: 1.3736, val acc: 0.4037  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 14180] train loss: 1.4010, train acc: 0.3945, val loss: 1.3733, val acc: 0.4108  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 14200] train loss: 1.4002, train acc: 0.3940, val loss: 1.3732, val acc: 0.4118  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 14220] train loss: 1.4015, train acc: 0.3963, val loss: 1.3734, val acc: 0.4034  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 14240] train loss: 1.3980, train acc: 0.3883, val loss: 1.3741, val acc: 0.4057  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 14260] train loss: 1.3976, train acc: 0.3960, val loss: 1.3740, val acc: 0.4098  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 14280] train loss: 1.3967, train acc: 0.3910, val loss: 1.3737, val acc: 0.4078  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 14300] train loss: 1.4015, train acc: 0.3938, val loss: 1.3739, val acc: 0.4067  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 14320] train loss: 1.4006, train acc: 0.3955, val loss: 1.3733, val acc: 0.4105  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 14340] train loss: 1.3980, train acc: 0.3957, val loss: 1.3731, val acc: 0.4088  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 14360] train loss: 1.4000, train acc: 0.3929, val loss: 1.3735, val acc: 0.4054  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 14380] train loss: 1.3998, train acc: 0.3931, val loss: 1.3733, val acc: 0.4078  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 14400] train loss: 1.3973, train acc: 0.3948, val loss: 1.3731, val acc: 0.4013  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 14420] train loss: 1.3996, train acc: 0.3946, val loss: 1.3733, val acc: 0.4027  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 14440] train loss: 1.3971, train acc: 0.3931, val loss: 1.3732, val acc: 0.4078  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 14460] train loss: 1.3981, train acc: 0.3951, val loss: 1.3725, val acc: 0.4111  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 14480] train loss: 1.3934, train acc: 0.3962, val loss: 1.3730, val acc: 0.4078  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 14500] train loss: 1.3981, train acc: 0.3952, val loss: 1.3731, val acc: 0.4044  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 14520] train loss: 1.3976, train acc: 0.3952, val loss: 1.3725, val acc: 0.4101  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 14540] train loss: 1.3941, train acc: 0.3951, val loss: 1.3724, val acc: 0.4071  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 14560] train loss: 1.3969, train acc: 0.3959, val loss: 1.3733, val acc: 0.4051  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 14580] train loss: 1.3956, train acc: 0.3929, val loss: 1.3728, val acc: 0.4030  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 14600] train loss: 1.3968, train acc: 0.3916, val loss: 1.3722, val acc: 0.4074  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 14620] train loss: 1.3961, train acc: 0.3984, val loss: 1.3723, val acc: 0.4047  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 14640] train loss: 1.3985, train acc: 0.3945, val loss: 1.3733, val acc: 0.4030  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 14660] train loss: 1.3949, train acc: 0.3994, val loss: 1.3717, val acc: 0.4047  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 14680] train loss: 1.3971, train acc: 0.3978, val loss: 1.3723, val acc: 0.4088  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 14700] train loss: 1.3975, train acc: 0.3984, val loss: 1.3723, val acc: 0.4051  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 14720] train loss: 1.3971, train acc: 0.3960, val loss: 1.3725, val acc: 0.4034  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 14740] train loss: 1.3981, train acc: 0.3947, val loss: 1.3721, val acc: 0.4034  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 14760] train loss: 1.3978, train acc: 0.3886, val loss: 1.3717, val acc: 0.4047  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 14780] train loss: 1.3948, train acc: 0.3973, val loss: 1.3721, val acc: 0.4030  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 14800] train loss: 1.3954, train acc: 0.3972, val loss: 1.3716, val acc: 0.4020  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 14820] train loss: 1.3963, train acc: 0.3916, val loss: 1.3719, val acc: 0.4061  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 14840] train loss: 1.3987, train acc: 0.3941, val loss: 1.3720, val acc: 0.4081  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 14860] train loss: 1.3947, train acc: 0.3980, val loss: 1.3720, val acc: 0.4051  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 14880] train loss: 1.3973, train acc: 0.3951, val loss: 1.3724, val acc: 0.4047  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 14900] train loss: 1.4000, train acc: 0.3952, val loss: 1.3714, val acc: 0.4084  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 14920] train loss: 1.3965, train acc: 0.3944, val loss: 1.3725, val acc: 0.4071  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 14940] train loss: 1.3974, train acc: 0.3940, val loss: 1.3724, val acc: 0.4098  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 14960] train loss: 1.3980, train acc: 0.3975, val loss: 1.3714, val acc: 0.4111  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 14980] train loss: 1.3978, train acc: 0.3946, val loss: 1.3714, val acc: 0.4091  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 15000] train loss: 1.3974, train acc: 0.3957, val loss: 1.3723, val acc: 0.4121  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 15020] train loss: 1.3959, train acc: 0.3947, val loss: 1.3710, val acc: 0.4074  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 15040] train loss: 1.4034, train acc: 0.3853, val loss: 1.3715, val acc: 0.4091  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 15060] train loss: 1.3964, train acc: 0.3969, val loss: 1.3703, val acc: 0.4105  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 15080] train loss: 1.3978, train acc: 0.3925, val loss: 1.3709, val acc: 0.4098  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 15100] train loss: 1.3991, train acc: 0.3937, val loss: 1.3703, val acc: 0.4108  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 15120] train loss: 1.3924, train acc: 0.3971, val loss: 1.3709, val acc: 0.4064  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 15140] train loss: 1.3932, train acc: 0.3974, val loss: 1.3702, val acc: 0.4101  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 15160] train loss: 1.3988, train acc: 0.3919, val loss: 1.3701, val acc: 0.4105  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 15180] train loss: 1.3957, train acc: 0.3952, val loss: 1.3705, val acc: 0.4044  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 15200] train loss: 1.3953, train acc: 0.3965, val loss: 1.3701, val acc: 0.4135  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 15220] train loss: 1.3982, train acc: 0.3888, val loss: 1.3711, val acc: 0.4074  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 15240] train loss: 1.3956, train acc: 0.3973, val loss: 1.3703, val acc: 0.4074  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 15260] train loss: 1.3974, train acc: 0.3986, val loss: 1.3698, val acc: 0.4108  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 15280] train loss: 1.3948, train acc: 0.3950, val loss: 1.3710, val acc: 0.4118  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 15300] train loss: 1.3956, train acc: 0.3963, val loss: 1.3701, val acc: 0.4071  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 15320] train loss: 1.3942, train acc: 0.3939, val loss: 1.3705, val acc: 0.4084  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 15340] train loss: 1.3986, train acc: 0.3974, val loss: 1.3695, val acc: 0.4088  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 15360] train loss: 1.3994, train acc: 0.3932, val loss: 1.3709, val acc: 0.4091  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 15380] train loss: 1.3953, train acc: 0.3978, val loss: 1.3696, val acc: 0.4054  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 15400] train loss: 1.3973, train acc: 0.3941, val loss: 1.3691, val acc: 0.4091  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 15420] train loss: 1.3967, train acc: 0.3928, val loss: 1.3708, val acc: 0.4108  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 15440] train loss: 1.3959, train acc: 0.3981, val loss: 1.3695, val acc: 0.4105  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 15460] train loss: 1.3961, train acc: 0.3966, val loss: 1.3695, val acc: 0.4067  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 15480] train loss: 1.3989, train acc: 0.3986, val loss: 1.3692, val acc: 0.4084  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 15500] train loss: 1.3968, train acc: 0.3974, val loss: 1.3683, val acc: 0.4037  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 15520] train loss: 1.3962, train acc: 0.3986, val loss: 1.3692, val acc: 0.4084  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 15540] train loss: 1.3982, train acc: 0.3961, val loss: 1.3687, val acc: 0.4101  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 15560] train loss: 1.3962, train acc: 0.3916, val loss: 1.3684, val acc: 0.4034  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 15580] train loss: 1.3983, train acc: 0.3978, val loss: 1.3687, val acc: 0.4101  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 15600] train loss: 1.3964, train acc: 0.3957, val loss: 1.3684, val acc: 0.3949  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 15620] train loss: 1.3938, train acc: 0.3957, val loss: 1.3674, val acc: 0.4074  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 15640] train loss: 1.3926, train acc: 0.3964, val loss: 1.3661, val acc: 0.4057  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 15660] train loss: 1.3924, train acc: 0.3809, val loss: 1.3666, val acc: 0.3997  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 15680] train loss: 1.3958, train acc: 0.3776, val loss: 1.3664, val acc: 0.4064  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 15700] train loss: 1.3959, train acc: 0.3804, val loss: 1.3665, val acc: 0.4071  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 15720] train loss: 1.3948, train acc: 0.3782, val loss: 1.3672, val acc: 0.4078  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 15740] train loss: 1.3946, train acc: 0.3793, val loss: 1.3662, val acc: 0.4067  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 15760] train loss: 1.3979, train acc: 0.3872, val loss: 1.3696, val acc: 0.4000  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 15780] train loss: 1.3953, train acc: 0.3744, val loss: 1.3705, val acc: 0.4081  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 15800] train loss: 1.3934, train acc: 0.3755, val loss: 1.3669, val acc: 0.4064  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 15820] train loss: 1.3965, train acc: 0.3968, val loss: 1.3659, val acc: 0.4067  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 15840] train loss: 1.3954, train acc: 0.3978, val loss: 1.3661, val acc: 0.4054  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 15860] train loss: 1.3927, train acc: 0.3939, val loss: 1.3661, val acc: 0.4051  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 15880] train loss: 1.3923, train acc: 0.3994, val loss: 1.3667, val acc: 0.4061  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 15900] train loss: 1.3913, train acc: 0.3992, val loss: 1.3652, val acc: 0.4084  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 15920] train loss: 1.3894, train acc: 0.3979, val loss: 1.3657, val acc: 0.4061  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 15940] train loss: 1.3919, train acc: 0.3987, val loss: 1.3650, val acc: 0.4094  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 15960] train loss: 1.3952, train acc: 0.3947, val loss: 1.3661, val acc: 0.4037  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 15980] train loss: 1.3935, train acc: 0.3797, val loss: 1.3670, val acc: 0.3970  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 16000] train loss: 1.3936, train acc: 0.3986, val loss: 1.3658, val acc: 0.4101  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 16020] train loss: 1.3942, train acc: 0.3958, val loss: 1.3656, val acc: 0.4081  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 16040] train loss: 1.3880, train acc: 0.3787, val loss: 1.3667, val acc: 0.4013  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 16060] train loss: 1.3965, train acc: 0.3786, val loss: 1.3655, val acc: 0.4020  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 16080] train loss: 1.3888, train acc: 0.3780, val loss: 1.3655, val acc: 0.4024  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 16100] train loss: 1.3915, train acc: 0.3875, val loss: 1.3683, val acc: 0.4128  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 16120] train loss: 1.4011, train acc: 0.3754, val loss: 1.3763, val acc: 0.4057  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 16140] train loss: 1.3980, train acc: 0.3665, val loss: 1.3670, val acc: 0.4000  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 16160] train loss: 1.3939, train acc: 0.3755, val loss: 1.3664, val acc: 0.4098  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 16180] train loss: 1.3923, train acc: 0.3736, val loss: 1.3665, val acc: 0.3990  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 16200] train loss: 1.3936, train acc: 0.3775, val loss: 1.3687, val acc: 0.3926  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 16220] train loss: 1.3911, train acc: 0.3801, val loss: 1.3658, val acc: 0.3926  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 16240] train loss: 1.3900, train acc: 0.3822, val loss: 1.3643, val acc: 0.3993  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 16260] train loss: 1.3900, train acc: 0.3835, val loss: 1.3651, val acc: 0.3936  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 16280] train loss: 1.3891, train acc: 0.3851, val loss: 1.3640, val acc: 0.4027  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 16300] train loss: 1.3848, train acc: 0.4072, val loss: 1.3744, val acc: 0.3939  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 16320] train loss: 1.3919, train acc: 0.3792, val loss: 1.3653, val acc: 0.3987  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 16340] train loss: 1.3935, train acc: 0.4007, val loss: 1.3635, val acc: 0.4010  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 16360] train loss: 1.3895, train acc: 0.4049, val loss: 1.3632, val acc: 0.3990  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 16380] train loss: 1.3905, train acc: 0.4017, val loss: 1.3630, val acc: 0.4047  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 16400] train loss: 1.3913, train acc: 0.4033, val loss: 1.3641, val acc: 0.3993  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 16420] train loss: 1.3905, train acc: 0.4064, val loss: 1.3691, val acc: 0.3929  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 16440] train loss: 1.3924, train acc: 0.3736, val loss: 1.3638, val acc: 0.4040  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 16460] train loss: 1.3890, train acc: 0.3790, val loss: 1.3621, val acc: 0.3949  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 16480] train loss: 1.3941, train acc: 0.3855, val loss: 1.3711, val acc: 0.4010  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 16500] train loss: 1.3898, train acc: 0.3847, val loss: 1.3680, val acc: 0.3976  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 16520] train loss: 1.3910, train acc: 0.3817, val loss: 1.3664, val acc: 0.3858  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 16540] train loss: 1.3816, train acc: 0.3864, val loss: 1.3640, val acc: 0.3926  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 16560] train loss: 1.3789, train acc: 0.3846, val loss: 1.3628, val acc: 0.3895  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 16580] train loss: 1.3803, train acc: 0.3861, val loss: 1.3622, val acc: 0.3912  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 16600] train loss: 1.3828, train acc: 0.3793, val loss: 1.3603, val acc: 0.3933  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 16620] train loss: 1.3852, train acc: 0.3796, val loss: 1.3610, val acc: 0.3889  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 16640] train loss: 1.3795, train acc: 0.3837, val loss: 1.3610, val acc: 0.3929  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 16660] train loss: 1.3803, train acc: 0.3840, val loss: 1.3623, val acc: 0.3946  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 16680] train loss: 1.3818, train acc: 0.3793, val loss: 1.3626, val acc: 0.3926  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 16700] train loss: 1.3802, train acc: 0.3914, val loss: 1.3609, val acc: 0.3993  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 16720] train loss: 1.3823, train acc: 0.3845, val loss: 1.3596, val acc: 0.4034  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 16740] train loss: 1.3853, train acc: 0.3870, val loss: 1.3596, val acc: 0.4040  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 16760] train loss: 1.3781, train acc: 0.3919, val loss: 1.3608, val acc: 0.3987  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 16780] train loss: 1.3991, train acc: 0.3850, val loss: 1.3731, val acc: 0.4040  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 16800] train loss: 1.3797, train acc: 0.3953, val loss: 1.3563, val acc: 0.4115  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 16820] train loss: 1.3798, train acc: 0.3982, val loss: 1.3520, val acc: 0.4078  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 16840] train loss: 1.3752, train acc: 0.4021, val loss: 1.3564, val acc: 0.4057  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 16860] train loss: 1.3747, train acc: 0.4043, val loss: 1.3495, val acc: 0.4162  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 16880] train loss: 1.3702, train acc: 0.4067, val loss: 1.3536, val acc: 0.4253  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 16900] train loss: 1.3795, train acc: 0.3991, val loss: 1.3611, val acc: 0.4125  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 16920] train loss: 1.3757, train acc: 0.4014, val loss: 1.3583, val acc: 0.4067  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 16940] train loss: 1.3771, train acc: 0.4004, val loss: 1.3512, val acc: 0.4172  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3646  @ epoch 16934 )\n",
      "[Epoch: 16960] train loss: 1.3536, train acc: 0.4111, val loss: 1.3372, val acc: 0.4132  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3536  @ epoch 16960 )\n",
      "[Epoch: 16980] train loss: 1.3535, train acc: 0.4044, val loss: 1.3290, val acc: 0.4260  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3404  @ epoch 16970 )\n",
      "[Epoch: 17000] train loss: 1.3375, train acc: 0.4211, val loss: 1.3146, val acc: 0.4253  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3367  @ epoch 16999 )\n",
      "[Epoch: 17020] train loss: 1.3578, train acc: 0.4087, val loss: 1.3615, val acc: 0.4064  (best train acc: 0.4375, best val acc: 0.4742, best train loss: 1.3203  @ epoch 17019 )\n",
      "[Epoch: 17040] train loss: 1.4732, train acc: 0.3396, val loss: 1.4568, val acc: 0.3450  (best train acc: 0.4375, best val acc: 0.4742, best train loss: 1.3203  @ epoch 17019 )\n",
      "[Epoch: 17060] train loss: 1.4158, train acc: 0.3626, val loss: 1.4075, val acc: 0.3548  (best train acc: 0.4375, best val acc: 0.4742, best train loss: 1.3203  @ epoch 17019 )\n",
      "[Epoch: 17080] train loss: 1.3972, train acc: 0.3828, val loss: 1.3894, val acc: 0.3825  (best train acc: 0.4375, best val acc: 0.4742, best train loss: 1.3203  @ epoch 17019 )\n",
      "[Epoch: 17100] train loss: 1.3768, train acc: 0.3942, val loss: 1.3625, val acc: 0.3875  (best train acc: 0.4375, best val acc: 0.4742, best train loss: 1.3203  @ epoch 17019 )\n",
      "[Epoch: 17120] train loss: 1.3772, train acc: 0.3954, val loss: 1.3527, val acc: 0.4020  (best train acc: 0.4375, best val acc: 0.4742, best train loss: 1.3203  @ epoch 17019 )\n",
      "[Epoch: 17140] train loss: 1.3777, train acc: 0.3949, val loss: 1.3645, val acc: 0.4013  (best train acc: 0.4375, best val acc: 0.4742, best train loss: 1.3203  @ epoch 17019 )\n",
      "[Epoch: 17160] train loss: 1.3772, train acc: 0.3923, val loss: 1.3590, val acc: 0.3926  (best train acc: 0.4375, best val acc: 0.4742, best train loss: 1.3203  @ epoch 17019 )\n",
      "[Epoch: 17180] train loss: 1.3575, train acc: 0.4013, val loss: 1.3447, val acc: 0.3990  (best train acc: 0.4375, best val acc: 0.4742, best train loss: 1.3203  @ epoch 17019 )\n",
      "[Epoch: 17200] train loss: 1.3549, train acc: 0.3998, val loss: 1.3342, val acc: 0.4223  (best train acc: 0.4375, best val acc: 0.4742, best train loss: 1.3203  @ epoch 17019 )\n",
      "[Epoch: 17220] train loss: 1.3589, train acc: 0.4017, val loss: 1.3509, val acc: 0.3960  (best train acc: 0.4375, best val acc: 0.4742, best train loss: 1.3203  @ epoch 17019 )\n",
      "[Epoch: 17240] train loss: 1.3907, train acc: 0.3898, val loss: 1.3802, val acc: 0.3828  (best train acc: 0.4375, best val acc: 0.4742, best train loss: 1.3203  @ epoch 17019 )\n",
      "[Epoch: 17260] train loss: 1.3751, train acc: 0.3865, val loss: 1.3620, val acc: 0.3906  (best train acc: 0.4375, best val acc: 0.4742, best train loss: 1.3203  @ epoch 17019 )\n",
      "[Epoch: 17280] train loss: 1.3653, train acc: 0.3989, val loss: 1.3501, val acc: 0.4024  (best train acc: 0.4375, best val acc: 0.4742, best train loss: 1.3203  @ epoch 17019 )\n",
      "[Epoch: 17300] train loss: 1.3559, train acc: 0.4033, val loss: 1.3450, val acc: 0.4047  (best train acc: 0.4375, best val acc: 0.4742, best train loss: 1.3203  @ epoch 17019 )\n",
      "[Epoch: 17320] train loss: 1.3453, train acc: 0.4088, val loss: 1.3402, val acc: 0.4091  (best train acc: 0.4375, best val acc: 0.4742, best train loss: 1.3203  @ epoch 17019 )\n",
      "[Epoch: 17340] train loss: 1.3317, train acc: 0.4204, val loss: 1.3083, val acc: 0.4341  (best train acc: 0.4375, best val acc: 0.4742, best train loss: 1.3203  @ epoch 17019 )\n",
      "[Epoch: 17360] train loss: 1.3810, train acc: 0.3869, val loss: 1.3498, val acc: 0.3939  (best train acc: 0.4375, best val acc: 0.4742, best train loss: 1.3203  @ epoch 17019 )\n",
      "[Epoch: 17380] train loss: 1.3364, train acc: 0.4123, val loss: 1.3190, val acc: 0.4209  (best train acc: 0.4418, best val acc: 0.4742, best train loss: 1.3194  @ epoch 17376 )\n",
      "[Epoch: 17400] train loss: 1.3245, train acc: 0.4357, val loss: 1.2995, val acc: 0.4489  (best train acc: 0.4497, best val acc: 0.4742, best train loss: 1.3056  @ epoch 17392 )\n",
      "[Epoch: 17420] train loss: 1.3156, train acc: 0.4278, val loss: 1.2874, val acc: 0.4503  (best train acc: 0.4497, best val acc: 0.4742, best train loss: 1.3003  @ epoch 17405 )\n",
      "[Epoch: 17440] train loss: 1.2974, train acc: 0.4419, val loss: 1.2713, val acc: 0.4472  (best train acc: 0.4497, best val acc: 0.4742, best train loss: 1.2943  @ epoch 17438 )\n",
      "[Epoch: 17460] train loss: 1.2903, train acc: 0.4487, val loss: 1.2619, val acc: 0.4573  (best train acc: 0.4514, best val acc: 0.4742, best train loss: 1.2903  @ epoch 17460 )\n",
      "[Epoch: 17480] train loss: 1.2911, train acc: 0.4475, val loss: 1.2562, val acc: 0.4607  (best train acc: 0.4539, best val acc: 0.4742, best train loss: 1.2831  @ epoch 17479 )\n",
      "[Epoch: 17500] train loss: 1.2838, train acc: 0.4566, val loss: 1.2509, val acc: 0.4651  (best train acc: 0.4566, best val acc: 0.4742, best train loss: 1.2822  @ epoch 17493 )\n",
      "[Epoch: 17520] train loss: 1.2802, train acc: 0.4537, val loss: 1.2480, val acc: 0.4678  (best train acc: 0.4591, best val acc: 0.4742, best train loss: 1.2763  @ epoch 17516 )\n",
      "[Epoch: 17540] train loss: 1.2813, train acc: 0.4576, val loss: 1.2450, val acc: 0.4722  (best train acc: 0.4614, best val acc: 0.4742, best train loss: 1.2763  @ epoch 17516 )\n",
      "[Epoch: 17560] train loss: 1.2777, train acc: 0.4607, val loss: 1.2423, val acc: 0.4715  (best train acc: 0.4654, best val acc: 0.4793, best train loss: 1.2751  @ epoch 17554 )\n",
      "[Epoch: 17580] train loss: 1.2740, train acc: 0.4686, val loss: 1.2449, val acc: 0.4678  (best train acc: 0.4686, best val acc: 0.4793, best train loss: 1.2740  @ epoch 17580 )\n",
      "[Epoch: 17600] train loss: 1.2735, train acc: 0.4648, val loss: 1.2391, val acc: 0.4772  (best train acc: 0.4686, best val acc: 0.4809, best train loss: 1.2735  @ epoch 17600 )\n",
      "[Epoch: 17620] train loss: 1.2716, train acc: 0.4618, val loss: 1.2416, val acc: 0.4776  (best train acc: 0.4686, best val acc: 0.4809, best train loss: 1.2716  @ epoch 17620 )\n",
      "[Epoch: 17640] train loss: 1.2728, train acc: 0.4650, val loss: 1.2364, val acc: 0.4759  (best train acc: 0.4686, best val acc: 0.4809, best train loss: 1.2699  @ epoch 17639 )\n",
      "[Epoch: 17660] train loss: 1.2743, train acc: 0.4636, val loss: 1.2370, val acc: 0.4823  (best train acc: 0.4686, best val acc: 0.4823, best train loss: 1.2669  @ epoch 17659 )\n",
      "[Epoch: 17680] train loss: 1.2692, train acc: 0.4641, val loss: 1.2349, val acc: 0.4803  (best train acc: 0.4711, best val acc: 0.4850, best train loss: 1.2669  @ epoch 17659 )\n",
      "[Epoch: 17700] train loss: 1.2709, train acc: 0.4643, val loss: 1.2326, val acc: 0.4830  (best train acc: 0.4711, best val acc: 0.4850, best train loss: 1.2669  @ epoch 17659 )\n",
      "[Epoch: 17720] train loss: 1.2682, train acc: 0.4667, val loss: 1.2330, val acc: 0.4863  (best train acc: 0.4711, best val acc: 0.4880, best train loss: 1.2669  @ epoch 17659 )\n",
      "[Epoch: 17740] train loss: 1.2693, train acc: 0.4569, val loss: 1.2317, val acc: 0.4793  (best train acc: 0.4711, best val acc: 0.4880, best train loss: 1.2665  @ epoch 17722 )\n",
      "[Epoch: 17760] train loss: 1.3153, train acc: 0.4393, val loss: 1.3039, val acc: 0.4438  (best train acc: 0.4711, best val acc: 0.4931, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 17780] train loss: 1.3184, train acc: 0.4275, val loss: 1.2794, val acc: 0.4550  (best train acc: 0.4711, best val acc: 0.4931, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 17800] train loss: 1.3778, train acc: 0.3942, val loss: 1.3314, val acc: 0.4371  (best train acc: 0.4711, best val acc: 0.4931, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 17820] train loss: 1.3086, train acc: 0.4305, val loss: 1.2654, val acc: 0.4543  (best train acc: 0.4711, best val acc: 0.4931, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 17840] train loss: 1.3108, train acc: 0.4359, val loss: 1.2782, val acc: 0.4560  (best train acc: 0.4711, best val acc: 0.4931, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 17860] train loss: 1.3464, train acc: 0.4333, val loss: 1.3086, val acc: 0.4351  (best train acc: 0.4711, best val acc: 0.4931, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 17880] train loss: 2.0068, train acc: 0.2392, val loss: 2.0570, val acc: 0.2381  (best train acc: 0.4711, best val acc: 0.4931, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 17900] train loss: 1.6310, train acc: 0.2645, val loss: 1.5573, val acc: 0.2843  (best train acc: 0.4711, best val acc: 0.4931, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 17920] train loss: 1.4671, train acc: 0.2733, val loss: 1.4343, val acc: 0.3116  (best train acc: 0.4711, best val acc: 0.4931, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 17940] train loss: 1.4337, train acc: 0.3467, val loss: 1.4047, val acc: 0.3690  (best train acc: 0.4711, best val acc: 0.4931, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 17960] train loss: 1.3979, train acc: 0.3817, val loss: 1.3612, val acc: 0.4331  (best train acc: 0.4711, best val acc: 0.4931, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 17980] train loss: 1.3913, train acc: 0.3826, val loss: 1.3489, val acc: 0.4489  (best train acc: 0.4711, best val acc: 0.4931, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 18000] train loss: 1.3895, train acc: 0.3840, val loss: 1.3533, val acc: 0.4287  (best train acc: 0.4711, best val acc: 0.4931, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 18020] train loss: 1.4176, train acc: 0.3678, val loss: 1.3721, val acc: 0.4084  (best train acc: 0.4711, best val acc: 0.4931, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 18040] train loss: 1.3805, train acc: 0.3843, val loss: 1.3408, val acc: 0.4408  (best train acc: 0.4711, best val acc: 0.4931, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 18060] train loss: 1.3692, train acc: 0.3986, val loss: 1.3301, val acc: 0.4422  (best train acc: 0.4711, best val acc: 0.4931, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 18080] train loss: 1.3700, train acc: 0.3890, val loss: 1.3336, val acc: 0.4391  (best train acc: 0.4711, best val acc: 0.4931, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 18100] train loss: 1.3686, train acc: 0.3952, val loss: 1.3374, val acc: 0.4442  (best train acc: 0.4711, best val acc: 0.4931, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 18120] train loss: 1.3571, train acc: 0.4057, val loss: 1.3302, val acc: 0.4395  (best train acc: 0.4711, best val acc: 0.4931, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 18140] train loss: 1.3496, train acc: 0.4006, val loss: 1.3113, val acc: 0.4449  (best train acc: 0.4711, best val acc: 0.4931, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 18160] train loss: 1.3486, train acc: 0.3999, val loss: 1.3068, val acc: 0.4560  (best train acc: 0.4711, best val acc: 0.4931, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 18180] train loss: 1.3500, train acc: 0.4031, val loss: 1.3047, val acc: 0.4550  (best train acc: 0.4711, best val acc: 0.4931, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 18200] train loss: 1.3430, train acc: 0.4069, val loss: 1.3048, val acc: 0.4556  (best train acc: 0.4711, best val acc: 0.4931, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 18220] train loss: 1.3467, train acc: 0.4032, val loss: 1.3275, val acc: 0.4307  (best train acc: 0.4711, best val acc: 0.4931, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 18240] train loss: 1.3573, train acc: 0.3894, val loss: 1.3205, val acc: 0.4411  (best train acc: 0.4711, best val acc: 0.4931, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 18260] train loss: 1.3395, train acc: 0.4065, val loss: 1.2972, val acc: 0.4445  (best train acc: 0.4711, best val acc: 0.4931, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 18280] train loss: 1.3359, train acc: 0.4030, val loss: 1.2927, val acc: 0.4465  (best train acc: 0.4711, best val acc: 0.4931, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 18300] train loss: 1.3344, train acc: 0.4015, val loss: 1.2881, val acc: 0.4489  (best train acc: 0.4711, best val acc: 0.4931, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 18320] train loss: 1.3323, train acc: 0.4067, val loss: 1.2850, val acc: 0.4513  (best train acc: 0.4711, best val acc: 0.4931, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 18340] train loss: 1.3335, train acc: 0.4021, val loss: 1.2893, val acc: 0.4476  (best train acc: 0.4711, best val acc: 0.4931, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 18360] train loss: 1.3294, train acc: 0.4079, val loss: 1.2854, val acc: 0.4519  (best train acc: 0.4711, best val acc: 0.4931, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 18380] train loss: 1.3228, train acc: 0.4097, val loss: 1.2792, val acc: 0.4513  (best train acc: 0.4711, best val acc: 0.4931, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 18400] train loss: 1.3285, train acc: 0.4041, val loss: 1.2745, val acc: 0.4489  (best train acc: 0.4711, best val acc: 0.4931, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 18420] train loss: 1.3194, train acc: 0.4069, val loss: 1.2687, val acc: 0.4570  (best train acc: 0.4711, best val acc: 0.4931, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 18440] train loss: 1.3206, train acc: 0.4053, val loss: 1.2651, val acc: 0.4583  (best train acc: 0.4711, best val acc: 0.4931, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 18460] train loss: 1.3159, train acc: 0.4092, val loss: 1.2619, val acc: 0.4435  (best train acc: 0.4711, best val acc: 0.4931, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 18480] train loss: 1.3151, train acc: 0.4064, val loss: 1.2590, val acc: 0.4513  (best train acc: 0.4711, best val acc: 0.4931, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 18500] train loss: 1.3774, train acc: 0.3853, val loss: 1.3600, val acc: 0.4155  (best train acc: 0.4711, best val acc: 0.4931, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 18520] train loss: 1.3801, train acc: 0.3856, val loss: 1.3480, val acc: 0.4260  (best train acc: 0.4711, best val acc: 0.4931, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 18540] train loss: 1.3772, train acc: 0.3905, val loss: 1.3446, val acc: 0.4236  (best train acc: 0.4711, best val acc: 0.4931, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 18560] train loss: 1.3725, train acc: 0.3960, val loss: 1.3381, val acc: 0.4364  (best train acc: 0.4711, best val acc: 0.4931, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 18580] train loss: 1.3685, train acc: 0.3961, val loss: 1.3380, val acc: 0.4320  (best train acc: 0.4711, best val acc: 0.4931, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 18600] train loss: 1.3708, train acc: 0.3936, val loss: 1.3357, val acc: 0.4293  (best train acc: 0.4711, best val acc: 0.4931, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 18620] train loss: 1.3668, train acc: 0.4015, val loss: 1.3347, val acc: 0.4337  (best train acc: 0.4711, best val acc: 0.4931, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 18640] train loss: 1.3668, train acc: 0.3986, val loss: 1.3347, val acc: 0.4293  (best train acc: 0.4711, best val acc: 0.4931, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 18660] train loss: 1.3674, train acc: 0.3957, val loss: 1.3357, val acc: 0.4324  (best train acc: 0.4711, best val acc: 0.4931, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 18680] train loss: 1.3645, train acc: 0.4015, val loss: 1.3348, val acc: 0.4277  (best train acc: 0.4711, best val acc: 0.4931, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 18700] train loss: 1.3698, train acc: 0.3978, val loss: 1.3396, val acc: 0.4256  (best train acc: 0.4711, best val acc: 0.4931, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 18720] train loss: 1.3719, train acc: 0.3933, val loss: 1.3351, val acc: 0.4243  (best train acc: 0.4711, best val acc: 0.4931, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 18740] train loss: 1.3663, train acc: 0.3947, val loss: 1.3350, val acc: 0.4263  (best train acc: 0.4711, best val acc: 0.4931, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 18760] train loss: 1.3682, train acc: 0.3928, val loss: 1.3334, val acc: 0.4212  (best train acc: 0.4711, best val acc: 0.4931, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 18780] train loss: 1.3652, train acc: 0.3971, val loss: 1.3300, val acc: 0.4337  (best train acc: 0.4711, best val acc: 0.4931, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 18800] train loss: 1.3583, train acc: 0.3984, val loss: 1.3232, val acc: 0.4368  (best train acc: 0.4711, best val acc: 0.4931, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 18820] train loss: 1.3549, train acc: 0.4025, val loss: 1.3229, val acc: 0.4341  (best train acc: 0.4711, best val acc: 0.4931, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 18840] train loss: 1.3068, train acc: 0.4441, val loss: 1.2540, val acc: 0.4944  (best train acc: 0.4711, best val acc: 0.4944, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 18860] train loss: 1.3042, train acc: 0.4380, val loss: 1.2408, val acc: 0.4948  (best train acc: 0.4711, best val acc: 0.5022, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 18880] train loss: 1.3352, train acc: 0.4318, val loss: 1.2811, val acc: 0.4715  (best train acc: 0.4711, best val acc: 0.5022, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 18900] train loss: 1.2788, train acc: 0.4557, val loss: 1.2349, val acc: 0.5184  (best train acc: 0.4711, best val acc: 0.5184, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 18920] train loss: 1.3748, train acc: 0.3611, val loss: 1.3483, val acc: 0.3454  (best train acc: 0.4711, best val acc: 0.5184, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 18940] train loss: 1.2921, train acc: 0.4370, val loss: 1.2237, val acc: 0.4863  (best train acc: 0.4711, best val acc: 0.5184, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 18960] train loss: 1.2828, train acc: 0.4448, val loss: 1.2116, val acc: 0.5069  (best train acc: 0.4711, best val acc: 0.5184, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 18980] train loss: 1.3918, train acc: 0.3697, val loss: 1.3569, val acc: 0.4027  (best train acc: 0.4711, best val acc: 0.5184, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 19000] train loss: 1.2892, train acc: 0.4541, val loss: 1.2407, val acc: 0.4654  (best train acc: 0.4711, best val acc: 0.5228, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 19020] train loss: 1.4599, train acc: 0.2881, val loss: 1.4389, val acc: 0.3430  (best train acc: 0.4711, best val acc: 0.5228, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 19040] train loss: 1.4291, train acc: 0.3182, val loss: 1.4088, val acc: 0.3258  (best train acc: 0.4711, best val acc: 0.5228, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 19060] train loss: 1.3929, train acc: 0.3509, val loss: 1.3410, val acc: 0.3966  (best train acc: 0.4711, best val acc: 0.5228, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 19080] train loss: 1.3061, train acc: 0.4625, val loss: 1.2680, val acc: 0.5012  (best train acc: 0.4711, best val acc: 0.5228, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 19100] train loss: 1.2683, train acc: 0.4508, val loss: 1.2512, val acc: 0.5467  (best train acc: 0.4717, best val acc: 0.5467, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 19120] train loss: 1.6037, train acc: 0.3167, val loss: 1.5620, val acc: 0.3406  (best train acc: 0.4760, best val acc: 0.5467, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 19140] train loss: 1.5052, train acc: 0.3219, val loss: 1.4689, val acc: 0.3349  (best train acc: 0.4760, best val acc: 0.5467, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 19160] train loss: 1.4491, train acc: 0.3664, val loss: 1.4290, val acc: 0.3572  (best train acc: 0.4760, best val acc: 0.5467, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 19180] train loss: 1.4425, train acc: 0.3895, val loss: 1.4234, val acc: 0.3734  (best train acc: 0.4760, best val acc: 0.5467, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 19200] train loss: 1.4356, train acc: 0.3872, val loss: 1.4192, val acc: 0.3727  (best train acc: 0.4760, best val acc: 0.5467, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 19220] train loss: 1.4354, train acc: 0.3862, val loss: 1.4183, val acc: 0.3713  (best train acc: 0.4760, best val acc: 0.5467, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 19240] train loss: 1.4334, train acc: 0.3858, val loss: 1.4167, val acc: 0.3700  (best train acc: 0.4760, best val acc: 0.5467, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 19260] train loss: 1.4319, train acc: 0.3803, val loss: 1.4162, val acc: 0.3666  (best train acc: 0.4760, best val acc: 0.5467, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 19280] train loss: 1.4295, train acc: 0.3824, val loss: 1.4152, val acc: 0.3690  (best train acc: 0.4760, best val acc: 0.5467, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 19300] train loss: 1.4311, train acc: 0.3782, val loss: 1.4123, val acc: 0.3646  (best train acc: 0.4760, best val acc: 0.5467, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 19320] train loss: 1.4236, train acc: 0.3733, val loss: 1.4096, val acc: 0.3669  (best train acc: 0.4760, best val acc: 0.5467, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 19340] train loss: 1.4233, train acc: 0.3765, val loss: 1.4070, val acc: 0.3659  (best train acc: 0.4760, best val acc: 0.5467, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 19360] train loss: 1.4247, train acc: 0.3717, val loss: 1.4050, val acc: 0.3676  (best train acc: 0.4760, best val acc: 0.5467, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 19380] train loss: 1.4186, train acc: 0.3741, val loss: 1.4026, val acc: 0.3703  (best train acc: 0.4760, best val acc: 0.5467, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 19400] train loss: 1.4156, train acc: 0.3667, val loss: 1.3964, val acc: 0.3781  (best train acc: 0.4760, best val acc: 0.5467, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 19420] train loss: 1.4088, train acc: 0.3660, val loss: 1.3865, val acc: 0.3815  (best train acc: 0.4760, best val acc: 0.5467, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 19440] train loss: 1.4018, train acc: 0.3704, val loss: 1.3760, val acc: 0.3855  (best train acc: 0.4760, best val acc: 0.5467, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 19460] train loss: 1.3808, train acc: 0.3823, val loss: 1.3446, val acc: 0.3970  (best train acc: 0.4760, best val acc: 0.5467, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 19480] train loss: 1.3724, train acc: 0.3886, val loss: 1.3369, val acc: 0.3943  (best train acc: 0.4760, best val acc: 0.5467, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 19500] train loss: 1.3296, train acc: 0.4379, val loss: 1.2796, val acc: 0.4843  (best train acc: 0.4760, best val acc: 0.5467, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 19520] train loss: 1.2868, train acc: 0.4580, val loss: 1.2257, val acc: 0.5089  (best train acc: 0.4760, best val acc: 0.5467, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 19540] train loss: 1.2661, train acc: 0.4660, val loss: 1.2029, val acc: 0.5197  (best train acc: 0.4760, best val acc: 0.5467, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 19560] train loss: 1.2521, train acc: 0.4654, val loss: 1.1915, val acc: 0.5211  (best train acc: 0.4760, best val acc: 0.5467, best train loss: 1.2509  @ epoch 19559 )\n",
      "[Epoch: 19580] train loss: 1.2498, train acc: 0.4772, val loss: 1.1827, val acc: 0.5133  (best train acc: 0.4912, best val acc: 0.5467, best train loss: 1.2433  @ epoch 19579 )\n",
      "[Epoch: 19600] train loss: 1.2433, train acc: 0.4923, val loss: 1.1854, val acc: 0.5309  (best train acc: 0.4923, best val acc: 0.5467, best train loss: 1.2423  @ epoch 19591 )\n",
      "[Epoch: 19620] train loss: 1.2518, train acc: 0.4832, val loss: 1.1903, val acc: 0.5218  (best train acc: 0.4944, best val acc: 0.5467, best train loss: 1.2423  @ epoch 19591 )\n",
      "[Epoch: 19640] train loss: 1.2727, train acc: 0.4680, val loss: 1.2138, val acc: 0.5123  (best train acc: 0.4944, best val acc: 0.5467, best train loss: 1.2423  @ epoch 19591 )\n",
      "[Epoch: 19660] train loss: 1.4498, train acc: 0.3991, val loss: 1.2826, val acc: 0.4594  (best train acc: 0.4944, best val acc: 0.5467, best train loss: 1.2423  @ epoch 19591 )\n",
      "[Epoch: 19680] train loss: 1.2731, train acc: 0.4561, val loss: 1.1907, val acc: 0.5150  (best train acc: 0.4944, best val acc: 0.5467, best train loss: 1.2423  @ epoch 19591 )\n",
      "[Epoch: 19700] train loss: 1.2526, train acc: 0.4749, val loss: 1.1926, val acc: 0.5029  (best train acc: 0.4944, best val acc: 0.5467, best train loss: 1.2423  @ epoch 19591 )\n",
      "[Epoch: 19720] train loss: 1.2454, train acc: 0.4761, val loss: 1.1884, val acc: 0.5039  (best train acc: 0.4944, best val acc: 0.5467, best train loss: 1.2406  @ epoch 19715 )\n",
      "[Epoch: 19740] train loss: 1.2469, train acc: 0.4777, val loss: 1.1821, val acc: 0.5312  (best train acc: 0.4944, best val acc: 0.5467, best train loss: 1.2406  @ epoch 19736 )\n",
      "[Epoch: 19760] train loss: 1.2324, train acc: 0.4717, val loss: 1.1674, val acc: 0.5110  (best train acc: 0.4944, best val acc: 0.5467, best train loss: 1.2311  @ epoch 19759 )\n",
      "[Epoch: 19780] train loss: 1.2248, train acc: 0.5026, val loss: 1.1638, val acc: 0.5295  (best train acc: 0.5031, best val acc: 0.5467, best train loss: 1.2191  @ epoch 19779 )\n",
      "[Epoch: 19800] train loss: 1.2203, train acc: 0.5038, val loss: 1.1510, val acc: 0.5501  (best train acc: 0.5074, best val acc: 0.5508, best train loss: 1.2162  @ epoch 19793 )\n",
      "[Epoch: 19820] train loss: 1.2176, train acc: 0.5033, val loss: 1.1477, val acc: 0.5511  (best train acc: 0.5074, best val acc: 0.5541, best train loss: 1.2122  @ epoch 19815 )\n",
      "[Epoch: 19840] train loss: 1.2179, train acc: 0.5025, val loss: 1.1455, val acc: 0.5467  (best train acc: 0.5074, best val acc: 0.5541, best train loss: 1.2114  @ epoch 19835 )\n",
      "[Epoch: 19860] train loss: 1.2128, train acc: 0.5039, val loss: 1.1441, val acc: 0.5491  (best train acc: 0.5084, best val acc: 0.5541, best train loss: 1.2084  @ epoch 19841 )\n",
      "[Epoch: 19880] train loss: 1.2139, train acc: 0.5039, val loss: 1.1406, val acc: 0.5494  (best train acc: 0.5084, best val acc: 0.5541, best train loss: 1.2079  @ epoch 19878 )\n",
      "[Epoch: 19900] train loss: 1.2239, train acc: 0.5048, val loss: 1.1569, val acc: 0.5373  (best train acc: 0.5084, best val acc: 0.5541, best train loss: 1.2079  @ epoch 19878 )\n",
      "[Epoch: 19920] train loss: 1.2198, train acc: 0.5041, val loss: 1.1524, val acc: 0.5393  (best train acc: 0.5084, best val acc: 0.5541, best train loss: 1.2079  @ epoch 19878 )\n",
      "[Epoch: 19940] train loss: 1.2209, train acc: 0.4999, val loss: 1.1466, val acc: 0.5440  (best train acc: 0.5084, best val acc: 0.5541, best train loss: 1.2075  @ epoch 19939 )\n",
      "[Epoch: 19960] train loss: 1.2164, train acc: 0.4980, val loss: 1.1453, val acc: 0.5393  (best train acc: 0.5084, best val acc: 0.5541, best train loss: 1.2075  @ epoch 19939 )\n",
      "[Epoch: 19980] train loss: 1.2098, train acc: 0.4996, val loss: 1.1422, val acc: 0.5437  (best train acc: 0.5084, best val acc: 0.5541, best train loss: 1.2075  @ epoch 19939 )\n",
      "[Epoch: 20000] train loss: 1.2086, train acc: 0.5007, val loss: 1.1423, val acc: 0.5417  (best train acc: 0.5084, best val acc: 0.5541, best train loss: 1.2052  @ epoch 19989 )\n",
      "[Epoch: 20020] train loss: 1.2153, train acc: 0.4990, val loss: 1.1403, val acc: 0.5410  (best train acc: 0.5084, best val acc: 0.5541, best train loss: 1.2052  @ epoch 19989 )\n",
      "[Epoch: 20040] train loss: 1.2127, train acc: 0.4980, val loss: 1.1397, val acc: 0.5413  (best train acc: 0.5084, best val acc: 0.5541, best train loss: 1.2052  @ epoch 19989 )\n",
      "[Epoch: 20060] train loss: 1.2127, train acc: 0.5013, val loss: 1.1403, val acc: 0.5396  (best train acc: 0.5084, best val acc: 0.5541, best train loss: 1.2049  @ epoch 20058 )\n",
      "[Epoch: 20080] train loss: 1.2063, train acc: 0.5022, val loss: 1.1385, val acc: 0.5457  (best train acc: 0.5084, best val acc: 0.5541, best train loss: 1.2029  @ epoch 20066 )\n",
      "[Epoch: 20100] train loss: 1.2071, train acc: 0.5024, val loss: 1.1372, val acc: 0.5406  (best train acc: 0.5084, best val acc: 0.5541, best train loss: 1.2029  @ epoch 20066 )\n",
      "[Epoch: 20120] train loss: 1.2080, train acc: 0.4975, val loss: 1.1362, val acc: 0.5400  (best train acc: 0.5084, best val acc: 0.5541, best train loss: 1.2020  @ epoch 20105 )\n",
      "[Epoch: 20140] train loss: 1.2083, train acc: 0.4994, val loss: 1.1385, val acc: 0.5427  (best train acc: 0.5084, best val acc: 0.5541, best train loss: 1.2020  @ epoch 20105 )\n",
      "[Epoch: 20160] train loss: 1.2108, train acc: 0.4935, val loss: 1.1374, val acc: 0.5278  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.2020  @ epoch 20105 )\n",
      "[Epoch: 20180] train loss: 1.1991, train acc: 0.4866, val loss: 1.1366, val acc: 0.5464  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1991  @ epoch 20180 )\n",
      "[Epoch: 20200] train loss: 1.1981, train acc: 0.4857, val loss: 1.1348, val acc: 0.5309  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1956  @ epoch 20195 )\n",
      "[Epoch: 20220] train loss: 1.2040, train acc: 0.4840, val loss: 1.1351, val acc: 0.5302  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 20240] train loss: 1.3484, train acc: 0.4316, val loss: 1.2902, val acc: 0.5012  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 20260] train loss: 1.4128, train acc: 0.3863, val loss: 1.3374, val acc: 0.4378  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 20280] train loss: 1.3984, train acc: 0.3782, val loss: 1.3827, val acc: 0.3818  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 20300] train loss: 1.3937, train acc: 0.3754, val loss: 1.3651, val acc: 0.3970  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 20320] train loss: 1.3866, train acc: 0.3790, val loss: 1.3622, val acc: 0.4027  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 20340] train loss: 1.3848, train acc: 0.3818, val loss: 1.3586, val acc: 0.4037  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 20360] train loss: 1.3887, train acc: 0.3733, val loss: 1.3575, val acc: 0.4020  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 20380] train loss: 1.3826, train acc: 0.3784, val loss: 1.3573, val acc: 0.4034  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 20400] train loss: 1.3838, train acc: 0.3782, val loss: 1.3559, val acc: 0.4047  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 20420] train loss: 1.3824, train acc: 0.3776, val loss: 1.3558, val acc: 0.4024  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 20440] train loss: 1.3827, train acc: 0.3816, val loss: 1.3553, val acc: 0.4054  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 20460] train loss: 1.3817, train acc: 0.3791, val loss: 1.3547, val acc: 0.4054  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 20480] train loss: 1.3801, train acc: 0.3785, val loss: 1.3538, val acc: 0.4054  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 20500] train loss: 1.3812, train acc: 0.3798, val loss: 1.3548, val acc: 0.4064  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 20520] train loss: 1.3856, train acc: 0.3776, val loss: 1.3534, val acc: 0.4051  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 20540] train loss: 1.3748, train acc: 0.3826, val loss: 1.3534, val acc: 0.4081  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 20560] train loss: 1.3796, train acc: 0.3842, val loss: 1.3537, val acc: 0.4074  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 20580] train loss: 1.3796, train acc: 0.3820, val loss: 1.3530, val acc: 0.4081  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 20600] train loss: 1.3810, train acc: 0.3894, val loss: 1.3527, val acc: 0.4064  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 20620] train loss: 1.3760, train acc: 0.3874, val loss: 1.3519, val acc: 0.4054  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 20640] train loss: 1.3815, train acc: 0.3884, val loss: 1.3522, val acc: 0.4061  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 20660] train loss: 1.3826, train acc: 0.3908, val loss: 1.3524, val acc: 0.4064  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 20680] train loss: 1.3833, train acc: 0.3882, val loss: 1.3531, val acc: 0.4071  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 20700] train loss: 1.3749, train acc: 0.3928, val loss: 1.3519, val acc: 0.4078  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 20720] train loss: 1.3777, train acc: 0.3919, val loss: 1.3520, val acc: 0.4064  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 20740] train loss: 1.3725, train acc: 0.3953, val loss: 1.3512, val acc: 0.4044  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 20760] train loss: 1.3737, train acc: 0.3949, val loss: 1.3514, val acc: 0.4071  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 20780] train loss: 1.3793, train acc: 0.4156, val loss: 1.3500, val acc: 0.4105  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 20800] train loss: 1.3790, train acc: 0.4102, val loss: 1.3496, val acc: 0.4121  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 20820] train loss: 1.3784, train acc: 0.4095, val loss: 1.3494, val acc: 0.4108  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 20840] train loss: 1.3726, train acc: 0.4148, val loss: 1.3490, val acc: 0.4101  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 20860] train loss: 1.3786, train acc: 0.4122, val loss: 1.3491, val acc: 0.4108  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 20880] train loss: 1.3762, train acc: 0.4098, val loss: 1.3488, val acc: 0.4067  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 20900] train loss: 1.3774, train acc: 0.4152, val loss: 1.3491, val acc: 0.4054  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 20920] train loss: 1.3766, train acc: 0.4109, val loss: 1.3493, val acc: 0.4067  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 20940] train loss: 1.3772, train acc: 0.4085, val loss: 1.3474, val acc: 0.4132  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 20960] train loss: 1.3776, train acc: 0.4115, val loss: 1.3468, val acc: 0.4067  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 20980] train loss: 1.3714, train acc: 0.4128, val loss: 1.3440, val acc: 0.4128  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 21000] train loss: 1.3697, train acc: 0.4126, val loss: 1.3322, val acc: 0.4074  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 21020] train loss: 1.3605, train acc: 0.4164, val loss: 1.3277, val acc: 0.4128  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 21040] train loss: 1.3572, train acc: 0.4119, val loss: 1.3249, val acc: 0.4165  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 21060] train loss: 1.3590, train acc: 0.4090, val loss: 1.3241, val acc: 0.4142  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 21080] train loss: 1.3547, train acc: 0.4091, val loss: 1.3224, val acc: 0.4159  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 21100] train loss: 1.3559, train acc: 0.4100, val loss: 1.3198, val acc: 0.4189  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 21120] train loss: 1.3579, train acc: 0.4148, val loss: 1.3181, val acc: 0.4196  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 21140] train loss: 1.3514, train acc: 0.4101, val loss: 1.3180, val acc: 0.4202  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 21160] train loss: 1.3488, train acc: 0.4130, val loss: 1.3151, val acc: 0.4196  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 21180] train loss: 1.3478, train acc: 0.4143, val loss: 1.3142, val acc: 0.4185  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 21200] train loss: 1.3482, train acc: 0.4097, val loss: 1.3123, val acc: 0.4189  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 21220] train loss: 1.3465, train acc: 0.4110, val loss: 1.3090, val acc: 0.4206  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 21240] train loss: 1.3394, train acc: 0.4154, val loss: 1.3078, val acc: 0.4277  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 21260] train loss: 1.3426, train acc: 0.4130, val loss: 1.3119, val acc: 0.4192  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 21280] train loss: 1.3395, train acc: 0.4156, val loss: 1.3090, val acc: 0.4239  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 21300] train loss: 1.3390, train acc: 0.4173, val loss: 1.3079, val acc: 0.4428  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 21320] train loss: 1.3415, train acc: 0.4246, val loss: 1.3039, val acc: 0.4401  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 21340] train loss: 1.3392, train acc: 0.4319, val loss: 1.3060, val acc: 0.4425  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 21360] train loss: 1.3355, train acc: 0.4313, val loss: 1.3039, val acc: 0.4260  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 21380] train loss: 1.3388, train acc: 0.4138, val loss: 1.3014, val acc: 0.4422  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 21400] train loss: 1.3389, train acc: 0.4276, val loss: 1.3023, val acc: 0.4516  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 21420] train loss: 1.3347, train acc: 0.4320, val loss: 1.3018, val acc: 0.4509  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 21440] train loss: 1.3329, train acc: 0.4359, val loss: 1.2987, val acc: 0.4563  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 21460] train loss: 1.3341, train acc: 0.4395, val loss: 1.2980, val acc: 0.4621  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 21480] train loss: 1.3332, train acc: 0.4385, val loss: 1.2959, val acc: 0.4621  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 21500] train loss: 1.3344, train acc: 0.4375, val loss: 1.2947, val acc: 0.4614  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 21520] train loss: 1.3388, train acc: 0.4316, val loss: 1.3031, val acc: 0.4516  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 21540] train loss: 1.3332, train acc: 0.4367, val loss: 1.3011, val acc: 0.4452  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 21560] train loss: 1.3375, train acc: 0.4308, val loss: 1.2996, val acc: 0.4543  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 21580] train loss: 1.3342, train acc: 0.4317, val loss: 1.3002, val acc: 0.4543  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 21600] train loss: 1.3318, train acc: 0.4327, val loss: 1.3001, val acc: 0.4499  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 21620] train loss: 1.3360, train acc: 0.4232, val loss: 1.2983, val acc: 0.4472  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 21640] train loss: 1.3301, train acc: 0.4352, val loss: 1.2943, val acc: 0.4513  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 21660] train loss: 1.3328, train acc: 0.4372, val loss: 1.2971, val acc: 0.4442  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 21680] train loss: 1.3369, train acc: 0.4315, val loss: 1.2978, val acc: 0.4499  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 21700] train loss: 1.3302, train acc: 0.4291, val loss: 1.2962, val acc: 0.4513  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 21720] train loss: 1.3316, train acc: 0.4333, val loss: 1.2955, val acc: 0.4482  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 21740] train loss: 1.3309, train acc: 0.4388, val loss: 1.2939, val acc: 0.4452  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 21760] train loss: 1.3243, train acc: 0.4427, val loss: 1.2945, val acc: 0.4509  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 21780] train loss: 1.3276, train acc: 0.4487, val loss: 1.2911, val acc: 0.4607  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 21800] train loss: 1.3260, train acc: 0.4458, val loss: 1.2889, val acc: 0.4658  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 21820] train loss: 1.3239, train acc: 0.4452, val loss: 1.2890, val acc: 0.4546  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 21840] train loss: 1.3220, train acc: 0.4453, val loss: 1.2871, val acc: 0.4607  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 21860] train loss: 1.3190, train acc: 0.4506, val loss: 1.2875, val acc: 0.4556  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 21880] train loss: 1.3206, train acc: 0.4487, val loss: 1.2842, val acc: 0.4570  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 21900] train loss: 1.3179, train acc: 0.4505, val loss: 1.3059, val acc: 0.4543  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 21920] train loss: 1.3519, train acc: 0.4338, val loss: 1.3161, val acc: 0.4341  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 21940] train loss: 1.3314, train acc: 0.4448, val loss: 1.2978, val acc: 0.4449  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 21960] train loss: 1.3240, train acc: 0.4539, val loss: 1.2902, val acc: 0.4644  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 21980] train loss: 1.3192, train acc: 0.4470, val loss: 1.2864, val acc: 0.4607  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 22000] train loss: 1.3169, train acc: 0.4547, val loss: 1.2797, val acc: 0.4782  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 22020] train loss: 1.3078, train acc: 0.4587, val loss: 1.2767, val acc: 0.4742  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 22040] train loss: 1.3190, train acc: 0.4558, val loss: 1.2824, val acc: 0.4675  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 22060] train loss: 1.3121, train acc: 0.4571, val loss: 1.2716, val acc: 0.4759  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 22080] train loss: 1.3042, train acc: 0.4604, val loss: 1.2684, val acc: 0.4715  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 22100] train loss: 1.3001, train acc: 0.4567, val loss: 1.2725, val acc: 0.4688  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 22120] train loss: 1.3124, train acc: 0.4521, val loss: 1.2683, val acc: 0.4610  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 22140] train loss: 1.2969, train acc: 0.4614, val loss: 1.2604, val acc: 0.4725  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 22160] train loss: 1.2946, train acc: 0.4594, val loss: 1.2551, val acc: 0.4735  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 22180] train loss: 1.6973, train acc: 0.2650, val loss: 1.6197, val acc: 0.2570  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 22200] train loss: 1.8550, train acc: 0.2547, val loss: 1.8212, val acc: 0.2371  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 22220] train loss: 1.5253, train acc: 0.2308, val loss: 1.5225, val acc: 0.2327  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 22240] train loss: 1.5222, train acc: 0.2308, val loss: 1.5216, val acc: 0.2354  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 22260] train loss: 1.5194, train acc: 0.2640, val loss: 1.5180, val acc: 0.2688  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 22280] train loss: 1.5181, train acc: 0.2538, val loss: 1.5178, val acc: 0.2317  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 22300] train loss: 1.5181, train acc: 0.2038, val loss: 1.5178, val acc: 0.2317  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 22320] train loss: 1.5181, train acc: 0.2097, val loss: 1.5178, val acc: 0.2314  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 22340] train loss: 1.5183, train acc: 0.2067, val loss: 1.5178, val acc: 0.2317  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 22360] train loss: 1.5177, train acc: 0.2177, val loss: 1.5178, val acc: 0.2304  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 22380] train loss: 1.5181, train acc: 0.2099, val loss: 1.5177, val acc: 0.2320  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 22400] train loss: 1.5176, train acc: 0.2107, val loss: 1.5177, val acc: 0.2317  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 22420] train loss: 1.5175, train acc: 0.2173, val loss: 1.5177, val acc: 0.2334  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 22440] train loss: 1.5174, train acc: 0.2184, val loss: 1.5177, val acc: 0.2381  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 22460] train loss: 1.5178, train acc: 0.2154, val loss: 1.5176, val acc: 0.2331  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 22480] train loss: 1.5175, train acc: 0.2180, val loss: 1.5176, val acc: 0.2327  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 22500] train loss: 1.5178, train acc: 0.2230, val loss: 1.5175, val acc: 0.2334  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 22520] train loss: 1.5173, train acc: 0.2159, val loss: 1.5174, val acc: 0.2307  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 22540] train loss: 1.5175, train acc: 0.2169, val loss: 1.5172, val acc: 0.2334  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 22560] train loss: 1.5175, train acc: 0.2143, val loss: 1.5171, val acc: 0.2304  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 22580] train loss: 1.5173, train acc: 0.2265, val loss: 1.5170, val acc: 0.2401  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 22600] train loss: 1.5167, train acc: 0.2353, val loss: 1.5164, val acc: 0.2364  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 22620] train loss: 1.5149, train acc: 0.2460, val loss: 1.5149, val acc: 0.2583  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 22640] train loss: 1.5146, train acc: 0.2785, val loss: 1.5140, val acc: 0.2786  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 22660] train loss: 1.5135, train acc: 0.2228, val loss: 1.5140, val acc: 0.2034  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 22680] train loss: 1.5133, train acc: 0.2809, val loss: 1.5135, val acc: 0.3039  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 22700] train loss: 1.5176, train acc: 0.2707, val loss: 1.5175, val acc: 0.2600  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 22720] train loss: 1.5169, train acc: 0.2084, val loss: 1.5171, val acc: 0.2121  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 22740] train loss: 1.5171, train acc: 0.2177, val loss: 1.5169, val acc: 0.2270  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 22760] train loss: 1.5162, train acc: 0.2161, val loss: 1.5168, val acc: 0.2253  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 22780] train loss: 1.5166, train acc: 0.2287, val loss: 1.5168, val acc: 0.2246  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 22800] train loss: 1.5168, train acc: 0.2196, val loss: 1.5167, val acc: 0.2250  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 22820] train loss: 1.5165, train acc: 0.2371, val loss: 1.5166, val acc: 0.2462  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 22840] train loss: 1.5159, train acc: 0.2313, val loss: 1.5165, val acc: 0.2384  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 22860] train loss: 1.5160, train acc: 0.2222, val loss: 1.5164, val acc: 0.2250  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 22880] train loss: 1.5155, train acc: 0.2449, val loss: 1.5162, val acc: 0.2587  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 22900] train loss: 1.5150, train acc: 0.2338, val loss: 1.5161, val acc: 0.2411  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 22920] train loss: 1.5155, train acc: 0.2337, val loss: 1.5159, val acc: 0.2418  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 22940] train loss: 1.5159, train acc: 0.2424, val loss: 1.5154, val acc: 0.2550  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 22960] train loss: 1.5146, train acc: 0.2451, val loss: 1.5149, val acc: 0.2567  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 22980] train loss: 1.6699, train acc: 0.2542, val loss: 1.6584, val acc: 0.2374  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 23000] train loss: 1.5263, train acc: 0.2633, val loss: 1.5268, val acc: 0.2577  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 23020] train loss: 1.5390, train acc: 0.2821, val loss: 1.5363, val acc: 0.2691  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 23040] train loss: 1.5228, train acc: 0.2243, val loss: 1.5217, val acc: 0.2344  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 23060] train loss: 1.5153, train acc: 0.2172, val loss: 1.5147, val acc: 0.2175  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 23080] train loss: 1.5144, train acc: 0.2534, val loss: 1.5141, val acc: 0.2556  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 23100] train loss: 1.5139, train acc: 0.2544, val loss: 1.5135, val acc: 0.2567  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 23120] train loss: 1.5140, train acc: 0.2458, val loss: 1.5132, val acc: 0.2499  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 23140] train loss: 1.5131, train acc: 0.2599, val loss: 1.5130, val acc: 0.2644  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 23160] train loss: 1.5131, train acc: 0.2621, val loss: 1.5128, val acc: 0.2641  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 23180] train loss: 1.5127, train acc: 0.2594, val loss: 1.5126, val acc: 0.2614  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 23200] train loss: 1.5120, train acc: 0.2627, val loss: 1.5124, val acc: 0.2671  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 23220] train loss: 1.5121, train acc: 0.2568, val loss: 1.5122, val acc: 0.2627  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 23240] train loss: 1.5123, train acc: 0.2658, val loss: 1.5120, val acc: 0.2614  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 23260] train loss: 1.5112, train acc: 0.2660, val loss: 1.5117, val acc: 0.2651  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 23280] train loss: 1.5120, train acc: 0.2661, val loss: 1.5115, val acc: 0.2627  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 23300] train loss: 1.5109, train acc: 0.2667, val loss: 1.5112, val acc: 0.2661  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 23320] train loss: 1.5114, train acc: 0.2668, val loss: 1.5109, val acc: 0.2722  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 23340] train loss: 1.5112, train acc: 0.2681, val loss: 1.5106, val acc: 0.2749  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 23360] train loss: 1.5100, train acc: 0.2777, val loss: 1.5103, val acc: 0.2806  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 23380] train loss: 1.5104, train acc: 0.2745, val loss: 1.5099, val acc: 0.2793  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 23400] train loss: 1.5096, train acc: 0.2763, val loss: 1.5095, val acc: 0.2833  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 23420] train loss: 1.5097, train acc: 0.2840, val loss: 1.5090, val acc: 0.2887  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 23440] train loss: 1.5092, train acc: 0.2786, val loss: 1.5085, val acc: 0.2931  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 23460] train loss: 1.5084, train acc: 0.2822, val loss: 1.5079, val acc: 0.2874  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 23480] train loss: 1.5087, train acc: 0.2854, val loss: 1.5067, val acc: 0.2887  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 23500] train loss: 1.5056, train acc: 0.2854, val loss: 1.5044, val acc: 0.2793  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 23520] train loss: 1.5030, train acc: 0.2996, val loss: 1.5020, val acc: 0.3042  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 23540] train loss: 1.5032, train acc: 0.2971, val loss: 1.5002, val acc: 0.3032  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 23560] train loss: 1.5027, train acc: 0.2980, val loss: 1.4987, val acc: 0.3035  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 23580] train loss: 1.5012, train acc: 0.2989, val loss: 1.4973, val acc: 0.3062  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 23600] train loss: 1.4991, train acc: 0.2997, val loss: 1.4955, val acc: 0.3049  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 23620] train loss: 1.4963, train acc: 0.3089, val loss: 1.4921, val acc: 0.3056  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 23640] train loss: 1.4947, train acc: 0.3091, val loss: 1.4878, val acc: 0.3140  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 23660] train loss: 1.4919, train acc: 0.3146, val loss: 1.4843, val acc: 0.3056  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 23680] train loss: 1.4913, train acc: 0.3094, val loss: 1.4826, val acc: 0.3133  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 23700] train loss: 1.4891, train acc: 0.3169, val loss: 1.4814, val acc: 0.3133  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 23720] train loss: 1.4892, train acc: 0.3172, val loss: 1.4808, val acc: 0.3201  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 23740] train loss: 1.4894, train acc: 0.3135, val loss: 1.4800, val acc: 0.3207  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 23760] train loss: 1.4875, train acc: 0.3206, val loss: 1.4796, val acc: 0.3248  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 23780] train loss: 1.4879, train acc: 0.3198, val loss: 1.4785, val acc: 0.3245  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 23800] train loss: 1.4870, train acc: 0.3194, val loss: 1.4768, val acc: 0.3241  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 23820] train loss: 1.4814, train acc: 0.3270, val loss: 1.4738, val acc: 0.3315  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 23840] train loss: 1.4791, train acc: 0.3361, val loss: 1.4697, val acc: 0.3396  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 23860] train loss: 1.4721, train acc: 0.3379, val loss: 1.4630, val acc: 0.3366  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 23880] train loss: 1.4603, train acc: 0.3547, val loss: 1.4464, val acc: 0.3582  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 23900] train loss: 1.4517, train acc: 0.3648, val loss: 1.4327, val acc: 0.3636  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 23920] train loss: 1.8682, train acc: 0.2371, val loss: 1.8019, val acc: 0.2371  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 23940] train loss: 1.5375, train acc: 0.2371, val loss: 1.5333, val acc: 0.2691  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 23960] train loss: 1.5108, train acc: 0.2593, val loss: 1.5106, val acc: 0.2445  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 23980] train loss: 1.5034, train acc: 0.3263, val loss: 1.5023, val acc: 0.2911  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 24000] train loss: 1.4982, train acc: 0.3142, val loss: 1.4996, val acc: 0.2934  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 24020] train loss: 1.4964, train acc: 0.3008, val loss: 1.4985, val acc: 0.2236  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 24040] train loss: 1.4900, train acc: 0.3237, val loss: 1.4909, val acc: 0.3147  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 24060] train loss: 1.4857, train acc: 0.3310, val loss: 1.4868, val acc: 0.3265  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 24080] train loss: 1.5006, train acc: 0.3265, val loss: 1.4972, val acc: 0.3707  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 24100] train loss: 1.4832, train acc: 0.3156, val loss: 1.4789, val acc: 0.3228  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 24120] train loss: 1.4746, train acc: 0.3082, val loss: 1.4651, val acc: 0.3194  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 24140] train loss: 1.4616, train acc: 0.3483, val loss: 1.4509, val acc: 0.3241  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 24160] train loss: 1.4530, train acc: 0.3258, val loss: 1.4442, val acc: 0.3396  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 24180] train loss: 1.4467, train acc: 0.3455, val loss: 1.4352, val acc: 0.3680  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 24200] train loss: 1.4385, train acc: 0.3480, val loss: 1.4285, val acc: 0.3720  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 24220] train loss: 1.4354, train acc: 0.3513, val loss: 1.4222, val acc: 0.4236  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 24240] train loss: 1.4303, train acc: 0.3921, val loss: 1.4171, val acc: 0.4236  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 24260] train loss: 1.4268, train acc: 0.3858, val loss: 1.4130, val acc: 0.4250  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 24280] train loss: 1.4249, train acc: 0.3865, val loss: 1.4103, val acc: 0.4165  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 24300] train loss: 1.4238, train acc: 0.3898, val loss: 1.4080, val acc: 0.4111  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 24320] train loss: 1.4213, train acc: 0.3905, val loss: 1.4071, val acc: 0.4135  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 24340] train loss: 1.4183, train acc: 0.3903, val loss: 1.4053, val acc: 0.4165  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 24360] train loss: 1.4175, train acc: 0.3903, val loss: 1.4036, val acc: 0.4159  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 24380] train loss: 1.4172, train acc: 0.3913, val loss: 1.4023, val acc: 0.4179  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 24400] train loss: 1.4238, train acc: 0.3859, val loss: 1.4041, val acc: 0.4121  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 24420] train loss: 1.4193, train acc: 0.3871, val loss: 1.3982, val acc: 0.4287  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 24440] train loss: 1.4158, train acc: 0.3918, val loss: 1.3955, val acc: 0.4226  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 24460] train loss: 1.4141, train acc: 0.3916, val loss: 1.3950, val acc: 0.4273  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 24480] train loss: 1.4116, train acc: 0.3955, val loss: 1.3929, val acc: 0.4223  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 24500] train loss: 1.4110, train acc: 0.3924, val loss: 1.3909, val acc: 0.4250  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 24520] train loss: 1.4085, train acc: 0.3942, val loss: 1.3898, val acc: 0.4209  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 24540] train loss: 1.4098, train acc: 0.3957, val loss: 1.3892, val acc: 0.4250  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 24560] train loss: 1.4062, train acc: 0.3974, val loss: 1.3878, val acc: 0.4253  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 24580] train loss: 1.4082, train acc: 0.3897, val loss: 1.3866, val acc: 0.4243  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 24600] train loss: 1.4065, train acc: 0.3946, val loss: 1.3853, val acc: 0.4236  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 24620] train loss: 1.4051, train acc: 0.3961, val loss: 1.3842, val acc: 0.4260  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 24640] train loss: 1.4028, train acc: 0.3987, val loss: 1.3839, val acc: 0.4260  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 24660] train loss: 1.4040, train acc: 0.3937, val loss: 1.3827, val acc: 0.4253  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 24680] train loss: 1.4003, train acc: 0.3967, val loss: 1.3812, val acc: 0.4250  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 24700] train loss: 1.4024, train acc: 0.3955, val loss: 1.3817, val acc: 0.4243  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 24720] train loss: 1.4015, train acc: 0.3904, val loss: 1.3811, val acc: 0.4233  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 24740] train loss: 1.4021, train acc: 0.3946, val loss: 1.3802, val acc: 0.4243  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 24760] train loss: 1.3983, train acc: 0.3980, val loss: 1.3790, val acc: 0.4256  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 24780] train loss: 1.3988, train acc: 0.3973, val loss: 1.3798, val acc: 0.4246  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 24800] train loss: 1.4002, train acc: 0.3935, val loss: 1.3785, val acc: 0.4253  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 24820] train loss: 1.4000, train acc: 0.3959, val loss: 1.3791, val acc: 0.4212  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 24840] train loss: 1.3982, train acc: 0.3959, val loss: 1.3774, val acc: 0.4263  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 24860] train loss: 1.3989, train acc: 0.3960, val loss: 1.3769, val acc: 0.4256  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 24880] train loss: 1.4012, train acc: 0.3945, val loss: 1.3766, val acc: 0.4239  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 24900] train loss: 1.3983, train acc: 0.3971, val loss: 1.3760, val acc: 0.4246  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 24920] train loss: 1.3960, train acc: 0.3955, val loss: 1.3745, val acc: 0.4260  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 24940] train loss: 1.3988, train acc: 0.3948, val loss: 1.3743, val acc: 0.4246  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 24960] train loss: 1.3962, train acc: 0.3995, val loss: 1.3749, val acc: 0.4229  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 24980] train loss: 1.3928, train acc: 0.3971, val loss: 1.3735, val acc: 0.4253  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 25000] train loss: 1.3976, train acc: 0.3940, val loss: 1.3742, val acc: 0.4219  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 25020] train loss: 1.3959, train acc: 0.3997, val loss: 1.3734, val acc: 0.4236  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 25040] train loss: 1.3944, train acc: 0.3981, val loss: 1.3730, val acc: 0.4229  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 25060] train loss: 1.3933, train acc: 0.3955, val loss: 1.3717, val acc: 0.4270  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 25080] train loss: 1.3945, train acc: 0.3953, val loss: 1.3718, val acc: 0.4253  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 25100] train loss: 1.3958, train acc: 0.3929, val loss: 1.3719, val acc: 0.4233  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 25120] train loss: 1.3939, train acc: 0.3970, val loss: 1.3713, val acc: 0.4266  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 25140] train loss: 1.3910, train acc: 0.3985, val loss: 1.3704, val acc: 0.4233  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 25160] train loss: 1.3942, train acc: 0.3952, val loss: 1.3715, val acc: 0.4246  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 25180] train loss: 1.3929, train acc: 0.3958, val loss: 1.3737, val acc: 0.4233  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 25200] train loss: 1.3970, train acc: 0.3861, val loss: 1.3697, val acc: 0.4236  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 25220] train loss: 1.3950, train acc: 0.3942, val loss: 1.3690, val acc: 0.4236  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 25240] train loss: 1.3898, train acc: 0.4007, val loss: 1.3690, val acc: 0.4223  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 25260] train loss: 1.3907, train acc: 0.3978, val loss: 1.3687, val acc: 0.4223  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 25280] train loss: 1.3890, train acc: 0.3962, val loss: 1.3681, val acc: 0.4266  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 25300] train loss: 1.3877, train acc: 0.3994, val loss: 1.3698, val acc: 0.4243  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 25320] train loss: 1.3905, train acc: 0.3956, val loss: 1.3679, val acc: 0.4212  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 25340] train loss: 1.3909, train acc: 0.3981, val loss: 1.3689, val acc: 0.4233  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 25360] train loss: 1.3930, train acc: 0.3946, val loss: 1.3681, val acc: 0.4226  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 25380] train loss: 1.3911, train acc: 0.3962, val loss: 1.3675, val acc: 0.4236  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 25400] train loss: 1.3891, train acc: 0.3953, val loss: 1.3673, val acc: 0.4226  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 25420] train loss: 1.3899, train acc: 0.3959, val loss: 1.3664, val acc: 0.4239  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 25440] train loss: 1.3900, train acc: 0.3952, val loss: 1.3667, val acc: 0.4223  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 25460] train loss: 1.3914, train acc: 0.3924, val loss: 1.3664, val acc: 0.4226  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 25480] train loss: 1.3898, train acc: 0.3976, val loss: 1.3671, val acc: 0.4250  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 25500] train loss: 1.3864, train acc: 0.4001, val loss: 1.3662, val acc: 0.4226  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 25520] train loss: 1.3863, train acc: 0.3978, val loss: 1.3662, val acc: 0.4223  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 25540] train loss: 1.3895, train acc: 0.3934, val loss: 1.3657, val acc: 0.4239  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 25560] train loss: 1.3874, train acc: 0.3937, val loss: 1.3652, val acc: 0.4226  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 25580] train loss: 1.3883, train acc: 0.3961, val loss: 1.3655, val acc: 0.4246  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 25600] train loss: 1.3882, train acc: 0.3933, val loss: 1.3659, val acc: 0.4216  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 25620] train loss: 1.3837, train acc: 0.3978, val loss: 1.3644, val acc: 0.4250  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 25640] train loss: 1.3875, train acc: 0.3966, val loss: 1.3652, val acc: 0.4209  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 25660] train loss: 1.3873, train acc: 0.3958, val loss: 1.3650, val acc: 0.4202  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 25680] train loss: 1.3894, train acc: 0.3933, val loss: 1.3643, val acc: 0.4236  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 25700] train loss: 1.3884, train acc: 0.4030, val loss: 1.3646, val acc: 0.4206  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 25720] train loss: 1.3860, train acc: 0.4021, val loss: 1.3648, val acc: 0.4226  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 25740] train loss: 1.3828, train acc: 0.4049, val loss: 1.3636, val acc: 0.4226  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 25760] train loss: 1.3844, train acc: 0.4030, val loss: 1.3641, val acc: 0.4233  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 25780] train loss: 1.3850, train acc: 0.4062, val loss: 1.3642, val acc: 0.4223  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 25800] train loss: 1.3824, train acc: 0.4081, val loss: 1.3633, val acc: 0.4209  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 25820] train loss: 1.3837, train acc: 0.4071, val loss: 1.3650, val acc: 0.4250  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 25840] train loss: 1.3855, train acc: 0.4041, val loss: 1.3633, val acc: 0.4199  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 25860] train loss: 1.3845, train acc: 0.4044, val loss: 1.3626, val acc: 0.4206  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 25880] train loss: 1.3852, train acc: 0.4031, val loss: 1.3636, val acc: 0.4226  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 25900] train loss: 1.3842, train acc: 0.4106, val loss: 1.3646, val acc: 0.4280  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 25920] train loss: 1.3812, train acc: 0.4119, val loss: 1.3628, val acc: 0.4206  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 25940] train loss: 1.3823, train acc: 0.4113, val loss: 1.3625, val acc: 0.4196  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 25960] train loss: 1.3812, train acc: 0.4132, val loss: 1.3619, val acc: 0.4219  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 25980] train loss: 1.3821, train acc: 0.4142, val loss: 1.3623, val acc: 0.4185  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 26000] train loss: 1.3834, train acc: 0.4155, val loss: 1.3649, val acc: 0.4250  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 26020] train loss: 1.3827, train acc: 0.4157, val loss: 1.3636, val acc: 0.4260  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 26040] train loss: 1.3866, train acc: 0.4158, val loss: 1.3656, val acc: 0.4273  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 26060] train loss: 1.3838, train acc: 0.4169, val loss: 1.3625, val acc: 0.4229  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 26080] train loss: 1.3821, train acc: 0.4169, val loss: 1.3621, val acc: 0.4202  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 26100] train loss: 1.3823, train acc: 0.4153, val loss: 1.3618, val acc: 0.4219  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 26120] train loss: 1.3845, train acc: 0.4176, val loss: 1.3616, val acc: 0.4209  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 26140] train loss: 1.3810, train acc: 0.4160, val loss: 1.3626, val acc: 0.4192  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 26160] train loss: 1.3799, train acc: 0.4214, val loss: 1.3620, val acc: 0.4223  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 26180] train loss: 1.3794, train acc: 0.4190, val loss: 1.3620, val acc: 0.4239  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 26200] train loss: 1.3809, train acc: 0.4187, val loss: 1.3612, val acc: 0.4189  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 26220] train loss: 1.3828, train acc: 0.4168, val loss: 1.3625, val acc: 0.4273  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 26240] train loss: 1.3778, train acc: 0.4231, val loss: 1.3617, val acc: 0.4223  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 26260] train loss: 1.3800, train acc: 0.4200, val loss: 1.3608, val acc: 0.4219  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 26280] train loss: 1.3805, train acc: 0.4183, val loss: 1.3628, val acc: 0.4243  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 26300] train loss: 1.3832, train acc: 0.4180, val loss: 1.3610, val acc: 0.4196  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 26320] train loss: 1.3797, train acc: 0.4199, val loss: 1.3609, val acc: 0.4196  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 26340] train loss: 1.3773, train acc: 0.4203, val loss: 1.3598, val acc: 0.4226  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 26360] train loss: 1.3797, train acc: 0.4184, val loss: 1.3604, val acc: 0.4212  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 26380] train loss: 1.3778, train acc: 0.4200, val loss: 1.3600, val acc: 0.4199  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 26400] train loss: 1.3759, train acc: 0.4208, val loss: 1.3606, val acc: 0.4239  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 26420] train loss: 1.3800, train acc: 0.4216, val loss: 1.3607, val acc: 0.4212  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 26440] train loss: 1.3854, train acc: 0.4262, val loss: 1.3639, val acc: 0.4270  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 26460] train loss: 1.3816, train acc: 0.4228, val loss: 1.3608, val acc: 0.4236  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 26480] train loss: 1.3756, train acc: 0.4240, val loss: 1.3601, val acc: 0.4226  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 26500] train loss: 1.3757, train acc: 0.4247, val loss: 1.3599, val acc: 0.4229  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 26520] train loss: 1.3794, train acc: 0.4214, val loss: 1.3605, val acc: 0.4229  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 26540] train loss: 1.3766, train acc: 0.4235, val loss: 1.3611, val acc: 0.4236  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 26560] train loss: 1.3765, train acc: 0.4263, val loss: 1.3600, val acc: 0.4229  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 26580] train loss: 1.3762, train acc: 0.4255, val loss: 1.3587, val acc: 0.4206  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 26600] train loss: 1.3746, train acc: 0.4243, val loss: 1.3615, val acc: 0.4280  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 26620] train loss: 1.3777, train acc: 0.4211, val loss: 1.3587, val acc: 0.4236  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 26640] train loss: 1.3758, train acc: 0.4198, val loss: 1.3581, val acc: 0.4229  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 26660] train loss: 1.3786, train acc: 0.4216, val loss: 1.3585, val acc: 0.4209  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 26680] train loss: 1.3770, train acc: 0.4220, val loss: 1.3596, val acc: 0.4229  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 26700] train loss: 1.3761, train acc: 0.4213, val loss: 1.3603, val acc: 0.4256  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 26720] train loss: 1.3781, train acc: 0.4234, val loss: 1.3588, val acc: 0.4239  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 26740] train loss: 1.3751, train acc: 0.4252, val loss: 1.3579, val acc: 0.4223  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 26760] train loss: 1.3741, train acc: 0.4234, val loss: 1.3577, val acc: 0.4226  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 26780] train loss: 1.3769, train acc: 0.4255, val loss: 1.3571, val acc: 0.4223  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 26800] train loss: 1.3732, train acc: 0.4219, val loss: 1.3575, val acc: 0.4223  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 26820] train loss: 1.3780, train acc: 0.4274, val loss: 1.3607, val acc: 0.4246  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 26840] train loss: 1.3766, train acc: 0.4304, val loss: 1.3620, val acc: 0.4287  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 26860] train loss: 1.3762, train acc: 0.4255, val loss: 1.3578, val acc: 0.4223  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 26880] train loss: 1.3761, train acc: 0.4265, val loss: 1.3576, val acc: 0.4253  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 26900] train loss: 1.3750, train acc: 0.4272, val loss: 1.3581, val acc: 0.4266  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 26920] train loss: 1.3721, train acc: 0.4274, val loss: 1.3572, val acc: 0.4196  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 26940] train loss: 1.3742, train acc: 0.4299, val loss: 1.3588, val acc: 0.4233  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 26960] train loss: 1.3719, train acc: 0.4255, val loss: 1.3579, val acc: 0.4239  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 26980] train loss: 1.3710, train acc: 0.4250, val loss: 1.3571, val acc: 0.4229  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 27000] train loss: 1.3746, train acc: 0.4273, val loss: 1.3566, val acc: 0.4229  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 27020] train loss: 1.3731, train acc: 0.4278, val loss: 1.3552, val acc: 0.4243  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 27040] train loss: 1.3734, train acc: 0.4252, val loss: 1.3539, val acc: 0.4219  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 27060] train loss: 1.3716, train acc: 0.4279, val loss: 1.3542, val acc: 0.4236  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 27080] train loss: 1.3726, train acc: 0.4234, val loss: 1.3540, val acc: 0.4250  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 27100] train loss: 1.3707, train acc: 0.4278, val loss: 1.3549, val acc: 0.4253  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 27120] train loss: 1.3728, train acc: 0.4290, val loss: 1.3568, val acc: 0.4273  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 27140] train loss: 1.3757, train acc: 0.4239, val loss: 1.3543, val acc: 0.4239  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 27160] train loss: 1.3731, train acc: 0.4249, val loss: 1.3541, val acc: 0.4209  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 27180] train loss: 1.3718, train acc: 0.4297, val loss: 1.3562, val acc: 0.4256  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 27200] train loss: 1.3720, train acc: 0.4246, val loss: 1.3537, val acc: 0.4236  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 27220] train loss: 1.3686, train acc: 0.4293, val loss: 1.3572, val acc: 0.4260  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 27240] train loss: 1.3705, train acc: 0.4216, val loss: 1.3560, val acc: 0.4236  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 27260] train loss: 1.3713, train acc: 0.4229, val loss: 1.3541, val acc: 0.4229  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 27280] train loss: 1.3689, train acc: 0.4276, val loss: 1.3536, val acc: 0.4229  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 27300] train loss: 1.3716, train acc: 0.4261, val loss: 1.3552, val acc: 0.4250  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 27320] train loss: 1.3702, train acc: 0.4281, val loss: 1.3544, val acc: 0.4229  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 27340] train loss: 1.3682, train acc: 0.4297, val loss: 1.3545, val acc: 0.4226  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 27360] train loss: 1.3725, train acc: 0.4211, val loss: 1.3537, val acc: 0.4229  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 27380] train loss: 1.3696, train acc: 0.4255, val loss: 1.3552, val acc: 0.4246  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 27400] train loss: 1.3702, train acc: 0.4230, val loss: 1.3533, val acc: 0.4202  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 27420] train loss: 1.3702, train acc: 0.4295, val loss: 1.3543, val acc: 0.4229  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 27440] train loss: 1.3670, train acc: 0.4268, val loss: 1.3535, val acc: 0.4196  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 27460] train loss: 1.3705, train acc: 0.4268, val loss: 1.3556, val acc: 0.4280  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 27480] train loss: 1.3685, train acc: 0.4289, val loss: 1.3533, val acc: 0.4233  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 27500] train loss: 1.3660, train acc: 0.4248, val loss: 1.3529, val acc: 0.4229  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 27520] train loss: 1.3706, train acc: 0.4271, val loss: 1.3529, val acc: 0.4219  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 27540] train loss: 1.3750, train acc: 0.4214, val loss: 1.3530, val acc: 0.4212  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 27560] train loss: 1.3675, train acc: 0.4275, val loss: 1.3539, val acc: 0.4233  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 27580] train loss: 1.3693, train acc: 0.4268, val loss: 1.3530, val acc: 0.4243  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 27600] train loss: 1.3706, train acc: 0.4268, val loss: 1.3523, val acc: 0.4223  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 27620] train loss: 1.3654, train acc: 0.4282, val loss: 1.3524, val acc: 0.4233  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 27640] train loss: 1.3714, train acc: 0.4294, val loss: 1.3531, val acc: 0.4239  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 27660] train loss: 1.3680, train acc: 0.4246, val loss: 1.3532, val acc: 0.4223  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 27680] train loss: 1.3665, train acc: 0.4277, val loss: 1.3525, val acc: 0.4226  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 27700] train loss: 1.3690, train acc: 0.4257, val loss: 1.3532, val acc: 0.4216  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 27720] train loss: 1.3672, train acc: 0.4263, val loss: 1.3523, val acc: 0.4223  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 27740] train loss: 1.3705, train acc: 0.4254, val loss: 1.3522, val acc: 0.4233  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 27760] train loss: 1.3693, train acc: 0.4296, val loss: 1.3530, val acc: 0.4256  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 27780] train loss: 1.3668, train acc: 0.4280, val loss: 1.3523, val acc: 0.4239  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 27800] train loss: 1.3706, train acc: 0.4265, val loss: 1.3525, val acc: 0.4233  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 27820] train loss: 1.3695, train acc: 0.4260, val loss: 1.3536, val acc: 0.4209  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 27840] train loss: 1.3697, train acc: 0.4235, val loss: 1.3532, val acc: 0.4243  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 27860] train loss: 1.3640, train acc: 0.4278, val loss: 1.3538, val acc: 0.4223  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 27880] train loss: 1.3677, train acc: 0.4260, val loss: 1.3521, val acc: 0.4229  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 27900] train loss: 1.3649, train acc: 0.4287, val loss: 1.3531, val acc: 0.4236  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 27920] train loss: 1.3721, train acc: 0.4276, val loss: 1.3519, val acc: 0.4219  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 27940] train loss: 1.3702, train acc: 0.4242, val loss: 1.3529, val acc: 0.4216  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 27960] train loss: 1.3675, train acc: 0.4264, val loss: 1.3518, val acc: 0.4216  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 27980] train loss: 1.3662, train acc: 0.4292, val loss: 1.3530, val acc: 0.4216  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 28000] train loss: 1.3653, train acc: 0.4266, val loss: 1.3533, val acc: 0.4266  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 28020] train loss: 1.3680, train acc: 0.4259, val loss: 1.3540, val acc: 0.4300  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 28040] train loss: 1.3679, train acc: 0.4260, val loss: 1.3568, val acc: 0.4310  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 28060] train loss: 1.3697, train acc: 0.4240, val loss: 1.3512, val acc: 0.4239  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 28080] train loss: 1.3680, train acc: 0.4269, val loss: 1.3513, val acc: 0.4212  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 28100] train loss: 1.3654, train acc: 0.4266, val loss: 1.3512, val acc: 0.4243  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 28120] train loss: 1.3654, train acc: 0.4260, val loss: 1.3512, val acc: 0.4219  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 28140] train loss: 1.3646, train acc: 0.4284, val loss: 1.3520, val acc: 0.4253  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 28160] train loss: 1.3706, train acc: 0.4237, val loss: 1.3520, val acc: 0.4233  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 28180] train loss: 1.3636, train acc: 0.4278, val loss: 1.3507, val acc: 0.4226  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 28200] train loss: 1.3662, train acc: 0.4241, val loss: 1.3507, val acc: 0.4239  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 28220] train loss: 1.3656, train acc: 0.4237, val loss: 1.3507, val acc: 0.4236  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 28240] train loss: 1.3611, train acc: 0.4263, val loss: 1.3508, val acc: 0.4216  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 28260] train loss: 1.3689, train acc: 0.4250, val loss: 1.3509, val acc: 0.4243  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 28280] train loss: 1.3673, train acc: 0.4313, val loss: 1.3529, val acc: 0.4256  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 28300] train loss: 1.3644, train acc: 0.4266, val loss: 1.3504, val acc: 0.4246  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 28320] train loss: 1.3679, train acc: 0.4238, val loss: 1.3507, val acc: 0.4239  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 28340] train loss: 1.3641, train acc: 0.4265, val loss: 1.3528, val acc: 0.4199  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 28360] train loss: 1.3647, train acc: 0.4263, val loss: 1.3513, val acc: 0.4243  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 28380] train loss: 1.3730, train acc: 0.4311, val loss: 1.3583, val acc: 0.4300  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 28400] train loss: 1.3625, train acc: 0.4280, val loss: 1.3514, val acc: 0.4246  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 28420] train loss: 1.3640, train acc: 0.4289, val loss: 1.3509, val acc: 0.4219  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 28440] train loss: 1.3670, train acc: 0.4258, val loss: 1.3511, val acc: 0.4263  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 28460] train loss: 1.3628, train acc: 0.4314, val loss: 1.3533, val acc: 0.4266  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 28480] train loss: 1.3651, train acc: 0.4245, val loss: 1.3515, val acc: 0.4199  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 28500] train loss: 1.3609, train acc: 0.4297, val loss: 1.3504, val acc: 0.4253  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 28520] train loss: 1.3648, train acc: 0.4258, val loss: 1.3508, val acc: 0.4196  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 28540] train loss: 1.3644, train acc: 0.4299, val loss: 1.3501, val acc: 0.4239  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 28560] train loss: 1.3685, train acc: 0.4266, val loss: 1.3504, val acc: 0.4212  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 28580] train loss: 1.3672, train acc: 0.4268, val loss: 1.3506, val acc: 0.4253  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 28600] train loss: 1.3655, train acc: 0.4250, val loss: 1.3503, val acc: 0.4246  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 28620] train loss: 1.3640, train acc: 0.4247, val loss: 1.3501, val acc: 0.4212  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 28640] train loss: 1.3640, train acc: 0.4296, val loss: 1.3505, val acc: 0.4199  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 28660] train loss: 1.3670, train acc: 0.4283, val loss: 1.3504, val acc: 0.4223  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 28680] train loss: 1.3646, train acc: 0.4281, val loss: 1.3529, val acc: 0.4297  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 28700] train loss: 1.3647, train acc: 0.4277, val loss: 1.3515, val acc: 0.4219  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 28720] train loss: 1.3629, train acc: 0.4278, val loss: 1.3507, val acc: 0.4263  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 28740] train loss: 1.3674, train acc: 0.4276, val loss: 1.3506, val acc: 0.4219  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 28760] train loss: 1.3642, train acc: 0.4295, val loss: 1.3506, val acc: 0.4206  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 28780] train loss: 1.3666, train acc: 0.4243, val loss: 1.3497, val acc: 0.4243  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 28800] train loss: 1.3652, train acc: 0.4247, val loss: 1.3514, val acc: 0.4293  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 28820] train loss: 1.3657, train acc: 0.4254, val loss: 1.3497, val acc: 0.4223  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 28840] train loss: 1.3660, train acc: 0.4260, val loss: 1.3500, val acc: 0.4226  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 28860] train loss: 1.3635, train acc: 0.4262, val loss: 1.3497, val acc: 0.4250  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 28880] train loss: 1.3659, train acc: 0.4246, val loss: 1.3496, val acc: 0.4192  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 28900] train loss: 1.3645, train acc: 0.4277, val loss: 1.3497, val acc: 0.4229  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 28920] train loss: 1.3671, train acc: 0.4242, val loss: 1.3504, val acc: 0.4212  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 28940] train loss: 1.3635, train acc: 0.4280, val loss: 1.3498, val acc: 0.4196  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 28960] train loss: 1.3675, train acc: 0.4250, val loss: 1.3515, val acc: 0.4172  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 28980] train loss: 1.3643, train acc: 0.4259, val loss: 1.3508, val acc: 0.4226  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 29000] train loss: 1.3681, train acc: 0.4274, val loss: 1.3514, val acc: 0.4212  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 29020] train loss: 1.3618, train acc: 0.4284, val loss: 1.3498, val acc: 0.4250  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 29040] train loss: 1.3653, train acc: 0.4258, val loss: 1.3485, val acc: 0.4256  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 29060] train loss: 1.3627, train acc: 0.4242, val loss: 1.3490, val acc: 0.4233  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 29080] train loss: 1.3649, train acc: 0.4252, val loss: 1.3502, val acc: 0.4206  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 29100] train loss: 1.3622, train acc: 0.4279, val loss: 1.3483, val acc: 0.4243  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 29120] train loss: 1.3656, train acc: 0.4305, val loss: 1.3494, val acc: 0.4233  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 29140] train loss: 1.3628, train acc: 0.4294, val loss: 1.3488, val acc: 0.4253  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 29160] train loss: 1.3639, train acc: 0.4242, val loss: 1.3487, val acc: 0.4273  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 29180] train loss: 1.3631, train acc: 0.4247, val loss: 1.3489, val acc: 0.4287  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 29200] train loss: 1.3607, train acc: 0.4282, val loss: 1.3485, val acc: 0.4209  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 29220] train loss: 1.3693, train acc: 0.4289, val loss: 1.3504, val acc: 0.4185  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 29240] train loss: 1.3632, train acc: 0.4278, val loss: 1.3477, val acc: 0.4250  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 29260] train loss: 1.3642, train acc: 0.4294, val loss: 1.3485, val acc: 0.4253  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 29280] train loss: 1.3633, train acc: 0.4264, val loss: 1.3489, val acc: 0.4256  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 29300] train loss: 1.3655, train acc: 0.4299, val loss: 1.3484, val acc: 0.4196  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 29320] train loss: 1.3610, train acc: 0.4280, val loss: 1.3486, val acc: 0.4250  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 29340] train loss: 1.3644, train acc: 0.4282, val loss: 1.3513, val acc: 0.4273  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 29360] train loss: 1.3639, train acc: 0.4270, val loss: 1.3480, val acc: 0.4246  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 29380] train loss: 1.3668, train acc: 0.4231, val loss: 1.3496, val acc: 0.4253  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 29400] train loss: 1.3671, train acc: 0.4257, val loss: 1.3519, val acc: 0.4253  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 29420] train loss: 1.3617, train acc: 0.4261, val loss: 1.3483, val acc: 0.4250  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 29440] train loss: 1.3636, train acc: 0.4296, val loss: 1.3496, val acc: 0.4226  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 29460] train loss: 1.3698, train acc: 0.4247, val loss: 1.3501, val acc: 0.4307  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 29480] train loss: 1.3621, train acc: 0.4286, val loss: 1.3474, val acc: 0.4250  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 29500] train loss: 1.3648, train acc: 0.4268, val loss: 1.3480, val acc: 0.4216  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 29520] train loss: 1.3616, train acc: 0.4291, val loss: 1.3479, val acc: 0.4250  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 29540] train loss: 1.3608, train acc: 0.4278, val loss: 1.3480, val acc: 0.4216  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 29560] train loss: 1.3611, train acc: 0.4272, val loss: 1.3479, val acc: 0.4290  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 29580] train loss: 1.3678, train acc: 0.4227, val loss: 1.3480, val acc: 0.4189  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 29600] train loss: 1.3642, train acc: 0.4268, val loss: 1.3476, val acc: 0.4256  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 29620] train loss: 1.3628, train acc: 0.4284, val loss: 1.3479, val acc: 0.4246  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 29640] train loss: 1.3622, train acc: 0.4266, val loss: 1.3473, val acc: 0.4250  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 29660] train loss: 1.3600, train acc: 0.4302, val loss: 1.3477, val acc: 0.4219  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 29680] train loss: 1.3609, train acc: 0.4276, val loss: 1.3473, val acc: 0.4263  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 29700] train loss: 1.3607, train acc: 0.4291, val loss: 1.3478, val acc: 0.4196  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 29720] train loss: 1.3628, train acc: 0.4248, val loss: 1.3472, val acc: 0.4263  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 29740] train loss: 1.3595, train acc: 0.4273, val loss: 1.3472, val acc: 0.4256  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 29760] train loss: 1.3612, train acc: 0.4258, val loss: 1.3482, val acc: 0.4206  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 29780] train loss: 1.3630, train acc: 0.4298, val loss: 1.3473, val acc: 0.4260  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 29800] train loss: 1.3655, train acc: 0.4267, val loss: 1.3499, val acc: 0.4209  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 29820] train loss: 1.3667, train acc: 0.4286, val loss: 1.3480, val acc: 0.4209  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 29840] train loss: 1.3621, train acc: 0.4262, val loss: 1.3474, val acc: 0.4243  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 29860] train loss: 1.3625, train acc: 0.4286, val loss: 1.3470, val acc: 0.4239  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 29880] train loss: 1.3647, train acc: 0.4283, val loss: 1.3483, val acc: 0.4189  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 29900] train loss: 1.3626, train acc: 0.4282, val loss: 1.3473, val acc: 0.4263  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 29920] train loss: 1.3617, train acc: 0.4264, val loss: 1.3474, val acc: 0.4256  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 29940] train loss: 1.3593, train acc: 0.4321, val loss: 1.3471, val acc: 0.4253  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 29960] train loss: 1.3614, train acc: 0.4292, val loss: 1.3472, val acc: 0.4260  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 29980] train loss: 1.3639, train acc: 0.4297, val loss: 1.3471, val acc: 0.4209  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 30000] train loss: 1.3619, train acc: 0.4286, val loss: 1.3478, val acc: 0.4253  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 30020] train loss: 1.3652, train acc: 0.4287, val loss: 1.3515, val acc: 0.4246  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 30040] train loss: 1.3685, train acc: 0.4299, val loss: 1.3530, val acc: 0.4307  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 30060] train loss: 1.3624, train acc: 0.4314, val loss: 1.3468, val acc: 0.4223  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 30080] train loss: 1.3624, train acc: 0.4294, val loss: 1.3540, val acc: 0.4287  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 30100] train loss: 1.3652, train acc: 0.4286, val loss: 1.3478, val acc: 0.4233  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 30120] train loss: 1.3636, train acc: 0.4291, val loss: 1.3471, val acc: 0.4212  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 30140] train loss: 1.3623, train acc: 0.4247, val loss: 1.3473, val acc: 0.4226  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 30160] train loss: 1.3609, train acc: 0.4279, val loss: 1.3477, val acc: 0.4219  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 30180] train loss: 1.3684, train acc: 0.4255, val loss: 1.3476, val acc: 0.4209  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 30200] train loss: 1.3622, train acc: 0.4263, val loss: 1.3500, val acc: 0.4293  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 30220] train loss: 1.3615, train acc: 0.4284, val loss: 1.3470, val acc: 0.4253  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 30240] train loss: 1.3594, train acc: 0.4307, val loss: 1.3495, val acc: 0.4233  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 30260] train loss: 1.3624, train acc: 0.4280, val loss: 1.3507, val acc: 0.4277  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 30280] train loss: 1.3627, train acc: 0.4297, val loss: 1.3496, val acc: 0.4196  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 30300] train loss: 1.3636, train acc: 0.4280, val loss: 1.3486, val acc: 0.4250  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 30320] train loss: 1.3624, train acc: 0.4292, val loss: 1.3510, val acc: 0.4280  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 30340] train loss: 1.3668, train acc: 0.4263, val loss: 1.3488, val acc: 0.4256  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 30360] train loss: 1.3629, train acc: 0.4289, val loss: 1.3501, val acc: 0.4233  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 30380] train loss: 1.3644, train acc: 0.4256, val loss: 1.3498, val acc: 0.4209  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 30400] train loss: 1.3634, train acc: 0.4267, val loss: 1.3509, val acc: 0.4310  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 30420] train loss: 1.3611, train acc: 0.4299, val loss: 1.3510, val acc: 0.4212  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 30440] train loss: 1.3638, train acc: 0.4291, val loss: 1.3477, val acc: 0.4263  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 30460] train loss: 1.3658, train acc: 0.4276, val loss: 1.3491, val acc: 0.4202  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 30480] train loss: 1.3587, train acc: 0.4263, val loss: 1.3485, val acc: 0.4199  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 30500] train loss: 1.3630, train acc: 0.4242, val loss: 1.3488, val acc: 0.4209  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 30520] train loss: 1.3605, train acc: 0.4270, val loss: 1.3486, val acc: 0.4243  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 30540] train loss: 1.3615, train acc: 0.4250, val loss: 1.3485, val acc: 0.4206  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 30560] train loss: 1.3607, train acc: 0.4310, val loss: 1.3492, val acc: 0.4223  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 30580] train loss: 1.3599, train acc: 0.4304, val loss: 1.3483, val acc: 0.4202  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 30600] train loss: 1.3633, train acc: 0.4266, val loss: 1.3481, val acc: 0.4223  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 30620] train loss: 1.3658, train acc: 0.4284, val loss: 1.3490, val acc: 0.4199  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 30640] train loss: 1.3646, train acc: 0.4285, val loss: 1.3477, val acc: 0.4266  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 30660] train loss: 1.3613, train acc: 0.4292, val loss: 1.3482, val acc: 0.4273  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 30680] train loss: 1.3633, train acc: 0.4271, val loss: 1.3494, val acc: 0.4223  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 30700] train loss: 1.3626, train acc: 0.4268, val loss: 1.3495, val acc: 0.4192  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 30720] train loss: 1.3642, train acc: 0.4243, val loss: 1.3480, val acc: 0.4243  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 30740] train loss: 1.3660, train acc: 0.4280, val loss: 1.3525, val acc: 0.4260  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 30760] train loss: 1.3615, train acc: 0.4268, val loss: 1.3493, val acc: 0.4216  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 30780] train loss: 1.3595, train acc: 0.4286, val loss: 1.3488, val acc: 0.4246  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 30800] train loss: 1.3591, train acc: 0.4297, val loss: 1.3493, val acc: 0.4256  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 30820] train loss: 1.3647, train acc: 0.4285, val loss: 1.3523, val acc: 0.4277  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 30840] train loss: 1.3643, train acc: 0.4271, val loss: 1.3495, val acc: 0.4236  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 30860] train loss: 1.3693, train acc: 0.4202, val loss: 1.3515, val acc: 0.4320  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 30880] train loss: 1.3657, train acc: 0.4286, val loss: 1.3479, val acc: 0.4226  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 30900] train loss: 1.3617, train acc: 0.4255, val loss: 1.3486, val acc: 0.4212  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 30920] train loss: 1.3625, train acc: 0.4302, val loss: 1.3475, val acc: 0.4233  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 30940] train loss: 1.3638, train acc: 0.4266, val loss: 1.3501, val acc: 0.4243  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 30960] train loss: 1.3661, train acc: 0.4281, val loss: 1.3500, val acc: 0.4206  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 30980] train loss: 1.3599, train acc: 0.4289, val loss: 1.3478, val acc: 0.4253  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 31000] train loss: 1.3633, train acc: 0.4273, val loss: 1.3491, val acc: 0.4206  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 31020] train loss: 1.3604, train acc: 0.4296, val loss: 1.3479, val acc: 0.4202  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 31040] train loss: 1.3627, train acc: 0.4282, val loss: 1.3476, val acc: 0.4270  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 31060] train loss: 1.3587, train acc: 0.4262, val loss: 1.3470, val acc: 0.4253  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 31080] train loss: 1.3586, train acc: 0.4289, val loss: 1.3481, val acc: 0.4233  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 31100] train loss: 1.3610, train acc: 0.4242, val loss: 1.3471, val acc: 0.4253  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 31120] train loss: 1.3624, train acc: 0.4265, val loss: 1.3484, val acc: 0.4256  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 31140] train loss: 1.3617, train acc: 0.4261, val loss: 1.3473, val acc: 0.4260  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 31160] train loss: 1.3603, train acc: 0.4274, val loss: 1.3472, val acc: 0.4253  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 31180] train loss: 1.3643, train acc: 0.4263, val loss: 1.3479, val acc: 0.4219  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 31200] train loss: 1.3615, train acc: 0.4299, val loss: 1.3472, val acc: 0.4233  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 31220] train loss: 1.3654, train acc: 0.4281, val loss: 1.3489, val acc: 0.4216  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 31240] train loss: 1.3673, train acc: 0.4236, val loss: 1.3531, val acc: 0.4202  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 31260] train loss: 1.3880, train acc: 0.4177, val loss: 1.3676, val acc: 0.4192  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 31280] train loss: 1.3733, train acc: 0.4227, val loss: 1.3609, val acc: 0.4277  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 31300] train loss: 1.3649, train acc: 0.4248, val loss: 1.3561, val acc: 0.4243  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 31320] train loss: 1.3684, train acc: 0.4260, val loss: 1.3547, val acc: 0.4246  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 31340] train loss: 1.3659, train acc: 0.4232, val loss: 1.3553, val acc: 0.4260  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 31360] train loss: 1.3655, train acc: 0.4228, val loss: 1.3567, val acc: 0.4266  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 31380] train loss: 1.3681, train acc: 0.4278, val loss: 1.3552, val acc: 0.4233  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 31400] train loss: 1.3676, train acc: 0.4272, val loss: 1.3539, val acc: 0.4256  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 31420] train loss: 1.3661, train acc: 0.4290, val loss: 1.3530, val acc: 0.4226  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 31440] train loss: 1.3690, train acc: 0.4258, val loss: 1.3529, val acc: 0.4239  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 31460] train loss: 1.3639, train acc: 0.4299, val loss: 1.3533, val acc: 0.4223  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 31480] train loss: 1.3647, train acc: 0.4282, val loss: 1.3525, val acc: 0.4206  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 31500] train loss: 1.3679, train acc: 0.4224, val loss: 1.3518, val acc: 0.4236  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 31520] train loss: 1.3650, train acc: 0.4246, val loss: 1.3520, val acc: 0.4246  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 31540] train loss: 1.3637, train acc: 0.4268, val loss: 1.3515, val acc: 0.4266  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 31560] train loss: 1.3611, train acc: 0.4265, val loss: 1.3516, val acc: 0.4216  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 31580] train loss: 1.3605, train acc: 0.4243, val loss: 1.3507, val acc: 0.4226  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 31600] train loss: 1.3648, train acc: 0.4284, val loss: 1.3538, val acc: 0.4317  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 31620] train loss: 1.3623, train acc: 0.4257, val loss: 1.3507, val acc: 0.4223  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 31640] train loss: 1.3655, train acc: 0.4271, val loss: 1.3497, val acc: 0.4260  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 31660] train loss: 1.3617, train acc: 0.4278, val loss: 1.3501, val acc: 0.4280  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 31680] train loss: 1.3633, train acc: 0.4312, val loss: 1.3495, val acc: 0.4243  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 31700] train loss: 1.3620, train acc: 0.4249, val loss: 1.3500, val acc: 0.4212  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 31720] train loss: 1.3652, train acc: 0.4295, val loss: 1.3496, val acc: 0.4229  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 31740] train loss: 1.3619, train acc: 0.4258, val loss: 1.3490, val acc: 0.4206  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 31760] train loss: 1.3633, train acc: 0.4223, val loss: 1.3497, val acc: 0.4266  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 31780] train loss: 1.3609, train acc: 0.4250, val loss: 1.3495, val acc: 0.4196  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 31800] train loss: 1.3636, train acc: 0.4276, val loss: 1.3503, val acc: 0.4270  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 31820] train loss: 1.3618, train acc: 0.4275, val loss: 1.3490, val acc: 0.4202  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 31840] train loss: 1.3590, train acc: 0.4287, val loss: 1.3490, val acc: 0.4209  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 31860] train loss: 1.3651, train acc: 0.4271, val loss: 1.3494, val acc: 0.4192  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 31880] train loss: 1.3649, train acc: 0.4271, val loss: 1.3494, val acc: 0.4239  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 31900] train loss: 1.3649, train acc: 0.4299, val loss: 1.3498, val acc: 0.4185  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 31920] train loss: 1.3625, train acc: 0.4264, val loss: 1.3510, val acc: 0.4243  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 31940] train loss: 1.3630, train acc: 0.4245, val loss: 1.3484, val acc: 0.4253  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 31960] train loss: 1.3618, train acc: 0.4258, val loss: 1.3500, val acc: 0.4216  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 31980] train loss: 1.3578, train acc: 0.4289, val loss: 1.3487, val acc: 0.4229  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 32000] train loss: 1.3611, train acc: 0.4254, val loss: 1.3480, val acc: 0.4212  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 32020] train loss: 1.3686, train acc: 0.4224, val loss: 1.3592, val acc: 0.4307  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 32040] train loss: 1.3614, train acc: 0.4302, val loss: 1.3506, val acc: 0.4263  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 32060] train loss: 1.3615, train acc: 0.4290, val loss: 1.3478, val acc: 0.4202  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 32080] train loss: 1.3633, train acc: 0.4279, val loss: 1.3484, val acc: 0.4246  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 32100] train loss: 1.3628, train acc: 0.4290, val loss: 1.3477, val acc: 0.4202  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 32120] train loss: 1.3603, train acc: 0.4263, val loss: 1.3508, val acc: 0.4246  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 32140] train loss: 1.3690, train acc: 0.4308, val loss: 1.3483, val acc: 0.4199  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 32160] train loss: 1.3612, train acc: 0.4293, val loss: 1.3487, val acc: 0.4212  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 32180] train loss: 1.3624, train acc: 0.4293, val loss: 1.3497, val acc: 0.4236  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 32200] train loss: 1.3602, train acc: 0.4258, val loss: 1.3495, val acc: 0.4239  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 32220] train loss: 1.3618, train acc: 0.4244, val loss: 1.3532, val acc: 0.4236  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 32240] train loss: 1.3615, train acc: 0.4271, val loss: 1.3480, val acc: 0.4196  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 32260] train loss: 1.3601, train acc: 0.4271, val loss: 1.3475, val acc: 0.4233  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 32280] train loss: 1.3615, train acc: 0.4274, val loss: 1.3475, val acc: 0.4260  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 32300] train loss: 1.3636, train acc: 0.4291, val loss: 1.3485, val acc: 0.4189  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 32320] train loss: 1.3627, train acc: 0.4278, val loss: 1.3491, val acc: 0.4246  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 32340] train loss: 1.3621, train acc: 0.4280, val loss: 1.3499, val acc: 0.4256  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 32360] train loss: 1.3623, train acc: 0.4269, val loss: 1.3496, val acc: 0.4250  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 32380] train loss: 1.3626, train acc: 0.4293, val loss: 1.3473, val acc: 0.4239  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 32400] train loss: 1.3599, train acc: 0.4297, val loss: 1.3468, val acc: 0.4256  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 32420] train loss: 1.3600, train acc: 0.4303, val loss: 1.3479, val acc: 0.4246  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 32440] train loss: 1.3582, train acc: 0.4279, val loss: 1.3470, val acc: 0.4223  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 32460] train loss: 1.3621, train acc: 0.4298, val loss: 1.3487, val acc: 0.4185  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 32480] train loss: 1.3590, train acc: 0.4283, val loss: 1.3468, val acc: 0.4246  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 32500] train loss: 1.3615, train acc: 0.4280, val loss: 1.3477, val acc: 0.4233  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 32520] train loss: 1.3613, train acc: 0.4289, val loss: 1.3466, val acc: 0.4192  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 32540] train loss: 1.3599, train acc: 0.4303, val loss: 1.3463, val acc: 0.4253  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 32560] train loss: 1.3583, train acc: 0.4278, val loss: 1.3463, val acc: 0.4196  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 32580] train loss: 1.3583, train acc: 0.4302, val loss: 1.3461, val acc: 0.4226  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 32600] train loss: 1.3639, train acc: 0.4260, val loss: 1.3463, val acc: 0.4243  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 32620] train loss: 1.3619, train acc: 0.4290, val loss: 1.3474, val acc: 0.4189  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 32640] train loss: 1.3628, train acc: 0.4244, val loss: 1.3471, val acc: 0.4212  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 32660] train loss: 1.3601, train acc: 0.4295, val loss: 1.3537, val acc: 0.4320  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 32680] train loss: 1.3603, train acc: 0.4263, val loss: 1.3464, val acc: 0.4209  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 32700] train loss: 1.3605, train acc: 0.4242, val loss: 1.3468, val acc: 0.4243  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 32720] train loss: 1.3624, train acc: 0.4286, val loss: 1.3463, val acc: 0.4206  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 32740] train loss: 1.3609, train acc: 0.4289, val loss: 1.3460, val acc: 0.4199  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 32760] train loss: 1.3557, train acc: 0.4293, val loss: 1.3456, val acc: 0.4202  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 32780] train loss: 1.3584, train acc: 0.4294, val loss: 1.3455, val acc: 0.4209  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 32800] train loss: 1.3619, train acc: 0.4255, val loss: 1.3459, val acc: 0.4260  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 32820] train loss: 1.3583, train acc: 0.4268, val loss: 1.3460, val acc: 0.4175  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 32840] train loss: 1.3597, train acc: 0.4305, val loss: 1.3455, val acc: 0.4179  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 32860] train loss: 1.3585, train acc: 0.4265, val loss: 1.3452, val acc: 0.4192  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 32880] train loss: 1.3626, train acc: 0.4279, val loss: 1.3470, val acc: 0.4246  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 32900] train loss: 1.3636, train acc: 0.4278, val loss: 1.3457, val acc: 0.4236  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 32920] train loss: 1.3581, train acc: 0.4283, val loss: 1.3454, val acc: 0.4196  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 32940] train loss: 1.3640, train acc: 0.4266, val loss: 1.3458, val acc: 0.4216  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 32960] train loss: 1.3749, train acc: 0.4196, val loss: 1.3566, val acc: 0.4314  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 32980] train loss: 1.3631, train acc: 0.4272, val loss: 1.3493, val acc: 0.4132  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 33000] train loss: 1.3696, train acc: 0.4206, val loss: 1.3547, val acc: 0.4273  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 33020] train loss: 1.3674, train acc: 0.4284, val loss: 1.3521, val acc: 0.4250  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 33040] train loss: 1.3670, train acc: 0.4251, val loss: 1.3527, val acc: 0.4270  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 33060] train loss: 1.3654, train acc: 0.4295, val loss: 1.3514, val acc: 0.4297  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 33080] train loss: 1.3688, train acc: 0.4234, val loss: 1.3519, val acc: 0.4239  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 33100] train loss: 1.3646, train acc: 0.4249, val loss: 1.3518, val acc: 0.4290  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 33120] train loss: 1.3646, train acc: 0.4271, val loss: 1.3515, val acc: 0.4277  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 33140] train loss: 1.3648, train acc: 0.4265, val loss: 1.3513, val acc: 0.4229  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 33160] train loss: 1.3624, train acc: 0.4278, val loss: 1.3508, val acc: 0.4273  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 33180] train loss: 1.3625, train acc: 0.4265, val loss: 1.3509, val acc: 0.4277  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 33200] train loss: 1.3654, train acc: 0.4245, val loss: 1.3523, val acc: 0.4287  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 33220] train loss: 1.3652, train acc: 0.4296, val loss: 1.3510, val acc: 0.4277  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 33240] train loss: 1.3671, train acc: 0.4270, val loss: 1.3516, val acc: 0.4236  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 33260] train loss: 1.3685, train acc: 0.4211, val loss: 1.3508, val acc: 0.4290  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 33280] train loss: 1.3639, train acc: 0.4289, val loss: 1.3506, val acc: 0.4270  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 33300] train loss: 1.3633, train acc: 0.4277, val loss: 1.3521, val acc: 0.4266  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 33320] train loss: 1.3658, train acc: 0.4261, val loss: 1.3508, val acc: 0.4256  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 33340] train loss: 1.3648, train acc: 0.4226, val loss: 1.3503, val acc: 0.4277  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 33360] train loss: 1.3611, train acc: 0.4276, val loss: 1.3508, val acc: 0.4293  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 33380] train loss: 1.3641, train acc: 0.4257, val loss: 1.3507, val acc: 0.4250  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 33400] train loss: 1.3660, train acc: 0.4252, val loss: 1.3511, val acc: 0.4239  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 33420] train loss: 1.3643, train acc: 0.4271, val loss: 1.3508, val acc: 0.4229  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 33440] train loss: 1.3619, train acc: 0.4286, val loss: 1.3505, val acc: 0.4236  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 33460] train loss: 1.3638, train acc: 0.4265, val loss: 1.3504, val acc: 0.4266  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 33480] train loss: 1.3644, train acc: 0.4245, val loss: 1.3540, val acc: 0.4246  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 33500] train loss: 1.3630, train acc: 0.4283, val loss: 1.3499, val acc: 0.4256  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 33520] train loss: 1.3622, train acc: 0.4263, val loss: 1.3509, val acc: 0.4246  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 33540] train loss: 1.3634, train acc: 0.4254, val loss: 1.3509, val acc: 0.4266  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 33560] train loss: 1.3613, train acc: 0.4278, val loss: 1.3501, val acc: 0.4290  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 33580] train loss: 1.3623, train acc: 0.4279, val loss: 1.3510, val acc: 0.4270  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 33600] train loss: 1.3674, train acc: 0.4236, val loss: 1.3501, val acc: 0.4229  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 33620] train loss: 1.3604, train acc: 0.4270, val loss: 1.3498, val acc: 0.4233  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 33640] train loss: 1.3565, train acc: 0.4312, val loss: 1.3430, val acc: 0.4358  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 33660] train loss: 1.3643, train acc: 0.4223, val loss: 1.3392, val acc: 0.4358  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 33680] train loss: 1.3569, train acc: 0.4369, val loss: 1.3365, val acc: 0.4331  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 33700] train loss: 1.3486, train acc: 0.4357, val loss: 1.3371, val acc: 0.4273  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 33720] train loss: 1.3511, train acc: 0.4386, val loss: 1.3347, val acc: 0.4293  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 33740] train loss: 1.3518, train acc: 0.4404, val loss: 1.3399, val acc: 0.4202  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 33760] train loss: 1.3516, train acc: 0.4431, val loss: 1.3347, val acc: 0.4368  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 33780] train loss: 1.3506, train acc: 0.4338, val loss: 1.3340, val acc: 0.4347  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 33800] train loss: 1.3591, train acc: 0.4234, val loss: 1.3406, val acc: 0.4307  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 33820] train loss: 1.3504, train acc: 0.4385, val loss: 1.3342, val acc: 0.4398  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 33840] train loss: 1.3428, train acc: 0.4406, val loss: 1.3302, val acc: 0.4432  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 33860] train loss: 1.3469, train acc: 0.4409, val loss: 1.3301, val acc: 0.4435  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 33880] train loss: 1.3512, train acc: 0.4312, val loss: 1.3336, val acc: 0.4341  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 33900] train loss: 1.3463, train acc: 0.4396, val loss: 1.3296, val acc: 0.4465  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 33920] train loss: 1.3460, train acc: 0.4372, val loss: 1.3289, val acc: 0.4465  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 33940] train loss: 1.3458, train acc: 0.4345, val loss: 1.3291, val acc: 0.4418  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 33960] train loss: 1.3449, train acc: 0.4371, val loss: 1.3292, val acc: 0.4408  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 33980] train loss: 1.3453, train acc: 0.4353, val loss: 1.3296, val acc: 0.4435  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 34000] train loss: 1.3505, train acc: 0.4409, val loss: 1.3293, val acc: 0.4435  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 34020] train loss: 1.3465, train acc: 0.4414, val loss: 1.3279, val acc: 0.4492  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 34040] train loss: 1.3432, train acc: 0.4363, val loss: 1.3303, val acc: 0.4499  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 34060] train loss: 1.3501, train acc: 0.4410, val loss: 1.3366, val acc: 0.4415  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 34080] train loss: 1.3488, train acc: 0.4385, val loss: 1.3290, val acc: 0.4422  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 34100] train loss: 1.3463, train acc: 0.4359, val loss: 1.3271, val acc: 0.4445  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 34120] train loss: 1.3434, train acc: 0.4419, val loss: 1.3272, val acc: 0.4432  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 34140] train loss: 1.3438, train acc: 0.4398, val loss: 1.3292, val acc: 0.4405  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 34160] train loss: 1.3440, train acc: 0.4398, val loss: 1.3342, val acc: 0.4320  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 34180] train loss: 1.3425, train acc: 0.4396, val loss: 1.3279, val acc: 0.4465  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 34200] train loss: 1.3457, train acc: 0.4380, val loss: 1.3273, val acc: 0.4482  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 34220] train loss: 1.3441, train acc: 0.4436, val loss: 1.3264, val acc: 0.4449  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 34240] train loss: 1.3472, train acc: 0.4419, val loss: 1.3284, val acc: 0.4378  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 34260] train loss: 1.3475, train acc: 0.4346, val loss: 1.3281, val acc: 0.4438  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 34280] train loss: 1.3461, train acc: 0.4426, val loss: 1.3266, val acc: 0.4432  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 34300] train loss: 1.3425, train acc: 0.4398, val loss: 1.3296, val acc: 0.4496  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 34320] train loss: 1.3499, train acc: 0.4336, val loss: 1.3381, val acc: 0.4415  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 34340] train loss: 1.3526, train acc: 0.4392, val loss: 1.3385, val acc: 0.4395  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 34360] train loss: 1.6056, train acc: 0.2343, val loss: 1.5563, val acc: 0.2563  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 34380] train loss: 1.4759, train acc: 0.3302, val loss: 1.4656, val acc: 0.3575  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 34400] train loss: 1.4498, train acc: 0.3793, val loss: 1.4480, val acc: 0.3862  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 34420] train loss: 1.4497, train acc: 0.3659, val loss: 1.4437, val acc: 0.3757  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 34440] train loss: 1.4473, train acc: 0.3573, val loss: 1.4418, val acc: 0.3599  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 34460] train loss: 1.4438, train acc: 0.3702, val loss: 1.4413, val acc: 0.3653  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 34480] train loss: 1.4413, train acc: 0.3725, val loss: 1.4397, val acc: 0.3666  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 34500] train loss: 1.4418, train acc: 0.3610, val loss: 1.4400, val acc: 0.3568  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 34520] train loss: 1.4412, train acc: 0.3540, val loss: 1.4380, val acc: 0.3568  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 34540] train loss: 1.4388, train acc: 0.3490, val loss: 1.4373, val acc: 0.3548  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 34560] train loss: 1.4408, train acc: 0.3464, val loss: 1.4367, val acc: 0.3848  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 34580] train loss: 1.4374, train acc: 0.3452, val loss: 1.4363, val acc: 0.3646  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 34600] train loss: 1.4360, train acc: 0.3563, val loss: 1.4360, val acc: 0.3565  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 34620] train loss: 1.4368, train acc: 0.3399, val loss: 1.4347, val acc: 0.3696  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 34640] train loss: 1.4028, train acc: 0.3931, val loss: 1.5988, val acc: 0.2411  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 34660] train loss: 1.5462, train acc: 0.3024, val loss: 1.4722, val acc: 0.2921  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 34680] train loss: 1.4513, train acc: 0.3686, val loss: 1.4145, val acc: 0.4098  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 34700] train loss: 1.3773, train acc: 0.4146, val loss: 1.3626, val acc: 0.4229  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 34720] train loss: 1.3611, train acc: 0.4383, val loss: 1.3463, val acc: 0.4293  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 34740] train loss: 1.3558, train acc: 0.4401, val loss: 1.3395, val acc: 0.4428  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 34760] train loss: 1.3532, train acc: 0.4393, val loss: 1.3431, val acc: 0.4398  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 34780] train loss: 1.3558, train acc: 0.4343, val loss: 1.3392, val acc: 0.4388  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 34800] train loss: 1.3527, train acc: 0.4388, val loss: 1.3358, val acc: 0.4425  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 34820] train loss: 1.3493, train acc: 0.4420, val loss: 1.3368, val acc: 0.4465  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 34840] train loss: 1.3458, train acc: 0.4409, val loss: 1.3341, val acc: 0.4476  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 34860] train loss: 1.3439, train acc: 0.4417, val loss: 1.3315, val acc: 0.4449  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 34880] train loss: 1.3436, train acc: 0.4448, val loss: 1.3302, val acc: 0.4509  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 34900] train loss: 1.3492, train acc: 0.4409, val loss: 1.3310, val acc: 0.4540  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 34920] train loss: 1.3465, train acc: 0.4426, val loss: 1.3325, val acc: 0.4509  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 34940] train loss: 1.3412, train acc: 0.4429, val loss: 1.3335, val acc: 0.4492  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 34960] train loss: 1.3454, train acc: 0.4411, val loss: 1.3308, val acc: 0.4489  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 34980] train loss: 1.3457, train acc: 0.4477, val loss: 1.3278, val acc: 0.4506  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 35000] train loss: 1.3408, train acc: 0.4489, val loss: 1.3244, val acc: 0.4519  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 35020] train loss: 1.3411, train acc: 0.4448, val loss: 1.3218, val acc: 0.4506  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 35040] train loss: 1.3405, train acc: 0.4452, val loss: 1.3221, val acc: 0.4499  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 35060] train loss: 1.3368, train acc: 0.4487, val loss: 1.3222, val acc: 0.4540  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 35080] train loss: 1.3349, train acc: 0.4454, val loss: 1.3218, val acc: 0.4533  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 35100] train loss: 1.3329, train acc: 0.4469, val loss: 1.3200, val acc: 0.4560  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 35120] train loss: 1.3363, train acc: 0.4437, val loss: 1.3190, val acc: 0.4533  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 35140] train loss: 1.3329, train acc: 0.4498, val loss: 1.3195, val acc: 0.4560  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 35160] train loss: 1.3368, train acc: 0.4448, val loss: 1.3191, val acc: 0.4543  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 35180] train loss: 1.3345, train acc: 0.4495, val loss: 1.3211, val acc: 0.4590  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 35200] train loss: 1.3357, train acc: 0.4465, val loss: 1.3179, val acc: 0.4489  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 35220] train loss: 1.3331, train acc: 0.4458, val loss: 1.3179, val acc: 0.4486  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 35240] train loss: 1.3348, train acc: 0.4460, val loss: 1.3225, val acc: 0.4543  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 35260] train loss: 1.3355, train acc: 0.4482, val loss: 1.3162, val acc: 0.4513  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 35280] train loss: 1.3345, train acc: 0.4458, val loss: 1.3207, val acc: 0.4526  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 35300] train loss: 1.3335, train acc: 0.4466, val loss: 1.3182, val acc: 0.4503  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 35320] train loss: 1.3344, train acc: 0.4470, val loss: 1.3178, val acc: 0.4509  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 35340] train loss: 1.3380, train acc: 0.4495, val loss: 1.3191, val acc: 0.4553  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 35360] train loss: 1.3285, train acc: 0.4463, val loss: 1.3186, val acc: 0.4418  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 35380] train loss: 1.3310, train acc: 0.4486, val loss: 1.3263, val acc: 0.4526  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 35400] train loss: 1.3291, train acc: 0.4476, val loss: 1.3211, val acc: 0.4556  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 35420] train loss: 1.3342, train acc: 0.4453, val loss: 1.3177, val acc: 0.4503  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 35440] train loss: 1.3298, train acc: 0.4466, val loss: 1.3164, val acc: 0.4449  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 35460] train loss: 1.3332, train acc: 0.4459, val loss: 1.3151, val acc: 0.4563  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 35480] train loss: 1.3263, train acc: 0.4500, val loss: 1.3175, val acc: 0.4614  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 35500] train loss: 1.3320, train acc: 0.4499, val loss: 1.3167, val acc: 0.4482  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 35520] train loss: 1.3277, train acc: 0.4477, val loss: 1.3138, val acc: 0.4550  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 35540] train loss: 1.3276, train acc: 0.4452, val loss: 1.3132, val acc: 0.4563  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 35560] train loss: 1.3304, train acc: 0.4479, val loss: 1.3150, val acc: 0.4415  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 35580] train loss: 1.3451, train acc: 0.4392, val loss: 1.3225, val acc: 0.4374  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 35600] train loss: 1.3348, train acc: 0.4438, val loss: 1.3139, val acc: 0.4503  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 35620] train loss: 1.3456, train acc: 0.4258, val loss: 1.3253, val acc: 0.4476  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 35640] train loss: 1.5076, train acc: 0.2984, val loss: 1.4601, val acc: 0.3788  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 35660] train loss: 1.3987, train acc: 0.4078, val loss: 1.3691, val acc: 0.4317  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 35680] train loss: 1.3600, train acc: 0.4331, val loss: 1.3391, val acc: 0.4344  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 35700] train loss: 1.3496, train acc: 0.4349, val loss: 1.3310, val acc: 0.4523  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 35720] train loss: 1.3345, train acc: 0.4448, val loss: 1.3113, val acc: 0.4644  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 35740] train loss: 1.3317, train acc: 0.4460, val loss: 1.3100, val acc: 0.4610  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 35760] train loss: 1.3283, train acc: 0.4475, val loss: 1.3097, val acc: 0.4624  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 35780] train loss: 1.3262, train acc: 0.4495, val loss: 1.3101, val acc: 0.4631  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 35800] train loss: 1.3260, train acc: 0.4511, val loss: 1.3081, val acc: 0.4668  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 35820] train loss: 1.3240, train acc: 0.4504, val loss: 1.3082, val acc: 0.4648  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 35840] train loss: 1.3262, train acc: 0.4486, val loss: 1.3078, val acc: 0.4654  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 35860] train loss: 1.3279, train acc: 0.4506, val loss: 1.3075, val acc: 0.4641  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 35880] train loss: 1.3273, train acc: 0.4464, val loss: 1.3072, val acc: 0.4614  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 35900] train loss: 1.3247, train acc: 0.4479, val loss: 1.3102, val acc: 0.4617  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 35920] train loss: 1.3254, train acc: 0.4500, val loss: 1.3086, val acc: 0.4583  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 35940] train loss: 1.3290, train acc: 0.4489, val loss: 1.3067, val acc: 0.4634  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 35960] train loss: 1.3255, train acc: 0.4488, val loss: 1.3072, val acc: 0.4590  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 35980] train loss: 1.3257, train acc: 0.4467, val loss: 1.3083, val acc: 0.4617  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 36000] train loss: 1.3224, train acc: 0.4473, val loss: 1.3090, val acc: 0.4590  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 36020] train loss: 1.5261, train acc: 0.2967, val loss: 1.5378, val acc: 0.2556  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 36040] train loss: 1.3813, train acc: 0.3757, val loss: 1.3436, val acc: 0.4388  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 36060] train loss: 1.3590, train acc: 0.4050, val loss: 1.3378, val acc: 0.4243  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 36080] train loss: 1.3564, train acc: 0.4106, val loss: 1.3360, val acc: 0.4277  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 36100] train loss: 1.3650, train acc: 0.3947, val loss: 1.3576, val acc: 0.4084  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 36120] train loss: 1.3555, train acc: 0.4272, val loss: 1.3400, val acc: 0.4293  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 36140] train loss: 1.3808, train acc: 0.3860, val loss: 1.3652, val acc: 0.4226  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 36160] train loss: 1.3594, train acc: 0.3992, val loss: 1.3395, val acc: 0.4094  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 36180] train loss: 1.3440, train acc: 0.4091, val loss: 1.3324, val acc: 0.4152  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 36200] train loss: 1.3509, train acc: 0.4227, val loss: 1.3343, val acc: 0.4283  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 36220] train loss: 1.4536, train acc: 0.3368, val loss: 1.4426, val acc: 0.3204  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 36240] train loss: 1.4288, train acc: 0.3761, val loss: 1.3978, val acc: 0.3781  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 36260] train loss: 1.3853, train acc: 0.4104, val loss: 1.3627, val acc: 0.4280  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 36280] train loss: 1.3767, train acc: 0.4231, val loss: 1.3522, val acc: 0.4290  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 36300] train loss: 1.3746, train acc: 0.4250, val loss: 1.3472, val acc: 0.4287  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 36320] train loss: 1.3682, train acc: 0.4277, val loss: 1.3451, val acc: 0.4320  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 36340] train loss: 1.3658, train acc: 0.4267, val loss: 1.3450, val acc: 0.4344  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 36360] train loss: 1.3629, train acc: 0.4289, val loss: 1.3453, val acc: 0.4341  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 36380] train loss: 1.3669, train acc: 0.4273, val loss: 1.3445, val acc: 0.4314  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 36400] train loss: 1.3643, train acc: 0.4318, val loss: 1.3440, val acc: 0.4314  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 36420] train loss: 1.3624, train acc: 0.4273, val loss: 1.3446, val acc: 0.4320  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 36440] train loss: 1.3636, train acc: 0.4310, val loss: 1.3432, val acc: 0.4334  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 36460] train loss: 1.3640, train acc: 0.4278, val loss: 1.3440, val acc: 0.4337  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 36480] train loss: 1.3669, train acc: 0.4268, val loss: 1.3434, val acc: 0.4331  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 36500] train loss: 1.3609, train acc: 0.4286, val loss: 1.3439, val acc: 0.4331  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 36520] train loss: 1.3623, train acc: 0.4282, val loss: 1.3431, val acc: 0.4337  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 36540] train loss: 1.3589, train acc: 0.4286, val loss: 1.3434, val acc: 0.4337  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 36560] train loss: 1.3608, train acc: 0.4286, val loss: 1.3430, val acc: 0.4317  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 36580] train loss: 1.3592, train acc: 0.4319, val loss: 1.3435, val acc: 0.4347  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 36600] train loss: 1.3576, train acc: 0.4305, val loss: 1.3432, val acc: 0.4324  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 36620] train loss: 1.3633, train acc: 0.4267, val loss: 1.3430, val acc: 0.4327  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 36640] train loss: 1.3622, train acc: 0.4315, val loss: 1.3427, val acc: 0.4317  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 36660] train loss: 1.3596, train acc: 0.4309, val loss: 1.3430, val acc: 0.4361  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 36680] train loss: 1.3588, train acc: 0.4329, val loss: 1.3419, val acc: 0.4317  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 36700] train loss: 1.3612, train acc: 0.4322, val loss: 1.3426, val acc: 0.4344  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 36720] train loss: 1.3630, train acc: 0.4291, val loss: 1.3419, val acc: 0.4300  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 36740] train loss: 1.3623, train acc: 0.4301, val loss: 1.3424, val acc: 0.4341  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 36760] train loss: 1.3588, train acc: 0.4295, val loss: 1.3425, val acc: 0.4354  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 36780] train loss: 1.3588, train acc: 0.4323, val loss: 1.3422, val acc: 0.4320  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 36800] train loss: 1.3591, train acc: 0.4328, val loss: 1.3419, val acc: 0.4307  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 36820] train loss: 1.3625, train acc: 0.4298, val loss: 1.3426, val acc: 0.4341  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 36840] train loss: 1.3573, train acc: 0.4325, val loss: 1.3433, val acc: 0.4341  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 36860] train loss: 1.3613, train acc: 0.4330, val loss: 1.3416, val acc: 0.4304  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 36880] train loss: 1.3614, train acc: 0.4274, val loss: 1.3419, val acc: 0.4324  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 36900] train loss: 1.3568, train acc: 0.4311, val loss: 1.3416, val acc: 0.4300  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 36920] train loss: 1.3608, train acc: 0.4287, val loss: 1.3423, val acc: 0.4331  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 36940] train loss: 1.3606, train acc: 0.4312, val loss: 1.3427, val acc: 0.4334  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 36960] train loss: 1.3606, train acc: 0.4278, val loss: 1.3425, val acc: 0.4351  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 36980] train loss: 1.3600, train acc: 0.4321, val loss: 1.3410, val acc: 0.4331  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 37000] train loss: 1.3623, train acc: 0.4271, val loss: 1.3419, val acc: 0.4334  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 37020] train loss: 1.3567, train acc: 0.4291, val loss: 1.3416, val acc: 0.4331  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 37040] train loss: 1.3571, train acc: 0.4313, val loss: 1.3436, val acc: 0.4341  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 37060] train loss: 1.3596, train acc: 0.4328, val loss: 1.3415, val acc: 0.4347  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 37080] train loss: 1.3601, train acc: 0.4328, val loss: 1.3413, val acc: 0.4344  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 37100] train loss: 1.3594, train acc: 0.4321, val loss: 1.3414, val acc: 0.4361  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 37120] train loss: 1.3582, train acc: 0.4309, val loss: 1.3427, val acc: 0.4307  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 37140] train loss: 1.3586, train acc: 0.4308, val loss: 1.3423, val acc: 0.4337  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 37160] train loss: 1.3582, train acc: 0.4320, val loss: 1.3415, val acc: 0.4317  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 37180] train loss: 1.3647, train acc: 0.4315, val loss: 1.3421, val acc: 0.4347  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 37200] train loss: 1.3569, train acc: 0.4334, val loss: 1.3407, val acc: 0.4280  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 37220] train loss: 1.3605, train acc: 0.4307, val loss: 1.3408, val acc: 0.4277  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 37240] train loss: 1.3558, train acc: 0.4312, val loss: 1.3415, val acc: 0.4327  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 37260] train loss: 1.3579, train acc: 0.4311, val loss: 1.3425, val acc: 0.4307  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 37280] train loss: 1.3610, train acc: 0.4343, val loss: 1.3412, val acc: 0.4277  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 37300] train loss: 1.3617, train acc: 0.4333, val loss: 1.3414, val acc: 0.4283  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 37320] train loss: 1.3562, train acc: 0.4324, val loss: 1.3414, val acc: 0.4337  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 37340] train loss: 1.3584, train acc: 0.4312, val loss: 1.3435, val acc: 0.4304  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 37360] train loss: 1.3581, train acc: 0.4320, val loss: 1.3408, val acc: 0.4304  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 37380] train loss: 1.3577, train acc: 0.4358, val loss: 1.3408, val acc: 0.4320  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 37400] train loss: 1.3575, train acc: 0.4315, val loss: 1.3410, val acc: 0.4324  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 37420] train loss: 1.3558, train acc: 0.4341, val loss: 1.3424, val acc: 0.4358  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 37440] train loss: 1.3548, train acc: 0.4323, val loss: 1.3413, val acc: 0.4297  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 37460] train loss: 1.3581, train acc: 0.4305, val loss: 1.3412, val acc: 0.4277  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 37480] train loss: 1.3590, train acc: 0.4320, val loss: 1.3413, val acc: 0.4273  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 37500] train loss: 1.3587, train acc: 0.4316, val loss: 1.3434, val acc: 0.4347  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 37520] train loss: 1.3599, train acc: 0.4318, val loss: 1.3412, val acc: 0.4260  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 37540] train loss: 1.3565, train acc: 0.4351, val loss: 1.3411, val acc: 0.4334  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 37560] train loss: 1.3570, train acc: 0.4349, val loss: 1.3400, val acc: 0.4334  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 37580] train loss: 1.3546, train acc: 0.4330, val loss: 1.3404, val acc: 0.4307  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 37600] train loss: 1.3578, train acc: 0.4296, val loss: 1.3418, val acc: 0.4331  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 37620] train loss: 1.3572, train acc: 0.4284, val loss: 1.3411, val acc: 0.4331  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 37640] train loss: 1.3555, train acc: 0.4320, val loss: 1.3410, val acc: 0.4317  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 37660] train loss: 1.3594, train acc: 0.4292, val loss: 1.3418, val acc: 0.4324  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 37680] train loss: 1.3607, train acc: 0.4322, val loss: 1.3477, val acc: 0.4337  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 37700] train loss: 1.3567, train acc: 0.4339, val loss: 1.3415, val acc: 0.4287  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 37720] train loss: 1.3549, train acc: 0.4332, val loss: 1.3404, val acc: 0.4341  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 37740] train loss: 1.3530, train acc: 0.4336, val loss: 1.3409, val acc: 0.4327  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 37760] train loss: 1.3653, train acc: 0.4307, val loss: 1.3437, val acc: 0.4290  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 37780] train loss: 1.3661, train acc: 0.4312, val loss: 1.3608, val acc: 0.4236  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 37800] train loss: 1.3571, train acc: 0.4284, val loss: 1.3423, val acc: 0.4354  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 37820] train loss: 1.3581, train acc: 0.4290, val loss: 1.3425, val acc: 0.4347  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 37840] train loss: 1.3556, train acc: 0.4284, val loss: 1.3417, val acc: 0.4401  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 37860] train loss: 1.3579, train acc: 0.4276, val loss: 1.3405, val acc: 0.4354  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 37880] train loss: 1.3499, train acc: 0.4339, val loss: 1.3399, val acc: 0.4317  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 37900] train loss: 1.3414, train acc: 0.4351, val loss: 1.3236, val acc: 0.4513  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 37920] train loss: 1.3398, train acc: 0.4334, val loss: 1.3237, val acc: 0.4435  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 37940] train loss: 1.3352, train acc: 0.4377, val loss: 1.3231, val acc: 0.4428  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 37960] train loss: 1.3379, train acc: 0.4361, val loss: 1.3220, val acc: 0.4445  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 37980] train loss: 1.3355, train acc: 0.4369, val loss: 1.3228, val acc: 0.4425  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 38000] train loss: 1.3354, train acc: 0.4361, val loss: 1.3205, val acc: 0.4432  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 38020] train loss: 1.3347, train acc: 0.4375, val loss: 1.3209, val acc: 0.4445  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 38040] train loss: 1.3368, train acc: 0.4333, val loss: 1.3214, val acc: 0.4438  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 38060] train loss: 1.3360, train acc: 0.4370, val loss: 1.3209, val acc: 0.4435  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 38080] train loss: 1.3332, train acc: 0.4359, val loss: 1.3221, val acc: 0.4347  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 38100] train loss: 1.3409, train acc: 0.4313, val loss: 1.2984, val acc: 0.4550  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 38120] train loss: 1.3066, train acc: 0.4500, val loss: 1.2918, val acc: 0.4550  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 38140] train loss: 1.3180, train acc: 0.4540, val loss: 1.3073, val acc: 0.4553  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 38160] train loss: 1.5104, train acc: 0.2252, val loss: 1.5385, val acc: 0.2351  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 38180] train loss: 1.4502, train acc: 0.3316, val loss: 1.4277, val acc: 0.2998  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 38200] train loss: 1.4358, train acc: 0.3573, val loss: 1.4108, val acc: 0.3612  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 38220] train loss: 1.4164, train acc: 0.3461, val loss: 1.4028, val acc: 0.3342  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 38240] train loss: 1.4156, train acc: 0.3676, val loss: 1.4008, val acc: 0.3575  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 38260] train loss: 1.4117, train acc: 0.3697, val loss: 1.4001, val acc: 0.3164  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 38280] train loss: 1.4087, train acc: 0.3581, val loss: 1.4007, val acc: 0.3646  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 38300] train loss: 1.3844, train acc: 0.3953, val loss: 1.3737, val acc: 0.3929  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 38320] train loss: 1.3777, train acc: 0.3971, val loss: 1.3648, val acc: 0.3963  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 38340] train loss: 1.3808, train acc: 0.4007, val loss: 1.3636, val acc: 0.3990  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 38360] train loss: 1.3602, train acc: 0.4051, val loss: 1.3455, val acc: 0.4047  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 38380] train loss: 1.3360, train acc: 0.4190, val loss: 1.3214, val acc: 0.4290  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 38400] train loss: 1.3456, train acc: 0.4129, val loss: 1.3287, val acc: 0.4256  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 38420] train loss: 1.3379, train acc: 0.4192, val loss: 1.3263, val acc: 0.4226  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 38440] train loss: 1.3228, train acc: 0.4209, val loss: 1.3152, val acc: 0.4381  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 38460] train loss: 1.3162, train acc: 0.4280, val loss: 1.2992, val acc: 0.4307  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 38480] train loss: 1.3091, train acc: 0.4247, val loss: 1.2916, val acc: 0.4320  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 38500] train loss: 1.3295, train acc: 0.4285, val loss: 1.2851, val acc: 0.4378  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 38520] train loss: 1.3643, train acc: 0.4190, val loss: 1.3422, val acc: 0.4337  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 38540] train loss: 1.3137, train acc: 0.4247, val loss: 1.2976, val acc: 0.4337  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 38560] train loss: 1.3037, train acc: 0.4280, val loss: 1.2903, val acc: 0.4368  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 38580] train loss: 1.2953, train acc: 0.4320, val loss: 1.2853, val acc: 0.4384  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 38600] train loss: 1.2906, train acc: 0.4376, val loss: 1.2788, val acc: 0.4395  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 38620] train loss: 1.2870, train acc: 0.4401, val loss: 1.2756, val acc: 0.4405  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 38640] train loss: 1.2884, train acc: 0.4371, val loss: 1.2759, val acc: 0.4391  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 38660] train loss: 1.2872, train acc: 0.4387, val loss: 1.2764, val acc: 0.4445  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 38680] train loss: 1.2855, train acc: 0.4403, val loss: 1.2719, val acc: 0.4398  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 38700] train loss: 1.2844, train acc: 0.4409, val loss: 1.2720, val acc: 0.4459  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 38720] train loss: 1.2837, train acc: 0.4414, val loss: 1.2717, val acc: 0.4452  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 38740] train loss: 1.2840, train acc: 0.4470, val loss: 1.2706, val acc: 0.4442  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 38760] train loss: 1.2821, train acc: 0.4467, val loss: 1.2706, val acc: 0.4489  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 38780] train loss: 1.2832, train acc: 0.4439, val loss: 1.2735, val acc: 0.4405  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 38800] train loss: 1.2841, train acc: 0.4389, val loss: 1.2775, val acc: 0.4553  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 38820] train loss: 1.2863, train acc: 0.4521, val loss: 1.2677, val acc: 0.4543  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 38840] train loss: 1.2761, train acc: 0.4559, val loss: 1.2601, val acc: 0.4627  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 38860] train loss: 1.2753, train acc: 0.4625, val loss: 1.2557, val acc: 0.4631  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 38880] train loss: 1.2709, train acc: 0.4734, val loss: 1.2533, val acc: 0.4742  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 38900] train loss: 1.2660, train acc: 0.4725, val loss: 1.2510, val acc: 0.4668  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 38920] train loss: 1.2749, train acc: 0.4761, val loss: 1.2585, val acc: 0.4735  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 38940] train loss: 1.2654, train acc: 0.4766, val loss: 1.3157, val acc: 0.4243  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 38960] train loss: 1.2931, train acc: 0.4635, val loss: 1.3887, val acc: 0.4287  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 38980] train loss: 1.2795, train acc: 0.4562, val loss: 1.2489, val acc: 0.4637  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 39000] train loss: 1.2711, train acc: 0.4777, val loss: 1.2394, val acc: 0.4641  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 39020] train loss: 1.2247, train acc: 0.4756, val loss: 1.2086, val acc: 0.5066  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 39040] train loss: 1.2213, train acc: 0.4834, val loss: 1.2013, val acc: 0.4890  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 39060] train loss: 1.1740, train acc: 0.5258, val loss: 1.1637, val acc: 0.5265  (best train acc: 0.5266, best val acc: 0.5568, best train loss: 1.1740  @ epoch 39060 )\n",
      "[Epoch: 39080] train loss: 1.1624, train acc: 0.5312, val loss: 1.1590, val acc: 0.5120  (best train acc: 0.5385, best val acc: 0.5568, best train loss: 1.1545  @ epoch 39079 )\n",
      "[Epoch: 39100] train loss: 1.3047, train acc: 0.4519, val loss: 1.2332, val acc: 0.4631  (best train acc: 0.5420, best val acc: 0.5568, best train loss: 1.1545  @ epoch 39079 )\n",
      "[Epoch: 39120] train loss: 1.1882, train acc: 0.5231, val loss: 1.1988, val acc: 0.5201  (best train acc: 0.5420, best val acc: 0.5568, best train loss: 1.1545  @ epoch 39079 )\n",
      "[Epoch: 39140] train loss: 1.1764, train acc: 0.5086, val loss: 1.1653, val acc: 0.5042  (best train acc: 0.5420, best val acc: 0.5568, best train loss: 1.1545  @ epoch 39079 )\n",
      "[Epoch: 39160] train loss: 1.1522, train acc: 0.5408, val loss: 1.1447, val acc: 0.5420  (best train acc: 0.5420, best val acc: 0.5568, best train loss: 1.1522  @ epoch 39160 )\n",
      "[Epoch: 39180] train loss: 1.1447, train acc: 0.5437, val loss: 1.1339, val acc: 0.5454  (best train acc: 0.5437, best val acc: 0.5568, best train loss: 1.1423  @ epoch 39173 )\n",
      "[Epoch: 39200] train loss: 1.1394, train acc: 0.5375, val loss: 1.1307, val acc: 0.5470  (best train acc: 0.5481, best val acc: 0.5568, best train loss: 1.1372  @ epoch 39187 )\n",
      "[Epoch: 39220] train loss: 1.1348, train acc: 0.5405, val loss: 1.1265, val acc: 0.5484  (best train acc: 0.5481, best val acc: 0.5568, best train loss: 1.1348  @ epoch 39220 )\n",
      "[Epoch: 39240] train loss: 1.4603, train acc: 0.4437, val loss: 1.6213, val acc: 0.4165  (best train acc: 0.5481, best val acc: 0.5568, best train loss: 1.1319  @ epoch 39234 )\n",
      "[Epoch: 39260] train loss: 1.2538, train acc: 0.4832, val loss: 1.2121, val acc: 0.4830  (best train acc: 0.5481, best val acc: 0.5568, best train loss: 1.1319  @ epoch 39234 )\n",
      "[Epoch: 39280] train loss: 1.2211, train acc: 0.4903, val loss: 1.1969, val acc: 0.4847  (best train acc: 0.5481, best val acc: 0.5568, best train loss: 1.1319  @ epoch 39234 )\n",
      "[Epoch: 39300] train loss: 1.1997, train acc: 0.5000, val loss: 1.1747, val acc: 0.5130  (best train acc: 0.5481, best val acc: 0.5568, best train loss: 1.1319  @ epoch 39234 )\n",
      "[Epoch: 39320] train loss: 1.1979, train acc: 0.5080, val loss: 1.1710, val acc: 0.5133  (best train acc: 0.5481, best val acc: 0.5568, best train loss: 1.1319  @ epoch 39234 )\n",
      "[Epoch: 39340] train loss: 1.1926, train acc: 0.5120, val loss: 1.1678, val acc: 0.5218  (best train acc: 0.5481, best val acc: 0.5568, best train loss: 1.1319  @ epoch 39234 )\n",
      "[Epoch: 39360] train loss: 1.1901, train acc: 0.5081, val loss: 1.1758, val acc: 0.5099  (best train acc: 0.5481, best val acc: 0.5568, best train loss: 1.1319  @ epoch 39234 )\n",
      "[Epoch: 39380] train loss: 1.1932, train acc: 0.5034, val loss: 1.1666, val acc: 0.5184  (best train acc: 0.5481, best val acc: 0.5568, best train loss: 1.1319  @ epoch 39234 )\n",
      "[Epoch: 39400] train loss: 1.1933, train acc: 0.5058, val loss: 1.1713, val acc: 0.5079  (best train acc: 0.5481, best val acc: 0.5568, best train loss: 1.1319  @ epoch 39234 )\n",
      "[Epoch: 39420] train loss: 1.1833, train acc: 0.5043, val loss: 1.1632, val acc: 0.5177  (best train acc: 0.5481, best val acc: 0.5568, best train loss: 1.1319  @ epoch 39234 )\n",
      "[Epoch: 39440] train loss: 1.1811, train acc: 0.5079, val loss: 1.1612, val acc: 0.5191  (best train acc: 0.5481, best val acc: 0.5568, best train loss: 1.1319  @ epoch 39234 )\n",
      "[Epoch: 39460] train loss: 1.1798, train acc: 0.5102, val loss: 1.1585, val acc: 0.5234  (best train acc: 0.5481, best val acc: 0.5568, best train loss: 1.1319  @ epoch 39234 )\n",
      "[Epoch: 39480] train loss: 1.1790, train acc: 0.5088, val loss: 1.1607, val acc: 0.5170  (best train acc: 0.5481, best val acc: 0.5568, best train loss: 1.1319  @ epoch 39234 )\n",
      "[Epoch: 39500] train loss: 1.1806, train acc: 0.5069, val loss: 1.1632, val acc: 0.5069  (best train acc: 0.5481, best val acc: 0.5568, best train loss: 1.1319  @ epoch 39234 )\n",
      "[Epoch: 39520] train loss: 1.1821, train acc: 0.5089, val loss: 1.1769, val acc: 0.4938  (best train acc: 0.5481, best val acc: 0.5568, best train loss: 1.1319  @ epoch 39234 )\n",
      "[Epoch: 39540] train loss: 1.1857, train acc: 0.4932, val loss: 1.1574, val acc: 0.5356  (best train acc: 0.5481, best val acc: 0.5568, best train loss: 1.1319  @ epoch 39234 )\n",
      "[Epoch: 39560] train loss: 1.1761, train acc: 0.5006, val loss: 1.1564, val acc: 0.5396  (best train acc: 0.5481, best val acc: 0.5568, best train loss: 1.1319  @ epoch 39234 )\n",
      "[Epoch: 39580] train loss: 1.1718, train acc: 0.5098, val loss: 1.1595, val acc: 0.5268  (best train acc: 0.5481, best val acc: 0.5568, best train loss: 1.1319  @ epoch 39234 )\n",
      "[Epoch: 39600] train loss: 1.1751, train acc: 0.5085, val loss: 1.1657, val acc: 0.5035  (best train acc: 0.5481, best val acc: 0.5568, best train loss: 1.1319  @ epoch 39234 )\n",
      "[Epoch: 39620] train loss: 1.1775, train acc: 0.5048, val loss: 1.1600, val acc: 0.5234  (best train acc: 0.5481, best val acc: 0.5568, best train loss: 1.1319  @ epoch 39234 )\n",
      "[Epoch: 39640] train loss: 1.1712, train acc: 0.5097, val loss: 1.1528, val acc: 0.5298  (best train acc: 0.5481, best val acc: 0.5568, best train loss: 1.1319  @ epoch 39234 )\n",
      "[Epoch: 39660] train loss: 1.1728, train acc: 0.4983, val loss: 1.1589, val acc: 0.5255  (best train acc: 0.5481, best val acc: 0.5568, best train loss: 1.1319  @ epoch 39234 )\n",
      "[Epoch: 39680] train loss: 1.1739, train acc: 0.5103, val loss: 1.1531, val acc: 0.5339  (best train acc: 0.5481, best val acc: 0.5568, best train loss: 1.1319  @ epoch 39234 )\n",
      "[Epoch: 39700] train loss: 1.1712, train acc: 0.5042, val loss: 1.1515, val acc: 0.5278  (best train acc: 0.5481, best val acc: 0.5568, best train loss: 1.1319  @ epoch 39234 )\n",
      "[Epoch: 39720] train loss: 1.1696, train acc: 0.4993, val loss: 1.1512, val acc: 0.5376  (best train acc: 0.5481, best val acc: 0.5568, best train loss: 1.1319  @ epoch 39234 )\n",
      "[Epoch: 39740] train loss: 1.1700, train acc: 0.5046, val loss: 1.1517, val acc: 0.5339  (best train acc: 0.5481, best val acc: 0.5568, best train loss: 1.1319  @ epoch 39234 )\n",
      "[Epoch: 39760] train loss: 1.1743, train acc: 0.4939, val loss: 1.1519, val acc: 0.5417  (best train acc: 0.5481, best val acc: 0.5568, best train loss: 1.1319  @ epoch 39234 )\n",
      "[Epoch: 39780] train loss: 1.2152, train acc: 0.4915, val loss: 1.1806, val acc: 0.5073  (best train acc: 0.5481, best val acc: 0.5568, best train loss: 1.1319  @ epoch 39234 )\n",
      "[Epoch: 39800] train loss: 1.1829, train acc: 0.4766, val loss: 1.1574, val acc: 0.5288  (best train acc: 0.5481, best val acc: 0.5568, best train loss: 1.1319  @ epoch 39234 )\n",
      "[Epoch: 39820] train loss: 1.2216, train acc: 0.4502, val loss: 1.2877, val acc: 0.4553  (best train acc: 0.5481, best val acc: 0.5568, best train loss: 1.1319  @ epoch 39234 )\n",
      "[Epoch: 39840] train loss: 1.1801, train acc: 0.5032, val loss: 1.1647, val acc: 0.5201  (best train acc: 0.5481, best val acc: 0.5568, best train loss: 1.1319  @ epoch 39234 )\n",
      "[Epoch: 39860] train loss: 1.1660, train acc: 0.5154, val loss: 1.1527, val acc: 0.5272  (best train acc: 0.5481, best val acc: 0.5568, best train loss: 1.1319  @ epoch 39234 )\n",
      "[Epoch: 39880] train loss: 1.1635, train acc: 0.5115, val loss: 1.1519, val acc: 0.5302  (best train acc: 0.5481, best val acc: 0.5568, best train loss: 1.1319  @ epoch 39234 )\n",
      "[Epoch: 39900] train loss: 1.1577, train acc: 0.5096, val loss: 1.1447, val acc: 0.5464  (best train acc: 0.5481, best val acc: 0.5568, best train loss: 1.1319  @ epoch 39234 )\n",
      "[Epoch: 39920] train loss: 1.1571, train acc: 0.5077, val loss: 1.1447, val acc: 0.5433  (best train acc: 0.5481, best val acc: 0.5568, best train loss: 1.1319  @ epoch 39234 )\n",
      "[Epoch: 39940] train loss: 1.1526, train acc: 0.5209, val loss: 1.1386, val acc: 0.5477  (best train acc: 0.5481, best val acc: 0.5568, best train loss: 1.1319  @ epoch 39234 )\n",
      "[Epoch: 39960] train loss: 1.1521, train acc: 0.5164, val loss: 1.1371, val acc: 0.5524  (best train acc: 0.5481, best val acc: 0.5568, best train loss: 1.1319  @ epoch 39234 )\n",
      "[Epoch: 39980] train loss: 1.1523, train acc: 0.5252, val loss: 1.1390, val acc: 0.5450  (best train acc: 0.5481, best val acc: 0.5568, best train loss: 1.1319  @ epoch 39234 )\n",
      "[Epoch: 40000] train loss: 1.1505, train acc: 0.5265, val loss: 1.1387, val acc: 0.5484  (best train acc: 0.5481, best val acc: 0.5568, best train loss: 1.1319  @ epoch 39234 )\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pmaur\\miniconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\pmaur\\miniconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\pmaur\\miniconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\pmaur\\miniconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\pmaur\\miniconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\pmaur\\miniconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       154\n",
      "           1       0.65      0.82      0.72       703\n",
      "           2       0.39      0.45      0.42       702\n",
      "           3       0.42      0.39      0.40       703\n",
      "           4       0.66      0.57      0.61       702\n",
      "\n",
      "    accuracy                           0.53      2964\n",
      "   macro avg       0.42      0.45      0.43      2964\n",
      "weighted avg       0.50      0.53      0.51      2964\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 504x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaMAAAGQCAYAAAADew/3AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAABT0ElEQVR4nO3dd3xT1fvA8c/TgWWU0TZtgbL3UByIoDJdgExBxb1YigNwj58LQVFxD5a4viA4GcpwsJeAypYlCBToplB2m5zfHwml6a4kuU143r7yMvfec849h6R5cs49OVeMMSillFJWCrK6AkoppZQGI6WUUpbTYKSUUspyGoyUUkpZToORUkopy4VYXQGllFJnp4d089i06JnmR/FUWSWhPSOllFKW056RUkr5uaAA6FdoMFJKKT8nYsnImkf5fzhVSinl97RnpJRSfk6H6ZRSSlkuSIfplFJKnWtEpLOIbBWRHSLyVD7HK4nILBFZJyKbROSeosrUnpFSSvk58WG/QkSCgQ+Ba4B4YLWIzDTGbM6RbAiw2RjTXURswFYRmWyMOVVQuRqMlFLKz/l4mK4VsMMYsxNARKYCPYGcwcgA4eKc5lcBSAOyCitUh+mUUkplE5GBIrImx2NgriTVgb05tuNd+3L6AGgC7Ac2AI8YYxyFnVd7Rkop5ec8OUxnjBkPjC/0dPlky7V9HbAW6ATUA34RkSXGmMMFFao9I6WU8nNBIh57FEM8UCPHdhzOHlBO9wDfG6cdwC6gcaFtKEF7lVJKqdVAAxGpIyJlgH7AzFxp9gBXAYhIDNAI2FlYoTpMp5RSfs6XP3o1xmSJyIPAPCAYmGSM2SQig13HxwIjgM9EZAPOYb0njTEphZWrwUgppfycr9emM8bMBmbn2jc2x/P9wLUlKVOH6ZRSSllOe0ZKKeXndG06pZRSltO16ZRSSikP0GCkAp6I9BOR30XkqIgkuZ4/IDmu+orIiyJiRKSVa/s2ETniehwXEUeO7SPWtUapvIQgjz2sosFIBTQReRR4F3gDiAVigMHAFUAZVxoB7sC5ftZdAMaYycaYCsaYCkAXYP/pbdc+pUqNIAny2MOyNlh2ZqW8TEQqAS8DDxhjvjXGZLh+Ef6XMeY2Y8xJV9K2QDXgEaCf64d8Sikf0mCkAlkb4DxgRhHp7gJmAdNc2928WSmlPE08+J9VNBipQBYFpBhjspeuF5HlIpLuug7UTkTKATcCU4wxmcC3uIbqlPIXOkynVOmWCkSJSPZPGIwxlxtjKruOBQG9cd5n5fSvyScDXVw3BFNK+YgGIxXIVgAncd74qyB34bz51x4RSQC+AUKBW7xfPaU8w3Nz6awbptMfvaqAZYxJF5GXgI9cM+bmAseAC4DyOG8IdhXO2XLrc2QdijNIvefTCiv1H1k5JdtTNBipgGaMeV1E9gFPAF8AR3EuZf8kzpt+rTXG/Jwzj4i8BzwqIs2NMRt9XWelzkUajFTAM8ZMxnktKD8v55N+P86hutPbC3HeQEypUikQlgPSYKSUUn5OF0pVSillOV/fz8gb/D+cKqWU8nvaM1JKKT+nw3TeZayugFJKeZHHxtZ0AoOXnbA7rK6CJcKCgxjzfz8XnTDAPDriWqYu2Wl1NSzRr21d/t53yOpq+FyT6pXYlXxu3pGjjk0Xf8+pVAcjpZRSRdMfvSqllLJcIAzT+X84VUop5fe0Z6SUUn5Oh+mUUkpZzsr7EHmK/7dAKaWU39OekVJK+Tkr70PkKRqMlFLKz4kO0ymllFJnT3tGSinl53SYTimllOV0Np1SSinlAdozUkopPyc6TKeUUspyQf4fjHSYTimllOW0Z6SUUv4uAFbt1mCklFJ+TnSYTiml1LlGRDqLyFYR2SEiT+Vz/HERWet6bBQRu4hEFFamBiOllPJ3Ip57FHkqCQY+BLoATYFbRKRpzjTGmDeMMRcaYy4EngYWGWPSCitXh+mUUsrf+XaYrhWwwxizE0BEpgI9gc0FpL8F+KqoQrVnpJRSKpuIDBSRNTkeA3MlqQ7szbEd79qXX1nlgM7Ad0WdV3tGSinl7zzYMzLGjAfGF5Ikv5OZAtJ2B5YVNUQHGoyUUsrviW+ndscDNXJsxwH7C0jbj2IM0YEO0ymllCqZ1UADEakjImVwBpyZuROJSCWgPTCjOIVqz0gppfydDycwGGOyRORBYB4QDEwyxmwSkcGu42NdSXsDPxtjjhan3HMiGC1bsoTRr47CYXfQu29f7hswwO24MYbRo0axdPFiwsqGMWLUKJo0bVZo3kPp6Tzx6HD279tHterVeeOtt6lYqZLP21aY2vUj6Xh9Y0SEjX/Es2rJv/mmi6lekVsHXsaPX69n+6ZEqkSVo9tNF2Qfr1SlHMvn7+DPFXuy97W8ohbtOzfio1cXcPxYprebUmLbN65hzldjMQ4HF7ftTNuuN7kdX79yPkvnfANAmbCydLv9QWJr1CUlIZ5vxr2ane5g8gE69ryDNtf0JmHvTmZ9+T6nTp6gcmQ0fQY8QVjZ8j5tV1H+XLWCiR+MweFwcE3XnvS59S634/F7/uX911/mn+1buf3e++l18+0AnDp1kmcfGURm5insdjuXt7+KW+4+c936x++nMXv6NwQHB3NJ6yu4e9DDPm1XcaxZuZyP330Th8NO5269uPmOe9yO7929izGjXuKfbVu4a8AD9L31zuxjb416id+XL6FylQjGffl19v6Mw4cY9fzTJCbsJya2Gs+8/BrhFSv6rE3F5uMVGIwxs4HZufaNzbX9GfBZccsM+GE6u93OqFdG8NG48fwwaxZzZ//EPzt2uKVZungxe3bvZtbcuTz/0ku88tLLReadNHECrVq3YdbcebRq3YZPJk7wedsKIwJXdW/C91/8yWfvL6PRBVWJsOX94BSBdtc25N8dKdn7DqYc48uPVvLlRyv538crycq0s31zUvbx8IrnUateJIfTj/ukLSXlcNj5afKH3D50BENGjGPDqoUk7d/tlqZyVCz3PPE6D7z0Me273cLML94DICo2jvtf+JD7X/iQQf/3HqFlwmhy8eUAzPj8Ha7pcw9DXvqYJhdfzrJ5RU4Q8im73c64d1/n+dfe5f1Pp7Fk/jz2/rvTLU2F8Ir0f/Axet10m9v+0NAyvPzWR7wzcQpvT5jMn6tWsHXzBgA2/LWGVcsX8+7EKbz/6TR63XS7z9pUXHa7nQ/feo1X3nyP8f/7loW/zmP3Lve2h1esxP1DH6dPvzvy5L+ma3deGfN+nv3T/vcZF15yKZOmTufCSy7l6/995q0mnPMCPhht3LCeGjVrElejBqFlytC5S1cWzp/vlmbB/Pl079kTEeGCFheSkXGY5OSkQvMumD+fHr16AtCjV08W/Pabz9tWmNi4SqSnHuPQweM47IatGxKo3yQ6T7qLWtdk+6ZEjh05lW85NetGkp52jIxDJ7L3dejamMU/b8MUNH/GYvt2bSMiuhoRtqqEhITSvFV7tqxd6ZamZv2mlC0fDkBc3cYcPpiSp5ydf6+liq0qlSNjAEhNiKdWw/MBqNf0Yv7+Y6mXW1Iy27dsomr1OGKrVSc0NJQrO13L78sXu6WpXCWCBo2bEhzsPigiIpQtWw4Ae1YW9qys7Ivic2Z+R59b7iK0TJnsMkqbrX9vompcDapWjyM0NJT2V1/LiqUL3dJUrhJBoybNCA7JOyB0/oUXE14x78jGiiWLuLpLNwCu7tKN5UsW5klTKgSJ5x5WNcGyM/tIUmISsbGx2dvRsTEkJiW6p0lKJCZHmpiYWJISkwrNm5aais3m/HC32aJJSyty5qJPVagY5hZAMg6doEL4ee5pws+jfpNo1q3emzt7tsbnx7JlQ0L2dr3GNo4cPkFywhHPV9pDDh9MoVIVW/Z2pSpRZBxMLTD9n0vn0aB5yzz7N65axPmXtc/ejq5em62uoLZpzRIOpeUNYFZKS0kmKjomezsyKpq05ORi57fb7QwdcBt33XAdLVq2omGT5gDsj9/D5g1refyBe3h26CC2bynot43WSU1Owpaj7VG2GFJL0PaCpB9MJTLK+V6KjLJx6GDp+jvPJkGee1jEK2cWkTARGSoiH4jIIBGx7NqUyefre54bUeWXRqR4eUup4tSyQ9dGLPl5e4E9nKBgoV5jG9s2OgNwSGgQl7Wry7Lf/vFcRX2lgH+QXVvW8eeSn7mm771u+7OyMtm67neaXdI2e1/Pu4exasEsxr78ECdPHM/3G7aV8nu/luTtGhwczDsTJjPx6x/ZvmUzu3c5X2eH3c6RjMO8/uEk7hr0MG+8/HT+57JQvn+rAbCS9bnEW39NnwOZwBLOrF/0SFGZXL/0HQgwbtw47ryv/1lXJCY2hoSEM9/skxISiY52H66KjoklMUeaxMQEbNE2MjNPFZg3IjKS5OQkbLZokpOTiIgoXUMXGYdPEF4pLHs7vFIYRzJOuqWJrV6J610TFcqWC6VuQxvG4WDH385vlHUaRJF44DDHjjqH8CpHlKNSlbLcOaSNs8yK53H7/a2ZPO73Aof5rFCxShSHDp75VnzoYArhlSPzpEvYu4sZn7/D7Y+MoFwF94vSOzasoWrNelSoVCV7n61qDe4cPgqAlIR4tq9f5aUW/DeRtmhScvT6U1OSiIiyFZIjfxUqhNO8xcX8tWoFterUI9IWTeu2HRERGjZphkgQhw+lU6lylaIL85Go6BiSc7Q9JTmRiKiosy63cpVIUlOSiYyykZqSTKVSOEQJump3YZoaY243xowD+gJti8oAzl/+GmNaGmNaDhyYewWK/6ZZ8/PZs3s38fHxZJ46xdw5s2nfsaNbmg6dOjJrxgyMMaxft5YK4eHYbNGF5u3QsRMzpzunz8+cPoOOnTp5pL6ekrDvMJUjy1GxclmCgoVG58fyz5YktzQT31qS/di2KZFff/w7OxABNL4gli3rzwTjlMQjfDx6YXaejMMn+d/HK0tVIAKoVrshaYn7OZicQFZWJhtXLaJxi9ZuadJTk5j20QhuuO9xomLj8pSxYdVCzm/VwW3fkcPpADgcDhb/NJWWHbp6qwn/SYPGTTmwby+JB/aRmZnJ0vk/06pNsf70OJR+kCNHMgA4efIE6/5cRfWatQC47Ir2bPhrDQD79u4mKyuTipUqe6UN/1Wjxk3Zv3cvCfudbV/068+0vqJ90RmL0PrKdvw650cAfp3zI23ann2ZXhEA14y81TPKnuvrmpPupdMULSQkhKeffY77B/TH4XDQq/cN1G/QgK+nTgXgpn79aNuuPUsXL6Zb5+sICwvj5ZGjCs0LcO+A/jw+bDjTv/uW2KrVePPtty1rY36MwzD/xy30uetigoKEjX/uIzXpKBdc6vzgXb86vtD8IaFB1KoXyS8z/vZFdT0qODiYrrfez5fvPIfDYeeiK64lunotVi/8CYBLO1zPollTOHY0g58mfwhAUFAwg/7POaPu1MkT/LP5L7rf4T59ecOqhaxe4PxganLR5Vx0xbU+bFXRgoNDGPDQ47z05MPY7Q6u7tKdmnXqMXemc9Zf5x59OJiWwmOD7+bYsaOICLO+m8r7n07lYGoK745+CYfDgXE4uKLD1VzqCmRXdenBB2+M4OF7+xESEsojT75Q6obAgkNCeGD4Ezw7/EEcDjvXXt+T2nXr8dP0bwG4vldf0lJTeLj/HRw7ehQJEqZ/8xXj/vcN5ctX4NUXnmH92jUcTk/n9t5duP2+Qc7p4bffzajnn2LeTzOIjonl2RGjLW5p4BJvjP2KiB04/UMnAcoCx1zPjTGmOBP1zQm7w+N18wdhwUGM+b+fra6Gzz064lqmLtlZdMIA1K9tXf7ed8jqavhck+qV2JVceifDeFMdWwWPRfSRDd/02Af5s9ses+Sbhld6RsaYYG+Uq5RSKh96zUgppZQ6e6VrbqpSSqkSK23X8P4LDUZKKeXvdJhOKaWUOnvaM1JKKX+nw3RKKaUsp8N0Siml1NnTnpFSSvm7AOgZaTBSSik/FwhTu3WYTimllOW0Z6SUUv5Oh+mUUkpZTofplFJKqbOnPSOllPJ3OkynlFLKaoEwm06DkVJK+bsA6BnpNSOllFKW056RUkr5uwDoGWkwUkopfxcA14x0mE4ppZTltGeklFL+TofplFJKWS0QpnbrMJ1SSinLac9IKaX8nQ7TKaWUspwO0ymllFJnT4wxVtehIKW2Ykop5QEe686MvvpTj31ePvnrPZZ0s0r1MN0Ju8PqKlgiLDiIHtLN6mr43EzzI7tTj1pdDUvUiizP4ROZVlfD5yqGhZJ27JTV1bBERLkynivM/0fpdJhOKaWU9TQYKaWUvxPx3KNYp5POIrJVRHaIyFMFpOkgImtFZJOILCqqzFI9TKeUUqpo4sOp3SISDHwIXAPEA6tFZKYxZnOONJWBj4DOxpg9IhJdVLnaM1JKKVUSrYAdxpidxphTwFSgZ640twLfG2P2ABhjkooqVIORUkr5O/HcQ0QGisiaHI+Buc5WHdibYzvetS+nhkAVEVkoIn+IyJ1FNUGH6ZRSyt958EevxpjxwPjCzpZftlzbIcAlwFVAWWCFiKw0xmwrqFANRkoppUoiHqiRYzsO2J9PmhRjzFHgqIgsBloABQYjHaZTSil/FySeexRtNdBAROqISBmgHzAzV5oZQFsRCRGRcsBlwN+FFao9I6WU8nc+/NGrMSZLRB4E5gHBwCRjzCYRGew6PtYY87eIzAXWAw5gojFmY2HlajBSSilVIsaY2cDsXPvG5tp+A3ijuGVqMFJKKX8XAKt2azBSSil/FwBX/wOgCUoppfyd9oyUUsrf6TCdUkopq0kABCMdplNKKWU57RkppZS/8/+OkQYjpZTyez68hYS36DCdUkopy2nPSCml/F0ATGDQYKSUUv7O/2ORDtMppZSynvaMlFLK3wXABAYNRkop5e/8PxbpMJ1SSinrnRPBaNmSJfTo2oVu113HJxMm5DlujOG1kSPpdt119O3Vk783byoy76H0dAbddy/dO1/HoPvu5fChQz5pS0lcfN3FfLRlLOO2j6fPk33zHC9XsRzPzXyed9e+zwcbP+Squ68GICouilfmj+LDzR/zwcYP6f5wj+w8V/S9gg82fsh0+0zqX1LfZ20pqdUrl3Fvv97cfWMPpn7xaZ7je/7dxSMD7uL69pfxzZQvSpT3mylfcO3lF3Mo/aDX6v9fLV+2lD49utG7Wxc++2RinuPGGN58bRS9u3Xhlr692fL3ZgBOnjzJXbf249Ybb+Cm3j0Z99EH2XkOHTrEkEH9uaF7V4YM6s/hw6XvvQ6wYtlSbu7Vnb49uvLFpPzb/tboV+nboyu333QDW11tP81ut3Nnvxt59OEhbvu/+WoyN/fqzq19evHBO295tQ3/mYjnHhbxajASkShvll8cdrudUa+M4KNx4/lh1izmzv6Jf3bscEuzdPFi9uzezay5c3n+pZd45aWXi8w7aeIEWrVuw6y582jVug2fTMwb5KwUFBTEoA/v56UuLzCk6QO0u6U9NZrUcEtz/ZDr2bt5D49c+BDPdHiae8fcR0hoCPYsO5Me/YQhTe/n8daP0XXI9dl5d2/czas3jGLT4k35nbZUsNvtfPDmaEaOeZ8JU75j4a9z2b1rp1ua8IqVeGDYE/S95Y4S5U1KTODPVSuJjon1SVtKwm638/qoV3j3o4/5+oeZ/Dx3Njv/+cctzfKlS9izZw/fz5rNM8+/yGuvjACgTJkyfDxxElO++Z4pX3/LimXL2LB+HQCfT5rIpa1a8/2s2VzaqjWff/KJz9tWFLvdzpjXRvLWBx/x1Xcz+GXuHHblavuKpUvYu2c338z4iaeee4HXR73idvzrKf+jdp06bvv+WL2KxQsX8OXX3zHlu+nceuddXm/LfyFB4rGHVbwSjESku4gkAxtEJF5ELvfGeYpj44b11KhZk7gaNQgtU4bOXbqycP58tzQL5s+ne8+eiAgXtLiQjIzDJCcnFZp3wfz59OjVE4AevXqy4LfffN62wjRo1ZADOw6QuCuRrMwslkxdzGU9W7ulMQbKhpcFoGyFshxJy8CeZedgwkF2/uX8Qz5+5Djxf+8lsnokAPFb4tm3bZ9vG1NCWzdvpFpcHFWrxxEaGkr7q69j+ZKFbmmqRETQqGkzgkNCSpR37Ltj6D9kaKlcmHLTxg3UqFGTuLgahIaGck3nLixa6P5eX7RgAdd374GIcP4FLcjIyCAlORkRoVy5cgBkZWWRlZWFuC5ELFqwgG49nO/1bj16snCBe5mlweaNG4irUZPqrrZffV0XFi9c4JZm8aIFdOnmbHvzC1pwxNV2cH7JWLZ0CT1693HL8/0307jjnvsoU6YMABERkb5p0DnIWz2jkUBbY0xVoA/wqpfOU6SkxCRiY898i42OjSExKdE9TVIiMTnSxMTEkpSYVGjetNRUbLZoAGy2aNLS0rzZjBKLrB5Jyt7k7O2U+JTsgHLaTx/8SFyTGny2/wve2/ABEx4ZjzHGLU10rWjqXlSXrb9v9Um9PSElORlbjp6LzRZNanLSWeddsWQRUbZo6jVo6NkKe0hyUpL7+zg6huTEpFxpEonJ0b7omBiSXO9pu93OrTf14dqO7bisdRuaX3ABAGlpqUTZbABE2WwcLGXvdXC2PTpXu5KTE/OkyfnvY4uJITnJ+e/zzhuv8+AjwwgKcv9I3Lt7N+v++pP77riV+++7m82bNnqxFWdBPPiwiLeCUZYxZguAMeZ3ILw4mURkoIisEZE148eP90hFcn+4Atnf+HIkyq8uxctbSuX3xT13ey667mJ2rd3J3dXuZOiFDzPog8HZPSWAsPJhPPXdM0wcOoHjGce9XWUPyv/1PJu8J04cZ8rnn3DXgMFnWTfvyff9mqvdppB/m+DgYKZ8/R0//fwbmzZuYMf27d6pqBfk265i/Z3D0sWLqBIRQeOmzfIct9vtZBw+zMQvJvPgsEd57onH8v13tlwAXDPy1tTuaBEZXtC2MSbfq4DGmPHA6ShkTtgdZ12RmNgYEhISsreTEhKJjo52r2xMLIk50iQmJmCLtpGZearAvBGRkSQnJ2GzRZOcnERERMRZ19WTUuJTiaphy96Oiosibb/7N9qr7rma7177FoAD/ziH9OIa12D76m0EhwTz1HfPsGjyQlb8sMKndT9bUbZokhPPvG7JyUlERNkKyVF03gP74knYv4/Bd/bL3v/APbfx/sQviIi0/NIo4OwNuL2PkxKJinZvd3R0LIk52peUmJjdwz8tvGJFLrn0UlYsX0r9Bg2IiIgkJTmZKJuNlORkqpSy9zpAdHQMSbnaFZWrXbZc/z7JrjTzf/2FJYsWsHzpEk6dOsnRo0d58dmneHHka9hiYuhw1dWICM2an09QkJB+8GCp/Dfwd97qGU3A2Rs6/ci5XcFL58xXs+bns2f3buLj48k8dYq5c2bTvmNHtzQdOnVk1owZGGNYv24tFcLDsdmiC83boWMnZk6fAcDM6TPo2KmTL5tVpO2rt1GtQTViascQEhpC237t+H3m725pUvYk0+KqFgBUjq5M9UZxJOx0/rE+9MkjxP+9lxlvT/d11c9aoybN2Be/lwP795GZmcmiX+fR5sr2Z5W3Tr0GfDP7N778/ie+/P4nbLZoPvp0cqkJRABNmzVnz5497IuPJzMzk1/mzqFde/f3ersOHfhp1kyMMWxYv44KFSpkD71lHD4MwIkTJ1i1ciW1a9fJzvPjTOd7/ceZM/L8/ZQGTZo1Z++e3ezf52z7r/Pm0LZDB7c0bdt3ZM6PzrZvXL+O8q62P/DwUGbO+40fZs9jxGtvcMmlrXhx5GsAtOvQiTWrnH83e3b/S2ZmJpWrVPF184oWJJ57WMQrPSNjzEsFHRORod44Z0FCQkJ4+tnnuH9AfxwOB71630D9Bg34eupUAG7q14+27dqzdPFiunW+jrCwMF4eOarQvAD3DujP48OGM/27b4mtWo03337bl80qksPuYNyDY3lx3ssEBQfx66Rf2Lt5D50HdQFg7rg5TBsxlUc+G8p76z9ARPj8yU/JSD1Mkyua0unOTvy7fhfv/PUeAF8+8wV/zFlD615tGPj+ICrZKvH8Ty+wc+0uXuz8vJVNzSM4JIQHhz/JM8OG4LA7uK5bD2rXrcePPzh7gd169yUtNYUH772dY0ePIkHCD9OmMGHKt5QvXyHfvP4gJCSEJ55+hofvH4TdYadHr97Uq1+f776eBkCfm27mirbtWLZ0Cb27dSEsrCzPv+ycTZeSksyLzz2Lw2HH4TBcfe11tG3fAYC77u3P048/yszp3xMTW5XX3ix905tDQkJ49MlnGPrAYBwOO9169qZuvfp8/83XANxw401cfmVbli9dzI09unJeWBjPvfhKEaVC9169Gfni/3Fb396EhIbyfy+PLJWTV/zk6kGhxNfjnyKyxxhTsxhJPTJM54/CgoPoId2srobPzTQ/sjv1qNXVsEStyPIcPpFpdTV8rmJYKGnHTlldDUtElCvjsRDy5r3feeyD/LFJfSwJbVYsBxQAMVwppUqR0thbKyErglEpnIqilFJ+LADW0vFKMBKRDPIPOgKUzWe/Ukqpc5i3JjAU63dFSimlPECH6ZRSSlmtVM7wK6EAGGlUSinl77RnpJRS/i4AuhUajJRSyt8FwDCdBiOllPJ3ARCMAqBzp5RSyt9pz0gppfxdAHQrNBgppZS/02E6pZRS6uxpMFJKKX/n4zu9ikhnEdkqIjtE5Kl8jncQkUMistb1KPI+MzpMp5RS/s6H3QoRCQY+BK4B4oHVIjLTGLM5V9Ilxphi3wtHe0ZKKaVKohWwwxiz0xhzCpgK9DzbQjUYKaWUv/PgMJ2IDBSRNTkeA3OdrTqwN8d2vGtfbm1EZJ2IzBGRZkU1QYfplFLK33lwNp0xZjwwvrCz5Zct1/afQC1jzBER6QpMBxoUdl7tGSmllCqJeKBGju04YH/OBMaYw8aYI67ns4FQEYkqrFANRkop5e+CPPgo2mqggYjUEZEyQD9gZs4EIhIrrvtaiEgrV8mphRWqw3RKKeXvfPijV2NMlog8CMwDgoFJxphNIjLYdXws0Be4X0SygONAP2NMfnf/zqbBSCmlVIm4ht5m59o3NsfzD4APSlKmBiOllPJ3AbAckAYjpZTydwFw9T8AmqCUUsrfac9IKaX8nQ7TeVdY8LnbcZtpfrS6CpaoFVne6ipYpmJYqNVVsEREuTJWV8H/+X8sKt3B6ITdYXUVLBEWHMTcv+KtrobPdb4ojjGjFlhdDUs8+kxHfl23z+pq+NzVLaqzJ/Wo1dWwRM1z+ItXfkp1MFJKKVUMQf7fNdJgpJRS/i4ArhmduxdllFJKlRoF9oxEJIMzK7GeDrvG9dwYYyp6uW5KKaWKw/87RgUHI2NMuC8ropRS6j8KgGtGxRqmE5ErReQe1/MoEanj3WoppZQ6lxQ5gUFEXgBaAo2AT4EywP+AK7xbNaWUUsUSABMYijObrjdwEc4792GM2S8iOoSnlFKlhf/HomIN051y3YfCAIiI/lJLKaWURxWnZ/S1iIwDKovIAOBeYIJ3q6WUUqrYAmACQ5HByBjzpohcAxwGGgLPG2N+8XrNlFJKFc85cs0IYANQFudQ3QbvVUcppdS5qMhrRiLSH1gF3IDzvuYrReReb1dMKaVUMYkHHxYpTs/oceAiY0wqgIhEAsuBSd6smFJKqWIKgGtGxZlNFw9k5NjOAPZ6pzpKKaXORYWtTTfc9XQf8LuIzMB5zagnzmE7pZRSpUGAT2A4/cPWf1yP02Z4rzpKKaVKLADuv1DYQqkv+bIiSimlzl3FWZvOBjwBNAPCTu83xnTyYr2UUkoVVwAM0xWnczcZ2ALUAV4C/gVWe7FOSimlSkLEcw+LFCcYRRpjPgEyjTGLjDH3Aq29XC+llFLnkOL8zijT9f8DInI9sB+I816VlFJKlUggT2DI4RURqQQ8CrwPVASGebVWSimlii8ArhkVZ6HUH11PDwEdvVsdpZRS56LCfvT6Pq57GOXHGPNwIXnvLOykxpgvilU7pZRSRQvwntGasyj30nz2CdAdqA74NBgtW7KE0a+OwmF30LtvX+4bMMDtuDGG0aNGsXTxYsLKhjFi1CiaNG1WaN5D6ek88ehw9u/bR7Xq1XnjrbepWKmSL5tVpL/XruL7zz/E4XDQulNXrul5i9vxNUt/5deZUwE477yy3NR/KNVr1eNgShL/++g1MtIPIkFCm07X06FrHwDi/93B1xPfISvzFEHBwdx47yPUqt/Y520rSu26EXS8pgEisHHdAVat2JNvupiq4dx61yX8OH0T27ckA9D/gdacOmXHGIPDYZj86R8AtOtUj3oNIrHbDekHjzPvxy2cPJnlszYVx6a1q/j20w9wOBxccVVXru11q9vxVUt+5ZcZrtc8LIx+/YcRV9v5mn/+4WscTk9DRLjy6m50dL3mf65YyE/ffE7ivj08PuojatVr5PN2Fcfqlcv46J03cdjtdOnem3533uN2fM+/u3hz5Ivs2LaFewYN4cZb7ywy7+HDhxj5f0+RcGA/sVWr8dyI0YRXrOjTdhVLIF8zMsZ8/l8LNcY8dPq5iAhwG/AksBIY+V/L/S/sdjujXhnBuImfEBMTw60330SHjh2pV79+dpqlixezZ/duZs2dy4b163jlpZeZPG1aoXknTZxAq9ZtuG/AAD6ZMIFPJk5g2KOP+bJphXI47Hwz6T0eePZ1KkfaGPPMA5x/SRti42pnp4m0VeXh59+mXIVwNv/1O9PGv8XwkR8SFBxMrzsGU6NOQ04cP8abTw+m8QWXEBtXm5mTx9O5zx00vegyNv31OzMnj+ehF96yrqH5EIGrrmvIt1+tJePwSW67pyU7tqeQlnIsT7p2Hevx7860PGV8M3ktx49nuu3bvSuNJQt2Yoyhbce6tLq8JksW7PRqW0rC4bDz9Sfv8tBzb1A50sbrT9/P+S0vp2qO1zwqOpZhLzpf801//c6U8WN4YtRHBAUHc8Mdg6lZ1/maj37K+ZpXjatNtRp1GPjYS3w1/m3rGlcEu93O+2+OZvS7HxEVHcOD991Om7btqVWnbnaa8IqVGDLsCZYtXlDsvNO+/JSLLmlFvzvvYeoXnzL1y08ZMOQRXzfvnOC1eCoiIa7bT2wGrgb6GmNuNsas99Y587Nxw3pq1KxJXI0ahJYpQ+cuXVk4f75bmgXz59O9Z09EhAtaXEhGxmGSk5MKzbtg/nx69OoJQI9ePVnw22++bFaRdu/Ygi22OlEx1QgJCeXiyzuyYc1ytzR1GjWjXAXnqk+1GzQlPc3ZM6hUJZIadRoCEFa2HDHVa5GelgKAiHDiuPND/cSxo1SsEumrJhVbbLWKpB88zqH0Ezgchq2bE6nfICpPuotaxrF9azLHjp0qVrm7dx3EGOfI9YF9hwkPP8+j9T5b/+Z6zS+5vBPrV7u/5nUbNc9+zes0aEp66pnXvGbdnK95zezXPDauFjHVavqwJSW3dfNGqsXFUbV6HKGhoXS4+jqWL1nolqZKRASNmjYjJCSk2HmXL1nENV27AXBN1255yiw1zpHfGZWYiAzBGYQuATobY+42xmz1xrmKkpSYRGxsbPZ2dGwMiUmJ7mmSEonJkSYmJpakxKRC86alpmKzRQNgs0WTlpb327WVDqWlUDnSlr1dOcLGIdeHS35WLphDkwtb5dmfmpRA/L87qF2/CQC973qAGZPH88ID/Zjxv7F0v6W/5yt/liqEn0fG4RPZ2xkZJ6mQK3BUqFCG+o1srPtzX75l9LmlBbff05LzL6ya7/HmLaqy65/S9Zqnp6VQJTI6e7tyZFT2F4z8LJ8/m2YXXZZnf2pSAvG7zrzm/iAlORlbzJm/1ShbNCnJSWed92BaKpFRzr+jyCgb6QdL12ueLQCCUXHv9FpS7wNJwJXALDnTQAGMMeYCL503j9PfZHOS3HeQyi+NSPHyllL5zjwp4I22fdNfrFwwh0deesdt/8kTx5n09ovccNcDhJUrD8CyX2bR+877ufCydvy1YiFfjXuTIc+94dnKn6XivEIdrmnAkvn/5PfS89UXf3L0yCnKlgul7y0XkpZ6jH17D2Ufv+zyWjgchr83JebNbKUC3sf52bbxL5YvmMPwl99123/ixHEmjHmBvnc/QFnXa+4PTD7v+ILa7sm8ynO8MpsO52+SlgIHOfOj2SKJyEBgIMC4ceO4876z/9YdExtDQkJC9nZSQiLR0dFuaaJjYknMkSYxMQFbtI3MzFMF5o2IjCQ5OQmbLZrk5CQiIiLOuq6eVDkiKnsIBiA9LZlK+Qyp7dv9D1+NG8Pgp16lfPiZCRj2rCwmvfUiLa+8ihat2mbvX7XoZ264awgAF7Zuz1fjx3ixFf9NRsZJwitmL6NIePh5HMk46ZYmtmo41/dqCkDZcqHUrReJcRh2bEvh6BHnsN3xY5ns2JZM1WoVs4NR0/NjqVs/km+mrPVNY0qgcqSNg6lnegPpqSlUqpJ3eHLf7n+YPO5NHnj6NSrkes0njnmBS9tezYWXtfNJnT3FZosmOfHM32pKclJ2j+Zs8laJiCQ1JZnIKBupKclUrlK6/s6zBcAEhsKasAb4o5BHYaoD7+K879HnwCCgOZBhjNldUCZjzHhjTEtjTMuBAwcWuxGFadb8fPbs3k18fDyZp04xd85s2nd0/7lUh04dmTVjBsYY1q9bS4XwcGy26ELzdujYiZnTnXfTmDl9Bh07la51Y2vWa0xywj5Skw6QlZXJn8sX0PySy93SpKUkMumtF7ljyNNEV6uRvd8Yw1fj3iSmek06Xn+jW55KVSLZsXkd4Px2bYut7v3GlFDC/gwqVylLxUphBAUJjZrG8M929yHKiR+tzH5s25LMr/O2sWNbCiGhQYSWCQYgJDSI2nUiSEk+Cjhn6LVqU5Pp324gK8vh83YVpVa9xiQd2EeK6zX/Y/l8zm/Zxi1NWkoi4998gbsefJqYXK/5/8a+QWz1mlzV7cbcRZd6jZo0Y1/8Xg7s30dmZiYLf51Hmyvbn3XeNle245fZzp9a/jL7Ry5vW7wyfU1EPPYo5vk6i8hWEdkhIk8Vku5SEbGLSN+iyvTWbLrHXBUpA7QELgfuBSaISLoxpul/LbukQkJCePrZ57h/QH8cDge9et9A/QYN+Hqqc3rrTf360bZde5YuXky3ztcRFhbGyyNHFZoX4N4B/Xl82HCmf/ctsVWr8ebbpWumUXBwMH3ueYiPRz3pnNrdsQtVa9Rm6S+zALjymu7M++5Ljh45zDeTnEM1QcHBPDbqY3Zu3cjqJb9QtWYdXn/S+aXg+n730eyiy7h54HDndHG7ndDQMvQbMLzAOljFGMP8n7fRp18LgoKEjesOkJpyjAsuqgbA+r/2F5i3fPky9OhzPgBBQcKWTYnZs+06XduAkJAg+t7SAnBOYvh17jYvt6b4goODueneh/hw5JM4HHbadOxCtRp1WPLzTADaXtuDOd86X/OpE9/NzvPka2P5Z+tGVi3+hWo16zLqcefPF3rcch/NL27N2lVL+GbS+xw5fIiPX3uGuNr1ePDZ1y1rZ36CQ0J4cPiTPD1sCA67g+u69aB23XrM+uFbALr37ktaagpD7r2dY0ePIkHC99OmMHHKt5QvXyHfvAD97riHEc89yZwfpxMdE8v/jSxd7baCiAQDHwLX4LwT+GoRmWmM2ZxPutHAvGKVm991kVwF2nBOy25KCW8h4VpGqA1whev/lYENxph7Cst3+hQn7KXv26cvhAUHMfeveKur4XOdL4pjzKgFRScMQI8+05Ff1+U/mSKQXd2iOntSj1pdDUvUjCzvsQtTb43/vfAP8hIYPvCyQuslIm2AF40x17m2nwYwxryaK91QnJdpLgV+NMZ8W1i5xZnAMBmYBlwPDAbuAgqeouOsxHic9z/KAH4HlgNvGWMOFuN8SimlSsCT8y1yXrt3GW+MGZ9juzqwN8d2POA2LVNEqgO9gU7kvwhCHsUJRpHGmE9E5BFjzCJgkYgsKiJPTeA8YDuwz1XZ9OJUSCmlVMl4cvafK/CMLyRJfifL3TN7B3jSGGMvbt28cgsJY0xn18oLzXBeL3oUaC4iacAKY8wLxaqdUkqp0iYeqJFjOw5nXMipJTDVFYiigK4ikmWMmV5QoV67hYRxXozaKCLpOFf8PgR0A1oBGoyUUspTfDu1ezXQQETq4Bz56ge4LYJojKlz+rmIfIbzmtH0wgr1yi0kRORhnD2iK3D2rJYBK4BJwIbilKGUUqp4fPkjXWNMlog8iHOWXDAwyRizSUQGu46P/S/lFhmMRORT8vnxq+v24wWpDXwLDDPGHPgvFVNKKVU6GWNmA7Nz7cs3CBlj7i5OmcUZpvsxx/MwnDMkCv6hhvPkpe/HJ0opFagCYPmi4gzTfZdzW0S+An71Wo2UUkqVSADEov902asBzqnbSimllEcU55pRBu7XjBJwrsiglFKqNAiArlFxhunCfVERpZRS/40E+X8wKnKYTkTy3MI0v31KKaXUf1XY/YzCgHJAlIhU4cwSEBWBaj6om1JKqeLw/45RocN0g4ChOAPPH5xp7mGcy4crpZQqBQLhzrSF3c/oXeBdEXnIGPO+D+uklFLqHFOcqd0OEal8ekNEqojIA96rklJKqZIQ8dzDKsUJRgOMMemnN1z3JBrgtRoppZQqmQCIRsUJRkGSY0DSdSvZMt6rklJKqXNNcdammwd8LSJjcf74dTAw16u1UkopVWwBPYEhhydx3oL2fpwz6n4GJnizUkoppUrAt/cz8ooim2CMcRhjxhpj+hpj+gCbcN5kTymllPKI4vSMEJELgVuAm4FdwPderJNSSqkSCOhhOhFpiPN2srcAqcA0QIwxxbrbq1JKKR8J5GAEbAGWAN2NMTsARGSYT2qllFLqnFLYNaM+OG8XsUBEJojIVQTECkhKKRVYAuBnRgUHI2PMD8aYm4HGwEJgGBAjIh+LyLU+qp9SSqkiiIjHHlYpzmy6o8aYycaYbkAcsBZ4ytsVU0opde4QY0zRqaxRaiumlFIe4LFuyLgZGz32eTmoZ3NLukfFmtptlRN2h9VVsERYcBC7U49aXQ2fqxVZnnc/W2N1NSzxyN0tz8m2P3J3S7YnHLa6GpZoEFvRY2UFwtTuAPjdrlJKKX9XqntGSimliiEAekYajJRSys8FQCzSYTqllFLW056RUkr5uwDoGmkwUkopPydB/h+MdJhOKaWU5bRnpJRSfi4ARuk0GCmllN8LgGikw3RKKaUspz0jpZTyc4GwHJAGI6WU8nf+H4t0mE4ppZT1tGeklFJ+LhB+Z6TBSCml/Jz/hyIdplNKKVVCItJZRLaKyA4RyXPnbxHpKSLrRWStiKwRkSuLKlN7Rkop5ed8OZtORIKBD4FrgHhgtYjMNMZszpHsN2CmMcaIyAXA10DjwsrVYKSUUn7OxzO7WwE7jDE7neeWqUBPIDsYGWOO5EhfHijytug6TKeUUiqbiAx0Da2dfgzMlaQ6sDfHdrxrX+5yeovIFuAn4N6izqs9I6WU8nOe7BkZY8YD4ws7XX7Z8innB+AHEWkHjACuLuy8GoyUUsrPiW/n08UDNXJsxwH7C0psjFksIvVEJMoYk1JQOh2mU0opVRKrgQYiUkdEygD9gJk5E4hIfXHNqhCRi4EyQGphhWrPSCml/JwvJzAYY7JE5EFgHhAMTDLGbBKRwa7jY4E+wJ0ikgkcB242xhQ6iUGDkVJK+Tlfr5NqjJkNzM61b2yO56OB0SUpU4fplFJKWe6c6BktW7KE0a+OwmF30LtvX+4bMMDtuDGG0aNGsXTxYsLKhjFi1CiaNG1WaN5D6ek88ehw9u/bR7Xq1XnjrbepWKmSz9tWmNUrl/HxO2/isNvp3L03/e68x+34nn93MWbki+zYtoW7Bw3hxlvvLHbeb6Z8wYQP3uGb2b9RqXIVn7SnJGpVr0j7VjURgU3bU1izIcHteN0alWlzUTUM4HAYFq/ay/4k508jLmoaQ7MGURgg9eAxfln2L3a7oUv7ulSpFAbAeWWCOXnKzpSZmylNvNHuyy6sRvMGURw/mQXA8j/28e++Qz5uWdH++H05498fg8Ph4Nrre3LjbXe7Hd+7+1/eee1l/tm+hTv7388N/e4AIDkpgbdGvsjBtFSCgoTruvemZ99b3PJ+P/VLJn38HpNn/EKlypV91KLi01tIFEBEMjgz1e/0v5Jxna+MMcZnQdButzPqlRGMm/gJMTEx3HrzTXTo2JF69etnp1m6eDF7du9m1ty5bFi/jldeepnJ06YVmnfSxAm0at2G+wYM4JMJE/hk4gSGPfqYr5pVJLvdzgdvjua1dz8iKjqGh+67nTZt21OrTt3sNOEVK/HAsCdYvnhBifImJSbw56qVRMfE+rRNxSUCHS6ryQ8/b+PIsUz6dWvCzj3ppB06kZ1m74HD7NybDkBUlbJ06VCXL3/YRPlyobRoEs2X0zdmB6CGdSL4e0cqcxbtzM7ftmUcJzPtvm5aobzVboC/Nify56ZEK5pVLHa7nY/feZ1XxnxApC2GYYPu4rIr2lGzds73e0UGPfwoK5cucssbHBzCfUOGUr9hY44dO8rQAXdyUcvLsvMmJyXw15pV2Erp+x10bboCGWPCjTEVXY9woBowEkgA3vXGOQuyccN6atSsSVyNGoSWKUPnLl1ZOH++W5oF8+fTvWdPRIQLWlxIRsZhkpOTCs27YP58evTqCUCPXj1Z8NtvvmxWkbZu3ki1uDiqVo8jNDSU9ldfx/IlC93SVImIoFHTZgSHhJQo79h3x9B/yNBS+20sJqo8hzJOcvjIKRwOw7ZdadStWdktTWaWI/t5SEiQ268kgoKEkOAgRCA0JIijxzLznKNBnQi27UzzVhP+E1+0u7Ta9vcmqlavQWw153u2Xadr8gSdylUiaNgk7/s9IjKK+g2dK9WUK1eeGrVqk5qcnH18wgdvc8/gh0rt+x2cPSNPPazi1R6KiFQGhgJ3AlOAS40xhU7v87SkxCRiY898o4mOjWHD+vXuaZISicmRJiYmlqTEpELzpqWmYrNFA2CzRZOWVro+mFKSk92+ydls0WzZvPGs865YsogoWzT1GjT0bIU9qEK5MmQcPZW9feToKWJtFfKkq1ezMpdfUp1yYaHM+HU7AEePZfLnxgTuvfECsuwO9uw7zJ79h93yVYupwLHjmaRnnPRuQ0rIm+1u0SSaJvUiSUw9xpLVezl5qnT1ClNTkrFFx2RvR9li2Pp38d7vOSUe2M/O7Vtp5Bqm/33ZIiKjbNStX3rf74HCKz0jEYkSkVeBP4Es4CJjzHNFBaKcy1CMH1/YD4CLL7/ZhHl+IJZfGpHi5S218m/T2eQ9ceI4Uz7/hLsGDD7LuvmeyadN/+xJ58sfNjFr/g7aXORczeS8MsHUrVmZz77dwCfT1hMaGkSjuhFu+RrViWDrrtL15aMgnmj3hi1JfPbdBibP3MzRY5m0vbRGnjIt54G/1ePHjjHq+ScZ8NBwypWvwIkTJ5j25afcfm/pf7+LeO5hFW/1jHYDycCnwDHgvpwfhMaYt/LLlGsZCnPC7sgvWYnExMaQkHDmIm5SQiLR0dFuaaJjYknMkSYxMQFbtI3MzFMF5o2IjCQ5OQmbLZrk5CQiItw/sKwWZYsmOfFM3ZOTk4iIsp1V3gP74knYv4/Bd/bL3v/APbfx/sQviIiM8mwDzsKRY6cIL18me7tC+TKFDjntTzxCpfDzCDsvhLjYcA5nnMy+WL9jdzrVoiuw1TUkJwL1a1Xhq1mla+ICeK/dx05kZefZuD2ZHlc18F4j/qNIWzTJSWeuaaUkJxIRVfz3ZFZWFqOef5IOV3fm8nadAEjYF0/igf08dN+trjKTGDrgdt4a+xlVStH7HfSaUWHewBmIAMJzPfKOG3hRs+bns2f3buLj48k8dYq5c2bTvmNHtzQdOnVk1owZGGNYv24tFcLDsdmiC83boWMnZk6fAcDM6TPo2KmTL5tVpEZNmrEvfi8H9u8jMzOTRb/Oo82V7c8qb516Dfhm9m98+f1PfPn9T9hs0Xz06eRSFYgAElOOUrliGBUrlCEoSGhYJyL7ov1plcLPy35uiyhHcJBw4mQWGa6hrZBg559GjarhpKWfmQBQs1pF0g6d4EgpvJ7irXaXKxuanad+zSqkph/3fmNKqGHjpuyP30PCAed7dvH8X7jsinbFymuM4d3RI6hRqza9b74te3/tevWZPONnJk2byaRpM4myRfPOhP+VukAUKLzSMzLGvFjQMREZ6o1zFiQkJISnn32O+wf0x+Fw0Kv3DdRv0ICvp04F4KZ+/Wjbrj1LFy+mW+frCAsL4+WRowrNC3DvgP48Pmw407/7ltiq1Xjz7bd92awiBYeE8ODwJ3lm2BAcdgfXdetB7br1+PGHbwHo1rsvaakpPHjv7Rw7ehQJEn6YNoUJU76lfPkK+eb1F8bAwpV76HVNQ0Rg845U0tJPcH4jZ89ww9Zk6teqQpN6kTiMISvLkT1TLjHlKDt2H+SWHk1wOCA57Rgbt525mN2wTgTbSukQnbfafWXLOGwRZcHA4SOn+G3FbsvaWJDgkBAGD32C5x97GIfDzjVde1CrTj1mz/gOgK49+3AwNYWhg+7i2NGjBAUJM76dysefT2PXPztY8PNsatetn90LunPAEC5tfYWVTSqR0jy5orikiBUaPH9CkT3GmJrFSOqRYTp/FBYcxO7Uo1ZXw+dqRZbn3c/WWF0NSzxyd8tzsu2P3N2S7QmHi04YgBrEVvRYBPluxb8e+yDv06a2JZHNihUY/D+EK6WU8igrVmDwbVdMKaUCXCAM0/liBQa3Q0BZb5xTKaXOVf4firw3gSHcG+UqpZQKTOfEQqlKKRXIAmCUToORUkr5u0C4ZqT3M1JKKWU57RkppZSf8/9+kQYjpZTyewEwSqfDdEoppaynPSOllPJzgTCBQYORUkr5uQCIRTpMp5RSynraM1JKKT/nP3egLpgGI6WU8nM6TKeUUkp5gPaMlFLKzwVCz0iDkVJK+bmgALhmpMN0SimlLKc9I6WU8nM6TKeUUspygRCMdJhOKaWU5bRnpJRSfk7XplNKKWU5/w9FOkynlFKqFNCekVJK+blAGKYTY4zVdShIqa2YUkp5gMciyMKNBzz2edmheVVLIlup7hmdsDusroIlwoKDOJppt7oaPlc+NJg/d6VaXQ1LXFwnkhmr9lhdDZ/r2aomb/SZYnU1LPH4d7daXYX/TEQ6A+8CwcBEY8xruY7fBjzp2jwC3G+MWVdYmaU6GCmllCqaL0fpRCQY+BC4BogHVovITGPM5hzJdgHtjTEHRaQLMB64rLByNRgppZSf8/H9jFoBO4wxOwFEZCrQE8gORsaY5TnSrwTiiipUZ9MppZTKJiIDRWRNjsfAXEmqA3tzbMe79hXkPmBOUefVnpFSSvk5Tw7TGWPG4xxWK/B0+WXLN6FIR5zB6MqizqvBSCml/JyPp3bHAzVybMcB+3MnEpELgIlAF2NMkTOTdJhOKaVUSawGGohIHREpA/QDZuZMICI1ge+BO4wx24pTqPaMlFLKz/myY2SMyRKRB4F5OKd2TzLGbBKRwa7jY4HngUjgI1evLcsY07KwcjUYKaWUn/P1CgzGmNnA7Fz7xuZ43h/oX5IydZhOKaWU5bRnpJRSfs7/V6bTYKSUUn4vANZJ1WE6pZRS1tOekVJK+blAuIWEBiOllPJzARCLdJhOKaWU9bRnpJRSfs7Hq3Z7hQYjpZTyczpMp5RSSnmA9oyUUsrP6Ww6pZRSlguAWKTBSCml/F0gBCO9ZqSUUspy2jNSSik/p1O7lVJKWU6H6ZRSSikPOCd6RsuWLGH0q6Nw2B307tuX+wYMcDtujGH0qFEsXbyYsLJhjBg1iiZNmxWa91B6Ok88Opz9+/ZRrXp13njrbSpWquTzthVm2dIlvPnaq9jtdnr36cs9/fO2+41XR7F0yWLCwsry0shRNGnaFIAXn3uWJYsXERERwTfTZ+Yp+4tPJ/HOmDf5bckyqlSp4pP2lMTaNSv54uN3cDjsdOzcnZ433+l2fN/efxk3ZiS7/tnGzXcNolvfW7OPHT2Swfh3XiX+350gwqBhz9Cw6fmsXDyfb//3Cfv3/suIdydSr2ETXzerSFvXr2bGlx9hHA5adehCx+793I7/uew3Fv40DYDzzitL77sfplqtegC8Oux2zgsriwQFERQczCMvfwTAj1+N5++/VhIcEkJkdDVuGvAYZctX8G3DiqH2hVW56t5LkCBh/W//sOqHzW7HazSLpveT7TiUdBSAbb/vZcU3G6lSLZwew6/MTlcppgLLpq7nj5+2ckW/C2jQqjrGAccOnWD2Bys5evC4T9tVHDq1uwAicmdhx40xX3jjvPmx2+2MemUE4yZ+QkxMDLfefBMdOnakXv362WmWLl7Mnt27mTV3LhvWr+OVl15m8rRpheadNHECrVq34b4BA/hkwgQ+mTiBYY8+5qtmFclutzP6lVf4aMJEYmJjuP3mm2nfsSN1651p97Ili9mzZzczZs9lw/r1vDriJb74yvlB1b1Xb26+9Taef+apPGUnHDjAyhUriK1a1WftKQmH3c6nH77JM6PeJTIqmmcfvo9LWrclrlad7DQVwity1/3DWLNicZ78n499hxaXtGbYc6PIyszk5MkTANSoXZfh/zeKie+97rO2lITDYeeHz99nwJOjqRQRxfvPP0jTi9sQU71WdpoIWyyDnx1DufLhbFm3iu8mvcNDL72ffXzQM29SPtz9S1XD5hfT5ab7CA4OZvbUCSyY9RVd+7l/sbGaBAnXDGjJ1y/PJyP1OHeMvo5/VseTGn/YLV3838l8/+oit30H92fw+WNzssu5f3wvtq/aC8DqGZtZNnU9ABd3bcjlNzbnl/GrfdCikgmAWOS1YbpL83m0AkYAk7x0znxt3LCeGjVrElejBqFlytC5S1cWzp/vlmbB/Pl079kTEeGCFheSkXGY5OSkQvMumD+fHr16AtCjV08W/PabL5tVpI0bNhB3uu6hZbiuS5c87V64YD7depxudwsyMjJITk4G4JKWLalUQE9vzOujGTr80VL7bWzH1s3EVo0jpmp1QkJDadP+atasWOKWplLlCOo1akpwsPv3sWNHj7Jlw1o6du4OQEhoKOUrhANQvWZtqtWoRWm195+tRMVUIzK6KiEhobRo3YFNfyx3S1O7YTPKlXe2p2b9Jhw6mFxkuQ3Pb0lwcHB2nvS0FM9X/ixVrR/JwYQjHEo8iiPLwZalu6l/aVyJy6l1fgzpiUc4nHwMgFPHs7KPhZ53TgwkWcYr/7rGmIdOPxfnJ9ZtwJPASmCkN85ZkKTEJGJjY7O3o2Nj2LB+vXuapERicqSJiYklKTGp0LxpqanYbNEA2GzRpKWlebMZJZaclOhe95hYNm7I1e7EJLd2R8fEkJyYiM1mK7DcRQvmEx0dTcPGjT1faQ85mJpMpC0mezsyysaOrZsLyXFGUsI+KlaqzNgxI9m9azt16zfmzvuHEhZW1lvV9ZhDB1OoFHHmtasUEcXef7YUmH71wrk0uuDSHHuECaOfQkS4rOP1tO50fd48i+bRonV7T1bbIypElCUj5Wj2dkbaMao2iMqTrlqjKO4a04UjacdZ+MVfpO495Ha88RW1+Hvpbrd9V956Ac3a1+HksUymvVC6vnSeFgiz6bw2gUFEQkSkP7AZuBroa4y52RizvoisHmWMyVu33C9cfmlEipe3lMq37rmrnk+awvr7x48f55Px4xj84EMFpikN8mtWcccx7HY7u3Zs45puvXntw885LyyMmdO+9GwFvaUEr+eOzWtZvXgOXW8+M9z2wPNvM/SVj7nvsZGs+HUmO7e4/6n+NmMyQcHBXHT5VR6ttkfk18xc/x6JO9MYN3gGnz86hz/nbKP3k+3cjgeFBFHv0upsXb7Hbf/SKesZN2gGfy/+l4u7NPR0zT1CxHMPq3glGInIEJxB6BKgszHmbmPM1mLkGygia0Rkzfjx4z1Sl5jYGBISErK3kxISiY6OdksTHRNLYo40iYkJ2KJtheaNiIwkOTkJgOTkJCIiIjxSX0+Jjol1r3tiQnZPLjtNbIxbu5MSE7Hl+rfJKX7vXvbt20e/Pr25/tqrSUpM5LYb+5CSUvRQjy9FRNlITU7M3k5NSaZKRN5vyfmJjIomIspG/cbOCSyXte3Irh1FvnVLhUoRNg6lnXktDqWlULFyZJ50B/bs5NtP3uKuoS9TPrzimfxVnP9GFSpVoVnLK9j7z5l2r1nyM3+v/Z1b7n+qVA7PHkk9TnhU+ezt8IhyHElzn2hw6ngWmSecw267/txPULBQNvy87ON1L6pK0s6DHDt0It9z/L30Xxq0ruGF2ivwXs/ofaAicCUwS0TWux4bRKTAnpExZrwxpqUxpuXAgQM9UpFmzc9nz+7dxMfHk3nqFHPnzKZ9x45uaTp06sisGTMwxrB+3VoqhIdjs0UXmrdDx07MnD4DgJnTZ9CxUyeP1NdTmjVvzt49u9kXH09m5inmzZmTp93tO3Tix5mn272OChXCCx2ia9CwIb8tXspPP//KTz//SnRMDJO/+Y6oqILzWKFeoyYk7I8nKWE/WZmZrFj0K5e0vrLojEDliEgibTHs3+scqtn41xriatYpIlfpEFe3ESkJ+0hLOkBWVibrVi6k6cVt3NIcTEnii3dfot+gJ7FVPXNN5dSJ45w4fiz7+fYNfxBbozbgnKG38Mdp3D3sZcqcF+az9pTEgR2pVKkaTqXo8gSFBNH4ylrsWLPPLU35ymfqHls/EhHheMbJ7H2Nr6ydZ4iuctXw7Of1WsaRts99QkRpESTisYdVvHVFrtT89YaEhPD0s89x/4D+OBwOevW+gfoNGvD11KkA3NSvH23btWfp4sV063wdYWFhvDxyVKF5Ae4d0J/Hhw1n+nffElu1Gm++/bZlbcxPSEgITz7zLEMGDcBhd9Cjd2/q1W/At9Oc7e57cz+ubNeOpUsW07NLZ8LKhvHiiDOX855+/DH+WL2K9PR0Ol/VkcEPPEivPn2sak6JBAeHcPcDw3n12WE4HHY6XNuNGrXr8stPPwBwzfW9SU9L5dmH7+X4saOIBDFn+jTeGDeFcuXLc/cDw/jg9ZfIyswkpmo1Bg1/FoDVyxbx2cdvcfhQOq8//xi16zbg6VHvWNhSd8HBwfS880EmvvE0DoeDS9tdR2xcbVb8NguANld159fpX3LsyGF++Pw9gOwp3BmH0/ninRcB56y8C9t0zL6eNP3zD8jKymTC6CcB5ySGPvcM9Xn7CmMchl8nrqHv/3UkKEjYMH8nqXsP0eJa5+zRdT/voGGbmlx4XX0cdkPWKTuz3l6WnT+kTDC1W8Ty87hVbuW2v70FVapVBGM4lHyMX3IdLy1KYWe1xCS/awteO5lIMNDPGDO5GMnNCbvD21UqlcKCgziaabe6Gj5XPjSYP3elWl0NS1xcJ5IZq/YUnTDA9GxVkzf6TLG6GpZ4/LtbPRZCtuw/5LEP8sbVKlkS2rx1zaiiiDwtIh+IyLXi9BCwE7jJG+dUSqlzVSBMYPDWMN2XwEFgBdAfeBwoA/Q0xqz10jmVUuqc5C+zfAvjrWBU1xhzPoCITARSgJrGmAwvnU8ppZQf81Ywyjz9xBhjF5FdGoiUUso7AmECg7eCUQsROT0HUoCyrm0BjDGmYsFZlVJKlURp/O1XSXlrOaBgb5SrlFIqMOnKf0op5ecCoGOkwUgppfxdIAzT6Z1elVJKWU57Rkop5ef8v1+kwUgppfyeDtMppZQ654hIZxHZKiI7ROSpfI43FpEVInJSRB4rTpnaM1JKKT/ny46Ra8HrD4FrgHhgtYjMNMbkvJ1yGvAw0Ku45WrPSCml/Jx48FEMrYAdxpidxphTwFSgZ84ExpgkY8xqcqzGUxQNRkoppbLlvOO265H7TqfVgb05tuNd+86KDtMppZS/8+A4nTFmPDC+sLPll+1sz6vBSCml/JyP59LFAzVybMcB+8+2UB2mU0opVRKrgQYiUkdEygD9gJlnW6j2jJRSys/5cjadMSZLRB4E5gHBwCRjzCYRGew6PlZEYoE1QEXAISJDgabGmMMFlavBSCml/Jyvf/JqjJkNzM61b2yO5wk4h++KTYfplFJKWU57Rkop5e8CYDkgDUZKKeXn/D8U6TCdUkqpUkB7Rkop5ecCYJROg5FSSvk//49GOkynlFLKcmLMWS8pFHBEZKBrfaZzzrna9nO13XDutj2Q2p1w+ITHPshjK4ZZ0s3SnlH+cq9Sey45V9t+rrYbzt22B0y7fXwLCa/QYKSUUspyOoFBKaX8nM6mC1wBMY78H52rbT9X2w3nbtsDqN3+H410AoNSSvm5pIyTHvsgjw4/z5LIpj0jpZTyczpMp5RSynIBEIt0Nl1OImIXkbUislFEvhGRclbXyZtE5Eg++14UkX05/h16WFE3TxORt103+Dq9PU9EJubYHiMiw0XEiMhDOfZ/ICJ3+7a23lHI631MRKILS+fPcv1dzxKRyq79tQP59fY3GozcHTfGXGiMaQ6cAgZbXSGLvG2MuRC4EZgkIoHwPlkOXA7gak8U0CzH8cuBZUAS8IjrdsrnihTgUasr4UU5/67TgCE5jgXG6x0APzQKhA8Zb1kC1Le6ElYyxvwNZOH84PZ3y3AFI5xBaCOQISJVROQ8oAlwEEgGfgPusqSW1pgE3CwiEVZXxAdWANVzbAfE6y0e/M8qGozyISIhQBdgg9V1sZKIXAY4cP7B+jVjzH4gS0Rq4gxKK4DfgTZAS2A9zt4wwGvAoyISbEVdLXAEZ0B6xOqKeJPr9bwKmJnr0Ln2epdKOoHBXVkRWet6vgT4xMK6WGmYiNwOZAA3m8CZ/3+6d3Q58BbOb8iXA4dwDuMBYIzZJSKrgFutqKRF3gPWisgYqyviBaf/rmsDfwC/5DwYCK+3zqYLPMdd10rOdW8bY960uhJecPq60fk4h+n24rxWchhnzyCnUcC3wGJfVtAqxph0EZkCPGB1XbzguDHmQhGpBPyI85rRe7nS+PXrHQCxSIfp1DllGdANSDPG2I0xaUBlnEN1K3ImNMZsATa70p8r3gIGEaBfUo0xh4CHgcdEJDTXMf9+vUU897CIBqNzWzkRic/xGG51hbxsA87JGCtz7TtkjEnJJ/1IIM4XFfORQl9v17/BD8B51lTP+4wxfwHrgH75HA6019uv6HJASinl59KPZ3rsg7xy2VBdDkgppVTJBcIEBh2mU0opZTntGSmllJ8LgI6RBiOllPJ7ATBOp8N0SimlLKfBSFnCkyuki8hnItLX9XyiiDQtJG0HEbm8oOOF5PtXRPKs0VfQ/lxpSrQKtmsl7cdKWkd17gqAdVI1GCnLFLpC+n9dJ8wY098Ys7mQJB04s2CqUgEhAH7zqsFIlQpLgPquXssC17I0G0QkWETeEJHVIrJeRAYBiNMHIrJZRH4Cct6LZ6GItHQ97ywif4rIOhH5TURq4wx6w1y9srYiYhOR71znWC0iV7jyRorIzyLyl4iMoxhfGkVkuoj8ISKbRGRgrmNjXHX5TURsrn31RGSuK88SEWnskX9NpfyQTmBQlsqxQvpc165WQHPX4pUDca6OcKnrNg/LRORn4CKgEc415mJwLuMyKVe5NmAC0M5VVoQxJk1ExgJHTq+95wp8bxtjlrpW9J6H83YSLwBLjTEvi8j1gFtwKcC9rnOUBVaLyHfGmFSgPPCnMeZREXneVfaDwHhgsDFmu2uF9I+ATv/hn1Gd8/x/AoMGI2WV/FZIvxxYZYzZ5dp/LXDB6etBQCWgAdAO+MoYYwf2i8j8fMpvDSw+XZZrHbr8XA00lTPjExVFJNx1jhtceX8SkYPFaNPDItLb9byGq66pOG/DMc21/3/A9yJSwdXeb3KcO2CX4VHeFQCT6TQYKcvkWSHd9aF8NOcu4CFjzLxc6boCRS1/IsVIA86h6jbGmOP51KXYS6yISAecga2NMeaYiCwEwgpIblznTddV4pVy0mtGqjSbB9x/eoVlEWkoIuVxLvPfz3VNqSrQMZ+8K4D2IlLHlff0XUwzgPAc6X7GOWSGK92FrqeLgdtc+7oAVYqoayXgoCsQNcbZMzstCDjdu7sV5/DfYWCXiNzoOoeISIsizqFUvnQ2nVLeNRHn9aA/RWQjMA5nb/4HYDvOFbc/BhblzmiMScZ5ned7EVnHmWGyWUDv0xMYcN5SoKVrgsRmzszqewloJyJ/4hwu3FNEXecCISKyHhiB+8rgR4FmIvIHzmtCL7v23wbc56rfJqBnMf5NlMojEGbT6ardSinl545n2T32QV42JNiSkKQ9I6WU8nu+Hahz/Wxiq4jsEJGn8jkuIvKe6/h6Ebm4qDJ1AoNSSvk5Xw6vuX6Q/iFwDRCP82cMM3P92LwLztmkDYDLcA6nX1ZYudozUkopVRKtgB3GmJ3GmFPAVPJe7+wJfGGcVgKVXZONCqQ9I6WU8nNhwUEe6xu5fmye80fe440x43NsVwf25tiOJ2+vJ7801YEDBZ1Xg5FSSqlsrsAzvpAk+QW+3BMoipPGjQ7TKaWUKol4nCuMnBYH7P8PadxoMFJKKVUSq4EGIlJHRMoA/YCZudLMBO50zaprjXONyQKH6ECH6ZRSSpWAMSZLRB7EuUJKMDDJGLNJRAa7jo8FZgNdgR3AMeCeosrVH70qpZSynA7TKaWUspwGI6WUUpbTYKSUUspyGoyUUkpZToORUkopy2kwUkopZTkNRkoppSz3/w1V9+97sIC0AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 504x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "gnn_model = GNN7L_GAT(data_with_nedbit).to(device)\n",
    "pred = train(gnn_model, data_with_nedbit, epochs, lr, weight_decay, classes, 'GAT')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GraphCONV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9d81132e8dc4c33a88cc7ab1fffae84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40001 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch: 0000] train loss: 104.5635, train acc: 0.1295, val loss: 68.5135, val acc: 0.0560  (best train acc: 0.1295, best val acc: 0.0560, best train loss: 104.5635  @ epoch 0 )\n",
      "[Epoch: 0020] train loss: 5.3364, train acc: 0.2971, val loss: 1.9966, val acc: 0.4233  (best train acc: 0.2971, best val acc: 0.4233, best train loss: 5.3364  @ epoch 20 )\n",
      "[Epoch: 0040] train loss: 1.4747, train acc: 0.4511, val loss: 1.1729, val acc: 0.5956  (best train acc: 0.4511, best val acc: 0.5956, best train loss: 1.4747  @ epoch 40 )\n",
      "[Epoch: 0060] train loss: 1.0889, train acc: 0.5657, val loss: 0.8806, val acc: 0.7056  (best train acc: 0.5695, best val acc: 0.7056, best train loss: 1.0889  @ epoch 60 )\n",
      "[Epoch: 0080] train loss: 0.9302, train acc: 0.6601, val loss: 0.7392, val acc: 0.7936  (best train acc: 0.6601, best val acc: 0.7936, best train loss: 0.9213  @ epoch 79 )\n",
      "[Epoch: 0100] train loss: 0.8352, train acc: 0.6979, val loss: 0.6636, val acc: 0.8037  (best train acc: 0.6979, best val acc: 0.8040, best train loss: 0.8352  @ epoch 100 )\n",
      "[Epoch: 0120] train loss: 0.7980, train acc: 0.7009, val loss: 0.6131, val acc: 0.8148  (best train acc: 0.7133, best val acc: 0.8148, best train loss: 0.7979  @ epoch 116 )\n",
      "[Epoch: 0140] train loss: 0.7444, train acc: 0.7345, val loss: 0.5652, val acc: 0.8307  (best train acc: 0.7345, best val acc: 0.8317, best train loss: 0.7401  @ epoch 139 )\n",
      "[Epoch: 0160] train loss: 0.7022, train acc: 0.7465, val loss: 0.5218, val acc: 0.8452  (best train acc: 0.7465, best val acc: 0.8452, best train loss: 0.6980  @ epoch 159 )\n",
      "[Epoch: 0180] train loss: 0.6777, train acc: 0.7587, val loss: 0.4977, val acc: 0.8486  (best train acc: 0.7594, best val acc: 0.8523, best train loss: 0.6741  @ epoch 178 )\n",
      "[Epoch: 0200] train loss: 0.6525, train acc: 0.7711, val loss: 0.4626, val acc: 0.8698  (best train acc: 0.7729, best val acc: 0.8698, best train loss: 0.6423  @ epoch 198 )\n",
      "[Epoch: 0220] train loss: 0.6310, train acc: 0.7775, val loss: 0.4421, val acc: 0.8749  (best train acc: 0.7833, best val acc: 0.8776, best train loss: 0.6243  @ epoch 216 )\n",
      "[Epoch: 0240] train loss: 0.6140, train acc: 0.7901, val loss: 0.4238, val acc: 0.8870  (best train acc: 0.7916, best val acc: 0.8870, best train loss: 0.6096  @ epoch 232 )\n",
      "[Epoch: 0260] train loss: 0.5824, train acc: 0.8010, val loss: 0.4057, val acc: 0.8907  (best train acc: 0.8010, best val acc: 0.8907, best train loss: 0.5824  @ epoch 260 )\n",
      "[Epoch: 0280] train loss: 0.5786, train acc: 0.8073, val loss: 0.3809, val acc: 0.8897  (best train acc: 0.8151, best val acc: 0.8931, best train loss: 0.5635  @ epoch 279 )\n",
      "[Epoch: 0300] train loss: 0.5517, train acc: 0.8123, val loss: 0.3664, val acc: 0.8894  (best train acc: 0.8182, best val acc: 0.8954, best train loss: 0.5373  @ epoch 289 )\n",
      "[Epoch: 0320] train loss: 0.5357, train acc: 0.8169, val loss: 0.3526, val acc: 0.8924  (best train acc: 0.8232, best val acc: 0.8968, best train loss: 0.5203  @ epoch 315 )\n",
      "[Epoch: 0340] train loss: 0.5110, train acc: 0.8192, val loss: 0.3431, val acc: 0.8948  (best train acc: 0.8279, best val acc: 0.8981, best train loss: 0.5068  @ epoch 336 )\n",
      "[Epoch: 0360] train loss: 0.4995, train acc: 0.8316, val loss: 0.3369, val acc: 0.8958  (best train acc: 0.8316, best val acc: 0.8981, best train loss: 0.4972  @ epoch 345 )\n",
      "[Epoch: 0380] train loss: 0.5049, train acc: 0.8224, val loss: 0.3319, val acc: 0.8998  (best train acc: 0.8334, best val acc: 0.9005, best train loss: 0.4883  @ epoch 375 )\n",
      "[Epoch: 0400] train loss: 0.4897, train acc: 0.8323, val loss: 0.3272, val acc: 0.8965  (best train acc: 0.8370, best val acc: 0.9015, best train loss: 0.4825  @ epoch 399 )\n",
      "[Epoch: 0420] train loss: 0.4835, train acc: 0.8330, val loss: 0.3209, val acc: 0.8995  (best train acc: 0.8391, best val acc: 0.9029, best train loss: 0.4789  @ epoch 410 )\n",
      "[Epoch: 0440] train loss: 0.4929, train acc: 0.8299, val loss: 0.3182, val acc: 0.9035  (best train acc: 0.8391, best val acc: 0.9035, best train loss: 0.4710  @ epoch 436 )\n",
      "[Epoch: 0460] train loss: 0.4663, train acc: 0.8404, val loss: 0.3094, val acc: 0.9052  (best train acc: 0.8436, best val acc: 0.9069, best train loss: 0.4547  @ epoch 456 )\n",
      "[Epoch: 0480] train loss: 0.4662, train acc: 0.8398, val loss: 0.3125, val acc: 0.9029  (best train acc: 0.8436, best val acc: 0.9076, best train loss: 0.4547  @ epoch 456 )\n",
      "[Epoch: 0500] train loss: 0.4689, train acc: 0.8398, val loss: 0.3038, val acc: 0.9069  (best train acc: 0.8436, best val acc: 0.9083, best train loss: 0.4547  @ epoch 456 )\n",
      "[Epoch: 0520] train loss: 0.4507, train acc: 0.8437, val loss: 0.3010, val acc: 0.9059  (best train acc: 0.8468, best val acc: 0.9086, best train loss: 0.4504  @ epoch 515 )\n",
      "[Epoch: 0540] train loss: 0.4612, train acc: 0.8444, val loss: 0.3023, val acc: 0.9066  (best train acc: 0.8477, best val acc: 0.9106, best train loss: 0.4423  @ epoch 534 )\n",
      "[Epoch: 0560] train loss: 0.4526, train acc: 0.8433, val loss: 0.2952, val acc: 0.9093  (best train acc: 0.8506, best val acc: 0.9106, best train loss: 0.4423  @ epoch 534 )\n",
      "[Epoch: 0580] train loss: 0.4547, train acc: 0.8389, val loss: 0.2923, val acc: 0.9076  (best train acc: 0.8506, best val acc: 0.9106, best train loss: 0.4403  @ epoch 573 )\n",
      "[Epoch: 0600] train loss: 0.4494, train acc: 0.8386, val loss: 0.2869, val acc: 0.9110  (best train acc: 0.8531, best val acc: 0.9120, best train loss: 0.4333  @ epoch 586 )\n",
      "[Epoch: 0620] train loss: 0.4211, train acc: 0.8528, val loss: 0.2856, val acc: 0.9089  (best train acc: 0.8537, best val acc: 0.9130, best train loss: 0.4211  @ epoch 620 )\n",
      "[Epoch: 0640] train loss: 0.4320, train acc: 0.8503, val loss: 0.2812, val acc: 0.9123  (best train acc: 0.8539, best val acc: 0.9130, best train loss: 0.4211  @ epoch 620 )\n",
      "[Epoch: 0660] train loss: 0.4349, train acc: 0.8456, val loss: 0.2991, val acc: 0.8954  (best train acc: 0.8539, best val acc: 0.9130, best train loss: 0.4211  @ epoch 620 )\n",
      "[Epoch: 0680] train loss: 0.4307, train acc: 0.8448, val loss: 0.2869, val acc: 0.9093  (best train acc: 0.8539, best val acc: 0.9130, best train loss: 0.4211  @ epoch 620 )\n",
      "[Epoch: 0700] train loss: 0.4374, train acc: 0.8428, val loss: 0.2809, val acc: 0.9059  (best train acc: 0.8539, best val acc: 0.9140, best train loss: 0.4189  @ epoch 691 )\n",
      "[Epoch: 0720] train loss: 0.4281, train acc: 0.8472, val loss: 0.2724, val acc: 0.9120  (best train acc: 0.8545, best val acc: 0.9143, best train loss: 0.4159  @ epoch 703 )\n",
      "[Epoch: 0740] train loss: 0.4204, train acc: 0.8493, val loss: 0.2719, val acc: 0.9103  (best train acc: 0.8545, best val acc: 0.9143, best train loss: 0.4159  @ epoch 703 )\n",
      "[Epoch: 0760] train loss: 0.4129, train acc: 0.8560, val loss: 0.2666, val acc: 0.9133  (best train acc: 0.8597, best val acc: 0.9150, best train loss: 0.4078  @ epoch 753 )\n",
      "[Epoch: 0780] train loss: 0.4143, train acc: 0.8592, val loss: 0.2661, val acc: 0.9113  (best train acc: 0.8597, best val acc: 0.9153, best train loss: 0.4069  @ epoch 776 )\n",
      "[Epoch: 0800] train loss: 0.4094, train acc: 0.8557, val loss: 0.2645, val acc: 0.9140  (best train acc: 0.8602, best val acc: 0.9153, best train loss: 0.3980  @ epoch 792 )\n",
      "[Epoch: 0820] train loss: 0.4172, train acc: 0.8565, val loss: 0.2785, val acc: 0.9039  (best train acc: 0.8602, best val acc: 0.9160, best train loss: 0.3971  @ epoch 806 )\n",
      "[Epoch: 0840] train loss: 0.4071, train acc: 0.8548, val loss: 0.2624, val acc: 0.9120  (best train acc: 0.8602, best val acc: 0.9160, best train loss: 0.3971  @ epoch 806 )\n",
      "[Epoch: 0860] train loss: 0.3963, train acc: 0.8590, val loss: 0.2583, val acc: 0.9150  (best train acc: 0.8628, best val acc: 0.9170, best train loss: 0.3928  @ epoch 855 )\n",
      "[Epoch: 0880] train loss: 0.4106, train acc: 0.8542, val loss: 0.2562, val acc: 0.9126  (best train acc: 0.8628, best val acc: 0.9170, best train loss: 0.3928  @ epoch 855 )\n",
      "[Epoch: 0900] train loss: 0.3997, train acc: 0.8589, val loss: 0.2737, val acc: 0.9049  (best train acc: 0.8628, best val acc: 0.9174, best train loss: 0.3928  @ epoch 855 )\n",
      "[Epoch: 0920] train loss: 0.4144, train acc: 0.8492, val loss: 0.2640, val acc: 0.9052  (best train acc: 0.8628, best val acc: 0.9174, best train loss: 0.3928  @ epoch 855 )\n",
      "[Epoch: 0940] train loss: 0.4068, train acc: 0.8587, val loss: 0.2525, val acc: 0.9137  (best train acc: 0.8638, best val acc: 0.9174, best train loss: 0.3884  @ epoch 931 )\n",
      "[Epoch: 0960] train loss: 0.3977, train acc: 0.8580, val loss: 0.2506, val acc: 0.9164  (best train acc: 0.8638, best val acc: 0.9177, best train loss: 0.3859  @ epoch 959 )\n",
      "[Epoch: 0980] train loss: 0.4162, train acc: 0.8489, val loss: 0.2686, val acc: 0.8978  (best train acc: 0.8639, best val acc: 0.9177, best train loss: 0.3851  @ epoch 967 )\n",
      "[Epoch: 1000] train loss: 0.3985, train acc: 0.8594, val loss: 0.2560, val acc: 0.9059  (best train acc: 0.8639, best val acc: 0.9177, best train loss: 0.3843  @ epoch 994 )\n",
      "[Epoch: 1020] train loss: 0.3974, train acc: 0.8565, val loss: 0.2477, val acc: 0.9133  (best train acc: 0.8639, best val acc: 0.9184, best train loss: 0.3843  @ epoch 994 )\n",
      "[Epoch: 1040] train loss: 0.4087, train acc: 0.8577, val loss: 0.2464, val acc: 0.9147  (best train acc: 0.8665, best val acc: 0.9194, best train loss: 0.3749  @ epoch 1033 )\n",
      "[Epoch: 1060] train loss: 0.3824, train acc: 0.8635, val loss: 0.2591, val acc: 0.9019  (best train acc: 0.8691, best val acc: 0.9194, best train loss: 0.3749  @ epoch 1033 )\n",
      "[Epoch: 1080] train loss: 0.3900, train acc: 0.8539, val loss: 0.2460, val acc: 0.9164  (best train acc: 0.8691, best val acc: 0.9194, best train loss: 0.3749  @ epoch 1033 )\n",
      "[Epoch: 1100] train loss: 0.4021, train acc: 0.8540, val loss: 0.2523, val acc: 0.9184  (best train acc: 0.8691, best val acc: 0.9194, best train loss: 0.3740  @ epoch 1088 )\n",
      "[Epoch: 1120] train loss: 0.3896, train acc: 0.8587, val loss: 0.2516, val acc: 0.9110  (best train acc: 0.8691, best val acc: 0.9194, best train loss: 0.3740  @ epoch 1088 )\n",
      "[Epoch: 1140] train loss: 0.3837, train acc: 0.8599, val loss: 0.2499, val acc: 0.9093  (best train acc: 0.8691, best val acc: 0.9197, best train loss: 0.3731  @ epoch 1138 )\n",
      "[Epoch: 1160] train loss: 0.3980, train acc: 0.8576, val loss: 0.2443, val acc: 0.9160  (best train acc: 0.8691, best val acc: 0.9204, best train loss: 0.3731  @ epoch 1138 )\n",
      "[Epoch: 1180] train loss: 0.3809, train acc: 0.8693, val loss: 0.2406, val acc: 0.9191  (best train acc: 0.8694, best val acc: 0.9204, best train loss: 0.3685  @ epoch 1177 )\n",
      "[Epoch: 1200] train loss: 0.3678, train acc: 0.8671, val loss: 0.2442, val acc: 0.9180  (best train acc: 0.8728, best val acc: 0.9204, best train loss: 0.3637  @ epoch 1188 )\n",
      "[Epoch: 1220] train loss: 0.3840, train acc: 0.8665, val loss: 0.2548, val acc: 0.9046  (best train acc: 0.8728, best val acc: 0.9211, best train loss: 0.3579  @ epoch 1206 )\n",
      "[Epoch: 1240] train loss: 0.3725, train acc: 0.8681, val loss: 0.2528, val acc: 0.9130  (best train acc: 0.8728, best val acc: 0.9211, best train loss: 0.3579  @ epoch 1206 )\n",
      "[Epoch: 1260] train loss: 0.3628, train acc: 0.8728, val loss: 0.2446, val acc: 0.9086  (best train acc: 0.8728, best val acc: 0.9211, best train loss: 0.3579  @ epoch 1206 )\n",
      "[Epoch: 1280] train loss: 0.3717, train acc: 0.8687, val loss: 0.2452, val acc: 0.9153  (best train acc: 0.8762, best val acc: 0.9211, best train loss: 0.3579  @ epoch 1206 )\n",
      "[Epoch: 1300] train loss: 0.3711, train acc: 0.8673, val loss: 0.2408, val acc: 0.9160  (best train acc: 0.8762, best val acc: 0.9211, best train loss: 0.3579  @ epoch 1206 )\n",
      "[Epoch: 1320] train loss: 0.3728, train acc: 0.8610, val loss: 0.2402, val acc: 0.9184  (best train acc: 0.8762, best val acc: 0.9211, best train loss: 0.3579  @ epoch 1206 )\n",
      "[Epoch: 1340] train loss: 0.3673, train acc: 0.8691, val loss: 0.2400, val acc: 0.9164  (best train acc: 0.8762, best val acc: 0.9211, best train loss: 0.3571  @ epoch 1326 )\n",
      "[Epoch: 1360] train loss: 0.3660, train acc: 0.8705, val loss: 0.2404, val acc: 0.9164  (best train acc: 0.8762, best val acc: 0.9211, best train loss: 0.3571  @ epoch 1326 )\n",
      "[Epoch: 1380] train loss: 0.3961, train acc: 0.8569, val loss: 0.2415, val acc: 0.9218  (best train acc: 0.8777, best val acc: 0.9218, best train loss: 0.3512  @ epoch 1363 )\n",
      "[Epoch: 1400] train loss: 0.3632, train acc: 0.8691, val loss: 0.2350, val acc: 0.9207  (best train acc: 0.8777, best val acc: 0.9218, best train loss: 0.3512  @ epoch 1363 )\n",
      "[Epoch: 1420] train loss: 0.3607, train acc: 0.8705, val loss: 0.2429, val acc: 0.9153  (best train acc: 0.8777, best val acc: 0.9224, best train loss: 0.3512  @ epoch 1363 )\n",
      "[Epoch: 1440] train loss: 0.3597, train acc: 0.8681, val loss: 0.2366, val acc: 0.9218  (best train acc: 0.8777, best val acc: 0.9224, best train loss: 0.3512  @ epoch 1363 )\n",
      "[Epoch: 1460] train loss: 0.3571, train acc: 0.8737, val loss: 0.2327, val acc: 0.9211  (best train acc: 0.8777, best val acc: 0.9228, best train loss: 0.3512  @ epoch 1363 )\n",
      "[Epoch: 1480] train loss: 0.3769, train acc: 0.8571, val loss: 0.2489, val acc: 0.9137  (best train acc: 0.8777, best val acc: 0.9228, best train loss: 0.3512  @ epoch 1363 )\n",
      "[Epoch: 1500] train loss: 0.3726, train acc: 0.8710, val loss: 0.2510, val acc: 0.9069  (best train acc: 0.8777, best val acc: 0.9231, best train loss: 0.3487  @ epoch 1496 )\n",
      "[Epoch: 1520] train loss: 0.3623, train acc: 0.8709, val loss: 0.2315, val acc: 0.9228  (best train acc: 0.8777, best val acc: 0.9241, best train loss: 0.3484  @ epoch 1513 )\n",
      "[Epoch: 1540] train loss: 0.3416, train acc: 0.8837, val loss: 0.2330, val acc: 0.9201  (best train acc: 0.8837, best val acc: 0.9241, best train loss: 0.3416  @ epoch 1540 )\n",
      "[Epoch: 1560] train loss: 0.3571, train acc: 0.8686, val loss: 0.2387, val acc: 0.9140  (best train acc: 0.8837, best val acc: 0.9241, best train loss: 0.3390  @ epoch 1547 )\n",
      "[Epoch: 1580] train loss: 0.3578, train acc: 0.8772, val loss: 0.2320, val acc: 0.9197  (best train acc: 0.8837, best val acc: 0.9248, best train loss: 0.3390  @ epoch 1547 )\n",
      "[Epoch: 1600] train loss: 0.3471, train acc: 0.8803, val loss: 0.2302, val acc: 0.9218  (best train acc: 0.8837, best val acc: 0.9248, best train loss: 0.3390  @ epoch 1547 )\n",
      "[Epoch: 1620] train loss: 0.3485, train acc: 0.8764, val loss: 0.2328, val acc: 0.9187  (best train acc: 0.8837, best val acc: 0.9248, best train loss: 0.3361  @ epoch 1611 )\n",
      "[Epoch: 1640] train loss: 0.3489, train acc: 0.8751, val loss: 0.2311, val acc: 0.9221  (best train acc: 0.8837, best val acc: 0.9248, best train loss: 0.3361  @ epoch 1611 )\n",
      "[Epoch: 1660] train loss: 0.3600, train acc: 0.8648, val loss: 0.2324, val acc: 0.9224  (best train acc: 0.8837, best val acc: 0.9248, best train loss: 0.3361  @ epoch 1611 )\n",
      "[Epoch: 1680] train loss: 0.3465, train acc: 0.8772, val loss: 0.2316, val acc: 0.9177  (best train acc: 0.8837, best val acc: 0.9248, best train loss: 0.3361  @ epoch 1611 )\n",
      "[Epoch: 1700] train loss: 0.3618, train acc: 0.8649, val loss: 0.2412, val acc: 0.9231  (best train acc: 0.8837, best val acc: 0.9248, best train loss: 0.3361  @ epoch 1611 )\n",
      "[Epoch: 1720] train loss: 0.3507, train acc: 0.8793, val loss: 0.2335, val acc: 0.9157  (best train acc: 0.8837, best val acc: 0.9248, best train loss: 0.3361  @ epoch 1611 )\n",
      "[Epoch: 1740] train loss: 0.3412, train acc: 0.8807, val loss: 0.2287, val acc: 0.9207  (best train acc: 0.8837, best val acc: 0.9248, best train loss: 0.3361  @ epoch 1611 )\n",
      "[Epoch: 1760] train loss: 0.3366, train acc: 0.8821, val loss: 0.2287, val acc: 0.9221  (best train acc: 0.8837, best val acc: 0.9248, best train loss: 0.3361  @ epoch 1611 )\n",
      "[Epoch: 1780] train loss: 0.3373, train acc: 0.8794, val loss: 0.2389, val acc: 0.9113  (best train acc: 0.8837, best val acc: 0.9248, best train loss: 0.3361  @ epoch 1611 )\n",
      "[Epoch: 1800] train loss: 0.3463, train acc: 0.8770, val loss: 0.2256, val acc: 0.9201  (best train acc: 0.8844, best val acc: 0.9248, best train loss: 0.3304  @ epoch 1798 )\n",
      "[Epoch: 1820] train loss: 0.3395, train acc: 0.8798, val loss: 0.2246, val acc: 0.9228  (best train acc: 0.8875, best val acc: 0.9248, best train loss: 0.3260  @ epoch 1818 )\n",
      "[Epoch: 1840] train loss: 0.3458, train acc: 0.8776, val loss: 0.2537, val acc: 0.9086  (best train acc: 0.8875, best val acc: 0.9248, best train loss: 0.3260  @ epoch 1818 )\n",
      "[Epoch: 1860] train loss: 0.3436, train acc: 0.8757, val loss: 0.2351, val acc: 0.9140  (best train acc: 0.8875, best val acc: 0.9248, best train loss: 0.3260  @ epoch 1818 )\n",
      "[Epoch: 1880] train loss: 0.3372, train acc: 0.8799, val loss: 0.2460, val acc: 0.9143  (best train acc: 0.8875, best val acc: 0.9248, best train loss: 0.3260  @ epoch 1818 )\n",
      "[Epoch: 1900] train loss: 0.3802, train acc: 0.8646, val loss: 0.2312, val acc: 0.9174  (best train acc: 0.8875, best val acc: 0.9248, best train loss: 0.3260  @ epoch 1818 )\n",
      "[Epoch: 1920] train loss: 0.3369, train acc: 0.8811, val loss: 0.2240, val acc: 0.9191  (best train acc: 0.8880, best val acc: 0.9248, best train loss: 0.3221  @ epoch 1917 )\n",
      "[Epoch: 1940] train loss: 0.3370, train acc: 0.8801, val loss: 0.2282, val acc: 0.9177  (best train acc: 0.8897, best val acc: 0.9248, best train loss: 0.3221  @ epoch 1917 )\n",
      "[Epoch: 1960] train loss: 0.3339, train acc: 0.8826, val loss: 0.2296, val acc: 0.9170  (best train acc: 0.8898, best val acc: 0.9248, best train loss: 0.3207  @ epoch 1959 )\n",
      "[Epoch: 1980] train loss: 0.3264, train acc: 0.8860, val loss: 0.2197, val acc: 0.9218  (best train acc: 0.8898, best val acc: 0.9248, best train loss: 0.3207  @ epoch 1959 )\n",
      "[Epoch: 2000] train loss: 0.3316, train acc: 0.8820, val loss: 0.2270, val acc: 0.9207  (best train acc: 0.8898, best val acc: 0.9248, best train loss: 0.3207  @ epoch 1959 )\n",
      "[Epoch: 2020] train loss: 0.3211, train acc: 0.8899, val loss: 0.2201, val acc: 0.9218  (best train acc: 0.8899, best val acc: 0.9248, best train loss: 0.3194  @ epoch 2016 )\n",
      "[Epoch: 2040] train loss: 0.3256, train acc: 0.8843, val loss: 0.2218, val acc: 0.9211  (best train acc: 0.8899, best val acc: 0.9248, best train loss: 0.3194  @ epoch 2016 )\n",
      "[Epoch: 2060] train loss: 0.3428, train acc: 0.8815, val loss: 0.2234, val acc: 0.9204  (best train acc: 0.8899, best val acc: 0.9248, best train loss: 0.3194  @ epoch 2016 )\n",
      "[Epoch: 2080] train loss: 0.3267, train acc: 0.8843, val loss: 0.2287, val acc: 0.9204  (best train acc: 0.8900, best val acc: 0.9248, best train loss: 0.3145  @ epoch 2076 )\n",
      "[Epoch: 2100] train loss: 0.3303, train acc: 0.8837, val loss: 0.2283, val acc: 0.9174  (best train acc: 0.8903, best val acc: 0.9248, best train loss: 0.3145  @ epoch 2076 )\n",
      "[Epoch: 2120] train loss: 0.3153, train acc: 0.8884, val loss: 0.2233, val acc: 0.9228  (best train acc: 0.8908, best val acc: 0.9251, best train loss: 0.3145  @ epoch 2076 )\n",
      "[Epoch: 2140] train loss: 0.3462, train acc: 0.8785, val loss: 0.2208, val acc: 0.9204  (best train acc: 0.8934, best val acc: 0.9251, best train loss: 0.3071  @ epoch 2130 )\n",
      "[Epoch: 2160] train loss: 0.3207, train acc: 0.8886, val loss: 0.2221, val acc: 0.9214  (best train acc: 0.8934, best val acc: 0.9251, best train loss: 0.3071  @ epoch 2130 )\n",
      "[Epoch: 2180] train loss: 0.3227, train acc: 0.8848, val loss: 0.2218, val acc: 0.9204  (best train acc: 0.8934, best val acc: 0.9251, best train loss: 0.3071  @ epoch 2130 )\n",
      "[Epoch: 2200] train loss: 0.3303, train acc: 0.8832, val loss: 0.2323, val acc: 0.9150  (best train acc: 0.8934, best val acc: 0.9251, best train loss: 0.3071  @ epoch 2130 )\n",
      "[Epoch: 2220] train loss: 0.3305, train acc: 0.8833, val loss: 0.2495, val acc: 0.9096  (best train acc: 0.8934, best val acc: 0.9251, best train loss: 0.3071  @ epoch 2130 )\n",
      "[Epoch: 2240] train loss: 0.3180, train acc: 0.8900, val loss: 0.2205, val acc: 0.9214  (best train acc: 0.8934, best val acc: 0.9251, best train loss: 0.3071  @ epoch 2130 )\n",
      "[Epoch: 2260] train loss: 0.3191, train acc: 0.8812, val loss: 0.2239, val acc: 0.9218  (best train acc: 0.8934, best val acc: 0.9251, best train loss: 0.3071  @ epoch 2130 )\n",
      "[Epoch: 2280] train loss: 0.3311, train acc: 0.8824, val loss: 0.2291, val acc: 0.9170  (best train acc: 0.8934, best val acc: 0.9251, best train loss: 0.3071  @ epoch 2130 )\n",
      "[Epoch: 2300] train loss: 0.3215, train acc: 0.8868, val loss: 0.2289, val acc: 0.9191  (best train acc: 0.8934, best val acc: 0.9251, best train loss: 0.3071  @ epoch 2130 )\n",
      "[Epoch: 2320] train loss: 0.3109, train acc: 0.8861, val loss: 0.2283, val acc: 0.9164  (best train acc: 0.8934, best val acc: 0.9251, best train loss: 0.3071  @ epoch 2130 )\n",
      "[Epoch: 2340] train loss: 0.3435, train acc: 0.8707, val loss: 0.2596, val acc: 0.8958  (best train acc: 0.8934, best val acc: 0.9258, best train loss: 0.3071  @ epoch 2130 )\n",
      "[Epoch: 2360] train loss: 0.3242, train acc: 0.8893, val loss: 0.2531, val acc: 0.9073  (best train acc: 0.8934, best val acc: 0.9258, best train loss: 0.3071  @ epoch 2130 )\n",
      "[Epoch: 2380] train loss: 0.3243, train acc: 0.8858, val loss: 0.2244, val acc: 0.9231  (best train acc: 0.8934, best val acc: 0.9258, best train loss: 0.3071  @ epoch 2130 )\n",
      "[Epoch: 2400] train loss: 0.3084, train acc: 0.8924, val loss: 0.2202, val acc: 0.9224  (best train acc: 0.8936, best val acc: 0.9258, best train loss: 0.3065  @ epoch 2394 )\n",
      "[Epoch: 2420] train loss: 0.3092, train acc: 0.8922, val loss: 0.2188, val acc: 0.9234  (best train acc: 0.8944, best val acc: 0.9258, best train loss: 0.3057  @ epoch 2411 )\n",
      "[Epoch: 2440] train loss: 0.3089, train acc: 0.8906, val loss: 0.2358, val acc: 0.9153  (best train acc: 0.8944, best val acc: 0.9258, best train loss: 0.3043  @ epoch 2437 )\n",
      "[Epoch: 2460] train loss: 0.3112, train acc: 0.8902, val loss: 0.2199, val acc: 0.9177  (best train acc: 0.8958, best val acc: 0.9258, best train loss: 0.3028  @ epoch 2453 )\n",
      "[Epoch: 2480] train loss: 0.3194, train acc: 0.8857, val loss: 0.2168, val acc: 0.9221  (best train acc: 0.8966, best val acc: 0.9258, best train loss: 0.2973  @ epoch 2479 )\n",
      "[Epoch: 2500] train loss: 0.3099, train acc: 0.8937, val loss: 0.2272, val acc: 0.9157  (best train acc: 0.8999, best val acc: 0.9258, best train loss: 0.2973  @ epoch 2479 )\n",
      "[Epoch: 2520] train loss: 0.3102, train acc: 0.8893, val loss: 0.2210, val acc: 0.9207  (best train acc: 0.8999, best val acc: 0.9258, best train loss: 0.2973  @ epoch 2479 )\n",
      "[Epoch: 2540] train loss: 0.3105, train acc: 0.8932, val loss: 0.2211, val acc: 0.9204  (best train acc: 0.8999, best val acc: 0.9258, best train loss: 0.2962  @ epoch 2531 )\n",
      "[Epoch: 2560] train loss: 0.3105, train acc: 0.8902, val loss: 0.2358, val acc: 0.9140  (best train acc: 0.8999, best val acc: 0.9258, best train loss: 0.2962  @ epoch 2531 )\n",
      "[Epoch: 2580] train loss: 0.3115, train acc: 0.8916, val loss: 0.2248, val acc: 0.9180  (best train acc: 0.8999, best val acc: 0.9258, best train loss: 0.2962  @ epoch 2531 )\n",
      "[Epoch: 2600] train loss: 0.3104, train acc: 0.8928, val loss: 0.2248, val acc: 0.9191  (best train acc: 0.8999, best val acc: 0.9258, best train loss: 0.2942  @ epoch 2591 )\n",
      "[Epoch: 2620] train loss: 0.3081, train acc: 0.8928, val loss: 0.2380, val acc: 0.9130  (best train acc: 0.8999, best val acc: 0.9258, best train loss: 0.2942  @ epoch 2591 )\n",
      "[Epoch: 2640] train loss: 0.2947, train acc: 0.8987, val loss: 0.2164, val acc: 0.9231  (best train acc: 0.8999, best val acc: 0.9258, best train loss: 0.2942  @ epoch 2591 )\n",
      "[Epoch: 2660] train loss: 0.3017, train acc: 0.8932, val loss: 0.2179, val acc: 0.9221  (best train acc: 0.8999, best val acc: 0.9261, best train loss: 0.2942  @ epoch 2591 )\n",
      "[Epoch: 2680] train loss: 0.3345, train acc: 0.8817, val loss: 0.2244, val acc: 0.9177  (best train acc: 0.8999, best val acc: 0.9261, best train loss: 0.2942  @ epoch 2591 )\n",
      "[Epoch: 2700] train loss: 0.2957, train acc: 0.8953, val loss: 0.2180, val acc: 0.9255  (best train acc: 0.8999, best val acc: 0.9261, best train loss: 0.2942  @ epoch 2591 )\n",
      "[Epoch: 2720] train loss: 0.3036, train acc: 0.8922, val loss: 0.2189, val acc: 0.9251  (best train acc: 0.8999, best val acc: 0.9261, best train loss: 0.2942  @ epoch 2591 )\n",
      "[Epoch: 2740] train loss: 0.3081, train acc: 0.8916, val loss: 0.2278, val acc: 0.9201  (best train acc: 0.8999, best val acc: 0.9261, best train loss: 0.2942  @ epoch 2591 )\n",
      "[Epoch: 2760] train loss: 0.2909, train acc: 0.8991, val loss: 0.2243, val acc: 0.9211  (best train acc: 0.8999, best val acc: 0.9268, best train loss: 0.2909  @ epoch 2760 )\n",
      "[Epoch: 2780] train loss: 0.3038, train acc: 0.8927, val loss: 0.2137, val acc: 0.9238  (best train acc: 0.8999, best val acc: 0.9268, best train loss: 0.2909  @ epoch 2760 )\n",
      "[Epoch: 2800] train loss: 0.2918, train acc: 0.8976, val loss: 0.2165, val acc: 0.9228  (best train acc: 0.8999, best val acc: 0.9268, best train loss: 0.2909  @ epoch 2760 )\n",
      "[Epoch: 2820] train loss: 0.3015, train acc: 0.8911, val loss: 0.2195, val acc: 0.9231  (best train acc: 0.8999, best val acc: 0.9268, best train loss: 0.2870  @ epoch 2815 )\n",
      "[Epoch: 2840] train loss: 0.2944, train acc: 0.8971, val loss: 0.2150, val acc: 0.9255  (best train acc: 0.9029, best val acc: 0.9268, best train loss: 0.2835  @ epoch 2833 )\n",
      "[Epoch: 2860] train loss: 0.2935, train acc: 0.8981, val loss: 0.2157, val acc: 0.9248  (best train acc: 0.9029, best val acc: 0.9268, best train loss: 0.2832  @ epoch 2853 )\n",
      "[Epoch: 2880] train loss: 0.2823, train acc: 0.8986, val loss: 0.2165, val acc: 0.9241  (best train acc: 0.9029, best val acc: 0.9268, best train loss: 0.2823  @ epoch 2880 )\n",
      "[Epoch: 2900] train loss: 0.2831, train acc: 0.9018, val loss: 0.2170, val acc: 0.9228  (best train acc: 0.9029, best val acc: 0.9275, best train loss: 0.2823  @ epoch 2880 )\n",
      "[Epoch: 2920] train loss: 0.3037, train acc: 0.8921, val loss: 0.2186, val acc: 0.9214  (best train acc: 0.9029, best val acc: 0.9275, best train loss: 0.2823  @ epoch 2880 )\n",
      "[Epoch: 2940] train loss: 0.3066, train acc: 0.8918, val loss: 0.2127, val acc: 0.9275  (best train acc: 0.9034, best val acc: 0.9275, best train loss: 0.2823  @ epoch 2880 )\n",
      "[Epoch: 2960] train loss: 0.3074, train acc: 0.8880, val loss: 0.2148, val acc: 0.9228  (best train acc: 0.9034, best val acc: 0.9275, best train loss: 0.2823  @ epoch 2880 )\n",
      "[Epoch: 2980] train loss: 0.2889, train acc: 0.8999, val loss: 0.2145, val acc: 0.9261  (best train acc: 0.9034, best val acc: 0.9275, best train loss: 0.2804  @ epoch 2977 )\n",
      "[Epoch: 3000] train loss: 0.2942, train acc: 0.8950, val loss: 0.2126, val acc: 0.9255  (best train acc: 0.9044, best val acc: 0.9275, best train loss: 0.2797  @ epoch 2990 )\n",
      "[Epoch: 3020] train loss: 0.2914, train acc: 0.8973, val loss: 0.2156, val acc: 0.9248  (best train acc: 0.9058, best val acc: 0.9275, best train loss: 0.2769  @ epoch 3004 )\n",
      "[Epoch: 3040] train loss: 0.2846, train acc: 0.9007, val loss: 0.2204, val acc: 0.9248  (best train acc: 0.9058, best val acc: 0.9275, best train loss: 0.2769  @ epoch 3004 )\n",
      "[Epoch: 3060] train loss: 0.2905, train acc: 0.8984, val loss: 0.2160, val acc: 0.9265  (best train acc: 0.9058, best val acc: 0.9275, best train loss: 0.2769  @ epoch 3004 )\n",
      "[Epoch: 3080] train loss: 0.2915, train acc: 0.8940, val loss: 0.2263, val acc: 0.9221  (best train acc: 0.9058, best val acc: 0.9275, best train loss: 0.2769  @ epoch 3004 )\n",
      "[Epoch: 3100] train loss: 0.3287, train acc: 0.8785, val loss: 0.2522, val acc: 0.9022  (best train acc: 0.9058, best val acc: 0.9275, best train loss: 0.2769  @ epoch 3004 )\n",
      "[Epoch: 3120] train loss: 0.2844, train acc: 0.9007, val loss: 0.2131, val acc: 0.9268  (best train acc: 0.9058, best val acc: 0.9285, best train loss: 0.2769  @ epoch 3004 )\n",
      "[Epoch: 3140] train loss: 0.2768, train acc: 0.9003, val loss: 0.2230, val acc: 0.9218  (best train acc: 0.9058, best val acc: 0.9285, best train loss: 0.2768  @ epoch 3140 )\n",
      "[Epoch: 3160] train loss: 0.3117, train acc: 0.8855, val loss: 0.2214, val acc: 0.9231  (best train acc: 0.9060, best val acc: 0.9288, best train loss: 0.2749  @ epoch 3154 )\n",
      "[Epoch: 3180] train loss: 0.2920, train acc: 0.8968, val loss: 0.2145, val acc: 0.9261  (best train acc: 0.9060, best val acc: 0.9302, best train loss: 0.2749  @ epoch 3154 )\n",
      "[Epoch: 3200] train loss: 0.2864, train acc: 0.8978, val loss: 0.2258, val acc: 0.9224  (best train acc: 0.9091, best val acc: 0.9302, best train loss: 0.2728  @ epoch 3199 )\n",
      "[Epoch: 3220] train loss: 0.2788, train acc: 0.9037, val loss: 0.2170, val acc: 0.9268  (best train acc: 0.9091, best val acc: 0.9302, best train loss: 0.2717  @ epoch 3214 )\n",
      "[Epoch: 3240] train loss: 0.2952, train acc: 0.8963, val loss: 0.2231, val acc: 0.9251  (best train acc: 0.9091, best val acc: 0.9302, best train loss: 0.2717  @ epoch 3214 )\n",
      "[Epoch: 3260] train loss: 0.2925, train acc: 0.8974, val loss: 0.2313, val acc: 0.9174  (best train acc: 0.9091, best val acc: 0.9302, best train loss: 0.2717  @ epoch 3214 )\n",
      "[Epoch: 3280] train loss: 0.2816, train acc: 0.8997, val loss: 0.2108, val acc: 0.9288  (best train acc: 0.9091, best val acc: 0.9302, best train loss: 0.2717  @ epoch 3214 )\n",
      "[Epoch: 3300] train loss: 0.2875, train acc: 0.9017, val loss: 0.2297, val acc: 0.9207  (best train acc: 0.9091, best val acc: 0.9305, best train loss: 0.2714  @ epoch 3289 )\n",
      "[Epoch: 3320] train loss: 0.2822, train acc: 0.9005, val loss: 0.2206, val acc: 0.9248  (best train acc: 0.9091, best val acc: 0.9305, best train loss: 0.2695  @ epoch 3310 )\n",
      "[Epoch: 3340] train loss: 0.2831, train acc: 0.9012, val loss: 0.2115, val acc: 0.9282  (best train acc: 0.9091, best val acc: 0.9305, best train loss: 0.2695  @ epoch 3310 )\n",
      "[Epoch: 3360] train loss: 0.2776, train acc: 0.9043, val loss: 0.2405, val acc: 0.9157  (best train acc: 0.9091, best val acc: 0.9305, best train loss: 0.2671  @ epoch 3347 )\n",
      "[Epoch: 3380] train loss: 0.2875, train acc: 0.8926, val loss: 0.2132, val acc: 0.9285  (best train acc: 0.9091, best val acc: 0.9305, best train loss: 0.2671  @ epoch 3347 )\n",
      "[Epoch: 3400] train loss: 0.2854, train acc: 0.8997, val loss: 0.2100, val acc: 0.9298  (best train acc: 0.9091, best val acc: 0.9305, best train loss: 0.2671  @ epoch 3347 )\n",
      "[Epoch: 3420] train loss: 0.2803, train acc: 0.8974, val loss: 0.2084, val acc: 0.9288  (best train acc: 0.9091, best val acc: 0.9305, best train loss: 0.2658  @ epoch 3412 )\n",
      "[Epoch: 3440] train loss: 0.2847, train acc: 0.8957, val loss: 0.2149, val acc: 0.9261  (best train acc: 0.9091, best val acc: 0.9305, best train loss: 0.2658  @ epoch 3412 )\n",
      "[Epoch: 3460] train loss: 0.2809, train acc: 0.9025, val loss: 0.2147, val acc: 0.9261  (best train acc: 0.9091, best val acc: 0.9315, best train loss: 0.2658  @ epoch 3412 )\n",
      "[Epoch: 3480] train loss: 0.2918, train acc: 0.8936, val loss: 0.2099, val acc: 0.9309  (best train acc: 0.9091, best val acc: 0.9315, best train loss: 0.2658  @ epoch 3412 )\n",
      "[Epoch: 3500] train loss: 0.2758, train acc: 0.9033, val loss: 0.2152, val acc: 0.9275  (best train acc: 0.9091, best val acc: 0.9315, best train loss: 0.2658  @ epoch 3412 )\n",
      "[Epoch: 3520] train loss: 0.2566, train acc: 0.9093, val loss: 0.2104, val acc: 0.9295  (best train acc: 0.9093, best val acc: 0.9315, best train loss: 0.2566  @ epoch 3520 )\n",
      "[Epoch: 3540] train loss: 0.2656, train acc: 0.9050, val loss: 0.2088, val acc: 0.9282  (best train acc: 0.9093, best val acc: 0.9325, best train loss: 0.2566  @ epoch 3520 )\n",
      "[Epoch: 3560] train loss: 0.2679, train acc: 0.9050, val loss: 0.2098, val acc: 0.9285  (best train acc: 0.9093, best val acc: 0.9325, best train loss: 0.2566  @ epoch 3520 )\n",
      "[Epoch: 3580] train loss: 0.2830, train acc: 0.9017, val loss: 0.2106, val acc: 0.9275  (best train acc: 0.9117, best val acc: 0.9325, best train loss: 0.2566  @ epoch 3520 )\n",
      "[Epoch: 3600] train loss: 0.2658, train acc: 0.9078, val loss: 0.2073, val acc: 0.9305  (best train acc: 0.9117, best val acc: 0.9325, best train loss: 0.2566  @ epoch 3520 )\n",
      "[Epoch: 3620] train loss: 0.2720, train acc: 0.9058, val loss: 0.2075, val acc: 0.9298  (best train acc: 0.9117, best val acc: 0.9325, best train loss: 0.2566  @ epoch 3520 )\n",
      "[Epoch: 3640] train loss: 0.2658, train acc: 0.9071, val loss: 0.2122, val acc: 0.9285  (best train acc: 0.9117, best val acc: 0.9325, best train loss: 0.2566  @ epoch 3520 )\n",
      "[Epoch: 3660] train loss: 0.3461, train acc: 0.8784, val loss: 0.2439, val acc: 0.9066  (best train acc: 0.9117, best val acc: 0.9325, best train loss: 0.2566  @ epoch 3520 )\n",
      "[Epoch: 3680] train loss: 0.2723, train acc: 0.9016, val loss: 0.2359, val acc: 0.9194  (best train acc: 0.9117, best val acc: 0.9325, best train loss: 0.2566  @ epoch 3520 )\n",
      "[Epoch: 3700] train loss: 0.2780, train acc: 0.8960, val loss: 0.2107, val acc: 0.9295  (best train acc: 0.9117, best val acc: 0.9325, best train loss: 0.2566  @ epoch 3520 )\n",
      "[Epoch: 3720] train loss: 0.2882, train acc: 0.8950, val loss: 0.2141, val acc: 0.9272  (best train acc: 0.9117, best val acc: 0.9325, best train loss: 0.2566  @ epoch 3520 )\n",
      "[Epoch: 3740] train loss: 0.2657, train acc: 0.9075, val loss: 0.2051, val acc: 0.9302  (best train acc: 0.9131, best val acc: 0.9336, best train loss: 0.2565  @ epoch 3727 )\n",
      "[Epoch: 3760] train loss: 0.2702, train acc: 0.9062, val loss: 0.2068, val acc: 0.9332  (best train acc: 0.9131, best val acc: 0.9336, best train loss: 0.2565  @ epoch 3727 )\n",
      "[Epoch: 3780] train loss: 0.2624, train acc: 0.9053, val loss: 0.2079, val acc: 0.9298  (best train acc: 0.9131, best val acc: 0.9336, best train loss: 0.2565  @ epoch 3727 )\n",
      "[Epoch: 3800] train loss: 0.2592, train acc: 0.9071, val loss: 0.2055, val acc: 0.9312  (best train acc: 0.9131, best val acc: 0.9336, best train loss: 0.2564  @ epoch 3796 )\n",
      "[Epoch: 3820] train loss: 0.2667, train acc: 0.9125, val loss: 0.2056, val acc: 0.9312  (best train acc: 0.9131, best val acc: 0.9336, best train loss: 0.2529  @ epoch 3805 )\n",
      "[Epoch: 3840] train loss: 0.2663, train acc: 0.9076, val loss: 0.2124, val acc: 0.9295  (best train acc: 0.9143, best val acc: 0.9336, best train loss: 0.2520  @ epoch 3839 )\n",
      "[Epoch: 3860] train loss: 0.2584, train acc: 0.9102, val loss: 0.2078, val acc: 0.9309  (best train acc: 0.9143, best val acc: 0.9336, best train loss: 0.2520  @ epoch 3839 )\n",
      "[Epoch: 3880] train loss: 0.2521, train acc: 0.9114, val loss: 0.2101, val acc: 0.9302  (best train acc: 0.9143, best val acc: 0.9336, best train loss: 0.2516  @ epoch 3874 )\n",
      "[Epoch: 3900] train loss: 0.2756, train acc: 0.8968, val loss: 0.2090, val acc: 0.9315  (best train acc: 0.9143, best val acc: 0.9336, best train loss: 0.2516  @ epoch 3874 )\n",
      "[Epoch: 3920] train loss: 0.2628, train acc: 0.9050, val loss: 0.2182, val acc: 0.9261  (best train acc: 0.9143, best val acc: 0.9336, best train loss: 0.2516  @ epoch 3874 )\n",
      "[Epoch: 3940] train loss: 0.2664, train acc: 0.9061, val loss: 0.2100, val acc: 0.9302  (best train acc: 0.9143, best val acc: 0.9336, best train loss: 0.2516  @ epoch 3874 )\n",
      "[Epoch: 3960] train loss: 0.2561, train acc: 0.9104, val loss: 0.2060, val acc: 0.9325  (best train acc: 0.9143, best val acc: 0.9339, best train loss: 0.2516  @ epoch 3874 )\n",
      "[Epoch: 3980] train loss: 0.3129, train acc: 0.8890, val loss: 0.2263, val acc: 0.9197  (best train acc: 0.9143, best val acc: 0.9339, best train loss: 0.2506  @ epoch 3962 )\n",
      "[Epoch: 4000] train loss: 0.2600, train acc: 0.9119, val loss: 0.2056, val acc: 0.9325  (best train acc: 0.9143, best val acc: 0.9339, best train loss: 0.2506  @ epoch 3962 )\n",
      "[Epoch: 4020] train loss: 0.2761, train acc: 0.9028, val loss: 0.2118, val acc: 0.9292  (best train acc: 0.9143, best val acc: 0.9339, best train loss: 0.2506  @ epoch 3962 )\n",
      "[Epoch: 4040] train loss: 0.2580, train acc: 0.9088, val loss: 0.2229, val acc: 0.9228  (best train acc: 0.9143, best val acc: 0.9339, best train loss: 0.2506  @ epoch 3962 )\n",
      "[Epoch: 4060] train loss: 0.2803, train acc: 0.9028, val loss: 0.2315, val acc: 0.9201  (best train acc: 0.9143, best val acc: 0.9342, best train loss: 0.2504  @ epoch 4051 )\n",
      "[Epoch: 4080] train loss: 0.2622, train acc: 0.9076, val loss: 0.2186, val acc: 0.9295  (best train acc: 0.9143, best val acc: 0.9342, best train loss: 0.2504  @ epoch 4051 )\n",
      "[Epoch: 4100] train loss: 0.2577, train acc: 0.9073, val loss: 0.2151, val acc: 0.9272  (best train acc: 0.9143, best val acc: 0.9346, best train loss: 0.2504  @ epoch 4051 )\n",
      "[Epoch: 4120] train loss: 0.2705, train acc: 0.9051, val loss: 0.2087, val acc: 0.9315  (best train acc: 0.9144, best val acc: 0.9346, best train loss: 0.2494  @ epoch 4107 )\n",
      "[Epoch: 4140] train loss: 0.2539, train acc: 0.9132, val loss: 0.2371, val acc: 0.9238  (best train acc: 0.9144, best val acc: 0.9346, best train loss: 0.2486  @ epoch 4121 )\n",
      "[Epoch: 4160] train loss: 0.2752, train acc: 0.8972, val loss: 0.2179, val acc: 0.9268  (best train acc: 0.9144, best val acc: 0.9346, best train loss: 0.2486  @ epoch 4121 )\n",
      "[Epoch: 4180] train loss: 0.2776, train acc: 0.8944, val loss: 0.2188, val acc: 0.9295  (best train acc: 0.9144, best val acc: 0.9346, best train loss: 0.2486  @ epoch 4121 )\n",
      "[Epoch: 4200] train loss: 0.2570, train acc: 0.9114, val loss: 0.2253, val acc: 0.9228  (best train acc: 0.9144, best val acc: 0.9346, best train loss: 0.2467  @ epoch 4199 )\n",
      "[Epoch: 4220] train loss: 0.2585, train acc: 0.9106, val loss: 0.2336, val acc: 0.9234  (best train acc: 0.9144, best val acc: 0.9346, best train loss: 0.2467  @ epoch 4199 )\n",
      "[Epoch: 4240] train loss: 0.2498, train acc: 0.9124, val loss: 0.2074, val acc: 0.9342  (best train acc: 0.9145, best val acc: 0.9352, best train loss: 0.2467  @ epoch 4199 )\n",
      "[Epoch: 4260] train loss: 0.2619, train acc: 0.9090, val loss: 0.2216, val acc: 0.9258  (best train acc: 0.9145, best val acc: 0.9352, best train loss: 0.2467  @ epoch 4199 )\n",
      "[Epoch: 4280] train loss: 0.2500, train acc: 0.9128, val loss: 0.2098, val acc: 0.9329  (best train acc: 0.9145, best val acc: 0.9352, best train loss: 0.2467  @ epoch 4199 )\n",
      "[Epoch: 4300] train loss: 0.2618, train acc: 0.9032, val loss: 0.2588, val acc: 0.9120  (best train acc: 0.9158, best val acc: 0.9352, best train loss: 0.2425  @ epoch 4288 )\n",
      "[Epoch: 4320] train loss: 0.2499, train acc: 0.9098, val loss: 0.2103, val acc: 0.9302  (best train acc: 0.9158, best val acc: 0.9352, best train loss: 0.2425  @ epoch 4288 )\n",
      "[Epoch: 4340] train loss: 0.2493, train acc: 0.9108, val loss: 0.2164, val acc: 0.9305  (best train acc: 0.9158, best val acc: 0.9352, best train loss: 0.2425  @ epoch 4288 )\n",
      "[Epoch: 4360] train loss: 0.2484, train acc: 0.9132, val loss: 0.2122, val acc: 0.9315  (best train acc: 0.9158, best val acc: 0.9352, best train loss: 0.2396  @ epoch 4345 )\n",
      "[Epoch: 4380] train loss: 0.2528, train acc: 0.9111, val loss: 0.2095, val acc: 0.9292  (best train acc: 0.9171, best val acc: 0.9356, best train loss: 0.2396  @ epoch 4345 )\n",
      "[Epoch: 4400] train loss: 0.2528, train acc: 0.9134, val loss: 0.2243, val acc: 0.9268  (best train acc: 0.9171, best val acc: 0.9356, best train loss: 0.2396  @ epoch 4345 )\n",
      "[Epoch: 4420] train loss: 0.2864, train acc: 0.9002, val loss: 0.2167, val acc: 0.9248  (best train acc: 0.9171, best val acc: 0.9356, best train loss: 0.2396  @ epoch 4345 )\n",
      "[Epoch: 4440] train loss: 0.2511, train acc: 0.9120, val loss: 0.2135, val acc: 0.9298  (best train acc: 0.9171, best val acc: 0.9356, best train loss: 0.2396  @ epoch 4345 )\n",
      "[Epoch: 4460] train loss: 0.2431, train acc: 0.9156, val loss: 0.2096, val acc: 0.9346  (best train acc: 0.9171, best val acc: 0.9363, best train loss: 0.2388  @ epoch 4451 )\n",
      "[Epoch: 4480] train loss: 0.2888, train acc: 0.8930, val loss: 0.2114, val acc: 0.9292  (best train acc: 0.9171, best val acc: 0.9363, best train loss: 0.2372  @ epoch 4464 )\n",
      "[Epoch: 4500] train loss: 0.2583, train acc: 0.9075, val loss: 0.2054, val acc: 0.9309  (best train acc: 0.9171, best val acc: 0.9363, best train loss: 0.2372  @ epoch 4464 )\n",
      "[Epoch: 4520] train loss: 0.2446, train acc: 0.9132, val loss: 0.2091, val acc: 0.9309  (best train acc: 0.9171, best val acc: 0.9366, best train loss: 0.2372  @ epoch 4464 )\n",
      "[Epoch: 4540] train loss: 0.2470, train acc: 0.9140, val loss: 0.2060, val acc: 0.9346  (best train acc: 0.9171, best val acc: 0.9366, best train loss: 0.2372  @ epoch 4464 )\n",
      "[Epoch: 4560] train loss: 0.2659, train acc: 0.9008, val loss: 0.2467, val acc: 0.9201  (best train acc: 0.9171, best val acc: 0.9366, best train loss: 0.2372  @ epoch 4464 )\n",
      "[Epoch: 4580] train loss: 0.2734, train acc: 0.9023, val loss: 0.2317, val acc: 0.9207  (best train acc: 0.9171, best val acc: 0.9366, best train loss: 0.2372  @ epoch 4464 )\n",
      "[Epoch: 4600] train loss: 0.2666, train acc: 0.9046, val loss: 0.2086, val acc: 0.9349  (best train acc: 0.9178, best val acc: 0.9366, best train loss: 0.2372  @ epoch 4464 )\n",
      "[Epoch: 4620] train loss: 0.2447, train acc: 0.9138, val loss: 0.2067, val acc: 0.9315  (best train acc: 0.9178, best val acc: 0.9366, best train loss: 0.2372  @ epoch 4464 )\n",
      "[Epoch: 4640] train loss: 0.2460, train acc: 0.9134, val loss: 0.2111, val acc: 0.9325  (best train acc: 0.9178, best val acc: 0.9366, best train loss: 0.2372  @ epoch 4464 )\n",
      "[Epoch: 4660] train loss: 0.2428, train acc: 0.9137, val loss: 0.2056, val acc: 0.9319  (best train acc: 0.9178, best val acc: 0.9379, best train loss: 0.2372  @ epoch 4464 )\n",
      "[Epoch: 4680] train loss: 0.2403, train acc: 0.9164, val loss: 0.2070, val acc: 0.9322  (best train acc: 0.9178, best val acc: 0.9379, best train loss: 0.2355  @ epoch 4675 )\n",
      "[Epoch: 4700] train loss: 0.2946, train acc: 0.8891, val loss: 0.2074, val acc: 0.9322  (best train acc: 0.9182, best val acc: 0.9379, best train loss: 0.2355  @ epoch 4675 )\n",
      "[Epoch: 4720] train loss: 0.2662, train acc: 0.9027, val loss: 0.2149, val acc: 0.9278  (best train acc: 0.9182, best val acc: 0.9379, best train loss: 0.2355  @ epoch 4675 )\n",
      "[Epoch: 4740] train loss: 0.2584, train acc: 0.8999, val loss: 0.2071, val acc: 0.9315  (best train acc: 0.9182, best val acc: 0.9379, best train loss: 0.2355  @ epoch 4675 )\n",
      "[Epoch: 4760] train loss: 0.2496, train acc: 0.9111, val loss: 0.2062, val acc: 0.9339  (best train acc: 0.9182, best val acc: 0.9379, best train loss: 0.2355  @ epoch 4675 )\n",
      "[Epoch: 4780] train loss: 0.2519, train acc: 0.9088, val loss: 0.2073, val acc: 0.9319  (best train acc: 0.9182, best val acc: 0.9379, best train loss: 0.2354  @ epoch 4764 )\n",
      "[Epoch: 4800] train loss: 0.2471, train acc: 0.9135, val loss: 0.2038, val acc: 0.9363  (best train acc: 0.9211, best val acc: 0.9379, best train loss: 0.2352  @ epoch 4798 )\n",
      "[Epoch: 4820] train loss: 0.2407, train acc: 0.9125, val loss: 0.2101, val acc: 0.9349  (best train acc: 0.9217, best val acc: 0.9379, best train loss: 0.2335  @ epoch 4819 )\n",
      "[Epoch: 4840] train loss: 0.2543, train acc: 0.9087, val loss: 0.2065, val acc: 0.9339  (best train acc: 0.9217, best val acc: 0.9379, best train loss: 0.2334  @ epoch 4822 )\n",
      "[Epoch: 4860] train loss: 0.2560, train acc: 0.9083, val loss: 0.2080, val acc: 0.9322  (best train acc: 0.9217, best val acc: 0.9379, best train loss: 0.2334  @ epoch 4822 )\n",
      "[Epoch: 4880] train loss: 0.2459, train acc: 0.9151, val loss: 0.2112, val acc: 0.9309  (best train acc: 0.9217, best val acc: 0.9379, best train loss: 0.2334  @ epoch 4822 )\n",
      "[Epoch: 4900] train loss: 0.2413, train acc: 0.9171, val loss: 0.2035, val acc: 0.9349  (best train acc: 0.9217, best val acc: 0.9379, best train loss: 0.2334  @ epoch 4822 )\n",
      "[Epoch: 4920] train loss: 0.2487, train acc: 0.9098, val loss: 0.2121, val acc: 0.9312  (best train acc: 0.9217, best val acc: 0.9379, best train loss: 0.2334  @ epoch 4822 )\n",
      "[Epoch: 4940] train loss: 0.2437, train acc: 0.9142, val loss: 0.2108, val acc: 0.9292  (best train acc: 0.9217, best val acc: 0.9379, best train loss: 0.2334  @ epoch 4822 )\n",
      "[Epoch: 4960] train loss: 0.2398, train acc: 0.9109, val loss: 0.2719, val acc: 0.9120  (best train acc: 0.9217, best val acc: 0.9379, best train loss: 0.2334  @ epoch 4822 )\n",
      "[Epoch: 4980] train loss: 0.2442, train acc: 0.9091, val loss: 0.2136, val acc: 0.9339  (best train acc: 0.9217, best val acc: 0.9379, best train loss: 0.2334  @ epoch 4822 )\n",
      "[Epoch: 5000] train loss: 0.3015, train acc: 0.8966, val loss: 0.2538, val acc: 0.9110  (best train acc: 0.9217, best val acc: 0.9379, best train loss: 0.2334  @ epoch 4822 )\n",
      "[Epoch: 5020] train loss: 0.2421, train acc: 0.9130, val loss: 0.2082, val acc: 0.9336  (best train acc: 0.9217, best val acc: 0.9379, best train loss: 0.2334  @ epoch 4822 )\n",
      "[Epoch: 5040] train loss: 0.2408, train acc: 0.9145, val loss: 0.2062, val acc: 0.9356  (best train acc: 0.9217, best val acc: 0.9386, best train loss: 0.2315  @ epoch 5033 )\n",
      "[Epoch: 5060] train loss: 0.2367, train acc: 0.9166, val loss: 0.2059, val acc: 0.9369  (best train acc: 0.9217, best val acc: 0.9386, best train loss: 0.2291  @ epoch 5059 )\n",
      "[Epoch: 5080] train loss: 0.2461, train acc: 0.9105, val loss: 0.2092, val acc: 0.9339  (best train acc: 0.9217, best val acc: 0.9386, best train loss: 0.2291  @ epoch 5059 )\n",
      "[Epoch: 5100] train loss: 0.2445, train acc: 0.9074, val loss: 0.2322, val acc: 0.9184  (best train acc: 0.9217, best val acc: 0.9386, best train loss: 0.2291  @ epoch 5059 )\n",
      "[Epoch: 5120] train loss: 0.2801, train acc: 0.9020, val loss: 0.2215, val acc: 0.9298  (best train acc: 0.9217, best val acc: 0.9386, best train loss: 0.2291  @ epoch 5059 )\n",
      "[Epoch: 5140] train loss: 0.2554, train acc: 0.9077, val loss: 0.2040, val acc: 0.9383  (best train acc: 0.9217, best val acc: 0.9386, best train loss: 0.2291  @ epoch 5059 )\n",
      "[Epoch: 5160] train loss: 0.2575, train acc: 0.9075, val loss: 0.2228, val acc: 0.9288  (best train acc: 0.9217, best val acc: 0.9386, best train loss: 0.2291  @ epoch 5059 )\n",
      "[Epoch: 5180] train loss: 0.2346, train acc: 0.9182, val loss: 0.2122, val acc: 0.9315  (best train acc: 0.9217, best val acc: 0.9386, best train loss: 0.2291  @ epoch 5059 )\n",
      "[Epoch: 5200] train loss: 0.2401, train acc: 0.9143, val loss: 0.2102, val acc: 0.9349  (best train acc: 0.9217, best val acc: 0.9386, best train loss: 0.2285  @ epoch 5197 )\n",
      "[Epoch: 5220] train loss: 0.2628, train acc: 0.9098, val loss: 0.2161, val acc: 0.9305  (best train acc: 0.9217, best val acc: 0.9386, best train loss: 0.2285  @ epoch 5197 )\n",
      "[Epoch: 5240] train loss: 0.2332, train acc: 0.9169, val loss: 0.2198, val acc: 0.9309  (best train acc: 0.9217, best val acc: 0.9390, best train loss: 0.2285  @ epoch 5197 )\n",
      "[Epoch: 5260] train loss: 0.2419, train acc: 0.9118, val loss: 0.2173, val acc: 0.9356  (best train acc: 0.9217, best val acc: 0.9390, best train loss: 0.2285  @ epoch 5197 )\n",
      "[Epoch: 5280] train loss: 0.2435, train acc: 0.9148, val loss: 0.2093, val acc: 0.9349  (best train acc: 0.9217, best val acc: 0.9393, best train loss: 0.2284  @ epoch 5277 )\n",
      "[Epoch: 5300] train loss: 0.2370, train acc: 0.9168, val loss: 0.2114, val acc: 0.9315  (best train acc: 0.9218, best val acc: 0.9393, best train loss: 0.2266  @ epoch 5288 )\n",
      "[Epoch: 5320] train loss: 0.2353, train acc: 0.9166, val loss: 0.2129, val acc: 0.9356  (best train acc: 0.9218, best val acc: 0.9393, best train loss: 0.2266  @ epoch 5288 )\n",
      "[Epoch: 5340] train loss: 0.2407, train acc: 0.9136, val loss: 0.2148, val acc: 0.9349  (best train acc: 0.9218, best val acc: 0.9393, best train loss: 0.2259  @ epoch 5337 )\n",
      "[Epoch: 5360] train loss: 0.2562, train acc: 0.9091, val loss: 0.2175, val acc: 0.9288  (best train acc: 0.9218, best val acc: 0.9393, best train loss: 0.2259  @ epoch 5337 )\n",
      "[Epoch: 5380] train loss: 0.2622, train acc: 0.9023, val loss: 0.2102, val acc: 0.9329  (best train acc: 0.9218, best val acc: 0.9393, best train loss: 0.2259  @ epoch 5337 )\n",
      "[Epoch: 5400] train loss: 0.2400, train acc: 0.9160, val loss: 0.2123, val acc: 0.9352  (best train acc: 0.9218, best val acc: 0.9393, best train loss: 0.2259  @ epoch 5337 )\n",
      "[Epoch: 5420] train loss: 0.2275, train acc: 0.9189, val loss: 0.2084, val acc: 0.9379  (best train acc: 0.9218, best val acc: 0.9400, best train loss: 0.2259  @ epoch 5337 )\n",
      "[Epoch: 5440] train loss: 0.2332, train acc: 0.9211, val loss: 0.2071, val acc: 0.9376  (best train acc: 0.9218, best val acc: 0.9413, best train loss: 0.2234  @ epoch 5439 )\n",
      "[Epoch: 5460] train loss: 0.2582, train acc: 0.9039, val loss: 0.2070, val acc: 0.9376  (best train acc: 0.9245, best val acc: 0.9413, best train loss: 0.2215  @ epoch 5451 )\n",
      "[Epoch: 5480] train loss: 0.2296, train acc: 0.9206, val loss: 0.2084, val acc: 0.9349  (best train acc: 0.9245, best val acc: 0.9413, best train loss: 0.2215  @ epoch 5451 )\n",
      "[Epoch: 5500] train loss: 0.2279, train acc: 0.9181, val loss: 0.2053, val acc: 0.9379  (best train acc: 0.9245, best val acc: 0.9413, best train loss: 0.2215  @ epoch 5451 )\n",
      "[Epoch: 5520] train loss: 0.2417, train acc: 0.9157, val loss: 0.2148, val acc: 0.9322  (best train acc: 0.9245, best val acc: 0.9417, best train loss: 0.2187  @ epoch 5508 )\n",
      "[Epoch: 5540] train loss: 0.2353, train acc: 0.9155, val loss: 0.2128, val acc: 0.9315  (best train acc: 0.9245, best val acc: 0.9417, best train loss: 0.2187  @ epoch 5508 )\n",
      "[Epoch: 5560] train loss: 0.2290, train acc: 0.9161, val loss: 0.2006, val acc: 0.9369  (best train acc: 0.9245, best val acc: 0.9417, best train loss: 0.2187  @ epoch 5508 )\n",
      "[Epoch: 5580] train loss: 0.2432, train acc: 0.9177, val loss: 0.2058, val acc: 0.9359  (best train acc: 0.9245, best val acc: 0.9417, best train loss: 0.2187  @ epoch 5508 )\n",
      "[Epoch: 5600] train loss: 0.2322, train acc: 0.9162, val loss: 0.2009, val acc: 0.9390  (best train acc: 0.9245, best val acc: 0.9417, best train loss: 0.2187  @ epoch 5508 )\n",
      "[Epoch: 5620] train loss: 0.2413, train acc: 0.9143, val loss: 0.2072, val acc: 0.9352  (best train acc: 0.9245, best val acc: 0.9417, best train loss: 0.2187  @ epoch 5508 )\n",
      "[Epoch: 5640] train loss: 0.2254, train acc: 0.9240, val loss: 0.2069, val acc: 0.9393  (best train acc: 0.9245, best val acc: 0.9417, best train loss: 0.2187  @ epoch 5508 )\n",
      "[Epoch: 5660] train loss: 0.2395, train acc: 0.9147, val loss: 0.2047, val acc: 0.9379  (best train acc: 0.9245, best val acc: 0.9417, best train loss: 0.2187  @ epoch 5508 )\n",
      "[Epoch: 5680] train loss: 0.2251, train acc: 0.9221, val loss: 0.1996, val acc: 0.9413  (best train acc: 0.9248, best val acc: 0.9417, best train loss: 0.2187  @ epoch 5668 )\n",
      "[Epoch: 5700] train loss: 0.2336, train acc: 0.9178, val loss: 0.2073, val acc: 0.9396  (best train acc: 0.9248, best val acc: 0.9420, best train loss: 0.2187  @ epoch 5668 )\n",
      "[Epoch: 5720] train loss: 0.2222, train acc: 0.9207, val loss: 0.2115, val acc: 0.9349  (best train acc: 0.9260, best val acc: 0.9420, best train loss: 0.2144  @ epoch 5702 )\n",
      "[Epoch: 5740] train loss: 0.2400, train acc: 0.9152, val loss: 0.2072, val acc: 0.9356  (best train acc: 0.9260, best val acc: 0.9420, best train loss: 0.2144  @ epoch 5702 )\n",
      "[Epoch: 5760] train loss: 0.2445, train acc: 0.9116, val loss: 0.2052, val acc: 0.9423  (best train acc: 0.9260, best val acc: 0.9423, best train loss: 0.2144  @ epoch 5702 )\n",
      "[Epoch: 5780] train loss: 0.2367, train acc: 0.9132, val loss: 0.2067, val acc: 0.9400  (best train acc: 0.9260, best val acc: 0.9423, best train loss: 0.2144  @ epoch 5702 )\n",
      "[Epoch: 5800] train loss: 0.2233, train acc: 0.9229, val loss: 0.2162, val acc: 0.9339  (best train acc: 0.9260, best val acc: 0.9423, best train loss: 0.2144  @ epoch 5702 )\n",
      "[Epoch: 5820] train loss: 0.2447, train acc: 0.9066, val loss: 0.2093, val acc: 0.9359  (best train acc: 0.9260, best val acc: 0.9423, best train loss: 0.2144  @ epoch 5702 )\n",
      "[Epoch: 5840] train loss: 0.2321, train acc: 0.9212, val loss: 0.2156, val acc: 0.9373  (best train acc: 0.9260, best val acc: 0.9423, best train loss: 0.2144  @ epoch 5702 )\n",
      "[Epoch: 5860] train loss: 0.2329, train acc: 0.9156, val loss: 0.2303, val acc: 0.9292  (best train acc: 0.9260, best val acc: 0.9423, best train loss: 0.2144  @ epoch 5702 )\n",
      "[Epoch: 5880] train loss: 0.2405, train acc: 0.9155, val loss: 0.2097, val acc: 0.9400  (best train acc: 0.9260, best val acc: 0.9423, best train loss: 0.2144  @ epoch 5702 )\n",
      "[Epoch: 5900] train loss: 0.2327, train acc: 0.9160, val loss: 0.2071, val acc: 0.9393  (best train acc: 0.9270, best val acc: 0.9423, best train loss: 0.2142  @ epoch 5889 )\n",
      "[Epoch: 5920] train loss: 0.2162, train acc: 0.9218, val loss: 0.2083, val acc: 0.9390  (best train acc: 0.9270, best val acc: 0.9433, best train loss: 0.2142  @ epoch 5889 )\n",
      "[Epoch: 5940] train loss: 0.2184, train acc: 0.9217, val loss: 0.2050, val acc: 0.9433  (best train acc: 0.9270, best val acc: 0.9433, best train loss: 0.2142  @ epoch 5889 )\n",
      "[Epoch: 5960] train loss: 0.2355, train acc: 0.9169, val loss: 0.2228, val acc: 0.9305  (best train acc: 0.9270, best val acc: 0.9433, best train loss: 0.2142  @ epoch 5889 )\n",
      "[Epoch: 5980] train loss: 0.2257, train acc: 0.9186, val loss: 0.2223, val acc: 0.9309  (best train acc: 0.9270, best val acc: 0.9433, best train loss: 0.2142  @ epoch 5889 )\n",
      "[Epoch: 6000] train loss: 0.2300, train acc: 0.9214, val loss: 0.2075, val acc: 0.9413  (best train acc: 0.9270, best val acc: 0.9433, best train loss: 0.2142  @ epoch 5889 )\n",
      "[Epoch: 6020] train loss: 0.2158, train acc: 0.9263, val loss: 0.2154, val acc: 0.9329  (best train acc: 0.9270, best val acc: 0.9433, best train loss: 0.2142  @ epoch 5889 )\n",
      "[Epoch: 6040] train loss: 0.2286, train acc: 0.9174, val loss: 0.2124, val acc: 0.9379  (best train acc: 0.9270, best val acc: 0.9433, best train loss: 0.2142  @ epoch 5889 )\n",
      "[Epoch: 6060] train loss: 0.2138, train acc: 0.9239, val loss: 0.2066, val acc: 0.9393  (best train acc: 0.9277, best val acc: 0.9433, best train loss: 0.2079  @ epoch 6059 )\n",
      "[Epoch: 6080] train loss: 0.2284, train acc: 0.9184, val loss: 0.2090, val acc: 0.9386  (best train acc: 0.9293, best val acc: 0.9440, best train loss: 0.2052  @ epoch 6062 )\n",
      "[Epoch: 6100] train loss: 0.2347, train acc: 0.9161, val loss: 0.2074, val acc: 0.9403  (best train acc: 0.9293, best val acc: 0.9440, best train loss: 0.2052  @ epoch 6062 )\n",
      "[Epoch: 6120] train loss: 0.2313, train acc: 0.9171, val loss: 0.2335, val acc: 0.9309  (best train acc: 0.9293, best val acc: 0.9440, best train loss: 0.2052  @ epoch 6062 )\n",
      "[Epoch: 6140] train loss: 0.2204, train acc: 0.9211, val loss: 0.2135, val acc: 0.9413  (best train acc: 0.9293, best val acc: 0.9440, best train loss: 0.2052  @ epoch 6062 )\n",
      "[Epoch: 6160] train loss: 0.2194, train acc: 0.9223, val loss: 0.2522, val acc: 0.9248  (best train acc: 0.9293, best val acc: 0.9440, best train loss: 0.2052  @ epoch 6062 )\n",
      "[Epoch: 6180] train loss: 0.2182, train acc: 0.9242, val loss: 0.2130, val acc: 0.9396  (best train acc: 0.9293, best val acc: 0.9440, best train loss: 0.2052  @ epoch 6062 )\n",
      "[Epoch: 6200] train loss: 0.2217, train acc: 0.9202, val loss: 0.2092, val acc: 0.9437  (best train acc: 0.9293, best val acc: 0.9440, best train loss: 0.2052  @ epoch 6062 )\n",
      "[Epoch: 6220] train loss: 0.2170, train acc: 0.9239, val loss: 0.2061, val acc: 0.9433  (best train acc: 0.9293, best val acc: 0.9440, best train loss: 0.2052  @ epoch 6062 )\n",
      "[Epoch: 6240] train loss: 0.2215, train acc: 0.9239, val loss: 0.2092, val acc: 0.9427  (best train acc: 0.9294, best val acc: 0.9440, best train loss: 0.2052  @ epoch 6062 )\n",
      "[Epoch: 6260] train loss: 0.2193, train acc: 0.9236, val loss: 0.2155, val acc: 0.9373  (best train acc: 0.9300, best val acc: 0.9440, best train loss: 0.2025  @ epoch 6247 )\n",
      "[Epoch: 6280] train loss: 0.2078, train acc: 0.9260, val loss: 0.2116, val acc: 0.9386  (best train acc: 0.9300, best val acc: 0.9440, best train loss: 0.2025  @ epoch 6247 )\n",
      "[Epoch: 6300] train loss: 0.2358, train acc: 0.9114, val loss: 0.2077, val acc: 0.9363  (best train acc: 0.9300, best val acc: 0.9440, best train loss: 0.2025  @ epoch 6247 )\n",
      "[Epoch: 6320] train loss: 0.2479, train acc: 0.9091, val loss: 0.2124, val acc: 0.9413  (best train acc: 0.9300, best val acc: 0.9440, best train loss: 0.2025  @ epoch 6247 )\n",
      "[Epoch: 6340] train loss: 0.2193, train acc: 0.9242, val loss: 0.2061, val acc: 0.9423  (best train acc: 0.9300, best val acc: 0.9440, best train loss: 0.2025  @ epoch 6247 )\n",
      "[Epoch: 6360] train loss: 0.2125, train acc: 0.9249, val loss: 0.2095, val acc: 0.9413  (best train acc: 0.9305, best val acc: 0.9454, best train loss: 0.2009  @ epoch 6353 )\n",
      "[Epoch: 6380] train loss: 0.2188, train acc: 0.9223, val loss: 0.2041, val acc: 0.9437  (best train acc: 0.9305, best val acc: 0.9454, best train loss: 0.2009  @ epoch 6353 )\n",
      "[Epoch: 6400] train loss: 0.2261, train acc: 0.9174, val loss: 0.2052, val acc: 0.9410  (best train acc: 0.9305, best val acc: 0.9454, best train loss: 0.2009  @ epoch 6353 )\n",
      "[Epoch: 6420] train loss: 0.2379, train acc: 0.9187, val loss: 0.2408, val acc: 0.9258  (best train acc: 0.9305, best val acc: 0.9454, best train loss: 0.2009  @ epoch 6353 )\n",
      "[Epoch: 6440] train loss: 0.2252, train acc: 0.9213, val loss: 0.2130, val acc: 0.9332  (best train acc: 0.9305, best val acc: 0.9454, best train loss: 0.2009  @ epoch 6353 )\n",
      "[Epoch: 6460] train loss: 0.2097, train acc: 0.9268, val loss: 0.2028, val acc: 0.9400  (best train acc: 0.9305, best val acc: 0.9454, best train loss: 0.2009  @ epoch 6353 )\n",
      "[Epoch: 6480] train loss: 0.2381, train acc: 0.9102, val loss: 0.2200, val acc: 0.9339  (best train acc: 0.9305, best val acc: 0.9454, best train loss: 0.2009  @ epoch 6353 )\n",
      "[Epoch: 6500] train loss: 0.2162, train acc: 0.9228, val loss: 0.2081, val acc: 0.9427  (best train acc: 0.9305, best val acc: 0.9454, best train loss: 0.2009  @ epoch 6353 )\n",
      "[Epoch: 6520] train loss: 0.2606, train acc: 0.9064, val loss: 0.2286, val acc: 0.9268  (best train acc: 0.9318, best val acc: 0.9454, best train loss: 0.2009  @ epoch 6353 )\n",
      "[Epoch: 6540] train loss: 0.2137, train acc: 0.9268, val loss: 0.2148, val acc: 0.9376  (best train acc: 0.9318, best val acc: 0.9454, best train loss: 0.2009  @ epoch 6353 )\n",
      "[Epoch: 6560] train loss: 0.2241, train acc: 0.9195, val loss: 0.2290, val acc: 0.9302  (best train acc: 0.9318, best val acc: 0.9454, best train loss: 0.1997  @ epoch 6546 )\n",
      "[Epoch: 6580] train loss: 0.2264, train acc: 0.9216, val loss: 0.2049, val acc: 0.9447  (best train acc: 0.9318, best val acc: 0.9454, best train loss: 0.1997  @ epoch 6546 )\n",
      "[Epoch: 6600] train loss: 0.2105, train acc: 0.9258, val loss: 0.2208, val acc: 0.9373  (best train acc: 0.9318, best val acc: 0.9454, best train loss: 0.1997  @ epoch 6546 )\n",
      "[Epoch: 6620] train loss: 0.2070, train acc: 0.9286, val loss: 0.2092, val acc: 0.9413  (best train acc: 0.9318, best val acc: 0.9454, best train loss: 0.1997  @ epoch 6546 )\n",
      "[Epoch: 6640] train loss: 0.2017, train acc: 0.9285, val loss: 0.2065, val acc: 0.9437  (best train acc: 0.9336, best val acc: 0.9454, best train loss: 0.1974  @ epoch 6638 )\n",
      "[Epoch: 6660] train loss: 0.2117, train acc: 0.9256, val loss: 0.2071, val acc: 0.9423  (best train acc: 0.9336, best val acc: 0.9454, best train loss: 0.1961  @ epoch 6655 )\n",
      "[Epoch: 6680] train loss: 0.2064, train acc: 0.9281, val loss: 0.2066, val acc: 0.9410  (best train acc: 0.9336, best val acc: 0.9454, best train loss: 0.1961  @ epoch 6655 )\n",
      "[Epoch: 6700] train loss: 0.2021, train acc: 0.9291, val loss: 0.2130, val acc: 0.9410  (best train acc: 0.9336, best val acc: 0.9454, best train loss: 0.1961  @ epoch 6655 )\n",
      "[Epoch: 6720] train loss: 0.1962, train acc: 0.9346, val loss: 0.2117, val acc: 0.9420  (best train acc: 0.9346, best val acc: 0.9454, best train loss: 0.1961  @ epoch 6655 )\n",
      "[Epoch: 6740] train loss: 0.2101, train acc: 0.9284, val loss: 0.2054, val acc: 0.9427  (best train acc: 0.9346, best val acc: 0.9454, best train loss: 0.1961  @ epoch 6655 )\n",
      "[Epoch: 6760] train loss: 0.1978, train acc: 0.9320, val loss: 0.2110, val acc: 0.9413  (best train acc: 0.9346, best val acc: 0.9454, best train loss: 0.1945  @ epoch 6747 )\n",
      "[Epoch: 6780] train loss: 0.2147, train acc: 0.9233, val loss: 0.2063, val acc: 0.9413  (best train acc: 0.9346, best val acc: 0.9454, best train loss: 0.1945  @ epoch 6747 )\n",
      "[Epoch: 6800] train loss: 0.2503, train acc: 0.9162, val loss: 0.2225, val acc: 0.9352  (best train acc: 0.9346, best val acc: 0.9454, best train loss: 0.1945  @ epoch 6747 )\n",
      "[Epoch: 6820] train loss: 0.2873, train acc: 0.9041, val loss: 0.2401, val acc: 0.9201  (best train acc: 0.9346, best val acc: 0.9454, best train loss: 0.1945  @ epoch 6747 )\n",
      "[Epoch: 6840] train loss: 0.2270, train acc: 0.9189, val loss: 0.2126, val acc: 0.9400  (best train acc: 0.9346, best val acc: 0.9454, best train loss: 0.1945  @ epoch 6747 )\n",
      "[Epoch: 6860] train loss: 0.2258, train acc: 0.9171, val loss: 0.2056, val acc: 0.9427  (best train acc: 0.9346, best val acc: 0.9454, best train loss: 0.1945  @ epoch 6747 )\n",
      "[Epoch: 6880] train loss: 0.2197, train acc: 0.9217, val loss: 0.2233, val acc: 0.9400  (best train acc: 0.9346, best val acc: 0.9454, best train loss: 0.1945  @ epoch 6747 )\n",
      "[Epoch: 6900] train loss: 0.2081, train acc: 0.9250, val loss: 0.2116, val acc: 0.9420  (best train acc: 0.9346, best val acc: 0.9457, best train loss: 0.1945  @ epoch 6747 )\n",
      "[Epoch: 6920] train loss: 0.2159, train acc: 0.9256, val loss: 0.2070, val acc: 0.9396  (best train acc: 0.9346, best val acc: 0.9457, best train loss: 0.1945  @ epoch 6747 )\n",
      "[Epoch: 6940] train loss: 0.2100, train acc: 0.9237, val loss: 0.2091, val acc: 0.9437  (best train acc: 0.9346, best val acc: 0.9457, best train loss: 0.1945  @ epoch 6747 )\n",
      "[Epoch: 6960] train loss: 0.2259, train acc: 0.9203, val loss: 0.2186, val acc: 0.9390  (best train acc: 0.9346, best val acc: 0.9457, best train loss: 0.1945  @ epoch 6747 )\n",
      "[Epoch: 6980] train loss: 0.2167, train acc: 0.9206, val loss: 0.2079, val acc: 0.9433  (best train acc: 0.9346, best val acc: 0.9457, best train loss: 0.1945  @ epoch 6747 )\n",
      "[Epoch: 7000] train loss: 0.1972, train acc: 0.9313, val loss: 0.2038, val acc: 0.9413  (best train acc: 0.9352, best val acc: 0.9457, best train loss: 0.1916  @ epoch 6996 )\n",
      "[Epoch: 7020] train loss: 0.1999, train acc: 0.9315, val loss: 0.2068, val acc: 0.9410  (best train acc: 0.9352, best val acc: 0.9457, best train loss: 0.1886  @ epoch 7015 )\n",
      "[Epoch: 7040] train loss: 0.2142, train acc: 0.9258, val loss: 0.2088, val acc: 0.9386  (best train acc: 0.9352, best val acc: 0.9460, best train loss: 0.1886  @ epoch 7015 )\n",
      "[Epoch: 7060] train loss: 0.2008, train acc: 0.9286, val loss: 0.2075, val acc: 0.9417  (best train acc: 0.9352, best val acc: 0.9460, best train loss: 0.1886  @ epoch 7015 )\n",
      "[Epoch: 7080] train loss: 0.2033, train acc: 0.9292, val loss: 0.2055, val acc: 0.9450  (best train acc: 0.9355, best val acc: 0.9460, best train loss: 0.1886  @ epoch 7015 )\n",
      "[Epoch: 7100] train loss: 0.2036, train acc: 0.9290, val loss: 0.2071, val acc: 0.9410  (best train acc: 0.9355, best val acc: 0.9460, best train loss: 0.1886  @ epoch 7015 )\n",
      "[Epoch: 7120] train loss: 0.2059, train acc: 0.9252, val loss: 0.2064, val acc: 0.9444  (best train acc: 0.9355, best val acc: 0.9460, best train loss: 0.1886  @ epoch 7015 )\n",
      "[Epoch: 7140] train loss: 0.1931, train acc: 0.9341, val loss: 0.2077, val acc: 0.9433  (best train acc: 0.9365, best val acc: 0.9460, best train loss: 0.1847  @ epoch 7132 )\n",
      "[Epoch: 7160] train loss: 0.2269, train acc: 0.9142, val loss: 0.3130, val acc: 0.9073  (best train acc: 0.9365, best val acc: 0.9460, best train loss: 0.1847  @ epoch 7132 )\n",
      "[Epoch: 7180] train loss: 0.2342, train acc: 0.9182, val loss: 0.2166, val acc: 0.9383  (best train acc: 0.9365, best val acc: 0.9460, best train loss: 0.1847  @ epoch 7132 )\n",
      "[Epoch: 7200] train loss: 0.2174, train acc: 0.9268, val loss: 0.2069, val acc: 0.9440  (best train acc: 0.9365, best val acc: 0.9460, best train loss: 0.1847  @ epoch 7132 )\n",
      "[Epoch: 7220] train loss: 0.1947, train acc: 0.9310, val loss: 0.2228, val acc: 0.9403  (best train acc: 0.9365, best val acc: 0.9460, best train loss: 0.1847  @ epoch 7132 )\n",
      "[Epoch: 7240] train loss: 0.2195, train acc: 0.9255, val loss: 0.2063, val acc: 0.9386  (best train acc: 0.9372, best val acc: 0.9460, best train loss: 0.1847  @ epoch 7132 )\n",
      "[Epoch: 7260] train loss: 0.2057, train acc: 0.9297, val loss: 0.2263, val acc: 0.9352  (best train acc: 0.9372, best val acc: 0.9460, best train loss: 0.1847  @ epoch 7132 )\n",
      "[Epoch: 7280] train loss: 0.2048, train acc: 0.9290, val loss: 0.2135, val acc: 0.9403  (best train acc: 0.9372, best val acc: 0.9467, best train loss: 0.1847  @ epoch 7132 )\n",
      "[Epoch: 7300] train loss: 0.1969, train acc: 0.9315, val loss: 0.2305, val acc: 0.9275  (best train acc: 0.9372, best val acc: 0.9467, best train loss: 0.1847  @ epoch 7132 )\n",
      "[Epoch: 7320] train loss: 0.2011, train acc: 0.9307, val loss: 0.2050, val acc: 0.9427  (best train acc: 0.9372, best val acc: 0.9467, best train loss: 0.1847  @ epoch 7132 )\n",
      "[Epoch: 7340] train loss: 0.1978, train acc: 0.9310, val loss: 0.2275, val acc: 0.9342  (best train acc: 0.9372, best val acc: 0.9467, best train loss: 0.1847  @ epoch 7132 )\n",
      "[Epoch: 7360] train loss: 0.2083, train acc: 0.9282, val loss: 0.2232, val acc: 0.9379  (best train acc: 0.9372, best val acc: 0.9467, best train loss: 0.1847  @ epoch 7132 )\n",
      "[Epoch: 7380] train loss: 0.1969, train acc: 0.9313, val loss: 0.2125, val acc: 0.9437  (best train acc: 0.9372, best val acc: 0.9467, best train loss: 0.1847  @ epoch 7132 )\n",
      "[Epoch: 7400] train loss: 0.1963, train acc: 0.9315, val loss: 0.2071, val acc: 0.9440  (best train acc: 0.9372, best val acc: 0.9467, best train loss: 0.1847  @ epoch 7132 )\n",
      "[Epoch: 7420] train loss: 0.1922, train acc: 0.9353, val loss: 0.2362, val acc: 0.9332  (best train acc: 0.9372, best val acc: 0.9467, best train loss: 0.1847  @ epoch 7132 )\n",
      "[Epoch: 7440] train loss: 0.2523, train acc: 0.9017, val loss: 0.2218, val acc: 0.9332  (best train acc: 0.9372, best val acc: 0.9467, best train loss: 0.1847  @ epoch 7132 )\n",
      "[Epoch: 7460] train loss: 0.2088, train acc: 0.9273, val loss: 0.2066, val acc: 0.9433  (best train acc: 0.9372, best val acc: 0.9467, best train loss: 0.1847  @ epoch 7132 )\n",
      "[Epoch: 7480] train loss: 0.1902, train acc: 0.9368, val loss: 0.2126, val acc: 0.9420  (best train acc: 0.9372, best val acc: 0.9467, best train loss: 0.1845  @ epoch 7469 )\n",
      "[Epoch: 7500] train loss: 0.1962, train acc: 0.9298, val loss: 0.2104, val acc: 0.9376  (best train acc: 0.9372, best val acc: 0.9467, best train loss: 0.1845  @ epoch 7469 )\n",
      "[Epoch: 7520] train loss: 0.2120, train acc: 0.9217, val loss: 0.2031, val acc: 0.9450  (best train acc: 0.9375, best val acc: 0.9467, best train loss: 0.1845  @ epoch 7469 )\n",
      "[Epoch: 7540] train loss: 0.1852, train acc: 0.9359, val loss: 0.2087, val acc: 0.9427  (best train acc: 0.9381, best val acc: 0.9467, best train loss: 0.1845  @ epoch 7469 )\n",
      "[Epoch: 7560] train loss: 0.1884, train acc: 0.9362, val loss: 0.2085, val acc: 0.9444  (best train acc: 0.9382, best val acc: 0.9467, best train loss: 0.1845  @ epoch 7555 )\n",
      "[Epoch: 7580] train loss: 0.1947, train acc: 0.9320, val loss: 0.2055, val acc: 0.9460  (best train acc: 0.9382, best val acc: 0.9467, best train loss: 0.1843  @ epoch 7563 )\n",
      "[Epoch: 7600] train loss: 0.2108, train acc: 0.9217, val loss: 0.2154, val acc: 0.9386  (best train acc: 0.9382, best val acc: 0.9467, best train loss: 0.1842  @ epoch 7581 )\n",
      "[Epoch: 7620] train loss: 0.2059, train acc: 0.9231, val loss: 0.2072, val acc: 0.9444  (best train acc: 0.9382, best val acc: 0.9467, best train loss: 0.1842  @ epoch 7581 )\n",
      "[Epoch: 7640] train loss: 0.1956, train acc: 0.9325, val loss: 0.2039, val acc: 0.9454  (best train acc: 0.9382, best val acc: 0.9474, best train loss: 0.1830  @ epoch 7626 )\n",
      "[Epoch: 7660] train loss: 0.2052, train acc: 0.9305, val loss: 0.2020, val acc: 0.9447  (best train acc: 0.9387, best val acc: 0.9474, best train loss: 0.1824  @ epoch 7659 )\n",
      "[Epoch: 7680] train loss: 0.1946, train acc: 0.9313, val loss: 0.2135, val acc: 0.9406  (best train acc: 0.9392, best val acc: 0.9477, best train loss: 0.1824  @ epoch 7659 )\n",
      "[Epoch: 7700] train loss: 0.2087, train acc: 0.9271, val loss: 0.2188, val acc: 0.9373  (best train acc: 0.9392, best val acc: 0.9477, best train loss: 0.1821  @ epoch 7682 )\n",
      "[Epoch: 7720] train loss: 0.2166, train acc: 0.9270, val loss: 0.2142, val acc: 0.9406  (best train acc: 0.9392, best val acc: 0.9477, best train loss: 0.1821  @ epoch 7682 )\n",
      "[Epoch: 7740] train loss: 0.2601, train acc: 0.8968, val loss: 0.2783, val acc: 0.9089  (best train acc: 0.9392, best val acc: 0.9477, best train loss: 0.1821  @ epoch 7682 )\n",
      "[Epoch: 7760] train loss: 0.2066, train acc: 0.9307, val loss: 0.2040, val acc: 0.9437  (best train acc: 0.9392, best val acc: 0.9481, best train loss: 0.1821  @ epoch 7682 )\n",
      "[Epoch: 7780] train loss: 0.1980, train acc: 0.9325, val loss: 0.2026, val acc: 0.9444  (best train acc: 0.9392, best val acc: 0.9481, best train loss: 0.1821  @ epoch 7682 )\n",
      "[Epoch: 7800] train loss: 0.1963, train acc: 0.9286, val loss: 0.2463, val acc: 0.9278  (best train acc: 0.9392, best val acc: 0.9494, best train loss: 0.1804  @ epoch 7782 )\n",
      "[Epoch: 7820] train loss: 0.2143, train acc: 0.9198, val loss: 0.2013, val acc: 0.9491  (best train acc: 0.9392, best val acc: 0.9494, best train loss: 0.1804  @ epoch 7782 )\n",
      "[Epoch: 7840] train loss: 0.2218, train acc: 0.9162, val loss: 0.2157, val acc: 0.9383  (best train acc: 0.9392, best val acc: 0.9494, best train loss: 0.1804  @ epoch 7782 )\n",
      "[Epoch: 7860] train loss: 0.1988, train acc: 0.9268, val loss: 0.2231, val acc: 0.9373  (best train acc: 0.9392, best val acc: 0.9494, best train loss: 0.1804  @ epoch 7782 )\n",
      "[Epoch: 7880] train loss: 0.2032, train acc: 0.9325, val loss: 0.2129, val acc: 0.9437  (best train acc: 0.9392, best val acc: 0.9494, best train loss: 0.1804  @ epoch 7782 )\n",
      "[Epoch: 7900] train loss: 0.1909, train acc: 0.9338, val loss: 0.2043, val acc: 0.9457  (best train acc: 0.9392, best val acc: 0.9494, best train loss: 0.1804  @ epoch 7782 )\n",
      "[Epoch: 7920] train loss: 0.1832, train acc: 0.9385, val loss: 0.2008, val acc: 0.9457  (best train acc: 0.9404, best val acc: 0.9494, best train loss: 0.1799  @ epoch 7918 )\n",
      "[Epoch: 7940] train loss: 0.1780, train acc: 0.9413, val loss: 0.2046, val acc: 0.9460  (best train acc: 0.9413, best val acc: 0.9504, best train loss: 0.1780  @ epoch 7940 )\n",
      "[Epoch: 7960] train loss: 0.1855, train acc: 0.9396, val loss: 0.2016, val acc: 0.9470  (best train acc: 0.9413, best val acc: 0.9504, best train loss: 0.1780  @ epoch 7940 )\n",
      "[Epoch: 7980] train loss: 0.1818, train acc: 0.9376, val loss: 0.2048, val acc: 0.9457  (best train acc: 0.9413, best val acc: 0.9504, best train loss: 0.1780  @ epoch 7940 )\n",
      "[Epoch: 8000] train loss: 0.2142, train acc: 0.9269, val loss: 0.2021, val acc: 0.9484  (best train acc: 0.9413, best val acc: 0.9504, best train loss: 0.1780  @ epoch 7940 )\n",
      "[Epoch: 8020] train loss: 0.1829, train acc: 0.9359, val loss: 0.2069, val acc: 0.9450  (best train acc: 0.9413, best val acc: 0.9508, best train loss: 0.1747  @ epoch 8007 )\n",
      "[Epoch: 8040] train loss: 0.1930, train acc: 0.9367, val loss: 0.2020, val acc: 0.9467  (best train acc: 0.9426, best val acc: 0.9508, best train loss: 0.1728  @ epoch 8039 )\n",
      "[Epoch: 8060] train loss: 0.1754, train acc: 0.9383, val loss: 0.2072, val acc: 0.9454  (best train acc: 0.9426, best val acc: 0.9508, best train loss: 0.1728  @ epoch 8039 )\n",
      "[Epoch: 8080] train loss: 0.2208, train acc: 0.9296, val loss: 0.2159, val acc: 0.9383  (best train acc: 0.9426, best val acc: 0.9508, best train loss: 0.1728  @ epoch 8039 )\n",
      "[Epoch: 8100] train loss: 0.1860, train acc: 0.9385, val loss: 0.2021, val acc: 0.9474  (best train acc: 0.9426, best val acc: 0.9508, best train loss: 0.1728  @ epoch 8039 )\n",
      "[Epoch: 8120] train loss: 0.1825, train acc: 0.9373, val loss: 0.2078, val acc: 0.9464  (best train acc: 0.9426, best val acc: 0.9508, best train loss: 0.1728  @ epoch 8039 )\n",
      "[Epoch: 8140] train loss: 0.1969, train acc: 0.9331, val loss: 0.2051, val acc: 0.9474  (best train acc: 0.9429, best val acc: 0.9508, best train loss: 0.1717  @ epoch 8134 )\n",
      "[Epoch: 8160] train loss: 0.1986, train acc: 0.9320, val loss: 0.1988, val acc: 0.9484  (best train acc: 0.9429, best val acc: 0.9508, best train loss: 0.1717  @ epoch 8134 )\n",
      "[Epoch: 8180] train loss: 0.1925, train acc: 0.9320, val loss: 0.2054, val acc: 0.9464  (best train acc: 0.9429, best val acc: 0.9508, best train loss: 0.1717  @ epoch 8134 )\n",
      "[Epoch: 8200] train loss: 0.1867, train acc: 0.9357, val loss: 0.2089, val acc: 0.9477  (best train acc: 0.9429, best val acc: 0.9518, best train loss: 0.1717  @ epoch 8134 )\n",
      "[Epoch: 8220] train loss: 0.1833, train acc: 0.9376, val loss: 0.2042, val acc: 0.9491  (best train acc: 0.9429, best val acc: 0.9518, best train loss: 0.1712  @ epoch 8205 )\n",
      "[Epoch: 8240] train loss: 0.1865, train acc: 0.9381, val loss: 0.2061, val acc: 0.9477  (best train acc: 0.9432, best val acc: 0.9518, best train loss: 0.1712  @ epoch 8205 )\n",
      "[Epoch: 8260] train loss: 0.2976, train acc: 0.8833, val loss: 0.2718, val acc: 0.9126  (best train acc: 0.9432, best val acc: 0.9518, best train loss: 0.1712  @ epoch 8205 )\n",
      "[Epoch: 8280] train loss: 0.2057, train acc: 0.9264, val loss: 0.2054, val acc: 0.9460  (best train acc: 0.9432, best val acc: 0.9518, best train loss: 0.1712  @ epoch 8205 )\n",
      "[Epoch: 8300] train loss: 0.2253, train acc: 0.9263, val loss: 0.2577, val acc: 0.9234  (best train acc: 0.9432, best val acc: 0.9518, best train loss: 0.1712  @ epoch 8205 )\n",
      "[Epoch: 8320] train loss: 0.1977, train acc: 0.9272, val loss: 0.2143, val acc: 0.9410  (best train acc: 0.9432, best val acc: 0.9518, best train loss: 0.1712  @ epoch 8205 )\n",
      "[Epoch: 8340] train loss: 0.1913, train acc: 0.9359, val loss: 0.2036, val acc: 0.9450  (best train acc: 0.9432, best val acc: 0.9518, best train loss: 0.1712  @ epoch 8205 )\n",
      "[Epoch: 8360] train loss: 0.2039, train acc: 0.9315, val loss: 0.2016, val acc: 0.9484  (best train acc: 0.9432, best val acc: 0.9518, best train loss: 0.1712  @ epoch 8205 )\n",
      "[Epoch: 8380] train loss: 0.1806, train acc: 0.9390, val loss: 0.1984, val acc: 0.9484  (best train acc: 0.9432, best val acc: 0.9518, best train loss: 0.1712  @ epoch 8205 )\n",
      "[Epoch: 8400] train loss: 0.1890, train acc: 0.9378, val loss: 0.2013, val acc: 0.9477  (best train acc: 0.9440, best val acc: 0.9518, best train loss: 0.1712  @ epoch 8205 )\n",
      "[Epoch: 8420] train loss: 0.1758, train acc: 0.9359, val loss: 0.1963, val acc: 0.9467  (best train acc: 0.9440, best val acc: 0.9518, best train loss: 0.1712  @ epoch 8205 )\n",
      "[Epoch: 8440] train loss: 0.1880, train acc: 0.9345, val loss: 0.2705, val acc: 0.9251  (best train acc: 0.9447, best val acc: 0.9518, best train loss: 0.1712  @ epoch 8205 )\n",
      "[Epoch: 8460] train loss: 0.1939, train acc: 0.9357, val loss: 0.2198, val acc: 0.9444  (best train acc: 0.9447, best val acc: 0.9518, best train loss: 0.1712  @ epoch 8205 )\n",
      "[Epoch: 8480] train loss: 0.1783, train acc: 0.9409, val loss: 0.2024, val acc: 0.9481  (best train acc: 0.9447, best val acc: 0.9518, best train loss: 0.1698  @ epoch 8479 )\n",
      "[Epoch: 8500] train loss: 0.1828, train acc: 0.9365, val loss: 0.1951, val acc: 0.9508  (best train acc: 0.9447, best val acc: 0.9518, best train loss: 0.1698  @ epoch 8479 )\n",
      "[Epoch: 8520] train loss: 0.1860, train acc: 0.9404, val loss: 0.1961, val acc: 0.9497  (best train acc: 0.9459, best val acc: 0.9518, best train loss: 0.1692  @ epoch 8505 )\n",
      "[Epoch: 8540] train loss: 0.1805, train acc: 0.9414, val loss: 0.1939, val acc: 0.9521  (best train acc: 0.9459, best val acc: 0.9521, best train loss: 0.1669  @ epoch 8530 )\n",
      "[Epoch: 8560] train loss: 0.1800, train acc: 0.9397, val loss: 0.1982, val acc: 0.9484  (best train acc: 0.9459, best val acc: 0.9521, best train loss: 0.1669  @ epoch 8530 )\n",
      "[Epoch: 8580] train loss: 0.2005, train acc: 0.9341, val loss: 0.1979, val acc: 0.9494  (best train acc: 0.9459, best val acc: 0.9521, best train loss: 0.1669  @ epoch 8530 )\n",
      "[Epoch: 8600] train loss: 0.1814, train acc: 0.9363, val loss: 0.1930, val acc: 0.9504  (best train acc: 0.9459, best val acc: 0.9521, best train loss: 0.1669  @ epoch 8530 )\n",
      "[Epoch: 8620] train loss: 0.1954, train acc: 0.9332, val loss: 0.2224, val acc: 0.9403  (best train acc: 0.9459, best val acc: 0.9521, best train loss: 0.1644  @ epoch 8607 )\n",
      "[Epoch: 8640] train loss: 0.1818, train acc: 0.9401, val loss: 0.1972, val acc: 0.9477  (best train acc: 0.9459, best val acc: 0.9521, best train loss: 0.1644  @ epoch 8607 )\n",
      "[Epoch: 8660] train loss: 0.1707, train acc: 0.9425, val loss: 0.1924, val acc: 0.9508  (best train acc: 0.9459, best val acc: 0.9524, best train loss: 0.1622  @ epoch 8649 )\n",
      "[Epoch: 8680] train loss: 0.2335, train acc: 0.9158, val loss: 0.1926, val acc: 0.9464  (best train acc: 0.9459, best val acc: 0.9524, best train loss: 0.1622  @ epoch 8649 )\n",
      "[Epoch: 8700] train loss: 0.2071, train acc: 0.9287, val loss: 0.1997, val acc: 0.9430  (best train acc: 0.9459, best val acc: 0.9524, best train loss: 0.1622  @ epoch 8649 )\n",
      "[Epoch: 8720] train loss: 0.1995, train acc: 0.9247, val loss: 0.2195, val acc: 0.9393  (best train acc: 0.9459, best val acc: 0.9524, best train loss: 0.1622  @ epoch 8649 )\n",
      "[Epoch: 8740] train loss: 0.1794, train acc: 0.9396, val loss: 0.1938, val acc: 0.9501  (best train acc: 0.9459, best val acc: 0.9524, best train loss: 0.1622  @ epoch 8649 )\n",
      "[Epoch: 8760] train loss: 0.1687, train acc: 0.9440, val loss: 0.1951, val acc: 0.9460  (best train acc: 0.9466, best val acc: 0.9524, best train loss: 0.1618  @ epoch 8747 )\n",
      "[Epoch: 8780] train loss: 0.1665, train acc: 0.9432, val loss: 0.1894, val acc: 0.9501  (best train acc: 0.9479, best val acc: 0.9531, best train loss: 0.1609  @ epoch 8773 )\n",
      "[Epoch: 8800] train loss: 0.1692, train acc: 0.9434, val loss: 0.2007, val acc: 0.9484  (best train acc: 0.9479, best val acc: 0.9531, best train loss: 0.1607  @ epoch 8786 )\n",
      "[Epoch: 8820] train loss: 0.1854, train acc: 0.9338, val loss: 0.2342, val acc: 0.9356  (best train acc: 0.9479, best val acc: 0.9531, best train loss: 0.1607  @ epoch 8786 )\n",
      "[Epoch: 8840] train loss: 0.1827, train acc: 0.9404, val loss: 0.2034, val acc: 0.9481  (best train acc: 0.9479, best val acc: 0.9531, best train loss: 0.1607  @ epoch 8786 )\n",
      "[Epoch: 8860] train loss: 0.2063, train acc: 0.9230, val loss: 0.1929, val acc: 0.9501  (best train acc: 0.9479, best val acc: 0.9531, best train loss: 0.1607  @ epoch 8786 )\n",
      "[Epoch: 8880] train loss: 0.1780, train acc: 0.9384, val loss: 0.2037, val acc: 0.9457  (best train acc: 0.9479, best val acc: 0.9531, best train loss: 0.1607  @ epoch 8786 )\n",
      "[Epoch: 8900] train loss: 0.1785, train acc: 0.9378, val loss: 0.1955, val acc: 0.9491  (best train acc: 0.9479, best val acc: 0.9531, best train loss: 0.1607  @ epoch 8786 )\n",
      "[Epoch: 8920] train loss: 0.1727, train acc: 0.9417, val loss: 0.1872, val acc: 0.9508  (best train acc: 0.9479, best val acc: 0.9538, best train loss: 0.1607  @ epoch 8786 )\n",
      "[Epoch: 8940] train loss: 0.1752, train acc: 0.9403, val loss: 0.2001, val acc: 0.9484  (best train acc: 0.9479, best val acc: 0.9538, best train loss: 0.1607  @ epoch 8786 )\n",
      "[Epoch: 8960] train loss: 0.1643, train acc: 0.9460, val loss: 0.1986, val acc: 0.9501  (best train acc: 0.9479, best val acc: 0.9538, best train loss: 0.1607  @ epoch 8786 )\n",
      "[Epoch: 8980] train loss: 0.1735, train acc: 0.9453, val loss: 0.1855, val acc: 0.9511  (best train acc: 0.9479, best val acc: 0.9538, best train loss: 0.1607  @ epoch 8786 )\n",
      "[Epoch: 9000] train loss: 0.1955, train acc: 0.9378, val loss: 0.1940, val acc: 0.9508  (best train acc: 0.9479, best val acc: 0.9538, best train loss: 0.1607  @ epoch 8786 )\n",
      "[Epoch: 9020] train loss: 0.1756, train acc: 0.9445, val loss: 0.1968, val acc: 0.9487  (best train acc: 0.9479, best val acc: 0.9538, best train loss: 0.1607  @ epoch 8786 )\n",
      "[Epoch: 9040] train loss: 0.1929, train acc: 0.9370, val loss: 0.1964, val acc: 0.9501  (best train acc: 0.9479, best val acc: 0.9538, best train loss: 0.1607  @ epoch 8786 )\n",
      "[Epoch: 9060] train loss: 0.1727, train acc: 0.9443, val loss: 0.1889, val acc: 0.9514  (best train acc: 0.9479, best val acc: 0.9538, best train loss: 0.1607  @ epoch 8786 )\n",
      "[Epoch: 9080] train loss: 0.1705, train acc: 0.9425, val loss: 0.1830, val acc: 0.9528  (best train acc: 0.9501, best val acc: 0.9538, best train loss: 0.1589  @ epoch 9073 )\n",
      "[Epoch: 9100] train loss: 0.1695, train acc: 0.9438, val loss: 0.1802, val acc: 0.9508  (best train acc: 0.9501, best val acc: 0.9538, best train loss: 0.1589  @ epoch 9073 )\n",
      "[Epoch: 9120] train loss: 0.1743, train acc: 0.9445, val loss: 0.1817, val acc: 0.9538  (best train acc: 0.9501, best val acc: 0.9538, best train loss: 0.1589  @ epoch 9073 )\n",
      "[Epoch: 9140] train loss: 0.1665, train acc: 0.9445, val loss: 0.1873, val acc: 0.9524  (best train acc: 0.9501, best val acc: 0.9541, best train loss: 0.1589  @ epoch 9073 )\n",
      "[Epoch: 9160] train loss: 0.1847, train acc: 0.9431, val loss: 0.1900, val acc: 0.9497  (best train acc: 0.9501, best val acc: 0.9541, best train loss: 0.1587  @ epoch 9143 )\n",
      "[Epoch: 9180] train loss: 0.2244, train acc: 0.9263, val loss: 0.2306, val acc: 0.9349  (best train acc: 0.9501, best val acc: 0.9541, best train loss: 0.1587  @ epoch 9143 )\n",
      "[Epoch: 9200] train loss: 0.1873, train acc: 0.9384, val loss: 0.1984, val acc: 0.9474  (best train acc: 0.9501, best val acc: 0.9541, best train loss: 0.1587  @ epoch 9143 )\n",
      "[Epoch: 9220] train loss: 0.1760, train acc: 0.9466, val loss: 0.1869, val acc: 0.9518  (best train acc: 0.9501, best val acc: 0.9541, best train loss: 0.1587  @ epoch 9143 )\n",
      "[Epoch: 9240] train loss: 0.2060, train acc: 0.9345, val loss: 0.2523, val acc: 0.9221  (best train acc: 0.9501, best val acc: 0.9545, best train loss: 0.1554  @ epoch 9233 )\n",
      "[Epoch: 9260] train loss: 0.1906, train acc: 0.9302, val loss: 0.1852, val acc: 0.9491  (best train acc: 0.9501, best val acc: 0.9545, best train loss: 0.1554  @ epoch 9233 )\n",
      "[Epoch: 9280] train loss: 0.2072, train acc: 0.9278, val loss: 0.1951, val acc: 0.9454  (best train acc: 0.9501, best val acc: 0.9545, best train loss: 0.1554  @ epoch 9233 )\n",
      "[Epoch: 9300] train loss: 0.1715, train acc: 0.9425, val loss: 0.1870, val acc: 0.9528  (best train acc: 0.9501, best val acc: 0.9545, best train loss: 0.1554  @ epoch 9233 )\n",
      "[Epoch: 9320] train loss: 0.1661, train acc: 0.9453, val loss: 0.1849, val acc: 0.9518  (best train acc: 0.9501, best val acc: 0.9558, best train loss: 0.1529  @ epoch 9310 )\n",
      "[Epoch: 9340] train loss: 0.1613, train acc: 0.9469, val loss: 0.1807, val acc: 0.9538  (best train acc: 0.9501, best val acc: 0.9558, best train loss: 0.1529  @ epoch 9310 )\n",
      "[Epoch: 9360] train loss: 0.1684, train acc: 0.9422, val loss: 0.1822, val acc: 0.9538  (best train acc: 0.9501, best val acc: 0.9558, best train loss: 0.1529  @ epoch 9310 )\n",
      "[Epoch: 9380] train loss: 0.1705, train acc: 0.9470, val loss: 0.1813, val acc: 0.9541  (best train acc: 0.9503, best val acc: 0.9558, best train loss: 0.1529  @ epoch 9310 )\n",
      "[Epoch: 9400] train loss: 0.2294, train acc: 0.9164, val loss: 0.2084, val acc: 0.9342  (best train acc: 0.9503, best val acc: 0.9558, best train loss: 0.1529  @ epoch 9310 )\n",
      "[Epoch: 9420] train loss: 0.1696, train acc: 0.9456, val loss: 0.1882, val acc: 0.9541  (best train acc: 0.9503, best val acc: 0.9558, best train loss: 0.1529  @ epoch 9310 )\n",
      "[Epoch: 9440] train loss: 0.1596, train acc: 0.9505, val loss: 0.1813, val acc: 0.9521  (best train acc: 0.9505, best val acc: 0.9558, best train loss: 0.1529  @ epoch 9310 )\n",
      "[Epoch: 9460] train loss: 0.1687, train acc: 0.9446, val loss: 0.1849, val acc: 0.9538  (best train acc: 0.9505, best val acc: 0.9558, best train loss: 0.1529  @ epoch 9310 )\n",
      "[Epoch: 9480] train loss: 0.1583, train acc: 0.9475, val loss: 0.1759, val acc: 0.9562  (best train acc: 0.9505, best val acc: 0.9562, best train loss: 0.1529  @ epoch 9310 )\n",
      "[Epoch: 9500] train loss: 0.1657, train acc: 0.9454, val loss: 0.1799, val acc: 0.9541  (best train acc: 0.9511, best val acc: 0.9562, best train loss: 0.1522  @ epoch 9490 )\n",
      "[Epoch: 9520] train loss: 0.1644, train acc: 0.9468, val loss: 0.1887, val acc: 0.9531  (best train acc: 0.9511, best val acc: 0.9562, best train loss: 0.1522  @ epoch 9490 )\n",
      "[Epoch: 9540] train loss: 0.1608, train acc: 0.9473, val loss: 0.1853, val acc: 0.9555  (best train acc: 0.9511, best val acc: 0.9562, best train loss: 0.1509  @ epoch 9527 )\n",
      "[Epoch: 9560] train loss: 0.1599, train acc: 0.9484, val loss: 0.1829, val acc: 0.9531  (best train acc: 0.9511, best val acc: 0.9568, best train loss: 0.1509  @ epoch 9527 )\n",
      "[Epoch: 9580] train loss: 0.1675, train acc: 0.9414, val loss: 0.1816, val acc: 0.9548  (best train acc: 0.9511, best val acc: 0.9568, best train loss: 0.1509  @ epoch 9527 )\n",
      "[Epoch: 9600] train loss: 0.2147, train acc: 0.9256, val loss: 0.1957, val acc: 0.9444  (best train acc: 0.9511, best val acc: 0.9568, best train loss: 0.1509  @ epoch 9527 )\n",
      "[Epoch: 9620] train loss: 0.1893, train acc: 0.9398, val loss: 0.1860, val acc: 0.9518  (best train acc: 0.9511, best val acc: 0.9568, best train loss: 0.1509  @ epoch 9527 )\n",
      "[Epoch: 9640] train loss: 0.1602, train acc: 0.9481, val loss: 0.1809, val acc: 0.9541  (best train acc: 0.9511, best val acc: 0.9568, best train loss: 0.1509  @ epoch 9527 )\n",
      "[Epoch: 9660] train loss: 0.1585, train acc: 0.9488, val loss: 0.1899, val acc: 0.9514  (best train acc: 0.9511, best val acc: 0.9568, best train loss: 0.1509  @ epoch 9527 )\n",
      "[Epoch: 9680] train loss: 0.1731, train acc: 0.9386, val loss: 0.2290, val acc: 0.9369  (best train acc: 0.9511, best val acc: 0.9568, best train loss: 0.1509  @ epoch 9527 )\n",
      "[Epoch: 9700] train loss: 0.1642, train acc: 0.9456, val loss: 0.2081, val acc: 0.9501  (best train acc: 0.9511, best val acc: 0.9568, best train loss: 0.1509  @ epoch 9527 )\n",
      "[Epoch: 9720] train loss: 0.1714, train acc: 0.9429, val loss: 0.1756, val acc: 0.9545  (best train acc: 0.9511, best val acc: 0.9568, best train loss: 0.1509  @ epoch 9527 )\n",
      "[Epoch: 9740] train loss: 0.1560, train acc: 0.9493, val loss: 0.1814, val acc: 0.9541  (best train acc: 0.9519, best val acc: 0.9568, best train loss: 0.1509  @ epoch 9527 )\n",
      "[Epoch: 9760] train loss: 0.1682, train acc: 0.9459, val loss: 0.1776, val acc: 0.9531  (best train acc: 0.9525, best val acc: 0.9568, best train loss: 0.1509  @ epoch 9527 )\n",
      "[Epoch: 9780] train loss: 0.1919, train acc: 0.9270, val loss: 0.2026, val acc: 0.9433  (best train acc: 0.9525, best val acc: 0.9568, best train loss: 0.1509  @ epoch 9527 )\n",
      "[Epoch: 9800] train loss: 0.1676, train acc: 0.9492, val loss: 0.1785, val acc: 0.9541  (best train acc: 0.9525, best val acc: 0.9568, best train loss: 0.1509  @ epoch 9527 )\n",
      "[Epoch: 9820] train loss: 0.1685, train acc: 0.9451, val loss: 0.1805, val acc: 0.9545  (best train acc: 0.9525, best val acc: 0.9578, best train loss: 0.1507  @ epoch 9801 )\n",
      "[Epoch: 9840] train loss: 0.1596, train acc: 0.9508, val loss: 0.1775, val acc: 0.9548  (best train acc: 0.9525, best val acc: 0.9578, best train loss: 0.1507  @ epoch 9801 )\n",
      "[Epoch: 9860] train loss: 0.1645, train acc: 0.9448, val loss: 0.1798, val acc: 0.9558  (best train acc: 0.9525, best val acc: 0.9578, best train loss: 0.1507  @ epoch 9801 )\n",
      "[Epoch: 9880] train loss: 0.1739, train acc: 0.9412, val loss: 0.1916, val acc: 0.9501  (best train acc: 0.9525, best val acc: 0.9578, best train loss: 0.1507  @ epoch 9801 )\n",
      "[Epoch: 9900] train loss: 0.1561, train acc: 0.9490, val loss: 0.1983, val acc: 0.9524  (best train acc: 0.9525, best val acc: 0.9582, best train loss: 0.1507  @ epoch 9801 )\n",
      "[Epoch: 9920] train loss: 0.1575, train acc: 0.9489, val loss: 0.2037, val acc: 0.9504  (best train acc: 0.9525, best val acc: 0.9582, best train loss: 0.1497  @ epoch 9904 )\n",
      "[Epoch: 9940] train loss: 0.1541, train acc: 0.9498, val loss: 0.1789, val acc: 0.9551  (best train acc: 0.9525, best val acc: 0.9582, best train loss: 0.1497  @ epoch 9904 )\n",
      "[Epoch: 9960] train loss: 0.1622, train acc: 0.9470, val loss: 0.1777, val acc: 0.9555  (best train acc: 0.9534, best val acc: 0.9582, best train loss: 0.1474  @ epoch 9949 )\n",
      "[Epoch: 9980] train loss: 0.1632, train acc: 0.9440, val loss: 0.1906, val acc: 0.9545  (best train acc: 0.9534, best val acc: 0.9582, best train loss: 0.1474  @ epoch 9949 )\n",
      "[Epoch: 10000] train loss: 0.1553, train acc: 0.9485, val loss: 0.1793, val acc: 0.9558  (best train acc: 0.9534, best val acc: 0.9582, best train loss: 0.1462  @ epoch 9996 )\n",
      "[Epoch: 10020] train loss: 0.1684, train acc: 0.9438, val loss: 0.1883, val acc: 0.9531  (best train acc: 0.9534, best val acc: 0.9582, best train loss: 0.1462  @ epoch 9996 )\n",
      "[Epoch: 10040] train loss: 0.1613, train acc: 0.9449, val loss: 0.1793, val acc: 0.9551  (best train acc: 0.9534, best val acc: 0.9582, best train loss: 0.1462  @ epoch 9996 )\n",
      "[Epoch: 10060] train loss: 0.1630, train acc: 0.9466, val loss: 0.1819, val acc: 0.9555  (best train acc: 0.9534, best val acc: 0.9582, best train loss: 0.1462  @ epoch 9996 )\n",
      "[Epoch: 10080] train loss: 0.1727, train acc: 0.9398, val loss: 0.1850, val acc: 0.9558  (best train acc: 0.9539, best val acc: 0.9582, best train loss: 0.1462  @ epoch 9996 )\n",
      "[Epoch: 10100] train loss: 0.1914, train acc: 0.9358, val loss: 0.1894, val acc: 0.9535  (best train acc: 0.9539, best val acc: 0.9582, best train loss: 0.1462  @ epoch 9996 )\n",
      "[Epoch: 10120] train loss: 0.1686, train acc: 0.9425, val loss: 0.1816, val acc: 0.9551  (best train acc: 0.9539, best val acc: 0.9582, best train loss: 0.1462  @ epoch 9996 )\n",
      "[Epoch: 10140] train loss: 0.1620, train acc: 0.9463, val loss: 0.1785, val acc: 0.9565  (best train acc: 0.9539, best val acc: 0.9582, best train loss: 0.1462  @ epoch 9996 )\n",
      "[Epoch: 10160] train loss: 0.1549, train acc: 0.9467, val loss: 0.1915, val acc: 0.9558  (best train acc: 0.9541, best val acc: 0.9582, best train loss: 0.1455  @ epoch 10156 )\n",
      "[Epoch: 10180] train loss: 0.1837, train acc: 0.9333, val loss: 0.2016, val acc: 0.9481  (best train acc: 0.9541, best val acc: 0.9582, best train loss: 0.1455  @ epoch 10156 )\n",
      "[Epoch: 10200] train loss: 0.1613, train acc: 0.9456, val loss: 0.1850, val acc: 0.9562  (best train acc: 0.9541, best val acc: 0.9582, best train loss: 0.1455  @ epoch 10156 )\n",
      "[Epoch: 10220] train loss: 0.1641, train acc: 0.9445, val loss: 0.1879, val acc: 0.9467  (best train acc: 0.9541, best val acc: 0.9582, best train loss: 0.1440  @ epoch 10215 )\n",
      "[Epoch: 10240] train loss: 0.1746, train acc: 0.9416, val loss: 0.1985, val acc: 0.9501  (best train acc: 0.9541, best val acc: 0.9582, best train loss: 0.1440  @ epoch 10215 )\n",
      "[Epoch: 10260] train loss: 0.1610, train acc: 0.9487, val loss: 0.1906, val acc: 0.9494  (best train acc: 0.9541, best val acc: 0.9582, best train loss: 0.1440  @ epoch 10215 )\n",
      "[Epoch: 10280] train loss: 0.1873, train acc: 0.9404, val loss: 0.1834, val acc: 0.9504  (best train acc: 0.9541, best val acc: 0.9582, best train loss: 0.1440  @ epoch 10215 )\n",
      "[Epoch: 10300] train loss: 0.1632, train acc: 0.9464, val loss: 0.1897, val acc: 0.9562  (best train acc: 0.9541, best val acc: 0.9582, best train loss: 0.1440  @ epoch 10215 )\n",
      "[Epoch: 10320] train loss: 0.1768, train acc: 0.9439, val loss: 0.1880, val acc: 0.9481  (best train acc: 0.9541, best val acc: 0.9582, best train loss: 0.1440  @ epoch 10215 )\n",
      "[Epoch: 10340] train loss: 0.1552, train acc: 0.9483, val loss: 0.1747, val acc: 0.9548  (best train acc: 0.9541, best val acc: 0.9582, best train loss: 0.1440  @ epoch 10215 )\n",
      "[Epoch: 10360] train loss: 0.1713, train acc: 0.9356, val loss: 0.1843, val acc: 0.9494  (best train acc: 0.9541, best val acc: 0.9582, best train loss: 0.1440  @ epoch 10215 )\n",
      "[Epoch: 10380] train loss: 0.1564, train acc: 0.9450, val loss: 0.1864, val acc: 0.9531  (best train acc: 0.9541, best val acc: 0.9582, best train loss: 0.1440  @ epoch 10215 )\n",
      "[Epoch: 10400] train loss: 0.1453, train acc: 0.9512, val loss: 0.1738, val acc: 0.9548  (best train acc: 0.9541, best val acc: 0.9582, best train loss: 0.1440  @ epoch 10215 )\n",
      "[Epoch: 10420] train loss: 0.1693, train acc: 0.9443, val loss: 0.1782, val acc: 0.9548  (best train acc: 0.9541, best val acc: 0.9582, best train loss: 0.1409  @ epoch 10417 )\n",
      "[Epoch: 10440] train loss: 0.1521, train acc: 0.9503, val loss: 0.1791, val acc: 0.9541  (best train acc: 0.9542, best val acc: 0.9582, best train loss: 0.1394  @ epoch 10429 )\n",
      "[Epoch: 10460] train loss: 0.2186, train acc: 0.9118, val loss: 0.1908, val acc: 0.9497  (best train acc: 0.9542, best val acc: 0.9589, best train loss: 0.1394  @ epoch 10429 )\n",
      "[Epoch: 10480] train loss: 0.1693, train acc: 0.9393, val loss: 0.1739, val acc: 0.9545  (best train acc: 0.9542, best val acc: 0.9589, best train loss: 0.1394  @ epoch 10429 )\n",
      "[Epoch: 10500] train loss: 0.1575, train acc: 0.9481, val loss: 0.1726, val acc: 0.9558  (best train acc: 0.9542, best val acc: 0.9589, best train loss: 0.1394  @ epoch 10429 )\n",
      "[Epoch: 10520] train loss: 0.1498, train acc: 0.9519, val loss: 0.1729, val acc: 0.9555  (best train acc: 0.9542, best val acc: 0.9589, best train loss: 0.1394  @ epoch 10429 )\n",
      "[Epoch: 10540] train loss: 0.1559, train acc: 0.9487, val loss: 0.1906, val acc: 0.9487  (best train acc: 0.9545, best val acc: 0.9589, best train loss: 0.1394  @ epoch 10429 )\n",
      "[Epoch: 10560] train loss: 0.1528, train acc: 0.9493, val loss: 0.1766, val acc: 0.9562  (best train acc: 0.9545, best val acc: 0.9589, best train loss: 0.1394  @ epoch 10429 )\n",
      "[Epoch: 10580] train loss: 0.1449, train acc: 0.9534, val loss: 0.1727, val acc: 0.9575  (best train acc: 0.9545, best val acc: 0.9592, best train loss: 0.1394  @ epoch 10429 )\n",
      "[Epoch: 10600] train loss: 0.1485, train acc: 0.9523, val loss: 0.1752, val acc: 0.9545  (best train acc: 0.9552, best val acc: 0.9592, best train loss: 0.1394  @ epoch 10429 )\n",
      "[Epoch: 10620] train loss: 0.1464, train acc: 0.9560, val loss: 0.1785, val acc: 0.9565  (best train acc: 0.9560, best val acc: 0.9595, best train loss: 0.1394  @ epoch 10429 )\n",
      "[Epoch: 10640] train loss: 0.1556, train acc: 0.9461, val loss: 0.1744, val acc: 0.9555  (best train acc: 0.9560, best val acc: 0.9599, best train loss: 0.1394  @ epoch 10429 )\n",
      "[Epoch: 10660] train loss: 0.1592, train acc: 0.9448, val loss: 0.1751, val acc: 0.9578  (best train acc: 0.9560, best val acc: 0.9599, best train loss: 0.1394  @ epoch 10429 )\n",
      "[Epoch: 10680] train loss: 0.1568, train acc: 0.9492, val loss: 0.1750, val acc: 0.9535  (best train acc: 0.9560, best val acc: 0.9599, best train loss: 0.1394  @ epoch 10429 )\n",
      "[Epoch: 10700] train loss: 0.1859, train acc: 0.9370, val loss: 0.2014, val acc: 0.9477  (best train acc: 0.9560, best val acc: 0.9599, best train loss: 0.1394  @ epoch 10429 )\n",
      "[Epoch: 10720] train loss: 0.1676, train acc: 0.9440, val loss: 0.1809, val acc: 0.9558  (best train acc: 0.9560, best val acc: 0.9599, best train loss: 0.1394  @ epoch 10429 )\n",
      "[Epoch: 10740] train loss: 0.1658, train acc: 0.9443, val loss: 0.1773, val acc: 0.9565  (best train acc: 0.9560, best val acc: 0.9599, best train loss: 0.1394  @ epoch 10429 )\n",
      "[Epoch: 10760] train loss: 0.1466, train acc: 0.9485, val loss: 0.1802, val acc: 0.9508  (best train acc: 0.9560, best val acc: 0.9599, best train loss: 0.1394  @ epoch 10429 )\n",
      "[Epoch: 10780] train loss: 0.1500, train acc: 0.9523, val loss: 0.1756, val acc: 0.9575  (best train acc: 0.9560, best val acc: 0.9599, best train loss: 0.1382  @ epoch 10778 )\n",
      "[Epoch: 10800] train loss: 0.1425, train acc: 0.9550, val loss: 0.1659, val acc: 0.9578  (best train acc: 0.9560, best val acc: 0.9599, best train loss: 0.1382  @ epoch 10778 )\n",
      "[Epoch: 10820] train loss: 0.1529, train acc: 0.9478, val loss: 0.1700, val acc: 0.9582  (best train acc: 0.9560, best val acc: 0.9599, best train loss: 0.1382  @ epoch 10778 )\n",
      "[Epoch: 10840] train loss: 0.1459, train acc: 0.9516, val loss: 0.1743, val acc: 0.9535  (best train acc: 0.9560, best val acc: 0.9599, best train loss: 0.1382  @ epoch 10778 )\n",
      "[Epoch: 10860] train loss: 0.1560, train acc: 0.9503, val loss: 0.1735, val acc: 0.9558  (best train acc: 0.9563, best val acc: 0.9599, best train loss: 0.1361  @ epoch 10856 )\n",
      "[Epoch: 10880] train loss: 0.1960, train acc: 0.9327, val loss: 0.1911, val acc: 0.9528  (best train acc: 0.9563, best val acc: 0.9599, best train loss: 0.1361  @ epoch 10856 )\n",
      "[Epoch: 10900] train loss: 0.1494, train acc: 0.9502, val loss: 0.1708, val acc: 0.9562  (best train acc: 0.9563, best val acc: 0.9599, best train loss: 0.1361  @ epoch 10856 )\n",
      "[Epoch: 10920] train loss: 0.1380, train acc: 0.9535, val loss: 0.1704, val acc: 0.9562  (best train acc: 0.9566, best val acc: 0.9599, best train loss: 0.1361  @ epoch 10856 )\n",
      "[Epoch: 10940] train loss: 0.1461, train acc: 0.9535, val loss: 0.1735, val acc: 0.9568  (best train acc: 0.9568, best val acc: 0.9602, best train loss: 0.1361  @ epoch 10856 )\n",
      "[Epoch: 10960] train loss: 0.1385, train acc: 0.9539, val loss: 0.1712, val acc: 0.9582  (best train acc: 0.9568, best val acc: 0.9602, best train loss: 0.1361  @ epoch 10856 )\n",
      "[Epoch: 10980] train loss: 0.1435, train acc: 0.9534, val loss: 0.1797, val acc: 0.9589  (best train acc: 0.9568, best val acc: 0.9609, best train loss: 0.1359  @ epoch 10964 )\n",
      "[Epoch: 11000] train loss: 0.1596, train acc: 0.9496, val loss: 0.1794, val acc: 0.9548  (best train acc: 0.9573, best val acc: 0.9609, best train loss: 0.1325  @ epoch 10988 )\n",
      "[Epoch: 11020] train loss: 0.1866, train acc: 0.9382, val loss: 0.2052, val acc: 0.9376  (best train acc: 0.9573, best val acc: 0.9609, best train loss: 0.1325  @ epoch 10988 )\n",
      "[Epoch: 11040] train loss: 0.1492, train acc: 0.9535, val loss: 0.1788, val acc: 0.9531  (best train acc: 0.9573, best val acc: 0.9609, best train loss: 0.1325  @ epoch 10988 )\n",
      "[Epoch: 11060] train loss: 0.1594, train acc: 0.9439, val loss: 0.1734, val acc: 0.9572  (best train acc: 0.9573, best val acc: 0.9609, best train loss: 0.1325  @ epoch 10988 )\n",
      "[Epoch: 11080] train loss: 0.1469, train acc: 0.9489, val loss: 0.1903, val acc: 0.9501  (best train acc: 0.9573, best val acc: 0.9609, best train loss: 0.1325  @ epoch 10988 )\n",
      "[Epoch: 11100] train loss: 0.1480, train acc: 0.9520, val loss: 0.1858, val acc: 0.9548  (best train acc: 0.9573, best val acc: 0.9609, best train loss: 0.1325  @ epoch 10988 )\n",
      "[Epoch: 11120] train loss: 0.1507, train acc: 0.9495, val loss: 0.1913, val acc: 0.9528  (best train acc: 0.9573, best val acc: 0.9609, best train loss: 0.1325  @ epoch 10988 )\n",
      "[Epoch: 11140] train loss: 0.1431, train acc: 0.9519, val loss: 0.1731, val acc: 0.9575  (best train acc: 0.9575, best val acc: 0.9609, best train loss: 0.1325  @ epoch 10988 )\n",
      "[Epoch: 11160] train loss: 0.1437, train acc: 0.9542, val loss: 0.1900, val acc: 0.9568  (best train acc: 0.9580, best val acc: 0.9616, best train loss: 0.1325  @ epoch 10988 )\n",
      "[Epoch: 11180] train loss: 0.1396, train acc: 0.9529, val loss: 0.1730, val acc: 0.9582  (best train acc: 0.9580, best val acc: 0.9616, best train loss: 0.1325  @ epoch 10988 )\n",
      "[Epoch: 11200] train loss: 0.1620, train acc: 0.9499, val loss: 0.1797, val acc: 0.9589  (best train acc: 0.9583, best val acc: 0.9616, best train loss: 0.1313  @ epoch 11193 )\n",
      "[Epoch: 11220] train loss: 0.1368, train acc: 0.9552, val loss: 0.1725, val acc: 0.9565  (best train acc: 0.9583, best val acc: 0.9616, best train loss: 0.1313  @ epoch 11193 )\n",
      "[Epoch: 11240] train loss: 0.1434, train acc: 0.9526, val loss: 0.2114, val acc: 0.9440  (best train acc: 0.9583, best val acc: 0.9616, best train loss: 0.1313  @ epoch 11193 )\n",
      "[Epoch: 11260] train loss: 0.1439, train acc: 0.9542, val loss: 0.1697, val acc: 0.9592  (best train acc: 0.9583, best val acc: 0.9616, best train loss: 0.1313  @ epoch 11193 )\n",
      "[Epoch: 11280] train loss: 0.1507, train acc: 0.9503, val loss: 0.1736, val acc: 0.9582  (best train acc: 0.9583, best val acc: 0.9616, best train loss: 0.1275  @ epoch 11270 )\n",
      "[Epoch: 11300] train loss: 0.1397, train acc: 0.9532, val loss: 0.1882, val acc: 0.9558  (best train acc: 0.9583, best val acc: 0.9616, best train loss: 0.1275  @ epoch 11270 )\n",
      "[Epoch: 11320] train loss: 0.1760, train acc: 0.9367, val loss: 0.1671, val acc: 0.9558  (best train acc: 0.9583, best val acc: 0.9616, best train loss: 0.1275  @ epoch 11270 )\n",
      "[Epoch: 11340] train loss: 0.1759, train acc: 0.9328, val loss: 0.2100, val acc: 0.9417  (best train acc: 0.9583, best val acc: 0.9616, best train loss: 0.1275  @ epoch 11270 )\n",
      "[Epoch: 11360] train loss: 0.1458, train acc: 0.9545, val loss: 0.1724, val acc: 0.9578  (best train acc: 0.9583, best val acc: 0.9616, best train loss: 0.1275  @ epoch 11270 )\n",
      "[Epoch: 11380] train loss: 0.1516, train acc: 0.9514, val loss: 0.1700, val acc: 0.9592  (best train acc: 0.9583, best val acc: 0.9616, best train loss: 0.1275  @ epoch 11270 )\n",
      "[Epoch: 11400] train loss: 0.1451, train acc: 0.9486, val loss: 0.1732, val acc: 0.9555  (best train acc: 0.9583, best val acc: 0.9616, best train loss: 0.1275  @ epoch 11270 )\n",
      "[Epoch: 11420] train loss: 0.1862, train acc: 0.9414, val loss: 0.1826, val acc: 0.9524  (best train acc: 0.9583, best val acc: 0.9616, best train loss: 0.1275  @ epoch 11270 )\n",
      "[Epoch: 11440] train loss: 0.1462, train acc: 0.9537, val loss: 0.1795, val acc: 0.9568  (best train acc: 0.9583, best val acc: 0.9616, best train loss: 0.1275  @ epoch 11270 )\n",
      "[Epoch: 11460] train loss: 0.1430, train acc: 0.9515, val loss: 0.1786, val acc: 0.9578  (best train acc: 0.9583, best val acc: 0.9616, best train loss: 0.1275  @ epoch 11270 )\n",
      "[Epoch: 11480] train loss: 0.1424, train acc: 0.9515, val loss: 0.1756, val acc: 0.9568  (best train acc: 0.9583, best val acc: 0.9616, best train loss: 0.1275  @ epoch 11270 )\n",
      "[Epoch: 11500] train loss: 0.1526, train acc: 0.9540, val loss: 0.1913, val acc: 0.9514  (best train acc: 0.9583, best val acc: 0.9616, best train loss: 0.1275  @ epoch 11270 )\n",
      "[Epoch: 11520] train loss: 0.1635, train acc: 0.9397, val loss: 0.1917, val acc: 0.9460  (best train acc: 0.9583, best val acc: 0.9616, best train loss: 0.1275  @ epoch 11270 )\n",
      "[Epoch: 11540] train loss: 0.1438, train acc: 0.9511, val loss: 0.1885, val acc: 0.9568  (best train acc: 0.9583, best val acc: 0.9616, best train loss: 0.1275  @ epoch 11270 )\n",
      "[Epoch: 11560] train loss: 0.1342, train acc: 0.9566, val loss: 0.1779, val acc: 0.9572  (best train acc: 0.9583, best val acc: 0.9616, best train loss: 0.1275  @ epoch 11270 )\n",
      "[Epoch: 11580] train loss: 0.1352, train acc: 0.9547, val loss: 0.1741, val acc: 0.9589  (best train acc: 0.9592, best val acc: 0.9616, best train loss: 0.1275  @ epoch 11270 )\n",
      "[Epoch: 11600] train loss: 0.1469, train acc: 0.9498, val loss: 0.1787, val acc: 0.9575  (best train acc: 0.9597, best val acc: 0.9616, best train loss: 0.1260  @ epoch 11595 )\n",
      "[Epoch: 11620] train loss: 0.1397, train acc: 0.9526, val loss: 0.1710, val acc: 0.9595  (best train acc: 0.9597, best val acc: 0.9616, best train loss: 0.1260  @ epoch 11595 )\n",
      "[Epoch: 11640] train loss: 0.1470, train acc: 0.9496, val loss: 0.1824, val acc: 0.9578  (best train acc: 0.9597, best val acc: 0.9616, best train loss: 0.1260  @ epoch 11595 )\n",
      "[Epoch: 11660] train loss: 0.1719, train acc: 0.9465, val loss: 0.2127, val acc: 0.9430  (best train acc: 0.9597, best val acc: 0.9616, best train loss: 0.1257  @ epoch 11656 )\n",
      "[Epoch: 11680] train loss: 0.1502, train acc: 0.9485, val loss: 0.1816, val acc: 0.9518  (best train acc: 0.9597, best val acc: 0.9616, best train loss: 0.1257  @ epoch 11656 )\n",
      "[Epoch: 11700] train loss: 0.1703, train acc: 0.9454, val loss: 0.1965, val acc: 0.9491  (best train acc: 0.9597, best val acc: 0.9616, best train loss: 0.1257  @ epoch 11656 )\n",
      "[Epoch: 11720] train loss: 0.1562, train acc: 0.9483, val loss: 0.1803, val acc: 0.9531  (best train acc: 0.9597, best val acc: 0.9616, best train loss: 0.1257  @ epoch 11656 )\n",
      "[Epoch: 11740] train loss: 0.1404, train acc: 0.9527, val loss: 0.1795, val acc: 0.9572  (best train acc: 0.9597, best val acc: 0.9616, best train loss: 0.1257  @ epoch 11656 )\n",
      "[Epoch: 11760] train loss: 0.1418, train acc: 0.9537, val loss: 0.1905, val acc: 0.9524  (best train acc: 0.9597, best val acc: 0.9619, best train loss: 0.1255  @ epoch 11755 )\n",
      "[Epoch: 11780] train loss: 0.1317, train acc: 0.9574, val loss: 0.1767, val acc: 0.9589  (best train acc: 0.9597, best val acc: 0.9619, best train loss: 0.1255  @ epoch 11755 )\n",
      "[Epoch: 11800] train loss: 0.1539, train acc: 0.9494, val loss: 0.1783, val acc: 0.9589  (best train acc: 0.9597, best val acc: 0.9619, best train loss: 0.1255  @ epoch 11799 )\n",
      "[Epoch: 11820] train loss: 0.1433, train acc: 0.9492, val loss: 0.1812, val acc: 0.9595  (best train acc: 0.9597, best val acc: 0.9619, best train loss: 0.1255  @ epoch 11799 )\n",
      "[Epoch: 11840] train loss: 0.1347, train acc: 0.9540, val loss: 0.1948, val acc: 0.9558  (best train acc: 0.9597, best val acc: 0.9619, best train loss: 0.1255  @ epoch 11799 )\n",
      "[Epoch: 11860] train loss: 0.1350, train acc: 0.9543, val loss: 0.2157, val acc: 0.9467  (best train acc: 0.9597, best val acc: 0.9619, best train loss: 0.1255  @ epoch 11799 )\n",
      "[Epoch: 11880] train loss: 0.1398, train acc: 0.9515, val loss: 0.1847, val acc: 0.9538  (best train acc: 0.9597, best val acc: 0.9619, best train loss: 0.1255  @ epoch 11799 )\n",
      "[Epoch: 11900] train loss: 0.1292, train acc: 0.9592, val loss: 0.1785, val acc: 0.9575  (best train acc: 0.9597, best val acc: 0.9619, best train loss: 0.1255  @ epoch 11799 )\n",
      "[Epoch: 11920] train loss: 0.1364, train acc: 0.9538, val loss: 0.1827, val acc: 0.9585  (best train acc: 0.9597, best val acc: 0.9619, best train loss: 0.1255  @ epoch 11799 )\n",
      "[Epoch: 11940] train loss: 0.1397, train acc: 0.9518, val loss: 0.2000, val acc: 0.9555  (best train acc: 0.9597, best val acc: 0.9619, best train loss: 0.1255  @ epoch 11799 )\n",
      "[Epoch: 11960] train loss: 0.1415, train acc: 0.9501, val loss: 0.1787, val acc: 0.9592  (best train acc: 0.9597, best val acc: 0.9619, best train loss: 0.1255  @ epoch 11799 )\n",
      "[Epoch: 11980] train loss: 0.1669, train acc: 0.9468, val loss: 0.2019, val acc: 0.9470  (best train acc: 0.9597, best val acc: 0.9619, best train loss: 0.1255  @ epoch 11799 )\n",
      "[Epoch: 12000] train loss: 0.1489, train acc: 0.9521, val loss: 0.1782, val acc: 0.9551  (best train acc: 0.9597, best val acc: 0.9619, best train loss: 0.1255  @ epoch 11799 )\n",
      "[Epoch: 12020] train loss: 0.1464, train acc: 0.9510, val loss: 0.1839, val acc: 0.9578  (best train acc: 0.9597, best val acc: 0.9619, best train loss: 0.1255  @ epoch 11799 )\n",
      "[Epoch: 12040] train loss: 0.1278, train acc: 0.9571, val loss: 0.1760, val acc: 0.9585  (best train acc: 0.9597, best val acc: 0.9619, best train loss: 0.1255  @ epoch 11799 )\n",
      "[Epoch: 12060] train loss: 0.1345, train acc: 0.9550, val loss: 0.1800, val acc: 0.9575  (best train acc: 0.9599, best val acc: 0.9619, best train loss: 0.1255  @ epoch 11799 )\n",
      "[Epoch: 12080] train loss: 0.1420, train acc: 0.9535, val loss: 0.1809, val acc: 0.9551  (best train acc: 0.9599, best val acc: 0.9619, best train loss: 0.1250  @ epoch 12069 )\n",
      "[Epoch: 12100] train loss: 0.1403, train acc: 0.9539, val loss: 0.1787, val acc: 0.9578  (best train acc: 0.9599, best val acc: 0.9619, best train loss: 0.1250  @ epoch 12069 )\n",
      "[Epoch: 12120] train loss: 0.1278, train acc: 0.9580, val loss: 0.1811, val acc: 0.9582  (best train acc: 0.9618, best val acc: 0.9619, best train loss: 0.1201  @ epoch 12117 )\n",
      "[Epoch: 12140] train loss: 0.1423, train acc: 0.9537, val loss: 0.1807, val acc: 0.9562  (best train acc: 0.9618, best val acc: 0.9629, best train loss: 0.1201  @ epoch 12117 )\n",
      "[Epoch: 12160] train loss: 0.1632, train acc: 0.9465, val loss: 0.1915, val acc: 0.9484  (best train acc: 0.9618, best val acc: 0.9629, best train loss: 0.1201  @ epoch 12117 )\n",
      "[Epoch: 12180] train loss: 0.1381, train acc: 0.9561, val loss: 0.1801, val acc: 0.9595  (best train acc: 0.9618, best val acc: 0.9629, best train loss: 0.1201  @ epoch 12117 )\n",
      "[Epoch: 12200] train loss: 0.1325, train acc: 0.9549, val loss: 0.1787, val acc: 0.9582  (best train acc: 0.9618, best val acc: 0.9629, best train loss: 0.1201  @ epoch 12117 )\n",
      "[Epoch: 12220] train loss: 0.1341, train acc: 0.9555, val loss: 0.1804, val acc: 0.9592  (best train acc: 0.9618, best val acc: 0.9629, best train loss: 0.1201  @ epoch 12117 )\n",
      "[Epoch: 12240] train loss: 0.1299, train acc: 0.9571, val loss: 0.1927, val acc: 0.9582  (best train acc: 0.9618, best val acc: 0.9629, best train loss: 0.1201  @ epoch 12117 )\n",
      "[Epoch: 12260] train loss: 0.1420, train acc: 0.9531, val loss: 0.1929, val acc: 0.9558  (best train acc: 0.9618, best val acc: 0.9629, best train loss: 0.1201  @ epoch 12117 )\n",
      "[Epoch: 12280] train loss: 0.1325, train acc: 0.9557, val loss: 0.1825, val acc: 0.9605  (best train acc: 0.9618, best val acc: 0.9629, best train loss: 0.1201  @ epoch 12117 )\n",
      "[Epoch: 12300] train loss: 0.1321, train acc: 0.9569, val loss: 0.1939, val acc: 0.9562  (best train acc: 0.9618, best val acc: 0.9629, best train loss: 0.1201  @ epoch 12117 )\n",
      "[Epoch: 12320] train loss: 0.1354, train acc: 0.9575, val loss: 0.1912, val acc: 0.9535  (best train acc: 0.9618, best val acc: 0.9629, best train loss: 0.1201  @ epoch 12117 )\n",
      "[Epoch: 12340] train loss: 0.1347, train acc: 0.9555, val loss: 0.2099, val acc: 0.9518  (best train acc: 0.9618, best val acc: 0.9629, best train loss: 0.1201  @ epoch 12117 )\n",
      "[Epoch: 12360] train loss: 0.1412, train acc: 0.9488, val loss: 0.2050, val acc: 0.9541  (best train acc: 0.9618, best val acc: 0.9629, best train loss: 0.1201  @ epoch 12117 )\n",
      "[Epoch: 12380] train loss: 0.1656, train acc: 0.9370, val loss: 0.2059, val acc: 0.9444  (best train acc: 0.9618, best val acc: 0.9629, best train loss: 0.1201  @ epoch 12117 )\n",
      "[Epoch: 12400] train loss: 0.1403, train acc: 0.9520, val loss: 0.1945, val acc: 0.9585  (best train acc: 0.9618, best val acc: 0.9629, best train loss: 0.1201  @ epoch 12117 )\n",
      "[Epoch: 12420] train loss: 0.1382, train acc: 0.9536, val loss: 0.1802, val acc: 0.9589  (best train acc: 0.9618, best val acc: 0.9629, best train loss: 0.1201  @ epoch 12117 )\n",
      "[Epoch: 12440] train loss: 0.1360, train acc: 0.9552, val loss: 0.1849, val acc: 0.9595  (best train acc: 0.9618, best val acc: 0.9629, best train loss: 0.1201  @ epoch 12117 )\n",
      "[Epoch: 12460] train loss: 0.1287, train acc: 0.9566, val loss: 0.1854, val acc: 0.9585  (best train acc: 0.9618, best val acc: 0.9629, best train loss: 0.1201  @ epoch 12117 )\n",
      "[Epoch: 12480] train loss: 0.1328, train acc: 0.9545, val loss: 0.2011, val acc: 0.9575  (best train acc: 0.9620, best val acc: 0.9629, best train loss: 0.1201  @ epoch 12117 )\n",
      "[Epoch: 12500] train loss: 0.1268, train acc: 0.9587, val loss: 0.1761, val acc: 0.9578  (best train acc: 0.9620, best val acc: 0.9629, best train loss: 0.1201  @ epoch 12117 )\n",
      "[Epoch: 12520] train loss: 0.1286, train acc: 0.9578, val loss: 0.1825, val acc: 0.9589  (best train acc: 0.9620, best val acc: 0.9629, best train loss: 0.1201  @ epoch 12516 )\n",
      "[Epoch: 12540] train loss: 0.1218, train acc: 0.9599, val loss: 0.1863, val acc: 0.9565  (best train acc: 0.9620, best val acc: 0.9629, best train loss: 0.1201  @ epoch 12516 )\n",
      "[Epoch: 12560] train loss: 0.1912, train acc: 0.9311, val loss: 0.1821, val acc: 0.9578  (best train acc: 0.9620, best val acc: 0.9629, best train loss: 0.1201  @ epoch 12516 )\n",
      "[Epoch: 12580] train loss: 0.1669, train acc: 0.9373, val loss: 0.2259, val acc: 0.9292  (best train acc: 0.9620, best val acc: 0.9629, best train loss: 0.1201  @ epoch 12516 )\n",
      "[Epoch: 12600] train loss: 0.1623, train acc: 0.9490, val loss: 0.2043, val acc: 0.9467  (best train acc: 0.9620, best val acc: 0.9629, best train loss: 0.1201  @ epoch 12516 )\n",
      "[Epoch: 12620] train loss: 0.1529, train acc: 0.9511, val loss: 0.1837, val acc: 0.9565  (best train acc: 0.9620, best val acc: 0.9629, best train loss: 0.1201  @ epoch 12516 )\n",
      "[Epoch: 12640] train loss: 0.1476, train acc: 0.9450, val loss: 0.2013, val acc: 0.9565  (best train acc: 0.9620, best val acc: 0.9629, best train loss: 0.1201  @ epoch 12516 )\n",
      "[Epoch: 12660] train loss: 0.1246, train acc: 0.9589, val loss: 0.1827, val acc: 0.9605  (best train acc: 0.9620, best val acc: 0.9629, best train loss: 0.1201  @ epoch 12516 )\n",
      "[Epoch: 12680] train loss: 0.1180, train acc: 0.9610, val loss: 0.1908, val acc: 0.9592  (best train acc: 0.9620, best val acc: 0.9629, best train loss: 0.1180  @ epoch 12680 )\n",
      "[Epoch: 12700] train loss: 0.1336, train acc: 0.9567, val loss: 0.1897, val acc: 0.9572  (best train acc: 0.9620, best val acc: 0.9629, best train loss: 0.1180  @ epoch 12680 )\n",
      "[Epoch: 12720] train loss: 0.1249, train acc: 0.9576, val loss: 0.1880, val acc: 0.9602  (best train acc: 0.9620, best val acc: 0.9629, best train loss: 0.1180  @ epoch 12680 )\n",
      "[Epoch: 12740] train loss: 0.1286, train acc: 0.9550, val loss: 0.2244, val acc: 0.9470  (best train acc: 0.9620, best val acc: 0.9629, best train loss: 0.1167  @ epoch 12728 )\n",
      "[Epoch: 12760] train loss: 0.1387, train acc: 0.9555, val loss: 0.1794, val acc: 0.9545  (best train acc: 0.9620, best val acc: 0.9629, best train loss: 0.1167  @ epoch 12728 )\n",
      "[Epoch: 12780] train loss: 0.1189, train acc: 0.9618, val loss: 0.1840, val acc: 0.9595  (best train acc: 0.9631, best val acc: 0.9629, best train loss: 0.1167  @ epoch 12728 )\n",
      "[Epoch: 12800] train loss: 0.1206, train acc: 0.9602, val loss: 0.2088, val acc: 0.9531  (best train acc: 0.9631, best val acc: 0.9629, best train loss: 0.1167  @ epoch 12728 )\n",
      "[Epoch: 12820] train loss: 0.1371, train acc: 0.9570, val loss: 0.1939, val acc: 0.9548  (best train acc: 0.9631, best val acc: 0.9629, best train loss: 0.1167  @ epoch 12728 )\n",
      "[Epoch: 12840] train loss: 0.1273, train acc: 0.9566, val loss: 0.1806, val acc: 0.9612  (best train acc: 0.9631, best val acc: 0.9629, best train loss: 0.1167  @ epoch 12728 )\n",
      "[Epoch: 12860] train loss: 0.1291, train acc: 0.9553, val loss: 0.1919, val acc: 0.9585  (best train acc: 0.9631, best val acc: 0.9629, best train loss: 0.1167  @ epoch 12728 )\n",
      "[Epoch: 12880] train loss: 0.1334, train acc: 0.9557, val loss: 0.1939, val acc: 0.9582  (best train acc: 0.9632, best val acc: 0.9629, best train loss: 0.1118  @ epoch 12864 )\n",
      "[Epoch: 12900] train loss: 0.1481, train acc: 0.9547, val loss: 0.2102, val acc: 0.9538  (best train acc: 0.9632, best val acc: 0.9629, best train loss: 0.1118  @ epoch 12864 )\n",
      "[Epoch: 12920] train loss: 0.1279, train acc: 0.9586, val loss: 0.1948, val acc: 0.9592  (best train acc: 0.9632, best val acc: 0.9629, best train loss: 0.1118  @ epoch 12864 )\n",
      "[Epoch: 12940] train loss: 0.1953, train acc: 0.9202, val loss: 0.1970, val acc: 0.9531  (best train acc: 0.9632, best val acc: 0.9629, best train loss: 0.1118  @ epoch 12864 )\n",
      "[Epoch: 12960] train loss: 0.1325, train acc: 0.9550, val loss: 0.1884, val acc: 0.9545  (best train acc: 0.9632, best val acc: 0.9629, best train loss: 0.1118  @ epoch 12864 )\n",
      "[Epoch: 12980] train loss: 0.1299, train acc: 0.9590, val loss: 0.1857, val acc: 0.9589  (best train acc: 0.9632, best val acc: 0.9629, best train loss: 0.1118  @ epoch 12864 )\n",
      "[Epoch: 13000] train loss: 0.1221, train acc: 0.9597, val loss: 0.1859, val acc: 0.9555  (best train acc: 0.9632, best val acc: 0.9629, best train loss: 0.1118  @ epoch 12864 )\n",
      "[Epoch: 13020] train loss: 0.1210, train acc: 0.9620, val loss: 0.1880, val acc: 0.9599  (best train acc: 0.9636, best val acc: 0.9629, best train loss: 0.1118  @ epoch 12864 )\n",
      "[Epoch: 13040] train loss: 0.1231, train acc: 0.9597, val loss: 0.1874, val acc: 0.9599  (best train acc: 0.9636, best val acc: 0.9629, best train loss: 0.1118  @ epoch 12864 )\n",
      "[Epoch: 13060] train loss: 0.1174, train acc: 0.9614, val loss: 0.1811, val acc: 0.9616  (best train acc: 0.9636, best val acc: 0.9629, best train loss: 0.1118  @ epoch 12864 )\n",
      "[Epoch: 13080] train loss: 0.1199, train acc: 0.9609, val loss: 0.1799, val acc: 0.9595  (best train acc: 0.9650, best val acc: 0.9629, best train loss: 0.1115  @ epoch 13065 )\n",
      "[Epoch: 13100] train loss: 0.1195, train acc: 0.9595, val loss: 0.1842, val acc: 0.9592  (best train acc: 0.9650, best val acc: 0.9629, best train loss: 0.1115  @ epoch 13065 )\n",
      "[Epoch: 13120] train loss: 0.1311, train acc: 0.9574, val loss: 0.1782, val acc: 0.9609  (best train acc: 0.9650, best val acc: 0.9632, best train loss: 0.1115  @ epoch 13065 )\n",
      "[Epoch: 13140] train loss: 0.1316, train acc: 0.9559, val loss: 0.1905, val acc: 0.9541  (best train acc: 0.9650, best val acc: 0.9632, best train loss: 0.1115  @ epoch 13065 )\n",
      "[Epoch: 13160] train loss: 0.1294, train acc: 0.9594, val loss: 0.1836, val acc: 0.9582  (best train acc: 0.9650, best val acc: 0.9632, best train loss: 0.1115  @ epoch 13065 )\n",
      "[Epoch: 13180] train loss: 0.1347, train acc: 0.9558, val loss: 0.1896, val acc: 0.9562  (best train acc: 0.9650, best val acc: 0.9632, best train loss: 0.1115  @ epoch 13065 )\n",
      "[Epoch: 13200] train loss: 0.1254, train acc: 0.9604, val loss: 0.1860, val acc: 0.9582  (best train acc: 0.9650, best val acc: 0.9632, best train loss: 0.1115  @ epoch 13065 )\n",
      "[Epoch: 13220] train loss: 0.1340, train acc: 0.9571, val loss: 0.1804, val acc: 0.9616  (best train acc: 0.9650, best val acc: 0.9632, best train loss: 0.1115  @ epoch 13065 )\n",
      "[Epoch: 13240] train loss: 0.1536, train acc: 0.9464, val loss: 0.1963, val acc: 0.9578  (best train acc: 0.9650, best val acc: 0.9632, best train loss: 0.1115  @ epoch 13065 )\n",
      "[Epoch: 13260] train loss: 0.1407, train acc: 0.9501, val loss: 0.2137, val acc: 0.9437  (best train acc: 0.9650, best val acc: 0.9632, best train loss: 0.1115  @ epoch 13065 )\n",
      "[Epoch: 13280] train loss: 0.1289, train acc: 0.9597, val loss: 0.1878, val acc: 0.9575  (best train acc: 0.9650, best val acc: 0.9632, best train loss: 0.1115  @ epoch 13065 )\n",
      "[Epoch: 13300] train loss: 0.1468, train acc: 0.9499, val loss: 0.2369, val acc: 0.9447  (best train acc: 0.9650, best val acc: 0.9632, best train loss: 0.1115  @ epoch 13065 )\n",
      "[Epoch: 13320] train loss: 0.1440, train acc: 0.9474, val loss: 0.1893, val acc: 0.9605  (best train acc: 0.9650, best val acc: 0.9632, best train loss: 0.1115  @ epoch 13065 )\n",
      "[Epoch: 13340] train loss: 0.1335, train acc: 0.9539, val loss: 0.1841, val acc: 0.9578  (best train acc: 0.9650, best val acc: 0.9632, best train loss: 0.1115  @ epoch 13065 )\n",
      "[Epoch: 13360] train loss: 0.1351, train acc: 0.9518, val loss: 0.1958, val acc: 0.9599  (best train acc: 0.9650, best val acc: 0.9632, best train loss: 0.1115  @ epoch 13065 )\n",
      "[Epoch: 13380] train loss: 0.1211, train acc: 0.9585, val loss: 0.2001, val acc: 0.9619  (best train acc: 0.9652, best val acc: 0.9632, best train loss: 0.1107  @ epoch 13376 )\n",
      "[Epoch: 13400] train loss: 0.1278, train acc: 0.9592, val loss: 0.1956, val acc: 0.9589  (best train acc: 0.9652, best val acc: 0.9632, best train loss: 0.1107  @ epoch 13376 )\n",
      "[Epoch: 13420] train loss: 0.1244, train acc: 0.9580, val loss: 0.2225, val acc: 0.9514  (best train acc: 0.9652, best val acc: 0.9642, best train loss: 0.1107  @ epoch 13376 )\n",
      "[Epoch: 13440] train loss: 0.1336, train acc: 0.9555, val loss: 0.1831, val acc: 0.9605  (best train acc: 0.9652, best val acc: 0.9642, best train loss: 0.1107  @ epoch 13376 )\n",
      "[Epoch: 13460] train loss: 0.1267, train acc: 0.9597, val loss: 0.1946, val acc: 0.9572  (best train acc: 0.9652, best val acc: 0.9642, best train loss: 0.1107  @ epoch 13376 )\n",
      "[Epoch: 13480] train loss: 0.1210, train acc: 0.9592, val loss: 0.1888, val acc: 0.9589  (best train acc: 0.9652, best val acc: 0.9642, best train loss: 0.1107  @ epoch 13376 )\n",
      "[Epoch: 13500] train loss: 0.1479, train acc: 0.9479, val loss: 0.1973, val acc: 0.9589  (best train acc: 0.9652, best val acc: 0.9642, best train loss: 0.1107  @ epoch 13376 )\n",
      "[Epoch: 13520] train loss: 0.1238, train acc: 0.9587, val loss: 0.1886, val acc: 0.9602  (best train acc: 0.9652, best val acc: 0.9642, best train loss: 0.1107  @ epoch 13376 )\n",
      "[Epoch: 13540] train loss: 0.1194, train acc: 0.9604, val loss: 0.1866, val acc: 0.9622  (best train acc: 0.9654, best val acc: 0.9642, best train loss: 0.1107  @ epoch 13376 )\n",
      "[Epoch: 13560] train loss: 0.1256, train acc: 0.9560, val loss: 0.1878, val acc: 0.9609  (best train acc: 0.9654, best val acc: 0.9646, best train loss: 0.1107  @ epoch 13376 )\n",
      "[Epoch: 13580] train loss: 0.2006, train acc: 0.9346, val loss: 0.1924, val acc: 0.9545  (best train acc: 0.9654, best val acc: 0.9646, best train loss: 0.1107  @ epoch 13376 )\n",
      "[Epoch: 13600] train loss: 0.1328, train acc: 0.9536, val loss: 0.1985, val acc: 0.9585  (best train acc: 0.9654, best val acc: 0.9646, best train loss: 0.1107  @ epoch 13376 )\n",
      "[Epoch: 13620] train loss: 0.1246, train acc: 0.9570, val loss: 0.1863, val acc: 0.9632  (best train acc: 0.9654, best val acc: 0.9646, best train loss: 0.1107  @ epoch 13376 )\n",
      "[Epoch: 13640] train loss: 0.1216, train acc: 0.9567, val loss: 0.1883, val acc: 0.9585  (best train acc: 0.9654, best val acc: 0.9646, best train loss: 0.1056  @ epoch 13639 )\n",
      "[Epoch: 13660] train loss: 0.1167, train acc: 0.9620, val loss: 0.1833, val acc: 0.9616  (best train acc: 0.9654, best val acc: 0.9646, best train loss: 0.1056  @ epoch 13639 )\n",
      "[Epoch: 13680] train loss: 0.1199, train acc: 0.9592, val loss: 0.1780, val acc: 0.9629  (best train acc: 0.9654, best val acc: 0.9646, best train loss: 0.1056  @ epoch 13639 )\n",
      "[Epoch: 13700] train loss: 0.1310, train acc: 0.9569, val loss: 0.1800, val acc: 0.9609  (best train acc: 0.9654, best val acc: 0.9646, best train loss: 0.1056  @ epoch 13639 )\n",
      "[Epoch: 13720] train loss: 0.1258, train acc: 0.9594, val loss: 0.1898, val acc: 0.9555  (best train acc: 0.9654, best val acc: 0.9646, best train loss: 0.1056  @ epoch 13639 )\n",
      "[Epoch: 13740] train loss: 0.1436, train acc: 0.9557, val loss: 0.1966, val acc: 0.9565  (best train acc: 0.9654, best val acc: 0.9646, best train loss: 0.1056  @ epoch 13639 )\n",
      "[Epoch: 13760] train loss: 0.1332, train acc: 0.9568, val loss: 0.1892, val acc: 0.9595  (best train acc: 0.9654, best val acc: 0.9646, best train loss: 0.1056  @ epoch 13639 )\n",
      "[Epoch: 13780] train loss: 0.1203, train acc: 0.9597, val loss: 0.1809, val acc: 0.9609  (best train acc: 0.9654, best val acc: 0.9646, best train loss: 0.1056  @ epoch 13639 )\n",
      "[Epoch: 13800] train loss: 0.1233, train acc: 0.9598, val loss: 0.1821, val acc: 0.9595  (best train acc: 0.9654, best val acc: 0.9646, best train loss: 0.1056  @ epoch 13639 )\n",
      "[Epoch: 13820] train loss: 0.1136, train acc: 0.9628, val loss: 0.1823, val acc: 0.9639  (best train acc: 0.9654, best val acc: 0.9646, best train loss: 0.1056  @ epoch 13639 )\n",
      "[Epoch: 13840] train loss: 0.1191, train acc: 0.9630, val loss: 0.1799, val acc: 0.9622  (best train acc: 0.9654, best val acc: 0.9646, best train loss: 0.1056  @ epoch 13639 )\n",
      "[Epoch: 13860] train loss: 0.1299, train acc: 0.9512, val loss: 0.2021, val acc: 0.9545  (best train acc: 0.9654, best val acc: 0.9646, best train loss: 0.1056  @ epoch 13639 )\n",
      "[Epoch: 13880] train loss: 0.1218, train acc: 0.9620, val loss: 0.1766, val acc: 0.9639  (best train acc: 0.9654, best val acc: 0.9646, best train loss: 0.1056  @ epoch 13639 )\n",
      "[Epoch: 13900] train loss: 0.1366, train acc: 0.9531, val loss: 0.1882, val acc: 0.9568  (best train acc: 0.9654, best val acc: 0.9646, best train loss: 0.1056  @ epoch 13639 )\n",
      "[Epoch: 13920] train loss: 0.1213, train acc: 0.9597, val loss: 0.1894, val acc: 0.9622  (best train acc: 0.9654, best val acc: 0.9646, best train loss: 0.1056  @ epoch 13639 )\n",
      "[Epoch: 13940] train loss: 0.1316, train acc: 0.9526, val loss: 0.1988, val acc: 0.9514  (best train acc: 0.9654, best val acc: 0.9646, best train loss: 0.1056  @ epoch 13639 )\n",
      "[Epoch: 13960] train loss: 0.1180, train acc: 0.9602, val loss: 0.1768, val acc: 0.9605  (best train acc: 0.9654, best val acc: 0.9646, best train loss: 0.1056  @ epoch 13639 )\n",
      "[Epoch: 13980] train loss: 0.1405, train acc: 0.9558, val loss: 0.1918, val acc: 0.9582  (best train acc: 0.9654, best val acc: 0.9646, best train loss: 0.1056  @ epoch 13639 )\n",
      "[Epoch: 14000] train loss: 0.1180, train acc: 0.9602, val loss: 0.1866, val acc: 0.9602  (best train acc: 0.9657, best val acc: 0.9646, best train loss: 0.1056  @ epoch 13639 )\n",
      "[Epoch: 14020] train loss: 0.1231, train acc: 0.9612, val loss: 0.1980, val acc: 0.9592  (best train acc: 0.9657, best val acc: 0.9646, best train loss: 0.1056  @ epoch 13639 )\n",
      "[Epoch: 14040] train loss: 0.1090, train acc: 0.9658, val loss: 0.1888, val acc: 0.9609  (best train acc: 0.9658, best val acc: 0.9646, best train loss: 0.1056  @ epoch 13639 )\n",
      "[Epoch: 14060] train loss: 0.1567, train acc: 0.9518, val loss: 0.2054, val acc: 0.9548  (best train acc: 0.9658, best val acc: 0.9646, best train loss: 0.1056  @ epoch 13639 )\n",
      "[Epoch: 14080] train loss: 0.1213, train acc: 0.9593, val loss: 0.1876, val acc: 0.9622  (best train acc: 0.9658, best val acc: 0.9646, best train loss: 0.1056  @ epoch 13639 )\n",
      "[Epoch: 14100] train loss: 0.1489, train acc: 0.9427, val loss: 0.2068, val acc: 0.9531  (best train acc: 0.9658, best val acc: 0.9646, best train loss: 0.1056  @ epoch 13639 )\n",
      "[Epoch: 14120] train loss: 0.1302, train acc: 0.9571, val loss: 0.1952, val acc: 0.9585  (best train acc: 0.9658, best val acc: 0.9646, best train loss: 0.1056  @ epoch 13639 )\n",
      "[Epoch: 14140] train loss: 0.1349, train acc: 0.9569, val loss: 0.1946, val acc: 0.9582  (best train acc: 0.9658, best val acc: 0.9646, best train loss: 0.1047  @ epoch 14128 )\n",
      "[Epoch: 14160] train loss: 0.1147, train acc: 0.9628, val loss: 0.1806, val acc: 0.9639  (best train acc: 0.9658, best val acc: 0.9649, best train loss: 0.1047  @ epoch 14128 )\n",
      "[Epoch: 14180] train loss: 0.1096, train acc: 0.9659, val loss: 0.1829, val acc: 0.9619  (best train acc: 0.9659, best val acc: 0.9649, best train loss: 0.1047  @ epoch 14128 )\n",
      "[Epoch: 14200] train loss: 0.1165, train acc: 0.9625, val loss: 0.1834, val acc: 0.9619  (best train acc: 0.9659, best val acc: 0.9649, best train loss: 0.1047  @ epoch 14128 )\n",
      "[Epoch: 14220] train loss: 0.1098, train acc: 0.9636, val loss: 0.1801, val acc: 0.9629  (best train acc: 0.9663, best val acc: 0.9649, best train loss: 0.1047  @ epoch 14128 )\n",
      "[Epoch: 14240] train loss: 0.1228, train acc: 0.9619, val loss: 0.1877, val acc: 0.9619  (best train acc: 0.9663, best val acc: 0.9653, best train loss: 0.1047  @ epoch 14128 )\n",
      "[Epoch: 14260] train loss: 0.1536, train acc: 0.9427, val loss: 0.1988, val acc: 0.9481  (best train acc: 0.9663, best val acc: 0.9653, best train loss: 0.1047  @ epoch 14128 )\n",
      "[Epoch: 14280] train loss: 0.1337, train acc: 0.9580, val loss: 0.1965, val acc: 0.9504  (best train acc: 0.9663, best val acc: 0.9653, best train loss: 0.1047  @ epoch 14128 )\n",
      "[Epoch: 14300] train loss: 0.1230, train acc: 0.9593, val loss: 0.1908, val acc: 0.9585  (best train acc: 0.9663, best val acc: 0.9653, best train loss: 0.1047  @ epoch 14128 )\n",
      "[Epoch: 14320] train loss: 0.1205, train acc: 0.9619, val loss: 0.1869, val acc: 0.9592  (best train acc: 0.9663, best val acc: 0.9653, best train loss: 0.1047  @ epoch 14128 )\n",
      "[Epoch: 14340] train loss: 0.1235, train acc: 0.9571, val loss: 0.2051, val acc: 0.9558  (best train acc: 0.9663, best val acc: 0.9653, best train loss: 0.1047  @ epoch 14128 )\n",
      "[Epoch: 14360] train loss: 0.1133, train acc: 0.9628, val loss: 0.1811, val acc: 0.9636  (best train acc: 0.9663, best val acc: 0.9653, best train loss: 0.1047  @ epoch 14128 )\n",
      "[Epoch: 14380] train loss: 0.1111, train acc: 0.9646, val loss: 0.1787, val acc: 0.9626  (best train acc: 0.9663, best val acc: 0.9653, best train loss: 0.1047  @ epoch 14128 )\n",
      "[Epoch: 14400] train loss: 0.1163, train acc: 0.9619, val loss: 0.1809, val acc: 0.9616  (best train acc: 0.9665, best val acc: 0.9653, best train loss: 0.1047  @ epoch 14128 )\n",
      "[Epoch: 14420] train loss: 0.1122, train acc: 0.9644, val loss: 0.1810, val acc: 0.9626  (best train acc: 0.9665, best val acc: 0.9653, best train loss: 0.1047  @ epoch 14128 )\n",
      "[Epoch: 14440] train loss: 0.1137, train acc: 0.9643, val loss: 0.1807, val acc: 0.9629  (best train acc: 0.9665, best val acc: 0.9653, best train loss: 0.1047  @ epoch 14128 )\n",
      "[Epoch: 14460] train loss: 0.1122, train acc: 0.9628, val loss: 0.1786, val acc: 0.9616  (best train acc: 0.9665, best val acc: 0.9653, best train loss: 0.1047  @ epoch 14128 )\n",
      "[Epoch: 14480] train loss: 0.1279, train acc: 0.9581, val loss: 0.1848, val acc: 0.9622  (best train acc: 0.9665, best val acc: 0.9653, best train loss: 0.1047  @ epoch 14128 )\n",
      "[Epoch: 14500] train loss: 0.1216, train acc: 0.9607, val loss: 0.2183, val acc: 0.9568  (best train acc: 0.9665, best val acc: 0.9653, best train loss: 0.1047  @ epoch 14128 )\n",
      "[Epoch: 14520] train loss: 0.1513, train acc: 0.9526, val loss: 0.1955, val acc: 0.9578  (best train acc: 0.9665, best val acc: 0.9653, best train loss: 0.1047  @ epoch 14128 )\n",
      "[Epoch: 14540] train loss: 0.1251, train acc: 0.9568, val loss: 0.1779, val acc: 0.9616  (best train acc: 0.9665, best val acc: 0.9653, best train loss: 0.1047  @ epoch 14128 )\n",
      "[Epoch: 14560] train loss: 0.1253, train acc: 0.9590, val loss: 0.1750, val acc: 0.9616  (best train acc: 0.9665, best val acc: 0.9653, best train loss: 0.1047  @ epoch 14128 )\n",
      "[Epoch: 14580] train loss: 0.1132, train acc: 0.9629, val loss: 0.1821, val acc: 0.9632  (best train acc: 0.9665, best val acc: 0.9653, best train loss: 0.1047  @ epoch 14128 )\n",
      "[Epoch: 14600] train loss: 0.1112, train acc: 0.9628, val loss: 0.1842, val acc: 0.9622  (best train acc: 0.9665, best val acc: 0.9653, best train loss: 0.1047  @ epoch 14128 )\n",
      "[Epoch: 14620] train loss: 0.1246, train acc: 0.9611, val loss: 0.1945, val acc: 0.9558  (best train acc: 0.9665, best val acc: 0.9653, best train loss: 0.1037  @ epoch 14610 )\n",
      "[Epoch: 14640] train loss: 0.1559, train acc: 0.9453, val loss: 0.1913, val acc: 0.9501  (best train acc: 0.9665, best val acc: 0.9653, best train loss: 0.1037  @ epoch 14610 )\n",
      "[Epoch: 14660] train loss: 0.1246, train acc: 0.9588, val loss: 0.1787, val acc: 0.9636  (best train acc: 0.9665, best val acc: 0.9653, best train loss: 0.1037  @ epoch 14610 )\n",
      "[Epoch: 14680] train loss: 0.1302, train acc: 0.9606, val loss: 0.1792, val acc: 0.9616  (best train acc: 0.9665, best val acc: 0.9653, best train loss: 0.1037  @ epoch 14610 )\n",
      "[Epoch: 14700] train loss: 0.1113, train acc: 0.9631, val loss: 0.1793, val acc: 0.9636  (best train acc: 0.9665, best val acc: 0.9653, best train loss: 0.1037  @ epoch 14610 )\n",
      "[Epoch: 14720] train loss: 0.1266, train acc: 0.9547, val loss: 0.1770, val acc: 0.9612  (best train acc: 0.9665, best val acc: 0.9653, best train loss: 0.1037  @ epoch 14610 )\n",
      "[Epoch: 14740] train loss: 0.1217, train acc: 0.9569, val loss: 0.1785, val acc: 0.9605  (best train acc: 0.9665, best val acc: 0.9653, best train loss: 0.1037  @ epoch 14610 )\n",
      "[Epoch: 14760] train loss: 0.1159, train acc: 0.9602, val loss: 0.1839, val acc: 0.9612  (best train acc: 0.9678, best val acc: 0.9653, best train loss: 0.1037  @ epoch 14610 )\n",
      "[Epoch: 14780] train loss: 0.1145, train acc: 0.9619, val loss: 0.1856, val acc: 0.9616  (best train acc: 0.9678, best val acc: 0.9653, best train loss: 0.1037  @ epoch 14610 )\n",
      "[Epoch: 14800] train loss: 0.1196, train acc: 0.9611, val loss: 0.1807, val acc: 0.9592  (best train acc: 0.9678, best val acc: 0.9653, best train loss: 0.1037  @ epoch 14610 )\n",
      "[Epoch: 14820] train loss: 0.1145, train acc: 0.9652, val loss: 0.1921, val acc: 0.9605  (best train acc: 0.9678, best val acc: 0.9653, best train loss: 0.1037  @ epoch 14610 )\n",
      "[Epoch: 14840] train loss: 0.1206, train acc: 0.9662, val loss: 0.1843, val acc: 0.9609  (best train acc: 0.9678, best val acc: 0.9653, best train loss: 0.1037  @ epoch 14610 )\n",
      "[Epoch: 14860] train loss: 0.1120, train acc: 0.9664, val loss: 0.1944, val acc: 0.9612  (best train acc: 0.9678, best val acc: 0.9653, best train loss: 0.1037  @ epoch 14610 )\n",
      "[Epoch: 14880] train loss: 0.1202, train acc: 0.9589, val loss: 0.1931, val acc: 0.9602  (best train acc: 0.9678, best val acc: 0.9653, best train loss: 0.1037  @ epoch 14610 )\n",
      "[Epoch: 14900] train loss: 0.1063, train acc: 0.9651, val loss: 0.2182, val acc: 0.9562  (best train acc: 0.9678, best val acc: 0.9653, best train loss: 0.1037  @ epoch 14610 )\n",
      "[Epoch: 14920] train loss: 0.1221, train acc: 0.9601, val loss: 0.1882, val acc: 0.9585  (best train acc: 0.9678, best val acc: 0.9653, best train loss: 0.1037  @ epoch 14610 )\n",
      "[Epoch: 14940] train loss: 0.1173, train acc: 0.9607, val loss: 0.1803, val acc: 0.9639  (best train acc: 0.9678, best val acc: 0.9653, best train loss: 0.1037  @ epoch 14610 )\n",
      "[Epoch: 14960] train loss: 0.1685, train acc: 0.9406, val loss: 0.1940, val acc: 0.9551  (best train acc: 0.9678, best val acc: 0.9653, best train loss: 0.1037  @ epoch 14610 )\n",
      "[Epoch: 14980] train loss: 0.1207, train acc: 0.9599, val loss: 0.1938, val acc: 0.9592  (best train acc: 0.9678, best val acc: 0.9653, best train loss: 0.1037  @ epoch 14610 )\n",
      "[Epoch: 15000] train loss: 0.1069, train acc: 0.9646, val loss: 0.1823, val acc: 0.9599  (best train acc: 0.9678, best val acc: 0.9653, best train loss: 0.1037  @ epoch 14610 )\n",
      "[Epoch: 15020] train loss: 0.1113, train acc: 0.9632, val loss: 0.1776, val acc: 0.9649  (best train acc: 0.9678, best val acc: 0.9653, best train loss: 0.1037  @ epoch 14610 )\n",
      "[Epoch: 15040] train loss: 0.1112, train acc: 0.9638, val loss: 0.1845, val acc: 0.9622  (best train acc: 0.9678, best val acc: 0.9653, best train loss: 0.1029  @ epoch 15036 )\n",
      "[Epoch: 15060] train loss: 0.1085, train acc: 0.9642, val loss: 0.1783, val acc: 0.9609  (best train acc: 0.9678, best val acc: 0.9653, best train loss: 0.1029  @ epoch 15036 )\n",
      "[Epoch: 15080] train loss: 0.1163, train acc: 0.9616, val loss: 0.1821, val acc: 0.9636  (best train acc: 0.9678, best val acc: 0.9653, best train loss: 0.1014  @ epoch 15072 )\n",
      "[Epoch: 15100] train loss: 0.1130, train acc: 0.9653, val loss: 0.1872, val acc: 0.9629  (best train acc: 0.9678, best val acc: 0.9653, best train loss: 0.1014  @ epoch 15072 )\n",
      "[Epoch: 15120] train loss: 0.1034, train acc: 0.9666, val loss: 0.1874, val acc: 0.9649  (best train acc: 0.9678, best val acc: 0.9656, best train loss: 0.1014  @ epoch 15072 )\n",
      "[Epoch: 15140] train loss: 0.1247, train acc: 0.9568, val loss: 0.1861, val acc: 0.9609  (best train acc: 0.9693, best val acc: 0.9656, best train loss: 0.0993  @ epoch 15121 )\n",
      "[Epoch: 15160] train loss: 0.1291, train acc: 0.9533, val loss: 0.2228, val acc: 0.9518  (best train acc: 0.9693, best val acc: 0.9656, best train loss: 0.0993  @ epoch 15121 )\n",
      "[Epoch: 15180] train loss: 0.1206, train acc: 0.9608, val loss: 0.1797, val acc: 0.9599  (best train acc: 0.9693, best val acc: 0.9656, best train loss: 0.0993  @ epoch 15121 )\n",
      "[Epoch: 15200] train loss: 0.1216, train acc: 0.9616, val loss: 0.1705, val acc: 0.9619  (best train acc: 0.9693, best val acc: 0.9656, best train loss: 0.0993  @ epoch 15121 )\n",
      "[Epoch: 15220] train loss: 0.1119, train acc: 0.9613, val loss: 0.1827, val acc: 0.9632  (best train acc: 0.9693, best val acc: 0.9656, best train loss: 0.0993  @ epoch 15121 )\n",
      "[Epoch: 15240] train loss: 0.1099, train acc: 0.9654, val loss: 0.1752, val acc: 0.9653  (best train acc: 0.9693, best val acc: 0.9656, best train loss: 0.0993  @ epoch 15121 )\n",
      "[Epoch: 15260] train loss: 0.1292, train acc: 0.9570, val loss: 0.1979, val acc: 0.9528  (best train acc: 0.9693, best val acc: 0.9656, best train loss: 0.0993  @ epoch 15121 )\n",
      "[Epoch: 15280] train loss: 0.1533, train acc: 0.9518, val loss: 0.2306, val acc: 0.9410  (best train acc: 0.9693, best val acc: 0.9656, best train loss: 0.0993  @ epoch 15121 )\n",
      "[Epoch: 15300] train loss: 0.1210, train acc: 0.9543, val loss: 0.1720, val acc: 0.9619  (best train acc: 0.9693, best val acc: 0.9656, best train loss: 0.0993  @ epoch 15121 )\n",
      "[Epoch: 15320] train loss: 0.1120, train acc: 0.9633, val loss: 0.2108, val acc: 0.9514  (best train acc: 0.9693, best val acc: 0.9656, best train loss: 0.0993  @ epoch 15121 )\n",
      "[Epoch: 15340] train loss: 0.1095, train acc: 0.9643, val loss: 0.1844, val acc: 0.9626  (best train acc: 0.9693, best val acc: 0.9656, best train loss: 0.0993  @ epoch 15121 )\n",
      "[Epoch: 15360] train loss: 0.1122, train acc: 0.9609, val loss: 0.1884, val acc: 0.9632  (best train acc: 0.9693, best val acc: 0.9659, best train loss: 0.0993  @ epoch 15121 )\n",
      "[Epoch: 15380] train loss: 0.1055, train acc: 0.9649, val loss: 0.1762, val acc: 0.9612  (best train acc: 0.9693, best val acc: 0.9663, best train loss: 0.0993  @ epoch 15121 )\n",
      "[Epoch: 15400] train loss: 0.1042, train acc: 0.9649, val loss: 0.1749, val acc: 0.9642  (best train acc: 0.9693, best val acc: 0.9663, best train loss: 0.0993  @ epoch 15121 )\n",
      "[Epoch: 15420] train loss: 0.1068, train acc: 0.9666, val loss: 0.1833, val acc: 0.9626  (best train acc: 0.9693, best val acc: 0.9663, best train loss: 0.0966  @ epoch 15415 )\n",
      "[Epoch: 15440] train loss: 0.1137, train acc: 0.9626, val loss: 0.1782, val acc: 0.9602  (best train acc: 0.9693, best val acc: 0.9663, best train loss: 0.0966  @ epoch 15415 )\n",
      "[Epoch: 15460] train loss: 0.1150, train acc: 0.9625, val loss: 0.1788, val acc: 0.9629  (best train acc: 0.9693, best val acc: 0.9663, best train loss: 0.0966  @ epoch 15415 )\n",
      "[Epoch: 15480] train loss: 0.1351, train acc: 0.9523, val loss: 0.2006, val acc: 0.9545  (best train acc: 0.9693, best val acc: 0.9663, best train loss: 0.0966  @ epoch 15415 )\n",
      "[Epoch: 15500] train loss: 0.2095, train acc: 0.9445, val loss: 0.1889, val acc: 0.9538  (best train acc: 0.9693, best val acc: 0.9663, best train loss: 0.0966  @ epoch 15415 )\n",
      "[Epoch: 15520] train loss: 0.1055, train acc: 0.9657, val loss: 0.1881, val acc: 0.9595  (best train acc: 0.9693, best val acc: 0.9663, best train loss: 0.0966  @ epoch 15415 )\n",
      "[Epoch: 15540] train loss: 0.1080, train acc: 0.9652, val loss: 0.1854, val acc: 0.9646  (best train acc: 0.9693, best val acc: 0.9663, best train loss: 0.0966  @ epoch 15415 )\n",
      "[Epoch: 15560] train loss: 0.1116, train acc: 0.9650, val loss: 0.1825, val acc: 0.9626  (best train acc: 0.9693, best val acc: 0.9663, best train loss: 0.0966  @ epoch 15415 )\n",
      "[Epoch: 15580] train loss: 0.1349, train acc: 0.9501, val loss: 0.1801, val acc: 0.9609  (best train acc: 0.9693, best val acc: 0.9663, best train loss: 0.0966  @ epoch 15415 )\n",
      "[Epoch: 15600] train loss: 0.1054, train acc: 0.9664, val loss: 0.1722, val acc: 0.9653  (best train acc: 0.9693, best val acc: 0.9663, best train loss: 0.0966  @ epoch 15415 )\n",
      "[Epoch: 15620] train loss: 0.1021, train acc: 0.9672, val loss: 0.1822, val acc: 0.9622  (best train acc: 0.9693, best val acc: 0.9663, best train loss: 0.0966  @ epoch 15415 )\n",
      "[Epoch: 15640] train loss: 0.1061, train acc: 0.9661, val loss: 0.1869, val acc: 0.9619  (best train acc: 0.9693, best val acc: 0.9663, best train loss: 0.0966  @ epoch 15415 )\n",
      "[Epoch: 15660] train loss: 0.1073, train acc: 0.9694, val loss: 0.1798, val acc: 0.9646  (best train acc: 0.9694, best val acc: 0.9663, best train loss: 0.0966  @ epoch 15415 )\n",
      "[Epoch: 15680] train loss: 0.1048, train acc: 0.9664, val loss: 0.1757, val acc: 0.9639  (best train acc: 0.9694, best val acc: 0.9663, best train loss: 0.0966  @ epoch 15415 )\n",
      "[Epoch: 15700] train loss: 0.1137, train acc: 0.9641, val loss: 0.1908, val acc: 0.9578  (best train acc: 0.9696, best val acc: 0.9663, best train loss: 0.0963  @ epoch 15696 )\n",
      "[Epoch: 15720] train loss: 0.1105, train acc: 0.9665, val loss: 0.1744, val acc: 0.9626  (best train acc: 0.9696, best val acc: 0.9663, best train loss: 0.0963  @ epoch 15696 )\n",
      "[Epoch: 15740] train loss: 0.1170, train acc: 0.9597, val loss: 0.2112, val acc: 0.9541  (best train acc: 0.9696, best val acc: 0.9663, best train loss: 0.0963  @ epoch 15696 )\n",
      "[Epoch: 15760] train loss: 0.1266, train acc: 0.9577, val loss: 0.1727, val acc: 0.9649  (best train acc: 0.9696, best val acc: 0.9663, best train loss: 0.0963  @ epoch 15696 )\n",
      "[Epoch: 15780] train loss: 0.1241, train acc: 0.9593, val loss: 0.1730, val acc: 0.9619  (best train acc: 0.9696, best val acc: 0.9663, best train loss: 0.0963  @ epoch 15696 )\n",
      "[Epoch: 15800] train loss: 0.1123, train acc: 0.9623, val loss: 0.1769, val acc: 0.9632  (best train acc: 0.9696, best val acc: 0.9663, best train loss: 0.0963  @ epoch 15696 )\n",
      "[Epoch: 15820] train loss: 0.1076, train acc: 0.9649, val loss: 0.1875, val acc: 0.9582  (best train acc: 0.9696, best val acc: 0.9663, best train loss: 0.0963  @ epoch 15814 )\n",
      "[Epoch: 15840] train loss: 0.1097, train acc: 0.9633, val loss: 0.1898, val acc: 0.9585  (best train acc: 0.9696, best val acc: 0.9663, best train loss: 0.0963  @ epoch 15814 )\n",
      "[Epoch: 15860] train loss: 0.1051, train acc: 0.9618, val loss: 0.2043, val acc: 0.9531  (best train acc: 0.9696, best val acc: 0.9663, best train loss: 0.0963  @ epoch 15814 )\n",
      "[Epoch: 15880] train loss: 0.1198, train acc: 0.9604, val loss: 0.1769, val acc: 0.9605  (best train acc: 0.9696, best val acc: 0.9663, best train loss: 0.0963  @ epoch 15814 )\n",
      "[Epoch: 15900] train loss: 0.1128, train acc: 0.9652, val loss: 0.1868, val acc: 0.9616  (best train acc: 0.9696, best val acc: 0.9663, best train loss: 0.0963  @ epoch 15814 )\n",
      "[Epoch: 15920] train loss: 0.1063, train acc: 0.9656, val loss: 0.1791, val acc: 0.9592  (best train acc: 0.9696, best val acc: 0.9663, best train loss: 0.0963  @ epoch 15814 )\n",
      "[Epoch: 15940] train loss: 0.1235, train acc: 0.9568, val loss: 0.1817, val acc: 0.9582  (best train acc: 0.9696, best val acc: 0.9663, best train loss: 0.0963  @ epoch 15814 )\n",
      "[Epoch: 15960] train loss: 0.1042, train acc: 0.9644, val loss: 0.1778, val acc: 0.9629  (best train acc: 0.9696, best val acc: 0.9663, best train loss: 0.0963  @ epoch 15814 )\n",
      "[Epoch: 15980] train loss: 0.0991, train acc: 0.9676, val loss: 0.1716, val acc: 0.9663  (best train acc: 0.9696, best val acc: 0.9663, best train loss: 0.0963  @ epoch 15814 )\n",
      "[Epoch: 16000] train loss: 0.1193, train acc: 0.9612, val loss: 0.1850, val acc: 0.9585  (best train acc: 0.9696, best val acc: 0.9666, best train loss: 0.0963  @ epoch 15814 )\n",
      "[Epoch: 16020] train loss: 0.1050, train acc: 0.9668, val loss: 0.2050, val acc: 0.9568  (best train acc: 0.9696, best val acc: 0.9666, best train loss: 0.0963  @ epoch 15814 )\n",
      "[Epoch: 16040] train loss: 0.1022, train acc: 0.9683, val loss: 0.1712, val acc: 0.9653  (best train acc: 0.9696, best val acc: 0.9666, best train loss: 0.0963  @ epoch 15814 )\n",
      "[Epoch: 16060] train loss: 0.1101, train acc: 0.9648, val loss: 0.1779, val acc: 0.9646  (best train acc: 0.9696, best val acc: 0.9666, best train loss: 0.0963  @ epoch 15814 )\n",
      "[Epoch: 16080] train loss: 0.1276, train acc: 0.9579, val loss: 0.1954, val acc: 0.9558  (best train acc: 0.9696, best val acc: 0.9666, best train loss: 0.0963  @ epoch 15814 )\n",
      "[Epoch: 16100] train loss: 0.1310, train acc: 0.9511, val loss: 0.1822, val acc: 0.9585  (best train acc: 0.9696, best val acc: 0.9666, best train loss: 0.0963  @ epoch 15814 )\n",
      "[Epoch: 16120] train loss: 0.1133, train acc: 0.9644, val loss: 0.1743, val acc: 0.9656  (best train acc: 0.9696, best val acc: 0.9666, best train loss: 0.0963  @ epoch 15814 )\n",
      "[Epoch: 16140] train loss: 0.1036, train acc: 0.9656, val loss: 0.1753, val acc: 0.9639  (best train acc: 0.9696, best val acc: 0.9666, best train loss: 0.0963  @ epoch 15814 )\n",
      "[Epoch: 16160] train loss: 0.1059, train acc: 0.9631, val loss: 0.1749, val acc: 0.9626  (best train acc: 0.9696, best val acc: 0.9666, best train loss: 0.0963  @ epoch 15814 )\n",
      "[Epoch: 16180] train loss: 0.1024, train acc: 0.9676, val loss: 0.1877, val acc: 0.9589  (best train acc: 0.9696, best val acc: 0.9666, best train loss: 0.0963  @ epoch 15814 )\n",
      "[Epoch: 16200] train loss: 0.1022, train acc: 0.9657, val loss: 0.1791, val acc: 0.9605  (best train acc: 0.9696, best val acc: 0.9666, best train loss: 0.0963  @ epoch 15814 )\n",
      "[Epoch: 16220] train loss: 0.1001, train acc: 0.9683, val loss: 0.1768, val acc: 0.9636  (best train acc: 0.9700, best val acc: 0.9666, best train loss: 0.0918  @ epoch 16209 )\n",
      "[Epoch: 16240] train loss: 0.1010, train acc: 0.9675, val loss: 0.1790, val acc: 0.9619  (best train acc: 0.9700, best val acc: 0.9666, best train loss: 0.0918  @ epoch 16209 )\n",
      "[Epoch: 16260] train loss: 0.1062, train acc: 0.9647, val loss: 0.1791, val acc: 0.9629  (best train acc: 0.9700, best val acc: 0.9666, best train loss: 0.0918  @ epoch 16209 )\n",
      "[Epoch: 16280] train loss: 0.1116, train acc: 0.9639, val loss: 0.1797, val acc: 0.9632  (best train acc: 0.9700, best val acc: 0.9666, best train loss: 0.0918  @ epoch 16209 )\n",
      "[Epoch: 16300] train loss: 0.1176, train acc: 0.9618, val loss: 0.1949, val acc: 0.9551  (best train acc: 0.9700, best val acc: 0.9666, best train loss: 0.0918  @ epoch 16209 )\n",
      "[Epoch: 16320] train loss: 0.1050, train acc: 0.9651, val loss: 0.2131, val acc: 0.9541  (best train acc: 0.9700, best val acc: 0.9666, best train loss: 0.0918  @ epoch 16209 )\n",
      "[Epoch: 16340] train loss: 0.1294, train acc: 0.9562, val loss: 0.1957, val acc: 0.9551  (best train acc: 0.9700, best val acc: 0.9666, best train loss: 0.0918  @ epoch 16209 )\n",
      "[Epoch: 16360] train loss: 0.1149, train acc: 0.9608, val loss: 0.1764, val acc: 0.9575  (best train acc: 0.9700, best val acc: 0.9666, best train loss: 0.0918  @ epoch 16209 )\n",
      "[Epoch: 16380] train loss: 0.1142, train acc: 0.9632, val loss: 0.1819, val acc: 0.9609  (best train acc: 0.9700, best val acc: 0.9666, best train loss: 0.0918  @ epoch 16209 )\n",
      "[Epoch: 16400] train loss: 0.1067, train acc: 0.9661, val loss: 0.1813, val acc: 0.9629  (best train acc: 0.9700, best val acc: 0.9676, best train loss: 0.0918  @ epoch 16209 )\n",
      "[Epoch: 16420] train loss: 0.1311, train acc: 0.9605, val loss: 0.1786, val acc: 0.9595  (best train acc: 0.9700, best val acc: 0.9676, best train loss: 0.0918  @ epoch 16209 )\n",
      "[Epoch: 16440] train loss: 0.1149, train acc: 0.9647, val loss: 0.1748, val acc: 0.9572  (best train acc: 0.9700, best val acc: 0.9676, best train loss: 0.0918  @ epoch 16209 )\n",
      "[Epoch: 16460] train loss: 0.1017, train acc: 0.9674, val loss: 0.1697, val acc: 0.9653  (best train acc: 0.9700, best val acc: 0.9676, best train loss: 0.0918  @ epoch 16209 )\n",
      "[Epoch: 16480] train loss: 0.1098, train acc: 0.9631, val loss: 0.1737, val acc: 0.9636  (best train acc: 0.9700, best val acc: 0.9676, best train loss: 0.0918  @ epoch 16209 )\n",
      "[Epoch: 16500] train loss: 0.1103, train acc: 0.9605, val loss: 0.1756, val acc: 0.9646  (best train acc: 0.9700, best val acc: 0.9676, best train loss: 0.0918  @ epoch 16209 )\n",
      "[Epoch: 16520] train loss: 0.1053, train acc: 0.9669, val loss: 0.1793, val acc: 0.9626  (best train acc: 0.9702, best val acc: 0.9676, best train loss: 0.0915  @ epoch 16509 )\n",
      "[Epoch: 16540] train loss: 0.0990, train acc: 0.9680, val loss: 0.1779, val acc: 0.9636  (best train acc: 0.9702, best val acc: 0.9676, best train loss: 0.0915  @ epoch 16509 )\n",
      "[Epoch: 16560] train loss: 0.1089, train acc: 0.9636, val loss: 0.1708, val acc: 0.9629  (best train acc: 0.9702, best val acc: 0.9676, best train loss: 0.0915  @ epoch 16509 )\n",
      "[Epoch: 16580] train loss: 0.1005, train acc: 0.9670, val loss: 0.1689, val acc: 0.9626  (best train acc: 0.9702, best val acc: 0.9676, best train loss: 0.0915  @ epoch 16509 )\n",
      "[Epoch: 16600] train loss: 0.1077, train acc: 0.9636, val loss: 0.1697, val acc: 0.9636  (best train acc: 0.9708, best val acc: 0.9676, best train loss: 0.0915  @ epoch 16509 )\n",
      "[Epoch: 16620] train loss: 0.1023, train acc: 0.9677, val loss: 0.1798, val acc: 0.9609  (best train acc: 0.9708, best val acc: 0.9676, best train loss: 0.0915  @ epoch 16509 )\n",
      "[Epoch: 16640] train loss: 0.1101, train acc: 0.9659, val loss: 0.1710, val acc: 0.9649  (best train acc: 0.9708, best val acc: 0.9676, best train loss: 0.0915  @ epoch 16509 )\n",
      "[Epoch: 16660] train loss: 0.1105, train acc: 0.9626, val loss: 0.1819, val acc: 0.9609  (best train acc: 0.9708, best val acc: 0.9676, best train loss: 0.0915  @ epoch 16509 )\n",
      "[Epoch: 16680] train loss: 0.1148, train acc: 0.9647, val loss: 0.1831, val acc: 0.9609  (best train acc: 0.9708, best val acc: 0.9676, best train loss: 0.0915  @ epoch 16509 )\n",
      "[Epoch: 16700] train loss: 0.1101, train acc: 0.9601, val loss: 0.1957, val acc: 0.9558  (best train acc: 0.9708, best val acc: 0.9676, best train loss: 0.0915  @ epoch 16509 )\n",
      "[Epoch: 16720] train loss: 0.1401, train acc: 0.9520, val loss: 0.1844, val acc: 0.9568  (best train acc: 0.9708, best val acc: 0.9676, best train loss: 0.0915  @ epoch 16509 )\n",
      "[Epoch: 16740] train loss: 0.1034, train acc: 0.9649, val loss: 0.1712, val acc: 0.9602  (best train acc: 0.9708, best val acc: 0.9676, best train loss: 0.0915  @ epoch 16509 )\n",
      "[Epoch: 16760] train loss: 0.0986, train acc: 0.9668, val loss: 0.1662, val acc: 0.9639  (best train acc: 0.9708, best val acc: 0.9676, best train loss: 0.0915  @ epoch 16509 )\n",
      "[Epoch: 16780] train loss: 0.1148, train acc: 0.9602, val loss: 0.1603, val acc: 0.9649  (best train acc: 0.9708, best val acc: 0.9676, best train loss: 0.0915  @ epoch 16509 )\n",
      "[Epoch: 16800] train loss: 0.1046, train acc: 0.9657, val loss: 0.1611, val acc: 0.9589  (best train acc: 0.9708, best val acc: 0.9676, best train loss: 0.0915  @ epoch 16509 )\n",
      "[Epoch: 16820] train loss: 0.1053, train acc: 0.9654, val loss: 0.1666, val acc: 0.9626  (best train acc: 0.9708, best val acc: 0.9676, best train loss: 0.0915  @ epoch 16509 )\n",
      "[Epoch: 16840] train loss: 0.0981, train acc: 0.9684, val loss: 0.1652, val acc: 0.9636  (best train acc: 0.9708, best val acc: 0.9676, best train loss: 0.0915  @ epoch 16509 )\n",
      "[Epoch: 16860] train loss: 0.1052, train acc: 0.9670, val loss: 0.1791, val acc: 0.9589  (best train acc: 0.9708, best val acc: 0.9676, best train loss: 0.0915  @ epoch 16509 )\n",
      "[Epoch: 16880] train loss: 0.1192, train acc: 0.9644, val loss: 0.1648, val acc: 0.9622  (best train acc: 0.9708, best val acc: 0.9676, best train loss: 0.0915  @ epoch 16509 )\n",
      "[Epoch: 16900] train loss: 0.1014, train acc: 0.9675, val loss: 0.1702, val acc: 0.9626  (best train acc: 0.9708, best val acc: 0.9676, best train loss: 0.0915  @ epoch 16509 )\n",
      "[Epoch: 16920] train loss: 0.1099, train acc: 0.9674, val loss: 0.1792, val acc: 0.9619  (best train acc: 0.9709, best val acc: 0.9676, best train loss: 0.0915  @ epoch 16509 )\n",
      "[Epoch: 16940] train loss: 0.1360, train acc: 0.9549, val loss: 0.2649, val acc: 0.9336  (best train acc: 0.9709, best val acc: 0.9676, best train loss: 0.0915  @ epoch 16509 )\n",
      "[Epoch: 16960] train loss: 0.1172, train acc: 0.9639, val loss: 0.1653, val acc: 0.9622  (best train acc: 0.9709, best val acc: 0.9676, best train loss: 0.0915  @ epoch 16509 )\n",
      "[Epoch: 16980] train loss: 0.1041, train acc: 0.9641, val loss: 0.1660, val acc: 0.9629  (best train acc: 0.9709, best val acc: 0.9676, best train loss: 0.0915  @ epoch 16509 )\n",
      "[Epoch: 17000] train loss: 0.0986, train acc: 0.9689, val loss: 0.1602, val acc: 0.9636  (best train acc: 0.9709, best val acc: 0.9676, best train loss: 0.0915  @ epoch 16509 )\n",
      "[Epoch: 17020] train loss: 0.0990, train acc: 0.9692, val loss: 0.1663, val acc: 0.9639  (best train acc: 0.9709, best val acc: 0.9676, best train loss: 0.0915  @ epoch 16509 )\n",
      "[Epoch: 17040] train loss: 0.1331, train acc: 0.9555, val loss: 0.3178, val acc: 0.9245  (best train acc: 0.9709, best val acc: 0.9676, best train loss: 0.0915  @ epoch 16509 )\n",
      "[Epoch: 17060] train loss: 0.1175, train acc: 0.9620, val loss: 0.1670, val acc: 0.9575  (best train acc: 0.9709, best val acc: 0.9676, best train loss: 0.0915  @ epoch 16509 )\n",
      "[Epoch: 17080] train loss: 0.1166, train acc: 0.9608, val loss: 0.1563, val acc: 0.9639  (best train acc: 0.9709, best val acc: 0.9676, best train loss: 0.0915  @ epoch 16509 )\n",
      "[Epoch: 17100] train loss: 0.1027, train acc: 0.9663, val loss: 0.1642, val acc: 0.9626  (best train acc: 0.9709, best val acc: 0.9676, best train loss: 0.0915  @ epoch 16509 )\n",
      "[Epoch: 17120] train loss: 0.0970, train acc: 0.9688, val loss: 0.1576, val acc: 0.9653  (best train acc: 0.9709, best val acc: 0.9676, best train loss: 0.0915  @ epoch 16509 )\n",
      "[Epoch: 17140] train loss: 0.1179, train acc: 0.9589, val loss: 0.1689, val acc: 0.9605  (best train acc: 0.9709, best val acc: 0.9680, best train loss: 0.0915  @ epoch 16509 )\n",
      "[Epoch: 17160] train loss: 0.1061, train acc: 0.9644, val loss: 0.1674, val acc: 0.9619  (best train acc: 0.9709, best val acc: 0.9680, best train loss: 0.0915  @ epoch 16509 )\n",
      "[Epoch: 17180] train loss: 0.1166, train acc: 0.9603, val loss: 0.1667, val acc: 0.9602  (best train acc: 0.9709, best val acc: 0.9680, best train loss: 0.0890  @ epoch 17176 )\n",
      "[Epoch: 17200] train loss: 0.1104, train acc: 0.9636, val loss: 0.1796, val acc: 0.9599  (best train acc: 0.9709, best val acc: 0.9680, best train loss: 0.0890  @ epoch 17176 )\n",
      "[Epoch: 17220] train loss: 0.0988, train acc: 0.9659, val loss: 0.1803, val acc: 0.9589  (best train acc: 0.9709, best val acc: 0.9680, best train loss: 0.0890  @ epoch 17176 )\n",
      "[Epoch: 17240] train loss: 0.1007, train acc: 0.9669, val loss: 0.1620, val acc: 0.9612  (best train acc: 0.9709, best val acc: 0.9680, best train loss: 0.0890  @ epoch 17176 )\n",
      "[Epoch: 17260] train loss: 0.1023, train acc: 0.9665, val loss: 0.1629, val acc: 0.9649  (best train acc: 0.9709, best val acc: 0.9680, best train loss: 0.0890  @ epoch 17176 )\n",
      "[Epoch: 17280] train loss: 0.1061, train acc: 0.9651, val loss: 0.1613, val acc: 0.9676  (best train acc: 0.9710, best val acc: 0.9680, best train loss: 0.0890  @ epoch 17176 )\n",
      "[Epoch: 17300] train loss: 0.1061, train acc: 0.9664, val loss: 0.1570, val acc: 0.9632  (best train acc: 0.9710, best val acc: 0.9680, best train loss: 0.0890  @ epoch 17176 )\n",
      "[Epoch: 17320] train loss: 0.0995, train acc: 0.9662, val loss: 0.1688, val acc: 0.9642  (best train acc: 0.9710, best val acc: 0.9680, best train loss: 0.0890  @ epoch 17176 )\n",
      "[Epoch: 17340] train loss: 0.1028, train acc: 0.9667, val loss: 0.1692, val acc: 0.9619  (best train acc: 0.9710, best val acc: 0.9680, best train loss: 0.0890  @ epoch 17176 )\n",
      "[Epoch: 17360] train loss: 0.1419, train acc: 0.9541, val loss: 0.1984, val acc: 0.9501  (best train acc: 0.9710, best val acc: 0.9680, best train loss: 0.0890  @ epoch 17176 )\n",
      "[Epoch: 17380] train loss: 0.1149, train acc: 0.9615, val loss: 0.1712, val acc: 0.9592  (best train acc: 0.9710, best val acc: 0.9680, best train loss: 0.0890  @ epoch 17176 )\n",
      "[Epoch: 17400] train loss: 0.1023, train acc: 0.9668, val loss: 0.1646, val acc: 0.9642  (best train acc: 0.9710, best val acc: 0.9680, best train loss: 0.0890  @ epoch 17176 )\n",
      "[Epoch: 17420] train loss: 0.1205, train acc: 0.9618, val loss: 0.1621, val acc: 0.9609  (best train acc: 0.9710, best val acc: 0.9680, best train loss: 0.0890  @ epoch 17176 )\n",
      "[Epoch: 17440] train loss: 0.1031, train acc: 0.9654, val loss: 0.1453, val acc: 0.9649  (best train acc: 0.9710, best val acc: 0.9680, best train loss: 0.0890  @ epoch 17176 )\n",
      "[Epoch: 17460] train loss: 0.1018, train acc: 0.9649, val loss: 0.1677, val acc: 0.9626  (best train acc: 0.9718, best val acc: 0.9680, best train loss: 0.0890  @ epoch 17176 )\n",
      "[Epoch: 17480] train loss: 0.1289, train acc: 0.9588, val loss: 0.1536, val acc: 0.9578  (best train acc: 0.9718, best val acc: 0.9680, best train loss: 0.0890  @ epoch 17176 )\n",
      "[Epoch: 17500] train loss: 0.1100, train acc: 0.9652, val loss: 0.1790, val acc: 0.9649  (best train acc: 0.9718, best val acc: 0.9680, best train loss: 0.0890  @ epoch 17176 )\n",
      "[Epoch: 17520] train loss: 0.1104, train acc: 0.9636, val loss: 0.1795, val acc: 0.9572  (best train acc: 0.9718, best val acc: 0.9680, best train loss: 0.0890  @ epoch 17176 )\n",
      "[Epoch: 17540] train loss: 0.0994, train acc: 0.9670, val loss: 0.1521, val acc: 0.9663  (best train acc: 0.9718, best val acc: 0.9680, best train loss: 0.0890  @ epoch 17176 )\n",
      "[Epoch: 17560] train loss: 0.0940, train acc: 0.9690, val loss: 0.1655, val acc: 0.9626  (best train acc: 0.9718, best val acc: 0.9680, best train loss: 0.0890  @ epoch 17176 )\n",
      "[Epoch: 17580] train loss: 0.1193, train acc: 0.9558, val loss: 0.1806, val acc: 0.9555  (best train acc: 0.9718, best val acc: 0.9680, best train loss: 0.0890  @ epoch 17176 )\n",
      "[Epoch: 17600] train loss: 0.1000, train acc: 0.9678, val loss: 0.1648, val acc: 0.9622  (best train acc: 0.9718, best val acc: 0.9680, best train loss: 0.0890  @ epoch 17176 )\n",
      "[Epoch: 17620] train loss: 0.0974, train acc: 0.9664, val loss: 0.1704, val acc: 0.9612  (best train acc: 0.9718, best val acc: 0.9680, best train loss: 0.0890  @ epoch 17176 )\n",
      "[Epoch: 17640] train loss: 0.0981, train acc: 0.9656, val loss: 0.1859, val acc: 0.9545  (best train acc: 0.9718, best val acc: 0.9680, best train loss: 0.0890  @ epoch 17176 )\n",
      "[Epoch: 17660] train loss: 0.1014, train acc: 0.9649, val loss: 0.1651, val acc: 0.9636  (best train acc: 0.9718, best val acc: 0.9680, best train loss: 0.0890  @ epoch 17176 )\n",
      "[Epoch: 17680] train loss: 0.0972, train acc: 0.9644, val loss: 0.1659, val acc: 0.9632  (best train acc: 0.9718, best val acc: 0.9680, best train loss: 0.0890  @ epoch 17176 )\n",
      "[Epoch: 17700] train loss: 0.1938, train acc: 0.9346, val loss: 0.2019, val acc: 0.9379  (best train acc: 0.9718, best val acc: 0.9680, best train loss: 0.0890  @ epoch 17176 )\n",
      "[Epoch: 17720] train loss: 0.1270, train acc: 0.9589, val loss: 0.1630, val acc: 0.9609  (best train acc: 0.9718, best val acc: 0.9680, best train loss: 0.0890  @ epoch 17176 )\n",
      "[Epoch: 17740] train loss: 0.1039, train acc: 0.9647, val loss: 0.1421, val acc: 0.9646  (best train acc: 0.9718, best val acc: 0.9680, best train loss: 0.0890  @ epoch 17176 )\n",
      "[Epoch: 17760] train loss: 0.1048, train acc: 0.9665, val loss: 0.1718, val acc: 0.9585  (best train acc: 0.9718, best val acc: 0.9680, best train loss: 0.0890  @ epoch 17176 )\n",
      "[Epoch: 17780] train loss: 0.1185, train acc: 0.9587, val loss: 0.1642, val acc: 0.9612  (best train acc: 0.9718, best val acc: 0.9680, best train loss: 0.0890  @ epoch 17176 )\n",
      "[Epoch: 17800] train loss: 0.0961, train acc: 0.9685, val loss: 0.1670, val acc: 0.9639  (best train acc: 0.9718, best val acc: 0.9680, best train loss: 0.0890  @ epoch 17176 )\n",
      "[Epoch: 17820] train loss: 0.0999, train acc: 0.9680, val loss: 0.1541, val acc: 0.9599  (best train acc: 0.9718, best val acc: 0.9680, best train loss: 0.0890  @ epoch 17176 )\n",
      "[Epoch: 17840] train loss: 0.1092, train acc: 0.9678, val loss: 0.1668, val acc: 0.9622  (best train acc: 0.9718, best val acc: 0.9680, best train loss: 0.0861  @ epoch 17825 )\n",
      "[Epoch: 17860] train loss: 0.0987, train acc: 0.9672, val loss: 0.1668, val acc: 0.9642  (best train acc: 0.9718, best val acc: 0.9680, best train loss: 0.0861  @ epoch 17825 )\n",
      "[Epoch: 17880] train loss: 0.1025, train acc: 0.9680, val loss: 0.1538, val acc: 0.9642  (best train acc: 0.9718, best val acc: 0.9680, best train loss: 0.0861  @ epoch 17825 )\n",
      "[Epoch: 17900] train loss: 0.1016, train acc: 0.9662, val loss: 0.1621, val acc: 0.9632  (best train acc: 0.9718, best val acc: 0.9680, best train loss: 0.0861  @ epoch 17825 )\n",
      "[Epoch: 17920] train loss: 0.1037, train acc: 0.9643, val loss: 0.1555, val acc: 0.9626  (best train acc: 0.9718, best val acc: 0.9680, best train loss: 0.0861  @ epoch 17825 )\n",
      "[Epoch: 17940] train loss: 0.0985, train acc: 0.9661, val loss: 0.1567, val acc: 0.9649  (best train acc: 0.9718, best val acc: 0.9680, best train loss: 0.0861  @ epoch 17825 )\n",
      "[Epoch: 17960] train loss: 0.0997, train acc: 0.9652, val loss: 0.1564, val acc: 0.9612  (best train acc: 0.9718, best val acc: 0.9680, best train loss: 0.0861  @ epoch 17825 )\n",
      "[Epoch: 17980] train loss: 0.1109, train acc: 0.9602, val loss: 0.1949, val acc: 0.9538  (best train acc: 0.9720, best val acc: 0.9680, best train loss: 0.0861  @ epoch 17825 )\n",
      "[Epoch: 18000] train loss: 0.1829, train acc: 0.9331, val loss: 0.1659, val acc: 0.9497  (best train acc: 0.9720, best val acc: 0.9680, best train loss: 0.0861  @ epoch 17825 )\n",
      "[Epoch: 18020] train loss: 0.1126, train acc: 0.9620, val loss: 0.1550, val acc: 0.9632  (best train acc: 0.9720, best val acc: 0.9680, best train loss: 0.0861  @ epoch 17825 )\n",
      "[Epoch: 18040] train loss: 0.1040, train acc: 0.9686, val loss: 0.1539, val acc: 0.9656  (best train acc: 0.9720, best val acc: 0.9680, best train loss: 0.0861  @ epoch 17825 )\n",
      "[Epoch: 18060] train loss: 0.0987, train acc: 0.9701, val loss: 0.1473, val acc: 0.9656  (best train acc: 0.9720, best val acc: 0.9680, best train loss: 0.0861  @ epoch 17825 )\n",
      "[Epoch: 18080] train loss: 0.1023, train acc: 0.9671, val loss: 0.1551, val acc: 0.9642  (best train acc: 0.9720, best val acc: 0.9680, best train loss: 0.0861  @ epoch 17825 )\n",
      "[Epoch: 18100] train loss: 0.0975, train acc: 0.9654, val loss: 0.1674, val acc: 0.9629  (best train acc: 0.9720, best val acc: 0.9680, best train loss: 0.0861  @ epoch 17825 )\n",
      "[Epoch: 18120] train loss: 0.1498, train acc: 0.9530, val loss: 0.2136, val acc: 0.9447  (best train acc: 0.9720, best val acc: 0.9680, best train loss: 0.0861  @ epoch 17825 )\n",
      "[Epoch: 18140] train loss: 0.1210, train acc: 0.9555, val loss: 0.1609, val acc: 0.9639  (best train acc: 0.9720, best val acc: 0.9680, best train loss: 0.0861  @ epoch 17825 )\n",
      "[Epoch: 18160] train loss: 0.0995, train acc: 0.9689, val loss: 0.1601, val acc: 0.9642  (best train acc: 0.9720, best val acc: 0.9680, best train loss: 0.0861  @ epoch 17825 )\n",
      "[Epoch: 18180] train loss: 0.0965, train acc: 0.9696, val loss: 0.1597, val acc: 0.9646  (best train acc: 0.9720, best val acc: 0.9680, best train loss: 0.0861  @ epoch 17825 )\n",
      "[Epoch: 18200] train loss: 0.0995, train acc: 0.9674, val loss: 0.1626, val acc: 0.9646  (best train acc: 0.9720, best val acc: 0.9680, best train loss: 0.0861  @ epoch 17825 )\n",
      "[Epoch: 18220] train loss: 0.1048, train acc: 0.9654, val loss: 0.1598, val acc: 0.9599  (best train acc: 0.9720, best val acc: 0.9680, best train loss: 0.0861  @ epoch 17825 )\n",
      "[Epoch: 18240] train loss: 0.1544, train acc: 0.9506, val loss: 0.1971, val acc: 0.9508  (best train acc: 0.9720, best val acc: 0.9680, best train loss: 0.0861  @ epoch 17825 )\n",
      "[Epoch: 18260] train loss: 0.1218, train acc: 0.9568, val loss: 0.1475, val acc: 0.9616  (best train acc: 0.9720, best val acc: 0.9680, best train loss: 0.0861  @ epoch 17825 )\n",
      "[Epoch: 18280] train loss: 0.0963, train acc: 0.9676, val loss: 0.1630, val acc: 0.9649  (best train acc: 0.9720, best val acc: 0.9680, best train loss: 0.0861  @ epoch 17825 )\n",
      "[Epoch: 18300] train loss: 0.0938, train acc: 0.9702, val loss: 0.1538, val acc: 0.9669  (best train acc: 0.9720, best val acc: 0.9680, best train loss: 0.0861  @ epoch 17825 )\n",
      "[Epoch: 18320] train loss: 0.0945, train acc: 0.9691, val loss: 0.1484, val acc: 0.9663  (best train acc: 0.9720, best val acc: 0.9680, best train loss: 0.0861  @ epoch 17825 )\n",
      "[Epoch: 18340] train loss: 0.0960, train acc: 0.9687, val loss: 0.1588, val acc: 0.9639  (best train acc: 0.9720, best val acc: 0.9680, best train loss: 0.0861  @ epoch 17825 )\n",
      "[Epoch: 18360] train loss: 0.0965, train acc: 0.9684, val loss: 0.1590, val acc: 0.9656  (best train acc: 0.9720, best val acc: 0.9680, best train loss: 0.0861  @ epoch 17825 )\n",
      "[Epoch: 18380] train loss: 0.1002, train acc: 0.9665, val loss: 0.1493, val acc: 0.9656  (best train acc: 0.9720, best val acc: 0.9680, best train loss: 0.0861  @ epoch 17825 )\n",
      "[Epoch: 18400] train loss: 0.0935, train acc: 0.9722, val loss: 0.1570, val acc: 0.9656  (best train acc: 0.9722, best val acc: 0.9680, best train loss: 0.0859  @ epoch 18386 )\n",
      "[Epoch: 18420] train loss: 0.1018, train acc: 0.9675, val loss: 0.1629, val acc: 0.9602  (best train acc: 0.9722, best val acc: 0.9680, best train loss: 0.0859  @ epoch 18386 )\n",
      "[Epoch: 18440] train loss: 0.0966, train acc: 0.9707, val loss: 0.1678, val acc: 0.9636  (best train acc: 0.9722, best val acc: 0.9680, best train loss: 0.0859  @ epoch 18386 )\n",
      "[Epoch: 18460] train loss: 0.1030, train acc: 0.9651, val loss: 0.1983, val acc: 0.9545  (best train acc: 0.9722, best val acc: 0.9680, best train loss: 0.0859  @ epoch 18386 )\n",
      "[Epoch: 18480] train loss: 0.1244, train acc: 0.9555, val loss: 0.1497, val acc: 0.9626  (best train acc: 0.9722, best val acc: 0.9680, best train loss: 0.0859  @ epoch 18386 )\n",
      "[Epoch: 18500] train loss: 0.0970, train acc: 0.9680, val loss: 0.1602, val acc: 0.9629  (best train acc: 0.9722, best val acc: 0.9680, best train loss: 0.0859  @ epoch 18386 )\n",
      "[Epoch: 18520] train loss: 0.1022, train acc: 0.9676, val loss: 0.1551, val acc: 0.9636  (best train acc: 0.9722, best val acc: 0.9680, best train loss: 0.0859  @ epoch 18386 )\n",
      "[Epoch: 18540] train loss: 0.0936, train acc: 0.9691, val loss: 0.1599, val acc: 0.9666  (best train acc: 0.9722, best val acc: 0.9680, best train loss: 0.0859  @ epoch 18386 )\n",
      "[Epoch: 18560] train loss: 0.0961, train acc: 0.9686, val loss: 0.1575, val acc: 0.9656  (best train acc: 0.9722, best val acc: 0.9680, best train loss: 0.0857  @ epoch 18557 )\n",
      "[Epoch: 18580] train loss: 0.1400, train acc: 0.9459, val loss: 0.2291, val acc: 0.9255  (best train acc: 0.9722, best val acc: 0.9680, best train loss: 0.0857  @ epoch 18557 )\n",
      "[Epoch: 18600] train loss: 0.1107, train acc: 0.9604, val loss: 0.1576, val acc: 0.9582  (best train acc: 0.9722, best val acc: 0.9680, best train loss: 0.0857  @ epoch 18557 )\n",
      "[Epoch: 18620] train loss: 0.1066, train acc: 0.9639, val loss: 0.1605, val acc: 0.9632  (best train acc: 0.9722, best val acc: 0.9683, best train loss: 0.0857  @ epoch 18557 )\n",
      "[Epoch: 18640] train loss: 0.1156, train acc: 0.9579, val loss: 0.1570, val acc: 0.9619  (best train acc: 0.9722, best val acc: 0.9683, best train loss: 0.0857  @ epoch 18557 )\n",
      "[Epoch: 18660] train loss: 0.0931, train acc: 0.9674, val loss: 0.1559, val acc: 0.9659  (best train acc: 0.9722, best val acc: 0.9683, best train loss: 0.0857  @ epoch 18557 )\n",
      "[Epoch: 18680] train loss: 0.1013, train acc: 0.9715, val loss: 0.1571, val acc: 0.9663  (best train acc: 0.9722, best val acc: 0.9683, best train loss: 0.0857  @ epoch 18557 )\n",
      "[Epoch: 18700] train loss: 0.0884, train acc: 0.9709, val loss: 0.1585, val acc: 0.9669  (best train acc: 0.9729, best val acc: 0.9683, best train loss: 0.0849  @ epoch 18686 )\n",
      "[Epoch: 18720] train loss: 0.0971, train acc: 0.9681, val loss: 0.1545, val acc: 0.9683  (best train acc: 0.9729, best val acc: 0.9683, best train loss: 0.0821  @ epoch 18704 )\n",
      "[Epoch: 18740] train loss: 0.0991, train acc: 0.9662, val loss: 0.1453, val acc: 0.9686  (best train acc: 0.9729, best val acc: 0.9686, best train loss: 0.0821  @ epoch 18704 )\n",
      "[Epoch: 18760] train loss: 0.0986, train acc: 0.9665, val loss: 0.1516, val acc: 0.9659  (best train acc: 0.9729, best val acc: 0.9686, best train loss: 0.0821  @ epoch 18704 )\n",
      "[Epoch: 18780] train loss: 0.0962, train acc: 0.9691, val loss: 0.1550, val acc: 0.9686  (best train acc: 0.9734, best val acc: 0.9686, best train loss: 0.0821  @ epoch 18704 )\n",
      "[Epoch: 18800] train loss: 0.1048, train acc: 0.9639, val loss: 0.1512, val acc: 0.9673  (best train acc: 0.9734, best val acc: 0.9686, best train loss: 0.0821  @ epoch 18704 )\n",
      "[Epoch: 18820] train loss: 0.0904, train acc: 0.9712, val loss: 0.1549, val acc: 0.9649  (best train acc: 0.9734, best val acc: 0.9686, best train loss: 0.0821  @ epoch 18704 )\n",
      "[Epoch: 18840] train loss: 0.0965, train acc: 0.9680, val loss: 0.1610, val acc: 0.9659  (best train acc: 0.9734, best val acc: 0.9686, best train loss: 0.0821  @ epoch 18704 )\n",
      "[Epoch: 18860] train loss: 0.1002, train acc: 0.9664, val loss: 0.1739, val acc: 0.9642  (best train acc: 0.9734, best val acc: 0.9686, best train loss: 0.0821  @ epoch 18704 )\n",
      "[Epoch: 18880] train loss: 0.1048, train acc: 0.9646, val loss: 0.1519, val acc: 0.9649  (best train acc: 0.9734, best val acc: 0.9686, best train loss: 0.0821  @ epoch 18704 )\n",
      "[Epoch: 18900] train loss: 0.0879, train acc: 0.9714, val loss: 0.1525, val acc: 0.9673  (best train acc: 0.9734, best val acc: 0.9686, best train loss: 0.0821  @ epoch 18704 )\n",
      "[Epoch: 18920] train loss: 0.0948, train acc: 0.9678, val loss: 0.1511, val acc: 0.9680  (best train acc: 0.9734, best val acc: 0.9686, best train loss: 0.0821  @ epoch 18704 )\n",
      "[Epoch: 18940] train loss: 0.0925, train acc: 0.9690, val loss: 0.1547, val acc: 0.9629  (best train acc: 0.9734, best val acc: 0.9686, best train loss: 0.0821  @ epoch 18704 )\n",
      "[Epoch: 18960] train loss: 0.0913, train acc: 0.9682, val loss: 0.1510, val acc: 0.9676  (best train acc: 0.9734, best val acc: 0.9686, best train loss: 0.0821  @ epoch 18704 )\n",
      "[Epoch: 18980] train loss: 0.1106, train acc: 0.9629, val loss: 0.1579, val acc: 0.9629  (best train acc: 0.9734, best val acc: 0.9686, best train loss: 0.0821  @ epoch 18704 )\n",
      "[Epoch: 19000] train loss: 0.1102, train acc: 0.9629, val loss: 0.1556, val acc: 0.9622  (best train acc: 0.9734, best val acc: 0.9686, best train loss: 0.0821  @ epoch 18704 )\n",
      "[Epoch: 19020] train loss: 0.0925, train acc: 0.9689, val loss: 0.1498, val acc: 0.9669  (best train acc: 0.9734, best val acc: 0.9686, best train loss: 0.0821  @ epoch 18704 )\n",
      "[Epoch: 19040] train loss: 0.1000, train acc: 0.9658, val loss: 0.1886, val acc: 0.9562  (best train acc: 0.9734, best val acc: 0.9686, best train loss: 0.0821  @ epoch 18704 )\n",
      "[Epoch: 19060] train loss: 0.1775, train acc: 0.9477, val loss: 0.1823, val acc: 0.9477  (best train acc: 0.9734, best val acc: 0.9686, best train loss: 0.0821  @ epoch 18704 )\n",
      "[Epoch: 19080] train loss: 0.1144, train acc: 0.9609, val loss: 0.1462, val acc: 0.9649  (best train acc: 0.9734, best val acc: 0.9686, best train loss: 0.0821  @ epoch 18704 )\n",
      "[Epoch: 19100] train loss: 0.0967, train acc: 0.9690, val loss: 0.1538, val acc: 0.9659  (best train acc: 0.9734, best val acc: 0.9686, best train loss: 0.0821  @ epoch 18704 )\n",
      "[Epoch: 19120] train loss: 0.0888, train acc: 0.9711, val loss: 0.1537, val acc: 0.9659  (best train acc: 0.9734, best val acc: 0.9686, best train loss: 0.0821  @ epoch 18704 )\n",
      "[Epoch: 19140] train loss: 0.1033, train acc: 0.9633, val loss: 0.1625, val acc: 0.9619  (best train acc: 0.9734, best val acc: 0.9686, best train loss: 0.0821  @ epoch 18704 )\n",
      "[Epoch: 19160] train loss: 0.1072, train acc: 0.9626, val loss: 0.1549, val acc: 0.9629  (best train acc: 0.9734, best val acc: 0.9686, best train loss: 0.0821  @ epoch 18704 )\n",
      "[Epoch: 19180] train loss: 0.0972, train acc: 0.9687, val loss: 0.1511, val acc: 0.9683  (best train acc: 0.9734, best val acc: 0.9686, best train loss: 0.0821  @ epoch 18704 )\n",
      "[Epoch: 19200] train loss: 0.1027, train acc: 0.9668, val loss: 0.1452, val acc: 0.9622  (best train acc: 0.9734, best val acc: 0.9686, best train loss: 0.0821  @ epoch 18704 )\n",
      "[Epoch: 19220] train loss: 0.1069, train acc: 0.9638, val loss: 0.1548, val acc: 0.9663  (best train acc: 0.9734, best val acc: 0.9686, best train loss: 0.0821  @ epoch 18704 )\n",
      "[Epoch: 19240] train loss: 0.0934, train acc: 0.9680, val loss: 0.1675, val acc: 0.9663  (best train acc: 0.9734, best val acc: 0.9690, best train loss: 0.0821  @ epoch 18704 )\n",
      "[Epoch: 19260] train loss: 0.1113, train acc: 0.9640, val loss: 0.1883, val acc: 0.9562  (best train acc: 0.9734, best val acc: 0.9690, best train loss: 0.0821  @ epoch 18704 )\n",
      "[Epoch: 19280] train loss: 0.1782, train acc: 0.9284, val loss: 0.2832, val acc: 0.9167  (best train acc: 0.9734, best val acc: 0.9690, best train loss: 0.0821  @ epoch 18704 )\n",
      "[Epoch: 19300] train loss: 0.1251, train acc: 0.9596, val loss: 0.1489, val acc: 0.9595  (best train acc: 0.9734, best val acc: 0.9690, best train loss: 0.0821  @ epoch 18704 )\n",
      "[Epoch: 19320] train loss: 0.1039, train acc: 0.9661, val loss: 0.1380, val acc: 0.9659  (best train acc: 0.9734, best val acc: 0.9690, best train loss: 0.0821  @ epoch 18704 )\n",
      "[Epoch: 19340] train loss: 0.1114, train acc: 0.9638, val loss: 0.1412, val acc: 0.9609  (best train acc: 0.9734, best val acc: 0.9690, best train loss: 0.0821  @ epoch 18704 )\n",
      "[Epoch: 19360] train loss: 0.0944, train acc: 0.9676, val loss: 0.1417, val acc: 0.9680  (best train acc: 0.9734, best val acc: 0.9690, best train loss: 0.0821  @ epoch 18704 )\n",
      "[Epoch: 19380] train loss: 0.0941, train acc: 0.9709, val loss: 0.1367, val acc: 0.9653  (best train acc: 0.9734, best val acc: 0.9690, best train loss: 0.0821  @ epoch 18704 )\n",
      "[Epoch: 19400] train loss: 0.0911, train acc: 0.9697, val loss: 0.1430, val acc: 0.9649  (best train acc: 0.9734, best val acc: 0.9690, best train loss: 0.0821  @ epoch 18704 )\n",
      "[Epoch: 19420] train loss: 0.0942, train acc: 0.9678, val loss: 0.1459, val acc: 0.9666  (best train acc: 0.9734, best val acc: 0.9690, best train loss: 0.0821  @ epoch 18704 )\n",
      "[Epoch: 19440] train loss: 0.1007, train acc: 0.9712, val loss: 0.1466, val acc: 0.9676  (best train acc: 0.9734, best val acc: 0.9690, best train loss: 0.0821  @ epoch 18704 )\n",
      "[Epoch: 19460] train loss: 0.0909, train acc: 0.9682, val loss: 0.1561, val acc: 0.9653  (best train acc: 0.9734, best val acc: 0.9690, best train loss: 0.0802  @ epoch 19459 )\n",
      "[Epoch: 19480] train loss: 0.1993, train acc: 0.9412, val loss: 0.1756, val acc: 0.9538  (best train acc: 0.9734, best val acc: 0.9690, best train loss: 0.0802  @ epoch 19459 )\n",
      "[Epoch: 19500] train loss: 0.1187, train acc: 0.9636, val loss: 0.1718, val acc: 0.9599  (best train acc: 0.9734, best val acc: 0.9690, best train loss: 0.0802  @ epoch 19459 )\n",
      "[Epoch: 19520] train loss: 0.1066, train acc: 0.9654, val loss: 0.1705, val acc: 0.9659  (best train acc: 0.9734, best val acc: 0.9690, best train loss: 0.0802  @ epoch 19459 )\n",
      "[Epoch: 19540] train loss: 0.0879, train acc: 0.9712, val loss: 0.1556, val acc: 0.9659  (best train acc: 0.9734, best val acc: 0.9690, best train loss: 0.0802  @ epoch 19459 )\n",
      "[Epoch: 19560] train loss: 0.1044, train acc: 0.9652, val loss: 0.1501, val acc: 0.9656  (best train acc: 0.9734, best val acc: 0.9690, best train loss: 0.0802  @ epoch 19459 )\n",
      "[Epoch: 19580] train loss: 0.0933, train acc: 0.9690, val loss: 0.1436, val acc: 0.9649  (best train acc: 0.9734, best val acc: 0.9690, best train loss: 0.0802  @ epoch 19459 )\n",
      "[Epoch: 19600] train loss: 0.1163, train acc: 0.9577, val loss: 0.1379, val acc: 0.9663  (best train acc: 0.9734, best val acc: 0.9690, best train loss: 0.0802  @ epoch 19459 )\n",
      "[Epoch: 19620] train loss: 0.0906, train acc: 0.9700, val loss: 0.1553, val acc: 0.9669  (best train acc: 0.9734, best val acc: 0.9690, best train loss: 0.0802  @ epoch 19459 )\n",
      "[Epoch: 19640] train loss: 0.1139, train acc: 0.9581, val loss: 0.1428, val acc: 0.9690  (best train acc: 0.9734, best val acc: 0.9690, best train loss: 0.0802  @ epoch 19459 )\n",
      "[Epoch: 19660] train loss: 0.0999, train acc: 0.9661, val loss: 0.1599, val acc: 0.9676  (best train acc: 0.9734, best val acc: 0.9690, best train loss: 0.0802  @ epoch 19459 )\n",
      "[Epoch: 19680] train loss: 0.1001, train acc: 0.9664, val loss: 0.1565, val acc: 0.9642  (best train acc: 0.9734, best val acc: 0.9690, best train loss: 0.0802  @ epoch 19459 )\n",
      "[Epoch: 19700] train loss: 0.0948, train acc: 0.9714, val loss: 0.1516, val acc: 0.9669  (best train acc: 0.9734, best val acc: 0.9690, best train loss: 0.0802  @ epoch 19459 )\n",
      "[Epoch: 19720] train loss: 0.0931, train acc: 0.9703, val loss: 0.1552, val acc: 0.9649  (best train acc: 0.9734, best val acc: 0.9690, best train loss: 0.0802  @ epoch 19459 )\n",
      "[Epoch: 19740] train loss: 0.0944, train acc: 0.9682, val loss: 0.1485, val acc: 0.9669  (best train acc: 0.9734, best val acc: 0.9690, best train loss: 0.0802  @ epoch 19459 )\n",
      "[Epoch: 19760] train loss: 0.0995, train acc: 0.9637, val loss: 0.1513, val acc: 0.9673  (best train acc: 0.9734, best val acc: 0.9690, best train loss: 0.0802  @ epoch 19459 )\n",
      "[Epoch: 19780] train loss: 0.1047, train acc: 0.9617, val loss: 0.1537, val acc: 0.9653  (best train acc: 0.9734, best val acc: 0.9690, best train loss: 0.0802  @ epoch 19459 )\n",
      "[Epoch: 19800] train loss: 0.0925, train acc: 0.9693, val loss: 0.1534, val acc: 0.9656  (best train acc: 0.9734, best val acc: 0.9690, best train loss: 0.0802  @ epoch 19459 )\n",
      "[Epoch: 19820] train loss: 0.0856, train acc: 0.9714, val loss: 0.1516, val acc: 0.9619  (best train acc: 0.9734, best val acc: 0.9690, best train loss: 0.0802  @ epoch 19459 )\n",
      "[Epoch: 19840] train loss: 0.1056, train acc: 0.9653, val loss: 0.1387, val acc: 0.9666  (best train acc: 0.9734, best val acc: 0.9690, best train loss: 0.0802  @ epoch 19459 )\n",
      "[Epoch: 19860] train loss: 0.0881, train acc: 0.9715, val loss: 0.1370, val acc: 0.9653  (best train acc: 0.9734, best val acc: 0.9690, best train loss: 0.0802  @ epoch 19459 )\n",
      "[Epoch: 19880] train loss: 0.0844, train acc: 0.9717, val loss: 0.1427, val acc: 0.9683  (best train acc: 0.9734, best val acc: 0.9690, best train loss: 0.0802  @ epoch 19459 )\n",
      "[Epoch: 19900] train loss: 0.1085, train acc: 0.9612, val loss: 0.1602, val acc: 0.9592  (best train acc: 0.9734, best val acc: 0.9693, best train loss: 0.0802  @ epoch 19459 )\n",
      "[Epoch: 19920] train loss: 0.0924, train acc: 0.9690, val loss: 0.1334, val acc: 0.9666  (best train acc: 0.9734, best val acc: 0.9693, best train loss: 0.0802  @ epoch 19459 )\n",
      "[Epoch: 19940] train loss: 0.1200, train acc: 0.9521, val loss: 0.1530, val acc: 0.9622  (best train acc: 0.9734, best val acc: 0.9693, best train loss: 0.0802  @ epoch 19459 )\n",
      "[Epoch: 19960] train loss: 0.0877, train acc: 0.9710, val loss: 0.1488, val acc: 0.9653  (best train acc: 0.9734, best val acc: 0.9693, best train loss: 0.0802  @ epoch 19459 )\n",
      "[Epoch: 19980] train loss: 0.0849, train acc: 0.9717, val loss: 0.1421, val acc: 0.9680  (best train acc: 0.9734, best val acc: 0.9693, best train loss: 0.0802  @ epoch 19459 )\n",
      "[Epoch: 20000] train loss: 0.0892, train acc: 0.9682, val loss: 0.1413, val acc: 0.9680  (best train acc: 0.9747, best val acc: 0.9693, best train loss: 0.0802  @ epoch 19459 )\n",
      "[Epoch: 20020] train loss: 0.1180, train acc: 0.9602, val loss: 0.1545, val acc: 0.9605  (best train acc: 0.9747, best val acc: 0.9693, best train loss: 0.0787  @ epoch 20004 )\n",
      "[Epoch: 20040] train loss: 0.0930, train acc: 0.9671, val loss: 0.1436, val acc: 0.9686  (best train acc: 0.9747, best val acc: 0.9693, best train loss: 0.0787  @ epoch 20004 )\n",
      "[Epoch: 20060] train loss: 0.0848, train acc: 0.9712, val loss: 0.1459, val acc: 0.9690  (best train acc: 0.9747, best val acc: 0.9693, best train loss: 0.0787  @ epoch 20004 )\n",
      "[Epoch: 20080] train loss: 0.0849, train acc: 0.9725, val loss: 0.1506, val acc: 0.9676  (best train acc: 0.9750, best val acc: 0.9696, best train loss: 0.0775  @ epoch 20073 )\n",
      "[Epoch: 20100] train loss: 0.0868, train acc: 0.9693, val loss: 0.1707, val acc: 0.9592  (best train acc: 0.9750, best val acc: 0.9696, best train loss: 0.0775  @ epoch 20073 )\n",
      "[Epoch: 20120] train loss: 0.0837, train acc: 0.9720, val loss: 0.1510, val acc: 0.9666  (best train acc: 0.9750, best val acc: 0.9696, best train loss: 0.0775  @ epoch 20073 )\n",
      "[Epoch: 20140] train loss: 0.0846, train acc: 0.9720, val loss: 0.1488, val acc: 0.9659  (best train acc: 0.9750, best val acc: 0.9696, best train loss: 0.0775  @ epoch 20073 )\n",
      "[Epoch: 20160] train loss: 0.0919, train acc: 0.9679, val loss: 0.1327, val acc: 0.9673  (best train acc: 0.9750, best val acc: 0.9696, best train loss: 0.0775  @ epoch 20073 )\n",
      "[Epoch: 20180] train loss: 0.0935, train acc: 0.9654, val loss: 0.1580, val acc: 0.9659  (best train acc: 0.9750, best val acc: 0.9696, best train loss: 0.0775  @ epoch 20073 )\n",
      "[Epoch: 20200] train loss: 0.1034, train acc: 0.9629, val loss: 0.1555, val acc: 0.9646  (best train acc: 0.9750, best val acc: 0.9696, best train loss: 0.0775  @ epoch 20073 )\n",
      "[Epoch: 20220] train loss: 0.0904, train acc: 0.9674, val loss: 0.1415, val acc: 0.9669  (best train acc: 0.9750, best val acc: 0.9696, best train loss: 0.0775  @ epoch 20073 )\n",
      "[Epoch: 20240] train loss: 0.0929, train acc: 0.9665, val loss: 0.1368, val acc: 0.9673  (best train acc: 0.9750, best val acc: 0.9696, best train loss: 0.0775  @ epoch 20073 )\n",
      "[Epoch: 20260] train loss: 0.0852, train acc: 0.9712, val loss: 0.1382, val acc: 0.9686  (best train acc: 0.9750, best val acc: 0.9696, best train loss: 0.0775  @ epoch 20073 )\n",
      "[Epoch: 20280] train loss: 0.0930, train acc: 0.9664, val loss: 0.1496, val acc: 0.9656  (best train acc: 0.9750, best val acc: 0.9696, best train loss: 0.0775  @ epoch 20073 )\n",
      "[Epoch: 20300] train loss: 0.0775, train acc: 0.9736, val loss: 0.1443, val acc: 0.9656  (best train acc: 0.9750, best val acc: 0.9696, best train loss: 0.0759  @ epoch 20294 )\n",
      "[Epoch: 20320] train loss: 0.0819, train acc: 0.9725, val loss: 0.1572, val acc: 0.9659  (best train acc: 0.9757, best val acc: 0.9696, best train loss: 0.0759  @ epoch 20294 )\n",
      "[Epoch: 20340] train loss: 0.0959, train acc: 0.9680, val loss: 0.1908, val acc: 0.9511  (best train acc: 0.9757, best val acc: 0.9696, best train loss: 0.0759  @ epoch 20294 )\n",
      "[Epoch: 20360] train loss: 0.0957, train acc: 0.9660, val loss: 0.1390, val acc: 0.9683  (best train acc: 0.9757, best val acc: 0.9696, best train loss: 0.0759  @ epoch 20294 )\n",
      "[Epoch: 20380] train loss: 0.0941, train acc: 0.9659, val loss: 0.1486, val acc: 0.9649  (best train acc: 0.9757, best val acc: 0.9696, best train loss: 0.0759  @ epoch 20294 )\n",
      "[Epoch: 20400] train loss: 0.0930, train acc: 0.9676, val loss: 0.1455, val acc: 0.9642  (best train acc: 0.9757, best val acc: 0.9696, best train loss: 0.0759  @ epoch 20294 )\n",
      "[Epoch: 20420] train loss: 0.0821, train acc: 0.9714, val loss: 0.1503, val acc: 0.9659  (best train acc: 0.9757, best val acc: 0.9696, best train loss: 0.0759  @ epoch 20294 )\n",
      "[Epoch: 20440] train loss: 0.1088, train acc: 0.9592, val loss: 0.2303, val acc: 0.9514  (best train acc: 0.9757, best val acc: 0.9696, best train loss: 0.0759  @ epoch 20294 )\n",
      "[Epoch: 20460] train loss: 0.0973, train acc: 0.9680, val loss: 0.1448, val acc: 0.9632  (best train acc: 0.9757, best val acc: 0.9696, best train loss: 0.0759  @ epoch 20294 )\n",
      "[Epoch: 20480] train loss: 0.0860, train acc: 0.9706, val loss: 0.1522, val acc: 0.9676  (best train acc: 0.9757, best val acc: 0.9696, best train loss: 0.0759  @ epoch 20294 )\n",
      "[Epoch: 20500] train loss: 0.0976, train acc: 0.9646, val loss: 0.1538, val acc: 0.9636  (best train acc: 0.9757, best val acc: 0.9696, best train loss: 0.0759  @ epoch 20294 )\n",
      "[Epoch: 20520] train loss: 0.0991, train acc: 0.9635, val loss: 0.1487, val acc: 0.9612  (best train acc: 0.9757, best val acc: 0.9696, best train loss: 0.0759  @ epoch 20294 )\n",
      "[Epoch: 20540] train loss: 0.1194, train acc: 0.9613, val loss: 0.1539, val acc: 0.9612  (best train acc: 0.9757, best val acc: 0.9696, best train loss: 0.0759  @ epoch 20294 )\n",
      "[Epoch: 20560] train loss: 0.0976, train acc: 0.9649, val loss: 0.1685, val acc: 0.9636  (best train acc: 0.9757, best val acc: 0.9696, best train loss: 0.0759  @ epoch 20294 )\n",
      "[Epoch: 20580] train loss: 0.1086, train acc: 0.9605, val loss: 0.1382, val acc: 0.9619  (best train acc: 0.9757, best val acc: 0.9696, best train loss: 0.0759  @ epoch 20294 )\n",
      "[Epoch: 20600] train loss: 0.0849, train acc: 0.9691, val loss: 0.1667, val acc: 0.9642  (best train acc: 0.9757, best val acc: 0.9696, best train loss: 0.0759  @ epoch 20294 )\n",
      "[Epoch: 20620] train loss: 0.1016, train acc: 0.9643, val loss: 0.1685, val acc: 0.9636  (best train acc: 0.9757, best val acc: 0.9696, best train loss: 0.0759  @ epoch 20294 )\n",
      "[Epoch: 20640] train loss: 0.0930, train acc: 0.9720, val loss: 0.1543, val acc: 0.9649  (best train acc: 0.9757, best val acc: 0.9696, best train loss: 0.0759  @ epoch 20294 )\n",
      "[Epoch: 20660] train loss: 0.0919, train acc: 0.9695, val loss: 0.1394, val acc: 0.9653  (best train acc: 0.9757, best val acc: 0.9696, best train loss: 0.0759  @ epoch 20294 )\n",
      "[Epoch: 20680] train loss: 0.0821, train acc: 0.9718, val loss: 0.1468, val acc: 0.9669  (best train acc: 0.9757, best val acc: 0.9696, best train loss: 0.0759  @ epoch 20294 )\n",
      "[Epoch: 20700] train loss: 0.0775, train acc: 0.9756, val loss: 0.1391, val acc: 0.9673  (best train acc: 0.9757, best val acc: 0.9696, best train loss: 0.0759  @ epoch 20294 )\n",
      "[Epoch: 20720] train loss: 0.0884, train acc: 0.9695, val loss: 0.1594, val acc: 0.9649  (best train acc: 0.9757, best val acc: 0.9696, best train loss: 0.0725  @ epoch 20714 )\n",
      "[Epoch: 20740] train loss: 0.0931, train acc: 0.9687, val loss: 0.1534, val acc: 0.9673  (best train acc: 0.9757, best val acc: 0.9696, best train loss: 0.0725  @ epoch 20714 )\n",
      "[Epoch: 20760] train loss: 0.1139, train acc: 0.9602, val loss: 0.1836, val acc: 0.9551  (best train acc: 0.9757, best val acc: 0.9696, best train loss: 0.0725  @ epoch 20714 )\n",
      "[Epoch: 20780] train loss: 0.0937, train acc: 0.9675, val loss: 0.1598, val acc: 0.9659  (best train acc: 0.9757, best val acc: 0.9696, best train loss: 0.0725  @ epoch 20714 )\n",
      "[Epoch: 20800] train loss: 0.0916, train acc: 0.9670, val loss: 0.1671, val acc: 0.9622  (best train acc: 0.9757, best val acc: 0.9696, best train loss: 0.0725  @ epoch 20714 )\n",
      "[Epoch: 20820] train loss: 0.0924, train acc: 0.9671, val loss: 0.1568, val acc: 0.9656  (best train acc: 0.9757, best val acc: 0.9696, best train loss: 0.0725  @ epoch 20714 )\n",
      "[Epoch: 20840] train loss: 0.0865, train acc: 0.9706, val loss: 0.1488, val acc: 0.9622  (best train acc: 0.9757, best val acc: 0.9696, best train loss: 0.0725  @ epoch 20714 )\n",
      "[Epoch: 20860] train loss: 0.0829, train acc: 0.9704, val loss: 0.1612, val acc: 0.9642  (best train acc: 0.9757, best val acc: 0.9696, best train loss: 0.0725  @ epoch 20714 )\n",
      "[Epoch: 20880] train loss: 0.0886, train acc: 0.9699, val loss: 0.1518, val acc: 0.9676  (best train acc: 0.9757, best val acc: 0.9696, best train loss: 0.0725  @ epoch 20714 )\n",
      "[Epoch: 20900] train loss: 0.0895, train acc: 0.9696, val loss: 0.1511, val acc: 0.9663  (best train acc: 0.9758, best val acc: 0.9696, best train loss: 0.0725  @ epoch 20714 )\n",
      "[Epoch: 20920] train loss: 0.0857, train acc: 0.9717, val loss: 0.1723, val acc: 0.9609  (best train acc: 0.9758, best val acc: 0.9696, best train loss: 0.0725  @ epoch 20714 )\n",
      "[Epoch: 20940] train loss: 0.0800, train acc: 0.9733, val loss: 0.1504, val acc: 0.9666  (best train acc: 0.9758, best val acc: 0.9696, best train loss: 0.0725  @ epoch 20714 )\n",
      "[Epoch: 20960] train loss: 0.0771, train acc: 0.9735, val loss: 0.1484, val acc: 0.9680  (best train acc: 0.9758, best val acc: 0.9696, best train loss: 0.0725  @ epoch 20714 )\n",
      "[Epoch: 20980] train loss: 0.0860, train acc: 0.9704, val loss: 0.1510, val acc: 0.9622  (best train acc: 0.9758, best val acc: 0.9696, best train loss: 0.0725  @ epoch 20714 )\n",
      "[Epoch: 21000] train loss: 0.1193, train acc: 0.9541, val loss: 0.1772, val acc: 0.9565  (best train acc: 0.9758, best val acc: 0.9696, best train loss: 0.0725  @ epoch 20714 )\n",
      "[Epoch: 21020] train loss: 0.0871, train acc: 0.9689, val loss: 0.1472, val acc: 0.9646  (best train acc: 0.9758, best val acc: 0.9696, best train loss: 0.0725  @ epoch 20714 )\n",
      "[Epoch: 21040] train loss: 0.0941, train acc: 0.9677, val loss: 0.1454, val acc: 0.9659  (best train acc: 0.9758, best val acc: 0.9696, best train loss: 0.0725  @ epoch 20714 )\n",
      "[Epoch: 21060] train loss: 0.0848, train acc: 0.9730, val loss: 0.1425, val acc: 0.9669  (best train acc: 0.9758, best val acc: 0.9696, best train loss: 0.0725  @ epoch 20714 )\n",
      "[Epoch: 21080] train loss: 0.0934, train acc: 0.9698, val loss: 0.1530, val acc: 0.9589  (best train acc: 0.9758, best val acc: 0.9696, best train loss: 0.0725  @ epoch 20714 )\n",
      "[Epoch: 21100] train loss: 0.0933, train acc: 0.9683, val loss: 0.1484, val acc: 0.9629  (best train acc: 0.9758, best val acc: 0.9696, best train loss: 0.0725  @ epoch 20714 )\n",
      "[Epoch: 21120] train loss: 0.0918, train acc: 0.9671, val loss: 0.1689, val acc: 0.9582  (best train acc: 0.9758, best val acc: 0.9696, best train loss: 0.0725  @ epoch 20714 )\n",
      "[Epoch: 21140] train loss: 0.0851, train acc: 0.9722, val loss: 0.1458, val acc: 0.9639  (best train acc: 0.9758, best val acc: 0.9696, best train loss: 0.0725  @ epoch 20714 )\n",
      "[Epoch: 21160] train loss: 0.0832, train acc: 0.9714, val loss: 0.1525, val acc: 0.9666  (best train acc: 0.9758, best val acc: 0.9696, best train loss: 0.0725  @ epoch 20714 )\n",
      "[Epoch: 21180] train loss: 0.0848, train acc: 0.9736, val loss: 0.1467, val acc: 0.9666  (best train acc: 0.9758, best val acc: 0.9696, best train loss: 0.0725  @ epoch 20714 )\n",
      "[Epoch: 21200] train loss: 0.0881, train acc: 0.9695, val loss: 0.1465, val acc: 0.9663  (best train acc: 0.9758, best val acc: 0.9696, best train loss: 0.0725  @ epoch 20714 )\n",
      "[Epoch: 21220] train loss: 0.0940, train acc: 0.9678, val loss: 0.1471, val acc: 0.9649  (best train acc: 0.9758, best val acc: 0.9696, best train loss: 0.0725  @ epoch 20714 )\n",
      "[Epoch: 21240] train loss: 0.0916, train acc: 0.9730, val loss: 0.1478, val acc: 0.9669  (best train acc: 0.9758, best val acc: 0.9696, best train loss: 0.0725  @ epoch 20714 )\n",
      "[Epoch: 21260] train loss: 0.0956, train acc: 0.9643, val loss: 0.1769, val acc: 0.9531  (best train acc: 0.9758, best val acc: 0.9696, best train loss: 0.0725  @ epoch 20714 )\n",
      "[Epoch: 21280] train loss: 0.0824, train acc: 0.9720, val loss: 0.1655, val acc: 0.9666  (best train acc: 0.9758, best val acc: 0.9696, best train loss: 0.0725  @ epoch 20714 )\n",
      "[Epoch: 21300] train loss: 0.0845, train acc: 0.9743, val loss: 0.1575, val acc: 0.9639  (best train acc: 0.9758, best val acc: 0.9696, best train loss: 0.0725  @ epoch 20714 )\n",
      "[Epoch: 21320] train loss: 0.0836, train acc: 0.9719, val loss: 0.1619, val acc: 0.9653  (best train acc: 0.9764, best val acc: 0.9696, best train loss: 0.0719  @ epoch 21316 )\n",
      "[Epoch: 21340] train loss: 0.0871, train acc: 0.9704, val loss: 0.1641, val acc: 0.9636  (best train acc: 0.9764, best val acc: 0.9696, best train loss: 0.0719  @ epoch 21316 )\n",
      "[Epoch: 21360] train loss: 0.0826, train acc: 0.9718, val loss: 0.1652, val acc: 0.9656  (best train acc: 0.9764, best val acc: 0.9700, best train loss: 0.0719  @ epoch 21316 )\n",
      "[Epoch: 21380] train loss: 0.1066, train acc: 0.9682, val loss: 0.1665, val acc: 0.9616  (best train acc: 0.9764, best val acc: 0.9700, best train loss: 0.0719  @ epoch 21316 )\n",
      "[Epoch: 21400] train loss: 0.1335, train acc: 0.9506, val loss: 0.1518, val acc: 0.9592  (best train acc: 0.9764, best val acc: 0.9700, best train loss: 0.0719  @ epoch 21316 )\n",
      "[Epoch: 21420] train loss: 0.0938, train acc: 0.9690, val loss: 0.1496, val acc: 0.9616  (best train acc: 0.9764, best val acc: 0.9700, best train loss: 0.0719  @ epoch 21316 )\n",
      "[Epoch: 21440] train loss: 0.0875, train acc: 0.9694, val loss: 0.1404, val acc: 0.9680  (best train acc: 0.9764, best val acc: 0.9700, best train loss: 0.0719  @ epoch 21316 )\n",
      "[Epoch: 21460] train loss: 0.0781, train acc: 0.9745, val loss: 0.1548, val acc: 0.9649  (best train acc: 0.9764, best val acc: 0.9700, best train loss: 0.0719  @ epoch 21316 )\n",
      "[Epoch: 21480] train loss: 0.0797, train acc: 0.9730, val loss: 0.1516, val acc: 0.9669  (best train acc: 0.9764, best val acc: 0.9703, best train loss: 0.0719  @ epoch 21316 )\n",
      "[Epoch: 21500] train loss: 0.0965, train acc: 0.9675, val loss: 0.1413, val acc: 0.9639  (best train acc: 0.9764, best val acc: 0.9703, best train loss: 0.0719  @ epoch 21316 )\n",
      "[Epoch: 21520] train loss: 0.0856, train acc: 0.9704, val loss: 0.1420, val acc: 0.9653  (best train acc: 0.9764, best val acc: 0.9703, best train loss: 0.0719  @ epoch 21316 )\n",
      "[Epoch: 21540] train loss: 0.0800, train acc: 0.9725, val loss: 0.1517, val acc: 0.9649  (best train acc: 0.9764, best val acc: 0.9703, best train loss: 0.0719  @ epoch 21316 )\n",
      "[Epoch: 21560] train loss: 0.0804, train acc: 0.9717, val loss: 0.1631, val acc: 0.9599  (best train acc: 0.9766, best val acc: 0.9703, best train loss: 0.0719  @ epoch 21316 )\n",
      "[Epoch: 21580] train loss: 0.0881, train acc: 0.9683, val loss: 0.1583, val acc: 0.9649  (best train acc: 0.9766, best val acc: 0.9703, best train loss: 0.0719  @ epoch 21316 )\n",
      "[Epoch: 21600] train loss: 0.0780, train acc: 0.9724, val loss: 0.1544, val acc: 0.9649  (best train acc: 0.9766, best val acc: 0.9703, best train loss: 0.0719  @ epoch 21316 )\n",
      "[Epoch: 21620] train loss: 0.0863, train acc: 0.9725, val loss: 0.1607, val acc: 0.9649  (best train acc: 0.9766, best val acc: 0.9703, best train loss: 0.0719  @ epoch 21316 )\n",
      "[Epoch: 21640] train loss: 0.0761, train acc: 0.9747, val loss: 0.1531, val acc: 0.9666  (best train acc: 0.9766, best val acc: 0.9703, best train loss: 0.0719  @ epoch 21316 )\n",
      "[Epoch: 21660] train loss: 0.0802, train acc: 0.9722, val loss: 0.1473, val acc: 0.9646  (best train acc: 0.9766, best val acc: 0.9703, best train loss: 0.0719  @ epoch 21316 )\n",
      "[Epoch: 21680] train loss: 0.0826, train acc: 0.9721, val loss: 0.1493, val acc: 0.9659  (best train acc: 0.9766, best val acc: 0.9703, best train loss: 0.0719  @ epoch 21316 )\n",
      "[Epoch: 21700] train loss: 0.0905, train acc: 0.9698, val loss: 0.1523, val acc: 0.9609  (best train acc: 0.9766, best val acc: 0.9703, best train loss: 0.0719  @ epoch 21316 )\n",
      "[Epoch: 21720] train loss: 0.0849, train acc: 0.9722, val loss: 0.1548, val acc: 0.9669  (best train acc: 0.9766, best val acc: 0.9703, best train loss: 0.0719  @ epoch 21316 )\n",
      "[Epoch: 21740] train loss: 0.0844, train acc: 0.9721, val loss: 0.1521, val acc: 0.9666  (best train acc: 0.9766, best val acc: 0.9703, best train loss: 0.0719  @ epoch 21316 )\n",
      "[Epoch: 21760] train loss: 0.1165, train acc: 0.9558, val loss: 0.1576, val acc: 0.9629  (best train acc: 0.9766, best val acc: 0.9703, best train loss: 0.0719  @ epoch 21316 )\n",
      "[Epoch: 21780] train loss: 0.0925, train acc: 0.9680, val loss: 0.1522, val acc: 0.9649  (best train acc: 0.9766, best val acc: 0.9703, best train loss: 0.0719  @ epoch 21316 )\n",
      "[Epoch: 21800] train loss: 0.0866, train acc: 0.9711, val loss: 0.1561, val acc: 0.9656  (best train acc: 0.9766, best val acc: 0.9703, best train loss: 0.0719  @ epoch 21316 )\n",
      "[Epoch: 21820] train loss: 0.0815, train acc: 0.9720, val loss: 0.1468, val acc: 0.9656  (best train acc: 0.9766, best val acc: 0.9703, best train loss: 0.0719  @ epoch 21316 )\n",
      "[Epoch: 21840] train loss: 0.0849, train acc: 0.9704, val loss: 0.1492, val acc: 0.9646  (best train acc: 0.9766, best val acc: 0.9703, best train loss: 0.0719  @ epoch 21316 )\n",
      "[Epoch: 21860] train loss: 0.1435, train acc: 0.9486, val loss: 0.1532, val acc: 0.9622  (best train acc: 0.9766, best val acc: 0.9703, best train loss: 0.0719  @ epoch 21316 )\n",
      "[Epoch: 21880] train loss: 0.0842, train acc: 0.9711, val loss: 0.1580, val acc: 0.9636  (best train acc: 0.9766, best val acc: 0.9703, best train loss: 0.0719  @ epoch 21316 )\n",
      "[Epoch: 21900] train loss: 0.0853, train acc: 0.9709, val loss: 0.1574, val acc: 0.9653  (best train acc: 0.9766, best val acc: 0.9703, best train loss: 0.0719  @ epoch 21316 )\n",
      "[Epoch: 21920] train loss: 0.0847, train acc: 0.9722, val loss: 0.1601, val acc: 0.9659  (best train acc: 0.9766, best val acc: 0.9703, best train loss: 0.0719  @ epoch 21316 )\n",
      "[Epoch: 21940] train loss: 0.0962, train acc: 0.9706, val loss: 0.1606, val acc: 0.9676  (best train acc: 0.9766, best val acc: 0.9703, best train loss: 0.0719  @ epoch 21316 )\n",
      "[Epoch: 21960] train loss: 0.0800, train acc: 0.9719, val loss: 0.1576, val acc: 0.9680  (best train acc: 0.9766, best val acc: 0.9703, best train loss: 0.0719  @ epoch 21316 )\n",
      "[Epoch: 21980] train loss: 0.1054, train acc: 0.9617, val loss: 0.1851, val acc: 0.9589  (best train acc: 0.9766, best val acc: 0.9703, best train loss: 0.0719  @ epoch 21316 )\n",
      "[Epoch: 22000] train loss: 0.0786, train acc: 0.9736, val loss: 0.1438, val acc: 0.9666  (best train acc: 0.9766, best val acc: 0.9703, best train loss: 0.0719  @ epoch 21316 )\n",
      "[Epoch: 22020] train loss: 0.0847, train acc: 0.9724, val loss: 0.1578, val acc: 0.9642  (best train acc: 0.9766, best val acc: 0.9703, best train loss: 0.0719  @ epoch 21316 )\n",
      "[Epoch: 22040] train loss: 0.0757, train acc: 0.9741, val loss: 0.1679, val acc: 0.9656  (best train acc: 0.9766, best val acc: 0.9703, best train loss: 0.0719  @ epoch 21316 )\n",
      "[Epoch: 22060] train loss: 0.0947, train acc: 0.9698, val loss: 0.1656, val acc: 0.9609  (best train acc: 0.9766, best val acc: 0.9703, best train loss: 0.0719  @ epoch 21316 )\n",
      "[Epoch: 22080] train loss: 0.0914, train acc: 0.9701, val loss: 0.1734, val acc: 0.9609  (best train acc: 0.9766, best val acc: 0.9703, best train loss: 0.0719  @ epoch 21316 )\n",
      "[Epoch: 22100] train loss: 0.0941, train acc: 0.9648, val loss: 0.1474, val acc: 0.9659  (best train acc: 0.9766, best val acc: 0.9703, best train loss: 0.0719  @ epoch 21316 )\n",
      "[Epoch: 22120] train loss: 0.0792, train acc: 0.9727, val loss: 0.1783, val acc: 0.9636  (best train acc: 0.9766, best val acc: 0.9703, best train loss: 0.0719  @ epoch 21316 )\n",
      "[Epoch: 22140] train loss: 0.0750, train acc: 0.9757, val loss: 0.1562, val acc: 0.9669  (best train acc: 0.9766, best val acc: 0.9703, best train loss: 0.0719  @ epoch 21316 )\n",
      "[Epoch: 22160] train loss: 0.0825, train acc: 0.9729, val loss: 0.1545, val acc: 0.9649  (best train acc: 0.9766, best val acc: 0.9703, best train loss: 0.0719  @ epoch 21316 )\n",
      "[Epoch: 22180] train loss: 0.1023, train acc: 0.9624, val loss: 0.1648, val acc: 0.9649  (best train acc: 0.9766, best val acc: 0.9703, best train loss: 0.0719  @ epoch 21316 )\n",
      "[Epoch: 22200] train loss: 0.0967, train acc: 0.9720, val loss: 0.1566, val acc: 0.9642  (best train acc: 0.9766, best val acc: 0.9703, best train loss: 0.0719  @ epoch 21316 )\n",
      "[Epoch: 22220] train loss: 0.0969, train acc: 0.9672, val loss: 0.1729, val acc: 0.9602  (best train acc: 0.9766, best val acc: 0.9703, best train loss: 0.0719  @ epoch 21316 )\n",
      "[Epoch: 22240] train loss: 0.0765, train acc: 0.9730, val loss: 0.1545, val acc: 0.9642  (best train acc: 0.9766, best val acc: 0.9703, best train loss: 0.0719  @ epoch 21316 )\n",
      "[Epoch: 22260] train loss: 0.0912, train acc: 0.9648, val loss: 0.1579, val acc: 0.9646  (best train acc: 0.9766, best val acc: 0.9703, best train loss: 0.0719  @ epoch 21316 )\n",
      "[Epoch: 22280] train loss: 0.0822, train acc: 0.9706, val loss: 0.1673, val acc: 0.9632  (best train acc: 0.9766, best val acc: 0.9703, best train loss: 0.0719  @ epoch 21316 )\n",
      "[Epoch: 22300] train loss: 0.0911, train acc: 0.9684, val loss: 0.1558, val acc: 0.9636  (best train acc: 0.9766, best val acc: 0.9703, best train loss: 0.0719  @ epoch 21316 )\n",
      "[Epoch: 22320] train loss: 0.1001, train acc: 0.9699, val loss: 0.1790, val acc: 0.9629  (best train acc: 0.9766, best val acc: 0.9703, best train loss: 0.0719  @ epoch 21316 )\n",
      "[Epoch: 22340] train loss: 0.0844, train acc: 0.9713, val loss: 0.1634, val acc: 0.9649  (best train acc: 0.9766, best val acc: 0.9703, best train loss: 0.0719  @ epoch 21316 )\n",
      "[Epoch: 22360] train loss: 0.0963, train acc: 0.9644, val loss: 0.1806, val acc: 0.9609  (best train acc: 0.9766, best val acc: 0.9703, best train loss: 0.0719  @ epoch 21316 )\n",
      "[Epoch: 22380] train loss: 0.0975, train acc: 0.9686, val loss: 0.1440, val acc: 0.9656  (best train acc: 0.9766, best val acc: 0.9703, best train loss: 0.0719  @ epoch 21316 )\n",
      "[Epoch: 22400] train loss: 0.0868, train acc: 0.9711, val loss: 0.1463, val acc: 0.9663  (best train acc: 0.9766, best val acc: 0.9703, best train loss: 0.0719  @ epoch 21316 )\n",
      "[Epoch: 22420] train loss: 0.0969, train acc: 0.9637, val loss: 0.1749, val acc: 0.9619  (best train acc: 0.9766, best val acc: 0.9703, best train loss: 0.0719  @ epoch 21316 )\n",
      "[Epoch: 22440] train loss: 0.0770, train acc: 0.9731, val loss: 0.1602, val acc: 0.9669  (best train acc: 0.9766, best val acc: 0.9703, best train loss: 0.0719  @ epoch 21316 )\n",
      "[Epoch: 22460] train loss: 0.0857, train acc: 0.9737, val loss: 0.1558, val acc: 0.9676  (best train acc: 0.9774, best val acc: 0.9703, best train loss: 0.0694  @ epoch 22459 )\n",
      "[Epoch: 22480] train loss: 0.0784, train acc: 0.9747, val loss: 0.1557, val acc: 0.9649  (best train acc: 0.9774, best val acc: 0.9703, best train loss: 0.0694  @ epoch 22459 )\n",
      "[Epoch: 22500] train loss: 0.0819, train acc: 0.9718, val loss: 0.1579, val acc: 0.9649  (best train acc: 0.9774, best val acc: 0.9703, best train loss: 0.0694  @ epoch 22459 )\n",
      "[Epoch: 22520] train loss: 0.0791, train acc: 0.9730, val loss: 0.1521, val acc: 0.9669  (best train acc: 0.9774, best val acc: 0.9703, best train loss: 0.0694  @ epoch 22459 )\n",
      "[Epoch: 22540] train loss: 0.1898, train acc: 0.9477, val loss: 0.2857, val acc: 0.9234  (best train acc: 0.9774, best val acc: 0.9703, best train loss: 0.0694  @ epoch 22459 )\n",
      "[Epoch: 22560] train loss: 0.1187, train acc: 0.9595, val loss: 0.1790, val acc: 0.9572  (best train acc: 0.9774, best val acc: 0.9703, best train loss: 0.0694  @ epoch 22459 )\n",
      "[Epoch: 22580] train loss: 0.0910, train acc: 0.9675, val loss: 0.1620, val acc: 0.9622  (best train acc: 0.9774, best val acc: 0.9703, best train loss: 0.0694  @ epoch 22459 )\n",
      "[Epoch: 22600] train loss: 0.0860, train acc: 0.9721, val loss: 0.1463, val acc: 0.9676  (best train acc: 0.9774, best val acc: 0.9703, best train loss: 0.0694  @ epoch 22459 )\n",
      "[Epoch: 22620] train loss: 0.0831, train acc: 0.9724, val loss: 0.1333, val acc: 0.9653  (best train acc: 0.9774, best val acc: 0.9703, best train loss: 0.0694  @ epoch 22459 )\n",
      "[Epoch: 22640] train loss: 0.0805, train acc: 0.9714, val loss: 0.1543, val acc: 0.9653  (best train acc: 0.9774, best val acc: 0.9703, best train loss: 0.0694  @ epoch 22459 )\n",
      "[Epoch: 22660] train loss: 0.0830, train acc: 0.9712, val loss: 0.1660, val acc: 0.9659  (best train acc: 0.9774, best val acc: 0.9703, best train loss: 0.0694  @ epoch 22459 )\n",
      "[Epoch: 22680] train loss: 0.0857, train acc: 0.9710, val loss: 0.1505, val acc: 0.9626  (best train acc: 0.9774, best val acc: 0.9703, best train loss: 0.0694  @ epoch 22459 )\n",
      "[Epoch: 22700] train loss: 0.0896, train acc: 0.9693, val loss: 0.1452, val acc: 0.9646  (best train acc: 0.9774, best val acc: 0.9703, best train loss: 0.0694  @ epoch 22459 )\n",
      "[Epoch: 22720] train loss: 0.0832, train acc: 0.9717, val loss: 0.1690, val acc: 0.9642  (best train acc: 0.9774, best val acc: 0.9703, best train loss: 0.0694  @ epoch 22459 )\n",
      "[Epoch: 22740] train loss: 0.0887, train acc: 0.9718, val loss: 0.1623, val acc: 0.9639  (best train acc: 0.9774, best val acc: 0.9703, best train loss: 0.0694  @ epoch 22459 )\n",
      "[Epoch: 22760] train loss: 0.0868, train acc: 0.9695, val loss: 0.1592, val acc: 0.9673  (best train acc: 0.9774, best val acc: 0.9703, best train loss: 0.0694  @ epoch 22459 )\n",
      "[Epoch: 22780] train loss: 0.0778, train acc: 0.9740, val loss: 0.1457, val acc: 0.9666  (best train acc: 0.9777, best val acc: 0.9703, best train loss: 0.0694  @ epoch 22459 )\n",
      "[Epoch: 22800] train loss: 0.0852, train acc: 0.9704, val loss: 0.1641, val acc: 0.9649  (best train acc: 0.9777, best val acc: 0.9703, best train loss: 0.0694  @ epoch 22459 )\n",
      "[Epoch: 22820] train loss: 0.0808, train acc: 0.9721, val loss: 0.1655, val acc: 0.9669  (best train acc: 0.9777, best val acc: 0.9703, best train loss: 0.0694  @ epoch 22459 )\n",
      "[Epoch: 22840] train loss: 0.0974, train acc: 0.9695, val loss: 0.1359, val acc: 0.9622  (best train acc: 0.9777, best val acc: 0.9703, best train loss: 0.0694  @ epoch 22459 )\n",
      "[Epoch: 22860] train loss: 0.1104, train acc: 0.9573, val loss: 0.1970, val acc: 0.9501  (best train acc: 0.9777, best val acc: 0.9703, best train loss: 0.0694  @ epoch 22459 )\n",
      "[Epoch: 22880] train loss: 0.1034, train acc: 0.9641, val loss: 0.1595, val acc: 0.9612  (best train acc: 0.9777, best val acc: 0.9703, best train loss: 0.0694  @ epoch 22459 )\n",
      "[Epoch: 22900] train loss: 0.0986, train acc: 0.9664, val loss: 0.1666, val acc: 0.9609  (best train acc: 0.9777, best val acc: 0.9703, best train loss: 0.0694  @ epoch 22459 )\n",
      "[Epoch: 22920] train loss: 0.0750, train acc: 0.9760, val loss: 0.1737, val acc: 0.9653  (best train acc: 0.9777, best val acc: 0.9703, best train loss: 0.0694  @ epoch 22459 )\n",
      "[Epoch: 22940] train loss: 0.0756, train acc: 0.9752, val loss: 0.1566, val acc: 0.9666  (best train acc: 0.9777, best val acc: 0.9703, best train loss: 0.0694  @ epoch 22459 )\n",
      "[Epoch: 22960] train loss: 0.0752, train acc: 0.9765, val loss: 0.1582, val acc: 0.9683  (best train acc: 0.9777, best val acc: 0.9703, best train loss: 0.0694  @ epoch 22459 )\n",
      "[Epoch: 22980] train loss: 0.0803, train acc: 0.9714, val loss: 0.1491, val acc: 0.9642  (best train acc: 0.9777, best val acc: 0.9703, best train loss: 0.0694  @ epoch 22459 )\n",
      "[Epoch: 23000] train loss: 0.0889, train acc: 0.9692, val loss: 0.1526, val acc: 0.9669  (best train acc: 0.9777, best val acc: 0.9703, best train loss: 0.0694  @ epoch 22459 )\n",
      "[Epoch: 23020] train loss: 0.0808, train acc: 0.9716, val loss: 0.1485, val acc: 0.9666  (best train acc: 0.9777, best val acc: 0.9703, best train loss: 0.0694  @ epoch 22459 )\n",
      "[Epoch: 23040] train loss: 0.1127, train acc: 0.9628, val loss: 0.1717, val acc: 0.9562  (best train acc: 0.9777, best val acc: 0.9703, best train loss: 0.0694  @ epoch 22459 )\n",
      "[Epoch: 23060] train loss: 0.0770, train acc: 0.9740, val loss: 0.1398, val acc: 0.9646  (best train acc: 0.9777, best val acc: 0.9703, best train loss: 0.0694  @ epoch 22459 )\n",
      "[Epoch: 23080] train loss: 0.0723, train acc: 0.9773, val loss: 0.1616, val acc: 0.9639  (best train acc: 0.9777, best val acc: 0.9703, best train loss: 0.0694  @ epoch 22459 )\n",
      "[Epoch: 23100] train loss: 0.0734, train acc: 0.9753, val loss: 0.1525, val acc: 0.9653  (best train acc: 0.9777, best val acc: 0.9703, best train loss: 0.0694  @ epoch 22459 )\n",
      "[Epoch: 23120] train loss: 0.0792, train acc: 0.9737, val loss: 0.1761, val acc: 0.9653  (best train acc: 0.9777, best val acc: 0.9703, best train loss: 0.0694  @ epoch 22459 )\n",
      "[Epoch: 23140] train loss: 0.0760, train acc: 0.9754, val loss: 0.1618, val acc: 0.9642  (best train acc: 0.9777, best val acc: 0.9703, best train loss: 0.0694  @ epoch 22459 )\n",
      "[Epoch: 23160] train loss: 0.0766, train acc: 0.9744, val loss: 0.1623, val acc: 0.9666  (best train acc: 0.9777, best val acc: 0.9703, best train loss: 0.0694  @ epoch 22459 )\n",
      "[Epoch: 23180] train loss: 0.0737, train acc: 0.9756, val loss: 0.1450, val acc: 0.9632  (best train acc: 0.9777, best val acc: 0.9703, best train loss: 0.0694  @ epoch 22459 )\n",
      "[Epoch: 23200] train loss: 0.0779, train acc: 0.9729, val loss: 0.1560, val acc: 0.9653  (best train acc: 0.9777, best val acc: 0.9703, best train loss: 0.0694  @ epoch 22459 )\n",
      "[Epoch: 23220] train loss: 0.0785, train acc: 0.9740, val loss: 0.1616, val acc: 0.9663  (best train acc: 0.9777, best val acc: 0.9703, best train loss: 0.0694  @ epoch 22459 )\n",
      "[Epoch: 23240] train loss: 0.0926, train acc: 0.9696, val loss: 0.1397, val acc: 0.9639  (best train acc: 0.9777, best val acc: 0.9703, best train loss: 0.0694  @ epoch 22459 )\n",
      "[Epoch: 23260] train loss: 0.0789, train acc: 0.9707, val loss: 0.1540, val acc: 0.9680  (best train acc: 0.9777, best val acc: 0.9703, best train loss: 0.0694  @ epoch 22459 )\n",
      "[Epoch: 23280] train loss: 0.0945, train acc: 0.9680, val loss: 0.1659, val acc: 0.9602  (best train acc: 0.9777, best val acc: 0.9703, best train loss: 0.0694  @ epoch 22459 )\n",
      "[Epoch: 23300] train loss: 0.0765, train acc: 0.9743, val loss: 0.1635, val acc: 0.9659  (best train acc: 0.9777, best val acc: 0.9703, best train loss: 0.0694  @ epoch 22459 )\n",
      "[Epoch: 23320] train loss: 0.0797, train acc: 0.9738, val loss: 0.1536, val acc: 0.9663  (best train acc: 0.9777, best val acc: 0.9703, best train loss: 0.0694  @ epoch 22459 )\n",
      "[Epoch: 23340] train loss: 0.0750, train acc: 0.9751, val loss: 0.1531, val acc: 0.9680  (best train acc: 0.9777, best val acc: 0.9703, best train loss: 0.0694  @ epoch 22459 )\n",
      "[Epoch: 23360] train loss: 0.0772, train acc: 0.9740, val loss: 0.1491, val acc: 0.9666  (best train acc: 0.9777, best val acc: 0.9703, best train loss: 0.0694  @ epoch 22459 )\n",
      "[Epoch: 23380] train loss: 0.0798, train acc: 0.9712, val loss: 0.1609, val acc: 0.9663  (best train acc: 0.9780, best val acc: 0.9703, best train loss: 0.0693  @ epoch 23371 )\n",
      "[Epoch: 23400] train loss: 0.0718, train acc: 0.9759, val loss: 0.1674, val acc: 0.9639  (best train acc: 0.9780, best val acc: 0.9703, best train loss: 0.0693  @ epoch 23371 )\n",
      "[Epoch: 23420] train loss: 0.0813, train acc: 0.9718, val loss: 0.1515, val acc: 0.9642  (best train acc: 0.9780, best val acc: 0.9703, best train loss: 0.0693  @ epoch 23371 )\n",
      "[Epoch: 23440] train loss: 0.1691, train acc: 0.9299, val loss: 0.1414, val acc: 0.9646  (best train acc: 0.9780, best val acc: 0.9703, best train loss: 0.0693  @ epoch 23371 )\n",
      "[Epoch: 23460] train loss: 0.1316, train acc: 0.9580, val loss: 0.1439, val acc: 0.9609  (best train acc: 0.9780, best val acc: 0.9703, best train loss: 0.0693  @ epoch 23371 )\n",
      "[Epoch: 23480] train loss: 0.0951, train acc: 0.9657, val loss: 0.1468, val acc: 0.9616  (best train acc: 0.9780, best val acc: 0.9703, best train loss: 0.0693  @ epoch 23371 )\n",
      "[Epoch: 23500] train loss: 0.0893, train acc: 0.9712, val loss: 0.1400, val acc: 0.9653  (best train acc: 0.9780, best val acc: 0.9703, best train loss: 0.0693  @ epoch 23371 )\n",
      "[Epoch: 23520] train loss: 0.0769, train acc: 0.9730, val loss: 0.1512, val acc: 0.9666  (best train acc: 0.9780, best val acc: 0.9703, best train loss: 0.0693  @ epoch 23371 )\n",
      "[Epoch: 23540] train loss: 0.0848, train acc: 0.9737, val loss: 0.1490, val acc: 0.9626  (best train acc: 0.9780, best val acc: 0.9703, best train loss: 0.0693  @ epoch 23371 )\n",
      "[Epoch: 23560] train loss: 0.0850, train acc: 0.9720, val loss: 0.1527, val acc: 0.9659  (best train acc: 0.9780, best val acc: 0.9703, best train loss: 0.0693  @ epoch 23371 )\n",
      "[Epoch: 23580] train loss: 0.0773, train acc: 0.9783, val loss: 0.1488, val acc: 0.9649  (best train acc: 0.9783, best val acc: 0.9703, best train loss: 0.0693  @ epoch 23371 )\n",
      "[Epoch: 23600] train loss: 0.0837, train acc: 0.9702, val loss: 0.1412, val acc: 0.9659  (best train acc: 0.9783, best val acc: 0.9703, best train loss: 0.0693  @ epoch 23371 )\n",
      "[Epoch: 23620] train loss: 0.0825, train acc: 0.9731, val loss: 0.1552, val acc: 0.9680  (best train acc: 0.9783, best val acc: 0.9703, best train loss: 0.0693  @ epoch 23371 )\n",
      "[Epoch: 23640] train loss: 0.0876, train acc: 0.9699, val loss: 0.1423, val acc: 0.9646  (best train acc: 0.9783, best val acc: 0.9703, best train loss: 0.0693  @ epoch 23371 )\n",
      "[Epoch: 23660] train loss: 0.0763, train acc: 0.9730, val loss: 0.1431, val acc: 0.9676  (best train acc: 0.9783, best val acc: 0.9703, best train loss: 0.0693  @ epoch 23371 )\n",
      "[Epoch: 23680] train loss: 0.0776, train acc: 0.9736, val loss: 0.1433, val acc: 0.9663  (best train acc: 0.9783, best val acc: 0.9703, best train loss: 0.0693  @ epoch 23371 )\n",
      "[Epoch: 23700] train loss: 0.1037, train acc: 0.9624, val loss: 0.1340, val acc: 0.9622  (best train acc: 0.9783, best val acc: 0.9703, best train loss: 0.0693  @ epoch 23371 )\n",
      "[Epoch: 23720] train loss: 0.0797, train acc: 0.9738, val loss: 0.1582, val acc: 0.9642  (best train acc: 0.9783, best val acc: 0.9703, best train loss: 0.0693  @ epoch 23371 )\n",
      "[Epoch: 23740] train loss: 0.0695, train acc: 0.9776, val loss: 0.1566, val acc: 0.9656  (best train acc: 0.9783, best val acc: 0.9703, best train loss: 0.0693  @ epoch 23371 )\n",
      "[Epoch: 23760] train loss: 0.0757, train acc: 0.9753, val loss: 0.1507, val acc: 0.9663  (best train acc: 0.9783, best val acc: 0.9703, best train loss: 0.0693  @ epoch 23371 )\n",
      "[Epoch: 23780] train loss: 0.0995, train acc: 0.9670, val loss: 0.1474, val acc: 0.9632  (best train acc: 0.9783, best val acc: 0.9703, best train loss: 0.0693  @ epoch 23371 )\n",
      "[Epoch: 23800] train loss: 0.0892, train acc: 0.9704, val loss: 0.1399, val acc: 0.9653  (best train acc: 0.9783, best val acc: 0.9703, best train loss: 0.0693  @ epoch 23371 )\n",
      "[Epoch: 23820] train loss: 0.0849, train acc: 0.9684, val loss: 0.1793, val acc: 0.9619  (best train acc: 0.9783, best val acc: 0.9703, best train loss: 0.0693  @ epoch 23371 )\n",
      "[Epoch: 23840] train loss: 0.0906, train acc: 0.9701, val loss: 0.1586, val acc: 0.9656  (best train acc: 0.9783, best val acc: 0.9703, best train loss: 0.0693  @ epoch 23371 )\n",
      "[Epoch: 23860] train loss: 0.0993, train acc: 0.9638, val loss: 0.2635, val acc: 0.9403  (best train acc: 0.9783, best val acc: 0.9703, best train loss: 0.0693  @ epoch 23371 )\n",
      "[Epoch: 23880] train loss: 0.1043, train acc: 0.9644, val loss: 0.1861, val acc: 0.9622  (best train acc: 0.9783, best val acc: 0.9703, best train loss: 0.0693  @ epoch 23371 )\n",
      "[Epoch: 23900] train loss: 0.0833, train acc: 0.9714, val loss: 0.1475, val acc: 0.9663  (best train acc: 0.9783, best val acc: 0.9703, best train loss: 0.0693  @ epoch 23371 )\n",
      "[Epoch: 23920] train loss: 0.0926, train acc: 0.9673, val loss: 0.1501, val acc: 0.9663  (best train acc: 0.9783, best val acc: 0.9703, best train loss: 0.0693  @ epoch 23371 )\n",
      "[Epoch: 23940] train loss: 0.0867, train acc: 0.9689, val loss: 0.1471, val acc: 0.9673  (best train acc: 0.9783, best val acc: 0.9703, best train loss: 0.0693  @ epoch 23371 )\n",
      "[Epoch: 23960] train loss: 0.0803, train acc: 0.9741, val loss: 0.1512, val acc: 0.9656  (best train acc: 0.9783, best val acc: 0.9703, best train loss: 0.0693  @ epoch 23371 )\n",
      "[Epoch: 23980] train loss: 0.0830, train acc: 0.9725, val loss: 0.1526, val acc: 0.9686  (best train acc: 0.9783, best val acc: 0.9703, best train loss: 0.0693  @ epoch 23371 )\n",
      "[Epoch: 24000] train loss: 0.0790, train acc: 0.9727, val loss: 0.1440, val acc: 0.9656  (best train acc: 0.9783, best val acc: 0.9703, best train loss: 0.0693  @ epoch 23371 )\n",
      "[Epoch: 24020] train loss: 0.0723, train acc: 0.9761, val loss: 0.1461, val acc: 0.9656  (best train acc: 0.9783, best val acc: 0.9703, best train loss: 0.0693  @ epoch 23371 )\n",
      "[Epoch: 24040] train loss: 0.0726, train acc: 0.9751, val loss: 0.1471, val acc: 0.9666  (best train acc: 0.9783, best val acc: 0.9703, best train loss: 0.0693  @ epoch 23371 )\n",
      "[Epoch: 24060] train loss: 0.1328, train acc: 0.9592, val loss: 0.1928, val acc: 0.9558  (best train acc: 0.9783, best val acc: 0.9703, best train loss: 0.0693  @ epoch 23371 )\n",
      "[Epoch: 24080] train loss: 0.0830, train acc: 0.9712, val loss: 0.1719, val acc: 0.9653  (best train acc: 0.9783, best val acc: 0.9703, best train loss: 0.0693  @ epoch 23371 )\n",
      "[Epoch: 24100] train loss: 0.0786, train acc: 0.9718, val loss: 0.1689, val acc: 0.9636  (best train acc: 0.9783, best val acc: 0.9703, best train loss: 0.0693  @ epoch 23371 )\n",
      "[Epoch: 24120] train loss: 0.0745, train acc: 0.9733, val loss: 0.1544, val acc: 0.9673  (best train acc: 0.9783, best val acc: 0.9703, best train loss: 0.0693  @ epoch 23371 )\n",
      "[Epoch: 24140] train loss: 0.0747, train acc: 0.9759, val loss: 0.1529, val acc: 0.9669  (best train acc: 0.9783, best val acc: 0.9703, best train loss: 0.0693  @ epoch 23371 )\n",
      "[Epoch: 24160] train loss: 0.0765, train acc: 0.9735, val loss: 0.1528, val acc: 0.9666  (best train acc: 0.9783, best val acc: 0.9703, best train loss: 0.0693  @ epoch 23371 )\n",
      "[Epoch: 24180] train loss: 0.0725, train acc: 0.9766, val loss: 0.1433, val acc: 0.9676  (best train acc: 0.9783, best val acc: 0.9703, best train loss: 0.0693  @ epoch 23371 )\n",
      "[Epoch: 24200] train loss: 0.0942, train acc: 0.9676, val loss: 0.1695, val acc: 0.9582  (best train acc: 0.9783, best val acc: 0.9703, best train loss: 0.0693  @ epoch 23371 )\n",
      "[Epoch: 24220] train loss: 0.0727, train acc: 0.9743, val loss: 0.1604, val acc: 0.9673  (best train acc: 0.9783, best val acc: 0.9703, best train loss: 0.0693  @ epoch 23371 )\n",
      "[Epoch: 24240] train loss: 0.0778, train acc: 0.9725, val loss: 0.1455, val acc: 0.9656  (best train acc: 0.9783, best val acc: 0.9703, best train loss: 0.0693  @ epoch 23371 )\n",
      "[Epoch: 24260] train loss: 0.0751, train acc: 0.9758, val loss: 0.1501, val acc: 0.9676  (best train acc: 0.9783, best val acc: 0.9703, best train loss: 0.0693  @ epoch 23371 )\n",
      "[Epoch: 24280] train loss: 0.0709, train acc: 0.9774, val loss: 0.1505, val acc: 0.9663  (best train acc: 0.9783, best val acc: 0.9703, best train loss: 0.0673  @ epoch 24273 )\n",
      "[Epoch: 24300] train loss: 0.0821, train acc: 0.9707, val loss: 0.1508, val acc: 0.9663  (best train acc: 0.9783, best val acc: 0.9703, best train loss: 0.0673  @ epoch 24273 )\n",
      "[Epoch: 24320] train loss: 0.0729, train acc: 0.9762, val loss: 0.1682, val acc: 0.9666  (best train acc: 0.9783, best val acc: 0.9703, best train loss: 0.0673  @ epoch 24273 )\n",
      "[Epoch: 24340] train loss: 0.0726, train acc: 0.9764, val loss: 0.1521, val acc: 0.9656  (best train acc: 0.9783, best val acc: 0.9703, best train loss: 0.0673  @ epoch 24273 )\n",
      "[Epoch: 24360] train loss: 0.0690, train acc: 0.9769, val loss: 0.1612, val acc: 0.9649  (best train acc: 0.9783, best val acc: 0.9703, best train loss: 0.0673  @ epoch 24273 )\n",
      "[Epoch: 24380] train loss: 0.1026, train acc: 0.9667, val loss: 0.1428, val acc: 0.9612  (best train acc: 0.9783, best val acc: 0.9703, best train loss: 0.0673  @ epoch 24273 )\n",
      "[Epoch: 24400] train loss: 0.0820, train acc: 0.9704, val loss: 0.1581, val acc: 0.9659  (best train acc: 0.9783, best val acc: 0.9703, best train loss: 0.0673  @ epoch 24273 )\n",
      "[Epoch: 24420] train loss: 0.0797, train acc: 0.9724, val loss: 0.1452, val acc: 0.9683  (best train acc: 0.9783, best val acc: 0.9703, best train loss: 0.0673  @ epoch 24273 )\n",
      "[Epoch: 24440] train loss: 0.0991, train acc: 0.9626, val loss: 0.1531, val acc: 0.9632  (best train acc: 0.9783, best val acc: 0.9703, best train loss: 0.0673  @ epoch 24273 )\n",
      "[Epoch: 24460] train loss: 0.0846, train acc: 0.9690, val loss: 0.1637, val acc: 0.9636  (best train acc: 0.9783, best val acc: 0.9703, best train loss: 0.0673  @ epoch 24273 )\n",
      "[Epoch: 24480] train loss: 0.0795, train acc: 0.9712, val loss: 0.1528, val acc: 0.9622  (best train acc: 0.9783, best val acc: 0.9703, best train loss: 0.0673  @ epoch 24273 )\n",
      "[Epoch: 24500] train loss: 0.0778, train acc: 0.9748, val loss: 0.1376, val acc: 0.9649  (best train acc: 0.9783, best val acc: 0.9703, best train loss: 0.0673  @ epoch 24273 )\n",
      "[Epoch: 24520] train loss: 0.0812, train acc: 0.9719, val loss: 0.1721, val acc: 0.9609  (best train acc: 0.9783, best val acc: 0.9703, best train loss: 0.0673  @ epoch 24273 )\n",
      "[Epoch: 24540] train loss: 0.0783, train acc: 0.9759, val loss: 0.1521, val acc: 0.9683  (best train acc: 0.9783, best val acc: 0.9703, best train loss: 0.0673  @ epoch 24273 )\n",
      "[Epoch: 24560] train loss: 0.0719, train acc: 0.9758, val loss: 0.1457, val acc: 0.9646  (best train acc: 0.9783, best val acc: 0.9703, best train loss: 0.0673  @ epoch 24273 )\n",
      "[Epoch: 24580] train loss: 0.0759, train acc: 0.9728, val loss: 0.1580, val acc: 0.9669  (best train acc: 0.9783, best val acc: 0.9703, best train loss: 0.0673  @ epoch 24273 )\n",
      "[Epoch: 24600] train loss: 0.0781, train acc: 0.9734, val loss: 0.1578, val acc: 0.9666  (best train acc: 0.9787, best val acc: 0.9703, best train loss: 0.0663  @ epoch 24583 )\n",
      "[Epoch: 24620] train loss: 0.1034, train acc: 0.9621, val loss: 0.1418, val acc: 0.9663  (best train acc: 0.9787, best val acc: 0.9703, best train loss: 0.0663  @ epoch 24583 )\n",
      "[Epoch: 24640] train loss: 0.0792, train acc: 0.9733, val loss: 0.1647, val acc: 0.9632  (best train acc: 0.9787, best val acc: 0.9703, best train loss: 0.0663  @ epoch 24583 )\n",
      "[Epoch: 24660] train loss: 0.0740, train acc: 0.9745, val loss: 0.1573, val acc: 0.9656  (best train acc: 0.9787, best val acc: 0.9703, best train loss: 0.0663  @ epoch 24583 )\n",
      "[Epoch: 24680] train loss: 0.0761, train acc: 0.9756, val loss: 0.1537, val acc: 0.9632  (best train acc: 0.9787, best val acc: 0.9703, best train loss: 0.0663  @ epoch 24583 )\n",
      "[Epoch: 24700] train loss: 0.0886, train acc: 0.9662, val loss: 0.1701, val acc: 0.9632  (best train acc: 0.9787, best val acc: 0.9703, best train loss: 0.0663  @ epoch 24583 )\n",
      "[Epoch: 24720] train loss: 0.0822, train acc: 0.9748, val loss: 0.1597, val acc: 0.9646  (best train acc: 0.9787, best val acc: 0.9703, best train loss: 0.0663  @ epoch 24583 )\n",
      "[Epoch: 24740] train loss: 0.0811, train acc: 0.9734, val loss: 0.1441, val acc: 0.9646  (best train acc: 0.9787, best val acc: 0.9703, best train loss: 0.0663  @ epoch 24583 )\n",
      "[Epoch: 24760] train loss: 0.0781, train acc: 0.9749, val loss: 0.1593, val acc: 0.9666  (best train acc: 0.9787, best val acc: 0.9703, best train loss: 0.0663  @ epoch 24583 )\n",
      "[Epoch: 24780] train loss: 0.0741, train acc: 0.9756, val loss: 0.1513, val acc: 0.9666  (best train acc: 0.9792, best val acc: 0.9703, best train loss: 0.0645  @ epoch 24767 )\n",
      "[Epoch: 24800] train loss: 0.0748, train acc: 0.9749, val loss: 0.1753, val acc: 0.9639  (best train acc: 0.9792, best val acc: 0.9703, best train loss: 0.0642  @ epoch 24796 )\n",
      "[Epoch: 24820] train loss: 0.0767, train acc: 0.9738, val loss: 0.1613, val acc: 0.9653  (best train acc: 0.9792, best val acc: 0.9703, best train loss: 0.0642  @ epoch 24796 )\n",
      "[Epoch: 24840] train loss: 0.0697, train acc: 0.9761, val loss: 0.1574, val acc: 0.9666  (best train acc: 0.9792, best val acc: 0.9703, best train loss: 0.0642  @ epoch 24796 )\n",
      "[Epoch: 24860] train loss: 0.0730, train acc: 0.9766, val loss: 0.1902, val acc: 0.9639  (best train acc: 0.9792, best val acc: 0.9703, best train loss: 0.0642  @ epoch 24796 )\n",
      "[Epoch: 24880] train loss: 0.0947, train acc: 0.9616, val loss: 0.1592, val acc: 0.9649  (best train acc: 0.9792, best val acc: 0.9703, best train loss: 0.0642  @ epoch 24796 )\n",
      "[Epoch: 24900] train loss: 0.0771, train acc: 0.9738, val loss: 0.1518, val acc: 0.9663  (best train acc: 0.9792, best val acc: 0.9703, best train loss: 0.0642  @ epoch 24796 )\n",
      "[Epoch: 24920] train loss: 0.0726, train acc: 0.9769, val loss: 0.1455, val acc: 0.9666  (best train acc: 0.9792, best val acc: 0.9703, best train loss: 0.0642  @ epoch 24796 )\n",
      "[Epoch: 24940] train loss: 0.0757, train acc: 0.9765, val loss: 0.1585, val acc: 0.9642  (best train acc: 0.9792, best val acc: 0.9703, best train loss: 0.0642  @ epoch 24796 )\n",
      "[Epoch: 24960] train loss: 0.1334, train acc: 0.9477, val loss: 0.1393, val acc: 0.9575  (best train acc: 0.9792, best val acc: 0.9703, best train loss: 0.0642  @ epoch 24796 )\n",
      "[Epoch: 24980] train loss: 0.1188, train acc: 0.9622, val loss: 0.1438, val acc: 0.9659  (best train acc: 0.9792, best val acc: 0.9703, best train loss: 0.0642  @ epoch 24796 )\n",
      "[Epoch: 25000] train loss: 0.0769, train acc: 0.9721, val loss: 0.1565, val acc: 0.9622  (best train acc: 0.9792, best val acc: 0.9703, best train loss: 0.0642  @ epoch 24796 )\n",
      "[Epoch: 25020] train loss: 0.0745, train acc: 0.9750, val loss: 0.1484, val acc: 0.9646  (best train acc: 0.9792, best val acc: 0.9703, best train loss: 0.0642  @ epoch 24796 )\n",
      "[Epoch: 25040] train loss: 0.0793, train acc: 0.9732, val loss: 0.1671, val acc: 0.9592  (best train acc: 0.9792, best val acc: 0.9703, best train loss: 0.0642  @ epoch 24796 )\n",
      "[Epoch: 25060] train loss: 0.0788, train acc: 0.9740, val loss: 0.1298, val acc: 0.9659  (best train acc: 0.9792, best val acc: 0.9703, best train loss: 0.0642  @ epoch 24796 )\n",
      "[Epoch: 25080] train loss: 0.1028, train acc: 0.9696, val loss: 0.1615, val acc: 0.9646  (best train acc: 0.9792, best val acc: 0.9703, best train loss: 0.0642  @ epoch 24796 )\n",
      "[Epoch: 25100] train loss: 0.0777, train acc: 0.9756, val loss: 0.1464, val acc: 0.9626  (best train acc: 0.9792, best val acc: 0.9703, best train loss: 0.0642  @ epoch 24796 )\n",
      "[Epoch: 25120] train loss: 0.0682, train acc: 0.9775, val loss: 0.1548, val acc: 0.9646  (best train acc: 0.9792, best val acc: 0.9703, best train loss: 0.0642  @ epoch 24796 )\n",
      "[Epoch: 25140] train loss: 0.0716, train acc: 0.9769, val loss: 0.1405, val acc: 0.9646  (best train acc: 0.9792, best val acc: 0.9703, best train loss: 0.0642  @ epoch 24796 )\n",
      "[Epoch: 25160] train loss: 0.0876, train acc: 0.9702, val loss: 0.1517, val acc: 0.9669  (best train acc: 0.9792, best val acc: 0.9703, best train loss: 0.0642  @ epoch 24796 )\n",
      "[Epoch: 25180] train loss: 0.0771, train acc: 0.9714, val loss: 0.1578, val acc: 0.9653  (best train acc: 0.9793, best val acc: 0.9703, best train loss: 0.0640  @ epoch 25173 )\n",
      "[Epoch: 25200] train loss: 0.0728, train acc: 0.9753, val loss: 0.1496, val acc: 0.9673  (best train acc: 0.9793, best val acc: 0.9703, best train loss: 0.0640  @ epoch 25173 )\n",
      "[Epoch: 25220] train loss: 0.0737, train acc: 0.9764, val loss: 0.1635, val acc: 0.9649  (best train acc: 0.9793, best val acc: 0.9703, best train loss: 0.0640  @ epoch 25173 )\n",
      "[Epoch: 25240] train loss: 0.0703, train acc: 0.9776, val loss: 0.1571, val acc: 0.9666  (best train acc: 0.9793, best val acc: 0.9703, best train loss: 0.0640  @ epoch 25173 )\n",
      "[Epoch: 25260] train loss: 0.0693, train acc: 0.9775, val loss: 0.1684, val acc: 0.9649  (best train acc: 0.9793, best val acc: 0.9703, best train loss: 0.0640  @ epoch 25173 )\n",
      "[Epoch: 25280] train loss: 0.0751, train acc: 0.9752, val loss: 0.1446, val acc: 0.9676  (best train acc: 0.9793, best val acc: 0.9703, best train loss: 0.0640  @ epoch 25173 )\n",
      "[Epoch: 25300] train loss: 0.0759, train acc: 0.9760, val loss: 0.1572, val acc: 0.9676  (best train acc: 0.9793, best val acc: 0.9703, best train loss: 0.0640  @ epoch 25173 )\n",
      "[Epoch: 25320] train loss: 0.0729, train acc: 0.9751, val loss: 0.1663, val acc: 0.9646  (best train acc: 0.9793, best val acc: 0.9703, best train loss: 0.0640  @ epoch 25173 )\n",
      "[Epoch: 25340] train loss: 0.1114, train acc: 0.9663, val loss: 0.1665, val acc: 0.9602  (best train acc: 0.9793, best val acc: 0.9703, best train loss: 0.0640  @ epoch 25173 )\n",
      "[Epoch: 25360] train loss: 0.0765, train acc: 0.9740, val loss: 0.1675, val acc: 0.9632  (best train acc: 0.9793, best val acc: 0.9703, best train loss: 0.0640  @ epoch 25173 )\n",
      "[Epoch: 25380] train loss: 0.0839, train acc: 0.9722, val loss: 0.1544, val acc: 0.9616  (best train acc: 0.9793, best val acc: 0.9703, best train loss: 0.0640  @ epoch 25173 )\n",
      "[Epoch: 25400] train loss: 0.0795, train acc: 0.9718, val loss: 0.1590, val acc: 0.9663  (best train acc: 0.9793, best val acc: 0.9703, best train loss: 0.0640  @ epoch 25173 )\n",
      "[Epoch: 25420] train loss: 0.0794, train acc: 0.9740, val loss: 0.1514, val acc: 0.9669  (best train acc: 0.9793, best val acc: 0.9703, best train loss: 0.0640  @ epoch 25173 )\n",
      "[Epoch: 25440] train loss: 0.0793, train acc: 0.9733, val loss: 0.1555, val acc: 0.9669  (best train acc: 0.9793, best val acc: 0.9703, best train loss: 0.0640  @ epoch 25173 )\n",
      "[Epoch: 25460] train loss: 0.0716, train acc: 0.9775, val loss: 0.1523, val acc: 0.9666  (best train acc: 0.9793, best val acc: 0.9703, best train loss: 0.0640  @ epoch 25173 )\n",
      "[Epoch: 25480] train loss: 0.0865, train acc: 0.9686, val loss: 0.1606, val acc: 0.9653  (best train acc: 0.9793, best val acc: 0.9703, best train loss: 0.0640  @ epoch 25173 )\n",
      "[Epoch: 25500] train loss: 0.0838, train acc: 0.9751, val loss: 0.1576, val acc: 0.9659  (best train acc: 0.9793, best val acc: 0.9703, best train loss: 0.0638  @ epoch 25485 )\n",
      "[Epoch: 25520] train loss: 0.0677, train acc: 0.9789, val loss: 0.1662, val acc: 0.9669  (best train acc: 0.9793, best val acc: 0.9703, best train loss: 0.0638  @ epoch 25485 )\n",
      "[Epoch: 25540] train loss: 0.0768, train acc: 0.9742, val loss: 0.1450, val acc: 0.9669  (best train acc: 0.9793, best val acc: 0.9703, best train loss: 0.0638  @ epoch 25485 )\n",
      "[Epoch: 25560] train loss: 0.0785, train acc: 0.9739, val loss: 0.1425, val acc: 0.9629  (best train acc: 0.9793, best val acc: 0.9703, best train loss: 0.0638  @ epoch 25485 )\n",
      "[Epoch: 25580] train loss: 0.0714, train acc: 0.9755, val loss: 0.1719, val acc: 0.9636  (best train acc: 0.9793, best val acc: 0.9703, best train loss: 0.0638  @ epoch 25485 )\n",
      "[Epoch: 25600] train loss: 0.0675, train acc: 0.9778, val loss: 0.1445, val acc: 0.9676  (best train acc: 0.9793, best val acc: 0.9703, best train loss: 0.0638  @ epoch 25485 )\n",
      "[Epoch: 25620] train loss: 0.0699, train acc: 0.9788, val loss: 0.1536, val acc: 0.9642  (best train acc: 0.9793, best val acc: 0.9703, best train loss: 0.0636  @ epoch 25619 )\n",
      "[Epoch: 25640] train loss: 0.1036, train acc: 0.9675, val loss: 0.1512, val acc: 0.9599  (best train acc: 0.9793, best val acc: 0.9703, best train loss: 0.0636  @ epoch 25619 )\n",
      "[Epoch: 25660] train loss: 0.0805, train acc: 0.9735, val loss: 0.1598, val acc: 0.9666  (best train acc: 0.9793, best val acc: 0.9703, best train loss: 0.0636  @ epoch 25619 )\n",
      "[Epoch: 25680] train loss: 0.0790, train acc: 0.9727, val loss: 0.1535, val acc: 0.9666  (best train acc: 0.9793, best val acc: 0.9703, best train loss: 0.0636  @ epoch 25619 )\n",
      "[Epoch: 25700] train loss: 0.0708, train acc: 0.9756, val loss: 0.1804, val acc: 0.9659  (best train acc: 0.9793, best val acc: 0.9703, best train loss: 0.0636  @ epoch 25619 )\n",
      "[Epoch: 25720] train loss: 0.0712, train acc: 0.9774, val loss: 0.1547, val acc: 0.9663  (best train acc: 0.9793, best val acc: 0.9703, best train loss: 0.0636  @ epoch 25619 )\n",
      "[Epoch: 25740] train loss: 0.0666, train acc: 0.9771, val loss: 0.1716, val acc: 0.9629  (best train acc: 0.9793, best val acc: 0.9703, best train loss: 0.0636  @ epoch 25619 )\n",
      "[Epoch: 25760] train loss: 0.0790, train acc: 0.9723, val loss: 0.1652, val acc: 0.9649  (best train acc: 0.9793, best val acc: 0.9703, best train loss: 0.0636  @ epoch 25619 )\n",
      "[Epoch: 25780] train loss: 0.0715, train acc: 0.9774, val loss: 0.1609, val acc: 0.9686  (best train acc: 0.9793, best val acc: 0.9703, best train loss: 0.0636  @ epoch 25619 )\n",
      "[Epoch: 25800] train loss: 0.0686, train acc: 0.9785, val loss: 0.1619, val acc: 0.9659  (best train acc: 0.9793, best val acc: 0.9703, best train loss: 0.0636  @ epoch 25619 )\n",
      "[Epoch: 25820] train loss: 0.0736, train acc: 0.9743, val loss: 0.1775, val acc: 0.9673  (best train acc: 0.9793, best val acc: 0.9703, best train loss: 0.0636  @ epoch 25619 )\n",
      "[Epoch: 25840] train loss: 0.0674, train acc: 0.9769, val loss: 0.1596, val acc: 0.9659  (best train acc: 0.9796, best val acc: 0.9703, best train loss: 0.0636  @ epoch 25619 )\n",
      "[Epoch: 25860] train loss: 0.0658, train acc: 0.9792, val loss: 0.1627, val acc: 0.9659  (best train acc: 0.9796, best val acc: 0.9703, best train loss: 0.0632  @ epoch 25858 )\n",
      "[Epoch: 25880] train loss: 0.0660, train acc: 0.9776, val loss: 0.1630, val acc: 0.9649  (best train acc: 0.9796, best val acc: 0.9703, best train loss: 0.0630  @ epoch 25865 )\n",
      "[Epoch: 25900] train loss: 0.0754, train acc: 0.9751, val loss: 0.1635, val acc: 0.9653  (best train acc: 0.9796, best val acc: 0.9703, best train loss: 0.0630  @ epoch 25865 )\n",
      "[Epoch: 25920] train loss: 0.0668, train acc: 0.9767, val loss: 0.1756, val acc: 0.9626  (best train acc: 0.9796, best val acc: 0.9703, best train loss: 0.0630  @ epoch 25865 )\n",
      "[Epoch: 25940] train loss: 0.0987, train acc: 0.9639, val loss: 0.1685, val acc: 0.9545  (best train acc: 0.9796, best val acc: 0.9703, best train loss: 0.0630  @ epoch 25865 )\n",
      "[Epoch: 25960] train loss: 0.0876, train acc: 0.9719, val loss: 0.1497, val acc: 0.9582  (best train acc: 0.9796, best val acc: 0.9703, best train loss: 0.0630  @ epoch 25865 )\n",
      "[Epoch: 25980] train loss: 0.0741, train acc: 0.9750, val loss: 0.1471, val acc: 0.9632  (best train acc: 0.9796, best val acc: 0.9703, best train loss: 0.0630  @ epoch 25865 )\n",
      "[Epoch: 26000] train loss: 0.0727, train acc: 0.9762, val loss: 0.1612, val acc: 0.9659  (best train acc: 0.9796, best val acc: 0.9703, best train loss: 0.0630  @ epoch 25865 )\n",
      "[Epoch: 26020] train loss: 0.0738, train acc: 0.9727, val loss: 0.1488, val acc: 0.9663  (best train acc: 0.9796, best val acc: 0.9703, best train loss: 0.0630  @ epoch 25865 )\n",
      "[Epoch: 26040] train loss: 0.0810, train acc: 0.9717, val loss: 0.1705, val acc: 0.9649  (best train acc: 0.9796, best val acc: 0.9703, best train loss: 0.0630  @ epoch 25865 )\n",
      "[Epoch: 26060] train loss: 0.0752, train acc: 0.9737, val loss: 0.1525, val acc: 0.9676  (best train acc: 0.9796, best val acc: 0.9703, best train loss: 0.0630  @ epoch 25865 )\n",
      "[Epoch: 26080] train loss: 0.0991, train acc: 0.9628, val loss: 0.1587, val acc: 0.9669  (best train acc: 0.9796, best val acc: 0.9703, best train loss: 0.0630  @ epoch 25865 )\n",
      "[Epoch: 26100] train loss: 0.0695, train acc: 0.9758, val loss: 0.1835, val acc: 0.9639  (best train acc: 0.9798, best val acc: 0.9703, best train loss: 0.0630  @ epoch 25865 )\n",
      "[Epoch: 26120] train loss: 0.0670, train acc: 0.9765, val loss: 0.1629, val acc: 0.9649  (best train acc: 0.9798, best val acc: 0.9703, best train loss: 0.0630  @ epoch 25865 )\n",
      "[Epoch: 26140] train loss: 0.0731, train acc: 0.9782, val loss: 0.1756, val acc: 0.9666  (best train acc: 0.9798, best val acc: 0.9703, best train loss: 0.0630  @ epoch 25865 )\n",
      "[Epoch: 26160] train loss: 0.0690, train acc: 0.9767, val loss: 0.1580, val acc: 0.9653  (best train acc: 0.9798, best val acc: 0.9703, best train loss: 0.0630  @ epoch 25865 )\n",
      "[Epoch: 26180] train loss: 0.0665, train acc: 0.9780, val loss: 0.1646, val acc: 0.9646  (best train acc: 0.9798, best val acc: 0.9703, best train loss: 0.0630  @ epoch 26162 )\n",
      "[Epoch: 26200] train loss: 0.1158, train acc: 0.9687, val loss: 0.1567, val acc: 0.9612  (best train acc: 0.9798, best val acc: 0.9703, best train loss: 0.0630  @ epoch 26162 )\n",
      "[Epoch: 26220] train loss: 0.0780, train acc: 0.9726, val loss: 0.1634, val acc: 0.9612  (best train acc: 0.9798, best val acc: 0.9703, best train loss: 0.0630  @ epoch 26162 )\n",
      "[Epoch: 26240] train loss: 0.0863, train acc: 0.9667, val loss: 0.1570, val acc: 0.9653  (best train acc: 0.9798, best val acc: 0.9703, best train loss: 0.0630  @ epoch 26162 )\n",
      "[Epoch: 26260] train loss: 0.0792, train acc: 0.9734, val loss: 0.1588, val acc: 0.9649  (best train acc: 0.9798, best val acc: 0.9703, best train loss: 0.0630  @ epoch 26162 )\n",
      "[Epoch: 26280] train loss: 0.0685, train acc: 0.9775, val loss: 0.1609, val acc: 0.9646  (best train acc: 0.9798, best val acc: 0.9703, best train loss: 0.0630  @ epoch 26162 )\n",
      "[Epoch: 26300] train loss: 0.0750, train acc: 0.9740, val loss: 0.1655, val acc: 0.9653  (best train acc: 0.9800, best val acc: 0.9703, best train loss: 0.0630  @ epoch 26162 )\n",
      "[Epoch: 26320] train loss: 0.0845, train acc: 0.9697, val loss: 0.1723, val acc: 0.9649  (best train acc: 0.9800, best val acc: 0.9703, best train loss: 0.0630  @ epoch 26162 )\n",
      "[Epoch: 26340] train loss: 0.0982, train acc: 0.9696, val loss: 0.1425, val acc: 0.9639  (best train acc: 0.9800, best val acc: 0.9703, best train loss: 0.0630  @ epoch 26162 )\n",
      "[Epoch: 26360] train loss: 0.0694, train acc: 0.9758, val loss: 0.1575, val acc: 0.9666  (best train acc: 0.9800, best val acc: 0.9703, best train loss: 0.0630  @ epoch 26162 )\n",
      "[Epoch: 26380] train loss: 0.0900, train acc: 0.9686, val loss: 0.1479, val acc: 0.9636  (best train acc: 0.9800, best val acc: 0.9703, best train loss: 0.0630  @ epoch 26162 )\n",
      "[Epoch: 26400] train loss: 0.0805, train acc: 0.9751, val loss: 0.1619, val acc: 0.9649  (best train acc: 0.9800, best val acc: 0.9703, best train loss: 0.0630  @ epoch 26162 )\n",
      "[Epoch: 26420] train loss: 0.0722, train acc: 0.9773, val loss: 0.1672, val acc: 0.9656  (best train acc: 0.9800, best val acc: 0.9703, best train loss: 0.0630  @ epoch 26162 )\n",
      "[Epoch: 26440] train loss: 0.0748, train acc: 0.9774, val loss: 0.1637, val acc: 0.9649  (best train acc: 0.9800, best val acc: 0.9703, best train loss: 0.0630  @ epoch 26162 )\n",
      "[Epoch: 26460] train loss: 0.0749, train acc: 0.9750, val loss: 0.1727, val acc: 0.9642  (best train acc: 0.9801, best val acc: 0.9703, best train loss: 0.0630  @ epoch 26162 )\n",
      "[Epoch: 26480] train loss: 0.0754, train acc: 0.9753, val loss: 0.1562, val acc: 0.9659  (best train acc: 0.9801, best val acc: 0.9703, best train loss: 0.0630  @ epoch 26162 )\n",
      "[Epoch: 26500] train loss: 0.0871, train acc: 0.9729, val loss: 0.1835, val acc: 0.9622  (best train acc: 0.9801, best val acc: 0.9703, best train loss: 0.0630  @ epoch 26162 )\n",
      "[Epoch: 26520] train loss: 0.0938, train acc: 0.9688, val loss: 0.1589, val acc: 0.9653  (best train acc: 0.9801, best val acc: 0.9703, best train loss: 0.0630  @ epoch 26162 )\n",
      "[Epoch: 26540] train loss: 0.0722, train acc: 0.9761, val loss: 0.1629, val acc: 0.9656  (best train acc: 0.9801, best val acc: 0.9703, best train loss: 0.0630  @ epoch 26162 )\n",
      "[Epoch: 26560] train loss: 0.0857, train acc: 0.9743, val loss: 0.1565, val acc: 0.9622  (best train acc: 0.9801, best val acc: 0.9703, best train loss: 0.0630  @ epoch 26162 )\n",
      "[Epoch: 26580] train loss: 0.0773, train acc: 0.9733, val loss: 0.1667, val acc: 0.9642  (best train acc: 0.9801, best val acc: 0.9703, best train loss: 0.0630  @ epoch 26162 )\n",
      "[Epoch: 26600] train loss: 0.0666, train acc: 0.9783, val loss: 0.1628, val acc: 0.9673  (best train acc: 0.9801, best val acc: 0.9703, best train loss: 0.0630  @ epoch 26162 )\n",
      "[Epoch: 26620] train loss: 0.0687, train acc: 0.9774, val loss: 0.1590, val acc: 0.9656  (best train acc: 0.9807, best val acc: 0.9703, best train loss: 0.0612  @ epoch 26614 )\n",
      "[Epoch: 26640] train loss: 0.0737, train acc: 0.9733, val loss: 0.1647, val acc: 0.9656  (best train acc: 0.9807, best val acc: 0.9703, best train loss: 0.0612  @ epoch 26614 )\n",
      "[Epoch: 26660] train loss: 0.0701, train acc: 0.9770, val loss: 0.1598, val acc: 0.9663  (best train acc: 0.9807, best val acc: 0.9703, best train loss: 0.0612  @ epoch 26614 )\n",
      "[Epoch: 26680] train loss: 0.0716, train acc: 0.9771, val loss: 0.1627, val acc: 0.9659  (best train acc: 0.9807, best val acc: 0.9703, best train loss: 0.0612  @ epoch 26614 )\n",
      "[Epoch: 26700] train loss: 0.0734, train acc: 0.9772, val loss: 0.1658, val acc: 0.9642  (best train acc: 0.9807, best val acc: 0.9703, best train loss: 0.0612  @ epoch 26614 )\n",
      "[Epoch: 26720] train loss: 0.0743, train acc: 0.9748, val loss: 0.1784, val acc: 0.9636  (best train acc: 0.9807, best val acc: 0.9703, best train loss: 0.0612  @ epoch 26614 )\n",
      "[Epoch: 26740] train loss: 0.0664, train acc: 0.9779, val loss: 0.1520, val acc: 0.9663  (best train acc: 0.9807, best val acc: 0.9703, best train loss: 0.0612  @ epoch 26614 )\n",
      "[Epoch: 26760] train loss: 0.0876, train acc: 0.9708, val loss: 0.1475, val acc: 0.9602  (best train acc: 0.9807, best val acc: 0.9703, best train loss: 0.0612  @ epoch 26614 )\n",
      "[Epoch: 26780] train loss: 0.0775, train acc: 0.9728, val loss: 0.1596, val acc: 0.9656  (best train acc: 0.9807, best val acc: 0.9703, best train loss: 0.0612  @ epoch 26614 )\n",
      "[Epoch: 26800] train loss: 0.0718, train acc: 0.9768, val loss: 0.1491, val acc: 0.9659  (best train acc: 0.9807, best val acc: 0.9703, best train loss: 0.0612  @ epoch 26614 )\n",
      "[Epoch: 26820] train loss: 0.0719, train acc: 0.9743, val loss: 0.1509, val acc: 0.9669  (best train acc: 0.9807, best val acc: 0.9703, best train loss: 0.0612  @ epoch 26614 )\n",
      "[Epoch: 26840] train loss: 0.0738, train acc: 0.9730, val loss: 0.1630, val acc: 0.9666  (best train acc: 0.9807, best val acc: 0.9703, best train loss: 0.0612  @ epoch 26614 )\n",
      "[Epoch: 26860] train loss: 0.0747, train acc: 0.9742, val loss: 0.1682, val acc: 0.9636  (best train acc: 0.9807, best val acc: 0.9703, best train loss: 0.0612  @ epoch 26614 )\n",
      "[Epoch: 26880] train loss: 0.0756, train acc: 0.9738, val loss: 0.1777, val acc: 0.9663  (best train acc: 0.9807, best val acc: 0.9703, best train loss: 0.0612  @ epoch 26614 )\n",
      "[Epoch: 26900] train loss: 0.0691, train acc: 0.9787, val loss: 0.1814, val acc: 0.9642  (best train acc: 0.9807, best val acc: 0.9703, best train loss: 0.0612  @ epoch 26614 )\n",
      "[Epoch: 26920] train loss: 0.0855, train acc: 0.9724, val loss: 0.1806, val acc: 0.9632  (best train acc: 0.9807, best val acc: 0.9703, best train loss: 0.0612  @ epoch 26614 )\n",
      "[Epoch: 26940] train loss: 0.0998, train acc: 0.9665, val loss: 0.1616, val acc: 0.9521  (best train acc: 0.9807, best val acc: 0.9703, best train loss: 0.0612  @ epoch 26614 )\n",
      "[Epoch: 26960] train loss: 0.0814, train acc: 0.9706, val loss: 0.1513, val acc: 0.9653  (best train acc: 0.9807, best val acc: 0.9703, best train loss: 0.0612  @ epoch 26614 )\n",
      "[Epoch: 26980] train loss: 0.0663, train acc: 0.9781, val loss: 0.1568, val acc: 0.9656  (best train acc: 0.9807, best val acc: 0.9703, best train loss: 0.0612  @ epoch 26614 )\n",
      "[Epoch: 27000] train loss: 0.0701, train acc: 0.9779, val loss: 0.1604, val acc: 0.9663  (best train acc: 0.9807, best val acc: 0.9703, best train loss: 0.0612  @ epoch 26614 )\n",
      "[Epoch: 27020] train loss: 0.0762, train acc: 0.9755, val loss: 0.1472, val acc: 0.9676  (best train acc: 0.9807, best val acc: 0.9703, best train loss: 0.0612  @ epoch 26614 )\n",
      "[Epoch: 27040] train loss: 0.0794, train acc: 0.9744, val loss: 0.1604, val acc: 0.9632  (best train acc: 0.9807, best val acc: 0.9703, best train loss: 0.0612  @ epoch 26614 )\n",
      "[Epoch: 27060] train loss: 0.0744, train acc: 0.9741, val loss: 0.1507, val acc: 0.9646  (best train acc: 0.9807, best val acc: 0.9703, best train loss: 0.0612  @ epoch 26614 )\n",
      "[Epoch: 27080] train loss: 0.0764, train acc: 0.9743, val loss: 0.1617, val acc: 0.9646  (best train acc: 0.9807, best val acc: 0.9703, best train loss: 0.0612  @ epoch 26614 )\n",
      "[Epoch: 27100] train loss: 0.0661, train acc: 0.9768, val loss: 0.1658, val acc: 0.9656  (best train acc: 0.9807, best val acc: 0.9703, best train loss: 0.0612  @ epoch 26614 )\n",
      "[Epoch: 27120] train loss: 0.0675, train acc: 0.9782, val loss: 0.1623, val acc: 0.9642  (best train acc: 0.9807, best val acc: 0.9703, best train loss: 0.0612  @ epoch 26614 )\n",
      "[Epoch: 27140] train loss: 0.1211, train acc: 0.9602, val loss: 0.1438, val acc: 0.9589  (best train acc: 0.9807, best val acc: 0.9703, best train loss: 0.0612  @ epoch 26614 )\n",
      "[Epoch: 27160] train loss: 0.0770, train acc: 0.9734, val loss: 0.1427, val acc: 0.9673  (best train acc: 0.9807, best val acc: 0.9703, best train loss: 0.0612  @ epoch 26614 )\n",
      "[Epoch: 27180] train loss: 0.0750, train acc: 0.9735, val loss: 0.1605, val acc: 0.9646  (best train acc: 0.9807, best val acc: 0.9703, best train loss: 0.0612  @ epoch 26614 )\n",
      "[Epoch: 27200] train loss: 0.0716, train acc: 0.9757, val loss: 0.1590, val acc: 0.9649  (best train acc: 0.9807, best val acc: 0.9703, best train loss: 0.0612  @ epoch 26614 )\n",
      "[Epoch: 27220] train loss: 0.0824, train acc: 0.9715, val loss: 0.1806, val acc: 0.9622  (best train acc: 0.9807, best val acc: 0.9703, best train loss: 0.0612  @ epoch 26614 )\n",
      "[Epoch: 27240] train loss: 0.0722, train acc: 0.9777, val loss: 0.1432, val acc: 0.9659  (best train acc: 0.9807, best val acc: 0.9703, best train loss: 0.0612  @ epoch 26614 )\n",
      "[Epoch: 27260] train loss: 0.1135, train acc: 0.9622, val loss: 0.1544, val acc: 0.9599  (best train acc: 0.9807, best val acc: 0.9703, best train loss: 0.0612  @ epoch 26614 )\n",
      "[Epoch: 27280] train loss: 0.0788, train acc: 0.9729, val loss: 0.1412, val acc: 0.9653  (best train acc: 0.9807, best val acc: 0.9703, best train loss: 0.0612  @ epoch 26614 )\n",
      "[Epoch: 27300] train loss: 0.0890, train acc: 0.9709, val loss: 0.1641, val acc: 0.9616  (best train acc: 0.9807, best val acc: 0.9703, best train loss: 0.0612  @ epoch 26614 )\n",
      "[Epoch: 27320] train loss: 0.0966, train acc: 0.9646, val loss: 0.1611, val acc: 0.9666  (best train acc: 0.9807, best val acc: 0.9703, best train loss: 0.0612  @ epoch 26614 )\n",
      "[Epoch: 27340] train loss: 0.0666, train acc: 0.9777, val loss: 0.1481, val acc: 0.9653  (best train acc: 0.9807, best val acc: 0.9703, best train loss: 0.0612  @ epoch 26614 )\n",
      "[Epoch: 27360] train loss: 0.0663, train acc: 0.9795, val loss: 0.1620, val acc: 0.9663  (best train acc: 0.9807, best val acc: 0.9703, best train loss: 0.0612  @ epoch 26614 )\n",
      "[Epoch: 27380] train loss: 0.0694, train acc: 0.9766, val loss: 0.1822, val acc: 0.9639  (best train acc: 0.9807, best val acc: 0.9703, best train loss: 0.0612  @ epoch 26614 )\n",
      "[Epoch: 27400] train loss: 0.0903, train acc: 0.9702, val loss: 0.1966, val acc: 0.9612  (best train acc: 0.9807, best val acc: 0.9703, best train loss: 0.0612  @ epoch 26614 )\n",
      "[Epoch: 27420] train loss: 0.0671, train acc: 0.9791, val loss: 0.1735, val acc: 0.9646  (best train acc: 0.9807, best val acc: 0.9703, best train loss: 0.0612  @ epoch 26614 )\n",
      "[Epoch: 27440] train loss: 0.0673, train acc: 0.9793, val loss: 0.1786, val acc: 0.9653  (best train acc: 0.9807, best val acc: 0.9703, best train loss: 0.0612  @ epoch 26614 )\n",
      "[Epoch: 27460] train loss: 0.0975, train acc: 0.9695, val loss: 0.1847, val acc: 0.9565  (best train acc: 0.9807, best val acc: 0.9703, best train loss: 0.0612  @ epoch 26614 )\n",
      "[Epoch: 27480] train loss: 0.0995, train acc: 0.9649, val loss: 0.1743, val acc: 0.9626  (best train acc: 0.9807, best val acc: 0.9703, best train loss: 0.0612  @ epoch 26614 )\n",
      "[Epoch: 27500] train loss: 0.0802, train acc: 0.9748, val loss: 0.1435, val acc: 0.9663  (best train acc: 0.9807, best val acc: 0.9703, best train loss: 0.0612  @ epoch 26614 )\n",
      "[Epoch: 27520] train loss: 0.0758, train acc: 0.9744, val loss: 0.1699, val acc: 0.9636  (best train acc: 0.9807, best val acc: 0.9703, best train loss: 0.0612  @ epoch 26614 )\n",
      "[Epoch: 27540] train loss: 0.0689, train acc: 0.9771, val loss: 0.1588, val acc: 0.9659  (best train acc: 0.9807, best val acc: 0.9703, best train loss: 0.0612  @ epoch 26614 )\n",
      "[Epoch: 27560] train loss: 0.0860, train acc: 0.9774, val loss: 0.1632, val acc: 0.9649  (best train acc: 0.9807, best val acc: 0.9703, best train loss: 0.0612  @ epoch 26614 )\n",
      "[Epoch: 27580] train loss: 0.0637, train acc: 0.9785, val loss: 0.1700, val acc: 0.9666  (best train acc: 0.9807, best val acc: 0.9703, best train loss: 0.0612  @ epoch 26614 )\n",
      "[Epoch: 27600] train loss: 0.0663, train acc: 0.9786, val loss: 0.1866, val acc: 0.9649  (best train acc: 0.9807, best val acc: 0.9703, best train loss: 0.0612  @ epoch 26614 )\n",
      "[Epoch: 27620] train loss: 0.0638, train acc: 0.9777, val loss: 0.1627, val acc: 0.9666  (best train acc: 0.9807, best val acc: 0.9703, best train loss: 0.0612  @ epoch 26614 )\n",
      "[Epoch: 27640] train loss: 0.0651, train acc: 0.9785, val loss: 0.1657, val acc: 0.9639  (best train acc: 0.9807, best val acc: 0.9703, best train loss: 0.0612  @ epoch 26614 )\n",
      "[Epoch: 27660] train loss: 0.0665, train acc: 0.9769, val loss: 0.1707, val acc: 0.9656  (best train acc: 0.9807, best val acc: 0.9703, best train loss: 0.0602  @ epoch 27647 )\n",
      "[Epoch: 27680] train loss: 0.0697, train acc: 0.9766, val loss: 0.1679, val acc: 0.9649  (best train acc: 0.9807, best val acc: 0.9703, best train loss: 0.0602  @ epoch 27647 )\n",
      "[Epoch: 27700] train loss: 0.0715, train acc: 0.9754, val loss: 0.1702, val acc: 0.9646  (best train acc: 0.9807, best val acc: 0.9703, best train loss: 0.0602  @ epoch 27647 )\n",
      "[Epoch: 27720] train loss: 0.0760, train acc: 0.9748, val loss: 0.1550, val acc: 0.9646  (best train acc: 0.9807, best val acc: 0.9703, best train loss: 0.0602  @ epoch 27647 )\n",
      "[Epoch: 27740] train loss: 0.0889, train acc: 0.9704, val loss: 0.1444, val acc: 0.9659  (best train acc: 0.9807, best val acc: 0.9703, best train loss: 0.0602  @ epoch 27647 )\n",
      "[Epoch: 27760] train loss: 0.0725, train acc: 0.9739, val loss: 0.1597, val acc: 0.9642  (best train acc: 0.9807, best val acc: 0.9703, best train loss: 0.0602  @ epoch 27647 )\n",
      "[Epoch: 27780] train loss: 0.1167, train acc: 0.9568, val loss: 0.1617, val acc: 0.9653  (best train acc: 0.9807, best val acc: 0.9703, best train loss: 0.0602  @ epoch 27647 )\n",
      "[Epoch: 27800] train loss: 0.0713, train acc: 0.9771, val loss: 0.1948, val acc: 0.9619  (best train acc: 0.9807, best val acc: 0.9703, best train loss: 0.0602  @ epoch 27647 )\n",
      "[Epoch: 27820] train loss: 0.0817, train acc: 0.9712, val loss: 0.1363, val acc: 0.9649  (best train acc: 0.9807, best val acc: 0.9703, best train loss: 0.0602  @ epoch 27647 )\n",
      "[Epoch: 27840] train loss: 0.0766, train acc: 0.9735, val loss: 0.1709, val acc: 0.9673  (best train acc: 0.9807, best val acc: 0.9703, best train loss: 0.0602  @ epoch 27647 )\n",
      "[Epoch: 27860] train loss: 0.0731, train acc: 0.9754, val loss: 0.1600, val acc: 0.9642  (best train acc: 0.9807, best val acc: 0.9703, best train loss: 0.0602  @ epoch 27647 )\n",
      "[Epoch: 27880] train loss: 0.0749, train acc: 0.9779, val loss: 0.1726, val acc: 0.9680  (best train acc: 0.9807, best val acc: 0.9703, best train loss: 0.0602  @ epoch 27647 )\n",
      "[Epoch: 27900] train loss: 0.0833, train acc: 0.9729, val loss: 0.1650, val acc: 0.9646  (best train acc: 0.9807, best val acc: 0.9703, best train loss: 0.0602  @ epoch 27647 )\n",
      "[Epoch: 27920] train loss: 0.0650, train acc: 0.9784, val loss: 0.1573, val acc: 0.9659  (best train acc: 0.9807, best val acc: 0.9703, best train loss: 0.0602  @ epoch 27647 )\n",
      "[Epoch: 27940] train loss: 0.0839, train acc: 0.9717, val loss: 0.1659, val acc: 0.9649  (best train acc: 0.9807, best val acc: 0.9703, best train loss: 0.0602  @ epoch 27647 )\n",
      "[Epoch: 27960] train loss: 0.0685, train acc: 0.9761, val loss: 0.1782, val acc: 0.9649  (best train acc: 0.9807, best val acc: 0.9703, best train loss: 0.0602  @ epoch 27647 )\n",
      "[Epoch: 27980] train loss: 0.0978, train acc: 0.9669, val loss: 0.1747, val acc: 0.9599  (best train acc: 0.9807, best val acc: 0.9703, best train loss: 0.0602  @ epoch 27647 )\n",
      "[Epoch: 28000] train loss: 0.0950, train acc: 0.9704, val loss: 0.1627, val acc: 0.9646  (best train acc: 0.9807, best val acc: 0.9703, best train loss: 0.0602  @ epoch 27647 )\n",
      "[Epoch: 28020] train loss: 0.0889, train acc: 0.9715, val loss: 0.1493, val acc: 0.9666  (best train acc: 0.9807, best val acc: 0.9703, best train loss: 0.0602  @ epoch 27647 )\n",
      "[Epoch: 28040] train loss: 0.0779, train acc: 0.9721, val loss: 0.1649, val acc: 0.9659  (best train acc: 0.9807, best val acc: 0.9703, best train loss: 0.0602  @ epoch 27647 )\n",
      "[Epoch: 28060] train loss: 0.0831, train acc: 0.9751, val loss: 0.1680, val acc: 0.9639  (best train acc: 0.9807, best val acc: 0.9703, best train loss: 0.0602  @ epoch 27647 )\n",
      "[Epoch: 28080] train loss: 0.0656, train acc: 0.9790, val loss: 0.1447, val acc: 0.9666  (best train acc: 0.9807, best val acc: 0.9703, best train loss: 0.0602  @ epoch 27647 )\n",
      "[Epoch: 28100] train loss: 0.0664, train acc: 0.9779, val loss: 0.1594, val acc: 0.9669  (best train acc: 0.9807, best val acc: 0.9703, best train loss: 0.0602  @ epoch 27647 )\n",
      "[Epoch: 28120] train loss: 0.0675, train acc: 0.9788, val loss: 0.1606, val acc: 0.9669  (best train acc: 0.9807, best val acc: 0.9703, best train loss: 0.0602  @ epoch 27647 )\n",
      "[Epoch: 28140] train loss: 0.0720, train acc: 0.9762, val loss: 0.1667, val acc: 0.9676  (best train acc: 0.9809, best val acc: 0.9703, best train loss: 0.0602  @ epoch 27647 )\n",
      "[Epoch: 28160] train loss: 0.0814, train acc: 0.9717, val loss: 0.1739, val acc: 0.9636  (best train acc: 0.9809, best val acc: 0.9703, best train loss: 0.0598  @ epoch 28152 )\n",
      "[Epoch: 28180] train loss: 0.0708, train acc: 0.9763, val loss: 0.1605, val acc: 0.9666  (best train acc: 0.9809, best val acc: 0.9703, best train loss: 0.0598  @ epoch 28152 )\n",
      "[Epoch: 28200] train loss: 0.0703, train acc: 0.9756, val loss: 0.1609, val acc: 0.9653  (best train acc: 0.9809, best val acc: 0.9703, best train loss: 0.0598  @ epoch 28152 )\n",
      "[Epoch: 28220] train loss: 0.0670, train acc: 0.9793, val loss: 0.1620, val acc: 0.9680  (best train acc: 0.9809, best val acc: 0.9703, best train loss: 0.0598  @ epoch 28152 )\n",
      "[Epoch: 28240] train loss: 0.0652, train acc: 0.9782, val loss: 0.1641, val acc: 0.9666  (best train acc: 0.9809, best val acc: 0.9703, best train loss: 0.0598  @ epoch 28152 )\n",
      "[Epoch: 28260] train loss: 0.0682, train acc: 0.9779, val loss: 0.1645, val acc: 0.9680  (best train acc: 0.9809, best val acc: 0.9703, best train loss: 0.0598  @ epoch 28152 )\n",
      "[Epoch: 28280] train loss: 0.0677, train acc: 0.9779, val loss: 0.1841, val acc: 0.9663  (best train acc: 0.9809, best val acc: 0.9703, best train loss: 0.0598  @ epoch 28152 )\n",
      "[Epoch: 28300] train loss: 0.0634, train acc: 0.9787, val loss: 0.1713, val acc: 0.9673  (best train acc: 0.9809, best val acc: 0.9703, best train loss: 0.0598  @ epoch 28152 )\n",
      "[Epoch: 28320] train loss: 0.0718, train acc: 0.9753, val loss: 0.1696, val acc: 0.9629  (best train acc: 0.9809, best val acc: 0.9703, best train loss: 0.0598  @ epoch 28152 )\n",
      "[Epoch: 28340] train loss: 0.0664, train acc: 0.9777, val loss: 0.1605, val acc: 0.9649  (best train acc: 0.9809, best val acc: 0.9703, best train loss: 0.0598  @ epoch 28152 )\n",
      "[Epoch: 28360] train loss: 0.0795, train acc: 0.9727, val loss: 0.1678, val acc: 0.9599  (best train acc: 0.9809, best val acc: 0.9703, best train loss: 0.0598  @ epoch 28152 )\n",
      "[Epoch: 28380] train loss: 0.0685, train acc: 0.9779, val loss: 0.1709, val acc: 0.9629  (best train acc: 0.9809, best val acc: 0.9703, best train loss: 0.0598  @ epoch 28152 )\n",
      "[Epoch: 28400] train loss: 0.0712, train acc: 0.9759, val loss: 0.1555, val acc: 0.9639  (best train acc: 0.9809, best val acc: 0.9703, best train loss: 0.0598  @ epoch 28152 )\n",
      "[Epoch: 28420] train loss: 0.0745, train acc: 0.9747, val loss: 0.1446, val acc: 0.9659  (best train acc: 0.9809, best val acc: 0.9703, best train loss: 0.0598  @ epoch 28152 )\n",
      "[Epoch: 28440] train loss: 0.0886, train acc: 0.9697, val loss: 0.1474, val acc: 0.9619  (best train acc: 0.9809, best val acc: 0.9703, best train loss: 0.0598  @ epoch 28152 )\n",
      "[Epoch: 28460] train loss: 0.0730, train acc: 0.9748, val loss: 0.1507, val acc: 0.9653  (best train acc: 0.9809, best val acc: 0.9703, best train loss: 0.0598  @ epoch 28152 )\n",
      "[Epoch: 28480] train loss: 0.0876, train acc: 0.9712, val loss: 0.1978, val acc: 0.9545  (best train acc: 0.9809, best val acc: 0.9703, best train loss: 0.0598  @ epoch 28152 )\n",
      "[Epoch: 28500] train loss: 0.0733, train acc: 0.9751, val loss: 0.1694, val acc: 0.9642  (best train acc: 0.9809, best val acc: 0.9703, best train loss: 0.0598  @ epoch 28152 )\n",
      "[Epoch: 28520] train loss: 0.0809, train acc: 0.9744, val loss: 0.1618, val acc: 0.9680  (best train acc: 0.9809, best val acc: 0.9703, best train loss: 0.0598  @ epoch 28152 )\n",
      "[Epoch: 28540] train loss: 0.0807, train acc: 0.9741, val loss: 0.1700, val acc: 0.9639  (best train acc: 0.9809, best val acc: 0.9703, best train loss: 0.0598  @ epoch 28152 )\n",
      "[Epoch: 28560] train loss: 0.0641, train acc: 0.9791, val loss: 0.1598, val acc: 0.9669  (best train acc: 0.9809, best val acc: 0.9703, best train loss: 0.0598  @ epoch 28152 )\n",
      "[Epoch: 28580] train loss: 0.0817, train acc: 0.9697, val loss: 0.1442, val acc: 0.9673  (best train acc: 0.9809, best val acc: 0.9703, best train loss: 0.0598  @ epoch 28152 )\n",
      "[Epoch: 28600] train loss: 0.0703, train acc: 0.9764, val loss: 0.1619, val acc: 0.9646  (best train acc: 0.9809, best val acc: 0.9703, best train loss: 0.0598  @ epoch 28152 )\n",
      "[Epoch: 28620] train loss: 0.0669, train acc: 0.9778, val loss: 0.1586, val acc: 0.9642  (best train acc: 0.9809, best val acc: 0.9703, best train loss: 0.0598  @ epoch 28152 )\n",
      "[Epoch: 28640] train loss: 0.0717, train acc: 0.9798, val loss: 0.1571, val acc: 0.9669  (best train acc: 0.9809, best val acc: 0.9703, best train loss: 0.0598  @ epoch 28152 )\n",
      "[Epoch: 28660] train loss: 0.0786, train acc: 0.9731, val loss: 0.1695, val acc: 0.9666  (best train acc: 0.9820, best val acc: 0.9703, best train loss: 0.0598  @ epoch 28152 )\n",
      "[Epoch: 28680] train loss: 0.0739, train acc: 0.9758, val loss: 0.1650, val acc: 0.9649  (best train acc: 0.9820, best val acc: 0.9703, best train loss: 0.0598  @ epoch 28152 )\n",
      "[Epoch: 28700] train loss: 0.0865, train acc: 0.9665, val loss: 0.1613, val acc: 0.9653  (best train acc: 0.9820, best val acc: 0.9703, best train loss: 0.0598  @ epoch 28152 )\n",
      "[Epoch: 28720] train loss: 0.0669, train acc: 0.9782, val loss: 0.1590, val acc: 0.9656  (best train acc: 0.9820, best val acc: 0.9703, best train loss: 0.0598  @ epoch 28152 )\n",
      "[Epoch: 28740] train loss: 0.0685, train acc: 0.9774, val loss: 0.1617, val acc: 0.9676  (best train acc: 0.9820, best val acc: 0.9703, best train loss: 0.0598  @ epoch 28152 )\n",
      "[Epoch: 28760] train loss: 0.0897, train acc: 0.9702, val loss: 0.1624, val acc: 0.9612  (best train acc: 0.9820, best val acc: 0.9703, best train loss: 0.0598  @ epoch 28152 )\n",
      "[Epoch: 28780] train loss: 0.0852, train acc: 0.9738, val loss: 0.1845, val acc: 0.9622  (best train acc: 0.9820, best val acc: 0.9703, best train loss: 0.0598  @ epoch 28152 )\n",
      "[Epoch: 28800] train loss: 0.0772, train acc: 0.9733, val loss: 0.1592, val acc: 0.9680  (best train acc: 0.9820, best val acc: 0.9703, best train loss: 0.0598  @ epoch 28152 )\n",
      "[Epoch: 28820] train loss: 0.0684, train acc: 0.9769, val loss: 0.1588, val acc: 0.9669  (best train acc: 0.9820, best val acc: 0.9703, best train loss: 0.0598  @ epoch 28152 )\n",
      "[Epoch: 28840] train loss: 0.0827, train acc: 0.9730, val loss: 0.1440, val acc: 0.9632  (best train acc: 0.9820, best val acc: 0.9703, best train loss: 0.0598  @ epoch 28152 )\n",
      "[Epoch: 28860] train loss: 0.0687, train acc: 0.9760, val loss: 0.1717, val acc: 0.9646  (best train acc: 0.9820, best val acc: 0.9703, best train loss: 0.0598  @ epoch 28152 )\n",
      "[Epoch: 28880] train loss: 0.0740, train acc: 0.9754, val loss: 0.1821, val acc: 0.9653  (best train acc: 0.9820, best val acc: 0.9703, best train loss: 0.0598  @ epoch 28152 )\n",
      "[Epoch: 28900] train loss: 0.0689, train acc: 0.9751, val loss: 0.1804, val acc: 0.9653  (best train acc: 0.9820, best val acc: 0.9703, best train loss: 0.0598  @ epoch 28152 )\n",
      "[Epoch: 28920] train loss: 0.0717, train acc: 0.9746, val loss: 0.1680, val acc: 0.9642  (best train acc: 0.9820, best val acc: 0.9703, best train loss: 0.0597  @ epoch 28914 )\n",
      "[Epoch: 28940] train loss: 0.0818, train acc: 0.9707, val loss: 0.1719, val acc: 0.9659  (best train acc: 0.9820, best val acc: 0.9703, best train loss: 0.0597  @ epoch 28914 )\n",
      "[Epoch: 28960] train loss: 0.0785, train acc: 0.9722, val loss: 0.1690, val acc: 0.9646  (best train acc: 0.9820, best val acc: 0.9703, best train loss: 0.0597  @ epoch 28914 )\n",
      "[Epoch: 28980] train loss: 0.0681, train acc: 0.9768, val loss: 0.1602, val acc: 0.9676  (best train acc: 0.9820, best val acc: 0.9703, best train loss: 0.0597  @ epoch 28914 )\n",
      "[Epoch: 29000] train loss: 0.0757, train acc: 0.9751, val loss: 0.1935, val acc: 0.9636  (best train acc: 0.9820, best val acc: 0.9703, best train loss: 0.0597  @ epoch 28914 )\n",
      "[Epoch: 29020] train loss: 0.0765, train acc: 0.9753, val loss: 0.1938, val acc: 0.9595  (best train acc: 0.9820, best val acc: 0.9703, best train loss: 0.0597  @ epoch 28914 )\n",
      "[Epoch: 29040] train loss: 0.0900, train acc: 0.9685, val loss: 0.1749, val acc: 0.9629  (best train acc: 0.9820, best val acc: 0.9703, best train loss: 0.0597  @ epoch 28914 )\n",
      "[Epoch: 29060] train loss: 0.0694, train acc: 0.9754, val loss: 0.1721, val acc: 0.9632  (best train acc: 0.9820, best val acc: 0.9703, best train loss: 0.0597  @ epoch 28914 )\n",
      "[Epoch: 29080] train loss: 0.0677, train acc: 0.9748, val loss: 0.1794, val acc: 0.9663  (best train acc: 0.9820, best val acc: 0.9703, best train loss: 0.0589  @ epoch 29068 )\n",
      "[Epoch: 29100] train loss: 0.0613, train acc: 0.9785, val loss: 0.1843, val acc: 0.9646  (best train acc: 0.9820, best val acc: 0.9703, best train loss: 0.0589  @ epoch 29068 )\n",
      "[Epoch: 29120] train loss: 0.0680, train acc: 0.9779, val loss: 0.1694, val acc: 0.9663  (best train acc: 0.9820, best val acc: 0.9703, best train loss: 0.0582  @ epoch 29115 )\n",
      "[Epoch: 29140] train loss: 0.0873, train acc: 0.9794, val loss: 0.1809, val acc: 0.9663  (best train acc: 0.9820, best val acc: 0.9703, best train loss: 0.0582  @ epoch 29115 )\n",
      "[Epoch: 29160] train loss: 0.0721, train acc: 0.9750, val loss: 0.1564, val acc: 0.9656  (best train acc: 0.9820, best val acc: 0.9703, best train loss: 0.0582  @ epoch 29115 )\n",
      "[Epoch: 29180] train loss: 0.0640, train acc: 0.9787, val loss: 0.1662, val acc: 0.9653  (best train acc: 0.9820, best val acc: 0.9703, best train loss: 0.0582  @ epoch 29115 )\n",
      "[Epoch: 29200] train loss: 0.0901, train acc: 0.9676, val loss: 0.2093, val acc: 0.9578  (best train acc: 0.9820, best val acc: 0.9703, best train loss: 0.0582  @ epoch 29115 )\n",
      "[Epoch: 29220] train loss: 0.0866, train acc: 0.9718, val loss: 0.1859, val acc: 0.9649  (best train acc: 0.9820, best val acc: 0.9703, best train loss: 0.0582  @ epoch 29115 )\n",
      "[Epoch: 29240] train loss: 0.0715, train acc: 0.9764, val loss: 0.1705, val acc: 0.9649  (best train acc: 0.9820, best val acc: 0.9703, best train loss: 0.0582  @ epoch 29115 )\n",
      "[Epoch: 29260] train loss: 0.0766, train acc: 0.9730, val loss: 0.1638, val acc: 0.9659  (best train acc: 0.9820, best val acc: 0.9703, best train loss: 0.0582  @ epoch 29115 )\n",
      "[Epoch: 29280] train loss: 0.0684, train acc: 0.9761, val loss: 0.1610, val acc: 0.9659  (best train acc: 0.9820, best val acc: 0.9703, best train loss: 0.0582  @ epoch 29115 )\n",
      "[Epoch: 29300] train loss: 0.0740, train acc: 0.9756, val loss: 0.1760, val acc: 0.9653  (best train acc: 0.9820, best val acc: 0.9703, best train loss: 0.0582  @ epoch 29115 )\n",
      "[Epoch: 29320] train loss: 0.0630, train acc: 0.9801, val loss: 0.1740, val acc: 0.9669  (best train acc: 0.9820, best val acc: 0.9703, best train loss: 0.0582  @ epoch 29115 )\n",
      "[Epoch: 29340] train loss: 0.0652, train acc: 0.9784, val loss: 0.1776, val acc: 0.9673  (best train acc: 0.9820, best val acc: 0.9703, best train loss: 0.0582  @ epoch 29115 )\n",
      "[Epoch: 29360] train loss: 0.0650, train acc: 0.9782, val loss: 0.1780, val acc: 0.9642  (best train acc: 0.9820, best val acc: 0.9703, best train loss: 0.0582  @ epoch 29115 )\n",
      "[Epoch: 29380] train loss: 0.0654, train acc: 0.9785, val loss: 0.1648, val acc: 0.9676  (best train acc: 0.9820, best val acc: 0.9703, best train loss: 0.0582  @ epoch 29115 )\n",
      "[Epoch: 29400] train loss: 0.0646, train acc: 0.9786, val loss: 0.1678, val acc: 0.9673  (best train acc: 0.9820, best val acc: 0.9703, best train loss: 0.0582  @ epoch 29115 )\n",
      "[Epoch: 29420] train loss: 0.0635, train acc: 0.9793, val loss: 0.1691, val acc: 0.9683  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0577  @ epoch 29408 )\n",
      "[Epoch: 29440] train loss: 0.0983, train acc: 0.9680, val loss: 0.2585, val acc: 0.9487  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0577  @ epoch 29408 )\n",
      "[Epoch: 29460] train loss: 0.0899, train acc: 0.9614, val loss: 0.1765, val acc: 0.9616  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0577  @ epoch 29408 )\n",
      "[Epoch: 29480] train loss: 0.0728, train acc: 0.9753, val loss: 0.1657, val acc: 0.9659  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0577  @ epoch 29408 )\n",
      "[Epoch: 29500] train loss: 0.0762, train acc: 0.9739, val loss: 0.1834, val acc: 0.9653  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0577  @ epoch 29408 )\n",
      "[Epoch: 29520] train loss: 0.0726, train acc: 0.9777, val loss: 0.1334, val acc: 0.9676  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0577  @ epoch 29408 )\n",
      "[Epoch: 29540] train loss: 0.0743, train acc: 0.9751, val loss: 0.1674, val acc: 0.9669  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0577  @ epoch 29408 )\n",
      "[Epoch: 29560] train loss: 0.0673, train acc: 0.9775, val loss: 0.1551, val acc: 0.9673  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0577  @ epoch 29408 )\n",
      "[Epoch: 29580] train loss: 0.0911, train acc: 0.9720, val loss: 0.1708, val acc: 0.9595  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0577  @ epoch 29408 )\n",
      "[Epoch: 29600] train loss: 0.0624, train acc: 0.9790, val loss: 0.1691, val acc: 0.9680  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0577  @ epoch 29408 )\n",
      "[Epoch: 29620] train loss: 0.0738, train acc: 0.9725, val loss: 0.1750, val acc: 0.9649  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0577  @ epoch 29408 )\n",
      "[Epoch: 29640] train loss: 0.0658, train acc: 0.9790, val loss: 0.1748, val acc: 0.9686  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0577  @ epoch 29408 )\n",
      "[Epoch: 29660] train loss: 0.0617, train acc: 0.9801, val loss: 0.1773, val acc: 0.9632  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0577  @ epoch 29408 )\n",
      "[Epoch: 29680] train loss: 0.0701, train acc: 0.9783, val loss: 0.1749, val acc: 0.9653  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0577  @ epoch 29408 )\n",
      "[Epoch: 29700] train loss: 0.0587, train acc: 0.9812, val loss: 0.1764, val acc: 0.9673  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0577  @ epoch 29408 )\n",
      "[Epoch: 29720] train loss: 0.0742, train acc: 0.9753, val loss: 0.1534, val acc: 0.9659  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0577  @ epoch 29408 )\n",
      "[Epoch: 29740] train loss: 0.0627, train acc: 0.9802, val loss: 0.1677, val acc: 0.9663  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0577  @ epoch 29408 )\n",
      "[Epoch: 29760] train loss: 0.0613, train acc: 0.9790, val loss: 0.1588, val acc: 0.9683  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0577  @ epoch 29408 )\n",
      "[Epoch: 29780] train loss: 0.1330, train acc: 0.9575, val loss: 0.1854, val acc: 0.9599  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0577  @ epoch 29408 )\n",
      "[Epoch: 29800] train loss: 0.0826, train acc: 0.9728, val loss: 0.1554, val acc: 0.9666  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0577  @ epoch 29408 )\n",
      "[Epoch: 29820] train loss: 0.0699, train acc: 0.9767, val loss: 0.1671, val acc: 0.9669  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0577  @ epoch 29408 )\n",
      "[Epoch: 29840] train loss: 0.1073, train acc: 0.9669, val loss: 0.2537, val acc: 0.9514  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0577  @ epoch 29408 )\n",
      "[Epoch: 29860] train loss: 0.0809, train acc: 0.9726, val loss: 0.1710, val acc: 0.9656  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0577  @ epoch 29408 )\n",
      "[Epoch: 29880] train loss: 0.0691, train acc: 0.9761, val loss: 0.1608, val acc: 0.9666  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0577  @ epoch 29408 )\n",
      "[Epoch: 29900] train loss: 0.0758, train acc: 0.9738, val loss: 0.1488, val acc: 0.9632  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0577  @ epoch 29408 )\n",
      "[Epoch: 29920] train loss: 0.0698, train acc: 0.9751, val loss: 0.1783, val acc: 0.9646  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0577  @ epoch 29408 )\n",
      "[Epoch: 29940] train loss: 0.0760, train acc: 0.9787, val loss: 0.1741, val acc: 0.9680  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0577  @ epoch 29408 )\n",
      "[Epoch: 29960] train loss: 0.0627, train acc: 0.9793, val loss: 0.1704, val acc: 0.9663  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0577  @ epoch 29408 )\n",
      "[Epoch: 29980] train loss: 0.0630, train acc: 0.9793, val loss: 0.1554, val acc: 0.9666  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0577  @ epoch 29408 )\n",
      "[Epoch: 30000] train loss: 0.0650, train acc: 0.9772, val loss: 0.1807, val acc: 0.9656  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0577  @ epoch 29408 )\n",
      "[Epoch: 30020] train loss: 0.0708, train acc: 0.9748, val loss: 0.1706, val acc: 0.9676  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0577  @ epoch 29408 )\n",
      "[Epoch: 30040] train loss: 0.0588, train acc: 0.9810, val loss: 0.1549, val acc: 0.9659  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0577  @ epoch 29408 )\n",
      "[Epoch: 30060] train loss: 0.0725, train acc: 0.9753, val loss: 0.1672, val acc: 0.9646  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0577  @ epoch 29408 )\n",
      "[Epoch: 30080] train loss: 0.0800, train acc: 0.9732, val loss: 0.1597, val acc: 0.9666  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0577  @ epoch 29408 )\n",
      "[Epoch: 30100] train loss: 0.0621, train acc: 0.9800, val loss: 0.1691, val acc: 0.9673  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0577  @ epoch 29408 )\n",
      "[Epoch: 30120] train loss: 0.0696, train acc: 0.9761, val loss: 0.1587, val acc: 0.9642  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0577  @ epoch 29408 )\n",
      "[Epoch: 30140] train loss: 0.0632, train acc: 0.9782, val loss: 0.1616, val acc: 0.9599  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0577  @ epoch 29408 )\n",
      "[Epoch: 30160] train loss: 0.1083, train acc: 0.9661, val loss: 0.1879, val acc: 0.9477  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0577  @ epoch 29408 )\n",
      "[Epoch: 30180] train loss: 0.1122, train acc: 0.9641, val loss: 0.1738, val acc: 0.9656  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0577  @ epoch 29408 )\n",
      "[Epoch: 30200] train loss: 0.0660, train acc: 0.9795, val loss: 0.1519, val acc: 0.9690  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0577  @ epoch 29408 )\n",
      "[Epoch: 30220] train loss: 0.0653, train acc: 0.9777, val loss: 0.1550, val acc: 0.9690  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0577  @ epoch 29408 )\n",
      "[Epoch: 30240] train loss: 0.0723, train acc: 0.9742, val loss: 0.1577, val acc: 0.9676  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0577  @ epoch 29408 )\n",
      "[Epoch: 30260] train loss: 0.0751, train acc: 0.9743, val loss: 0.1523, val acc: 0.9676  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0577  @ epoch 29408 )\n",
      "[Epoch: 30280] train loss: 0.0779, train acc: 0.9730, val loss: 0.1603, val acc: 0.9656  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0577  @ epoch 29408 )\n",
      "[Epoch: 30300] train loss: 0.0679, train acc: 0.9767, val loss: 0.1630, val acc: 0.9686  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0577  @ epoch 29408 )\n",
      "[Epoch: 30320] train loss: 0.0639, train acc: 0.9785, val loss: 0.1522, val acc: 0.9676  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0577  @ epoch 29408 )\n",
      "[Epoch: 30340] train loss: 0.0604, train acc: 0.9807, val loss: 0.1708, val acc: 0.9659  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0577  @ epoch 29408 )\n",
      "[Epoch: 30360] train loss: 0.0622, train acc: 0.9801, val loss: 0.1625, val acc: 0.9663  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0577  @ epoch 29408 )\n",
      "[Epoch: 30380] train loss: 0.0718, train acc: 0.9759, val loss: 0.1671, val acc: 0.9656  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0577  @ epoch 29408 )\n",
      "[Epoch: 30400] train loss: 0.0638, train acc: 0.9786, val loss: 0.1744, val acc: 0.9676  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0577  @ epoch 29408 )\n",
      "[Epoch: 30420] train loss: 0.0668, train acc: 0.9783, val loss: 0.1722, val acc: 0.9669  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0577  @ epoch 30413 )\n",
      "[Epoch: 30440] train loss: 0.0749, train acc: 0.9728, val loss: 0.1531, val acc: 0.9673  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0577  @ epoch 30413 )\n",
      "[Epoch: 30460] train loss: 0.0808, train acc: 0.9712, val loss: 0.1896, val acc: 0.9646  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0577  @ epoch 30413 )\n",
      "[Epoch: 30480] train loss: 0.0676, train acc: 0.9764, val loss: 0.1571, val acc: 0.9686  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0577  @ epoch 30413 )\n",
      "[Epoch: 30500] train loss: 0.0803, train acc: 0.9708, val loss: 0.1867, val acc: 0.9653  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0577  @ epoch 30413 )\n",
      "[Epoch: 30520] train loss: 0.0729, train acc: 0.9757, val loss: 0.1731, val acc: 0.9619  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0577  @ epoch 30413 )\n",
      "[Epoch: 30540] train loss: 0.0802, train acc: 0.9712, val loss: 0.1765, val acc: 0.9656  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0577  @ epoch 30413 )\n",
      "[Epoch: 30560] train loss: 0.0744, train acc: 0.9750, val loss: 0.1647, val acc: 0.9669  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0577  @ epoch 30413 )\n",
      "[Epoch: 30580] train loss: 0.0706, train acc: 0.9755, val loss: 0.1772, val acc: 0.9649  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0577  @ epoch 30413 )\n",
      "[Epoch: 30600] train loss: 0.0648, train acc: 0.9781, val loss: 0.1635, val acc: 0.9656  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0577  @ epoch 30413 )\n",
      "[Epoch: 30620] train loss: 0.0712, train acc: 0.9761, val loss: 0.1728, val acc: 0.9632  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0577  @ epoch 30413 )\n",
      "[Epoch: 30640] train loss: 0.0721, train acc: 0.9748, val loss: 0.1661, val acc: 0.9649  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0577  @ epoch 30413 )\n",
      "[Epoch: 30660] train loss: 0.0847, train acc: 0.9657, val loss: 0.2314, val acc: 0.9457  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0577  @ epoch 30413 )\n",
      "[Epoch: 30680] train loss: 0.0754, train acc: 0.9727, val loss: 0.1766, val acc: 0.9639  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0577  @ epoch 30413 )\n",
      "[Epoch: 30700] train loss: 0.0645, train acc: 0.9797, val loss: 0.1699, val acc: 0.9676  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0577  @ epoch 30413 )\n",
      "[Epoch: 30720] train loss: 0.0644, train acc: 0.9795, val loss: 0.1789, val acc: 0.9686  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0577  @ epoch 30413 )\n",
      "[Epoch: 30740] train loss: 0.0638, train acc: 0.9777, val loss: 0.1719, val acc: 0.9676  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0576  @ epoch 30738 )\n",
      "[Epoch: 30760] train loss: 0.0653, train acc: 0.9817, val loss: 0.1769, val acc: 0.9656  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0576  @ epoch 30738 )\n",
      "[Epoch: 30780] train loss: 0.0693, train acc: 0.9772, val loss: 0.1850, val acc: 0.9666  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0576  @ epoch 30738 )\n",
      "[Epoch: 30800] train loss: 0.0633, train acc: 0.9779, val loss: 0.1813, val acc: 0.9666  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0576  @ epoch 30738 )\n",
      "[Epoch: 30820] train loss: 0.0686, train acc: 0.9774, val loss: 0.1679, val acc: 0.9659  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0576  @ epoch 30738 )\n",
      "[Epoch: 30840] train loss: 0.0591, train acc: 0.9803, val loss: 0.1847, val acc: 0.9656  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0576  @ epoch 30738 )\n",
      "[Epoch: 30860] train loss: 0.0692, train acc: 0.9772, val loss: 0.1748, val acc: 0.9649  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0576  @ epoch 30738 )\n",
      "[Epoch: 30880] train loss: 0.0666, train acc: 0.9780, val loss: 0.1719, val acc: 0.9663  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0576  @ epoch 30738 )\n",
      "[Epoch: 30900] train loss: 0.0735, train acc: 0.9755, val loss: 0.1910, val acc: 0.9565  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0576  @ epoch 30738 )\n",
      "[Epoch: 30920] train loss: 0.0866, train acc: 0.9723, val loss: 0.1721, val acc: 0.9673  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0576  @ epoch 30738 )\n",
      "[Epoch: 30940] train loss: 0.0695, train acc: 0.9757, val loss: 0.1860, val acc: 0.9669  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0576  @ epoch 30738 )\n",
      "[Epoch: 30960] train loss: 0.0612, train acc: 0.9803, val loss: 0.1673, val acc: 0.9666  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0576  @ epoch 30738 )\n",
      "[Epoch: 30980] train loss: 0.0645, train acc: 0.9758, val loss: 0.1657, val acc: 0.9656  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0576  @ epoch 30738 )\n",
      "[Epoch: 31000] train loss: 0.0726, train acc: 0.9788, val loss: 0.1911, val acc: 0.9629  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0576  @ epoch 30738 )\n",
      "[Epoch: 31020] train loss: 0.0814, train acc: 0.9732, val loss: 0.1912, val acc: 0.9649  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0576  @ epoch 30738 )\n",
      "[Epoch: 31040] train loss: 0.0754, train acc: 0.9766, val loss: 0.1533, val acc: 0.9629  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0576  @ epoch 30738 )\n",
      "[Epoch: 31060] train loss: 0.0644, train acc: 0.9785, val loss: 0.1709, val acc: 0.9680  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0576  @ epoch 30738 )\n",
      "[Epoch: 31080] train loss: 0.0671, train acc: 0.9777, val loss: 0.1692, val acc: 0.9663  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0576  @ epoch 30738 )\n",
      "[Epoch: 31100] train loss: 0.0784, train acc: 0.9744, val loss: 0.2019, val acc: 0.9622  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0576  @ epoch 30738 )\n",
      "[Epoch: 31120] train loss: 0.0655, train acc: 0.9771, val loss: 0.1816, val acc: 0.9683  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0576  @ epoch 30738 )\n",
      "[Epoch: 31140] train loss: 0.0635, train acc: 0.9791, val loss: 0.1773, val acc: 0.9632  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0576  @ epoch 30738 )\n",
      "[Epoch: 31160] train loss: 0.0665, train acc: 0.9761, val loss: 0.1743, val acc: 0.9676  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0576  @ epoch 30738 )\n",
      "[Epoch: 31180] train loss: 0.0654, train acc: 0.9770, val loss: 0.1721, val acc: 0.9659  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0576  @ epoch 30738 )\n",
      "[Epoch: 31200] train loss: 0.0762, train acc: 0.9730, val loss: 0.1643, val acc: 0.9639  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0576  @ epoch 30738 )\n",
      "[Epoch: 31220] train loss: 0.0730, train acc: 0.9765, val loss: 0.1575, val acc: 0.9666  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0576  @ epoch 30738 )\n",
      "[Epoch: 31240] train loss: 0.0661, train acc: 0.9787, val loss: 0.1836, val acc: 0.9663  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0576  @ epoch 30738 )\n",
      "[Epoch: 31260] train loss: 0.0774, train acc: 0.9728, val loss: 0.1737, val acc: 0.9629  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0576  @ epoch 30738 )\n",
      "[Epoch: 31280] train loss: 0.0941, train acc: 0.9680, val loss: 0.1709, val acc: 0.9656  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0576  @ epoch 30738 )\n",
      "[Epoch: 31300] train loss: 0.0782, train acc: 0.9733, val loss: 0.1929, val acc: 0.9656  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0576  @ epoch 30738 )\n",
      "[Epoch: 31320] train loss: 0.0737, train acc: 0.9751, val loss: 0.1701, val acc: 0.9666  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0576  @ epoch 30738 )\n",
      "[Epoch: 31340] train loss: 0.0693, train acc: 0.9772, val loss: 0.1977, val acc: 0.9649  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0576  @ epoch 30738 )\n",
      "[Epoch: 31360] train loss: 0.0623, train acc: 0.9799, val loss: 0.1977, val acc: 0.9666  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0576  @ epoch 30738 )\n",
      "[Epoch: 31380] train loss: 0.0638, train acc: 0.9764, val loss: 0.1723, val acc: 0.9693  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0576  @ epoch 30738 )\n",
      "[Epoch: 31400] train loss: 0.0675, train acc: 0.9775, val loss: 0.1928, val acc: 0.9666  (best train acc: 0.9827, best val acc: 0.9703, best train loss: 0.0564  @ epoch 31398 )\n",
      "[Epoch: 31420] train loss: 0.0681, train acc: 0.9769, val loss: 0.1778, val acc: 0.9653  (best train acc: 0.9827, best val acc: 0.9703, best train loss: 0.0564  @ epoch 31398 )\n",
      "[Epoch: 31440] train loss: 0.0651, train acc: 0.9787, val loss: 0.1526, val acc: 0.9676  (best train acc: 0.9827, best val acc: 0.9703, best train loss: 0.0564  @ epoch 31398 )\n",
      "[Epoch: 31460] train loss: 0.0796, train acc: 0.9733, val loss: 0.1849, val acc: 0.9680  (best train acc: 0.9827, best val acc: 0.9703, best train loss: 0.0564  @ epoch 31398 )\n",
      "[Epoch: 31480] train loss: 0.0755, train acc: 0.9755, val loss: 0.1962, val acc: 0.9605  (best train acc: 0.9827, best val acc: 0.9703, best train loss: 0.0564  @ epoch 31398 )\n",
      "[Epoch: 31500] train loss: 0.0668, train acc: 0.9776, val loss: 0.1661, val acc: 0.9653  (best train acc: 0.9827, best val acc: 0.9703, best train loss: 0.0564  @ epoch 31398 )\n",
      "[Epoch: 31520] train loss: 0.0674, train acc: 0.9768, val loss: 0.1639, val acc: 0.9683  (best train acc: 0.9827, best val acc: 0.9703, best train loss: 0.0564  @ epoch 31398 )\n",
      "[Epoch: 31540] train loss: 0.0616, train acc: 0.9798, val loss: 0.2034, val acc: 0.9653  (best train acc: 0.9827, best val acc: 0.9703, best train loss: 0.0564  @ epoch 31398 )\n",
      "[Epoch: 31560] train loss: 0.0695, train acc: 0.9767, val loss: 0.1967, val acc: 0.9568  (best train acc: 0.9827, best val acc: 0.9703, best train loss: 0.0564  @ epoch 31398 )\n",
      "[Epoch: 31580] train loss: 0.0817, train acc: 0.9687, val loss: 0.1748, val acc: 0.9653  (best train acc: 0.9827, best val acc: 0.9703, best train loss: 0.0564  @ epoch 31398 )\n",
      "[Epoch: 31600] train loss: 0.0626, train acc: 0.9790, val loss: 0.1895, val acc: 0.9680  (best train acc: 0.9827, best val acc: 0.9703, best train loss: 0.0564  @ epoch 31398 )\n",
      "[Epoch: 31620] train loss: 0.0647, train acc: 0.9777, val loss: 0.2121, val acc: 0.9649  (best train acc: 0.9827, best val acc: 0.9703, best train loss: 0.0564  @ epoch 31398 )\n",
      "[Epoch: 31640] train loss: 0.0826, train acc: 0.9753, val loss: 0.1630, val acc: 0.9649  (best train acc: 0.9827, best val acc: 0.9703, best train loss: 0.0564  @ epoch 31398 )\n",
      "[Epoch: 31660] train loss: 0.0624, train acc: 0.9802, val loss: 0.1735, val acc: 0.9676  (best train acc: 0.9827, best val acc: 0.9703, best train loss: 0.0564  @ epoch 31398 )\n",
      "[Epoch: 31680] train loss: 0.0884, train acc: 0.9677, val loss: 0.1997, val acc: 0.9555  (best train acc: 0.9827, best val acc: 0.9703, best train loss: 0.0564  @ epoch 31398 )\n",
      "[Epoch: 31700] train loss: 0.0734, train acc: 0.9740, val loss: 0.2202, val acc: 0.9656  (best train acc: 0.9827, best val acc: 0.9703, best train loss: 0.0564  @ epoch 31398 )\n",
      "[Epoch: 31720] train loss: 0.0668, train acc: 0.9774, val loss: 0.1761, val acc: 0.9673  (best train acc: 0.9827, best val acc: 0.9703, best train loss: 0.0564  @ epoch 31398 )\n",
      "[Epoch: 31740] train loss: 0.0664, train acc: 0.9786, val loss: 0.1927, val acc: 0.9676  (best train acc: 0.9827, best val acc: 0.9703, best train loss: 0.0564  @ epoch 31398 )\n",
      "[Epoch: 31760] train loss: 0.0757, train acc: 0.9732, val loss: 0.2121, val acc: 0.9642  (best train acc: 0.9827, best val acc: 0.9703, best train loss: 0.0564  @ epoch 31398 )\n",
      "[Epoch: 31780] train loss: 0.0594, train acc: 0.9809, val loss: 0.1733, val acc: 0.9680  (best train acc: 0.9827, best val acc: 0.9703, best train loss: 0.0564  @ epoch 31398 )\n",
      "[Epoch: 31800] train loss: 0.0718, train acc: 0.9749, val loss: 0.1925, val acc: 0.9663  (best train acc: 0.9827, best val acc: 0.9703, best train loss: 0.0564  @ epoch 31398 )\n",
      "[Epoch: 31820] train loss: 0.0656, train acc: 0.9787, val loss: 0.1797, val acc: 0.9656  (best train acc: 0.9827, best val acc: 0.9703, best train loss: 0.0564  @ epoch 31398 )\n",
      "[Epoch: 31840] train loss: 0.0724, train acc: 0.9769, val loss: 0.1580, val acc: 0.9653  (best train acc: 0.9827, best val acc: 0.9703, best train loss: 0.0564  @ epoch 31398 )\n",
      "[Epoch: 31860] train loss: 0.0904, train acc: 0.9722, val loss: 0.1388, val acc: 0.9669  (best train acc: 0.9827, best val acc: 0.9703, best train loss: 0.0564  @ epoch 31398 )\n",
      "[Epoch: 31880] train loss: 0.0866, train acc: 0.9703, val loss: 0.1656, val acc: 0.9673  (best train acc: 0.9827, best val acc: 0.9703, best train loss: 0.0564  @ epoch 31398 )\n",
      "[Epoch: 31900] train loss: 0.0710, train acc: 0.9753, val loss: 0.1454, val acc: 0.9693  (best train acc: 0.9827, best val acc: 0.9703, best train loss: 0.0564  @ epoch 31398 )\n",
      "[Epoch: 31920] train loss: 0.0693, train acc: 0.9772, val loss: 0.1560, val acc: 0.9683  (best train acc: 0.9827, best val acc: 0.9703, best train loss: 0.0564  @ epoch 31398 )\n",
      "[Epoch: 31940] train loss: 0.0702, train acc: 0.9751, val loss: 0.1952, val acc: 0.9639  (best train acc: 0.9827, best val acc: 0.9703, best train loss: 0.0564  @ epoch 31398 )\n",
      "[Epoch: 31960] train loss: 0.0805, train acc: 0.9725, val loss: 0.1766, val acc: 0.9659  (best train acc: 0.9827, best val acc: 0.9703, best train loss: 0.0564  @ epoch 31398 )\n",
      "[Epoch: 31980] train loss: 0.0751, train acc: 0.9748, val loss: 0.1777, val acc: 0.9659  (best train acc: 0.9827, best val acc: 0.9703, best train loss: 0.0564  @ epoch 31398 )\n",
      "[Epoch: 32000] train loss: 0.0797, train acc: 0.9735, val loss: 0.1468, val acc: 0.9673  (best train acc: 0.9827, best val acc: 0.9703, best train loss: 0.0564  @ epoch 31398 )\n",
      "[Epoch: 32020] train loss: 0.0593, train acc: 0.9799, val loss: 0.1805, val acc: 0.9676  (best train acc: 0.9827, best val acc: 0.9703, best train loss: 0.0564  @ epoch 31398 )\n",
      "[Epoch: 32040] train loss: 0.0778, train acc: 0.9697, val loss: 0.1550, val acc: 0.9676  (best train acc: 0.9827, best val acc: 0.9703, best train loss: 0.0564  @ epoch 31398 )\n",
      "[Epoch: 32060] train loss: 0.0610, train acc: 0.9808, val loss: 0.1578, val acc: 0.9690  (best train acc: 0.9827, best val acc: 0.9703, best train loss: 0.0564  @ epoch 31398 )\n",
      "[Epoch: 32080] train loss: 0.0657, train acc: 0.9771, val loss: 0.1872, val acc: 0.9676  (best train acc: 0.9827, best val acc: 0.9703, best train loss: 0.0564  @ epoch 31398 )\n",
      "[Epoch: 32100] train loss: 0.0644, train acc: 0.9787, val loss: 0.1694, val acc: 0.9690  (best train acc: 0.9827, best val acc: 0.9703, best train loss: 0.0564  @ epoch 31398 )\n",
      "[Epoch: 32120] train loss: 0.0727, train acc: 0.9779, val loss: 0.1798, val acc: 0.9669  (best train acc: 0.9827, best val acc: 0.9703, best train loss: 0.0558  @ epoch 32116 )\n",
      "[Epoch: 32140] train loss: 0.0656, train acc: 0.9793, val loss: 0.1849, val acc: 0.9690  (best train acc: 0.9827, best val acc: 0.9703, best train loss: 0.0558  @ epoch 32116 )\n",
      "[Epoch: 32160] train loss: 0.0628, train acc: 0.9787, val loss: 0.1745, val acc: 0.9649  (best train acc: 0.9827, best val acc: 0.9703, best train loss: 0.0558  @ epoch 32116 )\n",
      "[Epoch: 32180] train loss: 0.1247, train acc: 0.9511, val loss: 0.1884, val acc: 0.9518  (best train acc: 0.9827, best val acc: 0.9703, best train loss: 0.0558  @ epoch 32116 )\n",
      "[Epoch: 32200] train loss: 0.0817, train acc: 0.9706, val loss: 0.1681, val acc: 0.9686  (best train acc: 0.9827, best val acc: 0.9703, best train loss: 0.0558  @ epoch 32116 )\n",
      "[Epoch: 32220] train loss: 0.1038, train acc: 0.9587, val loss: 0.2046, val acc: 0.9352  (best train acc: 0.9827, best val acc: 0.9703, best train loss: 0.0558  @ epoch 32116 )\n",
      "[Epoch: 32240] train loss: 0.0694, train acc: 0.9788, val loss: 0.1651, val acc: 0.9666  (best train acc: 0.9827, best val acc: 0.9703, best train loss: 0.0558  @ epoch 32116 )\n",
      "[Epoch: 32260] train loss: 0.0615, train acc: 0.9803, val loss: 0.1838, val acc: 0.9666  (best train acc: 0.9827, best val acc: 0.9713, best train loss: 0.0558  @ epoch 32116 )\n",
      "[Epoch: 32280] train loss: 0.0787, train acc: 0.9751, val loss: 0.1743, val acc: 0.9673  (best train acc: 0.9827, best val acc: 0.9713, best train loss: 0.0558  @ epoch 32116 )\n",
      "[Epoch: 32300] train loss: 0.0652, train acc: 0.9783, val loss: 0.1629, val acc: 0.9700  (best train acc: 0.9827, best val acc: 0.9713, best train loss: 0.0558  @ epoch 32116 )\n",
      "[Epoch: 32320] train loss: 0.0629, train acc: 0.9790, val loss: 0.1899, val acc: 0.9659  (best train acc: 0.9827, best val acc: 0.9713, best train loss: 0.0558  @ epoch 32116 )\n",
      "[Epoch: 32340] train loss: 0.0686, train acc: 0.9752, val loss: 0.1698, val acc: 0.9669  (best train acc: 0.9827, best val acc: 0.9713, best train loss: 0.0558  @ epoch 32116 )\n",
      "[Epoch: 32360] train loss: 0.0690, train acc: 0.9773, val loss: 0.1676, val acc: 0.9666  (best train acc: 0.9827, best val acc: 0.9713, best train loss: 0.0558  @ epoch 32116 )\n",
      "[Epoch: 32380] train loss: 0.0634, train acc: 0.9790, val loss: 0.1895, val acc: 0.9690  (best train acc: 0.9827, best val acc: 0.9713, best train loss: 0.0558  @ epoch 32116 )\n",
      "[Epoch: 32400] train loss: 0.0672, train acc: 0.9774, val loss: 0.1799, val acc: 0.9659  (best train acc: 0.9827, best val acc: 0.9713, best train loss: 0.0550  @ epoch 32395 )\n",
      "[Epoch: 32420] train loss: 0.0649, train acc: 0.9789, val loss: 0.1962, val acc: 0.9673  (best train acc: 0.9827, best val acc: 0.9713, best train loss: 0.0550  @ epoch 32395 )\n",
      "[Epoch: 32440] train loss: 0.0836, train acc: 0.9727, val loss: 0.1905, val acc: 0.9669  (best train acc: 0.9827, best val acc: 0.9713, best train loss: 0.0550  @ epoch 32395 )\n",
      "[Epoch: 32460] train loss: 0.0818, train acc: 0.9730, val loss: 0.1570, val acc: 0.9666  (best train acc: 0.9827, best val acc: 0.9713, best train loss: 0.0550  @ epoch 32395 )\n",
      "[Epoch: 32480] train loss: 0.0805, train acc: 0.9759, val loss: 0.1857, val acc: 0.9669  (best train acc: 0.9827, best val acc: 0.9713, best train loss: 0.0550  @ epoch 32395 )\n",
      "[Epoch: 32500] train loss: 0.0613, train acc: 0.9792, val loss: 0.1743, val acc: 0.9690  (best train acc: 0.9827, best val acc: 0.9713, best train loss: 0.0550  @ epoch 32395 )\n",
      "[Epoch: 32520] train loss: 0.0721, train acc: 0.9765, val loss: 0.1860, val acc: 0.9683  (best train acc: 0.9827, best val acc: 0.9713, best train loss: 0.0550  @ epoch 32395 )\n",
      "[Epoch: 32540] train loss: 0.0815, train acc: 0.9720, val loss: 0.1873, val acc: 0.9659  (best train acc: 0.9827, best val acc: 0.9713, best train loss: 0.0550  @ epoch 32395 )\n",
      "[Epoch: 32560] train loss: 0.0751, train acc: 0.9721, val loss: 0.1704, val acc: 0.9673  (best train acc: 0.9827, best val acc: 0.9713, best train loss: 0.0550  @ epoch 32395 )\n",
      "[Epoch: 32580] train loss: 0.0636, train acc: 0.9784, val loss: 0.1829, val acc: 0.9673  (best train acc: 0.9827, best val acc: 0.9713, best train loss: 0.0550  @ epoch 32395 )\n",
      "[Epoch: 32600] train loss: 0.0850, train acc: 0.9715, val loss: 0.1781, val acc: 0.9639  (best train acc: 0.9827, best val acc: 0.9713, best train loss: 0.0550  @ epoch 32395 )\n",
      "[Epoch: 32620] train loss: 0.0641, train acc: 0.9785, val loss: 0.1901, val acc: 0.9653  (best train acc: 0.9827, best val acc: 0.9713, best train loss: 0.0550  @ epoch 32395 )\n",
      "[Epoch: 32640] train loss: 0.0578, train acc: 0.9814, val loss: 0.1850, val acc: 0.9656  (best train acc: 0.9827, best val acc: 0.9713, best train loss: 0.0550  @ epoch 32395 )\n",
      "[Epoch: 32660] train loss: 0.0628, train acc: 0.9785, val loss: 0.1954, val acc: 0.9669  (best train acc: 0.9827, best val acc: 0.9713, best train loss: 0.0550  @ epoch 32395 )\n",
      "[Epoch: 32680] train loss: 0.0977, train acc: 0.9685, val loss: 0.1923, val acc: 0.9545  (best train acc: 0.9827, best val acc: 0.9713, best train loss: 0.0550  @ epoch 32395 )\n",
      "[Epoch: 32700] train loss: 0.1025, train acc: 0.9675, val loss: 0.2593, val acc: 0.9548  (best train acc: 0.9827, best val acc: 0.9713, best train loss: 0.0550  @ epoch 32395 )\n",
      "[Epoch: 32720] train loss: 0.0716, train acc: 0.9772, val loss: 0.1652, val acc: 0.9683  (best train acc: 0.9827, best val acc: 0.9713, best train loss: 0.0550  @ epoch 32395 )\n",
      "[Epoch: 32740] train loss: 0.0901, train acc: 0.9706, val loss: 0.2001, val acc: 0.9639  (best train acc: 0.9827, best val acc: 0.9713, best train loss: 0.0550  @ epoch 32395 )\n",
      "[Epoch: 32760] train loss: 0.0654, train acc: 0.9790, val loss: 0.1902, val acc: 0.9659  (best train acc: 0.9827, best val acc: 0.9713, best train loss: 0.0550  @ epoch 32395 )\n",
      "[Epoch: 32780] train loss: 0.0666, train acc: 0.9785, val loss: 0.1790, val acc: 0.9663  (best train acc: 0.9827, best val acc: 0.9713, best train loss: 0.0550  @ epoch 32395 )\n",
      "[Epoch: 32800] train loss: 0.0779, train acc: 0.9703, val loss: 0.1831, val acc: 0.9666  (best train acc: 0.9827, best val acc: 0.9713, best train loss: 0.0550  @ epoch 32395 )\n",
      "[Epoch: 32820] train loss: 0.0603, train acc: 0.9808, val loss: 0.2004, val acc: 0.9676  (best train acc: 0.9827, best val acc: 0.9713, best train loss: 0.0550  @ epoch 32395 )\n",
      "[Epoch: 32840] train loss: 0.0908, train acc: 0.9691, val loss: 0.1974, val acc: 0.9649  (best train acc: 0.9827, best val acc: 0.9713, best train loss: 0.0550  @ epoch 32395 )\n",
      "[Epoch: 32860] train loss: 0.0850, train acc: 0.9727, val loss: 0.1935, val acc: 0.9599  (best train acc: 0.9827, best val acc: 0.9713, best train loss: 0.0550  @ epoch 32395 )\n",
      "[Epoch: 32880] train loss: 0.0680, train acc: 0.9779, val loss: 0.2034, val acc: 0.9659  (best train acc: 0.9827, best val acc: 0.9713, best train loss: 0.0550  @ epoch 32395 )\n",
      "[Epoch: 32900] train loss: 0.0591, train acc: 0.9811, val loss: 0.1852, val acc: 0.9676  (best train acc: 0.9827, best val acc: 0.9713, best train loss: 0.0550  @ epoch 32395 )\n",
      "[Epoch: 32920] train loss: 0.0735, train acc: 0.9727, val loss: 0.1870, val acc: 0.9622  (best train acc: 0.9827, best val acc: 0.9713, best train loss: 0.0550  @ epoch 32395 )\n",
      "[Epoch: 32940] train loss: 0.0707, train acc: 0.9759, val loss: 0.1802, val acc: 0.9690  (best train acc: 0.9827, best val acc: 0.9713, best train loss: 0.0550  @ epoch 32395 )\n",
      "[Epoch: 32960] train loss: 0.0714, train acc: 0.9748, val loss: 0.1640, val acc: 0.9649  (best train acc: 0.9827, best val acc: 0.9713, best train loss: 0.0550  @ epoch 32395 )\n",
      "[Epoch: 32980] train loss: 0.0636, train acc: 0.9788, val loss: 0.1740, val acc: 0.9676  (best train acc: 0.9827, best val acc: 0.9713, best train loss: 0.0550  @ epoch 32395 )\n",
      "[Epoch: 33000] train loss: 0.0570, train acc: 0.9804, val loss: 0.1858, val acc: 0.9673  (best train acc: 0.9827, best val acc: 0.9713, best train loss: 0.0550  @ epoch 32395 )\n",
      "[Epoch: 33020] train loss: 0.0758, train acc: 0.9704, val loss: 0.1815, val acc: 0.9659  (best train acc: 0.9827, best val acc: 0.9713, best train loss: 0.0550  @ epoch 32395 )\n",
      "[Epoch: 33040] train loss: 0.0686, train acc: 0.9751, val loss: 0.1845, val acc: 0.9649  (best train acc: 0.9827, best val acc: 0.9713, best train loss: 0.0550  @ epoch 32395 )\n",
      "[Epoch: 33060] train loss: 0.0642, train acc: 0.9799, val loss: 0.1825, val acc: 0.9669  (best train acc: 0.9827, best val acc: 0.9713, best train loss: 0.0550  @ epoch 32395 )\n",
      "[Epoch: 33080] train loss: 0.0705, train acc: 0.9768, val loss: 0.1690, val acc: 0.9656  (best train acc: 0.9827, best val acc: 0.9713, best train loss: 0.0550  @ epoch 32395 )\n",
      "[Epoch: 33100] train loss: 0.0801, train acc: 0.9813, val loss: 0.1697, val acc: 0.9676  (best train acc: 0.9827, best val acc: 0.9713, best train loss: 0.0550  @ epoch 32395 )\n",
      "[Epoch: 33120] train loss: 0.0639, train acc: 0.9787, val loss: 0.1849, val acc: 0.9639  (best train acc: 0.9827, best val acc: 0.9713, best train loss: 0.0550  @ epoch 32395 )\n",
      "[Epoch: 33140] train loss: 0.0829, train acc: 0.9727, val loss: 0.1656, val acc: 0.9629  (best train acc: 0.9827, best val acc: 0.9713, best train loss: 0.0550  @ epoch 32395 )\n",
      "[Epoch: 33160] train loss: 0.0652, train acc: 0.9763, val loss: 0.1830, val acc: 0.9653  (best train acc: 0.9827, best val acc: 0.9713, best train loss: 0.0550  @ epoch 32395 )\n",
      "[Epoch: 33180] train loss: 0.0627, train acc: 0.9795, val loss: 0.2017, val acc: 0.9663  (best train acc: 0.9827, best val acc: 0.9713, best train loss: 0.0550  @ epoch 32395 )\n",
      "[Epoch: 33200] train loss: 0.0640, train acc: 0.9775, val loss: 0.1774, val acc: 0.9676  (best train acc: 0.9827, best val acc: 0.9713, best train loss: 0.0550  @ epoch 32395 )\n",
      "[Epoch: 33220] train loss: 0.0563, train acc: 0.9813, val loss: 0.1837, val acc: 0.9653  (best train acc: 0.9827, best val acc: 0.9713, best train loss: 0.0550  @ epoch 32395 )\n",
      "[Epoch: 33240] train loss: 0.1014, train acc: 0.9675, val loss: 0.1796, val acc: 0.9609  (best train acc: 0.9827, best val acc: 0.9713, best train loss: 0.0550  @ epoch 32395 )\n",
      "[Epoch: 33260] train loss: 0.0629, train acc: 0.9808, val loss: 0.1630, val acc: 0.9683  (best train acc: 0.9827, best val acc: 0.9713, best train loss: 0.0550  @ epoch 32395 )\n",
      "[Epoch: 33280] train loss: 0.0622, train acc: 0.9787, val loss: 0.1733, val acc: 0.9659  (best train acc: 0.9827, best val acc: 0.9713, best train loss: 0.0550  @ epoch 32395 )\n",
      "[Epoch: 33300] train loss: 0.0593, train acc: 0.9816, val loss: 0.1765, val acc: 0.9686  (best train acc: 0.9828, best val acc: 0.9713, best train loss: 0.0550  @ epoch 32395 )\n",
      "[Epoch: 33320] train loss: 0.0612, train acc: 0.9802, val loss: 0.1699, val acc: 0.9666  (best train acc: 0.9828, best val acc: 0.9713, best train loss: 0.0550  @ epoch 32395 )\n",
      "[Epoch: 33340] train loss: 0.0600, train acc: 0.9798, val loss: 0.1687, val acc: 0.9680  (best train acc: 0.9828, best val acc: 0.9713, best train loss: 0.0550  @ epoch 32395 )\n",
      "[Epoch: 33360] train loss: 0.0713, train acc: 0.9774, val loss: 0.1578, val acc: 0.9663  (best train acc: 0.9828, best val acc: 0.9713, best train loss: 0.0550  @ epoch 32395 )\n",
      "[Epoch: 33380] train loss: 0.0718, train acc: 0.9764, val loss: 0.1954, val acc: 0.9636  (best train acc: 0.9828, best val acc: 0.9713, best train loss: 0.0550  @ epoch 32395 )\n",
      "[Epoch: 33400] train loss: 0.0724, train acc: 0.9760, val loss: 0.1777, val acc: 0.9666  (best train acc: 0.9828, best val acc: 0.9713, best train loss: 0.0550  @ epoch 32395 )\n",
      "[Epoch: 33420] train loss: 0.0779, train acc: 0.9752, val loss: 0.1817, val acc: 0.9659  (best train acc: 0.9828, best val acc: 0.9713, best train loss: 0.0550  @ epoch 32395 )\n",
      "[Epoch: 33440] train loss: 0.0853, train acc: 0.9654, val loss: 0.2014, val acc: 0.9494  (best train acc: 0.9828, best val acc: 0.9713, best train loss: 0.0550  @ epoch 32395 )\n",
      "[Epoch: 33460] train loss: 0.1174, train acc: 0.9550, val loss: 0.2170, val acc: 0.9359  (best train acc: 0.9828, best val acc: 0.9713, best train loss: 0.0550  @ epoch 32395 )\n",
      "[Epoch: 33480] train loss: 0.0658, train acc: 0.9785, val loss: 0.2016, val acc: 0.9632  (best train acc: 0.9828, best val acc: 0.9713, best train loss: 0.0550  @ epoch 32395 )\n",
      "[Epoch: 33500] train loss: 0.0660, train acc: 0.9777, val loss: 0.1784, val acc: 0.9663  (best train acc: 0.9828, best val acc: 0.9713, best train loss: 0.0550  @ epoch 32395 )\n",
      "[Epoch: 33520] train loss: 0.0607, train acc: 0.9813, val loss: 0.1993, val acc: 0.9669  (best train acc: 0.9828, best val acc: 0.9713, best train loss: 0.0550  @ epoch 32395 )\n",
      "[Epoch: 33540] train loss: 0.0797, train acc: 0.9671, val loss: 0.1798, val acc: 0.9673  (best train acc: 0.9828, best val acc: 0.9713, best train loss: 0.0550  @ epoch 32395 )\n",
      "[Epoch: 33560] train loss: 0.0722, train acc: 0.9733, val loss: 0.1775, val acc: 0.9666  (best train acc: 0.9828, best val acc: 0.9713, best train loss: 0.0550  @ epoch 32395 )\n",
      "[Epoch: 33580] train loss: 0.0783, train acc: 0.9710, val loss: 0.1728, val acc: 0.9669  (best train acc: 0.9828, best val acc: 0.9713, best train loss: 0.0550  @ epoch 32395 )\n",
      "[Epoch: 33600] train loss: 0.0630, train acc: 0.9778, val loss: 0.2124, val acc: 0.9629  (best train acc: 0.9828, best val acc: 0.9713, best train loss: 0.0550  @ epoch 32395 )\n",
      "[Epoch: 33620] train loss: 0.0679, train acc: 0.9787, val loss: 0.1806, val acc: 0.9642  (best train acc: 0.9828, best val acc: 0.9713, best train loss: 0.0550  @ epoch 32395 )\n",
      "[Epoch: 33640] train loss: 0.0598, train acc: 0.9795, val loss: 0.1739, val acc: 0.9666  (best train acc: 0.9828, best val acc: 0.9713, best train loss: 0.0550  @ epoch 32395 )\n",
      "[Epoch: 33660] train loss: 0.0632, train acc: 0.9785, val loss: 0.1976, val acc: 0.9663  (best train acc: 0.9828, best val acc: 0.9713, best train loss: 0.0550  @ epoch 32395 )\n",
      "[Epoch: 33680] train loss: 0.0638, train acc: 0.9791, val loss: 0.2107, val acc: 0.9663  (best train acc: 0.9828, best val acc: 0.9713, best train loss: 0.0550  @ epoch 32395 )\n",
      "[Epoch: 33700] train loss: 0.0662, train acc: 0.9760, val loss: 0.1707, val acc: 0.9696  (best train acc: 0.9828, best val acc: 0.9713, best train loss: 0.0550  @ epoch 32395 )\n",
      "[Epoch: 33720] train loss: 0.0590, train acc: 0.9798, val loss: 0.1830, val acc: 0.9666  (best train acc: 0.9828, best val acc: 0.9713, best train loss: 0.0550  @ epoch 32395 )\n",
      "[Epoch: 33740] train loss: 0.0629, train acc: 0.9786, val loss: 0.1832, val acc: 0.9676  (best train acc: 0.9828, best val acc: 0.9713, best train loss: 0.0550  @ epoch 32395 )\n",
      "[Epoch: 33760] train loss: 0.0687, train acc: 0.9764, val loss: 0.1797, val acc: 0.9659  (best train acc: 0.9828, best val acc: 0.9713, best train loss: 0.0550  @ epoch 32395 )\n",
      "[Epoch: 33780] train loss: 0.0752, train acc: 0.9753, val loss: 0.1565, val acc: 0.9666  (best train acc: 0.9828, best val acc: 0.9713, best train loss: 0.0545  @ epoch 33774 )\n",
      "[Epoch: 33800] train loss: 0.0676, train acc: 0.9759, val loss: 0.1862, val acc: 0.9646  (best train acc: 0.9828, best val acc: 0.9713, best train loss: 0.0545  @ epoch 33774 )\n",
      "[Epoch: 33820] train loss: 0.0701, train acc: 0.9777, val loss: 0.1764, val acc: 0.9683  (best train acc: 0.9828, best val acc: 0.9713, best train loss: 0.0545  @ epoch 33774 )\n",
      "[Epoch: 33840] train loss: 0.0673, train acc: 0.9785, val loss: 0.1787, val acc: 0.9693  (best train acc: 0.9828, best val acc: 0.9713, best train loss: 0.0545  @ epoch 33774 )\n",
      "[Epoch: 33860] train loss: 0.0704, train acc: 0.9777, val loss: 0.1705, val acc: 0.9659  (best train acc: 0.9828, best val acc: 0.9713, best train loss: 0.0545  @ epoch 33774 )\n",
      "[Epoch: 33880] train loss: 0.1435, train acc: 0.9533, val loss: 0.2016, val acc: 0.9602  (best train acc: 0.9828, best val acc: 0.9713, best train loss: 0.0545  @ epoch 33774 )\n",
      "[Epoch: 33900] train loss: 0.0944, train acc: 0.9665, val loss: 0.2011, val acc: 0.9619  (best train acc: 0.9828, best val acc: 0.9713, best train loss: 0.0545  @ epoch 33774 )\n",
      "[Epoch: 33920] train loss: 0.0738, train acc: 0.9741, val loss: 0.1962, val acc: 0.9649  (best train acc: 0.9828, best val acc: 0.9713, best train loss: 0.0545  @ epoch 33774 )\n",
      "[Epoch: 33940] train loss: 0.0671, train acc: 0.9783, val loss: 0.1883, val acc: 0.9673  (best train acc: 0.9828, best val acc: 0.9713, best train loss: 0.0545  @ epoch 33774 )\n",
      "[Epoch: 33960] train loss: 0.0685, train acc: 0.9772, val loss: 0.1918, val acc: 0.9680  (best train acc: 0.9828, best val acc: 0.9713, best train loss: 0.0545  @ epoch 33774 )\n",
      "[Epoch: 33980] train loss: 0.0637, train acc: 0.9785, val loss: 0.1802, val acc: 0.9659  (best train acc: 0.9828, best val acc: 0.9713, best train loss: 0.0545  @ epoch 33774 )\n",
      "[Epoch: 34000] train loss: 0.0663, train acc: 0.9779, val loss: 0.1933, val acc: 0.9663  (best train acc: 0.9828, best val acc: 0.9713, best train loss: 0.0545  @ epoch 33774 )\n",
      "[Epoch: 34020] train loss: 0.0834, train acc: 0.9806, val loss: 0.1839, val acc: 0.9676  (best train acc: 0.9828, best val acc: 0.9713, best train loss: 0.0545  @ epoch 33774 )\n",
      "[Epoch: 34040] train loss: 0.0634, train acc: 0.9779, val loss: 0.1614, val acc: 0.9669  (best train acc: 0.9828, best val acc: 0.9713, best train loss: 0.0545  @ epoch 33774 )\n",
      "[Epoch: 34060] train loss: 0.0859, train acc: 0.9738, val loss: 0.1971, val acc: 0.9659  (best train acc: 0.9828, best val acc: 0.9713, best train loss: 0.0545  @ epoch 33774 )\n",
      "[Epoch: 34080] train loss: 0.0648, train acc: 0.9777, val loss: 0.1812, val acc: 0.9669  (best train acc: 0.9828, best val acc: 0.9713, best train loss: 0.0545  @ epoch 33774 )\n",
      "[Epoch: 34100] train loss: 0.0644, train acc: 0.9804, val loss: 0.1920, val acc: 0.9666  (best train acc: 0.9828, best val acc: 0.9713, best train loss: 0.0545  @ epoch 33774 )\n",
      "[Epoch: 34120] train loss: 0.0601, train acc: 0.9803, val loss: 0.1941, val acc: 0.9639  (best train acc: 0.9828, best val acc: 0.9713, best train loss: 0.0545  @ epoch 33774 )\n",
      "[Epoch: 34140] train loss: 0.0605, train acc: 0.9807, val loss: 0.1799, val acc: 0.9663  (best train acc: 0.9828, best val acc: 0.9713, best train loss: 0.0545  @ epoch 33774 )\n",
      "[Epoch: 34160] train loss: 0.0936, train acc: 0.9624, val loss: 0.1721, val acc: 0.9646  (best train acc: 0.9828, best val acc: 0.9713, best train loss: 0.0545  @ epoch 33774 )\n",
      "[Epoch: 34180] train loss: 0.0669, train acc: 0.9782, val loss: 0.2037, val acc: 0.9646  (best train acc: 0.9828, best val acc: 0.9713, best train loss: 0.0545  @ epoch 33774 )\n",
      "[Epoch: 34200] train loss: 0.0697, train acc: 0.9779, val loss: 0.1724, val acc: 0.9666  (best train acc: 0.9828, best val acc: 0.9713, best train loss: 0.0545  @ epoch 33774 )\n",
      "[Epoch: 34220] train loss: 0.0647, train acc: 0.9778, val loss: 0.1651, val acc: 0.9669  (best train acc: 0.9828, best val acc: 0.9713, best train loss: 0.0545  @ epoch 33774 )\n",
      "[Epoch: 34240] train loss: 0.0621, train acc: 0.9800, val loss: 0.1859, val acc: 0.9683  (best train acc: 0.9828, best val acc: 0.9713, best train loss: 0.0545  @ epoch 33774 )\n",
      "[Epoch: 34260] train loss: 0.0632, train acc: 0.9804, val loss: 0.1878, val acc: 0.9653  (best train acc: 0.9828, best val acc: 0.9713, best train loss: 0.0545  @ epoch 33774 )\n",
      "[Epoch: 34280] train loss: 0.0615, train acc: 0.9790, val loss: 0.1725, val acc: 0.9676  (best train acc: 0.9828, best val acc: 0.9713, best train loss: 0.0545  @ epoch 33774 )\n",
      "[Epoch: 34300] train loss: 0.0647, train acc: 0.9775, val loss: 0.1883, val acc: 0.9669  (best train acc: 0.9828, best val acc: 0.9713, best train loss: 0.0545  @ epoch 33774 )\n",
      "[Epoch: 34320] train loss: 0.0816, train acc: 0.9725, val loss: 0.2009, val acc: 0.9656  (best train acc: 0.9828, best val acc: 0.9713, best train loss: 0.0545  @ epoch 33774 )\n",
      "[Epoch: 34340] train loss: 0.0603, train acc: 0.9789, val loss: 0.1791, val acc: 0.9669  (best train acc: 0.9828, best val acc: 0.9713, best train loss: 0.0545  @ epoch 33774 )\n",
      "[Epoch: 34360] train loss: 0.0611, train acc: 0.9784, val loss: 0.2014, val acc: 0.9676  (best train acc: 0.9834, best val acc: 0.9713, best train loss: 0.0545  @ epoch 33774 )\n",
      "[Epoch: 34380] train loss: 0.0813, train acc: 0.9722, val loss: 0.1853, val acc: 0.9666  (best train acc: 0.9834, best val acc: 0.9713, best train loss: 0.0545  @ epoch 33774 )\n",
      "[Epoch: 34400] train loss: 0.1615, train acc: 0.9534, val loss: 0.2236, val acc: 0.9531  (best train acc: 0.9834, best val acc: 0.9713, best train loss: 0.0545  @ epoch 33774 )\n",
      "[Epoch: 34420] train loss: 0.1043, train acc: 0.9660, val loss: 0.2106, val acc: 0.9626  (best train acc: 0.9834, best val acc: 0.9713, best train loss: 0.0545  @ epoch 33774 )\n",
      "[Epoch: 34440] train loss: 0.0740, train acc: 0.9746, val loss: 0.1622, val acc: 0.9713  (best train acc: 0.9834, best val acc: 0.9713, best train loss: 0.0545  @ epoch 33774 )\n",
      "[Epoch: 34460] train loss: 0.0670, train acc: 0.9782, val loss: 0.1985, val acc: 0.9676  (best train acc: 0.9834, best val acc: 0.9713, best train loss: 0.0545  @ epoch 33774 )\n",
      "[Epoch: 34480] train loss: 0.0636, train acc: 0.9797, val loss: 0.1784, val acc: 0.9676  (best train acc: 0.9834, best val acc: 0.9713, best train loss: 0.0545  @ epoch 33774 )\n",
      "[Epoch: 34500] train loss: 0.0853, train acc: 0.9750, val loss: 0.2319, val acc: 0.9646  (best train acc: 0.9834, best val acc: 0.9713, best train loss: 0.0545  @ epoch 33774 )\n",
      "[Epoch: 34520] train loss: 0.0613, train acc: 0.9788, val loss: 0.1659, val acc: 0.9680  (best train acc: 0.9834, best val acc: 0.9713, best train loss: 0.0545  @ epoch 33774 )\n",
      "[Epoch: 34540] train loss: 0.0628, train acc: 0.9800, val loss: 0.1882, val acc: 0.9686  (best train acc: 0.9834, best val acc: 0.9713, best train loss: 0.0545  @ epoch 33774 )\n",
      "[Epoch: 34560] train loss: 0.0686, train acc: 0.9780, val loss: 0.1811, val acc: 0.9666  (best train acc: 0.9834, best val acc: 0.9713, best train loss: 0.0545  @ epoch 33774 )\n",
      "[Epoch: 34580] train loss: 0.0639, train acc: 0.9782, val loss: 0.1701, val acc: 0.9673  (best train acc: 0.9834, best val acc: 0.9713, best train loss: 0.0545  @ epoch 33774 )\n",
      "[Epoch: 34600] train loss: 0.0630, train acc: 0.9793, val loss: 0.1754, val acc: 0.9690  (best train acc: 0.9834, best val acc: 0.9713, best train loss: 0.0539  @ epoch 34595 )\n",
      "[Epoch: 34620] train loss: 0.0757, train acc: 0.9719, val loss: 0.1812, val acc: 0.9666  (best train acc: 0.9834, best val acc: 0.9713, best train loss: 0.0539  @ epoch 34595 )\n",
      "[Epoch: 34640] train loss: 0.0971, train acc: 0.9665, val loss: 0.1874, val acc: 0.9646  (best train acc: 0.9834, best val acc: 0.9713, best train loss: 0.0539  @ epoch 34595 )\n",
      "[Epoch: 34660] train loss: 0.0735, train acc: 0.9739, val loss: 0.2016, val acc: 0.9642  (best train acc: 0.9834, best val acc: 0.9713, best train loss: 0.0539  @ epoch 34595 )\n",
      "[Epoch: 34680] train loss: 0.0763, train acc: 0.9748, val loss: 0.1636, val acc: 0.9659  (best train acc: 0.9834, best val acc: 0.9713, best train loss: 0.0539  @ epoch 34595 )\n",
      "[Epoch: 34700] train loss: 0.0621, train acc: 0.9803, val loss: 0.1828, val acc: 0.9649  (best train acc: 0.9834, best val acc: 0.9713, best train loss: 0.0539  @ epoch 34595 )\n",
      "[Epoch: 34720] train loss: 0.0617, train acc: 0.9813, val loss: 0.1767, val acc: 0.9666  (best train acc: 0.9834, best val acc: 0.9713, best train loss: 0.0539  @ epoch 34595 )\n",
      "[Epoch: 34740] train loss: 0.0635, train acc: 0.9780, val loss: 0.1811, val acc: 0.9676  (best train acc: 0.9834, best val acc: 0.9713, best train loss: 0.0539  @ epoch 34595 )\n",
      "[Epoch: 34760] train loss: 0.0598, train acc: 0.9804, val loss: 0.1740, val acc: 0.9663  (best train acc: 0.9834, best val acc: 0.9713, best train loss: 0.0539  @ epoch 34595 )\n",
      "[Epoch: 34780] train loss: 0.0681, train acc: 0.9767, val loss: 0.1874, val acc: 0.9659  (best train acc: 0.9834, best val acc: 0.9713, best train loss: 0.0539  @ epoch 34595 )\n",
      "[Epoch: 34800] train loss: 0.0583, train acc: 0.9804, val loss: 0.1772, val acc: 0.9663  (best train acc: 0.9834, best val acc: 0.9713, best train loss: 0.0539  @ epoch 34595 )\n",
      "[Epoch: 34820] train loss: 0.0671, train acc: 0.9776, val loss: 0.1786, val acc: 0.9656  (best train acc: 0.9834, best val acc: 0.9713, best train loss: 0.0539  @ epoch 34595 )\n",
      "[Epoch: 34840] train loss: 0.0616, train acc: 0.9785, val loss: 0.1946, val acc: 0.9656  (best train acc: 0.9834, best val acc: 0.9713, best train loss: 0.0539  @ epoch 34595 )\n",
      "[Epoch: 34860] train loss: 0.0576, train acc: 0.9786, val loss: 0.1909, val acc: 0.9663  (best train acc: 0.9834, best val acc: 0.9713, best train loss: 0.0539  @ epoch 34595 )\n",
      "[Epoch: 34880] train loss: 0.0632, train acc: 0.9793, val loss: 0.1848, val acc: 0.9663  (best train acc: 0.9834, best val acc: 0.9713, best train loss: 0.0539  @ epoch 34595 )\n",
      "[Epoch: 34900] train loss: 0.0654, train acc: 0.9788, val loss: 0.1504, val acc: 0.9680  (best train acc: 0.9834, best val acc: 0.9713, best train loss: 0.0539  @ epoch 34595 )\n",
      "[Epoch: 34920] train loss: 0.0656, train acc: 0.9774, val loss: 0.1950, val acc: 0.9653  (best train acc: 0.9834, best val acc: 0.9713, best train loss: 0.0539  @ epoch 34595 )\n",
      "[Epoch: 34940] train loss: 0.0700, train acc: 0.9792, val loss: 0.1861, val acc: 0.9656  (best train acc: 0.9834, best val acc: 0.9713, best train loss: 0.0539  @ epoch 34595 )\n",
      "[Epoch: 34960] train loss: 0.0609, train acc: 0.9802, val loss: 0.1866, val acc: 0.9673  (best train acc: 0.9834, best val acc: 0.9713, best train loss: 0.0539  @ epoch 34595 )\n",
      "[Epoch: 34980] train loss: 0.0642, train acc: 0.9769, val loss: 0.1931, val acc: 0.9673  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0539  @ epoch 34595 )\n",
      "[Epoch: 35000] train loss: 0.0703, train acc: 0.9761, val loss: 0.1632, val acc: 0.9663  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0539  @ epoch 34595 )\n",
      "[Epoch: 35020] train loss: 0.0615, train acc: 0.9793, val loss: 0.1653, val acc: 0.9653  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0539  @ epoch 34595 )\n",
      "[Epoch: 35040] train loss: 0.0673, train acc: 0.9761, val loss: 0.1909, val acc: 0.9663  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0539  @ epoch 34595 )\n",
      "[Epoch: 35060] train loss: 0.0644, train acc: 0.9806, val loss: 0.1897, val acc: 0.9663  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0539  @ epoch 34595 )\n",
      "[Epoch: 35080] train loss: 0.0659, train acc: 0.9761, val loss: 0.1806, val acc: 0.9642  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0539  @ epoch 34595 )\n",
      "[Epoch: 35100] train loss: 0.0905, train acc: 0.9644, val loss: 0.1708, val acc: 0.9646  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0539  @ epoch 34595 )\n",
      "[Epoch: 35120] train loss: 0.0691, train acc: 0.9774, val loss: 0.1920, val acc: 0.9609  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0539  @ epoch 34595 )\n",
      "[Epoch: 35140] train loss: 0.0659, train acc: 0.9778, val loss: 0.2126, val acc: 0.9680  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0539  @ epoch 34595 )\n",
      "[Epoch: 35160] train loss: 0.0774, train acc: 0.9746, val loss: 0.1724, val acc: 0.9669  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0539  @ epoch 34595 )\n",
      "[Epoch: 35180] train loss: 0.0613, train acc: 0.9798, val loss: 0.1962, val acc: 0.9653  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0539  @ epoch 34595 )\n",
      "[Epoch: 35200] train loss: 0.0953, train acc: 0.9620, val loss: 0.1637, val acc: 0.9653  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0539  @ epoch 34595 )\n",
      "[Epoch: 35220] train loss: 0.0802, train acc: 0.9724, val loss: 0.1834, val acc: 0.9659  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0539  @ epoch 34595 )\n",
      "[Epoch: 35240] train loss: 0.0754, train acc: 0.9729, val loss: 0.1544, val acc: 0.9636  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0539  @ epoch 34595 )\n",
      "[Epoch: 35260] train loss: 0.0865, train acc: 0.9730, val loss: 0.1712, val acc: 0.9632  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0539  @ epoch 34595 )\n",
      "[Epoch: 35280] train loss: 0.0652, train acc: 0.9766, val loss: 0.2051, val acc: 0.9599  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0539  @ epoch 34595 )\n",
      "[Epoch: 35300] train loss: 0.0730, train acc: 0.9752, val loss: 0.1671, val acc: 0.9653  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0539  @ epoch 34595 )\n",
      "[Epoch: 35320] train loss: 0.0609, train acc: 0.9799, val loss: 0.1898, val acc: 0.9653  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0539  @ epoch 34595 )\n",
      "[Epoch: 35340] train loss: 0.0958, train acc: 0.9693, val loss: 0.1510, val acc: 0.9619  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0539  @ epoch 34595 )\n",
      "[Epoch: 35360] train loss: 0.0719, train acc: 0.9764, val loss: 0.1907, val acc: 0.9669  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0539  @ epoch 34595 )\n",
      "[Epoch: 35380] train loss: 0.0762, train acc: 0.9746, val loss: 0.1803, val acc: 0.9659  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0539  @ epoch 34595 )\n",
      "[Epoch: 35400] train loss: 0.0651, train acc: 0.9786, val loss: 0.1796, val acc: 0.9663  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0539  @ epoch 34595 )\n",
      "[Epoch: 35420] train loss: 0.0745, train acc: 0.9751, val loss: 0.1707, val acc: 0.9666  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0539  @ epoch 34595 )\n",
      "[Epoch: 35440] train loss: 0.0700, train acc: 0.9768, val loss: 0.1474, val acc: 0.9673  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0539  @ epoch 34595 )\n",
      "[Epoch: 35460] train loss: 0.0577, train acc: 0.9816, val loss: 0.1808, val acc: 0.9676  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0539  @ epoch 34595 )\n",
      "[Epoch: 35480] train loss: 0.0669, train acc: 0.9766, val loss: 0.1796, val acc: 0.9676  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0539  @ epoch 34595 )\n",
      "[Epoch: 35500] train loss: 0.0792, train acc: 0.9685, val loss: 0.1779, val acc: 0.9636  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0539  @ epoch 34595 )\n",
      "[Epoch: 35520] train loss: 0.0613, train acc: 0.9786, val loss: 0.2104, val acc: 0.9649  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0539  @ epoch 34595 )\n",
      "[Epoch: 35540] train loss: 0.0641, train acc: 0.9785, val loss: 0.1866, val acc: 0.9653  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0539  @ epoch 34595 )\n",
      "[Epoch: 35560] train loss: 0.0565, train acc: 0.9821, val loss: 0.1729, val acc: 0.9656  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0539  @ epoch 34595 )\n",
      "[Epoch: 35580] train loss: 0.0720, train acc: 0.9731, val loss: 0.1600, val acc: 0.9680  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0539  @ epoch 34595 )\n",
      "[Epoch: 35600] train loss: 0.0741, train acc: 0.9746, val loss: 0.2202, val acc: 0.9497  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0539  @ epoch 34595 )\n",
      "[Epoch: 35620] train loss: 0.1021, train acc: 0.9623, val loss: 0.1896, val acc: 0.9521  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0539  @ epoch 34595 )\n",
      "[Epoch: 35640] train loss: 0.0673, train acc: 0.9772, val loss: 0.1509, val acc: 0.9663  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0539  @ epoch 34595 )\n",
      "[Epoch: 35660] train loss: 0.0614, train acc: 0.9785, val loss: 0.1782, val acc: 0.9666  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0539  @ epoch 34595 )\n",
      "[Epoch: 35680] train loss: 0.0575, train acc: 0.9807, val loss: 0.1812, val acc: 0.9653  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0539  @ epoch 34595 )\n",
      "[Epoch: 35700] train loss: 0.0575, train acc: 0.9799, val loss: 0.1737, val acc: 0.9683  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0539  @ epoch 34595 )\n",
      "[Epoch: 35720] train loss: 0.0610, train acc: 0.9794, val loss: 0.1739, val acc: 0.9666  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0537  @ epoch 35711 )\n",
      "[Epoch: 35740] train loss: 0.0625, train acc: 0.9797, val loss: 0.1864, val acc: 0.9659  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0537  @ epoch 35711 )\n",
      "[Epoch: 35760] train loss: 0.0603, train acc: 0.9793, val loss: 0.1775, val acc: 0.9669  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0537  @ epoch 35711 )\n",
      "[Epoch: 35780] train loss: 0.0770, train acc: 0.9756, val loss: 0.2003, val acc: 0.9642  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0533  @ epoch 35770 )\n",
      "[Epoch: 35800] train loss: 0.0790, train acc: 0.9727, val loss: 0.1579, val acc: 0.9673  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0533  @ epoch 35770 )\n",
      "[Epoch: 35820] train loss: 0.0603, train acc: 0.9805, val loss: 0.2003, val acc: 0.9666  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0533  @ epoch 35770 )\n",
      "[Epoch: 35840] train loss: 0.0618, train acc: 0.9800, val loss: 0.1838, val acc: 0.9683  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0533  @ epoch 35770 )\n",
      "[Epoch: 35860] train loss: 0.0744, train acc: 0.9763, val loss: 0.2056, val acc: 0.9639  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0533  @ epoch 35770 )\n",
      "[Epoch: 35880] train loss: 0.0753, train acc: 0.9733, val loss: 0.1823, val acc: 0.9646  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0533  @ epoch 35770 )\n",
      "[Epoch: 35900] train loss: 0.0673, train acc: 0.9755, val loss: 0.1612, val acc: 0.9656  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0533  @ epoch 35770 )\n",
      "[Epoch: 35920] train loss: 0.0857, train acc: 0.9721, val loss: 0.1879, val acc: 0.9639  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0533  @ epoch 35770 )\n",
      "[Epoch: 35940] train loss: 0.0593, train acc: 0.9812, val loss: 0.1881, val acc: 0.9659  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0533  @ epoch 35770 )\n",
      "[Epoch: 35960] train loss: 0.0642, train acc: 0.9780, val loss: 0.1811, val acc: 0.9686  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0533  @ epoch 35770 )\n",
      "[Epoch: 35980] train loss: 0.0660, train acc: 0.9785, val loss: 0.1505, val acc: 0.9663  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0533  @ epoch 35770 )\n",
      "[Epoch: 36000] train loss: 0.0586, train acc: 0.9802, val loss: 0.1928, val acc: 0.9663  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0533  @ epoch 35770 )\n",
      "[Epoch: 36020] train loss: 0.0694, train acc: 0.9783, val loss: 0.1993, val acc: 0.9659  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0533  @ epoch 35770 )\n",
      "[Epoch: 36040] train loss: 0.0596, train acc: 0.9809, val loss: 0.1782, val acc: 0.9673  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0533  @ epoch 35770 )\n",
      "[Epoch: 36060] train loss: 0.0642, train acc: 0.9784, val loss: 0.1982, val acc: 0.9636  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0533  @ epoch 35770 )\n",
      "[Epoch: 36080] train loss: 0.0611, train acc: 0.9791, val loss: 0.1780, val acc: 0.9669  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0533  @ epoch 35770 )\n",
      "[Epoch: 36100] train loss: 0.0920, train acc: 0.9717, val loss: 0.1888, val acc: 0.9619  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0533  @ epoch 35770 )\n",
      "[Epoch: 36120] train loss: 0.0622, train acc: 0.9805, val loss: 0.2008, val acc: 0.9669  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0533  @ epoch 35770 )\n",
      "[Epoch: 36140] train loss: 0.0641, train acc: 0.9787, val loss: 0.1894, val acc: 0.9669  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0533  @ epoch 35770 )\n",
      "[Epoch: 36160] train loss: 0.0662, train acc: 0.9793, val loss: 0.1658, val acc: 0.9663  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0533  @ epoch 35770 )\n",
      "[Epoch: 36180] train loss: 0.0604, train acc: 0.9803, val loss: 0.1891, val acc: 0.9669  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0533  @ epoch 35770 )\n",
      "[Epoch: 36200] train loss: 0.0780, train acc: 0.9701, val loss: 0.1847, val acc: 0.9666  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0533  @ epoch 35770 )\n",
      "[Epoch: 36220] train loss: 0.0620, train acc: 0.9787, val loss: 0.1767, val acc: 0.9693  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0533  @ epoch 35770 )\n",
      "[Epoch: 36240] train loss: 0.0568, train acc: 0.9820, val loss: 0.1963, val acc: 0.9656  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0533  @ epoch 35770 )\n",
      "[Epoch: 36260] train loss: 0.0642, train acc: 0.9795, val loss: 0.1742, val acc: 0.9683  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0533  @ epoch 35770 )\n",
      "[Epoch: 36280] train loss: 0.0632, train acc: 0.9773, val loss: 0.1780, val acc: 0.9669  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0533  @ epoch 35770 )\n",
      "[Epoch: 36300] train loss: 0.0613, train acc: 0.9791, val loss: 0.1676, val acc: 0.9676  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0533  @ epoch 35770 )\n",
      "[Epoch: 36320] train loss: 0.1022, train acc: 0.9675, val loss: 0.2296, val acc: 0.9589  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0533  @ epoch 35770 )\n",
      "[Epoch: 36340] train loss: 0.0809, train acc: 0.9716, val loss: 0.2036, val acc: 0.9592  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0533  @ epoch 35770 )\n",
      "[Epoch: 36360] train loss: 0.0681, train acc: 0.9767, val loss: 0.1475, val acc: 0.9653  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0533  @ epoch 35770 )\n",
      "[Epoch: 36380] train loss: 0.1117, train acc: 0.9598, val loss: 0.1633, val acc: 0.9656  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0533  @ epoch 35770 )\n",
      "[Epoch: 36400] train loss: 0.0622, train acc: 0.9787, val loss: 0.1721, val acc: 0.9663  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0533  @ epoch 35770 )\n",
      "[Epoch: 36420] train loss: 0.0680, train acc: 0.9775, val loss: 0.1639, val acc: 0.9649  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0533  @ epoch 35770 )\n",
      "[Epoch: 36440] train loss: 0.0761, train acc: 0.9746, val loss: 0.1801, val acc: 0.9619  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0533  @ epoch 35770 )\n",
      "[Epoch: 36460] train loss: 0.0600, train acc: 0.9800, val loss: 0.1676, val acc: 0.9676  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0533  @ epoch 35770 )\n",
      "[Epoch: 36480] train loss: 0.0640, train acc: 0.9775, val loss: 0.1826, val acc: 0.9619  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0517  @ epoch 36462 )\n",
      "[Epoch: 36500] train loss: 0.0585, train acc: 0.9798, val loss: 0.1638, val acc: 0.9656  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0517  @ epoch 36462 )\n",
      "[Epoch: 36520] train loss: 0.0568, train acc: 0.9813, val loss: 0.1773, val acc: 0.9669  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0517  @ epoch 36462 )\n",
      "[Epoch: 36540] train loss: 0.0610, train acc: 0.9806, val loss: 0.1630, val acc: 0.9686  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0517  @ epoch 36462 )\n",
      "[Epoch: 36560] train loss: 0.0618, train acc: 0.9798, val loss: 0.1731, val acc: 0.9666  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0517  @ epoch 36462 )\n",
      "[Epoch: 36580] train loss: 0.1218, train acc: 0.9534, val loss: 0.1437, val acc: 0.9632  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0517  @ epoch 36462 )\n",
      "[Epoch: 36600] train loss: 0.0659, train acc: 0.9788, val loss: 0.1567, val acc: 0.9683  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0517  @ epoch 36462 )\n",
      "[Epoch: 36620] train loss: 0.0636, train acc: 0.9791, val loss: 0.1608, val acc: 0.9666  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0517  @ epoch 36462 )\n",
      "[Epoch: 36640] train loss: 0.0620, train acc: 0.9793, val loss: 0.1742, val acc: 0.9653  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0517  @ epoch 36462 )\n",
      "[Epoch: 36660] train loss: 0.0776, train acc: 0.9722, val loss: 0.1578, val acc: 0.9669  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0517  @ epoch 36462 )\n",
      "[Epoch: 36680] train loss: 0.0585, train acc: 0.9814, val loss: 0.1727, val acc: 0.9666  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0517  @ epoch 36462 )\n",
      "[Epoch: 36700] train loss: 0.0663, train acc: 0.9790, val loss: 0.1629, val acc: 0.9666  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0517  @ epoch 36462 )\n",
      "[Epoch: 36720] train loss: 0.0681, train acc: 0.9761, val loss: 0.1745, val acc: 0.9683  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0517  @ epoch 36462 )\n",
      "[Epoch: 36740] train loss: 0.1026, train acc: 0.9594, val loss: 0.1722, val acc: 0.9592  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0517  @ epoch 36462 )\n",
      "[Epoch: 36760] train loss: 0.0699, train acc: 0.9738, val loss: 0.1681, val acc: 0.9676  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0517  @ epoch 36462 )\n",
      "[Epoch: 36780] train loss: 0.0781, train acc: 0.9705, val loss: 0.1647, val acc: 0.9653  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0517  @ epoch 36462 )\n",
      "[Epoch: 36800] train loss: 0.0716, train acc: 0.9724, val loss: 0.1584, val acc: 0.9656  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0517  @ epoch 36462 )\n",
      "[Epoch: 36820] train loss: 0.0638, train acc: 0.9809, val loss: 0.1557, val acc: 0.9663  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0517  @ epoch 36462 )\n",
      "[Epoch: 36840] train loss: 0.0643, train acc: 0.9779, val loss: 0.1625, val acc: 0.9669  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0517  @ epoch 36462 )\n",
      "[Epoch: 36860] train loss: 0.0614, train acc: 0.9797, val loss: 0.1646, val acc: 0.9656  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0517  @ epoch 36462 )\n",
      "[Epoch: 36880] train loss: 0.0606, train acc: 0.9802, val loss: 0.1752, val acc: 0.9663  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0517  @ epoch 36462 )\n",
      "[Epoch: 36900] train loss: 0.0634, train acc: 0.9785, val loss: 0.1763, val acc: 0.9669  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0517  @ epoch 36462 )\n",
      "[Epoch: 36920] train loss: 0.0567, train acc: 0.9832, val loss: 0.1500, val acc: 0.9663  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0517  @ epoch 36462 )\n",
      "[Epoch: 36940] train loss: 0.0588, train acc: 0.9804, val loss: 0.1921, val acc: 0.9669  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0517  @ epoch 36462 )\n",
      "[Epoch: 36960] train loss: 0.0584, train acc: 0.9803, val loss: 0.1666, val acc: 0.9666  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0517  @ epoch 36462 )\n",
      "[Epoch: 36980] train loss: 0.0648, train acc: 0.9778, val loss: 0.1713, val acc: 0.9646  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0517  @ epoch 36462 )\n",
      "[Epoch: 37000] train loss: 0.0616, train acc: 0.9800, val loss: 0.1622, val acc: 0.9663  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0517  @ epoch 36462 )\n",
      "[Epoch: 37020] train loss: 0.0830, train acc: 0.9699, val loss: 0.1492, val acc: 0.9649  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0517  @ epoch 36462 )\n",
      "[Epoch: 37040] train loss: 0.0599, train acc: 0.9811, val loss: 0.1710, val acc: 0.9676  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0517  @ epoch 36462 )\n",
      "[Epoch: 37060] train loss: 0.0606, train acc: 0.9789, val loss: 0.1763, val acc: 0.9656  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0517  @ epoch 36462 )\n",
      "[Epoch: 37080] train loss: 0.0576, train acc: 0.9806, val loss: 0.1765, val acc: 0.9656  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0517  @ epoch 36462 )\n",
      "[Epoch: 37100] train loss: 0.0726, train acc: 0.9740, val loss: 0.1676, val acc: 0.9686  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0517  @ epoch 36462 )\n",
      "[Epoch: 37120] train loss: 0.0596, train acc: 0.9805, val loss: 0.1730, val acc: 0.9703  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0517  @ epoch 36462 )\n",
      "[Epoch: 37140] train loss: 0.0740, train acc: 0.9725, val loss: 0.1745, val acc: 0.9669  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0517  @ epoch 37129 )\n",
      "[Epoch: 37160] train loss: 0.0661, train acc: 0.9778, val loss: 0.1725, val acc: 0.9656  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0517  @ epoch 37129 )\n",
      "[Epoch: 37180] train loss: 0.0987, train acc: 0.9690, val loss: 0.1597, val acc: 0.9572  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0517  @ epoch 37129 )\n",
      "[Epoch: 37200] train loss: 0.0629, train acc: 0.9782, val loss: 0.1716, val acc: 0.9659  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0517  @ epoch 37129 )\n",
      "[Epoch: 37220] train loss: 0.0605, train acc: 0.9802, val loss: 0.1662, val acc: 0.9669  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0517  @ epoch 37129 )\n",
      "[Epoch: 37240] train loss: 0.0722, train acc: 0.9759, val loss: 0.1698, val acc: 0.9669  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0517  @ epoch 37129 )\n",
      "[Epoch: 37260] train loss: 0.0552, train acc: 0.9839, val loss: 0.1800, val acc: 0.9659  (best train acc: 0.9839, best val acc: 0.9713, best train loss: 0.0517  @ epoch 37129 )\n",
      "[Epoch: 37280] train loss: 0.0694, train acc: 0.9758, val loss: 0.1936, val acc: 0.9605  (best train acc: 0.9839, best val acc: 0.9713, best train loss: 0.0517  @ epoch 37129 )\n",
      "[Epoch: 37300] train loss: 0.0954, train acc: 0.9698, val loss: 0.1537, val acc: 0.9653  (best train acc: 0.9839, best val acc: 0.9713, best train loss: 0.0517  @ epoch 37129 )\n",
      "[Epoch: 37320] train loss: 0.0641, train acc: 0.9782, val loss: 0.1509, val acc: 0.9683  (best train acc: 0.9839, best val acc: 0.9713, best train loss: 0.0517  @ epoch 37129 )\n",
      "[Epoch: 37340] train loss: 0.0586, train acc: 0.9807, val loss: 0.1658, val acc: 0.9683  (best train acc: 0.9839, best val acc: 0.9713, best train loss: 0.0517  @ epoch 37129 )\n",
      "[Epoch: 37360] train loss: 0.0573, train acc: 0.9813, val loss: 0.1817, val acc: 0.9659  (best train acc: 0.9839, best val acc: 0.9713, best train loss: 0.0517  @ epoch 37129 )\n",
      "[Epoch: 37380] train loss: 0.0646, train acc: 0.9780, val loss: 0.1796, val acc: 0.9629  (best train acc: 0.9839, best val acc: 0.9713, best train loss: 0.0517  @ epoch 37129 )\n",
      "[Epoch: 37400] train loss: 0.0789, train acc: 0.9787, val loss: 0.1700, val acc: 0.9686  (best train acc: 0.9839, best val acc: 0.9713, best train loss: 0.0517  @ epoch 37129 )\n",
      "[Epoch: 37420] train loss: 0.0671, train acc: 0.9768, val loss: 0.1793, val acc: 0.9659  (best train acc: 0.9839, best val acc: 0.9713, best train loss: 0.0517  @ epoch 37129 )\n",
      "[Epoch: 37440] train loss: 0.1112, train acc: 0.9568, val loss: 0.1530, val acc: 0.9487  (best train acc: 0.9839, best val acc: 0.9713, best train loss: 0.0517  @ epoch 37129 )\n",
      "[Epoch: 37460] train loss: 0.0780, train acc: 0.9698, val loss: 0.1600, val acc: 0.9690  (best train acc: 0.9839, best val acc: 0.9713, best train loss: 0.0517  @ epoch 37129 )\n",
      "[Epoch: 37480] train loss: 0.0654, train acc: 0.9772, val loss: 0.1690, val acc: 0.9656  (best train acc: 0.9839, best val acc: 0.9713, best train loss: 0.0517  @ epoch 37129 )\n",
      "[Epoch: 37500] train loss: 0.0567, train acc: 0.9813, val loss: 0.1753, val acc: 0.9663  (best train acc: 0.9839, best val acc: 0.9713, best train loss: 0.0517  @ epoch 37129 )\n",
      "[Epoch: 37520] train loss: 0.1117, train acc: 0.9592, val loss: 0.1729, val acc: 0.9589  (best train acc: 0.9839, best val acc: 0.9713, best train loss: 0.0517  @ epoch 37129 )\n",
      "[Epoch: 37540] train loss: 0.0657, train acc: 0.9770, val loss: 0.1533, val acc: 0.9642  (best train acc: 0.9839, best val acc: 0.9713, best train loss: 0.0517  @ epoch 37129 )\n",
      "[Epoch: 37560] train loss: 0.0626, train acc: 0.9787, val loss: 0.1646, val acc: 0.9666  (best train acc: 0.9839, best val acc: 0.9713, best train loss: 0.0517  @ epoch 37129 )\n",
      "[Epoch: 37580] train loss: 0.0528, train acc: 0.9831, val loss: 0.1711, val acc: 0.9676  (best train acc: 0.9844, best val acc: 0.9713, best train loss: 0.0516  @ epoch 37579 )\n",
      "[Epoch: 37600] train loss: 0.0623, train acc: 0.9807, val loss: 0.1856, val acc: 0.9656  (best train acc: 0.9844, best val acc: 0.9713, best train loss: 0.0516  @ epoch 37579 )\n",
      "[Epoch: 37620] train loss: 0.0676, train acc: 0.9756, val loss: 0.1520, val acc: 0.9673  (best train acc: 0.9844, best val acc: 0.9713, best train loss: 0.0516  @ epoch 37579 )\n",
      "[Epoch: 37640] train loss: 0.0532, train acc: 0.9832, val loss: 0.1511, val acc: 0.9676  (best train acc: 0.9844, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 37660] train loss: 0.0596, train acc: 0.9796, val loss: 0.1716, val acc: 0.9676  (best train acc: 0.9844, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 37680] train loss: 0.0608, train acc: 0.9813, val loss: 0.1747, val acc: 0.9676  (best train acc: 0.9844, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 37700] train loss: 0.0657, train acc: 0.9815, val loss: 0.1697, val acc: 0.9649  (best train acc: 0.9844, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 37720] train loss: 0.0556, train acc: 0.9815, val loss: 0.1674, val acc: 0.9649  (best train acc: 0.9844, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 37740] train loss: 0.0753, train acc: 0.9764, val loss: 0.1556, val acc: 0.9653  (best train acc: 0.9844, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 37760] train loss: 0.0712, train acc: 0.9763, val loss: 0.1805, val acc: 0.9690  (best train acc: 0.9844, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 37780] train loss: 0.0872, train acc: 0.9719, val loss: 0.1500, val acc: 0.9636  (best train acc: 0.9844, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 37800] train loss: 0.0736, train acc: 0.9766, val loss: 0.1688, val acc: 0.9653  (best train acc: 0.9844, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 37820] train loss: 0.0887, train acc: 0.9701, val loss: 0.1493, val acc: 0.9686  (best train acc: 0.9844, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 37840] train loss: 0.0707, train acc: 0.9793, val loss: 0.1693, val acc: 0.9653  (best train acc: 0.9844, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 37860] train loss: 0.0567, train acc: 0.9833, val loss: 0.1627, val acc: 0.9686  (best train acc: 0.9844, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 37880] train loss: 0.0637, train acc: 0.9803, val loss: 0.1656, val acc: 0.9666  (best train acc: 0.9844, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 37900] train loss: 0.0583, train acc: 0.9807, val loss: 0.1574, val acc: 0.9666  (best train acc: 0.9844, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 37920] train loss: 0.0598, train acc: 0.9804, val loss: 0.1610, val acc: 0.9669  (best train acc: 0.9844, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 37940] train loss: 0.0604, train acc: 0.9801, val loss: 0.1949, val acc: 0.9663  (best train acc: 0.9844, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 37960] train loss: 0.0740, train acc: 0.9735, val loss: 0.1975, val acc: 0.9589  (best train acc: 0.9844, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 37980] train loss: 0.0623, train acc: 0.9795, val loss: 0.1791, val acc: 0.9676  (best train acc: 0.9844, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 38000] train loss: 0.0673, train acc: 0.9754, val loss: 0.1966, val acc: 0.9686  (best train acc: 0.9844, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 38020] train loss: 0.0618, train acc: 0.9813, val loss: 0.1672, val acc: 0.9663  (best train acc: 0.9844, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 38040] train loss: 0.0624, train acc: 0.9790, val loss: 0.1694, val acc: 0.9622  (best train acc: 0.9844, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 38060] train loss: 0.0573, train acc: 0.9803, val loss: 0.1711, val acc: 0.9666  (best train acc: 0.9844, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 38080] train loss: 0.0562, train acc: 0.9816, val loss: 0.1576, val acc: 0.9686  (best train acc: 0.9844, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 38100] train loss: 0.0766, train acc: 0.9731, val loss: 0.1670, val acc: 0.9669  (best train acc: 0.9844, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 38120] train loss: 0.0599, train acc: 0.9816, val loss: 0.1879, val acc: 0.9673  (best train acc: 0.9844, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 38140] train loss: 0.0596, train acc: 0.9798, val loss: 0.1836, val acc: 0.9639  (best train acc: 0.9844, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 38160] train loss: 0.0563, train acc: 0.9806, val loss: 0.1682, val acc: 0.9683  (best train acc: 0.9844, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 38180] train loss: 0.0690, train acc: 0.9767, val loss: 0.1802, val acc: 0.9663  (best train acc: 0.9844, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 38200] train loss: 0.0671, train acc: 0.9741, val loss: 0.1598, val acc: 0.9680  (best train acc: 0.9844, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 38220] train loss: 0.1014, train acc: 0.9665, val loss: 0.1754, val acc: 0.9595  (best train acc: 0.9844, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 38240] train loss: 0.1007, train acc: 0.9705, val loss: 0.2008, val acc: 0.9602  (best train acc: 0.9844, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 38260] train loss: 0.0613, train acc: 0.9803, val loss: 0.1697, val acc: 0.9693  (best train acc: 0.9844, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 38280] train loss: 0.0628, train acc: 0.9786, val loss: 0.1698, val acc: 0.9700  (best train acc: 0.9844, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 38300] train loss: 0.0548, train acc: 0.9829, val loss: 0.1692, val acc: 0.9666  (best train acc: 0.9844, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 38320] train loss: 0.0593, train acc: 0.9811, val loss: 0.1846, val acc: 0.9663  (best train acc: 0.9844, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 38340] train loss: 0.0656, train acc: 0.9782, val loss: 0.1738, val acc: 0.9676  (best train acc: 0.9847, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 38360] train loss: 0.0610, train acc: 0.9789, val loss: 0.1914, val acc: 0.9646  (best train acc: 0.9847, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 38380] train loss: 0.0742, train acc: 0.9719, val loss: 0.1500, val acc: 0.9663  (best train acc: 0.9847, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 38400] train loss: 0.0666, train acc: 0.9768, val loss: 0.1819, val acc: 0.9673  (best train acc: 0.9847, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 38420] train loss: 0.0648, train acc: 0.9771, val loss: 0.1761, val acc: 0.9683  (best train acc: 0.9847, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 38440] train loss: 0.0557, train acc: 0.9821, val loss: 0.1871, val acc: 0.9663  (best train acc: 0.9847, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 38460] train loss: 0.0542, train acc: 0.9814, val loss: 0.1609, val acc: 0.9646  (best train acc: 0.9847, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 38480] train loss: 0.0583, train acc: 0.9800, val loss: 0.1726, val acc: 0.9669  (best train acc: 0.9847, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 38500] train loss: 0.0639, train acc: 0.9786, val loss: 0.1766, val acc: 0.9669  (best train acc: 0.9847, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 38520] train loss: 0.0803, train acc: 0.9672, val loss: 0.1909, val acc: 0.9551  (best train acc: 0.9847, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 38540] train loss: 0.0854, train acc: 0.9675, val loss: 0.1871, val acc: 0.9605  (best train acc: 0.9847, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 38560] train loss: 0.0591, train acc: 0.9805, val loss: 0.1732, val acc: 0.9669  (best train acc: 0.9847, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 38580] train loss: 0.0637, train acc: 0.9800, val loss: 0.1331, val acc: 0.9669  (best train acc: 0.9847, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 38600] train loss: 0.0602, train acc: 0.9797, val loss: 0.1570, val acc: 0.9666  (best train acc: 0.9847, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 38620] train loss: 0.0937, train acc: 0.9670, val loss: 0.1501, val acc: 0.9636  (best train acc: 0.9847, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 38640] train loss: 0.0886, train acc: 0.9647, val loss: 0.1796, val acc: 0.9494  (best train acc: 0.9847, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 38660] train loss: 0.0782, train acc: 0.9730, val loss: 0.1678, val acc: 0.9616  (best train acc: 0.9847, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 38680] train loss: 0.0633, train acc: 0.9777, val loss: 0.1473, val acc: 0.9666  (best train acc: 0.9847, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 38700] train loss: 0.0969, train acc: 0.9699, val loss: 0.1579, val acc: 0.9649  (best train acc: 0.9847, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 38720] train loss: 0.0732, train acc: 0.9754, val loss: 0.1552, val acc: 0.9642  (best train acc: 0.9847, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 38740] train loss: 0.0625, train acc: 0.9801, val loss: 0.1409, val acc: 0.9666  (best train acc: 0.9847, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 38760] train loss: 0.0649, train acc: 0.9785, val loss: 0.1707, val acc: 0.9646  (best train acc: 0.9847, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 38780] train loss: 0.0645, train acc: 0.9786, val loss: 0.1689, val acc: 0.9663  (best train acc: 0.9847, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 38800] train loss: 0.0599, train acc: 0.9793, val loss: 0.1737, val acc: 0.9669  (best train acc: 0.9847, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 38820] train loss: 0.0582, train acc: 0.9803, val loss: 0.1499, val acc: 0.9663  (best train acc: 0.9847, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 38840] train loss: 0.0645, train acc: 0.9770, val loss: 0.1628, val acc: 0.9663  (best train acc: 0.9847, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 38860] train loss: 0.0670, train acc: 0.9776, val loss: 0.1783, val acc: 0.9659  (best train acc: 0.9847, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 38880] train loss: 0.0632, train acc: 0.9789, val loss: 0.1664, val acc: 0.9666  (best train acc: 0.9847, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 38900] train loss: 0.0718, train acc: 0.9734, val loss: 0.1941, val acc: 0.9619  (best train acc: 0.9847, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 38920] train loss: 0.0660, train acc: 0.9783, val loss: 0.1852, val acc: 0.9646  (best train acc: 0.9847, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 38940] train loss: 0.0690, train acc: 0.9749, val loss: 0.1622, val acc: 0.9673  (best train acc: 0.9847, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 38960] train loss: 0.0756, train acc: 0.9753, val loss: 0.1724, val acc: 0.9629  (best train acc: 0.9847, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 38980] train loss: 0.0685, train acc: 0.9773, val loss: 0.1707, val acc: 0.9656  (best train acc: 0.9847, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 39000] train loss: 0.0607, train acc: 0.9798, val loss: 0.1650, val acc: 0.9676  (best train acc: 0.9847, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 39020] train loss: 0.0584, train acc: 0.9814, val loss: 0.1756, val acc: 0.9683  (best train acc: 0.9847, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 39040] train loss: 0.0661, train acc: 0.9771, val loss: 0.1775, val acc: 0.9669  (best train acc: 0.9847, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 39060] train loss: 0.0649, train acc: 0.9764, val loss: 0.1513, val acc: 0.9669  (best train acc: 0.9847, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 39080] train loss: 0.0637, train acc: 0.9777, val loss: 0.1623, val acc: 0.9673  (best train acc: 0.9847, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 39100] train loss: 0.0571, train acc: 0.9810, val loss: 0.1874, val acc: 0.9663  (best train acc: 0.9847, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 39120] train loss: 0.0737, train acc: 0.9743, val loss: 0.1836, val acc: 0.9680  (best train acc: 0.9847, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 39140] train loss: 0.0584, train acc: 0.9819, val loss: 0.1792, val acc: 0.9676  (best train acc: 0.9847, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 39160] train loss: 0.0619, train acc: 0.9780, val loss: 0.1894, val acc: 0.9666  (best train acc: 0.9847, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 39180] train loss: 0.0637, train acc: 0.9796, val loss: 0.1884, val acc: 0.9676  (best train acc: 0.9847, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 39200] train loss: 0.0696, train acc: 0.9757, val loss: 0.1890, val acc: 0.9686  (best train acc: 0.9847, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 39220] train loss: 0.0620, train acc: 0.9798, val loss: 0.1820, val acc: 0.9683  (best train acc: 0.9847, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 39240] train loss: 0.0547, train acc: 0.9818, val loss: 0.1616, val acc: 0.9663  (best train acc: 0.9847, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 39260] train loss: 0.0564, train acc: 0.9804, val loss: 0.1714, val acc: 0.9669  (best train acc: 0.9847, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 39280] train loss: 0.0651, train acc: 0.9781, val loss: 0.1801, val acc: 0.9649  (best train acc: 0.9847, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 39300] train loss: 0.0578, train acc: 0.9829, val loss: 0.1756, val acc: 0.9676  (best train acc: 0.9847, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 39320] train loss: 0.0952, train acc: 0.9665, val loss: 0.1635, val acc: 0.9622  (best train acc: 0.9847, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 39340] train loss: 0.0724, train acc: 0.9768, val loss: 0.1854, val acc: 0.9686  (best train acc: 0.9847, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 39360] train loss: 0.0603, train acc: 0.9803, val loss: 0.1603, val acc: 0.9673  (best train acc: 0.9847, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 39380] train loss: 0.0596, train acc: 0.9808, val loss: 0.1677, val acc: 0.9659  (best train acc: 0.9847, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 39400] train loss: 0.0781, train acc: 0.9714, val loss: 0.1679, val acc: 0.9612  (best train acc: 0.9847, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 39420] train loss: 0.0586, train acc: 0.9821, val loss: 0.1655, val acc: 0.9642  (best train acc: 0.9847, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 39440] train loss: 0.0544, train acc: 0.9819, val loss: 0.1741, val acc: 0.9659  (best train acc: 0.9847, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 39460] train loss: 0.0668, train acc: 0.9765, val loss: 0.1584, val acc: 0.9673  (best train acc: 0.9847, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 39480] train loss: 0.0748, train acc: 0.9726, val loss: 0.1565, val acc: 0.9683  (best train acc: 0.9847, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 39500] train loss: 0.0579, train acc: 0.9806, val loss: 0.1724, val acc: 0.9669  (best train acc: 0.9847, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 39520] train loss: 0.0610, train acc: 0.9806, val loss: 0.1593, val acc: 0.9669  (best train acc: 0.9847, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 39540] train loss: 0.0553, train acc: 0.9821, val loss: 0.1782, val acc: 0.9663  (best train acc: 0.9847, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 39560] train loss: 0.0543, train acc: 0.9824, val loss: 0.1874, val acc: 0.9680  (best train acc: 0.9847, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 39580] train loss: 0.0704, train acc: 0.9744, val loss: 0.1767, val acc: 0.9676  (best train acc: 0.9847, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 39600] train loss: 0.0533, train acc: 0.9830, val loss: 0.1855, val acc: 0.9673  (best train acc: 0.9847, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 39620] train loss: 0.0554, train acc: 0.9812, val loss: 0.1745, val acc: 0.9649  (best train acc: 0.9847, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 39640] train loss: 0.0588, train acc: 0.9800, val loss: 0.1869, val acc: 0.9673  (best train acc: 0.9847, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 39660] train loss: 0.0641, train acc: 0.9767, val loss: 0.1584, val acc: 0.9680  (best train acc: 0.9847, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 39680] train loss: 0.0552, train acc: 0.9825, val loss: 0.1915, val acc: 0.9663  (best train acc: 0.9847, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 39700] train loss: 0.0664, train acc: 0.9774, val loss: 0.1805, val acc: 0.9680  (best train acc: 0.9847, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 39720] train loss: 0.0735, train acc: 0.9733, val loss: 0.1658, val acc: 0.9680  (best train acc: 0.9847, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 39740] train loss: 0.1226, train acc: 0.9574, val loss: 0.1584, val acc: 0.9501  (best train acc: 0.9847, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 39760] train loss: 0.0762, train acc: 0.9739, val loss: 0.1686, val acc: 0.9663  (best train acc: 0.9847, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 39780] train loss: 0.0833, train acc: 0.9738, val loss: 0.1502, val acc: 0.9656  (best train acc: 0.9847, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 39800] train loss: 0.0567, train acc: 0.9826, val loss: 0.1997, val acc: 0.9663  (best train acc: 0.9847, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 39820] train loss: 0.0595, train acc: 0.9799, val loss: 0.1941, val acc: 0.9676  (best train acc: 0.9847, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 39840] train loss: 0.0576, train acc: 0.9814, val loss: 0.1777, val acc: 0.9659  (best train acc: 0.9847, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 39860] train loss: 0.0657, train acc: 0.9777, val loss: 0.1935, val acc: 0.9646  (best train acc: 0.9847, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 39880] train loss: 0.0653, train acc: 0.9756, val loss: 0.1712, val acc: 0.9696  (best train acc: 0.9847, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 39900] train loss: 0.0552, train acc: 0.9816, val loss: 0.1681, val acc: 0.9693  (best train acc: 0.9847, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 39920] train loss: 0.0678, train acc: 0.9765, val loss: 0.1645, val acc: 0.9669  (best train acc: 0.9847, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 39940] train loss: 0.0580, train acc: 0.9821, val loss: 0.1679, val acc: 0.9700  (best train acc: 0.9847, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 39960] train loss: 0.0618, train acc: 0.9782, val loss: 0.1729, val acc: 0.9673  (best train acc: 0.9847, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 39980] train loss: 0.0614, train acc: 0.9794, val loss: 0.1613, val acc: 0.9669  (best train acc: 0.9847, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 40000] train loss: 0.0870, train acc: 0.9718, val loss: 0.1495, val acc: 0.9666  (best train acc: 0.9847, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.93      0.93       154\n",
      "           1       0.98      0.98      0.98       703\n",
      "           2       0.95      0.96      0.96       702\n",
      "           3       0.97      0.95      0.96       703\n",
      "           4       0.99      1.00      0.99       702\n",
      "\n",
      "    accuracy                           0.97      2964\n",
      "   macro avg       0.96      0.96      0.96      2964\n",
      "weighted avg       0.97      0.97      0.97      2964\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaYAAAGQCAYAAADlUsSzAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAA5RUlEQVR4nO3dd5wU9f3H8dfn7jg4EFAUDiPwoxoFUaNYwIoVWxRFxdiiCEYQRayIXdFo1GiiBo5isEuiRjSWGGwoKGIDEY1EFI9epEi98vn9sQNZ8coiuzu7s+8nj3nc7uzMzud7e9znvp/5znfM3REREckUeWEHICIiEk+JSUREMooSk4iIZBQlJhERyShKTCIiklEKwg5ARES2zq/t+KQNrx7vL1qy3uvnUo9JREQyinpMIiJZLi9ifQwlJhGRLGcWevUtqaKVZkVEJOupxyQikuVUyhMRkYySp1KeiIhI6qjHJCKS5SxifQwlJhGRLKdSnoiISAqpxyQikuVUyhMRkYyiUp6IiEgKqcckIpLldIGtiIhkFM2VJyIikkLqMYmIZDmV8kREJKNoVJ5IFjOzm8zssbDjEJHqKTFJ6Myst5m9b2arzWxR8Li/hXRG18x+Y2ZTzewHM5tvZi+b2YFxr3c0s/FmtsLMVpnZG2bWLe711mbmZvbPzd73sSAx7mRm5WbWropjP2dmd6e2hRI1Rl7SlkyQGVFIzjKzy4H7gT8AzYFi4HfAAUBhFdvnpziewcB9wO1BLK2Ah4ATg9fbAe8C04E2wC+A54B/mVnXzd5ufzM7YPNjuPtcYAJw9mbHbgIcC4xNXoskF+RZXtKWTJAZUUhOMrPGwC1Af3f/u7uv8piP3f1Md19vZn81s7+Y2UtmthrobmbHmdnHZrbSzL4zs5vi3nNjb6Wfmc0LejyXb3boQjN7JOjtzDCzLpvFM8Ddn3X31e5e5u4vuPuVwb43AZPdfai7Lwti/hPwKHDnZse5C7itmuaPZbPEBPQGZrj79C35PopEjRKThKkrUBd4vpbtfgMMAxoC7wCrgXOAbYHjgIvM7KTN9ukOdACOAq4xsyPiXvs18FSw/3jggbh46hHrAVXnSOBvVawfBxxgZvXj1j0I7LzZsTd6DtghvkRILFE9UsOxRapkSfyXCZSYJEw7AEvcvXzjCjObZGbLzWytmR0crH7e3d9190p3X+fub7r79OD5NOBJ4JDN3vvmoMczHXgYOCPutXfc/SV3ryDW09kjWL/95vFUE/P8KtbPJ/b/abu4deuIJdSf9JrcfS2xBHdO0O4OwN7AEzUcW6RKKuWJJM9SYr2GTZctuHs3d982eG3jz+d38TuZ2X7BgIPFZraC2DmpHTZ77/h9viV2LmijBXGP1wD1ghh+Ek8VlgA7VrF+R6AS+H6z9SOBYjM7oYp9xgKnmVk9Yr2lV9x9UQ3HFskJSkwSpsnAeoKBBTXwzZ4/QawE19LdGwPD4Sc1iJZxj1sB8xKMZx1wUg3b/Bs4tYr1pxE797QmfqW7lwE3A7duHqO7TySWDE8EzkJlPPmZkjcmT6U8yXHuvpzYL+2HzKyXmW1jZnlmtifQoIZdGwLL3H2dme1L7BzU5q43s/pm1gk4D3g6gXhWADcAD5rZScH+dczsGDO7K9jsZqCbmQ0zsyZm1tDMBhIryV1dzVs/SuxcWo8qXnuE2KCJbYEXaotRpCoaLi6SRO5+FzAYuApYBCwERhD7JT+pmt36A7eY2SpiiWRcFdu8BcwiNiz7bnf/V4Lx3BvEcx2wmFhJ8GLgH8HrXwEHEjsv9Q2xc0unAEe7+7vVvGcFcCPQpIqXHyHWo3va3dcnEqNI1Jn75lUSkexlZq2B2UCdWgYxiETGwPoXJe0X+Z/X/CX0ep7myhMRyXKaxFVERDJK1O7HpMQkkeLu3/DTEXoikkWUmEREspxKeemjURkiEmVJ69lH7X5MmZyYmPrfJWGHEIou7XZgXUVl2GGkXb38vJxsN8Tavra8Iuww0q6oID+nP3OpWkYnJhERqV2mXBibLEpMIiJZLmqlvGilWRERyXrqMYmIZDmV8kREJKNkyn2UkiVarRERkaynHpOISJbLlPsoJYsSk4hIljOV8kRERFJHPSYRkSynUp6IiGQUjcoTERFJIfWYRESynKmUJyIiGSUvWolJpTwREckoSkwiItnOLHlLQoezbc3s72b2hZnNNLOuZtbEzF4zs6+Cr9vFbT/EzGaZ2ZdmdnRt76/EJCKS5SzPkrYk6H7gFXffBdgDmAlcA0xw9w7AhOA5ZtYR6A10AnoAD5lZfk1vrsQkIiIJM7NGwMHAaAB33+Duy4ETgbHBZmOBk4LHJwJPuft6d58NzAL2rekYSkwiItkuiaU8M+tnZlPjln6bHa0tsBh42Mw+NrNRZtYAKHb3+QDB12bB9jsB38XtXxqsq5ZG5YmIZLskjspz9xKgpIZNCoC9gIHu/r6Z3U9QtqtGVcF5TTGoxyQiIluiFCh19/eD538nlqgWmtmOAMHXRXHbt4zbvwUwr6YDKDGJiGS7PEveUgt3XwB8Z2a/DFYdDnwOjAfODdadCzwfPB4P9DazumbWBugATKnpGCrliYhkOUtwmHcSDQQeN7NC4GvgPGIdnXFm1geYA5wK4O4zzGwcseRVDgxw94qa3lyJSUREtoi7fwJ0qeKlw6vZfhgwLNH3V2ISEcl2EZuSKCcTU8kfb+fjKe/SaNvtuPMvj/3otX8+8wRPjH6Q4U/+k4aNt+W/X37OqD/fGXvR4eQzz2efboeEEHVqLZg/n6FDrmHpkiWYGb1OO40zzz4n7LBSLlfbDbBy5UpuueEGZs36CjPjpltvY4899ww7rJS7YehQ3n7rTZo0acKz418IO5zkSH8pL6VyMjEddMSxHHnCKQy/59YfrV+6eCHTP/6A7ZsWb1rX4v/actv9o8nPL+D7ZUu4dsC57LXfAeTnR+tbl1+QzxVXXcWuHTuxevVqevc6hf27dqNd+/Zhh5ZSudpugLvuuINuBx7I3ffdR9mGDaxdty7skNLixJ4nccaZv2HoNTWNcJYw5eSovF0778k2DRv9ZP2jJX/ijPP7/+hEYt169TYlobINGyL3l8lGTZs2Y9eOnQBo0KABbdu2Y9GihSFHlXq52u4ffviBjz6cSs9TTgGgTmEhjRr99P9EFO3dZR8aNd427DCSK42j8tIhWn/2b4UP35tIk+2b8n9tO/zktVlfzKDkvttZsmghF11xfeR6S5ubO3cuX8ycSefd9wg7lLTKpXaXfvcd223XhBuGDuU/X35Bx06duOqaIRTVrx92aPJz6A62tTOzemY2yMweMLMLzSyjf5OvX7eO5596hF5nX1Dl6+136cRdwx/n1vtGMX7co2zYsD7NEabPmtWrufzSS7hyyDVss802YYeTNrnW7oqKCr6Y+Tmn9T6dp595lnpFRYwZNSrssESA1JXyxhIbSjgdOAa4J5Gd4udoKimpaUaM5Fo4fy6LF85jyIBzufS3p7BsyWKGXnI+y5ct/dF2O7VqTd169Sj95uu0xZZOZWVlDB50KccefwJHHHlU2OGkTS62u7i4mGbFxZt6h0cedRQzZ34eclTyc4Uwu3hKpaon09HdOwOY2Whqucp3o83maPKp/12SovB+rFWbdvzlyX9uen7pb0/htvtH07DxtixaMI/tmzYjP7+AxQsXML90Dk2Ld0xLXOnk7tx0/XW0bduWc37727DDSZtcbfcOTZvSvHlzvpk9m9Zt2vD+e+/Rtl27sMOSnytDEkqypCoxlW184O7lIVyVXKMH7ryRmdM+ZtXK5Vx89kn0OqsPhx59QpXbfjljGi/87VHyCwrIszzO638FDaN24hT4+KOPeHH8eDrsvDOn9ewJwMBBgzjokOgNjY+Xq+0GuPraoVx79VWUlZWxU4sW3HJbwtc/ZrWrr7icqVOmsHz5co7sfigXXXwxJ5/SK+ywJI651zjJ6897U7MKYPXGp0ARsCZ47O6eyPCftPWYMk2XdjuwrqIy7DDSrl5+Xk62G2JtX1te4ywtkVRUkJ/Ln3nS/mIftvPdSftFPvQ/V4Tek0hJj8nda7w7oYiIJFHESnnRGmMoIiJZL6OHcYuISO0y7Tz+1lJiEhHJdirliYiIpI56TCIi2U6lPBERySgq5YmIiKSOekwiItkuYj0mJSYRkSwXteHiKuWJiEhGUY9JRCTbqZQnIiIZRaU8ERGR1FGPSUQk26mUJyIimSRqo/KUmEREsl3Eekw6xyQiIhlFPSYRkWwXsR6TEpOISLaL2DkmlfJERCSjqMckIpLtVMoTEZFMErXh4irliYhIRlGPSUQk26mUJyIiGUWlPBERkdTJ6B5Tl3Y7hB1CaOrl5+bfDLnaboCigvywQwhFLn/mSaNSXvqsLa8MO4RQFBXkcUpez7DDSLtnKp9jdVlF2GGEokGdfNZV5N7Pe738vJxsNyQ5IUcrL6mUJyIimSWje0wiIpKAiA1+UGISEclyFrFzTCrliYhIRlGPSUQk20Wrw6TEJCKS9SJ2jkmlPBERyShKTCIi2S7PkrckwMy+MbPpZvaJmU0N1jUxs9fM7Kvg63Zx2w8xs1lm9qWZHV1rc372N0JERDKDJXFJXHd339PduwTPrwEmuHsHYELwHDPrCPQGOgE9gIfMrMZpTpSYREQkGU4ExgaPxwInxa1/yt3Xu/tsYBawb01vpMQkIpLtzJK2mFk/M5sat/Sr4ogO/MvMPox7vdjd5wMEX5sF63cCvovbtzRYVy2NyhMRyXZJ7GK4ewlQUstmB7j7PDNrBrxmZl/UsG1VBUKv6c3VYxIRkS3i7vOCr4uA54iV5haa2Y4AwddFwealQMu43VsA82p6fyUmEZFsl8RSXu2HsgZm1nDjY+Ao4DNgPHBusNm5wPPB4/FAbzOra2ZtgA7AlJqOoVKeiEiWs/ReYFsMPBccswB4wt1fMbMPgHFm1geYA5wK4O4zzGwc8DlQDgxw9xrvb6PEJCIiCXP3r4E9qli/FDi8mn2GAcMSPYYSk4hItovWjERKTCIiWU+3vRAREUkd9ZhERLJdxGYXV2ISEcl20cpLKuWJiEhmUY9JRCTbRWzwgxKTiEi2i1ZeUilPREQyi3pMcdavX8/555xN2YYNlFeUc8RRR9P/4oFhh5VU9RvXp//IAbTarRXu8GCfB9iwdj0X/uV31KlXSEV5BSMHlDDrg68oqFPAhcN/R7su7fHKSsYMGs2Mt2aE3YStdtN1Q5n49ls0adKEv/1jPACvvfoKIx56kNlff82jTz5Nx912CznK1Ht34kTuvON2Kisq6dmrF3369g07pLSJXNsjNiovpT0mM9shle+fbIWFhYwc8zDjnvsHTz/zHJPeeYdpn34SdlhJdf59F/Dxqx9zSceBXL7nZZTO/I6z7zyXcbeM44q9BvP0jU9y9p3nAHBE3yMBGLzHIG4+6mbOvfu8dM/JlRInnNSTB4b/eFb/du07cPd9f2KvvbtUs1e0VFRUcPttt/LQiBKee+EFXnnpn/x31qyww0qLKLbd8ixpSyZISWIysxPMbDEw3cxKzaxbKo6TbGZG/QYNACgvL6e8vCwSv4g3KmpYRMeDOzJh9L8BKC8rZ82KNeBOUaMiINaj+n7eMgBadGzJ9NenA7By8QpWL19Nuy7twwk+ifbu0oXGjRv/aF3bdu1o3aZNSBGl32fTp9GyVStatGxJncJCehxzLG++/nrYYaVFLrc9W6SqxzQMOMjddwROAe5I0XGSrqKigtNO7slhBx3I/l270Xn3n8xVmLWK2xazcvFKLh4zkD98eA8XjexP3fp1GXPZGM6561xGfDuSc/7wWx6/9jEAvv10Nvv8el/y8vNo1roZ7fZuxw4ttw+5FZIMixYuonnz5pueN2tezMJFC0OMKH0i2XZL4pIBUpWYyt39CwB3fx9omMhO8bf0LSmp7QaKqZGfn8+4Z5/j1dff4LPp05n11X9CiSMV8gvyabtXW14d/gpX7n0561evp+c1J3P0RUfz18FjuPD/+vLXwWPoP2oAABPGTGDp3CXc9cHdnPfHPnw56QsqyitDboUkg/tPbyBqmfJbKcUi2fY03o8pHVI1+KGZmQ2u7rm731vVTpvd0tfXhvhLsFGjRnTZd1/efecd2nfYObQ4kmlp6VKWli7lqylfATD575PoefXJ7HLgroy5dDQAk/42iYtGxhJTZUUlfx388Kb9h71zB/O/qvHGk5IlipsXs2DBgk3PFy1YSLNmzUKMKH1yue3ZIlU9ppHEekkbl/jn26TomFtt2bJlrFy5EoB169bx/uTJtInQeYflC5ez5Lsl/GLnXwDQ+fDdKZ1ZyvfzvqfTIZ1i6w7rzPyv5gNQWFRI3fp1Adj9iD2oLK+gdGZpOMFLUnXarTNzvv2W0tJSyjZs4JWXX+KQ7t3DDistItn2PEvekgFS0mNy95ure83MBqXimMmwZPFirr92CJWVFVRWVnLU0T04+NAs/4HdzOhLRnLpY5dRp7CAhV8v5IHz/8yU56dw/n19yC/IY8O6MoZf+BAAjZs15vpXbsQrnWVzl/Knc+4POfrkGHLlFXz4wRSWL19Oj8O787v+F9OocWPuumMY3y9bxiX9L2LnXXbhoZKRYYeaMgUFBQwZeh0X9b2AyspKTup5Mu07dAg7rLSIZNszI58kjVVVb03pAc3muHurBDYNtZQXpqKCPE7J6xl2GGn3TOVzrC6r8Y7LkdWgTj7rKnLv571efl5OthugXn7yuid3n/9M0n6RXzHmlNDTXBgX2IbeaBGRSMmQQQvJEkZiSm8XTUQk6iI2uVxKEpOZraLqBGRAUSqOKSIi0ZCqwQ8JXbckIiJJoFKeiIhkkihNnQaRq0yKiEi2U49JRCTbRayLocQkIpLtIlbKU2ISEcl2EUtMEesAiohItlOPSUQk20Wsi6HEJCKS7VTKExERSR31mEREsl3EekxKTCIi2S5ita+INUdERLKdekwiItlOpTwREckoEUtMKuWJiEhGUY9JRCTbRayLocQkIpLtVMoTERFJHfWYRESyXcR6TEpMIiLZLmK1r4g1R0REsp16TCIi2U6lvPQpKsjdDt0zlc+FHUIoGtTJDzuE0NTLz82f91xtd1JFKy9ldmJaW14ZdgihKCrIY3VZRdhhpF2DOvmcWad32GGE4vGyp3L2M19XkZv/z7M9IZtZPjAVmOvux5tZE+BpoDXwDXCau38fbDsE6ANUAJe4+6s1vXd2f2dERATyLHlL4i4FZsY9vwaY4O4dgAnBc8ysI9Ab6AT0AB4Kklr1zdmSKEREJAOZJW9J6HDWAjgOGBW3+kRgbPB4LHBS3Pqn3H29u88GZgH71vT+SkwiIrKJmfUzs6lxS78qNrsPuAqIr8MWu/t8gOBrs2D9TsB3cduVBuuqVe05JjNbBfjGp8FXDx67uzeq6Y1FRCRNkjj4wd1LgJJqD2V2PLDI3T80s0MTeMuqovMq1m1SbWJy94YJHFBERMK2ZeeGttYBwK/N7FigHtDIzB4DFprZju4+38x2BBYF25cCLeP2bwHMq+kACZXyzOxAMzsveLyDmbXZwoaIiEgEuPsQd2/h7q2JDWp43d3PAsYD5wabnQs8HzweD/Q2s7pB7ugATKnpGLUOFzezG4EuwC+Bh4FC4DFiWVNERMKWGRfY/h4YZ2Z9gDnAqQDuPsPMxgGfA+XAAHev8dqIRK5j6gn8CvgoOMg8M1OZT0QkU4SUl9z9TeDN4PFS4PBqthsGDEv0fRMp5W1wdyc4WWVmDRJ9cxERkS2VSI9pnJmNALY1s77A+cDI1IYlIiIJS+/gh5SrNTG5+91mdiSwEtgZuMHdX0t5ZCIikpjMOMeUNInOlTcdKCJWzpueunBERCTX1XqOycwuIDa072SgF/CemZ2f6sBERCRBlsQlAyTSY7oS+FUw4gIz2x6YBIxJZWAiIpKgiJ1jSmRUXimwKu75Kn4875GIiEjS1DRX3uDg4VzgfTN7ntg5phOp5apdERFJoxwa/LDxItr/BstGz1exrYiIhCVi94moaRLXm9MZiIiICCQ2V15TYvfd6ERsJlkA3P2wFMYlIiKJilgpL5EO4OPAF0Ab4GZi93L/IIUxiYjIlkjzHWxTLZHEtL27jwbK3P0tdz8f2D/FcYmISI5K5DqmsuDrfDM7jtgNnlqkLiQREdkiuTL4Ic5tZtYYuBz4M9AIuCylUYmISOIypASXLIlM4vpi8HAF0D214YiISK6r6QLbPxPcg6kq7n5JDfueU9NB3f2RhKITEZHa5VCPaepWvO8+Vawz4ARgJyAjE9P69es5/5yzKduwgfKKco446mj6Xzww7LBS5qbrhjLx7bdo0qQJf/vHeAD+ePcfmPjWmxQU1KFly5bcdNswGjZqFHKkyVG/cX36jriQFp1a4A4l/YZzzMBj2fGXOwavN2DNitVc2+UaAFp2bkWfhy6gqGER7s71+w+lbH1ZTYfIeFV95q+9+gojHnqQ2V9/zaNPPk3H3XYLOcrUe3fiRO6843YqKyrp2asXffr2DTukrZMr55jcfezPfVN33/Tb3MwMOBO4GniPLbi9broVFhYycszD1G/QgLKyMs47+ywOPOggdt9jz7BDS4kTTurJ6b85kxuuvWbTuv27dmPgoMsoKCjg/nvvYcyokVw6+PIQo0yes/94Lp/+6xPu7/1H8uvkU7d+Xf585v2bXj/zrrNYs2INAHn5efQfO4C//PZB5kybwzZNtqG8rDys0JOmqs+8XfsO3H3fnxh2803hBZZGFRUV3H7brYwYNZri4mJ+c/ppHNq9O+3atw87NAmkLM+aWUFwy4zPgSOAXu5+urtPS9Uxt5aZUb9B7M7x5eXllJeXYRHrIsfbu0sXGjdu/KN1XQ84gIKC2N8rnXffg0ULF4QRWtIVNSxilwN35c0xbwBQUVaxKQlttF+vrkx6ehIAnY/cnTnT5zBn2hwAflj2A15ZbWU7a1T1mbdt147WbdqEFFH6fTZ9Gi1btaJFy5bUKSykxzHH8ubrr4cd1taJ2HVMid4ocIuY2QDgUmAC0MPdv03FcVKhoqKCM07txXdz5nD6GWfQefc9wg4pNM8/9yxH9egRdhhJ0axtM1YtWcmFoy+i1e6tmP3RbB69bCzr16wHYJcDd2HFouUsnBVLxDvuvCM4XP3PITRs2oj3np7Ei/e8EGYTJEkWLVxE8+bNNz1v1ryY6dMy9u/lxGRIQkmWVPWYNg4rPxB4wcymBct0M8von4D8/HzGPfscr77+Bp9Nn86sr/4TdkihGDViOAX5+Rx7/Alhh5IUeQX5tP5VG/494jWG7jOE9avXc8JVJ256vWvvA5j81KT/bZ+fz87dfsmD5zzALYfcSJeT9qFT9+ife8kF7j/t+Vqm3CFPgBSNyiN2zdM7wPf87wLdWplZP6AfwIgRIzj7/AsS3TXpGjVqRJd99+Xdd96hfYedQ4sjDC88/w8mvv0Ww0eNiUwpc1npUpaVLuO/U2YBMOWZ9znhql8DsfNJ+5y0D9ftd+3/tp+7lC8mzuSHpbFbkX3y8ie0/lVrZrzxWfqDl6Qqbl7MggX/K1EvWrCQZs2ahRhREkRs8ENNzZkKfFjDUpOdgPuJ3bdpLHAhsBuwqqaynruXuHsXd+/Sr1+/hBuRLMuWLWPlypUArFu3jvcnT6ZNDtXeAd59ZyJ/HT2K+/78IEVFRWGHkzQrFq5gaenSWIkO6HTYbsydOReA3Q7vzLwv57Fs7rJN20/71zRadm5FYVEhefl57Hrwrpu2l+zWabfOzPn2W0pLSynbsIFXXn6JQ7pn9yWaZpa0JROkalTeFQBmVgh0AboB5wMjzWy5u3f8ue+dSksWL+b6a4dQWVlBZWUlRx3dg4MPze4f2JoMufIKPvxgCsuXL6fH4d35Xf+LGTOqhLINZVzUtw8QGwAx9Mabwg00SR4Z9DD9H7mYgsICFn29iBEXDAeg6+ndmPz0pB9tu2b5al6+75/cOnkY7vDpKx/zycsfhxF2UlX1mTdq3Ji77hjG98uWcUn/i9h5l114qGRk2KGmTEFBAUOGXsdFfS+gsrKSk3qeTPsOHcIOS+JYVfXWH20Qu+3F1UBHtvC2F8FURl2BA4Kv2wLT3f28BGLzteWVCWwWPUUFeawuqwg7jLRrUCefM+v0DjuMUDxe9lTOfubrKnLz/3m9/LykdU/uLXk/aUNGB/fbL/RuUyKj8h4HngaOA34HnAssrmkHMyshdv+mVcD7wCTgXnf/fquiFRGRn8iQClzSpOq2F62AusACYC5QCizfmkBFRKRqOXOOKc4W3/bC3XsEMz50InZ+6XJgNzNbBkx29xu3ImYREYmwlN32wmMnrz4zs+XEZiZfARwP7AsoMYmIJEvEhoun5LYXZnYJsZ7SAcR6XO8Ck4ExwPSfFamIiFQpU0pwyVJrYjKzh6niQtvgXFN1WgN/By5z9/k/OzoREck5iZTyXox7XA/oSew8U7XcffDWBCUiIlsg13pM7v5M/HMzexL4d8oiEhGRLRKxvPSzTpl1IDYcXEREJOkSOce0ih+fY1pAbCYIERHJBBHrMiVSymuYjkBEROTnseTNbpQRai3lmdmERNaJiIgkQ033Y6oH1Ad2MLPtYNOdtBoBv0hDbCIikohodZhqLOVdCAwiloQ+5H9NXwk8mNqwREQkUTlzga273w/cb2YD3f3PaYxJRERyWCLDxSvNbNuNT8xsOzPrn7qQRERkS5glb8kEiSSmvu6+fOOT4J5KfVMWkYiIbJmIZaZEElOexRUwzSwfKExdSCIikssSmSvvVWCcmQ0ndqHt74BXUhqViIgkLGqDHxLpMV0NTAAuAgYEj69MZVAiIrIF8pK41MLM6pnZFDP71MxmmNnNwfomZvaamX0VfN0ubp8hZjbLzL40s6MTaU6N3L3S3Ye7ey93PwWYQeyGgSIiknvWA4e5+x7AnkAPM9sfuAaY4O4diHVgrgEws45Ab2J3NO8BPBScEqpWQpO4mtmeZnanmX0D3Ap88bOaIyIiSWdmSVtq4zE/BE/rBIsDJwJjg/VjgZOCxycCT7n7enefDcwidifzatU088POxLLcGcBS4GnA3D2hu9iKiEiaJPEck5n1A/rFrSpx95LNtsknNvFCe+BBd3/fzIo33hjW3eebWbNg852A9+J2Lw3WVaumwQ9fABOBE9x9VhDMZbU3S0REslWQhEpq2aYC2DO4xvU5M9uths2rypo/uSt6vJpKeacQu8XFG2Y20swOr+YAIiISorAuYwqucX2T2LmjhWa2Yywe2xFYFGxWCrSM260FtdwFvdrE5O7PufvpwC7BgS8Dis3sL2Z21JaFLyIiqZLOc0xm1nTjbEBmVgQcQazCNh44N9jsXOD54PF4oLeZ1TWzNsRuNjulpmMkcj+m1cDjwONm1gQ4ldhoi3/V2gIREYmaHYGxwXmmPGCcu79oZpOJXfPaB5hDLFfg7jPMbBzwOVAODAhKgdUy9xpLfWHK2MBERJIgaadGRjz/WdJ+X1544m6hn7JJZOaH0Kwtrww7hFAUFeSxriL32l4vP4+V68vDDiMUjeoWcE7hmWGHkXaPbHicteU1/vEcWUUFNV7Ks0VyceYHERGRtMnoHpOIiCQgYj0mJSYRkSwXsbykUp6IiGQW9ZhERLJdxLpMSkwiIlnO8qKVmFTKExGRjKIek4hIlotYJU+JSUQk60UsM6mUJyIiGUU9JhGRLBe1KYmUmEREsl208pJKeSIiklnUYxIRyXJRu45JiUlEJMtFKy2plCciIhlGPSYRkSynUXkiIpJRIpaXVMoTEZHMoh6TiEiWi1qPSYlJRCTLWcTG5amUJyIiGUU9JhGRLKdSnoiIZJSoJSaV8kREJKOox7SZY448nAYNGpCXl09BQT5PjPt72CGlxbsTJ3LnHbdTWVFJz1696NO3b9ghpcyCBfO5aegQli5ZiuUZPU85lTPOOpv777mbiW+9SZ06dWjRsiU33HIbDRs1CjvcrVa/cX3OH9GXFp1agDuj+pYw6/1ZHNn/KI7ofyQV5ZV8+vInPD3kSfLr5HPeQ31os3dbvLKSxwY/yhdvzwy7CUn1zezZXHX54E3P55aWctHFAznrnHNCjGrr6ALbBJjZKsA3Pg2+enC8QnfP6IQ48uGxbLfddmGHkTYVFRXcftutjBg1muLiYn5z+mkc2r077dq3Dzu0lCjIL2DQ5VexS8eOrF69mnN6n8p+XbuyX9euDLh0EAUFBfz5j/fw19EjGXjZ5WGHu9XOuvdspr/6KQ/0vp/8OvnUrV+XXQ/pyF4n7M3QvYZQvqGchk1jCfjQPocBMHSva2jYtBFXvHAVN3W9Hnev6RBZpXWbNox79jkg9rN/VPdDOeyIw8MNaitFKy2lqJTn7g3dvVGwNAR+AQwDFgD3p+KY8vN9Nn0aLVu1okXLltQpLKTHMcfy5uuvhx1WyuzQtCm7dOwIQIMGDWjdpi2LFy1i/24HUFAQ+5tpt933YOHChWGGmRT1GhbxywN34a2H3wSgoqyCNSvWcNiFh/PiH8ZTvqEcgFWLVwKw06478fkbMzatW7N8NW32bhNK7Onw/nvv0aJlK37xi53CDmWrmFnSlkyQ0nNMZratmd0EfAo0BPZx94z+E9TMuKhvH8449RT+Pm5c2OGkxaKFi2jevPmm582aF7NwUfb/Uk7EvLlz+fKLmXTqvPuP1o9/7lm6HXhQSFElT7O2zVi5ZBV9R13IrVOGcf7wCyisX5fmHXZk5wN34cZ3bubaf19Hm73bAjBn2rfsdcLe5OXnsUPrprTeqw1NWm4fcitS59WXX+KYY48NOwzZTEoSk5ntYGZ3AB8B5cCv3P06d19ay379zGyqmU0tKSlJRWi1+utjT/DU35/lweEljHvyCT6c+kEocaRTVWWaqF2wV5U1a1Zz9eBBDL7qGrbZZptN68eUjKCgoIBjjjs+xOiSIz8/j9a/as2EEf/m+n2Hsn71ek646gTyC/JosG0Dbj7wRp665gkufmIgAG//9S2WlS7j5vdu46x7zmbW5K+oLK8MuRWpUbZhA2+98QZHHn102KFsNbPkLZkgVed6vgUWAw8Da4A+8V1Ed7+3qp3cvQTYmJF8bQj/IZo1awZAk+23p/sRR/DZ9Ons3WWftMeRTsXNi1mwYMGm54sWLNz0fYiq8rIyrh48iB7HHcdhRxy5af2Lz/+Dd95+i4dGjs6YssbWWDZ3GctKl/H1B/8F4INnp3D8lSewrHQZU/8R+6Pr66lfU1npNNyhIauWrOKJKx/btP/1b93IglkLqnzvbPfOOxPZpWNHtt9hh7BD2WrZ/5P6Y6kq5f2BWFKCWAkvftmmup3CtnbNGlavXr3p8eRJ79K+fYeQo0q9Trt1Zs6331JaWkrZhg288vJLHNK9e9hhpYy7c+uNN9C6TVvOPOe3m9ZPemcijzw8mnv+9AD1iorCCzCJVixcwbLSpTTfeUcAOh3WiXkz5/Lh+A/p2D12nq15h+YUFBawaskqCosKKaxfN7bt4btRUV7JvJlzQ4s/lV556SV6qIyXkVLSY3L3m6p7zcwGpeKYybB06VIGXxIraZRXlHPMccdzwEHZf56hNgUFBQwZeh0X9b2AyspKTup5Mu07RDchf/rxR7z04njad9iZ35x6MgADLhnE3b+/nQ0byhhw4QUAdN59D4Zcf2OYoSbFo5c9wkVj+5NfWMDi2YsYecEI1q9ezwUj+3H7x7+nfEM5JX2GA9CoWSOu/OfVeKXz/dzvGXHeX0KOPjXWrl3Le5Mmcd2NN4UdSlJEoXcfz9I9DNTM5rh7qwQ2DaWUlwmKCvJYV5F7ba+Xn8fK9eVhhxGKRnULOKfwzLDDSLtHNjzO2vKKsMMIRVFBftKyyTOTv0naL/JTurYOPcuFMfND6I0WEZHMFcaFrtG5Uk9EJANErZSXjpkffvQSEI2zyiIiGSJaaSl1gx8apuJ9RUQk+jJ6zjoREaldxCp5SkwiItkuaueYdD8mERHJKOoxiYhkuWj1l5SYRESyXsQqeSrliYhIZlGPSUQky2nwg4iIZJR03o/JzFqa2RtmNtPMZpjZpcH6Jmb2mpl9FXzdLm6fIWY2y8y+NLNab4ClxCQiIluiHLjc3XcF9gcGmFlH4Bpggrt3ACYEzwle6w10AnoAD5lZfk0HUGISEclylsR/tXH3+e7+UfB4FTAT2Ak4ERgbbDYWOCl4fCLwlLuvd/fZwCxg35qOocQkIpLlklnKM7N+ZjY1bulX/XGtNfAr4H2g2N3nQyx5ARtvg70T8F3cbqXBumpp8IOIiGzi7iVASW3bmdk2wDPAIHdfWcMAjKpeqPEuE0pMIiJZLt2D8sysDrGk9Li7PxusXmhmO7r7fDPbEVgUrC8FWsbt3gKYV9P7q5QnIpLl8rCkLbWxWNdoNDDT3e+Ne2k8cG7w+Fzg+bj1vc2srpm1AToAU2o6hnpMIiKyJQ4Azgamm9knwbprgd8D48ysDzAHOBXA3WeY2Tjgc2Ij+ga4e0VNB1BiEhHJcuks5bn7O1Q/Pd/h1ewzDBiW6DGUmEREslzEJn7QOSYREcks6jGJiGS5qM2Vp8QkIpLlopWWVMoTEZEMox6TiEiWUykvjYoKcrdDVy8/N9veqG5G/0im1CMbHg87hFAUFdQ40bQkIGJ5KbMT07qKyrBDCEW9/LycbHuuthtibV9bnnttLyrI49d2fNhhhGK8vxh2CBkroxOTiIjUTj0mERHJKIncRymb5OaJDBERyVjqMYmIZDmV8kREJKNEbbi4SnkiIpJR1GMSEclyEeswKTGJiGQ7lfJERERSSD0mEZEsF63+khKTiEjWi1glT6U8ERHJLOoxiYhkuagNflBiEhHJchHLSyrliYhIZlGPSUQky0VtdnElJhGRLKdSnoiISAqpxyQikuU0Kk9ERDJKxPKSEpOISLaLWmLSOSYREcko6jGJiGQ5DRcXEZGMolKeiIhICqnHtJl3J07kzjtup7Kikp69etGnb9+wQ0qLXG035Gbb169fz/nnnE3Zhg2UV5RzxFFH0//igWGHlXQNGjfg4lGX8H+7tcId/nT+/axfs57+wwdQb5t6LPpmEfec+QfWrlpLQZ0C+o8YQPsuHfBKZ+SlJXz21vSwm5AQDRdPgJmdU9Pr7v5IKo67tSoqKrj9tlsZMWo0xcXF/Ob00zi0e3fatW8fdmgplavthtxte2FhISPHPEz9Bg0oKyvjvLPP4sCDDmL3PfYMO7Sk6nt/Pz565UPuPPUOCuoUULd+XW557VbGXDGGGW9/xhHnHcnJV57C4zc8xlF9jwbgkt0vpnHTxtz48s1cvs9luHvIrahdxPJSykp5+1Sx7AvcCoxJ0TG32mfTp9GyVStatGxJncJCehxzLG++/nrYYaVcrrYbcrftZkb9Bg0AKC8vp7y8LHJ/dRc1LKLTwZ14bfS/ACgvK2f1itXs9MsWzHj7MwA+ee1jup7SDYCWHVsybcKnAKxYvILVy1fTvkuHcILPcSlJTO4+cOMCXAK8DxwCvAfslYpjJsOihYto3rz5pufNmhezcNHCECNKj1xtN+R22ysqKjjt5J4cdtCB7N+1G5133yPskJKqedvmrFi8kksfHsR9H93PxSMHUrd+Xb797Fv2+/V+ABxw6oHs0HIHAL75dDb7nbg/efl5FLcupt3e7Ta9luksif8yQcoGP5hZgZldAHwOHAH0cvfT3X1aqo65tarqsmfKB5VKudpuyO225+fnM+7Z53j19Tf4bPp0Zn31n7BDSqr8gnza7dWOl//yEoP2upR1q9fT65pT+dP593PsgOO4d+p9FDUsonxDOQCvjXmNJaVLuHfqfVxwX1++mPQFleUVIbciMWbJWzJBShKTmQ0glpD2Bnq4+2/d/csE9utnZlPNbGpJSUkqQqtRcfNiFixYsOn5ogULadasWdrjSLdcbTfkdts3atSoEV323Zd333kn7FCSaknpEpaULuE/U2IJd9Lf36XtXu2Y+2UpNx59A4O7DOLtJ99iwX9jn39lRSWjB49i0K8uYdhJt9Fg2wbM+2pemE3IWanqMf0ZaAQcCLxgZtOCZbqZVdtjcvcSd+/i7l369euXotCq12m3zsz59ltKS0sp27CBV15+iUO6d097HOmWq+2G3G37smXLWLlyJQDr1q3j/cmTadOmTchRJdfyhctZ8t0Sdtp5JwD2OHwPvvt8Do2bNgZi59lOu643rwx/GYDCorrUrV8XgD2P2JPK8gq+m/ldOMFvoTyzpC2ZIFXDxbPyJ7ygoIAhQ6/jor4XUFlZyUk9T6Z9h+if/MzVdkPutn3J4sVcf+0QKisrqKys5Kije3DwodFLyCUDhzP48SuoU1jAgq8XcP9593HYOYdz7IDjAJj87CT+/fBrAGzbrDE3vXoLXuksnbuUe8++J8zQt0iG5JOksXQOhTSzfKC3uz+ewOa+rqIy1SFlpHr5eeRi23O13RBr+9ry3Gt7UUEev7bjww4jFOP9xaSlky/mrUjaL/JdftE49DSXqnNMjcxsiJk9YGZHWcxA4GvgtFQcU0QkV0Vt8EOqSnmPAt8Dk4ELgCuBQuBEd/8kRccUEclJURtJmqrE1NbdOwOY2ShgCdDK3Vel6HgiIhIRqRqVV7bxgbtXALOVlEREUiOdpTwzG2Nmi8zss7h1TczsNTP7Kvi6XdxrQ8xslpl9aWZHJ9KeVCWmPcxsZbCsAnbf+NjMVqbomCIiOcnMkrYk4K9Aj83WXQNMcPcOwITgOWbWEegNdAr2eSgYBFejVE1JlO/ujYKlobsXxD1ulIpjiohI6rn728CyzVafCIwNHo8FTopb/5S7r3f32cAsYvOm1kj3YxIRyXLJLOXFz8ATLInMdlDs7vMBgq8bp0/ZCYi/Srk0WFcj3Y9JRCTLJXNmeHcvAZI1J1xVgdV6zZV6TCIisrUWmtmOAMHXRcH6UqBl3HYtgFonIFRiEhHJcpbE5WcaD5wbPD4XeD5ufW8zq2tmbYAOwJTa3kylPBGRLJfOmzya2ZPAocAOZlYK3Aj8HhhnZn2AOcCpAO4+w8zGEbvbRDkwILiEqEZKTCIikjB3P6Oalw6vZvthwLAtOYYSk4hIlsuUOe6SRYlJRCTLRSwvafCDiIhkFvWYRESyXcRqeUpMIiJZLlppSaU8ERHJMOoxiYhkuYhV8pSYRESyXcTykkp5IiKSWdRjEhHJdhGr5SkxiYhkuWilJZXyREQkw6jHJCKS5SJWyVNiEhHJftHKTCrliYhIRjH3Wm+/nnPMrF9w3/uck6ttz9V2Q+62PUrtXrByXdJ+kTdvVC/07pd6TFXrF3YAIcrVtudquyF32x6ZdmfArdWTSolJREQyigY/iIhkOY3Kyw2RqDv/TLna9lxtN+Ru2yPU7mhlJg1+EBHJcotWrU/aL/JmDeuGnuXUYxIRyXIq5YmISEaJWF7SqLx4ZlZhZp+Y2Wdm9jczqx92TKlkZj9Use4mM5sb9334dRixJZuZ/dHMBsU9f9XMRsU9v8fMBpuZm9nAuPUPmNlv0xttatTwea8xs2Y1bZfNNvt//YKZbRusbx3lzzubKTH92Fp339PddwM2AL8LO6CQ/NHd9wROBcaYWRR+TiYB3QCC9uwAdIp7vRvwLrAIuNTMCtMeYXiWAJeHHUQKxf+/XgYMiHstGp93xC5kisIvnFSZCLQPO4gwuftMoJzYL/Fs9y5BYiKWkD4DVpnZdmZWF9gV+B5YDEwAzg0lynCMAU43syZhB5IGk4Gd4p5H4vO2JP7LBEpMVTCzAuAYYHrYsYTJzPYDKon9581q7j4PKDezVsQS1GTgfaAr0AWYRqyXDPB74HIzyw8j1hD8QCw5XRp2IKkUfJ6HA+M3eynXPu+Mp8EPP1ZkZp8EjycCo0OMJUyXmdlZwCrgdI/ONQUbe03dgHuJ/eXcDVhBrNQHgLvPNrMpwG/CCDIkfwI+MbN7wg4kBTb+v24NfAi8Fv9iFD5vjcqLtrXBuZVc90d3vzvsIFJg43mmzsRKed8RO7eykliPId7twN+Bt9MZYFjcfbmZPQH0DzuWFFjr7nuaWWPgRWLnmP602TZZ/XlHLC+plCc55V3geGCZu1e4+zJgW2LlvMnxG7r7F8Dnwfa54l7gQiL6B6u7rwAuAa4wszqbvZbdn7dZ8pYMoMSU2+qbWWncMjjsgFJsOrGBHO9ttm6Fuy+pYvthQIt0BJYmNX7ewffgOaBuOOGlnrt/DHwK9K7i5ah93llLUxKJiGS55WvLkvaLfNuiOqF3myLZZRcRySUZUoFLGpXyREQko6jHJCKS5SLWYVJiEhHJehGr5amUJyIiGUWJSUKRzJnczeyvZtYreDzKzDrWsO2hZtatutdr2O8bM/vJnIHVrd9smy2arTuY8fuKLY1RclfE5nBVYpLQ1DiT+8+dt8zdL3D3z2vY5FD+N5mrSCRE7PpaJSbJCBOB9kFv5o1gapzpZpZvZn8wsw/MbJqZXQhgMQ+Y2edm9k8g/l5Cb5pZl+BxDzP7yMw+NbMJZtaaWAK8LOitHWRmTc3smeAYH5jZAcG+25vZv8zsYzMbQQJ/TJrZP8zsQzObYWb9NnvtniCWCWbWNFjXzsxeCfaZaGa7JOW7KZLlNPhBQhU3k/srwap9gd2CiTX7EZuVYZ/g1hTvmtm/gF8BvyQ2510xsalkxmz2vk2BkcDBwXs1cfdlZjYc+GHjXIBBEvyju78TzDz+KrFbYNwIvOPut5jZccCPEk01zg+OUQR8YGbPuPtSoAHwkbtfbmY3BO99MVAC/M7dvwpmcn8IOOxnfBsl52VIVydJlJgkLFXN5N4NmOLus4P1RwG7bzx/BDQGOgAHA0+6ewUwz8xer+L99wfe3vhewbx4VTkC6Gj/q2E0MrOGwTFODvb9p5l9n0CbLjGznsHjlkGsS4ndOuTpYP1jwLNmtk3Q3r/FHTuyUwFJamVKCS5ZlJgkLD+ZyT34Bb06fhUw0N1f3Wy7Y4HapmCxBLaBWDm7q7uvrSKWhKd5MbNDiSW5ru6+xszeBOpVs7kHx12u2exFfkrnmCSTvQpctHEmaDPb2cwaELs1Qe/gHNSOQPcq9p0MHGJmbYJ9N96ddRXQMG67fxErqxFst2fw8G3gzGDdMcB2tcTaGPg+SEq7EOuxbZQHbOz1/YZYiXAlMNvMTg2OYWa2Ry3HEKmSRuWJpM8oYuePPjKzz4ARxHr5zwFfEZsZ/C/AW5vv6O6LiZ0XetbMPuV/pbQXgJ4bBz8Quw1Cl2Bwxef8b3TgzcDBZvYRsZLinFpifQUoMLNpwK38eAbz1UAnM/uQ2DmkW4L1ZwJ9gvhmACcm8D0R+YmojcrT7OIiIllubXlF0n6RFxXkh56e1GMSEcl66S3mBZdifGlms8zsmqQ2BfWYRESy3rqKyqT9Iq+Xn1djdgoufv8PcCRQCnwAnFHLhe1bRD0mERHZEvsCs9z9a3ffADxFks+Pari4iEiWq62XsyWCC9vjLygvcfeSuOc7Ad/FPS8F9kvW8UGJSURE4gRJqKSGTapKgkk9J6RSnoiIbIlSYjObbNQCmJfMAygxiYjIlvgA6GBmbcysEOgNjE/mAVTKExGRhLl7uZldTGxmlnxgjLvPSOYxNFxcREQyikp5IiKSUZSYREQkoygxiYhIRlFiEhGRjKLEJCIiGUWJSUREMooSk4iIZJT/B1VSBHCW2xtPAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 504x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 504x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "gnn_model = GNN7L_GraphConv(data_with_nedbit).to(device)\n",
    "pred = train(gnn_model, data_with_nedbit, epochs, lr, weight_decay, classes, 'GraphCONV')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GraphSAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ea443e159984cc694febf7aa7ea30ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40001 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch: 0000] train loss: 56.1178, train acc: 0.1697, val loss: 16.4074, val acc: 0.2226  (best train acc: 0.1697, best val acc: 0.2226, best train loss: 56.1178  @ epoch 0 )\n",
      "[Epoch: 0020] train loss: 2.9711, train acc: 0.2465, val loss: 1.9095, val acc: 0.2206  (best train acc: 0.2809, best val acc: 0.2954, best train loss: 2.9711  @ epoch 20 )\n",
      "[Epoch: 0040] train loss: 1.6961, train acc: 0.2768, val loss: 1.5648, val acc: 0.2745  (best train acc: 0.2855, best val acc: 0.2954, best train loss: 1.6831  @ epoch 35 )\n",
      "[Epoch: 0060] train loss: 1.5092, train acc: 0.3507, val loss: 1.4421, val acc: 0.3403  (best train acc: 0.3535, best val acc: 0.3423, best train loss: 1.5044  @ epoch 55 )\n",
      "[Epoch: 0080] train loss: 1.4780, train acc: 0.3447, val loss: 1.4220, val acc: 0.3889  (best train acc: 0.3858, best val acc: 0.4105, best train loss: 1.4271  @ epoch 75 )\n",
      "[Epoch: 0100] train loss: 1.3875, train acc: 0.3900, val loss: 1.3639, val acc: 0.4418  (best train acc: 0.4259, best val acc: 0.4530, best train loss: 1.3439  @ epoch 96 )\n",
      "[Epoch: 0120] train loss: 1.3222, train acc: 0.4262, val loss: 1.2991, val acc: 0.4691  (best train acc: 0.4601, best val acc: 0.4796, best train loss: 1.2785  @ epoch 119 )\n",
      "[Epoch: 0140] train loss: 1.2646, train acc: 0.4606, val loss: 1.2612, val acc: 0.4705  (best train acc: 0.4918, best val acc: 0.4867, best train loss: 1.2111  @ epoch 135 )\n",
      "[Epoch: 0160] train loss: 1.1643, train acc: 0.5171, val loss: 1.2230, val acc: 0.5150  (best train acc: 0.5171, best val acc: 0.5150, best train loss: 1.1643  @ epoch 160 )\n",
      "[Epoch: 0180] train loss: 1.1412, train acc: 0.5299, val loss: 1.1833, val acc: 0.5069  (best train acc: 0.5299, best val acc: 0.5255, best train loss: 1.1322  @ epoch 178 )\n",
      "[Epoch: 0200] train loss: 1.1611, train acc: 0.5188, val loss: 1.1456, val acc: 0.5295  (best train acc: 0.5479, best val acc: 0.5474, best train loss: 1.1025  @ epoch 189 )\n",
      "[Epoch: 0220] train loss: 1.1920, train acc: 0.5092, val loss: 1.0890, val acc: 0.5858  (best train acc: 0.5523, best val acc: 0.5929, best train loss: 1.0842  @ epoch 212 )\n",
      "[Epoch: 0240] train loss: 1.0735, train acc: 0.5625, val loss: 1.0757, val acc: 0.5659  (best train acc: 0.5824, best val acc: 0.5929, best train loss: 1.0272  @ epoch 221 )\n",
      "[Epoch: 0260] train loss: 1.0252, train acc: 0.5803, val loss: 1.0489, val acc: 0.6132  (best train acc: 0.5857, best val acc: 0.6132, best train loss: 1.0252  @ epoch 260 )\n",
      "[Epoch: 0280] train loss: 1.0327, train acc: 0.5722, val loss: 1.0444, val acc: 0.5541  (best train acc: 0.6002, best val acc: 0.6185, best train loss: 0.9927  @ epoch 271 )\n",
      "[Epoch: 0300] train loss: 1.0551, train acc: 0.5740, val loss: 1.0075, val acc: 0.6455  (best train acc: 0.6138, best val acc: 0.6476, best train loss: 0.9535  @ epoch 291 )\n",
      "[Epoch: 0320] train loss: 1.0426, train acc: 0.5863, val loss: 0.9942, val acc: 0.6121  (best train acc: 0.6138, best val acc: 0.6644, best train loss: 0.9535  @ epoch 291 )\n",
      "[Epoch: 0340] train loss: 1.0427, train acc: 0.5816, val loss: 1.0161, val acc: 0.5497  (best train acc: 0.6248, best val acc: 0.6681, best train loss: 0.9496  @ epoch 334 )\n",
      "[Epoch: 0360] train loss: 0.9718, train acc: 0.6122, val loss: 0.9448, val acc: 0.6526  (best train acc: 0.6481, best val acc: 0.6786, best train loss: 0.9168  @ epoch 344 )\n",
      "[Epoch: 0380] train loss: 0.9724, train acc: 0.6220, val loss: 0.9380, val acc: 0.6536  (best train acc: 0.6533, best val acc: 0.6921, best train loss: 0.8970  @ epoch 370 )\n",
      "[Epoch: 0400] train loss: 1.0292, train acc: 0.5777, val loss: 0.9285, val acc: 0.6395  (best train acc: 0.6666, best val acc: 0.6931, best train loss: 0.8496  @ epoch 395 )\n",
      "[Epoch: 0420] train loss: 1.0150, train acc: 0.6018, val loss: 0.8873, val acc: 0.6762  (best train acc: 0.6677, best val acc: 0.6992, best train loss: 0.8496  @ epoch 395 )\n",
      "[Epoch: 0440] train loss: 0.9024, train acc: 0.6442, val loss: 0.8878, val acc: 0.6823  (best train acc: 0.6742, best val acc: 0.7147, best train loss: 0.8496  @ epoch 395 )\n",
      "[Epoch: 0460] train loss: 0.8860, train acc: 0.6567, val loss: 0.8582, val acc: 0.7160  (best train acc: 0.6875, best val acc: 0.7160, best train loss: 0.8485  @ epoch 450 )\n",
      "[Epoch: 0480] train loss: 1.0102, train acc: 0.5970, val loss: 0.8512, val acc: 0.7005  (best train acc: 0.6953, best val acc: 0.7545, best train loss: 0.8297  @ epoch 463 )\n",
      "[Epoch: 0500] train loss: 0.8678, train acc: 0.6692, val loss: 0.8457, val acc: 0.7197  (best train acc: 0.7014, best val acc: 0.7636, best train loss: 0.8095  @ epoch 494 )\n",
      "[Epoch: 0520] train loss: 0.8127, train acc: 0.6944, val loss: 0.8073, val acc: 0.7363  (best train acc: 0.7185, best val acc: 0.7636, best train loss: 0.7899  @ epoch 506 )\n",
      "[Epoch: 0540] train loss: 0.9286, train acc: 0.6275, val loss: 0.8129, val acc: 0.6907  (best train acc: 0.7185, best val acc: 0.7636, best train loss: 0.7899  @ epoch 506 )\n",
      "[Epoch: 0560] train loss: 0.9516, train acc: 0.6544, val loss: 0.7873, val acc: 0.7700  (best train acc: 0.7185, best val acc: 0.7882, best train loss: 0.7886  @ epoch 558 )\n",
      "[Epoch: 0580] train loss: 0.8194, train acc: 0.6927, val loss: 0.7898, val acc: 0.7174  (best train acc: 0.7345, best val acc: 0.7882, best train loss: 0.7512  @ epoch 578 )\n",
      "[Epoch: 0600] train loss: 0.7484, train acc: 0.7366, val loss: 0.7420, val acc: 0.8088  (best train acc: 0.7366, best val acc: 0.8088, best train loss: 0.7484  @ epoch 600 )\n",
      "[Epoch: 0620] train loss: 0.8216, train acc: 0.6974, val loss: 0.7476, val acc: 0.7713  (best train acc: 0.7371, best val acc: 0.8088, best train loss: 0.7269  @ epoch 616 )\n",
      "[Epoch: 0640] train loss: 0.7248, train acc: 0.7376, val loss: 0.7256, val acc: 0.7777  (best train acc: 0.7443, best val acc: 0.8088, best train loss: 0.7248  @ epoch 640 )\n",
      "[Epoch: 0660] train loss: 0.7392, train acc: 0.7279, val loss: 0.7297, val acc: 0.7673  (best train acc: 0.7570, best val acc: 0.8253, best train loss: 0.6988  @ epoch 658 )\n",
      "[Epoch: 0680] train loss: 0.7456, train acc: 0.7123, val loss: 0.7565, val acc: 0.6921  (best train acc: 0.7570, best val acc: 0.8287, best train loss: 0.6946  @ epoch 662 )\n",
      "[Epoch: 0700] train loss: 0.7657, train acc: 0.7021, val loss: 0.7152, val acc: 0.7555  (best train acc: 0.7700, best val acc: 0.8320, best train loss: 0.6722  @ epoch 688 )\n",
      "[Epoch: 0720] train loss: 0.7910, train acc: 0.7013, val loss: 0.7177, val acc: 0.7410  (best train acc: 0.7700, best val acc: 0.8320, best train loss: 0.6722  @ epoch 688 )\n",
      "[Epoch: 0740] train loss: 0.7041, train acc: 0.7357, val loss: 0.7078, val acc: 0.7346  (best train acc: 0.7700, best val acc: 0.8344, best train loss: 0.6722  @ epoch 688 )\n",
      "[Epoch: 0760] train loss: 0.8491, train acc: 0.7112, val loss: 0.7003, val acc: 0.8185  (best train acc: 0.7700, best val acc: 0.8415, best train loss: 0.6722  @ epoch 688 )\n",
      "[Epoch: 0780] train loss: 0.7178, train acc: 0.7453, val loss: 0.6619, val acc: 0.8371  (best train acc: 0.7700, best val acc: 0.8415, best train loss: 0.6722  @ epoch 688 )\n",
      "[Epoch: 0800] train loss: 0.7095, train acc: 0.7352, val loss: 0.6814, val acc: 0.7811  (best train acc: 0.7719, best val acc: 0.8438, best train loss: 0.6688  @ epoch 797 )\n",
      "[Epoch: 0820] train loss: 0.7772, train acc: 0.6743, val loss: 0.6984, val acc: 0.7342  (best train acc: 0.7740, best val acc: 0.8438, best train loss: 0.6519  @ epoch 816 )\n",
      "[Epoch: 0840] train loss: 0.8165, train acc: 0.6815, val loss: 0.6679, val acc: 0.7811  (best train acc: 0.7740, best val acc: 0.8455, best train loss: 0.6514  @ epoch 835 )\n",
      "[Epoch: 0860] train loss: 0.6542, train acc: 0.7659, val loss: 0.6491, val acc: 0.7909  (best train acc: 0.7749, best val acc: 0.8455, best train loss: 0.6475  @ epoch 858 )\n",
      "[Epoch: 0880] train loss: 0.6374, train acc: 0.7754, val loss: 0.6425, val acc: 0.7906  (best train acc: 0.7888, best val acc: 0.8455, best train loss: 0.6166  @ epoch 874 )\n",
      "[Epoch: 0900] train loss: 0.6874, train acc: 0.7377, val loss: 0.6224, val acc: 0.8378  (best train acc: 0.7888, best val acc: 0.8479, best train loss: 0.6166  @ epoch 874 )\n",
      "[Epoch: 0920] train loss: 0.6959, train acc: 0.7339, val loss: 0.6319, val acc: 0.8007  (best train acc: 0.7888, best val acc: 0.8479, best train loss: 0.6166  @ epoch 874 )\n",
      "[Epoch: 0940] train loss: 0.6185, train acc: 0.7871, val loss: 0.6075, val acc: 0.8499  (best train acc: 0.7888, best val acc: 0.8519, best train loss: 0.6166  @ epoch 874 )\n",
      "[Epoch: 0960] train loss: 0.7041, train acc: 0.7316, val loss: 0.6337, val acc: 0.8293  (best train acc: 0.7888, best val acc: 0.8563, best train loss: 0.6166  @ epoch 874 )\n",
      "[Epoch: 0980] train loss: 0.7287, train acc: 0.7180, val loss: 0.6473, val acc: 0.7747  (best train acc: 0.7914, best val acc: 0.8563, best train loss: 0.6166  @ epoch 874 )\n",
      "[Epoch: 1000] train loss: 0.6499, train acc: 0.7613, val loss: 0.5960, val acc: 0.8445  (best train acc: 0.7914, best val acc: 0.8563, best train loss: 0.6166  @ epoch 874 )\n",
      "[Epoch: 1020] train loss: 0.6701, train acc: 0.7501, val loss: 0.6080, val acc: 0.8125  (best train acc: 0.7925, best val acc: 0.8563, best train loss: 0.6100  @ epoch 1001 )\n",
      "[Epoch: 1040] train loss: 0.6639, train acc: 0.7510, val loss: 0.6435, val acc: 0.7673  (best train acc: 0.7925, best val acc: 0.8597, best train loss: 0.6007  @ epoch 1024 )\n",
      "[Epoch: 1060] train loss: 0.7003, train acc: 0.7277, val loss: 0.5935, val acc: 0.8297  (best train acc: 0.7925, best val acc: 0.8604, best train loss: 0.6007  @ epoch 1024 )\n",
      "[Epoch: 1080] train loss: 0.7849, train acc: 0.7020, val loss: 0.5794, val acc: 0.8614  (best train acc: 0.8020, best val acc: 0.8614, best train loss: 0.5874  @ epoch 1061 )\n",
      "[Epoch: 1100] train loss: 0.6594, train acc: 0.7449, val loss: 0.6102, val acc: 0.8135  (best train acc: 0.8020, best val acc: 0.8614, best train loss: 0.5874  @ epoch 1061 )\n",
      "[Epoch: 1120] train loss: 0.7260, train acc: 0.7233, val loss: 0.5886, val acc: 0.8199  (best train acc: 0.8020, best val acc: 0.8614, best train loss: 0.5874  @ epoch 1061 )\n",
      "[Epoch: 1140] train loss: 0.6854, train acc: 0.7350, val loss: 0.6407, val acc: 0.7659  (best train acc: 0.8020, best val acc: 0.8614, best train loss: 0.5874  @ epoch 1061 )\n",
      "[Epoch: 1160] train loss: 0.6184, train acc: 0.7791, val loss: 0.5602, val acc: 0.8567  (best train acc: 0.8020, best val acc: 0.8614, best train loss: 0.5874  @ epoch 1061 )\n",
      "[Epoch: 1180] train loss: 0.6272, train acc: 0.7702, val loss: 0.5527, val acc: 0.8624  (best train acc: 0.8020, best val acc: 0.8624, best train loss: 0.5874  @ epoch 1061 )\n",
      "[Epoch: 1200] train loss: 0.7556, train acc: 0.7092, val loss: 0.6126, val acc: 0.7879  (best train acc: 0.8020, best val acc: 0.8624, best train loss: 0.5874  @ epoch 1061 )\n",
      "[Epoch: 1220] train loss: 0.7131, train acc: 0.7086, val loss: 0.5531, val acc: 0.8614  (best train acc: 0.8038, best val acc: 0.8648, best train loss: 0.5833  @ epoch 1219 )\n",
      "[Epoch: 1240] train loss: 0.6258, train acc: 0.7714, val loss: 0.5530, val acc: 0.8503  (best train acc: 0.8038, best val acc: 0.8648, best train loss: 0.5798  @ epoch 1221 )\n",
      "[Epoch: 1260] train loss: 0.6914, train acc: 0.7396, val loss: 0.5817, val acc: 0.8182  (best train acc: 0.8038, best val acc: 0.8654, best train loss: 0.5761  @ epoch 1249 )\n",
      "[Epoch: 1280] train loss: 0.7444, train acc: 0.7102, val loss: 0.5613, val acc: 0.8347  (best train acc: 0.8038, best val acc: 0.8654, best train loss: 0.5761  @ epoch 1249 )\n",
      "[Epoch: 1300] train loss: 0.5872, train acc: 0.7989, val loss: 0.5357, val acc: 0.8621  (best train acc: 0.8038, best val acc: 0.8671, best train loss: 0.5761  @ epoch 1249 )\n",
      "[Epoch: 1320] train loss: 0.7047, train acc: 0.7320, val loss: 0.6023, val acc: 0.7895  (best train acc: 0.8038, best val acc: 0.8675, best train loss: 0.5740  @ epoch 1318 )\n",
      "[Epoch: 1340] train loss: 0.6311, train acc: 0.7553, val loss: 0.5439, val acc: 0.8320  (best train acc: 0.8038, best val acc: 0.8675, best train loss: 0.5740  @ epoch 1318 )\n",
      "[Epoch: 1360] train loss: 0.5522, train acc: 0.8177, val loss: 0.5452, val acc: 0.8523  (best train acc: 0.8177, best val acc: 0.8675, best train loss: 0.5522  @ epoch 1360 )\n",
      "[Epoch: 1380] train loss: 0.6229, train acc: 0.7598, val loss: 0.5437, val acc: 0.8307  (best train acc: 0.8177, best val acc: 0.8695, best train loss: 0.5522  @ epoch 1360 )\n",
      "[Epoch: 1400] train loss: 0.7051, train acc: 0.7530, val loss: 0.5364, val acc: 0.8479  (best train acc: 0.8177, best val acc: 0.8732, best train loss: 0.5522  @ epoch 1360 )\n",
      "[Epoch: 1420] train loss: 0.7148, train acc: 0.7293, val loss: 0.5627, val acc: 0.8169  (best train acc: 0.8177, best val acc: 0.8732, best train loss: 0.5522  @ epoch 1360 )\n",
      "[Epoch: 1440] train loss: 0.6985, train acc: 0.7376, val loss: 0.5511, val acc: 0.8347  (best train acc: 0.8177, best val acc: 0.8732, best train loss: 0.5522  @ epoch 1360 )\n",
      "[Epoch: 1460] train loss: 0.6524, train acc: 0.7420, val loss: 0.5452, val acc: 0.8405  (best train acc: 0.8177, best val acc: 0.8732, best train loss: 0.5522  @ epoch 1360 )\n",
      "[Epoch: 1480] train loss: 0.6178, train acc: 0.7585, val loss: 0.5120, val acc: 0.8604  (best train acc: 0.8195, best val acc: 0.8732, best train loss: 0.5437  @ epoch 1466 )\n",
      "[Epoch: 1500] train loss: 0.6505, train acc: 0.7582, val loss: 0.5334, val acc: 0.8621  (best train acc: 0.8195, best val acc: 0.8732, best train loss: 0.5437  @ epoch 1466 )\n",
      "[Epoch: 1520] train loss: 0.6764, train acc: 0.7516, val loss: 0.5130, val acc: 0.8654  (best train acc: 0.8231, best val acc: 0.8732, best train loss: 0.5389  @ epoch 1503 )\n",
      "[Epoch: 1540] train loss: 0.6202, train acc: 0.7638, val loss: 0.5313, val acc: 0.8378  (best train acc: 0.8231, best val acc: 0.8732, best train loss: 0.5389  @ epoch 1503 )\n",
      "[Epoch: 1560] train loss: 0.7345, train acc: 0.7295, val loss: 0.5062, val acc: 0.8614  (best train acc: 0.8231, best val acc: 0.8735, best train loss: 0.5389  @ epoch 1503 )\n",
      "[Epoch: 1580] train loss: 0.6115, train acc: 0.7634, val loss: 0.5066, val acc: 0.8600  (best train acc: 0.8231, best val acc: 0.8735, best train loss: 0.5389  @ epoch 1503 )\n",
      "[Epoch: 1600] train loss: 0.6040, train acc: 0.7671, val loss: 0.5040, val acc: 0.8641  (best train acc: 0.8231, best val acc: 0.8735, best train loss: 0.5379  @ epoch 1589 )\n",
      "[Epoch: 1620] train loss: 0.5577, train acc: 0.8016, val loss: 0.4894, val acc: 0.8658  (best train acc: 0.8231, best val acc: 0.8735, best train loss: 0.5225  @ epoch 1619 )\n",
      "[Epoch: 1640] train loss: 0.5604, train acc: 0.7976, val loss: 0.5022, val acc: 0.8637  (best train acc: 0.8231, best val acc: 0.8742, best train loss: 0.5225  @ epoch 1619 )\n",
      "[Epoch: 1660] train loss: 0.5739, train acc: 0.7979, val loss: 0.4993, val acc: 0.8766  (best train acc: 0.8231, best val acc: 0.8766, best train loss: 0.5225  @ epoch 1619 )\n",
      "[Epoch: 1680] train loss: 0.5555, train acc: 0.7963, val loss: 0.5110, val acc: 0.8476  (best train acc: 0.8231, best val acc: 0.8782, best train loss: 0.5225  @ epoch 1619 )\n",
      "[Epoch: 1700] train loss: 0.5395, train acc: 0.8063, val loss: 0.4875, val acc: 0.8688  (best train acc: 0.8231, best val acc: 0.8782, best train loss: 0.5225  @ epoch 1619 )\n",
      "[Epoch: 1720] train loss: 0.5594, train acc: 0.8049, val loss: 0.4813, val acc: 0.8688  (best train acc: 0.8231, best val acc: 0.8782, best train loss: 0.5225  @ epoch 1619 )\n",
      "[Epoch: 1740] train loss: 0.6511, train acc: 0.7457, val loss: 0.5155, val acc: 0.8459  (best train acc: 0.8231, best val acc: 0.8782, best train loss: 0.5213  @ epoch 1723 )\n",
      "[Epoch: 1760] train loss: 0.6830, train acc: 0.7342, val loss: 0.5219, val acc: 0.8283  (best train acc: 0.8231, best val acc: 0.8782, best train loss: 0.5213  @ epoch 1723 )\n",
      "[Epoch: 1780] train loss: 0.5226, train acc: 0.8190, val loss: 0.4974, val acc: 0.8627  (best train acc: 0.8231, best val acc: 0.8782, best train loss: 0.5213  @ epoch 1723 )\n",
      "[Epoch: 1800] train loss: 0.6231, train acc: 0.7632, val loss: 0.5004, val acc: 0.8587  (best train acc: 0.8231, best val acc: 0.8782, best train loss: 0.5213  @ epoch 1723 )\n",
      "[Epoch: 1820] train loss: 0.5340, train acc: 0.8049, val loss: 0.4855, val acc: 0.8715  (best train acc: 0.8231, best val acc: 0.8782, best train loss: 0.5213  @ epoch 1723 )\n",
      "[Epoch: 1840] train loss: 0.5727, train acc: 0.7902, val loss: 0.5264, val acc: 0.8239  (best train acc: 0.8231, best val acc: 0.8782, best train loss: 0.5213  @ epoch 1723 )\n",
      "[Epoch: 1860] train loss: 0.5568, train acc: 0.8075, val loss: 0.4853, val acc: 0.8712  (best train acc: 0.8231, best val acc: 0.8782, best train loss: 0.5213  @ epoch 1723 )\n",
      "[Epoch: 1880] train loss: 0.5464, train acc: 0.8057, val loss: 0.4987, val acc: 0.8530  (best train acc: 0.8231, best val acc: 0.8782, best train loss: 0.5213  @ epoch 1723 )\n",
      "[Epoch: 1900] train loss: 0.5524, train acc: 0.7969, val loss: 0.4909, val acc: 0.8614  (best train acc: 0.8231, best val acc: 0.8782, best train loss: 0.5213  @ epoch 1723 )\n",
      "[Epoch: 1920] train loss: 0.6377, train acc: 0.7591, val loss: 0.4662, val acc: 0.8762  (best train acc: 0.8231, best val acc: 0.8782, best train loss: 0.5213  @ epoch 1723 )\n",
      "[Epoch: 1940] train loss: 0.5652, train acc: 0.7974, val loss: 0.4701, val acc: 0.8742  (best train acc: 0.8231, best val acc: 0.8782, best train loss: 0.5213  @ epoch 1723 )\n",
      "[Epoch: 1960] train loss: 0.5619, train acc: 0.7905, val loss: 0.4759, val acc: 0.8698  (best train acc: 0.8231, best val acc: 0.8782, best train loss: 0.5213  @ epoch 1723 )\n",
      "[Epoch: 1980] train loss: 0.5849, train acc: 0.7797, val loss: 0.4732, val acc: 0.8712  (best train acc: 0.8231, best val acc: 0.8782, best train loss: 0.5213  @ epoch 1723 )\n",
      "[Epoch: 2000] train loss: 0.6237, train acc: 0.7710, val loss: 0.4896, val acc: 0.8641  (best train acc: 0.8231, best val acc: 0.8782, best train loss: 0.5213  @ epoch 1723 )\n",
      "[Epoch: 2020] train loss: 0.5888, train acc: 0.7803, val loss: 0.5017, val acc: 0.8469  (best train acc: 0.8231, best val acc: 0.8782, best train loss: 0.5131  @ epoch 2007 )\n",
      "[Epoch: 2040] train loss: 0.5819, train acc: 0.7825, val loss: 0.5233, val acc: 0.8331  (best train acc: 0.8231, best val acc: 0.8782, best train loss: 0.5131  @ epoch 2007 )\n",
      "[Epoch: 2060] train loss: 0.5641, train acc: 0.7934, val loss: 0.5486, val acc: 0.8132  (best train acc: 0.8231, best val acc: 0.8782, best train loss: 0.5113  @ epoch 2057 )\n",
      "[Epoch: 2080] train loss: 0.5904, train acc: 0.7655, val loss: 0.4685, val acc: 0.8769  (best train acc: 0.8253, best val acc: 0.8782, best train loss: 0.5013  @ epoch 2071 )\n",
      "[Epoch: 2100] train loss: 0.5332, train acc: 0.8096, val loss: 0.4652, val acc: 0.8722  (best train acc: 0.8253, best val acc: 0.8782, best train loss: 0.5013  @ epoch 2071 )\n",
      "[Epoch: 2120] train loss: 0.5897, train acc: 0.7839, val loss: 0.4798, val acc: 0.8725  (best train acc: 0.8253, best val acc: 0.8799, best train loss: 0.5013  @ epoch 2071 )\n",
      "[Epoch: 2140] train loss: 0.5511, train acc: 0.7968, val loss: 0.4660, val acc: 0.8722  (best train acc: 0.8253, best val acc: 0.8799, best train loss: 0.5013  @ epoch 2071 )\n",
      "[Epoch: 2160] train loss: 0.5442, train acc: 0.8115, val loss: 0.4862, val acc: 0.8637  (best train acc: 0.8314, best val acc: 0.8799, best train loss: 0.4992  @ epoch 2158 )\n",
      "[Epoch: 2180] train loss: 0.5229, train acc: 0.8117, val loss: 0.4587, val acc: 0.8749  (best train acc: 0.8314, best val acc: 0.8799, best train loss: 0.4992  @ epoch 2158 )\n",
      "[Epoch: 2200] train loss: 0.5788, train acc: 0.7856, val loss: 0.4638, val acc: 0.8769  (best train acc: 0.8314, best val acc: 0.8799, best train loss: 0.4992  @ epoch 2158 )\n",
      "[Epoch: 2220] train loss: 0.5681, train acc: 0.7783, val loss: 0.4538, val acc: 0.8766  (best train acc: 0.8314, best val acc: 0.8799, best train loss: 0.4992  @ epoch 2158 )\n",
      "[Epoch: 2240] train loss: 0.5396, train acc: 0.8043, val loss: 0.4562, val acc: 0.8732  (best train acc: 0.8314, best val acc: 0.8799, best train loss: 0.4992  @ epoch 2158 )\n",
      "[Epoch: 2260] train loss: 0.5855, train acc: 0.7775, val loss: 0.4865, val acc: 0.8651  (best train acc: 0.8314, best val acc: 0.8799, best train loss: 0.4992  @ epoch 2158 )\n",
      "[Epoch: 2280] train loss: 0.5774, train acc: 0.7731, val loss: 0.4493, val acc: 0.8779  (best train acc: 0.8314, best val acc: 0.8799, best train loss: 0.4992  @ epoch 2158 )\n",
      "[Epoch: 2300] train loss: 0.5609, train acc: 0.8022, val loss: 0.4683, val acc: 0.8681  (best train acc: 0.8314, best val acc: 0.8799, best train loss: 0.4992  @ epoch 2158 )\n",
      "[Epoch: 2320] train loss: 0.5616, train acc: 0.7937, val loss: 0.4945, val acc: 0.8496  (best train acc: 0.8314, best val acc: 0.8799, best train loss: 0.4992  @ epoch 2158 )\n",
      "[Epoch: 2340] train loss: 0.6343, train acc: 0.7567, val loss: 0.4586, val acc: 0.8739  (best train acc: 0.8314, best val acc: 0.8799, best train loss: 0.4992  @ epoch 2158 )\n",
      "[Epoch: 2360] train loss: 0.6339, train acc: 0.7689, val loss: 0.4613, val acc: 0.8722  (best train acc: 0.8314, best val acc: 0.8799, best train loss: 0.4992  @ epoch 2158 )\n",
      "[Epoch: 2380] train loss: 0.6501, train acc: 0.7549, val loss: 0.4467, val acc: 0.8745  (best train acc: 0.8314, best val acc: 0.8799, best train loss: 0.4987  @ epoch 2367 )\n",
      "[Epoch: 2400] train loss: 0.5594, train acc: 0.7864, val loss: 0.4565, val acc: 0.8769  (best train acc: 0.8314, best val acc: 0.8799, best train loss: 0.4885  @ epoch 2386 )\n",
      "[Epoch: 2420] train loss: 0.5973, train acc: 0.7694, val loss: 0.4525, val acc: 0.8789  (best train acc: 0.8314, best val acc: 0.8799, best train loss: 0.4885  @ epoch 2386 )\n",
      "[Epoch: 2440] train loss: 0.6265, train acc: 0.7578, val loss: 0.4652, val acc: 0.8651  (best train acc: 0.8314, best val acc: 0.8809, best train loss: 0.4885  @ epoch 2386 )\n",
      "[Epoch: 2460] train loss: 0.5653, train acc: 0.7832, val loss: 0.4595, val acc: 0.8688  (best train acc: 0.8314, best val acc: 0.8809, best train loss: 0.4885  @ epoch 2386 )\n",
      "[Epoch: 2480] train loss: 0.5373, train acc: 0.8037, val loss: 0.4488, val acc: 0.8742  (best train acc: 0.8314, best val acc: 0.8809, best train loss: 0.4885  @ epoch 2386 )\n",
      "[Epoch: 2500] train loss: 0.5131, train acc: 0.8112, val loss: 0.4418, val acc: 0.8769  (best train acc: 0.8314, best val acc: 0.8809, best train loss: 0.4885  @ epoch 2386 )\n",
      "[Epoch: 2520] train loss: 0.5491, train acc: 0.7965, val loss: 0.4333, val acc: 0.8782  (best train acc: 0.8314, best val acc: 0.8809, best train loss: 0.4885  @ epoch 2386 )\n",
      "[Epoch: 2540] train loss: 0.5482, train acc: 0.7945, val loss: 0.4425, val acc: 0.8715  (best train acc: 0.8314, best val acc: 0.8809, best train loss: 0.4880  @ epoch 2531 )\n",
      "[Epoch: 2560] train loss: 0.5632, train acc: 0.7791, val loss: 0.4501, val acc: 0.8732  (best train acc: 0.8314, best val acc: 0.8809, best train loss: 0.4880  @ epoch 2531 )\n",
      "[Epoch: 2580] train loss: 0.5033, train acc: 0.8215, val loss: 0.4388, val acc: 0.8776  (best train acc: 0.8314, best val acc: 0.8809, best train loss: 0.4880  @ epoch 2531 )\n",
      "[Epoch: 2600] train loss: 0.5150, train acc: 0.8065, val loss: 0.4290, val acc: 0.8718  (best train acc: 0.8314, best val acc: 0.8809, best train loss: 0.4748  @ epoch 2584 )\n",
      "[Epoch: 2620] train loss: 0.5599, train acc: 0.7856, val loss: 0.4574, val acc: 0.8654  (best train acc: 0.8314, best val acc: 0.8809, best train loss: 0.4748  @ epoch 2584 )\n",
      "[Epoch: 2640] train loss: 0.5301, train acc: 0.7996, val loss: 0.4364, val acc: 0.8728  (best train acc: 0.8314, best val acc: 0.8820, best train loss: 0.4748  @ epoch 2584 )\n",
      "[Epoch: 2660] train loss: 0.5163, train acc: 0.8106, val loss: 0.4338, val acc: 0.8796  (best train acc: 0.8314, best val acc: 0.8820, best train loss: 0.4748  @ epoch 2584 )\n",
      "[Epoch: 2680] train loss: 0.5077, train acc: 0.8090, val loss: 0.4277, val acc: 0.8786  (best train acc: 0.8314, best val acc: 0.8820, best train loss: 0.4748  @ epoch 2584 )\n",
      "[Epoch: 2700] train loss: 0.5515, train acc: 0.7903, val loss: 0.4407, val acc: 0.8661  (best train acc: 0.8314, best val acc: 0.8820, best train loss: 0.4748  @ epoch 2584 )\n",
      "[Epoch: 2720] train loss: 0.5244, train acc: 0.8099, val loss: 0.4180, val acc: 0.8809  (best train acc: 0.8314, best val acc: 0.8820, best train loss: 0.4748  @ epoch 2584 )\n",
      "[Epoch: 2740] train loss: 0.5125, train acc: 0.8122, val loss: 0.4291, val acc: 0.8776  (best train acc: 0.8314, best val acc: 0.8843, best train loss: 0.4748  @ epoch 2584 )\n",
      "[Epoch: 2760] train loss: 0.5940, train acc: 0.7829, val loss: 0.4403, val acc: 0.8728  (best train acc: 0.8314, best val acc: 0.8843, best train loss: 0.4748  @ epoch 2584 )\n",
      "[Epoch: 2780] train loss: 0.6417, train acc: 0.7661, val loss: 0.4418, val acc: 0.8759  (best train acc: 0.8314, best val acc: 0.8843, best train loss: 0.4748  @ epoch 2584 )\n",
      "[Epoch: 2800] train loss: 0.5183, train acc: 0.8128, val loss: 0.4350, val acc: 0.8661  (best train acc: 0.8314, best val acc: 0.8843, best train loss: 0.4748  @ epoch 2584 )\n",
      "[Epoch: 2820] train loss: 0.4721, train acc: 0.8386, val loss: 0.4171, val acc: 0.8796  (best train acc: 0.8386, best val acc: 0.8843, best train loss: 0.4721  @ epoch 2820 )\n",
      "[Epoch: 2840] train loss: 0.5253, train acc: 0.8058, val loss: 0.4258, val acc: 0.8809  (best train acc: 0.8386, best val acc: 0.8843, best train loss: 0.4721  @ epoch 2820 )\n",
      "[Epoch: 2860] train loss: 0.5367, train acc: 0.8000, val loss: 0.4331, val acc: 0.8735  (best train acc: 0.8386, best val acc: 0.8843, best train loss: 0.4721  @ epoch 2820 )\n",
      "[Epoch: 2880] train loss: 0.5654, train acc: 0.7775, val loss: 0.4171, val acc: 0.8820  (best train acc: 0.8386, best val acc: 0.8843, best train loss: 0.4721  @ epoch 2820 )\n",
      "[Epoch: 2900] train loss: 0.5096, train acc: 0.8224, val loss: 0.4153, val acc: 0.8648  (best train acc: 0.8386, best val acc: 0.8843, best train loss: 0.4721  @ epoch 2820 )\n",
      "[Epoch: 2920] train loss: 0.5060, train acc: 0.8165, val loss: 0.4160, val acc: 0.8820  (best train acc: 0.8386, best val acc: 0.8843, best train loss: 0.4721  @ epoch 2820 )\n",
      "[Epoch: 2940] train loss: 0.5060, train acc: 0.8033, val loss: 0.4105, val acc: 0.8752  (best train acc: 0.8386, best val acc: 0.8843, best train loss: 0.4685  @ epoch 2924 )\n",
      "[Epoch: 2960] train loss: 0.5142, train acc: 0.8179, val loss: 0.4118, val acc: 0.8816  (best train acc: 0.8386, best val acc: 0.8843, best train loss: 0.4685  @ epoch 2924 )\n",
      "[Epoch: 2980] train loss: 0.6038, train acc: 0.7680, val loss: 0.4163, val acc: 0.8722  (best train acc: 0.8386, best val acc: 0.8843, best train loss: 0.4685  @ epoch 2924 )\n",
      "[Epoch: 3000] train loss: 0.5175, train acc: 0.8034, val loss: 0.4037, val acc: 0.8668  (best train acc: 0.8386, best val acc: 0.8843, best train loss: 0.4685  @ epoch 2924 )\n",
      "[Epoch: 3020] train loss: 0.4987, train acc: 0.8089, val loss: 0.4038, val acc: 0.8796  (best train acc: 0.8386, best val acc: 0.8843, best train loss: 0.4685  @ epoch 2924 )\n",
      "[Epoch: 3040] train loss: 0.5310, train acc: 0.7992, val loss: 0.4061, val acc: 0.8789  (best train acc: 0.8386, best val acc: 0.8843, best train loss: 0.4685  @ epoch 2924 )\n",
      "[Epoch: 3060] train loss: 0.5438, train acc: 0.8063, val loss: 0.3955, val acc: 0.8776  (best train acc: 0.8386, best val acc: 0.8843, best train loss: 0.4685  @ epoch 2924 )\n",
      "[Epoch: 3080] train loss: 0.6145, train acc: 0.7668, val loss: 0.4187, val acc: 0.8637  (best train acc: 0.8386, best val acc: 0.8853, best train loss: 0.4614  @ epoch 3070 )\n",
      "[Epoch: 3100] train loss: 0.5081, train acc: 0.8138, val loss: 0.3939, val acc: 0.8749  (best train acc: 0.8386, best val acc: 0.8853, best train loss: 0.4614  @ epoch 3070 )\n",
      "[Epoch: 3120] train loss: 0.5240, train acc: 0.8055, val loss: 0.3895, val acc: 0.8782  (best train acc: 0.8386, best val acc: 0.8853, best train loss: 0.4614  @ epoch 3070 )\n",
      "[Epoch: 3140] train loss: 0.5279, train acc: 0.8092, val loss: 0.3869, val acc: 0.8799  (best train acc: 0.8386, best val acc: 0.8853, best train loss: 0.4614  @ epoch 3070 )\n",
      "[Epoch: 3160] train loss: 0.4976, train acc: 0.8144, val loss: 0.3948, val acc: 0.8813  (best train acc: 0.8386, best val acc: 0.8853, best train loss: 0.4614  @ epoch 3070 )\n",
      "[Epoch: 3180] train loss: 0.5308, train acc: 0.7929, val loss: 0.3827, val acc: 0.8863  (best train acc: 0.8386, best val acc: 0.8863, best train loss: 0.4614  @ epoch 3070 )\n",
      "[Epoch: 3200] train loss: 0.4894, train acc: 0.8072, val loss: 0.3637, val acc: 0.8799  (best train acc: 0.8386, best val acc: 0.8863, best train loss: 0.4614  @ epoch 3070 )\n",
      "[Epoch: 3220] train loss: 0.4970, train acc: 0.8089, val loss: 0.3615, val acc: 0.8833  (best train acc: 0.8421, best val acc: 0.8884, best train loss: 0.4562  @ epoch 3216 )\n",
      "[Epoch: 3240] train loss: 0.5811, train acc: 0.7773, val loss: 0.3516, val acc: 0.8874  (best train acc: 0.8421, best val acc: 0.8914, best train loss: 0.4562  @ epoch 3216 )\n",
      "[Epoch: 3260] train loss: 0.5691, train acc: 0.7778, val loss: 0.3582, val acc: 0.8938  (best train acc: 0.8467, best val acc: 0.8954, best train loss: 0.4249  @ epoch 3257 )\n",
      "[Epoch: 3280] train loss: 0.4772, train acc: 0.8209, val loss: 0.3647, val acc: 0.8877  (best train acc: 0.8467, best val acc: 0.8978, best train loss: 0.4249  @ epoch 3257 )\n",
      "[Epoch: 3300] train loss: 0.4728, train acc: 0.8261, val loss: 0.3555, val acc: 0.8924  (best train acc: 0.8549, best val acc: 0.8978, best train loss: 0.4184  @ epoch 3296 )\n",
      "[Epoch: 3320] train loss: 0.5075, train acc: 0.8124, val loss: 0.3640, val acc: 0.8884  (best train acc: 0.8549, best val acc: 0.9056, best train loss: 0.4184  @ epoch 3296 )\n",
      "[Epoch: 3340] train loss: 0.5787, train acc: 0.7752, val loss: 0.3389, val acc: 0.9106  (best train acc: 0.8558, best val acc: 0.9106, best train loss: 0.4174  @ epoch 3338 )\n",
      "[Epoch: 3360] train loss: 0.4642, train acc: 0.8243, val loss: 0.3247, val acc: 0.9019  (best train acc: 0.8581, best val acc: 0.9106, best train loss: 0.4070  @ epoch 3352 )\n",
      "[Epoch: 3380] train loss: 0.4279, train acc: 0.8428, val loss: 0.3274, val acc: 0.9012  (best train acc: 0.8581, best val acc: 0.9170, best train loss: 0.4070  @ epoch 3352 )\n",
      "[Epoch: 3400] train loss: 0.5246, train acc: 0.8018, val loss: 0.3199, val acc: 0.9076  (best train acc: 0.8581, best val acc: 0.9170, best train loss: 0.4070  @ epoch 3352 )\n",
      "[Epoch: 3420] train loss: 0.4303, train acc: 0.8424, val loss: 0.3267, val acc: 0.9083  (best train acc: 0.8684, best val acc: 0.9170, best train loss: 0.3925  @ epoch 3411 )\n",
      "[Epoch: 3440] train loss: 0.4030, train acc: 0.8529, val loss: 0.3057, val acc: 0.9180  (best train acc: 0.8684, best val acc: 0.9184, best train loss: 0.3925  @ epoch 3411 )\n",
      "[Epoch: 3460] train loss: 0.4403, train acc: 0.8396, val loss: 0.3069, val acc: 0.9120  (best train acc: 0.8689, best val acc: 0.9184, best train loss: 0.3842  @ epoch 3446 )\n",
      "[Epoch: 3480] train loss: 0.4205, train acc: 0.8636, val loss: 0.3075, val acc: 0.9137  (best train acc: 0.8689, best val acc: 0.9187, best train loss: 0.3842  @ epoch 3446 )\n",
      "[Epoch: 3500] train loss: 0.4047, train acc: 0.8621, val loss: 0.3064, val acc: 0.9096  (best train acc: 0.8713, best val acc: 0.9204, best train loss: 0.3842  @ epoch 3446 )\n",
      "[Epoch: 3520] train loss: 0.3651, train acc: 0.8718, val loss: 0.2936, val acc: 0.9177  (best train acc: 0.8718, best val acc: 0.9204, best train loss: 0.3651  @ epoch 3520 )\n",
      "[Epoch: 3540] train loss: 0.4316, train acc: 0.8389, val loss: 0.3251, val acc: 0.9025  (best train acc: 0.8742, best val acc: 0.9224, best train loss: 0.3651  @ epoch 3520 )\n",
      "[Epoch: 3560] train loss: 0.3865, train acc: 0.8669, val loss: 0.2988, val acc: 0.9167  (best train acc: 0.8742, best val acc: 0.9228, best train loss: 0.3651  @ epoch 3520 )\n",
      "[Epoch: 3580] train loss: 0.4127, train acc: 0.8553, val loss: 0.3187, val acc: 0.9093  (best train acc: 0.8743, best val acc: 0.9228, best train loss: 0.3651  @ epoch 3520 )\n",
      "[Epoch: 3600] train loss: 0.4038, train acc: 0.8558, val loss: 0.3236, val acc: 0.9076  (best train acc: 0.8743, best val acc: 0.9234, best train loss: 0.3651  @ epoch 3520 )\n",
      "[Epoch: 3620] train loss: 0.4441, train acc: 0.8308, val loss: 0.2934, val acc: 0.9228  (best train acc: 0.8743, best val acc: 0.9261, best train loss: 0.3651  @ epoch 3520 )\n",
      "[Epoch: 3640] train loss: 0.3932, train acc: 0.8689, val loss: 0.2868, val acc: 0.9248  (best train acc: 0.8743, best val acc: 0.9268, best train loss: 0.3651  @ epoch 3520 )\n",
      "[Epoch: 3660] train loss: 0.3876, train acc: 0.8731, val loss: 0.2838, val acc: 0.9241  (best train acc: 0.8798, best val acc: 0.9268, best train loss: 0.3651  @ epoch 3520 )\n",
      "[Epoch: 3680] train loss: 0.4156, train acc: 0.8569, val loss: 0.2910, val acc: 0.9275  (best train acc: 0.8837, best val acc: 0.9292, best train loss: 0.3592  @ epoch 3677 )\n",
      "[Epoch: 3700] train loss: 0.3836, train acc: 0.8689, val loss: 0.2881, val acc: 0.9278  (best train acc: 0.8837, best val acc: 0.9292, best train loss: 0.3592  @ epoch 3677 )\n",
      "[Epoch: 3720] train loss: 0.4921, train acc: 0.8258, val loss: 0.2912, val acc: 0.9204  (best train acc: 0.8837, best val acc: 0.9292, best train loss: 0.3570  @ epoch 3715 )\n",
      "[Epoch: 3740] train loss: 0.4187, train acc: 0.8423, val loss: 0.2701, val acc: 0.9272  (best train acc: 0.8837, best val acc: 0.9295, best train loss: 0.3570  @ epoch 3715 )\n",
      "[Epoch: 3760] train loss: 0.4040, train acc: 0.8623, val loss: 0.2904, val acc: 0.9248  (best train acc: 0.8837, best val acc: 0.9295, best train loss: 0.3570  @ epoch 3715 )\n",
      "[Epoch: 3780] train loss: 0.4675, train acc: 0.8258, val loss: 0.2773, val acc: 0.9255  (best train acc: 0.8837, best val acc: 0.9298, best train loss: 0.3570  @ epoch 3715 )\n",
      "[Epoch: 3800] train loss: 0.4215, train acc: 0.8436, val loss: 0.2747, val acc: 0.9295  (best train acc: 0.8837, best val acc: 0.9305, best train loss: 0.3570  @ epoch 3715 )\n",
      "[Epoch: 3820] train loss: 0.3678, train acc: 0.8723, val loss: 0.2724, val acc: 0.9288  (best train acc: 0.8837, best val acc: 0.9305, best train loss: 0.3570  @ epoch 3715 )\n",
      "[Epoch: 3840] train loss: 0.3922, train acc: 0.8650, val loss: 0.3012, val acc: 0.9147  (best train acc: 0.8837, best val acc: 0.9305, best train loss: 0.3570  @ epoch 3715 )\n",
      "[Epoch: 3860] train loss: 0.3591, train acc: 0.8775, val loss: 0.2718, val acc: 0.9258  (best train acc: 0.8837, best val acc: 0.9315, best train loss: 0.3570  @ epoch 3715 )\n",
      "[Epoch: 3880] train loss: 0.3864, train acc: 0.8709, val loss: 0.2659, val acc: 0.9285  (best train acc: 0.8884, best val acc: 0.9315, best train loss: 0.3494  @ epoch 3867 )\n",
      "[Epoch: 3900] train loss: 0.4576, train acc: 0.8206, val loss: 0.2629, val acc: 0.9298  (best train acc: 0.8884, best val acc: 0.9315, best train loss: 0.3494  @ epoch 3867 )\n",
      "[Epoch: 3920] train loss: 0.3918, train acc: 0.8687, val loss: 0.2839, val acc: 0.9241  (best train acc: 0.8884, best val acc: 0.9315, best train loss: 0.3494  @ epoch 3867 )\n",
      "[Epoch: 3940] train loss: 0.3755, train acc: 0.8723, val loss: 0.2699, val acc: 0.9285  (best train acc: 0.8884, best val acc: 0.9315, best train loss: 0.3494  @ epoch 3867 )\n",
      "[Epoch: 3960] train loss: 0.4028, train acc: 0.8555, val loss: 0.2643, val acc: 0.9214  (best train acc: 0.8884, best val acc: 0.9319, best train loss: 0.3494  @ epoch 3867 )\n",
      "[Epoch: 3980] train loss: 0.3950, train acc: 0.8647, val loss: 0.2600, val acc: 0.9221  (best train acc: 0.8884, best val acc: 0.9325, best train loss: 0.3494  @ epoch 3867 )\n",
      "[Epoch: 4000] train loss: 0.3904, train acc: 0.8590, val loss: 0.2580, val acc: 0.9312  (best train acc: 0.8884, best val acc: 0.9325, best train loss: 0.3494  @ epoch 3867 )\n",
      "[Epoch: 4020] train loss: 0.4554, train acc: 0.8197, val loss: 0.2582, val acc: 0.9285  (best train acc: 0.8884, best val acc: 0.9325, best train loss: 0.3494  @ epoch 3867 )\n",
      "[Epoch: 4040] train loss: 0.4641, train acc: 0.8280, val loss: 0.2739, val acc: 0.9309  (best train acc: 0.8884, best val acc: 0.9332, best train loss: 0.3494  @ epoch 3867 )\n",
      "[Epoch: 4060] train loss: 0.5282, train acc: 0.7943, val loss: 0.2599, val acc: 0.9295  (best train acc: 0.8884, best val acc: 0.9332, best train loss: 0.3494  @ epoch 3867 )\n",
      "[Epoch: 4080] train loss: 0.3619, train acc: 0.8731, val loss: 0.2781, val acc: 0.9302  (best train acc: 0.8884, best val acc: 0.9339, best train loss: 0.3494  @ epoch 3867 )\n",
      "[Epoch: 4100] train loss: 0.3786, train acc: 0.8634, val loss: 0.2836, val acc: 0.9207  (best train acc: 0.8884, best val acc: 0.9356, best train loss: 0.3494  @ epoch 3867 )\n",
      "[Epoch: 4120] train loss: 0.3989, train acc: 0.8577, val loss: 0.2674, val acc: 0.9272  (best train acc: 0.8884, best val acc: 0.9356, best train loss: 0.3494  @ epoch 3867 )\n",
      "[Epoch: 4140] train loss: 0.3666, train acc: 0.8756, val loss: 0.2519, val acc: 0.9312  (best train acc: 0.8884, best val acc: 0.9356, best train loss: 0.3433  @ epoch 4128 )\n",
      "[Epoch: 4160] train loss: 0.5504, train acc: 0.8010, val loss: 0.2727, val acc: 0.9255  (best train acc: 0.8884, best val acc: 0.9356, best train loss: 0.3433  @ epoch 4128 )\n",
      "[Epoch: 4180] train loss: 0.3877, train acc: 0.8611, val loss: 0.2462, val acc: 0.9329  (best train acc: 0.8884, best val acc: 0.9356, best train loss: 0.3433  @ epoch 4128 )\n",
      "[Epoch: 4200] train loss: 0.4141, train acc: 0.8472, val loss: 0.2496, val acc: 0.9282  (best train acc: 0.8884, best val acc: 0.9356, best train loss: 0.3433  @ epoch 4128 )\n",
      "[Epoch: 4220] train loss: 0.3570, train acc: 0.8754, val loss: 0.2464, val acc: 0.9325  (best train acc: 0.8884, best val acc: 0.9356, best train loss: 0.3433  @ epoch 4128 )\n",
      "[Epoch: 4240] train loss: 0.3938, train acc: 0.8542, val loss: 0.2584, val acc: 0.9272  (best train acc: 0.8884, best val acc: 0.9356, best train loss: 0.3433  @ epoch 4128 )\n",
      "[Epoch: 4260] train loss: 0.3913, train acc: 0.8610, val loss: 0.2554, val acc: 0.9295  (best train acc: 0.8884, best val acc: 0.9356, best train loss: 0.3381  @ epoch 4249 )\n",
      "[Epoch: 4280] train loss: 0.3850, train acc: 0.8632, val loss: 0.2636, val acc: 0.9295  (best train acc: 0.8884, best val acc: 0.9359, best train loss: 0.3381  @ epoch 4249 )\n",
      "[Epoch: 4300] train loss: 0.4228, train acc: 0.8404, val loss: 0.2626, val acc: 0.9282  (best train acc: 0.8884, best val acc: 0.9359, best train loss: 0.3381  @ epoch 4249 )\n",
      "[Epoch: 4320] train loss: 0.3736, train acc: 0.8736, val loss: 0.2521, val acc: 0.9319  (best train acc: 0.8884, best val acc: 0.9359, best train loss: 0.3381  @ epoch 4249 )\n",
      "[Epoch: 4340] train loss: 0.3943, train acc: 0.8594, val loss: 0.2584, val acc: 0.9150  (best train acc: 0.8884, best val acc: 0.9359, best train loss: 0.3381  @ epoch 4249 )\n",
      "[Epoch: 4360] train loss: 0.4387, train acc: 0.8349, val loss: 0.2495, val acc: 0.9319  (best train acc: 0.8884, best val acc: 0.9359, best train loss: 0.3381  @ epoch 4249 )\n",
      "[Epoch: 4380] train loss: 0.3575, train acc: 0.8761, val loss: 0.2486, val acc: 0.9319  (best train acc: 0.8884, best val acc: 0.9359, best train loss: 0.3381  @ epoch 4249 )\n",
      "[Epoch: 4400] train loss: 0.3948, train acc: 0.8472, val loss: 0.2696, val acc: 0.9099  (best train acc: 0.8884, best val acc: 0.9363, best train loss: 0.3381  @ epoch 4249 )\n",
      "[Epoch: 4420] train loss: 0.3960, train acc: 0.8664, val loss: 0.2544, val acc: 0.9312  (best train acc: 0.8884, best val acc: 0.9363, best train loss: 0.3381  @ epoch 4249 )\n",
      "[Epoch: 4440] train loss: 0.3651, train acc: 0.8761, val loss: 0.2474, val acc: 0.9295  (best train acc: 0.8884, best val acc: 0.9363, best train loss: 0.3381  @ epoch 4249 )\n",
      "[Epoch: 4460] train loss: 0.3957, train acc: 0.8559, val loss: 0.2511, val acc: 0.9224  (best train acc: 0.8884, best val acc: 0.9363, best train loss: 0.3345  @ epoch 4443 )\n",
      "[Epoch: 4480] train loss: 0.4277, train acc: 0.8336, val loss: 0.2516, val acc: 0.9305  (best train acc: 0.8884, best val acc: 0.9373, best train loss: 0.3345  @ epoch 4443 )\n",
      "[Epoch: 4500] train loss: 0.3968, train acc: 0.8492, val loss: 0.2548, val acc: 0.9309  (best train acc: 0.8884, best val acc: 0.9373, best train loss: 0.3345  @ epoch 4443 )\n",
      "[Epoch: 4520] train loss: 0.3625, train acc: 0.8731, val loss: 0.2471, val acc: 0.9346  (best train acc: 0.8884, best val acc: 0.9373, best train loss: 0.3345  @ epoch 4443 )\n",
      "[Epoch: 4540] train loss: 0.3551, train acc: 0.8767, val loss: 0.2593, val acc: 0.9295  (best train acc: 0.8884, best val acc: 0.9373, best train loss: 0.3345  @ epoch 4443 )\n",
      "[Epoch: 4560] train loss: 0.3766, train acc: 0.8595, val loss: 0.2482, val acc: 0.9295  (best train acc: 0.8892, best val acc: 0.9373, best train loss: 0.3345  @ epoch 4443 )\n",
      "[Epoch: 4580] train loss: 0.4174, train acc: 0.8439, val loss: 0.2642, val acc: 0.9224  (best train acc: 0.8913, best val acc: 0.9373, best train loss: 0.3345  @ epoch 4443 )\n",
      "[Epoch: 4600] train loss: 0.4006, train acc: 0.8477, val loss: 0.2422, val acc: 0.9305  (best train acc: 0.8913, best val acc: 0.9373, best train loss: 0.3345  @ epoch 4443 )\n",
      "[Epoch: 4620] train loss: 0.3902, train acc: 0.8595, val loss: 0.2423, val acc: 0.9352  (best train acc: 0.8913, best val acc: 0.9373, best train loss: 0.3345  @ epoch 4443 )\n",
      "[Epoch: 4640] train loss: 0.3751, train acc: 0.8691, val loss: 0.2501, val acc: 0.9292  (best train acc: 0.8913, best val acc: 0.9373, best train loss: 0.3345  @ epoch 4443 )\n",
      "[Epoch: 4660] train loss: 0.4006, train acc: 0.8437, val loss: 0.2558, val acc: 0.9278  (best train acc: 0.8955, best val acc: 0.9373, best train loss: 0.3203  @ epoch 4653 )\n",
      "[Epoch: 4680] train loss: 0.4363, train acc: 0.8269, val loss: 0.2499, val acc: 0.9298  (best train acc: 0.8955, best val acc: 0.9373, best train loss: 0.3203  @ epoch 4653 )\n",
      "[Epoch: 4700] train loss: 0.3747, train acc: 0.8681, val loss: 0.2416, val acc: 0.9288  (best train acc: 0.8955, best val acc: 0.9373, best train loss: 0.3203  @ epoch 4653 )\n",
      "[Epoch: 4720] train loss: 0.3453, train acc: 0.8774, val loss: 0.2309, val acc: 0.9312  (best train acc: 0.9017, best val acc: 0.9373, best train loss: 0.3161  @ epoch 4707 )\n",
      "[Epoch: 4740] train loss: 0.3516, train acc: 0.8813, val loss: 0.2350, val acc: 0.9363  (best train acc: 0.9017, best val acc: 0.9373, best train loss: 0.3161  @ epoch 4707 )\n",
      "[Epoch: 4760] train loss: 0.3453, train acc: 0.8812, val loss: 0.2469, val acc: 0.9302  (best train acc: 0.9017, best val acc: 0.9373, best train loss: 0.3161  @ epoch 4707 )\n",
      "[Epoch: 4780] train loss: 0.3947, train acc: 0.8543, val loss: 0.2393, val acc: 0.9292  (best train acc: 0.9017, best val acc: 0.9373, best train loss: 0.3161  @ epoch 4707 )\n",
      "[Epoch: 4800] train loss: 0.3593, train acc: 0.8774, val loss: 0.2442, val acc: 0.9272  (best train acc: 0.9017, best val acc: 0.9373, best train loss: 0.3161  @ epoch 4707 )\n",
      "[Epoch: 4820] train loss: 0.3640, train acc: 0.8814, val loss: 0.2442, val acc: 0.9302  (best train acc: 0.9017, best val acc: 0.9373, best train loss: 0.3161  @ epoch 4707 )\n",
      "[Epoch: 4840] train loss: 0.3524, train acc: 0.8712, val loss: 0.2388, val acc: 0.9346  (best train acc: 0.9017, best val acc: 0.9373, best train loss: 0.3161  @ epoch 4707 )\n",
      "[Epoch: 4860] train loss: 0.4086, train acc: 0.8321, val loss: 0.2336, val acc: 0.9332  (best train acc: 0.9017, best val acc: 0.9373, best train loss: 0.3161  @ epoch 4707 )\n",
      "[Epoch: 4880] train loss: 0.4578, train acc: 0.8326, val loss: 0.2714, val acc: 0.9140  (best train acc: 0.9017, best val acc: 0.9373, best train loss: 0.3161  @ epoch 4707 )\n",
      "[Epoch: 4900] train loss: 0.3636, train acc: 0.8736, val loss: 0.2353, val acc: 0.9349  (best train acc: 0.9017, best val acc: 0.9373, best train loss: 0.3161  @ epoch 4707 )\n",
      "[Epoch: 4920] train loss: 0.3474, train acc: 0.8791, val loss: 0.2356, val acc: 0.9339  (best train acc: 0.9017, best val acc: 0.9373, best train loss: 0.3161  @ epoch 4707 )\n",
      "[Epoch: 4940] train loss: 0.4449, train acc: 0.8460, val loss: 0.2222, val acc: 0.9363  (best train acc: 0.9017, best val acc: 0.9373, best train loss: 0.3161  @ epoch 4707 )\n",
      "[Epoch: 4960] train loss: 0.3401, train acc: 0.8822, val loss: 0.2371, val acc: 0.9322  (best train acc: 0.9017, best val acc: 0.9373, best train loss: 0.3161  @ epoch 4707 )\n",
      "[Epoch: 4980] train loss: 0.3588, train acc: 0.8711, val loss: 0.2362, val acc: 0.9349  (best train acc: 0.9017, best val acc: 0.9373, best train loss: 0.3161  @ epoch 4707 )\n",
      "[Epoch: 5000] train loss: 0.3507, train acc: 0.8735, val loss: 0.2417, val acc: 0.9325  (best train acc: 0.9017, best val acc: 0.9379, best train loss: 0.3161  @ epoch 4707 )\n",
      "[Epoch: 5020] train loss: 0.3339, train acc: 0.8771, val loss: 0.2351, val acc: 0.9305  (best train acc: 0.9017, best val acc: 0.9400, best train loss: 0.3161  @ epoch 4707 )\n",
      "[Epoch: 5040] train loss: 0.3592, train acc: 0.8732, val loss: 0.2342, val acc: 0.9288  (best train acc: 0.9017, best val acc: 0.9400, best train loss: 0.3161  @ epoch 4707 )\n",
      "[Epoch: 5060] train loss: 0.3445, train acc: 0.8776, val loss: 0.2425, val acc: 0.9319  (best train acc: 0.9017, best val acc: 0.9400, best train loss: 0.3161  @ epoch 4707 )\n",
      "[Epoch: 5080] train loss: 0.3463, train acc: 0.8703, val loss: 0.2284, val acc: 0.9336  (best train acc: 0.9017, best val acc: 0.9400, best train loss: 0.3161  @ epoch 4707 )\n",
      "[Epoch: 5100] train loss: 0.3625, train acc: 0.8587, val loss: 0.2253, val acc: 0.9342  (best train acc: 0.9017, best val acc: 0.9400, best train loss: 0.3161  @ epoch 4707 )\n",
      "[Epoch: 5120] train loss: 0.3447, train acc: 0.8676, val loss: 0.2280, val acc: 0.9245  (best train acc: 0.9017, best val acc: 0.9400, best train loss: 0.3161  @ epoch 4707 )\n",
      "[Epoch: 5140] train loss: 0.3257, train acc: 0.8917, val loss: 0.2255, val acc: 0.9349  (best train acc: 0.9017, best val acc: 0.9400, best train loss: 0.3161  @ epoch 4707 )\n",
      "[Epoch: 5160] train loss: 0.3893, train acc: 0.8586, val loss: 0.2414, val acc: 0.9295  (best train acc: 0.9017, best val acc: 0.9400, best train loss: 0.3085  @ epoch 5148 )\n",
      "[Epoch: 5180] train loss: 0.3472, train acc: 0.8833, val loss: 0.2374, val acc: 0.9285  (best train acc: 0.9017, best val acc: 0.9400, best train loss: 0.3085  @ epoch 5148 )\n",
      "[Epoch: 5200] train loss: 0.3955, train acc: 0.8574, val loss: 0.2346, val acc: 0.9288  (best train acc: 0.9017, best val acc: 0.9400, best train loss: 0.3085  @ epoch 5148 )\n",
      "[Epoch: 5220] train loss: 0.3851, train acc: 0.8567, val loss: 0.2390, val acc: 0.9305  (best train acc: 0.9017, best val acc: 0.9400, best train loss: 0.3031  @ epoch 5213 )\n",
      "[Epoch: 5240] train loss: 0.3252, train acc: 0.8898, val loss: 0.2277, val acc: 0.9309  (best train acc: 0.9017, best val acc: 0.9400, best train loss: 0.3031  @ epoch 5213 )\n",
      "[Epoch: 5260] train loss: 0.3372, train acc: 0.8865, val loss: 0.2289, val acc: 0.9325  (best train acc: 0.9017, best val acc: 0.9400, best train loss: 0.3031  @ epoch 5213 )\n",
      "[Epoch: 5280] train loss: 0.3296, train acc: 0.8744, val loss: 0.2250, val acc: 0.9359  (best train acc: 0.9028, best val acc: 0.9400, best train loss: 0.2967  @ epoch 5264 )\n",
      "[Epoch: 5300] train loss: 0.3310, train acc: 0.8750, val loss: 0.2176, val acc: 0.9349  (best train acc: 0.9028, best val acc: 0.9400, best train loss: 0.2967  @ epoch 5264 )\n",
      "[Epoch: 5320] train loss: 0.3292, train acc: 0.8845, val loss: 0.2226, val acc: 0.9366  (best train acc: 0.9028, best val acc: 0.9400, best train loss: 0.2967  @ epoch 5264 )\n",
      "[Epoch: 5340] train loss: 0.3316, train acc: 0.8911, val loss: 0.2287, val acc: 0.9349  (best train acc: 0.9028, best val acc: 0.9400, best train loss: 0.2967  @ epoch 5264 )\n",
      "[Epoch: 5360] train loss: 0.3666, train acc: 0.8695, val loss: 0.2218, val acc: 0.9346  (best train acc: 0.9028, best val acc: 0.9400, best train loss: 0.2967  @ epoch 5264 )\n",
      "[Epoch: 5380] train loss: 0.3616, train acc: 0.8598, val loss: 0.2223, val acc: 0.9390  (best train acc: 0.9028, best val acc: 0.9400, best train loss: 0.2967  @ epoch 5264 )\n",
      "[Epoch: 5400] train loss: 0.3425, train acc: 0.8779, val loss: 0.2288, val acc: 0.9248  (best train acc: 0.9028, best val acc: 0.9400, best train loss: 0.2967  @ epoch 5264 )\n",
      "[Epoch: 5420] train loss: 0.3662, train acc: 0.8585, val loss: 0.2320, val acc: 0.9332  (best train acc: 0.9028, best val acc: 0.9420, best train loss: 0.2967  @ epoch 5264 )\n",
      "[Epoch: 5440] train loss: 0.4506, train acc: 0.8440, val loss: 0.2522, val acc: 0.9167  (best train acc: 0.9028, best val acc: 0.9420, best train loss: 0.2967  @ epoch 5264 )\n",
      "[Epoch: 5460] train loss: 0.3228, train acc: 0.8823, val loss: 0.2299, val acc: 0.9255  (best train acc: 0.9028, best val acc: 0.9420, best train loss: 0.2967  @ epoch 5264 )\n",
      "[Epoch: 5480] train loss: 0.5239, train acc: 0.8002, val loss: 0.2321, val acc: 0.9197  (best train acc: 0.9028, best val acc: 0.9420, best train loss: 0.2967  @ epoch 5264 )\n",
      "[Epoch: 5500] train loss: 0.3452, train acc: 0.8759, val loss: 0.2166, val acc: 0.9393  (best train acc: 0.9028, best val acc: 0.9420, best train loss: 0.2967  @ epoch 5264 )\n",
      "[Epoch: 5520] train loss: 0.3018, train acc: 0.8926, val loss: 0.2180, val acc: 0.9379  (best train acc: 0.9028, best val acc: 0.9420, best train loss: 0.2967  @ epoch 5264 )\n",
      "[Epoch: 5540] train loss: 0.3208, train acc: 0.8803, val loss: 0.2296, val acc: 0.9245  (best train acc: 0.9028, best val acc: 0.9420, best train loss: 0.2967  @ epoch 5264 )\n",
      "[Epoch: 5560] train loss: 0.3221, train acc: 0.8790, val loss: 0.2157, val acc: 0.9359  (best train acc: 0.9028, best val acc: 0.9420, best train loss: 0.2967  @ epoch 5264 )\n",
      "[Epoch: 5580] train loss: 0.3177, train acc: 0.8790, val loss: 0.2262, val acc: 0.9406  (best train acc: 0.9028, best val acc: 0.9420, best train loss: 0.2967  @ epoch 5264 )\n",
      "[Epoch: 5600] train loss: 0.3488, train acc: 0.8831, val loss: 0.2232, val acc: 0.9396  (best train acc: 0.9028, best val acc: 0.9420, best train loss: 0.2967  @ epoch 5264 )\n",
      "[Epoch: 5620] train loss: 0.3428, train acc: 0.8616, val loss: 0.2131, val acc: 0.9376  (best train acc: 0.9028, best val acc: 0.9420, best train loss: 0.2942  @ epoch 5613 )\n",
      "[Epoch: 5640] train loss: 0.3538, train acc: 0.8621, val loss: 0.2270, val acc: 0.9322  (best train acc: 0.9028, best val acc: 0.9420, best train loss: 0.2942  @ epoch 5613 )\n",
      "[Epoch: 5660] train loss: 0.3281, train acc: 0.8803, val loss: 0.2124, val acc: 0.9396  (best train acc: 0.9028, best val acc: 0.9420, best train loss: 0.2942  @ epoch 5613 )\n",
      "[Epoch: 5680] train loss: 0.3552, train acc: 0.8751, val loss: 0.2432, val acc: 0.9211  (best train acc: 0.9028, best val acc: 0.9420, best train loss: 0.2942  @ epoch 5613 )\n",
      "[Epoch: 5700] train loss: 0.3845, train acc: 0.8574, val loss: 0.2310, val acc: 0.9336  (best train acc: 0.9028, best val acc: 0.9420, best train loss: 0.2942  @ epoch 5613 )\n",
      "[Epoch: 5720] train loss: 0.3110, train acc: 0.8843, val loss: 0.2166, val acc: 0.9386  (best train acc: 0.9028, best val acc: 0.9420, best train loss: 0.2942  @ epoch 5613 )\n",
      "[Epoch: 5740] train loss: 0.3151, train acc: 0.8937, val loss: 0.2178, val acc: 0.9379  (best train acc: 0.9028, best val acc: 0.9420, best train loss: 0.2905  @ epoch 5733 )\n",
      "[Epoch: 5760] train loss: 0.3343, train acc: 0.8796, val loss: 0.2271, val acc: 0.9336  (best train acc: 0.9028, best val acc: 0.9423, best train loss: 0.2905  @ epoch 5733 )\n",
      "[Epoch: 5780] train loss: 0.3332, train acc: 0.8654, val loss: 0.2175, val acc: 0.9349  (best train acc: 0.9028, best val acc: 0.9423, best train loss: 0.2905  @ epoch 5733 )\n",
      "[Epoch: 5800] train loss: 0.3320, train acc: 0.8780, val loss: 0.2211, val acc: 0.9352  (best train acc: 0.9028, best val acc: 0.9423, best train loss: 0.2905  @ epoch 5733 )\n",
      "[Epoch: 5820] train loss: 0.3135, train acc: 0.8945, val loss: 0.2282, val acc: 0.9342  (best train acc: 0.9028, best val acc: 0.9423, best train loss: 0.2905  @ epoch 5733 )\n",
      "[Epoch: 5840] train loss: 0.3427, train acc: 0.8790, val loss: 0.2214, val acc: 0.9400  (best train acc: 0.9028, best val acc: 0.9423, best train loss: 0.2905  @ epoch 5733 )\n",
      "[Epoch: 5860] train loss: 0.3138, train acc: 0.8914, val loss: 0.2184, val acc: 0.9329  (best train acc: 0.9028, best val acc: 0.9444, best train loss: 0.2905  @ epoch 5733 )\n",
      "[Epoch: 5880] train loss: 0.3819, train acc: 0.8562, val loss: 0.2438, val acc: 0.9248  (best train acc: 0.9028, best val acc: 0.9444, best train loss: 0.2905  @ epoch 5733 )\n",
      "[Epoch: 5900] train loss: 0.4295, train acc: 0.8407, val loss: 0.2261, val acc: 0.9346  (best train acc: 0.9028, best val acc: 0.9444, best train loss: 0.2905  @ epoch 5733 )\n",
      "[Epoch: 5920] train loss: 0.4484, train acc: 0.8386, val loss: 0.2122, val acc: 0.9393  (best train acc: 0.9028, best val acc: 0.9444, best train loss: 0.2905  @ epoch 5733 )\n",
      "[Epoch: 5940] train loss: 0.3129, train acc: 0.8898, val loss: 0.2107, val acc: 0.9349  (best train acc: 0.9029, best val acc: 0.9444, best train loss: 0.2905  @ epoch 5733 )\n",
      "[Epoch: 5960] train loss: 0.2965, train acc: 0.8968, val loss: 0.2191, val acc: 0.9352  (best train acc: 0.9029, best val acc: 0.9444, best train loss: 0.2905  @ epoch 5733 )\n",
      "[Epoch: 5980] train loss: 0.3870, train acc: 0.8539, val loss: 0.2067, val acc: 0.9393  (best train acc: 0.9029, best val acc: 0.9444, best train loss: 0.2905  @ epoch 5733 )\n",
      "[Epoch: 6000] train loss: 0.3360, train acc: 0.8840, val loss: 0.2165, val acc: 0.9410  (best train acc: 0.9029, best val acc: 0.9444, best train loss: 0.2905  @ epoch 5733 )\n",
      "[Epoch: 6020] train loss: 0.3261, train acc: 0.8855, val loss: 0.2282, val acc: 0.9292  (best train acc: 0.9051, best val acc: 0.9444, best train loss: 0.2815  @ epoch 6004 )\n",
      "[Epoch: 6040] train loss: 0.3379, train acc: 0.8818, val loss: 0.2160, val acc: 0.9363  (best train acc: 0.9051, best val acc: 0.9444, best train loss: 0.2815  @ epoch 6004 )\n",
      "[Epoch: 6060] train loss: 0.2938, train acc: 0.8957, val loss: 0.2052, val acc: 0.9393  (best train acc: 0.9051, best val acc: 0.9444, best train loss: 0.2815  @ epoch 6004 )\n",
      "[Epoch: 6080] train loss: 0.3399, train acc: 0.8816, val loss: 0.2140, val acc: 0.9390  (best train acc: 0.9051, best val acc: 0.9444, best train loss: 0.2815  @ epoch 6004 )\n",
      "[Epoch: 6100] train loss: 0.3275, train acc: 0.8788, val loss: 0.2102, val acc: 0.9406  (best train acc: 0.9051, best val acc: 0.9444, best train loss: 0.2815  @ epoch 6004 )\n",
      "[Epoch: 6120] train loss: 0.4267, train acc: 0.8381, val loss: 0.2288, val acc: 0.9272  (best train acc: 0.9051, best val acc: 0.9444, best train loss: 0.2815  @ epoch 6004 )\n",
      "[Epoch: 6140] train loss: 0.3809, train acc: 0.8560, val loss: 0.2094, val acc: 0.9386  (best train acc: 0.9051, best val acc: 0.9444, best train loss: 0.2815  @ epoch 6004 )\n",
      "[Epoch: 6160] train loss: 0.3642, train acc: 0.8564, val loss: 0.2602, val acc: 0.9153  (best train acc: 0.9051, best val acc: 0.9444, best train loss: 0.2815  @ epoch 6004 )\n",
      "[Epoch: 6180] train loss: 0.3855, train acc: 0.8528, val loss: 0.2088, val acc: 0.9346  (best train acc: 0.9051, best val acc: 0.9444, best train loss: 0.2815  @ epoch 6004 )\n",
      "[Epoch: 6200] train loss: 0.3126, train acc: 0.8890, val loss: 0.2087, val acc: 0.9376  (best train acc: 0.9051, best val acc: 0.9444, best train loss: 0.2815  @ epoch 6004 )\n",
      "[Epoch: 6220] train loss: 0.2991, train acc: 0.8920, val loss: 0.2189, val acc: 0.9204  (best train acc: 0.9051, best val acc: 0.9444, best train loss: 0.2815  @ epoch 6004 )\n",
      "[Epoch: 6240] train loss: 0.3459, train acc: 0.8673, val loss: 0.2126, val acc: 0.9413  (best train acc: 0.9096, best val acc: 0.9444, best train loss: 0.2695  @ epoch 6222 )\n",
      "[Epoch: 6260] train loss: 0.3788, train acc: 0.8613, val loss: 0.2137, val acc: 0.9349  (best train acc: 0.9096, best val acc: 0.9444, best train loss: 0.2695  @ epoch 6222 )\n",
      "[Epoch: 6280] train loss: 0.3136, train acc: 0.8924, val loss: 0.2169, val acc: 0.9255  (best train acc: 0.9096, best val acc: 0.9444, best train loss: 0.2695  @ epoch 6222 )\n",
      "[Epoch: 6300] train loss: 0.4231, train acc: 0.8469, val loss: 0.2183, val acc: 0.9346  (best train acc: 0.9096, best val acc: 0.9444, best train loss: 0.2695  @ epoch 6222 )\n",
      "[Epoch: 6320] train loss: 0.3617, train acc: 0.8690, val loss: 0.2301, val acc: 0.9298  (best train acc: 0.9096, best val acc: 0.9444, best train loss: 0.2695  @ epoch 6222 )\n",
      "[Epoch: 6340] train loss: 0.3326, train acc: 0.8744, val loss: 0.2061, val acc: 0.9400  (best train acc: 0.9096, best val acc: 0.9444, best train loss: 0.2695  @ epoch 6222 )\n",
      "[Epoch: 6360] train loss: 0.3335, train acc: 0.8800, val loss: 0.2001, val acc: 0.9393  (best train acc: 0.9096, best val acc: 0.9444, best train loss: 0.2695  @ epoch 6222 )\n",
      "[Epoch: 6380] train loss: 0.2826, train acc: 0.9041, val loss: 0.2142, val acc: 0.9417  (best train acc: 0.9096, best val acc: 0.9447, best train loss: 0.2695  @ epoch 6222 )\n",
      "[Epoch: 6400] train loss: 0.2985, train acc: 0.8973, val loss: 0.2051, val acc: 0.9420  (best train acc: 0.9096, best val acc: 0.9447, best train loss: 0.2695  @ epoch 6222 )\n",
      "[Epoch: 6420] train loss: 0.3353, train acc: 0.8850, val loss: 0.2023, val acc: 0.9373  (best train acc: 0.9096, best val acc: 0.9447, best train loss: 0.2695  @ epoch 6222 )\n",
      "[Epoch: 6440] train loss: 0.3595, train acc: 0.8571, val loss: 0.2085, val acc: 0.9403  (best train acc: 0.9096, best val acc: 0.9447, best train loss: 0.2695  @ epoch 6222 )\n",
      "[Epoch: 6460] train loss: 0.3301, train acc: 0.8793, val loss: 0.2116, val acc: 0.9444  (best train acc: 0.9096, best val acc: 0.9447, best train loss: 0.2695  @ epoch 6222 )\n",
      "[Epoch: 6480] train loss: 0.3545, train acc: 0.8715, val loss: 0.2082, val acc: 0.9413  (best train acc: 0.9096, best val acc: 0.9447, best train loss: 0.2695  @ epoch 6222 )\n",
      "[Epoch: 6500] train loss: 0.3200, train acc: 0.8865, val loss: 0.2050, val acc: 0.9390  (best train acc: 0.9096, best val acc: 0.9447, best train loss: 0.2695  @ epoch 6222 )\n",
      "[Epoch: 6520] train loss: 0.3247, train acc: 0.8827, val loss: 0.2061, val acc: 0.9322  (best train acc: 0.9096, best val acc: 0.9460, best train loss: 0.2695  @ epoch 6222 )\n",
      "[Epoch: 6540] train loss: 0.4161, train acc: 0.8410, val loss: 0.2321, val acc: 0.9356  (best train acc: 0.9096, best val acc: 0.9460, best train loss: 0.2695  @ epoch 6222 )\n",
      "[Epoch: 6560] train loss: 0.3050, train acc: 0.8922, val loss: 0.2071, val acc: 0.9302  (best train acc: 0.9096, best val acc: 0.9460, best train loss: 0.2695  @ epoch 6222 )\n",
      "[Epoch: 6580] train loss: 0.3109, train acc: 0.8787, val loss: 0.1978, val acc: 0.9406  (best train acc: 0.9096, best val acc: 0.9460, best train loss: 0.2695  @ epoch 6222 )\n",
      "[Epoch: 6600] train loss: 0.3778, train acc: 0.8569, val loss: 0.2091, val acc: 0.9403  (best train acc: 0.9096, best val acc: 0.9484, best train loss: 0.2695  @ epoch 6222 )\n",
      "[Epoch: 6620] train loss: 0.3084, train acc: 0.8890, val loss: 0.2071, val acc: 0.9423  (best train acc: 0.9096, best val acc: 0.9484, best train loss: 0.2695  @ epoch 6222 )\n",
      "[Epoch: 6640] train loss: 0.3274, train acc: 0.8915, val loss: 0.2020, val acc: 0.9420  (best train acc: 0.9096, best val acc: 0.9484, best train loss: 0.2695  @ epoch 6222 )\n",
      "[Epoch: 6660] train loss: 0.3812, train acc: 0.8557, val loss: 0.2048, val acc: 0.9417  (best train acc: 0.9096, best val acc: 0.9484, best train loss: 0.2695  @ epoch 6222 )\n",
      "[Epoch: 6680] train loss: 0.2995, train acc: 0.8921, val loss: 0.2048, val acc: 0.9390  (best train acc: 0.9096, best val acc: 0.9484, best train loss: 0.2695  @ epoch 6222 )\n",
      "[Epoch: 6700] train loss: 0.3184, train acc: 0.8855, val loss: 0.2007, val acc: 0.9433  (best train acc: 0.9096, best val acc: 0.9484, best train loss: 0.2695  @ epoch 6222 )\n",
      "[Epoch: 6720] train loss: 0.3341, train acc: 0.8737, val loss: 0.1940, val acc: 0.9444  (best train acc: 0.9096, best val acc: 0.9484, best train loss: 0.2695  @ epoch 6222 )\n",
      "[Epoch: 6740] train loss: 0.3124, train acc: 0.8864, val loss: 0.2056, val acc: 0.9440  (best train acc: 0.9096, best val acc: 0.9484, best train loss: 0.2695  @ epoch 6222 )\n",
      "[Epoch: 6760] train loss: 0.3040, train acc: 0.8986, val loss: 0.1971, val acc: 0.9454  (best train acc: 0.9096, best val acc: 0.9484, best train loss: 0.2695  @ epoch 6222 )\n",
      "[Epoch: 6780] train loss: 0.3343, train acc: 0.8775, val loss: 0.2437, val acc: 0.9089  (best train acc: 0.9096, best val acc: 0.9484, best train loss: 0.2695  @ epoch 6222 )\n",
      "[Epoch: 6800] train loss: 0.3609, train acc: 0.8732, val loss: 0.2188, val acc: 0.9302  (best train acc: 0.9096, best val acc: 0.9484, best train loss: 0.2695  @ epoch 6222 )\n",
      "[Epoch: 6820] train loss: 0.3201, train acc: 0.8853, val loss: 0.2027, val acc: 0.9467  (best train acc: 0.9096, best val acc: 0.9484, best train loss: 0.2695  @ epoch 6222 )\n",
      "[Epoch: 6840] train loss: 0.3382, train acc: 0.8737, val loss: 0.1959, val acc: 0.9393  (best train acc: 0.9125, best val acc: 0.9484, best train loss: 0.2695  @ epoch 6222 )\n",
      "[Epoch: 6860] train loss: 0.4464, train acc: 0.8488, val loss: 0.2092, val acc: 0.9447  (best train acc: 0.9125, best val acc: 0.9484, best train loss: 0.2695  @ epoch 6222 )\n",
      "[Epoch: 6880] train loss: 0.3211, train acc: 0.8849, val loss: 0.2027, val acc: 0.9450  (best train acc: 0.9125, best val acc: 0.9484, best train loss: 0.2695  @ epoch 6222 )\n",
      "[Epoch: 6900] train loss: 0.3066, train acc: 0.8939, val loss: 0.2049, val acc: 0.9383  (best train acc: 0.9125, best val acc: 0.9497, best train loss: 0.2695  @ epoch 6222 )\n",
      "[Epoch: 6920] train loss: 0.3638, train acc: 0.8698, val loss: 0.2197, val acc: 0.9319  (best train acc: 0.9125, best val acc: 0.9497, best train loss: 0.2695  @ epoch 6222 )\n",
      "[Epoch: 6940] train loss: 0.3072, train acc: 0.8913, val loss: 0.2146, val acc: 0.9325  (best train acc: 0.9125, best val acc: 0.9497, best train loss: 0.2695  @ epoch 6222 )\n",
      "[Epoch: 6960] train loss: 0.4139, train acc: 0.8566, val loss: 0.1941, val acc: 0.9447  (best train acc: 0.9125, best val acc: 0.9497, best train loss: 0.2695  @ epoch 6222 )\n",
      "[Epoch: 6980] train loss: 0.3386, train acc: 0.8722, val loss: 0.1949, val acc: 0.9433  (best train acc: 0.9125, best val acc: 0.9497, best train loss: 0.2695  @ epoch 6222 )\n",
      "[Epoch: 7000] train loss: 0.3430, train acc: 0.8708, val loss: 0.1923, val acc: 0.9484  (best train acc: 0.9125, best val acc: 0.9497, best train loss: 0.2695  @ epoch 6222 )\n",
      "[Epoch: 7020] train loss: 0.3018, train acc: 0.8854, val loss: 0.1956, val acc: 0.9481  (best train acc: 0.9125, best val acc: 0.9497, best train loss: 0.2695  @ epoch 6222 )\n",
      "[Epoch: 7040] train loss: 0.3340, train acc: 0.8727, val loss: 0.1968, val acc: 0.9450  (best train acc: 0.9125, best val acc: 0.9497, best train loss: 0.2695  @ epoch 6222 )\n",
      "[Epoch: 7060] train loss: 0.3432, train acc: 0.8862, val loss: 0.1996, val acc: 0.9427  (best train acc: 0.9125, best val acc: 0.9497, best train loss: 0.2695  @ epoch 6222 )\n",
      "[Epoch: 7080] train loss: 0.3384, train acc: 0.8824, val loss: 0.2199, val acc: 0.9265  (best train acc: 0.9125, best val acc: 0.9497, best train loss: 0.2695  @ epoch 6222 )\n",
      "[Epoch: 7100] train loss: 0.3569, train acc: 0.8592, val loss: 0.1943, val acc: 0.9447  (best train acc: 0.9125, best val acc: 0.9497, best train loss: 0.2695  @ epoch 6222 )\n",
      "[Epoch: 7120] train loss: 0.3115, train acc: 0.8849, val loss: 0.2077, val acc: 0.9393  (best train acc: 0.9125, best val acc: 0.9497, best train loss: 0.2695  @ epoch 6222 )\n",
      "[Epoch: 7140] train loss: 0.3738, train acc: 0.8624, val loss: 0.1937, val acc: 0.9460  (best train acc: 0.9125, best val acc: 0.9497, best train loss: 0.2695  @ epoch 6222 )\n",
      "[Epoch: 7160] train loss: 0.3077, train acc: 0.8895, val loss: 0.1960, val acc: 0.9481  (best train acc: 0.9125, best val acc: 0.9497, best train loss: 0.2695  @ epoch 6222 )\n",
      "[Epoch: 7180] train loss: 0.4279, train acc: 0.8414, val loss: 0.1895, val acc: 0.9474  (best train acc: 0.9125, best val acc: 0.9497, best train loss: 0.2695  @ epoch 6222 )\n",
      "[Epoch: 7200] train loss: 0.4292, train acc: 0.8482, val loss: 0.2034, val acc: 0.9393  (best train acc: 0.9125, best val acc: 0.9497, best train loss: 0.2695  @ epoch 6222 )\n",
      "[Epoch: 7220] train loss: 0.3680, train acc: 0.8611, val loss: 0.1996, val acc: 0.9447  (best train acc: 0.9125, best val acc: 0.9497, best train loss: 0.2695  @ epoch 6222 )\n",
      "[Epoch: 7240] train loss: 0.3132, train acc: 0.8897, val loss: 0.1958, val acc: 0.9393  (best train acc: 0.9125, best val acc: 0.9497, best train loss: 0.2695  @ epoch 6222 )\n",
      "[Epoch: 7260] train loss: 0.3360, train acc: 0.8832, val loss: 0.1930, val acc: 0.9447  (best train acc: 0.9125, best val acc: 0.9497, best train loss: 0.2695  @ epoch 6222 )\n",
      "[Epoch: 7280] train loss: 0.3108, train acc: 0.8864, val loss: 0.1937, val acc: 0.9470  (best train acc: 0.9125, best val acc: 0.9497, best train loss: 0.2695  @ epoch 6222 )\n",
      "[Epoch: 7300] train loss: 0.2833, train acc: 0.9117, val loss: 0.1982, val acc: 0.9454  (best train acc: 0.9125, best val acc: 0.9497, best train loss: 0.2695  @ epoch 6222 )\n",
      "[Epoch: 7320] train loss: 0.3085, train acc: 0.8973, val loss: 0.1926, val acc: 0.9417  (best train acc: 0.9125, best val acc: 0.9497, best train loss: 0.2695  @ epoch 6222 )\n",
      "[Epoch: 7340] train loss: 0.4062, train acc: 0.8583, val loss: 0.2314, val acc: 0.9238  (best train acc: 0.9125, best val acc: 0.9497, best train loss: 0.2695  @ epoch 6222 )\n",
      "[Epoch: 7360] train loss: 0.3220, train acc: 0.8854, val loss: 0.1922, val acc: 0.9423  (best train acc: 0.9125, best val acc: 0.9518, best train loss: 0.2695  @ epoch 6222 )\n",
      "[Epoch: 7380] train loss: 0.2766, train acc: 0.9072, val loss: 0.2189, val acc: 0.9157  (best train acc: 0.9125, best val acc: 0.9518, best train loss: 0.2695  @ epoch 6222 )\n",
      "[Epoch: 7400] train loss: 0.2989, train acc: 0.8916, val loss: 0.1897, val acc: 0.9433  (best train acc: 0.9125, best val acc: 0.9518, best train loss: 0.2695  @ epoch 6222 )\n",
      "[Epoch: 7420] train loss: 0.3473, train acc: 0.8652, val loss: 0.1980, val acc: 0.9420  (best train acc: 0.9125, best val acc: 0.9518, best train loss: 0.2695  @ epoch 6222 )\n",
      "[Epoch: 7440] train loss: 0.3320, train acc: 0.8923, val loss: 0.1896, val acc: 0.9457  (best train acc: 0.9125, best val acc: 0.9518, best train loss: 0.2695  @ epoch 6222 )\n",
      "[Epoch: 7460] train loss: 0.3265, train acc: 0.8776, val loss: 0.1914, val acc: 0.9508  (best train acc: 0.9125, best val acc: 0.9518, best train loss: 0.2695  @ epoch 6222 )\n",
      "[Epoch: 7480] train loss: 0.3196, train acc: 0.8897, val loss: 0.1828, val acc: 0.9474  (best train acc: 0.9125, best val acc: 0.9518, best train loss: 0.2695  @ epoch 6222 )\n",
      "[Epoch: 7500] train loss: 0.3117, train acc: 0.8853, val loss: 0.1904, val acc: 0.9470  (best train acc: 0.9125, best val acc: 0.9518, best train loss: 0.2695  @ epoch 6222 )\n",
      "[Epoch: 7520] train loss: 0.2813, train acc: 0.9046, val loss: 0.1879, val acc: 0.9440  (best train acc: 0.9125, best val acc: 0.9518, best train loss: 0.2695  @ epoch 6222 )\n",
      "[Epoch: 7540] train loss: 0.3306, train acc: 0.8786, val loss: 0.1951, val acc: 0.9454  (best train acc: 0.9125, best val acc: 0.9518, best train loss: 0.2695  @ epoch 6222 )\n",
      "[Epoch: 7560] train loss: 0.3837, train acc: 0.8564, val loss: 0.1861, val acc: 0.9447  (best train acc: 0.9125, best val acc: 0.9518, best train loss: 0.2695  @ epoch 6222 )\n",
      "[Epoch: 7580] train loss: 0.3333, train acc: 0.8796, val loss: 0.1872, val acc: 0.9420  (best train acc: 0.9125, best val acc: 0.9518, best train loss: 0.2695  @ epoch 6222 )\n",
      "[Epoch: 7600] train loss: 0.3526, train acc: 0.8807, val loss: 0.1850, val acc: 0.9433  (best train acc: 0.9125, best val acc: 0.9518, best train loss: 0.2695  @ epoch 6222 )\n",
      "[Epoch: 7620] train loss: 0.4173, train acc: 0.8561, val loss: 0.1856, val acc: 0.9420  (best train acc: 0.9125, best val acc: 0.9518, best train loss: 0.2695  @ epoch 6222 )\n",
      "[Epoch: 7640] train loss: 0.2914, train acc: 0.9056, val loss: 0.1842, val acc: 0.9437  (best train acc: 0.9125, best val acc: 0.9518, best train loss: 0.2695  @ epoch 6222 )\n",
      "[Epoch: 7660] train loss: 0.3116, train acc: 0.8862, val loss: 0.1926, val acc: 0.9376  (best train acc: 0.9125, best val acc: 0.9518, best train loss: 0.2695  @ epoch 6222 )\n",
      "[Epoch: 7680] train loss: 0.2824, train acc: 0.9036, val loss: 0.2051, val acc: 0.9352  (best train acc: 0.9125, best val acc: 0.9518, best train loss: 0.2695  @ epoch 6222 )\n",
      "[Epoch: 7700] train loss: 0.2848, train acc: 0.9006, val loss: 0.2009, val acc: 0.9339  (best train acc: 0.9125, best val acc: 0.9518, best train loss: 0.2695  @ epoch 6222 )\n",
      "[Epoch: 7720] train loss: 0.3730, train acc: 0.8642, val loss: 0.1946, val acc: 0.9379  (best train acc: 0.9125, best val acc: 0.9518, best train loss: 0.2695  @ epoch 6222 )\n",
      "[Epoch: 7740] train loss: 0.3097, train acc: 0.8922, val loss: 0.1911, val acc: 0.9460  (best train acc: 0.9125, best val acc: 0.9518, best train loss: 0.2695  @ epoch 6222 )\n",
      "[Epoch: 7760] train loss: 0.3024, train acc: 0.8918, val loss: 0.1998, val acc: 0.9403  (best train acc: 0.9125, best val acc: 0.9518, best train loss: 0.2695  @ epoch 6222 )\n",
      "[Epoch: 7780] train loss: 0.2898, train acc: 0.8971, val loss: 0.1911, val acc: 0.9484  (best train acc: 0.9125, best val acc: 0.9518, best train loss: 0.2695  @ epoch 6222 )\n",
      "[Epoch: 7800] train loss: 0.3023, train acc: 0.8950, val loss: 0.1889, val acc: 0.9457  (best train acc: 0.9125, best val acc: 0.9518, best train loss: 0.2695  @ epoch 6222 )\n",
      "[Epoch: 7820] train loss: 0.2780, train acc: 0.9076, val loss: 0.1857, val acc: 0.9400  (best train acc: 0.9125, best val acc: 0.9518, best train loss: 0.2695  @ epoch 6222 )\n",
      "[Epoch: 7840] train loss: 0.3230, train acc: 0.8918, val loss: 0.1925, val acc: 0.9474  (best train acc: 0.9125, best val acc: 0.9518, best train loss: 0.2695  @ epoch 6222 )\n",
      "[Epoch: 7860] train loss: 0.3264, train acc: 0.8725, val loss: 0.1945, val acc: 0.9383  (best train acc: 0.9125, best val acc: 0.9518, best train loss: 0.2695  @ epoch 6222 )\n",
      "[Epoch: 7880] train loss: 0.3518, train acc: 0.8686, val loss: 0.1955, val acc: 0.9406  (best train acc: 0.9125, best val acc: 0.9518, best train loss: 0.2695  @ epoch 6222 )\n",
      "[Epoch: 7900] train loss: 0.3270, train acc: 0.8851, val loss: 0.2080, val acc: 0.9204  (best train acc: 0.9125, best val acc: 0.9518, best train loss: 0.2679  @ epoch 7885 )\n",
      "[Epoch: 7920] train loss: 0.3406, train acc: 0.8814, val loss: 0.1835, val acc: 0.9497  (best train acc: 0.9125, best val acc: 0.9518, best train loss: 0.2679  @ epoch 7885 )\n",
      "[Epoch: 7940] train loss: 0.3300, train acc: 0.8742, val loss: 0.1884, val acc: 0.9454  (best train acc: 0.9125, best val acc: 0.9518, best train loss: 0.2679  @ epoch 7885 )\n",
      "[Epoch: 7960] train loss: 0.3167, train acc: 0.8866, val loss: 0.1841, val acc: 0.9504  (best train acc: 0.9125, best val acc: 0.9518, best train loss: 0.2679  @ epoch 7885 )\n",
      "[Epoch: 7980] train loss: 0.3127, train acc: 0.8905, val loss: 0.1862, val acc: 0.9484  (best train acc: 0.9125, best val acc: 0.9521, best train loss: 0.2679  @ epoch 7885 )\n",
      "[Epoch: 8000] train loss: 0.3647, train acc: 0.8689, val loss: 0.1950, val acc: 0.9309  (best train acc: 0.9125, best val acc: 0.9521, best train loss: 0.2679  @ epoch 7885 )\n",
      "[Epoch: 8020] train loss: 0.2930, train acc: 0.8938, val loss: 0.1917, val acc: 0.9390  (best train acc: 0.9125, best val acc: 0.9521, best train loss: 0.2679  @ epoch 7885 )\n",
      "[Epoch: 8040] train loss: 0.3274, train acc: 0.8815, val loss: 0.1933, val acc: 0.9366  (best train acc: 0.9125, best val acc: 0.9521, best train loss: 0.2679  @ epoch 7885 )\n",
      "[Epoch: 8060] train loss: 0.3083, train acc: 0.8890, val loss: 0.1902, val acc: 0.9447  (best train acc: 0.9125, best val acc: 0.9521, best train loss: 0.2679  @ epoch 7885 )\n",
      "[Epoch: 8080] train loss: 0.4009, train acc: 0.8431, val loss: 0.1864, val acc: 0.9487  (best train acc: 0.9125, best val acc: 0.9521, best train loss: 0.2679  @ epoch 7885 )\n",
      "[Epoch: 8100] train loss: 0.3651, train acc: 0.8581, val loss: 0.1793, val acc: 0.9470  (best train acc: 0.9125, best val acc: 0.9521, best train loss: 0.2679  @ epoch 7885 )\n",
      "[Epoch: 8120] train loss: 0.3201, train acc: 0.8837, val loss: 0.1874, val acc: 0.9450  (best train acc: 0.9125, best val acc: 0.9521, best train loss: 0.2679  @ epoch 7885 )\n",
      "[Epoch: 8140] train loss: 0.4242, train acc: 0.8365, val loss: 0.1919, val acc: 0.9477  (best train acc: 0.9125, best val acc: 0.9521, best train loss: 0.2662  @ epoch 8134 )\n",
      "[Epoch: 8160] train loss: 0.2956, train acc: 0.8963, val loss: 0.1813, val acc: 0.9494  (best train acc: 0.9125, best val acc: 0.9521, best train loss: 0.2662  @ epoch 8134 )\n",
      "[Epoch: 8180] train loss: 0.3443, train acc: 0.8756, val loss: 0.1818, val acc: 0.9447  (best train acc: 0.9125, best val acc: 0.9521, best train loss: 0.2662  @ epoch 8134 )\n",
      "[Epoch: 8200] train loss: 0.2919, train acc: 0.8978, val loss: 0.1855, val acc: 0.9410  (best train acc: 0.9125, best val acc: 0.9521, best train loss: 0.2662  @ epoch 8134 )\n",
      "[Epoch: 8220] train loss: 0.2767, train acc: 0.9049, val loss: 0.1841, val acc: 0.9454  (best train acc: 0.9125, best val acc: 0.9521, best train loss: 0.2662  @ epoch 8134 )\n",
      "[Epoch: 8240] train loss: 0.3229, train acc: 0.8838, val loss: 0.1841, val acc: 0.9460  (best train acc: 0.9125, best val acc: 0.9521, best train loss: 0.2662  @ epoch 8134 )\n",
      "[Epoch: 8260] train loss: 0.3487, train acc: 0.8728, val loss: 0.1968, val acc: 0.9413  (best train acc: 0.9125, best val acc: 0.9521, best train loss: 0.2662  @ epoch 8134 )\n",
      "[Epoch: 8280] train loss: 0.2867, train acc: 0.8977, val loss: 0.1764, val acc: 0.9470  (best train acc: 0.9125, best val acc: 0.9521, best train loss: 0.2662  @ epoch 8134 )\n",
      "[Epoch: 8300] train loss: 0.3404, train acc: 0.8839, val loss: 0.1756, val acc: 0.9491  (best train acc: 0.9157, best val acc: 0.9521, best train loss: 0.2566  @ epoch 8296 )\n",
      "[Epoch: 8320] train loss: 0.2799, train acc: 0.9059, val loss: 0.1847, val acc: 0.9447  (best train acc: 0.9157, best val acc: 0.9521, best train loss: 0.2566  @ epoch 8296 )\n",
      "[Epoch: 8340] train loss: 0.4075, train acc: 0.8573, val loss: 0.2024, val acc: 0.9275  (best train acc: 0.9157, best val acc: 0.9521, best train loss: 0.2566  @ epoch 8296 )\n",
      "[Epoch: 8360] train loss: 0.4081, train acc: 0.8571, val loss: 0.1777, val acc: 0.9484  (best train acc: 0.9157, best val acc: 0.9521, best train loss: 0.2566  @ epoch 8296 )\n",
      "[Epoch: 8380] train loss: 0.3062, train acc: 0.8964, val loss: 0.2012, val acc: 0.9261  (best train acc: 0.9157, best val acc: 0.9521, best train loss: 0.2566  @ epoch 8296 )\n",
      "[Epoch: 8400] train loss: 0.4192, train acc: 0.8433, val loss: 0.1837, val acc: 0.9464  (best train acc: 0.9203, best val acc: 0.9521, best train loss: 0.2528  @ epoch 8399 )\n",
      "[Epoch: 8420] train loss: 0.3276, train acc: 0.8832, val loss: 0.1788, val acc: 0.9477  (best train acc: 0.9203, best val acc: 0.9521, best train loss: 0.2528  @ epoch 8399 )\n",
      "[Epoch: 8440] train loss: 0.3053, train acc: 0.8866, val loss: 0.1907, val acc: 0.9447  (best train acc: 0.9203, best val acc: 0.9521, best train loss: 0.2528  @ epoch 8399 )\n",
      "[Epoch: 8460] train loss: 0.2869, train acc: 0.8989, val loss: 0.1842, val acc: 0.9433  (best train acc: 0.9203, best val acc: 0.9521, best train loss: 0.2528  @ epoch 8399 )\n",
      "[Epoch: 8480] train loss: 0.3557, train acc: 0.8731, val loss: 0.1866, val acc: 0.9484  (best train acc: 0.9203, best val acc: 0.9521, best train loss: 0.2528  @ epoch 8399 )\n",
      "[Epoch: 8500] train loss: 0.3907, train acc: 0.8568, val loss: 0.1855, val acc: 0.9379  (best train acc: 0.9203, best val acc: 0.9521, best train loss: 0.2528  @ epoch 8399 )\n",
      "[Epoch: 8520] train loss: 0.2989, train acc: 0.8936, val loss: 0.1720, val acc: 0.9481  (best train acc: 0.9203, best val acc: 0.9521, best train loss: 0.2528  @ epoch 8399 )\n",
      "[Epoch: 8540] train loss: 0.4180, train acc: 0.8435, val loss: 0.1771, val acc: 0.9427  (best train acc: 0.9203, best val acc: 0.9521, best train loss: 0.2528  @ epoch 8399 )\n",
      "[Epoch: 8560] train loss: 0.3573, train acc: 0.8602, val loss: 0.1960, val acc: 0.9400  (best train acc: 0.9203, best val acc: 0.9521, best train loss: 0.2528  @ epoch 8399 )\n",
      "[Epoch: 8580] train loss: 0.3226, train acc: 0.8775, val loss: 0.1827, val acc: 0.9454  (best train acc: 0.9203, best val acc: 0.9521, best train loss: 0.2528  @ epoch 8399 )\n",
      "[Epoch: 8600] train loss: 0.3046, train acc: 0.8903, val loss: 0.1776, val acc: 0.9464  (best train acc: 0.9203, best val acc: 0.9521, best train loss: 0.2528  @ epoch 8399 )\n",
      "[Epoch: 8620] train loss: 0.3527, train acc: 0.8717, val loss: 0.1814, val acc: 0.9447  (best train acc: 0.9203, best val acc: 0.9521, best train loss: 0.2528  @ epoch 8399 )\n",
      "[Epoch: 8640] train loss: 0.3058, train acc: 0.8891, val loss: 0.1792, val acc: 0.9467  (best train acc: 0.9203, best val acc: 0.9521, best train loss: 0.2501  @ epoch 8632 )\n",
      "[Epoch: 8660] train loss: 0.2849, train acc: 0.9028, val loss: 0.1772, val acc: 0.9474  (best train acc: 0.9203, best val acc: 0.9521, best train loss: 0.2501  @ epoch 8632 )\n",
      "[Epoch: 8680] train loss: 0.2933, train acc: 0.8936, val loss: 0.1737, val acc: 0.9457  (best train acc: 0.9203, best val acc: 0.9521, best train loss: 0.2501  @ epoch 8632 )\n",
      "[Epoch: 8700] train loss: 0.2814, train acc: 0.8947, val loss: 0.1671, val acc: 0.9470  (best train acc: 0.9203, best val acc: 0.9521, best train loss: 0.2501  @ epoch 8632 )\n",
      "[Epoch: 8720] train loss: 0.3053, train acc: 0.8848, val loss: 0.1693, val acc: 0.9450  (best train acc: 0.9203, best val acc: 0.9521, best train loss: 0.2501  @ epoch 8632 )\n",
      "[Epoch: 8740] train loss: 0.2584, train acc: 0.9104, val loss: 0.1670, val acc: 0.9454  (best train acc: 0.9203, best val acc: 0.9521, best train loss: 0.2501  @ epoch 8632 )\n",
      "[Epoch: 8760] train loss: 0.2757, train acc: 0.9040, val loss: 0.1681, val acc: 0.9501  (best train acc: 0.9203, best val acc: 0.9521, best train loss: 0.2501  @ epoch 8632 )\n",
      "[Epoch: 8780] train loss: 0.2943, train acc: 0.8877, val loss: 0.1717, val acc: 0.9366  (best train acc: 0.9203, best val acc: 0.9521, best train loss: 0.2501  @ epoch 8632 )\n",
      "[Epoch: 8800] train loss: 0.2967, train acc: 0.8891, val loss: 0.1707, val acc: 0.9430  (best train acc: 0.9203, best val acc: 0.9521, best train loss: 0.2501  @ epoch 8632 )\n",
      "[Epoch: 8820] train loss: 0.2716, train acc: 0.8978, val loss: 0.1717, val acc: 0.9477  (best train acc: 0.9203, best val acc: 0.9521, best train loss: 0.2501  @ epoch 8632 )\n",
      "[Epoch: 8840] train loss: 0.2658, train acc: 0.9028, val loss: 0.1709, val acc: 0.9457  (best train acc: 0.9203, best val acc: 0.9521, best train loss: 0.2501  @ epoch 8632 )\n",
      "[Epoch: 8860] train loss: 0.3132, train acc: 0.8746, val loss: 0.1618, val acc: 0.9437  (best train acc: 0.9203, best val acc: 0.9521, best train loss: 0.2399  @ epoch 8841 )\n",
      "[Epoch: 8880] train loss: 0.2637, train acc: 0.8902, val loss: 0.1707, val acc: 0.9423  (best train acc: 0.9203, best val acc: 0.9521, best train loss: 0.2399  @ epoch 8841 )\n",
      "[Epoch: 8900] train loss: 0.2453, train acc: 0.9048, val loss: 0.1630, val acc: 0.9444  (best train acc: 0.9203, best val acc: 0.9521, best train loss: 0.2386  @ epoch 8896 )\n",
      "[Epoch: 8920] train loss: 0.2530, train acc: 0.9088, val loss: 0.1613, val acc: 0.9487  (best train acc: 0.9203, best val acc: 0.9521, best train loss: 0.2373  @ epoch 8915 )\n",
      "[Epoch: 8940] train loss: 0.2397, train acc: 0.9138, val loss: 0.1718, val acc: 0.9383  (best train acc: 0.9203, best val acc: 0.9524, best train loss: 0.2373  @ epoch 8915 )\n",
      "[Epoch: 8960] train loss: 0.2716, train acc: 0.9030, val loss: 0.1763, val acc: 0.9504  (best train acc: 0.9203, best val acc: 0.9524, best train loss: 0.2373  @ epoch 8915 )\n",
      "[Epoch: 8980] train loss: 0.2612, train acc: 0.9046, val loss: 0.1929, val acc: 0.9363  (best train acc: 0.9203, best val acc: 0.9524, best train loss: 0.2373  @ epoch 8915 )\n",
      "[Epoch: 9000] train loss: 0.2717, train acc: 0.9017, val loss: 0.1629, val acc: 0.9467  (best train acc: 0.9226, best val acc: 0.9524, best train loss: 0.2357  @ epoch 8992 )\n",
      "[Epoch: 9020] train loss: 0.2902, train acc: 0.8990, val loss: 0.1904, val acc: 0.9373  (best train acc: 0.9226, best val acc: 0.9524, best train loss: 0.2357  @ epoch 8992 )\n",
      "[Epoch: 9040] train loss: 0.2423, train acc: 0.9127, val loss: 0.1635, val acc: 0.9491  (best train acc: 0.9226, best val acc: 0.9524, best train loss: 0.2357  @ epoch 8992 )\n",
      "[Epoch: 9060] train loss: 0.2502, train acc: 0.9099, val loss: 0.1738, val acc: 0.9457  (best train acc: 0.9226, best val acc: 0.9524, best train loss: 0.2357  @ epoch 8992 )\n",
      "[Epoch: 9080] train loss: 0.2635, train acc: 0.9062, val loss: 0.1641, val acc: 0.9470  (best train acc: 0.9226, best val acc: 0.9524, best train loss: 0.2348  @ epoch 9079 )\n",
      "[Epoch: 9100] train loss: 0.2520, train acc: 0.9151, val loss: 0.1662, val acc: 0.9494  (best train acc: 0.9226, best val acc: 0.9524, best train loss: 0.2348  @ epoch 9079 )\n",
      "[Epoch: 9120] train loss: 0.2776, train acc: 0.8947, val loss: 0.1666, val acc: 0.9494  (best train acc: 0.9226, best val acc: 0.9524, best train loss: 0.2348  @ epoch 9079 )\n",
      "[Epoch: 9140] train loss: 0.2472, train acc: 0.9156, val loss: 0.1669, val acc: 0.9497  (best train acc: 0.9226, best val acc: 0.9524, best train loss: 0.2348  @ epoch 9079 )\n",
      "[Epoch: 9160] train loss: 0.2713, train acc: 0.8984, val loss: 0.1683, val acc: 0.9470  (best train acc: 0.9226, best val acc: 0.9524, best train loss: 0.2254  @ epoch 9149 )\n",
      "[Epoch: 9180] train loss: 0.3518, train acc: 0.8592, val loss: 0.1734, val acc: 0.9481  (best train acc: 0.9226, best val acc: 0.9524, best train loss: 0.2254  @ epoch 9149 )\n",
      "[Epoch: 9200] train loss: 0.2851, train acc: 0.9031, val loss: 0.1676, val acc: 0.9487  (best train acc: 0.9226, best val acc: 0.9524, best train loss: 0.2254  @ epoch 9149 )\n",
      "[Epoch: 9220] train loss: 0.2976, train acc: 0.8927, val loss: 0.1689, val acc: 0.9504  (best train acc: 0.9226, best val acc: 0.9524, best train loss: 0.2254  @ epoch 9149 )\n",
      "[Epoch: 9240] train loss: 0.2641, train acc: 0.9068, val loss: 0.1697, val acc: 0.9457  (best train acc: 0.9226, best val acc: 0.9524, best train loss: 0.2239  @ epoch 9231 )\n",
      "[Epoch: 9260] train loss: 0.2705, train acc: 0.8954, val loss: 0.1605, val acc: 0.9450  (best train acc: 0.9226, best val acc: 0.9524, best train loss: 0.2239  @ epoch 9231 )\n",
      "[Epoch: 9280] train loss: 0.3823, train acc: 0.8521, val loss: 0.1625, val acc: 0.9477  (best train acc: 0.9226, best val acc: 0.9528, best train loss: 0.2239  @ epoch 9231 )\n",
      "[Epoch: 9300] train loss: 0.2419, train acc: 0.9131, val loss: 0.1670, val acc: 0.9450  (best train acc: 0.9238, best val acc: 0.9528, best train loss: 0.2239  @ epoch 9231 )\n",
      "[Epoch: 9320] train loss: 0.2447, train acc: 0.9160, val loss: 0.1722, val acc: 0.9484  (best train acc: 0.9238, best val acc: 0.9528, best train loss: 0.2239  @ epoch 9231 )\n",
      "[Epoch: 9340] train loss: 0.2909, train acc: 0.8957, val loss: 0.1599, val acc: 0.9504  (best train acc: 0.9238, best val acc: 0.9528, best train loss: 0.2239  @ epoch 9231 )\n",
      "[Epoch: 9360] train loss: 0.2842, train acc: 0.8935, val loss: 0.1883, val acc: 0.9369  (best train acc: 0.9238, best val acc: 0.9528, best train loss: 0.2239  @ epoch 9231 )\n",
      "[Epoch: 9380] train loss: 0.2625, train acc: 0.9049, val loss: 0.1613, val acc: 0.9518  (best train acc: 0.9238, best val acc: 0.9528, best train loss: 0.2239  @ epoch 9231 )\n",
      "[Epoch: 9400] train loss: 0.2403, train acc: 0.9091, val loss: 0.1622, val acc: 0.9484  (best train acc: 0.9238, best val acc: 0.9528, best train loss: 0.2239  @ epoch 9231 )\n",
      "[Epoch: 9420] train loss: 0.2383, train acc: 0.9156, val loss: 0.1674, val acc: 0.9440  (best train acc: 0.9238, best val acc: 0.9528, best train loss: 0.2239  @ epoch 9231 )\n",
      "[Epoch: 9440] train loss: 0.3597, train acc: 0.8695, val loss: 0.1699, val acc: 0.9487  (best train acc: 0.9250, best val acc: 0.9528, best train loss: 0.2239  @ epoch 9231 )\n",
      "[Epoch: 9460] train loss: 0.2503, train acc: 0.9118, val loss: 0.1753, val acc: 0.9481  (best train acc: 0.9250, best val acc: 0.9528, best train loss: 0.2239  @ epoch 9231 )\n",
      "[Epoch: 9480] train loss: 0.2944, train acc: 0.8895, val loss: 0.1667, val acc: 0.9511  (best train acc: 0.9250, best val acc: 0.9528, best train loss: 0.2239  @ epoch 9231 )\n",
      "[Epoch: 9500] train loss: 0.2829, train acc: 0.8960, val loss: 0.1716, val acc: 0.9366  (best train acc: 0.9250, best val acc: 0.9528, best train loss: 0.2239  @ epoch 9231 )\n",
      "[Epoch: 9520] train loss: 0.2413, train acc: 0.9158, val loss: 0.1666, val acc: 0.9501  (best train acc: 0.9250, best val acc: 0.9545, best train loss: 0.2239  @ epoch 9231 )\n",
      "[Epoch: 9540] train loss: 0.3146, train acc: 0.8764, val loss: 0.1662, val acc: 0.9474  (best train acc: 0.9250, best val acc: 0.9545, best train loss: 0.2239  @ epoch 9231 )\n",
      "[Epoch: 9560] train loss: 0.2601, train acc: 0.9080, val loss: 0.1760, val acc: 0.9481  (best train acc: 0.9250, best val acc: 0.9545, best train loss: 0.2239  @ epoch 9231 )\n",
      "[Epoch: 9580] train loss: 0.2251, train acc: 0.9245, val loss: 0.1703, val acc: 0.9396  (best train acc: 0.9250, best val acc: 0.9545, best train loss: 0.2239  @ epoch 9231 )\n",
      "[Epoch: 9600] train loss: 0.2798, train acc: 0.9020, val loss: 0.1617, val acc: 0.9467  (best train acc: 0.9250, best val acc: 0.9545, best train loss: 0.2239  @ epoch 9231 )\n",
      "[Epoch: 9620] train loss: 0.3070, train acc: 0.8769, val loss: 0.1726, val acc: 0.9457  (best train acc: 0.9250, best val acc: 0.9545, best train loss: 0.2239  @ epoch 9231 )\n",
      "[Epoch: 9640] train loss: 0.3153, train acc: 0.8742, val loss: 0.1690, val acc: 0.9474  (best train acc: 0.9250, best val acc: 0.9545, best train loss: 0.2239  @ epoch 9231 )\n",
      "[Epoch: 9660] train loss: 0.2622, train acc: 0.9082, val loss: 0.1639, val acc: 0.9531  (best train acc: 0.9250, best val acc: 0.9545, best train loss: 0.2239  @ epoch 9231 )\n",
      "[Epoch: 9680] train loss: 0.2722, train acc: 0.9033, val loss: 0.1695, val acc: 0.9457  (best train acc: 0.9250, best val acc: 0.9545, best train loss: 0.2239  @ epoch 9231 )\n",
      "[Epoch: 9700] train loss: 0.2712, train acc: 0.9060, val loss: 0.1649, val acc: 0.9450  (best train acc: 0.9250, best val acc: 0.9545, best train loss: 0.2239  @ epoch 9231 )\n",
      "[Epoch: 9720] train loss: 0.2555, train acc: 0.9056, val loss: 0.1743, val acc: 0.9349  (best train acc: 0.9250, best val acc: 0.9545, best train loss: 0.2239  @ epoch 9231 )\n",
      "[Epoch: 9740] train loss: 0.2582, train acc: 0.9085, val loss: 0.1660, val acc: 0.9440  (best train acc: 0.9250, best val acc: 0.9545, best train loss: 0.2239  @ epoch 9231 )\n",
      "[Epoch: 9760] train loss: 0.2611, train acc: 0.9122, val loss: 0.1656, val acc: 0.9474  (best train acc: 0.9250, best val acc: 0.9545, best train loss: 0.2239  @ epoch 9231 )\n",
      "[Epoch: 9780] train loss: 0.2709, train acc: 0.9018, val loss: 0.1567, val acc: 0.9470  (best train acc: 0.9250, best val acc: 0.9545, best train loss: 0.2233  @ epoch 9771 )\n",
      "[Epoch: 9800] train loss: 0.2879, train acc: 0.8872, val loss: 0.1628, val acc: 0.9484  (best train acc: 0.9250, best val acc: 0.9545, best train loss: 0.2233  @ epoch 9771 )\n",
      "[Epoch: 9820] train loss: 0.2767, train acc: 0.8957, val loss: 0.1724, val acc: 0.9393  (best train acc: 0.9250, best val acc: 0.9545, best train loss: 0.2233  @ epoch 9771 )\n",
      "[Epoch: 9840] train loss: 0.2478, train acc: 0.9097, val loss: 0.1657, val acc: 0.9514  (best train acc: 0.9250, best val acc: 0.9545, best train loss: 0.2233  @ epoch 9771 )\n",
      "[Epoch: 9860] train loss: 0.2833, train acc: 0.8901, val loss: 0.1684, val acc: 0.9457  (best train acc: 0.9250, best val acc: 0.9545, best train loss: 0.2233  @ epoch 9771 )\n",
      "[Epoch: 9880] train loss: 0.2913, train acc: 0.8936, val loss: 0.1657, val acc: 0.9491  (best train acc: 0.9250, best val acc: 0.9545, best train loss: 0.2233  @ epoch 9771 )\n",
      "[Epoch: 9900] train loss: 0.2565, train acc: 0.9093, val loss: 0.1625, val acc: 0.9460  (best train acc: 0.9250, best val acc: 0.9545, best train loss: 0.2233  @ epoch 9771 )\n",
      "[Epoch: 9920] train loss: 0.2554, train acc: 0.9122, val loss: 0.1714, val acc: 0.9460  (best train acc: 0.9250, best val acc: 0.9545, best train loss: 0.2233  @ epoch 9771 )\n",
      "[Epoch: 9940] train loss: 0.2692, train acc: 0.9070, val loss: 0.1581, val acc: 0.9474  (best train acc: 0.9250, best val acc: 0.9545, best train loss: 0.2233  @ epoch 9771 )\n",
      "[Epoch: 9960] train loss: 0.3950, train acc: 0.8592, val loss: 0.1659, val acc: 0.9379  (best train acc: 0.9250, best val acc: 0.9545, best train loss: 0.2233  @ epoch 9771 )\n",
      "[Epoch: 9980] train loss: 0.2755, train acc: 0.9040, val loss: 0.1844, val acc: 0.9410  (best train acc: 0.9250, best val acc: 0.9545, best train loss: 0.2233  @ epoch 9771 )\n",
      "[Epoch: 10000] train loss: 0.2605, train acc: 0.9041, val loss: 0.1594, val acc: 0.9477  (best train acc: 0.9250, best val acc: 0.9545, best train loss: 0.2233  @ epoch 9771 )\n",
      "[Epoch: 10020] train loss: 0.3581, train acc: 0.8627, val loss: 0.1836, val acc: 0.9403  (best train acc: 0.9250, best val acc: 0.9545, best train loss: 0.2233  @ epoch 9771 )\n",
      "[Epoch: 10040] train loss: 0.2820, train acc: 0.8997, val loss: 0.1653, val acc: 0.9494  (best train acc: 0.9250, best val acc: 0.9545, best train loss: 0.2233  @ epoch 9771 )\n",
      "[Epoch: 10060] train loss: 0.2803, train acc: 0.9053, val loss: 0.1607, val acc: 0.9497  (best train acc: 0.9250, best val acc: 0.9545, best train loss: 0.2233  @ epoch 9771 )\n",
      "[Epoch: 10080] train loss: 0.3105, train acc: 0.8813, val loss: 0.1632, val acc: 0.9484  (best train acc: 0.9250, best val acc: 0.9545, best train loss: 0.2233  @ epoch 9771 )\n",
      "[Epoch: 10100] train loss: 0.3366, train acc: 0.8724, val loss: 0.1707, val acc: 0.9487  (best train acc: 0.9250, best val acc: 0.9545, best train loss: 0.2233  @ epoch 9771 )\n",
      "[Epoch: 10120] train loss: 0.2940, train acc: 0.8818, val loss: 0.1660, val acc: 0.9373  (best train acc: 0.9250, best val acc: 0.9545, best train loss: 0.2233  @ epoch 9771 )\n",
      "[Epoch: 10140] train loss: 0.2331, train acc: 0.9129, val loss: 0.1693, val acc: 0.9356  (best train acc: 0.9250, best val acc: 0.9545, best train loss: 0.2233  @ epoch 9771 )\n",
      "[Epoch: 10160] train loss: 0.2503, train acc: 0.9127, val loss: 0.1613, val acc: 0.9508  (best train acc: 0.9250, best val acc: 0.9545, best train loss: 0.2233  @ epoch 9771 )\n",
      "[Epoch: 10180] train loss: 0.2396, train acc: 0.9113, val loss: 0.1729, val acc: 0.9454  (best train acc: 0.9250, best val acc: 0.9545, best train loss: 0.2233  @ epoch 9771 )\n",
      "[Epoch: 10200] train loss: 0.3062, train acc: 0.8853, val loss: 0.1660, val acc: 0.9481  (best train acc: 0.9250, best val acc: 0.9545, best train loss: 0.2233  @ epoch 9771 )\n",
      "[Epoch: 10220] train loss: 0.3998, train acc: 0.8685, val loss: 0.1693, val acc: 0.9494  (best train acc: 0.9250, best val acc: 0.9545, best train loss: 0.2233  @ epoch 9771 )\n",
      "[Epoch: 10240] train loss: 0.2296, train acc: 0.9205, val loss: 0.1649, val acc: 0.9477  (best train acc: 0.9250, best val acc: 0.9545, best train loss: 0.2233  @ epoch 9771 )\n",
      "[Epoch: 10260] train loss: 0.2468, train acc: 0.9089, val loss: 0.1593, val acc: 0.9464  (best train acc: 0.9250, best val acc: 0.9545, best train loss: 0.2233  @ epoch 9771 )\n",
      "[Epoch: 10280] train loss: 0.2542, train acc: 0.9033, val loss: 0.1577, val acc: 0.9491  (best train acc: 0.9250, best val acc: 0.9545, best train loss: 0.2233  @ epoch 9771 )\n",
      "[Epoch: 10300] train loss: 0.4487, train acc: 0.8398, val loss: 0.1631, val acc: 0.9464  (best train acc: 0.9250, best val acc: 0.9545, best train loss: 0.2215  @ epoch 10286 )\n",
      "[Epoch: 10320] train loss: 0.2423, train acc: 0.9179, val loss: 0.1577, val acc: 0.9457  (best train acc: 0.9250, best val acc: 0.9545, best train loss: 0.2215  @ epoch 10286 )\n",
      "[Epoch: 10340] train loss: 0.2521, train acc: 0.9118, val loss: 0.1585, val acc: 0.9433  (best train acc: 0.9250, best val acc: 0.9545, best train loss: 0.2215  @ epoch 10286 )\n",
      "[Epoch: 10360] train loss: 0.2411, train acc: 0.9109, val loss: 0.1553, val acc: 0.9487  (best train acc: 0.9250, best val acc: 0.9545, best train loss: 0.2215  @ epoch 10286 )\n",
      "[Epoch: 10380] train loss: 0.2590, train acc: 0.9072, val loss: 0.1610, val acc: 0.9514  (best train acc: 0.9250, best val acc: 0.9545, best train loss: 0.2215  @ epoch 10286 )\n",
      "[Epoch: 10400] train loss: 0.2582, train acc: 0.9085, val loss: 0.1703, val acc: 0.9366  (best train acc: 0.9250, best val acc: 0.9545, best train loss: 0.2215  @ epoch 10286 )\n",
      "[Epoch: 10420] train loss: 0.2554, train acc: 0.9038, val loss: 0.1585, val acc: 0.9467  (best train acc: 0.9250, best val acc: 0.9545, best train loss: 0.2215  @ epoch 10286 )\n",
      "[Epoch: 10440] train loss: 0.2547, train acc: 0.9127, val loss: 0.1623, val acc: 0.9514  (best train acc: 0.9250, best val acc: 0.9545, best train loss: 0.2215  @ epoch 10286 )\n",
      "[Epoch: 10460] train loss: 0.2595, train acc: 0.9062, val loss: 0.1688, val acc: 0.9457  (best train acc: 0.9250, best val acc: 0.9545, best train loss: 0.2215  @ epoch 10286 )\n",
      "[Epoch: 10480] train loss: 0.2495, train acc: 0.9046, val loss: 0.1646, val acc: 0.9464  (best train acc: 0.9250, best val acc: 0.9545, best train loss: 0.2215  @ epoch 10286 )\n",
      "[Epoch: 10500] train loss: 0.2568, train acc: 0.9081, val loss: 0.1560, val acc: 0.9477  (best train acc: 0.9250, best val acc: 0.9545, best train loss: 0.2215  @ epoch 10286 )\n",
      "[Epoch: 10520] train loss: 0.2434, train acc: 0.9178, val loss: 0.1571, val acc: 0.9450  (best train acc: 0.9250, best val acc: 0.9545, best train loss: 0.2215  @ epoch 10286 )\n",
      "[Epoch: 10540] train loss: 0.2690, train acc: 0.8986, val loss: 0.1741, val acc: 0.9322  (best train acc: 0.9250, best val acc: 0.9545, best train loss: 0.2184  @ epoch 10521 )\n",
      "[Epoch: 10560] train loss: 0.2427, train acc: 0.9141, val loss: 0.1567, val acc: 0.9524  (best train acc: 0.9250, best val acc: 0.9545, best train loss: 0.2184  @ epoch 10521 )\n",
      "[Epoch: 10580] train loss: 0.2453, train acc: 0.9051, val loss: 0.1656, val acc: 0.9484  (best train acc: 0.9250, best val acc: 0.9545, best train loss: 0.2184  @ epoch 10521 )\n",
      "[Epoch: 10600] train loss: 0.2565, train acc: 0.9096, val loss: 0.1578, val acc: 0.9410  (best train acc: 0.9250, best val acc: 0.9545, best train loss: 0.2184  @ epoch 10521 )\n",
      "[Epoch: 10620] train loss: 0.2780, train acc: 0.8965, val loss: 0.1583, val acc: 0.9481  (best train acc: 0.9295, best val acc: 0.9545, best train loss: 0.2063  @ epoch 10615 )\n",
      "[Epoch: 10640] train loss: 0.2357, train acc: 0.9132, val loss: 0.1611, val acc: 0.9511  (best train acc: 0.9295, best val acc: 0.9545, best train loss: 0.2063  @ epoch 10615 )\n",
      "[Epoch: 10660] train loss: 0.3041, train acc: 0.8810, val loss: 0.1565, val acc: 0.9521  (best train acc: 0.9295, best val acc: 0.9545, best train loss: 0.2063  @ epoch 10615 )\n",
      "[Epoch: 10680] train loss: 0.2306, train acc: 0.9216, val loss: 0.1599, val acc: 0.9514  (best train acc: 0.9295, best val acc: 0.9545, best train loss: 0.2063  @ epoch 10615 )\n",
      "[Epoch: 10700] train loss: 0.2859, train acc: 0.8881, val loss: 0.1538, val acc: 0.9494  (best train acc: 0.9295, best val acc: 0.9545, best train loss: 0.2063  @ epoch 10615 )\n",
      "[Epoch: 10720] train loss: 0.2466, train acc: 0.9117, val loss: 0.1669, val acc: 0.9329  (best train acc: 0.9295, best val acc: 0.9545, best train loss: 0.2063  @ epoch 10615 )\n",
      "[Epoch: 10740] train loss: 0.2437, train acc: 0.9142, val loss: 0.1604, val acc: 0.9444  (best train acc: 0.9295, best val acc: 0.9545, best train loss: 0.2063  @ epoch 10615 )\n",
      "[Epoch: 10760] train loss: 0.2510, train acc: 0.9096, val loss: 0.1577, val acc: 0.9454  (best train acc: 0.9295, best val acc: 0.9545, best train loss: 0.2063  @ epoch 10615 )\n",
      "[Epoch: 10780] train loss: 0.2508, train acc: 0.9073, val loss: 0.1771, val acc: 0.9312  (best train acc: 0.9295, best val acc: 0.9545, best train loss: 0.2063  @ epoch 10615 )\n",
      "[Epoch: 10800] train loss: 0.2340, train acc: 0.9191, val loss: 0.1608, val acc: 0.9484  (best train acc: 0.9295, best val acc: 0.9545, best train loss: 0.2063  @ epoch 10615 )\n",
      "[Epoch: 10820] train loss: 0.2857, train acc: 0.8837, val loss: 0.1606, val acc: 0.9423  (best train acc: 0.9295, best val acc: 0.9545, best train loss: 0.2063  @ epoch 10615 )\n",
      "[Epoch: 10840] train loss: 0.2690, train acc: 0.8976, val loss: 0.1620, val acc: 0.9464  (best train acc: 0.9295, best val acc: 0.9545, best train loss: 0.2063  @ epoch 10615 )\n",
      "[Epoch: 10860] train loss: 0.2728, train acc: 0.9008, val loss: 0.1569, val acc: 0.9437  (best train acc: 0.9295, best val acc: 0.9545, best train loss: 0.2063  @ epoch 10615 )\n",
      "[Epoch: 10880] train loss: 0.2941, train acc: 0.8978, val loss: 0.1606, val acc: 0.9464  (best train acc: 0.9295, best val acc: 0.9545, best train loss: 0.2063  @ epoch 10615 )\n",
      "[Epoch: 10900] train loss: 0.2645, train acc: 0.9083, val loss: 0.1535, val acc: 0.9508  (best train acc: 0.9295, best val acc: 0.9565, best train loss: 0.2063  @ epoch 10615 )\n",
      "[Epoch: 10920] train loss: 0.2672, train acc: 0.8981, val loss: 0.1575, val acc: 0.9457  (best train acc: 0.9295, best val acc: 0.9565, best train loss: 0.2063  @ epoch 10615 )\n",
      "[Epoch: 10940] train loss: 0.2725, train acc: 0.9016, val loss: 0.1660, val acc: 0.9450  (best train acc: 0.9295, best val acc: 0.9565, best train loss: 0.2063  @ epoch 10615 )\n",
      "[Epoch: 10960] train loss: 0.2838, train acc: 0.8897, val loss: 0.1702, val acc: 0.9383  (best train acc: 0.9295, best val acc: 0.9565, best train loss: 0.2063  @ epoch 10615 )\n",
      "[Epoch: 10980] train loss: 0.2744, train acc: 0.8996, val loss: 0.1568, val acc: 0.9538  (best train acc: 0.9295, best val acc: 0.9565, best train loss: 0.2063  @ epoch 10615 )\n",
      "[Epoch: 11000] train loss: 0.3061, train acc: 0.8827, val loss: 0.1512, val acc: 0.9494  (best train acc: 0.9295, best val acc: 0.9565, best train loss: 0.2063  @ epoch 10615 )\n",
      "[Epoch: 11020] train loss: 0.2505, train acc: 0.9048, val loss: 0.1535, val acc: 0.9504  (best train acc: 0.9295, best val acc: 0.9565, best train loss: 0.2063  @ epoch 10615 )\n",
      "[Epoch: 11040] train loss: 0.2758, train acc: 0.8963, val loss: 0.1512, val acc: 0.9491  (best train acc: 0.9295, best val acc: 0.9565, best train loss: 0.2063  @ epoch 10615 )\n",
      "[Epoch: 11060] train loss: 0.2429, train acc: 0.9153, val loss: 0.1603, val acc: 0.9430  (best train acc: 0.9295, best val acc: 0.9565, best train loss: 0.2063  @ epoch 10615 )\n",
      "[Epoch: 11080] train loss: 0.2732, train acc: 0.8943, val loss: 0.1638, val acc: 0.9417  (best train acc: 0.9295, best val acc: 0.9565, best train loss: 0.2063  @ epoch 10615 )\n",
      "[Epoch: 11100] train loss: 0.2306, train acc: 0.9187, val loss: 0.1545, val acc: 0.9487  (best train acc: 0.9295, best val acc: 0.9565, best train loss: 0.2063  @ epoch 10615 )\n",
      "[Epoch: 11120] train loss: 0.2467, train acc: 0.9117, val loss: 0.1485, val acc: 0.9531  (best train acc: 0.9295, best val acc: 0.9565, best train loss: 0.2063  @ epoch 10615 )\n",
      "[Epoch: 11140] train loss: 0.2612, train acc: 0.9009, val loss: 0.1479, val acc: 0.9535  (best train acc: 0.9295, best val acc: 0.9565, best train loss: 0.2063  @ epoch 10615 )\n",
      "[Epoch: 11160] train loss: 0.2439, train acc: 0.9111, val loss: 0.1475, val acc: 0.9514  (best train acc: 0.9295, best val acc: 0.9565, best train loss: 0.2063  @ epoch 10615 )\n",
      "[Epoch: 11180] train loss: 0.2512, train acc: 0.9064, val loss: 0.1688, val acc: 0.9444  (best train acc: 0.9295, best val acc: 0.9565, best train loss: 0.2063  @ epoch 10615 )\n",
      "[Epoch: 11200] train loss: 0.2425, train acc: 0.9142, val loss: 0.1533, val acc: 0.9477  (best train acc: 0.9295, best val acc: 0.9565, best train loss: 0.2063  @ epoch 10615 )\n",
      "[Epoch: 11220] train loss: 0.2485, train acc: 0.9081, val loss: 0.1601, val acc: 0.9484  (best train acc: 0.9295, best val acc: 0.9565, best train loss: 0.2063  @ epoch 10615 )\n",
      "[Epoch: 11240] train loss: 0.2627, train acc: 0.8991, val loss: 0.1504, val acc: 0.9487  (best train acc: 0.9295, best val acc: 0.9565, best train loss: 0.2063  @ epoch 10615 )\n",
      "[Epoch: 11260] train loss: 0.2676, train acc: 0.8934, val loss: 0.1577, val acc: 0.9444  (best train acc: 0.9295, best val acc: 0.9565, best train loss: 0.2063  @ epoch 10615 )\n",
      "[Epoch: 11280] train loss: 0.2444, train acc: 0.9104, val loss: 0.1615, val acc: 0.9491  (best train acc: 0.9295, best val acc: 0.9565, best train loss: 0.2063  @ epoch 10615 )\n",
      "[Epoch: 11300] train loss: 0.2579, train acc: 0.8988, val loss: 0.1536, val acc: 0.9491  (best train acc: 0.9295, best val acc: 0.9565, best train loss: 0.2063  @ epoch 10615 )\n",
      "[Epoch: 11320] train loss: 0.2404, train acc: 0.9131, val loss: 0.1517, val acc: 0.9494  (best train acc: 0.9295, best val acc: 0.9565, best train loss: 0.2063  @ epoch 10615 )\n",
      "[Epoch: 11340] train loss: 0.4122, train acc: 0.8580, val loss: 0.1667, val acc: 0.9440  (best train acc: 0.9295, best val acc: 0.9565, best train loss: 0.2063  @ epoch 10615 )\n",
      "[Epoch: 11360] train loss: 0.2772, train acc: 0.9028, val loss: 0.1642, val acc: 0.9457  (best train acc: 0.9295, best val acc: 0.9565, best train loss: 0.2063  @ epoch 10615 )\n",
      "[Epoch: 11380] train loss: 0.2437, train acc: 0.9122, val loss: 0.1542, val acc: 0.9494  (best train acc: 0.9295, best val acc: 0.9565, best train loss: 0.2063  @ epoch 10615 )\n",
      "[Epoch: 11400] train loss: 0.2506, train acc: 0.9128, val loss: 0.1516, val acc: 0.9511  (best train acc: 0.9295, best val acc: 0.9565, best train loss: 0.2063  @ epoch 10615 )\n",
      "[Epoch: 11420] train loss: 0.2489, train acc: 0.9106, val loss: 0.1469, val acc: 0.9494  (best train acc: 0.9295, best val acc: 0.9565, best train loss: 0.2063  @ epoch 10615 )\n",
      "[Epoch: 11440] train loss: 0.3195, train acc: 0.8858, val loss: 0.1455, val acc: 0.9497  (best train acc: 0.9295, best val acc: 0.9565, best train loss: 0.2063  @ epoch 10615 )\n",
      "[Epoch: 11460] train loss: 0.2866, train acc: 0.8849, val loss: 0.1726, val acc: 0.9383  (best train acc: 0.9295, best val acc: 0.9565, best train loss: 0.2063  @ epoch 10615 )\n",
      "[Epoch: 11480] train loss: 0.2388, train acc: 0.9193, val loss: 0.1543, val acc: 0.9433  (best train acc: 0.9295, best val acc: 0.9565, best train loss: 0.2063  @ epoch 10615 )\n",
      "[Epoch: 11500] train loss: 0.2875, train acc: 0.8920, val loss: 0.1489, val acc: 0.9481  (best train acc: 0.9295, best val acc: 0.9565, best train loss: 0.2063  @ epoch 10615 )\n",
      "[Epoch: 11520] train loss: 0.2315, train acc: 0.9174, val loss: 0.1553, val acc: 0.9474  (best train acc: 0.9295, best val acc: 0.9565, best train loss: 0.2063  @ epoch 10615 )\n",
      "[Epoch: 11540] train loss: 0.2781, train acc: 0.8930, val loss: 0.1634, val acc: 0.9376  (best train acc: 0.9295, best val acc: 0.9565, best train loss: 0.2063  @ epoch 10615 )\n",
      "[Epoch: 11560] train loss: 0.2755, train acc: 0.8987, val loss: 0.1527, val acc: 0.9524  (best train acc: 0.9295, best val acc: 0.9565, best train loss: 0.2063  @ epoch 10615 )\n",
      "[Epoch: 11580] train loss: 0.2521, train acc: 0.9070, val loss: 0.1671, val acc: 0.9481  (best train acc: 0.9295, best val acc: 0.9565, best train loss: 0.2063  @ epoch 10615 )\n",
      "[Epoch: 11600] train loss: 0.2414, train acc: 0.9114, val loss: 0.1470, val acc: 0.9535  (best train acc: 0.9295, best val acc: 0.9565, best train loss: 0.2063  @ epoch 10615 )\n",
      "[Epoch: 11620] train loss: 0.2411, train acc: 0.9147, val loss: 0.1456, val acc: 0.9477  (best train acc: 0.9295, best val acc: 0.9565, best train loss: 0.2063  @ epoch 10615 )\n",
      "[Epoch: 11640] train loss: 0.2864, train acc: 0.8954, val loss: 0.1556, val acc: 0.9481  (best train acc: 0.9295, best val acc: 0.9565, best train loss: 0.2063  @ epoch 10615 )\n",
      "[Epoch: 11660] train loss: 0.2791, train acc: 0.8916, val loss: 0.1496, val acc: 0.9524  (best train acc: 0.9295, best val acc: 0.9565, best train loss: 0.2063  @ epoch 10615 )\n",
      "[Epoch: 11680] train loss: 0.2629, train acc: 0.8999, val loss: 0.1502, val acc: 0.9501  (best train acc: 0.9295, best val acc: 0.9565, best train loss: 0.2063  @ epoch 10615 )\n",
      "[Epoch: 11700] train loss: 0.2689, train acc: 0.9055, val loss: 0.1595, val acc: 0.9444  (best train acc: 0.9295, best val acc: 0.9565, best train loss: 0.2063  @ epoch 10615 )\n",
      "[Epoch: 11720] train loss: 0.2435, train acc: 0.9109, val loss: 0.1496, val acc: 0.9511  (best train acc: 0.9295, best val acc: 0.9565, best train loss: 0.2063  @ epoch 10615 )\n",
      "[Epoch: 11740] train loss: 0.2439, train acc: 0.9159, val loss: 0.1506, val acc: 0.9467  (best train acc: 0.9295, best val acc: 0.9565, best train loss: 0.2063  @ epoch 10615 )\n",
      "[Epoch: 11760] train loss: 0.2673, train acc: 0.8918, val loss: 0.1507, val acc: 0.9457  (best train acc: 0.9295, best val acc: 0.9565, best train loss: 0.2063  @ epoch 10615 )\n",
      "[Epoch: 11780] train loss: 0.2451, train acc: 0.9091, val loss: 0.1525, val acc: 0.9518  (best train acc: 0.9295, best val acc: 0.9565, best train loss: 0.2063  @ epoch 10615 )\n",
      "[Epoch: 11800] train loss: 0.2751, train acc: 0.9005, val loss: 0.1627, val acc: 0.9393  (best train acc: 0.9295, best val acc: 0.9565, best train loss: 0.2063  @ epoch 10615 )\n",
      "[Epoch: 11820] train loss: 0.3002, train acc: 0.8811, val loss: 0.1622, val acc: 0.9464  (best train acc: 0.9295, best val acc: 0.9565, best train loss: 0.2063  @ epoch 10615 )\n",
      "[Epoch: 11840] train loss: 0.2283, train acc: 0.9189, val loss: 0.1450, val acc: 0.9497  (best train acc: 0.9295, best val acc: 0.9565, best train loss: 0.2063  @ epoch 10615 )\n",
      "[Epoch: 11860] train loss: 0.2883, train acc: 0.8806, val loss: 0.1507, val acc: 0.9450  (best train acc: 0.9295, best val acc: 0.9565, best train loss: 0.2063  @ epoch 10615 )\n",
      "[Epoch: 11880] train loss: 0.2451, train acc: 0.9127, val loss: 0.1555, val acc: 0.9440  (best train acc: 0.9295, best val acc: 0.9565, best train loss: 0.2063  @ epoch 10615 )\n",
      "[Epoch: 11900] train loss: 0.2406, train acc: 0.9100, val loss: 0.1565, val acc: 0.9497  (best train acc: 0.9295, best val acc: 0.9565, best train loss: 0.2063  @ epoch 10615 )\n",
      "[Epoch: 11920] train loss: 0.4071, train acc: 0.8643, val loss: 0.1446, val acc: 0.9501  (best train acc: 0.9295, best val acc: 0.9565, best train loss: 0.2063  @ epoch 10615 )\n",
      "[Epoch: 11940] train loss: 0.2762, train acc: 0.9017, val loss: 0.1496, val acc: 0.9514  (best train acc: 0.9295, best val acc: 0.9565, best train loss: 0.2063  @ epoch 10615 )\n",
      "[Epoch: 11960] train loss: 0.2357, train acc: 0.9135, val loss: 0.1489, val acc: 0.9477  (best train acc: 0.9295, best val acc: 0.9565, best train loss: 0.2063  @ epoch 10615 )\n",
      "[Epoch: 11980] train loss: 0.2360, train acc: 0.9189, val loss: 0.1457, val acc: 0.9474  (best train acc: 0.9295, best val acc: 0.9568, best train loss: 0.2063  @ epoch 10615 )\n",
      "[Epoch: 12000] train loss: 0.2259, train acc: 0.9213, val loss: 0.1559, val acc: 0.9538  (best train acc: 0.9295, best val acc: 0.9568, best train loss: 0.2063  @ epoch 10615 )\n",
      "[Epoch: 12020] train loss: 0.2479, train acc: 0.9054, val loss: 0.1551, val acc: 0.9460  (best train acc: 0.9295, best val acc: 0.9568, best train loss: 0.2063  @ epoch 10615 )\n",
      "[Epoch: 12040] train loss: 0.2267, train acc: 0.9184, val loss: 0.1468, val acc: 0.9467  (best train acc: 0.9295, best val acc: 0.9568, best train loss: 0.2063  @ epoch 10615 )\n",
      "[Epoch: 12060] train loss: 0.2741, train acc: 0.8965, val loss: 0.1586, val acc: 0.9379  (best train acc: 0.9295, best val acc: 0.9568, best train loss: 0.2063  @ epoch 10615 )\n",
      "[Epoch: 12080] train loss: 0.3339, train acc: 0.8750, val loss: 0.1480, val acc: 0.9494  (best train acc: 0.9295, best val acc: 0.9568, best train loss: 0.2063  @ epoch 10615 )\n",
      "[Epoch: 12100] train loss: 0.2359, train acc: 0.9131, val loss: 0.1487, val acc: 0.9501  (best train acc: 0.9295, best val acc: 0.9568, best train loss: 0.2063  @ epoch 10615 )\n",
      "[Epoch: 12120] train loss: 0.2441, train acc: 0.9122, val loss: 0.1531, val acc: 0.9474  (best train acc: 0.9295, best val acc: 0.9568, best train loss: 0.2063  @ epoch 10615 )\n",
      "[Epoch: 12140] train loss: 0.2439, train acc: 0.9106, val loss: 0.1492, val acc: 0.9508  (best train acc: 0.9295, best val acc: 0.9568, best train loss: 0.2063  @ epoch 10615 )\n",
      "[Epoch: 12160] train loss: 0.2668, train acc: 0.8926, val loss: 0.1543, val acc: 0.9433  (best train acc: 0.9295, best val acc: 0.9568, best train loss: 0.2063  @ epoch 10615 )\n",
      "[Epoch: 12180] train loss: 0.2276, train acc: 0.9215, val loss: 0.1436, val acc: 0.9521  (best train acc: 0.9295, best val acc: 0.9568, best train loss: 0.2063  @ epoch 10615 )\n",
      "[Epoch: 12200] train loss: 0.2250, train acc: 0.9203, val loss: 0.1481, val acc: 0.9484  (best train acc: 0.9295, best val acc: 0.9568, best train loss: 0.2063  @ epoch 10615 )\n",
      "[Epoch: 12220] train loss: 0.2762, train acc: 0.8983, val loss: 0.1457, val acc: 0.9474  (best train acc: 0.9295, best val acc: 0.9568, best train loss: 0.2063  @ epoch 10615 )\n",
      "[Epoch: 12240] train loss: 0.2309, train acc: 0.9238, val loss: 0.1525, val acc: 0.9393  (best train acc: 0.9295, best val acc: 0.9568, best train loss: 0.2063  @ epoch 10615 )\n",
      "[Epoch: 12260] train loss: 0.2496, train acc: 0.9068, val loss: 0.1530, val acc: 0.9494  (best train acc: 0.9295, best val acc: 0.9568, best train loss: 0.2063  @ epoch 10615 )\n",
      "[Epoch: 12280] train loss: 0.2784, train acc: 0.8887, val loss: 0.1472, val acc: 0.9511  (best train acc: 0.9295, best val acc: 0.9568, best train loss: 0.2063  @ epoch 10615 )\n",
      "[Epoch: 12300] train loss: 0.2595, train acc: 0.9027, val loss: 0.1427, val acc: 0.9518  (best train acc: 0.9295, best val acc: 0.9568, best train loss: 0.2063  @ epoch 10615 )\n",
      "[Epoch: 12320] train loss: 0.2617, train acc: 0.9027, val loss: 0.1424, val acc: 0.9531  (best train acc: 0.9295, best val acc: 0.9568, best train loss: 0.2063  @ epoch 10615 )\n",
      "[Epoch: 12340] train loss: 0.2372, train acc: 0.9115, val loss: 0.1463, val acc: 0.9531  (best train acc: 0.9310, best val acc: 0.9568, best train loss: 0.2025  @ epoch 12339 )\n",
      "[Epoch: 12360] train loss: 0.2316, train acc: 0.9150, val loss: 0.1377, val acc: 0.9538  (best train acc: 0.9310, best val acc: 0.9568, best train loss: 0.2025  @ epoch 12339 )\n",
      "[Epoch: 12380] train loss: 0.2643, train acc: 0.8944, val loss: 0.1443, val acc: 0.9457  (best train acc: 0.9310, best val acc: 0.9568, best train loss: 0.2025  @ epoch 12339 )\n",
      "[Epoch: 12400] train loss: 0.2464, train acc: 0.9135, val loss: 0.1380, val acc: 0.9494  (best train acc: 0.9310, best val acc: 0.9568, best train loss: 0.2009  @ epoch 12395 )\n",
      "[Epoch: 12420] train loss: 0.2449, train acc: 0.9164, val loss: 0.1362, val acc: 0.9521  (best train acc: 0.9310, best val acc: 0.9568, best train loss: 0.2009  @ epoch 12395 )\n",
      "[Epoch: 12440] train loss: 0.2194, train acc: 0.9190, val loss: 0.1365, val acc: 0.9541  (best train acc: 0.9310, best val acc: 0.9568, best train loss: 0.2009  @ epoch 12395 )\n",
      "[Epoch: 12460] train loss: 0.2465, train acc: 0.9104, val loss: 0.1478, val acc: 0.9528  (best train acc: 0.9310, best val acc: 0.9568, best train loss: 0.2009  @ epoch 12395 )\n",
      "[Epoch: 12480] train loss: 0.2254, train acc: 0.9234, val loss: 0.1450, val acc: 0.9454  (best train acc: 0.9310, best val acc: 0.9568, best train loss: 0.2009  @ epoch 12395 )\n",
      "[Epoch: 12500] train loss: 0.2485, train acc: 0.9060, val loss: 0.1399, val acc: 0.9551  (best train acc: 0.9310, best val acc: 0.9568, best train loss: 0.2009  @ epoch 12395 )\n",
      "[Epoch: 12520] train loss: 0.2220, train acc: 0.9203, val loss: 0.1587, val acc: 0.9450  (best train acc: 0.9310, best val acc: 0.9568, best train loss: 0.2009  @ epoch 12395 )\n",
      "[Epoch: 12540] train loss: 0.2420, train acc: 0.9121, val loss: 0.1400, val acc: 0.9514  (best train acc: 0.9310, best val acc: 0.9568, best train loss: 0.2009  @ epoch 12395 )\n",
      "[Epoch: 12560] train loss: 0.2477, train acc: 0.9143, val loss: 0.1377, val acc: 0.9514  (best train acc: 0.9310, best val acc: 0.9568, best train loss: 0.2009  @ epoch 12395 )\n",
      "[Epoch: 12580] train loss: 0.2198, train acc: 0.9134, val loss: 0.1444, val acc: 0.9518  (best train acc: 0.9310, best val acc: 0.9568, best train loss: 0.2009  @ epoch 12395 )\n",
      "[Epoch: 12600] train loss: 0.2292, train acc: 0.9183, val loss: 0.1484, val acc: 0.9501  (best train acc: 0.9310, best val acc: 0.9572, best train loss: 0.2009  @ epoch 12395 )\n",
      "[Epoch: 12620] train loss: 0.3905, train acc: 0.8637, val loss: 0.1397, val acc: 0.9514  (best train acc: 0.9310, best val acc: 0.9572, best train loss: 0.2009  @ epoch 12395 )\n",
      "[Epoch: 12640] train loss: 0.2595, train acc: 0.9119, val loss: 0.1369, val acc: 0.9535  (best train acc: 0.9310, best val acc: 0.9572, best train loss: 0.2009  @ epoch 12395 )\n",
      "[Epoch: 12660] train loss: 0.2210, train acc: 0.9214, val loss: 0.1448, val acc: 0.9444  (best train acc: 0.9310, best val acc: 0.9572, best train loss: 0.2009  @ epoch 12395 )\n",
      "[Epoch: 12680] train loss: 0.2559, train acc: 0.8963, val loss: 0.1376, val acc: 0.9528  (best train acc: 0.9310, best val acc: 0.9572, best train loss: 0.2009  @ epoch 12395 )\n",
      "[Epoch: 12700] train loss: 0.2258, train acc: 0.9134, val loss: 0.1394, val acc: 0.9511  (best train acc: 0.9310, best val acc: 0.9572, best train loss: 0.2009  @ epoch 12395 )\n",
      "[Epoch: 12720] train loss: 0.2410, train acc: 0.9076, val loss: 0.1430, val acc: 0.9494  (best train acc: 0.9310, best val acc: 0.9572, best train loss: 0.2009  @ epoch 12395 )\n",
      "[Epoch: 12740] train loss: 0.2368, train acc: 0.9148, val loss: 0.1386, val acc: 0.9538  (best train acc: 0.9310, best val acc: 0.9572, best train loss: 0.2009  @ epoch 12395 )\n",
      "[Epoch: 12760] train loss: 0.2194, train acc: 0.9184, val loss: 0.1387, val acc: 0.9541  (best train acc: 0.9310, best val acc: 0.9572, best train loss: 0.2009  @ epoch 12395 )\n",
      "[Epoch: 12780] train loss: 0.2110, train acc: 0.9229, val loss: 0.1341, val acc: 0.9528  (best train acc: 0.9310, best val acc: 0.9572, best train loss: 0.2009  @ epoch 12395 )\n",
      "[Epoch: 12800] train loss: 0.2402, train acc: 0.9095, val loss: 0.1321, val acc: 0.9538  (best train acc: 0.9310, best val acc: 0.9572, best train loss: 0.2009  @ epoch 12395 )\n",
      "[Epoch: 12820] train loss: 0.2773, train acc: 0.8999, val loss: 0.1478, val acc: 0.9524  (best train acc: 0.9310, best val acc: 0.9572, best train loss: 0.2009  @ epoch 12395 )\n",
      "[Epoch: 12840] train loss: 0.2209, train acc: 0.9184, val loss: 0.1387, val acc: 0.9531  (best train acc: 0.9310, best val acc: 0.9572, best train loss: 0.2009  @ epoch 12395 )\n",
      "[Epoch: 12860] train loss: 0.2259, train acc: 0.9195, val loss: 0.1355, val acc: 0.9541  (best train acc: 0.9310, best val acc: 0.9572, best train loss: 0.2009  @ epoch 12395 )\n",
      "[Epoch: 12880] train loss: 0.2345, train acc: 0.9116, val loss: 0.1505, val acc: 0.9511  (best train acc: 0.9310, best val acc: 0.9572, best train loss: 0.2009  @ epoch 12395 )\n",
      "[Epoch: 12900] train loss: 0.2233, train acc: 0.9190, val loss: 0.1382, val acc: 0.9470  (best train acc: 0.9310, best val acc: 0.9572, best train loss: 0.2009  @ epoch 12395 )\n",
      "[Epoch: 12920] train loss: 0.2170, train acc: 0.9185, val loss: 0.1351, val acc: 0.9548  (best train acc: 0.9310, best val acc: 0.9572, best train loss: 0.2009  @ epoch 12395 )\n",
      "[Epoch: 12940] train loss: 0.2129, train acc: 0.9206, val loss: 0.1479, val acc: 0.9524  (best train acc: 0.9310, best val acc: 0.9572, best train loss: 0.2009  @ epoch 12395 )\n",
      "[Epoch: 12960] train loss: 0.2305, train acc: 0.9115, val loss: 0.1484, val acc: 0.9511  (best train acc: 0.9310, best val acc: 0.9572, best train loss: 0.2009  @ epoch 12395 )\n",
      "[Epoch: 12980] train loss: 0.2264, train acc: 0.9181, val loss: 0.1453, val acc: 0.9440  (best train acc: 0.9310, best val acc: 0.9572, best train loss: 0.2009  @ epoch 12395 )\n",
      "[Epoch: 13000] train loss: 0.2497, train acc: 0.9033, val loss: 0.1426, val acc: 0.9494  (best train acc: 0.9310, best val acc: 0.9572, best train loss: 0.2009  @ epoch 12395 )\n",
      "[Epoch: 13020] train loss: 0.2307, train acc: 0.9175, val loss: 0.1476, val acc: 0.9535  (best train acc: 0.9310, best val acc: 0.9602, best train loss: 0.2009  @ epoch 12395 )\n",
      "[Epoch: 13040] train loss: 0.2138, train acc: 0.9238, val loss: 0.1421, val acc: 0.9538  (best train acc: 0.9310, best val acc: 0.9602, best train loss: 0.2009  @ epoch 12395 )\n",
      "[Epoch: 13060] train loss: 0.2152, train acc: 0.9224, val loss: 0.1434, val acc: 0.9545  (best train acc: 0.9310, best val acc: 0.9602, best train loss: 0.2009  @ epoch 12395 )\n",
      "[Epoch: 13080] train loss: 0.2481, train acc: 0.9059, val loss: 0.1350, val acc: 0.9508  (best train acc: 0.9310, best val acc: 0.9602, best train loss: 0.2009  @ epoch 12395 )\n",
      "[Epoch: 13100] train loss: 0.2544, train acc: 0.8991, val loss: 0.1400, val acc: 0.9524  (best train acc: 0.9310, best val acc: 0.9605, best train loss: 0.2009  @ epoch 12395 )\n",
      "[Epoch: 13120] train loss: 0.2225, train acc: 0.9205, val loss: 0.1387, val acc: 0.9491  (best train acc: 0.9310, best val acc: 0.9605, best train loss: 0.2009  @ epoch 12395 )\n",
      "[Epoch: 13140] train loss: 0.2674, train acc: 0.9033, val loss: 0.1517, val acc: 0.9555  (best train acc: 0.9310, best val acc: 0.9605, best train loss: 0.2009  @ epoch 12395 )\n",
      "[Epoch: 13160] train loss: 0.2397, train acc: 0.9127, val loss: 0.1414, val acc: 0.9464  (best train acc: 0.9310, best val acc: 0.9605, best train loss: 0.2009  @ epoch 12395 )\n",
      "[Epoch: 13180] train loss: 0.2234, train acc: 0.9164, val loss: 0.1390, val acc: 0.9531  (best train acc: 0.9310, best val acc: 0.9605, best train loss: 0.2009  @ epoch 12395 )\n",
      "[Epoch: 13200] train loss: 0.2908, train acc: 0.8869, val loss: 0.1369, val acc: 0.9528  (best train acc: 0.9310, best val acc: 0.9605, best train loss: 0.2009  @ epoch 12395 )\n",
      "[Epoch: 13220] train loss: 0.2156, train acc: 0.9248, val loss: 0.1361, val acc: 0.9538  (best train acc: 0.9310, best val acc: 0.9605, best train loss: 0.2009  @ epoch 12395 )\n",
      "[Epoch: 13240] train loss: 0.2174, train acc: 0.9169, val loss: 0.1376, val acc: 0.9535  (best train acc: 0.9319, best val acc: 0.9605, best train loss: 0.1951  @ epoch 13222 )\n",
      "[Epoch: 13260] train loss: 0.2357, train acc: 0.9119, val loss: 0.1365, val acc: 0.9551  (best train acc: 0.9319, best val acc: 0.9605, best train loss: 0.1951  @ epoch 13222 )\n",
      "[Epoch: 13280] train loss: 0.2245, train acc: 0.9186, val loss: 0.1556, val acc: 0.9437  (best train acc: 0.9319, best val acc: 0.9605, best train loss: 0.1951  @ epoch 13222 )\n",
      "[Epoch: 13300] train loss: 0.2213, train acc: 0.9203, val loss: 0.1615, val acc: 0.9349  (best train acc: 0.9319, best val acc: 0.9605, best train loss: 0.1951  @ epoch 13222 )\n",
      "[Epoch: 13320] train loss: 0.2320, train acc: 0.9187, val loss: 0.1448, val acc: 0.9470  (best train acc: 0.9319, best val acc: 0.9605, best train loss: 0.1951  @ epoch 13222 )\n",
      "[Epoch: 13340] train loss: 0.2765, train acc: 0.8948, val loss: 0.1530, val acc: 0.9460  (best train acc: 0.9319, best val acc: 0.9605, best train loss: 0.1951  @ epoch 13222 )\n",
      "[Epoch: 13360] train loss: 0.2188, train acc: 0.9221, val loss: 0.1523, val acc: 0.9481  (best train acc: 0.9319, best val acc: 0.9605, best train loss: 0.1951  @ epoch 13222 )\n",
      "[Epoch: 13380] train loss: 0.2105, train acc: 0.9185, val loss: 0.1418, val acc: 0.9481  (best train acc: 0.9319, best val acc: 0.9605, best train loss: 0.1951  @ epoch 13222 )\n",
      "[Epoch: 13400] train loss: 0.2350, train acc: 0.9114, val loss: 0.1419, val acc: 0.9548  (best train acc: 0.9319, best val acc: 0.9605, best train loss: 0.1951  @ epoch 13222 )\n",
      "[Epoch: 13420] train loss: 0.2269, train acc: 0.9182, val loss: 0.1446, val acc: 0.9521  (best train acc: 0.9319, best val acc: 0.9605, best train loss: 0.1951  @ epoch 13222 )\n",
      "[Epoch: 13440] train loss: 0.3331, train acc: 0.8789, val loss: 0.1420, val acc: 0.9548  (best train acc: 0.9319, best val acc: 0.9605, best train loss: 0.1951  @ epoch 13222 )\n",
      "[Epoch: 13460] train loss: 0.2938, train acc: 0.8884, val loss: 0.1452, val acc: 0.9531  (best train acc: 0.9319, best val acc: 0.9605, best train loss: 0.1951  @ epoch 13222 )\n",
      "[Epoch: 13480] train loss: 0.2608, train acc: 0.8900, val loss: 0.1360, val acc: 0.9535  (best train acc: 0.9319, best val acc: 0.9605, best train loss: 0.1951  @ epoch 13222 )\n",
      "[Epoch: 13500] train loss: 0.2463, train acc: 0.9038, val loss: 0.1400, val acc: 0.9541  (best train acc: 0.9319, best val acc: 0.9605, best train loss: 0.1951  @ epoch 13222 )\n",
      "[Epoch: 13520] train loss: 0.2523, train acc: 0.9016, val loss: 0.1403, val acc: 0.9494  (best train acc: 0.9319, best val acc: 0.9605, best train loss: 0.1951  @ epoch 13222 )\n",
      "[Epoch: 13540] train loss: 0.2271, train acc: 0.9119, val loss: 0.1345, val acc: 0.9555  (best train acc: 0.9319, best val acc: 0.9605, best train loss: 0.1951  @ epoch 13222 )\n",
      "[Epoch: 13560] train loss: 0.2897, train acc: 0.8787, val loss: 0.1375, val acc: 0.9572  (best train acc: 0.9319, best val acc: 0.9605, best train loss: 0.1951  @ epoch 13222 )\n",
      "[Epoch: 13580] train loss: 0.2195, train acc: 0.9191, val loss: 0.1366, val acc: 0.9524  (best train acc: 0.9319, best val acc: 0.9605, best train loss: 0.1951  @ epoch 13222 )\n",
      "[Epoch: 13600] train loss: 0.2149, train acc: 0.9204, val loss: 0.1448, val acc: 0.9497  (best train acc: 0.9319, best val acc: 0.9605, best train loss: 0.1951  @ epoch 13222 )\n",
      "[Epoch: 13620] train loss: 0.2292, train acc: 0.9179, val loss: 0.1514, val acc: 0.9383  (best train acc: 0.9319, best val acc: 0.9605, best train loss: 0.1951  @ epoch 13222 )\n",
      "[Epoch: 13640] train loss: 0.3036, train acc: 0.8761, val loss: 0.1333, val acc: 0.9538  (best train acc: 0.9319, best val acc: 0.9605, best train loss: 0.1951  @ epoch 13222 )\n",
      "[Epoch: 13660] train loss: 0.2185, train acc: 0.9213, val loss: 0.1386, val acc: 0.9511  (best train acc: 0.9319, best val acc: 0.9605, best train loss: 0.1951  @ epoch 13222 )\n",
      "[Epoch: 13680] train loss: 0.2467, train acc: 0.9080, val loss: 0.1410, val acc: 0.9524  (best train acc: 0.9319, best val acc: 0.9605, best train loss: 0.1951  @ epoch 13222 )\n",
      "[Epoch: 13700] train loss: 0.2288, train acc: 0.9156, val loss: 0.1424, val acc: 0.9514  (best train acc: 0.9319, best val acc: 0.9605, best train loss: 0.1951  @ epoch 13222 )\n",
      "[Epoch: 13720] train loss: 0.2178, train acc: 0.9165, val loss: 0.1404, val acc: 0.9514  (best train acc: 0.9319, best val acc: 0.9605, best train loss: 0.1951  @ epoch 13222 )\n",
      "[Epoch: 13740] train loss: 0.2194, train acc: 0.9227, val loss: 0.1317, val acc: 0.9551  (best train acc: 0.9319, best val acc: 0.9605, best train loss: 0.1951  @ epoch 13222 )\n",
      "[Epoch: 13760] train loss: 0.2255, train acc: 0.9230, val loss: 0.1340, val acc: 0.9575  (best train acc: 0.9319, best val acc: 0.9605, best train loss: 0.1951  @ epoch 13222 )\n",
      "[Epoch: 13780] train loss: 0.2135, train acc: 0.9202, val loss: 0.1369, val acc: 0.9541  (best train acc: 0.9319, best val acc: 0.9605, best train loss: 0.1951  @ epoch 13222 )\n",
      "[Epoch: 13800] train loss: 0.2231, train acc: 0.9217, val loss: 0.1424, val acc: 0.9538  (best train acc: 0.9319, best val acc: 0.9605, best train loss: 0.1951  @ epoch 13222 )\n",
      "[Epoch: 13820] train loss: 0.2179, train acc: 0.9174, val loss: 0.1452, val acc: 0.9477  (best train acc: 0.9319, best val acc: 0.9605, best train loss: 0.1951  @ epoch 13222 )\n",
      "[Epoch: 13840] train loss: 0.2356, train acc: 0.9107, val loss: 0.1302, val acc: 0.9575  (best train acc: 0.9319, best val acc: 0.9605, best train loss: 0.1951  @ epoch 13222 )\n",
      "[Epoch: 13860] train loss: 0.3791, train acc: 0.8579, val loss: 0.1527, val acc: 0.9433  (best train acc: 0.9319, best val acc: 0.9605, best train loss: 0.1951  @ epoch 13222 )\n",
      "[Epoch: 13880] train loss: 0.2169, train acc: 0.9179, val loss: 0.1417, val acc: 0.9457  (best train acc: 0.9319, best val acc: 0.9605, best train loss: 0.1951  @ epoch 13222 )\n",
      "[Epoch: 13900] train loss: 0.2242, train acc: 0.9186, val loss: 0.1396, val acc: 0.9528  (best train acc: 0.9319, best val acc: 0.9605, best train loss: 0.1951  @ epoch 13222 )\n",
      "[Epoch: 13920] train loss: 0.2216, train acc: 0.9184, val loss: 0.1455, val acc: 0.9548  (best train acc: 0.9319, best val acc: 0.9605, best train loss: 0.1951  @ epoch 13222 )\n",
      "[Epoch: 13940] train loss: 0.2151, train acc: 0.9202, val loss: 0.1480, val acc: 0.9437  (best train acc: 0.9319, best val acc: 0.9605, best train loss: 0.1951  @ epoch 13222 )\n",
      "[Epoch: 13960] train loss: 0.2205, train acc: 0.9151, val loss: 0.1351, val acc: 0.9538  (best train acc: 0.9319, best val acc: 0.9605, best train loss: 0.1951  @ epoch 13222 )\n",
      "[Epoch: 13980] train loss: 0.2226, train acc: 0.9182, val loss: 0.1337, val acc: 0.9541  (best train acc: 0.9319, best val acc: 0.9605, best train loss: 0.1951  @ epoch 13222 )\n",
      "[Epoch: 14000] train loss: 0.2247, train acc: 0.9138, val loss: 0.1390, val acc: 0.9497  (best train acc: 0.9319, best val acc: 0.9605, best train loss: 0.1951  @ epoch 13222 )\n",
      "[Epoch: 14020] train loss: 0.2261, train acc: 0.9138, val loss: 0.1360, val acc: 0.9558  (best train acc: 0.9319, best val acc: 0.9605, best train loss: 0.1951  @ epoch 13222 )\n",
      "[Epoch: 14040] train loss: 0.2291, train acc: 0.9104, val loss: 0.1471, val acc: 0.9582  (best train acc: 0.9319, best val acc: 0.9605, best train loss: 0.1951  @ epoch 13222 )\n",
      "[Epoch: 14060] train loss: 0.2111, train acc: 0.9172, val loss: 0.1527, val acc: 0.9352  (best train acc: 0.9319, best val acc: 0.9605, best train loss: 0.1951  @ epoch 13222 )\n",
      "[Epoch: 14080] train loss: 0.2063, train acc: 0.9259, val loss: 0.1336, val acc: 0.9528  (best train acc: 0.9319, best val acc: 0.9605, best train loss: 0.1951  @ epoch 13222 )\n",
      "[Epoch: 14100] train loss: 0.2944, train acc: 0.8832, val loss: 0.1351, val acc: 0.9528  (best train acc: 0.9319, best val acc: 0.9605, best train loss: 0.1951  @ epoch 13222 )\n",
      "[Epoch: 14120] train loss: 0.2076, train acc: 0.9226, val loss: 0.1518, val acc: 0.9454  (best train acc: 0.9319, best val acc: 0.9605, best train loss: 0.1951  @ epoch 13222 )\n",
      "[Epoch: 14140] train loss: 0.2150, train acc: 0.9200, val loss: 0.1546, val acc: 0.9349  (best train acc: 0.9319, best val acc: 0.9605, best train loss: 0.1951  @ epoch 13222 )\n",
      "[Epoch: 14160] train loss: 0.3331, train acc: 0.8794, val loss: 0.1367, val acc: 0.9555  (best train acc: 0.9319, best val acc: 0.9605, best train loss: 0.1951  @ epoch 13222 )\n",
      "[Epoch: 14180] train loss: 0.2127, train acc: 0.9213, val loss: 0.1461, val acc: 0.9575  (best train acc: 0.9319, best val acc: 0.9605, best train loss: 0.1951  @ epoch 13222 )\n",
      "[Epoch: 14200] train loss: 0.2298, train acc: 0.9142, val loss: 0.1421, val acc: 0.9487  (best train acc: 0.9319, best val acc: 0.9605, best train loss: 0.1951  @ epoch 13222 )\n",
      "[Epoch: 14220] train loss: 0.2206, train acc: 0.9218, val loss: 0.1555, val acc: 0.9305  (best train acc: 0.9319, best val acc: 0.9605, best train loss: 0.1951  @ epoch 13222 )\n",
      "[Epoch: 14240] train loss: 0.2181, train acc: 0.9135, val loss: 0.1465, val acc: 0.9491  (best train acc: 0.9319, best val acc: 0.9605, best train loss: 0.1951  @ epoch 13222 )\n",
      "[Epoch: 14260] train loss: 0.2478, train acc: 0.8989, val loss: 0.1435, val acc: 0.9508  (best train acc: 0.9319, best val acc: 0.9605, best train loss: 0.1951  @ epoch 13222 )\n",
      "[Epoch: 14280] train loss: 0.2441, train acc: 0.9121, val loss: 0.1466, val acc: 0.9497  (best train acc: 0.9319, best val acc: 0.9605, best train loss: 0.1951  @ epoch 13222 )\n",
      "[Epoch: 14300] train loss: 0.2380, train acc: 0.9024, val loss: 0.1505, val acc: 0.9514  (best train acc: 0.9319, best val acc: 0.9605, best train loss: 0.1951  @ epoch 13222 )\n",
      "[Epoch: 14320] train loss: 0.2410, train acc: 0.9127, val loss: 0.1393, val acc: 0.9545  (best train acc: 0.9319, best val acc: 0.9605, best train loss: 0.1951  @ epoch 13222 )\n",
      "[Epoch: 14340] train loss: 0.2102, train acc: 0.9226, val loss: 0.1423, val acc: 0.9497  (best train acc: 0.9319, best val acc: 0.9605, best train loss: 0.1951  @ epoch 13222 )\n",
      "[Epoch: 14360] train loss: 0.2027, train acc: 0.9229, val loss: 0.1477, val acc: 0.9518  (best train acc: 0.9319, best val acc: 0.9605, best train loss: 0.1951  @ epoch 13222 )\n",
      "[Epoch: 14380] train loss: 0.2321, train acc: 0.9198, val loss: 0.1356, val acc: 0.9572  (best train acc: 0.9319, best val acc: 0.9605, best train loss: 0.1951  @ epoch 13222 )\n",
      "[Epoch: 14400] train loss: 0.3140, train acc: 0.8705, val loss: 0.1545, val acc: 0.9501  (best train acc: 0.9319, best val acc: 0.9605, best train loss: 0.1951  @ epoch 13222 )\n",
      "[Epoch: 14420] train loss: 0.2512, train acc: 0.9158, val loss: 0.1344, val acc: 0.9555  (best train acc: 0.9319, best val acc: 0.9605, best train loss: 0.1951  @ epoch 13222 )\n",
      "[Epoch: 14440] train loss: 0.2077, train acc: 0.9258, val loss: 0.1303, val acc: 0.9602  (best train acc: 0.9319, best val acc: 0.9605, best train loss: 0.1951  @ epoch 13222 )\n",
      "[Epoch: 14460] train loss: 0.2289, train acc: 0.9115, val loss: 0.1394, val acc: 0.9521  (best train acc: 0.9319, best val acc: 0.9605, best train loss: 0.1951  @ epoch 13222 )\n",
      "[Epoch: 14480] train loss: 0.2179, train acc: 0.9205, val loss: 0.1565, val acc: 0.9518  (best train acc: 0.9319, best val acc: 0.9605, best train loss: 0.1951  @ epoch 13222 )\n",
      "[Epoch: 14500] train loss: 0.2158, train acc: 0.9174, val loss: 0.1407, val acc: 0.9555  (best train acc: 0.9319, best val acc: 0.9605, best train loss: 0.1951  @ epoch 13222 )\n",
      "[Epoch: 14520] train loss: 0.2375, train acc: 0.9079, val loss: 0.1327, val acc: 0.9535  (best train acc: 0.9319, best val acc: 0.9605, best train loss: 0.1951  @ epoch 13222 )\n",
      "[Epoch: 14540] train loss: 0.2079, train acc: 0.9272, val loss: 0.1377, val acc: 0.9538  (best train acc: 0.9319, best val acc: 0.9605, best train loss: 0.1951  @ epoch 13222 )\n",
      "[Epoch: 14560] train loss: 0.2835, train acc: 0.8957, val loss: 0.1395, val acc: 0.9514  (best train acc: 0.9319, best val acc: 0.9605, best train loss: 0.1951  @ epoch 13222 )\n",
      "[Epoch: 14580] train loss: 0.2262, train acc: 0.9138, val loss: 0.1458, val acc: 0.9433  (best train acc: 0.9319, best val acc: 0.9605, best train loss: 0.1951  @ epoch 13222 )\n",
      "[Epoch: 14600] train loss: 0.2050, train acc: 0.9250, val loss: 0.1410, val acc: 0.9508  (best train acc: 0.9319, best val acc: 0.9605, best train loss: 0.1951  @ epoch 13222 )\n",
      "[Epoch: 14620] train loss: 0.2529, train acc: 0.8992, val loss: 0.1409, val acc: 0.9565  (best train acc: 0.9319, best val acc: 0.9605, best train loss: 0.1947  @ epoch 14618 )\n",
      "[Epoch: 14640] train loss: 0.2176, train acc: 0.9255, val loss: 0.1387, val acc: 0.9508  (best train acc: 0.9319, best val acc: 0.9605, best train loss: 0.1947  @ epoch 14618 )\n",
      "[Epoch: 14660] train loss: 0.2201, train acc: 0.9184, val loss: 0.1448, val acc: 0.9460  (best train acc: 0.9319, best val acc: 0.9605, best train loss: 0.1947  @ epoch 14618 )\n",
      "[Epoch: 14680] train loss: 0.2373, train acc: 0.9048, val loss: 0.1400, val acc: 0.9548  (best train acc: 0.9319, best val acc: 0.9605, best train loss: 0.1925  @ epoch 14665 )\n",
      "[Epoch: 14700] train loss: 0.1932, train acc: 0.9285, val loss: 0.1473, val acc: 0.9518  (best train acc: 0.9319, best val acc: 0.9605, best train loss: 0.1925  @ epoch 14665 )\n",
      "[Epoch: 14720] train loss: 0.2821, train acc: 0.8943, val loss: 0.1478, val acc: 0.9450  (best train acc: 0.9319, best val acc: 0.9605, best train loss: 0.1925  @ epoch 14665 )\n",
      "[Epoch: 14740] train loss: 0.2424, train acc: 0.9130, val loss: 0.1392, val acc: 0.9548  (best train acc: 0.9319, best val acc: 0.9605, best train loss: 0.1925  @ epoch 14665 )\n",
      "[Epoch: 14760] train loss: 0.2195, train acc: 0.9198, val loss: 0.1438, val acc: 0.9454  (best train acc: 0.9319, best val acc: 0.9605, best train loss: 0.1925  @ epoch 14665 )\n",
      "[Epoch: 14780] train loss: 0.2403, train acc: 0.9094, val loss: 0.1368, val acc: 0.9562  (best train acc: 0.9319, best val acc: 0.9605, best train loss: 0.1925  @ epoch 14665 )\n",
      "[Epoch: 14800] train loss: 0.2204, train acc: 0.9114, val loss: 0.1401, val acc: 0.9528  (best train acc: 0.9319, best val acc: 0.9605, best train loss: 0.1925  @ epoch 14665 )\n",
      "[Epoch: 14820] train loss: 0.2261, train acc: 0.9120, val loss: 0.1388, val acc: 0.9535  (best train acc: 0.9319, best val acc: 0.9605, best train loss: 0.1925  @ epoch 14665 )\n",
      "[Epoch: 14840] train loss: 0.2344, train acc: 0.9204, val loss: 0.1304, val acc: 0.9562  (best train acc: 0.9319, best val acc: 0.9605, best train loss: 0.1925  @ epoch 14665 )\n",
      "[Epoch: 14860] train loss: 0.2159, train acc: 0.9210, val loss: 0.1414, val acc: 0.9474  (best train acc: 0.9319, best val acc: 0.9605, best train loss: 0.1925  @ epoch 14665 )\n",
      "[Epoch: 14880] train loss: 0.2533, train acc: 0.9018, val loss: 0.1382, val acc: 0.9565  (best train acc: 0.9319, best val acc: 0.9605, best train loss: 0.1925  @ epoch 14665 )\n",
      "[Epoch: 14900] train loss: 0.2462, train acc: 0.9070, val loss: 0.1325, val acc: 0.9558  (best train acc: 0.9319, best val acc: 0.9605, best train loss: 0.1925  @ epoch 14665 )\n",
      "[Epoch: 14920] train loss: 0.2001, train acc: 0.9282, val loss: 0.1367, val acc: 0.9541  (best train acc: 0.9319, best val acc: 0.9605, best train loss: 0.1925  @ epoch 14665 )\n",
      "[Epoch: 14940] train loss: 0.1944, train acc: 0.9297, val loss: 0.1476, val acc: 0.9457  (best train acc: 0.9319, best val acc: 0.9605, best train loss: 0.1861  @ epoch 14923 )\n",
      "[Epoch: 14960] train loss: 0.2080, train acc: 0.9216, val loss: 0.1341, val acc: 0.9565  (best train acc: 0.9319, best val acc: 0.9605, best train loss: 0.1861  @ epoch 14923 )\n",
      "[Epoch: 14980] train loss: 0.2322, train acc: 0.9063, val loss: 0.1660, val acc: 0.9228  (best train acc: 0.9319, best val acc: 0.9605, best train loss: 0.1861  @ epoch 14923 )\n",
      "[Epoch: 15000] train loss: 0.2193, train acc: 0.9218, val loss: 0.1433, val acc: 0.9437  (best train acc: 0.9319, best val acc: 0.9605, best train loss: 0.1861  @ epoch 14923 )\n",
      "[Epoch: 15020] train loss: 0.2260, train acc: 0.9153, val loss: 0.1461, val acc: 0.9430  (best train acc: 0.9319, best val acc: 0.9605, best train loss: 0.1861  @ epoch 14923 )\n",
      "[Epoch: 15040] train loss: 0.2177, train acc: 0.9201, val loss: 0.1348, val acc: 0.9541  (best train acc: 0.9319, best val acc: 0.9605, best train loss: 0.1861  @ epoch 14923 )\n",
      "[Epoch: 15060] train loss: 0.3349, train acc: 0.8859, val loss: 0.1381, val acc: 0.9562  (best train acc: 0.9319, best val acc: 0.9605, best train loss: 0.1861  @ epoch 14923 )\n",
      "[Epoch: 15080] train loss: 0.2251, train acc: 0.9109, val loss: 0.1366, val acc: 0.9555  (best train acc: 0.9319, best val acc: 0.9605, best train loss: 0.1861  @ epoch 14923 )\n",
      "[Epoch: 15100] train loss: 0.2479, train acc: 0.9023, val loss: 0.1355, val acc: 0.9538  (best train acc: 0.9319, best val acc: 0.9605, best train loss: 0.1861  @ epoch 14923 )\n",
      "[Epoch: 15120] train loss: 0.2285, train acc: 0.9076, val loss: 0.1438, val acc: 0.9538  (best train acc: 0.9319, best val acc: 0.9605, best train loss: 0.1861  @ epoch 14923 )\n",
      "[Epoch: 15140] train loss: 0.2117, train acc: 0.9198, val loss: 0.1327, val acc: 0.9578  (best train acc: 0.9319, best val acc: 0.9605, best train loss: 0.1861  @ epoch 14923 )\n",
      "[Epoch: 15160] train loss: 0.2288, train acc: 0.9169, val loss: 0.1481, val acc: 0.9454  (best train acc: 0.9319, best val acc: 0.9605, best train loss: 0.1861  @ epoch 14923 )\n",
      "[Epoch: 15180] train loss: 0.2290, train acc: 0.9158, val loss: 0.1483, val acc: 0.9460  (best train acc: 0.9319, best val acc: 0.9605, best train loss: 0.1861  @ epoch 14923 )\n",
      "[Epoch: 15200] train loss: 0.2405, train acc: 0.9058, val loss: 0.1449, val acc: 0.9524  (best train acc: 0.9319, best val acc: 0.9605, best train loss: 0.1861  @ epoch 14923 )\n",
      "[Epoch: 15220] train loss: 0.2005, train acc: 0.9265, val loss: 0.1390, val acc: 0.9538  (best train acc: 0.9319, best val acc: 0.9605, best train loss: 0.1861  @ epoch 14923 )\n",
      "[Epoch: 15240] train loss: 0.2198, train acc: 0.9159, val loss: 0.1346, val acc: 0.9568  (best train acc: 0.9333, best val acc: 0.9605, best train loss: 0.1861  @ epoch 14923 )\n",
      "[Epoch: 15260] train loss: 0.2264, train acc: 0.9205, val loss: 0.1344, val acc: 0.9528  (best train acc: 0.9333, best val acc: 0.9605, best train loss: 0.1861  @ epoch 14923 )\n",
      "[Epoch: 15280] train loss: 0.2505, train acc: 0.9114, val loss: 0.1370, val acc: 0.9555  (best train acc: 0.9333, best val acc: 0.9605, best train loss: 0.1861  @ epoch 14923 )\n",
      "[Epoch: 15300] train loss: 0.2135, train acc: 0.9218, val loss: 0.1313, val acc: 0.9562  (best train acc: 0.9333, best val acc: 0.9605, best train loss: 0.1861  @ epoch 14923 )\n",
      "[Epoch: 15320] train loss: 0.2038, train acc: 0.9269, val loss: 0.1347, val acc: 0.9518  (best train acc: 0.9333, best val acc: 0.9605, best train loss: 0.1861  @ epoch 14923 )\n",
      "[Epoch: 15340] train loss: 0.2427, train acc: 0.8984, val loss: 0.1363, val acc: 0.9562  (best train acc: 0.9333, best val acc: 0.9605, best train loss: 0.1861  @ epoch 14923 )\n",
      "[Epoch: 15360] train loss: 0.2035, train acc: 0.9271, val loss: 0.1364, val acc: 0.9545  (best train acc: 0.9333, best val acc: 0.9605, best train loss: 0.1861  @ epoch 14923 )\n",
      "[Epoch: 15380] train loss: 0.2134, train acc: 0.9169, val loss: 0.1461, val acc: 0.9501  (best train acc: 0.9333, best val acc: 0.9605, best train loss: 0.1861  @ epoch 14923 )\n",
      "[Epoch: 15400] train loss: 0.2156, train acc: 0.9206, val loss: 0.1453, val acc: 0.9433  (best train acc: 0.9333, best val acc: 0.9605, best train loss: 0.1861  @ epoch 14923 )\n",
      "[Epoch: 15420] train loss: 0.2126, train acc: 0.9202, val loss: 0.1342, val acc: 0.9555  (best train acc: 0.9333, best val acc: 0.9605, best train loss: 0.1861  @ epoch 14923 )\n",
      "[Epoch: 15440] train loss: 0.1887, train acc: 0.9324, val loss: 0.1419, val acc: 0.9504  (best train acc: 0.9333, best val acc: 0.9605, best train loss: 0.1861  @ epoch 14923 )\n",
      "[Epoch: 15460] train loss: 0.2053, train acc: 0.9268, val loss: 0.1358, val acc: 0.9474  (best train acc: 0.9333, best val acc: 0.9605, best train loss: 0.1861  @ epoch 14923 )\n",
      "[Epoch: 15480] train loss: 0.2363, train acc: 0.9154, val loss: 0.1423, val acc: 0.9484  (best train acc: 0.9333, best val acc: 0.9605, best train loss: 0.1861  @ epoch 14923 )\n",
      "[Epoch: 15500] train loss: 0.2058, train acc: 0.9221, val loss: 0.1362, val acc: 0.9521  (best train acc: 0.9333, best val acc: 0.9605, best train loss: 0.1861  @ epoch 14923 )\n",
      "[Epoch: 15520] train loss: 0.2123, train acc: 0.9175, val loss: 0.1451, val acc: 0.9521  (best train acc: 0.9333, best val acc: 0.9605, best train loss: 0.1861  @ epoch 15504 )\n",
      "[Epoch: 15540] train loss: 0.2044, train acc: 0.9198, val loss: 0.1301, val acc: 0.9562  (best train acc: 0.9333, best val acc: 0.9605, best train loss: 0.1861  @ epoch 15504 )\n",
      "[Epoch: 15560] train loss: 0.2075, train acc: 0.9190, val loss: 0.1496, val acc: 0.9410  (best train acc: 0.9333, best val acc: 0.9605, best train loss: 0.1861  @ epoch 15504 )\n",
      "[Epoch: 15580] train loss: 0.2233, train acc: 0.9112, val loss: 0.1405, val acc: 0.9521  (best train acc: 0.9333, best val acc: 0.9605, best train loss: 0.1861  @ epoch 15504 )\n",
      "[Epoch: 15600] train loss: 0.1943, train acc: 0.9336, val loss: 0.1406, val acc: 0.9474  (best train acc: 0.9338, best val acc: 0.9605, best train loss: 0.1855  @ epoch 15594 )\n",
      "[Epoch: 15620] train loss: 0.2040, train acc: 0.9190, val loss: 0.1378, val acc: 0.9481  (best train acc: 0.9338, best val acc: 0.9605, best train loss: 0.1852  @ epoch 15619 )\n",
      "[Epoch: 15640] train loss: 0.1905, train acc: 0.9275, val loss: 0.1371, val acc: 0.9531  (best train acc: 0.9338, best val acc: 0.9605, best train loss: 0.1852  @ epoch 15619 )\n",
      "[Epoch: 15660] train loss: 0.2083, train acc: 0.9264, val loss: 0.1323, val acc: 0.9572  (best train acc: 0.9338, best val acc: 0.9605, best train loss: 0.1852  @ epoch 15619 )\n",
      "[Epoch: 15680] train loss: 0.1914, train acc: 0.9297, val loss: 0.1341, val acc: 0.9497  (best train acc: 0.9351, best val acc: 0.9605, best train loss: 0.1817  @ epoch 15679 )\n",
      "[Epoch: 15700] train loss: 0.2131, train acc: 0.9175, val loss: 0.1347, val acc: 0.9551  (best train acc: 0.9351, best val acc: 0.9605, best train loss: 0.1817  @ epoch 15679 )\n",
      "[Epoch: 15720] train loss: 0.2052, train acc: 0.9217, val loss: 0.1382, val acc: 0.9521  (best train acc: 0.9351, best val acc: 0.9605, best train loss: 0.1817  @ epoch 15679 )\n",
      "[Epoch: 15740] train loss: 0.2070, train acc: 0.9219, val loss: 0.1446, val acc: 0.9477  (best train acc: 0.9351, best val acc: 0.9605, best train loss: 0.1817  @ epoch 15679 )\n",
      "[Epoch: 15760] train loss: 0.2145, train acc: 0.9190, val loss: 0.1443, val acc: 0.9420  (best train acc: 0.9351, best val acc: 0.9605, best train loss: 0.1817  @ epoch 15679 )\n",
      "[Epoch: 15780] train loss: 0.2040, train acc: 0.9246, val loss: 0.1393, val acc: 0.9521  (best train acc: 0.9351, best val acc: 0.9605, best train loss: 0.1817  @ epoch 15679 )\n",
      "[Epoch: 15800] train loss: 0.1999, train acc: 0.9275, val loss: 0.1327, val acc: 0.9528  (best train acc: 0.9351, best val acc: 0.9605, best train loss: 0.1816  @ epoch 15790 )\n",
      "[Epoch: 15820] train loss: 0.2096, train acc: 0.9229, val loss: 0.1376, val acc: 0.9555  (best train acc: 0.9368, best val acc: 0.9605, best train loss: 0.1816  @ epoch 15790 )\n",
      "[Epoch: 15840] train loss: 0.2042, train acc: 0.9242, val loss: 0.1402, val acc: 0.9551  (best train acc: 0.9368, best val acc: 0.9605, best train loss: 0.1816  @ epoch 15790 )\n",
      "[Epoch: 15860] train loss: 0.2082, train acc: 0.9169, val loss: 0.1369, val acc: 0.9511  (best train acc: 0.9368, best val acc: 0.9605, best train loss: 0.1816  @ epoch 15790 )\n",
      "[Epoch: 15880] train loss: 0.1892, train acc: 0.9297, val loss: 0.1358, val acc: 0.9545  (best train acc: 0.9368, best val acc: 0.9605, best train loss: 0.1816  @ epoch 15790 )\n",
      "[Epoch: 15900] train loss: 0.2200, train acc: 0.9195, val loss: 0.1317, val acc: 0.9551  (best train acc: 0.9368, best val acc: 0.9605, best train loss: 0.1816  @ epoch 15790 )\n",
      "[Epoch: 15920] train loss: 0.2397, train acc: 0.9050, val loss: 0.1394, val acc: 0.9535  (best train acc: 0.9368, best val acc: 0.9605, best train loss: 0.1816  @ epoch 15790 )\n",
      "[Epoch: 15940] train loss: 0.2975, train acc: 0.8806, val loss: 0.1295, val acc: 0.9565  (best train acc: 0.9368, best val acc: 0.9605, best train loss: 0.1816  @ epoch 15790 )\n",
      "[Epoch: 15960] train loss: 0.1972, train acc: 0.9274, val loss: 0.1531, val acc: 0.9403  (best train acc: 0.9368, best val acc: 0.9605, best train loss: 0.1816  @ epoch 15790 )\n",
      "[Epoch: 15980] train loss: 0.1971, train acc: 0.9286, val loss: 0.1368, val acc: 0.9562  (best train acc: 0.9368, best val acc: 0.9605, best train loss: 0.1816  @ epoch 15790 )\n",
      "[Epoch: 16000] train loss: 0.2216, train acc: 0.9176, val loss: 0.1310, val acc: 0.9568  (best train acc: 0.9368, best val acc: 0.9605, best train loss: 0.1800  @ epoch 15991 )\n",
      "[Epoch: 16020] train loss: 0.1965, train acc: 0.9284, val loss: 0.1428, val acc: 0.9508  (best train acc: 0.9368, best val acc: 0.9605, best train loss: 0.1800  @ epoch 15991 )\n",
      "[Epoch: 16040] train loss: 0.1961, train acc: 0.9318, val loss: 0.1278, val acc: 0.9572  (best train acc: 0.9368, best val acc: 0.9605, best train loss: 0.1800  @ epoch 15991 )\n",
      "[Epoch: 16060] train loss: 0.2019, train acc: 0.9242, val loss: 0.1350, val acc: 0.9521  (best train acc: 0.9368, best val acc: 0.9605, best train loss: 0.1800  @ epoch 15991 )\n",
      "[Epoch: 16080] train loss: 0.1916, train acc: 0.9280, val loss: 0.1255, val acc: 0.9589  (best train acc: 0.9368, best val acc: 0.9605, best train loss: 0.1800  @ epoch 15991 )\n",
      "[Epoch: 16100] train loss: 0.3156, train acc: 0.8824, val loss: 0.1401, val acc: 0.9514  (best train acc: 0.9368, best val acc: 0.9605, best train loss: 0.1800  @ epoch 15991 )\n",
      "[Epoch: 16120] train loss: 0.2180, train acc: 0.9247, val loss: 0.1383, val acc: 0.9457  (best train acc: 0.9377, best val acc: 0.9605, best train loss: 0.1800  @ epoch 15991 )\n",
      "[Epoch: 16140] train loss: 0.3037, train acc: 0.8921, val loss: 0.1373, val acc: 0.9548  (best train acc: 0.9377, best val acc: 0.9605, best train loss: 0.1800  @ epoch 15991 )\n",
      "[Epoch: 16160] train loss: 0.2054, train acc: 0.9238, val loss: 0.1365, val acc: 0.9538  (best train acc: 0.9377, best val acc: 0.9605, best train loss: 0.1800  @ epoch 15991 )\n",
      "[Epoch: 16180] train loss: 0.1990, train acc: 0.9276, val loss: 0.1291, val acc: 0.9538  (best train acc: 0.9377, best val acc: 0.9605, best train loss: 0.1800  @ epoch 15991 )\n",
      "[Epoch: 16200] train loss: 0.1962, train acc: 0.9253, val loss: 0.1301, val acc: 0.9562  (best train acc: 0.9377, best val acc: 0.9605, best train loss: 0.1800  @ epoch 15991 )\n",
      "[Epoch: 16220] train loss: 0.1958, train acc: 0.9277, val loss: 0.1325, val acc: 0.9511  (best train acc: 0.9377, best val acc: 0.9605, best train loss: 0.1800  @ epoch 15991 )\n",
      "[Epoch: 16240] train loss: 0.2239, train acc: 0.9187, val loss: 0.1258, val acc: 0.9572  (best train acc: 0.9377, best val acc: 0.9605, best train loss: 0.1800  @ epoch 15991 )\n",
      "[Epoch: 16260] train loss: 0.2146, train acc: 0.9157, val loss: 0.1352, val acc: 0.9508  (best train acc: 0.9377, best val acc: 0.9605, best train loss: 0.1800  @ epoch 15991 )\n",
      "[Epoch: 16280] train loss: 0.2152, train acc: 0.9229, val loss: 0.1312, val acc: 0.9524  (best train acc: 0.9377, best val acc: 0.9605, best train loss: 0.1800  @ epoch 15991 )\n",
      "[Epoch: 16300] train loss: 0.1928, train acc: 0.9292, val loss: 0.1305, val acc: 0.9572  (best train acc: 0.9377, best val acc: 0.9605, best train loss: 0.1800  @ epoch 15991 )\n",
      "[Epoch: 16320] train loss: 0.2413, train acc: 0.9109, val loss: 0.1288, val acc: 0.9589  (best train acc: 0.9377, best val acc: 0.9605, best train loss: 0.1800  @ epoch 15991 )\n",
      "[Epoch: 16340] train loss: 0.2086, train acc: 0.9169, val loss: 0.1285, val acc: 0.9582  (best train acc: 0.9377, best val acc: 0.9605, best train loss: 0.1800  @ epoch 15991 )\n",
      "[Epoch: 16360] train loss: 0.2068, train acc: 0.9249, val loss: 0.1260, val acc: 0.9558  (best train acc: 0.9377, best val acc: 0.9605, best train loss: 0.1800  @ epoch 15991 )\n",
      "[Epoch: 16380] train loss: 0.2313, train acc: 0.9070, val loss: 0.1298, val acc: 0.9545  (best train acc: 0.9377, best val acc: 0.9605, best train loss: 0.1800  @ epoch 15991 )\n",
      "[Epoch: 16400] train loss: 0.2124, train acc: 0.9216, val loss: 0.1265, val acc: 0.9548  (best train acc: 0.9377, best val acc: 0.9605, best train loss: 0.1800  @ epoch 15991 )\n",
      "[Epoch: 16420] train loss: 0.2085, train acc: 0.9177, val loss: 0.1277, val acc: 0.9562  (best train acc: 0.9377, best val acc: 0.9605, best train loss: 0.1794  @ epoch 16419 )\n",
      "[Epoch: 16440] train loss: 0.2075, train acc: 0.9165, val loss: 0.1323, val acc: 0.9538  (best train acc: 0.9377, best val acc: 0.9605, best train loss: 0.1794  @ epoch 16419 )\n",
      "[Epoch: 16460] train loss: 0.1973, train acc: 0.9203, val loss: 0.1280, val acc: 0.9551  (best train acc: 0.9377, best val acc: 0.9605, best train loss: 0.1794  @ epoch 16419 )\n",
      "[Epoch: 16480] train loss: 0.2074, train acc: 0.9280, val loss: 0.1310, val acc: 0.9562  (best train acc: 0.9377, best val acc: 0.9605, best train loss: 0.1794  @ epoch 16419 )\n",
      "[Epoch: 16500] train loss: 0.1886, train acc: 0.9325, val loss: 0.1297, val acc: 0.9535  (best train acc: 0.9377, best val acc: 0.9605, best train loss: 0.1794  @ epoch 16419 )\n",
      "[Epoch: 16520] train loss: 0.2417, train acc: 0.9090, val loss: 0.1398, val acc: 0.9470  (best train acc: 0.9377, best val acc: 0.9605, best train loss: 0.1794  @ epoch 16419 )\n",
      "[Epoch: 16540] train loss: 0.1925, train acc: 0.9311, val loss: 0.1259, val acc: 0.9595  (best train acc: 0.9377, best val acc: 0.9605, best train loss: 0.1794  @ epoch 16419 )\n",
      "[Epoch: 16560] train loss: 0.1991, train acc: 0.9242, val loss: 0.1253, val acc: 0.9582  (best train acc: 0.9377, best val acc: 0.9605, best train loss: 0.1794  @ epoch 16419 )\n",
      "[Epoch: 16580] train loss: 0.1946, train acc: 0.9275, val loss: 0.1383, val acc: 0.9504  (best train acc: 0.9377, best val acc: 0.9605, best train loss: 0.1794  @ epoch 16419 )\n",
      "[Epoch: 16600] train loss: 0.2031, train acc: 0.9278, val loss: 0.1276, val acc: 0.9558  (best train acc: 0.9377, best val acc: 0.9605, best train loss: 0.1742  @ epoch 16594 )\n",
      "[Epoch: 16620] train loss: 0.2640, train acc: 0.8835, val loss: 0.1383, val acc: 0.9511  (best train acc: 0.9378, best val acc: 0.9605, best train loss: 0.1742  @ epoch 16594 )\n",
      "[Epoch: 16640] train loss: 0.2065, train acc: 0.9281, val loss: 0.1314, val acc: 0.9531  (best train acc: 0.9378, best val acc: 0.9605, best train loss: 0.1742  @ epoch 16594 )\n",
      "[Epoch: 16660] train loss: 0.2386, train acc: 0.9100, val loss: 0.1262, val acc: 0.9562  (best train acc: 0.9378, best val acc: 0.9605, best train loss: 0.1742  @ epoch 16594 )\n",
      "[Epoch: 16680] train loss: 0.1981, train acc: 0.9234, val loss: 0.1227, val acc: 0.9605  (best train acc: 0.9378, best val acc: 0.9605, best train loss: 0.1742  @ epoch 16594 )\n",
      "[Epoch: 16700] train loss: 0.1845, train acc: 0.9325, val loss: 0.1341, val acc: 0.9545  (best train acc: 0.9378, best val acc: 0.9609, best train loss: 0.1742  @ epoch 16594 )\n",
      "[Epoch: 16720] train loss: 0.1972, train acc: 0.9226, val loss: 0.1279, val acc: 0.9555  (best train acc: 0.9378, best val acc: 0.9609, best train loss: 0.1742  @ epoch 16594 )\n",
      "[Epoch: 16740] train loss: 0.1796, train acc: 0.9350, val loss: 0.1324, val acc: 0.9548  (best train acc: 0.9378, best val acc: 0.9609, best train loss: 0.1742  @ epoch 16594 )\n",
      "[Epoch: 16760] train loss: 0.2138, train acc: 0.9267, val loss: 0.1315, val acc: 0.9548  (best train acc: 0.9378, best val acc: 0.9609, best train loss: 0.1742  @ epoch 16594 )\n",
      "[Epoch: 16780] train loss: 0.1832, train acc: 0.9327, val loss: 0.1326, val acc: 0.9555  (best train acc: 0.9378, best val acc: 0.9609, best train loss: 0.1742  @ epoch 16594 )\n",
      "[Epoch: 16800] train loss: 0.2103, train acc: 0.9239, val loss: 0.1428, val acc: 0.9457  (best train acc: 0.9378, best val acc: 0.9609, best train loss: 0.1742  @ epoch 16594 )\n",
      "[Epoch: 16820] train loss: 0.2355, train acc: 0.9000, val loss: 0.1293, val acc: 0.9528  (best train acc: 0.9378, best val acc: 0.9616, best train loss: 0.1742  @ epoch 16594 )\n",
      "[Epoch: 16840] train loss: 0.2329, train acc: 0.9123, val loss: 0.1270, val acc: 0.9551  (best train acc: 0.9379, best val acc: 0.9616, best train loss: 0.1742  @ epoch 16594 )\n",
      "[Epoch: 16860] train loss: 0.1916, train acc: 0.9252, val loss: 0.1365, val acc: 0.9508  (best train acc: 0.9379, best val acc: 0.9616, best train loss: 0.1742  @ epoch 16594 )\n",
      "[Epoch: 16880] train loss: 0.2019, train acc: 0.9227, val loss: 0.1229, val acc: 0.9551  (best train acc: 0.9379, best val acc: 0.9616, best train loss: 0.1742  @ epoch 16594 )\n",
      "[Epoch: 16900] train loss: 0.2052, train acc: 0.9323, val loss: 0.1316, val acc: 0.9555  (best train acc: 0.9379, best val acc: 0.9616, best train loss: 0.1742  @ epoch 16594 )\n",
      "[Epoch: 16920] train loss: 0.1961, train acc: 0.9328, val loss: 0.1268, val acc: 0.9555  (best train acc: 0.9379, best val acc: 0.9616, best train loss: 0.1732  @ epoch 16901 )\n",
      "[Epoch: 16940] train loss: 0.2061, train acc: 0.9288, val loss: 0.1221, val acc: 0.9572  (best train acc: 0.9379, best val acc: 0.9616, best train loss: 0.1732  @ epoch 16901 )\n",
      "[Epoch: 16960] train loss: 0.2222, train acc: 0.9231, val loss: 0.1316, val acc: 0.9538  (best train acc: 0.9379, best val acc: 0.9616, best train loss: 0.1732  @ epoch 16901 )\n",
      "[Epoch: 16980] train loss: 0.1959, train acc: 0.9229, val loss: 0.1250, val acc: 0.9562  (best train acc: 0.9387, best val acc: 0.9616, best train loss: 0.1695  @ epoch 16962 )\n",
      "[Epoch: 17000] train loss: 0.1913, train acc: 0.9256, val loss: 0.1456, val acc: 0.9497  (best train acc: 0.9387, best val acc: 0.9616, best train loss: 0.1695  @ epoch 16962 )\n",
      "[Epoch: 17020] train loss: 0.2188, train acc: 0.9163, val loss: 0.1287, val acc: 0.9558  (best train acc: 0.9387, best val acc: 0.9616, best train loss: 0.1695  @ epoch 16962 )\n",
      "[Epoch: 17040] train loss: 0.2057, train acc: 0.9257, val loss: 0.1285, val acc: 0.9572  (best train acc: 0.9387, best val acc: 0.9616, best train loss: 0.1695  @ epoch 16962 )\n",
      "[Epoch: 17060] train loss: 0.2097, train acc: 0.9171, val loss: 0.1515, val acc: 0.9396  (best train acc: 0.9387, best val acc: 0.9616, best train loss: 0.1695  @ epoch 16962 )\n",
      "[Epoch: 17080] train loss: 0.1886, train acc: 0.9282, val loss: 0.1270, val acc: 0.9575  (best train acc: 0.9387, best val acc: 0.9616, best train loss: 0.1695  @ epoch 16962 )\n",
      "[Epoch: 17100] train loss: 0.1851, train acc: 0.9283, val loss: 0.1272, val acc: 0.9595  (best train acc: 0.9387, best val acc: 0.9616, best train loss: 0.1695  @ epoch 16962 )\n",
      "[Epoch: 17120] train loss: 0.1810, train acc: 0.9355, val loss: 0.1423, val acc: 0.9508  (best train acc: 0.9387, best val acc: 0.9616, best train loss: 0.1695  @ epoch 16962 )\n",
      "[Epoch: 17140] train loss: 0.2247, train acc: 0.9152, val loss: 0.1329, val acc: 0.9558  (best train acc: 0.9387, best val acc: 0.9616, best train loss: 0.1695  @ epoch 16962 )\n",
      "[Epoch: 17160] train loss: 0.2055, train acc: 0.9245, val loss: 0.1460, val acc: 0.9487  (best train acc: 0.9387, best val acc: 0.9616, best train loss: 0.1695  @ epoch 16962 )\n",
      "[Epoch: 17180] train loss: 0.1937, train acc: 0.9289, val loss: 0.1345, val acc: 0.9518  (best train acc: 0.9387, best val acc: 0.9616, best train loss: 0.1695  @ epoch 16962 )\n",
      "[Epoch: 17200] train loss: 0.1956, train acc: 0.9226, val loss: 0.1369, val acc: 0.9531  (best train acc: 0.9387, best val acc: 0.9616, best train loss: 0.1695  @ epoch 16962 )\n",
      "[Epoch: 17220] train loss: 0.2325, train acc: 0.9104, val loss: 0.1354, val acc: 0.9538  (best train acc: 0.9387, best val acc: 0.9616, best train loss: 0.1695  @ epoch 16962 )\n",
      "[Epoch: 17240] train loss: 0.1945, train acc: 0.9261, val loss: 0.1257, val acc: 0.9548  (best train acc: 0.9387, best val acc: 0.9616, best train loss: 0.1695  @ epoch 16962 )\n",
      "[Epoch: 17260] train loss: 0.2110, train acc: 0.9205, val loss: 0.1366, val acc: 0.9484  (best train acc: 0.9387, best val acc: 0.9616, best train loss: 0.1695  @ epoch 16962 )\n",
      "[Epoch: 17280] train loss: 0.2562, train acc: 0.8950, val loss: 0.1374, val acc: 0.9460  (best train acc: 0.9387, best val acc: 0.9616, best train loss: 0.1695  @ epoch 16962 )\n",
      "[Epoch: 17300] train loss: 0.2132, train acc: 0.9200, val loss: 0.1307, val acc: 0.9551  (best train acc: 0.9387, best val acc: 0.9616, best train loss: 0.1695  @ epoch 16962 )\n",
      "[Epoch: 17320] train loss: 0.2010, train acc: 0.9195, val loss: 0.1320, val acc: 0.9535  (best train acc: 0.9387, best val acc: 0.9616, best train loss: 0.1695  @ epoch 16962 )\n",
      "[Epoch: 17340] train loss: 0.2819, train acc: 0.8897, val loss: 0.1250, val acc: 0.9535  (best train acc: 0.9387, best val acc: 0.9616, best train loss: 0.1695  @ epoch 16962 )\n",
      "[Epoch: 17360] train loss: 0.2645, train acc: 0.9031, val loss: 0.1270, val acc: 0.9575  (best train acc: 0.9387, best val acc: 0.9616, best train loss: 0.1695  @ epoch 16962 )\n",
      "[Epoch: 17380] train loss: 0.2271, train acc: 0.9109, val loss: 0.1382, val acc: 0.9524  (best train acc: 0.9387, best val acc: 0.9616, best train loss: 0.1695  @ epoch 16962 )\n",
      "[Epoch: 17400] train loss: 0.1921, train acc: 0.9276, val loss: 0.1365, val acc: 0.9481  (best train acc: 0.9387, best val acc: 0.9616, best train loss: 0.1695  @ epoch 16962 )\n",
      "[Epoch: 17420] train loss: 0.2317, train acc: 0.9092, val loss: 0.1307, val acc: 0.9514  (best train acc: 0.9387, best val acc: 0.9616, best train loss: 0.1695  @ epoch 16962 )\n",
      "[Epoch: 17440] train loss: 0.1842, train acc: 0.9283, val loss: 0.1348, val acc: 0.9504  (best train acc: 0.9387, best val acc: 0.9616, best train loss: 0.1695  @ epoch 16962 )\n",
      "[Epoch: 17460] train loss: 0.1823, train acc: 0.9301, val loss: 0.1295, val acc: 0.9555  (best train acc: 0.9387, best val acc: 0.9616, best train loss: 0.1695  @ epoch 16962 )\n",
      "[Epoch: 17480] train loss: 0.1940, train acc: 0.9267, val loss: 0.1300, val acc: 0.9541  (best train acc: 0.9387, best val acc: 0.9616, best train loss: 0.1695  @ epoch 16962 )\n",
      "[Epoch: 17500] train loss: 0.2105, train acc: 0.9203, val loss: 0.1325, val acc: 0.9541  (best train acc: 0.9387, best val acc: 0.9616, best train loss: 0.1695  @ epoch 16962 )\n",
      "[Epoch: 17520] train loss: 0.2004, train acc: 0.9220, val loss: 0.1272, val acc: 0.9548  (best train acc: 0.9387, best val acc: 0.9616, best train loss: 0.1695  @ epoch 16962 )\n",
      "[Epoch: 17540] train loss: 0.1976, train acc: 0.9262, val loss: 0.1410, val acc: 0.9541  (best train acc: 0.9387, best val acc: 0.9616, best train loss: 0.1695  @ epoch 16962 )\n",
      "[Epoch: 17560] train loss: 0.1863, train acc: 0.9345, val loss: 0.1287, val acc: 0.9551  (best train acc: 0.9387, best val acc: 0.9616, best train loss: 0.1695  @ epoch 16962 )\n",
      "[Epoch: 17580] train loss: 0.1910, train acc: 0.9289, val loss: 0.1327, val acc: 0.9551  (best train acc: 0.9387, best val acc: 0.9616, best train loss: 0.1695  @ epoch 16962 )\n",
      "[Epoch: 17600] train loss: 0.1851, train acc: 0.9297, val loss: 0.1301, val acc: 0.9545  (best train acc: 0.9387, best val acc: 0.9616, best train loss: 0.1695  @ epoch 16962 )\n",
      "[Epoch: 17620] train loss: 0.2083, train acc: 0.9181, val loss: 0.1271, val acc: 0.9551  (best train acc: 0.9387, best val acc: 0.9616, best train loss: 0.1695  @ epoch 16962 )\n",
      "[Epoch: 17640] train loss: 0.1979, train acc: 0.9279, val loss: 0.1283, val acc: 0.9531  (best train acc: 0.9387, best val acc: 0.9616, best train loss: 0.1695  @ epoch 16962 )\n",
      "[Epoch: 17660] train loss: 0.2286, train acc: 0.9143, val loss: 0.1340, val acc: 0.9504  (best train acc: 0.9387, best val acc: 0.9616, best train loss: 0.1695  @ epoch 16962 )\n",
      "[Epoch: 17680] train loss: 0.1797, train acc: 0.9355, val loss: 0.1384, val acc: 0.9467  (best train acc: 0.9387, best val acc: 0.9616, best train loss: 0.1695  @ epoch 16962 )\n",
      "[Epoch: 17700] train loss: 0.1887, train acc: 0.9313, val loss: 0.1274, val acc: 0.9582  (best train acc: 0.9387, best val acc: 0.9616, best train loss: 0.1695  @ epoch 16962 )\n",
      "[Epoch: 17720] train loss: 0.2019, train acc: 0.9247, val loss: 0.1280, val acc: 0.9565  (best train acc: 0.9387, best val acc: 0.9616, best train loss: 0.1695  @ epoch 16962 )\n",
      "[Epoch: 17740] train loss: 0.2280, train acc: 0.9031, val loss: 0.1372, val acc: 0.9464  (best train acc: 0.9387, best val acc: 0.9616, best train loss: 0.1695  @ epoch 16962 )\n",
      "[Epoch: 17760] train loss: 0.1947, train acc: 0.9259, val loss: 0.1312, val acc: 0.9565  (best train acc: 0.9387, best val acc: 0.9616, best train loss: 0.1695  @ epoch 16962 )\n",
      "[Epoch: 17780] train loss: 0.2202, train acc: 0.9194, val loss: 0.1307, val acc: 0.9511  (best train acc: 0.9387, best val acc: 0.9616, best train loss: 0.1695  @ epoch 16962 )\n",
      "[Epoch: 17800] train loss: 0.1944, train acc: 0.9262, val loss: 0.1326, val acc: 0.9545  (best train acc: 0.9387, best val acc: 0.9616, best train loss: 0.1695  @ epoch 16962 )\n",
      "[Epoch: 17820] train loss: 0.2030, train acc: 0.9268, val loss: 0.1273, val acc: 0.9551  (best train acc: 0.9387, best val acc: 0.9616, best train loss: 0.1695  @ epoch 16962 )\n",
      "[Epoch: 17840] train loss: 0.1906, train acc: 0.9324, val loss: 0.1332, val acc: 0.9535  (best train acc: 0.9387, best val acc: 0.9616, best train loss: 0.1695  @ epoch 16962 )\n",
      "[Epoch: 17860] train loss: 0.2090, train acc: 0.9204, val loss: 0.1386, val acc: 0.9491  (best train acc: 0.9387, best val acc: 0.9616, best train loss: 0.1695  @ epoch 16962 )\n",
      "[Epoch: 17880] train loss: 0.1967, train acc: 0.9209, val loss: 0.1428, val acc: 0.9427  (best train acc: 0.9387, best val acc: 0.9616, best train loss: 0.1695  @ epoch 16962 )\n",
      "[Epoch: 17900] train loss: 0.2077, train acc: 0.9192, val loss: 0.1381, val acc: 0.9470  (best train acc: 0.9387, best val acc: 0.9616, best train loss: 0.1695  @ epoch 16962 )\n",
      "[Epoch: 17920] train loss: 0.2013, train acc: 0.9231, val loss: 0.1257, val acc: 0.9578  (best train acc: 0.9387, best val acc: 0.9616, best train loss: 0.1695  @ epoch 16962 )\n",
      "[Epoch: 17940] train loss: 0.1999, train acc: 0.9271, val loss: 0.1319, val acc: 0.9558  (best train acc: 0.9387, best val acc: 0.9616, best train loss: 0.1695  @ epoch 16962 )\n",
      "[Epoch: 17960] train loss: 0.2977, train acc: 0.8924, val loss: 0.1252, val acc: 0.9582  (best train acc: 0.9387, best val acc: 0.9616, best train loss: 0.1695  @ epoch 16962 )\n",
      "[Epoch: 17980] train loss: 0.1982, train acc: 0.9249, val loss: 0.1310, val acc: 0.9545  (best train acc: 0.9387, best val acc: 0.9616, best train loss: 0.1695  @ epoch 16962 )\n",
      "[Epoch: 18000] train loss: 0.1902, train acc: 0.9281, val loss: 0.1362, val acc: 0.9514  (best train acc: 0.9387, best val acc: 0.9616, best train loss: 0.1695  @ epoch 16962 )\n",
      "[Epoch: 18020] train loss: 0.1968, train acc: 0.9296, val loss: 0.1342, val acc: 0.9538  (best train acc: 0.9387, best val acc: 0.9616, best train loss: 0.1695  @ epoch 16962 )\n",
      "[Epoch: 18040] train loss: 0.2295, train acc: 0.9076, val loss: 0.1309, val acc: 0.9535  (best train acc: 0.9387, best val acc: 0.9616, best train loss: 0.1695  @ epoch 16962 )\n",
      "[Epoch: 18060] train loss: 0.2001, train acc: 0.9233, val loss: 0.1412, val acc: 0.9410  (best train acc: 0.9387, best val acc: 0.9616, best train loss: 0.1695  @ epoch 16962 )\n",
      "[Epoch: 18080] train loss: 0.1859, train acc: 0.9322, val loss: 0.1330, val acc: 0.9511  (best train acc: 0.9387, best val acc: 0.9616, best train loss: 0.1695  @ epoch 16962 )\n",
      "[Epoch: 18100] train loss: 0.2142, train acc: 0.9179, val loss: 0.1342, val acc: 0.9541  (best train acc: 0.9387, best val acc: 0.9616, best train loss: 0.1695  @ epoch 16962 )\n",
      "[Epoch: 18120] train loss: 0.1902, train acc: 0.9315, val loss: 0.1308, val acc: 0.9501  (best train acc: 0.9387, best val acc: 0.9616, best train loss: 0.1695  @ epoch 16962 )\n",
      "[Epoch: 18140] train loss: 0.2355, train acc: 0.9056, val loss: 0.1315, val acc: 0.9548  (best train acc: 0.9387, best val acc: 0.9616, best train loss: 0.1695  @ epoch 16962 )\n",
      "[Epoch: 18160] train loss: 0.2204, train acc: 0.9135, val loss: 0.1268, val acc: 0.9531  (best train acc: 0.9387, best val acc: 0.9616, best train loss: 0.1695  @ epoch 16962 )\n",
      "[Epoch: 18180] train loss: 0.2266, train acc: 0.9112, val loss: 0.1328, val acc: 0.9538  (best train acc: 0.9387, best val acc: 0.9616, best train loss: 0.1695  @ epoch 16962 )\n",
      "[Epoch: 18200] train loss: 0.2364, train acc: 0.9123, val loss: 0.1260, val acc: 0.9548  (best train acc: 0.9387, best val acc: 0.9616, best train loss: 0.1695  @ epoch 16962 )\n",
      "[Epoch: 18220] train loss: 0.2178, train acc: 0.9166, val loss: 0.1319, val acc: 0.9531  (best train acc: 0.9387, best val acc: 0.9616, best train loss: 0.1695  @ epoch 16962 )\n",
      "[Epoch: 18240] train loss: 0.2114, train acc: 0.9182, val loss: 0.1309, val acc: 0.9514  (best train acc: 0.9387, best val acc: 0.9616, best train loss: 0.1695  @ epoch 16962 )\n",
      "[Epoch: 18260] train loss: 0.2214, train acc: 0.9157, val loss: 0.1399, val acc: 0.9481  (best train acc: 0.9387, best val acc: 0.9616, best train loss: 0.1695  @ epoch 16962 )\n",
      "[Epoch: 18280] train loss: 0.1869, train acc: 0.9293, val loss: 0.1264, val acc: 0.9548  (best train acc: 0.9387, best val acc: 0.9616, best train loss: 0.1695  @ epoch 16962 )\n",
      "[Epoch: 18300] train loss: 0.1872, train acc: 0.9300, val loss: 0.1428, val acc: 0.9427  (best train acc: 0.9387, best val acc: 0.9616, best train loss: 0.1695  @ epoch 16962 )\n",
      "[Epoch: 18320] train loss: 0.1968, train acc: 0.9272, val loss: 0.1312, val acc: 0.9491  (best train acc: 0.9387, best val acc: 0.9616, best train loss: 0.1695  @ epoch 16962 )\n",
      "[Epoch: 18340] train loss: 0.1937, train acc: 0.9260, val loss: 0.1330, val acc: 0.9508  (best train acc: 0.9387, best val acc: 0.9616, best train loss: 0.1695  @ epoch 16962 )\n",
      "[Epoch: 18360] train loss: 0.1837, train acc: 0.9299, val loss: 0.1298, val acc: 0.9545  (best train acc: 0.9387, best val acc: 0.9616, best train loss: 0.1695  @ epoch 16962 )\n",
      "[Epoch: 18380] train loss: 0.1988, train acc: 0.9264, val loss: 0.1313, val acc: 0.9508  (best train acc: 0.9387, best val acc: 0.9616, best train loss: 0.1695  @ epoch 16962 )\n",
      "[Epoch: 18400] train loss: 0.1816, train acc: 0.9370, val loss: 0.1307, val acc: 0.9521  (best train acc: 0.9387, best val acc: 0.9616, best train loss: 0.1695  @ epoch 16962 )\n",
      "[Epoch: 18420] train loss: 0.2017, train acc: 0.9255, val loss: 0.1275, val acc: 0.9565  (best train acc: 0.9387, best val acc: 0.9616, best train loss: 0.1695  @ epoch 16962 )\n",
      "[Epoch: 18440] train loss: 0.1872, train acc: 0.9320, val loss: 0.1382, val acc: 0.9511  (best train acc: 0.9387, best val acc: 0.9616, best train loss: 0.1695  @ epoch 16962 )\n",
      "[Epoch: 18460] train loss: 0.1944, train acc: 0.9206, val loss: 0.1281, val acc: 0.9575  (best train acc: 0.9387, best val acc: 0.9616, best train loss: 0.1695  @ epoch 16962 )\n",
      "[Epoch: 18480] train loss: 0.2239, train acc: 0.9132, val loss: 0.1360, val acc: 0.9528  (best train acc: 0.9387, best val acc: 0.9616, best train loss: 0.1695  @ epoch 16962 )\n",
      "[Epoch: 18500] train loss: 0.2403, train acc: 0.8995, val loss: 0.1240, val acc: 0.9562  (best train acc: 0.9387, best val acc: 0.9616, best train loss: 0.1695  @ epoch 16962 )\n",
      "[Epoch: 18520] train loss: 0.2101, train acc: 0.9206, val loss: 0.1283, val acc: 0.9562  (best train acc: 0.9387, best val acc: 0.9616, best train loss: 0.1695  @ epoch 16962 )\n",
      "[Epoch: 18540] train loss: 0.1906, train acc: 0.9237, val loss: 0.1279, val acc: 0.9528  (best train acc: 0.9387, best val acc: 0.9616, best train loss: 0.1695  @ epoch 16962 )\n",
      "[Epoch: 18560] train loss: 0.2216, train acc: 0.9118, val loss: 0.1263, val acc: 0.9565  (best train acc: 0.9387, best val acc: 0.9616, best train loss: 0.1695  @ epoch 16962 )\n",
      "[Epoch: 18580] train loss: 0.2066, train acc: 0.9247, val loss: 0.1352, val acc: 0.9474  (best train acc: 0.9387, best val acc: 0.9616, best train loss: 0.1695  @ epoch 16962 )\n",
      "[Epoch: 18600] train loss: 0.2118, train acc: 0.9192, val loss: 0.1247, val acc: 0.9538  (best train acc: 0.9387, best val acc: 0.9616, best train loss: 0.1695  @ epoch 16962 )\n",
      "[Epoch: 18620] train loss: 0.2195, train acc: 0.9047, val loss: 0.1297, val acc: 0.9568  (best train acc: 0.9387, best val acc: 0.9616, best train loss: 0.1695  @ epoch 16962 )\n",
      "[Epoch: 18640] train loss: 0.2046, train acc: 0.9242, val loss: 0.1412, val acc: 0.9413  (best train acc: 0.9387, best val acc: 0.9616, best train loss: 0.1695  @ epoch 16962 )\n",
      "[Epoch: 18660] train loss: 0.2073, train acc: 0.9208, val loss: 0.1397, val acc: 0.9444  (best train acc: 0.9387, best val acc: 0.9616, best train loss: 0.1695  @ epoch 16962 )\n",
      "[Epoch: 18680] train loss: 0.2243, train acc: 0.9115, val loss: 0.1228, val acc: 0.9558  (best train acc: 0.9387, best val acc: 0.9616, best train loss: 0.1695  @ epoch 16962 )\n",
      "[Epoch: 18700] train loss: 0.2007, train acc: 0.9216, val loss: 0.1374, val acc: 0.9454  (best train acc: 0.9387, best val acc: 0.9616, best train loss: 0.1695  @ epoch 16962 )\n",
      "[Epoch: 18720] train loss: 0.2097, train acc: 0.9187, val loss: 0.1183, val acc: 0.9578  (best train acc: 0.9387, best val acc: 0.9616, best train loss: 0.1695  @ epoch 16962 )\n",
      "[Epoch: 18740] train loss: 0.1972, train acc: 0.9270, val loss: 0.1223, val acc: 0.9565  (best train acc: 0.9387, best val acc: 0.9616, best train loss: 0.1695  @ epoch 16962 )\n",
      "[Epoch: 18760] train loss: 0.2121, train acc: 0.9248, val loss: 0.1356, val acc: 0.9467  (best train acc: 0.9387, best val acc: 0.9616, best train loss: 0.1695  @ epoch 16962 )\n",
      "[Epoch: 18780] train loss: 0.2128, train acc: 0.9240, val loss: 0.1218, val acc: 0.9575  (best train acc: 0.9387, best val acc: 0.9616, best train loss: 0.1695  @ epoch 16962 )\n",
      "[Epoch: 18800] train loss: 0.1819, train acc: 0.9358, val loss: 0.1246, val acc: 0.9568  (best train acc: 0.9387, best val acc: 0.9616, best train loss: 0.1695  @ epoch 16962 )\n",
      "[Epoch: 18820] train loss: 0.2092, train acc: 0.9202, val loss: 0.1380, val acc: 0.9460  (best train acc: 0.9387, best val acc: 0.9616, best train loss: 0.1695  @ epoch 16962 )\n",
      "[Epoch: 18840] train loss: 0.1877, train acc: 0.9334, val loss: 0.1259, val acc: 0.9582  (best train acc: 0.9387, best val acc: 0.9616, best train loss: 0.1695  @ epoch 16962 )\n",
      "[Epoch: 18860] train loss: 0.1833, train acc: 0.9368, val loss: 0.1271, val acc: 0.9565  (best train acc: 0.9387, best val acc: 0.9616, best train loss: 0.1695  @ epoch 16962 )\n",
      "[Epoch: 18880] train loss: 0.2100, train acc: 0.9229, val loss: 0.1309, val acc: 0.9538  (best train acc: 0.9387, best val acc: 0.9616, best train loss: 0.1695  @ epoch 16962 )\n",
      "[Epoch: 18900] train loss: 0.2013, train acc: 0.9235, val loss: 0.1408, val acc: 0.9491  (best train acc: 0.9387, best val acc: 0.9616, best train loss: 0.1695  @ epoch 16962 )\n",
      "[Epoch: 18920] train loss: 0.1847, train acc: 0.9341, val loss: 0.1308, val acc: 0.9504  (best train acc: 0.9387, best val acc: 0.9616, best train loss: 0.1695  @ epoch 16962 )\n",
      "[Epoch: 18940] train loss: 0.1972, train acc: 0.9187, val loss: 0.1325, val acc: 0.9562  (best train acc: 0.9387, best val acc: 0.9616, best train loss: 0.1695  @ epoch 16962 )\n",
      "[Epoch: 18960] train loss: 0.1864, train acc: 0.9272, val loss: 0.1289, val acc: 0.9565  (best train acc: 0.9387, best val acc: 0.9616, best train loss: 0.1695  @ epoch 16962 )\n",
      "[Epoch: 18980] train loss: 0.1851, train acc: 0.9337, val loss: 0.1262, val acc: 0.9548  (best train acc: 0.9387, best val acc: 0.9616, best train loss: 0.1695  @ epoch 16962 )\n",
      "[Epoch: 19000] train loss: 0.2804, train acc: 0.8895, val loss: 0.1261, val acc: 0.9555  (best train acc: 0.9387, best val acc: 0.9616, best train loss: 0.1695  @ epoch 16962 )\n",
      "[Epoch: 19020] train loss: 0.1910, train acc: 0.9317, val loss: 0.1273, val acc: 0.9548  (best train acc: 0.9387, best val acc: 0.9616, best train loss: 0.1684  @ epoch 19015 )\n",
      "[Epoch: 19040] train loss: 0.1805, train acc: 0.9358, val loss: 0.1288, val acc: 0.9511  (best train acc: 0.9387, best val acc: 0.9616, best train loss: 0.1684  @ epoch 19015 )\n",
      "[Epoch: 19060] train loss: 0.2409, train acc: 0.9034, val loss: 0.1247, val acc: 0.9555  (best train acc: 0.9387, best val acc: 0.9616, best train loss: 0.1684  @ epoch 19015 )\n",
      "[Epoch: 19080] train loss: 0.1842, train acc: 0.9322, val loss: 0.1333, val acc: 0.9514  (best train acc: 0.9387, best val acc: 0.9616, best train loss: 0.1684  @ epoch 19015 )\n",
      "[Epoch: 19100] train loss: 0.1911, train acc: 0.9276, val loss: 0.1288, val acc: 0.9528  (best train acc: 0.9387, best val acc: 0.9616, best train loss: 0.1684  @ epoch 19015 )\n",
      "[Epoch: 19120] train loss: 0.2294, train acc: 0.9110, val loss: 0.1219, val acc: 0.9555  (best train acc: 0.9387, best val acc: 0.9616, best train loss: 0.1684  @ epoch 19015 )\n",
      "[Epoch: 19140] train loss: 0.2170, train acc: 0.9208, val loss: 0.1255, val acc: 0.9551  (best train acc: 0.9387, best val acc: 0.9616, best train loss: 0.1684  @ epoch 19015 )\n",
      "[Epoch: 19160] train loss: 0.2002, train acc: 0.9244, val loss: 0.1251, val acc: 0.9531  (best train acc: 0.9387, best val acc: 0.9616, best train loss: 0.1684  @ epoch 19015 )\n",
      "[Epoch: 19180] train loss: 0.1947, train acc: 0.9245, val loss: 0.1318, val acc: 0.9508  (best train acc: 0.9387, best val acc: 0.9616, best train loss: 0.1684  @ epoch 19015 )\n",
      "[Epoch: 19200] train loss: 0.2286, train acc: 0.9177, val loss: 0.1241, val acc: 0.9541  (best train acc: 0.9387, best val acc: 0.9616, best train loss: 0.1684  @ epoch 19015 )\n",
      "[Epoch: 19220] train loss: 0.1991, train acc: 0.9243, val loss: 0.1408, val acc: 0.9420  (best train acc: 0.9387, best val acc: 0.9616, best train loss: 0.1684  @ epoch 19015 )\n",
      "[Epoch: 19240] train loss: 0.1729, train acc: 0.9330, val loss: 0.1280, val acc: 0.9504  (best train acc: 0.9387, best val acc: 0.9616, best train loss: 0.1684  @ epoch 19015 )\n",
      "[Epoch: 19260] train loss: 0.2236, train acc: 0.9091, val loss: 0.1236, val acc: 0.9582  (best train acc: 0.9387, best val acc: 0.9616, best train loss: 0.1684  @ epoch 19015 )\n",
      "[Epoch: 19280] train loss: 0.1827, train acc: 0.9328, val loss: 0.1229, val acc: 0.9565  (best train acc: 0.9387, best val acc: 0.9616, best train loss: 0.1684  @ epoch 19015 )\n",
      "[Epoch: 19300] train loss: 0.2236, train acc: 0.9139, val loss: 0.1232, val acc: 0.9562  (best train acc: 0.9387, best val acc: 0.9616, best train loss: 0.1684  @ epoch 19015 )\n",
      "[Epoch: 19320] train loss: 0.2045, train acc: 0.9234, val loss: 0.1224, val acc: 0.9568  (best train acc: 0.9387, best val acc: 0.9616, best train loss: 0.1684  @ epoch 19015 )\n",
      "[Epoch: 19340] train loss: 0.2009, train acc: 0.9211, val loss: 0.1244, val acc: 0.9548  (best train acc: 0.9393, best val acc: 0.9616, best train loss: 0.1678  @ epoch 19328 )\n",
      "[Epoch: 19360] train loss: 0.1744, train acc: 0.9322, val loss: 0.1292, val acc: 0.9538  (best train acc: 0.9393, best val acc: 0.9616, best train loss: 0.1678  @ epoch 19328 )\n",
      "[Epoch: 19380] train loss: 0.1950, train acc: 0.9287, val loss: 0.1239, val acc: 0.9572  (best train acc: 0.9393, best val acc: 0.9616, best train loss: 0.1678  @ epoch 19328 )\n",
      "[Epoch: 19400] train loss: 0.2034, train acc: 0.9197, val loss: 0.1251, val acc: 0.9494  (best train acc: 0.9393, best val acc: 0.9616, best train loss: 0.1678  @ epoch 19328 )\n",
      "[Epoch: 19420] train loss: 0.2130, train acc: 0.9122, val loss: 0.1301, val acc: 0.9511  (best train acc: 0.9393, best val acc: 0.9616, best train loss: 0.1678  @ epoch 19328 )\n",
      "[Epoch: 19440] train loss: 0.1958, train acc: 0.9226, val loss: 0.1227, val acc: 0.9551  (best train acc: 0.9393, best val acc: 0.9616, best train loss: 0.1678  @ epoch 19328 )\n",
      "[Epoch: 19460] train loss: 0.2632, train acc: 0.8960, val loss: 0.1181, val acc: 0.9578  (best train acc: 0.9393, best val acc: 0.9616, best train loss: 0.1678  @ epoch 19328 )\n",
      "[Epoch: 19480] train loss: 0.2860, train acc: 0.8840, val loss: 0.1317, val acc: 0.9548  (best train acc: 0.9393, best val acc: 0.9616, best train loss: 0.1678  @ epoch 19328 )\n",
      "[Epoch: 19500] train loss: 0.1765, train acc: 0.9372, val loss: 0.1200, val acc: 0.9568  (best train acc: 0.9393, best val acc: 0.9616, best train loss: 0.1678  @ epoch 19328 )\n",
      "[Epoch: 19520] train loss: 0.2002, train acc: 0.9235, val loss: 0.1324, val acc: 0.9477  (best train acc: 0.9393, best val acc: 0.9619, best train loss: 0.1678  @ epoch 19328 )\n",
      "[Epoch: 19540] train loss: 0.2313, train acc: 0.9184, val loss: 0.1224, val acc: 0.9538  (best train acc: 0.9393, best val acc: 0.9619, best train loss: 0.1678  @ epoch 19328 )\n",
      "[Epoch: 19560] train loss: 0.1833, train acc: 0.9320, val loss: 0.1345, val acc: 0.9511  (best train acc: 0.9393, best val acc: 0.9619, best train loss: 0.1678  @ epoch 19328 )\n",
      "[Epoch: 19580] train loss: 0.1816, train acc: 0.9332, val loss: 0.1274, val acc: 0.9521  (best train acc: 0.9393, best val acc: 0.9619, best train loss: 0.1678  @ epoch 19328 )\n",
      "[Epoch: 19600] train loss: 0.1972, train acc: 0.9225, val loss: 0.1232, val acc: 0.9585  (best train acc: 0.9393, best val acc: 0.9619, best train loss: 0.1678  @ epoch 19328 )\n",
      "[Epoch: 19620] train loss: 0.2131, train acc: 0.9160, val loss: 0.1251, val acc: 0.9575  (best train acc: 0.9393, best val acc: 0.9619, best train loss: 0.1678  @ epoch 19328 )\n",
      "[Epoch: 19640] train loss: 0.2207, train acc: 0.9096, val loss: 0.1254, val acc: 0.9568  (best train acc: 0.9393, best val acc: 0.9619, best train loss: 0.1678  @ epoch 19328 )\n",
      "[Epoch: 19660] train loss: 0.2219, train acc: 0.9029, val loss: 0.1199, val acc: 0.9558  (best train acc: 0.9393, best val acc: 0.9619, best train loss: 0.1678  @ epoch 19328 )\n",
      "[Epoch: 19680] train loss: 0.1756, train acc: 0.9367, val loss: 0.1226, val acc: 0.9551  (best train acc: 0.9393, best val acc: 0.9619, best train loss: 0.1678  @ epoch 19328 )\n",
      "[Epoch: 19700] train loss: 0.1787, train acc: 0.9310, val loss: 0.1244, val acc: 0.9558  (best train acc: 0.9395, best val acc: 0.9619, best train loss: 0.1678  @ epoch 19328 )\n",
      "[Epoch: 19720] train loss: 0.1935, train acc: 0.9323, val loss: 0.1311, val acc: 0.9551  (best train acc: 0.9395, best val acc: 0.9619, best train loss: 0.1678  @ epoch 19328 )\n",
      "[Epoch: 19740] train loss: 0.1771, train acc: 0.9359, val loss: 0.1286, val acc: 0.9528  (best train acc: 0.9395, best val acc: 0.9619, best train loss: 0.1678  @ epoch 19328 )\n",
      "[Epoch: 19760] train loss: 0.1980, train acc: 0.9263, val loss: 0.1291, val acc: 0.9504  (best train acc: 0.9395, best val acc: 0.9619, best train loss: 0.1678  @ epoch 19328 )\n",
      "[Epoch: 19780] train loss: 0.2275, train acc: 0.9124, val loss: 0.1277, val acc: 0.9538  (best train acc: 0.9401, best val acc: 0.9619, best train loss: 0.1678  @ epoch 19328 )\n",
      "[Epoch: 19800] train loss: 0.2013, train acc: 0.9229, val loss: 0.1375, val acc: 0.9423  (best train acc: 0.9401, best val acc: 0.9619, best train loss: 0.1678  @ epoch 19328 )\n",
      "[Epoch: 19820] train loss: 0.1863, train acc: 0.9341, val loss: 0.1337, val acc: 0.9545  (best train acc: 0.9401, best val acc: 0.9619, best train loss: 0.1678  @ epoch 19328 )\n",
      "[Epoch: 19840] train loss: 0.2024, train acc: 0.9265, val loss: 0.1272, val acc: 0.9511  (best train acc: 0.9401, best val acc: 0.9619, best train loss: 0.1678  @ epoch 19328 )\n",
      "[Epoch: 19860] train loss: 0.1837, train acc: 0.9323, val loss: 0.1238, val acc: 0.9572  (best train acc: 0.9401, best val acc: 0.9619, best train loss: 0.1678  @ epoch 19328 )\n",
      "[Epoch: 19880] train loss: 0.2072, train acc: 0.9151, val loss: 0.1231, val acc: 0.9558  (best train acc: 0.9401, best val acc: 0.9619, best train loss: 0.1678  @ epoch 19328 )\n",
      "[Epoch: 19900] train loss: 0.1914, train acc: 0.9315, val loss: 0.1234, val acc: 0.9558  (best train acc: 0.9401, best val acc: 0.9619, best train loss: 0.1678  @ epoch 19328 )\n",
      "[Epoch: 19920] train loss: 0.2155, train acc: 0.9167, val loss: 0.1218, val acc: 0.9565  (best train acc: 0.9401, best val acc: 0.9619, best train loss: 0.1668  @ epoch 19905 )\n",
      "[Epoch: 19940] train loss: 0.1873, train acc: 0.9279, val loss: 0.1269, val acc: 0.9538  (best train acc: 0.9401, best val acc: 0.9619, best train loss: 0.1668  @ epoch 19905 )\n",
      "[Epoch: 19960] train loss: 0.2278, train acc: 0.9097, val loss: 0.1231, val acc: 0.9541  (best train acc: 0.9401, best val acc: 0.9619, best train loss: 0.1668  @ epoch 19905 )\n",
      "[Epoch: 19980] train loss: 0.1943, train acc: 0.9305, val loss: 0.1367, val acc: 0.9481  (best train acc: 0.9401, best val acc: 0.9619, best train loss: 0.1668  @ epoch 19905 )\n",
      "[Epoch: 20000] train loss: 0.1903, train acc: 0.9248, val loss: 0.1438, val acc: 0.9406  (best train acc: 0.9401, best val acc: 0.9619, best train loss: 0.1668  @ epoch 19905 )\n",
      "[Epoch: 20020] train loss: 0.2116, train acc: 0.9264, val loss: 0.1297, val acc: 0.9562  (best train acc: 0.9401, best val acc: 0.9619, best train loss: 0.1668  @ epoch 19905 )\n",
      "[Epoch: 20040] train loss: 0.1841, train acc: 0.9284, val loss: 0.1238, val acc: 0.9558  (best train acc: 0.9401, best val acc: 0.9619, best train loss: 0.1668  @ epoch 19905 )\n",
      "[Epoch: 20060] train loss: 0.1909, train acc: 0.9295, val loss: 0.1307, val acc: 0.9558  (best train acc: 0.9401, best val acc: 0.9619, best train loss: 0.1668  @ epoch 19905 )\n",
      "[Epoch: 20080] train loss: 0.2162, train acc: 0.9122, val loss: 0.1290, val acc: 0.9572  (best train acc: 0.9401, best val acc: 0.9619, best train loss: 0.1668  @ epoch 19905 )\n",
      "[Epoch: 20100] train loss: 0.1801, train acc: 0.9387, val loss: 0.1278, val acc: 0.9524  (best train acc: 0.9401, best val acc: 0.9619, best train loss: 0.1668  @ epoch 19905 )\n",
      "[Epoch: 20120] train loss: 0.1931, train acc: 0.9269, val loss: 0.1306, val acc: 0.9481  (best train acc: 0.9401, best val acc: 0.9619, best train loss: 0.1668  @ epoch 19905 )\n",
      "[Epoch: 20140] train loss: 0.2015, train acc: 0.9280, val loss: 0.1242, val acc: 0.9582  (best train acc: 0.9401, best val acc: 0.9619, best train loss: 0.1668  @ epoch 19905 )\n",
      "[Epoch: 20160] train loss: 0.1960, train acc: 0.9286, val loss: 0.1244, val acc: 0.9555  (best train acc: 0.9401, best val acc: 0.9619, best train loss: 0.1668  @ epoch 19905 )\n",
      "[Epoch: 20180] train loss: 0.1958, train acc: 0.9273, val loss: 0.1350, val acc: 0.9474  (best train acc: 0.9401, best val acc: 0.9619, best train loss: 0.1668  @ epoch 19905 )\n",
      "[Epoch: 20200] train loss: 0.2326, train acc: 0.9136, val loss: 0.1256, val acc: 0.9562  (best train acc: 0.9401, best val acc: 0.9619, best train loss: 0.1668  @ epoch 19905 )\n",
      "[Epoch: 20220] train loss: 0.1808, train acc: 0.9335, val loss: 0.1278, val acc: 0.9524  (best train acc: 0.9401, best val acc: 0.9619, best train loss: 0.1668  @ epoch 19905 )\n",
      "[Epoch: 20240] train loss: 0.1903, train acc: 0.9284, val loss: 0.1265, val acc: 0.9545  (best train acc: 0.9401, best val acc: 0.9619, best train loss: 0.1668  @ epoch 19905 )\n",
      "[Epoch: 20260] train loss: 0.2147, train acc: 0.9169, val loss: 0.1230, val acc: 0.9555  (best train acc: 0.9401, best val acc: 0.9619, best train loss: 0.1668  @ epoch 19905 )\n",
      "[Epoch: 20280] train loss: 0.1978, train acc: 0.9289, val loss: 0.1263, val acc: 0.9562  (best train acc: 0.9401, best val acc: 0.9619, best train loss: 0.1668  @ epoch 19905 )\n",
      "[Epoch: 20300] train loss: 0.1886, train acc: 0.9291, val loss: 0.1250, val acc: 0.9524  (best train acc: 0.9401, best val acc: 0.9619, best train loss: 0.1668  @ epoch 19905 )\n",
      "[Epoch: 20320] train loss: 0.1921, train acc: 0.9307, val loss: 0.1407, val acc: 0.9410  (best train acc: 0.9401, best val acc: 0.9619, best train loss: 0.1668  @ epoch 19905 )\n",
      "[Epoch: 20340] train loss: 0.2028, train acc: 0.9227, val loss: 0.1388, val acc: 0.9491  (best train acc: 0.9401, best val acc: 0.9619, best train loss: 0.1668  @ epoch 19905 )\n",
      "[Epoch: 20360] train loss: 0.1934, train acc: 0.9257, val loss: 0.1213, val acc: 0.9541  (best train acc: 0.9401, best val acc: 0.9619, best train loss: 0.1668  @ epoch 19905 )\n",
      "[Epoch: 20380] train loss: 0.1960, train acc: 0.9287, val loss: 0.1232, val acc: 0.9562  (best train acc: 0.9401, best val acc: 0.9619, best train loss: 0.1668  @ epoch 19905 )\n",
      "[Epoch: 20400] train loss: 0.2038, train acc: 0.9184, val loss: 0.1195, val acc: 0.9572  (best train acc: 0.9401, best val acc: 0.9619, best train loss: 0.1668  @ epoch 19905 )\n",
      "[Epoch: 20420] train loss: 0.2020, train acc: 0.9202, val loss: 0.1272, val acc: 0.9541  (best train acc: 0.9401, best val acc: 0.9619, best train loss: 0.1668  @ epoch 19905 )\n",
      "[Epoch: 20440] train loss: 0.1983, train acc: 0.9224, val loss: 0.1232, val acc: 0.9531  (best train acc: 0.9401, best val acc: 0.9619, best train loss: 0.1668  @ epoch 19905 )\n",
      "[Epoch: 20460] train loss: 0.1797, train acc: 0.9307, val loss: 0.1209, val acc: 0.9578  (best train acc: 0.9401, best val acc: 0.9619, best train loss: 0.1668  @ epoch 19905 )\n",
      "[Epoch: 20480] train loss: 0.2113, train acc: 0.9174, val loss: 0.1194, val acc: 0.9572  (best train acc: 0.9401, best val acc: 0.9619, best train loss: 0.1668  @ epoch 19905 )\n",
      "[Epoch: 20500] train loss: 0.1897, train acc: 0.9332, val loss: 0.1288, val acc: 0.9524  (best train acc: 0.9401, best val acc: 0.9619, best train loss: 0.1668  @ epoch 19905 )\n",
      "[Epoch: 20520] train loss: 0.1905, train acc: 0.9313, val loss: 0.1238, val acc: 0.9568  (best train acc: 0.9401, best val acc: 0.9619, best train loss: 0.1668  @ epoch 19905 )\n",
      "[Epoch: 20540] train loss: 0.2195, train acc: 0.9146, val loss: 0.1299, val acc: 0.9524  (best train acc: 0.9401, best val acc: 0.9619, best train loss: 0.1668  @ epoch 19905 )\n",
      "[Epoch: 20560] train loss: 0.1994, train acc: 0.9189, val loss: 0.1242, val acc: 0.9568  (best train acc: 0.9401, best val acc: 0.9619, best train loss: 0.1668  @ epoch 19905 )\n",
      "[Epoch: 20580] train loss: 0.2081, train acc: 0.9201, val loss: 0.1255, val acc: 0.9568  (best train acc: 0.9401, best val acc: 0.9619, best train loss: 0.1668  @ epoch 19905 )\n",
      "[Epoch: 20600] train loss: 0.1830, train acc: 0.9342, val loss: 0.1259, val acc: 0.9568  (best train acc: 0.9401, best val acc: 0.9619, best train loss: 0.1668  @ epoch 19905 )\n",
      "[Epoch: 20620] train loss: 0.1708, train acc: 0.9355, val loss: 0.1314, val acc: 0.9491  (best train acc: 0.9401, best val acc: 0.9619, best train loss: 0.1668  @ epoch 19905 )\n",
      "[Epoch: 20640] train loss: 0.2546, train acc: 0.9017, val loss: 0.1292, val acc: 0.9514  (best train acc: 0.9409, best val acc: 0.9619, best train loss: 0.1668  @ epoch 19905 )\n",
      "[Epoch: 20660] train loss: 0.1788, train acc: 0.9338, val loss: 0.1316, val acc: 0.9511  (best train acc: 0.9409, best val acc: 0.9619, best train loss: 0.1668  @ epoch 19905 )\n",
      "[Epoch: 20680] train loss: 0.1914, train acc: 0.9302, val loss: 0.1215, val acc: 0.9575  (best train acc: 0.9409, best val acc: 0.9619, best train loss: 0.1668  @ epoch 19905 )\n",
      "[Epoch: 20700] train loss: 0.1991, train acc: 0.9239, val loss: 0.1237, val acc: 0.9575  (best train acc: 0.9409, best val acc: 0.9619, best train loss: 0.1668  @ epoch 19905 )\n",
      "[Epoch: 20720] train loss: 0.1837, train acc: 0.9322, val loss: 0.1229, val acc: 0.9562  (best train acc: 0.9409, best val acc: 0.9619, best train loss: 0.1668  @ epoch 19905 )\n",
      "[Epoch: 20740] train loss: 0.1792, train acc: 0.9336, val loss: 0.1345, val acc: 0.9504  (best train acc: 0.9409, best val acc: 0.9619, best train loss: 0.1668  @ epoch 19905 )\n",
      "[Epoch: 20760] train loss: 0.1947, train acc: 0.9298, val loss: 0.1233, val acc: 0.9568  (best train acc: 0.9409, best val acc: 0.9619, best train loss: 0.1668  @ epoch 19905 )\n",
      "[Epoch: 20780] train loss: 0.2121, train acc: 0.9158, val loss: 0.1253, val acc: 0.9521  (best train acc: 0.9409, best val acc: 0.9619, best train loss: 0.1668  @ epoch 19905 )\n",
      "[Epoch: 20800] train loss: 0.2441, train acc: 0.9072, val loss: 0.1240, val acc: 0.9565  (best train acc: 0.9409, best val acc: 0.9619, best train loss: 0.1668  @ epoch 19905 )\n",
      "[Epoch: 20820] train loss: 0.1753, train acc: 0.9347, val loss: 0.1253, val acc: 0.9555  (best train acc: 0.9409, best val acc: 0.9619, best train loss: 0.1668  @ epoch 19905 )\n",
      "[Epoch: 20840] train loss: 0.2384, train acc: 0.9075, val loss: 0.1273, val acc: 0.9582  (best train acc: 0.9409, best val acc: 0.9619, best train loss: 0.1668  @ epoch 19905 )\n",
      "[Epoch: 20860] train loss: 0.2304, train acc: 0.9108, val loss: 0.1483, val acc: 0.9440  (best train acc: 0.9409, best val acc: 0.9619, best train loss: 0.1668  @ epoch 19905 )\n",
      "[Epoch: 20880] train loss: 0.1953, train acc: 0.9198, val loss: 0.1303, val acc: 0.9531  (best train acc: 0.9409, best val acc: 0.9619, best train loss: 0.1668  @ epoch 19905 )\n",
      "[Epoch: 20900] train loss: 0.1890, train acc: 0.9297, val loss: 0.1226, val acc: 0.9558  (best train acc: 0.9409, best val acc: 0.9619, best train loss: 0.1668  @ epoch 19905 )\n",
      "[Epoch: 20920] train loss: 0.1868, train acc: 0.9285, val loss: 0.1273, val acc: 0.9524  (best train acc: 0.9409, best val acc: 0.9619, best train loss: 0.1668  @ epoch 19905 )\n",
      "[Epoch: 20940] train loss: 0.1856, train acc: 0.9347, val loss: 0.1281, val acc: 0.9568  (best train acc: 0.9409, best val acc: 0.9619, best train loss: 0.1668  @ epoch 19905 )\n",
      "[Epoch: 20960] train loss: 0.1768, train acc: 0.9310, val loss: 0.1245, val acc: 0.9568  (best train acc: 0.9409, best val acc: 0.9619, best train loss: 0.1659  @ epoch 20959 )\n",
      "[Epoch: 20980] train loss: 0.1842, train acc: 0.9286, val loss: 0.1260, val acc: 0.9558  (best train acc: 0.9409, best val acc: 0.9619, best train loss: 0.1643  @ epoch 20970 )\n",
      "[Epoch: 21000] train loss: 0.2025, train acc: 0.9278, val loss: 0.1269, val acc: 0.9531  (best train acc: 0.9409, best val acc: 0.9619, best train loss: 0.1643  @ epoch 20970 )\n",
      "[Epoch: 21020] train loss: 0.1740, train acc: 0.9338, val loss: 0.1237, val acc: 0.9562  (best train acc: 0.9409, best val acc: 0.9619, best train loss: 0.1643  @ epoch 20970 )\n",
      "[Epoch: 21040] train loss: 0.1771, train acc: 0.9349, val loss: 0.1226, val acc: 0.9545  (best train acc: 0.9409, best val acc: 0.9619, best train loss: 0.1643  @ epoch 20970 )\n",
      "[Epoch: 21060] train loss: 0.2013, train acc: 0.9237, val loss: 0.1240, val acc: 0.9551  (best train acc: 0.9409, best val acc: 0.9619, best train loss: 0.1643  @ epoch 20970 )\n",
      "[Epoch: 21080] train loss: 0.2089, train acc: 0.9112, val loss: 0.1171, val acc: 0.9568  (best train acc: 0.9409, best val acc: 0.9619, best train loss: 0.1643  @ epoch 20970 )\n",
      "[Epoch: 21100] train loss: 0.1997, train acc: 0.9297, val loss: 0.1204, val acc: 0.9582  (best train acc: 0.9409, best val acc: 0.9619, best train loss: 0.1643  @ epoch 20970 )\n",
      "[Epoch: 21120] train loss: 0.1866, train acc: 0.9300, val loss: 0.1263, val acc: 0.9565  (best train acc: 0.9409, best val acc: 0.9619, best train loss: 0.1643  @ epoch 20970 )\n",
      "[Epoch: 21140] train loss: 0.1760, train acc: 0.9351, val loss: 0.1316, val acc: 0.9501  (best train acc: 0.9409, best val acc: 0.9619, best train loss: 0.1643  @ epoch 20970 )\n",
      "[Epoch: 21160] train loss: 0.1818, train acc: 0.9356, val loss: 0.1259, val acc: 0.9535  (best train acc: 0.9409, best val acc: 0.9619, best train loss: 0.1643  @ epoch 20970 )\n",
      "[Epoch: 21180] train loss: 0.2239, train acc: 0.9181, val loss: 0.1441, val acc: 0.9444  (best train acc: 0.9409, best val acc: 0.9619, best train loss: 0.1643  @ epoch 20970 )\n",
      "[Epoch: 21200] train loss: 0.2068, train acc: 0.9195, val loss: 0.1210, val acc: 0.9589  (best train acc: 0.9409, best val acc: 0.9619, best train loss: 0.1643  @ epoch 20970 )\n",
      "[Epoch: 21220] train loss: 0.1840, train acc: 0.9296, val loss: 0.1198, val acc: 0.9595  (best train acc: 0.9409, best val acc: 0.9619, best train loss: 0.1643  @ epoch 20970 )\n",
      "[Epoch: 21240] train loss: 0.1773, train acc: 0.9348, val loss: 0.1230, val acc: 0.9568  (best train acc: 0.9409, best val acc: 0.9619, best train loss: 0.1643  @ epoch 20970 )\n",
      "[Epoch: 21260] train loss: 0.1744, train acc: 0.9348, val loss: 0.1298, val acc: 0.9497  (best train acc: 0.9409, best val acc: 0.9619, best train loss: 0.1643  @ epoch 20970 )\n",
      "[Epoch: 21280] train loss: 0.2065, train acc: 0.9195, val loss: 0.1320, val acc: 0.9508  (best train acc: 0.9409, best val acc: 0.9619, best train loss: 0.1643  @ epoch 20970 )\n",
      "[Epoch: 21300] train loss: 0.1982, train acc: 0.9205, val loss: 0.1263, val acc: 0.9541  (best train acc: 0.9409, best val acc: 0.9619, best train loss: 0.1643  @ epoch 20970 )\n",
      "[Epoch: 21320] train loss: 0.1925, train acc: 0.9245, val loss: 0.1262, val acc: 0.9538  (best train acc: 0.9409, best val acc: 0.9619, best train loss: 0.1643  @ epoch 20970 )\n",
      "[Epoch: 21340] train loss: 0.1896, train acc: 0.9276, val loss: 0.1324, val acc: 0.9511  (best train acc: 0.9409, best val acc: 0.9619, best train loss: 0.1643  @ epoch 20970 )\n",
      "[Epoch: 21360] train loss: 0.2001, train acc: 0.9232, val loss: 0.1337, val acc: 0.9531  (best train acc: 0.9409, best val acc: 0.9619, best train loss: 0.1643  @ epoch 20970 )\n",
      "[Epoch: 21380] train loss: 0.1744, train acc: 0.9375, val loss: 0.1211, val acc: 0.9599  (best train acc: 0.9409, best val acc: 0.9619, best train loss: 0.1643  @ epoch 20970 )\n",
      "[Epoch: 21400] train loss: 0.2615, train acc: 0.8985, val loss: 0.1271, val acc: 0.9551  (best train acc: 0.9409, best val acc: 0.9619, best train loss: 0.1643  @ epoch 20970 )\n",
      "[Epoch: 21420] train loss: 0.1916, train acc: 0.9300, val loss: 0.1256, val acc: 0.9562  (best train acc: 0.9409, best val acc: 0.9619, best train loss: 0.1643  @ epoch 20970 )\n",
      "[Epoch: 21440] train loss: 0.1715, train acc: 0.9364, val loss: 0.1223, val acc: 0.9555  (best train acc: 0.9409, best val acc: 0.9619, best train loss: 0.1643  @ epoch 20970 )\n",
      "[Epoch: 21460] train loss: 0.2456, train acc: 0.9070, val loss: 0.1527, val acc: 0.9477  (best train acc: 0.9409, best val acc: 0.9619, best train loss: 0.1643  @ epoch 20970 )\n",
      "[Epoch: 21480] train loss: 0.2097, train acc: 0.9208, val loss: 0.1239, val acc: 0.9605  (best train acc: 0.9409, best val acc: 0.9619, best train loss: 0.1643  @ epoch 20970 )\n",
      "[Epoch: 21500] train loss: 0.1848, train acc: 0.9290, val loss: 0.1253, val acc: 0.9535  (best train acc: 0.9409, best val acc: 0.9619, best train loss: 0.1643  @ epoch 20970 )\n",
      "[Epoch: 21520] train loss: 0.2128, train acc: 0.9207, val loss: 0.1382, val acc: 0.9444  (best train acc: 0.9409, best val acc: 0.9619, best train loss: 0.1643  @ epoch 20970 )\n",
      "[Epoch: 21540] train loss: 0.2039, train acc: 0.9205, val loss: 0.1289, val acc: 0.9487  (best train acc: 0.9409, best val acc: 0.9619, best train loss: 0.1643  @ epoch 20970 )\n",
      "[Epoch: 21560] train loss: 0.1961, train acc: 0.9239, val loss: 0.1309, val acc: 0.9541  (best train acc: 0.9409, best val acc: 0.9619, best train loss: 0.1643  @ epoch 20970 )\n",
      "[Epoch: 21580] train loss: 0.2028, train acc: 0.9208, val loss: 0.1312, val acc: 0.9494  (best train acc: 0.9409, best val acc: 0.9619, best train loss: 0.1643  @ epoch 20970 )\n",
      "[Epoch: 21600] train loss: 0.1942, train acc: 0.9306, val loss: 0.1323, val acc: 0.9491  (best train acc: 0.9409, best val acc: 0.9619, best train loss: 0.1643  @ epoch 20970 )\n",
      "[Epoch: 21620] train loss: 0.1918, train acc: 0.9260, val loss: 0.1216, val acc: 0.9565  (best train acc: 0.9409, best val acc: 0.9619, best train loss: 0.1643  @ epoch 20970 )\n",
      "[Epoch: 21640] train loss: 0.1943, train acc: 0.9299, val loss: 0.1249, val acc: 0.9558  (best train acc: 0.9409, best val acc: 0.9619, best train loss: 0.1643  @ epoch 20970 )\n",
      "[Epoch: 21660] train loss: 0.1981, train acc: 0.9177, val loss: 0.1214, val acc: 0.9565  (best train acc: 0.9409, best val acc: 0.9619, best train loss: 0.1643  @ epoch 20970 )\n",
      "[Epoch: 21680] train loss: 0.1933, train acc: 0.9257, val loss: 0.1205, val acc: 0.9592  (best train acc: 0.9409, best val acc: 0.9619, best train loss: 0.1643  @ epoch 20970 )\n",
      "[Epoch: 21700] train loss: 0.2108, train acc: 0.9247, val loss: 0.1224, val acc: 0.9582  (best train acc: 0.9409, best val acc: 0.9619, best train loss: 0.1643  @ epoch 20970 )\n",
      "[Epoch: 21720] train loss: 0.2075, train acc: 0.9265, val loss: 0.1247, val acc: 0.9545  (best train acc: 0.9409, best val acc: 0.9619, best train loss: 0.1643  @ epoch 20970 )\n",
      "[Epoch: 21740] train loss: 0.1846, train acc: 0.9314, val loss: 0.1299, val acc: 0.9565  (best train acc: 0.9409, best val acc: 0.9619, best train loss: 0.1643  @ epoch 20970 )\n",
      "[Epoch: 21760] train loss: 0.1723, train acc: 0.9365, val loss: 0.1168, val acc: 0.9568  (best train acc: 0.9409, best val acc: 0.9619, best train loss: 0.1643  @ epoch 20970 )\n",
      "[Epoch: 21780] train loss: 0.3066, train acc: 0.8890, val loss: 0.1173, val acc: 0.9548  (best train acc: 0.9409, best val acc: 0.9619, best train loss: 0.1643  @ epoch 20970 )\n",
      "[Epoch: 21800] train loss: 0.2186, train acc: 0.9144, val loss: 0.1187, val acc: 0.9558  (best train acc: 0.9409, best val acc: 0.9619, best train loss: 0.1643  @ epoch 20970 )\n",
      "[Epoch: 21820] train loss: 0.1876, train acc: 0.9321, val loss: 0.1187, val acc: 0.9599  (best train acc: 0.9409, best val acc: 0.9619, best train loss: 0.1643  @ epoch 20970 )\n",
      "[Epoch: 21840] train loss: 0.1988, train acc: 0.9266, val loss: 0.1232, val acc: 0.9528  (best train acc: 0.9409, best val acc: 0.9619, best train loss: 0.1643  @ epoch 20970 )\n",
      "[Epoch: 21860] train loss: 0.1899, train acc: 0.9265, val loss: 0.1176, val acc: 0.9592  (best train acc: 0.9409, best val acc: 0.9619, best train loss: 0.1643  @ epoch 20970 )\n",
      "[Epoch: 21880] train loss: 0.1977, train acc: 0.9235, val loss: 0.1242, val acc: 0.9575  (best train acc: 0.9409, best val acc: 0.9619, best train loss: 0.1643  @ epoch 20970 )\n",
      "[Epoch: 21900] train loss: 0.1966, train acc: 0.9203, val loss: 0.1309, val acc: 0.9511  (best train acc: 0.9409, best val acc: 0.9619, best train loss: 0.1643  @ epoch 20970 )\n",
      "[Epoch: 21920] train loss: 0.1720, train acc: 0.9366, val loss: 0.1233, val acc: 0.9551  (best train acc: 0.9409, best val acc: 0.9619, best train loss: 0.1643  @ epoch 20970 )\n",
      "[Epoch: 21940] train loss: 0.1989, train acc: 0.9278, val loss: 0.1225, val acc: 0.9535  (best train acc: 0.9409, best val acc: 0.9619, best train loss: 0.1643  @ epoch 20970 )\n",
      "[Epoch: 21960] train loss: 0.1881, train acc: 0.9312, val loss: 0.1183, val acc: 0.9578  (best train acc: 0.9419, best val acc: 0.9619, best train loss: 0.1643  @ epoch 20970 )\n",
      "[Epoch: 21980] train loss: 0.1944, train acc: 0.9271, val loss: 0.1255, val acc: 0.9545  (best train acc: 0.9419, best val acc: 0.9619, best train loss: 0.1643  @ epoch 20970 )\n",
      "[Epoch: 22000] train loss: 0.1988, train acc: 0.9229, val loss: 0.1205, val acc: 0.9562  (best train acc: 0.9419, best val acc: 0.9619, best train loss: 0.1643  @ epoch 20970 )\n",
      "[Epoch: 22020] train loss: 0.2010, train acc: 0.9229, val loss: 0.1324, val acc: 0.9504  (best train acc: 0.9419, best val acc: 0.9619, best train loss: 0.1643  @ epoch 20970 )\n",
      "[Epoch: 22040] train loss: 0.1842, train acc: 0.9349, val loss: 0.1334, val acc: 0.9474  (best train acc: 0.9419, best val acc: 0.9619, best train loss: 0.1643  @ epoch 20970 )\n",
      "[Epoch: 22060] train loss: 0.1672, train acc: 0.9393, val loss: 0.1227, val acc: 0.9551  (best train acc: 0.9419, best val acc: 0.9619, best train loss: 0.1643  @ epoch 20970 )\n",
      "[Epoch: 22080] train loss: 0.1725, train acc: 0.9370, val loss: 0.1233, val acc: 0.9541  (best train acc: 0.9419, best val acc: 0.9619, best train loss: 0.1643  @ epoch 20970 )\n",
      "[Epoch: 22100] train loss: 0.1994, train acc: 0.9254, val loss: 0.1481, val acc: 0.9406  (best train acc: 0.9419, best val acc: 0.9619, best train loss: 0.1643  @ epoch 20970 )\n",
      "[Epoch: 22120] train loss: 0.2188, train acc: 0.9114, val loss: 0.1258, val acc: 0.9541  (best train acc: 0.9419, best val acc: 0.9619, best train loss: 0.1643  @ epoch 20970 )\n",
      "[Epoch: 22140] train loss: 0.2177, train acc: 0.9203, val loss: 0.1274, val acc: 0.9535  (best train acc: 0.9419, best val acc: 0.9619, best train loss: 0.1643  @ epoch 20970 )\n",
      "[Epoch: 22160] train loss: 0.1875, train acc: 0.9252, val loss: 0.1293, val acc: 0.9521  (best train acc: 0.9419, best val acc: 0.9619, best train loss: 0.1643  @ epoch 20970 )\n",
      "[Epoch: 22180] train loss: 0.1781, train acc: 0.9347, val loss: 0.1285, val acc: 0.9548  (best train acc: 0.9419, best val acc: 0.9619, best train loss: 0.1643  @ epoch 20970 )\n",
      "[Epoch: 22200] train loss: 0.1876, train acc: 0.9265, val loss: 0.1370, val acc: 0.9444  (best train acc: 0.9419, best val acc: 0.9619, best train loss: 0.1643  @ epoch 20970 )\n",
      "[Epoch: 22220] train loss: 0.1915, train acc: 0.9294, val loss: 0.1205, val acc: 0.9518  (best train acc: 0.9419, best val acc: 0.9619, best train loss: 0.1643  @ epoch 20970 )\n",
      "[Epoch: 22240] train loss: 0.1998, train acc: 0.9266, val loss: 0.1222, val acc: 0.9572  (best train acc: 0.9419, best val acc: 0.9619, best train loss: 0.1643  @ epoch 20970 )\n",
      "[Epoch: 22260] train loss: 0.2029, train acc: 0.9263, val loss: 0.1241, val acc: 0.9565  (best train acc: 0.9419, best val acc: 0.9619, best train loss: 0.1643  @ epoch 20970 )\n",
      "[Epoch: 22280] train loss: 0.1905, train acc: 0.9329, val loss: 0.1239, val acc: 0.9551  (best train acc: 0.9419, best val acc: 0.9619, best train loss: 0.1643  @ epoch 20970 )\n",
      "[Epoch: 22300] train loss: 0.2340, train acc: 0.9125, val loss: 0.1199, val acc: 0.9578  (best train acc: 0.9419, best val acc: 0.9619, best train loss: 0.1643  @ epoch 20970 )\n",
      "[Epoch: 22320] train loss: 0.1910, train acc: 0.9249, val loss: 0.1184, val acc: 0.9578  (best train acc: 0.9419, best val acc: 0.9619, best train loss: 0.1643  @ epoch 20970 )\n",
      "[Epoch: 22340] train loss: 0.1976, train acc: 0.9242, val loss: 0.1205, val acc: 0.9531  (best train acc: 0.9419, best val acc: 0.9619, best train loss: 0.1643  @ epoch 20970 )\n",
      "[Epoch: 22360] train loss: 0.1966, train acc: 0.9224, val loss: 0.1231, val acc: 0.9538  (best train acc: 0.9419, best val acc: 0.9619, best train loss: 0.1643  @ epoch 20970 )\n",
      "[Epoch: 22380] train loss: 0.1825, train acc: 0.9290, val loss: 0.1304, val acc: 0.9521  (best train acc: 0.9419, best val acc: 0.9619, best train loss: 0.1643  @ epoch 20970 )\n",
      "[Epoch: 22400] train loss: 0.1969, train acc: 0.9248, val loss: 0.1377, val acc: 0.9450  (best train acc: 0.9419, best val acc: 0.9619, best train loss: 0.1643  @ epoch 20970 )\n",
      "[Epoch: 22420] train loss: 0.1937, train acc: 0.9298, val loss: 0.1273, val acc: 0.9538  (best train acc: 0.9419, best val acc: 0.9619, best train loss: 0.1643  @ epoch 20970 )\n",
      "[Epoch: 22440] train loss: 0.1839, train acc: 0.9252, val loss: 0.1226, val acc: 0.9578  (best train acc: 0.9419, best val acc: 0.9619, best train loss: 0.1643  @ epoch 20970 )\n",
      "[Epoch: 22460] train loss: 0.1781, train acc: 0.9359, val loss: 0.1195, val acc: 0.9578  (best train acc: 0.9419, best val acc: 0.9619, best train loss: 0.1643  @ epoch 20970 )\n",
      "[Epoch: 22480] train loss: 0.1831, train acc: 0.9323, val loss: 0.1192, val acc: 0.9605  (best train acc: 0.9419, best val acc: 0.9619, best train loss: 0.1643  @ epoch 20970 )\n",
      "[Epoch: 22500] train loss: 0.1854, train acc: 0.9325, val loss: 0.1206, val acc: 0.9578  (best train acc: 0.9419, best val acc: 0.9619, best train loss: 0.1643  @ epoch 20970 )\n",
      "[Epoch: 22520] train loss: 0.1835, train acc: 0.9271, val loss: 0.1360, val acc: 0.9427  (best train acc: 0.9419, best val acc: 0.9619, best train loss: 0.1643  @ epoch 20970 )\n",
      "[Epoch: 22540] train loss: 0.2219, train acc: 0.9071, val loss: 0.1204, val acc: 0.9568  (best train acc: 0.9419, best val acc: 0.9619, best train loss: 0.1643  @ epoch 20970 )\n",
      "[Epoch: 22560] train loss: 0.2054, train acc: 0.9213, val loss: 0.1227, val acc: 0.9558  (best train acc: 0.9419, best val acc: 0.9619, best train loss: 0.1643  @ epoch 20970 )\n",
      "[Epoch: 22580] train loss: 0.2073, train acc: 0.9195, val loss: 0.1207, val acc: 0.9572  (best train acc: 0.9445, best val acc: 0.9619, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 22600] train loss: 0.2002, train acc: 0.9193, val loss: 0.1202, val acc: 0.9558  (best train acc: 0.9445, best val acc: 0.9619, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 22620] train loss: 0.1818, train acc: 0.9312, val loss: 0.1343, val acc: 0.9494  (best train acc: 0.9445, best val acc: 0.9619, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 22640] train loss: 0.1880, train acc: 0.9352, val loss: 0.1152, val acc: 0.9582  (best train acc: 0.9445, best val acc: 0.9619, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 22660] train loss: 0.1853, train acc: 0.9297, val loss: 0.1396, val acc: 0.9481  (best train acc: 0.9445, best val acc: 0.9619, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 22680] train loss: 0.1809, train acc: 0.9333, val loss: 0.1257, val acc: 0.9572  (best train acc: 0.9445, best val acc: 0.9619, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 22700] train loss: 0.1813, train acc: 0.9331, val loss: 0.1233, val acc: 0.9572  (best train acc: 0.9445, best val acc: 0.9619, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 22720] train loss: 0.1853, train acc: 0.9308, val loss: 0.1240, val acc: 0.9575  (best train acc: 0.9445, best val acc: 0.9619, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 22740] train loss: 0.1841, train acc: 0.9282, val loss: 0.1186, val acc: 0.9572  (best train acc: 0.9445, best val acc: 0.9619, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 22760] train loss: 0.1993, train acc: 0.9272, val loss: 0.1314, val acc: 0.9518  (best train acc: 0.9445, best val acc: 0.9619, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 22780] train loss: 0.1825, train acc: 0.9320, val loss: 0.1228, val acc: 0.9528  (best train acc: 0.9445, best val acc: 0.9619, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 22800] train loss: 0.1933, train acc: 0.9287, val loss: 0.1200, val acc: 0.9585  (best train acc: 0.9445, best val acc: 0.9619, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 22820] train loss: 0.1754, train acc: 0.9367, val loss: 0.1261, val acc: 0.9497  (best train acc: 0.9445, best val acc: 0.9619, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 22840] train loss: 0.1886, train acc: 0.9277, val loss: 0.1215, val acc: 0.9558  (best train acc: 0.9445, best val acc: 0.9619, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 22860] train loss: 0.1979, train acc: 0.9318, val loss: 0.1273, val acc: 0.9535  (best train acc: 0.9445, best val acc: 0.9619, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 22880] train loss: 0.1886, train acc: 0.9317, val loss: 0.1280, val acc: 0.9535  (best train acc: 0.9445, best val acc: 0.9619, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 22900] train loss: 0.1848, train acc: 0.9308, val loss: 0.1171, val acc: 0.9589  (best train acc: 0.9445, best val acc: 0.9619, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 22920] train loss: 0.1880, train acc: 0.9277, val loss: 0.1311, val acc: 0.9558  (best train acc: 0.9445, best val acc: 0.9619, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 22940] train loss: 0.1980, train acc: 0.9252, val loss: 0.1300, val acc: 0.9521  (best train acc: 0.9445, best val acc: 0.9619, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 22960] train loss: 0.2762, train acc: 0.8847, val loss: 0.1274, val acc: 0.9551  (best train acc: 0.9445, best val acc: 0.9619, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 22980] train loss: 0.1922, train acc: 0.9266, val loss: 0.1247, val acc: 0.9551  (best train acc: 0.9445, best val acc: 0.9619, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 23000] train loss: 0.1892, train acc: 0.9293, val loss: 0.1314, val acc: 0.9494  (best train acc: 0.9445, best val acc: 0.9619, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 23020] train loss: 0.1755, train acc: 0.9359, val loss: 0.1311, val acc: 0.9481  (best train acc: 0.9445, best val acc: 0.9619, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 23040] train loss: 0.1900, train acc: 0.9261, val loss: 0.1190, val acc: 0.9592  (best train acc: 0.9445, best val acc: 0.9619, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 23060] train loss: 0.2279, train acc: 0.9123, val loss: 0.1216, val acc: 0.9541  (best train acc: 0.9445, best val acc: 0.9619, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 23080] train loss: 0.2134, train acc: 0.9155, val loss: 0.1462, val acc: 0.9420  (best train acc: 0.9445, best val acc: 0.9619, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 23100] train loss: 0.1773, train acc: 0.9303, val loss: 0.1215, val acc: 0.9548  (best train acc: 0.9445, best val acc: 0.9619, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 23120] train loss: 0.1933, train acc: 0.9235, val loss: 0.1217, val acc: 0.9568  (best train acc: 0.9445, best val acc: 0.9619, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 23140] train loss: 0.2261, train acc: 0.9152, val loss: 0.1219, val acc: 0.9562  (best train acc: 0.9445, best val acc: 0.9619, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 23160] train loss: 0.1806, train acc: 0.9298, val loss: 0.1278, val acc: 0.9545  (best train acc: 0.9445, best val acc: 0.9619, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 23180] train loss: 0.1875, train acc: 0.9299, val loss: 0.1381, val acc: 0.9396  (best train acc: 0.9445, best val acc: 0.9619, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 23200] train loss: 0.1760, train acc: 0.9334, val loss: 0.1307, val acc: 0.9545  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 23220] train loss: 0.1864, train acc: 0.9267, val loss: 0.1287, val acc: 0.9578  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 23240] train loss: 0.3091, train acc: 0.8957, val loss: 0.1174, val acc: 0.9568  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 23260] train loss: 0.1734, train acc: 0.9323, val loss: 0.1248, val acc: 0.9572  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 23280] train loss: 0.1804, train acc: 0.9350, val loss: 0.1250, val acc: 0.9548  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 23300] train loss: 0.2005, train acc: 0.9234, val loss: 0.1178, val acc: 0.9585  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 23320] train loss: 0.2319, train acc: 0.9056, val loss: 0.1222, val acc: 0.9535  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 23340] train loss: 0.1862, train acc: 0.9289, val loss: 0.1166, val acc: 0.9582  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 23360] train loss: 0.1706, train acc: 0.9371, val loss: 0.1208, val acc: 0.9575  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 23380] train loss: 0.2231, train acc: 0.9132, val loss: 0.1346, val acc: 0.9504  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 23400] train loss: 0.1776, train acc: 0.9318, val loss: 0.1240, val acc: 0.9548  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 23420] train loss: 0.2051, train acc: 0.9198, val loss: 0.1222, val acc: 0.9568  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 23440] train loss: 0.1932, train acc: 0.9238, val loss: 0.1268, val acc: 0.9514  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 23460] train loss: 0.1842, train acc: 0.9329, val loss: 0.1190, val acc: 0.9565  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 23480] train loss: 0.1879, train acc: 0.9250, val loss: 0.1260, val acc: 0.9521  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 23500] train loss: 0.1895, train acc: 0.9274, val loss: 0.1199, val acc: 0.9558  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 23520] train loss: 0.1979, train acc: 0.9219, val loss: 0.1200, val acc: 0.9609  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 23540] train loss: 0.1709, train acc: 0.9354, val loss: 0.1236, val acc: 0.9521  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 23560] train loss: 0.2684, train acc: 0.8897, val loss: 0.1287, val acc: 0.9541  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 23580] train loss: 0.2071, train acc: 0.9226, val loss: 0.1267, val acc: 0.9508  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 23600] train loss: 0.2224, train acc: 0.9174, val loss: 0.1278, val acc: 0.9531  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 23620] train loss: 0.1866, train acc: 0.9305, val loss: 0.1173, val acc: 0.9589  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 23640] train loss: 0.1856, train acc: 0.9358, val loss: 0.1205, val acc: 0.9562  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 23660] train loss: 0.1747, train acc: 0.9300, val loss: 0.1316, val acc: 0.9484  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 23680] train loss: 0.3445, train acc: 0.8757, val loss: 0.1306, val acc: 0.9551  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 23700] train loss: 0.1920, train acc: 0.9273, val loss: 0.1298, val acc: 0.9521  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 23720] train loss: 0.2265, train acc: 0.9160, val loss: 0.1289, val acc: 0.9521  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 23740] train loss: 0.1952, train acc: 0.9231, val loss: 0.1205, val acc: 0.9562  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 23760] train loss: 0.1964, train acc: 0.9287, val loss: 0.1269, val acc: 0.9565  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 23780] train loss: 0.1951, train acc: 0.9268, val loss: 0.1246, val acc: 0.9545  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 23800] train loss: 0.2608, train acc: 0.8952, val loss: 0.1154, val acc: 0.9589  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 23820] train loss: 0.1821, train acc: 0.9365, val loss: 0.1248, val acc: 0.9572  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 23840] train loss: 0.1697, train acc: 0.9377, val loss: 0.1217, val acc: 0.9562  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 23860] train loss: 0.1854, train acc: 0.9336, val loss: 0.1230, val acc: 0.9578  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 23880] train loss: 0.1788, train acc: 0.9331, val loss: 0.1252, val acc: 0.9565  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 23900] train loss: 0.1972, train acc: 0.9255, val loss: 0.1339, val acc: 0.9487  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 23920] train loss: 0.2181, train acc: 0.9132, val loss: 0.1285, val acc: 0.9535  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 23940] train loss: 0.1668, train acc: 0.9389, val loss: 0.1227, val acc: 0.9541  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 23960] train loss: 0.1783, train acc: 0.9347, val loss: 0.1392, val acc: 0.9433  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 23980] train loss: 0.2039, train acc: 0.9230, val loss: 0.1268, val acc: 0.9497  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 24000] train loss: 0.1786, train acc: 0.9284, val loss: 0.1267, val acc: 0.9538  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 24020] train loss: 0.1946, train acc: 0.9197, val loss: 0.1225, val acc: 0.9572  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 24040] train loss: 0.2742, train acc: 0.8904, val loss: 0.1327, val acc: 0.9501  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 24060] train loss: 0.2066, train acc: 0.9242, val loss: 0.1308, val acc: 0.9464  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 24080] train loss: 0.1862, train acc: 0.9237, val loss: 0.1530, val acc: 0.9363  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 24100] train loss: 0.1833, train acc: 0.9286, val loss: 0.1313, val acc: 0.9491  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 24120] train loss: 0.1910, train acc: 0.9252, val loss: 0.1356, val acc: 0.9474  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 24140] train loss: 0.1745, train acc: 0.9299, val loss: 0.1199, val acc: 0.9575  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 24160] train loss: 0.1829, train acc: 0.9336, val loss: 0.1257, val acc: 0.9521  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 24180] train loss: 0.1822, train acc: 0.9308, val loss: 0.1306, val acc: 0.9508  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 24200] train loss: 0.2023, train acc: 0.9114, val loss: 0.1227, val acc: 0.9548  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 24220] train loss: 0.1872, train acc: 0.9297, val loss: 0.1175, val acc: 0.9599  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 24240] train loss: 0.1970, train acc: 0.9273, val loss: 0.1221, val acc: 0.9609  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 24260] train loss: 0.1830, train acc: 0.9319, val loss: 0.1269, val acc: 0.9558  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 24280] train loss: 0.1928, train acc: 0.9244, val loss: 0.1173, val acc: 0.9585  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 24300] train loss: 0.2117, train acc: 0.9200, val loss: 0.1195, val acc: 0.9562  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 24320] train loss: 0.2080, train acc: 0.9189, val loss: 0.1201, val acc: 0.9565  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 24340] train loss: 0.1858, train acc: 0.9352, val loss: 0.1208, val acc: 0.9602  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 24360] train loss: 0.2100, train acc: 0.9200, val loss: 0.1249, val acc: 0.9555  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 24380] train loss: 0.1788, train acc: 0.9358, val loss: 0.1231, val acc: 0.9531  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 24400] train loss: 0.1788, train acc: 0.9291, val loss: 0.1203, val acc: 0.9578  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 24420] train loss: 0.2059, train acc: 0.9228, val loss: 0.1414, val acc: 0.9481  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 24440] train loss: 0.1725, train acc: 0.9386, val loss: 0.1318, val acc: 0.9508  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 24460] train loss: 0.2177, train acc: 0.9239, val loss: 0.1278, val acc: 0.9562  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 24480] train loss: 0.2009, train acc: 0.9228, val loss: 0.1353, val acc: 0.9501  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 24500] train loss: 0.1736, train acc: 0.9376, val loss: 0.1286, val acc: 0.9548  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 24520] train loss: 0.1851, train acc: 0.9268, val loss: 0.1342, val acc: 0.9528  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 24540] train loss: 0.1960, train acc: 0.9269, val loss: 0.1264, val acc: 0.9524  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 24560] train loss: 0.3309, train acc: 0.8791, val loss: 0.1255, val acc: 0.9521  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 24580] train loss: 0.1770, train acc: 0.9359, val loss: 0.1258, val acc: 0.9538  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 24600] train loss: 0.2437, train acc: 0.9124, val loss: 0.1239, val acc: 0.9541  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 24620] train loss: 0.2015, train acc: 0.9233, val loss: 0.1220, val acc: 0.9568  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 24640] train loss: 0.1684, train acc: 0.9373, val loss: 0.1245, val acc: 0.9562  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 24660] train loss: 0.1746, train acc: 0.9331, val loss: 0.1183, val acc: 0.9595  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 24680] train loss: 0.1869, train acc: 0.9288, val loss: 0.1250, val acc: 0.9551  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 24700] train loss: 0.1951, train acc: 0.9274, val loss: 0.1393, val acc: 0.9491  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 24720] train loss: 0.1818, train acc: 0.9311, val loss: 0.1308, val acc: 0.9508  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 24740] train loss: 0.1973, train acc: 0.9264, val loss: 0.1198, val acc: 0.9585  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 24760] train loss: 0.2159, train acc: 0.9119, val loss: 0.1236, val acc: 0.9565  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 24780] train loss: 0.3255, train acc: 0.8858, val loss: 0.1301, val acc: 0.9551  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 24800] train loss: 0.1858, train acc: 0.9325, val loss: 0.1214, val acc: 0.9582  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 24820] train loss: 0.1853, train acc: 0.9324, val loss: 0.1333, val acc: 0.9538  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 24840] train loss: 0.1848, train acc: 0.9294, val loss: 0.1333, val acc: 0.9535  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 24860] train loss: 0.1909, train acc: 0.9250, val loss: 0.1279, val acc: 0.9572  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 24880] train loss: 0.1728, train acc: 0.9385, val loss: 0.1307, val acc: 0.9575  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 24900] train loss: 0.3322, train acc: 0.8781, val loss: 0.1244, val acc: 0.9555  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 24920] train loss: 0.1747, train acc: 0.9307, val loss: 0.1189, val acc: 0.9572  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 24940] train loss: 0.1748, train acc: 0.9336, val loss: 0.1219, val acc: 0.9562  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 24960] train loss: 0.2824, train acc: 0.8856, val loss: 0.1228, val acc: 0.9565  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 24980] train loss: 0.1838, train acc: 0.9313, val loss: 0.1236, val acc: 0.9555  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 25000] train loss: 0.1902, train acc: 0.9364, val loss: 0.1302, val acc: 0.9545  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 25020] train loss: 0.1963, train acc: 0.9239, val loss: 0.1308, val acc: 0.9528  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 25040] train loss: 0.2030, train acc: 0.9249, val loss: 0.1306, val acc: 0.9575  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 25060] train loss: 0.1798, train acc: 0.9351, val loss: 0.1211, val acc: 0.9595  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 25080] train loss: 0.1837, train acc: 0.9297, val loss: 0.1415, val acc: 0.9437  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 25100] train loss: 0.2024, train acc: 0.9220, val loss: 0.1373, val acc: 0.9460  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 25120] train loss: 0.1919, train acc: 0.9302, val loss: 0.1313, val acc: 0.9541  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 25140] train loss: 0.1921, train acc: 0.9297, val loss: 0.1287, val acc: 0.9562  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 25160] train loss: 0.2223, train acc: 0.9107, val loss: 0.1215, val acc: 0.9565  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 25180] train loss: 0.2155, train acc: 0.9123, val loss: 0.1272, val acc: 0.9541  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 25200] train loss: 0.1756, train acc: 0.9393, val loss: 0.1339, val acc: 0.9497  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 25220] train loss: 0.2253, train acc: 0.9065, val loss: 0.1275, val acc: 0.9578  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 25240] train loss: 0.2096, train acc: 0.9202, val loss: 0.1307, val acc: 0.9555  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 25260] train loss: 0.1851, train acc: 0.9288, val loss: 0.1244, val acc: 0.9578  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 25280] train loss: 0.2004, train acc: 0.9246, val loss: 0.1262, val acc: 0.9575  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 25300] train loss: 0.1897, train acc: 0.9312, val loss: 0.1315, val acc: 0.9555  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 25320] train loss: 0.1858, train acc: 0.9234, val loss: 0.1263, val acc: 0.9501  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 25340] train loss: 0.1880, train acc: 0.9265, val loss: 0.1285, val acc: 0.9572  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 25360] train loss: 0.1922, train acc: 0.9303, val loss: 0.1245, val acc: 0.9562  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 25380] train loss: 0.1744, train acc: 0.9365, val loss: 0.1271, val acc: 0.9555  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 25400] train loss: 0.1912, train acc: 0.9273, val loss: 0.1233, val acc: 0.9558  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 25420] train loss: 0.1806, train acc: 0.9338, val loss: 0.1319, val acc: 0.9545  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 25440] train loss: 0.2008, train acc: 0.9271, val loss: 0.1263, val acc: 0.9578  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 25460] train loss: 0.2026, train acc: 0.9210, val loss: 0.1214, val acc: 0.9572  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 25480] train loss: 0.2113, train acc: 0.9199, val loss: 0.1297, val acc: 0.9481  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 25500] train loss: 0.1786, train acc: 0.9285, val loss: 0.1235, val acc: 0.9568  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 25520] train loss: 0.1808, train acc: 0.9363, val loss: 0.1320, val acc: 0.9504  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 25540] train loss: 0.1794, train acc: 0.9291, val loss: 0.1329, val acc: 0.9481  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 25560] train loss: 0.1951, train acc: 0.9267, val loss: 0.1247, val acc: 0.9575  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 25580] train loss: 0.1745, train acc: 0.9399, val loss: 0.1258, val acc: 0.9524  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 25600] train loss: 0.1932, train acc: 0.9279, val loss: 0.1318, val acc: 0.9528  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 25620] train loss: 0.1876, train acc: 0.9323, val loss: 0.1354, val acc: 0.9555  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 25640] train loss: 0.1795, train acc: 0.9278, val loss: 0.1224, val acc: 0.9582  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 25660] train loss: 0.1801, train acc: 0.9356, val loss: 0.1413, val acc: 0.9433  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 25680] train loss: 0.1800, train acc: 0.9306, val loss: 0.1402, val acc: 0.9511  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 25700] train loss: 0.1684, train acc: 0.9393, val loss: 0.1267, val acc: 0.9562  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 25720] train loss: 0.1915, train acc: 0.9241, val loss: 0.1289, val acc: 0.9541  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 25740] train loss: 0.2023, train acc: 0.9209, val loss: 0.1247, val acc: 0.9565  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 25760] train loss: 0.1912, train acc: 0.9268, val loss: 0.1274, val acc: 0.9548  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 25780] train loss: 0.1930, train acc: 0.9265, val loss: 0.1216, val acc: 0.9558  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 25800] train loss: 0.1956, train acc: 0.9229, val loss: 0.1254, val acc: 0.9531  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 25820] train loss: 0.1815, train acc: 0.9313, val loss: 0.1270, val acc: 0.9541  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 25840] train loss: 0.1952, train acc: 0.9232, val loss: 0.1324, val acc: 0.9487  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 25860] train loss: 0.2176, train acc: 0.9198, val loss: 0.1277, val acc: 0.9535  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 25880] train loss: 0.1768, train acc: 0.9362, val loss: 0.1253, val acc: 0.9585  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 25900] train loss: 0.1611, train acc: 0.9402, val loss: 0.1297, val acc: 0.9521  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 25920] train loss: 0.1758, train acc: 0.9343, val loss: 0.1301, val acc: 0.9535  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 25940] train loss: 0.1776, train acc: 0.9333, val loss: 0.1196, val acc: 0.9578  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 25960] train loss: 0.1877, train acc: 0.9300, val loss: 0.1377, val acc: 0.9474  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 25980] train loss: 0.2008, train acc: 0.9245, val loss: 0.1251, val acc: 0.9548  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 26000] train loss: 0.1812, train acc: 0.9332, val loss: 0.1296, val acc: 0.9524  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 26020] train loss: 0.1894, train acc: 0.9294, val loss: 0.1370, val acc: 0.9555  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 26040] train loss: 0.1911, train acc: 0.9252, val loss: 0.1301, val acc: 0.9575  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 26060] train loss: 0.2193, train acc: 0.9068, val loss: 0.1188, val acc: 0.9599  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 26080] train loss: 0.1879, train acc: 0.9312, val loss: 0.1279, val acc: 0.9585  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 26100] train loss: 0.1934, train acc: 0.9238, val loss: 0.1273, val acc: 0.9528  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 26120] train loss: 0.1880, train acc: 0.9315, val loss: 0.1371, val acc: 0.9511  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 26140] train loss: 0.2984, train acc: 0.8926, val loss: 0.1264, val acc: 0.9568  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 26160] train loss: 0.1816, train acc: 0.9298, val loss: 0.1240, val acc: 0.9545  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 26180] train loss: 0.1955, train acc: 0.9277, val loss: 0.1261, val acc: 0.9565  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 26200] train loss: 0.1764, train acc: 0.9395, val loss: 0.1226, val acc: 0.9558  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 26220] train loss: 0.1761, train acc: 0.9338, val loss: 0.1392, val acc: 0.9430  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 26240] train loss: 0.1975, train acc: 0.9276, val loss: 0.1282, val acc: 0.9548  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 26260] train loss: 0.1941, train acc: 0.9291, val loss: 0.1237, val acc: 0.9551  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 26280] train loss: 0.1874, train acc: 0.9288, val loss: 0.1239, val acc: 0.9592  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 26300] train loss: 0.2088, train acc: 0.9165, val loss: 0.1268, val acc: 0.9555  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 26320] train loss: 0.1808, train acc: 0.9320, val loss: 0.1270, val acc: 0.9568  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 26340] train loss: 0.1856, train acc: 0.9283, val loss: 0.1272, val acc: 0.9551  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 26360] train loss: 0.1745, train acc: 0.9359, val loss: 0.1303, val acc: 0.9538  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 26380] train loss: 0.2101, train acc: 0.9204, val loss: 0.1403, val acc: 0.9541  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 26400] train loss: 0.1906, train acc: 0.9265, val loss: 0.1335, val acc: 0.9538  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 26420] train loss: 0.2099, train acc: 0.9158, val loss: 0.1601, val acc: 0.9329  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 26440] train loss: 0.2025, train acc: 0.9265, val loss: 0.1279, val acc: 0.9562  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 26460] train loss: 0.1835, train acc: 0.9276, val loss: 0.1252, val acc: 0.9551  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 26480] train loss: 0.1899, train acc: 0.9276, val loss: 0.1273, val acc: 0.9531  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 26500] train loss: 0.1770, train acc: 0.9359, val loss: 0.1260, val acc: 0.9548  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 26520] train loss: 0.1727, train acc: 0.9359, val loss: 0.1276, val acc: 0.9535  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 26540] train loss: 0.1778, train acc: 0.9320, val loss: 0.1365, val acc: 0.9481  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 26560] train loss: 0.1781, train acc: 0.9305, val loss: 0.1254, val acc: 0.9541  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 26580] train loss: 0.1895, train acc: 0.9266, val loss: 0.1212, val acc: 0.9541  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 26600] train loss: 0.1833, train acc: 0.9295, val loss: 0.1291, val acc: 0.9558  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 26620] train loss: 0.2067, train acc: 0.9156, val loss: 0.1315, val acc: 0.9501  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 26640] train loss: 0.3010, train acc: 0.8974, val loss: 0.1307, val acc: 0.9531  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 26660] train loss: 0.1894, train acc: 0.9256, val loss: 0.1272, val acc: 0.9568  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 26680] train loss: 0.1769, train acc: 0.9327, val loss: 0.1316, val acc: 0.9518  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 26700] train loss: 0.1867, train acc: 0.9277, val loss: 0.1230, val acc: 0.9578  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 26720] train loss: 0.1895, train acc: 0.9232, val loss: 0.1230, val acc: 0.9568  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 26740] train loss: 0.1733, train acc: 0.9383, val loss: 0.1268, val acc: 0.9541  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 26760] train loss: 0.2067, train acc: 0.9190, val loss: 0.1213, val acc: 0.9592  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 26780] train loss: 0.1853, train acc: 0.9349, val loss: 0.1349, val acc: 0.9528  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 26800] train loss: 0.3161, train acc: 0.8761, val loss: 0.1212, val acc: 0.9562  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 26820] train loss: 0.2214, train acc: 0.9071, val loss: 0.1358, val acc: 0.9444  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 26840] train loss: 0.1923, train acc: 0.9225, val loss: 0.1385, val acc: 0.9487  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 26860] train loss: 0.1813, train acc: 0.9276, val loss: 0.1286, val acc: 0.9551  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 26880] train loss: 0.1956, train acc: 0.9223, val loss: 0.1255, val acc: 0.9551  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 26900] train loss: 0.1796, train acc: 0.9304, val loss: 0.1304, val acc: 0.9511  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 26920] train loss: 0.1968, train acc: 0.9213, val loss: 0.1259, val acc: 0.9565  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 26940] train loss: 0.1872, train acc: 0.9305, val loss: 0.1279, val acc: 0.9541  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 26960] train loss: 0.1715, train acc: 0.9404, val loss: 0.1311, val acc: 0.9497  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 26980] train loss: 0.1825, train acc: 0.9312, val loss: 0.1437, val acc: 0.9417  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 27000] train loss: 0.1649, train acc: 0.9376, val loss: 0.1275, val acc: 0.9555  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 27020] train loss: 0.2009, train acc: 0.9265, val loss: 0.1355, val acc: 0.9528  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 27040] train loss: 0.1885, train acc: 0.9294, val loss: 0.1310, val acc: 0.9521  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 27060] train loss: 0.1725, train acc: 0.9362, val loss: 0.1203, val acc: 0.9562  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 27080] train loss: 0.1698, train acc: 0.9380, val loss: 0.1286, val acc: 0.9538  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 27100] train loss: 0.1906, train acc: 0.9265, val loss: 0.1359, val acc: 0.9491  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 27120] train loss: 0.1863, train acc: 0.9328, val loss: 0.1224, val acc: 0.9555  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 27140] train loss: 0.1849, train acc: 0.9285, val loss: 0.1329, val acc: 0.9528  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 27160] train loss: 0.1717, train acc: 0.9354, val loss: 0.1303, val acc: 0.9558  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 27180] train loss: 0.2900, train acc: 0.9029, val loss: 0.1276, val acc: 0.9582  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 27200] train loss: 0.2145, train acc: 0.9199, val loss: 0.1287, val acc: 0.9504  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 27220] train loss: 0.1706, train acc: 0.9392, val loss: 0.1340, val acc: 0.9497  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 27240] train loss: 0.1724, train acc: 0.9383, val loss: 0.1313, val acc: 0.9470  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 27260] train loss: 0.2029, train acc: 0.9245, val loss: 0.1243, val acc: 0.9572  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 27280] train loss: 0.1616, train acc: 0.9396, val loss: 0.1306, val acc: 0.9501  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 27300] train loss: 0.1995, train acc: 0.9235, val loss: 0.1236, val acc: 0.9572  (best train acc: 0.9445, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 27320] train loss: 0.1927, train acc: 0.9252, val loss: 0.1258, val acc: 0.9551  (best train acc: 0.9451, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 27340] train loss: 0.1779, train acc: 0.9294, val loss: 0.1314, val acc: 0.9558  (best train acc: 0.9451, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 27360] train loss: 0.1993, train acc: 0.9242, val loss: 0.1299, val acc: 0.9504  (best train acc: 0.9451, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 27380] train loss: 0.1758, train acc: 0.9356, val loss: 0.1263, val acc: 0.9585  (best train acc: 0.9451, best val acc: 0.9622, best train loss: 0.1573  @ epoch 22574 )\n",
      "[Epoch: 27400] train loss: 0.1900, train acc: 0.9224, val loss: 0.1273, val acc: 0.9558  (best train acc: 0.9451, best val acc: 0.9622, best train loss: 0.1570  @ epoch 27397 )\n",
      "[Epoch: 27420] train loss: 0.2201, train acc: 0.9099, val loss: 0.1223, val acc: 0.9572  (best train acc: 0.9451, best val acc: 0.9622, best train loss: 0.1570  @ epoch 27397 )\n",
      "[Epoch: 27440] train loss: 0.2407, train acc: 0.9122, val loss: 0.1315, val acc: 0.9528  (best train acc: 0.9451, best val acc: 0.9622, best train loss: 0.1570  @ epoch 27397 )\n",
      "[Epoch: 27460] train loss: 0.2096, train acc: 0.9175, val loss: 0.1309, val acc: 0.9558  (best train acc: 0.9451, best val acc: 0.9622, best train loss: 0.1570  @ epoch 27397 )\n",
      "[Epoch: 27480] train loss: 0.2056, train acc: 0.9157, val loss: 0.1270, val acc: 0.9572  (best train acc: 0.9451, best val acc: 0.9622, best train loss: 0.1570  @ epoch 27397 )\n",
      "[Epoch: 27500] train loss: 0.1985, train acc: 0.9177, val loss: 0.1288, val acc: 0.9521  (best train acc: 0.9451, best val acc: 0.9622, best train loss: 0.1570  @ epoch 27397 )\n",
      "[Epoch: 27520] train loss: 0.1988, train acc: 0.9295, val loss: 0.1301, val acc: 0.9501  (best train acc: 0.9451, best val acc: 0.9622, best train loss: 0.1570  @ epoch 27397 )\n",
      "[Epoch: 27540] train loss: 0.1670, train acc: 0.9380, val loss: 0.1291, val acc: 0.9514  (best train acc: 0.9451, best val acc: 0.9622, best train loss: 0.1570  @ epoch 27397 )\n",
      "[Epoch: 27560] train loss: 0.1744, train acc: 0.9318, val loss: 0.1304, val acc: 0.9555  (best train acc: 0.9451, best val acc: 0.9622, best train loss: 0.1570  @ epoch 27397 )\n",
      "[Epoch: 27580] train loss: 0.1854, train acc: 0.9316, val loss: 0.1232, val acc: 0.9585  (best train acc: 0.9451, best val acc: 0.9622, best train loss: 0.1570  @ epoch 27397 )\n",
      "[Epoch: 27600] train loss: 0.2251, train acc: 0.9038, val loss: 0.1221, val acc: 0.9568  (best train acc: 0.9451, best val acc: 0.9622, best train loss: 0.1570  @ epoch 27397 )\n",
      "[Epoch: 27620] train loss: 0.1839, train acc: 0.9325, val loss: 0.1243, val acc: 0.9521  (best train acc: 0.9451, best val acc: 0.9622, best train loss: 0.1570  @ epoch 27397 )\n",
      "[Epoch: 27640] train loss: 0.1909, train acc: 0.9312, val loss: 0.1371, val acc: 0.9464  (best train acc: 0.9451, best val acc: 0.9622, best train loss: 0.1570  @ epoch 27397 )\n",
      "[Epoch: 27660] train loss: 0.1895, train acc: 0.9322, val loss: 0.1239, val acc: 0.9575  (best train acc: 0.9451, best val acc: 0.9622, best train loss: 0.1570  @ epoch 27397 )\n",
      "[Epoch: 27680] train loss: 0.1781, train acc: 0.9306, val loss: 0.1311, val acc: 0.9501  (best train acc: 0.9451, best val acc: 0.9622, best train loss: 0.1570  @ epoch 27397 )\n",
      "[Epoch: 27700] train loss: 0.2201, train acc: 0.9098, val loss: 0.1295, val acc: 0.9558  (best train acc: 0.9451, best val acc: 0.9622, best train loss: 0.1570  @ epoch 27397 )\n",
      "[Epoch: 27720] train loss: 0.1893, train acc: 0.9272, val loss: 0.1246, val acc: 0.9538  (best train acc: 0.9451, best val acc: 0.9622, best train loss: 0.1570  @ epoch 27397 )\n",
      "[Epoch: 27740] train loss: 0.1702, train acc: 0.9392, val loss: 0.1292, val acc: 0.9504  (best train acc: 0.9451, best val acc: 0.9622, best train loss: 0.1570  @ epoch 27397 )\n",
      "[Epoch: 27760] train loss: 0.1732, train acc: 0.9370, val loss: 0.1319, val acc: 0.9511  (best train acc: 0.9451, best val acc: 0.9622, best train loss: 0.1570  @ epoch 27397 )\n",
      "[Epoch: 27780] train loss: 0.1733, train acc: 0.9325, val loss: 0.1367, val acc: 0.9427  (best train acc: 0.9451, best val acc: 0.9622, best train loss: 0.1570  @ epoch 27397 )\n",
      "[Epoch: 27800] train loss: 0.1806, train acc: 0.9329, val loss: 0.1217, val acc: 0.9572  (best train acc: 0.9451, best val acc: 0.9622, best train loss: 0.1570  @ epoch 27397 )\n",
      "[Epoch: 27820] train loss: 0.1836, train acc: 0.9296, val loss: 0.1281, val acc: 0.9568  (best train acc: 0.9451, best val acc: 0.9622, best train loss: 0.1570  @ epoch 27397 )\n",
      "[Epoch: 27840] train loss: 0.2096, train acc: 0.9231, val loss: 0.1381, val acc: 0.9538  (best train acc: 0.9451, best val acc: 0.9622, best train loss: 0.1570  @ epoch 27397 )\n",
      "[Epoch: 27860] train loss: 0.1764, train acc: 0.9338, val loss: 0.1302, val acc: 0.9518  (best train acc: 0.9451, best val acc: 0.9622, best train loss: 0.1570  @ epoch 27397 )\n",
      "[Epoch: 27880] train loss: 0.2161, train acc: 0.9174, val loss: 0.1368, val acc: 0.9497  (best train acc: 0.9451, best val acc: 0.9622, best train loss: 0.1570  @ epoch 27397 )\n",
      "[Epoch: 27900] train loss: 0.1838, train acc: 0.9316, val loss: 0.1289, val acc: 0.9531  (best train acc: 0.9451, best val acc: 0.9622, best train loss: 0.1570  @ epoch 27397 )\n",
      "[Epoch: 27920] train loss: 0.3234, train acc: 0.8924, val loss: 0.1344, val acc: 0.9531  (best train acc: 0.9451, best val acc: 0.9622, best train loss: 0.1570  @ epoch 27397 )\n",
      "[Epoch: 27940] train loss: 0.1922, train acc: 0.9290, val loss: 0.1204, val acc: 0.9558  (best train acc: 0.9451, best val acc: 0.9622, best train loss: 0.1570  @ epoch 27397 )\n",
      "[Epoch: 27960] train loss: 0.1735, train acc: 0.9317, val loss: 0.1277, val acc: 0.9568  (best train acc: 0.9451, best val acc: 0.9622, best train loss: 0.1570  @ epoch 27397 )\n",
      "[Epoch: 27980] train loss: 0.1992, train acc: 0.9241, val loss: 0.1299, val acc: 0.9582  (best train acc: 0.9451, best val acc: 0.9622, best train loss: 0.1570  @ epoch 27397 )\n",
      "[Epoch: 28000] train loss: 0.2725, train acc: 0.8913, val loss: 0.1280, val acc: 0.9538  (best train acc: 0.9451, best val acc: 0.9622, best train loss: 0.1570  @ epoch 27397 )\n",
      "[Epoch: 28020] train loss: 0.1852, train acc: 0.9314, val loss: 0.1289, val acc: 0.9521  (best train acc: 0.9451, best val acc: 0.9622, best train loss: 0.1570  @ epoch 27397 )\n",
      "[Epoch: 28040] train loss: 0.2005, train acc: 0.9158, val loss: 0.1314, val acc: 0.9541  (best train acc: 0.9451, best val acc: 0.9622, best train loss: 0.1570  @ epoch 27397 )\n",
      "[Epoch: 28060] train loss: 0.1775, train acc: 0.9354, val loss: 0.1309, val acc: 0.9470  (best train acc: 0.9451, best val acc: 0.9622, best train loss: 0.1570  @ epoch 27397 )\n",
      "[Epoch: 28080] train loss: 0.1904, train acc: 0.9283, val loss: 0.1269, val acc: 0.9535  (best train acc: 0.9451, best val acc: 0.9622, best train loss: 0.1570  @ epoch 27397 )\n",
      "[Epoch: 28100] train loss: 0.1662, train acc: 0.9419, val loss: 0.1171, val acc: 0.9595  (best train acc: 0.9451, best val acc: 0.9622, best train loss: 0.1570  @ epoch 27397 )\n",
      "[Epoch: 28120] train loss: 0.1869, train acc: 0.9295, val loss: 0.1268, val acc: 0.9599  (best train acc: 0.9451, best val acc: 0.9622, best train loss: 0.1570  @ epoch 27397 )\n",
      "[Epoch: 28140] train loss: 0.1651, train acc: 0.9401, val loss: 0.1280, val acc: 0.9524  (best train acc: 0.9451, best val acc: 0.9622, best train loss: 0.1570  @ epoch 27397 )\n",
      "[Epoch: 28160] train loss: 0.1921, train acc: 0.9283, val loss: 0.1409, val acc: 0.9504  (best train acc: 0.9451, best val acc: 0.9622, best train loss: 0.1570  @ epoch 27397 )\n",
      "[Epoch: 28180] train loss: 0.1915, train acc: 0.9333, val loss: 0.1416, val acc: 0.9447  (best train acc: 0.9451, best val acc: 0.9622, best train loss: 0.1570  @ epoch 27397 )\n",
      "[Epoch: 28200] train loss: 0.1791, train acc: 0.9331, val loss: 0.1284, val acc: 0.9548  (best train acc: 0.9451, best val acc: 0.9622, best train loss: 0.1570  @ epoch 27397 )\n",
      "[Epoch: 28220] train loss: 0.1852, train acc: 0.9288, val loss: 0.1237, val acc: 0.9562  (best train acc: 0.9451, best val acc: 0.9622, best train loss: 0.1570  @ epoch 27397 )\n",
      "[Epoch: 28240] train loss: 0.1709, train acc: 0.9383, val loss: 0.1246, val acc: 0.9582  (best train acc: 0.9451, best val acc: 0.9622, best train loss: 0.1570  @ epoch 27397 )\n",
      "[Epoch: 28260] train loss: 0.1893, train acc: 0.9259, val loss: 0.1238, val acc: 0.9602  (best train acc: 0.9451, best val acc: 0.9622, best train loss: 0.1570  @ epoch 27397 )\n",
      "[Epoch: 28280] train loss: 0.3124, train acc: 0.8968, val loss: 0.1314, val acc: 0.9504  (best train acc: 0.9451, best val acc: 0.9622, best train loss: 0.1570  @ epoch 27397 )\n",
      "[Epoch: 28300] train loss: 0.1797, train acc: 0.9328, val loss: 0.1242, val acc: 0.9568  (best train acc: 0.9451, best val acc: 0.9622, best train loss: 0.1570  @ epoch 27397 )\n",
      "[Epoch: 28320] train loss: 0.1696, train acc: 0.9385, val loss: 0.1277, val acc: 0.9558  (best train acc: 0.9451, best val acc: 0.9622, best train loss: 0.1570  @ epoch 27397 )\n",
      "[Epoch: 28340] train loss: 0.2021, train acc: 0.9234, val loss: 0.1342, val acc: 0.9491  (best train acc: 0.9451, best val acc: 0.9622, best train loss: 0.1565  @ epoch 28335 )\n",
      "[Epoch: 28360] train loss: 0.2012, train acc: 0.9258, val loss: 0.1419, val acc: 0.9430  (best train acc: 0.9451, best val acc: 0.9622, best train loss: 0.1565  @ epoch 28335 )\n",
      "[Epoch: 28380] train loss: 0.1744, train acc: 0.9294, val loss: 0.1267, val acc: 0.9535  (best train acc: 0.9451, best val acc: 0.9622, best train loss: 0.1565  @ epoch 28335 )\n",
      "[Epoch: 28400] train loss: 0.1735, train acc: 0.9357, val loss: 0.1287, val acc: 0.9541  (best train acc: 0.9451, best val acc: 0.9622, best train loss: 0.1565  @ epoch 28335 )\n",
      "[Epoch: 28420] train loss: 0.1869, train acc: 0.9293, val loss: 0.1229, val acc: 0.9568  (best train acc: 0.9451, best val acc: 0.9622, best train loss: 0.1565  @ epoch 28335 )\n",
      "[Epoch: 28440] train loss: 0.1723, train acc: 0.9372, val loss: 0.1240, val acc: 0.9531  (best train acc: 0.9451, best val acc: 0.9622, best train loss: 0.1565  @ epoch 28335 )\n",
      "[Epoch: 28460] train loss: 0.1895, train acc: 0.9309, val loss: 0.1290, val acc: 0.9501  (best train acc: 0.9451, best val acc: 0.9622, best train loss: 0.1565  @ epoch 28335 )\n",
      "[Epoch: 28480] train loss: 0.2054, train acc: 0.9158, val loss: 0.1280, val acc: 0.9548  (best train acc: 0.9451, best val acc: 0.9622, best train loss: 0.1565  @ epoch 28335 )\n",
      "[Epoch: 28500] train loss: 0.1742, train acc: 0.9392, val loss: 0.1248, val acc: 0.9558  (best train acc: 0.9451, best val acc: 0.9622, best train loss: 0.1565  @ epoch 28335 )\n",
      "[Epoch: 28520] train loss: 0.2069, train acc: 0.9256, val loss: 0.1290, val acc: 0.9524  (best train acc: 0.9451, best val acc: 0.9622, best train loss: 0.1565  @ epoch 28335 )\n",
      "[Epoch: 28540] train loss: 0.1820, train acc: 0.9336, val loss: 0.1239, val acc: 0.9572  (best train acc: 0.9451, best val acc: 0.9622, best train loss: 0.1565  @ epoch 28335 )\n",
      "[Epoch: 28560] train loss: 0.2008, train acc: 0.9208, val loss: 0.1587, val acc: 0.9359  (best train acc: 0.9451, best val acc: 0.9622, best train loss: 0.1565  @ epoch 28335 )\n",
      "[Epoch: 28580] train loss: 0.1910, train acc: 0.9306, val loss: 0.1293, val acc: 0.9568  (best train acc: 0.9451, best val acc: 0.9622, best train loss: 0.1565  @ epoch 28335 )\n",
      "[Epoch: 28600] train loss: 0.1825, train acc: 0.9325, val loss: 0.1249, val acc: 0.9535  (best train acc: 0.9451, best val acc: 0.9622, best train loss: 0.1565  @ epoch 28335 )\n",
      "[Epoch: 28620] train loss: 0.1655, train acc: 0.9375, val loss: 0.1304, val acc: 0.9504  (best train acc: 0.9451, best val acc: 0.9622, best train loss: 0.1565  @ epoch 28335 )\n",
      "[Epoch: 28640] train loss: 0.1941, train acc: 0.9312, val loss: 0.1262, val acc: 0.9555  (best train acc: 0.9451, best val acc: 0.9622, best train loss: 0.1565  @ epoch 28335 )\n",
      "[Epoch: 28660] train loss: 0.1965, train acc: 0.9271, val loss: 0.1269, val acc: 0.9562  (best train acc: 0.9451, best val acc: 0.9622, best train loss: 0.1565  @ epoch 28335 )\n",
      "[Epoch: 28680] train loss: 0.1710, train acc: 0.9369, val loss: 0.1378, val acc: 0.9427  (best train acc: 0.9451, best val acc: 0.9622, best train loss: 0.1565  @ epoch 28335 )\n",
      "[Epoch: 28700] train loss: 0.2027, train acc: 0.9160, val loss: 0.1326, val acc: 0.9521  (best train acc: 0.9451, best val acc: 0.9622, best train loss: 0.1565  @ epoch 28335 )\n",
      "[Epoch: 28720] train loss: 0.1738, train acc: 0.9353, val loss: 0.1292, val acc: 0.9460  (best train acc: 0.9451, best val acc: 0.9622, best train loss: 0.1565  @ epoch 28335 )\n",
      "[Epoch: 28740] train loss: 0.1692, train acc: 0.9380, val loss: 0.1293, val acc: 0.9545  (best train acc: 0.9451, best val acc: 0.9622, best train loss: 0.1565  @ epoch 28335 )\n",
      "[Epoch: 28760] train loss: 0.1768, train acc: 0.9281, val loss: 0.1238, val acc: 0.9562  (best train acc: 0.9451, best val acc: 0.9622, best train loss: 0.1565  @ epoch 28335 )\n",
      "[Epoch: 28780] train loss: 0.1997, train acc: 0.9272, val loss: 0.1276, val acc: 0.9562  (best train acc: 0.9451, best val acc: 0.9622, best train loss: 0.1565  @ epoch 28335 )\n",
      "[Epoch: 28800] train loss: 0.1696, train acc: 0.9362, val loss: 0.1416, val acc: 0.9410  (best train acc: 0.9451, best val acc: 0.9622, best train loss: 0.1565  @ epoch 28335 )\n",
      "[Epoch: 28820] train loss: 0.1829, train acc: 0.9260, val loss: 0.1275, val acc: 0.9497  (best train acc: 0.9451, best val acc: 0.9622, best train loss: 0.1565  @ epoch 28335 )\n",
      "[Epoch: 28840] train loss: 0.1740, train acc: 0.9350, val loss: 0.1266, val acc: 0.9518  (best train acc: 0.9451, best val acc: 0.9622, best train loss: 0.1565  @ epoch 28335 )\n",
      "[Epoch: 28860] train loss: 0.1963, train acc: 0.9229, val loss: 0.1316, val acc: 0.9481  (best train acc: 0.9451, best val acc: 0.9622, best train loss: 0.1565  @ epoch 28335 )\n",
      "[Epoch: 28880] train loss: 0.1866, train acc: 0.9327, val loss: 0.1430, val acc: 0.9403  (best train acc: 0.9451, best val acc: 0.9622, best train loss: 0.1565  @ epoch 28335 )\n",
      "[Epoch: 28900] train loss: 0.1826, train acc: 0.9286, val loss: 0.1344, val acc: 0.9470  (best train acc: 0.9451, best val acc: 0.9622, best train loss: 0.1565  @ epoch 28335 )\n",
      "[Epoch: 28920] train loss: 0.2039, train acc: 0.9218, val loss: 0.1258, val acc: 0.9497  (best train acc: 0.9451, best val acc: 0.9622, best train loss: 0.1565  @ epoch 28335 )\n",
      "[Epoch: 28940] train loss: 0.2085, train acc: 0.9217, val loss: 0.1268, val acc: 0.9531  (best train acc: 0.9451, best val acc: 0.9622, best train loss: 0.1565  @ epoch 28335 )\n",
      "[Epoch: 28960] train loss: 0.1719, train acc: 0.9393, val loss: 0.1311, val acc: 0.9531  (best train acc: 0.9451, best val acc: 0.9622, best train loss: 0.1565  @ epoch 28335 )\n",
      "[Epoch: 28980] train loss: 0.1921, train acc: 0.9271, val loss: 0.1219, val acc: 0.9497  (best train acc: 0.9451, best val acc: 0.9622, best train loss: 0.1565  @ epoch 28335 )\n",
      "[Epoch: 29000] train loss: 0.1751, train acc: 0.9360, val loss: 0.1328, val acc: 0.9504  (best train acc: 0.9451, best val acc: 0.9622, best train loss: 0.1565  @ epoch 28335 )\n",
      "[Epoch: 29020] train loss: 0.1698, train acc: 0.9339, val loss: 0.1296, val acc: 0.9508  (best train acc: 0.9451, best val acc: 0.9622, best train loss: 0.1565  @ epoch 28335 )\n",
      "[Epoch: 29040] train loss: 0.2197, train acc: 0.9129, val loss: 0.1182, val acc: 0.9582  (best train acc: 0.9451, best val acc: 0.9622, best train loss: 0.1565  @ epoch 28335 )\n",
      "[Epoch: 29060] train loss: 0.1979, train acc: 0.9190, val loss: 0.1332, val acc: 0.9491  (best train acc: 0.9451, best val acc: 0.9622, best train loss: 0.1565  @ epoch 28335 )\n",
      "[Epoch: 29080] train loss: 0.1821, train acc: 0.9328, val loss: 0.1314, val acc: 0.9433  (best train acc: 0.9451, best val acc: 0.9622, best train loss: 0.1565  @ epoch 28335 )\n",
      "[Epoch: 29100] train loss: 0.1844, train acc: 0.9325, val loss: 0.1341, val acc: 0.9511  (best train acc: 0.9451, best val acc: 0.9622, best train loss: 0.1565  @ epoch 28335 )\n",
      "[Epoch: 29120] train loss: 0.2146, train acc: 0.9150, val loss: 0.1511, val acc: 0.9342  (best train acc: 0.9451, best val acc: 0.9622, best train loss: 0.1565  @ epoch 28335 )\n",
      "[Epoch: 29140] train loss: 0.1854, train acc: 0.9269, val loss: 0.1298, val acc: 0.9511  (best train acc: 0.9451, best val acc: 0.9622, best train loss: 0.1565  @ epoch 28335 )\n",
      "[Epoch: 29160] train loss: 0.1870, train acc: 0.9268, val loss: 0.1269, val acc: 0.9592  (best train acc: 0.9451, best val acc: 0.9622, best train loss: 0.1565  @ epoch 28335 )\n",
      "[Epoch: 29180] train loss: 0.1773, train acc: 0.9372, val loss: 0.1306, val acc: 0.9551  (best train acc: 0.9451, best val acc: 0.9622, best train loss: 0.1565  @ epoch 28335 )\n",
      "[Epoch: 29200] train loss: 0.1930, train acc: 0.9268, val loss: 0.1292, val acc: 0.9541  (best train acc: 0.9451, best val acc: 0.9622, best train loss: 0.1565  @ epoch 28335 )\n",
      "[Epoch: 29220] train loss: 0.1774, train acc: 0.9373, val loss: 0.1317, val acc: 0.9528  (best train acc: 0.9451, best val acc: 0.9622, best train loss: 0.1565  @ epoch 28335 )\n",
      "[Epoch: 29240] train loss: 0.2170, train acc: 0.9086, val loss: 0.1227, val acc: 0.9568  (best train acc: 0.9451, best val acc: 0.9622, best train loss: 0.1565  @ epoch 28335 )\n",
      "[Epoch: 29260] train loss: 0.2126, train acc: 0.9093, val loss: 0.1335, val acc: 0.9541  (best train acc: 0.9451, best val acc: 0.9622, best train loss: 0.1565  @ epoch 28335 )\n",
      "[Epoch: 29280] train loss: 0.2134, train acc: 0.9182, val loss: 0.1278, val acc: 0.9524  (best train acc: 0.9451, best val acc: 0.9622, best train loss: 0.1565  @ epoch 28335 )\n",
      "[Epoch: 29300] train loss: 0.1924, train acc: 0.9237, val loss: 0.1248, val acc: 0.9548  (best train acc: 0.9451, best val acc: 0.9622, best train loss: 0.1565  @ epoch 28335 )\n",
      "[Epoch: 29320] train loss: 0.1816, train acc: 0.9338, val loss: 0.1241, val acc: 0.9582  (best train acc: 0.9451, best val acc: 0.9622, best train loss: 0.1565  @ epoch 28335 )\n",
      "[Epoch: 29340] train loss: 0.1735, train acc: 0.9362, val loss: 0.1263, val acc: 0.9575  (best train acc: 0.9451, best val acc: 0.9622, best train loss: 0.1565  @ epoch 28335 )\n",
      "[Epoch: 29360] train loss: 0.2005, train acc: 0.9232, val loss: 0.1215, val acc: 0.9528  (best train acc: 0.9451, best val acc: 0.9622, best train loss: 0.1565  @ epoch 28335 )\n",
      "[Epoch: 29380] train loss: 0.1868, train acc: 0.9306, val loss: 0.1410, val acc: 0.9457  (best train acc: 0.9451, best val acc: 0.9622, best train loss: 0.1565  @ epoch 28335 )\n",
      "[Epoch: 29400] train loss: 0.1887, train acc: 0.9273, val loss: 0.1337, val acc: 0.9545  (best train acc: 0.9451, best val acc: 0.9622, best train loss: 0.1565  @ epoch 28335 )\n",
      "[Epoch: 29420] train loss: 0.2262, train acc: 0.9106, val loss: 0.1554, val acc: 0.9356  (best train acc: 0.9451, best val acc: 0.9622, best train loss: 0.1565  @ epoch 28335 )\n",
      "[Epoch: 29440] train loss: 0.2220, train acc: 0.9143, val loss: 0.1270, val acc: 0.9602  (best train acc: 0.9451, best val acc: 0.9622, best train loss: 0.1565  @ epoch 28335 )\n",
      "[Epoch: 29460] train loss: 0.1934, train acc: 0.9301, val loss: 0.1284, val acc: 0.9531  (best train acc: 0.9451, best val acc: 0.9622, best train loss: 0.1565  @ epoch 28335 )\n",
      "[Epoch: 29480] train loss: 0.2055, train acc: 0.9250, val loss: 0.1292, val acc: 0.9508  (best train acc: 0.9451, best val acc: 0.9622, best train loss: 0.1565  @ epoch 28335 )\n",
      "[Epoch: 29500] train loss: 0.1881, train acc: 0.9336, val loss: 0.1435, val acc: 0.9481  (best train acc: 0.9451, best val acc: 0.9622, best train loss: 0.1565  @ epoch 28335 )\n",
      "[Epoch: 29520] train loss: 0.1871, train acc: 0.9284, val loss: 0.1236, val acc: 0.9524  (best train acc: 0.9451, best val acc: 0.9622, best train loss: 0.1565  @ epoch 28335 )\n",
      "[Epoch: 29540] train loss: 0.1986, train acc: 0.9237, val loss: 0.1288, val acc: 0.9548  (best train acc: 0.9451, best val acc: 0.9622, best train loss: 0.1565  @ epoch 28335 )\n",
      "[Epoch: 29560] train loss: 0.1941, train acc: 0.9271, val loss: 0.1283, val acc: 0.9551  (best train acc: 0.9451, best val acc: 0.9622, best train loss: 0.1565  @ epoch 28335 )\n",
      "[Epoch: 29580] train loss: 0.1779, train acc: 0.9337, val loss: 0.1254, val acc: 0.9518  (best train acc: 0.9451, best val acc: 0.9622, best train loss: 0.1565  @ epoch 28335 )\n",
      "[Epoch: 29600] train loss: 0.1820, train acc: 0.9365, val loss: 0.1295, val acc: 0.9481  (best train acc: 0.9451, best val acc: 0.9622, best train loss: 0.1565  @ epoch 28335 )\n",
      "[Epoch: 29620] train loss: 0.1790, train acc: 0.9325, val loss: 0.1258, val acc: 0.9528  (best train acc: 0.9451, best val acc: 0.9622, best train loss: 0.1565  @ epoch 28335 )\n",
      "[Epoch: 29640] train loss: 0.1622, train acc: 0.9399, val loss: 0.1255, val acc: 0.9568  (best train acc: 0.9451, best val acc: 0.9622, best train loss: 0.1565  @ epoch 28335 )\n",
      "[Epoch: 29660] train loss: 0.1808, train acc: 0.9346, val loss: 0.1327, val acc: 0.9450  (best train acc: 0.9451, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 29680] train loss: 0.2009, train acc: 0.9253, val loss: 0.1401, val acc: 0.9464  (best train acc: 0.9451, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 29700] train loss: 0.2210, train acc: 0.9153, val loss: 0.1275, val acc: 0.9545  (best train acc: 0.9451, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 29720] train loss: 0.1954, train acc: 0.9223, val loss: 0.1252, val acc: 0.9578  (best train acc: 0.9451, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 29740] train loss: 0.1660, train acc: 0.9409, val loss: 0.1509, val acc: 0.9393  (best train acc: 0.9451, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 29760] train loss: 0.1796, train acc: 0.9338, val loss: 0.1350, val acc: 0.9470  (best train acc: 0.9451, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 29780] train loss: 0.1987, train acc: 0.9249, val loss: 0.1309, val acc: 0.9565  (best train acc: 0.9451, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 29800] train loss: 0.1826, train acc: 0.9322, val loss: 0.1428, val acc: 0.9383  (best train acc: 0.9451, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 29820] train loss: 0.1814, train acc: 0.9316, val loss: 0.1398, val acc: 0.9467  (best train acc: 0.9451, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 29840] train loss: 0.1754, train acc: 0.9391, val loss: 0.1265, val acc: 0.9555  (best train acc: 0.9451, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 29860] train loss: 0.1780, train acc: 0.9336, val loss: 0.1257, val acc: 0.9558  (best train acc: 0.9451, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 29880] train loss: 0.1707, train acc: 0.9357, val loss: 0.1342, val acc: 0.9545  (best train acc: 0.9459, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 29900] train loss: 0.1797, train acc: 0.9315, val loss: 0.1334, val acc: 0.9423  (best train acc: 0.9459, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 29920] train loss: 0.2057, train acc: 0.9275, val loss: 0.1374, val acc: 0.9477  (best train acc: 0.9459, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 29940] train loss: 0.1737, train acc: 0.9393, val loss: 0.1314, val acc: 0.9470  (best train acc: 0.9459, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 29960] train loss: 0.1658, train acc: 0.9347, val loss: 0.1295, val acc: 0.9501  (best train acc: 0.9459, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 29980] train loss: 0.1581, train acc: 0.9425, val loss: 0.1298, val acc: 0.9491  (best train acc: 0.9459, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 30000] train loss: 0.1668, train acc: 0.9380, val loss: 0.1342, val acc: 0.9444  (best train acc: 0.9459, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 30020] train loss: 0.2222, train acc: 0.9111, val loss: 0.1275, val acc: 0.9555  (best train acc: 0.9459, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 30040] train loss: 0.1805, train acc: 0.9299, val loss: 0.1244, val acc: 0.9541  (best train acc: 0.9459, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 30060] train loss: 0.1761, train acc: 0.9346, val loss: 0.1231, val acc: 0.9575  (best train acc: 0.9459, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 30080] train loss: 0.1802, train acc: 0.9346, val loss: 0.1273, val acc: 0.9535  (best train acc: 0.9459, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 30100] train loss: 0.1754, train acc: 0.9328, val loss: 0.1319, val acc: 0.9501  (best train acc: 0.9459, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 30120] train loss: 0.1834, train acc: 0.9318, val loss: 0.1329, val acc: 0.9511  (best train acc: 0.9460, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 30140] train loss: 0.1649, train acc: 0.9408, val loss: 0.1372, val acc: 0.9474  (best train acc: 0.9460, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 30160] train loss: 0.1656, train acc: 0.9409, val loss: 0.1321, val acc: 0.9548  (best train acc: 0.9460, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 30180] train loss: 0.1923, train acc: 0.9238, val loss: 0.1325, val acc: 0.9551  (best train acc: 0.9460, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 30200] train loss: 0.1723, train acc: 0.9408, val loss: 0.1318, val acc: 0.9535  (best train acc: 0.9460, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 30220] train loss: 0.1751, train acc: 0.9341, val loss: 0.1304, val acc: 0.9454  (best train acc: 0.9460, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 30240] train loss: 0.1752, train acc: 0.9367, val loss: 0.1291, val acc: 0.9572  (best train acc: 0.9460, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 30260] train loss: 1.0140, train acc: 0.6204, val loss: 0.6092, val acc: 0.6728  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 30280] train loss: 0.5549, train acc: 0.8047, val loss: 0.4198, val acc: 0.8634  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 30300] train loss: 0.4203, train acc: 0.8569, val loss: 0.2790, val acc: 0.9191  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 30320] train loss: 0.4109, train acc: 0.8456, val loss: 0.2385, val acc: 0.9373  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 30340] train loss: 0.3254, train acc: 0.8752, val loss: 0.2359, val acc: 0.9349  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 30360] train loss: 0.3003, train acc: 0.8947, val loss: 0.2171, val acc: 0.9440  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 30380] train loss: 0.3095, train acc: 0.8913, val loss: 0.2322, val acc: 0.9143  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 30400] train loss: 0.3356, train acc: 0.8712, val loss: 0.2141, val acc: 0.9417  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 30420] train loss: 0.3026, train acc: 0.8918, val loss: 0.2144, val acc: 0.9440  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 30440] train loss: 0.3172, train acc: 0.8686, val loss: 0.2026, val acc: 0.9433  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 30460] train loss: 0.3056, train acc: 0.8935, val loss: 0.2078, val acc: 0.9309  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 30480] train loss: 0.2841, train acc: 0.9072, val loss: 0.2011, val acc: 0.9450  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 30500] train loss: 0.2807, train acc: 0.9028, val loss: 0.2103, val acc: 0.9396  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 30520] train loss: 0.2992, train acc: 0.8967, val loss: 0.2070, val acc: 0.9174  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 30540] train loss: 0.3226, train acc: 0.8769, val loss: 0.2173, val acc: 0.9325  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 30560] train loss: 0.3128, train acc: 0.8767, val loss: 0.2019, val acc: 0.9329  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 30580] train loss: 0.3049, train acc: 0.8757, val loss: 0.2055, val acc: 0.9454  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 30600] train loss: 0.2895, train acc: 0.8885, val loss: 0.1943, val acc: 0.9430  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 30620] train loss: 0.2749, train acc: 0.9065, val loss: 0.2012, val acc: 0.9376  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 30640] train loss: 0.2963, train acc: 0.8871, val loss: 0.1846, val acc: 0.9467  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 30660] train loss: 0.2847, train acc: 0.8981, val loss: 0.1904, val acc: 0.9430  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 30680] train loss: 0.2826, train acc: 0.8997, val loss: 0.2016, val acc: 0.9140  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 30700] train loss: 0.2537, train acc: 0.9189, val loss: 0.2015, val acc: 0.9332  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 30720] train loss: 0.3492, train acc: 0.8764, val loss: 0.2172, val acc: 0.9332  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 30740] train loss: 0.2570, train acc: 0.9174, val loss: 0.1924, val acc: 0.9366  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 30760] train loss: 0.3830, train acc: 0.8511, val loss: 0.1979, val acc: 0.9312  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 30780] train loss: 0.2475, train acc: 0.9169, val loss: 0.1828, val acc: 0.9417  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 30800] train loss: 0.3468, train acc: 0.8717, val loss: 0.1896, val acc: 0.9406  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 30820] train loss: 0.3354, train acc: 0.8584, val loss: 0.1875, val acc: 0.9282  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 30840] train loss: 0.2497, train acc: 0.9104, val loss: 0.1788, val acc: 0.9450  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 30860] train loss: 0.2760, train acc: 0.8971, val loss: 0.1877, val acc: 0.9285  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 30880] train loss: 0.2842, train acc: 0.8940, val loss: 0.1920, val acc: 0.9447  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 30900] train loss: 0.2589, train acc: 0.9100, val loss: 0.1843, val acc: 0.9373  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 30920] train loss: 0.2506, train acc: 0.9132, val loss: 0.1783, val acc: 0.9450  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 30940] train loss: 0.2954, train acc: 0.9021, val loss: 0.1766, val acc: 0.9457  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 30960] train loss: 0.2687, train acc: 0.9072, val loss: 0.1818, val acc: 0.9437  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 30980] train loss: 0.3753, train acc: 0.8460, val loss: 0.1987, val acc: 0.9332  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 31000] train loss: 0.2558, train acc: 0.9153, val loss: 0.1759, val acc: 0.9460  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 31020] train loss: 0.2478, train acc: 0.9109, val loss: 0.1751, val acc: 0.9467  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 31040] train loss: 0.2608, train acc: 0.9052, val loss: 0.1844, val acc: 0.9349  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 31060] train loss: 0.2569, train acc: 0.9044, val loss: 0.1829, val acc: 0.9322  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 31080] train loss: 0.2553, train acc: 0.9138, val loss: 0.1842, val acc: 0.9390  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 31100] train loss: 0.2412, train acc: 0.9117, val loss: 0.1761, val acc: 0.9474  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 31120] train loss: 0.2473, train acc: 0.9111, val loss: 0.1722, val acc: 0.9460  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 31140] train loss: 0.2880, train acc: 0.8956, val loss: 0.1730, val acc: 0.9454  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 31160] train loss: 0.2450, train acc: 0.9052, val loss: 0.1785, val acc: 0.9430  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 31180] train loss: 0.2604, train acc: 0.8999, val loss: 0.1809, val acc: 0.9336  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 31200] train loss: 0.2420, train acc: 0.9188, val loss: 0.1780, val acc: 0.9450  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 31220] train loss: 0.2794, train acc: 0.8873, val loss: 0.1680, val acc: 0.9444  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 31240] train loss: 0.2565, train acc: 0.9065, val loss: 0.1698, val acc: 0.9467  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 31260] train loss: 0.2298, train acc: 0.9213, val loss: 0.1675, val acc: 0.9481  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 31280] train loss: 0.2773, train acc: 0.8967, val loss: 0.1715, val acc: 0.9450  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 31300] train loss: 0.2453, train acc: 0.9118, val loss: 0.1729, val acc: 0.9440  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 31320] train loss: 0.2523, train acc: 0.9143, val loss: 0.1885, val acc: 0.9211  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 31340] train loss: 0.2457, train acc: 0.9156, val loss: 0.1701, val acc: 0.9457  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 31360] train loss: 0.2298, train acc: 0.9224, val loss: 0.1756, val acc: 0.9481  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 31380] train loss: 0.2543, train acc: 0.9104, val loss: 0.1690, val acc: 0.9470  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 31400] train loss: 0.2337, train acc: 0.9166, val loss: 0.1667, val acc: 0.9454  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 31420] train loss: 0.2565, train acc: 0.9023, val loss: 0.1760, val acc: 0.9447  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 31440] train loss: 0.2231, train acc: 0.9222, val loss: 0.1711, val acc: 0.9427  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 31460] train loss: 0.2422, train acc: 0.9203, val loss: 0.1643, val acc: 0.9454  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 31480] train loss: 0.2370, train acc: 0.9138, val loss: 0.1662, val acc: 0.9464  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 31500] train loss: 0.2561, train acc: 0.8979, val loss: 0.1703, val acc: 0.9417  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 31520] train loss: 0.2375, train acc: 0.9096, val loss: 0.1997, val acc: 0.9194  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 31540] train loss: 0.2506, train acc: 0.9085, val loss: 0.1728, val acc: 0.9444  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 31560] train loss: 0.2220, train acc: 0.9190, val loss: 0.1685, val acc: 0.9487  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 31580] train loss: 0.2361, train acc: 0.9173, val loss: 0.1703, val acc: 0.9413  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 31600] train loss: 0.2638, train acc: 0.8957, val loss: 0.1684, val acc: 0.9410  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 31620] train loss: 0.2993, train acc: 0.8913, val loss: 0.1707, val acc: 0.9447  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 31640] train loss: 0.2772, train acc: 0.8946, val loss: 0.1669, val acc: 0.9464  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 31660] train loss: 0.2282, train acc: 0.9184, val loss: 0.1631, val acc: 0.9487  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 31680] train loss: 0.2780, train acc: 0.8883, val loss: 0.1653, val acc: 0.9444  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 31700] train loss: 0.2219, train acc: 0.9255, val loss: 0.1710, val acc: 0.9491  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 31720] train loss: 0.3005, train acc: 0.8858, val loss: 0.1674, val acc: 0.9477  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 31740] train loss: 0.2250, train acc: 0.9184, val loss: 0.1708, val acc: 0.9470  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 31760] train loss: 0.2329, train acc: 0.9247, val loss: 0.1680, val acc: 0.9417  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 31780] train loss: 0.2097, train acc: 0.9297, val loss: 0.1736, val acc: 0.9400  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 31800] train loss: 0.2177, train acc: 0.9243, val loss: 0.1631, val acc: 0.9484  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 31820] train loss: 0.2518, train acc: 0.9093, val loss: 0.1799, val acc: 0.9487  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 31840] train loss: 0.2266, train acc: 0.9180, val loss: 0.1615, val acc: 0.9450  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 31860] train loss: 0.2112, train acc: 0.9237, val loss: 0.1707, val acc: 0.9437  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 31880] train loss: 0.2597, train acc: 0.9101, val loss: 0.1707, val acc: 0.9427  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 31900] train loss: 0.2297, train acc: 0.9100, val loss: 0.1639, val acc: 0.9460  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 31920] train loss: 0.2198, train acc: 0.9189, val loss: 0.1525, val acc: 0.9460  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 31940] train loss: 0.2627, train acc: 0.9003, val loss: 0.1579, val acc: 0.9491  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 31960] train loss: 0.2211, train acc: 0.9139, val loss: 0.1613, val acc: 0.9427  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 31980] train loss: 0.2673, train acc: 0.9011, val loss: 0.1606, val acc: 0.9454  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 32000] train loss: 0.2310, train acc: 0.9106, val loss: 0.1601, val acc: 0.9494  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 32020] train loss: 0.2416, train acc: 0.9122, val loss: 0.1633, val acc: 0.9430  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 32040] train loss: 0.2128, train acc: 0.9247, val loss: 0.1554, val acc: 0.9477  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 32060] train loss: 0.2675, train acc: 0.9004, val loss: 0.1538, val acc: 0.9464  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 32080] train loss: 0.2452, train acc: 0.9052, val loss: 0.1597, val acc: 0.9470  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 32100] train loss: 0.1999, train acc: 0.9284, val loss: 0.1535, val acc: 0.9457  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 32120] train loss: 0.2292, train acc: 0.9117, val loss: 0.1665, val acc: 0.9504  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 32140] train loss: 0.2191, train acc: 0.9214, val loss: 0.1628, val acc: 0.9433  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 32160] train loss: 0.2213, train acc: 0.9248, val loss: 0.1562, val acc: 0.9487  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 32180] train loss: 0.2306, train acc: 0.9119, val loss: 0.1564, val acc: 0.9497  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 32200] train loss: 0.2124, train acc: 0.9279, val loss: 0.1623, val acc: 0.9423  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 32220] train loss: 0.2461, train acc: 0.9056, val loss: 0.1543, val acc: 0.9481  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 32240] train loss: 0.2246, train acc: 0.9147, val loss: 0.1616, val acc: 0.9484  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 32260] train loss: 0.2120, train acc: 0.9202, val loss: 0.1616, val acc: 0.9454  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 32280] train loss: 0.2328, train acc: 0.9213, val loss: 0.1591, val acc: 0.9474  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 32300] train loss: 0.2469, train acc: 0.9143, val loss: 0.1625, val acc: 0.9467  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 32320] train loss: 0.2129, train acc: 0.9258, val loss: 0.1544, val acc: 0.9477  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 32340] train loss: 0.2459, train acc: 0.9058, val loss: 0.1600, val acc: 0.9487  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 32360] train loss: 0.2560, train acc: 0.9119, val loss: 0.1590, val acc: 0.9501  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 32380] train loss: 0.2182, train acc: 0.9198, val loss: 0.1598, val acc: 0.9491  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 32400] train loss: 0.2104, train acc: 0.9278, val loss: 0.1585, val acc: 0.9427  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 32420] train loss: 0.2161, train acc: 0.9196, val loss: 0.1548, val acc: 0.9494  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 32440] train loss: 0.2113, train acc: 0.9280, val loss: 0.1605, val acc: 0.9454  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 32460] train loss: 0.2198, train acc: 0.9160, val loss: 0.1507, val acc: 0.9491  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 32480] train loss: 0.2039, train acc: 0.9269, val loss: 0.1574, val acc: 0.9444  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 32500] train loss: 0.1955, train acc: 0.9289, val loss: 0.1587, val acc: 0.9447  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 32520] train loss: 0.2342, train acc: 0.9126, val loss: 0.1604, val acc: 0.9514  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 32540] train loss: 0.2054, train acc: 0.9246, val loss: 0.1509, val acc: 0.9484  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 32560] train loss: 0.2178, train acc: 0.9174, val loss: 0.1558, val acc: 0.9494  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 32580] train loss: 0.2197, train acc: 0.9206, val loss: 0.1655, val acc: 0.9437  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 32600] train loss: 0.2446, train acc: 0.9088, val loss: 0.1678, val acc: 0.9464  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 32620] train loss: 0.2427, train acc: 0.9001, val loss: 0.1542, val acc: 0.9497  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 32640] train loss: 0.2102, train acc: 0.9284, val loss: 0.1569, val acc: 0.9464  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 32660] train loss: 0.2331, train acc: 0.9121, val loss: 0.1569, val acc: 0.9464  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 32680] train loss: 0.2223, train acc: 0.9179, val loss: 0.1562, val acc: 0.9497  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 32700] train loss: 0.2196, train acc: 0.9200, val loss: 0.1580, val acc: 0.9491  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 32720] train loss: 0.2399, train acc: 0.9148, val loss: 0.1566, val acc: 0.9487  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 32740] train loss: 0.2213, train acc: 0.9234, val loss: 0.1606, val acc: 0.9521  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 32760] train loss: 0.1954, train acc: 0.9312, val loss: 0.1544, val acc: 0.9504  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 32780] train loss: 0.2750, train acc: 0.9014, val loss: 0.1546, val acc: 0.9467  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 32800] train loss: 0.2149, train acc: 0.9135, val loss: 0.1566, val acc: 0.9467  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 32820] train loss: 0.1924, train acc: 0.9338, val loss: 0.1691, val acc: 0.9359  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 32840] train loss: 0.2126, train acc: 0.9230, val loss: 0.1614, val acc: 0.9450  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 32860] train loss: 0.2406, train acc: 0.9089, val loss: 0.1577, val acc: 0.9497  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 32880] train loss: 0.2477, train acc: 0.9055, val loss: 0.1584, val acc: 0.9484  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 32900] train loss: 0.2327, train acc: 0.9164, val loss: 0.1532, val acc: 0.9521  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 32920] train loss: 0.2174, train acc: 0.9216, val loss: 0.1498, val acc: 0.9487  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 32940] train loss: 0.2106, train acc: 0.9237, val loss: 0.1566, val acc: 0.9491  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 32960] train loss: 0.3490, train acc: 0.8886, val loss: 0.1527, val acc: 0.9511  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 32980] train loss: 0.2011, train acc: 0.9281, val loss: 0.1503, val acc: 0.9521  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 33000] train loss: 0.2012, train acc: 0.9233, val loss: 0.1551, val acc: 0.9481  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 33020] train loss: 0.2267, train acc: 0.9122, val loss: 0.1674, val acc: 0.9420  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 33040] train loss: 0.2134, train acc: 0.9265, val loss: 0.1733, val acc: 0.9390  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 33060] train loss: 0.1989, train acc: 0.9315, val loss: 0.1713, val acc: 0.9349  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 33080] train loss: 0.2184, train acc: 0.9155, val loss: 0.1577, val acc: 0.9467  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 33100] train loss: 0.2043, train acc: 0.9250, val loss: 0.1737, val acc: 0.9369  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 33120] train loss: 0.2252, train acc: 0.9140, val loss: 0.1469, val acc: 0.9514  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 33140] train loss: 0.1904, train acc: 0.9308, val loss: 0.1595, val acc: 0.9467  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 33160] train loss: 0.2304, train acc: 0.9119, val loss: 0.1570, val acc: 0.9494  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 33180] train loss: 0.2100, train acc: 0.9235, val loss: 0.1704, val acc: 0.9467  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 33200] train loss: 0.2131, train acc: 0.9239, val loss: 0.1579, val acc: 0.9481  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 33220] train loss: 0.2306, train acc: 0.9057, val loss: 0.1478, val acc: 0.9497  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 33240] train loss: 0.2104, train acc: 0.9162, val loss: 0.1518, val acc: 0.9467  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 33260] train loss: 0.2110, train acc: 0.9229, val loss: 0.1558, val acc: 0.9481  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 33280] train loss: 0.2160, train acc: 0.9165, val loss: 0.1578, val acc: 0.9433  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 33300] train loss: 0.2359, train acc: 0.9064, val loss: 0.1553, val acc: 0.9504  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 33320] train loss: 0.2098, train acc: 0.9241, val loss: 0.1537, val acc: 0.9518  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 33340] train loss: 0.2083, train acc: 0.9248, val loss: 0.1579, val acc: 0.9504  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 33360] train loss: 0.2130, train acc: 0.9226, val loss: 0.1597, val acc: 0.9521  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 33380] train loss: 0.2202, train acc: 0.9226, val loss: 0.1584, val acc: 0.9504  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 33400] train loss: 0.2496, train acc: 0.9042, val loss: 0.1556, val acc: 0.9504  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 33420] train loss: 0.2140, train acc: 0.9263, val loss: 0.1522, val acc: 0.9497  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 33440] train loss: 0.2261, train acc: 0.9223, val loss: 0.1555, val acc: 0.9477  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 33460] train loss: 0.2063, train acc: 0.9272, val loss: 0.1564, val acc: 0.9484  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 33480] train loss: 0.2164, train acc: 0.9192, val loss: 0.1797, val acc: 0.9369  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 33500] train loss: 0.2140, train acc: 0.9199, val loss: 0.1546, val acc: 0.9481  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 33520] train loss: 0.2172, train acc: 0.9156, val loss: 0.1629, val acc: 0.9487  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 33540] train loss: 0.2011, train acc: 0.9282, val loss: 0.1589, val acc: 0.9454  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 33560] train loss: 0.2115, train acc: 0.9213, val loss: 0.1542, val acc: 0.9508  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 33580] train loss: 0.2000, train acc: 0.9287, val loss: 0.1579, val acc: 0.9484  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 33600] train loss: 0.2092, train acc: 0.9273, val loss: 0.1509, val acc: 0.9504  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 33620] train loss: 0.2092, train acc: 0.9232, val loss: 0.1506, val acc: 0.9497  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 33640] train loss: 0.2532, train acc: 0.9034, val loss: 0.1521, val acc: 0.9491  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 33660] train loss: 0.2222, train acc: 0.9164, val loss: 0.1479, val acc: 0.9494  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 33680] train loss: 0.2209, train acc: 0.9185, val loss: 0.1596, val acc: 0.9464  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 33700] train loss: 0.2316, train acc: 0.9093, val loss: 0.1581, val acc: 0.9491  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 33720] train loss: 0.2185, train acc: 0.9244, val loss: 0.1670, val acc: 0.9433  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 33740] train loss: 0.2662, train acc: 0.8973, val loss: 0.1503, val acc: 0.9464  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 33760] train loss: 0.2745, train acc: 0.8938, val loss: 0.1506, val acc: 0.9511  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 33780] train loss: 0.2350, train acc: 0.9175, val loss: 0.1495, val acc: 0.9531  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 33800] train loss: 0.2068, train acc: 0.9208, val loss: 0.1605, val acc: 0.9470  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 33820] train loss: 0.2285, train acc: 0.9127, val loss: 0.1623, val acc: 0.9430  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 33840] train loss: 0.2236, train acc: 0.9165, val loss: 0.1552, val acc: 0.9467  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 33860] train loss: 0.2053, train acc: 0.9263, val loss: 0.1420, val acc: 0.9518  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 33880] train loss: 0.2407, train acc: 0.9085, val loss: 0.1514, val acc: 0.9497  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 33900] train loss: 0.3007, train acc: 0.8908, val loss: 0.1626, val acc: 0.9504  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 33920] train loss: 0.2246, train acc: 0.9184, val loss: 0.1563, val acc: 0.9484  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 33940] train loss: 0.2225, train acc: 0.9197, val loss: 0.1536, val acc: 0.9508  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 33960] train loss: 0.1866, train acc: 0.9294, val loss: 0.1451, val acc: 0.9521  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 33980] train loss: 0.2349, train acc: 0.9048, val loss: 0.1509, val acc: 0.9481  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 34000] train loss: 0.2667, train acc: 0.9023, val loss: 0.1614, val acc: 0.9447  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 34020] train loss: 0.2106, train acc: 0.9218, val loss: 0.1529, val acc: 0.9504  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 34040] train loss: 0.1989, train acc: 0.9228, val loss: 0.1473, val acc: 0.9504  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 34060] train loss: 0.2133, train acc: 0.9241, val loss: 0.1571, val acc: 0.9484  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 34080] train loss: 0.2041, train acc: 0.9266, val loss: 0.1594, val acc: 0.9474  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 34100] train loss: 0.2021, train acc: 0.9224, val loss: 0.1505, val acc: 0.9511  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 34120] train loss: 0.1897, train acc: 0.9315, val loss: 0.1520, val acc: 0.9464  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 34140] train loss: 0.2215, train acc: 0.9196, val loss: 0.1524, val acc: 0.9497  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 34160] train loss: 0.1952, train acc: 0.9258, val loss: 0.1530, val acc: 0.9521  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 34180] train loss: 0.1936, train acc: 0.9318, val loss: 0.1538, val acc: 0.9474  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 34200] train loss: 0.2247, train acc: 0.9182, val loss: 0.1531, val acc: 0.9521  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 34220] train loss: 0.2258, train acc: 0.9178, val loss: 0.1587, val acc: 0.9491  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 34240] train loss: 0.2035, train acc: 0.9218, val loss: 0.1475, val acc: 0.9508  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 34260] train loss: 0.1854, train acc: 0.9353, val loss: 0.1533, val acc: 0.9508  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 34280] train loss: 0.2669, train acc: 0.8947, val loss: 0.1540, val acc: 0.9491  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 34300] train loss: 0.2435, train acc: 0.9093, val loss: 0.1657, val acc: 0.9413  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 34320] train loss: 0.2222, train acc: 0.9135, val loss: 0.1585, val acc: 0.9524  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 34340] train loss: 0.2405, train acc: 0.9088, val loss: 0.1676, val acc: 0.9487  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 34360] train loss: 0.2648, train acc: 0.9015, val loss: 0.1679, val acc: 0.9400  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 34380] train loss: 0.2020, train acc: 0.9250, val loss: 0.1566, val acc: 0.9504  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 34400] train loss: 0.2247, train acc: 0.9082, val loss: 0.1521, val acc: 0.9518  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 34420] train loss: 0.2149, train acc: 0.9195, val loss: 0.1558, val acc: 0.9487  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 34440] train loss: 0.1974, train acc: 0.9290, val loss: 0.1459, val acc: 0.9528  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 34460] train loss: 0.1870, train acc: 0.9306, val loss: 0.1510, val acc: 0.9514  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 34480] train loss: 0.1904, train acc: 0.9304, val loss: 0.1474, val acc: 0.9528  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 34500] train loss: 0.2103, train acc: 0.9222, val loss: 0.1461, val acc: 0.9555  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 34520] train loss: 0.2164, train acc: 0.9230, val loss: 0.1561, val acc: 0.9481  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 34540] train loss: 0.2115, train acc: 0.9193, val loss: 0.1519, val acc: 0.9447  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 34560] train loss: 0.1896, train acc: 0.9291, val loss: 0.1554, val acc: 0.9470  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 34580] train loss: 0.2168, train acc: 0.9175, val loss: 0.1622, val acc: 0.9474  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 34600] train loss: 0.2270, train acc: 0.9075, val loss: 0.1590, val acc: 0.9484  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 34620] train loss: 0.2149, train acc: 0.9200, val loss: 0.1581, val acc: 0.9457  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 34640] train loss: 0.2180, train acc: 0.9117, val loss: 0.1591, val acc: 0.9477  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 34660] train loss: 0.2057, train acc: 0.9241, val loss: 0.1538, val acc: 0.9504  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 34680] train loss: 0.2046, train acc: 0.9241, val loss: 0.1533, val acc: 0.9447  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 34700] train loss: 0.2563, train acc: 0.9002, val loss: 0.1526, val acc: 0.9501  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 34720] train loss: 0.1985, train acc: 0.9255, val loss: 0.1643, val acc: 0.9508  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 34740] train loss: 0.1886, train acc: 0.9289, val loss: 0.1487, val acc: 0.9541  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 34760] train loss: 0.3076, train acc: 0.9043, val loss: 0.1461, val acc: 0.9528  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 34780] train loss: 0.2022, train acc: 0.9213, val loss: 0.1529, val acc: 0.9524  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 34800] train loss: 0.2154, train acc: 0.9215, val loss: 0.1514, val acc: 0.9521  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 34820] train loss: 0.1862, train acc: 0.9321, val loss: 0.1465, val acc: 0.9555  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 34840] train loss: 0.2373, train acc: 0.9148, val loss: 0.1593, val acc: 0.9440  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 34860] train loss: 0.1956, train acc: 0.9313, val loss: 0.1678, val acc: 0.9518  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 34880] train loss: 0.1971, train acc: 0.9304, val loss: 0.1557, val acc: 0.9508  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 34900] train loss: 0.2155, train acc: 0.9163, val loss: 0.1445, val acc: 0.9528  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 34920] train loss: 0.1926, train acc: 0.9242, val loss: 0.1566, val acc: 0.9481  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 34940] train loss: 0.2233, train acc: 0.9119, val loss: 0.1542, val acc: 0.9541  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 34960] train loss: 0.2147, train acc: 0.9179, val loss: 0.1612, val acc: 0.9535  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 34980] train loss: 0.1694, train acc: 0.9387, val loss: 0.1536, val acc: 0.9545  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 35000] train loss: 0.1781, train acc: 0.9346, val loss: 0.1435, val acc: 0.9528  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 35020] train loss: 0.2192, train acc: 0.9124, val loss: 0.1490, val acc: 0.9494  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 35040] train loss: 0.3180, train acc: 0.8926, val loss: 0.1544, val acc: 0.9491  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 35060] train loss: 0.2229, train acc: 0.9106, val loss: 0.1496, val acc: 0.9524  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 35080] train loss: 0.1993, train acc: 0.9284, val loss: 0.1524, val acc: 0.9521  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 35100] train loss: 0.2401, train acc: 0.9062, val loss: 0.1485, val acc: 0.9572  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 35120] train loss: 0.1776, train acc: 0.9357, val loss: 0.1641, val acc: 0.9460  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 35140] train loss: 0.1965, train acc: 0.9224, val loss: 0.1478, val acc: 0.9511  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 35160] train loss: 0.2060, train acc: 0.9149, val loss: 0.1516, val acc: 0.9491  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 35180] train loss: 0.2022, train acc: 0.9294, val loss: 0.1605, val acc: 0.9504  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 35200] train loss: 0.1933, train acc: 0.9278, val loss: 0.1524, val acc: 0.9514  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 35220] train loss: 0.1983, train acc: 0.9266, val loss: 0.1504, val acc: 0.9524  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 35240] train loss: 0.2213, train acc: 0.9186, val loss: 0.1559, val acc: 0.9467  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 35260] train loss: 0.1932, train acc: 0.9302, val loss: 0.1593, val acc: 0.9494  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 35280] train loss: 0.2317, train acc: 0.9157, val loss: 0.1506, val acc: 0.9497  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 35300] train loss: 0.1795, train acc: 0.9343, val loss: 0.1533, val acc: 0.9531  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 35320] train loss: 0.2113, train acc: 0.9187, val loss: 0.1551, val acc: 0.9514  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 35340] train loss: 0.1853, train acc: 0.9270, val loss: 0.1601, val acc: 0.9494  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 35360] train loss: 0.1943, train acc: 0.9260, val loss: 0.1512, val acc: 0.9460  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 35380] train loss: 0.2013, train acc: 0.9212, val loss: 0.1617, val acc: 0.9454  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 35400] train loss: 0.1722, train acc: 0.9362, val loss: 0.1593, val acc: 0.9555  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 35420] train loss: 0.1979, train acc: 0.9288, val loss: 0.1575, val acc: 0.9497  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 35440] train loss: 0.1985, train acc: 0.9153, val loss: 0.1476, val acc: 0.9538  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 35460] train loss: 0.2274, train acc: 0.9102, val loss: 0.1566, val acc: 0.9511  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 35480] train loss: 0.1780, train acc: 0.9358, val loss: 0.1526, val acc: 0.9565  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 35500] train loss: 0.1886, train acc: 0.9224, val loss: 0.1567, val acc: 0.9514  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 35520] train loss: 0.2013, train acc: 0.9301, val loss: 0.1454, val acc: 0.9568  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 35540] train loss: 0.1902, train acc: 0.9258, val loss: 0.1534, val acc: 0.9518  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 35560] train loss: 0.2018, train acc: 0.9259, val loss: 0.1545, val acc: 0.9535  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 35580] train loss: 0.1778, train acc: 0.9320, val loss: 0.1448, val acc: 0.9514  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 35600] train loss: 0.1926, train acc: 0.9315, val loss: 0.1502, val acc: 0.9524  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 35620] train loss: 0.1824, train acc: 0.9328, val loss: 0.1534, val acc: 0.9501  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 35640] train loss: 0.2003, train acc: 0.9302, val loss: 0.1460, val acc: 0.9535  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 35660] train loss: 0.1921, train acc: 0.9275, val loss: 0.1468, val acc: 0.9541  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 35680] train loss: 0.1869, train acc: 0.9337, val loss: 0.1505, val acc: 0.9497  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 35700] train loss: 0.2084, train acc: 0.9187, val loss: 0.1526, val acc: 0.9494  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 35720] train loss: 0.2003, train acc: 0.9215, val loss: 0.1459, val acc: 0.9551  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 35740] train loss: 0.1891, train acc: 0.9290, val loss: 0.1516, val acc: 0.9518  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 35760] train loss: 0.1860, train acc: 0.9315, val loss: 0.1548, val acc: 0.9464  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 35780] train loss: 0.1930, train acc: 0.9256, val loss: 0.1519, val acc: 0.9538  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 35800] train loss: 0.2000, train acc: 0.9143, val loss: 0.1480, val acc: 0.9521  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 35820] train loss: 0.1876, train acc: 0.9252, val loss: 0.1473, val acc: 0.9487  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 35840] train loss: 0.1965, train acc: 0.9254, val loss: 0.1556, val acc: 0.9481  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 35860] train loss: 0.1956, train acc: 0.9224, val loss: 0.1520, val acc: 0.9497  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 35880] train loss: 0.2281, train acc: 0.9127, val loss: 0.1678, val acc: 0.9437  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 35900] train loss: 0.1887, train acc: 0.9297, val loss: 0.1574, val acc: 0.9447  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 35920] train loss: 0.1808, train acc: 0.9320, val loss: 0.1503, val acc: 0.9538  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 35940] train loss: 0.1740, train acc: 0.9395, val loss: 0.1502, val acc: 0.9494  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 35960] train loss: 0.2055, train acc: 0.9213, val loss: 0.1432, val acc: 0.9521  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 35980] train loss: 0.1965, train acc: 0.9191, val loss: 0.1487, val acc: 0.9497  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 36000] train loss: 0.1840, train acc: 0.9308, val loss: 0.1478, val acc: 0.9531  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 36020] train loss: 0.1770, train acc: 0.9389, val loss: 0.1486, val acc: 0.9511  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 36040] train loss: 0.1991, train acc: 0.9268, val loss: 0.1451, val acc: 0.9531  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 36060] train loss: 0.1834, train acc: 0.9319, val loss: 0.1463, val acc: 0.9535  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 36080] train loss: 0.2199, train acc: 0.9291, val loss: 0.1581, val acc: 0.9487  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 36100] train loss: 0.2012, train acc: 0.9233, val loss: 0.1664, val acc: 0.9417  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 36120] train loss: 0.1732, train acc: 0.9352, val loss: 0.1501, val acc: 0.9470  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 36140] train loss: 0.1832, train acc: 0.9328, val loss: 0.1517, val acc: 0.9535  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 36160] train loss: 0.1779, train acc: 0.9298, val loss: 0.1465, val acc: 0.9535  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 36180] train loss: 0.1919, train acc: 0.9280, val loss: 0.1514, val acc: 0.9555  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 36200] train loss: 0.1804, train acc: 0.9307, val loss: 0.1476, val acc: 0.9538  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 36220] train loss: 0.1865, train acc: 0.9301, val loss: 0.1623, val acc: 0.9450  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 36240] train loss: 0.1993, train acc: 0.9254, val loss: 0.1524, val acc: 0.9514  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 36260] train loss: 0.1839, train acc: 0.9271, val loss: 0.1497, val acc: 0.9481  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 36280] train loss: 0.2009, train acc: 0.9268, val loss: 0.1453, val acc: 0.9555  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 36300] train loss: 0.1794, train acc: 0.9329, val loss: 0.1500, val acc: 0.9535  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 36320] train loss: 0.1800, train acc: 0.9341, val loss: 0.1493, val acc: 0.9484  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 36340] train loss: 0.1963, train acc: 0.9273, val loss: 0.1420, val acc: 0.9524  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 36360] train loss: 0.2078, train acc: 0.9152, val loss: 0.1539, val acc: 0.9508  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 36380] train loss: 0.2175, train acc: 0.9119, val loss: 0.1674, val acc: 0.9454  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 36400] train loss: 0.2000, train acc: 0.9281, val loss: 0.1860, val acc: 0.9332  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 36420] train loss: 0.2143, train acc: 0.9169, val loss: 0.1544, val acc: 0.9511  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 36440] train loss: 0.1849, train acc: 0.9312, val loss: 0.1538, val acc: 0.9541  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 36460] train loss: 0.1825, train acc: 0.9308, val loss: 0.1430, val acc: 0.9514  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 36480] train loss: 0.2357, train acc: 0.9033, val loss: 0.1789, val acc: 0.9376  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 36500] train loss: 0.1901, train acc: 0.9307, val loss: 0.1567, val acc: 0.9511  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 36520] train loss: 0.1701, train acc: 0.9334, val loss: 0.1448, val acc: 0.9521  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 36540] train loss: 0.1981, train acc: 0.9291, val loss: 0.1483, val acc: 0.9524  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 36560] train loss: 0.1986, train acc: 0.9274, val loss: 0.1621, val acc: 0.9433  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 36580] train loss: 0.1781, train acc: 0.9304, val loss: 0.1435, val acc: 0.9514  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 36600] train loss: 0.1955, train acc: 0.9221, val loss: 0.1424, val acc: 0.9555  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 36620] train loss: 0.1880, train acc: 0.9313, val loss: 0.1477, val acc: 0.9518  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 36640] train loss: 0.2165, train acc: 0.9235, val loss: 0.1578, val acc: 0.9494  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 36660] train loss: 0.1768, train acc: 0.9333, val loss: 0.1495, val acc: 0.9518  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 36680] train loss: 0.1760, train acc: 0.9325, val loss: 0.1489, val acc: 0.9548  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 36700] train loss: 0.2005, train acc: 0.9234, val loss: 0.1497, val acc: 0.9491  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 36720] train loss: 0.2191, train acc: 0.9068, val loss: 0.1540, val acc: 0.9535  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 36740] train loss: 0.2302, train acc: 0.9006, val loss: 0.1646, val acc: 0.9420  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 36760] train loss: 0.2059, train acc: 0.9145, val loss: 0.1475, val acc: 0.9504  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 36780] train loss: 0.1946, train acc: 0.9238, val loss: 0.1476, val acc: 0.9514  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 36800] train loss: 0.1821, train acc: 0.9320, val loss: 0.1438, val acc: 0.9545  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 36820] train loss: 0.1684, train acc: 0.9369, val loss: 0.1544, val acc: 0.9477  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 36840] train loss: 0.2028, train acc: 0.9221, val loss: 0.1481, val acc: 0.9494  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 36860] train loss: 0.2005, train acc: 0.9250, val loss: 0.1484, val acc: 0.9514  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 36880] train loss: 0.1912, train acc: 0.9342, val loss: 0.1525, val acc: 0.9501  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 36900] train loss: 0.2072, train acc: 0.9226, val loss: 0.1506, val acc: 0.9524  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 36920] train loss: 0.2260, train acc: 0.9223, val loss: 0.1473, val acc: 0.9531  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 36940] train loss: 0.1747, train acc: 0.9386, val loss: 0.1492, val acc: 0.9497  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 36960] train loss: 0.1718, train acc: 0.9357, val loss: 0.1483, val acc: 0.9524  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 36980] train loss: 0.1860, train acc: 0.9295, val loss: 0.1450, val acc: 0.9514  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 37000] train loss: 0.1831, train acc: 0.9302, val loss: 0.1457, val acc: 0.9555  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 37020] train loss: 0.1826, train acc: 0.9308, val loss: 0.1456, val acc: 0.9551  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 37040] train loss: 0.1842, train acc: 0.9247, val loss: 0.1615, val acc: 0.9437  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 37060] train loss: 0.1669, train acc: 0.9360, val loss: 0.1508, val acc: 0.9440  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 37080] train loss: 0.1905, train acc: 0.9279, val loss: 0.1626, val acc: 0.9457  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 37100] train loss: 0.1858, train acc: 0.9314, val loss: 0.1553, val acc: 0.9504  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 37120] train loss: 0.1909, train acc: 0.9286, val loss: 0.1697, val acc: 0.9454  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 37140] train loss: 0.1819, train acc: 0.9243, val loss: 0.1468, val acc: 0.9477  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 37160] train loss: 0.1865, train acc: 0.9275, val loss: 0.1390, val acc: 0.9538  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 37180] train loss: 0.1881, train acc: 0.9320, val loss: 0.1577, val acc: 0.9518  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 37200] train loss: 0.1919, train acc: 0.9282, val loss: 0.1627, val acc: 0.9454  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 37220] train loss: 0.2006, train acc: 0.9213, val loss: 0.1559, val acc: 0.9508  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 37240] train loss: 0.1928, train acc: 0.9292, val loss: 0.1527, val acc: 0.9487  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 37260] train loss: 0.2189, train acc: 0.9155, val loss: 0.1464, val acc: 0.9528  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 37280] train loss: 0.1736, train acc: 0.9372, val loss: 0.1557, val acc: 0.9474  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 37300] train loss: 0.2120, train acc: 0.9170, val loss: 0.1554, val acc: 0.9535  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 37320] train loss: 0.2453, train acc: 0.9069, val loss: 0.1498, val acc: 0.9535  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 37340] train loss: 0.1864, train acc: 0.9258, val loss: 0.1456, val acc: 0.9521  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 37360] train loss: 0.1770, train acc: 0.9297, val loss: 0.1526, val acc: 0.9501  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 37380] train loss: 0.1989, train acc: 0.9219, val loss: 0.1497, val acc: 0.9541  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 37400] train loss: 0.1900, train acc: 0.9344, val loss: 0.1535, val acc: 0.9508  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 37420] train loss: 0.1980, train acc: 0.9282, val loss: 0.1500, val acc: 0.9558  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 37440] train loss: 0.1783, train acc: 0.9335, val loss: 0.1534, val acc: 0.9474  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 37460] train loss: 0.1928, train acc: 0.9307, val loss: 0.1478, val acc: 0.9558  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 37480] train loss: 0.1833, train acc: 0.9324, val loss: 0.1529, val acc: 0.9538  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 37500] train loss: 0.1838, train acc: 0.9318, val loss: 0.1487, val acc: 0.9491  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 37520] train loss: 0.2059, train acc: 0.9199, val loss: 0.1486, val acc: 0.9504  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 37540] train loss: 0.1924, train acc: 0.9228, val loss: 0.1650, val acc: 0.9427  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 37560] train loss: 0.2234, train acc: 0.9203, val loss: 0.1439, val acc: 0.9518  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 37580] train loss: 0.1964, train acc: 0.9212, val loss: 0.1460, val acc: 0.9528  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 37600] train loss: 0.2222, train acc: 0.9197, val loss: 0.1530, val acc: 0.9491  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 37620] train loss: 0.1853, train acc: 0.9255, val loss: 0.1599, val acc: 0.9460  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 37640] train loss: 0.1853, train acc: 0.9354, val loss: 0.1448, val acc: 0.9504  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 37660] train loss: 0.1888, train acc: 0.9328, val loss: 0.1610, val acc: 0.9535  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 37680] train loss: 0.1775, train acc: 0.9338, val loss: 0.1567, val acc: 0.9464  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 37700] train loss: 0.1899, train acc: 0.9298, val loss: 0.1469, val acc: 0.9518  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 37720] train loss: 0.1995, train acc: 0.9209, val loss: 0.1533, val acc: 0.9545  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 37740] train loss: 0.1815, train acc: 0.9290, val loss: 0.1494, val acc: 0.9504  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 37760] train loss: 0.2218, train acc: 0.9122, val loss: 0.1595, val acc: 0.9447  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 37780] train loss: 0.1903, train acc: 0.9249, val loss: 0.1494, val acc: 0.9514  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 37800] train loss: 0.1613, train acc: 0.9431, val loss: 0.1443, val acc: 0.9551  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 37820] train loss: 0.1994, train acc: 0.9248, val loss: 0.1562, val acc: 0.9481  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 37840] train loss: 0.1896, train acc: 0.9333, val loss: 0.1461, val acc: 0.9511  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 37860] train loss: 0.1857, train acc: 0.9363, val loss: 0.1491, val acc: 0.9524  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 37880] train loss: 0.1794, train acc: 0.9281, val loss: 0.1474, val acc: 0.9508  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 37900] train loss: 0.1880, train acc: 0.9298, val loss: 0.1466, val acc: 0.9528  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 37920] train loss: 0.2393, train acc: 0.9125, val loss: 0.1483, val acc: 0.9491  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 37940] train loss: 0.1923, train acc: 0.9269, val loss: 0.1466, val acc: 0.9551  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 37960] train loss: 0.2496, train acc: 0.9042, val loss: 0.1630, val acc: 0.9400  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 37980] train loss: 0.1911, train acc: 0.9325, val loss: 0.1493, val acc: 0.9484  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 38000] train loss: 0.1737, train acc: 0.9316, val loss: 0.1528, val acc: 0.9474  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 38020] train loss: 0.1838, train acc: 0.9323, val loss: 0.1456, val acc: 0.9565  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 38040] train loss: 0.1705, train acc: 0.9332, val loss: 0.1482, val acc: 0.9504  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 38060] train loss: 0.1690, train acc: 0.9370, val loss: 0.1493, val acc: 0.9514  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 38080] train loss: 0.1753, train acc: 0.9341, val loss: 0.1479, val acc: 0.9518  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 38100] train loss: 0.2014, train acc: 0.9213, val loss: 0.1402, val acc: 0.9531  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 38120] train loss: 0.2030, train acc: 0.9190, val loss: 0.1636, val acc: 0.9400  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 38140] train loss: 0.1964, train acc: 0.9291, val loss: 0.1436, val acc: 0.9545  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 38160] train loss: 0.1845, train acc: 0.9344, val loss: 0.1441, val acc: 0.9538  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 38180] train loss: 0.1876, train acc: 0.9248, val loss: 0.1531, val acc: 0.9528  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 38200] train loss: 0.1729, train acc: 0.9396, val loss: 0.1544, val acc: 0.9524  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 38220] train loss: 0.2254, train acc: 0.9124, val loss: 0.1493, val acc: 0.9491  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 38240] train loss: 0.1752, train acc: 0.9325, val loss: 0.1423, val acc: 0.9535  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 38260] train loss: 0.1774, train acc: 0.9327, val loss: 0.1531, val acc: 0.9508  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 38280] train loss: 0.1761, train acc: 0.9358, val loss: 0.1444, val acc: 0.9514  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 38300] train loss: 0.1793, train acc: 0.9323, val loss: 0.1510, val acc: 0.9504  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 38320] train loss: 0.2140, train acc: 0.9234, val loss: 0.1451, val acc: 0.9518  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 38340] train loss: 0.1955, train acc: 0.9242, val loss: 0.1463, val acc: 0.9535  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 38360] train loss: 0.1830, train acc: 0.9291, val loss: 0.1481, val acc: 0.9494  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 38380] train loss: 0.2049, train acc: 0.9203, val loss: 0.1608, val acc: 0.9484  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 38400] train loss: 0.2253, train acc: 0.9136, val loss: 0.1470, val acc: 0.9558  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 38420] train loss: 0.1909, train acc: 0.9239, val loss: 0.1511, val acc: 0.9487  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 38440] train loss: 0.1781, train acc: 0.9331, val loss: 0.1469, val acc: 0.9518  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 38460] train loss: 0.1866, train acc: 0.9281, val loss: 0.1461, val acc: 0.9514  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 38480] train loss: 0.1801, train acc: 0.9341, val loss: 0.1512, val acc: 0.9501  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 38500] train loss: 0.1874, train acc: 0.9288, val loss: 0.1509, val acc: 0.9521  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 38520] train loss: 0.1866, train acc: 0.9307, val loss: 0.1502, val acc: 0.9524  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 38540] train loss: 0.1753, train acc: 0.9330, val loss: 0.1480, val acc: 0.9508  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 38560] train loss: 0.1904, train acc: 0.9248, val loss: 0.1444, val acc: 0.9491  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 38580] train loss: 0.1783, train acc: 0.9344, val loss: 0.1541, val acc: 0.9481  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 38600] train loss: 0.1827, train acc: 0.9270, val loss: 0.1489, val acc: 0.9535  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 38620] train loss: 0.2445, train acc: 0.9061, val loss: 0.1647, val acc: 0.9538  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 38640] train loss: 0.1932, train acc: 0.9261, val loss: 0.1401, val acc: 0.9545  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 38660] train loss: 0.1841, train acc: 0.9277, val loss: 0.1499, val acc: 0.9508  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 38680] train loss: 0.1732, train acc: 0.9305, val loss: 0.1473, val acc: 0.9470  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 38700] train loss: 0.2044, train acc: 0.9295, val loss: 0.1450, val acc: 0.9494  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 38720] train loss: 0.1602, train acc: 0.9382, val loss: 0.1420, val acc: 0.9541  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 38740] train loss: 0.1907, train acc: 0.9239, val loss: 0.1474, val acc: 0.9501  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 38760] train loss: 0.1731, train acc: 0.9373, val loss: 0.1556, val acc: 0.9494  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 38780] train loss: 0.3037, train acc: 0.8910, val loss: 0.1650, val acc: 0.9379  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 38800] train loss: 0.2228, train acc: 0.9088, val loss: 0.1453, val acc: 0.9518  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 38820] train loss: 0.1693, train acc: 0.9349, val loss: 0.1481, val acc: 0.9504  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 38840] train loss: 0.1843, train acc: 0.9371, val loss: 0.1498, val acc: 0.9548  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 38860] train loss: 0.1826, train acc: 0.9327, val loss: 0.1380, val acc: 0.9521  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 38880] train loss: 0.1873, train acc: 0.9315, val loss: 0.1627, val acc: 0.9420  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 38900] train loss: 0.2254, train acc: 0.9045, val loss: 0.1534, val acc: 0.9521  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 38920] train loss: 0.1731, train acc: 0.9337, val loss: 0.1393, val acc: 0.9538  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 38940] train loss: 0.1879, train acc: 0.9354, val loss: 0.1393, val acc: 0.9514  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 38960] train loss: 0.1707, train acc: 0.9428, val loss: 0.1424, val acc: 0.9514  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 38980] train loss: 0.2064, train acc: 0.9202, val loss: 0.1495, val acc: 0.9447  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 39000] train loss: 0.1726, train acc: 0.9361, val loss: 0.1448, val acc: 0.9535  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 39020] train loss: 0.1755, train acc: 0.9292, val loss: 0.1389, val acc: 0.9545  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 39040] train loss: 0.2267, train acc: 0.9178, val loss: 0.1524, val acc: 0.9491  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 39060] train loss: 0.1742, train acc: 0.9336, val loss: 0.1514, val acc: 0.9524  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 39080] train loss: 0.2214, train acc: 0.9174, val loss: 0.1448, val acc: 0.9524  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 39100] train loss: 0.1815, train acc: 0.9312, val loss: 0.1487, val acc: 0.9460  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 39120] train loss: 0.1723, train acc: 0.9375, val loss: 0.1471, val acc: 0.9501  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 39140] train loss: 0.1816, train acc: 0.9311, val loss: 0.1414, val acc: 0.9541  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 39160] train loss: 0.2063, train acc: 0.9205, val loss: 0.1411, val acc: 0.9521  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 39180] train loss: 0.1836, train acc: 0.9304, val loss: 0.1494, val acc: 0.9555  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 39200] train loss: 0.2144, train acc: 0.9105, val loss: 0.1472, val acc: 0.9511  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 39220] train loss: 0.1675, train acc: 0.9383, val loss: 0.1464, val acc: 0.9538  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 39240] train loss: 0.1640, train acc: 0.9401, val loss: 0.1527, val acc: 0.9535  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 39260] train loss: 0.1779, train acc: 0.9340, val loss: 0.1387, val acc: 0.9562  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 39280] train loss: 0.2296, train acc: 0.8997, val loss: 0.1495, val acc: 0.9511  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 39300] train loss: 0.1732, train acc: 0.9318, val loss: 0.1399, val acc: 0.9531  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 39320] train loss: 0.1786, train acc: 0.9352, val loss: 0.1417, val acc: 0.9575  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 39340] train loss: 0.1847, train acc: 0.9323, val loss: 0.1391, val acc: 0.9572  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 39360] train loss: 0.1956, train acc: 0.9273, val loss: 0.1465, val acc: 0.9535  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 39380] train loss: 0.1685, train acc: 0.9355, val loss: 0.1396, val acc: 0.9521  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 39400] train loss: 0.1775, train acc: 0.9277, val loss: 0.1528, val acc: 0.9535  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 39420] train loss: 0.1673, train acc: 0.9351, val loss: 0.1476, val acc: 0.9511  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 39440] train loss: 0.1708, train acc: 0.9345, val loss: 0.1490, val acc: 0.9511  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 39460] train loss: 0.1818, train acc: 0.9305, val loss: 0.1441, val acc: 0.9497  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 39480] train loss: 0.1811, train acc: 0.9276, val loss: 0.1527, val acc: 0.9454  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 39500] train loss: 0.1904, train acc: 0.9191, val loss: 0.1481, val acc: 0.9511  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 39520] train loss: 0.1813, train acc: 0.9345, val loss: 0.1534, val acc: 0.9484  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 39540] train loss: 0.1683, train acc: 0.9369, val loss: 0.1463, val acc: 0.9511  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 39560] train loss: 0.1829, train acc: 0.9282, val loss: 0.1437, val acc: 0.9518  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 39580] train loss: 0.1752, train acc: 0.9282, val loss: 0.1507, val acc: 0.9521  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 39600] train loss: 0.2048, train acc: 0.9148, val loss: 0.1448, val acc: 0.9511  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 39620] train loss: 0.1833, train acc: 0.9247, val loss: 0.1486, val acc: 0.9501  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 39640] train loss: 0.1860, train acc: 0.9285, val loss: 0.1453, val acc: 0.9528  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 39660] train loss: 0.1664, train acc: 0.9356, val loss: 0.1438, val acc: 0.9541  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1504  @ epoch 29651 )\n",
      "[Epoch: 39680] train loss: 0.1732, train acc: 0.9320, val loss: 0.1404, val acc: 0.9538  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1486  @ epoch 39661 )\n",
      "[Epoch: 39700] train loss: 0.1891, train acc: 0.9274, val loss: 0.1489, val acc: 0.9504  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1486  @ epoch 39661 )\n",
      "[Epoch: 39720] train loss: 0.1831, train acc: 0.9274, val loss: 0.1502, val acc: 0.9538  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1486  @ epoch 39661 )\n",
      "[Epoch: 39740] train loss: 0.1912, train acc: 0.9249, val loss: 0.1442, val acc: 0.9548  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1486  @ epoch 39661 )\n",
      "[Epoch: 39760] train loss: 0.1668, train acc: 0.9348, val loss: 0.1580, val acc: 0.9413  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1486  @ epoch 39661 )\n",
      "[Epoch: 39780] train loss: 0.1739, train acc: 0.9298, val loss: 0.1453, val acc: 0.9508  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1486  @ epoch 39661 )\n",
      "[Epoch: 39800] train loss: 0.1681, train acc: 0.9353, val loss: 0.1481, val acc: 0.9497  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1486  @ epoch 39661 )\n",
      "[Epoch: 39820] train loss: 0.1597, train acc: 0.9350, val loss: 0.1389, val acc: 0.9548  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1486  @ epoch 39661 )\n",
      "[Epoch: 39840] train loss: 0.1695, train acc: 0.9346, val loss: 0.1492, val acc: 0.9487  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1486  @ epoch 39661 )\n",
      "[Epoch: 39860] train loss: 0.1872, train acc: 0.9229, val loss: 0.1501, val acc: 0.9464  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1486  @ epoch 39661 )\n",
      "[Epoch: 39880] train loss: 0.1721, train acc: 0.9338, val loss: 0.1521, val acc: 0.9474  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1486  @ epoch 39661 )\n",
      "[Epoch: 39900] train loss: 0.2123, train acc: 0.9127, val loss: 0.1446, val acc: 0.9518  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1486  @ epoch 39661 )\n",
      "[Epoch: 39920] train loss: 0.1837, train acc: 0.9267, val loss: 0.1448, val acc: 0.9514  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1486  @ epoch 39661 )\n",
      "[Epoch: 39940] train loss: 0.1921, train acc: 0.9268, val loss: 0.1433, val acc: 0.9518  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1486  @ epoch 39661 )\n",
      "[Epoch: 39960] train loss: 0.1710, train acc: 0.9357, val loss: 0.1591, val acc: 0.9450  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1486  @ epoch 39661 )\n",
      "[Epoch: 39980] train loss: 0.1696, train acc: 0.9336, val loss: 0.1384, val acc: 0.9538  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1486  @ epoch 39661 )\n",
      "[Epoch: 40000] train loss: 0.2031, train acc: 0.9182, val loss: 0.1468, val acc: 0.9521  (best train acc: 0.9481, best val acc: 0.9622, best train loss: 0.1486  @ epoch 39661 )\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.99       154\n",
      "           1       0.93      0.97      0.95       703\n",
      "           2       0.89      0.89      0.89       702\n",
      "           3       0.96      0.90      0.93       703\n",
      "           4       0.99      1.00      0.99       702\n",
      "\n",
      "    accuracy                           0.94      2964\n",
      "   macro avg       0.95      0.95      0.95      2964\n",
      "weighted avg       0.94      0.94      0.94      2964\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 504x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 504x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "gnn_model = GNN7L_Sage(data_with_nedbit).to(device)\n",
    "pred = train(gnn_model, data_with_nedbit, epochs, lr, weight_decay, classes, 'GraphSAGE')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6aeea6fe202f5c7201d5940e4573c0a76b23e4e16f0e3784ac81597546f2b3b3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
