{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CreateDataset import get_dataset_from_graph\n",
    "from CreateGraph import create_graph_from_PPI\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn.conv import SAGEConv\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Reading PPI...ok\n",
      "[+] Creating the graph...ok\n",
      "\t[+] Added 19764 nodes\n",
      "\t[+] Added 682198 edges\n",
      "[+] Removing self loops...ok\n",
      "\t[+] 19764 nodes\n",
      "\t[+] 678932 edges\n",
      "[+] Taking the LCC...ok\n",
      "\t[+] 19761 nodes\n",
      "\t[+] 678932 edges\n",
      "[+] Adding NeDBIT features...ok\n",
      "[+] Saving graph to path: Graphs/grafo_prova_v2.gml\n",
      "[i] Elapsed time: 48.04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Graphs/grafo_prova_v2.gml'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_path = create_graph_from_PPI('Datasets/BIOGRID-ORGANISM-Homo_sapiens-4.4.206.tab3.txt', 'C0006142', 'grafo_prova_v2')\n",
    "graph_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Reading graph...ok\n",
      "[+] Creating dataset...ok\n",
      "[i] Elapsed time: 36.049\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Data(edge_index=[2, 1357864], x=[19761, 6], y=[19761], num_classes=5, train_mask=[19761], test_mask=[19761], val_mask=[19761])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = get_dataset_from_graph(graph_path, 'C0006142')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN7L_Sage (nn.Module):\n",
    "    def __init__(self, data):\n",
    "        super().__init__()\n",
    "        self.conv1 = SAGEConv(data.num_features, 16, aggr='max')\n",
    "        self.conv2 = SAGEConv(16, 16, aggr='max')\n",
    "        self.conv3 = SAGEConv(16, 16, aggr='max')\n",
    "        self.conv4 = SAGEConv(16, 16, aggr='max')\n",
    "        self.conv5 = SAGEConv(16, 16, aggr='max')\n",
    "        self.conv6 = SAGEConv(16, 16, aggr='max')\n",
    "        self.conv7 = SAGEConv(16, int(data.num_classes), aggr='max')\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        x = F.relu(self.conv3(x, edge_index))\n",
    "        x = F.relu(self.conv4(x, edge_index))\n",
    "        x = F.relu(self.conv5(x, edge_index))\n",
    "        x = F.relu(self.conv6(x, edge_index))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv7(x, edge_index)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data, epochs, lr, weight_decay, classes, model_name):    \n",
    "    data = data.to(device)\n",
    "\n",
    "    title = model_name + '_' + str(epochs) + '_' + str(weight_decay).replace('.', '_')\n",
    "\n",
    "    model_path  = 'Models/' + title\n",
    "    image_path  = 'Images/' + title\n",
    "    report_path = 'Reports/' + title + '.csv'\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    # optimizer = torch.optim.RMSprop(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    train_mask  = data['train_mask']\n",
    "    test_mask   = data['test_mask']\n",
    "    val_mask    = data['val_mask']\n",
    "\n",
    "    labels = data.y\n",
    "    output = ''\n",
    "\n",
    "    # list to plot the train accuracy\n",
    "    train_acc_curve = []\n",
    "    train_lss_curve = []\n",
    "\n",
    "    best_train_acc  = 0\n",
    "    best_val_acc    = 0\n",
    "    best_train_lss  = 999\n",
    "    best_loss_epoch = 0\n",
    "\n",
    "    for e in tqdm(range(epochs+1)):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        logits      = model(data)\n",
    "        output      = logits.argmax(1)\n",
    "        #Â train_loss  = F.cross_entropy(logits[train_mask], labels[train_mask])\n",
    "        train_loss  = F.nll_loss(logits[train_mask], labels[train_mask])\n",
    "        train_acc   = (output[train_mask] == labels[train_mask]).float().mean()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Append train acc. to plot curve later\n",
    "        train_acc_curve.append(train_acc.item())\n",
    "        train_lss_curve.append(train_loss.item())\n",
    "\n",
    "        if train_acc > best_train_acc:\n",
    "            best_train_acc = train_acc\n",
    "\n",
    "        # Evaluation and test\n",
    "        model.eval()\n",
    "        logits      = model(data)\n",
    "        output      = logits.argmax(1)\n",
    "        val_loss    = F.nll_loss(logits[val_mask], labels[val_mask])\n",
    "        val_acc     = (output[val_mask] == labels[val_mask]).float().mean()\n",
    "\n",
    "        # Update best test/val acc.\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "        \n",
    "        # Save model with best train loss\n",
    "        if train_loss < best_train_lss:\n",
    "            best_train_lss = train_loss\n",
    "            best_loss_epoch = e\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "\n",
    "        if e % 20 == 0 or e == epochs:\n",
    "            print('[Epoch: {:04d}]'.format(e),\n",
    "            'train loss: {:.4f},'.format(train_loss.item()),\n",
    "            'train acc: {:.4f},'.format(train_acc.item()),\n",
    "            'val loss: {:.4f},'.format(val_loss.item()),\n",
    "            'val acc: {:.4f} '.format(val_acc.item()),\n",
    "            '(best train acc: {:.4f},'.format(best_train_acc.item()),\n",
    "            'best val acc: {:.4f},'.format(best_val_acc.item()),\n",
    "            'best train loss: {:.4f} '.format(best_train_lss),\n",
    "            '@ epoch', best_loss_epoch ,')')\n",
    "    \n",
    "    # Plot training accuracy curve\n",
    "    plt.figure(figsize = (12,7))\n",
    "    plt.plot(train_acc_curve)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Train Accuracy')\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize = (12,7))\n",
    "    plt.plot(train_lss_curve)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Train Loss')\n",
    "    plt.show()\n",
    "\n",
    "    # Load best model\n",
    "    loaded_model = GNN7L_Sage(data)\n",
    "    \n",
    "    loaded_model = loaded_model.to(device)\n",
    "    loaded_model.load_state_dict(torch.load(model_path))\n",
    "    loaded_model.eval()\n",
    "    logits = loaded_model(data)\n",
    "    output = logits.argmax(1)\n",
    "\n",
    "    print(classification_report(labels[test_mask].to('cpu'), output[test_mask].to('cpu')))\n",
    "\n",
    "    class_report = classification_report(labels[test_mask].to('cpu'), output[test_mask].to('cpu'), output_dict=True)\n",
    "    classification_report_dataframe = pd.DataFrame(class_report)\n",
    "    classification_report_dataframe.to_csv(report_path)\n",
    "\n",
    "    #Confusion Matrix\n",
    "    norms = [None, \"true\"]\n",
    "    for norm in norms:\n",
    "        cm = confusion_matrix(labels[test_mask].to('cpu'), output[test_mask].to('cpu'), normalize=norm)\n",
    "\n",
    "        plt.figure(figsize=(7,7))\n",
    "        \n",
    "        if norm == \"true\":\n",
    "            sn.heatmap(cm, annot=True, fmt=\".3f\", linewidths=.5, square = True, cmap = 'BuPu', xticklabels = classes, yticklabels = classes)\n",
    "        else:\n",
    "            sn.heatmap(cm, annot=True, fmt=\".0f\", linewidths=.5, square = True, cmap = 'BuPu', xticklabels = classes, yticklabels = classes)\n",
    "        plt.title(model_name)\n",
    "        plt.ylabel('Actual label')\n",
    "        plt.xlabel('Predicted label')\n",
    "\n",
    "        if norm == None:\n",
    "            plt.savefig(image_path + '_notNorm.png')\n",
    "        else:\n",
    "            plt.savefig(image_path + '_Norm.png')\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4813bb972b8f411d9faf4651a0f60fec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40001 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch: 0000] train loss: 288.6915, train acc: 0.2379, val loss: 135.1217, val acc: 0.2368  (best train acc: 0.2379, best val acc: 0.2368, best train loss: 288.6915  @ epoch 0 )\n",
      "[Epoch: 0020] train loss: 16.5392, train acc: 0.2360, val loss: 5.0993, val acc: 0.4223  (best train acc: 0.2632, best val acc: 0.4334, best train loss: 16.5392  @ epoch 20 )\n",
      "[Epoch: 0040] train loss: 4.2565, train acc: 0.2787, val loss: 1.7209, val acc: 0.3160  (best train acc: 0.3022, best val acc: 0.4334, best train loss: 4.2565  @ epoch 40 )\n",
      "[Epoch: 0060] train loss: 2.0474, train acc: 0.2684, val loss: 1.6103, val acc: 0.2897  (best train acc: 0.3035, best val acc: 0.4334, best train loss: 2.0474  @ epoch 60 )\n",
      "[Epoch: 0080] train loss: 1.6139, train acc: 0.2619, val loss: 1.4941, val acc: 0.3066  (best train acc: 0.3035, best val acc: 0.4334, best train loss: 1.6139  @ epoch 80 )\n",
      "[Epoch: 0100] train loss: 1.5387, train acc: 0.2902, val loss: 1.4774, val acc: 0.3086  (best train acc: 0.3035, best val acc: 0.4334, best train loss: 1.5314  @ epoch 98 )\n",
      "[Epoch: 0120] train loss: 1.4988, train acc: 0.2986, val loss: 1.4537, val acc: 0.3187  (best train acc: 0.3064, best val acc: 0.4334, best train loss: 1.4907  @ epoch 116 )\n",
      "[Epoch: 0140] train loss: 1.4618, train acc: 0.3076, val loss: 1.4304, val acc: 0.3447  (best train acc: 0.3086, best val acc: 0.4334, best train loss: 1.4618  @ epoch 140 )\n",
      "[Epoch: 0160] train loss: 1.4607, train acc: 0.3091, val loss: 1.4065, val acc: 0.3605  (best train acc: 0.3133, best val acc: 0.4334, best train loss: 1.4546  @ epoch 152 )\n",
      "[Epoch: 0180] train loss: 1.4366, train acc: 0.3163, val loss: 1.3955, val acc: 0.3686  (best train acc: 0.3269, best val acc: 0.4334, best train loss: 1.4319  @ epoch 172 )\n",
      "[Epoch: 0200] train loss: 1.4270, train acc: 0.3235, val loss: 1.3821, val acc: 0.3798  (best train acc: 0.3269, best val acc: 0.4334, best train loss: 1.4270  @ epoch 200 )\n",
      "[Epoch: 0220] train loss: 1.4110, train acc: 0.3224, val loss: 1.3710, val acc: 0.3922  (best train acc: 0.3330, best val acc: 0.4334, best train loss: 1.4042  @ epoch 216 )\n",
      "[Epoch: 0240] train loss: 1.4034, train acc: 0.3156, val loss: 1.3561, val acc: 0.3973  (best train acc: 0.3376, best val acc: 0.4334, best train loss: 1.3923  @ epoch 238 )\n",
      "[Epoch: 0260] train loss: 1.3829, train acc: 0.3379, val loss: 1.3422, val acc: 0.4125  (best train acc: 0.3440, best val acc: 0.4334, best train loss: 1.3829  @ epoch 260 )\n",
      "[Epoch: 0280] train loss: 1.3839, train acc: 0.3803, val loss: 1.3306, val acc: 0.4293  (best train acc: 0.3933, best val acc: 0.4334, best train loss: 1.3772  @ epoch 267 )\n",
      "[Epoch: 0300] train loss: 1.3651, train acc: 0.3966, val loss: 1.3168, val acc: 0.4273  (best train acc: 0.4014, best val acc: 0.4358, best train loss: 1.3593  @ epoch 296 )\n",
      "[Epoch: 0320] train loss: 1.3671, train acc: 0.3900, val loss: 1.3115, val acc: 0.4506  (best train acc: 0.4082, best val acc: 0.4526, best train loss: 1.3529  @ epoch 304 )\n",
      "[Epoch: 0340] train loss: 1.3503, train acc: 0.4086, val loss: 1.2941, val acc: 0.4543  (best train acc: 0.4113, best val acc: 0.4560, best train loss: 1.3414  @ epoch 334 )\n",
      "[Epoch: 0360] train loss: 1.3320, train acc: 0.4179, val loss: 1.2891, val acc: 0.4728  (best train acc: 0.4199, best val acc: 0.4728, best train loss: 1.3300  @ epoch 347 )\n",
      "[Epoch: 0380] train loss: 1.3339, train acc: 0.4235, val loss: 1.2742, val acc: 0.4712  (best train acc: 0.4256, best val acc: 0.4803, best train loss: 1.3190  @ epoch 378 )\n",
      "[Epoch: 0400] train loss: 1.3219, train acc: 0.4237, val loss: 1.2608, val acc: 0.4944  (best train acc: 0.4339, best val acc: 0.4988, best train loss: 1.3118  @ epoch 392 )\n",
      "[Epoch: 0420] train loss: 1.3126, train acc: 0.4217, val loss: 1.2511, val acc: 0.5089  (best train acc: 0.4414, best val acc: 0.5089, best train loss: 1.2944  @ epoch 415 )\n",
      "[Epoch: 0440] train loss: 1.2921, train acc: 0.4379, val loss: 1.2201, val acc: 0.5035  (best train acc: 0.4568, best val acc: 0.5191, best train loss: 1.2743  @ epoch 434 )\n",
      "[Epoch: 0460] train loss: 1.2640, train acc: 0.4573, val loss: 1.1974, val acc: 0.5248  (best train acc: 0.4577, best val acc: 0.5288, best train loss: 1.2640  @ epoch 460 )\n",
      "[Epoch: 0480] train loss: 1.2453, train acc: 0.4560, val loss: 1.1675, val acc: 0.5393  (best train acc: 0.4716, best val acc: 0.5413, best train loss: 1.2424  @ epoch 475 )\n",
      "[Epoch: 0500] train loss: 1.2199, train acc: 0.4725, val loss: 1.1469, val acc: 0.5578  (best train acc: 0.4787, best val acc: 0.5609, best train loss: 1.2152  @ epoch 498 )\n",
      "[Epoch: 0520] train loss: 1.2202, train acc: 0.4643, val loss: 1.1114, val acc: 0.5757  (best train acc: 0.4928, best val acc: 0.5858, best train loss: 1.1826  @ epoch 516 )\n",
      "[Epoch: 0540] train loss: 1.1495, train acc: 0.5074, val loss: 1.0834, val acc: 0.5808  (best train acc: 0.5118, best val acc: 0.5943, best train loss: 1.1495  @ epoch 540 )\n",
      "[Epoch: 0560] train loss: 1.1442, train acc: 0.5184, val loss: 1.0208, val acc: 0.5973  (best train acc: 0.5495, best val acc: 0.6135, best train loss: 1.1060  @ epoch 557 )\n",
      "[Epoch: 0580] train loss: 1.0251, train acc: 0.5772, val loss: 0.9750, val acc: 0.6250  (best train acc: 0.5836, best val acc: 0.6486, best train loss: 1.0251  @ epoch 580 )\n",
      "[Epoch: 0600] train loss: 1.0464, train acc: 0.5714, val loss: 0.9222, val acc: 0.6472  (best train acc: 0.5934, best val acc: 0.6604, best train loss: 1.0011  @ epoch 595 )\n",
      "[Epoch: 0620] train loss: 1.0771, train acc: 0.5453, val loss: 0.8934, val acc: 0.6408  (best train acc: 0.5964, best val acc: 0.6691, best train loss: 0.9784  @ epoch 613 )\n",
      "[Epoch: 0640] train loss: 1.0108, train acc: 0.5665, val loss: 0.8676, val acc: 0.6759  (best train acc: 0.6225, best val acc: 0.6880, best train loss: 0.9197  @ epoch 633 )\n",
      "[Epoch: 0660] train loss: 1.0027, train acc: 0.5794, val loss: 0.8485, val acc: 0.6944  (best train acc: 0.6225, best val acc: 0.6951, best train loss: 0.9197  @ epoch 633 )\n",
      "[Epoch: 0680] train loss: 1.0282, train acc: 0.5631, val loss: 0.8241, val acc: 0.6954  (best train acc: 0.6225, best val acc: 0.7008, best train loss: 0.9197  @ epoch 633 )\n",
      "[Epoch: 0700] train loss: 1.0222, train acc: 0.5824, val loss: 0.8246, val acc: 0.6809  (best train acc: 0.6355, best val acc: 0.7025, best train loss: 0.8829  @ epoch 682 )\n",
      "[Epoch: 0720] train loss: 1.0483, train acc: 0.5588, val loss: 0.8056, val acc: 0.7019  (best train acc: 0.6408, best val acc: 0.7073, best train loss: 0.8829  @ epoch 682 )\n",
      "[Epoch: 0740] train loss: 0.9990, train acc: 0.5821, val loss: 0.8025, val acc: 0.6863  (best train acc: 0.6589, best val acc: 0.7153, best train loss: 0.8406  @ epoch 734 )\n",
      "[Epoch: 0760] train loss: 0.9365, train acc: 0.6097, val loss: 0.7724, val acc: 0.7140  (best train acc: 0.6614, best val acc: 0.7177, best train loss: 0.8226  @ epoch 753 )\n",
      "[Epoch: 0780] train loss: 1.0040, train acc: 0.5911, val loss: 0.7593, val acc: 0.7234  (best train acc: 0.6689, best val acc: 0.7238, best train loss: 0.8226  @ epoch 753 )\n",
      "[Epoch: 0800] train loss: 1.0201, train acc: 0.5700, val loss: 0.7867, val acc: 0.6836  (best train acc: 0.6852, best val acc: 0.7285, best train loss: 0.8202  @ epoch 782 )\n",
      "[Epoch: 0820] train loss: 0.9016, train acc: 0.6241, val loss: 0.7473, val acc: 0.7265  (best train acc: 0.6852, best val acc: 0.7329, best train loss: 0.8202  @ epoch 782 )\n",
      "[Epoch: 0840] train loss: 0.9063, train acc: 0.6348, val loss: 0.7317, val acc: 0.7315  (best train acc: 0.6852, best val acc: 0.7342, best train loss: 0.7886  @ epoch 836 )\n",
      "[Epoch: 0860] train loss: 0.7966, train acc: 0.6815, val loss: 0.7319, val acc: 0.7322  (best train acc: 0.6859, best val acc: 0.7433, best train loss: 0.7886  @ epoch 836 )\n",
      "[Epoch: 0880] train loss: 0.8809, train acc: 0.6518, val loss: 0.7240, val acc: 0.7369  (best train acc: 0.6946, best val acc: 0.7450, best train loss: 0.7739  @ epoch 872 )\n",
      "[Epoch: 0900] train loss: 0.9514, train acc: 0.6165, val loss: 0.7290, val acc: 0.7234  (best train acc: 0.6946, best val acc: 0.7484, best train loss: 0.7739  @ epoch 872 )\n",
      "[Epoch: 0920] train loss: 0.8152, train acc: 0.6664, val loss: 0.7101, val acc: 0.7477  (best train acc: 0.6986, best val acc: 0.7484, best train loss: 0.7739  @ epoch 872 )\n",
      "[Epoch: 0940] train loss: 0.8275, train acc: 0.6814, val loss: 0.6851, val acc: 0.7528  (best train acc: 0.7053, best val acc: 0.7531, best train loss: 0.7678  @ epoch 937 )\n",
      "[Epoch: 0960] train loss: 0.8300, train acc: 0.6825, val loss: 0.6866, val acc: 0.7481  (best train acc: 0.7266, best val acc: 0.7568, best train loss: 0.7395  @ epoch 956 )\n",
      "[Epoch: 0980] train loss: 0.8547, train acc: 0.6387, val loss: 0.6986, val acc: 0.7386  (best train acc: 0.7266, best val acc: 0.7693, best train loss: 0.7331  @ epoch 975 )\n",
      "[Epoch: 1000] train loss: 0.7967, train acc: 0.7005, val loss: 0.6747, val acc: 0.7764  (best train acc: 0.7308, best val acc: 0.7798, best train loss: 0.7204  @ epoch 987 )\n",
      "[Epoch: 1020] train loss: 0.7416, train acc: 0.7250, val loss: 0.6452, val acc: 0.7680  (best train acc: 0.7361, best val acc: 0.7798, best train loss: 0.6986  @ epoch 1003 )\n",
      "[Epoch: 1040] train loss: 0.7584, train acc: 0.7188, val loss: 0.6353, val acc: 0.7612  (best train acc: 0.7446, best val acc: 0.7798, best train loss: 0.6986  @ epoch 1003 )\n",
      "[Epoch: 1060] train loss: 0.7772, train acc: 0.6968, val loss: 0.6366, val acc: 0.7734  (best train acc: 0.7507, best val acc: 0.7875, best train loss: 0.6799  @ epoch 1051 )\n",
      "[Epoch: 1080] train loss: 0.6790, train acc: 0.7432, val loss: 0.6186, val acc: 0.7899  (best train acc: 0.7507, best val acc: 0.7899, best train loss: 0.6790  @ epoch 1080 )\n",
      "[Epoch: 1100] train loss: 0.7499, train acc: 0.7124, val loss: 0.6126, val acc: 0.7855  (best train acc: 0.7551, best val acc: 0.7899, best train loss: 0.6742  @ epoch 1091 )\n",
      "[Epoch: 1120] train loss: 0.7588, train acc: 0.7105, val loss: 0.6324, val acc: 0.7545  (best train acc: 0.7551, best val acc: 0.7926, best train loss: 0.6742  @ epoch 1091 )\n",
      "[Epoch: 1140] train loss: 0.7349, train acc: 0.7210, val loss: 0.6091, val acc: 0.8020  (best train acc: 0.7551, best val acc: 0.8020, best train loss: 0.6722  @ epoch 1126 )\n",
      "[Epoch: 1160] train loss: 0.7687, train acc: 0.7043, val loss: 0.6259, val acc: 0.7707  (best train acc: 0.7653, best val acc: 0.8020, best train loss: 0.6378  @ epoch 1145 )\n",
      "[Epoch: 1180] train loss: 0.6681, train acc: 0.7410, val loss: 0.6084, val acc: 0.7504  (best train acc: 0.7653, best val acc: 0.8020, best train loss: 0.6378  @ epoch 1145 )\n",
      "[Epoch: 1200] train loss: 0.6306, train acc: 0.7773, val loss: 0.5609, val acc: 0.8010  (best train acc: 0.7773, best val acc: 0.8152, best train loss: 0.6158  @ epoch 1196 )\n",
      "[Epoch: 1220] train loss: 0.6536, train acc: 0.7420, val loss: 0.5496, val acc: 0.7858  (best train acc: 0.7773, best val acc: 0.8314, best train loss: 0.6158  @ epoch 1196 )\n",
      "[Epoch: 1240] train loss: 0.6198, train acc: 0.7634, val loss: 0.6348, val acc: 0.7133  (best train acc: 0.7773, best val acc: 0.8314, best train loss: 0.6158  @ epoch 1196 )\n",
      "[Epoch: 1260] train loss: 0.6787, train acc: 0.7392, val loss: 0.6084, val acc: 0.7558  (best train acc: 0.7859, best val acc: 0.8314, best train loss: 0.5820  @ epoch 1244 )\n",
      "[Epoch: 1280] train loss: 0.6600, train acc: 0.7432, val loss: 0.5488, val acc: 0.8034  (best train acc: 0.7859, best val acc: 0.8314, best train loss: 0.5820  @ epoch 1244 )\n",
      "[Epoch: 1300] train loss: 0.6047, train acc: 0.7674, val loss: 0.5464, val acc: 0.8162  (best train acc: 0.7859, best val acc: 0.8347, best train loss: 0.5820  @ epoch 1244 )\n",
      "[Epoch: 1320] train loss: 0.5707, train acc: 0.7869, val loss: 0.5170, val acc: 0.8239  (best train acc: 0.7948, best val acc: 0.8347, best train loss: 0.5591  @ epoch 1302 )\n",
      "[Epoch: 1340] train loss: 0.5970, train acc: 0.7749, val loss: 0.5437, val acc: 0.7997  (best train acc: 0.7948, best val acc: 0.8347, best train loss: 0.5591  @ epoch 1302 )\n",
      "[Epoch: 1360] train loss: 0.5957, train acc: 0.7707, val loss: 0.5237, val acc: 0.8337  (best train acc: 0.7992, best val acc: 0.8428, best train loss: 0.5580  @ epoch 1358 )\n",
      "[Epoch: 1380] train loss: 0.5913, train acc: 0.7764, val loss: 0.5538, val acc: 0.7838  (best train acc: 0.7992, best val acc: 0.8597, best train loss: 0.5332  @ epoch 1372 )\n",
      "[Epoch: 1400] train loss: 0.7014, train acc: 0.7152, val loss: 0.5117, val acc: 0.8172  (best train acc: 0.7992, best val acc: 0.8597, best train loss: 0.5332  @ epoch 1372 )\n",
      "[Epoch: 1420] train loss: 0.7282, train acc: 0.7018, val loss: 0.5111, val acc: 0.8405  (best train acc: 0.7992, best val acc: 0.8597, best train loss: 0.5332  @ epoch 1372 )\n",
      "[Epoch: 1440] train loss: 0.6653, train acc: 0.7405, val loss: 0.5496, val acc: 0.7845  (best train acc: 0.7992, best val acc: 0.8597, best train loss: 0.5332  @ epoch 1372 )\n",
      "[Epoch: 1460] train loss: 0.6380, train acc: 0.7465, val loss: 0.5177, val acc: 0.8300  (best train acc: 0.8102, best val acc: 0.8597, best train loss: 0.5332  @ epoch 1372 )\n",
      "[Epoch: 1480] train loss: 0.6176, train acc: 0.7353, val loss: 0.4980, val acc: 0.8411  (best train acc: 0.8102, best val acc: 0.8597, best train loss: 0.5292  @ epoch 1463 )\n",
      "[Epoch: 1500] train loss: 0.5830, train acc: 0.7801, val loss: 0.5085, val acc: 0.8169  (best train acc: 0.8102, best val acc: 0.8597, best train loss: 0.5292  @ epoch 1463 )\n",
      "[Epoch: 1520] train loss: 0.6259, train acc: 0.7593, val loss: 0.4937, val acc: 0.8469  (best train acc: 0.8107, best val acc: 0.8658, best train loss: 0.5250  @ epoch 1517 )\n",
      "[Epoch: 1540] train loss: 0.5279, train acc: 0.8005, val loss: 0.4787, val acc: 0.8546  (best train acc: 0.8107, best val acc: 0.8658, best train loss: 0.5250  @ epoch 1517 )\n",
      "[Epoch: 1560] train loss: 0.5312, train acc: 0.8016, val loss: 0.4850, val acc: 0.8334  (best train acc: 0.8107, best val acc: 0.8658, best train loss: 0.5191  @ epoch 1542 )\n",
      "[Epoch: 1580] train loss: 0.5985, train acc: 0.7736, val loss: 0.4700, val acc: 0.8516  (best train acc: 0.8107, best val acc: 0.8658, best train loss: 0.5191  @ epoch 1542 )\n",
      "[Epoch: 1600] train loss: 0.6410, train acc: 0.7615, val loss: 0.5486, val acc: 0.7734  (best train acc: 0.8107, best val acc: 0.8658, best train loss: 0.5191  @ epoch 1542 )\n",
      "[Epoch: 1620] train loss: 0.5431, train acc: 0.7898, val loss: 0.4736, val acc: 0.8476  (best train acc: 0.8107, best val acc: 0.8658, best train loss: 0.5191  @ epoch 1542 )\n",
      "[Epoch: 1640] train loss: 0.6580, train acc: 0.7424, val loss: 0.5529, val acc: 0.7609  (best train acc: 0.8107, best val acc: 0.8658, best train loss: 0.5109  @ epoch 1622 )\n",
      "[Epoch: 1660] train loss: 0.6303, train acc: 0.7538, val loss: 0.5121, val acc: 0.8121  (best train acc: 0.8107, best val acc: 0.8658, best train loss: 0.5109  @ epoch 1622 )\n",
      "[Epoch: 1680] train loss: 0.6296, train acc: 0.7545, val loss: 0.5117, val acc: 0.8135  (best train acc: 0.8107, best val acc: 0.8658, best train loss: 0.5109  @ epoch 1622 )\n",
      "[Epoch: 1700] train loss: 0.5797, train acc: 0.7879, val loss: 0.4906, val acc: 0.8428  (best train acc: 0.8107, best val acc: 0.8658, best train loss: 0.5109  @ epoch 1622 )\n",
      "[Epoch: 1720] train loss: 0.6465, train acc: 0.7568, val loss: 0.4608, val acc: 0.8594  (best train acc: 0.8107, best val acc: 0.8658, best train loss: 0.5109  @ epoch 1622 )\n",
      "[Epoch: 1740] train loss: 0.5063, train acc: 0.8154, val loss: 0.4656, val acc: 0.8597  (best train acc: 0.8162, best val acc: 0.8658, best train loss: 0.5063  @ epoch 1740 )\n",
      "[Epoch: 1760] train loss: 0.5822, train acc: 0.7669, val loss: 0.4695, val acc: 0.8479  (best train acc: 0.8331, best val acc: 0.8658, best train loss: 0.4881  @ epoch 1754 )\n",
      "[Epoch: 1780] train loss: 0.5381, train acc: 0.8080, val loss: 0.4614, val acc: 0.8604  (best train acc: 0.8331, best val acc: 0.8658, best train loss: 0.4881  @ epoch 1754 )\n",
      "[Epoch: 1800] train loss: 0.5668, train acc: 0.7842, val loss: 0.4950, val acc: 0.8206  (best train acc: 0.8331, best val acc: 0.8685, best train loss: 0.4881  @ epoch 1754 )\n",
      "[Epoch: 1820] train loss: 0.6138, train acc: 0.7624, val loss: 0.4715, val acc: 0.8455  (best train acc: 0.8331, best val acc: 0.8685, best train loss: 0.4881  @ epoch 1754 )\n",
      "[Epoch: 1840] train loss: 0.7082, train acc: 0.7308, val loss: 0.4789, val acc: 0.8374  (best train acc: 0.8331, best val acc: 0.8685, best train loss: 0.4881  @ epoch 1754 )\n",
      "[Epoch: 1860] train loss: 0.5831, train acc: 0.7748, val loss: 0.4730, val acc: 0.8395  (best train acc: 0.8331, best val acc: 0.8685, best train loss: 0.4881  @ epoch 1754 )\n",
      "[Epoch: 1880] train loss: 0.5978, train acc: 0.7752, val loss: 0.4597, val acc: 0.8631  (best train acc: 0.8331, best val acc: 0.8685, best train loss: 0.4881  @ epoch 1754 )\n",
      "[Epoch: 1900] train loss: 0.5733, train acc: 0.7828, val loss: 0.5109, val acc: 0.8111  (best train acc: 0.8331, best val acc: 0.8685, best train loss: 0.4881  @ epoch 1754 )\n",
      "[Epoch: 1920] train loss: 0.5727, train acc: 0.7825, val loss: 0.4543, val acc: 0.8661  (best train acc: 0.8331, best val acc: 0.8708, best train loss: 0.4881  @ epoch 1754 )\n",
      "[Epoch: 1940] train loss: 0.5631, train acc: 0.7754, val loss: 0.4565, val acc: 0.8435  (best train acc: 0.8331, best val acc: 0.8722, best train loss: 0.4881  @ epoch 1754 )\n",
      "[Epoch: 1960] train loss: 0.5961, train acc: 0.7807, val loss: 0.4896, val acc: 0.8250  (best train acc: 0.8331, best val acc: 0.8722, best train loss: 0.4881  @ epoch 1754 )\n",
      "[Epoch: 1980] train loss: 0.5586, train acc: 0.7833, val loss: 0.4653, val acc: 0.8610  (best train acc: 0.8331, best val acc: 0.8722, best train loss: 0.4881  @ epoch 1754 )\n",
      "[Epoch: 2000] train loss: 0.5041, train acc: 0.8171, val loss: 0.4494, val acc: 0.8715  (best train acc: 0.8331, best val acc: 0.8722, best train loss: 0.4881  @ epoch 1754 )\n",
      "[Epoch: 2020] train loss: 0.5380, train acc: 0.7997, val loss: 0.4551, val acc: 0.8580  (best train acc: 0.8331, best val acc: 0.8722, best train loss: 0.4881  @ epoch 1754 )\n",
      "[Epoch: 2040] train loss: 0.5882, train acc: 0.7721, val loss: 0.4607, val acc: 0.8543  (best train acc: 0.8331, best val acc: 0.8722, best train loss: 0.4881  @ epoch 1754 )\n",
      "[Epoch: 2060] train loss: 0.5395, train acc: 0.7921, val loss: 0.4827, val acc: 0.8331  (best train acc: 0.8331, best val acc: 0.8755, best train loss: 0.4881  @ epoch 1754 )\n",
      "[Epoch: 2080] train loss: 0.5457, train acc: 0.7899, val loss: 0.4491, val acc: 0.8651  (best train acc: 0.8331, best val acc: 0.8755, best train loss: 0.4881  @ epoch 1754 )\n",
      "[Epoch: 2100] train loss: 0.5638, train acc: 0.7658, val loss: 0.4571, val acc: 0.8651  (best train acc: 0.8331, best val acc: 0.8755, best train loss: 0.4881  @ epoch 1754 )\n",
      "[Epoch: 2120] train loss: 0.4982, train acc: 0.8209, val loss: 0.4602, val acc: 0.8519  (best train acc: 0.8331, best val acc: 0.8755, best train loss: 0.4881  @ epoch 1754 )\n",
      "[Epoch: 2140] train loss: 0.4867, train acc: 0.8229, val loss: 0.4460, val acc: 0.8718  (best train acc: 0.8331, best val acc: 0.8755, best train loss: 0.4867  @ epoch 2140 )\n",
      "[Epoch: 2160] train loss: 0.5004, train acc: 0.8145, val loss: 0.4552, val acc: 0.8489  (best train acc: 0.8331, best val acc: 0.8766, best train loss: 0.4822  @ epoch 2141 )\n",
      "[Epoch: 2180] train loss: 0.5422, train acc: 0.8024, val loss: 0.4432, val acc: 0.8762  (best train acc: 0.8331, best val acc: 0.8766, best train loss: 0.4822  @ epoch 2141 )\n",
      "[Epoch: 2200] train loss: 0.5983, train acc: 0.7807, val loss: 0.4326, val acc: 0.8799  (best train acc: 0.8331, best val acc: 0.8799, best train loss: 0.4819  @ epoch 2199 )\n",
      "[Epoch: 2220] train loss: 0.5249, train acc: 0.7956, val loss: 0.4440, val acc: 0.8661  (best train acc: 0.8331, best val acc: 0.8799, best train loss: 0.4743  @ epoch 2214 )\n",
      "[Epoch: 2240] train loss: 0.5222, train acc: 0.8143, val loss: 0.4330, val acc: 0.8718  (best train acc: 0.8331, best val acc: 0.8799, best train loss: 0.4743  @ epoch 2214 )\n",
      "[Epoch: 2260] train loss: 0.4949, train acc: 0.8203, val loss: 0.4350, val acc: 0.8678  (best train acc: 0.8331, best val acc: 0.8799, best train loss: 0.4743  @ epoch 2214 )\n",
      "[Epoch: 2280] train loss: 0.4836, train acc: 0.8281, val loss: 0.4361, val acc: 0.8755  (best train acc: 0.8331, best val acc: 0.8799, best train loss: 0.4743  @ epoch 2214 )\n",
      "[Epoch: 2300] train loss: 0.4980, train acc: 0.8143, val loss: 0.4328, val acc: 0.8806  (best train acc: 0.8331, best val acc: 0.8806, best train loss: 0.4743  @ epoch 2214 )\n",
      "[Epoch: 2320] train loss: 0.5737, train acc: 0.7740, val loss: 0.4461, val acc: 0.8651  (best train acc: 0.8331, best val acc: 0.8806, best train loss: 0.4735  @ epoch 2313 )\n",
      "[Epoch: 2340] train loss: 0.5786, train acc: 0.7853, val loss: 0.4748, val acc: 0.8277  (best train acc: 0.8331, best val acc: 0.8806, best train loss: 0.4735  @ epoch 2313 )\n",
      "[Epoch: 2360] train loss: 0.5232, train acc: 0.8088, val loss: 0.4301, val acc: 0.8728  (best train acc: 0.8356, best val acc: 0.8806, best train loss: 0.4735  @ epoch 2313 )\n",
      "[Epoch: 2380] train loss: 0.5461, train acc: 0.7899, val loss: 0.4500, val acc: 0.8600  (best train acc: 0.8356, best val acc: 0.8806, best train loss: 0.4735  @ epoch 2313 )\n",
      "[Epoch: 2400] train loss: 0.5458, train acc: 0.7934, val loss: 0.4556, val acc: 0.8469  (best train acc: 0.8464, best val acc: 0.8823, best train loss: 0.4562  @ epoch 2383 )\n",
      "[Epoch: 2420] train loss: 0.5094, train acc: 0.8000, val loss: 0.4342, val acc: 0.8793  (best train acc: 0.8464, best val acc: 0.8823, best train loss: 0.4562  @ epoch 2383 )\n",
      "[Epoch: 2440] train loss: 0.4679, train acc: 0.8386, val loss: 0.4348, val acc: 0.8648  (best train acc: 0.8464, best val acc: 0.8823, best train loss: 0.4562  @ epoch 2383 )\n",
      "[Epoch: 2460] train loss: 0.5542, train acc: 0.8007, val loss: 0.4333, val acc: 0.8725  (best train acc: 0.8464, best val acc: 0.8823, best train loss: 0.4562  @ epoch 2383 )\n",
      "[Epoch: 2480] train loss: 0.5571, train acc: 0.7762, val loss: 0.4456, val acc: 0.8621  (best train acc: 0.8464, best val acc: 0.8823, best train loss: 0.4562  @ epoch 2383 )\n",
      "[Epoch: 2500] train loss: 0.4755, train acc: 0.8334, val loss: 0.4382, val acc: 0.8691  (best train acc: 0.8464, best val acc: 0.8823, best train loss: 0.4562  @ epoch 2383 )\n",
      "[Epoch: 2520] train loss: 0.4812, train acc: 0.8143, val loss: 0.4269, val acc: 0.8759  (best train acc: 0.8464, best val acc: 0.8823, best train loss: 0.4562  @ epoch 2383 )\n",
      "[Epoch: 2540] train loss: 0.4954, train acc: 0.8245, val loss: 0.4359, val acc: 0.8769  (best train acc: 0.8464, best val acc: 0.8823, best train loss: 0.4562  @ epoch 2383 )\n",
      "[Epoch: 2560] train loss: 0.6093, train acc: 0.7593, val loss: 0.4597, val acc: 0.8462  (best train acc: 0.8464, best val acc: 0.8823, best train loss: 0.4562  @ epoch 2383 )\n",
      "[Epoch: 2580] train loss: 0.6025, train acc: 0.7580, val loss: 0.4574, val acc: 0.8533  (best train acc: 0.8464, best val acc: 0.8823, best train loss: 0.4562  @ epoch 2383 )\n",
      "[Epoch: 2600] train loss: 0.4884, train acc: 0.8251, val loss: 0.4254, val acc: 0.8755  (best train acc: 0.8464, best val acc: 0.8823, best train loss: 0.4562  @ epoch 2383 )\n",
      "[Epoch: 2620] train loss: 0.4983, train acc: 0.8176, val loss: 0.4290, val acc: 0.8772  (best train acc: 0.8464, best val acc: 0.8847, best train loss: 0.4562  @ epoch 2383 )\n",
      "[Epoch: 2640] train loss: 0.4757, train acc: 0.8383, val loss: 0.4175, val acc: 0.8820  (best train acc: 0.8464, best val acc: 0.8847, best train loss: 0.4562  @ epoch 2383 )\n",
      "[Epoch: 2660] train loss: 0.4670, train acc: 0.8337, val loss: 0.4242, val acc: 0.8779  (best train acc: 0.8464, best val acc: 0.8847, best train loss: 0.4550  @ epoch 2643 )\n",
      "[Epoch: 2680] train loss: 0.6028, train acc: 0.7679, val loss: 0.4229, val acc: 0.8772  (best train acc: 0.8464, best val acc: 0.8847, best train loss: 0.4530  @ epoch 2664 )\n",
      "[Epoch: 2700] train loss: 0.4522, train acc: 0.8399, val loss: 0.4268, val acc: 0.8732  (best train acc: 0.8464, best val acc: 0.8847, best train loss: 0.4522  @ epoch 2700 )\n",
      "[Epoch: 2720] train loss: 0.4915, train acc: 0.8169, val loss: 0.4337, val acc: 0.8695  (best train acc: 0.8464, best val acc: 0.8847, best train loss: 0.4522  @ epoch 2700 )\n",
      "[Epoch: 2740] train loss: 0.5362, train acc: 0.7943, val loss: 0.4469, val acc: 0.8516  (best train acc: 0.8464, best val acc: 0.8847, best train loss: 0.4485  @ epoch 2727 )\n",
      "[Epoch: 2760] train loss: 0.5662, train acc: 0.7792, val loss: 0.4424, val acc: 0.8583  (best train acc: 0.8464, best val acc: 0.8847, best train loss: 0.4485  @ epoch 2727 )\n",
      "[Epoch: 2780] train loss: 0.4939, train acc: 0.8104, val loss: 0.4192, val acc: 0.8755  (best train acc: 0.8464, best val acc: 0.8847, best train loss: 0.4485  @ epoch 2727 )\n",
      "[Epoch: 2800] train loss: 0.5570, train acc: 0.7841, val loss: 0.4262, val acc: 0.8705  (best train acc: 0.8464, best val acc: 0.8847, best train loss: 0.4485  @ epoch 2727 )\n",
      "[Epoch: 2820] train loss: 0.5484, train acc: 0.7734, val loss: 0.4130, val acc: 0.8766  (best train acc: 0.8464, best val acc: 0.8847, best train loss: 0.4485  @ epoch 2727 )\n",
      "[Epoch: 2840] train loss: 0.5155, train acc: 0.8073, val loss: 0.4249, val acc: 0.8728  (best train acc: 0.8464, best val acc: 0.8847, best train loss: 0.4485  @ epoch 2727 )\n",
      "[Epoch: 2860] train loss: 0.4578, train acc: 0.8430, val loss: 0.4036, val acc: 0.8840  (best train acc: 0.8464, best val acc: 0.8894, best train loss: 0.4485  @ epoch 2727 )\n",
      "[Epoch: 2880] train loss: 0.5453, train acc: 0.7983, val loss: 0.4183, val acc: 0.8850  (best train acc: 0.8464, best val acc: 0.8894, best train loss: 0.4485  @ epoch 2727 )\n",
      "[Epoch: 2900] train loss: 0.4538, train acc: 0.8418, val loss: 0.4206, val acc: 0.8732  (best train acc: 0.8464, best val acc: 0.8894, best train loss: 0.4469  @ epoch 2887 )\n",
      "[Epoch: 2920] train loss: 0.4748, train acc: 0.8316, val loss: 0.4060, val acc: 0.8803  (best train acc: 0.8464, best val acc: 0.8894, best train loss: 0.4469  @ epoch 2887 )\n",
      "[Epoch: 2940] train loss: 0.5175, train acc: 0.7964, val loss: 0.4001, val acc: 0.8823  (best train acc: 0.8464, best val acc: 0.8894, best train loss: 0.4469  @ epoch 2887 )\n",
      "[Epoch: 2960] train loss: 0.4727, train acc: 0.8197, val loss: 0.3770, val acc: 0.8850  (best train acc: 0.8503, best val acc: 0.8894, best train loss: 0.4394  @ epoch 2941 )\n",
      "[Epoch: 2980] train loss: 0.4635, train acc: 0.8329, val loss: 0.3772, val acc: 0.8823  (best train acc: 0.8503, best val acc: 0.8894, best train loss: 0.4382  @ epoch 2963 )\n",
      "[Epoch: 3000] train loss: 0.4956, train acc: 0.8002, val loss: 0.3745, val acc: 0.8786  (best train acc: 0.8503, best val acc: 0.8894, best train loss: 0.4291  @ epoch 2991 )\n",
      "[Epoch: 3020] train loss: 0.5516, train acc: 0.7932, val loss: 0.3819, val acc: 0.8826  (best train acc: 0.8503, best val acc: 0.8894, best train loss: 0.4291  @ epoch 2991 )\n",
      "[Epoch: 3040] train loss: 0.5005, train acc: 0.8102, val loss: 0.3750, val acc: 0.8870  (best train acc: 0.8503, best val acc: 0.8894, best train loss: 0.4291  @ epoch 2991 )\n",
      "[Epoch: 3060] train loss: 0.4800, train acc: 0.8178, val loss: 0.3736, val acc: 0.8847  (best train acc: 0.8503, best val acc: 0.8904, best train loss: 0.4291  @ epoch 2991 )\n",
      "[Epoch: 3080] train loss: 0.4331, train acc: 0.8407, val loss: 0.3758, val acc: 0.8901  (best train acc: 0.8503, best val acc: 0.8904, best train loss: 0.4291  @ epoch 2991 )\n",
      "[Epoch: 3100] train loss: 0.5104, train acc: 0.8026, val loss: 0.3744, val acc: 0.8857  (best train acc: 0.8503, best val acc: 0.8911, best train loss: 0.4291  @ epoch 2991 )\n",
      "[Epoch: 3120] train loss: 0.5448, train acc: 0.7949, val loss: 0.3878, val acc: 0.8772  (best train acc: 0.8503, best val acc: 0.8914, best train loss: 0.4291  @ epoch 2991 )\n",
      "[Epoch: 3140] train loss: 0.5040, train acc: 0.8173, val loss: 0.3697, val acc: 0.8914  (best train acc: 0.8503, best val acc: 0.8914, best train loss: 0.4291  @ epoch 2991 )\n",
      "[Epoch: 3160] train loss: 0.4377, train acc: 0.8474, val loss: 0.3687, val acc: 0.8762  (best train acc: 0.8503, best val acc: 0.8948, best train loss: 0.4291  @ epoch 2991 )\n",
      "[Epoch: 3180] train loss: 0.5269, train acc: 0.7989, val loss: 0.3696, val acc: 0.8897  (best train acc: 0.8503, best val acc: 0.8951, best train loss: 0.4291  @ epoch 2991 )\n",
      "[Epoch: 3200] train loss: 0.4649, train acc: 0.8312, val loss: 0.4261, val acc: 0.8378  (best train acc: 0.8503, best val acc: 0.8965, best train loss: 0.4291  @ epoch 2991 )\n",
      "[Epoch: 3220] train loss: 0.4940, train acc: 0.8092, val loss: 0.3866, val acc: 0.8644  (best train acc: 0.8503, best val acc: 0.8965, best train loss: 0.4291  @ epoch 2991 )\n",
      "[Epoch: 3240] train loss: 0.4903, train acc: 0.8169, val loss: 0.3727, val acc: 0.8776  (best train acc: 0.8503, best val acc: 0.8965, best train loss: 0.4291  @ epoch 2991 )\n",
      "[Epoch: 3260] train loss: 0.5398, train acc: 0.7896, val loss: 0.3698, val acc: 0.8870  (best train acc: 0.8503, best val acc: 0.8965, best train loss: 0.4291  @ epoch 2991 )\n",
      "[Epoch: 3280] train loss: 0.5239, train acc: 0.8038, val loss: 0.3867, val acc: 0.8762  (best train acc: 0.8503, best val acc: 0.8965, best train loss: 0.4291  @ epoch 2991 )\n",
      "[Epoch: 3300] train loss: 0.5181, train acc: 0.8021, val loss: 0.3751, val acc: 0.8809  (best train acc: 0.8503, best val acc: 0.8965, best train loss: 0.4291  @ epoch 2991 )\n",
      "[Epoch: 3320] train loss: 0.5011, train acc: 0.8192, val loss: 0.3629, val acc: 0.8853  (best train acc: 0.8503, best val acc: 0.8965, best train loss: 0.4291  @ epoch 2991 )\n",
      "[Epoch: 3340] train loss: 0.5990, train acc: 0.7604, val loss: 0.3761, val acc: 0.8840  (best train acc: 0.8503, best val acc: 0.8965, best train loss: 0.4291  @ epoch 2991 )\n",
      "[Epoch: 3360] train loss: 0.4934, train acc: 0.8137, val loss: 0.3750, val acc: 0.8772  (best train acc: 0.8503, best val acc: 0.8981, best train loss: 0.4291  @ epoch 2991 )\n",
      "[Epoch: 3380] train loss: 0.4741, train acc: 0.8164, val loss: 0.3686, val acc: 0.8978  (best train acc: 0.8503, best val acc: 0.8981, best train loss: 0.4291  @ epoch 2991 )\n",
      "[Epoch: 3400] train loss: 0.4610, train acc: 0.8279, val loss: 0.3889, val acc: 0.8681  (best train acc: 0.8503, best val acc: 0.8981, best train loss: 0.4291  @ epoch 2991 )\n",
      "[Epoch: 3420] train loss: 0.4664, train acc: 0.8384, val loss: 0.3896, val acc: 0.8671  (best train acc: 0.8503, best val acc: 0.8995, best train loss: 0.4291  @ epoch 2991 )\n",
      "[Epoch: 3440] train loss: 0.4597, train acc: 0.8306, val loss: 0.3638, val acc: 0.8759  (best train acc: 0.8503, best val acc: 0.8995, best train loss: 0.4291  @ epoch 2991 )\n",
      "[Epoch: 3460] train loss: 0.4649, train acc: 0.8194, val loss: 0.3581, val acc: 0.8938  (best train acc: 0.8503, best val acc: 0.8995, best train loss: 0.4291  @ epoch 2991 )\n",
      "[Epoch: 3480] train loss: 0.4814, train acc: 0.8219, val loss: 0.4149, val acc: 0.8425  (best train acc: 0.8503, best val acc: 0.8995, best train loss: 0.4291  @ epoch 2991 )\n",
      "[Epoch: 3500] train loss: 0.4459, train acc: 0.8386, val loss: 0.3524, val acc: 0.8924  (best train acc: 0.8503, best val acc: 0.8995, best train loss: 0.4291  @ epoch 2991 )\n",
      "[Epoch: 3520] train loss: 0.4662, train acc: 0.8282, val loss: 0.3622, val acc: 0.8907  (best train acc: 0.8536, best val acc: 0.8995, best train loss: 0.4291  @ epoch 2991 )\n",
      "[Epoch: 3540] train loss: 0.5354, train acc: 0.7965, val loss: 0.3703, val acc: 0.8826  (best train acc: 0.8563, best val acc: 0.8995, best train loss: 0.4236  @ epoch 3536 )\n",
      "[Epoch: 3560] train loss: 0.4373, train acc: 0.8433, val loss: 0.3611, val acc: 0.8884  (best train acc: 0.8563, best val acc: 0.8995, best train loss: 0.4236  @ epoch 3536 )\n",
      "[Epoch: 3580] train loss: 0.4627, train acc: 0.8251, val loss: 0.3673, val acc: 0.8820  (best train acc: 0.8563, best val acc: 0.8995, best train loss: 0.4236  @ epoch 3536 )\n",
      "[Epoch: 3600] train loss: 0.4361, train acc: 0.8389, val loss: 0.3489, val acc: 0.8981  (best train acc: 0.8563, best val acc: 0.8998, best train loss: 0.4194  @ epoch 3586 )\n",
      "[Epoch: 3620] train loss: 0.5462, train acc: 0.7804, val loss: 0.3642, val acc: 0.8907  (best train acc: 0.8563, best val acc: 0.8998, best train loss: 0.4194  @ epoch 3586 )\n",
      "[Epoch: 3640] train loss: 0.5229, train acc: 0.7907, val loss: 0.3635, val acc: 0.8830  (best train acc: 0.8563, best val acc: 0.8998, best train loss: 0.4194  @ epoch 3586 )\n",
      "[Epoch: 3660] train loss: 0.4654, train acc: 0.8247, val loss: 0.3551, val acc: 0.8948  (best train acc: 0.8563, best val acc: 0.8998, best train loss: 0.4194  @ epoch 3586 )\n",
      "[Epoch: 3680] train loss: 0.5362, train acc: 0.7816, val loss: 0.3684, val acc: 0.8870  (best train acc: 0.8563, best val acc: 0.8998, best train loss: 0.4194  @ epoch 3586 )\n",
      "[Epoch: 3700] train loss: 0.4886, train acc: 0.8131, val loss: 0.3749, val acc: 0.8739  (best train acc: 0.8563, best val acc: 0.9015, best train loss: 0.4194  @ epoch 3586 )\n",
      "[Epoch: 3720] train loss: 0.4673, train acc: 0.8251, val loss: 0.3655, val acc: 0.8823  (best train acc: 0.8563, best val acc: 0.9015, best train loss: 0.4194  @ epoch 3586 )\n",
      "[Epoch: 3740] train loss: 0.4930, train acc: 0.8197, val loss: 0.3772, val acc: 0.8944  (best train acc: 0.8563, best val acc: 0.9015, best train loss: 0.4194  @ epoch 3586 )\n",
      "[Epoch: 3760] train loss: 0.4564, train acc: 0.8289, val loss: 0.3474, val acc: 0.8901  (best train acc: 0.8563, best val acc: 0.9015, best train loss: 0.4194  @ epoch 3586 )\n",
      "[Epoch: 3780] train loss: 0.4237, train acc: 0.8464, val loss: 0.3751, val acc: 0.8776  (best train acc: 0.8563, best val acc: 0.9015, best train loss: 0.4194  @ epoch 3586 )\n",
      "[Epoch: 3800] train loss: 0.4781, train acc: 0.8124, val loss: 0.3715, val acc: 0.8742  (best train acc: 0.8563, best val acc: 0.9015, best train loss: 0.4184  @ epoch 3790 )\n",
      "[Epoch: 3820] train loss: 0.4763, train acc: 0.8185, val loss: 0.3540, val acc: 0.8951  (best train acc: 0.8563, best val acc: 0.9015, best train loss: 0.4184  @ epoch 3790 )\n",
      "[Epoch: 3840] train loss: 0.4806, train acc: 0.8256, val loss: 0.3420, val acc: 0.8968  (best train acc: 0.8582, best val acc: 0.9015, best train loss: 0.4100  @ epoch 3825 )\n",
      "[Epoch: 3860] train loss: 0.4797, train acc: 0.8253, val loss: 0.3478, val acc: 0.8965  (best train acc: 0.8582, best val acc: 0.9015, best train loss: 0.4100  @ epoch 3825 )\n",
      "[Epoch: 3880] train loss: 0.4422, train acc: 0.8396, val loss: 0.3439, val acc: 0.8941  (best train acc: 0.8582, best val acc: 0.9015, best train loss: 0.4100  @ epoch 3825 )\n",
      "[Epoch: 3900] train loss: 0.4278, train acc: 0.8473, val loss: 0.3369, val acc: 0.9015  (best train acc: 0.8582, best val acc: 0.9015, best train loss: 0.4100  @ epoch 3825 )\n",
      "[Epoch: 3920] train loss: 0.4307, train acc: 0.8389, val loss: 0.3449, val acc: 0.8978  (best train acc: 0.8582, best val acc: 0.9015, best train loss: 0.4060  @ epoch 3903 )\n",
      "[Epoch: 3940] train loss: 0.4505, train acc: 0.8340, val loss: 0.3441, val acc: 0.8961  (best train acc: 0.8582, best val acc: 0.9015, best train loss: 0.4060  @ epoch 3903 )\n",
      "[Epoch: 3960] train loss: 0.4991, train acc: 0.8011, val loss: 0.3423, val acc: 0.8971  (best train acc: 0.8582, best val acc: 0.9015, best train loss: 0.4060  @ epoch 3903 )\n",
      "[Epoch: 3980] train loss: 0.4667, train acc: 0.8211, val loss: 0.3401, val acc: 0.8981  (best train acc: 0.8582, best val acc: 0.9022, best train loss: 0.4060  @ epoch 3903 )\n",
      "[Epoch: 4000] train loss: 0.4623, train acc: 0.8218, val loss: 0.3330, val acc: 0.9015  (best train acc: 0.8582, best val acc: 0.9025, best train loss: 0.4012  @ epoch 3999 )\n",
      "[Epoch: 4020] train loss: 0.4147, train acc: 0.8488, val loss: 0.3386, val acc: 0.8971  (best train acc: 0.8582, best val acc: 0.9025, best train loss: 0.4004  @ epoch 4018 )\n",
      "[Epoch: 4040] train loss: 0.4244, train acc: 0.8422, val loss: 0.3391, val acc: 0.8965  (best train acc: 0.8582, best val acc: 0.9025, best train loss: 0.4004  @ epoch 4018 )\n",
      "[Epoch: 4060] train loss: 0.4132, train acc: 0.8477, val loss: 0.3327, val acc: 0.9012  (best train acc: 0.8614, best val acc: 0.9029, best train loss: 0.4004  @ epoch 4018 )\n",
      "[Epoch: 4080] train loss: 0.4739, train acc: 0.8182, val loss: 0.3342, val acc: 0.8917  (best train acc: 0.8614, best val acc: 0.9029, best train loss: 0.4004  @ epoch 4018 )\n",
      "[Epoch: 4100] train loss: 0.4831, train acc: 0.8133, val loss: 0.3462, val acc: 0.8927  (best train acc: 0.8631, best val acc: 0.9029, best train loss: 0.3994  @ epoch 4098 )\n",
      "[Epoch: 4120] train loss: 0.4775, train acc: 0.8180, val loss: 0.3361, val acc: 0.9002  (best train acc: 0.8631, best val acc: 0.9029, best train loss: 0.3994  @ epoch 4098 )\n",
      "[Epoch: 4140] train loss: 0.4652, train acc: 0.8295, val loss: 0.3407, val acc: 0.8971  (best train acc: 0.8631, best val acc: 0.9029, best train loss: 0.3994  @ epoch 4098 )\n",
      "[Epoch: 4160] train loss: 0.4212, train acc: 0.8470, val loss: 0.3314, val acc: 0.9015  (best train acc: 0.8631, best val acc: 0.9029, best train loss: 0.3994  @ epoch 4098 )\n",
      "[Epoch: 4180] train loss: 0.4503, train acc: 0.8313, val loss: 0.3329, val acc: 0.9022  (best train acc: 0.8631, best val acc: 0.9029, best train loss: 0.3994  @ epoch 4098 )\n",
      "[Epoch: 4200] train loss: 0.4434, train acc: 0.8386, val loss: 0.3359, val acc: 0.9008  (best train acc: 0.8631, best val acc: 0.9032, best train loss: 0.3994  @ epoch 4098 )\n",
      "[Epoch: 4220] train loss: 0.4517, train acc: 0.8277, val loss: 0.3338, val acc: 0.9005  (best train acc: 0.8631, best val acc: 0.9032, best train loss: 0.3994  @ epoch 4098 )\n",
      "[Epoch: 4240] train loss: 0.4257, train acc: 0.8401, val loss: 0.3505, val acc: 0.8894  (best train acc: 0.8631, best val acc: 0.9032, best train loss: 0.3994  @ epoch 4098 )\n",
      "[Epoch: 4260] train loss: 0.4273, train acc: 0.8436, val loss: 0.3430, val acc: 0.8981  (best train acc: 0.8645, best val acc: 0.9032, best train loss: 0.3915  @ epoch 4251 )\n",
      "[Epoch: 4280] train loss: 0.3923, train acc: 0.8634, val loss: 0.3317, val acc: 0.8988  (best train acc: 0.8645, best val acc: 0.9049, best train loss: 0.3915  @ epoch 4251 )\n",
      "[Epoch: 4300] train loss: 0.4330, train acc: 0.8389, val loss: 0.3417, val acc: 0.8894  (best train acc: 0.8658, best val acc: 0.9049, best train loss: 0.3915  @ epoch 4251 )\n",
      "[Epoch: 4320] train loss: 0.4078, train acc: 0.8532, val loss: 0.3305, val acc: 0.9032  (best train acc: 0.8658, best val acc: 0.9049, best train loss: 0.3915  @ epoch 4251 )\n",
      "[Epoch: 4340] train loss: 0.4118, train acc: 0.8512, val loss: 0.3263, val acc: 0.9046  (best train acc: 0.8658, best val acc: 0.9049, best train loss: 0.3915  @ epoch 4251 )\n",
      "[Epoch: 4360] train loss: 0.4122, train acc: 0.8528, val loss: 0.3403, val acc: 0.8948  (best train acc: 0.8658, best val acc: 0.9049, best train loss: 0.3915  @ epoch 4251 )\n",
      "[Epoch: 4380] train loss: 0.5091, train acc: 0.8028, val loss: 0.3360, val acc: 0.8965  (best train acc: 0.8658, best val acc: 0.9049, best train loss: 0.3907  @ epoch 4376 )\n",
      "[Epoch: 4400] train loss: 0.4439, train acc: 0.8248, val loss: 0.3414, val acc: 0.8948  (best train acc: 0.8658, best val acc: 0.9049, best train loss: 0.3907  @ epoch 4376 )\n",
      "[Epoch: 4420] train loss: 0.4321, train acc: 0.8467, val loss: 0.3443, val acc: 0.8958  (best train acc: 0.8658, best val acc: 0.9049, best train loss: 0.3907  @ epoch 4376 )\n",
      "[Epoch: 4440] train loss: 0.4184, train acc: 0.8518, val loss: 0.3465, val acc: 0.8961  (best train acc: 0.8658, best val acc: 0.9052, best train loss: 0.3907  @ epoch 4376 )\n",
      "[Epoch: 4460] train loss: 0.5560, train acc: 0.7731, val loss: 0.3812, val acc: 0.8668  (best train acc: 0.8658, best val acc: 0.9052, best train loss: 0.3880  @ epoch 4443 )\n",
      "[Epoch: 4480] train loss: 0.4127, train acc: 0.8431, val loss: 0.3320, val acc: 0.9042  (best train acc: 0.8658, best val acc: 0.9069, best train loss: 0.3880  @ epoch 4443 )\n",
      "[Epoch: 4500] train loss: 0.4948, train acc: 0.8163, val loss: 0.3689, val acc: 0.8759  (best train acc: 0.8658, best val acc: 0.9069, best train loss: 0.3880  @ epoch 4443 )\n",
      "[Epoch: 4520] train loss: 0.5098, train acc: 0.8049, val loss: 0.3536, val acc: 0.8938  (best train acc: 0.8658, best val acc: 0.9069, best train loss: 0.3880  @ epoch 4443 )\n",
      "[Epoch: 4540] train loss: 0.4185, train acc: 0.8483, val loss: 0.3480, val acc: 0.8944  (best train acc: 0.8658, best val acc: 0.9069, best train loss: 0.3880  @ epoch 4443 )\n",
      "[Epoch: 4560] train loss: 0.4907, train acc: 0.8033, val loss: 0.3387, val acc: 0.8975  (best train acc: 0.8658, best val acc: 0.9069, best train loss: 0.3880  @ epoch 4443 )\n",
      "[Epoch: 4580] train loss: 0.4598, train acc: 0.8149, val loss: 0.3402, val acc: 0.9002  (best train acc: 0.8664, best val acc: 0.9069, best train loss: 0.3880  @ epoch 4443 )\n",
      "[Epoch: 4600] train loss: 0.4219, train acc: 0.8448, val loss: 0.3248, val acc: 0.9005  (best train acc: 0.8664, best val acc: 0.9073, best train loss: 0.3880  @ epoch 4443 )\n",
      "[Epoch: 4620] train loss: 0.4062, train acc: 0.8600, val loss: 0.3278, val acc: 0.9022  (best train acc: 0.8664, best val acc: 0.9073, best train loss: 0.3880  @ epoch 4443 )\n",
      "[Epoch: 4640] train loss: 0.4427, train acc: 0.8303, val loss: 0.3239, val acc: 0.9042  (best train acc: 0.8664, best val acc: 0.9073, best train loss: 0.3880  @ epoch 4443 )\n",
      "[Epoch: 4660] train loss: 0.4173, train acc: 0.8508, val loss: 0.3236, val acc: 0.9056  (best train acc: 0.8664, best val acc: 0.9073, best train loss: 0.3880  @ epoch 4443 )\n",
      "[Epoch: 4680] train loss: 0.3957, train acc: 0.8630, val loss: 0.3232, val acc: 0.9049  (best train acc: 0.8664, best val acc: 0.9073, best train loss: 0.3880  @ epoch 4443 )\n",
      "[Epoch: 4700] train loss: 0.4660, train acc: 0.8193, val loss: 0.3555, val acc: 0.8904  (best train acc: 0.8664, best val acc: 0.9073, best train loss: 0.3880  @ epoch 4443 )\n",
      "[Epoch: 4720] train loss: 0.4353, train acc: 0.8369, val loss: 0.3310, val acc: 0.8992  (best train acc: 0.8664, best val acc: 0.9073, best train loss: 0.3880  @ epoch 4443 )\n",
      "[Epoch: 4740] train loss: 0.3842, train acc: 0.8663, val loss: 0.3441, val acc: 0.8938  (best train acc: 0.8664, best val acc: 0.9073, best train loss: 0.3842  @ epoch 4740 )\n",
      "[Epoch: 4760] train loss: 0.4063, train acc: 0.8596, val loss: 0.3518, val acc: 0.8934  (best train acc: 0.8664, best val acc: 0.9073, best train loss: 0.3842  @ epoch 4740 )\n",
      "[Epoch: 4780] train loss: 0.4191, train acc: 0.8407, val loss: 0.3482, val acc: 0.8968  (best train acc: 0.8664, best val acc: 0.9073, best train loss: 0.3842  @ epoch 4740 )\n",
      "[Epoch: 4800] train loss: 0.4380, train acc: 0.8496, val loss: 0.3293, val acc: 0.8954  (best train acc: 0.8664, best val acc: 0.9073, best train loss: 0.3842  @ epoch 4740 )\n",
      "[Epoch: 4820] train loss: 0.4099, train acc: 0.8559, val loss: 0.3317, val acc: 0.9002  (best train acc: 0.8664, best val acc: 0.9073, best train loss: 0.3839  @ epoch 4805 )\n",
      "[Epoch: 4840] train loss: 0.4052, train acc: 0.8530, val loss: 0.3401, val acc: 0.9025  (best train acc: 0.8664, best val acc: 0.9073, best train loss: 0.3839  @ epoch 4805 )\n",
      "[Epoch: 4860] train loss: 0.3982, train acc: 0.8619, val loss: 0.3363, val acc: 0.9008  (best train acc: 0.8701, best val acc: 0.9073, best train loss: 0.3839  @ epoch 4805 )\n",
      "[Epoch: 4880] train loss: 0.4867, train acc: 0.8167, val loss: 0.3402, val acc: 0.8944  (best train acc: 0.8722, best val acc: 0.9076, best train loss: 0.3768  @ epoch 4863 )\n",
      "[Epoch: 4900] train loss: 0.3738, train acc: 0.8681, val loss: 0.3254, val acc: 0.9046  (best train acc: 0.8722, best val acc: 0.9076, best train loss: 0.3736  @ epoch 4894 )\n",
      "[Epoch: 4920] train loss: 0.4239, train acc: 0.8399, val loss: 0.3414, val acc: 0.8975  (best train acc: 0.8722, best val acc: 0.9093, best train loss: 0.3698  @ epoch 4903 )\n",
      "[Epoch: 4940] train loss: 0.4040, train acc: 0.8480, val loss: 0.3371, val acc: 0.9022  (best train acc: 0.8722, best val acc: 0.9093, best train loss: 0.3698  @ epoch 4903 )\n",
      "[Epoch: 4960] train loss: 0.3778, train acc: 0.8704, val loss: 0.3463, val acc: 0.8998  (best train acc: 0.8731, best val acc: 0.9093, best train loss: 0.3698  @ epoch 4903 )\n",
      "[Epoch: 4980] train loss: 0.4226, train acc: 0.8424, val loss: 0.3230, val acc: 0.9008  (best train acc: 0.8741, best val acc: 0.9093, best train loss: 0.3646  @ epoch 4965 )\n",
      "[Epoch: 5000] train loss: 0.4172, train acc: 0.8383, val loss: 0.3351, val acc: 0.9042  (best train acc: 0.8741, best val acc: 0.9093, best train loss: 0.3646  @ epoch 4965 )\n",
      "[Epoch: 5020] train loss: 0.4192, train acc: 0.8497, val loss: 0.3498, val acc: 0.8971  (best train acc: 0.8741, best val acc: 0.9093, best train loss: 0.3646  @ epoch 4965 )\n",
      "[Epoch: 5040] train loss: 0.4056, train acc: 0.8578, val loss: 0.3375, val acc: 0.9025  (best train acc: 0.8790, best val acc: 0.9093, best train loss: 0.3646  @ epoch 4965 )\n",
      "[Epoch: 5060] train loss: 0.3729, train acc: 0.8659, val loss: 0.3263, val acc: 0.9059  (best train acc: 0.8790, best val acc: 0.9093, best train loss: 0.3646  @ epoch 4965 )\n",
      "[Epoch: 5080] train loss: 0.3791, train acc: 0.8660, val loss: 0.3193, val acc: 0.9096  (best train acc: 0.8790, best val acc: 0.9096, best train loss: 0.3592  @ epoch 5065 )\n",
      "[Epoch: 5100] train loss: 0.3951, train acc: 0.8674, val loss: 0.3287, val acc: 0.9012  (best train acc: 0.8790, best val acc: 0.9096, best train loss: 0.3592  @ epoch 5065 )\n",
      "[Epoch: 5120] train loss: 0.4155, train acc: 0.8399, val loss: 0.3335, val acc: 0.9032  (best train acc: 0.8790, best val acc: 0.9096, best train loss: 0.3592  @ epoch 5065 )\n",
      "[Epoch: 5140] train loss: 0.4105, train acc: 0.8569, val loss: 0.3313, val acc: 0.9022  (best train acc: 0.8790, best val acc: 0.9099, best train loss: 0.3592  @ epoch 5065 )\n",
      "[Epoch: 5160] train loss: 0.4021, train acc: 0.8488, val loss: 0.3217, val acc: 0.9066  (best train acc: 0.8790, best val acc: 0.9106, best train loss: 0.3592  @ epoch 5065 )\n",
      "[Epoch: 5180] train loss: 0.3611, train acc: 0.8689, val loss: 0.3160, val acc: 0.9073  (best train acc: 0.8790, best val acc: 0.9106, best train loss: 0.3592  @ epoch 5065 )\n",
      "[Epoch: 5200] train loss: 0.4186, train acc: 0.8476, val loss: 0.3315, val acc: 0.9002  (best train acc: 0.8790, best val acc: 0.9106, best train loss: 0.3592  @ epoch 5065 )\n",
      "[Epoch: 5220] train loss: 0.4099, train acc: 0.8482, val loss: 0.3216, val acc: 0.9076  (best train acc: 0.8790, best val acc: 0.9106, best train loss: 0.3553  @ epoch 5219 )\n",
      "[Epoch: 5240] train loss: 0.4039, train acc: 0.8504, val loss: 0.3223, val acc: 0.9096  (best train acc: 0.8790, best val acc: 0.9106, best train loss: 0.3553  @ epoch 5219 )\n",
      "[Epoch: 5260] train loss: 0.3959, train acc: 0.8592, val loss: 0.3246, val acc: 0.9029  (best train acc: 0.8790, best val acc: 0.9106, best train loss: 0.3553  @ epoch 5219 )\n",
      "[Epoch: 5280] train loss: 0.3767, train acc: 0.8723, val loss: 0.3217, val acc: 0.9056  (best train acc: 0.8790, best val acc: 0.9106, best train loss: 0.3553  @ epoch 5219 )\n",
      "[Epoch: 5300] train loss: 0.3742, train acc: 0.8715, val loss: 0.3315, val acc: 0.9093  (best train acc: 0.8790, best val acc: 0.9106, best train loss: 0.3553  @ epoch 5219 )\n",
      "[Epoch: 5320] train loss: 0.3825, train acc: 0.8607, val loss: 0.3342, val acc: 0.9015  (best train acc: 0.8790, best val acc: 0.9106, best train loss: 0.3553  @ epoch 5219 )\n",
      "[Epoch: 5340] train loss: 0.3610, train acc: 0.8794, val loss: 0.3332, val acc: 0.9035  (best train acc: 0.8794, best val acc: 0.9113, best train loss: 0.3553  @ epoch 5219 )\n",
      "[Epoch: 5360] train loss: 0.3864, train acc: 0.8656, val loss: 0.3300, val acc: 0.9019  (best train acc: 0.8794, best val acc: 0.9120, best train loss: 0.3553  @ epoch 5219 )\n",
      "[Epoch: 5380] train loss: 0.3679, train acc: 0.8740, val loss: 0.3240, val acc: 0.9039  (best train acc: 0.8794, best val acc: 0.9120, best train loss: 0.3553  @ epoch 5219 )\n",
      "[Epoch: 5400] train loss: 0.3757, train acc: 0.8695, val loss: 0.3468, val acc: 0.8958  (best train acc: 0.8794, best val acc: 0.9120, best train loss: 0.3553  @ epoch 5219 )\n",
      "[Epoch: 5420] train loss: 0.3737, train acc: 0.8681, val loss: 0.3144, val acc: 0.9086  (best train acc: 0.8794, best val acc: 0.9137, best train loss: 0.3553  @ epoch 5219 )\n",
      "[Epoch: 5440] train loss: 0.3811, train acc: 0.8730, val loss: 0.3144, val acc: 0.9093  (best train acc: 0.8797, best val acc: 0.9137, best train loss: 0.3553  @ epoch 5219 )\n",
      "[Epoch: 5460] train loss: 0.3848, train acc: 0.8534, val loss: 0.3305, val acc: 0.9073  (best train acc: 0.8818, best val acc: 0.9137, best train loss: 0.3499  @ epoch 5443 )\n",
      "[Epoch: 5480] train loss: 0.3702, train acc: 0.8756, val loss: 0.3206, val acc: 0.9083  (best train acc: 0.8818, best val acc: 0.9143, best train loss: 0.3499  @ epoch 5443 )\n",
      "[Epoch: 5500] train loss: 0.3740, train acc: 0.8699, val loss: 0.3235, val acc: 0.9052  (best train acc: 0.8818, best val acc: 0.9143, best train loss: 0.3499  @ epoch 5443 )\n",
      "[Epoch: 5520] train loss: 0.3702, train acc: 0.8699, val loss: 0.3170, val acc: 0.9076  (best train acc: 0.8820, best val acc: 0.9143, best train loss: 0.3493  @ epoch 5515 )\n",
      "[Epoch: 5540] train loss: 0.3624, train acc: 0.8715, val loss: 0.3184, val acc: 0.9076  (best train acc: 0.8820, best val acc: 0.9143, best train loss: 0.3493  @ epoch 5515 )\n",
      "[Epoch: 5560] train loss: 0.3681, train acc: 0.8650, val loss: 0.3125, val acc: 0.9153  (best train acc: 0.8820, best val acc: 0.9157, best train loss: 0.3493  @ epoch 5515 )\n",
      "[Epoch: 5580] train loss: 0.3951, train acc: 0.8510, val loss: 0.3259, val acc: 0.9062  (best train acc: 0.8820, best val acc: 0.9157, best train loss: 0.3493  @ epoch 5515 )\n",
      "[Epoch: 5600] train loss: 0.4246, train acc: 0.8475, val loss: 0.3090, val acc: 0.9137  (best train acc: 0.8844, best val acc: 0.9157, best train loss: 0.3385  @ epoch 5592 )\n",
      "[Epoch: 5620] train loss: 0.3666, train acc: 0.8754, val loss: 0.3172, val acc: 0.9079  (best train acc: 0.8844, best val acc: 0.9170, best train loss: 0.3385  @ epoch 5592 )\n",
      "[Epoch: 5640] train loss: 0.4004, train acc: 0.8575, val loss: 0.3361, val acc: 0.9015  (best train acc: 0.8844, best val acc: 0.9170, best train loss: 0.3385  @ epoch 5592 )\n",
      "[Epoch: 5660] train loss: 0.3600, train acc: 0.8735, val loss: 0.3129, val acc: 0.9120  (best train acc: 0.8844, best val acc: 0.9170, best train loss: 0.3385  @ epoch 5592 )\n",
      "[Epoch: 5680] train loss: 0.3505, train acc: 0.8782, val loss: 0.2991, val acc: 0.9177  (best train acc: 0.8844, best val acc: 0.9177, best train loss: 0.3385  @ epoch 5592 )\n",
      "[Epoch: 5700] train loss: 0.4041, train acc: 0.8485, val loss: 0.3264, val acc: 0.9083  (best train acc: 0.8844, best val acc: 0.9180, best train loss: 0.3385  @ epoch 5592 )\n",
      "[Epoch: 5720] train loss: 0.3635, train acc: 0.8717, val loss: 0.3047, val acc: 0.9126  (best train acc: 0.8844, best val acc: 0.9180, best train loss: 0.3385  @ epoch 5592 )\n",
      "[Epoch: 5740] train loss: 0.3876, train acc: 0.8613, val loss: 0.3221, val acc: 0.9066  (best train acc: 0.8844, best val acc: 0.9180, best train loss: 0.3385  @ epoch 5592 )\n",
      "[Epoch: 5760] train loss: 0.3898, train acc: 0.8553, val loss: 0.3339, val acc: 0.8981  (best train acc: 0.8844, best val acc: 0.9180, best train loss: 0.3385  @ epoch 5592 )\n",
      "[Epoch: 5780] train loss: 0.3876, train acc: 0.8608, val loss: 0.3066, val acc: 0.9180  (best train acc: 0.8844, best val acc: 0.9191, best train loss: 0.3385  @ epoch 5592 )\n",
      "[Epoch: 5800] train loss: 0.3855, train acc: 0.8636, val loss: 0.3092, val acc: 0.9157  (best train acc: 0.8844, best val acc: 0.9191, best train loss: 0.3385  @ epoch 5592 )\n",
      "[Epoch: 5820] train loss: 0.4429, train acc: 0.8396, val loss: 0.3138, val acc: 0.9096  (best train acc: 0.8844, best val acc: 0.9204, best train loss: 0.3385  @ epoch 5592 )\n",
      "[Epoch: 5840] train loss: 0.3564, train acc: 0.8744, val loss: 0.2988, val acc: 0.9174  (best train acc: 0.8844, best val acc: 0.9204, best train loss: 0.3355  @ epoch 5835 )\n",
      "[Epoch: 5860] train loss: 0.4457, train acc: 0.8422, val loss: 0.3076, val acc: 0.9137  (best train acc: 0.8844, best val acc: 0.9204, best train loss: 0.3355  @ epoch 5835 )\n",
      "[Epoch: 5880] train loss: 0.3665, train acc: 0.8680, val loss: 0.3138, val acc: 0.9019  (best train acc: 0.8844, best val acc: 0.9204, best train loss: 0.3355  @ epoch 5835 )\n",
      "[Epoch: 5900] train loss: 0.4277, train acc: 0.8326, val loss: 0.3220, val acc: 0.9039  (best train acc: 0.8844, best val acc: 0.9214, best train loss: 0.3355  @ epoch 5835 )\n",
      "[Epoch: 5920] train loss: 0.3762, train acc: 0.8702, val loss: 0.3115, val acc: 0.9113  (best train acc: 0.8844, best val acc: 0.9214, best train loss: 0.3355  @ epoch 5835 )\n",
      "[Epoch: 5940] train loss: 0.3671, train acc: 0.8699, val loss: 0.3075, val acc: 0.9147  (best train acc: 0.8844, best val acc: 0.9214, best train loss: 0.3355  @ epoch 5835 )\n",
      "[Epoch: 5960] train loss: 0.3625, train acc: 0.8753, val loss: 0.3097, val acc: 0.9137  (best train acc: 0.8844, best val acc: 0.9221, best train loss: 0.3355  @ epoch 5835 )\n",
      "[Epoch: 5980] train loss: 0.3984, train acc: 0.8551, val loss: 0.3328, val acc: 0.9029  (best train acc: 0.8844, best val acc: 0.9221, best train loss: 0.3355  @ epoch 5835 )\n",
      "[Epoch: 6000] train loss: 0.4027, train acc: 0.8576, val loss: 0.3235, val acc: 0.9005  (best train acc: 0.8844, best val acc: 0.9221, best train loss: 0.3355  @ epoch 5835 )\n",
      "[Epoch: 6020] train loss: 0.3539, train acc: 0.8802, val loss: 0.2943, val acc: 0.9224  (best train acc: 0.8844, best val acc: 0.9224, best train loss: 0.3355  @ epoch 5835 )\n",
      "[Epoch: 6040] train loss: 0.3496, train acc: 0.8798, val loss: 0.2982, val acc: 0.9204  (best train acc: 0.8844, best val acc: 0.9224, best train loss: 0.3355  @ epoch 5835 )\n",
      "[Epoch: 6060] train loss: 0.3629, train acc: 0.8706, val loss: 0.3147, val acc: 0.9032  (best train acc: 0.8875, best val acc: 0.9224, best train loss: 0.3355  @ epoch 5835 )\n",
      "[Epoch: 6080] train loss: 0.3907, train acc: 0.8565, val loss: 0.2886, val acc: 0.9164  (best train acc: 0.8875, best val acc: 0.9224, best train loss: 0.3355  @ epoch 5835 )\n",
      "[Epoch: 6100] train loss: 0.3545, train acc: 0.8783, val loss: 0.2998, val acc: 0.9177  (best train acc: 0.8875, best val acc: 0.9224, best train loss: 0.3355  @ epoch 5835 )\n",
      "[Epoch: 6120] train loss: 0.3677, train acc: 0.8761, val loss: 0.3296, val acc: 0.9015  (best train acc: 0.8875, best val acc: 0.9224, best train loss: 0.3355  @ epoch 5835 )\n",
      "[Epoch: 6140] train loss: 0.4738, train acc: 0.8177, val loss: 0.2955, val acc: 0.9180  (best train acc: 0.8875, best val acc: 0.9224, best train loss: 0.3355  @ epoch 5835 )\n",
      "[Epoch: 6160] train loss: 0.3647, train acc: 0.8729, val loss: 0.2941, val acc: 0.9180  (best train acc: 0.8875, best val acc: 0.9224, best train loss: 0.3355  @ epoch 5835 )\n",
      "[Epoch: 6180] train loss: 0.3467, train acc: 0.8790, val loss: 0.3177, val acc: 0.9103  (best train acc: 0.8875, best val acc: 0.9224, best train loss: 0.3355  @ epoch 5835 )\n",
      "[Epoch: 6200] train loss: 0.3692, train acc: 0.8663, val loss: 0.2978, val acc: 0.9153  (best train acc: 0.8875, best val acc: 0.9231, best train loss: 0.3347  @ epoch 6194 )\n",
      "[Epoch: 6220] train loss: 0.3484, train acc: 0.8845, val loss: 0.3184, val acc: 0.9093  (best train acc: 0.8875, best val acc: 0.9231, best train loss: 0.3347  @ epoch 6194 )\n",
      "[Epoch: 6240] train loss: 0.3509, train acc: 0.8759, val loss: 0.2927, val acc: 0.9113  (best train acc: 0.8875, best val acc: 0.9231, best train loss: 0.3347  @ epoch 6194 )\n",
      "[Epoch: 6260] train loss: 0.4384, train acc: 0.8369, val loss: 0.3082, val acc: 0.9073  (best train acc: 0.8875, best val acc: 0.9231, best train loss: 0.3347  @ epoch 6194 )\n",
      "[Epoch: 6280] train loss: 0.4019, train acc: 0.8546, val loss: 0.3076, val acc: 0.9160  (best train acc: 0.8875, best val acc: 0.9231, best train loss: 0.3347  @ epoch 6194 )\n",
      "[Epoch: 6300] train loss: 0.3683, train acc: 0.8723, val loss: 0.3208, val acc: 0.9049  (best train acc: 0.8875, best val acc: 0.9231, best train loss: 0.3347  @ epoch 6194 )\n",
      "[Epoch: 6320] train loss: 0.4449, train acc: 0.8331, val loss: 0.2980, val acc: 0.9170  (best train acc: 0.8875, best val acc: 0.9231, best train loss: 0.3347  @ epoch 6194 )\n",
      "[Epoch: 6340] train loss: 0.3894, train acc: 0.8485, val loss: 0.3076, val acc: 0.9012  (best train acc: 0.8875, best val acc: 0.9231, best train loss: 0.3347  @ epoch 6194 )\n",
      "[Epoch: 6360] train loss: 0.3948, train acc: 0.8554, val loss: 0.2995, val acc: 0.9116  (best train acc: 0.8875, best val acc: 0.9231, best train loss: 0.3347  @ epoch 6194 )\n",
      "[Epoch: 6380] train loss: 0.3953, train acc: 0.8541, val loss: 0.2964, val acc: 0.9150  (best train acc: 0.8875, best val acc: 0.9238, best train loss: 0.3306  @ epoch 6370 )\n",
      "[Epoch: 6400] train loss: 0.4175, train acc: 0.8336, val loss: 0.3217, val acc: 0.9086  (best train acc: 0.8875, best val acc: 0.9238, best train loss: 0.3306  @ epoch 6370 )\n",
      "[Epoch: 6420] train loss: 0.3947, train acc: 0.8615, val loss: 0.2957, val acc: 0.9180  (best train acc: 0.8875, best val acc: 0.9238, best train loss: 0.3306  @ epoch 6370 )\n",
      "[Epoch: 6440] train loss: 0.3558, train acc: 0.8749, val loss: 0.3008, val acc: 0.9150  (best train acc: 0.8875, best val acc: 0.9238, best train loss: 0.3306  @ epoch 6370 )\n",
      "[Epoch: 6460] train loss: 0.3857, train acc: 0.8623, val loss: 0.2989, val acc: 0.9187  (best train acc: 0.8890, best val acc: 0.9258, best train loss: 0.3306  @ epoch 6370 )\n",
      "[Epoch: 6480] train loss: 0.3488, train acc: 0.8764, val loss: 0.2873, val acc: 0.9214  (best train acc: 0.8890, best val acc: 0.9258, best train loss: 0.3306  @ epoch 6370 )\n",
      "[Epoch: 6500] train loss: 0.3672, train acc: 0.8733, val loss: 0.3020, val acc: 0.9153  (best train acc: 0.8890, best val acc: 0.9258, best train loss: 0.3306  @ epoch 6370 )\n",
      "[Epoch: 6520] train loss: 0.3462, train acc: 0.8751, val loss: 0.2814, val acc: 0.9221  (best train acc: 0.8890, best val acc: 0.9258, best train loss: 0.3250  @ epoch 6518 )\n",
      "[Epoch: 6540] train loss: 0.3513, train acc: 0.8755, val loss: 0.2829, val acc: 0.9234  (best train acc: 0.8890, best val acc: 0.9258, best train loss: 0.3250  @ epoch 6518 )\n",
      "[Epoch: 6560] train loss: 0.4109, train acc: 0.8519, val loss: 0.2840, val acc: 0.9251  (best train acc: 0.8890, best val acc: 0.9258, best train loss: 0.3250  @ epoch 6518 )\n",
      "[Epoch: 6580] train loss: 0.3262, train acc: 0.8908, val loss: 0.2783, val acc: 0.9204  (best train acc: 0.8908, best val acc: 0.9258, best train loss: 0.3250  @ epoch 6518 )\n",
      "[Epoch: 6600] train loss: 0.4110, train acc: 0.8425, val loss: 0.2890, val acc: 0.9214  (best train acc: 0.8908, best val acc: 0.9258, best train loss: 0.3250  @ epoch 6518 )\n",
      "[Epoch: 6620] train loss: 0.3416, train acc: 0.8866, val loss: 0.2796, val acc: 0.9228  (best train acc: 0.8908, best val acc: 0.9258, best train loss: 0.3250  @ epoch 6518 )\n",
      "[Epoch: 6640] train loss: 0.4264, train acc: 0.8383, val loss: 0.2983, val acc: 0.9204  (best train acc: 0.8908, best val acc: 0.9258, best train loss: 0.3250  @ epoch 6518 )\n",
      "[Epoch: 6660] train loss: 0.3627, train acc: 0.8740, val loss: 0.2796, val acc: 0.9268  (best train acc: 0.8908, best val acc: 0.9268, best train loss: 0.3250  @ epoch 6518 )\n",
      "[Epoch: 6680] train loss: 0.3926, train acc: 0.8617, val loss: 0.2969, val acc: 0.9096  (best train acc: 0.8908, best val acc: 0.9268, best train loss: 0.3250  @ epoch 6518 )\n",
      "[Epoch: 6700] train loss: 0.3493, train acc: 0.8798, val loss: 0.2811, val acc: 0.9231  (best train acc: 0.8908, best val acc: 0.9268, best train loss: 0.3250  @ epoch 6518 )\n",
      "[Epoch: 6720] train loss: 0.3364, train acc: 0.8820, val loss: 0.2851, val acc: 0.9211  (best train acc: 0.8908, best val acc: 0.9268, best train loss: 0.3250  @ epoch 6518 )\n",
      "[Epoch: 6740] train loss: 0.3329, train acc: 0.8791, val loss: 0.2859, val acc: 0.9201  (best train acc: 0.8908, best val acc: 0.9268, best train loss: 0.3250  @ epoch 6518 )\n",
      "[Epoch: 6760] train loss: 0.3453, train acc: 0.8793, val loss: 0.2822, val acc: 0.9197  (best train acc: 0.8908, best val acc: 0.9268, best train loss: 0.3250  @ epoch 6518 )\n",
      "[Epoch: 6780] train loss: 0.3790, train acc: 0.8570, val loss: 0.2831, val acc: 0.9204  (best train acc: 0.8908, best val acc: 0.9268, best train loss: 0.3250  @ epoch 6518 )\n",
      "[Epoch: 6800] train loss: 0.3403, train acc: 0.8822, val loss: 0.2814, val acc: 0.9174  (best train acc: 0.8908, best val acc: 0.9268, best train loss: 0.3221  @ epoch 6784 )\n",
      "[Epoch: 6820] train loss: 0.3518, train acc: 0.8733, val loss: 0.2745, val acc: 0.9187  (best train acc: 0.8908, best val acc: 0.9268, best train loss: 0.3221  @ epoch 6784 )\n",
      "[Epoch: 6840] train loss: 0.3315, train acc: 0.8874, val loss: 0.2698, val acc: 0.9265  (best train acc: 0.8908, best val acc: 0.9285, best train loss: 0.3221  @ epoch 6784 )\n",
      "[Epoch: 6860] train loss: 0.3615, train acc: 0.8676, val loss: 0.2707, val acc: 0.9258  (best train acc: 0.8908, best val acc: 0.9285, best train loss: 0.3202  @ epoch 6855 )\n",
      "[Epoch: 6880] train loss: 0.3348, train acc: 0.8867, val loss: 0.2809, val acc: 0.9204  (best train acc: 0.8908, best val acc: 0.9285, best train loss: 0.3202  @ epoch 6855 )\n",
      "[Epoch: 6900] train loss: 0.3524, train acc: 0.8775, val loss: 0.2715, val acc: 0.9191  (best train acc: 0.8908, best val acc: 0.9285, best train loss: 0.3202  @ epoch 6855 )\n",
      "[Epoch: 6920] train loss: 0.3219, train acc: 0.8877, val loss: 0.2691, val acc: 0.9248  (best train acc: 0.8916, best val acc: 0.9285, best train loss: 0.3157  @ epoch 6919 )\n",
      "[Epoch: 6940] train loss: 0.3822, train acc: 0.8589, val loss: 0.2853, val acc: 0.9255  (best train acc: 0.8938, best val acc: 0.9285, best train loss: 0.3132  @ epoch 6926 )\n",
      "[Epoch: 6960] train loss: 0.3397, train acc: 0.8803, val loss: 0.2709, val acc: 0.9258  (best train acc: 0.8938, best val acc: 0.9285, best train loss: 0.3132  @ epoch 6926 )\n",
      "[Epoch: 6980] train loss: 0.3561, train acc: 0.8681, val loss: 0.2751, val acc: 0.9255  (best train acc: 0.8938, best val acc: 0.9285, best train loss: 0.3132  @ epoch 6926 )\n",
      "[Epoch: 7000] train loss: 0.3475, train acc: 0.8732, val loss: 0.2738, val acc: 0.9278  (best train acc: 0.8938, best val acc: 0.9302, best train loss: 0.3132  @ epoch 6926 )\n",
      "[Epoch: 7020] train loss: 0.3339, train acc: 0.8797, val loss: 0.2663, val acc: 0.9245  (best train acc: 0.8938, best val acc: 0.9302, best train loss: 0.3132  @ epoch 6926 )\n",
      "[Epoch: 7040] train loss: 0.3202, train acc: 0.8824, val loss: 0.2659, val acc: 0.9255  (best train acc: 0.8938, best val acc: 0.9309, best train loss: 0.3132  @ epoch 6926 )\n",
      "[Epoch: 7060] train loss: 0.3320, train acc: 0.8878, val loss: 0.2818, val acc: 0.9207  (best train acc: 0.8938, best val acc: 0.9309, best train loss: 0.3132  @ epoch 6926 )\n",
      "[Epoch: 7080] train loss: 0.3339, train acc: 0.8814, val loss: 0.2644, val acc: 0.9265  (best train acc: 0.8938, best val acc: 0.9309, best train loss: 0.3132  @ epoch 6926 )\n",
      "[Epoch: 7100] train loss: 0.3363, train acc: 0.8770, val loss: 0.2594, val acc: 0.9268  (best train acc: 0.8938, best val acc: 0.9319, best train loss: 0.3132  @ epoch 6926 )\n",
      "[Epoch: 7120] train loss: 0.3325, train acc: 0.8831, val loss: 0.2788, val acc: 0.9261  (best train acc: 0.8951, best val acc: 0.9319, best train loss: 0.3132  @ epoch 6926 )\n",
      "[Epoch: 7140] train loss: 0.3776, train acc: 0.8584, val loss: 0.2676, val acc: 0.9288  (best train acc: 0.8951, best val acc: 0.9319, best train loss: 0.3045  @ epoch 7138 )\n",
      "[Epoch: 7160] train loss: 0.3713, train acc: 0.8604, val loss: 0.2613, val acc: 0.9309  (best train acc: 0.8978, best val acc: 0.9319, best train loss: 0.3045  @ epoch 7138 )\n",
      "[Epoch: 7180] train loss: 0.3681, train acc: 0.8616, val loss: 0.2823, val acc: 0.9143  (best train acc: 0.8978, best val acc: 0.9336, best train loss: 0.3036  @ epoch 7178 )\n",
      "[Epoch: 7200] train loss: 0.3700, train acc: 0.8563, val loss: 0.2646, val acc: 0.9201  (best train acc: 0.8978, best val acc: 0.9336, best train loss: 0.3036  @ epoch 7178 )\n",
      "[Epoch: 7220] train loss: 0.3230, train acc: 0.8869, val loss: 0.2597, val acc: 0.9295  (best train acc: 0.8978, best val acc: 0.9356, best train loss: 0.3036  @ epoch 7178 )\n",
      "[Epoch: 7240] train loss: 0.3310, train acc: 0.8826, val loss: 0.2673, val acc: 0.9184  (best train acc: 0.8978, best val acc: 0.9356, best train loss: 0.3036  @ epoch 7178 )\n",
      "[Epoch: 7260] train loss: 0.3416, train acc: 0.8801, val loss: 0.2567, val acc: 0.9258  (best train acc: 0.8978, best val acc: 0.9356, best train loss: 0.3036  @ epoch 7178 )\n",
      "[Epoch: 7280] train loss: 0.3359, train acc: 0.8759, val loss: 0.2543, val acc: 0.9305  (best train acc: 0.8978, best val acc: 0.9356, best train loss: 0.3036  @ epoch 7178 )\n",
      "[Epoch: 7300] train loss: 0.3188, train acc: 0.8872, val loss: 0.2488, val acc: 0.9302  (best train acc: 0.8978, best val acc: 0.9356, best train loss: 0.3036  @ epoch 7178 )\n",
      "[Epoch: 7320] train loss: 0.3200, train acc: 0.8859, val loss: 0.2545, val acc: 0.9305  (best train acc: 0.8978, best val acc: 0.9356, best train loss: 0.3036  @ epoch 7178 )\n",
      "[Epoch: 7340] train loss: 0.3108, train acc: 0.8930, val loss: 0.2450, val acc: 0.9292  (best train acc: 0.8978, best val acc: 0.9356, best train loss: 0.3010  @ epoch 7322 )\n",
      "[Epoch: 7360] train loss: 0.4341, train acc: 0.8496, val loss: 0.2507, val acc: 0.9288  (best train acc: 0.8996, best val acc: 0.9356, best train loss: 0.2926  @ epoch 7355 )\n",
      "[Epoch: 7380] train loss: 0.3145, train acc: 0.8919, val loss: 0.2496, val acc: 0.9349  (best train acc: 0.9017, best val acc: 0.9356, best train loss: 0.2926  @ epoch 7355 )\n",
      "[Epoch: 7400] train loss: 0.3949, train acc: 0.8631, val loss: 0.2514, val acc: 0.9265  (best train acc: 0.9017, best val acc: 0.9379, best train loss: 0.2926  @ epoch 7355 )\n",
      "[Epoch: 7420] train loss: 0.3651, train acc: 0.8550, val loss: 0.2577, val acc: 0.9234  (best train acc: 0.9017, best val acc: 0.9379, best train loss: 0.2926  @ epoch 7355 )\n",
      "[Epoch: 7440] train loss: 0.3462, train acc: 0.8713, val loss: 0.2469, val acc: 0.9282  (best train acc: 0.9017, best val acc: 0.9379, best train loss: 0.2926  @ epoch 7355 )\n",
      "[Epoch: 7460] train loss: 0.4735, train acc: 0.8250, val loss: 0.2791, val acc: 0.9056  (best train acc: 0.9017, best val acc: 0.9379, best train loss: 0.2926  @ epoch 7355 )\n",
      "[Epoch: 7480] train loss: 0.3318, train acc: 0.8872, val loss: 0.2738, val acc: 0.9228  (best train acc: 0.9017, best val acc: 0.9379, best train loss: 0.2926  @ epoch 7355 )\n",
      "[Epoch: 7500] train loss: 0.3483, train acc: 0.8723, val loss: 0.2398, val acc: 0.9295  (best train acc: 0.9017, best val acc: 0.9379, best train loss: 0.2926  @ epoch 7355 )\n",
      "[Epoch: 7520] train loss: 0.3127, train acc: 0.8882, val loss: 0.2400, val acc: 0.9288  (best train acc: 0.9017, best val acc: 0.9379, best train loss: 0.2913  @ epoch 7502 )\n",
      "[Epoch: 7540] train loss: 0.3497, train acc: 0.8679, val loss: 0.2378, val acc: 0.9292  (best train acc: 0.9017, best val acc: 0.9379, best train loss: 0.2913  @ epoch 7502 )\n",
      "[Epoch: 7560] train loss: 0.3428, train acc: 0.8756, val loss: 0.2297, val acc: 0.9325  (best train acc: 0.9017, best val acc: 0.9379, best train loss: 0.2906  @ epoch 7546 )\n",
      "[Epoch: 7580] train loss: 0.4203, train acc: 0.8368, val loss: 0.3016, val acc: 0.8971  (best train acc: 0.9029, best val acc: 0.9379, best train loss: 0.2895  @ epoch 7565 )\n",
      "[Epoch: 7600] train loss: 0.3709, train acc: 0.8616, val loss: 0.2514, val acc: 0.9319  (best train acc: 0.9029, best val acc: 0.9379, best train loss: 0.2895  @ epoch 7565 )\n",
      "[Epoch: 7620] train loss: 0.3254, train acc: 0.8840, val loss: 0.2277, val acc: 0.9363  (best train acc: 0.9029, best val acc: 0.9379, best train loss: 0.2895  @ epoch 7565 )\n",
      "[Epoch: 7640] train loss: 0.3509, train acc: 0.8780, val loss: 0.2417, val acc: 0.9265  (best train acc: 0.9031, best val acc: 0.9379, best train loss: 0.2835  @ epoch 7633 )\n",
      "[Epoch: 7660] train loss: 0.3579, train acc: 0.8636, val loss: 0.2396, val acc: 0.9305  (best train acc: 0.9031, best val acc: 0.9379, best train loss: 0.2835  @ epoch 7633 )\n",
      "[Epoch: 7680] train loss: 0.3537, train acc: 0.8615, val loss: 0.2303, val acc: 0.9322  (best train acc: 0.9031, best val acc: 0.9379, best train loss: 0.2835  @ epoch 7633 )\n",
      "[Epoch: 7700] train loss: 0.3355, train acc: 0.8827, val loss: 0.2263, val acc: 0.9336  (best train acc: 0.9031, best val acc: 0.9379, best train loss: 0.2835  @ epoch 7633 )\n",
      "[Epoch: 7720] train loss: 0.3228, train acc: 0.8864, val loss: 0.2269, val acc: 0.9312  (best train acc: 0.9119, best val acc: 0.9379, best train loss: 0.2706  @ epoch 7712 )\n",
      "[Epoch: 7740] train loss: 0.3151, train acc: 0.8861, val loss: 0.2274, val acc: 0.9346  (best train acc: 0.9119, best val acc: 0.9379, best train loss: 0.2706  @ epoch 7712 )\n",
      "[Epoch: 7760] train loss: 0.3347, train acc: 0.8736, val loss: 0.2277, val acc: 0.9342  (best train acc: 0.9119, best val acc: 0.9390, best train loss: 0.2706  @ epoch 7712 )\n",
      "[Epoch: 7780] train loss: 0.2821, train acc: 0.9013, val loss: 0.2171, val acc: 0.9349  (best train acc: 0.9119, best val acc: 0.9390, best train loss: 0.2706  @ epoch 7712 )\n",
      "[Epoch: 7800] train loss: 0.2967, train acc: 0.8932, val loss: 0.2176, val acc: 0.9373  (best train acc: 0.9119, best val acc: 0.9390, best train loss: 0.2706  @ epoch 7712 )\n",
      "[Epoch: 7820] train loss: 0.3569, train acc: 0.8736, val loss: 0.2283, val acc: 0.9373  (best train acc: 0.9119, best val acc: 0.9400, best train loss: 0.2706  @ epoch 7712 )\n",
      "[Epoch: 7840] train loss: 0.3373, train acc: 0.8765, val loss: 0.2118, val acc: 0.9319  (best train acc: 0.9119, best val acc: 0.9400, best train loss: 0.2706  @ epoch 7712 )\n",
      "[Epoch: 7860] train loss: 0.3093, train acc: 0.8884, val loss: 0.2430, val acc: 0.9342  (best train acc: 0.9119, best val acc: 0.9400, best train loss: 0.2706  @ epoch 7712 )\n",
      "[Epoch: 7880] train loss: 0.2943, train acc: 0.9026, val loss: 0.2145, val acc: 0.9352  (best train acc: 0.9119, best val acc: 0.9400, best train loss: 0.2706  @ epoch 7712 )\n",
      "[Epoch: 7900] train loss: 0.3692, train acc: 0.8712, val loss: 0.2670, val acc: 0.9312  (best train acc: 0.9119, best val acc: 0.9400, best train loss: 0.2706  @ epoch 7712 )\n",
      "[Epoch: 7920] train loss: 0.3129, train acc: 0.8934, val loss: 0.2314, val acc: 0.9211  (best train acc: 0.9119, best val acc: 0.9400, best train loss: 0.2706  @ epoch 7712 )\n",
      "[Epoch: 7940] train loss: 0.3027, train acc: 0.8878, val loss: 0.2071, val acc: 0.9379  (best train acc: 0.9119, best val acc: 0.9400, best train loss: 0.2706  @ epoch 7712 )\n",
      "[Epoch: 7960] train loss: 0.2830, train acc: 0.9049, val loss: 0.2130, val acc: 0.9251  (best train acc: 0.9119, best val acc: 0.9413, best train loss: 0.2706  @ epoch 7712 )\n",
      "[Epoch: 7980] train loss: 0.2905, train acc: 0.8932, val loss: 0.2078, val acc: 0.9363  (best train acc: 0.9119, best val acc: 0.9420, best train loss: 0.2706  @ epoch 7712 )\n",
      "[Epoch: 8000] train loss: 0.2780, train acc: 0.8994, val loss: 0.2347, val acc: 0.9315  (best train acc: 0.9119, best val acc: 0.9430, best train loss: 0.2706  @ epoch 7712 )\n",
      "[Epoch: 8020] train loss: 0.2847, train acc: 0.8968, val loss: 0.2075, val acc: 0.9376  (best train acc: 0.9119, best val acc: 0.9430, best train loss: 0.2670  @ epoch 8009 )\n",
      "[Epoch: 8040] train loss: 0.2739, train acc: 0.9081, val loss: 0.2212, val acc: 0.9342  (best train acc: 0.9119, best val acc: 0.9430, best train loss: 0.2621  @ epoch 8031 )\n",
      "[Epoch: 8060] train loss: 0.3147, train acc: 0.8865, val loss: 0.2129, val acc: 0.9386  (best train acc: 0.9119, best val acc: 0.9430, best train loss: 0.2621  @ epoch 8031 )\n",
      "[Epoch: 8080] train loss: 0.3090, train acc: 0.8894, val loss: 0.2101, val acc: 0.9390  (best train acc: 0.9119, best val acc: 0.9430, best train loss: 0.2621  @ epoch 8031 )\n",
      "[Epoch: 8100] train loss: 0.3896, train acc: 0.8640, val loss: 0.2045, val acc: 0.9383  (best train acc: 0.9119, best val acc: 0.9430, best train loss: 0.2621  @ epoch 8031 )\n",
      "[Epoch: 8120] train loss: 0.2845, train acc: 0.9036, val loss: 0.1982, val acc: 0.9393  (best train acc: 0.9119, best val acc: 0.9430, best train loss: 0.2592  @ epoch 8112 )\n",
      "[Epoch: 8140] train loss: 0.3758, train acc: 0.8571, val loss: 0.2003, val acc: 0.9386  (best train acc: 0.9119, best val acc: 0.9433, best train loss: 0.2592  @ epoch 8112 )\n",
      "[Epoch: 8160] train loss: 0.3235, train acc: 0.8769, val loss: 0.2208, val acc: 0.9096  (best train acc: 0.9119, best val acc: 0.9433, best train loss: 0.2592  @ epoch 8112 )\n",
      "[Epoch: 8180] train loss: 0.2715, train acc: 0.9075, val loss: 0.2022, val acc: 0.9406  (best train acc: 0.9166, best val acc: 0.9437, best train loss: 0.2455  @ epoch 8179 )\n",
      "[Epoch: 8200] train loss: 0.2677, train acc: 0.9087, val loss: 0.2033, val acc: 0.9315  (best train acc: 0.9166, best val acc: 0.9437, best train loss: 0.2455  @ epoch 8179 )\n",
      "[Epoch: 8220] train loss: 0.2946, train acc: 0.9006, val loss: 0.2011, val acc: 0.9410  (best train acc: 0.9166, best val acc: 0.9457, best train loss: 0.2455  @ epoch 8179 )\n",
      "[Epoch: 8240] train loss: 0.2801, train acc: 0.8991, val loss: 0.2204, val acc: 0.9390  (best train acc: 0.9166, best val acc: 0.9457, best train loss: 0.2455  @ epoch 8179 )\n",
      "[Epoch: 8260] train loss: 0.2767, train acc: 0.9020, val loss: 0.2040, val acc: 0.9430  (best train acc: 0.9166, best val acc: 0.9457, best train loss: 0.2455  @ epoch 8179 )\n",
      "[Epoch: 8280] train loss: 0.2660, train acc: 0.9059, val loss: 0.1947, val acc: 0.9406  (best train acc: 0.9166, best val acc: 0.9457, best train loss: 0.2455  @ epoch 8179 )\n",
      "[Epoch: 8300] train loss: 0.2692, train acc: 0.9101, val loss: 0.1978, val acc: 0.9430  (best train acc: 0.9166, best val acc: 0.9457, best train loss: 0.2455  @ epoch 8179 )\n",
      "[Epoch: 8320] train loss: 0.2705, train acc: 0.9015, val loss: 0.1960, val acc: 0.9413  (best train acc: 0.9167, best val acc: 0.9457, best train loss: 0.2455  @ epoch 8179 )\n",
      "[Epoch: 8340] train loss: 0.2686, train acc: 0.9032, val loss: 0.1944, val acc: 0.9423  (best train acc: 0.9167, best val acc: 0.9460, best train loss: 0.2455  @ epoch 8179 )\n",
      "[Epoch: 8360] train loss: 0.3685, train acc: 0.8691, val loss: 0.2096, val acc: 0.9359  (best train acc: 0.9167, best val acc: 0.9460, best train loss: 0.2455  @ epoch 8179 )\n",
      "[Epoch: 8380] train loss: 0.2925, train acc: 0.8981, val loss: 0.2238, val acc: 0.9278  (best train acc: 0.9167, best val acc: 0.9460, best train loss: 0.2455  @ epoch 8179 )\n",
      "[Epoch: 8400] train loss: 0.2874, train acc: 0.8973, val loss: 0.2137, val acc: 0.9423  (best train acc: 0.9167, best val acc: 0.9460, best train loss: 0.2455  @ epoch 8179 )\n",
      "[Epoch: 8420] train loss: 0.2736, train acc: 0.9025, val loss: 0.1918, val acc: 0.9440  (best train acc: 0.9167, best val acc: 0.9460, best train loss: 0.2455  @ epoch 8179 )\n",
      "[Epoch: 8440] train loss: 0.2741, train acc: 0.9047, val loss: 0.2207, val acc: 0.9261  (best train acc: 0.9167, best val acc: 0.9460, best train loss: 0.2455  @ epoch 8179 )\n",
      "[Epoch: 8460] train loss: 0.2645, train acc: 0.9029, val loss: 0.1875, val acc: 0.9427  (best train acc: 0.9167, best val acc: 0.9460, best train loss: 0.2455  @ epoch 8179 )\n",
      "[Epoch: 8480] train loss: 0.2493, train acc: 0.9186, val loss: 0.1894, val acc: 0.9406  (best train acc: 0.9186, best val acc: 0.9460, best train loss: 0.2409  @ epoch 8461 )\n",
      "[Epoch: 8500] train loss: 0.2717, train acc: 0.8999, val loss: 0.1878, val acc: 0.9379  (best train acc: 0.9186, best val acc: 0.9460, best train loss: 0.2409  @ epoch 8461 )\n",
      "[Epoch: 8520] train loss: 0.3557, train acc: 0.8789, val loss: 0.1880, val acc: 0.9420  (best train acc: 0.9186, best val acc: 0.9460, best train loss: 0.2386  @ epoch 8514 )\n",
      "[Epoch: 8540] train loss: 0.2642, train acc: 0.9076, val loss: 0.1892, val acc: 0.9369  (best train acc: 0.9189, best val acc: 0.9481, best train loss: 0.2386  @ epoch 8514 )\n",
      "[Epoch: 8560] train loss: 0.2816, train acc: 0.8934, val loss: 0.1943, val acc: 0.9390  (best train acc: 0.9189, best val acc: 0.9481, best train loss: 0.2386  @ epoch 8514 )\n",
      "[Epoch: 8580] train loss: 0.2625, train acc: 0.9061, val loss: 0.1891, val acc: 0.9433  (best train acc: 0.9189, best val acc: 0.9481, best train loss: 0.2386  @ epoch 8514 )\n",
      "[Epoch: 8600] train loss: 0.2622, train acc: 0.9036, val loss: 0.1812, val acc: 0.9437  (best train acc: 0.9189, best val acc: 0.9481, best train loss: 0.2386  @ epoch 8514 )\n",
      "[Epoch: 8620] train loss: 0.2477, train acc: 0.9163, val loss: 0.1943, val acc: 0.9460  (best train acc: 0.9189, best val acc: 0.9481, best train loss: 0.2386  @ epoch 8514 )\n",
      "[Epoch: 8640] train loss: 0.2745, train acc: 0.8942, val loss: 0.1967, val acc: 0.9440  (best train acc: 0.9189, best val acc: 0.9481, best train loss: 0.2386  @ epoch 8514 )\n",
      "[Epoch: 8660] train loss: 0.2558, train acc: 0.9053, val loss: 0.1946, val acc: 0.9423  (best train acc: 0.9189, best val acc: 0.9481, best train loss: 0.2386  @ epoch 8514 )\n",
      "[Epoch: 8680] train loss: 0.2589, train acc: 0.9088, val loss: 0.1924, val acc: 0.9379  (best train acc: 0.9189, best val acc: 0.9481, best train loss: 0.2380  @ epoch 8668 )\n",
      "[Epoch: 8700] train loss: 0.2601, train acc: 0.9121, val loss: 0.1827, val acc: 0.9450  (best train acc: 0.9221, best val acc: 0.9481, best train loss: 0.2315  @ epoch 8689 )\n",
      "[Epoch: 8720] train loss: 0.2506, train acc: 0.9148, val loss: 0.1770, val acc: 0.9427  (best train acc: 0.9250, best val acc: 0.9481, best train loss: 0.2315  @ epoch 8689 )\n",
      "[Epoch: 8740] train loss: 0.2534, train acc: 0.9117, val loss: 0.1936, val acc: 0.9437  (best train acc: 0.9250, best val acc: 0.9487, best train loss: 0.2315  @ epoch 8689 )\n",
      "[Epoch: 8760] train loss: 0.2305, train acc: 0.9204, val loss: 0.1847, val acc: 0.9430  (best train acc: 0.9250, best val acc: 0.9487, best train loss: 0.2236  @ epoch 8747 )\n",
      "[Epoch: 8780] train loss: 0.2704, train acc: 0.8962, val loss: 0.1842, val acc: 0.9410  (best train acc: 0.9250, best val acc: 0.9487, best train loss: 0.2236  @ epoch 8747 )\n",
      "[Epoch: 8800] train loss: 0.2770, train acc: 0.9062, val loss: 0.2045, val acc: 0.9386  (best train acc: 0.9250, best val acc: 0.9487, best train loss: 0.2236  @ epoch 8747 )\n",
      "[Epoch: 8820] train loss: 0.2802, train acc: 0.9085, val loss: 0.1981, val acc: 0.9396  (best train acc: 0.9250, best val acc: 0.9487, best train loss: 0.2236  @ epoch 8747 )\n",
      "[Epoch: 8840] train loss: 0.2786, train acc: 0.9037, val loss: 0.2010, val acc: 0.9413  (best train acc: 0.9250, best val acc: 0.9487, best train loss: 0.2236  @ epoch 8747 )\n",
      "[Epoch: 8860] train loss: 0.2761, train acc: 0.9083, val loss: 0.2169, val acc: 0.9386  (best train acc: 0.9250, best val acc: 0.9487, best train loss: 0.2236  @ epoch 8747 )\n",
      "[Epoch: 8880] train loss: 0.2557, train acc: 0.9076, val loss: 0.1782, val acc: 0.9460  (best train acc: 0.9250, best val acc: 0.9487, best train loss: 0.2236  @ epoch 8747 )\n",
      "[Epoch: 8900] train loss: 0.2710, train acc: 0.8880, val loss: 0.1881, val acc: 0.9410  (best train acc: 0.9250, best val acc: 0.9487, best train loss: 0.2236  @ epoch 8747 )\n",
      "[Epoch: 8920] train loss: 0.2392, train acc: 0.9165, val loss: 0.1825, val acc: 0.9396  (best train acc: 0.9255, best val acc: 0.9487, best train loss: 0.2236  @ epoch 8747 )\n",
      "[Epoch: 8940] train loss: 0.2643, train acc: 0.9071, val loss: 0.1815, val acc: 0.9460  (best train acc: 0.9255, best val acc: 0.9487, best train loss: 0.2194  @ epoch 8925 )\n",
      "[Epoch: 8960] train loss: 0.2905, train acc: 0.8986, val loss: 0.2183, val acc: 0.9379  (best train acc: 0.9255, best val acc: 0.9487, best train loss: 0.2194  @ epoch 8925 )\n",
      "[Epoch: 8980] train loss: 0.2721, train acc: 0.8989, val loss: 0.1818, val acc: 0.9450  (best train acc: 0.9255, best val acc: 0.9487, best train loss: 0.2194  @ epoch 8925 )\n",
      "[Epoch: 9000] train loss: 0.2505, train acc: 0.9165, val loss: 0.1830, val acc: 0.9440  (best train acc: 0.9255, best val acc: 0.9487, best train loss: 0.2194  @ epoch 8925 )\n",
      "[Epoch: 9020] train loss: 0.3401, train acc: 0.8721, val loss: 0.1939, val acc: 0.9413  (best train acc: 0.9255, best val acc: 0.9487, best train loss: 0.2194  @ epoch 8925 )\n",
      "[Epoch: 9040] train loss: 0.2364, train acc: 0.9129, val loss: 0.1839, val acc: 0.9406  (best train acc: 0.9255, best val acc: 0.9491, best train loss: 0.2194  @ epoch 8925 )\n",
      "[Epoch: 9060] train loss: 0.2824, train acc: 0.8962, val loss: 0.1838, val acc: 0.9390  (best train acc: 0.9255, best val acc: 0.9491, best train loss: 0.2194  @ epoch 8925 )\n",
      "[Epoch: 9080] train loss: 0.3360, train acc: 0.8869, val loss: 0.1892, val acc: 0.9440  (best train acc: 0.9255, best val acc: 0.9491, best train loss: 0.2194  @ epoch 8925 )\n",
      "[Epoch: 9100] train loss: 0.2541, train acc: 0.9132, val loss: 0.2110, val acc: 0.9369  (best train acc: 0.9255, best val acc: 0.9501, best train loss: 0.2194  @ epoch 8925 )\n",
      "[Epoch: 9120] train loss: 0.2897, train acc: 0.8882, val loss: 0.1916, val acc: 0.9319  (best train acc: 0.9255, best val acc: 0.9501, best train loss: 0.2194  @ epoch 8925 )\n",
      "[Epoch: 9140] train loss: 0.2464, train acc: 0.9153, val loss: 0.1810, val acc: 0.9457  (best train acc: 0.9255, best val acc: 0.9501, best train loss: 0.2194  @ epoch 8925 )\n",
      "[Epoch: 9160] train loss: 0.2808, train acc: 0.8982, val loss: 0.1790, val acc: 0.9467  (best train acc: 0.9255, best val acc: 0.9501, best train loss: 0.2194  @ epoch 8925 )\n",
      "[Epoch: 9180] train loss: 0.3820, train acc: 0.8705, val loss: 0.1938, val acc: 0.9410  (best train acc: 0.9255, best val acc: 0.9501, best train loss: 0.2194  @ epoch 8925 )\n",
      "[Epoch: 9200] train loss: 0.2848, train acc: 0.8963, val loss: 0.1790, val acc: 0.9470  (best train acc: 0.9255, best val acc: 0.9501, best train loss: 0.2194  @ epoch 8925 )\n",
      "[Epoch: 9220] train loss: 0.2647, train acc: 0.9019, val loss: 0.1871, val acc: 0.9383  (best train acc: 0.9255, best val acc: 0.9501, best train loss: 0.2194  @ epoch 8925 )\n",
      "[Epoch: 9240] train loss: 0.2314, train acc: 0.9241, val loss: 0.1787, val acc: 0.9467  (best train acc: 0.9255, best val acc: 0.9501, best train loss: 0.2194  @ epoch 8925 )\n",
      "[Epoch: 9260] train loss: 0.2553, train acc: 0.9088, val loss: 0.2064, val acc: 0.9430  (best train acc: 0.9255, best val acc: 0.9504, best train loss: 0.2194  @ epoch 8925 )\n",
      "[Epoch: 9280] train loss: 0.2598, train acc: 0.9040, val loss: 0.1764, val acc: 0.9427  (best train acc: 0.9255, best val acc: 0.9504, best train loss: 0.2194  @ epoch 8925 )\n",
      "[Epoch: 9300] train loss: 0.2456, train acc: 0.9122, val loss: 0.1757, val acc: 0.9444  (best train acc: 0.9255, best val acc: 0.9504, best train loss: 0.2194  @ epoch 8925 )\n",
      "[Epoch: 9320] train loss: 0.2536, train acc: 0.9133, val loss: 0.1815, val acc: 0.9497  (best train acc: 0.9255, best val acc: 0.9504, best train loss: 0.2194  @ epoch 8925 )\n",
      "[Epoch: 9340] train loss: 0.2407, train acc: 0.9216, val loss: 0.1777, val acc: 0.9413  (best train acc: 0.9255, best val acc: 0.9504, best train loss: 0.2194  @ epoch 8925 )\n",
      "[Epoch: 9360] train loss: 0.2781, train acc: 0.8971, val loss: 0.2147, val acc: 0.9332  (best train acc: 0.9255, best val acc: 0.9504, best train loss: 0.2194  @ epoch 8925 )\n",
      "[Epoch: 9380] train loss: 0.2535, train acc: 0.9092, val loss: 0.1809, val acc: 0.9481  (best train acc: 0.9257, best val acc: 0.9504, best train loss: 0.2194  @ epoch 8925 )\n",
      "[Epoch: 9400] train loss: 0.2490, train acc: 0.9101, val loss: 0.1969, val acc: 0.9437  (best train acc: 0.9257, best val acc: 0.9514, best train loss: 0.2194  @ epoch 8925 )\n",
      "[Epoch: 9420] train loss: 0.2445, train acc: 0.9215, val loss: 0.1799, val acc: 0.9511  (best train acc: 0.9257, best val acc: 0.9518, best train loss: 0.2194  @ epoch 8925 )\n",
      "[Epoch: 9440] train loss: 0.2523, train acc: 0.9153, val loss: 0.1761, val acc: 0.9487  (best train acc: 0.9260, best val acc: 0.9518, best train loss: 0.2194  @ epoch 8925 )\n",
      "[Epoch: 9460] train loss: 0.2423, train acc: 0.9162, val loss: 0.1747, val acc: 0.9460  (best train acc: 0.9265, best val acc: 0.9518, best train loss: 0.2194  @ epoch 8925 )\n",
      "[Epoch: 9480] train loss: 0.2997, train acc: 0.8939, val loss: 0.2076, val acc: 0.9403  (best train acc: 0.9265, best val acc: 0.9518, best train loss: 0.2194  @ epoch 8925 )\n",
      "[Epoch: 9500] train loss: 0.2673, train acc: 0.9049, val loss: 0.1973, val acc: 0.9393  (best train acc: 0.9265, best val acc: 0.9518, best train loss: 0.2194  @ epoch 8925 )\n",
      "[Epoch: 9520] train loss: 0.2455, train acc: 0.9074, val loss: 0.2014, val acc: 0.9322  (best train acc: 0.9265, best val acc: 0.9518, best train loss: 0.2194  @ epoch 8925 )\n",
      "[Epoch: 9540] train loss: 0.2746, train acc: 0.8952, val loss: 0.1936, val acc: 0.9366  (best train acc: 0.9265, best val acc: 0.9518, best train loss: 0.2194  @ epoch 8925 )\n",
      "[Epoch: 9560] train loss: 0.2747, train acc: 0.8904, val loss: 0.1812, val acc: 0.9373  (best train acc: 0.9265, best val acc: 0.9518, best train loss: 0.2194  @ epoch 8925 )\n",
      "[Epoch: 9580] train loss: 0.2400, train acc: 0.9125, val loss: 0.1816, val acc: 0.9447  (best train acc: 0.9265, best val acc: 0.9518, best train loss: 0.2194  @ epoch 8925 )\n",
      "[Epoch: 9600] train loss: 0.3117, train acc: 0.8906, val loss: 0.1770, val acc: 0.9491  (best train acc: 0.9265, best val acc: 0.9524, best train loss: 0.2194  @ epoch 8925 )\n",
      "[Epoch: 9620] train loss: 0.2563, train acc: 0.9025, val loss: 0.1844, val acc: 0.9447  (best train acc: 0.9265, best val acc: 0.9524, best train loss: 0.2194  @ epoch 8925 )\n",
      "[Epoch: 9640] train loss: 0.2787, train acc: 0.8939, val loss: 0.1692, val acc: 0.9501  (best train acc: 0.9265, best val acc: 0.9524, best train loss: 0.2194  @ epoch 8925 )\n",
      "[Epoch: 9660] train loss: 0.2463, train acc: 0.9187, val loss: 0.1826, val acc: 0.9427  (best train acc: 0.9265, best val acc: 0.9524, best train loss: 0.2194  @ epoch 8925 )\n",
      "[Epoch: 9680] train loss: 0.2301, train acc: 0.9262, val loss: 0.1981, val acc: 0.9393  (best train acc: 0.9265, best val acc: 0.9524, best train loss: 0.2194  @ epoch 8925 )\n",
      "[Epoch: 9700] train loss: 0.3240, train acc: 0.8758, val loss: 0.1934, val acc: 0.9352  (best train acc: 0.9265, best val acc: 0.9524, best train loss: 0.2194  @ epoch 8925 )\n",
      "[Epoch: 9720] train loss: 0.2383, train acc: 0.9154, val loss: 0.1761, val acc: 0.9447  (best train acc: 0.9265, best val acc: 0.9524, best train loss: 0.2183  @ epoch 9718 )\n",
      "[Epoch: 9740] train loss: 0.2322, train acc: 0.9255, val loss: 0.1714, val acc: 0.9460  (best train acc: 0.9281, best val acc: 0.9524, best train loss: 0.2183  @ epoch 9718 )\n",
      "[Epoch: 9760] train loss: 0.2335, train acc: 0.9215, val loss: 0.1707, val acc: 0.9460  (best train acc: 0.9281, best val acc: 0.9524, best train loss: 0.2167  @ epoch 9759 )\n",
      "[Epoch: 9780] train loss: 0.2678, train acc: 0.8986, val loss: 0.1932, val acc: 0.9349  (best train acc: 0.9281, best val acc: 0.9524, best train loss: 0.2167  @ epoch 9759 )\n",
      "[Epoch: 9800] train loss: 0.2488, train acc: 0.9080, val loss: 0.1820, val acc: 0.9447  (best train acc: 0.9281, best val acc: 0.9524, best train loss: 0.2167  @ epoch 9759 )\n",
      "[Epoch: 9820] train loss: 0.2238, train acc: 0.9219, val loss: 0.1972, val acc: 0.9373  (best train acc: 0.9281, best val acc: 0.9524, best train loss: 0.2167  @ epoch 9759 )\n",
      "[Epoch: 9840] train loss: 0.2680, train acc: 0.8960, val loss: 0.1868, val acc: 0.9440  (best train acc: 0.9281, best val acc: 0.9524, best train loss: 0.2167  @ epoch 9759 )\n",
      "[Epoch: 9860] train loss: 0.2758, train acc: 0.8980, val loss: 0.1975, val acc: 0.9376  (best train acc: 0.9281, best val acc: 0.9524, best train loss: 0.2162  @ epoch 9848 )\n",
      "[Epoch: 9880] train loss: 0.2665, train acc: 0.9143, val loss: 0.1712, val acc: 0.9467  (best train acc: 0.9281, best val acc: 0.9524, best train loss: 0.2162  @ epoch 9848 )\n",
      "[Epoch: 9900] train loss: 0.2389, train acc: 0.9151, val loss: 0.1778, val acc: 0.9457  (best train acc: 0.9281, best val acc: 0.9524, best train loss: 0.2158  @ epoch 9881 )\n",
      "[Epoch: 9920] train loss: 0.2533, train acc: 0.9096, val loss: 0.1878, val acc: 0.9423  (best train acc: 0.9292, best val acc: 0.9524, best train loss: 0.2158  @ epoch 9881 )\n",
      "[Epoch: 9940] train loss: 0.2579, train acc: 0.9044, val loss: 0.1750, val acc: 0.9491  (best train acc: 0.9292, best val acc: 0.9524, best train loss: 0.2158  @ epoch 9881 )\n",
      "[Epoch: 9960] train loss: 0.2563, train acc: 0.9054, val loss: 0.1736, val acc: 0.9474  (best train acc: 0.9302, best val acc: 0.9524, best train loss: 0.2120  @ epoch 9959 )\n",
      "[Epoch: 9980] train loss: 0.2321, train acc: 0.9182, val loss: 0.1815, val acc: 0.9470  (best train acc: 0.9302, best val acc: 0.9524, best train loss: 0.2120  @ epoch 9959 )\n",
      "[Epoch: 10000] train loss: 0.2362, train acc: 0.9153, val loss: 0.1768, val acc: 0.9417  (best train acc: 0.9302, best val acc: 0.9524, best train loss: 0.2120  @ epoch 9959 )\n",
      "[Epoch: 10020] train loss: 0.2384, train acc: 0.9179, val loss: 0.1743, val acc: 0.9464  (best train acc: 0.9302, best val acc: 0.9524, best train loss: 0.2120  @ epoch 9959 )\n",
      "[Epoch: 10040] train loss: 0.2205, train acc: 0.9253, val loss: 0.1697, val acc: 0.9470  (best train acc: 0.9302, best val acc: 0.9524, best train loss: 0.2071  @ epoch 10037 )\n",
      "[Epoch: 10060] train loss: 0.2303, train acc: 0.9203, val loss: 0.1732, val acc: 0.9481  (best train acc: 0.9302, best val acc: 0.9524, best train loss: 0.2071  @ epoch 10037 )\n",
      "[Epoch: 10080] train loss: 0.2723, train acc: 0.9019, val loss: 0.1861, val acc: 0.9376  (best train acc: 0.9302, best val acc: 0.9524, best train loss: 0.2071  @ epoch 10037 )\n",
      "[Epoch: 10100] train loss: 0.2453, train acc: 0.9091, val loss: 0.1895, val acc: 0.9403  (best train acc: 0.9302, best val acc: 0.9524, best train loss: 0.2071  @ epoch 10037 )\n",
      "[Epoch: 10120] train loss: 0.2286, train acc: 0.9210, val loss: 0.1755, val acc: 0.9467  (best train acc: 0.9302, best val acc: 0.9524, best train loss: 0.2071  @ epoch 10037 )\n",
      "[Epoch: 10140] train loss: 0.2287, train acc: 0.9211, val loss: 0.1748, val acc: 0.9447  (best train acc: 0.9302, best val acc: 0.9524, best train loss: 0.2071  @ epoch 10037 )\n",
      "[Epoch: 10160] train loss: 0.2325, train acc: 0.9138, val loss: 0.1781, val acc: 0.9379  (best train acc: 0.9302, best val acc: 0.9531, best train loss: 0.2071  @ epoch 10037 )\n",
      "[Epoch: 10180] train loss: 0.2432, train acc: 0.9130, val loss: 0.1680, val acc: 0.9491  (best train acc: 0.9302, best val acc: 0.9531, best train loss: 0.2071  @ epoch 10037 )\n",
      "[Epoch: 10200] train loss: 0.2619, train acc: 0.9022, val loss: 0.1740, val acc: 0.9497  (best train acc: 0.9302, best val acc: 0.9531, best train loss: 0.2071  @ epoch 10037 )\n",
      "[Epoch: 10220] train loss: 0.2332, train acc: 0.9193, val loss: 0.1741, val acc: 0.9440  (best train acc: 0.9302, best val acc: 0.9531, best train loss: 0.2071  @ epoch 10037 )\n",
      "[Epoch: 10240] train loss: 0.2320, train acc: 0.9182, val loss: 0.1793, val acc: 0.9359  (best train acc: 0.9302, best val acc: 0.9531, best train loss: 0.2071  @ epoch 10037 )\n",
      "[Epoch: 10260] train loss: 0.3082, train acc: 0.8853, val loss: 0.1871, val acc: 0.9390  (best train acc: 0.9323, best val acc: 0.9531, best train loss: 0.2071  @ epoch 10037 )\n",
      "[Epoch: 10280] train loss: 0.2323, train acc: 0.9186, val loss: 0.1751, val acc: 0.9487  (best train acc: 0.9323, best val acc: 0.9531, best train loss: 0.2071  @ epoch 10037 )\n",
      "[Epoch: 10300] train loss: 0.2194, train acc: 0.9274, val loss: 0.1720, val acc: 0.9508  (best train acc: 0.9323, best val acc: 0.9531, best train loss: 0.2071  @ epoch 10037 )\n",
      "[Epoch: 10320] train loss: 0.2512, train acc: 0.9092, val loss: 0.1737, val acc: 0.9457  (best train acc: 0.9323, best val acc: 0.9531, best train loss: 0.2070  @ epoch 10313 )\n",
      "[Epoch: 10340] train loss: 0.2630, train acc: 0.9117, val loss: 0.1841, val acc: 0.9433  (best train acc: 0.9323, best val acc: 0.9531, best train loss: 0.2070  @ epoch 10313 )\n",
      "[Epoch: 10360] train loss: 0.2504, train acc: 0.9029, val loss: 0.1853, val acc: 0.9363  (best train acc: 0.9323, best val acc: 0.9531, best train loss: 0.2070  @ epoch 10313 )\n",
      "[Epoch: 10380] train loss: 0.2780, train acc: 0.9023, val loss: 0.1762, val acc: 0.9420  (best train acc: 0.9323, best val acc: 0.9531, best train loss: 0.2070  @ epoch 10313 )\n",
      "[Epoch: 10400] train loss: 0.2323, train acc: 0.9220, val loss: 0.1815, val acc: 0.9484  (best train acc: 0.9323, best val acc: 0.9531, best train loss: 0.2070  @ epoch 10313 )\n",
      "[Epoch: 10420] train loss: 0.2180, train acc: 0.9221, val loss: 0.1749, val acc: 0.9474  (best train acc: 0.9323, best val acc: 0.9531, best train loss: 0.2037  @ epoch 10412 )\n",
      "[Epoch: 10440] train loss: 0.2257, train acc: 0.9221, val loss: 0.1710, val acc: 0.9403  (best train acc: 0.9323, best val acc: 0.9531, best train loss: 0.2037  @ epoch 10412 )\n",
      "[Epoch: 10460] train loss: 0.2179, train acc: 0.9250, val loss: 0.1720, val acc: 0.9454  (best train acc: 0.9323, best val acc: 0.9531, best train loss: 0.2037  @ epoch 10412 )\n",
      "[Epoch: 10480] train loss: 0.2320, train acc: 0.9155, val loss: 0.1860, val acc: 0.9464  (best train acc: 0.9323, best val acc: 0.9531, best train loss: 0.2037  @ epoch 10412 )\n",
      "[Epoch: 10500] train loss: 0.2589, train acc: 0.9036, val loss: 0.1937, val acc: 0.9396  (best train acc: 0.9323, best val acc: 0.9531, best train loss: 0.2037  @ epoch 10412 )\n",
      "[Epoch: 10520] train loss: 0.2321, train acc: 0.9199, val loss: 0.1675, val acc: 0.9481  (best train acc: 0.9334, best val acc: 0.9531, best train loss: 0.1926  @ epoch 10508 )\n",
      "[Epoch: 10540] train loss: 0.2155, train acc: 0.9221, val loss: 0.1700, val acc: 0.9497  (best train acc: 0.9334, best val acc: 0.9531, best train loss: 0.1926  @ epoch 10508 )\n",
      "[Epoch: 10560] train loss: 0.2154, train acc: 0.9231, val loss: 0.1756, val acc: 0.9410  (best train acc: 0.9334, best val acc: 0.9531, best train loss: 0.1926  @ epoch 10508 )\n",
      "[Epoch: 10580] train loss: 0.2422, train acc: 0.9130, val loss: 0.1854, val acc: 0.9447  (best train acc: 0.9336, best val acc: 0.9531, best train loss: 0.1926  @ epoch 10508 )\n",
      "[Epoch: 10600] train loss: 0.2117, train acc: 0.9250, val loss: 0.1654, val acc: 0.9450  (best train acc: 0.9336, best val acc: 0.9531, best train loss: 0.1926  @ epoch 10508 )\n",
      "[Epoch: 10620] train loss: 0.2113, train acc: 0.9265, val loss: 0.1763, val acc: 0.9460  (best train acc: 0.9336, best val acc: 0.9531, best train loss: 0.1926  @ epoch 10508 )\n",
      "[Epoch: 10640] train loss: 0.2145, train acc: 0.9276, val loss: 0.1681, val acc: 0.9467  (best train acc: 0.9336, best val acc: 0.9531, best train loss: 0.1926  @ epoch 10508 )\n",
      "[Epoch: 10660] train loss: 0.2206, train acc: 0.9218, val loss: 0.1692, val acc: 0.9484  (best train acc: 0.9336, best val acc: 0.9531, best train loss: 0.1926  @ epoch 10508 )\n",
      "[Epoch: 10680] train loss: 0.2687, train acc: 0.9049, val loss: 0.1716, val acc: 0.9484  (best train acc: 0.9336, best val acc: 0.9531, best train loss: 0.1926  @ epoch 10508 )\n",
      "[Epoch: 10700] train loss: 0.2403, train acc: 0.9062, val loss: 0.1925, val acc: 0.9251  (best train acc: 0.9336, best val acc: 0.9531, best train loss: 0.1926  @ epoch 10508 )\n",
      "[Epoch: 10720] train loss: 0.2340, train acc: 0.9201, val loss: 0.1672, val acc: 0.9433  (best train acc: 0.9336, best val acc: 0.9531, best train loss: 0.1926  @ epoch 10508 )\n",
      "[Epoch: 10740] train loss: 0.2188, train acc: 0.9213, val loss: 0.1680, val acc: 0.9501  (best train acc: 0.9337, best val acc: 0.9535, best train loss: 0.1926  @ epoch 10508 )\n",
      "[Epoch: 10760] train loss: 0.2204, train acc: 0.9264, val loss: 0.1731, val acc: 0.9410  (best train acc: 0.9337, best val acc: 0.9535, best train loss: 0.1926  @ epoch 10508 )\n",
      "[Epoch: 10780] train loss: 0.2292, train acc: 0.9223, val loss: 0.1628, val acc: 0.9524  (best train acc: 0.9337, best val acc: 0.9535, best train loss: 0.1926  @ epoch 10508 )\n",
      "[Epoch: 10800] train loss: 0.2234, train acc: 0.9221, val loss: 0.1656, val acc: 0.9514  (best train acc: 0.9337, best val acc: 0.9535, best train loss: 0.1926  @ epoch 10508 )\n",
      "[Epoch: 10820] train loss: 0.2117, train acc: 0.9265, val loss: 0.1601, val acc: 0.9467  (best train acc: 0.9337, best val acc: 0.9535, best train loss: 0.1926  @ epoch 10508 )\n",
      "[Epoch: 10840] train loss: 0.2232, train acc: 0.9253, val loss: 0.1776, val acc: 0.9470  (best train acc: 0.9337, best val acc: 0.9535, best train loss: 0.1926  @ epoch 10508 )\n",
      "[Epoch: 10860] train loss: 0.2231, train acc: 0.9200, val loss: 0.1660, val acc: 0.9457  (best train acc: 0.9337, best val acc: 0.9535, best train loss: 0.1926  @ epoch 10508 )\n",
      "[Epoch: 10880] train loss: 0.2561, train acc: 0.9072, val loss: 0.1753, val acc: 0.9423  (best train acc: 0.9337, best val acc: 0.9535, best train loss: 0.1926  @ epoch 10508 )\n",
      "[Epoch: 10900] train loss: 0.2080, train acc: 0.9282, val loss: 0.1680, val acc: 0.9477  (best train acc: 0.9337, best val acc: 0.9535, best train loss: 0.1926  @ epoch 10508 )\n",
      "[Epoch: 10920] train loss: 0.2162, train acc: 0.9254, val loss: 0.1905, val acc: 0.9423  (best train acc: 0.9337, best val acc: 0.9535, best train loss: 0.1926  @ epoch 10508 )\n",
      "[Epoch: 10940] train loss: 0.2381, train acc: 0.9119, val loss: 0.1761, val acc: 0.9433  (best train acc: 0.9337, best val acc: 0.9538, best train loss: 0.1926  @ epoch 10508 )\n",
      "[Epoch: 10960] train loss: 0.2276, train acc: 0.9196, val loss: 0.1667, val acc: 0.9487  (best train acc: 0.9337, best val acc: 0.9541, best train loss: 0.1926  @ epoch 10508 )\n",
      "[Epoch: 10980] train loss: 0.2187, train acc: 0.9221, val loss: 0.1673, val acc: 0.9487  (best train acc: 0.9337, best val acc: 0.9541, best train loss: 0.1926  @ epoch 10508 )\n",
      "[Epoch: 11000] train loss: 0.2029, train acc: 0.9305, val loss: 0.1669, val acc: 0.9440  (best train acc: 0.9337, best val acc: 0.9541, best train loss: 0.1926  @ epoch 10508 )\n",
      "[Epoch: 11020] train loss: 0.2515, train acc: 0.9102, val loss: 0.1648, val acc: 0.9470  (best train acc: 0.9337, best val acc: 0.9541, best train loss: 0.1926  @ epoch 10508 )\n",
      "[Epoch: 11040] train loss: 0.2393, train acc: 0.9094, val loss: 0.1698, val acc: 0.9386  (best train acc: 0.9337, best val acc: 0.9541, best train loss: 0.1926  @ epoch 10508 )\n",
      "[Epoch: 11060] train loss: 0.2048, train acc: 0.9305, val loss: 0.1613, val acc: 0.9524  (best train acc: 0.9338, best val acc: 0.9548, best train loss: 0.1926  @ epoch 11056 )\n",
      "[Epoch: 11080] train loss: 0.2202, train acc: 0.9199, val loss: 0.1933, val acc: 0.9275  (best train acc: 0.9363, best val acc: 0.9548, best train loss: 0.1871  @ epoch 11063 )\n",
      "[Epoch: 11100] train loss: 0.2344, train acc: 0.9128, val loss: 0.1638, val acc: 0.9437  (best train acc: 0.9363, best val acc: 0.9548, best train loss: 0.1871  @ epoch 11063 )\n",
      "[Epoch: 11120] train loss: 0.2335, train acc: 0.9157, val loss: 0.1803, val acc: 0.9501  (best train acc: 0.9363, best val acc: 0.9548, best train loss: 0.1871  @ epoch 11063 )\n",
      "[Epoch: 11140] train loss: 0.2269, train acc: 0.9245, val loss: 0.1664, val acc: 0.9447  (best train acc: 0.9363, best val acc: 0.9548, best train loss: 0.1871  @ epoch 11063 )\n",
      "[Epoch: 11160] train loss: 0.1996, train acc: 0.9328, val loss: 0.1609, val acc: 0.9518  (best train acc: 0.9363, best val acc: 0.9548, best train loss: 0.1871  @ epoch 11063 )\n",
      "[Epoch: 11180] train loss: 0.2075, train acc: 0.9208, val loss: 0.1627, val acc: 0.9494  (best train acc: 0.9363, best val acc: 0.9548, best train loss: 0.1871  @ epoch 11063 )\n",
      "[Epoch: 11200] train loss: 0.2132, train acc: 0.9248, val loss: 0.1831, val acc: 0.9447  (best train acc: 0.9363, best val acc: 0.9548, best train loss: 0.1871  @ epoch 11063 )\n",
      "[Epoch: 11220] train loss: 0.2149, train acc: 0.9236, val loss: 0.2023, val acc: 0.9376  (best train acc: 0.9378, best val acc: 0.9548, best train loss: 0.1871  @ epoch 11063 )\n",
      "[Epoch: 11240] train loss: 0.2351, train acc: 0.9165, val loss: 0.1592, val acc: 0.9481  (best train acc: 0.9378, best val acc: 0.9548, best train loss: 0.1871  @ epoch 11063 )\n",
      "[Epoch: 11260] train loss: 0.1894, train acc: 0.9367, val loss: 0.1631, val acc: 0.9504  (best train acc: 0.9378, best val acc: 0.9548, best train loss: 0.1871  @ epoch 11063 )\n",
      "[Epoch: 11280] train loss: 0.2537, train acc: 0.9098, val loss: 0.1730, val acc: 0.9504  (best train acc: 0.9378, best val acc: 0.9548, best train loss: 0.1871  @ epoch 11063 )\n",
      "[Epoch: 11300] train loss: 0.2360, train acc: 0.9075, val loss: 0.1804, val acc: 0.9413  (best train acc: 0.9378, best val acc: 0.9548, best train loss: 0.1871  @ epoch 11063 )\n",
      "[Epoch: 11320] train loss: 0.2407, train acc: 0.9141, val loss: 0.1674, val acc: 0.9467  (best train acc: 0.9378, best val acc: 0.9548, best train loss: 0.1871  @ epoch 11063 )\n",
      "[Epoch: 11340] train loss: 0.2204, train acc: 0.9246, val loss: 0.1669, val acc: 0.9514  (best train acc: 0.9378, best val acc: 0.9548, best train loss: 0.1871  @ epoch 11063 )\n",
      "[Epoch: 11360] train loss: 0.2360, train acc: 0.9144, val loss: 0.1866, val acc: 0.9464  (best train acc: 0.9378, best val acc: 0.9548, best train loss: 0.1871  @ epoch 11063 )\n",
      "[Epoch: 11380] train loss: 0.2673, train acc: 0.8919, val loss: 0.1686, val acc: 0.9481  (best train acc: 0.9378, best val acc: 0.9548, best train loss: 0.1871  @ epoch 11063 )\n",
      "[Epoch: 11400] train loss: 0.2487, train acc: 0.9001, val loss: 0.1705, val acc: 0.9491  (best train acc: 0.9378, best val acc: 0.9548, best train loss: 0.1871  @ epoch 11063 )\n",
      "[Epoch: 11420] train loss: 0.2140, train acc: 0.9275, val loss: 0.1597, val acc: 0.9508  (best train acc: 0.9378, best val acc: 0.9548, best train loss: 0.1871  @ epoch 11063 )\n",
      "[Epoch: 11440] train loss: 0.2013, train acc: 0.9278, val loss: 0.1646, val acc: 0.9521  (best train acc: 0.9378, best val acc: 0.9548, best train loss: 0.1871  @ epoch 11063 )\n",
      "[Epoch: 11460] train loss: 0.2333, train acc: 0.9133, val loss: 0.1664, val acc: 0.9481  (best train acc: 0.9378, best val acc: 0.9548, best train loss: 0.1871  @ epoch 11063 )\n",
      "[Epoch: 11480] train loss: 0.2082, train acc: 0.9325, val loss: 0.1632, val acc: 0.9467  (best train acc: 0.9378, best val acc: 0.9548, best train loss: 0.1871  @ epoch 11063 )\n",
      "[Epoch: 11500] train loss: 0.2070, train acc: 0.9255, val loss: 0.1634, val acc: 0.9487  (best train acc: 0.9378, best val acc: 0.9548, best train loss: 0.1871  @ epoch 11063 )\n",
      "[Epoch: 11520] train loss: 0.2103, train acc: 0.9260, val loss: 0.1731, val acc: 0.9497  (best train acc: 0.9378, best val acc: 0.9548, best train loss: 0.1871  @ epoch 11063 )\n",
      "[Epoch: 11540] train loss: 0.2271, train acc: 0.9206, val loss: 0.1664, val acc: 0.9427  (best train acc: 0.9378, best val acc: 0.9548, best train loss: 0.1871  @ epoch 11063 )\n",
      "[Epoch: 11560] train loss: 0.1980, train acc: 0.9304, val loss: 0.1665, val acc: 0.9454  (best train acc: 0.9378, best val acc: 0.9548, best train loss: 0.1871  @ epoch 11063 )\n",
      "[Epoch: 11580] train loss: 0.2120, train acc: 0.9244, val loss: 0.1678, val acc: 0.9487  (best train acc: 0.9378, best val acc: 0.9548, best train loss: 0.1810  @ epoch 11568 )\n",
      "[Epoch: 11600] train loss: 0.2526, train acc: 0.9112, val loss: 0.1687, val acc: 0.9477  (best train acc: 0.9378, best val acc: 0.9548, best train loss: 0.1810  @ epoch 11568 )\n",
      "[Epoch: 11620] train loss: 0.2185, train acc: 0.9193, val loss: 0.1830, val acc: 0.9261  (best train acc: 0.9378, best val acc: 0.9548, best train loss: 0.1810  @ epoch 11568 )\n",
      "[Epoch: 11640] train loss: 0.2264, train acc: 0.9185, val loss: 0.1708, val acc: 0.9481  (best train acc: 0.9378, best val acc: 0.9548, best train loss: 0.1810  @ epoch 11568 )\n",
      "[Epoch: 11660] train loss: 0.2127, train acc: 0.9244, val loss: 0.1683, val acc: 0.9487  (best train acc: 0.9378, best val acc: 0.9548, best train loss: 0.1810  @ epoch 11568 )\n",
      "[Epoch: 11680] train loss: 0.2373, train acc: 0.9135, val loss: 0.1686, val acc: 0.9447  (best train acc: 0.9378, best val acc: 0.9548, best train loss: 0.1810  @ epoch 11568 )\n",
      "[Epoch: 11700] train loss: 0.1945, train acc: 0.9346, val loss: 0.1697, val acc: 0.9511  (best train acc: 0.9378, best val acc: 0.9548, best train loss: 0.1810  @ epoch 11568 )\n",
      "[Epoch: 11720] train loss: 0.2181, train acc: 0.9245, val loss: 0.1679, val acc: 0.9518  (best train acc: 0.9378, best val acc: 0.9548, best train loss: 0.1810  @ epoch 11568 )\n",
      "[Epoch: 11740] train loss: 0.2047, train acc: 0.9256, val loss: 0.1705, val acc: 0.9457  (best train acc: 0.9378, best val acc: 0.9551, best train loss: 0.1810  @ epoch 11568 )\n",
      "[Epoch: 11760] train loss: 0.2484, train acc: 0.9174, val loss: 0.1625, val acc: 0.9501  (best train acc: 0.9378, best val acc: 0.9551, best train loss: 0.1810  @ epoch 11568 )\n",
      "[Epoch: 11780] train loss: 0.2208, train acc: 0.9165, val loss: 0.1758, val acc: 0.9410  (best train acc: 0.9378, best val acc: 0.9551, best train loss: 0.1810  @ epoch 11568 )\n",
      "[Epoch: 11800] train loss: 0.2142, train acc: 0.9169, val loss: 0.1699, val acc: 0.9470  (best train acc: 0.9378, best val acc: 0.9551, best train loss: 0.1810  @ epoch 11568 )\n",
      "[Epoch: 11820] train loss: 0.2338, train acc: 0.9150, val loss: 0.1658, val acc: 0.9535  (best train acc: 0.9378, best val acc: 0.9551, best train loss: 0.1810  @ epoch 11568 )\n",
      "[Epoch: 11840] train loss: 0.1965, train acc: 0.9284, val loss: 0.1716, val acc: 0.9356  (best train acc: 0.9378, best val acc: 0.9551, best train loss: 0.1810  @ epoch 11568 )\n",
      "[Epoch: 11860] train loss: 0.2070, train acc: 0.9291, val loss: 0.1789, val acc: 0.9238  (best train acc: 0.9378, best val acc: 0.9551, best train loss: 0.1810  @ epoch 11568 )\n",
      "[Epoch: 11880] train loss: 0.2257, train acc: 0.9218, val loss: 0.1715, val acc: 0.9467  (best train acc: 0.9378, best val acc: 0.9551, best train loss: 0.1810  @ epoch 11568 )\n",
      "[Epoch: 11900] train loss: 0.2009, train acc: 0.9270, val loss: 0.1654, val acc: 0.9484  (best train acc: 0.9378, best val acc: 0.9551, best train loss: 0.1810  @ epoch 11568 )\n",
      "[Epoch: 11920] train loss: 0.2189, train acc: 0.9260, val loss: 0.1734, val acc: 0.9481  (best train acc: 0.9378, best val acc: 0.9551, best train loss: 0.1810  @ epoch 11568 )\n",
      "[Epoch: 11940] train loss: 0.2045, train acc: 0.9297, val loss: 0.1723, val acc: 0.9504  (best train acc: 0.9378, best val acc: 0.9551, best train loss: 0.1810  @ epoch 11568 )\n",
      "[Epoch: 11960] train loss: 0.2386, train acc: 0.9137, val loss: 0.1673, val acc: 0.9528  (best train acc: 0.9378, best val acc: 0.9551, best train loss: 0.1810  @ epoch 11568 )\n",
      "[Epoch: 11980] train loss: 0.1935, train acc: 0.9363, val loss: 0.1767, val acc: 0.9450  (best train acc: 0.9378, best val acc: 0.9551, best train loss: 0.1810  @ epoch 11568 )\n",
      "[Epoch: 12000] train loss: 0.2406, train acc: 0.9092, val loss: 0.1726, val acc: 0.9484  (best train acc: 0.9378, best val acc: 0.9551, best train loss: 0.1810  @ epoch 11568 )\n",
      "[Epoch: 12020] train loss: 0.2077, train acc: 0.9219, val loss: 0.1789, val acc: 0.9447  (best train acc: 0.9378, best val acc: 0.9551, best train loss: 0.1810  @ epoch 11568 )\n",
      "[Epoch: 12040] train loss: 0.2193, train acc: 0.9194, val loss: 0.1717, val acc: 0.9470  (best train acc: 0.9378, best val acc: 0.9551, best train loss: 0.1810  @ epoch 11568 )\n",
      "[Epoch: 12060] train loss: 0.2202, train acc: 0.9218, val loss: 0.1699, val acc: 0.9433  (best train acc: 0.9378, best val acc: 0.9551, best train loss: 0.1810  @ epoch 11568 )\n",
      "[Epoch: 12080] train loss: 0.1972, train acc: 0.9308, val loss: 0.1658, val acc: 0.9491  (best train acc: 0.9378, best val acc: 0.9551, best train loss: 0.1810  @ epoch 11568 )\n",
      "[Epoch: 12100] train loss: 0.2389, train acc: 0.9128, val loss: 0.1687, val acc: 0.9474  (best train acc: 0.9378, best val acc: 0.9551, best train loss: 0.1810  @ epoch 11568 )\n",
      "[Epoch: 12120] train loss: 0.2136, train acc: 0.9244, val loss: 0.1732, val acc: 0.9413  (best train acc: 0.9378, best val acc: 0.9551, best train loss: 0.1810  @ epoch 11568 )\n",
      "[Epoch: 12140] train loss: 0.2031, train acc: 0.9338, val loss: 0.1772, val acc: 0.9447  (best train acc: 0.9387, best val acc: 0.9551, best train loss: 0.1782  @ epoch 12130 )\n",
      "[Epoch: 12160] train loss: 0.2225, train acc: 0.9172, val loss: 0.1679, val acc: 0.9467  (best train acc: 0.9387, best val acc: 0.9551, best train loss: 0.1782  @ epoch 12130 )\n",
      "[Epoch: 12180] train loss: 0.2311, train acc: 0.9137, val loss: 0.1651, val acc: 0.9400  (best train acc: 0.9387, best val acc: 0.9558, best train loss: 0.1782  @ epoch 12130 )\n",
      "[Epoch: 12200] train loss: 0.2016, train acc: 0.9322, val loss: 0.1604, val acc: 0.9494  (best train acc: 0.9388, best val acc: 0.9558, best train loss: 0.1782  @ epoch 12130 )\n",
      "[Epoch: 12220] train loss: 0.2141, train acc: 0.9224, val loss: 0.1690, val acc: 0.9460  (best train acc: 0.9388, best val acc: 0.9558, best train loss: 0.1782  @ epoch 12130 )\n",
      "[Epoch: 12240] train loss: 0.2203, train acc: 0.9160, val loss: 0.1611, val acc: 0.9511  (best train acc: 0.9388, best val acc: 0.9558, best train loss: 0.1782  @ epoch 12130 )\n",
      "[Epoch: 12260] train loss: 0.2209, train acc: 0.9223, val loss: 0.1648, val acc: 0.9494  (best train acc: 0.9388, best val acc: 0.9558, best train loss: 0.1782  @ epoch 12130 )\n",
      "[Epoch: 12280] train loss: 0.2241, train acc: 0.9118, val loss: 0.1760, val acc: 0.9487  (best train acc: 0.9388, best val acc: 0.9558, best train loss: 0.1782  @ epoch 12130 )\n",
      "[Epoch: 12300] train loss: 0.2050, train acc: 0.9274, val loss: 0.1576, val acc: 0.9508  (best train acc: 0.9388, best val acc: 0.9558, best train loss: 0.1782  @ epoch 12130 )\n",
      "[Epoch: 12320] train loss: 0.2101, train acc: 0.9209, val loss: 0.1625, val acc: 0.9484  (best train acc: 0.9388, best val acc: 0.9558, best train loss: 0.1778  @ epoch 12315 )\n",
      "[Epoch: 12340] train loss: 0.2035, train acc: 0.9269, val loss: 0.1623, val acc: 0.9508  (best train acc: 0.9388, best val acc: 0.9558, best train loss: 0.1745  @ epoch 12331 )\n",
      "[Epoch: 12360] train loss: 0.2035, train acc: 0.9237, val loss: 0.1598, val acc: 0.9447  (best train acc: 0.9422, best val acc: 0.9558, best train loss: 0.1745  @ epoch 12350 )\n",
      "[Epoch: 12380] train loss: 0.1887, train acc: 0.9336, val loss: 0.1592, val acc: 0.9508  (best train acc: 0.9422, best val acc: 0.9558, best train loss: 0.1745  @ epoch 12350 )\n",
      "[Epoch: 12400] train loss: 0.2051, train acc: 0.9251, val loss: 0.1691, val acc: 0.9521  (best train acc: 0.9422, best val acc: 0.9558, best train loss: 0.1745  @ epoch 12350 )\n",
      "[Epoch: 12420] train loss: 0.2177, train acc: 0.9213, val loss: 0.1827, val acc: 0.9211  (best train acc: 0.9422, best val acc: 0.9558, best train loss: 0.1745  @ epoch 12350 )\n",
      "[Epoch: 12440] train loss: 0.2126, train acc: 0.9238, val loss: 0.2046, val acc: 0.9288  (best train acc: 0.9422, best val acc: 0.9558, best train loss: 0.1745  @ epoch 12350 )\n",
      "[Epoch: 12460] train loss: 0.2260, train acc: 0.9182, val loss: 0.1579, val acc: 0.9535  (best train acc: 0.9422, best val acc: 0.9558, best train loss: 0.1745  @ epoch 12350 )\n",
      "[Epoch: 12480] train loss: 0.1893, train acc: 0.9315, val loss: 0.1548, val acc: 0.9518  (best train acc: 0.9422, best val acc: 0.9558, best train loss: 0.1745  @ epoch 12350 )\n",
      "[Epoch: 12500] train loss: 0.2109, train acc: 0.9270, val loss: 0.1700, val acc: 0.9447  (best train acc: 0.9422, best val acc: 0.9558, best train loss: 0.1745  @ epoch 12350 )\n",
      "[Epoch: 12520] train loss: 0.1953, train acc: 0.9346, val loss: 0.1615, val acc: 0.9501  (best train acc: 0.9422, best val acc: 0.9558, best train loss: 0.1745  @ epoch 12350 )\n",
      "[Epoch: 12540] train loss: 0.2180, train acc: 0.9263, val loss: 0.1562, val acc: 0.9511  (best train acc: 0.9422, best val acc: 0.9558, best train loss: 0.1745  @ epoch 12350 )\n",
      "[Epoch: 12560] train loss: 0.1990, train acc: 0.9297, val loss: 0.1595, val acc: 0.9447  (best train acc: 0.9422, best val acc: 0.9558, best train loss: 0.1745  @ epoch 12350 )\n",
      "[Epoch: 12580] train loss: 0.2124, train acc: 0.9289, val loss: 0.1734, val acc: 0.9390  (best train acc: 0.9422, best val acc: 0.9562, best train loss: 0.1745  @ epoch 12350 )\n",
      "[Epoch: 12600] train loss: 0.2077, train acc: 0.9274, val loss: 0.1612, val acc: 0.9430  (best train acc: 0.9422, best val acc: 0.9562, best train loss: 0.1745  @ epoch 12350 )\n",
      "[Epoch: 12620] train loss: 0.2276, train acc: 0.9228, val loss: 0.1797, val acc: 0.9457  (best train acc: 0.9422, best val acc: 0.9562, best train loss: 0.1745  @ epoch 12350 )\n",
      "[Epoch: 12640] train loss: 0.2903, train acc: 0.8965, val loss: 0.2691, val acc: 0.9251  (best train acc: 0.9422, best val acc: 0.9562, best train loss: 0.1745  @ epoch 12350 )\n",
      "[Epoch: 12660] train loss: 0.2237, train acc: 0.9211, val loss: 0.1870, val acc: 0.9417  (best train acc: 0.9422, best val acc: 0.9562, best train loss: 0.1745  @ epoch 12350 )\n",
      "[Epoch: 12680] train loss: 0.2572, train acc: 0.9075, val loss: 0.1714, val acc: 0.9481  (best train acc: 0.9422, best val acc: 0.9562, best train loss: 0.1745  @ epoch 12350 )\n",
      "[Epoch: 12700] train loss: 0.1926, train acc: 0.9311, val loss: 0.1581, val acc: 0.9528  (best train acc: 0.9422, best val acc: 0.9562, best train loss: 0.1745  @ epoch 12350 )\n",
      "[Epoch: 12720] train loss: 0.2105, train acc: 0.9223, val loss: 0.1705, val acc: 0.9501  (best train acc: 0.9422, best val acc: 0.9562, best train loss: 0.1745  @ epoch 12350 )\n",
      "[Epoch: 12740] train loss: 0.2570, train acc: 0.9028, val loss: 0.1741, val acc: 0.9487  (best train acc: 0.9422, best val acc: 0.9562, best train loss: 0.1745  @ epoch 12350 )\n",
      "[Epoch: 12760] train loss: 0.2284, train acc: 0.9250, val loss: 0.1604, val acc: 0.9514  (best train acc: 0.9422, best val acc: 0.9562, best train loss: 0.1745  @ epoch 12350 )\n",
      "[Epoch: 12780] train loss: 0.2187, train acc: 0.9169, val loss: 0.1711, val acc: 0.9518  (best train acc: 0.9432, best val acc: 0.9562, best train loss: 0.1745  @ epoch 12350 )\n",
      "[Epoch: 12800] train loss: 0.2218, train acc: 0.9268, val loss: 0.1781, val acc: 0.9481  (best train acc: 0.9432, best val acc: 0.9562, best train loss: 0.1745  @ epoch 12350 )\n",
      "[Epoch: 12820] train loss: 0.2226, train acc: 0.9163, val loss: 0.2203, val acc: 0.9292  (best train acc: 0.9432, best val acc: 0.9562, best train loss: 0.1745  @ epoch 12350 )\n",
      "[Epoch: 12840] train loss: 0.2963, train acc: 0.8952, val loss: 0.2263, val acc: 0.9349  (best train acc: 0.9432, best val acc: 0.9562, best train loss: 0.1745  @ epoch 12350 )\n",
      "[Epoch: 12860] train loss: 0.3023, train acc: 0.9046, val loss: 0.2132, val acc: 0.9406  (best train acc: 0.9432, best val acc: 0.9562, best train loss: 0.1745  @ epoch 12350 )\n",
      "[Epoch: 12880] train loss: 0.2483, train acc: 0.9146, val loss: 0.1924, val acc: 0.9396  (best train acc: 0.9432, best val acc: 0.9562, best train loss: 0.1745  @ epoch 12350 )\n",
      "[Epoch: 12900] train loss: 0.2281, train acc: 0.9148, val loss: 0.1877, val acc: 0.9444  (best train acc: 0.9432, best val acc: 0.9562, best train loss: 0.1745  @ epoch 12350 )\n",
      "[Epoch: 12920] train loss: 0.2119, train acc: 0.9292, val loss: 0.1961, val acc: 0.9403  (best train acc: 0.9432, best val acc: 0.9562, best train loss: 0.1745  @ epoch 12350 )\n",
      "[Epoch: 12940] train loss: 0.2078, train acc: 0.9314, val loss: 0.1838, val acc: 0.9430  (best train acc: 0.9432, best val acc: 0.9562, best train loss: 0.1745  @ epoch 12350 )\n",
      "[Epoch: 12960] train loss: 0.2223, train acc: 0.9182, val loss: 0.1652, val acc: 0.9487  (best train acc: 0.9432, best val acc: 0.9562, best train loss: 0.1745  @ epoch 12350 )\n",
      "[Epoch: 12980] train loss: 0.1941, train acc: 0.9312, val loss: 0.1715, val acc: 0.9410  (best train acc: 0.9432, best val acc: 0.9562, best train loss: 0.1745  @ epoch 12350 )\n",
      "[Epoch: 13000] train loss: 0.1979, train acc: 0.9341, val loss: 0.1721, val acc: 0.9470  (best train acc: 0.9432, best val acc: 0.9562, best train loss: 0.1745  @ epoch 12350 )\n",
      "[Epoch: 13020] train loss: 0.2223, train acc: 0.9176, val loss: 0.1774, val acc: 0.9433  (best train acc: 0.9432, best val acc: 0.9562, best train loss: 0.1745  @ epoch 12350 )\n",
      "[Epoch: 13040] train loss: 0.2030, train acc: 0.9288, val loss: 0.1924, val acc: 0.9396  (best train acc: 0.9432, best val acc: 0.9562, best train loss: 0.1745  @ epoch 12350 )\n",
      "[Epoch: 13060] train loss: 0.2296, train acc: 0.9091, val loss: 0.1661, val acc: 0.9494  (best train acc: 0.9432, best val acc: 0.9562, best train loss: 0.1745  @ epoch 12350 )\n",
      "[Epoch: 13080] train loss: 0.2139, train acc: 0.9233, val loss: 0.1700, val acc: 0.9440  (best train acc: 0.9432, best val acc: 0.9562, best train loss: 0.1745  @ epoch 12350 )\n",
      "[Epoch: 13100] train loss: 0.2225, train acc: 0.9127, val loss: 0.1635, val acc: 0.9433  (best train acc: 0.9432, best val acc: 0.9562, best train loss: 0.1745  @ epoch 12350 )\n",
      "[Epoch: 13120] train loss: 0.1917, train acc: 0.9315, val loss: 0.1688, val acc: 0.9450  (best train acc: 0.9432, best val acc: 0.9562, best train loss: 0.1745  @ epoch 12350 )\n",
      "[Epoch: 13140] train loss: 0.2205, train acc: 0.9258, val loss: 0.1689, val acc: 0.9481  (best train acc: 0.9432, best val acc: 0.9562, best train loss: 0.1745  @ epoch 12350 )\n",
      "[Epoch: 13160] train loss: 0.2340, train acc: 0.9205, val loss: 0.1661, val acc: 0.9497  (best train acc: 0.9432, best val acc: 0.9562, best train loss: 0.1745  @ epoch 12350 )\n",
      "[Epoch: 13180] train loss: 0.1871, train acc: 0.9319, val loss: 0.1689, val acc: 0.9518  (best train acc: 0.9432, best val acc: 0.9562, best train loss: 0.1745  @ epoch 12350 )\n",
      "[Epoch: 13200] train loss: 0.1999, train acc: 0.9277, val loss: 0.1652, val acc: 0.9444  (best train acc: 0.9432, best val acc: 0.9562, best train loss: 0.1745  @ epoch 12350 )\n",
      "[Epoch: 13220] train loss: 0.2056, train acc: 0.9279, val loss: 0.1725, val acc: 0.9481  (best train acc: 0.9432, best val acc: 0.9562, best train loss: 0.1745  @ epoch 12350 )\n",
      "[Epoch: 13240] train loss: 0.1966, train acc: 0.9288, val loss: 0.1896, val acc: 0.9444  (best train acc: 0.9432, best val acc: 0.9562, best train loss: 0.1745  @ epoch 12350 )\n",
      "[Epoch: 13260] train loss: 0.2125, train acc: 0.9201, val loss: 0.1673, val acc: 0.9433  (best train acc: 0.9432, best val acc: 0.9562, best train loss: 0.1745  @ epoch 12350 )\n",
      "[Epoch: 13280] train loss: 0.2081, train acc: 0.9240, val loss: 0.1728, val acc: 0.9470  (best train acc: 0.9432, best val acc: 0.9562, best train loss: 0.1745  @ epoch 12350 )\n",
      "[Epoch: 13300] train loss: 0.2115, train acc: 0.9163, val loss: 0.1604, val acc: 0.9504  (best train acc: 0.9432, best val acc: 0.9562, best train loss: 0.1745  @ epoch 12350 )\n",
      "[Epoch: 13320] train loss: 0.2398, train acc: 0.9093, val loss: 0.1737, val acc: 0.9464  (best train acc: 0.9432, best val acc: 0.9562, best train loss: 0.1745  @ epoch 12350 )\n",
      "[Epoch: 13340] train loss: 0.2127, train acc: 0.9264, val loss: 0.1625, val acc: 0.9433  (best train acc: 0.9432, best val acc: 0.9562, best train loss: 0.1745  @ epoch 12350 )\n",
      "[Epoch: 13360] train loss: 0.1949, train acc: 0.9335, val loss: 0.1718, val acc: 0.9477  (best train acc: 0.9432, best val acc: 0.9562, best train loss: 0.1745  @ epoch 12350 )\n",
      "[Epoch: 13380] train loss: 0.2122, train acc: 0.9218, val loss: 0.1648, val acc: 0.9501  (best train acc: 0.9432, best val acc: 0.9562, best train loss: 0.1745  @ epoch 12350 )\n",
      "[Epoch: 13400] train loss: 0.2129, train acc: 0.9228, val loss: 0.1688, val acc: 0.9484  (best train acc: 0.9432, best val acc: 0.9585, best train loss: 0.1745  @ epoch 12350 )\n",
      "[Epoch: 13420] train loss: 0.2064, train acc: 0.9235, val loss: 0.1901, val acc: 0.9464  (best train acc: 0.9432, best val acc: 0.9585, best train loss: 0.1745  @ epoch 12350 )\n",
      "[Epoch: 13440] train loss: 0.2232, train acc: 0.9182, val loss: 0.1571, val acc: 0.9491  (best train acc: 0.9432, best val acc: 0.9585, best train loss: 0.1745  @ epoch 12350 )\n",
      "[Epoch: 13460] train loss: 0.1939, train acc: 0.9380, val loss: 0.2178, val acc: 0.9393  (best train acc: 0.9432, best val acc: 0.9585, best train loss: 0.1745  @ epoch 12350 )\n",
      "[Epoch: 13480] train loss: 0.2220, train acc: 0.9148, val loss: 0.1650, val acc: 0.9494  (best train acc: 0.9432, best val acc: 0.9585, best train loss: 0.1745  @ epoch 12350 )\n",
      "[Epoch: 13500] train loss: 0.2346, train acc: 0.9218, val loss: 0.1734, val acc: 0.9470  (best train acc: 0.9432, best val acc: 0.9585, best train loss: 0.1745  @ epoch 12350 )\n",
      "[Epoch: 13520] train loss: 0.1932, train acc: 0.9312, val loss: 0.1633, val acc: 0.9511  (best train acc: 0.9432, best val acc: 0.9585, best train loss: 0.1710  @ epoch 13510 )\n",
      "[Epoch: 13540] train loss: 0.1921, train acc: 0.9284, val loss: 0.1670, val acc: 0.9484  (best train acc: 0.9432, best val acc: 0.9585, best train loss: 0.1710  @ epoch 13510 )\n",
      "[Epoch: 13560] train loss: 0.2174, train acc: 0.9208, val loss: 0.1627, val acc: 0.9538  (best train acc: 0.9432, best val acc: 0.9585, best train loss: 0.1710  @ epoch 13510 )\n",
      "[Epoch: 13580] train loss: 0.1950, train acc: 0.9298, val loss: 0.1660, val acc: 0.9440  (best train acc: 0.9432, best val acc: 0.9585, best train loss: 0.1710  @ epoch 13510 )\n",
      "[Epoch: 13600] train loss: 0.1808, train acc: 0.9332, val loss: 0.1519, val acc: 0.9514  (best train acc: 0.9432, best val acc: 0.9585, best train loss: 0.1710  @ epoch 13510 )\n",
      "[Epoch: 13620] train loss: 0.1859, train acc: 0.9314, val loss: 0.1548, val acc: 0.9535  (best train acc: 0.9432, best val acc: 0.9585, best train loss: 0.1710  @ epoch 13510 )\n",
      "[Epoch: 13640] train loss: 0.1977, train acc: 0.9275, val loss: 0.1591, val acc: 0.9531  (best train acc: 0.9432, best val acc: 0.9585, best train loss: 0.1710  @ epoch 13510 )\n",
      "[Epoch: 13660] train loss: 0.1897, train acc: 0.9334, val loss: 0.1658, val acc: 0.9477  (best train acc: 0.9432, best val acc: 0.9585, best train loss: 0.1710  @ epoch 13510 )\n",
      "[Epoch: 13680] train loss: 0.1953, train acc: 0.9333, val loss: 0.1577, val acc: 0.9514  (best train acc: 0.9432, best val acc: 0.9585, best train loss: 0.1710  @ epoch 13510 )\n",
      "[Epoch: 13700] train loss: 0.1796, train acc: 0.9380, val loss: 0.1634, val acc: 0.9491  (best train acc: 0.9432, best val acc: 0.9585, best train loss: 0.1710  @ epoch 13510 )\n",
      "[Epoch: 13720] train loss: 0.1923, train acc: 0.9302, val loss: 0.1643, val acc: 0.9383  (best train acc: 0.9432, best val acc: 0.9585, best train loss: 0.1710  @ epoch 13510 )\n",
      "[Epoch: 13740] train loss: 0.2082, train acc: 0.9252, val loss: 0.1603, val acc: 0.9551  (best train acc: 0.9432, best val acc: 0.9585, best train loss: 0.1710  @ epoch 13510 )\n",
      "[Epoch: 13760] train loss: 0.1844, train acc: 0.9359, val loss: 0.1565, val acc: 0.9477  (best train acc: 0.9432, best val acc: 0.9585, best train loss: 0.1710  @ epoch 13510 )\n",
      "[Epoch: 13780] train loss: 0.2357, train acc: 0.9164, val loss: 0.1589, val acc: 0.9535  (best train acc: 0.9432, best val acc: 0.9585, best train loss: 0.1710  @ epoch 13510 )\n",
      "[Epoch: 13800] train loss: 0.1808, train acc: 0.9362, val loss: 0.1612, val acc: 0.9541  (best train acc: 0.9432, best val acc: 0.9585, best train loss: 0.1710  @ epoch 13510 )\n",
      "[Epoch: 13820] train loss: 0.2013, train acc: 0.9232, val loss: 0.1517, val acc: 0.9528  (best train acc: 0.9432, best val acc: 0.9585, best train loss: 0.1710  @ epoch 13510 )\n",
      "[Epoch: 13840] train loss: 0.1746, train acc: 0.9427, val loss: 0.1595, val acc: 0.9477  (best train acc: 0.9432, best val acc: 0.9585, best train loss: 0.1710  @ epoch 13510 )\n",
      "[Epoch: 13860] train loss: 0.2029, train acc: 0.9258, val loss: 0.1608, val acc: 0.9528  (best train acc: 0.9432, best val acc: 0.9585, best train loss: 0.1710  @ epoch 13510 )\n",
      "[Epoch: 13880] train loss: 0.2047, train acc: 0.9262, val loss: 0.1571, val acc: 0.9521  (best train acc: 0.9432, best val acc: 0.9585, best train loss: 0.1710  @ epoch 13510 )\n",
      "[Epoch: 13900] train loss: 0.1826, train acc: 0.9364, val loss: 0.1616, val acc: 0.9538  (best train acc: 0.9432, best val acc: 0.9585, best train loss: 0.1710  @ epoch 13510 )\n",
      "[Epoch: 13920] train loss: 0.1956, train acc: 0.9278, val loss: 0.1552, val acc: 0.9592  (best train acc: 0.9432, best val acc: 0.9592, best train loss: 0.1710  @ epoch 13510 )\n",
      "[Epoch: 13940] train loss: 0.1996, train acc: 0.9309, val loss: 0.1525, val acc: 0.9528  (best train acc: 0.9432, best val acc: 0.9592, best train loss: 0.1710  @ epoch 13510 )\n",
      "[Epoch: 13960] train loss: 0.1907, train acc: 0.9383, val loss: 0.1539, val acc: 0.9531  (best train acc: 0.9432, best val acc: 0.9592, best train loss: 0.1710  @ epoch 13510 )\n",
      "[Epoch: 13980] train loss: 0.1871, train acc: 0.9352, val loss: 0.1522, val acc: 0.9555  (best train acc: 0.9432, best val acc: 0.9592, best train loss: 0.1705  @ epoch 13979 )\n",
      "[Epoch: 14000] train loss: 0.2254, train acc: 0.9135, val loss: 0.1673, val acc: 0.9514  (best train acc: 0.9436, best val acc: 0.9592, best train loss: 0.1705  @ epoch 13979 )\n",
      "[Epoch: 14020] train loss: 0.2337, train acc: 0.9131, val loss: 0.1587, val acc: 0.9501  (best train acc: 0.9436, best val acc: 0.9592, best train loss: 0.1705  @ epoch 13979 )\n",
      "[Epoch: 14040] train loss: 0.1800, train acc: 0.9338, val loss: 0.1638, val acc: 0.9538  (best train acc: 0.9436, best val acc: 0.9592, best train loss: 0.1705  @ epoch 13979 )\n",
      "[Epoch: 14060] train loss: 0.1906, train acc: 0.9311, val loss: 0.1579, val acc: 0.9497  (best train acc: 0.9436, best val acc: 0.9592, best train loss: 0.1705  @ epoch 13979 )\n",
      "[Epoch: 14080] train loss: 0.1914, train acc: 0.9298, val loss: 0.1599, val acc: 0.9528  (best train acc: 0.9436, best val acc: 0.9592, best train loss: 0.1705  @ epoch 13979 )\n",
      "[Epoch: 14100] train loss: 0.1885, train acc: 0.9368, val loss: 0.1535, val acc: 0.9585  (best train acc: 0.9436, best val acc: 0.9592, best train loss: 0.1705  @ epoch 13979 )\n",
      "[Epoch: 14120] train loss: 0.1950, train acc: 0.9386, val loss: 0.1735, val acc: 0.9477  (best train acc: 0.9436, best val acc: 0.9592, best train loss: 0.1683  @ epoch 14115 )\n",
      "[Epoch: 14140] train loss: 0.1798, train acc: 0.9374, val loss: 0.1552, val acc: 0.9551  (best train acc: 0.9436, best val acc: 0.9592, best train loss: 0.1683  @ epoch 14115 )\n",
      "[Epoch: 14160] train loss: 0.1918, train acc: 0.9265, val loss: 0.1730, val acc: 0.9440  (best train acc: 0.9436, best val acc: 0.9592, best train loss: 0.1683  @ epoch 14115 )\n",
      "[Epoch: 14180] train loss: 0.2106, train acc: 0.9302, val loss: 0.1651, val acc: 0.9457  (best train acc: 0.9436, best val acc: 0.9592, best train loss: 0.1683  @ epoch 14115 )\n",
      "[Epoch: 14200] train loss: 0.1927, train acc: 0.9319, val loss: 0.1563, val acc: 0.9524  (best train acc: 0.9436, best val acc: 0.9592, best train loss: 0.1683  @ epoch 14115 )\n",
      "[Epoch: 14220] train loss: 0.1981, train acc: 0.9306, val loss: 0.1547, val acc: 0.9501  (best train acc: 0.9436, best val acc: 0.9592, best train loss: 0.1683  @ epoch 14115 )\n",
      "[Epoch: 14240] train loss: 0.1862, train acc: 0.9364, val loss: 0.1674, val acc: 0.9477  (best train acc: 0.9436, best val acc: 0.9592, best train loss: 0.1683  @ epoch 14115 )\n",
      "[Epoch: 14260] train loss: 0.2484, train acc: 0.9093, val loss: 0.1646, val acc: 0.9501  (best train acc: 0.9436, best val acc: 0.9592, best train loss: 0.1666  @ epoch 14248 )\n",
      "[Epoch: 14280] train loss: 0.2053, train acc: 0.9258, val loss: 0.1611, val acc: 0.9501  (best train acc: 0.9436, best val acc: 0.9592, best train loss: 0.1666  @ epoch 14248 )\n",
      "[Epoch: 14300] train loss: 0.2047, train acc: 0.9255, val loss: 0.1729, val acc: 0.9477  (best train acc: 0.9436, best val acc: 0.9592, best train loss: 0.1666  @ epoch 14248 )\n",
      "[Epoch: 14320] train loss: 0.2075, train acc: 0.9303, val loss: 0.1562, val acc: 0.9548  (best train acc: 0.9436, best val acc: 0.9592, best train loss: 0.1666  @ epoch 14248 )\n",
      "[Epoch: 14340] train loss: 0.1922, train acc: 0.9299, val loss: 0.1533, val acc: 0.9484  (best train acc: 0.9436, best val acc: 0.9592, best train loss: 0.1666  @ epoch 14248 )\n",
      "[Epoch: 14360] train loss: 0.2068, train acc: 0.9325, val loss: 0.1546, val acc: 0.9551  (best train acc: 0.9436, best val acc: 0.9592, best train loss: 0.1666  @ epoch 14248 )\n",
      "[Epoch: 14380] train loss: 0.1878, train acc: 0.9335, val loss: 0.1650, val acc: 0.9464  (best train acc: 0.9436, best val acc: 0.9592, best train loss: 0.1666  @ epoch 14248 )\n",
      "[Epoch: 14400] train loss: 0.2207, train acc: 0.9169, val loss: 0.1567, val acc: 0.9528  (best train acc: 0.9436, best val acc: 0.9592, best train loss: 0.1666  @ epoch 14248 )\n",
      "[Epoch: 14420] train loss: 0.1939, train acc: 0.9298, val loss: 0.1583, val acc: 0.9494  (best train acc: 0.9436, best val acc: 0.9592, best train loss: 0.1666  @ epoch 14248 )\n",
      "[Epoch: 14440] train loss: 0.1961, train acc: 0.9365, val loss: 0.1613, val acc: 0.9514  (best train acc: 0.9436, best val acc: 0.9592, best train loss: 0.1666  @ epoch 14248 )\n",
      "[Epoch: 14460] train loss: 0.2080, train acc: 0.9177, val loss: 0.1529, val acc: 0.9521  (best train acc: 0.9436, best val acc: 0.9592, best train loss: 0.1666  @ epoch 14248 )\n",
      "[Epoch: 14480] train loss: 0.1958, train acc: 0.9233, val loss: 0.1738, val acc: 0.9440  (best train acc: 0.9436, best val acc: 0.9592, best train loss: 0.1666  @ epoch 14248 )\n",
      "[Epoch: 14500] train loss: 0.2271, train acc: 0.9153, val loss: 0.1670, val acc: 0.9528  (best train acc: 0.9436, best val acc: 0.9592, best train loss: 0.1666  @ epoch 14248 )\n",
      "[Epoch: 14520] train loss: 0.2156, train acc: 0.9294, val loss: 0.1629, val acc: 0.9403  (best train acc: 0.9436, best val acc: 0.9592, best train loss: 0.1666  @ epoch 14248 )\n",
      "[Epoch: 14540] train loss: 0.1879, train acc: 0.9389, val loss: 0.1549, val acc: 0.9541  (best train acc: 0.9436, best val acc: 0.9592, best train loss: 0.1666  @ epoch 14248 )\n",
      "[Epoch: 14560] train loss: 0.2017, train acc: 0.9274, val loss: 0.1618, val acc: 0.9508  (best train acc: 0.9436, best val acc: 0.9592, best train loss: 0.1666  @ epoch 14248 )\n",
      "[Epoch: 14580] train loss: 0.1779, train acc: 0.9396, val loss: 0.1664, val acc: 0.9464  (best train acc: 0.9436, best val acc: 0.9592, best train loss: 0.1666  @ epoch 14248 )\n",
      "[Epoch: 14600] train loss: 0.1695, train acc: 0.9372, val loss: 0.1515, val acc: 0.9541  (best train acc: 0.9436, best val acc: 0.9592, best train loss: 0.1666  @ epoch 14248 )\n",
      "[Epoch: 14620] train loss: 0.2022, train acc: 0.9294, val loss: 0.1625, val acc: 0.9521  (best train acc: 0.9436, best val acc: 0.9592, best train loss: 0.1666  @ epoch 14248 )\n",
      "[Epoch: 14640] train loss: 0.1808, train acc: 0.9349, val loss: 0.1563, val acc: 0.9406  (best train acc: 0.9436, best val acc: 0.9592, best train loss: 0.1666  @ epoch 14248 )\n",
      "[Epoch: 14660] train loss: 0.1983, train acc: 0.9331, val loss: 0.1583, val acc: 0.9548  (best train acc: 0.9436, best val acc: 0.9592, best train loss: 0.1666  @ epoch 14248 )\n",
      "[Epoch: 14680] train loss: 0.2061, train acc: 0.9190, val loss: 0.1530, val acc: 0.9541  (best train acc: 0.9436, best val acc: 0.9592, best train loss: 0.1666  @ epoch 14248 )\n",
      "[Epoch: 14700] train loss: 0.1943, train acc: 0.9269, val loss: 0.1642, val acc: 0.9501  (best train acc: 0.9436, best val acc: 0.9592, best train loss: 0.1666  @ epoch 14248 )\n",
      "[Epoch: 14720] train loss: 0.1989, train acc: 0.9263, val loss: 0.1582, val acc: 0.9535  (best train acc: 0.9436, best val acc: 0.9592, best train loss: 0.1666  @ epoch 14248 )\n",
      "[Epoch: 14740] train loss: 0.2074, train acc: 0.9313, val loss: 0.1565, val acc: 0.9508  (best train acc: 0.9436, best val acc: 0.9592, best train loss: 0.1666  @ epoch 14248 )\n",
      "[Epoch: 14760] train loss: 0.1881, train acc: 0.9297, val loss: 0.1530, val acc: 0.9528  (best train acc: 0.9436, best val acc: 0.9592, best train loss: 0.1666  @ epoch 14248 )\n",
      "[Epoch: 14780] train loss: 0.1916, train acc: 0.9299, val loss: 0.1548, val acc: 0.9508  (best train acc: 0.9436, best val acc: 0.9592, best train loss: 0.1666  @ epoch 14248 )\n",
      "[Epoch: 14800] train loss: 0.1783, train acc: 0.9344, val loss: 0.1558, val acc: 0.9396  (best train acc: 0.9436, best val acc: 0.9592, best train loss: 0.1666  @ epoch 14248 )\n",
      "[Epoch: 14820] train loss: 0.1972, train acc: 0.9277, val loss: 0.1535, val acc: 0.9470  (best train acc: 0.9436, best val acc: 0.9592, best train loss: 0.1666  @ epoch 14248 )\n",
      "[Epoch: 14840] train loss: 0.1983, train acc: 0.9240, val loss: 0.1672, val acc: 0.9484  (best train acc: 0.9436, best val acc: 0.9592, best train loss: 0.1666  @ epoch 14248 )\n",
      "[Epoch: 14860] train loss: 0.2444, train acc: 0.9151, val loss: 0.1500, val acc: 0.9518  (best train acc: 0.9436, best val acc: 0.9592, best train loss: 0.1666  @ epoch 14248 )\n",
      "[Epoch: 14880] train loss: 0.3020, train acc: 0.8929, val loss: 0.1597, val acc: 0.9518  (best train acc: 0.9436, best val acc: 0.9592, best train loss: 0.1666  @ epoch 14248 )\n",
      "[Epoch: 14900] train loss: 0.2031, train acc: 0.9235, val loss: 0.1582, val acc: 0.9501  (best train acc: 0.9436, best val acc: 0.9592, best train loss: 0.1666  @ epoch 14248 )\n",
      "[Epoch: 14920] train loss: 0.1884, train acc: 0.9357, val loss: 0.1562, val acc: 0.9508  (best train acc: 0.9436, best val acc: 0.9592, best train loss: 0.1644  @ epoch 14918 )\n",
      "[Epoch: 14940] train loss: 0.1776, train acc: 0.9373, val loss: 0.1542, val acc: 0.9562  (best train acc: 0.9436, best val acc: 0.9592, best train loss: 0.1644  @ epoch 14918 )\n",
      "[Epoch: 14960] train loss: 0.1884, train acc: 0.9304, val loss: 0.1527, val acc: 0.9551  (best train acc: 0.9436, best val acc: 0.9592, best train loss: 0.1644  @ epoch 14918 )\n",
      "[Epoch: 14980] train loss: 0.2171, train acc: 0.9225, val loss: 0.1568, val acc: 0.9558  (best train acc: 0.9436, best val acc: 0.9592, best train loss: 0.1644  @ epoch 14918 )\n",
      "[Epoch: 15000] train loss: 0.2666, train acc: 0.8921, val loss: 0.1631, val acc: 0.9386  (best train acc: 0.9436, best val acc: 0.9592, best train loss: 0.1644  @ epoch 14918 )\n",
      "[Epoch: 15020] train loss: 0.1840, train acc: 0.9359, val loss: 0.1553, val acc: 0.9558  (best train acc: 0.9436, best val acc: 0.9592, best train loss: 0.1644  @ epoch 14918 )\n",
      "[Epoch: 15040] train loss: 0.1829, train acc: 0.9365, val loss: 0.1668, val acc: 0.9484  (best train acc: 0.9436, best val acc: 0.9592, best train loss: 0.1644  @ epoch 14918 )\n",
      "[Epoch: 15060] train loss: 0.2067, train acc: 0.9255, val loss: 0.1683, val acc: 0.9474  (best train acc: 0.9436, best val acc: 0.9592, best train loss: 0.1644  @ epoch 14918 )\n",
      "[Epoch: 15080] train loss: 0.2298, train acc: 0.9166, val loss: 0.1589, val acc: 0.9508  (best train acc: 0.9436, best val acc: 0.9592, best train loss: 0.1644  @ epoch 14918 )\n",
      "[Epoch: 15100] train loss: 0.1906, train acc: 0.9369, val loss: 0.1641, val acc: 0.9535  (best train acc: 0.9436, best val acc: 0.9592, best train loss: 0.1644  @ epoch 14918 )\n",
      "[Epoch: 15120] train loss: 0.2221, train acc: 0.9103, val loss: 0.1844, val acc: 0.9383  (best train acc: 0.9436, best val acc: 0.9592, best train loss: 0.1644  @ epoch 14918 )\n",
      "[Epoch: 15140] train loss: 0.1982, train acc: 0.9301, val loss: 0.1499, val acc: 0.9565  (best train acc: 0.9436, best val acc: 0.9592, best train loss: 0.1644  @ epoch 14918 )\n",
      "[Epoch: 15160] train loss: 0.1788, train acc: 0.9389, val loss: 0.1640, val acc: 0.9541  (best train acc: 0.9436, best val acc: 0.9592, best train loss: 0.1644  @ epoch 14918 )\n",
      "[Epoch: 15180] train loss: 0.1714, train acc: 0.9371, val loss: 0.1511, val acc: 0.9518  (best train acc: 0.9436, best val acc: 0.9592, best train loss: 0.1644  @ epoch 14918 )\n",
      "[Epoch: 15200] train loss: 0.1913, train acc: 0.9359, val loss: 0.1797, val acc: 0.9450  (best train acc: 0.9436, best val acc: 0.9592, best train loss: 0.1644  @ epoch 14918 )\n",
      "[Epoch: 15220] train loss: 0.1758, train acc: 0.9349, val loss: 0.1495, val acc: 0.9521  (best train acc: 0.9436, best val acc: 0.9592, best train loss: 0.1644  @ epoch 14918 )\n",
      "[Epoch: 15240] train loss: 0.1970, train acc: 0.9253, val loss: 0.1579, val acc: 0.9494  (best train acc: 0.9436, best val acc: 0.9592, best train loss: 0.1644  @ epoch 14918 )\n",
      "[Epoch: 15260] train loss: 0.1886, train acc: 0.9333, val loss: 0.1548, val acc: 0.9541  (best train acc: 0.9436, best val acc: 0.9592, best train loss: 0.1644  @ epoch 14918 )\n",
      "[Epoch: 15280] train loss: 0.1982, train acc: 0.9343, val loss: 0.1564, val acc: 0.9487  (best train acc: 0.9436, best val acc: 0.9592, best train loss: 0.1644  @ epoch 14918 )\n",
      "[Epoch: 15300] train loss: 0.2256, train acc: 0.9206, val loss: 0.1660, val acc: 0.9501  (best train acc: 0.9436, best val acc: 0.9592, best train loss: 0.1644  @ epoch 14918 )\n",
      "[Epoch: 15320] train loss: 0.1769, train acc: 0.9403, val loss: 0.1553, val acc: 0.9501  (best train acc: 0.9436, best val acc: 0.9592, best train loss: 0.1644  @ epoch 14918 )\n",
      "[Epoch: 15340] train loss: 0.1849, train acc: 0.9357, val loss: 0.1533, val acc: 0.9578  (best train acc: 0.9436, best val acc: 0.9592, best train loss: 0.1644  @ epoch 14918 )\n",
      "[Epoch: 15360] train loss: 0.1986, train acc: 0.9269, val loss: 0.1580, val acc: 0.9464  (best train acc: 0.9436, best val acc: 0.9592, best train loss: 0.1644  @ epoch 14918 )\n",
      "[Epoch: 15380] train loss: 0.1926, train acc: 0.9338, val loss: 0.1552, val acc: 0.9501  (best train acc: 0.9436, best val acc: 0.9592, best train loss: 0.1644  @ epoch 14918 )\n",
      "[Epoch: 15400] train loss: 0.1946, train acc: 0.9295, val loss: 0.1571, val acc: 0.9487  (best train acc: 0.9445, best val acc: 0.9592, best train loss: 0.1641  @ epoch 15383 )\n",
      "[Epoch: 15420] train loss: 0.1830, train acc: 0.9346, val loss: 0.1538, val acc: 0.9575  (best train acc: 0.9445, best val acc: 0.9592, best train loss: 0.1641  @ epoch 15383 )\n",
      "[Epoch: 15440] train loss: 0.1688, train acc: 0.9441, val loss: 0.1654, val acc: 0.9484  (best train acc: 0.9445, best val acc: 0.9592, best train loss: 0.1641  @ epoch 15383 )\n",
      "[Epoch: 15460] train loss: 0.1661, train acc: 0.9412, val loss: 0.1568, val acc: 0.9484  (best train acc: 0.9445, best val acc: 0.9592, best train loss: 0.1641  @ epoch 15383 )\n",
      "[Epoch: 15480] train loss: 0.1664, train acc: 0.9435, val loss: 0.1513, val acc: 0.9551  (best train acc: 0.9445, best val acc: 0.9592, best train loss: 0.1641  @ epoch 15383 )\n",
      "[Epoch: 15500] train loss: 0.2102, train acc: 0.9224, val loss: 0.1556, val acc: 0.9457  (best train acc: 0.9445, best val acc: 0.9592, best train loss: 0.1641  @ epoch 15383 )\n",
      "[Epoch: 15520] train loss: 0.2121, train acc: 0.9262, val loss: 0.1598, val acc: 0.9548  (best train acc: 0.9445, best val acc: 0.9592, best train loss: 0.1641  @ epoch 15383 )\n",
      "[Epoch: 15540] train loss: 0.2066, train acc: 0.9273, val loss: 0.1691, val acc: 0.9481  (best train acc: 0.9445, best val acc: 0.9592, best train loss: 0.1641  @ epoch 15383 )\n",
      "[Epoch: 15560] train loss: 0.1740, train acc: 0.9369, val loss: 0.1583, val acc: 0.9521  (best train acc: 0.9445, best val acc: 0.9592, best train loss: 0.1641  @ epoch 15383 )\n",
      "[Epoch: 15580] train loss: 0.1839, train acc: 0.9331, val loss: 0.1579, val acc: 0.9555  (best train acc: 0.9445, best val acc: 0.9592, best train loss: 0.1641  @ epoch 15383 )\n",
      "[Epoch: 15600] train loss: 0.1837, train acc: 0.9312, val loss: 0.1551, val acc: 0.9551  (best train acc: 0.9445, best val acc: 0.9592, best train loss: 0.1641  @ epoch 15383 )\n",
      "[Epoch: 15620] train loss: 0.2074, train acc: 0.9192, val loss: 0.1513, val acc: 0.9508  (best train acc: 0.9445, best val acc: 0.9592, best train loss: 0.1641  @ epoch 15383 )\n",
      "[Epoch: 15640] train loss: 0.1964, train acc: 0.9327, val loss: 0.1486, val acc: 0.9497  (best train acc: 0.9445, best val acc: 0.9592, best train loss: 0.1641  @ epoch 15383 )\n",
      "[Epoch: 15660] train loss: 0.2018, train acc: 0.9256, val loss: 0.1674, val acc: 0.9541  (best train acc: 0.9445, best val acc: 0.9592, best train loss: 0.1641  @ epoch 15383 )\n",
      "[Epoch: 15680] train loss: 0.1813, train acc: 0.9361, val loss: 0.1588, val acc: 0.9531  (best train acc: 0.9445, best val acc: 0.9602, best train loss: 0.1641  @ epoch 15383 )\n",
      "[Epoch: 15700] train loss: 0.1865, train acc: 0.9260, val loss: 0.1501, val acc: 0.9484  (best train acc: 0.9445, best val acc: 0.9602, best train loss: 0.1641  @ epoch 15383 )\n",
      "[Epoch: 15720] train loss: 0.1934, train acc: 0.9297, val loss: 0.1583, val acc: 0.9501  (best train acc: 0.9445, best val acc: 0.9602, best train loss: 0.1641  @ epoch 15383 )\n",
      "[Epoch: 15740] train loss: 0.1661, train acc: 0.9427, val loss: 0.1463, val acc: 0.9524  (best train acc: 0.9449, best val acc: 0.9602, best train loss: 0.1641  @ epoch 15383 )\n",
      "[Epoch: 15760] train loss: 0.1783, train acc: 0.9342, val loss: 0.1613, val acc: 0.9524  (best train acc: 0.9449, best val acc: 0.9602, best train loss: 0.1641  @ epoch 15383 )\n",
      "[Epoch: 15780] train loss: 0.1919, train acc: 0.9297, val loss: 0.1744, val acc: 0.9450  (best train acc: 0.9449, best val acc: 0.9602, best train loss: 0.1641  @ epoch 15383 )\n",
      "[Epoch: 15800] train loss: 0.1895, train acc: 0.9329, val loss: 0.1616, val acc: 0.9518  (best train acc: 0.9449, best val acc: 0.9602, best train loss: 0.1641  @ epoch 15383 )\n",
      "[Epoch: 15820] train loss: 0.2044, train acc: 0.9239, val loss: 0.1777, val acc: 0.9363  (best train acc: 0.9449, best val acc: 0.9602, best train loss: 0.1641  @ epoch 15383 )\n",
      "[Epoch: 15840] train loss: 0.2368, train acc: 0.9184, val loss: 0.1566, val acc: 0.9528  (best train acc: 0.9449, best val acc: 0.9602, best train loss: 0.1641  @ epoch 15383 )\n",
      "[Epoch: 15860] train loss: 0.1899, train acc: 0.9333, val loss: 0.1497, val acc: 0.9572  (best train acc: 0.9449, best val acc: 0.9602, best train loss: 0.1641  @ epoch 15383 )\n",
      "[Epoch: 15880] train loss: 0.2252, train acc: 0.9215, val loss: 0.1527, val acc: 0.9555  (best train acc: 0.9449, best val acc: 0.9602, best train loss: 0.1641  @ epoch 15383 )\n",
      "[Epoch: 15900] train loss: 0.1747, train acc: 0.9377, val loss: 0.1598, val acc: 0.9518  (best train acc: 0.9449, best val acc: 0.9602, best train loss: 0.1641  @ epoch 15383 )\n",
      "[Epoch: 15920] train loss: 0.2250, train acc: 0.9208, val loss: 0.1855, val acc: 0.9497  (best train acc: 0.9449, best val acc: 0.9602, best train loss: 0.1641  @ epoch 15383 )\n",
      "[Epoch: 15940] train loss: 0.1862, train acc: 0.9358, val loss: 0.1563, val acc: 0.9494  (best train acc: 0.9449, best val acc: 0.9602, best train loss: 0.1641  @ epoch 15383 )\n",
      "[Epoch: 15960] train loss: 0.1784, train acc: 0.9335, val loss: 0.1600, val acc: 0.9501  (best train acc: 0.9449, best val acc: 0.9602, best train loss: 0.1641  @ epoch 15383 )\n",
      "[Epoch: 15980] train loss: 0.2204, train acc: 0.9156, val loss: 0.1576, val acc: 0.9447  (best train acc: 0.9449, best val acc: 0.9602, best train loss: 0.1641  @ epoch 15383 )\n",
      "[Epoch: 16000] train loss: 0.2065, train acc: 0.9245, val loss: 0.1547, val acc: 0.9555  (best train acc: 0.9449, best val acc: 0.9602, best train loss: 0.1641  @ epoch 15383 )\n",
      "[Epoch: 16020] train loss: 0.1906, train acc: 0.9388, val loss: 0.1741, val acc: 0.9481  (best train acc: 0.9449, best val acc: 0.9602, best train loss: 0.1641  @ epoch 15383 )\n",
      "[Epoch: 16040] train loss: 0.1837, train acc: 0.9367, val loss: 0.1574, val acc: 0.9524  (best train acc: 0.9449, best val acc: 0.9602, best train loss: 0.1624  @ epoch 16033 )\n",
      "[Epoch: 16060] train loss: 0.1757, train acc: 0.9395, val loss: 0.1605, val acc: 0.9511  (best train acc: 0.9449, best val acc: 0.9602, best train loss: 0.1624  @ epoch 16033 )\n",
      "[Epoch: 16080] train loss: 0.1709, train acc: 0.9398, val loss: 0.1603, val acc: 0.9548  (best train acc: 0.9449, best val acc: 0.9602, best train loss: 0.1624  @ epoch 16033 )\n",
      "[Epoch: 16100] train loss: 0.1767, train acc: 0.9385, val loss: 0.1509, val acc: 0.9538  (best train acc: 0.9449, best val acc: 0.9605, best train loss: 0.1624  @ epoch 16033 )\n",
      "[Epoch: 16120] train loss: 0.1745, train acc: 0.9391, val loss: 0.1520, val acc: 0.9521  (best train acc: 0.9449, best val acc: 0.9605, best train loss: 0.1624  @ epoch 16033 )\n",
      "[Epoch: 16140] train loss: 0.1842, train acc: 0.9364, val loss: 0.1523, val acc: 0.9535  (best train acc: 0.9449, best val acc: 0.9605, best train loss: 0.1624  @ epoch 16033 )\n",
      "[Epoch: 16160] train loss: 0.2131, train acc: 0.9174, val loss: 0.1650, val acc: 0.9420  (best train acc: 0.9449, best val acc: 0.9605, best train loss: 0.1624  @ epoch 16033 )\n",
      "[Epoch: 16180] train loss: 0.1876, train acc: 0.9335, val loss: 0.1560, val acc: 0.9578  (best train acc: 0.9449, best val acc: 0.9605, best train loss: 0.1624  @ epoch 16033 )\n",
      "[Epoch: 16200] train loss: 0.2101, train acc: 0.9268, val loss: 0.1548, val acc: 0.9524  (best train acc: 0.9449, best val acc: 0.9605, best train loss: 0.1624  @ epoch 16033 )\n",
      "[Epoch: 16220] train loss: 0.1719, train acc: 0.9377, val loss: 0.1498, val acc: 0.9578  (best train acc: 0.9449, best val acc: 0.9605, best train loss: 0.1624  @ epoch 16033 )\n",
      "[Epoch: 16240] train loss: 0.2142, train acc: 0.9214, val loss: 0.1516, val acc: 0.9565  (best train acc: 0.9453, best val acc: 0.9605, best train loss: 0.1583  @ epoch 16234 )\n",
      "[Epoch: 16260] train loss: 0.1797, train acc: 0.9343, val loss: 0.1528, val acc: 0.9481  (best train acc: 0.9453, best val acc: 0.9605, best train loss: 0.1583  @ epoch 16234 )\n",
      "[Epoch: 16280] train loss: 0.1881, train acc: 0.9340, val loss: 0.1555, val acc: 0.9491  (best train acc: 0.9453, best val acc: 0.9605, best train loss: 0.1583  @ epoch 16234 )\n",
      "[Epoch: 16300] train loss: 0.2031, train acc: 0.9263, val loss: 0.1518, val acc: 0.9454  (best train acc: 0.9453, best val acc: 0.9605, best train loss: 0.1583  @ epoch 16234 )\n",
      "[Epoch: 16320] train loss: 0.1866, train acc: 0.9315, val loss: 0.1542, val acc: 0.9494  (best train acc: 0.9453, best val acc: 0.9605, best train loss: 0.1583  @ epoch 16234 )\n",
      "[Epoch: 16340] train loss: 0.1892, train acc: 0.9310, val loss: 0.1491, val acc: 0.9578  (best train acc: 0.9453, best val acc: 0.9605, best train loss: 0.1583  @ epoch 16234 )\n",
      "[Epoch: 16360] train loss: 0.1909, train acc: 0.9399, val loss: 0.1461, val acc: 0.9545  (best train acc: 0.9453, best val acc: 0.9605, best train loss: 0.1583  @ epoch 16234 )\n",
      "[Epoch: 16380] train loss: 0.1956, train acc: 0.9337, val loss: 0.1813, val acc: 0.9444  (best train acc: 0.9453, best val acc: 0.9605, best train loss: 0.1583  @ epoch 16234 )\n",
      "[Epoch: 16400] train loss: 0.1882, train acc: 0.9333, val loss: 0.1630, val acc: 0.9521  (best train acc: 0.9453, best val acc: 0.9605, best train loss: 0.1583  @ epoch 16234 )\n",
      "[Epoch: 16420] train loss: 0.1941, train acc: 0.9312, val loss: 0.1562, val acc: 0.9535  (best train acc: 0.9453, best val acc: 0.9605, best train loss: 0.1583  @ epoch 16234 )\n",
      "[Epoch: 16440] train loss: 0.1660, train acc: 0.9406, val loss: 0.1534, val acc: 0.9541  (best train acc: 0.9453, best val acc: 0.9605, best train loss: 0.1583  @ epoch 16234 )\n",
      "[Epoch: 16460] train loss: 0.1996, train acc: 0.9253, val loss: 0.1616, val acc: 0.9562  (best train acc: 0.9453, best val acc: 0.9605, best train loss: 0.1583  @ epoch 16234 )\n",
      "[Epoch: 16480] train loss: 0.1866, train acc: 0.9365, val loss: 0.1586, val acc: 0.9545  (best train acc: 0.9453, best val acc: 0.9605, best train loss: 0.1583  @ epoch 16234 )\n",
      "[Epoch: 16500] train loss: 0.2168, train acc: 0.9231, val loss: 0.1918, val acc: 0.9376  (best train acc: 0.9453, best val acc: 0.9605, best train loss: 0.1583  @ epoch 16234 )\n",
      "[Epoch: 16520] train loss: 0.1785, train acc: 0.9341, val loss: 0.1468, val acc: 0.9551  (best train acc: 0.9453, best val acc: 0.9605, best train loss: 0.1583  @ epoch 16234 )\n",
      "[Epoch: 16540] train loss: 0.1817, train acc: 0.9349, val loss: 0.1558, val acc: 0.9528  (best train acc: 0.9453, best val acc: 0.9605, best train loss: 0.1583  @ epoch 16234 )\n",
      "[Epoch: 16560] train loss: 0.1888, train acc: 0.9307, val loss: 0.1574, val acc: 0.9562  (best train acc: 0.9453, best val acc: 0.9605, best train loss: 0.1583  @ epoch 16234 )\n",
      "[Epoch: 16580] train loss: 0.1799, train acc: 0.9330, val loss: 0.1591, val acc: 0.9508  (best train acc: 0.9453, best val acc: 0.9605, best train loss: 0.1583  @ epoch 16234 )\n",
      "[Epoch: 16600] train loss: 0.2516, train acc: 0.9054, val loss: 0.1727, val acc: 0.9474  (best train acc: 0.9453, best val acc: 0.9605, best train loss: 0.1583  @ epoch 16234 )\n",
      "[Epoch: 16620] train loss: 0.2563, train acc: 0.9079, val loss: 0.1652, val acc: 0.9484  (best train acc: 0.9453, best val acc: 0.9605, best train loss: 0.1583  @ epoch 16234 )\n",
      "[Epoch: 16640] train loss: 0.1854, train acc: 0.9333, val loss: 0.1583, val acc: 0.9521  (best train acc: 0.9453, best val acc: 0.9605, best train loss: 0.1583  @ epoch 16234 )\n",
      "[Epoch: 16660] train loss: 0.1726, train acc: 0.9375, val loss: 0.1568, val acc: 0.9541  (best train acc: 0.9453, best val acc: 0.9605, best train loss: 0.1583  @ epoch 16234 )\n",
      "[Epoch: 16680] train loss: 0.1951, train acc: 0.9251, val loss: 0.1558, val acc: 0.9521  (best train acc: 0.9453, best val acc: 0.9609, best train loss: 0.1583  @ epoch 16234 )\n",
      "[Epoch: 16700] train loss: 0.2172, train acc: 0.9291, val loss: 0.1615, val acc: 0.9524  (best train acc: 0.9453, best val acc: 0.9609, best train loss: 0.1583  @ epoch 16234 )\n",
      "[Epoch: 16720] train loss: 0.2120, train acc: 0.9210, val loss: 0.1571, val acc: 0.9464  (best train acc: 0.9453, best val acc: 0.9609, best train loss: 0.1583  @ epoch 16234 )\n",
      "[Epoch: 16740] train loss: 0.1824, train acc: 0.9398, val loss: 0.1614, val acc: 0.9582  (best train acc: 0.9453, best val acc: 0.9609, best train loss: 0.1583  @ epoch 16234 )\n",
      "[Epoch: 16760] train loss: 0.1783, train acc: 0.9395, val loss: 0.1574, val acc: 0.9589  (best train acc: 0.9453, best val acc: 0.9609, best train loss: 0.1583  @ epoch 16234 )\n",
      "[Epoch: 16780] train loss: 0.1806, train acc: 0.9362, val loss: 0.1518, val acc: 0.9555  (best train acc: 0.9453, best val acc: 0.9609, best train loss: 0.1583  @ epoch 16234 )\n",
      "[Epoch: 16800] train loss: 0.2036, train acc: 0.9337, val loss: 0.1689, val acc: 0.9491  (best train acc: 0.9477, best val acc: 0.9609, best train loss: 0.1568  @ epoch 16790 )\n",
      "[Epoch: 16820] train loss: 0.1671, train acc: 0.9421, val loss: 0.1476, val acc: 0.9551  (best train acc: 0.9477, best val acc: 0.9609, best train loss: 0.1568  @ epoch 16790 )\n",
      "[Epoch: 16840] train loss: 0.1876, train acc: 0.9328, val loss: 0.1737, val acc: 0.9454  (best train acc: 0.9477, best val acc: 0.9609, best train loss: 0.1568  @ epoch 16790 )\n",
      "[Epoch: 16860] train loss: 0.2172, train acc: 0.9276, val loss: 0.1609, val acc: 0.9497  (best train acc: 0.9477, best val acc: 0.9616, best train loss: 0.1568  @ epoch 16790 )\n",
      "[Epoch: 16880] train loss: 0.1784, train acc: 0.9332, val loss: 0.1627, val acc: 0.9470  (best train acc: 0.9477, best val acc: 0.9616, best train loss: 0.1568  @ epoch 16790 )\n",
      "[Epoch: 16900] train loss: 0.1964, train acc: 0.9294, val loss: 0.1593, val acc: 0.9508  (best train acc: 0.9477, best val acc: 0.9616, best train loss: 0.1568  @ epoch 16790 )\n",
      "[Epoch: 16920] train loss: 0.1822, train acc: 0.9359, val loss: 0.1588, val acc: 0.9457  (best train acc: 0.9477, best val acc: 0.9616, best train loss: 0.1568  @ epoch 16790 )\n",
      "[Epoch: 16940] train loss: 0.1778, train acc: 0.9459, val loss: 0.1586, val acc: 0.9518  (best train acc: 0.9477, best val acc: 0.9616, best train loss: 0.1568  @ epoch 16790 )\n",
      "[Epoch: 16960] train loss: 0.1627, train acc: 0.9448, val loss: 0.1617, val acc: 0.9548  (best train acc: 0.9477, best val acc: 0.9616, best train loss: 0.1568  @ epoch 16790 )\n",
      "[Epoch: 16980] train loss: 0.1980, train acc: 0.9275, val loss: 0.1592, val acc: 0.9538  (best train acc: 0.9477, best val acc: 0.9616, best train loss: 0.1568  @ epoch 16790 )\n",
      "[Epoch: 17000] train loss: 0.1910, train acc: 0.9242, val loss: 0.1663, val acc: 0.9535  (best train acc: 0.9477, best val acc: 0.9616, best train loss: 0.1568  @ epoch 16790 )\n",
      "[Epoch: 17020] train loss: 0.2316, train acc: 0.9209, val loss: 0.1676, val acc: 0.9491  (best train acc: 0.9477, best val acc: 0.9616, best train loss: 0.1568  @ epoch 16790 )\n",
      "[Epoch: 17040] train loss: 0.1673, train acc: 0.9435, val loss: 0.1482, val acc: 0.9551  (best train acc: 0.9477, best val acc: 0.9616, best train loss: 0.1568  @ epoch 16790 )\n",
      "[Epoch: 17060] train loss: 0.1764, train acc: 0.9341, val loss: 0.1552, val acc: 0.9487  (best train acc: 0.9477, best val acc: 0.9616, best train loss: 0.1568  @ epoch 16790 )\n",
      "[Epoch: 17080] train loss: 0.1908, train acc: 0.9364, val loss: 0.1503, val acc: 0.9521  (best train acc: 0.9477, best val acc: 0.9616, best train loss: 0.1568  @ epoch 16790 )\n",
      "[Epoch: 17100] train loss: 0.1896, train acc: 0.9333, val loss: 0.1631, val acc: 0.9555  (best train acc: 0.9477, best val acc: 0.9616, best train loss: 0.1568  @ epoch 16790 )\n",
      "[Epoch: 17120] train loss: 0.1781, train acc: 0.9445, val loss: 0.1585, val acc: 0.9504  (best train acc: 0.9477, best val acc: 0.9616, best train loss: 0.1568  @ epoch 16790 )\n",
      "[Epoch: 17140] train loss: 0.1834, train acc: 0.9305, val loss: 0.1619, val acc: 0.9538  (best train acc: 0.9477, best val acc: 0.9616, best train loss: 0.1568  @ epoch 16790 )\n",
      "[Epoch: 17160] train loss: 0.2030, train acc: 0.9249, val loss: 0.1674, val acc: 0.9497  (best train acc: 0.9477, best val acc: 0.9616, best train loss: 0.1568  @ epoch 16790 )\n",
      "[Epoch: 17180] train loss: 0.2048, train acc: 0.9337, val loss: 0.1480, val acc: 0.9535  (best train acc: 0.9477, best val acc: 0.9616, best train loss: 0.1568  @ epoch 16790 )\n",
      "[Epoch: 17200] train loss: 0.2876, train acc: 0.9106, val loss: 0.1811, val acc: 0.9427  (best train acc: 0.9477, best val acc: 0.9616, best train loss: 0.1568  @ epoch 16790 )\n",
      "[Epoch: 17220] train loss: 0.1920, train acc: 0.9323, val loss: 0.1693, val acc: 0.9484  (best train acc: 0.9477, best val acc: 0.9616, best train loss: 0.1568  @ epoch 16790 )\n",
      "[Epoch: 17240] train loss: 0.1758, train acc: 0.9405, val loss: 0.1518, val acc: 0.9548  (best train acc: 0.9477, best val acc: 0.9616, best train loss: 0.1568  @ epoch 16790 )\n",
      "[Epoch: 17260] train loss: 0.2015, train acc: 0.9264, val loss: 0.1557, val acc: 0.9595  (best train acc: 0.9477, best val acc: 0.9616, best train loss: 0.1568  @ epoch 16790 )\n",
      "[Epoch: 17280] train loss: 0.1931, train acc: 0.9313, val loss: 0.1512, val acc: 0.9592  (best train acc: 0.9477, best val acc: 0.9616, best train loss: 0.1568  @ epoch 16790 )\n",
      "[Epoch: 17300] train loss: 0.1669, train acc: 0.9415, val loss: 0.1520, val acc: 0.9538  (best train acc: 0.9488, best val acc: 0.9616, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 17320] train loss: 0.1699, train acc: 0.9421, val loss: 0.1555, val acc: 0.9582  (best train acc: 0.9488, best val acc: 0.9616, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 17340] train loss: 0.1894, train acc: 0.9393, val loss: 0.1513, val acc: 0.9514  (best train acc: 0.9488, best val acc: 0.9616, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 17360] train loss: 0.1765, train acc: 0.9382, val loss: 0.1495, val acc: 0.9578  (best train acc: 0.9488, best val acc: 0.9619, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 17380] train loss: 0.1682, train acc: 0.9414, val loss: 0.1640, val acc: 0.9460  (best train acc: 0.9488, best val acc: 0.9619, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 17400] train loss: 0.1708, train acc: 0.9443, val loss: 0.1514, val acc: 0.9524  (best train acc: 0.9488, best val acc: 0.9619, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 17420] train loss: 0.1719, train acc: 0.9406, val loss: 0.1681, val acc: 0.9481  (best train acc: 0.9488, best val acc: 0.9619, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 17440] train loss: 0.1781, train acc: 0.9430, val loss: 0.1719, val acc: 0.9450  (best train acc: 0.9488, best val acc: 0.9619, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 17460] train loss: 0.1895, train acc: 0.9319, val loss: 0.1524, val acc: 0.9497  (best train acc: 0.9488, best val acc: 0.9619, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 17480] train loss: 0.2043, train acc: 0.9231, val loss: 0.1594, val acc: 0.9531  (best train acc: 0.9488, best val acc: 0.9619, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 17500] train loss: 0.1785, train acc: 0.9327, val loss: 0.1520, val acc: 0.9578  (best train acc: 0.9488, best val acc: 0.9619, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 17520] train loss: 0.1776, train acc: 0.9392, val loss: 0.1536, val acc: 0.9589  (best train acc: 0.9488, best val acc: 0.9619, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 17540] train loss: 0.2018, train acc: 0.9224, val loss: 0.1604, val acc: 0.9558  (best train acc: 0.9488, best val acc: 0.9619, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 17560] train loss: 0.1856, train acc: 0.9376, val loss: 0.1548, val acc: 0.9541  (best train acc: 0.9488, best val acc: 0.9619, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 17580] train loss: 0.1754, train acc: 0.9353, val loss: 0.1614, val acc: 0.9528  (best train acc: 0.9488, best val acc: 0.9619, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 17600] train loss: 0.1991, train acc: 0.9346, val loss: 0.1513, val acc: 0.9538  (best train acc: 0.9488, best val acc: 0.9619, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 17620] train loss: 0.2165, train acc: 0.9220, val loss: 0.1593, val acc: 0.9481  (best train acc: 0.9488, best val acc: 0.9619, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 17640] train loss: 0.2255, train acc: 0.9098, val loss: 0.1791, val acc: 0.9383  (best train acc: 0.9488, best val acc: 0.9619, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 17660] train loss: 0.1857, train acc: 0.9325, val loss: 0.1549, val acc: 0.9528  (best train acc: 0.9488, best val acc: 0.9619, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 17680] train loss: 0.1819, train acc: 0.9369, val loss: 0.1576, val acc: 0.9430  (best train acc: 0.9488, best val acc: 0.9619, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 17700] train loss: 0.1834, train acc: 0.9405, val loss: 0.1519, val acc: 0.9457  (best train acc: 0.9488, best val acc: 0.9619, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 17720] train loss: 0.2173, train acc: 0.9174, val loss: 0.1686, val acc: 0.9363  (best train acc: 0.9488, best val acc: 0.9619, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 17740] train loss: 0.2018, train acc: 0.9287, val loss: 0.1597, val acc: 0.9551  (best train acc: 0.9488, best val acc: 0.9619, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 17760] train loss: 0.1730, train acc: 0.9330, val loss: 0.1523, val acc: 0.9444  (best train acc: 0.9488, best val acc: 0.9619, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 17780] train loss: 0.2040, train acc: 0.9201, val loss: 0.1587, val acc: 0.9538  (best train acc: 0.9488, best val acc: 0.9619, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 17800] train loss: 0.1796, train acc: 0.9347, val loss: 0.1509, val acc: 0.9599  (best train acc: 0.9488, best val acc: 0.9619, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 17820] train loss: 0.1949, train acc: 0.9341, val loss: 0.1508, val acc: 0.9575  (best train acc: 0.9488, best val acc: 0.9619, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 17840] train loss: 0.2037, train acc: 0.9312, val loss: 0.1775, val acc: 0.9497  (best train acc: 0.9488, best val acc: 0.9619, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 17860] train loss: 0.1952, train acc: 0.9319, val loss: 0.2156, val acc: 0.9373  (best train acc: 0.9488, best val acc: 0.9619, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 17880] train loss: 0.2134, train acc: 0.9211, val loss: 0.1801, val acc: 0.9454  (best train acc: 0.9488, best val acc: 0.9619, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 17900] train loss: 0.1922, train acc: 0.9329, val loss: 0.1590, val acc: 0.9524  (best train acc: 0.9488, best val acc: 0.9619, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 17920] train loss: 0.2056, train acc: 0.9241, val loss: 0.1838, val acc: 0.9433  (best train acc: 0.9488, best val acc: 0.9619, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 17940] train loss: 0.1696, train acc: 0.9409, val loss: 0.1851, val acc: 0.9403  (best train acc: 0.9488, best val acc: 0.9619, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 17960] train loss: 0.1750, train acc: 0.9408, val loss: 0.1510, val acc: 0.9609  (best train acc: 0.9488, best val acc: 0.9619, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 17980] train loss: 0.1828, train acc: 0.9342, val loss: 0.1471, val acc: 0.9575  (best train acc: 0.9488, best val acc: 0.9619, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 18000] train loss: 0.1747, train acc: 0.9367, val loss: 0.1608, val acc: 0.9508  (best train acc: 0.9488, best val acc: 0.9619, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 18020] train loss: 0.1908, train acc: 0.9332, val loss: 0.1519, val acc: 0.9531  (best train acc: 0.9488, best val acc: 0.9619, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 18040] train loss: 0.1895, train acc: 0.9294, val loss: 0.1656, val acc: 0.9514  (best train acc: 0.9488, best val acc: 0.9619, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 18060] train loss: 0.1853, train acc: 0.9376, val loss: 0.1464, val acc: 0.9535  (best train acc: 0.9488, best val acc: 0.9619, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 18080] train loss: 0.1821, train acc: 0.9354, val loss: 0.1535, val acc: 0.9578  (best train acc: 0.9488, best val acc: 0.9619, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 18100] train loss: 0.1897, train acc: 0.9341, val loss: 0.1447, val acc: 0.9612  (best train acc: 0.9488, best val acc: 0.9619, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 18120] train loss: 0.1864, train acc: 0.9288, val loss: 0.1554, val acc: 0.9568  (best train acc: 0.9488, best val acc: 0.9619, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 18140] train loss: 0.2148, train acc: 0.9230, val loss: 0.1557, val acc: 0.9595  (best train acc: 0.9488, best val acc: 0.9619, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 18160] train loss: 0.1635, train acc: 0.9427, val loss: 0.1564, val acc: 0.9572  (best train acc: 0.9488, best val acc: 0.9619, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 18180] train loss: 0.2100, train acc: 0.9194, val loss: 0.1485, val acc: 0.9572  (best train acc: 0.9488, best val acc: 0.9619, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 18200] train loss: 0.2205, train acc: 0.9184, val loss: 0.1569, val acc: 0.9551  (best train acc: 0.9488, best val acc: 0.9619, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 18220] train loss: 0.1728, train acc: 0.9389, val loss: 0.1479, val acc: 0.9555  (best train acc: 0.9488, best val acc: 0.9619, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 18240] train loss: 0.1783, train acc: 0.9396, val loss: 0.1587, val acc: 0.9531  (best train acc: 0.9488, best val acc: 0.9619, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 18260] train loss: 0.2074, train acc: 0.9264, val loss: 0.1700, val acc: 0.9535  (best train acc: 0.9488, best val acc: 0.9619, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 18280] train loss: 0.1847, train acc: 0.9284, val loss: 0.1470, val acc: 0.9609  (best train acc: 0.9488, best val acc: 0.9619, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 18300] train loss: 0.1766, train acc: 0.9413, val loss: 0.1513, val acc: 0.9582  (best train acc: 0.9488, best val acc: 0.9619, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 18320] train loss: 0.1726, train acc: 0.9398, val loss: 0.1442, val acc: 0.9582  (best train acc: 0.9488, best val acc: 0.9619, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 18340] train loss: 0.1850, train acc: 0.9320, val loss: 0.1475, val acc: 0.9599  (best train acc: 0.9488, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 18360] train loss: 0.1802, train acc: 0.9338, val loss: 0.1658, val acc: 0.9511  (best train acc: 0.9488, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 18380] train loss: 0.1940, train acc: 0.9305, val loss: 0.1772, val acc: 0.9433  (best train acc: 0.9488, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 18400] train loss: 0.1896, train acc: 0.9324, val loss: 0.1605, val acc: 0.9410  (best train acc: 0.9488, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 18420] train loss: 0.1869, train acc: 0.9354, val loss: 0.1561, val acc: 0.9545  (best train acc: 0.9488, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 18440] train loss: 0.1738, train acc: 0.9413, val loss: 0.1552, val acc: 0.9562  (best train acc: 0.9488, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 18460] train loss: 0.1860, train acc: 0.9369, val loss: 0.1632, val acc: 0.9521  (best train acc: 0.9488, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 18480] train loss: 0.1896, train acc: 0.9312, val loss: 0.1476, val acc: 0.9541  (best train acc: 0.9488, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 18500] train loss: 0.1828, train acc: 0.9349, val loss: 0.1497, val acc: 0.9531  (best train acc: 0.9488, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 18520] train loss: 0.1782, train acc: 0.9413, val loss: 0.1814, val acc: 0.9437  (best train acc: 0.9488, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 18540] train loss: 0.2043, train acc: 0.9213, val loss: 0.1560, val acc: 0.9575  (best train acc: 0.9488, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 18560] train loss: 0.1835, train acc: 0.9352, val loss: 0.1518, val acc: 0.9582  (best train acc: 0.9488, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 18580] train loss: 0.1726, train acc: 0.9438, val loss: 0.1528, val acc: 0.9535  (best train acc: 0.9488, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 18600] train loss: 0.2220, train acc: 0.9246, val loss: 0.1570, val acc: 0.9578  (best train acc: 0.9488, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 18620] train loss: 0.2581, train acc: 0.9011, val loss: 0.2062, val acc: 0.9396  (best train acc: 0.9488, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 18640] train loss: 0.1883, train acc: 0.9385, val loss: 0.1461, val acc: 0.9589  (best train acc: 0.9488, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 18660] train loss: 0.1965, train acc: 0.9327, val loss: 0.1465, val acc: 0.9504  (best train acc: 0.9488, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 18680] train loss: 0.2275, train acc: 0.9128, val loss: 0.1494, val acc: 0.9524  (best train acc: 0.9488, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 18700] train loss: 0.1873, train acc: 0.9311, val loss: 0.1499, val acc: 0.9508  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 18720] train loss: 0.2065, train acc: 0.9255, val loss: 0.1677, val acc: 0.9460  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 18740] train loss: 0.2119, train acc: 0.9274, val loss: 0.1597, val acc: 0.9514  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 18760] train loss: 0.1722, train acc: 0.9384, val loss: 0.1495, val acc: 0.9545  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 18780] train loss: 0.2089, train acc: 0.9266, val loss: 0.1766, val acc: 0.9454  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 18800] train loss: 0.1725, train acc: 0.9411, val loss: 0.1489, val acc: 0.9545  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 18820] train loss: 0.2073, train acc: 0.9188, val loss: 0.1812, val acc: 0.9369  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 18840] train loss: 0.1695, train acc: 0.9417, val loss: 0.1547, val acc: 0.9467  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 18860] train loss: 0.1929, train acc: 0.9239, val loss: 0.1478, val acc: 0.9514  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 18880] train loss: 0.1967, train acc: 0.9337, val loss: 0.1580, val acc: 0.9531  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 18900] train loss: 0.1719, train acc: 0.9387, val loss: 0.1485, val acc: 0.9514  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 18920] train loss: 0.1881, train acc: 0.9372, val loss: 0.1521, val acc: 0.9551  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 18940] train loss: 0.2030, train acc: 0.9286, val loss: 0.1497, val acc: 0.9558  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 18960] train loss: 0.1840, train acc: 0.9383, val loss: 0.1509, val acc: 0.9555  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 18980] train loss: 0.1765, train acc: 0.9393, val loss: 0.1700, val acc: 0.9494  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 19000] train loss: 0.1845, train acc: 0.9351, val loss: 0.1518, val acc: 0.9423  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 19020] train loss: 0.1734, train acc: 0.9356, val loss: 0.1512, val acc: 0.9531  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 19040] train loss: 0.1831, train acc: 0.9384, val loss: 0.1503, val acc: 0.9508  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 19060] train loss: 0.1796, train acc: 0.9412, val loss: 0.1450, val acc: 0.9514  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 19080] train loss: 0.1960, train acc: 0.9325, val loss: 0.1503, val acc: 0.9565  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 19100] train loss: 0.1909, train acc: 0.9350, val loss: 0.1566, val acc: 0.9575  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 19120] train loss: 0.1999, train acc: 0.9302, val loss: 0.1473, val acc: 0.9582  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 19140] train loss: 0.1926, train acc: 0.9320, val loss: 0.1512, val acc: 0.9565  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 19160] train loss: 0.1831, train acc: 0.9301, val loss: 0.1582, val acc: 0.9400  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 19180] train loss: 0.1808, train acc: 0.9305, val loss: 0.1788, val acc: 0.9430  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 19200] train loss: 0.2207, train acc: 0.9222, val loss: 0.1639, val acc: 0.9518  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 19220] train loss: 0.2006, train acc: 0.9310, val loss: 0.1600, val acc: 0.9474  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 19240] train loss: 0.1658, train acc: 0.9412, val loss: 0.1530, val acc: 0.9578  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 19260] train loss: 0.1789, train acc: 0.9421, val loss: 0.1567, val acc: 0.9514  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 19280] train loss: 0.1714, train acc: 0.9435, val loss: 0.1519, val acc: 0.9568  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 19300] train loss: 0.2269, train acc: 0.9247, val loss: 0.1642, val acc: 0.9481  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 19320] train loss: 0.1761, train acc: 0.9377, val loss: 0.1490, val acc: 0.9521  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 19340] train loss: 0.1776, train acc: 0.9373, val loss: 0.1616, val acc: 0.9511  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 19360] train loss: 0.1809, train acc: 0.9321, val loss: 0.1483, val acc: 0.9524  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 19380] train loss: 0.1987, train acc: 0.9309, val loss: 0.1619, val acc: 0.9524  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 19400] train loss: 0.1915, train acc: 0.9334, val loss: 0.1713, val acc: 0.9511  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 19420] train loss: 0.1756, train acc: 0.9378, val loss: 0.1581, val acc: 0.9528  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 19440] train loss: 0.2025, train acc: 0.9272, val loss: 0.1549, val acc: 0.9575  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 19460] train loss: 0.2304, train acc: 0.9198, val loss: 0.1739, val acc: 0.9427  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 19480] train loss: 0.1819, train acc: 0.9373, val loss: 0.1507, val acc: 0.9568  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 19500] train loss: 0.1783, train acc: 0.9404, val loss: 0.1491, val acc: 0.9535  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 19520] train loss: 0.1588, train acc: 0.9450, val loss: 0.1440, val acc: 0.9605  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 19540] train loss: 0.1636, train acc: 0.9479, val loss: 0.1462, val acc: 0.9568  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 19560] train loss: 0.1699, train acc: 0.9405, val loss: 0.1529, val acc: 0.9551  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 19580] train loss: 0.1766, train acc: 0.9359, val loss: 0.1551, val acc: 0.9528  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 19600] train loss: 0.1716, train acc: 0.9415, val loss: 0.1536, val acc: 0.9535  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 19620] train loss: 0.1790, train acc: 0.9353, val loss: 0.1540, val acc: 0.9541  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 19640] train loss: 0.1853, train acc: 0.9380, val loss: 0.1546, val acc: 0.9578  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 19660] train loss: 0.2524, train acc: 0.9051, val loss: 0.1517, val acc: 0.9589  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 19680] train loss: 0.1843, train acc: 0.9338, val loss: 0.1487, val acc: 0.9555  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 19700] train loss: 0.1820, train acc: 0.9333, val loss: 0.1464, val acc: 0.9487  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 19720] train loss: 0.1874, train acc: 0.9319, val loss: 0.1492, val acc: 0.9609  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 19740] train loss: 0.1823, train acc: 0.9375, val loss: 0.1460, val acc: 0.9470  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 19760] train loss: 0.1817, train acc: 0.9397, val loss: 0.1462, val acc: 0.9572  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 19780] train loss: 0.1819, train acc: 0.9312, val loss: 0.1526, val acc: 0.9585  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 19800] train loss: 0.1642, train acc: 0.9427, val loss: 0.1515, val acc: 0.9541  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 19820] train loss: 0.1817, train acc: 0.9352, val loss: 0.1474, val acc: 0.9535  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 19840] train loss: 0.1814, train acc: 0.9400, val loss: 0.1569, val acc: 0.9555  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 19860] train loss: 0.1769, train acc: 0.9330, val loss: 0.1678, val acc: 0.9501  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 19880] train loss: 0.1862, train acc: 0.9324, val loss: 0.1558, val acc: 0.9541  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 19900] train loss: 0.1655, train acc: 0.9409, val loss: 0.1462, val acc: 0.9568  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 19920] train loss: 0.1843, train acc: 0.9356, val loss: 0.1624, val acc: 0.9508  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 19940] train loss: 0.1933, train acc: 0.9275, val loss: 0.1472, val acc: 0.9541  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 19960] train loss: 0.1808, train acc: 0.9319, val loss: 0.1518, val acc: 0.9558  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 19980] train loss: 0.1663, train acc: 0.9429, val loss: 0.1454, val acc: 0.9575  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 20000] train loss: 0.1834, train acc: 0.9372, val loss: 0.1608, val acc: 0.9518  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 20020] train loss: 0.1744, train acc: 0.9362, val loss: 0.1462, val acc: 0.9595  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 20040] train loss: 0.1895, train acc: 0.9313, val loss: 0.1471, val acc: 0.9575  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 20060] train loss: 0.1888, train acc: 0.9385, val loss: 0.1436, val acc: 0.9575  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 20080] train loss: 0.1968, train acc: 0.9349, val loss: 0.1466, val acc: 0.9592  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 20100] train loss: 0.1682, train acc: 0.9391, val loss: 0.1607, val acc: 0.9494  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 20120] train loss: 0.1888, train acc: 0.9253, val loss: 0.1649, val acc: 0.9325  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 20140] train loss: 0.2254, train acc: 0.9250, val loss: 0.1762, val acc: 0.9538  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 20160] train loss: 0.1887, train acc: 0.9331, val loss: 0.1511, val acc: 0.9565  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 20180] train loss: 0.1946, train acc: 0.9321, val loss: 0.1518, val acc: 0.9531  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 20200] train loss: 0.1854, train acc: 0.9412, val loss: 0.1467, val acc: 0.9572  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 20220] train loss: 0.1706, train acc: 0.9421, val loss: 0.1538, val acc: 0.9551  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 20240] train loss: 0.2085, train acc: 0.9271, val loss: 0.1813, val acc: 0.9437  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 20260] train loss: 0.1952, train acc: 0.9273, val loss: 0.1436, val acc: 0.9582  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 20280] train loss: 0.1838, train acc: 0.9323, val loss: 0.1508, val acc: 0.9487  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 20300] train loss: 0.2003, train acc: 0.9213, val loss: 0.1654, val acc: 0.9511  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 20320] train loss: 0.1674, train acc: 0.9429, val loss: 0.1591, val acc: 0.9349  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 20340] train loss: 0.1755, train acc: 0.9359, val loss: 0.1466, val acc: 0.9565  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 20360] train loss: 0.1694, train acc: 0.9385, val loss: 0.1580, val acc: 0.9531  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 20380] train loss: 0.1763, train acc: 0.9347, val loss: 0.1493, val acc: 0.9562  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 20400] train loss: 0.2224, train acc: 0.9119, val loss: 0.1549, val acc: 0.9548  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 20420] train loss: 0.1946, train acc: 0.9297, val loss: 0.1522, val acc: 0.9589  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 20440] train loss: 0.2020, train acc: 0.9294, val loss: 0.1437, val acc: 0.9585  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 20460] train loss: 0.1679, train acc: 0.9412, val loss: 0.1456, val acc: 0.9585  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 20480] train loss: 0.2059, train acc: 0.9260, val loss: 0.1507, val acc: 0.9578  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 20500] train loss: 0.1881, train acc: 0.9315, val loss: 0.1485, val acc: 0.9589  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 20520] train loss: 0.1634, train acc: 0.9430, val loss: 0.1534, val acc: 0.9518  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 20540] train loss: 0.1802, train acc: 0.9358, val loss: 0.1450, val acc: 0.9578  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 20560] train loss: 0.1907, train acc: 0.9304, val loss: 0.1657, val acc: 0.9423  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 20580] train loss: 0.2262, train acc: 0.9171, val loss: 0.1535, val acc: 0.9457  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 20600] train loss: 0.1873, train acc: 0.9359, val loss: 0.1600, val acc: 0.9548  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 20620] train loss: 0.1939, train acc: 0.9265, val loss: 0.1553, val acc: 0.9551  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 20640] train loss: 0.1894, train acc: 0.9359, val loss: 0.1602, val acc: 0.9545  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 20660] train loss: 0.1698, train acc: 0.9419, val loss: 0.1470, val acc: 0.9470  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 20680] train loss: 0.1680, train acc: 0.9396, val loss: 0.1453, val acc: 0.9535  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 20700] train loss: 0.1778, train acc: 0.9371, val loss: 0.1673, val acc: 0.9514  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 20720] train loss: 0.2164, train acc: 0.9192, val loss: 0.1592, val acc: 0.9514  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 20740] train loss: 0.1759, train acc: 0.9349, val loss: 0.1447, val acc: 0.9514  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 20760] train loss: 0.2136, train acc: 0.9254, val loss: 0.1520, val acc: 0.9467  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 20780] train loss: 0.2194, train acc: 0.9171, val loss: 0.1607, val acc: 0.9501  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 20800] train loss: 0.1771, train acc: 0.9420, val loss: 0.1511, val acc: 0.9555  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 20820] train loss: 0.2341, train acc: 0.9132, val loss: 0.1512, val acc: 0.9555  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 20840] train loss: 0.2049, train acc: 0.9266, val loss: 0.1642, val acc: 0.9511  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 20860] train loss: 0.1870, train acc: 0.9289, val loss: 0.1533, val acc: 0.9528  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 20880] train loss: 0.1815, train acc: 0.9276, val loss: 0.1427, val acc: 0.9575  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 20900] train loss: 0.1672, train acc: 0.9418, val loss: 0.1565, val acc: 0.9450  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 20920] train loss: 0.1878, train acc: 0.9349, val loss: 0.1471, val acc: 0.9555  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 20940] train loss: 0.1796, train acc: 0.9416, val loss: 0.1508, val acc: 0.9538  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 20960] train loss: 0.1899, train acc: 0.9330, val loss: 0.1878, val acc: 0.9433  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 20980] train loss: 0.1662, train acc: 0.9431, val loss: 0.1392, val acc: 0.9555  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 21000] train loss: 0.2115, train acc: 0.9301, val loss: 0.1642, val acc: 0.9555  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 21020] train loss: 0.1720, train acc: 0.9367, val loss: 0.1481, val acc: 0.9528  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 21040] train loss: 0.1673, train acc: 0.9431, val loss: 0.1538, val acc: 0.9575  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 21060] train loss: 0.1844, train acc: 0.9328, val loss: 0.1503, val acc: 0.9578  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 21080] train loss: 0.1982, train acc: 0.9323, val loss: 0.1653, val acc: 0.9484  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 21100] train loss: 0.2177, train acc: 0.9294, val loss: 0.1499, val acc: 0.9504  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 21120] train loss: 0.1705, train acc: 0.9405, val loss: 0.1501, val acc: 0.9558  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 21140] train loss: 0.1764, train acc: 0.9365, val loss: 0.1639, val acc: 0.9464  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 21160] train loss: 0.1757, train acc: 0.9414, val loss: 0.1505, val acc: 0.9578  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 21180] train loss: 0.1940, train acc: 0.9308, val loss: 0.1509, val acc: 0.9595  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 21200] train loss: 0.1836, train acc: 0.9349, val loss: 0.1558, val acc: 0.9572  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 21220] train loss: 0.1839, train acc: 0.9382, val loss: 0.1418, val acc: 0.9568  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 21240] train loss: 0.1995, train acc: 0.9224, val loss: 0.1514, val acc: 0.9521  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 21260] train loss: 0.1867, train acc: 0.9391, val loss: 0.1716, val acc: 0.9420  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 21280] train loss: 0.1958, train acc: 0.9305, val loss: 0.1549, val acc: 0.9578  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 21300] train loss: 0.1923, train acc: 0.9318, val loss: 0.1456, val acc: 0.9494  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 21320] train loss: 0.1733, train acc: 0.9380, val loss: 0.1513, val acc: 0.9572  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 21340] train loss: 0.1818, train acc: 0.9310, val loss: 0.1420, val acc: 0.9575  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 21360] train loss: 0.1918, train acc: 0.9318, val loss: 0.1515, val acc: 0.9508  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 21380] train loss: 0.1856, train acc: 0.9372, val loss: 0.1558, val acc: 0.9548  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 21400] train loss: 0.1751, train acc: 0.9422, val loss: 0.1452, val acc: 0.9582  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 21420] train loss: 0.1715, train acc: 0.9391, val loss: 0.1423, val acc: 0.9565  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 21440] train loss: 0.1660, train acc: 0.9413, val loss: 0.1535, val acc: 0.9572  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 21460] train loss: 0.2238, train acc: 0.9157, val loss: 0.1488, val acc: 0.9585  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 21480] train loss: 0.1761, train acc: 0.9318, val loss: 0.1561, val acc: 0.9572  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 21500] train loss: 0.1757, train acc: 0.9331, val loss: 0.1590, val acc: 0.9521  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 21520] train loss: 0.1672, train acc: 0.9435, val loss: 0.1503, val acc: 0.9521  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 21540] train loss: 0.1677, train acc: 0.9396, val loss: 0.1615, val acc: 0.9514  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 21560] train loss: 0.1849, train acc: 0.9305, val loss: 0.1449, val acc: 0.9582  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 21580] train loss: 0.2152, train acc: 0.9191, val loss: 0.1453, val acc: 0.9477  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 21600] train loss: 0.1856, train acc: 0.9362, val loss: 0.1484, val acc: 0.9531  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 21620] train loss: 0.1758, train acc: 0.9350, val loss: 0.1560, val acc: 0.9541  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 21640] train loss: 0.1836, train acc: 0.9357, val loss: 0.1478, val acc: 0.9521  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 21660] train loss: 0.1681, train acc: 0.9423, val loss: 0.1581, val acc: 0.9548  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 21680] train loss: 0.1717, train acc: 0.9375, val loss: 0.1476, val acc: 0.9538  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 21700] train loss: 0.1659, train acc: 0.9441, val loss: 0.1516, val acc: 0.9541  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 21720] train loss: 0.2030, train acc: 0.9314, val loss: 0.1752, val acc: 0.9514  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 21740] train loss: 0.1944, train acc: 0.9286, val loss: 0.1492, val acc: 0.9538  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 21760] train loss: 0.1698, train acc: 0.9414, val loss: 0.1470, val acc: 0.9582  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 21780] train loss: 0.1795, train acc: 0.9375, val loss: 0.1526, val acc: 0.9582  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 21800] train loss: 0.2211, train acc: 0.9122, val loss: 0.1822, val acc: 0.9454  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 21820] train loss: 0.1813, train acc: 0.9375, val loss: 0.1635, val acc: 0.9508  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 21840] train loss: 0.1869, train acc: 0.9338, val loss: 0.1649, val acc: 0.9518  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 21860] train loss: 0.2001, train acc: 0.9323, val loss: 0.1565, val acc: 0.9555  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 21880] train loss: 0.1845, train acc: 0.9324, val loss: 0.1509, val acc: 0.9568  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 21900] train loss: 0.1816, train acc: 0.9360, val loss: 0.1512, val acc: 0.9575  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 21920] train loss: 0.1875, train acc: 0.9406, val loss: 0.1506, val acc: 0.9565  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 21940] train loss: 0.1872, train acc: 0.9385, val loss: 0.1481, val acc: 0.9558  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 21960] train loss: 0.1871, train acc: 0.9357, val loss: 0.1562, val acc: 0.9572  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 21980] train loss: 0.1905, train acc: 0.9303, val loss: 0.1596, val acc: 0.9508  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 22000] train loss: 0.2101, train acc: 0.9234, val loss: 0.1666, val acc: 0.9514  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 22020] train loss: 0.1614, train acc: 0.9459, val loss: 0.1553, val acc: 0.9545  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 22040] train loss: 0.1557, train acc: 0.9456, val loss: 0.1495, val acc: 0.9565  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 22060] train loss: 0.1890, train acc: 0.9297, val loss: 0.1448, val acc: 0.9558  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 22080] train loss: 0.1637, train acc: 0.9454, val loss: 0.1508, val acc: 0.9555  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 22100] train loss: 0.1777, train acc: 0.9390, val loss: 0.1509, val acc: 0.9562  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 22120] train loss: 0.1753, train acc: 0.9379, val loss: 0.1477, val acc: 0.9582  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 22140] train loss: 0.2413, train acc: 0.9057, val loss: 0.1716, val acc: 0.9514  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 22160] train loss: 0.1948, train acc: 0.9286, val loss: 0.1685, val acc: 0.9497  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 22180] train loss: 0.1990, train acc: 0.9365, val loss: 0.1604, val acc: 0.9531  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 22200] train loss: 0.1949, train acc: 0.9338, val loss: 0.1521, val acc: 0.9460  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 22220] train loss: 0.1802, train acc: 0.9348, val loss: 0.1538, val acc: 0.9551  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 22240] train loss: 0.1894, train acc: 0.9368, val loss: 0.1428, val acc: 0.9565  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 22260] train loss: 0.1781, train acc: 0.9388, val loss: 0.1551, val acc: 0.9578  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 22280] train loss: 0.1763, train acc: 0.9409, val loss: 0.1506, val acc: 0.9575  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 22300] train loss: 0.1715, train acc: 0.9358, val loss: 0.1467, val acc: 0.9568  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 22320] train loss: 0.1805, train acc: 0.9316, val loss: 0.1462, val acc: 0.9531  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 22340] train loss: 0.1832, train acc: 0.9348, val loss: 0.1827, val acc: 0.9423  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 22360] train loss: 0.1766, train acc: 0.9354, val loss: 0.1535, val acc: 0.9491  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 22380] train loss: 0.1735, train acc: 0.9361, val loss: 0.1504, val acc: 0.9548  (best train acc: 0.9495, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 22400] train loss: 0.1775, train acc: 0.9432, val loss: 0.1524, val acc: 0.9572  (best train acc: 0.9500, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 22420] train loss: 0.1849, train acc: 0.9350, val loss: 0.1512, val acc: 0.9558  (best train acc: 0.9500, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 22440] train loss: 0.1705, train acc: 0.9372, val loss: 0.1515, val acc: 0.9578  (best train acc: 0.9500, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 22460] train loss: 0.1823, train acc: 0.9335, val loss: 0.1722, val acc: 0.9444  (best train acc: 0.9500, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 22480] train loss: 0.1906, train acc: 0.9373, val loss: 0.1519, val acc: 0.9562  (best train acc: 0.9500, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 22500] train loss: 0.1784, train acc: 0.9365, val loss: 0.1630, val acc: 0.9511  (best train acc: 0.9500, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 22520] train loss: 0.1693, train acc: 0.9417, val loss: 0.1604, val acc: 0.9487  (best train acc: 0.9500, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 22540] train loss: 0.1887, train acc: 0.9362, val loss: 0.1504, val acc: 0.9524  (best train acc: 0.9500, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 22560] train loss: 0.1709, train acc: 0.9356, val loss: 0.1660, val acc: 0.9511  (best train acc: 0.9500, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 22580] train loss: 0.2208, train acc: 0.9101, val loss: 0.1574, val acc: 0.9514  (best train acc: 0.9500, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 22600] train loss: 0.1884, train acc: 0.9341, val loss: 0.1531, val acc: 0.9548  (best train acc: 0.9500, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 22620] train loss: 0.1732, train acc: 0.9414, val loss: 0.1456, val acc: 0.9521  (best train acc: 0.9500, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 22640] train loss: 0.1726, train acc: 0.9385, val loss: 0.1662, val acc: 0.9518  (best train acc: 0.9500, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 22660] train loss: 0.1859, train acc: 0.9352, val loss: 0.1435, val acc: 0.9555  (best train acc: 0.9500, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 22680] train loss: 0.1780, train acc: 0.9381, val loss: 0.1405, val acc: 0.9562  (best train acc: 0.9500, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 22700] train loss: 0.1858, train acc: 0.9398, val loss: 0.1486, val acc: 0.9575  (best train acc: 0.9500, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 22720] train loss: 0.1880, train acc: 0.9258, val loss: 0.1467, val acc: 0.9501  (best train acc: 0.9500, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 22740] train loss: 0.1734, train acc: 0.9404, val loss: 0.1542, val acc: 0.9538  (best train acc: 0.9500, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 22760] train loss: 0.1762, train acc: 0.9397, val loss: 0.1548, val acc: 0.9551  (best train acc: 0.9500, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 22780] train loss: 0.1743, train acc: 0.9414, val loss: 0.1515, val acc: 0.9585  (best train acc: 0.9500, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 22800] train loss: 0.1821, train acc: 0.9315, val loss: 0.1500, val acc: 0.9521  (best train acc: 0.9500, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 22820] train loss: 0.1652, train acc: 0.9442, val loss: 0.1465, val acc: 0.9568  (best train acc: 0.9500, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 22840] train loss: 0.1944, train acc: 0.9349, val loss: 0.1490, val acc: 0.9545  (best train acc: 0.9500, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 22860] train loss: 0.2033, train acc: 0.9192, val loss: 0.1440, val acc: 0.9504  (best train acc: 0.9500, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 22880] train loss: 0.1789, train acc: 0.9385, val loss: 0.1550, val acc: 0.9578  (best train acc: 0.9500, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 22900] train loss: 0.1712, train acc: 0.9376, val loss: 0.1449, val acc: 0.9565  (best train acc: 0.9500, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 22920] train loss: 0.1742, train acc: 0.9359, val loss: 0.1468, val acc: 0.9541  (best train acc: 0.9500, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 22940] train loss: 0.1875, train acc: 0.9310, val loss: 0.1441, val acc: 0.9602  (best train acc: 0.9500, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 22960] train loss: 0.1819, train acc: 0.9383, val loss: 0.1449, val acc: 0.9551  (best train acc: 0.9500, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 22980] train loss: 0.1792, train acc: 0.9422, val loss: 0.1588, val acc: 0.9508  (best train acc: 0.9500, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 23000] train loss: 0.1786, train acc: 0.9368, val loss: 0.1571, val acc: 0.9531  (best train acc: 0.9500, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 23020] train loss: 0.1687, train acc: 0.9415, val loss: 0.1483, val acc: 0.9562  (best train acc: 0.9500, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 23040] train loss: 0.1860, train acc: 0.9333, val loss: 0.1517, val acc: 0.9555  (best train acc: 0.9500, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 23060] train loss: 0.2245, train acc: 0.9226, val loss: 0.1741, val acc: 0.9437  (best train acc: 0.9500, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 23080] train loss: 0.1773, train acc: 0.9357, val loss: 0.1475, val acc: 0.9541  (best train acc: 0.9500, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 23100] train loss: 0.1787, train acc: 0.9377, val loss: 0.1492, val acc: 0.9541  (best train acc: 0.9500, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 23120] train loss: 0.1742, train acc: 0.9336, val loss: 0.1621, val acc: 0.9504  (best train acc: 0.9500, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 23140] train loss: 0.1680, train acc: 0.9425, val loss: 0.1648, val acc: 0.9487  (best train acc: 0.9500, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 23160] train loss: 0.1774, train acc: 0.9354, val loss: 0.1464, val acc: 0.9528  (best train acc: 0.9500, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 23180] train loss: 0.1661, train acc: 0.9371, val loss: 0.1511, val acc: 0.9592  (best train acc: 0.9500, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 23200] train loss: 0.1780, train acc: 0.9406, val loss: 0.1846, val acc: 0.9524  (best train acc: 0.9500, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 23220] train loss: 0.1724, train acc: 0.9357, val loss: 0.1645, val acc: 0.9538  (best train acc: 0.9500, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 23240] train loss: 0.1934, train acc: 0.9268, val loss: 0.1581, val acc: 0.9521  (best train acc: 0.9500, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 23260] train loss: 0.1675, train acc: 0.9393, val loss: 0.1471, val acc: 0.9541  (best train acc: 0.9500, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 23280] train loss: 0.1863, train acc: 0.9399, val loss: 0.1472, val acc: 0.9582  (best train acc: 0.9500, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 23300] train loss: 0.1746, train acc: 0.9365, val loss: 0.1516, val acc: 0.9568  (best train acc: 0.9500, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 23320] train loss: 0.1802, train acc: 0.9404, val loss: 0.1464, val acc: 0.9538  (best train acc: 0.9500, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 23340] train loss: 0.1805, train acc: 0.9386, val loss: 0.1494, val acc: 0.9531  (best train acc: 0.9500, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 23360] train loss: 0.1732, train acc: 0.9369, val loss: 0.1553, val acc: 0.9545  (best train acc: 0.9500, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 23380] train loss: 0.1678, train acc: 0.9422, val loss: 0.1741, val acc: 0.9474  (best train acc: 0.9500, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 23400] train loss: 0.1754, train acc: 0.9401, val loss: 0.1534, val acc: 0.9555  (best train acc: 0.9500, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 23420] train loss: 0.1722, train acc: 0.9369, val loss: 0.1460, val acc: 0.9565  (best train acc: 0.9500, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 23440] train loss: 0.1794, train acc: 0.9374, val loss: 0.1546, val acc: 0.9575  (best train acc: 0.9500, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 23460] train loss: 0.1634, train acc: 0.9422, val loss: 0.1659, val acc: 0.9508  (best train acc: 0.9500, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 23480] train loss: 0.2046, train acc: 0.9263, val loss: 0.1571, val acc: 0.9558  (best train acc: 0.9500, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 23500] train loss: 0.1773, train acc: 0.9370, val loss: 0.1475, val acc: 0.9582  (best train acc: 0.9500, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 23520] train loss: 0.1595, train acc: 0.9466, val loss: 0.1678, val acc: 0.9491  (best train acc: 0.9500, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 23540] train loss: 0.1728, train acc: 0.9371, val loss: 0.1505, val acc: 0.9528  (best train acc: 0.9500, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 23560] train loss: 0.1976, train acc: 0.9265, val loss: 0.1582, val acc: 0.9504  (best train acc: 0.9500, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 23580] train loss: 0.2343, train acc: 0.9135, val loss: 0.1626, val acc: 0.9508  (best train acc: 0.9500, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 23600] train loss: 0.2049, train acc: 0.9330, val loss: 0.1529, val acc: 0.9575  (best train acc: 0.9500, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 23620] train loss: 0.1724, train acc: 0.9359, val loss: 0.1550, val acc: 0.9551  (best train acc: 0.9500, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 23640] train loss: 0.1808, train acc: 0.9359, val loss: 0.1501, val acc: 0.9582  (best train acc: 0.9500, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 23660] train loss: 0.1685, train acc: 0.9408, val loss: 0.1455, val acc: 0.9585  (best train acc: 0.9500, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 23680] train loss: 0.1767, train acc: 0.9338, val loss: 0.1508, val acc: 0.9514  (best train acc: 0.9500, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 23700] train loss: 0.1934, train acc: 0.9271, val loss: 0.1548, val acc: 0.9541  (best train acc: 0.9500, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 23720] train loss: 0.1822, train acc: 0.9370, val loss: 0.1631, val acc: 0.9487  (best train acc: 0.9500, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 23740] train loss: 0.2011, train acc: 0.9245, val loss: 0.1685, val acc: 0.9467  (best train acc: 0.9500, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 23760] train loss: 0.1707, train acc: 0.9397, val loss: 0.1408, val acc: 0.9558  (best train acc: 0.9500, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 23780] train loss: 0.1862, train acc: 0.9398, val loss: 0.1567, val acc: 0.9555  (best train acc: 0.9500, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 23800] train loss: 0.2033, train acc: 0.9359, val loss: 0.1516, val acc: 0.9524  (best train acc: 0.9500, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 23820] train loss: 0.1779, train acc: 0.9423, val loss: 0.1490, val acc: 0.9582  (best train acc: 0.9500, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 23840] train loss: 0.1694, train acc: 0.9432, val loss: 0.1462, val acc: 0.9582  (best train acc: 0.9500, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 23860] train loss: 0.1755, train acc: 0.9367, val loss: 0.1491, val acc: 0.9548  (best train acc: 0.9500, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 23880] train loss: 0.1653, train acc: 0.9432, val loss: 0.1470, val acc: 0.9551  (best train acc: 0.9500, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 23900] train loss: 0.1781, train acc: 0.9439, val loss: 0.1509, val acc: 0.9599  (best train acc: 0.9500, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 23920] train loss: 0.1666, train acc: 0.9434, val loss: 0.1504, val acc: 0.9568  (best train acc: 0.9500, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 23940] train loss: 0.1734, train acc: 0.9387, val loss: 0.1663, val acc: 0.9504  (best train acc: 0.9500, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 23960] train loss: 0.1884, train acc: 0.9354, val loss: 0.1448, val acc: 0.9524  (best train acc: 0.9500, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 23980] train loss: 0.1745, train acc: 0.9434, val loss: 0.1474, val acc: 0.9578  (best train acc: 0.9500, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 24000] train loss: 0.1788, train acc: 0.9349, val loss: 0.1605, val acc: 0.9531  (best train acc: 0.9500, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 24020] train loss: 0.2285, train acc: 0.9115, val loss: 0.1645, val acc: 0.9474  (best train acc: 0.9500, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 24040] train loss: 0.1800, train acc: 0.9327, val loss: 0.1537, val acc: 0.9551  (best train acc: 0.9500, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 24060] train loss: 0.1871, train acc: 0.9294, val loss: 0.1748, val acc: 0.9491  (best train acc: 0.9500, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 24080] train loss: 0.1998, train acc: 0.9250, val loss: 0.1488, val acc: 0.9568  (best train acc: 0.9500, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 24100] train loss: 0.1717, train acc: 0.9361, val loss: 0.1476, val acc: 0.9521  (best train acc: 0.9500, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 24120] train loss: 0.1565, train acc: 0.9457, val loss: 0.1420, val acc: 0.9555  (best train acc: 0.9500, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 24140] train loss: 0.1894, train acc: 0.9294, val loss: 0.1657, val acc: 0.9504  (best train acc: 0.9500, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 24160] train loss: 0.1984, train acc: 0.9375, val loss: 0.1583, val acc: 0.9551  (best train acc: 0.9500, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 24180] train loss: 0.1710, train acc: 0.9365, val loss: 0.1439, val acc: 0.9578  (best train acc: 0.9500, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 24200] train loss: 0.1724, train acc: 0.9373, val loss: 0.1464, val acc: 0.9572  (best train acc: 0.9500, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 24220] train loss: 0.1571, train acc: 0.9405, val loss: 0.1452, val acc: 0.9582  (best train acc: 0.9500, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 24240] train loss: 0.1769, train acc: 0.9380, val loss: 0.1676, val acc: 0.9467  (best train acc: 0.9500, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 24260] train loss: 0.2048, train acc: 0.9276, val loss: 0.1654, val acc: 0.9524  (best train acc: 0.9500, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 24280] train loss: 0.1906, train acc: 0.9312, val loss: 0.1500, val acc: 0.9558  (best train acc: 0.9500, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 24300] train loss: 0.1784, train acc: 0.9357, val loss: 0.1522, val acc: 0.9575  (best train acc: 0.9500, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 24320] train loss: 0.1805, train acc: 0.9310, val loss: 0.1457, val acc: 0.9592  (best train acc: 0.9500, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 24340] train loss: 0.1776, train acc: 0.9360, val loss: 0.1443, val acc: 0.9568  (best train acc: 0.9500, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 24360] train loss: 0.1722, train acc: 0.9375, val loss: 0.1485, val acc: 0.9585  (best train acc: 0.9500, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 24380] train loss: 0.1779, train acc: 0.9400, val loss: 0.1471, val acc: 0.9548  (best train acc: 0.9500, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 24400] train loss: 0.1696, train acc: 0.9414, val loss: 0.1457, val acc: 0.9592  (best train acc: 0.9500, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 24420] train loss: 0.1732, train acc: 0.9401, val loss: 0.1525, val acc: 0.9565  (best train acc: 0.9500, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 24440] train loss: 0.1628, train acc: 0.9449, val loss: 0.1458, val acc: 0.9470  (best train acc: 0.9500, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 24460] train loss: 0.1861, train acc: 0.9349, val loss: 0.1513, val acc: 0.9514  (best train acc: 0.9500, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 24480] train loss: 0.1762, train acc: 0.9364, val loss: 0.1474, val acc: 0.9551  (best train acc: 0.9500, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 24500] train loss: 0.1830, train acc: 0.9365, val loss: 0.1584, val acc: 0.9582  (best train acc: 0.9500, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 24520] train loss: 0.1862, train acc: 0.9327, val loss: 0.1525, val acc: 0.9531  (best train acc: 0.9500, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 24540] train loss: 0.2114, train acc: 0.9159, val loss: 0.1462, val acc: 0.9528  (best train acc: 0.9500, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 24560] train loss: 0.2292, train acc: 0.9151, val loss: 0.1488, val acc: 0.9555  (best train acc: 0.9500, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 24580] train loss: 0.1897, train acc: 0.9351, val loss: 0.1429, val acc: 0.9568  (best train acc: 0.9500, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 24600] train loss: 0.1955, train acc: 0.9242, val loss: 0.1541, val acc: 0.9555  (best train acc: 0.9500, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 24620] train loss: 0.1726, train acc: 0.9369, val loss: 0.1635, val acc: 0.9514  (best train acc: 0.9500, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 24640] train loss: 0.2027, train acc: 0.9290, val loss: 0.1663, val acc: 0.9457  (best train acc: 0.9500, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 24660] train loss: 0.1708, train acc: 0.9427, val loss: 0.1516, val acc: 0.9535  (best train acc: 0.9500, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 24680] train loss: 0.1769, train acc: 0.9398, val loss: 0.1553, val acc: 0.9518  (best train acc: 0.9500, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 24700] train loss: 0.1697, train acc: 0.9414, val loss: 0.1522, val acc: 0.9477  (best train acc: 0.9500, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 24720] train loss: 0.1674, train acc: 0.9420, val loss: 0.1515, val acc: 0.9572  (best train acc: 0.9500, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 24740] train loss: 0.1796, train acc: 0.9364, val loss: 0.1549, val acc: 0.9551  (best train acc: 0.9500, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 24760] train loss: 0.1599, train acc: 0.9456, val loss: 0.1505, val acc: 0.9551  (best train acc: 0.9500, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 24780] train loss: 0.1879, train acc: 0.9278, val loss: 0.1474, val acc: 0.9572  (best train acc: 0.9500, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 24800] train loss: 0.1861, train acc: 0.9328, val loss: 0.1500, val acc: 0.9508  (best train acc: 0.9500, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 24820] train loss: 0.2100, train acc: 0.9252, val loss: 0.1557, val acc: 0.9450  (best train acc: 0.9500, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 24840] train loss: 0.1923, train acc: 0.9308, val loss: 0.1566, val acc: 0.9514  (best train acc: 0.9500, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 24860] train loss: 0.1869, train acc: 0.9325, val loss: 0.1685, val acc: 0.9400  (best train acc: 0.9500, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 24880] train loss: 0.1893, train acc: 0.9353, val loss: 0.1433, val acc: 0.9565  (best train acc: 0.9500, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 24900] train loss: 0.2156, train acc: 0.9142, val loss: 0.1434, val acc: 0.9558  (best train acc: 0.9500, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 24920] train loss: 0.1993, train acc: 0.9280, val loss: 0.1702, val acc: 0.9454  (best train acc: 0.9500, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 24940] train loss: 0.1878, train acc: 0.9329, val loss: 0.1577, val acc: 0.9562  (best train acc: 0.9500, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 24960] train loss: 0.1755, train acc: 0.9384, val loss: 0.1476, val acc: 0.9562  (best train acc: 0.9500, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 24980] train loss: 0.1659, train acc: 0.9391, val loss: 0.1436, val acc: 0.9531  (best train acc: 0.9500, best val acc: 0.9622, best train loss: 0.1510  @ epoch 17285 )\n",
      "[Epoch: 25000] train loss: 0.1795, train acc: 0.9328, val loss: 0.1517, val acc: 0.9555  (best train acc: 0.9500, best val acc: 0.9622, best train loss: 0.1497  @ epoch 24998 )\n",
      "[Epoch: 25020] train loss: 0.1651, train acc: 0.9461, val loss: 0.1501, val acc: 0.9535  (best train acc: 0.9500, best val acc: 0.9622, best train loss: 0.1497  @ epoch 24998 )\n",
      "[Epoch: 25040] train loss: 0.1833, train acc: 0.9409, val loss: 0.1477, val acc: 0.9595  (best train acc: 0.9500, best val acc: 0.9622, best train loss: 0.1497  @ epoch 24998 )\n",
      "[Epoch: 25060] train loss: 0.1720, train acc: 0.9398, val loss: 0.1620, val acc: 0.9548  (best train acc: 0.9500, best val acc: 0.9622, best train loss: 0.1497  @ epoch 24998 )\n",
      "[Epoch: 25080] train loss: 0.1578, train acc: 0.9438, val loss: 0.1453, val acc: 0.9521  (best train acc: 0.9500, best val acc: 0.9622, best train loss: 0.1497  @ epoch 24998 )\n",
      "[Epoch: 25100] train loss: 0.1924, train acc: 0.9296, val loss: 0.1445, val acc: 0.9535  (best train acc: 0.9500, best val acc: 0.9622, best train loss: 0.1497  @ epoch 24998 )\n",
      "[Epoch: 25120] train loss: 0.2019, train acc: 0.9289, val loss: 0.1461, val acc: 0.9562  (best train acc: 0.9500, best val acc: 0.9622, best train loss: 0.1497  @ epoch 24998 )\n",
      "[Epoch: 25140] train loss: 0.1703, train acc: 0.9378, val loss: 0.1444, val acc: 0.9578  (best train acc: 0.9500, best val acc: 0.9622, best train loss: 0.1497  @ epoch 24998 )\n",
      "[Epoch: 25160] train loss: 0.1724, train acc: 0.9364, val loss: 0.1495, val acc: 0.9589  (best train acc: 0.9500, best val acc: 0.9622, best train loss: 0.1497  @ epoch 24998 )\n",
      "[Epoch: 25180] train loss: 0.1573, train acc: 0.9497, val loss: 0.1503, val acc: 0.9558  (best train acc: 0.9500, best val acc: 0.9622, best train loss: 0.1497  @ epoch 24998 )\n",
      "[Epoch: 25200] train loss: 0.1746, train acc: 0.9415, val loss: 0.1409, val acc: 0.9518  (best train acc: 0.9500, best val acc: 0.9622, best train loss: 0.1497  @ epoch 24998 )\n",
      "[Epoch: 25220] train loss: 0.1685, train acc: 0.9375, val loss: 0.1439, val acc: 0.9551  (best train acc: 0.9500, best val acc: 0.9622, best train loss: 0.1497  @ epoch 24998 )\n",
      "[Epoch: 25240] train loss: 0.1755, train acc: 0.9388, val loss: 0.1654, val acc: 0.9514  (best train acc: 0.9500, best val acc: 0.9622, best train loss: 0.1497  @ epoch 24998 )\n",
      "[Epoch: 25260] train loss: 0.2276, train acc: 0.9212, val loss: 0.1720, val acc: 0.9423  (best train acc: 0.9500, best val acc: 0.9622, best train loss: 0.1497  @ epoch 24998 )\n",
      "[Epoch: 25280] train loss: 0.1698, train acc: 0.9440, val loss: 0.1502, val acc: 0.9514  (best train acc: 0.9500, best val acc: 0.9622, best train loss: 0.1497  @ epoch 24998 )\n",
      "[Epoch: 25300] train loss: 0.1851, train acc: 0.9383, val loss: 0.1560, val acc: 0.9528  (best train acc: 0.9500, best val acc: 0.9622, best train loss: 0.1497  @ epoch 24998 )\n",
      "[Epoch: 25320] train loss: 0.1661, train acc: 0.9374, val loss: 0.1577, val acc: 0.9545  (best train acc: 0.9500, best val acc: 0.9622, best train loss: 0.1497  @ epoch 24998 )\n",
      "[Epoch: 25340] train loss: 0.1576, train acc: 0.9472, val loss: 0.1567, val acc: 0.9521  (best train acc: 0.9500, best val acc: 0.9622, best train loss: 0.1497  @ epoch 24998 )\n",
      "[Epoch: 25360] train loss: 0.1926, train acc: 0.9307, val loss: 0.1974, val acc: 0.9430  (best train acc: 0.9500, best val acc: 0.9622, best train loss: 0.1497  @ epoch 24998 )\n",
      "[Epoch: 25380] train loss: 0.1943, train acc: 0.9368, val loss: 0.1619, val acc: 0.9548  (best train acc: 0.9500, best val acc: 0.9622, best train loss: 0.1497  @ epoch 24998 )\n",
      "[Epoch: 25400] train loss: 0.1971, train acc: 0.9306, val loss: 0.1617, val acc: 0.9491  (best train acc: 0.9500, best val acc: 0.9622, best train loss: 0.1497  @ epoch 24998 )\n",
      "[Epoch: 25420] train loss: 0.1792, train acc: 0.9360, val loss: 0.1574, val acc: 0.9545  (best train acc: 0.9500, best val acc: 0.9622, best train loss: 0.1497  @ epoch 24998 )\n",
      "[Epoch: 25440] train loss: 0.1735, train acc: 0.9409, val loss: 0.1439, val acc: 0.9585  (best train acc: 0.9500, best val acc: 0.9622, best train loss: 0.1497  @ epoch 24998 )\n",
      "[Epoch: 25460] train loss: 0.1621, train acc: 0.9434, val loss: 0.1533, val acc: 0.9551  (best train acc: 0.9500, best val acc: 0.9622, best train loss: 0.1497  @ epoch 24998 )\n",
      "[Epoch: 25480] train loss: 0.1730, train acc: 0.9365, val loss: 0.1502, val acc: 0.9572  (best train acc: 0.9500, best val acc: 0.9622, best train loss: 0.1497  @ epoch 24998 )\n",
      "[Epoch: 25500] train loss: 0.1777, train acc: 0.9353, val loss: 0.1690, val acc: 0.9464  (best train acc: 0.9500, best val acc: 0.9622, best train loss: 0.1497  @ epoch 24998 )\n",
      "[Epoch: 25520] train loss: 0.2086, train acc: 0.9230, val loss: 0.1509, val acc: 0.9568  (best train acc: 0.9500, best val acc: 0.9622, best train loss: 0.1497  @ epoch 24998 )\n",
      "[Epoch: 25540] train loss: 0.1880, train acc: 0.9300, val loss: 0.1503, val acc: 0.9403  (best train acc: 0.9500, best val acc: 0.9622, best train loss: 0.1497  @ epoch 24998 )\n",
      "[Epoch: 25560] train loss: 0.1788, train acc: 0.9386, val loss: 0.1509, val acc: 0.9545  (best train acc: 0.9500, best val acc: 0.9622, best train loss: 0.1497  @ epoch 24998 )\n",
      "[Epoch: 25580] train loss: 0.1638, train acc: 0.9448, val loss: 0.1655, val acc: 0.9508  (best train acc: 0.9504, best val acc: 0.9622, best train loss: 0.1497  @ epoch 24998 )\n",
      "[Epoch: 25600] train loss: 0.2201, train acc: 0.9203, val loss: 0.1572, val acc: 0.9383  (best train acc: 0.9504, best val acc: 0.9622, best train loss: 0.1497  @ epoch 24998 )\n",
      "[Epoch: 25620] train loss: 0.1774, train acc: 0.9338, val loss: 0.1487, val acc: 0.9551  (best train acc: 0.9504, best val acc: 0.9622, best train loss: 0.1497  @ epoch 24998 )\n",
      "[Epoch: 25640] train loss: 0.1828, train acc: 0.9349, val loss: 0.1692, val acc: 0.9508  (best train acc: 0.9504, best val acc: 0.9622, best train loss: 0.1497  @ epoch 24998 )\n",
      "[Epoch: 25660] train loss: 0.1707, train acc: 0.9384, val loss: 0.1511, val acc: 0.9582  (best train acc: 0.9504, best val acc: 0.9622, best train loss: 0.1497  @ epoch 24998 )\n",
      "[Epoch: 25680] train loss: 0.1945, train acc: 0.9359, val loss: 0.1533, val acc: 0.9541  (best train acc: 0.9504, best val acc: 0.9622, best train loss: 0.1497  @ epoch 24998 )\n",
      "[Epoch: 25700] train loss: 0.2090, train acc: 0.9239, val loss: 0.1465, val acc: 0.9548  (best train acc: 0.9504, best val acc: 0.9622, best train loss: 0.1497  @ epoch 24998 )\n",
      "[Epoch: 25720] train loss: 0.1511, train acc: 0.9498, val loss: 0.1545, val acc: 0.9548  (best train acc: 0.9504, best val acc: 0.9622, best train loss: 0.1497  @ epoch 24998 )\n",
      "[Epoch: 25740] train loss: 0.1722, train acc: 0.9370, val loss: 0.1541, val acc: 0.9558  (best train acc: 0.9504, best val acc: 0.9622, best train loss: 0.1497  @ epoch 24998 )\n",
      "[Epoch: 25760] train loss: 0.1534, train acc: 0.9439, val loss: 0.1587, val acc: 0.9538  (best train acc: 0.9504, best val acc: 0.9622, best train loss: 0.1497  @ epoch 24998 )\n",
      "[Epoch: 25780] train loss: 0.1643, train acc: 0.9404, val loss: 0.1442, val acc: 0.9582  (best train acc: 0.9504, best val acc: 0.9622, best train loss: 0.1497  @ epoch 24998 )\n",
      "[Epoch: 25800] train loss: 0.1766, train acc: 0.9324, val loss: 0.1762, val acc: 0.9454  (best train acc: 0.9504, best val acc: 0.9622, best train loss: 0.1497  @ epoch 24998 )\n",
      "[Epoch: 25820] train loss: 0.1836, train acc: 0.9312, val loss: 0.1548, val acc: 0.9562  (best train acc: 0.9504, best val acc: 0.9622, best train loss: 0.1497  @ epoch 24998 )\n",
      "[Epoch: 25840] train loss: 0.1771, train acc: 0.9341, val loss: 0.1571, val acc: 0.9518  (best train acc: 0.9504, best val acc: 0.9622, best train loss: 0.1497  @ epoch 24998 )\n",
      "[Epoch: 25860] train loss: 0.1737, train acc: 0.9352, val loss: 0.1490, val acc: 0.9562  (best train acc: 0.9504, best val acc: 0.9622, best train loss: 0.1497  @ epoch 24998 )\n",
      "[Epoch: 25880] train loss: 0.1741, train acc: 0.9415, val loss: 0.1485, val acc: 0.9562  (best train acc: 0.9504, best val acc: 0.9622, best train loss: 0.1497  @ epoch 24998 )\n",
      "[Epoch: 25900] train loss: 0.1707, train acc: 0.9391, val loss: 0.1624, val acc: 0.9474  (best train acc: 0.9504, best val acc: 0.9622, best train loss: 0.1497  @ epoch 24998 )\n",
      "[Epoch: 25920] train loss: 0.1677, train acc: 0.9409, val loss: 0.1466, val acc: 0.9582  (best train acc: 0.9504, best val acc: 0.9622, best train loss: 0.1497  @ epoch 24998 )\n",
      "[Epoch: 25940] train loss: 0.1754, train acc: 0.9339, val loss: 0.1504, val acc: 0.9545  (best train acc: 0.9504, best val acc: 0.9622, best train loss: 0.1497  @ epoch 24998 )\n",
      "[Epoch: 25960] train loss: 0.1911, train acc: 0.9310, val loss: 0.1464, val acc: 0.9484  (best train acc: 0.9504, best val acc: 0.9622, best train loss: 0.1497  @ epoch 24998 )\n",
      "[Epoch: 25980] train loss: 0.1670, train acc: 0.9375, val loss: 0.1426, val acc: 0.9531  (best train acc: 0.9504, best val acc: 0.9622, best train loss: 0.1497  @ epoch 24998 )\n",
      "[Epoch: 26000] train loss: 0.1977, train acc: 0.9294, val loss: 0.1394, val acc: 0.9524  (best train acc: 0.9504, best val acc: 0.9622, best train loss: 0.1497  @ epoch 24998 )\n",
      "[Epoch: 26020] train loss: 0.1655, train acc: 0.9457, val loss: 0.1416, val acc: 0.9551  (best train acc: 0.9504, best val acc: 0.9622, best train loss: 0.1497  @ epoch 24998 )\n",
      "[Epoch: 26040] train loss: 0.1788, train acc: 0.9375, val loss: 0.1474, val acc: 0.9568  (best train acc: 0.9504, best val acc: 0.9622, best train loss: 0.1497  @ epoch 24998 )\n",
      "[Epoch: 26060] train loss: 0.1972, train acc: 0.9282, val loss: 0.1441, val acc: 0.9558  (best train acc: 0.9504, best val acc: 0.9622, best train loss: 0.1497  @ epoch 24998 )\n",
      "[Epoch: 26080] train loss: 0.1692, train acc: 0.9414, val loss: 0.1565, val acc: 0.9558  (best train acc: 0.9504, best val acc: 0.9622, best train loss: 0.1497  @ epoch 24998 )\n",
      "[Epoch: 26100] train loss: 0.1566, train acc: 0.9433, val loss: 0.1451, val acc: 0.9612  (best train acc: 0.9504, best val acc: 0.9622, best train loss: 0.1497  @ epoch 24998 )\n",
      "[Epoch: 26120] train loss: 0.1663, train acc: 0.9448, val loss: 0.1576, val acc: 0.9555  (best train acc: 0.9504, best val acc: 0.9622, best train loss: 0.1497  @ epoch 24998 )\n",
      "[Epoch: 26140] train loss: 0.1807, train acc: 0.9375, val loss: 0.1517, val acc: 0.9511  (best train acc: 0.9504, best val acc: 0.9622, best train loss: 0.1497  @ epoch 24998 )\n",
      "[Epoch: 26160] train loss: 0.2049, train acc: 0.9273, val loss: 0.1480, val acc: 0.9524  (best train acc: 0.9504, best val acc: 0.9622, best train loss: 0.1497  @ epoch 24998 )\n",
      "[Epoch: 26180] train loss: 0.1723, train acc: 0.9393, val loss: 0.1654, val acc: 0.9518  (best train acc: 0.9504, best val acc: 0.9622, best train loss: 0.1497  @ epoch 24998 )\n",
      "[Epoch: 26200] train loss: 0.1892, train acc: 0.9303, val loss: 0.1479, val acc: 0.9548  (best train acc: 0.9504, best val acc: 0.9622, best train loss: 0.1497  @ epoch 24998 )\n",
      "[Epoch: 26220] train loss: 0.1639, train acc: 0.9374, val loss: 0.1429, val acc: 0.9545  (best train acc: 0.9504, best val acc: 0.9622, best train loss: 0.1497  @ epoch 24998 )\n",
      "[Epoch: 26240] train loss: 0.1718, train acc: 0.9393, val loss: 0.1582, val acc: 0.9545  (best train acc: 0.9504, best val acc: 0.9622, best train loss: 0.1497  @ epoch 24998 )\n",
      "[Epoch: 26260] train loss: 0.1764, train acc: 0.9357, val loss: 0.1554, val acc: 0.9538  (best train acc: 0.9504, best val acc: 0.9622, best train loss: 0.1497  @ epoch 24998 )\n",
      "[Epoch: 26280] train loss: 0.1712, train acc: 0.9391, val loss: 0.1705, val acc: 0.9474  (best train acc: 0.9504, best val acc: 0.9622, best train loss: 0.1497  @ epoch 24998 )\n",
      "[Epoch: 26300] train loss: 0.1826, train acc: 0.9409, val loss: 0.1456, val acc: 0.9548  (best train acc: 0.9504, best val acc: 0.9622, best train loss: 0.1497  @ epoch 24998 )\n",
      "[Epoch: 26320] train loss: 0.1669, train acc: 0.9397, val loss: 0.1436, val acc: 0.9565  (best train acc: 0.9504, best val acc: 0.9622, best train loss: 0.1497  @ epoch 24998 )\n",
      "[Epoch: 26340] train loss: 0.1879, train acc: 0.9295, val loss: 0.1688, val acc: 0.9477  (best train acc: 0.9504, best val acc: 0.9622, best train loss: 0.1497  @ epoch 24998 )\n",
      "[Epoch: 26360] train loss: 0.1735, train acc: 0.9380, val loss: 0.1434, val acc: 0.9582  (best train acc: 0.9504, best val acc: 0.9622, best train loss: 0.1497  @ epoch 24998 )\n",
      "[Epoch: 26380] train loss: 0.2087, train acc: 0.9281, val loss: 0.1861, val acc: 0.9342  (best train acc: 0.9504, best val acc: 0.9622, best train loss: 0.1497  @ epoch 24998 )\n",
      "[Epoch: 26400] train loss: 0.1787, train acc: 0.9296, val loss: 0.1465, val acc: 0.9541  (best train acc: 0.9504, best val acc: 0.9622, best train loss: 0.1497  @ epoch 24998 )\n",
      "[Epoch: 26420] train loss: 0.1754, train acc: 0.9307, val loss: 0.1462, val acc: 0.9562  (best train acc: 0.9504, best val acc: 0.9622, best train loss: 0.1497  @ epoch 24998 )\n",
      "[Epoch: 26440] train loss: 0.1729, train acc: 0.9337, val loss: 0.1484, val acc: 0.9575  (best train acc: 0.9504, best val acc: 0.9622, best train loss: 0.1497  @ epoch 24998 )\n",
      "[Epoch: 26460] train loss: 0.1791, train acc: 0.9320, val loss: 0.1712, val acc: 0.9437  (best train acc: 0.9504, best val acc: 0.9622, best train loss: 0.1497  @ epoch 24998 )\n",
      "[Epoch: 26480] train loss: 0.1784, train acc: 0.9352, val loss: 0.1437, val acc: 0.9538  (best train acc: 0.9504, best val acc: 0.9622, best train loss: 0.1497  @ epoch 24998 )\n",
      "[Epoch: 26500] train loss: 0.2612, train acc: 0.9075, val loss: 0.1442, val acc: 0.9538  (best train acc: 0.9504, best val acc: 0.9622, best train loss: 0.1497  @ epoch 24998 )\n",
      "[Epoch: 26520] train loss: 0.1658, train acc: 0.9406, val loss: 0.1515, val acc: 0.9562  (best train acc: 0.9504, best val acc: 0.9622, best train loss: 0.1497  @ epoch 24998 )\n",
      "[Epoch: 26540] train loss: 0.1649, train acc: 0.9378, val loss: 0.1366, val acc: 0.9538  (best train acc: 0.9504, best val acc: 0.9622, best train loss: 0.1497  @ epoch 24998 )\n",
      "[Epoch: 26560] train loss: 0.1525, train acc: 0.9471, val loss: 0.1428, val acc: 0.9551  (best train acc: 0.9504, best val acc: 0.9622, best train loss: 0.1497  @ epoch 24998 )\n",
      "[Epoch: 26580] train loss: 0.1704, train acc: 0.9404, val loss: 0.1470, val acc: 0.9555  (best train acc: 0.9504, best val acc: 0.9622, best train loss: 0.1497  @ epoch 24998 )\n",
      "[Epoch: 26600] train loss: 0.1639, train acc: 0.9406, val loss: 0.1334, val acc: 0.9599  (best train acc: 0.9504, best val acc: 0.9622, best train loss: 0.1493  @ epoch 26594 )\n",
      "[Epoch: 26620] train loss: 0.1738, train acc: 0.9367, val loss: 0.1364, val acc: 0.9555  (best train acc: 0.9504, best val acc: 0.9622, best train loss: 0.1493  @ epoch 26594 )\n",
      "[Epoch: 26640] train loss: 0.1650, train acc: 0.9436, val loss: 0.1333, val acc: 0.9541  (best train acc: 0.9504, best val acc: 0.9622, best train loss: 0.1493  @ epoch 26594 )\n",
      "[Epoch: 26660] train loss: 0.1735, train acc: 0.9331, val loss: 0.1425, val acc: 0.9592  (best train acc: 0.9504, best val acc: 0.9622, best train loss: 0.1493  @ epoch 26594 )\n",
      "[Epoch: 26680] train loss: 0.1855, train acc: 0.9263, val loss: 0.1431, val acc: 0.9528  (best train acc: 0.9504, best val acc: 0.9622, best train loss: 0.1449  @ epoch 26669 )\n",
      "[Epoch: 26700] train loss: 0.1851, train acc: 0.9288, val loss: 0.1407, val acc: 0.9565  (best train acc: 0.9512, best val acc: 0.9622, best train loss: 0.1449  @ epoch 26669 )\n",
      "[Epoch: 26720] train loss: 0.1598, train acc: 0.9401, val loss: 0.1408, val acc: 0.9541  (best train acc: 0.9512, best val acc: 0.9622, best train loss: 0.1449  @ epoch 26669 )\n",
      "[Epoch: 26740] train loss: 0.1558, train acc: 0.9459, val loss: 0.1338, val acc: 0.9585  (best train acc: 0.9512, best val acc: 0.9622, best train loss: 0.1449  @ epoch 26669 )\n",
      "[Epoch: 26760] train loss: 0.1527, train acc: 0.9477, val loss: 0.1329, val acc: 0.9582  (best train acc: 0.9512, best val acc: 0.9622, best train loss: 0.1449  @ epoch 26669 )\n",
      "[Epoch: 26780] train loss: 0.1650, train acc: 0.9395, val loss: 0.1345, val acc: 0.9578  (best train acc: 0.9512, best val acc: 0.9622, best train loss: 0.1449  @ epoch 26669 )\n",
      "[Epoch: 26800] train loss: 0.1486, train acc: 0.9494, val loss: 0.1375, val acc: 0.9592  (best train acc: 0.9512, best val acc: 0.9622, best train loss: 0.1449  @ epoch 26669 )\n",
      "[Epoch: 26820] train loss: 0.1715, train acc: 0.9375, val loss: 0.1355, val acc: 0.9582  (best train acc: 0.9512, best val acc: 0.9622, best train loss: 0.1449  @ epoch 26669 )\n",
      "[Epoch: 26840] train loss: 0.1614, train acc: 0.9451, val loss: 0.1287, val acc: 0.9565  (best train acc: 0.9512, best val acc: 0.9622, best train loss: 0.1449  @ epoch 26669 )\n",
      "[Epoch: 26860] train loss: 0.1709, train acc: 0.9378, val loss: 0.1317, val acc: 0.9589  (best train acc: 0.9512, best val acc: 0.9622, best train loss: 0.1449  @ epoch 26669 )\n",
      "[Epoch: 26880] train loss: 0.1918, train acc: 0.9265, val loss: 0.1396, val acc: 0.9551  (best train acc: 0.9512, best val acc: 0.9622, best train loss: 0.1449  @ epoch 26669 )\n",
      "[Epoch: 26900] train loss: 0.1942, train acc: 0.9229, val loss: 0.1336, val acc: 0.9524  (best train acc: 0.9512, best val acc: 0.9622, best train loss: 0.1449  @ epoch 26669 )\n",
      "[Epoch: 26920] train loss: 0.1729, train acc: 0.9375, val loss: 0.1475, val acc: 0.9555  (best train acc: 0.9512, best val acc: 0.9622, best train loss: 0.1449  @ epoch 26669 )\n",
      "[Epoch: 26940] train loss: 0.1671, train acc: 0.9380, val loss: 0.1337, val acc: 0.9568  (best train acc: 0.9512, best val acc: 0.9622, best train loss: 0.1449  @ epoch 26669 )\n",
      "[Epoch: 26960] train loss: 0.1688, train acc: 0.9377, val loss: 0.1259, val acc: 0.9565  (best train acc: 0.9512, best val acc: 0.9622, best train loss: 0.1449  @ epoch 26669 )\n",
      "[Epoch: 26980] train loss: 0.1617, train acc: 0.9391, val loss: 0.1329, val acc: 0.9589  (best train acc: 0.9512, best val acc: 0.9622, best train loss: 0.1449  @ epoch 26669 )\n",
      "[Epoch: 27000] train loss: 0.1580, train acc: 0.9446, val loss: 0.1331, val acc: 0.9572  (best train acc: 0.9512, best val acc: 0.9622, best train loss: 0.1449  @ epoch 26669 )\n",
      "[Epoch: 27020] train loss: 0.1810, train acc: 0.9364, val loss: 0.1311, val acc: 0.9582  (best train acc: 0.9512, best val acc: 0.9622, best train loss: 0.1449  @ epoch 26669 )\n",
      "[Epoch: 27040] train loss: 0.1747, train acc: 0.9349, val loss: 0.1437, val acc: 0.9568  (best train acc: 0.9512, best val acc: 0.9622, best train loss: 0.1449  @ epoch 26669 )\n",
      "[Epoch: 27060] train loss: 0.1665, train acc: 0.9381, val loss: 0.1415, val acc: 0.9575  (best train acc: 0.9512, best val acc: 0.9622, best train loss: 0.1449  @ epoch 26669 )\n",
      "[Epoch: 27080] train loss: 0.1721, train acc: 0.9443, val loss: 0.1309, val acc: 0.9589  (best train acc: 0.9512, best val acc: 0.9622, best train loss: 0.1449  @ epoch 26669 )\n",
      "[Epoch: 27100] train loss: 0.1748, train acc: 0.9342, val loss: 0.1381, val acc: 0.9599  (best train acc: 0.9512, best val acc: 0.9622, best train loss: 0.1449  @ epoch 26669 )\n",
      "[Epoch: 27120] train loss: 0.1777, train acc: 0.9375, val loss: 0.1465, val acc: 0.9511  (best train acc: 0.9512, best val acc: 0.9622, best train loss: 0.1449  @ epoch 26669 )\n",
      "[Epoch: 27140] train loss: 0.1722, train acc: 0.9351, val loss: 0.1450, val acc: 0.9558  (best train acc: 0.9512, best val acc: 0.9622, best train loss: 0.1449  @ epoch 26669 )\n",
      "[Epoch: 27160] train loss: 0.1587, train acc: 0.9416, val loss: 0.1445, val acc: 0.9528  (best train acc: 0.9512, best val acc: 0.9622, best train loss: 0.1449  @ epoch 26669 )\n",
      "[Epoch: 27180] train loss: 0.1774, train acc: 0.9375, val loss: 0.1280, val acc: 0.9619  (best train acc: 0.9512, best val acc: 0.9622, best train loss: 0.1449  @ epoch 26669 )\n",
      "[Epoch: 27200] train loss: 0.1821, train acc: 0.9314, val loss: 0.1535, val acc: 0.9545  (best train acc: 0.9512, best val acc: 0.9622, best train loss: 0.1449  @ epoch 26669 )\n",
      "[Epoch: 27220] train loss: 0.2013, train acc: 0.9198, val loss: 0.1298, val acc: 0.9568  (best train acc: 0.9512, best val acc: 0.9622, best train loss: 0.1449  @ epoch 26669 )\n",
      "[Epoch: 27240] train loss: 0.1591, train acc: 0.9474, val loss: 0.1292, val acc: 0.9592  (best train acc: 0.9512, best val acc: 0.9626, best train loss: 0.1449  @ epoch 26669 )\n",
      "[Epoch: 27260] train loss: 0.1732, train acc: 0.9425, val loss: 0.1389, val acc: 0.9589  (best train acc: 0.9512, best val acc: 0.9626, best train loss: 0.1449  @ epoch 27257 )\n",
      "[Epoch: 27280] train loss: 0.1510, train acc: 0.9435, val loss: 0.1372, val acc: 0.9524  (best train acc: 0.9512, best val acc: 0.9626, best train loss: 0.1449  @ epoch 27257 )\n",
      "[Epoch: 27300] train loss: 0.1640, train acc: 0.9380, val loss: 0.1383, val acc: 0.9582  (best train acc: 0.9512, best val acc: 0.9626, best train loss: 0.1449  @ epoch 27257 )\n",
      "[Epoch: 27320] train loss: 0.1769, train acc: 0.9364, val loss: 0.1508, val acc: 0.9518  (best train acc: 0.9512, best val acc: 0.9626, best train loss: 0.1449  @ epoch 27257 )\n",
      "[Epoch: 27340] train loss: 0.1588, train acc: 0.9398, val loss: 0.1398, val acc: 0.9592  (best train acc: 0.9512, best val acc: 0.9626, best train loss: 0.1449  @ epoch 27257 )\n",
      "[Epoch: 27360] train loss: 0.1688, train acc: 0.9387, val loss: 0.1385, val acc: 0.9555  (best train acc: 0.9512, best val acc: 0.9626, best train loss: 0.1449  @ epoch 27257 )\n",
      "[Epoch: 27380] train loss: 0.1541, train acc: 0.9445, val loss: 0.1372, val acc: 0.9595  (best train acc: 0.9512, best val acc: 0.9626, best train loss: 0.1449  @ epoch 27257 )\n",
      "[Epoch: 27400] train loss: 0.1619, train acc: 0.9393, val loss: 0.1305, val acc: 0.9575  (best train acc: 0.9512, best val acc: 0.9626, best train loss: 0.1449  @ epoch 27257 )\n",
      "[Epoch: 27420] train loss: 0.1735, train acc: 0.9406, val loss: 0.1469, val acc: 0.9572  (best train acc: 0.9512, best val acc: 0.9626, best train loss: 0.1449  @ epoch 27257 )\n",
      "[Epoch: 27440] train loss: 0.1880, train acc: 0.9323, val loss: 0.1445, val acc: 0.9575  (best train acc: 0.9512, best val acc: 0.9626, best train loss: 0.1449  @ epoch 27257 )\n",
      "[Epoch: 27460] train loss: 0.1516, train acc: 0.9454, val loss: 0.1287, val acc: 0.9524  (best train acc: 0.9512, best val acc: 0.9626, best train loss: 0.1442  @ epoch 27449 )\n",
      "[Epoch: 27480] train loss: 0.1584, train acc: 0.9424, val loss: 0.1285, val acc: 0.9609  (best train acc: 0.9512, best val acc: 0.9626, best train loss: 0.1442  @ epoch 27449 )\n",
      "[Epoch: 27500] train loss: 0.1687, train acc: 0.9405, val loss: 0.1326, val acc: 0.9578  (best train acc: 0.9512, best val acc: 0.9626, best train loss: 0.1442  @ epoch 27449 )\n",
      "[Epoch: 27520] train loss: 0.1580, train acc: 0.9474, val loss: 0.1288, val acc: 0.9538  (best train acc: 0.9517, best val acc: 0.9626, best train loss: 0.1427  @ epoch 27504 )\n",
      "[Epoch: 27540] train loss: 0.1570, train acc: 0.9496, val loss: 0.1411, val acc: 0.9582  (best train acc: 0.9517, best val acc: 0.9626, best train loss: 0.1427  @ epoch 27504 )\n",
      "[Epoch: 27560] train loss: 0.1626, train acc: 0.9454, val loss: 0.1404, val acc: 0.9528  (best train acc: 0.9517, best val acc: 0.9626, best train loss: 0.1427  @ epoch 27504 )\n",
      "[Epoch: 27580] train loss: 0.1602, train acc: 0.9455, val loss: 0.1301, val acc: 0.9592  (best train acc: 0.9517, best val acc: 0.9629, best train loss: 0.1427  @ epoch 27504 )\n",
      "[Epoch: 27600] train loss: 0.1648, train acc: 0.9435, val loss: 0.1268, val acc: 0.9572  (best train acc: 0.9517, best val acc: 0.9629, best train loss: 0.1425  @ epoch 27587 )\n",
      "[Epoch: 27620] train loss: 0.1715, train acc: 0.9375, val loss: 0.1455, val acc: 0.9565  (best train acc: 0.9517, best val acc: 0.9629, best train loss: 0.1425  @ epoch 27587 )\n",
      "[Epoch: 27640] train loss: 0.1961, train acc: 0.9303, val loss: 0.1649, val acc: 0.9454  (best train acc: 0.9517, best val acc: 0.9629, best train loss: 0.1425  @ epoch 27587 )\n",
      "[Epoch: 27660] train loss: 0.1536, train acc: 0.9435, val loss: 0.1430, val acc: 0.9541  (best train acc: 0.9517, best val acc: 0.9629, best train loss: 0.1425  @ epoch 27587 )\n",
      "[Epoch: 27680] train loss: 0.1820, train acc: 0.9315, val loss: 0.1398, val acc: 0.9555  (best train acc: 0.9517, best val acc: 0.9629, best train loss: 0.1425  @ epoch 27587 )\n",
      "[Epoch: 27700] train loss: 0.1857, train acc: 0.9258, val loss: 0.1314, val acc: 0.9605  (best train acc: 0.9517, best val acc: 0.9629, best train loss: 0.1425  @ epoch 27587 )\n",
      "[Epoch: 27720] train loss: 0.1744, train acc: 0.9343, val loss: 0.1281, val acc: 0.9578  (best train acc: 0.9517, best val acc: 0.9629, best train loss: 0.1425  @ epoch 27587 )\n",
      "[Epoch: 27740] train loss: 0.2638, train acc: 0.9208, val loss: 0.1302, val acc: 0.9572  (best train acc: 0.9517, best val acc: 0.9629, best train loss: 0.1425  @ epoch 27587 )\n",
      "[Epoch: 27760] train loss: 0.1716, train acc: 0.9371, val loss: 0.1363, val acc: 0.9575  (best train acc: 0.9517, best val acc: 0.9629, best train loss: 0.1425  @ epoch 27587 )\n",
      "[Epoch: 27780] train loss: 0.1564, train acc: 0.9487, val loss: 0.1259, val acc: 0.9605  (best train acc: 0.9517, best val acc: 0.9629, best train loss: 0.1425  @ epoch 27587 )\n",
      "[Epoch: 27800] train loss: 0.1540, train acc: 0.9440, val loss: 0.1325, val acc: 0.9595  (best train acc: 0.9517, best val acc: 0.9629, best train loss: 0.1425  @ epoch 27587 )\n",
      "[Epoch: 27820] train loss: 0.1678, train acc: 0.9391, val loss: 0.1311, val acc: 0.9589  (best train acc: 0.9517, best val acc: 0.9629, best train loss: 0.1425  @ epoch 27587 )\n",
      "[Epoch: 27840] train loss: 0.1624, train acc: 0.9474, val loss: 0.1274, val acc: 0.9548  (best train acc: 0.9517, best val acc: 0.9629, best train loss: 0.1425  @ epoch 27587 )\n",
      "[Epoch: 27860] train loss: 0.1533, train acc: 0.9467, val loss: 0.1568, val acc: 0.9531  (best train acc: 0.9517, best val acc: 0.9629, best train loss: 0.1425  @ epoch 27587 )\n",
      "[Epoch: 27880] train loss: 0.1744, train acc: 0.9365, val loss: 0.1357, val acc: 0.9622  (best train acc: 0.9517, best val acc: 0.9629, best train loss: 0.1425  @ epoch 27587 )\n",
      "[Epoch: 27900] train loss: 0.1617, train acc: 0.9434, val loss: 0.1364, val acc: 0.9605  (best train acc: 0.9517, best val acc: 0.9629, best train loss: 0.1425  @ epoch 27587 )\n",
      "[Epoch: 27920] train loss: 0.1487, train acc: 0.9467, val loss: 0.1301, val acc: 0.9548  (best train acc: 0.9517, best val acc: 0.9629, best train loss: 0.1425  @ epoch 27587 )\n",
      "[Epoch: 27940] train loss: 0.1761, train acc: 0.9315, val loss: 0.1359, val acc: 0.9585  (best train acc: 0.9517, best val acc: 0.9629, best train loss: 0.1425  @ epoch 27587 )\n",
      "[Epoch: 27960] train loss: 0.1794, train acc: 0.9244, val loss: 0.1394, val acc: 0.9545  (best train acc: 0.9517, best val acc: 0.9629, best train loss: 0.1425  @ epoch 27587 )\n",
      "[Epoch: 27980] train loss: 0.1663, train acc: 0.9428, val loss: 0.1303, val acc: 0.9609  (best train acc: 0.9517, best val acc: 0.9629, best train loss: 0.1425  @ epoch 27587 )\n",
      "[Epoch: 28000] train loss: 0.1766, train acc: 0.9409, val loss: 0.1394, val acc: 0.9508  (best train acc: 0.9517, best val acc: 0.9629, best train loss: 0.1425  @ epoch 27587 )\n",
      "[Epoch: 28020] train loss: 0.1646, train acc: 0.9437, val loss: 0.1540, val acc: 0.9474  (best train acc: 0.9517, best val acc: 0.9629, best train loss: 0.1425  @ epoch 27587 )\n",
      "[Epoch: 28040] train loss: 0.1949, train acc: 0.9271, val loss: 0.1309, val acc: 0.9504  (best train acc: 0.9517, best val acc: 0.9629, best train loss: 0.1425  @ epoch 27587 )\n",
      "[Epoch: 28060] train loss: 0.1629, train acc: 0.9397, val loss: 0.1286, val acc: 0.9551  (best train acc: 0.9517, best val acc: 0.9629, best train loss: 0.1425  @ epoch 27587 )\n",
      "[Epoch: 28080] train loss: 0.1616, train acc: 0.9454, val loss: 0.1276, val acc: 0.9619  (best train acc: 0.9517, best val acc: 0.9629, best train loss: 0.1425  @ epoch 27587 )\n",
      "[Epoch: 28100] train loss: 0.1610, train acc: 0.9398, val loss: 0.1344, val acc: 0.9538  (best train acc: 0.9517, best val acc: 0.9632, best train loss: 0.1425  @ epoch 27587 )\n",
      "[Epoch: 28120] train loss: 0.1978, train acc: 0.9250, val loss: 0.1274, val acc: 0.9541  (best train acc: 0.9517, best val acc: 0.9632, best train loss: 0.1425  @ epoch 27587 )\n",
      "[Epoch: 28140] train loss: 0.1627, train acc: 0.9407, val loss: 0.1375, val acc: 0.9582  (best train acc: 0.9517, best val acc: 0.9632, best train loss: 0.1425  @ epoch 27587 )\n",
      "[Epoch: 28160] train loss: 0.1606, train acc: 0.9435, val loss: 0.1387, val acc: 0.9565  (best train acc: 0.9517, best val acc: 0.9632, best train loss: 0.1425  @ epoch 27587 )\n",
      "[Epoch: 28180] train loss: 0.1673, train acc: 0.9437, val loss: 0.1347, val acc: 0.9457  (best train acc: 0.9517, best val acc: 0.9632, best train loss: 0.1425  @ epoch 27587 )\n",
      "[Epoch: 28200] train loss: 0.1639, train acc: 0.9426, val loss: 0.1257, val acc: 0.9585  (best train acc: 0.9517, best val acc: 0.9632, best train loss: 0.1425  @ epoch 27587 )\n",
      "[Epoch: 28220] train loss: 0.1744, train acc: 0.9380, val loss: 0.1486, val acc: 0.9504  (best train acc: 0.9517, best val acc: 0.9632, best train loss: 0.1425  @ epoch 27587 )\n",
      "[Epoch: 28240] train loss: 0.1581, train acc: 0.9440, val loss: 0.1265, val acc: 0.9592  (best train acc: 0.9517, best val acc: 0.9632, best train loss: 0.1425  @ epoch 27587 )\n",
      "[Epoch: 28260] train loss: 0.1539, train acc: 0.9453, val loss: 0.1278, val acc: 0.9592  (best train acc: 0.9517, best val acc: 0.9632, best train loss: 0.1425  @ epoch 27587 )\n",
      "[Epoch: 28280] train loss: 0.1722, train acc: 0.9369, val loss: 0.1370, val acc: 0.9582  (best train acc: 0.9517, best val acc: 0.9632, best train loss: 0.1425  @ epoch 27587 )\n",
      "[Epoch: 28300] train loss: 0.1735, train acc: 0.9380, val loss: 0.1231, val acc: 0.9589  (best train acc: 0.9517, best val acc: 0.9632, best train loss: 0.1425  @ epoch 27587 )\n",
      "[Epoch: 28320] train loss: 0.1723, train acc: 0.9402, val loss: 0.1497, val acc: 0.9470  (best train acc: 0.9517, best val acc: 0.9632, best train loss: 0.1425  @ epoch 27587 )\n",
      "[Epoch: 28340] train loss: 0.1786, train acc: 0.9367, val loss: 0.1269, val acc: 0.9575  (best train acc: 0.9517, best val acc: 0.9632, best train loss: 0.1425  @ epoch 27587 )\n",
      "[Epoch: 28360] train loss: 0.1566, train acc: 0.9475, val loss: 0.1433, val acc: 0.9562  (best train acc: 0.9517, best val acc: 0.9632, best train loss: 0.1425  @ epoch 27587 )\n",
      "[Epoch: 28380] train loss: 0.1517, train acc: 0.9477, val loss: 0.1304, val acc: 0.9585  (best train acc: 0.9517, best val acc: 0.9632, best train loss: 0.1425  @ epoch 27587 )\n",
      "[Epoch: 28400] train loss: 0.1484, train acc: 0.9461, val loss: 0.1339, val acc: 0.9585  (best train acc: 0.9517, best val acc: 0.9632, best train loss: 0.1425  @ epoch 27587 )\n",
      "[Epoch: 28420] train loss: 0.1599, train acc: 0.9414, val loss: 0.1387, val acc: 0.9582  (best train acc: 0.9517, best val acc: 0.9632, best train loss: 0.1425  @ epoch 27587 )\n",
      "[Epoch: 28440] train loss: 0.1505, train acc: 0.9426, val loss: 0.1350, val acc: 0.9572  (best train acc: 0.9517, best val acc: 0.9632, best train loss: 0.1425  @ epoch 27587 )\n",
      "[Epoch: 28460] train loss: 0.1843, train acc: 0.9332, val loss: 0.1491, val acc: 0.9491  (best train acc: 0.9517, best val acc: 0.9632, best train loss: 0.1425  @ epoch 27587 )\n",
      "[Epoch: 28480] train loss: 0.1581, train acc: 0.9437, val loss: 0.1372, val acc: 0.9592  (best train acc: 0.9517, best val acc: 0.9632, best train loss: 0.1425  @ epoch 27587 )\n",
      "[Epoch: 28500] train loss: 0.1670, train acc: 0.9382, val loss: 0.1385, val acc: 0.9521  (best train acc: 0.9517, best val acc: 0.9632, best train loss: 0.1425  @ epoch 27587 )\n",
      "[Epoch: 28520] train loss: 0.1773, train acc: 0.9409, val loss: 0.1387, val acc: 0.9568  (best train acc: 0.9517, best val acc: 0.9632, best train loss: 0.1409  @ epoch 28517 )\n",
      "[Epoch: 28540] train loss: 0.1498, train acc: 0.9458, val loss: 0.1302, val acc: 0.9589  (best train acc: 0.9517, best val acc: 0.9632, best train loss: 0.1409  @ epoch 28517 )\n",
      "[Epoch: 28560] train loss: 0.1507, train acc: 0.9472, val loss: 0.1314, val acc: 0.9612  (best train acc: 0.9517, best val acc: 0.9632, best train loss: 0.1409  @ epoch 28517 )\n",
      "[Epoch: 28580] train loss: 0.1530, train acc: 0.9441, val loss: 0.1342, val acc: 0.9592  (best train acc: 0.9517, best val acc: 0.9632, best train loss: 0.1409  @ epoch 28517 )\n",
      "[Epoch: 28600] train loss: 0.1928, train acc: 0.9327, val loss: 0.1240, val acc: 0.9622  (best train acc: 0.9517, best val acc: 0.9632, best train loss: 0.1409  @ epoch 28517 )\n",
      "[Epoch: 28620] train loss: 0.1509, train acc: 0.9487, val loss: 0.1346, val acc: 0.9555  (best train acc: 0.9519, best val acc: 0.9632, best train loss: 0.1377  @ epoch 28619 )\n",
      "[Epoch: 28640] train loss: 0.1495, train acc: 0.9469, val loss: 0.1367, val acc: 0.9551  (best train acc: 0.9519, best val acc: 0.9632, best train loss: 0.1377  @ epoch 28619 )\n",
      "[Epoch: 28660] train loss: 0.1652, train acc: 0.9376, val loss: 0.1247, val acc: 0.9592  (best train acc: 0.9519, best val acc: 0.9632, best train loss: 0.1377  @ epoch 28619 )\n",
      "[Epoch: 28680] train loss: 0.1641, train acc: 0.9378, val loss: 0.1388, val acc: 0.9430  (best train acc: 0.9519, best val acc: 0.9632, best train loss: 0.1377  @ epoch 28619 )\n",
      "[Epoch: 28700] train loss: 0.1587, train acc: 0.9440, val loss: 0.1272, val acc: 0.9592  (best train acc: 0.9519, best val acc: 0.9632, best train loss: 0.1377  @ epoch 28619 )\n",
      "[Epoch: 28720] train loss: 0.1492, train acc: 0.9468, val loss: 0.1292, val acc: 0.9548  (best train acc: 0.9519, best val acc: 0.9632, best train loss: 0.1377  @ epoch 28619 )\n",
      "[Epoch: 28740] train loss: 0.1611, train acc: 0.9441, val loss: 0.1379, val acc: 0.9589  (best train acc: 0.9519, best val acc: 0.9632, best train loss: 0.1377  @ epoch 28619 )\n",
      "[Epoch: 28760] train loss: 0.1870, train acc: 0.9310, val loss: 0.1310, val acc: 0.9585  (best train acc: 0.9519, best val acc: 0.9632, best train loss: 0.1377  @ epoch 28619 )\n",
      "[Epoch: 28780] train loss: 0.1731, train acc: 0.9398, val loss: 0.1415, val acc: 0.9558  (best train acc: 0.9519, best val acc: 0.9632, best train loss: 0.1377  @ epoch 28619 )\n",
      "[Epoch: 28800] train loss: 0.1687, train acc: 0.9362, val loss: 0.1331, val acc: 0.9599  (best train acc: 0.9519, best val acc: 0.9632, best train loss: 0.1377  @ epoch 28619 )\n",
      "[Epoch: 28820] train loss: 0.1651, train acc: 0.9419, val loss: 0.1393, val acc: 0.9582  (best train acc: 0.9519, best val acc: 0.9632, best train loss: 0.1377  @ epoch 28619 )\n",
      "[Epoch: 28840] train loss: 0.1639, train acc: 0.9454, val loss: 0.1228, val acc: 0.9578  (best train acc: 0.9519, best val acc: 0.9632, best train loss: 0.1377  @ epoch 28619 )\n",
      "[Epoch: 28860] train loss: 0.1559, train acc: 0.9435, val loss: 0.1324, val acc: 0.9585  (best train acc: 0.9519, best val acc: 0.9632, best train loss: 0.1377  @ epoch 28619 )\n",
      "[Epoch: 28880] train loss: 0.1743, train acc: 0.9391, val loss: 0.1325, val acc: 0.9464  (best train acc: 0.9519, best val acc: 0.9632, best train loss: 0.1377  @ epoch 28619 )\n",
      "[Epoch: 28900] train loss: 0.1660, train acc: 0.9421, val loss: 0.1353, val acc: 0.9595  (best train acc: 0.9519, best val acc: 0.9632, best train loss: 0.1377  @ epoch 28619 )\n",
      "[Epoch: 28920] train loss: 0.1631, train acc: 0.9402, val loss: 0.1299, val acc: 0.9589  (best train acc: 0.9519, best val acc: 0.9632, best train loss: 0.1377  @ epoch 28619 )\n",
      "[Epoch: 28940] train loss: 0.1799, train acc: 0.9344, val loss: 0.1301, val acc: 0.9602  (best train acc: 0.9519, best val acc: 0.9632, best train loss: 0.1377  @ epoch 28619 )\n",
      "[Epoch: 28960] train loss: 0.1622, train acc: 0.9456, val loss: 0.1320, val acc: 0.9599  (best train acc: 0.9519, best val acc: 0.9632, best train loss: 0.1377  @ epoch 28619 )\n",
      "[Epoch: 28980] train loss: 0.1642, train acc: 0.9410, val loss: 0.1475, val acc: 0.9541  (best train acc: 0.9519, best val acc: 0.9632, best train loss: 0.1377  @ epoch 28619 )\n",
      "[Epoch: 29000] train loss: 0.1768, train acc: 0.9299, val loss: 0.1310, val acc: 0.9589  (best train acc: 0.9519, best val acc: 0.9632, best train loss: 0.1377  @ epoch 28619 )\n",
      "[Epoch: 29020] train loss: 0.1669, train acc: 0.9398, val loss: 0.1260, val acc: 0.9548  (best train acc: 0.9519, best val acc: 0.9632, best train loss: 0.1377  @ epoch 28619 )\n",
      "[Epoch: 29040] train loss: 0.1457, train acc: 0.9492, val loss: 0.1347, val acc: 0.9619  (best train acc: 0.9519, best val acc: 0.9632, best train loss: 0.1377  @ epoch 28619 )\n",
      "[Epoch: 29060] train loss: 0.1462, train acc: 0.9464, val loss: 0.1259, val acc: 0.9602  (best train acc: 0.9519, best val acc: 0.9632, best train loss: 0.1377  @ epoch 28619 )\n",
      "[Epoch: 29080] train loss: 0.1640, train acc: 0.9391, val loss: 0.1295, val acc: 0.9592  (best train acc: 0.9519, best val acc: 0.9632, best train loss: 0.1377  @ epoch 28619 )\n",
      "[Epoch: 29100] train loss: 0.1449, train acc: 0.9459, val loss: 0.1345, val acc: 0.9518  (best train acc: 0.9519, best val acc: 0.9632, best train loss: 0.1377  @ epoch 28619 )\n",
      "[Epoch: 29120] train loss: 0.1624, train acc: 0.9448, val loss: 0.1323, val acc: 0.9575  (best train acc: 0.9519, best val acc: 0.9632, best train loss: 0.1377  @ epoch 28619 )\n",
      "[Epoch: 29140] train loss: 0.1491, train acc: 0.9462, val loss: 0.1301, val acc: 0.9562  (best train acc: 0.9519, best val acc: 0.9632, best train loss: 0.1377  @ epoch 28619 )\n",
      "[Epoch: 29160] train loss: 0.1725, train acc: 0.9351, val loss: 0.1347, val acc: 0.9551  (best train acc: 0.9519, best val acc: 0.9632, best train loss: 0.1377  @ epoch 28619 )\n",
      "[Epoch: 29180] train loss: 0.1604, train acc: 0.9419, val loss: 0.1364, val acc: 0.9558  (best train acc: 0.9519, best val acc: 0.9632, best train loss: 0.1377  @ epoch 28619 )\n",
      "[Epoch: 29200] train loss: 0.1656, train acc: 0.9375, val loss: 0.1350, val acc: 0.9592  (best train acc: 0.9519, best val acc: 0.9632, best train loss: 0.1377  @ epoch 28619 )\n",
      "[Epoch: 29220] train loss: 0.1712, train acc: 0.9366, val loss: 0.1369, val acc: 0.9558  (best train acc: 0.9519, best val acc: 0.9632, best train loss: 0.1377  @ epoch 28619 )\n",
      "[Epoch: 29240] train loss: 0.1656, train acc: 0.9404, val loss: 0.1284, val acc: 0.9622  (best train acc: 0.9519, best val acc: 0.9632, best train loss: 0.1377  @ epoch 28619 )\n",
      "[Epoch: 29260] train loss: 0.1523, train acc: 0.9454, val loss: 0.1323, val acc: 0.9585  (best train acc: 0.9519, best val acc: 0.9632, best train loss: 0.1377  @ epoch 28619 )\n",
      "[Epoch: 29280] train loss: 0.1584, train acc: 0.9465, val loss: 0.1295, val acc: 0.9612  (best train acc: 0.9519, best val acc: 0.9632, best train loss: 0.1377  @ epoch 28619 )\n",
      "[Epoch: 29300] train loss: 0.1691, train acc: 0.9402, val loss: 0.1310, val acc: 0.9572  (best train acc: 0.9519, best val acc: 0.9632, best train loss: 0.1377  @ epoch 28619 )\n",
      "[Epoch: 29320] train loss: 0.1520, train acc: 0.9452, val loss: 0.1311, val acc: 0.9605  (best train acc: 0.9519, best val acc: 0.9632, best train loss: 0.1377  @ epoch 28619 )\n",
      "[Epoch: 29340] train loss: 0.1557, train acc: 0.9445, val loss: 0.1368, val acc: 0.9548  (best train acc: 0.9519, best val acc: 0.9632, best train loss: 0.1377  @ epoch 28619 )\n",
      "[Epoch: 29360] train loss: 0.1668, train acc: 0.9349, val loss: 0.1373, val acc: 0.9541  (best train acc: 0.9519, best val acc: 0.9632, best train loss: 0.1377  @ epoch 28619 )\n",
      "[Epoch: 29380] train loss: 0.1559, train acc: 0.9477, val loss: 0.1358, val acc: 0.9589  (best train acc: 0.9519, best val acc: 0.9632, best train loss: 0.1377  @ epoch 28619 )\n",
      "[Epoch: 29400] train loss: 0.1725, train acc: 0.9338, val loss: 0.1324, val acc: 0.9592  (best train acc: 0.9519, best val acc: 0.9632, best train loss: 0.1377  @ epoch 28619 )\n",
      "[Epoch: 29420] train loss: 0.1589, train acc: 0.9461, val loss: 0.1286, val acc: 0.9508  (best train acc: 0.9519, best val acc: 0.9632, best train loss: 0.1377  @ epoch 28619 )\n",
      "[Epoch: 29440] train loss: 0.1828, train acc: 0.9340, val loss: 0.1383, val acc: 0.9562  (best train acc: 0.9519, best val acc: 0.9632, best train loss: 0.1377  @ epoch 28619 )\n",
      "[Epoch: 29460] train loss: 0.1628, train acc: 0.9466, val loss: 0.1458, val acc: 0.9548  (best train acc: 0.9519, best val acc: 0.9632, best train loss: 0.1377  @ epoch 28619 )\n",
      "[Epoch: 29480] train loss: 0.1584, train acc: 0.9432, val loss: 0.1276, val acc: 0.9616  (best train acc: 0.9519, best val acc: 0.9632, best train loss: 0.1377  @ epoch 28619 )\n",
      "[Epoch: 29500] train loss: 0.1714, train acc: 0.9391, val loss: 0.1347, val acc: 0.9595  (best train acc: 0.9519, best val acc: 0.9632, best train loss: 0.1377  @ epoch 28619 )\n",
      "[Epoch: 29520] train loss: 0.1505, train acc: 0.9457, val loss: 0.1346, val acc: 0.9592  (best train acc: 0.9519, best val acc: 0.9632, best train loss: 0.1377  @ epoch 28619 )\n",
      "[Epoch: 29540] train loss: 0.1582, train acc: 0.9453, val loss: 0.1351, val acc: 0.9541  (best train acc: 0.9519, best val acc: 0.9632, best train loss: 0.1377  @ epoch 28619 )\n",
      "[Epoch: 29560] train loss: 0.1953, train acc: 0.9274, val loss: 0.1772, val acc: 0.9454  (best train acc: 0.9519, best val acc: 0.9632, best train loss: 0.1377  @ epoch 28619 )\n",
      "[Epoch: 29580] train loss: 0.1517, train acc: 0.9507, val loss: 0.1341, val acc: 0.9595  (best train acc: 0.9519, best val acc: 0.9632, best train loss: 0.1377  @ epoch 28619 )\n",
      "[Epoch: 29600] train loss: 0.1694, train acc: 0.9407, val loss: 0.1258, val acc: 0.9578  (best train acc: 0.9519, best val acc: 0.9632, best train loss: 0.1377  @ epoch 28619 )\n",
      "[Epoch: 29620] train loss: 0.1626, train acc: 0.9427, val loss: 0.1323, val acc: 0.9585  (best train acc: 0.9519, best val acc: 0.9632, best train loss: 0.1377  @ epoch 28619 )\n",
      "[Epoch: 29640] train loss: 0.1639, train acc: 0.9435, val loss: 0.1402, val acc: 0.9541  (best train acc: 0.9519, best val acc: 0.9632, best train loss: 0.1377  @ epoch 28619 )\n",
      "[Epoch: 29660] train loss: 0.1694, train acc: 0.9430, val loss: 0.1289, val acc: 0.9612  (best train acc: 0.9519, best val acc: 0.9639, best train loss: 0.1377  @ epoch 28619 )\n",
      "[Epoch: 29680] train loss: 0.1921, train acc: 0.9346, val loss: 0.1286, val acc: 0.9599  (best train acc: 0.9519, best val acc: 0.9639, best train loss: 0.1377  @ epoch 28619 )\n",
      "[Epoch: 29700] train loss: 0.1574, train acc: 0.9466, val loss: 0.1580, val acc: 0.9470  (best train acc: 0.9519, best val acc: 0.9639, best train loss: 0.1377  @ epoch 28619 )\n",
      "[Epoch: 29720] train loss: 0.1492, train acc: 0.9495, val loss: 0.1372, val acc: 0.9589  (best train acc: 0.9532, best val acc: 0.9639, best train loss: 0.1369  @ epoch 29712 )\n",
      "[Epoch: 29740] train loss: 0.1496, train acc: 0.9477, val loss: 0.1297, val acc: 0.9511  (best train acc: 0.9532, best val acc: 0.9639, best train loss: 0.1369  @ epoch 29712 )\n",
      "[Epoch: 29760] train loss: 0.1635, train acc: 0.9395, val loss: 0.1320, val acc: 0.9589  (best train acc: 0.9532, best val acc: 0.9639, best train loss: 0.1369  @ epoch 29712 )\n",
      "[Epoch: 29780] train loss: 0.1689, train acc: 0.9398, val loss: 0.1339, val acc: 0.9605  (best train acc: 0.9532, best val acc: 0.9639, best train loss: 0.1369  @ epoch 29712 )\n",
      "[Epoch: 29800] train loss: 0.1627, train acc: 0.9392, val loss: 0.1326, val acc: 0.9477  (best train acc: 0.9532, best val acc: 0.9639, best train loss: 0.1369  @ epoch 29712 )\n",
      "[Epoch: 29820] train loss: 0.1508, train acc: 0.9471, val loss: 0.1352, val acc: 0.9572  (best train acc: 0.9532, best val acc: 0.9639, best train loss: 0.1369  @ epoch 29712 )\n",
      "[Epoch: 29840] train loss: 0.1759, train acc: 0.9338, val loss: 0.1462, val acc: 0.9511  (best train acc: 0.9532, best val acc: 0.9639, best train loss: 0.1369  @ epoch 29712 )\n",
      "[Epoch: 29860] train loss: 0.1589, train acc: 0.9382, val loss: 0.1450, val acc: 0.9555  (best train acc: 0.9532, best val acc: 0.9639, best train loss: 0.1369  @ epoch 29712 )\n",
      "[Epoch: 29880] train loss: 0.1662, train acc: 0.9442, val loss: 0.1308, val acc: 0.9609  (best train acc: 0.9532, best val acc: 0.9639, best train loss: 0.1369  @ epoch 29712 )\n",
      "[Epoch: 29900] train loss: 0.1595, train acc: 0.9460, val loss: 0.1481, val acc: 0.9538  (best train acc: 0.9532, best val acc: 0.9639, best train loss: 0.1369  @ epoch 29712 )\n",
      "[Epoch: 29920] train loss: 0.1577, train acc: 0.9430, val loss: 0.1288, val acc: 0.9568  (best train acc: 0.9532, best val acc: 0.9639, best train loss: 0.1369  @ epoch 29712 )\n",
      "[Epoch: 29940] train loss: 0.1529, train acc: 0.9481, val loss: 0.1268, val acc: 0.9568  (best train acc: 0.9532, best val acc: 0.9639, best train loss: 0.1369  @ epoch 29712 )\n",
      "[Epoch: 29960] train loss: 0.1580, train acc: 0.9413, val loss: 0.1392, val acc: 0.9572  (best train acc: 0.9532, best val acc: 0.9639, best train loss: 0.1369  @ epoch 29712 )\n",
      "[Epoch: 29980] train loss: 0.1855, train acc: 0.9271, val loss: 0.1330, val acc: 0.9497  (best train acc: 0.9532, best val acc: 0.9639, best train loss: 0.1369  @ epoch 29712 )\n",
      "[Epoch: 30000] train loss: 0.1631, train acc: 0.9440, val loss: 0.1335, val acc: 0.9589  (best train acc: 0.9532, best val acc: 0.9639, best train loss: 0.1369  @ epoch 29712 )\n",
      "[Epoch: 30020] train loss: 0.1715, train acc: 0.9393, val loss: 0.1383, val acc: 0.9565  (best train acc: 0.9532, best val acc: 0.9639, best train loss: 0.1369  @ epoch 29712 )\n",
      "[Epoch: 30040] train loss: 0.1840, train acc: 0.9303, val loss: 0.1578, val acc: 0.9433  (best train acc: 0.9532, best val acc: 0.9639, best train loss: 0.1369  @ epoch 29712 )\n",
      "[Epoch: 30060] train loss: 0.1777, train acc: 0.9413, val loss: 0.1245, val acc: 0.9558  (best train acc: 0.9532, best val acc: 0.9639, best train loss: 0.1369  @ epoch 29712 )\n",
      "[Epoch: 30080] train loss: 0.1887, train acc: 0.9342, val loss: 0.1314, val acc: 0.9565  (best train acc: 0.9532, best val acc: 0.9639, best train loss: 0.1369  @ epoch 29712 )\n",
      "[Epoch: 30100] train loss: 0.1597, train acc: 0.9412, val loss: 0.1331, val acc: 0.9589  (best train acc: 0.9532, best val acc: 0.9639, best train loss: 0.1369  @ epoch 29712 )\n",
      "[Epoch: 30120] train loss: 0.1813, train acc: 0.9354, val loss: 0.1269, val acc: 0.9528  (best train acc: 0.9532, best val acc: 0.9639, best train loss: 0.1369  @ epoch 29712 )\n",
      "[Epoch: 30140] train loss: 0.1674, train acc: 0.9394, val loss: 0.1301, val acc: 0.9551  (best train acc: 0.9532, best val acc: 0.9639, best train loss: 0.1369  @ epoch 29712 )\n",
      "[Epoch: 30160] train loss: 0.1670, train acc: 0.9412, val loss: 0.1327, val acc: 0.9501  (best train acc: 0.9532, best val acc: 0.9639, best train loss: 0.1369  @ epoch 29712 )\n",
      "[Epoch: 30180] train loss: 0.1637, train acc: 0.9388, val loss: 0.1262, val acc: 0.9572  (best train acc: 0.9532, best val acc: 0.9639, best train loss: 0.1369  @ epoch 29712 )\n",
      "[Epoch: 30200] train loss: 0.1521, train acc: 0.9443, val loss: 0.1276, val acc: 0.9599  (best train acc: 0.9532, best val acc: 0.9639, best train loss: 0.1369  @ epoch 29712 )\n",
      "[Epoch: 30220] train loss: 0.1561, train acc: 0.9445, val loss: 0.1321, val acc: 0.9565  (best train acc: 0.9532, best val acc: 0.9639, best train loss: 0.1369  @ epoch 29712 )\n",
      "[Epoch: 30240] train loss: 0.1434, train acc: 0.9489, val loss: 0.1434, val acc: 0.9589  (best train acc: 0.9532, best val acc: 0.9639, best train loss: 0.1369  @ epoch 29712 )\n",
      "[Epoch: 30260] train loss: 0.1523, train acc: 0.9485, val loss: 0.1303, val acc: 0.9589  (best train acc: 0.9532, best val acc: 0.9639, best train loss: 0.1369  @ epoch 29712 )\n",
      "[Epoch: 30280] train loss: 0.1898, train acc: 0.9245, val loss: 0.1300, val acc: 0.9562  (best train acc: 0.9532, best val acc: 0.9639, best train loss: 0.1369  @ epoch 29712 )\n",
      "[Epoch: 30300] train loss: 0.1590, train acc: 0.9410, val loss: 0.1391, val acc: 0.9524  (best train acc: 0.9532, best val acc: 0.9639, best train loss: 0.1369  @ epoch 29712 )\n",
      "[Epoch: 30320] train loss: 0.1658, train acc: 0.9378, val loss: 0.1280, val acc: 0.9568  (best train acc: 0.9532, best val acc: 0.9639, best train loss: 0.1369  @ epoch 29712 )\n",
      "[Epoch: 30340] train loss: 0.1650, train acc: 0.9419, val loss: 0.1388, val acc: 0.9531  (best train acc: 0.9532, best val acc: 0.9639, best train loss: 0.1369  @ epoch 29712 )\n",
      "[Epoch: 30360] train loss: 0.1793, train acc: 0.9328, val loss: 0.1471, val acc: 0.9484  (best train acc: 0.9532, best val acc: 0.9639, best train loss: 0.1369  @ epoch 29712 )\n",
      "[Epoch: 30380] train loss: 0.1700, train acc: 0.9340, val loss: 0.1340, val acc: 0.9558  (best train acc: 0.9532, best val acc: 0.9639, best train loss: 0.1369  @ epoch 29712 )\n",
      "[Epoch: 30400] train loss: 0.1737, train acc: 0.9382, val loss: 0.1719, val acc: 0.9251  (best train acc: 0.9532, best val acc: 0.9639, best train loss: 0.1369  @ epoch 29712 )\n",
      "[Epoch: 30420] train loss: 0.1710, train acc: 0.9312, val loss: 0.1404, val acc: 0.9514  (best train acc: 0.9532, best val acc: 0.9639, best train loss: 0.1369  @ epoch 29712 )\n",
      "[Epoch: 30440] train loss: 0.1479, train acc: 0.9458, val loss: 0.1454, val acc: 0.9524  (best train acc: 0.9532, best val acc: 0.9639, best train loss: 0.1369  @ epoch 29712 )\n",
      "[Epoch: 30460] train loss: 0.1501, train acc: 0.9437, val loss: 0.1324, val acc: 0.9568  (best train acc: 0.9532, best val acc: 0.9639, best train loss: 0.1369  @ epoch 29712 )\n",
      "[Epoch: 30480] train loss: 0.1635, train acc: 0.9427, val loss: 0.1287, val acc: 0.9592  (best train acc: 0.9532, best val acc: 0.9639, best train loss: 0.1369  @ epoch 29712 )\n",
      "[Epoch: 30500] train loss: 0.1503, train acc: 0.9434, val loss: 0.1269, val acc: 0.9589  (best train acc: 0.9532, best val acc: 0.9639, best train loss: 0.1369  @ epoch 29712 )\n",
      "[Epoch: 30520] train loss: 0.1689, train acc: 0.9383, val loss: 0.1266, val acc: 0.9582  (best train acc: 0.9532, best val acc: 0.9639, best train loss: 0.1369  @ epoch 29712 )\n",
      "[Epoch: 30540] train loss: 0.1495, train acc: 0.9488, val loss: 0.1410, val acc: 0.9565  (best train acc: 0.9532, best val acc: 0.9639, best train loss: 0.1369  @ epoch 29712 )\n",
      "[Epoch: 30560] train loss: 0.1665, train acc: 0.9356, val loss: 0.1365, val acc: 0.9558  (best train acc: 0.9532, best val acc: 0.9639, best train loss: 0.1369  @ epoch 29712 )\n",
      "[Epoch: 30580] train loss: 0.1657, train acc: 0.9393, val loss: 0.1520, val acc: 0.9497  (best train acc: 0.9532, best val acc: 0.9639, best train loss: 0.1369  @ epoch 29712 )\n",
      "[Epoch: 30600] train loss: 0.1890, train acc: 0.9268, val loss: 0.1376, val acc: 0.9545  (best train acc: 0.9532, best val acc: 0.9639, best train loss: 0.1369  @ epoch 29712 )\n",
      "[Epoch: 30620] train loss: 0.1907, train acc: 0.9271, val loss: 0.1352, val acc: 0.9464  (best train acc: 0.9532, best val acc: 0.9639, best train loss: 0.1369  @ epoch 29712 )\n",
      "[Epoch: 30640] train loss: 0.1759, train acc: 0.9336, val loss: 0.1476, val acc: 0.9538  (best train acc: 0.9532, best val acc: 0.9639, best train loss: 0.1369  @ epoch 29712 )\n",
      "[Epoch: 30660] train loss: 0.1872, train acc: 0.9366, val loss: 0.1314, val acc: 0.9521  (best train acc: 0.9532, best val acc: 0.9639, best train loss: 0.1369  @ epoch 29712 )\n",
      "[Epoch: 30680] train loss: 0.1954, train acc: 0.9381, val loss: 0.1574, val acc: 0.9508  (best train acc: 0.9532, best val acc: 0.9639, best train loss: 0.1369  @ epoch 29712 )\n",
      "[Epoch: 30700] train loss: 0.1476, train acc: 0.9444, val loss: 0.1388, val acc: 0.9585  (best train acc: 0.9532, best val acc: 0.9639, best train loss: 0.1369  @ epoch 29712 )\n",
      "[Epoch: 30720] train loss: 0.1543, train acc: 0.9465, val loss: 0.1348, val acc: 0.9521  (best train acc: 0.9532, best val acc: 0.9639, best train loss: 0.1369  @ epoch 29712 )\n",
      "[Epoch: 30740] train loss: 0.1653, train acc: 0.9410, val loss: 0.1407, val acc: 0.9568  (best train acc: 0.9532, best val acc: 0.9639, best train loss: 0.1369  @ epoch 29712 )\n",
      "[Epoch: 30760] train loss: 0.1601, train acc: 0.9475, val loss: 0.1325, val acc: 0.9599  (best train acc: 0.9532, best val acc: 0.9639, best train loss: 0.1369  @ epoch 29712 )\n",
      "[Epoch: 30780] train loss: 0.1507, train acc: 0.9474, val loss: 0.1349, val acc: 0.9582  (best train acc: 0.9532, best val acc: 0.9639, best train loss: 0.1369  @ epoch 29712 )\n",
      "[Epoch: 30800] train loss: 0.1870, train acc: 0.9328, val loss: 0.1305, val acc: 0.9548  (best train acc: 0.9532, best val acc: 0.9639, best train loss: 0.1369  @ epoch 29712 )\n",
      "[Epoch: 30820] train loss: 0.1545, train acc: 0.9410, val loss: 0.1558, val acc: 0.9484  (best train acc: 0.9532, best val acc: 0.9639, best train loss: 0.1369  @ epoch 29712 )\n",
      "[Epoch: 30840] train loss: 0.1583, train acc: 0.9423, val loss: 0.1323, val acc: 0.9585  (best train acc: 0.9532, best val acc: 0.9639, best train loss: 0.1369  @ epoch 29712 )\n",
      "[Epoch: 30860] train loss: 0.1707, train acc: 0.9345, val loss: 0.1457, val acc: 0.9437  (best train acc: 0.9532, best val acc: 0.9639, best train loss: 0.1369  @ epoch 29712 )\n",
      "[Epoch: 30880] train loss: 0.1693, train acc: 0.9386, val loss: 0.1267, val acc: 0.9602  (best train acc: 0.9532, best val acc: 0.9639, best train loss: 0.1369  @ epoch 29712 )\n",
      "[Epoch: 30900] train loss: 0.1583, train acc: 0.9406, val loss: 0.1312, val acc: 0.9589  (best train acc: 0.9532, best val acc: 0.9639, best train loss: 0.1369  @ epoch 29712 )\n",
      "[Epoch: 30920] train loss: 0.1763, train acc: 0.9370, val loss: 0.1435, val acc: 0.9524  (best train acc: 0.9532, best val acc: 0.9639, best train loss: 0.1369  @ epoch 29712 )\n",
      "[Epoch: 30940] train loss: 0.1591, train acc: 0.9407, val loss: 0.1361, val acc: 0.9565  (best train acc: 0.9532, best val acc: 0.9639, best train loss: 0.1369  @ epoch 29712 )\n",
      "[Epoch: 30960] train loss: 0.1529, train acc: 0.9481, val loss: 0.1313, val acc: 0.9582  (best train acc: 0.9532, best val acc: 0.9639, best train loss: 0.1369  @ epoch 29712 )\n",
      "[Epoch: 30980] train loss: 0.2279, train acc: 0.9132, val loss: 0.1632, val acc: 0.9440  (best train acc: 0.9532, best val acc: 0.9639, best train loss: 0.1369  @ epoch 29712 )\n",
      "[Epoch: 31000] train loss: 0.1694, train acc: 0.9418, val loss: 0.1266, val acc: 0.9578  (best train acc: 0.9532, best val acc: 0.9639, best train loss: 0.1369  @ epoch 29712 )\n",
      "[Epoch: 31020] train loss: 0.1523, train acc: 0.9457, val loss: 0.1255, val acc: 0.9592  (best train acc: 0.9532, best val acc: 0.9639, best train loss: 0.1369  @ epoch 29712 )\n",
      "[Epoch: 31040] train loss: 0.1519, train acc: 0.9455, val loss: 0.1305, val acc: 0.9572  (best train acc: 0.9532, best val acc: 0.9639, best train loss: 0.1369  @ epoch 29712 )\n",
      "[Epoch: 31060] train loss: 0.1601, train acc: 0.9420, val loss: 0.1233, val acc: 0.9565  (best train acc: 0.9532, best val acc: 0.9639, best train loss: 0.1369  @ epoch 29712 )\n",
      "[Epoch: 31080] train loss: 0.1629, train acc: 0.9398, val loss: 0.1464, val acc: 0.9562  (best train acc: 0.9532, best val acc: 0.9639, best train loss: 0.1369  @ epoch 29712 )\n",
      "[Epoch: 31100] train loss: 0.1570, train acc: 0.9438, val loss: 0.1375, val acc: 0.9575  (best train acc: 0.9532, best val acc: 0.9639, best train loss: 0.1369  @ epoch 29712 )\n",
      "[Epoch: 31120] train loss: 0.1608, train acc: 0.9436, val loss: 0.1303, val acc: 0.9592  (best train acc: 0.9532, best val acc: 0.9639, best train loss: 0.1369  @ epoch 29712 )\n",
      "[Epoch: 31140] train loss: 0.2013, train acc: 0.9250, val loss: 0.1357, val acc: 0.9558  (best train acc: 0.9532, best val acc: 0.9639, best train loss: 0.1369  @ epoch 29712 )\n",
      "[Epoch: 31160] train loss: 0.1641, train acc: 0.9424, val loss: 0.1479, val acc: 0.9548  (best train acc: 0.9532, best val acc: 0.9639, best train loss: 0.1369  @ epoch 29712 )\n",
      "[Epoch: 31180] train loss: 0.1648, train acc: 0.9402, val loss: 0.1384, val acc: 0.9575  (best train acc: 0.9532, best val acc: 0.9639, best train loss: 0.1369  @ epoch 29712 )\n",
      "[Epoch: 31200] train loss: 0.1505, train acc: 0.9465, val loss: 0.1388, val acc: 0.9595  (best train acc: 0.9532, best val acc: 0.9639, best train loss: 0.1369  @ epoch 29712 )\n",
      "[Epoch: 31220] train loss: 0.1480, train acc: 0.9477, val loss: 0.1522, val acc: 0.9562  (best train acc: 0.9532, best val acc: 0.9639, best train loss: 0.1369  @ epoch 29712 )\n",
      "[Epoch: 31240] train loss: 0.1508, train acc: 0.9463, val loss: 0.1342, val acc: 0.9551  (best train acc: 0.9532, best val acc: 0.9639, best train loss: 0.1369  @ epoch 29712 )\n",
      "[Epoch: 31260] train loss: 0.1557, train acc: 0.9441, val loss: 0.1433, val acc: 0.9531  (best train acc: 0.9532, best val acc: 0.9639, best train loss: 0.1369  @ epoch 29712 )\n",
      "[Epoch: 31280] train loss: 0.1635, train acc: 0.9414, val loss: 0.1532, val acc: 0.9501  (best train acc: 0.9532, best val acc: 0.9639, best train loss: 0.1369  @ epoch 29712 )\n",
      "[Epoch: 31300] train loss: 0.1539, train acc: 0.9426, val loss: 0.1345, val acc: 0.9572  (best train acc: 0.9532, best val acc: 0.9639, best train loss: 0.1369  @ epoch 29712 )\n",
      "[Epoch: 31320] train loss: 0.1683, train acc: 0.9435, val loss: 0.1371, val acc: 0.9582  (best train acc: 0.9532, best val acc: 0.9639, best train loss: 0.1369  @ epoch 29712 )\n",
      "[Epoch: 31340] train loss: 0.1752, train acc: 0.9339, val loss: 0.1395, val acc: 0.9545  (best train acc: 0.9532, best val acc: 0.9639, best train loss: 0.1369  @ epoch 29712 )\n",
      "[Epoch: 31360] train loss: 0.1687, train acc: 0.9394, val loss: 0.1491, val acc: 0.9531  (best train acc: 0.9532, best val acc: 0.9639, best train loss: 0.1369  @ epoch 29712 )\n",
      "[Epoch: 31380] train loss: 0.1519, train acc: 0.9487, val loss: 0.1279, val acc: 0.9602  (best train acc: 0.9532, best val acc: 0.9639, best train loss: 0.1369  @ epoch 29712 )\n",
      "[Epoch: 31400] train loss: 0.1536, train acc: 0.9448, val loss: 0.1443, val acc: 0.9545  (best train acc: 0.9532, best val acc: 0.9639, best train loss: 0.1369  @ epoch 29712 )\n",
      "[Epoch: 31420] train loss: 0.2242, train acc: 0.9195, val loss: 0.1522, val acc: 0.9470  (best train acc: 0.9532, best val acc: 0.9639, best train loss: 0.1369  @ epoch 29712 )\n",
      "[Epoch: 31440] train loss: 0.1712, train acc: 0.9402, val loss: 0.1366, val acc: 0.9578  (best train acc: 0.9532, best val acc: 0.9639, best train loss: 0.1369  @ epoch 29712 )\n",
      "[Epoch: 31460] train loss: 0.1567, train acc: 0.9438, val loss: 0.1282, val acc: 0.9605  (best train acc: 0.9532, best val acc: 0.9639, best train loss: 0.1369  @ epoch 29712 )\n",
      "[Epoch: 31480] train loss: 0.1609, train acc: 0.9399, val loss: 0.1460, val acc: 0.9524  (best train acc: 0.9532, best val acc: 0.9639, best train loss: 0.1369  @ epoch 29712 )\n",
      "[Epoch: 31500] train loss: 0.1534, train acc: 0.9454, val loss: 0.1287, val acc: 0.9568  (best train acc: 0.9532, best val acc: 0.9639, best train loss: 0.1369  @ epoch 29712 )\n",
      "[Epoch: 31520] train loss: 0.1549, train acc: 0.9432, val loss: 0.1265, val acc: 0.9595  (best train acc: 0.9532, best val acc: 0.9639, best train loss: 0.1369  @ epoch 29712 )\n",
      "[Epoch: 31540] train loss: 0.1615, train acc: 0.9448, val loss: 0.1353, val acc: 0.9501  (best train acc: 0.9532, best val acc: 0.9639, best train loss: 0.1369  @ epoch 29712 )\n",
      "[Epoch: 31560] train loss: 0.1485, train acc: 0.9430, val loss: 0.1482, val acc: 0.9551  (best train acc: 0.9532, best val acc: 0.9639, best train loss: 0.1369  @ epoch 29712 )\n",
      "[Epoch: 31580] train loss: 0.1557, train acc: 0.9433, val loss: 0.1390, val acc: 0.9592  (best train acc: 0.9532, best val acc: 0.9639, best train loss: 0.1369  @ epoch 29712 )\n",
      "[Epoch: 31600] train loss: 0.1668, train acc: 0.9400, val loss: 0.1307, val acc: 0.9551  (best train acc: 0.9532, best val acc: 0.9639, best train loss: 0.1369  @ epoch 29712 )\n",
      "[Epoch: 31620] train loss: 0.1661, train acc: 0.9432, val loss: 0.1369, val acc: 0.9572  (best train acc: 0.9532, best val acc: 0.9639, best train loss: 0.1369  @ epoch 29712 )\n",
      "[Epoch: 31640] train loss: 0.1824, train acc: 0.9325, val loss: 0.1275, val acc: 0.9602  (best train acc: 0.9532, best val acc: 0.9639, best train loss: 0.1369  @ epoch 29712 )\n",
      "[Epoch: 31660] train loss: 0.1643, train acc: 0.9375, val loss: 0.1332, val acc: 0.9551  (best train acc: 0.9532, best val acc: 0.9639, best train loss: 0.1369  @ epoch 29712 )\n",
      "[Epoch: 31680] train loss: 0.1630, train acc: 0.9360, val loss: 0.1374, val acc: 0.9528  (best train acc: 0.9532, best val acc: 0.9639, best train loss: 0.1369  @ epoch 29712 )\n",
      "[Epoch: 31700] train loss: 0.1635, train acc: 0.9455, val loss: 0.1347, val acc: 0.9565  (best train acc: 0.9532, best val acc: 0.9639, best train loss: 0.1369  @ epoch 29712 )\n",
      "[Epoch: 31720] train loss: 0.1557, train acc: 0.9422, val loss: 0.1373, val acc: 0.9565  (best train acc: 0.9532, best val acc: 0.9639, best train loss: 0.1369  @ epoch 29712 )\n",
      "[Epoch: 31740] train loss: 0.1773, train acc: 0.9344, val loss: 0.1329, val acc: 0.9487  (best train acc: 0.9532, best val acc: 0.9639, best train loss: 0.1369  @ epoch 29712 )\n",
      "[Epoch: 31760] train loss: 0.1777, train acc: 0.9363, val loss: 0.1330, val acc: 0.9595  (best train acc: 0.9532, best val acc: 0.9639, best train loss: 0.1369  @ epoch 29712 )\n",
      "[Epoch: 31780] train loss: 0.1524, train acc: 0.9495, val loss: 0.1346, val acc: 0.9562  (best train acc: 0.9532, best val acc: 0.9639, best train loss: 0.1369  @ epoch 29712 )\n",
      "[Epoch: 31800] train loss: 0.1544, train acc: 0.9472, val loss: 0.1477, val acc: 0.9524  (best train acc: 0.9532, best val acc: 0.9639, best train loss: 0.1369  @ epoch 29712 )\n",
      "[Epoch: 31820] train loss: 0.1685, train acc: 0.9433, val loss: 0.1340, val acc: 0.9589  (best train acc: 0.9532, best val acc: 0.9639, best train loss: 0.1369  @ epoch 29712 )\n",
      "[Epoch: 31840] train loss: 0.1628, train acc: 0.9396, val loss: 0.1459, val acc: 0.9548  (best train acc: 0.9532, best val acc: 0.9639, best train loss: 0.1369  @ epoch 29712 )\n",
      "[Epoch: 31860] train loss: 0.1481, train acc: 0.9504, val loss: 0.1276, val acc: 0.9555  (best train acc: 0.9532, best val acc: 0.9639, best train loss: 0.1369  @ epoch 29712 )\n",
      "[Epoch: 31880] train loss: 0.1694, train acc: 0.9374, val loss: 0.1298, val acc: 0.9568  (best train acc: 0.9532, best val acc: 0.9639, best train loss: 0.1369  @ epoch 29712 )\n",
      "[Epoch: 31900] train loss: 0.1513, train acc: 0.9469, val loss: 0.1298, val acc: 0.9572  (best train acc: 0.9532, best val acc: 0.9639, best train loss: 0.1369  @ epoch 29712 )\n",
      "[Epoch: 31920] train loss: 0.1643, train acc: 0.9440, val loss: 0.1276, val acc: 0.9575  (best train acc: 0.9532, best val acc: 0.9639, best train loss: 0.1369  @ epoch 29712 )\n",
      "[Epoch: 31940] train loss: 0.1508, train acc: 0.9451, val loss: 0.1281, val acc: 0.9585  (best train acc: 0.9535, best val acc: 0.9639, best train loss: 0.1369  @ epoch 29712 )\n",
      "[Epoch: 31960] train loss: 0.1780, train acc: 0.9346, val loss: 0.1327, val acc: 0.9545  (best train acc: 0.9535, best val acc: 0.9639, best train loss: 0.1369  @ epoch 29712 )\n",
      "[Epoch: 31980] train loss: 0.1558, train acc: 0.9472, val loss: 0.1403, val acc: 0.9568  (best train acc: 0.9535, best val acc: 0.9639, best train loss: 0.1369  @ epoch 29712 )\n",
      "[Epoch: 32000] train loss: 0.1552, train acc: 0.9440, val loss: 0.1397, val acc: 0.9575  (best train acc: 0.9535, best val acc: 0.9639, best train loss: 0.1369  @ epoch 29712 )\n",
      "[Epoch: 32020] train loss: 0.1694, train acc: 0.9393, val loss: 0.1311, val acc: 0.9562  (best train acc: 0.9535, best val acc: 0.9639, best train loss: 0.1369  @ epoch 29712 )\n",
      "[Epoch: 32040] train loss: 0.1631, train acc: 0.9405, val loss: 0.1295, val acc: 0.9575  (best train acc: 0.9535, best val acc: 0.9639, best train loss: 0.1369  @ epoch 29712 )\n",
      "[Epoch: 32060] train loss: 0.1660, train acc: 0.9414, val loss: 0.1284, val acc: 0.9541  (best train acc: 0.9535, best val acc: 0.9639, best train loss: 0.1369  @ epoch 29712 )\n",
      "[Epoch: 32080] train loss: 0.1681, train acc: 0.9351, val loss: 0.1376, val acc: 0.9514  (best train acc: 0.9535, best val acc: 0.9639, best train loss: 0.1369  @ epoch 29712 )\n",
      "[Epoch: 32100] train loss: 0.1666, train acc: 0.9412, val loss: 0.1386, val acc: 0.9562  (best train acc: 0.9535, best val acc: 0.9639, best train loss: 0.1369  @ epoch 29712 )\n",
      "[Epoch: 32120] train loss: 0.1682, train acc: 0.9370, val loss: 0.1481, val acc: 0.9518  (best train acc: 0.9535, best val acc: 0.9639, best train loss: 0.1369  @ epoch 29712 )\n",
      "[Epoch: 32140] train loss: 0.1619, train acc: 0.9438, val loss: 0.1320, val acc: 0.9578  (best train acc: 0.9535, best val acc: 0.9639, best train loss: 0.1369  @ epoch 29712 )\n",
      "[Epoch: 32160] train loss: 0.1828, train acc: 0.9370, val loss: 0.1289, val acc: 0.9555  (best train acc: 0.9535, best val acc: 0.9639, best train loss: 0.1366  @ epoch 32150 )\n",
      "[Epoch: 32180] train loss: 0.1797, train acc: 0.9328, val loss: 0.1468, val acc: 0.9528  (best train acc: 0.9535, best val acc: 0.9639, best train loss: 0.1366  @ epoch 32150 )\n",
      "[Epoch: 32200] train loss: 0.1457, train acc: 0.9491, val loss: 0.1362, val acc: 0.9582  (best train acc: 0.9535, best val acc: 0.9639, best train loss: 0.1366  @ epoch 32150 )\n",
      "[Epoch: 32220] train loss: 0.1514, train acc: 0.9432, val loss: 0.1451, val acc: 0.9528  (best train acc: 0.9535, best val acc: 0.9639, best train loss: 0.1366  @ epoch 32150 )\n",
      "[Epoch: 32240] train loss: 0.1695, train acc: 0.9366, val loss: 0.1581, val acc: 0.9491  (best train acc: 0.9535, best val acc: 0.9639, best train loss: 0.1366  @ epoch 32150 )\n",
      "[Epoch: 32260] train loss: 0.1415, train acc: 0.9513, val loss: 0.1253, val acc: 0.9592  (best train acc: 0.9535, best val acc: 0.9639, best train loss: 0.1366  @ epoch 32150 )\n",
      "[Epoch: 32280] train loss: 0.1737, train acc: 0.9398, val loss: 0.1303, val acc: 0.9562  (best train acc: 0.9535, best val acc: 0.9639, best train loss: 0.1366  @ epoch 32150 )\n",
      "[Epoch: 32300] train loss: 0.1498, train acc: 0.9489, val loss: 0.1303, val acc: 0.9565  (best train acc: 0.9535, best val acc: 0.9639, best train loss: 0.1366  @ epoch 32150 )\n",
      "[Epoch: 32320] train loss: 0.1638, train acc: 0.9412, val loss: 0.1348, val acc: 0.9521  (best train acc: 0.9535, best val acc: 0.9639, best train loss: 0.1366  @ epoch 32150 )\n",
      "[Epoch: 32340] train loss: 0.1723, train acc: 0.9429, val loss: 0.1355, val acc: 0.9501  (best train acc: 0.9535, best val acc: 0.9639, best train loss: 0.1366  @ epoch 32150 )\n",
      "[Epoch: 32360] train loss: 0.1768, train acc: 0.9406, val loss: 0.1335, val acc: 0.9605  (best train acc: 0.9535, best val acc: 0.9639, best train loss: 0.1366  @ epoch 32150 )\n",
      "[Epoch: 32380] train loss: 0.1473, train acc: 0.9492, val loss: 0.1311, val acc: 0.9602  (best train acc: 0.9535, best val acc: 0.9639, best train loss: 0.1366  @ epoch 32150 )\n",
      "[Epoch: 32400] train loss: 0.1621, train acc: 0.9472, val loss: 0.1382, val acc: 0.9568  (best train acc: 0.9535, best val acc: 0.9639, best train loss: 0.1366  @ epoch 32150 )\n",
      "[Epoch: 32420] train loss: 0.1542, train acc: 0.9484, val loss: 0.1301, val acc: 0.9551  (best train acc: 0.9535, best val acc: 0.9639, best train loss: 0.1366  @ epoch 32150 )\n",
      "[Epoch: 32440] train loss: 0.1603, train acc: 0.9418, val loss: 0.1368, val acc: 0.9518  (best train acc: 0.9535, best val acc: 0.9639, best train loss: 0.1366  @ epoch 32150 )\n",
      "[Epoch: 32460] train loss: 0.1607, train acc: 0.9365, val loss: 0.1379, val acc: 0.9599  (best train acc: 0.9535, best val acc: 0.9639, best train loss: 0.1366  @ epoch 32150 )\n",
      "[Epoch: 32480] train loss: 0.1578, train acc: 0.9474, val loss: 0.1355, val acc: 0.9589  (best train acc: 0.9535, best val acc: 0.9639, best train loss: 0.1366  @ epoch 32150 )\n",
      "[Epoch: 32500] train loss: 0.1608, train acc: 0.9372, val loss: 0.1349, val acc: 0.9592  (best train acc: 0.9535, best val acc: 0.9639, best train loss: 0.1366  @ epoch 32150 )\n",
      "[Epoch: 32520] train loss: 0.1631, train acc: 0.9423, val loss: 0.1322, val acc: 0.9578  (best train acc: 0.9535, best val acc: 0.9639, best train loss: 0.1366  @ epoch 32150 )\n",
      "[Epoch: 32540] train loss: 0.1545, train acc: 0.9444, val loss: 0.1366, val acc: 0.9538  (best train acc: 0.9535, best val acc: 0.9639, best train loss: 0.1366  @ epoch 32150 )\n",
      "[Epoch: 32560] train loss: 0.1607, train acc: 0.9422, val loss: 0.1520, val acc: 0.9491  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1311  @ epoch 32548 )\n",
      "[Epoch: 32580] train loss: 0.1675, train acc: 0.9404, val loss: 0.1379, val acc: 0.9562  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1311  @ epoch 32548 )\n",
      "[Epoch: 32600] train loss: 0.1580, train acc: 0.9432, val loss: 0.1405, val acc: 0.9555  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1311  @ epoch 32548 )\n",
      "[Epoch: 32620] train loss: 0.1669, train acc: 0.9406, val loss: 0.1321, val acc: 0.9528  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1311  @ epoch 32548 )\n",
      "[Epoch: 32640] train loss: 0.1779, train acc: 0.9339, val loss: 0.1336, val acc: 0.9585  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1311  @ epoch 32548 )\n",
      "[Epoch: 32660] train loss: 0.2302, train acc: 0.9059, val loss: 0.1396, val acc: 0.9558  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1311  @ epoch 32548 )\n",
      "[Epoch: 32680] train loss: 0.1571, train acc: 0.9459, val loss: 0.1425, val acc: 0.9413  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1311  @ epoch 32548 )\n",
      "[Epoch: 32700] train loss: 0.1786, train acc: 0.9310, val loss: 0.1504, val acc: 0.9535  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1311  @ epoch 32548 )\n",
      "[Epoch: 32720] train loss: 0.1555, train acc: 0.9417, val loss: 0.1514, val acc: 0.9535  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1311  @ epoch 32548 )\n",
      "[Epoch: 32740] train loss: 0.1616, train acc: 0.9431, val loss: 0.1368, val acc: 0.9558  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1311  @ epoch 32548 )\n",
      "[Epoch: 32760] train loss: 0.1570, train acc: 0.9404, val loss: 0.1281, val acc: 0.9595  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1311  @ epoch 32548 )\n",
      "[Epoch: 32780] train loss: 0.1711, train acc: 0.9338, val loss: 0.1378, val acc: 0.9582  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1311  @ epoch 32548 )\n",
      "[Epoch: 32800] train loss: 0.1653, train acc: 0.9398, val loss: 0.1329, val acc: 0.9545  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1311  @ epoch 32548 )\n",
      "[Epoch: 32820] train loss: 0.1735, train acc: 0.9393, val loss: 0.1294, val acc: 0.9612  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1311  @ epoch 32548 )\n",
      "[Epoch: 32840] train loss: 0.1507, train acc: 0.9486, val loss: 0.1468, val acc: 0.9578  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1311  @ epoch 32548 )\n",
      "[Epoch: 32860] train loss: 0.1608, train acc: 0.9428, val loss: 0.1283, val acc: 0.9595  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1311  @ epoch 32548 )\n",
      "[Epoch: 32880] train loss: 0.1778, train acc: 0.9364, val loss: 0.1717, val acc: 0.9470  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1311  @ epoch 32548 )\n",
      "[Epoch: 32900] train loss: 0.1637, train acc: 0.9417, val loss: 0.1356, val acc: 0.9548  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1311  @ epoch 32548 )\n",
      "[Epoch: 32920] train loss: 0.1468, train acc: 0.9453, val loss: 0.1341, val acc: 0.9582  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1311  @ epoch 32548 )\n",
      "[Epoch: 32940] train loss: 0.1734, train acc: 0.9406, val loss: 0.1369, val acc: 0.9609  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1311  @ epoch 32548 )\n",
      "[Epoch: 32960] train loss: 0.1534, train acc: 0.9446, val loss: 0.1352, val acc: 0.9578  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1311  @ epoch 32548 )\n",
      "[Epoch: 32980] train loss: 0.1672, train acc: 0.9360, val loss: 0.1349, val acc: 0.9578  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1311  @ epoch 32548 )\n",
      "[Epoch: 33000] train loss: 0.1560, train acc: 0.9435, val loss: 0.1458, val acc: 0.9535  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1311  @ epoch 32548 )\n",
      "[Epoch: 33020] train loss: 0.1535, train acc: 0.9462, val loss: 0.1322, val acc: 0.9592  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1311  @ epoch 32548 )\n",
      "[Epoch: 33040] train loss: 0.1823, train acc: 0.9355, val loss: 0.1491, val acc: 0.9501  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1311  @ epoch 32548 )\n",
      "[Epoch: 33060] train loss: 0.1924, train acc: 0.9289, val loss: 0.1522, val acc: 0.9575  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1311  @ epoch 32548 )\n",
      "[Epoch: 33080] train loss: 0.1556, train acc: 0.9431, val loss: 0.1632, val acc: 0.9464  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1311  @ epoch 32548 )\n",
      "[Epoch: 33100] train loss: 0.1688, train acc: 0.9371, val loss: 0.1317, val acc: 0.9575  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1311  @ epoch 32548 )\n",
      "[Epoch: 33120] train loss: 0.1638, train acc: 0.9390, val loss: 0.1348, val acc: 0.9558  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1311  @ epoch 32548 )\n",
      "[Epoch: 33140] train loss: 0.1540, train acc: 0.9437, val loss: 0.1386, val acc: 0.9595  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1311  @ epoch 32548 )\n",
      "[Epoch: 33160] train loss: 0.1622, train acc: 0.9383, val loss: 0.1354, val acc: 0.9578  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1311  @ epoch 32548 )\n",
      "[Epoch: 33180] train loss: 0.1698, train acc: 0.9327, val loss: 0.1241, val acc: 0.9575  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1311  @ epoch 32548 )\n",
      "[Epoch: 33200] train loss: 0.1447, train acc: 0.9466, val loss: 0.1403, val acc: 0.9551  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1311  @ epoch 32548 )\n",
      "[Epoch: 33220] train loss: 0.1702, train acc: 0.9396, val loss: 0.1443, val acc: 0.9541  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1311  @ epoch 32548 )\n",
      "[Epoch: 33240] train loss: 0.1678, train acc: 0.9404, val loss: 0.1397, val acc: 0.9585  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1311  @ epoch 32548 )\n",
      "[Epoch: 33260] train loss: 0.1568, train acc: 0.9419, val loss: 0.1371, val acc: 0.9528  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1311  @ epoch 32548 )\n",
      "[Epoch: 33280] train loss: 0.1546, train acc: 0.9440, val loss: 0.1519, val acc: 0.9548  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1311  @ epoch 32548 )\n",
      "[Epoch: 33300] train loss: 0.1706, train acc: 0.9364, val loss: 0.1337, val acc: 0.9609  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1311  @ epoch 32548 )\n",
      "[Epoch: 33320] train loss: 0.1529, train acc: 0.9434, val loss: 0.1351, val acc: 0.9565  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1311  @ epoch 32548 )\n",
      "[Epoch: 33340] train loss: 0.1551, train acc: 0.9433, val loss: 0.1316, val acc: 0.9582  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1311  @ epoch 32548 )\n",
      "[Epoch: 33360] train loss: 0.1622, train acc: 0.9372, val loss: 0.1446, val acc: 0.9514  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1311  @ epoch 32548 )\n",
      "[Epoch: 33380] train loss: 0.1558, train acc: 0.9436, val loss: 0.1339, val acc: 0.9592  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1311  @ epoch 32548 )\n",
      "[Epoch: 33400] train loss: 0.1652, train acc: 0.9409, val loss: 0.1432, val acc: 0.9558  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1311  @ epoch 32548 )\n",
      "[Epoch: 33420] train loss: 0.1534, train acc: 0.9441, val loss: 0.1378, val acc: 0.9572  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1311  @ epoch 32548 )\n",
      "[Epoch: 33440] train loss: 0.1775, train acc: 0.9320, val loss: 0.1368, val acc: 0.9589  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1311  @ epoch 32548 )\n",
      "[Epoch: 33460] train loss: 0.1499, train acc: 0.9494, val loss: 0.1325, val acc: 0.9582  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1311  @ epoch 32548 )\n",
      "[Epoch: 33480] train loss: 0.1464, train acc: 0.9469, val loss: 0.1467, val acc: 0.9487  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1311  @ epoch 32548 )\n",
      "[Epoch: 33500] train loss: 0.2033, train acc: 0.9338, val loss: 0.1318, val acc: 0.9524  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1311  @ epoch 32548 )\n",
      "[Epoch: 33520] train loss: 0.1570, train acc: 0.9490, val loss: 0.1303, val acc: 0.9535  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1311  @ epoch 32548 )\n",
      "[Epoch: 33540] train loss: 0.1530, train acc: 0.9428, val loss: 0.1351, val acc: 0.9592  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1311  @ epoch 32548 )\n",
      "[Epoch: 33560] train loss: 0.1832, train acc: 0.9323, val loss: 0.1475, val acc: 0.9582  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1311  @ epoch 32548 )\n",
      "[Epoch: 33580] train loss: 0.1575, train acc: 0.9440, val loss: 0.1378, val acc: 0.9474  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1311  @ epoch 32548 )\n",
      "[Epoch: 33600] train loss: 0.1705, train acc: 0.9375, val loss: 0.1299, val acc: 0.9599  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1311  @ epoch 32548 )\n",
      "[Epoch: 33620] train loss: 0.1736, train acc: 0.9388, val loss: 0.1311, val acc: 0.9575  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1311  @ epoch 32548 )\n",
      "[Epoch: 33640] train loss: 0.1533, train acc: 0.9419, val loss: 0.1411, val acc: 0.9572  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1311  @ epoch 32548 )\n",
      "[Epoch: 33660] train loss: 0.1438, train acc: 0.9500, val loss: 0.1429, val acc: 0.9551  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1311  @ epoch 32548 )\n",
      "[Epoch: 33680] train loss: 0.1734, train acc: 0.9383, val loss: 0.1298, val acc: 0.9589  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1311  @ epoch 32548 )\n",
      "[Epoch: 33700] train loss: 0.1557, train acc: 0.9416, val loss: 0.1396, val acc: 0.9589  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1311  @ epoch 32548 )\n",
      "[Epoch: 33720] train loss: 0.1576, train acc: 0.9456, val loss: 0.1304, val acc: 0.9545  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1311  @ epoch 32548 )\n",
      "[Epoch: 33740] train loss: 0.2259, train acc: 0.9132, val loss: 0.1311, val acc: 0.9585  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1311  @ epoch 32548 )\n",
      "[Epoch: 33760] train loss: 0.1401, train acc: 0.9505, val loss: 0.1359, val acc: 0.9572  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1311  @ epoch 32548 )\n",
      "[Epoch: 33780] train loss: 0.1519, train acc: 0.9461, val loss: 0.1329, val acc: 0.9562  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1311  @ epoch 32548 )\n",
      "[Epoch: 33800] train loss: 0.1776, train acc: 0.9375, val loss: 0.1292, val acc: 0.9582  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1311  @ epoch 32548 )\n",
      "[Epoch: 33820] train loss: 0.1449, train acc: 0.9459, val loss: 0.1609, val acc: 0.9464  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1311  @ epoch 32548 )\n",
      "[Epoch: 33840] train loss: 0.1717, train acc: 0.9393, val loss: 0.1579, val acc: 0.9470  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1311  @ epoch 32548 )\n",
      "[Epoch: 33860] train loss: 0.1493, train acc: 0.9466, val loss: 0.1332, val acc: 0.9541  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1311  @ epoch 32548 )\n",
      "[Epoch: 33880] train loss: 0.1524, train acc: 0.9472, val loss: 0.1248, val acc: 0.9575  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1311  @ epoch 32548 )\n",
      "[Epoch: 33900] train loss: 0.1996, train acc: 0.9234, val loss: 0.1407, val acc: 0.9541  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1311  @ epoch 32548 )\n",
      "[Epoch: 33920] train loss: 0.1539, train acc: 0.9448, val loss: 0.1317, val acc: 0.9612  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1311  @ epoch 32548 )\n",
      "[Epoch: 33940] train loss: 0.1547, train acc: 0.9432, val loss: 0.1265, val acc: 0.9592  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 33960] train loss: 0.1513, train acc: 0.9415, val loss: 0.1366, val acc: 0.9521  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 33980] train loss: 0.1412, train acc: 0.9524, val loss: 0.1277, val acc: 0.9605  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 34000] train loss: 0.1439, train acc: 0.9487, val loss: 0.1230, val acc: 0.9605  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 34020] train loss: 0.1395, train acc: 0.9481, val loss: 0.1236, val acc: 0.9612  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 34040] train loss: 0.1560, train acc: 0.9431, val loss: 0.1288, val acc: 0.9555  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 34060] train loss: 0.1483, train acc: 0.9471, val loss: 0.1261, val acc: 0.9602  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 34080] train loss: 0.1406, train acc: 0.9487, val loss: 0.1286, val acc: 0.9602  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 34100] train loss: 0.1497, train acc: 0.9469, val loss: 0.1293, val acc: 0.9605  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 34120] train loss: 0.1611, train acc: 0.9448, val loss: 0.1275, val acc: 0.9602  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 34140] train loss: 0.1568, train acc: 0.9449, val loss: 0.1524, val acc: 0.9444  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 34160] train loss: 0.1521, train acc: 0.9403, val loss: 0.1459, val acc: 0.9551  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 34180] train loss: 0.1686, train acc: 0.9357, val loss: 0.1267, val acc: 0.9595  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 34200] train loss: 0.1708, train acc: 0.9323, val loss: 0.1342, val acc: 0.9575  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 34220] train loss: 0.1445, train acc: 0.9504, val loss: 0.1309, val acc: 0.9592  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 34240] train loss: 0.1511, train acc: 0.9453, val loss: 0.1331, val acc: 0.9572  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 34260] train loss: 0.1378, train acc: 0.9468, val loss: 0.1338, val acc: 0.9545  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 34280] train loss: 0.1852, train acc: 0.9325, val loss: 0.1268, val acc: 0.9572  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 34300] train loss: 0.1593, train acc: 0.9435, val loss: 0.1372, val acc: 0.9578  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 34320] train loss: 0.1516, train acc: 0.9487, val loss: 0.1305, val acc: 0.9562  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 34340] train loss: 0.1380, train acc: 0.9461, val loss: 0.1252, val acc: 0.9582  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 34360] train loss: 0.1568, train acc: 0.9409, val loss: 0.1325, val acc: 0.9589  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 34380] train loss: 0.1589, train acc: 0.9412, val loss: 0.1266, val acc: 0.9548  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 34400] train loss: 0.1576, train acc: 0.9402, val loss: 0.1263, val acc: 0.9582  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 34420] train loss: 0.1480, train acc: 0.9441, val loss: 0.1267, val acc: 0.9592  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 34440] train loss: 0.1589, train acc: 0.9390, val loss: 0.1224, val acc: 0.9572  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 34460] train loss: 0.1613, train acc: 0.9355, val loss: 0.1273, val acc: 0.9575  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 34480] train loss: 0.1598, train acc: 0.9435, val loss: 0.1303, val acc: 0.9565  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 34500] train loss: 0.1504, train acc: 0.9466, val loss: 0.1235, val acc: 0.9582  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 34520] train loss: 0.1465, train acc: 0.9464, val loss: 0.1315, val acc: 0.9568  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 34540] train loss: 0.1419, train acc: 0.9521, val loss: 0.1234, val acc: 0.9592  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 34560] train loss: 0.1389, train acc: 0.9487, val loss: 0.1293, val acc: 0.9578  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 34580] train loss: 0.1506, train acc: 0.9407, val loss: 0.1266, val acc: 0.9589  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 34600] train loss: 0.1780, train acc: 0.9289, val loss: 0.1400, val acc: 0.9541  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 34620] train loss: 0.1819, train acc: 0.9329, val loss: 0.1315, val acc: 0.9568  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 34640] train loss: 0.1766, train acc: 0.9393, val loss: 0.1352, val acc: 0.9585  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 34660] train loss: 0.1587, train acc: 0.9441, val loss: 0.1368, val acc: 0.9582  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 34680] train loss: 0.1456, train acc: 0.9492, val loss: 0.1261, val acc: 0.9585  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 34700] train loss: 0.1480, train acc: 0.9439, val loss: 0.1207, val acc: 0.9585  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 34720] train loss: 0.1748, train acc: 0.9336, val loss: 0.1353, val acc: 0.9538  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 34740] train loss: 0.1475, train acc: 0.9465, val loss: 0.1269, val acc: 0.9572  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 34760] train loss: 0.1715, train acc: 0.9346, val loss: 0.1290, val acc: 0.9595  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 34780] train loss: 0.2037, train acc: 0.9231, val loss: 0.1314, val acc: 0.9538  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 34800] train loss: 0.1529, train acc: 0.9483, val loss: 0.1231, val acc: 0.9592  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 34820] train loss: 0.1539, train acc: 0.9451, val loss: 0.1314, val acc: 0.9595  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 34840] train loss: 0.1515, train acc: 0.9475, val loss: 0.1265, val acc: 0.9589  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 34860] train loss: 0.1588, train acc: 0.9374, val loss: 0.1349, val acc: 0.9572  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 34880] train loss: 0.1422, train acc: 0.9479, val loss: 0.1250, val acc: 0.9605  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 34900] train loss: 0.1585, train acc: 0.9413, val loss: 0.1540, val acc: 0.9514  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 34920] train loss: 0.1534, train acc: 0.9440, val loss: 0.1551, val acc: 0.9470  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 34940] train loss: 0.1726, train acc: 0.9369, val loss: 0.1516, val acc: 0.9514  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 34960] train loss: 0.1601, train acc: 0.9394, val loss: 0.1468, val acc: 0.9508  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 34980] train loss: 0.1531, train acc: 0.9448, val loss: 0.1363, val acc: 0.9545  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 35000] train loss: 0.1603, train acc: 0.9372, val loss: 0.1514, val acc: 0.9514  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 35020] train loss: 0.1471, train acc: 0.9471, val loss: 0.1424, val acc: 0.9565  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 35040] train loss: 0.1840, train acc: 0.9308, val loss: 0.1465, val acc: 0.9504  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 35060] train loss: 0.1523, train acc: 0.9414, val loss: 0.1363, val acc: 0.9568  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 35080] train loss: 0.1514, train acc: 0.9446, val loss: 0.1228, val acc: 0.9599  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 35100] train loss: 0.1736, train acc: 0.9297, val loss: 0.1523, val acc: 0.9447  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 35120] train loss: 0.1647, train acc: 0.9416, val loss: 0.1228, val acc: 0.9568  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 35140] train loss: 0.1659, train acc: 0.9385, val loss: 0.1276, val acc: 0.9592  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 35160] train loss: 0.1658, train acc: 0.9370, val loss: 0.1240, val acc: 0.9602  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 35180] train loss: 0.1464, train acc: 0.9476, val loss: 0.1257, val acc: 0.9558  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 35200] train loss: 0.2029, train acc: 0.9237, val loss: 0.1344, val acc: 0.9558  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 35220] train loss: 0.1498, train acc: 0.9479, val loss: 0.1452, val acc: 0.9528  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 35240] train loss: 0.1429, train acc: 0.9512, val loss: 0.1394, val acc: 0.9551  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 35260] train loss: 0.1512, train acc: 0.9489, val loss: 0.1353, val acc: 0.9565  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 35280] train loss: 0.1680, train acc: 0.9346, val loss: 0.1252, val acc: 0.9562  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 35300] train loss: 0.1465, train acc: 0.9474, val loss: 0.1279, val acc: 0.9585  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 35320] train loss: 0.1611, train acc: 0.9435, val loss: 0.1248, val acc: 0.9585  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 35340] train loss: 0.1426, train acc: 0.9498, val loss: 0.1364, val acc: 0.9565  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 35360] train loss: 0.1403, train acc: 0.9510, val loss: 0.1318, val acc: 0.9619  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 35380] train loss: 0.1581, train acc: 0.9433, val loss: 0.1254, val acc: 0.9565  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 35400] train loss: 0.1480, train acc: 0.9444, val loss: 0.1405, val acc: 0.9562  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 35420] train loss: 0.1523, train acc: 0.9456, val loss: 0.1302, val acc: 0.9585  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 35440] train loss: 0.1528, train acc: 0.9443, val loss: 0.1248, val acc: 0.9582  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 35460] train loss: 0.1597, train acc: 0.9422, val loss: 0.1411, val acc: 0.9531  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 35480] train loss: 0.1563, train acc: 0.9426, val loss: 0.1263, val acc: 0.9592  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 35500] train loss: 0.1400, train acc: 0.9452, val loss: 0.1248, val acc: 0.9605  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 35520] train loss: 0.1641, train acc: 0.9456, val loss: 0.1284, val acc: 0.9578  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 35540] train loss: 0.1507, train acc: 0.9452, val loss: 0.1453, val acc: 0.9545  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 35560] train loss: 0.1370, train acc: 0.9504, val loss: 0.1260, val acc: 0.9535  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 35580] train loss: 0.1624, train acc: 0.9412, val loss: 0.1318, val acc: 0.9578  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 35600] train loss: 0.1573, train acc: 0.9434, val loss: 0.1267, val acc: 0.9595  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 35620] train loss: 0.1462, train acc: 0.9482, val loss: 0.1328, val acc: 0.9585  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 35640] train loss: 0.1753, train acc: 0.9329, val loss: 0.1552, val acc: 0.9481  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 35660] train loss: 0.1556, train acc: 0.9418, val loss: 0.1282, val acc: 0.9585  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 35680] train loss: 0.1397, train acc: 0.9500, val loss: 0.1245, val acc: 0.9602  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 35700] train loss: 0.1493, train acc: 0.9447, val loss: 0.1416, val acc: 0.9565  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 35720] train loss: 0.1697, train acc: 0.9366, val loss: 0.1366, val acc: 0.9568  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 35740] train loss: 0.1476, train acc: 0.9475, val loss: 0.1236, val acc: 0.9592  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 35760] train loss: 0.1626, train acc: 0.9421, val loss: 0.1363, val acc: 0.9538  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 35780] train loss: 0.1377, train acc: 0.9503, val loss: 0.1258, val acc: 0.9592  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 35800] train loss: 0.1538, train acc: 0.9427, val loss: 0.1299, val acc: 0.9589  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 35820] train loss: 0.1546, train acc: 0.9424, val loss: 0.1291, val acc: 0.9595  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 35840] train loss: 0.1412, train acc: 0.9478, val loss: 0.1294, val acc: 0.9595  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 35860] train loss: 0.1556, train acc: 0.9437, val loss: 0.1239, val acc: 0.9599  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 35880] train loss: 0.1647, train acc: 0.9422, val loss: 0.1372, val acc: 0.9541  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 35900] train loss: 0.1537, train acc: 0.9446, val loss: 0.1261, val acc: 0.9589  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 35920] train loss: 0.1588, train acc: 0.9485, val loss: 0.1544, val acc: 0.9477  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 35940] train loss: 0.1370, train acc: 0.9493, val loss: 0.1404, val acc: 0.9568  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 35960] train loss: 0.1621, train acc: 0.9357, val loss: 0.1353, val acc: 0.9518  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 35980] train loss: 0.1532, train acc: 0.9414, val loss: 0.1349, val acc: 0.9572  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 36000] train loss: 0.1572, train acc: 0.9391, val loss: 0.1346, val acc: 0.9558  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 36020] train loss: 0.1391, train acc: 0.9521, val loss: 0.1257, val acc: 0.9558  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 36040] train loss: 0.1571, train acc: 0.9388, val loss: 0.1313, val acc: 0.9589  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 36060] train loss: 0.1494, train acc: 0.9498, val loss: 0.1279, val acc: 0.9599  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 36080] train loss: 0.1562, train acc: 0.9417, val loss: 0.1496, val acc: 0.9447  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 36100] train loss: 0.1512, train acc: 0.9427, val loss: 0.1416, val acc: 0.9535  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 36120] train loss: 0.1422, train acc: 0.9487, val loss: 0.1286, val acc: 0.9572  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 36140] train loss: 0.1481, train acc: 0.9488, val loss: 0.1387, val acc: 0.9538  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 36160] train loss: 0.1473, train acc: 0.9464, val loss: 0.1303, val acc: 0.9595  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 36180] train loss: 0.1507, train acc: 0.9470, val loss: 0.1230, val acc: 0.9595  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 36200] train loss: 0.1900, train acc: 0.9283, val loss: 0.1334, val acc: 0.9558  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 36220] train loss: 0.1449, train acc: 0.9483, val loss: 0.1491, val acc: 0.9497  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 36240] train loss: 0.1686, train acc: 0.9348, val loss: 0.1330, val acc: 0.9578  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 36260] train loss: 0.1518, train acc: 0.9472, val loss: 0.1241, val acc: 0.9602  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 36280] train loss: 0.1414, train acc: 0.9513, val loss: 0.1237, val acc: 0.9605  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 36300] train loss: 0.1506, train acc: 0.9466, val loss: 0.1291, val acc: 0.9592  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 36320] train loss: 0.1315, train acc: 0.9522, val loss: 0.1404, val acc: 0.9545  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 36340] train loss: 0.1393, train acc: 0.9477, val loss: 0.1311, val acc: 0.9578  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 36360] train loss: 0.1615, train acc: 0.9393, val loss: 0.1355, val acc: 0.9582  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 36380] train loss: 0.1549, train acc: 0.9429, val loss: 0.1420, val acc: 0.9555  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 36400] train loss: 0.1445, train acc: 0.9472, val loss: 0.1306, val acc: 0.9568  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 36420] train loss: 0.1520, train acc: 0.9427, val loss: 0.1264, val acc: 0.9595  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 36440] train loss: 0.1802, train acc: 0.9324, val loss: 0.1375, val acc: 0.9545  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 36460] train loss: 0.1596, train acc: 0.9428, val loss: 0.1274, val acc: 0.9595  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 36480] train loss: 0.1597, train acc: 0.9420, val loss: 0.1462, val acc: 0.9508  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 36500] train loss: 0.1511, train acc: 0.9472, val loss: 0.1266, val acc: 0.9595  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 36520] train loss: 0.1360, train acc: 0.9501, val loss: 0.1333, val acc: 0.9568  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 36540] train loss: 0.1592, train acc: 0.9389, val loss: 0.1369, val acc: 0.9582  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 36560] train loss: 0.1485, train acc: 0.9423, val loss: 0.1329, val acc: 0.9558  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 36580] train loss: 0.1559, train acc: 0.9408, val loss: 0.1432, val acc: 0.9541  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 36600] train loss: 0.1689, train acc: 0.9400, val loss: 0.1349, val acc: 0.9585  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 36620] train loss: 0.1492, train acc: 0.9438, val loss: 0.1244, val acc: 0.9575  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 36640] train loss: 0.1494, train acc: 0.9469, val loss: 0.1305, val acc: 0.9612  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 36660] train loss: 0.1489, train acc: 0.9455, val loss: 0.1244, val acc: 0.9585  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 36680] train loss: 0.1620, train acc: 0.9380, val loss: 0.1277, val acc: 0.9582  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 36700] train loss: 0.1769, train acc: 0.9337, val loss: 0.1233, val acc: 0.9548  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 36720] train loss: 0.1582, train acc: 0.9412, val loss: 0.1393, val acc: 0.9548  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 36740] train loss: 0.1435, train acc: 0.9468, val loss: 0.1422, val acc: 0.9555  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 36760] train loss: 0.1924, train acc: 0.9298, val loss: 0.1612, val acc: 0.9518  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 36780] train loss: 0.1772, train acc: 0.9354, val loss: 0.1399, val acc: 0.9551  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 36800] train loss: 0.1447, train acc: 0.9459, val loss: 0.1413, val acc: 0.9538  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 36820] train loss: 0.1552, train acc: 0.9417, val loss: 0.1508, val acc: 0.9508  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 36840] train loss: 0.1801, train acc: 0.9248, val loss: 0.1361, val acc: 0.9491  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 36860] train loss: 0.1455, train acc: 0.9445, val loss: 0.1614, val acc: 0.9491  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 36880] train loss: 0.1478, train acc: 0.9462, val loss: 0.1282, val acc: 0.9585  (best train acc: 0.9560, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 36900] train loss: 0.1413, train acc: 0.9465, val loss: 0.1288, val acc: 0.9589  (best train acc: 0.9564, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 36920] train loss: 0.1659, train acc: 0.9487, val loss: 0.1243, val acc: 0.9585  (best train acc: 0.9564, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 36940] train loss: 0.1493, train acc: 0.9477, val loss: 0.1256, val acc: 0.9602  (best train acc: 0.9564, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 36960] train loss: 0.1344, train acc: 0.9514, val loss: 0.1431, val acc: 0.9558  (best train acc: 0.9564, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 36980] train loss: 0.1464, train acc: 0.9486, val loss: 0.1339, val acc: 0.9568  (best train acc: 0.9564, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 37000] train loss: 0.1817, train acc: 0.9312, val loss: 0.1354, val acc: 0.9572  (best train acc: 0.9564, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 37020] train loss: 0.1464, train acc: 0.9443, val loss: 0.1297, val acc: 0.9589  (best train acc: 0.9564, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 37040] train loss: 0.1504, train acc: 0.9416, val loss: 0.1611, val acc: 0.9484  (best train acc: 0.9564, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 37060] train loss: 0.1649, train acc: 0.9393, val loss: 0.1341, val acc: 0.9582  (best train acc: 0.9564, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 37080] train loss: 0.1754, train acc: 0.9284, val loss: 0.1556, val acc: 0.9464  (best train acc: 0.9564, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 37100] train loss: 0.1471, train acc: 0.9475, val loss: 0.1287, val acc: 0.9619  (best train acc: 0.9564, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 37120] train loss: 0.1723, train acc: 0.9310, val loss: 0.1285, val acc: 0.9538  (best train acc: 0.9564, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 37140] train loss: 0.1619, train acc: 0.9415, val loss: 0.1475, val acc: 0.9521  (best train acc: 0.9564, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 37160] train loss: 0.1453, train acc: 0.9489, val loss: 0.1586, val acc: 0.9460  (best train acc: 0.9564, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 37180] train loss: 0.1386, train acc: 0.9499, val loss: 0.1302, val acc: 0.9595  (best train acc: 0.9564, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 37200] train loss: 0.1586, train acc: 0.9411, val loss: 0.1431, val acc: 0.9545  (best train acc: 0.9564, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 37220] train loss: 0.1532, train acc: 0.9411, val loss: 0.1453, val acc: 0.9521  (best train acc: 0.9564, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 37240] train loss: 0.1513, train acc: 0.9451, val loss: 0.1255, val acc: 0.9609  (best train acc: 0.9564, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 37260] train loss: 0.1389, train acc: 0.9509, val loss: 0.1232, val acc: 0.9595  (best train acc: 0.9564, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 37280] train loss: 0.1451, train acc: 0.9467, val loss: 0.1382, val acc: 0.9562  (best train acc: 0.9564, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 37300] train loss: 0.1618, train acc: 0.9386, val loss: 0.1444, val acc: 0.9545  (best train acc: 0.9564, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 37320] train loss: 0.1477, train acc: 0.9488, val loss: 0.1324, val acc: 0.9575  (best train acc: 0.9564, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 37340] train loss: 0.1581, train acc: 0.9420, val loss: 0.1300, val acc: 0.9562  (best train acc: 0.9564, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 37360] train loss: 0.1393, train acc: 0.9482, val loss: 0.1542, val acc: 0.9504  (best train acc: 0.9564, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 37380] train loss: 0.2142, train acc: 0.9197, val loss: 0.1583, val acc: 0.9484  (best train acc: 0.9564, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 37400] train loss: 0.1473, train acc: 0.9482, val loss: 0.1326, val acc: 0.9582  (best train acc: 0.9564, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 37420] train loss: 0.1528, train acc: 0.9467, val loss: 0.1292, val acc: 0.9592  (best train acc: 0.9564, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 37440] train loss: 0.1408, train acc: 0.9480, val loss: 0.1293, val acc: 0.9562  (best train acc: 0.9564, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 37460] train loss: 0.1650, train acc: 0.9384, val loss: 0.1237, val acc: 0.9592  (best train acc: 0.9564, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 37480] train loss: 0.1456, train acc: 0.9477, val loss: 0.1559, val acc: 0.9484  (best train acc: 0.9564, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 37500] train loss: 0.1547, train acc: 0.9466, val loss: 0.1344, val acc: 0.9568  (best train acc: 0.9564, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 37520] train loss: 0.1883, train acc: 0.9341, val loss: 0.1319, val acc: 0.9592  (best train acc: 0.9564, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 37540] train loss: 0.1359, train acc: 0.9521, val loss: 0.1325, val acc: 0.9568  (best train acc: 0.9564, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 37560] train loss: 0.1607, train acc: 0.9454, val loss: 0.1531, val acc: 0.9501  (best train acc: 0.9564, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 37580] train loss: 0.1357, train acc: 0.9523, val loss: 0.1338, val acc: 0.9551  (best train acc: 0.9564, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 37600] train loss: 0.1618, train acc: 0.9397, val loss: 0.1277, val acc: 0.9558  (best train acc: 0.9564, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 37620] train loss: 0.1571, train acc: 0.9436, val loss: 0.1358, val acc: 0.9551  (best train acc: 0.9564, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 37640] train loss: 0.1448, train acc: 0.9458, val loss: 0.1389, val acc: 0.9578  (best train acc: 0.9564, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 37660] train loss: 0.1408, train acc: 0.9514, val loss: 0.1358, val acc: 0.9565  (best train acc: 0.9564, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 37680] train loss: 0.1577, train acc: 0.9417, val loss: 0.1520, val acc: 0.9444  (best train acc: 0.9564, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 37700] train loss: 0.1738, train acc: 0.9328, val loss: 0.1294, val acc: 0.9528  (best train acc: 0.9564, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 37720] train loss: 0.1694, train acc: 0.9393, val loss: 0.1406, val acc: 0.9565  (best train acc: 0.9564, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 37740] train loss: 0.1498, train acc: 0.9473, val loss: 0.1351, val acc: 0.9558  (best train acc: 0.9564, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 37760] train loss: 0.1668, train acc: 0.9345, val loss: 0.1415, val acc: 0.9535  (best train acc: 0.9564, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 37780] train loss: 0.1491, train acc: 0.9461, val loss: 0.1329, val acc: 0.9578  (best train acc: 0.9564, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 37800] train loss: 0.1602, train acc: 0.9419, val loss: 0.1593, val acc: 0.9504  (best train acc: 0.9564, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 37820] train loss: 0.1408, train acc: 0.9468, val loss: 0.1332, val acc: 0.9568  (best train acc: 0.9564, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 37840] train loss: 0.1378, train acc: 0.9476, val loss: 0.1236, val acc: 0.9585  (best train acc: 0.9564, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 37860] train loss: 0.1400, train acc: 0.9505, val loss: 0.1277, val acc: 0.9592  (best train acc: 0.9564, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 37880] train loss: 0.1454, train acc: 0.9479, val loss: 0.1314, val acc: 0.9585  (best train acc: 0.9564, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 37900] train loss: 0.1578, train acc: 0.9447, val loss: 0.1451, val acc: 0.9548  (best train acc: 0.9564, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 37920] train loss: 0.1356, train acc: 0.9511, val loss: 0.1383, val acc: 0.9585  (best train acc: 0.9564, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 37940] train loss: 0.1381, train acc: 0.9488, val loss: 0.1266, val acc: 0.9592  (best train acc: 0.9564, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 37960] train loss: 0.1473, train acc: 0.9456, val loss: 0.1308, val acc: 0.9578  (best train acc: 0.9564, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 37980] train loss: 0.1495, train acc: 0.9445, val loss: 0.1282, val acc: 0.9599  (best train acc: 0.9564, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 38000] train loss: 0.1435, train acc: 0.9495, val loss: 0.1258, val acc: 0.9562  (best train acc: 0.9564, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 38020] train loss: 0.1392, train acc: 0.9534, val loss: 0.1256, val acc: 0.9541  (best train acc: 0.9564, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 38040] train loss: 0.1485, train acc: 0.9465, val loss: 0.1561, val acc: 0.9545  (best train acc: 0.9564, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 38060] train loss: 0.1398, train acc: 0.9482, val loss: 0.1363, val acc: 0.9575  (best train acc: 0.9564, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 38080] train loss: 0.1463, train acc: 0.9489, val loss: 0.1328, val acc: 0.9585  (best train acc: 0.9564, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 38100] train loss: 0.1391, train acc: 0.9496, val loss: 0.1340, val acc: 0.9589  (best train acc: 0.9564, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 38120] train loss: 0.1490, train acc: 0.9476, val loss: 0.1439, val acc: 0.9555  (best train acc: 0.9564, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 38140] train loss: 0.1582, train acc: 0.9421, val loss: 0.1314, val acc: 0.9585  (best train acc: 0.9564, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 38160] train loss: 0.1506, train acc: 0.9430, val loss: 0.1398, val acc: 0.9565  (best train acc: 0.9564, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 38180] train loss: 0.1426, train acc: 0.9458, val loss: 0.1266, val acc: 0.9572  (best train acc: 0.9564, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 38200] train loss: 0.1519, train acc: 0.9450, val loss: 0.1299, val acc: 0.9548  (best train acc: 0.9564, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 38220] train loss: 0.1516, train acc: 0.9436, val loss: 0.1278, val acc: 0.9538  (best train acc: 0.9564, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 38240] train loss: 0.1532, train acc: 0.9454, val loss: 0.1290, val acc: 0.9582  (best train acc: 0.9564, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 38260] train loss: 0.1488, train acc: 0.9430, val loss: 0.1331, val acc: 0.9555  (best train acc: 0.9564, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 38280] train loss: 0.1559, train acc: 0.9435, val loss: 0.1374, val acc: 0.9518  (best train acc: 0.9564, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 38300] train loss: 0.1309, train acc: 0.9524, val loss: 0.1321, val acc: 0.9578  (best train acc: 0.9564, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 38320] train loss: 0.1515, train acc: 0.9477, val loss: 0.1555, val acc: 0.9487  (best train acc: 0.9564, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 38340] train loss: 0.1525, train acc: 0.9372, val loss: 0.1374, val acc: 0.9575  (best train acc: 0.9564, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 38360] train loss: 0.1576, train acc: 0.9404, val loss: 0.1341, val acc: 0.9572  (best train acc: 0.9564, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 38380] train loss: 0.1402, train acc: 0.9427, val loss: 0.1615, val acc: 0.9521  (best train acc: 0.9564, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 38400] train loss: 0.1436, train acc: 0.9485, val loss: 0.1220, val acc: 0.9572  (best train acc: 0.9564, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 38420] train loss: 0.1805, train acc: 0.9360, val loss: 0.1355, val acc: 0.9589  (best train acc: 0.9564, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 38440] train loss: 0.1443, train acc: 0.9474, val loss: 0.1321, val acc: 0.9572  (best train acc: 0.9564, best val acc: 0.9639, best train loss: 0.1268  @ epoch 33922 )\n",
      "[Epoch: 38460] train loss: 0.1491, train acc: 0.9477, val loss: 0.1298, val acc: 0.9599  (best train acc: 0.9564, best val acc: 0.9639, best train loss: 0.1204  @ epoch 38446 )\n",
      "[Epoch: 38480] train loss: 0.1559, train acc: 0.9432, val loss: 0.1448, val acc: 0.9524  (best train acc: 0.9564, best val acc: 0.9639, best train loss: 0.1204  @ epoch 38446 )\n",
      "[Epoch: 38500] train loss: 0.1586, train acc: 0.9377, val loss: 0.1298, val acc: 0.9589  (best train acc: 0.9564, best val acc: 0.9639, best train loss: 0.1204  @ epoch 38446 )\n",
      "[Epoch: 38520] train loss: 0.1509, train acc: 0.9449, val loss: 0.1233, val acc: 0.9568  (best train acc: 0.9564, best val acc: 0.9639, best train loss: 0.1204  @ epoch 38446 )\n",
      "[Epoch: 38540] train loss: 0.1553, train acc: 0.9380, val loss: 0.1341, val acc: 0.9568  (best train acc: 0.9564, best val acc: 0.9639, best train loss: 0.1204  @ epoch 38446 )\n",
      "[Epoch: 38560] train loss: 0.1581, train acc: 0.9398, val loss: 0.1390, val acc: 0.9551  (best train acc: 0.9564, best val acc: 0.9639, best train loss: 0.1204  @ epoch 38446 )\n",
      "[Epoch: 38580] train loss: 0.1297, train acc: 0.9539, val loss: 0.1237, val acc: 0.9612  (best train acc: 0.9568, best val acc: 0.9639, best train loss: 0.1204  @ epoch 38446 )\n",
      "[Epoch: 38600] train loss: 0.1385, train acc: 0.9493, val loss: 0.1429, val acc: 0.9589  (best train acc: 0.9570, best val acc: 0.9639, best train loss: 0.1188  @ epoch 38583 )\n",
      "[Epoch: 38620] train loss: 0.1468, train acc: 0.9469, val loss: 0.1332, val acc: 0.9562  (best train acc: 0.9570, best val acc: 0.9639, best train loss: 0.1188  @ epoch 38583 )\n",
      "[Epoch: 38640] train loss: 0.1451, train acc: 0.9448, val loss: 0.1287, val acc: 0.9578  (best train acc: 0.9570, best val acc: 0.9639, best train loss: 0.1188  @ epoch 38583 )\n",
      "[Epoch: 38660] train loss: 0.1694, train acc: 0.9370, val loss: 0.1568, val acc: 0.9501  (best train acc: 0.9570, best val acc: 0.9639, best train loss: 0.1188  @ epoch 38583 )\n",
      "[Epoch: 38680] train loss: 0.1327, train acc: 0.9490, val loss: 0.1501, val acc: 0.9548  (best train acc: 0.9570, best val acc: 0.9639, best train loss: 0.1188  @ epoch 38583 )\n",
      "[Epoch: 38700] train loss: 0.1264, train acc: 0.9537, val loss: 0.1326, val acc: 0.9565  (best train acc: 0.9570, best val acc: 0.9639, best train loss: 0.1188  @ epoch 38583 )\n",
      "[Epoch: 38720] train loss: 0.1370, train acc: 0.9475, val loss: 0.1322, val acc: 0.9582  (best train acc: 0.9570, best val acc: 0.9639, best train loss: 0.1188  @ epoch 38583 )\n",
      "[Epoch: 38740] train loss: 0.2335, train acc: 0.9286, val loss: 0.1309, val acc: 0.9592  (best train acc: 0.9570, best val acc: 0.9639, best train loss: 0.1188  @ epoch 38583 )\n",
      "[Epoch: 38760] train loss: 0.1426, train acc: 0.9410, val loss: 0.1365, val acc: 0.9568  (best train acc: 0.9570, best val acc: 0.9639, best train loss: 0.1188  @ epoch 38583 )\n",
      "[Epoch: 38780] train loss: 0.1466, train acc: 0.9461, val loss: 0.1520, val acc: 0.9511  (best train acc: 0.9570, best val acc: 0.9639, best train loss: 0.1188  @ epoch 38583 )\n",
      "[Epoch: 38800] train loss: 0.1396, train acc: 0.9464, val loss: 0.1413, val acc: 0.9538  (best train acc: 0.9570, best val acc: 0.9639, best train loss: 0.1188  @ epoch 38583 )\n",
      "[Epoch: 38820] train loss: 0.1809, train acc: 0.9305, val loss: 0.1478, val acc: 0.9555  (best train acc: 0.9570, best val acc: 0.9639, best train loss: 0.1188  @ epoch 38583 )\n",
      "[Epoch: 38840] train loss: 0.1525, train acc: 0.9380, val loss: 0.1405, val acc: 0.9562  (best train acc: 0.9570, best val acc: 0.9639, best train loss: 0.1188  @ epoch 38583 )\n",
      "[Epoch: 38860] train loss: 0.1647, train acc: 0.9394, val loss: 0.1394, val acc: 0.9548  (best train acc: 0.9570, best val acc: 0.9639, best train loss: 0.1188  @ epoch 38583 )\n",
      "[Epoch: 38880] train loss: 0.1370, train acc: 0.9504, val loss: 0.1239, val acc: 0.9585  (best train acc: 0.9570, best val acc: 0.9639, best train loss: 0.1188  @ epoch 38583 )\n",
      "[Epoch: 38900] train loss: 0.1442, train acc: 0.9466, val loss: 0.1367, val acc: 0.9575  (best train acc: 0.9576, best val acc: 0.9639, best train loss: 0.1188  @ epoch 38583 )\n",
      "[Epoch: 38920] train loss: 0.1572, train acc: 0.9432, val loss: 0.1293, val acc: 0.9568  (best train acc: 0.9576, best val acc: 0.9639, best train loss: 0.1188  @ epoch 38583 )\n",
      "[Epoch: 38940] train loss: 0.1359, train acc: 0.9548, val loss: 0.1255, val acc: 0.9585  (best train acc: 0.9576, best val acc: 0.9639, best train loss: 0.1188  @ epoch 38583 )\n",
      "[Epoch: 38960] train loss: 0.1312, train acc: 0.9521, val loss: 0.1240, val acc: 0.9589  (best train acc: 0.9576, best val acc: 0.9639, best train loss: 0.1188  @ epoch 38583 )\n",
      "[Epoch: 38980] train loss: 0.1490, train acc: 0.9428, val loss: 0.1479, val acc: 0.9541  (best train acc: 0.9576, best val acc: 0.9639, best train loss: 0.1188  @ epoch 38583 )\n",
      "[Epoch: 39000] train loss: 0.1346, train acc: 0.9503, val loss: 0.1300, val acc: 0.9582  (best train acc: 0.9576, best val acc: 0.9639, best train loss: 0.1188  @ epoch 38583 )\n",
      "[Epoch: 39020] train loss: 0.1469, train acc: 0.9445, val loss: 0.1443, val acc: 0.9575  (best train acc: 0.9576, best val acc: 0.9639, best train loss: 0.1188  @ epoch 38583 )\n",
      "[Epoch: 39040] train loss: 0.1456, train acc: 0.9451, val loss: 0.1446, val acc: 0.9521  (best train acc: 0.9576, best val acc: 0.9639, best train loss: 0.1188  @ epoch 38583 )\n",
      "[Epoch: 39060] train loss: 0.1400, train acc: 0.9485, val loss: 0.1500, val acc: 0.9538  (best train acc: 0.9576, best val acc: 0.9639, best train loss: 0.1188  @ epoch 38583 )\n",
      "[Epoch: 39080] train loss: 0.1479, train acc: 0.9422, val loss: 0.1429, val acc: 0.9562  (best train acc: 0.9576, best val acc: 0.9639, best train loss: 0.1188  @ epoch 38583 )\n",
      "[Epoch: 39100] train loss: 0.1408, train acc: 0.9479, val loss: 0.1514, val acc: 0.9464  (best train acc: 0.9576, best val acc: 0.9639, best train loss: 0.1188  @ epoch 38583 )\n",
      "[Epoch: 39120] train loss: 0.1284, train acc: 0.9508, val loss: 0.1225, val acc: 0.9602  (best train acc: 0.9576, best val acc: 0.9639, best train loss: 0.1188  @ epoch 38583 )\n",
      "[Epoch: 39140] train loss: 0.1331, train acc: 0.9471, val loss: 0.1338, val acc: 0.9582  (best train acc: 0.9576, best val acc: 0.9639, best train loss: 0.1188  @ epoch 38583 )\n",
      "[Epoch: 39160] train loss: 0.1365, train acc: 0.9475, val loss: 0.1282, val acc: 0.9572  (best train acc: 0.9576, best val acc: 0.9639, best train loss: 0.1188  @ epoch 38583 )\n",
      "[Epoch: 39180] train loss: 0.1311, train acc: 0.9552, val loss: 0.1370, val acc: 0.9582  (best train acc: 0.9576, best val acc: 0.9639, best train loss: 0.1188  @ epoch 38583 )\n",
      "[Epoch: 39200] train loss: 0.1384, train acc: 0.9490, val loss: 0.1395, val acc: 0.9592  (best train acc: 0.9579, best val acc: 0.9639, best train loss: 0.1175  @ epoch 39193 )\n",
      "[Epoch: 39220] train loss: 0.1284, train acc: 0.9524, val loss: 0.1283, val acc: 0.9612  (best train acc: 0.9579, best val acc: 0.9639, best train loss: 0.1175  @ epoch 39193 )\n",
      "[Epoch: 39240] train loss: 0.1405, train acc: 0.9473, val loss: 0.1424, val acc: 0.9524  (best train acc: 0.9579, best val acc: 0.9639, best train loss: 0.1175  @ epoch 39193 )\n",
      "[Epoch: 39260] train loss: 0.1373, train acc: 0.9486, val loss: 0.1240, val acc: 0.9538  (best train acc: 0.9579, best val acc: 0.9639, best train loss: 0.1175  @ epoch 39193 )\n",
      "[Epoch: 39280] train loss: 0.1391, train acc: 0.9459, val loss: 0.1444, val acc: 0.9524  (best train acc: 0.9579, best val acc: 0.9639, best train loss: 0.1175  @ epoch 39193 )\n",
      "[Epoch: 39300] train loss: 0.1318, train acc: 0.9545, val loss: 0.1358, val acc: 0.9609  (best train acc: 0.9579, best val acc: 0.9639, best train loss: 0.1175  @ epoch 39193 )\n",
      "[Epoch: 39320] train loss: 0.1388, train acc: 0.9486, val loss: 0.1375, val acc: 0.9538  (best train acc: 0.9579, best val acc: 0.9639, best train loss: 0.1175  @ epoch 39193 )\n",
      "[Epoch: 39340] train loss: 0.1541, train acc: 0.9475, val loss: 0.1357, val acc: 0.9551  (best train acc: 0.9579, best val acc: 0.9639, best train loss: 0.1175  @ epoch 39193 )\n",
      "[Epoch: 39360] train loss: 0.1368, train acc: 0.9524, val loss: 0.1334, val acc: 0.9578  (best train acc: 0.9579, best val acc: 0.9639, best train loss: 0.1175  @ epoch 39193 )\n",
      "[Epoch: 39380] train loss: 0.1418, train acc: 0.9478, val loss: 0.1276, val acc: 0.9605  (best train acc: 0.9579, best val acc: 0.9639, best train loss: 0.1175  @ epoch 39193 )\n",
      "[Epoch: 39400] train loss: 0.1464, train acc: 0.9435, val loss: 0.1418, val acc: 0.9592  (best train acc: 0.9579, best val acc: 0.9639, best train loss: 0.1175  @ epoch 39193 )\n",
      "[Epoch: 39420] train loss: 0.1498, train acc: 0.9457, val loss: 0.1383, val acc: 0.9562  (best train acc: 0.9579, best val acc: 0.9639, best train loss: 0.1175  @ epoch 39193 )\n",
      "[Epoch: 39440] train loss: 0.1395, train acc: 0.9492, val loss: 0.1275, val acc: 0.9585  (best train acc: 0.9579, best val acc: 0.9639, best train loss: 0.1175  @ epoch 39193 )\n",
      "[Epoch: 39460] train loss: 0.1412, train acc: 0.9487, val loss: 0.1258, val acc: 0.9592  (best train acc: 0.9579, best val acc: 0.9639, best train loss: 0.1175  @ epoch 39193 )\n",
      "[Epoch: 39480] train loss: 0.1328, train acc: 0.9515, val loss: 0.1391, val acc: 0.9578  (best train acc: 0.9579, best val acc: 0.9639, best train loss: 0.1172  @ epoch 39478 )\n",
      "[Epoch: 39500] train loss: 0.1517, train acc: 0.9396, val loss: 0.1411, val acc: 0.9578  (best train acc: 0.9579, best val acc: 0.9639, best train loss: 0.1172  @ epoch 39478 )\n",
      "[Epoch: 39520] train loss: 0.1296, train acc: 0.9522, val loss: 0.1357, val acc: 0.9558  (best train acc: 0.9579, best val acc: 0.9639, best train loss: 0.1172  @ epoch 39478 )\n",
      "[Epoch: 39540] train loss: 0.1309, train acc: 0.9521, val loss: 0.1333, val acc: 0.9589  (best train acc: 0.9579, best val acc: 0.9639, best train loss: 0.1172  @ epoch 39478 )\n",
      "[Epoch: 39560] train loss: 0.1432, train acc: 0.9496, val loss: 0.1268, val acc: 0.9585  (best train acc: 0.9579, best val acc: 0.9639, best train loss: 0.1172  @ epoch 39478 )\n",
      "[Epoch: 39580] train loss: 0.1422, train acc: 0.9472, val loss: 0.1889, val acc: 0.9400  (best train acc: 0.9579, best val acc: 0.9639, best train loss: 0.1172  @ epoch 39478 )\n",
      "[Epoch: 39600] train loss: 0.1522, train acc: 0.9389, val loss: 0.1419, val acc: 0.9572  (best train acc: 0.9579, best val acc: 0.9639, best train loss: 0.1172  @ epoch 39478 )\n",
      "[Epoch: 39620] train loss: 0.1272, train acc: 0.9526, val loss: 0.1436, val acc: 0.9538  (best train acc: 0.9579, best val acc: 0.9639, best train loss: 0.1172  @ epoch 39478 )\n",
      "[Epoch: 39640] train loss: 0.1465, train acc: 0.9454, val loss: 0.1403, val acc: 0.9555  (best train acc: 0.9579, best val acc: 0.9639, best train loss: 0.1172  @ epoch 39478 )\n",
      "[Epoch: 39660] train loss: 0.1269, train acc: 0.9542, val loss: 0.1395, val acc: 0.9555  (best train acc: 0.9579, best val acc: 0.9639, best train loss: 0.1172  @ epoch 39478 )\n",
      "[Epoch: 39680] train loss: 0.1279, train acc: 0.9502, val loss: 0.1352, val acc: 0.9578  (best train acc: 0.9579, best val acc: 0.9639, best train loss: 0.1172  @ epoch 39478 )\n",
      "[Epoch: 39700] train loss: 0.1451, train acc: 0.9475, val loss: 0.1289, val acc: 0.9582  (best train acc: 0.9579, best val acc: 0.9639, best train loss: 0.1172  @ epoch 39478 )\n",
      "[Epoch: 39720] train loss: 0.1276, train acc: 0.9524, val loss: 0.1421, val acc: 0.9548  (best train acc: 0.9579, best val acc: 0.9639, best train loss: 0.1172  @ epoch 39478 )\n",
      "[Epoch: 39740] train loss: 0.1372, train acc: 0.9494, val loss: 0.1382, val acc: 0.9551  (best train acc: 0.9579, best val acc: 0.9639, best train loss: 0.1172  @ epoch 39478 )\n",
      "[Epoch: 39760] train loss: 0.1352, train acc: 0.9495, val loss: 0.1353, val acc: 0.9565  (best train acc: 0.9579, best val acc: 0.9639, best train loss: 0.1172  @ epoch 39478 )\n",
      "[Epoch: 39780] train loss: 0.1292, train acc: 0.9497, val loss: 0.1332, val acc: 0.9585  (best train acc: 0.9579, best val acc: 0.9639, best train loss: 0.1172  @ epoch 39478 )\n",
      "[Epoch: 39800] train loss: 0.1344, train acc: 0.9477, val loss: 0.1274, val acc: 0.9605  (best train acc: 0.9579, best val acc: 0.9639, best train loss: 0.1172  @ epoch 39478 )\n",
      "[Epoch: 39820] train loss: 0.1313, train acc: 0.9496, val loss: 0.1468, val acc: 0.9528  (best train acc: 0.9579, best val acc: 0.9639, best train loss: 0.1172  @ epoch 39478 )\n",
      "[Epoch: 39840] train loss: 0.1434, train acc: 0.9413, val loss: 0.1360, val acc: 0.9609  (best train acc: 0.9579, best val acc: 0.9639, best train loss: 0.1172  @ epoch 39478 )\n",
      "[Epoch: 39860] train loss: 0.1512, train acc: 0.9417, val loss: 0.1445, val acc: 0.9551  (best train acc: 0.9579, best val acc: 0.9639, best train loss: 0.1172  @ epoch 39478 )\n",
      "[Epoch: 39880] train loss: 0.1366, train acc: 0.9527, val loss: 0.1233, val acc: 0.9595  (best train acc: 0.9579, best val acc: 0.9639, best train loss: 0.1172  @ epoch 39478 )\n",
      "[Epoch: 39900] train loss: 0.1335, train acc: 0.9513, val loss: 0.1369, val acc: 0.9592  (best train acc: 0.9579, best val acc: 0.9639, best train loss: 0.1172  @ epoch 39478 )\n",
      "[Epoch: 39920] train loss: 0.1323, train acc: 0.9521, val loss: 0.1285, val acc: 0.9605  (best train acc: 0.9579, best val acc: 0.9639, best train loss: 0.1172  @ epoch 39478 )\n",
      "[Epoch: 39940] train loss: 0.1496, train acc: 0.9421, val loss: 0.1642, val acc: 0.9420  (best train acc: 0.9579, best val acc: 0.9639, best train loss: 0.1172  @ epoch 39478 )\n",
      "[Epoch: 39960] train loss: 0.1620, train acc: 0.9398, val loss: 0.1286, val acc: 0.9599  (best train acc: 0.9579, best val acc: 0.9639, best train loss: 0.1172  @ epoch 39478 )\n",
      "[Epoch: 39980] train loss: 0.1532, train acc: 0.9438, val loss: 0.1856, val acc: 0.9413  (best train acc: 0.9579, best val acc: 0.9639, best train loss: 0.1172  @ epoch 39478 )\n",
      "[Epoch: 40000] train loss: 0.1601, train acc: 0.9383, val loss: 0.1421, val acc: 0.9558  (best train acc: 0.9579, best val acc: 0.9639, best train loss: 0.1172  @ epoch 39478 )\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98       154\n",
      "           1       0.95      0.97      0.96       703\n",
      "           2       0.90      0.92      0.91       702\n",
      "           3       0.96      0.91      0.94       703\n",
      "           4       0.99      1.00      0.99       702\n",
      "\n",
      "    accuracy                           0.95      2964\n",
      "   macro avg       0.96      0.96      0.96      2964\n",
      "weighted avg       0.95      0.95      0.95      2964\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaYAAAGUCAYAAAB+w4alAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAA7QklEQVR4nO3debxUdf3H8df73ssuCKgssigm5g8trVxSW9xSXBJQTFySysRMUtwlTTNzqyxL08A0KXNB00Qzl3CN3KhUFDVJBK9syqIIyHLv5/fHnEsj3mWQmXtmeT95nMedc+Ysn+/MZT738z3fOUcRgZmZWbGoSjsAMzOzbE5MZmZWVJyYzMysqDgxmZlZUXFiMjOzouLEZGZmRcWJyczMcibpk5Key5rekzRGUndJD0l6LfnZLWubsZJmSHpV0v4tHsPfYzIzs49DUjXwFrArcBKwKCIuk3QO0C0izpY0CLgF2AXYHPgbsE1E1DW1X1dMZmb2ce0D/DciZgFDgAnJ8gnA0OTxEODWiFgZETOBGWSSVJNqChOrmZm1lkN0cN66vibFvVqP1UeQqYYAekbEXICImCupR7K8D/BU1ja1ybImuWIyM7O1JI2SNDVrGtXEem2BQ4DbW9plI8uaTaSumMzMSlxVHmuMiBgPjM9h1QOAf0XE/GR+vqTeSbXUG1iQLK8F+mVt1xeY09yOXTGZmZU4SXmb1sOR/K8bD2ASMDJ5PBK4O2v5CEntJA0ABgLPNLdjV0xmZrZeJHUEvgKckLX4MmCipOOA2cDhABHxkqSJwHRgDXBScyPywMPFzcxK3mFVw/L2Qf6n+rvWq2wqBFdMZmYlrmr9uuCKns8xmZlZUXHFZGZW4lRmNYYTk5lZiXNXnpmZWQG5YjIzK3HuyjMzs6LirjwzM7MCcsVkZlbi8nmtvGLgxGRmVuLW8xp3Ra+80qyZmZU8V0xmZiXOXXlmZlZUPCrPLIukH0q6Ke04zKx8ODGVIUkjJD0taZmkBcnj7yqFM6SSviDpH5LelbRI0hRJO6+zTidJ70u6r5Ht20o6X9KrSXvekvRXSftlrfOGpBXJPhqmq1ujfWmQ1EPSLZLmJK/rFEm75rDdNyT9PU8xvCFp33zsyzacqMrbVAyKIwrLG0mnA78Efgr0AnoC3wH2ANo2sn51AWPpAtwLXAV0B/oAFwIr11l1eLJsv+SWzNnuAIYAxwLdgAFk2nfQOut9NSI2yppG57EdklRM/1c2Ap4FPkfmdZ0A/EXSRqlGZampUlXepmJQHFFYXkjaGPgR8N2IuCMilkbGvyPi6IhYKelGSddKuk/SMmAvSQdJ+rek9yS9KemHWfvcUlJIGpX8hT43SX7Z2kr6vaSlkl6StFOyfBuAiLglIuoiYkVEPBgRL6yz/UjgN8ALwNFZx96XzF0yh0TE0xGxKpnuj4hTNvC1+kZSaVyVVB2vSNon6/lHJV0saQqwHNhK0u6Snk3Wf1bS7sm6IyRNXWf/p0qalDxu8vVtJr77JY1eZ9nzkg6NiNcj4ucRMTd5XceT+aPjk83s7//IvMa7JRXlkmR5O0k/kzRb0nxJv5HUIXluU0n3SlqSVLtPSKqS9AegP3BPsq+zWn7FzXLnxFRedgPaAXe3sN5RwMVAZ+DvwDIyFUlXMpXIiZKGrrPNXsBAYD/gnHW6cQ4Bbk22nwQ0dKP9B6iTNEHSAZK6rRuIpP7AnsAfk+nYrKf3BZ6OiNoW2vNx7Qq8DmwKXADcKal71vNfB0aReZ2WAn8BfgVsAvycTJWyCZk2f1LSwKxtjwJuTh7n8vqu62bgyIYZSYOALZIYPkTSjmQS04ymdhYRL5OpnJ9MKsquyVOXk/kDYkdgazJV7fnJc6cDtcBmZCrv72d2FV8nc+vshir1Jy20xQpMefxXDJyYysumwDsRsaZhQXJ+Z0lyDuZLyeK7I2JKRNRHxAcR8WhETEvmXwBuAb68zr4vjIhlETEN+B1ZH5rA3yPivoioA/4A7AAQEe8BXwACuA54W9IkST2ztj0WeCEipifH3U7SZ7LaMy+rLd2Ttrwr6YN14vtz8lzDdHwOr9cC4MqIWB0RtwGv8uEuwhsj4qXk9dwPeC0i/hARayLiFuAVMh/Oy8n8MXBkEudAYFsyCYscX9913QXsKGmLZP5o4M6I+FA3aNJd+gcy78+7ObQ5e1sBxwOnRsSiiFgKXAKMSFZZDfQGtkheoyciIm+38Lb8cVeeFbOFwKaS1n4NICJ2T/46Xsj/3u83szeStKukRyS9LeldMn9Zb7rOvrO3mQVsnjU/L+vxcqB9QwwR8XJEfCMi+gLbJ9tdmbX+sWQqJSJiDvAYma69hvasPeeUfHh2JXNupd068Q2NiK5Z03W07K11PmjXbVd2mzdPns82i0yFAR+ucI4C/pwkrFxf3w9JksRf+F+SGEHyOjVIutzuAZ6KiEub218TNgM6Av9sSOjA/clyyJynnAE8KOl1Sed8jGOYrTcnpvLyJJlBBENaWG/dv3pvJvPXfb+I2JjMuYh1a/p+WY/7A3PWN7iIeAW4kUyCIjlHMxAYK2mepHlkuteOTBLbZGBnSX3X91g56pNUDQ3WbVf26zSHTFdatv7AW8njB8n8UbAjmQR1c9Z6uby+jbmFzGuxG9ABeKThCUntgD8nxz8hh33BR9/3d4AVwHZZCX3jiNgIMskxIk6PiK2ArwKnZZ2Hc+VURPI3Js9deZZnEbGEzKi3ayQNl7RRcrJ6R6BTM5t2BhZFxAeSdiHzF/+6fiCpo6TtgG8Ct7UUj6RtJZ3ekFgk9SPzof1UsspI4CFgEJlzHDuSSVodgQMi4kEyH8Z/TqqOtpLaAJ9v6dg56gGcLKmNpMOB/wM+MmQ9cR+wjaSjJNVIOiKJ+16ApLvvDjJVRvekXQ1yeX2bOuYWZAa03BYR9QDJa3AHmaRybMPyHMwH+kpqm8RcT6aL9ReSeiT77iNp/+TxwZK2TpL3e0BdMjXsa6scj2sF5uHiVtSSE9GnAWeROYcyHxgHnA38o4nNvgv8SNJSMie+JzayzmNkunUmAz9LkkZLlpKpgJ5WZgTgU8CLwOmS2gNfA66KiHlZ00wy50wauvMOJfPhfxOwBJhJ5nzL4HWO1TBCrGG6K4f4niZTsb1DZjDI8IhY2NiKyfKDyQwIWEjm9T04It7JWu1mMgM2bs8+z0dur29jx1wJ3JnsM7sC2z2JZT9gSVabv9jCLh8GXgLmSWqI+2wy7+tTkt4D/sb/RvcNTObfJ1ONXxMRjybPXQqcl3QBnpFLe8xyJZ/LtOZI2pJMMmizzodtSZP0DeDbEfGFtGMx21Df63hi3j7Ir1p+ber9eb5WnplZiSu3i7iWV2vMsiRfFn2/kek3accGIOnoJuJ7aQP2WdRttsKQlLepGLgrz8ysxI3pNDpvH+RXLrs69ezkrjwzsxJXbl15xZyYXMqZWTnLW2VSbvdjKubExNOvvZ12CKnYdeBmfFCX61dTykf76qqKbDdUbtsrtd2Qabs1rqgTk5mZtaxYvhibL05MZmYlrty68sorzZqZWclzxWRmVuLclWdmZkWlWO6jlC/l1RozMyt5rpjMzEpcsdxHKV+cmMzMSpzclWdmZlY4rpjMzEqcu/LMzKyoeFSemZlZAbliMjMrcXJXnpmZFZWq8kpM7sozM7Oi4sRkZlbqpPxNOR1OXSXdIekVSS9L2k1Sd0kPSXot+dkta/2xkmZIelXS/i3t34nJzKzEqUp5m3L0S+D+iNgW2AF4GTgHmBwRA4HJyTySBgEjgO2AwcA1kqqb27kTk5mZ5UxSF+BLwPUAEbEqIpYAQ4AJyWoTgKHJ4yHArRGxMiJmAjOAXZo7hhOTmVmpa92uvK2At4HfSfq3pN9K6gT0jIi5AMnPHsn6fYA3s7avTZY1yYnJzKzUVSlvk6RRkqZmTaPWOVoN8Fng2oj4DLCMpNuuCY1lu2iuOR4ubmZma0XEeGB8M6vUArUR8XQyfweZxDRfUu+ImCupN7Aga/1+Wdv3BeY0F4MrJjOzUpfHiqklETEPeFPSJ5NF+wDTgUnAyGTZSODu5PEkYISkdpIGAAOBZ5o7hismM7MSpxyHeefR94A/SmoLvA58k0yhM1HSccBs4HCAiHhJ0kQyyWsNcFJE1DW3cycmMzNbLxHxHLBTI0/t08T6FwMX57p/JyYzs1JXZpckqsjEdN2Vl/Dcs/+gy8bduPSaPwBw5x+v57EH7qHzxl0BOPzYE9hh59148d/PMvHGa1mzZg01NTWM+NZJDNrhcylGXxhTnniCyy+9hPq6eoYNH85xxx+fdkitplLbfv655/L4Y4/SvXt37px0T9rhtJqybHfrd+UVVEUOfvjivgdy5oVXfGT5/kO/xo+vupEfX3UjO+y8GwAbddmYU8//CZf8+veMOvU8xl1xUWuHW3B1dXVc8uOLuGbceO665x7uv+8v/HfGjLTDahWV3PYhw4Zy7fjmBl+Vp0ptdympyMS07fY70qlzl5zW3fIT29Btk00B6LPFAFatXsXq1asKGV6re3HaC/Tr35++/frRpm1bBh9wII8+/HDaYbWKSm7753bamS5JD0ElKct2t+KovNZQkYmpKX+7907OHT2S6668hGXvv/eR55+d8ihbbDWQNm3aphBd4SyYv4BevXqtne/RqyfzF8xPMaLWU8lttzKiqvxNRaAgUUhqL2mMpKslnSCp6M9l7XPgMH523W1c9Kvf0bX7Jtz826s/9HztrNeZeOO1fHP0WSlFWDgRH/0SdrndeKwpldx2s2JVqPQ4gcxQwmnAAcBHT+g0IvtSGONbuQ94427dqaqupqqqij33P4TX//Py2ucWvbOAX178fUaddh49ezd7iaeS1LNXT+bNm7d2fsG8+fTo0aOZLcpHJbfdykcKVxcvqEIlpkERcUxEjAOGA1/MZaOIGB8RO0XETqNGrXt5psJasuidtY//+eTj9N1iKwCWvb+UK354Jl8b+R22GfTpVo2ptWy3/aeYPWsWtbW1rF61ivv/eh9f3muvtMNqFZXcdisjZXaOqVBdbKsbHkTEmhS+ldysa35yAS9Pe47331vCKSOHcejRx/HytH8z+/XXkMSmPXrxzdFnAvC3e//E/LlvcfetN3L3rTcCcNZFv6BL127NHKG01NTUMPbc8zjx+G9TX1/P0GGHsvXAgWmH1Soque1nn3E6U595hiVLlvCVvfbkxNGjOfSw4WmHVXCV2u5Sosb62Dd4p1IdmSvOQubKsh2A5cnjiIhchsTF06+9nffYSsGuAzfjg7r6tMNode2rqyqy3VC5ba/UdgO0r85feXLxNj/L2wf5uf85I/VKoiAVU0Q0e3dCMzPLoyLpgsuX4hgbaGZmlij6YdxmZta8YjuPv6GcmMzMSp278szMzArHFZOZWalzV56ZmRUVd+WZmZkVjismM7NSV2YVkxOTmVmJK7fh4u7KMzOzouKKycys1Lkrz8zMioq78szMzArHFZOZWalzV56ZmRWTchuV58RkZlbqyqxi8jkmMzMrKq6YzMxKXZlVTE5MZmalrszOMbkrz8zMioorJjOzUueuPDMzKyblNlzcXXlmZlZUXDGZmZU6d+WZmVlRcVeemZlZ4RR1xbTrwM3SDiE17asr82+GSm03VG7bK7XdeeWuvNbzQV192iGkon11FSNqDk87jFZ365rbeX/VmrTDSMVGbWsq8ve9fXVVRbYb8pyQyysvuSvPzMyKS1FXTGZmloMyG/zgxGRmVuJUZueY3JVnZmZFxRWTmVmpK6+CyYnJzKzkldk5JnflmZlZUXFiMjMrdVXK35QDSW9ImibpOUlTk2XdJT0k6bXkZ7es9cdKmiHpVUn7t9icj/1CmJlZcVAep9ztFRE7RsROyfw5wOSIGAhMTuaRNAgYAWwHDAaukVTd3I6dmMzMLB+GABOSxxOAoVnLb42IlRExE5gB7NLcjpyYzMxKnZS3SdIoSVOzplGNHDGAByX9M+v5nhExFyD52SNZ3gd4M2vb2mRZkzwqz8ys1OWxxIiI8cD4FlbbIyLmSOoBPCTplWbWbayDMJrbuSsmMzNbLxExJ/m5ALiLTNfcfEm9AZKfC5LVa4F+WZv3BeY0t38nJjOzUpfHrryWD6VOkjo3PAb2A14EJgEjk9VGAncnjycBIyS1kzQAGAg809wx3JVnZlbi1LpfsO0J3JUcswa4OSLul/QsMFHSccBs4HCAiHhJ0kRgOrAGOCki6po7gBOTmZnlLCJeB3ZoZPlCYJ8mtrkYuDjXYzgxmZmVuvK6IpETk5lZyfNtL8zMzArHFZOZWakrs6uLOzGZmZW68spL7sozM7Pi4orJzKzUldngBycmM7NSV155yV1565ryxBMccuABHLz//lx/3XVph5N3HTfuyKm3nc4VL17JFdN+wcDPb8MWO2zJRVMu5rKpP+Xipy7jEztv/aFtNum3KTcu+QMHn/bVlKLOrwt/cB77fvmLfG3YkLXL/vPqK3zj6KP42rChjBn9Xd5///0UI2wd5f673pxKbnspKGhikrRpIfefb3V1dVzy44u4Ztx47rrnHu6/7y/8d8aMtMPKq5G/+CbPPfBvTt9+DGd99kzeermWoy87hj9ddDvn7HQmt194G0dfdsyHtjn2ipE8d/+/U4o4/746ZChXXTvuQ8suuuB8vjfmVCbe9Wf22mdffv+7G1KKrnVUwu96U8qy7a14rbzWUJDEJOmrkt4GpkmqlbR7IY6Tby9Oe4F+/fvTt18/2rRty+ADDuTRhx9OO6y86dC5A//3xUE8ckOmTXWr17D83eVEBB06dwSgY5eOLJ6zeO02Ox2yMwtmLqB2+puN7rMUfXanndh4440/tGzWG2/w2Z0yN+LcdbfdePhvD6URWqsp99/15pRj21WlvE3FoFAV08XAFyOiN3AYcGmBjpNXC+YvoFevXmvne/TqyfwF81OMKL96bNWT9955jxOvP4lLn/0Jo8Z9h3Yd2zHhtBs5+vKv8+uZ13LMT47llnP/CEC7ju045Kyh3PGj21OOvPA+sfVAHnvkEQD+9sADzJ83L+WICqvcf9ebU8ltLxWFSkxrIuIVgIh4GuhcoOPkVcRH712lMjqrWF1TxYDPDOChcQ8wduezWLlsJUPOHspXTtiP359+IycNOJHfn34jJ1x3IgCH//Br3Hflvaxc9kHKkRfe+T+6iIm33sLRXzuc5cuX06ZNm7RDKqhy/11vTlm2XXmcikChRuX1kHRaU/MR8fPGNkpu0TsKYNy4cRx73LcLFF7jevbqybysv5QXzJtPjx49mtmitCysXcSi2oXMeCbTn/70nU9yyFnD2HaPbZlw6u8AeOqOJxk1/jsAbL3LQHY99PMcfdkxdOzaiagPVn+wmgeuuT+1NhTKgK224prxmZPgs954g78//ljKERVWuf+uN6cs214k54bypVCJ6To+XCVlzzd5S911bukbH9TVFya6Jmy3/aeYPWsWtbW19OzRg/v/eh+X/uSnrRpDIb07fwkLaxfSe5vNmfufOWy/96d46+Vaeg7owaAvD2L6Y9PZfu/tmfda5j/tD/c8f+22w88/nA/e/6AskxLAooUL6b7JJtTX13P9+HEc9rUj0g6poMr9d705ldz2UlGQxBQRFzb1nKQxhThmPtTU1DD23PM48fhvU19fz9Bhh7L1wIFph5VXvzvlBkb//mRq2tawYOZ8fnPcNUyd9Cwjf/5NqmuqWL1yNdedOK7lHZWw7591BlOffZYlS5ZwwD57c8JJJ7F8+XJuv/UWAPbaZ18OGTos5SgLqxJ+15tSlm0vkkEL+aLG+lsLekBpdkT0z2HVVq+YikX76ipG1Byedhit7tY1t/P+qjVph5GKjdrWUIm/7+2rqyqy3QDtq/OXTX72rT/l7YP8jBsOSz3LpfEF29QbbWZmxSuNSxK1bolmZlbuPPihZZKW0ngCEtChEMc0M6tYZXZxuUINfiiJ7y2ZmVnx8dXFzcxKnbvyzMysmKjMElOZ9UyamVmpc8VkZlbqyqzEcGIyMyt1ZdaV58RkZlbqyiwxlVkBaGZmpc4Vk5lZqSuzEsOJycys1Lkrz8zMrHBcMZmZlboyq5icmMzMSl2Z9X2VWXPMzKzUuWIyMyt17sozM7OiUmaJyV15ZmZWVFwxmZmVujIrMZyYzMxKnbvyzMzMCscVk5lZqSuzismJycys1JVZ31eZNcfMzEqdKyYzs1LnrrzW0766cgu6W9fcnnYIqdiobVH/ShZUpf6+V2q786q88lJxJ6YVa+rTDiEVHWqqWLxiddphtLpuHdowptPotMNIxZXLrua9lWvSDqPVdWlXwwd1lfn/vNQTsqRqYCrwVkQcLKk7cBuwJfAG8LWIWJysOxY4DqgDTo6IB5rbd2m/MmZmBlXK35S7U4CXs+bPASZHxEBgcjKPpEHACGA7YDBwTZLUmm7O+kRhZmZFSMrflNPh1Bc4CPht1uIhwITk8QRgaNbyWyNiZUTMBGYAuzS3/ya78iQtBaJhNvkZyeOIiC45tcDMzMrNlcBZQOesZT0jYi5ARMyV1CNZ3gd4Kmu92mRZk5pMTBHRuannzMysiORx8IOkUcCorEXjI2J81vMHAwsi4p+S9vyY0UUjy9bKafCDpC8AAyPid5I2BTonJZmZmaVt/c4NNStJQuObWWUP4BBJBwLtgS6SbgLmS+qdVEu9gQXJ+rVAv6zt+wJzmouhxXNMki4AzgbGJovaAje1tJ2ZmZWfiBgbEX0jYksygxoejohjgEnAyGS1kcDdyeNJwAhJ7SQNAAYCzzR3jFwqpmHAZ4B/JUHNkeRuPjOzYlEcX7C9DJgo6ThgNnA4QES8JGkiMB1YA5wUEXXN7SiXxLQqIkJSAEjqtEGhm5lZfqWUlyLiUeDR5PFCYJ8m1rsYuDjX/eYyXHyipHFAV0nHA38Drsv1AGZmZuujxYopIn4m6SvAe8A2wPkR8VDBIzMzs9zkcfBDMcj1kkTTgA5khvhNK1w4Zma23orjHFPe5DIq79tkRlAcCgwHnpL0rUIHZmZmlSmXiulM4DPJiS0kbQL8A7ihkIGZmVmOyqtgyikx1QJLs+aXAm8WJhwzM1tvlXKOSdJpycO3gKcl3U3mHNMQWvhylJmZ2cfVXMXU8CXa/yZTg7sbWdfMzNJSZoMfmruI64WtGYiZmX1MZXYDoxbPMUnajMzlzbcjc8E+ACJi7wLGZWZmFSqXPPtH4BVgAHAhmVvmPlvAmMzMbH208o0CCy2XxLRJRFwPrI6IxyLiW8DnCxyXmZnlqswSUy7DxVcnP+dKOojMfTT6Fi4kMzOrZLkkph9L2hg4HbgK6AKcWtCozMwsd5U2+CEi7k0evgvsVdhwzMxsvRVJF1y+NPcF26to5r7sEXFyQSIyM7OK1lzFNPXj7lTSsc09HxG//7j7NjOzdVRKxRQREzZgvzs3skzAV4E+QFEmppUrV/KtY7/O6lWrWFO3hn3325/vjv5e2mEVxMqVKznxWyNZtXoVdWvq2Hvfr3D8d0cz7tdX8fijD1OlKrp1784PfnQxm/XokXa4edFh4w4c8euj6D2oNwTccuIfeeOZmQDsdco+DLlkGOf2P5tlC5fR/3NbcMTVR2Y2FNx/8X1Mu+eFFKPfcPPmzeWH545l4TsLUZUYdtjhHHnM17n26l/x+COPoCrRvfsmXHBR+bznTZnyxBNcfukl1NfVM2z4cI47/vi0Q9owZXaOSRFN9tbl5wCSgKOBs8nc8/3iiMjlf3isWFNf0Ng+csAIVixfTsdOnVi9ejXf/PoxnDV2LJ/eYcdWjaNDTRWLV6xuecUNEBGsWLGCjh07smb1akZ981hOO+scBmz1CTpttBEAt918E2+8/l/OPu+CgsbSoFuHNozpNLpg+z9q/Nd5fcoMnprwJNVtqmnbsS0r3l1B1z5dGXHN0fTYpidXfOFyli1cRpsObahbVUd9XT1denXhzKfGcsEnzqW+rjC/k1cuu5r3Vq4pyL4bvPP227zz9ttsO2gQy5Yt49gRh/PTK39Fj5692Ch5z2/9403MfP2/jP1B67znXdrV8EGBXtOm1NXVcciBBzDut9fTs2dPjjria1z205/xia23btU42lfn78qrV1w9JW8f5KeP3iP18qtgeVZSTXIvp+nAvsDwiDgix6SUCkl07NQJgDVr1rBmzWpUZiVyA0l07NgRaGjrGpDWJiWAD1asKJsugnad2/OJPT7BUxOeBKBudR0r3l0BwNDLD2PSeX+GrD/SVq9YvTYJ1bRr86HnStWmm23GtoMGAdCpUye2HLAVby9YsDYpAaxYsQKV2z0U1vHitBfo178/ffv1o03btgw+4EAeffjhtMPaMBX4Pab1Jukk4BRgMjA4ImYV4jiFUFdXx5GHD+fN2bM54sgj+dSnd0g7pIKpq6vjG0d+jdo3Z3PYEUey/ac+DcC1V/2Sv947iY026syvryuP225tOmAT3n/nfY4adwybf6oPb/77Te468w4G7vlJ3p27hDnT3vrINlvstAUjrj2G7v27c9O3JxSsWkrDnLfe4tVXXma75D2/5le/5C/3TGKjjTbiN9f/LuXoCmvB/AX06tVr7XyPXj2Z9kLR/r2cmyJJKPnSZMUk6SpJv2pqamG/Dd93+gJwj6QXkmmapKL+DaiurmbinXfxwMOP8OK0acx47T9ph1Qw1dXV/GHin5j0wGSmvziN/854DYATv3cKkx6YzP4HHsQdt96ccpT5UVVdTd8d+zHluif42e6Xs2r5SgafeyD7nbU/f73oL41uM2vqLC7f+WJ+/qWfsO8Z+1HTriB/x7W65cuXcfZpYzjtrHPWVkvfPfkU/vLQZAYfdDATbymP97wpjZ2+KPcqsdQ015U3FfhnM1NzTicz0GFY8rNhOjj52ShJoyRNlTR1/PjxubahILp06cJOu+zClL//PdU4WkPnLl347E4789SUD7d1vwMO4pHJf0spqvxaMmcx7761hFlTM8X783c9R98d+9F9y00466mxnD/9Qjbu05UzppxN556dP7Tt/Ffns2rZKnoP2jyN0PNqzerVnH3aGAYfdBB77/uVjzw/+MCDePhvD6UQWevp2asn8+bNWzu/YN58epT6YI+qPE5FoFCj8voAvwS2BV4gcyv2KcCTEbGomWOOBxoyUqsPfli0aBE1NTV06dKFDz74gKeffJJvHndcq8bQWhYnbe2ctPXZp5/i69/8FrNnzaL/FlsA8MRjj7DFgAEpR5ofS+cvZXHtYnoM7MGC1xawzZ6fpPa5N7nmoKvWrnP+9Au54os/YdnCZXTfYhOW1C6mvq6ebv260WObniyavTDFFmy4iOCiC85nywFbcfSx31i7PPs9f/zRR9iyTN7zpmy3/aeYPWsWtbW19OzRg/v/eh+X/uSnaYe1QcrtXHiut704GxhEjre9iIgzkm3bAjsBuwPfAq6TtCQiBm1g3AXxzttv84Pvj6W+vo76+nr2238wX9qzPC928c47b3PRD86lrr6OqA/22W9/vvClPTnn9DHMfuMNVCV69d6cs889P+1Q8+bOM27nmBu+QU3bahbOfIebv3NTk+tutftW7HPaftSvqaO+PrhjzG0sW7isFaPNv+f//S/uu3cSWw/chqMOPxSAk04ew913/olZb7xBVVUVvXr3brUReWmpqalh7LnnceLx36a+vp6hww5l64ED0w7LsrQ4XFzSg8BtwBnAd4CRwNsRcXaLO89cY283YI/kZ1dgWkR8M4fYWr1iKhatMVy8GBV6uHgxa43h4sUojeHixSKfw8V/Pv7pvA0bPW3UrqmXX7mczd0kIq6XdEpEPAY8Jumx5jaQNJ7MjQWXAk+T6cr7eUQs3uCIzczsQ8qsJ69gt73oD7QDXgPeAmqBJR8zRjMza0bFnWPiY9z2IiIGJ1d82I7M+aXTge0lLSIzAKK8O7HNzOxjK9htLyJz8upFSUuSbd8lM1x8F8CJycwsX4pkmHe+5DIq73c0cvuL5BbrTW1zMplKaQ8yXYFTgCeBG4BpHzdYMzP7qErsyrs363F7Ml+andPCNlsCdwCnRsTcjxeamZlVoly68v6UPS/pFqDZywFExGkbGJeZmeWqAiumdQ0kM+rOzMyKQJnlpZzOMS3lw+eY5pG5EoSZmVne5dKV17mldczMLEVlVjK1OMhQ0uRclpmZWTpUpbxNxaDJiklSe6AjsKmkbrD2hiVdgNK//r+ZmRWl5rryTgDGkElC/+R/iek94NeFDcvMzHJWHIVO3jR3P6ZfAr+U9L2IuKqp9czMLF3l9gXbXC5kUS+pa8OMpG6Svlu4kMzMrJLlkpiOj4glDTPJrSuOL1hEZma2XqT8TcUgly/YVklSclFWJFUDbQsblpmZ5axYMkqe5JKYHgAmSvoNmS/afge4v6BRmZlZxcolMZ0NjAJOJDP240HgukIGZWZmuau4wQ8RUR8Rv4mI4RFxGPASmRsGmplZMajK49QCSe0lPSPpeUkvSbowWd5d0kOSXkt+dsvaZqykGZJelbR/Ls3JJZAdJV0u6Q3gIuCVXLYzM7OysxLYOyJ2AHYEBkv6PHAOMDkiBgKTk3kkDQJGkLmj+WDgmmSsQpOau/LDNsnOjgQWArcBioic72JrZmaF15pdeclAuPeT2TbJFMAQYM9k+QTgUTKngoYAt0bESmCmpBlk7mT+ZFPHaK5iegXYB/hqRHwh+ZJt3cdtjJmZFUgrjxeXVC3pOWAB8FBEPA30bLgxbPKzR7J6H+DNrM1rk2VNai4xHUbmFhePSLpO0j6U3YUvzMwsm6RRkqZmTaPWXSci6iJiR6AvsIuk7ZvbZSPLopFlazV3SaK7gLskdQKGAqcCPSVdC9wVEQ82t2MzM2sd+ezJi4jxwPgc110i6VEy547mS+odEXMl9SZTTUGmQuqXtVlfYE5z+81lVN6yiPhjRByc7PA5kpNaZmaWPkl5m3I41mYNl6mT1AHYl8ypn0nAyGS1kcDdyeNJwAhJ7SQNIHMX9GeaO8Z63Vo9IhYB45LJzMwqT29gQjKyrgqYGBH3SnqSzMUYjgNmA4cDRMRLkiYC04E1wEkR0ex4BSVXGipGRRuYmVke5K0DbtzdL+bt8/KEIdunPpZgvSqm1vZBXX3aIaSifXVVRba9fXUVb7+/Mu0wUrHZRu0Y02l02mG0uiuXXc2KNZU52LdDTbNf5VkvFXflBzMzs9ZU1BWTmZnloMwqJicmM7MSV2Z5yV15ZmZWXFwxmZmVujIrmZyYzMxKnKrKKzG5K8/MzIqKKyYzsxJXZj15TkxmZiWvzDKTu/LMzKyouGIyMytx5XZJIicmM7NSV155yV15ZmZWXFwxmZmVuHL7HpMTk5lZiSuvtOSuPDMzKzKumMzMSpxH5ZmZWVEps7zkrjwzMysurpjMzEpcuVVMTkxmZiVOZTYuz115ZmZWVFwxmZmVOHflmZlZUXFiKmPnn3sujz/2KN27d+fOSfekHU6rqrS2Dz94MB07dqSquprq6mquv+lWHn7oQW4Yfy2zZr7Odb+/mW0HbZd2mHnRYeMOHPHro+g9qDcE3HLiH3njmZkA7HXKPgy5ZBjn9j+bZQuX0f9zW3DE1UdmNhTcf/F9TLvnhRSjz783Zs7krNNPWzv/Vm0tJ47+Hscce2yKUVk2J6YsQ4YN5cijj+Lcc85JO5RWV4lt/9W46+nardva+a223ppLfvpzfnLJRSlGlX/DfjqcVx6azo3HXE91m2radmwLQNc+Xfnk3tuyaPaitevOnT6HK77wE+rr6unSqwtnPjWWl+57kfq6+rTCz7stBwxg4p13AVBXV8d+e+3J3vvuk25QG6jcvmBbkMEPkpZKei+ZlmbNL5e0phDHzIfP7bQzXTbumnYYqajktjfYcsBW9N9yQNph5FW7zu35xB6f4KkJTwJQt7qOFe+uAGDo5Ycx6bw/Q8Ta9VevWL02CdW0a/Oh58rR0089Rd9+/dl88z5ph7JBlMepGBSkYoqIztnzkjoD3wVOAO4qxDHN1ocEp510AkgMOexwhhw6PO2QCmLTAZvw/jvvc9S4Y9j8U314899vcteZdzBwz0/y7twlzJn21ke22WKnLRhx7TF079+dm749oayqpXU98Nf7OODAA9MOY4OVW8VU0K48SV2BMcCxwM3AzhGxsJDHNMvFtTf8nk0368HiRQsZ890T2GLLLdnxszulHVbeVVVX03fHftx5+u3MmjqLYT89jMHnHsgn9tiaaw+5utFtZk2dxeU7X0zPT/bkqPFf5+UHp7NmZdF2dHxsq1et4rFHHuHkMaemHYqto1BdeZtKuhT4F7AG+ExEnNdSUpI0StJUSVPHjx9fiNDMANh0sx4AdOu+CV/aa2+mv/hiyhEVxpI5i3n3rSXMmjoLgOfveo6+O/aj+5abcNZTYzl/+oVs3KcrZ0w5m849P9TRwfxX57Nq2Sp6D9o8jdAL7u9/f4JtBw1ik003TTuUDSblbyoGhaqYZgFvA78DlgPHZZeaEfHzxjaKiPFAQ0aKD8q4C8HSs2LFcqI+6NipEytWLOfZp57kG8efkHZYBbF0/lIW1y6mx8AeLHhtAdvs+Ulqn3uTaw66au0650+/kCu++BOWLVxG9y02YUntYurr6unWrxs9tunJotnl2clx/333MbgMuvGgeM4N5UuhEtNPgYazpp3Xea5oz6aefcbpTH3mGZYsWcJX9tqTE0eP5tDDyvPcw7oqqe2LFi7i+2eMATKjsr4y+AA+v/sXeOzhyVz500tZsngxZ55yEgO32Zaf//o36QabB3eecTvH3PANatpWs3DmO9z8nZuaXHer3bdin9P2o35NHfX1wR1jbmPZwmWtGG3rWLFiBU/94x+cd8EP0w7FGqFo5VE3ksZExJU5rFqxFVP76ioqse3tq6t4+/2VaYeRis02aseYTqPTDqPVXbnsalasqUs7jFR0qKnOW6Fzxz/eyNsH+fDdt0y9AEvjWnmntbyKmZnlqtzOMaWRmIqk6WZmVozSuPJD0Z5jMjMrRf4eUw4kLaXxBCSgQyGOaWZWqcorLbXSlR/MzMxy5Yu4mpmVuDLryXNiMjMrdeV2jsm3Vjczs6LiisnMrMSVV73kxGRmVvLKrCfPXXlmZlZcXDGZmZU4D34wM7Oi0prXypPUT9Ijkl6W9JKkU5Ll3SU9JOm15Ge3rG3GSpoh6VVJ+7d0DCcmMzNbH2uA0yPi/4DPAydJGgScA0yOiIHA5GSe5LkRwHbAYOAaSdXNHcCJycysxCmP/1oSEXMj4l/J46XAy0AfYAgwIVltAjA0eTwEuDUiVkbETGAGsEtzx3BiMjMrcfnsypM0StLUrGlU08fVlsBngKeBnhExFzLJC+iRrNYHeDNrs9pkWZM8+MHMzNaKiPHA+JbWk7QR8CdgTES818wAjMaeaPYuE05MZmYlrrUH5UlqQyYp/TEi7kwWz5fUOyLmSuoNLEiW1wL9sjbvC8xpbv/uyjMzK3FVKG9TS5Qpja4HXo6In2c9NQkYmTweCdydtXyEpHaSBgADgWeaO4YrJjMzWx97AF8Hpkl6Lln2feAyYKKk44DZwOEAEfGSpInAdDIj+k6KiLrmDuDEZGZW4lqzKy8i/k7Tl+fbp4ltLgYuzvUYTkxmZiWuzC784HNMZmZWXFwxmZmVuHK7Vp4Tk5lZiSuvtOSuPDMzKzKumMzMSly5deUpotkrQ6SpaAMzM8uDvGWTR1+cm7fPyz237516livqiumDuvq0Q0hF++qqimx7pbYbKrft7aurOEQHpx1GKibFvWmHULSKOjGZmVnLyqwnz4nJzKzU5XIfpVLiUXlmZlZUXDGZmZU4d+WZmVlRKbfh4u7KMzOzouKKycysxJVZweTEZGZW6tyVZ2ZmVkCumMzMSlx51UtOTGZmJa/MevLclWdmZsXFFZOZWYkrt8EPTkxmZiWuzPKSu/LMzKy4uGIyMytx5XZ1cScmM7MS5648MzOzAnLFZGZW4jwqz8zMikqZ5SUnJjOzUlduicnnmMzMrKi4YjIzK3EeLm5mZkXFXXlmZmYFVJCKSdKxzT0fEb8vxHHzYcoTT3D5pZdQX1fPsOHDOe7449MOqVVUaruhctt+/rnn8vhjj9K9e3funHRP2uHkXZ9t+nDmbWevne+1VS9uPv8mHv79w5x129n02LInC96Yz+Vfu4xlS5bRuXtnzr5jLAN3HsjDN05m3Pd+k2L066fchosXqmLauZFpF+Ai4IYCHXOD1dXVccmPL+KaceO56557uP++v/DfGTPSDqvgKrXdUNltHzJsKNeOH592GAXz1n/eYsxnTmbMZ07mtM+NYeXylTx515MMP+dwnp/8PN/ZZhTPT36e4eccDsCqD1bxxx/cxO/OKNqPqCZJ+ZuKQUESU0R8r2ECTgaeBr4MPAV8thDHzIcXp71Av/796duvH23atmXwAQfy6MMPpx1WwVVqu6Gy2/65nXamy8Zd0w6jVXx6nx2Y99+5vD37bXYZsisPT5gMwMMTJrPr0M8DsHL5Sl6eMp1VH6xKM1SjgOeYJNVI+jYwHdgXGB4RR0TEC4U65oZaMH8BvXr1Wjvfo1dP5i+Yn2JEraNS2w2V3fZK8qURX+LxWx4HoGvPriyetxiAxfMW07VH1xQjyw/l8V8xKEhiknQSmYT0OWBwRHwjIl4txLHyKSI+sqxY3qhCqtR2Q2W3vVLUtKlhl0N2Ycrtf087lIJxV15urgK6AF8A7pH0QjJNk9RkxSRplKSpkqaOT6Hvu2evnsybN2/t/IJ58+nRo0erx9HaKrXdUNltrxSfO+Bz/Pdf/2XJgiUALJm/hG69ugHQrVe3tcuteBQqMQ0AdgUOBr6aNTXMNyoixkfEThGx06hRowoUWtO22/5TzJ41i9raWlavWsX9f72PL++1V6vH0doqtd1Q2W2vFF888stru/EAnpn0NHuP3AeAvUfuwzN3P51WaHlTJeVtKgYFGS4eEbMaWy6pGhgBNPp82mpqahh77nmcePy3qa+vZ+iwQ9l64MC0wyq4Sm03VHbbzz7jdKY+8wxLlizhK3vtyYmjR3PoYcPTDiuv2nZox45f2ZFrTrh67bI/XXYHZ008h68ctx9vz36byw+/dO1z1828no5dOlLTtoZdh36eC/b7AW++/GYaoa+XIskneaPG+tg3eKdSF+AkoA8wCXgIGA2cATwXEUNy2E18UFef99hKQfvqKiqx7ZXabqjctrevruIQHZx2GKmYFPfmLZ28MufdvH2Qb7v5xqmnuUJdkugPwGLgSeDbwJlAW2BIRDxXoGOamVWkcquYCpWYtoqITwFI+i3wDtA/IpYW6HhmZhWr3EaSFmrww+qGBxFRB8x0UjIzs1wUKjHtIOm9ZFoKfLrhsaT3CnRMM7OK1JrfY5J0g6QFkl7MWtZd0kOSXkt+dst6bqykGZJelbR/Lu0p1CWJqiOiSzJ1joiarMddCnFMM7NKJSlvUw5uBAavs+wcYHJEDAQmJ/NIGkRmJPZ2yTbXJKOzm+XbXpiZWc4i4nFg0TqLhwATkscTgKFZy2+NiJURMROYQeaC3s3yjQLNzEpcEYzK6xkRcwEiYq6khsun9CFz8e4GtcmyZjkxmZmVuHzej0nSKCD70jvjI+LjXiOuscBa/M6VE5OZma2VJKH1TUTzJfVOqqXewIJkeS3QL2u9vsCclnbmc0xmZiVOeZw+pknAyOTxSODurOUjJLWTNAAYCDzT0s5cMZmZlbjWvLW6pFuAPYFNJdUCFwCXARMlHQfMBg4HiIiXJE0kcxukNcBJyXdbm+XEZGZmOYuII5t4ap8m1r8YuHh9juHEZGZW4opgVF5eOTGZmZW4MstLHvxgZmbFxRWTmVmpK7O+PCcmM7MSV15pyV15ZmZWZFwxmZmVuDLryXNiMjMrdWWWl9yVZ2ZmxcUVk5lZqSuzvjwnJjOzEldeacldeWZmVmRcMZmZlbgy68lzYjIzK33llZnclWdmZkVFES3efr3iSBq1Afe4L2mV2vZKbTdUbtvLqd3z3vsgbx/kvbq0T738csXUuFFpB5CiSm17pbYbKrftZdPuIri1el45MZmZWVHx4AczsxLnUXmVoSz6nT+mSm17pbYbKrftZdTu8spMHvxgZlbiFixdmbcP8h6d26We5VwxmZmVuHLryvPghyyS6iQ9J+lFSbdL6ph2TIUk6f1Glv1Q0ltZr8MhacSWb5J+IWlM1vwDkn6bNX+FpNMkhaTvZS2/WtI3Wjfawmjm/V4uqUdz65Wydf5f3yOpa7J8y3J5vz0qr7ytiIgdI2J7YBXwnbQDSskvImJH4HDgBknl8HvyD2B3gKQ9mwLbZT2/OzAFWACcIqltq0eYnneA09MOooCy/18vAk7Keq4S3++iVw4fOIXyBLB12kGkKSJeBtaQ+RAvdVNIEhOZhPQisFRSN0ntgP8DFgNvA5OBkalEmY4bgCMkdU87kFbwJNAna7483u8yK5mcmBohqQY4AJiWdixpkrQrUE/mP29Ji4g5wBpJ/ckkqCeBp4HdgJ2AF8hUyQCXAadLqk4j1hS8TyY5nZJ2IIWUvJ/7AJPWeark32/l8V8xcGL6sA6SngOmArOB69MNJzWnJq/Dz4AjonyGbjZUTQ2J6cms+X80rBQRM4FngKNSiDEtvwJGSuqSdiAF0PD/eiHQHXgo+8kKfb+LmkflfdiK5NxKpftFRPws7SAKoOE806fIdOW9SebcyntkKoZslwB3AI+3ZoBpiYglkm4Gvpt2LAWwIiJ2lLQxcC+Zc0y/Wmedkn6/PSrPrHRNAQ4GFkVEXUQsArqS6c57MnvFiHgFmJ6sXyl+DpxAmf7BGhHvAicDZ0hqs85zJf1+l9kpJiemCtdRUm3WdFraARXYNDIDOZ5aZ9m7EfFOI+tfDPRtjcBaSbPvd/Ia3AW0Sye8wouIfwPPAyMaebp0328pf1MR8JUfzMxK3OIVq/P2Qd6tQ5vUs1NZluxmZpUk9UySZ05MZmYlrkh64PLG55jMzKyouGIyMytxZVYwOTGZmZW8MuvLc1eepSKfV3KXdKOk4cnj30oa1My6e0ravannm9nuDUkfuWZgU8vXWWe9rtadXPH7jPWN0axcODFZWpq9kvvHvW5ZRHw7IqY3s8qe/O9irmZlwV+wNcu/J4Ctk2rmkeTSONMkVUv6qaRnJb0g6QQAZVwtabqkvwDZ9xJ6VNJOyePBkv4l6XlJkyVtSSYBnppUa1+UtJmkPyXHeFbSHsm2m0h6UNK/JY0jh/+zkv4s6Z+SXpI0ap3nrkhimSxps2TZJyTdn2zzhKRt8/JqWsUps+/X+hyTpSvrSu73J4t2AbaPiJnJh/u7EbFzcmuKKZIeBD4DfJLMNe96krmUzA3r7Hcz4DrgS8m+ukfEIkm/Ad5vuBZgkgR/ERF/T648/gCZW2BcAPw9In4k6SDgQ4mmCd9KjtEBeFbSnyJiIdAJ+FdEnC7p/GTfo4HxwHci4rXkSu7XAHt/jJfRrKw4MVlaGq74DJmK6XoyXWzPJFd7BtgP+HTD+SNgY2Ag8CXgloioA+ZIeriR/X8eeLxhX8l18RqzLzBI//tTsYukzskxDk22/YukxTm06WRJw5LH/ZJYF5K5dchtyfKbgDslbZS09/asY5ftpYCs0Iqk1MkTJyZLy0eu5J58QC/LXgR8LyIeWGe9A4GWLsGiHNaBTHf2bhGxopFYcr7Mi6Q9ySS53SJiuaRHgfZNrB7JcZf4avaWD8XSBZcvPsdkxewB4MSGK0FL2kZSJzK3JhiRnIPqDezVyLZPAl+WNCDZtuHurEuBzlnrPUimW41kvR2Th48DRyfLDgC6tRDrxsDiJCltS6Zia1AFNFR9R5HpInwPmCnp8OQYkrRDC8cwqwhOTFbMfkvm/NG/JL0IjCNT5d8FvEbmyuDXAo+tu2FEvE3mvNCdkp7nf11p9wDDGgY/kLkNwk7J4Irp/G904IXAlyT9i0yX4uwWYr0fqJH0AnARH76C+TJgO0n/JHMO6UfJ8qOB45L4XgKG5PCamH1EuY3K89XFzcxK3Io1dXn7IO9QU516fnLFZGZm6yX5KsarkmZIOifv+3fFZGZW2lasqc9jxVTVbMWUfPn9P8BXgFrgWeDIFr7Yvl5cMZmZlbhW/oLtLsCMiHg9IlYBt5Ln86NOTGZmtj76AG9mzdcmy/LG32MyMytx7aub735bH8kVV7KvdDI+IsZnr9LIZnk9J+TEZGZmayVJaHwzq9SSubJJg77AnHzG4K48MzNbH88CAyUNkNQWGAFMyucBXDGZmVnOImKNpNFkrsxSDdwQES/l8xgeLm5mZkXFXXlmZlZUnJjMzKyoODGZmVlRcWIyM7Oi4sRkZmZFxYnJzMyKihOTmZkVFScmMzMrKv8P3SjFf+4tfrIAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 504x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 504x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "lr              = 0.001\n",
    "epochs          = 40000\n",
    "weight_decay    = 0.0005\n",
    "\n",
    "classes         = ['P', 'LP', 'WN', 'LN', 'RN']\n",
    "gnn_model = GNN7L_Sage(dataset).to(device)\n",
    "pred = train(gnn_model, dataset, epochs, lr, weight_decay, classes, 'GraphSAGE_prova_v2_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.24      0.37       154\n",
      "           1       0.91      0.91      0.91       703\n",
      "           2       0.83      0.86      0.85       702\n",
      "           3       0.85      0.93      0.89       703\n",
      "           4       0.99      1.00      0.99       702\n",
      "\n",
      "    accuracy                           0.89      2964\n",
      "   macro avg       0.88      0.79      0.80      2964\n",
      "weighted avg       0.89      0.89      0.88      2964\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 504x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 504x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = dataset.to(device)\n",
    "\n",
    "model_name = 'GraphSAGE_prova_v2_test'\n",
    "\n",
    "title = model_name + '_' + str(epochs) + '_' + str(weight_decay).replace('.', '_')\n",
    "\n",
    "model_path  = 'Models/' + title\n",
    "image_path  = 'Images/' + title\n",
    "report_path = 'Reports/' + title + '.csv'\n",
    "\n",
    "train_mask  = data['train_mask']\n",
    "test_mask   = data['test_mask']\n",
    "val_mask    = data['val_mask']\n",
    "\n",
    "labels = data.y\n",
    "\n",
    "loaded_model = GNN7L_Sage(data)\n",
    "    \n",
    "loaded_model = loaded_model.to(device)\n",
    "loaded_model.load_state_dict(torch.load(model_path))\n",
    "loaded_model.eval()\n",
    "logits = loaded_model(data)\n",
    "output = logits.argmax(1)\n",
    "\n",
    "print(classification_report(labels[test_mask].to('cpu'), output[test_mask].to('cpu')))\n",
    "\n",
    "class_report = classification_report(labels[test_mask].to('cpu'), output[test_mask].to('cpu'), output_dict=True)\n",
    "classification_report_dataframe = pd.DataFrame(class_report)\n",
    "classification_report_dataframe.to_csv(report_path)\n",
    "\n",
    "#Confusion Matrix\n",
    "norms = [None, \"true\"]\n",
    "for norm in norms:\n",
    "    cm = confusion_matrix(labels[test_mask].to('cpu'), output[test_mask].to('cpu'), normalize=norm)\n",
    "\n",
    "    plt.figure(figsize=(7,7))\n",
    "    \n",
    "    if norm == \"true\":\n",
    "        sn.heatmap(cm, annot=True, fmt=\".3f\", linewidths=.5, square = True, cmap = 'BuPu', xticklabels = classes, yticklabels = classes)\n",
    "    else:\n",
    "        sn.heatmap(cm, annot=True, fmt=\".0f\", linewidths=.5, square = True, cmap = 'BuPu', xticklabels = classes, yticklabels = classes)\n",
    "    plt.title(model_name)\n",
    "    plt.ylabel('Actual label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "    if norm == None:\n",
    "        plt.savefig(image_path + '_notNorm.png')\n",
    "    else:\n",
    "        plt.savefig(image_path + '_Norm.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6aeea6fe202f5c7201d5940e4573c0a76b23e4e16f0e3784ac81597546f2b3b3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
