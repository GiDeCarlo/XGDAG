{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from GNNTrain import train, predict_from_saved_model\n",
    "from GraphSageModel import GNN7L_Sage\n",
    "from CreateGraph import create_graph_from_PPI\n",
    "from CreateDataset import get_dataset_from_graph\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn.conv import SAGEConv\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "disease_Id = 'C0036341'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Reading PPI...ok\n",
      "[+] Creating the graph...ok\n",
      "\t[+] Added 19764 nodes\n",
      "\t[+] Added 682198 edges\n",
      "[+] Removing self loops...ok\n",
      "\t[+] 19764 nodes\n",
      "\t[+] 678932 edges\n",
      "[+] Taking the LCC...ok\n",
      "\t[+] 19761 nodes\n",
      "\t[+] 678932 edges\n",
      "[+] Adding NeDBIT features...ok\n",
      "[+] Saving graph to path: Graphs/grafo_nedbit_C0036341.gml\n",
      "[i] Elapsed time: 44.156\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Graphs/grafo_nedbit_C0036341.gml'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_path = create_graph_from_PPI('Datasets/BIOGRID-ORGANISM-Homo_sapiens-4.4.206.tab3.txt', disease_Id, 'grafo_nedbit_' + disease_Id)\n",
    "graph_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Reading graph...ok\n",
      "[+] Creating dataset...ok\n",
      "[i] Elapsed time: 33.837\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Data(edge_index=[2, 1357864], x=[19761, 6], y=[19761], num_classes=5, train_mask=[19761], test_mask=[19761], val_mask=[19761])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = get_dataset_from_graph(graph_path, disease_Id)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results without training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.97      0.71       124\n",
      "           1       0.89      0.72      0.80       710\n",
      "           2       0.52      0.86      0.65       710\n",
      "           3       0.82      0.07      0.13       710\n",
      "           4       0.75      0.99      0.86       710\n",
      "\n",
      "    accuracy                           0.68      2964\n",
      "   macro avg       0.71      0.72      0.63      2964\n",
      "weighted avg       0.74      0.68      0.61      2964\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 504x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaMAAAGQCAYAAAADew/3AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAABSbUlEQVR4nO3dd3wU1drA8d+TBAgl9GRDCYIUaYJeEUWlKVKUKqio1y5gvyrX7qtS7WKXJpZrQRGkSVEphtAEC9WG0gKk0ANSks15/5hJ2A0pG9jdyW6eL5/9kN05Z+Y8W+aZc+bsrBhjUEoppZwU4XQDlFJKKU1GSimlHKfJSCmllOM0GSmllHKcJiOllFKOi3K6AUoppU5Pb+npt2nRM81s8de6ikN7RkoppRynPSOllApxEWHQr9BkpJRSIU7EkZE1vwr9dKqUUirkac9IKaVCnA7TKaWUclyEDtMppZRSp097RkopFeIkDPoVmoyUUirE6TCdUkop5QfaM1JKqRCnw3RKKaUcp8N0SimllB9oz0gppUKcfulVKaWU4/TadEoppZQfaM9IKaVCnA7TKaWUcpzOplNKKaX8QJNRGBKRxSJyRyHLx4rI/53uepRSJYMQ4bebU0I2GYnIJSKyTEQOiMheEVkqIud7LK8oIodEZE4+dcuKyNMi8ruIHBaRHSIyV0S6epTZIiJH7HXk3N6ylz2R5/EjIpItIjXt5SVmJy4it4hIkudjxpg7jTEjnGpTUURkvP3aZIvILfksP1NEZotIhojsFpEXPZZtEZEuecqf9ByIyPUistp+/XbZr/8lp9HmK0UkSUT2i0iKiEwQkZhTXZ8TROR9ETEi0sjjsXIiMklEDtpxPZSnzjki8qOI/GP/f06e5Q/a9Q7Y6ynnsay6iHxlfwa3isj1PrbzMhH5zd7mIhE5w4c6IiIviMge+/aieExBE5H69rr+sded9z10vd3GwyIyXUSqeyz7QESO59knRPoSi79ESITfbk4JyWQkIpWB2cCbQHWgDjAMOOZRbIB9v6uI1Mqzii+BPsBNQDWgAfA6cGWecr2MMZU8bvcCGGNGez4OvAAsNsbs9mugpdca4G7gp7wLRKQs8C2wEIgH6gIfF2fl9g71NWA04ALqAe9gvSdOVRVgJFAbaGa366XTWF9Q2Ym4YT6LngUaA2cAnYFHRKS7XacsMAPr+a8GfAjMsB9HRLoBjwGXAfWBM7E+pzneBo5jvQY3AO+KSIsi2lkTmAb8H9ZnfzXwuQ8hDgb6Aq2BVkBPYIjH8s+An4EawJPAlyISa2+zBTAOuNFu6z9Y7xdPL+bZV7h9aJPyEJLJCGgCYIz5zBjjNsYcMcZ8Y4xZ61HmZmAssBbrjQ6AfcRzOdDHGLPSGHPcvs0zxvynuA2xj65uxPognhL7qMyIyK0isl1E9onInSJyvoistY+23/Io/6yIfJxP/ag8622G9Ry0s4/W9tuPfyAiIz3K9RGRX+yj379ydjZ51tVQRBbaR5W7ReQTEanqsfxRsXqYGWL1ai6zH29r90AOikiqiLxa1PNhjHnbGLMAOJrP4luAncaYV40xh40xR/O87oUSkSrAcOAeY8w0ex2ZxphZxpiHC6lXW6wesOcR8bn2c1HGGPOp/R76xxizD5gAXOxDexaLyEixevmHRGSWiNSwn9+DIrJKROp7lH/dfo8ctHsi7T2WzRGRVzzufy4ik3xoQxTWgd29+Sy+CRhhjNlnjPnVjusWe1knrElQrxljjhlj3gAEuNRefjPwnjFmg/2cjMipKyIVgf7A/xljDhljkoCZWJ+lwlwFbDDGTDHGHMVKlq1FpGkR9W4GXjHGJBtjdgCveLSlCfAv4Bl7XzIVWGe3D6z9xyxjTKIx5hBWIrxKSlDPV/z4zymhmoz+ANwi8qGI9BCRap4LRaQe1gflE/t2k8fiLsBKY0yyn9rSHutoaaof1nUB1lHotVhH7k9itbcFcI2IdCzOyuydx53AcvtorWreMiLSFvgIeBioCnQAtuSzOgGe48SRfwLWjgAROQtrR3a+MSYG6OaxjteB140xlbGOvL8oTgz5uBDYItaw2m57Z352Meq3A6KBr4qzUWPMTmA5J3ZQANcDXxpjMvOp0gHY4OPqB2LthOtgPUfLgfexjvx/BZ7xKLsKOMde9ikwRUSi7WW3ATeKyKUicgNwPuDLAdaDQGLepG5/rmpj9VRzrMF6P2L/v9YYYzyWr82zPG9dl4jUwDqgdBtj/ihg3QXxWqcx5jDwV3Hr5RPH38aYjEKWe27zL6weXROP8neLdbrgRxHxfI8EhQ7TOcQYcxC4BDBYR2rpIjJTRFx2kZuwPiQbsbrfLUTkXHtZTSAlZ11ijVvvF2tMO++R+HR7Wc5tUD7NuRlrh3TID6GNsI/0vwEOA58ZY9LsI7klwLmFVz8ltwOTjDHfGmOyjTE7jDG/5S1kjNlklzlmjEkHXgVykqMbKAc0t3sJW+wPLEAm0EhEatpHwCtOs711sXbeb2DtKL/GY2jI5vW64T2kUgPYbYzJOoVtfwpcB7k94oH2Y15E5HKs98XTPq73fWPMX8aYA8Bc4C9jzHd2G6fg8bobYz42xuwxxmQZY17Bet7PspelYB18fIh1EHBTnh3sSUQkAWu4Kr+2VrL/P+Dx2AEgxmP5AbwVtjzn7xgf6hbEX/UOAJXs17G4ceRd/gbWQWQcVq/pAxEpslesvIVkMgLrqN8Yc4sxpi7QEmvH9Jq9+CasHlHOEe33WDsHgD1ALY/17LV7DOdhfbA99TXGVPW4TfBcKCLlgas5jSG6PFI9/j6Sz/1K+F8C1pFloUQkTkQm20NxB7HOE9QEK1EBD2D1lNLscrXtqrdjHUH+Zg859TzN9h4Bkowxc40xx4GXsRJMM48yXq8b1vmnHHuAmnmHNH30JdaQZ22sno/BOkjIJSIXYiWoAXmO+gvj8+suIkNF5Ff74Gk/1rmqmh7lZwORwO/20FdRXgOG24kwr5wDrMoej1UGMjyWV8ZbYctz/s7woW5B/FWvMnDI7tUVNw6v5caYnzwOEOZg7XuuKqI9fuW/uXQ6THda7CP5D4CWInIR1lHK42LN4knBGv66zt4BLQDOF5G6ftj0VcBeYLEf1lUch4EKHvfjCylrClkGsJ38T1zn9Zy9rlb2kNu/4cQ71z5ncgnWiW6DNakDY8yfxpjrsI4aX8A6MVzRh+0VZC1Fx1SY5VjnovoWt6IxZj/wDXAN1hDdZ55DVHbveyZwm33Oy6/s80OP2tuvZifaA+C1BxmFNbRXS0Su82G1lwEveXxWAJaLyPX2eZ5dWCf9c7TmxPDjBqCV3bvI0SrP8rx1U40xe7CG2qNEpHEB6y6I1zrt91LD4tbLJ44z85wDyrvcc5tnYh24FnSwYSC4e3Wd2u0QEWlqHyHWte8nYA2frMDqAX0LNMcaWz8Hq+dUAehhD4EtwhrKuUCsad5lsM5FFNfNwEd5xsxzRIlItMetzCmsvyC/AB1EpJ59Qv7xQsqmAnXzDGN5eg+4VazpshEiUqeAk8ExWEeI+0WkDtY5JsA6Z2SfpyiHtaM/gjV0h4j8W0RijTHZwH67SqEzjezXJBrrA13Gfv5y3qsfAxeKSBexps8+AOzG2gEXye4BPA28LSJ9RaSCiJSxzz2+WFR9rF7PTVjnjnKH6ESkJTAPuM8YM8uXtpyCGCALSMd6fz2NxxG7iHQAbrXbdxPwpv1aFaYJ1o72HPsG0IsT59Q+Ap4SkWr2+2IQ1oEfWAdhbuB+saaA50yAWOhR93YRaW6ff3oqp659rmcaMFysr2FcjDWb8X9FtPcrrIPO/vZ75GmsIfmThpbz+Ah4yH5/1waGerTlD6zP1DP2e60fVlLNOQ/8CdBLRNrbyW84MC1nCFREBohIJfvz0xXrQG1mEe1ReYRkMsLqHl8ArBSRw1hJaD3WG+wa4E1jTIrHbTPWmzxnqO4qrOGMj7F2kJuxZszknUU2S7y/O5B70tv+kF+K9SbPz7tYO+Wc2/unGXMuY8y3WNNZ1wI/2rEUZCHWkV2KiJw09dwY8wPWDmwM1lH291i9m7yGYc04OoB1nmaax7JywPNYSSEFqxf0hL2sO7BBRA5hnccYaM+CKsw3WM/ZRcB4++8Odnt/x/qwjwX2Ye3AettDdj4xxrwKPIS1c0zH6h3eC0z3ofpMrJ53qjHG84T4UCAWeM/j/eLrBAZfzcc6p/QHsBUr8W+H3K87fATca5/3S8I60Hg/T8/Fi31OMvezYj+82xhzxP77Gaxh3K1Y742XjDHz7LrHsXqYN2F9jm7DGiI9bi+fB7yIdfC31b55Tsa4GygPpGGd273LGFPoc2afr+yP1QPch7UfGFhYHds4YBbWLLn1WO/hcR7LBwJt7HU+jzXMmm5vcwPWubhP7LbG4D30+x9gh/0cvAQMMsYs9qFNfhMh4rebUyT/g3qllFKh4j8V7vHbjvz1f952JCPphVKVUirEFdL5DRmhOkwXckTkhjxDfoEayinxSvJzIdb3l/Jr2xNF1y5wnfmt75B4fGE1kMS6FmF+2x8bjO0Xl5x8ua2c29wi6m0ooN4NhdVTJYMO0ymlVIh7qOL9ftuRv3r4DR2my0OzpFIqnPltpx8Ov2dUkpMRA6OudroJjpicNYWDx07lAgGhrXK5KI66s51uhiOiIyNKZeylNW6wYlcnlOhkpJRSqmhOflnVXzQZKaVUiAuHYbrQT6dKKaVCnvaMlFIqxOkwnVJKKcc5+TtE/hL6ESillAp52jNSSqkQ5+TvEPmLJiOllApxosN0Siml1OnTnpFSSoU4HaZTSinlOJ1Np5RSSvmB9oyUUirEiQ7TKaWUclxE6CcjHaZTSinlOO0ZKaVUqAuDq3ZrMlJKqRAnOkynlFJKnT7tGSmlVKjTYTqllFKO02E6pZRS6vRpz0gppUJdGPSMNBkppVSIkzA4Z6TDdEoppRynPSOllAp1YTBMVyp6Rq27ncOrG17ntd/epPcjfU9aXrFqRR768mFe+OllRi5/jrotEnKXVahSgQc/H8or61/jlXVjaHxhEwDOaF2fEUtH8fzqlxi14nkant8oWOH4bFnSEvr3upJ+V3bng/cmnLTcGMPLz4+m35Xdua5/P37buBGAY8eOcfP113L9gH5c0683495+K7fO66+8zIDePbmufz8efuB+Mg4eDFo8xbF0yRJ6X9GDnt268d6E/GN/ftQoenbrxoC+ffh144Yi6x7Yv58ht99Gr+7dGHL7bRw8cCAosRRHaY0bSnfsiPjv5pCwT0YSEcFtb9zO8z1HMfTsB7n42oup06yuV5m+j1/F1jWbefRf/+WdW97kljG35i67ecyt/DL/Z4a2fIBH/vUwO35NBuCG5//N1BFTeKzNw0wZ9jk3PP/voMZVFLfbzYujR/H6u2P5YvpMvpk7h7//2uRVZlnSErZt3cq02XN54ulneX7kcADKli3LuxMn8emXX/HpF1NZvjSJdWvWAHBBu3ZMnjadz6Z+Rb0zzsg3yTnN7XYzeuQI3hk3nq9mzWLenK/5a5N37EmJiWzbupVZ8+bx9LBhjBw2vMi6kyZOoO2F7Zg1bz5tL2zHexNLVuylNW4o3bGHi7BPRo3aNiLlrxTSNqfhzsxi2RdLadO7jVeZOs3qsn7hegB2/r6T2DNiqRJXhfIx5WnWvjmLJi0EwJ2ZxT8H/gGso6zyMRUAqFC5Avt27gtiVEXbsH4dCfUSqFs3gTJlynJ59yv4ftEirzLfL1rIlb16IyKc3bo1GRkZ7E5PR0SoUKEiAFlZWWRlZeWeIL3woouJirJGd1u2ak1qampwA/PB+nVrSahXj7oJCZQpW5buPa5g8cKFXmUWLVxIrz59EBFatT6HjIyDpKenFVp30cKF9O7bB4DeffuwaMGCoMdWmNIaN5Tu2AFrmM5fN6dCcGzLQVK9dnX2bN+Te39v8l6q167hVWbb2i207XcBAA3Pb0TNM2KpXrcGcWe6OLj7IHe9dw/PrXqRwePupFyFcgB8+NAH3PDCjby9+V3+/eJNfPbkJ8ELygfpqam4XLVy77tcLtLTvBNHeloarvj43PtxLhdpdhm32831V19F107tuaBdO1q2anXSNmZ+NY2LLmkfoAhOXVpqGvGeccW7SM0Te1paqlfsLlc8aalphdbdu2cPsbFxAMTGxrF3795AhlFspTVuKN2xAyAR/rs5JCBbFpFoEXlARN4SkSEi4txEiXwSvTHG6/6MF6ZTsWpFnl/9Et3v6cGWnzfjznITGRVBg3Mb8O24+Tx+/iMcO3yMPo/2BeDyIV35aOgH3NPgLj4a+gFDJtwVhGB8Z/J5LO/0z7zPg2eZyMhIPp0yja+/XciG9evY9OefXuUmjR9HVFQUPa7s6bc2+0u+ceV9IxQQu091S6jSGjeU7tjDRaDS4IdAG2Ad0AN4xZdKIjJYRFaLyOrx48f7pSF7d+ylRsKJnlD1utXZt8v76OZIxhHG3vEOj7V5mLdveZPKsZVJ35zGnuS97E3ew6YfrPHjldOWU//cMwHoeFMnfvhqJQArvlxe4iYwxLlcpKbuyr2fmppKTfsIz6tMSkru/bTU1NyjwBwxlStzXpu2LF+alPvY7BnTSUr8nhHPvVAiv9/gineR4hlXSipxcXljj/eKPTU1hdi42ELrVq9Rg/T0NADS09OoXr16IMMottIaN5Tu2MG6are/bk4JVDJqboz5tzFmHDAA8Gksxxgz3hjTxhjTZvDgwX5pyF+rNhHfqBax9eOILBPFRddczI+zVnuVqVClApFlrM7bpbdfxq9LfuVIxhEOpO5nT/IeajWpDUDLS8/OncCwb+demndsbj/ekpQ/UyhJmrdoybat29iRnExm5nG+nTeHDp06e5Xp0KkzX8+aiTGGdWvWUCmmEjVjY9m3d2/uLLmjR4/yw4rl1G/QALAmPXz0/nu88sZbRJcvH/S4fNGi5dls27qV5ORkMo8fZ97cOXTs7B17p0s7M2vGDIwxrF3zC5ViYoiNjSu0bqfOlzJz+gwAZk6fQedLLw16bIUprXFD6Y4dCItzRoEaPsvM+cMYk+Xk0XO2O5v3//MeT8x5kojICBZ9sIjkjcl0GXw5AN+N/5Y6zepy9/v3ku3OZsevyYwb9G5u/ff/M4l7P7qfqLJRpG1OZezt7wAw/s5x3PzqrURGRZB5LJMJd41zJL6CREVF8cgTT3L/XYNxu7Pp3bcfDRs1YuoXnwPQ/5prubh9B5YuSaTflT2Ijo7m6REjAdi9O51nn3qCbHc22dnZdOnWjfYdOwHw0nOjOH48k3uG3AHA2a1a8/j/PeNIjAWJiori8Sef4q5Bd5CdnU3fflfRqHFjvpg8GYBrBg6kfYeOJCUm0rN7N6Kjoxk+anShdQFuG3QHDz/4ENOnfkl8rdq8PGaMYzHmp7TGDaU79nAh+Y2XnvZKRdzA4Zy7QHngH/tvY4yp7MNqzMCoq/3etlAwOWsKB49lOd2MoKtcLoqj7mynm+GI6MiIUhl7aY0bIDrSf92QUU1e9tuO/Mk//utI7yEgPSNjTGQg1quUUiofegUGpZRSpY2IdBeR30Vkk4g8ls/yKiIyS0TWiMgGEbk1v/V40mvTKaVUiAvmeXkRiQTeBi4HkoFVIjLTGLPRo9g9wEZjTC8RiQV+F5FPjDHHC1qvJiOllAp1wR2mawtsMsb8DSAik4E+gGcyMkCMWFmyErAXKPREuA7TKaWUyuX5fU/7lvd7NnWA7R73k+3HPL0FNAN2Yn3f9D/GmEJnqmjPSCmlQp0fh+mMMeOBwq46kN/G8s7m6wb8AlwKNAS+FZElxpgCL/OvPSOllAp1wf3SazKQ4HG/LlYPyNOtwDRj2QRsBpoWGkIxwlVKKaVWAY1FpIGIlAUGAjPzlNkGXAYgIi7gLODvwlaqw3RKKRXqgjiBwb6qzr3AfCASmGSM2SAid9rLxwIjgA9EZB3WsN6jxpjdha1Xk5FSSoW4YF9yzRgzB5iT57GxHn/vBLoWZ506TKeUUspx2jNSSqlQFwaXA9JkpJRSoa4E/q5YcekwnVJKKcdpz0gppUKdDtMppZRympM/YOovmoyUUirUhUHPSM8ZKaWUcpz2jJRSKtSFQc9Ik5FSSoW6MDhnpMN0SimlHKc9I6WUCnU6TKeUUspp4TC1W4fplFJKOU57RkopFep0mE4ppZTjdJhOKaWUOn0lumc0OWuK001wTOVyJfqlCZjoyNJ7fFRaYy+tcfuVDtMF1q4DR51ugiNqVYnmxSs+croZQffInJv4Zctep5vhiHPqV+eoO9vpZgRddGREqYwb/JyEQz8X6TCdUkop55XonpFSSikfhMEEBk1GSikV4iQMzhnpMJ1SSinHac9IKaVCXeh3jDQZKaVUyAuDc0Y6TKeUUspx2jNSSqlQFwYTGDQZKaVUqAv9XKTDdEoppZynPSOllAp1YTCBQZORUkqFujAY4wqDEJRSSoU67RkppVSo02E6pZRSTpMwSEY6TKeUUspx2jNSSqlQF/odI01GSikV8sLgCgw6TKeUUspx2jNSSqlQFwYTGDQZKaVUqAv9XKTDdEoppZynPSOllAp1YTCBQZORUkqFutDPRTpMp5RSynmlome0cvlS3nrlBdzZ2VzZpx833Hy713JjDG++8gIrliURHR3NY0+PoEnTZgB8OfkTZk+fCsZwZd/+XH3dvwF4b+xbLE1cjEgE1apX47GnR1AzNi7osRWmwXm1uWzI+UiEsHb+JlZOWe+1vG3/FjTr1ACAiEihRkIV3rruC8pER3Hl0EuoWC0aY2DNvD/4ccZvAERXKkvvxztQJa4SB9IOMeO5RI4dOh702Iryy6rlfDD2NbLdbi7t0Zu+197ktXzHti28++ooNm/6nYE3D6HX1TfkLvt62mcsnDsLRKjXoCF3DX2SsmXL8dqop9iZvA2Afw5nUKFiDC+++1FQ4yrK0iVLeOG50WS7s+k3YAC3DxrktdwYwwujR5OUmEh0+WhGjB5Ns+YtCq17YP9+Hhn6EDt37KB2nTq89OoYKlepEvTYilKaYw+H2XQB7RmJSM1Art8Xbreb118czQuvv8OHn3/Fwvnz2PL3X15lVi5LInn7Nj6ZOouhjz/NmBdGAvD3X38ye/pUxn7wCRM/mcLypESSt20FYOC/b2HSp1/y3idf0O6SDnw4cVzQYyuMRAhd7r6AKU8v4L07Z9KsY31qJHh/iH6YuoEP75vNh/fNJvGDn9m+PpWjh46T7TYsmria9+6cyccPzeHcnk1z615wTUu2/pLChEHT2fpLChde3dKJ8AqV7XYz6e1XeHzkq7w64TOWLvqW5K2bvcpUqlyZW+56kF79r/d6fO/uNOZOn8Jzb03ilfGfkO12s2zxdwA88ORIXnz3I1589yPaXtyZthd3DFpMvnC73YweOYJ3xo3nq1mzmDfna/7atMmrTFJiItu2bmXWvHk8PWwYI4cNL7LupIkTaHthO2bNm0/bC9vx3sQJQY+tKKU5drA+7/66OSUgyUhEeolIOrBORJJF5KJAbMcXv21YT526CdSuU5cyZcpwadfuLE1c7FVmaeIiul3RCxGhxdmtOJSRwZ7d6WzbvJnmLVsRHV2eqKgozvnXeSxZvBCAipUq5dY/euRoibtQYa0mNdi/M4MDKYfIzsrm18QtNGqXUGD5Zp0a8OviLQAc3neE1L/2AnD8SBZ7th2gUs0KADS+MIH131nJfP13f9G4kHU6ZdPvG3HVrourVh2iypThok5dWLU80atMlarVaXRWcyKjTh4cyHa7OX7sGG53FsePHaVaDe9jKmMMKxIXcHHnrgGNo7jWr1tLQr161E1IoEzZsnTvcQWLFy70KrNo4UJ69emDiNCq9TlkZBwkPT2t0LqLFi6kd98+APTu24dFCxYEPbailObYw0WgekajgPbGmFpAf+C5AG2nSOnpacS64nPvx8bFkZ6e6l0mLY1Yl8ujjIv0tDQaNGzE2p9/5MD+/Rw9eoQVS5NIS03JLTfxnTe5umdXvp33NbcNuTvwwRRDpRoVyNh9OPd+xu5/iKlRId+yUeUiaXBebf5YuvWkZZXjKuJqWJ1dv+0GoELV8hzedwSwklaFKtEBaP3p2bsnnRoeQ6Y1asaxb3e6T3Wr14yj54DrufvGfgy5rhflK1ai9XkXeJX5df0vVKlWnVp1SlYiTktNIz7+xHs9Lt5Fapr3ez0tLRWXRxmXK5601LRC6+7ds4dY+/mMjY1j7969gQzjlJTm2AFrAoO/bg4JVDLKMsb8BmCMWQnE+FJJRAaLyGoRWT1+/Hj/tMSYk7fjwzMuIpzR4Eyuu+lW/nvfEB65/24aNm5CZOSJI+k77r6PKbO/4fLuV/LVlMn+aa+f5NdTy+epAKDRBQns2JjG0TznfspER9H3yU4sGL+K40cyA9HMgDD5Bepjz/VQxkFWL1/CWx9OZeynszh29ChLFszzKrNs0bdc1OlyfzTVr/KL+6T3en5lRHyrW4KV5tgB6/3tr5tDApWM4kTkoZxbPvfzZYwZb4xpY4xpM3jwYL80JDbORbpHbyY9Le2kiQaxcXGkp6Z6lEmlZmwsAFf2uYoJ//ucN8a/T+UqVahbr95J27isWw++X/idX9rrLxm7DxNTs2Lu/ZiaFTi09598yzbtUJ9fv9/i9VhEpND3yU5sXPw3fy7blvv4P/uPULFaeQAqVivPPweO+r/xp6lGzTj2pKfl3t+zO+2kobaCrPt5FXHxtahctRpRUVG0vbgjv29cl7vc7c7ih6WLuahjF7+3+3S54l2kpJx4r6elpBIX5/1ej3PFk+pRJjU1hdi42ELrVq9Rg3T7+UxPT6N69eqBDOOUlObYw0WgktEErN5Qzs3zfqVC6vndWc1bkLx9G7t2JJOZmcnCb+ZxUXvvE88Xte/E/DmzMMawYd1aKlaqRI2aVjLat3cPAKkpu0hctIDLuvYAyJ3IALAscTH16jcIUkS+2fXHHqrVjqGKqxIRURE061CfTSu2n1SubIUyJJztYtNy72XdH7iIPdv3s/qrX70e37QimZZdGgLQsktD/sxnnU5reFYzUnZsJy1lJ1mZmSxb/B1tLmzvU92acfH8+esGjh09ijGG9b+spk69+rnL1/20itoJZ3gNA5YULVqezbatW0lOTibz+HHmzZ1Dx86dvcp0urQzs2bMwBjD2jW/UCkmhtjYuELrdup8KTOnzwBg5vQZdL700qDHVpTSHDtgfenVXzeHBGRqtzFmWEHLROSBQGyzIFFRUfzn4cd5+P67yM7OpkevvjRo2IgZU78AoE//a7jw4vasXJbEDVf1pFx0NI/+3/Dc+k8/OpSDBw8QFRnFAw8/QUzlygCMf/t1tm3dQkREBK74Wjz02FPBDKtIJtvw3bs/cPXILkiEsO6bTezZdoBzrmgCwC9z/gCgyUX12PLTTjKPZeXWrdM8jpaXNSRt8z5ufrMnAEs+/Jm/V+9gxZT19Hm8A626NuJg+mFmjP4++MEVITIyitvuGcroJx4gOzubTl17klD/TL6dPQ2Ay3texf69e3j8vls58s9hRCKYM/1zXhn/GY2btuCC9p157J6biYiMokGjJnTp0Sd33cu+/46LS+AQHVjv9ceffIq7Bt1BdnY2fftdRaPGjflisjWEfM3AgbTv0JGkxER6du9GdHQ0w0eNLrQuwG2D7uDhBx9i+tQvia9Vm5fHjHEsxoKU5tiBsPjSq+Q7vh7IDYpsM8acPNZ1MrOrBA4BBUOtKtG8eEXJ+v5KMDwy5yZ+2VJCTxAH2Dn1q3PUne10M4IuOjKiVMYNEB3pv27Iy7dN9duO/L+T+juS2pz40msY5HCllCpBSthXS06FE8kouF0xpZQKd2FwYbeAJCMRySD/pCNA+UBsUymlVOgK1AQGn75XpJRSyg90mE4ppZTTStrlyE5FGIw0KqWUCnXaM1JKqVAXBt0KTUZKKRXqwmCYTpORUkqFujBIRmHQuVNKKRXqtGeklFKhLgy6FZqMlFIq1OkwnVJKKXX6tGeklFKhLgx6RpqMlFIq1IXBGFcYhKCUUiqYRKS7iPwuIptE5LECynQSkV9EZIOIFPkrnNozUkqpUBfEYToRiQTeBi4HkoFVIjLTGLPRo0xV4B2guzFmm4jEFbVe7RkppVSoE/HfrWhtgU3GmL+NMceByUCfPGWuB6YZY7YBGGPSilqpJiOllFK5RGSwiKz2uA3OU6QOsN3jfrL9mKcmQDURWSwiP4rITUVtV4fplFIq1PmxW2GMGQ+ML6RIft2nvD+mGgWcB1yG9YOqy0VkhTHmj4JWqslIKaVCXXCndicDCR736wI78ymz2xhzGDgsIolAa6DAZKTDdEoppYpjFdBYRBqISFlgIDAzT5kZQHsRiRKRCsAFwK+FrVR7RkopFeqC2DMyxmSJyL3AfCASmGSM2SAid9rLxxpjfhWRecBaIBuYaIxZX9h6NRkppVSoC/IYlzFmDjAnz2Nj89x/CXjJ13XqMJ1SSinHac9IKaVCnV6bLrBqVYl2ugmOeWROkdPyw9I59as73QTHREeWzoGK0hq3X4V+LirZyejg0Uynm+CIytFlSMs45nQzgi4uphzDqz/tdDMc8fTe4RzJcjvdjKArHxXJP5mlL26ACmUinW5CiVKik5FSSikfRIR+10iTkVJKhbowOGekg7VKKaUcV2DPSEQyOHG9oZy0a+y/jTGmcoDbppRSyheh3zEqOBkZY2KC2RCllFKnKAzOGfk0TCcil4jIrfbfNUWkQWCbpZRSqjQpcgKDiDwDtAHOAt4HygIfAxcHtmlKKaV8EgYTGHyZTdcPOBf4CcAYs1NEdAhPKaVKitDPRT4N0x03xhjsyQwiUjGwTVJKKVXa+NIz+kJExgFVRWQQcBswIbDNUkop5bMwmMBQZDIyxrwsIpcDB7F+1/xpY8y3AW+ZUkop35SSc0YA67B+x9zYfyullFJ+U+Q5IxG5A/gBuAoYAKwQkdsC3TCllFI+Ej/eHOJLz+hh4FxjzB4AEakBLAMmBbJhSimlfBQG54x8mU2XDGR43M8AtgemOUoppUqjwq5N95D95w5gpYjMwDpn1Adr2E4ppVRJEOYTGHK+2PqXfcsxI3DNUUopVWxh8PsLhV0odVgwG6KUUqr08uXadLHAI0ALIDrncWPMpQFsl1JKKV+FwTCdL527T4DfgAbAMGALsCqAbVJKKVUcIv67OcSXZFTDGPMekGmM+d4YcxtwYYDbpZRSqhTx5XtGmfb/u0TkSmAnUDdwTVJKKVUs4TyBwcNIEakCDAXeBCoDDwa0VUoppXwXBueMfLlQ6mz7zwNA58A2RymlVGlU2Jde38T+DaP8GGPuL6TuTYVt1BjzkU+tU0opVbQw7xmtPo31np/PYwL0AuoAQU1Gy5Ym8coLz5Od7aZPv/7ccvsdXsuNMbzywnMsTVpCdHQ0z4wYRdNmzTl27BiDb72ZzMzjZGW5uezyyxly970APP7wULZu3QLAoYwMKsXE8OkXU4MZVpFWLkvi9ZdfIDs7m559r+Lft9zutdwYw+svv8CKpUsoFx3NE8+O4Kymzdm2ZTPPPPFIbrmdO5K5fcjdXHP9jUwa9w6zpk+jarVqAAy++37aXdI+qHH5ouFljeg2+goiIoWf//cTS19f4rW8XEw5+o0bQOW6VYiIimD5W0tZ8+nP1rLK0fR6ow9xTeMwwKz7ppO8ajsdH+3MuTeexz97DgOwcMR3bPruz2CHVqilS5bw4vPPke1206//AG4bNMhruTGGF58bTVJiItHlyzN81GiaNW8OwDNPPUni999TvXp1ps6YmVvnwP79PPLfoezcsYPaderw0iuvUrlKlaDG5YulSUt4yY69b/8B3HZH/rEvXZJIdHR5hnnE/uxTT5KYaMX+5fSZJ637o/cnMeaVl1m4ZCnV7Pd+iRLO54yMMR+e6kqNMffl/C0iAtwAPAqsAEad6npPhdvt5sXRI3lr3ARcrnhuvv5aOnTqzJkNG+aWWZa0hG3btjFt1hzWr1vL8yNH8MEnn1G2bFnenTiJChUqkJWZyR233MRFl7Tn7Fatee6lV3Lrj3n5JSpVqhTMsIrkdrt59YXRjHl7PLEuF4Nuuo6LO3SiwZkn4l6xNInk7Vv57KvZbFy/lleeG8n4Dz+lXv0GvP/plNz1XHVFFzp0viy33jXX/5vrbrwl2CH5TCKEHi/25OOrPuTgzoPcsWAIv8/7jd2/p+eWOf+OC0j/PY3J139ChRoVuOeH+1k3ZS3ZmW66P9eDvxb8yZe3fE5EmUjKlC+TW2/l2OUsf2upE2EVye1289yokYydMBGXy8UN115Lx86dadioUW6ZpCWJbNu6lZlz57Fu7VpGDR/Gx5M/B6B3334MvP4Gnnr8Ma/1Tpo4kQsuuJDbBg1i0oQJTJo4kQeGDg1qbEVxu908P3Ik706YiCveI/aGeWLftpUZc6zYR48Yxv8+s2Lv1bcf115/A//3xGMnrTtl1y5WLF9OfK1aQYunNApYPhWRKPvnJzYCXYABxphrjTFrA7XN/GxYv46EhHrUrZtAmTJluLx7D75fvNCrzPeLFnFlr96ICGe3ak1GRga709MRESpUqABAVlYWWVlZSJ5rrBtj+O6beXTrcUXQYvLFrxvWUyehHrXr1qVMmTJc1rU7Sd8v8iqT9P0iul/RCxGhxdmtOZSRwe7d6V5lfly1ktp1EoivVTuYzT8tdc6ry77Ne9m/dR/ZmW42TFvHWT2aepUxxlC2UjkAylYsy5F9R8jOyqZsTDnqXVSfn//3EwDZmW6OHTwa9BhOxfp19ns9IYEyZcvS7YoeLF7k/V5fvHAhPXv3QURo1dp6r6enW6/5eW3a5NvjWbxoIb369gWgV9++LFq4IOCxFNf6detIqGfHXqYs3Xr0YPHCvJ/zwmOvUkBv7+UXX+A/Dw1FSvJQWCn5nlGxicg9WEnoPKC7MeYWY8zvgdhWUdLT0nDFx+fed8W5SE9Ny1MmFZfrRJk4l4u0tFTAOuK6/pr+dO3cgQsubEfLVq286v7804/UqFGDemecEcAoii89LZU4lyv3fmyci91peeJOTyPO47mJdZ1cZsH8eXTp1sPrsWlfTObmgf15btjTZBw8GIDWn56YWjEc2HEg9/7BnQeJqVXZq8yqiSuJbRLLgxsf5s6ke5j/+FwwhmpnVOOf3Yfp/VY/Bi2+i56v96FMhRM9o/PvaMuQJXfT682+RFeJpiRJS00lvpbHe90VT1qe93paWhrxnp8Hl4u01NRC17tnzx5iY2MBiI2NZe/evX5stX+kpaV6f85d8aTneS+npRY/9sWLFhIXF8dZTZsWWs5xmowKlDMF/BJgloistW/rRCSoPSNjTp6DkfcIx+QzTyOnTGRkJJ9+MZWvv1nAhvXr2PSn9zmCb+bOoWv3ktUrKlDeuIt4bjIzM1mauJjOXbrmPtZ3wLVMnv417386hRo1a/LWmJcD195Tld8HKk+sDS9tRMr6XYxp/hLjOr5L9xevpGxMOSKiIqjVuhY/vr+KCZ3eJfOf41z8gHVObPWkH3jzX68xrsO7HErJ4PKR3YMRjc/yfx/nKePD5yEk5RNX3h+KK27sR44c4b3x47jr3vsKLKP8JyCz6bC+k5QE7OPEl2aLJCKDgcEA48aNY+BNt/patUBxLhepKSm591PTUqkZF+tdJi6e1NQTZdJSU4mNjfMqE1O5Muedfz7LlyXRqHFjwBq6W7TgOz6a/MVpt9PfYuO8j/rS01KpGZs3bhdpHs9NemoqNTzKrFiaRJOmzaheo0buY55/9+rXn0cfuDcQzT8tGTsPUqXOiSGXyrUrk5GS4VXmnOv/xdLXrEkNOUN6NRvX5EDyAQ7uPMiOH5MB+HXGxtxkdDj9cG79nz76kesm3xDoUIrF5YonZZfHez01hdi4uDxlXKR4fh5SU08qk1eNGjVIT08nNjaW9PR0qlev7t+G+0GcK977c56actJn2BVfvNiTt29nx44dXNu/H2DtF66/uj//m/w5NWvGFljPEWEwgaGwEFYDPxZyK0wd4HWs3z36EBgCtAQyjDFbC6pkjBlvjGljjGkzePBgn4MoTPMWLdm2bRs7kpPJzMzk23lz6dDR++tSHTp14utZMzHGsG7tGipVqkTN2Fj27d2bOwx19OhRflixgvr1G+TW+2HlCs5ocKbXEF9J0bR5C5K3b2XnDivuBd/M45IOnbzKXNyxE/PmzMIYw4Z1a6hUKcbrQ/bd/LlclmeIzvOcUuKihTRo2DigcZyKHT/toPqZ1alaryoRZSJpcdXZ/DHvN68yB5L306DjmQBUjK1IjUY12bdlH4fTDnFwx0FqNLKSboOOZ5L+uzXcU8l1YpJK057NSPvVexjIaS1atmTbtq3We/34cebPmUvHzt7v9Y6dL2X2zBkYY1i7xnrNY2ML37F27NyZWdOnAzBr+nQ6dS5510j2ij3zOPPnzqVT3tg7FS/2xk2asDAxiTnffMecb74jzuXi0ylTS14iwurh+evmlEDNpvsvgIiUBdoAFwG3ARNEZL8xpvmprru4oqKieOTxJ7j/riG4s9307tuPho0aMfULaxZN/2uu5eL2HViatIR+PXsQHV2ep4ePAKwd77NPPUl2tpvsbEOXrt1o37FT7rq/mTeXbt175LdZx0VFRfHgw08w9L67yHa7ubJ3Xxo0bMT0L61eXN8B19Du4vasWLqEgX2vJDo6msefGZFb/+jRI6z+YTkPP/l/Xut99/UxbPrjNxChVq3a/PfJp4Maly+MO5u5j3zNDV/ehERG8MsnP5H+Wzrn3dIGgB8/WE3iy9/T5+1+DEm6BxFYMOwbjuz9B4C5j35Nv3EDiCwbyb4t+5h571cAdHm2K66za4Ex7N+2n68fOnkKsJOioqJ47MknuWvwILKzs+nTrx+NGjVmyueTAbj62oG079CBpMREevXoTnR0NMNGnpjc+th//8vqVT+wf/9+ul7ambvuuZd+/ftz2x2DeOShB/lq2lRq1arFS6+OcSrEAkVFRfHoE09y95BBZLut2Bvmif2SDh1IWpJI7x7diS4fzbMjPGJ/+L/8aMfe7bLO3Hm3FbsKHslvHNWrgPUTEo8CzSnmT0jYlxFqB1xs/18VWGeM8WX8zRw86vMIX1ipHF2GtIxjTjcj6OJiyjG8eslLbsHw9N7hHMlyO92MoCsfFck/maUvboAKZSL91g15dfzKwnfkxfDQ4Asc6R75cm26T4DPgSuBO4GbgfTCKojIeKzfP8oAVgLLgFeNMftOq7VKKaVOEg5zUAL1ExL1gHJACrADSAb2n05DlVJK5S+szxl5KPZPSBhjuttXXmiBdb5oKNBSRPYCy40xz5xGm5VSSoWZgP2EhLFORq0Xkf1YV/w+APQE2gKajJRSyl/CYGp3QH5CQkTux+oRXYzVs1oKLAcmAetOqaVKKaXyFQ5fXC4yGYnI++Tz5Vf73FFB6gNfAg8aY3adcuuUUkqVCr4M0832+Dsa6Id13qhAxpiHTqdRSimliqE09IyMMV4/0iMinwHfBaxFSimliiUMctEpnfZqjDV1WymllPILX84ZZeB9zigF64oMSimlSoIw6Br5MkwXE4yGKKWUOjUSEfrJqMhhOhE56Wcd83tMKaWUOlWF/Z5RNFABqCki1TjxU1WVgdD5DWqllAp3od8xKnSYbgjwAFbi+ZET4R4E3g5ss5RSSvkqrL/0aox5HXhdRO4zxrwZxDYppZQqZXyZ2p0tIlVz7ohINRG5O3BNUkopVRwi/rs5xZdkNMgYsz/njv2bRIMC1iKllFLFEwbZyJdkFCEeA5IiEgmUDVyTlFJKlTa+XJtuPvCFiIzF+vLrncC8gLZKKaWUz8J6AoOHR4HBwF1YM+q+ASYEslFKKaWKIQx+z6jIEIwx2caYscaYAcaY/sAGrB/ZU0oppfzCl54RInIOcB1wLbAZmBbANimllCqGsB6mE5EmwECsJLQH+BwQY4xPv/aqlFIqSMI5GQG/AUuAXsaYTQAi8mBQWqWUUqpUKeycUX+sn4tYJCITROQywuIKSEopFV7C4GtGBScjY8xXxphrgabAYuBBwCUi74pI1yC1TymlVBFExG83p/gym+6wMeYTY0xPoC7wC/BYoBumlFKq9BBjTNGlnFFiG6aUUn7gt27IuBnr/ba/HNKnpSPdI5+mdjvlSFa2001wRPmoiFIZe/moCF55cLbTzXDE0DE9ST90zOlmBF1spXJ8tWKr081wRL8Lz/DbusJhancYfG9XKaVUMIlIdxH5XUQ2iUiBp21E5HwRcYvIgKLWWaJ7RkoppXwQxJ6RfbHst4HLgWRglYjMNMZszKfcC1jXNy2S9oyUUirEBXlqd1tgkzHmb2PMcWAy0CefcvcBU4E0X1aqyUgppVRx1AG2e9xPth/LJSJ1gH7AWF9XqslIKaVCnR+7RiIyWERWe9wG591aPi3IO5vvNeBRY4zb1xD0nJFSSoU4ifDfOSNjzHhgfCFFkoEEj/t1gZ15yrQBJtuz/GoCV4hIljFmekEr1WSklFKqOFYBjUWkAbAD64La13sWMMY0yPlbRD4AZheWiECTkVJKhbxgfs3IGJMlIvdizZKLBCYZYzaIyJ32cp/PE3nSZKSUUqEuyF96NcbMAebkeSzfJGSMucWXdeoEBqWUUo7TnpFSSoW4cLgckCYjpZQKdaGfi3SYTimllPO0Z6SUUiHOn98zcoomI6WUCnGhn4p0mE4ppVQJoD0jpZQKcTqbTimllOPCIBfpMJ1SSinnac9IKaVCXDj0jDQZKaVUiJMwmE+nw3RKKaUcpz0jpZQKcTpMp5RSynHhkIx0mE4ppZTjSkXPaOmSJbz4/Giy3dn06z+A2wYN8lpujOHF50aTlJhIdPloho8aTbPmLXyq++H7kxjz8kssSlpGtWrVghaTL0pr3AD1m8bSuV8LRIT1K7fxw4K/TipTt2ENOvdrTkRkBEcOHeeLt5cXWje2dmW6XH02UWUiyM42LPhyPSnb9gczrCKtWJbE6y+/QLY7m559r+LGW2/3Wm6M4fWXXmD50iVER0fzxLMjOKtZc7Zt2czTjz+SW27njmTuuPNurrn+Rt5+7RWWJn5PmTJlqF03gSeeHU5MTOVgh1ak39euYtYn72Kyszm/Y3c69RzotXzDT8v4duqHSIQQERFJrxvuon6TlgBMmfgKv/2ygkqVq/Lg6Am5ddb+kMh3X/2P9F3buOeZN6nboElQY/KVfum1ACKSAZicu/b/xt5eWWNM0JKg2+3muVEjGDvhPVwuFzdcew0dO3emYaNGuWWSliSybetWZs6dx7q1axg1fDgfT/68yLopu3axYtkyatWqFaxwfFZa4wZryOKy/i35cuxKMvYf4YYH27NpfSp7Uw/llikXHUWXAS2ZOm4lGfuPUr5S2SLrdujdjOXz/2DLb+k0aBZHh17NchNYSeB2u3n1+dGMeWc8cS4Xd9x4HZd07ESDMxvmllmxNInt27cyefpsNqxfy8vPjWTCR59Sr34DPvhsSu56+vXoQofOlwFw/gXtGHLvf4iKiuKdN8bwv/ff4+77H3QkxoJkZ7uZ8dFb3P7I81SpXpO3nr2PZue2w1XnjNwyjZqfS/Nz2yEi7Nr2N5++M5Khz08C4LxLLueiLr35YvyLXuuNr1ufG+9/mmkfvB7UeIor9FNRgIbpjDExxpjK9i0GqA2MAlKAoL6q69etJSGhHnUTEihTtizdrriCxYsWepVZvHAhPXv3QURo1focMjIOkp6eVmTdl194ngeG/rdEDtiW1rgB4utVZf/uwxzY8w/ZbsPvP++gUUuXV5mm59Xhz7UpZOw/CsCRQ8eLrmsM5aKt46hy0VEcOnA0eEH54NcN66mbUI86detSpkwZunTtTtLiRV5llny/iO5X9kJEaHl2aw4dymB3erpXmR9/WEmdugnE16oNQNt2FxEVZcXdomUr0lNTgxNQMWz/+3dquGpTI64WUVFlaH1BRzb+tMyrTLno8rk9iOPHj+K5Cz+zaSvKV4w5ab1xtesRWyshoG33BxHx280pAe2hiEhV4AHgJuBT4HxjzJ5AbjOvtNQ04mvF5953uVysW7vWu0xaKvHxnmXiSUtNK7Tu4oULiXW5OKtp0wBHcGpKa9wAlaqWz00yABkHjlKrnvdQYrXYikRGRnDNPe0oWy6SnxI3s3H1jkLrLvpqI/3vvICOvZuDCJ+9sTQ4AfkoPS2VONeJpBvrcrFx/TqvMrvT0ohznXht4+Jc7E5Po2ZsbO5j330zjy7deuS7ja9nfsVlXbv7ueWn7+C+3VSpfiKGKtVj2f7XbyeVW786iflfTuLQwQPc8tCIYDZRFSEgPSMRqSkizwE/AVnAucaYp4pKRCIyWERWi8jq8ePH+6UtJne00Gs73mVM/mUKqnvkyBEmjh/H3ffe55c2BkJpjRsKGrLwjikiIoK4ulWYNuEHpo5byYVdm1AttmKhdVtffAaLp29g/PAFLJ6xgW4DW/m34acpn5fTp9fcs4ebmZnJ0u8X07lL15OKffjeeCIjo+ja48rTbqu/5RdWfj33lm0uYejzk7jx/mf4duqHgW9YkIj47+aUQPWMtgLpwPvAP8Dtnh8KY8yr+VUyxowHcrKQOZKVfdoNcblcpOxKyb2fmppKbFxcnjLxpKR4lkkhNi6WzMzj+dZN3r6dHTuSueaqvgCkpaZy3YD+fDz5c68jTCeV1rgBMvYfIaZqdO79mCrRJw2pHTpwhCOHj5N13E3WcTfJf+0htnblQuu2OL8ui77aAMAfv+yi67UlKxnFuVykeQyhpaemUrOm9+sS63KRlnritU1L8y6zYmkSTZo2o3qNGl715s6awbIlibz+7oQSebK8SvWaHNh7YrjxwN50KletXmD5M5u2YsqElziccYCKMVWC0cSAKnmvSPEFamr3S1iJCCAmz61SgLaZrxYtz2bbtq3sSE4m8/hx5s+ZQ8fOnb3KdOzcmdkzZ2CMYe2aX6hUKYbY2LgC6zZu0oRFS5Yy99sFzP12AXEuF599ObVE7ZBLa9wAKdsPUDW2IpWrlyciUjjr3Dr8tcH7PMemdanUObM6EiFElYmg1hlV2ZN6qNC6hw4epW5Dayddr3EN9qcfDnpshWnavAXbt29l545kMjMz+e6beVzcsZNXmUs6dGLe17MwxrB+3RoqVYrxHqKbP5cu3b2H6FYsS+KTD9/n+TFvEF2+fDBCKba6Dc5iT+oO9qbvIisrkzUrv6f5ue28yuxO3ZHbM9yx5U/cWVlUqFTyZgWWVgHpGRljni1omYg8EIhtFiQqKorHnnyKuwbfQXZ2Nn36XUWjRo2Z8vlkAK6+diDtO3QkKTGRXj26ER0dzbCRowutGwpKa9wAJtuwcOoG+g+5gIgIYf3K7exJOUSri+oBsHbZNvamHWLLb2nc/HAHjDGsW7GdPSkZAPnWBfj287XWlO+ICNxZbr75Yl2BbXBCVFQUDz3yBA/dexfZbjdX9unLmQ0bMf3LLwDoO+Aa2l3SnuVLl3Btnytzp3bnOHrkCKtWLufhJ/7Pa71jXniOzMzjPHj3EABanN3qpDJOi4yMpPeN9zLppSfIzs6mTYduuOrWZ8XC2QBceGlP1q9O4qek74iMiqRMmXJcf8+Tub28z94Zzd+/reXwoQOMfuB6Lu93I+d37MH61UnM/PgdDmcc4INXn6JWvYbc/vBzToaar5LYWy0uyXcMOZAbFNlmjKnnQ1G/DNOFovJREZTG2MtHRfDKg7OdboYjho7pSfqhY043I+hiK5XjqxVbnW6GI/pdeIbfMsjU5Vv8tiPv366+I5nNiSswhH4KV0op5VdOXIEhuF0xpZQKc+EwTBeMKzB4LQJK5hlQpZQKUaGfigI3geHkrzIrpZRSBSgVF0pVSqlwFgajdJqMlFIq1IXDOSP9PSOllFKO056RUkqFuNDvF2kyUkqpkBcGo3Q6TKeUUsp52jNSSqkQFw4TGDQZKaVUiAuDXKTDdEoppZynPSOllApxEgbz6TQZKaVUiNNhOqWUUsoPtGeklFIhLhx6RpqMlFIqxEWEwTkjHaZTSinlOO0ZKaVUiNNhOqWUUo4Lh2Skw3RKKaUcpz0jpZQKcXptOqWUUo4L/VSkw3RKKaVKAO0ZKaVUiAuHYToxxjjdhoKU2IYppZQf+C2DLF6/y2/7y04tazmS2Up0z+ioO9vpJjgiOjKiVMZeWuMGK/YjWaUv9vJREfSWnk43wxEzzWynm1CilOhkpJRSqmhhMEqnyUgppUJdOPyekc6mU0op5TjtGSmlVIjTYTqllFKOC4ep3TpMp5RSynHaM1JKqRAXBh0jTUZKKRXqdJhOKaWU8gPtGSmlVIgL/X6RJiOllAp5YTBKp8N0SimlnKc9I6WUCnHhMIFBk5FSSoW4MMhFOkynlFLKeZqMlFIqxIkf//m0PZHuIvK7iGwSkcfyWX6DiKy1b8tEpHVR69RhOqWUCnHBHKYTkUjgbeByIBlYJSIzjTEbPYptBjoaY/aJSA9gPHBBYevVnpFSSqniaAtsMsb8bYw5DkwG+ngWMMYsM8bss++uAOoWtVJNRkopFeJExJ+3wSKy2uM2OM/m6gDbPe4n248V5HZgblEx6DCdUkqFOH8O0xljxmMNqxW4ufyq5VtQpDNWMrqkqO1qMlJKqRAX5KndyUCCx/26wM68hUSkFTAR6GGM2VPUSnWYTimlVHGsAhqLSAMRKQsMBGZ6FhCResA04EZjzB++rFR7RkopFeJ8nZLtD8aYLBG5F5gPRAKTjDEbROROe/lY4GmgBvCOfXWILGNMm8LWq8lIKaVCXLCvwGCMmQPMyfPYWI+/7wDuKM46dZhOKaWU40pFMlq6ZAm9r+hBz27deG/ChJOWG2N4ftQoenbrxoC+ffh144Yi634zbx79evXknBbN2bB+fVDiKK5AxH1g/36G3H4bvbp3Y8jtt3HwwIGgxFJcpTX2pUuW0OfKHvTq3o1JBcT9wuhR9Orejav7nRx3QXU/++Rj+lzZg6t692TMyy8FPI5T8a9u/+Kd38Yy7s/x9H90wEnLK1atyOPTnuSNNW/y8spXqdfijNxlve7vzZvr3uat9W/T+z+9cx+/Yfi/eWPNm7z28xsMmz+c6rWqByWW4vLn1G6nBCQZichNhd0Csc2CuN1uRo8cwTvjxvPVrFnMm/M1f23a5FUmKTGRbVu3MmvePJ4eNoyRw4YXWbdR48aMeeNNzmtT6DCoYwIV96SJE2h7YTtmzZtP2wvb8d7Ek3d4Tiutsbvdbp4bNYK3x45n2swC4l5ixT1z7jz+79lhjBo+vMi6q1auZPHCBUz5agbTZs7m5ltvC3psRYmIiGDI23cxrMcz3NP8bjpc15GEZgleZa5+4ho2//I397e+jzE3vcqg162vz9RrcQZdB3VjaNuHuL/1fbTp2ZZajWoDMO2lqdzf+j4eOPd+Vs1exbVPXxf02Hwh4r+bUwLVMzo/n1tbYAQwKUDbzNf6dWtJqFePugkJlClblu49rmDxwoVeZRYtXEivPn0QEVq1PoeMjIOkp6cVWvfMhg2p36BBMEMplkDFvWjhQnr3tb5s3btvHxYtWBD02IpSWmNfv24tCQkn2t7tiitYvMg77sULF9KzdwFxF1D3i88nc+sdgyhbtiwA1WvUCHpsRWnctgm7Nu0idXMqWZlZLJmcyAV9LvQqk9C8HmsWrAFgx+/JxNWPo2pcVRKa1eX3Fb9x/Mgxst3ZbPh+Pe36tQPgSMaR3PrRFaPB5Pt1GuUHAUlGxpj7cm7A/cBKoCPWZSH+FYhtFiQtNY34+Pjc+3HxLlLTUr3LpKXi8ijjcsWTlprmU92SKlBx792zh9jYOABiY+PYu3dvIMM4JaU19rTUNOJrecbkIi315LjjC4q7gLpbt2zhpx9/5N8Dr+X2m29k/bp1AY6k+GrUqcHu7em593cn76ZGHe+kuWXNZtpddREAjc9vQtwZcdSoW4Ot67fSokNLYqrHULZ8Oc67og01E2rm1vv3yBt5b9v7dLyhE588/XFwAiqmYF8oNRACds5IRKJE5A5gI9AFGGCMudYYszZQ28yPyedI5qQnPL8yIr7VLaFKa9xQemM3+XwJPu85gHzjEym0rtudRcbBg/zvs8k8MPRhHhn6YL7rcVJ+w0t52/jl81OoVK0ir/38Bj3v68nfP/+FOyub5N+SmfbClwz/dgTD5g1j85rNuLPcufU+fup/3F7vVr7/ZDFX3tsz0KGcEh2mK4CI3IOVhM4DuhtjbjHG/O5DvdxrIo0fX9jVKHznineRkpKSez8tJZW4uDivMnGueFI9yqSmphAbF+tT3ZIqUHFXr1GD9PQ0ANLT06heveSd0C2tsbtcLlJ2ecaUSmyeuF2ueK/4cuMupK7LFc+lXS5HRDi7VSsiIiLYt28fJcnu5D3UTIjNvV+zbk327vTuuR7JOMIbt73OA+fez5ibXqVybBVSN1sxfzvpWx487wEe7/gYh/ZmsPPPky4owPefLuai/hcHNpBSLFA9ozeByljXI5rl8bsW60SkwJ6RMWa8MaaNMabN4MF5r813alq0PJttW7eSnJxM5vHjzJs7h46dO3uV6XRpZ2bNmIExhrVrfqFSTAyxsXE+1S2pAhV3p86XMnP6DABmTp9B50svDXpsRSmtsbdoeTbbtm1lh932+XNOjrtj587MnukRdyWPuAuo2/myy1i1cgUAW7dsJjMzk2rVqgU9vsL8ueoPajeujau+i6gyUbQf2IGVM1d6lalYpSJRZayvVna9oxsbEjfknhOqElsFgJoJsbS7qh2Jn30PkDuRAaBt7wtI/i05GOEUW4SI325OCdSXXkvMmf2oqCgef/Ip7hp0B9nZ2fTtdxWNGjfmi8mTAbhm4EDad+hIUmIiPbt3Izo6muGjRhdaF2DBd9/y/KhR7Nu7l3vvupOzmjZl7ISJjsWZV6Divm3QHTz84ENMn/ol8bVq8/KYMY7FWJDSGntUVBSPPfkUdw222t6n31U0atSYKZ9bcV997Ym4e/Ww4h42cnShdQH69ruKZ/7vKfr36UWZMmUYMeo5R6cA5yfbnc24e8fy7PzhRERG8N2kb9m+cRvdh/QAYN64udRtlsCDHz1EttvN9o3beeP213PrPzb1CWJqxODOdDP2nrEc3n8YgJufv5k6Z9XFZGeTtjWdd+5825H4ilLCXo5TIsEc+7V/lGmgMeYTH4qbo+7sQDepRIqOjKA0xl5a4wYr9iNZpS/28lER9JaSeR4m0Gaa2X5LIb/tPOC3HXnT2lUcSW2BOmdUWUQeF5G3RKSrWO4D/gauCcQ2lVKqtAqHCQyBGqb7H7APWI51faKHgbJAH2PMLwHaplJKlUqhMuOzMIFKRmcaY84GEJGJwG6gnjEmI0DbU0opFcIClYwyc/4wxrhFZLMmIqWUCoxwmMAQqGTUWkQO2n8LUN6+L4AxxlQO0HaVUqrUKWmzG09FQJKRMSYyEOtVSikVnvTH9ZRSKsSFQcdIk5FSSoW6cBimKxU/rqeUUqpk056RUkqFuNDvF2kyUkqpkKfDdEoppZQfaM9IKaVCXBh0jDQZKaVUqAuDXKTDdEoppZynPSOllAp1YTBOp8lIKaVCXOinIh2mU0opVQJoz0gppUJcGIzSaTJSSqlQFwa5SIfplFJKOU97RkopFerCYJxOk5FSSoW40E9FOkynlFKqBNCekVJKhbgwGKXTZKSUUqEv9LORDtMppZRynBhjnG5DiSMig40x451uhxNKa+ylNW4ovbGHU9wpB4/6bUceXznakW6W9ozyN9jpBjiotMZeWuOG0ht72MQtfrw5RZORUkopx+kEBqWUCnE6my58hcU48ikqrbGX1rih9MYeRnGHfjbSCQxKKRXi0jKO+W1HHhdTzpHMpj0jpZQKcTpMp5RSynFhkIt0Np0nEXGLyC8isl5EpohIBafbFEgiciifx54VkR0ez0NvJ9rmbyIyRkQe8Lg/X0Qmetx/RUQeEhEjIvd5PP6WiNwS3NYGRiGv9z8iEldYuVCW53M9S0Sq2o/XD+fXO9RoMvJ2xBhzjjGmJXAcuNPpBjlkjDHmHOBqYJKIhMP7ZBlwEYAdT02ghcfyi4ClQBrwHxEpG/QWOmc3MNTpRgSQ5+d6L3CPx7LweL3D4ItG4bCTCZQlQCOnG+EkY8yvQBbWjjvULcVORlhJaD2QISLVRKQc0AzYB6QDC4CbHWmlMyYB14pIdacbEgTLgToe98Pi9RY//nOKJqN8iEgU0ANY53RbnCQiFwDZWB/YkGaM2QlkiUg9rKS0HFgJtAPaAGuxesMAzwNDRSTSibY64BBWQvqP0w0JJPv1vAyYmWdRaXu9SySdwOCtvIj8Yv+9BHjPwbY46UER+TeQAVxrwmf+f07v6CLgVawj5IuAA1jDeAAYYzaLyA/A9U400iFvAL+IyCtONyQAcj7X9YEfgW89F4bD662z6cLPEftcSWk3xhjzstONCICc80ZnYw3Tbcc6V3IQq2fgaTTwJZAYzAY6xRizX0Q+Be52ui0BcMQYc46IVAFmY50zeiNPmZB+vcMgF+kwnSpVlgI9gb3GGLcxZi9QFWuobrlnQWPMb8BGu3xp8SowhDA9SDXGHADuB/4rImXyLAvt11vEfzeHaDIq3SqISLLH7SGnGxRg67AmY6zI89gBY8zufMqPAuoGo2FBUujrbT8HXwHlnGle4BljfgbWAAPzWRxur3dI0csBKaVUiNt/JNNvO/Kq5cvo5YCUUkoVXzhMYNBhOqWUUo7TnpFSSoW4MOgYaTJSSqmQFwbjdDpMp5RSynGajJQj/HmFdBH5QEQG2H9PFJHmhZTtJCIXFbS8kHpbROSka/QV9HieMsW6CrZ9Je3/FreNqvQKg+ukajJSjin0Cumnep0wY8wdxpiNhRTpxIkLpioVFsLgO6+ajFSJsARoZPdaFtmXpVknIpEi8pKIrBKRtSIyBEAsb4nIRhH5GvD8LZ7FItLG/ru7iPwkImtEZIGI1MdKeg/avbL2IhIrIlPtbawSkYvtujVE5BsR+VlExuHDQaOITBeRH0Vkg4gMzrPsFbstC0Qk1n6soYjMs+ssEZGmfnk2lQpBOoFBOcrjCunz7IfaAi3ti1cOxro6wvn2zzwsFZFvgHOBs7CuMefCuozLpDzrjQUmAB3sdVU3xuwVkbHAoZxr79mJb4wxJsm+ovd8rJ+TeAZIMsYMF5ErAa/kUoDb7G2UB1aJyFRjzB6gIvCTMWaoiDxtr/teYDxwpzHmT/sK6e8Al57C06hKvdCfwKDJSDklvyukXwT8YIzZbD/eFWiVcz4IqAI0BjoAnxlj3MBOEVmYz/ovBBJz1mVfhy4/XYDmcmJ8orKIxNjbuMqu+7WI7PMhpvtFpJ/9d4Ld1j1YP8Pxuf34x8A0EalkxzvFY9thexkeFVhhMJlOk5FyzElXSLd3yoc9HwLuM8bMz1PuCqCoy5+ID2XAGqpuZ4w5kk9bfL7Eioh0wkps7Ywx/4jIYiC6gOLG3u5+vUq8UhY9Z6RKsvnAXTlXWBaRJiJSEesy/wPtc0q1gM751F0OdBSRBnbdnF8xzQBiPMp9gzVkhl3uHPvPROAG+7EeQLUi2loF2GcnoqZYPbMcEUBO7+56rOG/g8BmEbna3oaISOsitqFUvnQ2nVKBNRHrfNBPIrIeGIfVm/8K+BPritvvAt/nrWiMScc6zzNNRNZwYphsFtAvZwID1k8KtLEnSGzkxKy+YUAHEfkJa7hwWxFtnQdEichaYATeVwY/DLQQkR+xzgkNtx+/Abjdbt8GoI8Pz4lSJwmH2XR61W6llApxR7LcftuRl4+KdCQlac9IKaVCXnAH6uyvTfwuIptE5LF8louIvGEvXysi/ypqnTqBQSmlQlwwh9fsL6S/DVwOJGN9jWFmni+b98CaTdoYuABrOP2CwtarPSOllFLF0RbYZIz52xhzHJjMyec7+wAfGcsKoKo92ahA2jNSSqkQFx0Z4be+kf1lc88veY83xoz3uF8H2O5xP5mTez35lakD7Cpou5qMlFJK5bITz/hCiuSX+PJOoPCljBcdplNKKVUcyVhXGMlRF9h5CmW8aDJSSilVHKuAxiLSQETKAgOBmXnKzARusmfVXYh1jckCh+hAh+mUUkoVgzEmS0TuxbpCSiQwyRizQUTutJePBeYAVwCbgH+AW4tar37pVSmllON0mE4ppZTjNBkppZRynCYjpZRSjtNkpJRSynGajJRSSjlOk5FSSinHaTJSSinluP8HaeMLRYXZercAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 504x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "predict_from_saved_model('SAGE7L_multiclass_16HC_v2_max_40000_0_0005', dataset.to(device), ['P', 'LP', 'WN', 'LN', 'RN'], files_name='GraphSAGE_' + disease_Id + '_nonTrained')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results with training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "882b28039b9f4d388c85de5ddfb74b65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40001 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch: 0000] train loss: 51.6491, train acc: 0.1092, val loss: 17.1695, val acc: 0.0425  (best train acc: 0.1092, best val acc: 0.0425, best train loss: 51.6491  @ epoch 0 )\n",
      "[Epoch: 0020] train loss: 2.3694, train acc: 0.1708, val loss: 1.8692, val acc: 0.0975  (best train acc: 0.2640, best val acc: 0.2944, best train loss: 2.3694  @ epoch 20 )\n",
      "[Epoch: 0040] train loss: 1.5887, train acc: 0.2984, val loss: 1.4797, val acc: 0.3427  (best train acc: 0.3044, best val acc: 0.3427, best train loss: 1.5887  @ epoch 40 )\n",
      "[Epoch: 0060] train loss: 1.4484, train acc: 0.3560, val loss: 1.3877, val acc: 0.4118  (best train acc: 0.3560, best val acc: 0.4142, best train loss: 1.4475  @ epoch 58 )\n",
      "[Epoch: 0080] train loss: 1.3862, train acc: 0.3944, val loss: 1.3446, val acc: 0.4304  (best train acc: 0.3944, best val acc: 0.4304, best train loss: 1.3862  @ epoch 80 )\n",
      "[Epoch: 0100] train loss: 1.3894, train acc: 0.3679, val loss: 1.3182, val acc: 0.4546  (best train acc: 0.4069, best val acc: 0.4546, best train loss: 1.3606  @ epoch 95 )\n",
      "[Epoch: 0120] train loss: 1.3329, train acc: 0.4138, val loss: 1.2914, val acc: 0.4577  (best train acc: 0.4230, best val acc: 0.4644, best train loss: 1.3329  @ epoch 120 )\n",
      "[Epoch: 0140] train loss: 1.3247, train acc: 0.4163, val loss: 1.2701, val acc: 0.4722  (best train acc: 0.4312, best val acc: 0.4722, best train loss: 1.3169  @ epoch 139 )\n",
      "[Epoch: 0160] train loss: 1.2970, train acc: 0.4337, val loss: 1.2550, val acc: 0.4718  (best train acc: 0.4343, best val acc: 0.4742, best train loss: 1.2933  @ epoch 159 )\n",
      "[Epoch: 0180] train loss: 1.2824, train acc: 0.4480, val loss: 1.2322, val acc: 0.4847  (best train acc: 0.4480, best val acc: 0.4857, best train loss: 1.2691  @ epoch 177 )\n",
      "[Epoch: 0200] train loss: 1.2382, train acc: 0.4699, val loss: 1.2140, val acc: 0.4897  (best train acc: 0.4699, best val acc: 0.4944, best train loss: 1.2382  @ epoch 200 )\n",
      "[Epoch: 0220] train loss: 1.2349, train acc: 0.4689, val loss: 1.2017, val acc: 0.5039  (best train acc: 0.4723, best val acc: 0.5079, best train loss: 1.2349  @ epoch 220 )\n",
      "[Epoch: 0240] train loss: 1.2084, train acc: 0.4775, val loss: 1.1780, val acc: 0.5116  (best train acc: 0.4946, best val acc: 0.5170, best train loss: 1.2040  @ epoch 237 )\n",
      "[Epoch: 0260] train loss: 1.1948, train acc: 0.4709, val loss: 1.1634, val acc: 0.5143  (best train acc: 0.4985, best val acc: 0.5265, best train loss: 1.1732  @ epoch 254 )\n",
      "[Epoch: 0280] train loss: 1.1775, train acc: 0.4871, val loss: 1.1434, val acc: 0.5302  (best train acc: 0.5087, best val acc: 0.5386, best train loss: 1.1732  @ epoch 254 )\n",
      "[Epoch: 0300] train loss: 1.1551, train acc: 0.5100, val loss: 1.1334, val acc: 0.5288  (best train acc: 0.5168, best val acc: 0.5386, best train loss: 1.1426  @ epoch 291 )\n",
      "[Epoch: 0320] train loss: 1.2304, train acc: 0.4662, val loss: 1.1257, val acc: 0.5234  (best train acc: 0.5192, best val acc: 0.5386, best train loss: 1.1426  @ epoch 291 )\n",
      "[Epoch: 0340] train loss: 1.1531, train acc: 0.5056, val loss: 1.1108, val acc: 0.5383  (best train acc: 0.5252, best val acc: 0.5457, best train loss: 1.1318  @ epoch 336 )\n",
      "[Epoch: 0360] train loss: 1.1659, train acc: 0.4939, val loss: 1.1017, val acc: 0.5346  (best train acc: 0.5291, best val acc: 0.5464, best train loss: 1.1108  @ epoch 351 )\n",
      "[Epoch: 0380] train loss: 1.1120, train acc: 0.5313, val loss: 1.0839, val acc: 0.5541  (best train acc: 0.5354, best val acc: 0.5558, best train loss: 1.1072  @ epoch 362 )\n",
      "[Epoch: 0400] train loss: 1.1441, train acc: 0.4982, val loss: 1.0739, val acc: 0.5336  (best train acc: 0.5541, best val acc: 0.5656, best train loss: 1.0817  @ epoch 387 )\n",
      "[Epoch: 0420] train loss: 1.1578, train acc: 0.4952, val loss: 1.0446, val acc: 0.5710  (best train acc: 0.5541, best val acc: 0.5804, best train loss: 1.0660  @ epoch 412 )\n",
      "[Epoch: 0440] train loss: 1.0629, train acc: 0.5426, val loss: 1.0064, val acc: 0.5899  (best train acc: 0.5569, best val acc: 0.5902, best train loss: 1.0371  @ epoch 438 )\n",
      "[Epoch: 0460] train loss: 1.0118, train acc: 0.5588, val loss: 0.9815, val acc: 0.5987  (best train acc: 0.5594, best val acc: 0.6057, best train loss: 1.0118  @ epoch 460 )\n",
      "[Epoch: 0480] train loss: 1.0281, train acc: 0.5602, val loss: 0.9732, val acc: 0.6165  (best train acc: 0.5775, best val acc: 0.6246, best train loss: 0.9992  @ epoch 477 )\n",
      "[Epoch: 0500] train loss: 1.0091, train acc: 0.5433, val loss: 0.9658, val acc: 0.5686  (best train acc: 0.5784, best val acc: 0.6280, best train loss: 0.9912  @ epoch 488 )\n",
      "[Epoch: 0520] train loss: 0.9765, train acc: 0.5844, val loss: 0.9353, val acc: 0.6172  (best train acc: 0.5968, best val acc: 0.6344, best train loss: 0.9500  @ epoch 515 )\n",
      "[Epoch: 0540] train loss: 1.0361, train acc: 0.5531, val loss: 0.9130, val acc: 0.6078  (best train acc: 0.6061, best val acc: 0.6503, best train loss: 0.9378  @ epoch 529 )\n",
      "[Epoch: 0560] train loss: 1.0097, train acc: 0.5463, val loss: 0.9013, val acc: 0.6084  (best train acc: 0.6122, best val acc: 0.6556, best train loss: 0.9378  @ epoch 529 )\n",
      "[Epoch: 0580] train loss: 0.9413, train acc: 0.6105, val loss: 0.8833, val acc: 0.6273  (best train acc: 0.6149, best val acc: 0.6556, best train loss: 0.8982  @ epoch 568 )\n",
      "[Epoch: 0600] train loss: 0.8613, train acc: 0.6561, val loss: 0.8798, val acc: 0.6965  (best train acc: 0.6561, best val acc: 0.6965, best train loss: 0.8613  @ epoch 600 )\n",
      "[Epoch: 0620] train loss: 0.9000, train acc: 0.6341, val loss: 0.8617, val acc: 0.6361  (best train acc: 0.6561, best val acc: 0.6965, best train loss: 0.8613  @ epoch 600 )\n",
      "[Epoch: 0640] train loss: 0.8906, train acc: 0.6063, val loss: 0.8696, val acc: 0.5639  (best train acc: 0.6617, best val acc: 0.7019, best train loss: 0.8537  @ epoch 627 )\n",
      "[Epoch: 0660] train loss: 0.8847, train acc: 0.6348, val loss: 0.8071, val acc: 0.6961  (best train acc: 0.6617, best val acc: 0.7069, best train loss: 0.8516  @ epoch 649 )\n",
      "[Epoch: 0680] train loss: 0.8524, train acc: 0.6398, val loss: 0.7995, val acc: 0.6597  (best train acc: 0.6617, best val acc: 0.7069, best train loss: 0.8269  @ epoch 676 )\n",
      "[Epoch: 0700] train loss: 0.9591, train acc: 0.5568, val loss: 0.8241, val acc: 0.6013  (best train acc: 0.6617, best val acc: 0.7224, best train loss: 0.7991  @ epoch 695 )\n",
      "[Epoch: 0720] train loss: 0.8088, train acc: 0.6543, val loss: 0.7725, val acc: 0.7008  (best train acc: 0.6620, best val acc: 0.7224, best train loss: 0.7991  @ epoch 695 )\n",
      "[Epoch: 0740] train loss: 0.8173, train acc: 0.6576, val loss: 0.7670, val acc: 0.7032  (best train acc: 0.6648, best val acc: 0.7224, best train loss: 0.7991  @ epoch 695 )\n",
      "[Epoch: 0760] train loss: 0.8965, train acc: 0.6277, val loss: 0.7761, val acc: 0.6607  (best train acc: 0.6715, best val acc: 0.7261, best train loss: 0.7986  @ epoch 742 )\n",
      "[Epoch: 0780] train loss: 0.8152, train acc: 0.6442, val loss: 0.7500, val acc: 0.7015  (best train acc: 0.6715, best val acc: 0.7261, best train loss: 0.7812  @ epoch 772 )\n",
      "[Epoch: 0800] train loss: 0.8466, train acc: 0.6525, val loss: 0.7312, val acc: 0.7224  (best train acc: 0.6938, best val acc: 0.7349, best train loss: 0.7536  @ epoch 796 )\n",
      "[Epoch: 0820] train loss: 0.7918, train acc: 0.6692, val loss: 0.7372, val acc: 0.7137  (best train acc: 0.6938, best val acc: 0.7349, best train loss: 0.7536  @ epoch 796 )\n",
      "[Epoch: 0840] train loss: 0.8336, train acc: 0.6443, val loss: 0.7622, val acc: 0.6745  (best train acc: 0.6938, best val acc: 0.7349, best train loss: 0.7536  @ epoch 796 )\n",
      "[Epoch: 0860] train loss: 0.7940, train acc: 0.6476, val loss: 0.7148, val acc: 0.7157  (best train acc: 0.6938, best val acc: 0.7359, best train loss: 0.7374  @ epoch 853 )\n",
      "[Epoch: 0880] train loss: 0.8742, train acc: 0.6293, val loss: 0.7006, val acc: 0.7413  (best train acc: 0.6938, best val acc: 0.7555, best train loss: 0.7374  @ epoch 853 )\n",
      "[Epoch: 0900] train loss: 0.7690, train acc: 0.6667, val loss: 0.6868, val acc: 0.7417  (best train acc: 0.6938, best val acc: 0.7555, best train loss: 0.7374  @ epoch 853 )\n",
      "[Epoch: 0920] train loss: 0.7346, train acc: 0.6848, val loss: 0.6882, val acc: 0.7396  (best train acc: 0.6938, best val acc: 0.7555, best train loss: 0.7346  @ epoch 920 )\n",
      "[Epoch: 0940] train loss: 0.8024, train acc: 0.6617, val loss: 0.6707, val acc: 0.7272  (best train acc: 0.6998, best val acc: 0.7555, best train loss: 0.7267  @ epoch 927 )\n",
      "[Epoch: 0960] train loss: 0.7467, train acc: 0.6938, val loss: 0.6712, val acc: 0.7150  (best train acc: 0.7064, best val acc: 0.7555, best train loss: 0.7059  @ epoch 948 )\n",
      "[Epoch: 0980] train loss: 0.7524, train acc: 0.6886, val loss: 0.6454, val acc: 0.7592  (best train acc: 0.7076, best val acc: 0.7592, best train loss: 0.7059  @ epoch 948 )\n",
      "[Epoch: 1000] train loss: 0.7052, train acc: 0.7077, val loss: 0.6513, val acc: 0.7551  (best train acc: 0.7077, best val acc: 0.7599, best train loss: 0.7052  @ epoch 1000 )\n",
      "[Epoch: 1020] train loss: 0.7679, train acc: 0.6691, val loss: 0.6503, val acc: 0.7332  (best train acc: 0.7077, best val acc: 0.7599, best train loss: 0.7052  @ epoch 1000 )\n",
      "[Epoch: 1040] train loss: 0.7619, train acc: 0.6700, val loss: 0.6682, val acc: 0.7025  (best train acc: 0.7185, best val acc: 0.7680, best train loss: 0.6883  @ epoch 1028 )\n",
      "[Epoch: 1060] train loss: 0.7185, train acc: 0.6916, val loss: 0.6492, val acc: 0.7501  (best train acc: 0.7185, best val acc: 0.7683, best train loss: 0.6805  @ epoch 1046 )\n",
      "[Epoch: 1080] train loss: 0.7696, train acc: 0.6739, val loss: 0.6467, val acc: 0.7437  (best train acc: 0.7185, best val acc: 0.7683, best train loss: 0.6805  @ epoch 1046 )\n",
      "[Epoch: 1100] train loss: 0.6933, train acc: 0.7108, val loss: 0.6259, val acc: 0.7700  (best train acc: 0.7204, best val acc: 0.7700, best train loss: 0.6805  @ epoch 1046 )\n",
      "[Epoch: 1120] train loss: 0.8071, train acc: 0.6643, val loss: 0.6516, val acc: 0.7059  (best train acc: 0.7204, best val acc: 0.7710, best train loss: 0.6805  @ epoch 1046 )\n",
      "[Epoch: 1140] train loss: 0.7596, train acc: 0.6880, val loss: 0.6446, val acc: 0.7204  (best train acc: 0.7204, best val acc: 0.7710, best train loss: 0.6805  @ epoch 1046 )\n",
      "[Epoch: 1160] train loss: 0.7896, train acc: 0.6675, val loss: 0.6486, val acc: 0.7089  (best train acc: 0.7204, best val acc: 0.7710, best train loss: 0.6805  @ epoch 1046 )\n",
      "[Epoch: 1180] train loss: 0.7382, train acc: 0.7055, val loss: 0.6332, val acc: 0.7481  (best train acc: 0.7204, best val acc: 0.7767, best train loss: 0.6783  @ epoch 1163 )\n",
      "[Epoch: 1200] train loss: 0.6902, train acc: 0.7105, val loss: 0.6243, val acc: 0.7585  (best train acc: 0.7303, best val acc: 0.7767, best train loss: 0.6592  @ epoch 1195 )\n",
      "[Epoch: 1220] train loss: 0.7394, train acc: 0.6883, val loss: 0.6701, val acc: 0.6951  (best train acc: 0.7303, best val acc: 0.7794, best train loss: 0.6592  @ epoch 1195 )\n",
      "[Epoch: 1240] train loss: 0.8021, train acc: 0.6583, val loss: 0.6492, val acc: 0.7089  (best train acc: 0.7303, best val acc: 0.7794, best train loss: 0.6592  @ epoch 1195 )\n",
      "[Epoch: 1260] train loss: 0.7097, train acc: 0.6982, val loss: 0.6217, val acc: 0.7450  (best train acc: 0.7314, best val acc: 0.7794, best train loss: 0.6592  @ epoch 1195 )\n",
      "[Epoch: 1280] train loss: 0.7355, train acc: 0.6768, val loss: 0.6188, val acc: 0.7336  (best train acc: 0.7314, best val acc: 0.7794, best train loss: 0.6592  @ epoch 1195 )\n",
      "[Epoch: 1300] train loss: 0.7304, train acc: 0.7005, val loss: 0.6008, val acc: 0.7666  (best train acc: 0.7386, best val acc: 0.7794, best train loss: 0.6574  @ epoch 1288 )\n",
      "[Epoch: 1320] train loss: 0.7794, train acc: 0.6679, val loss: 0.6012, val acc: 0.7528  (best train acc: 0.7386, best val acc: 0.7794, best train loss: 0.6574  @ epoch 1288 )\n",
      "[Epoch: 1340] train loss: 0.6911, train acc: 0.7123, val loss: 0.5975, val acc: 0.7680  (best train acc: 0.7386, best val acc: 0.7825, best train loss: 0.6574  @ epoch 1288 )\n",
      "[Epoch: 1360] train loss: 0.6825, train acc: 0.7167, val loss: 0.6067, val acc: 0.7619  (best train acc: 0.7386, best val acc: 0.7825, best train loss: 0.6574  @ epoch 1288 )\n",
      "[Epoch: 1380] train loss: 0.6847, train acc: 0.7126, val loss: 0.6132, val acc: 0.7737  (best train acc: 0.7386, best val acc: 0.7825, best train loss: 0.6574  @ epoch 1288 )\n",
      "[Epoch: 1400] train loss: 0.7149, train acc: 0.6990, val loss: 0.6099, val acc: 0.7528  (best train acc: 0.7386, best val acc: 0.7841, best train loss: 0.6574  @ epoch 1288 )\n",
      "[Epoch: 1420] train loss: 0.7270, train acc: 0.6987, val loss: 0.6095, val acc: 0.7379  (best train acc: 0.7386, best val acc: 0.7841, best train loss: 0.6574  @ epoch 1288 )\n",
      "[Epoch: 1440] train loss: 0.6745, train acc: 0.7218, val loss: 0.6225, val acc: 0.7278  (best train acc: 0.7386, best val acc: 0.7841, best train loss: 0.6439  @ epoch 1437 )\n",
      "[Epoch: 1460] train loss: 0.7141, train acc: 0.6961, val loss: 0.6182, val acc: 0.7191  (best train acc: 0.7386, best val acc: 0.7841, best train loss: 0.6439  @ epoch 1437 )\n",
      "[Epoch: 1480] train loss: 0.6727, train acc: 0.7037, val loss: 0.6032, val acc: 0.7477  (best train acc: 0.7386, best val acc: 0.7868, best train loss: 0.6439  @ epoch 1437 )\n",
      "[Epoch: 1500] train loss: 0.7054, train acc: 0.7057, val loss: 0.6099, val acc: 0.7356  (best train acc: 0.7386, best val acc: 0.7868, best train loss: 0.6439  @ epoch 1437 )\n",
      "[Epoch: 1520] train loss: 0.6777, train acc: 0.7188, val loss: 0.5860, val acc: 0.7649  (best train acc: 0.7386, best val acc: 0.7868, best train loss: 0.6439  @ epoch 1437 )\n",
      "[Epoch: 1540] train loss: 0.6863, train acc: 0.7059, val loss: 0.5765, val acc: 0.7740  (best train acc: 0.7454, best val acc: 0.7868, best train loss: 0.6397  @ epoch 1523 )\n",
      "[Epoch: 1560] train loss: 0.6722, train acc: 0.7281, val loss: 0.5754, val acc: 0.7781  (best train acc: 0.7454, best val acc: 0.7868, best train loss: 0.6397  @ epoch 1523 )\n",
      "[Epoch: 1580] train loss: 0.6690, train acc: 0.7310, val loss: 0.5857, val acc: 0.7804  (best train acc: 0.7454, best val acc: 0.7868, best train loss: 0.6397  @ epoch 1523 )\n",
      "[Epoch: 1600] train loss: 0.6876, train acc: 0.7151, val loss: 0.5865, val acc: 0.7538  (best train acc: 0.7454, best val acc: 0.7868, best train loss: 0.6397  @ epoch 1523 )\n",
      "[Epoch: 1620] train loss: 0.7757, train acc: 0.6640, val loss: 0.6540, val acc: 0.6793  (best train acc: 0.7479, best val acc: 0.7868, best train loss: 0.6389  @ epoch 1619 )\n",
      "[Epoch: 1640] train loss: 0.7080, train acc: 0.7191, val loss: 0.5871, val acc: 0.7683  (best train acc: 0.7479, best val acc: 0.7868, best train loss: 0.6389  @ epoch 1619 )\n",
      "[Epoch: 1660] train loss: 0.6536, train acc: 0.7289, val loss: 0.5783, val acc: 0.7676  (best train acc: 0.7479, best val acc: 0.7882, best train loss: 0.6389  @ epoch 1619 )\n",
      "[Epoch: 1680] train loss: 0.7226, train acc: 0.6955, val loss: 0.5746, val acc: 0.7565  (best train acc: 0.7479, best val acc: 0.7882, best train loss: 0.6345  @ epoch 1663 )\n",
      "[Epoch: 1700] train loss: 0.6895, train acc: 0.7222, val loss: 0.5805, val acc: 0.7524  (best train acc: 0.7479, best val acc: 0.7882, best train loss: 0.6286  @ epoch 1693 )\n",
      "[Epoch: 1720] train loss: 0.6804, train acc: 0.7110, val loss: 0.5650, val acc: 0.7717  (best train acc: 0.7479, best val acc: 0.7882, best train loss: 0.6286  @ epoch 1693 )\n",
      "[Epoch: 1740] train loss: 0.6651, train acc: 0.7217, val loss: 0.5900, val acc: 0.7420  (best train acc: 0.7543, best val acc: 0.7882, best train loss: 0.6065  @ epoch 1737 )\n",
      "[Epoch: 1760] train loss: 0.6929, train acc: 0.6961, val loss: 0.6234, val acc: 0.6971  (best train acc: 0.7543, best val acc: 0.7906, best train loss: 0.6065  @ epoch 1737 )\n",
      "[Epoch: 1780] train loss: 0.6481, train acc: 0.7247, val loss: 0.5661, val acc: 0.7730  (best train acc: 0.7543, best val acc: 0.7916, best train loss: 0.6065  @ epoch 1737 )\n",
      "[Epoch: 1800] train loss: 0.6563, train acc: 0.7387, val loss: 0.5973, val acc: 0.7349  (best train acc: 0.7543, best val acc: 0.7916, best train loss: 0.6065  @ epoch 1737 )\n",
      "[Epoch: 1820] train loss: 0.6298, train acc: 0.7512, val loss: 0.5702, val acc: 0.7801  (best train acc: 0.7543, best val acc: 0.7916, best train loss: 0.6065  @ epoch 1737 )\n",
      "[Epoch: 1840] train loss: 0.6819, train acc: 0.7178, val loss: 0.6174, val acc: 0.7153  (best train acc: 0.7543, best val acc: 0.7916, best train loss: 0.6065  @ epoch 1737 )\n",
      "[Epoch: 1860] train loss: 0.6647, train acc: 0.7295, val loss: 0.5634, val acc: 0.7767  (best train acc: 0.7543, best val acc: 0.7916, best train loss: 0.6065  @ epoch 1737 )\n",
      "[Epoch: 1880] train loss: 0.6673, train acc: 0.7178, val loss: 0.5571, val acc: 0.7686  (best train acc: 0.7543, best val acc: 0.7916, best train loss: 0.6065  @ epoch 1737 )\n",
      "[Epoch: 1900] train loss: 0.7745, train acc: 0.6683, val loss: 0.6493, val acc: 0.6698  (best train acc: 0.7543, best val acc: 0.7916, best train loss: 0.6065  @ epoch 1737 )\n",
      "[Epoch: 1920] train loss: 0.6532, train acc: 0.7293, val loss: 0.5545, val acc: 0.7693  (best train acc: 0.7543, best val acc: 0.7916, best train loss: 0.6065  @ epoch 1737 )\n",
      "[Epoch: 1940] train loss: 0.6741, train acc: 0.7303, val loss: 0.5500, val acc: 0.7747  (best train acc: 0.7543, best val acc: 0.7916, best train loss: 0.6065  @ epoch 1737 )\n",
      "[Epoch: 1960] train loss: 0.6726, train acc: 0.7122, val loss: 0.5875, val acc: 0.7214  (best train acc: 0.7543, best val acc: 0.7916, best train loss: 0.6010  @ epoch 1953 )\n",
      "[Epoch: 1980] train loss: 0.7458, train acc: 0.6832, val loss: 0.5469, val acc: 0.7609  (best train acc: 0.7543, best val acc: 0.7943, best train loss: 0.6010  @ epoch 1953 )\n",
      "[Epoch: 2000] train loss: 0.6363, train acc: 0.7456, val loss: 0.5336, val acc: 0.7815  (best train acc: 0.7543, best val acc: 0.7943, best train loss: 0.6010  @ epoch 1953 )\n",
      "[Epoch: 2020] train loss: 0.6260, train acc: 0.7433, val loss: 0.5737, val acc: 0.7191  (best train acc: 0.7543, best val acc: 0.7970, best train loss: 0.6010  @ epoch 1953 )\n",
      "[Epoch: 2040] train loss: 0.6349, train acc: 0.7300, val loss: 0.5267, val acc: 0.7831  (best train acc: 0.7609, best val acc: 0.7970, best train loss: 0.6010  @ epoch 1953 )\n",
      "[Epoch: 2060] train loss: 0.6945, train acc: 0.7012, val loss: 0.5271, val acc: 0.7808  (best train acc: 0.7609, best val acc: 0.7970, best train loss: 0.5907  @ epoch 2045 )\n",
      "[Epoch: 2080] train loss: 0.6094, train acc: 0.7394, val loss: 0.5190, val acc: 0.7841  (best train acc: 0.7609, best val acc: 0.7970, best train loss: 0.5907  @ epoch 2045 )\n",
      "[Epoch: 2100] train loss: 0.6103, train acc: 0.7349, val loss: 0.5374, val acc: 0.7946  (best train acc: 0.7609, best val acc: 0.8010, best train loss: 0.5884  @ epoch 2095 )\n",
      "[Epoch: 2120] train loss: 0.6217, train acc: 0.7422, val loss: 0.5599, val acc: 0.7464  (best train acc: 0.7668, best val acc: 0.8010, best train loss: 0.5884  @ epoch 2095 )\n",
      "[Epoch: 2140] train loss: 0.6321, train acc: 0.7356, val loss: 0.5237, val acc: 0.8024  (best train acc: 0.7668, best val acc: 0.8024, best train loss: 0.5874  @ epoch 2126 )\n",
      "[Epoch: 2160] train loss: 0.6201, train acc: 0.7371, val loss: 0.5104, val acc: 0.7872  (best train acc: 0.7668, best val acc: 0.8024, best train loss: 0.5874  @ epoch 2126 )\n",
      "[Epoch: 2180] train loss: 0.5944, train acc: 0.7501, val loss: 0.5176, val acc: 0.7953  (best train acc: 0.7696, best val acc: 0.8051, best train loss: 0.5765  @ epoch 2170 )\n",
      "[Epoch: 2200] train loss: 0.6123, train acc: 0.7502, val loss: 0.5246, val acc: 0.7956  (best train acc: 0.7696, best val acc: 0.8108, best train loss: 0.5765  @ epoch 2170 )\n",
      "[Epoch: 2220] train loss: 0.6619, train acc: 0.7263, val loss: 0.5509, val acc: 0.7406  (best train acc: 0.7696, best val acc: 0.8108, best train loss: 0.5765  @ epoch 2170 )\n",
      "[Epoch: 2240] train loss: 0.6670, train acc: 0.7212, val loss: 0.5116, val acc: 0.7825  (best train acc: 0.7716, best val acc: 0.8108, best train loss: 0.5765  @ epoch 2231 )\n",
      "[Epoch: 2260] train loss: 0.6328, train acc: 0.7406, val loss: 0.5171, val acc: 0.7912  (best train acc: 0.7716, best val acc: 0.8108, best train loss: 0.5731  @ epoch 2241 )\n",
      "[Epoch: 2280] train loss: 0.6715, train acc: 0.7071, val loss: 0.5182, val acc: 0.7690  (best train acc: 0.7716, best val acc: 0.8108, best train loss: 0.5731  @ epoch 2241 )\n",
      "[Epoch: 2300] train loss: 0.6555, train acc: 0.7341, val loss: 0.5295, val acc: 0.7562  (best train acc: 0.7716, best val acc: 0.8108, best train loss: 0.5731  @ epoch 2241 )\n",
      "[Epoch: 2320] train loss: 0.6343, train acc: 0.7313, val loss: 0.5075, val acc: 0.7902  (best train acc: 0.7716, best val acc: 0.8108, best train loss: 0.5731  @ epoch 2241 )\n",
      "[Epoch: 2340] train loss: 0.6049, train acc: 0.7436, val loss: 0.5000, val acc: 0.7953  (best train acc: 0.7716, best val acc: 0.8108, best train loss: 0.5731  @ epoch 2241 )\n",
      "[Epoch: 2360] train loss: 0.6850, train acc: 0.7111, val loss: 0.5194, val acc: 0.7666  (best train acc: 0.7716, best val acc: 0.8108, best train loss: 0.5731  @ epoch 2241 )\n",
      "[Epoch: 2380] train loss: 0.6437, train acc: 0.7278, val loss: 0.5152, val acc: 0.7794  (best train acc: 0.7716, best val acc: 0.8108, best train loss: 0.5731  @ epoch 2241 )\n",
      "[Epoch: 2400] train loss: 0.6099, train acc: 0.7487, val loss: 0.5063, val acc: 0.8044  (best train acc: 0.7716, best val acc: 0.8108, best train loss: 0.5731  @ epoch 2241 )\n",
      "[Epoch: 2420] train loss: 0.6073, train acc: 0.7509, val loss: 0.5077, val acc: 0.7980  (best train acc: 0.7716, best val acc: 0.8108, best train loss: 0.5709  @ epoch 2412 )\n",
      "[Epoch: 2440] train loss: 0.6215, train acc: 0.7421, val loss: 0.5239, val acc: 0.7673  (best train acc: 0.7716, best val acc: 0.8108, best train loss: 0.5709  @ epoch 2412 )\n",
      "[Epoch: 2460] train loss: 0.6295, train acc: 0.7341, val loss: 0.5279, val acc: 0.7740  (best train acc: 0.7716, best val acc: 0.8108, best train loss: 0.5709  @ epoch 2412 )\n",
      "[Epoch: 2480] train loss: 0.6059, train acc: 0.7522, val loss: 0.4935, val acc: 0.7875  (best train acc: 0.7716, best val acc: 0.8108, best train loss: 0.5709  @ epoch 2412 )\n",
      "[Epoch: 2500] train loss: 0.5938, train acc: 0.7482, val loss: 0.4943, val acc: 0.7885  (best train acc: 0.7716, best val acc: 0.8108, best train loss: 0.5518  @ epoch 2481 )\n",
      "[Epoch: 2520] train loss: 0.6173, train acc: 0.7493, val loss: 0.5247, val acc: 0.7572  (best train acc: 0.7729, best val acc: 0.8108, best train loss: 0.5514  @ epoch 2519 )\n",
      "[Epoch: 2540] train loss: 0.5754, train acc: 0.7647, val loss: 0.5037, val acc: 0.7818  (best train acc: 0.7729, best val acc: 0.8108, best train loss: 0.5514  @ epoch 2519 )\n",
      "[Epoch: 2560] train loss: 0.6239, train acc: 0.7350, val loss: 0.5118, val acc: 0.7646  (best train acc: 0.7729, best val acc: 0.8108, best train loss: 0.5514  @ epoch 2519 )\n",
      "[Epoch: 2580] train loss: 0.6115, train acc: 0.7434, val loss: 0.4967, val acc: 0.7862  (best train acc: 0.7729, best val acc: 0.8108, best train loss: 0.5514  @ epoch 2519 )\n",
      "[Epoch: 2600] train loss: 0.6049, train acc: 0.7504, val loss: 0.5072, val acc: 0.7713  (best train acc: 0.7786, best val acc: 0.8108, best train loss: 0.5399  @ epoch 2596 )\n",
      "[Epoch: 2620] train loss: 0.6157, train acc: 0.7415, val loss: 0.4797, val acc: 0.7976  (best train acc: 0.7786, best val acc: 0.8108, best train loss: 0.5399  @ epoch 2596 )\n",
      "[Epoch: 2640] train loss: 0.6057, train acc: 0.7475, val loss: 0.4788, val acc: 0.8013  (best train acc: 0.7793, best val acc: 0.8108, best train loss: 0.5399  @ epoch 2596 )\n",
      "[Epoch: 2660] train loss: 0.5942, train acc: 0.7627, val loss: 0.5117, val acc: 0.7764  (best train acc: 0.7793, best val acc: 0.8108, best train loss: 0.5399  @ epoch 2596 )\n",
      "[Epoch: 2680] train loss: 0.5595, train acc: 0.7612, val loss: 0.4810, val acc: 0.7953  (best train acc: 0.7793, best val acc: 0.8108, best train loss: 0.5399  @ epoch 2596 )\n",
      "[Epoch: 2700] train loss: 0.5740, train acc: 0.7599, val loss: 0.4886, val acc: 0.7963  (best train acc: 0.7793, best val acc: 0.8108, best train loss: 0.5399  @ epoch 2596 )\n",
      "[Epoch: 2720] train loss: 0.5669, train acc: 0.7606, val loss: 0.4823, val acc: 0.7946  (best train acc: 0.7793, best val acc: 0.8108, best train loss: 0.5399  @ epoch 2596 )\n",
      "[Epoch: 2740] train loss: 0.5826, train acc: 0.7493, val loss: 0.5110, val acc: 0.7717  (best train acc: 0.7793, best val acc: 0.8108, best train loss: 0.5399  @ epoch 2596 )\n",
      "[Epoch: 2760] train loss: 0.5450, train acc: 0.7797, val loss: 0.4872, val acc: 0.8054  (best train acc: 0.7797, best val acc: 0.8108, best train loss: 0.5399  @ epoch 2596 )\n",
      "[Epoch: 2780] train loss: 0.5692, train acc: 0.7637, val loss: 0.5019, val acc: 0.7811  (best train acc: 0.7797, best val acc: 0.8108, best train loss: 0.5399  @ epoch 2596 )\n",
      "[Epoch: 2800] train loss: 0.5723, train acc: 0.7615, val loss: 0.4870, val acc: 0.7858  (best train acc: 0.7797, best val acc: 0.8108, best train loss: 0.5399  @ epoch 2596 )\n",
      "[Epoch: 2820] train loss: 0.5878, train acc: 0.7535, val loss: 0.4777, val acc: 0.7970  (best train acc: 0.7801, best val acc: 0.8108, best train loss: 0.5399  @ epoch 2596 )\n",
      "[Epoch: 2840] train loss: 0.6279, train acc: 0.7418, val loss: 0.4959, val acc: 0.7804  (best train acc: 0.7801, best val acc: 0.8108, best train loss: 0.5399  @ epoch 2596 )\n",
      "[Epoch: 2860] train loss: 0.5761, train acc: 0.7620, val loss: 0.4815, val acc: 0.7922  (best train acc: 0.7801, best val acc: 0.8108, best train loss: 0.5399  @ epoch 2596 )\n",
      "[Epoch: 2880] train loss: 0.5528, train acc: 0.7737, val loss: 0.4949, val acc: 0.7781  (best train acc: 0.7801, best val acc: 0.8108, best train loss: 0.5399  @ epoch 2596 )\n",
      "[Epoch: 2900] train loss: 0.5917, train acc: 0.7556, val loss: 0.4996, val acc: 0.7717  (best train acc: 0.7801, best val acc: 0.8108, best train loss: 0.5399  @ epoch 2596 )\n",
      "[Epoch: 2920] train loss: 0.5831, train acc: 0.7487, val loss: 0.4890, val acc: 0.7862  (best train acc: 0.7801, best val acc: 0.8108, best train loss: 0.5399  @ epoch 2596 )\n",
      "[Epoch: 2940] train loss: 0.5819, train acc: 0.7574, val loss: 0.4774, val acc: 0.8108  (best train acc: 0.7801, best val acc: 0.8138, best train loss: 0.5399  @ epoch 2596 )\n",
      "[Epoch: 2960] train loss: 0.5652, train acc: 0.7567, val loss: 0.4788, val acc: 0.7899  (best train acc: 0.7801, best val acc: 0.8138, best train loss: 0.5399  @ epoch 2596 )\n",
      "[Epoch: 2980] train loss: 0.5726, train acc: 0.7612, val loss: 0.4613, val acc: 0.8000  (best train acc: 0.7801, best val acc: 0.8138, best train loss: 0.5399  @ epoch 2596 )\n",
      "[Epoch: 3000] train loss: 0.5914, train acc: 0.7541, val loss: 0.4685, val acc: 0.7943  (best train acc: 0.7804, best val acc: 0.8138, best train loss: 0.5399  @ epoch 2596 )\n",
      "[Epoch: 3020] train loss: 0.5968, train acc: 0.7480, val loss: 0.4647, val acc: 0.8108  (best train acc: 0.7855, best val acc: 0.8138, best train loss: 0.5370  @ epoch 3005 )\n",
      "[Epoch: 3040] train loss: 0.6297, train acc: 0.7274, val loss: 0.4937, val acc: 0.7811  (best train acc: 0.7855, best val acc: 0.8138, best train loss: 0.5370  @ epoch 3005 )\n",
      "[Epoch: 3060] train loss: 0.5600, train acc: 0.7765, val loss: 0.4663, val acc: 0.7997  (best train acc: 0.7855, best val acc: 0.8172, best train loss: 0.5360  @ epoch 3053 )\n",
      "[Epoch: 3080] train loss: 0.5328, train acc: 0.7772, val loss: 0.4715, val acc: 0.8135  (best train acc: 0.7855, best val acc: 0.8172, best train loss: 0.5328  @ epoch 3080 )\n",
      "[Epoch: 3100] train loss: 0.5544, train acc: 0.7671, val loss: 0.4774, val acc: 0.7943  (best train acc: 0.7855, best val acc: 0.8172, best train loss: 0.5244  @ epoch 3088 )\n",
      "[Epoch: 3120] train loss: 0.6068, train acc: 0.7377, val loss: 0.4696, val acc: 0.8064  (best train acc: 0.7855, best val acc: 0.8172, best train loss: 0.5244  @ epoch 3088 )\n",
      "[Epoch: 3140] train loss: 0.6684, train acc: 0.7139, val loss: 0.4927, val acc: 0.7754  (best train acc: 0.7855, best val acc: 0.8172, best train loss: 0.5206  @ epoch 3136 )\n",
      "[Epoch: 3160] train loss: 0.5837, train acc: 0.7563, val loss: 0.5129, val acc: 0.7578  (best train acc: 0.7855, best val acc: 0.8172, best train loss: 0.5206  @ epoch 3136 )\n",
      "[Epoch: 3180] train loss: 0.5204, train acc: 0.7903, val loss: 0.4714, val acc: 0.7966  (best train acc: 0.7903, best val acc: 0.8175, best train loss: 0.5204  @ epoch 3180 )\n",
      "[Epoch: 3200] train loss: 0.5342, train acc: 0.7749, val loss: 0.5214, val acc: 0.7555  (best train acc: 0.7903, best val acc: 0.8175, best train loss: 0.5204  @ epoch 3180 )\n",
      "[Epoch: 3220] train loss: 0.5909, train acc: 0.7402, val loss: 0.4752, val acc: 0.7926  (best train acc: 0.7903, best val acc: 0.8202, best train loss: 0.5204  @ epoch 3180 )\n",
      "[Epoch: 3240] train loss: 0.5699, train acc: 0.7563, val loss: 0.4734, val acc: 0.7899  (best train acc: 0.7903, best val acc: 0.8202, best train loss: 0.5204  @ epoch 3180 )\n",
      "[Epoch: 3260] train loss: 0.5399, train acc: 0.7689, val loss: 0.4620, val acc: 0.8125  (best train acc: 0.7903, best val acc: 0.8202, best train loss: 0.5204  @ epoch 3180 )\n",
      "[Epoch: 3280] train loss: 0.5789, train acc: 0.7580, val loss: 0.4873, val acc: 0.7804  (best train acc: 0.7903, best val acc: 0.8202, best train loss: 0.5115  @ epoch 3270 )\n",
      "[Epoch: 3300] train loss: 0.5251, train acc: 0.7832, val loss: 0.4763, val acc: 0.7953  (best train acc: 0.7903, best val acc: 0.8202, best train loss: 0.5115  @ epoch 3270 )\n",
      "[Epoch: 3320] train loss: 0.5694, train acc: 0.7622, val loss: 0.4505, val acc: 0.8105  (best train acc: 0.7903, best val acc: 0.8202, best train loss: 0.5115  @ epoch 3270 )\n",
      "[Epoch: 3340] train loss: 0.5625, train acc: 0.7632, val loss: 0.4480, val acc: 0.8152  (best train acc: 0.7903, best val acc: 0.8202, best train loss: 0.5115  @ epoch 3270 )\n",
      "[Epoch: 3360] train loss: 0.5865, train acc: 0.7397, val loss: 0.5025, val acc: 0.7680  (best train acc: 0.7903, best val acc: 0.8202, best train loss: 0.5115  @ epoch 3270 )\n",
      "[Epoch: 3380] train loss: 0.5974, train acc: 0.7415, val loss: 0.4509, val acc: 0.8081  (best train acc: 0.7903, best val acc: 0.8202, best train loss: 0.5115  @ epoch 3270 )\n",
      "[Epoch: 3400] train loss: 0.5541, train acc: 0.7714, val loss: 0.4633, val acc: 0.8209  (best train acc: 0.7903, best val acc: 0.8236, best train loss: 0.5115  @ epoch 3270 )\n",
      "[Epoch: 3420] train loss: 0.5360, train acc: 0.7766, val loss: 0.4535, val acc: 0.8064  (best train acc: 0.7903, best val acc: 0.8236, best train loss: 0.5115  @ epoch 3270 )\n",
      "[Epoch: 3440] train loss: 0.5686, train acc: 0.7621, val loss: 0.4573, val acc: 0.8051  (best train acc: 0.7903, best val acc: 0.8253, best train loss: 0.5115  @ epoch 3270 )\n",
      "[Epoch: 3460] train loss: 0.5749, train acc: 0.7568, val loss: 0.4671, val acc: 0.7993  (best train acc: 0.7903, best val acc: 0.8266, best train loss: 0.5115  @ epoch 3270 )\n",
      "[Epoch: 3480] train loss: 0.5863, train acc: 0.7539, val loss: 0.4645, val acc: 0.7987  (best train acc: 0.7976, best val acc: 0.8266, best train loss: 0.5052  @ epoch 3461 )\n",
      "[Epoch: 3500] train loss: 0.5659, train acc: 0.7735, val loss: 0.4533, val acc: 0.8182  (best train acc: 0.7976, best val acc: 0.8266, best train loss: 0.5052  @ epoch 3461 )\n",
      "[Epoch: 3520] train loss: 0.5304, train acc: 0.7741, val loss: 0.4660, val acc: 0.7929  (best train acc: 0.7976, best val acc: 0.8266, best train loss: 0.5052  @ epoch 3461 )\n",
      "[Epoch: 3540] train loss: 0.5195, train acc: 0.7867, val loss: 0.4481, val acc: 0.8216  (best train acc: 0.7976, best val acc: 0.8266, best train loss: 0.5052  @ epoch 3461 )\n",
      "[Epoch: 3560] train loss: 0.5378, train acc: 0.7820, val loss: 0.4672, val acc: 0.8040  (best train acc: 0.7976, best val acc: 0.8266, best train loss: 0.5052  @ epoch 3461 )\n",
      "[Epoch: 3580] train loss: 0.5715, train acc: 0.7615, val loss: 0.4472, val acc: 0.8175  (best train acc: 0.7976, best val acc: 0.8270, best train loss: 0.5052  @ epoch 3461 )\n",
      "[Epoch: 3600] train loss: 0.5169, train acc: 0.7789, val loss: 0.4418, val acc: 0.8138  (best train acc: 0.7976, best val acc: 0.8270, best train loss: 0.5052  @ epoch 3461 )\n",
      "[Epoch: 3620] train loss: 0.5311, train acc: 0.7707, val loss: 0.4542, val acc: 0.8027  (best train acc: 0.7976, best val acc: 0.8270, best train loss: 0.5052  @ epoch 3461 )\n",
      "[Epoch: 3640] train loss: 0.5379, train acc: 0.7774, val loss: 0.4734, val acc: 0.7926  (best train acc: 0.7976, best val acc: 0.8270, best train loss: 0.5052  @ epoch 3461 )\n",
      "[Epoch: 3660] train loss: 0.5134, train acc: 0.7874, val loss: 0.4572, val acc: 0.8125  (best train acc: 0.7976, best val acc: 0.8270, best train loss: 0.5052  @ epoch 3461 )\n",
      "[Epoch: 3680] train loss: 0.5700, train acc: 0.7600, val loss: 0.4363, val acc: 0.8165  (best train acc: 0.7976, best val acc: 0.8270, best train loss: 0.5052  @ epoch 3461 )\n",
      "[Epoch: 3700] train loss: 0.5410, train acc: 0.7782, val loss: 0.4432, val acc: 0.8185  (best train acc: 0.7976, best val acc: 0.8270, best train loss: 0.5036  @ epoch 3681 )\n",
      "[Epoch: 3720] train loss: 0.5870, train acc: 0.7538, val loss: 0.4540, val acc: 0.8003  (best train acc: 0.7976, best val acc: 0.8270, best train loss: 0.5036  @ epoch 3681 )\n",
      "[Epoch: 3740] train loss: 0.5693, train acc: 0.7571, val loss: 0.4502, val acc: 0.8071  (best train acc: 0.7976, best val acc: 0.8270, best train loss: 0.5036  @ epoch 3681 )\n",
      "[Epoch: 3760] train loss: 0.5442, train acc: 0.7632, val loss: 0.4685, val acc: 0.7909  (best train acc: 0.7984, best val acc: 0.8270, best train loss: 0.4905  @ epoch 3756 )\n",
      "[Epoch: 3780] train loss: 0.5311, train acc: 0.7846, val loss: 0.4476, val acc: 0.8108  (best train acc: 0.7984, best val acc: 0.8270, best train loss: 0.4905  @ epoch 3756 )\n",
      "[Epoch: 3800] train loss: 0.5110, train acc: 0.7884, val loss: 0.4576, val acc: 0.7943  (best train acc: 0.8004, best val acc: 0.8270, best train loss: 0.4905  @ epoch 3756 )\n",
      "[Epoch: 3820] train loss: 0.5224, train acc: 0.7817, val loss: 0.4409, val acc: 0.8152  (best train acc: 0.8004, best val acc: 0.8270, best train loss: 0.4905  @ epoch 3756 )\n",
      "[Epoch: 3840] train loss: 0.5567, train acc: 0.7629, val loss: 0.4576, val acc: 0.7933  (best train acc: 0.8004, best val acc: 0.8277, best train loss: 0.4905  @ epoch 3756 )\n",
      "[Epoch: 3860] train loss: 0.5386, train acc: 0.7624, val loss: 0.4444, val acc: 0.8169  (best train acc: 0.8004, best val acc: 0.8277, best train loss: 0.4905  @ epoch 3756 )\n",
      "[Epoch: 3880] train loss: 0.5502, train acc: 0.7615, val loss: 0.4684, val acc: 0.7902  (best train acc: 0.8004, best val acc: 0.8283, best train loss: 0.4905  @ epoch 3756 )\n",
      "[Epoch: 3900] train loss: 0.5317, train acc: 0.7787, val loss: 0.4687, val acc: 0.7946  (best train acc: 0.8004, best val acc: 0.8283, best train loss: 0.4905  @ epoch 3756 )\n",
      "[Epoch: 3920] train loss: 0.5003, train acc: 0.7920, val loss: 0.4388, val acc: 0.8229  (best train acc: 0.8004, best val acc: 0.8293, best train loss: 0.4905  @ epoch 3756 )\n",
      "[Epoch: 3940] train loss: 0.5328, train acc: 0.7715, val loss: 0.4325, val acc: 0.8266  (best train acc: 0.8004, best val acc: 0.8293, best train loss: 0.4905  @ epoch 3756 )\n",
      "[Epoch: 3960] train loss: 0.5394, train acc: 0.7726, val loss: 0.4477, val acc: 0.8223  (best train acc: 0.8004, best val acc: 0.8293, best train loss: 0.4905  @ epoch 3756 )\n",
      "[Epoch: 3980] train loss: 0.5859, train acc: 0.7452, val loss: 0.4423, val acc: 0.8121  (best train acc: 0.8004, best val acc: 0.8293, best train loss: 0.4905  @ epoch 3756 )\n",
      "[Epoch: 4000] train loss: 0.5829, train acc: 0.7498, val loss: 0.4885, val acc: 0.7767  (best train acc: 0.8004, best val acc: 0.8293, best train loss: 0.4905  @ epoch 3756 )\n",
      "[Epoch: 4020] train loss: 0.6084, train acc: 0.7419, val loss: 0.4435, val acc: 0.8209  (best train acc: 0.8004, best val acc: 0.8300, best train loss: 0.4905  @ epoch 3756 )\n",
      "[Epoch: 4040] train loss: 0.5651, train acc: 0.7489, val loss: 0.4807, val acc: 0.7899  (best train acc: 0.8004, best val acc: 0.8300, best train loss: 0.4905  @ epoch 3756 )\n",
      "[Epoch: 4060] train loss: 0.5849, train acc: 0.7470, val loss: 0.4467, val acc: 0.8287  (best train acc: 0.8004, best val acc: 0.8300, best train loss: 0.4905  @ epoch 3756 )\n",
      "[Epoch: 4080] train loss: 0.5547, train acc: 0.7530, val loss: 0.4430, val acc: 0.8064  (best train acc: 0.8004, best val acc: 0.8300, best train loss: 0.4905  @ epoch 3756 )\n",
      "[Epoch: 4100] train loss: 0.5427, train acc: 0.7792, val loss: 0.4439, val acc: 0.8236  (best train acc: 0.8004, best val acc: 0.8304, best train loss: 0.4905  @ epoch 3756 )\n",
      "[Epoch: 4120] train loss: 0.5351, train acc: 0.7684, val loss: 0.4659, val acc: 0.7895  (best train acc: 0.8004, best val acc: 0.8344, best train loss: 0.4905  @ epoch 3756 )\n",
      "[Epoch: 4140] train loss: 0.5225, train acc: 0.7780, val loss: 0.4640, val acc: 0.7980  (best train acc: 0.8016, best val acc: 0.8344, best train loss: 0.4817  @ epoch 4139 )\n",
      "[Epoch: 4160] train loss: 0.5492, train acc: 0.7760, val loss: 0.4307, val acc: 0.8121  (best train acc: 0.8035, best val acc: 0.8344, best train loss: 0.4817  @ epoch 4139 )\n",
      "[Epoch: 4180] train loss: 0.5401, train acc: 0.7766, val loss: 0.4935, val acc: 0.7727  (best train acc: 0.8035, best val acc: 0.8344, best train loss: 0.4817  @ epoch 4139 )\n",
      "[Epoch: 4200] train loss: 0.4938, train acc: 0.7979, val loss: 0.4330, val acc: 0.8152  (best train acc: 0.8035, best val acc: 0.8344, best train loss: 0.4817  @ epoch 4139 )\n",
      "[Epoch: 4220] train loss: 0.4780, train acc: 0.8005, val loss: 0.4219, val acc: 0.8216  (best train acc: 0.8035, best val acc: 0.8344, best train loss: 0.4780  @ epoch 4220 )\n",
      "[Epoch: 4240] train loss: 0.5104, train acc: 0.7800, val loss: 0.4260, val acc: 0.8206  (best train acc: 0.8044, best val acc: 0.8344, best train loss: 0.4780  @ epoch 4220 )\n",
      "[Epoch: 4260] train loss: 0.4902, train acc: 0.7974, val loss: 0.4219, val acc: 0.8256  (best train acc: 0.8044, best val acc: 0.8344, best train loss: 0.4763  @ epoch 4248 )\n",
      "[Epoch: 4280] train loss: 0.4995, train acc: 0.7891, val loss: 0.4244, val acc: 0.8206  (best train acc: 0.8044, best val acc: 0.8344, best train loss: 0.4763  @ epoch 4248 )\n",
      "[Epoch: 4300] train loss: 0.5342, train acc: 0.7815, val loss: 0.4522, val acc: 0.8013  (best train acc: 0.8044, best val acc: 0.8344, best train loss: 0.4763  @ epoch 4248 )\n",
      "[Epoch: 4320] train loss: 0.5108, train acc: 0.7896, val loss: 0.4449, val acc: 0.7970  (best train acc: 0.8105, best val acc: 0.8344, best train loss: 0.4732  @ epoch 4304 )\n",
      "[Epoch: 4340] train loss: 0.5302, train acc: 0.7782, val loss: 0.4975, val acc: 0.7619  (best train acc: 0.8105, best val acc: 0.8344, best train loss: 0.4705  @ epoch 4327 )\n",
      "[Epoch: 4360] train loss: 0.5334, train acc: 0.7674, val loss: 0.4311, val acc: 0.8196  (best train acc: 0.8105, best val acc: 0.8344, best train loss: 0.4705  @ epoch 4327 )\n",
      "[Epoch: 4380] train loss: 0.4819, train acc: 0.8002, val loss: 0.4200, val acc: 0.8277  (best train acc: 0.8105, best val acc: 0.8344, best train loss: 0.4648  @ epoch 4368 )\n",
      "[Epoch: 4400] train loss: 0.5427, train acc: 0.7710, val loss: 0.4586, val acc: 0.7906  (best train acc: 0.8177, best val acc: 0.8344, best train loss: 0.4636  @ epoch 4381 )\n",
      "[Epoch: 4420] train loss: 0.5838, train acc: 0.7556, val loss: 0.4517, val acc: 0.8000  (best train acc: 0.8177, best val acc: 0.8344, best train loss: 0.4636  @ epoch 4381 )\n",
      "[Epoch: 4440] train loss: 0.4876, train acc: 0.7952, val loss: 0.4186, val acc: 0.8270  (best train acc: 0.8177, best val acc: 0.8344, best train loss: 0.4589  @ epoch 4439 )\n",
      "[Epoch: 4460] train loss: 0.5335, train acc: 0.7784, val loss: 0.4620, val acc: 0.7804  (best train acc: 0.8177, best val acc: 0.8344, best train loss: 0.4589  @ epoch 4439 )\n",
      "[Epoch: 4480] train loss: 0.5043, train acc: 0.7864, val loss: 0.4949, val acc: 0.7599  (best train acc: 0.8177, best val acc: 0.8344, best train loss: 0.4589  @ epoch 4439 )\n",
      "[Epoch: 4500] train loss: 0.4902, train acc: 0.7854, val loss: 0.4260, val acc: 0.8179  (best train acc: 0.8177, best val acc: 0.8354, best train loss: 0.4589  @ epoch 4439 )\n",
      "[Epoch: 4520] train loss: 0.4962, train acc: 0.7914, val loss: 0.4800, val acc: 0.7771  (best train acc: 0.8177, best val acc: 0.8354, best train loss: 0.4589  @ epoch 4439 )\n",
      "[Epoch: 4540] train loss: 0.4811, train acc: 0.8010, val loss: 0.4202, val acc: 0.8324  (best train acc: 0.8177, best val acc: 0.8354, best train loss: 0.4589  @ epoch 4439 )\n",
      "[Epoch: 4560] train loss: 0.4851, train acc: 0.8004, val loss: 0.4292, val acc: 0.8216  (best train acc: 0.8177, best val acc: 0.8354, best train loss: 0.4589  @ epoch 4439 )\n",
      "[Epoch: 4580] train loss: 0.4760, train acc: 0.7938, val loss: 0.4504, val acc: 0.7960  (best train acc: 0.8177, best val acc: 0.8354, best train loss: 0.4589  @ epoch 4439 )\n",
      "[Epoch: 4600] train loss: 0.4885, train acc: 0.7934, val loss: 0.4118, val acc: 0.8277  (best train acc: 0.8177, best val acc: 0.8354, best train loss: 0.4589  @ epoch 4439 )\n",
      "[Epoch: 4620] train loss: 0.4938, train acc: 0.7976, val loss: 0.4166, val acc: 0.8290  (best train acc: 0.8177, best val acc: 0.8354, best train loss: 0.4578  @ epoch 4601 )\n",
      "[Epoch: 4640] train loss: 0.4941, train acc: 0.7888, val loss: 0.4194, val acc: 0.8219  (best train acc: 0.8177, best val acc: 0.8354, best train loss: 0.4578  @ epoch 4601 )\n",
      "[Epoch: 4660] train loss: 0.5158, train acc: 0.7872, val loss: 0.4311, val acc: 0.8121  (best train acc: 0.8177, best val acc: 0.8354, best train loss: 0.4578  @ epoch 4601 )\n",
      "[Epoch: 4680] train loss: 0.4893, train acc: 0.8003, val loss: 0.4124, val acc: 0.8283  (best train acc: 0.8177, best val acc: 0.8354, best train loss: 0.4578  @ epoch 4601 )\n",
      "[Epoch: 4700] train loss: 0.5284, train acc: 0.7703, val loss: 0.4493, val acc: 0.7960  (best train acc: 0.8177, best val acc: 0.8354, best train loss: 0.4578  @ epoch 4601 )\n",
      "[Epoch: 4720] train loss: 0.5102, train acc: 0.7891, val loss: 0.4266, val acc: 0.8108  (best train acc: 0.8177, best val acc: 0.8354, best train loss: 0.4578  @ epoch 4601 )\n",
      "[Epoch: 4740] train loss: 0.5258, train acc: 0.7762, val loss: 0.4165, val acc: 0.8253  (best train acc: 0.8177, best val acc: 0.8358, best train loss: 0.4578  @ epoch 4601 )\n",
      "[Epoch: 4760] train loss: 0.4750, train acc: 0.7956, val loss: 0.4141, val acc: 0.8270  (best train acc: 0.8177, best val acc: 0.8358, best train loss: 0.4578  @ epoch 4601 )\n",
      "[Epoch: 4780] train loss: 0.4986, train acc: 0.7932, val loss: 0.4118, val acc: 0.8226  (best train acc: 0.8177, best val acc: 0.8358, best train loss: 0.4501  @ epoch 4762 )\n",
      "[Epoch: 4800] train loss: 0.5078, train acc: 0.7866, val loss: 0.4376, val acc: 0.8057  (best train acc: 0.8177, best val acc: 0.8358, best train loss: 0.4486  @ epoch 4783 )\n",
      "[Epoch: 4820] train loss: 0.4865, train acc: 0.8058, val loss: 0.4123, val acc: 0.8331  (best train acc: 0.8177, best val acc: 0.8358, best train loss: 0.4486  @ epoch 4783 )\n",
      "[Epoch: 4840] train loss: 0.5110, train acc: 0.7806, val loss: 0.4573, val acc: 0.7956  (best train acc: 0.8177, best val acc: 0.8358, best train loss: 0.4486  @ epoch 4783 )\n",
      "[Epoch: 4860] train loss: 0.4832, train acc: 0.8011, val loss: 0.4150, val acc: 0.8226  (best train acc: 0.8177, best val acc: 0.8358, best train loss: 0.4486  @ epoch 4783 )\n",
      "[Epoch: 4880] train loss: 0.5208, train acc: 0.7815, val loss: 0.4271, val acc: 0.8145  (best train acc: 0.8232, best val acc: 0.8358, best train loss: 0.4389  @ epoch 4878 )\n",
      "[Epoch: 4900] train loss: 0.5025, train acc: 0.7843, val loss: 0.4426, val acc: 0.8094  (best train acc: 0.8232, best val acc: 0.8358, best train loss: 0.4389  @ epoch 4878 )\n",
      "[Epoch: 4920] train loss: 0.5086, train acc: 0.7885, val loss: 0.4326, val acc: 0.8057  (best train acc: 0.8232, best val acc: 0.8358, best train loss: 0.4389  @ epoch 4878 )\n",
      "[Epoch: 4940] train loss: 0.4500, train acc: 0.8100, val loss: 0.4022, val acc: 0.8280  (best train acc: 0.8232, best val acc: 0.8358, best train loss: 0.4389  @ epoch 4878 )\n",
      "[Epoch: 4960] train loss: 0.4982, train acc: 0.7967, val loss: 0.4107, val acc: 0.8253  (best train acc: 0.8232, best val acc: 0.8358, best train loss: 0.4389  @ epoch 4878 )\n",
      "[Epoch: 4980] train loss: 0.5066, train acc: 0.7775, val loss: 0.4355, val acc: 0.8047  (best train acc: 0.8232, best val acc: 0.8358, best train loss: 0.4389  @ epoch 4878 )\n",
      "[Epoch: 5000] train loss: 0.4983, train acc: 0.7903, val loss: 0.4192, val acc: 0.8182  (best train acc: 0.8232, best val acc: 0.8358, best train loss: 0.4389  @ epoch 4878 )\n",
      "[Epoch: 5020] train loss: 0.5242, train acc: 0.7855, val loss: 0.4249, val acc: 0.8185  (best train acc: 0.8232, best val acc: 0.8381, best train loss: 0.4389  @ epoch 4878 )\n",
      "[Epoch: 5040] train loss: 0.5372, train acc: 0.7795, val loss: 0.4474, val acc: 0.8024  (best train acc: 0.8232, best val acc: 0.8381, best train loss: 0.4389  @ epoch 4878 )\n",
      "[Epoch: 5060] train loss: 0.4960, train acc: 0.7987, val loss: 0.3985, val acc: 0.8287  (best train acc: 0.8232, best val acc: 0.8381, best train loss: 0.4389  @ epoch 4878 )\n",
      "[Epoch: 5080] train loss: 0.4934, train acc: 0.7916, val loss: 0.3998, val acc: 0.8317  (best train acc: 0.8232, best val acc: 0.8381, best train loss: 0.4389  @ epoch 4878 )\n",
      "[Epoch: 5100] train loss: 0.5147, train acc: 0.7851, val loss: 0.4327, val acc: 0.8159  (best train acc: 0.8232, best val acc: 0.8381, best train loss: 0.4389  @ epoch 4878 )\n",
      "[Epoch: 5120] train loss: 0.4811, train acc: 0.7964, val loss: 0.4200, val acc: 0.8179  (best train acc: 0.8232, best val acc: 0.8381, best train loss: 0.4389  @ epoch 4878 )\n",
      "[Epoch: 5140] train loss: 0.4826, train acc: 0.7969, val loss: 0.4043, val acc: 0.8273  (best train acc: 0.8232, best val acc: 0.8381, best train loss: 0.4389  @ epoch 4878 )\n",
      "[Epoch: 5160] train loss: 0.4749, train acc: 0.7895, val loss: 0.4066, val acc: 0.8297  (best train acc: 0.8232, best val acc: 0.8398, best train loss: 0.4389  @ epoch 4878 )\n",
      "[Epoch: 5180] train loss: 0.5184, train acc: 0.7833, val loss: 0.4447, val acc: 0.8061  (best train acc: 0.8232, best val acc: 0.8398, best train loss: 0.4389  @ epoch 4878 )\n",
      "[Epoch: 5200] train loss: 0.4930, train acc: 0.7957, val loss: 0.4091, val acc: 0.8253  (best train acc: 0.8232, best val acc: 0.8398, best train loss: 0.4389  @ epoch 4878 )\n",
      "[Epoch: 5220] train loss: 0.4896, train acc: 0.8018, val loss: 0.3976, val acc: 0.8361  (best train acc: 0.8232, best val acc: 0.8398, best train loss: 0.4389  @ epoch 4878 )\n",
      "[Epoch: 5240] train loss: 0.4754, train acc: 0.7979, val loss: 0.4282, val acc: 0.8105  (best train acc: 0.8232, best val acc: 0.8398, best train loss: 0.4389  @ epoch 4878 )\n",
      "[Epoch: 5260] train loss: 0.5051, train acc: 0.7982, val loss: 0.4320, val acc: 0.8159  (best train acc: 0.8232, best val acc: 0.8398, best train loss: 0.4389  @ epoch 4878 )\n",
      "[Epoch: 5280] train loss: 0.4616, train acc: 0.8078, val loss: 0.3946, val acc: 0.8358  (best train acc: 0.8232, best val acc: 0.8398, best train loss: 0.4389  @ epoch 4878 )\n",
      "[Epoch: 5300] train loss: 0.5175, train acc: 0.7825, val loss: 0.4717, val acc: 0.7895  (best train acc: 0.8232, best val acc: 0.8398, best train loss: 0.4389  @ epoch 4878 )\n",
      "[Epoch: 5320] train loss: 0.4731, train acc: 0.8041, val loss: 0.4068, val acc: 0.8287  (best train acc: 0.8232, best val acc: 0.8398, best train loss: 0.4389  @ epoch 4878 )\n",
      "[Epoch: 5340] train loss: 0.5094, train acc: 0.7863, val loss: 0.4089, val acc: 0.8287  (best train acc: 0.8232, best val acc: 0.8398, best train loss: 0.4389  @ epoch 4878 )\n",
      "[Epoch: 5360] train loss: 0.5042, train acc: 0.7939, val loss: 0.4099, val acc: 0.8300  (best train acc: 0.8232, best val acc: 0.8398, best train loss: 0.4389  @ epoch 4878 )\n",
      "[Epoch: 5380] train loss: 0.4586, train acc: 0.8094, val loss: 0.4111, val acc: 0.8273  (best train acc: 0.8232, best val acc: 0.8449, best train loss: 0.4389  @ epoch 4878 )\n",
      "[Epoch: 5400] train loss: 0.4995, train acc: 0.7870, val loss: 0.4142, val acc: 0.8212  (best train acc: 0.8232, best val acc: 0.8449, best train loss: 0.4389  @ epoch 4878 )\n",
      "[Epoch: 5420] train loss: 0.4617, train acc: 0.8094, val loss: 0.4092, val acc: 0.8219  (best train acc: 0.8232, best val acc: 0.8449, best train loss: 0.4389  @ epoch 4878 )\n",
      "[Epoch: 5440] train loss: 0.4822, train acc: 0.8013, val loss: 0.4013, val acc: 0.8354  (best train acc: 0.8232, best val acc: 0.8449, best train loss: 0.4389  @ epoch 4878 )\n",
      "[Epoch: 5460] train loss: 0.4599, train acc: 0.8092, val loss: 0.4178, val acc: 0.8196  (best train acc: 0.8232, best val acc: 0.8449, best train loss: 0.4389  @ epoch 4878 )\n",
      "[Epoch: 5480] train loss: 0.4927, train acc: 0.7963, val loss: 0.4111, val acc: 0.8331  (best train acc: 0.8232, best val acc: 0.8449, best train loss: 0.4358  @ epoch 5470 )\n",
      "[Epoch: 5500] train loss: 0.4538, train acc: 0.8087, val loss: 0.3976, val acc: 0.8310  (best train acc: 0.8232, best val acc: 0.8449, best train loss: 0.4358  @ epoch 5470 )\n",
      "[Epoch: 5520] train loss: 0.4831, train acc: 0.8015, val loss: 0.4019, val acc: 0.8391  (best train acc: 0.8232, best val acc: 0.8449, best train loss: 0.4358  @ epoch 5470 )\n",
      "[Epoch: 5540] train loss: 0.4693, train acc: 0.8020, val loss: 0.3935, val acc: 0.8327  (best train acc: 0.8232, best val acc: 0.8449, best train loss: 0.4358  @ epoch 5470 )\n",
      "[Epoch: 5560] train loss: 0.5127, train acc: 0.7906, val loss: 0.4055, val acc: 0.8263  (best train acc: 0.8232, best val acc: 0.8449, best train loss: 0.4358  @ epoch 5470 )\n",
      "[Epoch: 5580] train loss: 0.4965, train acc: 0.7954, val loss: 0.4368, val acc: 0.8115  (best train acc: 0.8232, best val acc: 0.8449, best train loss: 0.4358  @ epoch 5470 )\n",
      "[Epoch: 5600] train loss: 0.4595, train acc: 0.8081, val loss: 0.4017, val acc: 0.8290  (best train acc: 0.8232, best val acc: 0.8449, best train loss: 0.4358  @ epoch 5470 )\n",
      "[Epoch: 5620] train loss: 0.5340, train acc: 0.7723, val loss: 0.4410, val acc: 0.8057  (best train acc: 0.8232, best val acc: 0.8449, best train loss: 0.4358  @ epoch 5470 )\n",
      "[Epoch: 5640] train loss: 0.4847, train acc: 0.7987, val loss: 0.4314, val acc: 0.8128  (best train acc: 0.8241, best val acc: 0.8449, best train loss: 0.4358  @ epoch 5470 )\n",
      "[Epoch: 5660] train loss: 0.4541, train acc: 0.8081, val loss: 0.3943, val acc: 0.8405  (best train acc: 0.8241, best val acc: 0.8449, best train loss: 0.4358  @ epoch 5470 )\n",
      "[Epoch: 5680] train loss: 0.4827, train acc: 0.8092, val loss: 0.4030, val acc: 0.8280  (best train acc: 0.8241, best val acc: 0.8449, best train loss: 0.4358  @ epoch 5470 )\n",
      "[Epoch: 5700] train loss: 0.4963, train acc: 0.7979, val loss: 0.4042, val acc: 0.8239  (best train acc: 0.8241, best val acc: 0.8449, best train loss: 0.4295  @ epoch 5694 )\n",
      "[Epoch: 5720] train loss: 0.4733, train acc: 0.8043, val loss: 0.4129, val acc: 0.8175  (best train acc: 0.8241, best val acc: 0.8449, best train loss: 0.4295  @ epoch 5694 )\n",
      "[Epoch: 5740] train loss: 0.5375, train acc: 0.7676, val loss: 0.4235, val acc: 0.8206  (best train acc: 0.8241, best val acc: 0.8449, best train loss: 0.4295  @ epoch 5694 )\n",
      "[Epoch: 5760] train loss: 0.4900, train acc: 0.7925, val loss: 0.3784, val acc: 0.8388  (best train acc: 0.8241, best val acc: 0.8449, best train loss: 0.4295  @ epoch 5694 )\n",
      "[Epoch: 5780] train loss: 0.4986, train acc: 0.7757, val loss: 0.4325, val acc: 0.8121  (best train acc: 0.8241, best val acc: 0.8449, best train loss: 0.4295  @ epoch 5694 )\n",
      "[Epoch: 5800] train loss: 0.4513, train acc: 0.8091, val loss: 0.4105, val acc: 0.8273  (best train acc: 0.8241, best val acc: 0.8449, best train loss: 0.4295  @ epoch 5694 )\n",
      "[Epoch: 5820] train loss: 0.4621, train acc: 0.8104, val loss: 0.3993, val acc: 0.8331  (best train acc: 0.8241, best val acc: 0.8449, best train loss: 0.4295  @ epoch 5694 )\n",
      "[Epoch: 5840] train loss: 0.4896, train acc: 0.7948, val loss: 0.4018, val acc: 0.8260  (best train acc: 0.8241, best val acc: 0.8449, best train loss: 0.4295  @ epoch 5694 )\n",
      "[Epoch: 5860] train loss: 0.4382, train acc: 0.8175, val loss: 0.3942, val acc: 0.8364  (best train acc: 0.8241, best val acc: 0.8449, best train loss: 0.4295  @ epoch 5694 )\n",
      "[Epoch: 5880] train loss: 0.4818, train acc: 0.8007, val loss: 0.3943, val acc: 0.8317  (best train acc: 0.8241, best val acc: 0.8449, best train loss: 0.4295  @ epoch 5694 )\n",
      "[Epoch: 5900] train loss: 0.4766, train acc: 0.8031, val loss: 0.4739, val acc: 0.7892  (best train acc: 0.8241, best val acc: 0.8449, best train loss: 0.4295  @ epoch 5694 )\n",
      "[Epoch: 5920] train loss: 0.4768, train acc: 0.7940, val loss: 0.4471, val acc: 0.8017  (best train acc: 0.8241, best val acc: 0.8449, best train loss: 0.4295  @ epoch 5694 )\n",
      "[Epoch: 5940] train loss: 0.4541, train acc: 0.8073, val loss: 0.3893, val acc: 0.8327  (best train acc: 0.8241, best val acc: 0.8449, best train loss: 0.4295  @ epoch 5694 )\n",
      "[Epoch: 5960] train loss: 0.4497, train acc: 0.8148, val loss: 0.4097, val acc: 0.8243  (best train acc: 0.8241, best val acc: 0.8449, best train loss: 0.4295  @ epoch 5694 )\n",
      "[Epoch: 5980] train loss: 0.5293, train acc: 0.7751, val loss: 0.4055, val acc: 0.8280  (best train acc: 0.8241, best val acc: 0.8449, best train loss: 0.4295  @ epoch 5694 )\n",
      "[Epoch: 6000] train loss: 0.4792, train acc: 0.7968, val loss: 0.4023, val acc: 0.8280  (best train acc: 0.8241, best val acc: 0.8449, best train loss: 0.4295  @ epoch 5694 )\n",
      "[Epoch: 6020] train loss: 0.5152, train acc: 0.7854, val loss: 0.3948, val acc: 0.8317  (best train acc: 0.8241, best val acc: 0.8449, best train loss: 0.4295  @ epoch 5694 )\n",
      "[Epoch: 6040] train loss: 0.5392, train acc: 0.7692, val loss: 0.3881, val acc: 0.8334  (best train acc: 0.8241, best val acc: 0.8449, best train loss: 0.4295  @ epoch 5694 )\n",
      "[Epoch: 6060] train loss: 0.5597, train acc: 0.7615, val loss: 0.3918, val acc: 0.8388  (best train acc: 0.8241, best val acc: 0.8449, best train loss: 0.4295  @ epoch 5694 )\n",
      "[Epoch: 6080] train loss: 0.4727, train acc: 0.8010, val loss: 0.3942, val acc: 0.8327  (best train acc: 0.8241, best val acc: 0.8449, best train loss: 0.4295  @ epoch 5694 )\n",
      "[Epoch: 6100] train loss: 0.4578, train acc: 0.8166, val loss: 0.3990, val acc: 0.8277  (best train acc: 0.8241, best val acc: 0.8449, best train loss: 0.4295  @ epoch 5694 )\n",
      "[Epoch: 6120] train loss: 0.4954, train acc: 0.7909, val loss: 0.3976, val acc: 0.8290  (best train acc: 0.8241, best val acc: 0.8449, best train loss: 0.4295  @ epoch 5694 )\n",
      "[Epoch: 6140] train loss: 0.5107, train acc: 0.7873, val loss: 0.4082, val acc: 0.8287  (best train acc: 0.8241, best val acc: 0.8449, best train loss: 0.4295  @ epoch 5694 )\n",
      "[Epoch: 6160] train loss: 0.4916, train acc: 0.8028, val loss: 0.3981, val acc: 0.8337  (best train acc: 0.8300, best val acc: 0.8449, best train loss: 0.4295  @ epoch 5694 )\n",
      "[Epoch: 6180] train loss: 0.4511, train acc: 0.8156, val loss: 0.3875, val acc: 0.8428  (best train acc: 0.8300, best val acc: 0.8449, best train loss: 0.4295  @ epoch 5694 )\n",
      "[Epoch: 6200] train loss: 0.4656, train acc: 0.8092, val loss: 0.3959, val acc: 0.8310  (best train acc: 0.8300, best val acc: 0.8452, best train loss: 0.4295  @ epoch 5694 )\n",
      "[Epoch: 6220] train loss: 0.5021, train acc: 0.7840, val loss: 0.4179, val acc: 0.8199  (best train acc: 0.8300, best val acc: 0.8452, best train loss: 0.4295  @ epoch 5694 )\n",
      "[Epoch: 6240] train loss: 0.4821, train acc: 0.8010, val loss: 0.4477, val acc: 0.8030  (best train acc: 0.8300, best val acc: 0.8452, best train loss: 0.4295  @ epoch 5694 )\n",
      "[Epoch: 6260] train loss: 0.4549, train acc: 0.8138, val loss: 0.4021, val acc: 0.8347  (best train acc: 0.8300, best val acc: 0.8452, best train loss: 0.4295  @ epoch 5694 )\n",
      "[Epoch: 6280] train loss: 0.4573, train acc: 0.8097, val loss: 0.4081, val acc: 0.8280  (best train acc: 0.8300, best val acc: 0.8452, best train loss: 0.4295  @ epoch 5694 )\n",
      "[Epoch: 6300] train loss: 0.4807, train acc: 0.7931, val loss: 0.4534, val acc: 0.7976  (best train acc: 0.8300, best val acc: 0.8452, best train loss: 0.4295  @ epoch 5694 )\n",
      "[Epoch: 6320] train loss: 0.4521, train acc: 0.8127, val loss: 0.3825, val acc: 0.8418  (best train acc: 0.8300, best val acc: 0.8452, best train loss: 0.4295  @ epoch 5694 )\n",
      "[Epoch: 6340] train loss: 0.4621, train acc: 0.8098, val loss: 0.4037, val acc: 0.8260  (best train acc: 0.8300, best val acc: 0.8452, best train loss: 0.4295  @ epoch 5694 )\n",
      "[Epoch: 6360] train loss: 0.4765, train acc: 0.8004, val loss: 0.4069, val acc: 0.8219  (best train acc: 0.8300, best val acc: 0.8492, best train loss: 0.4295  @ epoch 5694 )\n",
      "[Epoch: 6380] train loss: 0.5050, train acc: 0.7926, val loss: 0.4025, val acc: 0.8270  (best train acc: 0.8300, best val acc: 0.8492, best train loss: 0.4295  @ epoch 5694 )\n",
      "[Epoch: 6400] train loss: 0.5055, train acc: 0.7822, val loss: 0.4180, val acc: 0.8209  (best train acc: 0.8300, best val acc: 0.8492, best train loss: 0.4295  @ epoch 5694 )\n",
      "[Epoch: 6420] train loss: 0.4755, train acc: 0.8039, val loss: 0.4024, val acc: 0.8324  (best train acc: 0.8300, best val acc: 0.8492, best train loss: 0.4295  @ epoch 5694 )\n",
      "[Epoch: 6440] train loss: 0.4962, train acc: 0.8039, val loss: 0.4317, val acc: 0.8121  (best train acc: 0.8300, best val acc: 0.8492, best train loss: 0.4295  @ epoch 5694 )\n",
      "[Epoch: 6460] train loss: 0.4927, train acc: 0.7872, val loss: 0.3901, val acc: 0.8341  (best train acc: 0.8300, best val acc: 0.8492, best train loss: 0.4295  @ epoch 5694 )\n",
      "[Epoch: 6480] train loss: 0.4626, train acc: 0.8090, val loss: 0.3894, val acc: 0.8381  (best train acc: 0.8300, best val acc: 0.8492, best train loss: 0.4295  @ epoch 5694 )\n",
      "[Epoch: 6500] train loss: 0.4453, train acc: 0.8102, val loss: 0.4075, val acc: 0.8189  (best train acc: 0.8300, best val acc: 0.8492, best train loss: 0.4270  @ epoch 6488 )\n",
      "[Epoch: 6520] train loss: 0.4639, train acc: 0.8109, val loss: 0.4153, val acc: 0.8206  (best train acc: 0.8300, best val acc: 0.8492, best train loss: 0.4270  @ epoch 6488 )\n",
      "[Epoch: 6540] train loss: 0.4344, train acc: 0.8174, val loss: 0.3880, val acc: 0.8401  (best train acc: 0.8300, best val acc: 0.8492, best train loss: 0.4270  @ epoch 6488 )\n",
      "[Epoch: 6560] train loss: 0.4749, train acc: 0.8033, val loss: 0.3854, val acc: 0.8347  (best train acc: 0.8300, best val acc: 0.8492, best train loss: 0.4266  @ epoch 6543 )\n",
      "[Epoch: 6580] train loss: 0.4478, train acc: 0.8203, val loss: 0.4240, val acc: 0.8162  (best train acc: 0.8300, best val acc: 0.8492, best train loss: 0.4266  @ epoch 6543 )\n",
      "[Epoch: 6600] train loss: 0.4929, train acc: 0.7987, val loss: 0.4062, val acc: 0.8273  (best train acc: 0.8300, best val acc: 0.8492, best train loss: 0.4266  @ epoch 6543 )\n",
      "[Epoch: 6620] train loss: 0.4679, train acc: 0.8001, val loss: 0.3858, val acc: 0.8401  (best train acc: 0.8300, best val acc: 0.8492, best train loss: 0.4266  @ epoch 6543 )\n",
      "[Epoch: 6640] train loss: 0.4538, train acc: 0.8096, val loss: 0.3807, val acc: 0.8391  (best train acc: 0.8300, best val acc: 0.8492, best train loss: 0.4266  @ epoch 6543 )\n",
      "[Epoch: 6660] train loss: 0.4818, train acc: 0.8012, val loss: 0.3918, val acc: 0.8334  (best train acc: 0.8300, best val acc: 0.8492, best train loss: 0.4209  @ epoch 6658 )\n",
      "[Epoch: 6680] train loss: 0.4671, train acc: 0.8007, val loss: 0.3837, val acc: 0.8358  (best train acc: 0.8300, best val acc: 0.8492, best train loss: 0.4209  @ epoch 6658 )\n",
      "[Epoch: 6700] train loss: 0.4844, train acc: 0.8035, val loss: 0.4084, val acc: 0.8219  (best train acc: 0.8300, best val acc: 0.8492, best train loss: 0.4209  @ epoch 6658 )\n",
      "[Epoch: 6720] train loss: 0.4660, train acc: 0.8069, val loss: 0.4176, val acc: 0.8216  (best train acc: 0.8300, best val acc: 0.8492, best train loss: 0.4209  @ epoch 6658 )\n",
      "[Epoch: 6740] train loss: 0.5019, train acc: 0.7958, val loss: 0.4058, val acc: 0.8270  (best train acc: 0.8300, best val acc: 0.8492, best train loss: 0.4209  @ epoch 6658 )\n",
      "[Epoch: 6760] train loss: 0.4462, train acc: 0.8172, val loss: 0.3906, val acc: 0.8341  (best train acc: 0.8300, best val acc: 0.8492, best train loss: 0.4209  @ epoch 6658 )\n",
      "[Epoch: 6780] train loss: 0.5133, train acc: 0.7726, val loss: 0.4091, val acc: 0.8199  (best train acc: 0.8300, best val acc: 0.8492, best train loss: 0.4209  @ epoch 6658 )\n",
      "[Epoch: 6800] train loss: 0.4913, train acc: 0.7967, val loss: 0.3744, val acc: 0.8398  (best train acc: 0.8300, best val acc: 0.8492, best train loss: 0.4209  @ epoch 6658 )\n",
      "[Epoch: 6820] train loss: 0.4904, train acc: 0.7906, val loss: 0.4235, val acc: 0.8172  (best train acc: 0.8300, best val acc: 0.8492, best train loss: 0.4209  @ epoch 6658 )\n",
      "[Epoch: 6840] train loss: 0.4572, train acc: 0.8135, val loss: 0.3821, val acc: 0.8391  (best train acc: 0.8300, best val acc: 0.8492, best train loss: 0.4209  @ epoch 6658 )\n",
      "[Epoch: 6860] train loss: 0.4630, train acc: 0.8138, val loss: 0.3930, val acc: 0.8324  (best train acc: 0.8300, best val acc: 0.8492, best train loss: 0.4209  @ epoch 6658 )\n",
      "[Epoch: 6880] train loss: 0.4623, train acc: 0.8047, val loss: 0.4086, val acc: 0.8152  (best train acc: 0.8313, best val acc: 0.8492, best train loss: 0.4209  @ epoch 6658 )\n",
      "[Epoch: 6900] train loss: 0.4927, train acc: 0.7925, val loss: 0.4208, val acc: 0.8165  (best train acc: 0.8313, best val acc: 0.8492, best train loss: 0.4207  @ epoch 6897 )\n",
      "[Epoch: 6920] train loss: 0.4695, train acc: 0.8071, val loss: 0.4080, val acc: 0.8229  (best train acc: 0.8313, best val acc: 0.8492, best train loss: 0.4207  @ epoch 6897 )\n",
      "[Epoch: 6940] train loss: 0.4663, train acc: 0.8083, val loss: 0.3944, val acc: 0.8314  (best train acc: 0.8313, best val acc: 0.8492, best train loss: 0.4189  @ epoch 6926 )\n",
      "[Epoch: 6960] train loss: 0.4353, train acc: 0.8161, val loss: 0.3811, val acc: 0.8455  (best train acc: 0.8313, best val acc: 0.8492, best train loss: 0.4189  @ epoch 6926 )\n",
      "[Epoch: 6980] train loss: 0.4657, train acc: 0.8039, val loss: 0.3996, val acc: 0.8287  (best train acc: 0.8313, best val acc: 0.8492, best train loss: 0.4189  @ epoch 6926 )\n",
      "[Epoch: 7000] train loss: 0.4965, train acc: 0.7874, val loss: 0.3855, val acc: 0.8449  (best train acc: 0.8313, best val acc: 0.8492, best train loss: 0.4177  @ epoch 6995 )\n",
      "[Epoch: 7020] train loss: 0.4695, train acc: 0.8125, val loss: 0.3761, val acc: 0.8401  (best train acc: 0.8313, best val acc: 0.8492, best train loss: 0.4177  @ epoch 6995 )\n",
      "[Epoch: 7040] train loss: 0.4535, train acc: 0.8159, val loss: 0.3840, val acc: 0.8361  (best train acc: 0.8313, best val acc: 0.8492, best train loss: 0.4177  @ epoch 6995 )\n",
      "[Epoch: 7060] train loss: 0.4590, train acc: 0.8100, val loss: 0.3874, val acc: 0.8344  (best train acc: 0.8313, best val acc: 0.8492, best train loss: 0.4177  @ epoch 6995 )\n",
      "[Epoch: 7080] train loss: 0.4597, train acc: 0.8126, val loss: 0.3914, val acc: 0.8300  (best train acc: 0.8313, best val acc: 0.8492, best train loss: 0.4177  @ epoch 6995 )\n",
      "[Epoch: 7100] train loss: 0.4271, train acc: 0.8238, val loss: 0.3819, val acc: 0.8327  (best train acc: 0.8323, best val acc: 0.8492, best train loss: 0.4134  @ epoch 7099 )\n",
      "[Epoch: 7120] train loss: 0.4620, train acc: 0.8124, val loss: 0.3726, val acc: 0.8391  (best train acc: 0.8323, best val acc: 0.8492, best train loss: 0.4122  @ epoch 7101 )\n",
      "[Epoch: 7140] train loss: 0.4762, train acc: 0.8081, val loss: 0.3784, val acc: 0.8398  (best train acc: 0.8323, best val acc: 0.8492, best train loss: 0.4122  @ epoch 7101 )\n",
      "[Epoch: 7160] train loss: 0.4495, train acc: 0.8117, val loss: 0.4366, val acc: 0.8105  (best train acc: 0.8323, best val acc: 0.8492, best train loss: 0.4122  @ epoch 7101 )\n",
      "[Epoch: 7180] train loss: 0.4615, train acc: 0.8124, val loss: 0.3919, val acc: 0.8290  (best train acc: 0.8323, best val acc: 0.8492, best train loss: 0.4122  @ epoch 7101 )\n",
      "[Epoch: 7200] train loss: 0.4499, train acc: 0.8139, val loss: 0.3866, val acc: 0.8300  (best train acc: 0.8323, best val acc: 0.8492, best train loss: 0.4122  @ epoch 7101 )\n",
      "[Epoch: 7220] train loss: 0.4686, train acc: 0.8044, val loss: 0.3779, val acc: 0.8415  (best train acc: 0.8323, best val acc: 0.8492, best train loss: 0.4122  @ epoch 7101 )\n",
      "[Epoch: 7240] train loss: 0.4249, train acc: 0.8253, val loss: 0.3946, val acc: 0.8293  (best train acc: 0.8323, best val acc: 0.8492, best train loss: 0.4122  @ epoch 7101 )\n",
      "[Epoch: 7260] train loss: 0.4562, train acc: 0.8143, val loss: 0.3818, val acc: 0.8391  (best train acc: 0.8334, best val acc: 0.8509, best train loss: 0.4083  @ epoch 7246 )\n",
      "[Epoch: 7280] train loss: 0.4326, train acc: 0.8214, val loss: 0.3828, val acc: 0.8378  (best train acc: 0.8334, best val acc: 0.8509, best train loss: 0.4057  @ epoch 7273 )\n",
      "[Epoch: 7300] train loss: 0.5274, train acc: 0.7691, val loss: 0.3815, val acc: 0.8472  (best train acc: 0.8334, best val acc: 0.8509, best train loss: 0.4057  @ epoch 7273 )\n",
      "[Epoch: 7320] train loss: 0.4485, train acc: 0.8065, val loss: 0.3690, val acc: 0.8459  (best train acc: 0.8334, best val acc: 0.8516, best train loss: 0.4057  @ epoch 7273 )\n",
      "[Epoch: 7340] train loss: 0.4553, train acc: 0.8096, val loss: 0.3903, val acc: 0.8347  (best train acc: 0.8344, best val acc: 0.8516, best train loss: 0.4057  @ epoch 7273 )\n",
      "[Epoch: 7360] train loss: 0.4360, train acc: 0.8181, val loss: 0.3641, val acc: 0.8452  (best train acc: 0.8344, best val acc: 0.8516, best train loss: 0.4057  @ epoch 7273 )\n",
      "[Epoch: 7380] train loss: 0.4880, train acc: 0.7963, val loss: 0.4251, val acc: 0.8148  (best train acc: 0.8385, best val acc: 0.8516, best train loss: 0.3997  @ epoch 7363 )\n",
      "[Epoch: 7400] train loss: 0.4550, train acc: 0.8146, val loss: 0.3875, val acc: 0.8351  (best train acc: 0.8385, best val acc: 0.8516, best train loss: 0.3997  @ epoch 7363 )\n",
      "[Epoch: 7420] train loss: 0.4523, train acc: 0.8145, val loss: 0.3707, val acc: 0.8462  (best train acc: 0.8385, best val acc: 0.8516, best train loss: 0.3997  @ epoch 7363 )\n",
      "[Epoch: 7440] train loss: 0.4761, train acc: 0.8080, val loss: 0.3869, val acc: 0.8432  (best train acc: 0.8385, best val acc: 0.8523, best train loss: 0.3997  @ epoch 7363 )\n",
      "[Epoch: 7460] train loss: 0.4198, train acc: 0.8244, val loss: 0.3656, val acc: 0.8486  (best train acc: 0.8385, best val acc: 0.8523, best train loss: 0.3997  @ epoch 7363 )\n",
      "[Epoch: 7480] train loss: 0.4565, train acc: 0.8109, val loss: 0.3919, val acc: 0.8297  (best train acc: 0.8390, best val acc: 0.8523, best train loss: 0.3968  @ epoch 7463 )\n",
      "[Epoch: 7500] train loss: 0.4133, train acc: 0.8283, val loss: 0.3666, val acc: 0.8465  (best train acc: 0.8390, best val acc: 0.8523, best train loss: 0.3968  @ epoch 7463 )\n",
      "[Epoch: 7520] train loss: 0.4700, train acc: 0.7969, val loss: 0.3827, val acc: 0.8320  (best train acc: 0.8390, best val acc: 0.8523, best train loss: 0.3968  @ epoch 7463 )\n",
      "[Epoch: 7540] train loss: 0.4253, train acc: 0.8240, val loss: 0.4067, val acc: 0.8216  (best train acc: 0.8390, best val acc: 0.8523, best train loss: 0.3968  @ epoch 7463 )\n",
      "[Epoch: 7560] train loss: 0.4442, train acc: 0.8125, val loss: 0.3676, val acc: 0.8438  (best train acc: 0.8390, best val acc: 0.8523, best train loss: 0.3968  @ epoch 7463 )\n",
      "[Epoch: 7580] train loss: 0.4097, train acc: 0.8276, val loss: 0.3642, val acc: 0.8486  (best train acc: 0.8390, best val acc: 0.8523, best train loss: 0.3968  @ epoch 7463 )\n",
      "[Epoch: 7600] train loss: 0.4518, train acc: 0.8127, val loss: 0.3770, val acc: 0.8347  (best train acc: 0.8390, best val acc: 0.8523, best train loss: 0.3968  @ epoch 7463 )\n",
      "[Epoch: 7620] train loss: 0.5022, train acc: 0.7834, val loss: 0.3710, val acc: 0.8476  (best train acc: 0.8390, best val acc: 0.8523, best train loss: 0.3968  @ epoch 7463 )\n",
      "[Epoch: 7640] train loss: 0.4300, train acc: 0.8240, val loss: 0.3599, val acc: 0.8489  (best train acc: 0.8390, best val acc: 0.8523, best train loss: 0.3968  @ epoch 7463 )\n",
      "[Epoch: 7660] train loss: 0.4521, train acc: 0.8195, val loss: 0.3710, val acc: 0.8408  (best train acc: 0.8390, best val acc: 0.8523, best train loss: 0.3968  @ epoch 7463 )\n",
      "[Epoch: 7680] train loss: 0.4634, train acc: 0.7955, val loss: 0.3659, val acc: 0.8503  (best train acc: 0.8390, best val acc: 0.8523, best train loss: 0.3968  @ epoch 7463 )\n",
      "[Epoch: 7700] train loss: 0.4233, train acc: 0.8276, val loss: 0.3868, val acc: 0.8344  (best train acc: 0.8390, best val acc: 0.8543, best train loss: 0.3968  @ epoch 7463 )\n",
      "[Epoch: 7720] train loss: 0.4970, train acc: 0.7833, val loss: 0.3882, val acc: 0.8263  (best train acc: 0.8390, best val acc: 0.8543, best train loss: 0.3968  @ epoch 7463 )\n",
      "[Epoch: 7740] train loss: 0.4242, train acc: 0.8281, val loss: 0.3668, val acc: 0.8489  (best train acc: 0.8390, best val acc: 0.8543, best train loss: 0.3968  @ epoch 7463 )\n",
      "[Epoch: 7760] train loss: 0.4633, train acc: 0.8045, val loss: 0.3702, val acc: 0.8425  (best train acc: 0.8390, best val acc: 0.8543, best train loss: 0.3968  @ epoch 7463 )\n",
      "[Epoch: 7780] train loss: 0.4760, train acc: 0.8048, val loss: 0.3683, val acc: 0.8378  (best train acc: 0.8390, best val acc: 0.8543, best train loss: 0.3968  @ epoch 7463 )\n",
      "[Epoch: 7800] train loss: 0.4555, train acc: 0.8099, val loss: 0.3755, val acc: 0.8479  (best train acc: 0.8390, best val acc: 0.8543, best train loss: 0.3968  @ epoch 7463 )\n",
      "[Epoch: 7820] train loss: 0.4297, train acc: 0.8209, val loss: 0.3671, val acc: 0.8438  (best train acc: 0.8390, best val acc: 0.8543, best train loss: 0.3968  @ epoch 7463 )\n",
      "[Epoch: 7840] train loss: 0.4161, train acc: 0.8274, val loss: 0.3924, val acc: 0.8361  (best train acc: 0.8390, best val acc: 0.8543, best train loss: 0.3968  @ epoch 7463 )\n",
      "[Epoch: 7860] train loss: 0.4516, train acc: 0.8125, val loss: 0.3837, val acc: 0.8358  (best train acc: 0.8390, best val acc: 0.8543, best train loss: 0.3968  @ epoch 7463 )\n",
      "[Epoch: 7880] train loss: 0.3991, train acc: 0.8347, val loss: 0.3677, val acc: 0.8432  (best train acc: 0.8390, best val acc: 0.8543, best train loss: 0.3968  @ epoch 7871 )\n",
      "[Epoch: 7900] train loss: 0.4217, train acc: 0.8316, val loss: 0.3818, val acc: 0.8405  (best train acc: 0.8399, best val acc: 0.8543, best train loss: 0.3968  @ epoch 7871 )\n",
      "[Epoch: 7920] train loss: 0.3926, train acc: 0.8411, val loss: 0.3643, val acc: 0.8455  (best train acc: 0.8411, best val acc: 0.8543, best train loss: 0.3926  @ epoch 7920 )\n",
      "[Epoch: 7940] train loss: 0.4517, train acc: 0.8135, val loss: 0.3725, val acc: 0.8486  (best train acc: 0.8411, best val acc: 0.8543, best train loss: 0.3926  @ epoch 7920 )\n",
      "[Epoch: 7960] train loss: 0.4192, train acc: 0.8242, val loss: 0.3565, val acc: 0.8519  (best train acc: 0.8411, best val acc: 0.8543, best train loss: 0.3926  @ epoch 7920 )\n",
      "[Epoch: 7980] train loss: 0.4166, train acc: 0.8253, val loss: 0.3664, val acc: 0.8395  (best train acc: 0.8411, best val acc: 0.8543, best train loss: 0.3887  @ epoch 7973 )\n",
      "[Epoch: 8000] train loss: 0.4667, train acc: 0.8052, val loss: 0.3634, val acc: 0.8445  (best train acc: 0.8411, best val acc: 0.8543, best train loss: 0.3887  @ epoch 7973 )\n",
      "[Epoch: 8020] train loss: 0.4166, train acc: 0.8218, val loss: 0.3547, val acc: 0.8489  (best train acc: 0.8411, best val acc: 0.8543, best train loss: 0.3887  @ epoch 7973 )\n",
      "[Epoch: 8040] train loss: 0.4329, train acc: 0.8093, val loss: 0.3550, val acc: 0.8503  (best train acc: 0.8411, best val acc: 0.8543, best train loss: 0.3887  @ epoch 7973 )\n",
      "[Epoch: 8060] train loss: 0.3989, train acc: 0.8364, val loss: 0.3601, val acc: 0.8536  (best train acc: 0.8411, best val acc: 0.8543, best train loss: 0.3887  @ epoch 7973 )\n",
      "[Epoch: 8080] train loss: 0.4901, train acc: 0.7962, val loss: 0.3645, val acc: 0.8479  (best train acc: 0.8411, best val acc: 0.8556, best train loss: 0.3887  @ epoch 7973 )\n",
      "[Epoch: 8100] train loss: 0.4598, train acc: 0.8126, val loss: 0.3525, val acc: 0.8516  (best train acc: 0.8411, best val acc: 0.8556, best train loss: 0.3887  @ epoch 7973 )\n",
      "[Epoch: 8120] train loss: 0.3995, train acc: 0.8349, val loss: 0.3478, val acc: 0.8536  (best train acc: 0.8411, best val acc: 0.8577, best train loss: 0.3887  @ epoch 7973 )\n",
      "[Epoch: 8140] train loss: 0.4585, train acc: 0.8104, val loss: 0.3940, val acc: 0.8273  (best train acc: 0.8411, best val acc: 0.8577, best train loss: 0.3887  @ epoch 7973 )\n",
      "[Epoch: 8160] train loss: 0.3947, train acc: 0.8349, val loss: 0.3690, val acc: 0.8513  (best train acc: 0.8411, best val acc: 0.8577, best train loss: 0.3887  @ epoch 7973 )\n",
      "[Epoch: 8180] train loss: 0.4615, train acc: 0.8016, val loss: 0.3559, val acc: 0.8519  (best train acc: 0.8411, best val acc: 0.8577, best train loss: 0.3887  @ epoch 7973 )\n",
      "[Epoch: 8200] train loss: 0.4348, train acc: 0.8166, val loss: 0.3926, val acc: 0.8256  (best train acc: 0.8411, best val acc: 0.8577, best train loss: 0.3887  @ epoch 7973 )\n",
      "[Epoch: 8220] train loss: 0.4111, train acc: 0.8235, val loss: 0.3510, val acc: 0.8556  (best train acc: 0.8417, best val acc: 0.8577, best train loss: 0.3767  @ epoch 8213 )\n",
      "[Epoch: 8240] train loss: 0.3960, train acc: 0.8329, val loss: 0.3479, val acc: 0.8530  (best train acc: 0.8417, best val acc: 0.8577, best train loss: 0.3767  @ epoch 8213 )\n",
      "[Epoch: 8260] train loss: 0.4240, train acc: 0.8237, val loss: 0.3413, val acc: 0.8556  (best train acc: 0.8417, best val acc: 0.8577, best train loss: 0.3767  @ epoch 8213 )\n",
      "[Epoch: 8280] train loss: 0.4173, train acc: 0.8242, val loss: 0.3450, val acc: 0.8523  (best train acc: 0.8417, best val acc: 0.8577, best train loss: 0.3767  @ epoch 8213 )\n",
      "[Epoch: 8300] train loss: 0.4230, train acc: 0.8177, val loss: 0.3443, val acc: 0.8546  (best train acc: 0.8438, best val acc: 0.8577, best train loss: 0.3767  @ epoch 8213 )\n",
      "[Epoch: 8320] train loss: 0.4067, train acc: 0.8284, val loss: 0.3894, val acc: 0.8293  (best train acc: 0.8438, best val acc: 0.8577, best train loss: 0.3767  @ epoch 8213 )\n",
      "[Epoch: 8340] train loss: 0.4130, train acc: 0.8292, val loss: 0.3697, val acc: 0.8422  (best train acc: 0.8438, best val acc: 0.8577, best train loss: 0.3767  @ epoch 8213 )\n",
      "[Epoch: 8360] train loss: 0.4190, train acc: 0.8268, val loss: 0.3483, val acc: 0.8506  (best train acc: 0.8438, best val acc: 0.8580, best train loss: 0.3767  @ epoch 8213 )\n",
      "[Epoch: 8380] train loss: 0.3995, train acc: 0.8322, val loss: 0.3970, val acc: 0.8270  (best train acc: 0.8438, best val acc: 0.8580, best train loss: 0.3767  @ epoch 8213 )\n",
      "[Epoch: 8400] train loss: 0.4101, train acc: 0.8336, val loss: 0.3929, val acc: 0.8209  (best train acc: 0.8438, best val acc: 0.8580, best train loss: 0.3767  @ epoch 8213 )\n",
      "[Epoch: 8420] train loss: 0.4382, train acc: 0.8145, val loss: 0.3760, val acc: 0.8287  (best train acc: 0.8438, best val acc: 0.8580, best train loss: 0.3767  @ epoch 8213 )\n",
      "[Epoch: 8440] train loss: 0.3889, train acc: 0.8386, val loss: 0.3456, val acc: 0.8546  (best train acc: 0.8438, best val acc: 0.8580, best train loss: 0.3767  @ epoch 8213 )\n",
      "[Epoch: 8460] train loss: 0.4097, train acc: 0.8219, val loss: 0.3407, val acc: 0.8543  (best train acc: 0.8438, best val acc: 0.8590, best train loss: 0.3767  @ epoch 8213 )\n",
      "[Epoch: 8480] train loss: 0.4052, train acc: 0.8282, val loss: 0.3472, val acc: 0.8503  (best train acc: 0.8438, best val acc: 0.8590, best train loss: 0.3711  @ epoch 8479 )\n",
      "[Epoch: 8500] train loss: 0.4185, train acc: 0.8300, val loss: 0.3640, val acc: 0.8452  (best train acc: 0.8438, best val acc: 0.8590, best train loss: 0.3711  @ epoch 8499 )\n",
      "[Epoch: 8520] train loss: 0.4071, train acc: 0.8326, val loss: 0.3635, val acc: 0.8371  (best train acc: 0.8438, best val acc: 0.8590, best train loss: 0.3711  @ epoch 8499 )\n",
      "[Epoch: 8540] train loss: 0.4549, train acc: 0.8073, val loss: 0.3628, val acc: 0.8536  (best train acc: 0.8438, best val acc: 0.8590, best train loss: 0.3711  @ epoch 8499 )\n",
      "[Epoch: 8560] train loss: 0.4243, train acc: 0.8218, val loss: 0.3529, val acc: 0.8476  (best train acc: 0.8511, best val acc: 0.8590, best train loss: 0.3711  @ epoch 8499 )\n",
      "[Epoch: 8580] train loss: 0.4307, train acc: 0.8209, val loss: 0.3418, val acc: 0.8567  (best train acc: 0.8511, best val acc: 0.8590, best train loss: 0.3711  @ epoch 8499 )\n",
      "[Epoch: 8600] train loss: 0.4593, train acc: 0.7992, val loss: 0.3539, val acc: 0.8496  (best train acc: 0.8511, best val acc: 0.8590, best train loss: 0.3711  @ epoch 8499 )\n",
      "[Epoch: 8620] train loss: 0.4350, train acc: 0.8221, val loss: 0.3698, val acc: 0.8405  (best train acc: 0.8511, best val acc: 0.8590, best train loss: 0.3711  @ epoch 8499 )\n",
      "[Epoch: 8640] train loss: 0.3954, train acc: 0.8369, val loss: 0.3476, val acc: 0.8486  (best train acc: 0.8511, best val acc: 0.8590, best train loss: 0.3711  @ epoch 8499 )\n",
      "[Epoch: 8660] train loss: 0.4304, train acc: 0.8191, val loss: 0.3439, val acc: 0.8479  (best train acc: 0.8511, best val acc: 0.8597, best train loss: 0.3711  @ epoch 8499 )\n",
      "[Epoch: 8680] train loss: 0.4121, train acc: 0.8235, val loss: 0.3355, val acc: 0.8560  (best train acc: 0.8511, best val acc: 0.8597, best train loss: 0.3711  @ epoch 8499 )\n",
      "[Epoch: 8700] train loss: 0.3979, train acc: 0.8279, val loss: 0.3899, val acc: 0.8266  (best train acc: 0.8511, best val acc: 0.8597, best train loss: 0.3711  @ epoch 8499 )\n",
      "[Epoch: 8720] train loss: 0.4247, train acc: 0.8182, val loss: 0.3453, val acc: 0.8503  (best train acc: 0.8511, best val acc: 0.8597, best train loss: 0.3711  @ epoch 8499 )\n",
      "[Epoch: 8740] train loss: 0.4062, train acc: 0.8336, val loss: 0.3594, val acc: 0.8546  (best train acc: 0.8511, best val acc: 0.8597, best train loss: 0.3711  @ epoch 8499 )\n",
      "[Epoch: 8760] train loss: 0.4527, train acc: 0.8026, val loss: 0.3620, val acc: 0.8405  (best train acc: 0.8511, best val acc: 0.8597, best train loss: 0.3711  @ epoch 8499 )\n",
      "[Epoch: 8780] train loss: 0.4938, train acc: 0.7841, val loss: 0.3543, val acc: 0.8513  (best train acc: 0.8511, best val acc: 0.8597, best train loss: 0.3711  @ epoch 8499 )\n",
      "[Epoch: 8800] train loss: 0.4304, train acc: 0.8218, val loss: 0.3486, val acc: 0.8553  (best train acc: 0.8511, best val acc: 0.8597, best train loss: 0.3711  @ epoch 8499 )\n",
      "[Epoch: 8820] train loss: 0.3899, train acc: 0.8384, val loss: 0.3432, val acc: 0.8580  (best train acc: 0.8511, best val acc: 0.8597, best train loss: 0.3711  @ epoch 8499 )\n",
      "[Epoch: 8840] train loss: 0.4299, train acc: 0.8167, val loss: 0.3445, val acc: 0.8492  (best train acc: 0.8511, best val acc: 0.8597, best train loss: 0.3711  @ epoch 8499 )\n",
      "[Epoch: 8860] train loss: 0.3848, train acc: 0.8389, val loss: 0.3468, val acc: 0.8553  (best train acc: 0.8511, best val acc: 0.8597, best train loss: 0.3711  @ epoch 8499 )\n",
      "[Epoch: 8880] train loss: 0.4258, train acc: 0.8201, val loss: 0.3551, val acc: 0.8479  (best train acc: 0.8511, best val acc: 0.8597, best train loss: 0.3711  @ epoch 8499 )\n",
      "[Epoch: 8900] train loss: 0.3796, train acc: 0.8401, val loss: 0.3384, val acc: 0.8583  (best train acc: 0.8511, best val acc: 0.8597, best train loss: 0.3711  @ epoch 8499 )\n",
      "[Epoch: 8920] train loss: 0.4050, train acc: 0.8270, val loss: 0.3450, val acc: 0.8563  (best train acc: 0.8511, best val acc: 0.8600, best train loss: 0.3711  @ epoch 8499 )\n",
      "[Epoch: 8940] train loss: 0.4146, train acc: 0.8193, val loss: 0.3676, val acc: 0.8418  (best train acc: 0.8511, best val acc: 0.8607, best train loss: 0.3711  @ epoch 8499 )\n",
      "[Epoch: 8960] train loss: 0.4106, train acc: 0.8240, val loss: 0.3504, val acc: 0.8472  (best train acc: 0.8511, best val acc: 0.8607, best train loss: 0.3711  @ epoch 8499 )\n",
      "[Epoch: 8980] train loss: 0.3759, train acc: 0.8412, val loss: 0.3343, val acc: 0.8580  (best train acc: 0.8511, best val acc: 0.8607, best train loss: 0.3711  @ epoch 8499 )\n",
      "[Epoch: 9000] train loss: 0.4516, train acc: 0.8068, val loss: 0.3532, val acc: 0.8482  (best train acc: 0.8511, best val acc: 0.8614, best train loss: 0.3711  @ epoch 8499 )\n",
      "[Epoch: 9020] train loss: 0.3823, train acc: 0.8422, val loss: 0.3510, val acc: 0.8540  (best train acc: 0.8511, best val acc: 0.8617, best train loss: 0.3711  @ epoch 8499 )\n",
      "[Epoch: 9040] train loss: 0.4132, train acc: 0.8292, val loss: 0.3482, val acc: 0.8523  (best train acc: 0.8511, best val acc: 0.8648, best train loss: 0.3711  @ epoch 8499 )\n",
      "[Epoch: 9060] train loss: 0.3838, train acc: 0.8344, val loss: 0.3438, val acc: 0.8536  (best train acc: 0.8511, best val acc: 0.8648, best train loss: 0.3711  @ epoch 8499 )\n",
      "[Epoch: 9080] train loss: 0.4110, train acc: 0.8248, val loss: 0.3567, val acc: 0.8523  (best train acc: 0.8511, best val acc: 0.8648, best train loss: 0.3711  @ epoch 8499 )\n",
      "[Epoch: 9100] train loss: 0.4270, train acc: 0.8195, val loss: 0.3522, val acc: 0.8503  (best train acc: 0.8511, best val acc: 0.8648, best train loss: 0.3711  @ epoch 8499 )\n",
      "[Epoch: 9120] train loss: 0.3727, train acc: 0.8406, val loss: 0.3485, val acc: 0.8530  (best train acc: 0.8511, best val acc: 0.8648, best train loss: 0.3711  @ epoch 8499 )\n",
      "[Epoch: 9140] train loss: 0.4058, train acc: 0.8257, val loss: 0.3553, val acc: 0.8422  (best train acc: 0.8511, best val acc: 0.8648, best train loss: 0.3711  @ epoch 8499 )\n",
      "[Epoch: 9160] train loss: 0.4045, train acc: 0.8323, val loss: 0.3439, val acc: 0.8513  (best train acc: 0.8511, best val acc: 0.8648, best train loss: 0.3711  @ epoch 8499 )\n",
      "[Epoch: 9180] train loss: 0.4316, train acc: 0.8153, val loss: 0.3406, val acc: 0.8540  (best train acc: 0.8511, best val acc: 0.8648, best train loss: 0.3711  @ epoch 8499 )\n",
      "[Epoch: 9200] train loss: 0.4011, train acc: 0.8314, val loss: 0.3468, val acc: 0.8509  (best train acc: 0.8511, best val acc: 0.8648, best train loss: 0.3711  @ epoch 8499 )\n",
      "[Epoch: 9220] train loss: 0.3935, train acc: 0.8386, val loss: 0.3457, val acc: 0.8503  (best train acc: 0.8511, best val acc: 0.8648, best train loss: 0.3711  @ epoch 8499 )\n",
      "[Epoch: 9240] train loss: 0.4028, train acc: 0.8244, val loss: 0.3413, val acc: 0.8573  (best train acc: 0.8511, best val acc: 0.8648, best train loss: 0.3711  @ epoch 8499 )\n",
      "[Epoch: 9260] train loss: 0.3708, train acc: 0.8422, val loss: 0.3631, val acc: 0.8469  (best train acc: 0.8511, best val acc: 0.8648, best train loss: 0.3698  @ epoch 9243 )\n",
      "[Epoch: 9280] train loss: 0.4202, train acc: 0.8237, val loss: 0.3341, val acc: 0.8570  (best train acc: 0.8511, best val acc: 0.8648, best train loss: 0.3695  @ epoch 9272 )\n",
      "[Epoch: 9300] train loss: 0.4473, train acc: 0.8081, val loss: 0.3265, val acc: 0.8610  (best train acc: 0.8511, best val acc: 0.8648, best train loss: 0.3695  @ epoch 9272 )\n",
      "[Epoch: 9320] train loss: 0.4271, train acc: 0.8159, val loss: 0.3411, val acc: 0.8462  (best train acc: 0.8511, best val acc: 0.8661, best train loss: 0.3695  @ epoch 9272 )\n",
      "[Epoch: 9340] train loss: 0.3854, train acc: 0.8409, val loss: 0.3311, val acc: 0.8587  (best train acc: 0.8511, best val acc: 0.8661, best train loss: 0.3695  @ epoch 9272 )\n",
      "[Epoch: 9360] train loss: 0.4327, train acc: 0.8086, val loss: 0.3544, val acc: 0.8395  (best train acc: 0.8511, best val acc: 0.8661, best train loss: 0.3695  @ epoch 9272 )\n",
      "[Epoch: 9380] train loss: 0.4000, train acc: 0.8287, val loss: 0.3363, val acc: 0.8580  (best train acc: 0.8511, best val acc: 0.8661, best train loss: 0.3695  @ epoch 9272 )\n",
      "[Epoch: 9400] train loss: 0.4200, train acc: 0.8139, val loss: 0.3520, val acc: 0.8479  (best train acc: 0.8511, best val acc: 0.8661, best train loss: 0.3662  @ epoch 9387 )\n",
      "[Epoch: 9420] train loss: 0.3954, train acc: 0.8310, val loss: 0.3336, val acc: 0.8597  (best train acc: 0.8511, best val acc: 0.8661, best train loss: 0.3662  @ epoch 9387 )\n",
      "[Epoch: 9440] train loss: 0.4104, train acc: 0.8239, val loss: 0.3399, val acc: 0.8607  (best train acc: 0.8511, best val acc: 0.8661, best train loss: 0.3662  @ epoch 9387 )\n",
      "[Epoch: 9460] train loss: 0.3894, train acc: 0.8383, val loss: 0.3355, val acc: 0.8617  (best train acc: 0.8511, best val acc: 0.8661, best train loss: 0.3662  @ epoch 9387 )\n",
      "[Epoch: 9480] train loss: 0.3851, train acc: 0.8373, val loss: 0.3358, val acc: 0.8610  (best train acc: 0.8511, best val acc: 0.8661, best train loss: 0.3648  @ epoch 9465 )\n",
      "[Epoch: 9500] train loss: 0.4592, train acc: 0.7992, val loss: 0.3365, val acc: 0.8587  (best train acc: 0.8511, best val acc: 0.8661, best train loss: 0.3648  @ epoch 9465 )\n",
      "[Epoch: 9520] train loss: 0.3973, train acc: 0.8330, val loss: 0.3334, val acc: 0.8580  (best train acc: 0.8511, best val acc: 0.8661, best train loss: 0.3616  @ epoch 9501 )\n",
      "[Epoch: 9540] train loss: 0.4182, train acc: 0.8264, val loss: 0.3359, val acc: 0.8550  (best train acc: 0.8511, best val acc: 0.8661, best train loss: 0.3616  @ epoch 9501 )\n",
      "[Epoch: 9560] train loss: 0.4085, train acc: 0.8302, val loss: 0.3363, val acc: 0.8513  (best train acc: 0.8511, best val acc: 0.8661, best train loss: 0.3616  @ epoch 9501 )\n",
      "[Epoch: 9580] train loss: 0.3656, train acc: 0.8454, val loss: 0.3330, val acc: 0.8546  (best train acc: 0.8511, best val acc: 0.8661, best train loss: 0.3616  @ epoch 9501 )\n",
      "[Epoch: 9600] train loss: 0.3809, train acc: 0.8389, val loss: 0.3296, val acc: 0.8563  (best train acc: 0.8511, best val acc: 0.8661, best train loss: 0.3616  @ epoch 9501 )\n",
      "[Epoch: 9620] train loss: 0.4077, train acc: 0.8306, val loss: 0.3452, val acc: 0.8503  (best train acc: 0.8511, best val acc: 0.8661, best train loss: 0.3616  @ epoch 9501 )\n",
      "[Epoch: 9640] train loss: 0.4029, train acc: 0.8300, val loss: 0.3735, val acc: 0.8428  (best train acc: 0.8511, best val acc: 0.8661, best train loss: 0.3616  @ epoch 9501 )\n",
      "[Epoch: 9660] train loss: 0.4260, train acc: 0.8208, val loss: 0.3625, val acc: 0.8486  (best train acc: 0.8511, best val acc: 0.8661, best train loss: 0.3616  @ epoch 9501 )\n",
      "[Epoch: 9680] train loss: 0.4412, train acc: 0.8096, val loss: 0.3420, val acc: 0.8577  (best train acc: 0.8511, best val acc: 0.8661, best train loss: 0.3616  @ epoch 9501 )\n",
      "[Epoch: 9700] train loss: 0.3900, train acc: 0.8362, val loss: 0.3342, val acc: 0.8583  (best train acc: 0.8511, best val acc: 0.8661, best train loss: 0.3616  @ epoch 9501 )\n",
      "[Epoch: 9720] train loss: 0.3868, train acc: 0.8344, val loss: 0.3403, val acc: 0.8590  (best train acc: 0.8511, best val acc: 0.8661, best train loss: 0.3616  @ epoch 9501 )\n",
      "[Epoch: 9740] train loss: 0.3790, train acc: 0.8365, val loss: 0.3416, val acc: 0.8530  (best train acc: 0.8511, best val acc: 0.8661, best train loss: 0.3611  @ epoch 9725 )\n",
      "[Epoch: 9760] train loss: 0.4474, train acc: 0.8053, val loss: 0.3496, val acc: 0.8496  (best train acc: 0.8511, best val acc: 0.8661, best train loss: 0.3611  @ epoch 9725 )\n",
      "[Epoch: 9780] train loss: 0.3777, train acc: 0.8425, val loss: 0.3384, val acc: 0.8590  (best train acc: 0.8516, best val acc: 0.8661, best train loss: 0.3562  @ epoch 9776 )\n",
      "[Epoch: 9800] train loss: 0.4175, train acc: 0.8274, val loss: 0.3314, val acc: 0.8600  (best train acc: 0.8516, best val acc: 0.8661, best train loss: 0.3562  @ epoch 9776 )\n",
      "[Epoch: 9820] train loss: 0.4365, train acc: 0.8137, val loss: 0.3641, val acc: 0.8371  (best train acc: 0.8516, best val acc: 0.8661, best train loss: 0.3562  @ epoch 9776 )\n",
      "[Epoch: 9840] train loss: 0.3911, train acc: 0.8287, val loss: 0.3658, val acc: 0.8398  (best train acc: 0.8516, best val acc: 0.8661, best train loss: 0.3562  @ epoch 9776 )\n",
      "[Epoch: 9860] train loss: 0.4008, train acc: 0.8278, val loss: 0.3523, val acc: 0.8482  (best train acc: 0.8516, best val acc: 0.8661, best train loss: 0.3562  @ epoch 9776 )\n",
      "[Epoch: 9880] train loss: 0.3955, train acc: 0.8300, val loss: 0.3304, val acc: 0.8600  (best train acc: 0.8516, best val acc: 0.8661, best train loss: 0.3562  @ epoch 9776 )\n",
      "[Epoch: 9900] train loss: 0.3925, train acc: 0.8319, val loss: 0.3539, val acc: 0.8465  (best train acc: 0.8516, best val acc: 0.8661, best train loss: 0.3562  @ epoch 9776 )\n",
      "[Epoch: 9920] train loss: 0.3655, train acc: 0.8424, val loss: 0.3354, val acc: 0.8590  (best train acc: 0.8516, best val acc: 0.8661, best train loss: 0.3562  @ epoch 9776 )\n",
      "[Epoch: 9940] train loss: 0.3919, train acc: 0.8359, val loss: 0.3342, val acc: 0.8583  (best train acc: 0.8516, best val acc: 0.8661, best train loss: 0.3562  @ epoch 9776 )\n",
      "[Epoch: 9960] train loss: 0.3958, train acc: 0.8313, val loss: 0.3291, val acc: 0.8573  (best train acc: 0.8516, best val acc: 0.8661, best train loss: 0.3562  @ epoch 9776 )\n",
      "[Epoch: 9980] train loss: 0.3711, train acc: 0.8430, val loss: 0.3281, val acc: 0.8614  (best train acc: 0.8516, best val acc: 0.8661, best train loss: 0.3562  @ epoch 9776 )\n",
      "[Epoch: 10000] train loss: 0.3781, train acc: 0.8430, val loss: 0.3296, val acc: 0.8610  (best train acc: 0.8516, best val acc: 0.8661, best train loss: 0.3562  @ epoch 9776 )\n",
      "[Epoch: 10020] train loss: 0.4192, train acc: 0.8209, val loss: 0.3654, val acc: 0.8415  (best train acc: 0.8516, best val acc: 0.8661, best train loss: 0.3562  @ epoch 9776 )\n",
      "[Epoch: 10040] train loss: 0.3895, train acc: 0.8370, val loss: 0.3347, val acc: 0.8600  (best train acc: 0.8519, best val acc: 0.8661, best train loss: 0.3562  @ epoch 9776 )\n",
      "[Epoch: 10060] train loss: 0.3850, train acc: 0.8406, val loss: 0.3407, val acc: 0.8597  (best train acc: 0.8519, best val acc: 0.8661, best train loss: 0.3562  @ epoch 9776 )\n",
      "[Epoch: 10080] train loss: 0.4428, train acc: 0.8079, val loss: 0.3494, val acc: 0.8567  (best train acc: 0.8519, best val acc: 0.8661, best train loss: 0.3562  @ epoch 9776 )\n",
      "[Epoch: 10100] train loss: 0.4134, train acc: 0.8278, val loss: 0.3667, val acc: 0.8361  (best train acc: 0.8519, best val acc: 0.8661, best train loss: 0.3562  @ epoch 9776 )\n",
      "[Epoch: 10120] train loss: 0.3623, train acc: 0.8454, val loss: 0.3547, val acc: 0.8445  (best train acc: 0.8519, best val acc: 0.8661, best train loss: 0.3562  @ epoch 9776 )\n",
      "[Epoch: 10140] train loss: 0.4011, train acc: 0.8349, val loss: 0.3284, val acc: 0.8624  (best train acc: 0.8519, best val acc: 0.8691, best train loss: 0.3562  @ epoch 9776 )\n",
      "[Epoch: 10160] train loss: 0.4075, train acc: 0.8237, val loss: 0.3294, val acc: 0.8617  (best train acc: 0.8519, best val acc: 0.8691, best train loss: 0.3562  @ epoch 9776 )\n",
      "[Epoch: 10180] train loss: 0.4014, train acc: 0.8281, val loss: 0.3461, val acc: 0.8496  (best train acc: 0.8519, best val acc: 0.8691, best train loss: 0.3562  @ epoch 9776 )\n",
      "[Epoch: 10200] train loss: 0.3969, train acc: 0.8351, val loss: 0.3484, val acc: 0.8530  (best train acc: 0.8519, best val acc: 0.8691, best train loss: 0.3562  @ epoch 9776 )\n",
      "[Epoch: 10220] train loss: 0.3959, train acc: 0.8347, val loss: 0.3351, val acc: 0.8648  (best train acc: 0.8519, best val acc: 0.8691, best train loss: 0.3562  @ epoch 9776 )\n",
      "[Epoch: 10240] train loss: 0.4128, train acc: 0.8223, val loss: 0.3383, val acc: 0.8577  (best train acc: 0.8519, best val acc: 0.8691, best train loss: 0.3562  @ epoch 9776 )\n",
      "[Epoch: 10260] train loss: 0.3779, train acc: 0.8428, val loss: 0.3580, val acc: 0.8432  (best train acc: 0.8519, best val acc: 0.8691, best train loss: 0.3562  @ epoch 9776 )\n",
      "[Epoch: 10280] train loss: 0.3751, train acc: 0.8463, val loss: 0.3598, val acc: 0.8432  (best train acc: 0.8519, best val acc: 0.8691, best train loss: 0.3562  @ epoch 9776 )\n",
      "[Epoch: 10300] train loss: 0.4054, train acc: 0.8286, val loss: 0.3454, val acc: 0.8496  (best train acc: 0.8519, best val acc: 0.8691, best train loss: 0.3562  @ epoch 9776 )\n",
      "[Epoch: 10320] train loss: 0.4101, train acc: 0.8240, val loss: 0.3294, val acc: 0.8610  (best train acc: 0.8519, best val acc: 0.8691, best train loss: 0.3562  @ epoch 9776 )\n",
      "[Epoch: 10340] train loss: 0.3908, train acc: 0.8359, val loss: 0.3389, val acc: 0.8533  (best train acc: 0.8519, best val acc: 0.8691, best train loss: 0.3562  @ epoch 9776 )\n",
      "[Epoch: 10360] train loss: 0.4130, train acc: 0.8270, val loss: 0.3379, val acc: 0.8583  (best train acc: 0.8519, best val acc: 0.8691, best train loss: 0.3562  @ epoch 9776 )\n",
      "[Epoch: 10380] train loss: 0.3798, train acc: 0.8426, val loss: 0.3270, val acc: 0.8600  (best train acc: 0.8519, best val acc: 0.8691, best train loss: 0.3562  @ epoch 9776 )\n",
      "[Epoch: 10400] train loss: 0.4610, train acc: 0.8003, val loss: 0.3388, val acc: 0.8560  (best train acc: 0.8519, best val acc: 0.8691, best train loss: 0.3562  @ epoch 9776 )\n",
      "[Epoch: 10420] train loss: 0.3776, train acc: 0.8438, val loss: 0.3418, val acc: 0.8597  (best train acc: 0.8519, best val acc: 0.8691, best train loss: 0.3562  @ epoch 9776 )\n",
      "[Epoch: 10440] train loss: 0.4224, train acc: 0.8220, val loss: 0.3709, val acc: 0.8358  (best train acc: 0.8519, best val acc: 0.8691, best train loss: 0.3562  @ epoch 9776 )\n",
      "[Epoch: 10460] train loss: 0.3637, train acc: 0.8459, val loss: 0.3360, val acc: 0.8573  (best train acc: 0.8519, best val acc: 0.8691, best train loss: 0.3562  @ epoch 9776 )\n",
      "[Epoch: 10480] train loss: 0.3757, train acc: 0.8411, val loss: 0.3838, val acc: 0.8273  (best train acc: 0.8519, best val acc: 0.8691, best train loss: 0.3562  @ epoch 9776 )\n",
      "[Epoch: 10500] train loss: 0.4537, train acc: 0.8011, val loss: 0.3443, val acc: 0.8509  (best train acc: 0.8519, best val acc: 0.8691, best train loss: 0.3562  @ epoch 9776 )\n",
      "[Epoch: 10520] train loss: 0.3849, train acc: 0.8403, val loss: 0.3439, val acc: 0.8492  (best train acc: 0.8519, best val acc: 0.8691, best train loss: 0.3562  @ epoch 9776 )\n",
      "[Epoch: 10540] train loss: 0.4088, train acc: 0.8325, val loss: 0.3290, val acc: 0.8607  (best train acc: 0.8519, best val acc: 0.8691, best train loss: 0.3562  @ epoch 9776 )\n",
      "[Epoch: 10560] train loss: 0.3887, train acc: 0.8397, val loss: 0.3423, val acc: 0.8573  (best train acc: 0.8519, best val acc: 0.8691, best train loss: 0.3562  @ epoch 9776 )\n",
      "[Epoch: 10580] train loss: 0.3779, train acc: 0.8402, val loss: 0.3375, val acc: 0.8553  (best train acc: 0.8519, best val acc: 0.8691, best train loss: 0.3562  @ epoch 9776 )\n",
      "[Epoch: 10600] train loss: 0.4077, train acc: 0.8282, val loss: 0.3186, val acc: 0.8654  (best train acc: 0.8519, best val acc: 0.8691, best train loss: 0.3562  @ epoch 9776 )\n",
      "[Epoch: 10620] train loss: 0.4408, train acc: 0.8096, val loss: 0.3382, val acc: 0.8583  (best train acc: 0.8519, best val acc: 0.8691, best train loss: 0.3562  @ epoch 9776 )\n",
      "[Epoch: 10640] train loss: 0.3968, train acc: 0.8363, val loss: 0.3272, val acc: 0.8610  (best train acc: 0.8519, best val acc: 0.8691, best train loss: 0.3562  @ epoch 9776 )\n",
      "[Epoch: 10660] train loss: 0.3957, train acc: 0.8323, val loss: 0.3191, val acc: 0.8664  (best train acc: 0.8519, best val acc: 0.8691, best train loss: 0.3562  @ epoch 9776 )\n",
      "[Epoch: 10680] train loss: 0.3723, train acc: 0.8383, val loss: 0.3199, val acc: 0.8637  (best train acc: 0.8519, best val acc: 0.8691, best train loss: 0.3534  @ epoch 10665 )\n",
      "[Epoch: 10700] train loss: 0.3958, train acc: 0.8342, val loss: 0.3273, val acc: 0.8567  (best train acc: 0.8528, best val acc: 0.8691, best train loss: 0.3534  @ epoch 10665 )\n",
      "[Epoch: 10720] train loss: 0.3837, train acc: 0.8325, val loss: 0.3493, val acc: 0.8455  (best train acc: 0.8549, best val acc: 0.8691, best train loss: 0.3534  @ epoch 10665 )\n",
      "[Epoch: 10740] train loss: 0.3944, train acc: 0.8285, val loss: 0.3252, val acc: 0.8600  (best train acc: 0.8549, best val acc: 0.8691, best train loss: 0.3534  @ epoch 10665 )\n",
      "[Epoch: 10760] train loss: 0.3828, train acc: 0.8369, val loss: 0.3212, val acc: 0.8624  (best train acc: 0.8549, best val acc: 0.8691, best train loss: 0.3534  @ epoch 10665 )\n",
      "[Epoch: 10780] train loss: 0.3715, train acc: 0.8415, val loss: 0.3349, val acc: 0.8614  (best train acc: 0.8549, best val acc: 0.8691, best train loss: 0.3534  @ epoch 10665 )\n",
      "[Epoch: 10800] train loss: 0.3689, train acc: 0.8420, val loss: 0.3244, val acc: 0.8610  (best train acc: 0.8549, best val acc: 0.8691, best train loss: 0.3534  @ epoch 10665 )\n",
      "[Epoch: 10820] train loss: 0.4121, train acc: 0.8344, val loss: 0.3262, val acc: 0.8627  (best train acc: 0.8549, best val acc: 0.8691, best train loss: 0.3534  @ epoch 10665 )\n",
      "[Epoch: 10840] train loss: 0.3769, train acc: 0.8407, val loss: 0.3249, val acc: 0.8607  (best train acc: 0.8549, best val acc: 0.8691, best train loss: 0.3534  @ epoch 10665 )\n",
      "[Epoch: 10860] train loss: 0.3697, train acc: 0.8430, val loss: 0.3189, val acc: 0.8668  (best train acc: 0.8549, best val acc: 0.8691, best train loss: 0.3534  @ epoch 10665 )\n",
      "[Epoch: 10880] train loss: 0.3652, train acc: 0.8438, val loss: 0.3383, val acc: 0.8546  (best train acc: 0.8549, best val acc: 0.8691, best train loss: 0.3534  @ epoch 10665 )\n",
      "[Epoch: 10900] train loss: 0.3777, train acc: 0.8425, val loss: 0.3360, val acc: 0.8573  (best train acc: 0.8549, best val acc: 0.8691, best train loss: 0.3534  @ epoch 10665 )\n",
      "[Epoch: 10920] train loss: 0.4212, train acc: 0.8229, val loss: 0.3289, val acc: 0.8614  (best train acc: 0.8594, best val acc: 0.8691, best train loss: 0.3524  @ epoch 10915 )\n",
      "[Epoch: 10940] train loss: 0.4255, train acc: 0.8254, val loss: 0.3250, val acc: 0.8651  (best train acc: 0.8594, best val acc: 0.8691, best train loss: 0.3524  @ epoch 10915 )\n",
      "[Epoch: 10960] train loss: 0.4994, train acc: 0.7819, val loss: 0.3348, val acc: 0.8573  (best train acc: 0.8594, best val acc: 0.8691, best train loss: 0.3524  @ epoch 10915 )\n",
      "[Epoch: 10980] train loss: 0.3767, train acc: 0.8397, val loss: 0.3392, val acc: 0.8553  (best train acc: 0.8594, best val acc: 0.8691, best train loss: 0.3524  @ epoch 10915 )\n",
      "[Epoch: 11000] train loss: 0.3865, train acc: 0.8380, val loss: 0.3389, val acc: 0.8573  (best train acc: 0.8594, best val acc: 0.8691, best train loss: 0.3524  @ epoch 10915 )\n",
      "[Epoch: 11020] train loss: 0.4185, train acc: 0.8162, val loss: 0.3549, val acc: 0.8492  (best train acc: 0.8594, best val acc: 0.8691, best train loss: 0.3524  @ epoch 10915 )\n",
      "[Epoch: 11040] train loss: 0.3777, train acc: 0.8416, val loss: 0.3354, val acc: 0.8553  (best train acc: 0.8594, best val acc: 0.8691, best train loss: 0.3524  @ epoch 10915 )\n",
      "[Epoch: 11060] train loss: 0.3812, train acc: 0.8399, val loss: 0.3316, val acc: 0.8553  (best train acc: 0.8594, best val acc: 0.8691, best train loss: 0.3524  @ epoch 10915 )\n",
      "[Epoch: 11080] train loss: 0.3862, train acc: 0.8360, val loss: 0.3422, val acc: 0.8560  (best train acc: 0.8594, best val acc: 0.8691, best train loss: 0.3524  @ epoch 10915 )\n",
      "[Epoch: 11100] train loss: 0.4280, train acc: 0.8098, val loss: 0.3495, val acc: 0.8486  (best train acc: 0.8594, best val acc: 0.8691, best train loss: 0.3524  @ epoch 10915 )\n",
      "[Epoch: 11120] train loss: 0.3836, train acc: 0.8368, val loss: 0.3308, val acc: 0.8590  (best train acc: 0.8594, best val acc: 0.8691, best train loss: 0.3524  @ epoch 10915 )\n",
      "[Epoch: 11140] train loss: 0.3833, train acc: 0.8297, val loss: 0.3233, val acc: 0.8631  (best train acc: 0.8594, best val acc: 0.8691, best train loss: 0.3524  @ epoch 10915 )\n",
      "[Epoch: 11160] train loss: 0.3896, train acc: 0.8352, val loss: 0.3262, val acc: 0.8631  (best train acc: 0.8594, best val acc: 0.8691, best train loss: 0.3502  @ epoch 11141 )\n",
      "[Epoch: 11180] train loss: 0.4001, train acc: 0.8301, val loss: 0.3258, val acc: 0.8634  (best train acc: 0.8594, best val acc: 0.8691, best train loss: 0.3502  @ epoch 11141 )\n",
      "[Epoch: 11200] train loss: 0.4045, train acc: 0.8235, val loss: 0.3370, val acc: 0.8553  (best train acc: 0.8594, best val acc: 0.8691, best train loss: 0.3475  @ epoch 11186 )\n",
      "[Epoch: 11220] train loss: 0.4238, train acc: 0.8158, val loss: 0.3596, val acc: 0.8398  (best train acc: 0.8594, best val acc: 0.8691, best train loss: 0.3475  @ epoch 11186 )\n",
      "[Epoch: 11240] train loss: 0.3760, train acc: 0.8404, val loss: 0.3407, val acc: 0.8553  (best train acc: 0.8594, best val acc: 0.8691, best train loss: 0.3475  @ epoch 11186 )\n",
      "[Epoch: 11260] train loss: 0.4003, train acc: 0.8283, val loss: 0.3174, val acc: 0.8648  (best train acc: 0.8594, best val acc: 0.8712, best train loss: 0.3475  @ epoch 11186 )\n",
      "[Epoch: 11280] train loss: 0.4151, train acc: 0.8254, val loss: 0.3395, val acc: 0.8519  (best train acc: 0.8594, best val acc: 0.8712, best train loss: 0.3475  @ epoch 11186 )\n",
      "[Epoch: 11300] train loss: 0.3603, train acc: 0.8486, val loss: 0.3286, val acc: 0.8624  (best train acc: 0.8594, best val acc: 0.8712, best train loss: 0.3475  @ epoch 11186 )\n",
      "[Epoch: 11320] train loss: 0.3643, train acc: 0.8494, val loss: 0.3312, val acc: 0.8583  (best train acc: 0.8594, best val acc: 0.8712, best train loss: 0.3431  @ epoch 11303 )\n",
      "[Epoch: 11340] train loss: 0.3663, train acc: 0.8394, val loss: 0.3208, val acc: 0.8621  (best train acc: 0.8594, best val acc: 0.8712, best train loss: 0.3431  @ epoch 11303 )\n",
      "[Epoch: 11360] train loss: 0.4159, train acc: 0.8147, val loss: 0.3320, val acc: 0.8570  (best train acc: 0.8594, best val acc: 0.8712, best train loss: 0.3431  @ epoch 11303 )\n",
      "[Epoch: 11380] train loss: 0.3708, train acc: 0.8446, val loss: 0.3104, val acc: 0.8678  (best train acc: 0.8594, best val acc: 0.8712, best train loss: 0.3431  @ epoch 11303 )\n",
      "[Epoch: 11400] train loss: 0.4077, train acc: 0.8312, val loss: 0.3361, val acc: 0.8580  (best train acc: 0.8594, best val acc: 0.8712, best train loss: 0.3431  @ epoch 11303 )\n",
      "[Epoch: 11420] train loss: 0.3710, train acc: 0.8423, val loss: 0.3166, val acc: 0.8648  (best train acc: 0.8594, best val acc: 0.8712, best train loss: 0.3431  @ epoch 11303 )\n",
      "[Epoch: 11440] train loss: 0.3639, train acc: 0.8436, val loss: 0.3304, val acc: 0.8573  (best train acc: 0.8594, best val acc: 0.8712, best train loss: 0.3431  @ epoch 11303 )\n",
      "[Epoch: 11460] train loss: 0.3876, train acc: 0.8336, val loss: 0.3279, val acc: 0.8590  (best train acc: 0.8594, best val acc: 0.8712, best train loss: 0.3431  @ epoch 11303 )\n",
      "[Epoch: 11480] train loss: 0.3640, train acc: 0.8453, val loss: 0.3098, val acc: 0.8644  (best train acc: 0.8594, best val acc: 0.8712, best train loss: 0.3431  @ epoch 11303 )\n",
      "[Epoch: 11500] train loss: 0.3988, train acc: 0.8319, val loss: 0.3246, val acc: 0.8637  (best train acc: 0.8594, best val acc: 0.8712, best train loss: 0.3431  @ epoch 11303 )\n",
      "[Epoch: 11520] train loss: 0.3959, train acc: 0.8323, val loss: 0.3538, val acc: 0.8465  (best train acc: 0.8594, best val acc: 0.8712, best train loss: 0.3431  @ epoch 11303 )\n",
      "[Epoch: 11540] train loss: 0.3784, train acc: 0.8395, val loss: 0.3481, val acc: 0.8506  (best train acc: 0.8594, best val acc: 0.8712, best train loss: 0.3431  @ epoch 11303 )\n",
      "[Epoch: 11560] train loss: 0.3928, train acc: 0.8317, val loss: 0.3318, val acc: 0.8546  (best train acc: 0.8594, best val acc: 0.8712, best train loss: 0.3431  @ epoch 11303 )\n",
      "[Epoch: 11580] train loss: 0.3996, train acc: 0.8352, val loss: 0.3224, val acc: 0.8631  (best train acc: 0.8594, best val acc: 0.8712, best train loss: 0.3431  @ epoch 11303 )\n",
      "[Epoch: 11600] train loss: 0.3532, train acc: 0.8463, val loss: 0.3240, val acc: 0.8654  (best train acc: 0.8594, best val acc: 0.8712, best train loss: 0.3431  @ epoch 11303 )\n",
      "[Epoch: 11620] train loss: 0.4087, train acc: 0.8222, val loss: 0.3230, val acc: 0.8604  (best train acc: 0.8594, best val acc: 0.8712, best train loss: 0.3431  @ epoch 11303 )\n",
      "[Epoch: 11640] train loss: 0.3707, train acc: 0.8433, val loss: 0.3339, val acc: 0.8546  (best train acc: 0.8594, best val acc: 0.8712, best train loss: 0.3431  @ epoch 11303 )\n",
      "[Epoch: 11660] train loss: 0.3917, train acc: 0.8324, val loss: 0.3367, val acc: 0.8570  (best train acc: 0.8594, best val acc: 0.8712, best train loss: 0.3431  @ epoch 11303 )\n",
      "[Epoch: 11680] train loss: 0.3954, train acc: 0.8284, val loss: 0.3064, val acc: 0.8607  (best train acc: 0.8594, best val acc: 0.8712, best train loss: 0.3431  @ epoch 11303 )\n",
      "[Epoch: 11700] train loss: 0.4374, train acc: 0.8108, val loss: 0.3366, val acc: 0.8570  (best train acc: 0.8594, best val acc: 0.8712, best train loss: 0.3431  @ epoch 11303 )\n",
      "[Epoch: 11720] train loss: 0.3837, train acc: 0.8406, val loss: 0.3193, val acc: 0.8705  (best train acc: 0.8594, best val acc: 0.8712, best train loss: 0.3431  @ epoch 11303 )\n",
      "[Epoch: 11740] train loss: 0.3721, train acc: 0.8405, val loss: 0.3235, val acc: 0.8583  (best train acc: 0.8594, best val acc: 0.8712, best train loss: 0.3431  @ epoch 11303 )\n",
      "[Epoch: 11760] train loss: 0.3548, train acc: 0.8451, val loss: 0.3386, val acc: 0.8496  (best train acc: 0.8594, best val acc: 0.8712, best train loss: 0.3431  @ epoch 11303 )\n",
      "[Epoch: 11780] train loss: 0.3475, train acc: 0.8527, val loss: 0.3042, val acc: 0.8634  (best train acc: 0.8594, best val acc: 0.8712, best train loss: 0.3374  @ epoch 11763 )\n",
      "[Epoch: 11800] train loss: 0.3482, train acc: 0.8496, val loss: 0.3056, val acc: 0.8661  (best train acc: 0.8615, best val acc: 0.8712, best train loss: 0.3342  @ epoch 11792 )\n",
      "[Epoch: 11820] train loss: 0.3616, train acc: 0.8457, val loss: 0.3244, val acc: 0.8553  (best train acc: 0.8615, best val acc: 0.8712, best train loss: 0.3342  @ epoch 11792 )\n",
      "[Epoch: 11840] train loss: 0.3497, train acc: 0.8464, val loss: 0.3073, val acc: 0.8695  (best train acc: 0.8615, best val acc: 0.8712, best train loss: 0.3331  @ epoch 11833 )\n",
      "[Epoch: 11860] train loss: 0.3726, train acc: 0.8411, val loss: 0.3356, val acc: 0.8533  (best train acc: 0.8615, best val acc: 0.8712, best train loss: 0.3331  @ epoch 11833 )\n",
      "[Epoch: 11880] train loss: 0.3449, train acc: 0.8516, val loss: 0.3143, val acc: 0.8678  (best train acc: 0.8615, best val acc: 0.8712, best train loss: 0.3331  @ epoch 11833 )\n",
      "[Epoch: 11900] train loss: 0.3273, train acc: 0.8611, val loss: 0.3134, val acc: 0.8644  (best train acc: 0.8615, best val acc: 0.8712, best train loss: 0.3273  @ epoch 11900 )\n",
      "[Epoch: 11920] train loss: 0.3795, train acc: 0.8355, val loss: 0.3711, val acc: 0.8317  (best train acc: 0.8615, best val acc: 0.8712, best train loss: 0.3269  @ epoch 11907 )\n",
      "[Epoch: 11940] train loss: 0.3447, train acc: 0.8524, val loss: 0.3192, val acc: 0.8664  (best train acc: 0.8615, best val acc: 0.8712, best train loss: 0.3269  @ epoch 11907 )\n",
      "[Epoch: 11960] train loss: 0.3752, train acc: 0.8409, val loss: 0.3443, val acc: 0.8513  (best train acc: 0.8615, best val acc: 0.8712, best train loss: 0.3269  @ epoch 11907 )\n",
      "[Epoch: 11980] train loss: 0.3592, train acc: 0.8456, val loss: 0.3080, val acc: 0.8658  (best train acc: 0.8615, best val acc: 0.8712, best train loss: 0.3269  @ epoch 11907 )\n",
      "[Epoch: 12000] train loss: 0.3441, train acc: 0.8553, val loss: 0.3133, val acc: 0.8637  (best train acc: 0.8615, best val acc: 0.8725, best train loss: 0.3269  @ epoch 11907 )\n",
      "[Epoch: 12020] train loss: 0.3583, train acc: 0.8475, val loss: 0.3106, val acc: 0.8678  (best train acc: 0.8618, best val acc: 0.8725, best train loss: 0.3269  @ epoch 11907 )\n",
      "[Epoch: 12040] train loss: 0.3400, train acc: 0.8522, val loss: 0.3083, val acc: 0.8681  (best train acc: 0.8618, best val acc: 0.8725, best train loss: 0.3269  @ epoch 11907 )\n",
      "[Epoch: 12060] train loss: 0.3611, train acc: 0.8458, val loss: 0.3266, val acc: 0.8570  (best train acc: 0.8618, best val acc: 0.8725, best train loss: 0.3269  @ epoch 11907 )\n",
      "[Epoch: 12080] train loss: 0.3857, train acc: 0.8316, val loss: 0.3241, val acc: 0.8627  (best train acc: 0.8618, best val acc: 0.8725, best train loss: 0.3269  @ epoch 11907 )\n",
      "[Epoch: 12100] train loss: 0.3450, train acc: 0.8550, val loss: 0.2984, val acc: 0.8675  (best train acc: 0.8618, best val acc: 0.8725, best train loss: 0.3269  @ epoch 11907 )\n",
      "[Epoch: 12120] train loss: 0.3345, train acc: 0.8593, val loss: 0.3250, val acc: 0.8580  (best train acc: 0.8634, best val acc: 0.8725, best train loss: 0.3269  @ epoch 11907 )\n",
      "[Epoch: 12140] train loss: 0.3498, train acc: 0.8547, val loss: 0.3107, val acc: 0.8634  (best train acc: 0.8634, best val acc: 0.8725, best train loss: 0.3269  @ epoch 11907 )\n",
      "[Epoch: 12160] train loss: 0.3527, train acc: 0.8494, val loss: 0.3115, val acc: 0.8641  (best train acc: 0.8650, best val acc: 0.8725, best train loss: 0.3269  @ epoch 11907 )\n",
      "[Epoch: 12180] train loss: 0.3431, train acc: 0.8621, val loss: 0.3083, val acc: 0.8621  (best train acc: 0.8650, best val acc: 0.8725, best train loss: 0.3269  @ epoch 11907 )\n",
      "[Epoch: 12200] train loss: 0.3436, train acc: 0.8562, val loss: 0.3123, val acc: 0.8735  (best train acc: 0.8650, best val acc: 0.8735, best train loss: 0.3245  @ epoch 12185 )\n",
      "[Epoch: 12220] train loss: 0.3617, train acc: 0.8454, val loss: 0.3077, val acc: 0.8691  (best train acc: 0.8650, best val acc: 0.8735, best train loss: 0.3245  @ epoch 12185 )\n",
      "[Epoch: 12240] train loss: 0.3488, train acc: 0.8536, val loss: 0.3145, val acc: 0.8671  (best train acc: 0.8650, best val acc: 0.8739, best train loss: 0.3245  @ epoch 12185 )\n",
      "[Epoch: 12260] train loss: 0.3476, train acc: 0.8524, val loss: 0.2999, val acc: 0.8664  (best train acc: 0.8658, best val acc: 0.8739, best train loss: 0.3245  @ epoch 12185 )\n",
      "[Epoch: 12280] train loss: 0.3366, train acc: 0.8581, val loss: 0.3034, val acc: 0.8675  (best train acc: 0.8658, best val acc: 0.8739, best train loss: 0.3194  @ epoch 12275 )\n",
      "[Epoch: 12300] train loss: 0.3957, train acc: 0.8366, val loss: 0.3084, val acc: 0.8702  (best train acc: 0.8658, best val acc: 0.8739, best train loss: 0.3194  @ epoch 12275 )\n",
      "[Epoch: 12320] train loss: 0.3250, train acc: 0.8657, val loss: 0.3206, val acc: 0.8664  (best train acc: 0.8658, best val acc: 0.8739, best train loss: 0.3194  @ epoch 12275 )\n",
      "[Epoch: 12340] train loss: 0.3595, train acc: 0.8489, val loss: 0.3190, val acc: 0.8644  (best train acc: 0.8658, best val acc: 0.8739, best train loss: 0.3194  @ epoch 12275 )\n",
      "[Epoch: 12360] train loss: 0.3281, train acc: 0.8612, val loss: 0.3002, val acc: 0.8695  (best train acc: 0.8658, best val acc: 0.8739, best train loss: 0.3194  @ epoch 12275 )\n",
      "[Epoch: 12380] train loss: 0.3575, train acc: 0.8560, val loss: 0.3119, val acc: 0.8648  (best train acc: 0.8658, best val acc: 0.8752, best train loss: 0.3194  @ epoch 12275 )\n",
      "[Epoch: 12400] train loss: 0.3408, train acc: 0.8478, val loss: 0.3011, val acc: 0.8702  (best train acc: 0.8658, best val acc: 0.8752, best train loss: 0.3194  @ epoch 12275 )\n",
      "[Epoch: 12420] train loss: 0.3246, train acc: 0.8643, val loss: 0.3045, val acc: 0.8678  (best train acc: 0.8658, best val acc: 0.8752, best train loss: 0.3194  @ epoch 12275 )\n",
      "[Epoch: 12440] train loss: 0.3620, train acc: 0.8459, val loss: 0.3122, val acc: 0.8648  (best train acc: 0.8658, best val acc: 0.8752, best train loss: 0.3194  @ epoch 12275 )\n",
      "[Epoch: 12460] train loss: 0.3577, train acc: 0.8488, val loss: 0.3153, val acc: 0.8631  (best train acc: 0.8658, best val acc: 0.8752, best train loss: 0.3194  @ epoch 12275 )\n",
      "[Epoch: 12480] train loss: 0.3456, train acc: 0.8574, val loss: 0.3346, val acc: 0.8516  (best train acc: 0.8658, best val acc: 0.8759, best train loss: 0.3194  @ epoch 12275 )\n",
      "[Epoch: 12500] train loss: 0.3486, train acc: 0.8548, val loss: 0.3067, val acc: 0.8695  (best train acc: 0.8658, best val acc: 0.8759, best train loss: 0.3194  @ epoch 12275 )\n",
      "[Epoch: 12520] train loss: 0.3246, train acc: 0.8663, val loss: 0.3118, val acc: 0.8661  (best train acc: 0.8683, best val acc: 0.8759, best train loss: 0.3190  @ epoch 12518 )\n",
      "[Epoch: 12540] train loss: 0.3547, train acc: 0.8584, val loss: 0.3022, val acc: 0.8648  (best train acc: 0.8683, best val acc: 0.8759, best train loss: 0.3190  @ epoch 12518 )\n",
      "[Epoch: 12560] train loss: 0.3389, train acc: 0.8519, val loss: 0.3058, val acc: 0.8691  (best train acc: 0.8683, best val acc: 0.8759, best train loss: 0.3190  @ epoch 12518 )\n",
      "[Epoch: 12580] train loss: 0.3467, train acc: 0.8532, val loss: 0.2965, val acc: 0.8668  (best train acc: 0.8683, best val acc: 0.8762, best train loss: 0.3190  @ epoch 12518 )\n",
      "[Epoch: 12600] train loss: 0.3683, train acc: 0.8442, val loss: 0.2983, val acc: 0.8691  (best train acc: 0.8683, best val acc: 0.8762, best train loss: 0.3190  @ epoch 12518 )\n",
      "[Epoch: 12620] train loss: 0.3836, train acc: 0.8383, val loss: 0.3071, val acc: 0.8651  (best train acc: 0.8683, best val acc: 0.8762, best train loss: 0.3190  @ epoch 12518 )\n",
      "[Epoch: 12640] train loss: 0.3448, train acc: 0.8598, val loss: 0.3138, val acc: 0.8637  (best train acc: 0.8683, best val acc: 0.8762, best train loss: 0.3190  @ epoch 12518 )\n",
      "[Epoch: 12660] train loss: 0.3268, train acc: 0.8610, val loss: 0.3035, val acc: 0.8681  (best train acc: 0.8683, best val acc: 0.8762, best train loss: 0.3190  @ epoch 12518 )\n",
      "[Epoch: 12680] train loss: 0.3472, train acc: 0.8546, val loss: 0.3067, val acc: 0.8654  (best train acc: 0.8683, best val acc: 0.8762, best train loss: 0.3190  @ epoch 12518 )\n",
      "[Epoch: 12700] train loss: 0.3454, train acc: 0.8508, val loss: 0.3065, val acc: 0.8671  (best train acc: 0.8710, best val acc: 0.8762, best train loss: 0.3180  @ epoch 12697 )\n",
      "[Epoch: 12720] train loss: 0.3785, train acc: 0.8434, val loss: 0.3045, val acc: 0.8691  (best train acc: 0.8710, best val acc: 0.8762, best train loss: 0.3171  @ epoch 12709 )\n",
      "[Epoch: 12740] train loss: 0.3426, train acc: 0.8578, val loss: 0.3116, val acc: 0.8641  (best train acc: 0.8710, best val acc: 0.8762, best train loss: 0.3171  @ epoch 12709 )\n",
      "[Epoch: 12760] train loss: 0.4158, train acc: 0.8279, val loss: 0.2996, val acc: 0.8712  (best train acc: 0.8710, best val acc: 0.8762, best train loss: 0.3171  @ epoch 12709 )\n",
      "[Epoch: 12780] train loss: 0.3382, train acc: 0.8566, val loss: 0.3069, val acc: 0.8685  (best train acc: 0.8710, best val acc: 0.8762, best train loss: 0.3171  @ epoch 12709 )\n",
      "[Epoch: 12800] train loss: 0.3461, train acc: 0.8596, val loss: 0.2989, val acc: 0.8691  (best train acc: 0.8710, best val acc: 0.8762, best train loss: 0.3171  @ epoch 12709 )\n",
      "[Epoch: 12820] train loss: 0.3597, train acc: 0.8498, val loss: 0.3341, val acc: 0.8519  (best train acc: 0.8710, best val acc: 0.8762, best train loss: 0.3171  @ epoch 12709 )\n",
      "[Epoch: 12840] train loss: 0.3454, train acc: 0.8527, val loss: 0.3122, val acc: 0.8634  (best train acc: 0.8710, best val acc: 0.8762, best train loss: 0.3171  @ epoch 12709 )\n",
      "[Epoch: 12860] train loss: 0.3434, train acc: 0.8555, val loss: 0.3032, val acc: 0.8718  (best train acc: 0.8710, best val acc: 0.8762, best train loss: 0.3171  @ epoch 12709 )\n",
      "[Epoch: 12880] train loss: 0.3541, train acc: 0.8483, val loss: 0.3192, val acc: 0.8648  (best train acc: 0.8710, best val acc: 0.8762, best train loss: 0.3171  @ epoch 12709 )\n",
      "[Epoch: 12900] train loss: 0.3673, train acc: 0.8444, val loss: 0.3047, val acc: 0.8678  (best train acc: 0.8710, best val acc: 0.8762, best train loss: 0.3171  @ epoch 12709 )\n",
      "[Epoch: 12920] train loss: 0.3255, train acc: 0.8578, val loss: 0.2977, val acc: 0.8634  (best train acc: 0.8710, best val acc: 0.8762, best train loss: 0.3134  @ epoch 12913 )\n",
      "[Epoch: 12940] train loss: 0.3072, train acc: 0.8722, val loss: 0.3057, val acc: 0.8678  (best train acc: 0.8722, best val acc: 0.8762, best train loss: 0.3072  @ epoch 12940 )\n",
      "[Epoch: 12960] train loss: 0.3332, train acc: 0.8594, val loss: 0.2943, val acc: 0.8695  (best train acc: 0.8722, best val acc: 0.8762, best train loss: 0.3072  @ epoch 12940 )\n",
      "[Epoch: 12980] train loss: 0.3296, train acc: 0.8588, val loss: 0.3175, val acc: 0.8621  (best train acc: 0.8722, best val acc: 0.8762, best train loss: 0.3072  @ epoch 12940 )\n",
      "[Epoch: 13000] train loss: 0.3755, train acc: 0.8472, val loss: 0.3061, val acc: 0.8624  (best train acc: 0.8722, best val acc: 0.8762, best train loss: 0.3072  @ epoch 12940 )\n",
      "[Epoch: 13020] train loss: 0.3403, train acc: 0.8548, val loss: 0.2981, val acc: 0.8725  (best train acc: 0.8722, best val acc: 0.8762, best train loss: 0.3072  @ epoch 12940 )\n",
      "[Epoch: 13040] train loss: 0.3392, train acc: 0.8530, val loss: 0.3061, val acc: 0.8668  (best train acc: 0.8722, best val acc: 0.8762, best train loss: 0.3072  @ epoch 12940 )\n",
      "[Epoch: 13060] train loss: 0.3806, train acc: 0.8372, val loss: 0.3008, val acc: 0.8695  (best train acc: 0.8722, best val acc: 0.8762, best train loss: 0.3072  @ epoch 12940 )\n",
      "[Epoch: 13080] train loss: 0.3335, train acc: 0.8603, val loss: 0.3025, val acc: 0.8705  (best train acc: 0.8722, best val acc: 0.8762, best train loss: 0.3072  @ epoch 12940 )\n",
      "[Epoch: 13100] train loss: 0.3598, train acc: 0.8540, val loss: 0.2944, val acc: 0.8732  (best train acc: 0.8722, best val acc: 0.8762, best train loss: 0.3072  @ epoch 12940 )\n",
      "[Epoch: 13120] train loss: 0.3269, train acc: 0.8600, val loss: 0.2963, val acc: 0.8742  (best train acc: 0.8722, best val acc: 0.8762, best train loss: 0.3072  @ epoch 12940 )\n",
      "[Epoch: 13140] train loss: 0.3809, train acc: 0.8460, val loss: 0.3074, val acc: 0.8695  (best train acc: 0.8728, best val acc: 0.8762, best train loss: 0.3072  @ epoch 12940 )\n",
      "[Epoch: 13160] train loss: 0.3365, train acc: 0.8576, val loss: 0.3054, val acc: 0.8722  (best train acc: 0.8728, best val acc: 0.8762, best train loss: 0.3072  @ epoch 12940 )\n",
      "[Epoch: 13180] train loss: 0.3309, train acc: 0.8598, val loss: 0.3218, val acc: 0.8580  (best train acc: 0.8728, best val acc: 0.8762, best train loss: 0.3072  @ epoch 12940 )\n",
      "[Epoch: 13200] train loss: 0.3325, train acc: 0.8612, val loss: 0.3003, val acc: 0.8698  (best train acc: 0.8728, best val acc: 0.8762, best train loss: 0.3072  @ epoch 12940 )\n",
      "[Epoch: 13220] train loss: 0.3263, train acc: 0.8623, val loss: 0.2960, val acc: 0.8739  (best train acc: 0.8728, best val acc: 0.8762, best train loss: 0.3072  @ epoch 12940 )\n",
      "[Epoch: 13240] train loss: 0.3696, train acc: 0.8447, val loss: 0.3114, val acc: 0.8671  (best train acc: 0.8728, best val acc: 0.8766, best train loss: 0.3072  @ epoch 12940 )\n",
      "[Epoch: 13260] train loss: 0.3201, train acc: 0.8720, val loss: 0.2931, val acc: 0.8742  (best train acc: 0.8728, best val acc: 0.8766, best train loss: 0.3072  @ epoch 12940 )\n",
      "[Epoch: 13280] train loss: 0.3948, train acc: 0.8346, val loss: 0.3016, val acc: 0.8644  (best train acc: 0.8728, best val acc: 0.8766, best train loss: 0.3072  @ epoch 12940 )\n",
      "[Epoch: 13300] train loss: 0.3347, train acc: 0.8568, val loss: 0.2955, val acc: 0.8722  (best train acc: 0.8728, best val acc: 0.8766, best train loss: 0.3072  @ epoch 12940 )\n",
      "[Epoch: 13320] train loss: 0.3616, train acc: 0.8481, val loss: 0.2877, val acc: 0.8752  (best train acc: 0.8728, best val acc: 0.8766, best train loss: 0.3072  @ epoch 12940 )\n",
      "[Epoch: 13340] train loss: 0.3296, train acc: 0.8569, val loss: 0.2994, val acc: 0.8691  (best train acc: 0.8728, best val acc: 0.8766, best train loss: 0.3072  @ epoch 12940 )\n",
      "[Epoch: 13360] train loss: 0.3421, train acc: 0.8526, val loss: 0.3017, val acc: 0.8722  (best train acc: 0.8728, best val acc: 0.8766, best train loss: 0.3072  @ epoch 12940 )\n",
      "[Epoch: 13380] train loss: 0.3549, train acc: 0.8530, val loss: 0.2969, val acc: 0.8759  (best train acc: 0.8728, best val acc: 0.8772, best train loss: 0.3072  @ epoch 12940 )\n",
      "[Epoch: 13400] train loss: 0.3469, train acc: 0.8584, val loss: 0.2952, val acc: 0.8732  (best train acc: 0.8728, best val acc: 0.8772, best train loss: 0.3072  @ epoch 12940 )\n",
      "[Epoch: 13420] train loss: 0.3417, train acc: 0.8567, val loss: 0.3042, val acc: 0.8695  (best train acc: 0.8728, best val acc: 0.8772, best train loss: 0.3072  @ epoch 12940 )\n",
      "[Epoch: 13440] train loss: 0.3227, train acc: 0.8633, val loss: 0.2956, val acc: 0.8691  (best train acc: 0.8728, best val acc: 0.8772, best train loss: 0.3072  @ epoch 12940 )\n",
      "[Epoch: 13460] train loss: 0.3394, train acc: 0.8579, val loss: 0.2982, val acc: 0.8695  (best train acc: 0.8728, best val acc: 0.8772, best train loss: 0.3072  @ epoch 12940 )\n",
      "[Epoch: 13480] train loss: 0.3320, train acc: 0.8566, val loss: 0.2911, val acc: 0.8728  (best train acc: 0.8728, best val acc: 0.8772, best train loss: 0.3072  @ epoch 12940 )\n",
      "[Epoch: 13500] train loss: 0.3603, train acc: 0.8529, val loss: 0.3131, val acc: 0.8641  (best train acc: 0.8728, best val acc: 0.8776, best train loss: 0.3072  @ epoch 12940 )\n",
      "[Epoch: 13520] train loss: 0.3310, train acc: 0.8600, val loss: 0.3087, val acc: 0.8634  (best train acc: 0.8728, best val acc: 0.8776, best train loss: 0.3072  @ epoch 12940 )\n",
      "[Epoch: 13540] train loss: 0.3407, train acc: 0.8626, val loss: 0.3426, val acc: 0.8388  (best train acc: 0.8728, best val acc: 0.8776, best train loss: 0.3072  @ epoch 12940 )\n",
      "[Epoch: 13560] train loss: 0.3175, train acc: 0.8681, val loss: 0.2958, val acc: 0.8745  (best train acc: 0.8728, best val acc: 0.8803, best train loss: 0.3072  @ epoch 12940 )\n",
      "[Epoch: 13580] train loss: 0.3304, train acc: 0.8584, val loss: 0.2935, val acc: 0.8681  (best train acc: 0.8728, best val acc: 0.8803, best train loss: 0.3072  @ epoch 12940 )\n",
      "[Epoch: 13600] train loss: 0.3541, train acc: 0.8499, val loss: 0.2969, val acc: 0.8732  (best train acc: 0.8728, best val acc: 0.8803, best train loss: 0.3072  @ epoch 12940 )\n",
      "[Epoch: 13620] train loss: 0.3655, train acc: 0.8488, val loss: 0.3016, val acc: 0.8654  (best train acc: 0.8728, best val acc: 0.8803, best train loss: 0.3072  @ epoch 12940 )\n",
      "[Epoch: 13640] train loss: 0.3364, train acc: 0.8592, val loss: 0.2976, val acc: 0.8739  (best train acc: 0.8728, best val acc: 0.8803, best train loss: 0.3072  @ epoch 12940 )\n",
      "[Epoch: 13660] train loss: 0.3165, train acc: 0.8700, val loss: 0.2890, val acc: 0.8752  (best train acc: 0.8728, best val acc: 0.8803, best train loss: 0.3072  @ epoch 12940 )\n",
      "[Epoch: 13680] train loss: 0.3119, train acc: 0.8641, val loss: 0.2838, val acc: 0.8749  (best train acc: 0.8736, best val acc: 0.8803, best train loss: 0.3035  @ epoch 13668 )\n",
      "[Epoch: 13700] train loss: 0.3580, train acc: 0.8524, val loss: 0.2985, val acc: 0.8732  (best train acc: 0.8736, best val acc: 0.8803, best train loss: 0.3035  @ epoch 13668 )\n",
      "[Epoch: 13720] train loss: 0.3483, train acc: 0.8546, val loss: 0.2993, val acc: 0.8739  (best train acc: 0.8736, best val acc: 0.8803, best train loss: 0.3035  @ epoch 13668 )\n",
      "[Epoch: 13740] train loss: 0.3844, train acc: 0.8370, val loss: 0.3503, val acc: 0.8449  (best train acc: 0.8736, best val acc: 0.8803, best train loss: 0.3035  @ epoch 13668 )\n",
      "[Epoch: 13760] train loss: 0.3246, train acc: 0.8639, val loss: 0.2886, val acc: 0.8735  (best train acc: 0.8736, best val acc: 0.8803, best train loss: 0.3035  @ epoch 13668 )\n",
      "[Epoch: 13780] train loss: 0.3495, train acc: 0.8540, val loss: 0.3093, val acc: 0.8658  (best train acc: 0.8736, best val acc: 0.8803, best train loss: 0.3035  @ epoch 13668 )\n",
      "[Epoch: 13800] train loss: 0.3162, train acc: 0.8639, val loss: 0.2916, val acc: 0.8718  (best train acc: 0.8736, best val acc: 0.8803, best train loss: 0.3035  @ epoch 13668 )\n",
      "[Epoch: 13820] train loss: 0.3335, train acc: 0.8569, val loss: 0.3098, val acc: 0.8691  (best train acc: 0.8736, best val acc: 0.8803, best train loss: 0.3035  @ epoch 13668 )\n",
      "[Epoch: 13840] train loss: 0.3547, train acc: 0.8524, val loss: 0.2968, val acc: 0.8728  (best train acc: 0.8736, best val acc: 0.8803, best train loss: 0.3035  @ epoch 13668 )\n",
      "[Epoch: 13860] train loss: 0.3139, train acc: 0.8702, val loss: 0.3031, val acc: 0.8634  (best train acc: 0.8736, best val acc: 0.8803, best train loss: 0.3035  @ epoch 13668 )\n",
      "[Epoch: 13880] train loss: 0.3442, train acc: 0.8576, val loss: 0.2947, val acc: 0.8695  (best train acc: 0.8736, best val acc: 0.8803, best train loss: 0.3035  @ epoch 13668 )\n",
      "[Epoch: 13900] train loss: 0.3238, train acc: 0.8637, val loss: 0.2969, val acc: 0.8658  (best train acc: 0.8736, best val acc: 0.8803, best train loss: 0.3035  @ epoch 13668 )\n",
      "[Epoch: 13920] train loss: 0.3491, train acc: 0.8476, val loss: 0.2941, val acc: 0.8725  (best train acc: 0.8736, best val acc: 0.8803, best train loss: 0.3035  @ epoch 13668 )\n",
      "[Epoch: 13940] train loss: 0.3312, train acc: 0.8606, val loss: 0.2940, val acc: 0.8742  (best train acc: 0.8736, best val acc: 0.8803, best train loss: 0.3035  @ epoch 13668 )\n",
      "[Epoch: 13960] train loss: 0.3268, train acc: 0.8638, val loss: 0.3006, val acc: 0.8681  (best train acc: 0.8736, best val acc: 0.8803, best train loss: 0.3035  @ epoch 13668 )\n",
      "[Epoch: 13980] train loss: 0.3514, train acc: 0.8517, val loss: 0.3058, val acc: 0.8698  (best train acc: 0.8736, best val acc: 0.8803, best train loss: 0.3035  @ epoch 13668 )\n",
      "[Epoch: 14000] train loss: 0.3473, train acc: 0.8561, val loss: 0.2945, val acc: 0.8742  (best train acc: 0.8736, best val acc: 0.8803, best train loss: 0.3035  @ epoch 13668 )\n",
      "[Epoch: 14020] train loss: 0.3524, train acc: 0.8521, val loss: 0.2955, val acc: 0.8735  (best train acc: 0.8736, best val acc: 0.8803, best train loss: 0.3035  @ epoch 13668 )\n",
      "[Epoch: 14040] train loss: 0.3272, train acc: 0.8581, val loss: 0.2967, val acc: 0.8685  (best train acc: 0.8736, best val acc: 0.8803, best train loss: 0.3035  @ epoch 13668 )\n",
      "[Epoch: 14060] train loss: 0.3440, train acc: 0.8529, val loss: 0.3030, val acc: 0.8664  (best train acc: 0.8736, best val acc: 0.8803, best train loss: 0.3035  @ epoch 13668 )\n",
      "[Epoch: 14080] train loss: 0.3471, train acc: 0.8506, val loss: 0.2954, val acc: 0.8722  (best train acc: 0.8736, best val acc: 0.8803, best train loss: 0.3035  @ epoch 13668 )\n",
      "[Epoch: 14100] train loss: 0.3253, train acc: 0.8613, val loss: 0.3153, val acc: 0.8617  (best train acc: 0.8740, best val acc: 0.8803, best train loss: 0.3035  @ epoch 13668 )\n",
      "[Epoch: 14120] train loss: 0.3481, train acc: 0.8561, val loss: 0.3069, val acc: 0.8634  (best train acc: 0.8740, best val acc: 0.8803, best train loss: 0.3035  @ epoch 13668 )\n",
      "[Epoch: 14140] train loss: 0.3526, train acc: 0.8622, val loss: 0.2997, val acc: 0.8708  (best train acc: 0.8740, best val acc: 0.8803, best train loss: 0.3035  @ epoch 13668 )\n",
      "[Epoch: 14160] train loss: 0.3184, train acc: 0.8661, val loss: 0.3082, val acc: 0.8627  (best train acc: 0.8740, best val acc: 0.8803, best train loss: 0.3035  @ epoch 13668 )\n",
      "[Epoch: 14180] train loss: 0.3353, train acc: 0.8641, val loss: 0.2979, val acc: 0.8675  (best train acc: 0.8740, best val acc: 0.8803, best train loss: 0.3035  @ epoch 13668 )\n",
      "[Epoch: 14200] train loss: 0.3147, train acc: 0.8644, val loss: 0.2961, val acc: 0.8691  (best train acc: 0.8740, best val acc: 0.8803, best train loss: 0.3035  @ epoch 13668 )\n",
      "[Epoch: 14220] train loss: 0.3360, train acc: 0.8628, val loss: 0.2917, val acc: 0.8712  (best train acc: 0.8740, best val acc: 0.8803, best train loss: 0.3035  @ epoch 13668 )\n",
      "[Epoch: 14240] train loss: 0.3143, train acc: 0.8660, val loss: 0.2886, val acc: 0.8732  (best train acc: 0.8744, best val acc: 0.8803, best train loss: 0.3035  @ epoch 13668 )\n",
      "[Epoch: 14260] train loss: 0.3323, train acc: 0.8575, val loss: 0.2909, val acc: 0.8722  (best train acc: 0.8744, best val acc: 0.8803, best train loss: 0.3035  @ epoch 13668 )\n",
      "[Epoch: 14280] train loss: 0.3352, train acc: 0.8605, val loss: 0.2909, val acc: 0.8718  (best train acc: 0.8744, best val acc: 0.8803, best train loss: 0.3035  @ epoch 13668 )\n",
      "[Epoch: 14300] train loss: 0.3911, train acc: 0.8365, val loss: 0.3346, val acc: 0.8533  (best train acc: 0.8744, best val acc: 0.8803, best train loss: 0.3035  @ epoch 13668 )\n",
      "[Epoch: 14320] train loss: 0.3437, train acc: 0.8537, val loss: 0.3118, val acc: 0.8627  (best train acc: 0.8744, best val acc: 0.8803, best train loss: 0.3035  @ epoch 13668 )\n",
      "[Epoch: 14340] train loss: 0.3295, train acc: 0.8670, val loss: 0.2883, val acc: 0.8752  (best train acc: 0.8744, best val acc: 0.8803, best train loss: 0.3000  @ epoch 14332 )\n",
      "[Epoch: 14360] train loss: 0.3388, train acc: 0.8631, val loss: 0.2880, val acc: 0.8766  (best train acc: 0.8744, best val acc: 0.8803, best train loss: 0.3000  @ epoch 14332 )\n",
      "[Epoch: 14380] train loss: 0.3346, train acc: 0.8577, val loss: 0.3000, val acc: 0.8688  (best train acc: 0.8744, best val acc: 0.8803, best train loss: 0.3000  @ epoch 14332 )\n",
      "[Epoch: 14400] train loss: 0.3749, train acc: 0.8484, val loss: 0.3138, val acc: 0.8627  (best train acc: 0.8744, best val acc: 0.8803, best train loss: 0.3000  @ epoch 14332 )\n",
      "[Epoch: 14420] train loss: 0.3223, train acc: 0.8634, val loss: 0.2923, val acc: 0.8698  (best train acc: 0.8744, best val acc: 0.8803, best train loss: 0.3000  @ epoch 14332 )\n",
      "[Epoch: 14440] train loss: 0.3099, train acc: 0.8689, val loss: 0.2960, val acc: 0.8685  (best train acc: 0.8756, best val acc: 0.8803, best train loss: 0.3000  @ epoch 14332 )\n",
      "[Epoch: 14460] train loss: 0.3121, train acc: 0.8668, val loss: 0.3021, val acc: 0.8691  (best train acc: 0.8756, best val acc: 0.8803, best train loss: 0.3000  @ epoch 14332 )\n",
      "[Epoch: 14480] train loss: 0.3581, train acc: 0.8556, val loss: 0.2882, val acc: 0.8671  (best train acc: 0.8756, best val acc: 0.8803, best train loss: 0.3000  @ epoch 14332 )\n",
      "[Epoch: 14500] train loss: 0.3323, train acc: 0.8640, val loss: 0.2946, val acc: 0.8762  (best train acc: 0.8760, best val acc: 0.8803, best train loss: 0.3000  @ epoch 14332 )\n",
      "[Epoch: 14520] train loss: 0.3657, train acc: 0.8425, val loss: 0.3263, val acc: 0.8543  (best train acc: 0.8760, best val acc: 0.8803, best train loss: 0.2994  @ epoch 14505 )\n",
      "[Epoch: 14540] train loss: 0.3238, train acc: 0.8676, val loss: 0.2923, val acc: 0.8725  (best train acc: 0.8760, best val acc: 0.8803, best train loss: 0.2994  @ epoch 14505 )\n",
      "[Epoch: 14560] train loss: 0.3349, train acc: 0.8630, val loss: 0.2962, val acc: 0.8644  (best train acc: 0.8760, best val acc: 0.8803, best train loss: 0.2994  @ epoch 14505 )\n",
      "[Epoch: 14580] train loss: 0.3228, train acc: 0.8635, val loss: 0.2974, val acc: 0.8668  (best train acc: 0.8760, best val acc: 0.8803, best train loss: 0.2994  @ epoch 14505 )\n",
      "[Epoch: 14600] train loss: 0.3202, train acc: 0.8691, val loss: 0.3034, val acc: 0.8651  (best train acc: 0.8760, best val acc: 0.8803, best train loss: 0.2994  @ epoch 14505 )\n",
      "[Epoch: 14620] train loss: 0.3268, train acc: 0.8612, val loss: 0.3115, val acc: 0.8604  (best train acc: 0.8760, best val acc: 0.8803, best train loss: 0.2994  @ epoch 14505 )\n",
      "[Epoch: 14640] train loss: 0.3389, train acc: 0.8571, val loss: 0.2989, val acc: 0.8668  (best train acc: 0.8760, best val acc: 0.8803, best train loss: 0.2994  @ epoch 14505 )\n",
      "[Epoch: 14660] train loss: 0.3223, train acc: 0.8601, val loss: 0.3005, val acc: 0.8654  (best train acc: 0.8760, best val acc: 0.8803, best train loss: 0.2994  @ epoch 14505 )\n",
      "[Epoch: 14680] train loss: 0.3169, train acc: 0.8663, val loss: 0.3028, val acc: 0.8661  (best train acc: 0.8760, best val acc: 0.8803, best train loss: 0.2994  @ epoch 14505 )\n",
      "[Epoch: 14700] train loss: 0.3186, train acc: 0.8708, val loss: 0.3102, val acc: 0.8614  (best train acc: 0.8760, best val acc: 0.8803, best train loss: 0.2994  @ epoch 14505 )\n",
      "[Epoch: 14720] train loss: 0.3529, train acc: 0.8458, val loss: 0.3009, val acc: 0.8637  (best train acc: 0.8764, best val acc: 0.8803, best train loss: 0.2945  @ epoch 14702 )\n",
      "[Epoch: 14740] train loss: 0.3355, train acc: 0.8627, val loss: 0.2821, val acc: 0.8789  (best train acc: 0.8764, best val acc: 0.8803, best train loss: 0.2945  @ epoch 14702 )\n",
      "[Epoch: 14760] train loss: 0.3244, train acc: 0.8670, val loss: 0.2832, val acc: 0.8769  (best train acc: 0.8764, best val acc: 0.8803, best train loss: 0.2945  @ epoch 14702 )\n",
      "[Epoch: 14780] train loss: 0.3275, train acc: 0.8607, val loss: 0.2877, val acc: 0.8678  (best train acc: 0.8764, best val acc: 0.8803, best train loss: 0.2945  @ epoch 14702 )\n",
      "[Epoch: 14800] train loss: 0.4136, train acc: 0.8300, val loss: 0.2940, val acc: 0.8695  (best train acc: 0.8789, best val acc: 0.8803, best train loss: 0.2945  @ epoch 14702 )\n",
      "[Epoch: 14820] train loss: 0.3414, train acc: 0.8593, val loss: 0.2953, val acc: 0.8715  (best train acc: 0.8789, best val acc: 0.8803, best train loss: 0.2945  @ epoch 14702 )\n",
      "[Epoch: 14840] train loss: 0.3192, train acc: 0.8655, val loss: 0.2883, val acc: 0.8776  (best train acc: 0.8789, best val acc: 0.8813, best train loss: 0.2945  @ epoch 14702 )\n",
      "[Epoch: 14860] train loss: 0.3209, train acc: 0.8665, val loss: 0.2915, val acc: 0.8732  (best train acc: 0.8789, best val acc: 0.8813, best train loss: 0.2945  @ epoch 14702 )\n",
      "[Epoch: 14880] train loss: 0.3236, train acc: 0.8590, val loss: 0.3021, val acc: 0.8735  (best train acc: 0.8789, best val acc: 0.8813, best train loss: 0.2945  @ epoch 14702 )\n",
      "[Epoch: 14900] train loss: 0.3554, train acc: 0.8558, val loss: 0.2984, val acc: 0.8685  (best train acc: 0.8789, best val acc: 0.8813, best train loss: 0.2945  @ epoch 14702 )\n",
      "[Epoch: 14920] train loss: 0.3104, train acc: 0.8689, val loss: 0.2881, val acc: 0.8735  (best train acc: 0.8789, best val acc: 0.8813, best train loss: 0.2945  @ epoch 14702 )\n",
      "[Epoch: 14940] train loss: 0.3303, train acc: 0.8661, val loss: 0.2866, val acc: 0.8772  (best train acc: 0.8789, best val acc: 0.8813, best train loss: 0.2945  @ epoch 14702 )\n",
      "[Epoch: 14960] train loss: 0.3225, train acc: 0.8689, val loss: 0.3034, val acc: 0.8651  (best train acc: 0.8789, best val acc: 0.8813, best train loss: 0.2945  @ epoch 14702 )\n",
      "[Epoch: 14980] train loss: 0.3239, train acc: 0.8671, val loss: 0.2920, val acc: 0.8735  (best train acc: 0.8789, best val acc: 0.8813, best train loss: 0.2945  @ epoch 14702 )\n",
      "[Epoch: 15000] train loss: 0.3720, train acc: 0.8525, val loss: 0.2906, val acc: 0.8772  (best train acc: 0.8789, best val acc: 0.8813, best train loss: 0.2945  @ epoch 14702 )\n",
      "[Epoch: 15020] train loss: 0.3367, train acc: 0.8633, val loss: 0.2962, val acc: 0.8762  (best train acc: 0.8789, best val acc: 0.8813, best train loss: 0.2945  @ epoch 14702 )\n",
      "[Epoch: 15040] train loss: 0.3268, train acc: 0.8664, val loss: 0.2834, val acc: 0.8772  (best train acc: 0.8789, best val acc: 0.8820, best train loss: 0.2945  @ epoch 14702 )\n",
      "[Epoch: 15060] train loss: 0.4051, train acc: 0.8273, val loss: 0.3363, val acc: 0.8486  (best train acc: 0.8789, best val acc: 0.8820, best train loss: 0.2945  @ epoch 14702 )\n",
      "[Epoch: 15080] train loss: 0.3543, train acc: 0.8512, val loss: 0.3124, val acc: 0.8651  (best train acc: 0.8789, best val acc: 0.8820, best train loss: 0.2945  @ epoch 14702 )\n",
      "[Epoch: 15100] train loss: 0.4081, train acc: 0.8311, val loss: 0.3044, val acc: 0.8634  (best train acc: 0.8789, best val acc: 0.8820, best train loss: 0.2945  @ epoch 14702 )\n",
      "[Epoch: 15120] train loss: 0.3254, train acc: 0.8697, val loss: 0.2947, val acc: 0.8749  (best train acc: 0.8789, best val acc: 0.8820, best train loss: 0.2945  @ epoch 14702 )\n",
      "[Epoch: 15140] train loss: 0.3206, train acc: 0.8701, val loss: 0.2890, val acc: 0.8718  (best train acc: 0.8789, best val acc: 0.8820, best train loss: 0.2945  @ epoch 14702 )\n",
      "[Epoch: 15160] train loss: 0.3167, train acc: 0.8660, val loss: 0.2895, val acc: 0.8752  (best train acc: 0.8789, best val acc: 0.8820, best train loss: 0.2945  @ epoch 14702 )\n",
      "[Epoch: 15180] train loss: 0.3205, train acc: 0.8706, val loss: 0.2904, val acc: 0.8698  (best train acc: 0.8789, best val acc: 0.8820, best train loss: 0.2945  @ epoch 14702 )\n",
      "[Epoch: 15200] train loss: 0.3244, train acc: 0.8622, val loss: 0.2919, val acc: 0.8735  (best train acc: 0.8789, best val acc: 0.8820, best train loss: 0.2945  @ epoch 14702 )\n",
      "[Epoch: 15220] train loss: 0.3759, train acc: 0.8385, val loss: 0.2873, val acc: 0.8732  (best train acc: 0.8789, best val acc: 0.8820, best train loss: 0.2945  @ epoch 14702 )\n",
      "[Epoch: 15240] train loss: 0.3452, train acc: 0.8551, val loss: 0.2855, val acc: 0.8766  (best train acc: 0.8789, best val acc: 0.8820, best train loss: 0.2945  @ epoch 14702 )\n",
      "[Epoch: 15260] train loss: 0.3149, train acc: 0.8672, val loss: 0.3099, val acc: 0.8637  (best train acc: 0.8789, best val acc: 0.8820, best train loss: 0.2945  @ epoch 14702 )\n",
      "[Epoch: 15280] train loss: 0.3334, train acc: 0.8601, val loss: 0.3087, val acc: 0.8658  (best train acc: 0.8789, best val acc: 0.8820, best train loss: 0.2945  @ epoch 14702 )\n",
      "[Epoch: 15300] train loss: 0.3452, train acc: 0.8527, val loss: 0.3056, val acc: 0.8685  (best train acc: 0.8789, best val acc: 0.8820, best train loss: 0.2945  @ epoch 14702 )\n",
      "[Epoch: 15320] train loss: 0.3535, train acc: 0.8515, val loss: 0.2942, val acc: 0.8688  (best train acc: 0.8789, best val acc: 0.8820, best train loss: 0.2945  @ epoch 14702 )\n",
      "[Epoch: 15340] train loss: 0.3201, train acc: 0.8638, val loss: 0.2927, val acc: 0.8695  (best train acc: 0.8789, best val acc: 0.8820, best train loss: 0.2945  @ epoch 14702 )\n",
      "[Epoch: 15360] train loss: 0.3403, train acc: 0.8631, val loss: 0.2994, val acc: 0.8708  (best train acc: 0.8789, best val acc: 0.8820, best train loss: 0.2945  @ epoch 14702 )\n",
      "[Epoch: 15380] train loss: 0.3472, train acc: 0.8582, val loss: 0.2869, val acc: 0.8745  (best train acc: 0.8789, best val acc: 0.8820, best train loss: 0.2945  @ epoch 14702 )\n",
      "[Epoch: 15400] train loss: 0.3223, train acc: 0.8653, val loss: 0.2903, val acc: 0.8732  (best train acc: 0.8789, best val acc: 0.8820, best train loss: 0.2945  @ epoch 14702 )\n",
      "[Epoch: 15420] train loss: 0.3383, train acc: 0.8573, val loss: 0.2913, val acc: 0.8759  (best train acc: 0.8789, best val acc: 0.8820, best train loss: 0.2945  @ epoch 14702 )\n",
      "[Epoch: 15440] train loss: 0.3089, train acc: 0.8744, val loss: 0.2844, val acc: 0.8762  (best train acc: 0.8789, best val acc: 0.8820, best train loss: 0.2945  @ epoch 14702 )\n",
      "[Epoch: 15460] train loss: 0.3191, train acc: 0.8681, val loss: 0.2895, val acc: 0.8715  (best train acc: 0.8795, best val acc: 0.8820, best train loss: 0.2945  @ epoch 14702 )\n",
      "[Epoch: 15480] train loss: 0.3256, train acc: 0.8650, val loss: 0.3254, val acc: 0.8519  (best train acc: 0.8795, best val acc: 0.8820, best train loss: 0.2945  @ epoch 14702 )\n",
      "[Epoch: 15500] train loss: 0.3239, train acc: 0.8690, val loss: 0.2850, val acc: 0.8742  (best train acc: 0.8795, best val acc: 0.8820, best train loss: 0.2945  @ epoch 14702 )\n",
      "[Epoch: 15520] train loss: 0.3590, train acc: 0.8503, val loss: 0.2896, val acc: 0.8722  (best train acc: 0.8795, best val acc: 0.8820, best train loss: 0.2945  @ epoch 14702 )\n",
      "[Epoch: 15540] train loss: 0.3588, train acc: 0.8571, val loss: 0.3059, val acc: 0.8651  (best train acc: 0.8795, best val acc: 0.8820, best train loss: 0.2945  @ epoch 14702 )\n",
      "[Epoch: 15560] train loss: 0.3234, train acc: 0.8677, val loss: 0.2953, val acc: 0.8745  (best train acc: 0.8795, best val acc: 0.8820, best train loss: 0.2945  @ epoch 14702 )\n",
      "[Epoch: 15580] train loss: 0.3601, train acc: 0.8519, val loss: 0.2799, val acc: 0.8759  (best train acc: 0.8795, best val acc: 0.8843, best train loss: 0.2945  @ epoch 14702 )\n",
      "[Epoch: 15600] train loss: 0.3072, train acc: 0.8726, val loss: 0.2880, val acc: 0.8742  (best train acc: 0.8795, best val acc: 0.8843, best train loss: 0.2945  @ epoch 14702 )\n",
      "[Epoch: 15620] train loss: 0.3224, train acc: 0.8684, val loss: 0.2853, val acc: 0.8755  (best train acc: 0.8801, best val acc: 0.8843, best train loss: 0.2915  @ epoch 15614 )\n",
      "[Epoch: 15640] train loss: 0.3217, train acc: 0.8732, val loss: 0.2928, val acc: 0.8742  (best train acc: 0.8801, best val acc: 0.8843, best train loss: 0.2915  @ epoch 15614 )\n",
      "[Epoch: 15660] train loss: 0.3056, train acc: 0.8756, val loss: 0.2901, val acc: 0.8755  (best train acc: 0.8801, best val acc: 0.8843, best train loss: 0.2915  @ epoch 15614 )\n",
      "[Epoch: 15680] train loss: 0.3278, train acc: 0.8640, val loss: 0.2886, val acc: 0.8769  (best train acc: 0.8801, best val acc: 0.8843, best train loss: 0.2915  @ epoch 15614 )\n",
      "[Epoch: 15700] train loss: 0.3107, train acc: 0.8724, val loss: 0.2817, val acc: 0.8782  (best train acc: 0.8814, best val acc: 0.8843, best train loss: 0.2905  @ epoch 15687 )\n",
      "[Epoch: 15720] train loss: 0.3311, train acc: 0.8632, val loss: 0.2881, val acc: 0.8769  (best train acc: 0.8814, best val acc: 0.8843, best train loss: 0.2905  @ epoch 15687 )\n",
      "[Epoch: 15740] train loss: 0.3273, train acc: 0.8634, val loss: 0.3053, val acc: 0.8651  (best train acc: 0.8814, best val acc: 0.8843, best train loss: 0.2905  @ epoch 15687 )\n",
      "[Epoch: 15760] train loss: 0.3293, train acc: 0.8629, val loss: 0.2939, val acc: 0.8739  (best train acc: 0.8814, best val acc: 0.8843, best train loss: 0.2905  @ epoch 15687 )\n",
      "[Epoch: 15780] train loss: 0.3728, train acc: 0.8379, val loss: 0.3105, val acc: 0.8644  (best train acc: 0.8814, best val acc: 0.8843, best train loss: 0.2905  @ epoch 15687 )\n",
      "[Epoch: 15800] train loss: 0.3373, train acc: 0.8539, val loss: 0.2829, val acc: 0.8749  (best train acc: 0.8814, best val acc: 0.8843, best train loss: 0.2905  @ epoch 15687 )\n",
      "[Epoch: 15820] train loss: 0.3536, train acc: 0.8621, val loss: 0.3034, val acc: 0.8698  (best train acc: 0.8814, best val acc: 0.8843, best train loss: 0.2905  @ epoch 15687 )\n",
      "[Epoch: 15840] train loss: 0.3226, train acc: 0.8667, val loss: 0.3005, val acc: 0.8698  (best train acc: 0.8814, best val acc: 0.8843, best train loss: 0.2905  @ epoch 15687 )\n",
      "[Epoch: 15860] train loss: 0.3166, train acc: 0.8650, val loss: 0.2847, val acc: 0.8776  (best train acc: 0.8814, best val acc: 0.8843, best train loss: 0.2884  @ epoch 15842 )\n",
      "[Epoch: 15880] train loss: 0.3211, train acc: 0.8651, val loss: 0.2823, val acc: 0.8725  (best train acc: 0.8814, best val acc: 0.8843, best train loss: 0.2884  @ epoch 15842 )\n",
      "[Epoch: 15900] train loss: 0.3401, train acc: 0.8576, val loss: 0.2925, val acc: 0.8745  (best train acc: 0.8814, best val acc: 0.8843, best train loss: 0.2884  @ epoch 15842 )\n",
      "[Epoch: 15920] train loss: 0.3123, train acc: 0.8657, val loss: 0.2850, val acc: 0.8752  (best train acc: 0.8814, best val acc: 0.8843, best train loss: 0.2884  @ epoch 15842 )\n",
      "[Epoch: 15940] train loss: 0.3241, train acc: 0.8678, val loss: 0.2933, val acc: 0.8718  (best train acc: 0.8814, best val acc: 0.8843, best train loss: 0.2884  @ epoch 15842 )\n",
      "[Epoch: 15960] train loss: 0.3311, train acc: 0.8608, val loss: 0.2789, val acc: 0.8789  (best train acc: 0.8814, best val acc: 0.8843, best train loss: 0.2884  @ epoch 15842 )\n",
      "[Epoch: 15980] train loss: 0.3112, train acc: 0.8712, val loss: 0.3098, val acc: 0.8654  (best train acc: 0.8814, best val acc: 0.8843, best train loss: 0.2884  @ epoch 15842 )\n",
      "[Epoch: 16000] train loss: 0.3163, train acc: 0.8683, val loss: 0.2799, val acc: 0.8745  (best train acc: 0.8814, best val acc: 0.8843, best train loss: 0.2884  @ epoch 15842 )\n",
      "[Epoch: 16020] train loss: 0.3142, train acc: 0.8672, val loss: 0.2826, val acc: 0.8776  (best train acc: 0.8814, best val acc: 0.8843, best train loss: 0.2884  @ epoch 15842 )\n",
      "[Epoch: 16040] train loss: 0.3605, train acc: 0.8503, val loss: 0.2897, val acc: 0.8725  (best train acc: 0.8814, best val acc: 0.8843, best train loss: 0.2884  @ epoch 15842 )\n",
      "[Epoch: 16060] train loss: 0.3246, train acc: 0.8642, val loss: 0.2858, val acc: 0.8782  (best train acc: 0.8814, best val acc: 0.8843, best train loss: 0.2884  @ epoch 15842 )\n",
      "[Epoch: 16080] train loss: 0.3832, train acc: 0.8331, val loss: 0.3439, val acc: 0.8465  (best train acc: 0.8814, best val acc: 0.8843, best train loss: 0.2884  @ epoch 15842 )\n",
      "[Epoch: 16100] train loss: 0.3067, train acc: 0.8702, val loss: 0.2935, val acc: 0.8702  (best train acc: 0.8814, best val acc: 0.8843, best train loss: 0.2884  @ epoch 15842 )\n",
      "[Epoch: 16120] train loss: 0.3211, train acc: 0.8697, val loss: 0.2886, val acc: 0.8779  (best train acc: 0.8814, best val acc: 0.8843, best train loss: 0.2884  @ epoch 15842 )\n",
      "[Epoch: 16140] train loss: 0.3286, train acc: 0.8618, val loss: 0.2960, val acc: 0.8685  (best train acc: 0.8814, best val acc: 0.8843, best train loss: 0.2884  @ epoch 15842 )\n",
      "[Epoch: 16160] train loss: 0.3173, train acc: 0.8691, val loss: 0.2781, val acc: 0.8762  (best train acc: 0.8814, best val acc: 0.8843, best train loss: 0.2884  @ epoch 15842 )\n",
      "[Epoch: 16180] train loss: 0.3794, train acc: 0.8360, val loss: 0.3083, val acc: 0.8641  (best train acc: 0.8814, best val acc: 0.8843, best train loss: 0.2884  @ epoch 15842 )\n",
      "[Epoch: 16200] train loss: 0.3155, train acc: 0.8639, val loss: 0.3326, val acc: 0.8513  (best train acc: 0.8814, best val acc: 0.8843, best train loss: 0.2884  @ epoch 15842 )\n",
      "[Epoch: 16220] train loss: 0.3599, train acc: 0.8527, val loss: 0.2898, val acc: 0.8725  (best train acc: 0.8814, best val acc: 0.8843, best train loss: 0.2884  @ epoch 15842 )\n",
      "[Epoch: 16240] train loss: 0.3411, train acc: 0.8555, val loss: 0.2891, val acc: 0.8735  (best train acc: 0.8814, best val acc: 0.8843, best train loss: 0.2884  @ epoch 15842 )\n",
      "[Epoch: 16260] train loss: 0.3237, train acc: 0.8582, val loss: 0.2972, val acc: 0.8718  (best train acc: 0.8814, best val acc: 0.8843, best train loss: 0.2884  @ epoch 15842 )\n",
      "[Epoch: 16280] train loss: 0.3240, train acc: 0.8637, val loss: 0.2829, val acc: 0.8725  (best train acc: 0.8814, best val acc: 0.8843, best train loss: 0.2884  @ epoch 15842 )\n",
      "[Epoch: 16300] train loss: 0.3157, train acc: 0.8682, val loss: 0.2846, val acc: 0.8779  (best train acc: 0.8814, best val acc: 0.8843, best train loss: 0.2884  @ epoch 15842 )\n",
      "[Epoch: 16320] train loss: 0.3179, train acc: 0.8688, val loss: 0.2864, val acc: 0.8762  (best train acc: 0.8819, best val acc: 0.8843, best train loss: 0.2884  @ epoch 15842 )\n",
      "[Epoch: 16340] train loss: 0.3080, train acc: 0.8749, val loss: 0.2906, val acc: 0.8685  (best train acc: 0.8819, best val acc: 0.8843, best train loss: 0.2884  @ epoch 15842 )\n",
      "[Epoch: 16360] train loss: 0.3029, train acc: 0.8673, val loss: 0.2867, val acc: 0.8681  (best train acc: 0.8819, best val acc: 0.8843, best train loss: 0.2884  @ epoch 15842 )\n",
      "[Epoch: 16380] train loss: 0.3166, train acc: 0.8671, val loss: 0.2824, val acc: 0.8755  (best train acc: 0.8819, best val acc: 0.8843, best train loss: 0.2884  @ epoch 15842 )\n",
      "[Epoch: 16400] train loss: 0.3279, train acc: 0.8544, val loss: 0.2985, val acc: 0.8671  (best train acc: 0.8819, best val acc: 0.8843, best train loss: 0.2884  @ epoch 15842 )\n",
      "[Epoch: 16420] train loss: 0.3282, train acc: 0.8648, val loss: 0.2894, val acc: 0.8688  (best train acc: 0.8819, best val acc: 0.8843, best train loss: 0.2884  @ epoch 15842 )\n",
      "[Epoch: 16440] train loss: 0.3628, train acc: 0.8465, val loss: 0.2812, val acc: 0.8766  (best train acc: 0.8819, best val acc: 0.8843, best train loss: 0.2884  @ epoch 15842 )\n",
      "[Epoch: 16460] train loss: 0.3257, train acc: 0.8630, val loss: 0.2806, val acc: 0.8715  (best train acc: 0.8819, best val acc: 0.8843, best train loss: 0.2884  @ epoch 15842 )\n",
      "[Epoch: 16480] train loss: 0.3226, train acc: 0.8653, val loss: 0.2884, val acc: 0.8712  (best train acc: 0.8819, best val acc: 0.8843, best train loss: 0.2884  @ epoch 15842 )\n",
      "[Epoch: 16500] train loss: 0.3064, train acc: 0.8755, val loss: 0.2812, val acc: 0.8806  (best train acc: 0.8819, best val acc: 0.8843, best train loss: 0.2884  @ epoch 15842 )\n",
      "[Epoch: 16520] train loss: 0.3239, train acc: 0.8600, val loss: 0.2938, val acc: 0.8702  (best train acc: 0.8819, best val acc: 0.8850, best train loss: 0.2884  @ epoch 15842 )\n",
      "[Epoch: 16540] train loss: 0.3300, train acc: 0.8621, val loss: 0.2831, val acc: 0.8796  (best train acc: 0.8819, best val acc: 0.8850, best train loss: 0.2884  @ epoch 15842 )\n",
      "[Epoch: 16560] train loss: 0.2993, train acc: 0.8754, val loss: 0.2772, val acc: 0.8843  (best train acc: 0.8819, best val acc: 0.8853, best train loss: 0.2884  @ epoch 15842 )\n",
      "[Epoch: 16580] train loss: 0.3273, train acc: 0.8618, val loss: 0.2808, val acc: 0.8793  (best train acc: 0.8819, best val acc: 0.8853, best train loss: 0.2884  @ epoch 15842 )\n",
      "[Epoch: 16600] train loss: 0.3077, train acc: 0.8719, val loss: 0.2840, val acc: 0.8769  (best train acc: 0.8819, best val acc: 0.8853, best train loss: 0.2884  @ epoch 15842 )\n",
      "[Epoch: 16620] train loss: 0.3145, train acc: 0.8704, val loss: 0.2840, val acc: 0.8762  (best train acc: 0.8819, best val acc: 0.8853, best train loss: 0.2884  @ epoch 15842 )\n",
      "[Epoch: 16640] train loss: 0.3065, train acc: 0.8754, val loss: 0.2811, val acc: 0.8816  (best train acc: 0.8819, best val acc: 0.8853, best train loss: 0.2884  @ epoch 15842 )\n",
      "[Epoch: 16660] train loss: 0.3309, train acc: 0.8608, val loss: 0.3117, val acc: 0.8654  (best train acc: 0.8819, best val acc: 0.8853, best train loss: 0.2884  @ epoch 15842 )\n",
      "[Epoch: 16680] train loss: 0.2954, train acc: 0.8777, val loss: 0.2767, val acc: 0.8803  (best train acc: 0.8819, best val acc: 0.8853, best train loss: 0.2884  @ epoch 15842 )\n",
      "[Epoch: 16700] train loss: 0.2968, train acc: 0.8755, val loss: 0.2811, val acc: 0.8755  (best train acc: 0.8837, best val acc: 0.8853, best train loss: 0.2878  @ epoch 16694 )\n",
      "[Epoch: 16720] train loss: 0.3147, train acc: 0.8715, val loss: 0.2763, val acc: 0.8779  (best train acc: 0.8842, best val acc: 0.8853, best train loss: 0.2815  @ epoch 16707 )\n",
      "[Epoch: 16740] train loss: 0.3318, train acc: 0.8563, val loss: 0.2839, val acc: 0.8732  (best train acc: 0.8842, best val acc: 0.8853, best train loss: 0.2815  @ epoch 16707 )\n",
      "[Epoch: 16760] train loss: 0.3029, train acc: 0.8769, val loss: 0.2908, val acc: 0.8718  (best train acc: 0.8842, best val acc: 0.8853, best train loss: 0.2815  @ epoch 16707 )\n",
      "[Epoch: 16780] train loss: 0.2954, train acc: 0.8797, val loss: 0.2962, val acc: 0.8732  (best train acc: 0.8850, best val acc: 0.8853, best train loss: 0.2788  @ epoch 16778 )\n",
      "[Epoch: 16800] train loss: 0.3149, train acc: 0.8671, val loss: 0.2749, val acc: 0.8762  (best train acc: 0.8850, best val acc: 0.8853, best train loss: 0.2788  @ epoch 16778 )\n",
      "[Epoch: 16820] train loss: 0.3413, train acc: 0.8608, val loss: 0.2971, val acc: 0.8732  (best train acc: 0.8850, best val acc: 0.8853, best train loss: 0.2788  @ epoch 16778 )\n",
      "[Epoch: 16840] train loss: 0.3537, train acc: 0.8559, val loss: 0.2735, val acc: 0.8796  (best train acc: 0.8850, best val acc: 0.8853, best train loss: 0.2788  @ epoch 16778 )\n",
      "[Epoch: 16860] train loss: 0.2860, train acc: 0.8836, val loss: 0.2719, val acc: 0.8806  (best train acc: 0.8850, best val acc: 0.8853, best train loss: 0.2788  @ epoch 16778 )\n",
      "[Epoch: 16880] train loss: 0.2811, train acc: 0.8887, val loss: 0.2728, val acc: 0.8793  (best train acc: 0.8887, best val acc: 0.8853, best train loss: 0.2784  @ epoch 16876 )\n",
      "[Epoch: 16900] train loss: 0.2971, train acc: 0.8814, val loss: 0.2977, val acc: 0.8715  (best train acc: 0.8887, best val acc: 0.8853, best train loss: 0.2784  @ epoch 16876 )\n",
      "[Epoch: 16920] train loss: 0.3418, train acc: 0.8577, val loss: 0.2873, val acc: 0.8715  (best train acc: 0.8887, best val acc: 0.8853, best train loss: 0.2784  @ epoch 16876 )\n",
      "[Epoch: 16940] train loss: 0.2929, train acc: 0.8722, val loss: 0.2753, val acc: 0.8793  (best train acc: 0.8887, best val acc: 0.8853, best train loss: 0.2747  @ epoch 16922 )\n",
      "[Epoch: 16960] train loss: 0.2913, train acc: 0.8788, val loss: 0.3036, val acc: 0.8705  (best train acc: 0.8887, best val acc: 0.8853, best train loss: 0.2747  @ epoch 16922 )\n",
      "[Epoch: 16980] train loss: 0.2786, train acc: 0.8845, val loss: 0.2726, val acc: 0.8793  (best train acc: 0.8887, best val acc: 0.8860, best train loss: 0.2747  @ epoch 16922 )\n",
      "[Epoch: 17000] train loss: 0.3126, train acc: 0.8680, val loss: 0.2853, val acc: 0.8712  (best train acc: 0.8887, best val acc: 0.8860, best train loss: 0.2747  @ epoch 16922 )\n",
      "[Epoch: 17020] train loss: 0.3031, train acc: 0.8644, val loss: 0.3076, val acc: 0.8627  (best train acc: 0.8887, best val acc: 0.8860, best train loss: 0.2747  @ epoch 16922 )\n",
      "[Epoch: 17040] train loss: 0.3028, train acc: 0.8715, val loss: 0.2805, val acc: 0.8796  (best train acc: 0.8887, best val acc: 0.8860, best train loss: 0.2747  @ epoch 16922 )\n",
      "[Epoch: 17060] train loss: 0.3271, train acc: 0.8624, val loss: 0.3138, val acc: 0.8641  (best train acc: 0.8887, best val acc: 0.8860, best train loss: 0.2747  @ epoch 16922 )\n",
      "[Epoch: 17080] train loss: 0.2968, train acc: 0.8711, val loss: 0.2862, val acc: 0.8759  (best train acc: 0.8887, best val acc: 0.8860, best train loss: 0.2747  @ epoch 16922 )\n",
      "[Epoch: 17100] train loss: 0.2866, train acc: 0.8768, val loss: 0.2808, val acc: 0.8715  (best train acc: 0.8887, best val acc: 0.8860, best train loss: 0.2747  @ epoch 16922 )\n",
      "[Epoch: 17120] train loss: 0.3101, train acc: 0.8701, val loss: 0.2734, val acc: 0.8806  (best train acc: 0.8887, best val acc: 0.8860, best train loss: 0.2747  @ epoch 16922 )\n",
      "[Epoch: 17140] train loss: 0.2878, train acc: 0.8798, val loss: 0.2788, val acc: 0.8749  (best train acc: 0.8887, best val acc: 0.8860, best train loss: 0.2747  @ epoch 16922 )\n",
      "[Epoch: 17160] train loss: 0.2843, train acc: 0.8798, val loss: 0.2851, val acc: 0.8752  (best train acc: 0.8887, best val acc: 0.8860, best train loss: 0.2747  @ epoch 16922 )\n",
      "[Epoch: 17180] train loss: 0.2920, train acc: 0.8746, val loss: 0.2702, val acc: 0.8796  (best train acc: 0.8887, best val acc: 0.8860, best train loss: 0.2747  @ epoch 16922 )\n",
      "[Epoch: 17200] train loss: 0.2897, train acc: 0.8775, val loss: 0.2823, val acc: 0.8762  (best train acc: 0.8887, best val acc: 0.8860, best train loss: 0.2747  @ epoch 16922 )\n",
      "[Epoch: 17220] train loss: 0.3443, train acc: 0.8580, val loss: 0.3192, val acc: 0.8610  (best train acc: 0.8887, best val acc: 0.8860, best train loss: 0.2747  @ epoch 16922 )\n",
      "[Epoch: 17240] train loss: 0.2925, train acc: 0.8796, val loss: 0.2814, val acc: 0.8803  (best train acc: 0.8887, best val acc: 0.8860, best train loss: 0.2747  @ epoch 16922 )\n",
      "[Epoch: 17260] train loss: 0.3164, train acc: 0.8621, val loss: 0.2804, val acc: 0.8749  (best train acc: 0.8897, best val acc: 0.8860, best train loss: 0.2747  @ epoch 16922 )\n",
      "[Epoch: 17280] train loss: 0.3026, train acc: 0.8706, val loss: 0.2730, val acc: 0.8786  (best train acc: 0.8897, best val acc: 0.8860, best train loss: 0.2747  @ epoch 16922 )\n",
      "[Epoch: 17300] train loss: 0.2908, train acc: 0.8772, val loss: 0.2735, val acc: 0.8789  (best train acc: 0.8897, best val acc: 0.8860, best train loss: 0.2704  @ epoch 17296 )\n",
      "[Epoch: 17320] train loss: 0.2811, train acc: 0.8805, val loss: 0.2803, val acc: 0.8809  (best train acc: 0.8897, best val acc: 0.8860, best train loss: 0.2704  @ epoch 17314 )\n",
      "[Epoch: 17340] train loss: 0.3234, train acc: 0.8711, val loss: 0.2803, val acc: 0.8782  (best train acc: 0.8897, best val acc: 0.8860, best train loss: 0.2704  @ epoch 17314 )\n",
      "[Epoch: 17360] train loss: 0.2807, train acc: 0.8848, val loss: 0.3028, val acc: 0.8702  (best train acc: 0.8897, best val acc: 0.8860, best train loss: 0.2704  @ epoch 17314 )\n",
      "[Epoch: 17380] train loss: 0.3229, train acc: 0.8644, val loss: 0.2777, val acc: 0.8772  (best train acc: 0.8897, best val acc: 0.8860, best train loss: 0.2704  @ epoch 17314 )\n",
      "[Epoch: 17400] train loss: 0.3593, train acc: 0.8530, val loss: 0.3034, val acc: 0.8658  (best train acc: 0.8897, best val acc: 0.8860, best train loss: 0.2704  @ epoch 17314 )\n",
      "[Epoch: 17420] train loss: 0.3084, train acc: 0.8728, val loss: 0.2834, val acc: 0.8823  (best train acc: 0.8897, best val acc: 0.8860, best train loss: 0.2704  @ epoch 17314 )\n",
      "[Epoch: 17440] train loss: 0.2955, train acc: 0.8783, val loss: 0.2901, val acc: 0.8745  (best train acc: 0.8897, best val acc: 0.8860, best train loss: 0.2704  @ epoch 17314 )\n",
      "[Epoch: 17460] train loss: 0.3019, train acc: 0.8746, val loss: 0.2832, val acc: 0.8772  (best train acc: 0.8897, best val acc: 0.8860, best train loss: 0.2704  @ epoch 17314 )\n",
      "[Epoch: 17480] train loss: 0.2981, train acc: 0.8767, val loss: 0.2859, val acc: 0.8772  (best train acc: 0.8897, best val acc: 0.8860, best train loss: 0.2701  @ epoch 17476 )\n",
      "[Epoch: 17500] train loss: 0.2966, train acc: 0.8773, val loss: 0.2858, val acc: 0.8749  (best train acc: 0.8897, best val acc: 0.8860, best train loss: 0.2701  @ epoch 17476 )\n",
      "[Epoch: 17520] train loss: 0.3023, train acc: 0.8720, val loss: 0.2884, val acc: 0.8728  (best train acc: 0.8897, best val acc: 0.8860, best train loss: 0.2701  @ epoch 17476 )\n",
      "[Epoch: 17540] train loss: 0.3284, train acc: 0.8630, val loss: 0.2800, val acc: 0.8776  (best train acc: 0.8897, best val acc: 0.8860, best train loss: 0.2701  @ epoch 17476 )\n",
      "[Epoch: 17560] train loss: 0.2747, train acc: 0.8823, val loss: 0.2710, val acc: 0.8820  (best train acc: 0.8897, best val acc: 0.8860, best train loss: 0.2701  @ epoch 17476 )\n",
      "[Epoch: 17580] train loss: 0.2955, train acc: 0.8762, val loss: 0.2871, val acc: 0.8752  (best train acc: 0.8897, best val acc: 0.8863, best train loss: 0.2701  @ epoch 17476 )\n",
      "[Epoch: 17600] train loss: 0.3082, train acc: 0.8691, val loss: 0.2709, val acc: 0.8772  (best train acc: 0.8897, best val acc: 0.8863, best train loss: 0.2701  @ epoch 17476 )\n",
      "[Epoch: 17620] train loss: 0.2822, train acc: 0.8849, val loss: 0.2682, val acc: 0.8813  (best train acc: 0.8897, best val acc: 0.8863, best train loss: 0.2701  @ epoch 17476 )\n",
      "[Epoch: 17640] train loss: 0.2799, train acc: 0.8832, val loss: 0.3260, val acc: 0.8526  (best train acc: 0.8903, best val acc: 0.8863, best train loss: 0.2688  @ epoch 17625 )\n",
      "[Epoch: 17660] train loss: 0.2984, train acc: 0.8704, val loss: 0.2935, val acc: 0.8705  (best train acc: 0.8903, best val acc: 0.8863, best train loss: 0.2688  @ epoch 17625 )\n",
      "[Epoch: 17680] train loss: 0.3686, train acc: 0.8436, val loss: 0.3113, val acc: 0.8617  (best train acc: 0.8903, best val acc: 0.8863, best train loss: 0.2688  @ epoch 17625 )\n",
      "[Epoch: 17700] train loss: 0.3002, train acc: 0.8752, val loss: 0.2738, val acc: 0.8786  (best train acc: 0.8903, best val acc: 0.8863, best train loss: 0.2688  @ epoch 17625 )\n",
      "[Epoch: 17720] train loss: 0.2774, train acc: 0.8848, val loss: 0.2705, val acc: 0.8789  (best train acc: 0.8903, best val acc: 0.8863, best train loss: 0.2688  @ epoch 17625 )\n",
      "[Epoch: 17740] train loss: 0.3102, train acc: 0.8741, val loss: 0.2669, val acc: 0.8840  (best train acc: 0.8903, best val acc: 0.8863, best train loss: 0.2688  @ epoch 17625 )\n",
      "[Epoch: 17760] train loss: 0.3094, train acc: 0.8687, val loss: 0.2722, val acc: 0.8779  (best train acc: 0.8903, best val acc: 0.8863, best train loss: 0.2688  @ epoch 17625 )\n",
      "[Epoch: 17780] train loss: 0.3107, train acc: 0.8750, val loss: 0.2833, val acc: 0.8742  (best train acc: 0.8903, best val acc: 0.8863, best train loss: 0.2688  @ epoch 17625 )\n",
      "[Epoch: 17800] train loss: 0.3146, train acc: 0.8671, val loss: 0.2785, val acc: 0.8766  (best train acc: 0.8903, best val acc: 0.8863, best train loss: 0.2688  @ epoch 17625 )\n",
      "[Epoch: 17820] train loss: 0.2826, train acc: 0.8856, val loss: 0.2746, val acc: 0.8833  (best train acc: 0.8903, best val acc: 0.8863, best train loss: 0.2688  @ epoch 17625 )\n",
      "[Epoch: 17840] train loss: 0.2770, train acc: 0.8837, val loss: 0.2779, val acc: 0.8766  (best train acc: 0.8903, best val acc: 0.8863, best train loss: 0.2688  @ epoch 17625 )\n",
      "[Epoch: 17860] train loss: 0.2907, train acc: 0.8770, val loss: 0.2743, val acc: 0.8796  (best train acc: 0.8903, best val acc: 0.8863, best train loss: 0.2688  @ epoch 17625 )\n",
      "[Epoch: 17880] train loss: 0.3297, train acc: 0.8665, val loss: 0.2772, val acc: 0.8816  (best train acc: 0.8903, best val acc: 0.8863, best train loss: 0.2688  @ epoch 17625 )\n",
      "[Epoch: 17900] train loss: 0.2947, train acc: 0.8784, val loss: 0.2704, val acc: 0.8820  (best train acc: 0.8903, best val acc: 0.8863, best train loss: 0.2688  @ epoch 17625 )\n",
      "[Epoch: 17920] train loss: 0.3122, train acc: 0.8675, val loss: 0.3051, val acc: 0.8624  (best train acc: 0.8903, best val acc: 0.8863, best train loss: 0.2688  @ epoch 17625 )\n",
      "[Epoch: 17940] train loss: 0.3090, train acc: 0.8686, val loss: 0.3151, val acc: 0.8644  (best train acc: 0.8903, best val acc: 0.8863, best train loss: 0.2688  @ epoch 17625 )\n",
      "[Epoch: 17960] train loss: 0.3156, train acc: 0.8694, val loss: 0.2961, val acc: 0.8658  (best train acc: 0.8903, best val acc: 0.8863, best train loss: 0.2688  @ epoch 17625 )\n",
      "[Epoch: 17980] train loss: 0.2795, train acc: 0.8825, val loss: 0.2905, val acc: 0.8759  (best train acc: 0.8925, best val acc: 0.8863, best train loss: 0.2683  @ epoch 17974 )\n",
      "[Epoch: 18000] train loss: 0.3025, train acc: 0.8713, val loss: 0.2871, val acc: 0.8712  (best train acc: 0.8925, best val acc: 0.8863, best train loss: 0.2683  @ epoch 17974 )\n",
      "[Epoch: 18020] train loss: 0.3017, train acc: 0.8712, val loss: 0.2991, val acc: 0.8651  (best train acc: 0.8925, best val acc: 0.8863, best train loss: 0.2683  @ epoch 17974 )\n",
      "[Epoch: 18040] train loss: 0.2918, train acc: 0.8781, val loss: 0.2691, val acc: 0.8793  (best train acc: 0.8925, best val acc: 0.8863, best train loss: 0.2683  @ epoch 17974 )\n",
      "[Epoch: 18060] train loss: 0.3037, train acc: 0.8769, val loss: 0.3103, val acc: 0.8664  (best train acc: 0.8925, best val acc: 0.8863, best train loss: 0.2683  @ epoch 17974 )\n",
      "[Epoch: 18080] train loss: 0.3133, train acc: 0.8704, val loss: 0.2901, val acc: 0.8789  (best train acc: 0.8925, best val acc: 0.8863, best train loss: 0.2683  @ epoch 17974 )\n",
      "[Epoch: 18100] train loss: 0.2856, train acc: 0.8802, val loss: 0.2748, val acc: 0.8779  (best train acc: 0.8925, best val acc: 0.8863, best train loss: 0.2683  @ epoch 17974 )\n",
      "[Epoch: 18120] train loss: 0.2911, train acc: 0.8849, val loss: 0.2732, val acc: 0.8806  (best train acc: 0.8925, best val acc: 0.8863, best train loss: 0.2683  @ epoch 17974 )\n",
      "[Epoch: 18140] train loss: 0.3564, train acc: 0.8496, val loss: 0.3386, val acc: 0.8516  (best train acc: 0.8925, best val acc: 0.8863, best train loss: 0.2683  @ epoch 17974 )\n",
      "[Epoch: 18160] train loss: 0.2936, train acc: 0.8790, val loss: 0.2797, val acc: 0.8776  (best train acc: 0.8925, best val acc: 0.8863, best train loss: 0.2683  @ epoch 17974 )\n",
      "[Epoch: 18180] train loss: 0.2847, train acc: 0.8806, val loss: 0.2766, val acc: 0.8782  (best train acc: 0.8925, best val acc: 0.8863, best train loss: 0.2683  @ epoch 17974 )\n",
      "[Epoch: 18200] train loss: 0.2879, train acc: 0.8842, val loss: 0.2879, val acc: 0.8772  (best train acc: 0.8925, best val acc: 0.8863, best train loss: 0.2683  @ epoch 17974 )\n",
      "[Epoch: 18220] train loss: 0.2786, train acc: 0.8867, val loss: 0.2930, val acc: 0.8728  (best train acc: 0.8925, best val acc: 0.8863, best train loss: 0.2683  @ epoch 17974 )\n",
      "[Epoch: 18240] train loss: 0.3072, train acc: 0.8703, val loss: 0.2896, val acc: 0.8745  (best train acc: 0.8925, best val acc: 0.8863, best train loss: 0.2683  @ epoch 17974 )\n",
      "[Epoch: 18260] train loss: 0.3005, train acc: 0.8790, val loss: 0.2754, val acc: 0.8830  (best train acc: 0.8925, best val acc: 0.8863, best train loss: 0.2683  @ epoch 17974 )\n",
      "[Epoch: 18280] train loss: 0.2914, train acc: 0.8836, val loss: 0.2725, val acc: 0.8782  (best train acc: 0.8925, best val acc: 0.8863, best train loss: 0.2683  @ epoch 17974 )\n",
      "[Epoch: 18300] train loss: 0.2934, train acc: 0.8767, val loss: 0.2747, val acc: 0.8789  (best train acc: 0.8925, best val acc: 0.8863, best train loss: 0.2683  @ epoch 17974 )\n",
      "[Epoch: 18320] train loss: 0.2956, train acc: 0.8762, val loss: 0.2671, val acc: 0.8850  (best train acc: 0.8925, best val acc: 0.8863, best train loss: 0.2683  @ epoch 17974 )\n",
      "[Epoch: 18340] train loss: 0.3086, train acc: 0.8664, val loss: 0.2947, val acc: 0.8641  (best train acc: 0.8925, best val acc: 0.8863, best train loss: 0.2683  @ epoch 17974 )\n",
      "[Epoch: 18360] train loss: 0.2819, train acc: 0.8810, val loss: 0.2813, val acc: 0.8806  (best train acc: 0.8925, best val acc: 0.8863, best train loss: 0.2683  @ epoch 17974 )\n",
      "[Epoch: 18380] train loss: 0.2829, train acc: 0.8817, val loss: 0.2661, val acc: 0.8843  (best train acc: 0.8925, best val acc: 0.8863, best train loss: 0.2683  @ epoch 17974 )\n",
      "[Epoch: 18400] train loss: 0.2956, train acc: 0.8746, val loss: 0.2772, val acc: 0.8769  (best train acc: 0.8925, best val acc: 0.8863, best train loss: 0.2683  @ epoch 17974 )\n",
      "[Epoch: 18420] train loss: 0.2745, train acc: 0.8830, val loss: 0.2923, val acc: 0.8715  (best train acc: 0.8925, best val acc: 0.8863, best train loss: 0.2683  @ epoch 17974 )\n",
      "[Epoch: 18440] train loss: 0.3441, train acc: 0.8585, val loss: 0.2796, val acc: 0.8796  (best train acc: 0.8925, best val acc: 0.8863, best train loss: 0.2683  @ epoch 17974 )\n",
      "[Epoch: 18460] train loss: 0.2908, train acc: 0.8745, val loss: 0.2758, val acc: 0.8826  (best train acc: 0.8925, best val acc: 0.8863, best train loss: 0.2683  @ epoch 17974 )\n",
      "[Epoch: 18480] train loss: 0.3207, train acc: 0.8737, val loss: 0.2955, val acc: 0.8691  (best train acc: 0.8925, best val acc: 0.8863, best train loss: 0.2683  @ epoch 17974 )\n",
      "[Epoch: 18500] train loss: 0.2835, train acc: 0.8807, val loss: 0.2697, val acc: 0.8826  (best train acc: 0.8925, best val acc: 0.8863, best train loss: 0.2683  @ epoch 17974 )\n",
      "[Epoch: 18520] train loss: 0.2939, train acc: 0.8743, val loss: 0.2813, val acc: 0.8722  (best train acc: 0.8938, best val acc: 0.8867, best train loss: 0.2683  @ epoch 17974 )\n",
      "[Epoch: 18540] train loss: 0.3092, train acc: 0.8725, val loss: 0.2729, val acc: 0.8826  (best train acc: 0.8938, best val acc: 0.8867, best train loss: 0.2683  @ epoch 17974 )\n",
      "[Epoch: 18560] train loss: 0.3012, train acc: 0.8757, val loss: 0.2931, val acc: 0.8722  (best train acc: 0.8938, best val acc: 0.8867, best train loss: 0.2683  @ epoch 17974 )\n",
      "[Epoch: 18580] train loss: 0.2827, train acc: 0.8798, val loss: 0.3196, val acc: 0.8536  (best train acc: 0.8938, best val acc: 0.8867, best train loss: 0.2683  @ epoch 17974 )\n",
      "[Epoch: 18600] train loss: 0.2777, train acc: 0.8845, val loss: 0.2796, val acc: 0.8803  (best train acc: 0.8938, best val acc: 0.8867, best train loss: 0.2683  @ epoch 17974 )\n",
      "[Epoch: 18620] train loss: 0.3048, train acc: 0.8806, val loss: 0.2737, val acc: 0.8793  (best train acc: 0.8938, best val acc: 0.8867, best train loss: 0.2683  @ epoch 17974 )\n",
      "[Epoch: 18640] train loss: 0.2880, train acc: 0.8812, val loss: 0.2805, val acc: 0.8796  (best train acc: 0.8938, best val acc: 0.8867, best train loss: 0.2674  @ epoch 18629 )\n",
      "[Epoch: 18660] train loss: 0.2798, train acc: 0.8825, val loss: 0.2769, val acc: 0.8742  (best train acc: 0.8938, best val acc: 0.8867, best train loss: 0.2674  @ epoch 18629 )\n",
      "[Epoch: 18680] train loss: 0.3164, train acc: 0.8657, val loss: 0.2770, val acc: 0.8799  (best train acc: 0.8952, best val acc: 0.8867, best train loss: 0.2630  @ epoch 18678 )\n",
      "[Epoch: 18700] train loss: 0.3067, train acc: 0.8770, val loss: 0.2778, val acc: 0.8752  (best train acc: 0.8952, best val acc: 0.8877, best train loss: 0.2630  @ epoch 18678 )\n",
      "[Epoch: 18720] train loss: 0.2999, train acc: 0.8765, val loss: 0.3040, val acc: 0.8691  (best train acc: 0.8952, best val acc: 0.8877, best train loss: 0.2630  @ epoch 18678 )\n",
      "[Epoch: 18740] train loss: 0.2840, train acc: 0.8790, val loss: 0.2779, val acc: 0.8830  (best train acc: 0.8952, best val acc: 0.8877, best train loss: 0.2630  @ epoch 18678 )\n",
      "[Epoch: 18760] train loss: 0.2770, train acc: 0.8851, val loss: 0.2656, val acc: 0.8816  (best train acc: 0.8952, best val acc: 0.8877, best train loss: 0.2630  @ epoch 18678 )\n",
      "[Epoch: 18780] train loss: 0.3023, train acc: 0.8722, val loss: 0.2984, val acc: 0.8664  (best train acc: 0.8952, best val acc: 0.8877, best train loss: 0.2630  @ epoch 18678 )\n",
      "[Epoch: 18800] train loss: 0.2813, train acc: 0.8897, val loss: 0.2929, val acc: 0.8685  (best train acc: 0.8952, best val acc: 0.8877, best train loss: 0.2630  @ epoch 18678 )\n",
      "[Epoch: 18820] train loss: 0.3208, train acc: 0.8632, val loss: 0.3226, val acc: 0.8610  (best train acc: 0.8952, best val acc: 0.8877, best train loss: 0.2630  @ epoch 18678 )\n",
      "[Epoch: 18840] train loss: 0.2950, train acc: 0.8764, val loss: 0.2793, val acc: 0.8799  (best train acc: 0.8952, best val acc: 0.8877, best train loss: 0.2630  @ epoch 18678 )\n",
      "[Epoch: 18860] train loss: 0.2965, train acc: 0.8749, val loss: 0.2980, val acc: 0.8715  (best train acc: 0.8952, best val acc: 0.8877, best train loss: 0.2630  @ epoch 18678 )\n",
      "[Epoch: 18880] train loss: 0.2909, train acc: 0.8817, val loss: 0.2979, val acc: 0.8732  (best train acc: 0.8952, best val acc: 0.8877, best train loss: 0.2630  @ epoch 18678 )\n",
      "[Epoch: 18900] train loss: 0.3082, train acc: 0.8688, val loss: 0.2715, val acc: 0.8843  (best train acc: 0.8952, best val acc: 0.8877, best train loss: 0.2630  @ epoch 18678 )\n",
      "[Epoch: 18920] train loss: 0.3503, train acc: 0.8636, val loss: 0.2653, val acc: 0.8816  (best train acc: 0.8952, best val acc: 0.8877, best train loss: 0.2630  @ epoch 18678 )\n",
      "[Epoch: 18940] train loss: 0.3067, train acc: 0.8746, val loss: 0.2807, val acc: 0.8776  (best train acc: 0.8952, best val acc: 0.8877, best train loss: 0.2630  @ epoch 18678 )\n",
      "[Epoch: 18960] train loss: 0.3558, train acc: 0.8496, val loss: 0.2880, val acc: 0.8732  (best train acc: 0.8952, best val acc: 0.8877, best train loss: 0.2630  @ epoch 18678 )\n",
      "[Epoch: 18980] train loss: 0.2754, train acc: 0.8842, val loss: 0.2744, val acc: 0.8759  (best train acc: 0.8952, best val acc: 0.8877, best train loss: 0.2630  @ epoch 18678 )\n",
      "[Epoch: 19000] train loss: 0.2817, train acc: 0.8817, val loss: 0.2799, val acc: 0.8779  (best train acc: 0.8952, best val acc: 0.8877, best train loss: 0.2630  @ epoch 18678 )\n",
      "[Epoch: 19020] train loss: 0.2836, train acc: 0.8843, val loss: 0.2702, val acc: 0.8769  (best train acc: 0.8952, best val acc: 0.8877, best train loss: 0.2630  @ epoch 18678 )\n",
      "[Epoch: 19040] train loss: 0.3038, train acc: 0.8743, val loss: 0.3041, val acc: 0.8685  (best train acc: 0.8952, best val acc: 0.8877, best train loss: 0.2630  @ epoch 18678 )\n",
      "[Epoch: 19060] train loss: 0.2859, train acc: 0.8810, val loss: 0.2686, val acc: 0.8786  (best train acc: 0.8952, best val acc: 0.8877, best train loss: 0.2630  @ epoch 18678 )\n",
      "[Epoch: 19080] train loss: 0.3069, train acc: 0.8757, val loss: 0.2964, val acc: 0.8725  (best train acc: 0.8952, best val acc: 0.8877, best train loss: 0.2630  @ epoch 18678 )\n",
      "[Epoch: 19100] train loss: 0.3025, train acc: 0.8770, val loss: 0.2732, val acc: 0.8742  (best train acc: 0.8952, best val acc: 0.8877, best train loss: 0.2630  @ epoch 18678 )\n",
      "[Epoch: 19120] train loss: 0.2713, train acc: 0.8851, val loss: 0.2682, val acc: 0.8830  (best train acc: 0.8952, best val acc: 0.8877, best train loss: 0.2630  @ epoch 18678 )\n",
      "[Epoch: 19140] train loss: 0.2983, train acc: 0.8761, val loss: 0.2766, val acc: 0.8776  (best train acc: 0.8952, best val acc: 0.8877, best train loss: 0.2630  @ epoch 18678 )\n",
      "[Epoch: 19160] train loss: 0.3114, train acc: 0.8651, val loss: 0.2736, val acc: 0.8809  (best train acc: 0.8952, best val acc: 0.8877, best train loss: 0.2630  @ epoch 18678 )\n",
      "[Epoch: 19180] train loss: 0.2926, train acc: 0.8787, val loss: 0.2697, val acc: 0.8836  (best train acc: 0.8952, best val acc: 0.8877, best train loss: 0.2630  @ epoch 18678 )\n",
      "[Epoch: 19200] train loss: 0.2927, train acc: 0.8817, val loss: 0.3002, val acc: 0.8651  (best train acc: 0.8952, best val acc: 0.8877, best train loss: 0.2630  @ epoch 18678 )\n",
      "[Epoch: 19220] train loss: 0.2920, train acc: 0.8803, val loss: 0.2842, val acc: 0.8715  (best train acc: 0.8952, best val acc: 0.8877, best train loss: 0.2630  @ epoch 18678 )\n",
      "[Epoch: 19240] train loss: 0.2985, train acc: 0.8705, val loss: 0.2802, val acc: 0.8782  (best train acc: 0.8952, best val acc: 0.8877, best train loss: 0.2630  @ epoch 18678 )\n",
      "[Epoch: 19260] train loss: 0.2997, train acc: 0.8787, val loss: 0.2705, val acc: 0.8850  (best train acc: 0.8952, best val acc: 0.8877, best train loss: 0.2630  @ epoch 18678 )\n",
      "[Epoch: 19280] train loss: 0.2930, train acc: 0.8764, val loss: 0.2907, val acc: 0.8691  (best train acc: 0.8952, best val acc: 0.8877, best train loss: 0.2630  @ epoch 18678 )\n",
      "[Epoch: 19300] train loss: 0.3440, train acc: 0.8597, val loss: 0.2822, val acc: 0.8776  (best train acc: 0.8952, best val acc: 0.8877, best train loss: 0.2617  @ epoch 19294 )\n",
      "[Epoch: 19320] train loss: 0.2875, train acc: 0.8801, val loss: 0.2681, val acc: 0.8847  (best train acc: 0.8965, best val acc: 0.8877, best train loss: 0.2617  @ epoch 19294 )\n",
      "[Epoch: 19340] train loss: 0.2924, train acc: 0.8856, val loss: 0.3010, val acc: 0.8702  (best train acc: 0.8965, best val acc: 0.8877, best train loss: 0.2617  @ epoch 19294 )\n",
      "[Epoch: 19360] train loss: 0.2764, train acc: 0.8789, val loss: 0.2694, val acc: 0.8786  (best train acc: 0.8965, best val acc: 0.8877, best train loss: 0.2617  @ epoch 19294 )\n",
      "[Epoch: 19380] train loss: 0.2766, train acc: 0.8871, val loss: 0.2774, val acc: 0.8769  (best train acc: 0.8965, best val acc: 0.8877, best train loss: 0.2617  @ epoch 19294 )\n",
      "[Epoch: 19400] train loss: 0.3054, train acc: 0.8729, val loss: 0.2712, val acc: 0.8799  (best train acc: 0.8965, best val acc: 0.8877, best train loss: 0.2617  @ epoch 19294 )\n",
      "[Epoch: 19420] train loss: 0.2949, train acc: 0.8715, val loss: 0.2715, val acc: 0.8809  (best train acc: 0.8965, best val acc: 0.8877, best train loss: 0.2617  @ epoch 19294 )\n",
      "[Epoch: 19440] train loss: 0.2833, train acc: 0.8806, val loss: 0.2847, val acc: 0.8782  (best train acc: 0.8965, best val acc: 0.8877, best train loss: 0.2617  @ epoch 19294 )\n",
      "[Epoch: 19460] train loss: 0.3048, train acc: 0.8739, val loss: 0.2680, val acc: 0.8782  (best train acc: 0.8965, best val acc: 0.8877, best train loss: 0.2617  @ epoch 19294 )\n",
      "[Epoch: 19480] train loss: 0.2941, train acc: 0.8810, val loss: 0.2881, val acc: 0.8769  (best train acc: 0.8965, best val acc: 0.8877, best train loss: 0.2617  @ epoch 19294 )\n",
      "[Epoch: 19500] train loss: 0.3094, train acc: 0.8694, val loss: 0.2707, val acc: 0.8820  (best train acc: 0.8965, best val acc: 0.8877, best train loss: 0.2617  @ epoch 19294 )\n",
      "[Epoch: 19520] train loss: 0.2891, train acc: 0.8767, val loss: 0.2865, val acc: 0.8755  (best train acc: 0.8965, best val acc: 0.8877, best train loss: 0.2617  @ epoch 19294 )\n",
      "[Epoch: 19540] train loss: 0.2928, train acc: 0.8745, val loss: 0.3072, val acc: 0.8627  (best train acc: 0.8965, best val acc: 0.8877, best train loss: 0.2617  @ epoch 19294 )\n",
      "[Epoch: 19560] train loss: 0.3013, train acc: 0.8748, val loss: 0.2771, val acc: 0.8796  (best train acc: 0.8965, best val acc: 0.8877, best train loss: 0.2617  @ epoch 19294 )\n",
      "[Epoch: 19580] train loss: 0.2869, train acc: 0.8791, val loss: 0.2690, val acc: 0.8796  (best train acc: 0.8965, best val acc: 0.8877, best train loss: 0.2617  @ epoch 19294 )\n",
      "[Epoch: 19600] train loss: 0.3037, train acc: 0.8738, val loss: 0.3022, val acc: 0.8702  (best train acc: 0.8965, best val acc: 0.8877, best train loss: 0.2617  @ epoch 19294 )\n",
      "[Epoch: 19620] train loss: 0.3087, train acc: 0.8736, val loss: 0.2735, val acc: 0.8742  (best train acc: 0.8965, best val acc: 0.8877, best train loss: 0.2617  @ epoch 19294 )\n",
      "[Epoch: 19640] train loss: 0.3204, train acc: 0.8628, val loss: 0.2829, val acc: 0.8782  (best train acc: 0.8965, best val acc: 0.8877, best train loss: 0.2617  @ epoch 19294 )\n",
      "[Epoch: 19660] train loss: 0.3773, train acc: 0.8495, val loss: 0.3071, val acc: 0.8678  (best train acc: 0.8965, best val acc: 0.8877, best train loss: 0.2617  @ epoch 19294 )\n",
      "[Epoch: 19680] train loss: 0.2814, train acc: 0.8817, val loss: 0.2743, val acc: 0.8766  (best train acc: 0.8965, best val acc: 0.8877, best train loss: 0.2617  @ epoch 19294 )\n",
      "[Epoch: 19700] train loss: 0.2743, train acc: 0.8823, val loss: 0.2734, val acc: 0.8816  (best train acc: 0.8965, best val acc: 0.8877, best train loss: 0.2617  @ epoch 19294 )\n",
      "[Epoch: 19720] train loss: 0.2714, train acc: 0.8830, val loss: 0.2647, val acc: 0.8776  (best train acc: 0.8965, best val acc: 0.8877, best train loss: 0.2617  @ epoch 19294 )\n",
      "[Epoch: 19740] train loss: 0.2962, train acc: 0.8770, val loss: 0.2751, val acc: 0.8779  (best train acc: 0.8965, best val acc: 0.8877, best train loss: 0.2617  @ epoch 19294 )\n",
      "[Epoch: 19760] train loss: 0.2762, train acc: 0.8816, val loss: 0.2759, val acc: 0.8782  (best train acc: 0.8965, best val acc: 0.8877, best train loss: 0.2617  @ epoch 19294 )\n",
      "[Epoch: 19780] train loss: 0.2773, train acc: 0.8815, val loss: 0.2711, val acc: 0.8796  (best train acc: 0.8965, best val acc: 0.8877, best train loss: 0.2617  @ epoch 19294 )\n",
      "[Epoch: 19800] train loss: 0.2868, train acc: 0.8890, val loss: 0.3221, val acc: 0.8577  (best train acc: 0.8965, best val acc: 0.8877, best train loss: 0.2617  @ epoch 19294 )\n",
      "[Epoch: 19820] train loss: 0.2981, train acc: 0.8769, val loss: 0.2875, val acc: 0.8799  (best train acc: 0.8965, best val acc: 0.8877, best train loss: 0.2617  @ epoch 19294 )\n",
      "[Epoch: 19840] train loss: 0.2796, train acc: 0.8864, val loss: 0.2880, val acc: 0.8708  (best train acc: 0.8965, best val acc: 0.8877, best train loss: 0.2617  @ epoch 19294 )\n",
      "[Epoch: 19860] train loss: 0.2713, train acc: 0.8897, val loss: 0.2741, val acc: 0.8806  (best train acc: 0.8965, best val acc: 0.8877, best train loss: 0.2617  @ epoch 19294 )\n",
      "[Epoch: 19880] train loss: 0.2943, train acc: 0.8809, val loss: 0.2851, val acc: 0.8759  (best train acc: 0.8965, best val acc: 0.8877, best train loss: 0.2617  @ epoch 19294 )\n",
      "[Epoch: 19900] train loss: 0.2751, train acc: 0.8864, val loss: 0.2749, val acc: 0.8813  (best train acc: 0.8965, best val acc: 0.8877, best train loss: 0.2617  @ epoch 19294 )\n",
      "[Epoch: 19920] train loss: 0.2953, train acc: 0.8797, val loss: 0.3018, val acc: 0.8671  (best train acc: 0.8965, best val acc: 0.8877, best train loss: 0.2617  @ epoch 19294 )\n",
      "[Epoch: 19940] train loss: 0.2838, train acc: 0.8809, val loss: 0.2686, val acc: 0.8759  (best train acc: 0.8965, best val acc: 0.8877, best train loss: 0.2617  @ epoch 19294 )\n",
      "[Epoch: 19960] train loss: 0.2905, train acc: 0.8828, val loss: 0.2723, val acc: 0.8793  (best train acc: 0.8965, best val acc: 0.8877, best train loss: 0.2617  @ epoch 19294 )\n",
      "[Epoch: 19980] train loss: 0.3010, train acc: 0.8717, val loss: 0.2996, val acc: 0.8688  (best train acc: 0.8965, best val acc: 0.8877, best train loss: 0.2617  @ epoch 19294 )\n",
      "[Epoch: 20000] train loss: 0.2843, train acc: 0.8772, val loss: 0.2801, val acc: 0.8803  (best train acc: 0.8965, best val acc: 0.8877, best train loss: 0.2617  @ epoch 19294 )\n",
      "[Epoch: 20020] train loss: 0.2851, train acc: 0.8817, val loss: 0.2712, val acc: 0.8809  (best train acc: 0.8965, best val acc: 0.8880, best train loss: 0.2617  @ epoch 19294 )\n",
      "[Epoch: 20040] train loss: 0.3083, train acc: 0.8723, val loss: 0.2716, val acc: 0.8769  (best train acc: 0.8965, best val acc: 0.8880, best train loss: 0.2617  @ epoch 19294 )\n",
      "[Epoch: 20060] train loss: 0.2892, train acc: 0.8767, val loss: 0.2850, val acc: 0.8739  (best train acc: 0.8965, best val acc: 0.8880, best train loss: 0.2617  @ epoch 19294 )\n",
      "[Epoch: 20080] train loss: 0.3176, train acc: 0.8726, val loss: 0.2789, val acc: 0.8769  (best train acc: 0.8965, best val acc: 0.8880, best train loss: 0.2617  @ epoch 19294 )\n",
      "[Epoch: 20100] train loss: 0.2981, train acc: 0.8762, val loss: 0.2695, val acc: 0.8823  (best train acc: 0.8965, best val acc: 0.8880, best train loss: 0.2617  @ epoch 19294 )\n",
      "[Epoch: 20120] train loss: 0.2705, train acc: 0.8898, val loss: 0.2835, val acc: 0.8752  (best train acc: 0.8965, best val acc: 0.8880, best train loss: 0.2617  @ epoch 19294 )\n",
      "[Epoch: 20140] train loss: 0.2902, train acc: 0.8777, val loss: 0.2778, val acc: 0.8762  (best train acc: 0.8965, best val acc: 0.8880, best train loss: 0.2617  @ epoch 19294 )\n",
      "[Epoch: 20160] train loss: 0.3021, train acc: 0.8759, val loss: 0.2767, val acc: 0.8809  (best train acc: 0.8965, best val acc: 0.8880, best train loss: 0.2603  @ epoch 20157 )\n",
      "[Epoch: 20180] train loss: 0.3134, train acc: 0.8697, val loss: 0.2862, val acc: 0.8776  (best train acc: 0.8965, best val acc: 0.8880, best train loss: 0.2603  @ epoch 20157 )\n",
      "[Epoch: 20200] train loss: 0.3067, train acc: 0.8754, val loss: 0.3050, val acc: 0.8728  (best train acc: 0.8965, best val acc: 0.8880, best train loss: 0.2603  @ epoch 20157 )\n",
      "[Epoch: 20220] train loss: 0.2954, train acc: 0.8738, val loss: 0.3156, val acc: 0.8631  (best train acc: 0.8965, best val acc: 0.8880, best train loss: 0.2603  @ epoch 20157 )\n",
      "[Epoch: 20240] train loss: 0.3130, train acc: 0.8720, val loss: 0.2722, val acc: 0.8796  (best train acc: 0.8965, best val acc: 0.8880, best train loss: 0.2603  @ epoch 20157 )\n",
      "[Epoch: 20260] train loss: 0.2913, train acc: 0.8886, val loss: 0.2838, val acc: 0.8793  (best train acc: 0.8965, best val acc: 0.8880, best train loss: 0.2603  @ epoch 20157 )\n",
      "[Epoch: 20280] train loss: 0.3052, train acc: 0.8831, val loss: 0.2858, val acc: 0.8789  (best train acc: 0.8965, best val acc: 0.8880, best train loss: 0.2603  @ epoch 20157 )\n",
      "[Epoch: 20300] train loss: 0.2891, train acc: 0.8783, val loss: 0.2805, val acc: 0.8793  (best train acc: 0.8965, best val acc: 0.8880, best train loss: 0.2603  @ epoch 20157 )\n",
      "[Epoch: 20320] train loss: 0.3671, train acc: 0.8380, val loss: 0.2747, val acc: 0.8799  (best train acc: 0.8965, best val acc: 0.8880, best train loss: 0.2603  @ epoch 20157 )\n",
      "[Epoch: 20340] train loss: 0.2873, train acc: 0.8785, val loss: 0.2712, val acc: 0.8786  (best train acc: 0.8965, best val acc: 0.8880, best train loss: 0.2603  @ epoch 20157 )\n",
      "[Epoch: 20360] train loss: 0.2762, train acc: 0.8815, val loss: 0.2700, val acc: 0.8813  (best train acc: 0.8965, best val acc: 0.8880, best train loss: 0.2603  @ epoch 20157 )\n",
      "[Epoch: 20380] train loss: 0.3092, train acc: 0.8689, val loss: 0.2806, val acc: 0.8796  (best train acc: 0.8965, best val acc: 0.8880, best train loss: 0.2603  @ epoch 20157 )\n",
      "[Epoch: 20400] train loss: 0.2823, train acc: 0.8805, val loss: 0.2774, val acc: 0.8806  (best train acc: 0.8965, best val acc: 0.8880, best train loss: 0.2603  @ epoch 20157 )\n",
      "[Epoch: 20420] train loss: 0.2896, train acc: 0.8798, val loss: 0.2788, val acc: 0.8796  (best train acc: 0.8965, best val acc: 0.8880, best train loss: 0.2603  @ epoch 20157 )\n",
      "[Epoch: 20440] train loss: 0.3104, train acc: 0.8744, val loss: 0.2810, val acc: 0.8749  (best train acc: 0.8965, best val acc: 0.8880, best train loss: 0.2603  @ epoch 20157 )\n",
      "[Epoch: 20460] train loss: 0.2657, train acc: 0.8908, val loss: 0.2757, val acc: 0.8772  (best train acc: 0.8965, best val acc: 0.8880, best train loss: 0.2603  @ epoch 20157 )\n",
      "[Epoch: 20480] train loss: 0.3014, train acc: 0.8758, val loss: 0.2946, val acc: 0.8712  (best train acc: 0.8965, best val acc: 0.8880, best train loss: 0.2603  @ epoch 20157 )\n",
      "[Epoch: 20500] train loss: 0.2628, train acc: 0.8903, val loss: 0.2734, val acc: 0.8796  (best train acc: 0.8965, best val acc: 0.8880, best train loss: 0.2603  @ epoch 20157 )\n",
      "[Epoch: 20520] train loss: 0.2697, train acc: 0.8871, val loss: 0.2842, val acc: 0.8786  (best train acc: 0.8965, best val acc: 0.8880, best train loss: 0.2603  @ epoch 20157 )\n",
      "[Epoch: 20540] train loss: 0.2748, train acc: 0.8890, val loss: 0.2726, val acc: 0.8826  (best train acc: 0.8965, best val acc: 0.8880, best train loss: 0.2603  @ epoch 20157 )\n",
      "[Epoch: 20560] train loss: 0.2882, train acc: 0.8811, val loss: 0.2695, val acc: 0.8863  (best train acc: 0.8965, best val acc: 0.8880, best train loss: 0.2603  @ epoch 20157 )\n",
      "[Epoch: 20580] train loss: 0.3059, train acc: 0.8757, val loss: 0.3257, val acc: 0.8530  (best train acc: 0.8965, best val acc: 0.8880, best train loss: 0.2603  @ epoch 20157 )\n",
      "[Epoch: 20600] train loss: 0.2858, train acc: 0.8819, val loss: 0.2689, val acc: 0.8830  (best train acc: 0.8965, best val acc: 0.8880, best train loss: 0.2603  @ epoch 20157 )\n",
      "[Epoch: 20620] train loss: 0.2777, train acc: 0.8814, val loss: 0.2919, val acc: 0.8728  (best train acc: 0.8965, best val acc: 0.8880, best train loss: 0.2603  @ epoch 20157 )\n",
      "[Epoch: 20640] train loss: 0.2788, train acc: 0.8812, val loss: 0.2823, val acc: 0.8779  (best train acc: 0.8965, best val acc: 0.8880, best train loss: 0.2603  @ epoch 20157 )\n",
      "[Epoch: 20660] train loss: 0.2752, train acc: 0.8887, val loss: 0.2807, val acc: 0.8786  (best train acc: 0.8965, best val acc: 0.8880, best train loss: 0.2603  @ epoch 20157 )\n",
      "[Epoch: 20680] train loss: 0.2650, train acc: 0.8903, val loss: 0.2729, val acc: 0.8776  (best train acc: 0.8965, best val acc: 0.8880, best train loss: 0.2603  @ epoch 20157 )\n",
      "[Epoch: 20700] train loss: 0.2753, train acc: 0.8854, val loss: 0.2764, val acc: 0.8769  (best train acc: 0.8965, best val acc: 0.8880, best train loss: 0.2603  @ epoch 20157 )\n",
      "[Epoch: 20720] train loss: 0.3153, train acc: 0.8652, val loss: 0.2765, val acc: 0.8799  (best train acc: 0.8965, best val acc: 0.8880, best train loss: 0.2603  @ epoch 20157 )\n",
      "[Epoch: 20740] train loss: 0.2784, train acc: 0.8880, val loss: 0.2886, val acc: 0.8789  (best train acc: 0.8965, best val acc: 0.8880, best train loss: 0.2603  @ epoch 20157 )\n",
      "[Epoch: 20760] train loss: 0.2760, train acc: 0.8866, val loss: 0.2978, val acc: 0.8718  (best train acc: 0.8965, best val acc: 0.8880, best train loss: 0.2603  @ epoch 20157 )\n",
      "[Epoch: 20780] train loss: 0.2962, train acc: 0.8796, val loss: 0.3233, val acc: 0.8648  (best train acc: 0.8965, best val acc: 0.8880, best train loss: 0.2603  @ epoch 20157 )\n",
      "[Epoch: 20800] train loss: 0.2825, train acc: 0.8834, val loss: 0.2748, val acc: 0.8799  (best train acc: 0.8965, best val acc: 0.8880, best train loss: 0.2603  @ epoch 20157 )\n",
      "[Epoch: 20820] train loss: 0.3007, train acc: 0.8794, val loss: 0.2684, val acc: 0.8806  (best train acc: 0.8965, best val acc: 0.8880, best train loss: 0.2603  @ epoch 20157 )\n",
      "[Epoch: 20840] train loss: 0.3044, train acc: 0.8752, val loss: 0.2777, val acc: 0.8752  (best train acc: 0.8965, best val acc: 0.8880, best train loss: 0.2603  @ epoch 20157 )\n",
      "[Epoch: 20860] train loss: 0.2890, train acc: 0.8787, val loss: 0.2822, val acc: 0.8762  (best train acc: 0.8965, best val acc: 0.8880, best train loss: 0.2603  @ epoch 20157 )\n",
      "[Epoch: 20880] train loss: 0.2768, train acc: 0.8856, val loss: 0.3084, val acc: 0.8688  (best train acc: 0.8965, best val acc: 0.8880, best train loss: 0.2603  @ epoch 20157 )\n",
      "[Epoch: 20900] train loss: 0.3035, train acc: 0.8739, val loss: 0.2753, val acc: 0.8786  (best train acc: 0.8965, best val acc: 0.8880, best train loss: 0.2603  @ epoch 20157 )\n",
      "[Epoch: 20920] train loss: 0.3247, train acc: 0.8615, val loss: 0.2776, val acc: 0.8786  (best train acc: 0.8965, best val acc: 0.8880, best train loss: 0.2603  @ epoch 20157 )\n",
      "[Epoch: 20940] train loss: 0.2981, train acc: 0.8754, val loss: 0.2801, val acc: 0.8786  (best train acc: 0.8965, best val acc: 0.8880, best train loss: 0.2603  @ epoch 20157 )\n",
      "[Epoch: 20960] train loss: 0.2711, train acc: 0.8847, val loss: 0.2806, val acc: 0.8772  (best train acc: 0.8965, best val acc: 0.8880, best train loss: 0.2603  @ epoch 20157 )\n",
      "[Epoch: 20980] train loss: 0.2679, train acc: 0.8873, val loss: 0.2734, val acc: 0.8799  (best train acc: 0.8965, best val acc: 0.8880, best train loss: 0.2603  @ epoch 20157 )\n",
      "[Epoch: 21000] train loss: 0.2719, train acc: 0.8865, val loss: 0.2754, val acc: 0.8826  (best train acc: 0.8965, best val acc: 0.8880, best train loss: 0.2576  @ epoch 20984 )\n",
      "[Epoch: 21020] train loss: 0.3191, train acc: 0.8710, val loss: 0.2919, val acc: 0.8739  (best train acc: 0.8965, best val acc: 0.8880, best train loss: 0.2576  @ epoch 20984 )\n",
      "[Epoch: 21040] train loss: 0.2906, train acc: 0.8828, val loss: 0.2855, val acc: 0.8806  (best train acc: 0.8965, best val acc: 0.8880, best train loss: 0.2576  @ epoch 20984 )\n",
      "[Epoch: 21060] train loss: 0.2844, train acc: 0.8826, val loss: 0.2822, val acc: 0.8853  (best train acc: 0.8965, best val acc: 0.8880, best train loss: 0.2576  @ epoch 20984 )\n",
      "[Epoch: 21080] train loss: 0.2914, train acc: 0.8741, val loss: 0.2899, val acc: 0.8772  (best train acc: 0.8965, best val acc: 0.8880, best train loss: 0.2576  @ epoch 20984 )\n",
      "[Epoch: 21100] train loss: 0.2928, train acc: 0.8812, val loss: 0.2870, val acc: 0.8836  (best train acc: 0.8965, best val acc: 0.8880, best train loss: 0.2576  @ epoch 20984 )\n",
      "[Epoch: 21120] train loss: 0.2928, train acc: 0.8800, val loss: 0.2742, val acc: 0.8789  (best train acc: 0.8965, best val acc: 0.8880, best train loss: 0.2576  @ epoch 20984 )\n",
      "[Epoch: 21140] train loss: 0.2771, train acc: 0.8853, val loss: 0.3209, val acc: 0.8685  (best train acc: 0.8965, best val acc: 0.8880, best train loss: 0.2576  @ epoch 20984 )\n",
      "[Epoch: 21160] train loss: 0.2953, train acc: 0.8775, val loss: 0.3153, val acc: 0.8668  (best train acc: 0.8965, best val acc: 0.8880, best train loss: 0.2576  @ epoch 20984 )\n",
      "[Epoch: 21180] train loss: 0.3133, train acc: 0.8756, val loss: 0.2809, val acc: 0.8847  (best train acc: 0.8965, best val acc: 0.8880, best train loss: 0.2576  @ epoch 20984 )\n",
      "[Epoch: 21200] train loss: 0.2655, train acc: 0.8900, val loss: 0.2712, val acc: 0.8843  (best train acc: 0.8973, best val acc: 0.8880, best train loss: 0.2576  @ epoch 20984 )\n",
      "[Epoch: 21220] train loss: 0.2829, train acc: 0.8825, val loss: 0.2808, val acc: 0.8826  (best train acc: 0.8973, best val acc: 0.8880, best train loss: 0.2576  @ epoch 20984 )\n",
      "[Epoch: 21240] train loss: 0.2742, train acc: 0.8879, val loss: 0.2760, val acc: 0.8809  (best train acc: 0.8973, best val acc: 0.8880, best train loss: 0.2576  @ epoch 20984 )\n",
      "[Epoch: 21260] train loss: 0.2817, train acc: 0.8855, val loss: 0.2829, val acc: 0.8833  (best train acc: 0.8973, best val acc: 0.8880, best train loss: 0.2576  @ epoch 20984 )\n",
      "[Epoch: 21280] train loss: 0.2934, train acc: 0.8743, val loss: 0.2971, val acc: 0.8725  (best train acc: 0.8973, best val acc: 0.8880, best train loss: 0.2576  @ epoch 20984 )\n",
      "[Epoch: 21300] train loss: 0.2788, train acc: 0.8843, val loss: 0.2821, val acc: 0.8809  (best train acc: 0.8973, best val acc: 0.8880, best train loss: 0.2576  @ epoch 20984 )\n",
      "[Epoch: 21320] train loss: 0.2761, train acc: 0.8857, val loss: 0.2977, val acc: 0.8755  (best train acc: 0.8973, best val acc: 0.8880, best train loss: 0.2576  @ epoch 20984 )\n",
      "[Epoch: 21340] train loss: 0.2775, train acc: 0.8847, val loss: 0.2739, val acc: 0.8857  (best train acc: 0.8973, best val acc: 0.8880, best train loss: 0.2576  @ epoch 20984 )\n",
      "[Epoch: 21360] train loss: 0.2825, train acc: 0.8820, val loss: 0.2729, val acc: 0.8836  (best train acc: 0.8973, best val acc: 0.8880, best train loss: 0.2576  @ epoch 20984 )\n",
      "[Epoch: 21380] train loss: 0.2835, train acc: 0.8809, val loss: 0.3009, val acc: 0.8749  (best train acc: 0.8973, best val acc: 0.8880, best train loss: 0.2576  @ epoch 20984 )\n",
      "[Epoch: 21400] train loss: 0.3379, train acc: 0.8566, val loss: 0.3309, val acc: 0.8567  (best train acc: 0.8973, best val acc: 0.8880, best train loss: 0.2576  @ epoch 20984 )\n",
      "[Epoch: 21420] train loss: 0.3161, train acc: 0.8699, val loss: 0.2963, val acc: 0.8793  (best train acc: 0.8973, best val acc: 0.8880, best train loss: 0.2576  @ epoch 20984 )\n",
      "[Epoch: 21440] train loss: 0.3058, train acc: 0.8757, val loss: 0.2970, val acc: 0.8769  (best train acc: 0.8973, best val acc: 0.8880, best train loss: 0.2576  @ epoch 20984 )\n",
      "[Epoch: 21460] train loss: 0.2867, train acc: 0.8772, val loss: 0.2875, val acc: 0.8786  (best train acc: 0.8973, best val acc: 0.8880, best train loss: 0.2576  @ epoch 20984 )\n",
      "[Epoch: 21480] train loss: 0.2621, train acc: 0.8954, val loss: 0.2756, val acc: 0.8820  (best train acc: 0.8973, best val acc: 0.8880, best train loss: 0.2576  @ epoch 20984 )\n",
      "[Epoch: 21500] train loss: 0.3445, train acc: 0.8620, val loss: 0.2904, val acc: 0.8779  (best train acc: 0.8973, best val acc: 0.8880, best train loss: 0.2576  @ epoch 20984 )\n",
      "[Epoch: 21520] train loss: 0.2790, train acc: 0.8853, val loss: 0.2856, val acc: 0.8799  (best train acc: 0.8973, best val acc: 0.8880, best train loss: 0.2576  @ epoch 20984 )\n",
      "[Epoch: 21540] train loss: 0.2600, train acc: 0.8947, val loss: 0.2730, val acc: 0.8833  (best train acc: 0.8973, best val acc: 0.8880, best train loss: 0.2576  @ epoch 20984 )\n",
      "[Epoch: 21560] train loss: 0.2782, train acc: 0.8845, val loss: 0.2976, val acc: 0.8702  (best train acc: 0.8973, best val acc: 0.8880, best train loss: 0.2576  @ epoch 20984 )\n",
      "[Epoch: 21580] train loss: 0.2652, train acc: 0.8895, val loss: 0.2756, val acc: 0.8820  (best train acc: 0.8973, best val acc: 0.8880, best train loss: 0.2576  @ epoch 20984 )\n",
      "[Epoch: 21600] train loss: 0.2825, train acc: 0.8821, val loss: 0.2717, val acc: 0.8833  (best train acc: 0.8973, best val acc: 0.8880, best train loss: 0.2564  @ epoch 21590 )\n",
      "[Epoch: 21620] train loss: 0.2777, train acc: 0.8838, val loss: 0.2865, val acc: 0.8779  (best train acc: 0.8973, best val acc: 0.8880, best train loss: 0.2564  @ epoch 21590 )\n",
      "[Epoch: 21640] train loss: 0.3353, train acc: 0.8657, val loss: 0.2817, val acc: 0.8796  (best train acc: 0.8973, best val acc: 0.8880, best train loss: 0.2564  @ epoch 21590 )\n",
      "[Epoch: 21660] train loss: 0.2753, train acc: 0.8888, val loss: 0.2957, val acc: 0.8732  (best train acc: 0.8973, best val acc: 0.8880, best train loss: 0.2564  @ epoch 21590 )\n",
      "[Epoch: 21680] train loss: 0.2797, train acc: 0.8866, val loss: 0.2915, val acc: 0.8718  (best train acc: 0.8973, best val acc: 0.8880, best train loss: 0.2564  @ epoch 21590 )\n",
      "[Epoch: 21700] train loss: 0.2770, train acc: 0.8889, val loss: 0.2730, val acc: 0.8823  (best train acc: 0.8973, best val acc: 0.8880, best train loss: 0.2564  @ epoch 21590 )\n",
      "[Epoch: 21720] train loss: 0.3363, train acc: 0.8573, val loss: 0.3264, val acc: 0.8617  (best train acc: 0.8973, best val acc: 0.8880, best train loss: 0.2564  @ epoch 21590 )\n",
      "[Epoch: 21740] train loss: 0.2876, train acc: 0.8851, val loss: 0.2764, val acc: 0.8823  (best train acc: 0.8973, best val acc: 0.8880, best train loss: 0.2564  @ epoch 21590 )\n",
      "[Epoch: 21760] train loss: 0.2915, train acc: 0.8816, val loss: 0.2821, val acc: 0.8813  (best train acc: 0.8973, best val acc: 0.8880, best train loss: 0.2564  @ epoch 21590 )\n",
      "[Epoch: 21780] train loss: 0.2973, train acc: 0.8734, val loss: 0.2821, val acc: 0.8782  (best train acc: 0.8973, best val acc: 0.8880, best train loss: 0.2564  @ epoch 21590 )\n",
      "[Epoch: 21800] train loss: 0.3338, train acc: 0.8683, val loss: 0.3025, val acc: 0.8755  (best train acc: 0.8973, best val acc: 0.8880, best train loss: 0.2564  @ epoch 21590 )\n",
      "[Epoch: 21820] train loss: 0.2711, train acc: 0.8889, val loss: 0.2798, val acc: 0.8799  (best train acc: 0.8973, best val acc: 0.8884, best train loss: 0.2564  @ epoch 21590 )\n",
      "[Epoch: 21840] train loss: 0.2914, train acc: 0.8775, val loss: 0.2764, val acc: 0.8816  (best train acc: 0.8973, best val acc: 0.8884, best train loss: 0.2564  @ epoch 21590 )\n",
      "[Epoch: 21860] train loss: 0.2888, train acc: 0.8828, val loss: 0.2762, val acc: 0.8860  (best train acc: 0.8973, best val acc: 0.8884, best train loss: 0.2564  @ epoch 21590 )\n",
      "[Epoch: 21880] train loss: 0.2867, train acc: 0.8830, val loss: 0.2859, val acc: 0.8793  (best train acc: 0.8973, best val acc: 0.8884, best train loss: 0.2564  @ epoch 21590 )\n",
      "[Epoch: 21900] train loss: 0.3181, train acc: 0.8719, val loss: 0.2788, val acc: 0.8803  (best train acc: 0.8973, best val acc: 0.8884, best train loss: 0.2564  @ epoch 21590 )\n",
      "[Epoch: 21920] train loss: 0.2780, train acc: 0.8850, val loss: 0.2833, val acc: 0.8793  (best train acc: 0.8973, best val acc: 0.8884, best train loss: 0.2564  @ epoch 21590 )\n",
      "[Epoch: 21940] train loss: 0.2690, train acc: 0.8891, val loss: 0.2824, val acc: 0.8836  (best train acc: 0.8973, best val acc: 0.8884, best train loss: 0.2564  @ epoch 21590 )\n",
      "[Epoch: 21960] train loss: 0.2900, train acc: 0.8856, val loss: 0.2861, val acc: 0.8803  (best train acc: 0.8973, best val acc: 0.8884, best train loss: 0.2564  @ epoch 21590 )\n",
      "[Epoch: 21980] train loss: 0.2801, train acc: 0.8861, val loss: 0.2782, val acc: 0.8836  (best train acc: 0.8973, best val acc: 0.8884, best train loss: 0.2564  @ epoch 21590 )\n",
      "[Epoch: 22000] train loss: 0.2739, train acc: 0.8882, val loss: 0.2783, val acc: 0.8803  (best train acc: 0.8973, best val acc: 0.8884, best train loss: 0.2564  @ epoch 21590 )\n",
      "[Epoch: 22020] train loss: 0.2796, train acc: 0.8820, val loss: 0.2770, val acc: 0.8806  (best train acc: 0.8973, best val acc: 0.8884, best train loss: 0.2564  @ epoch 21590 )\n",
      "[Epoch: 22040] train loss: 0.2658, train acc: 0.8932, val loss: 0.2813, val acc: 0.8830  (best train acc: 0.8973, best val acc: 0.8884, best train loss: 0.2564  @ epoch 21590 )\n",
      "[Epoch: 22060] train loss: 0.2945, train acc: 0.8770, val loss: 0.2765, val acc: 0.8816  (best train acc: 0.8973, best val acc: 0.8884, best train loss: 0.2564  @ epoch 21590 )\n",
      "[Epoch: 22080] train loss: 0.2842, train acc: 0.8832, val loss: 0.2970, val acc: 0.8742  (best train acc: 0.8973, best val acc: 0.8884, best train loss: 0.2564  @ epoch 21590 )\n",
      "[Epoch: 22100] train loss: 0.2838, train acc: 0.8785, val loss: 0.2934, val acc: 0.8766  (best train acc: 0.8973, best val acc: 0.8884, best train loss: 0.2564  @ epoch 21590 )\n",
      "[Epoch: 22120] train loss: 0.2934, train acc: 0.8754, val loss: 0.2967, val acc: 0.8739  (best train acc: 0.8973, best val acc: 0.8884, best train loss: 0.2564  @ epoch 21590 )\n",
      "[Epoch: 22140] train loss: 0.2690, train acc: 0.8910, val loss: 0.2803, val acc: 0.8830  (best train acc: 0.8973, best val acc: 0.8884, best train loss: 0.2564  @ epoch 21590 )\n",
      "[Epoch: 22160] train loss: 0.2776, train acc: 0.8849, val loss: 0.2759, val acc: 0.8836  (best train acc: 0.8985, best val acc: 0.8884, best train loss: 0.2536  @ epoch 22155 )\n",
      "[Epoch: 22180] train loss: 0.2836, train acc: 0.8834, val loss: 0.2830, val acc: 0.8823  (best train acc: 0.8985, best val acc: 0.8884, best train loss: 0.2515  @ epoch 22161 )\n",
      "[Epoch: 22200] train loss: 0.2710, train acc: 0.8874, val loss: 0.2724, val acc: 0.8803  (best train acc: 0.8985, best val acc: 0.8884, best train loss: 0.2515  @ epoch 22161 )\n",
      "[Epoch: 22220] train loss: 0.3124, train acc: 0.8717, val loss: 0.2752, val acc: 0.8857  (best train acc: 0.8985, best val acc: 0.8884, best train loss: 0.2515  @ epoch 22161 )\n",
      "[Epoch: 22240] train loss: 0.2921, train acc: 0.8809, val loss: 0.2708, val acc: 0.8843  (best train acc: 0.8985, best val acc: 0.8884, best train loss: 0.2515  @ epoch 22161 )\n",
      "[Epoch: 22260] train loss: 0.2830, train acc: 0.8834, val loss: 0.2758, val acc: 0.8853  (best train acc: 0.8985, best val acc: 0.8884, best train loss: 0.2515  @ epoch 22161 )\n",
      "[Epoch: 22280] train loss: 0.3000, train acc: 0.8807, val loss: 0.2885, val acc: 0.8799  (best train acc: 0.8985, best val acc: 0.8884, best train loss: 0.2515  @ epoch 22161 )\n",
      "[Epoch: 22300] train loss: 0.2764, train acc: 0.8912, val loss: 0.2883, val acc: 0.8759  (best train acc: 0.8985, best val acc: 0.8884, best train loss: 0.2515  @ epoch 22161 )\n",
      "[Epoch: 22320] train loss: 0.2821, train acc: 0.8819, val loss: 0.2874, val acc: 0.8830  (best train acc: 0.8985, best val acc: 0.8897, best train loss: 0.2515  @ epoch 22161 )\n",
      "[Epoch: 22340] train loss: 0.2600, train acc: 0.8929, val loss: 0.2996, val acc: 0.8708  (best train acc: 0.8985, best val acc: 0.8897, best train loss: 0.2515  @ epoch 22161 )\n",
      "[Epoch: 22360] train loss: 0.2974, train acc: 0.8800, val loss: 0.2719, val acc: 0.8863  (best train acc: 0.8985, best val acc: 0.8897, best train loss: 0.2515  @ epoch 22161 )\n",
      "[Epoch: 22380] train loss: 0.2652, train acc: 0.8939, val loss: 0.2761, val acc: 0.8803  (best train acc: 0.8985, best val acc: 0.8897, best train loss: 0.2515  @ epoch 22161 )\n",
      "[Epoch: 22400] train loss: 0.2764, train acc: 0.8850, val loss: 0.2785, val acc: 0.8789  (best train acc: 0.8985, best val acc: 0.8897, best train loss: 0.2515  @ epoch 22161 )\n",
      "[Epoch: 22420] train loss: 0.2699, train acc: 0.8914, val loss: 0.3509, val acc: 0.8462  (best train acc: 0.8985, best val acc: 0.8897, best train loss: 0.2515  @ epoch 22161 )\n",
      "[Epoch: 22440] train loss: 0.2997, train acc: 0.8751, val loss: 0.3459, val acc: 0.8492  (best train acc: 0.8985, best val acc: 0.8897, best train loss: 0.2515  @ epoch 22161 )\n",
      "[Epoch: 22460] train loss: 0.2832, train acc: 0.8817, val loss: 0.2767, val acc: 0.8823  (best train acc: 0.8985, best val acc: 0.8897, best train loss: 0.2515  @ epoch 22161 )\n",
      "[Epoch: 22480] train loss: 0.2908, train acc: 0.8824, val loss: 0.2736, val acc: 0.8830  (best train acc: 0.8985, best val acc: 0.8897, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 22500] train loss: 0.2910, train acc: 0.8811, val loss: 0.3151, val acc: 0.8668  (best train acc: 0.8985, best val acc: 0.8897, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 22520] train loss: 0.2803, train acc: 0.8884, val loss: 0.2785, val acc: 0.8799  (best train acc: 0.8985, best val acc: 0.8897, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 22540] train loss: 0.3002, train acc: 0.8797, val loss: 0.3082, val acc: 0.8702  (best train acc: 0.8985, best val acc: 0.8897, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 22560] train loss: 0.3121, train acc: 0.8629, val loss: 0.2719, val acc: 0.8850  (best train acc: 0.8985, best val acc: 0.8897, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 22580] train loss: 0.2854, train acc: 0.8832, val loss: 0.2798, val acc: 0.8823  (best train acc: 0.8985, best val acc: 0.8897, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 22600] train loss: 0.2844, train acc: 0.8864, val loss: 0.3083, val acc: 0.8648  (best train acc: 0.8985, best val acc: 0.8897, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 22620] train loss: 0.2910, train acc: 0.8814, val loss: 0.2943, val acc: 0.8779  (best train acc: 0.8985, best val acc: 0.8897, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 22640] train loss: 0.2896, train acc: 0.8786, val loss: 0.3416, val acc: 0.8479  (best train acc: 0.8985, best val acc: 0.8897, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 22660] train loss: 0.2838, train acc: 0.8819, val loss: 0.3182, val acc: 0.8590  (best train acc: 0.8985, best val acc: 0.8897, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 22680] train loss: 0.2794, train acc: 0.8856, val loss: 0.2882, val acc: 0.8823  (best train acc: 0.8985, best val acc: 0.8897, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 22700] train loss: 0.2871, train acc: 0.8792, val loss: 0.3063, val acc: 0.8698  (best train acc: 0.8985, best val acc: 0.8897, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 22720] train loss: 0.2687, train acc: 0.8887, val loss: 0.2774, val acc: 0.8823  (best train acc: 0.8985, best val acc: 0.8897, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 22740] train loss: 0.2636, train acc: 0.8928, val loss: 0.2881, val acc: 0.8786  (best train acc: 0.8985, best val acc: 0.8897, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 22760] train loss: 0.2737, train acc: 0.8882, val loss: 0.2768, val acc: 0.8840  (best train acc: 0.8985, best val acc: 0.8897, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 22780] train loss: 0.2889, train acc: 0.8763, val loss: 0.2898, val acc: 0.8745  (best train acc: 0.8985, best val acc: 0.8897, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 22800] train loss: 0.2811, train acc: 0.8817, val loss: 0.2875, val acc: 0.8782  (best train acc: 0.8985, best val acc: 0.8897, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 22820] train loss: 0.2885, train acc: 0.8739, val loss: 0.2869, val acc: 0.8772  (best train acc: 0.8985, best val acc: 0.8897, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 22840] train loss: 0.3316, train acc: 0.8722, val loss: 0.2782, val acc: 0.8850  (best train acc: 0.8985, best val acc: 0.8897, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 22860] train loss: 0.2641, train acc: 0.8867, val loss: 0.2668, val acc: 0.8847  (best train acc: 0.8985, best val acc: 0.8897, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 22880] train loss: 0.2864, train acc: 0.8784, val loss: 0.2834, val acc: 0.8755  (best train acc: 0.8985, best val acc: 0.8897, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 22900] train loss: 0.2612, train acc: 0.8921, val loss: 0.2771, val acc: 0.8782  (best train acc: 0.8985, best val acc: 0.8897, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 22920] train loss: 0.2729, train acc: 0.8871, val loss: 0.2824, val acc: 0.8806  (best train acc: 0.8985, best val acc: 0.8897, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 22940] train loss: 0.3149, train acc: 0.8723, val loss: 0.3110, val acc: 0.8641  (best train acc: 0.8985, best val acc: 0.8897, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 22960] train loss: 0.2690, train acc: 0.8888, val loss: 0.2809, val acc: 0.8820  (best train acc: 0.8985, best val acc: 0.8897, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 22980] train loss: 0.2935, train acc: 0.8748, val loss: 0.2880, val acc: 0.8752  (best train acc: 0.8985, best val acc: 0.8897, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 23000] train loss: 0.2680, train acc: 0.8883, val loss: 0.2797, val acc: 0.8836  (best train acc: 0.8985, best val acc: 0.8897, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 23020] train loss: 0.2591, train acc: 0.8951, val loss: 0.2791, val acc: 0.8803  (best train acc: 0.8985, best val acc: 0.8897, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 23040] train loss: 0.2821, train acc: 0.8826, val loss: 0.2858, val acc: 0.8833  (best train acc: 0.8985, best val acc: 0.8897, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 23060] train loss: 0.2767, train acc: 0.8869, val loss: 0.2851, val acc: 0.8762  (best train acc: 0.8985, best val acc: 0.8897, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 23080] train loss: 0.2895, train acc: 0.8796, val loss: 0.2888, val acc: 0.8776  (best train acc: 0.8985, best val acc: 0.8897, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 23100] train loss: 0.2862, train acc: 0.8822, val loss: 0.2876, val acc: 0.8840  (best train acc: 0.8985, best val acc: 0.8897, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 23120] train loss: 0.2815, train acc: 0.8837, val loss: 0.2797, val acc: 0.8806  (best train acc: 0.8985, best val acc: 0.8897, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 23140] train loss: 0.2828, train acc: 0.8856, val loss: 0.2717, val acc: 0.8880  (best train acc: 0.8985, best val acc: 0.8897, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 23160] train loss: 0.2775, train acc: 0.8860, val loss: 0.2804, val acc: 0.8820  (best train acc: 0.8985, best val acc: 0.8897, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 23180] train loss: 0.2667, train acc: 0.8885, val loss: 0.2811, val acc: 0.8826  (best train acc: 0.8985, best val acc: 0.8897, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 23200] train loss: 0.3075, train acc: 0.8767, val loss: 0.2888, val acc: 0.8772  (best train acc: 0.8985, best val acc: 0.8897, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 23220] train loss: 0.2769, train acc: 0.8848, val loss: 0.2758, val acc: 0.8833  (best train acc: 0.8985, best val acc: 0.8897, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 23240] train loss: 0.3116, train acc: 0.8660, val loss: 0.3195, val acc: 0.8617  (best train acc: 0.8985, best val acc: 0.8897, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 23260] train loss: 0.2584, train acc: 0.8897, val loss: 0.2758, val acc: 0.8799  (best train acc: 0.8985, best val acc: 0.8897, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 23280] train loss: 0.2963, train acc: 0.8819, val loss: 0.2862, val acc: 0.8809  (best train acc: 0.8985, best val acc: 0.8897, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 23300] train loss: 0.3120, train acc: 0.8770, val loss: 0.2941, val acc: 0.8759  (best train acc: 0.8985, best val acc: 0.8897, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 23320] train loss: 0.3161, train acc: 0.8705, val loss: 0.2867, val acc: 0.8772  (best train acc: 0.8985, best val acc: 0.8897, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 23340] train loss: 0.2720, train acc: 0.8853, val loss: 0.2840, val acc: 0.8816  (best train acc: 0.8985, best val acc: 0.8897, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 23360] train loss: 0.2786, train acc: 0.8859, val loss: 0.2841, val acc: 0.8745  (best train acc: 0.8985, best val acc: 0.8897, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 23380] train loss: 0.2833, train acc: 0.8814, val loss: 0.2913, val acc: 0.8735  (best train acc: 0.8985, best val acc: 0.8897, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 23400] train loss: 0.3059, train acc: 0.8729, val loss: 0.2782, val acc: 0.8823  (best train acc: 0.8985, best val acc: 0.8897, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 23420] train loss: 0.2706, train acc: 0.8899, val loss: 0.2787, val acc: 0.8870  (best train acc: 0.8985, best val acc: 0.8897, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 23440] train loss: 0.3014, train acc: 0.8803, val loss: 0.3000, val acc: 0.8718  (best train acc: 0.8985, best val acc: 0.8897, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 23460] train loss: 0.2749, train acc: 0.8852, val loss: 0.2772, val acc: 0.8823  (best train acc: 0.8985, best val acc: 0.8897, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 23480] train loss: 0.2714, train acc: 0.8870, val loss: 0.2913, val acc: 0.8806  (best train acc: 0.8985, best val acc: 0.8897, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 23500] train loss: 0.2905, train acc: 0.8793, val loss: 0.2755, val acc: 0.8806  (best train acc: 0.8985, best val acc: 0.8897, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 23520] train loss: 0.2870, train acc: 0.8890, val loss: 0.3049, val acc: 0.8688  (best train acc: 0.8985, best val acc: 0.8901, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 23540] train loss: 0.2685, train acc: 0.8929, val loss: 0.2788, val acc: 0.8836  (best train acc: 0.8985, best val acc: 0.8901, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 23560] train loss: 0.2832, train acc: 0.8825, val loss: 0.2772, val acc: 0.8820  (best train acc: 0.8985, best val acc: 0.8901, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 23580] train loss: 0.2625, train acc: 0.8879, val loss: 0.2883, val acc: 0.8752  (best train acc: 0.8985, best val acc: 0.8904, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 23600] train loss: 0.2532, train acc: 0.8975, val loss: 0.2753, val acc: 0.8796  (best train acc: 0.8985, best val acc: 0.8904, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 23620] train loss: 0.3073, train acc: 0.8756, val loss: 0.2783, val acc: 0.8813  (best train acc: 0.8985, best val acc: 0.8904, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 23640] train loss: 0.2903, train acc: 0.8801, val loss: 0.2791, val acc: 0.8806  (best train acc: 0.8985, best val acc: 0.8904, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 23660] train loss: 0.2708, train acc: 0.8860, val loss: 0.2779, val acc: 0.8772  (best train acc: 0.8985, best val acc: 0.8904, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 23680] train loss: 0.2807, train acc: 0.8833, val loss: 0.2748, val acc: 0.8884  (best train acc: 0.8985, best val acc: 0.8904, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 23700] train loss: 0.2934, train acc: 0.8824, val loss: 0.2883, val acc: 0.8809  (best train acc: 0.8985, best val acc: 0.8904, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 23720] train loss: 0.3011, train acc: 0.8793, val loss: 0.2980, val acc: 0.8739  (best train acc: 0.8985, best val acc: 0.8904, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 23740] train loss: 0.2722, train acc: 0.8929, val loss: 0.2663, val acc: 0.8840  (best train acc: 0.8985, best val acc: 0.8904, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 23760] train loss: 0.2875, train acc: 0.8832, val loss: 0.2738, val acc: 0.8813  (best train acc: 0.8985, best val acc: 0.8904, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 23780] train loss: 0.2664, train acc: 0.8895, val loss: 0.2709, val acc: 0.8884  (best train acc: 0.8985, best val acc: 0.8904, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 23800] train loss: 0.3028, train acc: 0.8686, val loss: 0.3344, val acc: 0.8580  (best train acc: 0.8985, best val acc: 0.8904, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 23820] train loss: 0.2752, train acc: 0.8873, val loss: 0.2795, val acc: 0.8847  (best train acc: 0.8985, best val acc: 0.8904, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 23840] train loss: 0.2787, train acc: 0.8842, val loss: 0.2776, val acc: 0.8799  (best train acc: 0.8985, best val acc: 0.8911, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 23860] train loss: 0.3098, train acc: 0.8745, val loss: 0.2888, val acc: 0.8799  (best train acc: 0.8985, best val acc: 0.8911, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 23880] train loss: 0.2949, train acc: 0.8814, val loss: 0.2743, val acc: 0.8836  (best train acc: 0.8985, best val acc: 0.8911, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 23900] train loss: 0.2685, train acc: 0.8887, val loss: 0.2741, val acc: 0.8843  (best train acc: 0.8985, best val acc: 0.8911, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 23920] train loss: 0.2685, train acc: 0.8951, val loss: 0.3062, val acc: 0.8658  (best train acc: 0.8985, best val acc: 0.8911, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 23940] train loss: 0.2861, train acc: 0.8800, val loss: 0.2787, val acc: 0.8793  (best train acc: 0.8985, best val acc: 0.8911, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 23960] train loss: 0.2730, train acc: 0.8879, val loss: 0.2710, val acc: 0.8863  (best train acc: 0.8985, best val acc: 0.8911, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 23980] train loss: 0.3104, train acc: 0.8721, val loss: 0.2887, val acc: 0.8745  (best train acc: 0.8985, best val acc: 0.8911, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 24000] train loss: 0.2691, train acc: 0.8869, val loss: 0.2747, val acc: 0.8853  (best train acc: 0.8985, best val acc: 0.8911, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 24020] train loss: 0.2927, train acc: 0.8811, val loss: 0.2960, val acc: 0.8749  (best train acc: 0.8985, best val acc: 0.8911, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 24040] train loss: 0.2817, train acc: 0.8872, val loss: 0.2743, val acc: 0.8840  (best train acc: 0.8985, best val acc: 0.8911, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 24060] train loss: 0.2648, train acc: 0.8859, val loss: 0.2819, val acc: 0.8809  (best train acc: 0.8985, best val acc: 0.8911, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 24080] train loss: 0.2956, train acc: 0.8798, val loss: 0.2710, val acc: 0.8870  (best train acc: 0.8985, best val acc: 0.8911, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 24100] train loss: 0.2689, train acc: 0.8887, val loss: 0.2699, val acc: 0.8840  (best train acc: 0.8985, best val acc: 0.8911, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 24120] train loss: 0.2895, train acc: 0.8827, val loss: 0.2792, val acc: 0.8816  (best train acc: 0.8985, best val acc: 0.8911, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 24140] train loss: 0.2787, train acc: 0.8827, val loss: 0.2818, val acc: 0.8806  (best train acc: 0.8985, best val acc: 0.8911, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 24160] train loss: 0.2896, train acc: 0.8835, val loss: 0.2800, val acc: 0.8789  (best train acc: 0.8985, best val acc: 0.8911, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 24180] train loss: 0.2639, train acc: 0.8890, val loss: 0.2944, val acc: 0.8718  (best train acc: 0.8985, best val acc: 0.8911, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 24200] train loss: 0.2690, train acc: 0.8887, val loss: 0.2859, val acc: 0.8759  (best train acc: 0.8985, best val acc: 0.8911, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 24220] train loss: 0.2652, train acc: 0.8890, val loss: 0.2700, val acc: 0.8867  (best train acc: 0.8985, best val acc: 0.8911, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 24240] train loss: 0.2741, train acc: 0.8854, val loss: 0.2889, val acc: 0.8769  (best train acc: 0.8985, best val acc: 0.8911, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 24260] train loss: 0.2774, train acc: 0.8833, val loss: 0.3069, val acc: 0.8654  (best train acc: 0.8985, best val acc: 0.8911, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 24280] train loss: 0.3330, train acc: 0.8644, val loss: 0.3066, val acc: 0.8671  (best train acc: 0.8985, best val acc: 0.8911, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 24300] train loss: 0.2796, train acc: 0.8870, val loss: 0.2821, val acc: 0.8772  (best train acc: 0.8985, best val acc: 0.8911, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 24320] train loss: 0.2693, train acc: 0.8896, val loss: 0.2744, val acc: 0.8803  (best train acc: 0.8985, best val acc: 0.8911, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 24340] train loss: 0.2737, train acc: 0.8842, val loss: 0.2778, val acc: 0.8853  (best train acc: 0.8985, best val acc: 0.8911, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 24360] train loss: 0.3048, train acc: 0.8748, val loss: 0.2728, val acc: 0.8836  (best train acc: 0.8993, best val acc: 0.8911, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 24380] train loss: 0.2794, train acc: 0.8843, val loss: 0.2708, val acc: 0.8863  (best train acc: 0.8993, best val acc: 0.8911, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 24400] train loss: 0.2791, train acc: 0.8904, val loss: 0.2935, val acc: 0.8786  (best train acc: 0.8993, best val acc: 0.8911, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 24420] train loss: 0.3092, train acc: 0.8759, val loss: 0.2787, val acc: 0.8874  (best train acc: 0.8993, best val acc: 0.8911, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 24440] train loss: 0.2636, train acc: 0.8918, val loss: 0.2782, val acc: 0.8830  (best train acc: 0.8993, best val acc: 0.8911, best train loss: 0.2495  @ epoch 24423 )\n",
      "[Epoch: 24460] train loss: 0.2674, train acc: 0.8904, val loss: 0.2927, val acc: 0.8799  (best train acc: 0.8993, best val acc: 0.8911, best train loss: 0.2495  @ epoch 24423 )\n",
      "[Epoch: 24480] train loss: 0.2731, train acc: 0.8871, val loss: 0.2692, val acc: 0.8816  (best train acc: 0.8993, best val acc: 0.8911, best train loss: 0.2495  @ epoch 24423 )\n",
      "[Epoch: 24500] train loss: 0.2836, train acc: 0.8830, val loss: 0.2843, val acc: 0.8847  (best train acc: 0.8993, best val acc: 0.8911, best train loss: 0.2495  @ epoch 24423 )\n",
      "[Epoch: 24520] train loss: 0.3103, train acc: 0.8639, val loss: 0.3008, val acc: 0.8698  (best train acc: 0.8993, best val acc: 0.8911, best train loss: 0.2495  @ epoch 24423 )\n",
      "[Epoch: 24540] train loss: 0.2652, train acc: 0.8895, val loss: 0.2685, val acc: 0.8833  (best train acc: 0.9020, best val acc: 0.8911, best train loss: 0.2495  @ epoch 24423 )\n",
      "[Epoch: 24560] train loss: 0.2570, train acc: 0.8934, val loss: 0.2802, val acc: 0.8857  (best train acc: 0.9020, best val acc: 0.8911, best train loss: 0.2495  @ epoch 24423 )\n",
      "[Epoch: 24580] train loss: 0.2750, train acc: 0.8921, val loss: 0.2749, val acc: 0.8847  (best train acc: 0.9020, best val acc: 0.8911, best train loss: 0.2494  @ epoch 24576 )\n",
      "[Epoch: 24600] train loss: 0.2764, train acc: 0.8891, val loss: 0.2851, val acc: 0.8809  (best train acc: 0.9020, best val acc: 0.8911, best train loss: 0.2494  @ epoch 24576 )\n",
      "[Epoch: 24620] train loss: 0.2696, train acc: 0.8932, val loss: 0.2783, val acc: 0.8863  (best train acc: 0.9020, best val acc: 0.8911, best train loss: 0.2494  @ epoch 24576 )\n",
      "[Epoch: 24640] train loss: 0.2885, train acc: 0.8855, val loss: 0.2842, val acc: 0.8752  (best train acc: 0.9020, best val acc: 0.8911, best train loss: 0.2494  @ epoch 24576 )\n",
      "[Epoch: 24660] train loss: 0.2804, train acc: 0.8858, val loss: 0.2845, val acc: 0.8779  (best train acc: 0.9020, best val acc: 0.8911, best train loss: 0.2494  @ epoch 24576 )\n",
      "[Epoch: 24680] train loss: 0.3123, train acc: 0.8751, val loss: 0.2786, val acc: 0.8884  (best train acc: 0.9020, best val acc: 0.8911, best train loss: 0.2494  @ epoch 24576 )\n",
      "[Epoch: 24700] train loss: 0.2578, train acc: 0.8914, val loss: 0.2880, val acc: 0.8735  (best train acc: 0.9020, best val acc: 0.8911, best train loss: 0.2494  @ epoch 24576 )\n",
      "[Epoch: 24720] train loss: 0.3043, train acc: 0.8829, val loss: 0.2815, val acc: 0.8890  (best train acc: 0.9020, best val acc: 0.8911, best train loss: 0.2494  @ epoch 24576 )\n",
      "[Epoch: 24740] train loss: 0.2724, train acc: 0.8869, val loss: 0.2675, val acc: 0.8870  (best train acc: 0.9020, best val acc: 0.8911, best train loss: 0.2494  @ epoch 24576 )\n",
      "[Epoch: 24760] train loss: 0.2699, train acc: 0.8886, val loss: 0.2811, val acc: 0.8823  (best train acc: 0.9020, best val acc: 0.8911, best train loss: 0.2494  @ epoch 24576 )\n",
      "[Epoch: 24780] train loss: 0.2662, train acc: 0.8898, val loss: 0.2751, val acc: 0.8830  (best train acc: 0.9020, best val acc: 0.8911, best train loss: 0.2492  @ epoch 24762 )\n",
      "[Epoch: 24800] train loss: 0.2696, train acc: 0.8902, val loss: 0.2739, val acc: 0.8796  (best train acc: 0.9020, best val acc: 0.8911, best train loss: 0.2492  @ epoch 24762 )\n",
      "[Epoch: 24820] train loss: 0.2636, train acc: 0.8954, val loss: 0.2929, val acc: 0.8769  (best train acc: 0.9020, best val acc: 0.8911, best train loss: 0.2492  @ epoch 24762 )\n",
      "[Epoch: 24840] train loss: 0.2677, train acc: 0.8875, val loss: 0.2820, val acc: 0.8779  (best train acc: 0.9020, best val acc: 0.8911, best train loss: 0.2492  @ epoch 24762 )\n",
      "[Epoch: 24860] train loss: 0.2773, train acc: 0.8860, val loss: 0.2864, val acc: 0.8786  (best train acc: 0.9020, best val acc: 0.8911, best train loss: 0.2492  @ epoch 24762 )\n",
      "[Epoch: 24880] train loss: 0.2671, train acc: 0.8862, val loss: 0.2705, val acc: 0.8894  (best train acc: 0.9020, best val acc: 0.8911, best train loss: 0.2492  @ epoch 24762 )\n",
      "[Epoch: 24900] train loss: 0.3057, train acc: 0.8703, val loss: 0.2905, val acc: 0.8749  (best train acc: 0.9020, best val acc: 0.8911, best train loss: 0.2492  @ epoch 24762 )\n",
      "[Epoch: 24920] train loss: 0.3526, train acc: 0.8525, val loss: 0.3702, val acc: 0.8381  (best train acc: 0.9020, best val acc: 0.8911, best train loss: 0.2492  @ epoch 24762 )\n",
      "[Epoch: 24940] train loss: 0.2836, train acc: 0.8821, val loss: 0.2802, val acc: 0.8830  (best train acc: 0.9020, best val acc: 0.8911, best train loss: 0.2492  @ epoch 24762 )\n",
      "[Epoch: 24960] train loss: 0.2856, train acc: 0.8858, val loss: 0.2777, val acc: 0.8806  (best train acc: 0.9020, best val acc: 0.8911, best train loss: 0.2492  @ epoch 24762 )\n",
      "[Epoch: 24980] train loss: 0.2810, train acc: 0.8873, val loss: 0.2906, val acc: 0.8769  (best train acc: 0.9020, best val acc: 0.8911, best train loss: 0.2492  @ epoch 24762 )\n",
      "[Epoch: 25000] train loss: 0.2878, train acc: 0.8858, val loss: 0.2864, val acc: 0.8816  (best train acc: 0.9020, best val acc: 0.8911, best train loss: 0.2492  @ epoch 24762 )\n",
      "[Epoch: 25020] train loss: 0.2606, train acc: 0.8898, val loss: 0.2697, val acc: 0.8874  (best train acc: 0.9020, best val acc: 0.8911, best train loss: 0.2492  @ epoch 24762 )\n",
      "[Epoch: 25040] train loss: 0.2685, train acc: 0.8920, val loss: 0.3057, val acc: 0.8610  (best train acc: 0.9020, best val acc: 0.8911, best train loss: 0.2492  @ epoch 24762 )\n",
      "[Epoch: 25060] train loss: 0.2850, train acc: 0.8882, val loss: 0.2862, val acc: 0.8789  (best train acc: 0.9020, best val acc: 0.8911, best train loss: 0.2492  @ epoch 24762 )\n",
      "[Epoch: 25080] train loss: 0.2811, train acc: 0.8808, val loss: 0.2859, val acc: 0.8816  (best train acc: 0.9020, best val acc: 0.8911, best train loss: 0.2492  @ epoch 24762 )\n",
      "[Epoch: 25100] train loss: 0.2781, train acc: 0.8815, val loss: 0.2754, val acc: 0.8863  (best train acc: 0.9020, best val acc: 0.8911, best train loss: 0.2492  @ epoch 24762 )\n",
      "[Epoch: 25120] train loss: 0.2724, train acc: 0.8872, val loss: 0.2753, val acc: 0.8782  (best train acc: 0.9020, best val acc: 0.8911, best train loss: 0.2492  @ epoch 24762 )\n",
      "[Epoch: 25140] train loss: 0.2840, train acc: 0.8870, val loss: 0.2917, val acc: 0.8712  (best train acc: 0.9020, best val acc: 0.8911, best train loss: 0.2492  @ epoch 24762 )\n",
      "[Epoch: 25160] train loss: 0.2539, train acc: 0.8937, val loss: 0.2719, val acc: 0.8823  (best train acc: 0.9020, best val acc: 0.8911, best train loss: 0.2492  @ epoch 24762 )\n",
      "[Epoch: 25180] train loss: 0.2650, train acc: 0.8958, val loss: 0.2900, val acc: 0.8803  (best train acc: 0.9020, best val acc: 0.8911, best train loss: 0.2492  @ epoch 24762 )\n",
      "[Epoch: 25200] train loss: 0.2844, train acc: 0.8827, val loss: 0.2983, val acc: 0.8762  (best train acc: 0.9020, best val acc: 0.8911, best train loss: 0.2492  @ epoch 24762 )\n",
      "[Epoch: 25220] train loss: 0.3050, train acc: 0.8738, val loss: 0.2823, val acc: 0.8809  (best train acc: 0.9020, best val acc: 0.8911, best train loss: 0.2492  @ epoch 24762 )\n",
      "[Epoch: 25240] train loss: 0.2726, train acc: 0.8882, val loss: 0.3113, val acc: 0.8651  (best train acc: 0.9020, best val acc: 0.8911, best train loss: 0.2492  @ epoch 24762 )\n",
      "[Epoch: 25260] train loss: 0.3859, train acc: 0.8344, val loss: 0.2873, val acc: 0.8813  (best train acc: 0.9020, best val acc: 0.8911, best train loss: 0.2492  @ epoch 24762 )\n",
      "[Epoch: 25280] train loss: 0.2763, train acc: 0.8863, val loss: 0.2713, val acc: 0.8880  (best train acc: 0.9020, best val acc: 0.8927, best train loss: 0.2492  @ epoch 24762 )\n",
      "[Epoch: 25300] train loss: 0.2558, train acc: 0.8897, val loss: 0.2712, val acc: 0.8853  (best train acc: 0.9020, best val acc: 0.8927, best train loss: 0.2492  @ epoch 24762 )\n",
      "[Epoch: 25320] train loss: 0.2841, train acc: 0.8809, val loss: 0.2786, val acc: 0.8809  (best train acc: 0.9020, best val acc: 0.8927, best train loss: 0.2492  @ epoch 24762 )\n",
      "[Epoch: 25340] train loss: 0.2584, train acc: 0.8910, val loss: 0.2693, val acc: 0.8863  (best train acc: 0.9020, best val acc: 0.8927, best train loss: 0.2492  @ epoch 24762 )\n",
      "[Epoch: 25360] train loss: 0.2718, train acc: 0.8939, val loss: 0.2988, val acc: 0.8772  (best train acc: 0.9020, best val acc: 0.8927, best train loss: 0.2470  @ epoch 25353 )\n",
      "[Epoch: 25380] train loss: 0.2535, train acc: 0.8931, val loss: 0.2743, val acc: 0.8860  (best train acc: 0.9020, best val acc: 0.8927, best train loss: 0.2470  @ epoch 25353 )\n",
      "[Epoch: 25400] train loss: 0.2698, train acc: 0.8911, val loss: 0.2759, val acc: 0.8857  (best train acc: 0.9020, best val acc: 0.8927, best train loss: 0.2470  @ epoch 25353 )\n",
      "[Epoch: 25420] train loss: 0.2641, train acc: 0.8880, val loss: 0.2685, val acc: 0.8863  (best train acc: 0.9020, best val acc: 0.8927, best train loss: 0.2424  @ epoch 25408 )\n",
      "[Epoch: 25440] train loss: 0.2630, train acc: 0.8953, val loss: 0.2794, val acc: 0.8853  (best train acc: 0.9020, best val acc: 0.8927, best train loss: 0.2424  @ epoch 25408 )\n",
      "[Epoch: 25460] train loss: 0.2630, train acc: 0.8910, val loss: 0.2684, val acc: 0.8867  (best train acc: 0.9020, best val acc: 0.8927, best train loss: 0.2424  @ epoch 25408 )\n",
      "[Epoch: 25480] train loss: 0.2735, train acc: 0.8856, val loss: 0.3164, val acc: 0.8678  (best train acc: 0.9020, best val acc: 0.8927, best train loss: 0.2424  @ epoch 25408 )\n",
      "[Epoch: 25500] train loss: 0.2668, train acc: 0.8892, val loss: 0.2718, val acc: 0.8833  (best train acc: 0.9020, best val acc: 0.8927, best train loss: 0.2424  @ epoch 25408 )\n",
      "[Epoch: 25520] train loss: 0.2569, train acc: 0.8920, val loss: 0.2894, val acc: 0.8769  (best train acc: 0.9020, best val acc: 0.8927, best train loss: 0.2424  @ epoch 25408 )\n",
      "[Epoch: 25540] train loss: 0.2613, train acc: 0.8908, val loss: 0.2683, val acc: 0.8907  (best train acc: 0.9020, best val acc: 0.8927, best train loss: 0.2424  @ epoch 25408 )\n",
      "[Epoch: 25560] train loss: 0.2836, train acc: 0.8824, val loss: 0.2645, val acc: 0.8870  (best train acc: 0.9020, best val acc: 0.8927, best train loss: 0.2424  @ epoch 25408 )\n",
      "[Epoch: 25580] train loss: 0.2850, train acc: 0.8793, val loss: 0.2924, val acc: 0.8793  (best train acc: 0.9020, best val acc: 0.8927, best train loss: 0.2424  @ epoch 25408 )\n",
      "[Epoch: 25600] train loss: 0.3446, train acc: 0.8603, val loss: 0.3404, val acc: 0.8462  (best train acc: 0.9020, best val acc: 0.8927, best train loss: 0.2424  @ epoch 25408 )\n",
      "[Epoch: 25620] train loss: 0.2566, train acc: 0.8936, val loss: 0.2721, val acc: 0.8867  (best train acc: 0.9020, best val acc: 0.8927, best train loss: 0.2424  @ epoch 25408 )\n",
      "[Epoch: 25640] train loss: 0.2587, train acc: 0.8906, val loss: 0.2913, val acc: 0.8728  (best train acc: 0.9020, best val acc: 0.8927, best train loss: 0.2424  @ epoch 25408 )\n",
      "[Epoch: 25660] train loss: 0.2636, train acc: 0.8861, val loss: 0.2722, val acc: 0.8820  (best train acc: 0.9020, best val acc: 0.8927, best train loss: 0.2424  @ epoch 25408 )\n",
      "[Epoch: 25680] train loss: 0.3205, train acc: 0.8671, val loss: 0.3067, val acc: 0.8735  (best train acc: 0.9020, best val acc: 0.8927, best train loss: 0.2424  @ epoch 25408 )\n",
      "[Epoch: 25700] train loss: 0.2975, train acc: 0.8852, val loss: 0.2865, val acc: 0.8833  (best train acc: 0.9020, best val acc: 0.8927, best train loss: 0.2424  @ epoch 25408 )\n",
      "[Epoch: 25720] train loss: 0.2640, train acc: 0.8941, val loss: 0.2712, val acc: 0.8863  (best train acc: 0.9020, best val acc: 0.8927, best train loss: 0.2424  @ epoch 25408 )\n",
      "[Epoch: 25740] train loss: 0.2917, train acc: 0.8813, val loss: 0.2716, val acc: 0.8843  (best train acc: 0.9057, best val acc: 0.8927, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 25760] train loss: 0.2758, train acc: 0.8907, val loss: 0.2752, val acc: 0.8840  (best train acc: 0.9057, best val acc: 0.8927, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 25780] train loss: 0.2651, train acc: 0.8897, val loss: 0.2827, val acc: 0.8833  (best train acc: 0.9057, best val acc: 0.8927, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 25800] train loss: 0.2678, train acc: 0.8898, val loss: 0.2863, val acc: 0.8843  (best train acc: 0.9057, best val acc: 0.8927, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 25820] train loss: 0.2792, train acc: 0.8815, val loss: 0.2757, val acc: 0.8813  (best train acc: 0.9057, best val acc: 0.8927, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 25840] train loss: 0.2789, train acc: 0.8916, val loss: 0.2810, val acc: 0.8860  (best train acc: 0.9057, best val acc: 0.8927, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 25860] train loss: 0.2857, train acc: 0.8800, val loss: 0.2836, val acc: 0.8816  (best train acc: 0.9057, best val acc: 0.8927, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 25880] train loss: 0.2832, train acc: 0.8835, val loss: 0.2808, val acc: 0.8823  (best train acc: 0.9057, best val acc: 0.8927, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 25900] train loss: 0.2439, train acc: 0.9012, val loss: 0.2654, val acc: 0.8877  (best train acc: 0.9057, best val acc: 0.8927, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 25920] train loss: 0.2652, train acc: 0.8998, val loss: 0.2883, val acc: 0.8782  (best train acc: 0.9057, best val acc: 0.8927, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 25940] train loss: 0.2660, train acc: 0.8913, val loss: 0.2917, val acc: 0.8786  (best train acc: 0.9057, best val acc: 0.8927, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 25960] train loss: 0.2956, train acc: 0.8780, val loss: 0.2709, val acc: 0.8863  (best train acc: 0.9057, best val acc: 0.8927, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 25980] train loss: 0.2891, train acc: 0.8783, val loss: 0.3126, val acc: 0.8617  (best train acc: 0.9057, best val acc: 0.8927, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 26000] train loss: 0.2626, train acc: 0.8924, val loss: 0.2732, val acc: 0.8840  (best train acc: 0.9057, best val acc: 0.8927, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 26020] train loss: 0.2552, train acc: 0.8976, val loss: 0.2842, val acc: 0.8789  (best train acc: 0.9057, best val acc: 0.8927, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 26040] train loss: 0.2815, train acc: 0.8822, val loss: 0.3227, val acc: 0.8553  (best train acc: 0.9057, best val acc: 0.8927, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 26060] train loss: 0.2619, train acc: 0.8929, val loss: 0.2876, val acc: 0.8813  (best train acc: 0.9057, best val acc: 0.8927, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 26080] train loss: 0.2687, train acc: 0.8856, val loss: 0.2737, val acc: 0.8813  (best train acc: 0.9057, best val acc: 0.8927, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 26100] train loss: 0.2915, train acc: 0.8858, val loss: 0.2701, val acc: 0.8901  (best train acc: 0.9057, best val acc: 0.8927, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 26120] train loss: 0.2679, train acc: 0.8935, val loss: 0.2771, val acc: 0.8843  (best train acc: 0.9057, best val acc: 0.8927, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 26140] train loss: 0.2949, train acc: 0.8785, val loss: 0.2740, val acc: 0.8823  (best train acc: 0.9057, best val acc: 0.8927, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 26160] train loss: 0.2929, train acc: 0.8823, val loss: 0.3066, val acc: 0.8648  (best train acc: 0.9057, best val acc: 0.8927, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 26180] train loss: 0.2481, train acc: 0.8976, val loss: 0.2756, val acc: 0.8840  (best train acc: 0.9057, best val acc: 0.8927, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 26200] train loss: 0.2608, train acc: 0.8989, val loss: 0.3259, val acc: 0.8604  (best train acc: 0.9057, best val acc: 0.8927, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 26220] train loss: 0.2884, train acc: 0.8833, val loss: 0.2853, val acc: 0.8847  (best train acc: 0.9057, best val acc: 0.8927, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 26240] train loss: 0.2729, train acc: 0.8884, val loss: 0.2737, val acc: 0.8789  (best train acc: 0.9057, best val acc: 0.8927, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 26260] train loss: 0.2694, train acc: 0.8829, val loss: 0.2763, val acc: 0.8803  (best train acc: 0.9057, best val acc: 0.8927, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 26280] train loss: 0.2794, train acc: 0.8863, val loss: 0.2757, val acc: 0.8850  (best train acc: 0.9057, best val acc: 0.8927, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 26300] train loss: 0.2755, train acc: 0.8882, val loss: 0.2721, val acc: 0.8857  (best train acc: 0.9057, best val acc: 0.8927, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 26320] train loss: 0.2743, train acc: 0.8886, val loss: 0.2851, val acc: 0.8779  (best train acc: 0.9057, best val acc: 0.8927, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 26340] train loss: 0.2622, train acc: 0.8908, val loss: 0.2876, val acc: 0.8796  (best train acc: 0.9057, best val acc: 0.8927, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 26360] train loss: 0.2702, train acc: 0.8901, val loss: 0.2757, val acc: 0.8843  (best train acc: 0.9057, best val acc: 0.8927, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 26380] train loss: 0.2837, train acc: 0.8864, val loss: 0.2853, val acc: 0.8826  (best train acc: 0.9057, best val acc: 0.8927, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 26400] train loss: 0.2871, train acc: 0.8888, val loss: 0.2759, val acc: 0.8833  (best train acc: 0.9057, best val acc: 0.8927, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 26420] train loss: 0.2618, train acc: 0.8926, val loss: 0.2697, val acc: 0.8857  (best train acc: 0.9057, best val acc: 0.8927, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 26440] train loss: 0.2583, train acc: 0.8967, val loss: 0.2718, val acc: 0.8867  (best train acc: 0.9057, best val acc: 0.8927, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 26460] train loss: 0.2655, train acc: 0.8916, val loss: 0.2676, val acc: 0.8863  (best train acc: 0.9057, best val acc: 0.8927, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 26480] train loss: 0.2717, train acc: 0.8900, val loss: 0.2800, val acc: 0.8823  (best train acc: 0.9057, best val acc: 0.8927, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 26500] train loss: 0.2572, train acc: 0.8979, val loss: 0.2730, val acc: 0.8847  (best train acc: 0.9057, best val acc: 0.8927, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 26520] train loss: 0.2656, train acc: 0.8872, val loss: 0.2715, val acc: 0.8843  (best train acc: 0.9057, best val acc: 0.8927, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 26540] train loss: 0.2626, train acc: 0.8899, val loss: 0.2715, val acc: 0.8840  (best train acc: 0.9057, best val acc: 0.8927, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 26560] train loss: 0.2939, train acc: 0.8767, val loss: 0.2784, val acc: 0.8850  (best train acc: 0.9057, best val acc: 0.8927, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 26580] train loss: 0.2729, train acc: 0.8826, val loss: 0.2758, val acc: 0.8863  (best train acc: 0.9057, best val acc: 0.8927, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 26600] train loss: 0.2632, train acc: 0.8869, val loss: 0.2878, val acc: 0.8772  (best train acc: 0.9057, best val acc: 0.8927, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 26620] train loss: 0.2489, train acc: 0.8953, val loss: 0.2698, val acc: 0.8857  (best train acc: 0.9057, best val acc: 0.8927, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 26640] train loss: 0.2698, train acc: 0.8892, val loss: 0.2814, val acc: 0.8823  (best train acc: 0.9057, best val acc: 0.8927, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 26660] train loss: 0.2483, train acc: 0.8999, val loss: 0.2817, val acc: 0.8813  (best train acc: 0.9057, best val acc: 0.8931, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 26680] train loss: 0.2499, train acc: 0.8963, val loss: 0.2662, val acc: 0.8884  (best train acc: 0.9057, best val acc: 0.8931, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 26700] train loss: 0.2628, train acc: 0.8889, val loss: 0.2819, val acc: 0.8823  (best train acc: 0.9057, best val acc: 0.8931, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 26720] train loss: 0.2592, train acc: 0.8924, val loss: 0.2828, val acc: 0.8782  (best train acc: 0.9057, best val acc: 0.8931, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 26740] train loss: 0.2691, train acc: 0.8889, val loss: 0.2893, val acc: 0.8752  (best train acc: 0.9057, best val acc: 0.8931, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 26760] train loss: 0.2898, train acc: 0.8789, val loss: 0.2673, val acc: 0.8863  (best train acc: 0.9057, best val acc: 0.8931, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 26780] train loss: 0.2687, train acc: 0.8902, val loss: 0.2908, val acc: 0.8813  (best train acc: 0.9057, best val acc: 0.8931, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 26800] train loss: 0.2703, train acc: 0.8892, val loss: 0.2733, val acc: 0.8836  (best train acc: 0.9057, best val acc: 0.8931, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 26820] train loss: 0.2825, train acc: 0.8878, val loss: 0.2707, val acc: 0.8890  (best train acc: 0.9057, best val acc: 0.8931, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 26840] train loss: 0.2595, train acc: 0.8894, val loss: 0.2970, val acc: 0.8722  (best train acc: 0.9057, best val acc: 0.8931, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 26860] train loss: 0.2696, train acc: 0.8848, val loss: 0.2923, val acc: 0.8766  (best train acc: 0.9057, best val acc: 0.8931, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 26880] train loss: 0.2534, train acc: 0.8945, val loss: 0.2681, val acc: 0.8894  (best train acc: 0.9057, best val acc: 0.8931, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 26900] train loss: 0.2601, train acc: 0.8935, val loss: 0.2735, val acc: 0.8830  (best train acc: 0.9057, best val acc: 0.8931, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 26920] train loss: 0.2678, train acc: 0.8891, val loss: 0.2729, val acc: 0.8857  (best train acc: 0.9057, best val acc: 0.8931, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 26940] train loss: 0.2822, train acc: 0.8777, val loss: 0.2819, val acc: 0.8769  (best train acc: 0.9057, best val acc: 0.8931, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 26960] train loss: 0.3051, train acc: 0.8805, val loss: 0.2742, val acc: 0.8816  (best train acc: 0.9057, best val acc: 0.8931, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 26980] train loss: 0.2589, train acc: 0.8944, val loss: 0.3072, val acc: 0.8641  (best train acc: 0.9057, best val acc: 0.8931, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 27000] train loss: 0.2517, train acc: 0.8970, val loss: 0.2765, val acc: 0.8887  (best train acc: 0.9057, best val acc: 0.8931, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 27020] train loss: 0.2591, train acc: 0.8958, val loss: 0.2988, val acc: 0.8732  (best train acc: 0.9057, best val acc: 0.8931, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 27040] train loss: 0.2736, train acc: 0.8876, val loss: 0.2711, val acc: 0.8880  (best train acc: 0.9057, best val acc: 0.8931, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 27060] train loss: 0.2691, train acc: 0.8905, val loss: 0.2702, val acc: 0.8853  (best train acc: 0.9057, best val acc: 0.8931, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 27080] train loss: 0.2521, train acc: 0.8973, val loss: 0.2709, val acc: 0.8850  (best train acc: 0.9057, best val acc: 0.8931, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 27100] train loss: 0.2836, train acc: 0.8761, val loss: 0.2696, val acc: 0.8847  (best train acc: 0.9057, best val acc: 0.8931, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 27120] train loss: 0.2810, train acc: 0.8877, val loss: 0.2700, val acc: 0.8870  (best train acc: 0.9057, best val acc: 0.8931, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 27140] train loss: 0.2831, train acc: 0.8834, val loss: 0.2696, val acc: 0.8853  (best train acc: 0.9057, best val acc: 0.8931, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 27160] train loss: 0.2688, train acc: 0.8890, val loss: 0.2696, val acc: 0.8894  (best train acc: 0.9057, best val acc: 0.8931, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 27180] train loss: 0.2716, train acc: 0.8876, val loss: 0.2881, val acc: 0.8718  (best train acc: 0.9057, best val acc: 0.8931, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 27200] train loss: 0.2605, train acc: 0.8943, val loss: 0.3262, val acc: 0.8573  (best train acc: 0.9057, best val acc: 0.8931, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 27220] train loss: 0.2641, train acc: 0.8873, val loss: 0.2748, val acc: 0.8823  (best train acc: 0.9057, best val acc: 0.8931, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 27240] train loss: 0.2751, train acc: 0.8850, val loss: 0.2983, val acc: 0.8668  (best train acc: 0.9057, best val acc: 0.8931, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 27260] train loss: 0.2623, train acc: 0.8908, val loss: 0.2759, val acc: 0.8867  (best train acc: 0.9057, best val acc: 0.8931, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 27280] train loss: 0.2902, train acc: 0.8845, val loss: 0.3007, val acc: 0.8678  (best train acc: 0.9057, best val acc: 0.8931, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 27300] train loss: 0.2539, train acc: 0.8947, val loss: 0.2809, val acc: 0.8779  (best train acc: 0.9057, best val acc: 0.8931, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 27320] train loss: 0.3234, train acc: 0.8673, val loss: 0.2834, val acc: 0.8759  (best train acc: 0.9057, best val acc: 0.8931, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 27340] train loss: 0.2597, train acc: 0.8971, val loss: 0.2755, val acc: 0.8843  (best train acc: 0.9057, best val acc: 0.8931, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 27360] train loss: 0.2458, train acc: 0.9003, val loss: 0.2782, val acc: 0.8853  (best train acc: 0.9057, best val acc: 0.8931, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 27380] train loss: 0.2617, train acc: 0.8946, val loss: 0.2718, val acc: 0.8870  (best train acc: 0.9057, best val acc: 0.8931, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 27400] train loss: 0.2528, train acc: 0.8981, val loss: 0.2770, val acc: 0.8857  (best train acc: 0.9057, best val acc: 0.8931, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 27420] train loss: 0.2865, train acc: 0.8812, val loss: 0.2752, val acc: 0.8833  (best train acc: 0.9057, best val acc: 0.8931, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 27440] train loss: 0.2718, train acc: 0.8858, val loss: 0.2660, val acc: 0.8874  (best train acc: 0.9057, best val acc: 0.8931, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 27460] train loss: 0.2569, train acc: 0.8946, val loss: 0.2747, val acc: 0.8806  (best train acc: 0.9057, best val acc: 0.8931, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 27480] train loss: 0.2488, train acc: 0.9014, val loss: 0.2883, val acc: 0.8796  (best train acc: 0.9057, best val acc: 0.8931, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 27500] train loss: 0.2665, train acc: 0.8877, val loss: 0.2716, val acc: 0.8847  (best train acc: 0.9057, best val acc: 0.8931, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 27520] train loss: 0.2767, train acc: 0.8848, val loss: 0.2711, val acc: 0.8860  (best train acc: 0.9057, best val acc: 0.8931, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 27540] train loss: 0.2577, train acc: 0.8902, val loss: 0.2771, val acc: 0.8820  (best train acc: 0.9057, best val acc: 0.8931, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 27560] train loss: 0.2671, train acc: 0.8924, val loss: 0.2691, val acc: 0.8836  (best train acc: 0.9057, best val acc: 0.8931, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 27580] train loss: 0.2485, train acc: 0.8976, val loss: 0.2679, val acc: 0.8907  (best train acc: 0.9057, best val acc: 0.8931, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 27600] train loss: 0.2892, train acc: 0.8790, val loss: 0.3172, val acc: 0.8624  (best train acc: 0.9057, best val acc: 0.8931, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 27620] train loss: 0.2617, train acc: 0.8955, val loss: 0.2642, val acc: 0.8884  (best train acc: 0.9057, best val acc: 0.8931, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 27640] train loss: 0.2632, train acc: 0.8909, val loss: 0.2837, val acc: 0.8823  (best train acc: 0.9057, best val acc: 0.8931, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 27660] train loss: 0.2583, train acc: 0.8965, val loss: 0.2678, val acc: 0.8867  (best train acc: 0.9057, best val acc: 0.8941, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 27680] train loss: 0.2646, train acc: 0.8921, val loss: 0.2784, val acc: 0.8813  (best train acc: 0.9057, best val acc: 0.8941, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 27700] train loss: 0.2635, train acc: 0.8910, val loss: 0.2742, val acc: 0.8843  (best train acc: 0.9057, best val acc: 0.8941, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 27720] train loss: 0.2656, train acc: 0.8904, val loss: 0.2845, val acc: 0.8840  (best train acc: 0.9057, best val acc: 0.8941, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 27740] train loss: 0.2531, train acc: 0.8953, val loss: 0.2698, val acc: 0.8847  (best train acc: 0.9057, best val acc: 0.8941, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 27760] train loss: 0.2836, train acc: 0.8814, val loss: 0.2815, val acc: 0.8809  (best train acc: 0.9057, best val acc: 0.8941, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 27780] train loss: 0.2638, train acc: 0.8937, val loss: 0.2800, val acc: 0.8833  (best train acc: 0.9057, best val acc: 0.8941, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 27800] train loss: 0.2682, train acc: 0.8900, val loss: 0.2714, val acc: 0.8806  (best train acc: 0.9057, best val acc: 0.8941, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 27820] train loss: 0.2518, train acc: 0.8952, val loss: 0.2698, val acc: 0.8867  (best train acc: 0.9057, best val acc: 0.8941, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 27840] train loss: 0.2550, train acc: 0.8937, val loss: 0.2777, val acc: 0.8806  (best train acc: 0.9057, best val acc: 0.8941, best train loss: 0.2368  @ epoch 27830 )\n",
      "[Epoch: 27860] train loss: 0.2589, train acc: 0.8884, val loss: 0.2685, val acc: 0.8847  (best train acc: 0.9057, best val acc: 0.8941, best train loss: 0.2368  @ epoch 27830 )\n",
      "[Epoch: 27880] train loss: 0.2892, train acc: 0.8831, val loss: 0.2637, val acc: 0.8840  (best train acc: 0.9057, best val acc: 0.8941, best train loss: 0.2368  @ epoch 27830 )\n",
      "[Epoch: 27900] train loss: 0.2756, train acc: 0.8877, val loss: 0.2713, val acc: 0.8887  (best train acc: 0.9057, best val acc: 0.8941, best train loss: 0.2368  @ epoch 27830 )\n",
      "[Epoch: 27920] train loss: 0.2851, train acc: 0.8829, val loss: 0.2836, val acc: 0.8877  (best train acc: 0.9057, best val acc: 0.8941, best train loss: 0.2368  @ epoch 27830 )\n",
      "[Epoch: 27940] train loss: 0.2918, train acc: 0.8777, val loss: 0.2911, val acc: 0.8766  (best train acc: 0.9057, best val acc: 0.8941, best train loss: 0.2368  @ epoch 27830 )\n",
      "[Epoch: 27960] train loss: 0.3064, train acc: 0.8728, val loss: 0.2706, val acc: 0.8840  (best train acc: 0.9057, best val acc: 0.8941, best train loss: 0.2368  @ epoch 27830 )\n",
      "[Epoch: 27980] train loss: 0.2570, train acc: 0.8871, val loss: 0.2711, val acc: 0.8816  (best train acc: 0.9057, best val acc: 0.8941, best train loss: 0.2368  @ epoch 27830 )\n",
      "[Epoch: 28000] train loss: 0.2964, train acc: 0.8759, val loss: 0.2775, val acc: 0.8806  (best train acc: 0.9057, best val acc: 0.8941, best train loss: 0.2368  @ epoch 27830 )\n",
      "[Epoch: 28020] train loss: 0.2623, train acc: 0.8943, val loss: 0.2677, val acc: 0.8840  (best train acc: 0.9057, best val acc: 0.8941, best train loss: 0.2368  @ epoch 27830 )\n",
      "[Epoch: 28040] train loss: 0.2671, train acc: 0.8909, val loss: 0.2816, val acc: 0.8793  (best train acc: 0.9057, best val acc: 0.8941, best train loss: 0.2368  @ epoch 27830 )\n",
      "[Epoch: 28060] train loss: 0.2684, train acc: 0.8882, val loss: 0.2735, val acc: 0.8877  (best train acc: 0.9057, best val acc: 0.8941, best train loss: 0.2368  @ epoch 27830 )\n",
      "[Epoch: 28080] train loss: 0.2738, train acc: 0.8902, val loss: 0.2665, val acc: 0.8867  (best train acc: 0.9057, best val acc: 0.8941, best train loss: 0.2368  @ epoch 27830 )\n",
      "[Epoch: 28100] train loss: 0.2872, train acc: 0.8848, val loss: 0.3016, val acc: 0.8749  (best train acc: 0.9057, best val acc: 0.8941, best train loss: 0.2368  @ epoch 27830 )\n",
      "[Epoch: 28120] train loss: 0.2585, train acc: 0.8932, val loss: 0.2928, val acc: 0.8752  (best train acc: 0.9057, best val acc: 0.8941, best train loss: 0.2368  @ epoch 27830 )\n",
      "[Epoch: 28140] train loss: 0.2727, train acc: 0.8878, val loss: 0.2667, val acc: 0.8897  (best train acc: 0.9057, best val acc: 0.8941, best train loss: 0.2368  @ epoch 27830 )\n",
      "[Epoch: 28160] train loss: 0.3033, train acc: 0.8747, val loss: 0.2827, val acc: 0.8779  (best train acc: 0.9057, best val acc: 0.8941, best train loss: 0.2368  @ epoch 27830 )\n",
      "[Epoch: 28180] train loss: 0.3111, train acc: 0.8635, val loss: 0.2695, val acc: 0.8857  (best train acc: 0.9057, best val acc: 0.8941, best train loss: 0.2368  @ epoch 27830 )\n",
      "[Epoch: 28200] train loss: 0.2714, train acc: 0.8901, val loss: 0.2702, val acc: 0.8894  (best train acc: 0.9057, best val acc: 0.8941, best train loss: 0.2368  @ epoch 27830 )\n",
      "[Epoch: 28220] train loss: 0.2722, train acc: 0.8827, val loss: 0.2648, val acc: 0.8833  (best train acc: 0.9057, best val acc: 0.8941, best train loss: 0.2368  @ epoch 27830 )\n",
      "[Epoch: 28240] train loss: 0.2686, train acc: 0.8811, val loss: 0.2781, val acc: 0.8823  (best train acc: 0.9057, best val acc: 0.8941, best train loss: 0.2368  @ epoch 27830 )\n",
      "[Epoch: 28260] train loss: 0.2607, train acc: 0.8927, val loss: 0.2741, val acc: 0.8833  (best train acc: 0.9057, best val acc: 0.8941, best train loss: 0.2368  @ epoch 27830 )\n",
      "[Epoch: 28280] train loss: 0.2703, train acc: 0.8910, val loss: 0.2634, val acc: 0.8863  (best train acc: 0.9057, best val acc: 0.8941, best train loss: 0.2368  @ epoch 27830 )\n",
      "[Epoch: 28300] train loss: 0.2557, train acc: 0.8924, val loss: 0.2635, val acc: 0.8870  (best train acc: 0.9057, best val acc: 0.8941, best train loss: 0.2368  @ epoch 27830 )\n",
      "[Epoch: 28320] train loss: 0.2569, train acc: 0.8942, val loss: 0.2673, val acc: 0.8863  (best train acc: 0.9057, best val acc: 0.8941, best train loss: 0.2368  @ epoch 27830 )\n",
      "[Epoch: 28340] train loss: 0.2798, train acc: 0.8833, val loss: 0.2647, val acc: 0.8890  (best train acc: 0.9057, best val acc: 0.8941, best train loss: 0.2368  @ epoch 27830 )\n",
      "[Epoch: 28360] train loss: 0.2715, train acc: 0.8884, val loss: 0.2653, val acc: 0.8917  (best train acc: 0.9057, best val acc: 0.8941, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 28380] train loss: 0.2520, train acc: 0.8937, val loss: 0.2665, val acc: 0.8863  (best train acc: 0.9057, best val acc: 0.8941, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 28400] train loss: 0.2640, train acc: 0.8949, val loss: 0.2732, val acc: 0.8850  (best train acc: 0.9057, best val acc: 0.8941, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 28420] train loss: 0.2595, train acc: 0.8947, val loss: 0.3363, val acc: 0.8536  (best train acc: 0.9057, best val acc: 0.8941, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 28440] train loss: 0.2766, train acc: 0.8803, val loss: 0.2724, val acc: 0.8850  (best train acc: 0.9057, best val acc: 0.8941, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 28460] train loss: 0.3022, train acc: 0.8754, val loss: 0.2747, val acc: 0.8799  (best train acc: 0.9057, best val acc: 0.8941, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 28480] train loss: 0.2886, train acc: 0.8786, val loss: 0.2858, val acc: 0.8759  (best train acc: 0.9057, best val acc: 0.8941, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 28500] train loss: 0.2552, train acc: 0.8986, val loss: 0.2712, val acc: 0.8877  (best train acc: 0.9057, best val acc: 0.8941, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 28520] train loss: 0.3164, train acc: 0.8673, val loss: 0.3499, val acc: 0.8573  (best train acc: 0.9057, best val acc: 0.8941, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 28540] train loss: 0.2572, train acc: 0.8938, val loss: 0.2706, val acc: 0.8880  (best train acc: 0.9057, best val acc: 0.8941, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 28560] train loss: 0.2784, train acc: 0.8784, val loss: 0.2707, val acc: 0.8847  (best train acc: 0.9057, best val acc: 0.8941, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 28580] train loss: 0.2517, train acc: 0.9015, val loss: 0.3000, val acc: 0.8752  (best train acc: 0.9057, best val acc: 0.8941, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 28600] train loss: 0.2700, train acc: 0.8896, val loss: 0.2721, val acc: 0.8877  (best train acc: 0.9057, best val acc: 0.8941, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 28620] train loss: 0.2725, train acc: 0.8882, val loss: 0.2694, val acc: 0.8850  (best train acc: 0.9057, best val acc: 0.8941, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 28640] train loss: 0.2608, train acc: 0.8922, val loss: 0.2652, val acc: 0.8874  (best train acc: 0.9057, best val acc: 0.8944, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 28660] train loss: 0.2914, train acc: 0.8863, val loss: 0.2931, val acc: 0.8752  (best train acc: 0.9057, best val acc: 0.8944, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 28680] train loss: 0.2873, train acc: 0.8756, val loss: 0.2991, val acc: 0.8732  (best train acc: 0.9057, best val acc: 0.8944, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 28700] train loss: 0.2858, train acc: 0.8767, val loss: 0.2814, val acc: 0.8809  (best train acc: 0.9057, best val acc: 0.8944, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 28720] train loss: 0.2680, train acc: 0.8885, val loss: 0.2708, val acc: 0.8860  (best train acc: 0.9057, best val acc: 0.8944, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 28740] train loss: 0.2850, train acc: 0.8837, val loss: 0.2858, val acc: 0.8779  (best train acc: 0.9057, best val acc: 0.8944, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 28760] train loss: 0.2490, train acc: 0.8970, val loss: 0.2647, val acc: 0.8890  (best train acc: 0.9057, best val acc: 0.8944, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 28780] train loss: 0.2850, train acc: 0.8785, val loss: 0.2818, val acc: 0.8762  (best train acc: 0.9057, best val acc: 0.8944, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 28800] train loss: 0.2521, train acc: 0.8938, val loss: 0.2671, val acc: 0.8843  (best train acc: 0.9057, best val acc: 0.8944, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 28820] train loss: 0.2783, train acc: 0.8888, val loss: 0.2933, val acc: 0.8745  (best train acc: 0.9057, best val acc: 0.8944, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 28840] train loss: 0.2705, train acc: 0.8871, val loss: 0.2686, val acc: 0.8874  (best train acc: 0.9057, best val acc: 0.8944, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 28860] train loss: 0.2573, train acc: 0.8931, val loss: 0.2728, val acc: 0.8857  (best train acc: 0.9057, best val acc: 0.8944, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 28880] train loss: 0.2407, train acc: 0.9027, val loss: 0.2684, val acc: 0.8863  (best train acc: 0.9057, best val acc: 0.8944, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 28900] train loss: 0.2710, train acc: 0.8921, val loss: 0.2712, val acc: 0.8880  (best train acc: 0.9057, best val acc: 0.8944, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 28920] train loss: 0.2661, train acc: 0.8879, val loss: 0.2773, val acc: 0.8830  (best train acc: 0.9057, best val acc: 0.8944, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 28940] train loss: 0.2739, train acc: 0.8859, val loss: 0.2660, val acc: 0.8863  (best train acc: 0.9057, best val acc: 0.8944, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 28960] train loss: 0.2611, train acc: 0.8921, val loss: 0.2696, val acc: 0.8860  (best train acc: 0.9057, best val acc: 0.8944, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 28980] train loss: 0.2554, train acc: 0.8953, val loss: 0.2695, val acc: 0.8863  (best train acc: 0.9057, best val acc: 0.8944, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 29000] train loss: 0.2837, train acc: 0.8805, val loss: 0.2644, val acc: 0.8833  (best train acc: 0.9057, best val acc: 0.8944, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 29020] train loss: 0.2865, train acc: 0.8773, val loss: 0.2851, val acc: 0.8745  (best train acc: 0.9057, best val acc: 0.8944, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 29040] train loss: 0.2553, train acc: 0.8957, val loss: 0.2710, val acc: 0.8816  (best train acc: 0.9057, best val acc: 0.8944, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 29060] train loss: 0.2681, train acc: 0.8871, val loss: 0.2743, val acc: 0.8894  (best train acc: 0.9057, best val acc: 0.8944, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 29080] train loss: 0.2701, train acc: 0.8892, val loss: 0.2833, val acc: 0.8813  (best train acc: 0.9057, best val acc: 0.8944, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 29100] train loss: 0.2460, train acc: 0.9002, val loss: 0.2750, val acc: 0.8789  (best train acc: 0.9057, best val acc: 0.8944, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 29120] train loss: 0.2700, train acc: 0.8924, val loss: 0.2669, val acc: 0.8884  (best train acc: 0.9057, best val acc: 0.8944, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 29140] train loss: 0.2503, train acc: 0.8989, val loss: 0.2741, val acc: 0.8830  (best train acc: 0.9057, best val acc: 0.8944, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 29160] train loss: 0.2675, train acc: 0.8890, val loss: 0.2838, val acc: 0.8799  (best train acc: 0.9057, best val acc: 0.8944, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 29180] train loss: 0.2957, train acc: 0.8759, val loss: 0.2697, val acc: 0.8860  (best train acc: 0.9057, best val acc: 0.8944, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 29200] train loss: 0.2967, train acc: 0.8722, val loss: 0.3360, val acc: 0.8577  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 29220] train loss: 0.2907, train acc: 0.8836, val loss: 0.2728, val acc: 0.8874  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 29240] train loss: 0.2778, train acc: 0.8874, val loss: 0.2682, val acc: 0.8813  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 29260] train loss: 0.2536, train acc: 0.8966, val loss: 0.2733, val acc: 0.8793  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 29280] train loss: 0.2736, train acc: 0.8879, val loss: 0.2817, val acc: 0.8782  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 29300] train loss: 0.2608, train acc: 0.8937, val loss: 0.2859, val acc: 0.8745  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 29320] train loss: 0.2924, train acc: 0.8785, val loss: 0.2829, val acc: 0.8739  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 29340] train loss: 0.2520, train acc: 0.8989, val loss: 0.2657, val acc: 0.8911  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 29360] train loss: 0.2655, train acc: 0.8923, val loss: 0.2714, val acc: 0.8853  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 29380] train loss: 0.2686, train acc: 0.8841, val loss: 0.2764, val acc: 0.8776  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 29400] train loss: 0.3140, train acc: 0.8683, val loss: 0.3498, val acc: 0.8543  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 29420] train loss: 0.2823, train acc: 0.8821, val loss: 0.2945, val acc: 0.8803  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 29440] train loss: 0.2604, train acc: 0.8925, val loss: 0.2797, val acc: 0.8789  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 29460] train loss: 0.2850, train acc: 0.8895, val loss: 0.2723, val acc: 0.8867  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 29480] train loss: 0.2641, train acc: 0.8905, val loss: 0.2736, val acc: 0.8830  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 29500] train loss: 0.2456, train acc: 0.8989, val loss: 0.2715, val acc: 0.8769  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 29520] train loss: 0.2526, train acc: 0.8983, val loss: 0.2722, val acc: 0.8867  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 29540] train loss: 0.2773, train acc: 0.8879, val loss: 0.2859, val acc: 0.8712  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 29560] train loss: 0.2851, train acc: 0.8871, val loss: 0.2694, val acc: 0.8850  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 29580] train loss: 0.2515, train acc: 0.8970, val loss: 0.2672, val acc: 0.8850  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 29600] train loss: 0.2487, train acc: 0.8983, val loss: 0.2739, val acc: 0.8860  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 29620] train loss: 0.2675, train acc: 0.8903, val loss: 0.2616, val acc: 0.8857  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 29640] train loss: 0.2767, train acc: 0.8848, val loss: 0.2618, val acc: 0.8884  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 29660] train loss: 0.2527, train acc: 0.8945, val loss: 0.2769, val acc: 0.8786  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 29680] train loss: 0.2650, train acc: 0.8940, val loss: 0.2640, val acc: 0.8853  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 29700] train loss: 0.2616, train acc: 0.8936, val loss: 0.2592, val acc: 0.8884  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 29720] train loss: 0.2597, train acc: 0.8879, val loss: 0.2701, val acc: 0.8853  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 29740] train loss: 0.2669, train acc: 0.8909, val loss: 0.2625, val acc: 0.8820  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 29760] train loss: 0.2687, train acc: 0.8838, val loss: 0.2942, val acc: 0.8742  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 29780] train loss: 0.2711, train acc: 0.8872, val loss: 0.2693, val acc: 0.8813  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 29800] train loss: 0.2602, train acc: 0.8901, val loss: 0.2871, val acc: 0.8782  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 29820] train loss: 0.2508, train acc: 0.8942, val loss: 0.2645, val acc: 0.8836  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 29840] train loss: 0.2533, train acc: 0.8957, val loss: 0.2798, val acc: 0.8823  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 29860] train loss: 0.2679, train acc: 0.8884, val loss: 0.2737, val acc: 0.8809  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 29880] train loss: 0.2512, train acc: 0.8961, val loss: 0.2875, val acc: 0.8772  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 29900] train loss: 0.2727, train acc: 0.8882, val loss: 0.2632, val acc: 0.8853  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 29920] train loss: 0.2779, train acc: 0.8869, val loss: 0.2622, val acc: 0.8860  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 29940] train loss: 0.2582, train acc: 0.8937, val loss: 0.3036, val acc: 0.8651  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 29960] train loss: 0.2632, train acc: 0.8918, val loss: 0.2668, val acc: 0.8823  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 29980] train loss: 0.2587, train acc: 0.8912, val loss: 0.2623, val acc: 0.8867  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 30000] train loss: 0.2570, train acc: 0.8946, val loss: 0.2798, val acc: 0.8772  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 30020] train loss: 0.2604, train acc: 0.8894, val loss: 0.2772, val acc: 0.8820  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 30040] train loss: 0.2996, train acc: 0.8729, val loss: 0.2706, val acc: 0.8806  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 30060] train loss: 0.2449, train acc: 0.9028, val loss: 0.2600, val acc: 0.8823  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 30080] train loss: 0.2529, train acc: 0.8917, val loss: 0.2759, val acc: 0.8752  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 30100] train loss: 0.2560, train acc: 0.8969, val loss: 0.2581, val acc: 0.8884  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 30120] train loss: 0.2575, train acc: 0.8928, val loss: 0.2606, val acc: 0.8867  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 30140] train loss: 0.2662, train acc: 0.8944, val loss: 0.2784, val acc: 0.8820  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 30160] train loss: 0.2759, train acc: 0.8885, val loss: 0.2674, val acc: 0.8820  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 30180] train loss: 0.2781, train acc: 0.8861, val loss: 0.2762, val acc: 0.8816  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 30200] train loss: 0.2591, train acc: 0.8959, val loss: 0.2638, val acc: 0.8850  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 30220] train loss: 0.2610, train acc: 0.8889, val loss: 0.2628, val acc: 0.8894  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2352  @ epoch 30210 )\n",
      "[Epoch: 30240] train loss: 0.2525, train acc: 0.8946, val loss: 0.2586, val acc: 0.8887  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2352  @ epoch 30210 )\n",
      "[Epoch: 30260] train loss: 0.2386, train acc: 0.8991, val loss: 0.2684, val acc: 0.8843  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2352  @ epoch 30210 )\n",
      "[Epoch: 30280] train loss: 0.2367, train acc: 0.9038, val loss: 0.2575, val acc: 0.8853  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2352  @ epoch 30210 )\n",
      "[Epoch: 30300] train loss: 0.2652, train acc: 0.8947, val loss: 0.2923, val acc: 0.8712  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2352  @ epoch 30210 )\n",
      "[Epoch: 30320] train loss: 0.2780, train acc: 0.8809, val loss: 0.2629, val acc: 0.8890  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2352  @ epoch 30210 )\n",
      "[Epoch: 30340] train loss: 0.2489, train acc: 0.8965, val loss: 0.2647, val acc: 0.8823  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2352  @ epoch 30210 )\n",
      "[Epoch: 30360] train loss: 0.2625, train acc: 0.8921, val loss: 0.2616, val acc: 0.8894  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2352  @ epoch 30210 )\n",
      "[Epoch: 30380] train loss: 0.2519, train acc: 0.8957, val loss: 0.2974, val acc: 0.8725  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2352  @ epoch 30210 )\n",
      "[Epoch: 30400] train loss: 0.2600, train acc: 0.8934, val loss: 0.2783, val acc: 0.8813  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2352  @ epoch 30210 )\n",
      "[Epoch: 30420] train loss: 0.2437, train acc: 0.9002, val loss: 0.2788, val acc: 0.8853  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2352  @ epoch 30210 )\n",
      "[Epoch: 30440] train loss: 0.2637, train acc: 0.8908, val loss: 0.2632, val acc: 0.8863  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2352  @ epoch 30210 )\n",
      "[Epoch: 30460] train loss: 0.2462, train acc: 0.9017, val loss: 0.3047, val acc: 0.8675  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2352  @ epoch 30210 )\n",
      "[Epoch: 30480] train loss: 0.2699, train acc: 0.8882, val loss: 0.2654, val acc: 0.8826  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2352  @ epoch 30210 )\n",
      "[Epoch: 30500] train loss: 0.3332, train acc: 0.8675, val loss: 0.2791, val acc: 0.8759  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2352  @ epoch 30210 )\n",
      "[Epoch: 30520] train loss: 0.2608, train acc: 0.8917, val loss: 0.2797, val acc: 0.8722  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2352  @ epoch 30210 )\n",
      "[Epoch: 30540] train loss: 0.2613, train acc: 0.8904, val loss: 0.2795, val acc: 0.8769  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2352  @ epoch 30210 )\n",
      "[Epoch: 30560] train loss: 0.2821, train acc: 0.8841, val loss: 0.2669, val acc: 0.8836  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2352  @ epoch 30210 )\n",
      "[Epoch: 30580] train loss: 0.2526, train acc: 0.8991, val loss: 0.2700, val acc: 0.8836  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2352  @ epoch 30210 )\n",
      "[Epoch: 30600] train loss: 0.2755, train acc: 0.8870, val loss: 0.2755, val acc: 0.8755  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2352  @ epoch 30210 )\n",
      "[Epoch: 30620] train loss: 0.2625, train acc: 0.8966, val loss: 0.2584, val acc: 0.8867  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2352  @ epoch 30210 )\n",
      "[Epoch: 30640] train loss: 0.2726, train acc: 0.8926, val loss: 0.2629, val acc: 0.8884  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2352  @ epoch 30210 )\n",
      "[Epoch: 30660] train loss: 0.2507, train acc: 0.8971, val loss: 0.2555, val acc: 0.8843  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2352  @ epoch 30210 )\n",
      "[Epoch: 30680] train loss: 0.2684, train acc: 0.8924, val loss: 0.2866, val acc: 0.8776  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2352  @ epoch 30210 )\n",
      "[Epoch: 30700] train loss: 0.2700, train acc: 0.8901, val loss: 0.2597, val acc: 0.8907  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2352  @ epoch 30210 )\n",
      "[Epoch: 30720] train loss: 0.2581, train acc: 0.8994, val loss: 0.2613, val acc: 0.8897  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2352  @ epoch 30210 )\n",
      "[Epoch: 30740] train loss: 0.2582, train acc: 0.8926, val loss: 0.2556, val acc: 0.8894  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2352  @ epoch 30210 )\n",
      "[Epoch: 30760] train loss: 0.2509, train acc: 0.8978, val loss: 0.2566, val acc: 0.8884  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2352  @ epoch 30210 )\n",
      "[Epoch: 30780] train loss: 0.3012, train acc: 0.8787, val loss: 0.2611, val acc: 0.8860  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2352  @ epoch 30210 )\n",
      "[Epoch: 30800] train loss: 0.2544, train acc: 0.8969, val loss: 0.2834, val acc: 0.8698  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2352  @ epoch 30210 )\n",
      "[Epoch: 30820] train loss: 0.2837, train acc: 0.8773, val loss: 0.3036, val acc: 0.8637  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2352  @ epoch 30210 )\n",
      "[Epoch: 30840] train loss: 0.2385, train acc: 0.9003, val loss: 0.2612, val acc: 0.8830  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2352  @ epoch 30210 )\n",
      "[Epoch: 30860] train loss: 0.2606, train acc: 0.8930, val loss: 0.2566, val acc: 0.8917  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2352  @ epoch 30210 )\n",
      "[Epoch: 30880] train loss: 0.2608, train acc: 0.8908, val loss: 0.2819, val acc: 0.8806  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2352  @ epoch 30210 )\n",
      "[Epoch: 30900] train loss: 0.2573, train acc: 0.8957, val loss: 0.2710, val acc: 0.8823  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2350  @ epoch 30886 )\n",
      "[Epoch: 30920] train loss: 0.2748, train acc: 0.8844, val loss: 0.2712, val acc: 0.8793  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2350  @ epoch 30886 )\n",
      "[Epoch: 30940] train loss: 0.2652, train acc: 0.8923, val loss: 0.2611, val acc: 0.8816  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2350  @ epoch 30886 )\n",
      "[Epoch: 30960] train loss: 0.2683, train acc: 0.8890, val loss: 0.2623, val acc: 0.8860  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2350  @ epoch 30886 )\n",
      "[Epoch: 30980] train loss: 0.2629, train acc: 0.8923, val loss: 0.2572, val acc: 0.8850  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2350  @ epoch 30886 )\n",
      "[Epoch: 31000] train loss: 0.2500, train acc: 0.8939, val loss: 0.2588, val acc: 0.8857  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2350  @ epoch 30886 )\n",
      "[Epoch: 31020] train loss: 0.2651, train acc: 0.8921, val loss: 0.2750, val acc: 0.8782  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2350  @ epoch 30886 )\n",
      "[Epoch: 31040] train loss: 0.2506, train acc: 0.9012, val loss: 0.2723, val acc: 0.8843  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2350  @ epoch 30886 )\n",
      "[Epoch: 31060] train loss: 0.2617, train acc: 0.8866, val loss: 0.2533, val acc: 0.8904  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2350  @ epoch 30886 )\n",
      "[Epoch: 31080] train loss: 0.3086, train acc: 0.8690, val loss: 0.2714, val acc: 0.8779  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2350  @ epoch 30886 )\n",
      "[Epoch: 31100] train loss: 0.2864, train acc: 0.8823, val loss: 0.2576, val acc: 0.8877  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2350  @ epoch 30886 )\n",
      "[Epoch: 31120] train loss: 0.2587, train acc: 0.8910, val loss: 0.3084, val acc: 0.8705  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2350  @ epoch 30886 )\n",
      "[Epoch: 31140] train loss: 0.2565, train acc: 0.8941, val loss: 0.2641, val acc: 0.8836  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2350  @ epoch 30886 )\n",
      "[Epoch: 31160] train loss: 0.2645, train acc: 0.8861, val loss: 0.2605, val acc: 0.8836  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2350  @ epoch 30886 )\n",
      "[Epoch: 31180] train loss: 0.2495, train acc: 0.8974, val loss: 0.3066, val acc: 0.8712  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2350  @ epoch 30886 )\n",
      "[Epoch: 31200] train loss: 0.2593, train acc: 0.8946, val loss: 0.2610, val acc: 0.8874  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2350  @ epoch 30886 )\n",
      "[Epoch: 31220] train loss: 0.2472, train acc: 0.8984, val loss: 0.2598, val acc: 0.8796  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2350  @ epoch 30886 )\n",
      "[Epoch: 31240] train loss: 0.2976, train acc: 0.8806, val loss: 0.2709, val acc: 0.8752  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2350  @ epoch 30886 )\n",
      "[Epoch: 31260] train loss: 0.2695, train acc: 0.8919, val loss: 0.2659, val acc: 0.8884  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2350  @ epoch 30886 )\n",
      "[Epoch: 31280] train loss: 0.2722, train acc: 0.8914, val loss: 0.2603, val acc: 0.8853  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2350  @ epoch 30886 )\n",
      "[Epoch: 31300] train loss: 0.2518, train acc: 0.8943, val loss: 0.2677, val acc: 0.8874  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2350  @ epoch 30886 )\n",
      "[Epoch: 31320] train loss: 0.2555, train acc: 0.8983, val loss: 0.2587, val acc: 0.8863  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 31340] train loss: 0.3022, train acc: 0.8817, val loss: 0.2749, val acc: 0.8799  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 31360] train loss: 0.2576, train acc: 0.8940, val loss: 0.2802, val acc: 0.8762  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 31380] train loss: 0.2492, train acc: 0.8955, val loss: 0.2543, val acc: 0.8850  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 31400] train loss: 0.2844, train acc: 0.8830, val loss: 0.2669, val acc: 0.8840  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 31420] train loss: 0.2442, train acc: 0.8994, val loss: 0.2629, val acc: 0.8897  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 31440] train loss: 0.2525, train acc: 0.8988, val loss: 0.2561, val acc: 0.8863  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 31460] train loss: 0.2625, train acc: 0.8886, val loss: 0.2589, val acc: 0.8816  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 31480] train loss: 0.2607, train acc: 0.8931, val loss: 0.2682, val acc: 0.8836  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 31500] train loss: 0.2471, train acc: 0.8989, val loss: 0.2596, val acc: 0.8860  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 31520] train loss: 0.2902, train acc: 0.8789, val loss: 0.2789, val acc: 0.8789  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 31540] train loss: 0.2763, train acc: 0.8861, val loss: 0.3042, val acc: 0.8648  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 31560] train loss: 0.2732, train acc: 0.8884, val loss: 0.2574, val acc: 0.8887  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 31580] train loss: 0.2519, train acc: 0.8982, val loss: 0.2807, val acc: 0.8786  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 31600] train loss: 0.2652, train acc: 0.8950, val loss: 0.2607, val acc: 0.8867  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 31620] train loss: 0.2815, train acc: 0.8791, val loss: 0.3100, val acc: 0.8708  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 31640] train loss: 0.2579, train acc: 0.8951, val loss: 0.3003, val acc: 0.8668  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 31660] train loss: 0.2431, train acc: 0.8989, val loss: 0.2541, val acc: 0.8887  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 31680] train loss: 0.2543, train acc: 0.8947, val loss: 0.2775, val acc: 0.8806  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 31700] train loss: 0.2655, train acc: 0.8895, val loss: 0.2647, val acc: 0.8833  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 31720] train loss: 0.2565, train acc: 0.8930, val loss: 0.2593, val acc: 0.8840  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 31740] train loss: 0.2615, train acc: 0.8969, val loss: 0.2552, val acc: 0.8857  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 31760] train loss: 0.2841, train acc: 0.8838, val loss: 0.2760, val acc: 0.8806  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 31780] train loss: 0.2451, train acc: 0.9008, val loss: 0.2545, val acc: 0.8850  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 31800] train loss: 0.2853, train acc: 0.8862, val loss: 0.3440, val acc: 0.8530  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 31820] train loss: 0.2512, train acc: 0.8955, val loss: 0.2597, val acc: 0.8857  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 31840] train loss: 0.2736, train acc: 0.8845, val loss: 0.2783, val acc: 0.8786  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 31860] train loss: 0.3219, train acc: 0.8700, val loss: 0.2667, val acc: 0.8823  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 31880] train loss: 0.2723, train acc: 0.8916, val loss: 0.2593, val acc: 0.8870  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 31900] train loss: 0.2645, train acc: 0.8900, val loss: 0.2608, val acc: 0.8823  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 31920] train loss: 0.2502, train acc: 0.8990, val loss: 0.2588, val acc: 0.8853  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 31940] train loss: 0.2503, train acc: 0.8944, val loss: 0.2662, val acc: 0.8803  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 31960] train loss: 0.2492, train acc: 0.8997, val loss: 0.2701, val acc: 0.8789  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 31980] train loss: 0.2479, train acc: 0.9008, val loss: 0.2594, val acc: 0.8863  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 32000] train loss: 0.2403, train acc: 0.9035, val loss: 0.2624, val acc: 0.8870  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 32020] train loss: 0.2577, train acc: 0.8931, val loss: 0.2695, val acc: 0.8820  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 32040] train loss: 0.2503, train acc: 0.8930, val loss: 0.2693, val acc: 0.8874  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 32060] train loss: 0.2783, train acc: 0.8893, val loss: 0.2707, val acc: 0.8840  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 32080] train loss: 0.2616, train acc: 0.8894, val loss: 0.2531, val acc: 0.8884  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 32100] train loss: 0.2501, train acc: 0.8971, val loss: 0.2624, val acc: 0.8843  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 32120] train loss: 0.2948, train acc: 0.8812, val loss: 0.2705, val acc: 0.8850  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 32140] train loss: 0.2448, train acc: 0.8991, val loss: 0.2560, val acc: 0.8857  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 32160] train loss: 0.2668, train acc: 0.8898, val loss: 0.2919, val acc: 0.8718  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 32180] train loss: 0.2398, train acc: 0.9038, val loss: 0.2600, val acc: 0.8857  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 32200] train loss: 0.2679, train acc: 0.8907, val loss: 0.2571, val acc: 0.8863  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 32220] train loss: 0.2769, train acc: 0.8840, val loss: 0.2639, val acc: 0.8884  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 32240] train loss: 0.2607, train acc: 0.8908, val loss: 0.2842, val acc: 0.8857  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 32260] train loss: 0.2698, train acc: 0.8864, val loss: 0.2542, val acc: 0.8843  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 32280] train loss: 0.2515, train acc: 0.9018, val loss: 0.2649, val acc: 0.8840  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 32300] train loss: 0.2601, train acc: 0.8892, val loss: 0.2674, val acc: 0.8826  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 32320] train loss: 0.2757, train acc: 0.8871, val loss: 0.2627, val acc: 0.8826  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 32340] train loss: 0.2715, train acc: 0.8932, val loss: 0.2906, val acc: 0.8678  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 32360] train loss: 0.2683, train acc: 0.8858, val loss: 0.2846, val acc: 0.8745  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 32380] train loss: 0.2673, train acc: 0.8887, val loss: 0.2752, val acc: 0.8833  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 32400] train loss: 0.2569, train acc: 0.8890, val loss: 0.2656, val acc: 0.8816  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 32420] train loss: 0.2603, train acc: 0.8963, val loss: 0.2673, val acc: 0.8830  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 32440] train loss: 0.2516, train acc: 0.8970, val loss: 0.2580, val acc: 0.8887  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 32460] train loss: 0.2444, train acc: 0.9032, val loss: 0.2551, val acc: 0.8847  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 32480] train loss: 0.2704, train acc: 0.8891, val loss: 0.2767, val acc: 0.8776  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 32500] train loss: 0.2428, train acc: 0.8965, val loss: 0.2598, val acc: 0.8836  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 32520] train loss: 0.2552, train acc: 0.8953, val loss: 0.3040, val acc: 0.8654  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 32540] train loss: 0.2449, train acc: 0.8987, val loss: 0.2589, val acc: 0.8874  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 32560] train loss: 0.3019, train acc: 0.8776, val loss: 0.2562, val acc: 0.8847  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 32580] train loss: 0.2586, train acc: 0.8947, val loss: 0.2617, val acc: 0.8847  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 32600] train loss: 0.2336, train acc: 0.9076, val loss: 0.2535, val acc: 0.8884  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 32620] train loss: 0.2644, train acc: 0.8950, val loss: 0.2577, val acc: 0.8894  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 32640] train loss: 0.3009, train acc: 0.8791, val loss: 0.2602, val acc: 0.8833  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 32660] train loss: 0.2669, train acc: 0.8926, val loss: 0.2653, val acc: 0.8826  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 32680] train loss: 0.2690, train acc: 0.8910, val loss: 0.2565, val acc: 0.8860  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 32700] train loss: 0.2711, train acc: 0.8838, val loss: 0.2612, val acc: 0.8847  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 32720] train loss: 0.2585, train acc: 0.8926, val loss: 0.2630, val acc: 0.8890  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 32740] train loss: 0.2836, train acc: 0.8794, val loss: 0.3168, val acc: 0.8634  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 32760] train loss: 0.2459, train acc: 0.8999, val loss: 0.2667, val acc: 0.8847  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 32780] train loss: 0.2470, train acc: 0.8977, val loss: 0.2609, val acc: 0.8880  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 32800] train loss: 0.2593, train acc: 0.8913, val loss: 0.2636, val acc: 0.8853  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 32820] train loss: 0.2479, train acc: 0.8984, val loss: 0.2635, val acc: 0.8803  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 32840] train loss: 0.2650, train acc: 0.8887, val loss: 0.2637, val acc: 0.8820  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 32860] train loss: 0.2672, train acc: 0.8927, val loss: 0.2619, val acc: 0.8840  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 32880] train loss: 0.2798, train acc: 0.8737, val loss: 0.2631, val acc: 0.8782  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 32900] train loss: 0.2609, train acc: 0.8939, val loss: 0.2794, val acc: 0.8755  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 32920] train loss: 0.2513, train acc: 0.9004, val loss: 0.2971, val acc: 0.8718  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 32940] train loss: 0.2502, train acc: 0.8968, val loss: 0.2524, val acc: 0.8850  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 32960] train loss: 0.2649, train acc: 0.8950, val loss: 0.2788, val acc: 0.8806  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 32980] train loss: 0.2382, train acc: 0.9044, val loss: 0.2580, val acc: 0.8904  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 33000] train loss: 0.2503, train acc: 0.8997, val loss: 0.2539, val acc: 0.8826  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 33020] train loss: 0.2502, train acc: 0.8974, val loss: 0.2560, val acc: 0.8897  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 33040] train loss: 0.2510, train acc: 0.8961, val loss: 0.2799, val acc: 0.8772  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 33060] train loss: 0.2547, train acc: 0.8952, val loss: 0.2680, val acc: 0.8843  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 33080] train loss: 0.2507, train acc: 0.8970, val loss: 0.2786, val acc: 0.8769  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 33100] train loss: 0.2513, train acc: 0.8940, val loss: 0.2581, val acc: 0.8863  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 33120] train loss: 0.2691, train acc: 0.8905, val loss: 0.2584, val acc: 0.8867  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 33140] train loss: 0.3138, train acc: 0.8755, val loss: 0.2799, val acc: 0.8745  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 33160] train loss: 0.2534, train acc: 0.8971, val loss: 0.2594, val acc: 0.8880  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 33180] train loss: 0.2509, train acc: 0.8987, val loss: 0.2511, val acc: 0.8860  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 33200] train loss: 0.3313, train acc: 0.8721, val loss: 0.3019, val acc: 0.8702  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 33220] train loss: 0.2591, train acc: 0.8915, val loss: 0.2602, val acc: 0.8870  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 33240] train loss: 0.2393, train acc: 0.9024, val loss: 0.2734, val acc: 0.8793  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 33260] train loss: 0.2618, train acc: 0.8955, val loss: 0.2573, val acc: 0.8860  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 33280] train loss: 0.2813, train acc: 0.8872, val loss: 0.2703, val acc: 0.8803  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 33300] train loss: 0.2472, train acc: 0.8960, val loss: 0.2711, val acc: 0.8836  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 33320] train loss: 0.2449, train acc: 0.9002, val loss: 0.2595, val acc: 0.8847  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 33340] train loss: 0.2478, train acc: 0.8967, val loss: 0.2514, val acc: 0.8874  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 33360] train loss: 0.2504, train acc: 0.8973, val loss: 0.2633, val acc: 0.8860  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 33380] train loss: 0.2473, train acc: 0.8930, val loss: 0.2592, val acc: 0.8809  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 33400] train loss: 0.2608, train acc: 0.8933, val loss: 0.2763, val acc: 0.8809  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 33420] train loss: 0.2432, train acc: 0.8978, val loss: 0.2634, val acc: 0.8874  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 33440] train loss: 0.2440, train acc: 0.8968, val loss: 0.2526, val acc: 0.8874  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 33460] train loss: 0.2455, train acc: 0.9001, val loss: 0.2648, val acc: 0.8820  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 33480] train loss: 0.2341, train acc: 0.9066, val loss: 0.2539, val acc: 0.8850  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 33500] train loss: 0.2435, train acc: 0.8990, val loss: 0.2556, val acc: 0.8847  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 33520] train loss: 0.2454, train acc: 0.8997, val loss: 0.2579, val acc: 0.8816  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 33540] train loss: 0.2641, train acc: 0.8938, val loss: 0.2738, val acc: 0.8752  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 33560] train loss: 0.2843, train acc: 0.8822, val loss: 0.2573, val acc: 0.8853  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 33580] train loss: 0.2423, train acc: 0.8988, val loss: 0.2602, val acc: 0.8816  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 33600] train loss: 0.2498, train acc: 0.8947, val loss: 0.2583, val acc: 0.8826  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 33620] train loss: 0.2468, train acc: 0.8974, val loss: 0.2526, val acc: 0.8880  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 33640] train loss: 0.2787, train acc: 0.8850, val loss: 0.2933, val acc: 0.8755  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 33660] train loss: 0.2356, train acc: 0.9001, val loss: 0.2546, val acc: 0.8874  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 33680] train loss: 0.2550, train acc: 0.8973, val loss: 0.2682, val acc: 0.8813  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 33700] train loss: 0.2642, train acc: 0.8892, val loss: 0.3011, val acc: 0.8607  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 33720] train loss: 0.2649, train acc: 0.8882, val loss: 0.2546, val acc: 0.8850  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 33740] train loss: 0.2537, train acc: 0.8943, val loss: 0.2680, val acc: 0.8863  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 33760] train loss: 0.2616, train acc: 0.8897, val loss: 0.2958, val acc: 0.8732  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 33780] train loss: 0.2463, train acc: 0.8971, val loss: 0.2595, val acc: 0.8826  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 33800] train loss: 0.2628, train acc: 0.8895, val loss: 0.2591, val acc: 0.8836  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 33820] train loss: 0.2679, train acc: 0.8932, val loss: 0.2638, val acc: 0.8830  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 33840] train loss: 0.2695, train acc: 0.8895, val loss: 0.2694, val acc: 0.8813  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 33860] train loss: 0.2699, train acc: 0.8907, val loss: 0.2548, val acc: 0.8874  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 33880] train loss: 0.2438, train acc: 0.9008, val loss: 0.2540, val acc: 0.8860  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 33900] train loss: 0.2715, train acc: 0.8882, val loss: 0.2593, val acc: 0.8836  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 33920] train loss: 0.2787, train acc: 0.8889, val loss: 0.2770, val acc: 0.8772  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 33940] train loss: 0.2389, train acc: 0.9007, val loss: 0.2660, val acc: 0.8806  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 33960] train loss: 0.2812, train acc: 0.8825, val loss: 0.2721, val acc: 0.8779  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 33980] train loss: 0.2549, train acc: 0.8963, val loss: 0.2568, val acc: 0.8870  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 34000] train loss: 0.2396, train acc: 0.9011, val loss: 0.2626, val acc: 0.8863  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 34020] train loss: 0.2521, train acc: 0.8991, val loss: 0.2911, val acc: 0.8651  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 34040] train loss: 0.2629, train acc: 0.8868, val loss: 0.2617, val acc: 0.8867  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 34060] train loss: 0.2525, train acc: 0.8953, val loss: 0.2569, val acc: 0.8850  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 34080] train loss: 0.2447, train acc: 0.9007, val loss: 0.2770, val acc: 0.8732  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 34100] train loss: 0.2690, train acc: 0.8910, val loss: 0.2582, val acc: 0.8843  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 34120] train loss: 0.2587, train acc: 0.8924, val loss: 0.2692, val acc: 0.8850  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 34140] train loss: 0.2324, train acc: 0.9024, val loss: 0.2547, val acc: 0.8820  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 34160] train loss: 0.2613, train acc: 0.8882, val loss: 0.2529, val acc: 0.8863  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 34180] train loss: 0.2461, train acc: 0.9013, val loss: 0.2591, val acc: 0.8847  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 34200] train loss: 0.2564, train acc: 0.8937, val loss: 0.2526, val acc: 0.8880  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 34220] train loss: 0.2506, train acc: 0.8967, val loss: 0.2638, val acc: 0.8766  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 34240] train loss: 0.2661, train acc: 0.8900, val loss: 0.2720, val acc: 0.8799  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 34260] train loss: 0.2657, train acc: 0.8928, val loss: 0.2732, val acc: 0.8766  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 34280] train loss: 0.2559, train acc: 0.8940, val loss: 0.2679, val acc: 0.8762  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 34300] train loss: 0.2424, train acc: 0.9005, val loss: 0.2592, val acc: 0.8853  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 34320] train loss: 0.2470, train acc: 0.9002, val loss: 0.2587, val acc: 0.8840  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 34340] train loss: 0.2677, train acc: 0.8949, val loss: 0.2972, val acc: 0.8698  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 34360] train loss: 0.2727, train acc: 0.8863, val loss: 0.2532, val acc: 0.8860  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 34380] train loss: 0.2514, train acc: 0.8995, val loss: 0.2636, val acc: 0.8836  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 34400] train loss: 0.2553, train acc: 0.8976, val loss: 0.2803, val acc: 0.8728  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 34420] train loss: 0.2749, train acc: 0.8886, val loss: 0.2568, val acc: 0.8857  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 34440] train loss: 0.2575, train acc: 0.8940, val loss: 0.2720, val acc: 0.8823  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 34460] train loss: 0.2732, train acc: 0.8890, val loss: 0.2644, val acc: 0.8799  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 34480] train loss: 0.2664, train acc: 0.8838, val loss: 0.2604, val acc: 0.8830  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 34500] train loss: 0.2449, train acc: 0.9014, val loss: 0.2598, val acc: 0.8840  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 34520] train loss: 0.2802, train acc: 0.8864, val loss: 0.2560, val acc: 0.8789  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 34540] train loss: 0.2561, train acc: 0.8984, val loss: 0.2698, val acc: 0.8749  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 34560] train loss: 0.2630, train acc: 0.8928, val loss: 0.2551, val acc: 0.8843  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 34580] train loss: 0.2828, train acc: 0.8806, val loss: 0.2625, val acc: 0.8789  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 34600] train loss: 0.2431, train acc: 0.9033, val loss: 0.2739, val acc: 0.8782  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 34620] train loss: 0.2532, train acc: 0.8948, val loss: 0.2824, val acc: 0.8799  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 34640] train loss: 0.2673, train acc: 0.8915, val loss: 0.2648, val acc: 0.8813  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 34660] train loss: 0.2402, train acc: 0.8992, val loss: 0.2611, val acc: 0.8830  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 34680] train loss: 0.3652, train acc: 0.8637, val loss: 0.2625, val acc: 0.8853  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 34700] train loss: 0.2470, train acc: 0.8994, val loss: 0.2715, val acc: 0.8776  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 34720] train loss: 0.2495, train acc: 0.8965, val loss: 0.2493, val acc: 0.8833  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 34740] train loss: 0.2398, train acc: 0.8991, val loss: 0.2729, val acc: 0.8816  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 34760] train loss: 0.2426, train acc: 0.8967, val loss: 0.2623, val acc: 0.8850  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 34780] train loss: 0.2831, train acc: 0.8845, val loss: 0.3050, val acc: 0.8641  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 34800] train loss: 0.2680, train acc: 0.8872, val loss: 0.2586, val acc: 0.8820  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 34820] train loss: 0.2769, train acc: 0.8866, val loss: 0.2688, val acc: 0.8847  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 34840] train loss: 0.2450, train acc: 0.8951, val loss: 0.2570, val acc: 0.8793  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 34860] train loss: 0.2973, train acc: 0.8830, val loss: 0.2558, val acc: 0.8833  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 34880] train loss: 0.2642, train acc: 0.8874, val loss: 0.2779, val acc: 0.8755  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 34900] train loss: 0.3053, train acc: 0.8800, val loss: 0.2744, val acc: 0.8698  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 34920] train loss: 0.2612, train acc: 0.8874, val loss: 0.2772, val acc: 0.8776  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 34940] train loss: 0.2998, train acc: 0.8845, val loss: 0.2469, val acc: 0.8843  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 34960] train loss: 0.2681, train acc: 0.8892, val loss: 0.2933, val acc: 0.8691  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 34980] train loss: 0.2418, train acc: 0.9017, val loss: 0.2532, val acc: 0.8820  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 35000] train loss: 0.2536, train acc: 0.8957, val loss: 0.2620, val acc: 0.8806  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 35020] train loss: 0.2527, train acc: 0.8956, val loss: 0.2586, val acc: 0.8833  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 35040] train loss: 0.2907, train acc: 0.8757, val loss: 0.2786, val acc: 0.8732  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 35060] train loss: 0.2543, train acc: 0.8905, val loss: 0.2501, val acc: 0.8840  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 35080] train loss: 0.2500, train acc: 0.8981, val loss: 0.2531, val acc: 0.8843  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 35100] train loss: 0.2419, train acc: 0.9008, val loss: 0.2588, val acc: 0.8887  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 35120] train loss: 0.2421, train acc: 0.8979, val loss: 0.2673, val acc: 0.8762  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 35140] train loss: 0.2530, train acc: 0.8976, val loss: 0.2698, val acc: 0.8853  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 35160] train loss: 0.2438, train acc: 0.9017, val loss: 0.2603, val acc: 0.8853  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 35180] train loss: 0.2479, train acc: 0.8978, val loss: 0.2624, val acc: 0.8769  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 35200] train loss: 0.2689, train acc: 0.8931, val loss: 0.2769, val acc: 0.8803  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 35220] train loss: 0.2489, train acc: 0.8959, val loss: 0.2589, val acc: 0.8863  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 35240] train loss: 0.2479, train acc: 0.8973, val loss: 0.2497, val acc: 0.8803  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 35260] train loss: 0.2584, train acc: 0.8968, val loss: 0.2521, val acc: 0.8877  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 35280] train loss: 0.2457, train acc: 0.8976, val loss: 0.2535, val acc: 0.8847  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 35300] train loss: 0.2609, train acc: 0.8948, val loss: 0.2582, val acc: 0.8786  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 35320] train loss: 0.2921, train acc: 0.8766, val loss: 0.2567, val acc: 0.8833  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 35340] train loss: 0.2709, train acc: 0.8871, val loss: 0.2570, val acc: 0.8830  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 35360] train loss: 0.2425, train acc: 0.9015, val loss: 0.2607, val acc: 0.8796  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 35380] train loss: 0.2583, train acc: 0.8907, val loss: 0.2517, val acc: 0.8887  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 35400] train loss: 0.2569, train acc: 0.8988, val loss: 0.2536, val acc: 0.8820  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 35420] train loss: 0.2768, train acc: 0.8876, val loss: 0.2599, val acc: 0.8884  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 35440] train loss: 0.2604, train acc: 0.8931, val loss: 0.2501, val acc: 0.8857  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 35460] train loss: 0.2709, train acc: 0.8908, val loss: 0.2548, val acc: 0.8880  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 35480] train loss: 0.2354, train acc: 0.9029, val loss: 0.2525, val acc: 0.8857  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 35500] train loss: 0.2646, train acc: 0.8900, val loss: 0.2736, val acc: 0.8789  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 35520] train loss: 0.2610, train acc: 0.8900, val loss: 0.2490, val acc: 0.8830  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 35540] train loss: 0.2504, train acc: 0.8955, val loss: 0.2574, val acc: 0.8833  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 35560] train loss: 0.2512, train acc: 0.8958, val loss: 0.2649, val acc: 0.8796  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 35580] train loss: 0.2411, train acc: 0.9008, val loss: 0.2604, val acc: 0.8850  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 35600] train loss: 0.3213, train acc: 0.8762, val loss: 0.2696, val acc: 0.8803  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 35620] train loss: 0.2496, train acc: 0.8999, val loss: 0.2540, val acc: 0.8850  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 35640] train loss: 0.2427, train acc: 0.9025, val loss: 0.2532, val acc: 0.8853  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 35660] train loss: 0.2356, train acc: 0.9045, val loss: 0.2535, val acc: 0.8843  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 35680] train loss: 0.2721, train acc: 0.8850, val loss: 0.2593, val acc: 0.8847  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 35700] train loss: 0.3113, train acc: 0.8704, val loss: 0.3118, val acc: 0.8641  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 35720] train loss: 0.2596, train acc: 0.8890, val loss: 0.2648, val acc: 0.8742  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 35740] train loss: 0.2884, train acc: 0.8759, val loss: 0.2633, val acc: 0.8786  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 35760] train loss: 0.2439, train acc: 0.8986, val loss: 0.2548, val acc: 0.8874  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 35780] train loss: 0.2373, train acc: 0.8985, val loss: 0.2549, val acc: 0.8850  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 35800] train loss: 0.2575, train acc: 0.8955, val loss: 0.2697, val acc: 0.8766  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 35820] train loss: 0.2422, train acc: 0.9050, val loss: 0.2519, val acc: 0.8887  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 35840] train loss: 0.2676, train acc: 0.8918, val loss: 0.2689, val acc: 0.8772  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 35860] train loss: 0.2551, train acc: 0.8944, val loss: 0.2527, val acc: 0.8840  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 35880] train loss: 0.2570, train acc: 0.8917, val loss: 0.2554, val acc: 0.8820  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 35900] train loss: 0.2577, train acc: 0.8924, val loss: 0.2547, val acc: 0.8830  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 35920] train loss: 0.2679, train acc: 0.8861, val loss: 0.2600, val acc: 0.8847  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 35940] train loss: 0.2449, train acc: 0.8993, val loss: 0.2808, val acc: 0.8779  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 35960] train loss: 0.2619, train acc: 0.8934, val loss: 0.2675, val acc: 0.8796  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 35980] train loss: 0.2590, train acc: 0.8929, val loss: 0.2536, val acc: 0.8847  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 36000] train loss: 0.2941, train acc: 0.8774, val loss: 0.2693, val acc: 0.8739  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 36020] train loss: 0.2709, train acc: 0.8915, val loss: 0.2779, val acc: 0.8732  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 36040] train loss: 0.2432, train acc: 0.9004, val loss: 0.2603, val acc: 0.8833  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 36060] train loss: 0.2438, train acc: 0.8988, val loss: 0.2576, val acc: 0.8823  (best train acc: 0.9093, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 36080] train loss: 0.2404, train acc: 0.9036, val loss: 0.2619, val acc: 0.8857  (best train acc: 0.9093, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 36100] train loss: 0.2653, train acc: 0.8882, val loss: 0.2551, val acc: 0.8853  (best train acc: 0.9093, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 36120] train loss: 0.2437, train acc: 0.9029, val loss: 0.2504, val acc: 0.8880  (best train acc: 0.9093, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 36140] train loss: 0.2436, train acc: 0.9055, val loss: 0.2522, val acc: 0.8867  (best train acc: 0.9093, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 36160] train loss: 0.2750, train acc: 0.8861, val loss: 0.2637, val acc: 0.8796  (best train acc: 0.9093, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 36180] train loss: 0.2487, train acc: 0.8967, val loss: 0.2618, val acc: 0.8857  (best train acc: 0.9093, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 36200] train loss: 0.2462, train acc: 0.8950, val loss: 0.2622, val acc: 0.8759  (best train acc: 0.9093, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 36220] train loss: 0.2537, train acc: 0.8970, val loss: 0.2522, val acc: 0.8847  (best train acc: 0.9093, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 36240] train loss: 0.2542, train acc: 0.8961, val loss: 0.2766, val acc: 0.8769  (best train acc: 0.9093, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 36260] train loss: 0.2457, train acc: 0.9017, val loss: 0.2622, val acc: 0.8870  (best train acc: 0.9093, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 36280] train loss: 0.2539, train acc: 0.8939, val loss: 0.2600, val acc: 0.8867  (best train acc: 0.9093, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 36300] train loss: 0.2514, train acc: 0.8936, val loss: 0.2535, val acc: 0.8799  (best train acc: 0.9093, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 36320] train loss: 0.2444, train acc: 0.8995, val loss: 0.2516, val acc: 0.8826  (best train acc: 0.9093, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 36340] train loss: 0.2591, train acc: 0.8924, val loss: 0.2551, val acc: 0.8840  (best train acc: 0.9093, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 36360] train loss: 0.2308, train acc: 0.9042, val loss: 0.2984, val acc: 0.8728  (best train acc: 0.9093, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 36380] train loss: 0.2459, train acc: 0.8970, val loss: 0.2552, val acc: 0.8826  (best train acc: 0.9093, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 36400] train loss: 0.2346, train acc: 0.9045, val loss: 0.2578, val acc: 0.8890  (best train acc: 0.9093, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 36420] train loss: 0.2435, train acc: 0.9036, val loss: 0.2590, val acc: 0.8843  (best train acc: 0.9093, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 36440] train loss: 0.2594, train acc: 0.8960, val loss: 0.2719, val acc: 0.8745  (best train acc: 0.9093, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 36460] train loss: 0.2317, train acc: 0.9052, val loss: 0.2533, val acc: 0.8799  (best train acc: 0.9093, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 36480] train loss: 0.2534, train acc: 0.8958, val loss: 0.2947, val acc: 0.8634  (best train acc: 0.9093, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 36500] train loss: 0.2423, train acc: 0.8972, val loss: 0.2595, val acc: 0.8826  (best train acc: 0.9093, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 36520] train loss: 0.2844, train acc: 0.8834, val loss: 0.2566, val acc: 0.8826  (best train acc: 0.9093, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 36540] train loss: 0.2947, train acc: 0.8790, val loss: 0.2771, val acc: 0.8766  (best train acc: 0.9093, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 36560] train loss: 0.3029, train acc: 0.8798, val loss: 0.2646, val acc: 0.8840  (best train acc: 0.9093, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 36580] train loss: 0.2505, train acc: 0.8973, val loss: 0.2484, val acc: 0.8860  (best train acc: 0.9093, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 36600] train loss: 0.2545, train acc: 0.8984, val loss: 0.2789, val acc: 0.8769  (best train acc: 0.9093, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 36620] train loss: 0.2862, train acc: 0.8832, val loss: 0.2471, val acc: 0.8867  (best train acc: 0.9093, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 36640] train loss: 0.2692, train acc: 0.8877, val loss: 0.2702, val acc: 0.8755  (best train acc: 0.9093, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 36660] train loss: 0.2737, train acc: 0.8904, val loss: 0.2709, val acc: 0.8779  (best train acc: 0.9093, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 36680] train loss: 0.2510, train acc: 0.9017, val loss: 0.2580, val acc: 0.8867  (best train acc: 0.9093, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 36700] train loss: 0.2588, train acc: 0.8947, val loss: 0.2922, val acc: 0.8708  (best train acc: 0.9093, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 36720] train loss: 0.2482, train acc: 0.8980, val loss: 0.2484, val acc: 0.8863  (best train acc: 0.9093, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 36740] train loss: 0.2541, train acc: 0.8976, val loss: 0.2517, val acc: 0.8809  (best train acc: 0.9093, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 36760] train loss: 0.2738, train acc: 0.8903, val loss: 0.2876, val acc: 0.8715  (best train acc: 0.9103, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 36780] train loss: 0.2337, train acc: 0.9020, val loss: 0.2533, val acc: 0.8850  (best train acc: 0.9103, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 36800] train loss: 0.2807, train acc: 0.8814, val loss: 0.2510, val acc: 0.8840  (best train acc: 0.9103, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 36820] train loss: 0.2510, train acc: 0.8996, val loss: 0.2772, val acc: 0.8752  (best train acc: 0.9103, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 36840] train loss: 0.2407, train acc: 0.9067, val loss: 0.2971, val acc: 0.8681  (best train acc: 0.9103, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 36860] train loss: 0.3275, train acc: 0.8683, val loss: 0.2494, val acc: 0.8860  (best train acc: 0.9103, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 36880] train loss: 0.2494, train acc: 0.9017, val loss: 0.2513, val acc: 0.8853  (best train acc: 0.9103, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 36900] train loss: 0.2486, train acc: 0.8984, val loss: 0.2530, val acc: 0.8860  (best train acc: 0.9103, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 36920] train loss: 0.2614, train acc: 0.8933, val loss: 0.2576, val acc: 0.8860  (best train acc: 0.9103, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 36940] train loss: 0.2468, train acc: 0.8970, val loss: 0.2652, val acc: 0.8799  (best train acc: 0.9103, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 36960] train loss: 0.2402, train acc: 0.8991, val loss: 0.2542, val acc: 0.8803  (best train acc: 0.9103, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 36980] train loss: 0.2528, train acc: 0.8937, val loss: 0.2473, val acc: 0.8890  (best train acc: 0.9103, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 37000] train loss: 0.2389, train acc: 0.9020, val loss: 0.2674, val acc: 0.8803  (best train acc: 0.9103, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 37020] train loss: 0.2517, train acc: 0.8956, val loss: 0.2545, val acc: 0.8857  (best train acc: 0.9103, best val acc: 0.8944, best train loss: 0.2262  @ epoch 37003 )\n",
      "[Epoch: 37040] train loss: 0.2505, train acc: 0.8986, val loss: 0.2493, val acc: 0.8887  (best train acc: 0.9103, best val acc: 0.8944, best train loss: 0.2262  @ epoch 37003 )\n",
      "[Epoch: 37060] train loss: 0.2401, train acc: 0.9012, val loss: 0.2558, val acc: 0.8789  (best train acc: 0.9103, best val acc: 0.8944, best train loss: 0.2262  @ epoch 37003 )\n",
      "[Epoch: 37080] train loss: 0.2542, train acc: 0.8950, val loss: 0.2665, val acc: 0.8786  (best train acc: 0.9103, best val acc: 0.8944, best train loss: 0.2262  @ epoch 37003 )\n",
      "[Epoch: 37100] train loss: 0.2592, train acc: 0.8911, val loss: 0.2503, val acc: 0.8836  (best train acc: 0.9103, best val acc: 0.8944, best train loss: 0.2242  @ epoch 37087 )\n",
      "[Epoch: 37120] train loss: 0.2335, train acc: 0.9027, val loss: 0.2597, val acc: 0.8820  (best train acc: 0.9103, best val acc: 0.8944, best train loss: 0.2242  @ epoch 37087 )\n",
      "[Epoch: 37140] train loss: 0.2587, train acc: 0.8905, val loss: 0.2653, val acc: 0.8836  (best train acc: 0.9103, best val acc: 0.8944, best train loss: 0.2242  @ epoch 37087 )\n",
      "[Epoch: 37160] train loss: 0.2404, train acc: 0.9020, val loss: 0.2606, val acc: 0.8867  (best train acc: 0.9103, best val acc: 0.8944, best train loss: 0.2242  @ epoch 37087 )\n",
      "[Epoch: 37180] train loss: 0.2347, train acc: 0.9009, val loss: 0.2602, val acc: 0.8840  (best train acc: 0.9103, best val acc: 0.8944, best train loss: 0.2242  @ epoch 37087 )\n",
      "[Epoch: 37200] train loss: 0.2686, train acc: 0.8867, val loss: 0.2701, val acc: 0.8759  (best train acc: 0.9103, best val acc: 0.8944, best train loss: 0.2242  @ epoch 37087 )\n",
      "[Epoch: 37220] train loss: 0.2391, train acc: 0.9025, val loss: 0.2505, val acc: 0.8877  (best train acc: 0.9103, best val acc: 0.8944, best train loss: 0.2242  @ epoch 37087 )\n",
      "[Epoch: 37240] train loss: 0.2328, train acc: 0.9061, val loss: 0.2860, val acc: 0.8728  (best train acc: 0.9103, best val acc: 0.8944, best train loss: 0.2242  @ epoch 37087 )\n",
      "[Epoch: 37260] train loss: 0.2390, train acc: 0.8997, val loss: 0.2507, val acc: 0.8840  (best train acc: 0.9103, best val acc: 0.8944, best train loss: 0.2242  @ epoch 37087 )\n",
      "[Epoch: 37280] train loss: 0.2489, train acc: 0.8975, val loss: 0.2737, val acc: 0.8823  (best train acc: 0.9103, best val acc: 0.8944, best train loss: 0.2242  @ epoch 37087 )\n",
      "[Epoch: 37300] train loss: 0.2488, train acc: 0.9003, val loss: 0.2580, val acc: 0.8877  (best train acc: 0.9103, best val acc: 0.8944, best train loss: 0.2242  @ epoch 37087 )\n",
      "[Epoch: 37320] train loss: 0.2546, train acc: 0.8914, val loss: 0.2494, val acc: 0.8867  (best train acc: 0.9103, best val acc: 0.8944, best train loss: 0.2228  @ epoch 37306 )\n",
      "[Epoch: 37340] train loss: 0.2452, train acc: 0.8963, val loss: 0.2645, val acc: 0.8799  (best train acc: 0.9103, best val acc: 0.8944, best train loss: 0.2228  @ epoch 37306 )\n",
      "[Epoch: 37360] train loss: 0.2432, train acc: 0.8956, val loss: 0.2528, val acc: 0.8833  (best train acc: 0.9103, best val acc: 0.8944, best train loss: 0.2228  @ epoch 37306 )\n",
      "[Epoch: 37380] train loss: 0.2307, train acc: 0.9043, val loss: 0.2565, val acc: 0.8830  (best train acc: 0.9103, best val acc: 0.8944, best train loss: 0.2228  @ epoch 37306 )\n",
      "[Epoch: 37400] train loss: 0.2362, train acc: 0.8986, val loss: 0.2492, val acc: 0.8884  (best train acc: 0.9103, best val acc: 0.8944, best train loss: 0.2228  @ epoch 37306 )\n",
      "[Epoch: 37420] train loss: 0.2468, train acc: 0.8944, val loss: 0.2570, val acc: 0.8877  (best train acc: 0.9103, best val acc: 0.8944, best train loss: 0.2228  @ epoch 37306 )\n",
      "[Epoch: 37440] train loss: 0.2814, train acc: 0.8857, val loss: 0.3198, val acc: 0.8631  (best train acc: 0.9103, best val acc: 0.8944, best train loss: 0.2228  @ epoch 37306 )\n",
      "[Epoch: 37460] train loss: 0.2378, train acc: 0.9003, val loss: 0.2701, val acc: 0.8789  (best train acc: 0.9103, best val acc: 0.8944, best train loss: 0.2228  @ epoch 37306 )\n",
      "[Epoch: 37480] train loss: 0.2441, train acc: 0.8965, val loss: 0.2615, val acc: 0.8857  (best train acc: 0.9103, best val acc: 0.8944, best train loss: 0.2228  @ epoch 37306 )\n",
      "[Epoch: 37500] train loss: 0.2449, train acc: 0.8989, val loss: 0.2529, val acc: 0.8850  (best train acc: 0.9103, best val acc: 0.8944, best train loss: 0.2228  @ epoch 37306 )\n",
      "[Epoch: 37520] train loss: 0.2837, train acc: 0.8858, val loss: 0.2580, val acc: 0.8826  (best train acc: 0.9103, best val acc: 0.8944, best train loss: 0.2228  @ epoch 37306 )\n",
      "[Epoch: 37540] train loss: 0.2378, train acc: 0.8978, val loss: 0.2558, val acc: 0.8826  (best train acc: 0.9103, best val acc: 0.8944, best train loss: 0.2228  @ epoch 37306 )\n",
      "[Epoch: 37560] train loss: 0.2338, train acc: 0.9018, val loss: 0.2555, val acc: 0.8847  (best train acc: 0.9103, best val acc: 0.8944, best train loss: 0.2209  @ epoch 37554 )\n",
      "[Epoch: 37580] train loss: 0.2317, train acc: 0.9024, val loss: 0.2639, val acc: 0.8786  (best train acc: 0.9103, best val acc: 0.8944, best train loss: 0.2209  @ epoch 37554 )\n",
      "[Epoch: 37600] train loss: 0.2345, train acc: 0.9072, val loss: 0.2675, val acc: 0.8762  (best train acc: 0.9103, best val acc: 0.8944, best train loss: 0.2209  @ epoch 37554 )\n",
      "[Epoch: 37620] train loss: 0.2391, train acc: 0.8986, val loss: 0.2662, val acc: 0.8847  (best train acc: 0.9110, best val acc: 0.8944, best train loss: 0.2209  @ epoch 37554 )\n",
      "[Epoch: 37640] train loss: 0.2357, train acc: 0.9007, val loss: 0.2576, val acc: 0.8860  (best train acc: 0.9110, best val acc: 0.8944, best train loss: 0.2184  @ epoch 37622 )\n",
      "[Epoch: 37660] train loss: 0.2277, train acc: 0.9095, val loss: 0.2707, val acc: 0.8813  (best train acc: 0.9110, best val acc: 0.8944, best train loss: 0.2184  @ epoch 37622 )\n",
      "[Epoch: 37680] train loss: 0.2398, train acc: 0.9016, val loss: 0.2732, val acc: 0.8739  (best train acc: 0.9110, best val acc: 0.8944, best train loss: 0.2184  @ epoch 37622 )\n",
      "[Epoch: 37700] train loss: 0.2684, train acc: 0.8857, val loss: 0.2689, val acc: 0.8826  (best train acc: 0.9110, best val acc: 0.8944, best train loss: 0.2184  @ epoch 37622 )\n",
      "[Epoch: 37720] train loss: 0.2570, train acc: 0.8941, val loss: 0.2894, val acc: 0.8772  (best train acc: 0.9110, best val acc: 0.8944, best train loss: 0.2183  @ epoch 37714 )\n",
      "[Epoch: 37740] train loss: 0.2451, train acc: 0.8976, val loss: 0.2506, val acc: 0.8867  (best train acc: 0.9110, best val acc: 0.8944, best train loss: 0.2183  @ epoch 37714 )\n",
      "[Epoch: 37760] train loss: 0.2432, train acc: 0.9018, val loss: 0.2663, val acc: 0.8803  (best train acc: 0.9110, best val acc: 0.8944, best train loss: 0.2183  @ epoch 37714 )\n",
      "[Epoch: 37780] train loss: 0.2723, train acc: 0.8900, val loss: 0.2520, val acc: 0.8853  (best train acc: 0.9110, best val acc: 0.8944, best train loss: 0.2183  @ epoch 37714 )\n",
      "[Epoch: 37800] train loss: 0.2415, train acc: 0.8950, val loss: 0.2596, val acc: 0.8830  (best train acc: 0.9110, best val acc: 0.8944, best train loss: 0.2183  @ epoch 37714 )\n",
      "[Epoch: 37820] train loss: 0.2424, train acc: 0.9012, val loss: 0.2560, val acc: 0.8826  (best train acc: 0.9110, best val acc: 0.8944, best train loss: 0.2183  @ epoch 37714 )\n",
      "[Epoch: 37840] train loss: 0.2302, train acc: 0.9048, val loss: 0.2577, val acc: 0.8850  (best train acc: 0.9110, best val acc: 0.8944, best train loss: 0.2183  @ epoch 37714 )\n",
      "[Epoch: 37860] train loss: 0.2279, train acc: 0.9068, val loss: 0.2614, val acc: 0.8840  (best train acc: 0.9110, best val acc: 0.8944, best train loss: 0.2183  @ epoch 37714 )\n",
      "[Epoch: 37880] train loss: 0.2436, train acc: 0.9004, val loss: 0.2670, val acc: 0.8843  (best train acc: 0.9110, best val acc: 0.8944, best train loss: 0.2183  @ epoch 37714 )\n",
      "[Epoch: 37900] train loss: 0.2326, train acc: 0.9045, val loss: 0.3119, val acc: 0.8607  (best train acc: 0.9110, best val acc: 0.8944, best train loss: 0.2183  @ epoch 37714 )\n",
      "[Epoch: 37920] train loss: 0.2593, train acc: 0.8944, val loss: 0.2676, val acc: 0.8836  (best train acc: 0.9110, best val acc: 0.8944, best train loss: 0.2183  @ epoch 37714 )\n",
      "[Epoch: 37940] train loss: 0.2230, train acc: 0.9081, val loss: 0.2467, val acc: 0.8820  (best train acc: 0.9110, best val acc: 0.8944, best train loss: 0.2183  @ epoch 37714 )\n",
      "[Epoch: 37960] train loss: 0.2615, train acc: 0.8914, val loss: 0.2596, val acc: 0.8826  (best train acc: 0.9110, best val acc: 0.8944, best train loss: 0.2183  @ epoch 37714 )\n",
      "[Epoch: 37980] train loss: 0.2414, train acc: 0.8976, val loss: 0.2552, val acc: 0.8884  (best train acc: 0.9110, best val acc: 0.8944, best train loss: 0.2183  @ epoch 37714 )\n",
      "[Epoch: 38000] train loss: 0.2395, train acc: 0.8994, val loss: 0.2517, val acc: 0.8843  (best train acc: 0.9110, best val acc: 0.8944, best train loss: 0.2183  @ epoch 37714 )\n",
      "[Epoch: 38020] train loss: 0.2315, train acc: 0.9026, val loss: 0.2481, val acc: 0.8847  (best train acc: 0.9110, best val acc: 0.8944, best train loss: 0.2183  @ epoch 37714 )\n",
      "[Epoch: 38040] train loss: 0.2411, train acc: 0.8997, val loss: 0.2631, val acc: 0.8769  (best train acc: 0.9110, best val acc: 0.8944, best train loss: 0.2183  @ epoch 37714 )\n",
      "[Epoch: 38060] train loss: 0.2317, train acc: 0.9024, val loss: 0.2728, val acc: 0.8762  (best train acc: 0.9110, best val acc: 0.8944, best train loss: 0.2183  @ epoch 37714 )\n",
      "[Epoch: 38080] train loss: 0.2531, train acc: 0.9043, val loss: 0.2467, val acc: 0.8843  (best train acc: 0.9110, best val acc: 0.8944, best train loss: 0.2183  @ epoch 37714 )\n",
      "[Epoch: 38100] train loss: 0.2397, train acc: 0.8989, val loss: 0.2630, val acc: 0.8860  (best train acc: 0.9110, best val acc: 0.8944, best train loss: 0.2183  @ epoch 37714 )\n",
      "[Epoch: 38120] train loss: 0.2334, train acc: 0.9036, val loss: 0.2765, val acc: 0.8779  (best train acc: 0.9110, best val acc: 0.8944, best train loss: 0.2183  @ epoch 37714 )\n",
      "[Epoch: 38140] train loss: 0.2399, train acc: 0.9007, val loss: 0.2788, val acc: 0.8766  (best train acc: 0.9110, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 38160] train loss: 0.2455, train acc: 0.8953, val loss: 0.2604, val acc: 0.8809  (best train acc: 0.9110, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 38180] train loss: 0.2418, train acc: 0.9001, val loss: 0.2482, val acc: 0.8877  (best train acc: 0.9112, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 38200] train loss: 0.2219, train acc: 0.9099, val loss: 0.2535, val acc: 0.8833  (best train acc: 0.9112, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 38220] train loss: 0.2537, train acc: 0.8937, val loss: 0.2977, val acc: 0.8718  (best train acc: 0.9112, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 38240] train loss: 0.2299, train acc: 0.9067, val loss: 0.2560, val acc: 0.8874  (best train acc: 0.9112, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 38260] train loss: 0.2357, train acc: 0.9012, val loss: 0.2717, val acc: 0.8789  (best train acc: 0.9112, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 38280] train loss: 0.2724, train acc: 0.8850, val loss: 0.2520, val acc: 0.8823  (best train acc: 0.9112, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 38300] train loss: 0.2360, train acc: 0.9033, val loss: 0.2548, val acc: 0.8840  (best train acc: 0.9112, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 38320] train loss: 0.2494, train acc: 0.8970, val loss: 0.2633, val acc: 0.8863  (best train acc: 0.9112, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 38340] train loss: 0.2500, train acc: 0.8950, val loss: 0.2822, val acc: 0.8728  (best train acc: 0.9112, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 38360] train loss: 0.2287, train acc: 0.9036, val loss: 0.2617, val acc: 0.8789  (best train acc: 0.9112, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 38380] train loss: 0.2550, train acc: 0.8923, val loss: 0.2505, val acc: 0.8860  (best train acc: 0.9112, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 38400] train loss: 0.2633, train acc: 0.8908, val loss: 0.2760, val acc: 0.8735  (best train acc: 0.9112, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 38420] train loss: 0.2469, train acc: 0.8997, val loss: 0.2545, val acc: 0.8863  (best train acc: 0.9112, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 38440] train loss: 0.2511, train acc: 0.8916, val loss: 0.2553, val acc: 0.8853  (best train acc: 0.9112, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 38460] train loss: 0.2545, train acc: 0.8987, val loss: 0.2864, val acc: 0.8718  (best train acc: 0.9112, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 38480] train loss: 0.2269, train acc: 0.9072, val loss: 0.2631, val acc: 0.8826  (best train acc: 0.9112, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 38500] train loss: 0.2251, train acc: 0.9074, val loss: 0.2442, val acc: 0.8890  (best train acc: 0.9123, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 38520] train loss: 0.2358, train acc: 0.8995, val loss: 0.2566, val acc: 0.8840  (best train acc: 0.9123, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 38540] train loss: 0.2460, train acc: 0.8942, val loss: 0.2757, val acc: 0.8755  (best train acc: 0.9123, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 38560] train loss: 0.2310, train acc: 0.9030, val loss: 0.2555, val acc: 0.8874  (best train acc: 0.9123, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 38580] train loss: 0.2770, train acc: 0.8820, val loss: 0.2632, val acc: 0.8772  (best train acc: 0.9123, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 38600] train loss: 0.2410, train acc: 0.8986, val loss: 0.2482, val acc: 0.8874  (best train acc: 0.9123, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 38620] train loss: 0.2571, train acc: 0.8897, val loss: 0.2578, val acc: 0.8833  (best train acc: 0.9123, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 38640] train loss: 0.2266, train acc: 0.9025, val loss: 0.2569, val acc: 0.8820  (best train acc: 0.9123, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 38660] train loss: 0.2365, train acc: 0.9017, val loss: 0.2602, val acc: 0.8850  (best train acc: 0.9123, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 38680] train loss: 0.2515, train acc: 0.8918, val loss: 0.2514, val acc: 0.8860  (best train acc: 0.9123, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 38700] train loss: 0.2261, train acc: 0.9054, val loss: 0.2734, val acc: 0.8799  (best train acc: 0.9123, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 38720] train loss: 0.2314, train acc: 0.9059, val loss: 0.2564, val acc: 0.8830  (best train acc: 0.9123, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 38740] train loss: 0.2280, train acc: 0.9038, val loss: 0.2505, val acc: 0.8870  (best train acc: 0.9123, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 38760] train loss: 0.2510, train acc: 0.8943, val loss: 0.2522, val acc: 0.8843  (best train acc: 0.9123, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 38780] train loss: 0.2303, train acc: 0.9037, val loss: 0.2464, val acc: 0.8880  (best train acc: 0.9123, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 38800] train loss: 0.2604, train acc: 0.8985, val loss: 0.2495, val acc: 0.8833  (best train acc: 0.9123, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 38820] train loss: 0.2618, train acc: 0.8944, val loss: 0.2962, val acc: 0.8664  (best train acc: 0.9123, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 38840] train loss: 0.2645, train acc: 0.8884, val loss: 0.2600, val acc: 0.8793  (best train acc: 0.9123, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 38860] train loss: 0.2378, train acc: 0.9010, val loss: 0.2691, val acc: 0.8853  (best train acc: 0.9123, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 38880] train loss: 0.2463, train acc: 0.8971, val loss: 0.2655, val acc: 0.8843  (best train acc: 0.9123, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 38900] train loss: 0.2414, train acc: 0.9018, val loss: 0.2617, val acc: 0.8874  (best train acc: 0.9123, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 38920] train loss: 0.2358, train acc: 0.9004, val loss: 0.2545, val acc: 0.8850  (best train acc: 0.9123, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 38940] train loss: 0.2220, train acc: 0.9065, val loss: 0.2683, val acc: 0.8850  (best train acc: 0.9123, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 38960] train loss: 0.2505, train acc: 0.8978, val loss: 0.2812, val acc: 0.8695  (best train acc: 0.9123, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 38980] train loss: 0.2724, train acc: 0.8866, val loss: 0.2572, val acc: 0.8816  (best train acc: 0.9123, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 39000] train loss: 0.2276, train acc: 0.9049, val loss: 0.2578, val acc: 0.8833  (best train acc: 0.9123, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 39020] train loss: 0.2459, train acc: 0.8952, val loss: 0.2507, val acc: 0.8867  (best train acc: 0.9123, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 39040] train loss: 0.2454, train acc: 0.8973, val loss: 0.2707, val acc: 0.8769  (best train acc: 0.9123, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 39060] train loss: 0.2589, train acc: 0.8960, val loss: 0.2693, val acc: 0.8769  (best train acc: 0.9123, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 39080] train loss: 0.2639, train acc: 0.8880, val loss: 0.2521, val acc: 0.8840  (best train acc: 0.9123, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 39100] train loss: 0.2504, train acc: 0.8963, val loss: 0.2618, val acc: 0.8830  (best train acc: 0.9123, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 39120] train loss: 0.2411, train acc: 0.9036, val loss: 0.2726, val acc: 0.8772  (best train acc: 0.9123, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 39140] train loss: 0.2461, train acc: 0.8953, val loss: 0.2602, val acc: 0.8823  (best train acc: 0.9123, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 39160] train loss: 0.2769, train acc: 0.8798, val loss: 0.3321, val acc: 0.8506  (best train acc: 0.9123, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 39180] train loss: 0.2375, train acc: 0.9028, val loss: 0.2982, val acc: 0.8712  (best train acc: 0.9123, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 39200] train loss: 0.2700, train acc: 0.8917, val loss: 0.2598, val acc: 0.8847  (best train acc: 0.9123, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 39220] train loss: 0.2317, train acc: 0.9025, val loss: 0.2635, val acc: 0.8826  (best train acc: 0.9123, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 39240] train loss: 0.2474, train acc: 0.8962, val loss: 0.2484, val acc: 0.8860  (best train acc: 0.9123, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 39260] train loss: 0.2431, train acc: 0.8981, val loss: 0.2651, val acc: 0.8843  (best train acc: 0.9123, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 39280] train loss: 0.2302, train acc: 0.9083, val loss: 0.2573, val acc: 0.8884  (best train acc: 0.9123, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 39300] train loss: 0.2418, train acc: 0.8992, val loss: 0.2589, val acc: 0.8877  (best train acc: 0.9123, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 39320] train loss: 0.2547, train acc: 0.8970, val loss: 0.2638, val acc: 0.8870  (best train acc: 0.9123, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 39340] train loss: 0.2376, train acc: 0.9046, val loss: 0.2487, val acc: 0.8840  (best train acc: 0.9123, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 39360] train loss: 0.2313, train acc: 0.9064, val loss: 0.2517, val acc: 0.8816  (best train acc: 0.9123, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 39380] train loss: 0.2469, train acc: 0.8993, val loss: 0.2609, val acc: 0.8809  (best train acc: 0.9123, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 39400] train loss: 0.2216, train acc: 0.9069, val loss: 0.2594, val acc: 0.8806  (best train acc: 0.9123, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 39420] train loss: 0.2358, train acc: 0.9048, val loss: 0.2531, val acc: 0.8843  (best train acc: 0.9123, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 39440] train loss: 0.2257, train acc: 0.9061, val loss: 0.2598, val acc: 0.8789  (best train acc: 0.9123, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 39460] train loss: 0.2407, train acc: 0.9000, val loss: 0.2593, val acc: 0.8863  (best train acc: 0.9123, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 39480] train loss: 0.2357, train acc: 0.9002, val loss: 0.2476, val acc: 0.8860  (best train acc: 0.9123, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 39500] train loss: 0.2341, train acc: 0.9028, val loss: 0.2506, val acc: 0.8833  (best train acc: 0.9123, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 39520] train loss: 0.2230, train acc: 0.9090, val loss: 0.2712, val acc: 0.8742  (best train acc: 0.9123, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 39540] train loss: 0.2185, train acc: 0.9084, val loss: 0.2718, val acc: 0.8830  (best train acc: 0.9123, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 39560] train loss: 0.2312, train acc: 0.9018, val loss: 0.2598, val acc: 0.8793  (best train acc: 0.9123, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 39580] train loss: 0.2580, train acc: 0.8946, val loss: 0.2525, val acc: 0.8857  (best train acc: 0.9123, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 39600] train loss: 0.2230, train acc: 0.9079, val loss: 0.2506, val acc: 0.8820  (best train acc: 0.9123, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 39620] train loss: 0.2501, train acc: 0.8992, val loss: 0.2633, val acc: 0.8803  (best train acc: 0.9123, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 39640] train loss: 0.2352, train acc: 0.8976, val loss: 0.2860, val acc: 0.8779  (best train acc: 0.9123, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 39660] train loss: 0.2324, train acc: 0.9015, val loss: 0.2507, val acc: 0.8911  (best train acc: 0.9123, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 39680] train loss: 0.2468, train acc: 0.8974, val loss: 0.2610, val acc: 0.8857  (best train acc: 0.9123, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 39700] train loss: 0.2368, train acc: 0.9000, val loss: 0.2517, val acc: 0.8874  (best train acc: 0.9123, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 39720] train loss: 0.2595, train acc: 0.8891, val loss: 0.2515, val acc: 0.8857  (best train acc: 0.9123, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 39740] train loss: 0.2374, train acc: 0.8998, val loss: 0.2583, val acc: 0.8776  (best train acc: 0.9123, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 39760] train loss: 0.2278, train acc: 0.9050, val loss: 0.2509, val acc: 0.8840  (best train acc: 0.9123, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 39780] train loss: 0.2279, train acc: 0.9067, val loss: 0.2696, val acc: 0.8759  (best train acc: 0.9123, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 39800] train loss: 0.2375, train acc: 0.9005, val loss: 0.2529, val acc: 0.8850  (best train acc: 0.9123, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 39820] train loss: 0.2457, train acc: 0.8981, val loss: 0.2542, val acc: 0.8853  (best train acc: 0.9123, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 39840] train loss: 0.2451, train acc: 0.9010, val loss: 0.2510, val acc: 0.8833  (best train acc: 0.9123, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 39860] train loss: 0.2360, train acc: 0.9003, val loss: 0.2522, val acc: 0.8853  (best train acc: 0.9123, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 39880] train loss: 0.2232, train acc: 0.9061, val loss: 0.2681, val acc: 0.8796  (best train acc: 0.9123, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 39900] train loss: 0.2280, train acc: 0.9057, val loss: 0.2604, val acc: 0.8799  (best train acc: 0.9123, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 39920] train loss: 0.2313, train acc: 0.9087, val loss: 0.2632, val acc: 0.8813  (best train acc: 0.9123, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 39940] train loss: 0.2341, train acc: 0.8997, val loss: 0.2605, val acc: 0.8867  (best train acc: 0.9123, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 39960] train loss: 0.2310, train acc: 0.9033, val loss: 0.2757, val acc: 0.8799  (best train acc: 0.9123, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 39980] train loss: 0.2383, train acc: 0.9015, val loss: 0.2641, val acc: 0.8826  (best train acc: 0.9123, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 40000] train loss: 0.2373, train acc: 0.9001, val loss: 0.2479, val acc: 0.8877  (best train acc: 0.9123, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98       124\n",
      "           1       0.94      0.96      0.95       710\n",
      "           2       0.94      0.88      0.91       710\n",
      "           3       0.84      0.84      0.84       710\n",
      "           4       0.86      0.90      0.88       710\n",
      "\n",
      "    accuracy                           0.90      2964\n",
      "   macro avg       0.91      0.91      0.91      2964\n",
      "weighted avg       0.90      0.90      0.90      2964\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 504x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 504x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "lr              = 0.001\n",
    "epochs          = 40000\n",
    "weight_decay    = 0.0005\n",
    "\n",
    "classes         = ['P', 'LP', 'WN', 'LN', 'RN']\n",
    "gnn_model = GNN7L_Sage(dataset).to(device)\n",
    "pred = train(gnn_model, dataset, epochs, lr, weight_decay, classes, 'GraphSAGE_' + disease_Id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6aeea6fe202f5c7201d5940e4573c0a76b23e4e16f0e3784ac81597546f2b3b3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
