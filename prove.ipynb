{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from GraphSageModel import GNN7L_Sage\n",
    "from CreateDataset import get_dataset_from_graph\n",
    "from Paths import PATH_TO_DATASETS, PATH_TO_GRAPHS\n",
    "from GNNTrain import train, predict_from_saved_model\n",
    "from CreateGraph_from_diamond import create_graph_from_PPI\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# disease_Ids = ['C3714756','C0860207','C0011581','C0005586','C0001973']\n",
    "disease_Ids = ['C0006142','C0009402','C0023893']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Creating the graph...ok\n",
      "\t[+] Added 13458 nodes\n",
      "\t[+] Added 141272 edges\n",
      "[+] Removing self loops...ok\n",
      "\t[+] 13458 nodes\n",
      "\t[+] 138405 edges\n",
      "[+] Taking the LCC...ok\n",
      "\t[+] 13328 nodes\n",
      "\t[+] 138334 edges\n",
      "[+] Adding NeDBIT features...ok\n",
      "[+] Saving graph to path: Graphs/grafo_diamond_nedbit_C0006142.gml\n",
      "[i] Elapsed time: 1.141\n",
      "[+] Creating the graph...ok\n",
      "\t[+] Added 13458 nodes\n",
      "\t[+] Added 141272 edges\n",
      "[+] Removing self loops...ok\n",
      "\t[+] 13458 nodes\n",
      "\t[+] 138405 edges\n",
      "[+] Taking the LCC...ok\n",
      "\t[+] 13328 nodes\n",
      "\t[+] 138334 edges\n",
      "[+] Adding NeDBIT features...ok\n",
      "[+] Saving graph to path: Graphs/grafo_diamond_nedbit_C0009402.gml\n",
      "[i] Elapsed time: 1.099\n",
      "[+] Creating the graph...ok\n",
      "\t[+] Added 13458 nodes\n",
      "\t[+] Added 141272 edges\n",
      "[+] Removing self loops...ok\n",
      "\t[+] 13458 nodes\n",
      "\t[+] 138405 edges\n",
      "[+] Taking the LCC...ok\n",
      "\t[+] 13328 nodes\n",
      "\t[+] 138334 edges\n",
      "[+] Adding NeDBIT features...ok\n",
      "[+] Saving graph to path: Graphs/grafo_diamond_nedbit_C0023893.gml\n",
      "[i] Elapsed time: 1.106\n"
     ]
    }
   ],
   "source": [
    "for disease_Id in disease_Ids:\n",
    "    graph_path = create_graph_from_PPI('Datasets_v2/Diamond_dataset/diamond_ppi.txt', disease_Id, 'grafo_diamond_nedbit_' + disease_Id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Reading graph...ok\n",
      "[+] Creating dataset...ok\n",
      "[i] Elapsed time: 33.384\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Data(edge_index=[2, 1357864], x=[19761, 6], y=[19761], num_classes=5, train_mask=[19761], test_mask=[19761], val_mask=[19761])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = get_dataset_from_graph(PATH_TO_GRAPHS + 'grafo_nedbit_' + disease_Id + '.gml', disease_Id)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results without training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      1.00      0.69       112\n",
      "           1       0.41      0.89      0.56       713\n",
      "           2       0.43      0.39      0.41       713\n",
      "           3       1.00      0.00      0.01       713\n",
      "           4       0.75      0.58      0.66       713\n",
      "\n",
      "    accuracy                           0.49      2964\n",
      "   macro avg       0.62      0.57      0.46      2964\n",
      "weighted avg       0.64      0.49      0.42      2964\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 504x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 504x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "predict_from_saved_model('GraphSAGE_C0006142_40000_0_0005', dataset.to(device), ['P', 'LP', 'WN', 'LN', 'RN'], files_name='GraphSAGE_' + disease_Id + '_nonTrained')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results with training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "882b28039b9f4d388c85de5ddfb74b65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40001 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch: 0000] train loss: 51.6491, train acc: 0.1092, val loss: 17.1695, val acc: 0.0425  (best train acc: 0.1092, best val acc: 0.0425, best train loss: 51.6491  @ epoch 0 )\n",
      "[Epoch: 0020] train loss: 2.3694, train acc: 0.1708, val loss: 1.8692, val acc: 0.0975  (best train acc: 0.2640, best val acc: 0.2944, best train loss: 2.3694  @ epoch 20 )\n",
      "[Epoch: 0040] train loss: 1.5887, train acc: 0.2984, val loss: 1.4797, val acc: 0.3427  (best train acc: 0.3044, best val acc: 0.3427, best train loss: 1.5887  @ epoch 40 )\n",
      "[Epoch: 0060] train loss: 1.4484, train acc: 0.3560, val loss: 1.3877, val acc: 0.4118  (best train acc: 0.3560, best val acc: 0.4142, best train loss: 1.4475  @ epoch 58 )\n",
      "[Epoch: 0080] train loss: 1.3862, train acc: 0.3944, val loss: 1.3446, val acc: 0.4304  (best train acc: 0.3944, best val acc: 0.4304, best train loss: 1.3862  @ epoch 80 )\n",
      "[Epoch: 0100] train loss: 1.3894, train acc: 0.3679, val loss: 1.3182, val acc: 0.4546  (best train acc: 0.4069, best val acc: 0.4546, best train loss: 1.3606  @ epoch 95 )\n",
      "[Epoch: 0120] train loss: 1.3329, train acc: 0.4138, val loss: 1.2914, val acc: 0.4577  (best train acc: 0.4230, best val acc: 0.4644, best train loss: 1.3329  @ epoch 120 )\n",
      "[Epoch: 0140] train loss: 1.3247, train acc: 0.4163, val loss: 1.2701, val acc: 0.4722  (best train acc: 0.4312, best val acc: 0.4722, best train loss: 1.3169  @ epoch 139 )\n",
      "[Epoch: 0160] train loss: 1.2970, train acc: 0.4337, val loss: 1.2550, val acc: 0.4718  (best train acc: 0.4343, best val acc: 0.4742, best train loss: 1.2933  @ epoch 159 )\n",
      "[Epoch: 0180] train loss: 1.2824, train acc: 0.4480, val loss: 1.2322, val acc: 0.4847  (best train acc: 0.4480, best val acc: 0.4857, best train loss: 1.2691  @ epoch 177 )\n",
      "[Epoch: 0200] train loss: 1.2382, train acc: 0.4699, val loss: 1.2140, val acc: 0.4897  (best train acc: 0.4699, best val acc: 0.4944, best train loss: 1.2382  @ epoch 200 )\n",
      "[Epoch: 0220] train loss: 1.2349, train acc: 0.4689, val loss: 1.2017, val acc: 0.5039  (best train acc: 0.4723, best val acc: 0.5079, best train loss: 1.2349  @ epoch 220 )\n",
      "[Epoch: 0240] train loss: 1.2084, train acc: 0.4775, val loss: 1.1780, val acc: 0.5116  (best train acc: 0.4946, best val acc: 0.5170, best train loss: 1.2040  @ epoch 237 )\n",
      "[Epoch: 0260] train loss: 1.1948, train acc: 0.4709, val loss: 1.1634, val acc: 0.5143  (best train acc: 0.4985, best val acc: 0.5265, best train loss: 1.1732  @ epoch 254 )\n",
      "[Epoch: 0280] train loss: 1.1775, train acc: 0.4871, val loss: 1.1434, val acc: 0.5302  (best train acc: 0.5087, best val acc: 0.5386, best train loss: 1.1732  @ epoch 254 )\n",
      "[Epoch: 0300] train loss: 1.1551, train acc: 0.5100, val loss: 1.1334, val acc: 0.5288  (best train acc: 0.5168, best val acc: 0.5386, best train loss: 1.1426  @ epoch 291 )\n",
      "[Epoch: 0320] train loss: 1.2304, train acc: 0.4662, val loss: 1.1257, val acc: 0.5234  (best train acc: 0.5192, best val acc: 0.5386, best train loss: 1.1426  @ epoch 291 )\n",
      "[Epoch: 0340] train loss: 1.1531, train acc: 0.5056, val loss: 1.1108, val acc: 0.5383  (best train acc: 0.5252, best val acc: 0.5457, best train loss: 1.1318  @ epoch 336 )\n",
      "[Epoch: 0360] train loss: 1.1659, train acc: 0.4939, val loss: 1.1017, val acc: 0.5346  (best train acc: 0.5291, best val acc: 0.5464, best train loss: 1.1108  @ epoch 351 )\n",
      "[Epoch: 0380] train loss: 1.1120, train acc: 0.5313, val loss: 1.0839, val acc: 0.5541  (best train acc: 0.5354, best val acc: 0.5558, best train loss: 1.1072  @ epoch 362 )\n",
      "[Epoch: 0400] train loss: 1.1441, train acc: 0.4982, val loss: 1.0739, val acc: 0.5336  (best train acc: 0.5541, best val acc: 0.5656, best train loss: 1.0817  @ epoch 387 )\n",
      "[Epoch: 0420] train loss: 1.1578, train acc: 0.4952, val loss: 1.0446, val acc: 0.5710  (best train acc: 0.5541, best val acc: 0.5804, best train loss: 1.0660  @ epoch 412 )\n",
      "[Epoch: 0440] train loss: 1.0629, train acc: 0.5426, val loss: 1.0064, val acc: 0.5899  (best train acc: 0.5569, best val acc: 0.5902, best train loss: 1.0371  @ epoch 438 )\n",
      "[Epoch: 0460] train loss: 1.0118, train acc: 0.5588, val loss: 0.9815, val acc: 0.5987  (best train acc: 0.5594, best val acc: 0.6057, best train loss: 1.0118  @ epoch 460 )\n",
      "[Epoch: 0480] train loss: 1.0281, train acc: 0.5602, val loss: 0.9732, val acc: 0.6165  (best train acc: 0.5775, best val acc: 0.6246, best train loss: 0.9992  @ epoch 477 )\n",
      "[Epoch: 0500] train loss: 1.0091, train acc: 0.5433, val loss: 0.9658, val acc: 0.5686  (best train acc: 0.5784, best val acc: 0.6280, best train loss: 0.9912  @ epoch 488 )\n",
      "[Epoch: 0520] train loss: 0.9765, train acc: 0.5844, val loss: 0.9353, val acc: 0.6172  (best train acc: 0.5968, best val acc: 0.6344, best train loss: 0.9500  @ epoch 515 )\n",
      "[Epoch: 0540] train loss: 1.0361, train acc: 0.5531, val loss: 0.9130, val acc: 0.6078  (best train acc: 0.6061, best val acc: 0.6503, best train loss: 0.9378  @ epoch 529 )\n",
      "[Epoch: 0560] train loss: 1.0097, train acc: 0.5463, val loss: 0.9013, val acc: 0.6084  (best train acc: 0.6122, best val acc: 0.6556, best train loss: 0.9378  @ epoch 529 )\n",
      "[Epoch: 0580] train loss: 0.9413, train acc: 0.6105, val loss: 0.8833, val acc: 0.6273  (best train acc: 0.6149, best val acc: 0.6556, best train loss: 0.8982  @ epoch 568 )\n",
      "[Epoch: 0600] train loss: 0.8613, train acc: 0.6561, val loss: 0.8798, val acc: 0.6965  (best train acc: 0.6561, best val acc: 0.6965, best train loss: 0.8613  @ epoch 600 )\n",
      "[Epoch: 0620] train loss: 0.9000, train acc: 0.6341, val loss: 0.8617, val acc: 0.6361  (best train acc: 0.6561, best val acc: 0.6965, best train loss: 0.8613  @ epoch 600 )\n",
      "[Epoch: 0640] train loss: 0.8906, train acc: 0.6063, val loss: 0.8696, val acc: 0.5639  (best train acc: 0.6617, best val acc: 0.7019, best train loss: 0.8537  @ epoch 627 )\n",
      "[Epoch: 0660] train loss: 0.8847, train acc: 0.6348, val loss: 0.8071, val acc: 0.6961  (best train acc: 0.6617, best val acc: 0.7069, best train loss: 0.8516  @ epoch 649 )\n",
      "[Epoch: 0680] train loss: 0.8524, train acc: 0.6398, val loss: 0.7995, val acc: 0.6597  (best train acc: 0.6617, best val acc: 0.7069, best train loss: 0.8269  @ epoch 676 )\n",
      "[Epoch: 0700] train loss: 0.9591, train acc: 0.5568, val loss: 0.8241, val acc: 0.6013  (best train acc: 0.6617, best val acc: 0.7224, best train loss: 0.7991  @ epoch 695 )\n",
      "[Epoch: 0720] train loss: 0.8088, train acc: 0.6543, val loss: 0.7725, val acc: 0.7008  (best train acc: 0.6620, best val acc: 0.7224, best train loss: 0.7991  @ epoch 695 )\n",
      "[Epoch: 0740] train loss: 0.8173, train acc: 0.6576, val loss: 0.7670, val acc: 0.7032  (best train acc: 0.6648, best val acc: 0.7224, best train loss: 0.7991  @ epoch 695 )\n",
      "[Epoch: 0760] train loss: 0.8965, train acc: 0.6277, val loss: 0.7761, val acc: 0.6607  (best train acc: 0.6715, best val acc: 0.7261, best train loss: 0.7986  @ epoch 742 )\n",
      "[Epoch: 0780] train loss: 0.8152, train acc: 0.6442, val loss: 0.7500, val acc: 0.7015  (best train acc: 0.6715, best val acc: 0.7261, best train loss: 0.7812  @ epoch 772 )\n",
      "[Epoch: 0800] train loss: 0.8466, train acc: 0.6525, val loss: 0.7312, val acc: 0.7224  (best train acc: 0.6938, best val acc: 0.7349, best train loss: 0.7536  @ epoch 796 )\n",
      "[Epoch: 0820] train loss: 0.7918, train acc: 0.6692, val loss: 0.7372, val acc: 0.7137  (best train acc: 0.6938, best val acc: 0.7349, best train loss: 0.7536  @ epoch 796 )\n",
      "[Epoch: 0840] train loss: 0.8336, train acc: 0.6443, val loss: 0.7622, val acc: 0.6745  (best train acc: 0.6938, best val acc: 0.7349, best train loss: 0.7536  @ epoch 796 )\n",
      "[Epoch: 0860] train loss: 0.7940, train acc: 0.6476, val loss: 0.7148, val acc: 0.7157  (best train acc: 0.6938, best val acc: 0.7359, best train loss: 0.7374  @ epoch 853 )\n",
      "[Epoch: 0880] train loss: 0.8742, train acc: 0.6293, val loss: 0.7006, val acc: 0.7413  (best train acc: 0.6938, best val acc: 0.7555, best train loss: 0.7374  @ epoch 853 )\n",
      "[Epoch: 0900] train loss: 0.7690, train acc: 0.6667, val loss: 0.6868, val acc: 0.7417  (best train acc: 0.6938, best val acc: 0.7555, best train loss: 0.7374  @ epoch 853 )\n",
      "[Epoch: 0920] train loss: 0.7346, train acc: 0.6848, val loss: 0.6882, val acc: 0.7396  (best train acc: 0.6938, best val acc: 0.7555, best train loss: 0.7346  @ epoch 920 )\n",
      "[Epoch: 0940] train loss: 0.8024, train acc: 0.6617, val loss: 0.6707, val acc: 0.7272  (best train acc: 0.6998, best val acc: 0.7555, best train loss: 0.7267  @ epoch 927 )\n",
      "[Epoch: 0960] train loss: 0.7467, train acc: 0.6938, val loss: 0.6712, val acc: 0.7150  (best train acc: 0.7064, best val acc: 0.7555, best train loss: 0.7059  @ epoch 948 )\n",
      "[Epoch: 0980] train loss: 0.7524, train acc: 0.6886, val loss: 0.6454, val acc: 0.7592  (best train acc: 0.7076, best val acc: 0.7592, best train loss: 0.7059  @ epoch 948 )\n",
      "[Epoch: 1000] train loss: 0.7052, train acc: 0.7077, val loss: 0.6513, val acc: 0.7551  (best train acc: 0.7077, best val acc: 0.7599, best train loss: 0.7052  @ epoch 1000 )\n",
      "[Epoch: 1020] train loss: 0.7679, train acc: 0.6691, val loss: 0.6503, val acc: 0.7332  (best train acc: 0.7077, best val acc: 0.7599, best train loss: 0.7052  @ epoch 1000 )\n",
      "[Epoch: 1040] train loss: 0.7619, train acc: 0.6700, val loss: 0.6682, val acc: 0.7025  (best train acc: 0.7185, best val acc: 0.7680, best train loss: 0.6883  @ epoch 1028 )\n",
      "[Epoch: 1060] train loss: 0.7185, train acc: 0.6916, val loss: 0.6492, val acc: 0.7501  (best train acc: 0.7185, best val acc: 0.7683, best train loss: 0.6805  @ epoch 1046 )\n",
      "[Epoch: 1080] train loss: 0.7696, train acc: 0.6739, val loss: 0.6467, val acc: 0.7437  (best train acc: 0.7185, best val acc: 0.7683, best train loss: 0.6805  @ epoch 1046 )\n",
      "[Epoch: 1100] train loss: 0.6933, train acc: 0.7108, val loss: 0.6259, val acc: 0.7700  (best train acc: 0.7204, best val acc: 0.7700, best train loss: 0.6805  @ epoch 1046 )\n",
      "[Epoch: 1120] train loss: 0.8071, train acc: 0.6643, val loss: 0.6516, val acc: 0.7059  (best train acc: 0.7204, best val acc: 0.7710, best train loss: 0.6805  @ epoch 1046 )\n",
      "[Epoch: 1140] train loss: 0.7596, train acc: 0.6880, val loss: 0.6446, val acc: 0.7204  (best train acc: 0.7204, best val acc: 0.7710, best train loss: 0.6805  @ epoch 1046 )\n",
      "[Epoch: 1160] train loss: 0.7896, train acc: 0.6675, val loss: 0.6486, val acc: 0.7089  (best train acc: 0.7204, best val acc: 0.7710, best train loss: 0.6805  @ epoch 1046 )\n",
      "[Epoch: 1180] train loss: 0.7382, train acc: 0.7055, val loss: 0.6332, val acc: 0.7481  (best train acc: 0.7204, best val acc: 0.7767, best train loss: 0.6783  @ epoch 1163 )\n",
      "[Epoch: 1200] train loss: 0.6902, train acc: 0.7105, val loss: 0.6243, val acc: 0.7585  (best train acc: 0.7303, best val acc: 0.7767, best train loss: 0.6592  @ epoch 1195 )\n",
      "[Epoch: 1220] train loss: 0.7394, train acc: 0.6883, val loss: 0.6701, val acc: 0.6951  (best train acc: 0.7303, best val acc: 0.7794, best train loss: 0.6592  @ epoch 1195 )\n",
      "[Epoch: 1240] train loss: 0.8021, train acc: 0.6583, val loss: 0.6492, val acc: 0.7089  (best train acc: 0.7303, best val acc: 0.7794, best train loss: 0.6592  @ epoch 1195 )\n",
      "[Epoch: 1260] train loss: 0.7097, train acc: 0.6982, val loss: 0.6217, val acc: 0.7450  (best train acc: 0.7314, best val acc: 0.7794, best train loss: 0.6592  @ epoch 1195 )\n",
      "[Epoch: 1280] train loss: 0.7355, train acc: 0.6768, val loss: 0.6188, val acc: 0.7336  (best train acc: 0.7314, best val acc: 0.7794, best train loss: 0.6592  @ epoch 1195 )\n",
      "[Epoch: 1300] train loss: 0.7304, train acc: 0.7005, val loss: 0.6008, val acc: 0.7666  (best train acc: 0.7386, best val acc: 0.7794, best train loss: 0.6574  @ epoch 1288 )\n",
      "[Epoch: 1320] train loss: 0.7794, train acc: 0.6679, val loss: 0.6012, val acc: 0.7528  (best train acc: 0.7386, best val acc: 0.7794, best train loss: 0.6574  @ epoch 1288 )\n",
      "[Epoch: 1340] train loss: 0.6911, train acc: 0.7123, val loss: 0.5975, val acc: 0.7680  (best train acc: 0.7386, best val acc: 0.7825, best train loss: 0.6574  @ epoch 1288 )\n",
      "[Epoch: 1360] train loss: 0.6825, train acc: 0.7167, val loss: 0.6067, val acc: 0.7619  (best train acc: 0.7386, best val acc: 0.7825, best train loss: 0.6574  @ epoch 1288 )\n",
      "[Epoch: 1380] train loss: 0.6847, train acc: 0.7126, val loss: 0.6132, val acc: 0.7737  (best train acc: 0.7386, best val acc: 0.7825, best train loss: 0.6574  @ epoch 1288 )\n",
      "[Epoch: 1400] train loss: 0.7149, train acc: 0.6990, val loss: 0.6099, val acc: 0.7528  (best train acc: 0.7386, best val acc: 0.7841, best train loss: 0.6574  @ epoch 1288 )\n",
      "[Epoch: 1420] train loss: 0.7270, train acc: 0.6987, val loss: 0.6095, val acc: 0.7379  (best train acc: 0.7386, best val acc: 0.7841, best train loss: 0.6574  @ epoch 1288 )\n",
      "[Epoch: 1440] train loss: 0.6745, train acc: 0.7218, val loss: 0.6225, val acc: 0.7278  (best train acc: 0.7386, best val acc: 0.7841, best train loss: 0.6439  @ epoch 1437 )\n",
      "[Epoch: 1460] train loss: 0.7141, train acc: 0.6961, val loss: 0.6182, val acc: 0.7191  (best train acc: 0.7386, best val acc: 0.7841, best train loss: 0.6439  @ epoch 1437 )\n",
      "[Epoch: 1480] train loss: 0.6727, train acc: 0.7037, val loss: 0.6032, val acc: 0.7477  (best train acc: 0.7386, best val acc: 0.7868, best train loss: 0.6439  @ epoch 1437 )\n",
      "[Epoch: 1500] train loss: 0.7054, train acc: 0.7057, val loss: 0.6099, val acc: 0.7356  (best train acc: 0.7386, best val acc: 0.7868, best train loss: 0.6439  @ epoch 1437 )\n",
      "[Epoch: 1520] train loss: 0.6777, train acc: 0.7188, val loss: 0.5860, val acc: 0.7649  (best train acc: 0.7386, best val acc: 0.7868, best train loss: 0.6439  @ epoch 1437 )\n",
      "[Epoch: 1540] train loss: 0.6863, train acc: 0.7059, val loss: 0.5765, val acc: 0.7740  (best train acc: 0.7454, best val acc: 0.7868, best train loss: 0.6397  @ epoch 1523 )\n",
      "[Epoch: 1560] train loss: 0.6722, train acc: 0.7281, val loss: 0.5754, val acc: 0.7781  (best train acc: 0.7454, best val acc: 0.7868, best train loss: 0.6397  @ epoch 1523 )\n",
      "[Epoch: 1580] train loss: 0.6690, train acc: 0.7310, val loss: 0.5857, val acc: 0.7804  (best train acc: 0.7454, best val acc: 0.7868, best train loss: 0.6397  @ epoch 1523 )\n",
      "[Epoch: 1600] train loss: 0.6876, train acc: 0.7151, val loss: 0.5865, val acc: 0.7538  (best train acc: 0.7454, best val acc: 0.7868, best train loss: 0.6397  @ epoch 1523 )\n",
      "[Epoch: 1620] train loss: 0.7757, train acc: 0.6640, val loss: 0.6540, val acc: 0.6793  (best train acc: 0.7479, best val acc: 0.7868, best train loss: 0.6389  @ epoch 1619 )\n",
      "[Epoch: 1640] train loss: 0.7080, train acc: 0.7191, val loss: 0.5871, val acc: 0.7683  (best train acc: 0.7479, best val acc: 0.7868, best train loss: 0.6389  @ epoch 1619 )\n",
      "[Epoch: 1660] train loss: 0.6536, train acc: 0.7289, val loss: 0.5783, val acc: 0.7676  (best train acc: 0.7479, best val acc: 0.7882, best train loss: 0.6389  @ epoch 1619 )\n",
      "[Epoch: 1680] train loss: 0.7226, train acc: 0.6955, val loss: 0.5746, val acc: 0.7565  (best train acc: 0.7479, best val acc: 0.7882, best train loss: 0.6345  @ epoch 1663 )\n",
      "[Epoch: 1700] train loss: 0.6895, train acc: 0.7222, val loss: 0.5805, val acc: 0.7524  (best train acc: 0.7479, best val acc: 0.7882, best train loss: 0.6286  @ epoch 1693 )\n",
      "[Epoch: 1720] train loss: 0.6804, train acc: 0.7110, val loss: 0.5650, val acc: 0.7717  (best train acc: 0.7479, best val acc: 0.7882, best train loss: 0.6286  @ epoch 1693 )\n",
      "[Epoch: 1740] train loss: 0.6651, train acc: 0.7217, val loss: 0.5900, val acc: 0.7420  (best train acc: 0.7543, best val acc: 0.7882, best train loss: 0.6065  @ epoch 1737 )\n",
      "[Epoch: 1760] train loss: 0.6929, train acc: 0.6961, val loss: 0.6234, val acc: 0.6971  (best train acc: 0.7543, best val acc: 0.7906, best train loss: 0.6065  @ epoch 1737 )\n",
      "[Epoch: 1780] train loss: 0.6481, train acc: 0.7247, val loss: 0.5661, val acc: 0.7730  (best train acc: 0.7543, best val acc: 0.7916, best train loss: 0.6065  @ epoch 1737 )\n",
      "[Epoch: 1800] train loss: 0.6563, train acc: 0.7387, val loss: 0.5973, val acc: 0.7349  (best train acc: 0.7543, best val acc: 0.7916, best train loss: 0.6065  @ epoch 1737 )\n",
      "[Epoch: 1820] train loss: 0.6298, train acc: 0.7512, val loss: 0.5702, val acc: 0.7801  (best train acc: 0.7543, best val acc: 0.7916, best train loss: 0.6065  @ epoch 1737 )\n",
      "[Epoch: 1840] train loss: 0.6819, train acc: 0.7178, val loss: 0.6174, val acc: 0.7153  (best train acc: 0.7543, best val acc: 0.7916, best train loss: 0.6065  @ epoch 1737 )\n",
      "[Epoch: 1860] train loss: 0.6647, train acc: 0.7295, val loss: 0.5634, val acc: 0.7767  (best train acc: 0.7543, best val acc: 0.7916, best train loss: 0.6065  @ epoch 1737 )\n",
      "[Epoch: 1880] train loss: 0.6673, train acc: 0.7178, val loss: 0.5571, val acc: 0.7686  (best train acc: 0.7543, best val acc: 0.7916, best train loss: 0.6065  @ epoch 1737 )\n",
      "[Epoch: 1900] train loss: 0.7745, train acc: 0.6683, val loss: 0.6493, val acc: 0.6698  (best train acc: 0.7543, best val acc: 0.7916, best train loss: 0.6065  @ epoch 1737 )\n",
      "[Epoch: 1920] train loss: 0.6532, train acc: 0.7293, val loss: 0.5545, val acc: 0.7693  (best train acc: 0.7543, best val acc: 0.7916, best train loss: 0.6065  @ epoch 1737 )\n",
      "[Epoch: 1940] train loss: 0.6741, train acc: 0.7303, val loss: 0.5500, val acc: 0.7747  (best train acc: 0.7543, best val acc: 0.7916, best train loss: 0.6065  @ epoch 1737 )\n",
      "[Epoch: 1960] train loss: 0.6726, train acc: 0.7122, val loss: 0.5875, val acc: 0.7214  (best train acc: 0.7543, best val acc: 0.7916, best train loss: 0.6010  @ epoch 1953 )\n",
      "[Epoch: 1980] train loss: 0.7458, train acc: 0.6832, val loss: 0.5469, val acc: 0.7609  (best train acc: 0.7543, best val acc: 0.7943, best train loss: 0.6010  @ epoch 1953 )\n",
      "[Epoch: 2000] train loss: 0.6363, train acc: 0.7456, val loss: 0.5336, val acc: 0.7815  (best train acc: 0.7543, best val acc: 0.7943, best train loss: 0.6010  @ epoch 1953 )\n",
      "[Epoch: 2020] train loss: 0.6260, train acc: 0.7433, val loss: 0.5737, val acc: 0.7191  (best train acc: 0.7543, best val acc: 0.7970, best train loss: 0.6010  @ epoch 1953 )\n",
      "[Epoch: 2040] train loss: 0.6349, train acc: 0.7300, val loss: 0.5267, val acc: 0.7831  (best train acc: 0.7609, best val acc: 0.7970, best train loss: 0.6010  @ epoch 1953 )\n",
      "[Epoch: 2060] train loss: 0.6945, train acc: 0.7012, val loss: 0.5271, val acc: 0.7808  (best train acc: 0.7609, best val acc: 0.7970, best train loss: 0.5907  @ epoch 2045 )\n",
      "[Epoch: 2080] train loss: 0.6094, train acc: 0.7394, val loss: 0.5190, val acc: 0.7841  (best train acc: 0.7609, best val acc: 0.7970, best train loss: 0.5907  @ epoch 2045 )\n",
      "[Epoch: 2100] train loss: 0.6103, train acc: 0.7349, val loss: 0.5374, val acc: 0.7946  (best train acc: 0.7609, best val acc: 0.8010, best train loss: 0.5884  @ epoch 2095 )\n",
      "[Epoch: 2120] train loss: 0.6217, train acc: 0.7422, val loss: 0.5599, val acc: 0.7464  (best train acc: 0.7668, best val acc: 0.8010, best train loss: 0.5884  @ epoch 2095 )\n",
      "[Epoch: 2140] train loss: 0.6321, train acc: 0.7356, val loss: 0.5237, val acc: 0.8024  (best train acc: 0.7668, best val acc: 0.8024, best train loss: 0.5874  @ epoch 2126 )\n",
      "[Epoch: 2160] train loss: 0.6201, train acc: 0.7371, val loss: 0.5104, val acc: 0.7872  (best train acc: 0.7668, best val acc: 0.8024, best train loss: 0.5874  @ epoch 2126 )\n",
      "[Epoch: 2180] train loss: 0.5944, train acc: 0.7501, val loss: 0.5176, val acc: 0.7953  (best train acc: 0.7696, best val acc: 0.8051, best train loss: 0.5765  @ epoch 2170 )\n",
      "[Epoch: 2200] train loss: 0.6123, train acc: 0.7502, val loss: 0.5246, val acc: 0.7956  (best train acc: 0.7696, best val acc: 0.8108, best train loss: 0.5765  @ epoch 2170 )\n",
      "[Epoch: 2220] train loss: 0.6619, train acc: 0.7263, val loss: 0.5509, val acc: 0.7406  (best train acc: 0.7696, best val acc: 0.8108, best train loss: 0.5765  @ epoch 2170 )\n",
      "[Epoch: 2240] train loss: 0.6670, train acc: 0.7212, val loss: 0.5116, val acc: 0.7825  (best train acc: 0.7716, best val acc: 0.8108, best train loss: 0.5765  @ epoch 2231 )\n",
      "[Epoch: 2260] train loss: 0.6328, train acc: 0.7406, val loss: 0.5171, val acc: 0.7912  (best train acc: 0.7716, best val acc: 0.8108, best train loss: 0.5731  @ epoch 2241 )\n",
      "[Epoch: 2280] train loss: 0.6715, train acc: 0.7071, val loss: 0.5182, val acc: 0.7690  (best train acc: 0.7716, best val acc: 0.8108, best train loss: 0.5731  @ epoch 2241 )\n",
      "[Epoch: 2300] train loss: 0.6555, train acc: 0.7341, val loss: 0.5295, val acc: 0.7562  (best train acc: 0.7716, best val acc: 0.8108, best train loss: 0.5731  @ epoch 2241 )\n",
      "[Epoch: 2320] train loss: 0.6343, train acc: 0.7313, val loss: 0.5075, val acc: 0.7902  (best train acc: 0.7716, best val acc: 0.8108, best train loss: 0.5731  @ epoch 2241 )\n",
      "[Epoch: 2340] train loss: 0.6049, train acc: 0.7436, val loss: 0.5000, val acc: 0.7953  (best train acc: 0.7716, best val acc: 0.8108, best train loss: 0.5731  @ epoch 2241 )\n",
      "[Epoch: 2360] train loss: 0.6850, train acc: 0.7111, val loss: 0.5194, val acc: 0.7666  (best train acc: 0.7716, best val acc: 0.8108, best train loss: 0.5731  @ epoch 2241 )\n",
      "[Epoch: 2380] train loss: 0.6437, train acc: 0.7278, val loss: 0.5152, val acc: 0.7794  (best train acc: 0.7716, best val acc: 0.8108, best train loss: 0.5731  @ epoch 2241 )\n",
      "[Epoch: 2400] train loss: 0.6099, train acc: 0.7487, val loss: 0.5063, val acc: 0.8044  (best train acc: 0.7716, best val acc: 0.8108, best train loss: 0.5731  @ epoch 2241 )\n",
      "[Epoch: 2420] train loss: 0.6073, train acc: 0.7509, val loss: 0.5077, val acc: 0.7980  (best train acc: 0.7716, best val acc: 0.8108, best train loss: 0.5709  @ epoch 2412 )\n",
      "[Epoch: 2440] train loss: 0.6215, train acc: 0.7421, val loss: 0.5239, val acc: 0.7673  (best train acc: 0.7716, best val acc: 0.8108, best train loss: 0.5709  @ epoch 2412 )\n",
      "[Epoch: 2460] train loss: 0.6295, train acc: 0.7341, val loss: 0.5279, val acc: 0.7740  (best train acc: 0.7716, best val acc: 0.8108, best train loss: 0.5709  @ epoch 2412 )\n",
      "[Epoch: 2480] train loss: 0.6059, train acc: 0.7522, val loss: 0.4935, val acc: 0.7875  (best train acc: 0.7716, best val acc: 0.8108, best train loss: 0.5709  @ epoch 2412 )\n",
      "[Epoch: 2500] train loss: 0.5938, train acc: 0.7482, val loss: 0.4943, val acc: 0.7885  (best train acc: 0.7716, best val acc: 0.8108, best train loss: 0.5518  @ epoch 2481 )\n",
      "[Epoch: 2520] train loss: 0.6173, train acc: 0.7493, val loss: 0.5247, val acc: 0.7572  (best train acc: 0.7729, best val acc: 0.8108, best train loss: 0.5514  @ epoch 2519 )\n",
      "[Epoch: 2540] train loss: 0.5754, train acc: 0.7647, val loss: 0.5037, val acc: 0.7818  (best train acc: 0.7729, best val acc: 0.8108, best train loss: 0.5514  @ epoch 2519 )\n",
      "[Epoch: 2560] train loss: 0.6239, train acc: 0.7350, val loss: 0.5118, val acc: 0.7646  (best train acc: 0.7729, best val acc: 0.8108, best train loss: 0.5514  @ epoch 2519 )\n",
      "[Epoch: 2580] train loss: 0.6115, train acc: 0.7434, val loss: 0.4967, val acc: 0.7862  (best train acc: 0.7729, best val acc: 0.8108, best train loss: 0.5514  @ epoch 2519 )\n",
      "[Epoch: 2600] train loss: 0.6049, train acc: 0.7504, val loss: 0.5072, val acc: 0.7713  (best train acc: 0.7786, best val acc: 0.8108, best train loss: 0.5399  @ epoch 2596 )\n",
      "[Epoch: 2620] train loss: 0.6157, train acc: 0.7415, val loss: 0.4797, val acc: 0.7976  (best train acc: 0.7786, best val acc: 0.8108, best train loss: 0.5399  @ epoch 2596 )\n",
      "[Epoch: 2640] train loss: 0.6057, train acc: 0.7475, val loss: 0.4788, val acc: 0.8013  (best train acc: 0.7793, best val acc: 0.8108, best train loss: 0.5399  @ epoch 2596 )\n",
      "[Epoch: 2660] train loss: 0.5942, train acc: 0.7627, val loss: 0.5117, val acc: 0.7764  (best train acc: 0.7793, best val acc: 0.8108, best train loss: 0.5399  @ epoch 2596 )\n",
      "[Epoch: 2680] train loss: 0.5595, train acc: 0.7612, val loss: 0.4810, val acc: 0.7953  (best train acc: 0.7793, best val acc: 0.8108, best train loss: 0.5399  @ epoch 2596 )\n",
      "[Epoch: 2700] train loss: 0.5740, train acc: 0.7599, val loss: 0.4886, val acc: 0.7963  (best train acc: 0.7793, best val acc: 0.8108, best train loss: 0.5399  @ epoch 2596 )\n",
      "[Epoch: 2720] train loss: 0.5669, train acc: 0.7606, val loss: 0.4823, val acc: 0.7946  (best train acc: 0.7793, best val acc: 0.8108, best train loss: 0.5399  @ epoch 2596 )\n",
      "[Epoch: 2740] train loss: 0.5826, train acc: 0.7493, val loss: 0.5110, val acc: 0.7717  (best train acc: 0.7793, best val acc: 0.8108, best train loss: 0.5399  @ epoch 2596 )\n",
      "[Epoch: 2760] train loss: 0.5450, train acc: 0.7797, val loss: 0.4872, val acc: 0.8054  (best train acc: 0.7797, best val acc: 0.8108, best train loss: 0.5399  @ epoch 2596 )\n",
      "[Epoch: 2780] train loss: 0.5692, train acc: 0.7637, val loss: 0.5019, val acc: 0.7811  (best train acc: 0.7797, best val acc: 0.8108, best train loss: 0.5399  @ epoch 2596 )\n",
      "[Epoch: 2800] train loss: 0.5723, train acc: 0.7615, val loss: 0.4870, val acc: 0.7858  (best train acc: 0.7797, best val acc: 0.8108, best train loss: 0.5399  @ epoch 2596 )\n",
      "[Epoch: 2820] train loss: 0.5878, train acc: 0.7535, val loss: 0.4777, val acc: 0.7970  (best train acc: 0.7801, best val acc: 0.8108, best train loss: 0.5399  @ epoch 2596 )\n",
      "[Epoch: 2840] train loss: 0.6279, train acc: 0.7418, val loss: 0.4959, val acc: 0.7804  (best train acc: 0.7801, best val acc: 0.8108, best train loss: 0.5399  @ epoch 2596 )\n",
      "[Epoch: 2860] train loss: 0.5761, train acc: 0.7620, val loss: 0.4815, val acc: 0.7922  (best train acc: 0.7801, best val acc: 0.8108, best train loss: 0.5399  @ epoch 2596 )\n",
      "[Epoch: 2880] train loss: 0.5528, train acc: 0.7737, val loss: 0.4949, val acc: 0.7781  (best train acc: 0.7801, best val acc: 0.8108, best train loss: 0.5399  @ epoch 2596 )\n",
      "[Epoch: 2900] train loss: 0.5917, train acc: 0.7556, val loss: 0.4996, val acc: 0.7717  (best train acc: 0.7801, best val acc: 0.8108, best train loss: 0.5399  @ epoch 2596 )\n",
      "[Epoch: 2920] train loss: 0.5831, train acc: 0.7487, val loss: 0.4890, val acc: 0.7862  (best train acc: 0.7801, best val acc: 0.8108, best train loss: 0.5399  @ epoch 2596 )\n",
      "[Epoch: 2940] train loss: 0.5819, train acc: 0.7574, val loss: 0.4774, val acc: 0.8108  (best train acc: 0.7801, best val acc: 0.8138, best train loss: 0.5399  @ epoch 2596 )\n",
      "[Epoch: 2960] train loss: 0.5652, train acc: 0.7567, val loss: 0.4788, val acc: 0.7899  (best train acc: 0.7801, best val acc: 0.8138, best train loss: 0.5399  @ epoch 2596 )\n",
      "[Epoch: 2980] train loss: 0.5726, train acc: 0.7612, val loss: 0.4613, val acc: 0.8000  (best train acc: 0.7801, best val acc: 0.8138, best train loss: 0.5399  @ epoch 2596 )\n",
      "[Epoch: 3000] train loss: 0.5914, train acc: 0.7541, val loss: 0.4685, val acc: 0.7943  (best train acc: 0.7804, best val acc: 0.8138, best train loss: 0.5399  @ epoch 2596 )\n",
      "[Epoch: 3020] train loss: 0.5968, train acc: 0.7480, val loss: 0.4647, val acc: 0.8108  (best train acc: 0.7855, best val acc: 0.8138, best train loss: 0.5370  @ epoch 3005 )\n",
      "[Epoch: 3040] train loss: 0.6297, train acc: 0.7274, val loss: 0.4937, val acc: 0.7811  (best train acc: 0.7855, best val acc: 0.8138, best train loss: 0.5370  @ epoch 3005 )\n",
      "[Epoch: 3060] train loss: 0.5600, train acc: 0.7765, val loss: 0.4663, val acc: 0.7997  (best train acc: 0.7855, best val acc: 0.8172, best train loss: 0.5360  @ epoch 3053 )\n",
      "[Epoch: 3080] train loss: 0.5328, train acc: 0.7772, val loss: 0.4715, val acc: 0.8135  (best train acc: 0.7855, best val acc: 0.8172, best train loss: 0.5328  @ epoch 3080 )\n",
      "[Epoch: 3100] train loss: 0.5544, train acc: 0.7671, val loss: 0.4774, val acc: 0.7943  (best train acc: 0.7855, best val acc: 0.8172, best train loss: 0.5244  @ epoch 3088 )\n",
      "[Epoch: 3120] train loss: 0.6068, train acc: 0.7377, val loss: 0.4696, val acc: 0.8064  (best train acc: 0.7855, best val acc: 0.8172, best train loss: 0.5244  @ epoch 3088 )\n",
      "[Epoch: 3140] train loss: 0.6684, train acc: 0.7139, val loss: 0.4927, val acc: 0.7754  (best train acc: 0.7855, best val acc: 0.8172, best train loss: 0.5206  @ epoch 3136 )\n",
      "[Epoch: 3160] train loss: 0.5837, train acc: 0.7563, val loss: 0.5129, val acc: 0.7578  (best train acc: 0.7855, best val acc: 0.8172, best train loss: 0.5206  @ epoch 3136 )\n",
      "[Epoch: 3180] train loss: 0.5204, train acc: 0.7903, val loss: 0.4714, val acc: 0.7966  (best train acc: 0.7903, best val acc: 0.8175, best train loss: 0.5204  @ epoch 3180 )\n",
      "[Epoch: 3200] train loss: 0.5342, train acc: 0.7749, val loss: 0.5214, val acc: 0.7555  (best train acc: 0.7903, best val acc: 0.8175, best train loss: 0.5204  @ epoch 3180 )\n",
      "[Epoch: 3220] train loss: 0.5909, train acc: 0.7402, val loss: 0.4752, val acc: 0.7926  (best train acc: 0.7903, best val acc: 0.8202, best train loss: 0.5204  @ epoch 3180 )\n",
      "[Epoch: 3240] train loss: 0.5699, train acc: 0.7563, val loss: 0.4734, val acc: 0.7899  (best train acc: 0.7903, best val acc: 0.8202, best train loss: 0.5204  @ epoch 3180 )\n",
      "[Epoch: 3260] train loss: 0.5399, train acc: 0.7689, val loss: 0.4620, val acc: 0.8125  (best train acc: 0.7903, best val acc: 0.8202, best train loss: 0.5204  @ epoch 3180 )\n",
      "[Epoch: 3280] train loss: 0.5789, train acc: 0.7580, val loss: 0.4873, val acc: 0.7804  (best train acc: 0.7903, best val acc: 0.8202, best train loss: 0.5115  @ epoch 3270 )\n",
      "[Epoch: 3300] train loss: 0.5251, train acc: 0.7832, val loss: 0.4763, val acc: 0.7953  (best train acc: 0.7903, best val acc: 0.8202, best train loss: 0.5115  @ epoch 3270 )\n",
      "[Epoch: 3320] train loss: 0.5694, train acc: 0.7622, val loss: 0.4505, val acc: 0.8105  (best train acc: 0.7903, best val acc: 0.8202, best train loss: 0.5115  @ epoch 3270 )\n",
      "[Epoch: 3340] train loss: 0.5625, train acc: 0.7632, val loss: 0.4480, val acc: 0.8152  (best train acc: 0.7903, best val acc: 0.8202, best train loss: 0.5115  @ epoch 3270 )\n",
      "[Epoch: 3360] train loss: 0.5865, train acc: 0.7397, val loss: 0.5025, val acc: 0.7680  (best train acc: 0.7903, best val acc: 0.8202, best train loss: 0.5115  @ epoch 3270 )\n",
      "[Epoch: 3380] train loss: 0.5974, train acc: 0.7415, val loss: 0.4509, val acc: 0.8081  (best train acc: 0.7903, best val acc: 0.8202, best train loss: 0.5115  @ epoch 3270 )\n",
      "[Epoch: 3400] train loss: 0.5541, train acc: 0.7714, val loss: 0.4633, val acc: 0.8209  (best train acc: 0.7903, best val acc: 0.8236, best train loss: 0.5115  @ epoch 3270 )\n",
      "[Epoch: 3420] train loss: 0.5360, train acc: 0.7766, val loss: 0.4535, val acc: 0.8064  (best train acc: 0.7903, best val acc: 0.8236, best train loss: 0.5115  @ epoch 3270 )\n",
      "[Epoch: 3440] train loss: 0.5686, train acc: 0.7621, val loss: 0.4573, val acc: 0.8051  (best train acc: 0.7903, best val acc: 0.8253, best train loss: 0.5115  @ epoch 3270 )\n",
      "[Epoch: 3460] train loss: 0.5749, train acc: 0.7568, val loss: 0.4671, val acc: 0.7993  (best train acc: 0.7903, best val acc: 0.8266, best train loss: 0.5115  @ epoch 3270 )\n",
      "[Epoch: 3480] train loss: 0.5863, train acc: 0.7539, val loss: 0.4645, val acc: 0.7987  (best train acc: 0.7976, best val acc: 0.8266, best train loss: 0.5052  @ epoch 3461 )\n",
      "[Epoch: 3500] train loss: 0.5659, train acc: 0.7735, val loss: 0.4533, val acc: 0.8182  (best train acc: 0.7976, best val acc: 0.8266, best train loss: 0.5052  @ epoch 3461 )\n",
      "[Epoch: 3520] train loss: 0.5304, train acc: 0.7741, val loss: 0.4660, val acc: 0.7929  (best train acc: 0.7976, best val acc: 0.8266, best train loss: 0.5052  @ epoch 3461 )\n",
      "[Epoch: 3540] train loss: 0.5195, train acc: 0.7867, val loss: 0.4481, val acc: 0.8216  (best train acc: 0.7976, best val acc: 0.8266, best train loss: 0.5052  @ epoch 3461 )\n",
      "[Epoch: 3560] train loss: 0.5378, train acc: 0.7820, val loss: 0.4672, val acc: 0.8040  (best train acc: 0.7976, best val acc: 0.8266, best train loss: 0.5052  @ epoch 3461 )\n",
      "[Epoch: 3580] train loss: 0.5715, train acc: 0.7615, val loss: 0.4472, val acc: 0.8175  (best train acc: 0.7976, best val acc: 0.8270, best train loss: 0.5052  @ epoch 3461 )\n",
      "[Epoch: 3600] train loss: 0.5169, train acc: 0.7789, val loss: 0.4418, val acc: 0.8138  (best train acc: 0.7976, best val acc: 0.8270, best train loss: 0.5052  @ epoch 3461 )\n",
      "[Epoch: 3620] train loss: 0.5311, train acc: 0.7707, val loss: 0.4542, val acc: 0.8027  (best train acc: 0.7976, best val acc: 0.8270, best train loss: 0.5052  @ epoch 3461 )\n",
      "[Epoch: 3640] train loss: 0.5379, train acc: 0.7774, val loss: 0.4734, val acc: 0.7926  (best train acc: 0.7976, best val acc: 0.8270, best train loss: 0.5052  @ epoch 3461 )\n",
      "[Epoch: 3660] train loss: 0.5134, train acc: 0.7874, val loss: 0.4572, val acc: 0.8125  (best train acc: 0.7976, best val acc: 0.8270, best train loss: 0.5052  @ epoch 3461 )\n",
      "[Epoch: 3680] train loss: 0.5700, train acc: 0.7600, val loss: 0.4363, val acc: 0.8165  (best train acc: 0.7976, best val acc: 0.8270, best train loss: 0.5052  @ epoch 3461 )\n",
      "[Epoch: 3700] train loss: 0.5410, train acc: 0.7782, val loss: 0.4432, val acc: 0.8185  (best train acc: 0.7976, best val acc: 0.8270, best train loss: 0.5036  @ epoch 3681 )\n",
      "[Epoch: 3720] train loss: 0.5870, train acc: 0.7538, val loss: 0.4540, val acc: 0.8003  (best train acc: 0.7976, best val acc: 0.8270, best train loss: 0.5036  @ epoch 3681 )\n",
      "[Epoch: 3740] train loss: 0.5693, train acc: 0.7571, val loss: 0.4502, val acc: 0.8071  (best train acc: 0.7976, best val acc: 0.8270, best train loss: 0.5036  @ epoch 3681 )\n",
      "[Epoch: 3760] train loss: 0.5442, train acc: 0.7632, val loss: 0.4685, val acc: 0.7909  (best train acc: 0.7984, best val acc: 0.8270, best train loss: 0.4905  @ epoch 3756 )\n",
      "[Epoch: 3780] train loss: 0.5311, train acc: 0.7846, val loss: 0.4476, val acc: 0.8108  (best train acc: 0.7984, best val acc: 0.8270, best train loss: 0.4905  @ epoch 3756 )\n",
      "[Epoch: 3800] train loss: 0.5110, train acc: 0.7884, val loss: 0.4576, val acc: 0.7943  (best train acc: 0.8004, best val acc: 0.8270, best train loss: 0.4905  @ epoch 3756 )\n",
      "[Epoch: 3820] train loss: 0.5224, train acc: 0.7817, val loss: 0.4409, val acc: 0.8152  (best train acc: 0.8004, best val acc: 0.8270, best train loss: 0.4905  @ epoch 3756 )\n",
      "[Epoch: 3840] train loss: 0.5567, train acc: 0.7629, val loss: 0.4576, val acc: 0.7933  (best train acc: 0.8004, best val acc: 0.8277, best train loss: 0.4905  @ epoch 3756 )\n",
      "[Epoch: 3860] train loss: 0.5386, train acc: 0.7624, val loss: 0.4444, val acc: 0.8169  (best train acc: 0.8004, best val acc: 0.8277, best train loss: 0.4905  @ epoch 3756 )\n",
      "[Epoch: 3880] train loss: 0.5502, train acc: 0.7615, val loss: 0.4684, val acc: 0.7902  (best train acc: 0.8004, best val acc: 0.8283, best train loss: 0.4905  @ epoch 3756 )\n",
      "[Epoch: 3900] train loss: 0.5317, train acc: 0.7787, val loss: 0.4687, val acc: 0.7946  (best train acc: 0.8004, best val acc: 0.8283, best train loss: 0.4905  @ epoch 3756 )\n",
      "[Epoch: 3920] train loss: 0.5003, train acc: 0.7920, val loss: 0.4388, val acc: 0.8229  (best train acc: 0.8004, best val acc: 0.8293, best train loss: 0.4905  @ epoch 3756 )\n",
      "[Epoch: 3940] train loss: 0.5328, train acc: 0.7715, val loss: 0.4325, val acc: 0.8266  (best train acc: 0.8004, best val acc: 0.8293, best train loss: 0.4905  @ epoch 3756 )\n",
      "[Epoch: 3960] train loss: 0.5394, train acc: 0.7726, val loss: 0.4477, val acc: 0.8223  (best train acc: 0.8004, best val acc: 0.8293, best train loss: 0.4905  @ epoch 3756 )\n",
      "[Epoch: 3980] train loss: 0.5859, train acc: 0.7452, val loss: 0.4423, val acc: 0.8121  (best train acc: 0.8004, best val acc: 0.8293, best train loss: 0.4905  @ epoch 3756 )\n",
      "[Epoch: 4000] train loss: 0.5829, train acc: 0.7498, val loss: 0.4885, val acc: 0.7767  (best train acc: 0.8004, best val acc: 0.8293, best train loss: 0.4905  @ epoch 3756 )\n",
      "[Epoch: 4020] train loss: 0.6084, train acc: 0.7419, val loss: 0.4435, val acc: 0.8209  (best train acc: 0.8004, best val acc: 0.8300, best train loss: 0.4905  @ epoch 3756 )\n",
      "[Epoch: 4040] train loss: 0.5651, train acc: 0.7489, val loss: 0.4807, val acc: 0.7899  (best train acc: 0.8004, best val acc: 0.8300, best train loss: 0.4905  @ epoch 3756 )\n",
      "[Epoch: 4060] train loss: 0.5849, train acc: 0.7470, val loss: 0.4467, val acc: 0.8287  (best train acc: 0.8004, best val acc: 0.8300, best train loss: 0.4905  @ epoch 3756 )\n",
      "[Epoch: 4080] train loss: 0.5547, train acc: 0.7530, val loss: 0.4430, val acc: 0.8064  (best train acc: 0.8004, best val acc: 0.8300, best train loss: 0.4905  @ epoch 3756 )\n",
      "[Epoch: 4100] train loss: 0.5427, train acc: 0.7792, val loss: 0.4439, val acc: 0.8236  (best train acc: 0.8004, best val acc: 0.8304, best train loss: 0.4905  @ epoch 3756 )\n",
      "[Epoch: 4120] train loss: 0.5351, train acc: 0.7684, val loss: 0.4659, val acc: 0.7895  (best train acc: 0.8004, best val acc: 0.8344, best train loss: 0.4905  @ epoch 3756 )\n",
      "[Epoch: 4140] train loss: 0.5225, train acc: 0.7780, val loss: 0.4640, val acc: 0.7980  (best train acc: 0.8016, best val acc: 0.8344, best train loss: 0.4817  @ epoch 4139 )\n",
      "[Epoch: 4160] train loss: 0.5492, train acc: 0.7760, val loss: 0.4307, val acc: 0.8121  (best train acc: 0.8035, best val acc: 0.8344, best train loss: 0.4817  @ epoch 4139 )\n",
      "[Epoch: 4180] train loss: 0.5401, train acc: 0.7766, val loss: 0.4935, val acc: 0.7727  (best train acc: 0.8035, best val acc: 0.8344, best train loss: 0.4817  @ epoch 4139 )\n",
      "[Epoch: 4200] train loss: 0.4938, train acc: 0.7979, val loss: 0.4330, val acc: 0.8152  (best train acc: 0.8035, best val acc: 0.8344, best train loss: 0.4817  @ epoch 4139 )\n",
      "[Epoch: 4220] train loss: 0.4780, train acc: 0.8005, val loss: 0.4219, val acc: 0.8216  (best train acc: 0.8035, best val acc: 0.8344, best train loss: 0.4780  @ epoch 4220 )\n",
      "[Epoch: 4240] train loss: 0.5104, train acc: 0.7800, val loss: 0.4260, val acc: 0.8206  (best train acc: 0.8044, best val acc: 0.8344, best train loss: 0.4780  @ epoch 4220 )\n",
      "[Epoch: 4260] train loss: 0.4902, train acc: 0.7974, val loss: 0.4219, val acc: 0.8256  (best train acc: 0.8044, best val acc: 0.8344, best train loss: 0.4763  @ epoch 4248 )\n",
      "[Epoch: 4280] train loss: 0.4995, train acc: 0.7891, val loss: 0.4244, val acc: 0.8206  (best train acc: 0.8044, best val acc: 0.8344, best train loss: 0.4763  @ epoch 4248 )\n",
      "[Epoch: 4300] train loss: 0.5342, train acc: 0.7815, val loss: 0.4522, val acc: 0.8013  (best train acc: 0.8044, best val acc: 0.8344, best train loss: 0.4763  @ epoch 4248 )\n",
      "[Epoch: 4320] train loss: 0.5108, train acc: 0.7896, val loss: 0.4449, val acc: 0.7970  (best train acc: 0.8105, best val acc: 0.8344, best train loss: 0.4732  @ epoch 4304 )\n",
      "[Epoch: 4340] train loss: 0.5302, train acc: 0.7782, val loss: 0.4975, val acc: 0.7619  (best train acc: 0.8105, best val acc: 0.8344, best train loss: 0.4705  @ epoch 4327 )\n",
      "[Epoch: 4360] train loss: 0.5334, train acc: 0.7674, val loss: 0.4311, val acc: 0.8196  (best train acc: 0.8105, best val acc: 0.8344, best train loss: 0.4705  @ epoch 4327 )\n",
      "[Epoch: 4380] train loss: 0.4819, train acc: 0.8002, val loss: 0.4200, val acc: 0.8277  (best train acc: 0.8105, best val acc: 0.8344, best train loss: 0.4648  @ epoch 4368 )\n",
      "[Epoch: 4400] train loss: 0.5427, train acc: 0.7710, val loss: 0.4586, val acc: 0.7906  (best train acc: 0.8177, best val acc: 0.8344, best train loss: 0.4636  @ epoch 4381 )\n",
      "[Epoch: 4420] train loss: 0.5838, train acc: 0.7556, val loss: 0.4517, val acc: 0.8000  (best train acc: 0.8177, best val acc: 0.8344, best train loss: 0.4636  @ epoch 4381 )\n",
      "[Epoch: 4440] train loss: 0.4876, train acc: 0.7952, val loss: 0.4186, val acc: 0.8270  (best train acc: 0.8177, best val acc: 0.8344, best train loss: 0.4589  @ epoch 4439 )\n",
      "[Epoch: 4460] train loss: 0.5335, train acc: 0.7784, val loss: 0.4620, val acc: 0.7804  (best train acc: 0.8177, best val acc: 0.8344, best train loss: 0.4589  @ epoch 4439 )\n",
      "[Epoch: 4480] train loss: 0.5043, train acc: 0.7864, val loss: 0.4949, val acc: 0.7599  (best train acc: 0.8177, best val acc: 0.8344, best train loss: 0.4589  @ epoch 4439 )\n",
      "[Epoch: 4500] train loss: 0.4902, train acc: 0.7854, val loss: 0.4260, val acc: 0.8179  (best train acc: 0.8177, best val acc: 0.8354, best train loss: 0.4589  @ epoch 4439 )\n",
      "[Epoch: 4520] train loss: 0.4962, train acc: 0.7914, val loss: 0.4800, val acc: 0.7771  (best train acc: 0.8177, best val acc: 0.8354, best train loss: 0.4589  @ epoch 4439 )\n",
      "[Epoch: 4540] train loss: 0.4811, train acc: 0.8010, val loss: 0.4202, val acc: 0.8324  (best train acc: 0.8177, best val acc: 0.8354, best train loss: 0.4589  @ epoch 4439 )\n",
      "[Epoch: 4560] train loss: 0.4851, train acc: 0.8004, val loss: 0.4292, val acc: 0.8216  (best train acc: 0.8177, best val acc: 0.8354, best train loss: 0.4589  @ epoch 4439 )\n",
      "[Epoch: 4580] train loss: 0.4760, train acc: 0.7938, val loss: 0.4504, val acc: 0.7960  (best train acc: 0.8177, best val acc: 0.8354, best train loss: 0.4589  @ epoch 4439 )\n",
      "[Epoch: 4600] train loss: 0.4885, train acc: 0.7934, val loss: 0.4118, val acc: 0.8277  (best train acc: 0.8177, best val acc: 0.8354, best train loss: 0.4589  @ epoch 4439 )\n",
      "[Epoch: 4620] train loss: 0.4938, train acc: 0.7976, val loss: 0.4166, val acc: 0.8290  (best train acc: 0.8177, best val acc: 0.8354, best train loss: 0.4578  @ epoch 4601 )\n",
      "[Epoch: 4640] train loss: 0.4941, train acc: 0.7888, val loss: 0.4194, val acc: 0.8219  (best train acc: 0.8177, best val acc: 0.8354, best train loss: 0.4578  @ epoch 4601 )\n",
      "[Epoch: 4660] train loss: 0.5158, train acc: 0.7872, val loss: 0.4311, val acc: 0.8121  (best train acc: 0.8177, best val acc: 0.8354, best train loss: 0.4578  @ epoch 4601 )\n",
      "[Epoch: 4680] train loss: 0.4893, train acc: 0.8003, val loss: 0.4124, val acc: 0.8283  (best train acc: 0.8177, best val acc: 0.8354, best train loss: 0.4578  @ epoch 4601 )\n",
      "[Epoch: 4700] train loss: 0.5284, train acc: 0.7703, val loss: 0.4493, val acc: 0.7960  (best train acc: 0.8177, best val acc: 0.8354, best train loss: 0.4578  @ epoch 4601 )\n",
      "[Epoch: 4720] train loss: 0.5102, train acc: 0.7891, val loss: 0.4266, val acc: 0.8108  (best train acc: 0.8177, best val acc: 0.8354, best train loss: 0.4578  @ epoch 4601 )\n",
      "[Epoch: 4740] train loss: 0.5258, train acc: 0.7762, val loss: 0.4165, val acc: 0.8253  (best train acc: 0.8177, best val acc: 0.8358, best train loss: 0.4578  @ epoch 4601 )\n",
      "[Epoch: 4760] train loss: 0.4750, train acc: 0.7956, val loss: 0.4141, val acc: 0.8270  (best train acc: 0.8177, best val acc: 0.8358, best train loss: 0.4578  @ epoch 4601 )\n",
      "[Epoch: 4780] train loss: 0.4986, train acc: 0.7932, val loss: 0.4118, val acc: 0.8226  (best train acc: 0.8177, best val acc: 0.8358, best train loss: 0.4501  @ epoch 4762 )\n",
      "[Epoch: 4800] train loss: 0.5078, train acc: 0.7866, val loss: 0.4376, val acc: 0.8057  (best train acc: 0.8177, best val acc: 0.8358, best train loss: 0.4486  @ epoch 4783 )\n",
      "[Epoch: 4820] train loss: 0.4865, train acc: 0.8058, val loss: 0.4123, val acc: 0.8331  (best train acc: 0.8177, best val acc: 0.8358, best train loss: 0.4486  @ epoch 4783 )\n",
      "[Epoch: 4840] train loss: 0.5110, train acc: 0.7806, val loss: 0.4573, val acc: 0.7956  (best train acc: 0.8177, best val acc: 0.8358, best train loss: 0.4486  @ epoch 4783 )\n",
      "[Epoch: 4860] train loss: 0.4832, train acc: 0.8011, val loss: 0.4150, val acc: 0.8226  (best train acc: 0.8177, best val acc: 0.8358, best train loss: 0.4486  @ epoch 4783 )\n",
      "[Epoch: 4880] train loss: 0.5208, train acc: 0.7815, val loss: 0.4271, val acc: 0.8145  (best train acc: 0.8232, best val acc: 0.8358, best train loss: 0.4389  @ epoch 4878 )\n",
      "[Epoch: 4900] train loss: 0.5025, train acc: 0.7843, val loss: 0.4426, val acc: 0.8094  (best train acc: 0.8232, best val acc: 0.8358, best train loss: 0.4389  @ epoch 4878 )\n",
      "[Epoch: 4920] train loss: 0.5086, train acc: 0.7885, val loss: 0.4326, val acc: 0.8057  (best train acc: 0.8232, best val acc: 0.8358, best train loss: 0.4389  @ epoch 4878 )\n",
      "[Epoch: 4940] train loss: 0.4500, train acc: 0.8100, val loss: 0.4022, val acc: 0.8280  (best train acc: 0.8232, best val acc: 0.8358, best train loss: 0.4389  @ epoch 4878 )\n",
      "[Epoch: 4960] train loss: 0.4982, train acc: 0.7967, val loss: 0.4107, val acc: 0.8253  (best train acc: 0.8232, best val acc: 0.8358, best train loss: 0.4389  @ epoch 4878 )\n",
      "[Epoch: 4980] train loss: 0.5066, train acc: 0.7775, val loss: 0.4355, val acc: 0.8047  (best train acc: 0.8232, best val acc: 0.8358, best train loss: 0.4389  @ epoch 4878 )\n",
      "[Epoch: 5000] train loss: 0.4983, train acc: 0.7903, val loss: 0.4192, val acc: 0.8182  (best train acc: 0.8232, best val acc: 0.8358, best train loss: 0.4389  @ epoch 4878 )\n",
      "[Epoch: 5020] train loss: 0.5242, train acc: 0.7855, val loss: 0.4249, val acc: 0.8185  (best train acc: 0.8232, best val acc: 0.8381, best train loss: 0.4389  @ epoch 4878 )\n",
      "[Epoch: 5040] train loss: 0.5372, train acc: 0.7795, val loss: 0.4474, val acc: 0.8024  (best train acc: 0.8232, best val acc: 0.8381, best train loss: 0.4389  @ epoch 4878 )\n",
      "[Epoch: 5060] train loss: 0.4960, train acc: 0.7987, val loss: 0.3985, val acc: 0.8287  (best train acc: 0.8232, best val acc: 0.8381, best train loss: 0.4389  @ epoch 4878 )\n",
      "[Epoch: 5080] train loss: 0.4934, train acc: 0.7916, val loss: 0.3998, val acc: 0.8317  (best train acc: 0.8232, best val acc: 0.8381, best train loss: 0.4389  @ epoch 4878 )\n",
      "[Epoch: 5100] train loss: 0.5147, train acc: 0.7851, val loss: 0.4327, val acc: 0.8159  (best train acc: 0.8232, best val acc: 0.8381, best train loss: 0.4389  @ epoch 4878 )\n",
      "[Epoch: 5120] train loss: 0.4811, train acc: 0.7964, val loss: 0.4200, val acc: 0.8179  (best train acc: 0.8232, best val acc: 0.8381, best train loss: 0.4389  @ epoch 4878 )\n",
      "[Epoch: 5140] train loss: 0.4826, train acc: 0.7969, val loss: 0.4043, val acc: 0.8273  (best train acc: 0.8232, best val acc: 0.8381, best train loss: 0.4389  @ epoch 4878 )\n",
      "[Epoch: 5160] train loss: 0.4749, train acc: 0.7895, val loss: 0.4066, val acc: 0.8297  (best train acc: 0.8232, best val acc: 0.8398, best train loss: 0.4389  @ epoch 4878 )\n",
      "[Epoch: 5180] train loss: 0.5184, train acc: 0.7833, val loss: 0.4447, val acc: 0.8061  (best train acc: 0.8232, best val acc: 0.8398, best train loss: 0.4389  @ epoch 4878 )\n",
      "[Epoch: 5200] train loss: 0.4930, train acc: 0.7957, val loss: 0.4091, val acc: 0.8253  (best train acc: 0.8232, best val acc: 0.8398, best train loss: 0.4389  @ epoch 4878 )\n",
      "[Epoch: 5220] train loss: 0.4896, train acc: 0.8018, val loss: 0.3976, val acc: 0.8361  (best train acc: 0.8232, best val acc: 0.8398, best train loss: 0.4389  @ epoch 4878 )\n",
      "[Epoch: 5240] train loss: 0.4754, train acc: 0.7979, val loss: 0.4282, val acc: 0.8105  (best train acc: 0.8232, best val acc: 0.8398, best train loss: 0.4389  @ epoch 4878 )\n",
      "[Epoch: 5260] train loss: 0.5051, train acc: 0.7982, val loss: 0.4320, val acc: 0.8159  (best train acc: 0.8232, best val acc: 0.8398, best train loss: 0.4389  @ epoch 4878 )\n",
      "[Epoch: 5280] train loss: 0.4616, train acc: 0.8078, val loss: 0.3946, val acc: 0.8358  (best train acc: 0.8232, best val acc: 0.8398, best train loss: 0.4389  @ epoch 4878 )\n",
      "[Epoch: 5300] train loss: 0.5175, train acc: 0.7825, val loss: 0.4717, val acc: 0.7895  (best train acc: 0.8232, best val acc: 0.8398, best train loss: 0.4389  @ epoch 4878 )\n",
      "[Epoch: 5320] train loss: 0.4731, train acc: 0.8041, val loss: 0.4068, val acc: 0.8287  (best train acc: 0.8232, best val acc: 0.8398, best train loss: 0.4389  @ epoch 4878 )\n",
      "[Epoch: 5340] train loss: 0.5094, train acc: 0.7863, val loss: 0.4089, val acc: 0.8287  (best train acc: 0.8232, best val acc: 0.8398, best train loss: 0.4389  @ epoch 4878 )\n",
      "[Epoch: 5360] train loss: 0.5042, train acc: 0.7939, val loss: 0.4099, val acc: 0.8300  (best train acc: 0.8232, best val acc: 0.8398, best train loss: 0.4389  @ epoch 4878 )\n",
      "[Epoch: 5380] train loss: 0.4586, train acc: 0.8094, val loss: 0.4111, val acc: 0.8273  (best train acc: 0.8232, best val acc: 0.8449, best train loss: 0.4389  @ epoch 4878 )\n",
      "[Epoch: 5400] train loss: 0.4995, train acc: 0.7870, val loss: 0.4142, val acc: 0.8212  (best train acc: 0.8232, best val acc: 0.8449, best train loss: 0.4389  @ epoch 4878 )\n",
      "[Epoch: 5420] train loss: 0.4617, train acc: 0.8094, val loss: 0.4092, val acc: 0.8219  (best train acc: 0.8232, best val acc: 0.8449, best train loss: 0.4389  @ epoch 4878 )\n",
      "[Epoch: 5440] train loss: 0.4822, train acc: 0.8013, val loss: 0.4013, val acc: 0.8354  (best train acc: 0.8232, best val acc: 0.8449, best train loss: 0.4389  @ epoch 4878 )\n",
      "[Epoch: 5460] train loss: 0.4599, train acc: 0.8092, val loss: 0.4178, val acc: 0.8196  (best train acc: 0.8232, best val acc: 0.8449, best train loss: 0.4389  @ epoch 4878 )\n",
      "[Epoch: 5480] train loss: 0.4927, train acc: 0.7963, val loss: 0.4111, val acc: 0.8331  (best train acc: 0.8232, best val acc: 0.8449, best train loss: 0.4358  @ epoch 5470 )\n",
      "[Epoch: 5500] train loss: 0.4538, train acc: 0.8087, val loss: 0.3976, val acc: 0.8310  (best train acc: 0.8232, best val acc: 0.8449, best train loss: 0.4358  @ epoch 5470 )\n",
      "[Epoch: 5520] train loss: 0.4831, train acc: 0.8015, val loss: 0.4019, val acc: 0.8391  (best train acc: 0.8232, best val acc: 0.8449, best train loss: 0.4358  @ epoch 5470 )\n",
      "[Epoch: 5540] train loss: 0.4693, train acc: 0.8020, val loss: 0.3935, val acc: 0.8327  (best train acc: 0.8232, best val acc: 0.8449, best train loss: 0.4358  @ epoch 5470 )\n",
      "[Epoch: 5560] train loss: 0.5127, train acc: 0.7906, val loss: 0.4055, val acc: 0.8263  (best train acc: 0.8232, best val acc: 0.8449, best train loss: 0.4358  @ epoch 5470 )\n",
      "[Epoch: 5580] train loss: 0.4965, train acc: 0.7954, val loss: 0.4368, val acc: 0.8115  (best train acc: 0.8232, best val acc: 0.8449, best train loss: 0.4358  @ epoch 5470 )\n",
      "[Epoch: 5600] train loss: 0.4595, train acc: 0.8081, val loss: 0.4017, val acc: 0.8290  (best train acc: 0.8232, best val acc: 0.8449, best train loss: 0.4358  @ epoch 5470 )\n",
      "[Epoch: 5620] train loss: 0.5340, train acc: 0.7723, val loss: 0.4410, val acc: 0.8057  (best train acc: 0.8232, best val acc: 0.8449, best train loss: 0.4358  @ epoch 5470 )\n",
      "[Epoch: 5640] train loss: 0.4847, train acc: 0.7987, val loss: 0.4314, val acc: 0.8128  (best train acc: 0.8241, best val acc: 0.8449, best train loss: 0.4358  @ epoch 5470 )\n",
      "[Epoch: 5660] train loss: 0.4541, train acc: 0.8081, val loss: 0.3943, val acc: 0.8405  (best train acc: 0.8241, best val acc: 0.8449, best train loss: 0.4358  @ epoch 5470 )\n",
      "[Epoch: 5680] train loss: 0.4827, train acc: 0.8092, val loss: 0.4030, val acc: 0.8280  (best train acc: 0.8241, best val acc: 0.8449, best train loss: 0.4358  @ epoch 5470 )\n",
      "[Epoch: 5700] train loss: 0.4963, train acc: 0.7979, val loss: 0.4042, val acc: 0.8239  (best train acc: 0.8241, best val acc: 0.8449, best train loss: 0.4295  @ epoch 5694 )\n",
      "[Epoch: 5720] train loss: 0.4733, train acc: 0.8043, val loss: 0.4129, val acc: 0.8175  (best train acc: 0.8241, best val acc: 0.8449, best train loss: 0.4295  @ epoch 5694 )\n",
      "[Epoch: 5740] train loss: 0.5375, train acc: 0.7676, val loss: 0.4235, val acc: 0.8206  (best train acc: 0.8241, best val acc: 0.8449, best train loss: 0.4295  @ epoch 5694 )\n",
      "[Epoch: 5760] train loss: 0.4900, train acc: 0.7925, val loss: 0.3784, val acc: 0.8388  (best train acc: 0.8241, best val acc: 0.8449, best train loss: 0.4295  @ epoch 5694 )\n",
      "[Epoch: 5780] train loss: 0.4986, train acc: 0.7757, val loss: 0.4325, val acc: 0.8121  (best train acc: 0.8241, best val acc: 0.8449, best train loss: 0.4295  @ epoch 5694 )\n",
      "[Epoch: 5800] train loss: 0.4513, train acc: 0.8091, val loss: 0.4105, val acc: 0.8273  (best train acc: 0.8241, best val acc: 0.8449, best train loss: 0.4295  @ epoch 5694 )\n",
      "[Epoch: 5820] train loss: 0.4621, train acc: 0.8104, val loss: 0.3993, val acc: 0.8331  (best train acc: 0.8241, best val acc: 0.8449, best train loss: 0.4295  @ epoch 5694 )\n",
      "[Epoch: 5840] train loss: 0.4896, train acc: 0.7948, val loss: 0.4018, val acc: 0.8260  (best train acc: 0.8241, best val acc: 0.8449, best train loss: 0.4295  @ epoch 5694 )\n",
      "[Epoch: 5860] train loss: 0.4382, train acc: 0.8175, val loss: 0.3942, val acc: 0.8364  (best train acc: 0.8241, best val acc: 0.8449, best train loss: 0.4295  @ epoch 5694 )\n",
      "[Epoch: 5880] train loss: 0.4818, train acc: 0.8007, val loss: 0.3943, val acc: 0.8317  (best train acc: 0.8241, best val acc: 0.8449, best train loss: 0.4295  @ epoch 5694 )\n",
      "[Epoch: 5900] train loss: 0.4766, train acc: 0.8031, val loss: 0.4739, val acc: 0.7892  (best train acc: 0.8241, best val acc: 0.8449, best train loss: 0.4295  @ epoch 5694 )\n",
      "[Epoch: 5920] train loss: 0.4768, train acc: 0.7940, val loss: 0.4471, val acc: 0.8017  (best train acc: 0.8241, best val acc: 0.8449, best train loss: 0.4295  @ epoch 5694 )\n",
      "[Epoch: 5940] train loss: 0.4541, train acc: 0.8073, val loss: 0.3893, val acc: 0.8327  (best train acc: 0.8241, best val acc: 0.8449, best train loss: 0.4295  @ epoch 5694 )\n",
      "[Epoch: 5960] train loss: 0.4497, train acc: 0.8148, val loss: 0.4097, val acc: 0.8243  (best train acc: 0.8241, best val acc: 0.8449, best train loss: 0.4295  @ epoch 5694 )\n",
      "[Epoch: 5980] train loss: 0.5293, train acc: 0.7751, val loss: 0.4055, val acc: 0.8280  (best train acc: 0.8241, best val acc: 0.8449, best train loss: 0.4295  @ epoch 5694 )\n",
      "[Epoch: 6000] train loss: 0.4792, train acc: 0.7968, val loss: 0.4023, val acc: 0.8280  (best train acc: 0.8241, best val acc: 0.8449, best train loss: 0.4295  @ epoch 5694 )\n",
      "[Epoch: 6020] train loss: 0.5152, train acc: 0.7854, val loss: 0.3948, val acc: 0.8317  (best train acc: 0.8241, best val acc: 0.8449, best train loss: 0.4295  @ epoch 5694 )\n",
      "[Epoch: 6040] train loss: 0.5392, train acc: 0.7692, val loss: 0.3881, val acc: 0.8334  (best train acc: 0.8241, best val acc: 0.8449, best train loss: 0.4295  @ epoch 5694 )\n",
      "[Epoch: 6060] train loss: 0.5597, train acc: 0.7615, val loss: 0.3918, val acc: 0.8388  (best train acc: 0.8241, best val acc: 0.8449, best train loss: 0.4295  @ epoch 5694 )\n",
      "[Epoch: 6080] train loss: 0.4727, train acc: 0.8010, val loss: 0.3942, val acc: 0.8327  (best train acc: 0.8241, best val acc: 0.8449, best train loss: 0.4295  @ epoch 5694 )\n",
      "[Epoch: 6100] train loss: 0.4578, train acc: 0.8166, val loss: 0.3990, val acc: 0.8277  (best train acc: 0.8241, best val acc: 0.8449, best train loss: 0.4295  @ epoch 5694 )\n",
      "[Epoch: 6120] train loss: 0.4954, train acc: 0.7909, val loss: 0.3976, val acc: 0.8290  (best train acc: 0.8241, best val acc: 0.8449, best train loss: 0.4295  @ epoch 5694 )\n",
      "[Epoch: 6140] train loss: 0.5107, train acc: 0.7873, val loss: 0.4082, val acc: 0.8287  (best train acc: 0.8241, best val acc: 0.8449, best train loss: 0.4295  @ epoch 5694 )\n",
      "[Epoch: 6160] train loss: 0.4916, train acc: 0.8028, val loss: 0.3981, val acc: 0.8337  (best train acc: 0.8300, best val acc: 0.8449, best train loss: 0.4295  @ epoch 5694 )\n",
      "[Epoch: 6180] train loss: 0.4511, train acc: 0.8156, val loss: 0.3875, val acc: 0.8428  (best train acc: 0.8300, best val acc: 0.8449, best train loss: 0.4295  @ epoch 5694 )\n",
      "[Epoch: 6200] train loss: 0.4656, train acc: 0.8092, val loss: 0.3959, val acc: 0.8310  (best train acc: 0.8300, best val acc: 0.8452, best train loss: 0.4295  @ epoch 5694 )\n",
      "[Epoch: 6220] train loss: 0.5021, train acc: 0.7840, val loss: 0.4179, val acc: 0.8199  (best train acc: 0.8300, best val acc: 0.8452, best train loss: 0.4295  @ epoch 5694 )\n",
      "[Epoch: 6240] train loss: 0.4821, train acc: 0.8010, val loss: 0.4477, val acc: 0.8030  (best train acc: 0.8300, best val acc: 0.8452, best train loss: 0.4295  @ epoch 5694 )\n",
      "[Epoch: 6260] train loss: 0.4549, train acc: 0.8138, val loss: 0.4021, val acc: 0.8347  (best train acc: 0.8300, best val acc: 0.8452, best train loss: 0.4295  @ epoch 5694 )\n",
      "[Epoch: 6280] train loss: 0.4573, train acc: 0.8097, val loss: 0.4081, val acc: 0.8280  (best train acc: 0.8300, best val acc: 0.8452, best train loss: 0.4295  @ epoch 5694 )\n",
      "[Epoch: 6300] train loss: 0.4807, train acc: 0.7931, val loss: 0.4534, val acc: 0.7976  (best train acc: 0.8300, best val acc: 0.8452, best train loss: 0.4295  @ epoch 5694 )\n",
      "[Epoch: 6320] train loss: 0.4521, train acc: 0.8127, val loss: 0.3825, val acc: 0.8418  (best train acc: 0.8300, best val acc: 0.8452, best train loss: 0.4295  @ epoch 5694 )\n",
      "[Epoch: 6340] train loss: 0.4621, train acc: 0.8098, val loss: 0.4037, val acc: 0.8260  (best train acc: 0.8300, best val acc: 0.8452, best train loss: 0.4295  @ epoch 5694 )\n",
      "[Epoch: 6360] train loss: 0.4765, train acc: 0.8004, val loss: 0.4069, val acc: 0.8219  (best train acc: 0.8300, best val acc: 0.8492, best train loss: 0.4295  @ epoch 5694 )\n",
      "[Epoch: 6380] train loss: 0.5050, train acc: 0.7926, val loss: 0.4025, val acc: 0.8270  (best train acc: 0.8300, best val acc: 0.8492, best train loss: 0.4295  @ epoch 5694 )\n",
      "[Epoch: 6400] train loss: 0.5055, train acc: 0.7822, val loss: 0.4180, val acc: 0.8209  (best train acc: 0.8300, best val acc: 0.8492, best train loss: 0.4295  @ epoch 5694 )\n",
      "[Epoch: 6420] train loss: 0.4755, train acc: 0.8039, val loss: 0.4024, val acc: 0.8324  (best train acc: 0.8300, best val acc: 0.8492, best train loss: 0.4295  @ epoch 5694 )\n",
      "[Epoch: 6440] train loss: 0.4962, train acc: 0.8039, val loss: 0.4317, val acc: 0.8121  (best train acc: 0.8300, best val acc: 0.8492, best train loss: 0.4295  @ epoch 5694 )\n",
      "[Epoch: 6460] train loss: 0.4927, train acc: 0.7872, val loss: 0.3901, val acc: 0.8341  (best train acc: 0.8300, best val acc: 0.8492, best train loss: 0.4295  @ epoch 5694 )\n",
      "[Epoch: 6480] train loss: 0.4626, train acc: 0.8090, val loss: 0.3894, val acc: 0.8381  (best train acc: 0.8300, best val acc: 0.8492, best train loss: 0.4295  @ epoch 5694 )\n",
      "[Epoch: 6500] train loss: 0.4453, train acc: 0.8102, val loss: 0.4075, val acc: 0.8189  (best train acc: 0.8300, best val acc: 0.8492, best train loss: 0.4270  @ epoch 6488 )\n",
      "[Epoch: 6520] train loss: 0.4639, train acc: 0.8109, val loss: 0.4153, val acc: 0.8206  (best train acc: 0.8300, best val acc: 0.8492, best train loss: 0.4270  @ epoch 6488 )\n",
      "[Epoch: 6540] train loss: 0.4344, train acc: 0.8174, val loss: 0.3880, val acc: 0.8401  (best train acc: 0.8300, best val acc: 0.8492, best train loss: 0.4270  @ epoch 6488 )\n",
      "[Epoch: 6560] train loss: 0.4749, train acc: 0.8033, val loss: 0.3854, val acc: 0.8347  (best train acc: 0.8300, best val acc: 0.8492, best train loss: 0.4266  @ epoch 6543 )\n",
      "[Epoch: 6580] train loss: 0.4478, train acc: 0.8203, val loss: 0.4240, val acc: 0.8162  (best train acc: 0.8300, best val acc: 0.8492, best train loss: 0.4266  @ epoch 6543 )\n",
      "[Epoch: 6600] train loss: 0.4929, train acc: 0.7987, val loss: 0.4062, val acc: 0.8273  (best train acc: 0.8300, best val acc: 0.8492, best train loss: 0.4266  @ epoch 6543 )\n",
      "[Epoch: 6620] train loss: 0.4679, train acc: 0.8001, val loss: 0.3858, val acc: 0.8401  (best train acc: 0.8300, best val acc: 0.8492, best train loss: 0.4266  @ epoch 6543 )\n",
      "[Epoch: 6640] train loss: 0.4538, train acc: 0.8096, val loss: 0.3807, val acc: 0.8391  (best train acc: 0.8300, best val acc: 0.8492, best train loss: 0.4266  @ epoch 6543 )\n",
      "[Epoch: 6660] train loss: 0.4818, train acc: 0.8012, val loss: 0.3918, val acc: 0.8334  (best train acc: 0.8300, best val acc: 0.8492, best train loss: 0.4209  @ epoch 6658 )\n",
      "[Epoch: 6680] train loss: 0.4671, train acc: 0.8007, val loss: 0.3837, val acc: 0.8358  (best train acc: 0.8300, best val acc: 0.8492, best train loss: 0.4209  @ epoch 6658 )\n",
      "[Epoch: 6700] train loss: 0.4844, train acc: 0.8035, val loss: 0.4084, val acc: 0.8219  (best train acc: 0.8300, best val acc: 0.8492, best train loss: 0.4209  @ epoch 6658 )\n",
      "[Epoch: 6720] train loss: 0.4660, train acc: 0.8069, val loss: 0.4176, val acc: 0.8216  (best train acc: 0.8300, best val acc: 0.8492, best train loss: 0.4209  @ epoch 6658 )\n",
      "[Epoch: 6740] train loss: 0.5019, train acc: 0.7958, val loss: 0.4058, val acc: 0.8270  (best train acc: 0.8300, best val acc: 0.8492, best train loss: 0.4209  @ epoch 6658 )\n",
      "[Epoch: 6760] train loss: 0.4462, train acc: 0.8172, val loss: 0.3906, val acc: 0.8341  (best train acc: 0.8300, best val acc: 0.8492, best train loss: 0.4209  @ epoch 6658 )\n",
      "[Epoch: 6780] train loss: 0.5133, train acc: 0.7726, val loss: 0.4091, val acc: 0.8199  (best train acc: 0.8300, best val acc: 0.8492, best train loss: 0.4209  @ epoch 6658 )\n",
      "[Epoch: 6800] train loss: 0.4913, train acc: 0.7967, val loss: 0.3744, val acc: 0.8398  (best train acc: 0.8300, best val acc: 0.8492, best train loss: 0.4209  @ epoch 6658 )\n",
      "[Epoch: 6820] train loss: 0.4904, train acc: 0.7906, val loss: 0.4235, val acc: 0.8172  (best train acc: 0.8300, best val acc: 0.8492, best train loss: 0.4209  @ epoch 6658 )\n",
      "[Epoch: 6840] train loss: 0.4572, train acc: 0.8135, val loss: 0.3821, val acc: 0.8391  (best train acc: 0.8300, best val acc: 0.8492, best train loss: 0.4209  @ epoch 6658 )\n",
      "[Epoch: 6860] train loss: 0.4630, train acc: 0.8138, val loss: 0.3930, val acc: 0.8324  (best train acc: 0.8300, best val acc: 0.8492, best train loss: 0.4209  @ epoch 6658 )\n",
      "[Epoch: 6880] train loss: 0.4623, train acc: 0.8047, val loss: 0.4086, val acc: 0.8152  (best train acc: 0.8313, best val acc: 0.8492, best train loss: 0.4209  @ epoch 6658 )\n",
      "[Epoch: 6900] train loss: 0.4927, train acc: 0.7925, val loss: 0.4208, val acc: 0.8165  (best train acc: 0.8313, best val acc: 0.8492, best train loss: 0.4207  @ epoch 6897 )\n",
      "[Epoch: 6920] train loss: 0.4695, train acc: 0.8071, val loss: 0.4080, val acc: 0.8229  (best train acc: 0.8313, best val acc: 0.8492, best train loss: 0.4207  @ epoch 6897 )\n",
      "[Epoch: 6940] train loss: 0.4663, train acc: 0.8083, val loss: 0.3944, val acc: 0.8314  (best train acc: 0.8313, best val acc: 0.8492, best train loss: 0.4189  @ epoch 6926 )\n",
      "[Epoch: 6960] train loss: 0.4353, train acc: 0.8161, val loss: 0.3811, val acc: 0.8455  (best train acc: 0.8313, best val acc: 0.8492, best train loss: 0.4189  @ epoch 6926 )\n",
      "[Epoch: 6980] train loss: 0.4657, train acc: 0.8039, val loss: 0.3996, val acc: 0.8287  (best train acc: 0.8313, best val acc: 0.8492, best train loss: 0.4189  @ epoch 6926 )\n",
      "[Epoch: 7000] train loss: 0.4965, train acc: 0.7874, val loss: 0.3855, val acc: 0.8449  (best train acc: 0.8313, best val acc: 0.8492, best train loss: 0.4177  @ epoch 6995 )\n",
      "[Epoch: 7020] train loss: 0.4695, train acc: 0.8125, val loss: 0.3761, val acc: 0.8401  (best train acc: 0.8313, best val acc: 0.8492, best train loss: 0.4177  @ epoch 6995 )\n",
      "[Epoch: 7040] train loss: 0.4535, train acc: 0.8159, val loss: 0.3840, val acc: 0.8361  (best train acc: 0.8313, best val acc: 0.8492, best train loss: 0.4177  @ epoch 6995 )\n",
      "[Epoch: 7060] train loss: 0.4590, train acc: 0.8100, val loss: 0.3874, val acc: 0.8344  (best train acc: 0.8313, best val acc: 0.8492, best train loss: 0.4177  @ epoch 6995 )\n",
      "[Epoch: 7080] train loss: 0.4597, train acc: 0.8126, val loss: 0.3914, val acc: 0.8300  (best train acc: 0.8313, best val acc: 0.8492, best train loss: 0.4177  @ epoch 6995 )\n",
      "[Epoch: 7100] train loss: 0.4271, train acc: 0.8238, val loss: 0.3819, val acc: 0.8327  (best train acc: 0.8323, best val acc: 0.8492, best train loss: 0.4134  @ epoch 7099 )\n",
      "[Epoch: 7120] train loss: 0.4620, train acc: 0.8124, val loss: 0.3726, val acc: 0.8391  (best train acc: 0.8323, best val acc: 0.8492, best train loss: 0.4122  @ epoch 7101 )\n",
      "[Epoch: 7140] train loss: 0.4762, train acc: 0.8081, val loss: 0.3784, val acc: 0.8398  (best train acc: 0.8323, best val acc: 0.8492, best train loss: 0.4122  @ epoch 7101 )\n",
      "[Epoch: 7160] train loss: 0.4495, train acc: 0.8117, val loss: 0.4366, val acc: 0.8105  (best train acc: 0.8323, best val acc: 0.8492, best train loss: 0.4122  @ epoch 7101 )\n",
      "[Epoch: 7180] train loss: 0.4615, train acc: 0.8124, val loss: 0.3919, val acc: 0.8290  (best train acc: 0.8323, best val acc: 0.8492, best train loss: 0.4122  @ epoch 7101 )\n",
      "[Epoch: 7200] train loss: 0.4499, train acc: 0.8139, val loss: 0.3866, val acc: 0.8300  (best train acc: 0.8323, best val acc: 0.8492, best train loss: 0.4122  @ epoch 7101 )\n",
      "[Epoch: 7220] train loss: 0.4686, train acc: 0.8044, val loss: 0.3779, val acc: 0.8415  (best train acc: 0.8323, best val acc: 0.8492, best train loss: 0.4122  @ epoch 7101 )\n",
      "[Epoch: 7240] train loss: 0.4249, train acc: 0.8253, val loss: 0.3946, val acc: 0.8293  (best train acc: 0.8323, best val acc: 0.8492, best train loss: 0.4122  @ epoch 7101 )\n",
      "[Epoch: 7260] train loss: 0.4562, train acc: 0.8143, val loss: 0.3818, val acc: 0.8391  (best train acc: 0.8334, best val acc: 0.8509, best train loss: 0.4083  @ epoch 7246 )\n",
      "[Epoch: 7280] train loss: 0.4326, train acc: 0.8214, val loss: 0.3828, val acc: 0.8378  (best train acc: 0.8334, best val acc: 0.8509, best train loss: 0.4057  @ epoch 7273 )\n",
      "[Epoch: 7300] train loss: 0.5274, train acc: 0.7691, val loss: 0.3815, val acc: 0.8472  (best train acc: 0.8334, best val acc: 0.8509, best train loss: 0.4057  @ epoch 7273 )\n",
      "[Epoch: 7320] train loss: 0.4485, train acc: 0.8065, val loss: 0.3690, val acc: 0.8459  (best train acc: 0.8334, best val acc: 0.8516, best train loss: 0.4057  @ epoch 7273 )\n",
      "[Epoch: 7340] train loss: 0.4553, train acc: 0.8096, val loss: 0.3903, val acc: 0.8347  (best train acc: 0.8344, best val acc: 0.8516, best train loss: 0.4057  @ epoch 7273 )\n",
      "[Epoch: 7360] train loss: 0.4360, train acc: 0.8181, val loss: 0.3641, val acc: 0.8452  (best train acc: 0.8344, best val acc: 0.8516, best train loss: 0.4057  @ epoch 7273 )\n",
      "[Epoch: 7380] train loss: 0.4880, train acc: 0.7963, val loss: 0.4251, val acc: 0.8148  (best train acc: 0.8385, best val acc: 0.8516, best train loss: 0.3997  @ epoch 7363 )\n",
      "[Epoch: 7400] train loss: 0.4550, train acc: 0.8146, val loss: 0.3875, val acc: 0.8351  (best train acc: 0.8385, best val acc: 0.8516, best train loss: 0.3997  @ epoch 7363 )\n",
      "[Epoch: 7420] train loss: 0.4523, train acc: 0.8145, val loss: 0.3707, val acc: 0.8462  (best train acc: 0.8385, best val acc: 0.8516, best train loss: 0.3997  @ epoch 7363 )\n",
      "[Epoch: 7440] train loss: 0.4761, train acc: 0.8080, val loss: 0.3869, val acc: 0.8432  (best train acc: 0.8385, best val acc: 0.8523, best train loss: 0.3997  @ epoch 7363 )\n",
      "[Epoch: 7460] train loss: 0.4198, train acc: 0.8244, val loss: 0.3656, val acc: 0.8486  (best train acc: 0.8385, best val acc: 0.8523, best train loss: 0.3997  @ epoch 7363 )\n",
      "[Epoch: 7480] train loss: 0.4565, train acc: 0.8109, val loss: 0.3919, val acc: 0.8297  (best train acc: 0.8390, best val acc: 0.8523, best train loss: 0.3968  @ epoch 7463 )\n",
      "[Epoch: 7500] train loss: 0.4133, train acc: 0.8283, val loss: 0.3666, val acc: 0.8465  (best train acc: 0.8390, best val acc: 0.8523, best train loss: 0.3968  @ epoch 7463 )\n",
      "[Epoch: 7520] train loss: 0.4700, train acc: 0.7969, val loss: 0.3827, val acc: 0.8320  (best train acc: 0.8390, best val acc: 0.8523, best train loss: 0.3968  @ epoch 7463 )\n",
      "[Epoch: 7540] train loss: 0.4253, train acc: 0.8240, val loss: 0.4067, val acc: 0.8216  (best train acc: 0.8390, best val acc: 0.8523, best train loss: 0.3968  @ epoch 7463 )\n",
      "[Epoch: 7560] train loss: 0.4442, train acc: 0.8125, val loss: 0.3676, val acc: 0.8438  (best train acc: 0.8390, best val acc: 0.8523, best train loss: 0.3968  @ epoch 7463 )\n",
      "[Epoch: 7580] train loss: 0.4097, train acc: 0.8276, val loss: 0.3642, val acc: 0.8486  (best train acc: 0.8390, best val acc: 0.8523, best train loss: 0.3968  @ epoch 7463 )\n",
      "[Epoch: 7600] train loss: 0.4518, train acc: 0.8127, val loss: 0.3770, val acc: 0.8347  (best train acc: 0.8390, best val acc: 0.8523, best train loss: 0.3968  @ epoch 7463 )\n",
      "[Epoch: 7620] train loss: 0.5022, train acc: 0.7834, val loss: 0.3710, val acc: 0.8476  (best train acc: 0.8390, best val acc: 0.8523, best train loss: 0.3968  @ epoch 7463 )\n",
      "[Epoch: 7640] train loss: 0.4300, train acc: 0.8240, val loss: 0.3599, val acc: 0.8489  (best train acc: 0.8390, best val acc: 0.8523, best train loss: 0.3968  @ epoch 7463 )\n",
      "[Epoch: 7660] train loss: 0.4521, train acc: 0.8195, val loss: 0.3710, val acc: 0.8408  (best train acc: 0.8390, best val acc: 0.8523, best train loss: 0.3968  @ epoch 7463 )\n",
      "[Epoch: 7680] train loss: 0.4634, train acc: 0.7955, val loss: 0.3659, val acc: 0.8503  (best train acc: 0.8390, best val acc: 0.8523, best train loss: 0.3968  @ epoch 7463 )\n",
      "[Epoch: 7700] train loss: 0.4233, train acc: 0.8276, val loss: 0.3868, val acc: 0.8344  (best train acc: 0.8390, best val acc: 0.8543, best train loss: 0.3968  @ epoch 7463 )\n",
      "[Epoch: 7720] train loss: 0.4970, train acc: 0.7833, val loss: 0.3882, val acc: 0.8263  (best train acc: 0.8390, best val acc: 0.8543, best train loss: 0.3968  @ epoch 7463 )\n",
      "[Epoch: 7740] train loss: 0.4242, train acc: 0.8281, val loss: 0.3668, val acc: 0.8489  (best train acc: 0.8390, best val acc: 0.8543, best train loss: 0.3968  @ epoch 7463 )\n",
      "[Epoch: 7760] train loss: 0.4633, train acc: 0.8045, val loss: 0.3702, val acc: 0.8425  (best train acc: 0.8390, best val acc: 0.8543, best train loss: 0.3968  @ epoch 7463 )\n",
      "[Epoch: 7780] train loss: 0.4760, train acc: 0.8048, val loss: 0.3683, val acc: 0.8378  (best train acc: 0.8390, best val acc: 0.8543, best train loss: 0.3968  @ epoch 7463 )\n",
      "[Epoch: 7800] train loss: 0.4555, train acc: 0.8099, val loss: 0.3755, val acc: 0.8479  (best train acc: 0.8390, best val acc: 0.8543, best train loss: 0.3968  @ epoch 7463 )\n",
      "[Epoch: 7820] train loss: 0.4297, train acc: 0.8209, val loss: 0.3671, val acc: 0.8438  (best train acc: 0.8390, best val acc: 0.8543, best train loss: 0.3968  @ epoch 7463 )\n",
      "[Epoch: 7840] train loss: 0.4161, train acc: 0.8274, val loss: 0.3924, val acc: 0.8361  (best train acc: 0.8390, best val acc: 0.8543, best train loss: 0.3968  @ epoch 7463 )\n",
      "[Epoch: 7860] train loss: 0.4516, train acc: 0.8125, val loss: 0.3837, val acc: 0.8358  (best train acc: 0.8390, best val acc: 0.8543, best train loss: 0.3968  @ epoch 7463 )\n",
      "[Epoch: 7880] train loss: 0.3991, train acc: 0.8347, val loss: 0.3677, val acc: 0.8432  (best train acc: 0.8390, best val acc: 0.8543, best train loss: 0.3968  @ epoch 7871 )\n",
      "[Epoch: 7900] train loss: 0.4217, train acc: 0.8316, val loss: 0.3818, val acc: 0.8405  (best train acc: 0.8399, best val acc: 0.8543, best train loss: 0.3968  @ epoch 7871 )\n",
      "[Epoch: 7920] train loss: 0.3926, train acc: 0.8411, val loss: 0.3643, val acc: 0.8455  (best train acc: 0.8411, best val acc: 0.8543, best train loss: 0.3926  @ epoch 7920 )\n",
      "[Epoch: 7940] train loss: 0.4517, train acc: 0.8135, val loss: 0.3725, val acc: 0.8486  (best train acc: 0.8411, best val acc: 0.8543, best train loss: 0.3926  @ epoch 7920 )\n",
      "[Epoch: 7960] train loss: 0.4192, train acc: 0.8242, val loss: 0.3565, val acc: 0.8519  (best train acc: 0.8411, best val acc: 0.8543, best train loss: 0.3926  @ epoch 7920 )\n",
      "[Epoch: 7980] train loss: 0.4166, train acc: 0.8253, val loss: 0.3664, val acc: 0.8395  (best train acc: 0.8411, best val acc: 0.8543, best train loss: 0.3887  @ epoch 7973 )\n",
      "[Epoch: 8000] train loss: 0.4667, train acc: 0.8052, val loss: 0.3634, val acc: 0.8445  (best train acc: 0.8411, best val acc: 0.8543, best train loss: 0.3887  @ epoch 7973 )\n",
      "[Epoch: 8020] train loss: 0.4166, train acc: 0.8218, val loss: 0.3547, val acc: 0.8489  (best train acc: 0.8411, best val acc: 0.8543, best train loss: 0.3887  @ epoch 7973 )\n",
      "[Epoch: 8040] train loss: 0.4329, train acc: 0.8093, val loss: 0.3550, val acc: 0.8503  (best train acc: 0.8411, best val acc: 0.8543, best train loss: 0.3887  @ epoch 7973 )\n",
      "[Epoch: 8060] train loss: 0.3989, train acc: 0.8364, val loss: 0.3601, val acc: 0.8536  (best train acc: 0.8411, best val acc: 0.8543, best train loss: 0.3887  @ epoch 7973 )\n",
      "[Epoch: 8080] train loss: 0.4901, train acc: 0.7962, val loss: 0.3645, val acc: 0.8479  (best train acc: 0.8411, best val acc: 0.8556, best train loss: 0.3887  @ epoch 7973 )\n",
      "[Epoch: 8100] train loss: 0.4598, train acc: 0.8126, val loss: 0.3525, val acc: 0.8516  (best train acc: 0.8411, best val acc: 0.8556, best train loss: 0.3887  @ epoch 7973 )\n",
      "[Epoch: 8120] train loss: 0.3995, train acc: 0.8349, val loss: 0.3478, val acc: 0.8536  (best train acc: 0.8411, best val acc: 0.8577, best train loss: 0.3887  @ epoch 7973 )\n",
      "[Epoch: 8140] train loss: 0.4585, train acc: 0.8104, val loss: 0.3940, val acc: 0.8273  (best train acc: 0.8411, best val acc: 0.8577, best train loss: 0.3887  @ epoch 7973 )\n",
      "[Epoch: 8160] train loss: 0.3947, train acc: 0.8349, val loss: 0.3690, val acc: 0.8513  (best train acc: 0.8411, best val acc: 0.8577, best train loss: 0.3887  @ epoch 7973 )\n",
      "[Epoch: 8180] train loss: 0.4615, train acc: 0.8016, val loss: 0.3559, val acc: 0.8519  (best train acc: 0.8411, best val acc: 0.8577, best train loss: 0.3887  @ epoch 7973 )\n",
      "[Epoch: 8200] train loss: 0.4348, train acc: 0.8166, val loss: 0.3926, val acc: 0.8256  (best train acc: 0.8411, best val acc: 0.8577, best train loss: 0.3887  @ epoch 7973 )\n",
      "[Epoch: 8220] train loss: 0.4111, train acc: 0.8235, val loss: 0.3510, val acc: 0.8556  (best train acc: 0.8417, best val acc: 0.8577, best train loss: 0.3767  @ epoch 8213 )\n",
      "[Epoch: 8240] train loss: 0.3960, train acc: 0.8329, val loss: 0.3479, val acc: 0.8530  (best train acc: 0.8417, best val acc: 0.8577, best train loss: 0.3767  @ epoch 8213 )\n",
      "[Epoch: 8260] train loss: 0.4240, train acc: 0.8237, val loss: 0.3413, val acc: 0.8556  (best train acc: 0.8417, best val acc: 0.8577, best train loss: 0.3767  @ epoch 8213 )\n",
      "[Epoch: 8280] train loss: 0.4173, train acc: 0.8242, val loss: 0.3450, val acc: 0.8523  (best train acc: 0.8417, best val acc: 0.8577, best train loss: 0.3767  @ epoch 8213 )\n",
      "[Epoch: 8300] train loss: 0.4230, train acc: 0.8177, val loss: 0.3443, val acc: 0.8546  (best train acc: 0.8438, best val acc: 0.8577, best train loss: 0.3767  @ epoch 8213 )\n",
      "[Epoch: 8320] train loss: 0.4067, train acc: 0.8284, val loss: 0.3894, val acc: 0.8293  (best train acc: 0.8438, best val acc: 0.8577, best train loss: 0.3767  @ epoch 8213 )\n",
      "[Epoch: 8340] train loss: 0.4130, train acc: 0.8292, val loss: 0.3697, val acc: 0.8422  (best train acc: 0.8438, best val acc: 0.8577, best train loss: 0.3767  @ epoch 8213 )\n",
      "[Epoch: 8360] train loss: 0.4190, train acc: 0.8268, val loss: 0.3483, val acc: 0.8506  (best train acc: 0.8438, best val acc: 0.8580, best train loss: 0.3767  @ epoch 8213 )\n",
      "[Epoch: 8380] train loss: 0.3995, train acc: 0.8322, val loss: 0.3970, val acc: 0.8270  (best train acc: 0.8438, best val acc: 0.8580, best train loss: 0.3767  @ epoch 8213 )\n",
      "[Epoch: 8400] train loss: 0.4101, train acc: 0.8336, val loss: 0.3929, val acc: 0.8209  (best train acc: 0.8438, best val acc: 0.8580, best train loss: 0.3767  @ epoch 8213 )\n",
      "[Epoch: 8420] train loss: 0.4382, train acc: 0.8145, val loss: 0.3760, val acc: 0.8287  (best train acc: 0.8438, best val acc: 0.8580, best train loss: 0.3767  @ epoch 8213 )\n",
      "[Epoch: 8440] train loss: 0.3889, train acc: 0.8386, val loss: 0.3456, val acc: 0.8546  (best train acc: 0.8438, best val acc: 0.8580, best train loss: 0.3767  @ epoch 8213 )\n",
      "[Epoch: 8460] train loss: 0.4097, train acc: 0.8219, val loss: 0.3407, val acc: 0.8543  (best train acc: 0.8438, best val acc: 0.8590, best train loss: 0.3767  @ epoch 8213 )\n",
      "[Epoch: 8480] train loss: 0.4052, train acc: 0.8282, val loss: 0.3472, val acc: 0.8503  (best train acc: 0.8438, best val acc: 0.8590, best train loss: 0.3711  @ epoch 8479 )\n",
      "[Epoch: 8500] train loss: 0.4185, train acc: 0.8300, val loss: 0.3640, val acc: 0.8452  (best train acc: 0.8438, best val acc: 0.8590, best train loss: 0.3711  @ epoch 8499 )\n",
      "[Epoch: 8520] train loss: 0.4071, train acc: 0.8326, val loss: 0.3635, val acc: 0.8371  (best train acc: 0.8438, best val acc: 0.8590, best train loss: 0.3711  @ epoch 8499 )\n",
      "[Epoch: 8540] train loss: 0.4549, train acc: 0.8073, val loss: 0.3628, val acc: 0.8536  (best train acc: 0.8438, best val acc: 0.8590, best train loss: 0.3711  @ epoch 8499 )\n",
      "[Epoch: 8560] train loss: 0.4243, train acc: 0.8218, val loss: 0.3529, val acc: 0.8476  (best train acc: 0.8511, best val acc: 0.8590, best train loss: 0.3711  @ epoch 8499 )\n",
      "[Epoch: 8580] train loss: 0.4307, train acc: 0.8209, val loss: 0.3418, val acc: 0.8567  (best train acc: 0.8511, best val acc: 0.8590, best train loss: 0.3711  @ epoch 8499 )\n",
      "[Epoch: 8600] train loss: 0.4593, train acc: 0.7992, val loss: 0.3539, val acc: 0.8496  (best train acc: 0.8511, best val acc: 0.8590, best train loss: 0.3711  @ epoch 8499 )\n",
      "[Epoch: 8620] train loss: 0.4350, train acc: 0.8221, val loss: 0.3698, val acc: 0.8405  (best train acc: 0.8511, best val acc: 0.8590, best train loss: 0.3711  @ epoch 8499 )\n",
      "[Epoch: 8640] train loss: 0.3954, train acc: 0.8369, val loss: 0.3476, val acc: 0.8486  (best train acc: 0.8511, best val acc: 0.8590, best train loss: 0.3711  @ epoch 8499 )\n",
      "[Epoch: 8660] train loss: 0.4304, train acc: 0.8191, val loss: 0.3439, val acc: 0.8479  (best train acc: 0.8511, best val acc: 0.8597, best train loss: 0.3711  @ epoch 8499 )\n",
      "[Epoch: 8680] train loss: 0.4121, train acc: 0.8235, val loss: 0.3355, val acc: 0.8560  (best train acc: 0.8511, best val acc: 0.8597, best train loss: 0.3711  @ epoch 8499 )\n",
      "[Epoch: 8700] train loss: 0.3979, train acc: 0.8279, val loss: 0.3899, val acc: 0.8266  (best train acc: 0.8511, best val acc: 0.8597, best train loss: 0.3711  @ epoch 8499 )\n",
      "[Epoch: 8720] train loss: 0.4247, train acc: 0.8182, val loss: 0.3453, val acc: 0.8503  (best train acc: 0.8511, best val acc: 0.8597, best train loss: 0.3711  @ epoch 8499 )\n",
      "[Epoch: 8740] train loss: 0.4062, train acc: 0.8336, val loss: 0.3594, val acc: 0.8546  (best train acc: 0.8511, best val acc: 0.8597, best train loss: 0.3711  @ epoch 8499 )\n",
      "[Epoch: 8760] train loss: 0.4527, train acc: 0.8026, val loss: 0.3620, val acc: 0.8405  (best train acc: 0.8511, best val acc: 0.8597, best train loss: 0.3711  @ epoch 8499 )\n",
      "[Epoch: 8780] train loss: 0.4938, train acc: 0.7841, val loss: 0.3543, val acc: 0.8513  (best train acc: 0.8511, best val acc: 0.8597, best train loss: 0.3711  @ epoch 8499 )\n",
      "[Epoch: 8800] train loss: 0.4304, train acc: 0.8218, val loss: 0.3486, val acc: 0.8553  (best train acc: 0.8511, best val acc: 0.8597, best train loss: 0.3711  @ epoch 8499 )\n",
      "[Epoch: 8820] train loss: 0.3899, train acc: 0.8384, val loss: 0.3432, val acc: 0.8580  (best train acc: 0.8511, best val acc: 0.8597, best train loss: 0.3711  @ epoch 8499 )\n",
      "[Epoch: 8840] train loss: 0.4299, train acc: 0.8167, val loss: 0.3445, val acc: 0.8492  (best train acc: 0.8511, best val acc: 0.8597, best train loss: 0.3711  @ epoch 8499 )\n",
      "[Epoch: 8860] train loss: 0.3848, train acc: 0.8389, val loss: 0.3468, val acc: 0.8553  (best train acc: 0.8511, best val acc: 0.8597, best train loss: 0.3711  @ epoch 8499 )\n",
      "[Epoch: 8880] train loss: 0.4258, train acc: 0.8201, val loss: 0.3551, val acc: 0.8479  (best train acc: 0.8511, best val acc: 0.8597, best train loss: 0.3711  @ epoch 8499 )\n",
      "[Epoch: 8900] train loss: 0.3796, train acc: 0.8401, val loss: 0.3384, val acc: 0.8583  (best train acc: 0.8511, best val acc: 0.8597, best train loss: 0.3711  @ epoch 8499 )\n",
      "[Epoch: 8920] train loss: 0.4050, train acc: 0.8270, val loss: 0.3450, val acc: 0.8563  (best train acc: 0.8511, best val acc: 0.8600, best train loss: 0.3711  @ epoch 8499 )\n",
      "[Epoch: 8940] train loss: 0.4146, train acc: 0.8193, val loss: 0.3676, val acc: 0.8418  (best train acc: 0.8511, best val acc: 0.8607, best train loss: 0.3711  @ epoch 8499 )\n",
      "[Epoch: 8960] train loss: 0.4106, train acc: 0.8240, val loss: 0.3504, val acc: 0.8472  (best train acc: 0.8511, best val acc: 0.8607, best train loss: 0.3711  @ epoch 8499 )\n",
      "[Epoch: 8980] train loss: 0.3759, train acc: 0.8412, val loss: 0.3343, val acc: 0.8580  (best train acc: 0.8511, best val acc: 0.8607, best train loss: 0.3711  @ epoch 8499 )\n",
      "[Epoch: 9000] train loss: 0.4516, train acc: 0.8068, val loss: 0.3532, val acc: 0.8482  (best train acc: 0.8511, best val acc: 0.8614, best train loss: 0.3711  @ epoch 8499 )\n",
      "[Epoch: 9020] train loss: 0.3823, train acc: 0.8422, val loss: 0.3510, val acc: 0.8540  (best train acc: 0.8511, best val acc: 0.8617, best train loss: 0.3711  @ epoch 8499 )\n",
      "[Epoch: 9040] train loss: 0.4132, train acc: 0.8292, val loss: 0.3482, val acc: 0.8523  (best train acc: 0.8511, best val acc: 0.8648, best train loss: 0.3711  @ epoch 8499 )\n",
      "[Epoch: 9060] train loss: 0.3838, train acc: 0.8344, val loss: 0.3438, val acc: 0.8536  (best train acc: 0.8511, best val acc: 0.8648, best train loss: 0.3711  @ epoch 8499 )\n",
      "[Epoch: 9080] train loss: 0.4110, train acc: 0.8248, val loss: 0.3567, val acc: 0.8523  (best train acc: 0.8511, best val acc: 0.8648, best train loss: 0.3711  @ epoch 8499 )\n",
      "[Epoch: 9100] train loss: 0.4270, train acc: 0.8195, val loss: 0.3522, val acc: 0.8503  (best train acc: 0.8511, best val acc: 0.8648, best train loss: 0.3711  @ epoch 8499 )\n",
      "[Epoch: 9120] train loss: 0.3727, train acc: 0.8406, val loss: 0.3485, val acc: 0.8530  (best train acc: 0.8511, best val acc: 0.8648, best train loss: 0.3711  @ epoch 8499 )\n",
      "[Epoch: 9140] train loss: 0.4058, train acc: 0.8257, val loss: 0.3553, val acc: 0.8422  (best train acc: 0.8511, best val acc: 0.8648, best train loss: 0.3711  @ epoch 8499 )\n",
      "[Epoch: 9160] train loss: 0.4045, train acc: 0.8323, val loss: 0.3439, val acc: 0.8513  (best train acc: 0.8511, best val acc: 0.8648, best train loss: 0.3711  @ epoch 8499 )\n",
      "[Epoch: 9180] train loss: 0.4316, train acc: 0.8153, val loss: 0.3406, val acc: 0.8540  (best train acc: 0.8511, best val acc: 0.8648, best train loss: 0.3711  @ epoch 8499 )\n",
      "[Epoch: 9200] train loss: 0.4011, train acc: 0.8314, val loss: 0.3468, val acc: 0.8509  (best train acc: 0.8511, best val acc: 0.8648, best train loss: 0.3711  @ epoch 8499 )\n",
      "[Epoch: 9220] train loss: 0.3935, train acc: 0.8386, val loss: 0.3457, val acc: 0.8503  (best train acc: 0.8511, best val acc: 0.8648, best train loss: 0.3711  @ epoch 8499 )\n",
      "[Epoch: 9240] train loss: 0.4028, train acc: 0.8244, val loss: 0.3413, val acc: 0.8573  (best train acc: 0.8511, best val acc: 0.8648, best train loss: 0.3711  @ epoch 8499 )\n",
      "[Epoch: 9260] train loss: 0.3708, train acc: 0.8422, val loss: 0.3631, val acc: 0.8469  (best train acc: 0.8511, best val acc: 0.8648, best train loss: 0.3698  @ epoch 9243 )\n",
      "[Epoch: 9280] train loss: 0.4202, train acc: 0.8237, val loss: 0.3341, val acc: 0.8570  (best train acc: 0.8511, best val acc: 0.8648, best train loss: 0.3695  @ epoch 9272 )\n",
      "[Epoch: 9300] train loss: 0.4473, train acc: 0.8081, val loss: 0.3265, val acc: 0.8610  (best train acc: 0.8511, best val acc: 0.8648, best train loss: 0.3695  @ epoch 9272 )\n",
      "[Epoch: 9320] train loss: 0.4271, train acc: 0.8159, val loss: 0.3411, val acc: 0.8462  (best train acc: 0.8511, best val acc: 0.8661, best train loss: 0.3695  @ epoch 9272 )\n",
      "[Epoch: 9340] train loss: 0.3854, train acc: 0.8409, val loss: 0.3311, val acc: 0.8587  (best train acc: 0.8511, best val acc: 0.8661, best train loss: 0.3695  @ epoch 9272 )\n",
      "[Epoch: 9360] train loss: 0.4327, train acc: 0.8086, val loss: 0.3544, val acc: 0.8395  (best train acc: 0.8511, best val acc: 0.8661, best train loss: 0.3695  @ epoch 9272 )\n",
      "[Epoch: 9380] train loss: 0.4000, train acc: 0.8287, val loss: 0.3363, val acc: 0.8580  (best train acc: 0.8511, best val acc: 0.8661, best train loss: 0.3695  @ epoch 9272 )\n",
      "[Epoch: 9400] train loss: 0.4200, train acc: 0.8139, val loss: 0.3520, val acc: 0.8479  (best train acc: 0.8511, best val acc: 0.8661, best train loss: 0.3662  @ epoch 9387 )\n",
      "[Epoch: 9420] train loss: 0.3954, train acc: 0.8310, val loss: 0.3336, val acc: 0.8597  (best train acc: 0.8511, best val acc: 0.8661, best train loss: 0.3662  @ epoch 9387 )\n",
      "[Epoch: 9440] train loss: 0.4104, train acc: 0.8239, val loss: 0.3399, val acc: 0.8607  (best train acc: 0.8511, best val acc: 0.8661, best train loss: 0.3662  @ epoch 9387 )\n",
      "[Epoch: 9460] train loss: 0.3894, train acc: 0.8383, val loss: 0.3355, val acc: 0.8617  (best train acc: 0.8511, best val acc: 0.8661, best train loss: 0.3662  @ epoch 9387 )\n",
      "[Epoch: 9480] train loss: 0.3851, train acc: 0.8373, val loss: 0.3358, val acc: 0.8610  (best train acc: 0.8511, best val acc: 0.8661, best train loss: 0.3648  @ epoch 9465 )\n",
      "[Epoch: 9500] train loss: 0.4592, train acc: 0.7992, val loss: 0.3365, val acc: 0.8587  (best train acc: 0.8511, best val acc: 0.8661, best train loss: 0.3648  @ epoch 9465 )\n",
      "[Epoch: 9520] train loss: 0.3973, train acc: 0.8330, val loss: 0.3334, val acc: 0.8580  (best train acc: 0.8511, best val acc: 0.8661, best train loss: 0.3616  @ epoch 9501 )\n",
      "[Epoch: 9540] train loss: 0.4182, train acc: 0.8264, val loss: 0.3359, val acc: 0.8550  (best train acc: 0.8511, best val acc: 0.8661, best train loss: 0.3616  @ epoch 9501 )\n",
      "[Epoch: 9560] train loss: 0.4085, train acc: 0.8302, val loss: 0.3363, val acc: 0.8513  (best train acc: 0.8511, best val acc: 0.8661, best train loss: 0.3616  @ epoch 9501 )\n",
      "[Epoch: 9580] train loss: 0.3656, train acc: 0.8454, val loss: 0.3330, val acc: 0.8546  (best train acc: 0.8511, best val acc: 0.8661, best train loss: 0.3616  @ epoch 9501 )\n",
      "[Epoch: 9600] train loss: 0.3809, train acc: 0.8389, val loss: 0.3296, val acc: 0.8563  (best train acc: 0.8511, best val acc: 0.8661, best train loss: 0.3616  @ epoch 9501 )\n",
      "[Epoch: 9620] train loss: 0.4077, train acc: 0.8306, val loss: 0.3452, val acc: 0.8503  (best train acc: 0.8511, best val acc: 0.8661, best train loss: 0.3616  @ epoch 9501 )\n",
      "[Epoch: 9640] train loss: 0.4029, train acc: 0.8300, val loss: 0.3735, val acc: 0.8428  (best train acc: 0.8511, best val acc: 0.8661, best train loss: 0.3616  @ epoch 9501 )\n",
      "[Epoch: 9660] train loss: 0.4260, train acc: 0.8208, val loss: 0.3625, val acc: 0.8486  (best train acc: 0.8511, best val acc: 0.8661, best train loss: 0.3616  @ epoch 9501 )\n",
      "[Epoch: 9680] train loss: 0.4412, train acc: 0.8096, val loss: 0.3420, val acc: 0.8577  (best train acc: 0.8511, best val acc: 0.8661, best train loss: 0.3616  @ epoch 9501 )\n",
      "[Epoch: 9700] train loss: 0.3900, train acc: 0.8362, val loss: 0.3342, val acc: 0.8583  (best train acc: 0.8511, best val acc: 0.8661, best train loss: 0.3616  @ epoch 9501 )\n",
      "[Epoch: 9720] train loss: 0.3868, train acc: 0.8344, val loss: 0.3403, val acc: 0.8590  (best train acc: 0.8511, best val acc: 0.8661, best train loss: 0.3616  @ epoch 9501 )\n",
      "[Epoch: 9740] train loss: 0.3790, train acc: 0.8365, val loss: 0.3416, val acc: 0.8530  (best train acc: 0.8511, best val acc: 0.8661, best train loss: 0.3611  @ epoch 9725 )\n",
      "[Epoch: 9760] train loss: 0.4474, train acc: 0.8053, val loss: 0.3496, val acc: 0.8496  (best train acc: 0.8511, best val acc: 0.8661, best train loss: 0.3611  @ epoch 9725 )\n",
      "[Epoch: 9780] train loss: 0.3777, train acc: 0.8425, val loss: 0.3384, val acc: 0.8590  (best train acc: 0.8516, best val acc: 0.8661, best train loss: 0.3562  @ epoch 9776 )\n",
      "[Epoch: 9800] train loss: 0.4175, train acc: 0.8274, val loss: 0.3314, val acc: 0.8600  (best train acc: 0.8516, best val acc: 0.8661, best train loss: 0.3562  @ epoch 9776 )\n",
      "[Epoch: 9820] train loss: 0.4365, train acc: 0.8137, val loss: 0.3641, val acc: 0.8371  (best train acc: 0.8516, best val acc: 0.8661, best train loss: 0.3562  @ epoch 9776 )\n",
      "[Epoch: 9840] train loss: 0.3911, train acc: 0.8287, val loss: 0.3658, val acc: 0.8398  (best train acc: 0.8516, best val acc: 0.8661, best train loss: 0.3562  @ epoch 9776 )\n",
      "[Epoch: 9860] train loss: 0.4008, train acc: 0.8278, val loss: 0.3523, val acc: 0.8482  (best train acc: 0.8516, best val acc: 0.8661, best train loss: 0.3562  @ epoch 9776 )\n",
      "[Epoch: 9880] train loss: 0.3955, train acc: 0.8300, val loss: 0.3304, val acc: 0.8600  (best train acc: 0.8516, best val acc: 0.8661, best train loss: 0.3562  @ epoch 9776 )\n",
      "[Epoch: 9900] train loss: 0.3925, train acc: 0.8319, val loss: 0.3539, val acc: 0.8465  (best train acc: 0.8516, best val acc: 0.8661, best train loss: 0.3562  @ epoch 9776 )\n",
      "[Epoch: 9920] train loss: 0.3655, train acc: 0.8424, val loss: 0.3354, val acc: 0.8590  (best train acc: 0.8516, best val acc: 0.8661, best train loss: 0.3562  @ epoch 9776 )\n",
      "[Epoch: 9940] train loss: 0.3919, train acc: 0.8359, val loss: 0.3342, val acc: 0.8583  (best train acc: 0.8516, best val acc: 0.8661, best train loss: 0.3562  @ epoch 9776 )\n",
      "[Epoch: 9960] train loss: 0.3958, train acc: 0.8313, val loss: 0.3291, val acc: 0.8573  (best train acc: 0.8516, best val acc: 0.8661, best train loss: 0.3562  @ epoch 9776 )\n",
      "[Epoch: 9980] train loss: 0.3711, train acc: 0.8430, val loss: 0.3281, val acc: 0.8614  (best train acc: 0.8516, best val acc: 0.8661, best train loss: 0.3562  @ epoch 9776 )\n",
      "[Epoch: 10000] train loss: 0.3781, train acc: 0.8430, val loss: 0.3296, val acc: 0.8610  (best train acc: 0.8516, best val acc: 0.8661, best train loss: 0.3562  @ epoch 9776 )\n",
      "[Epoch: 10020] train loss: 0.4192, train acc: 0.8209, val loss: 0.3654, val acc: 0.8415  (best train acc: 0.8516, best val acc: 0.8661, best train loss: 0.3562  @ epoch 9776 )\n",
      "[Epoch: 10040] train loss: 0.3895, train acc: 0.8370, val loss: 0.3347, val acc: 0.8600  (best train acc: 0.8519, best val acc: 0.8661, best train loss: 0.3562  @ epoch 9776 )\n",
      "[Epoch: 10060] train loss: 0.3850, train acc: 0.8406, val loss: 0.3407, val acc: 0.8597  (best train acc: 0.8519, best val acc: 0.8661, best train loss: 0.3562  @ epoch 9776 )\n",
      "[Epoch: 10080] train loss: 0.4428, train acc: 0.8079, val loss: 0.3494, val acc: 0.8567  (best train acc: 0.8519, best val acc: 0.8661, best train loss: 0.3562  @ epoch 9776 )\n",
      "[Epoch: 10100] train loss: 0.4134, train acc: 0.8278, val loss: 0.3667, val acc: 0.8361  (best train acc: 0.8519, best val acc: 0.8661, best train loss: 0.3562  @ epoch 9776 )\n",
      "[Epoch: 10120] train loss: 0.3623, train acc: 0.8454, val loss: 0.3547, val acc: 0.8445  (best train acc: 0.8519, best val acc: 0.8661, best train loss: 0.3562  @ epoch 9776 )\n",
      "[Epoch: 10140] train loss: 0.4011, train acc: 0.8349, val loss: 0.3284, val acc: 0.8624  (best train acc: 0.8519, best val acc: 0.8691, best train loss: 0.3562  @ epoch 9776 )\n",
      "[Epoch: 10160] train loss: 0.4075, train acc: 0.8237, val loss: 0.3294, val acc: 0.8617  (best train acc: 0.8519, best val acc: 0.8691, best train loss: 0.3562  @ epoch 9776 )\n",
      "[Epoch: 10180] train loss: 0.4014, train acc: 0.8281, val loss: 0.3461, val acc: 0.8496  (best train acc: 0.8519, best val acc: 0.8691, best train loss: 0.3562  @ epoch 9776 )\n",
      "[Epoch: 10200] train loss: 0.3969, train acc: 0.8351, val loss: 0.3484, val acc: 0.8530  (best train acc: 0.8519, best val acc: 0.8691, best train loss: 0.3562  @ epoch 9776 )\n",
      "[Epoch: 10220] train loss: 0.3959, train acc: 0.8347, val loss: 0.3351, val acc: 0.8648  (best train acc: 0.8519, best val acc: 0.8691, best train loss: 0.3562  @ epoch 9776 )\n",
      "[Epoch: 10240] train loss: 0.4128, train acc: 0.8223, val loss: 0.3383, val acc: 0.8577  (best train acc: 0.8519, best val acc: 0.8691, best train loss: 0.3562  @ epoch 9776 )\n",
      "[Epoch: 10260] train loss: 0.3779, train acc: 0.8428, val loss: 0.3580, val acc: 0.8432  (best train acc: 0.8519, best val acc: 0.8691, best train loss: 0.3562  @ epoch 9776 )\n",
      "[Epoch: 10280] train loss: 0.3751, train acc: 0.8463, val loss: 0.3598, val acc: 0.8432  (best train acc: 0.8519, best val acc: 0.8691, best train loss: 0.3562  @ epoch 9776 )\n",
      "[Epoch: 10300] train loss: 0.4054, train acc: 0.8286, val loss: 0.3454, val acc: 0.8496  (best train acc: 0.8519, best val acc: 0.8691, best train loss: 0.3562  @ epoch 9776 )\n",
      "[Epoch: 10320] train loss: 0.4101, train acc: 0.8240, val loss: 0.3294, val acc: 0.8610  (best train acc: 0.8519, best val acc: 0.8691, best train loss: 0.3562  @ epoch 9776 )\n",
      "[Epoch: 10340] train loss: 0.3908, train acc: 0.8359, val loss: 0.3389, val acc: 0.8533  (best train acc: 0.8519, best val acc: 0.8691, best train loss: 0.3562  @ epoch 9776 )\n",
      "[Epoch: 10360] train loss: 0.4130, train acc: 0.8270, val loss: 0.3379, val acc: 0.8583  (best train acc: 0.8519, best val acc: 0.8691, best train loss: 0.3562  @ epoch 9776 )\n",
      "[Epoch: 10380] train loss: 0.3798, train acc: 0.8426, val loss: 0.3270, val acc: 0.8600  (best train acc: 0.8519, best val acc: 0.8691, best train loss: 0.3562  @ epoch 9776 )\n",
      "[Epoch: 10400] train loss: 0.4610, train acc: 0.8003, val loss: 0.3388, val acc: 0.8560  (best train acc: 0.8519, best val acc: 0.8691, best train loss: 0.3562  @ epoch 9776 )\n",
      "[Epoch: 10420] train loss: 0.3776, train acc: 0.8438, val loss: 0.3418, val acc: 0.8597  (best train acc: 0.8519, best val acc: 0.8691, best train loss: 0.3562  @ epoch 9776 )\n",
      "[Epoch: 10440] train loss: 0.4224, train acc: 0.8220, val loss: 0.3709, val acc: 0.8358  (best train acc: 0.8519, best val acc: 0.8691, best train loss: 0.3562  @ epoch 9776 )\n",
      "[Epoch: 10460] train loss: 0.3637, train acc: 0.8459, val loss: 0.3360, val acc: 0.8573  (best train acc: 0.8519, best val acc: 0.8691, best train loss: 0.3562  @ epoch 9776 )\n",
      "[Epoch: 10480] train loss: 0.3757, train acc: 0.8411, val loss: 0.3838, val acc: 0.8273  (best train acc: 0.8519, best val acc: 0.8691, best train loss: 0.3562  @ epoch 9776 )\n",
      "[Epoch: 10500] train loss: 0.4537, train acc: 0.8011, val loss: 0.3443, val acc: 0.8509  (best train acc: 0.8519, best val acc: 0.8691, best train loss: 0.3562  @ epoch 9776 )\n",
      "[Epoch: 10520] train loss: 0.3849, train acc: 0.8403, val loss: 0.3439, val acc: 0.8492  (best train acc: 0.8519, best val acc: 0.8691, best train loss: 0.3562  @ epoch 9776 )\n",
      "[Epoch: 10540] train loss: 0.4088, train acc: 0.8325, val loss: 0.3290, val acc: 0.8607  (best train acc: 0.8519, best val acc: 0.8691, best train loss: 0.3562  @ epoch 9776 )\n",
      "[Epoch: 10560] train loss: 0.3887, train acc: 0.8397, val loss: 0.3423, val acc: 0.8573  (best train acc: 0.8519, best val acc: 0.8691, best train loss: 0.3562  @ epoch 9776 )\n",
      "[Epoch: 10580] train loss: 0.3779, train acc: 0.8402, val loss: 0.3375, val acc: 0.8553  (best train acc: 0.8519, best val acc: 0.8691, best train loss: 0.3562  @ epoch 9776 )\n",
      "[Epoch: 10600] train loss: 0.4077, train acc: 0.8282, val loss: 0.3186, val acc: 0.8654  (best train acc: 0.8519, best val acc: 0.8691, best train loss: 0.3562  @ epoch 9776 )\n",
      "[Epoch: 10620] train loss: 0.4408, train acc: 0.8096, val loss: 0.3382, val acc: 0.8583  (best train acc: 0.8519, best val acc: 0.8691, best train loss: 0.3562  @ epoch 9776 )\n",
      "[Epoch: 10640] train loss: 0.3968, train acc: 0.8363, val loss: 0.3272, val acc: 0.8610  (best train acc: 0.8519, best val acc: 0.8691, best train loss: 0.3562  @ epoch 9776 )\n",
      "[Epoch: 10660] train loss: 0.3957, train acc: 0.8323, val loss: 0.3191, val acc: 0.8664  (best train acc: 0.8519, best val acc: 0.8691, best train loss: 0.3562  @ epoch 9776 )\n",
      "[Epoch: 10680] train loss: 0.3723, train acc: 0.8383, val loss: 0.3199, val acc: 0.8637  (best train acc: 0.8519, best val acc: 0.8691, best train loss: 0.3534  @ epoch 10665 )\n",
      "[Epoch: 10700] train loss: 0.3958, train acc: 0.8342, val loss: 0.3273, val acc: 0.8567  (best train acc: 0.8528, best val acc: 0.8691, best train loss: 0.3534  @ epoch 10665 )\n",
      "[Epoch: 10720] train loss: 0.3837, train acc: 0.8325, val loss: 0.3493, val acc: 0.8455  (best train acc: 0.8549, best val acc: 0.8691, best train loss: 0.3534  @ epoch 10665 )\n",
      "[Epoch: 10740] train loss: 0.3944, train acc: 0.8285, val loss: 0.3252, val acc: 0.8600  (best train acc: 0.8549, best val acc: 0.8691, best train loss: 0.3534  @ epoch 10665 )\n",
      "[Epoch: 10760] train loss: 0.3828, train acc: 0.8369, val loss: 0.3212, val acc: 0.8624  (best train acc: 0.8549, best val acc: 0.8691, best train loss: 0.3534  @ epoch 10665 )\n",
      "[Epoch: 10780] train loss: 0.3715, train acc: 0.8415, val loss: 0.3349, val acc: 0.8614  (best train acc: 0.8549, best val acc: 0.8691, best train loss: 0.3534  @ epoch 10665 )\n",
      "[Epoch: 10800] train loss: 0.3689, train acc: 0.8420, val loss: 0.3244, val acc: 0.8610  (best train acc: 0.8549, best val acc: 0.8691, best train loss: 0.3534  @ epoch 10665 )\n",
      "[Epoch: 10820] train loss: 0.4121, train acc: 0.8344, val loss: 0.3262, val acc: 0.8627  (best train acc: 0.8549, best val acc: 0.8691, best train loss: 0.3534  @ epoch 10665 )\n",
      "[Epoch: 10840] train loss: 0.3769, train acc: 0.8407, val loss: 0.3249, val acc: 0.8607  (best train acc: 0.8549, best val acc: 0.8691, best train loss: 0.3534  @ epoch 10665 )\n",
      "[Epoch: 10860] train loss: 0.3697, train acc: 0.8430, val loss: 0.3189, val acc: 0.8668  (best train acc: 0.8549, best val acc: 0.8691, best train loss: 0.3534  @ epoch 10665 )\n",
      "[Epoch: 10880] train loss: 0.3652, train acc: 0.8438, val loss: 0.3383, val acc: 0.8546  (best train acc: 0.8549, best val acc: 0.8691, best train loss: 0.3534  @ epoch 10665 )\n",
      "[Epoch: 10900] train loss: 0.3777, train acc: 0.8425, val loss: 0.3360, val acc: 0.8573  (best train acc: 0.8549, best val acc: 0.8691, best train loss: 0.3534  @ epoch 10665 )\n",
      "[Epoch: 10920] train loss: 0.4212, train acc: 0.8229, val loss: 0.3289, val acc: 0.8614  (best train acc: 0.8594, best val acc: 0.8691, best train loss: 0.3524  @ epoch 10915 )\n",
      "[Epoch: 10940] train loss: 0.4255, train acc: 0.8254, val loss: 0.3250, val acc: 0.8651  (best train acc: 0.8594, best val acc: 0.8691, best train loss: 0.3524  @ epoch 10915 )\n",
      "[Epoch: 10960] train loss: 0.4994, train acc: 0.7819, val loss: 0.3348, val acc: 0.8573  (best train acc: 0.8594, best val acc: 0.8691, best train loss: 0.3524  @ epoch 10915 )\n",
      "[Epoch: 10980] train loss: 0.3767, train acc: 0.8397, val loss: 0.3392, val acc: 0.8553  (best train acc: 0.8594, best val acc: 0.8691, best train loss: 0.3524  @ epoch 10915 )\n",
      "[Epoch: 11000] train loss: 0.3865, train acc: 0.8380, val loss: 0.3389, val acc: 0.8573  (best train acc: 0.8594, best val acc: 0.8691, best train loss: 0.3524  @ epoch 10915 )\n",
      "[Epoch: 11020] train loss: 0.4185, train acc: 0.8162, val loss: 0.3549, val acc: 0.8492  (best train acc: 0.8594, best val acc: 0.8691, best train loss: 0.3524  @ epoch 10915 )\n",
      "[Epoch: 11040] train loss: 0.3777, train acc: 0.8416, val loss: 0.3354, val acc: 0.8553  (best train acc: 0.8594, best val acc: 0.8691, best train loss: 0.3524  @ epoch 10915 )\n",
      "[Epoch: 11060] train loss: 0.3812, train acc: 0.8399, val loss: 0.3316, val acc: 0.8553  (best train acc: 0.8594, best val acc: 0.8691, best train loss: 0.3524  @ epoch 10915 )\n",
      "[Epoch: 11080] train loss: 0.3862, train acc: 0.8360, val loss: 0.3422, val acc: 0.8560  (best train acc: 0.8594, best val acc: 0.8691, best train loss: 0.3524  @ epoch 10915 )\n",
      "[Epoch: 11100] train loss: 0.4280, train acc: 0.8098, val loss: 0.3495, val acc: 0.8486  (best train acc: 0.8594, best val acc: 0.8691, best train loss: 0.3524  @ epoch 10915 )\n",
      "[Epoch: 11120] train loss: 0.3836, train acc: 0.8368, val loss: 0.3308, val acc: 0.8590  (best train acc: 0.8594, best val acc: 0.8691, best train loss: 0.3524  @ epoch 10915 )\n",
      "[Epoch: 11140] train loss: 0.3833, train acc: 0.8297, val loss: 0.3233, val acc: 0.8631  (best train acc: 0.8594, best val acc: 0.8691, best train loss: 0.3524  @ epoch 10915 )\n",
      "[Epoch: 11160] train loss: 0.3896, train acc: 0.8352, val loss: 0.3262, val acc: 0.8631  (best train acc: 0.8594, best val acc: 0.8691, best train loss: 0.3502  @ epoch 11141 )\n",
      "[Epoch: 11180] train loss: 0.4001, train acc: 0.8301, val loss: 0.3258, val acc: 0.8634  (best train acc: 0.8594, best val acc: 0.8691, best train loss: 0.3502  @ epoch 11141 )\n",
      "[Epoch: 11200] train loss: 0.4045, train acc: 0.8235, val loss: 0.3370, val acc: 0.8553  (best train acc: 0.8594, best val acc: 0.8691, best train loss: 0.3475  @ epoch 11186 )\n",
      "[Epoch: 11220] train loss: 0.4238, train acc: 0.8158, val loss: 0.3596, val acc: 0.8398  (best train acc: 0.8594, best val acc: 0.8691, best train loss: 0.3475  @ epoch 11186 )\n",
      "[Epoch: 11240] train loss: 0.3760, train acc: 0.8404, val loss: 0.3407, val acc: 0.8553  (best train acc: 0.8594, best val acc: 0.8691, best train loss: 0.3475  @ epoch 11186 )\n",
      "[Epoch: 11260] train loss: 0.4003, train acc: 0.8283, val loss: 0.3174, val acc: 0.8648  (best train acc: 0.8594, best val acc: 0.8712, best train loss: 0.3475  @ epoch 11186 )\n",
      "[Epoch: 11280] train loss: 0.4151, train acc: 0.8254, val loss: 0.3395, val acc: 0.8519  (best train acc: 0.8594, best val acc: 0.8712, best train loss: 0.3475  @ epoch 11186 )\n",
      "[Epoch: 11300] train loss: 0.3603, train acc: 0.8486, val loss: 0.3286, val acc: 0.8624  (best train acc: 0.8594, best val acc: 0.8712, best train loss: 0.3475  @ epoch 11186 )\n",
      "[Epoch: 11320] train loss: 0.3643, train acc: 0.8494, val loss: 0.3312, val acc: 0.8583  (best train acc: 0.8594, best val acc: 0.8712, best train loss: 0.3431  @ epoch 11303 )\n",
      "[Epoch: 11340] train loss: 0.3663, train acc: 0.8394, val loss: 0.3208, val acc: 0.8621  (best train acc: 0.8594, best val acc: 0.8712, best train loss: 0.3431  @ epoch 11303 )\n",
      "[Epoch: 11360] train loss: 0.4159, train acc: 0.8147, val loss: 0.3320, val acc: 0.8570  (best train acc: 0.8594, best val acc: 0.8712, best train loss: 0.3431  @ epoch 11303 )\n",
      "[Epoch: 11380] train loss: 0.3708, train acc: 0.8446, val loss: 0.3104, val acc: 0.8678  (best train acc: 0.8594, best val acc: 0.8712, best train loss: 0.3431  @ epoch 11303 )\n",
      "[Epoch: 11400] train loss: 0.4077, train acc: 0.8312, val loss: 0.3361, val acc: 0.8580  (best train acc: 0.8594, best val acc: 0.8712, best train loss: 0.3431  @ epoch 11303 )\n",
      "[Epoch: 11420] train loss: 0.3710, train acc: 0.8423, val loss: 0.3166, val acc: 0.8648  (best train acc: 0.8594, best val acc: 0.8712, best train loss: 0.3431  @ epoch 11303 )\n",
      "[Epoch: 11440] train loss: 0.3639, train acc: 0.8436, val loss: 0.3304, val acc: 0.8573  (best train acc: 0.8594, best val acc: 0.8712, best train loss: 0.3431  @ epoch 11303 )\n",
      "[Epoch: 11460] train loss: 0.3876, train acc: 0.8336, val loss: 0.3279, val acc: 0.8590  (best train acc: 0.8594, best val acc: 0.8712, best train loss: 0.3431  @ epoch 11303 )\n",
      "[Epoch: 11480] train loss: 0.3640, train acc: 0.8453, val loss: 0.3098, val acc: 0.8644  (best train acc: 0.8594, best val acc: 0.8712, best train loss: 0.3431  @ epoch 11303 )\n",
      "[Epoch: 11500] train loss: 0.3988, train acc: 0.8319, val loss: 0.3246, val acc: 0.8637  (best train acc: 0.8594, best val acc: 0.8712, best train loss: 0.3431  @ epoch 11303 )\n",
      "[Epoch: 11520] train loss: 0.3959, train acc: 0.8323, val loss: 0.3538, val acc: 0.8465  (best train acc: 0.8594, best val acc: 0.8712, best train loss: 0.3431  @ epoch 11303 )\n",
      "[Epoch: 11540] train loss: 0.3784, train acc: 0.8395, val loss: 0.3481, val acc: 0.8506  (best train acc: 0.8594, best val acc: 0.8712, best train loss: 0.3431  @ epoch 11303 )\n",
      "[Epoch: 11560] train loss: 0.3928, train acc: 0.8317, val loss: 0.3318, val acc: 0.8546  (best train acc: 0.8594, best val acc: 0.8712, best train loss: 0.3431  @ epoch 11303 )\n",
      "[Epoch: 11580] train loss: 0.3996, train acc: 0.8352, val loss: 0.3224, val acc: 0.8631  (best train acc: 0.8594, best val acc: 0.8712, best train loss: 0.3431  @ epoch 11303 )\n",
      "[Epoch: 11600] train loss: 0.3532, train acc: 0.8463, val loss: 0.3240, val acc: 0.8654  (best train acc: 0.8594, best val acc: 0.8712, best train loss: 0.3431  @ epoch 11303 )\n",
      "[Epoch: 11620] train loss: 0.4087, train acc: 0.8222, val loss: 0.3230, val acc: 0.8604  (best train acc: 0.8594, best val acc: 0.8712, best train loss: 0.3431  @ epoch 11303 )\n",
      "[Epoch: 11640] train loss: 0.3707, train acc: 0.8433, val loss: 0.3339, val acc: 0.8546  (best train acc: 0.8594, best val acc: 0.8712, best train loss: 0.3431  @ epoch 11303 )\n",
      "[Epoch: 11660] train loss: 0.3917, train acc: 0.8324, val loss: 0.3367, val acc: 0.8570  (best train acc: 0.8594, best val acc: 0.8712, best train loss: 0.3431  @ epoch 11303 )\n",
      "[Epoch: 11680] train loss: 0.3954, train acc: 0.8284, val loss: 0.3064, val acc: 0.8607  (best train acc: 0.8594, best val acc: 0.8712, best train loss: 0.3431  @ epoch 11303 )\n",
      "[Epoch: 11700] train loss: 0.4374, train acc: 0.8108, val loss: 0.3366, val acc: 0.8570  (best train acc: 0.8594, best val acc: 0.8712, best train loss: 0.3431  @ epoch 11303 )\n",
      "[Epoch: 11720] train loss: 0.3837, train acc: 0.8406, val loss: 0.3193, val acc: 0.8705  (best train acc: 0.8594, best val acc: 0.8712, best train loss: 0.3431  @ epoch 11303 )\n",
      "[Epoch: 11740] train loss: 0.3721, train acc: 0.8405, val loss: 0.3235, val acc: 0.8583  (best train acc: 0.8594, best val acc: 0.8712, best train loss: 0.3431  @ epoch 11303 )\n",
      "[Epoch: 11760] train loss: 0.3548, train acc: 0.8451, val loss: 0.3386, val acc: 0.8496  (best train acc: 0.8594, best val acc: 0.8712, best train loss: 0.3431  @ epoch 11303 )\n",
      "[Epoch: 11780] train loss: 0.3475, train acc: 0.8527, val loss: 0.3042, val acc: 0.8634  (best train acc: 0.8594, best val acc: 0.8712, best train loss: 0.3374  @ epoch 11763 )\n",
      "[Epoch: 11800] train loss: 0.3482, train acc: 0.8496, val loss: 0.3056, val acc: 0.8661  (best train acc: 0.8615, best val acc: 0.8712, best train loss: 0.3342  @ epoch 11792 )\n",
      "[Epoch: 11820] train loss: 0.3616, train acc: 0.8457, val loss: 0.3244, val acc: 0.8553  (best train acc: 0.8615, best val acc: 0.8712, best train loss: 0.3342  @ epoch 11792 )\n",
      "[Epoch: 11840] train loss: 0.3497, train acc: 0.8464, val loss: 0.3073, val acc: 0.8695  (best train acc: 0.8615, best val acc: 0.8712, best train loss: 0.3331  @ epoch 11833 )\n",
      "[Epoch: 11860] train loss: 0.3726, train acc: 0.8411, val loss: 0.3356, val acc: 0.8533  (best train acc: 0.8615, best val acc: 0.8712, best train loss: 0.3331  @ epoch 11833 )\n",
      "[Epoch: 11880] train loss: 0.3449, train acc: 0.8516, val loss: 0.3143, val acc: 0.8678  (best train acc: 0.8615, best val acc: 0.8712, best train loss: 0.3331  @ epoch 11833 )\n",
      "[Epoch: 11900] train loss: 0.3273, train acc: 0.8611, val loss: 0.3134, val acc: 0.8644  (best train acc: 0.8615, best val acc: 0.8712, best train loss: 0.3273  @ epoch 11900 )\n",
      "[Epoch: 11920] train loss: 0.3795, train acc: 0.8355, val loss: 0.3711, val acc: 0.8317  (best train acc: 0.8615, best val acc: 0.8712, best train loss: 0.3269  @ epoch 11907 )\n",
      "[Epoch: 11940] train loss: 0.3447, train acc: 0.8524, val loss: 0.3192, val acc: 0.8664  (best train acc: 0.8615, best val acc: 0.8712, best train loss: 0.3269  @ epoch 11907 )\n",
      "[Epoch: 11960] train loss: 0.3752, train acc: 0.8409, val loss: 0.3443, val acc: 0.8513  (best train acc: 0.8615, best val acc: 0.8712, best train loss: 0.3269  @ epoch 11907 )\n",
      "[Epoch: 11980] train loss: 0.3592, train acc: 0.8456, val loss: 0.3080, val acc: 0.8658  (best train acc: 0.8615, best val acc: 0.8712, best train loss: 0.3269  @ epoch 11907 )\n",
      "[Epoch: 12000] train loss: 0.3441, train acc: 0.8553, val loss: 0.3133, val acc: 0.8637  (best train acc: 0.8615, best val acc: 0.8725, best train loss: 0.3269  @ epoch 11907 )\n",
      "[Epoch: 12020] train loss: 0.3583, train acc: 0.8475, val loss: 0.3106, val acc: 0.8678  (best train acc: 0.8618, best val acc: 0.8725, best train loss: 0.3269  @ epoch 11907 )\n",
      "[Epoch: 12040] train loss: 0.3400, train acc: 0.8522, val loss: 0.3083, val acc: 0.8681  (best train acc: 0.8618, best val acc: 0.8725, best train loss: 0.3269  @ epoch 11907 )\n",
      "[Epoch: 12060] train loss: 0.3611, train acc: 0.8458, val loss: 0.3266, val acc: 0.8570  (best train acc: 0.8618, best val acc: 0.8725, best train loss: 0.3269  @ epoch 11907 )\n",
      "[Epoch: 12080] train loss: 0.3857, train acc: 0.8316, val loss: 0.3241, val acc: 0.8627  (best train acc: 0.8618, best val acc: 0.8725, best train loss: 0.3269  @ epoch 11907 )\n",
      "[Epoch: 12100] train loss: 0.3450, train acc: 0.8550, val loss: 0.2984, val acc: 0.8675  (best train acc: 0.8618, best val acc: 0.8725, best train loss: 0.3269  @ epoch 11907 )\n",
      "[Epoch: 12120] train loss: 0.3345, train acc: 0.8593, val loss: 0.3250, val acc: 0.8580  (best train acc: 0.8634, best val acc: 0.8725, best train loss: 0.3269  @ epoch 11907 )\n",
      "[Epoch: 12140] train loss: 0.3498, train acc: 0.8547, val loss: 0.3107, val acc: 0.8634  (best train acc: 0.8634, best val acc: 0.8725, best train loss: 0.3269  @ epoch 11907 )\n",
      "[Epoch: 12160] train loss: 0.3527, train acc: 0.8494, val loss: 0.3115, val acc: 0.8641  (best train acc: 0.8650, best val acc: 0.8725, best train loss: 0.3269  @ epoch 11907 )\n",
      "[Epoch: 12180] train loss: 0.3431, train acc: 0.8621, val loss: 0.3083, val acc: 0.8621  (best train acc: 0.8650, best val acc: 0.8725, best train loss: 0.3269  @ epoch 11907 )\n",
      "[Epoch: 12200] train loss: 0.3436, train acc: 0.8562, val loss: 0.3123, val acc: 0.8735  (best train acc: 0.8650, best val acc: 0.8735, best train loss: 0.3245  @ epoch 12185 )\n",
      "[Epoch: 12220] train loss: 0.3617, train acc: 0.8454, val loss: 0.3077, val acc: 0.8691  (best train acc: 0.8650, best val acc: 0.8735, best train loss: 0.3245  @ epoch 12185 )\n",
      "[Epoch: 12240] train loss: 0.3488, train acc: 0.8536, val loss: 0.3145, val acc: 0.8671  (best train acc: 0.8650, best val acc: 0.8739, best train loss: 0.3245  @ epoch 12185 )\n",
      "[Epoch: 12260] train loss: 0.3476, train acc: 0.8524, val loss: 0.2999, val acc: 0.8664  (best train acc: 0.8658, best val acc: 0.8739, best train loss: 0.3245  @ epoch 12185 )\n",
      "[Epoch: 12280] train loss: 0.3366, train acc: 0.8581, val loss: 0.3034, val acc: 0.8675  (best train acc: 0.8658, best val acc: 0.8739, best train loss: 0.3194  @ epoch 12275 )\n",
      "[Epoch: 12300] train loss: 0.3957, train acc: 0.8366, val loss: 0.3084, val acc: 0.8702  (best train acc: 0.8658, best val acc: 0.8739, best train loss: 0.3194  @ epoch 12275 )\n",
      "[Epoch: 12320] train loss: 0.3250, train acc: 0.8657, val loss: 0.3206, val acc: 0.8664  (best train acc: 0.8658, best val acc: 0.8739, best train loss: 0.3194  @ epoch 12275 )\n",
      "[Epoch: 12340] train loss: 0.3595, train acc: 0.8489, val loss: 0.3190, val acc: 0.8644  (best train acc: 0.8658, best val acc: 0.8739, best train loss: 0.3194  @ epoch 12275 )\n",
      "[Epoch: 12360] train loss: 0.3281, train acc: 0.8612, val loss: 0.3002, val acc: 0.8695  (best train acc: 0.8658, best val acc: 0.8739, best train loss: 0.3194  @ epoch 12275 )\n",
      "[Epoch: 12380] train loss: 0.3575, train acc: 0.8560, val loss: 0.3119, val acc: 0.8648  (best train acc: 0.8658, best val acc: 0.8752, best train loss: 0.3194  @ epoch 12275 )\n",
      "[Epoch: 12400] train loss: 0.3408, train acc: 0.8478, val loss: 0.3011, val acc: 0.8702  (best train acc: 0.8658, best val acc: 0.8752, best train loss: 0.3194  @ epoch 12275 )\n",
      "[Epoch: 12420] train loss: 0.3246, train acc: 0.8643, val loss: 0.3045, val acc: 0.8678  (best train acc: 0.8658, best val acc: 0.8752, best train loss: 0.3194  @ epoch 12275 )\n",
      "[Epoch: 12440] train loss: 0.3620, train acc: 0.8459, val loss: 0.3122, val acc: 0.8648  (best train acc: 0.8658, best val acc: 0.8752, best train loss: 0.3194  @ epoch 12275 )\n",
      "[Epoch: 12460] train loss: 0.3577, train acc: 0.8488, val loss: 0.3153, val acc: 0.8631  (best train acc: 0.8658, best val acc: 0.8752, best train loss: 0.3194  @ epoch 12275 )\n",
      "[Epoch: 12480] train loss: 0.3456, train acc: 0.8574, val loss: 0.3346, val acc: 0.8516  (best train acc: 0.8658, best val acc: 0.8759, best train loss: 0.3194  @ epoch 12275 )\n",
      "[Epoch: 12500] train loss: 0.3486, train acc: 0.8548, val loss: 0.3067, val acc: 0.8695  (best train acc: 0.8658, best val acc: 0.8759, best train loss: 0.3194  @ epoch 12275 )\n",
      "[Epoch: 12520] train loss: 0.3246, train acc: 0.8663, val loss: 0.3118, val acc: 0.8661  (best train acc: 0.8683, best val acc: 0.8759, best train loss: 0.3190  @ epoch 12518 )\n",
      "[Epoch: 12540] train loss: 0.3547, train acc: 0.8584, val loss: 0.3022, val acc: 0.8648  (best train acc: 0.8683, best val acc: 0.8759, best train loss: 0.3190  @ epoch 12518 )\n",
      "[Epoch: 12560] train loss: 0.3389, train acc: 0.8519, val loss: 0.3058, val acc: 0.8691  (best train acc: 0.8683, best val acc: 0.8759, best train loss: 0.3190  @ epoch 12518 )\n",
      "[Epoch: 12580] train loss: 0.3467, train acc: 0.8532, val loss: 0.2965, val acc: 0.8668  (best train acc: 0.8683, best val acc: 0.8762, best train loss: 0.3190  @ epoch 12518 )\n",
      "[Epoch: 12600] train loss: 0.3683, train acc: 0.8442, val loss: 0.2983, val acc: 0.8691  (best train acc: 0.8683, best val acc: 0.8762, best train loss: 0.3190  @ epoch 12518 )\n",
      "[Epoch: 12620] train loss: 0.3836, train acc: 0.8383, val loss: 0.3071, val acc: 0.8651  (best train acc: 0.8683, best val acc: 0.8762, best train loss: 0.3190  @ epoch 12518 )\n",
      "[Epoch: 12640] train loss: 0.3448, train acc: 0.8598, val loss: 0.3138, val acc: 0.8637  (best train acc: 0.8683, best val acc: 0.8762, best train loss: 0.3190  @ epoch 12518 )\n",
      "[Epoch: 12660] train loss: 0.3268, train acc: 0.8610, val loss: 0.3035, val acc: 0.8681  (best train acc: 0.8683, best val acc: 0.8762, best train loss: 0.3190  @ epoch 12518 )\n",
      "[Epoch: 12680] train loss: 0.3472, train acc: 0.8546, val loss: 0.3067, val acc: 0.8654  (best train acc: 0.8683, best val acc: 0.8762, best train loss: 0.3190  @ epoch 12518 )\n",
      "[Epoch: 12700] train loss: 0.3454, train acc: 0.8508, val loss: 0.3065, val acc: 0.8671  (best train acc: 0.8710, best val acc: 0.8762, best train loss: 0.3180  @ epoch 12697 )\n",
      "[Epoch: 12720] train loss: 0.3785, train acc: 0.8434, val loss: 0.3045, val acc: 0.8691  (best train acc: 0.8710, best val acc: 0.8762, best train loss: 0.3171  @ epoch 12709 )\n",
      "[Epoch: 12740] train loss: 0.3426, train acc: 0.8578, val loss: 0.3116, val acc: 0.8641  (best train acc: 0.8710, best val acc: 0.8762, best train loss: 0.3171  @ epoch 12709 )\n",
      "[Epoch: 12760] train loss: 0.4158, train acc: 0.8279, val loss: 0.2996, val acc: 0.8712  (best train acc: 0.8710, best val acc: 0.8762, best train loss: 0.3171  @ epoch 12709 )\n",
      "[Epoch: 12780] train loss: 0.3382, train acc: 0.8566, val loss: 0.3069, val acc: 0.8685  (best train acc: 0.8710, best val acc: 0.8762, best train loss: 0.3171  @ epoch 12709 )\n",
      "[Epoch: 12800] train loss: 0.3461, train acc: 0.8596, val loss: 0.2989, val acc: 0.8691  (best train acc: 0.8710, best val acc: 0.8762, best train loss: 0.3171  @ epoch 12709 )\n",
      "[Epoch: 12820] train loss: 0.3597, train acc: 0.8498, val loss: 0.3341, val acc: 0.8519  (best train acc: 0.8710, best val acc: 0.8762, best train loss: 0.3171  @ epoch 12709 )\n",
      "[Epoch: 12840] train loss: 0.3454, train acc: 0.8527, val loss: 0.3122, val acc: 0.8634  (best train acc: 0.8710, best val acc: 0.8762, best train loss: 0.3171  @ epoch 12709 )\n",
      "[Epoch: 12860] train loss: 0.3434, train acc: 0.8555, val loss: 0.3032, val acc: 0.8718  (best train acc: 0.8710, best val acc: 0.8762, best train loss: 0.3171  @ epoch 12709 )\n",
      "[Epoch: 12880] train loss: 0.3541, train acc: 0.8483, val loss: 0.3192, val acc: 0.8648  (best train acc: 0.8710, best val acc: 0.8762, best train loss: 0.3171  @ epoch 12709 )\n",
      "[Epoch: 12900] train loss: 0.3673, train acc: 0.8444, val loss: 0.3047, val acc: 0.8678  (best train acc: 0.8710, best val acc: 0.8762, best train loss: 0.3171  @ epoch 12709 )\n",
      "[Epoch: 12920] train loss: 0.3255, train acc: 0.8578, val loss: 0.2977, val acc: 0.8634  (best train acc: 0.8710, best val acc: 0.8762, best train loss: 0.3134  @ epoch 12913 )\n",
      "[Epoch: 12940] train loss: 0.3072, train acc: 0.8722, val loss: 0.3057, val acc: 0.8678  (best train acc: 0.8722, best val acc: 0.8762, best train loss: 0.3072  @ epoch 12940 )\n",
      "[Epoch: 12960] train loss: 0.3332, train acc: 0.8594, val loss: 0.2943, val acc: 0.8695  (best train acc: 0.8722, best val acc: 0.8762, best train loss: 0.3072  @ epoch 12940 )\n",
      "[Epoch: 12980] train loss: 0.3296, train acc: 0.8588, val loss: 0.3175, val acc: 0.8621  (best train acc: 0.8722, best val acc: 0.8762, best train loss: 0.3072  @ epoch 12940 )\n",
      "[Epoch: 13000] train loss: 0.3755, train acc: 0.8472, val loss: 0.3061, val acc: 0.8624  (best train acc: 0.8722, best val acc: 0.8762, best train loss: 0.3072  @ epoch 12940 )\n",
      "[Epoch: 13020] train loss: 0.3403, train acc: 0.8548, val loss: 0.2981, val acc: 0.8725  (best train acc: 0.8722, best val acc: 0.8762, best train loss: 0.3072  @ epoch 12940 )\n",
      "[Epoch: 13040] train loss: 0.3392, train acc: 0.8530, val loss: 0.3061, val acc: 0.8668  (best train acc: 0.8722, best val acc: 0.8762, best train loss: 0.3072  @ epoch 12940 )\n",
      "[Epoch: 13060] train loss: 0.3806, train acc: 0.8372, val loss: 0.3008, val acc: 0.8695  (best train acc: 0.8722, best val acc: 0.8762, best train loss: 0.3072  @ epoch 12940 )\n",
      "[Epoch: 13080] train loss: 0.3335, train acc: 0.8603, val loss: 0.3025, val acc: 0.8705  (best train acc: 0.8722, best val acc: 0.8762, best train loss: 0.3072  @ epoch 12940 )\n",
      "[Epoch: 13100] train loss: 0.3598, train acc: 0.8540, val loss: 0.2944, val acc: 0.8732  (best train acc: 0.8722, best val acc: 0.8762, best train loss: 0.3072  @ epoch 12940 )\n",
      "[Epoch: 13120] train loss: 0.3269, train acc: 0.8600, val loss: 0.2963, val acc: 0.8742  (best train acc: 0.8722, best val acc: 0.8762, best train loss: 0.3072  @ epoch 12940 )\n",
      "[Epoch: 13140] train loss: 0.3809, train acc: 0.8460, val loss: 0.3074, val acc: 0.8695  (best train acc: 0.8728, best val acc: 0.8762, best train loss: 0.3072  @ epoch 12940 )\n",
      "[Epoch: 13160] train loss: 0.3365, train acc: 0.8576, val loss: 0.3054, val acc: 0.8722  (best train acc: 0.8728, best val acc: 0.8762, best train loss: 0.3072  @ epoch 12940 )\n",
      "[Epoch: 13180] train loss: 0.3309, train acc: 0.8598, val loss: 0.3218, val acc: 0.8580  (best train acc: 0.8728, best val acc: 0.8762, best train loss: 0.3072  @ epoch 12940 )\n",
      "[Epoch: 13200] train loss: 0.3325, train acc: 0.8612, val loss: 0.3003, val acc: 0.8698  (best train acc: 0.8728, best val acc: 0.8762, best train loss: 0.3072  @ epoch 12940 )\n",
      "[Epoch: 13220] train loss: 0.3263, train acc: 0.8623, val loss: 0.2960, val acc: 0.8739  (best train acc: 0.8728, best val acc: 0.8762, best train loss: 0.3072  @ epoch 12940 )\n",
      "[Epoch: 13240] train loss: 0.3696, train acc: 0.8447, val loss: 0.3114, val acc: 0.8671  (best train acc: 0.8728, best val acc: 0.8766, best train loss: 0.3072  @ epoch 12940 )\n",
      "[Epoch: 13260] train loss: 0.3201, train acc: 0.8720, val loss: 0.2931, val acc: 0.8742  (best train acc: 0.8728, best val acc: 0.8766, best train loss: 0.3072  @ epoch 12940 )\n",
      "[Epoch: 13280] train loss: 0.3948, train acc: 0.8346, val loss: 0.3016, val acc: 0.8644  (best train acc: 0.8728, best val acc: 0.8766, best train loss: 0.3072  @ epoch 12940 )\n",
      "[Epoch: 13300] train loss: 0.3347, train acc: 0.8568, val loss: 0.2955, val acc: 0.8722  (best train acc: 0.8728, best val acc: 0.8766, best train loss: 0.3072  @ epoch 12940 )\n",
      "[Epoch: 13320] train loss: 0.3616, train acc: 0.8481, val loss: 0.2877, val acc: 0.8752  (best train acc: 0.8728, best val acc: 0.8766, best train loss: 0.3072  @ epoch 12940 )\n",
      "[Epoch: 13340] train loss: 0.3296, train acc: 0.8569, val loss: 0.2994, val acc: 0.8691  (best train acc: 0.8728, best val acc: 0.8766, best train loss: 0.3072  @ epoch 12940 )\n",
      "[Epoch: 13360] train loss: 0.3421, train acc: 0.8526, val loss: 0.3017, val acc: 0.8722  (best train acc: 0.8728, best val acc: 0.8766, best train loss: 0.3072  @ epoch 12940 )\n",
      "[Epoch: 13380] train loss: 0.3549, train acc: 0.8530, val loss: 0.2969, val acc: 0.8759  (best train acc: 0.8728, best val acc: 0.8772, best train loss: 0.3072  @ epoch 12940 )\n",
      "[Epoch: 13400] train loss: 0.3469, train acc: 0.8584, val loss: 0.2952, val acc: 0.8732  (best train acc: 0.8728, best val acc: 0.8772, best train loss: 0.3072  @ epoch 12940 )\n",
      "[Epoch: 13420] train loss: 0.3417, train acc: 0.8567, val loss: 0.3042, val acc: 0.8695  (best train acc: 0.8728, best val acc: 0.8772, best train loss: 0.3072  @ epoch 12940 )\n",
      "[Epoch: 13440] train loss: 0.3227, train acc: 0.8633, val loss: 0.2956, val acc: 0.8691  (best train acc: 0.8728, best val acc: 0.8772, best train loss: 0.3072  @ epoch 12940 )\n",
      "[Epoch: 13460] train loss: 0.3394, train acc: 0.8579, val loss: 0.2982, val acc: 0.8695  (best train acc: 0.8728, best val acc: 0.8772, best train loss: 0.3072  @ epoch 12940 )\n",
      "[Epoch: 13480] train loss: 0.3320, train acc: 0.8566, val loss: 0.2911, val acc: 0.8728  (best train acc: 0.8728, best val acc: 0.8772, best train loss: 0.3072  @ epoch 12940 )\n",
      "[Epoch: 13500] train loss: 0.3603, train acc: 0.8529, val loss: 0.3131, val acc: 0.8641  (best train acc: 0.8728, best val acc: 0.8776, best train loss: 0.3072  @ epoch 12940 )\n",
      "[Epoch: 13520] train loss: 0.3310, train acc: 0.8600, val loss: 0.3087, val acc: 0.8634  (best train acc: 0.8728, best val acc: 0.8776, best train loss: 0.3072  @ epoch 12940 )\n",
      "[Epoch: 13540] train loss: 0.3407, train acc: 0.8626, val loss: 0.3426, val acc: 0.8388  (best train acc: 0.8728, best val acc: 0.8776, best train loss: 0.3072  @ epoch 12940 )\n",
      "[Epoch: 13560] train loss: 0.3175, train acc: 0.8681, val loss: 0.2958, val acc: 0.8745  (best train acc: 0.8728, best val acc: 0.8803, best train loss: 0.3072  @ epoch 12940 )\n",
      "[Epoch: 13580] train loss: 0.3304, train acc: 0.8584, val loss: 0.2935, val acc: 0.8681  (best train acc: 0.8728, best val acc: 0.8803, best train loss: 0.3072  @ epoch 12940 )\n",
      "[Epoch: 13600] train loss: 0.3541, train acc: 0.8499, val loss: 0.2969, val acc: 0.8732  (best train acc: 0.8728, best val acc: 0.8803, best train loss: 0.3072  @ epoch 12940 )\n",
      "[Epoch: 13620] train loss: 0.3655, train acc: 0.8488, val loss: 0.3016, val acc: 0.8654  (best train acc: 0.8728, best val acc: 0.8803, best train loss: 0.3072  @ epoch 12940 )\n",
      "[Epoch: 13640] train loss: 0.3364, train acc: 0.8592, val loss: 0.2976, val acc: 0.8739  (best train acc: 0.8728, best val acc: 0.8803, best train loss: 0.3072  @ epoch 12940 )\n",
      "[Epoch: 13660] train loss: 0.3165, train acc: 0.8700, val loss: 0.2890, val acc: 0.8752  (best train acc: 0.8728, best val acc: 0.8803, best train loss: 0.3072  @ epoch 12940 )\n",
      "[Epoch: 13680] train loss: 0.3119, train acc: 0.8641, val loss: 0.2838, val acc: 0.8749  (best train acc: 0.8736, best val acc: 0.8803, best train loss: 0.3035  @ epoch 13668 )\n",
      "[Epoch: 13700] train loss: 0.3580, train acc: 0.8524, val loss: 0.2985, val acc: 0.8732  (best train acc: 0.8736, best val acc: 0.8803, best train loss: 0.3035  @ epoch 13668 )\n",
      "[Epoch: 13720] train loss: 0.3483, train acc: 0.8546, val loss: 0.2993, val acc: 0.8739  (best train acc: 0.8736, best val acc: 0.8803, best train loss: 0.3035  @ epoch 13668 )\n",
      "[Epoch: 13740] train loss: 0.3844, train acc: 0.8370, val loss: 0.3503, val acc: 0.8449  (best train acc: 0.8736, best val acc: 0.8803, best train loss: 0.3035  @ epoch 13668 )\n",
      "[Epoch: 13760] train loss: 0.3246, train acc: 0.8639, val loss: 0.2886, val acc: 0.8735  (best train acc: 0.8736, best val acc: 0.8803, best train loss: 0.3035  @ epoch 13668 )\n",
      "[Epoch: 13780] train loss: 0.3495, train acc: 0.8540, val loss: 0.3093, val acc: 0.8658  (best train acc: 0.8736, best val acc: 0.8803, best train loss: 0.3035  @ epoch 13668 )\n",
      "[Epoch: 13800] train loss: 0.3162, train acc: 0.8639, val loss: 0.2916, val acc: 0.8718  (best train acc: 0.8736, best val acc: 0.8803, best train loss: 0.3035  @ epoch 13668 )\n",
      "[Epoch: 13820] train loss: 0.3335, train acc: 0.8569, val loss: 0.3098, val acc: 0.8691  (best train acc: 0.8736, best val acc: 0.8803, best train loss: 0.3035  @ epoch 13668 )\n",
      "[Epoch: 13840] train loss: 0.3547, train acc: 0.8524, val loss: 0.2968, val acc: 0.8728  (best train acc: 0.8736, best val acc: 0.8803, best train loss: 0.3035  @ epoch 13668 )\n",
      "[Epoch: 13860] train loss: 0.3139, train acc: 0.8702, val loss: 0.3031, val acc: 0.8634  (best train acc: 0.8736, best val acc: 0.8803, best train loss: 0.3035  @ epoch 13668 )\n",
      "[Epoch: 13880] train loss: 0.3442, train acc: 0.8576, val loss: 0.2947, val acc: 0.8695  (best train acc: 0.8736, best val acc: 0.8803, best train loss: 0.3035  @ epoch 13668 )\n",
      "[Epoch: 13900] train loss: 0.3238, train acc: 0.8637, val loss: 0.2969, val acc: 0.8658  (best train acc: 0.8736, best val acc: 0.8803, best train loss: 0.3035  @ epoch 13668 )\n",
      "[Epoch: 13920] train loss: 0.3491, train acc: 0.8476, val loss: 0.2941, val acc: 0.8725  (best train acc: 0.8736, best val acc: 0.8803, best train loss: 0.3035  @ epoch 13668 )\n",
      "[Epoch: 13940] train loss: 0.3312, train acc: 0.8606, val loss: 0.2940, val acc: 0.8742  (best train acc: 0.8736, best val acc: 0.8803, best train loss: 0.3035  @ epoch 13668 )\n",
      "[Epoch: 13960] train loss: 0.3268, train acc: 0.8638, val loss: 0.3006, val acc: 0.8681  (best train acc: 0.8736, best val acc: 0.8803, best train loss: 0.3035  @ epoch 13668 )\n",
      "[Epoch: 13980] train loss: 0.3514, train acc: 0.8517, val loss: 0.3058, val acc: 0.8698  (best train acc: 0.8736, best val acc: 0.8803, best train loss: 0.3035  @ epoch 13668 )\n",
      "[Epoch: 14000] train loss: 0.3473, train acc: 0.8561, val loss: 0.2945, val acc: 0.8742  (best train acc: 0.8736, best val acc: 0.8803, best train loss: 0.3035  @ epoch 13668 )\n",
      "[Epoch: 14020] train loss: 0.3524, train acc: 0.8521, val loss: 0.2955, val acc: 0.8735  (best train acc: 0.8736, best val acc: 0.8803, best train loss: 0.3035  @ epoch 13668 )\n",
      "[Epoch: 14040] train loss: 0.3272, train acc: 0.8581, val loss: 0.2967, val acc: 0.8685  (best train acc: 0.8736, best val acc: 0.8803, best train loss: 0.3035  @ epoch 13668 )\n",
      "[Epoch: 14060] train loss: 0.3440, train acc: 0.8529, val loss: 0.3030, val acc: 0.8664  (best train acc: 0.8736, best val acc: 0.8803, best train loss: 0.3035  @ epoch 13668 )\n",
      "[Epoch: 14080] train loss: 0.3471, train acc: 0.8506, val loss: 0.2954, val acc: 0.8722  (best train acc: 0.8736, best val acc: 0.8803, best train loss: 0.3035  @ epoch 13668 )\n",
      "[Epoch: 14100] train loss: 0.3253, train acc: 0.8613, val loss: 0.3153, val acc: 0.8617  (best train acc: 0.8740, best val acc: 0.8803, best train loss: 0.3035  @ epoch 13668 )\n",
      "[Epoch: 14120] train loss: 0.3481, train acc: 0.8561, val loss: 0.3069, val acc: 0.8634  (best train acc: 0.8740, best val acc: 0.8803, best train loss: 0.3035  @ epoch 13668 )\n",
      "[Epoch: 14140] train loss: 0.3526, train acc: 0.8622, val loss: 0.2997, val acc: 0.8708  (best train acc: 0.8740, best val acc: 0.8803, best train loss: 0.3035  @ epoch 13668 )\n",
      "[Epoch: 14160] train loss: 0.3184, train acc: 0.8661, val loss: 0.3082, val acc: 0.8627  (best train acc: 0.8740, best val acc: 0.8803, best train loss: 0.3035  @ epoch 13668 )\n",
      "[Epoch: 14180] train loss: 0.3353, train acc: 0.8641, val loss: 0.2979, val acc: 0.8675  (best train acc: 0.8740, best val acc: 0.8803, best train loss: 0.3035  @ epoch 13668 )\n",
      "[Epoch: 14200] train loss: 0.3147, train acc: 0.8644, val loss: 0.2961, val acc: 0.8691  (best train acc: 0.8740, best val acc: 0.8803, best train loss: 0.3035  @ epoch 13668 )\n",
      "[Epoch: 14220] train loss: 0.3360, train acc: 0.8628, val loss: 0.2917, val acc: 0.8712  (best train acc: 0.8740, best val acc: 0.8803, best train loss: 0.3035  @ epoch 13668 )\n",
      "[Epoch: 14240] train loss: 0.3143, train acc: 0.8660, val loss: 0.2886, val acc: 0.8732  (best train acc: 0.8744, best val acc: 0.8803, best train loss: 0.3035  @ epoch 13668 )\n",
      "[Epoch: 14260] train loss: 0.3323, train acc: 0.8575, val loss: 0.2909, val acc: 0.8722  (best train acc: 0.8744, best val acc: 0.8803, best train loss: 0.3035  @ epoch 13668 )\n",
      "[Epoch: 14280] train loss: 0.3352, train acc: 0.8605, val loss: 0.2909, val acc: 0.8718  (best train acc: 0.8744, best val acc: 0.8803, best train loss: 0.3035  @ epoch 13668 )\n",
      "[Epoch: 14300] train loss: 0.3911, train acc: 0.8365, val loss: 0.3346, val acc: 0.8533  (best train acc: 0.8744, best val acc: 0.8803, best train loss: 0.3035  @ epoch 13668 )\n",
      "[Epoch: 14320] train loss: 0.3437, train acc: 0.8537, val loss: 0.3118, val acc: 0.8627  (best train acc: 0.8744, best val acc: 0.8803, best train loss: 0.3035  @ epoch 13668 )\n",
      "[Epoch: 14340] train loss: 0.3295, train acc: 0.8670, val loss: 0.2883, val acc: 0.8752  (best train acc: 0.8744, best val acc: 0.8803, best train loss: 0.3000  @ epoch 14332 )\n",
      "[Epoch: 14360] train loss: 0.3388, train acc: 0.8631, val loss: 0.2880, val acc: 0.8766  (best train acc: 0.8744, best val acc: 0.8803, best train loss: 0.3000  @ epoch 14332 )\n",
      "[Epoch: 14380] train loss: 0.3346, train acc: 0.8577, val loss: 0.3000, val acc: 0.8688  (best train acc: 0.8744, best val acc: 0.8803, best train loss: 0.3000  @ epoch 14332 )\n",
      "[Epoch: 14400] train loss: 0.3749, train acc: 0.8484, val loss: 0.3138, val acc: 0.8627  (best train acc: 0.8744, best val acc: 0.8803, best train loss: 0.3000  @ epoch 14332 )\n",
      "[Epoch: 14420] train loss: 0.3223, train acc: 0.8634, val loss: 0.2923, val acc: 0.8698  (best train acc: 0.8744, best val acc: 0.8803, best train loss: 0.3000  @ epoch 14332 )\n",
      "[Epoch: 14440] train loss: 0.3099, train acc: 0.8689, val loss: 0.2960, val acc: 0.8685  (best train acc: 0.8756, best val acc: 0.8803, best train loss: 0.3000  @ epoch 14332 )\n",
      "[Epoch: 14460] train loss: 0.3121, train acc: 0.8668, val loss: 0.3021, val acc: 0.8691  (best train acc: 0.8756, best val acc: 0.8803, best train loss: 0.3000  @ epoch 14332 )\n",
      "[Epoch: 14480] train loss: 0.3581, train acc: 0.8556, val loss: 0.2882, val acc: 0.8671  (best train acc: 0.8756, best val acc: 0.8803, best train loss: 0.3000  @ epoch 14332 )\n",
      "[Epoch: 14500] train loss: 0.3323, train acc: 0.8640, val loss: 0.2946, val acc: 0.8762  (best train acc: 0.8760, best val acc: 0.8803, best train loss: 0.3000  @ epoch 14332 )\n",
      "[Epoch: 14520] train loss: 0.3657, train acc: 0.8425, val loss: 0.3263, val acc: 0.8543  (best train acc: 0.8760, best val acc: 0.8803, best train loss: 0.2994  @ epoch 14505 )\n",
      "[Epoch: 14540] train loss: 0.3238, train acc: 0.8676, val loss: 0.2923, val acc: 0.8725  (best train acc: 0.8760, best val acc: 0.8803, best train loss: 0.2994  @ epoch 14505 )\n",
      "[Epoch: 14560] train loss: 0.3349, train acc: 0.8630, val loss: 0.2962, val acc: 0.8644  (best train acc: 0.8760, best val acc: 0.8803, best train loss: 0.2994  @ epoch 14505 )\n",
      "[Epoch: 14580] train loss: 0.3228, train acc: 0.8635, val loss: 0.2974, val acc: 0.8668  (best train acc: 0.8760, best val acc: 0.8803, best train loss: 0.2994  @ epoch 14505 )\n",
      "[Epoch: 14600] train loss: 0.3202, train acc: 0.8691, val loss: 0.3034, val acc: 0.8651  (best train acc: 0.8760, best val acc: 0.8803, best train loss: 0.2994  @ epoch 14505 )\n",
      "[Epoch: 14620] train loss: 0.3268, train acc: 0.8612, val loss: 0.3115, val acc: 0.8604  (best train acc: 0.8760, best val acc: 0.8803, best train loss: 0.2994  @ epoch 14505 )\n",
      "[Epoch: 14640] train loss: 0.3389, train acc: 0.8571, val loss: 0.2989, val acc: 0.8668  (best train acc: 0.8760, best val acc: 0.8803, best train loss: 0.2994  @ epoch 14505 )\n",
      "[Epoch: 14660] train loss: 0.3223, train acc: 0.8601, val loss: 0.3005, val acc: 0.8654  (best train acc: 0.8760, best val acc: 0.8803, best train loss: 0.2994  @ epoch 14505 )\n",
      "[Epoch: 14680] train loss: 0.3169, train acc: 0.8663, val loss: 0.3028, val acc: 0.8661  (best train acc: 0.8760, best val acc: 0.8803, best train loss: 0.2994  @ epoch 14505 )\n",
      "[Epoch: 14700] train loss: 0.3186, train acc: 0.8708, val loss: 0.3102, val acc: 0.8614  (best train acc: 0.8760, best val acc: 0.8803, best train loss: 0.2994  @ epoch 14505 )\n",
      "[Epoch: 14720] train loss: 0.3529, train acc: 0.8458, val loss: 0.3009, val acc: 0.8637  (best train acc: 0.8764, best val acc: 0.8803, best train loss: 0.2945  @ epoch 14702 )\n",
      "[Epoch: 14740] train loss: 0.3355, train acc: 0.8627, val loss: 0.2821, val acc: 0.8789  (best train acc: 0.8764, best val acc: 0.8803, best train loss: 0.2945  @ epoch 14702 )\n",
      "[Epoch: 14760] train loss: 0.3244, train acc: 0.8670, val loss: 0.2832, val acc: 0.8769  (best train acc: 0.8764, best val acc: 0.8803, best train loss: 0.2945  @ epoch 14702 )\n",
      "[Epoch: 14780] train loss: 0.3275, train acc: 0.8607, val loss: 0.2877, val acc: 0.8678  (best train acc: 0.8764, best val acc: 0.8803, best train loss: 0.2945  @ epoch 14702 )\n",
      "[Epoch: 14800] train loss: 0.4136, train acc: 0.8300, val loss: 0.2940, val acc: 0.8695  (best train acc: 0.8789, best val acc: 0.8803, best train loss: 0.2945  @ epoch 14702 )\n",
      "[Epoch: 14820] train loss: 0.3414, train acc: 0.8593, val loss: 0.2953, val acc: 0.8715  (best train acc: 0.8789, best val acc: 0.8803, best train loss: 0.2945  @ epoch 14702 )\n",
      "[Epoch: 14840] train loss: 0.3192, train acc: 0.8655, val loss: 0.2883, val acc: 0.8776  (best train acc: 0.8789, best val acc: 0.8813, best train loss: 0.2945  @ epoch 14702 )\n",
      "[Epoch: 14860] train loss: 0.3209, train acc: 0.8665, val loss: 0.2915, val acc: 0.8732  (best train acc: 0.8789, best val acc: 0.8813, best train loss: 0.2945  @ epoch 14702 )\n",
      "[Epoch: 14880] train loss: 0.3236, train acc: 0.8590, val loss: 0.3021, val acc: 0.8735  (best train acc: 0.8789, best val acc: 0.8813, best train loss: 0.2945  @ epoch 14702 )\n",
      "[Epoch: 14900] train loss: 0.3554, train acc: 0.8558, val loss: 0.2984, val acc: 0.8685  (best train acc: 0.8789, best val acc: 0.8813, best train loss: 0.2945  @ epoch 14702 )\n",
      "[Epoch: 14920] train loss: 0.3104, train acc: 0.8689, val loss: 0.2881, val acc: 0.8735  (best train acc: 0.8789, best val acc: 0.8813, best train loss: 0.2945  @ epoch 14702 )\n",
      "[Epoch: 14940] train loss: 0.3303, train acc: 0.8661, val loss: 0.2866, val acc: 0.8772  (best train acc: 0.8789, best val acc: 0.8813, best train loss: 0.2945  @ epoch 14702 )\n",
      "[Epoch: 14960] train loss: 0.3225, train acc: 0.8689, val loss: 0.3034, val acc: 0.8651  (best train acc: 0.8789, best val acc: 0.8813, best train loss: 0.2945  @ epoch 14702 )\n",
      "[Epoch: 14980] train loss: 0.3239, train acc: 0.8671, val loss: 0.2920, val acc: 0.8735  (best train acc: 0.8789, best val acc: 0.8813, best train loss: 0.2945  @ epoch 14702 )\n",
      "[Epoch: 15000] train loss: 0.3720, train acc: 0.8525, val loss: 0.2906, val acc: 0.8772  (best train acc: 0.8789, best val acc: 0.8813, best train loss: 0.2945  @ epoch 14702 )\n",
      "[Epoch: 15020] train loss: 0.3367, train acc: 0.8633, val loss: 0.2962, val acc: 0.8762  (best train acc: 0.8789, best val acc: 0.8813, best train loss: 0.2945  @ epoch 14702 )\n",
      "[Epoch: 15040] train loss: 0.3268, train acc: 0.8664, val loss: 0.2834, val acc: 0.8772  (best train acc: 0.8789, best val acc: 0.8820, best train loss: 0.2945  @ epoch 14702 )\n",
      "[Epoch: 15060] train loss: 0.4051, train acc: 0.8273, val loss: 0.3363, val acc: 0.8486  (best train acc: 0.8789, best val acc: 0.8820, best train loss: 0.2945  @ epoch 14702 )\n",
      "[Epoch: 15080] train loss: 0.3543, train acc: 0.8512, val loss: 0.3124, val acc: 0.8651  (best train acc: 0.8789, best val acc: 0.8820, best train loss: 0.2945  @ epoch 14702 )\n",
      "[Epoch: 15100] train loss: 0.4081, train acc: 0.8311, val loss: 0.3044, val acc: 0.8634  (best train acc: 0.8789, best val acc: 0.8820, best train loss: 0.2945  @ epoch 14702 )\n",
      "[Epoch: 15120] train loss: 0.3254, train acc: 0.8697, val loss: 0.2947, val acc: 0.8749  (best train acc: 0.8789, best val acc: 0.8820, best train loss: 0.2945  @ epoch 14702 )\n",
      "[Epoch: 15140] train loss: 0.3206, train acc: 0.8701, val loss: 0.2890, val acc: 0.8718  (best train acc: 0.8789, best val acc: 0.8820, best train loss: 0.2945  @ epoch 14702 )\n",
      "[Epoch: 15160] train loss: 0.3167, train acc: 0.8660, val loss: 0.2895, val acc: 0.8752  (best train acc: 0.8789, best val acc: 0.8820, best train loss: 0.2945  @ epoch 14702 )\n",
      "[Epoch: 15180] train loss: 0.3205, train acc: 0.8706, val loss: 0.2904, val acc: 0.8698  (best train acc: 0.8789, best val acc: 0.8820, best train loss: 0.2945  @ epoch 14702 )\n",
      "[Epoch: 15200] train loss: 0.3244, train acc: 0.8622, val loss: 0.2919, val acc: 0.8735  (best train acc: 0.8789, best val acc: 0.8820, best train loss: 0.2945  @ epoch 14702 )\n",
      "[Epoch: 15220] train loss: 0.3759, train acc: 0.8385, val loss: 0.2873, val acc: 0.8732  (best train acc: 0.8789, best val acc: 0.8820, best train loss: 0.2945  @ epoch 14702 )\n",
      "[Epoch: 15240] train loss: 0.3452, train acc: 0.8551, val loss: 0.2855, val acc: 0.8766  (best train acc: 0.8789, best val acc: 0.8820, best train loss: 0.2945  @ epoch 14702 )\n",
      "[Epoch: 15260] train loss: 0.3149, train acc: 0.8672, val loss: 0.3099, val acc: 0.8637  (best train acc: 0.8789, best val acc: 0.8820, best train loss: 0.2945  @ epoch 14702 )\n",
      "[Epoch: 15280] train loss: 0.3334, train acc: 0.8601, val loss: 0.3087, val acc: 0.8658  (best train acc: 0.8789, best val acc: 0.8820, best train loss: 0.2945  @ epoch 14702 )\n",
      "[Epoch: 15300] train loss: 0.3452, train acc: 0.8527, val loss: 0.3056, val acc: 0.8685  (best train acc: 0.8789, best val acc: 0.8820, best train loss: 0.2945  @ epoch 14702 )\n",
      "[Epoch: 15320] train loss: 0.3535, train acc: 0.8515, val loss: 0.2942, val acc: 0.8688  (best train acc: 0.8789, best val acc: 0.8820, best train loss: 0.2945  @ epoch 14702 )\n",
      "[Epoch: 15340] train loss: 0.3201, train acc: 0.8638, val loss: 0.2927, val acc: 0.8695  (best train acc: 0.8789, best val acc: 0.8820, best train loss: 0.2945  @ epoch 14702 )\n",
      "[Epoch: 15360] train loss: 0.3403, train acc: 0.8631, val loss: 0.2994, val acc: 0.8708  (best train acc: 0.8789, best val acc: 0.8820, best train loss: 0.2945  @ epoch 14702 )\n",
      "[Epoch: 15380] train loss: 0.3472, train acc: 0.8582, val loss: 0.2869, val acc: 0.8745  (best train acc: 0.8789, best val acc: 0.8820, best train loss: 0.2945  @ epoch 14702 )\n",
      "[Epoch: 15400] train loss: 0.3223, train acc: 0.8653, val loss: 0.2903, val acc: 0.8732  (best train acc: 0.8789, best val acc: 0.8820, best train loss: 0.2945  @ epoch 14702 )\n",
      "[Epoch: 15420] train loss: 0.3383, train acc: 0.8573, val loss: 0.2913, val acc: 0.8759  (best train acc: 0.8789, best val acc: 0.8820, best train loss: 0.2945  @ epoch 14702 )\n",
      "[Epoch: 15440] train loss: 0.3089, train acc: 0.8744, val loss: 0.2844, val acc: 0.8762  (best train acc: 0.8789, best val acc: 0.8820, best train loss: 0.2945  @ epoch 14702 )\n",
      "[Epoch: 15460] train loss: 0.3191, train acc: 0.8681, val loss: 0.2895, val acc: 0.8715  (best train acc: 0.8795, best val acc: 0.8820, best train loss: 0.2945  @ epoch 14702 )\n",
      "[Epoch: 15480] train loss: 0.3256, train acc: 0.8650, val loss: 0.3254, val acc: 0.8519  (best train acc: 0.8795, best val acc: 0.8820, best train loss: 0.2945  @ epoch 14702 )\n",
      "[Epoch: 15500] train loss: 0.3239, train acc: 0.8690, val loss: 0.2850, val acc: 0.8742  (best train acc: 0.8795, best val acc: 0.8820, best train loss: 0.2945  @ epoch 14702 )\n",
      "[Epoch: 15520] train loss: 0.3590, train acc: 0.8503, val loss: 0.2896, val acc: 0.8722  (best train acc: 0.8795, best val acc: 0.8820, best train loss: 0.2945  @ epoch 14702 )\n",
      "[Epoch: 15540] train loss: 0.3588, train acc: 0.8571, val loss: 0.3059, val acc: 0.8651  (best train acc: 0.8795, best val acc: 0.8820, best train loss: 0.2945  @ epoch 14702 )\n",
      "[Epoch: 15560] train loss: 0.3234, train acc: 0.8677, val loss: 0.2953, val acc: 0.8745  (best train acc: 0.8795, best val acc: 0.8820, best train loss: 0.2945  @ epoch 14702 )\n",
      "[Epoch: 15580] train loss: 0.3601, train acc: 0.8519, val loss: 0.2799, val acc: 0.8759  (best train acc: 0.8795, best val acc: 0.8843, best train loss: 0.2945  @ epoch 14702 )\n",
      "[Epoch: 15600] train loss: 0.3072, train acc: 0.8726, val loss: 0.2880, val acc: 0.8742  (best train acc: 0.8795, best val acc: 0.8843, best train loss: 0.2945  @ epoch 14702 )\n",
      "[Epoch: 15620] train loss: 0.3224, train acc: 0.8684, val loss: 0.2853, val acc: 0.8755  (best train acc: 0.8801, best val acc: 0.8843, best train loss: 0.2915  @ epoch 15614 )\n",
      "[Epoch: 15640] train loss: 0.3217, train acc: 0.8732, val loss: 0.2928, val acc: 0.8742  (best train acc: 0.8801, best val acc: 0.8843, best train loss: 0.2915  @ epoch 15614 )\n",
      "[Epoch: 15660] train loss: 0.3056, train acc: 0.8756, val loss: 0.2901, val acc: 0.8755  (best train acc: 0.8801, best val acc: 0.8843, best train loss: 0.2915  @ epoch 15614 )\n",
      "[Epoch: 15680] train loss: 0.3278, train acc: 0.8640, val loss: 0.2886, val acc: 0.8769  (best train acc: 0.8801, best val acc: 0.8843, best train loss: 0.2915  @ epoch 15614 )\n",
      "[Epoch: 15700] train loss: 0.3107, train acc: 0.8724, val loss: 0.2817, val acc: 0.8782  (best train acc: 0.8814, best val acc: 0.8843, best train loss: 0.2905  @ epoch 15687 )\n",
      "[Epoch: 15720] train loss: 0.3311, train acc: 0.8632, val loss: 0.2881, val acc: 0.8769  (best train acc: 0.8814, best val acc: 0.8843, best train loss: 0.2905  @ epoch 15687 )\n",
      "[Epoch: 15740] train loss: 0.3273, train acc: 0.8634, val loss: 0.3053, val acc: 0.8651  (best train acc: 0.8814, best val acc: 0.8843, best train loss: 0.2905  @ epoch 15687 )\n",
      "[Epoch: 15760] train loss: 0.3293, train acc: 0.8629, val loss: 0.2939, val acc: 0.8739  (best train acc: 0.8814, best val acc: 0.8843, best train loss: 0.2905  @ epoch 15687 )\n",
      "[Epoch: 15780] train loss: 0.3728, train acc: 0.8379, val loss: 0.3105, val acc: 0.8644  (best train acc: 0.8814, best val acc: 0.8843, best train loss: 0.2905  @ epoch 15687 )\n",
      "[Epoch: 15800] train loss: 0.3373, train acc: 0.8539, val loss: 0.2829, val acc: 0.8749  (best train acc: 0.8814, best val acc: 0.8843, best train loss: 0.2905  @ epoch 15687 )\n",
      "[Epoch: 15820] train loss: 0.3536, train acc: 0.8621, val loss: 0.3034, val acc: 0.8698  (best train acc: 0.8814, best val acc: 0.8843, best train loss: 0.2905  @ epoch 15687 )\n",
      "[Epoch: 15840] train loss: 0.3226, train acc: 0.8667, val loss: 0.3005, val acc: 0.8698  (best train acc: 0.8814, best val acc: 0.8843, best train loss: 0.2905  @ epoch 15687 )\n",
      "[Epoch: 15860] train loss: 0.3166, train acc: 0.8650, val loss: 0.2847, val acc: 0.8776  (best train acc: 0.8814, best val acc: 0.8843, best train loss: 0.2884  @ epoch 15842 )\n",
      "[Epoch: 15880] train loss: 0.3211, train acc: 0.8651, val loss: 0.2823, val acc: 0.8725  (best train acc: 0.8814, best val acc: 0.8843, best train loss: 0.2884  @ epoch 15842 )\n",
      "[Epoch: 15900] train loss: 0.3401, train acc: 0.8576, val loss: 0.2925, val acc: 0.8745  (best train acc: 0.8814, best val acc: 0.8843, best train loss: 0.2884  @ epoch 15842 )\n",
      "[Epoch: 15920] train loss: 0.3123, train acc: 0.8657, val loss: 0.2850, val acc: 0.8752  (best train acc: 0.8814, best val acc: 0.8843, best train loss: 0.2884  @ epoch 15842 )\n",
      "[Epoch: 15940] train loss: 0.3241, train acc: 0.8678, val loss: 0.2933, val acc: 0.8718  (best train acc: 0.8814, best val acc: 0.8843, best train loss: 0.2884  @ epoch 15842 )\n",
      "[Epoch: 15960] train loss: 0.3311, train acc: 0.8608, val loss: 0.2789, val acc: 0.8789  (best train acc: 0.8814, best val acc: 0.8843, best train loss: 0.2884  @ epoch 15842 )\n",
      "[Epoch: 15980] train loss: 0.3112, train acc: 0.8712, val loss: 0.3098, val acc: 0.8654  (best train acc: 0.8814, best val acc: 0.8843, best train loss: 0.2884  @ epoch 15842 )\n",
      "[Epoch: 16000] train loss: 0.3163, train acc: 0.8683, val loss: 0.2799, val acc: 0.8745  (best train acc: 0.8814, best val acc: 0.8843, best train loss: 0.2884  @ epoch 15842 )\n",
      "[Epoch: 16020] train loss: 0.3142, train acc: 0.8672, val loss: 0.2826, val acc: 0.8776  (best train acc: 0.8814, best val acc: 0.8843, best train loss: 0.2884  @ epoch 15842 )\n",
      "[Epoch: 16040] train loss: 0.3605, train acc: 0.8503, val loss: 0.2897, val acc: 0.8725  (best train acc: 0.8814, best val acc: 0.8843, best train loss: 0.2884  @ epoch 15842 )\n",
      "[Epoch: 16060] train loss: 0.3246, train acc: 0.8642, val loss: 0.2858, val acc: 0.8782  (best train acc: 0.8814, best val acc: 0.8843, best train loss: 0.2884  @ epoch 15842 )\n",
      "[Epoch: 16080] train loss: 0.3832, train acc: 0.8331, val loss: 0.3439, val acc: 0.8465  (best train acc: 0.8814, best val acc: 0.8843, best train loss: 0.2884  @ epoch 15842 )\n",
      "[Epoch: 16100] train loss: 0.3067, train acc: 0.8702, val loss: 0.2935, val acc: 0.8702  (best train acc: 0.8814, best val acc: 0.8843, best train loss: 0.2884  @ epoch 15842 )\n",
      "[Epoch: 16120] train loss: 0.3211, train acc: 0.8697, val loss: 0.2886, val acc: 0.8779  (best train acc: 0.8814, best val acc: 0.8843, best train loss: 0.2884  @ epoch 15842 )\n",
      "[Epoch: 16140] train loss: 0.3286, train acc: 0.8618, val loss: 0.2960, val acc: 0.8685  (best train acc: 0.8814, best val acc: 0.8843, best train loss: 0.2884  @ epoch 15842 )\n",
      "[Epoch: 16160] train loss: 0.3173, train acc: 0.8691, val loss: 0.2781, val acc: 0.8762  (best train acc: 0.8814, best val acc: 0.8843, best train loss: 0.2884  @ epoch 15842 )\n",
      "[Epoch: 16180] train loss: 0.3794, train acc: 0.8360, val loss: 0.3083, val acc: 0.8641  (best train acc: 0.8814, best val acc: 0.8843, best train loss: 0.2884  @ epoch 15842 )\n",
      "[Epoch: 16200] train loss: 0.3155, train acc: 0.8639, val loss: 0.3326, val acc: 0.8513  (best train acc: 0.8814, best val acc: 0.8843, best train loss: 0.2884  @ epoch 15842 )\n",
      "[Epoch: 16220] train loss: 0.3599, train acc: 0.8527, val loss: 0.2898, val acc: 0.8725  (best train acc: 0.8814, best val acc: 0.8843, best train loss: 0.2884  @ epoch 15842 )\n",
      "[Epoch: 16240] train loss: 0.3411, train acc: 0.8555, val loss: 0.2891, val acc: 0.8735  (best train acc: 0.8814, best val acc: 0.8843, best train loss: 0.2884  @ epoch 15842 )\n",
      "[Epoch: 16260] train loss: 0.3237, train acc: 0.8582, val loss: 0.2972, val acc: 0.8718  (best train acc: 0.8814, best val acc: 0.8843, best train loss: 0.2884  @ epoch 15842 )\n",
      "[Epoch: 16280] train loss: 0.3240, train acc: 0.8637, val loss: 0.2829, val acc: 0.8725  (best train acc: 0.8814, best val acc: 0.8843, best train loss: 0.2884  @ epoch 15842 )\n",
      "[Epoch: 16300] train loss: 0.3157, train acc: 0.8682, val loss: 0.2846, val acc: 0.8779  (best train acc: 0.8814, best val acc: 0.8843, best train loss: 0.2884  @ epoch 15842 )\n",
      "[Epoch: 16320] train loss: 0.3179, train acc: 0.8688, val loss: 0.2864, val acc: 0.8762  (best train acc: 0.8819, best val acc: 0.8843, best train loss: 0.2884  @ epoch 15842 )\n",
      "[Epoch: 16340] train loss: 0.3080, train acc: 0.8749, val loss: 0.2906, val acc: 0.8685  (best train acc: 0.8819, best val acc: 0.8843, best train loss: 0.2884  @ epoch 15842 )\n",
      "[Epoch: 16360] train loss: 0.3029, train acc: 0.8673, val loss: 0.2867, val acc: 0.8681  (best train acc: 0.8819, best val acc: 0.8843, best train loss: 0.2884  @ epoch 15842 )\n",
      "[Epoch: 16380] train loss: 0.3166, train acc: 0.8671, val loss: 0.2824, val acc: 0.8755  (best train acc: 0.8819, best val acc: 0.8843, best train loss: 0.2884  @ epoch 15842 )\n",
      "[Epoch: 16400] train loss: 0.3279, train acc: 0.8544, val loss: 0.2985, val acc: 0.8671  (best train acc: 0.8819, best val acc: 0.8843, best train loss: 0.2884  @ epoch 15842 )\n",
      "[Epoch: 16420] train loss: 0.3282, train acc: 0.8648, val loss: 0.2894, val acc: 0.8688  (best train acc: 0.8819, best val acc: 0.8843, best train loss: 0.2884  @ epoch 15842 )\n",
      "[Epoch: 16440] train loss: 0.3628, train acc: 0.8465, val loss: 0.2812, val acc: 0.8766  (best train acc: 0.8819, best val acc: 0.8843, best train loss: 0.2884  @ epoch 15842 )\n",
      "[Epoch: 16460] train loss: 0.3257, train acc: 0.8630, val loss: 0.2806, val acc: 0.8715  (best train acc: 0.8819, best val acc: 0.8843, best train loss: 0.2884  @ epoch 15842 )\n",
      "[Epoch: 16480] train loss: 0.3226, train acc: 0.8653, val loss: 0.2884, val acc: 0.8712  (best train acc: 0.8819, best val acc: 0.8843, best train loss: 0.2884  @ epoch 15842 )\n",
      "[Epoch: 16500] train loss: 0.3064, train acc: 0.8755, val loss: 0.2812, val acc: 0.8806  (best train acc: 0.8819, best val acc: 0.8843, best train loss: 0.2884  @ epoch 15842 )\n",
      "[Epoch: 16520] train loss: 0.3239, train acc: 0.8600, val loss: 0.2938, val acc: 0.8702  (best train acc: 0.8819, best val acc: 0.8850, best train loss: 0.2884  @ epoch 15842 )\n",
      "[Epoch: 16540] train loss: 0.3300, train acc: 0.8621, val loss: 0.2831, val acc: 0.8796  (best train acc: 0.8819, best val acc: 0.8850, best train loss: 0.2884  @ epoch 15842 )\n",
      "[Epoch: 16560] train loss: 0.2993, train acc: 0.8754, val loss: 0.2772, val acc: 0.8843  (best train acc: 0.8819, best val acc: 0.8853, best train loss: 0.2884  @ epoch 15842 )\n",
      "[Epoch: 16580] train loss: 0.3273, train acc: 0.8618, val loss: 0.2808, val acc: 0.8793  (best train acc: 0.8819, best val acc: 0.8853, best train loss: 0.2884  @ epoch 15842 )\n",
      "[Epoch: 16600] train loss: 0.3077, train acc: 0.8719, val loss: 0.2840, val acc: 0.8769  (best train acc: 0.8819, best val acc: 0.8853, best train loss: 0.2884  @ epoch 15842 )\n",
      "[Epoch: 16620] train loss: 0.3145, train acc: 0.8704, val loss: 0.2840, val acc: 0.8762  (best train acc: 0.8819, best val acc: 0.8853, best train loss: 0.2884  @ epoch 15842 )\n",
      "[Epoch: 16640] train loss: 0.3065, train acc: 0.8754, val loss: 0.2811, val acc: 0.8816  (best train acc: 0.8819, best val acc: 0.8853, best train loss: 0.2884  @ epoch 15842 )\n",
      "[Epoch: 16660] train loss: 0.3309, train acc: 0.8608, val loss: 0.3117, val acc: 0.8654  (best train acc: 0.8819, best val acc: 0.8853, best train loss: 0.2884  @ epoch 15842 )\n",
      "[Epoch: 16680] train loss: 0.2954, train acc: 0.8777, val loss: 0.2767, val acc: 0.8803  (best train acc: 0.8819, best val acc: 0.8853, best train loss: 0.2884  @ epoch 15842 )\n",
      "[Epoch: 16700] train loss: 0.2968, train acc: 0.8755, val loss: 0.2811, val acc: 0.8755  (best train acc: 0.8837, best val acc: 0.8853, best train loss: 0.2878  @ epoch 16694 )\n",
      "[Epoch: 16720] train loss: 0.3147, train acc: 0.8715, val loss: 0.2763, val acc: 0.8779  (best train acc: 0.8842, best val acc: 0.8853, best train loss: 0.2815  @ epoch 16707 )\n",
      "[Epoch: 16740] train loss: 0.3318, train acc: 0.8563, val loss: 0.2839, val acc: 0.8732  (best train acc: 0.8842, best val acc: 0.8853, best train loss: 0.2815  @ epoch 16707 )\n",
      "[Epoch: 16760] train loss: 0.3029, train acc: 0.8769, val loss: 0.2908, val acc: 0.8718  (best train acc: 0.8842, best val acc: 0.8853, best train loss: 0.2815  @ epoch 16707 )\n",
      "[Epoch: 16780] train loss: 0.2954, train acc: 0.8797, val loss: 0.2962, val acc: 0.8732  (best train acc: 0.8850, best val acc: 0.8853, best train loss: 0.2788  @ epoch 16778 )\n",
      "[Epoch: 16800] train loss: 0.3149, train acc: 0.8671, val loss: 0.2749, val acc: 0.8762  (best train acc: 0.8850, best val acc: 0.8853, best train loss: 0.2788  @ epoch 16778 )\n",
      "[Epoch: 16820] train loss: 0.3413, train acc: 0.8608, val loss: 0.2971, val acc: 0.8732  (best train acc: 0.8850, best val acc: 0.8853, best train loss: 0.2788  @ epoch 16778 )\n",
      "[Epoch: 16840] train loss: 0.3537, train acc: 0.8559, val loss: 0.2735, val acc: 0.8796  (best train acc: 0.8850, best val acc: 0.8853, best train loss: 0.2788  @ epoch 16778 )\n",
      "[Epoch: 16860] train loss: 0.2860, train acc: 0.8836, val loss: 0.2719, val acc: 0.8806  (best train acc: 0.8850, best val acc: 0.8853, best train loss: 0.2788  @ epoch 16778 )\n",
      "[Epoch: 16880] train loss: 0.2811, train acc: 0.8887, val loss: 0.2728, val acc: 0.8793  (best train acc: 0.8887, best val acc: 0.8853, best train loss: 0.2784  @ epoch 16876 )\n",
      "[Epoch: 16900] train loss: 0.2971, train acc: 0.8814, val loss: 0.2977, val acc: 0.8715  (best train acc: 0.8887, best val acc: 0.8853, best train loss: 0.2784  @ epoch 16876 )\n",
      "[Epoch: 16920] train loss: 0.3418, train acc: 0.8577, val loss: 0.2873, val acc: 0.8715  (best train acc: 0.8887, best val acc: 0.8853, best train loss: 0.2784  @ epoch 16876 )\n",
      "[Epoch: 16940] train loss: 0.2929, train acc: 0.8722, val loss: 0.2753, val acc: 0.8793  (best train acc: 0.8887, best val acc: 0.8853, best train loss: 0.2747  @ epoch 16922 )\n",
      "[Epoch: 16960] train loss: 0.2913, train acc: 0.8788, val loss: 0.3036, val acc: 0.8705  (best train acc: 0.8887, best val acc: 0.8853, best train loss: 0.2747  @ epoch 16922 )\n",
      "[Epoch: 16980] train loss: 0.2786, train acc: 0.8845, val loss: 0.2726, val acc: 0.8793  (best train acc: 0.8887, best val acc: 0.8860, best train loss: 0.2747  @ epoch 16922 )\n",
      "[Epoch: 17000] train loss: 0.3126, train acc: 0.8680, val loss: 0.2853, val acc: 0.8712  (best train acc: 0.8887, best val acc: 0.8860, best train loss: 0.2747  @ epoch 16922 )\n",
      "[Epoch: 17020] train loss: 0.3031, train acc: 0.8644, val loss: 0.3076, val acc: 0.8627  (best train acc: 0.8887, best val acc: 0.8860, best train loss: 0.2747  @ epoch 16922 )\n",
      "[Epoch: 17040] train loss: 0.3028, train acc: 0.8715, val loss: 0.2805, val acc: 0.8796  (best train acc: 0.8887, best val acc: 0.8860, best train loss: 0.2747  @ epoch 16922 )\n",
      "[Epoch: 17060] train loss: 0.3271, train acc: 0.8624, val loss: 0.3138, val acc: 0.8641  (best train acc: 0.8887, best val acc: 0.8860, best train loss: 0.2747  @ epoch 16922 )\n",
      "[Epoch: 17080] train loss: 0.2968, train acc: 0.8711, val loss: 0.2862, val acc: 0.8759  (best train acc: 0.8887, best val acc: 0.8860, best train loss: 0.2747  @ epoch 16922 )\n",
      "[Epoch: 17100] train loss: 0.2866, train acc: 0.8768, val loss: 0.2808, val acc: 0.8715  (best train acc: 0.8887, best val acc: 0.8860, best train loss: 0.2747  @ epoch 16922 )\n",
      "[Epoch: 17120] train loss: 0.3101, train acc: 0.8701, val loss: 0.2734, val acc: 0.8806  (best train acc: 0.8887, best val acc: 0.8860, best train loss: 0.2747  @ epoch 16922 )\n",
      "[Epoch: 17140] train loss: 0.2878, train acc: 0.8798, val loss: 0.2788, val acc: 0.8749  (best train acc: 0.8887, best val acc: 0.8860, best train loss: 0.2747  @ epoch 16922 )\n",
      "[Epoch: 17160] train loss: 0.2843, train acc: 0.8798, val loss: 0.2851, val acc: 0.8752  (best train acc: 0.8887, best val acc: 0.8860, best train loss: 0.2747  @ epoch 16922 )\n",
      "[Epoch: 17180] train loss: 0.2920, train acc: 0.8746, val loss: 0.2702, val acc: 0.8796  (best train acc: 0.8887, best val acc: 0.8860, best train loss: 0.2747  @ epoch 16922 )\n",
      "[Epoch: 17200] train loss: 0.2897, train acc: 0.8775, val loss: 0.2823, val acc: 0.8762  (best train acc: 0.8887, best val acc: 0.8860, best train loss: 0.2747  @ epoch 16922 )\n",
      "[Epoch: 17220] train loss: 0.3443, train acc: 0.8580, val loss: 0.3192, val acc: 0.8610  (best train acc: 0.8887, best val acc: 0.8860, best train loss: 0.2747  @ epoch 16922 )\n",
      "[Epoch: 17240] train loss: 0.2925, train acc: 0.8796, val loss: 0.2814, val acc: 0.8803  (best train acc: 0.8887, best val acc: 0.8860, best train loss: 0.2747  @ epoch 16922 )\n",
      "[Epoch: 17260] train loss: 0.3164, train acc: 0.8621, val loss: 0.2804, val acc: 0.8749  (best train acc: 0.8897, best val acc: 0.8860, best train loss: 0.2747  @ epoch 16922 )\n",
      "[Epoch: 17280] train loss: 0.3026, train acc: 0.8706, val loss: 0.2730, val acc: 0.8786  (best train acc: 0.8897, best val acc: 0.8860, best train loss: 0.2747  @ epoch 16922 )\n",
      "[Epoch: 17300] train loss: 0.2908, train acc: 0.8772, val loss: 0.2735, val acc: 0.8789  (best train acc: 0.8897, best val acc: 0.8860, best train loss: 0.2704  @ epoch 17296 )\n",
      "[Epoch: 17320] train loss: 0.2811, train acc: 0.8805, val loss: 0.2803, val acc: 0.8809  (best train acc: 0.8897, best val acc: 0.8860, best train loss: 0.2704  @ epoch 17314 )\n",
      "[Epoch: 17340] train loss: 0.3234, train acc: 0.8711, val loss: 0.2803, val acc: 0.8782  (best train acc: 0.8897, best val acc: 0.8860, best train loss: 0.2704  @ epoch 17314 )\n",
      "[Epoch: 17360] train loss: 0.2807, train acc: 0.8848, val loss: 0.3028, val acc: 0.8702  (best train acc: 0.8897, best val acc: 0.8860, best train loss: 0.2704  @ epoch 17314 )\n",
      "[Epoch: 17380] train loss: 0.3229, train acc: 0.8644, val loss: 0.2777, val acc: 0.8772  (best train acc: 0.8897, best val acc: 0.8860, best train loss: 0.2704  @ epoch 17314 )\n",
      "[Epoch: 17400] train loss: 0.3593, train acc: 0.8530, val loss: 0.3034, val acc: 0.8658  (best train acc: 0.8897, best val acc: 0.8860, best train loss: 0.2704  @ epoch 17314 )\n",
      "[Epoch: 17420] train loss: 0.3084, train acc: 0.8728, val loss: 0.2834, val acc: 0.8823  (best train acc: 0.8897, best val acc: 0.8860, best train loss: 0.2704  @ epoch 17314 )\n",
      "[Epoch: 17440] train loss: 0.2955, train acc: 0.8783, val loss: 0.2901, val acc: 0.8745  (best train acc: 0.8897, best val acc: 0.8860, best train loss: 0.2704  @ epoch 17314 )\n",
      "[Epoch: 17460] train loss: 0.3019, train acc: 0.8746, val loss: 0.2832, val acc: 0.8772  (best train acc: 0.8897, best val acc: 0.8860, best train loss: 0.2704  @ epoch 17314 )\n",
      "[Epoch: 17480] train loss: 0.2981, train acc: 0.8767, val loss: 0.2859, val acc: 0.8772  (best train acc: 0.8897, best val acc: 0.8860, best train loss: 0.2701  @ epoch 17476 )\n",
      "[Epoch: 17500] train loss: 0.2966, train acc: 0.8773, val loss: 0.2858, val acc: 0.8749  (best train acc: 0.8897, best val acc: 0.8860, best train loss: 0.2701  @ epoch 17476 )\n",
      "[Epoch: 17520] train loss: 0.3023, train acc: 0.8720, val loss: 0.2884, val acc: 0.8728  (best train acc: 0.8897, best val acc: 0.8860, best train loss: 0.2701  @ epoch 17476 )\n",
      "[Epoch: 17540] train loss: 0.3284, train acc: 0.8630, val loss: 0.2800, val acc: 0.8776  (best train acc: 0.8897, best val acc: 0.8860, best train loss: 0.2701  @ epoch 17476 )\n",
      "[Epoch: 17560] train loss: 0.2747, train acc: 0.8823, val loss: 0.2710, val acc: 0.8820  (best train acc: 0.8897, best val acc: 0.8860, best train loss: 0.2701  @ epoch 17476 )\n",
      "[Epoch: 17580] train loss: 0.2955, train acc: 0.8762, val loss: 0.2871, val acc: 0.8752  (best train acc: 0.8897, best val acc: 0.8863, best train loss: 0.2701  @ epoch 17476 )\n",
      "[Epoch: 17600] train loss: 0.3082, train acc: 0.8691, val loss: 0.2709, val acc: 0.8772  (best train acc: 0.8897, best val acc: 0.8863, best train loss: 0.2701  @ epoch 17476 )\n",
      "[Epoch: 17620] train loss: 0.2822, train acc: 0.8849, val loss: 0.2682, val acc: 0.8813  (best train acc: 0.8897, best val acc: 0.8863, best train loss: 0.2701  @ epoch 17476 )\n",
      "[Epoch: 17640] train loss: 0.2799, train acc: 0.8832, val loss: 0.3260, val acc: 0.8526  (best train acc: 0.8903, best val acc: 0.8863, best train loss: 0.2688  @ epoch 17625 )\n",
      "[Epoch: 17660] train loss: 0.2984, train acc: 0.8704, val loss: 0.2935, val acc: 0.8705  (best train acc: 0.8903, best val acc: 0.8863, best train loss: 0.2688  @ epoch 17625 )\n",
      "[Epoch: 17680] train loss: 0.3686, train acc: 0.8436, val loss: 0.3113, val acc: 0.8617  (best train acc: 0.8903, best val acc: 0.8863, best train loss: 0.2688  @ epoch 17625 )\n",
      "[Epoch: 17700] train loss: 0.3002, train acc: 0.8752, val loss: 0.2738, val acc: 0.8786  (best train acc: 0.8903, best val acc: 0.8863, best train loss: 0.2688  @ epoch 17625 )\n",
      "[Epoch: 17720] train loss: 0.2774, train acc: 0.8848, val loss: 0.2705, val acc: 0.8789  (best train acc: 0.8903, best val acc: 0.8863, best train loss: 0.2688  @ epoch 17625 )\n",
      "[Epoch: 17740] train loss: 0.3102, train acc: 0.8741, val loss: 0.2669, val acc: 0.8840  (best train acc: 0.8903, best val acc: 0.8863, best train loss: 0.2688  @ epoch 17625 )\n",
      "[Epoch: 17760] train loss: 0.3094, train acc: 0.8687, val loss: 0.2722, val acc: 0.8779  (best train acc: 0.8903, best val acc: 0.8863, best train loss: 0.2688  @ epoch 17625 )\n",
      "[Epoch: 17780] train loss: 0.3107, train acc: 0.8750, val loss: 0.2833, val acc: 0.8742  (best train acc: 0.8903, best val acc: 0.8863, best train loss: 0.2688  @ epoch 17625 )\n",
      "[Epoch: 17800] train loss: 0.3146, train acc: 0.8671, val loss: 0.2785, val acc: 0.8766  (best train acc: 0.8903, best val acc: 0.8863, best train loss: 0.2688  @ epoch 17625 )\n",
      "[Epoch: 17820] train loss: 0.2826, train acc: 0.8856, val loss: 0.2746, val acc: 0.8833  (best train acc: 0.8903, best val acc: 0.8863, best train loss: 0.2688  @ epoch 17625 )\n",
      "[Epoch: 17840] train loss: 0.2770, train acc: 0.8837, val loss: 0.2779, val acc: 0.8766  (best train acc: 0.8903, best val acc: 0.8863, best train loss: 0.2688  @ epoch 17625 )\n",
      "[Epoch: 17860] train loss: 0.2907, train acc: 0.8770, val loss: 0.2743, val acc: 0.8796  (best train acc: 0.8903, best val acc: 0.8863, best train loss: 0.2688  @ epoch 17625 )\n",
      "[Epoch: 17880] train loss: 0.3297, train acc: 0.8665, val loss: 0.2772, val acc: 0.8816  (best train acc: 0.8903, best val acc: 0.8863, best train loss: 0.2688  @ epoch 17625 )\n",
      "[Epoch: 17900] train loss: 0.2947, train acc: 0.8784, val loss: 0.2704, val acc: 0.8820  (best train acc: 0.8903, best val acc: 0.8863, best train loss: 0.2688  @ epoch 17625 )\n",
      "[Epoch: 17920] train loss: 0.3122, train acc: 0.8675, val loss: 0.3051, val acc: 0.8624  (best train acc: 0.8903, best val acc: 0.8863, best train loss: 0.2688  @ epoch 17625 )\n",
      "[Epoch: 17940] train loss: 0.3090, train acc: 0.8686, val loss: 0.3151, val acc: 0.8644  (best train acc: 0.8903, best val acc: 0.8863, best train loss: 0.2688  @ epoch 17625 )\n",
      "[Epoch: 17960] train loss: 0.3156, train acc: 0.8694, val loss: 0.2961, val acc: 0.8658  (best train acc: 0.8903, best val acc: 0.8863, best train loss: 0.2688  @ epoch 17625 )\n",
      "[Epoch: 17980] train loss: 0.2795, train acc: 0.8825, val loss: 0.2905, val acc: 0.8759  (best train acc: 0.8925, best val acc: 0.8863, best train loss: 0.2683  @ epoch 17974 )\n",
      "[Epoch: 18000] train loss: 0.3025, train acc: 0.8713, val loss: 0.2871, val acc: 0.8712  (best train acc: 0.8925, best val acc: 0.8863, best train loss: 0.2683  @ epoch 17974 )\n",
      "[Epoch: 18020] train loss: 0.3017, train acc: 0.8712, val loss: 0.2991, val acc: 0.8651  (best train acc: 0.8925, best val acc: 0.8863, best train loss: 0.2683  @ epoch 17974 )\n",
      "[Epoch: 18040] train loss: 0.2918, train acc: 0.8781, val loss: 0.2691, val acc: 0.8793  (best train acc: 0.8925, best val acc: 0.8863, best train loss: 0.2683  @ epoch 17974 )\n",
      "[Epoch: 18060] train loss: 0.3037, train acc: 0.8769, val loss: 0.3103, val acc: 0.8664  (best train acc: 0.8925, best val acc: 0.8863, best train loss: 0.2683  @ epoch 17974 )\n",
      "[Epoch: 18080] train loss: 0.3133, train acc: 0.8704, val loss: 0.2901, val acc: 0.8789  (best train acc: 0.8925, best val acc: 0.8863, best train loss: 0.2683  @ epoch 17974 )\n",
      "[Epoch: 18100] train loss: 0.2856, train acc: 0.8802, val loss: 0.2748, val acc: 0.8779  (best train acc: 0.8925, best val acc: 0.8863, best train loss: 0.2683  @ epoch 17974 )\n",
      "[Epoch: 18120] train loss: 0.2911, train acc: 0.8849, val loss: 0.2732, val acc: 0.8806  (best train acc: 0.8925, best val acc: 0.8863, best train loss: 0.2683  @ epoch 17974 )\n",
      "[Epoch: 18140] train loss: 0.3564, train acc: 0.8496, val loss: 0.3386, val acc: 0.8516  (best train acc: 0.8925, best val acc: 0.8863, best train loss: 0.2683  @ epoch 17974 )\n",
      "[Epoch: 18160] train loss: 0.2936, train acc: 0.8790, val loss: 0.2797, val acc: 0.8776  (best train acc: 0.8925, best val acc: 0.8863, best train loss: 0.2683  @ epoch 17974 )\n",
      "[Epoch: 18180] train loss: 0.2847, train acc: 0.8806, val loss: 0.2766, val acc: 0.8782  (best train acc: 0.8925, best val acc: 0.8863, best train loss: 0.2683  @ epoch 17974 )\n",
      "[Epoch: 18200] train loss: 0.2879, train acc: 0.8842, val loss: 0.2879, val acc: 0.8772  (best train acc: 0.8925, best val acc: 0.8863, best train loss: 0.2683  @ epoch 17974 )\n",
      "[Epoch: 18220] train loss: 0.2786, train acc: 0.8867, val loss: 0.2930, val acc: 0.8728  (best train acc: 0.8925, best val acc: 0.8863, best train loss: 0.2683  @ epoch 17974 )\n",
      "[Epoch: 18240] train loss: 0.3072, train acc: 0.8703, val loss: 0.2896, val acc: 0.8745  (best train acc: 0.8925, best val acc: 0.8863, best train loss: 0.2683  @ epoch 17974 )\n",
      "[Epoch: 18260] train loss: 0.3005, train acc: 0.8790, val loss: 0.2754, val acc: 0.8830  (best train acc: 0.8925, best val acc: 0.8863, best train loss: 0.2683  @ epoch 17974 )\n",
      "[Epoch: 18280] train loss: 0.2914, train acc: 0.8836, val loss: 0.2725, val acc: 0.8782  (best train acc: 0.8925, best val acc: 0.8863, best train loss: 0.2683  @ epoch 17974 )\n",
      "[Epoch: 18300] train loss: 0.2934, train acc: 0.8767, val loss: 0.2747, val acc: 0.8789  (best train acc: 0.8925, best val acc: 0.8863, best train loss: 0.2683  @ epoch 17974 )\n",
      "[Epoch: 18320] train loss: 0.2956, train acc: 0.8762, val loss: 0.2671, val acc: 0.8850  (best train acc: 0.8925, best val acc: 0.8863, best train loss: 0.2683  @ epoch 17974 )\n",
      "[Epoch: 18340] train loss: 0.3086, train acc: 0.8664, val loss: 0.2947, val acc: 0.8641  (best train acc: 0.8925, best val acc: 0.8863, best train loss: 0.2683  @ epoch 17974 )\n",
      "[Epoch: 18360] train loss: 0.2819, train acc: 0.8810, val loss: 0.2813, val acc: 0.8806  (best train acc: 0.8925, best val acc: 0.8863, best train loss: 0.2683  @ epoch 17974 )\n",
      "[Epoch: 18380] train loss: 0.2829, train acc: 0.8817, val loss: 0.2661, val acc: 0.8843  (best train acc: 0.8925, best val acc: 0.8863, best train loss: 0.2683  @ epoch 17974 )\n",
      "[Epoch: 18400] train loss: 0.2956, train acc: 0.8746, val loss: 0.2772, val acc: 0.8769  (best train acc: 0.8925, best val acc: 0.8863, best train loss: 0.2683  @ epoch 17974 )\n",
      "[Epoch: 18420] train loss: 0.2745, train acc: 0.8830, val loss: 0.2923, val acc: 0.8715  (best train acc: 0.8925, best val acc: 0.8863, best train loss: 0.2683  @ epoch 17974 )\n",
      "[Epoch: 18440] train loss: 0.3441, train acc: 0.8585, val loss: 0.2796, val acc: 0.8796  (best train acc: 0.8925, best val acc: 0.8863, best train loss: 0.2683  @ epoch 17974 )\n",
      "[Epoch: 18460] train loss: 0.2908, train acc: 0.8745, val loss: 0.2758, val acc: 0.8826  (best train acc: 0.8925, best val acc: 0.8863, best train loss: 0.2683  @ epoch 17974 )\n",
      "[Epoch: 18480] train loss: 0.3207, train acc: 0.8737, val loss: 0.2955, val acc: 0.8691  (best train acc: 0.8925, best val acc: 0.8863, best train loss: 0.2683  @ epoch 17974 )\n",
      "[Epoch: 18500] train loss: 0.2835, train acc: 0.8807, val loss: 0.2697, val acc: 0.8826  (best train acc: 0.8925, best val acc: 0.8863, best train loss: 0.2683  @ epoch 17974 )\n",
      "[Epoch: 18520] train loss: 0.2939, train acc: 0.8743, val loss: 0.2813, val acc: 0.8722  (best train acc: 0.8938, best val acc: 0.8867, best train loss: 0.2683  @ epoch 17974 )\n",
      "[Epoch: 18540] train loss: 0.3092, train acc: 0.8725, val loss: 0.2729, val acc: 0.8826  (best train acc: 0.8938, best val acc: 0.8867, best train loss: 0.2683  @ epoch 17974 )\n",
      "[Epoch: 18560] train loss: 0.3012, train acc: 0.8757, val loss: 0.2931, val acc: 0.8722  (best train acc: 0.8938, best val acc: 0.8867, best train loss: 0.2683  @ epoch 17974 )\n",
      "[Epoch: 18580] train loss: 0.2827, train acc: 0.8798, val loss: 0.3196, val acc: 0.8536  (best train acc: 0.8938, best val acc: 0.8867, best train loss: 0.2683  @ epoch 17974 )\n",
      "[Epoch: 18600] train loss: 0.2777, train acc: 0.8845, val loss: 0.2796, val acc: 0.8803  (best train acc: 0.8938, best val acc: 0.8867, best train loss: 0.2683  @ epoch 17974 )\n",
      "[Epoch: 18620] train loss: 0.3048, train acc: 0.8806, val loss: 0.2737, val acc: 0.8793  (best train acc: 0.8938, best val acc: 0.8867, best train loss: 0.2683  @ epoch 17974 )\n",
      "[Epoch: 18640] train loss: 0.2880, train acc: 0.8812, val loss: 0.2805, val acc: 0.8796  (best train acc: 0.8938, best val acc: 0.8867, best train loss: 0.2674  @ epoch 18629 )\n",
      "[Epoch: 18660] train loss: 0.2798, train acc: 0.8825, val loss: 0.2769, val acc: 0.8742  (best train acc: 0.8938, best val acc: 0.8867, best train loss: 0.2674  @ epoch 18629 )\n",
      "[Epoch: 18680] train loss: 0.3164, train acc: 0.8657, val loss: 0.2770, val acc: 0.8799  (best train acc: 0.8952, best val acc: 0.8867, best train loss: 0.2630  @ epoch 18678 )\n",
      "[Epoch: 18700] train loss: 0.3067, train acc: 0.8770, val loss: 0.2778, val acc: 0.8752  (best train acc: 0.8952, best val acc: 0.8877, best train loss: 0.2630  @ epoch 18678 )\n",
      "[Epoch: 18720] train loss: 0.2999, train acc: 0.8765, val loss: 0.3040, val acc: 0.8691  (best train acc: 0.8952, best val acc: 0.8877, best train loss: 0.2630  @ epoch 18678 )\n",
      "[Epoch: 18740] train loss: 0.2840, train acc: 0.8790, val loss: 0.2779, val acc: 0.8830  (best train acc: 0.8952, best val acc: 0.8877, best train loss: 0.2630  @ epoch 18678 )\n",
      "[Epoch: 18760] train loss: 0.2770, train acc: 0.8851, val loss: 0.2656, val acc: 0.8816  (best train acc: 0.8952, best val acc: 0.8877, best train loss: 0.2630  @ epoch 18678 )\n",
      "[Epoch: 18780] train loss: 0.3023, train acc: 0.8722, val loss: 0.2984, val acc: 0.8664  (best train acc: 0.8952, best val acc: 0.8877, best train loss: 0.2630  @ epoch 18678 )\n",
      "[Epoch: 18800] train loss: 0.2813, train acc: 0.8897, val loss: 0.2929, val acc: 0.8685  (best train acc: 0.8952, best val acc: 0.8877, best train loss: 0.2630  @ epoch 18678 )\n",
      "[Epoch: 18820] train loss: 0.3208, train acc: 0.8632, val loss: 0.3226, val acc: 0.8610  (best train acc: 0.8952, best val acc: 0.8877, best train loss: 0.2630  @ epoch 18678 )\n",
      "[Epoch: 18840] train loss: 0.2950, train acc: 0.8764, val loss: 0.2793, val acc: 0.8799  (best train acc: 0.8952, best val acc: 0.8877, best train loss: 0.2630  @ epoch 18678 )\n",
      "[Epoch: 18860] train loss: 0.2965, train acc: 0.8749, val loss: 0.2980, val acc: 0.8715  (best train acc: 0.8952, best val acc: 0.8877, best train loss: 0.2630  @ epoch 18678 )\n",
      "[Epoch: 18880] train loss: 0.2909, train acc: 0.8817, val loss: 0.2979, val acc: 0.8732  (best train acc: 0.8952, best val acc: 0.8877, best train loss: 0.2630  @ epoch 18678 )\n",
      "[Epoch: 18900] train loss: 0.3082, train acc: 0.8688, val loss: 0.2715, val acc: 0.8843  (best train acc: 0.8952, best val acc: 0.8877, best train loss: 0.2630  @ epoch 18678 )\n",
      "[Epoch: 18920] train loss: 0.3503, train acc: 0.8636, val loss: 0.2653, val acc: 0.8816  (best train acc: 0.8952, best val acc: 0.8877, best train loss: 0.2630  @ epoch 18678 )\n",
      "[Epoch: 18940] train loss: 0.3067, train acc: 0.8746, val loss: 0.2807, val acc: 0.8776  (best train acc: 0.8952, best val acc: 0.8877, best train loss: 0.2630  @ epoch 18678 )\n",
      "[Epoch: 18960] train loss: 0.3558, train acc: 0.8496, val loss: 0.2880, val acc: 0.8732  (best train acc: 0.8952, best val acc: 0.8877, best train loss: 0.2630  @ epoch 18678 )\n",
      "[Epoch: 18980] train loss: 0.2754, train acc: 0.8842, val loss: 0.2744, val acc: 0.8759  (best train acc: 0.8952, best val acc: 0.8877, best train loss: 0.2630  @ epoch 18678 )\n",
      "[Epoch: 19000] train loss: 0.2817, train acc: 0.8817, val loss: 0.2799, val acc: 0.8779  (best train acc: 0.8952, best val acc: 0.8877, best train loss: 0.2630  @ epoch 18678 )\n",
      "[Epoch: 19020] train loss: 0.2836, train acc: 0.8843, val loss: 0.2702, val acc: 0.8769  (best train acc: 0.8952, best val acc: 0.8877, best train loss: 0.2630  @ epoch 18678 )\n",
      "[Epoch: 19040] train loss: 0.3038, train acc: 0.8743, val loss: 0.3041, val acc: 0.8685  (best train acc: 0.8952, best val acc: 0.8877, best train loss: 0.2630  @ epoch 18678 )\n",
      "[Epoch: 19060] train loss: 0.2859, train acc: 0.8810, val loss: 0.2686, val acc: 0.8786  (best train acc: 0.8952, best val acc: 0.8877, best train loss: 0.2630  @ epoch 18678 )\n",
      "[Epoch: 19080] train loss: 0.3069, train acc: 0.8757, val loss: 0.2964, val acc: 0.8725  (best train acc: 0.8952, best val acc: 0.8877, best train loss: 0.2630  @ epoch 18678 )\n",
      "[Epoch: 19100] train loss: 0.3025, train acc: 0.8770, val loss: 0.2732, val acc: 0.8742  (best train acc: 0.8952, best val acc: 0.8877, best train loss: 0.2630  @ epoch 18678 )\n",
      "[Epoch: 19120] train loss: 0.2713, train acc: 0.8851, val loss: 0.2682, val acc: 0.8830  (best train acc: 0.8952, best val acc: 0.8877, best train loss: 0.2630  @ epoch 18678 )\n",
      "[Epoch: 19140] train loss: 0.2983, train acc: 0.8761, val loss: 0.2766, val acc: 0.8776  (best train acc: 0.8952, best val acc: 0.8877, best train loss: 0.2630  @ epoch 18678 )\n",
      "[Epoch: 19160] train loss: 0.3114, train acc: 0.8651, val loss: 0.2736, val acc: 0.8809  (best train acc: 0.8952, best val acc: 0.8877, best train loss: 0.2630  @ epoch 18678 )\n",
      "[Epoch: 19180] train loss: 0.2926, train acc: 0.8787, val loss: 0.2697, val acc: 0.8836  (best train acc: 0.8952, best val acc: 0.8877, best train loss: 0.2630  @ epoch 18678 )\n",
      "[Epoch: 19200] train loss: 0.2927, train acc: 0.8817, val loss: 0.3002, val acc: 0.8651  (best train acc: 0.8952, best val acc: 0.8877, best train loss: 0.2630  @ epoch 18678 )\n",
      "[Epoch: 19220] train loss: 0.2920, train acc: 0.8803, val loss: 0.2842, val acc: 0.8715  (best train acc: 0.8952, best val acc: 0.8877, best train loss: 0.2630  @ epoch 18678 )\n",
      "[Epoch: 19240] train loss: 0.2985, train acc: 0.8705, val loss: 0.2802, val acc: 0.8782  (best train acc: 0.8952, best val acc: 0.8877, best train loss: 0.2630  @ epoch 18678 )\n",
      "[Epoch: 19260] train loss: 0.2997, train acc: 0.8787, val loss: 0.2705, val acc: 0.8850  (best train acc: 0.8952, best val acc: 0.8877, best train loss: 0.2630  @ epoch 18678 )\n",
      "[Epoch: 19280] train loss: 0.2930, train acc: 0.8764, val loss: 0.2907, val acc: 0.8691  (best train acc: 0.8952, best val acc: 0.8877, best train loss: 0.2630  @ epoch 18678 )\n",
      "[Epoch: 19300] train loss: 0.3440, train acc: 0.8597, val loss: 0.2822, val acc: 0.8776  (best train acc: 0.8952, best val acc: 0.8877, best train loss: 0.2617  @ epoch 19294 )\n",
      "[Epoch: 19320] train loss: 0.2875, train acc: 0.8801, val loss: 0.2681, val acc: 0.8847  (best train acc: 0.8965, best val acc: 0.8877, best train loss: 0.2617  @ epoch 19294 )\n",
      "[Epoch: 19340] train loss: 0.2924, train acc: 0.8856, val loss: 0.3010, val acc: 0.8702  (best train acc: 0.8965, best val acc: 0.8877, best train loss: 0.2617  @ epoch 19294 )\n",
      "[Epoch: 19360] train loss: 0.2764, train acc: 0.8789, val loss: 0.2694, val acc: 0.8786  (best train acc: 0.8965, best val acc: 0.8877, best train loss: 0.2617  @ epoch 19294 )\n",
      "[Epoch: 19380] train loss: 0.2766, train acc: 0.8871, val loss: 0.2774, val acc: 0.8769  (best train acc: 0.8965, best val acc: 0.8877, best train loss: 0.2617  @ epoch 19294 )\n",
      "[Epoch: 19400] train loss: 0.3054, train acc: 0.8729, val loss: 0.2712, val acc: 0.8799  (best train acc: 0.8965, best val acc: 0.8877, best train loss: 0.2617  @ epoch 19294 )\n",
      "[Epoch: 19420] train loss: 0.2949, train acc: 0.8715, val loss: 0.2715, val acc: 0.8809  (best train acc: 0.8965, best val acc: 0.8877, best train loss: 0.2617  @ epoch 19294 )\n",
      "[Epoch: 19440] train loss: 0.2833, train acc: 0.8806, val loss: 0.2847, val acc: 0.8782  (best train acc: 0.8965, best val acc: 0.8877, best train loss: 0.2617  @ epoch 19294 )\n",
      "[Epoch: 19460] train loss: 0.3048, train acc: 0.8739, val loss: 0.2680, val acc: 0.8782  (best train acc: 0.8965, best val acc: 0.8877, best train loss: 0.2617  @ epoch 19294 )\n",
      "[Epoch: 19480] train loss: 0.2941, train acc: 0.8810, val loss: 0.2881, val acc: 0.8769  (best train acc: 0.8965, best val acc: 0.8877, best train loss: 0.2617  @ epoch 19294 )\n",
      "[Epoch: 19500] train loss: 0.3094, train acc: 0.8694, val loss: 0.2707, val acc: 0.8820  (best train acc: 0.8965, best val acc: 0.8877, best train loss: 0.2617  @ epoch 19294 )\n",
      "[Epoch: 19520] train loss: 0.2891, train acc: 0.8767, val loss: 0.2865, val acc: 0.8755  (best train acc: 0.8965, best val acc: 0.8877, best train loss: 0.2617  @ epoch 19294 )\n",
      "[Epoch: 19540] train loss: 0.2928, train acc: 0.8745, val loss: 0.3072, val acc: 0.8627  (best train acc: 0.8965, best val acc: 0.8877, best train loss: 0.2617  @ epoch 19294 )\n",
      "[Epoch: 19560] train loss: 0.3013, train acc: 0.8748, val loss: 0.2771, val acc: 0.8796  (best train acc: 0.8965, best val acc: 0.8877, best train loss: 0.2617  @ epoch 19294 )\n",
      "[Epoch: 19580] train loss: 0.2869, train acc: 0.8791, val loss: 0.2690, val acc: 0.8796  (best train acc: 0.8965, best val acc: 0.8877, best train loss: 0.2617  @ epoch 19294 )\n",
      "[Epoch: 19600] train loss: 0.3037, train acc: 0.8738, val loss: 0.3022, val acc: 0.8702  (best train acc: 0.8965, best val acc: 0.8877, best train loss: 0.2617  @ epoch 19294 )\n",
      "[Epoch: 19620] train loss: 0.3087, train acc: 0.8736, val loss: 0.2735, val acc: 0.8742  (best train acc: 0.8965, best val acc: 0.8877, best train loss: 0.2617  @ epoch 19294 )\n",
      "[Epoch: 19640] train loss: 0.3204, train acc: 0.8628, val loss: 0.2829, val acc: 0.8782  (best train acc: 0.8965, best val acc: 0.8877, best train loss: 0.2617  @ epoch 19294 )\n",
      "[Epoch: 19660] train loss: 0.3773, train acc: 0.8495, val loss: 0.3071, val acc: 0.8678  (best train acc: 0.8965, best val acc: 0.8877, best train loss: 0.2617  @ epoch 19294 )\n",
      "[Epoch: 19680] train loss: 0.2814, train acc: 0.8817, val loss: 0.2743, val acc: 0.8766  (best train acc: 0.8965, best val acc: 0.8877, best train loss: 0.2617  @ epoch 19294 )\n",
      "[Epoch: 19700] train loss: 0.2743, train acc: 0.8823, val loss: 0.2734, val acc: 0.8816  (best train acc: 0.8965, best val acc: 0.8877, best train loss: 0.2617  @ epoch 19294 )\n",
      "[Epoch: 19720] train loss: 0.2714, train acc: 0.8830, val loss: 0.2647, val acc: 0.8776  (best train acc: 0.8965, best val acc: 0.8877, best train loss: 0.2617  @ epoch 19294 )\n",
      "[Epoch: 19740] train loss: 0.2962, train acc: 0.8770, val loss: 0.2751, val acc: 0.8779  (best train acc: 0.8965, best val acc: 0.8877, best train loss: 0.2617  @ epoch 19294 )\n",
      "[Epoch: 19760] train loss: 0.2762, train acc: 0.8816, val loss: 0.2759, val acc: 0.8782  (best train acc: 0.8965, best val acc: 0.8877, best train loss: 0.2617  @ epoch 19294 )\n",
      "[Epoch: 19780] train loss: 0.2773, train acc: 0.8815, val loss: 0.2711, val acc: 0.8796  (best train acc: 0.8965, best val acc: 0.8877, best train loss: 0.2617  @ epoch 19294 )\n",
      "[Epoch: 19800] train loss: 0.2868, train acc: 0.8890, val loss: 0.3221, val acc: 0.8577  (best train acc: 0.8965, best val acc: 0.8877, best train loss: 0.2617  @ epoch 19294 )\n",
      "[Epoch: 19820] train loss: 0.2981, train acc: 0.8769, val loss: 0.2875, val acc: 0.8799  (best train acc: 0.8965, best val acc: 0.8877, best train loss: 0.2617  @ epoch 19294 )\n",
      "[Epoch: 19840] train loss: 0.2796, train acc: 0.8864, val loss: 0.2880, val acc: 0.8708  (best train acc: 0.8965, best val acc: 0.8877, best train loss: 0.2617  @ epoch 19294 )\n",
      "[Epoch: 19860] train loss: 0.2713, train acc: 0.8897, val loss: 0.2741, val acc: 0.8806  (best train acc: 0.8965, best val acc: 0.8877, best train loss: 0.2617  @ epoch 19294 )\n",
      "[Epoch: 19880] train loss: 0.2943, train acc: 0.8809, val loss: 0.2851, val acc: 0.8759  (best train acc: 0.8965, best val acc: 0.8877, best train loss: 0.2617  @ epoch 19294 )\n",
      "[Epoch: 19900] train loss: 0.2751, train acc: 0.8864, val loss: 0.2749, val acc: 0.8813  (best train acc: 0.8965, best val acc: 0.8877, best train loss: 0.2617  @ epoch 19294 )\n",
      "[Epoch: 19920] train loss: 0.2953, train acc: 0.8797, val loss: 0.3018, val acc: 0.8671  (best train acc: 0.8965, best val acc: 0.8877, best train loss: 0.2617  @ epoch 19294 )\n",
      "[Epoch: 19940] train loss: 0.2838, train acc: 0.8809, val loss: 0.2686, val acc: 0.8759  (best train acc: 0.8965, best val acc: 0.8877, best train loss: 0.2617  @ epoch 19294 )\n",
      "[Epoch: 19960] train loss: 0.2905, train acc: 0.8828, val loss: 0.2723, val acc: 0.8793  (best train acc: 0.8965, best val acc: 0.8877, best train loss: 0.2617  @ epoch 19294 )\n",
      "[Epoch: 19980] train loss: 0.3010, train acc: 0.8717, val loss: 0.2996, val acc: 0.8688  (best train acc: 0.8965, best val acc: 0.8877, best train loss: 0.2617  @ epoch 19294 )\n",
      "[Epoch: 20000] train loss: 0.2843, train acc: 0.8772, val loss: 0.2801, val acc: 0.8803  (best train acc: 0.8965, best val acc: 0.8877, best train loss: 0.2617  @ epoch 19294 )\n",
      "[Epoch: 20020] train loss: 0.2851, train acc: 0.8817, val loss: 0.2712, val acc: 0.8809  (best train acc: 0.8965, best val acc: 0.8880, best train loss: 0.2617  @ epoch 19294 )\n",
      "[Epoch: 20040] train loss: 0.3083, train acc: 0.8723, val loss: 0.2716, val acc: 0.8769  (best train acc: 0.8965, best val acc: 0.8880, best train loss: 0.2617  @ epoch 19294 )\n",
      "[Epoch: 20060] train loss: 0.2892, train acc: 0.8767, val loss: 0.2850, val acc: 0.8739  (best train acc: 0.8965, best val acc: 0.8880, best train loss: 0.2617  @ epoch 19294 )\n",
      "[Epoch: 20080] train loss: 0.3176, train acc: 0.8726, val loss: 0.2789, val acc: 0.8769  (best train acc: 0.8965, best val acc: 0.8880, best train loss: 0.2617  @ epoch 19294 )\n",
      "[Epoch: 20100] train loss: 0.2981, train acc: 0.8762, val loss: 0.2695, val acc: 0.8823  (best train acc: 0.8965, best val acc: 0.8880, best train loss: 0.2617  @ epoch 19294 )\n",
      "[Epoch: 20120] train loss: 0.2705, train acc: 0.8898, val loss: 0.2835, val acc: 0.8752  (best train acc: 0.8965, best val acc: 0.8880, best train loss: 0.2617  @ epoch 19294 )\n",
      "[Epoch: 20140] train loss: 0.2902, train acc: 0.8777, val loss: 0.2778, val acc: 0.8762  (best train acc: 0.8965, best val acc: 0.8880, best train loss: 0.2617  @ epoch 19294 )\n",
      "[Epoch: 20160] train loss: 0.3021, train acc: 0.8759, val loss: 0.2767, val acc: 0.8809  (best train acc: 0.8965, best val acc: 0.8880, best train loss: 0.2603  @ epoch 20157 )\n",
      "[Epoch: 20180] train loss: 0.3134, train acc: 0.8697, val loss: 0.2862, val acc: 0.8776  (best train acc: 0.8965, best val acc: 0.8880, best train loss: 0.2603  @ epoch 20157 )\n",
      "[Epoch: 20200] train loss: 0.3067, train acc: 0.8754, val loss: 0.3050, val acc: 0.8728  (best train acc: 0.8965, best val acc: 0.8880, best train loss: 0.2603  @ epoch 20157 )\n",
      "[Epoch: 20220] train loss: 0.2954, train acc: 0.8738, val loss: 0.3156, val acc: 0.8631  (best train acc: 0.8965, best val acc: 0.8880, best train loss: 0.2603  @ epoch 20157 )\n",
      "[Epoch: 20240] train loss: 0.3130, train acc: 0.8720, val loss: 0.2722, val acc: 0.8796  (best train acc: 0.8965, best val acc: 0.8880, best train loss: 0.2603  @ epoch 20157 )\n",
      "[Epoch: 20260] train loss: 0.2913, train acc: 0.8886, val loss: 0.2838, val acc: 0.8793  (best train acc: 0.8965, best val acc: 0.8880, best train loss: 0.2603  @ epoch 20157 )\n",
      "[Epoch: 20280] train loss: 0.3052, train acc: 0.8831, val loss: 0.2858, val acc: 0.8789  (best train acc: 0.8965, best val acc: 0.8880, best train loss: 0.2603  @ epoch 20157 )\n",
      "[Epoch: 20300] train loss: 0.2891, train acc: 0.8783, val loss: 0.2805, val acc: 0.8793  (best train acc: 0.8965, best val acc: 0.8880, best train loss: 0.2603  @ epoch 20157 )\n",
      "[Epoch: 20320] train loss: 0.3671, train acc: 0.8380, val loss: 0.2747, val acc: 0.8799  (best train acc: 0.8965, best val acc: 0.8880, best train loss: 0.2603  @ epoch 20157 )\n",
      "[Epoch: 20340] train loss: 0.2873, train acc: 0.8785, val loss: 0.2712, val acc: 0.8786  (best train acc: 0.8965, best val acc: 0.8880, best train loss: 0.2603  @ epoch 20157 )\n",
      "[Epoch: 20360] train loss: 0.2762, train acc: 0.8815, val loss: 0.2700, val acc: 0.8813  (best train acc: 0.8965, best val acc: 0.8880, best train loss: 0.2603  @ epoch 20157 )\n",
      "[Epoch: 20380] train loss: 0.3092, train acc: 0.8689, val loss: 0.2806, val acc: 0.8796  (best train acc: 0.8965, best val acc: 0.8880, best train loss: 0.2603  @ epoch 20157 )\n",
      "[Epoch: 20400] train loss: 0.2823, train acc: 0.8805, val loss: 0.2774, val acc: 0.8806  (best train acc: 0.8965, best val acc: 0.8880, best train loss: 0.2603  @ epoch 20157 )\n",
      "[Epoch: 20420] train loss: 0.2896, train acc: 0.8798, val loss: 0.2788, val acc: 0.8796  (best train acc: 0.8965, best val acc: 0.8880, best train loss: 0.2603  @ epoch 20157 )\n",
      "[Epoch: 20440] train loss: 0.3104, train acc: 0.8744, val loss: 0.2810, val acc: 0.8749  (best train acc: 0.8965, best val acc: 0.8880, best train loss: 0.2603  @ epoch 20157 )\n",
      "[Epoch: 20460] train loss: 0.2657, train acc: 0.8908, val loss: 0.2757, val acc: 0.8772  (best train acc: 0.8965, best val acc: 0.8880, best train loss: 0.2603  @ epoch 20157 )\n",
      "[Epoch: 20480] train loss: 0.3014, train acc: 0.8758, val loss: 0.2946, val acc: 0.8712  (best train acc: 0.8965, best val acc: 0.8880, best train loss: 0.2603  @ epoch 20157 )\n",
      "[Epoch: 20500] train loss: 0.2628, train acc: 0.8903, val loss: 0.2734, val acc: 0.8796  (best train acc: 0.8965, best val acc: 0.8880, best train loss: 0.2603  @ epoch 20157 )\n",
      "[Epoch: 20520] train loss: 0.2697, train acc: 0.8871, val loss: 0.2842, val acc: 0.8786  (best train acc: 0.8965, best val acc: 0.8880, best train loss: 0.2603  @ epoch 20157 )\n",
      "[Epoch: 20540] train loss: 0.2748, train acc: 0.8890, val loss: 0.2726, val acc: 0.8826  (best train acc: 0.8965, best val acc: 0.8880, best train loss: 0.2603  @ epoch 20157 )\n",
      "[Epoch: 20560] train loss: 0.2882, train acc: 0.8811, val loss: 0.2695, val acc: 0.8863  (best train acc: 0.8965, best val acc: 0.8880, best train loss: 0.2603  @ epoch 20157 )\n",
      "[Epoch: 20580] train loss: 0.3059, train acc: 0.8757, val loss: 0.3257, val acc: 0.8530  (best train acc: 0.8965, best val acc: 0.8880, best train loss: 0.2603  @ epoch 20157 )\n",
      "[Epoch: 20600] train loss: 0.2858, train acc: 0.8819, val loss: 0.2689, val acc: 0.8830  (best train acc: 0.8965, best val acc: 0.8880, best train loss: 0.2603  @ epoch 20157 )\n",
      "[Epoch: 20620] train loss: 0.2777, train acc: 0.8814, val loss: 0.2919, val acc: 0.8728  (best train acc: 0.8965, best val acc: 0.8880, best train loss: 0.2603  @ epoch 20157 )\n",
      "[Epoch: 20640] train loss: 0.2788, train acc: 0.8812, val loss: 0.2823, val acc: 0.8779  (best train acc: 0.8965, best val acc: 0.8880, best train loss: 0.2603  @ epoch 20157 )\n",
      "[Epoch: 20660] train loss: 0.2752, train acc: 0.8887, val loss: 0.2807, val acc: 0.8786  (best train acc: 0.8965, best val acc: 0.8880, best train loss: 0.2603  @ epoch 20157 )\n",
      "[Epoch: 20680] train loss: 0.2650, train acc: 0.8903, val loss: 0.2729, val acc: 0.8776  (best train acc: 0.8965, best val acc: 0.8880, best train loss: 0.2603  @ epoch 20157 )\n",
      "[Epoch: 20700] train loss: 0.2753, train acc: 0.8854, val loss: 0.2764, val acc: 0.8769  (best train acc: 0.8965, best val acc: 0.8880, best train loss: 0.2603  @ epoch 20157 )\n",
      "[Epoch: 20720] train loss: 0.3153, train acc: 0.8652, val loss: 0.2765, val acc: 0.8799  (best train acc: 0.8965, best val acc: 0.8880, best train loss: 0.2603  @ epoch 20157 )\n",
      "[Epoch: 20740] train loss: 0.2784, train acc: 0.8880, val loss: 0.2886, val acc: 0.8789  (best train acc: 0.8965, best val acc: 0.8880, best train loss: 0.2603  @ epoch 20157 )\n",
      "[Epoch: 20760] train loss: 0.2760, train acc: 0.8866, val loss: 0.2978, val acc: 0.8718  (best train acc: 0.8965, best val acc: 0.8880, best train loss: 0.2603  @ epoch 20157 )\n",
      "[Epoch: 20780] train loss: 0.2962, train acc: 0.8796, val loss: 0.3233, val acc: 0.8648  (best train acc: 0.8965, best val acc: 0.8880, best train loss: 0.2603  @ epoch 20157 )\n",
      "[Epoch: 20800] train loss: 0.2825, train acc: 0.8834, val loss: 0.2748, val acc: 0.8799  (best train acc: 0.8965, best val acc: 0.8880, best train loss: 0.2603  @ epoch 20157 )\n",
      "[Epoch: 20820] train loss: 0.3007, train acc: 0.8794, val loss: 0.2684, val acc: 0.8806  (best train acc: 0.8965, best val acc: 0.8880, best train loss: 0.2603  @ epoch 20157 )\n",
      "[Epoch: 20840] train loss: 0.3044, train acc: 0.8752, val loss: 0.2777, val acc: 0.8752  (best train acc: 0.8965, best val acc: 0.8880, best train loss: 0.2603  @ epoch 20157 )\n",
      "[Epoch: 20860] train loss: 0.2890, train acc: 0.8787, val loss: 0.2822, val acc: 0.8762  (best train acc: 0.8965, best val acc: 0.8880, best train loss: 0.2603  @ epoch 20157 )\n",
      "[Epoch: 20880] train loss: 0.2768, train acc: 0.8856, val loss: 0.3084, val acc: 0.8688  (best train acc: 0.8965, best val acc: 0.8880, best train loss: 0.2603  @ epoch 20157 )\n",
      "[Epoch: 20900] train loss: 0.3035, train acc: 0.8739, val loss: 0.2753, val acc: 0.8786  (best train acc: 0.8965, best val acc: 0.8880, best train loss: 0.2603  @ epoch 20157 )\n",
      "[Epoch: 20920] train loss: 0.3247, train acc: 0.8615, val loss: 0.2776, val acc: 0.8786  (best train acc: 0.8965, best val acc: 0.8880, best train loss: 0.2603  @ epoch 20157 )\n",
      "[Epoch: 20940] train loss: 0.2981, train acc: 0.8754, val loss: 0.2801, val acc: 0.8786  (best train acc: 0.8965, best val acc: 0.8880, best train loss: 0.2603  @ epoch 20157 )\n",
      "[Epoch: 20960] train loss: 0.2711, train acc: 0.8847, val loss: 0.2806, val acc: 0.8772  (best train acc: 0.8965, best val acc: 0.8880, best train loss: 0.2603  @ epoch 20157 )\n",
      "[Epoch: 20980] train loss: 0.2679, train acc: 0.8873, val loss: 0.2734, val acc: 0.8799  (best train acc: 0.8965, best val acc: 0.8880, best train loss: 0.2603  @ epoch 20157 )\n",
      "[Epoch: 21000] train loss: 0.2719, train acc: 0.8865, val loss: 0.2754, val acc: 0.8826  (best train acc: 0.8965, best val acc: 0.8880, best train loss: 0.2576  @ epoch 20984 )\n",
      "[Epoch: 21020] train loss: 0.3191, train acc: 0.8710, val loss: 0.2919, val acc: 0.8739  (best train acc: 0.8965, best val acc: 0.8880, best train loss: 0.2576  @ epoch 20984 )\n",
      "[Epoch: 21040] train loss: 0.2906, train acc: 0.8828, val loss: 0.2855, val acc: 0.8806  (best train acc: 0.8965, best val acc: 0.8880, best train loss: 0.2576  @ epoch 20984 )\n",
      "[Epoch: 21060] train loss: 0.2844, train acc: 0.8826, val loss: 0.2822, val acc: 0.8853  (best train acc: 0.8965, best val acc: 0.8880, best train loss: 0.2576  @ epoch 20984 )\n",
      "[Epoch: 21080] train loss: 0.2914, train acc: 0.8741, val loss: 0.2899, val acc: 0.8772  (best train acc: 0.8965, best val acc: 0.8880, best train loss: 0.2576  @ epoch 20984 )\n",
      "[Epoch: 21100] train loss: 0.2928, train acc: 0.8812, val loss: 0.2870, val acc: 0.8836  (best train acc: 0.8965, best val acc: 0.8880, best train loss: 0.2576  @ epoch 20984 )\n",
      "[Epoch: 21120] train loss: 0.2928, train acc: 0.8800, val loss: 0.2742, val acc: 0.8789  (best train acc: 0.8965, best val acc: 0.8880, best train loss: 0.2576  @ epoch 20984 )\n",
      "[Epoch: 21140] train loss: 0.2771, train acc: 0.8853, val loss: 0.3209, val acc: 0.8685  (best train acc: 0.8965, best val acc: 0.8880, best train loss: 0.2576  @ epoch 20984 )\n",
      "[Epoch: 21160] train loss: 0.2953, train acc: 0.8775, val loss: 0.3153, val acc: 0.8668  (best train acc: 0.8965, best val acc: 0.8880, best train loss: 0.2576  @ epoch 20984 )\n",
      "[Epoch: 21180] train loss: 0.3133, train acc: 0.8756, val loss: 0.2809, val acc: 0.8847  (best train acc: 0.8965, best val acc: 0.8880, best train loss: 0.2576  @ epoch 20984 )\n",
      "[Epoch: 21200] train loss: 0.2655, train acc: 0.8900, val loss: 0.2712, val acc: 0.8843  (best train acc: 0.8973, best val acc: 0.8880, best train loss: 0.2576  @ epoch 20984 )\n",
      "[Epoch: 21220] train loss: 0.2829, train acc: 0.8825, val loss: 0.2808, val acc: 0.8826  (best train acc: 0.8973, best val acc: 0.8880, best train loss: 0.2576  @ epoch 20984 )\n",
      "[Epoch: 21240] train loss: 0.2742, train acc: 0.8879, val loss: 0.2760, val acc: 0.8809  (best train acc: 0.8973, best val acc: 0.8880, best train loss: 0.2576  @ epoch 20984 )\n",
      "[Epoch: 21260] train loss: 0.2817, train acc: 0.8855, val loss: 0.2829, val acc: 0.8833  (best train acc: 0.8973, best val acc: 0.8880, best train loss: 0.2576  @ epoch 20984 )\n",
      "[Epoch: 21280] train loss: 0.2934, train acc: 0.8743, val loss: 0.2971, val acc: 0.8725  (best train acc: 0.8973, best val acc: 0.8880, best train loss: 0.2576  @ epoch 20984 )\n",
      "[Epoch: 21300] train loss: 0.2788, train acc: 0.8843, val loss: 0.2821, val acc: 0.8809  (best train acc: 0.8973, best val acc: 0.8880, best train loss: 0.2576  @ epoch 20984 )\n",
      "[Epoch: 21320] train loss: 0.2761, train acc: 0.8857, val loss: 0.2977, val acc: 0.8755  (best train acc: 0.8973, best val acc: 0.8880, best train loss: 0.2576  @ epoch 20984 )\n",
      "[Epoch: 21340] train loss: 0.2775, train acc: 0.8847, val loss: 0.2739, val acc: 0.8857  (best train acc: 0.8973, best val acc: 0.8880, best train loss: 0.2576  @ epoch 20984 )\n",
      "[Epoch: 21360] train loss: 0.2825, train acc: 0.8820, val loss: 0.2729, val acc: 0.8836  (best train acc: 0.8973, best val acc: 0.8880, best train loss: 0.2576  @ epoch 20984 )\n",
      "[Epoch: 21380] train loss: 0.2835, train acc: 0.8809, val loss: 0.3009, val acc: 0.8749  (best train acc: 0.8973, best val acc: 0.8880, best train loss: 0.2576  @ epoch 20984 )\n",
      "[Epoch: 21400] train loss: 0.3379, train acc: 0.8566, val loss: 0.3309, val acc: 0.8567  (best train acc: 0.8973, best val acc: 0.8880, best train loss: 0.2576  @ epoch 20984 )\n",
      "[Epoch: 21420] train loss: 0.3161, train acc: 0.8699, val loss: 0.2963, val acc: 0.8793  (best train acc: 0.8973, best val acc: 0.8880, best train loss: 0.2576  @ epoch 20984 )\n",
      "[Epoch: 21440] train loss: 0.3058, train acc: 0.8757, val loss: 0.2970, val acc: 0.8769  (best train acc: 0.8973, best val acc: 0.8880, best train loss: 0.2576  @ epoch 20984 )\n",
      "[Epoch: 21460] train loss: 0.2867, train acc: 0.8772, val loss: 0.2875, val acc: 0.8786  (best train acc: 0.8973, best val acc: 0.8880, best train loss: 0.2576  @ epoch 20984 )\n",
      "[Epoch: 21480] train loss: 0.2621, train acc: 0.8954, val loss: 0.2756, val acc: 0.8820  (best train acc: 0.8973, best val acc: 0.8880, best train loss: 0.2576  @ epoch 20984 )\n",
      "[Epoch: 21500] train loss: 0.3445, train acc: 0.8620, val loss: 0.2904, val acc: 0.8779  (best train acc: 0.8973, best val acc: 0.8880, best train loss: 0.2576  @ epoch 20984 )\n",
      "[Epoch: 21520] train loss: 0.2790, train acc: 0.8853, val loss: 0.2856, val acc: 0.8799  (best train acc: 0.8973, best val acc: 0.8880, best train loss: 0.2576  @ epoch 20984 )\n",
      "[Epoch: 21540] train loss: 0.2600, train acc: 0.8947, val loss: 0.2730, val acc: 0.8833  (best train acc: 0.8973, best val acc: 0.8880, best train loss: 0.2576  @ epoch 20984 )\n",
      "[Epoch: 21560] train loss: 0.2782, train acc: 0.8845, val loss: 0.2976, val acc: 0.8702  (best train acc: 0.8973, best val acc: 0.8880, best train loss: 0.2576  @ epoch 20984 )\n",
      "[Epoch: 21580] train loss: 0.2652, train acc: 0.8895, val loss: 0.2756, val acc: 0.8820  (best train acc: 0.8973, best val acc: 0.8880, best train loss: 0.2576  @ epoch 20984 )\n",
      "[Epoch: 21600] train loss: 0.2825, train acc: 0.8821, val loss: 0.2717, val acc: 0.8833  (best train acc: 0.8973, best val acc: 0.8880, best train loss: 0.2564  @ epoch 21590 )\n",
      "[Epoch: 21620] train loss: 0.2777, train acc: 0.8838, val loss: 0.2865, val acc: 0.8779  (best train acc: 0.8973, best val acc: 0.8880, best train loss: 0.2564  @ epoch 21590 )\n",
      "[Epoch: 21640] train loss: 0.3353, train acc: 0.8657, val loss: 0.2817, val acc: 0.8796  (best train acc: 0.8973, best val acc: 0.8880, best train loss: 0.2564  @ epoch 21590 )\n",
      "[Epoch: 21660] train loss: 0.2753, train acc: 0.8888, val loss: 0.2957, val acc: 0.8732  (best train acc: 0.8973, best val acc: 0.8880, best train loss: 0.2564  @ epoch 21590 )\n",
      "[Epoch: 21680] train loss: 0.2797, train acc: 0.8866, val loss: 0.2915, val acc: 0.8718  (best train acc: 0.8973, best val acc: 0.8880, best train loss: 0.2564  @ epoch 21590 )\n",
      "[Epoch: 21700] train loss: 0.2770, train acc: 0.8889, val loss: 0.2730, val acc: 0.8823  (best train acc: 0.8973, best val acc: 0.8880, best train loss: 0.2564  @ epoch 21590 )\n",
      "[Epoch: 21720] train loss: 0.3363, train acc: 0.8573, val loss: 0.3264, val acc: 0.8617  (best train acc: 0.8973, best val acc: 0.8880, best train loss: 0.2564  @ epoch 21590 )\n",
      "[Epoch: 21740] train loss: 0.2876, train acc: 0.8851, val loss: 0.2764, val acc: 0.8823  (best train acc: 0.8973, best val acc: 0.8880, best train loss: 0.2564  @ epoch 21590 )\n",
      "[Epoch: 21760] train loss: 0.2915, train acc: 0.8816, val loss: 0.2821, val acc: 0.8813  (best train acc: 0.8973, best val acc: 0.8880, best train loss: 0.2564  @ epoch 21590 )\n",
      "[Epoch: 21780] train loss: 0.2973, train acc: 0.8734, val loss: 0.2821, val acc: 0.8782  (best train acc: 0.8973, best val acc: 0.8880, best train loss: 0.2564  @ epoch 21590 )\n",
      "[Epoch: 21800] train loss: 0.3338, train acc: 0.8683, val loss: 0.3025, val acc: 0.8755  (best train acc: 0.8973, best val acc: 0.8880, best train loss: 0.2564  @ epoch 21590 )\n",
      "[Epoch: 21820] train loss: 0.2711, train acc: 0.8889, val loss: 0.2798, val acc: 0.8799  (best train acc: 0.8973, best val acc: 0.8884, best train loss: 0.2564  @ epoch 21590 )\n",
      "[Epoch: 21840] train loss: 0.2914, train acc: 0.8775, val loss: 0.2764, val acc: 0.8816  (best train acc: 0.8973, best val acc: 0.8884, best train loss: 0.2564  @ epoch 21590 )\n",
      "[Epoch: 21860] train loss: 0.2888, train acc: 0.8828, val loss: 0.2762, val acc: 0.8860  (best train acc: 0.8973, best val acc: 0.8884, best train loss: 0.2564  @ epoch 21590 )\n",
      "[Epoch: 21880] train loss: 0.2867, train acc: 0.8830, val loss: 0.2859, val acc: 0.8793  (best train acc: 0.8973, best val acc: 0.8884, best train loss: 0.2564  @ epoch 21590 )\n",
      "[Epoch: 21900] train loss: 0.3181, train acc: 0.8719, val loss: 0.2788, val acc: 0.8803  (best train acc: 0.8973, best val acc: 0.8884, best train loss: 0.2564  @ epoch 21590 )\n",
      "[Epoch: 21920] train loss: 0.2780, train acc: 0.8850, val loss: 0.2833, val acc: 0.8793  (best train acc: 0.8973, best val acc: 0.8884, best train loss: 0.2564  @ epoch 21590 )\n",
      "[Epoch: 21940] train loss: 0.2690, train acc: 0.8891, val loss: 0.2824, val acc: 0.8836  (best train acc: 0.8973, best val acc: 0.8884, best train loss: 0.2564  @ epoch 21590 )\n",
      "[Epoch: 21960] train loss: 0.2900, train acc: 0.8856, val loss: 0.2861, val acc: 0.8803  (best train acc: 0.8973, best val acc: 0.8884, best train loss: 0.2564  @ epoch 21590 )\n",
      "[Epoch: 21980] train loss: 0.2801, train acc: 0.8861, val loss: 0.2782, val acc: 0.8836  (best train acc: 0.8973, best val acc: 0.8884, best train loss: 0.2564  @ epoch 21590 )\n",
      "[Epoch: 22000] train loss: 0.2739, train acc: 0.8882, val loss: 0.2783, val acc: 0.8803  (best train acc: 0.8973, best val acc: 0.8884, best train loss: 0.2564  @ epoch 21590 )\n",
      "[Epoch: 22020] train loss: 0.2796, train acc: 0.8820, val loss: 0.2770, val acc: 0.8806  (best train acc: 0.8973, best val acc: 0.8884, best train loss: 0.2564  @ epoch 21590 )\n",
      "[Epoch: 22040] train loss: 0.2658, train acc: 0.8932, val loss: 0.2813, val acc: 0.8830  (best train acc: 0.8973, best val acc: 0.8884, best train loss: 0.2564  @ epoch 21590 )\n",
      "[Epoch: 22060] train loss: 0.2945, train acc: 0.8770, val loss: 0.2765, val acc: 0.8816  (best train acc: 0.8973, best val acc: 0.8884, best train loss: 0.2564  @ epoch 21590 )\n",
      "[Epoch: 22080] train loss: 0.2842, train acc: 0.8832, val loss: 0.2970, val acc: 0.8742  (best train acc: 0.8973, best val acc: 0.8884, best train loss: 0.2564  @ epoch 21590 )\n",
      "[Epoch: 22100] train loss: 0.2838, train acc: 0.8785, val loss: 0.2934, val acc: 0.8766  (best train acc: 0.8973, best val acc: 0.8884, best train loss: 0.2564  @ epoch 21590 )\n",
      "[Epoch: 22120] train loss: 0.2934, train acc: 0.8754, val loss: 0.2967, val acc: 0.8739  (best train acc: 0.8973, best val acc: 0.8884, best train loss: 0.2564  @ epoch 21590 )\n",
      "[Epoch: 22140] train loss: 0.2690, train acc: 0.8910, val loss: 0.2803, val acc: 0.8830  (best train acc: 0.8973, best val acc: 0.8884, best train loss: 0.2564  @ epoch 21590 )\n",
      "[Epoch: 22160] train loss: 0.2776, train acc: 0.8849, val loss: 0.2759, val acc: 0.8836  (best train acc: 0.8985, best val acc: 0.8884, best train loss: 0.2536  @ epoch 22155 )\n",
      "[Epoch: 22180] train loss: 0.2836, train acc: 0.8834, val loss: 0.2830, val acc: 0.8823  (best train acc: 0.8985, best val acc: 0.8884, best train loss: 0.2515  @ epoch 22161 )\n",
      "[Epoch: 22200] train loss: 0.2710, train acc: 0.8874, val loss: 0.2724, val acc: 0.8803  (best train acc: 0.8985, best val acc: 0.8884, best train loss: 0.2515  @ epoch 22161 )\n",
      "[Epoch: 22220] train loss: 0.3124, train acc: 0.8717, val loss: 0.2752, val acc: 0.8857  (best train acc: 0.8985, best val acc: 0.8884, best train loss: 0.2515  @ epoch 22161 )\n",
      "[Epoch: 22240] train loss: 0.2921, train acc: 0.8809, val loss: 0.2708, val acc: 0.8843  (best train acc: 0.8985, best val acc: 0.8884, best train loss: 0.2515  @ epoch 22161 )\n",
      "[Epoch: 22260] train loss: 0.2830, train acc: 0.8834, val loss: 0.2758, val acc: 0.8853  (best train acc: 0.8985, best val acc: 0.8884, best train loss: 0.2515  @ epoch 22161 )\n",
      "[Epoch: 22280] train loss: 0.3000, train acc: 0.8807, val loss: 0.2885, val acc: 0.8799  (best train acc: 0.8985, best val acc: 0.8884, best train loss: 0.2515  @ epoch 22161 )\n",
      "[Epoch: 22300] train loss: 0.2764, train acc: 0.8912, val loss: 0.2883, val acc: 0.8759  (best train acc: 0.8985, best val acc: 0.8884, best train loss: 0.2515  @ epoch 22161 )\n",
      "[Epoch: 22320] train loss: 0.2821, train acc: 0.8819, val loss: 0.2874, val acc: 0.8830  (best train acc: 0.8985, best val acc: 0.8897, best train loss: 0.2515  @ epoch 22161 )\n",
      "[Epoch: 22340] train loss: 0.2600, train acc: 0.8929, val loss: 0.2996, val acc: 0.8708  (best train acc: 0.8985, best val acc: 0.8897, best train loss: 0.2515  @ epoch 22161 )\n",
      "[Epoch: 22360] train loss: 0.2974, train acc: 0.8800, val loss: 0.2719, val acc: 0.8863  (best train acc: 0.8985, best val acc: 0.8897, best train loss: 0.2515  @ epoch 22161 )\n",
      "[Epoch: 22380] train loss: 0.2652, train acc: 0.8939, val loss: 0.2761, val acc: 0.8803  (best train acc: 0.8985, best val acc: 0.8897, best train loss: 0.2515  @ epoch 22161 )\n",
      "[Epoch: 22400] train loss: 0.2764, train acc: 0.8850, val loss: 0.2785, val acc: 0.8789  (best train acc: 0.8985, best val acc: 0.8897, best train loss: 0.2515  @ epoch 22161 )\n",
      "[Epoch: 22420] train loss: 0.2699, train acc: 0.8914, val loss: 0.3509, val acc: 0.8462  (best train acc: 0.8985, best val acc: 0.8897, best train loss: 0.2515  @ epoch 22161 )\n",
      "[Epoch: 22440] train loss: 0.2997, train acc: 0.8751, val loss: 0.3459, val acc: 0.8492  (best train acc: 0.8985, best val acc: 0.8897, best train loss: 0.2515  @ epoch 22161 )\n",
      "[Epoch: 22460] train loss: 0.2832, train acc: 0.8817, val loss: 0.2767, val acc: 0.8823  (best train acc: 0.8985, best val acc: 0.8897, best train loss: 0.2515  @ epoch 22161 )\n",
      "[Epoch: 22480] train loss: 0.2908, train acc: 0.8824, val loss: 0.2736, val acc: 0.8830  (best train acc: 0.8985, best val acc: 0.8897, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 22500] train loss: 0.2910, train acc: 0.8811, val loss: 0.3151, val acc: 0.8668  (best train acc: 0.8985, best val acc: 0.8897, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 22520] train loss: 0.2803, train acc: 0.8884, val loss: 0.2785, val acc: 0.8799  (best train acc: 0.8985, best val acc: 0.8897, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 22540] train loss: 0.3002, train acc: 0.8797, val loss: 0.3082, val acc: 0.8702  (best train acc: 0.8985, best val acc: 0.8897, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 22560] train loss: 0.3121, train acc: 0.8629, val loss: 0.2719, val acc: 0.8850  (best train acc: 0.8985, best val acc: 0.8897, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 22580] train loss: 0.2854, train acc: 0.8832, val loss: 0.2798, val acc: 0.8823  (best train acc: 0.8985, best val acc: 0.8897, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 22600] train loss: 0.2844, train acc: 0.8864, val loss: 0.3083, val acc: 0.8648  (best train acc: 0.8985, best val acc: 0.8897, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 22620] train loss: 0.2910, train acc: 0.8814, val loss: 0.2943, val acc: 0.8779  (best train acc: 0.8985, best val acc: 0.8897, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 22640] train loss: 0.2896, train acc: 0.8786, val loss: 0.3416, val acc: 0.8479  (best train acc: 0.8985, best val acc: 0.8897, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 22660] train loss: 0.2838, train acc: 0.8819, val loss: 0.3182, val acc: 0.8590  (best train acc: 0.8985, best val acc: 0.8897, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 22680] train loss: 0.2794, train acc: 0.8856, val loss: 0.2882, val acc: 0.8823  (best train acc: 0.8985, best val acc: 0.8897, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 22700] train loss: 0.2871, train acc: 0.8792, val loss: 0.3063, val acc: 0.8698  (best train acc: 0.8985, best val acc: 0.8897, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 22720] train loss: 0.2687, train acc: 0.8887, val loss: 0.2774, val acc: 0.8823  (best train acc: 0.8985, best val acc: 0.8897, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 22740] train loss: 0.2636, train acc: 0.8928, val loss: 0.2881, val acc: 0.8786  (best train acc: 0.8985, best val acc: 0.8897, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 22760] train loss: 0.2737, train acc: 0.8882, val loss: 0.2768, val acc: 0.8840  (best train acc: 0.8985, best val acc: 0.8897, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 22780] train loss: 0.2889, train acc: 0.8763, val loss: 0.2898, val acc: 0.8745  (best train acc: 0.8985, best val acc: 0.8897, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 22800] train loss: 0.2811, train acc: 0.8817, val loss: 0.2875, val acc: 0.8782  (best train acc: 0.8985, best val acc: 0.8897, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 22820] train loss: 0.2885, train acc: 0.8739, val loss: 0.2869, val acc: 0.8772  (best train acc: 0.8985, best val acc: 0.8897, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 22840] train loss: 0.3316, train acc: 0.8722, val loss: 0.2782, val acc: 0.8850  (best train acc: 0.8985, best val acc: 0.8897, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 22860] train loss: 0.2641, train acc: 0.8867, val loss: 0.2668, val acc: 0.8847  (best train acc: 0.8985, best val acc: 0.8897, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 22880] train loss: 0.2864, train acc: 0.8784, val loss: 0.2834, val acc: 0.8755  (best train acc: 0.8985, best val acc: 0.8897, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 22900] train loss: 0.2612, train acc: 0.8921, val loss: 0.2771, val acc: 0.8782  (best train acc: 0.8985, best val acc: 0.8897, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 22920] train loss: 0.2729, train acc: 0.8871, val loss: 0.2824, val acc: 0.8806  (best train acc: 0.8985, best val acc: 0.8897, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 22940] train loss: 0.3149, train acc: 0.8723, val loss: 0.3110, val acc: 0.8641  (best train acc: 0.8985, best val acc: 0.8897, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 22960] train loss: 0.2690, train acc: 0.8888, val loss: 0.2809, val acc: 0.8820  (best train acc: 0.8985, best val acc: 0.8897, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 22980] train loss: 0.2935, train acc: 0.8748, val loss: 0.2880, val acc: 0.8752  (best train acc: 0.8985, best val acc: 0.8897, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 23000] train loss: 0.2680, train acc: 0.8883, val loss: 0.2797, val acc: 0.8836  (best train acc: 0.8985, best val acc: 0.8897, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 23020] train loss: 0.2591, train acc: 0.8951, val loss: 0.2791, val acc: 0.8803  (best train acc: 0.8985, best val acc: 0.8897, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 23040] train loss: 0.2821, train acc: 0.8826, val loss: 0.2858, val acc: 0.8833  (best train acc: 0.8985, best val acc: 0.8897, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 23060] train loss: 0.2767, train acc: 0.8869, val loss: 0.2851, val acc: 0.8762  (best train acc: 0.8985, best val acc: 0.8897, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 23080] train loss: 0.2895, train acc: 0.8796, val loss: 0.2888, val acc: 0.8776  (best train acc: 0.8985, best val acc: 0.8897, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 23100] train loss: 0.2862, train acc: 0.8822, val loss: 0.2876, val acc: 0.8840  (best train acc: 0.8985, best val acc: 0.8897, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 23120] train loss: 0.2815, train acc: 0.8837, val loss: 0.2797, val acc: 0.8806  (best train acc: 0.8985, best val acc: 0.8897, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 23140] train loss: 0.2828, train acc: 0.8856, val loss: 0.2717, val acc: 0.8880  (best train acc: 0.8985, best val acc: 0.8897, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 23160] train loss: 0.2775, train acc: 0.8860, val loss: 0.2804, val acc: 0.8820  (best train acc: 0.8985, best val acc: 0.8897, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 23180] train loss: 0.2667, train acc: 0.8885, val loss: 0.2811, val acc: 0.8826  (best train acc: 0.8985, best val acc: 0.8897, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 23200] train loss: 0.3075, train acc: 0.8767, val loss: 0.2888, val acc: 0.8772  (best train acc: 0.8985, best val acc: 0.8897, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 23220] train loss: 0.2769, train acc: 0.8848, val loss: 0.2758, val acc: 0.8833  (best train acc: 0.8985, best val acc: 0.8897, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 23240] train loss: 0.3116, train acc: 0.8660, val loss: 0.3195, val acc: 0.8617  (best train acc: 0.8985, best val acc: 0.8897, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 23260] train loss: 0.2584, train acc: 0.8897, val loss: 0.2758, val acc: 0.8799  (best train acc: 0.8985, best val acc: 0.8897, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 23280] train loss: 0.2963, train acc: 0.8819, val loss: 0.2862, val acc: 0.8809  (best train acc: 0.8985, best val acc: 0.8897, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 23300] train loss: 0.3120, train acc: 0.8770, val loss: 0.2941, val acc: 0.8759  (best train acc: 0.8985, best val acc: 0.8897, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 23320] train loss: 0.3161, train acc: 0.8705, val loss: 0.2867, val acc: 0.8772  (best train acc: 0.8985, best val acc: 0.8897, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 23340] train loss: 0.2720, train acc: 0.8853, val loss: 0.2840, val acc: 0.8816  (best train acc: 0.8985, best val acc: 0.8897, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 23360] train loss: 0.2786, train acc: 0.8859, val loss: 0.2841, val acc: 0.8745  (best train acc: 0.8985, best val acc: 0.8897, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 23380] train loss: 0.2833, train acc: 0.8814, val loss: 0.2913, val acc: 0.8735  (best train acc: 0.8985, best val acc: 0.8897, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 23400] train loss: 0.3059, train acc: 0.8729, val loss: 0.2782, val acc: 0.8823  (best train acc: 0.8985, best val acc: 0.8897, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 23420] train loss: 0.2706, train acc: 0.8899, val loss: 0.2787, val acc: 0.8870  (best train acc: 0.8985, best val acc: 0.8897, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 23440] train loss: 0.3014, train acc: 0.8803, val loss: 0.3000, val acc: 0.8718  (best train acc: 0.8985, best val acc: 0.8897, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 23460] train loss: 0.2749, train acc: 0.8852, val loss: 0.2772, val acc: 0.8823  (best train acc: 0.8985, best val acc: 0.8897, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 23480] train loss: 0.2714, train acc: 0.8870, val loss: 0.2913, val acc: 0.8806  (best train acc: 0.8985, best val acc: 0.8897, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 23500] train loss: 0.2905, train acc: 0.8793, val loss: 0.2755, val acc: 0.8806  (best train acc: 0.8985, best val acc: 0.8897, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 23520] train loss: 0.2870, train acc: 0.8890, val loss: 0.3049, val acc: 0.8688  (best train acc: 0.8985, best val acc: 0.8901, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 23540] train loss: 0.2685, train acc: 0.8929, val loss: 0.2788, val acc: 0.8836  (best train acc: 0.8985, best val acc: 0.8901, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 23560] train loss: 0.2832, train acc: 0.8825, val loss: 0.2772, val acc: 0.8820  (best train acc: 0.8985, best val acc: 0.8901, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 23580] train loss: 0.2625, train acc: 0.8879, val loss: 0.2883, val acc: 0.8752  (best train acc: 0.8985, best val acc: 0.8904, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 23600] train loss: 0.2532, train acc: 0.8975, val loss: 0.2753, val acc: 0.8796  (best train acc: 0.8985, best val acc: 0.8904, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 23620] train loss: 0.3073, train acc: 0.8756, val loss: 0.2783, val acc: 0.8813  (best train acc: 0.8985, best val acc: 0.8904, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 23640] train loss: 0.2903, train acc: 0.8801, val loss: 0.2791, val acc: 0.8806  (best train acc: 0.8985, best val acc: 0.8904, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 23660] train loss: 0.2708, train acc: 0.8860, val loss: 0.2779, val acc: 0.8772  (best train acc: 0.8985, best val acc: 0.8904, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 23680] train loss: 0.2807, train acc: 0.8833, val loss: 0.2748, val acc: 0.8884  (best train acc: 0.8985, best val acc: 0.8904, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 23700] train loss: 0.2934, train acc: 0.8824, val loss: 0.2883, val acc: 0.8809  (best train acc: 0.8985, best val acc: 0.8904, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 23720] train loss: 0.3011, train acc: 0.8793, val loss: 0.2980, val acc: 0.8739  (best train acc: 0.8985, best val acc: 0.8904, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 23740] train loss: 0.2722, train acc: 0.8929, val loss: 0.2663, val acc: 0.8840  (best train acc: 0.8985, best val acc: 0.8904, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 23760] train loss: 0.2875, train acc: 0.8832, val loss: 0.2738, val acc: 0.8813  (best train acc: 0.8985, best val acc: 0.8904, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 23780] train loss: 0.2664, train acc: 0.8895, val loss: 0.2709, val acc: 0.8884  (best train acc: 0.8985, best val acc: 0.8904, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 23800] train loss: 0.3028, train acc: 0.8686, val loss: 0.3344, val acc: 0.8580  (best train acc: 0.8985, best val acc: 0.8904, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 23820] train loss: 0.2752, train acc: 0.8873, val loss: 0.2795, val acc: 0.8847  (best train acc: 0.8985, best val acc: 0.8904, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 23840] train loss: 0.2787, train acc: 0.8842, val loss: 0.2776, val acc: 0.8799  (best train acc: 0.8985, best val acc: 0.8911, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 23860] train loss: 0.3098, train acc: 0.8745, val loss: 0.2888, val acc: 0.8799  (best train acc: 0.8985, best val acc: 0.8911, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 23880] train loss: 0.2949, train acc: 0.8814, val loss: 0.2743, val acc: 0.8836  (best train acc: 0.8985, best val acc: 0.8911, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 23900] train loss: 0.2685, train acc: 0.8887, val loss: 0.2741, val acc: 0.8843  (best train acc: 0.8985, best val acc: 0.8911, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 23920] train loss: 0.2685, train acc: 0.8951, val loss: 0.3062, val acc: 0.8658  (best train acc: 0.8985, best val acc: 0.8911, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 23940] train loss: 0.2861, train acc: 0.8800, val loss: 0.2787, val acc: 0.8793  (best train acc: 0.8985, best val acc: 0.8911, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 23960] train loss: 0.2730, train acc: 0.8879, val loss: 0.2710, val acc: 0.8863  (best train acc: 0.8985, best val acc: 0.8911, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 23980] train loss: 0.3104, train acc: 0.8721, val loss: 0.2887, val acc: 0.8745  (best train acc: 0.8985, best val acc: 0.8911, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 24000] train loss: 0.2691, train acc: 0.8869, val loss: 0.2747, val acc: 0.8853  (best train acc: 0.8985, best val acc: 0.8911, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 24020] train loss: 0.2927, train acc: 0.8811, val loss: 0.2960, val acc: 0.8749  (best train acc: 0.8985, best val acc: 0.8911, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 24040] train loss: 0.2817, train acc: 0.8872, val loss: 0.2743, val acc: 0.8840  (best train acc: 0.8985, best val acc: 0.8911, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 24060] train loss: 0.2648, train acc: 0.8859, val loss: 0.2819, val acc: 0.8809  (best train acc: 0.8985, best val acc: 0.8911, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 24080] train loss: 0.2956, train acc: 0.8798, val loss: 0.2710, val acc: 0.8870  (best train acc: 0.8985, best val acc: 0.8911, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 24100] train loss: 0.2689, train acc: 0.8887, val loss: 0.2699, val acc: 0.8840  (best train acc: 0.8985, best val acc: 0.8911, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 24120] train loss: 0.2895, train acc: 0.8827, val loss: 0.2792, val acc: 0.8816  (best train acc: 0.8985, best val acc: 0.8911, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 24140] train loss: 0.2787, train acc: 0.8827, val loss: 0.2818, val acc: 0.8806  (best train acc: 0.8985, best val acc: 0.8911, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 24160] train loss: 0.2896, train acc: 0.8835, val loss: 0.2800, val acc: 0.8789  (best train acc: 0.8985, best val acc: 0.8911, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 24180] train loss: 0.2639, train acc: 0.8890, val loss: 0.2944, val acc: 0.8718  (best train acc: 0.8985, best val acc: 0.8911, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 24200] train loss: 0.2690, train acc: 0.8887, val loss: 0.2859, val acc: 0.8759  (best train acc: 0.8985, best val acc: 0.8911, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 24220] train loss: 0.2652, train acc: 0.8890, val loss: 0.2700, val acc: 0.8867  (best train acc: 0.8985, best val acc: 0.8911, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 24240] train loss: 0.2741, train acc: 0.8854, val loss: 0.2889, val acc: 0.8769  (best train acc: 0.8985, best val acc: 0.8911, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 24260] train loss: 0.2774, train acc: 0.8833, val loss: 0.3069, val acc: 0.8654  (best train acc: 0.8985, best val acc: 0.8911, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 24280] train loss: 0.3330, train acc: 0.8644, val loss: 0.3066, val acc: 0.8671  (best train acc: 0.8985, best val acc: 0.8911, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 24300] train loss: 0.2796, train acc: 0.8870, val loss: 0.2821, val acc: 0.8772  (best train acc: 0.8985, best val acc: 0.8911, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 24320] train loss: 0.2693, train acc: 0.8896, val loss: 0.2744, val acc: 0.8803  (best train acc: 0.8985, best val acc: 0.8911, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 24340] train loss: 0.2737, train acc: 0.8842, val loss: 0.2778, val acc: 0.8853  (best train acc: 0.8985, best val acc: 0.8911, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 24360] train loss: 0.3048, train acc: 0.8748, val loss: 0.2728, val acc: 0.8836  (best train acc: 0.8993, best val acc: 0.8911, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 24380] train loss: 0.2794, train acc: 0.8843, val loss: 0.2708, val acc: 0.8863  (best train acc: 0.8993, best val acc: 0.8911, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 24400] train loss: 0.2791, train acc: 0.8904, val loss: 0.2935, val acc: 0.8786  (best train acc: 0.8993, best val acc: 0.8911, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 24420] train loss: 0.3092, train acc: 0.8759, val loss: 0.2787, val acc: 0.8874  (best train acc: 0.8993, best val acc: 0.8911, best train loss: 0.2513  @ epoch 22477 )\n",
      "[Epoch: 24440] train loss: 0.2636, train acc: 0.8918, val loss: 0.2782, val acc: 0.8830  (best train acc: 0.8993, best val acc: 0.8911, best train loss: 0.2495  @ epoch 24423 )\n",
      "[Epoch: 24460] train loss: 0.2674, train acc: 0.8904, val loss: 0.2927, val acc: 0.8799  (best train acc: 0.8993, best val acc: 0.8911, best train loss: 0.2495  @ epoch 24423 )\n",
      "[Epoch: 24480] train loss: 0.2731, train acc: 0.8871, val loss: 0.2692, val acc: 0.8816  (best train acc: 0.8993, best val acc: 0.8911, best train loss: 0.2495  @ epoch 24423 )\n",
      "[Epoch: 24500] train loss: 0.2836, train acc: 0.8830, val loss: 0.2843, val acc: 0.8847  (best train acc: 0.8993, best val acc: 0.8911, best train loss: 0.2495  @ epoch 24423 )\n",
      "[Epoch: 24520] train loss: 0.3103, train acc: 0.8639, val loss: 0.3008, val acc: 0.8698  (best train acc: 0.8993, best val acc: 0.8911, best train loss: 0.2495  @ epoch 24423 )\n",
      "[Epoch: 24540] train loss: 0.2652, train acc: 0.8895, val loss: 0.2685, val acc: 0.8833  (best train acc: 0.9020, best val acc: 0.8911, best train loss: 0.2495  @ epoch 24423 )\n",
      "[Epoch: 24560] train loss: 0.2570, train acc: 0.8934, val loss: 0.2802, val acc: 0.8857  (best train acc: 0.9020, best val acc: 0.8911, best train loss: 0.2495  @ epoch 24423 )\n",
      "[Epoch: 24580] train loss: 0.2750, train acc: 0.8921, val loss: 0.2749, val acc: 0.8847  (best train acc: 0.9020, best val acc: 0.8911, best train loss: 0.2494  @ epoch 24576 )\n",
      "[Epoch: 24600] train loss: 0.2764, train acc: 0.8891, val loss: 0.2851, val acc: 0.8809  (best train acc: 0.9020, best val acc: 0.8911, best train loss: 0.2494  @ epoch 24576 )\n",
      "[Epoch: 24620] train loss: 0.2696, train acc: 0.8932, val loss: 0.2783, val acc: 0.8863  (best train acc: 0.9020, best val acc: 0.8911, best train loss: 0.2494  @ epoch 24576 )\n",
      "[Epoch: 24640] train loss: 0.2885, train acc: 0.8855, val loss: 0.2842, val acc: 0.8752  (best train acc: 0.9020, best val acc: 0.8911, best train loss: 0.2494  @ epoch 24576 )\n",
      "[Epoch: 24660] train loss: 0.2804, train acc: 0.8858, val loss: 0.2845, val acc: 0.8779  (best train acc: 0.9020, best val acc: 0.8911, best train loss: 0.2494  @ epoch 24576 )\n",
      "[Epoch: 24680] train loss: 0.3123, train acc: 0.8751, val loss: 0.2786, val acc: 0.8884  (best train acc: 0.9020, best val acc: 0.8911, best train loss: 0.2494  @ epoch 24576 )\n",
      "[Epoch: 24700] train loss: 0.2578, train acc: 0.8914, val loss: 0.2880, val acc: 0.8735  (best train acc: 0.9020, best val acc: 0.8911, best train loss: 0.2494  @ epoch 24576 )\n",
      "[Epoch: 24720] train loss: 0.3043, train acc: 0.8829, val loss: 0.2815, val acc: 0.8890  (best train acc: 0.9020, best val acc: 0.8911, best train loss: 0.2494  @ epoch 24576 )\n",
      "[Epoch: 24740] train loss: 0.2724, train acc: 0.8869, val loss: 0.2675, val acc: 0.8870  (best train acc: 0.9020, best val acc: 0.8911, best train loss: 0.2494  @ epoch 24576 )\n",
      "[Epoch: 24760] train loss: 0.2699, train acc: 0.8886, val loss: 0.2811, val acc: 0.8823  (best train acc: 0.9020, best val acc: 0.8911, best train loss: 0.2494  @ epoch 24576 )\n",
      "[Epoch: 24780] train loss: 0.2662, train acc: 0.8898, val loss: 0.2751, val acc: 0.8830  (best train acc: 0.9020, best val acc: 0.8911, best train loss: 0.2492  @ epoch 24762 )\n",
      "[Epoch: 24800] train loss: 0.2696, train acc: 0.8902, val loss: 0.2739, val acc: 0.8796  (best train acc: 0.9020, best val acc: 0.8911, best train loss: 0.2492  @ epoch 24762 )\n",
      "[Epoch: 24820] train loss: 0.2636, train acc: 0.8954, val loss: 0.2929, val acc: 0.8769  (best train acc: 0.9020, best val acc: 0.8911, best train loss: 0.2492  @ epoch 24762 )\n",
      "[Epoch: 24840] train loss: 0.2677, train acc: 0.8875, val loss: 0.2820, val acc: 0.8779  (best train acc: 0.9020, best val acc: 0.8911, best train loss: 0.2492  @ epoch 24762 )\n",
      "[Epoch: 24860] train loss: 0.2773, train acc: 0.8860, val loss: 0.2864, val acc: 0.8786  (best train acc: 0.9020, best val acc: 0.8911, best train loss: 0.2492  @ epoch 24762 )\n",
      "[Epoch: 24880] train loss: 0.2671, train acc: 0.8862, val loss: 0.2705, val acc: 0.8894  (best train acc: 0.9020, best val acc: 0.8911, best train loss: 0.2492  @ epoch 24762 )\n",
      "[Epoch: 24900] train loss: 0.3057, train acc: 0.8703, val loss: 0.2905, val acc: 0.8749  (best train acc: 0.9020, best val acc: 0.8911, best train loss: 0.2492  @ epoch 24762 )\n",
      "[Epoch: 24920] train loss: 0.3526, train acc: 0.8525, val loss: 0.3702, val acc: 0.8381  (best train acc: 0.9020, best val acc: 0.8911, best train loss: 0.2492  @ epoch 24762 )\n",
      "[Epoch: 24940] train loss: 0.2836, train acc: 0.8821, val loss: 0.2802, val acc: 0.8830  (best train acc: 0.9020, best val acc: 0.8911, best train loss: 0.2492  @ epoch 24762 )\n",
      "[Epoch: 24960] train loss: 0.2856, train acc: 0.8858, val loss: 0.2777, val acc: 0.8806  (best train acc: 0.9020, best val acc: 0.8911, best train loss: 0.2492  @ epoch 24762 )\n",
      "[Epoch: 24980] train loss: 0.2810, train acc: 0.8873, val loss: 0.2906, val acc: 0.8769  (best train acc: 0.9020, best val acc: 0.8911, best train loss: 0.2492  @ epoch 24762 )\n",
      "[Epoch: 25000] train loss: 0.2878, train acc: 0.8858, val loss: 0.2864, val acc: 0.8816  (best train acc: 0.9020, best val acc: 0.8911, best train loss: 0.2492  @ epoch 24762 )\n",
      "[Epoch: 25020] train loss: 0.2606, train acc: 0.8898, val loss: 0.2697, val acc: 0.8874  (best train acc: 0.9020, best val acc: 0.8911, best train loss: 0.2492  @ epoch 24762 )\n",
      "[Epoch: 25040] train loss: 0.2685, train acc: 0.8920, val loss: 0.3057, val acc: 0.8610  (best train acc: 0.9020, best val acc: 0.8911, best train loss: 0.2492  @ epoch 24762 )\n",
      "[Epoch: 25060] train loss: 0.2850, train acc: 0.8882, val loss: 0.2862, val acc: 0.8789  (best train acc: 0.9020, best val acc: 0.8911, best train loss: 0.2492  @ epoch 24762 )\n",
      "[Epoch: 25080] train loss: 0.2811, train acc: 0.8808, val loss: 0.2859, val acc: 0.8816  (best train acc: 0.9020, best val acc: 0.8911, best train loss: 0.2492  @ epoch 24762 )\n",
      "[Epoch: 25100] train loss: 0.2781, train acc: 0.8815, val loss: 0.2754, val acc: 0.8863  (best train acc: 0.9020, best val acc: 0.8911, best train loss: 0.2492  @ epoch 24762 )\n",
      "[Epoch: 25120] train loss: 0.2724, train acc: 0.8872, val loss: 0.2753, val acc: 0.8782  (best train acc: 0.9020, best val acc: 0.8911, best train loss: 0.2492  @ epoch 24762 )\n",
      "[Epoch: 25140] train loss: 0.2840, train acc: 0.8870, val loss: 0.2917, val acc: 0.8712  (best train acc: 0.9020, best val acc: 0.8911, best train loss: 0.2492  @ epoch 24762 )\n",
      "[Epoch: 25160] train loss: 0.2539, train acc: 0.8937, val loss: 0.2719, val acc: 0.8823  (best train acc: 0.9020, best val acc: 0.8911, best train loss: 0.2492  @ epoch 24762 )\n",
      "[Epoch: 25180] train loss: 0.2650, train acc: 0.8958, val loss: 0.2900, val acc: 0.8803  (best train acc: 0.9020, best val acc: 0.8911, best train loss: 0.2492  @ epoch 24762 )\n",
      "[Epoch: 25200] train loss: 0.2844, train acc: 0.8827, val loss: 0.2983, val acc: 0.8762  (best train acc: 0.9020, best val acc: 0.8911, best train loss: 0.2492  @ epoch 24762 )\n",
      "[Epoch: 25220] train loss: 0.3050, train acc: 0.8738, val loss: 0.2823, val acc: 0.8809  (best train acc: 0.9020, best val acc: 0.8911, best train loss: 0.2492  @ epoch 24762 )\n",
      "[Epoch: 25240] train loss: 0.2726, train acc: 0.8882, val loss: 0.3113, val acc: 0.8651  (best train acc: 0.9020, best val acc: 0.8911, best train loss: 0.2492  @ epoch 24762 )\n",
      "[Epoch: 25260] train loss: 0.3859, train acc: 0.8344, val loss: 0.2873, val acc: 0.8813  (best train acc: 0.9020, best val acc: 0.8911, best train loss: 0.2492  @ epoch 24762 )\n",
      "[Epoch: 25280] train loss: 0.2763, train acc: 0.8863, val loss: 0.2713, val acc: 0.8880  (best train acc: 0.9020, best val acc: 0.8927, best train loss: 0.2492  @ epoch 24762 )\n",
      "[Epoch: 25300] train loss: 0.2558, train acc: 0.8897, val loss: 0.2712, val acc: 0.8853  (best train acc: 0.9020, best val acc: 0.8927, best train loss: 0.2492  @ epoch 24762 )\n",
      "[Epoch: 25320] train loss: 0.2841, train acc: 0.8809, val loss: 0.2786, val acc: 0.8809  (best train acc: 0.9020, best val acc: 0.8927, best train loss: 0.2492  @ epoch 24762 )\n",
      "[Epoch: 25340] train loss: 0.2584, train acc: 0.8910, val loss: 0.2693, val acc: 0.8863  (best train acc: 0.9020, best val acc: 0.8927, best train loss: 0.2492  @ epoch 24762 )\n",
      "[Epoch: 25360] train loss: 0.2718, train acc: 0.8939, val loss: 0.2988, val acc: 0.8772  (best train acc: 0.9020, best val acc: 0.8927, best train loss: 0.2470  @ epoch 25353 )\n",
      "[Epoch: 25380] train loss: 0.2535, train acc: 0.8931, val loss: 0.2743, val acc: 0.8860  (best train acc: 0.9020, best val acc: 0.8927, best train loss: 0.2470  @ epoch 25353 )\n",
      "[Epoch: 25400] train loss: 0.2698, train acc: 0.8911, val loss: 0.2759, val acc: 0.8857  (best train acc: 0.9020, best val acc: 0.8927, best train loss: 0.2470  @ epoch 25353 )\n",
      "[Epoch: 25420] train loss: 0.2641, train acc: 0.8880, val loss: 0.2685, val acc: 0.8863  (best train acc: 0.9020, best val acc: 0.8927, best train loss: 0.2424  @ epoch 25408 )\n",
      "[Epoch: 25440] train loss: 0.2630, train acc: 0.8953, val loss: 0.2794, val acc: 0.8853  (best train acc: 0.9020, best val acc: 0.8927, best train loss: 0.2424  @ epoch 25408 )\n",
      "[Epoch: 25460] train loss: 0.2630, train acc: 0.8910, val loss: 0.2684, val acc: 0.8867  (best train acc: 0.9020, best val acc: 0.8927, best train loss: 0.2424  @ epoch 25408 )\n",
      "[Epoch: 25480] train loss: 0.2735, train acc: 0.8856, val loss: 0.3164, val acc: 0.8678  (best train acc: 0.9020, best val acc: 0.8927, best train loss: 0.2424  @ epoch 25408 )\n",
      "[Epoch: 25500] train loss: 0.2668, train acc: 0.8892, val loss: 0.2718, val acc: 0.8833  (best train acc: 0.9020, best val acc: 0.8927, best train loss: 0.2424  @ epoch 25408 )\n",
      "[Epoch: 25520] train loss: 0.2569, train acc: 0.8920, val loss: 0.2894, val acc: 0.8769  (best train acc: 0.9020, best val acc: 0.8927, best train loss: 0.2424  @ epoch 25408 )\n",
      "[Epoch: 25540] train loss: 0.2613, train acc: 0.8908, val loss: 0.2683, val acc: 0.8907  (best train acc: 0.9020, best val acc: 0.8927, best train loss: 0.2424  @ epoch 25408 )\n",
      "[Epoch: 25560] train loss: 0.2836, train acc: 0.8824, val loss: 0.2645, val acc: 0.8870  (best train acc: 0.9020, best val acc: 0.8927, best train loss: 0.2424  @ epoch 25408 )\n",
      "[Epoch: 25580] train loss: 0.2850, train acc: 0.8793, val loss: 0.2924, val acc: 0.8793  (best train acc: 0.9020, best val acc: 0.8927, best train loss: 0.2424  @ epoch 25408 )\n",
      "[Epoch: 25600] train loss: 0.3446, train acc: 0.8603, val loss: 0.3404, val acc: 0.8462  (best train acc: 0.9020, best val acc: 0.8927, best train loss: 0.2424  @ epoch 25408 )\n",
      "[Epoch: 25620] train loss: 0.2566, train acc: 0.8936, val loss: 0.2721, val acc: 0.8867  (best train acc: 0.9020, best val acc: 0.8927, best train loss: 0.2424  @ epoch 25408 )\n",
      "[Epoch: 25640] train loss: 0.2587, train acc: 0.8906, val loss: 0.2913, val acc: 0.8728  (best train acc: 0.9020, best val acc: 0.8927, best train loss: 0.2424  @ epoch 25408 )\n",
      "[Epoch: 25660] train loss: 0.2636, train acc: 0.8861, val loss: 0.2722, val acc: 0.8820  (best train acc: 0.9020, best val acc: 0.8927, best train loss: 0.2424  @ epoch 25408 )\n",
      "[Epoch: 25680] train loss: 0.3205, train acc: 0.8671, val loss: 0.3067, val acc: 0.8735  (best train acc: 0.9020, best val acc: 0.8927, best train loss: 0.2424  @ epoch 25408 )\n",
      "[Epoch: 25700] train loss: 0.2975, train acc: 0.8852, val loss: 0.2865, val acc: 0.8833  (best train acc: 0.9020, best val acc: 0.8927, best train loss: 0.2424  @ epoch 25408 )\n",
      "[Epoch: 25720] train loss: 0.2640, train acc: 0.8941, val loss: 0.2712, val acc: 0.8863  (best train acc: 0.9020, best val acc: 0.8927, best train loss: 0.2424  @ epoch 25408 )\n",
      "[Epoch: 25740] train loss: 0.2917, train acc: 0.8813, val loss: 0.2716, val acc: 0.8843  (best train acc: 0.9057, best val acc: 0.8927, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 25760] train loss: 0.2758, train acc: 0.8907, val loss: 0.2752, val acc: 0.8840  (best train acc: 0.9057, best val acc: 0.8927, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 25780] train loss: 0.2651, train acc: 0.8897, val loss: 0.2827, val acc: 0.8833  (best train acc: 0.9057, best val acc: 0.8927, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 25800] train loss: 0.2678, train acc: 0.8898, val loss: 0.2863, val acc: 0.8843  (best train acc: 0.9057, best val acc: 0.8927, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 25820] train loss: 0.2792, train acc: 0.8815, val loss: 0.2757, val acc: 0.8813  (best train acc: 0.9057, best val acc: 0.8927, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 25840] train loss: 0.2789, train acc: 0.8916, val loss: 0.2810, val acc: 0.8860  (best train acc: 0.9057, best val acc: 0.8927, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 25860] train loss: 0.2857, train acc: 0.8800, val loss: 0.2836, val acc: 0.8816  (best train acc: 0.9057, best val acc: 0.8927, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 25880] train loss: 0.2832, train acc: 0.8835, val loss: 0.2808, val acc: 0.8823  (best train acc: 0.9057, best val acc: 0.8927, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 25900] train loss: 0.2439, train acc: 0.9012, val loss: 0.2654, val acc: 0.8877  (best train acc: 0.9057, best val acc: 0.8927, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 25920] train loss: 0.2652, train acc: 0.8998, val loss: 0.2883, val acc: 0.8782  (best train acc: 0.9057, best val acc: 0.8927, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 25940] train loss: 0.2660, train acc: 0.8913, val loss: 0.2917, val acc: 0.8786  (best train acc: 0.9057, best val acc: 0.8927, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 25960] train loss: 0.2956, train acc: 0.8780, val loss: 0.2709, val acc: 0.8863  (best train acc: 0.9057, best val acc: 0.8927, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 25980] train loss: 0.2891, train acc: 0.8783, val loss: 0.3126, val acc: 0.8617  (best train acc: 0.9057, best val acc: 0.8927, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 26000] train loss: 0.2626, train acc: 0.8924, val loss: 0.2732, val acc: 0.8840  (best train acc: 0.9057, best val acc: 0.8927, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 26020] train loss: 0.2552, train acc: 0.8976, val loss: 0.2842, val acc: 0.8789  (best train acc: 0.9057, best val acc: 0.8927, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 26040] train loss: 0.2815, train acc: 0.8822, val loss: 0.3227, val acc: 0.8553  (best train acc: 0.9057, best val acc: 0.8927, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 26060] train loss: 0.2619, train acc: 0.8929, val loss: 0.2876, val acc: 0.8813  (best train acc: 0.9057, best val acc: 0.8927, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 26080] train loss: 0.2687, train acc: 0.8856, val loss: 0.2737, val acc: 0.8813  (best train acc: 0.9057, best val acc: 0.8927, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 26100] train loss: 0.2915, train acc: 0.8858, val loss: 0.2701, val acc: 0.8901  (best train acc: 0.9057, best val acc: 0.8927, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 26120] train loss: 0.2679, train acc: 0.8935, val loss: 0.2771, val acc: 0.8843  (best train acc: 0.9057, best val acc: 0.8927, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 26140] train loss: 0.2949, train acc: 0.8785, val loss: 0.2740, val acc: 0.8823  (best train acc: 0.9057, best val acc: 0.8927, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 26160] train loss: 0.2929, train acc: 0.8823, val loss: 0.3066, val acc: 0.8648  (best train acc: 0.9057, best val acc: 0.8927, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 26180] train loss: 0.2481, train acc: 0.8976, val loss: 0.2756, val acc: 0.8840  (best train acc: 0.9057, best val acc: 0.8927, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 26200] train loss: 0.2608, train acc: 0.8989, val loss: 0.3259, val acc: 0.8604  (best train acc: 0.9057, best val acc: 0.8927, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 26220] train loss: 0.2884, train acc: 0.8833, val loss: 0.2853, val acc: 0.8847  (best train acc: 0.9057, best val acc: 0.8927, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 26240] train loss: 0.2729, train acc: 0.8884, val loss: 0.2737, val acc: 0.8789  (best train acc: 0.9057, best val acc: 0.8927, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 26260] train loss: 0.2694, train acc: 0.8829, val loss: 0.2763, val acc: 0.8803  (best train acc: 0.9057, best val acc: 0.8927, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 26280] train loss: 0.2794, train acc: 0.8863, val loss: 0.2757, val acc: 0.8850  (best train acc: 0.9057, best val acc: 0.8927, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 26300] train loss: 0.2755, train acc: 0.8882, val loss: 0.2721, val acc: 0.8857  (best train acc: 0.9057, best val acc: 0.8927, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 26320] train loss: 0.2743, train acc: 0.8886, val loss: 0.2851, val acc: 0.8779  (best train acc: 0.9057, best val acc: 0.8927, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 26340] train loss: 0.2622, train acc: 0.8908, val loss: 0.2876, val acc: 0.8796  (best train acc: 0.9057, best val acc: 0.8927, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 26360] train loss: 0.2702, train acc: 0.8901, val loss: 0.2757, val acc: 0.8843  (best train acc: 0.9057, best val acc: 0.8927, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 26380] train loss: 0.2837, train acc: 0.8864, val loss: 0.2853, val acc: 0.8826  (best train acc: 0.9057, best val acc: 0.8927, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 26400] train loss: 0.2871, train acc: 0.8888, val loss: 0.2759, val acc: 0.8833  (best train acc: 0.9057, best val acc: 0.8927, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 26420] train loss: 0.2618, train acc: 0.8926, val loss: 0.2697, val acc: 0.8857  (best train acc: 0.9057, best val acc: 0.8927, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 26440] train loss: 0.2583, train acc: 0.8967, val loss: 0.2718, val acc: 0.8867  (best train acc: 0.9057, best val acc: 0.8927, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 26460] train loss: 0.2655, train acc: 0.8916, val loss: 0.2676, val acc: 0.8863  (best train acc: 0.9057, best val acc: 0.8927, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 26480] train loss: 0.2717, train acc: 0.8900, val loss: 0.2800, val acc: 0.8823  (best train acc: 0.9057, best val acc: 0.8927, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 26500] train loss: 0.2572, train acc: 0.8979, val loss: 0.2730, val acc: 0.8847  (best train acc: 0.9057, best val acc: 0.8927, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 26520] train loss: 0.2656, train acc: 0.8872, val loss: 0.2715, val acc: 0.8843  (best train acc: 0.9057, best val acc: 0.8927, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 26540] train loss: 0.2626, train acc: 0.8899, val loss: 0.2715, val acc: 0.8840  (best train acc: 0.9057, best val acc: 0.8927, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 26560] train loss: 0.2939, train acc: 0.8767, val loss: 0.2784, val acc: 0.8850  (best train acc: 0.9057, best val acc: 0.8927, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 26580] train loss: 0.2729, train acc: 0.8826, val loss: 0.2758, val acc: 0.8863  (best train acc: 0.9057, best val acc: 0.8927, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 26600] train loss: 0.2632, train acc: 0.8869, val loss: 0.2878, val acc: 0.8772  (best train acc: 0.9057, best val acc: 0.8927, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 26620] train loss: 0.2489, train acc: 0.8953, val loss: 0.2698, val acc: 0.8857  (best train acc: 0.9057, best val acc: 0.8927, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 26640] train loss: 0.2698, train acc: 0.8892, val loss: 0.2814, val acc: 0.8823  (best train acc: 0.9057, best val acc: 0.8927, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 26660] train loss: 0.2483, train acc: 0.8999, val loss: 0.2817, val acc: 0.8813  (best train acc: 0.9057, best val acc: 0.8931, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 26680] train loss: 0.2499, train acc: 0.8963, val loss: 0.2662, val acc: 0.8884  (best train acc: 0.9057, best val acc: 0.8931, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 26700] train loss: 0.2628, train acc: 0.8889, val loss: 0.2819, val acc: 0.8823  (best train acc: 0.9057, best val acc: 0.8931, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 26720] train loss: 0.2592, train acc: 0.8924, val loss: 0.2828, val acc: 0.8782  (best train acc: 0.9057, best val acc: 0.8931, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 26740] train loss: 0.2691, train acc: 0.8889, val loss: 0.2893, val acc: 0.8752  (best train acc: 0.9057, best val acc: 0.8931, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 26760] train loss: 0.2898, train acc: 0.8789, val loss: 0.2673, val acc: 0.8863  (best train acc: 0.9057, best val acc: 0.8931, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 26780] train loss: 0.2687, train acc: 0.8902, val loss: 0.2908, val acc: 0.8813  (best train acc: 0.9057, best val acc: 0.8931, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 26800] train loss: 0.2703, train acc: 0.8892, val loss: 0.2733, val acc: 0.8836  (best train acc: 0.9057, best val acc: 0.8931, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 26820] train loss: 0.2825, train acc: 0.8878, val loss: 0.2707, val acc: 0.8890  (best train acc: 0.9057, best val acc: 0.8931, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 26840] train loss: 0.2595, train acc: 0.8894, val loss: 0.2970, val acc: 0.8722  (best train acc: 0.9057, best val acc: 0.8931, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 26860] train loss: 0.2696, train acc: 0.8848, val loss: 0.2923, val acc: 0.8766  (best train acc: 0.9057, best val acc: 0.8931, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 26880] train loss: 0.2534, train acc: 0.8945, val loss: 0.2681, val acc: 0.8894  (best train acc: 0.9057, best val acc: 0.8931, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 26900] train loss: 0.2601, train acc: 0.8935, val loss: 0.2735, val acc: 0.8830  (best train acc: 0.9057, best val acc: 0.8931, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 26920] train loss: 0.2678, train acc: 0.8891, val loss: 0.2729, val acc: 0.8857  (best train acc: 0.9057, best val acc: 0.8931, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 26940] train loss: 0.2822, train acc: 0.8777, val loss: 0.2819, val acc: 0.8769  (best train acc: 0.9057, best val acc: 0.8931, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 26960] train loss: 0.3051, train acc: 0.8805, val loss: 0.2742, val acc: 0.8816  (best train acc: 0.9057, best val acc: 0.8931, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 26980] train loss: 0.2589, train acc: 0.8944, val loss: 0.3072, val acc: 0.8641  (best train acc: 0.9057, best val acc: 0.8931, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 27000] train loss: 0.2517, train acc: 0.8970, val loss: 0.2765, val acc: 0.8887  (best train acc: 0.9057, best val acc: 0.8931, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 27020] train loss: 0.2591, train acc: 0.8958, val loss: 0.2988, val acc: 0.8732  (best train acc: 0.9057, best val acc: 0.8931, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 27040] train loss: 0.2736, train acc: 0.8876, val loss: 0.2711, val acc: 0.8880  (best train acc: 0.9057, best val acc: 0.8931, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 27060] train loss: 0.2691, train acc: 0.8905, val loss: 0.2702, val acc: 0.8853  (best train acc: 0.9057, best val acc: 0.8931, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 27080] train loss: 0.2521, train acc: 0.8973, val loss: 0.2709, val acc: 0.8850  (best train acc: 0.9057, best val acc: 0.8931, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 27100] train loss: 0.2836, train acc: 0.8761, val loss: 0.2696, val acc: 0.8847  (best train acc: 0.9057, best val acc: 0.8931, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 27120] train loss: 0.2810, train acc: 0.8877, val loss: 0.2700, val acc: 0.8870  (best train acc: 0.9057, best val acc: 0.8931, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 27140] train loss: 0.2831, train acc: 0.8834, val loss: 0.2696, val acc: 0.8853  (best train acc: 0.9057, best val acc: 0.8931, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 27160] train loss: 0.2688, train acc: 0.8890, val loss: 0.2696, val acc: 0.8894  (best train acc: 0.9057, best val acc: 0.8931, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 27180] train loss: 0.2716, train acc: 0.8876, val loss: 0.2881, val acc: 0.8718  (best train acc: 0.9057, best val acc: 0.8931, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 27200] train loss: 0.2605, train acc: 0.8943, val loss: 0.3262, val acc: 0.8573  (best train acc: 0.9057, best val acc: 0.8931, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 27220] train loss: 0.2641, train acc: 0.8873, val loss: 0.2748, val acc: 0.8823  (best train acc: 0.9057, best val acc: 0.8931, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 27240] train loss: 0.2751, train acc: 0.8850, val loss: 0.2983, val acc: 0.8668  (best train acc: 0.9057, best val acc: 0.8931, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 27260] train loss: 0.2623, train acc: 0.8908, val loss: 0.2759, val acc: 0.8867  (best train acc: 0.9057, best val acc: 0.8931, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 27280] train loss: 0.2902, train acc: 0.8845, val loss: 0.3007, val acc: 0.8678  (best train acc: 0.9057, best val acc: 0.8931, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 27300] train loss: 0.2539, train acc: 0.8947, val loss: 0.2809, val acc: 0.8779  (best train acc: 0.9057, best val acc: 0.8931, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 27320] train loss: 0.3234, train acc: 0.8673, val loss: 0.2834, val acc: 0.8759  (best train acc: 0.9057, best val acc: 0.8931, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 27340] train loss: 0.2597, train acc: 0.8971, val loss: 0.2755, val acc: 0.8843  (best train acc: 0.9057, best val acc: 0.8931, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 27360] train loss: 0.2458, train acc: 0.9003, val loss: 0.2782, val acc: 0.8853  (best train acc: 0.9057, best val acc: 0.8931, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 27380] train loss: 0.2617, train acc: 0.8946, val loss: 0.2718, val acc: 0.8870  (best train acc: 0.9057, best val acc: 0.8931, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 27400] train loss: 0.2528, train acc: 0.8981, val loss: 0.2770, val acc: 0.8857  (best train acc: 0.9057, best val acc: 0.8931, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 27420] train loss: 0.2865, train acc: 0.8812, val loss: 0.2752, val acc: 0.8833  (best train acc: 0.9057, best val acc: 0.8931, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 27440] train loss: 0.2718, train acc: 0.8858, val loss: 0.2660, val acc: 0.8874  (best train acc: 0.9057, best val acc: 0.8931, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 27460] train loss: 0.2569, train acc: 0.8946, val loss: 0.2747, val acc: 0.8806  (best train acc: 0.9057, best val acc: 0.8931, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 27480] train loss: 0.2488, train acc: 0.9014, val loss: 0.2883, val acc: 0.8796  (best train acc: 0.9057, best val acc: 0.8931, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 27500] train loss: 0.2665, train acc: 0.8877, val loss: 0.2716, val acc: 0.8847  (best train acc: 0.9057, best val acc: 0.8931, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 27520] train loss: 0.2767, train acc: 0.8848, val loss: 0.2711, val acc: 0.8860  (best train acc: 0.9057, best val acc: 0.8931, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 27540] train loss: 0.2577, train acc: 0.8902, val loss: 0.2771, val acc: 0.8820  (best train acc: 0.9057, best val acc: 0.8931, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 27560] train loss: 0.2671, train acc: 0.8924, val loss: 0.2691, val acc: 0.8836  (best train acc: 0.9057, best val acc: 0.8931, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 27580] train loss: 0.2485, train acc: 0.8976, val loss: 0.2679, val acc: 0.8907  (best train acc: 0.9057, best val acc: 0.8931, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 27600] train loss: 0.2892, train acc: 0.8790, val loss: 0.3172, val acc: 0.8624  (best train acc: 0.9057, best val acc: 0.8931, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 27620] train loss: 0.2617, train acc: 0.8955, val loss: 0.2642, val acc: 0.8884  (best train acc: 0.9057, best val acc: 0.8931, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 27640] train loss: 0.2632, train acc: 0.8909, val loss: 0.2837, val acc: 0.8823  (best train acc: 0.9057, best val acc: 0.8931, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 27660] train loss: 0.2583, train acc: 0.8965, val loss: 0.2678, val acc: 0.8867  (best train acc: 0.9057, best val acc: 0.8941, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 27680] train loss: 0.2646, train acc: 0.8921, val loss: 0.2784, val acc: 0.8813  (best train acc: 0.9057, best val acc: 0.8941, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 27700] train loss: 0.2635, train acc: 0.8910, val loss: 0.2742, val acc: 0.8843  (best train acc: 0.9057, best val acc: 0.8941, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 27720] train loss: 0.2656, train acc: 0.8904, val loss: 0.2845, val acc: 0.8840  (best train acc: 0.9057, best val acc: 0.8941, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 27740] train loss: 0.2531, train acc: 0.8953, val loss: 0.2698, val acc: 0.8847  (best train acc: 0.9057, best val acc: 0.8941, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 27760] train loss: 0.2836, train acc: 0.8814, val loss: 0.2815, val acc: 0.8809  (best train acc: 0.9057, best val acc: 0.8941, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 27780] train loss: 0.2638, train acc: 0.8937, val loss: 0.2800, val acc: 0.8833  (best train acc: 0.9057, best val acc: 0.8941, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 27800] train loss: 0.2682, train acc: 0.8900, val loss: 0.2714, val acc: 0.8806  (best train acc: 0.9057, best val acc: 0.8941, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 27820] train loss: 0.2518, train acc: 0.8952, val loss: 0.2698, val acc: 0.8867  (best train acc: 0.9057, best val acc: 0.8941, best train loss: 0.2399  @ epoch 25723 )\n",
      "[Epoch: 27840] train loss: 0.2550, train acc: 0.8937, val loss: 0.2777, val acc: 0.8806  (best train acc: 0.9057, best val acc: 0.8941, best train loss: 0.2368  @ epoch 27830 )\n",
      "[Epoch: 27860] train loss: 0.2589, train acc: 0.8884, val loss: 0.2685, val acc: 0.8847  (best train acc: 0.9057, best val acc: 0.8941, best train loss: 0.2368  @ epoch 27830 )\n",
      "[Epoch: 27880] train loss: 0.2892, train acc: 0.8831, val loss: 0.2637, val acc: 0.8840  (best train acc: 0.9057, best val acc: 0.8941, best train loss: 0.2368  @ epoch 27830 )\n",
      "[Epoch: 27900] train loss: 0.2756, train acc: 0.8877, val loss: 0.2713, val acc: 0.8887  (best train acc: 0.9057, best val acc: 0.8941, best train loss: 0.2368  @ epoch 27830 )\n",
      "[Epoch: 27920] train loss: 0.2851, train acc: 0.8829, val loss: 0.2836, val acc: 0.8877  (best train acc: 0.9057, best val acc: 0.8941, best train loss: 0.2368  @ epoch 27830 )\n",
      "[Epoch: 27940] train loss: 0.2918, train acc: 0.8777, val loss: 0.2911, val acc: 0.8766  (best train acc: 0.9057, best val acc: 0.8941, best train loss: 0.2368  @ epoch 27830 )\n",
      "[Epoch: 27960] train loss: 0.3064, train acc: 0.8728, val loss: 0.2706, val acc: 0.8840  (best train acc: 0.9057, best val acc: 0.8941, best train loss: 0.2368  @ epoch 27830 )\n",
      "[Epoch: 27980] train loss: 0.2570, train acc: 0.8871, val loss: 0.2711, val acc: 0.8816  (best train acc: 0.9057, best val acc: 0.8941, best train loss: 0.2368  @ epoch 27830 )\n",
      "[Epoch: 28000] train loss: 0.2964, train acc: 0.8759, val loss: 0.2775, val acc: 0.8806  (best train acc: 0.9057, best val acc: 0.8941, best train loss: 0.2368  @ epoch 27830 )\n",
      "[Epoch: 28020] train loss: 0.2623, train acc: 0.8943, val loss: 0.2677, val acc: 0.8840  (best train acc: 0.9057, best val acc: 0.8941, best train loss: 0.2368  @ epoch 27830 )\n",
      "[Epoch: 28040] train loss: 0.2671, train acc: 0.8909, val loss: 0.2816, val acc: 0.8793  (best train acc: 0.9057, best val acc: 0.8941, best train loss: 0.2368  @ epoch 27830 )\n",
      "[Epoch: 28060] train loss: 0.2684, train acc: 0.8882, val loss: 0.2735, val acc: 0.8877  (best train acc: 0.9057, best val acc: 0.8941, best train loss: 0.2368  @ epoch 27830 )\n",
      "[Epoch: 28080] train loss: 0.2738, train acc: 0.8902, val loss: 0.2665, val acc: 0.8867  (best train acc: 0.9057, best val acc: 0.8941, best train loss: 0.2368  @ epoch 27830 )\n",
      "[Epoch: 28100] train loss: 0.2872, train acc: 0.8848, val loss: 0.3016, val acc: 0.8749  (best train acc: 0.9057, best val acc: 0.8941, best train loss: 0.2368  @ epoch 27830 )\n",
      "[Epoch: 28120] train loss: 0.2585, train acc: 0.8932, val loss: 0.2928, val acc: 0.8752  (best train acc: 0.9057, best val acc: 0.8941, best train loss: 0.2368  @ epoch 27830 )\n",
      "[Epoch: 28140] train loss: 0.2727, train acc: 0.8878, val loss: 0.2667, val acc: 0.8897  (best train acc: 0.9057, best val acc: 0.8941, best train loss: 0.2368  @ epoch 27830 )\n",
      "[Epoch: 28160] train loss: 0.3033, train acc: 0.8747, val loss: 0.2827, val acc: 0.8779  (best train acc: 0.9057, best val acc: 0.8941, best train loss: 0.2368  @ epoch 27830 )\n",
      "[Epoch: 28180] train loss: 0.3111, train acc: 0.8635, val loss: 0.2695, val acc: 0.8857  (best train acc: 0.9057, best val acc: 0.8941, best train loss: 0.2368  @ epoch 27830 )\n",
      "[Epoch: 28200] train loss: 0.2714, train acc: 0.8901, val loss: 0.2702, val acc: 0.8894  (best train acc: 0.9057, best val acc: 0.8941, best train loss: 0.2368  @ epoch 27830 )\n",
      "[Epoch: 28220] train loss: 0.2722, train acc: 0.8827, val loss: 0.2648, val acc: 0.8833  (best train acc: 0.9057, best val acc: 0.8941, best train loss: 0.2368  @ epoch 27830 )\n",
      "[Epoch: 28240] train loss: 0.2686, train acc: 0.8811, val loss: 0.2781, val acc: 0.8823  (best train acc: 0.9057, best val acc: 0.8941, best train loss: 0.2368  @ epoch 27830 )\n",
      "[Epoch: 28260] train loss: 0.2607, train acc: 0.8927, val loss: 0.2741, val acc: 0.8833  (best train acc: 0.9057, best val acc: 0.8941, best train loss: 0.2368  @ epoch 27830 )\n",
      "[Epoch: 28280] train loss: 0.2703, train acc: 0.8910, val loss: 0.2634, val acc: 0.8863  (best train acc: 0.9057, best val acc: 0.8941, best train loss: 0.2368  @ epoch 27830 )\n",
      "[Epoch: 28300] train loss: 0.2557, train acc: 0.8924, val loss: 0.2635, val acc: 0.8870  (best train acc: 0.9057, best val acc: 0.8941, best train loss: 0.2368  @ epoch 27830 )\n",
      "[Epoch: 28320] train loss: 0.2569, train acc: 0.8942, val loss: 0.2673, val acc: 0.8863  (best train acc: 0.9057, best val acc: 0.8941, best train loss: 0.2368  @ epoch 27830 )\n",
      "[Epoch: 28340] train loss: 0.2798, train acc: 0.8833, val loss: 0.2647, val acc: 0.8890  (best train acc: 0.9057, best val acc: 0.8941, best train loss: 0.2368  @ epoch 27830 )\n",
      "[Epoch: 28360] train loss: 0.2715, train acc: 0.8884, val loss: 0.2653, val acc: 0.8917  (best train acc: 0.9057, best val acc: 0.8941, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 28380] train loss: 0.2520, train acc: 0.8937, val loss: 0.2665, val acc: 0.8863  (best train acc: 0.9057, best val acc: 0.8941, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 28400] train loss: 0.2640, train acc: 0.8949, val loss: 0.2732, val acc: 0.8850  (best train acc: 0.9057, best val acc: 0.8941, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 28420] train loss: 0.2595, train acc: 0.8947, val loss: 0.3363, val acc: 0.8536  (best train acc: 0.9057, best val acc: 0.8941, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 28440] train loss: 0.2766, train acc: 0.8803, val loss: 0.2724, val acc: 0.8850  (best train acc: 0.9057, best val acc: 0.8941, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 28460] train loss: 0.3022, train acc: 0.8754, val loss: 0.2747, val acc: 0.8799  (best train acc: 0.9057, best val acc: 0.8941, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 28480] train loss: 0.2886, train acc: 0.8786, val loss: 0.2858, val acc: 0.8759  (best train acc: 0.9057, best val acc: 0.8941, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 28500] train loss: 0.2552, train acc: 0.8986, val loss: 0.2712, val acc: 0.8877  (best train acc: 0.9057, best val acc: 0.8941, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 28520] train loss: 0.3164, train acc: 0.8673, val loss: 0.3499, val acc: 0.8573  (best train acc: 0.9057, best val acc: 0.8941, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 28540] train loss: 0.2572, train acc: 0.8938, val loss: 0.2706, val acc: 0.8880  (best train acc: 0.9057, best val acc: 0.8941, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 28560] train loss: 0.2784, train acc: 0.8784, val loss: 0.2707, val acc: 0.8847  (best train acc: 0.9057, best val acc: 0.8941, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 28580] train loss: 0.2517, train acc: 0.9015, val loss: 0.3000, val acc: 0.8752  (best train acc: 0.9057, best val acc: 0.8941, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 28600] train loss: 0.2700, train acc: 0.8896, val loss: 0.2721, val acc: 0.8877  (best train acc: 0.9057, best val acc: 0.8941, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 28620] train loss: 0.2725, train acc: 0.8882, val loss: 0.2694, val acc: 0.8850  (best train acc: 0.9057, best val acc: 0.8941, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 28640] train loss: 0.2608, train acc: 0.8922, val loss: 0.2652, val acc: 0.8874  (best train acc: 0.9057, best val acc: 0.8944, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 28660] train loss: 0.2914, train acc: 0.8863, val loss: 0.2931, val acc: 0.8752  (best train acc: 0.9057, best val acc: 0.8944, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 28680] train loss: 0.2873, train acc: 0.8756, val loss: 0.2991, val acc: 0.8732  (best train acc: 0.9057, best val acc: 0.8944, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 28700] train loss: 0.2858, train acc: 0.8767, val loss: 0.2814, val acc: 0.8809  (best train acc: 0.9057, best val acc: 0.8944, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 28720] train loss: 0.2680, train acc: 0.8885, val loss: 0.2708, val acc: 0.8860  (best train acc: 0.9057, best val acc: 0.8944, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 28740] train loss: 0.2850, train acc: 0.8837, val loss: 0.2858, val acc: 0.8779  (best train acc: 0.9057, best val acc: 0.8944, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 28760] train loss: 0.2490, train acc: 0.8970, val loss: 0.2647, val acc: 0.8890  (best train acc: 0.9057, best val acc: 0.8944, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 28780] train loss: 0.2850, train acc: 0.8785, val loss: 0.2818, val acc: 0.8762  (best train acc: 0.9057, best val acc: 0.8944, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 28800] train loss: 0.2521, train acc: 0.8938, val loss: 0.2671, val acc: 0.8843  (best train acc: 0.9057, best val acc: 0.8944, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 28820] train loss: 0.2783, train acc: 0.8888, val loss: 0.2933, val acc: 0.8745  (best train acc: 0.9057, best val acc: 0.8944, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 28840] train loss: 0.2705, train acc: 0.8871, val loss: 0.2686, val acc: 0.8874  (best train acc: 0.9057, best val acc: 0.8944, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 28860] train loss: 0.2573, train acc: 0.8931, val loss: 0.2728, val acc: 0.8857  (best train acc: 0.9057, best val acc: 0.8944, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 28880] train loss: 0.2407, train acc: 0.9027, val loss: 0.2684, val acc: 0.8863  (best train acc: 0.9057, best val acc: 0.8944, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 28900] train loss: 0.2710, train acc: 0.8921, val loss: 0.2712, val acc: 0.8880  (best train acc: 0.9057, best val acc: 0.8944, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 28920] train loss: 0.2661, train acc: 0.8879, val loss: 0.2773, val acc: 0.8830  (best train acc: 0.9057, best val acc: 0.8944, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 28940] train loss: 0.2739, train acc: 0.8859, val loss: 0.2660, val acc: 0.8863  (best train acc: 0.9057, best val acc: 0.8944, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 28960] train loss: 0.2611, train acc: 0.8921, val loss: 0.2696, val acc: 0.8860  (best train acc: 0.9057, best val acc: 0.8944, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 28980] train loss: 0.2554, train acc: 0.8953, val loss: 0.2695, val acc: 0.8863  (best train acc: 0.9057, best val acc: 0.8944, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 29000] train loss: 0.2837, train acc: 0.8805, val loss: 0.2644, val acc: 0.8833  (best train acc: 0.9057, best val acc: 0.8944, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 29020] train loss: 0.2865, train acc: 0.8773, val loss: 0.2851, val acc: 0.8745  (best train acc: 0.9057, best val acc: 0.8944, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 29040] train loss: 0.2553, train acc: 0.8957, val loss: 0.2710, val acc: 0.8816  (best train acc: 0.9057, best val acc: 0.8944, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 29060] train loss: 0.2681, train acc: 0.8871, val loss: 0.2743, val acc: 0.8894  (best train acc: 0.9057, best val acc: 0.8944, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 29080] train loss: 0.2701, train acc: 0.8892, val loss: 0.2833, val acc: 0.8813  (best train acc: 0.9057, best val acc: 0.8944, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 29100] train loss: 0.2460, train acc: 0.9002, val loss: 0.2750, val acc: 0.8789  (best train acc: 0.9057, best val acc: 0.8944, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 29120] train loss: 0.2700, train acc: 0.8924, val loss: 0.2669, val acc: 0.8884  (best train acc: 0.9057, best val acc: 0.8944, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 29140] train loss: 0.2503, train acc: 0.8989, val loss: 0.2741, val acc: 0.8830  (best train acc: 0.9057, best val acc: 0.8944, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 29160] train loss: 0.2675, train acc: 0.8890, val loss: 0.2838, val acc: 0.8799  (best train acc: 0.9057, best val acc: 0.8944, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 29180] train loss: 0.2957, train acc: 0.8759, val loss: 0.2697, val acc: 0.8860  (best train acc: 0.9057, best val acc: 0.8944, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 29200] train loss: 0.2967, train acc: 0.8722, val loss: 0.3360, val acc: 0.8577  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 29220] train loss: 0.2907, train acc: 0.8836, val loss: 0.2728, val acc: 0.8874  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 29240] train loss: 0.2778, train acc: 0.8874, val loss: 0.2682, val acc: 0.8813  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 29260] train loss: 0.2536, train acc: 0.8966, val loss: 0.2733, val acc: 0.8793  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 29280] train loss: 0.2736, train acc: 0.8879, val loss: 0.2817, val acc: 0.8782  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 29300] train loss: 0.2608, train acc: 0.8937, val loss: 0.2859, val acc: 0.8745  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 29320] train loss: 0.2924, train acc: 0.8785, val loss: 0.2829, val acc: 0.8739  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 29340] train loss: 0.2520, train acc: 0.8989, val loss: 0.2657, val acc: 0.8911  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 29360] train loss: 0.2655, train acc: 0.8923, val loss: 0.2714, val acc: 0.8853  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 29380] train loss: 0.2686, train acc: 0.8841, val loss: 0.2764, val acc: 0.8776  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 29400] train loss: 0.3140, train acc: 0.8683, val loss: 0.3498, val acc: 0.8543  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 29420] train loss: 0.2823, train acc: 0.8821, val loss: 0.2945, val acc: 0.8803  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 29440] train loss: 0.2604, train acc: 0.8925, val loss: 0.2797, val acc: 0.8789  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 29460] train loss: 0.2850, train acc: 0.8895, val loss: 0.2723, val acc: 0.8867  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 29480] train loss: 0.2641, train acc: 0.8905, val loss: 0.2736, val acc: 0.8830  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 29500] train loss: 0.2456, train acc: 0.8989, val loss: 0.2715, val acc: 0.8769  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 29520] train loss: 0.2526, train acc: 0.8983, val loss: 0.2722, val acc: 0.8867  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 29540] train loss: 0.2773, train acc: 0.8879, val loss: 0.2859, val acc: 0.8712  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 29560] train loss: 0.2851, train acc: 0.8871, val loss: 0.2694, val acc: 0.8850  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 29580] train loss: 0.2515, train acc: 0.8970, val loss: 0.2672, val acc: 0.8850  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 29600] train loss: 0.2487, train acc: 0.8983, val loss: 0.2739, val acc: 0.8860  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 29620] train loss: 0.2675, train acc: 0.8903, val loss: 0.2616, val acc: 0.8857  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 29640] train loss: 0.2767, train acc: 0.8848, val loss: 0.2618, val acc: 0.8884  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 29660] train loss: 0.2527, train acc: 0.8945, val loss: 0.2769, val acc: 0.8786  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 29680] train loss: 0.2650, train acc: 0.8940, val loss: 0.2640, val acc: 0.8853  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 29700] train loss: 0.2616, train acc: 0.8936, val loss: 0.2592, val acc: 0.8884  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 29720] train loss: 0.2597, train acc: 0.8879, val loss: 0.2701, val acc: 0.8853  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 29740] train loss: 0.2669, train acc: 0.8909, val loss: 0.2625, val acc: 0.8820  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 29760] train loss: 0.2687, train acc: 0.8838, val loss: 0.2942, val acc: 0.8742  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 29780] train loss: 0.2711, train acc: 0.8872, val loss: 0.2693, val acc: 0.8813  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 29800] train loss: 0.2602, train acc: 0.8901, val loss: 0.2871, val acc: 0.8782  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 29820] train loss: 0.2508, train acc: 0.8942, val loss: 0.2645, val acc: 0.8836  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 29840] train loss: 0.2533, train acc: 0.8957, val loss: 0.2798, val acc: 0.8823  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 29860] train loss: 0.2679, train acc: 0.8884, val loss: 0.2737, val acc: 0.8809  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 29880] train loss: 0.2512, train acc: 0.8961, val loss: 0.2875, val acc: 0.8772  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 29900] train loss: 0.2727, train acc: 0.8882, val loss: 0.2632, val acc: 0.8853  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 29920] train loss: 0.2779, train acc: 0.8869, val loss: 0.2622, val acc: 0.8860  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 29940] train loss: 0.2582, train acc: 0.8937, val loss: 0.3036, val acc: 0.8651  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 29960] train loss: 0.2632, train acc: 0.8918, val loss: 0.2668, val acc: 0.8823  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 29980] train loss: 0.2587, train acc: 0.8912, val loss: 0.2623, val acc: 0.8867  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 30000] train loss: 0.2570, train acc: 0.8946, val loss: 0.2798, val acc: 0.8772  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 30020] train loss: 0.2604, train acc: 0.8894, val loss: 0.2772, val acc: 0.8820  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 30040] train loss: 0.2996, train acc: 0.8729, val loss: 0.2706, val acc: 0.8806  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 30060] train loss: 0.2449, train acc: 0.9028, val loss: 0.2600, val acc: 0.8823  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 30080] train loss: 0.2529, train acc: 0.8917, val loss: 0.2759, val acc: 0.8752  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 30100] train loss: 0.2560, train acc: 0.8969, val loss: 0.2581, val acc: 0.8884  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 30120] train loss: 0.2575, train acc: 0.8928, val loss: 0.2606, val acc: 0.8867  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 30140] train loss: 0.2662, train acc: 0.8944, val loss: 0.2784, val acc: 0.8820  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 30160] train loss: 0.2759, train acc: 0.8885, val loss: 0.2674, val acc: 0.8820  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 30180] train loss: 0.2781, train acc: 0.8861, val loss: 0.2762, val acc: 0.8816  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 30200] train loss: 0.2591, train acc: 0.8959, val loss: 0.2638, val acc: 0.8850  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2362  @ epoch 28341 )\n",
      "[Epoch: 30220] train loss: 0.2610, train acc: 0.8889, val loss: 0.2628, val acc: 0.8894  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2352  @ epoch 30210 )\n",
      "[Epoch: 30240] train loss: 0.2525, train acc: 0.8946, val loss: 0.2586, val acc: 0.8887  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2352  @ epoch 30210 )\n",
      "[Epoch: 30260] train loss: 0.2386, train acc: 0.8991, val loss: 0.2684, val acc: 0.8843  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2352  @ epoch 30210 )\n",
      "[Epoch: 30280] train loss: 0.2367, train acc: 0.9038, val loss: 0.2575, val acc: 0.8853  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2352  @ epoch 30210 )\n",
      "[Epoch: 30300] train loss: 0.2652, train acc: 0.8947, val loss: 0.2923, val acc: 0.8712  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2352  @ epoch 30210 )\n",
      "[Epoch: 30320] train loss: 0.2780, train acc: 0.8809, val loss: 0.2629, val acc: 0.8890  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2352  @ epoch 30210 )\n",
      "[Epoch: 30340] train loss: 0.2489, train acc: 0.8965, val loss: 0.2647, val acc: 0.8823  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2352  @ epoch 30210 )\n",
      "[Epoch: 30360] train loss: 0.2625, train acc: 0.8921, val loss: 0.2616, val acc: 0.8894  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2352  @ epoch 30210 )\n",
      "[Epoch: 30380] train loss: 0.2519, train acc: 0.8957, val loss: 0.2974, val acc: 0.8725  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2352  @ epoch 30210 )\n",
      "[Epoch: 30400] train loss: 0.2600, train acc: 0.8934, val loss: 0.2783, val acc: 0.8813  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2352  @ epoch 30210 )\n",
      "[Epoch: 30420] train loss: 0.2437, train acc: 0.9002, val loss: 0.2788, val acc: 0.8853  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2352  @ epoch 30210 )\n",
      "[Epoch: 30440] train loss: 0.2637, train acc: 0.8908, val loss: 0.2632, val acc: 0.8863  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2352  @ epoch 30210 )\n",
      "[Epoch: 30460] train loss: 0.2462, train acc: 0.9017, val loss: 0.3047, val acc: 0.8675  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2352  @ epoch 30210 )\n",
      "[Epoch: 30480] train loss: 0.2699, train acc: 0.8882, val loss: 0.2654, val acc: 0.8826  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2352  @ epoch 30210 )\n",
      "[Epoch: 30500] train loss: 0.3332, train acc: 0.8675, val loss: 0.2791, val acc: 0.8759  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2352  @ epoch 30210 )\n",
      "[Epoch: 30520] train loss: 0.2608, train acc: 0.8917, val loss: 0.2797, val acc: 0.8722  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2352  @ epoch 30210 )\n",
      "[Epoch: 30540] train loss: 0.2613, train acc: 0.8904, val loss: 0.2795, val acc: 0.8769  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2352  @ epoch 30210 )\n",
      "[Epoch: 30560] train loss: 0.2821, train acc: 0.8841, val loss: 0.2669, val acc: 0.8836  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2352  @ epoch 30210 )\n",
      "[Epoch: 30580] train loss: 0.2526, train acc: 0.8991, val loss: 0.2700, val acc: 0.8836  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2352  @ epoch 30210 )\n",
      "[Epoch: 30600] train loss: 0.2755, train acc: 0.8870, val loss: 0.2755, val acc: 0.8755  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2352  @ epoch 30210 )\n",
      "[Epoch: 30620] train loss: 0.2625, train acc: 0.8966, val loss: 0.2584, val acc: 0.8867  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2352  @ epoch 30210 )\n",
      "[Epoch: 30640] train loss: 0.2726, train acc: 0.8926, val loss: 0.2629, val acc: 0.8884  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2352  @ epoch 30210 )\n",
      "[Epoch: 30660] train loss: 0.2507, train acc: 0.8971, val loss: 0.2555, val acc: 0.8843  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2352  @ epoch 30210 )\n",
      "[Epoch: 30680] train loss: 0.2684, train acc: 0.8924, val loss: 0.2866, val acc: 0.8776  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2352  @ epoch 30210 )\n",
      "[Epoch: 30700] train loss: 0.2700, train acc: 0.8901, val loss: 0.2597, val acc: 0.8907  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2352  @ epoch 30210 )\n",
      "[Epoch: 30720] train loss: 0.2581, train acc: 0.8994, val loss: 0.2613, val acc: 0.8897  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2352  @ epoch 30210 )\n",
      "[Epoch: 30740] train loss: 0.2582, train acc: 0.8926, val loss: 0.2556, val acc: 0.8894  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2352  @ epoch 30210 )\n",
      "[Epoch: 30760] train loss: 0.2509, train acc: 0.8978, val loss: 0.2566, val acc: 0.8884  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2352  @ epoch 30210 )\n",
      "[Epoch: 30780] train loss: 0.3012, train acc: 0.8787, val loss: 0.2611, val acc: 0.8860  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2352  @ epoch 30210 )\n",
      "[Epoch: 30800] train loss: 0.2544, train acc: 0.8969, val loss: 0.2834, val acc: 0.8698  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2352  @ epoch 30210 )\n",
      "[Epoch: 30820] train loss: 0.2837, train acc: 0.8773, val loss: 0.3036, val acc: 0.8637  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2352  @ epoch 30210 )\n",
      "[Epoch: 30840] train loss: 0.2385, train acc: 0.9003, val loss: 0.2612, val acc: 0.8830  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2352  @ epoch 30210 )\n",
      "[Epoch: 30860] train loss: 0.2606, train acc: 0.8930, val loss: 0.2566, val acc: 0.8917  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2352  @ epoch 30210 )\n",
      "[Epoch: 30880] train loss: 0.2608, train acc: 0.8908, val loss: 0.2819, val acc: 0.8806  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2352  @ epoch 30210 )\n",
      "[Epoch: 30900] train loss: 0.2573, train acc: 0.8957, val loss: 0.2710, val acc: 0.8823  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2350  @ epoch 30886 )\n",
      "[Epoch: 30920] train loss: 0.2748, train acc: 0.8844, val loss: 0.2712, val acc: 0.8793  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2350  @ epoch 30886 )\n",
      "[Epoch: 30940] train loss: 0.2652, train acc: 0.8923, val loss: 0.2611, val acc: 0.8816  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2350  @ epoch 30886 )\n",
      "[Epoch: 30960] train loss: 0.2683, train acc: 0.8890, val loss: 0.2623, val acc: 0.8860  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2350  @ epoch 30886 )\n",
      "[Epoch: 30980] train loss: 0.2629, train acc: 0.8923, val loss: 0.2572, val acc: 0.8850  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2350  @ epoch 30886 )\n",
      "[Epoch: 31000] train loss: 0.2500, train acc: 0.8939, val loss: 0.2588, val acc: 0.8857  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2350  @ epoch 30886 )\n",
      "[Epoch: 31020] train loss: 0.2651, train acc: 0.8921, val loss: 0.2750, val acc: 0.8782  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2350  @ epoch 30886 )\n",
      "[Epoch: 31040] train loss: 0.2506, train acc: 0.9012, val loss: 0.2723, val acc: 0.8843  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2350  @ epoch 30886 )\n",
      "[Epoch: 31060] train loss: 0.2617, train acc: 0.8866, val loss: 0.2533, val acc: 0.8904  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2350  @ epoch 30886 )\n",
      "[Epoch: 31080] train loss: 0.3086, train acc: 0.8690, val loss: 0.2714, val acc: 0.8779  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2350  @ epoch 30886 )\n",
      "[Epoch: 31100] train loss: 0.2864, train acc: 0.8823, val loss: 0.2576, val acc: 0.8877  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2350  @ epoch 30886 )\n",
      "[Epoch: 31120] train loss: 0.2587, train acc: 0.8910, val loss: 0.3084, val acc: 0.8705  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2350  @ epoch 30886 )\n",
      "[Epoch: 31140] train loss: 0.2565, train acc: 0.8941, val loss: 0.2641, val acc: 0.8836  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2350  @ epoch 30886 )\n",
      "[Epoch: 31160] train loss: 0.2645, train acc: 0.8861, val loss: 0.2605, val acc: 0.8836  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2350  @ epoch 30886 )\n",
      "[Epoch: 31180] train loss: 0.2495, train acc: 0.8974, val loss: 0.3066, val acc: 0.8712  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2350  @ epoch 30886 )\n",
      "[Epoch: 31200] train loss: 0.2593, train acc: 0.8946, val loss: 0.2610, val acc: 0.8874  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2350  @ epoch 30886 )\n",
      "[Epoch: 31220] train loss: 0.2472, train acc: 0.8984, val loss: 0.2598, val acc: 0.8796  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2350  @ epoch 30886 )\n",
      "[Epoch: 31240] train loss: 0.2976, train acc: 0.8806, val loss: 0.2709, val acc: 0.8752  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2350  @ epoch 30886 )\n",
      "[Epoch: 31260] train loss: 0.2695, train acc: 0.8919, val loss: 0.2659, val acc: 0.8884  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2350  @ epoch 30886 )\n",
      "[Epoch: 31280] train loss: 0.2722, train acc: 0.8914, val loss: 0.2603, val acc: 0.8853  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2350  @ epoch 30886 )\n",
      "[Epoch: 31300] train loss: 0.2518, train acc: 0.8943, val loss: 0.2677, val acc: 0.8874  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2350  @ epoch 30886 )\n",
      "[Epoch: 31320] train loss: 0.2555, train acc: 0.8983, val loss: 0.2587, val acc: 0.8863  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 31340] train loss: 0.3022, train acc: 0.8817, val loss: 0.2749, val acc: 0.8799  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 31360] train loss: 0.2576, train acc: 0.8940, val loss: 0.2802, val acc: 0.8762  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 31380] train loss: 0.2492, train acc: 0.8955, val loss: 0.2543, val acc: 0.8850  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 31400] train loss: 0.2844, train acc: 0.8830, val loss: 0.2669, val acc: 0.8840  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 31420] train loss: 0.2442, train acc: 0.8994, val loss: 0.2629, val acc: 0.8897  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 31440] train loss: 0.2525, train acc: 0.8988, val loss: 0.2561, val acc: 0.8863  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 31460] train loss: 0.2625, train acc: 0.8886, val loss: 0.2589, val acc: 0.8816  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 31480] train loss: 0.2607, train acc: 0.8931, val loss: 0.2682, val acc: 0.8836  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 31500] train loss: 0.2471, train acc: 0.8989, val loss: 0.2596, val acc: 0.8860  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 31520] train loss: 0.2902, train acc: 0.8789, val loss: 0.2789, val acc: 0.8789  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 31540] train loss: 0.2763, train acc: 0.8861, val loss: 0.3042, val acc: 0.8648  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 31560] train loss: 0.2732, train acc: 0.8884, val loss: 0.2574, val acc: 0.8887  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 31580] train loss: 0.2519, train acc: 0.8982, val loss: 0.2807, val acc: 0.8786  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 31600] train loss: 0.2652, train acc: 0.8950, val loss: 0.2607, val acc: 0.8867  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 31620] train loss: 0.2815, train acc: 0.8791, val loss: 0.3100, val acc: 0.8708  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 31640] train loss: 0.2579, train acc: 0.8951, val loss: 0.3003, val acc: 0.8668  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 31660] train loss: 0.2431, train acc: 0.8989, val loss: 0.2541, val acc: 0.8887  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 31680] train loss: 0.2543, train acc: 0.8947, val loss: 0.2775, val acc: 0.8806  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 31700] train loss: 0.2655, train acc: 0.8895, val loss: 0.2647, val acc: 0.8833  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 31720] train loss: 0.2565, train acc: 0.8930, val loss: 0.2593, val acc: 0.8840  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 31740] train loss: 0.2615, train acc: 0.8969, val loss: 0.2552, val acc: 0.8857  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 31760] train loss: 0.2841, train acc: 0.8838, val loss: 0.2760, val acc: 0.8806  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 31780] train loss: 0.2451, train acc: 0.9008, val loss: 0.2545, val acc: 0.8850  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 31800] train loss: 0.2853, train acc: 0.8862, val loss: 0.3440, val acc: 0.8530  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 31820] train loss: 0.2512, train acc: 0.8955, val loss: 0.2597, val acc: 0.8857  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 31840] train loss: 0.2736, train acc: 0.8845, val loss: 0.2783, val acc: 0.8786  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 31860] train loss: 0.3219, train acc: 0.8700, val loss: 0.2667, val acc: 0.8823  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 31880] train loss: 0.2723, train acc: 0.8916, val loss: 0.2593, val acc: 0.8870  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 31900] train loss: 0.2645, train acc: 0.8900, val loss: 0.2608, val acc: 0.8823  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 31920] train loss: 0.2502, train acc: 0.8990, val loss: 0.2588, val acc: 0.8853  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 31940] train loss: 0.2503, train acc: 0.8944, val loss: 0.2662, val acc: 0.8803  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 31960] train loss: 0.2492, train acc: 0.8997, val loss: 0.2701, val acc: 0.8789  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 31980] train loss: 0.2479, train acc: 0.9008, val loss: 0.2594, val acc: 0.8863  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 32000] train loss: 0.2403, train acc: 0.9035, val loss: 0.2624, val acc: 0.8870  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 32020] train loss: 0.2577, train acc: 0.8931, val loss: 0.2695, val acc: 0.8820  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 32040] train loss: 0.2503, train acc: 0.8930, val loss: 0.2693, val acc: 0.8874  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 32060] train loss: 0.2783, train acc: 0.8893, val loss: 0.2707, val acc: 0.8840  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 32080] train loss: 0.2616, train acc: 0.8894, val loss: 0.2531, val acc: 0.8884  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 32100] train loss: 0.2501, train acc: 0.8971, val loss: 0.2624, val acc: 0.8843  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 32120] train loss: 0.2948, train acc: 0.8812, val loss: 0.2705, val acc: 0.8850  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 32140] train loss: 0.2448, train acc: 0.8991, val loss: 0.2560, val acc: 0.8857  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 32160] train loss: 0.2668, train acc: 0.8898, val loss: 0.2919, val acc: 0.8718  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 32180] train loss: 0.2398, train acc: 0.9038, val loss: 0.2600, val acc: 0.8857  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 32200] train loss: 0.2679, train acc: 0.8907, val loss: 0.2571, val acc: 0.8863  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 32220] train loss: 0.2769, train acc: 0.8840, val loss: 0.2639, val acc: 0.8884  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 32240] train loss: 0.2607, train acc: 0.8908, val loss: 0.2842, val acc: 0.8857  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 32260] train loss: 0.2698, train acc: 0.8864, val loss: 0.2542, val acc: 0.8843  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 32280] train loss: 0.2515, train acc: 0.9018, val loss: 0.2649, val acc: 0.8840  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 32300] train loss: 0.2601, train acc: 0.8892, val loss: 0.2674, val acc: 0.8826  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 32320] train loss: 0.2757, train acc: 0.8871, val loss: 0.2627, val acc: 0.8826  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 32340] train loss: 0.2715, train acc: 0.8932, val loss: 0.2906, val acc: 0.8678  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 32360] train loss: 0.2683, train acc: 0.8858, val loss: 0.2846, val acc: 0.8745  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 32380] train loss: 0.2673, train acc: 0.8887, val loss: 0.2752, val acc: 0.8833  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 32400] train loss: 0.2569, train acc: 0.8890, val loss: 0.2656, val acc: 0.8816  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 32420] train loss: 0.2603, train acc: 0.8963, val loss: 0.2673, val acc: 0.8830  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 32440] train loss: 0.2516, train acc: 0.8970, val loss: 0.2580, val acc: 0.8887  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 32460] train loss: 0.2444, train acc: 0.9032, val loss: 0.2551, val acc: 0.8847  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 32480] train loss: 0.2704, train acc: 0.8891, val loss: 0.2767, val acc: 0.8776  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 32500] train loss: 0.2428, train acc: 0.8965, val loss: 0.2598, val acc: 0.8836  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 32520] train loss: 0.2552, train acc: 0.8953, val loss: 0.3040, val acc: 0.8654  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 32540] train loss: 0.2449, train acc: 0.8987, val loss: 0.2589, val acc: 0.8874  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 32560] train loss: 0.3019, train acc: 0.8776, val loss: 0.2562, val acc: 0.8847  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 32580] train loss: 0.2586, train acc: 0.8947, val loss: 0.2617, val acc: 0.8847  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 32600] train loss: 0.2336, train acc: 0.9076, val loss: 0.2535, val acc: 0.8884  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 32620] train loss: 0.2644, train acc: 0.8950, val loss: 0.2577, val acc: 0.8894  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 32640] train loss: 0.3009, train acc: 0.8791, val loss: 0.2602, val acc: 0.8833  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 32660] train loss: 0.2669, train acc: 0.8926, val loss: 0.2653, val acc: 0.8826  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 32680] train loss: 0.2690, train acc: 0.8910, val loss: 0.2565, val acc: 0.8860  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 32700] train loss: 0.2711, train acc: 0.8838, val loss: 0.2612, val acc: 0.8847  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 32720] train loss: 0.2585, train acc: 0.8926, val loss: 0.2630, val acc: 0.8890  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 32740] train loss: 0.2836, train acc: 0.8794, val loss: 0.3168, val acc: 0.8634  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 32760] train loss: 0.2459, train acc: 0.8999, val loss: 0.2667, val acc: 0.8847  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 32780] train loss: 0.2470, train acc: 0.8977, val loss: 0.2609, val acc: 0.8880  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 32800] train loss: 0.2593, train acc: 0.8913, val loss: 0.2636, val acc: 0.8853  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 32820] train loss: 0.2479, train acc: 0.8984, val loss: 0.2635, val acc: 0.8803  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 32840] train loss: 0.2650, train acc: 0.8887, val loss: 0.2637, val acc: 0.8820  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 32860] train loss: 0.2672, train acc: 0.8927, val loss: 0.2619, val acc: 0.8840  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 32880] train loss: 0.2798, train acc: 0.8737, val loss: 0.2631, val acc: 0.8782  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 32900] train loss: 0.2609, train acc: 0.8939, val loss: 0.2794, val acc: 0.8755  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 32920] train loss: 0.2513, train acc: 0.9004, val loss: 0.2971, val acc: 0.8718  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 32940] train loss: 0.2502, train acc: 0.8968, val loss: 0.2524, val acc: 0.8850  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 32960] train loss: 0.2649, train acc: 0.8950, val loss: 0.2788, val acc: 0.8806  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 32980] train loss: 0.2382, train acc: 0.9044, val loss: 0.2580, val acc: 0.8904  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 33000] train loss: 0.2503, train acc: 0.8997, val loss: 0.2539, val acc: 0.8826  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 33020] train loss: 0.2502, train acc: 0.8974, val loss: 0.2560, val acc: 0.8897  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 33040] train loss: 0.2510, train acc: 0.8961, val loss: 0.2799, val acc: 0.8772  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 33060] train loss: 0.2547, train acc: 0.8952, val loss: 0.2680, val acc: 0.8843  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 33080] train loss: 0.2507, train acc: 0.8970, val loss: 0.2786, val acc: 0.8769  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 33100] train loss: 0.2513, train acc: 0.8940, val loss: 0.2581, val acc: 0.8863  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 33120] train loss: 0.2691, train acc: 0.8905, val loss: 0.2584, val acc: 0.8867  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 33140] train loss: 0.3138, train acc: 0.8755, val loss: 0.2799, val acc: 0.8745  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 33160] train loss: 0.2534, train acc: 0.8971, val loss: 0.2594, val acc: 0.8880  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 33180] train loss: 0.2509, train acc: 0.8987, val loss: 0.2511, val acc: 0.8860  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 33200] train loss: 0.3313, train acc: 0.8721, val loss: 0.3019, val acc: 0.8702  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 33220] train loss: 0.2591, train acc: 0.8915, val loss: 0.2602, val acc: 0.8870  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 33240] train loss: 0.2393, train acc: 0.9024, val loss: 0.2734, val acc: 0.8793  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 33260] train loss: 0.2618, train acc: 0.8955, val loss: 0.2573, val acc: 0.8860  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 33280] train loss: 0.2813, train acc: 0.8872, val loss: 0.2703, val acc: 0.8803  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 33300] train loss: 0.2472, train acc: 0.8960, val loss: 0.2711, val acc: 0.8836  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 33320] train loss: 0.2449, train acc: 0.9002, val loss: 0.2595, val acc: 0.8847  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 33340] train loss: 0.2478, train acc: 0.8967, val loss: 0.2514, val acc: 0.8874  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 33360] train loss: 0.2504, train acc: 0.8973, val loss: 0.2633, val acc: 0.8860  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 33380] train loss: 0.2473, train acc: 0.8930, val loss: 0.2592, val acc: 0.8809  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 33400] train loss: 0.2608, train acc: 0.8933, val loss: 0.2763, val acc: 0.8809  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 33420] train loss: 0.2432, train acc: 0.8978, val loss: 0.2634, val acc: 0.8874  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 33440] train loss: 0.2440, train acc: 0.8968, val loss: 0.2526, val acc: 0.8874  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 33460] train loss: 0.2455, train acc: 0.9001, val loss: 0.2648, val acc: 0.8820  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 33480] train loss: 0.2341, train acc: 0.9066, val loss: 0.2539, val acc: 0.8850  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 33500] train loss: 0.2435, train acc: 0.8990, val loss: 0.2556, val acc: 0.8847  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 33520] train loss: 0.2454, train acc: 0.8997, val loss: 0.2579, val acc: 0.8816  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 33540] train loss: 0.2641, train acc: 0.8938, val loss: 0.2738, val acc: 0.8752  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 33560] train loss: 0.2843, train acc: 0.8822, val loss: 0.2573, val acc: 0.8853  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 33580] train loss: 0.2423, train acc: 0.8988, val loss: 0.2602, val acc: 0.8816  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 33600] train loss: 0.2498, train acc: 0.8947, val loss: 0.2583, val acc: 0.8826  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 33620] train loss: 0.2468, train acc: 0.8974, val loss: 0.2526, val acc: 0.8880  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 33640] train loss: 0.2787, train acc: 0.8850, val loss: 0.2933, val acc: 0.8755  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 33660] train loss: 0.2356, train acc: 0.9001, val loss: 0.2546, val acc: 0.8874  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 33680] train loss: 0.2550, train acc: 0.8973, val loss: 0.2682, val acc: 0.8813  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 33700] train loss: 0.2642, train acc: 0.8892, val loss: 0.3011, val acc: 0.8607  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 33720] train loss: 0.2649, train acc: 0.8882, val loss: 0.2546, val acc: 0.8850  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 33740] train loss: 0.2537, train acc: 0.8943, val loss: 0.2680, val acc: 0.8863  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 33760] train loss: 0.2616, train acc: 0.8897, val loss: 0.2958, val acc: 0.8732  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 33780] train loss: 0.2463, train acc: 0.8971, val loss: 0.2595, val acc: 0.8826  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 33800] train loss: 0.2628, train acc: 0.8895, val loss: 0.2591, val acc: 0.8836  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 33820] train loss: 0.2679, train acc: 0.8932, val loss: 0.2638, val acc: 0.8830  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 33840] train loss: 0.2695, train acc: 0.8895, val loss: 0.2694, val acc: 0.8813  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 33860] train loss: 0.2699, train acc: 0.8907, val loss: 0.2548, val acc: 0.8874  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 33880] train loss: 0.2438, train acc: 0.9008, val loss: 0.2540, val acc: 0.8860  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 33900] train loss: 0.2715, train acc: 0.8882, val loss: 0.2593, val acc: 0.8836  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 33920] train loss: 0.2787, train acc: 0.8889, val loss: 0.2770, val acc: 0.8772  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 33940] train loss: 0.2389, train acc: 0.9007, val loss: 0.2660, val acc: 0.8806  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 33960] train loss: 0.2812, train acc: 0.8825, val loss: 0.2721, val acc: 0.8779  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 33980] train loss: 0.2549, train acc: 0.8963, val loss: 0.2568, val acc: 0.8870  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 34000] train loss: 0.2396, train acc: 0.9011, val loss: 0.2626, val acc: 0.8863  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 34020] train loss: 0.2521, train acc: 0.8991, val loss: 0.2911, val acc: 0.8651  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 34040] train loss: 0.2629, train acc: 0.8868, val loss: 0.2617, val acc: 0.8867  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 34060] train loss: 0.2525, train acc: 0.8953, val loss: 0.2569, val acc: 0.8850  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 34080] train loss: 0.2447, train acc: 0.9007, val loss: 0.2770, val acc: 0.8732  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 34100] train loss: 0.2690, train acc: 0.8910, val loss: 0.2582, val acc: 0.8843  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 34120] train loss: 0.2587, train acc: 0.8924, val loss: 0.2692, val acc: 0.8850  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 34140] train loss: 0.2324, train acc: 0.9024, val loss: 0.2547, val acc: 0.8820  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 34160] train loss: 0.2613, train acc: 0.8882, val loss: 0.2529, val acc: 0.8863  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 34180] train loss: 0.2461, train acc: 0.9013, val loss: 0.2591, val acc: 0.8847  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 34200] train loss: 0.2564, train acc: 0.8937, val loss: 0.2526, val acc: 0.8880  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 34220] train loss: 0.2506, train acc: 0.8967, val loss: 0.2638, val acc: 0.8766  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 34240] train loss: 0.2661, train acc: 0.8900, val loss: 0.2720, val acc: 0.8799  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 34260] train loss: 0.2657, train acc: 0.8928, val loss: 0.2732, val acc: 0.8766  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 34280] train loss: 0.2559, train acc: 0.8940, val loss: 0.2679, val acc: 0.8762  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 34300] train loss: 0.2424, train acc: 0.9005, val loss: 0.2592, val acc: 0.8853  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2287  @ epoch 31313 )\n",
      "[Epoch: 34320] train loss: 0.2470, train acc: 0.9002, val loss: 0.2587, val acc: 0.8840  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 34340] train loss: 0.2677, train acc: 0.8949, val loss: 0.2972, val acc: 0.8698  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 34360] train loss: 0.2727, train acc: 0.8863, val loss: 0.2532, val acc: 0.8860  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 34380] train loss: 0.2514, train acc: 0.8995, val loss: 0.2636, val acc: 0.8836  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 34400] train loss: 0.2553, train acc: 0.8976, val loss: 0.2803, val acc: 0.8728  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 34420] train loss: 0.2749, train acc: 0.8886, val loss: 0.2568, val acc: 0.8857  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 34440] train loss: 0.2575, train acc: 0.8940, val loss: 0.2720, val acc: 0.8823  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 34460] train loss: 0.2732, train acc: 0.8890, val loss: 0.2644, val acc: 0.8799  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 34480] train loss: 0.2664, train acc: 0.8838, val loss: 0.2604, val acc: 0.8830  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 34500] train loss: 0.2449, train acc: 0.9014, val loss: 0.2598, val acc: 0.8840  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 34520] train loss: 0.2802, train acc: 0.8864, val loss: 0.2560, val acc: 0.8789  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 34540] train loss: 0.2561, train acc: 0.8984, val loss: 0.2698, val acc: 0.8749  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 34560] train loss: 0.2630, train acc: 0.8928, val loss: 0.2551, val acc: 0.8843  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 34580] train loss: 0.2828, train acc: 0.8806, val loss: 0.2625, val acc: 0.8789  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 34600] train loss: 0.2431, train acc: 0.9033, val loss: 0.2739, val acc: 0.8782  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 34620] train loss: 0.2532, train acc: 0.8948, val loss: 0.2824, val acc: 0.8799  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 34640] train loss: 0.2673, train acc: 0.8915, val loss: 0.2648, val acc: 0.8813  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 34660] train loss: 0.2402, train acc: 0.8992, val loss: 0.2611, val acc: 0.8830  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 34680] train loss: 0.3652, train acc: 0.8637, val loss: 0.2625, val acc: 0.8853  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 34700] train loss: 0.2470, train acc: 0.8994, val loss: 0.2715, val acc: 0.8776  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 34720] train loss: 0.2495, train acc: 0.8965, val loss: 0.2493, val acc: 0.8833  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 34740] train loss: 0.2398, train acc: 0.8991, val loss: 0.2729, val acc: 0.8816  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 34760] train loss: 0.2426, train acc: 0.8967, val loss: 0.2623, val acc: 0.8850  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 34780] train loss: 0.2831, train acc: 0.8845, val loss: 0.3050, val acc: 0.8641  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 34800] train loss: 0.2680, train acc: 0.8872, val loss: 0.2586, val acc: 0.8820  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 34820] train loss: 0.2769, train acc: 0.8866, val loss: 0.2688, val acc: 0.8847  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 34840] train loss: 0.2450, train acc: 0.8951, val loss: 0.2570, val acc: 0.8793  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 34860] train loss: 0.2973, train acc: 0.8830, val loss: 0.2558, val acc: 0.8833  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 34880] train loss: 0.2642, train acc: 0.8874, val loss: 0.2779, val acc: 0.8755  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 34900] train loss: 0.3053, train acc: 0.8800, val loss: 0.2744, val acc: 0.8698  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 34920] train loss: 0.2612, train acc: 0.8874, val loss: 0.2772, val acc: 0.8776  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 34940] train loss: 0.2998, train acc: 0.8845, val loss: 0.2469, val acc: 0.8843  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 34960] train loss: 0.2681, train acc: 0.8892, val loss: 0.2933, val acc: 0.8691  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 34980] train loss: 0.2418, train acc: 0.9017, val loss: 0.2532, val acc: 0.8820  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 35000] train loss: 0.2536, train acc: 0.8957, val loss: 0.2620, val acc: 0.8806  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 35020] train loss: 0.2527, train acc: 0.8956, val loss: 0.2586, val acc: 0.8833  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 35040] train loss: 0.2907, train acc: 0.8757, val loss: 0.2786, val acc: 0.8732  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 35060] train loss: 0.2543, train acc: 0.8905, val loss: 0.2501, val acc: 0.8840  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 35080] train loss: 0.2500, train acc: 0.8981, val loss: 0.2531, val acc: 0.8843  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 35100] train loss: 0.2419, train acc: 0.9008, val loss: 0.2588, val acc: 0.8887  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 35120] train loss: 0.2421, train acc: 0.8979, val loss: 0.2673, val acc: 0.8762  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 35140] train loss: 0.2530, train acc: 0.8976, val loss: 0.2698, val acc: 0.8853  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 35160] train loss: 0.2438, train acc: 0.9017, val loss: 0.2603, val acc: 0.8853  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 35180] train loss: 0.2479, train acc: 0.8978, val loss: 0.2624, val acc: 0.8769  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 35200] train loss: 0.2689, train acc: 0.8931, val loss: 0.2769, val acc: 0.8803  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 35220] train loss: 0.2489, train acc: 0.8959, val loss: 0.2589, val acc: 0.8863  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 35240] train loss: 0.2479, train acc: 0.8973, val loss: 0.2497, val acc: 0.8803  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 35260] train loss: 0.2584, train acc: 0.8968, val loss: 0.2521, val acc: 0.8877  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 35280] train loss: 0.2457, train acc: 0.8976, val loss: 0.2535, val acc: 0.8847  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 35300] train loss: 0.2609, train acc: 0.8948, val loss: 0.2582, val acc: 0.8786  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 35320] train loss: 0.2921, train acc: 0.8766, val loss: 0.2567, val acc: 0.8833  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 35340] train loss: 0.2709, train acc: 0.8871, val loss: 0.2570, val acc: 0.8830  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 35360] train loss: 0.2425, train acc: 0.9015, val loss: 0.2607, val acc: 0.8796  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 35380] train loss: 0.2583, train acc: 0.8907, val loss: 0.2517, val acc: 0.8887  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 35400] train loss: 0.2569, train acc: 0.8988, val loss: 0.2536, val acc: 0.8820  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 35420] train loss: 0.2768, train acc: 0.8876, val loss: 0.2599, val acc: 0.8884  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 35440] train loss: 0.2604, train acc: 0.8931, val loss: 0.2501, val acc: 0.8857  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 35460] train loss: 0.2709, train acc: 0.8908, val loss: 0.2548, val acc: 0.8880  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 35480] train loss: 0.2354, train acc: 0.9029, val loss: 0.2525, val acc: 0.8857  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 35500] train loss: 0.2646, train acc: 0.8900, val loss: 0.2736, val acc: 0.8789  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 35520] train loss: 0.2610, train acc: 0.8900, val loss: 0.2490, val acc: 0.8830  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 35540] train loss: 0.2504, train acc: 0.8955, val loss: 0.2574, val acc: 0.8833  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 35560] train loss: 0.2512, train acc: 0.8958, val loss: 0.2649, val acc: 0.8796  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 35580] train loss: 0.2411, train acc: 0.9008, val loss: 0.2604, val acc: 0.8850  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 35600] train loss: 0.3213, train acc: 0.8762, val loss: 0.2696, val acc: 0.8803  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 35620] train loss: 0.2496, train acc: 0.8999, val loss: 0.2540, val acc: 0.8850  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 35640] train loss: 0.2427, train acc: 0.9025, val loss: 0.2532, val acc: 0.8853  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 35660] train loss: 0.2356, train acc: 0.9045, val loss: 0.2535, val acc: 0.8843  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 35680] train loss: 0.2721, train acc: 0.8850, val loss: 0.2593, val acc: 0.8847  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 35700] train loss: 0.3113, train acc: 0.8704, val loss: 0.3118, val acc: 0.8641  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 35720] train loss: 0.2596, train acc: 0.8890, val loss: 0.2648, val acc: 0.8742  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 35740] train loss: 0.2884, train acc: 0.8759, val loss: 0.2633, val acc: 0.8786  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 35760] train loss: 0.2439, train acc: 0.8986, val loss: 0.2548, val acc: 0.8874  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 35780] train loss: 0.2373, train acc: 0.8985, val loss: 0.2549, val acc: 0.8850  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 35800] train loss: 0.2575, train acc: 0.8955, val loss: 0.2697, val acc: 0.8766  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 35820] train loss: 0.2422, train acc: 0.9050, val loss: 0.2519, val acc: 0.8887  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 35840] train loss: 0.2676, train acc: 0.8918, val loss: 0.2689, val acc: 0.8772  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 35860] train loss: 0.2551, train acc: 0.8944, val loss: 0.2527, val acc: 0.8840  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 35880] train loss: 0.2570, train acc: 0.8917, val loss: 0.2554, val acc: 0.8820  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 35900] train loss: 0.2577, train acc: 0.8924, val loss: 0.2547, val acc: 0.8830  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 35920] train loss: 0.2679, train acc: 0.8861, val loss: 0.2600, val acc: 0.8847  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 35940] train loss: 0.2449, train acc: 0.8993, val loss: 0.2808, val acc: 0.8779  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 35960] train loss: 0.2619, train acc: 0.8934, val loss: 0.2675, val acc: 0.8796  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 35980] train loss: 0.2590, train acc: 0.8929, val loss: 0.2536, val acc: 0.8847  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 36000] train loss: 0.2941, train acc: 0.8774, val loss: 0.2693, val acc: 0.8739  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 36020] train loss: 0.2709, train acc: 0.8915, val loss: 0.2779, val acc: 0.8732  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 36040] train loss: 0.2432, train acc: 0.9004, val loss: 0.2603, val acc: 0.8833  (best train acc: 0.9088, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 36060] train loss: 0.2438, train acc: 0.8988, val loss: 0.2576, val acc: 0.8823  (best train acc: 0.9093, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 36080] train loss: 0.2404, train acc: 0.9036, val loss: 0.2619, val acc: 0.8857  (best train acc: 0.9093, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 36100] train loss: 0.2653, train acc: 0.8882, val loss: 0.2551, val acc: 0.8853  (best train acc: 0.9093, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 36120] train loss: 0.2437, train acc: 0.9029, val loss: 0.2504, val acc: 0.8880  (best train acc: 0.9093, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 36140] train loss: 0.2436, train acc: 0.9055, val loss: 0.2522, val acc: 0.8867  (best train acc: 0.9093, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 36160] train loss: 0.2750, train acc: 0.8861, val loss: 0.2637, val acc: 0.8796  (best train acc: 0.9093, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 36180] train loss: 0.2487, train acc: 0.8967, val loss: 0.2618, val acc: 0.8857  (best train acc: 0.9093, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 36200] train loss: 0.2462, train acc: 0.8950, val loss: 0.2622, val acc: 0.8759  (best train acc: 0.9093, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 36220] train loss: 0.2537, train acc: 0.8970, val loss: 0.2522, val acc: 0.8847  (best train acc: 0.9093, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 36240] train loss: 0.2542, train acc: 0.8961, val loss: 0.2766, val acc: 0.8769  (best train acc: 0.9093, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 36260] train loss: 0.2457, train acc: 0.9017, val loss: 0.2622, val acc: 0.8870  (best train acc: 0.9093, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 36280] train loss: 0.2539, train acc: 0.8939, val loss: 0.2600, val acc: 0.8867  (best train acc: 0.9093, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 36300] train loss: 0.2514, train acc: 0.8936, val loss: 0.2535, val acc: 0.8799  (best train acc: 0.9093, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 36320] train loss: 0.2444, train acc: 0.8995, val loss: 0.2516, val acc: 0.8826  (best train acc: 0.9093, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 36340] train loss: 0.2591, train acc: 0.8924, val loss: 0.2551, val acc: 0.8840  (best train acc: 0.9093, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 36360] train loss: 0.2308, train acc: 0.9042, val loss: 0.2984, val acc: 0.8728  (best train acc: 0.9093, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 36380] train loss: 0.2459, train acc: 0.8970, val loss: 0.2552, val acc: 0.8826  (best train acc: 0.9093, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 36400] train loss: 0.2346, train acc: 0.9045, val loss: 0.2578, val acc: 0.8890  (best train acc: 0.9093, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 36420] train loss: 0.2435, train acc: 0.9036, val loss: 0.2590, val acc: 0.8843  (best train acc: 0.9093, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 36440] train loss: 0.2594, train acc: 0.8960, val loss: 0.2719, val acc: 0.8745  (best train acc: 0.9093, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 36460] train loss: 0.2317, train acc: 0.9052, val loss: 0.2533, val acc: 0.8799  (best train acc: 0.9093, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 36480] train loss: 0.2534, train acc: 0.8958, val loss: 0.2947, val acc: 0.8634  (best train acc: 0.9093, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 36500] train loss: 0.2423, train acc: 0.8972, val loss: 0.2595, val acc: 0.8826  (best train acc: 0.9093, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 36520] train loss: 0.2844, train acc: 0.8834, val loss: 0.2566, val acc: 0.8826  (best train acc: 0.9093, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 36540] train loss: 0.2947, train acc: 0.8790, val loss: 0.2771, val acc: 0.8766  (best train acc: 0.9093, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 36560] train loss: 0.3029, train acc: 0.8798, val loss: 0.2646, val acc: 0.8840  (best train acc: 0.9093, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 36580] train loss: 0.2505, train acc: 0.8973, val loss: 0.2484, val acc: 0.8860  (best train acc: 0.9093, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 36600] train loss: 0.2545, train acc: 0.8984, val loss: 0.2789, val acc: 0.8769  (best train acc: 0.9093, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 36620] train loss: 0.2862, train acc: 0.8832, val loss: 0.2471, val acc: 0.8867  (best train acc: 0.9093, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 36640] train loss: 0.2692, train acc: 0.8877, val loss: 0.2702, val acc: 0.8755  (best train acc: 0.9093, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 36660] train loss: 0.2737, train acc: 0.8904, val loss: 0.2709, val acc: 0.8779  (best train acc: 0.9093, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 36680] train loss: 0.2510, train acc: 0.9017, val loss: 0.2580, val acc: 0.8867  (best train acc: 0.9093, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 36700] train loss: 0.2588, train acc: 0.8947, val loss: 0.2922, val acc: 0.8708  (best train acc: 0.9093, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 36720] train loss: 0.2482, train acc: 0.8980, val loss: 0.2484, val acc: 0.8863  (best train acc: 0.9093, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 36740] train loss: 0.2541, train acc: 0.8976, val loss: 0.2517, val acc: 0.8809  (best train acc: 0.9093, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 36760] train loss: 0.2738, train acc: 0.8903, val loss: 0.2876, val acc: 0.8715  (best train acc: 0.9103, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 36780] train loss: 0.2337, train acc: 0.9020, val loss: 0.2533, val acc: 0.8850  (best train acc: 0.9103, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 36800] train loss: 0.2807, train acc: 0.8814, val loss: 0.2510, val acc: 0.8840  (best train acc: 0.9103, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 36820] train loss: 0.2510, train acc: 0.8996, val loss: 0.2772, val acc: 0.8752  (best train acc: 0.9103, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 36840] train loss: 0.2407, train acc: 0.9067, val loss: 0.2971, val acc: 0.8681  (best train acc: 0.9103, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 36860] train loss: 0.3275, train acc: 0.8683, val loss: 0.2494, val acc: 0.8860  (best train acc: 0.9103, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 36880] train loss: 0.2494, train acc: 0.9017, val loss: 0.2513, val acc: 0.8853  (best train acc: 0.9103, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 36900] train loss: 0.2486, train acc: 0.8984, val loss: 0.2530, val acc: 0.8860  (best train acc: 0.9103, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 36920] train loss: 0.2614, train acc: 0.8933, val loss: 0.2576, val acc: 0.8860  (best train acc: 0.9103, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 36940] train loss: 0.2468, train acc: 0.8970, val loss: 0.2652, val acc: 0.8799  (best train acc: 0.9103, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 36960] train loss: 0.2402, train acc: 0.8991, val loss: 0.2542, val acc: 0.8803  (best train acc: 0.9103, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 36980] train loss: 0.2528, train acc: 0.8937, val loss: 0.2473, val acc: 0.8890  (best train acc: 0.9103, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 37000] train loss: 0.2389, train acc: 0.9020, val loss: 0.2674, val acc: 0.8803  (best train acc: 0.9103, best val acc: 0.8944, best train loss: 0.2272  @ epoch 34313 )\n",
      "[Epoch: 37020] train loss: 0.2517, train acc: 0.8956, val loss: 0.2545, val acc: 0.8857  (best train acc: 0.9103, best val acc: 0.8944, best train loss: 0.2262  @ epoch 37003 )\n",
      "[Epoch: 37040] train loss: 0.2505, train acc: 0.8986, val loss: 0.2493, val acc: 0.8887  (best train acc: 0.9103, best val acc: 0.8944, best train loss: 0.2262  @ epoch 37003 )\n",
      "[Epoch: 37060] train loss: 0.2401, train acc: 0.9012, val loss: 0.2558, val acc: 0.8789  (best train acc: 0.9103, best val acc: 0.8944, best train loss: 0.2262  @ epoch 37003 )\n",
      "[Epoch: 37080] train loss: 0.2542, train acc: 0.8950, val loss: 0.2665, val acc: 0.8786  (best train acc: 0.9103, best val acc: 0.8944, best train loss: 0.2262  @ epoch 37003 )\n",
      "[Epoch: 37100] train loss: 0.2592, train acc: 0.8911, val loss: 0.2503, val acc: 0.8836  (best train acc: 0.9103, best val acc: 0.8944, best train loss: 0.2242  @ epoch 37087 )\n",
      "[Epoch: 37120] train loss: 0.2335, train acc: 0.9027, val loss: 0.2597, val acc: 0.8820  (best train acc: 0.9103, best val acc: 0.8944, best train loss: 0.2242  @ epoch 37087 )\n",
      "[Epoch: 37140] train loss: 0.2587, train acc: 0.8905, val loss: 0.2653, val acc: 0.8836  (best train acc: 0.9103, best val acc: 0.8944, best train loss: 0.2242  @ epoch 37087 )\n",
      "[Epoch: 37160] train loss: 0.2404, train acc: 0.9020, val loss: 0.2606, val acc: 0.8867  (best train acc: 0.9103, best val acc: 0.8944, best train loss: 0.2242  @ epoch 37087 )\n",
      "[Epoch: 37180] train loss: 0.2347, train acc: 0.9009, val loss: 0.2602, val acc: 0.8840  (best train acc: 0.9103, best val acc: 0.8944, best train loss: 0.2242  @ epoch 37087 )\n",
      "[Epoch: 37200] train loss: 0.2686, train acc: 0.8867, val loss: 0.2701, val acc: 0.8759  (best train acc: 0.9103, best val acc: 0.8944, best train loss: 0.2242  @ epoch 37087 )\n",
      "[Epoch: 37220] train loss: 0.2391, train acc: 0.9025, val loss: 0.2505, val acc: 0.8877  (best train acc: 0.9103, best val acc: 0.8944, best train loss: 0.2242  @ epoch 37087 )\n",
      "[Epoch: 37240] train loss: 0.2328, train acc: 0.9061, val loss: 0.2860, val acc: 0.8728  (best train acc: 0.9103, best val acc: 0.8944, best train loss: 0.2242  @ epoch 37087 )\n",
      "[Epoch: 37260] train loss: 0.2390, train acc: 0.8997, val loss: 0.2507, val acc: 0.8840  (best train acc: 0.9103, best val acc: 0.8944, best train loss: 0.2242  @ epoch 37087 )\n",
      "[Epoch: 37280] train loss: 0.2489, train acc: 0.8975, val loss: 0.2737, val acc: 0.8823  (best train acc: 0.9103, best val acc: 0.8944, best train loss: 0.2242  @ epoch 37087 )\n",
      "[Epoch: 37300] train loss: 0.2488, train acc: 0.9003, val loss: 0.2580, val acc: 0.8877  (best train acc: 0.9103, best val acc: 0.8944, best train loss: 0.2242  @ epoch 37087 )\n",
      "[Epoch: 37320] train loss: 0.2546, train acc: 0.8914, val loss: 0.2494, val acc: 0.8867  (best train acc: 0.9103, best val acc: 0.8944, best train loss: 0.2228  @ epoch 37306 )\n",
      "[Epoch: 37340] train loss: 0.2452, train acc: 0.8963, val loss: 0.2645, val acc: 0.8799  (best train acc: 0.9103, best val acc: 0.8944, best train loss: 0.2228  @ epoch 37306 )\n",
      "[Epoch: 37360] train loss: 0.2432, train acc: 0.8956, val loss: 0.2528, val acc: 0.8833  (best train acc: 0.9103, best val acc: 0.8944, best train loss: 0.2228  @ epoch 37306 )\n",
      "[Epoch: 37380] train loss: 0.2307, train acc: 0.9043, val loss: 0.2565, val acc: 0.8830  (best train acc: 0.9103, best val acc: 0.8944, best train loss: 0.2228  @ epoch 37306 )\n",
      "[Epoch: 37400] train loss: 0.2362, train acc: 0.8986, val loss: 0.2492, val acc: 0.8884  (best train acc: 0.9103, best val acc: 0.8944, best train loss: 0.2228  @ epoch 37306 )\n",
      "[Epoch: 37420] train loss: 0.2468, train acc: 0.8944, val loss: 0.2570, val acc: 0.8877  (best train acc: 0.9103, best val acc: 0.8944, best train loss: 0.2228  @ epoch 37306 )\n",
      "[Epoch: 37440] train loss: 0.2814, train acc: 0.8857, val loss: 0.3198, val acc: 0.8631  (best train acc: 0.9103, best val acc: 0.8944, best train loss: 0.2228  @ epoch 37306 )\n",
      "[Epoch: 37460] train loss: 0.2378, train acc: 0.9003, val loss: 0.2701, val acc: 0.8789  (best train acc: 0.9103, best val acc: 0.8944, best train loss: 0.2228  @ epoch 37306 )\n",
      "[Epoch: 37480] train loss: 0.2441, train acc: 0.8965, val loss: 0.2615, val acc: 0.8857  (best train acc: 0.9103, best val acc: 0.8944, best train loss: 0.2228  @ epoch 37306 )\n",
      "[Epoch: 37500] train loss: 0.2449, train acc: 0.8989, val loss: 0.2529, val acc: 0.8850  (best train acc: 0.9103, best val acc: 0.8944, best train loss: 0.2228  @ epoch 37306 )\n",
      "[Epoch: 37520] train loss: 0.2837, train acc: 0.8858, val loss: 0.2580, val acc: 0.8826  (best train acc: 0.9103, best val acc: 0.8944, best train loss: 0.2228  @ epoch 37306 )\n",
      "[Epoch: 37540] train loss: 0.2378, train acc: 0.8978, val loss: 0.2558, val acc: 0.8826  (best train acc: 0.9103, best val acc: 0.8944, best train loss: 0.2228  @ epoch 37306 )\n",
      "[Epoch: 37560] train loss: 0.2338, train acc: 0.9018, val loss: 0.2555, val acc: 0.8847  (best train acc: 0.9103, best val acc: 0.8944, best train loss: 0.2209  @ epoch 37554 )\n",
      "[Epoch: 37580] train loss: 0.2317, train acc: 0.9024, val loss: 0.2639, val acc: 0.8786  (best train acc: 0.9103, best val acc: 0.8944, best train loss: 0.2209  @ epoch 37554 )\n",
      "[Epoch: 37600] train loss: 0.2345, train acc: 0.9072, val loss: 0.2675, val acc: 0.8762  (best train acc: 0.9103, best val acc: 0.8944, best train loss: 0.2209  @ epoch 37554 )\n",
      "[Epoch: 37620] train loss: 0.2391, train acc: 0.8986, val loss: 0.2662, val acc: 0.8847  (best train acc: 0.9110, best val acc: 0.8944, best train loss: 0.2209  @ epoch 37554 )\n",
      "[Epoch: 37640] train loss: 0.2357, train acc: 0.9007, val loss: 0.2576, val acc: 0.8860  (best train acc: 0.9110, best val acc: 0.8944, best train loss: 0.2184  @ epoch 37622 )\n",
      "[Epoch: 37660] train loss: 0.2277, train acc: 0.9095, val loss: 0.2707, val acc: 0.8813  (best train acc: 0.9110, best val acc: 0.8944, best train loss: 0.2184  @ epoch 37622 )\n",
      "[Epoch: 37680] train loss: 0.2398, train acc: 0.9016, val loss: 0.2732, val acc: 0.8739  (best train acc: 0.9110, best val acc: 0.8944, best train loss: 0.2184  @ epoch 37622 )\n",
      "[Epoch: 37700] train loss: 0.2684, train acc: 0.8857, val loss: 0.2689, val acc: 0.8826  (best train acc: 0.9110, best val acc: 0.8944, best train loss: 0.2184  @ epoch 37622 )\n",
      "[Epoch: 37720] train loss: 0.2570, train acc: 0.8941, val loss: 0.2894, val acc: 0.8772  (best train acc: 0.9110, best val acc: 0.8944, best train loss: 0.2183  @ epoch 37714 )\n",
      "[Epoch: 37740] train loss: 0.2451, train acc: 0.8976, val loss: 0.2506, val acc: 0.8867  (best train acc: 0.9110, best val acc: 0.8944, best train loss: 0.2183  @ epoch 37714 )\n",
      "[Epoch: 37760] train loss: 0.2432, train acc: 0.9018, val loss: 0.2663, val acc: 0.8803  (best train acc: 0.9110, best val acc: 0.8944, best train loss: 0.2183  @ epoch 37714 )\n",
      "[Epoch: 37780] train loss: 0.2723, train acc: 0.8900, val loss: 0.2520, val acc: 0.8853  (best train acc: 0.9110, best val acc: 0.8944, best train loss: 0.2183  @ epoch 37714 )\n",
      "[Epoch: 37800] train loss: 0.2415, train acc: 0.8950, val loss: 0.2596, val acc: 0.8830  (best train acc: 0.9110, best val acc: 0.8944, best train loss: 0.2183  @ epoch 37714 )\n",
      "[Epoch: 37820] train loss: 0.2424, train acc: 0.9012, val loss: 0.2560, val acc: 0.8826  (best train acc: 0.9110, best val acc: 0.8944, best train loss: 0.2183  @ epoch 37714 )\n",
      "[Epoch: 37840] train loss: 0.2302, train acc: 0.9048, val loss: 0.2577, val acc: 0.8850  (best train acc: 0.9110, best val acc: 0.8944, best train loss: 0.2183  @ epoch 37714 )\n",
      "[Epoch: 37860] train loss: 0.2279, train acc: 0.9068, val loss: 0.2614, val acc: 0.8840  (best train acc: 0.9110, best val acc: 0.8944, best train loss: 0.2183  @ epoch 37714 )\n",
      "[Epoch: 37880] train loss: 0.2436, train acc: 0.9004, val loss: 0.2670, val acc: 0.8843  (best train acc: 0.9110, best val acc: 0.8944, best train loss: 0.2183  @ epoch 37714 )\n",
      "[Epoch: 37900] train loss: 0.2326, train acc: 0.9045, val loss: 0.3119, val acc: 0.8607  (best train acc: 0.9110, best val acc: 0.8944, best train loss: 0.2183  @ epoch 37714 )\n",
      "[Epoch: 37920] train loss: 0.2593, train acc: 0.8944, val loss: 0.2676, val acc: 0.8836  (best train acc: 0.9110, best val acc: 0.8944, best train loss: 0.2183  @ epoch 37714 )\n",
      "[Epoch: 37940] train loss: 0.2230, train acc: 0.9081, val loss: 0.2467, val acc: 0.8820  (best train acc: 0.9110, best val acc: 0.8944, best train loss: 0.2183  @ epoch 37714 )\n",
      "[Epoch: 37960] train loss: 0.2615, train acc: 0.8914, val loss: 0.2596, val acc: 0.8826  (best train acc: 0.9110, best val acc: 0.8944, best train loss: 0.2183  @ epoch 37714 )\n",
      "[Epoch: 37980] train loss: 0.2414, train acc: 0.8976, val loss: 0.2552, val acc: 0.8884  (best train acc: 0.9110, best val acc: 0.8944, best train loss: 0.2183  @ epoch 37714 )\n",
      "[Epoch: 38000] train loss: 0.2395, train acc: 0.8994, val loss: 0.2517, val acc: 0.8843  (best train acc: 0.9110, best val acc: 0.8944, best train loss: 0.2183  @ epoch 37714 )\n",
      "[Epoch: 38020] train loss: 0.2315, train acc: 0.9026, val loss: 0.2481, val acc: 0.8847  (best train acc: 0.9110, best val acc: 0.8944, best train loss: 0.2183  @ epoch 37714 )\n",
      "[Epoch: 38040] train loss: 0.2411, train acc: 0.8997, val loss: 0.2631, val acc: 0.8769  (best train acc: 0.9110, best val acc: 0.8944, best train loss: 0.2183  @ epoch 37714 )\n",
      "[Epoch: 38060] train loss: 0.2317, train acc: 0.9024, val loss: 0.2728, val acc: 0.8762  (best train acc: 0.9110, best val acc: 0.8944, best train loss: 0.2183  @ epoch 37714 )\n",
      "[Epoch: 38080] train loss: 0.2531, train acc: 0.9043, val loss: 0.2467, val acc: 0.8843  (best train acc: 0.9110, best val acc: 0.8944, best train loss: 0.2183  @ epoch 37714 )\n",
      "[Epoch: 38100] train loss: 0.2397, train acc: 0.8989, val loss: 0.2630, val acc: 0.8860  (best train acc: 0.9110, best val acc: 0.8944, best train loss: 0.2183  @ epoch 37714 )\n",
      "[Epoch: 38120] train loss: 0.2334, train acc: 0.9036, val loss: 0.2765, val acc: 0.8779  (best train acc: 0.9110, best val acc: 0.8944, best train loss: 0.2183  @ epoch 37714 )\n",
      "[Epoch: 38140] train loss: 0.2399, train acc: 0.9007, val loss: 0.2788, val acc: 0.8766  (best train acc: 0.9110, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 38160] train loss: 0.2455, train acc: 0.8953, val loss: 0.2604, val acc: 0.8809  (best train acc: 0.9110, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 38180] train loss: 0.2418, train acc: 0.9001, val loss: 0.2482, val acc: 0.8877  (best train acc: 0.9112, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 38200] train loss: 0.2219, train acc: 0.9099, val loss: 0.2535, val acc: 0.8833  (best train acc: 0.9112, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 38220] train loss: 0.2537, train acc: 0.8937, val loss: 0.2977, val acc: 0.8718  (best train acc: 0.9112, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 38240] train loss: 0.2299, train acc: 0.9067, val loss: 0.2560, val acc: 0.8874  (best train acc: 0.9112, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 38260] train loss: 0.2357, train acc: 0.9012, val loss: 0.2717, val acc: 0.8789  (best train acc: 0.9112, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 38280] train loss: 0.2724, train acc: 0.8850, val loss: 0.2520, val acc: 0.8823  (best train acc: 0.9112, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 38300] train loss: 0.2360, train acc: 0.9033, val loss: 0.2548, val acc: 0.8840  (best train acc: 0.9112, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 38320] train loss: 0.2494, train acc: 0.8970, val loss: 0.2633, val acc: 0.8863  (best train acc: 0.9112, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 38340] train loss: 0.2500, train acc: 0.8950, val loss: 0.2822, val acc: 0.8728  (best train acc: 0.9112, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 38360] train loss: 0.2287, train acc: 0.9036, val loss: 0.2617, val acc: 0.8789  (best train acc: 0.9112, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 38380] train loss: 0.2550, train acc: 0.8923, val loss: 0.2505, val acc: 0.8860  (best train acc: 0.9112, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 38400] train loss: 0.2633, train acc: 0.8908, val loss: 0.2760, val acc: 0.8735  (best train acc: 0.9112, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 38420] train loss: 0.2469, train acc: 0.8997, val loss: 0.2545, val acc: 0.8863  (best train acc: 0.9112, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 38440] train loss: 0.2511, train acc: 0.8916, val loss: 0.2553, val acc: 0.8853  (best train acc: 0.9112, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 38460] train loss: 0.2545, train acc: 0.8987, val loss: 0.2864, val acc: 0.8718  (best train acc: 0.9112, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 38480] train loss: 0.2269, train acc: 0.9072, val loss: 0.2631, val acc: 0.8826  (best train acc: 0.9112, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 38500] train loss: 0.2251, train acc: 0.9074, val loss: 0.2442, val acc: 0.8890  (best train acc: 0.9123, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 38520] train loss: 0.2358, train acc: 0.8995, val loss: 0.2566, val acc: 0.8840  (best train acc: 0.9123, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 38540] train loss: 0.2460, train acc: 0.8942, val loss: 0.2757, val acc: 0.8755  (best train acc: 0.9123, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 38560] train loss: 0.2310, train acc: 0.9030, val loss: 0.2555, val acc: 0.8874  (best train acc: 0.9123, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 38580] train loss: 0.2770, train acc: 0.8820, val loss: 0.2632, val acc: 0.8772  (best train acc: 0.9123, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 38600] train loss: 0.2410, train acc: 0.8986, val loss: 0.2482, val acc: 0.8874  (best train acc: 0.9123, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 38620] train loss: 0.2571, train acc: 0.8897, val loss: 0.2578, val acc: 0.8833  (best train acc: 0.9123, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 38640] train loss: 0.2266, train acc: 0.9025, val loss: 0.2569, val acc: 0.8820  (best train acc: 0.9123, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 38660] train loss: 0.2365, train acc: 0.9017, val loss: 0.2602, val acc: 0.8850  (best train acc: 0.9123, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 38680] train loss: 0.2515, train acc: 0.8918, val loss: 0.2514, val acc: 0.8860  (best train acc: 0.9123, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 38700] train loss: 0.2261, train acc: 0.9054, val loss: 0.2734, val acc: 0.8799  (best train acc: 0.9123, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 38720] train loss: 0.2314, train acc: 0.9059, val loss: 0.2564, val acc: 0.8830  (best train acc: 0.9123, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 38740] train loss: 0.2280, train acc: 0.9038, val loss: 0.2505, val acc: 0.8870  (best train acc: 0.9123, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 38760] train loss: 0.2510, train acc: 0.8943, val loss: 0.2522, val acc: 0.8843  (best train acc: 0.9123, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 38780] train loss: 0.2303, train acc: 0.9037, val loss: 0.2464, val acc: 0.8880  (best train acc: 0.9123, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 38800] train loss: 0.2604, train acc: 0.8985, val loss: 0.2495, val acc: 0.8833  (best train acc: 0.9123, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 38820] train loss: 0.2618, train acc: 0.8944, val loss: 0.2962, val acc: 0.8664  (best train acc: 0.9123, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 38840] train loss: 0.2645, train acc: 0.8884, val loss: 0.2600, val acc: 0.8793  (best train acc: 0.9123, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 38860] train loss: 0.2378, train acc: 0.9010, val loss: 0.2691, val acc: 0.8853  (best train acc: 0.9123, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 38880] train loss: 0.2463, train acc: 0.8971, val loss: 0.2655, val acc: 0.8843  (best train acc: 0.9123, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 38900] train loss: 0.2414, train acc: 0.9018, val loss: 0.2617, val acc: 0.8874  (best train acc: 0.9123, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 38920] train loss: 0.2358, train acc: 0.9004, val loss: 0.2545, val acc: 0.8850  (best train acc: 0.9123, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 38940] train loss: 0.2220, train acc: 0.9065, val loss: 0.2683, val acc: 0.8850  (best train acc: 0.9123, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 38960] train loss: 0.2505, train acc: 0.8978, val loss: 0.2812, val acc: 0.8695  (best train acc: 0.9123, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 38980] train loss: 0.2724, train acc: 0.8866, val loss: 0.2572, val acc: 0.8816  (best train acc: 0.9123, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 39000] train loss: 0.2276, train acc: 0.9049, val loss: 0.2578, val acc: 0.8833  (best train acc: 0.9123, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 39020] train loss: 0.2459, train acc: 0.8952, val loss: 0.2507, val acc: 0.8867  (best train acc: 0.9123, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 39040] train loss: 0.2454, train acc: 0.8973, val loss: 0.2707, val acc: 0.8769  (best train acc: 0.9123, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 39060] train loss: 0.2589, train acc: 0.8960, val loss: 0.2693, val acc: 0.8769  (best train acc: 0.9123, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 39080] train loss: 0.2639, train acc: 0.8880, val loss: 0.2521, val acc: 0.8840  (best train acc: 0.9123, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 39100] train loss: 0.2504, train acc: 0.8963, val loss: 0.2618, val acc: 0.8830  (best train acc: 0.9123, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 39120] train loss: 0.2411, train acc: 0.9036, val loss: 0.2726, val acc: 0.8772  (best train acc: 0.9123, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 39140] train loss: 0.2461, train acc: 0.8953, val loss: 0.2602, val acc: 0.8823  (best train acc: 0.9123, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 39160] train loss: 0.2769, train acc: 0.8798, val loss: 0.3321, val acc: 0.8506  (best train acc: 0.9123, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 39180] train loss: 0.2375, train acc: 0.9028, val loss: 0.2982, val acc: 0.8712  (best train acc: 0.9123, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 39200] train loss: 0.2700, train acc: 0.8917, val loss: 0.2598, val acc: 0.8847  (best train acc: 0.9123, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 39220] train loss: 0.2317, train acc: 0.9025, val loss: 0.2635, val acc: 0.8826  (best train acc: 0.9123, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 39240] train loss: 0.2474, train acc: 0.8962, val loss: 0.2484, val acc: 0.8860  (best train acc: 0.9123, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 39260] train loss: 0.2431, train acc: 0.8981, val loss: 0.2651, val acc: 0.8843  (best train acc: 0.9123, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 39280] train loss: 0.2302, train acc: 0.9083, val loss: 0.2573, val acc: 0.8884  (best train acc: 0.9123, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 39300] train loss: 0.2418, train acc: 0.8992, val loss: 0.2589, val acc: 0.8877  (best train acc: 0.9123, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 39320] train loss: 0.2547, train acc: 0.8970, val loss: 0.2638, val acc: 0.8870  (best train acc: 0.9123, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 39340] train loss: 0.2376, train acc: 0.9046, val loss: 0.2487, val acc: 0.8840  (best train acc: 0.9123, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 39360] train loss: 0.2313, train acc: 0.9064, val loss: 0.2517, val acc: 0.8816  (best train acc: 0.9123, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 39380] train loss: 0.2469, train acc: 0.8993, val loss: 0.2609, val acc: 0.8809  (best train acc: 0.9123, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 39400] train loss: 0.2216, train acc: 0.9069, val loss: 0.2594, val acc: 0.8806  (best train acc: 0.9123, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 39420] train loss: 0.2358, train acc: 0.9048, val loss: 0.2531, val acc: 0.8843  (best train acc: 0.9123, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 39440] train loss: 0.2257, train acc: 0.9061, val loss: 0.2598, val acc: 0.8789  (best train acc: 0.9123, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 39460] train loss: 0.2407, train acc: 0.9000, val loss: 0.2593, val acc: 0.8863  (best train acc: 0.9123, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 39480] train loss: 0.2357, train acc: 0.9002, val loss: 0.2476, val acc: 0.8860  (best train acc: 0.9123, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 39500] train loss: 0.2341, train acc: 0.9028, val loss: 0.2506, val acc: 0.8833  (best train acc: 0.9123, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 39520] train loss: 0.2230, train acc: 0.9090, val loss: 0.2712, val acc: 0.8742  (best train acc: 0.9123, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 39540] train loss: 0.2185, train acc: 0.9084, val loss: 0.2718, val acc: 0.8830  (best train acc: 0.9123, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 39560] train loss: 0.2312, train acc: 0.9018, val loss: 0.2598, val acc: 0.8793  (best train acc: 0.9123, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 39580] train loss: 0.2580, train acc: 0.8946, val loss: 0.2525, val acc: 0.8857  (best train acc: 0.9123, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 39600] train loss: 0.2230, train acc: 0.9079, val loss: 0.2506, val acc: 0.8820  (best train acc: 0.9123, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 39620] train loss: 0.2501, train acc: 0.8992, val loss: 0.2633, val acc: 0.8803  (best train acc: 0.9123, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 39640] train loss: 0.2352, train acc: 0.8976, val loss: 0.2860, val acc: 0.8779  (best train acc: 0.9123, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 39660] train loss: 0.2324, train acc: 0.9015, val loss: 0.2507, val acc: 0.8911  (best train acc: 0.9123, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 39680] train loss: 0.2468, train acc: 0.8974, val loss: 0.2610, val acc: 0.8857  (best train acc: 0.9123, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 39700] train loss: 0.2368, train acc: 0.9000, val loss: 0.2517, val acc: 0.8874  (best train acc: 0.9123, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 39720] train loss: 0.2595, train acc: 0.8891, val loss: 0.2515, val acc: 0.8857  (best train acc: 0.9123, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 39740] train loss: 0.2374, train acc: 0.8998, val loss: 0.2583, val acc: 0.8776  (best train acc: 0.9123, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 39760] train loss: 0.2278, train acc: 0.9050, val loss: 0.2509, val acc: 0.8840  (best train acc: 0.9123, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 39780] train loss: 0.2279, train acc: 0.9067, val loss: 0.2696, val acc: 0.8759  (best train acc: 0.9123, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 39800] train loss: 0.2375, train acc: 0.9005, val loss: 0.2529, val acc: 0.8850  (best train acc: 0.9123, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 39820] train loss: 0.2457, train acc: 0.8981, val loss: 0.2542, val acc: 0.8853  (best train acc: 0.9123, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 39840] train loss: 0.2451, train acc: 0.9010, val loss: 0.2510, val acc: 0.8833  (best train acc: 0.9123, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 39860] train loss: 0.2360, train acc: 0.9003, val loss: 0.2522, val acc: 0.8853  (best train acc: 0.9123, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 39880] train loss: 0.2232, train acc: 0.9061, val loss: 0.2681, val acc: 0.8796  (best train acc: 0.9123, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 39900] train loss: 0.2280, train acc: 0.9057, val loss: 0.2604, val acc: 0.8799  (best train acc: 0.9123, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 39920] train loss: 0.2313, train acc: 0.9087, val loss: 0.2632, val acc: 0.8813  (best train acc: 0.9123, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 39940] train loss: 0.2341, train acc: 0.8997, val loss: 0.2605, val acc: 0.8867  (best train acc: 0.9123, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 39960] train loss: 0.2310, train acc: 0.9033, val loss: 0.2757, val acc: 0.8799  (best train acc: 0.9123, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 39980] train loss: 0.2383, train acc: 0.9015, val loss: 0.2641, val acc: 0.8826  (best train acc: 0.9123, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n",
      "[Epoch: 40000] train loss: 0.2373, train acc: 0.9001, val loss: 0.2479, val acc: 0.8877  (best train acc: 0.9123, best val acc: 0.8944, best train loss: 0.2162  @ epoch 38138 )\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98       124\n",
      "           1       0.94      0.96      0.95       710\n",
      "           2       0.94      0.88      0.91       710\n",
      "           3       0.84      0.84      0.84       710\n",
      "           4       0.86      0.90      0.88       710\n",
      "\n",
      "    accuracy                           0.90      2964\n",
      "   macro avg       0.91      0.91      0.91      2964\n",
      "weighted avg       0.90      0.90      0.90      2964\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 504x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 504x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "lr              = 0.001\n",
    "epochs          = 40000\n",
    "weight_decay    = 0.0005\n",
    "\n",
    "classes         = ['P', 'LP', 'WN', 'LN', 'RN']\n",
    "gnn_model = GNN7L_Sage(dataset).to(device)\n",
    "pred = train(gnn_model, dataset, epochs, lr, weight_decay, classes, 'GraphSAGE_' + disease_Id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_csv('precision_positive_genes.csv', index_col=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_positives = df['n_positives'].to_list()\n",
    "C0006142 = df['C0006142'].to_list()\n",
    "C0009402 = df['C0009402'].to_list()\n",
    "C0023893 = df['C0023893'].to_list()\n",
    "C0036341 = df['C0036341'].to_list()\n",
    "C0376358 = df['C0376358'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "mode": "lines",
         "name": "C0006142",
         "type": "scatter",
         "x": [
          10,
          20,
          30,
          40,
          50,
          60,
          70,
          80,
          90,
          100,
          110,
          120,
          130,
          140,
          153,
          97,
          117,
          125,
          91
         ],
         "y": [
          84,
          79,
          79,
          75,
          78,
          80,
          78,
          77,
          80,
          74,
          75,
          77,
          76,
          76,
          75,
          null,
          null,
          null,
          null
         ]
        },
        {
         "mode": "lines",
         "name": "C0009402",
         "type": "scatter",
         "x": [
          10,
          20,
          30,
          40,
          50,
          60,
          70,
          80,
          90,
          100,
          110,
          120,
          130,
          140,
          153,
          97,
          117,
          125,
          91
         ],
         "y": [
          60,
          53,
          56,
          60,
          65,
          66,
          64,
          62,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          63,
          null,
          null,
          null
         ]
        },
        {
         "mode": "lines",
         "name": "C0023893",
         "type": "scatter",
         "x": [
          10,
          20,
          30,
          40,
          50,
          60,
          70,
          80,
          90,
          100,
          110,
          120,
          130,
          140,
          153,
          97,
          117,
          125,
          91
         ],
         "y": [
          3,
          2,
          2,
          2,
          2,
          2,
          3,
          3,
          2,
          2,
          null,
          null,
          null,
          null,
          null,
          null,
          3,
          null,
          null
         ]
        },
        {
         "mode": "lines",
         "name": "C0036341",
         "type": "scatter",
         "x": [
          10,
          20,
          30,
          40,
          50,
          60,
          70,
          80,
          90,
          100,
          110,
          120,
          130,
          140,
          153,
          97,
          117,
          125,
          91
         ],
         "y": [
          19,
          21,
          21,
          23,
          25,
          25,
          24,
          23,
          23,
          23,
          23,
          null,
          null,
          null,
          null,
          null,
          null,
          21,
          null
         ]
        },
        {
         "mode": "lines",
         "name": "C0376358",
         "type": "scatter",
         "x": [
          10,
          20,
          30,
          40,
          50,
          60,
          70,
          80,
          90,
          100,
          110,
          120,
          130,
          140,
          153,
          97,
          117,
          125,
          91
         ],
         "y": [
          74,
          63,
          65,
          61,
          59,
          63,
          62,
          61,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          61
         ]
        }
       ],
       "layout": {
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Validation accuracy per number of P genes"
        },
        "width": 600,
        "xaxis": {
         "title": {
          "text": "Number of P genes"
         }
        },
        "yaxis": {
         "title": {
          "text": "Validation accuracy (%)"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from turtle import width\n",
    "\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=n_positives,\n",
    "    y=C0006142,\n",
    "    mode='lines',\n",
    "    name='C0006142'\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=n_positives,\n",
    "    y=C0009402,\n",
    "    mode='lines',\n",
    "    name='C0009402'\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=n_positives,\n",
    "    y=C0023893,\n",
    "    mode='lines',\n",
    "    name='C0023893'\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=n_positives,\n",
    "    y=C0036341,\n",
    "    mode='lines',\n",
    "    name='C0036341'\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=n_positives,\n",
    "    y=C0376358,\n",
    "    mode='lines',\n",
    "    name='C0376358'\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Validation accuracy per number of P genes',\n",
    "    xaxis_title='Number of P genes',\n",
    "    yaxis_title='Validation accuracy (%)',\n",
    "    width=600\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.10 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6be87094300d2fd0b14f70b1fe8798b4c48d70f4967b41c52e40ae094b87f126"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
