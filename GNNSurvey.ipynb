{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Survey of different GNNs methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Methods in this notebook:\n",
    "- GCN\n",
    "- GAT\n",
    "- GraphConv\n",
    "- GraphSage\n",
    "- AP-GCN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Methods are compared on the multiclass classification based on the NeDBIT features from the [NIAPU paper](https://arxiv.org/pdf/2108.06158.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Packets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.11.0\n",
      "2.0.4\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "import networkx as nx\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric\n",
    "from torch_geometric.utils import from_networkx\n",
    "from torch_geometric.data import InMemoryDataset\n",
    "from torch_geometric.nn.conv import SAGEConv, GATv2Conv, GraphConv, GCNConv\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(torch.__version__)\n",
    "print(torch_geometric.__version__)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN7L_GCN (nn.Module):\n",
    "    def __init__(self, data):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(data.num_features, 16, improved=True)\n",
    "        self.conv2 = GCNConv(16, 16, improved=True)\n",
    "        self.conv3 = GCNConv(16, 16, improved=True)\n",
    "        self.conv4 = GCNConv(16, 16, improved=True)\n",
    "        self.conv5 = GCNConv(16, 16, improved=True)\n",
    "        self.conv6 = GCNConv(16, 16, improved=True)\n",
    "        self.conv7 = GCNConv(16, int(data.num_classes), improved=True)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        x = F.relu(self.conv3(x, edge_index))\n",
    "        x = F.relu(self.conv4(x, edge_index))\n",
    "        x = F.relu(self.conv5(x, edge_index))\n",
    "        x = F.relu(self.conv6(x, edge_index))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv7(x, edge_index)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "class GNN7L_GAT (nn.Module):\n",
    "    def __init__(self, data):\n",
    "        super().__init__()\n",
    "        self.conv1 = GATv2Conv(data.num_features, 16)\n",
    "        self.conv2 = GATv2Conv(16, 16)\n",
    "        self.conv3 = GATv2Conv(16, 16)\n",
    "        self.conv4 = GATv2Conv(16, 16)\n",
    "        self.conv5 = GATv2Conv(16, 16)\n",
    "        self.conv6 = GATv2Conv(16, 16)\n",
    "        self.conv7 = GATv2Conv(16, int(data.num_classes), dropout=0.3)\n",
    "    \n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        \n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        x = F.relu(self.conv3(x, edge_index))\n",
    "        x = F.relu(self.conv4(x, edge_index))\n",
    "        x = F.relu(self.conv5(x, edge_index))\n",
    "        x = F.relu(self.conv6(x, edge_index))\n",
    "        x = self.conv7(x, edge_index)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "class GNN7L_GraphConv (nn.Module):\n",
    "    def __init__(self, data):\n",
    "        super().__init__()\n",
    "        self.conv1 = GraphConv(data.num_features, 16, aggr='mean')\n",
    "        self.conv2 = GraphConv(16, 16, aggr='mean')\n",
    "        self.conv3 = GraphConv(16, 16, aggr='mean')\n",
    "        self.conv4 = GraphConv(16, 16, aggr='mean')\n",
    "        self.conv5 = GraphConv(16, 16, aggr='mean')\n",
    "        self.conv6 = GraphConv(16, 16, aggr='mean')\n",
    "        self.conv7 = GraphConv(16, int(data.num_classes), aggr='mean')\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        x = F.relu(self.conv3(x, edge_index))\n",
    "        x = F.relu(self.conv4(x, edge_index))\n",
    "        x = F.relu(self.conv5(x, edge_index))\n",
    "        x = F.relu(self.conv6(x, edge_index))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv7(x, edge_index)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "class GNN7L_Sage (nn.Module):\n",
    "    def __init__(self, data):\n",
    "        super().__init__()\n",
    "        self.conv1 = SAGEConv(data.num_features, 16)\n",
    "        self.conv2 = SAGEConv(16, 16)\n",
    "        self.conv3 = SAGEConv(16, 16)\n",
    "        self.conv4 = SAGEConv(16, 16)\n",
    "        self.conv5 = SAGEConv(16, 16)\n",
    "        self.conv6 = SAGEConv(16, 16)\n",
    "        self.conv7 = SAGEConv(16, int(data.num_classes))\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        x = F.relu(self.conv3(x, edge_index))\n",
    "        x = F.relu(self.conv4(x, edge_index))\n",
    "        x = F.relu(self.conv5(x, edge_index))\n",
    "        x = F.relu(self.conv6(x, edge_index))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv7(x, edge_index)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Loader for PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset class: it allows to translate a vector (Graph, Attributes, Labels)\n",
    "# into a dataset compatible with the PyTorch models.\n",
    "# \n",
    "# Parameters:\n",
    "# - G: NetworkX graph\n",
    "# - Labels: of the nodes used for classification\n",
    "# - attributes: List of the nodes' attributes\n",
    "\n",
    "class MyDataset(InMemoryDataset):\n",
    "  def __init__(self, G, labels, attributes, num_classes=2):\n",
    "    super(MyDataset, self).__init__('.', None, None, None)\n",
    "\n",
    "    # import data from the networkx graph with the attributes of the nodes\n",
    "    data = from_networkx(G, attributes)\n",
    "      \n",
    "    y = torch.from_numpy(labels).type(torch.long)\n",
    "\n",
    "    data.x = data.x.float()\n",
    "    data.y = y.clone().detach()\n",
    "    data.num_classes = num_classes\n",
    "\n",
    "    # Using train_test_split function from sklearn to stratify train/test/val sets\n",
    "    indices = range(G.number_of_nodes())\n",
    "    # Stratified split of train/test/val sets. Returned indices are used to create the masks\n",
    "    X_train, X_test, y_train, y_test, train_idx, test_idx = train_test_split(data.x, data.y, indices, test_size=0.3, stratify=labels, random_state=42)\n",
    "    # To create validation set, test set is splitted in half\n",
    "    X_test, X_val, y_test, y_val, test_idx, val_idx = train_test_split(X_test, y_test, test_idx, test_size=0.5, stratify=y_test, random_state=42)\n",
    "\n",
    "    n_nodes = G.number_of_nodes()\n",
    "    train_mask  = torch.zeros(n_nodes, dtype=torch.bool)\n",
    "    test_mask   = torch.zeros(n_nodes, dtype=torch.bool)\n",
    "    val_mask    = torch.zeros(n_nodes, dtype=torch.bool)\n",
    "    \n",
    "    for idx in train_idx:\n",
    "      train_mask[idx] = True\n",
    "\n",
    "    for idx in test_idx:\n",
    "      test_mask[idx] = True\n",
    "    \n",
    "    for idx in val_idx:\n",
    "      val_mask[idx] = True\n",
    "\n",
    "    data['train_mask']  = train_mask\n",
    "    data['test_mask']   = test_mask\n",
    "    data['val_mask']    = val_mask\n",
    "\n",
    "    self.data, self.slices = self.collate([data])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saves as 'best model' the model with the lower training loss. Then the metrics are computed on the best model at the end of the training epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data, epochs, lr, weight_decay, classes, model_name, cm_title):\n",
    "\n",
    "    model_names = ['GCN', 'GAT', 'GCONV', 'GSAGE']\n",
    "\n",
    "    if model_name not in model_names:\n",
    "        print('[ERR] No GNN model has been found for:', model_name)\n",
    "        return -1\n",
    "\n",
    "    title = cm_title + '_' + str(epochs) + '_' + str(weight_decay).replace('.', '_')\n",
    "\n",
    "    model_path  = 'Models/' + title\n",
    "    image_path  = 'Images/' + title\n",
    "    report_path = 'Reports/' + title + '.csv'\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    # optimizer = torch.optim.RMSprop(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    train_mask  = data['train_mask']\n",
    "    test_mask   = data['test_mask']\n",
    "    val_mask    = data['val_mask']\n",
    "\n",
    "    labels = data.y\n",
    "    output = ''\n",
    "\n",
    "    # list to plot the train accuracy\n",
    "    train_acc_curve = []\n",
    "    train_lss_curve = []\n",
    "\n",
    "    best_train_acc  = 0\n",
    "    best_val_acc    = 0\n",
    "    best_train_lss  = 999\n",
    "    best_loss_epoch = 0\n",
    "\n",
    "    for e in tqdm(range(epochs+1)):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        logits      = model(data)\n",
    "        output      = logits.argmax(1)\n",
    "        #Â train_loss  = F.cross_entropy(logits[train_mask], labels[train_mask])\n",
    "        train_loss  = F.nll_loss(logits[train_mask], labels[train_mask])\n",
    "        train_acc   = (output[train_mask] == labels[train_mask]).float().mean()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Append train acc. to plot curve later\n",
    "        train_acc_curve.append(train_acc.item())\n",
    "        train_lss_curve.append(train_loss.item())\n",
    "\n",
    "        if train_acc > best_train_acc:\n",
    "            best_train_acc = train_acc\n",
    "\n",
    "        # Evaluation and test\n",
    "        model.eval()\n",
    "        logits      = model(data)\n",
    "        output      = logits.argmax(1)\n",
    "        val_loss    = F.nll_loss(logits[val_mask], labels[val_mask])\n",
    "        val_acc     = (output[val_mask] == labels[val_mask]).float().mean()\n",
    "\n",
    "        # Update best test/val acc.\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "        \n",
    "        # Save model with best train loss\n",
    "        if train_loss < best_train_lss:\n",
    "            best_train_lss = train_loss\n",
    "            best_loss_epoch = e\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "\n",
    "        if e % 20 == 0 or e == epochs:\n",
    "            print('[Epoch: {:04d}]'.format(e),\n",
    "            'train loss: {:.4f},'.format(train_loss.item()),\n",
    "            'train acc: {:.4f},'.format(train_acc.item()),\n",
    "            'val loss: {:.4f},'.format(val_loss.item()),\n",
    "            'val acc: {:.4f} '.format(val_acc.item()),\n",
    "            '(best train acc: {:.4f},'.format(best_train_acc.item()),\n",
    "            'best val acc: {:.4f},'.format(best_val_acc.item()),\n",
    "            'best train loss: {:.4f} '.format(best_train_lss),\n",
    "            '@ epoch', best_loss_epoch ,')')\n",
    "    \n",
    "    # Plot training accuracy curve\n",
    "    plt.figure(figsize = (12,7))\n",
    "    plt.plot(train_acc_curve)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Train Accuracy')\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize = (12,7))\n",
    "    plt.plot(train_lss_curve)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Train Loss')\n",
    "    plt.show()\n",
    "\n",
    "    # Load best model\n",
    "    loaded_model = None\n",
    "    if num_layers == 2:\n",
    "        loaded_model = GNN2L_Sage(data).to(device)\n",
    "    elif num_layers == 4:\n",
    "        loaded_model = GNN4L_Sage(data).to(device)\n",
    "    elif num_layers == 7:\n",
    "        loaded_model = GNN7L_Sage(data).to(device)\n",
    "    else:\n",
    "        print('Wrong number of layer. No model available with', num_layers, 'layers')\n",
    "        return -1\n",
    "\n",
    "    loaded_model.load_state_dict(torch.load(model_path))\n",
    "    loaded_model.eval()\n",
    "    logits = loaded_model(data)\n",
    "    output = logits.argmax(1)\n",
    "\n",
    "    print(classification_report(labels[test_mask].to('cpu'), output[test_mask].to('cpu')))\n",
    "\n",
    "    class_report = classification_report(labels[test_mask].to('cpu'), output[test_mask].to('cpu'), output_dict=True)\n",
    "    classification_report_dataframe = pd.DataFrame(class_report)\n",
    "    classification_report_dataframe.to_csv(report_path)\n",
    "\n",
    "    #Confusion Matrix\n",
    "    norms = [None, \"true\"]\n",
    "    for norm in norms:\n",
    "        cm = confusion_matrix(labels[test_mask].to('cpu'), output[test_mask].to('cpu'), normalize=norm)\n",
    "\n",
    "        plt.figure(figsize=(7,7))\n",
    "        \n",
    "        if norm == \"true\":\n",
    "            sn.heatmap(cm, annot=True, fmt=\".3f\", linewidths=.5, square = True, cmap = 'BuPu', xticklabels = classes, yticklabels = classes)\n",
    "        else:\n",
    "            sn.heatmap(cm, annot=True, fmt=\".0f\", linewidths=.5, square = True, cmap = 'BuPu', xticklabels = classes, yticklabels = classes)\n",
    "        plt.title(cm_title)\n",
    "        plt.ylabel('Actual label')\n",
    "        plt.xlabel('Predicted label')\n",
    "\n",
    "        if norm == None:\n",
    "            plt.savefig(image_path + '_notNorm.png')\n",
    "        else:\n",
    "            plt.savefig(image_path + '_Norm.png')\n",
    "\n",
    "    return output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6aeea6fe202f5c7201d5940e4573c0a76b23e4e16f0e3784ac81597546f2b3b3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
