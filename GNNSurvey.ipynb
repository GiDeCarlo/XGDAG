{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Survey of different GNNs methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Methods in this notebook:\n",
    "- GCN\n",
    "- GAT\n",
    "- GraphConv\n",
    "- GraphSage\n",
    "- AP-GCN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Methods are compared on the multiclass classification based on the NeDBIT features from the [NIAPU paper](https://arxiv.org/pdf/2108.06158.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Packets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.11.0\n",
      "2.0.4\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "import networkx as nx\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric\n",
    "from torch_geometric.utils import from_networkx\n",
    "from torch_geometric.data import InMemoryDataset\n",
    "from torch_geometric.nn.conv import SAGEConv, GATv2Conv, GraphConv, GCNConv\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(torch.__version__)\n",
    "print(torch_geometric.__version__)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GNN models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN7L_GCN (nn.Module):\n",
    "    def __init__(self, data):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(data.num_features, 16, improved=True)\n",
    "        self.conv2 = GCNConv(16, 16, improved=True)\n",
    "        self.conv3 = GCNConv(16, 16, improved=True)\n",
    "        self.conv4 = GCNConv(16, 16, improved=True)\n",
    "        self.conv5 = GCNConv(16, 16, improved=True)\n",
    "        self.conv6 = GCNConv(16, 16, improved=True)\n",
    "        self.conv7 = GCNConv(16, int(data.num_classes), improved=True)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        x = F.relu(self.conv3(x, edge_index))\n",
    "        x = F.relu(self.conv4(x, edge_index))\n",
    "        x = F.relu(self.conv5(x, edge_index))\n",
    "        x = F.relu(self.conv6(x, edge_index))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv7(x, edge_index)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "class GNN7L_GAT (nn.Module):\n",
    "    def __init__(self, data):\n",
    "        super().__init__()\n",
    "        self.conv1 = GATv2Conv(data.num_features, 16)\n",
    "        self.conv2 = GATv2Conv(16, 16)\n",
    "        self.conv3 = GATv2Conv(16, 16)\n",
    "        self.conv4 = GATv2Conv(16, 16)\n",
    "        self.conv5 = GATv2Conv(16, 16)\n",
    "        self.conv6 = GATv2Conv(16, 16)\n",
    "        self.conv7 = GATv2Conv(16, int(data.num_classes), dropout=0.3)\n",
    "    \n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        \n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        x = F.relu(self.conv3(x, edge_index))\n",
    "        x = F.relu(self.conv4(x, edge_index))\n",
    "        x = F.relu(self.conv5(x, edge_index))\n",
    "        x = F.relu(self.conv6(x, edge_index))\n",
    "        x = self.conv7(x, edge_index)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "class GNN7L_GraphConv (nn.Module):\n",
    "    def __init__(self, data):\n",
    "        super().__init__()\n",
    "        self.conv1 = GraphConv(data.num_features, 16, aggr='mean')\n",
    "        self.conv2 = GraphConv(16, 16, aggr='mean')\n",
    "        self.conv3 = GraphConv(16, 16, aggr='mean')\n",
    "        self.conv4 = GraphConv(16, 16, aggr='mean')\n",
    "        self.conv5 = GraphConv(16, 16, aggr='mean')\n",
    "        self.conv6 = GraphConv(16, 16, aggr='mean')\n",
    "        self.conv7 = GraphConv(16, int(data.num_classes), aggr='mean')\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        x = F.relu(self.conv3(x, edge_index))\n",
    "        x = F.relu(self.conv4(x, edge_index))\n",
    "        x = F.relu(self.conv5(x, edge_index))\n",
    "        x = F.relu(self.conv6(x, edge_index))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv7(x, edge_index)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "class GNN7L_Sage (nn.Module):\n",
    "    def __init__(self, data):\n",
    "        super().__init__()\n",
    "        self.conv1 = SAGEConv(data.num_features, 16)\n",
    "        self.conv2 = SAGEConv(16, 16)\n",
    "        self.conv3 = SAGEConv(16, 16)\n",
    "        self.conv4 = SAGEConv(16, 16)\n",
    "        self.conv5 = SAGEConv(16, 16)\n",
    "        self.conv6 = SAGEConv(16, 16)\n",
    "        self.conv7 = SAGEConv(16, int(data.num_classes))\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        x = F.relu(self.conv3(x, edge_index))\n",
    "        x = F.relu(self.conv4(x, edge_index))\n",
    "        x = F.relu(self.conv5(x, edge_index))\n",
    "        x = F.relu(self.conv6(x, edge_index))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv7(x, edge_index)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Loader for PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset class: it allows to translate a vector (Graph, Attributes, Labels)\n",
    "# into a dataset compatible with the PyTorch models.\n",
    "# \n",
    "# Parameters:\n",
    "# - G: NetworkX graph\n",
    "# - Labels: of the nodes used for classification\n",
    "# - attributes: List of the nodes' attributes\n",
    "\n",
    "class MyDataset(InMemoryDataset):\n",
    "  def __init__(self, G, labels, attributes, num_classes=2):\n",
    "    super(MyDataset, self).__init__('.', None, None, None)\n",
    "\n",
    "    # import data from the networkx graph with the attributes of the nodes\n",
    "    data = from_networkx(G, attributes)\n",
    "      \n",
    "    y = torch.from_numpy(labels).type(torch.long)\n",
    "\n",
    "    data.x = data.x.float()\n",
    "    data.y = y.clone().detach()\n",
    "    data.num_classes = num_classes\n",
    "\n",
    "    # Using train_test_split function from sklearn to stratify train/test/val sets\n",
    "    indices = range(G.number_of_nodes())\n",
    "    # Stratified split of train/test/val sets. Returned indices are used to create the masks\n",
    "    X_train, X_test, y_train, y_test, train_idx, test_idx = train_test_split(data.x, data.y, indices, test_size=0.3, stratify=labels, random_state=42)\n",
    "    # To create validation set, test set is splitted in half\n",
    "    X_test, X_val, y_test, y_val, test_idx, val_idx = train_test_split(X_test, y_test, test_idx, test_size=0.5, stratify=y_test, random_state=42)\n",
    "\n",
    "    n_nodes = G.number_of_nodes()\n",
    "    train_mask  = torch.zeros(n_nodes, dtype=torch.bool)\n",
    "    test_mask   = torch.zeros(n_nodes, dtype=torch.bool)\n",
    "    val_mask    = torch.zeros(n_nodes, dtype=torch.bool)\n",
    "    \n",
    "    for idx in train_idx:\n",
    "      train_mask[idx] = True\n",
    "\n",
    "    for idx in test_idx:\n",
    "      test_mask[idx] = True\n",
    "    \n",
    "    for idx in val_idx:\n",
    "      val_mask[idx] = True\n",
    "\n",
    "    data['train_mask']  = train_mask\n",
    "    data['test_mask']   = test_mask\n",
    "    data['val_mask']    = val_mask\n",
    "\n",
    "    self.data, self.slices = self.collate([data])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saves as 'best model' the model with the lower training loss. Then the metrics are computed on the best model at the end of the training epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data, epochs, lr, weight_decay, classes, model_name):\n",
    "    model_names = ['GCN', 'GAT', 'GraphCONV', 'GraphSAGE']\n",
    "\n",
    "    if model_name not in model_names:\n",
    "        print('[ERR] No GNN model has been found for:', model_name)\n",
    "        return -1\n",
    "    \n",
    "    data = data.to(device)\n",
    "\n",
    "    title = model_name + '_' + str(epochs) + '_' + str(weight_decay).replace('.', '_')\n",
    "\n",
    "    model_path  = 'Models/' + title\n",
    "    image_path  = 'Images/' + title\n",
    "    report_path = 'Reports/' + title + '.csv'\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    # optimizer = torch.optim.RMSprop(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    train_mask  = data['train_mask']\n",
    "    test_mask   = data['test_mask']\n",
    "    val_mask    = data['val_mask']\n",
    "\n",
    "    labels = data.y\n",
    "    output = ''\n",
    "\n",
    "    # list to plot the train accuracy\n",
    "    train_acc_curve = []\n",
    "    train_lss_curve = []\n",
    "\n",
    "    best_train_acc  = 0\n",
    "    best_val_acc    = 0\n",
    "    best_train_lss  = 999\n",
    "    best_loss_epoch = 0\n",
    "\n",
    "    for e in tqdm(range(epochs+1)):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        logits      = model(data)\n",
    "        output      = logits.argmax(1)\n",
    "        # train_loss  = F.cross_entropy(logits[train_mask], labels[train_mask])\n",
    "        train_loss  = F.nll_loss(logits[train_mask], labels[train_mask])\n",
    "        train_acc   = (output[train_mask] == labels[train_mask]).float().mean()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Append train acc. to plot curve later\n",
    "        train_acc_curve.append(train_acc.item())\n",
    "        train_lss_curve.append(train_loss.item())\n",
    "\n",
    "        if train_acc > best_train_acc:\n",
    "            best_train_acc = train_acc\n",
    "\n",
    "        # Evaluation and test\n",
    "        model.eval()\n",
    "        logits      = model(data)\n",
    "        output      = logits.argmax(1)\n",
    "        val_loss    = F.nll_loss(logits[val_mask], labels[val_mask])\n",
    "        val_acc     = (output[val_mask] == labels[val_mask]).float().mean()\n",
    "\n",
    "        # Update best test/val acc.\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "        \n",
    "        # Save model with best train loss\n",
    "        if train_loss < best_train_lss:\n",
    "            best_train_lss = train_loss\n",
    "            best_loss_epoch = e\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "\n",
    "        if e % 20 == 0 or e == epochs:\n",
    "            print('[Epoch: {:04d}]'.format(e),\n",
    "            'train loss: {:.4f},'.format(train_loss.item()),\n",
    "            'train acc: {:.4f},'.format(train_acc.item()),\n",
    "            'val loss: {:.4f},'.format(val_loss.item()),\n",
    "            'val acc: {:.4f} '.format(val_acc.item()),\n",
    "            '(best train acc: {:.4f},'.format(best_train_acc.item()),\n",
    "            'best val acc: {:.4f},'.format(best_val_acc.item()),\n",
    "            'best train loss: {:.4f} '.format(best_train_lss),\n",
    "            '@ epoch', best_loss_epoch ,')')\n",
    "    \n",
    "    # Plot training accuracy curve\n",
    "    plt.figure(figsize = (12,7))\n",
    "    plt.plot(train_acc_curve)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Train Accuracy')\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize = (12,7))\n",
    "    plt.plot(train_lss_curve)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Train Loss')\n",
    "    plt.show()\n",
    "\n",
    "    # Load best model\n",
    "    loaded_model = None\n",
    "    if model_name == 'GCN':\n",
    "        loaded_model = GNN7L_GCN(data)\n",
    "    elif model_name == 'GAT':\n",
    "        loaded_model = GNN7L_GAT(data)\n",
    "    elif model_name == 'GraphCONV':\n",
    "        loaded_model = GNN7L_GraphConv(data)\n",
    "    else:\n",
    "        loaded_model = GNN7L_Sage(data)\n",
    "    \n",
    "    loaded_model = loaded_model.to(device)\n",
    "    loaded_model.load_state_dict(torch.load(model_path))\n",
    "    loaded_model.eval()\n",
    "    logits = loaded_model(data)\n",
    "    output = logits.argmax(1)\n",
    "\n",
    "    print(classification_report(labels[test_mask].to('cpu'), output[test_mask].to('cpu')))\n",
    "\n",
    "    class_report = classification_report(labels[test_mask].to('cpu'), output[test_mask].to('cpu'), output_dict=True)\n",
    "    classification_report_dataframe = pd.DataFrame(class_report)\n",
    "    classification_report_dataframe.to_csv(report_path)\n",
    "\n",
    "    #Confusion Matrix\n",
    "    norms = [None, \"true\"]\n",
    "    for norm in norms:\n",
    "        cm = confusion_matrix(labels[test_mask].to('cpu'), output[test_mask].to('cpu'), normalize=norm)\n",
    "\n",
    "        plt.figure(figsize=(7,7))\n",
    "        \n",
    "        if norm == \"true\":\n",
    "            sn.heatmap(cm, annot=True, fmt=\".3f\", linewidths=.5, square = True, cmap = 'BuPu', xticklabels = classes, yticklabels = classes)\n",
    "        else:\n",
    "            sn.heatmap(cm, annot=True, fmt=\".0f\", linewidths=.5, square = True, cmap = 'BuPu', xticklabels = classes, yticklabels = classes)\n",
    "        plt.title(model_name)\n",
    "        plt.ylabel('Actual label')\n",
    "        plt.xlabel('Predicted label')\n",
    "\n",
    "        if norm == None:\n",
    "            plt.savefig(image_path + '_notNorm.png')\n",
    "        else:\n",
    "            plt.savefig(image_path + '_Norm.png')\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataset from PPI graph and [NeDBIT features](https://arxiv.org/pdf/2108.06158.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "G = nx.read_gml('Graphs/graph_with_normalized_nedbit.gml')\n",
    "\n",
    "seed_genes          = pd.read_csv('Datasets/C0006142_Malignant_neoplasm_of_breast_seed_genes.txt', header=None, sep=' ')\n",
    "seed_genes.columns  = [\"name\", \"GDA Score\"]\n",
    "seeds_list          = seed_genes[\"name\"].values.tolist()\n",
    "\n",
    "nedbit_scores = pd.read_csv('Datasets/C0006142_Malignant_neoplasm_of_breast_features_Score.csv')\n",
    "\n",
    "# Remove seed genes\n",
    "nedbit_scores_not_seed = nedbit_scores[~nedbit_scores['name'].isin(seeds_list)]\n",
    "\n",
    "# Sort scores for quartile division\n",
    "nedbit_scores_not_seed = nedbit_scores_not_seed.sort_values(by = \"out\", ascending = False)\n",
    "pseudo_labels = pd.qcut(x = nedbit_scores_not_seed[\"out\"], q = 4, labels = [\"RN\", \"LN\", \"WN\", \"LP\"])\n",
    "\n",
    "nedbit_scores_not_seed['label'] = pseudo_labels\n",
    "\n",
    "nedbit_scores_seed = nedbit_scores[nedbit_scores['name'].isin(seeds_list)]\n",
    "nedbit_scores_seed = nedbit_scores_seed.assign(label = 'P')\n",
    "\n",
    "# Convert dataframe to dict for searching nodes and their labels\n",
    "not_seed_labels = dict(zip(nedbit_scores_not_seed['name'], nedbit_scores_not_seed['label']))\n",
    "seed_labels     = dict(zip(nedbit_scores_seed['name'], nedbit_scores_seed['label']))\n",
    "\n",
    "labels_dict = {'P':0, 'LP': 1, 'WN': 2, 'LN': 3, 'RN': 4}\n",
    "labels = []\n",
    "\n",
    "for node in G:\n",
    "    if node in not_seed_labels:\n",
    "        labels.append(labels_dict[not_seed_labels[node]])\n",
    "    else:\n",
    "        labels.append(labels_dict[seed_labels[node]])\n",
    "\n",
    "labels = np.asarray(labels)\n",
    "\n",
    "attributes = ['degree', 'ring', 'NetRank', 'NetShort', 'HeatDiff', 'InfoDiff']\n",
    "\n",
    "dataset_with_nedbit = MyDataset(G, labels, attributes, num_classes=5)\n",
    "data_with_nedbit = dataset_with_nedbit[0]\n",
    "\n",
    "plt.hist(labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr              = 0.001\n",
    "epochs          = 40000\n",
    "weight_decay    = 0.0005\n",
    "\n",
    "classes         = ['P', 'LP', 'WN', 'LN', 'RN']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b866e05f1f0a45698a34406367e57391",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40001 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch: 0000] train loss: 154.2524, train acc: 0.2447, val loss: 139.0947, val acc: 0.2371  (best train acc: 0.2447, best val acc: 0.2371, best train loss: 154.2524  @ epoch 0 )\n",
      "[Epoch: 0020] train loss: 28.8249, train acc: 0.2140, val loss: 24.1395, val acc: 0.2368  (best train acc: 0.2454, best val acc: 0.2371, best train loss: 28.8249  @ epoch 20 )\n",
      "[Epoch: 0040] train loss: 8.3824, train acc: 0.2384, val loss: 6.8963, val acc: 0.2371  (best train acc: 0.2572, best val acc: 0.2371, best train loss: 8.3824  @ epoch 40 )\n",
      "[Epoch: 0060] train loss: 3.5954, train acc: 0.2574, val loss: 2.8943, val acc: 0.2371  (best train acc: 0.2940, best val acc: 0.2371, best train loss: 3.5954  @ epoch 60 )\n",
      "[Epoch: 0080] train loss: 1.8750, train acc: 0.2728, val loss: 1.6885, val acc: 0.2371  (best train acc: 0.2998, best val acc: 0.2371, best train loss: 1.8346  @ epoch 78 )\n",
      "[Epoch: 0100] train loss: 1.5814, train acc: 0.2689, val loss: 1.5075, val acc: 0.2368  (best train acc: 0.3037, best val acc: 0.2371, best train loss: 1.5723  @ epoch 96 )\n",
      "[Epoch: 0120] train loss: 1.5305, train acc: 0.2953, val loss: 1.4758, val acc: 0.2398  (best train acc: 0.3040, best val acc: 0.2398, best train loss: 1.5214  @ epoch 115 )\n",
      "[Epoch: 0140] train loss: 1.5106, train acc: 0.2788, val loss: 1.4696, val acc: 0.2388  (best train acc: 0.3040, best val acc: 0.2398, best train loss: 1.5029  @ epoch 139 )\n",
      "[Epoch: 0160] train loss: 1.4955, train acc: 0.2879, val loss: 1.4619, val acc: 0.2428  (best train acc: 0.3040, best val acc: 0.2428, best train loss: 1.4894  @ epoch 159 )\n",
      "[Epoch: 0180] train loss: 1.4912, train acc: 0.2848, val loss: 1.4579, val acc: 0.2428  (best train acc: 0.3040, best val acc: 0.2442, best train loss: 1.4817  @ epoch 172 )\n",
      "[Epoch: 0200] train loss: 1.4834, train acc: 0.2955, val loss: 1.4540, val acc: 0.2492  (best train acc: 0.3040, best val acc: 0.2509, best train loss: 1.4756  @ epoch 190 )\n",
      "[Epoch: 0220] train loss: 1.4818, train acc: 0.3032, val loss: 1.4498, val acc: 0.2513  (best train acc: 0.3096, best val acc: 0.2523, best train loss: 1.4650  @ epoch 211 )\n",
      "[Epoch: 0240] train loss: 1.4664, train acc: 0.3118, val loss: 1.4473, val acc: 0.2513  (best train acc: 0.3156, best val acc: 0.2553, best train loss: 1.4648  @ epoch 231 )\n",
      "[Epoch: 0260] train loss: 1.4538, train acc: 0.3218, val loss: 1.4413, val acc: 0.2550  (best train acc: 0.3229, best val acc: 0.2914, best train loss: 1.4538  @ epoch 260 )\n",
      "[Epoch: 0280] train loss: 1.4552, train acc: 0.3190, val loss: 1.4374, val acc: 0.3255  (best train acc: 0.3285, best val acc: 0.3285, best train loss: 1.4509  @ epoch 267 )\n",
      "[Epoch: 0300] train loss: 1.4554, train acc: 0.3177, val loss: 1.4329, val acc: 0.3356  (best train acc: 0.3367, best val acc: 0.3356, best train loss: 1.4505  @ epoch 299 )\n",
      "[Epoch: 0320] train loss: 1.4491, train acc: 0.3331, val loss: 1.4283, val acc: 0.3454  (best train acc: 0.3433, best val acc: 0.3487, best train loss: 1.4375  @ epoch 315 )\n",
      "[Epoch: 0340] train loss: 1.4415, train acc: 0.3472, val loss: 1.4239, val acc: 0.3524  (best train acc: 0.3549, best val acc: 0.3673, best train loss: 1.4367  @ epoch 335 )\n",
      "[Epoch: 0360] train loss: 1.4321, train acc: 0.3486, val loss: 1.4199, val acc: 0.3545  (best train acc: 0.3568, best val acc: 0.3707, best train loss: 1.4295  @ epoch 358 )\n",
      "[Epoch: 0380] train loss: 1.4255, train acc: 0.3600, val loss: 1.4158, val acc: 0.3676  (best train acc: 0.3655, best val acc: 0.3970, best train loss: 1.4248  @ epoch 378 )\n",
      "[Epoch: 0400] train loss: 1.4218, train acc: 0.3663, val loss: 1.4100, val acc: 0.3855  (best train acc: 0.3751, best val acc: 0.3970, best train loss: 1.4212  @ epoch 395 )\n",
      "[Epoch: 0420] train loss: 1.4204, train acc: 0.3684, val loss: 1.4057, val acc: 0.3872  (best train acc: 0.3791, best val acc: 0.4020, best train loss: 1.4169  @ epoch 412 )\n",
      "[Epoch: 0440] train loss: 1.4101, train acc: 0.3691, val loss: 1.4003, val acc: 0.3906  (best train acc: 0.3819, best val acc: 0.4071, best train loss: 1.4101  @ epoch 440 )\n",
      "[Epoch: 0460] train loss: 1.4031, train acc: 0.3757, val loss: 1.3935, val acc: 0.4074  (best train acc: 0.3835, best val acc: 0.4125, best train loss: 1.4029  @ epoch 456 )\n",
      "[Epoch: 0480] train loss: 1.3980, train acc: 0.3913, val loss: 1.3865, val acc: 0.4105  (best train acc: 0.3942, best val acc: 0.4128, best train loss: 1.3948  @ epoch 478 )\n",
      "[Epoch: 0500] train loss: 1.3883, train acc: 0.4057, val loss: 1.3790, val acc: 0.4148  (best train acc: 0.4057, best val acc: 0.4229, best train loss: 1.3875  @ epoch 498 )\n",
      "[Epoch: 0520] train loss: 1.3758, train acc: 0.4030, val loss: 1.3713, val acc: 0.4202  (best train acc: 0.4125, best val acc: 0.4310, best train loss: 1.3758  @ epoch 520 )\n",
      "[Epoch: 0540] train loss: 1.3695, train acc: 0.4003, val loss: 1.3628, val acc: 0.4256  (best train acc: 0.4286, best val acc: 0.4492, best train loss: 1.3695  @ epoch 540 )\n",
      "[Epoch: 0560] train loss: 1.3608, train acc: 0.4142, val loss: 1.3507, val acc: 0.4435  (best train acc: 0.4291, best val acc: 0.4492, best train loss: 1.3576  @ epoch 558 )\n",
      "[Epoch: 0580] train loss: 1.3441, train acc: 0.4263, val loss: 1.3387, val acc: 0.4401  (best train acc: 0.4321, best val acc: 0.4492, best train loss: 1.3441  @ epoch 580 )\n",
      "[Epoch: 0600] train loss: 1.3398, train acc: 0.4273, val loss: 1.3265, val acc: 0.4492  (best train acc: 0.4346, best val acc: 0.4536, best train loss: 1.3389  @ epoch 599 )\n",
      "[Epoch: 0620] train loss: 1.3456, train acc: 0.4414, val loss: 1.3266, val acc: 0.4583  (best train acc: 0.4414, best val acc: 0.4621, best train loss: 1.3373  @ epoch 617 )\n",
      "[Epoch: 0640] train loss: 1.3300, train acc: 0.4354, val loss: 1.3110, val acc: 0.4533  (best train acc: 0.4493, best val acc: 0.4658, best train loss: 1.3275  @ epoch 639 )\n",
      "[Epoch: 0660] train loss: 1.3232, train acc: 0.4447, val loss: 1.3188, val acc: 0.4513  (best train acc: 0.4505, best val acc: 0.4708, best train loss: 1.3209  @ epoch 657 )\n",
      "[Epoch: 0680] train loss: 1.3185, train acc: 0.4250, val loss: 1.2998, val acc: 0.4320  (best train acc: 0.4555, best val acc: 0.4708, best train loss: 1.3185  @ epoch 680 )\n",
      "[Epoch: 0700] train loss: 1.3206, train acc: 0.4338, val loss: 1.2963, val acc: 0.4452  (best train acc: 0.4555, best val acc: 0.4708, best train loss: 1.3095  @ epoch 692 )\n",
      "[Epoch: 0720] train loss: 1.3075, train acc: 0.4354, val loss: 1.2910, val acc: 0.4459  (best train acc: 0.4572, best val acc: 0.4708, best train loss: 1.3075  @ epoch 720 )\n",
      "[Epoch: 0740] train loss: 1.3013, train acc: 0.4450, val loss: 1.2766, val acc: 0.4587  (best train acc: 0.4625, best val acc: 0.4715, best train loss: 1.2959  @ epoch 738 )\n",
      "[Epoch: 0760] train loss: 1.3443, train acc: 0.4131, val loss: 1.3360, val acc: 0.3875  (best train acc: 0.4625, best val acc: 0.4715, best train loss: 1.2959  @ epoch 738 )\n",
      "[Epoch: 0780] train loss: 1.3049, train acc: 0.4356, val loss: 1.2788, val acc: 0.4401  (best train acc: 0.4625, best val acc: 0.4715, best train loss: 1.2959  @ epoch 738 )\n",
      "[Epoch: 0800] train loss: 1.3016, train acc: 0.4586, val loss: 1.2745, val acc: 0.4594  (best train acc: 0.4625, best val acc: 0.4715, best train loss: 1.2934  @ epoch 796 )\n",
      "[Epoch: 0820] train loss: 1.3074, train acc: 0.4477, val loss: 1.2827, val acc: 0.4408  (best train acc: 0.4631, best val acc: 0.4752, best train loss: 1.2934  @ epoch 796 )\n",
      "[Epoch: 0840] train loss: 1.2836, train acc: 0.4545, val loss: 1.2660, val acc: 0.4506  (best train acc: 0.4631, best val acc: 0.4752, best train loss: 1.2836  @ epoch 840 )\n",
      "[Epoch: 0860] train loss: 1.2846, train acc: 0.4633, val loss: 1.2599, val acc: 0.4398  (best train acc: 0.4633, best val acc: 0.4752, best train loss: 1.2826  @ epoch 858 )\n",
      "[Epoch: 0880] train loss: 1.2828, train acc: 0.4529, val loss: 1.2554, val acc: 0.4516  (best train acc: 0.4633, best val acc: 0.4752, best train loss: 1.2810  @ epoch 879 )\n",
      "[Epoch: 0900] train loss: 1.2762, train acc: 0.4572, val loss: 1.2526, val acc: 0.4644  (best train acc: 0.4633, best val acc: 0.4752, best train loss: 1.2758  @ epoch 897 )\n",
      "[Epoch: 0920] train loss: 1.2741, train acc: 0.4578, val loss: 1.2495, val acc: 0.4634  (best train acc: 0.4633, best val acc: 0.4752, best train loss: 1.2741  @ epoch 920 )\n",
      "[Epoch: 0940] train loss: 1.2805, train acc: 0.4602, val loss: 1.2470, val acc: 0.4621  (best train acc: 0.4730, best val acc: 0.4752, best train loss: 1.2710  @ epoch 939 )\n",
      "[Epoch: 0960] train loss: 1.2759, train acc: 0.4592, val loss: 1.2472, val acc: 0.4550  (best train acc: 0.4730, best val acc: 0.4850, best train loss: 1.2710  @ epoch 939 )\n",
      "[Epoch: 0980] train loss: 1.2715, train acc: 0.4607, val loss: 1.2447, val acc: 0.4583  (best train acc: 0.4743, best val acc: 0.4857, best train loss: 1.2710  @ epoch 939 )\n",
      "[Epoch: 1000] train loss: 1.2749, train acc: 0.4626, val loss: 1.2479, val acc: 0.4654  (best train acc: 0.4743, best val acc: 0.4857, best train loss: 1.2670  @ epoch 997 )\n",
      "[Epoch: 1020] train loss: 1.2762, train acc: 0.4576, val loss: 1.2422, val acc: 0.4678  (best train acc: 0.4743, best val acc: 0.4857, best train loss: 1.2621  @ epoch 1014 )\n",
      "[Epoch: 1040] train loss: 1.2654, train acc: 0.4636, val loss: 1.2372, val acc: 0.4782  (best train acc: 0.4743, best val acc: 0.4867, best train loss: 1.2621  @ epoch 1014 )\n",
      "[Epoch: 1060] train loss: 1.2712, train acc: 0.4700, val loss: 1.2469, val acc: 0.4803  (best train acc: 0.4824, best val acc: 0.4944, best train loss: 1.2621  @ epoch 1014 )\n",
      "[Epoch: 1080] train loss: 1.2589, train acc: 0.4748, val loss: 1.2350, val acc: 0.4833  (best train acc: 0.4824, best val acc: 0.4944, best train loss: 1.2589  @ epoch 1080 )\n",
      "[Epoch: 1100] train loss: 1.2669, train acc: 0.4913, val loss: 1.2385, val acc: 0.4968  (best train acc: 0.4913, best val acc: 0.4968, best train loss: 1.2589  @ epoch 1080 )\n",
      "[Epoch: 1120] train loss: 1.2621, train acc: 0.4707, val loss: 1.2335, val acc: 0.4853  (best train acc: 0.4913, best val acc: 0.4968, best train loss: 1.2589  @ epoch 1080 )\n",
      "[Epoch: 1140] train loss: 1.2696, train acc: 0.4641, val loss: 1.2355, val acc: 0.4671  (best train acc: 0.4913, best val acc: 0.4968, best train loss: 1.2546  @ epoch 1124 )\n",
      "[Epoch: 1160] train loss: 1.2578, train acc: 0.4752, val loss: 1.2311, val acc: 0.4833  (best train acc: 0.4913, best val acc: 0.4968, best train loss: 1.2546  @ epoch 1124 )\n",
      "[Epoch: 1180] train loss: 1.2595, train acc: 0.4633, val loss: 1.2293, val acc: 0.4894  (best train acc: 0.4913, best val acc: 0.4968, best train loss: 1.2532  @ epoch 1162 )\n",
      "[Epoch: 1200] train loss: 1.2521, train acc: 0.4730, val loss: 1.2275, val acc: 0.4857  (best train acc: 0.4913, best val acc: 0.4968, best train loss: 1.2521  @ epoch 1200 )\n",
      "[Epoch: 1220] train loss: 1.3183, train acc: 0.4324, val loss: 1.3058, val acc: 0.4384  (best train acc: 0.4913, best val acc: 0.4968, best train loss: 1.2503  @ epoch 1210 )\n",
      "[Epoch: 1240] train loss: 1.2611, train acc: 0.4684, val loss: 1.2298, val acc: 0.4789  (best train acc: 0.4913, best val acc: 0.4988, best train loss: 1.2503  @ epoch 1210 )\n",
      "[Epoch: 1260] train loss: 1.2609, train acc: 0.4651, val loss: 1.2266, val acc: 0.4749  (best train acc: 0.4913, best val acc: 0.4988, best train loss: 1.2494  @ epoch 1254 )\n",
      "[Epoch: 1280] train loss: 1.2857, train acc: 0.4639, val loss: 1.2730, val acc: 0.4685  (best train acc: 0.4913, best val acc: 0.4988, best train loss: 1.2489  @ epoch 1261 )\n",
      "[Epoch: 1300] train loss: 1.2602, train acc: 0.4757, val loss: 1.2273, val acc: 0.4863  (best train acc: 0.4913, best val acc: 0.5005, best train loss: 1.2489  @ epoch 1261 )\n",
      "[Epoch: 1320] train loss: 1.2512, train acc: 0.4740, val loss: 1.2233, val acc: 0.4755  (best train acc: 0.4913, best val acc: 0.5005, best train loss: 1.2453  @ epoch 1317 )\n",
      "[Epoch: 1340] train loss: 1.2462, train acc: 0.4772, val loss: 1.2222, val acc: 0.4796  (best train acc: 0.4913, best val acc: 0.5005, best train loss: 1.2442  @ epoch 1338 )\n",
      "[Epoch: 1360] train loss: 1.2445, train acc: 0.4776, val loss: 1.2212, val acc: 0.4843  (best train acc: 0.4913, best val acc: 0.5005, best train loss: 1.2421  @ epoch 1346 )\n",
      "[Epoch: 1380] train loss: 1.2438, train acc: 0.4803, val loss: 1.2204, val acc: 0.4830  (best train acc: 0.4913, best val acc: 0.5005, best train loss: 1.2415  @ epoch 1374 )\n",
      "[Epoch: 1400] train loss: 1.2418, train acc: 0.4793, val loss: 1.2193, val acc: 0.4924  (best train acc: 0.4913, best val acc: 0.5005, best train loss: 1.2394  @ epoch 1384 )\n",
      "[Epoch: 1420] train loss: 1.2419, train acc: 0.4797, val loss: 1.2192, val acc: 0.4958  (best train acc: 0.4913, best val acc: 0.5005, best train loss: 1.2394  @ epoch 1384 )\n",
      "[Epoch: 1440] train loss: 1.2464, train acc: 0.4816, val loss: 1.2182, val acc: 0.4975  (best train acc: 0.4913, best val acc: 0.5005, best train loss: 1.2394  @ epoch 1384 )\n",
      "[Epoch: 1460] train loss: 1.2534, train acc: 0.4657, val loss: 1.2244, val acc: 0.4681  (best train acc: 0.4913, best val acc: 0.5005, best train loss: 1.2388  @ epoch 1458 )\n",
      "[Epoch: 1480] train loss: 1.2474, train acc: 0.4679, val loss: 1.2181, val acc: 0.4833  (best train acc: 0.4939, best val acc: 0.5029, best train loss: 1.2335  @ epoch 1477 )\n",
      "[Epoch: 1500] train loss: 1.2392, train acc: 0.4808, val loss: 1.2158, val acc: 0.4911  (best train acc: 0.4939, best val acc: 0.5029, best train loss: 1.2325  @ epoch 1490 )\n",
      "[Epoch: 1520] train loss: 1.2442, train acc: 0.4863, val loss: 1.2155, val acc: 0.4843  (best train acc: 0.4939, best val acc: 0.5029, best train loss: 1.2325  @ epoch 1490 )\n",
      "[Epoch: 1540] train loss: 1.2332, train acc: 0.4806, val loss: 1.2149, val acc: 0.4840  (best train acc: 0.4939, best val acc: 0.5029, best train loss: 1.2325  @ epoch 1490 )\n",
      "[Epoch: 1560] train loss: 1.2439, train acc: 0.4641, val loss: 1.2185, val acc: 0.4745  (best train acc: 0.4939, best val acc: 0.5029, best train loss: 1.2325  @ epoch 1490 )\n",
      "[Epoch: 1580] train loss: 1.2387, train acc: 0.4902, val loss: 1.2131, val acc: 0.4887  (best train acc: 0.4939, best val acc: 0.5029, best train loss: 1.2300  @ epoch 1571 )\n",
      "[Epoch: 1600] train loss: 1.2328, train acc: 0.4854, val loss: 1.2125, val acc: 0.4911  (best train acc: 0.4939, best val acc: 0.5029, best train loss: 1.2300  @ epoch 1571 )\n",
      "[Epoch: 1620] train loss: 1.2398, train acc: 0.4748, val loss: 1.2119, val acc: 0.4867  (best train acc: 0.4939, best val acc: 0.5029, best train loss: 1.2284  @ epoch 1611 )\n",
      "[Epoch: 1640] train loss: 1.2383, train acc: 0.4738, val loss: 1.2129, val acc: 0.4776  (best train acc: 0.4939, best val acc: 0.5029, best train loss: 1.2284  @ epoch 1611 )\n",
      "[Epoch: 1660] train loss: 1.2382, train acc: 0.4798, val loss: 1.2138, val acc: 0.4847  (best train acc: 0.4986, best val acc: 0.5029, best train loss: 1.2266  @ epoch 1642 )\n",
      "[Epoch: 1680] train loss: 1.2324, train acc: 0.4800, val loss: 1.2107, val acc: 0.4843  (best train acc: 0.4986, best val acc: 0.5029, best train loss: 1.2253  @ epoch 1673 )\n",
      "[Epoch: 1700] train loss: 1.2337, train acc: 0.4779, val loss: 1.2098, val acc: 0.4917  (best train acc: 0.4986, best val acc: 0.5029, best train loss: 1.2253  @ epoch 1673 )\n",
      "[Epoch: 1720] train loss: 1.2306, train acc: 0.4816, val loss: 1.2096, val acc: 0.4850  (best train acc: 0.4986, best val acc: 0.5029, best train loss: 1.2235  @ epoch 1706 )\n",
      "[Epoch: 1740] train loss: 1.2264, train acc: 0.4890, val loss: 1.2084, val acc: 0.4944  (best train acc: 0.4986, best val acc: 0.5029, best train loss: 1.2235  @ epoch 1706 )\n",
      "[Epoch: 1760] train loss: 1.2274, train acc: 0.4810, val loss: 1.2077, val acc: 0.4850  (best train acc: 0.4986, best val acc: 0.5029, best train loss: 1.2232  @ epoch 1757 )\n",
      "[Epoch: 1780] train loss: 1.2327, train acc: 0.4809, val loss: 1.2075, val acc: 0.4863  (best train acc: 0.4986, best val acc: 0.5029, best train loss: 1.2221  @ epoch 1764 )\n",
      "[Epoch: 1800] train loss: 1.2195, train acc: 0.4859, val loss: 1.2064, val acc: 0.4914  (best train acc: 0.4986, best val acc: 0.5029, best train loss: 1.2195  @ epoch 1800 )\n",
      "[Epoch: 1820] train loss: 1.2321, train acc: 0.4850, val loss: 1.2061, val acc: 0.4863  (best train acc: 0.4986, best val acc: 0.5029, best train loss: 1.2195  @ epoch 1800 )\n",
      "[Epoch: 1840] train loss: 1.2223, train acc: 0.4890, val loss: 1.2053, val acc: 0.4874  (best train acc: 0.4986, best val acc: 0.5029, best train loss: 1.2185  @ epoch 1824 )\n",
      "[Epoch: 1860] train loss: 1.2218, train acc: 0.4852, val loss: 1.2049, val acc: 0.4877  (best train acc: 0.4986, best val acc: 0.5029, best train loss: 1.2185  @ epoch 1824 )\n",
      "[Epoch: 1880] train loss: 1.2210, train acc: 0.4949, val loss: 1.2041, val acc: 0.4975  (best train acc: 0.4995, best val acc: 0.5029, best train loss: 1.2154  @ epoch 1874 )\n",
      "[Epoch: 1900] train loss: 1.2239, train acc: 0.4863, val loss: 1.2038, val acc: 0.4840  (best train acc: 0.4995, best val acc: 0.5029, best train loss: 1.2154  @ epoch 1874 )\n",
      "[Epoch: 1920] train loss: 1.2247, train acc: 0.4836, val loss: 1.2042, val acc: 0.4995  (best train acc: 0.4995, best val acc: 0.5029, best train loss: 1.2154  @ epoch 1874 )\n",
      "[Epoch: 1940] train loss: 1.2152, train acc: 0.4936, val loss: 1.2021, val acc: 0.4901  (best train acc: 0.4995, best val acc: 0.5029, best train loss: 1.2145  @ epoch 1928 )\n",
      "[Epoch: 1960] train loss: 1.2187, train acc: 0.4874, val loss: 1.2021, val acc: 0.4853  (best train acc: 0.4995, best val acc: 0.5029, best train loss: 1.2133  @ epoch 1957 )\n",
      "[Epoch: 1980] train loss: 1.2221, train acc: 0.4837, val loss: 1.2025, val acc: 0.4847  (best train acc: 0.4995, best val acc: 0.5029, best train loss: 1.2133  @ epoch 1957 )\n",
      "[Epoch: 2000] train loss: 1.2163, train acc: 0.4800, val loss: 1.2008, val acc: 0.4934  (best train acc: 0.4995, best val acc: 0.5029, best train loss: 1.2106  @ epoch 1981 )\n",
      "[Epoch: 2020] train loss: 1.2183, train acc: 0.4855, val loss: 1.1998, val acc: 0.4911  (best train acc: 0.4995, best val acc: 0.5029, best train loss: 1.2106  @ epoch 1981 )\n",
      "[Epoch: 2040] train loss: 1.2227, train acc: 0.4847, val loss: 1.2011, val acc: 0.4847  (best train acc: 0.4995, best val acc: 0.5029, best train loss: 1.2106  @ epoch 1981 )\n",
      "[Epoch: 2060] train loss: 1.2185, train acc: 0.4920, val loss: 1.2004, val acc: 0.4897  (best train acc: 0.4995, best val acc: 0.5029, best train loss: 1.2106  @ epoch 1981 )\n",
      "[Epoch: 2080] train loss: 1.2162, train acc: 0.4903, val loss: 1.1999, val acc: 0.4911  (best train acc: 0.4995, best val acc: 0.5029, best train loss: 1.2106  @ epoch 1981 )\n",
      "[Epoch: 2100] train loss: 1.2205, train acc: 0.4899, val loss: 1.1986, val acc: 0.4954  (best train acc: 0.4995, best val acc: 0.5029, best train loss: 1.2106  @ epoch 1981 )\n",
      "[Epoch: 2120] train loss: 1.2178, train acc: 0.4904, val loss: 1.1977, val acc: 0.4884  (best train acc: 0.4995, best val acc: 0.5029, best train loss: 1.2055  @ epoch 2112 )\n",
      "[Epoch: 2140] train loss: 1.2176, train acc: 0.4918, val loss: 1.1980, val acc: 0.4938  (best train acc: 0.4995, best val acc: 0.5029, best train loss: 1.2055  @ epoch 2112 )\n",
      "[Epoch: 2160] train loss: 1.2111, train acc: 0.4916, val loss: 1.1972, val acc: 0.4985  (best train acc: 0.4995, best val acc: 0.5073, best train loss: 1.2055  @ epoch 2112 )\n",
      "[Epoch: 2180] train loss: 1.2118, train acc: 0.4858, val loss: 1.1956, val acc: 0.4897  (best train acc: 0.4995, best val acc: 0.5073, best train loss: 1.2053  @ epoch 2179 )\n",
      "[Epoch: 2200] train loss: 1.2090, train acc: 0.4910, val loss: 1.1954, val acc: 0.4968  (best train acc: 0.4995, best val acc: 0.5073, best train loss: 1.2043  @ epoch 2199 )\n",
      "[Epoch: 2220] train loss: 1.2115, train acc: 0.4839, val loss: 1.1950, val acc: 0.4954  (best train acc: 0.4995, best val acc: 0.5073, best train loss: 1.2043  @ epoch 2199 )\n",
      "[Epoch: 2240] train loss: 1.2109, train acc: 0.4886, val loss: 1.1941, val acc: 0.4927  (best train acc: 0.4995, best val acc: 0.5073, best train loss: 1.2043  @ epoch 2199 )\n",
      "[Epoch: 2260] train loss: 1.2085, train acc: 0.4897, val loss: 1.1938, val acc: 0.4907  (best train acc: 0.4995, best val acc: 0.5073, best train loss: 1.2012  @ epoch 2244 )\n",
      "[Epoch: 2280] train loss: 1.2097, train acc: 0.4907, val loss: 1.1932, val acc: 0.4934  (best train acc: 0.4995, best val acc: 0.5073, best train loss: 1.2004  @ epoch 2266 )\n",
      "[Epoch: 2300] train loss: 1.2076, train acc: 0.4954, val loss: 1.1929, val acc: 0.4897  (best train acc: 0.4995, best val acc: 0.5073, best train loss: 1.2004  @ epoch 2266 )\n",
      "[Epoch: 2320] train loss: 1.2053, train acc: 0.4917, val loss: 1.1923, val acc: 0.4927  (best train acc: 0.4995, best val acc: 0.5073, best train loss: 1.2004  @ epoch 2266 )\n",
      "[Epoch: 2340] train loss: 1.2078, train acc: 0.4840, val loss: 1.1918, val acc: 0.4954  (best train acc: 0.5008, best val acc: 0.5073, best train loss: 1.2004  @ epoch 2266 )\n",
      "[Epoch: 2360] train loss: 1.2111, train acc: 0.4927, val loss: 1.1918, val acc: 0.4934  (best train acc: 0.5017, best val acc: 0.5073, best train loss: 1.2002  @ epoch 2341 )\n",
      "[Epoch: 2380] train loss: 1.2119, train acc: 0.4951, val loss: 1.1913, val acc: 0.4924  (best train acc: 0.5017, best val acc: 0.5073, best train loss: 1.1991  @ epoch 2363 )\n",
      "[Epoch: 2400] train loss: 1.2061, train acc: 0.4873, val loss: 1.1903, val acc: 0.4938  (best train acc: 0.5017, best val acc: 0.5073, best train loss: 1.1965  @ epoch 2382 )\n",
      "[Epoch: 2420] train loss: 1.2108, train acc: 0.4900, val loss: 1.1899, val acc: 0.4941  (best train acc: 0.5017, best val acc: 0.5073, best train loss: 1.1960  @ epoch 2418 )\n",
      "[Epoch: 2440] train loss: 1.2016, train acc: 0.4939, val loss: 1.1895, val acc: 0.4975  (best train acc: 0.5017, best val acc: 0.5073, best train loss: 1.1960  @ epoch 2418 )\n",
      "[Epoch: 2460] train loss: 1.2006, train acc: 0.4944, val loss: 1.1890, val acc: 0.4907  (best train acc: 0.5017, best val acc: 0.5073, best train loss: 1.1931  @ epoch 2450 )\n",
      "[Epoch: 2480] train loss: 1.1897, train acc: 0.4999, val loss: 1.1867, val acc: 0.5005  (best train acc: 0.5059, best val acc: 0.5073, best train loss: 1.1850  @ epoch 2477 )\n",
      "[Epoch: 2500] train loss: 1.2005, train acc: 0.4907, val loss: 1.1850, val acc: 0.4938  (best train acc: 0.5059, best val acc: 0.5073, best train loss: 1.1850  @ epoch 2477 )\n",
      "[Epoch: 2520] train loss: 1.1925, train acc: 0.4986, val loss: 1.1845, val acc: 0.4944  (best train acc: 0.5059, best val acc: 0.5073, best train loss: 1.1850  @ epoch 2477 )\n",
      "[Epoch: 2540] train loss: 1.1965, train acc: 0.4906, val loss: 1.1833, val acc: 0.4958  (best train acc: 0.5059, best val acc: 0.5073, best train loss: 1.1850  @ epoch 2477 )\n",
      "[Epoch: 2560] train loss: 1.1913, train acc: 0.5010, val loss: 1.1832, val acc: 0.4958  (best train acc: 0.5059, best val acc: 0.5073, best train loss: 1.1850  @ epoch 2477 )\n",
      "[Epoch: 2580] train loss: 1.1917, train acc: 0.4903, val loss: 1.1821, val acc: 0.4978  (best train acc: 0.5059, best val acc: 0.5073, best train loss: 1.1850  @ epoch 2477 )\n",
      "[Epoch: 2600] train loss: 1.1932, train acc: 0.4954, val loss: 1.1821, val acc: 0.4941  (best train acc: 0.5059, best val acc: 0.5073, best train loss: 1.1848  @ epoch 2591 )\n",
      "[Epoch: 2620] train loss: 1.1941, train acc: 0.4978, val loss: 1.1812, val acc: 0.4998  (best train acc: 0.5059, best val acc: 0.5073, best train loss: 1.1848  @ epoch 2591 )\n",
      "[Epoch: 2640] train loss: 1.1926, train acc: 0.4967, val loss: 1.1803, val acc: 0.4978  (best train acc: 0.5059, best val acc: 0.5073, best train loss: 1.1824  @ epoch 2621 )\n",
      "[Epoch: 2660] train loss: 1.1878, train acc: 0.4966, val loss: 1.1799, val acc: 0.4988  (best train acc: 0.5059, best val acc: 0.5073, best train loss: 1.1824  @ epoch 2621 )\n",
      "[Epoch: 2680] train loss: 1.1882, train acc: 0.4936, val loss: 1.1793, val acc: 0.4985  (best train acc: 0.5059, best val acc: 0.5073, best train loss: 1.1824  @ epoch 2621 )\n",
      "[Epoch: 2700] train loss: 1.1901, train acc: 0.4961, val loss: 1.1794, val acc: 0.4924  (best train acc: 0.5059, best val acc: 0.5073, best train loss: 1.1806  @ epoch 2683 )\n",
      "[Epoch: 2720] train loss: 1.1869, train acc: 0.5019, val loss: 1.1795, val acc: 0.4978  (best train acc: 0.5064, best val acc: 0.5073, best train loss: 1.1806  @ epoch 2683 )\n",
      "[Epoch: 2740] train loss: 1.1868, train acc: 0.4993, val loss: 1.1782, val acc: 0.5002  (best train acc: 0.5064, best val acc: 0.5073, best train loss: 1.1806  @ epoch 2683 )\n",
      "[Epoch: 2760] train loss: 1.1838, train acc: 0.4988, val loss: 1.1788, val acc: 0.5042  (best train acc: 0.5064, best val acc: 0.5073, best train loss: 1.1806  @ epoch 2683 )\n",
      "[Epoch: 2780] train loss: 1.1920, train acc: 0.4932, val loss: 1.1773, val acc: 0.4981  (best train acc: 0.5064, best val acc: 0.5073, best train loss: 1.1805  @ epoch 2767 )\n",
      "[Epoch: 2800] train loss: 1.1826, train acc: 0.4975, val loss: 1.1772, val acc: 0.5008  (best train acc: 0.5064, best val acc: 0.5073, best train loss: 1.1795  @ epoch 2788 )\n",
      "[Epoch: 2820] train loss: 1.1850, train acc: 0.4979, val loss: 1.1766, val acc: 0.5032  (best train acc: 0.5064, best val acc: 0.5073, best train loss: 1.1795  @ epoch 2788 )\n",
      "[Epoch: 2840] train loss: 1.1931, train acc: 0.4940, val loss: 1.1772, val acc: 0.5005  (best train acc: 0.5064, best val acc: 0.5073, best train loss: 1.1790  @ epoch 2831 )\n",
      "[Epoch: 2860] train loss: 1.1911, train acc: 0.4949, val loss: 1.1767, val acc: 0.5046  (best train acc: 0.5064, best val acc: 0.5073, best train loss: 1.1790  @ epoch 2831 )\n",
      "[Epoch: 2880] train loss: 1.1830, train acc: 0.4949, val loss: 1.1757, val acc: 0.4944  (best train acc: 0.5064, best val acc: 0.5073, best train loss: 1.1729  @ epoch 2865 )\n",
      "[Epoch: 2900] train loss: 1.1822, train acc: 0.4923, val loss: 1.1761, val acc: 0.4958  (best train acc: 0.5064, best val acc: 0.5073, best train loss: 1.1729  @ epoch 2865 )\n",
      "[Epoch: 2920] train loss: 1.1857, train acc: 0.5030, val loss: 1.1758, val acc: 0.4931  (best train acc: 0.5065, best val acc: 0.5073, best train loss: 1.1729  @ epoch 2865 )\n",
      "[Epoch: 2940] train loss: 1.1781, train acc: 0.5014, val loss: 1.1744, val acc: 0.5025  (best train acc: 0.5065, best val acc: 0.5073, best train loss: 1.1729  @ epoch 2865 )\n",
      "[Epoch: 2960] train loss: 1.1788, train acc: 0.5022, val loss: 1.1736, val acc: 0.5022  (best train acc: 0.5065, best val acc: 0.5073, best train loss: 1.1725  @ epoch 2953 )\n",
      "[Epoch: 2980] train loss: 1.1810, train acc: 0.4968, val loss: 1.1739, val acc: 0.5012  (best train acc: 0.5065, best val acc: 0.5073, best train loss: 1.1723  @ epoch 2965 )\n",
      "[Epoch: 3000] train loss: 1.1780, train acc: 0.5059, val loss: 1.1733, val acc: 0.5062  (best train acc: 0.5096, best val acc: 0.5073, best train loss: 1.1723  @ epoch 2965 )\n",
      "[Epoch: 3020] train loss: 1.1736, train acc: 0.5030, val loss: 1.1725, val acc: 0.5008  (best train acc: 0.5096, best val acc: 0.5079, best train loss: 1.1685  @ epoch 3013 )\n",
      "[Epoch: 3040] train loss: 1.1796, train acc: 0.4996, val loss: 1.1724, val acc: 0.5022  (best train acc: 0.5096, best val acc: 0.5079, best train loss: 1.1685  @ epoch 3013 )\n",
      "[Epoch: 3060] train loss: 1.1746, train acc: 0.5051, val loss: 1.1724, val acc: 0.5035  (best train acc: 0.5131, best val acc: 0.5113, best train loss: 1.1685  @ epoch 3013 )\n",
      "[Epoch: 3080] train loss: 1.1853, train acc: 0.5045, val loss: 1.1748, val acc: 0.4961  (best train acc: 0.5131, best val acc: 0.5116, best train loss: 1.1685  @ epoch 3013 )\n",
      "[Epoch: 3100] train loss: 1.1824, train acc: 0.4956, val loss: 1.1732, val acc: 0.5093  (best train acc: 0.5131, best val acc: 0.5130, best train loss: 1.1685  @ epoch 3013 )\n",
      "[Epoch: 3120] train loss: 1.1782, train acc: 0.4979, val loss: 1.1724, val acc: 0.5025  (best train acc: 0.5131, best val acc: 0.5130, best train loss: 1.1685  @ epoch 3013 )\n",
      "[Epoch: 3140] train loss: 1.1717, train acc: 0.5053, val loss: 1.1717, val acc: 0.5005  (best train acc: 0.5131, best val acc: 0.5130, best train loss: 1.1685  @ epoch 3013 )\n",
      "[Epoch: 3160] train loss: 1.1663, train acc: 0.5111, val loss: 1.1699, val acc: 0.5062  (best train acc: 0.5131, best val acc: 0.5130, best train loss: 1.1652  @ epoch 3145 )\n",
      "[Epoch: 3180] train loss: 1.1736, train acc: 0.4983, val loss: 1.1693, val acc: 0.5019  (best train acc: 0.5131, best val acc: 0.5130, best train loss: 1.1652  @ epoch 3145 )\n",
      "[Epoch: 3200] train loss: 1.1734, train acc: 0.5009, val loss: 1.1715, val acc: 0.5029  (best train acc: 0.5131, best val acc: 0.5130, best train loss: 1.1652  @ epoch 3145 )\n",
      "[Epoch: 3220] train loss: 1.1666, train acc: 0.5053, val loss: 1.1696, val acc: 0.5086  (best train acc: 0.5131, best val acc: 0.5130, best train loss: 1.1652  @ epoch 3145 )\n",
      "[Epoch: 3240] train loss: 1.1737, train acc: 0.5035, val loss: 1.1688, val acc: 0.5052  (best train acc: 0.5134, best val acc: 0.5130, best train loss: 1.1652  @ epoch 3145 )\n",
      "[Epoch: 3260] train loss: 1.1670, train acc: 0.5108, val loss: 1.1723, val acc: 0.5147  (best train acc: 0.5134, best val acc: 0.5147, best train loss: 1.1644  @ epoch 3246 )\n",
      "[Epoch: 3280] train loss: 1.1690, train acc: 0.5026, val loss: 1.1692, val acc: 0.4975  (best train acc: 0.5134, best val acc: 0.5147, best train loss: 1.1644  @ epoch 3246 )\n",
      "[Epoch: 3300] train loss: 1.1699, train acc: 0.5100, val loss: 1.1687, val acc: 0.5062  (best train acc: 0.5134, best val acc: 0.5147, best train loss: 1.1623  @ epoch 3299 )\n",
      "[Epoch: 3320] train loss: 1.1645, train acc: 0.5115, val loss: 1.1670, val acc: 0.5056  (best train acc: 0.5134, best val acc: 0.5147, best train loss: 1.1622  @ epoch 3317 )\n",
      "[Epoch: 3340] train loss: 1.1698, train acc: 0.5157, val loss: 1.1674, val acc: 0.5089  (best train acc: 0.5157, best val acc: 0.5147, best train loss: 1.1622  @ epoch 3317 )\n",
      "[Epoch: 3360] train loss: 1.1681, train acc: 0.5022, val loss: 1.1691, val acc: 0.5029  (best train acc: 0.5157, best val acc: 0.5147, best train loss: 1.1620  @ epoch 3343 )\n",
      "[Epoch: 3380] train loss: 1.1681, train acc: 0.5074, val loss: 1.1668, val acc: 0.5052  (best train acc: 0.5157, best val acc: 0.5147, best train loss: 1.1620  @ epoch 3343 )\n",
      "[Epoch: 3400] train loss: 1.1717, train acc: 0.5061, val loss: 1.1661, val acc: 0.5052  (best train acc: 0.5157, best val acc: 0.5147, best train loss: 1.1620  @ epoch 3343 )\n",
      "[Epoch: 3420] train loss: 1.1737, train acc: 0.5016, val loss: 1.1669, val acc: 0.5052  (best train acc: 0.5157, best val acc: 0.5147, best train loss: 1.1620  @ epoch 3343 )\n",
      "[Epoch: 3440] train loss: 1.1699, train acc: 0.5068, val loss: 1.1672, val acc: 0.5130  (best train acc: 0.5157, best val acc: 0.5147, best train loss: 1.1620  @ epoch 3343 )\n",
      "[Epoch: 3460] train loss: 1.1679, train acc: 0.5103, val loss: 1.1654, val acc: 0.5093  (best train acc: 0.5158, best val acc: 0.5147, best train loss: 1.1606  @ epoch 3452 )\n",
      "[Epoch: 3480] train loss: 1.1640, train acc: 0.5128, val loss: 1.1683, val acc: 0.5022  (best train acc: 0.5192, best val acc: 0.5147, best train loss: 1.1600  @ epoch 3472 )\n",
      "[Epoch: 3500] train loss: 1.1611, train acc: 0.5089, val loss: 1.1669, val acc: 0.5019  (best train acc: 0.5192, best val acc: 0.5150, best train loss: 1.1600  @ epoch 3472 )\n",
      "[Epoch: 3520] train loss: 1.1675, train acc: 0.5119, val loss: 1.1653, val acc: 0.5120  (best train acc: 0.5192, best val acc: 0.5150, best train loss: 1.1600  @ epoch 3472 )\n",
      "[Epoch: 3540] train loss: 1.1618, train acc: 0.5101, val loss: 1.1649, val acc: 0.5086  (best train acc: 0.5192, best val acc: 0.5150, best train loss: 1.1583  @ epoch 3535 )\n",
      "[Epoch: 3560] train loss: 1.1679, train acc: 0.5152, val loss: 1.1643, val acc: 0.5110  (best train acc: 0.5192, best val acc: 0.5150, best train loss: 1.1580  @ epoch 3553 )\n",
      "[Epoch: 3580] train loss: 1.1670, train acc: 0.5137, val loss: 1.1639, val acc: 0.5056  (best train acc: 0.5192, best val acc: 0.5150, best train loss: 1.1557  @ epoch 3562 )\n",
      "[Epoch: 3600] train loss: 1.1631, train acc: 0.5128, val loss: 1.1641, val acc: 0.5099  (best train acc: 0.5192, best val acc: 0.5157, best train loss: 1.1557  @ epoch 3562 )\n",
      "[Epoch: 3620] train loss: 1.1561, train acc: 0.5145, val loss: 1.1643, val acc: 0.5049  (best train acc: 0.5192, best val acc: 0.5157, best train loss: 1.1557  @ epoch 3562 )\n",
      "[Epoch: 3640] train loss: 1.1665, train acc: 0.5064, val loss: 1.1631, val acc: 0.5052  (best train acc: 0.5192, best val acc: 0.5157, best train loss: 1.1557  @ epoch 3562 )\n",
      "[Epoch: 3660] train loss: 1.1671, train acc: 0.5040, val loss: 1.1639, val acc: 0.5099  (best train acc: 0.5192, best val acc: 0.5157, best train loss: 1.1557  @ epoch 3562 )\n",
      "[Epoch: 3680] train loss: 1.1543, train acc: 0.5142, val loss: 1.1626, val acc: 0.5089  (best train acc: 0.5192, best val acc: 0.5157, best train loss: 1.1543  @ epoch 3680 )\n",
      "[Epoch: 3700] train loss: 1.1653, train acc: 0.4987, val loss: 1.1634, val acc: 0.5039  (best train acc: 0.5192, best val acc: 0.5157, best train loss: 1.1543  @ epoch 3680 )\n",
      "[Epoch: 3720] train loss: 1.1677, train acc: 0.5064, val loss: 1.1645, val acc: 0.5150  (best train acc: 0.5192, best val acc: 0.5157, best train loss: 1.1543  @ epoch 3680 )\n",
      "[Epoch: 3740] train loss: 1.1637, train acc: 0.5080, val loss: 1.1630, val acc: 0.5076  (best train acc: 0.5192, best val acc: 0.5174, best train loss: 1.1543  @ epoch 3680 )\n",
      "[Epoch: 3760] train loss: 1.1761, train acc: 0.4970, val loss: 1.1651, val acc: 0.5035  (best train acc: 0.5207, best val acc: 0.5174, best train loss: 1.1543  @ epoch 3680 )\n",
      "[Epoch: 3780] train loss: 1.1629, train acc: 0.5105, val loss: 1.1649, val acc: 0.5025  (best train acc: 0.5207, best val acc: 0.5174, best train loss: 1.1543  @ epoch 3680 )\n",
      "[Epoch: 3800] train loss: 1.1626, train acc: 0.5123, val loss: 1.1615, val acc: 0.5069  (best train acc: 0.5207, best val acc: 0.5174, best train loss: 1.1543  @ epoch 3680 )\n",
      "[Epoch: 3820] train loss: 1.1698, train acc: 0.5077, val loss: 1.1695, val acc: 0.5073  (best train acc: 0.5207, best val acc: 0.5174, best train loss: 1.1543  @ epoch 3680 )\n",
      "[Epoch: 3840] train loss: 1.1812, train acc: 0.5139, val loss: 1.1749, val acc: 0.5019  (best train acc: 0.5207, best val acc: 0.5174, best train loss: 1.1541  @ epoch 3839 )\n",
      "[Epoch: 3860] train loss: 1.2141, train acc: 0.4727, val loss: 1.1766, val acc: 0.5073  (best train acc: 0.5207, best val acc: 0.5174, best train loss: 1.1541  @ epoch 3839 )\n",
      "[Epoch: 3880] train loss: 1.1676, train acc: 0.5104, val loss: 1.1697, val acc: 0.5079  (best train acc: 0.5207, best val acc: 0.5174, best train loss: 1.1541  @ epoch 3839 )\n",
      "[Epoch: 3900] train loss: 1.1576, train acc: 0.5144, val loss: 1.1659, val acc: 0.5066  (best train acc: 0.5207, best val acc: 0.5174, best train loss: 1.1541  @ epoch 3839 )\n",
      "[Epoch: 3920] train loss: 1.1629, train acc: 0.5072, val loss: 1.1637, val acc: 0.5046  (best train acc: 0.5207, best val acc: 0.5174, best train loss: 1.1541  @ epoch 3839 )\n",
      "[Epoch: 3940] train loss: 1.1598, train acc: 0.5072, val loss: 1.1630, val acc: 0.5056  (best train acc: 0.5207, best val acc: 0.5174, best train loss: 1.1534  @ epoch 3925 )\n",
      "[Epoch: 3960] train loss: 1.1670, train acc: 0.5076, val loss: 1.1623, val acc: 0.5052  (best train acc: 0.5207, best val acc: 0.5174, best train loss: 1.1534  @ epoch 3925 )\n",
      "[Epoch: 3980] train loss: 1.1585, train acc: 0.5093, val loss: 1.1617, val acc: 0.5106  (best train acc: 0.5207, best val acc: 0.5174, best train loss: 1.1497  @ epoch 3973 )\n",
      "[Epoch: 4000] train loss: 1.1732, train acc: 0.5075, val loss: 1.1607, val acc: 0.5079  (best train acc: 0.5207, best val acc: 0.5174, best train loss: 1.1497  @ epoch 3973 )\n",
      "[Epoch: 4020] train loss: 1.2114, train acc: 0.5117, val loss: 1.2045, val acc: 0.5099  (best train acc: 0.5207, best val acc: 0.5174, best train loss: 1.1497  @ epoch 3973 )\n",
      "[Epoch: 4040] train loss: 1.1597, train acc: 0.5205, val loss: 1.1660, val acc: 0.5123  (best train acc: 0.5207, best val acc: 0.5174, best train loss: 1.1497  @ epoch 3973 )\n",
      "[Epoch: 4060] train loss: 1.2113, train acc: 0.4933, val loss: 1.2073, val acc: 0.5025  (best train acc: 0.5207, best val acc: 0.5174, best train loss: 1.1497  @ epoch 3973 )\n",
      "[Epoch: 4080] train loss: 1.1691, train acc: 0.5171, val loss: 1.1706, val acc: 0.5143  (best train acc: 0.5207, best val acc: 0.5194, best train loss: 1.1497  @ epoch 3973 )\n",
      "[Epoch: 4100] train loss: 1.1679, train acc: 0.5065, val loss: 1.1671, val acc: 0.5073  (best train acc: 0.5207, best val acc: 0.5194, best train loss: 1.1497  @ epoch 3973 )\n",
      "[Epoch: 4120] train loss: 1.1590, train acc: 0.5112, val loss: 1.1632, val acc: 0.5137  (best train acc: 0.5207, best val acc: 0.5194, best train loss: 1.1497  @ epoch 3973 )\n",
      "[Epoch: 4140] train loss: 1.1608, train acc: 0.5083, val loss: 1.1621, val acc: 0.5096  (best train acc: 0.5207, best val acc: 0.5194, best train loss: 1.1497  @ epoch 3973 )\n",
      "[Epoch: 4160] train loss: 1.1632, train acc: 0.5085, val loss: 1.1612, val acc: 0.5110  (best train acc: 0.5207, best val acc: 0.5194, best train loss: 1.1497  @ epoch 3973 )\n",
      "[Epoch: 4180] train loss: 1.1538, train acc: 0.5152, val loss: 1.1609, val acc: 0.5099  (best train acc: 0.5207, best val acc: 0.5194, best train loss: 1.1497  @ epoch 3973 )\n",
      "[Epoch: 4200] train loss: 1.1550, train acc: 0.5197, val loss: 1.1606, val acc: 0.5103  (best train acc: 0.5207, best val acc: 0.5194, best train loss: 1.1497  @ epoch 3973 )\n",
      "[Epoch: 4220] train loss: 1.1582, train acc: 0.5175, val loss: 1.1599, val acc: 0.5096  (best train acc: 0.5207, best val acc: 0.5194, best train loss: 1.1497  @ epoch 3973 )\n",
      "[Epoch: 4240] train loss: 1.1568, train acc: 0.5134, val loss: 1.1595, val acc: 0.5103  (best train acc: 0.5210, best val acc: 0.5194, best train loss: 1.1497  @ epoch 3973 )\n",
      "[Epoch: 4260] train loss: 1.1647, train acc: 0.5082, val loss: 1.1592, val acc: 0.5123  (best train acc: 0.5210, best val acc: 0.5194, best train loss: 1.1497  @ epoch 3973 )\n",
      "[Epoch: 4280] train loss: 1.2097, train acc: 0.4935, val loss: 1.2101, val acc: 0.4985  (best train acc: 0.5210, best val acc: 0.5194, best train loss: 1.1497  @ epoch 3973 )\n",
      "[Epoch: 4300] train loss: 1.1759, train acc: 0.5098, val loss: 1.1782, val acc: 0.5137  (best train acc: 0.5210, best val acc: 0.5194, best train loss: 1.1497  @ epoch 3973 )\n",
      "[Epoch: 4320] train loss: 1.1603, train acc: 0.5093, val loss: 1.1607, val acc: 0.5167  (best train acc: 0.5210, best val acc: 0.5194, best train loss: 1.1497  @ epoch 3973 )\n",
      "[Epoch: 4340] train loss: 1.1599, train acc: 0.5103, val loss: 1.1597, val acc: 0.5079  (best train acc: 0.5241, best val acc: 0.5194, best train loss: 1.1497  @ epoch 3973 )\n",
      "[Epoch: 4360] train loss: 1.1571, train acc: 0.5055, val loss: 1.1593, val acc: 0.5110  (best train acc: 0.5241, best val acc: 0.5194, best train loss: 1.1497  @ epoch 3973 )\n",
      "[Epoch: 4380] train loss: 1.1567, train acc: 0.5162, val loss: 1.1592, val acc: 0.5143  (best train acc: 0.5241, best val acc: 0.5201, best train loss: 1.1497  @ epoch 3973 )\n",
      "[Epoch: 4400] train loss: 1.1542, train acc: 0.5167, val loss: 1.1599, val acc: 0.5126  (best train acc: 0.5241, best val acc: 0.5201, best train loss: 1.1497  @ epoch 3973 )\n",
      "[Epoch: 4420] train loss: 1.1627, train acc: 0.5082, val loss: 1.1591, val acc: 0.5103  (best train acc: 0.5253, best val acc: 0.5201, best train loss: 1.1495  @ epoch 4407 )\n",
      "[Epoch: 4440] train loss: 1.1536, train acc: 0.5084, val loss: 1.1572, val acc: 0.5083  (best train acc: 0.5253, best val acc: 0.5201, best train loss: 1.1495  @ epoch 4407 )\n",
      "[Epoch: 4460] train loss: 1.1559, train acc: 0.5134, val loss: 1.1565, val acc: 0.5143  (best train acc: 0.5253, best val acc: 0.5201, best train loss: 1.1495  @ epoch 4407 )\n",
      "[Epoch: 4480] train loss: 1.1563, train acc: 0.5155, val loss: 1.1571, val acc: 0.5130  (best train acc: 0.5253, best val acc: 0.5201, best train loss: 1.1494  @ epoch 4478 )\n",
      "[Epoch: 4500] train loss: 1.1557, train acc: 0.5107, val loss: 1.1559, val acc: 0.5130  (best train acc: 0.5253, best val acc: 0.5201, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 4520] train loss: 1.1770, train acc: 0.5098, val loss: 1.1840, val acc: 0.5079  (best train acc: 0.5253, best val acc: 0.5201, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 4540] train loss: 1.2508, train acc: 0.4833, val loss: 1.2459, val acc: 0.4954  (best train acc: 0.5253, best val acc: 0.5201, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 4560] train loss: 1.6906, train acc: 0.4152, val loss: 1.5656, val acc: 0.3841  (best train acc: 0.5253, best val acc: 0.5201, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 4580] train loss: 1.2378, train acc: 0.4944, val loss: 1.2223, val acc: 0.5042  (best train acc: 0.5253, best val acc: 0.5201, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 4600] train loss: 1.2301, train acc: 0.4926, val loss: 1.2268, val acc: 0.4978  (best train acc: 0.5253, best val acc: 0.5201, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 4620] train loss: 1.2189, train acc: 0.4892, val loss: 1.2221, val acc: 0.4927  (best train acc: 0.5253, best val acc: 0.5201, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 4640] train loss: 1.2033, train acc: 0.5015, val loss: 1.2145, val acc: 0.4992  (best train acc: 0.5253, best val acc: 0.5201, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 4660] train loss: 1.2045, train acc: 0.5033, val loss: 1.2071, val acc: 0.4985  (best train acc: 0.5253, best val acc: 0.5201, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 4680] train loss: 1.1959, train acc: 0.4998, val loss: 1.2000, val acc: 0.5002  (best train acc: 0.5253, best val acc: 0.5201, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 4700] train loss: 1.1906, train acc: 0.5015, val loss: 1.1972, val acc: 0.4988  (best train acc: 0.5253, best val acc: 0.5201, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 4720] train loss: 1.1856, train acc: 0.5082, val loss: 1.1959, val acc: 0.5019  (best train acc: 0.5253, best val acc: 0.5201, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 4740] train loss: 1.1883, train acc: 0.5104, val loss: 1.1951, val acc: 0.5042  (best train acc: 0.5253, best val acc: 0.5201, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 4760] train loss: 1.1864, train acc: 0.5057, val loss: 1.1940, val acc: 0.4981  (best train acc: 0.5253, best val acc: 0.5201, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 4780] train loss: 1.1924, train acc: 0.5009, val loss: 1.1913, val acc: 0.5008  (best train acc: 0.5253, best val acc: 0.5201, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 4800] train loss: 1.1909, train acc: 0.5046, val loss: 1.1899, val acc: 0.5025  (best train acc: 0.5253, best val acc: 0.5201, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 4820] train loss: 1.1828, train acc: 0.5151, val loss: 1.1891, val acc: 0.5066  (best train acc: 0.5253, best val acc: 0.5201, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 4840] train loss: 1.1812, train acc: 0.5071, val loss: 1.1911, val acc: 0.4961  (best train acc: 0.5253, best val acc: 0.5201, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 4860] train loss: 1.1817, train acc: 0.5061, val loss: 1.1872, val acc: 0.5073  (best train acc: 0.5253, best val acc: 0.5201, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 4880] train loss: 1.1845, train acc: 0.5058, val loss: 1.1862, val acc: 0.5042  (best train acc: 0.5253, best val acc: 0.5201, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 4900] train loss: 1.1803, train acc: 0.5121, val loss: 1.1878, val acc: 0.5093  (best train acc: 0.5253, best val acc: 0.5201, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 4920] train loss: 1.1852, train acc: 0.5112, val loss: 1.1861, val acc: 0.4998  (best train acc: 0.5253, best val acc: 0.5201, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 4940] train loss: 1.1817, train acc: 0.5045, val loss: 1.1844, val acc: 0.5083  (best train acc: 0.5253, best val acc: 0.5201, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 4960] train loss: 1.1851, train acc: 0.5050, val loss: 1.1838, val acc: 0.5073  (best train acc: 0.5253, best val acc: 0.5201, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 4980] train loss: 1.1843, train acc: 0.5118, val loss: 1.1832, val acc: 0.5056  (best train acc: 0.5253, best val acc: 0.5201, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 5000] train loss: 1.1753, train acc: 0.5161, val loss: 1.1825, val acc: 0.5029  (best train acc: 0.5253, best val acc: 0.5201, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 5020] train loss: 1.1758, train acc: 0.5102, val loss: 1.1822, val acc: 0.5066  (best train acc: 0.5253, best val acc: 0.5201, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 5040] train loss: 1.1838, train acc: 0.5150, val loss: 1.1822, val acc: 0.5032  (best train acc: 0.5253, best val acc: 0.5201, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 5060] train loss: 1.1797, train acc: 0.5064, val loss: 1.1817, val acc: 0.5110  (best train acc: 0.5253, best val acc: 0.5201, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 5080] train loss: 1.1747, train acc: 0.5134, val loss: 1.1826, val acc: 0.5110  (best train acc: 0.5253, best val acc: 0.5201, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 5100] train loss: 1.1773, train acc: 0.5067, val loss: 1.1803, val acc: 0.5035  (best train acc: 0.5253, best val acc: 0.5201, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 5120] train loss: 1.1745, train acc: 0.5156, val loss: 1.1796, val acc: 0.5056  (best train acc: 0.5253, best val acc: 0.5201, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 5140] train loss: 1.1796, train acc: 0.5028, val loss: 1.1793, val acc: 0.5076  (best train acc: 0.5253, best val acc: 0.5201, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 5160] train loss: 1.1752, train acc: 0.5007, val loss: 1.1790, val acc: 0.5066  (best train acc: 0.5253, best val acc: 0.5201, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 5180] train loss: 1.1709, train acc: 0.5144, val loss: 1.1795, val acc: 0.5056  (best train acc: 0.5253, best val acc: 0.5201, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 5200] train loss: 1.1733, train acc: 0.5074, val loss: 1.1777, val acc: 0.5076  (best train acc: 0.5253, best val acc: 0.5201, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 5220] train loss: 1.1741, train acc: 0.5127, val loss: 1.1788, val acc: 0.5066  (best train acc: 0.5253, best val acc: 0.5201, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 5240] train loss: 1.1691, train acc: 0.5151, val loss: 1.1778, val acc: 0.5110  (best train acc: 0.5253, best val acc: 0.5201, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 5260] train loss: 1.1771, train acc: 0.5085, val loss: 1.1766, val acc: 0.5093  (best train acc: 0.5253, best val acc: 0.5201, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 5280] train loss: 1.1706, train acc: 0.5148, val loss: 1.1792, val acc: 0.5137  (best train acc: 0.5253, best val acc: 0.5201, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 5300] train loss: 1.1686, train acc: 0.5187, val loss: 1.1762, val acc: 0.5076  (best train acc: 0.5253, best val acc: 0.5201, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 5320] train loss: 1.1756, train acc: 0.5115, val loss: 1.1756, val acc: 0.5059  (best train acc: 0.5253, best val acc: 0.5201, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 5340] train loss: 1.1796, train acc: 0.5010, val loss: 1.1797, val acc: 0.5170  (best train acc: 0.5253, best val acc: 0.5201, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 5360] train loss: 1.1767, train acc: 0.5173, val loss: 1.1771, val acc: 0.5130  (best train acc: 0.5253, best val acc: 0.5201, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 5380] train loss: 1.1724, train acc: 0.5215, val loss: 1.1752, val acc: 0.5093  (best train acc: 0.5253, best val acc: 0.5201, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 5400] train loss: 1.1747, train acc: 0.5134, val loss: 1.1748, val acc: 0.5066  (best train acc: 0.5253, best val acc: 0.5201, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 5420] train loss: 1.1730, train acc: 0.5192, val loss: 1.1752, val acc: 0.5096  (best train acc: 0.5253, best val acc: 0.5201, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 5440] train loss: 1.1798, train acc: 0.5176, val loss: 1.1833, val acc: 0.5147  (best train acc: 0.5253, best val acc: 0.5201, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 5460] train loss: 1.1840, train acc: 0.5147, val loss: 1.1767, val acc: 0.5157  (best train acc: 0.5265, best val acc: 0.5201, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 5480] train loss: 1.1739, train acc: 0.5187, val loss: 1.1783, val acc: 0.5153  (best train acc: 0.5265, best val acc: 0.5201, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 5500] train loss: 1.1658, train acc: 0.5261, val loss: 1.1747, val acc: 0.5150  (best train acc: 0.5265, best val acc: 0.5201, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 5520] train loss: 1.1790, train acc: 0.5138, val loss: 1.1739, val acc: 0.5143  (best train acc: 0.5265, best val acc: 0.5201, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 5540] train loss: 1.1730, train acc: 0.5187, val loss: 1.1751, val acc: 0.5069  (best train acc: 0.5265, best val acc: 0.5201, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 5560] train loss: 1.1631, train acc: 0.5181, val loss: 1.1736, val acc: 0.5083  (best train acc: 0.5265, best val acc: 0.5201, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 5580] train loss: 1.1654, train acc: 0.5216, val loss: 1.1728, val acc: 0.5123  (best train acc: 0.5265, best val acc: 0.5201, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 5600] train loss: 1.1739, train acc: 0.5082, val loss: 1.1726, val acc: 0.5160  (best train acc: 0.5265, best val acc: 0.5201, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 5620] train loss: 1.1701, train acc: 0.5151, val loss: 1.1721, val acc: 0.5076  (best train acc: 0.5265, best val acc: 0.5201, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 5640] train loss: 1.1734, train acc: 0.5146, val loss: 1.1715, val acc: 0.5069  (best train acc: 0.5265, best val acc: 0.5201, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 5660] train loss: 1.1669, train acc: 0.5218, val loss: 1.1740, val acc: 0.5093  (best train acc: 0.5265, best val acc: 0.5201, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 5680] train loss: 1.1667, train acc: 0.5172, val loss: 1.1708, val acc: 0.5073  (best train acc: 0.5265, best val acc: 0.5201, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 5700] train loss: 1.1677, train acc: 0.5212, val loss: 1.1708, val acc: 0.5130  (best train acc: 0.5265, best val acc: 0.5204, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 5720] train loss: 1.1742, train acc: 0.5136, val loss: 1.1704, val acc: 0.5113  (best train acc: 0.5265, best val acc: 0.5204, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 5740] train loss: 1.1732, train acc: 0.5154, val loss: 1.1704, val acc: 0.5113  (best train acc: 0.5265, best val acc: 0.5204, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 5760] train loss: 1.1668, train acc: 0.5121, val loss: 1.1697, val acc: 0.5143  (best train acc: 0.5265, best val acc: 0.5204, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 5780] train loss: 1.1622, train acc: 0.5184, val loss: 1.1694, val acc: 0.5079  (best train acc: 0.5265, best val acc: 0.5204, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 5800] train loss: 1.1717, train acc: 0.5160, val loss: 1.1699, val acc: 0.5143  (best train acc: 0.5265, best val acc: 0.5204, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 5820] train loss: 1.1891, train acc: 0.5098, val loss: 1.1703, val acc: 0.5177  (best train acc: 0.5265, best val acc: 0.5204, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 5840] train loss: 1.1783, train acc: 0.5095, val loss: 1.1720, val acc: 0.5177  (best train acc: 0.5265, best val acc: 0.5204, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 5860] train loss: 1.1685, train acc: 0.5162, val loss: 1.1685, val acc: 0.5147  (best train acc: 0.5265, best val acc: 0.5204, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 5880] train loss: 1.1628, train acc: 0.5122, val loss: 1.1719, val acc: 0.5191  (best train acc: 0.5265, best val acc: 0.5211, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 5900] train loss: 1.1630, train acc: 0.5132, val loss: 1.1708, val acc: 0.5187  (best train acc: 0.5296, best val acc: 0.5211, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 5920] train loss: 1.1703, train acc: 0.5169, val loss: 1.1679, val acc: 0.5184  (best train acc: 0.5296, best val acc: 0.5214, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 5940] train loss: 1.1656, train acc: 0.5129, val loss: 1.1709, val acc: 0.5184  (best train acc: 0.5296, best val acc: 0.5214, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 5960] train loss: 1.1628, train acc: 0.5192, val loss: 1.1670, val acc: 0.5150  (best train acc: 0.5296, best val acc: 0.5214, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 5980] train loss: 1.1655, train acc: 0.5193, val loss: 1.1678, val acc: 0.5184  (best train acc: 0.5296, best val acc: 0.5214, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 6000] train loss: 1.1659, train acc: 0.5205, val loss: 1.1662, val acc: 0.5153  (best train acc: 0.5296, best val acc: 0.5214, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 6020] train loss: 1.1565, train acc: 0.5171, val loss: 1.1659, val acc: 0.5167  (best train acc: 0.5296, best val acc: 0.5214, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 6040] train loss: 1.1921, train acc: 0.5070, val loss: 1.2314, val acc: 0.4840  (best train acc: 0.5304, best val acc: 0.5221, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 6060] train loss: 1.1850, train acc: 0.5014, val loss: 1.1934, val acc: 0.4931  (best train acc: 0.5304, best val acc: 0.5221, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 6080] train loss: 1.1773, train acc: 0.5090, val loss: 1.1795, val acc: 0.5052  (best train acc: 0.5304, best val acc: 0.5221, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 6100] train loss: 1.1808, train acc: 0.5089, val loss: 1.1774, val acc: 0.5029  (best train acc: 0.5304, best val acc: 0.5221, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 6120] train loss: 1.1712, train acc: 0.5080, val loss: 1.1774, val acc: 0.5022  (best train acc: 0.5304, best val acc: 0.5221, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 6140] train loss: 1.1722, train acc: 0.5174, val loss: 1.1772, val acc: 0.5086  (best train acc: 0.5304, best val acc: 0.5221, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 6160] train loss: 1.1696, train acc: 0.5155, val loss: 1.1755, val acc: 0.5029  (best train acc: 0.5304, best val acc: 0.5221, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 6180] train loss: 1.1624, train acc: 0.5150, val loss: 1.1751, val acc: 0.5096  (best train acc: 0.5304, best val acc: 0.5221, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 6200] train loss: 1.1691, train acc: 0.5153, val loss: 1.1743, val acc: 0.5052  (best train acc: 0.5304, best val acc: 0.5221, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 6220] train loss: 1.1711, train acc: 0.5069, val loss: 1.1733, val acc: 0.5035  (best train acc: 0.5304, best val acc: 0.5221, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 6240] train loss: 1.1578, train acc: 0.5171, val loss: 1.1727, val acc: 0.5093  (best train acc: 0.5304, best val acc: 0.5221, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 6260] train loss: 1.1604, train acc: 0.5215, val loss: 1.1718, val acc: 0.5059  (best train acc: 0.5304, best val acc: 0.5221, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 6280] train loss: 1.1689, train acc: 0.5142, val loss: 1.1702, val acc: 0.5062  (best train acc: 0.5304, best val acc: 0.5221, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 6300] train loss: 1.1916, train acc: 0.5060, val loss: 1.1841, val acc: 0.5073  (best train acc: 0.5304, best val acc: 0.5221, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 6320] train loss: 1.1778, train acc: 0.5131, val loss: 1.1757, val acc: 0.5083  (best train acc: 0.5304, best val acc: 0.5221, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 6340] train loss: 1.1643, train acc: 0.5101, val loss: 1.1716, val acc: 0.5052  (best train acc: 0.5304, best val acc: 0.5221, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 6360] train loss: 1.1663, train acc: 0.5145, val loss: 1.1688, val acc: 0.5039  (best train acc: 0.5304, best val acc: 0.5221, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 6380] train loss: 1.1910, train acc: 0.5061, val loss: 1.1760, val acc: 0.5096  (best train acc: 0.5304, best val acc: 0.5221, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 6400] train loss: 1.1663, train acc: 0.5083, val loss: 1.1761, val acc: 0.4965  (best train acc: 0.5304, best val acc: 0.5221, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 6420] train loss: 1.1776, train acc: 0.5099, val loss: 1.1717, val acc: 0.5066  (best train acc: 0.5304, best val acc: 0.5221, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 6440] train loss: 1.1735, train acc: 0.5096, val loss: 1.1710, val acc: 0.5076  (best train acc: 0.5304, best val acc: 0.5221, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 6460] train loss: 1.1654, train acc: 0.5183, val loss: 1.1706, val acc: 0.5056  (best train acc: 0.5304, best val acc: 0.5221, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 6480] train loss: 1.1596, train acc: 0.5130, val loss: 1.1697, val acc: 0.5103  (best train acc: 0.5304, best val acc: 0.5221, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 6500] train loss: 1.1618, train acc: 0.5122, val loss: 1.1695, val acc: 0.5069  (best train acc: 0.5304, best val acc: 0.5221, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 6520] train loss: 1.1634, train acc: 0.5146, val loss: 1.1686, val acc: 0.5106  (best train acc: 0.5304, best val acc: 0.5221, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 6540] train loss: 1.1604, train acc: 0.5179, val loss: 1.1682, val acc: 0.5120  (best train acc: 0.5304, best val acc: 0.5221, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 6560] train loss: 1.1629, train acc: 0.5180, val loss: 1.1676, val acc: 0.5120  (best train acc: 0.5304, best val acc: 0.5221, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 6580] train loss: 1.1614, train acc: 0.5154, val loss: 1.1669, val acc: 0.5103  (best train acc: 0.5304, best val acc: 0.5221, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 6600] train loss: 1.1593, train acc: 0.5209, val loss: 1.1669, val acc: 0.5099  (best train acc: 0.5304, best val acc: 0.5221, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 6620] train loss: 1.1589, train acc: 0.5211, val loss: 1.1659, val acc: 0.5140  (best train acc: 0.5304, best val acc: 0.5221, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 6640] train loss: 1.1703, train acc: 0.5121, val loss: 1.1651, val acc: 0.5130  (best train acc: 0.5304, best val acc: 0.5221, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 6660] train loss: 1.1648, train acc: 0.5173, val loss: 1.1642, val acc: 0.5113  (best train acc: 0.5304, best val acc: 0.5221, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 6680] train loss: 1.1612, train acc: 0.5163, val loss: 1.1630, val acc: 0.5120  (best train acc: 0.5304, best val acc: 0.5221, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 6700] train loss: 1.1886, train acc: 0.4743, val loss: 1.2463, val acc: 0.4621  (best train acc: 0.5304, best val acc: 0.5221, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 6720] train loss: 1.1826, train acc: 0.5134, val loss: 1.1852, val acc: 0.4988  (best train acc: 0.5304, best val acc: 0.5221, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 6740] train loss: 1.1677, train acc: 0.5194, val loss: 1.1761, val acc: 0.5143  (best train acc: 0.5304, best val acc: 0.5221, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 6760] train loss: 1.1639, train acc: 0.5244, val loss: 1.1734, val acc: 0.5147  (best train acc: 0.5304, best val acc: 0.5221, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 6780] train loss: 1.1653, train acc: 0.5184, val loss: 1.1717, val acc: 0.5164  (best train acc: 0.5304, best val acc: 0.5221, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 6800] train loss: 1.1589, train acc: 0.5214, val loss: 1.1706, val acc: 0.5187  (best train acc: 0.5304, best val acc: 0.5221, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 6820] train loss: 1.1669, train acc: 0.5182, val loss: 1.1696, val acc: 0.5191  (best train acc: 0.5304, best val acc: 0.5221, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 6840] train loss: 1.1731, train acc: 0.5124, val loss: 1.1696, val acc: 0.5177  (best train acc: 0.5304, best val acc: 0.5221, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 6860] train loss: 1.1639, train acc: 0.5219, val loss: 1.1691, val acc: 0.5221  (best train acc: 0.5304, best val acc: 0.5228, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 6880] train loss: 1.1542, train acc: 0.5294, val loss: 1.1684, val acc: 0.5201  (best train acc: 0.5305, best val acc: 0.5228, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 6900] train loss: 1.1673, train acc: 0.5198, val loss: 1.1687, val acc: 0.5174  (best train acc: 0.5305, best val acc: 0.5228, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 6920] train loss: 1.1562, train acc: 0.5262, val loss: 1.1675, val acc: 0.5187  (best train acc: 0.5337, best val acc: 0.5245, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 6940] train loss: 1.1601, train acc: 0.5251, val loss: 1.1675, val acc: 0.5214  (best train acc: 0.5337, best val acc: 0.5245, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 6960] train loss: 1.1557, train acc: 0.5248, val loss: 1.1665, val acc: 0.5214  (best train acc: 0.5337, best val acc: 0.5245, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 6980] train loss: 1.1501, train acc: 0.5310, val loss: 1.1661, val acc: 0.5207  (best train acc: 0.5337, best val acc: 0.5245, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 7000] train loss: 1.1532, train acc: 0.5285, val loss: 1.1657, val acc: 0.5187  (best train acc: 0.5337, best val acc: 0.5245, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 7020] train loss: 1.1608, train acc: 0.5228, val loss: 1.1652, val acc: 0.5231  (best train acc: 0.5337, best val acc: 0.5245, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 7040] train loss: 1.1622, train acc: 0.5230, val loss: 1.1649, val acc: 0.5201  (best train acc: 0.5337, best val acc: 0.5245, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 7060] train loss: 1.1561, train acc: 0.5250, val loss: 1.1646, val acc: 0.5231  (best train acc: 0.5337, best val acc: 0.5245, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 7080] train loss: 1.1533, train acc: 0.5275, val loss: 1.1639, val acc: 0.5214  (best train acc: 0.5337, best val acc: 0.5251, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 7100] train loss: 1.1496, train acc: 0.5280, val loss: 1.1638, val acc: 0.5197  (best train acc: 0.5337, best val acc: 0.5251, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 7120] train loss: 1.1557, train acc: 0.5296, val loss: 1.1633, val acc: 0.5201  (best train acc: 0.5341, best val acc: 0.5251, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 7140] train loss: 1.1640, train acc: 0.5215, val loss: 1.1631, val acc: 0.5228  (best train acc: 0.5341, best val acc: 0.5258, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 7160] train loss: 1.1578, train acc: 0.5241, val loss: 1.1625, val acc: 0.5218  (best train acc: 0.5341, best val acc: 0.5258, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 7180] train loss: 1.1499, train acc: 0.5332, val loss: 1.1620, val acc: 0.5251  (best train acc: 0.5377, best val acc: 0.5258, best train loss: 1.1438  @ epoch 4490 )\n",
      "[Epoch: 7200] train loss: 1.1503, train acc: 0.5247, val loss: 1.1615, val acc: 0.5207  (best train acc: 0.5377, best val acc: 0.5258, best train loss: 1.1428  @ epoch 7193 )\n",
      "[Epoch: 7220] train loss: 1.1618, train acc: 0.5205, val loss: 1.1611, val acc: 0.5221  (best train acc: 0.5377, best val acc: 0.5265, best train loss: 1.1428  @ epoch 7193 )\n",
      "[Epoch: 7240] train loss: 1.1635, train acc: 0.5222, val loss: 1.1606, val acc: 0.5241  (best train acc: 0.5377, best val acc: 0.5265, best train loss: 1.1428  @ epoch 7193 )\n",
      "[Epoch: 7260] train loss: 1.1516, train acc: 0.5291, val loss: 1.1603, val acc: 0.5231  (best train acc: 0.5377, best val acc: 0.5265, best train loss: 1.1428  @ epoch 7193 )\n",
      "[Epoch: 7280] train loss: 1.1532, train acc: 0.5254, val loss: 1.1604, val acc: 0.5194  (best train acc: 0.5377, best val acc: 0.5265, best train loss: 1.1428  @ epoch 7193 )\n",
      "[Epoch: 7300] train loss: 1.1536, train acc: 0.5290, val loss: 1.1592, val acc: 0.5251  (best train acc: 0.5377, best val acc: 0.5265, best train loss: 1.1428  @ epoch 7193 )\n",
      "[Epoch: 7320] train loss: 1.1551, train acc: 0.5251, val loss: 1.1588, val acc: 0.5241  (best train acc: 0.5377, best val acc: 0.5265, best train loss: 1.1428  @ epoch 7193 )\n",
      "[Epoch: 7340] train loss: 1.1548, train acc: 0.5303, val loss: 1.1585, val acc: 0.5245  (best train acc: 0.5377, best val acc: 0.5265, best train loss: 1.1428  @ epoch 7193 )\n",
      "[Epoch: 7360] train loss: 1.1520, train acc: 0.5253, val loss: 1.1578, val acc: 0.5231  (best train acc: 0.5387, best val acc: 0.5275, best train loss: 1.1390  @ epoch 7356 )\n",
      "[Epoch: 7380] train loss: 1.1522, train acc: 0.5340, val loss: 1.1577, val acc: 0.5272  (best train acc: 0.5387, best val acc: 0.5278, best train loss: 1.1390  @ epoch 7356 )\n",
      "[Epoch: 7400] train loss: 1.1466, train acc: 0.5296, val loss: 1.1571, val acc: 0.5234  (best train acc: 0.5387, best val acc: 0.5278, best train loss: 1.1390  @ epoch 7356 )\n",
      "[Epoch: 7420] train loss: 1.1542, train acc: 0.5309, val loss: 1.1569, val acc: 0.5234  (best train acc: 0.5387, best val acc: 0.5288, best train loss: 1.1390  @ epoch 7356 )\n",
      "[Epoch: 7440] train loss: 1.1551, train acc: 0.5275, val loss: 1.1566, val acc: 0.5258  (best train acc: 0.5387, best val acc: 0.5288, best train loss: 1.1390  @ epoch 7356 )\n",
      "[Epoch: 7460] train loss: 1.1503, train acc: 0.5265, val loss: 1.1560, val acc: 0.5245  (best train acc: 0.5387, best val acc: 0.5288, best train loss: 1.1390  @ epoch 7356 )\n",
      "[Epoch: 7480] train loss: 1.1477, train acc: 0.5308, val loss: 1.1556, val acc: 0.5261  (best train acc: 0.5387, best val acc: 0.5288, best train loss: 1.1390  @ epoch 7356 )\n",
      "[Epoch: 7500] train loss: 1.1498, train acc: 0.5275, val loss: 1.1555, val acc: 0.5248  (best train acc: 0.5387, best val acc: 0.5298, best train loss: 1.1367  @ epoch 7498 )\n",
      "[Epoch: 7520] train loss: 1.1482, train acc: 0.5296, val loss: 1.1553, val acc: 0.5285  (best train acc: 0.5387, best val acc: 0.5298, best train loss: 1.1367  @ epoch 7498 )\n",
      "[Epoch: 7540] train loss: 1.1431, train acc: 0.5301, val loss: 1.1543, val acc: 0.5272  (best train acc: 0.5387, best val acc: 0.5309, best train loss: 1.1367  @ epoch 7498 )\n",
      "[Epoch: 7560] train loss: 1.1509, train acc: 0.5283, val loss: 1.1542, val acc: 0.5245  (best train acc: 0.5387, best val acc: 0.5309, best train loss: 1.1367  @ epoch 7498 )\n",
      "[Epoch: 7580] train loss: 1.1612, train acc: 0.5213, val loss: 1.1540, val acc: 0.5312  (best train acc: 0.5387, best val acc: 0.5319, best train loss: 1.1367  @ epoch 7498 )\n",
      "[Epoch: 7600] train loss: 1.1468, train acc: 0.5295, val loss: 1.1534, val acc: 0.5218  (best train acc: 0.5387, best val acc: 0.5319, best train loss: 1.1367  @ epoch 7498 )\n",
      "[Epoch: 7620] train loss: 1.1439, train acc: 0.5331, val loss: 1.1525, val acc: 0.5265  (best train acc: 0.5387, best val acc: 0.5319, best train loss: 1.1367  @ epoch 7498 )\n",
      "[Epoch: 7640] train loss: 1.1555, train acc: 0.5276, val loss: 1.1518, val acc: 0.5295  (best train acc: 0.5387, best val acc: 0.5325, best train loss: 1.1367  @ epoch 7498 )\n",
      "[Epoch: 7660] train loss: 1.1389, train acc: 0.5233, val loss: 1.1514, val acc: 0.5322  (best train acc: 0.5387, best val acc: 0.5325, best train loss: 1.1363  @ epoch 7644 )\n",
      "[Epoch: 7680] train loss: 1.1507, train acc: 0.5300, val loss: 1.1520, val acc: 0.5275  (best train acc: 0.5390, best val acc: 0.5325, best train loss: 1.1324  @ epoch 7672 )\n",
      "[Epoch: 7700] train loss: 1.1474, train acc: 0.5303, val loss: 1.1512, val acc: 0.5312  (best train acc: 0.5390, best val acc: 0.5336, best train loss: 1.1317  @ epoch 7696 )\n",
      "[Epoch: 7720] train loss: 1.1552, train acc: 0.5252, val loss: 1.1503, val acc: 0.5275  (best train acc: 0.5390, best val acc: 0.5339, best train loss: 1.1317  @ epoch 7696 )\n",
      "[Epoch: 7740] train loss: 1.1492, train acc: 0.5270, val loss: 1.1498, val acc: 0.5325  (best train acc: 0.5390, best val acc: 0.5346, best train loss: 1.1317  @ epoch 7696 )\n",
      "[Epoch: 7760] train loss: 1.1558, train acc: 0.5179, val loss: 1.1490, val acc: 0.5285  (best train acc: 0.5390, best val acc: 0.5346, best train loss: 1.1317  @ epoch 7696 )\n",
      "[Epoch: 7780] train loss: 1.1564, train acc: 0.5247, val loss: 1.1487, val acc: 0.5248  (best train acc: 0.5390, best val acc: 0.5346, best train loss: 1.1305  @ epoch 7777 )\n",
      "[Epoch: 7800] train loss: 1.1429, train acc: 0.5262, val loss: 1.1484, val acc: 0.5325  (best train acc: 0.5390, best val acc: 0.5346, best train loss: 1.1305  @ epoch 7777 )\n",
      "[Epoch: 7820] train loss: 1.1513, train acc: 0.5275, val loss: 1.1480, val acc: 0.5319  (best train acc: 0.5390, best val acc: 0.5346, best train loss: 1.1305  @ epoch 7777 )\n",
      "[Epoch: 7840] train loss: 1.1493, train acc: 0.5312, val loss: 1.1480, val acc: 0.5298  (best train acc: 0.5390, best val acc: 0.5346, best train loss: 1.1305  @ epoch 7777 )\n",
      "[Epoch: 7860] train loss: 1.1434, train acc: 0.5226, val loss: 1.1488, val acc: 0.5278  (best train acc: 0.5390, best val acc: 0.5346, best train loss: 1.1305  @ epoch 7777 )\n",
      "[Epoch: 7880] train loss: 1.1454, train acc: 0.5265, val loss: 1.1472, val acc: 0.5342  (best train acc: 0.5390, best val acc: 0.5346, best train loss: 1.1305  @ epoch 7777 )\n",
      "[Epoch: 7900] train loss: 1.1445, train acc: 0.5241, val loss: 1.1477, val acc: 0.5241  (best train acc: 0.5390, best val acc: 0.5346, best train loss: 1.1305  @ epoch 7777 )\n",
      "[Epoch: 7920] train loss: 1.1442, train acc: 0.5273, val loss: 1.1473, val acc: 0.5305  (best train acc: 0.5390, best val acc: 0.5346, best train loss: 1.1305  @ epoch 7777 )\n",
      "[Epoch: 7940] train loss: 1.1527, train acc: 0.5242, val loss: 1.1462, val acc: 0.5322  (best train acc: 0.5390, best val acc: 0.5349, best train loss: 1.1305  @ epoch 7777 )\n",
      "[Epoch: 7960] train loss: 1.1404, train acc: 0.5335, val loss: 1.1468, val acc: 0.5275  (best train acc: 0.5390, best val acc: 0.5349, best train loss: 1.1305  @ epoch 7777 )\n",
      "[Epoch: 7980] train loss: 1.1371, train acc: 0.5335, val loss: 1.1482, val acc: 0.5191  (best train acc: 0.5390, best val acc: 0.5349, best train loss: 1.1305  @ epoch 7777 )\n",
      "[Epoch: 8000] train loss: 1.1365, train acc: 0.5359, val loss: 1.1450, val acc: 0.5336  (best train acc: 0.5390, best val acc: 0.5349, best train loss: 1.1305  @ epoch 7777 )\n",
      "[Epoch: 8020] train loss: 1.1458, train acc: 0.5271, val loss: 1.1446, val acc: 0.5305  (best train acc: 0.5390, best val acc: 0.5349, best train loss: 1.1305  @ epoch 7777 )\n",
      "[Epoch: 8040] train loss: 1.1358, train acc: 0.5315, val loss: 1.1451, val acc: 0.5322  (best train acc: 0.5390, best val acc: 0.5349, best train loss: 1.1305  @ epoch 7777 )\n",
      "[Epoch: 8060] train loss: 1.1359, train acc: 0.5378, val loss: 1.1442, val acc: 0.5302  (best train acc: 0.5390, best val acc: 0.5349, best train loss: 1.1302  @ epoch 8046 )\n",
      "[Epoch: 8080] train loss: 1.1466, train acc: 0.5213, val loss: 1.1439, val acc: 0.5305  (best train acc: 0.5390, best val acc: 0.5349, best train loss: 1.1302  @ epoch 8046 )\n",
      "[Epoch: 8100] train loss: 1.1345, train acc: 0.5339, val loss: 1.1429, val acc: 0.5332  (best train acc: 0.5390, best val acc: 0.5349, best train loss: 1.1302  @ epoch 8046 )\n",
      "[Epoch: 8120] train loss: 1.1427, train acc: 0.5293, val loss: 1.1425, val acc: 0.5305  (best train acc: 0.5390, best val acc: 0.5349, best train loss: 1.1302  @ epoch 8046 )\n",
      "[Epoch: 8140] train loss: 1.1457, train acc: 0.5269, val loss: 1.1423, val acc: 0.5325  (best train acc: 0.5390, best val acc: 0.5349, best train loss: 1.1302  @ epoch 8046 )\n",
      "[Epoch: 8160] train loss: 1.1397, train acc: 0.5290, val loss: 1.1443, val acc: 0.5285  (best train acc: 0.5390, best val acc: 0.5349, best train loss: 1.1302  @ epoch 8046 )\n",
      "[Epoch: 8180] train loss: 1.1457, train acc: 0.5283, val loss: 1.1418, val acc: 0.5319  (best train acc: 0.5390, best val acc: 0.5349, best train loss: 1.1302  @ epoch 8046 )\n",
      "[Epoch: 8200] train loss: 1.1425, train acc: 0.5293, val loss: 1.1444, val acc: 0.5278  (best train acc: 0.5390, best val acc: 0.5349, best train loss: 1.1302  @ epoch 8046 )\n",
      "[Epoch: 8220] train loss: 1.1351, train acc: 0.5301, val loss: 1.1415, val acc: 0.5295  (best train acc: 0.5390, best val acc: 0.5349, best train loss: 1.1256  @ epoch 8208 )\n",
      "[Epoch: 8240] train loss: 1.1422, train acc: 0.5285, val loss: 1.1460, val acc: 0.5177  (best train acc: 0.5390, best val acc: 0.5349, best train loss: 1.1256  @ epoch 8208 )\n",
      "[Epoch: 8260] train loss: 1.1317, train acc: 0.5270, val loss: 1.1409, val acc: 0.5261  (best train acc: 0.5390, best val acc: 0.5349, best train loss: 1.1256  @ epoch 8208 )\n",
      "[Epoch: 8280] train loss: 1.1277, train acc: 0.5318, val loss: 1.1400, val acc: 0.5329  (best train acc: 0.5390, best val acc: 0.5349, best train loss: 1.1256  @ epoch 8208 )\n",
      "[Epoch: 8300] train loss: 1.1317, train acc: 0.5331, val loss: 1.1398, val acc: 0.5315  (best train acc: 0.5390, best val acc: 0.5349, best train loss: 1.1256  @ epoch 8208 )\n",
      "[Epoch: 8320] train loss: 1.1445, train acc: 0.5221, val loss: 1.1400, val acc: 0.5319  (best train acc: 0.5390, best val acc: 0.5349, best train loss: 1.1256  @ epoch 8208 )\n",
      "[Epoch: 8340] train loss: 1.1440, train acc: 0.5270, val loss: 1.1400, val acc: 0.5278  (best train acc: 0.5390, best val acc: 0.5349, best train loss: 1.1256  @ epoch 8208 )\n",
      "[Epoch: 8360] train loss: 1.1310, train acc: 0.5343, val loss: 1.1409, val acc: 0.5285  (best train acc: 0.5390, best val acc: 0.5349, best train loss: 1.1256  @ epoch 8208 )\n",
      "[Epoch: 8380] train loss: 1.1334, train acc: 0.5335, val loss: 1.1385, val acc: 0.5329  (best train acc: 0.5390, best val acc: 0.5349, best train loss: 1.1256  @ epoch 8208 )\n",
      "[Epoch: 8400] train loss: 1.1291, train acc: 0.5335, val loss: 1.1400, val acc: 0.5298  (best train acc: 0.5390, best val acc: 0.5349, best train loss: 1.1256  @ epoch 8208 )\n",
      "[Epoch: 8420] train loss: 1.1323, train acc: 0.5327, val loss: 1.1387, val acc: 0.5339  (best train acc: 0.5390, best val acc: 0.5349, best train loss: 1.1256  @ epoch 8208 )\n",
      "[Epoch: 8440] train loss: 1.1334, train acc: 0.5348, val loss: 1.1384, val acc: 0.5298  (best train acc: 0.5390, best val acc: 0.5349, best train loss: 1.1256  @ epoch 8208 )\n",
      "[Epoch: 8460] train loss: 1.1297, train acc: 0.5321, val loss: 1.1379, val acc: 0.5305  (best train acc: 0.5390, best val acc: 0.5349, best train loss: 1.1228  @ epoch 8445 )\n",
      "[Epoch: 8480] train loss: 1.1384, train acc: 0.5304, val loss: 1.1437, val acc: 0.5218  (best train acc: 0.5390, best val acc: 0.5349, best train loss: 1.1228  @ epoch 8445 )\n",
      "[Epoch: 8500] train loss: 1.1453, train acc: 0.5257, val loss: 1.1392, val acc: 0.5298  (best train acc: 0.5390, best val acc: 0.5349, best train loss: 1.1228  @ epoch 8445 )\n",
      "[Epoch: 8520] train loss: 1.1420, train acc: 0.5315, val loss: 1.1368, val acc: 0.5302  (best train acc: 0.5390, best val acc: 0.5349, best train loss: 1.1228  @ epoch 8445 )\n",
      "[Epoch: 8540] train loss: 1.1351, train acc: 0.5309, val loss: 1.1362, val acc: 0.5329  (best train acc: 0.5390, best val acc: 0.5349, best train loss: 1.1228  @ epoch 8445 )\n",
      "[Epoch: 8560] train loss: 1.1400, train acc: 0.5271, val loss: 1.1389, val acc: 0.5275  (best train acc: 0.5390, best val acc: 0.5349, best train loss: 1.1228  @ epoch 8445 )\n",
      "[Epoch: 8580] train loss: 1.1316, train acc: 0.5341, val loss: 1.1389, val acc: 0.5224  (best train acc: 0.5390, best val acc: 0.5349, best train loss: 1.1219  @ epoch 8564 )\n",
      "[Epoch: 8600] train loss: 1.1394, train acc: 0.5285, val loss: 1.1352, val acc: 0.5336  (best train acc: 0.5390, best val acc: 0.5349, best train loss: 1.1219  @ epoch 8564 )\n",
      "[Epoch: 8620] train loss: 1.1449, train acc: 0.5278, val loss: 1.1348, val acc: 0.5349  (best train acc: 0.5396, best val acc: 0.5349, best train loss: 1.1219  @ epoch 8564 )\n",
      "[Epoch: 8640] train loss: 1.1358, train acc: 0.5299, val loss: 1.1352, val acc: 0.5305  (best train acc: 0.5396, best val acc: 0.5349, best train loss: 1.1219  @ epoch 8564 )\n",
      "[Epoch: 8660] train loss: 1.1271, train acc: 0.5328, val loss: 1.1361, val acc: 0.5312  (best train acc: 0.5396, best val acc: 0.5349, best train loss: 1.1204  @ epoch 8646 )\n",
      "[Epoch: 8680] train loss: 1.1267, train acc: 0.5349, val loss: 1.1353, val acc: 0.5268  (best train acc: 0.5396, best val acc: 0.5349, best train loss: 1.1204  @ epoch 8646 )\n",
      "[Epoch: 8700] train loss: 1.1238, train acc: 0.5333, val loss: 1.1344, val acc: 0.5305  (best train acc: 0.5396, best val acc: 0.5349, best train loss: 1.1204  @ epoch 8646 )\n",
      "[Epoch: 8720] train loss: 1.1377, train acc: 0.5301, val loss: 1.1333, val acc: 0.5305  (best train acc: 0.5406, best val acc: 0.5349, best train loss: 1.1204  @ epoch 8646 )\n",
      "[Epoch: 8740] train loss: 1.1473, train acc: 0.5230, val loss: 1.1333, val acc: 0.5336  (best train acc: 0.5406, best val acc: 0.5349, best train loss: 1.1177  @ epoch 8734 )\n",
      "[Epoch: 8760] train loss: 1.1320, train acc: 0.5294, val loss: 1.1364, val acc: 0.5228  (best train acc: 0.5406, best val acc: 0.5349, best train loss: 1.1177  @ epoch 8734 )\n",
      "[Epoch: 8780] train loss: 1.1306, train acc: 0.5323, val loss: 1.1344, val acc: 0.5278  (best train acc: 0.5406, best val acc: 0.5352, best train loss: 1.1177  @ epoch 8734 )\n",
      "[Epoch: 8800] train loss: 1.1397, train acc: 0.5312, val loss: 1.1320, val acc: 0.5312  (best train acc: 0.5406, best val acc: 0.5352, best train loss: 1.1177  @ epoch 8734 )\n",
      "[Epoch: 8820] train loss: 1.1291, train acc: 0.5310, val loss: 1.1330, val acc: 0.5288  (best train acc: 0.5406, best val acc: 0.5352, best train loss: 1.1177  @ epoch 8734 )\n",
      "[Epoch: 8840] train loss: 1.1214, train acc: 0.5376, val loss: 1.1315, val acc: 0.5342  (best train acc: 0.5406, best val acc: 0.5359, best train loss: 1.1177  @ epoch 8734 )\n",
      "[Epoch: 8860] train loss: 1.1273, train acc: 0.5295, val loss: 1.1311, val acc: 0.5325  (best train acc: 0.5406, best val acc: 0.5359, best train loss: 1.1177  @ epoch 8734 )\n",
      "[Epoch: 8880] train loss: 1.1273, train acc: 0.5358, val loss: 1.1322, val acc: 0.5309  (best train acc: 0.5412, best val acc: 0.5359, best train loss: 1.1177  @ epoch 8734 )\n",
      "[Epoch: 8900] train loss: 1.1344, train acc: 0.5314, val loss: 1.1305, val acc: 0.5329  (best train acc: 0.5412, best val acc: 0.5359, best train loss: 1.1177  @ epoch 8734 )\n",
      "[Epoch: 8920] train loss: 1.1323, train acc: 0.5281, val loss: 1.1302, val acc: 0.5305  (best train acc: 0.5412, best val acc: 0.5359, best train loss: 1.1177  @ epoch 8734 )\n",
      "[Epoch: 8940] train loss: 1.1304, train acc: 0.5330, val loss: 1.1312, val acc: 0.5278  (best train acc: 0.5412, best val acc: 0.5359, best train loss: 1.1177  @ epoch 8734 )\n",
      "[Epoch: 8960] train loss: 1.1309, train acc: 0.5274, val loss: 1.1299, val acc: 0.5288  (best train acc: 0.5412, best val acc: 0.5359, best train loss: 1.1161  @ epoch 8945 )\n",
      "[Epoch: 8980] train loss: 1.1244, train acc: 0.5338, val loss: 1.1279, val acc: 0.5329  (best train acc: 0.5412, best val acc: 0.5359, best train loss: 1.1161  @ epoch 8945 )\n",
      "[Epoch: 9000] train loss: 1.1262, train acc: 0.5285, val loss: 1.1297, val acc: 0.5315  (best train acc: 0.5412, best val acc: 0.5359, best train loss: 1.1155  @ epoch 8988 )\n",
      "[Epoch: 9020] train loss: 1.1302, train acc: 0.5325, val loss: 1.1271, val acc: 0.5325  (best train acc: 0.5412, best val acc: 0.5359, best train loss: 1.1102  @ epoch 9018 )\n",
      "[Epoch: 9040] train loss: 1.1297, train acc: 0.5239, val loss: 1.1262, val acc: 0.5342  (best train acc: 0.5412, best val acc: 0.5359, best train loss: 1.1102  @ epoch 9018 )\n",
      "[Epoch: 9060] train loss: 1.1147, train acc: 0.5375, val loss: 1.1254, val acc: 0.5329  (best train acc: 0.5412, best val acc: 0.5373, best train loss: 1.1102  @ epoch 9018 )\n",
      "[Epoch: 9080] train loss: 1.1246, train acc: 0.5352, val loss: 1.1262, val acc: 0.5315  (best train acc: 0.5412, best val acc: 0.5373, best train loss: 1.1102  @ epoch 9018 )\n",
      "[Epoch: 9100] train loss: 1.1299, train acc: 0.5311, val loss: 1.1257, val acc: 0.5352  (best train acc: 0.5412, best val acc: 0.5373, best train loss: 1.1102  @ epoch 9018 )\n",
      "[Epoch: 9120] train loss: 1.1196, train acc: 0.5374, val loss: 1.1267, val acc: 0.5312  (best train acc: 0.5412, best val acc: 0.5390, best train loss: 1.1072  @ epoch 9114 )\n",
      "[Epoch: 9140] train loss: 1.1288, train acc: 0.5287, val loss: 1.1248, val acc: 0.5325  (best train acc: 0.5412, best val acc: 0.5390, best train loss: 1.1072  @ epoch 9114 )\n",
      "[Epoch: 9160] train loss: 1.1242, train acc: 0.5339, val loss: 1.1244, val acc: 0.5349  (best train acc: 0.5412, best val acc: 0.5390, best train loss: 1.1072  @ epoch 9114 )\n",
      "[Epoch: 9180] train loss: 1.1303, train acc: 0.5287, val loss: 1.1237, val acc: 0.5329  (best train acc: 0.5412, best val acc: 0.5390, best train loss: 1.1072  @ epoch 9114 )\n",
      "[Epoch: 9200] train loss: 1.1296, train acc: 0.5251, val loss: 1.1238, val acc: 0.5336  (best train acc: 0.5412, best val acc: 0.5390, best train loss: 1.1072  @ epoch 9114 )\n",
      "[Epoch: 9220] train loss: 1.1281, train acc: 0.5310, val loss: 1.1228, val acc: 0.5336  (best train acc: 0.5412, best val acc: 0.5390, best train loss: 1.1072  @ epoch 9114 )\n",
      "[Epoch: 9240] train loss: 1.1155, train acc: 0.5333, val loss: 1.1237, val acc: 0.5309  (best train acc: 0.5412, best val acc: 0.5390, best train loss: 1.1072  @ epoch 9114 )\n",
      "[Epoch: 9260] train loss: 1.1188, train acc: 0.5381, val loss: 1.1219, val acc: 0.5342  (best train acc: 0.5412, best val acc: 0.5390, best train loss: 1.1072  @ epoch 9114 )\n",
      "[Epoch: 9280] train loss: 1.1182, train acc: 0.5294, val loss: 1.1253, val acc: 0.5255  (best train acc: 0.5412, best val acc: 0.5390, best train loss: 1.1072  @ epoch 9114 )\n",
      "[Epoch: 9300] train loss: 1.1220, train acc: 0.5286, val loss: 1.1219, val acc: 0.5315  (best train acc: 0.5412, best val acc: 0.5390, best train loss: 1.1072  @ epoch 9114 )\n",
      "[Epoch: 9320] train loss: 1.1295, train acc: 0.5309, val loss: 1.1218, val acc: 0.5359  (best train acc: 0.5412, best val acc: 0.5390, best train loss: 1.1072  @ epoch 9114 )\n",
      "[Epoch: 9340] train loss: 1.1130, train acc: 0.5366, val loss: 1.1207, val acc: 0.5322  (best train acc: 0.5412, best val acc: 0.5390, best train loss: 1.1072  @ epoch 9114 )\n",
      "[Epoch: 9360] train loss: 1.1184, train acc: 0.5348, val loss: 1.1237, val acc: 0.5305  (best train acc: 0.5412, best val acc: 0.5390, best train loss: 1.1067  @ epoch 9348 )\n",
      "[Epoch: 9380] train loss: 1.1192, train acc: 0.5313, val loss: 1.1213, val acc: 0.5319  (best train acc: 0.5412, best val acc: 0.5390, best train loss: 1.1067  @ epoch 9348 )\n",
      "[Epoch: 9400] train loss: 1.1151, train acc: 0.5379, val loss: 1.1214, val acc: 0.5322  (best train acc: 0.5412, best val acc: 0.5390, best train loss: 1.1067  @ epoch 9348 )\n",
      "[Epoch: 9420] train loss: 1.1178, train acc: 0.5390, val loss: 1.1194, val acc: 0.5342  (best train acc: 0.5412, best val acc: 0.5390, best train loss: 1.1067  @ epoch 9348 )\n",
      "[Epoch: 9440] train loss: 1.1178, train acc: 0.5348, val loss: 1.1197, val acc: 0.5349  (best train acc: 0.5412, best val acc: 0.5390, best train loss: 1.1062  @ epoch 9433 )\n",
      "[Epoch: 9460] train loss: 1.1166, train acc: 0.5307, val loss: 1.1188, val acc: 0.5356  (best train acc: 0.5412, best val acc: 0.5390, best train loss: 1.1062  @ epoch 9433 )\n",
      "[Epoch: 9480] train loss: 1.1243, train acc: 0.5322, val loss: 1.1186, val acc: 0.5325  (best train acc: 0.5412, best val acc: 0.5390, best train loss: 1.1062  @ epoch 9433 )\n",
      "[Epoch: 9500] train loss: 1.1134, train acc: 0.5359, val loss: 1.1214, val acc: 0.5336  (best train acc: 0.5412, best val acc: 0.5390, best train loss: 1.1062  @ epoch 9433 )\n",
      "[Epoch: 9520] train loss: 1.1261, train acc: 0.5279, val loss: 1.1185, val acc: 0.5332  (best train acc: 0.5412, best val acc: 0.5390, best train loss: 1.1062  @ epoch 9433 )\n",
      "[Epoch: 9540] train loss: 1.1252, train acc: 0.5273, val loss: 1.1192, val acc: 0.5319  (best train acc: 0.5412, best val acc: 0.5390, best train loss: 1.1047  @ epoch 9528 )\n",
      "[Epoch: 9560] train loss: 1.1113, train acc: 0.5347, val loss: 1.1207, val acc: 0.5295  (best train acc: 0.5412, best val acc: 0.5390, best train loss: 1.1047  @ epoch 9528 )\n",
      "[Epoch: 9580] train loss: 1.1144, train acc: 0.5344, val loss: 1.1172, val acc: 0.5352  (best train acc: 0.5412, best val acc: 0.5390, best train loss: 1.1047  @ epoch 9528 )\n",
      "[Epoch: 9600] train loss: 1.1279, train acc: 0.5309, val loss: 1.1192, val acc: 0.5319  (best train acc: 0.5412, best val acc: 0.5390, best train loss: 1.1047  @ epoch 9528 )\n",
      "[Epoch: 9620] train loss: 1.1216, train acc: 0.5275, val loss: 1.1175, val acc: 0.5342  (best train acc: 0.5412, best val acc: 0.5390, best train loss: 1.1047  @ epoch 9528 )\n",
      "[Epoch: 9640] train loss: 1.1172, train acc: 0.5294, val loss: 1.1165, val acc: 0.5339  (best train acc: 0.5416, best val acc: 0.5393, best train loss: 1.1015  @ epoch 9629 )\n",
      "[Epoch: 9660] train loss: 1.1166, train acc: 0.5369, val loss: 1.1157, val acc: 0.5386  (best train acc: 0.5416, best val acc: 0.5393, best train loss: 1.1010  @ epoch 9650 )\n",
      "[Epoch: 9680] train loss: 1.1168, train acc: 0.5281, val loss: 1.1166, val acc: 0.5332  (best train acc: 0.5416, best val acc: 0.5393, best train loss: 1.1010  @ epoch 9650 )\n",
      "[Epoch: 9700] train loss: 1.1265, train acc: 0.5275, val loss: 1.1155, val acc: 0.5383  (best train acc: 0.5416, best val acc: 0.5393, best train loss: 1.1010  @ epoch 9650 )\n",
      "[Epoch: 9720] train loss: 1.1087, train acc: 0.5367, val loss: 1.1170, val acc: 0.5349  (best train acc: 0.5416, best val acc: 0.5393, best train loss: 1.1010  @ epoch 9650 )\n",
      "[Epoch: 9740] train loss: 1.1234, train acc: 0.5364, val loss: 1.1176, val acc: 0.5282  (best train acc: 0.5416, best val acc: 0.5393, best train loss: 1.1010  @ epoch 9650 )\n",
      "[Epoch: 9760] train loss: 1.1206, train acc: 0.5269, val loss: 1.1145, val acc: 0.5369  (best train acc: 0.5416, best val acc: 0.5396, best train loss: 1.1010  @ epoch 9650 )\n",
      "[Epoch: 9780] train loss: 1.1213, train acc: 0.5265, val loss: 1.1145, val acc: 0.5359  (best train acc: 0.5416, best val acc: 0.5400, best train loss: 1.1010  @ epoch 9650 )\n",
      "[Epoch: 9800] train loss: 1.1217, train acc: 0.5253, val loss: 1.1150, val acc: 0.5356  (best train acc: 0.5416, best val acc: 0.5406, best train loss: 1.1010  @ epoch 9650 )\n",
      "[Epoch: 9820] train loss: 1.1230, train acc: 0.5333, val loss: 1.1151, val acc: 0.5312  (best train acc: 0.5416, best val acc: 0.5406, best train loss: 1.1010  @ epoch 9650 )\n",
      "[Epoch: 9840] train loss: 1.1371, train acc: 0.5188, val loss: 1.1176, val acc: 0.5349  (best train acc: 0.5416, best val acc: 0.5406, best train loss: 1.1010  @ epoch 9650 )\n",
      "[Epoch: 9860] train loss: 1.1171, train acc: 0.5319, val loss: 1.1142, val acc: 0.5369  (best train acc: 0.5416, best val acc: 0.5406, best train loss: 1.1010  @ epoch 9650 )\n",
      "[Epoch: 9880] train loss: 1.1145, train acc: 0.5286, val loss: 1.1176, val acc: 0.5275  (best train acc: 0.5416, best val acc: 0.5406, best train loss: 1.1010  @ epoch 9650 )\n",
      "[Epoch: 9900] train loss: 1.1158, train acc: 0.5331, val loss: 1.1133, val acc: 0.5359  (best train acc: 0.5416, best val acc: 0.5406, best train loss: 1.1010  @ epoch 9650 )\n",
      "[Epoch: 9920] train loss: 1.1165, train acc: 0.5299, val loss: 1.1139, val acc: 0.5292  (best train acc: 0.5457, best val acc: 0.5406, best train loss: 1.1010  @ epoch 9650 )\n",
      "[Epoch: 9940] train loss: 1.1107, train acc: 0.5338, val loss: 1.1136, val acc: 0.5359  (best train acc: 0.5457, best val acc: 0.5406, best train loss: 1.1010  @ epoch 9650 )\n",
      "[Epoch: 9960] train loss: 1.1236, train acc: 0.5306, val loss: 1.1135, val acc: 0.5349  (best train acc: 0.5457, best val acc: 0.5406, best train loss: 1.1009  @ epoch 9944 )\n",
      "[Epoch: 9980] train loss: 1.1133, train acc: 0.5398, val loss: 1.1144, val acc: 0.5329  (best train acc: 0.5457, best val acc: 0.5406, best train loss: 1.1009  @ epoch 9944 )\n",
      "[Epoch: 10000] train loss: 1.1181, train acc: 0.5265, val loss: 1.1134, val acc: 0.5312  (best train acc: 0.5457, best val acc: 0.5406, best train loss: 1.0995  @ epoch 9992 )\n",
      "[Epoch: 10020] train loss: 1.1119, train acc: 0.5359, val loss: 1.1133, val acc: 0.5329  (best train acc: 0.5457, best val acc: 0.5406, best train loss: 1.0995  @ epoch 9992 )\n",
      "[Epoch: 10040] train loss: 1.1075, train acc: 0.5375, val loss: 1.1123, val acc: 0.5369  (best train acc: 0.5464, best val acc: 0.5413, best train loss: 1.0963  @ epoch 10024 )\n",
      "[Epoch: 10060] train loss: 1.1216, train acc: 0.5234, val loss: 1.1131, val acc: 0.5319  (best train acc: 0.5464, best val acc: 0.5413, best train loss: 1.0960  @ epoch 10058 )\n",
      "[Epoch: 10080] train loss: 1.1200, train acc: 0.5268, val loss: 1.1121, val acc: 0.5322  (best train acc: 0.5464, best val acc: 0.5413, best train loss: 1.0960  @ epoch 10058 )\n",
      "[Epoch: 10100] train loss: 1.1103, train acc: 0.5359, val loss: 1.1129, val acc: 0.5356  (best train acc: 0.5464, best val acc: 0.5413, best train loss: 1.0960  @ epoch 10058 )\n",
      "[Epoch: 10120] train loss: 1.1196, train acc: 0.5315, val loss: 1.1130, val acc: 0.5309  (best train acc: 0.5464, best val acc: 0.5413, best train loss: 1.0960  @ epoch 10058 )\n",
      "[Epoch: 10140] train loss: 1.0981, train acc: 0.5388, val loss: 1.1118, val acc: 0.5325  (best train acc: 0.5464, best val acc: 0.5413, best train loss: 1.0960  @ epoch 10058 )\n",
      "[Epoch: 10160] train loss: 1.1121, train acc: 0.5340, val loss: 1.1120, val acc: 0.5346  (best train acc: 0.5464, best val acc: 0.5413, best train loss: 1.0960  @ epoch 10058 )\n",
      "[Epoch: 10180] train loss: 1.1142, train acc: 0.5314, val loss: 1.1107, val acc: 0.5359  (best train acc: 0.5464, best val acc: 0.5430, best train loss: 1.0960  @ epoch 10058 )\n",
      "[Epoch: 10200] train loss: 1.1191, train acc: 0.5312, val loss: 1.1112, val acc: 0.5342  (best train acc: 0.5464, best val acc: 0.5430, best train loss: 1.0960  @ epoch 10058 )\n",
      "[Epoch: 10220] train loss: 1.1175, train acc: 0.5296, val loss: 1.1144, val acc: 0.5339  (best train acc: 0.5464, best val acc: 0.5430, best train loss: 1.0960  @ epoch 10058 )\n",
      "[Epoch: 10240] train loss: 1.1066, train acc: 0.5350, val loss: 1.1119, val acc: 0.5342  (best train acc: 0.5464, best val acc: 0.5430, best train loss: 1.0960  @ epoch 10058 )\n",
      "[Epoch: 10260] train loss: 1.1101, train acc: 0.5370, val loss: 1.1099, val acc: 0.5406  (best train acc: 0.5464, best val acc: 0.5430, best train loss: 1.0958  @ epoch 10250 )\n",
      "[Epoch: 10280] train loss: 1.1195, train acc: 0.5341, val loss: 1.1122, val acc: 0.5329  (best train acc: 0.5464, best val acc: 0.5430, best train loss: 1.0955  @ epoch 10276 )\n",
      "[Epoch: 10300] train loss: 1.1140, train acc: 0.5357, val loss: 1.1091, val acc: 0.5369  (best train acc: 0.5464, best val acc: 0.5430, best train loss: 1.0946  @ epoch 10292 )\n",
      "[Epoch: 10320] train loss: 1.1223, train acc: 0.5241, val loss: 1.1105, val acc: 0.5356  (best train acc: 0.5464, best val acc: 0.5430, best train loss: 1.0929  @ epoch 10315 )\n",
      "[Epoch: 10340] train loss: 1.1088, train acc: 0.5310, val loss: 1.1113, val acc: 0.5366  (best train acc: 0.5464, best val acc: 0.5430, best train loss: 1.0929  @ epoch 10315 )\n",
      "[Epoch: 10360] train loss: 1.1071, train acc: 0.5371, val loss: 1.1089, val acc: 0.5423  (best train acc: 0.5464, best val acc: 0.5444, best train loss: 1.0929  @ epoch 10315 )\n",
      "[Epoch: 10380] train loss: 1.1050, train acc: 0.5377, val loss: 1.1095, val acc: 0.5376  (best train acc: 0.5464, best val acc: 0.5444, best train loss: 1.0929  @ epoch 10315 )\n",
      "[Epoch: 10400] train loss: 1.1051, train acc: 0.5344, val loss: 1.1088, val acc: 0.5363  (best train acc: 0.5464, best val acc: 0.5444, best train loss: 1.0929  @ epoch 10315 )\n",
      "[Epoch: 10420] train loss: 1.1151, train acc: 0.5317, val loss: 1.1088, val acc: 0.5356  (best train acc: 0.5464, best val acc: 0.5444, best train loss: 1.0929  @ epoch 10315 )\n",
      "[Epoch: 10440] train loss: 1.1107, train acc: 0.5317, val loss: 1.1079, val acc: 0.5406  (best train acc: 0.5464, best val acc: 0.5444, best train loss: 1.0929  @ epoch 10315 )\n",
      "[Epoch: 10460] train loss: 1.1319, train acc: 0.5119, val loss: 1.1278, val acc: 0.5143  (best train acc: 0.5464, best val acc: 0.5444, best train loss: 1.0929  @ epoch 10315 )\n",
      "[Epoch: 10480] train loss: 1.1894, train acc: 0.4747, val loss: 1.1758, val acc: 0.4880  (best train acc: 0.5464, best val acc: 0.5444, best train loss: 1.0929  @ epoch 10315 )\n",
      "[Epoch: 10500] train loss: 1.1578, train acc: 0.5252, val loss: 1.1450, val acc: 0.5309  (best train acc: 0.5464, best val acc: 0.5444, best train loss: 1.0929  @ epoch 10315 )\n",
      "[Epoch: 10520] train loss: 1.1395, train acc: 0.5331, val loss: 1.1328, val acc: 0.5332  (best train acc: 0.5464, best val acc: 0.5444, best train loss: 1.0929  @ epoch 10315 )\n",
      "[Epoch: 10540] train loss: 1.1351, train acc: 0.5295, val loss: 1.1285, val acc: 0.5346  (best train acc: 0.5464, best val acc: 0.5444, best train loss: 1.0929  @ epoch 10315 )\n",
      "[Epoch: 10560] train loss: 1.1334, train acc: 0.5390, val loss: 1.1259, val acc: 0.5363  (best train acc: 0.5464, best val acc: 0.5444, best train loss: 1.0929  @ epoch 10315 )\n",
      "[Epoch: 10580] train loss: 1.1222, train acc: 0.5348, val loss: 1.1258, val acc: 0.5339  (best train acc: 0.5464, best val acc: 0.5444, best train loss: 1.0929  @ epoch 10315 )\n",
      "[Epoch: 10600] train loss: 1.1146, train acc: 0.5404, val loss: 1.1242, val acc: 0.5336  (best train acc: 0.5464, best val acc: 0.5444, best train loss: 1.0929  @ epoch 10315 )\n",
      "[Epoch: 10620] train loss: 1.1234, train acc: 0.5329, val loss: 1.1229, val acc: 0.5325  (best train acc: 0.5464, best val acc: 0.5444, best train loss: 1.0929  @ epoch 10315 )\n",
      "[Epoch: 10640] train loss: 1.1123, train acc: 0.5385, val loss: 1.1204, val acc: 0.5342  (best train acc: 0.5464, best val acc: 0.5444, best train loss: 1.0929  @ epoch 10315 )\n",
      "[Epoch: 10660] train loss: 1.1199, train acc: 0.5385, val loss: 1.1199, val acc: 0.5349  (best train acc: 0.5464, best val acc: 0.5444, best train loss: 1.0929  @ epoch 10315 )\n",
      "[Epoch: 10680] train loss: 1.1236, train acc: 0.5336, val loss: 1.1194, val acc: 0.5376  (best train acc: 0.5464, best val acc: 0.5444, best train loss: 1.0929  @ epoch 10315 )\n",
      "[Epoch: 10700] train loss: 1.1176, train acc: 0.5408, val loss: 1.1189, val acc: 0.5349  (best train acc: 0.5464, best val acc: 0.5444, best train loss: 1.0929  @ epoch 10315 )\n",
      "[Epoch: 10720] train loss: 1.1107, train acc: 0.5385, val loss: 1.1193, val acc: 0.5369  (best train acc: 0.5464, best val acc: 0.5444, best train loss: 1.0929  @ epoch 10315 )\n",
      "[Epoch: 10740] train loss: 1.1248, train acc: 0.5314, val loss: 1.1174, val acc: 0.5376  (best train acc: 0.5464, best val acc: 0.5444, best train loss: 1.0929  @ epoch 10315 )\n",
      "[Epoch: 10760] train loss: 1.1218, train acc: 0.5307, val loss: 1.1176, val acc: 0.5369  (best train acc: 0.5464, best val acc: 0.5444, best train loss: 1.0929  @ epoch 10315 )\n",
      "[Epoch: 10780] train loss: 1.1177, train acc: 0.5365, val loss: 1.1176, val acc: 0.5369  (best train acc: 0.5464, best val acc: 0.5444, best train loss: 1.0929  @ epoch 10315 )\n",
      "[Epoch: 10800] train loss: 1.1218, train acc: 0.5329, val loss: 1.1166, val acc: 0.5366  (best train acc: 0.5464, best val acc: 0.5444, best train loss: 1.0929  @ epoch 10315 )\n",
      "[Epoch: 10820] train loss: 1.1136, train acc: 0.5384, val loss: 1.1156, val acc: 0.5359  (best train acc: 0.5464, best val acc: 0.5444, best train loss: 1.0929  @ epoch 10315 )\n",
      "[Epoch: 10840] train loss: 1.1205, train acc: 0.5333, val loss: 1.1160, val acc: 0.5366  (best train acc: 0.5464, best val acc: 0.5444, best train loss: 1.0929  @ epoch 10315 )\n",
      "[Epoch: 10860] train loss: 1.1145, train acc: 0.5355, val loss: 1.1153, val acc: 0.5386  (best train acc: 0.5464, best val acc: 0.5444, best train loss: 1.0929  @ epoch 10315 )\n",
      "[Epoch: 10880] train loss: 1.1072, train acc: 0.5355, val loss: 1.1147, val acc: 0.5366  (best train acc: 0.5464, best val acc: 0.5444, best train loss: 1.0929  @ epoch 10315 )\n",
      "[Epoch: 10900] train loss: 1.1052, train acc: 0.5403, val loss: 1.1145, val acc: 0.5383  (best train acc: 0.5464, best val acc: 0.5444, best train loss: 1.0929  @ epoch 10315 )\n",
      "[Epoch: 10920] train loss: 1.1232, train acc: 0.5356, val loss: 1.1156, val acc: 0.5329  (best train acc: 0.5464, best val acc: 0.5444, best train loss: 1.0929  @ epoch 10315 )\n",
      "[Epoch: 10940] train loss: 1.1168, train acc: 0.5364, val loss: 1.1131, val acc: 0.5373  (best train acc: 0.5464, best val acc: 0.5444, best train loss: 1.0929  @ epoch 10315 )\n",
      "[Epoch: 10960] train loss: 1.1218, train acc: 0.5307, val loss: 1.1127, val acc: 0.5383  (best train acc: 0.5464, best val acc: 0.5444, best train loss: 1.0929  @ epoch 10315 )\n",
      "[Epoch: 10980] train loss: 1.0987, train acc: 0.5406, val loss: 1.1121, val acc: 0.5386  (best train acc: 0.5464, best val acc: 0.5444, best train loss: 1.0929  @ epoch 10315 )\n",
      "[Epoch: 11000] train loss: 1.1199, train acc: 0.5320, val loss: 1.1114, val acc: 0.5376  (best train acc: 0.5464, best val acc: 0.5444, best train loss: 1.0929  @ epoch 10315 )\n",
      "[Epoch: 11020] train loss: 1.0968, train acc: 0.5410, val loss: 1.1109, val acc: 0.5369  (best train acc: 0.5464, best val acc: 0.5444, best train loss: 1.0929  @ epoch 10315 )\n",
      "[Epoch: 11040] train loss: 1.1196, train acc: 0.5301, val loss: 1.1101, val acc: 0.5386  (best train acc: 0.5486, best val acc: 0.5444, best train loss: 1.0883  @ epoch 11028 )\n",
      "[Epoch: 11060] train loss: 1.1032, train acc: 0.5398, val loss: 1.1097, val acc: 0.5359  (best train acc: 0.5486, best val acc: 0.5444, best train loss: 1.0883  @ epoch 11028 )\n",
      "[Epoch: 11080] train loss: 1.1123, train acc: 0.5346, val loss: 1.1085, val acc: 0.5400  (best train acc: 0.5486, best val acc: 0.5444, best train loss: 1.0883  @ epoch 11028 )\n",
      "[Epoch: 11100] train loss: 1.1154, train acc: 0.5349, val loss: 1.1077, val acc: 0.5390  (best train acc: 0.5486, best val acc: 0.5444, best train loss: 1.0883  @ epoch 11028 )\n",
      "[Epoch: 11120] train loss: 1.1136, train acc: 0.5359, val loss: 1.1074, val acc: 0.5390  (best train acc: 0.5486, best val acc: 0.5444, best train loss: 1.0883  @ epoch 11028 )\n",
      "[Epoch: 11140] train loss: 1.1122, train acc: 0.5353, val loss: 1.1092, val acc: 0.5400  (best train acc: 0.5486, best val acc: 0.5444, best train loss: 1.0883  @ epoch 11028 )\n",
      "[Epoch: 11160] train loss: 1.1093, train acc: 0.5375, val loss: 1.1060, val acc: 0.5386  (best train acc: 0.5486, best val acc: 0.5444, best train loss: 1.0883  @ epoch 11028 )\n",
      "[Epoch: 11180] train loss: 1.0983, train acc: 0.5425, val loss: 1.1051, val acc: 0.5413  (best train acc: 0.5486, best val acc: 0.5444, best train loss: 1.0883  @ epoch 11028 )\n",
      "[Epoch: 11200] train loss: 1.0968, train acc: 0.5425, val loss: 1.1051, val acc: 0.5376  (best train acc: 0.5486, best val acc: 0.5444, best train loss: 1.0883  @ epoch 11028 )\n",
      "[Epoch: 11220] train loss: 1.1053, train acc: 0.5401, val loss: 1.1036, val acc: 0.5393  (best train acc: 0.5486, best val acc: 0.5444, best train loss: 1.0883  @ epoch 11028 )\n",
      "[Epoch: 11240] train loss: 1.1018, train acc: 0.5405, val loss: 1.1030, val acc: 0.5393  (best train acc: 0.5486, best val acc: 0.5444, best train loss: 1.0883  @ epoch 11028 )\n",
      "[Epoch: 11260] train loss: 1.0985, train acc: 0.5390, val loss: 1.1026, val acc: 0.5433  (best train acc: 0.5486, best val acc: 0.5444, best train loss: 1.0883  @ epoch 11028 )\n",
      "[Epoch: 11280] train loss: 1.0983, train acc: 0.5428, val loss: 1.1035, val acc: 0.5403  (best train acc: 0.5486, best val acc: 0.5444, best train loss: 1.0883  @ epoch 11028 )\n",
      "[Epoch: 11300] train loss: 1.0965, train acc: 0.5414, val loss: 1.1018, val acc: 0.5410  (best train acc: 0.5486, best val acc: 0.5444, best train loss: 1.0883  @ epoch 11028 )\n",
      "[Epoch: 11320] train loss: 1.0920, train acc: 0.5412, val loss: 1.1034, val acc: 0.5386  (best train acc: 0.5489, best val acc: 0.5444, best train loss: 1.0862  @ epoch 11314 )\n",
      "[Epoch: 11340] train loss: 1.0961, train acc: 0.5421, val loss: 1.1018, val acc: 0.5406  (best train acc: 0.5489, best val acc: 0.5444, best train loss: 1.0862  @ epoch 11314 )\n",
      "[Epoch: 11360] train loss: 1.1020, train acc: 0.5361, val loss: 1.1021, val acc: 0.5393  (best train acc: 0.5489, best val acc: 0.5444, best train loss: 1.0862  @ epoch 11314 )\n",
      "[Epoch: 11380] train loss: 1.1008, train acc: 0.5361, val loss: 1.1020, val acc: 0.5329  (best train acc: 0.5489, best val acc: 0.5444, best train loss: 1.0862  @ epoch 11314 )\n",
      "[Epoch: 11400] train loss: 1.1027, train acc: 0.5383, val loss: 1.1003, val acc: 0.5403  (best train acc: 0.5489, best val acc: 0.5444, best train loss: 1.0862  @ epoch 11314 )\n",
      "[Epoch: 11420] train loss: 1.1004, train acc: 0.5395, val loss: 1.1000, val acc: 0.5406  (best train acc: 0.5489, best val acc: 0.5444, best train loss: 1.0862  @ epoch 11314 )\n",
      "[Epoch: 11440] train loss: 1.1026, train acc: 0.5397, val loss: 1.0993, val acc: 0.5420  (best train acc: 0.5489, best val acc: 0.5444, best train loss: 1.0862  @ epoch 11314 )\n",
      "[Epoch: 11460] train loss: 1.1079, train acc: 0.5357, val loss: 1.0990, val acc: 0.5437  (best train acc: 0.5489, best val acc: 0.5444, best train loss: 1.0862  @ epoch 11314 )\n",
      "[Epoch: 11480] train loss: 1.1054, train acc: 0.5377, val loss: 1.1000, val acc: 0.5386  (best train acc: 0.5489, best val acc: 0.5444, best train loss: 1.0862  @ epoch 11314 )\n",
      "[Epoch: 11500] train loss: 1.0977, train acc: 0.5406, val loss: 1.0993, val acc: 0.5403  (best train acc: 0.5489, best val acc: 0.5444, best train loss: 1.0859  @ epoch 11490 )\n",
      "[Epoch: 11520] train loss: 1.0895, train acc: 0.5450, val loss: 1.0990, val acc: 0.5396  (best train acc: 0.5489, best val acc: 0.5444, best train loss: 1.0859  @ epoch 11490 )\n",
      "[Epoch: 11540] train loss: 1.0989, train acc: 0.5419, val loss: 1.0999, val acc: 0.5359  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0783  @ epoch 11529 )\n",
      "[Epoch: 11560] train loss: 1.0971, train acc: 0.5399, val loss: 1.0982, val acc: 0.5413  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0783  @ epoch 11529 )\n",
      "[Epoch: 11580] train loss: 1.1085, train acc: 0.5334, val loss: 1.0976, val acc: 0.5386  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0783  @ epoch 11529 )\n",
      "[Epoch: 11600] train loss: 1.1011, train acc: 0.5358, val loss: 1.0975, val acc: 0.5383  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0783  @ epoch 11529 )\n",
      "[Epoch: 11620] train loss: 1.0912, train acc: 0.5437, val loss: 1.0977, val acc: 0.5393  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0783  @ epoch 11529 )\n",
      "[Epoch: 11640] train loss: 1.0884, train acc: 0.5455, val loss: 1.0988, val acc: 0.5400  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0783  @ epoch 11529 )\n",
      "[Epoch: 11660] train loss: 1.1003, train acc: 0.5401, val loss: 1.0969, val acc: 0.5366  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0783  @ epoch 11529 )\n",
      "[Epoch: 11680] train loss: 1.0970, train acc: 0.5378, val loss: 1.0969, val acc: 0.5383  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0783  @ epoch 11529 )\n",
      "[Epoch: 11700] train loss: 1.0974, train acc: 0.5373, val loss: 1.0958, val acc: 0.5379  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0783  @ epoch 11529 )\n",
      "[Epoch: 11720] train loss: 1.0946, train acc: 0.5419, val loss: 1.0951, val acc: 0.5423  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0783  @ epoch 11529 )\n",
      "[Epoch: 11740] train loss: 1.0969, train acc: 0.5342, val loss: 1.0952, val acc: 0.5393  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0783  @ epoch 11529 )\n",
      "[Epoch: 11760] train loss: 1.0986, train acc: 0.5419, val loss: 1.0945, val acc: 0.5413  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0783  @ epoch 11529 )\n",
      "[Epoch: 11780] train loss: 1.0804, train acc: 0.5452, val loss: 1.0971, val acc: 0.5336  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0783  @ epoch 11529 )\n",
      "[Epoch: 11800] train loss: 1.1075, train acc: 0.5381, val loss: 1.0940, val acc: 0.5396  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0783  @ epoch 11529 )\n",
      "[Epoch: 11820] train loss: 1.0877, train acc: 0.5452, val loss: 1.0942, val acc: 0.5386  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 11840] train loss: 1.0907, train acc: 0.5356, val loss: 1.0932, val acc: 0.5396  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 11860] train loss: 1.0777, train acc: 0.5444, val loss: 1.0930, val acc: 0.5413  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 11880] train loss: 1.0869, train acc: 0.5468, val loss: 1.0928, val acc: 0.5383  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 11900] train loss: 1.0875, train acc: 0.5414, val loss: 1.0950, val acc: 0.5433  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 11920] train loss: 1.1019, train acc: 0.5390, val loss: 1.0911, val acc: 0.5410  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 11940] train loss: 1.0923, train acc: 0.5406, val loss: 1.0924, val acc: 0.5383  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 11960] train loss: 1.0928, train acc: 0.5427, val loss: 1.0942, val acc: 0.5433  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 11980] train loss: 1.8046, train acc: 0.3654, val loss: 1.5873, val acc: 0.4000  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 12000] train loss: 1.2954, train acc: 0.4187, val loss: 1.2855, val acc: 0.4226  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 12020] train loss: 1.2155, train acc: 0.4910, val loss: 1.2151, val acc: 0.4890  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 12040] train loss: 1.2031, train acc: 0.4983, val loss: 1.2071, val acc: 0.5019  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 12060] train loss: 1.2061, train acc: 0.4963, val loss: 1.2044, val acc: 0.4961  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 12080] train loss: 1.1913, train acc: 0.5034, val loss: 1.2025, val acc: 0.4924  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 12100] train loss: 1.1956, train acc: 0.4975, val loss: 1.1996, val acc: 0.4941  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 12120] train loss: 1.1940, train acc: 0.5020, val loss: 1.1977, val acc: 0.4988  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 12140] train loss: 1.1881, train acc: 0.4999, val loss: 1.1959, val acc: 0.4988  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 12160] train loss: 1.1856, train acc: 0.5001, val loss: 1.1941, val acc: 0.4917  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 12180] train loss: 1.1870, train acc: 0.4991, val loss: 1.1932, val acc: 0.4948  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 12200] train loss: 1.1894, train acc: 0.4978, val loss: 1.1915, val acc: 0.4924  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 12220] train loss: 1.1864, train acc: 0.4997, val loss: 1.1908, val acc: 0.4958  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 12240] train loss: 1.1840, train acc: 0.4981, val loss: 1.1895, val acc: 0.4924  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 12260] train loss: 1.1845, train acc: 0.5020, val loss: 1.1889, val acc: 0.4931  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 12280] train loss: 1.1863, train acc: 0.4978, val loss: 1.1876, val acc: 0.4921  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 12300] train loss: 1.1826, train acc: 0.4996, val loss: 1.1871, val acc: 0.4951  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 12320] train loss: 1.1752, train acc: 0.5034, val loss: 1.1863, val acc: 0.4954  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 12340] train loss: 1.1810, train acc: 0.4991, val loss: 1.1852, val acc: 0.4934  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 12360] train loss: 1.1811, train acc: 0.5018, val loss: 1.1840, val acc: 0.4931  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 12380] train loss: 1.1747, train acc: 0.5001, val loss: 1.1834, val acc: 0.4948  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 12400] train loss: 1.1741, train acc: 0.5033, val loss: 1.1825, val acc: 0.4931  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 12420] train loss: 1.1663, train acc: 0.5072, val loss: 1.1814, val acc: 0.4944  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 12440] train loss: 1.1749, train acc: 0.5076, val loss: 1.1804, val acc: 0.4944  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 12460] train loss: 1.1725, train acc: 0.5075, val loss: 1.1795, val acc: 0.4965  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 12480] train loss: 1.1786, train acc: 0.5014, val loss: 1.1783, val acc: 0.4975  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 12500] train loss: 1.1662, train acc: 0.5098, val loss: 1.1773, val acc: 0.5002  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 12520] train loss: 1.1613, train acc: 0.5124, val loss: 1.1762, val acc: 0.5042  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 12540] train loss: 1.1668, train acc: 0.5085, val loss: 1.1747, val acc: 0.5005  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 12560] train loss: 1.1681, train acc: 0.5095, val loss: 1.1735, val acc: 0.5056  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 12580] train loss: 1.1673, train acc: 0.5114, val loss: 1.1721, val acc: 0.5035  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 12600] train loss: 1.1614, train acc: 0.5131, val loss: 1.1710, val acc: 0.5096  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 12620] train loss: 1.1622, train acc: 0.5133, val loss: 1.1695, val acc: 0.5049  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 12640] train loss: 1.1682, train acc: 0.5114, val loss: 1.1679, val acc: 0.5086  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 12660] train loss: 1.1624, train acc: 0.5152, val loss: 1.1672, val acc: 0.5120  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 12680] train loss: 1.1678, train acc: 0.5116, val loss: 1.1651, val acc: 0.5126  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 12700] train loss: 1.1553, train acc: 0.5209, val loss: 1.1640, val acc: 0.5093  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 12720] train loss: 1.1547, train acc: 0.5228, val loss: 1.1625, val acc: 0.5194  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 12740] train loss: 1.1596, train acc: 0.5208, val loss: 1.1613, val acc: 0.5116  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 12760] train loss: 1.1625, train acc: 0.5194, val loss: 1.1594, val acc: 0.5143  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 12780] train loss: 1.1597, train acc: 0.5197, val loss: 1.1582, val acc: 0.5164  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 12800] train loss: 1.1546, train acc: 0.5199, val loss: 1.1570, val acc: 0.5167  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 12820] train loss: 1.1617, train acc: 0.5181, val loss: 1.1561, val acc: 0.5170  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 12840] train loss: 1.1522, train acc: 0.5245, val loss: 1.1551, val acc: 0.5174  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 12860] train loss: 1.1530, train acc: 0.5216, val loss: 1.1539, val acc: 0.5197  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 12880] train loss: 1.1491, train acc: 0.5247, val loss: 1.1533, val acc: 0.5234  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 12900] train loss: 1.1600, train acc: 0.5175, val loss: 1.1580, val acc: 0.5086  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 12920] train loss: 1.1496, train acc: 0.5249, val loss: 1.1513, val acc: 0.5238  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 12940] train loss: 1.1477, train acc: 0.5262, val loss: 1.1576, val acc: 0.5099  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 12960] train loss: 1.1449, train acc: 0.5248, val loss: 1.1497, val acc: 0.5251  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 12980] train loss: 1.1527, train acc: 0.5244, val loss: 1.1495, val acc: 0.5228  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 13000] train loss: 1.1577, train acc: 0.5134, val loss: 1.1503, val acc: 0.5164  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 13020] train loss: 1.1463, train acc: 0.5239, val loss: 1.1501, val acc: 0.5288  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 13040] train loss: 1.1482, train acc: 0.5238, val loss: 1.1462, val acc: 0.5268  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 13060] train loss: 1.1426, train acc: 0.5273, val loss: 1.1475, val acc: 0.5282  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 13080] train loss: 1.1497, train acc: 0.5230, val loss: 1.1478, val acc: 0.5258  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 13100] train loss: 1.1423, train acc: 0.5299, val loss: 1.1500, val acc: 0.5295  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 13120] train loss: 1.1461, train acc: 0.5244, val loss: 1.1442, val acc: 0.5288  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 13140] train loss: 1.1435, train acc: 0.5271, val loss: 1.1441, val acc: 0.5288  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 13160] train loss: 1.1446, train acc: 0.5221, val loss: 1.1439, val acc: 0.5258  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 13180] train loss: 1.1471, train acc: 0.5257, val loss: 1.1429, val acc: 0.5295  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 13200] train loss: 1.1427, train acc: 0.5221, val loss: 1.1432, val acc: 0.5282  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 13220] train loss: 1.1359, train acc: 0.5266, val loss: 1.1423, val acc: 0.5295  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 13240] train loss: 1.1393, train acc: 0.5244, val loss: 1.1430, val acc: 0.5261  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 13260] train loss: 1.1442, train acc: 0.5257, val loss: 1.1420, val acc: 0.5278  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 13280] train loss: 1.1419, train acc: 0.5271, val loss: 1.1415, val acc: 0.5288  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 13300] train loss: 1.1304, train acc: 0.5262, val loss: 1.1458, val acc: 0.5268  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 13320] train loss: 1.1410, train acc: 0.5265, val loss: 1.1411, val acc: 0.5292  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 13340] train loss: 1.1549, train acc: 0.5201, val loss: 1.1445, val acc: 0.5221  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 13360] train loss: 1.1364, train acc: 0.5270, val loss: 1.1412, val acc: 0.5282  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 13380] train loss: 1.1573, train acc: 0.5121, val loss: 1.1450, val acc: 0.5201  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 13400] train loss: 1.1468, train acc: 0.5224, val loss: 1.1427, val acc: 0.5282  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 13420] train loss: 1.1419, train acc: 0.5243, val loss: 1.1421, val acc: 0.5278  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 13440] train loss: 1.1457, train acc: 0.5267, val loss: 1.1398, val acc: 0.5309  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 13460] train loss: 1.1368, train acc: 0.5283, val loss: 1.1413, val acc: 0.5241  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 13480] train loss: 1.1411, train acc: 0.5280, val loss: 1.1407, val acc: 0.5278  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 13500] train loss: 1.1423, train acc: 0.5235, val loss: 1.1398, val acc: 0.5292  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 13520] train loss: 1.1431, train acc: 0.5231, val loss: 1.1392, val acc: 0.5315  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 13540] train loss: 1.1465, train acc: 0.5208, val loss: 1.1391, val acc: 0.5319  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 13560] train loss: 1.1361, train acc: 0.5296, val loss: 1.1419, val acc: 0.5305  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 13580] train loss: 1.1471, train acc: 0.5167, val loss: 1.1514, val acc: 0.5120  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 13600] train loss: 1.1340, train acc: 0.5268, val loss: 1.1449, val acc: 0.5174  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 13620] train loss: 1.1378, train acc: 0.5272, val loss: 1.1386, val acc: 0.5298  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 13640] train loss: 1.1419, train acc: 0.5253, val loss: 1.1425, val acc: 0.5224  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 13660] train loss: 1.1377, train acc: 0.5265, val loss: 1.1380, val acc: 0.5302  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 13680] train loss: 1.1370, train acc: 0.5296, val loss: 1.1408, val acc: 0.5278  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 13700] train loss: 1.1543, train acc: 0.5123, val loss: 1.1529, val acc: 0.5177  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 13720] train loss: 1.1342, train acc: 0.5285, val loss: 1.1388, val acc: 0.5309  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 13740] train loss: 1.1349, train acc: 0.5337, val loss: 1.1394, val acc: 0.5282  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 13760] train loss: 1.1300, train acc: 0.5292, val loss: 1.1370, val acc: 0.5305  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 13780] train loss: 1.1438, train acc: 0.5262, val loss: 1.1386, val acc: 0.5248  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 13800] train loss: 1.1296, train acc: 0.5294, val loss: 1.1376, val acc: 0.5278  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 13820] train loss: 1.1397, train acc: 0.5278, val loss: 1.1372, val acc: 0.5315  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 13840] train loss: 1.1373, train acc: 0.5314, val loss: 1.1368, val acc: 0.5309  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 13860] train loss: 1.1437, train acc: 0.5250, val loss: 1.1391, val acc: 0.5285  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 13880] train loss: 1.1368, train acc: 0.5270, val loss: 1.1399, val acc: 0.5302  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 13900] train loss: 1.1407, train acc: 0.5267, val loss: 1.1433, val acc: 0.5295  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 13920] train loss: 1.1398, train acc: 0.5275, val loss: 1.1367, val acc: 0.5305  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 13940] train loss: 1.1347, train acc: 0.5250, val loss: 1.1361, val acc: 0.5309  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 13960] train loss: 1.1347, train acc: 0.5282, val loss: 1.1432, val acc: 0.5278  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 13980] train loss: 1.1379, train acc: 0.5281, val loss: 1.1370, val acc: 0.5292  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 14000] train loss: 1.1334, train acc: 0.5289, val loss: 1.1398, val acc: 0.5241  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 14020] train loss: 1.1384, train acc: 0.5236, val loss: 1.1363, val acc: 0.5295  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 14040] train loss: 1.1393, train acc: 0.5197, val loss: 1.1353, val acc: 0.5309  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 14060] train loss: 1.1351, train acc: 0.5271, val loss: 1.1397, val acc: 0.5231  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 14080] train loss: 1.1364, train acc: 0.5260, val loss: 1.1448, val acc: 0.5197  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 14100] train loss: 1.1355, train acc: 0.5260, val loss: 1.1463, val acc: 0.5272  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 14120] train loss: 1.1351, train acc: 0.5312, val loss: 1.1366, val acc: 0.5288  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 14140] train loss: 1.1375, train acc: 0.5281, val loss: 1.1349, val acc: 0.5292  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 14160] train loss: 1.1436, train acc: 0.5223, val loss: 1.1374, val acc: 0.5241  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 14180] train loss: 1.1380, train acc: 0.5264, val loss: 1.1494, val acc: 0.5157  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 14200] train loss: 1.1385, train acc: 0.5270, val loss: 1.1358, val acc: 0.5315  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 14220] train loss: 1.1331, train acc: 0.5345, val loss: 1.1355, val acc: 0.5275  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 14240] train loss: 1.1357, train acc: 0.5270, val loss: 1.1354, val acc: 0.5275  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 14260] train loss: 1.1491, train acc: 0.5248, val loss: 1.1674, val acc: 0.5120  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 14280] train loss: 1.1399, train acc: 0.5248, val loss: 1.1377, val acc: 0.5272  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 14300] train loss: 1.1398, train acc: 0.5279, val loss: 1.1358, val acc: 0.5282  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 14320] train loss: 1.1467, train acc: 0.5174, val loss: 1.1460, val acc: 0.5255  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 14340] train loss: 1.1374, train acc: 0.5266, val loss: 1.1355, val acc: 0.5305  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 14360] train loss: 1.1283, train acc: 0.5301, val loss: 1.1343, val acc: 0.5305  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 14380] train loss: 1.1622, train acc: 0.4970, val loss: 1.1526, val acc: 0.5116  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 14400] train loss: 1.1498, train acc: 0.5194, val loss: 1.1483, val acc: 0.5197  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 14420] train loss: 1.1432, train acc: 0.5166, val loss: 1.1355, val acc: 0.5275  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 14440] train loss: 1.1411, train acc: 0.5312, val loss: 1.1355, val acc: 0.5312  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 14460] train loss: 1.1334, train acc: 0.5250, val loss: 1.1340, val acc: 0.5315  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 14480] train loss: 1.1413, train acc: 0.5243, val loss: 1.1334, val acc: 0.5319  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 14500] train loss: 1.1308, train acc: 0.5306, val loss: 1.1329, val acc: 0.5309  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 14520] train loss: 1.1320, train acc: 0.5318, val loss: 1.1345, val acc: 0.5275  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 14540] train loss: 1.1275, train acc: 0.5269, val loss: 1.1326, val acc: 0.5288  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 14560] train loss: 1.1339, train acc: 0.5278, val loss: 1.1328, val acc: 0.5329  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 14580] train loss: 1.1299, train acc: 0.5296, val loss: 1.1371, val acc: 0.5275  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 14600] train loss: 1.1379, train acc: 0.5254, val loss: 1.1330, val acc: 0.5285  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 14620] train loss: 1.1232, train acc: 0.5288, val loss: 1.1366, val acc: 0.5255  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 14640] train loss: 1.1379, train acc: 0.5231, val loss: 1.1349, val acc: 0.5312  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 14660] train loss: 1.1406, train acc: 0.5294, val loss: 1.1341, val acc: 0.5305  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 14680] train loss: 1.1369, train acc: 0.5272, val loss: 1.1330, val acc: 0.5285  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 14700] train loss: 1.1410, train acc: 0.5301, val loss: 1.1339, val acc: 0.5261  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 14720] train loss: 1.1344, train acc: 0.5268, val loss: 1.1312, val acc: 0.5319  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 14740] train loss: 1.1447, train acc: 0.5226, val loss: 1.1320, val acc: 0.5285  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 14760] train loss: 1.1257, train acc: 0.5312, val loss: 1.1343, val acc: 0.5282  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 14780] train loss: 1.1438, train acc: 0.5229, val loss: 1.1360, val acc: 0.5248  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 14800] train loss: 1.1355, train acc: 0.5263, val loss: 1.1314, val acc: 0.5302  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 14820] train loss: 1.1456, train acc: 0.5192, val loss: 1.1438, val acc: 0.5204  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 14840] train loss: 1.1355, train acc: 0.5246, val loss: 1.1411, val acc: 0.5170  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 14860] train loss: 1.1416, train acc: 0.5191, val loss: 1.1337, val acc: 0.5336  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 14880] train loss: 1.1276, train acc: 0.5315, val loss: 1.1305, val acc: 0.5309  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 14900] train loss: 1.1279, train acc: 0.5291, val loss: 1.1316, val acc: 0.5292  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 14920] train loss: 1.1318, train acc: 0.5251, val loss: 1.1312, val acc: 0.5288  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 14940] train loss: 1.1279, train acc: 0.5278, val loss: 1.1306, val acc: 0.5302  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 14960] train loss: 1.1232, train acc: 0.5316, val loss: 1.1316, val acc: 0.5302  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 14980] train loss: 1.1333, train acc: 0.5275, val loss: 1.1326, val acc: 0.5261  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 15000] train loss: 1.1318, train acc: 0.5301, val loss: 1.1296, val acc: 0.5339  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 15020] train loss: 1.1270, train acc: 0.5338, val loss: 1.1303, val acc: 0.5298  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 15040] train loss: 1.1384, train acc: 0.5234, val loss: 1.1317, val acc: 0.5322  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 15060] train loss: 1.1336, train acc: 0.5267, val loss: 1.1294, val acc: 0.5339  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 15080] train loss: 1.1544, train acc: 0.5073, val loss: 1.1629, val acc: 0.4938  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 15100] train loss: 1.1628, train acc: 0.5121, val loss: 1.1631, val acc: 0.5130  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 15120] train loss: 1.1351, train acc: 0.5303, val loss: 1.1383, val acc: 0.5258  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 15140] train loss: 1.1353, train acc: 0.5303, val loss: 1.1377, val acc: 0.5325  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 15160] train loss: 1.1351, train acc: 0.5270, val loss: 1.1379, val acc: 0.5255  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 15180] train loss: 1.1433, train acc: 0.5204, val loss: 1.1329, val acc: 0.5319  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 15200] train loss: 1.1347, train acc: 0.5307, val loss: 1.1350, val acc: 0.5329  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 15220] train loss: 1.1259, train acc: 0.5339, val loss: 1.1297, val acc: 0.5322  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 15240] train loss: 1.1256, train acc: 0.5332, val loss: 1.1301, val acc: 0.5336  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 15260] train loss: 1.1311, train acc: 0.5321, val loss: 1.1288, val acc: 0.5305  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 15280] train loss: 1.1325, train acc: 0.5254, val loss: 1.1287, val acc: 0.5302  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 15300] train loss: 1.1532, train acc: 0.5140, val loss: 1.1548, val acc: 0.5039  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 15320] train loss: 1.1304, train acc: 0.5278, val loss: 1.1352, val acc: 0.5292  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 15340] train loss: 1.1243, train acc: 0.5307, val loss: 1.1296, val acc: 0.5356  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 15360] train loss: 1.1355, train acc: 0.5275, val loss: 1.1345, val acc: 0.5224  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 15380] train loss: 1.1467, train acc: 0.5173, val loss: 1.1408, val acc: 0.5272  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 15400] train loss: 1.1313, train acc: 0.5281, val loss: 1.1302, val acc: 0.5332  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 15420] train loss: 1.1452, train acc: 0.5206, val loss: 1.1284, val acc: 0.5339  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 15440] train loss: 1.1334, train acc: 0.5348, val loss: 1.1298, val acc: 0.5322  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 15460] train loss: 1.1281, train acc: 0.5325, val loss: 1.1287, val acc: 0.5342  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 15480] train loss: 1.1307, train acc: 0.5344, val loss: 1.1292, val acc: 0.5288  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 15500] train loss: 1.1357, train acc: 0.5274, val loss: 1.1294, val acc: 0.5292  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 15520] train loss: 1.1313, train acc: 0.5270, val loss: 1.1271, val acc: 0.5339  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 15540] train loss: 1.1367, train acc: 0.5269, val loss: 1.1280, val acc: 0.5329  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 15560] train loss: 1.1311, train acc: 0.5243, val loss: 1.1317, val acc: 0.5241  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 15580] train loss: 1.1256, train acc: 0.5295, val loss: 1.1307, val acc: 0.5278  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 15600] train loss: 1.1275, train acc: 0.5250, val loss: 1.1349, val acc: 0.5349  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 15620] train loss: 1.1530, train acc: 0.5221, val loss: 1.1588, val acc: 0.5177  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 15640] train loss: 1.1337, train acc: 0.5281, val loss: 1.1302, val acc: 0.5228  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 15660] train loss: 1.1210, train acc: 0.5359, val loss: 1.1290, val acc: 0.5383  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 15680] train loss: 1.1312, train acc: 0.5253, val loss: 1.1286, val acc: 0.5339  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 15700] train loss: 1.1277, train acc: 0.5320, val loss: 1.1268, val acc: 0.5373  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 15720] train loss: 1.1211, train acc: 0.5324, val loss: 1.1262, val acc: 0.5366  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 15740] train loss: 1.1259, train acc: 0.5296, val loss: 1.1262, val acc: 0.5312  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 15760] train loss: 1.1190, train acc: 0.5345, val loss: 1.1307, val acc: 0.5255  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 15780] train loss: 1.1372, train acc: 0.5273, val loss: 1.1328, val acc: 0.5336  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 15800] train loss: 1.1270, train acc: 0.5272, val loss: 1.1348, val acc: 0.5201  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 15820] train loss: 1.1289, train acc: 0.5236, val loss: 1.1339, val acc: 0.5218  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 15840] train loss: 1.1218, train acc: 0.5324, val loss: 1.1276, val acc: 0.5325  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 15860] train loss: 1.1246, train acc: 0.5294, val loss: 1.1351, val acc: 0.5342  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 15880] train loss: 1.1259, train acc: 0.5338, val loss: 1.1254, val acc: 0.5325  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 15900] train loss: 1.1278, train acc: 0.5262, val loss: 1.1261, val acc: 0.5329  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 15920] train loss: 1.1310, train acc: 0.5316, val loss: 1.1248, val acc: 0.5363  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 15940] train loss: 1.1287, train acc: 0.5338, val loss: 1.1244, val acc: 0.5339  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 15960] train loss: 1.1289, train acc: 0.5292, val loss: 1.1277, val acc: 0.5288  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 15980] train loss: 1.1256, train acc: 0.5320, val loss: 1.1255, val acc: 0.5329  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 16000] train loss: 1.1300, train acc: 0.5288, val loss: 1.1238, val acc: 0.5352  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 16020] train loss: 1.1233, train acc: 0.5327, val loss: 1.1241, val acc: 0.5356  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 16040] train loss: 1.1215, train acc: 0.5348, val loss: 1.1238, val acc: 0.5352  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 16060] train loss: 1.1248, train acc: 0.5308, val loss: 1.1248, val acc: 0.5349  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 16080] train loss: 1.1460, train acc: 0.5134, val loss: 1.1496, val acc: 0.5089  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 16100] train loss: 1.1290, train acc: 0.5290, val loss: 1.1304, val acc: 0.5332  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 16120] train loss: 1.1747, train acc: 0.4871, val loss: 1.1783, val acc: 0.4887  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 16140] train loss: 1.1510, train acc: 0.5194, val loss: 1.1601, val acc: 0.5076  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 16160] train loss: 1.1519, train acc: 0.5187, val loss: 1.1546, val acc: 0.5174  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 16180] train loss: 1.1495, train acc: 0.5179, val loss: 1.1507, val acc: 0.5164  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 16200] train loss: 1.1477, train acc: 0.5207, val loss: 1.1481, val acc: 0.5170  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 16220] train loss: 1.1237, train acc: 0.5351, val loss: 1.1293, val acc: 0.5336  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 16240] train loss: 1.1279, train acc: 0.5336, val loss: 1.1270, val acc: 0.5268  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 16260] train loss: 1.1159, train acc: 0.5377, val loss: 1.1245, val acc: 0.5346  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 16280] train loss: 1.1245, train acc: 0.5317, val loss: 1.1331, val acc: 0.5268  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 16300] train loss: 1.1309, train acc: 0.5277, val loss: 1.1253, val acc: 0.5352  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 16320] train loss: 1.1359, train acc: 0.5185, val loss: 1.1255, val acc: 0.5386  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 16340] train loss: 1.1368, train acc: 0.5234, val loss: 1.1284, val acc: 0.5336  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 16360] train loss: 1.1310, train acc: 0.5282, val loss: 1.1239, val acc: 0.5359  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 16380] train loss: 1.1242, train acc: 0.5346, val loss: 1.1343, val acc: 0.5238  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 16400] train loss: 1.1258, train acc: 0.5294, val loss: 1.1325, val acc: 0.5238  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 16420] train loss: 1.1181, train acc: 0.5364, val loss: 1.1266, val acc: 0.5390  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 16440] train loss: 1.1267, train acc: 0.5343, val loss: 1.1257, val acc: 0.5329  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 16460] train loss: 1.1257, train acc: 0.5269, val loss: 1.1222, val acc: 0.5373  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 16480] train loss: 1.1520, train acc: 0.5207, val loss: 1.1478, val acc: 0.5207  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 16500] train loss: 1.1191, train acc: 0.5351, val loss: 1.1299, val acc: 0.5255  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 16520] train loss: 1.1395, train acc: 0.5257, val loss: 1.1415, val acc: 0.5160  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 16540] train loss: 1.1365, train acc: 0.5279, val loss: 1.1489, val acc: 0.5180  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 16560] train loss: 1.1205, train acc: 0.5348, val loss: 1.1269, val acc: 0.5336  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 16580] train loss: 1.1506, train acc: 0.5150, val loss: 1.1550, val acc: 0.5110  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 16600] train loss: 1.1522, train acc: 0.5189, val loss: 1.1451, val acc: 0.5180  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 16620] train loss: 1.1389, train acc: 0.5300, val loss: 1.1404, val acc: 0.5275  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 16640] train loss: 1.1311, train acc: 0.5281, val loss: 1.1230, val acc: 0.5342  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 16660] train loss: 1.1358, train acc: 0.5291, val loss: 1.1333, val acc: 0.5410  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 16680] train loss: 1.1246, train acc: 0.5338, val loss: 1.1239, val acc: 0.5369  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 16700] train loss: 1.1268, train acc: 0.5322, val loss: 1.1324, val acc: 0.5218  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 16720] train loss: 1.1352, train acc: 0.5277, val loss: 1.1220, val acc: 0.5373  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 16740] train loss: 1.1200, train acc: 0.5317, val loss: 1.1349, val acc: 0.5258  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 16760] train loss: 1.1240, train acc: 0.5361, val loss: 1.1241, val acc: 0.5329  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 16780] train loss: 1.1259, train acc: 0.5334, val loss: 1.1291, val acc: 0.5312  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 16800] train loss: 1.1204, train acc: 0.5349, val loss: 1.1221, val acc: 0.5359  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 16820] train loss: 1.1237, train acc: 0.5309, val loss: 1.1199, val acc: 0.5390  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 16840] train loss: 1.1297, train acc: 0.5330, val loss: 1.1251, val acc: 0.5373  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 16860] train loss: 1.1229, train acc: 0.5343, val loss: 1.1267, val acc: 0.5369  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 16880] train loss: 1.1259, train acc: 0.5270, val loss: 1.1335, val acc: 0.5218  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 16900] train loss: 1.1474, train acc: 0.5266, val loss: 1.1409, val acc: 0.5309  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 16920] train loss: 1.1299, train acc: 0.5316, val loss: 1.1203, val acc: 0.5363  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 16940] train loss: 1.1171, train acc: 0.5330, val loss: 1.1217, val acc: 0.5413  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 16960] train loss: 1.1145, train acc: 0.5346, val loss: 1.1254, val acc: 0.5390  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 16980] train loss: 1.1258, train acc: 0.5367, val loss: 1.1201, val acc: 0.5373  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 17000] train loss: 1.1254, train acc: 0.5296, val loss: 1.1194, val acc: 0.5386  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 17020] train loss: 1.1231, train acc: 0.5354, val loss: 1.1265, val acc: 0.5373  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 17040] train loss: 1.1255, train acc: 0.5309, val loss: 1.1235, val acc: 0.5329  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 17060] train loss: 1.1240, train acc: 0.5382, val loss: 1.1191, val acc: 0.5406  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 17080] train loss: 1.1170, train acc: 0.5415, val loss: 1.1220, val acc: 0.5413  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 17100] train loss: 1.1317, train acc: 0.5227, val loss: 1.1410, val acc: 0.5214  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 17120] train loss: 1.1277, train acc: 0.5339, val loss: 1.1302, val acc: 0.5366  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 17140] train loss: 1.1365, train acc: 0.5283, val loss: 1.1215, val acc: 0.5403  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 17160] train loss: 1.1239, train acc: 0.5364, val loss: 1.1305, val acc: 0.5292  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 17180] train loss: 1.1176, train acc: 0.5361, val loss: 1.1196, val acc: 0.5366  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 17200] train loss: 1.1291, train acc: 0.5336, val loss: 1.1345, val acc: 0.5268  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 17220] train loss: 1.1204, train acc: 0.5401, val loss: 1.1190, val acc: 0.5400  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 17240] train loss: 1.1110, train acc: 0.5396, val loss: 1.1188, val acc: 0.5332  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 17260] train loss: 1.1121, train acc: 0.5359, val loss: 1.1175, val acc: 0.5383  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 17280] train loss: 1.1289, train acc: 0.5254, val loss: 1.1456, val acc: 0.5150  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 17300] train loss: 1.1316, train acc: 0.5298, val loss: 1.1239, val acc: 0.5373  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 17320] train loss: 1.1215, train acc: 0.5382, val loss: 1.1249, val acc: 0.5322  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 17340] train loss: 1.1429, train acc: 0.5235, val loss: 1.1424, val acc: 0.5268  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 17360] train loss: 1.1241, train acc: 0.5346, val loss: 1.1376, val acc: 0.5268  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 17380] train loss: 1.1163, train acc: 0.5366, val loss: 1.1176, val acc: 0.5386  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 17400] train loss: 1.1227, train acc: 0.5286, val loss: 1.1205, val acc: 0.5339  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 17420] train loss: 1.1138, train acc: 0.5361, val loss: 1.1209, val acc: 0.5373  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 17440] train loss: 1.1223, train acc: 0.5354, val loss: 1.1203, val acc: 0.5346  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 17460] train loss: 1.1247, train acc: 0.5334, val loss: 1.1175, val acc: 0.5433  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 17480] train loss: 1.1281, train acc: 0.5304, val loss: 1.1162, val acc: 0.5400  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 17500] train loss: 1.1208, train acc: 0.5396, val loss: 1.1153, val acc: 0.5390  (best train acc: 0.5536, best val acc: 0.5444, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 17520] train loss: 1.1196, train acc: 0.5390, val loss: 1.1147, val acc: 0.5423  (best train acc: 0.5536, best val acc: 0.5447, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 17540] train loss: 1.1166, train acc: 0.5372, val loss: 1.1163, val acc: 0.5349  (best train acc: 0.5536, best val acc: 0.5447, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 17560] train loss: 1.1083, train acc: 0.5377, val loss: 1.1224, val acc: 0.5410  (best train acc: 0.5536, best val acc: 0.5447, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 17580] train loss: 1.1250, train acc: 0.5298, val loss: 1.1149, val acc: 0.5417  (best train acc: 0.5536, best val acc: 0.5447, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 17600] train loss: 1.1162, train acc: 0.5309, val loss: 1.1173, val acc: 0.5373  (best train acc: 0.5536, best val acc: 0.5447, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 17620] train loss: 1.1259, train acc: 0.5329, val loss: 1.1675, val acc: 0.4968  (best train acc: 0.5536, best val acc: 0.5450, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 17640] train loss: 1.1601, train acc: 0.5187, val loss: 1.1544, val acc: 0.5224  (best train acc: 0.5536, best val acc: 0.5450, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 17660] train loss: 1.1248, train acc: 0.5319, val loss: 1.1443, val acc: 0.5245  (best train acc: 0.5536, best val acc: 0.5450, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 17680] train loss: 1.1329, train acc: 0.5353, val loss: 1.1390, val acc: 0.5298  (best train acc: 0.5536, best val acc: 0.5450, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 17700] train loss: 1.1257, train acc: 0.5310, val loss: 1.1368, val acc: 0.5332  (best train acc: 0.5536, best val acc: 0.5450, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 17720] train loss: 1.1341, train acc: 0.5316, val loss: 1.1354, val acc: 0.5309  (best train acc: 0.5536, best val acc: 0.5450, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 17740] train loss: 1.1304, train acc: 0.5274, val loss: 1.1331, val acc: 0.5339  (best train acc: 0.5536, best val acc: 0.5450, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 17760] train loss: 1.1253, train acc: 0.5362, val loss: 1.1309, val acc: 0.5342  (best train acc: 0.5536, best val acc: 0.5450, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 17780] train loss: 1.1247, train acc: 0.5296, val loss: 1.1297, val acc: 0.5309  (best train acc: 0.5536, best val acc: 0.5450, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 17800] train loss: 1.1289, train acc: 0.5365, val loss: 1.1292, val acc: 0.5332  (best train acc: 0.5536, best val acc: 0.5450, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 17820] train loss: 1.1128, train acc: 0.5381, val loss: 1.1277, val acc: 0.5336  (best train acc: 0.5536, best val acc: 0.5450, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 17840] train loss: 1.1305, train acc: 0.5330, val loss: 1.1269, val acc: 0.5332  (best train acc: 0.5536, best val acc: 0.5450, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 17860] train loss: 1.1310, train acc: 0.5359, val loss: 1.1262, val acc: 0.5342  (best train acc: 0.5536, best val acc: 0.5450, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 17880] train loss: 1.1194, train acc: 0.5381, val loss: 1.1256, val acc: 0.5373  (best train acc: 0.5536, best val acc: 0.5450, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 17900] train loss: 1.1344, train acc: 0.5269, val loss: 1.1252, val acc: 0.5386  (best train acc: 0.5536, best val acc: 0.5450, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 17920] train loss: 1.1233, train acc: 0.5364, val loss: 1.1242, val acc: 0.5383  (best train acc: 0.5536, best val acc: 0.5450, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 17940] train loss: 1.1267, train acc: 0.5372, val loss: 1.1240, val acc: 0.5400  (best train acc: 0.5536, best val acc: 0.5450, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 17960] train loss: 1.1263, train acc: 0.5389, val loss: 1.1242, val acc: 0.5352  (best train acc: 0.5536, best val acc: 0.5450, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 17980] train loss: 1.1170, train acc: 0.5354, val loss: 1.1229, val acc: 0.5373  (best train acc: 0.5536, best val acc: 0.5450, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 18000] train loss: 1.1298, train acc: 0.5368, val loss: 1.1223, val acc: 0.5386  (best train acc: 0.5536, best val acc: 0.5450, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 18020] train loss: 1.1235, train acc: 0.5382, val loss: 1.1216, val acc: 0.5390  (best train acc: 0.5536, best val acc: 0.5450, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 18040] train loss: 1.1176, train acc: 0.5349, val loss: 1.1221, val acc: 0.5349  (best train acc: 0.5536, best val acc: 0.5450, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 18060] train loss: 1.1153, train acc: 0.5417, val loss: 1.1212, val acc: 0.5369  (best train acc: 0.5536, best val acc: 0.5450, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 18080] train loss: 1.1120, train acc: 0.5441, val loss: 1.1200, val acc: 0.5417  (best train acc: 0.5536, best val acc: 0.5450, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 18100] train loss: 1.1259, train acc: 0.5369, val loss: 1.1191, val acc: 0.5396  (best train acc: 0.5536, best val acc: 0.5450, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 18120] train loss: 1.1193, train acc: 0.5409, val loss: 1.1198, val acc: 0.5386  (best train acc: 0.5536, best val acc: 0.5450, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 18140] train loss: 1.1281, train acc: 0.5392, val loss: 1.1189, val acc: 0.5390  (best train acc: 0.5536, best val acc: 0.5450, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 18160] train loss: 1.1092, train acc: 0.5445, val loss: 1.1186, val acc: 0.5420  (best train acc: 0.5536, best val acc: 0.5450, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 18180] train loss: 1.1238, train acc: 0.5376, val loss: 1.1179, val acc: 0.5427  (best train acc: 0.5536, best val acc: 0.5460, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 18200] train loss: 1.1289, train acc: 0.5374, val loss: 1.1183, val acc: 0.5379  (best train acc: 0.5536, best val acc: 0.5460, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 18220] train loss: 1.1150, train acc: 0.5403, val loss: 1.1170, val acc: 0.5430  (best train acc: 0.5536, best val acc: 0.5460, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 18240] train loss: 1.1152, train acc: 0.5406, val loss: 1.1167, val acc: 0.5423  (best train acc: 0.5536, best val acc: 0.5464, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 18260] train loss: 1.1110, train acc: 0.5476, val loss: 1.1166, val acc: 0.5450  (best train acc: 0.5536, best val acc: 0.5464, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 18280] train loss: 1.1123, train acc: 0.5432, val loss: 1.1153, val acc: 0.5460  (best train acc: 0.5536, best val acc: 0.5467, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 18300] train loss: 1.1221, train acc: 0.5382, val loss: 1.1168, val acc: 0.5454  (best train acc: 0.5536, best val acc: 0.5467, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 18320] train loss: 1.1116, train acc: 0.5428, val loss: 1.1152, val acc: 0.5427  (best train acc: 0.5536, best val acc: 0.5481, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 18340] train loss: 1.1148, train acc: 0.5440, val loss: 1.1172, val acc: 0.5390  (best train acc: 0.5536, best val acc: 0.5481, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 18360] train loss: 1.1126, train acc: 0.5400, val loss: 1.1153, val acc: 0.5457  (best train acc: 0.5536, best val acc: 0.5481, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 18380] train loss: 1.1174, train acc: 0.5335, val loss: 1.1137, val acc: 0.5450  (best train acc: 0.5536, best val acc: 0.5481, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 18400] train loss: 1.1122, train acc: 0.5415, val loss: 1.1141, val acc: 0.5470  (best train acc: 0.5536, best val acc: 0.5484, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 18420] train loss: 1.1157, train acc: 0.5386, val loss: 1.1128, val acc: 0.5437  (best train acc: 0.5536, best val acc: 0.5484, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 18440] train loss: 1.1111, train acc: 0.5378, val loss: 1.1130, val acc: 0.5423  (best train acc: 0.5536, best val acc: 0.5484, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 18460] train loss: 1.1146, train acc: 0.5383, val loss: 1.1122, val acc: 0.5467  (best train acc: 0.5536, best val acc: 0.5484, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 18480] train loss: 1.1219, train acc: 0.5414, val loss: 1.1134, val acc: 0.5417  (best train acc: 0.5536, best val acc: 0.5484, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 18500] train loss: 1.1183, train acc: 0.5416, val loss: 1.1126, val acc: 0.5427  (best train acc: 0.5536, best val acc: 0.5484, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 18520] train loss: 1.1116, train acc: 0.5416, val loss: 1.1120, val acc: 0.5444  (best train acc: 0.5536, best val acc: 0.5491, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 18540] train loss: 1.1041, train acc: 0.5404, val loss: 1.1109, val acc: 0.5470  (best train acc: 0.5536, best val acc: 0.5491, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 18560] train loss: 1.1193, train acc: 0.5375, val loss: 1.1106, val acc: 0.5427  (best train acc: 0.5536, best val acc: 0.5497, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 18580] train loss: 1.1121, train acc: 0.5386, val loss: 1.1130, val acc: 0.5444  (best train acc: 0.5536, best val acc: 0.5497, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 18600] train loss: 1.1145, train acc: 0.5440, val loss: 1.1104, val acc: 0.5470  (best train acc: 0.5536, best val acc: 0.5504, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 18620] train loss: 1.1052, train acc: 0.5438, val loss: 1.1099, val acc: 0.5450  (best train acc: 0.5536, best val acc: 0.5504, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 18640] train loss: 1.1124, train acc: 0.5421, val loss: 1.1135, val acc: 0.5410  (best train acc: 0.5536, best val acc: 0.5504, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 18660] train loss: 1.1151, train acc: 0.5432, val loss: 1.1104, val acc: 0.5400  (best train acc: 0.5536, best val acc: 0.5504, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 18680] train loss: 1.1166, train acc: 0.5393, val loss: 1.1105, val acc: 0.5417  (best train acc: 0.5536, best val acc: 0.5504, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 18700] train loss: 1.1128, train acc: 0.5378, val loss: 1.1093, val acc: 0.5464  (best train acc: 0.5536, best val acc: 0.5504, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 18720] train loss: 1.1051, train acc: 0.5489, val loss: 1.1086, val acc: 0.5447  (best train acc: 0.5536, best val acc: 0.5504, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 18740] train loss: 1.1119, train acc: 0.5432, val loss: 1.1093, val acc: 0.5464  (best train acc: 0.5536, best val acc: 0.5504, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 18760] train loss: 1.1101, train acc: 0.5397, val loss: 1.1083, val acc: 0.5464  (best train acc: 0.5536, best val acc: 0.5504, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 18780] train loss: 1.1049, train acc: 0.5452, val loss: 1.1078, val acc: 0.5474  (best train acc: 0.5536, best val acc: 0.5504, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 18800] train loss: 1.1143, train acc: 0.5417, val loss: 1.1087, val acc: 0.5457  (best train acc: 0.5536, best val acc: 0.5504, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 18820] train loss: 1.1293, train acc: 0.5354, val loss: 1.1073, val acc: 0.5494  (best train acc: 0.5543, best val acc: 0.5504, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 18840] train loss: 1.1135, train acc: 0.5420, val loss: 1.1076, val acc: 0.5454  (best train acc: 0.5543, best val acc: 0.5504, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 18860] train loss: 1.1168, train acc: 0.5332, val loss: 1.1092, val acc: 0.5406  (best train acc: 0.5543, best val acc: 0.5504, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 18880] train loss: 1.1152, train acc: 0.5399, val loss: 1.1090, val acc: 0.5491  (best train acc: 0.5543, best val acc: 0.5504, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 18900] train loss: 1.1165, train acc: 0.5358, val loss: 1.1096, val acc: 0.5406  (best train acc: 0.5543, best val acc: 0.5504, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 18920] train loss: 1.1090, train acc: 0.5400, val loss: 1.1150, val acc: 0.5366  (best train acc: 0.5543, best val acc: 0.5504, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 18940] train loss: 1.1044, train acc: 0.5440, val loss: 1.1080, val acc: 0.5491  (best train acc: 0.5543, best val acc: 0.5504, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 18960] train loss: 1.1021, train acc: 0.5445, val loss: 1.1063, val acc: 0.5444  (best train acc: 0.5543, best val acc: 0.5504, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 18980] train loss: 1.0962, train acc: 0.5490, val loss: 1.1059, val acc: 0.5437  (best train acc: 0.5543, best val acc: 0.5504, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 19000] train loss: 1.1119, train acc: 0.5434, val loss: 1.1052, val acc: 0.5467  (best train acc: 0.5543, best val acc: 0.5504, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 19020] train loss: 1.1001, train acc: 0.5437, val loss: 1.1059, val acc: 0.5447  (best train acc: 0.5543, best val acc: 0.5508, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 19040] train loss: 1.1100, train acc: 0.5415, val loss: 1.1112, val acc: 0.5454  (best train acc: 0.5543, best val acc: 0.5508, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 19060] train loss: 1.1023, train acc: 0.5479, val loss: 1.1062, val acc: 0.5474  (best train acc: 0.5549, best val acc: 0.5508, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 19080] train loss: 1.1123, train acc: 0.5360, val loss: 1.1128, val acc: 0.5454  (best train acc: 0.5549, best val acc: 0.5508, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 19100] train loss: 1.1114, train acc: 0.5435, val loss: 1.1041, val acc: 0.5511  (best train acc: 0.5549, best val acc: 0.5511, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 19120] train loss: 1.1193, train acc: 0.5372, val loss: 1.1062, val acc: 0.5447  (best train acc: 0.5549, best val acc: 0.5511, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 19140] train loss: 1.1203, train acc: 0.5364, val loss: 1.1045, val acc: 0.5481  (best train acc: 0.5549, best val acc: 0.5511, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 19160] train loss: 1.1037, train acc: 0.5453, val loss: 1.1051, val acc: 0.5470  (best train acc: 0.5549, best val acc: 0.5511, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 19180] train loss: 1.0910, train acc: 0.5535, val loss: 1.1037, val acc: 0.5457  (best train acc: 0.5549, best val acc: 0.5511, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 19200] train loss: 1.1111, train acc: 0.5456, val loss: 1.1068, val acc: 0.5393  (best train acc: 0.5549, best val acc: 0.5514, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 19220] train loss: 1.1062, train acc: 0.5408, val loss: 1.1068, val acc: 0.5481  (best train acc: 0.5549, best val acc: 0.5514, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 19240] train loss: 1.1183, train acc: 0.5392, val loss: 1.1048, val acc: 0.5477  (best train acc: 0.5549, best val acc: 0.5514, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 19260] train loss: 1.1007, train acc: 0.5461, val loss: 1.1032, val acc: 0.5481  (best train acc: 0.5549, best val acc: 0.5514, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 19280] train loss: 1.1165, train acc: 0.5427, val loss: 1.1047, val acc: 0.5450  (best train acc: 0.5549, best val acc: 0.5518, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 19300] train loss: 1.1021, train acc: 0.5455, val loss: 1.1026, val acc: 0.5467  (best train acc: 0.5549, best val acc: 0.5528, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 19320] train loss: 1.1160, train acc: 0.5380, val loss: 1.1034, val acc: 0.5494  (best train acc: 0.5549, best val acc: 0.5528, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 19340] train loss: 1.1150, train acc: 0.5378, val loss: 1.1024, val acc: 0.5504  (best train acc: 0.5549, best val acc: 0.5528, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 19360] train loss: 1.1036, train acc: 0.5440, val loss: 1.1014, val acc: 0.5484  (best train acc: 0.5549, best val acc: 0.5528, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 19380] train loss: 1.0997, train acc: 0.5455, val loss: 1.1063, val acc: 0.5450  (best train acc: 0.5549, best val acc: 0.5528, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 19400] train loss: 1.1049, train acc: 0.5428, val loss: 1.1023, val acc: 0.5491  (best train acc: 0.5549, best val acc: 0.5528, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 19420] train loss: 1.0997, train acc: 0.5411, val loss: 1.1043, val acc: 0.5470  (best train acc: 0.5549, best val acc: 0.5528, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 19440] train loss: 1.0962, train acc: 0.5489, val loss: 1.1003, val acc: 0.5508  (best train acc: 0.5549, best val acc: 0.5528, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 19460] train loss: 1.1064, train acc: 0.5471, val loss: 1.1038, val acc: 0.5423  (best train acc: 0.5549, best val acc: 0.5528, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 19480] train loss: 1.1120, train acc: 0.5408, val loss: 1.1075, val acc: 0.5417  (best train acc: 0.5549, best val acc: 0.5528, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 19500] train loss: 1.0982, train acc: 0.5514, val loss: 1.1008, val acc: 0.5501  (best train acc: 0.5549, best val acc: 0.5528, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 19520] train loss: 1.1162, train acc: 0.5368, val loss: 1.1049, val acc: 0.5440  (best train acc: 0.5553, best val acc: 0.5531, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 19540] train loss: 1.1043, train acc: 0.5403, val loss: 1.0993, val acc: 0.5504  (best train acc: 0.5553, best val acc: 0.5531, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 19560] train loss: 1.1047, train acc: 0.5414, val loss: 1.1004, val acc: 0.5481  (best train acc: 0.5555, best val acc: 0.5531, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 19580] train loss: 1.1066, train acc: 0.5419, val loss: 1.0996, val acc: 0.5497  (best train acc: 0.5555, best val acc: 0.5531, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 19600] train loss: 1.1163, train acc: 0.5391, val loss: 1.1040, val acc: 0.5417  (best train acc: 0.5555, best val acc: 0.5531, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 19620] train loss: 1.1022, train acc: 0.5416, val loss: 1.0986, val acc: 0.5470  (best train acc: 0.5555, best val acc: 0.5531, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 19640] train loss: 1.0998, train acc: 0.5457, val loss: 1.0986, val acc: 0.5501  (best train acc: 0.5555, best val acc: 0.5531, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 19660] train loss: 1.1207, train acc: 0.5363, val loss: 1.0975, val acc: 0.5481  (best train acc: 0.5555, best val acc: 0.5545, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 19680] train loss: 1.0862, train acc: 0.5527, val loss: 1.1010, val acc: 0.5487  (best train acc: 0.5555, best val acc: 0.5545, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 19700] train loss: 1.1128, train acc: 0.5445, val loss: 1.1087, val acc: 0.5470  (best train acc: 0.5555, best val acc: 0.5545, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 19720] train loss: 1.1100, train acc: 0.5476, val loss: 1.1059, val acc: 0.5467  (best train acc: 0.5555, best val acc: 0.5545, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 19740] train loss: 1.1032, train acc: 0.5447, val loss: 1.0984, val acc: 0.5484  (best train acc: 0.5555, best val acc: 0.5545, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 19760] train loss: 1.1026, train acc: 0.5510, val loss: 1.0983, val acc: 0.5508  (best train acc: 0.5555, best val acc: 0.5545, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 19780] train loss: 1.1117, train acc: 0.5455, val loss: 1.0997, val acc: 0.5474  (best train acc: 0.5555, best val acc: 0.5548, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 19800] train loss: 1.0966, train acc: 0.5506, val loss: 1.0995, val acc: 0.5467  (best train acc: 0.5555, best val acc: 0.5548, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 19820] train loss: 1.1106, train acc: 0.5491, val loss: 1.0973, val acc: 0.5521  (best train acc: 0.5555, best val acc: 0.5548, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 19840] train loss: 1.0954, train acc: 0.5460, val loss: 1.0987, val acc: 0.5518  (best train acc: 0.5555, best val acc: 0.5548, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 19860] train loss: 1.0946, train acc: 0.5463, val loss: 1.0991, val acc: 0.5464  (best train acc: 0.5577, best val acc: 0.5548, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 19880] train loss: 1.1118, train acc: 0.5377, val loss: 1.0971, val acc: 0.5504  (best train acc: 0.5577, best val acc: 0.5548, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 19900] train loss: 1.0905, train acc: 0.5561, val loss: 1.0985, val acc: 0.5460  (best train acc: 0.5577, best val acc: 0.5548, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 19920] train loss: 1.1119, train acc: 0.5410, val loss: 1.0962, val acc: 0.5481  (best train acc: 0.5577, best val acc: 0.5548, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 19940] train loss: 1.0988, train acc: 0.5453, val loss: 1.0968, val acc: 0.5511  (best train acc: 0.5577, best val acc: 0.5548, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 19960] train loss: 1.1033, train acc: 0.5382, val loss: 1.0988, val acc: 0.5504  (best train acc: 0.5577, best val acc: 0.5548, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 19980] train loss: 1.0997, train acc: 0.5431, val loss: 1.0990, val acc: 0.5464  (best train acc: 0.5577, best val acc: 0.5548, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 20000] train loss: 1.0985, train acc: 0.5474, val loss: 1.0952, val acc: 0.5501  (best train acc: 0.5577, best val acc: 0.5548, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 20020] train loss: 1.1156, train acc: 0.5345, val loss: 1.0963, val acc: 0.5511  (best train acc: 0.5577, best val acc: 0.5548, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 20040] train loss: 1.1082, train acc: 0.5441, val loss: 1.0955, val acc: 0.5518  (best train acc: 0.5577, best val acc: 0.5548, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 20060] train loss: 1.0994, train acc: 0.5469, val loss: 1.1009, val acc: 0.5477  (best train acc: 0.5577, best val acc: 0.5548, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 20080] train loss: 1.1003, train acc: 0.5477, val loss: 1.0980, val acc: 0.5504  (best train acc: 0.5577, best val acc: 0.5548, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 20100] train loss: 1.1065, train acc: 0.5370, val loss: 1.1128, val acc: 0.5423  (best train acc: 0.5577, best val acc: 0.5548, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 20120] train loss: 1.1215, train acc: 0.5338, val loss: 1.0989, val acc: 0.5413  (best train acc: 0.5577, best val acc: 0.5548, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 20140] train loss: 1.0918, train acc: 0.5479, val loss: 1.0954, val acc: 0.5474  (best train acc: 0.5577, best val acc: 0.5548, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 20160] train loss: 1.1230, train acc: 0.5334, val loss: 1.1068, val acc: 0.5447  (best train acc: 0.5577, best val acc: 0.5551, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 20180] train loss: 1.1056, train acc: 0.5444, val loss: 1.0965, val acc: 0.5467  (best train acc: 0.5577, best val acc: 0.5551, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 20200] train loss: 1.1117, train acc: 0.5418, val loss: 1.0985, val acc: 0.5440  (best train acc: 0.5577, best val acc: 0.5551, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 20220] train loss: 1.0964, train acc: 0.5496, val loss: 1.0950, val acc: 0.5497  (best train acc: 0.5577, best val acc: 0.5551, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 20240] train loss: 1.1099, train acc: 0.5419, val loss: 1.0925, val acc: 0.5508  (best train acc: 0.5577, best val acc: 0.5551, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 20260] train loss: 1.0999, train acc: 0.5478, val loss: 1.0932, val acc: 0.5504  (best train acc: 0.5577, best val acc: 0.5551, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 20280] train loss: 1.0964, train acc: 0.5464, val loss: 1.0928, val acc: 0.5484  (best train acc: 0.5577, best val acc: 0.5551, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 20300] train loss: 1.1071, train acc: 0.5412, val loss: 1.0933, val acc: 0.5491  (best train acc: 0.5577, best val acc: 0.5551, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 20320] train loss: 1.0979, train acc: 0.5429, val loss: 1.0942, val acc: 0.5501  (best train acc: 0.5577, best val acc: 0.5551, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 20340] train loss: 1.1058, train acc: 0.5407, val loss: 1.0959, val acc: 0.5477  (best train acc: 0.5577, best val acc: 0.5551, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 20360] train loss: 1.0953, train acc: 0.5535, val loss: 1.0942, val acc: 0.5464  (best train acc: 0.5577, best val acc: 0.5551, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 20380] train loss: 1.1017, train acc: 0.5439, val loss: 1.0945, val acc: 0.5514  (best train acc: 0.5577, best val acc: 0.5551, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 20400] train loss: 1.0945, train acc: 0.5508, val loss: 1.0965, val acc: 0.5484  (best train acc: 0.5577, best val acc: 0.5551, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 20420] train loss: 1.1081, train acc: 0.5403, val loss: 1.0925, val acc: 0.5484  (best train acc: 0.5577, best val acc: 0.5551, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 20440] train loss: 1.1037, train acc: 0.5462, val loss: 1.0932, val acc: 0.5518  (best train acc: 0.5577, best val acc: 0.5551, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 20460] train loss: 1.1125, train acc: 0.5382, val loss: 1.0927, val acc: 0.5497  (best train acc: 0.5577, best val acc: 0.5551, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 20480] train loss: 1.1018, train acc: 0.5393, val loss: 1.0906, val acc: 0.5504  (best train acc: 0.5577, best val acc: 0.5551, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 20500] train loss: 1.1053, train acc: 0.5438, val loss: 1.0900, val acc: 0.5508  (best train acc: 0.5577, best val acc: 0.5551, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 20520] train loss: 1.0905, train acc: 0.5487, val loss: 1.0900, val acc: 0.5501  (best train acc: 0.5577, best val acc: 0.5551, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 20540] train loss: 1.0881, train acc: 0.5434, val loss: 1.0949, val acc: 0.5437  (best train acc: 0.5577, best val acc: 0.5551, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 20560] train loss: 1.0856, train acc: 0.5505, val loss: 1.0928, val acc: 0.5487  (best train acc: 0.5577, best val acc: 0.5551, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 20580] train loss: 1.0935, train acc: 0.5516, val loss: 1.0895, val acc: 0.5497  (best train acc: 0.5577, best val acc: 0.5551, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 20600] train loss: 1.0931, train acc: 0.5488, val loss: 1.0900, val acc: 0.5531  (best train acc: 0.5577, best val acc: 0.5551, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 20620] train loss: 1.1152, train acc: 0.5364, val loss: 1.0940, val acc: 0.5464  (best train acc: 0.5577, best val acc: 0.5551, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 20640] train loss: 1.1057, train acc: 0.5448, val loss: 1.0899, val acc: 0.5562  (best train acc: 0.5577, best val acc: 0.5562, best train loss: 1.0742  @ epoch 11802 )\n",
      "[Epoch: 20660] train loss: 1.0736, train acc: 0.5628, val loss: 1.0929, val acc: 0.5474  (best train acc: 0.5628, best val acc: 0.5562, best train loss: 1.0736  @ epoch 20660 )\n",
      "[Epoch: 20680] train loss: 1.0832, train acc: 0.5455, val loss: 1.0901, val acc: 0.5494  (best train acc: 0.5628, best val acc: 0.5562, best train loss: 1.0736  @ epoch 20660 )\n",
      "[Epoch: 20700] train loss: 1.1060, train acc: 0.5450, val loss: 1.0942, val acc: 0.5477  (best train acc: 0.5628, best val acc: 0.5562, best train loss: 1.0736  @ epoch 20660 )\n",
      "[Epoch: 20720] train loss: 1.0957, train acc: 0.5461, val loss: 1.0938, val acc: 0.5481  (best train acc: 0.5628, best val acc: 0.5562, best train loss: 1.0736  @ epoch 20660 )\n",
      "[Epoch: 20740] train loss: 1.0930, train acc: 0.5471, val loss: 1.0938, val acc: 0.5487  (best train acc: 0.5628, best val acc: 0.5562, best train loss: 1.0736  @ epoch 20660 )\n",
      "[Epoch: 20760] train loss: 1.1041, train acc: 0.5440, val loss: 1.0882, val acc: 0.5497  (best train acc: 0.5628, best val acc: 0.5562, best train loss: 1.0736  @ epoch 20660 )\n",
      "[Epoch: 20780] train loss: 1.1002, train acc: 0.5463, val loss: 1.0874, val acc: 0.5501  (best train acc: 0.5628, best val acc: 0.5562, best train loss: 1.0736  @ epoch 20660 )\n",
      "[Epoch: 20800] train loss: 1.1013, train acc: 0.5440, val loss: 1.0871, val acc: 0.5494  (best train acc: 0.5628, best val acc: 0.5562, best train loss: 1.0736  @ epoch 20660 )\n",
      "[Epoch: 20820] train loss: 1.0930, train acc: 0.5497, val loss: 1.0877, val acc: 0.5497  (best train acc: 0.5628, best val acc: 0.5562, best train loss: 1.0736  @ epoch 20660 )\n",
      "[Epoch: 20840] train loss: 1.0903, train acc: 0.5530, val loss: 1.0875, val acc: 0.5494  (best train acc: 0.5628, best val acc: 0.5562, best train loss: 1.0736  @ epoch 20660 )\n",
      "[Epoch: 20860] train loss: 1.0905, train acc: 0.5513, val loss: 1.0879, val acc: 0.5521  (best train acc: 0.5628, best val acc: 0.5562, best train loss: 1.0736  @ epoch 20660 )\n",
      "[Epoch: 20880] train loss: 1.1108, train acc: 0.5378, val loss: 1.1123, val acc: 0.5366  (best train acc: 0.5628, best val acc: 0.5562, best train loss: 1.0736  @ epoch 20660 )\n",
      "[Epoch: 20900] train loss: 1.0945, train acc: 0.5503, val loss: 1.0925, val acc: 0.5501  (best train acc: 0.5628, best val acc: 0.5562, best train loss: 1.0736  @ epoch 20660 )\n",
      "[Epoch: 20920] train loss: 1.0908, train acc: 0.5496, val loss: 1.0945, val acc: 0.5494  (best train acc: 0.5628, best val acc: 0.5562, best train loss: 1.0736  @ epoch 20660 )\n",
      "[Epoch: 20940] train loss: 1.0965, train acc: 0.5456, val loss: 1.0886, val acc: 0.5491  (best train acc: 0.5628, best val acc: 0.5562, best train loss: 1.0736  @ epoch 20660 )\n",
      "[Epoch: 20960] train loss: 1.0915, train acc: 0.5450, val loss: 1.0949, val acc: 0.5467  (best train acc: 0.5628, best val acc: 0.5562, best train loss: 1.0736  @ epoch 20660 )\n",
      "[Epoch: 20980] train loss: 1.0817, train acc: 0.5534, val loss: 1.0879, val acc: 0.5491  (best train acc: 0.5628, best val acc: 0.5562, best train loss: 1.0736  @ epoch 20660 )\n",
      "[Epoch: 21000] train loss: 1.1155, train acc: 0.5356, val loss: 1.1044, val acc: 0.5454  (best train acc: 0.5628, best val acc: 0.5562, best train loss: 1.0736  @ epoch 20660 )\n",
      "[Epoch: 21020] train loss: 1.1057, train acc: 0.5477, val loss: 1.0904, val acc: 0.5504  (best train acc: 0.5628, best val acc: 0.5562, best train loss: 1.0736  @ epoch 20660 )\n",
      "[Epoch: 21040] train loss: 1.0854, train acc: 0.5526, val loss: 1.0872, val acc: 0.5508  (best train acc: 0.5628, best val acc: 0.5562, best train loss: 1.0736  @ epoch 20660 )\n",
      "[Epoch: 21060] train loss: 1.0897, train acc: 0.5490, val loss: 1.0869, val acc: 0.5514  (best train acc: 0.5628, best val acc: 0.5562, best train loss: 1.0736  @ epoch 20660 )\n",
      "[Epoch: 21080] train loss: 1.1003, train acc: 0.5471, val loss: 1.0876, val acc: 0.5551  (best train acc: 0.5628, best val acc: 0.5562, best train loss: 1.0736  @ epoch 20660 )\n",
      "[Epoch: 21100] train loss: 1.0983, train acc: 0.5463, val loss: 1.0857, val acc: 0.5528  (best train acc: 0.5628, best val acc: 0.5562, best train loss: 1.0736  @ epoch 20660 )\n",
      "[Epoch: 21120] train loss: 1.0861, train acc: 0.5497, val loss: 1.0848, val acc: 0.5535  (best train acc: 0.5628, best val acc: 0.5562, best train loss: 1.0736  @ epoch 20660 )\n",
      "[Epoch: 21140] train loss: 1.0926, train acc: 0.5427, val loss: 1.0998, val acc: 0.5457  (best train acc: 0.5628, best val acc: 0.5562, best train loss: 1.0736  @ epoch 20660 )\n",
      "[Epoch: 21160] train loss: 1.1080, train acc: 0.5452, val loss: 1.1088, val acc: 0.5403  (best train acc: 0.5628, best val acc: 0.5562, best train loss: 1.0736  @ epoch 20660 )\n",
      "[Epoch: 21180] train loss: 1.0909, train acc: 0.5525, val loss: 1.0892, val acc: 0.5484  (best train acc: 0.5628, best val acc: 0.5562, best train loss: 1.0736  @ epoch 20660 )\n",
      "[Epoch: 21200] train loss: 1.0916, train acc: 0.5497, val loss: 1.0860, val acc: 0.5521  (best train acc: 0.5628, best val acc: 0.5562, best train loss: 1.0736  @ epoch 20660 )\n",
      "[Epoch: 21220] train loss: 1.0788, train acc: 0.5580, val loss: 1.0848, val acc: 0.5494  (best train acc: 0.5628, best val acc: 0.5562, best train loss: 1.0736  @ epoch 20660 )\n",
      "[Epoch: 21240] train loss: 1.0766, train acc: 0.5541, val loss: 1.0867, val acc: 0.5501  (best train acc: 0.5628, best val acc: 0.5562, best train loss: 1.0736  @ epoch 20660 )\n",
      "[Epoch: 21260] train loss: 1.0852, train acc: 0.5536, val loss: 1.0861, val acc: 0.5518  (best train acc: 0.5628, best val acc: 0.5562, best train loss: 1.0736  @ epoch 20660 )\n",
      "[Epoch: 21280] train loss: 1.0931, train acc: 0.5452, val loss: 1.0856, val acc: 0.5521  (best train acc: 0.5628, best val acc: 0.5562, best train loss: 1.0736  @ epoch 20660 )\n",
      "[Epoch: 21300] train loss: 1.0880, train acc: 0.5519, val loss: 1.0835, val acc: 0.5508  (best train acc: 0.5628, best val acc: 0.5562, best train loss: 1.0736  @ epoch 20660 )\n",
      "[Epoch: 21320] train loss: 1.0953, train acc: 0.5445, val loss: 1.0904, val acc: 0.5494  (best train acc: 0.5628, best val acc: 0.5562, best train loss: 1.0736  @ epoch 20660 )\n",
      "[Epoch: 21340] train loss: 1.0956, train acc: 0.5453, val loss: 1.0835, val acc: 0.5555  (best train acc: 0.5628, best val acc: 0.5562, best train loss: 1.0736  @ epoch 20660 )\n",
      "[Epoch: 21360] train loss: 1.1042, train acc: 0.5405, val loss: 1.1070, val acc: 0.5427  (best train acc: 0.5628, best val acc: 0.5562, best train loss: 1.0736  @ epoch 20660 )\n",
      "[Epoch: 21380] train loss: 1.1010, train acc: 0.5448, val loss: 1.0862, val acc: 0.5474  (best train acc: 0.5628, best val acc: 0.5562, best train loss: 1.0736  @ epoch 20660 )\n",
      "[Epoch: 21400] train loss: 1.0864, train acc: 0.5479, val loss: 1.0849, val acc: 0.5531  (best train acc: 0.5628, best val acc: 0.5562, best train loss: 1.0736  @ epoch 20660 )\n",
      "[Epoch: 21420] train loss: 1.0945, train acc: 0.5492, val loss: 1.0841, val acc: 0.5508  (best train acc: 0.5628, best val acc: 0.5565, best train loss: 1.0736  @ epoch 20660 )\n",
      "[Epoch: 21440] train loss: 1.0828, train acc: 0.5541, val loss: 1.0839, val acc: 0.5538  (best train acc: 0.5628, best val acc: 0.5565, best train loss: 1.0736  @ epoch 20660 )\n",
      "[Epoch: 21460] train loss: 1.0948, train acc: 0.5438, val loss: 1.0844, val acc: 0.5484  (best train acc: 0.5628, best val acc: 0.5565, best train loss: 1.0736  @ epoch 20660 )\n",
      "[Epoch: 21480] train loss: 1.1002, train acc: 0.5495, val loss: 1.0870, val acc: 0.5508  (best train acc: 0.5628, best val acc: 0.5565, best train loss: 1.0736  @ epoch 20660 )\n",
      "[Epoch: 21500] train loss: 1.1126, train acc: 0.5402, val loss: 1.1047, val acc: 0.5420  (best train acc: 0.5628, best val acc: 0.5565, best train loss: 1.0695  @ epoch 21496 )\n",
      "[Epoch: 21520] train loss: 1.0967, train acc: 0.5508, val loss: 1.0939, val acc: 0.5504  (best train acc: 0.5628, best val acc: 0.5565, best train loss: 1.0695  @ epoch 21496 )\n",
      "[Epoch: 21540] train loss: 1.0832, train acc: 0.5541, val loss: 1.0985, val acc: 0.5460  (best train acc: 0.5628, best val acc: 0.5565, best train loss: 1.0695  @ epoch 21496 )\n",
      "[Epoch: 21560] train loss: 1.1132, train acc: 0.5396, val loss: 1.0873, val acc: 0.5487  (best train acc: 0.5628, best val acc: 0.5565, best train loss: 1.0695  @ epoch 21496 )\n",
      "[Epoch: 21580] train loss: 1.0930, train acc: 0.5515, val loss: 1.0876, val acc: 0.5511  (best train acc: 0.5628, best val acc: 0.5565, best train loss: 1.0695  @ epoch 21496 )\n",
      "[Epoch: 21600] train loss: 1.1070, train acc: 0.5385, val loss: 1.0860, val acc: 0.5524  (best train acc: 0.5628, best val acc: 0.5565, best train loss: 1.0695  @ epoch 21496 )\n",
      "[Epoch: 21620] train loss: 1.0950, train acc: 0.5517, val loss: 1.0896, val acc: 0.5460  (best train acc: 0.5628, best val acc: 0.5565, best train loss: 1.0695  @ epoch 21496 )\n",
      "[Epoch: 21640] train loss: 1.0942, train acc: 0.5497, val loss: 1.0928, val acc: 0.5467  (best train acc: 0.5628, best val acc: 0.5565, best train loss: 1.0695  @ epoch 21496 )\n",
      "[Epoch: 21660] train loss: 1.1026, train acc: 0.5460, val loss: 1.0888, val acc: 0.5545  (best train acc: 0.5628, best val acc: 0.5565, best train loss: 1.0695  @ epoch 21496 )\n",
      "[Epoch: 21680] train loss: 1.0780, train acc: 0.5535, val loss: 1.0835, val acc: 0.5535  (best train acc: 0.5628, best val acc: 0.5565, best train loss: 1.0695  @ epoch 21496 )\n",
      "[Epoch: 21700] train loss: 1.0974, train acc: 0.5453, val loss: 1.0839, val acc: 0.5524  (best train acc: 0.5628, best val acc: 0.5565, best train loss: 1.0695  @ epoch 21496 )\n",
      "[Epoch: 21720] train loss: 1.0852, train acc: 0.5481, val loss: 1.0846, val acc: 0.5524  (best train acc: 0.5628, best val acc: 0.5565, best train loss: 1.0695  @ epoch 21496 )\n",
      "[Epoch: 21740] train loss: 1.0911, train acc: 0.5489, val loss: 1.0823, val acc: 0.5524  (best train acc: 0.5628, best val acc: 0.5565, best train loss: 1.0695  @ epoch 21496 )\n",
      "[Epoch: 21760] train loss: 1.1077, train acc: 0.5357, val loss: 1.0874, val acc: 0.5454  (best train acc: 0.5628, best val acc: 0.5565, best train loss: 1.0695  @ epoch 21496 )\n",
      "[Epoch: 21780] train loss: 1.0945, train acc: 0.5440, val loss: 1.0865, val acc: 0.5508  (best train acc: 0.5628, best val acc: 0.5565, best train loss: 1.0695  @ epoch 21496 )\n",
      "[Epoch: 21800] train loss: 1.1018, train acc: 0.5448, val loss: 1.0831, val acc: 0.5508  (best train acc: 0.5628, best val acc: 0.5565, best train loss: 1.0695  @ epoch 21496 )\n",
      "[Epoch: 21820] train loss: 1.0800, train acc: 0.5532, val loss: 1.0835, val acc: 0.5524  (best train acc: 0.5628, best val acc: 0.5565, best train loss: 1.0695  @ epoch 21496 )\n",
      "[Epoch: 21840] train loss: 1.0810, train acc: 0.5571, val loss: 1.0828, val acc: 0.5491  (best train acc: 0.5628, best val acc: 0.5565, best train loss: 1.0680  @ epoch 21835 )\n",
      "[Epoch: 21860] train loss: 1.0871, train acc: 0.5510, val loss: 1.0823, val acc: 0.5514  (best train acc: 0.5628, best val acc: 0.5565, best train loss: 1.0680  @ epoch 21835 )\n",
      "[Epoch: 21880] train loss: 1.1009, train acc: 0.5398, val loss: 1.0818, val acc: 0.5477  (best train acc: 0.5628, best val acc: 0.5565, best train loss: 1.0680  @ epoch 21835 )\n",
      "[Epoch: 21900] train loss: 1.0766, train acc: 0.5560, val loss: 1.0810, val acc: 0.5548  (best train acc: 0.5628, best val acc: 0.5565, best train loss: 1.0680  @ epoch 21835 )\n",
      "[Epoch: 21920] train loss: 1.0846, train acc: 0.5547, val loss: 1.0837, val acc: 0.5518  (best train acc: 0.5628, best val acc: 0.5575, best train loss: 1.0680  @ epoch 21835 )\n",
      "[Epoch: 21940] train loss: 1.0829, train acc: 0.5517, val loss: 1.0814, val acc: 0.5558  (best train acc: 0.5652, best val acc: 0.5575, best train loss: 1.0680  @ epoch 21835 )\n",
      "[Epoch: 21960] train loss: 1.0823, train acc: 0.5523, val loss: 1.0807, val acc: 0.5545  (best train acc: 0.5652, best val acc: 0.5575, best train loss: 1.0680  @ epoch 21835 )\n",
      "[Epoch: 21980] train loss: 1.0835, train acc: 0.5513, val loss: 1.0792, val acc: 0.5531  (best train acc: 0.5652, best val acc: 0.5575, best train loss: 1.0680  @ epoch 21835 )\n",
      "[Epoch: 22000] train loss: 1.0908, train acc: 0.5476, val loss: 1.0841, val acc: 0.5524  (best train acc: 0.5652, best val acc: 0.5575, best train loss: 1.0680  @ epoch 21835 )\n",
      "[Epoch: 22020] train loss: 1.0771, train acc: 0.5508, val loss: 1.0788, val acc: 0.5481  (best train acc: 0.5652, best val acc: 0.5595, best train loss: 1.0680  @ epoch 21835 )\n",
      "[Epoch: 22040] train loss: 1.0800, train acc: 0.5544, val loss: 1.0810, val acc: 0.5548  (best train acc: 0.5652, best val acc: 0.5595, best train loss: 1.0680  @ epoch 21835 )\n",
      "[Epoch: 22060] train loss: 1.0935, train acc: 0.5447, val loss: 1.0782, val acc: 0.5511  (best train acc: 0.5652, best val acc: 0.5595, best train loss: 1.0674  @ epoch 22045 )\n",
      "[Epoch: 22080] train loss: 1.0926, train acc: 0.5499, val loss: 1.0805, val acc: 0.5551  (best train acc: 0.5652, best val acc: 0.5595, best train loss: 1.0613  @ epoch 22077 )\n",
      "[Epoch: 22100] train loss: 1.0870, train acc: 0.5515, val loss: 1.0817, val acc: 0.5521  (best train acc: 0.5652, best val acc: 0.5595, best train loss: 1.0613  @ epoch 22077 )\n",
      "[Epoch: 22120] train loss: 1.0798, train acc: 0.5555, val loss: 1.0822, val acc: 0.5531  (best train acc: 0.5652, best val acc: 0.5595, best train loss: 1.0613  @ epoch 22077 )\n",
      "[Epoch: 22140] train loss: 1.0799, train acc: 0.5556, val loss: 1.0775, val acc: 0.5578  (best train acc: 0.5652, best val acc: 0.5595, best train loss: 1.0613  @ epoch 22077 )\n",
      "[Epoch: 22160] train loss: 1.0750, train acc: 0.5591, val loss: 1.0783, val acc: 0.5545  (best train acc: 0.5652, best val acc: 0.5595, best train loss: 1.0613  @ epoch 22077 )\n",
      "[Epoch: 22180] train loss: 1.0849, train acc: 0.5494, val loss: 1.0772, val acc: 0.5558  (best train acc: 0.5652, best val acc: 0.5595, best train loss: 1.0613  @ epoch 22077 )\n",
      "[Epoch: 22200] train loss: 1.0817, train acc: 0.5543, val loss: 1.0769, val acc: 0.5578  (best train acc: 0.5652, best val acc: 0.5595, best train loss: 1.0613  @ epoch 22077 )\n",
      "[Epoch: 22220] train loss: 1.0840, train acc: 0.5476, val loss: 1.0788, val acc: 0.5508  (best train acc: 0.5652, best val acc: 0.5595, best train loss: 1.0613  @ epoch 22077 )\n",
      "[Epoch: 22240] train loss: 1.1000, train acc: 0.5465, val loss: 1.0821, val acc: 0.5501  (best train acc: 0.5652, best val acc: 0.5595, best train loss: 1.0613  @ epoch 22077 )\n",
      "[Epoch: 22260] train loss: 1.0831, train acc: 0.5541, val loss: 1.0807, val acc: 0.5535  (best train acc: 0.5652, best val acc: 0.5595, best train loss: 1.0613  @ epoch 22077 )\n",
      "[Epoch: 22280] train loss: 1.0664, train acc: 0.5558, val loss: 1.0788, val acc: 0.5528  (best train acc: 0.5675, best val acc: 0.5595, best train loss: 1.0597  @ epoch 22274 )\n",
      "[Epoch: 22300] train loss: 1.1660, train acc: 0.5220, val loss: 1.1538, val acc: 0.5228  (best train acc: 0.5675, best val acc: 0.5595, best train loss: 1.0597  @ epoch 22274 )\n",
      "[Epoch: 22320] train loss: 1.1056, train acc: 0.5437, val loss: 1.0907, val acc: 0.5417  (best train acc: 0.5675, best val acc: 0.5595, best train loss: 1.0597  @ epoch 22274 )\n",
      "[Epoch: 22340] train loss: 1.0802, train acc: 0.5537, val loss: 1.0799, val acc: 0.5508  (best train acc: 0.5675, best val acc: 0.5595, best train loss: 1.0597  @ epoch 22274 )\n",
      "[Epoch: 22360] train loss: 1.1053, train acc: 0.5410, val loss: 1.0785, val acc: 0.5508  (best train acc: 0.5675, best val acc: 0.5595, best train loss: 1.0597  @ epoch 22274 )\n",
      "[Epoch: 22380] train loss: 1.0821, train acc: 0.5485, val loss: 1.0782, val acc: 0.5511  (best train acc: 0.5675, best val acc: 0.5595, best train loss: 1.0597  @ epoch 22274 )\n",
      "[Epoch: 22400] train loss: 1.0788, train acc: 0.5591, val loss: 1.0757, val acc: 0.5562  (best train acc: 0.5675, best val acc: 0.5595, best train loss: 1.0597  @ epoch 22274 )\n",
      "[Epoch: 22420] train loss: 1.0931, train acc: 0.5476, val loss: 1.0922, val acc: 0.5437  (best train acc: 0.5675, best val acc: 0.5595, best train loss: 1.0597  @ epoch 22274 )\n",
      "[Epoch: 22440] train loss: 1.0792, train acc: 0.5505, val loss: 1.0779, val acc: 0.5555  (best train acc: 0.5675, best val acc: 0.5595, best train loss: 1.0597  @ epoch 22274 )\n",
      "[Epoch: 22460] train loss: 1.0870, train acc: 0.5532, val loss: 1.0754, val acc: 0.5551  (best train acc: 0.5675, best val acc: 0.5595, best train loss: 1.0597  @ epoch 22274 )\n",
      "[Epoch: 22480] train loss: 1.0854, train acc: 0.5512, val loss: 1.0739, val acc: 0.5538  (best train acc: 0.5675, best val acc: 0.5595, best train loss: 1.0597  @ epoch 22274 )\n",
      "[Epoch: 22500] train loss: 1.0811, train acc: 0.5514, val loss: 1.0747, val acc: 0.5582  (best train acc: 0.5675, best val acc: 0.5595, best train loss: 1.0597  @ epoch 22274 )\n",
      "[Epoch: 22520] train loss: 1.0813, train acc: 0.5484, val loss: 1.0998, val acc: 0.5497  (best train acc: 0.5675, best val acc: 0.5595, best train loss: 1.0597  @ epoch 22274 )\n",
      "[Epoch: 22540] train loss: 1.0863, train acc: 0.5422, val loss: 1.0762, val acc: 0.5555  (best train acc: 0.5675, best val acc: 0.5595, best train loss: 1.0591  @ epoch 22537 )\n",
      "[Epoch: 22560] train loss: 1.0842, train acc: 0.5480, val loss: 1.0744, val acc: 0.5545  (best train acc: 0.5675, best val acc: 0.5595, best train loss: 1.0591  @ epoch 22537 )\n",
      "[Epoch: 22580] train loss: 1.0933, train acc: 0.5446, val loss: 1.0752, val acc: 0.5555  (best train acc: 0.5675, best val acc: 0.5599, best train loss: 1.0591  @ epoch 22537 )\n",
      "[Epoch: 22600] train loss: 1.0877, train acc: 0.5505, val loss: 1.0726, val acc: 0.5524  (best train acc: 0.5675, best val acc: 0.5599, best train loss: 1.0591  @ epoch 22537 )\n",
      "[Epoch: 22620] train loss: 1.0867, train acc: 0.5468, val loss: 1.0726, val acc: 0.5545  (best train acc: 0.5675, best val acc: 0.5599, best train loss: 1.0544  @ epoch 22607 )\n",
      "[Epoch: 22640] train loss: 1.0679, train acc: 0.5565, val loss: 1.0718, val acc: 0.5538  (best train acc: 0.5675, best val acc: 0.5599, best train loss: 1.0544  @ epoch 22607 )\n",
      "[Epoch: 22660] train loss: 1.0717, train acc: 0.5552, val loss: 1.0713, val acc: 0.5555  (best train acc: 0.5675, best val acc: 0.5605, best train loss: 1.0544  @ epoch 22607 )\n",
      "[Epoch: 22680] train loss: 1.0702, train acc: 0.5561, val loss: 1.0732, val acc: 0.5545  (best train acc: 0.5675, best val acc: 0.5605, best train loss: 1.0544  @ epoch 22607 )\n",
      "[Epoch: 22700] train loss: 1.0571, train acc: 0.5655, val loss: 1.0713, val acc: 0.5518  (best train acc: 0.5675, best val acc: 0.5605, best train loss: 1.0544  @ epoch 22607 )\n",
      "[Epoch: 22720] train loss: 1.0838, train acc: 0.5560, val loss: 1.0721, val acc: 0.5551  (best train acc: 0.5675, best val acc: 0.5605, best train loss: 1.0544  @ epoch 22607 )\n",
      "[Epoch: 22740] train loss: 1.0850, train acc: 0.5505, val loss: 1.0765, val acc: 0.5582  (best train acc: 0.5675, best val acc: 0.5605, best train loss: 1.0544  @ epoch 22607 )\n",
      "[Epoch: 22760] train loss: 1.1268, train acc: 0.5338, val loss: 1.0985, val acc: 0.5460  (best train acc: 0.5675, best val acc: 0.5609, best train loss: 1.0544  @ epoch 22607 )\n",
      "[Epoch: 22780] train loss: 1.0709, train acc: 0.5559, val loss: 1.0780, val acc: 0.5572  (best train acc: 0.5675, best val acc: 0.5609, best train loss: 1.0544  @ epoch 22607 )\n",
      "[Epoch: 22800] train loss: 1.0826, train acc: 0.5438, val loss: 1.0849, val acc: 0.5531  (best train acc: 0.5675, best val acc: 0.5609, best train loss: 1.0544  @ epoch 22607 )\n",
      "[Epoch: 22820] train loss: 1.0757, train acc: 0.5552, val loss: 1.0750, val acc: 0.5555  (best train acc: 0.5675, best val acc: 0.5609, best train loss: 1.0544  @ epoch 22607 )\n",
      "[Epoch: 22840] train loss: 1.1376, train acc: 0.5322, val loss: 1.1295, val acc: 0.5197  (best train acc: 0.5675, best val acc: 0.5609, best train loss: 1.0544  @ epoch 22607 )\n",
      "[Epoch: 22860] train loss: 1.0867, train acc: 0.5487, val loss: 1.0749, val acc: 0.5558  (best train acc: 0.5675, best val acc: 0.5609, best train loss: 1.0544  @ epoch 22607 )\n",
      "[Epoch: 22880] train loss: 1.0925, train acc: 0.5486, val loss: 1.0732, val acc: 0.5501  (best train acc: 0.5675, best val acc: 0.5609, best train loss: 1.0544  @ epoch 22607 )\n",
      "[Epoch: 22900] train loss: 1.0974, train acc: 0.5474, val loss: 1.0922, val acc: 0.5494  (best train acc: 0.5675, best val acc: 0.5609, best train loss: 1.0544  @ epoch 22607 )\n",
      "[Epoch: 22920] train loss: 1.1167, train acc: 0.5362, val loss: 1.1058, val acc: 0.5379  (best train acc: 0.5675, best val acc: 0.5609, best train loss: 1.0544  @ epoch 22607 )\n",
      "[Epoch: 22940] train loss: 1.0759, train acc: 0.5533, val loss: 1.0800, val acc: 0.5514  (best train acc: 0.5675, best val acc: 0.5609, best train loss: 1.0544  @ epoch 22607 )\n",
      "[Epoch: 22960] train loss: 1.0939, train acc: 0.5395, val loss: 1.0739, val acc: 0.5511  (best train acc: 0.5675, best val acc: 0.5609, best train loss: 1.0544  @ epoch 22607 )\n",
      "[Epoch: 22980] train loss: 1.0801, train acc: 0.5522, val loss: 1.0836, val acc: 0.5548  (best train acc: 0.5675, best val acc: 0.5609, best train loss: 1.0544  @ epoch 22607 )\n",
      "[Epoch: 23000] train loss: 1.0833, train acc: 0.5553, val loss: 1.0787, val acc: 0.5568  (best train acc: 0.5675, best val acc: 0.5609, best train loss: 1.0544  @ epoch 22607 )\n",
      "[Epoch: 23020] train loss: 1.0737, train acc: 0.5510, val loss: 1.0739, val acc: 0.5555  (best train acc: 0.5675, best val acc: 0.5609, best train loss: 1.0544  @ epoch 22607 )\n",
      "[Epoch: 23040] train loss: 1.0825, train acc: 0.5471, val loss: 1.0721, val acc: 0.5558  (best train acc: 0.5675, best val acc: 0.5609, best train loss: 1.0544  @ epoch 22607 )\n",
      "[Epoch: 23060] train loss: 1.0834, train acc: 0.5511, val loss: 1.0755, val acc: 0.5531  (best train acc: 0.5675, best val acc: 0.5609, best train loss: 1.0544  @ epoch 22607 )\n",
      "[Epoch: 23080] train loss: 1.0964, train acc: 0.5449, val loss: 1.1052, val acc: 0.5447  (best train acc: 0.5675, best val acc: 0.5609, best train loss: 1.0544  @ epoch 22607 )\n",
      "[Epoch: 23100] train loss: 1.0724, train acc: 0.5591, val loss: 1.0757, val acc: 0.5541  (best train acc: 0.5675, best val acc: 0.5612, best train loss: 1.0544  @ epoch 22607 )\n",
      "[Epoch: 23120] train loss: 1.0696, train acc: 0.5568, val loss: 1.0751, val acc: 0.5568  (best train acc: 0.5675, best val acc: 0.5612, best train loss: 1.0544  @ epoch 22607 )\n",
      "[Epoch: 23140] train loss: 1.0977, train acc: 0.5471, val loss: 1.0765, val acc: 0.5578  (best train acc: 0.5675, best val acc: 0.5612, best train loss: 1.0544  @ epoch 22607 )\n",
      "[Epoch: 23160] train loss: 1.0736, train acc: 0.5502, val loss: 1.0783, val acc: 0.5538  (best train acc: 0.5675, best val acc: 0.5612, best train loss: 1.0544  @ epoch 22607 )\n",
      "[Epoch: 23180] train loss: 1.1149, train acc: 0.5355, val loss: 1.1188, val acc: 0.5379  (best train acc: 0.5675, best val acc: 0.5612, best train loss: 1.0544  @ epoch 22607 )\n",
      "[Epoch: 23200] train loss: 1.0853, train acc: 0.5502, val loss: 1.0730, val acc: 0.5562  (best train acc: 0.5675, best val acc: 0.5612, best train loss: 1.0544  @ epoch 22607 )\n",
      "[Epoch: 23220] train loss: 1.0766, train acc: 0.5520, val loss: 1.0707, val acc: 0.5558  (best train acc: 0.5675, best val acc: 0.5616, best train loss: 1.0544  @ epoch 22607 )\n",
      "[Epoch: 23240] train loss: 1.0845, train acc: 0.5462, val loss: 1.0757, val acc: 0.5565  (best train acc: 0.5675, best val acc: 0.5616, best train loss: 1.0544  @ epoch 22607 )\n",
      "[Epoch: 23260] train loss: 1.0778, train acc: 0.5536, val loss: 1.0733, val acc: 0.5578  (best train acc: 0.5675, best val acc: 0.5616, best train loss: 1.0544  @ epoch 22607 )\n",
      "[Epoch: 23280] train loss: 1.0676, train acc: 0.5588, val loss: 1.0692, val acc: 0.5545  (best train acc: 0.5675, best val acc: 0.5616, best train loss: 1.0542  @ epoch 23279 )\n",
      "[Epoch: 23300] train loss: 1.0844, train acc: 0.5451, val loss: 1.0695, val acc: 0.5565  (best train acc: 0.5675, best val acc: 0.5616, best train loss: 1.0542  @ epoch 23279 )\n",
      "[Epoch: 23320] train loss: 1.0761, train acc: 0.5578, val loss: 1.0712, val acc: 0.5599  (best train acc: 0.5675, best val acc: 0.5616, best train loss: 1.0542  @ epoch 23279 )\n",
      "[Epoch: 23340] train loss: 1.0679, train acc: 0.5535, val loss: 1.0685, val acc: 0.5555  (best train acc: 0.5675, best val acc: 0.5616, best train loss: 1.0542  @ epoch 23279 )\n",
      "[Epoch: 23360] train loss: 1.0885, train acc: 0.5479, val loss: 1.0872, val acc: 0.5538  (best train acc: 0.5675, best val acc: 0.5616, best train loss: 1.0542  @ epoch 23279 )\n",
      "[Epoch: 23380] train loss: 1.0967, train acc: 0.5523, val loss: 1.0788, val acc: 0.5494  (best train acc: 0.5675, best val acc: 0.5616, best train loss: 1.0542  @ epoch 23279 )\n",
      "[Epoch: 23400] train loss: 1.0655, train acc: 0.5615, val loss: 1.0724, val acc: 0.5599  (best train acc: 0.5675, best val acc: 0.5616, best train loss: 1.0542  @ epoch 23279 )\n",
      "[Epoch: 23420] train loss: 1.0735, train acc: 0.5500, val loss: 1.0677, val acc: 0.5568  (best train acc: 0.5675, best val acc: 0.5616, best train loss: 1.0542  @ epoch 23279 )\n",
      "[Epoch: 23440] train loss: 1.0664, train acc: 0.5600, val loss: 1.0689, val acc: 0.5551  (best train acc: 0.5675, best val acc: 0.5616, best train loss: 1.0542  @ epoch 23279 )\n",
      "[Epoch: 23460] train loss: 1.0713, train acc: 0.5534, val loss: 1.0693, val acc: 0.5572  (best train acc: 0.5675, best val acc: 0.5616, best train loss: 1.0542  @ epoch 23279 )\n",
      "[Epoch: 23480] train loss: 1.0889, train acc: 0.5461, val loss: 1.0784, val acc: 0.5501  (best train acc: 0.5675, best val acc: 0.5616, best train loss: 1.0542  @ epoch 23279 )\n",
      "[Epoch: 23500] train loss: 1.0723, train acc: 0.5506, val loss: 1.0706, val acc: 0.5572  (best train acc: 0.5675, best val acc: 0.5616, best train loss: 1.0542  @ epoch 23279 )\n",
      "[Epoch: 23520] train loss: 1.0832, train acc: 0.5416, val loss: 1.0672, val acc: 0.5535  (best train acc: 0.5675, best val acc: 0.5622, best train loss: 1.0542  @ epoch 23279 )\n",
      "[Epoch: 23540] train loss: 1.0588, train acc: 0.5591, val loss: 1.0657, val acc: 0.5562  (best train acc: 0.5675, best val acc: 0.5622, best train loss: 1.0542  @ epoch 23279 )\n",
      "[Epoch: 23560] train loss: 1.0652, train acc: 0.5592, val loss: 1.0723, val acc: 0.5592  (best train acc: 0.5675, best val acc: 0.5622, best train loss: 1.0542  @ epoch 23279 )\n",
      "[Epoch: 23580] train loss: 1.0696, train acc: 0.5544, val loss: 1.0709, val acc: 0.5605  (best train acc: 0.5675, best val acc: 0.5622, best train loss: 1.0542  @ epoch 23279 )\n",
      "[Epoch: 23600] train loss: 1.0987, train acc: 0.5429, val loss: 1.0691, val acc: 0.5609  (best train acc: 0.5675, best val acc: 0.5622, best train loss: 1.0534  @ epoch 23596 )\n",
      "[Epoch: 23620] train loss: 1.0642, train acc: 0.5586, val loss: 1.0675, val acc: 0.5605  (best train acc: 0.5675, best val acc: 0.5622, best train loss: 1.0534  @ epoch 23596 )\n",
      "[Epoch: 23640] train loss: 1.0663, train acc: 0.5588, val loss: 1.0669, val acc: 0.5589  (best train acc: 0.5675, best val acc: 0.5622, best train loss: 1.0534  @ epoch 23596 )\n",
      "[Epoch: 23660] train loss: 1.0656, train acc: 0.5544, val loss: 1.0633, val acc: 0.5575  (best train acc: 0.5675, best val acc: 0.5626, best train loss: 1.0534  @ epoch 23596 )\n",
      "[Epoch: 23680] train loss: 1.0641, train acc: 0.5556, val loss: 1.0635, val acc: 0.5616  (best train acc: 0.5675, best val acc: 0.5626, best train loss: 1.0534  @ epoch 23596 )\n",
      "[Epoch: 23700] train loss: 1.0932, train acc: 0.5424, val loss: 1.0608, val acc: 0.5575  (best train acc: 0.5675, best val acc: 0.5626, best train loss: 1.0528  @ epoch 23688 )\n",
      "[Epoch: 23720] train loss: 1.0686, train acc: 0.5526, val loss: 1.0668, val acc: 0.5589  (best train acc: 0.5675, best val acc: 0.5626, best train loss: 1.0528  @ epoch 23688 )\n",
      "[Epoch: 23740] train loss: 1.0670, train acc: 0.5557, val loss: 1.0678, val acc: 0.5629  (best train acc: 0.5675, best val acc: 0.5629, best train loss: 1.0528  @ epoch 23688 )\n",
      "[Epoch: 23760] train loss: 1.0638, train acc: 0.5541, val loss: 1.0632, val acc: 0.5605  (best train acc: 0.5675, best val acc: 0.5629, best train loss: 1.0528  @ epoch 23688 )\n",
      "[Epoch: 23780] train loss: 1.0724, train acc: 0.5547, val loss: 1.0692, val acc: 0.5568  (best train acc: 0.5675, best val acc: 0.5639, best train loss: 1.0528  @ epoch 23688 )\n",
      "[Epoch: 23800] train loss: 1.0661, train acc: 0.5549, val loss: 1.0619, val acc: 0.5592  (best train acc: 0.5675, best val acc: 0.5639, best train loss: 1.0528  @ epoch 23688 )\n",
      "[Epoch: 23820] train loss: 1.0620, train acc: 0.5591, val loss: 1.0607, val acc: 0.5595  (best train acc: 0.5675, best val acc: 0.5639, best train loss: 1.0528  @ epoch 23688 )\n",
      "[Epoch: 23840] train loss: 1.0837, train acc: 0.5494, val loss: 1.0651, val acc: 0.5572  (best train acc: 0.5675, best val acc: 0.5639, best train loss: 1.0528  @ epoch 23688 )\n",
      "[Epoch: 23860] train loss: 1.0862, train acc: 0.5421, val loss: 1.0731, val acc: 0.5558  (best train acc: 0.5678, best val acc: 0.5639, best train loss: 1.0528  @ epoch 23688 )\n",
      "[Epoch: 23880] train loss: 1.1033, train acc: 0.5453, val loss: 1.0751, val acc: 0.5582  (best train acc: 0.5678, best val acc: 0.5639, best train loss: 1.0528  @ epoch 23688 )\n",
      "[Epoch: 23900] train loss: 1.0820, train acc: 0.5495, val loss: 1.0707, val acc: 0.5551  (best train acc: 0.5678, best val acc: 0.5639, best train loss: 1.0528  @ epoch 23688 )\n",
      "[Epoch: 23920] train loss: 1.0779, train acc: 0.5496, val loss: 1.0701, val acc: 0.5578  (best train acc: 0.5678, best val acc: 0.5639, best train loss: 1.0528  @ epoch 23688 )\n",
      "[Epoch: 23940] train loss: 1.0782, train acc: 0.5571, val loss: 1.0645, val acc: 0.5592  (best train acc: 0.5678, best val acc: 0.5639, best train loss: 1.0517  @ epoch 23930 )\n",
      "[Epoch: 23960] train loss: 1.0658, train acc: 0.5531, val loss: 1.0611, val acc: 0.5605  (best train acc: 0.5693, best val acc: 0.5656, best train loss: 1.0447  @ epoch 23955 )\n",
      "[Epoch: 23980] train loss: 1.0696, train acc: 0.5481, val loss: 1.0663, val acc: 0.5592  (best train acc: 0.5693, best val acc: 0.5656, best train loss: 1.0447  @ epoch 23955 )\n",
      "[Epoch: 24000] train loss: 1.0616, train acc: 0.5564, val loss: 1.0582, val acc: 0.5629  (best train acc: 0.5693, best val acc: 0.5656, best train loss: 1.0447  @ epoch 23955 )\n",
      "[Epoch: 24020] train loss: 1.0632, train acc: 0.5599, val loss: 1.0597, val acc: 0.5619  (best train acc: 0.5693, best val acc: 0.5656, best train loss: 1.0447  @ epoch 23955 )\n",
      "[Epoch: 24040] train loss: 1.0698, train acc: 0.5546, val loss: 1.0581, val acc: 0.5629  (best train acc: 0.5693, best val acc: 0.5656, best train loss: 1.0447  @ epoch 23955 )\n",
      "[Epoch: 24060] train loss: 1.0625, train acc: 0.5632, val loss: 1.0714, val acc: 0.5592  (best train acc: 0.5693, best val acc: 0.5659, best train loss: 1.0447  @ epoch 23955 )\n",
      "[Epoch: 24080] train loss: 1.0810, train acc: 0.5552, val loss: 1.0819, val acc: 0.5565  (best train acc: 0.5693, best val acc: 0.5659, best train loss: 1.0447  @ epoch 23955 )\n",
      "[Epoch: 24100] train loss: 1.0665, train acc: 0.5581, val loss: 1.0729, val acc: 0.5582  (best train acc: 0.5693, best val acc: 0.5659, best train loss: 1.0447  @ epoch 23955 )\n",
      "[Epoch: 24120] train loss: 1.0772, train acc: 0.5620, val loss: 1.0666, val acc: 0.5609  (best train acc: 0.5693, best val acc: 0.5659, best train loss: 1.0447  @ epoch 23955 )\n",
      "[Epoch: 24140] train loss: 1.0736, train acc: 0.5546, val loss: 1.0706, val acc: 0.5558  (best train acc: 0.5693, best val acc: 0.5659, best train loss: 1.0447  @ epoch 23955 )\n",
      "[Epoch: 24160] train loss: 1.0811, train acc: 0.5460, val loss: 1.0648, val acc: 0.5609  (best train acc: 0.5693, best val acc: 0.5659, best train loss: 1.0447  @ epoch 23955 )\n",
      "[Epoch: 24180] train loss: 1.0832, train acc: 0.5481, val loss: 1.0656, val acc: 0.5585  (best train acc: 0.5693, best val acc: 0.5659, best train loss: 1.0447  @ epoch 23955 )\n",
      "[Epoch: 24200] train loss: 1.0807, train acc: 0.5479, val loss: 1.0680, val acc: 0.5602  (best train acc: 0.5693, best val acc: 0.5659, best train loss: 1.0447  @ epoch 23955 )\n",
      "[Epoch: 24220] train loss: 1.0828, train acc: 0.5552, val loss: 1.0696, val acc: 0.5538  (best train acc: 0.5693, best val acc: 0.5659, best train loss: 1.0447  @ epoch 23955 )\n",
      "[Epoch: 24240] train loss: 1.0805, train acc: 0.5522, val loss: 1.0702, val acc: 0.5595  (best train acc: 0.5693, best val acc: 0.5659, best train loss: 1.0447  @ epoch 23955 )\n",
      "[Epoch: 24260] train loss: 1.0842, train acc: 0.5492, val loss: 1.0671, val acc: 0.5565  (best train acc: 0.5693, best val acc: 0.5659, best train loss: 1.0447  @ epoch 23955 )\n",
      "[Epoch: 24280] train loss: 1.0940, train acc: 0.5476, val loss: 1.0663, val acc: 0.5558  (best train acc: 0.5693, best val acc: 0.5659, best train loss: 1.0447  @ epoch 23955 )\n",
      "[Epoch: 24300] train loss: 1.0749, train acc: 0.5566, val loss: 1.0633, val acc: 0.5609  (best train acc: 0.5693, best val acc: 0.5659, best train loss: 1.0447  @ epoch 23955 )\n",
      "[Epoch: 24320] train loss: 1.0553, train acc: 0.5578, val loss: 1.0622, val acc: 0.5642  (best train acc: 0.5693, best val acc: 0.5659, best train loss: 1.0447  @ epoch 23955 )\n",
      "[Epoch: 24340] train loss: 1.0793, train acc: 0.5521, val loss: 1.0655, val acc: 0.5589  (best train acc: 0.5693, best val acc: 0.5659, best train loss: 1.0447  @ epoch 23955 )\n",
      "[Epoch: 24360] train loss: 1.0620, train acc: 0.5606, val loss: 1.0667, val acc: 0.5632  (best train acc: 0.5693, best val acc: 0.5659, best train loss: 1.0447  @ epoch 23955 )\n",
      "[Epoch: 24380] train loss: 1.0748, train acc: 0.5607, val loss: 1.0609, val acc: 0.5619  (best train acc: 0.5693, best val acc: 0.5659, best train loss: 1.0447  @ epoch 23955 )\n",
      "[Epoch: 24400] train loss: 1.0857, train acc: 0.5474, val loss: 1.0653, val acc: 0.5585  (best train acc: 0.5693, best val acc: 0.5659, best train loss: 1.0447  @ epoch 23955 )\n",
      "[Epoch: 24420] train loss: 1.0614, train acc: 0.5622, val loss: 1.0611, val acc: 0.5612  (best train acc: 0.5693, best val acc: 0.5659, best train loss: 1.0447  @ epoch 23955 )\n",
      "[Epoch: 24440] train loss: 1.0701, train acc: 0.5524, val loss: 1.0648, val acc: 0.5619  (best train acc: 0.5693, best val acc: 0.5659, best train loss: 1.0447  @ epoch 23955 )\n",
      "[Epoch: 24460] train loss: 1.0747, train acc: 0.5522, val loss: 1.0675, val acc: 0.5589  (best train acc: 0.5693, best val acc: 0.5659, best train loss: 1.0447  @ epoch 23955 )\n",
      "[Epoch: 24480] train loss: 1.0754, train acc: 0.5486, val loss: 1.0647, val acc: 0.5582  (best train acc: 0.5693, best val acc: 0.5659, best train loss: 1.0447  @ epoch 23955 )\n",
      "[Epoch: 24500] train loss: 1.0860, train acc: 0.5523, val loss: 1.0648, val acc: 0.5609  (best train acc: 0.5693, best val acc: 0.5659, best train loss: 1.0447  @ epoch 23955 )\n",
      "[Epoch: 24520] train loss: 1.0739, train acc: 0.5528, val loss: 1.0631, val acc: 0.5629  (best train acc: 0.5693, best val acc: 0.5659, best train loss: 1.0447  @ epoch 23955 )\n",
      "[Epoch: 24540] train loss: 1.0674, train acc: 0.5567, val loss: 1.0669, val acc: 0.5592  (best train acc: 0.5693, best val acc: 0.5673, best train loss: 1.0447  @ epoch 23955 )\n",
      "[Epoch: 24560] train loss: 1.0728, train acc: 0.5518, val loss: 1.0619, val acc: 0.5656  (best train acc: 0.5693, best val acc: 0.5673, best train loss: 1.0447  @ epoch 23955 )\n",
      "[Epoch: 24580] train loss: 1.0780, train acc: 0.5523, val loss: 1.0596, val acc: 0.5612  (best train acc: 0.5693, best val acc: 0.5673, best train loss: 1.0447  @ epoch 23955 )\n",
      "[Epoch: 24600] train loss: 1.0725, train acc: 0.5547, val loss: 1.0604, val acc: 0.5609  (best train acc: 0.5693, best val acc: 0.5673, best train loss: 1.0447  @ epoch 23955 )\n",
      "[Epoch: 24620] train loss: 1.0845, train acc: 0.5454, val loss: 1.0662, val acc: 0.5602  (best train acc: 0.5693, best val acc: 0.5673, best train loss: 1.0447  @ epoch 23955 )\n",
      "[Epoch: 24640] train loss: 1.0791, train acc: 0.5549, val loss: 1.0590, val acc: 0.5629  (best train acc: 0.5693, best val acc: 0.5673, best train loss: 1.0447  @ epoch 23955 )\n",
      "[Epoch: 24660] train loss: 1.0698, train acc: 0.5612, val loss: 1.0593, val acc: 0.5609  (best train acc: 0.5693, best val acc: 0.5673, best train loss: 1.0447  @ epoch 23955 )\n",
      "[Epoch: 24680] train loss: 1.0644, train acc: 0.5561, val loss: 1.0674, val acc: 0.5595  (best train acc: 0.5693, best val acc: 0.5676, best train loss: 1.0447  @ epoch 23955 )\n",
      "[Epoch: 24700] train loss: 1.0590, train acc: 0.5665, val loss: 1.0614, val acc: 0.5632  (best train acc: 0.5693, best val acc: 0.5676, best train loss: 1.0447  @ epoch 23955 )\n",
      "[Epoch: 24720] train loss: 1.0722, train acc: 0.5597, val loss: 1.0660, val acc: 0.5582  (best train acc: 0.5693, best val acc: 0.5676, best train loss: 1.0447  @ epoch 23955 )\n",
      "[Epoch: 24740] train loss: 1.0619, train acc: 0.5578, val loss: 1.0683, val acc: 0.5599  (best train acc: 0.5693, best val acc: 0.5676, best train loss: 1.0447  @ epoch 23955 )\n",
      "[Epoch: 24760] train loss: 1.0588, train acc: 0.5636, val loss: 1.0643, val acc: 0.5626  (best train acc: 0.5693, best val acc: 0.5676, best train loss: 1.0447  @ epoch 23955 )\n",
      "[Epoch: 24780] train loss: 1.0738, train acc: 0.5468, val loss: 1.0650, val acc: 0.5646  (best train acc: 0.5693, best val acc: 0.5676, best train loss: 1.0447  @ epoch 23955 )\n",
      "[Epoch: 24800] train loss: 1.0784, train acc: 0.5466, val loss: 1.0627, val acc: 0.5659  (best train acc: 0.5693, best val acc: 0.5676, best train loss: 1.0447  @ epoch 23955 )\n",
      "[Epoch: 24820] train loss: 1.0709, train acc: 0.5536, val loss: 1.0646, val acc: 0.5592  (best train acc: 0.5697, best val acc: 0.5676, best train loss: 1.0447  @ epoch 23955 )\n",
      "[Epoch: 24840] train loss: 1.0592, train acc: 0.5599, val loss: 1.0603, val acc: 0.5659  (best train acc: 0.5697, best val acc: 0.5676, best train loss: 1.0447  @ epoch 23955 )\n",
      "[Epoch: 24860] train loss: 1.0650, train acc: 0.5600, val loss: 1.0597, val acc: 0.5659  (best train acc: 0.5720, best val acc: 0.5690, best train loss: 1.0447  @ epoch 23955 )\n",
      "[Epoch: 24880] train loss: 1.0840, train acc: 0.5452, val loss: 1.0565, val acc: 0.5666  (best train acc: 0.5720, best val acc: 0.5690, best train loss: 1.0447  @ epoch 23955 )\n",
      "[Epoch: 24900] train loss: 1.0582, train acc: 0.5634, val loss: 1.0548, val acc: 0.5683  (best train acc: 0.5720, best val acc: 0.5690, best train loss: 1.0447  @ epoch 23955 )\n",
      "[Epoch: 24920] train loss: 1.0779, train acc: 0.5503, val loss: 1.0585, val acc: 0.5639  (best train acc: 0.5720, best val acc: 0.5690, best train loss: 1.0447  @ epoch 23955 )\n",
      "[Epoch: 24940] train loss: 1.0534, train acc: 0.5615, val loss: 1.0529, val acc: 0.5629  (best train acc: 0.5720, best val acc: 0.5690, best train loss: 1.0447  @ epoch 23955 )\n",
      "[Epoch: 24960] train loss: 1.0831, train acc: 0.5547, val loss: 1.0574, val acc: 0.5669  (best train acc: 0.5720, best val acc: 0.5710, best train loss: 1.0447  @ epoch 23955 )\n",
      "[Epoch: 24980] train loss: 1.0680, train acc: 0.5568, val loss: 1.0550, val acc: 0.5693  (best train acc: 0.5720, best val acc: 0.5710, best train loss: 1.0447  @ epoch 23955 )\n",
      "[Epoch: 25000] train loss: 1.0634, train acc: 0.5575, val loss: 1.0588, val acc: 0.5690  (best train acc: 0.5720, best val acc: 0.5710, best train loss: 1.0447  @ epoch 23955 )\n",
      "[Epoch: 25020] train loss: 1.0727, train acc: 0.5507, val loss: 1.0558, val acc: 0.5626  (best train acc: 0.5720, best val acc: 0.5710, best train loss: 1.0447  @ epoch 23955 )\n",
      "[Epoch: 25040] train loss: 1.0784, train acc: 0.5472, val loss: 1.0582, val acc: 0.5663  (best train acc: 0.5722, best val acc: 0.5720, best train loss: 1.0447  @ epoch 23955 )\n",
      "[Epoch: 25060] train loss: 1.0541, train acc: 0.5596, val loss: 1.0531, val acc: 0.5666  (best train acc: 0.5722, best val acc: 0.5720, best train loss: 1.0447  @ epoch 23955 )\n",
      "[Epoch: 25080] train loss: 1.0532, train acc: 0.5589, val loss: 1.0639, val acc: 0.5659  (best train acc: 0.5729, best val acc: 0.5720, best train loss: 1.0418  @ epoch 25073 )\n",
      "[Epoch: 25100] train loss: 1.0577, train acc: 0.5601, val loss: 1.0533, val acc: 0.5676  (best train acc: 0.5729, best val acc: 0.5720, best train loss: 1.0418  @ epoch 25073 )\n",
      "[Epoch: 25120] train loss: 1.0830, train acc: 0.5510, val loss: 1.0575, val acc: 0.5680  (best train acc: 0.5729, best val acc: 0.5720, best train loss: 1.0418  @ epoch 25073 )\n",
      "[Epoch: 25140] train loss: 1.0634, train acc: 0.5562, val loss: 1.0523, val acc: 0.5659  (best train acc: 0.5729, best val acc: 0.5727, best train loss: 1.0418  @ epoch 25073 )\n",
      "[Epoch: 25160] train loss: 1.0616, train acc: 0.5598, val loss: 1.0564, val acc: 0.5659  (best train acc: 0.5735, best val acc: 0.5727, best train loss: 1.0418  @ epoch 25073 )\n",
      "[Epoch: 25180] train loss: 1.0551, train acc: 0.5568, val loss: 1.0569, val acc: 0.5622  (best train acc: 0.5735, best val acc: 0.5727, best train loss: 1.0418  @ epoch 25073 )\n",
      "[Epoch: 25200] train loss: 1.0587, train acc: 0.5607, val loss: 1.0592, val acc: 0.5612  (best train acc: 0.5735, best val acc: 0.5727, best train loss: 1.0418  @ epoch 25073 )\n",
      "[Epoch: 25220] train loss: 1.0608, train acc: 0.5625, val loss: 1.0523, val acc: 0.5673  (best train acc: 0.5735, best val acc: 0.5727, best train loss: 1.0385  @ epoch 25218 )\n",
      "[Epoch: 25240] train loss: 1.0803, train acc: 0.5463, val loss: 1.0520, val acc: 0.5666  (best train acc: 0.5735, best val acc: 0.5727, best train loss: 1.0385  @ epoch 25218 )\n",
      "[Epoch: 25260] train loss: 1.0637, train acc: 0.5525, val loss: 1.0517, val acc: 0.5659  (best train acc: 0.5735, best val acc: 0.5730, best train loss: 1.0385  @ epoch 25218 )\n",
      "[Epoch: 25280] train loss: 1.0616, train acc: 0.5557, val loss: 1.0520, val acc: 0.5686  (best train acc: 0.5735, best val acc: 0.5730, best train loss: 1.0385  @ epoch 25218 )\n",
      "[Epoch: 25300] train loss: 1.0698, train acc: 0.5500, val loss: 1.0483, val acc: 0.5639  (best train acc: 0.5735, best val acc: 0.5730, best train loss: 1.0385  @ epoch 25218 )\n",
      "[Epoch: 25320] train loss: 1.0448, train acc: 0.5636, val loss: 1.0515, val acc: 0.5656  (best train acc: 0.5735, best val acc: 0.5740, best train loss: 1.0385  @ epoch 25218 )\n",
      "[Epoch: 25340] train loss: 1.0614, train acc: 0.5536, val loss: 1.0465, val acc: 0.5683  (best train acc: 0.5735, best val acc: 0.5740, best train loss: 1.0385  @ epoch 25218 )\n",
      "[Epoch: 25360] train loss: 1.0641, train acc: 0.5522, val loss: 1.0482, val acc: 0.5632  (best train acc: 0.5735, best val acc: 0.5740, best train loss: 1.0385  @ epoch 25218 )\n",
      "[Epoch: 25380] train loss: 1.0536, train acc: 0.5646, val loss: 1.0504, val acc: 0.5690  (best train acc: 0.5735, best val acc: 0.5740, best train loss: 1.0385  @ epoch 25218 )\n",
      "[Epoch: 25400] train loss: 1.0612, train acc: 0.5585, val loss: 1.0521, val acc: 0.5683  (best train acc: 0.5735, best val acc: 0.5740, best train loss: 1.0385  @ epoch 25218 )\n",
      "[Epoch: 25420] train loss: 1.0591, train acc: 0.5625, val loss: 1.0604, val acc: 0.5636  (best train acc: 0.5735, best val acc: 0.5740, best train loss: 1.0385  @ epoch 25218 )\n",
      "[Epoch: 25440] train loss: 1.0503, train acc: 0.5701, val loss: 1.0480, val acc: 0.5629  (best train acc: 0.5735, best val acc: 0.5740, best train loss: 1.0385  @ epoch 25218 )\n",
      "[Epoch: 25460] train loss: 1.0721, train acc: 0.5528, val loss: 1.0484, val acc: 0.5710  (best train acc: 0.5735, best val acc: 0.5740, best train loss: 1.0385  @ epoch 25218 )\n",
      "[Epoch: 25480] train loss: 1.0648, train acc: 0.5565, val loss: 1.0484, val acc: 0.5669  (best train acc: 0.5735, best val acc: 0.5740, best train loss: 1.0385  @ epoch 25218 )\n",
      "[Epoch: 25500] train loss: 1.0659, train acc: 0.5586, val loss: 1.0449, val acc: 0.5690  (best train acc: 0.5760, best val acc: 0.5740, best train loss: 1.0318  @ epoch 25482 )\n",
      "[Epoch: 25520] train loss: 1.0705, train acc: 0.5524, val loss: 1.0473, val acc: 0.5700  (best train acc: 0.5760, best val acc: 0.5740, best train loss: 1.0318  @ epoch 25482 )\n",
      "[Epoch: 25540] train loss: 1.0477, train acc: 0.5653, val loss: 1.0467, val acc: 0.5680  (best train acc: 0.5760, best val acc: 0.5740, best train loss: 1.0318  @ epoch 25482 )\n",
      "[Epoch: 25560] train loss: 1.0576, train acc: 0.5494, val loss: 1.0415, val acc: 0.5693  (best train acc: 0.5760, best val acc: 0.5740, best train loss: 1.0318  @ epoch 25482 )\n",
      "[Epoch: 25580] train loss: 1.0540, train acc: 0.5612, val loss: 1.0474, val acc: 0.5703  (best train acc: 0.5760, best val acc: 0.5740, best train loss: 1.0318  @ epoch 25482 )\n",
      "[Epoch: 25600] train loss: 1.0560, train acc: 0.5578, val loss: 1.0539, val acc: 0.5730  (best train acc: 0.5760, best val acc: 0.5740, best train loss: 1.0318  @ epoch 25482 )\n",
      "[Epoch: 25620] train loss: 1.0708, train acc: 0.5511, val loss: 1.0404, val acc: 0.5707  (best train acc: 0.5760, best val acc: 0.5740, best train loss: 1.0318  @ epoch 25482 )\n",
      "[Epoch: 25640] train loss: 1.0561, train acc: 0.5612, val loss: 1.0457, val acc: 0.5727  (best train acc: 0.5760, best val acc: 0.5744, best train loss: 1.0318  @ epoch 25482 )\n",
      "[Epoch: 25660] train loss: 1.0486, train acc: 0.5586, val loss: 1.0427, val acc: 0.5696  (best train acc: 0.5760, best val acc: 0.5744, best train loss: 1.0318  @ epoch 25482 )\n",
      "[Epoch: 25680] train loss: 1.0691, train acc: 0.5558, val loss: 1.0417, val acc: 0.5713  (best train acc: 0.5760, best val acc: 0.5744, best train loss: 1.0318  @ epoch 25482 )\n",
      "[Epoch: 25700] train loss: 1.0560, train acc: 0.5620, val loss: 1.0460, val acc: 0.5669  (best train acc: 0.5760, best val acc: 0.5750, best train loss: 1.0318  @ epoch 25482 )\n",
      "[Epoch: 25720] train loss: 1.0453, train acc: 0.5605, val loss: 1.0472, val acc: 0.5723  (best train acc: 0.5760, best val acc: 0.5750, best train loss: 1.0318  @ epoch 25482 )\n",
      "[Epoch: 25740] train loss: 1.0395, train acc: 0.5635, val loss: 1.0417, val acc: 0.5713  (best train acc: 0.5760, best val acc: 0.5750, best train loss: 1.0318  @ epoch 25482 )\n",
      "[Epoch: 25760] train loss: 1.0489, train acc: 0.5546, val loss: 1.0433, val acc: 0.5730  (best train acc: 0.5760, best val acc: 0.5750, best train loss: 1.0318  @ epoch 25482 )\n",
      "[Epoch: 25780] train loss: 1.0542, train acc: 0.5594, val loss: 1.0447, val acc: 0.5690  (best train acc: 0.5760, best val acc: 0.5764, best train loss: 1.0318  @ epoch 25482 )\n",
      "[Epoch: 25800] train loss: 1.0778, train acc: 0.5512, val loss: 1.0624, val acc: 0.5696  (best train acc: 0.5760, best val acc: 0.5764, best train loss: 1.0318  @ epoch 25482 )\n",
      "[Epoch: 25820] train loss: 1.0758, train acc: 0.5434, val loss: 1.0593, val acc: 0.5582  (best train acc: 0.5760, best val acc: 0.5764, best train loss: 1.0318  @ epoch 25482 )\n",
      "[Epoch: 25840] train loss: 1.0678, train acc: 0.5549, val loss: 1.0562, val acc: 0.5642  (best train acc: 0.5760, best val acc: 0.5764, best train loss: 1.0318  @ epoch 25482 )\n",
      "[Epoch: 25860] train loss: 1.0676, train acc: 0.5541, val loss: 1.0490, val acc: 0.5720  (best train acc: 0.5760, best val acc: 0.5764, best train loss: 1.0318  @ epoch 25482 )\n",
      "[Epoch: 25880] train loss: 1.0656, train acc: 0.5518, val loss: 1.0428, val acc: 0.5700  (best train acc: 0.5760, best val acc: 0.5764, best train loss: 1.0318  @ epoch 25482 )\n",
      "[Epoch: 25900] train loss: 1.0408, train acc: 0.5711, val loss: 1.0460, val acc: 0.5713  (best train acc: 0.5760, best val acc: 0.5764, best train loss: 1.0318  @ epoch 25482 )\n",
      "[Epoch: 25920] train loss: 1.0714, train acc: 0.5583, val loss: 1.0420, val acc: 0.5744  (best train acc: 0.5760, best val acc: 0.5764, best train loss: 1.0318  @ epoch 25482 )\n",
      "[Epoch: 25940] train loss: 1.0648, train acc: 0.5519, val loss: 1.0441, val acc: 0.5696  (best train acc: 0.5760, best val acc: 0.5764, best train loss: 1.0318  @ epoch 25482 )\n",
      "[Epoch: 25960] train loss: 1.0494, train acc: 0.5643, val loss: 1.0460, val acc: 0.5713  (best train acc: 0.5760, best val acc: 0.5764, best train loss: 1.0318  @ epoch 25482 )\n",
      "[Epoch: 25980] train loss: 1.0564, train acc: 0.5580, val loss: 1.0428, val acc: 0.5744  (best train acc: 0.5760, best val acc: 0.5774, best train loss: 1.0318  @ epoch 25482 )\n",
      "[Epoch: 26000] train loss: 1.0455, train acc: 0.5649, val loss: 1.0427, val acc: 0.5737  (best train acc: 0.5760, best val acc: 0.5774, best train loss: 1.0318  @ epoch 25482 )\n",
      "[Epoch: 26020] train loss: 1.0580, train acc: 0.5620, val loss: 1.0431, val acc: 0.5690  (best train acc: 0.5760, best val acc: 0.5774, best train loss: 1.0318  @ epoch 25482 )\n",
      "[Epoch: 26040] train loss: 1.0574, train acc: 0.5648, val loss: 1.0510, val acc: 0.5680  (best train acc: 0.5760, best val acc: 0.5774, best train loss: 1.0273  @ epoch 26032 )\n",
      "[Epoch: 26060] train loss: 1.0557, train acc: 0.5653, val loss: 1.0442, val acc: 0.5713  (best train acc: 0.5760, best val acc: 0.5774, best train loss: 1.0273  @ epoch 26032 )\n",
      "[Epoch: 26080] train loss: 1.0607, train acc: 0.5520, val loss: 1.0444, val acc: 0.5686  (best train acc: 0.5760, best val acc: 0.5774, best train loss: 1.0273  @ epoch 26032 )\n",
      "[Epoch: 26100] train loss: 1.0526, train acc: 0.5637, val loss: 1.0449, val acc: 0.5703  (best train acc: 0.5760, best val acc: 0.5774, best train loss: 1.0273  @ epoch 26032 )\n",
      "[Epoch: 26120] train loss: 1.0584, train acc: 0.5594, val loss: 1.0403, val acc: 0.5720  (best train acc: 0.5760, best val acc: 0.5774, best train loss: 1.0273  @ epoch 26032 )\n",
      "[Epoch: 26140] train loss: 1.0493, train acc: 0.5633, val loss: 1.0393, val acc: 0.5720  (best train acc: 0.5760, best val acc: 0.5774, best train loss: 1.0273  @ epoch 26032 )\n",
      "[Epoch: 26160] train loss: 1.0554, train acc: 0.5609, val loss: 1.0407, val acc: 0.5720  (best train acc: 0.5760, best val acc: 0.5774, best train loss: 1.0273  @ epoch 26032 )\n",
      "[Epoch: 26180] train loss: 1.0458, train acc: 0.5624, val loss: 1.0403, val acc: 0.5713  (best train acc: 0.5760, best val acc: 0.5774, best train loss: 1.0273  @ epoch 26032 )\n",
      "[Epoch: 26200] train loss: 1.0616, train acc: 0.5513, val loss: 1.0446, val acc: 0.5717  (best train acc: 0.5763, best val acc: 0.5774, best train loss: 1.0273  @ epoch 26032 )\n",
      "[Epoch: 26220] train loss: 1.0531, train acc: 0.5609, val loss: 1.0431, val acc: 0.5727  (best train acc: 0.5763, best val acc: 0.5774, best train loss: 1.0273  @ epoch 26032 )\n",
      "[Epoch: 26240] train loss: 1.0588, train acc: 0.5567, val loss: 1.0391, val acc: 0.5720  (best train acc: 0.5763, best val acc: 0.5774, best train loss: 1.0273  @ epoch 26032 )\n",
      "[Epoch: 26260] train loss: 1.0415, train acc: 0.5689, val loss: 1.0412, val acc: 0.5767  (best train acc: 0.5763, best val acc: 0.5774, best train loss: 1.0273  @ epoch 26032 )\n",
      "[Epoch: 26280] train loss: 1.0467, train acc: 0.5662, val loss: 1.0446, val acc: 0.5710  (best train acc: 0.5763, best val acc: 0.5774, best train loss: 1.0273  @ epoch 26032 )\n",
      "[Epoch: 26300] train loss: 1.0536, train acc: 0.5588, val loss: 1.0390, val acc: 0.5723  (best train acc: 0.5763, best val acc: 0.5774, best train loss: 1.0273  @ epoch 26032 )\n",
      "[Epoch: 26320] train loss: 1.0442, train acc: 0.5635, val loss: 1.0455, val acc: 0.5723  (best train acc: 0.5768, best val acc: 0.5774, best train loss: 1.0261  @ epoch 26317 )\n",
      "[Epoch: 26340] train loss: 1.0410, train acc: 0.5678, val loss: 1.0389, val acc: 0.5703  (best train acc: 0.5768, best val acc: 0.5774, best train loss: 1.0261  @ epoch 26317 )\n",
      "[Epoch: 26360] train loss: 1.0341, train acc: 0.5681, val loss: 1.0415, val acc: 0.5740  (best train acc: 0.5768, best val acc: 0.5774, best train loss: 1.0261  @ epoch 26317 )\n",
      "[Epoch: 26380] train loss: 1.0380, train acc: 0.5693, val loss: 1.0382, val acc: 0.5710  (best train acc: 0.5768, best val acc: 0.5774, best train loss: 1.0261  @ epoch 26317 )\n",
      "[Epoch: 26400] train loss: 1.0536, train acc: 0.5568, val loss: 1.0377, val acc: 0.5734  (best train acc: 0.5768, best val acc: 0.5774, best train loss: 1.0261  @ epoch 26317 )\n",
      "[Epoch: 26420] train loss: 1.0626, train acc: 0.5473, val loss: 1.0404, val acc: 0.5713  (best train acc: 0.5768, best val acc: 0.5774, best train loss: 1.0261  @ epoch 26317 )\n",
      "[Epoch: 26440] train loss: 1.0540, train acc: 0.5630, val loss: 1.0371, val acc: 0.5720  (best train acc: 0.5768, best val acc: 0.5774, best train loss: 1.0261  @ epoch 26317 )\n",
      "[Epoch: 26460] train loss: 1.0511, train acc: 0.5625, val loss: 1.0412, val acc: 0.5754  (best train acc: 0.5768, best val acc: 0.5774, best train loss: 1.0261  @ epoch 26317 )\n",
      "[Epoch: 26480] train loss: 1.0348, train acc: 0.5702, val loss: 1.0394, val acc: 0.5730  (best train acc: 0.5768, best val acc: 0.5774, best train loss: 1.0261  @ epoch 26317 )\n",
      "[Epoch: 26500] train loss: 1.0595, train acc: 0.5612, val loss: 1.0456, val acc: 0.5683  (best train acc: 0.5768, best val acc: 0.5774, best train loss: 1.0261  @ epoch 26317 )\n",
      "[Epoch: 26520] train loss: 1.0952, train acc: 0.5431, val loss: 1.0923, val acc: 0.5440  (best train acc: 0.5768, best val acc: 0.5774, best train loss: 1.0261  @ epoch 26317 )\n",
      "[Epoch: 26540] train loss: 1.0569, train acc: 0.5566, val loss: 1.0410, val acc: 0.5727  (best train acc: 0.5768, best val acc: 0.5774, best train loss: 1.0261  @ epoch 26317 )\n",
      "[Epoch: 26560] train loss: 1.0528, train acc: 0.5670, val loss: 1.0385, val acc: 0.5700  (best train acc: 0.5768, best val acc: 0.5774, best train loss: 1.0261  @ epoch 26317 )\n",
      "[Epoch: 26580] train loss: 1.0462, train acc: 0.5682, val loss: 1.0411, val acc: 0.5737  (best train acc: 0.5768, best val acc: 0.5784, best train loss: 1.0261  @ epoch 26317 )\n",
      "[Epoch: 26600] train loss: 1.0630, train acc: 0.5525, val loss: 1.0394, val acc: 0.5717  (best train acc: 0.5768, best val acc: 0.5784, best train loss: 1.0261  @ epoch 26317 )\n",
      "[Epoch: 26620] train loss: 1.0507, train acc: 0.5625, val loss: 1.0383, val acc: 0.5754  (best train acc: 0.5768, best val acc: 0.5784, best train loss: 1.0261  @ epoch 26317 )\n",
      "[Epoch: 26640] train loss: 1.0712, train acc: 0.5608, val loss: 1.0434, val acc: 0.5730  (best train acc: 0.5768, best val acc: 0.5784, best train loss: 1.0261  @ epoch 26317 )\n",
      "[Epoch: 26660] train loss: 1.0393, train acc: 0.5654, val loss: 1.0339, val acc: 0.5717  (best train acc: 0.5768, best val acc: 0.5784, best train loss: 1.0261  @ epoch 26317 )\n",
      "[Epoch: 26680] train loss: 1.0514, train acc: 0.5667, val loss: 1.0364, val acc: 0.5747  (best train acc: 0.5768, best val acc: 0.5784, best train loss: 1.0261  @ epoch 26317 )\n",
      "[Epoch: 26700] train loss: 1.0597, train acc: 0.5513, val loss: 1.0435, val acc: 0.5696  (best train acc: 0.5768, best val acc: 0.5784, best train loss: 1.0261  @ epoch 26317 )\n",
      "[Epoch: 26720] train loss: 1.0521, train acc: 0.5651, val loss: 1.0377, val acc: 0.5720  (best train acc: 0.5768, best val acc: 0.5784, best train loss: 1.0261  @ epoch 26317 )\n",
      "[Epoch: 26740] train loss: 1.0490, train acc: 0.5587, val loss: 1.0397, val acc: 0.5747  (best train acc: 0.5768, best val acc: 0.5784, best train loss: 1.0261  @ epoch 26317 )\n",
      "[Epoch: 26760] train loss: 1.0580, train acc: 0.5614, val loss: 1.0392, val acc: 0.5727  (best train acc: 0.5768, best val acc: 0.5784, best train loss: 1.0261  @ epoch 26317 )\n",
      "[Epoch: 26780] train loss: 1.0489, train acc: 0.5567, val loss: 1.0389, val acc: 0.5713  (best train acc: 0.5768, best val acc: 0.5784, best train loss: 1.0261  @ epoch 26317 )\n",
      "[Epoch: 26800] train loss: 1.0423, train acc: 0.5685, val loss: 1.0369, val acc: 0.5723  (best train acc: 0.5768, best val acc: 0.5784, best train loss: 1.0261  @ epoch 26317 )\n",
      "[Epoch: 26820] train loss: 1.0594, train acc: 0.5480, val loss: 1.0364, val acc: 0.5720  (best train acc: 0.5774, best val acc: 0.5784, best train loss: 1.0261  @ epoch 26317 )\n",
      "[Epoch: 26840] train loss: 1.0614, train acc: 0.5515, val loss: 1.0689, val acc: 0.5568  (best train acc: 0.5774, best val acc: 0.5784, best train loss: 1.0261  @ epoch 26317 )\n",
      "[Epoch: 26860] train loss: 1.0618, train acc: 0.5618, val loss: 1.0479, val acc: 0.5723  (best train acc: 0.5774, best val acc: 0.5784, best train loss: 1.0261  @ epoch 26317 )\n",
      "[Epoch: 26880] train loss: 1.0485, train acc: 0.5592, val loss: 1.0424, val acc: 0.5727  (best train acc: 0.5774, best val acc: 0.5784, best train loss: 1.0261  @ epoch 26317 )\n",
      "[Epoch: 26900] train loss: 1.0479, train acc: 0.5616, val loss: 1.0366, val acc: 0.5727  (best train acc: 0.5774, best val acc: 0.5784, best train loss: 1.0261  @ epoch 26317 )\n",
      "[Epoch: 26920] train loss: 1.0524, train acc: 0.5570, val loss: 1.0412, val acc: 0.5727  (best train acc: 0.5774, best val acc: 0.5784, best train loss: 1.0261  @ epoch 26317 )\n",
      "[Epoch: 26940] train loss: 1.0663, train acc: 0.5492, val loss: 1.0410, val acc: 0.5740  (best train acc: 0.5774, best val acc: 0.5784, best train loss: 1.0261  @ epoch 26317 )\n",
      "[Epoch: 26960] train loss: 1.0623, train acc: 0.5636, val loss: 1.0366, val acc: 0.5707  (best train acc: 0.5774, best val acc: 0.5784, best train loss: 1.0261  @ epoch 26317 )\n",
      "[Epoch: 26980] train loss: 1.0531, train acc: 0.5565, val loss: 1.0341, val acc: 0.5703  (best train acc: 0.5774, best val acc: 0.5784, best train loss: 1.0261  @ epoch 26317 )\n",
      "[Epoch: 27000] train loss: 1.0447, train acc: 0.5648, val loss: 1.0409, val acc: 0.5717  (best train acc: 0.5774, best val acc: 0.5784, best train loss: 1.0261  @ epoch 26317 )\n",
      "[Epoch: 27020] train loss: 1.0419, train acc: 0.5664, val loss: 1.0384, val acc: 0.5696  (best train acc: 0.5774, best val acc: 0.5784, best train loss: 1.0261  @ epoch 26317 )\n",
      "[Epoch: 27040] train loss: 1.0558, train acc: 0.5674, val loss: 1.0398, val acc: 0.5713  (best train acc: 0.5774, best val acc: 0.5784, best train loss: 1.0261  @ epoch 26317 )\n",
      "[Epoch: 27060] train loss: 1.0628, train acc: 0.5560, val loss: 1.0362, val acc: 0.5713  (best train acc: 0.5774, best val acc: 0.5784, best train loss: 1.0261  @ epoch 26317 )\n",
      "[Epoch: 27080] train loss: 1.0427, train acc: 0.5651, val loss: 1.0354, val acc: 0.5737  (best train acc: 0.5774, best val acc: 0.5784, best train loss: 1.0261  @ epoch 26317 )\n",
      "[Epoch: 27100] train loss: 1.0598, train acc: 0.5709, val loss: 1.0714, val acc: 0.5589  (best train acc: 0.5774, best val acc: 0.5784, best train loss: 1.0261  @ epoch 26317 )\n",
      "[Epoch: 27120] train loss: 1.0492, train acc: 0.5608, val loss: 1.0516, val acc: 0.5676  (best train acc: 0.5774, best val acc: 0.5784, best train loss: 1.0261  @ epoch 26317 )\n",
      "[Epoch: 27140] train loss: 1.0414, train acc: 0.5688, val loss: 1.0415, val acc: 0.5683  (best train acc: 0.5774, best val acc: 0.5784, best train loss: 1.0261  @ epoch 26317 )\n",
      "[Epoch: 27160] train loss: 1.0469, train acc: 0.5695, val loss: 1.0357, val acc: 0.5720  (best train acc: 0.5774, best val acc: 0.5784, best train loss: 1.0261  @ epoch 26317 )\n",
      "[Epoch: 27180] train loss: 1.0545, train acc: 0.5531, val loss: 1.0358, val acc: 0.5713  (best train acc: 0.5774, best val acc: 0.5784, best train loss: 1.0206  @ epoch 27173 )\n",
      "[Epoch: 27200] train loss: 1.0361, train acc: 0.5688, val loss: 1.0404, val acc: 0.5747  (best train acc: 0.5774, best val acc: 0.5784, best train loss: 1.0206  @ epoch 27173 )\n",
      "[Epoch: 27220] train loss: 1.0628, train acc: 0.5592, val loss: 1.0366, val acc: 0.5723  (best train acc: 0.5774, best val acc: 0.5784, best train loss: 1.0206  @ epoch 27173 )\n",
      "[Epoch: 27240] train loss: 1.0383, train acc: 0.5732, val loss: 1.0347, val acc: 0.5703  (best train acc: 0.5774, best val acc: 0.5784, best train loss: 1.0206  @ epoch 27173 )\n",
      "[Epoch: 27260] train loss: 1.0571, train acc: 0.5630, val loss: 1.0337, val acc: 0.5750  (best train acc: 0.5774, best val acc: 0.5784, best train loss: 1.0206  @ epoch 27173 )\n",
      "[Epoch: 27280] train loss: 1.0315, train acc: 0.5703, val loss: 1.0347, val acc: 0.5757  (best train acc: 0.5774, best val acc: 0.5788, best train loss: 1.0206  @ epoch 27173 )\n",
      "[Epoch: 27300] train loss: 1.0470, train acc: 0.5585, val loss: 1.0338, val acc: 0.5761  (best train acc: 0.5776, best val acc: 0.5788, best train loss: 1.0206  @ epoch 27173 )\n",
      "[Epoch: 27320] train loss: 1.0504, train acc: 0.5622, val loss: 1.0379, val acc: 0.5703  (best train acc: 0.5776, best val acc: 0.5788, best train loss: 1.0206  @ epoch 27173 )\n",
      "[Epoch: 27340] train loss: 1.0481, train acc: 0.5658, val loss: 1.0522, val acc: 0.5663  (best train acc: 0.5776, best val acc: 0.5788, best train loss: 1.0206  @ epoch 27173 )\n",
      "[Epoch: 27360] train loss: 1.0634, train acc: 0.5646, val loss: 1.0468, val acc: 0.5642  (best train acc: 0.5776, best val acc: 0.5788, best train loss: 1.0206  @ epoch 27173 )\n",
      "[Epoch: 27380] train loss: 1.0399, train acc: 0.5695, val loss: 1.0391, val acc: 0.5747  (best train acc: 0.5776, best val acc: 0.5788, best train loss: 1.0206  @ epoch 27173 )\n",
      "[Epoch: 27400] train loss: 1.0385, train acc: 0.5673, val loss: 1.0319, val acc: 0.5744  (best train acc: 0.5776, best val acc: 0.5788, best train loss: 1.0206  @ epoch 27173 )\n",
      "[Epoch: 27420] train loss: 1.0400, train acc: 0.5649, val loss: 1.0374, val acc: 0.5713  (best train acc: 0.5776, best val acc: 0.5788, best train loss: 1.0206  @ epoch 27173 )\n",
      "[Epoch: 27440] train loss: 1.0580, train acc: 0.5546, val loss: 1.0313, val acc: 0.5720  (best train acc: 0.5776, best val acc: 0.5798, best train loss: 1.0206  @ epoch 27173 )\n",
      "[Epoch: 27460] train loss: 1.0391, train acc: 0.5677, val loss: 1.0376, val acc: 0.5747  (best train acc: 0.5776, best val acc: 0.5798, best train loss: 1.0206  @ epoch 27173 )\n",
      "[Epoch: 27480] train loss: 1.0342, train acc: 0.5675, val loss: 1.0340, val acc: 0.5740  (best train acc: 0.5776, best val acc: 0.5798, best train loss: 1.0206  @ epoch 27173 )\n",
      "[Epoch: 27500] train loss: 1.0486, train acc: 0.5630, val loss: 1.0360, val acc: 0.5666  (best train acc: 0.5776, best val acc: 0.5798, best train loss: 1.0206  @ epoch 27173 )\n",
      "[Epoch: 27520] train loss: 1.0477, train acc: 0.5610, val loss: 1.0391, val acc: 0.5707  (best train acc: 0.5776, best val acc: 0.5798, best train loss: 1.0206  @ epoch 27173 )\n",
      "[Epoch: 27540] train loss: 1.0423, train acc: 0.5636, val loss: 1.0349, val acc: 0.5737  (best train acc: 0.5776, best val acc: 0.5798, best train loss: 1.0206  @ epoch 27173 )\n",
      "[Epoch: 27560] train loss: 1.0366, train acc: 0.5656, val loss: 1.0307, val acc: 0.5713  (best train acc: 0.5776, best val acc: 0.5798, best train loss: 1.0206  @ epoch 27173 )\n",
      "[Epoch: 27580] train loss: 1.0481, train acc: 0.5616, val loss: 1.0348, val acc: 0.5740  (best train acc: 0.5776, best val acc: 0.5798, best train loss: 1.0206  @ epoch 27173 )\n",
      "[Epoch: 27600] train loss: 1.0292, train acc: 0.5722, val loss: 1.0336, val acc: 0.5727  (best train acc: 0.5776, best val acc: 0.5798, best train loss: 1.0206  @ epoch 27173 )\n",
      "[Epoch: 27620] train loss: 1.0402, train acc: 0.5611, val loss: 1.0307, val acc: 0.5740  (best train acc: 0.5776, best val acc: 0.5798, best train loss: 1.0206  @ epoch 27173 )\n",
      "[Epoch: 27640] train loss: 1.0479, train acc: 0.5568, val loss: 1.0391, val acc: 0.5771  (best train acc: 0.5776, best val acc: 0.5798, best train loss: 1.0206  @ epoch 27173 )\n",
      "[Epoch: 27660] train loss: 1.0440, train acc: 0.5622, val loss: 1.0323, val acc: 0.5777  (best train acc: 0.5782, best val acc: 0.5798, best train loss: 1.0206  @ epoch 27173 )\n",
      "[Epoch: 27680] train loss: 1.0569, train acc: 0.5552, val loss: 1.0480, val acc: 0.5720  (best train acc: 0.5782, best val acc: 0.5798, best train loss: 1.0206  @ epoch 27173 )\n",
      "[Epoch: 27700] train loss: 1.0406, train acc: 0.5663, val loss: 1.0334, val acc: 0.5767  (best train acc: 0.5782, best val acc: 0.5798, best train loss: 1.0206  @ epoch 27173 )\n",
      "[Epoch: 27720] train loss: 1.0402, train acc: 0.5668, val loss: 1.0381, val acc: 0.5700  (best train acc: 0.5782, best val acc: 0.5798, best train loss: 1.0195  @ epoch 27715 )\n",
      "[Epoch: 27740] train loss: 1.0382, train acc: 0.5682, val loss: 1.0304, val acc: 0.5754  (best train acc: 0.5782, best val acc: 0.5798, best train loss: 1.0195  @ epoch 27715 )\n",
      "[Epoch: 27760] train loss: 1.0432, train acc: 0.5721, val loss: 1.0312, val acc: 0.5707  (best train acc: 0.5782, best val acc: 0.5798, best train loss: 1.0195  @ epoch 27715 )\n",
      "[Epoch: 27780] train loss: 1.0367, train acc: 0.5677, val loss: 1.0318, val acc: 0.5727  (best train acc: 0.5787, best val acc: 0.5798, best train loss: 1.0195  @ epoch 27715 )\n",
      "[Epoch: 27800] train loss: 1.0400, train acc: 0.5634, val loss: 1.0313, val acc: 0.5690  (best train acc: 0.5787, best val acc: 0.5798, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 27820] train loss: 1.0182, train acc: 0.5776, val loss: 1.0310, val acc: 0.5717  (best train acc: 0.5787, best val acc: 0.5798, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 27840] train loss: 1.0454, train acc: 0.5603, val loss: 1.0304, val acc: 0.5750  (best train acc: 0.5787, best val acc: 0.5798, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 27860] train loss: 1.0356, train acc: 0.5703, val loss: 1.0301, val acc: 0.5757  (best train acc: 0.5787, best val acc: 0.5798, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 27880] train loss: 1.0330, train acc: 0.5656, val loss: 1.0301, val acc: 0.5727  (best train acc: 0.5787, best val acc: 0.5798, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 27900] train loss: 1.0461, train acc: 0.5575, val loss: 1.0299, val acc: 0.5750  (best train acc: 0.5787, best val acc: 0.5798, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 27920] train loss: 1.0236, train acc: 0.5677, val loss: 1.0299, val acc: 0.5771  (best train acc: 0.5787, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 27940] train loss: 1.0515, train acc: 0.5566, val loss: 1.3834, val acc: 0.4614  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 27960] train loss: 1.2066, train acc: 0.5132, val loss: 1.1752, val acc: 0.5325  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 27980] train loss: 1.1580, train acc: 0.5232, val loss: 1.1469, val acc: 0.5298  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 28000] train loss: 1.1634, train acc: 0.5260, val loss: 1.1444, val acc: 0.5312  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 28020] train loss: 1.1315, train acc: 0.5395, val loss: 1.1432, val acc: 0.5309  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 28040] train loss: 1.1472, train acc: 0.5319, val loss: 1.1428, val acc: 0.5305  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 28060] train loss: 1.1449, train acc: 0.5304, val loss: 1.1432, val acc: 0.5336  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 28080] train loss: 1.1558, train acc: 0.5260, val loss: 1.1422, val acc: 0.5315  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 28100] train loss: 1.1595, train acc: 0.5294, val loss: 1.1414, val acc: 0.5305  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 28120] train loss: 1.1413, train acc: 0.5325, val loss: 1.1408, val acc: 0.5309  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 28140] train loss: 1.1396, train acc: 0.5352, val loss: 1.1403, val acc: 0.5322  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 28160] train loss: 1.1341, train acc: 0.5353, val loss: 1.1414, val acc: 0.5352  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 28180] train loss: 1.1443, train acc: 0.5296, val loss: 1.1432, val acc: 0.5406  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 28200] train loss: 1.1599, train acc: 0.5280, val loss: 1.1407, val acc: 0.5376  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 28220] train loss: 1.1311, train acc: 0.5388, val loss: 1.1410, val acc: 0.5410  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 28240] train loss: 1.1408, train acc: 0.5357, val loss: 1.1383, val acc: 0.5325  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 28260] train loss: 1.1408, train acc: 0.5341, val loss: 1.1381, val acc: 0.5352  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 28280] train loss: 1.1341, train acc: 0.5341, val loss: 1.1376, val acc: 0.5339  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 28300] train loss: 1.1527, train acc: 0.5274, val loss: 1.1377, val acc: 0.5349  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 28320] train loss: 1.1486, train acc: 0.5291, val loss: 1.1370, val acc: 0.5336  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 28340] train loss: 1.1432, train acc: 0.5335, val loss: 1.1381, val acc: 0.5352  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 28360] train loss: 1.1358, train acc: 0.5396, val loss: 1.1361, val acc: 0.5336  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 28380] train loss: 1.1249, train acc: 0.5382, val loss: 1.1368, val acc: 0.5302  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 28400] train loss: 1.1253, train acc: 0.5412, val loss: 1.1351, val acc: 0.5315  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 28420] train loss: 1.1428, train acc: 0.5270, val loss: 1.1360, val acc: 0.5359  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 28440] train loss: 1.1337, train acc: 0.5364, val loss: 1.1347, val acc: 0.5356  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 28460] train loss: 1.1262, train acc: 0.5444, val loss: 1.1346, val acc: 0.5339  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 28480] train loss: 1.1285, train acc: 0.5388, val loss: 1.1358, val acc: 0.5356  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 28500] train loss: 1.1330, train acc: 0.5375, val loss: 1.1340, val acc: 0.5346  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 28520] train loss: 1.1370, train acc: 0.5303, val loss: 1.1328, val acc: 0.5346  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 28540] train loss: 1.1239, train acc: 0.5405, val loss: 1.1335, val acc: 0.5349  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 28560] train loss: 1.1366, train acc: 0.5301, val loss: 1.1325, val acc: 0.5379  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 28580] train loss: 1.1502, train acc: 0.5264, val loss: 1.1323, val acc: 0.5352  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 28600] train loss: 1.1370, train acc: 0.5326, val loss: 1.1323, val acc: 0.5349  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 28620] train loss: 1.1318, train acc: 0.5373, val loss: 1.1318, val acc: 0.5356  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 28640] train loss: 1.1400, train acc: 0.5320, val loss: 1.1311, val acc: 0.5352  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 28660] train loss: 1.1480, train acc: 0.5267, val loss: 1.1321, val acc: 0.5336  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 28680] train loss: 1.1346, train acc: 0.5320, val loss: 1.1306, val acc: 0.5373  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 28700] train loss: 1.1561, train acc: 0.5245, val loss: 1.1304, val acc: 0.5383  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 28720] train loss: 1.1224, train acc: 0.5417, val loss: 1.1301, val acc: 0.5366  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 28740] train loss: 1.1378, train acc: 0.5333, val loss: 1.1302, val acc: 0.5325  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 28760] train loss: 1.1328, train acc: 0.5366, val loss: 1.1303, val acc: 0.5339  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 28780] train loss: 1.1361, train acc: 0.5378, val loss: 1.1293, val acc: 0.5363  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 28800] train loss: 1.1473, train acc: 0.5329, val loss: 1.1304, val acc: 0.5332  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 28820] train loss: 1.1182, train acc: 0.5382, val loss: 1.1291, val acc: 0.5386  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 28840] train loss: 1.1389, train acc: 0.5346, val loss: 1.1295, val acc: 0.5352  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 28860] train loss: 1.1414, train acc: 0.5281, val loss: 1.1367, val acc: 0.5383  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 28880] train loss: 1.1244, train acc: 0.5367, val loss: 1.1286, val acc: 0.5336  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 28900] train loss: 1.1457, train acc: 0.5273, val loss: 1.1290, val acc: 0.5319  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 28920] train loss: 1.1392, train acc: 0.5302, val loss: 1.1309, val acc: 0.5329  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 28940] train loss: 1.1337, train acc: 0.5358, val loss: 1.1321, val acc: 0.5363  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 28960] train loss: 1.1206, train acc: 0.5382, val loss: 1.1272, val acc: 0.5393  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 28980] train loss: 1.1259, train acc: 0.5400, val loss: 1.1277, val acc: 0.5376  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 29000] train loss: 1.1333, train acc: 0.5302, val loss: 1.1267, val acc: 0.5369  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 29020] train loss: 1.1252, train acc: 0.5351, val loss: 1.1287, val acc: 0.5366  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 29040] train loss: 1.1176, train acc: 0.5416, val loss: 1.1315, val acc: 0.5336  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 29060] train loss: 1.1321, train acc: 0.5350, val loss: 1.1282, val acc: 0.5342  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 29080] train loss: 1.1325, train acc: 0.5319, val loss: 1.1305, val acc: 0.5278  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 29100] train loss: 1.1369, train acc: 0.5341, val loss: 1.1280, val acc: 0.5352  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 29120] train loss: 1.1031, train acc: 0.5480, val loss: 1.1263, val acc: 0.5369  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 29140] train loss: 1.1365, train acc: 0.5310, val loss: 1.1291, val acc: 0.5359  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 29160] train loss: 1.1251, train acc: 0.5356, val loss: 1.1257, val acc: 0.5383  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 29180] train loss: 1.1333, train acc: 0.5376, val loss: 1.1284, val acc: 0.5373  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 29200] train loss: 1.1247, train acc: 0.5330, val loss: 1.1256, val acc: 0.5373  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 29220] train loss: 1.1231, train acc: 0.5361, val loss: 1.1253, val acc: 0.5363  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 29240] train loss: 1.1332, train acc: 0.5446, val loss: 1.1269, val acc: 0.5379  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 29260] train loss: 1.1317, train acc: 0.5363, val loss: 1.1246, val acc: 0.5363  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 29280] train loss: 1.1355, train acc: 0.5337, val loss: 1.1328, val acc: 0.5295  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 29300] train loss: 1.1401, train acc: 0.5312, val loss: 1.1248, val acc: 0.5363  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 29320] train loss: 1.1490, train acc: 0.5291, val loss: 1.1268, val acc: 0.5332  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 29340] train loss: 1.1318, train acc: 0.5271, val loss: 1.1258, val acc: 0.5363  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 29360] train loss: 1.1551, train acc: 0.5201, val loss: 1.1240, val acc: 0.5379  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 29380] train loss: 1.1312, train acc: 0.5351, val loss: 1.1257, val acc: 0.5332  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 29400] train loss: 1.1499, train acc: 0.5321, val loss: 1.1238, val acc: 0.5376  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 29420] train loss: 1.1265, train acc: 0.5362, val loss: 1.1346, val acc: 0.5352  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 29440] train loss: 1.1443, train acc: 0.5298, val loss: 1.1255, val acc: 0.5393  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 29460] train loss: 1.1320, train acc: 0.5330, val loss: 1.1241, val acc: 0.5339  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 29480] train loss: 1.1158, train acc: 0.5367, val loss: 1.1258, val acc: 0.5336  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 29500] train loss: 1.1374, train acc: 0.5294, val loss: 1.1250, val acc: 0.5329  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 29520] train loss: 1.1355, train acc: 0.5258, val loss: 1.1229, val acc: 0.5386  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 29540] train loss: 1.1311, train acc: 0.5361, val loss: 1.1231, val acc: 0.5339  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 29560] train loss: 1.1135, train acc: 0.5443, val loss: 1.1266, val acc: 0.5393  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 29580] train loss: 1.1385, train acc: 0.5299, val loss: 1.1233, val acc: 0.5336  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 29600] train loss: 1.1461, train acc: 0.5264, val loss: 1.1238, val acc: 0.5390  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 29620] train loss: 1.1353, train acc: 0.5360, val loss: 1.1236, val acc: 0.5309  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 29640] train loss: 1.1171, train acc: 0.5380, val loss: 1.1235, val acc: 0.5339  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 29660] train loss: 1.1448, train acc: 0.5236, val loss: 1.1225, val acc: 0.5369  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 29680] train loss: 1.1370, train acc: 0.5341, val loss: 1.1219, val acc: 0.5349  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 29700] train loss: 1.1206, train acc: 0.5388, val loss: 1.1226, val acc: 0.5396  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 29720] train loss: 1.1483, train acc: 0.5207, val loss: 1.1228, val acc: 0.5386  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 29740] train loss: 1.1434, train acc: 0.5295, val loss: 1.1282, val acc: 0.5413  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 29760] train loss: 1.1297, train acc: 0.5380, val loss: 1.1235, val acc: 0.5339  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 29780] train loss: 1.1285, train acc: 0.5327, val loss: 1.1230, val acc: 0.5346  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 29800] train loss: 1.1248, train acc: 0.5391, val loss: 1.1227, val acc: 0.5379  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 29820] train loss: 1.1188, train acc: 0.5382, val loss: 1.1228, val acc: 0.5342  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 29840] train loss: 1.1325, train acc: 0.5320, val loss: 1.1231, val acc: 0.5390  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 29860] train loss: 1.1130, train acc: 0.5421, val loss: 1.1214, val acc: 0.5363  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 29880] train loss: 1.1427, train acc: 0.5297, val loss: 1.1192, val acc: 0.5363  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 29900] train loss: 1.1192, train acc: 0.5382, val loss: 1.1245, val acc: 0.5346  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 29920] train loss: 1.1253, train acc: 0.5343, val loss: 1.1182, val acc: 0.5406  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 29940] train loss: 1.1218, train acc: 0.5335, val loss: 1.1137, val acc: 0.5393  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 29960] train loss: 1.1132, train acc: 0.5380, val loss: 1.1139, val acc: 0.5359  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 29980] train loss: 1.1228, train acc: 0.5385, val loss: 1.1084, val acc: 0.5359  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 30000] train loss: 1.1132, train acc: 0.5415, val loss: 1.1108, val acc: 0.5376  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 30020] train loss: 1.1350, train acc: 0.5322, val loss: 1.1131, val acc: 0.5379  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 30040] train loss: 1.1176, train acc: 0.5377, val loss: 1.1074, val acc: 0.5396  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 30060] train loss: 1.1316, train acc: 0.5307, val loss: 1.1465, val acc: 0.5238  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 30080] train loss: 1.1167, train acc: 0.5348, val loss: 1.1160, val acc: 0.5359  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 30100] train loss: 1.0950, train acc: 0.5506, val loss: 1.1068, val acc: 0.5332  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 30120] train loss: 1.1442, train acc: 0.5211, val loss: 1.1375, val acc: 0.5275  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 30140] train loss: 1.1045, train acc: 0.5450, val loss: 1.1063, val acc: 0.5383  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 30160] train loss: 1.1073, train acc: 0.5427, val loss: 1.1018, val acc: 0.5420  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 30180] train loss: 1.1112, train acc: 0.5388, val loss: 1.0983, val acc: 0.5447  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 30200] train loss: 1.1038, train acc: 0.5436, val loss: 1.0976, val acc: 0.5457  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 30220] train loss: 1.0838, train acc: 0.5562, val loss: 1.0970, val acc: 0.5460  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 30240] train loss: 1.0960, train acc: 0.5450, val loss: 1.0945, val acc: 0.5481  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 30260] train loss: 1.1060, train acc: 0.5319, val loss: 1.1101, val acc: 0.5423  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 30280] train loss: 1.1111, train acc: 0.5346, val loss: 1.1064, val acc: 0.5491  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 30300] train loss: 1.1181, train acc: 0.5356, val loss: 1.0985, val acc: 0.5511  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 30320] train loss: 1.0851, train acc: 0.5579, val loss: 1.0940, val acc: 0.5531  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 30340] train loss: 1.0763, train acc: 0.5611, val loss: 1.0890, val acc: 0.5545  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 30360] train loss: 1.0872, train acc: 0.5504, val loss: 1.0941, val acc: 0.5497  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 30380] train loss: 1.0890, train acc: 0.5523, val loss: 1.1083, val acc: 0.5444  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 30400] train loss: 1.1514, train acc: 0.5312, val loss: 1.1404, val acc: 0.5369  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 30420] train loss: 1.1261, train acc: 0.5474, val loss: 1.1344, val acc: 0.5393  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 30440] train loss: 1.1446, train acc: 0.5350, val loss: 1.1340, val acc: 0.5427  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 30460] train loss: 1.1368, train acc: 0.5385, val loss: 1.1332, val acc: 0.5390  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 30480] train loss: 1.1342, train acc: 0.5362, val loss: 1.1337, val acc: 0.5329  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 30500] train loss: 1.1221, train acc: 0.5426, val loss: 1.1322, val acc: 0.5369  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 30520] train loss: 1.1240, train acc: 0.5416, val loss: 1.1316, val acc: 0.5410  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 30540] train loss: 1.1281, train acc: 0.5393, val loss: 1.1314, val acc: 0.5406  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 30560] train loss: 1.1394, train acc: 0.5317, val loss: 1.1309, val acc: 0.5420  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 30580] train loss: 1.1381, train acc: 0.5360, val loss: 1.1310, val acc: 0.5400  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 30600] train loss: 1.1433, train acc: 0.5320, val loss: 1.1305, val acc: 0.5403  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 30620] train loss: 1.1216, train acc: 0.5375, val loss: 1.1310, val acc: 0.5386  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 30640] train loss: 1.1236, train acc: 0.5438, val loss: 1.1299, val acc: 0.5420  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 30660] train loss: 1.1221, train acc: 0.5421, val loss: 1.1314, val acc: 0.5346  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 30680] train loss: 1.1228, train acc: 0.5422, val loss: 1.1298, val acc: 0.5420  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 30700] train loss: 1.1264, train acc: 0.5398, val loss: 1.1291, val acc: 0.5417  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 30720] train loss: 1.1211, train acc: 0.5408, val loss: 1.1290, val acc: 0.5413  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 30740] train loss: 1.1299, train acc: 0.5367, val loss: 1.1292, val acc: 0.5400  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 30760] train loss: 1.1217, train acc: 0.5400, val loss: 1.1293, val acc: 0.5406  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 30780] train loss: 1.1352, train acc: 0.5400, val loss: 1.1290, val acc: 0.5406  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 30800] train loss: 1.1198, train acc: 0.5458, val loss: 1.1284, val acc: 0.5423  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 30820] train loss: 1.1321, train acc: 0.5317, val loss: 1.1282, val acc: 0.5396  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 30840] train loss: 1.1346, train acc: 0.5349, val loss: 1.1285, val acc: 0.5393  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 30860] train loss: 1.1308, train acc: 0.5369, val loss: 1.1281, val acc: 0.5390  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 30880] train loss: 1.1270, train acc: 0.5395, val loss: 1.1287, val acc: 0.5379  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 30900] train loss: 1.1333, train acc: 0.5323, val loss: 1.1282, val acc: 0.5406  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 30920] train loss: 1.1176, train acc: 0.5402, val loss: 1.1282, val acc: 0.5406  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 30940] train loss: 1.1252, train acc: 0.5371, val loss: 1.1281, val acc: 0.5366  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 30960] train loss: 1.1317, train acc: 0.5396, val loss: 1.1294, val acc: 0.5383  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 30980] train loss: 1.1395, train acc: 0.5316, val loss: 1.1279, val acc: 0.5390  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 31000] train loss: 1.1396, train acc: 0.5352, val loss: 1.1276, val acc: 0.5400  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 31020] train loss: 1.1224, train acc: 0.5419, val loss: 1.1279, val acc: 0.5390  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 31040] train loss: 1.1217, train acc: 0.5374, val loss: 1.1276, val acc: 0.5403  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 31060] train loss: 1.1078, train acc: 0.5461, val loss: 1.1279, val acc: 0.5356  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 31080] train loss: 1.1287, train acc: 0.5392, val loss: 1.1277, val acc: 0.5386  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 31100] train loss: 1.1338, train acc: 0.5339, val loss: 1.1282, val acc: 0.5369  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 31120] train loss: 1.1201, train acc: 0.5393, val loss: 1.1274, val acc: 0.5400  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 31140] train loss: 1.1139, train acc: 0.5481, val loss: 1.1272, val acc: 0.5417  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 31160] train loss: 1.1208, train acc: 0.5439, val loss: 1.1275, val acc: 0.5363  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 31180] train loss: 1.1147, train acc: 0.5425, val loss: 1.1275, val acc: 0.5363  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 31200] train loss: 1.1166, train acc: 0.5448, val loss: 1.1274, val acc: 0.5400  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 31220] train loss: 1.1216, train acc: 0.5416, val loss: 1.1290, val acc: 0.5373  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 31240] train loss: 1.1345, train acc: 0.5346, val loss: 1.1272, val acc: 0.5403  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 31260] train loss: 1.1305, train acc: 0.5371, val loss: 1.1302, val acc: 0.5390  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 31280] train loss: 1.1201, train acc: 0.5410, val loss: 1.1268, val acc: 0.5383  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 31300] train loss: 1.1176, train acc: 0.5378, val loss: 1.1288, val acc: 0.5363  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 31320] train loss: 1.1243, train acc: 0.5405, val loss: 1.1269, val acc: 0.5410  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 31340] train loss: 1.1331, train acc: 0.5398, val loss: 1.1267, val acc: 0.5403  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 31360] train loss: 1.1451, train acc: 0.5276, val loss: 1.1272, val acc: 0.5373  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 31380] train loss: 1.1108, train acc: 0.5423, val loss: 1.1279, val acc: 0.5369  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 31400] train loss: 1.1130, train acc: 0.5476, val loss: 1.1264, val acc: 0.5433  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 31420] train loss: 1.1299, train acc: 0.5395, val loss: 1.1263, val acc: 0.5403  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 31440] train loss: 1.1110, train acc: 0.5442, val loss: 1.1262, val acc: 0.5403  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 31460] train loss: 1.1249, train acc: 0.5361, val loss: 1.1262, val acc: 0.5423  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 31480] train loss: 1.1231, train acc: 0.5364, val loss: 1.1259, val acc: 0.5400  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 31500] train loss: 1.1254, train acc: 0.5388, val loss: 1.1267, val acc: 0.5363  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 31520] train loss: 1.1419, train acc: 0.5297, val loss: 1.1270, val acc: 0.5373  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 31540] train loss: 1.1359, train acc: 0.5313, val loss: 1.1261, val acc: 0.5393  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 31560] train loss: 1.1389, train acc: 0.5336, val loss: 1.1259, val acc: 0.5373  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 31580] train loss: 1.1175, train acc: 0.5378, val loss: 1.1251, val acc: 0.5383  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 31600] train loss: 1.1298, train acc: 0.5363, val loss: 1.1260, val acc: 0.5366  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 31620] train loss: 1.1322, train acc: 0.5385, val loss: 1.1248, val acc: 0.5417  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 31640] train loss: 1.1466, train acc: 0.5275, val loss: 1.1267, val acc: 0.5329  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 31660] train loss: 1.1078, train acc: 0.5431, val loss: 1.1252, val acc: 0.5379  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 31680] train loss: 1.1224, train acc: 0.5388, val loss: 1.1243, val acc: 0.5400  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 31700] train loss: 1.1246, train acc: 0.5374, val loss: 1.1244, val acc: 0.5383  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 31720] train loss: 1.1329, train acc: 0.5401, val loss: 1.1244, val acc: 0.5390  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 31740] train loss: 1.1182, train acc: 0.5418, val loss: 1.1255, val acc: 0.5336  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 31760] train loss: 1.1130, train acc: 0.5433, val loss: 1.1253, val acc: 0.5373  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 31780] train loss: 1.1311, train acc: 0.5393, val loss: 1.1238, val acc: 0.5400  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 31800] train loss: 1.1256, train acc: 0.5380, val loss: 1.1245, val acc: 0.5363  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 31820] train loss: 1.1240, train acc: 0.5406, val loss: 1.1238, val acc: 0.5406  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 31840] train loss: 1.1164, train acc: 0.5408, val loss: 1.1237, val acc: 0.5400  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 31860] train loss: 1.1369, train acc: 0.5328, val loss: 1.1230, val acc: 0.5393  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 31880] train loss: 1.1254, train acc: 0.5356, val loss: 1.1174, val acc: 0.5406  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 31900] train loss: 1.1072, train acc: 0.5421, val loss: 1.1103, val acc: 0.5400  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 31920] train loss: 1.0980, train acc: 0.5437, val loss: 1.0986, val acc: 0.5447  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 31940] train loss: 1.1055, train acc: 0.5445, val loss: 1.0941, val acc: 0.5511  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 31960] train loss: 1.1159, train acc: 0.5314, val loss: 1.0950, val acc: 0.5477  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 31980] train loss: 1.1548, train acc: 0.5124, val loss: 1.1232, val acc: 0.5339  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 32000] train loss: 1.1178, train acc: 0.5381, val loss: 1.1169, val acc: 0.5413  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 32020] train loss: 1.0831, train acc: 0.5474, val loss: 1.0969, val acc: 0.5454  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 32040] train loss: 1.0932, train acc: 0.5455, val loss: 1.0916, val acc: 0.5464  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 32060] train loss: 1.0884, train acc: 0.5468, val loss: 1.0946, val acc: 0.5504  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 32080] train loss: 1.0871, train acc: 0.5532, val loss: 1.0898, val acc: 0.5568  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 32100] train loss: 1.0938, train acc: 0.5445, val loss: 1.0886, val acc: 0.5551  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 32120] train loss: 1.1038, train acc: 0.5442, val loss: 1.0884, val acc: 0.5575  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 32140] train loss: 1.1104, train acc: 0.5425, val loss: 1.1065, val acc: 0.5282  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 32160] train loss: 1.1393, train acc: 0.5221, val loss: 1.1182, val acc: 0.5420  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 32180] train loss: 1.0920, train acc: 0.5479, val loss: 1.0894, val acc: 0.5538  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 32200] train loss: 1.0942, train acc: 0.5457, val loss: 1.0866, val acc: 0.5535  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 32220] train loss: 1.0760, train acc: 0.5559, val loss: 1.0863, val acc: 0.5555  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 32240] train loss: 1.0952, train acc: 0.5455, val loss: 1.0850, val acc: 0.5565  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 32260] train loss: 1.0845, train acc: 0.5481, val loss: 1.0851, val acc: 0.5531  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 32280] train loss: 1.0974, train acc: 0.5464, val loss: 1.0847, val acc: 0.5558  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 32300] train loss: 1.0808, train acc: 0.5523, val loss: 1.0834, val acc: 0.5555  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 32320] train loss: 1.1036, train acc: 0.5409, val loss: 1.0832, val acc: 0.5555  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 32340] train loss: 1.1012, train acc: 0.5434, val loss: 1.0827, val acc: 0.5541  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 32360] train loss: 1.0895, train acc: 0.5464, val loss: 1.0832, val acc: 0.5555  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 32380] train loss: 1.0799, train acc: 0.5536, val loss: 1.0823, val acc: 0.5528  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 32400] train loss: 1.0791, train acc: 0.5523, val loss: 1.0816, val acc: 0.5541  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 32420] train loss: 1.0841, train acc: 0.5518, val loss: 1.0818, val acc: 0.5558  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 32440] train loss: 1.1012, train acc: 0.5448, val loss: 1.0840, val acc: 0.5541  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 32460] train loss: 1.0876, train acc: 0.5497, val loss: 1.0845, val acc: 0.5585  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 32480] train loss: 1.0656, train acc: 0.5598, val loss: 1.0817, val acc: 0.5545  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 32500] train loss: 1.0946, train acc: 0.5452, val loss: 1.0815, val acc: 0.5585  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 32520] train loss: 1.0831, train acc: 0.5534, val loss: 1.0813, val acc: 0.5535  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 32540] train loss: 1.0663, train acc: 0.5570, val loss: 1.0801, val acc: 0.5545  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 32560] train loss: 1.0820, train acc: 0.5535, val loss: 1.0836, val acc: 0.5555  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 32580] train loss: 1.0836, train acc: 0.5492, val loss: 1.0826, val acc: 0.5524  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 32600] train loss: 1.0891, train acc: 0.5507, val loss: 1.0810, val acc: 0.5585  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 32620] train loss: 1.0897, train acc: 0.5476, val loss: 1.0803, val acc: 0.5538  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 32640] train loss: 1.0876, train acc: 0.5518, val loss: 1.0798, val acc: 0.5578  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 32660] train loss: 1.0796, train acc: 0.5506, val loss: 1.0792, val acc: 0.5524  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 32680] train loss: 1.0825, train acc: 0.5543, val loss: 1.0801, val acc: 0.5565  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 32700] train loss: 1.0744, train acc: 0.5560, val loss: 1.0791, val acc: 0.5562  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 32720] train loss: 1.0853, train acc: 0.5496, val loss: 1.0790, val acc: 0.5565  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 32740] train loss: 1.0818, train acc: 0.5518, val loss: 1.0909, val acc: 0.5524  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 32760] train loss: 1.1024, train acc: 0.5474, val loss: 1.1213, val acc: 0.5363  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 32780] train loss: 1.1473, train acc: 0.5305, val loss: 1.1376, val acc: 0.5309  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 32800] train loss: 1.1281, train acc: 0.5422, val loss: 1.1309, val acc: 0.5420  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 32820] train loss: 1.1274, train acc: 0.5413, val loss: 1.1288, val acc: 0.5396  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 32840] train loss: 1.1330, train acc: 0.5356, val loss: 1.1291, val acc: 0.5400  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 32860] train loss: 1.1250, train acc: 0.5458, val loss: 1.1279, val acc: 0.5393  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 32880] train loss: 1.1285, train acc: 0.5401, val loss: 1.1262, val acc: 0.5417  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 32900] train loss: 1.1118, train acc: 0.5439, val loss: 1.1264, val acc: 0.5356  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 32920] train loss: 1.1232, train acc: 0.5416, val loss: 1.1253, val acc: 0.5417  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 32940] train loss: 1.1139, train acc: 0.5448, val loss: 1.1261, val acc: 0.5379  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 32960] train loss: 1.1234, train acc: 0.5463, val loss: 1.1245, val acc: 0.5417  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 32980] train loss: 1.1262, train acc: 0.5384, val loss: 1.1273, val acc: 0.5390  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 33000] train loss: 1.1240, train acc: 0.5440, val loss: 1.1241, val acc: 0.5410  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 33020] train loss: 1.1327, train acc: 0.5335, val loss: 1.1243, val acc: 0.5403  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 33040] train loss: 1.1171, train acc: 0.5424, val loss: 1.1242, val acc: 0.5376  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 33060] train loss: 1.1200, train acc: 0.5411, val loss: 1.1233, val acc: 0.5413  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 33080] train loss: 1.1176, train acc: 0.5403, val loss: 1.1234, val acc: 0.5393  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 33100] train loss: 1.1220, train acc: 0.5393, val loss: 1.1233, val acc: 0.5386  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 33120] train loss: 1.1179, train acc: 0.5430, val loss: 1.1259, val acc: 0.5383  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 33140] train loss: 1.1117, train acc: 0.5411, val loss: 1.1268, val acc: 0.5393  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 33160] train loss: 1.1150, train acc: 0.5438, val loss: 1.1226, val acc: 0.5430  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 33180] train loss: 1.1202, train acc: 0.5392, val loss: 1.1225, val acc: 0.5400  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 33200] train loss: 1.1216, train acc: 0.5425, val loss: 1.1223, val acc: 0.5430  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 33220] train loss: 1.1240, train acc: 0.5367, val loss: 1.1221, val acc: 0.5423  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 33240] train loss: 1.1072, train acc: 0.5467, val loss: 1.1283, val acc: 0.5406  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 33260] train loss: 1.1319, train acc: 0.5350, val loss: 1.1227, val acc: 0.5376  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 33280] train loss: 1.1013, train acc: 0.5471, val loss: 1.1056, val acc: 0.5423  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 33300] train loss: 1.1000, train acc: 0.5364, val loss: 1.0986, val acc: 0.5396  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 33320] train loss: 1.0673, train acc: 0.5623, val loss: 1.0880, val acc: 0.5454  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 33340] train loss: 1.0718, train acc: 0.5573, val loss: 1.0886, val acc: 0.5545  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 33360] train loss: 1.0993, train acc: 0.5398, val loss: 1.0853, val acc: 0.5518  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 33380] train loss: 1.0950, train acc: 0.5439, val loss: 1.0858, val acc: 0.5494  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 33400] train loss: 1.0949, train acc: 0.5456, val loss: 1.0995, val acc: 0.5346  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 33420] train loss: 1.0855, train acc: 0.5506, val loss: 1.1085, val acc: 0.5413  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 33440] train loss: 1.1013, train acc: 0.5425, val loss: 1.1008, val acc: 0.5535  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 33460] train loss: 1.0786, train acc: 0.5539, val loss: 1.0873, val acc: 0.5484  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 33480] train loss: 1.0748, train acc: 0.5592, val loss: 1.0864, val acc: 0.5501  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 33500] train loss: 1.0855, train acc: 0.5524, val loss: 1.0834, val acc: 0.5551  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 33520] train loss: 1.0804, train acc: 0.5502, val loss: 1.0835, val acc: 0.5568  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 33540] train loss: 1.0900, train acc: 0.5469, val loss: 1.0867, val acc: 0.5460  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 33560] train loss: 1.0821, train acc: 0.5531, val loss: 1.0828, val acc: 0.5562  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 33580] train loss: 1.0748, train acc: 0.5549, val loss: 1.0819, val acc: 0.5551  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 33600] train loss: 1.0927, train acc: 0.5430, val loss: 1.1053, val acc: 0.5319  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 33620] train loss: 1.1083, train acc: 0.5384, val loss: 1.1079, val acc: 0.5376  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 33640] train loss: 1.0974, train acc: 0.5417, val loss: 1.0842, val acc: 0.5535  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 33660] train loss: 1.0850, train acc: 0.5508, val loss: 1.0934, val acc: 0.5578  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 33680] train loss: 1.1203, train acc: 0.5482, val loss: 1.1309, val acc: 0.5396  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 33700] train loss: 1.1157, train acc: 0.5440, val loss: 1.1270, val acc: 0.5400  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 33720] train loss: 1.0971, train acc: 0.5424, val loss: 1.0883, val acc: 0.5551  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 33740] train loss: 1.0868, train acc: 0.5583, val loss: 1.0851, val acc: 0.5592  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 33760] train loss: 1.1115, train acc: 0.5387, val loss: 1.0833, val acc: 0.5558  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 33780] train loss: 1.0770, train acc: 0.5534, val loss: 1.0834, val acc: 0.5558  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 33800] train loss: 1.0815, train acc: 0.5527, val loss: 1.0812, val acc: 0.5578  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 33820] train loss: 1.0805, train acc: 0.5503, val loss: 1.0808, val acc: 0.5558  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 33840] train loss: 1.0783, train acc: 0.5510, val loss: 1.0809, val acc: 0.5592  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 33860] train loss: 1.0572, train acc: 0.5700, val loss: 1.0807, val acc: 0.5572  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 33880] train loss: 1.0777, train acc: 0.5586, val loss: 1.0824, val acc: 0.5575  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 33900] train loss: 1.0979, train acc: 0.5418, val loss: 1.0802, val acc: 0.5572  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 33920] train loss: 1.0773, train acc: 0.5576, val loss: 1.0808, val acc: 0.5589  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 33940] train loss: 1.0708, train acc: 0.5562, val loss: 1.0801, val acc: 0.5531  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 33960] train loss: 1.0554, train acc: 0.5635, val loss: 1.0806, val acc: 0.5558  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 33980] train loss: 1.0850, train acc: 0.5491, val loss: 1.0819, val acc: 0.5558  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 34000] train loss: 1.0828, train acc: 0.5505, val loss: 1.0801, val acc: 0.5568  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 34020] train loss: 1.0714, train acc: 0.5591, val loss: 1.0779, val acc: 0.5555  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 34040] train loss: 1.0784, train acc: 0.5523, val loss: 1.0778, val acc: 0.5538  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 34060] train loss: 1.0689, train acc: 0.5565, val loss: 1.0774, val acc: 0.5541  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 34080] train loss: 1.0852, train acc: 0.5512, val loss: 1.0791, val acc: 0.5514  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 34100] train loss: 1.0726, train acc: 0.5533, val loss: 1.0806, val acc: 0.5582  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 34120] train loss: 1.0863, train acc: 0.5489, val loss: 1.0790, val acc: 0.5541  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 34140] train loss: 1.0850, train acc: 0.5498, val loss: 1.0799, val acc: 0.5609  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 34160] train loss: 1.0695, train acc: 0.5539, val loss: 1.0775, val acc: 0.5589  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 34180] train loss: 1.0715, train acc: 0.5580, val loss: 1.0774, val acc: 0.5568  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 34200] train loss: 1.0718, train acc: 0.5541, val loss: 1.0768, val acc: 0.5582  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 34220] train loss: 1.0833, train acc: 0.5497, val loss: 1.0758, val acc: 0.5575  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 34240] train loss: 1.0839, train acc: 0.5510, val loss: 1.0756, val acc: 0.5568  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 34260] train loss: 1.1015, train acc: 0.5364, val loss: 1.1230, val acc: 0.5363  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 34280] train loss: 1.1967, train acc: 0.5058, val loss: 1.2577, val acc: 0.4715  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 34300] train loss: 1.1329, train acc: 0.5336, val loss: 1.1432, val acc: 0.5248  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 34320] train loss: 1.1336, train acc: 0.5402, val loss: 1.1365, val acc: 0.5403  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 34340] train loss: 1.1337, train acc: 0.5385, val loss: 1.1334, val acc: 0.5413  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 34360] train loss: 1.1204, train acc: 0.5461, val loss: 1.1332, val acc: 0.5403  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 34380] train loss: 1.1159, train acc: 0.5445, val loss: 1.1319, val acc: 0.5393  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 34400] train loss: 1.1312, train acc: 0.5366, val loss: 1.1303, val acc: 0.5369  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 34420] train loss: 1.1362, train acc: 0.5330, val loss: 1.1299, val acc: 0.5403  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 34440] train loss: 1.1096, train acc: 0.5495, val loss: 1.1293, val acc: 0.5383  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 34460] train loss: 1.1123, train acc: 0.5468, val loss: 1.1295, val acc: 0.5400  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 34480] train loss: 1.1246, train acc: 0.5447, val loss: 1.1301, val acc: 0.5376  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 34500] train loss: 1.1165, train acc: 0.5429, val loss: 1.1306, val acc: 0.5366  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 34520] train loss: 1.1154, train acc: 0.5446, val loss: 1.1290, val acc: 0.5406  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 34540] train loss: 1.1313, train acc: 0.5351, val loss: 1.1289, val acc: 0.5390  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 34560] train loss: 1.1272, train acc: 0.5378, val loss: 1.1288, val acc: 0.5410  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 34580] train loss: 1.1315, train acc: 0.5362, val loss: 1.1289, val acc: 0.5386  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 34600] train loss: 1.1461, train acc: 0.5336, val loss: 1.1289, val acc: 0.5400  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 34620] train loss: 1.1193, train acc: 0.5445, val loss: 1.1290, val acc: 0.5400  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 34640] train loss: 1.1468, train acc: 0.5301, val loss: 1.1288, val acc: 0.5393  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 34660] train loss: 1.1258, train acc: 0.5383, val loss: 1.1288, val acc: 0.5413  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 34680] train loss: 1.1161, train acc: 0.5474, val loss: 1.1291, val acc: 0.5352  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 34700] train loss: 1.1381, train acc: 0.5350, val loss: 1.1288, val acc: 0.5396  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 34720] train loss: 1.1343, train acc: 0.5369, val loss: 1.1286, val acc: 0.5390  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 34740] train loss: 1.1064, train acc: 0.5507, val loss: 1.1293, val acc: 0.5363  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 34760] train loss: 1.1230, train acc: 0.5424, val loss: 1.1285, val acc: 0.5400  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 34780] train loss: 1.1405, train acc: 0.5311, val loss: 1.1292, val acc: 0.5376  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 34800] train loss: 1.1314, train acc: 0.5382, val loss: 1.1285, val acc: 0.5420  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 34820] train loss: 1.1010, train acc: 0.5518, val loss: 1.1285, val acc: 0.5417  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 34840] train loss: 1.1201, train acc: 0.5406, val loss: 1.1283, val acc: 0.5396  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 34860] train loss: 1.1163, train acc: 0.5455, val loss: 1.1295, val acc: 0.5390  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 34880] train loss: 1.1404, train acc: 0.5335, val loss: 1.1296, val acc: 0.5376  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 34900] train loss: 1.1330, train acc: 0.5396, val loss: 1.1292, val acc: 0.5390  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 34920] train loss: 1.1201, train acc: 0.5427, val loss: 1.1284, val acc: 0.5410  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 34940] train loss: 1.1284, train acc: 0.5398, val loss: 1.1332, val acc: 0.5417  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 34960] train loss: 1.1093, train acc: 0.5484, val loss: 1.1303, val acc: 0.5390  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 34980] train loss: 1.1236, train acc: 0.5408, val loss: 1.1287, val acc: 0.5396  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 35000] train loss: 1.1176, train acc: 0.5392, val loss: 1.1306, val acc: 0.5396  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 35020] train loss: 1.1257, train acc: 0.5424, val loss: 1.1291, val acc: 0.5383  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 35040] train loss: 1.1373, train acc: 0.5346, val loss: 1.1295, val acc: 0.5379  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 35060] train loss: 1.1314, train acc: 0.5350, val loss: 1.1285, val acc: 0.5390  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 35080] train loss: 1.1128, train acc: 0.5433, val loss: 1.1282, val acc: 0.5406  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 35100] train loss: 1.1322, train acc: 0.5374, val loss: 1.1284, val acc: 0.5396  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 35120] train loss: 1.1179, train acc: 0.5432, val loss: 1.1287, val acc: 0.5406  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 35140] train loss: 1.1280, train acc: 0.5415, val loss: 1.1285, val acc: 0.5400  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 35160] train loss: 1.1235, train acc: 0.5436, val loss: 1.1293, val acc: 0.5386  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 35180] train loss: 1.1244, train acc: 0.5427, val loss: 1.1282, val acc: 0.5427  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 35200] train loss: 1.1190, train acc: 0.5474, val loss: 1.1281, val acc: 0.5390  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 35220] train loss: 1.1279, train acc: 0.5355, val loss: 1.1289, val acc: 0.5386  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 35240] train loss: 1.1245, train acc: 0.5442, val loss: 1.1298, val acc: 0.5383  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 35260] train loss: 1.1185, train acc: 0.5424, val loss: 1.1287, val acc: 0.5406  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 35280] train loss: 1.1163, train acc: 0.5452, val loss: 1.1281, val acc: 0.5400  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 35300] train loss: 1.1155, train acc: 0.5440, val loss: 1.1281, val acc: 0.5390  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 35320] train loss: 1.1289, train acc: 0.5343, val loss: 1.1284, val acc: 0.5383  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 35340] train loss: 1.1322, train acc: 0.5384, val loss: 1.1280, val acc: 0.5390  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 35360] train loss: 1.1265, train acc: 0.5402, val loss: 1.1293, val acc: 0.5332  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 35380] train loss: 1.1180, train acc: 0.5369, val loss: 1.1279, val acc: 0.5417  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 35400] train loss: 1.1153, train acc: 0.5505, val loss: 1.1289, val acc: 0.5359  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 35420] train loss: 1.1117, train acc: 0.5483, val loss: 1.1280, val acc: 0.5406  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 35440] train loss: 1.1253, train acc: 0.5382, val loss: 1.1280, val acc: 0.5352  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 35460] train loss: 1.1420, train acc: 0.5290, val loss: 1.1285, val acc: 0.5356  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 35480] train loss: 1.1221, train acc: 0.5404, val loss: 1.1284, val acc: 0.5403  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 35500] train loss: 1.1218, train acc: 0.5393, val loss: 1.1293, val acc: 0.5379  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 35520] train loss: 1.1222, train acc: 0.5411, val loss: 1.1279, val acc: 0.5390  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 35540] train loss: 1.1192, train acc: 0.5436, val loss: 1.1280, val acc: 0.5393  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 35560] train loss: 1.1256, train acc: 0.5434, val loss: 1.1284, val acc: 0.5396  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 35580] train loss: 1.1228, train acc: 0.5396, val loss: 1.1280, val acc: 0.5403  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 35600] train loss: 1.1287, train acc: 0.5388, val loss: 1.1281, val acc: 0.5396  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 35620] train loss: 1.1241, train acc: 0.5393, val loss: 1.1278, val acc: 0.5423  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 35640] train loss: 1.1206, train acc: 0.5452, val loss: 1.1279, val acc: 0.5417  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 35660] train loss: 1.1276, train acc: 0.5399, val loss: 1.1283, val acc: 0.5423  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 35680] train loss: 1.1305, train acc: 0.5390, val loss: 1.1280, val acc: 0.5413  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 35700] train loss: 1.1423, train acc: 0.5312, val loss: 1.1279, val acc: 0.5393  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 35720] train loss: 1.1247, train acc: 0.5403, val loss: 1.1284, val acc: 0.5386  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 35740] train loss: 1.1304, train acc: 0.5407, val loss: 1.1307, val acc: 0.5383  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 35760] train loss: 1.1156, train acc: 0.5479, val loss: 1.1278, val acc: 0.5373  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 35780] train loss: 1.1313, train acc: 0.5315, val loss: 1.1278, val acc: 0.5406  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 35800] train loss: 1.1349, train acc: 0.5359, val loss: 1.1285, val acc: 0.5386  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 35820] train loss: 1.1397, train acc: 0.5320, val loss: 1.1292, val acc: 0.5379  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 35840] train loss: 1.1223, train acc: 0.5382, val loss: 1.1278, val acc: 0.5423  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 35860] train loss: 1.1313, train acc: 0.5422, val loss: 1.1289, val acc: 0.5339  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 35880] train loss: 1.1313, train acc: 0.5359, val loss: 1.1277, val acc: 0.5423  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 35900] train loss: 1.1179, train acc: 0.5423, val loss: 1.1279, val acc: 0.5417  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 35920] train loss: 1.1182, train acc: 0.5432, val loss: 1.1290, val acc: 0.5376  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 35940] train loss: 1.1270, train acc: 0.5377, val loss: 1.1277, val acc: 0.5423  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 35960] train loss: 1.1323, train acc: 0.5371, val loss: 1.1283, val acc: 0.5390  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 35980] train loss: 1.1324, train acc: 0.5361, val loss: 1.1276, val acc: 0.5390  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 36000] train loss: 1.1146, train acc: 0.5475, val loss: 1.1280, val acc: 0.5390  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 36020] train loss: 1.1163, train acc: 0.5422, val loss: 1.1278, val acc: 0.5390  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 36040] train loss: 1.1331, train acc: 0.5330, val loss: 1.1278, val acc: 0.5379  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 36060] train loss: 1.1313, train acc: 0.5362, val loss: 1.1278, val acc: 0.5383  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 36080] train loss: 1.1227, train acc: 0.5436, val loss: 1.1295, val acc: 0.5359  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 36100] train loss: 1.1126, train acc: 0.5489, val loss: 1.1280, val acc: 0.5383  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 36120] train loss: 1.1166, train acc: 0.5460, val loss: 1.1284, val acc: 0.5379  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 36140] train loss: 1.1243, train acc: 0.5380, val loss: 1.1324, val acc: 0.5406  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 36160] train loss: 1.1276, train acc: 0.5387, val loss: 1.1290, val acc: 0.5400  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 36180] train loss: 1.1188, train acc: 0.5405, val loss: 1.1276, val acc: 0.5410  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 36200] train loss: 1.1277, train acc: 0.5334, val loss: 1.1281, val acc: 0.5386  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 36220] train loss: 1.1168, train acc: 0.5440, val loss: 1.1282, val acc: 0.5393  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 36240] train loss: 1.1156, train acc: 0.5424, val loss: 1.1278, val acc: 0.5400  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 36260] train loss: 1.1374, train acc: 0.5337, val loss: 1.1277, val acc: 0.5393  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 36280] train loss: 1.1192, train acc: 0.5416, val loss: 1.1279, val acc: 0.5413  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 36300] train loss: 1.1169, train acc: 0.5453, val loss: 1.1281, val acc: 0.5386  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 36320] train loss: 1.1223, train acc: 0.5434, val loss: 1.1289, val acc: 0.5400  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 36340] train loss: 1.1257, train acc: 0.5387, val loss: 1.1278, val acc: 0.5369  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 36360] train loss: 1.1241, train acc: 0.5419, val loss: 1.1280, val acc: 0.5386  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 36380] train loss: 1.1268, train acc: 0.5352, val loss: 1.1300, val acc: 0.5400  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 36400] train loss: 1.1193, train acc: 0.5452, val loss: 1.1279, val acc: 0.5433  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 36420] train loss: 1.1076, train acc: 0.5479, val loss: 1.1277, val acc: 0.5403  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 36440] train loss: 1.1215, train acc: 0.5461, val loss: 1.1279, val acc: 0.5376  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 36460] train loss: 1.1354, train acc: 0.5341, val loss: 1.1286, val acc: 0.5393  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 36480] train loss: 1.1273, train acc: 0.5406, val loss: 1.1286, val acc: 0.5393  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 36500] train loss: 1.1118, train acc: 0.5459, val loss: 1.1283, val acc: 0.5390  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 36520] train loss: 1.1273, train acc: 0.5369, val loss: 1.1288, val acc: 0.5356  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 36540] train loss: 1.1286, train acc: 0.5369, val loss: 1.1275, val acc: 0.5390  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 36560] train loss: 1.1163, train acc: 0.5439, val loss: 1.1279, val acc: 0.5393  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 36580] train loss: 1.1129, train acc: 0.5497, val loss: 1.1283, val acc: 0.5383  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 36600] train loss: 1.1236, train acc: 0.5390, val loss: 1.1284, val acc: 0.5373  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 36620] train loss: 1.1271, train acc: 0.5410, val loss: 1.1276, val acc: 0.5393  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 36640] train loss: 1.1353, train acc: 0.5376, val loss: 1.1277, val acc: 0.5386  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 36660] train loss: 1.1090, train acc: 0.5459, val loss: 1.1276, val acc: 0.5390  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 36680] train loss: 1.1260, train acc: 0.5401, val loss: 1.1288, val acc: 0.5336  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 36700] train loss: 1.1327, train acc: 0.5344, val loss: 1.1274, val acc: 0.5386  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 36720] train loss: 1.1388, train acc: 0.5324, val loss: 1.1273, val acc: 0.5410  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 36740] train loss: 1.1181, train acc: 0.5411, val loss: 1.1277, val acc: 0.5386  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 36760] train loss: 1.1288, train acc: 0.5376, val loss: 1.1292, val acc: 0.5383  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 36780] train loss: 1.1315, train acc: 0.5381, val loss: 1.1279, val acc: 0.5403  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 36800] train loss: 1.1150, train acc: 0.5442, val loss: 1.1280, val acc: 0.5383  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 36820] train loss: 1.1306, train acc: 0.5383, val loss: 1.1275, val acc: 0.5396  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 36840] train loss: 1.1299, train acc: 0.5368, val loss: 1.1275, val acc: 0.5386  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 36860] train loss: 1.1141, train acc: 0.5508, val loss: 1.1275, val acc: 0.5369  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 36880] train loss: 1.1054, train acc: 0.5547, val loss: 1.1277, val acc: 0.5376  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 36900] train loss: 1.1280, train acc: 0.5423, val loss: 1.1276, val acc: 0.5379  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 36920] train loss: 1.1248, train acc: 0.5414, val loss: 1.1275, val acc: 0.5379  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 36940] train loss: 1.1195, train acc: 0.5467, val loss: 1.1273, val acc: 0.5410  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 36960] train loss: 1.1078, train acc: 0.5496, val loss: 1.1277, val acc: 0.5369  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 36980] train loss: 1.1294, train acc: 0.5377, val loss: 1.1275, val acc: 0.5373  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 37000] train loss: 1.1050, train acc: 0.5489, val loss: 1.1298, val acc: 0.5325  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 37020] train loss: 1.1249, train acc: 0.5368, val loss: 1.1275, val acc: 0.5376  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 37040] train loss: 1.1215, train acc: 0.5400, val loss: 1.1271, val acc: 0.5427  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 37060] train loss: 1.1134, train acc: 0.5457, val loss: 1.1298, val acc: 0.5406  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 37080] train loss: 1.1153, train acc: 0.5458, val loss: 1.1275, val acc: 0.5386  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 37100] train loss: 1.1204, train acc: 0.5367, val loss: 1.1282, val acc: 0.5359  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 37120] train loss: 1.1210, train acc: 0.5437, val loss: 1.1300, val acc: 0.5393  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 37140] train loss: 1.1155, train acc: 0.5415, val loss: 1.1287, val acc: 0.5369  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 37160] train loss: 1.1245, train acc: 0.5412, val loss: 1.1281, val acc: 0.5369  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 37180] train loss: 1.1077, train acc: 0.5483, val loss: 1.1278, val acc: 0.5379  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 37200] train loss: 1.1375, train acc: 0.5355, val loss: 1.1281, val acc: 0.5386  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 37220] train loss: 1.1238, train acc: 0.5464, val loss: 1.1274, val acc: 0.5390  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 37240] train loss: 1.1086, train acc: 0.5491, val loss: 1.1278, val acc: 0.5373  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 37260] train loss: 1.1171, train acc: 0.5458, val loss: 1.1274, val acc: 0.5376  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 37280] train loss: 1.1092, train acc: 0.5463, val loss: 1.1276, val acc: 0.5383  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 37300] train loss: 1.1322, train acc: 0.5318, val loss: 1.1275, val acc: 0.5369  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 37320] train loss: 1.1297, train acc: 0.5360, val loss: 1.1284, val acc: 0.5366  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 37340] train loss: 1.1307, train acc: 0.5392, val loss: 1.1275, val acc: 0.5359  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 37360] train loss: 1.1226, train acc: 0.5409, val loss: 1.1272, val acc: 0.5390  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 37380] train loss: 1.1148, train acc: 0.5468, val loss: 1.1270, val acc: 0.5410  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 37400] train loss: 1.1199, train acc: 0.5431, val loss: 1.1272, val acc: 0.5413  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 37420] train loss: 1.1124, train acc: 0.5464, val loss: 1.1275, val acc: 0.5417  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 37440] train loss: 1.1203, train acc: 0.5443, val loss: 1.1298, val acc: 0.5400  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 37460] train loss: 1.1147, train acc: 0.5448, val loss: 1.1275, val acc: 0.5359  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 37480] train loss: 1.1176, train acc: 0.5428, val loss: 1.1272, val acc: 0.5383  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 37500] train loss: 1.1204, train acc: 0.5403, val loss: 1.1289, val acc: 0.5396  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 37520] train loss: 1.1261, train acc: 0.5432, val loss: 1.1275, val acc: 0.5386  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 37540] train loss: 1.1178, train acc: 0.5464, val loss: 1.1270, val acc: 0.5390  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 37560] train loss: 1.1256, train acc: 0.5437, val loss: 1.1267, val acc: 0.5420  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 37580] train loss: 1.1248, train acc: 0.5425, val loss: 1.1293, val acc: 0.5386  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 37600] train loss: 1.1179, train acc: 0.5403, val loss: 1.1285, val acc: 0.5379  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 37620] train loss: 1.1135, train acc: 0.5502, val loss: 1.1281, val acc: 0.5359  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 37640] train loss: 1.1348, train acc: 0.5337, val loss: 1.1270, val acc: 0.5430  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 37660] train loss: 1.1263, train acc: 0.5395, val loss: 1.1274, val acc: 0.5386  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 37680] train loss: 1.1372, train acc: 0.5318, val loss: 1.1276, val acc: 0.5393  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 37700] train loss: 1.1219, train acc: 0.5388, val loss: 1.1282, val acc: 0.5342  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 37720] train loss: 1.1370, train acc: 0.5348, val loss: 1.1272, val acc: 0.5410  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 37740] train loss: 1.1134, train acc: 0.5406, val loss: 1.1278, val acc: 0.5390  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 37760] train loss: 1.1234, train acc: 0.5401, val loss: 1.1276, val acc: 0.5366  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 37780] train loss: 1.1390, train acc: 0.5361, val loss: 1.1267, val acc: 0.5413  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 37800] train loss: 1.1193, train acc: 0.5395, val loss: 1.1284, val acc: 0.5390  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 37820] train loss: 1.1069, train acc: 0.5461, val loss: 1.1267, val acc: 0.5406  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 37840] train loss: 1.1082, train acc: 0.5496, val loss: 1.1269, val acc: 0.5417  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 37860] train loss: 1.1429, train acc: 0.5309, val loss: 1.1266, val acc: 0.5406  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 37880] train loss: 1.1138, train acc: 0.5463, val loss: 1.1265, val acc: 0.5383  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 37900] train loss: 1.1129, train acc: 0.5458, val loss: 1.1264, val acc: 0.5390  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 37920] train loss: 1.1209, train acc: 0.5419, val loss: 1.1271, val acc: 0.5379  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 37940] train loss: 1.1100, train acc: 0.5441, val loss: 1.1267, val acc: 0.5386  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 37960] train loss: 1.1226, train acc: 0.5414, val loss: 1.1269, val acc: 0.5359  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 37980] train loss: 1.1140, train acc: 0.5474, val loss: 1.1262, val acc: 0.5400  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 38000] train loss: 1.1291, train acc: 0.5385, val loss: 1.1280, val acc: 0.5390  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 38020] train loss: 1.1187, train acc: 0.5389, val loss: 1.1264, val acc: 0.5369  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 38040] train loss: 1.1195, train acc: 0.5419, val loss: 1.1268, val acc: 0.5390  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 38060] train loss: 1.1119, train acc: 0.5487, val loss: 1.1294, val acc: 0.5332  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 38080] train loss: 1.1110, train acc: 0.5445, val loss: 1.1261, val acc: 0.5410  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 38100] train loss: 1.1206, train acc: 0.5424, val loss: 1.1257, val acc: 0.5400  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 38120] train loss: 1.1238, train acc: 0.5383, val loss: 1.1264, val acc: 0.5406  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 38140] train loss: 1.1215, train acc: 0.5445, val loss: 1.1260, val acc: 0.5363  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 38160] train loss: 1.1226, train acc: 0.5431, val loss: 1.1255, val acc: 0.5390  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 38180] train loss: 1.1247, train acc: 0.5368, val loss: 1.1254, val acc: 0.5393  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 38200] train loss: 1.1149, train acc: 0.5467, val loss: 1.1269, val acc: 0.5376  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 38220] train loss: 1.1346, train acc: 0.5341, val loss: 1.1251, val acc: 0.5403  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 38240] train loss: 1.1081, train acc: 0.5471, val loss: 1.1262, val acc: 0.5383  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 38260] train loss: 1.1157, train acc: 0.5427, val loss: 1.1253, val acc: 0.5403  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 38280] train loss: 1.1193, train acc: 0.5415, val loss: 1.1249, val acc: 0.5400  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 38300] train loss: 1.1162, train acc: 0.5434, val loss: 1.1257, val acc: 0.5369  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 38320] train loss: 1.1285, train acc: 0.5371, val loss: 1.1266, val acc: 0.5400  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 38340] train loss: 1.1137, train acc: 0.5426, val loss: 1.1252, val acc: 0.5410  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 38360] train loss: 1.1133, train acc: 0.5427, val loss: 1.1262, val acc: 0.5393  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 38380] train loss: 1.1148, train acc: 0.5408, val loss: 1.1245, val acc: 0.5400  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 38400] train loss: 1.1080, train acc: 0.5468, val loss: 1.1243, val acc: 0.5400  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 38420] train loss: 1.1235, train acc: 0.5433, val loss: 1.1275, val acc: 0.5417  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 38440] train loss: 1.1215, train acc: 0.5412, val loss: 1.1243, val acc: 0.5359  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 38460] train loss: 1.1236, train acc: 0.5382, val loss: 1.1264, val acc: 0.5386  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 38480] train loss: 1.1252, train acc: 0.5389, val loss: 1.1249, val acc: 0.5390  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 38500] train loss: 1.1181, train acc: 0.5408, val loss: 1.1254, val acc: 0.5369  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 38520] train loss: 1.1124, train acc: 0.5463, val loss: 1.1245, val acc: 0.5369  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 38540] train loss: 1.1301, train acc: 0.5362, val loss: 1.1252, val acc: 0.5390  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 38560] train loss: 1.1090, train acc: 0.5484, val loss: 1.1243, val acc: 0.5373  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 38580] train loss: 1.0980, train acc: 0.5536, val loss: 1.1236, val acc: 0.5376  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 38600] train loss: 1.1308, train acc: 0.5388, val loss: 1.1233, val acc: 0.5413  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 38620] train loss: 1.1275, train acc: 0.5383, val loss: 1.1231, val acc: 0.5386  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 38640] train loss: 1.1170, train acc: 0.5427, val loss: 1.1269, val acc: 0.5403  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 38660] train loss: 1.1051, train acc: 0.5499, val loss: 1.1241, val acc: 0.5420  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 38680] train loss: 1.1136, train acc: 0.5459, val loss: 1.1227, val acc: 0.5379  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 38700] train loss: 1.1094, train acc: 0.5437, val loss: 1.1206, val acc: 0.5403  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 38720] train loss: 1.1229, train acc: 0.5376, val loss: 1.1229, val acc: 0.5403  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 38740] train loss: 1.1399, train acc: 0.5324, val loss: 1.1264, val acc: 0.5349  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 38760] train loss: 1.1286, train acc: 0.5394, val loss: 1.1192, val acc: 0.5403  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 38780] train loss: 1.1059, train acc: 0.5451, val loss: 1.1216, val acc: 0.5363  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 38800] train loss: 1.1344, train acc: 0.5334, val loss: 1.1186, val acc: 0.5420  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 38820] train loss: 1.1137, train acc: 0.5397, val loss: 1.1213, val acc: 0.5336  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 38840] train loss: 1.1171, train acc: 0.5404, val loss: 1.1174, val acc: 0.5393  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 38860] train loss: 1.1314, train acc: 0.5358, val loss: 1.1189, val acc: 0.5420  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 38880] train loss: 1.1012, train acc: 0.5441, val loss: 1.1205, val acc: 0.5430  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 38900] train loss: 1.1096, train acc: 0.5421, val loss: 1.1182, val acc: 0.5423  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 38920] train loss: 1.1077, train acc: 0.5454, val loss: 1.1154, val acc: 0.5417  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 38940] train loss: 1.1109, train acc: 0.5434, val loss: 1.1171, val acc: 0.5363  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 38960] train loss: 1.1109, train acc: 0.5420, val loss: 1.1149, val acc: 0.5410  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 38980] train loss: 1.0957, train acc: 0.5444, val loss: 1.1132, val acc: 0.5342  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 39000] train loss: 1.1106, train acc: 0.5394, val loss: 1.1124, val acc: 0.5413  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 39020] train loss: 1.1257, train acc: 0.5335, val loss: 1.1121, val acc: 0.5420  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 39040] train loss: 1.1263, train acc: 0.5338, val loss: 1.1109, val acc: 0.5417  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 39060] train loss: 1.0993, train acc: 0.5476, val loss: 1.1117, val acc: 0.5433  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 39080] train loss: 1.1188, train acc: 0.5404, val loss: 1.1123, val acc: 0.5423  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 39100] train loss: 1.1036, train acc: 0.5429, val loss: 1.1109, val acc: 0.5363  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 39120] train loss: 1.0994, train acc: 0.5470, val loss: 1.1077, val acc: 0.5403  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 39140] train loss: 1.1121, train acc: 0.5429, val loss: 1.1069, val acc: 0.5417  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 39160] train loss: 1.1106, train acc: 0.5439, val loss: 1.1061, val acc: 0.5410  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 39180] train loss: 1.1090, train acc: 0.5418, val loss: 1.1071, val acc: 0.5430  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 39200] train loss: 1.1083, train acc: 0.5411, val loss: 1.1045, val acc: 0.5390  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 39220] train loss: 1.1006, train acc: 0.5440, val loss: 1.1045, val acc: 0.5420  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 39240] train loss: 1.1070, train acc: 0.5416, val loss: 1.1044, val acc: 0.5444  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 39260] train loss: 1.1064, train acc: 0.5400, val loss: 1.1028, val acc: 0.5369  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 39280] train loss: 1.1139, train acc: 0.5367, val loss: 1.1035, val acc: 0.5379  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 39300] train loss: 1.0918, train acc: 0.5494, val loss: 1.1012, val acc: 0.5423  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 39320] train loss: 1.0966, train acc: 0.5476, val loss: 1.1024, val acc: 0.5447  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 39340] train loss: 1.1012, train acc: 0.5416, val loss: 1.0999, val acc: 0.5444  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 39360] train loss: 1.0949, train acc: 0.5430, val loss: 1.1000, val acc: 0.5433  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 39380] train loss: 1.0995, train acc: 0.5427, val loss: 1.0978, val acc: 0.5410  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 39400] train loss: 1.0999, train acc: 0.5426, val loss: 1.1032, val acc: 0.5447  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 39420] train loss: 1.1051, train acc: 0.5382, val loss: 1.1003, val acc: 0.5319  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 39440] train loss: 1.1061, train acc: 0.5388, val loss: 1.0961, val acc: 0.5447  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 39460] train loss: 1.0964, train acc: 0.5447, val loss: 1.0952, val acc: 0.5433  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 39480] train loss: 1.0855, train acc: 0.5508, val loss: 1.0957, val acc: 0.5420  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 39500] train loss: 1.0895, train acc: 0.5493, val loss: 1.0957, val acc: 0.5444  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 39520] train loss: 1.0995, train acc: 0.5446, val loss: 1.1016, val acc: 0.5470  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 39540] train loss: 1.1129, train acc: 0.5382, val loss: 1.0935, val acc: 0.5440  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 39560] train loss: 1.1010, train acc: 0.5432, val loss: 1.0966, val acc: 0.5325  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 39580] train loss: 1.0873, train acc: 0.5490, val loss: 1.0920, val acc: 0.5454  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 39600] train loss: 1.0767, train acc: 0.5542, val loss: 1.0940, val acc: 0.5474  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 39620] train loss: 1.1006, train acc: 0.5428, val loss: 1.0912, val acc: 0.5403  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 39640] train loss: 1.0723, train acc: 0.5675, val loss: 1.0941, val acc: 0.5400  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 39660] train loss: 1.0909, train acc: 0.5476, val loss: 1.0889, val acc: 0.5454  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 39680] train loss: 1.0809, train acc: 0.5581, val loss: 1.0880, val acc: 0.5474  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 39700] train loss: 1.0804, train acc: 0.5601, val loss: 1.0869, val acc: 0.5497  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 39720] train loss: 1.0822, train acc: 0.5496, val loss: 1.0869, val acc: 0.5521  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 39740] train loss: 1.0962, train acc: 0.5424, val loss: 1.0859, val acc: 0.5524  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 39760] train loss: 1.0743, train acc: 0.5545, val loss: 1.0869, val acc: 0.5551  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 39780] train loss: 1.0971, train acc: 0.5432, val loss: 1.0850, val acc: 0.5511  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 39800] train loss: 1.0906, train acc: 0.5461, val loss: 1.0841, val acc: 0.5511  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 39820] train loss: 1.0993, train acc: 0.5470, val loss: 1.0856, val acc: 0.5582  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 39840] train loss: 1.0795, train acc: 0.5506, val loss: 1.0838, val acc: 0.5548  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 39860] train loss: 1.0895, train acc: 0.5467, val loss: 1.0831, val acc: 0.5551  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 39880] train loss: 1.0891, train acc: 0.5519, val loss: 1.0824, val acc: 0.5541  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 39900] train loss: 1.0935, train acc: 0.5472, val loss: 1.0850, val acc: 0.5477  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 39920] train loss: 1.0827, train acc: 0.5518, val loss: 1.0811, val acc: 0.5521  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 39940] train loss: 1.0861, train acc: 0.5483, val loss: 1.0803, val acc: 0.5524  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 39960] train loss: 1.0839, train acc: 0.5487, val loss: 1.0800, val acc: 0.5524  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 39980] train loss: 1.0715, train acc: 0.5575, val loss: 1.0798, val acc: 0.5518  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n",
      "[Epoch: 40000] train loss: 1.0776, train acc: 0.5513, val loss: 1.0785, val acc: 0.5545  (best train acc: 0.5855, best val acc: 0.5801, best train loss: 1.0158  @ epoch 27792 )\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pmaur\\miniconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\pmaur\\miniconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\pmaur\\miniconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\pmaur\\miniconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\pmaur\\miniconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\pmaur\\miniconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       154\n",
      "           1       0.66      0.65      0.66       703\n",
      "           2       0.42      0.30      0.35       702\n",
      "           3       0.48      0.61      0.53       703\n",
      "           4       0.73      0.89      0.80       702\n",
      "\n",
      "    accuracy                           0.58      2964\n",
      "   macro avg       0.46      0.49      0.47      2964\n",
      "weighted avg       0.54      0.58      0.56      2964\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaYAAAGQCAYAAADlUsSzAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAA/ZElEQVR4nO3dd3wU5fbH8c9Jo0gNkIQqIFgALxb02gUrVoqoWFER1GvvDQv23ssPxALqFREbehUboogNBKWoKIpAgBB6b0nO74/dxIAhBNjd2d1837zmlZ3Z2ZkzWbJnzzPPPGPujoiISLxICToAERGR0pSYREQkrigxiYhIXFFiEhGRuKLEJCIicSUt6ABERGT7nGjHR6x79Qh/3yK1rW2liklEROKKKiYRkQSXkmQ1hhKTiEiCMwu89S2ikivNiohIwlPFJCKS4NSUJyIicSVFTXkiIiLRo4pJRCTBWZLVGEpMIiIJTk15IiIiUaSKSUQkwSVbU15yHY2ISCWUYhaxqSLMrI6ZDTezX83sFzPb38wyzewTM/s9/LNuqfVvNLPpZjbNzI7e4vFsx+9CREQqp8eBke6+K9Ae+AW4AfjM3VsDn4XnMbM2QE+gLdAZeMbMUsvbuBKTiEiCS4ngvy0xs1rAIcDzAO6+3t2XAl2AweHVBgNdw4+7AEPdfZ27zwCmA/uWfzwiIpLQzCxiUwW0BBYAL5rZRDMbZGY7ANnuPg8g/DMrvH5jYHap1+eGl22WEpOIiJQws75mNr7U1HeTVdKAvYBn3X1PYBXhZrvNbbKMZeXeP0q98kREElwkx8pz94HAwHJWyQVy3f278PxwQolpvpk1dPd5ZtYQyC+1ftNSr28CzC0vBlVMIiIJLpa98tw9D5htZruEFx0O/AyMAHqFl/UC3g0/HgH0NLMqZtYCaA18X94+VDGJiMjWuhR41cwygD+BcwkVOsPMrDcwCzgZwN2nmtkwQsmrALjY3QvL27gqJklqZtbTzL4zs1Vmlh9+/B8Ln+U1s33N7AMzW2pmi83sezM7N/xcRzNzM3t6k21+ZWbnBHA4ImUyUiI2VYS7/+juHdz9X+7e1d2XuPsidz/c3VuHfy4utf7d7r6Tu+/i7h9uaftKTJK0zOxqQtdbPAjkANnAhcCBQIaZ7Q+MAr4AWgH1gIuAY0ptZhVwtpk1j13kIlsnxVIiNsWD+IhCJMLMrDZwB/Afdx/u7is8ZKK7n+Hu6wglrMHufr+7Lww//4O7n1JqU0uBl4DbYn4QIpWUEpMkq/2BKvx9AnYjZlY9vM7wCmzrbuCkUid7ReKKRfBfPFBikmRVH1jo7gXFC8zs6/C5pDXAPoT+/8/b0obCvZD+j1AFJhJ31JQnkhgWAfXNrKTnqbsf4O51ws/tABQBDSu4vfuBo82sfaQDFZGNKTFJsvoGWEdonK6yrA6vc1JFNubui4DHgDsjEZxIJEWuT158NOXpOiZJSu6+1Mz6ExrJ2ICRhJLRvwhVSwDXAR+b2UzgBXdfFK6IbnT3nmVs9hFC12zEx1+vSJjuxySSINz9AeAqQgkoH5gPDACuB75296+Bw8LTn2a2mNBQLB9sZnvLgQeAzOhHL1J5mXu5Y+mJiEicu7T6RRH7IH9y9bOBtwioKU9EJMFFchDXeKDEJCKS4Cp4H6WEkVxpVkREEp4qJhGRBKemvNhRrwwRSWYRa3+ryH2UEkk8JybWFhYFHUIgqqamkLd8bdBhxFxOraosXr0+6DACkVk9g/wVle89z6pZlZXrC7a8YhKqkRHXH7+B0m9GRCTBJdsFtkpMIiIJLtma8pIrzYqISMJTxSQikuDUlCciInElXu6jFCnJdTQiIpLwVDGJiCS4eLmPUqQoMYmIJDhTU56IiEj0qGISEUlwasoTEZG4ol55IiIiUaSKSUQkwZma8kREJK6kJFdiUlOeiIjEFVVMIiKJLslGF1diEhFJcKamPBERkehRxSQikujUlCciInFFTXkiIiLRo4pJRCTRJVnFpMQkIpLgLMnOMakpT0RE4ooqJhGRRKemvOQ2dswY7r/3HooKi+jWowe9+/QJOqSoOfXEY6hWvTqpKamkpqUycMhrvDjwWd5/503q1MkEoM/Fl7LfgQcHHGlkrVu3jot6n8OG9espLCyk0xFH0ueiixn0f8/w7ltvUrduXQAuvOQyDjj4kICjjbzCwkL6nHUa9bOyeOCxp5j+2zQeuvcu1qxeTU6jRtx6573sUKNG0GFGVP9b+jHmyy/IzMxk2NvvAjDgmad5+83hJe/3xZddwUGHJOj7nWRNeUpMpRQWFnLPXXcyYNDzZGdnc/qpp9CxUyd2atUq6NCi5rH/G0SdOnU3WnbyaWfR86xeAUUUfRkZGTw18HmqV69OwYYNXHBeL/Y/8CAAep55FmecfU6wAUbZG6+9yo4tWrJq1UoA7r+rP/+5/Cr23LsD/3v3bV57+SXOv+iSgKOMrBO6dOWU007ntptv3Gj56WedzdnnnBtQVLI5OsdUypTJk2jarBlNmjYlPSODzsccy+hRo4IOSyLMzKhevToABQUFFBQUJN3J483Jnz+fb8aO4fiu3UqWzZr5F3vstTcAHf69P6NHfRZUeFGzV4cO1K5dO+gwoifFIjfFASWmUvLn55OTk1Myn5WTzfz8+QFGFGUG11xyIX3O6smIt4aXLH77jaGce1oP7rvjVlYsXx5ggNFTWFjI2af24NjDD2Xf/faj7e7/AmD40Nc485Tu3HX7LSxfvizgKCPviYcf4D+XXbnRHU9b7tSKr74YDcDnn35M/vy8gKKLvWGv/ZdTu3ej/y39WL4sgd9vS4ncFAeiEoWZVTWzK8zsKTO7wMwSosnQ3f+xLNluwFXa04MGM+iV13ng8ad5Z/jr/DThB7qcdAr/fft9nn91GPXqN+Dpxx4KOsyoSE1NZcjrw3n3o0/5ecoU/pj+O91PPoXh733AkKHDqV+/AU88klzHPnbMF9TNzGSX3dpstPyGW/vz9htD6X1mT9asXk16enpAEcZWj1NO5d0PRvLa8Dep36ABjz70YNAhSVi00uNgoAMwGTgGeLgiLzKzvmY23szGDxw4MEqhbV52TjZ5eX9/W8zPm09WVlbM44iV+g1Cx1Y3sx4HdzyMX6ZOIbNePVJTU0lJSeH4rt35deqUgKOMrpo1a7FXh3349uuxZNarX3LsXbqfxC9TkuvYJ//0I2O/HM3JJxzD7Tdfz4Rx47jjlhvZsXkLHnl6AM+/MpTDj+5M48ZNgg41JurV//v97nZSD6ZOmRx0SNvMUixiUzyIVmJq4+5nuvsAoAdQoW5d7j7Q3Tu4e4e+fftGKbTNa9tud2bNnElubi4b1q9n5IcfcGinTjGPIxbWrFnN6lWrSh6P+/YbWuzUikULF5SsM2b0KFrslHwdP5YsXsyKFaEmyrVr1zLuu2/ZsXkLFi74+9hHj/qMlkl27BdecjlvffAJb7z3IbfffT977bMPt955L0sWLwKgqKiIIc8/R5eTTg440thYUOr9/vyzT9mpVesAo9lOSXaOKVpNbBuKH7h7QaKcWE5LS+PGm/txUZ/zKSoqomu37rRqncD/WcuxZNFi+l13JQCFBQUc0flY/n3Agdx1601M/20aZkZOw0Zcc9MtAUcaeYsWLuCOW/tRVFSIFzmHHXkUBx1yKP373chv037FzGjYsDHX97s16FBj4tOPRvLWG0MBOLTT4Rx7YtdgA4qCm667hvHjxrF06VKOOfwwLrj4Yn4YN45pv4be70aNG3HTrbcHHaaEWVnnVbZ7o2aFwKriWaAasDr82N29VgU242sLiyIeWyKomppC3vK1QYcRczm1qrJ49fqgwwhEZvUM8ldUvvc8q2ZVVq4vCDqMQNTISIvYN/a7d34oYh/kN/92TeCVRFQqJndPjcZ2RUSkDHHSBBcp8dE3UEREJCwhunGLiMjmJcp5/IpSxSQikuhi3CvPzP4ys8lm9qOZjQ8vyzSzT8zs9/DPuqXWv9HMppvZNDM7eouHs82/CBERqcw6ufse7t4hPH8D8Jm7twY+C89jZm2AnkBboDPwjJmV2w9BiUlEJNGZRW7adl0IDa5A+GfXUsuHuvs6d58BTAf2LW9DSkwiIokugk15pUfgCU9ljXbgwMdm9kOp57PdfR5A+GfxsDmNgdmlXpsbXrZZ6vwgIiIl3H0gsKUx4Q5097lmlgV8Yma/lrNuWWVYudddKTGJiCS6GF/H5O5zwz/zzextQk1z882sobvPM7OGQH549VygaamXNwHmlrd9NeWJiCQ4M4vYVIF97WBmNYsfA0cBU4ARQPEdRnsB74YfjwB6mlkVM2sBtAa+L28fqphERGRrZANvh5NYGvBfdx9pZuOAYWbWG5gFnAzg7lPNbBjwM1AAXOzuheXtQIlJRCTRxbApz93/BNqXsXwRcPhmXnM3cHdF96HEJCKS6DTyg4iISPSoYhIRSXRJNrq4EpOISIJLtkFclZhERBJdklVMOsckIiJxRRWTiEiiS7KKSYlJRCTRJdk5JjXliYhIXFHFJCKS6NSUJyIi8STZuourKU9EROKKKiYRkUSnpjwREYkrasoTERGJnriumKqmVt68mVOratAhBCKzekbQIQQmq2blfM9rZMT1x1BiUFNe7KwtLAo6hEBUTU3hgc6Dgw4j5q4b2Ytvf8sPOoxA7LdzFtPmLQs6jJjbpWFt1hSUezPTpFUtLTVyG0uuvKSmPBERiS9xXTGJiEgFJFnnByUmEZEEZ0l2jklNeSIiEldUMYmIJLrkKpiUmEREEl6SnWNSU56IiMQVVUwiIokuyTo/KDGJiCS65MpLasoTEZH4oopJRCTRJVnnByUmEZFEl2RtX0l2OCIikuhUMYmIJDo15YmISDyxJEtMasoTEZG4oopJRCTRJVfBpMQkIpLwkmzkBzXliYhIXFHFJCKS6JKs84MSk4hIokuuvKSmPBERiS+qmEREEl2SdX5QYhIRSXTJlZfUlCciIvFFFdMmxo4Zw/333kNRYRHdevSgd58+QYcUcZZinP3EcaxctJo3bxvFgWe251+dd2b1srUAjHlpAn+Om0NKqtH5igPIblWPlFRjymd/8N3rUwKOftsMevxefhz3NbVq1+Wep4cA8OYrg5jw3RhSLIWatevS54qbqFuvPgUbNvDi0w/y1/RpmBln9L2c3XbfM+AjiIwRw4fy8fvv4DhHHdeVLiefxozpv/HMI/exds0asnIacnW/O6i+Q42gQ42av2bM4LqrryqZn5Oby0WXXMqZZ58dYFTbSb3yKs7M6rv7wmjuI5IKCwu55647GTDoebKzszn91FPo2KkTO7VqFXRoEbV3191YNHsZVaqnlywb//bPjHtz6kbr7XJwc1LTU3nxohGkVUml98Cu/DJ6Bsvnr4p1yNvtoMOP4YjjujPw0btLlh3b/TROOvN8AD4eMZx3h77EORdfw+iP3wPg7qcGs3zpEh66/Rpuf+Q5UlISu4Fh5p9/8PH77/Dw/71EWloat193OfvsfyBPPng35110Oe322ItPPhjBW0Nf4czeFwYdbtQ0b9GCYW+9DYT+5o/q1JHDjjg82KC2kyXZOaao/KWZ2QlmtgCYbGa5ZnZANPYTaVMmT6Jps2Y0adqU9IwMOh9zLKNHjQo6rIiqUb86O+3ThEkjf6/Q+ulV07AUIy0jjcINhaxftSHKEUbHru32YIeatTZaVq36DiWP161bU9JOP3fWX7RpvzcAterUZYcdajBj+q8xizVaZs+awS5t2lGlalVS09Jou8defDNmNHNmz6Jt+1BFuEeHf/PNl58HG2gMfffttzRp2oxGjRoHHYqUEq2vgHcDB7t7Q+Ak4N4o7Sei8ufnk5OTUzKflZPN/Pz5AUYUeYdfsA+jnx+Pu2+0fK8Td+WcZ0+g85UHUKVGBgDTxvzFhrUFXPzfU7jw5ZMY9+ZU1q5cH0TYUTN8yECuPPckvhn9Cd3P6A1A0xatmPjdVxQWFrAgby5//fEbixfkBxzp9tuxxU5MnTSR5cuWsm7tWn74diwL8+ezY4uWfDf2SwDGjv6UhUn2f748H334Accce2zQYWw/i+AUB6KVmArc/VcAd/8OqFmRF5lZXzMbb2bjBw4cGKXQNm/TD2sAi5d3KgJ22rcJq5euZf70xRstn/j+NAae+xYv/ec9Vi1eQ6c+HQBouEt9vMh55oxhDOz1Fvuc1JbaOcl17qHH2X159MU32b/jkXz6/lsAHHLksdSt14Dbr+zDq4OepNWu7UhNTQ040u3XdMcWdD/tbG695lJuu+4yWuzUmtTUVC677hY+eGc4V/Y9mzWrV5OWXjlOPW9Yv54vPv+cI48+OuhQtp9Z5KY4EK3/gVlmdtXm5t39kbJe5O4DgeKM5GsLi6IUXtmyc7LJy8srmc/Pm09WVlZMY4imxm2zaLVfU1ru24TU9FSqVE/nuOsO4n8PfFWyzk8jf+Ok/qH29t06teTPH+ZQVOisXraW3Kn55LSux7K8lUEdQtTsf+iRPNL/Orqf0ZvU1DTO6HNZyXN3XnsR2Y2aBBhd5Bx1XBeOOq4LAEOee4b6DbJosmNz7njoSQDmzJ7J+G/HBhlizHz11Rh2bdOGevXrBx2KbCJaFdNzhKqk4qn0fNx+5W7bbndmzZxJbm4uG9avZ+SHH3Bop05BhxUxX744gWfPGs6AXm/y3n1fMOunefzvga/YIbNayTo7H7AjC/9aCsDy/FXs2L4hAOlV0mi0awMW5y4PIvSoyJs7u+TxxO++omGTZgCsW7uWdWvXADBl4jhSUlNp3KxFIDFG2tIloWp5wfw8vvnycw45/KiSZUVFRQx7+QU6n9g9yBBjZuQHH9A5GZrxIHSBbaSmOBCVisnd+2/uOTO7Ihr7jIS0tDRuvLkfF/U5n6KiIrp2606r1q2DDivqOvbem6yWmTjO8vmr+OiJbwCY+N6vHHP1gZw3IPQNe8on01kwY0mQoW6zZx68nV8nT2Tl8mVccU53up1+HpPGf8u8ObOwFKN+gxx6XXwNAMuXLeGh267GLIW69epzwVX9Ao4+cu679XpWLF9OaloqF15xLTVq1mLE8KF88M4bAOx/cCeOOOaEgKOMvjVr1vDt11/T77bbgw4lMuIjn0SMlXVeJao7NJvl7s0qsGrMm/LiRdXUFB7oPDjoMGLuupG9+Pa3xO9ksC322zmLafOWBR1GzO3SsDZrCgqDDiMQ1dJSI5ZOHjrvzYh9kF/zwkmBp7kgznIGftAiIkklTjotREoQVwzGtkQTEUl2KRGcKsjMUs1sopm9H57PNLNPzOz38M+6pda90cymm9k0M9tiN8hoXWC7wsyWlzGtABpFY58iIhJTlwO/lJq/AfjM3VsDn4XnMbM2QE+gLdAZeMbMyr3+IiqJyd1runutMqaa7l45LpIQEYmVGF/HZGZNgOOAQaUWdwGKT44PBrqWWj7U3de5+wxgOrBvedtP7MG/REQEM4vkVDLQQXjqW8YuHwOuA0r3UMt293kA4Z/FF4E2BmaXWi83vGyzVL2IiEiJTQY6+AczOx7Id/cfzKxjBTZZVhlWbl8DJSYRkUQX27avA4ETzexYoCpQy8xeAeabWUN3n2dmDYHiaz9ygaalXt8EmFveDtSUJyKS6GJ4jsndb3T3Ju7enFCnhlHufiYwAugVXq0X8G748Qigp5lVMbMWQGvg+/L2oYpJRCTRxcd1TPcBw8ysNzALOBnA3aea2TDgZ6AAuNjdy72qWolJRES2ibuPBkaHHy8CyrzjorvfTeh2SBWixCQikuiS7KSMEpOISKKLj6a8iEmyPCsiIolOFZOISKJLsopJiUlEJNElWdtXkh2OiIgkOlVMIiKJTk15IiISV5IsMakpT0RE4ooqJhGRRJdkJYYSk4hIolNTnoiISPSoYhIRSXRJVjEpMYmIJLoka/tKssMREZFEp4pJRCTRqSkvdqqmVt6C7rqRvba8UhLab+esoEMIzC4NawcdQiCqpaUGHULiS668FN+JaW1hUdAhBKJqagqf/jQn6DBi7oj2jRk65s+gwwhEz4NbMuDdKUGHEXMXdGnH3KVrgg4jEI3qVAs6hLgV14lJREQqICW5SiYlJhGRRJdk55gq70kcERGJS5utmMxsBeDFs+GfHn7s7l4ryrGJiEhFJFfBtPnE5O41YxmIiIhsoyQ7x1ShpjwzO8jMzg0/rm9mLaIbloiIVFZb7PxgZrcBHYBdgBeBDOAV4MDohiYiIhWSZJ0fKtIrrxuwJzABwN3nmpma+URE4kVy5aUKNeWtd3cn3BHCzHaIbkgiIlKZVaRiGmZmA4A6ZtYHOA94LrphiYhIhSVZ54ctJiZ3f8jMjgSWAzsDt7r7J1GPTEREKqYSnmMCmAxUI9ScNzl64YiISGW3xXNMZnY+8D3QHegBfGtm50U7MBERqSCL4BQHKlIxXQvs6e6LAMysHvA18EI0AxMRkQpKsnNMFemVlwusKDW/ApgdnXBERKSyK2+svKvCD+cA35nZu4TOMXUh1LQnIiLxoBJ1fii+iPaP8FTs3eiFIyIiWy3J7hNR3iCu/WMZiIiICFRsrLwGwHVAW6Bq8XJ3PyyKcYmISEUlWVNeRQrAV4FfgRZAf+AvYFwUYxIRka1hFrkpDlQkMdVz9+eBDe7+hbufB+wX5bhERKSSqsh1TBvCP+eZ2XHAXKBJ9EISEZGtUlk6P5Ryl5nVBq4GngRqAVdGNSoREam4OGmCi5SKDOL6fvjhMqBTdMMREZHKrrwLbJ8kfA+msrj7ZeW89uzyduruQyoUnYiIbFklqpjGb8d29yljmQEnAI2BuE1MY8eM4f5776GosIhuPXrQu0+foEOKqJefeYApE76lZu069Hv47+EOR3/4Fl+MfIeU1FTa7bUf3c68gO/HfMqnI14vWWfurD+5/v4BNG3eKojQt8uyxQt46/mHWLlsCZZi7H3IMex/RFemjh/D5yNeYeG82fS5+TEaN98ZgNw/p/Hey08A4O50OvEMdtvrwCAPYZusWLqQD4c+weqVSzEzdv/3kex10PEATBz7AT+O/ZCU1BRa7Lo3hxwX+j75/ai3mDzuM1IshU5dzqP5LnsGeQgRs3LFch68+w5m/DkdM+O6frcz5vPP+PqrL0lPT6dR4yZcf0t/atSsFXSoW6+ynGNy98HbulF3v7T4sZkZcAZwPfAtcPe2bjfaCgsLueeuOxkw6Hmys7M5/dRT6NipEzu1SrwP4s3Zr+PRHNq5K0Oevq9k2W9TJjJp/Nfc9NAg0tMzWLFsCQD7HnwE+x58BABzZv3JgAduScikBJCSksrRp/Sh0Y6tWLd2NQPuvIyd2uxJVqMd6fmfW3hvyBMbrZ/VeEf69nuC1NRUVixdzLP9/8PO7fcjNTU1oCPYNpaSyqHHn0N2k5asX7uGV564lh1bt2fViqX8MfV7zrrqEdLS0lm9chkAi+bP5tefvqLX1Y+xavlihg/sz7nXPUlKSmIdd1mefOQB9t3/APrf9xAbNmxg3do1rN53P/r85zJS09IY8NRjvDr4BS645IqgQ630opZnzSwtfMuMn4EjgB7ufqq7T4rWPrfXlMmTaNqsGU2aNiU9I4POxxzL6FGjgg4rolq3ac8ONTb+RvjlxyM4qstppKdnAFCzdt1/vG78V6PocGDiXlNds04mjXYMJdUqVatTv2FTVixZRINGzaif889OphlVqpYkoYIN64mb+wFspRq16pLdpCUAGVWrUS+rCSuXLWbStx+xT6dupKWlA1C9Rm0A/pg6jl3bH0RaWjq1M7OpUz+HvNnTA4s/UlatXMmkiRM49sRuAKSnp1OjZi322e8AUtNC38/btPsXC/LnBxnmtkuy65gqeqPArWJmFwOXA58Bnd19ZjT2E2n58/PJyckpmc/KyWbypLjNoxGTPy+X6b9OZsTQ50lPz6D7WReyY6tdN1pnwjefc8G1dwUUYWQtWTifvFl/0LjlLuWul/vnr7zz0qMsW5RP997XJFy1tKlli/PJnzuDnGat+fJ/Q5gz4xfGjnyN1LR0Dj2+FzlNW7Fi+SIaNtu55DU1atdj5bLFAUYdGfPm5lKnbl3uv/NW/vj9N3betQ2XXHUd1apVK1nnw/feodMRRwcY5XaIk4QSKdGqmIq7lR8EvGdmk8LTZDOL209693/29bAE/aa8NYqKClm9cgXX3v003c66gOcfvWOj38WM338hI6MqjZq1CDDKyFi3dg2vP3MXnU+9gKrVdih33SYtd+WSOwbQ9+bHGfPBMDZsWB+jKCNv/bo1vPfyg3Q84VyqVK1OUVEha9es4rRL7uWQ487m/VceDr3nZXV3SoI/gcLCQn6b9isndj+F515+napVq/La4L/Psb7y4nOkpqZyROdjA4xSikWlVx6ha56+Apbw9wW6W2RmfYG+AAMGDODs3udX9KURkZ2TTV5eXsl8ft58srKyYhpDEOpkNmCPfx+MmdG81W5YirFyxTJq1qoDwA9jR7F3AjfjFSssKOD1Z+/iX/t1os3eFe/I0KBRM9KrVCV/zl8lnSMSSWFhAe+9/CC77XkwrXcPDdpSo3Y9Wrf7N2ZGw2atMTPWrFpOzdr1WLl0UclrVy5bRI1amUGFHjENsrJpkJVFm3a7A3DoYUfy3yGhxDTyfyP45qsxPPz0ACxRK48k6/xQ3uGMB34oZypPY+BxQvdtGgxcALQDVpTXrOfuA929g7t36Nu3b4UPIlLattudWTNnkpuby4b16xn54Qcc2in5L91qv8+B/DZlIgDz586moKCAGjVD5xyKioqY+O0XdDgwsX8P7s67gx+jQcOmHHBU9y2uv2RBHoWFhQAsXTSfRXm51KmXHe0wI87d+fiNZ8jMasLeh5xYsrxV232ZNX0yAEsWzKWwsIBqO9SiZZsO/PrTVxQUbGDZ4vksXTiPnKaJ2eGltMx69cnKymHWzL8AmDD+O5q3aMn334xl6JCXuPuhx6hatVr5G4ljZhaxKR5Eq1feNQBmlgF0AA4AzgOeM7Ol7t5mW7cdTWlpadx4cz8u6nM+RUVFdO3WnVatWwcdVkS98Nid/P7zT6xcsYybLzyF4045h/0PO4ZXnnmQu64+j7S0NM6++PqS/6DTf5lEnXoNqJ/dKODIt8+s6VP56ZvPyG7cnGf7XwzA4d16UViwgQ9ee5ZVK5bx6uO3kdOsJWdfeTezpk9lzIfDSE1Nw8w47syL2SGcrBPJ3L9+5ZcJX1A/pxkvP3o1AAd2Pp12+xzGR288w+CHryA1NY3Op16KmVE/pxm7/OsABj90OSkpqRzWtU9S9MgDuOya67n71psoKNhAw0aNuf6WO7jw3DPYsH4911x6IRDqAHHVDf0CjlSsrPMqG60Quu3F9UAbtvK2F+GhjPYHDgz/rANMdvdzKxCbry0sqsBqyadqagqf/jQn6DBi7oj2jRk65s+gwwhEz4NbMuDdKUGHEXMXdGnH3KVrgg4jEI3qVItYefLIwO/K/yDfClf1/XfgZVNFb3vxC1tx2wszG2hmY4HXCSWkr4GTw810FUlKIiJSQbHsLW5mVc3sezP7ycymmln/8PJMM/vEzH4P/6xb6jU3mtl0M5tmZlvs+hit2140A6oAecAcIBdYWoF9iYjIVorxOaZ1wGHu3h7YA+hsZvsBNwCfuXtrQpcK3RCOrQ3Qk9DNZjsDz5hZue3DFUlMG932wsz2ZAu3vXD3zoSGJXoovOhqYJyZfVycXUVEJPF4yMrwbHp4cqALoc5uhH92DT/uAgx193XuPgOYDuxb3j6idtsLD528mmJmSwmNTL4MOD4c0G0V2K+IiFREBLuLl75sJ2yguw/cZJ1UQr2zWwFPu/t3Zpbt7vMA3H2emRVfa9OY0HB0xXLDyzYrKre9MLPLCPXEO5BQxTUW+AZ4AZhckW2IiEjFRLKbdzgJDdzCOoXAHmZWB3jbzNqVF15Zmyhv+1tMTGb2YlkbCZ9r2pzmwHDgyuIMKiIiycXdl5rZaELnjuabWcNwtdQQyA+vlgs0LfWyJoTuhL5ZFSkA3wf+F54+I9SUt7K8F7j7Ve4+XElJRCQGYtgtz8wahCslzKwaoUG6fwVGAL3Cq/UC3g0/HgH0NLMqZtYCaE1o8IXNqkhT3pubBPUa8OkWoxcRkZiI8YANDYHB4fNMKcAwd3/fzL4BhplZb2AWcDKAu081s2GE7jRRAFwcbgrcrG0ZXbw1oe7gIiJSyYRvXfSPu0e6+yLg8M285m624l58FTnHtIKNzzHlERoJQkRE4kGcjHEXKRVpyqsZi0BERGTbWEpyJaYtdn4ws88qskxERCQSyrsfU1WgOlA/POZRcUquBST2UNMiIskkuQqmcpvyLgCuIJSEfuDvQ18OPB3dsEREpKLi5T5KkVLe/ZgeBx43s0vd/ckYxiQiIpVYRS6wLSq+mArAzOqa2X+iF5KIiGyNWN72IhYqkpj6uPvS4hl3XwL0iVpEIiKydZIsM1UkMaVYqQbM8NW+GdELSUREKrOKjPzwEaFhJv6P0IW2FwIjoxqViIhUWKXp/FDK9YTuzXERoZ55HwPPRTMoERHZChG8H1M82OLhuHuRu/+fu/dw95OAqYRuGCgiIhJxFRrE1cz2AE4DTgVmAG9FMSYREdkKlaYpz8x2BnoSSkiLgNcBc/cK3cVWRERipLIkJkI3fhoDnODu0wHM7MqYRCUiIpVWeeeYTiJ0i4vPzew5MzucpBuRSUQk8SXZZUybT0zu/ra7nwrsCowGrgSyzexZMzsqRvGJiMgWmFnEpnhQkV55q9z9VXc/HmgC/AjcEO3ARESkcjJ33/JawYjbwEREIiBi5cmAd6dE7PPygi7tAi+bKtRdPChrC4uCDiEQVVNTWLm+MOgwYq5GRiqTZy8JOoxA7N60Lg+eOjToMGLu2td78tUv84MOIxAH7ZYdsW3FSxNcpCTZ9cIiIpLo4rpiEhGRCkiyikmJSUQkwSVZXlJTnoiIxBdVTCIiiS7JSiYlJhGRBGcpyZWY1JQnIiJxRRWTiEiCS7KWPCUmEZGEl2SZSU15IiISV1QxiYgkuGQbkkiJSUQk0SVXXlJTnoiIxBdVTCIiCS7ZrmNSYhIRSXDJlZbUlCciInFGFZOISIJTrzwREYkrSZaX1JQnIiLxRRWTiEiCS7aKSYlJRCTBWZL1y1NTnoiIxBVVTCIiCU5NeSIiEleSLTGpKU9EROKKKqZNjB0zhvvvvYeiwiK69ehB7z59gg4pavrfcjNjvvyCzMxMhr09YqPnhrz0Ao8//BCffjmWunXrBhRh5Dz94F388N1Yatepy6OD/gvA1198xrAhg5gz6y/ufeoFWu2yGwA//fAdrw56hoINBaSlp3FW30vZfc8OQYa/3cyMs+49ipWLV/PWA2M49Iz27LR3Y4oKilg6fyUfPvsd61ZvICU1haP6diCnZSbuzqiXJjL75/ygw98mLzx5H5PGf03N2nW584nBGz038p3XeOOlZ3lsyAhq1qpDQUEBg5++n5l//EZRUSH7d+zMcT3ODCjyrZdsF9hGpWIysxVmtjw8rSg1v9rMCqKxz0goLCzknrvu5JkBA3n7vfcY+cH/+GP69KDDipoTunTjyWcH/mN5Xt48vvvmG3IaNgwgqujodPRx9Lv30Y2WNWvekmtvv4/ddt9jo+U1a9Xhhjsf4pFBr3LJdbfy5H39YxhpdOx97M4smrO8ZH7m5Pm8eM2HvHTdSBbPW8G/u7YBoP3hLQF46dqRvHHXaDqetUfCDsR24GGdufLWB/+xfPGC+fz843gyG2SXLBs/9nM2bNjAHU8M5paHB/HFRyNYOH9eLMPdLhbBKR5EJTG5e013rxWeagKNgLuBPODxaOwzEqZMnkTTZs1o0rQp6RkZdD7mWEaPGhV0WFGzV4cO1K5d+x/LH3ngfi6/6uqk+hbW5l97UqNmrY2WNdmxBY2b7viPdVu23oXM+g0AaNq8JevXr2PD+vUxiTMaamRWo+WejZg86o+SZX9NysOLHIB5vy+kZr1qANRrUptZk+cDsHr5Otat2kBOy8zYBx0Bu7Tdgx1q1PrH8qEvPMXJvS7aqIu1mbF+7VoKCwvYsG4daelpVK2+QyzD3S5mFrEpHkT1HJOZ1TGz24GfgJrAPu5+dTT3uT3y5+eTk5NTMp+Vk838/PkBRhR7X3w+igZZWey8y65BhxIXvh3zOS1a7Ux6RkbQoWyzw3rtxRev/oh72c+369SSGRND1UH+zKW02qcxlmLUbrAD2S3rUqte9RhGG10/fv8VderVp2mLVhst3/uAjmRUrcpV53bj2j4nc3SXnv/4IiOxE62mvPpmdi8wASgA9nT3fu6+aAuv62tm481s/MCB/2xiijYv4y832S5cK8+aNWt4/rkBXHjxpUGHEhdm//Unrzz3NBdceUPQoWyzlns1YvXytcyfsaTM5/fr1gYvdH7+aiYAkz//kxWL1nD2vUfRqdeezP1tIUVFm8loCWbdurW8/8bLdD2t9z+em/H7L6SkpPDwC29z/4DX+ejd11mQNzeAKLeNWeSmeBCtzg8zgQXAi8BqoHfpEtHdHynrRe4+ECjOSL62sChK4ZUtOyebvLy8kvn8vPlkZWXFNIYg5c6ezdw5czitRzcA8ufP54xTTmLIa69TP9y0VVksWpDPA7ddz6XX30pOoyZBh7PNGu9Sn1Z7N6blHo1Iy0gho1o6x12yH/976lvaHtKcnfZqxOt3fl6yvhc5nw+ZWDJ/+h1HsGTeiiBCj7gF8+awMH8et19xHgBLFi3gjqvOp9+DA/juy09ot+e/SUtLo1adurTabXf+mv4rDXIaBRx1xcRJPomYaCWmB4Hir1k1N3kubr9+tW23O7NmziQ3N5fsrCxGfvgB9z7wz5Onyar1zjvz6Rdflcwff/QRvDz0jaTolbc1Vq1cwT03X8UZvS9i13btgw5nu4x5bRJjXpsEQNM2Wexz/C7876lvad4+h3277MbQ20dRsL6wZP20jFTMYMO6QnbcPZuioqKNOk0ksibNd+KxwX/3Pr2uzync8vBAataqQ2aDbH6dPIH9Ox7F+nVr+XPaVI484eQAo63copKY3P32zT1nZldEY5+RkJaWxo039+OiPudTVFRE127dadW6ddBhRc1N113D+HHfs3TpUo45vBMXXHwJXbufFHRYUfHo3bcw9acJrFi2lL49T+DUXn2oUbMWzz/1MMuXLeXem6+i+U47c8v9j/PhO2+QNzeX4a++yPBXXwTglvsep3bdxOwEUJYjztub1LRUTunXEYC5vy/ik0HjqV67KiffdCjuzsrFa/jgqW+DDXQ7DHi4P9OmTGTl8mVc0/skuvQ8l4OPPL7MdQ87phsvPHkft17WC3fnoMOPpWnznWIc8baLZacFM2sKDAFygCJgoLs/bmaZwOtAc+Av4BR3XxJ+zY1Ab6AQuMzdPyp3H2WdV4kmM5vl7s0qsGrMm/LiRdXUFFaW+hZbWdTISGXy7LLPhSS73ZvW5cFThwYdRsxd+3pPvvqlcnUwKnbQbtkRyyZvfvNXxD7IT9q/eblxmVlDoKG7TzCzmsAPQFfgHGCxu99nZjcAdd39ejNrA7wG7Euoh/anwM7uvtkPuSBGfki25lARkUrD3ee5+4Tw4xXAL0BjoAtQfCXzYELJivDyoe6+zt1nANMJJanNCiIxxe05JhGRRBTJ65hK944OT33L2W9zYE/gOyDb3edBKHkBxT3HGgOzS70sN7xss6JyjsnMVlB2AjKgWjT2KSJSWUWyGWqT3tGb36dZDeBN4Ap3X17Oea6ynii3QIlW54dNe+KJiEiSMLN0QknpVXd/K7x4vpk1dPd54fNQxYMs5gJNS728CVDuRWIaXVxEJMHF8gJbC5VGzwO/bHJN6gigV/hxL+DdUst7mlkVM2sBtAa+L28fGl1cRCTBxXiMuwOBs4DJZvZjeNlNwH3AMDPrDcwCTgZw96lmNgz4mdBIQBeX1yMPlJhERGQruPtXbP601uGbec3dhAbyrhAlJhGRBJds1+AoMYmIJLh4GXw1UtT5QURE4ooqJhGRBBcvN/iLFCUmEZEEl2R5SU15IiISX1QxiYgkuGS707YSk4hIglNTnoiISBSpYhIRSXDJVjEpMYmIJLiUJDvHpKY8ERGJK6qYREQSnJryREQkriRbYlJTnoiIxBVVTCIiCU5j5YmISFxJrrSkpjwREYkzqphERBJcsjXlmbsHHcPmxG1gIiIRELFsMnrKvIh9XnZs1zDwLBfXFdPawqKgQwhE1dSUSnnsVVNTWLWhMOgwArFDeipzlqwOOoyYa1y3Oifa8UGHEYgR/n7QIcStuE5MIiKyZUnWkqfEJCKS6JLtfkzqlSciInFFFZOISIJTU56IiMSVZOsurqY8ERGJK6qYREQSXJIVTEpMIiKJTk15IiIiUaSKSUQkwSVXvaTEJCKS8JKsJU9NeSIiEl9UMYmIJLhk6/ygxCQikuCSLC+pKU9EROKLKiYRkQSXbKOLKzGJiCQ4NeWJiIhEkSomEZEEp155IiISV5IsLykxiYgkumRLTDrHJCIicUUVk4hIglN3cRERiStqyhMREYkiVUybGDtmDPffew9FhUV069GD3n36BB1STNx68818+cVoMjMzeWvEe0GHE1W397uZMV9+QWZmJm+8MwKAZcuWcsPVVzN37hwaNWrM/Q8/Qq3atQOONPJWrljBQ/f0Z8aff2AY1/a7jba7t+etYa/xzvDXSU1NZb8DDuaCS68IOtSI2KH2Dlwy6DJ2bNcMd3jivMfZv/v+7HvCvhSsL2DeH3k8ce5jrFq2iqwds3j6l2eZM20OANO+ncazFz0d8BFUjLqLV4CZnV3e8+4+JBr73V6FhYXcc9edDBj0PNnZ2Zx+6il07NSJnVq1Cjq0qOvSrSunnXE6N99wQ9ChRN0JXbtx6ulncOtNfx/ri4MGse9++3Hu+X14cdBzvPj8IC6/6uoAo4yOpx59gH32O4Db732IDRs2sG7tWib+MI6vvxzNoFeGkZGRwZLFi4MOM2L6PN6XCSN/4P6T7yUtPY0q1avw4yfVGHLjYIoKi+h13zn0uPFkBt/wEgB5f+RxxZ6XBRv0NkiyvBS1prx9ypj2Be4EXojSPrfblMmTaNqsGU2aNiU9I4POxxzL6FGjgg4rJvbusA+1atcJOoyY2LtDB2pvUg198fkoju/SFYDju3Rl9KjPAogsulatWsmkiRM49sRuAKSnp1OjZk1GvPUGp519LhkZGQDUzcwMMsyIqVazGm0Pacsnz38MQMGGAlYtW8WPn0ykqLAICFVF9ZrUDzJMKUNUKiZ3v7T4sYVqzDOA64Fvgbujsc9IyJ+fT05OTsl8Vk42kydNCjAiiZVFixbRoEEDABo0aMDiJKoais2bM4fadevywJ238cf039h5l924+KrryJ01k8k/TeT5/3uajCoZXHjpVezapm3Q4W63nJY5LFuwnMtfvIIW7Vsw/YfpPHf5QNatXleyzhHnHclXr39ZMp/dIpvHJjzO6uWreaXfK/z81dQgQt9qydYrL2qdH8wszczOB34GjgB6uPup7h63n/Tu/o9lyfaGS+VVWFjA79N+5cTuJzNwyFCqVqvGa0NeoLCwkBXLl/P080O44JIruePm68r8W0g0qWmp7LTXTnz47AdcsdflrF21jh43nFzy/Mk3nUJhQSGjXx0NwOJ5i+nd7Fyu2Otynr9qEFf/9xqq1awWUPRbxyxyUzyISmIys4sJJaS9gc7ufo67T6vA6/qa2XgzGz9w4MBohFau7Jxs8vLySubz8+aTlZUV8zgk9urVq8eCBQsAWLBgAZlJ0pxVWoOsbBo0yGK3drsDcMhhR/D7tF9pkJXNwR0Px8zYrW07LCWFZUuXBBzt9luYu5CFuQv57fvfAPh6+Fha7rUTAIedfRj7HL8vD5/xUMn6BesLWLF4BQB/TPiDvD/yaLxz49gHLlGrmJ4EagEHAe+Z2aTwNNnMNlsxuftAd+/g7h369u0bpdA2r2273Zk1cya5ublsWL+ekR9+wKGdOsU8Dom9Qzp24v133wHg/Xff4dBOhwUbUBRk1qtPVnYOs2b+BcCEcd+zY4uWHHhIRyb+8D0As2fNpGDDBmrXqRtgpJGxdP5SFs5eWJJc2h/entk/z2Kvo/ei+/U9uOvEO1i/5u9mvVr1a5GSEvpIzG6RTaPWjcj7M6/MbcebFLOITVtiZi+YWb6ZTSm1LNPMPjGz38M/65Z67kYzm25m08zs6Iocj0WjZDezHct73t1nVmAzvjZ8gjKWxnzxBQ/cdy9FRUV07dadPhdeGPMYqqamEOtjv/6aqxn//fcsXbqUzHr1uOiSS+h+Uo+YxlA1NYVVGwqjvp8br72GH8b9fawX/ucSOh5+ONdffSV58+aR07AhDzzyKLVj2Blkh/RU5ixZHfX9TP9tGg/d05+CDQU0bNyY6/r1p2q1ajx41+1M/30aaWnpXHjZlezVYd+oxwLQuG51TrTjo7b9Fu1bcMmgy0jPSCPvzzweP/cxHhn3KGlV0lmxKFQdFXcL37/7AZxxxxkUFhRRVFjIf2/7L+Pe/z5qsY3w9yPWcPbr3GUR+yDftVHtcuMys0OAlcAQd28XXvYAsNjd7zOzG4C67n69mbUBXiPU+a0R8Cmws7uX+4celcS02Z2ZpQI93f3VCqweSGKKB0EkpngQq8QUj2KVmOJNtBNTPEvUxARgZs2B90slpmlAR3efZ2YNgdHuvouZ3Qjg7veG1/sIuN3dvylv+9E6x1QrXL49ZWZHWcilwJ/AKdHYp4hIZRXJzg+lz/WHp4qcV8l293kA4Z/FJ+cbA7NLrZcbXlauaI388DKwBPgGOB+4FsgAurj7j1Hap4hIpRTJ3sPuPhCIVO+zsgLbYnUXrcTU0t13BzCzQcBCoJm7r4jS/kREJDjzzaxhqaa8/PDyXKBpqfWaAHO3tLFo9crbUPwgfJJrhpKSiEh0xMF1TCOAXuHHvYB3Sy3vaWZVzKwF0BrYYo+SaFVM7c1sefixAdXC8wa4u9eK0n5FRCqdWA7iamavAR2B+maWC9wG3AcMM7PewCzgZAB3n2pmwwhd11oAXLylHnkQvSGJUqOxXRERCZa7n7aZpw7fzPp3s5VD0em2FyIiCS5ehhKKFCUmEZEEl2z3Y9IdbEVEJK6oYhIRSXDJVS8pMYmIJDw15YmIiESRKiYRkQSXZAWTEpOISKJLsrykpjwREYkvqphERBJdkrXlKTGJiCS45EpLasoTEZE4o4pJRCTBJVlLnhKTiEiiS7K8pKY8ERGJL6qYREQSXZK15SkxiYgkuORKS2rKExGROKOKSUQkwSVZS54Sk4hI4kuuzKSmPBERiSvm7kHHEHfMrK+7Dww6jiBU1mOvrMcNlffYk+m485avjdgHeU6tqoGXX6qYytY36AACVFmPvbIeN1TeY0+a47YITvFAiUlEROKKOj+IiCQ49cqrHJKi3XkbVdZjr6zHDZX32JPouJMrM6nzg4hIgstfsS5iH+RZNasEnuVUMYmIJDg15YmISFxJsrykXnmlmVmhmf1oZlPM7A0zqx50TNFkZivLWHa7mc0p9Xs4MYjYIs3MHjWzK0rNf2Rmg0rNP2xmV5mZm9mlpZY/ZWbnxDba6Cjn/V5tZlnlrZfINvm7fs/M6oSXN0/m9zuRKTFtbI277+Hu7YD1wIVBBxSQR919D+Bk4AUzS4b/J18DBwCEj6c+0LbU8wcAY4F84HIzy4h5hMFZCFwddBBRVPrvejFwcannkuP9TrILmZLhAydaxgCtgg4iSO7+C1BA6EM80Y0lnJgIJaQpwAozq2tmVYDdgCXAAuAzoFcgUQbjBeBUM8sMOpAY+AZoXGo+Kd5vi+C/eKDEVAYzSwOOASYHHUuQzOzfQBGhP96E5u5zgQIza0YoQX0DfAfsD3QAJhGqkgHuA642s9QgYg3ASkLJ6fKgA4mm8Pt5ODBik6cq2/sd99T5YWPVzOzH8OMxwPMBxhKkK83sTGAFcKonzzUFxVXTAcAjhL45HwAsI9TUB4C7zzCz74HTgwgyIE8AP5rZw0EHEgXFf9fNgR+AT0o/mQzvt3rlJbc14XMrld2j7v5Q0EFEQfF5pt0JNeXNJnRuZTmhiqG0e4DhwJexDDAo7r7UzP4L/CfoWKJgjbvvYWa1gfcJnWN6YpN1Evr9TrK8pKY8qVTGAscDi9290N0XA3UINed9U3pFd/8V+Dm8fmXxCHABSfqF1d2XAZcB15hZ+ibPJfb7bRa5KQ4oMVVu1c0st9R0VdABRdlkQh05vt1k2TJ3X1jG+ncDTWIRWIyU+36HfwdvA1WCCS/63H0i8BPQs4ynk+39TlgakkhEJMEtXbMhYh/kdaqlB142JWXJLiJSmcRJC1zEqClPRETiiiomEZEEl2QFkxKTiEjCS7K2PDXliYhIXFFikkBEciR3M3vJzHqEHw8yszblrNvRzA7Y3PPlvO4vM/vHmIGbW77JOls1Wnd4xO9rtjZGqbySbAxXJSYJTLkjuW/ruGXufr67/1zOKh35ezBXkaSQZNfXKjFJXBgDtApXM5+Hh8aZbGapZvagmY0zs0lmdgGAhTxlZj+b2f+A0vcSGm1mHcKPO5vZBDP7ycw+M7PmhBLgleFq7WAza2Bmb4b3Mc7MDgy/tp6ZfWxmE81sABX4Mmlm75jZD2Y21cz6bvLcw+FYPjOzBuFlO5nZyPBrxpjZrhH5bYokOHV+kECVGsl9ZHjRvkC78MCafQmNyrBP+NYUY83sY2BPYBdCY95lExpK5oVNttsAeA44JLytTHdfbGb/B6wsHgswnAQfdfevwiOPf0ToFhi3AV+5+x1mdhywUaLZjPPC+6gGjDOzN919EbADMMHdrzazW8PbvgQYCFzo7r+HR3J/BjhsG36NUunFSakTIUpMEpSyRnI/APje3WeElx8F/Kv4/BFQG2gNHAK85u6FwFwzG1XG9vcDvizeVnhcvLIcAbSxv9swaplZzfA+uodf+z8zW1KBY7rMzLqFHzcNx7qI0K1DXg8vfwV4y8xqhI/3jVL7TtqhgCS64qUJLlKUmCQo/xjJPfwBvar0IuBSd/9ok/WOBbY0BItVYB0INWfv7+5ryoilwsO8mFlHQkluf3dfbWajgaqbWd3D+12q0exF/knnmCSefQRcVDwStJntbGY7ELo1Qc/wOaiGQKcyXvsNcKiZtQi/tvjurCuAmqXW+5hQsxrh9fYIP/wSOCO87Big7hZirQ0sCSelXQlVbMVSgOKq73RCTYTLgRlmdnJ4H2Zm7bewD5EyqVeeSOwMInT+aIKZTQEGEKry3wZ+JzQy+LPAF5u+0N0XEDov9JaZ/cTfTWnvAd2KOz8Qug1Ch3Dnip/5u3dgf+AQM5tAqElx1hZiHQmkmdkk4E42HsF8FdDWzH4gdA7pjvDyM4De4fimAl0q8DsR+Ydk65Wn0cVFRBLcmoLCiH2QV0tLDTw9qWISEUl4sW3MC1+KMc3MppvZDRE9FFQxiYgkvLWFRRH7IK+amlJudgpf/P4bcCSQC4wDTtvChe1bRRWTiIhsjX2B6e7+p7uvB4YS4fOj6i4uIpLgtlTlbI3whe2lLygf6O4DS803BmaXms8F/h2p/YMSk4iIlBJOQgPLWaWsJBjRc0JqyhMRka2RS2hkk2JNgLmR3IESk4iIbI1xQGsza2FmGUBPYEQkd6CmPBERqTB3LzCzSwiNzJIKvODuUyO5D3UXFxGRuKKmPBERiStKTCIiEleUmEREJK4oMYmISFxRYhIRkbiixCQiInFFiUlEROLK/wO+24X4RWoAKwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 504x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaMAAAGQCAYAAAADew/3AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAABVvklEQVR4nO3dd3xT1fvA8c/TFijQMtqmLRSQLVMcyFJkOAAFCoLI171A3DIU/elXZTpQUXFAQdyKinwZMpUCZYmIsgVBECjQTaEsKcn5/ZFQmtKRQtI04Xn7ui9zc8899xzS5Ml57sm9YoxBKaWU8qYAbzdAKaWU0mCklFLK6zQYKaWU8joNRkoppbxOg5FSSimvC/J2A5RSSl2YntLdbdOiZ5sfxV11FYeOjJRSSnmdjoyUUsrHBfjBuEKDkVJK+TgRr2TW3Mr3w6lSSimfpyMjpZTycZqmU0op5XUBmqZTSimlLpyOjJRSyseJH4wrNBgppZSP0zSdUkop5QY6MlJKKR+naTqllFJep2k6pZRSyg10ZKSUUj5Of/SqlFLK6/TadEoppZQb6MhIKaV8nKbplFJKeZ3OplNKKaXcQIOR8msi0l9E1ojIMRFJcTx+VBxnfEWklYjME5FMEckQkV9F5H7Hto4iYkTkgzx1rhCR+7zQHaXyJQS4bfEWDUbKb4nIUOBdYBwQDUQBg4BrgLIi0haIB5YB9YFw4BGgW65qjgH3iEjtkmu5UsUTIAFuW7zWB68dWSkPEpHKwEjgUWPMdGNMlrH7wxhzpzHmX+xB6jNjzOvGmDTH9nXGmH65qsoEPgVeLvFOKHUR0WCk/FVboBwwK7+NIlLBUWa6C3WNAfqIyKXua55S7iNu/M9bNBgpfxUBpBljTp95QkRWOc4NnQCuxv73f7CoiowxScBE7CMtpUodTdMpVXqlAxEikvPzBWNMO2NMFce2ioANqOZifa8DXUSkhbsbqpTSYKT812rgXyC2gO3HHWX6uFKZMSYdeAcY5Y7GKeVO7ptL5700nf7oVfklY0ymiIwAPnRM416APQBdhn1UBPAssEhE9gBTjTHpjpHP88aY/vlU+zawC7z4jlUqH/5wPyPf74FSBTDGvAEMwR50UoBkYBIwHFhljFkFdHYsu0QkA4gD5hVQ3xHgDSDM861X6uIixhhvt0EppdQFeKLCI277IJ9w/COvjPw1TaeUUj5OL5SqlFLK6/R+RkoppZQb6MhIKaV8nKbpPEtnViil/Jnbcmv+cD+j0hyMOGm1ebsJXhEcGMDkuX96uxklbsAtjVmyqcir8/ilTs2r8eXSv73djBJ3V8d6JB467u1meEWNqhW83YRSxffHdkopdZEr6fsZiUhXEdkuIjtF5Ll8tlcWkTkiskFEtpy5R1hhSvXISCmlVNFKMk0nIoHAB8CNQCKwVkRmG2O25ir2GLDVGNNDRCzAdhH5yhhzqqB6dWSklFKqOFoBO40xuxzBZRrnXgPSAKGOS3GFABnAaQqhIyOllPJx7rw2nYgMBAbmeirOGBOXaz0G2JdrPRFonaea94HZwAEgFLjdGFPoJAANRkop5ePceR8iR+CJK6RIfjnBvLOfuwDrsV/3sR7wk4gsd1zfMV+aplNKKVUciUDNXOs1sI+AcrsfmGHsdgK7gUaFVarBSCmlfFwJ389oLdBAROqISFmgP/aUXG57gesBRCQKuBT77VcKpGk6pZTycVKCtws3xpwWkceBhUAg9nuBbRGRQY7tE7HfhPJTEdmEPa033BiTVli9GoyUUkoVizFmHnnu++UIQmceHwBuKk6dGoyUUsrHefN24e6iwUgppXycO2fTeYvv90AppZTP05GRUkr5ONE0nVJKKa8L8P1gpGk6pZRSXqcjI6WU8nV6cz2llFLeJpqmU0oppS6cjoyUUsrXaZpOKaWU12maTimllLpwOjJSSilf5wcjIw1GSinl48QPzhlpmk4ppZTX6chIKaV8nabpfMPK5ct5/dWx2Kw2evfty4MDBjhtN8bw+tixrEhIILh8MKPGjqVxk6aF7ns4M5Nnhw7hwP79VI+JYdzb46lUuXKJ960wu//8nfiZUzA2G83b3Ejr6/s4bd+5eQ0r5n+NiBAQEEinXg9So24TAOJGDaBsufJIQAABAYHcPeQtAOZ8Po6MlP0A/HviGOXKV+TeYe+UaL9cseWPNXz3yfvYbFauuf4Wuva+02n7moSfWDTzGwDKBZfnjoGDqVG7PhlpKXw6YSxHMjMQCeDaG7tz/S19AZj89giSD+wF4Pixo1SoGMKLb35csh0rws7Nv7Hwu0kYm40rru3CNV37OW3fvn41S2d/gUgAAQEB3HT7w9Sqb/9bn/3ZeHZs+pWKoVUY9PJHOfsk7fubeV+9z+nsbAICAuh2x2PE1Lm0RPvlil9Xr+SD8eOw2Wzc3LMX/7nnAaftxhg+ePsN1qxeSblywTz73xE0bNQYgB++/Zp5s2ZgjOGW2Fvp09/+9zLqheHs2/sPAEezsggJDSXui29LtF8u8YM0nd8HI6vVytjRo5g05WOioqK44/Z+dOzUiXr16+eUWZGQwN49e5izYAGbNm5g9IiRfPXtt4XuO3XKZFq1acuDAwbw8eTJfDxlMoOHDvNiT53ZbFZ+njGJ2waNILRyOF+Of4Z6TVsREV0zp0ytBpdxb9NWiAipB/5hzufjeOC5D3K293t0NBVCKjnV2+OeZ3IeL5k1lXLBFT3fmWKyWa18M+VdnnrpTaqGWXj1uUFc1vIaqtesnVMmIrIaQ0a+S8WQUDb/voYvJ77Fc699RGBgIH3vfZRadRty8sRxxj47kMaXtaR6zdoMGPJyzv7TP/uQ8hVKV99tNisLvvmQO58eQ6WqEUx59WkaXtYGS/VaOWXqNLqchi3aICIkJ+7mh7hXeXRkHAAt2t7A1Z16MOuTt5zqXfzDVK7rfgf1m13Njk1rWTxjKvcMfb1E+1YUq9XKe2++xhvvfYQlMopH77+Ttu07ULtOvZwyv65eQeK+vXz+/Sz+3LKJd98YywdTv2D33zuZN2sGH0z9gjJBZXju6cdo3e5aatS6hP+OOdvPj959i4ohId7o3kXB788Zbd60kZq1alGjZk3KlC1L1243szQ+3qnMkvh4esTGIiJc1uJysrKOkJqaUui+S+Lj6dkrFoCevWJZsnhxifetMEl7d1A1ohpVwqMJDCpDoyuu5e/Na5zKlC1XPufEZ/apk1CMy9AbY/hrw0oaX9nenc12i392biMyOgZLVHWCypTh6ms6s3HtSqcy9Ro1o2JIKAB1GjbhUEYqAJWrhlOrbkMAgstXIDrmEjIz0pz2NcawbtUSWl57fQn0xnUHdv9F1cjqVLVUIzCoDE1bXsf2DaudypQNzvWa/3vS6Rv1JQ2bU75C6LkVi/DvieOAfTQcUjnMc504T9u2biamRk2qx9SgTJkydLqxC6sSljqVWZmwjJtu7o6I0KTZZRw9mkV6Wip7/9lN46bNCQ4uT2BQEJddeRUrli1x2tcYw7LFP9H5xq4l2KtiCBD3LV7i9yOjlOQUoqOjc9Yjo6PYtHGjc5mUZKJylYmKiiYlOaXQfTPS07FYIgGwWCLJyMjwZDeKLetwBqFVInLWQ6qEc3DPjnPK7dj4C8vnfcHxrMPcOuDFsxtEmD7pFUTgsrZdaNG2i9N+ibu2UiGkClUt1T3Wh/N1KCOVqhGWnPUq4RZ279haYPmVi+fS7IpW5zyflnKQff/soE6Dxk7P7/xzI6GVqxJVrYb7Gu0GRzLTqVT17GteqWoE+3dvP6fctj9WEf+/TzmWlcl/Hh9RZL039RvI1+/+l59/+BhjDPc9+6Zb2+0OaakpWCKjctYtkVH8uWVzPmWincqkpaZQu249Pp74PocPZ1KuXDnWrFrBpY2aOO27af3vVA0Lo0atSzzbkfPlB3d69UgwEpFgYBBQH9gEfGyMOe2JYxXFGHPOc+fciCq/MiKu7Vta5dunc4s1uKwNDS5rw76/t7Bi/tf0e2QkAHc88RohlcM4lpXJ9ImvEBZZg5r1mubst+2P5TQqhaMiAM7teoFTX7dv/oNV8fMYNnqC0/MnTxwn7s2X6Xff4+ek49auWMzVpWxUZOfa32ujK9rR6Ip27PlrE0tnf8Fdg8cWWuu6ZfO4qd8AGl95LVt+S+DHz98tcp8Sl99rfk6Z/N/nl9SpS/+77+PZJx6hfIXy1GvQkMAg54/G+EUL6FRaR0V+wlPh9DOgJfZA1A14q/DidiIyUER+E5Hf4uLi3NKQqOgokpKSctZTkpKJjIx0KhMZFU1yrjLJyUlYIi2F7hsWHk5qagoAqakphIWVrtRFaJVwsjLPppeOZqYTUqngNtas15TM9CSOHz0CkJOKqRhahfrNW5O09+yoyma1smPjahpdfq2HWn9hqoZbOJSWmrOemZ5KlVwjhjMS//mbLz4axyPDxxASenbyifX0aeLefJlW7W/gijbXOe1jtZ7mjzXLaXlNJ8914DxVqhLBkUNnX/Mjh9IIqVLwa35Jw+YcSj3I8aOHC6134+qfaXTFNQA0uao9+/85d7TlbRGRkaSmJOesp6YkE26x5CkTRWpKknMZxwj65p69mfT5N7wzcSqhlSoTU+PseTbr6dMsXxpPpxudswOliQSI2xZv8VQwamKMucsYMwnoC7j0FdoYE2eMaWmMaTlw4EC3NKRps+bs3bOHxMREsk+dYsH8eXTo5PxB0rFzJ+bMmoUxho0b1hMSGorFElnovh07dWb2zFkAzJ45i06dO7ulve4SXbMBh1IPkpmejPV0Ntv+WEG9Zs6pqEOpB3NGf8mJf2M7fZryFUM59e9JTp08AcCpf0+y56/1RESffXPu+WsDYZE1nNKApckl9S8l5WAiackHOZ2dzdqV8Vx2dTunMhmpyUx687/c/8T/EVX97KQOYwyff/gG0TVqcUOPfnmrZtvGdUTH1KJqeOQ527yteu2GZKQc4FBaEtbT2Wz5LYGGLdo4lclIOZDzmh/cuxOr9TTlK1bKr7ocIVXC2fPXJgD+2baBsMgYz3TgAjRq3JT9+/Zy8MB+srOzWfLTQtq17+hUpl37Diya9yPGGLZu3kjFkJCcYHTIkWZPTjrIiqXxdL7p7Cho3do11Kpd2ykNWOroOaMCZZ95YIw57c1fBwcFBfH8Cy/yyICHsNls9Op9K/UbNOC7adMA6Ne/P+2v68CKhAS6d+1CcHAwI8eMLXRfgAcGPMQzg4cw84fpRFerzpvjx3utj/kJCAzk+lsH8EPcCGw2K81b3UBEdC3Wr1oAwOXtuvLXxtVs/W0JAYGBBJUpR/d7hiEiHD+ayayprwH2GVqNr7yOOo2vzKl72/pSnKIDAgODuP2hp3hv9DPYbDbade5G9Zp1SFho//JwXZdY5k7/jGNZR/hmiv11CwgI5P/eiOPvbZtYk7CImFp1GT3sQQBi7xhA8yvtH+prV8Zz9TWl64vHGQGBgXTt/whfv/sixmajxTU3EVn9EtYtmwvAVR1u4c/fV7Lxl8UEBgYRVKYstw54LieFOWPK6+zZvpHjR4/wzvC76dDjLq64tgvd736Shd9OwmazEhRUhu53PeHNbuYrMCiIJ4YNZ/hTj2Kz2ejWPZbadesxZ8b3APS49TZat7uWNatWcHffngQHB/PMi6/k7P/K88M4cjiToKAgnhz2HKGVzgboJT8tLL0TF/yI5Hde5IIrFbECx86sAuWB447HxhhT+FcxO3PSanN723xBcGAAk+f+6e1mlLgBtzRmyaaD3m6GV3RqXo0vl/7t7WaUuLs61iPx0HFvN8MralSt4LZv6WMavum2D/IX/hrmldGDR0ZGxphAT9SrlFIqH35wBQbfnw+olFLK5/n974yUUsrf+cNVuzUYKaWUr9M0nVJKKXXhdGSklFK+TtN0SimlvE7TdEoppdSF05GRUkr5Oh0ZKaWU8jYRcdvi4vG6ish2EdkpIs/ls/0ZEVnvWDaLiFVECr2atAYjpZRSLhORQOAD7HdkaAL8R0ScbgBljBlnjLncGHM58DywzBhT6E3fNE2nlFK+rmTTdK2AncaYXQAiMg2IBQq6g+V/gG+KqlRHRkop5etE3Lbkvq+cY8l7P58YYF+u9UTHc/k0SyoAXYEfiuqCjoyUUkrlMMbEAYXd3TS/YVhBVw3vAawsKkUHGoyUUsr3lWyaLhGomWu9BnCggLL9cSFFBxqMlFLK55XwhVLXAg1EpA6wH3vAuSOfNlUGOgB3uVKpBiOllPJ1JTgycty9+3FgIRAITDXGbBGRQY7tEx1FewOLjDHHCqjKiQYjpZRSxWKMmQfMy/PcxDzrnwKfulqnBiOllPJ1fnAFBg1GSinl6/zgqt36OyOllFJepyMjpZTydZqmU0op5W0lPLXbIzRNp5RSyut0ZKSUUr5O03RKKaW8TtN0Siml1IUTYwq62KrXldqGKaWUG7htOPP6DZ+47fNy+M/3e2WYVarTdCetNm83wSuCAwN4o+tn3m5GiXt2wb388leKt5vhFW0aRrL94GFvN6PEXVqtMidOW73dDK8oHxTovsp8P0unaTqllFLeV6pHRkoppVzgBxMYNBgppZSPEz+Y2q1pOqWUUl6nIyOllPJ1vj8w0mCklFI+zw/OGWmaTimllNfpyEgppXydH0xg0GCklFK+zvdjkabplFJKeZ+OjJRSytf5wQQGDUZKKeXr/CDH5QddUEop5et0ZKSUUr5O03RKKaW8TfwgGGmaTimllNfpyEgppXyd7w+MNBgppZTP84MrMGiaTimllNfpyEgppXydH0xg0GCklFK+zvdjkabplFJKeZ8GI6WU8nUB4r7FBSLSVUS2i8hOEXmugDIdRWS9iGwRkWVF1alpOqWU8nUlmKYTkUDgA+BGIBFYKyKzjTFbc5WpAnwIdDXG7BWRyKLq1ZGRUkqp4mgF7DTG7DLGnAKmAbF5ytwBzDDG7AUwxqQUVelFMTJauXw5r786FpvVRu++fXlwwACn7cYYXh87lhUJCQSXD2bU2LE0btK00H0PZ2by7NAhHNi/n+oxMYx7ezyVKlcu8b4Vps5V1bn+kVZIgLBxwQ7WfLf5nDI1L4ui88OtCAwK4MThk3zz7EIAHv6sD6eOZ2OzGYzVxudPzgXg0vaXcM1dlxNeszJfPDWXpB3pJdonV21ct4avJr+LzWajw43d6X7bXU7bD+zbw5R3X2XP33/R5+4B3Hzrf3K2LZr9PUsXzsEYQ8cuPegS2w+APbt28NmHb5J96hQBgYHc88gQ6jVsUqL9Ksq6NauZ8v5bWK02brollr533uu0PXHPP7z7+kj+3rGdux98hN79nf9drFYrQx6+l/AICy+9Nh6AFUt/5ptPJ5O45x/e/OgTGjQqXX0+Y+Xy5bzx2qvYrFZ69+nLA/m8z9949cz7vDwjx4ylcRN7X15+8QUSli0jLCyMH2bNztnncGYmzw4bevZ9/tbbpe59Drh1Np2IDAQG5noqzhgTl2s9BtiXaz0RaJ2nmoZAGRFZCoQC7xpjPi/suB4dGYlIhCfrd4XVamXs6FF8OCmO/82Zw4J5c/l7506nMisSEti7Zw9zFizgpREjGD1iZJH7Tp0ymVZt2jJnwUJatWnLx1Mml3jfCiMBwg2PteH7F3/m44GzaNyxDuG1nN9E5SqW4cbH2jDjlXimPjyLWWOc07rThi/ks8fm5AQigNR/Mpk5agn7NieXSD/Oh81q5fOJbzP0lTd59YMv+CXhZ/bv3e1UJiS0EncNfIpuvfs7PZ+4ZxdLF87h5bfiGD3hE9avXUXSAfv77ttPPiK2//2Meu8Tbr3zQb775KMS65MrrFYrk959g5dff5cPPvuWhPiF7P1nl1OZkEqVGPjkMHrffme+dcz5YRo1L6nt9Nwlderx/Mg3aHrZFZ5q+gWzWq28OmY0H0ycxIzZc1gwb9657/Pl9vf57PkL+O8rIxgzckTOtp69evPhpLi81TJ1yhRat27DnPkLaN26DVOnTPF4X86HBIjbFmNMnDGmZa4l7z9MfpHP5FkPAq4CbgG6AP8VkYaF9cEjwUhEeohIKrBJRBJFpJ0njuOKzZs2UrNWLWrUrEmZsmXp2u1mlsbHO5VZEh9Pj9hYRITLWlxOVtYRUlNTCt13SXw8PXvZR6Y9e8WyZPHiEu9bYapdGkHmwSMcTjqK7bSNP5ftpn7bmk5lGneqy1+r9pKVegyA44dPFllvxr7DZCQe8Uib3WXXjj+JqhZDZHR1gsqUofV11/P7mhVOZSpVqUrdho0JDHJODhzYt4d6lzahXHAwgYFBNGp2OetWJwD2L58nTzj+rY4do0qY179rOdmxbQvVYmoQXT2GMmXK0L7zTaxZmeBUpkrVMBo0akJg4LlJkbSUZH77ZSU33uKccal5SR1q1LrEo22/UJs3baJmzbPv1S43d2PpEuf3+dL4eLr3PPM+b0FWVhapqakAXNWyZb4jnqVL4unRqxcAPXr1Ykl86Xqfe0kikPvDpAZwIJ8yC4wxx4wxaUAC0KKwSj01MhoDtDfGVAP6AK966DhFSklOITo6Omc9MjqK5BTnb/UpKclE5SoTFRVNSnJKoftmpKdjsdjPyVkskWRkZHiyG8UWEl4hJ8gAZKUdJzS8olOZsJhKBIeUpf8bXbhnQneaXl83Z5sxhn5jb+SeCd1p0a1BibXbHQ6lpxIWcfZ8aVi4hUPpaS7tW+OSOmzfsoGjRw7z78mTbPjtFzLS7OnuOwc8ybSpHzL4/j5Mm/oBt937sEfaf77SU1OJsETlrEdYIkl3fNi6Ysr747nv4ScIEN87lZySnEx0tXPfw05lUpzfz1FRUaQkFz7CT09Px2KxAGCxWErd+zyHuHEp2lqggYjUEZGyQH9gdp4ys4D2IhIkIhWwp/H+LKxST50zOm2M2QZgjFkjIqGu7JQ7Vzlp0iTuefChC26IMXlHjyB5/8XzKyPi2r6lVH4p5Lz9CQgMILp+ON8+t4igcoHcNf5mDmxL49D+I3w9ZD5HM05QoXIw/V69kfR9R0gsxam53PJ52VxOqVevWZtb+tzJG/8dTLnyFahVpz4BAYEAxM+byR0PPcHV13RkzfJ4Pn7vNYaPfsd9Db9A5pxMiev9XrtqOZWrVqX+pY3Z9Mc6N7fM81zpe77vZz+4cgFQoldgMMacFpHHgYVAIDDVGLNFRAY5tk80xvwpIguAjYANmGKMOfekdS6eCkaRIjKkoHVjzNv57eTITZ7JT5qTVtsFNyQqOoqkpKSc9ZSkZCIjnWcZRkZFk5yrTHJyEpZIC9nZpwrcNyw8nNTUFCyWSFJTUwgLC7vgtrpTVtpxQi1nR0KhERU4mnE8T5ljnDhykux/T5P972n2bU4msm5VDu0/wtGME4A9dbdj1V6qXRrhM8EoLMKSM5oByEhPLVZKrcNN3elwU3cAvv98EmHh9td8RfwC7hz4FACtru3E1Amvu7HVFy7CEkla6tnXKC01hbAIi0v7bt28kV9XLmfdL6s4depfjh8/xlujX2LoiyM91Vy3ioqKJulg3vdwZJ4yzp8FycnJ55TJKzw8nNTUVCwWC6mpqaXufe4txph5wLw8z03Msz4OGOdqnZ4aj0/GPoPizJJ7PcRDx8xX02bN2btnD4mJiWSfOsWC+fPo0KmTU5mOnTsxZ9YsjDFs3LCekNBQLJbIQvft2Kkzs2fOAmD2zFl06ty5JLtVpIPb06havRKVo0IICAqgcYc67Pwl0anMjtX7qNE0CgkQgsoFUu3SCNL3HqZMuSDKlrd/TylTLojaV1Yn7Z9D3ujGeanToBHJBxJJTTrA6exs1iQs5opW17q8/5FMe1/TU5JZtyqBNh1uAKBKWATbNq8HYOvGdURVr+H2tl+IBpc24UDiPpIO7ic7O5vl8Yto3a69S/veO/AxPpn+I1O+ncUzL43hsita+kwgAmjarBl79+5hv+O9unDe/HPe5x06debH2Wfe5xsICQnNScEVpEOnTsyZOROAOTNn0rFT6Xqf5yjhH716gkdGRsaYEQVtE5GnPXHMggQFBfH8Cy/yyICHsNls9Op9K/UbNOC7adMA6Ne/P+2v68CKhAS6d+1CcHAwI8eMLXRfgAcGPMQzg4cw84fpRFerzpvjx5dkt4pkbIafP1zDbWNuQAIC2LRoB+l7Mrn8ZvuElvXz/iJj32F2r9vP/R/1tL9BF+wgbU8mlaND6P2S/Y0cEBjA1iW72L3Ofn6yQbta3PBIK8pXDqbPyOtJ2ZXB9y/87LV+5icwMIi7Bw1m3MtDsdlsXHfDLdS4pA7x82cC0LlbLzIPpfPK4AGcOH6MgIAAFs3+nlc//ILyFSoy4dUXOZp12F7PI4OpGGLPMj/w+LN8OfldbFYrZcqW5f7Hn/ViL88VGBTEw089wyvPPInNZuOGbj2oVace82f9AEC32D4cSk9jyMP3cfz4MQJEmD19Gh98No0KFQv+jrh6+RLi3n2Lw4cPMfL5IdSt34AR4yaUVLdcEhQUxHMvvMAjAwdgs9mI7d2b+vUb8P239vf5bbf3p/1117EiIYEe3boSHBzMiNFjcvZ/btgwflv7K5mZmdzUuROPPPY4vfv04YGHBvDskMH8b8YPVKtWjXFvl673eQ4/yDZKfnlUjx5QZK8xppYLRd2SpvNFwYEBvNH1M283o8Q9u+BefvmryN/G+aU2DSPZfvCwt5tR4i6tVpkTp63eboZXlA8KdFsIefOBH9z2QT5sah+vhDZv/OjVD2K4UkqVIn4wEcMbwahkh2JKKeXvfG82/jk8EoxEJIv8g44A5T1xTKWUUr7LUxMYXPpdkVJKKTfQNJ1SSilv84cf7/pBplEppZSv05GRUkr5Oj8YVmgwUkopX+cHaToNRkop5ev8IBj5weBOKaWUr9ORkVJK+To/GFZoMFJKKV+naTqllFLqwunISCmlfJ0fjIw0GCmllK/zgxyXH3RBKaWUr9ORkVJK+TpN0ymllPI6PwhGmqZTSinldToyUkopX+cHwwoNRkop5es0TaeUUkpdOB0ZKaWUr/ODkZEGI6WU8nV+kOPygy4opZTydToyUkopX6dpOs8KDrx4B27PLrjX203wijYNI73dBK+5tFplbzfBK8oHBXq7Cb7P92NR6Q5GJ602bzfBK4IDA/h5w35vN6PE3dAihmnLd3m7GV7Rv31dJs3a7O1mlLiHY5txIPOEt5vhFdWrlPd2E0qVUh2MlFJKuSDA94dGGoyUUsrX+cE5o4v3pIxSSqlSo8BgJCJZInLEsWTlWs8SkSMl2UillFKFEDcurhxOpKuIbBeRnSLyXD7bO4rIYRFZ71heKqrOAtN0xphQ15qllFLKq0rwnJGIBAIfADcCicBaEZltjNmap+hyY0x3V+t1KU0nIteKyP2OxxEiUsfVAyillPIrrYCdxphdxphTwDQg9kIrLTIYicjLwHDgecdTZYEvL/TASiml3ETEbYuIDBSR33ItA/McLQbYl2s90fFcXm1FZIOIzBeRpkV1wZXZdL2BK4DfAYwxB0REU3hKKVVauDFLZ4yJA+KKeTSTZ/134BJjzFERuRmYCTQo7LiupOlOGWPMmYOJSEUX9lFKKeWfEoGaudZrAAdyFzDGHDHGHHU8ngeUEZGIwip1JRh9JyKTgCoiMgD4GZhcnJYrpZTyoABx31K0tUADEakjImWB/sDs3AVEJFrE/uMnEWmFPdakF1ZpkWk6Y8ybInIjcARoCLxkjPnJlRYrpZQqASX4o1djzGkReRxYCAQCU40xW0RkkGP7RKAv8IiInAZOAP0dGbYCuXoFhk1Aeeypuk3n2QellFJ+wJF6m5fnuYm5Hr8PvF+cOl2ZTfcQ8CtwK/Zo94uIPFCcgyillPKgEv7Rqye4MjJ6BrjCGJMOICLhwCpgqicbppRSykV+cKFUVyYwJAJZudazcJ5jrpRSSl2QAkdGIjLE8XA/sEZEZmE/ZxSLPW2nlFKqNPCDq3YXlqY788PWvx3LGbM81xyllFLF5gf3XyjsQqkjSrIhSimlLl5FTmAQEQvwLNAUCD7zvDGmswfbpZRSylV+kKZzZXD3FbANqAOMAP7B/gtcpZRSpYEbL5TqLa4Eo3BjzMdAtjFmmTHmAaCNh9ullFLqIuLK74yyHf8/KCK3YL8gXg3PNUkppVSx+PMEhlxGi0hlYCgwAagEDPZoq5RSSrnOD84ZuXKh1B8dDw8DnTzbHKWUUhejwn70OoFzb5iUwxjzZCH73lPYQY0xn7vUOqWUUkXz85HRbxdQ79X5PCdAD+y3py3RYLRy+XJef3UsNquN3n378uCAAU7bjTG8PnYsKxISCC4fzKixY2ncpGmh+x7OzOTZoUM4sH8/1WNiGPf2eCpVrlyS3SrSlvW/Mv2T97HZbFxz/c3c1OsOp+2/Lv+Zn2ZNA6BccDD9HxpMjdr1yD51ivEvP8Xp09lYrVauaNOB7v3uA2DfPzuZNnk82adOERgYyO0PPUXt+o1LumtF2rH5N+Z/MxFjs3Fl+660v7mf0/Ztf6wmfubnSEAAAQGBdO0/kEsaNCt038UzP2f7H6uRgAAqhlam1wNDqVQlvMT7Vpjd2/9g6ayp2IyN5q2up1WnW52279zyK6sWfoOIvd8de95PTJ3GZKTsZ+5Xb+eUO5yRTLub+nNl++6cOJ7F3K/e5khGCpXCIul+51CCK4SUdNeK9Ovqlbz/9htYbTZu6dmbO+51vp6zMYYJb7/BmlUrCA4OZvh/R9Kwkf1v9/tvvmDurP8hItSt14Dh/x1B2XLl2PnXNt5+bQynTv1LYGAQTz/7PI2bNvdG9wrnz+eMjDGfnW+lxpgnzjx23GDpTmA48Asw5nzrPR9Wq5Wxo0cxacrHREVFccft/ejYqRP16tfPKbMiIYG9e/YwZ8ECNm3cwOgRI/nq228L3XfqlMm0atOWBwcM4OPJk/l4ymQGDx1Wkl0rlM1m5buP3+WJF8dRJdzCG88/QvOW7ahWo3ZOmYjIaAa/Mp4KIaFs+WMNX8e9xbNjPySoTBmefPltgoPLYz19mrdeepKml7eiTsMmzPxyEjf3vYemV7Rm8++/MPPLOJ5+Zbz3OpoPm83K3K8+4J4hY6lUNYK40U9x6eWtiax+SU6ZOo0v55HL2yAiJO3bzfeTxvLE6MmF7ntNlz5c38s+6P/l51ksm/M1Pe5+oqBmlDibzUr8/ybTZ8BLhFYO56sJw6nX5GrCo87elLNW/ebUa3I1IkLqwX/48cu3uP+ZCYRFxnD34Ldy6okbPZD6zVoBsHbJ/6hVvzmtOt3Kr0tm8OvS/3HdzXd7pY8FsVqtvDvuVcZNmIglMopB991Ju/YdqF23Xk6ZNatWsH/fXr6cPps/N29i/Btj+Gjql6SmJDPj22/4dNoMygUH88r/PUP8Twvo2j2WSRPe4d6HHqZ1u2v5ZeVyJr3/Du989LEXe+q/PBZPRSTIcfuJrcANQF9jzO3GmI2eOmZ+Nm/aSM1atahRsyZlypala7ebWRof71RmSXw8PWJjEREua3E5WVlHSE1NKXTfJfHx9OwVC0DPXrEsWby4JLtVpH92bsMSHUNEVHWCgspwVbvObFy7yqlM3UubUSHEftWnOg2akJmeCoCIEBxcHgCr9TQ26+mcNICIcPLEcQBOHj9G5aqla2QAsH/3X4RFVifMUo2goDI0a9WBbet/cSpTLrg8jhtRkn3qJGeunV/YvsHlK+bsf+rUyZLpTDEk7dtJlYhoqoRHExhUhkYtruXvLc4/CSxbLne//815nNvenZuoEh5FpaqRAPy9ZS1NrrKfLm5yVSf+3lz6Lk25betmqteoSfWYGpQpU4bON3ZhZcJSpzIrE5ZyU7fuiAhNml/Gsaws0tPsf/NWq5V///0X6+nT/HvyJOERFvtOIhw7dgyAY0ePnn2+tPGD3xm5enO9YhGRx4CngMVAV2PMHk8cxxUpySlER0fnrEdGR7Fpo3M8TElJJipXmaioaFKSUwrdNyM9HYvF/ma1WCLJyMjwZDeKLTMjjarhkTnrVcIj+GfHnwWWXxU/j6ZXtM5Zt9msvDZ8EKlJ++nQpRd1GtjTGX3vfYz3xwxnxhf2NNbQ0RM814nzdORQGpWrnv3QqFw1gsRd288p9+fvK/l5xqccO5LJnU+NdGnfn2d8yobViwkuX5H7nnnNg70ovqOHMwitHJGzHlI5jIP7dpxTbsfmNayY/yXHjx6h9wP/d8727etXcunl1+asHz+aSUilqvY6K1Xl+LHDHmj9hUlLSSEy6ux71RIZxZ9bnO8DmpbqXCYiMoq01BQubdyUfnfew+2xXSlXLpiWrdtwdZt2ADw++BmefepRJr73NsbYmDD5vBNGnuUH54w8NTI6MwX8WmCOiGx0LJtEpERHRvnd6Vby3kEqvzIiru1bWhXQp/z8tfkPVi2ZT+ydZ8+lBQQE8n/jJjNm4nf88/c2DuzdDUDCotn0ufdRxnz0LX3ufYyvJr7pmfa7Wz5db3zlNTwxejL9H3+J+JmFnMbMte8Nt97H0HFf0LxNJ9bEz3F/Oy+Ia3+vDZq15v5nJhB777OsWviN0zbr6Wz+3rqWhpe181grPcHk1/c8f+/53/VayDpyhFUJS/nmf3OZPncRJ0+c4Kf5cwGYNeN7Hn16GN/NWcijTw9j3Bi9ZKenFBiMRGSCiLxX0FJEvUOxT1bo7fj/maW74/8FHXOgiPwmIr/FxcUVvzf5iIqOIikpKWc9JSmZyMhIpzKRUdEk5yqTnJyEJdJS6L5h4eGkpqYAkJqaQlhYmFva6y5Vwi0cSk/JWc9MT6Ny1Yhzyu3f8zdfTXqTh58ZRUjouRMwKlQMoUGTFmxdb0/NrFm2iMtbtwfgyrYd2LNzm4d6cP4qVY3g8KHUnPXDh9IILWSiQe2GzclIPcixrMMu73tZ6478uW6lext+gUIqh5N1OC1n/ejhDEIqFfx3WaNuUzLTkzlx7EjOc7u3/0FUTF0qhlbJea5CSBWOHjlkr/PIISpULF0TdcA+EkpJPvteTU1JPiellrdMWkoyERYL69b+QnT1GKpUDSMoqAztO13P5k3rAVg0dw7XdboegI7X38S2LZs935nzEeDGxUsKO/RvwLpClsLEAO9iv+/RZ8DDQDMgq7CUnTEmzhjT0hjTcuDAgS53ojBNmzVn7549JCYmkn3qFAvmz6NDJ+efS3Xs3Ik5s2ZhjGHjhvWEhIZisUQWum/HTp2ZPdN+N43ZM2fRqXPpum7sJfUakXJwP2kpBzl9Opt1q+Jp3rKtU5mMtGTi3nyZex9/nqjqZ09yZx3J5PixowCcOvUv2zf9TlRMLQAqh4WzY+sGALZv/gNLdEwJ9ch11Ws3JCP5AIdSkzh9OpvNvy6jUQvnK1ilJx/I+aZ8YM9OrKdPUyGkUqH7pifvz9l/2/pfiKhWui5EEl2jPplpBzmckYz1dDbbNqygbpOWTmUOpR3M6Xdy4i6s1tMEVwjN2b59/QqnFB1A3SYt2bpuCQBb1y2hXtP8Jst6V6PGTdm/by8HD+wnOzub+J8W0u66Dk5l2rXvwKL5P2KMYeumjVQMCSE8wkJkVDW2bt7IyZMnMMbw+9o1XFK7LgDhFgsbfrdPLP79t1+JqVmrxPvmChFx2+ItnppNNwxARMoCLYF2wAPAZBHJNMY0Od+6iysoKIjnX3iRRwY8hM1mo1fvW6nfoAHfTbNPae7Xvz/tr+vAioQEunftQnBwMCPHjC10X4AHBjzEM4OHMPOH6URXq86b40vXjLLAwED6PfAEH4wZjs1mpW2nblSvWYfli2YD0P6mnsyf/gXHjh5h2pR3c/YZ/tpEjhxK5/MPXsdms2GMjSvbdqT5VfZAdsfDQx3Txa0ElSnLHQ8P9VofCxIYGMjNdzzCF++8iM1m5YprbiIy5hLWLrWnXq7ueAtbf1/BhtWLCQwMIqhMWW57+DlEpMB9AX764RPSkxIRESqHR5aqmXQAAYGBdIp9iB+mjMLYbDS7ujMR0bXYsHohAC3admHHpl/48/elBATY+939ziFOExr27NjADbc+7FRvq0638uNXb7H518WEVrXQ/a5S+JoHBfHksOd49slHsNlsdOsRS5269Zk943sAet56G22uac+aVSu4q08PygUHM/y/9pRbk2bN6dD5Bgbe8x8CAwNp0LAR3Xv1AWDY8y8x4e03sFqtlC1XlqHP/9drffR3kn8eNVcB+y0khgNNKOYtJByXEWoLXOP4fxVgkzHmfhfaZk5abS4U8z/BgQH8vGF/0QX9zA0tYpi2fJe3m+EV/dvXZdKsUpoC8qCHY5txIPOEt5vhFdWrlHfbMOTtuDWFf5AXw5CBrb0yPHJlNt1XwLfALcAg4F4gtbAdRCQO+/2PsoA1wCrgbWPMoQtqrVJKqXP4wWQ6j91CohZQDkgC9gOJQOaFNFQppVT+/PqcUS7FvoWEMaar48oLTbGfLxoKNBORDGC1MeblC2izUkopP+OxW0gY+8mozSKSif2K34exT+1uBWgwUkopd/Hna9OdcT63kBCRJ7GPiK7BPrJaCawGpgKbCtlVKaVUMXkzveYuRQYjEfmEfH7a7Th3VJDawHRgsDHm4Hm3Timl1EXBlTTdj7keB2O/qsKBwnYwxgy5kEYppZQqhothZGSM+SH3uoh8A/zssRYppZQqFj+IRed12qsB9qnbSimllFu4cs4oC+dzRknYr8iglFKqNPCDoZErabrQosoopZTyHgnw/WBUZJpORM65hWl+zymllFLnq7D7GQWLSBgQISJVRSTMsdQGqpdYC5VSShVO3Li4cjiRriKyXUR2ishzhZS7WkSsItK3qDoLS9M9DDyNPfCsy9XMI8AHrjVZKaWUp5Xkj15FJBB7DLgR+3VH14rIbGPM1nzKvQ4sdKXewu5n9C7wrog8YYyZcN4tV0op5U9aATuNMbsARGQaEAtszVPuCeAHwKW7MboytdsmIlXOrDhSdo+6UrlSSinPE3HnIgNF5LdcS97bbscA+3KtJzqey9UeicF+gYSJrvbBlSswDDDG5KTljDGHRGQA8KGrB1FKKeVBbkzTGWPigLjCjpbfbnnW3wGGG2OsrqYQXQlGASIijqtwn8kDlnWpdqWUUv4mEaiZa70G514iriUwzRGIIoCbReS0MWZmQZW6EowWAt+JyETs0W8QsMD1diullPKkEr5q91qggYjUwX7z1P7AHbkLGGPq5Grbp8CPhQUicC0YDQcGAo9gH54tAiYXo+FKKaU8qQTvZ2SMOS0ij2MfqAQCU40xW0RkkGO7y+eJcnPlCgw27CehJgKIyLXYb7L32PkcUCmllG8zxswD5uV5Lt8gZIy5z5U6XRkZISKXA/8Bbgd2AzNc2U8ppZTn+fXN9USkIfZc4H+AdOBbQIwxLt3tVSmlVAnx52AEbAOWAz2MMTsBRGRwibRKKaXURaWw0159sN8uYomITBaR63H5ykVKKaVKijt/9OotBQYjY8z/jDG3A42ApcBgIEpEPhKRm0qofUoppYogIm5bvKXICYHGmGPGmK+MMd2x/7hpPVDgVVqVUkqp4hLHhRVKo1LbMKWUcgO3DUMmzdrsts/Lh2ObeWV45NLUbm85abV5uwleERwYwNFTVm83o8SFlA1k075D3m6GVzSvWZVxt0/zdjNK3DPf9mfFn8neboZXXNs4ym11+cPU7hL83a5SSimVv1I9MlJKKeUCPxgZaTBSSikf5wexSNN0SimlvE9HRkop5ev8YGikwUgppXycBPh+MNI0nVJKKa/TkZFSSvk4P8jSaTBSSimf5wfRSNN0SimlvE5HRkop5eP84XJAGoyUUsrX+X4s0jSdUkop79ORkVJK+Th/+J2RBiOllPJxvh+KNE2nlFKqFNCRkVJK+TidTaeUUsrr/CAWaZpOKaWU9+nISCmlfJw/jIw0GCmllI8TP5hPp2k6pZRSXqcjI6WU8nGaplNKKeV1/hCMNE2nlFLK6y6KkdHK5ct5/dWx2Kw2evfty4MDBjhtN8bw+tixrEhIILh8MKPGjqVxk6aF7ns4M5Nnhw7hwP79VI+JYdzb46lUuXKJ960wq1Ys583XX8VqtdLr1r7c/9C5/R732lhWLk8gOLg8r4weS+MmTUhKOshL//c86WlpBAQIvfv244677gbg8OFMnh82lAMH9lO9egyvvfl2qes3wB+/ruaTD8djs9m4vltPev/nHqft+/f+wwfjRrNr53b+c/8gYvvdmbPtkTt7Ub58RQICAwgIDOSNDz8F4J+/dxD3zuucPHECS3Q0Tz0/kgoVK5Zkt4pUu0U01993JRIgbIzfxa+z/jynTM0mkXS+9woCAgM4kfUv00bEA9B1UCvqXlmd40dO8umwBTnlgyuWpcfT7ahsqcjh1GPMfmcl/x7LLrE+uWrT72v4Zsp7GJuN9jfews197nLa/suyRcyf8TUA5YLLc/egodSsUx+AqRNeY+NvqwitXJVR732Ws8/alUuYPe0TDibu4cVxk6hdv1HJdagYSvpHryLSFXgXCASmGGNey7M9FhgF2IDTwNPGmBWF1emRkZGIZInIEceSlWv9uIic9sQxC2K1Whk7ehQfTorjf3PmsGDeXP7eudOpzIqEBPbu2cOcBQt4acQIRo8YWeS+U6dMplWbtsxZsJBWbdry8ZTJJdmtIlmtVl4bM5r3PpzE9FlzWDh/Hrv+du73yuUJ7Nuzh5lzF/DiyyN4dfQIAAIDgxg87Fl+mP0jn341je+nfZ2z76cfT+Hq1m2YOXcBV7duw6cfTynxvhXFarUyZcKbvDB2POM//oYVSxaxb89upzIhoZV44LEh9LztjnzreOWtD3hz0hc5gQjgo7fGcudDj/L2lK9odU1HZn33pSe7UWwiwo0PtGT6q8uYOmQ+ja+pRXhMJacy5SqU4YYHr2LGG8v5ZNh8Zo9fmbNt87LdTH912Tn1tu7VmD2bk5ny9Fz2bE6mdWwTj/eluGxWK19NGs/gl8YxasLnrFm+mAP7/nEqExFVjWfHTGDEu5/So9+9fPbhuJxt13TuyuCXxpFXTK06PPbcaBo2aeHpLlwQceNS5LFEAoEPgG5AE+A/IpL3j2Ix0MIYcznwAFDkB4VHgpExJtQYU8mxhALVgTFAEvZoWmI2b9pIzVq1qFGzJmXKlqVrt5tZGh/vVGZJfDw9YmMRES5rcTlZWUdITU0pdN8l8fH07BULQM9esSxZvLgku1WkLZs2nW17mbLc1K0bS5c493vZknhu6Wnvd/MWLTialUVqaioWi4XGTex/WxUrVqROnbqkJKfk7NM9thcA3WN7sXRJ6eo3wM7tW4muXoOo6jGUKVOGazreyNqVCU5lKlcNo36jJgQGup4cOJC4hyaXXQFAi6tasWb5Ere2+0JVqx/GoeQsDqccw2a1sW3VXupfHeNUpvG1l7Dj10Sy0o8DcPzIvznbEv9M5eTRU+fUW79lDFuW2YP5lmW7aZCnztJg144/iawWgyW6OkFlytDq2uv5Y43zF/H6jZpTMSQUgLqXNuVQemrOtkubXk7FEOfADVC9Zm2iY2p5tvFuICJuW1zQCthpjNlljDkFTANicxcwxhw1xhjHakXAUASPnjMSkSoi8gqwAQgFrjbGDPXkMfNKSU4hOjo6Zz0yOorklGTnMinJROUqExUVTUpySqH7ZqSnY7FEAmCxRJKRkeHJbhRbfn1KdQSUs2VSnMpERkWRmuff5sD+/Wzb9ifNLrsMgPT0dCwWCwAWi4WM9NLVb4CMtFQiIiNz1sMtkWTk+uApiogwaviTPPvIvfz048yc52vWrsfaVcsBWJ2wmLTUlAJq8I6QsPI5QQYgK/0EIVXLO5WpWi2U4Ipluf2lztz96k00va52kfVWqBzMscyTABzLPEmFSsFubbc7ZGakERZx9jWvGm4hM6Pg13z5zz/S/MrWJdE0fxQD7Mu1nuh4zomI9BaRbcBc7KOjQnkqTRchIq8Cv2PPF15hjHnRGJNexH4DReQ3EfktLi7OLW05G5xzHSfvYDS/MiKu7VtK5dv2c7pdeP+OHz/GM4OfYtjw5wkJCXF7Gz0l/365bvQ7cYyb+DkvjB3PgtnT2brxDwAeG/YCC2ZP59lH7uXE8eMEBZWyU64ufKsNCBCi6lZlxuvLmD52KW1vbUrVaqEl0DjPyu81L+hV37bpd1b8PJe+9wzybKNKkIg7l7Ofw45lYN7D5dOEc14AY8z/jDGNgF7Yzx8VylPvpj1AKvAJcBx4MPfwzxjzdn47GWPigDNRyJy02i64IVHRUSQlJeWspyQlE5nrWzNAZFQ0ybnKJCcnYYm0kJ19qsB9w8LDSU1NwWKJJDU1hbCwsAtuqztF5dOniDz9joqKciqTkpycUyY7O5tnBj9Nt1u60/mGG3PKhIeH56TyUlNTCQsvXf0G+0goLeXsqCU9NYWq4RaX9w+LsJetXDWMVtd0YMe2rTS57ApiatXmpdffA+BA4l5+X7PKvQ2/QEfTjxMaXiFnPTS8PEcPnXAqk5VxghNZSWT/ayX7Xyv7/kzFckkVDh3MKrDe44dPUrGKfXRUsUowx4+c9FgfzlfVcAsZaWdf80PpqVQJizin3L5//ubT99/g6ZfGEVKp9E28OV/u/Iqc53M4P4lAzVzrNYADhdSXICL1RCTCGJNWUDlPpenGYQ9EYE/P5V5K9Ct202bN2btnD4mJiWSfOsWC+fPo0KmTU5mOnTsxZ9YsjDFs3LCekNBQLJbIQvft2Kkzs2fOAmD2zFl06ty5JLtVpCbNmrFvzx72JyaSnX2KRfPn06Gjc7+v69SZubPt/d60YQMhIaFYLBaMMYx6+b/UqVuXu+69z3mfjp34cdZMAH6cNZMOnUpXvwHqX9qYg/v3kXzwANnZ2axc+hNXt2vv0r4nT5zgxPFjOY83rPuVWrXrAnD4kD0labPZmP7lJ9zYvbdnOnCeDv6dQdXoUCpb7DMBG7Wrxc7f9juV2fnbfmo0siABQlDZQKo1CCNj/5FC6935236adqgDQNMOdc6pszSo06ARyQcTSU0+wOnsbH5dsZjLW13jVCY9NZkPX3uRhwa/QHRMzQJqUi5YCzQQkToiUhboD8zOXUBE6otjBCIiVwJlgcIzY/kPbz1HRJ42xrzjQlG3jIwAli9bxhuvvYrNZqNX71sZMGgQ302bBkC//v0xxvDq6FGsXLGC4OBgRo4ZS9NmzQrcFyAz8xDPDB5C0sEDRFerzpvjx1O5ShW3tDc4MICjp6wXXM+KhGW89cZrWK02Ynv35sGBg5j+nb3fffvZ+/36mNGsWmnv9yujx9CkaTP++H0dD917N/UbNCTAcTvjx558mmuv60BmZibPDRtM0sGDRFerxutvjady5SoX3FaAkLKBbNp3yC11/b5mVc7U7s5du9PnzvtZOGcGAF163MqhjHSGP3ofJ44fQySA4PLleefjaWQdzuSNV4YD9ll57TvfRJ877wdg7oxvWTBrOgCtr+3InQ896rYptc1rVmXc7dMuuJ46l1ezT9sOCGDT0l388r+ttLihHgAbfv4bgKt7NKJZxzr2LyHxu1g37y8Auj/ZlppNIikfWo7jh0+y8vvNbFqyi+CQsvR8+hoqRVTgSNpxZo9fyclj5050OB/PfNufFX8mF13QBRt/W820qROwWW1ce8PNdL/tHpYusH9h7Ng1lk/ff511q5cRbrGfJw0IDOSlt+yzYCe9NYLtm//g6JHDVKoSRmz/+2l/Y3d+/yWBrye/S9bhTCpUDKFmnfoMeeUtt7T32sZRbhvQTF/1j9s+yPu2q11ku0TkZuAd7FO7pxpjxojIIABjzEQRGQ7cA2QDJ4Bnipra7Y1gtNcY48r0FLcFI1/jrmDka9wZjHyNu4KRr3FnMPI17gxGP6x2XzDq07boYOQJ3rgCg2/MAFBKKVVivDEdqGSHYkop5ef0tuMFEJEs8g86ApTP53mllFLnyfdDkYeCkeOqC0oppZRLStmv9pRSShWXH2TpNBgppZSv84dzRno/I6WUUl6nIyOllPJxvj8u0mCklFI+zw+ydJqmU0op5X06MlJKKR/nDxMYNBgppZSP84NYpGk6pZRS3qcjI6WU8nG+cgfqwmgwUkopH6dpOqWUUsoNdGSklFI+zh9GRhqMlFLKxwX4wTkjTdMppZTyOh0ZKaWUj9M0nVJKKa/zh2CkaTqllFJepyMjpZTycXptOqWUUl7n+6FI03RKKaVKAR0ZKaWUj/OHNJ0YY7zdhoKU2oYppZQbuC2CLN180G2flx2bVfNKZCvVI6OTVpu3m+AVwYEBF2XfgwMDOJZt9XYzvKJimUD2Hzru7WaUuJiqFegp3b3dDK+YbX70dhNKlVIdjJRSShXND7J0GoyUUsrX+cP9jHQ2nVJKKa/TkZFSSvk4TdMppZTyOn+Y2q1pOqWUUl6nwUgppXyciPsW144nXUVku4jsFJHn8tl+p4hsdCyrRKRFUXVqmk4ppXxcSabpRCQQ+AC4EUgE1orIbGPM1lzFdgMdjDGHRKQbEAe0LqxeHRkppZQqjlbATmPMLmPMKWAaEJu7gDFmlTHmkGP1F6BGUZVqMFJKKR8n7lxEBorIb7mWgXkOFwPsy7We6HiuIA8C84vqg6bplFLKx7kzS2eMicOeVivwcPntlm9BkU7Yg9G1RR1Xg5FSSqniSARq5lqvARzIW0hELgOmAN2MMelFVarBSCmlfFwJ/85oLdBAROoA+4H+wB152lMLmAHcbYz5y5VKNRgppZSPK8lYZIw5LSKPAwuBQGCqMWaLiAxybJ8IvASEAx86AuVpY0zLwurVYKSUUqpYjDHzgHl5npuY6/FDwEPFqVODkVJK+Th/uGq3BiOllPJxfnBpOv2dkVJKKe/TkZFSSvk4f7hqtwYjpZTycX4QizQYKaWUr/OHYKTnjJRSSnmdjoyUUsrH6dRupZRSXqdpOqWUUsoNLopgtHL5cnre3I3uXbrw8eTJ52w3xvDamDF079KFvr1i+XPrliL3PZyZycMPPkCPrl14+MEHOHL4cIn0pTg80e9FCxbQu0d3Lm/ahC2bN5dIP87HyhXL6d39Znp268InU/Lv+xtjx9CzWxf69e7Fn1vP3qTylRdf4PrrruW2Xj3zrfvzT6ZyZbMmHDp0KN/t3vTr6pXc068Xd/XtydefTz1nuzGGCW+9zl19e/LQnf34a9ufOdt++PZrHrijL/f/pw/Tp311zr7ffvU5ndtcweHM0tdvgCu7XMmH2yYyaUccfYb3PWd7hUoVeHH2S7y7fgLvb/6A6++7IWdbjyd7MmHTB7y/+QN6PnX2dQ+pGsLIRaOY+FccIxeNomKViiXSl+ISEbct3uKRYCQi9xS2eOKYBbFarYwdPYoPJ8XxvzlzWDBvLn/v3OlUZkVCAnv37GHOggW8NGIEo0eMLHLfqVMm06pNW+YsWEirNm35OJ8PPG/yVL/rN2jA+PcmcFXLQq956FVWq5XXR49mwkeT+GH2HBbMm8euv537vnJ5Anv37mHWvAW8+MoIXh01Imdbj169eX9i/rdzSTp4kF9Wrya6WjWP9uF8WK1W3n3zNV4b/z6ffPMD8YsW8M/uv53KrFm9gv379vLF97MY8vyLvPPGWAB2/72TubNm8OHUL5jyxbf8siKBxL17cvZLSU5i3a+/EBkdXaJ9clVAQAAPf/AII7q9zGNNHuW6/3SgZuOaTmVueewW9m3dy1OXP8H/dXyeB956kKAyQdRqegk3DejC0FZDeLLFE7Ts3opq9asD0Pe529iweAODGg5kw+IN9H3uNm90r0gi7lu8xVMjo6vzWVoBo4Bzv6550OZNG6lZqxY1atakTNmydO12M0vj453KLImPp0dsLCLCZS0uJyvrCKmpKYXuuyQ+np697Hfa7dkrliWLF5dkt4rkqX7XrVeP2nXqeKNLLtu8aRM1zrS/TFm6dOt2Tt+XLomne88zfW9BVlYWqampAFzVsiWVK1fOt+633nidp4cMLZU/Mty2dTMxNWpSPaYGZcqUofONXViVsNSpzKqEZdx4c3dEhCbNLuPo0SzS01LZ889umjRtTnBweQKDgmhx5VWsWLYkZ78P33mThx9/qtSeKG/QqiEHdx4keXcyp7NPs3xaAq1j2ziVMQbKh5YHoHxIeY5mZGE9baVm4xps/2Ubp078i81qY8uyzbTt3RaAVrGtif/M/t6O/2wxrXs516ncxyPByBjzxJkFeBJYA3TAfi/0Kz1xzIKkJKcQnevbXGR0FMkpyc5lUpKJylUmKiqalOSUQvfNSE/HYokEwGKJJCMjw5PdKDZP9dsXpKYkO7c/KpqUlBSnMinJKU59j4yKIjW58D4uWxJPZGQkDRs1cm+D3SQtNYXIyKic9YjIqJwA61zmbL8tkVGkpaZQp249Nq7/ncOHMzl58gRrVq0gJTkJgJUJS4mwRFKvwaUl05HzEB4TTtq+s31NS0wjPCbcqczc93+kRuOafHrgc97b9D6Tn4rDGMOezXtoel0zQsNCKVu+HFfd3JKImhEAVImqwqEke1ryUNIhqkRWKbE+FYe48T9v8dhsOhEJAu4DhmIPRn2NMds9dbyCGHPu3XDP+QfPr4yIa/uWUhdrv6GAvudtfj5lCstRnDhxgo/jJvFB3JQLbJ3n5Nulc8rk/5pfUqcu/e++j2eeeITyFcpTr0FDAoOCOHnyBF99+jFvvPehZxrtJvm9dHn7ekWXK9m9fhcvdv4/qtWrxsifRrGlxRMkbktkxuvTGfnTKE4ePcnuDbuxnraWUMvdoxQO1IvNU+eMHgO2AlcBXY0x97kSiERkoIj8JiK/xcUVdgt210VFR5GUlJSznpKUTGRkpFOZyKhoknOVSU5OwhJpKXTfsPBwUlPt37ZTU1MICwtzS3vdxVP99gWRUdHO7U9OyhnF5pSJjnLqe0pyMpZC+pi4bx/79++nf5/e3HLTDaQkJ3PnbX1IS0stcJ+SZomMJCXXCDYtJZkIiyVPmShSUs72OzUlmfAIe5mbe/Ym7vNveHfiVEIrVaZGjVocSEwk6eB+Btx1O//pdTOpqSk8fO8dZKSnlUynXJSWmE5EzbN9jagRQcYB52zF9fffwOoZqwE4+Lc9pVejkf280k9Tf2LwVU/zfIfnOJqRxYEd9rtoZyZnUjW6KgBVo6uSmZJZAr25OHnqnNEEoBJwLTBHRDY6lk0isrGgnYwxccaYlsaYlgMHDnRLQ5o2a87ePXtITEwk+9QpFsyfR4dOnZzKdOzciTmzZmGMYeOG9YSEhmKxRBa6b8dOnZk9cxYAs2fOolPnzm5pr7t4qt++oGmzZuzbu4f9iYlkZ59i4fz557S/Q8fO/Dj7TN83EBISiiXPB3duDRo2ZHHCCuYu+pm5i34mMiqKr77/gYiIgvcpaY0aN2X/vr0cPLCf7Oxs4n9aSNv2HZ3KtGvfgZ/m/Ygxhq2bN1IxJCQnGB1ypJqTkw6yfGk8nW/qSt36DZgxP55vZs7jm5nzsFgimfTZ14SFR5R09wq1Y+1fVG9QnajaUQSVCaJ9/+tYM3uNU5m0vam0uL4FAFUiqxBzaQ2SdtkDc2WL/RxhRE0LbW9tS8I3ywD4dfYaOt97PQCd772eX2c511laBIi4bfEWT6XpSs0Z7qCgIJ5/4UUeGfAQNpuNXr1vpX6DBnw3bRoA/fr3p/11HViRkED3rl0IDg5m5Jixhe4L8MCAh3hm8BBm/jCd6GrVeXP8eK/1MT+e6vfin3/itTFjOJSRweOPDOLSRo2YOLl0pa6CgoIY/n8v8NjDA7BZbfTs3Zt69Rsw/Vt73/ve3p9rr7uOFcsTiO3WleDywbwyakzO/s8/M4x1a38lMzOTrtd3YtCjj9OrTx9vdcdlgUFBPDFsOMOfehSrzUa37rHUqVuP2TO+B6DnrbfRut21rFm1grv69iQ4OJhnX3wlZ/9Xnh/GkcOZBAYF8dSw5witVMlLPSk+m9XGpMcn8srCkQQEBvDz1J/Yt3UvXR/uBsCCSfP5dtQ0nvr0ad7b+D4iwmfDPyEr/QgAz/3wf4SGh2LNtjLxsYkcyzwGwA+vTefZ757jxgdvInVvKq/f9qrX+lgYf0jTSX45ZI8dTCQQ6G+MOfdHDOcyJ602TzepVAoODOBi7HtwYADHsn0rV+8uFcsEsv/QcW83o8TFVK1AT+nu7WZ4xWzzo9tCyLYDh932Qd6oemWvhDZPnTOqJCLPi8j7InKT2D0B7AL6eeKYSil1sfKH3xl5Kk33BXAIWA08BDwDlAVijTHrPXRMpZS6KPnSbNeCeCoY1TXGNAcQkSlAGlDLGJPloeMppZTyYZ4KRtlnHhhjrCKyWwORUkp5hj9MYPBUMGohIkccjwUo71gXwBhjfGeajlJKlXKl8fJUxeWRYGSMCfREvUoppfyT3lxPKaV8nB8MjDQYKaWUr/OHNN1FcXM9pZRSpZuOjJRSysf5/rhIg5FSSvk8TdMppZRSbqAjI6WU8nF+MDDSYKSUUr7OD2KRpumUUkp5nwYjpZTydSV8DwkR6Soi20Vkp4g8l8/2RiKyWkT+FZFhrtSpaTqllPJxJZmmc9wk9QPgRiARWCsis40xW3MVywCeBHq5Wq+OjJRSShVHK2CnMWaXMeYUMA2IzV3AGJNijFlLrjs4FEWDkVJK+Th3ZulEZKCI/JZrGZjncDHAvlzriY7nLoim6ZRSyse5M01njIkD4op5OHOhx9WRkVJKqeJIBGrmWq8BHLjQSnVkpJRSvq5kf/W6FmggInWA/UB/4I4LrVSDkVJK+biSDEXGmNMi8jiwEAgEphpjtojIIMf2iSISDfwGVAJsIvI00MQYc6SgejUYKaWUKhZjzDxgXp7nJuZ6nIQ9fecyDUZKKeXj9Np0SimlSgHfj0Y6m04ppZTXiTEXPD3c74jIQMdc+4vOxdr3i7XfcPH23Z/6nXTkpNs+yKMrBXtlmKUjo/zl/cXxxeRi7fvF2m+4ePvuN/0WNy7eosFIKaWU1+kEBqWU8nE6m85/+UUe+TxdrH2/WPsNF2/f/ajfvh+NdAKDUkr5uJSsf932QR4ZWs4rkU1HRkop5eM0TaeUUsrr/CAW6Wy63ETEKiLrRWSziHwvIhW83SZPEpGj+Tz3iojsz/Xv0NMbbXM3ERnvuFjjmfWFIjIl1/pbIjJERIyIPJHr+fdF5L6Sba1nFPJ6HxeRyMLK+bI87+s5IlLF8Xxtf369fY0GI2cnjDGXG2OaAaeAQd5ukJeMN8ZcDtwGTBURf/g7WQW0A3D0JwJommt7O2AlkAI8JSJlS7yF3pMGDPV2Izwo9/s6A3gs1zb/eL394IdG/vAh4ynLgfreboQ3GWP+BE5j/+D2dStxBCPsQWgzkCUiVUWkHNAYOASkAouBe73SSu+YCtwuImHebkgJWI3zLbL94vUWN/7nLRqM8iEiQUA3YJO32+JNItIasGF/w/o0Y8wB4LSI1MIelFYDa4C2QEtgI/bRMMBrwFARCfRGW73gKPaA9JS3G+JJjtfzemB2nk0X2+tdKukEBmflRWS94/Fy4GMvtsWbBovIXUAWcLvxn/n/Z0ZH7YC3sX9Dbgccxp7GA8AYs1tEfsUNd6/0Ie8B60XkLW83xAPOvK9rA+uAn3Jv9IfXW2fT+Z8TjnMlF7vxxpg3vd0IDzhz3qg59jTdPuznSo5gHxnkNhaYDiSUZAO9xRiTKSJfA496uy0ecMIYc7mIVAZ+xH7O6L08ZXz69faDWKRpOnVRWQl0BzKMMVZjTAZQBXuqbnXugsaYbcBWR/mLxdvAw/jpl1RjzGHgSWCYiJTJs823X28R9y1eosHo4lZBRBJzLUO83SAP24R9MsYveZ47bIxJy6f8GIp56+RSrtDX2/Fv8D+gnHea53nGmD+ADUD/fDb72+vtU/RyQEop5eMyT2S77YO8SvkyejkgpZRSxecPExg0TaeUUsrrdGSklFI+zg8GRhqMlFLK5/lBnk7TdEoppbxOg5HyCndeIV1EPhWRvo7HU0SkSSFlO4pIu4K2F7LfPyJyzjX6Cno+T5liXQXbcSXtYcVto7p4+cF1UjUYKa8p9Arp53udMGPMQ8aYrYUU6cjZC6Yq5Rf84DevGoxUqbAcqO8YtSxxXJZmk4gEisg4EVkrIhtF5GEAsXtfRLaKyFwg9714lopIS8fjriLyu4hsEJHFIlIbe9Ab7BiVtRcRi4j84DjGWhG5xrFvuIgsEpE/RGQSLnxpFJGZIrJORLaIyMA8295ytGWxiFgcz9UTkQWOfZaLSCO3/Gsq5YN0AoPyqlxXSF/geKoV0Mxx8cqB2K+OcLXjNg8rRWQRcAVwKfZrzEVhv4zL1Dz1WoDJwHWOusKMMRkiMhE4eubae47AN94Ys8JxRe+F2G8n8TKwwhgzUkRuAZyCSwEecByjPLBWRH4wxqQDFYHfjTFDReQlR92PA3HAIGPMDscV0j8EOp/HP6O66Pn+BAYNRspb8rtCejvgV2PMbsfzNwGXnTkfBFQGGgDXAd8YY6zAARGJz6f+NkDCmboc16HLzw1AEzmbn6gkIqGOY9zq2HeuiBxyoU9Pikhvx+OajramY78Nx7eO578EZohIiKO/3+c6tt9ehkd5lh9MptNgpLzmnCukOz6Uj+V+CnjCGLMwT7mbgaIufyIulAF7qrqtMeZEPm1x+RIrItIRe2Bra4w5LiJLgeACihvHcTP1KvFK2ek5I1WaLQQeOXOFZRFpKCIVsV/mv7/jnFI1oFM++64GOohIHce+Z+5imgWE5iq3CHvKDEe5yx0PE4A7Hc91A6oW0dbKwCFHIGqEfWR2RgBwZnR3B/b03xFgt4jc5jiGiEiLIo6hVL50Np1SnjUF+/mg30VkMzAJ+2j+f8AO7Ffc/ghYlndHY0wq9vM8M0RkA2fTZHOA3mcmMGC/pUBLxwSJrZyd1TcCuE5EfseeLtxbRFsXAEEishEYhfOVwY8BTUVkHfZzQiMdz98JPOho3xYg1oV/E6XO4Q+z6fSq3Uop5eNOnLa67YO8fFCgV0KSjoyUUsrnlWyizvGzie0islNEnstnu4jIe47tG0XkyqLq1AkMSinl40oyveb4QfoHwI1AIvafMczO82PzbthnkzYAWmNPp7curF4dGSmllCqOVsBOY8wuY8wpYBrnnu+MBT43dr8AVRyTjQqkIyOllPJxwYEBbhsbOX5snvtH3nHGmLhc6zHAvlzriZw76smvTAxwsKDjajBSSimVwxF44gopkl/gyzuBwpUyTjRNp5RSqjgSsV9h5IwawIHzKONEg5FSSqniWAs0EJE6IlIW6A/MzlNmNnCPY1ZdG+zXmCwwRQeaplNKKVUMxpjTIvI49iukBAJTjTFbRGSQY/tEYB5wM7ATOA7cX1S9+qNXpZRSXqdpOqWUUl6nwUgppZTXaTBSSinldRqMlFJKeZ0GI6WUUl6nwUgppZTXaTBSSinldf8P9IFoRvdvGFgAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 504x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "gnn_model = GNN7L_GCN(data_with_nedbit).to(device)\n",
    "pred = train(gnn_model, data_with_nedbit, epochs, lr, weight_decay, classes, 'GCN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1eef5f4474b64f68b18e24fb01eb6122",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40001 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch: 0000] train loss: 185.8041, train acc: 0.0518, val loss: 156.9525, val acc: 0.2371  (best train acc: 0.0518, best val acc: 0.2371, best train loss: 185.8041  @ epoch 0 )\n",
      "[Epoch: 0020] train loss: 9.9967, train acc: 0.2379, val loss: 7.4373, val acc: 0.2354  (best train acc: 0.2450, best val acc: 0.2371, best train loss: 9.9967  @ epoch 20 )\n",
      "[Epoch: 0040] train loss: 3.1875, train acc: 0.2420, val loss: 2.8824, val acc: 0.2310  (best train acc: 0.2450, best val acc: 0.2371, best train loss: 3.1038  @ epoch 37 )\n",
      "[Epoch: 0060] train loss: 1.5522, train acc: 0.1844, val loss: 1.5360, val acc: 0.1922  (best train acc: 0.2450, best val acc: 0.2371, best train loss: 1.5522  @ epoch 60 )\n",
      "[Epoch: 0080] train loss: 1.5619, train acc: 0.1973, val loss: 1.5477, val acc: 0.1744  (best train acc: 0.2963, best val acc: 0.3170, best train loss: 1.5404  @ epoch 75 )\n",
      "[Epoch: 0100] train loss: 1.5269, train acc: 0.2240, val loss: 1.5189, val acc: 0.2351  (best train acc: 0.2963, best val acc: 0.3170, best train loss: 1.5246  @ epoch 97 )\n",
      "[Epoch: 0120] train loss: 1.5174, train acc: 0.3119, val loss: 1.5098, val acc: 0.3251  (best train acc: 0.3119, best val acc: 0.3278, best train loss: 1.5174  @ epoch 120 )\n",
      "[Epoch: 0140] train loss: 1.5115, train acc: 0.3102, val loss: 1.5039, val acc: 0.3201  (best train acc: 0.3165, best val acc: 0.3278, best train loss: 1.5115  @ epoch 140 )\n",
      "[Epoch: 0160] train loss: 1.5060, train acc: 0.3164, val loss: 1.4984, val acc: 0.3288  (best train acc: 0.3206, best val acc: 0.3302, best train loss: 1.5060  @ epoch 160 )\n",
      "[Epoch: 0180] train loss: 1.5003, train acc: 0.3251, val loss: 1.4936, val acc: 0.3312  (best train acc: 0.3276, best val acc: 0.3393, best train loss: 1.5003  @ epoch 180 )\n",
      "[Epoch: 0200] train loss: 1.4967, train acc: 0.3190, val loss: 1.4890, val acc: 0.3258  (best train acc: 0.3276, best val acc: 0.3393, best train loss: 1.4967  @ epoch 200 )\n",
      "[Epoch: 0220] train loss: 1.4935, train acc: 0.3239, val loss: 1.4844, val acc: 0.3295  (best train acc: 0.3276, best val acc: 0.3393, best train loss: 1.4933  @ epoch 219 )\n",
      "[Epoch: 0240] train loss: 1.4895, train acc: 0.3233, val loss: 1.4805, val acc: 0.3467  (best train acc: 0.3287, best val acc: 0.3467, best train loss: 1.4878  @ epoch 233 )\n",
      "[Epoch: 0260] train loss: 1.4864, train acc: 0.3230, val loss: 1.4768, val acc: 0.3315  (best train acc: 0.3376, best val acc: 0.3467, best train loss: 1.4859  @ epoch 259 )\n",
      "[Epoch: 0280] train loss: 1.4820, train acc: 0.3253, val loss: 1.4732, val acc: 0.3352  (best train acc: 0.3376, best val acc: 0.3467, best train loss: 1.4812  @ epoch 279 )\n",
      "[Epoch: 0300] train loss: 1.4795, train acc: 0.3259, val loss: 1.4694, val acc: 0.3342  (best train acc: 0.3376, best val acc: 0.3467, best train loss: 1.4786  @ epoch 295 )\n",
      "[Epoch: 0320] train loss: 1.4776, train acc: 0.3212, val loss: 1.4672, val acc: 0.3403  (best train acc: 0.3376, best val acc: 0.3467, best train loss: 1.4761  @ epoch 309 )\n",
      "[Epoch: 0340] train loss: 1.4755, train acc: 0.3221, val loss: 1.4650, val acc: 0.3319  (best train acc: 0.3376, best val acc: 0.3467, best train loss: 1.4741  @ epoch 336 )\n",
      "[Epoch: 0360] train loss: 1.4740, train acc: 0.3258, val loss: 1.4629, val acc: 0.3336  (best train acc: 0.3376, best val acc: 0.3467, best train loss: 1.4722  @ epoch 353 )\n",
      "[Epoch: 0380] train loss: 1.4709, train acc: 0.3213, val loss: 1.4610, val acc: 0.3309  (best train acc: 0.3376, best val acc: 0.3467, best train loss: 1.4688  @ epoch 378 )\n",
      "[Epoch: 0400] train loss: 1.4704, train acc: 0.3213, val loss: 1.4592, val acc: 0.3295  (best train acc: 0.3376, best val acc: 0.3467, best train loss: 1.4688  @ epoch 399 )\n",
      "[Epoch: 0420] train loss: 1.4688, train acc: 0.3211, val loss: 1.4572, val acc: 0.3298  (best train acc: 0.3376, best val acc: 0.3467, best train loss: 1.4671  @ epoch 413 )\n",
      "[Epoch: 0440] train loss: 1.4660, train acc: 0.3222, val loss: 1.4561, val acc: 0.3322  (best train acc: 0.3376, best val acc: 0.3467, best train loss: 1.4660  @ epoch 440 )\n",
      "[Epoch: 0460] train loss: 1.4675, train acc: 0.3240, val loss: 1.4541, val acc: 0.3403  (best train acc: 0.3376, best val acc: 0.3467, best train loss: 1.4644  @ epoch 459 )\n",
      "[Epoch: 0480] train loss: 1.4630, train acc: 0.3255, val loss: 1.4522, val acc: 0.3342  (best train acc: 0.3376, best val acc: 0.3508, best train loss: 1.4624  @ epoch 479 )\n",
      "[Epoch: 0500] train loss: 1.4619, train acc: 0.3271, val loss: 1.4507, val acc: 0.3379  (best train acc: 0.3376, best val acc: 0.3508, best train loss: 1.4613  @ epoch 495 )\n",
      "[Epoch: 0520] train loss: 1.4617, train acc: 0.3266, val loss: 1.4490, val acc: 0.3396  (best train acc: 0.3464, best val acc: 0.3602, best train loss: 1.4593  @ epoch 515 )\n",
      "[Epoch: 0540] train loss: 1.4612, train acc: 0.3321, val loss: 1.4483, val acc: 0.3470  (best train acc: 0.3464, best val acc: 0.3602, best train loss: 1.4582  @ epoch 536 )\n",
      "[Epoch: 0560] train loss: 1.4596, train acc: 0.3297, val loss: 1.4473, val acc: 0.3514  (best train acc: 0.3546, best val acc: 0.3680, best train loss: 1.4581  @ epoch 546 )\n",
      "[Epoch: 0580] train loss: 1.4582, train acc: 0.3355, val loss: 1.4460, val acc: 0.3535  (best train acc: 0.3598, best val acc: 0.3730, best train loss: 1.4557  @ epoch 573 )\n",
      "[Epoch: 0600] train loss: 1.4562, train acc: 0.3368, val loss: 1.4416, val acc: 0.3562  (best train acc: 0.3598, best val acc: 0.3730, best train loss: 1.4550  @ epoch 593 )\n",
      "[Epoch: 0620] train loss: 1.4493, train acc: 0.3728, val loss: 1.4365, val acc: 0.3788  (best train acc: 0.3728, best val acc: 0.3885, best train loss: 1.4464  @ epoch 617 )\n",
      "[Epoch: 0640] train loss: 1.4469, train acc: 0.3637, val loss: 1.4312, val acc: 0.3808  (best train acc: 0.3802, best val acc: 0.3909, best train loss: 1.4444  @ epoch 628 )\n",
      "[Epoch: 0660] train loss: 1.4460, train acc: 0.3779, val loss: 1.4315, val acc: 0.3862  (best train acc: 0.3810, best val acc: 0.4000, best train loss: 1.4408  @ epoch 658 )\n",
      "[Epoch: 0680] train loss: 1.4403, train acc: 0.3720, val loss: 1.4255, val acc: 0.3976  (best train acc: 0.3881, best val acc: 0.4017, best train loss: 1.4364  @ epoch 673 )\n",
      "[Epoch: 0700] train loss: 1.4381, train acc: 0.3950, val loss: 1.4208, val acc: 0.4044  (best train acc: 0.3955, best val acc: 0.4125, best train loss: 1.4359  @ epoch 698 )\n",
      "[Epoch: 0720] train loss: 1.4354, train acc: 0.3925, val loss: 1.4195, val acc: 0.4040  (best train acc: 0.3975, best val acc: 0.4125, best train loss: 1.4334  @ epoch 705 )\n",
      "[Epoch: 0740] train loss: 1.4313, train acc: 0.3800, val loss: 1.4173, val acc: 0.4007  (best train acc: 0.3975, best val acc: 0.4125, best train loss: 1.4308  @ epoch 738 )\n",
      "[Epoch: 0760] train loss: 1.4289, train acc: 0.3960, val loss: 1.4157, val acc: 0.4057  (best train acc: 0.4000, best val acc: 0.4125, best train loss: 1.4280  @ epoch 752 )\n",
      "[Epoch: 0780] train loss: 1.4275, train acc: 0.3910, val loss: 1.4132, val acc: 0.4044  (best train acc: 0.4000, best val acc: 0.4128, best train loss: 1.4261  @ epoch 774 )\n",
      "[Epoch: 0800] train loss: 1.4283, train acc: 0.3940, val loss: 1.4109, val acc: 0.4078  (best train acc: 0.4000, best val acc: 0.4128, best train loss: 1.4239  @ epoch 798 )\n",
      "[Epoch: 0820] train loss: 1.4232, train acc: 0.3946, val loss: 1.4095, val acc: 0.4111  (best train acc: 0.4000, best val acc: 0.4128, best train loss: 1.4232  @ epoch 820 )\n",
      "[Epoch: 0840] train loss: 1.4260, train acc: 0.3964, val loss: 1.4084, val acc: 0.4057  (best train acc: 0.4000, best val acc: 0.4138, best train loss: 1.4232  @ epoch 839 )\n",
      "[Epoch: 0860] train loss: 1.4233, train acc: 0.3979, val loss: 1.4071, val acc: 0.4081  (best train acc: 0.4009, best val acc: 0.4138, best train loss: 1.4209  @ epoch 847 )\n",
      "[Epoch: 0880] train loss: 1.4241, train acc: 0.3962, val loss: 1.4060, val acc: 0.4111  (best train acc: 0.4009, best val acc: 0.4138, best train loss: 1.4196  @ epoch 874 )\n",
      "[Epoch: 0900] train loss: 1.4215, train acc: 0.3980, val loss: 1.4045, val acc: 0.4091  (best train acc: 0.4034, best val acc: 0.4145, best train loss: 1.4175  @ epoch 885 )\n",
      "[Epoch: 0920] train loss: 1.4187, train acc: 0.4009, val loss: 1.4023, val acc: 0.4148  (best train acc: 0.4034, best val acc: 0.4172, best train loss: 1.4152  @ epoch 919 )\n",
      "[Epoch: 0940] train loss: 1.4151, train acc: 0.4015, val loss: 1.4007, val acc: 0.4155  (best train acc: 0.4034, best val acc: 0.4172, best train loss: 1.4149  @ epoch 924 )\n",
      "[Epoch: 0960] train loss: 1.4170, train acc: 0.3999, val loss: 1.3999, val acc: 0.4145  (best train acc: 0.4034, best val acc: 0.4172, best train loss: 1.4144  @ epoch 959 )\n",
      "[Epoch: 0980] train loss: 1.4170, train acc: 0.3990, val loss: 1.4011, val acc: 0.4159  (best train acc: 0.4034, best val acc: 0.4172, best train loss: 1.4144  @ epoch 959 )\n",
      "[Epoch: 1000] train loss: 1.4142, train acc: 0.3981, val loss: 1.3990, val acc: 0.4118  (best train acc: 0.4034, best val acc: 0.4172, best train loss: 1.4122  @ epoch 998 )\n",
      "[Epoch: 1020] train loss: 1.4148, train acc: 0.3967, val loss: 1.3981, val acc: 0.4138  (best train acc: 0.4034, best val acc: 0.4172, best train loss: 1.4117  @ epoch 1003 )\n",
      "[Epoch: 1040] train loss: 1.4148, train acc: 0.3963, val loss: 1.3974, val acc: 0.4098  (best train acc: 0.4034, best val acc: 0.4172, best train loss: 1.4117  @ epoch 1003 )\n",
      "[Epoch: 1060] train loss: 1.4158, train acc: 0.3969, val loss: 1.3973, val acc: 0.4101  (best train acc: 0.4034, best val acc: 0.4172, best train loss: 1.4117  @ epoch 1003 )\n",
      "[Epoch: 1080] train loss: 1.4115, train acc: 0.3987, val loss: 1.3957, val acc: 0.4128  (best train acc: 0.4034, best val acc: 0.4172, best train loss: 1.4106  @ epoch 1069 )\n",
      "[Epoch: 1100] train loss: 1.4146, train acc: 0.3987, val loss: 1.3940, val acc: 0.4135  (best train acc: 0.4041, best val acc: 0.4172, best train loss: 1.4097  @ epoch 1082 )\n",
      "[Epoch: 1120] train loss: 1.4120, train acc: 0.4011, val loss: 1.3927, val acc: 0.4172  (best train acc: 0.4041, best val acc: 0.4172, best train loss: 1.4090  @ epoch 1102 )\n",
      "[Epoch: 1140] train loss: 1.4120, train acc: 0.3997, val loss: 1.3913, val acc: 0.4182  (best train acc: 0.4041, best val acc: 0.4182, best train loss: 1.4082  @ epoch 1131 )\n",
      "[Epoch: 1160] train loss: 1.4080, train acc: 0.4055, val loss: 1.3919, val acc: 0.4182  (best train acc: 0.4055, best val acc: 0.4192, best train loss: 1.4062  @ epoch 1156 )\n",
      "[Epoch: 1180] train loss: 1.4093, train acc: 0.4016, val loss: 1.3899, val acc: 0.4152  (best train acc: 0.4055, best val acc: 0.4192, best train loss: 1.4061  @ epoch 1179 )\n",
      "[Epoch: 1200] train loss: 1.4109, train acc: 0.4002, val loss: 1.3903, val acc: 0.4125  (best train acc: 0.4055, best val acc: 0.4192, best train loss: 1.4052  @ epoch 1183 )\n",
      "[Epoch: 1220] train loss: 1.4070, train acc: 0.4025, val loss: 1.3884, val acc: 0.4155  (best train acc: 0.4055, best val acc: 0.4192, best train loss: 1.4049  @ epoch 1218 )\n",
      "[Epoch: 1240] train loss: 1.4078, train acc: 0.4033, val loss: 1.3869, val acc: 0.4175  (best train acc: 0.4055, best val acc: 0.4192, best train loss: 1.4043  @ epoch 1221 )\n",
      "[Epoch: 1260] train loss: 1.4051, train acc: 0.4012, val loss: 1.3868, val acc: 0.4182  (best train acc: 0.4058, best val acc: 0.4199, best train loss: 1.4030  @ epoch 1242 )\n",
      "[Epoch: 1280] train loss: 1.4083, train acc: 0.4001, val loss: 1.3882, val acc: 0.4172  (best train acc: 0.4070, best val acc: 0.4226, best train loss: 1.4030  @ epoch 1242 )\n",
      "[Epoch: 1300] train loss: 1.4090, train acc: 0.4040, val loss: 1.3849, val acc: 0.4179  (best train acc: 0.4070, best val acc: 0.4226, best train loss: 1.4016  @ epoch 1296 )\n",
      "[Epoch: 1320] train loss: 1.4075, train acc: 0.4024, val loss: 1.3842, val acc: 0.4202  (best train acc: 0.4070, best val acc: 0.4226, best train loss: 1.4016  @ epoch 1318 )\n",
      "[Epoch: 1340] train loss: 1.4039, train acc: 0.4064, val loss: 1.3841, val acc: 0.4172  (best train acc: 0.4070, best val acc: 0.4226, best train loss: 1.4016  @ epoch 1318 )\n",
      "[Epoch: 1360] train loss: 1.4014, train acc: 0.4059, val loss: 1.3833, val acc: 0.4185  (best train acc: 0.4070, best val acc: 0.4226, best train loss: 1.4007  @ epoch 1358 )\n",
      "[Epoch: 1380] train loss: 1.4035, train acc: 0.4039, val loss: 1.3829, val acc: 0.4206  (best train acc: 0.4070, best val acc: 0.4226, best train loss: 1.3997  @ epoch 1368 )\n",
      "[Epoch: 1400] train loss: 1.4057, train acc: 0.4025, val loss: 1.3817, val acc: 0.4202  (best train acc: 0.4083, best val acc: 0.4226, best train loss: 1.3976  @ epoch 1390 )\n",
      "[Epoch: 1420] train loss: 1.4029, train acc: 0.4017, val loss: 1.3817, val acc: 0.4165  (best train acc: 0.4083, best val acc: 0.4226, best train loss: 1.3976  @ epoch 1390 )\n",
      "[Epoch: 1440] train loss: 1.3986, train acc: 0.4051, val loss: 1.3811, val acc: 0.4202  (best train acc: 0.4083, best val acc: 0.4226, best train loss: 1.3976  @ epoch 1390 )\n",
      "[Epoch: 1460] train loss: 1.3993, train acc: 0.4071, val loss: 1.3808, val acc: 0.4196  (best train acc: 0.4083, best val acc: 0.4226, best train loss: 1.3973  @ epoch 1446 )\n",
      "[Epoch: 1480] train loss: 1.3985, train acc: 0.4056, val loss: 1.3797, val acc: 0.4189  (best train acc: 0.4083, best val acc: 0.4226, best train loss: 1.3973  @ epoch 1446 )\n",
      "[Epoch: 1500] train loss: 1.4014, train acc: 0.4042, val loss: 1.3797, val acc: 0.4169  (best train acc: 0.4083, best val acc: 0.4226, best train loss: 1.3973  @ epoch 1446 )\n",
      "[Epoch: 1520] train loss: 1.3984, train acc: 0.4063, val loss: 1.3784, val acc: 0.4172  (best train acc: 0.4083, best val acc: 0.4236, best train loss: 1.3951  @ epoch 1502 )\n",
      "[Epoch: 1540] train loss: 1.3987, train acc: 0.4061, val loss: 1.3784, val acc: 0.4212  (best train acc: 0.4083, best val acc: 0.4236, best train loss: 1.3951  @ epoch 1502 )\n",
      "[Epoch: 1560] train loss: 1.3978, train acc: 0.4039, val loss: 1.3779, val acc: 0.4223  (best train acc: 0.4083, best val acc: 0.4236, best train loss: 1.3951  @ epoch 1502 )\n",
      "[Epoch: 1580] train loss: 1.3981, train acc: 0.4054, val loss: 1.3789, val acc: 0.4182  (best train acc: 0.4083, best val acc: 0.4236, best train loss: 1.3951  @ epoch 1502 )\n",
      "[Epoch: 1600] train loss: 1.4021, train acc: 0.4048, val loss: 1.3801, val acc: 0.4169  (best train acc: 0.4083, best val acc: 0.4236, best train loss: 1.3951  @ epoch 1502 )\n",
      "[Epoch: 1620] train loss: 1.3971, train acc: 0.4044, val loss: 1.3778, val acc: 0.4179  (best train acc: 0.4083, best val acc: 0.4236, best train loss: 1.3951  @ epoch 1502 )\n",
      "[Epoch: 1640] train loss: 1.3989, train acc: 0.4035, val loss: 1.3777, val acc: 0.4185  (best train acc: 0.4083, best val acc: 0.4236, best train loss: 1.3939  @ epoch 1638 )\n",
      "[Epoch: 1660] train loss: 1.3999, train acc: 0.4065, val loss: 1.3770, val acc: 0.4216  (best train acc: 0.4083, best val acc: 0.4236, best train loss: 1.3939  @ epoch 1638 )\n",
      "[Epoch: 1680] train loss: 1.3990, train acc: 0.4032, val loss: 1.3766, val acc: 0.4169  (best train acc: 0.4083, best val acc: 0.4236, best train loss: 1.3939  @ epoch 1638 )\n",
      "[Epoch: 1700] train loss: 1.3974, train acc: 0.4040, val loss: 1.3780, val acc: 0.4179  (best train acc: 0.4083, best val acc: 0.4236, best train loss: 1.3939  @ epoch 1638 )\n",
      "[Epoch: 1720] train loss: 1.4111, train acc: 0.4007, val loss: 1.3823, val acc: 0.4111  (best train acc: 0.4083, best val acc: 0.4236, best train loss: 1.3921  @ epoch 1703 )\n",
      "[Epoch: 1740] train loss: 1.4035, train acc: 0.3996, val loss: 1.3803, val acc: 0.4105  (best train acc: 0.4083, best val acc: 0.4236, best train loss: 1.3921  @ epoch 1703 )\n",
      "[Epoch: 1760] train loss: 1.3973, train acc: 0.4055, val loss: 1.3783, val acc: 0.4064  (best train acc: 0.4083, best val acc: 0.4236, best train loss: 1.3921  @ epoch 1703 )\n",
      "[Epoch: 1780] train loss: 1.3951, train acc: 0.4014, val loss: 1.3760, val acc: 0.4152  (best train acc: 0.4083, best val acc: 0.4236, best train loss: 1.3921  @ epoch 1703 )\n",
      "[Epoch: 1800] train loss: 1.3942, train acc: 0.4043, val loss: 1.3794, val acc: 0.4152  (best train acc: 0.4083, best val acc: 0.4236, best train loss: 1.3921  @ epoch 1703 )\n",
      "[Epoch: 1820] train loss: 1.3942, train acc: 0.4068, val loss: 1.3769, val acc: 0.4229  (best train acc: 0.4083, best val acc: 0.4236, best train loss: 1.3921  @ epoch 1703 )\n",
      "[Epoch: 1840] train loss: 1.3920, train acc: 0.4060, val loss: 1.3721, val acc: 0.4219  (best train acc: 0.4083, best val acc: 0.4236, best train loss: 1.3920  @ epoch 1840 )\n",
      "[Epoch: 1860] train loss: 1.3958, train acc: 0.4049, val loss: 1.3706, val acc: 0.4223  (best train acc: 0.4083, best val acc: 0.4253, best train loss: 1.3920  @ epoch 1840 )\n",
      "[Epoch: 1880] train loss: 1.3953, train acc: 0.4040, val loss: 1.3704, val acc: 0.4233  (best train acc: 0.4085, best val acc: 0.4253, best train loss: 1.3903  @ epoch 1864 )\n",
      "[Epoch: 1900] train loss: 1.3938, train acc: 0.4048, val loss: 1.3760, val acc: 0.4115  (best train acc: 0.4087, best val acc: 0.4256, best train loss: 1.3903  @ epoch 1864 )\n",
      "[Epoch: 1920] train loss: 1.3959, train acc: 0.4054, val loss: 1.3737, val acc: 0.4165  (best train acc: 0.4087, best val acc: 0.4263, best train loss: 1.3903  @ epoch 1864 )\n",
      "[Epoch: 1940] train loss: 1.3939, train acc: 0.4080, val loss: 1.3734, val acc: 0.4223  (best train acc: 0.4087, best val acc: 0.4263, best train loss: 1.3903  @ epoch 1864 )\n",
      "[Epoch: 1960] train loss: 1.3967, train acc: 0.4030, val loss: 1.3775, val acc: 0.4115  (best train acc: 0.4091, best val acc: 0.4263, best train loss: 1.3903  @ epoch 1864 )\n",
      "[Epoch: 1980] train loss: 1.3938, train acc: 0.4080, val loss: 1.3748, val acc: 0.4236  (best train acc: 0.4091, best val acc: 0.4283, best train loss: 1.3903  @ epoch 1864 )\n",
      "[Epoch: 2000] train loss: 1.3927, train acc: 0.4072, val loss: 1.3747, val acc: 0.4108  (best train acc: 0.4091, best val acc: 0.4283, best train loss: 1.3903  @ epoch 1864 )\n",
      "[Epoch: 2020] train loss: 1.3960, train acc: 0.4029, val loss: 1.3697, val acc: 0.4236  (best train acc: 0.4093, best val acc: 0.4283, best train loss: 1.3901  @ epoch 2013 )\n",
      "[Epoch: 2040] train loss: 1.4019, train acc: 0.3997, val loss: 1.3697, val acc: 0.4256  (best train acc: 0.4093, best val acc: 0.4290, best train loss: 1.3901  @ epoch 2013 )\n",
      "[Epoch: 2060] train loss: 1.3919, train acc: 0.4029, val loss: 1.3693, val acc: 0.4239  (best train acc: 0.4093, best val acc: 0.4290, best train loss: 1.3901  @ epoch 2013 )\n",
      "[Epoch: 2080] train loss: 1.3934, train acc: 0.4045, val loss: 1.3684, val acc: 0.4253  (best train acc: 0.4093, best val acc: 0.4297, best train loss: 1.3893  @ epoch 2074 )\n",
      "[Epoch: 2100] train loss: 1.3914, train acc: 0.4054, val loss: 1.3664, val acc: 0.4236  (best train acc: 0.4093, best val acc: 0.4297, best train loss: 1.3871  @ epoch 2094 )\n",
      "[Epoch: 2120] train loss: 1.3962, train acc: 0.4041, val loss: 1.3728, val acc: 0.4175  (best train acc: 0.4093, best val acc: 0.4314, best train loss: 1.3866  @ epoch 2109 )\n",
      "[Epoch: 2140] train loss: 1.3936, train acc: 0.4076, val loss: 1.3694, val acc: 0.4270  (best train acc: 0.4093, best val acc: 0.4314, best train loss: 1.3866  @ epoch 2109 )\n",
      "[Epoch: 2160] train loss: 1.3935, train acc: 0.4066, val loss: 1.3697, val acc: 0.4162  (best train acc: 0.4093, best val acc: 0.4314, best train loss: 1.3866  @ epoch 2109 )\n",
      "[Epoch: 2180] train loss: 1.3893, train acc: 0.4078, val loss: 1.3674, val acc: 0.4219  (best train acc: 0.4093, best val acc: 0.4314, best train loss: 1.3866  @ epoch 2109 )\n",
      "[Epoch: 2200] train loss: 1.3881, train acc: 0.4067, val loss: 1.3691, val acc: 0.4216  (best train acc: 0.4096, best val acc: 0.4314, best train loss: 1.3866  @ epoch 2109 )\n",
      "[Epoch: 2220] train loss: 1.3883, train acc: 0.4062, val loss: 1.3756, val acc: 0.4155  (best train acc: 0.4105, best val acc: 0.4314, best train loss: 1.3866  @ epoch 2109 )\n",
      "[Epoch: 2240] train loss: 1.3909, train acc: 0.4060, val loss: 1.3667, val acc: 0.4162  (best train acc: 0.4105, best val acc: 0.4314, best train loss: 1.3866  @ epoch 2109 )\n",
      "[Epoch: 2260] train loss: 1.3875, train acc: 0.4111, val loss: 1.3704, val acc: 0.4145  (best train acc: 0.4111, best val acc: 0.4314, best train loss: 1.3866  @ epoch 2109 )\n",
      "[Epoch: 2280] train loss: 1.3929, train acc: 0.4035, val loss: 1.3669, val acc: 0.4280  (best train acc: 0.4111, best val acc: 0.4314, best train loss: 1.3866  @ epoch 2109 )\n",
      "[Epoch: 2300] train loss: 1.4187, train acc: 0.3942, val loss: 1.3728, val acc: 0.4260  (best train acc: 0.4111, best val acc: 0.4314, best train loss: 1.3866  @ epoch 2109 )\n",
      "[Epoch: 2320] train loss: 1.4625, train acc: 0.3377, val loss: 1.5198, val acc: 0.3575  (best train acc: 0.4111, best val acc: 0.4314, best train loss: 1.3866  @ epoch 2109 )\n",
      "[Epoch: 2340] train loss: 1.4487, train acc: 0.3816, val loss: 1.4067, val acc: 0.4000  (best train acc: 0.4111, best val acc: 0.4314, best train loss: 1.3866  @ epoch 2109 )\n",
      "[Epoch: 2360] train loss: 1.4179, train acc: 0.3458, val loss: 1.3972, val acc: 0.3960  (best train acc: 0.4111, best val acc: 0.4314, best train loss: 1.3866  @ epoch 2109 )\n",
      "[Epoch: 2380] train loss: 1.4084, train acc: 0.3942, val loss: 1.3915, val acc: 0.4061  (best train acc: 0.4111, best val acc: 0.4314, best train loss: 1.3866  @ epoch 2109 )\n",
      "[Epoch: 2400] train loss: 1.3996, train acc: 0.3937, val loss: 1.3861, val acc: 0.3902  (best train acc: 0.4111, best val acc: 0.4314, best train loss: 1.3866  @ epoch 2109 )\n",
      "[Epoch: 2420] train loss: 1.3978, train acc: 0.3884, val loss: 1.3845, val acc: 0.4034  (best train acc: 0.4111, best val acc: 0.4314, best train loss: 1.3866  @ epoch 2109 )\n",
      "[Epoch: 2440] train loss: 1.3984, train acc: 0.3926, val loss: 1.3837, val acc: 0.4047  (best train acc: 0.4111, best val acc: 0.4314, best train loss: 1.3866  @ epoch 2109 )\n",
      "[Epoch: 2460] train loss: 1.3980, train acc: 0.3931, val loss: 1.3825, val acc: 0.4044  (best train acc: 0.4111, best val acc: 0.4314, best train loss: 1.3866  @ epoch 2109 )\n",
      "[Epoch: 2480] train loss: 1.3986, train acc: 0.3952, val loss: 1.3820, val acc: 0.4040  (best train acc: 0.4111, best val acc: 0.4314, best train loss: 1.3866  @ epoch 2109 )\n",
      "[Epoch: 2500] train loss: 1.3977, train acc: 0.3951, val loss: 1.3816, val acc: 0.4047  (best train acc: 0.4111, best val acc: 0.4314, best train loss: 1.3866  @ epoch 2109 )\n",
      "[Epoch: 2520] train loss: 1.3995, train acc: 0.3957, val loss: 1.3813, val acc: 0.4054  (best train acc: 0.4111, best val acc: 0.4314, best train loss: 1.3866  @ epoch 2109 )\n",
      "[Epoch: 2540] train loss: 1.3965, train acc: 0.3961, val loss: 1.3811, val acc: 0.4067  (best train acc: 0.4111, best val acc: 0.4314, best train loss: 1.3866  @ epoch 2109 )\n",
      "[Epoch: 2560] train loss: 1.3964, train acc: 0.3930, val loss: 1.3801, val acc: 0.4074  (best train acc: 0.4111, best val acc: 0.4314, best train loss: 1.3866  @ epoch 2109 )\n",
      "[Epoch: 2580] train loss: 1.3961, train acc: 0.3916, val loss: 1.3804, val acc: 0.4064  (best train acc: 0.4111, best val acc: 0.4314, best train loss: 1.3866  @ epoch 2109 )\n",
      "[Epoch: 2600] train loss: 1.3952, train acc: 0.3924, val loss: 1.3802, val acc: 0.4088  (best train acc: 0.4111, best val acc: 0.4314, best train loss: 1.3866  @ epoch 2109 )\n",
      "[Epoch: 2620] train loss: 1.3960, train acc: 0.3979, val loss: 1.3788, val acc: 0.4088  (best train acc: 0.4111, best val acc: 0.4314, best train loss: 1.3866  @ epoch 2109 )\n",
      "[Epoch: 2640] train loss: 1.3966, train acc: 0.3942, val loss: 1.3784, val acc: 0.4088  (best train acc: 0.4111, best val acc: 0.4314, best train loss: 1.3866  @ epoch 2109 )\n",
      "[Epoch: 2660] train loss: 1.3928, train acc: 0.3989, val loss: 1.3785, val acc: 0.4088  (best train acc: 0.4111, best val acc: 0.4314, best train loss: 1.3866  @ epoch 2109 )\n",
      "[Epoch: 2680] train loss: 1.3926, train acc: 0.3995, val loss: 1.3779, val acc: 0.4088  (best train acc: 0.4111, best val acc: 0.4314, best train loss: 1.3866  @ epoch 2109 )\n",
      "[Epoch: 2700] train loss: 1.3932, train acc: 0.3968, val loss: 1.3778, val acc: 0.4081  (best train acc: 0.4111, best val acc: 0.4314, best train loss: 1.3866  @ epoch 2109 )\n",
      "[Epoch: 2720] train loss: 1.3938, train acc: 0.3963, val loss: 1.3768, val acc: 0.4105  (best train acc: 0.4111, best val acc: 0.4314, best train loss: 1.3866  @ epoch 2109 )\n",
      "[Epoch: 2740] train loss: 1.3926, train acc: 0.3947, val loss: 1.3772, val acc: 0.4101  (best train acc: 0.4111, best val acc: 0.4314, best train loss: 1.3866  @ epoch 2109 )\n",
      "[Epoch: 2760] train loss: 1.3900, train acc: 0.3973, val loss: 1.3762, val acc: 0.4078  (best train acc: 0.4111, best val acc: 0.4314, best train loss: 1.3866  @ epoch 2109 )\n",
      "[Epoch: 2780] train loss: 1.3938, train acc: 0.3980, val loss: 1.3769, val acc: 0.4108  (best train acc: 0.4111, best val acc: 0.4314, best train loss: 1.3866  @ epoch 2109 )\n",
      "[Epoch: 2800] train loss: 1.4038, train acc: 0.3788, val loss: 1.3799, val acc: 0.4067  (best train acc: 0.4111, best val acc: 0.4314, best train loss: 1.3866  @ epoch 2109 )\n",
      "[Epoch: 2820] train loss: 1.3941, train acc: 0.3989, val loss: 1.3760, val acc: 0.4111  (best train acc: 0.4111, best val acc: 0.4314, best train loss: 1.3866  @ epoch 2109 )\n",
      "[Epoch: 2840] train loss: 1.3902, train acc: 0.3994, val loss: 1.3815, val acc: 0.4084  (best train acc: 0.4111, best val acc: 0.4314, best train loss: 1.3866  @ epoch 2109 )\n",
      "[Epoch: 2860] train loss: 1.4060, train acc: 0.3540, val loss: 1.3861, val acc: 0.3933  (best train acc: 0.4111, best val acc: 0.4314, best train loss: 1.3866  @ epoch 2109 )\n",
      "[Epoch: 2880] train loss: 1.3923, train acc: 0.3975, val loss: 1.3761, val acc: 0.3882  (best train acc: 0.4111, best val acc: 0.4314, best train loss: 1.3866  @ epoch 2109 )\n",
      "[Epoch: 2900] train loss: 1.3860, train acc: 0.3959, val loss: 1.3738, val acc: 0.4125  (best train acc: 0.4111, best val acc: 0.4314, best train loss: 1.3833  @ epoch 2896 )\n",
      "[Epoch: 2920] train loss: 1.3823, train acc: 0.4025, val loss: 1.3735, val acc: 0.4071  (best train acc: 0.4111, best val acc: 0.4314, best train loss: 1.3823  @ epoch 2920 )\n",
      "[Epoch: 2940] train loss: 1.3910, train acc: 0.3892, val loss: 1.3827, val acc: 0.3700  (best train acc: 0.4111, best val acc: 0.4314, best train loss: 1.3823  @ epoch 2920 )\n",
      "[Epoch: 2960] train loss: 1.4066, train acc: 0.3790, val loss: 1.4049, val acc: 0.3838  (best train acc: 0.4111, best val acc: 0.4314, best train loss: 1.3823  @ epoch 2920 )\n",
      "[Epoch: 2980] train loss: 1.4017, train acc: 0.3596, val loss: 1.3881, val acc: 0.4135  (best train acc: 0.4111, best val acc: 0.4314, best train loss: 1.3823  @ epoch 2920 )\n",
      "[Epoch: 3000] train loss: 1.4209, train acc: 0.3786, val loss: 1.3835, val acc: 0.3592  (best train acc: 0.4111, best val acc: 0.4314, best train loss: 1.3823  @ epoch 2920 )\n",
      "[Epoch: 3020] train loss: 1.3961, train acc: 0.3930, val loss: 1.3802, val acc: 0.4007  (best train acc: 0.4111, best val acc: 0.4314, best train loss: 1.3823  @ epoch 2920 )\n",
      "[Epoch: 3040] train loss: 1.3858, train acc: 0.3857, val loss: 1.3784, val acc: 0.4094  (best train acc: 0.4111, best val acc: 0.4314, best train loss: 1.3823  @ epoch 2920 )\n",
      "[Epoch: 3060] train loss: 1.4005, train acc: 0.3932, val loss: 1.3767, val acc: 0.3875  (best train acc: 0.4111, best val acc: 0.4314, best train loss: 1.3823  @ epoch 2920 )\n",
      "[Epoch: 3080] train loss: 1.3998, train acc: 0.3887, val loss: 1.3749, val acc: 0.4061  (best train acc: 0.4111, best val acc: 0.4314, best train loss: 1.3823  @ epoch 2920 )\n",
      "[Epoch: 3100] train loss: 1.3889, train acc: 0.3971, val loss: 1.3746, val acc: 0.4088  (best train acc: 0.4111, best val acc: 0.4314, best train loss: 1.3823  @ epoch 2920 )\n",
      "[Epoch: 3120] train loss: 1.3833, train acc: 0.3991, val loss: 1.3746, val acc: 0.4081  (best train acc: 0.4111, best val acc: 0.4314, best train loss: 1.3822  @ epoch 3117 )\n",
      "[Epoch: 3140] train loss: 1.3857, train acc: 0.3974, val loss: 1.3729, val acc: 0.4135  (best train acc: 0.4111, best val acc: 0.4314, best train loss: 1.3822  @ epoch 3117 )\n",
      "[Epoch: 3160] train loss: 1.3954, train acc: 0.3931, val loss: 1.3719, val acc: 0.4084  (best train acc: 0.4111, best val acc: 0.4314, best train loss: 1.3822  @ epoch 3117 )\n",
      "[Epoch: 3180] train loss: 1.4183, train acc: 0.3652, val loss: 1.3863, val acc: 0.3663  (best train acc: 0.4148, best val acc: 0.4314, best train loss: 1.3729  @ epoch 3173 )\n",
      "[Epoch: 3200] train loss: 1.4148, train acc: 0.3490, val loss: 1.4120, val acc: 0.3963  (best train acc: 0.4148, best val acc: 0.4314, best train loss: 1.3729  @ epoch 3173 )\n",
      "[Epoch: 3220] train loss: 1.4101, train acc: 0.3867, val loss: 1.3880, val acc: 0.3710  (best train acc: 0.4148, best val acc: 0.4314, best train loss: 1.3729  @ epoch 3173 )\n",
      "[Epoch: 3240] train loss: 1.3972, train acc: 0.3928, val loss: 1.3791, val acc: 0.4081  (best train acc: 0.4148, best val acc: 0.4314, best train loss: 1.3729  @ epoch 3173 )\n",
      "[Epoch: 3260] train loss: 1.3955, train acc: 0.3908, val loss: 1.3821, val acc: 0.3761  (best train acc: 0.4148, best val acc: 0.4314, best train loss: 1.3729  @ epoch 3173 )\n",
      "[Epoch: 3280] train loss: 1.3892, train acc: 0.4017, val loss: 1.3779, val acc: 0.4037  (best train acc: 0.4148, best val acc: 0.4314, best train loss: 1.3729  @ epoch 3173 )\n",
      "[Epoch: 3300] train loss: 1.4104, train acc: 0.3352, val loss: 1.4649, val acc: 0.3868  (best train acc: 0.4148, best val acc: 0.4314, best train loss: 1.3729  @ epoch 3173 )\n",
      "[Epoch: 3320] train loss: 1.5121, train acc: 0.3334, val loss: 1.4296, val acc: 0.3906  (best train acc: 0.4148, best val acc: 0.4314, best train loss: 1.3729  @ epoch 3173 )\n",
      "[Epoch: 3340] train loss: 1.4563, train acc: 0.3561, val loss: 1.4155, val acc: 0.4064  (best train acc: 0.4148, best val acc: 0.4314, best train loss: 1.3729  @ epoch 3173 )\n",
      "[Epoch: 3360] train loss: 1.3915, train acc: 0.3892, val loss: 1.3832, val acc: 0.4209  (best train acc: 0.4148, best val acc: 0.4314, best train loss: 1.3729  @ epoch 3173 )\n",
      "[Epoch: 3380] train loss: 1.3899, train acc: 0.4017, val loss: 1.3648, val acc: 0.4202  (best train acc: 0.4148, best val acc: 0.4314, best train loss: 1.3729  @ epoch 3173 )\n",
      "[Epoch: 3400] train loss: 1.4100, train acc: 0.4044, val loss: 1.3902, val acc: 0.3872  (best train acc: 0.4148, best val acc: 0.4371, best train loss: 1.3729  @ epoch 3173 )\n",
      "[Epoch: 3420] train loss: 1.3816, train acc: 0.3884, val loss: 1.3594, val acc: 0.4152  (best train acc: 0.4148, best val acc: 0.4371, best train loss: 1.3729  @ epoch 3173 )\n",
      "[Epoch: 3440] train loss: 1.3806, train acc: 0.4062, val loss: 1.3563, val acc: 0.4266  (best train acc: 0.4218, best val acc: 0.4425, best train loss: 1.3726  @ epoch 3437 )\n",
      "[Epoch: 3460] train loss: 1.3744, train acc: 0.4021, val loss: 1.3618, val acc: 0.4266  (best train acc: 0.4218, best val acc: 0.4442, best train loss: 1.3716  @ epoch 3442 )\n",
      "[Epoch: 3480] train loss: 1.3738, train acc: 0.4059, val loss: 1.3560, val acc: 0.4236  (best train acc: 0.4218, best val acc: 0.4442, best train loss: 1.3716  @ epoch 3442 )\n",
      "[Epoch: 3500] train loss: 1.3810, train acc: 0.4145, val loss: 1.3585, val acc: 0.4132  (best train acc: 0.4218, best val acc: 0.4442, best train loss: 1.3709  @ epoch 3496 )\n",
      "[Epoch: 3520] train loss: 1.3712, train acc: 0.4088, val loss: 1.3540, val acc: 0.4243  (best train acc: 0.4218, best val acc: 0.4442, best train loss: 1.3682  @ epoch 3519 )\n",
      "[Epoch: 3540] train loss: 1.3820, train acc: 0.4030, val loss: 1.3590, val acc: 0.4405  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3519 )\n",
      "[Epoch: 3560] train loss: 1.4640, train acc: 0.3900, val loss: 1.4375, val acc: 0.3555  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3519 )\n",
      "[Epoch: 3580] train loss: 1.4139, train acc: 0.3670, val loss: 1.3880, val acc: 0.3636  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3519 )\n",
      "[Epoch: 3600] train loss: 1.4172, train acc: 0.3736, val loss: 1.3692, val acc: 0.4091  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3519 )\n",
      "[Epoch: 3620] train loss: 1.3880, train acc: 0.4064, val loss: 1.3674, val acc: 0.4145  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3519 )\n",
      "[Epoch: 3640] train loss: 1.3736, train acc: 0.4083, val loss: 1.3557, val acc: 0.4206  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3519 )\n",
      "[Epoch: 3660] train loss: 1.3682, train acc: 0.4108, val loss: 1.3555, val acc: 0.4206  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 3680] train loss: 1.3876, train acc: 0.4041, val loss: 1.3650, val acc: 0.4034  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 3700] train loss: 1.3872, train acc: 0.3488, val loss: 1.3638, val acc: 0.4054  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 3720] train loss: 1.7846, train acc: 0.3336, val loss: 1.6077, val acc: 0.3545  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 3740] train loss: 1.4842, train acc: 0.3422, val loss: 1.4705, val acc: 0.3639  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 3760] train loss: 1.4325, train acc: 0.3524, val loss: 1.4284, val acc: 0.3568  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 3780] train loss: 1.4112, train acc: 0.3686, val loss: 1.4067, val acc: 0.3673  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 3800] train loss: 1.3977, train acc: 0.3710, val loss: 1.3901, val acc: 0.3717  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 3820] train loss: 1.3992, train acc: 0.3650, val loss: 1.3892, val acc: 0.3693  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 3840] train loss: 1.4001, train acc: 0.3707, val loss: 1.3851, val acc: 0.3757  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 3860] train loss: 1.3815, train acc: 0.3727, val loss: 1.3801, val acc: 0.3727  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 3880] train loss: 1.4315, train acc: 0.3588, val loss: 1.4367, val acc: 0.3605  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 3900] train loss: 1.4066, train acc: 0.3567, val loss: 1.3950, val acc: 0.3642  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 3920] train loss: 1.4808, train acc: 0.3018, val loss: 1.6483, val acc: 0.3170  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 3940] train loss: 1.4950, train acc: 0.3514, val loss: 1.4583, val acc: 0.3578  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 3960] train loss: 1.4451, train acc: 0.3510, val loss: 1.4322, val acc: 0.3575  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 3980] train loss: 1.4378, train acc: 0.3508, val loss: 1.4272, val acc: 0.3555  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 4000] train loss: 1.4320, train acc: 0.3542, val loss: 1.4217, val acc: 0.3568  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 4020] train loss: 1.4304, train acc: 0.3544, val loss: 1.4202, val acc: 0.3562  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 4040] train loss: 1.4278, train acc: 0.3552, val loss: 1.4199, val acc: 0.3575  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 4060] train loss: 1.4258, train acc: 0.3568, val loss: 1.4191, val acc: 0.3589  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 4080] train loss: 1.4283, train acc: 0.3563, val loss: 1.4185, val acc: 0.3585  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 4100] train loss: 1.4249, train acc: 0.3564, val loss: 1.4184, val acc: 0.3589  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 4120] train loss: 1.4222, train acc: 0.3586, val loss: 1.4182, val acc: 0.3592  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 4140] train loss: 1.4241, train acc: 0.3572, val loss: 1.4179, val acc: 0.3589  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 4160] train loss: 1.4218, train acc: 0.3579, val loss: 1.4187, val acc: 0.3568  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 4180] train loss: 1.4229, train acc: 0.3578, val loss: 1.4172, val acc: 0.3592  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 4200] train loss: 1.4236, train acc: 0.3582, val loss: 1.4168, val acc: 0.3592  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 4220] train loss: 1.4232, train acc: 0.3583, val loss: 1.4165, val acc: 0.3589  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 4240] train loss: 1.4239, train acc: 0.3566, val loss: 1.4165, val acc: 0.3592  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 4260] train loss: 1.4239, train acc: 0.3580, val loss: 1.4161, val acc: 0.3592  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 4280] train loss: 1.4247, train acc: 0.3573, val loss: 1.4161, val acc: 0.3592  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 4300] train loss: 1.4170, train acc: 0.3605, val loss: 1.4156, val acc: 0.3592  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 4320] train loss: 1.4217, train acc: 0.3591, val loss: 1.4155, val acc: 0.3589  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 4340] train loss: 1.4214, train acc: 0.3590, val loss: 1.4150, val acc: 0.3585  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 4360] train loss: 1.4214, train acc: 0.3574, val loss: 1.4153, val acc: 0.3582  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 4380] train loss: 1.4202, train acc: 0.3576, val loss: 1.4144, val acc: 0.3582  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 4400] train loss: 1.4197, train acc: 0.3594, val loss: 1.4139, val acc: 0.3582  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 4420] train loss: 1.4219, train acc: 0.3573, val loss: 1.4137, val acc: 0.3578  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 4440] train loss: 1.4224, train acc: 0.3569, val loss: 1.4134, val acc: 0.3582  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 4460] train loss: 1.4201, train acc: 0.3584, val loss: 1.4130, val acc: 0.3582  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 4480] train loss: 1.4184, train acc: 0.3575, val loss: 1.4125, val acc: 0.3582  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 4500] train loss: 1.4212, train acc: 0.3589, val loss: 1.4118, val acc: 0.3585  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 4520] train loss: 1.4195, train acc: 0.3578, val loss: 1.4112, val acc: 0.3585  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 4540] train loss: 1.4178, train acc: 0.3572, val loss: 1.4106, val acc: 0.3585  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 4560] train loss: 1.4198, train acc: 0.3583, val loss: 1.4094, val acc: 0.3582  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 4580] train loss: 1.4139, train acc: 0.3587, val loss: 1.4089, val acc: 0.3578  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 4600] train loss: 1.4193, train acc: 0.3571, val loss: 1.4083, val acc: 0.3585  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 4620] train loss: 1.4163, train acc: 0.3592, val loss: 1.4083, val acc: 0.3578  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 4640] train loss: 1.4168, train acc: 0.3576, val loss: 1.4081, val acc: 0.3585  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 4660] train loss: 1.4146, train acc: 0.3599, val loss: 1.4078, val acc: 0.3578  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 4680] train loss: 1.4144, train acc: 0.3587, val loss: 1.4078, val acc: 0.3589  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 4700] train loss: 1.4155, train acc: 0.3583, val loss: 1.4078, val acc: 0.3582  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 4720] train loss: 1.4167, train acc: 0.3571, val loss: 1.4081, val acc: 0.3589  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 4740] train loss: 1.4158, train acc: 0.3587, val loss: 1.4080, val acc: 0.3589  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 4760] train loss: 1.4163, train acc: 0.3589, val loss: 1.4070, val acc: 0.3592  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 4780] train loss: 1.4152, train acc: 0.3585, val loss: 1.4069, val acc: 0.3595  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 4800] train loss: 1.4125, train acc: 0.3607, val loss: 1.4065, val acc: 0.3595  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 4820] train loss: 1.4135, train acc: 0.3600, val loss: 1.4058, val acc: 0.3592  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 4840] train loss: 1.4138, train acc: 0.3597, val loss: 1.4058, val acc: 0.3592  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 4860] train loss: 1.4122, train acc: 0.3612, val loss: 1.4060, val acc: 0.3595  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 4880] train loss: 1.4160, train acc: 0.3596, val loss: 1.4056, val acc: 0.3595  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 4900] train loss: 1.4109, train acc: 0.3599, val loss: 1.4052, val acc: 0.3592  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 4920] train loss: 1.4138, train acc: 0.3593, val loss: 1.4051, val acc: 0.3595  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 4940] train loss: 1.4125, train acc: 0.3595, val loss: 1.4050, val acc: 0.3595  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 4960] train loss: 1.4135, train acc: 0.3603, val loss: 1.4048, val acc: 0.3602  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 4980] train loss: 1.4122, train acc: 0.3592, val loss: 1.4044, val acc: 0.3602  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 5000] train loss: 1.4118, train acc: 0.3607, val loss: 1.4046, val acc: 0.3612  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 5020] train loss: 1.4102, train acc: 0.3613, val loss: 1.4039, val acc: 0.3605  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 5040] train loss: 1.4098, train acc: 0.3626, val loss: 1.4036, val acc: 0.3599  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 5060] train loss: 1.4128, train acc: 0.3601, val loss: 1.4044, val acc: 0.3622  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 5080] train loss: 1.4119, train acc: 0.3603, val loss: 1.4037, val acc: 0.3605  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 5100] train loss: 1.4112, train acc: 0.3616, val loss: 1.4028, val acc: 0.3612  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 5120] train loss: 1.4090, train acc: 0.3616, val loss: 1.4012, val acc: 0.3612  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 5140] train loss: 1.4085, train acc: 0.3621, val loss: 1.3996, val acc: 0.3636  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 5160] train loss: 1.4113, train acc: 0.3609, val loss: 1.3992, val acc: 0.3632  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 5180] train loss: 1.4119, train acc: 0.3589, val loss: 1.4033, val acc: 0.3629  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 5200] train loss: 1.4118, train acc: 0.3597, val loss: 1.4042, val acc: 0.3605  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 5220] train loss: 1.4098, train acc: 0.3592, val loss: 1.4028, val acc: 0.3619  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 5240] train loss: 1.4107, train acc: 0.3594, val loss: 1.3997, val acc: 0.3592  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 5260] train loss: 1.4088, train acc: 0.3577, val loss: 1.3962, val acc: 0.3589  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 5280] train loss: 1.4058, train acc: 0.3594, val loss: 1.3954, val acc: 0.3619  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 5300] train loss: 1.4048, train acc: 0.3586, val loss: 1.3961, val acc: 0.3592  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 5320] train loss: 1.4039, train acc: 0.3595, val loss: 1.3951, val acc: 0.3605  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 5340] train loss: 1.4040, train acc: 0.3583, val loss: 1.3946, val acc: 0.3609  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 5360] train loss: 1.4048, train acc: 0.3595, val loss: 1.3927, val acc: 0.3626  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 5380] train loss: 1.4060, train acc: 0.3576, val loss: 1.3932, val acc: 0.3639  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 5400] train loss: 1.4034, train acc: 0.3603, val loss: 1.3932, val acc: 0.3636  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 5420] train loss: 1.4037, train acc: 0.3589, val loss: 1.3937, val acc: 0.3626  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 5440] train loss: 1.4029, train acc: 0.3583, val loss: 1.3923, val acc: 0.3639  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 5460] train loss: 1.4039, train acc: 0.3586, val loss: 1.3928, val acc: 0.3632  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 5480] train loss: 1.4054, train acc: 0.3583, val loss: 1.3923, val acc: 0.3646  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 5500] train loss: 1.4010, train acc: 0.3599, val loss: 1.3926, val acc: 0.3656  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 5520] train loss: 1.4035, train acc: 0.3580, val loss: 1.3926, val acc: 0.3642  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 5540] train loss: 1.4042, train acc: 0.3599, val loss: 1.3920, val acc: 0.3642  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 5560] train loss: 1.4026, train acc: 0.3590, val loss: 1.3919, val acc: 0.3642  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 5580] train loss: 1.4041, train acc: 0.3579, val loss: 1.3916, val acc: 0.3649  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 5600] train loss: 1.4016, train acc: 0.3582, val loss: 1.3921, val acc: 0.3636  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 5620] train loss: 1.4034, train acc: 0.3597, val loss: 1.3917, val acc: 0.3642  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 5640] train loss: 1.4045, train acc: 0.3590, val loss: 1.3916, val acc: 0.3649  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 5660] train loss: 1.4030, train acc: 0.3582, val loss: 1.3916, val acc: 0.3649  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 5680] train loss: 1.4003, train acc: 0.3618, val loss: 1.3916, val acc: 0.3649  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 5700] train loss: 1.3987, train acc: 0.3621, val loss: 1.3912, val acc: 0.3656  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 5720] train loss: 1.4009, train acc: 0.3592, val loss: 1.3913, val acc: 0.3653  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 5740] train loss: 1.4015, train acc: 0.3607, val loss: 1.3910, val acc: 0.3653  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 5760] train loss: 1.4000, train acc: 0.3616, val loss: 1.3910, val acc: 0.3653  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 5780] train loss: 1.4022, train acc: 0.3577, val loss: 1.3917, val acc: 0.3646  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 5800] train loss: 1.4036, train acc: 0.3576, val loss: 1.3907, val acc: 0.3656  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 5820] train loss: 1.3995, train acc: 0.3611, val loss: 1.3909, val acc: 0.3629  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 5840] train loss: 1.4020, train acc: 0.3595, val loss: 1.3906, val acc: 0.3636  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 5860] train loss: 1.4022, train acc: 0.3606, val loss: 1.3906, val acc: 0.3622  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 5880] train loss: 1.4016, train acc: 0.3587, val loss: 1.3901, val acc: 0.3636  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 5900] train loss: 1.4016, train acc: 0.3587, val loss: 1.3899, val acc: 0.3629  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 5920] train loss: 1.4017, train acc: 0.3616, val loss: 1.3902, val acc: 0.3622  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 5940] train loss: 1.3970, train acc: 0.3616, val loss: 1.3903, val acc: 0.3622  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 5960] train loss: 1.3994, train acc: 0.3616, val loss: 1.3912, val acc: 0.3616  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 5980] train loss: 1.4011, train acc: 0.3598, val loss: 1.3902, val acc: 0.3636  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 6000] train loss: 1.3980, train acc: 0.3605, val loss: 1.3896, val acc: 0.3626  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 6020] train loss: 1.3991, train acc: 0.3591, val loss: 1.3897, val acc: 0.3639  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 6040] train loss: 1.3966, train acc: 0.3613, val loss: 1.3895, val acc: 0.3632  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 6060] train loss: 1.4002, train acc: 0.3613, val loss: 1.3899, val acc: 0.3626  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 6080] train loss: 1.3994, train acc: 0.3618, val loss: 1.3897, val acc: 0.3636  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 6100] train loss: 1.4015, train acc: 0.3602, val loss: 1.3896, val acc: 0.3646  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 6120] train loss: 1.3999, train acc: 0.3592, val loss: 1.3893, val acc: 0.3619  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 6140] train loss: 1.4021, train acc: 0.3603, val loss: 1.3894, val acc: 0.3639  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 6160] train loss: 1.4010, train acc: 0.3594, val loss: 1.3898, val acc: 0.3629  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 6180] train loss: 1.4026, train acc: 0.3593, val loss: 1.3910, val acc: 0.3656  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 6200] train loss: 1.4000, train acc: 0.3613, val loss: 1.3897, val acc: 0.3636  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 6220] train loss: 1.4018, train acc: 0.3590, val loss: 1.3896, val acc: 0.3622  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 6240] train loss: 1.3999, train acc: 0.3605, val loss: 1.3897, val acc: 0.3639  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 6260] train loss: 1.4010, train acc: 0.3584, val loss: 1.3894, val acc: 0.3619  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 6280] train loss: 1.3973, train acc: 0.3611, val loss: 1.3918, val acc: 0.3639  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 6300] train loss: 1.4389, train acc: 0.3584, val loss: 1.4924, val acc: 0.2934  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 6320] train loss: 1.4321, train acc: 0.3522, val loss: 1.4309, val acc: 0.3619  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 6340] train loss: 2.0324, train acc: 0.3394, val loss: 1.9294, val acc: 0.3595  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 6360] train loss: 1.4742, train acc: 0.3361, val loss: 1.4661, val acc: 0.3616  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 6380] train loss: 1.4507, train acc: 0.3456, val loss: 1.4416, val acc: 0.3589  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 6400] train loss: 1.4486, train acc: 0.3420, val loss: 1.4381, val acc: 0.3572  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 6420] train loss: 1.4451, train acc: 0.3442, val loss: 1.4371, val acc: 0.3565  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 6440] train loss: 1.4450, train acc: 0.3459, val loss: 1.4366, val acc: 0.3572  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 6460] train loss: 1.4455, train acc: 0.3451, val loss: 1.4363, val acc: 0.3572  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 6480] train loss: 1.4433, train acc: 0.3476, val loss: 1.4359, val acc: 0.3575  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 6500] train loss: 1.4446, train acc: 0.3462, val loss: 1.4355, val acc: 0.3575  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 6520] train loss: 1.4431, train acc: 0.3456, val loss: 1.4351, val acc: 0.3568  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 6540] train loss: 1.4439, train acc: 0.3462, val loss: 1.4347, val acc: 0.3562  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 6560] train loss: 1.4417, train acc: 0.3475, val loss: 1.4344, val acc: 0.3575  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 6580] train loss: 1.4424, train acc: 0.3469, val loss: 1.4342, val acc: 0.3568  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 6600] train loss: 1.4397, train acc: 0.3498, val loss: 1.4337, val acc: 0.3572  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 6620] train loss: 1.4419, train acc: 0.3467, val loss: 1.4334, val acc: 0.3568  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 6640] train loss: 1.4423, train acc: 0.3575, val loss: 1.4326, val acc: 0.3582  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 6660] train loss: 1.4412, train acc: 0.3610, val loss: 1.4324, val acc: 0.3602  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 6680] train loss: 1.4375, train acc: 0.3590, val loss: 1.4317, val acc: 0.3589  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 6700] train loss: 1.4373, train acc: 0.3614, val loss: 1.4311, val acc: 0.3602  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 6720] train loss: 1.4402, train acc: 0.3623, val loss: 1.4308, val acc: 0.3599  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 6740] train loss: 1.4354, train acc: 0.3622, val loss: 1.4300, val acc: 0.3605  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 6760] train loss: 1.4367, train acc: 0.3789, val loss: 1.4300, val acc: 0.3659  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 6780] train loss: 1.4363, train acc: 0.3762, val loss: 1.4293, val acc: 0.3616  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 6800] train loss: 1.4349, train acc: 0.3772, val loss: 1.4274, val acc: 0.3612  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 6820] train loss: 1.4294, train acc: 0.3776, val loss: 1.4243, val acc: 0.3639  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 6840] train loss: 1.4247, train acc: 0.3795, val loss: 1.4189, val acc: 0.3690  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 6860] train loss: 1.4207, train acc: 0.3813, val loss: 1.4047, val acc: 0.3825  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 6880] train loss: 1.4156, train acc: 0.3794, val loss: 1.4016, val acc: 0.3707  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 6900] train loss: 1.4130, train acc: 0.3805, val loss: 1.3963, val acc: 0.3784  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 6920] train loss: 1.4114, train acc: 0.3750, val loss: 1.3996, val acc: 0.3764  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 6940] train loss: 1.4115, train acc: 0.3710, val loss: 1.3996, val acc: 0.3754  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 6960] train loss: 1.4033, train acc: 0.3843, val loss: 1.3886, val acc: 0.3845  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 6980] train loss: 1.3993, train acc: 0.3766, val loss: 1.3790, val acc: 0.3791  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 7000] train loss: 1.4005, train acc: 0.3736, val loss: 1.3805, val acc: 0.3852  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 7020] train loss: 1.3930, train acc: 0.3795, val loss: 1.3776, val acc: 0.3821  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 7040] train loss: 1.3901, train acc: 0.3835, val loss: 1.3765, val acc: 0.3845  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 7060] train loss: 1.3891, train acc: 0.3852, val loss: 1.3696, val acc: 0.3936  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 7080] train loss: 1.5723, train acc: 0.3292, val loss: 1.6132, val acc: 0.3231  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 7100] train loss: 1.4637, train acc: 0.3357, val loss: 1.4664, val acc: 0.3147  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 7120] train loss: 1.4565, train acc: 0.3045, val loss: 1.4547, val acc: 0.2789  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 7140] train loss: 1.4559, train acc: 0.3496, val loss: 1.4519, val acc: 0.3231  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 7160] train loss: 1.4552, train acc: 0.3498, val loss: 1.4510, val acc: 0.3228  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 7180] train loss: 1.4535, train acc: 0.3499, val loss: 1.4505, val acc: 0.3228  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 7200] train loss: 1.4547, train acc: 0.3501, val loss: 1.4503, val acc: 0.3234  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 7220] train loss: 1.4507, train acc: 0.3527, val loss: 1.4501, val acc: 0.3234  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 7240] train loss: 1.4556, train acc: 0.3507, val loss: 1.4500, val acc: 0.3231  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 7260] train loss: 1.4514, train acc: 0.3541, val loss: 1.4501, val acc: 0.3231  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 7280] train loss: 1.4509, train acc: 0.3544, val loss: 1.4501, val acc: 0.3234  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 7300] train loss: 1.4514, train acc: 0.3542, val loss: 1.4500, val acc: 0.3238  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 7320] train loss: 1.4516, train acc: 0.3542, val loss: 1.4501, val acc: 0.3234  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 7340] train loss: 1.4519, train acc: 0.3536, val loss: 1.4500, val acc: 0.3238  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 7360] train loss: 1.4516, train acc: 0.3529, val loss: 1.4501, val acc: 0.3238  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 7380] train loss: 1.4526, train acc: 0.3549, val loss: 1.4501, val acc: 0.3238  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 7400] train loss: 1.4520, train acc: 0.3521, val loss: 1.4501, val acc: 0.3245  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 7420] train loss: 1.4525, train acc: 0.3540, val loss: 1.4501, val acc: 0.3234  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 7440] train loss: 1.4521, train acc: 0.3537, val loss: 1.4501, val acc: 0.3248  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 7460] train loss: 1.4528, train acc: 0.3563, val loss: 1.4501, val acc: 0.3248  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 7480] train loss: 1.4504, train acc: 0.3564, val loss: 1.4500, val acc: 0.3241  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 7500] train loss: 1.4507, train acc: 0.3574, val loss: 1.4500, val acc: 0.3245  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 7520] train loss: 1.4535, train acc: 0.3512, val loss: 1.4500, val acc: 0.3248  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 7540] train loss: 1.4542, train acc: 0.3541, val loss: 1.4500, val acc: 0.3245  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 7560] train loss: 1.4509, train acc: 0.3549, val loss: 1.4500, val acc: 0.3241  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 7580] train loss: 1.4532, train acc: 0.3522, val loss: 1.4500, val acc: 0.3241  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 7600] train loss: 1.4498, train acc: 0.3550, val loss: 1.4500, val acc: 0.3255  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 7620] train loss: 1.4520, train acc: 0.3532, val loss: 1.4499, val acc: 0.3245  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 7640] train loss: 1.4510, train acc: 0.3537, val loss: 1.4499, val acc: 0.3241  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 7660] train loss: 1.4512, train acc: 0.3541, val loss: 1.4499, val acc: 0.3245  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 7680] train loss: 1.4545, train acc: 0.3519, val loss: 1.4499, val acc: 0.3241  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 7700] train loss: 1.4516, train acc: 0.3524, val loss: 1.4498, val acc: 0.3241  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 7720] train loss: 1.4513, train acc: 0.3545, val loss: 1.4499, val acc: 0.3248  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 7740] train loss: 1.4512, train acc: 0.3545, val loss: 1.4500, val acc: 0.3241  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 7760] train loss: 1.4512, train acc: 0.3532, val loss: 1.4499, val acc: 0.3241  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 7780] train loss: 1.4509, train acc: 0.3529, val loss: 1.4499, val acc: 0.3245  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 7800] train loss: 1.4536, train acc: 0.3514, val loss: 1.4498, val acc: 0.3241  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 7820] train loss: 1.4551, train acc: 0.3513, val loss: 1.4498, val acc: 0.3238  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 7840] train loss: 1.4516, train acc: 0.3532, val loss: 1.4498, val acc: 0.3248  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 7860] train loss: 1.4503, train acc: 0.3536, val loss: 1.4498, val acc: 0.3251  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 7880] train loss: 1.4508, train acc: 0.3541, val loss: 1.4497, val acc: 0.3238  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 7900] train loss: 1.4498, train acc: 0.3540, val loss: 1.4498, val acc: 0.3248  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 7920] train loss: 1.4563, train acc: 0.3501, val loss: 1.4498, val acc: 0.3248  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 7940] train loss: 1.4507, train acc: 0.3523, val loss: 1.4497, val acc: 0.3251  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 7960] train loss: 1.4518, train acc: 0.3540, val loss: 1.4498, val acc: 0.3251  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 7980] train loss: 1.4517, train acc: 0.3554, val loss: 1.4497, val acc: 0.3248  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 8000] train loss: 1.4540, train acc: 0.3535, val loss: 1.4496, val acc: 0.3241  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 8020] train loss: 1.4516, train acc: 0.3527, val loss: 1.4497, val acc: 0.3248  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 8040] train loss: 1.4512, train acc: 0.3530, val loss: 1.4496, val acc: 0.3238  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 8060] train loss: 1.4513, train acc: 0.3529, val loss: 1.4496, val acc: 0.3248  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 8080] train loss: 1.4537, train acc: 0.3518, val loss: 1.4496, val acc: 0.3238  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 8100] train loss: 1.4513, train acc: 0.3550, val loss: 1.4495, val acc: 0.3251  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 8120] train loss: 1.4529, train acc: 0.3519, val loss: 1.4496, val acc: 0.3238  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 8140] train loss: 1.4517, train acc: 0.3534, val loss: 1.4495, val acc: 0.3241  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 8160] train loss: 1.4520, train acc: 0.3546, val loss: 1.4495, val acc: 0.3238  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 8180] train loss: 1.4512, train acc: 0.3522, val loss: 1.4494, val acc: 0.3251  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 8200] train loss: 1.4506, train acc: 0.3537, val loss: 1.4494, val acc: 0.3251  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 8220] train loss: 1.4480, train acc: 0.3545, val loss: 1.4495, val acc: 0.3251  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 8240] train loss: 1.4510, train acc: 0.3527, val loss: 1.4495, val acc: 0.3245  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 8260] train loss: 1.4531, train acc: 0.3539, val loss: 1.4494, val acc: 0.3251  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 8280] train loss: 1.4529, train acc: 0.3548, val loss: 1.4494, val acc: 0.3251  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 8300] train loss: 1.4515, train acc: 0.3527, val loss: 1.4494, val acc: 0.3251  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 8320] train loss: 1.4525, train acc: 0.3511, val loss: 1.4493, val acc: 0.3248  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 8340] train loss: 1.4487, train acc: 0.3546, val loss: 1.4493, val acc: 0.3251  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 8360] train loss: 1.4494, train acc: 0.3542, val loss: 1.4493, val acc: 0.3245  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 8380] train loss: 1.4523, train acc: 0.3546, val loss: 1.4493, val acc: 0.3251  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 8400] train loss: 1.4521, train acc: 0.3547, val loss: 1.4493, val acc: 0.3251  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 8420] train loss: 1.4512, train acc: 0.3527, val loss: 1.4493, val acc: 0.3251  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 8440] train loss: 1.4502, train acc: 0.3540, val loss: 1.4492, val acc: 0.3251  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 8460] train loss: 1.4523, train acc: 0.3570, val loss: 1.4492, val acc: 0.3251  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 8480] train loss: 1.4528, train acc: 0.3532, val loss: 1.4491, val acc: 0.3251  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 8500] train loss: 1.4493, train acc: 0.3529, val loss: 1.4492, val acc: 0.3251  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 8520] train loss: 1.4517, train acc: 0.3532, val loss: 1.4491, val acc: 0.3258  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 8540] train loss: 1.4490, train acc: 0.3535, val loss: 1.4491, val acc: 0.3251  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 8560] train loss: 1.4512, train acc: 0.3541, val loss: 1.4490, val acc: 0.3248  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 8580] train loss: 1.4524, train acc: 0.3531, val loss: 1.4490, val acc: 0.3248  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 8600] train loss: 1.4488, train acc: 0.3542, val loss: 1.4490, val acc: 0.3251  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 8620] train loss: 1.4497, train acc: 0.3532, val loss: 1.4489, val acc: 0.3251  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 8640] train loss: 1.4515, train acc: 0.3532, val loss: 1.4489, val acc: 0.3251  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 8660] train loss: 1.4494, train acc: 0.3535, val loss: 1.4488, val acc: 0.3261  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 8680] train loss: 1.4484, train acc: 0.3555, val loss: 1.4488, val acc: 0.3251  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 8700] train loss: 1.4507, train acc: 0.3548, val loss: 1.4488, val acc: 0.3255  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 8720] train loss: 1.4499, train acc: 0.3545, val loss: 1.4488, val acc: 0.3261  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 8740] train loss: 1.4520, train acc: 0.3507, val loss: 1.4487, val acc: 0.3261  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 8760] train loss: 1.4517, train acc: 0.3545, val loss: 1.4486, val acc: 0.3258  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 8780] train loss: 1.4509, train acc: 0.3538, val loss: 1.4486, val acc: 0.3261  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 8800] train loss: 1.4496, train acc: 0.3534, val loss: 1.4486, val acc: 0.3258  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 8820] train loss: 1.4521, train acc: 0.3531, val loss: 1.4485, val acc: 0.3265  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 8840] train loss: 1.4494, train acc: 0.3534, val loss: 1.4485, val acc: 0.3265  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 8860] train loss: 1.4507, train acc: 0.3544, val loss: 1.4483, val acc: 0.3255  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 8880] train loss: 1.4483, train acc: 0.3555, val loss: 1.4484, val acc: 0.3265  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 8900] train loss: 1.4483, train acc: 0.3545, val loss: 1.4483, val acc: 0.3261  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 8920] train loss: 1.4509, train acc: 0.3539, val loss: 1.4483, val acc: 0.3265  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 8940] train loss: 1.4492, train acc: 0.3550, val loss: 1.4482, val acc: 0.3261  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 8960] train loss: 1.4489, train acc: 0.3550, val loss: 1.4481, val acc: 0.3265  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 8980] train loss: 1.4502, train acc: 0.3541, val loss: 1.4480, val acc: 0.3265  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 9000] train loss: 1.4475, train acc: 0.3558, val loss: 1.4479, val acc: 0.3265  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 9020] train loss: 1.4503, train acc: 0.3523, val loss: 1.4479, val acc: 0.3261  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 9040] train loss: 1.4470, train acc: 0.3554, val loss: 1.4478, val acc: 0.3265  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 9060] train loss: 1.4470, train acc: 0.3536, val loss: 1.4478, val acc: 0.3265  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 9080] train loss: 1.4488, train acc: 0.3548, val loss: 1.4476, val acc: 0.3265  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 9100] train loss: 1.4480, train acc: 0.3545, val loss: 1.4474, val acc: 0.3275  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 9120] train loss: 1.4489, train acc: 0.3540, val loss: 1.4474, val acc: 0.3275  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 9140] train loss: 1.4484, train acc: 0.3551, val loss: 1.4473, val acc: 0.3282  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 9160] train loss: 1.4483, train acc: 0.3563, val loss: 1.4472, val acc: 0.3285  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 9180] train loss: 1.4477, train acc: 0.3554, val loss: 1.4470, val acc: 0.3282  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 9200] train loss: 1.4497, train acc: 0.3546, val loss: 1.4470, val acc: 0.3285  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 9220] train loss: 1.4477, train acc: 0.3557, val loss: 1.4469, val acc: 0.3292  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 9240] train loss: 1.4470, train acc: 0.3594, val loss: 1.4466, val acc: 0.3288  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 9260] train loss: 1.4460, train acc: 0.3576, val loss: 1.4465, val acc: 0.3288  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 9280] train loss: 1.4476, train acc: 0.3555, val loss: 1.4465, val acc: 0.3288  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 9300] train loss: 1.4477, train acc: 0.3561, val loss: 1.4461, val acc: 0.3288  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 9320] train loss: 1.4455, train acc: 0.3598, val loss: 1.4460, val acc: 0.3292  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 9340] train loss: 1.4453, train acc: 0.3556, val loss: 1.4459, val acc: 0.3288  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 9360] train loss: 1.4451, train acc: 0.3597, val loss: 1.4457, val acc: 0.3305  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 9380] train loss: 1.4443, train acc: 0.3561, val loss: 1.4454, val acc: 0.3298  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 9400] train loss: 1.4443, train acc: 0.3577, val loss: 1.4452, val acc: 0.3302  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 9420] train loss: 1.4439, train acc: 0.3615, val loss: 1.4450, val acc: 0.3305  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 9440] train loss: 1.4458, train acc: 0.3600, val loss: 1.4448, val acc: 0.3309  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 9460] train loss: 1.4434, train acc: 0.3596, val loss: 1.4449, val acc: 0.3312  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 9480] train loss: 1.4443, train acc: 0.3581, val loss: 1.4444, val acc: 0.3312  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 9500] train loss: 1.4434, train acc: 0.3605, val loss: 1.4440, val acc: 0.3339  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 9520] train loss: 1.4415, train acc: 0.3608, val loss: 1.4438, val acc: 0.3319  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 9540] train loss: 1.4424, train acc: 0.3634, val loss: 1.4434, val acc: 0.3336  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 9560] train loss: 1.4423, train acc: 0.3667, val loss: 1.4431, val acc: 0.3349  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 9580] train loss: 1.4422, train acc: 0.3662, val loss: 1.4428, val acc: 0.3366  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 9600] train loss: 1.4402, train acc: 0.3683, val loss: 1.4424, val acc: 0.3369  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 9620] train loss: 1.4401, train acc: 0.3707, val loss: 1.4421, val acc: 0.3393  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 9640] train loss: 1.4426, train acc: 0.3699, val loss: 1.4418, val acc: 0.3386  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 9660] train loss: 1.4410, train acc: 0.3726, val loss: 1.4414, val acc: 0.3403  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 9680] train loss: 1.4392, train acc: 0.3790, val loss: 1.4408, val acc: 0.3450  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 9700] train loss: 1.4369, train acc: 0.3784, val loss: 1.4405, val acc: 0.3457  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 9720] train loss: 1.4375, train acc: 0.3803, val loss: 1.4404, val acc: 0.3511  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 9740] train loss: 1.4389, train acc: 0.3822, val loss: 1.4397, val acc: 0.3491  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 9760] train loss: 1.4373, train acc: 0.3856, val loss: 1.4392, val acc: 0.3528  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 9780] train loss: 1.4353, train acc: 0.3877, val loss: 1.4384, val acc: 0.3568  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 9800] train loss: 1.4365, train acc: 0.3857, val loss: 1.4380, val acc: 0.3616  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 9820] train loss: 1.4330, train acc: 0.3929, val loss: 1.4380, val acc: 0.3642  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 9840] train loss: 1.4377, train acc: 0.3857, val loss: 1.4366, val acc: 0.3632  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 9860] train loss: 1.4342, train acc: 0.3904, val loss: 1.4360, val acc: 0.3669  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 9880] train loss: 1.4333, train acc: 0.3984, val loss: 1.4354, val acc: 0.3696  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 9900] train loss: 1.4337, train acc: 0.3941, val loss: 1.4355, val acc: 0.3717  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 9920] train loss: 1.4305, train acc: 0.3933, val loss: 1.4341, val acc: 0.3686  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 9940] train loss: 1.4307, train acc: 0.3948, val loss: 1.4331, val acc: 0.3710  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 9960] train loss: 1.4314, train acc: 0.3950, val loss: 1.4327, val acc: 0.3734  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 9980] train loss: 1.4271, train acc: 0.4011, val loss: 1.4328, val acc: 0.3723  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 10000] train loss: 1.4300, train acc: 0.3935, val loss: 1.4323, val acc: 0.3717  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 10020] train loss: 1.4274, train acc: 0.3934, val loss: 1.4305, val acc: 0.3727  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 10040] train loss: 1.4283, train acc: 0.3992, val loss: 1.4294, val acc: 0.3727  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 10060] train loss: 1.4275, train acc: 0.3989, val loss: 1.4295, val acc: 0.3744  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 10080] train loss: 1.4261, train acc: 0.3965, val loss: 1.4288, val acc: 0.3761  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 10100] train loss: 1.6022, train acc: 0.3176, val loss: 1.5298, val acc: 0.3352  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 10120] train loss: 1.4396, train acc: 0.3475, val loss: 1.4316, val acc: 0.3592  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 10140] train loss: 1.4368, train acc: 0.3510, val loss: 1.4291, val acc: 0.3612  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 10160] train loss: 1.4336, train acc: 0.3506, val loss: 1.4288, val acc: 0.3612  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 10180] train loss: 1.4341, train acc: 0.3535, val loss: 1.4254, val acc: 0.3612  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 10200] train loss: 1.4271, train acc: 0.3536, val loss: 1.4221, val acc: 0.3612  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 10220] train loss: 1.4289, train acc: 0.3537, val loss: 1.4206, val acc: 0.3612  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 10240] train loss: 1.4267, train acc: 0.3538, val loss: 1.4201, val acc: 0.3616  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 10260] train loss: 1.4274, train acc: 0.3520, val loss: 1.4190, val acc: 0.3619  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 10280] train loss: 1.4274, train acc: 0.3542, val loss: 1.4181, val acc: 0.3616  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 10300] train loss: 1.4254, train acc: 0.3543, val loss: 1.4169, val acc: 0.3616  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 10320] train loss: 1.4270, train acc: 0.3533, val loss: 1.4162, val acc: 0.3616  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 10340] train loss: 1.4246, train acc: 0.3527, val loss: 1.4155, val acc: 0.3612  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 10360] train loss: 1.4243, train acc: 0.3529, val loss: 1.4147, val acc: 0.3622  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 10380] train loss: 1.4261, train acc: 0.3529, val loss: 1.4140, val acc: 0.3622  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 10400] train loss: 1.4269, train acc: 0.3522, val loss: 1.4144, val acc: 0.3629  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 10420] train loss: 1.4242, train acc: 0.3532, val loss: 1.4130, val acc: 0.3619  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 10440] train loss: 1.4233, train acc: 0.3533, val loss: 1.4127, val acc: 0.3622  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 10460] train loss: 1.4187, train acc: 0.3555, val loss: 1.4121, val acc: 0.3622  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 10480] train loss: 1.4217, train acc: 0.3541, val loss: 1.4118, val acc: 0.3622  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 10500] train loss: 1.4200, train acc: 0.3535, val loss: 1.4115, val acc: 0.3622  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 10520] train loss: 1.4221, train acc: 0.3525, val loss: 1.4113, val acc: 0.3622  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 10540] train loss: 1.4213, train acc: 0.3540, val loss: 1.4110, val acc: 0.3622  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 10560] train loss: 1.4239, train acc: 0.3539, val loss: 1.4109, val acc: 0.3626  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 10580] train loss: 1.4204, train acc: 0.3551, val loss: 1.4112, val acc: 0.3629  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 10600] train loss: 1.4219, train acc: 0.3553, val loss: 1.4109, val acc: 0.3629  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 10620] train loss: 1.4184, train acc: 0.3565, val loss: 1.4103, val acc: 0.3626  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 10640] train loss: 1.4224, train acc: 0.3563, val loss: 1.4107, val acc: 0.3626  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 10660] train loss: 1.4221, train acc: 0.3561, val loss: 1.4105, val acc: 0.3616  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 10680] train loss: 1.4225, train acc: 0.3548, val loss: 1.4104, val acc: 0.3626  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 10700] train loss: 1.4216, train acc: 0.3561, val loss: 1.4101, val acc: 0.3626  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 10720] train loss: 1.4191, train acc: 0.3567, val loss: 1.4102, val acc: 0.3619  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 10740] train loss: 1.4255, train acc: 0.3540, val loss: 1.4098, val acc: 0.3626  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 10760] train loss: 1.4216, train acc: 0.3544, val loss: 1.4097, val acc: 0.3622  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 10780] train loss: 1.4201, train acc: 0.3559, val loss: 1.4098, val acc: 0.3619  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 10800] train loss: 1.4230, train acc: 0.3558, val loss: 1.4097, val acc: 0.3629  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 10820] train loss: 1.4208, train acc: 0.3551, val loss: 1.4104, val acc: 0.3619  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 10840] train loss: 1.4225, train acc: 0.3553, val loss: 1.4106, val acc: 0.3619  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 10860] train loss: 1.4205, train acc: 0.3571, val loss: 1.4095, val acc: 0.3622  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 10880] train loss: 1.4209, train acc: 0.3583, val loss: 1.4108, val acc: 0.3616  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 10900] train loss: 1.4196, train acc: 0.3565, val loss: 1.4121, val acc: 0.3619  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 10920] train loss: 1.4161, train acc: 0.3567, val loss: 1.4100, val acc: 0.3612  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 10940] train loss: 1.4206, train acc: 0.3570, val loss: 1.4096, val acc: 0.3622  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 10960] train loss: 1.4211, train acc: 0.3563, val loss: 1.4093, val acc: 0.3609  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 10980] train loss: 1.4171, train acc: 0.3568, val loss: 1.4093, val acc: 0.3619  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 11000] train loss: 1.4167, train acc: 0.3574, val loss: 1.4091, val acc: 0.3622  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 11020] train loss: 1.4177, train acc: 0.3587, val loss: 1.4090, val acc: 0.3622  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 11040] train loss: 1.4182, train acc: 0.3582, val loss: 1.4083, val acc: 0.3619  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 11060] train loss: 1.4213, train acc: 0.3723, val loss: 1.4088, val acc: 0.3605  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 11080] train loss: 1.4189, train acc: 0.3720, val loss: 1.4082, val acc: 0.3622  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 11100] train loss: 1.4204, train acc: 0.3702, val loss: 1.4083, val acc: 0.3619  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 11120] train loss: 1.4206, train acc: 0.3718, val loss: 1.4087, val acc: 0.3605  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 11140] train loss: 1.4189, train acc: 0.3757, val loss: 1.4087, val acc: 0.3609  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 11160] train loss: 1.4160, train acc: 0.3743, val loss: 1.4080, val acc: 0.3619  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 11180] train loss: 1.4195, train acc: 0.3728, val loss: 1.4082, val acc: 0.3619  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 11200] train loss: 1.4195, train acc: 0.3718, val loss: 1.4077, val acc: 0.3622  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 11220] train loss: 1.4173, train acc: 0.3738, val loss: 1.4074, val acc: 0.3622  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 11240] train loss: 1.4162, train acc: 0.3735, val loss: 1.4076, val acc: 0.3622  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 11260] train loss: 1.4168, train acc: 0.3724, val loss: 1.4076, val acc: 0.3616  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 11280] train loss: 1.4191, train acc: 0.3721, val loss: 1.4078, val acc: 0.3616  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 11300] train loss: 1.4188, train acc: 0.3710, val loss: 1.4074, val acc: 0.3612  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 11320] train loss: 1.4153, train acc: 0.3735, val loss: 1.4074, val acc: 0.3609  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 11340] train loss: 1.4150, train acc: 0.3743, val loss: 1.4068, val acc: 0.3619  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 11360] train loss: 1.4184, train acc: 0.3715, val loss: 1.4049, val acc: 0.3609  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 11380] train loss: 1.4117, train acc: 0.3725, val loss: 1.4036, val acc: 0.3609  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 11400] train loss: 1.4228, train acc: 0.3702, val loss: 1.4100, val acc: 0.3622  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 11420] train loss: 1.5429, train acc: 0.2592, val loss: 1.5362, val acc: 0.2388  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 11440] train loss: 1.5253, train acc: 0.2671, val loss: 1.5198, val acc: 0.2388  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 11460] train loss: 1.5241, train acc: 0.2269, val loss: 1.5186, val acc: 0.2293  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 11480] train loss: 1.5206, train acc: 0.2619, val loss: 1.5177, val acc: 0.2384  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 11500] train loss: 1.5195, train acc: 0.2701, val loss: 1.5166, val acc: 0.2648  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 11520] train loss: 1.5187, train acc: 0.2462, val loss: 1.5156, val acc: 0.2806  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 11540] train loss: 1.5173, train acc: 0.2497, val loss: 1.5155, val acc: 0.2712  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 11560] train loss: 1.5173, train acc: 0.2467, val loss: 1.5158, val acc: 0.2594  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 11580] train loss: 1.5162, train acc: 0.2582, val loss: 1.5150, val acc: 0.2573  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 11600] train loss: 1.5162, train acc: 0.2640, val loss: 1.5146, val acc: 0.2678  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 11620] train loss: 1.5158, train acc: 0.2731, val loss: 1.5140, val acc: 0.2735  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 11640] train loss: 1.5147, train acc: 0.2741, val loss: 1.5130, val acc: 0.2786  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 11660] train loss: 1.5140, train acc: 0.2795, val loss: 1.5122, val acc: 0.2799  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 11680] train loss: 1.5118, train acc: 0.2788, val loss: 1.5106, val acc: 0.2702  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 11700] train loss: 1.5100, train acc: 0.2880, val loss: 1.5082, val acc: 0.2816  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 11720] train loss: 1.5056, train acc: 0.2981, val loss: 1.5043, val acc: 0.2934  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 11740] train loss: 1.4984, train acc: 0.3167, val loss: 1.4973, val acc: 0.3029  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 11760] train loss: 1.4903, train acc: 0.3223, val loss: 1.4861, val acc: 0.3137  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 11780] train loss: 1.4819, train acc: 0.3188, val loss: 1.4739, val acc: 0.3147  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 11800] train loss: 1.4769, train acc: 0.3348, val loss: 1.4652, val acc: 0.3255  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 11820] train loss: 1.4731, train acc: 0.3308, val loss: 1.4598, val acc: 0.3292  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 11840] train loss: 1.4688, train acc: 0.3292, val loss: 1.4543, val acc: 0.3241  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 11860] train loss: 1.4654, train acc: 0.3298, val loss: 1.4506, val acc: 0.3282  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 11880] train loss: 1.4590, train acc: 0.3320, val loss: 1.4468, val acc: 0.3265  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 11900] train loss: 1.4559, train acc: 0.3277, val loss: 1.4438, val acc: 0.3221  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 11920] train loss: 1.4510, train acc: 0.3286, val loss: 1.4406, val acc: 0.3295  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 11940] train loss: 1.4497, train acc: 0.3307, val loss: 1.4378, val acc: 0.3231  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 11960] train loss: 1.4470, train acc: 0.3287, val loss: 1.4345, val acc: 0.3066  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 11980] train loss: 1.4422, train acc: 0.3470, val loss: 1.4306, val acc: 0.3491  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 12000] train loss: 1.4397, train acc: 0.3708, val loss: 1.4279, val acc: 0.3757  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 12020] train loss: 1.4390, train acc: 0.3812, val loss: 1.4260, val acc: 0.3838  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 12040] train loss: 1.4340, train acc: 0.3830, val loss: 1.4211, val acc: 0.3862  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 12060] train loss: 1.4310, train acc: 0.3570, val loss: 1.4188, val acc: 0.3841  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 12080] train loss: 1.4358, train acc: 0.3516, val loss: 1.4178, val acc: 0.3875  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 12100] train loss: 1.4342, train acc: 0.3538, val loss: 1.4151, val acc: 0.3862  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 12120] train loss: 1.4331, train acc: 0.3837, val loss: 1.4133, val acc: 0.3899  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 12140] train loss: 1.4294, train acc: 0.3847, val loss: 1.4109, val acc: 0.3895  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 12160] train loss: 1.4301, train acc: 0.3817, val loss: 1.4092, val acc: 0.3906  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 12180] train loss: 1.4288, train acc: 0.3795, val loss: 1.4081, val acc: 0.3909  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 12200] train loss: 1.4275, train acc: 0.3808, val loss: 1.4086, val acc: 0.3919  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 12220] train loss: 1.4278, train acc: 0.3833, val loss: 1.4064, val acc: 0.3906  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 12240] train loss: 1.4243, train acc: 0.3799, val loss: 1.4049, val acc: 0.3895  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 12260] train loss: 1.4257, train acc: 0.3810, val loss: 1.4037, val acc: 0.3909  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 12280] train loss: 1.4263, train acc: 0.3791, val loss: 1.4012, val acc: 0.3865  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 12300] train loss: 1.4190, train acc: 0.3742, val loss: 1.3982, val acc: 0.3892  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 12320] train loss: 1.4169, train acc: 0.3817, val loss: 1.3957, val acc: 0.3899  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 12340] train loss: 1.4135, train acc: 0.3827, val loss: 1.3923, val acc: 0.3892  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 12360] train loss: 1.4131, train acc: 0.3842, val loss: 1.3894, val acc: 0.3926  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 12380] train loss: 1.4144, train acc: 0.3852, val loss: 1.3874, val acc: 0.3946  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 12400] train loss: 1.4108, train acc: 0.3834, val loss: 1.3855, val acc: 0.3933  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 12420] train loss: 1.4089, train acc: 0.3910, val loss: 1.3849, val acc: 0.3973  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 12440] train loss: 1.4116, train acc: 0.3879, val loss: 1.3836, val acc: 0.3946  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 12460] train loss: 1.4068, train acc: 0.3854, val loss: 1.3826, val acc: 0.3970  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 12480] train loss: 1.4071, train acc: 0.3905, val loss: 1.3818, val acc: 0.3966  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 12500] train loss: 1.4046, train acc: 0.3926, val loss: 1.3806, val acc: 0.3949  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 12520] train loss: 1.4055, train acc: 0.3873, val loss: 1.3803, val acc: 0.3939  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 12540] train loss: 1.4040, train acc: 0.3901, val loss: 1.3797, val acc: 0.3926  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 12560] train loss: 1.4052, train acc: 0.3871, val loss: 1.3787, val acc: 0.3983  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 12580] train loss: 1.4038, train acc: 0.3942, val loss: 1.3790, val acc: 0.4000  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 12600] train loss: 1.4067, train acc: 0.3913, val loss: 1.3788, val acc: 0.3997  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 12620] train loss: 1.4021, train acc: 0.3853, val loss: 1.3778, val acc: 0.4000  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 12640] train loss: 1.4040, train acc: 0.3869, val loss: 1.3776, val acc: 0.4000  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 12660] train loss: 1.4024, train acc: 0.3922, val loss: 1.3806, val acc: 0.4044  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 12680] train loss: 1.4027, train acc: 0.3919, val loss: 1.3768, val acc: 0.4024  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 12700] train loss: 1.4043, train acc: 0.3895, val loss: 1.3774, val acc: 0.4037  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 12720] train loss: 1.4031, train acc: 0.3913, val loss: 1.3769, val acc: 0.4067  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 12740] train loss: 1.4056, train acc: 0.3883, val loss: 1.3774, val acc: 0.4044  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 12760] train loss: 1.4036, train acc: 0.3885, val loss: 1.3775, val acc: 0.4064  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 12780] train loss: 1.3981, train acc: 0.3904, val loss: 1.3762, val acc: 0.4067  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 12800] train loss: 1.4025, train acc: 0.3889, val loss: 1.3775, val acc: 0.4047  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 12820] train loss: 1.4035, train acc: 0.3937, val loss: 1.3775, val acc: 0.4064  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 12840] train loss: 1.4027, train acc: 0.3902, val loss: 1.3771, val acc: 0.4057  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 12860] train loss: 1.4000, train acc: 0.3872, val loss: 1.3770, val acc: 0.4037  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 12880] train loss: 1.4042, train acc: 0.3911, val loss: 1.3763, val acc: 0.4047  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 12900] train loss: 1.4008, train acc: 0.3913, val loss: 1.3772, val acc: 0.4071  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 12920] train loss: 1.3998, train acc: 0.3913, val loss: 1.3765, val acc: 0.4071  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 12940] train loss: 1.4005, train acc: 0.3890, val loss: 1.3765, val acc: 0.4047  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 12960] train loss: 1.4034, train acc: 0.3929, val loss: 1.3763, val acc: 0.4064  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 12980] train loss: 1.4024, train acc: 0.3881, val loss: 1.3766, val acc: 0.4037  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 13000] train loss: 1.4017, train acc: 0.3929, val loss: 1.3767, val acc: 0.4074  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 13020] train loss: 1.4043, train acc: 0.3931, val loss: 1.3764, val acc: 0.4054  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 13040] train loss: 1.4021, train acc: 0.3916, val loss: 1.3766, val acc: 0.4044  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 13060] train loss: 1.3974, train acc: 0.3933, val loss: 1.3757, val acc: 0.4047  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 13080] train loss: 1.4038, train acc: 0.3902, val loss: 1.3759, val acc: 0.4044  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 13100] train loss: 1.3985, train acc: 0.3905, val loss: 1.3758, val acc: 0.4054  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 13120] train loss: 1.4010, train acc: 0.3903, val loss: 1.3760, val acc: 0.4061  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 13140] train loss: 1.4051, train acc: 0.3903, val loss: 1.3754, val acc: 0.4047  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 13160] train loss: 1.4024, train acc: 0.3915, val loss: 1.3759, val acc: 0.4027  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 13180] train loss: 1.4038, train acc: 0.3916, val loss: 1.3756, val acc: 0.4054  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 13200] train loss: 1.4035, train acc: 0.3903, val loss: 1.3755, val acc: 0.4057  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 13220] train loss: 1.4024, train acc: 0.3926, val loss: 1.3757, val acc: 0.4044  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 13240] train loss: 1.4017, train acc: 0.3941, val loss: 1.3759, val acc: 0.4047  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 13260] train loss: 1.3994, train acc: 0.3942, val loss: 1.3760, val acc: 0.4044  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 13280] train loss: 1.3994, train acc: 0.3926, val loss: 1.3758, val acc: 0.4047  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 13300] train loss: 1.3963, train acc: 0.3927, val loss: 1.3752, val acc: 0.4037  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 13320] train loss: 1.4012, train acc: 0.3913, val loss: 1.3750, val acc: 0.4051  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 13340] train loss: 1.4009, train acc: 0.3929, val loss: 1.3755, val acc: 0.4071  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 13360] train loss: 1.3999, train acc: 0.3936, val loss: 1.3758, val acc: 0.4054  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 13380] train loss: 1.4007, train acc: 0.3938, val loss: 1.3751, val acc: 0.4044  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 13400] train loss: 1.4025, train acc: 0.3947, val loss: 1.3755, val acc: 0.4057  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 13420] train loss: 1.3981, train acc: 0.3937, val loss: 1.3748, val acc: 0.4078  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 13440] train loss: 1.4003, train acc: 0.3918, val loss: 1.3751, val acc: 0.4054  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 13460] train loss: 1.4004, train acc: 0.3926, val loss: 1.3745, val acc: 0.4078  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 13480] train loss: 1.3981, train acc: 0.3900, val loss: 1.3743, val acc: 0.4074  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 13500] train loss: 1.3998, train acc: 0.3921, val loss: 1.3750, val acc: 0.4057  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 13520] train loss: 1.4007, train acc: 0.3926, val loss: 1.3744, val acc: 0.4061  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 13540] train loss: 1.4028, train acc: 0.3935, val loss: 1.3745, val acc: 0.4071  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 13560] train loss: 1.4026, train acc: 0.3930, val loss: 1.3743, val acc: 0.4054  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 13580] train loss: 1.3976, train acc: 0.3942, val loss: 1.3746, val acc: 0.4057  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 13600] train loss: 1.4018, train acc: 0.3939, val loss: 1.3746, val acc: 0.4034  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 13620] train loss: 1.4045, train acc: 0.3869, val loss: 1.3748, val acc: 0.4027  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 13640] train loss: 1.3995, train acc: 0.3879, val loss: 1.3750, val acc: 0.4061  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 13660] train loss: 1.4025, train acc: 0.3936, val loss: 1.3752, val acc: 0.4057  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 13680] train loss: 1.3992, train acc: 0.3935, val loss: 1.3755, val acc: 0.4054  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 13700] train loss: 1.3983, train acc: 0.3913, val loss: 1.3744, val acc: 0.4078  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 13720] train loss: 1.4004, train acc: 0.3930, val loss: 1.3747, val acc: 0.4081  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 13740] train loss: 1.3974, train acc: 0.3965, val loss: 1.3738, val acc: 0.4067  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 13760] train loss: 1.4013, train acc: 0.3951, val loss: 1.3749, val acc: 0.4074  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 13780] train loss: 1.4001, train acc: 0.3955, val loss: 1.3745, val acc: 0.4084  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 13800] train loss: 1.4017, train acc: 0.3918, val loss: 1.3745, val acc: 0.4074  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 13820] train loss: 1.4014, train acc: 0.3952, val loss: 1.3751, val acc: 0.4071  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 13840] train loss: 1.4016, train acc: 0.3960, val loss: 1.3747, val acc: 0.4078  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 13860] train loss: 1.4025, train acc: 0.3930, val loss: 1.3752, val acc: 0.4071  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 13880] train loss: 1.3975, train acc: 0.3943, val loss: 1.3747, val acc: 0.4084  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 13900] train loss: 1.3980, train acc: 0.3953, val loss: 1.3747, val acc: 0.4088  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 13920] train loss: 1.4017, train acc: 0.3922, val loss: 1.3742, val acc: 0.4081  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 13940] train loss: 1.4032, train acc: 0.3909, val loss: 1.3753, val acc: 0.4054  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 13960] train loss: 1.3990, train acc: 0.3907, val loss: 1.3741, val acc: 0.4061  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 13980] train loss: 1.4010, train acc: 0.3950, val loss: 1.3757, val acc: 0.4084  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 14000] train loss: 1.4006, train acc: 0.3966, val loss: 1.3745, val acc: 0.4111  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 14020] train loss: 1.3994, train acc: 0.3960, val loss: 1.3738, val acc: 0.4094  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 14040] train loss: 1.3979, train acc: 0.3976, val loss: 1.3742, val acc: 0.4051  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 14060] train loss: 1.4003, train acc: 0.3926, val loss: 1.3748, val acc: 0.3987  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 14080] train loss: 1.4049, train acc: 0.3916, val loss: 1.3741, val acc: 0.4128  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 14100] train loss: 1.4004, train acc: 0.3974, val loss: 1.3738, val acc: 0.4135  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 14120] train loss: 1.3991, train acc: 0.3924, val loss: 1.3737, val acc: 0.4098  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 14140] train loss: 1.3989, train acc: 0.3954, val loss: 1.3745, val acc: 0.4115  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 14160] train loss: 1.4013, train acc: 0.3912, val loss: 1.3736, val acc: 0.4037  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 14180] train loss: 1.4010, train acc: 0.3945, val loss: 1.3733, val acc: 0.4108  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 14200] train loss: 1.4002, train acc: 0.3940, val loss: 1.3732, val acc: 0.4118  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 14220] train loss: 1.4015, train acc: 0.3963, val loss: 1.3734, val acc: 0.4034  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 14240] train loss: 1.3980, train acc: 0.3883, val loss: 1.3741, val acc: 0.4057  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 14260] train loss: 1.3976, train acc: 0.3960, val loss: 1.3740, val acc: 0.4098  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 14280] train loss: 1.3967, train acc: 0.3910, val loss: 1.3737, val acc: 0.4078  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 14300] train loss: 1.4015, train acc: 0.3938, val loss: 1.3739, val acc: 0.4067  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 14320] train loss: 1.4006, train acc: 0.3955, val loss: 1.3733, val acc: 0.4105  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 14340] train loss: 1.3980, train acc: 0.3957, val loss: 1.3731, val acc: 0.4088  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 14360] train loss: 1.4000, train acc: 0.3929, val loss: 1.3735, val acc: 0.4054  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 14380] train loss: 1.3998, train acc: 0.3931, val loss: 1.3733, val acc: 0.4078  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 14400] train loss: 1.3973, train acc: 0.3948, val loss: 1.3731, val acc: 0.4013  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 14420] train loss: 1.3996, train acc: 0.3946, val loss: 1.3733, val acc: 0.4027  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 14440] train loss: 1.3971, train acc: 0.3931, val loss: 1.3732, val acc: 0.4078  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 14460] train loss: 1.3981, train acc: 0.3951, val loss: 1.3725, val acc: 0.4111  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 14480] train loss: 1.3934, train acc: 0.3962, val loss: 1.3730, val acc: 0.4078  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 14500] train loss: 1.3981, train acc: 0.3952, val loss: 1.3731, val acc: 0.4044  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 14520] train loss: 1.3976, train acc: 0.3952, val loss: 1.3725, val acc: 0.4101  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 14540] train loss: 1.3941, train acc: 0.3951, val loss: 1.3724, val acc: 0.4071  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 14560] train loss: 1.3969, train acc: 0.3959, val loss: 1.3733, val acc: 0.4051  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 14580] train loss: 1.3956, train acc: 0.3929, val loss: 1.3728, val acc: 0.4030  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 14600] train loss: 1.3968, train acc: 0.3916, val loss: 1.3722, val acc: 0.4074  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 14620] train loss: 1.3961, train acc: 0.3984, val loss: 1.3723, val acc: 0.4047  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 14640] train loss: 1.3985, train acc: 0.3945, val loss: 1.3733, val acc: 0.4030  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 14660] train loss: 1.3949, train acc: 0.3994, val loss: 1.3717, val acc: 0.4047  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 14680] train loss: 1.3971, train acc: 0.3978, val loss: 1.3723, val acc: 0.4088  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 14700] train loss: 1.3975, train acc: 0.3984, val loss: 1.3723, val acc: 0.4051  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 14720] train loss: 1.3971, train acc: 0.3960, val loss: 1.3725, val acc: 0.4034  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 14740] train loss: 1.3981, train acc: 0.3947, val loss: 1.3721, val acc: 0.4034  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 14760] train loss: 1.3978, train acc: 0.3886, val loss: 1.3717, val acc: 0.4047  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 14780] train loss: 1.3948, train acc: 0.3973, val loss: 1.3721, val acc: 0.4030  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 14800] train loss: 1.3954, train acc: 0.3972, val loss: 1.3716, val acc: 0.4020  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 14820] train loss: 1.3963, train acc: 0.3916, val loss: 1.3719, val acc: 0.4061  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 14840] train loss: 1.3987, train acc: 0.3941, val loss: 1.3720, val acc: 0.4081  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 14860] train loss: 1.3947, train acc: 0.3980, val loss: 1.3720, val acc: 0.4051  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 14880] train loss: 1.3973, train acc: 0.3951, val loss: 1.3724, val acc: 0.4047  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 14900] train loss: 1.4000, train acc: 0.3952, val loss: 1.3714, val acc: 0.4084  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 14920] train loss: 1.3965, train acc: 0.3944, val loss: 1.3725, val acc: 0.4071  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 14940] train loss: 1.3974, train acc: 0.3940, val loss: 1.3724, val acc: 0.4098  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 14960] train loss: 1.3980, train acc: 0.3975, val loss: 1.3714, val acc: 0.4111  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 14980] train loss: 1.3978, train acc: 0.3946, val loss: 1.3714, val acc: 0.4091  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 15000] train loss: 1.3974, train acc: 0.3957, val loss: 1.3723, val acc: 0.4121  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 15020] train loss: 1.3959, train acc: 0.3947, val loss: 1.3710, val acc: 0.4074  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 15040] train loss: 1.4034, train acc: 0.3853, val loss: 1.3715, val acc: 0.4091  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 15060] train loss: 1.3964, train acc: 0.3969, val loss: 1.3703, val acc: 0.4105  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 15080] train loss: 1.3978, train acc: 0.3925, val loss: 1.3709, val acc: 0.4098  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 15100] train loss: 1.3991, train acc: 0.3937, val loss: 1.3703, val acc: 0.4108  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 15120] train loss: 1.3924, train acc: 0.3971, val loss: 1.3709, val acc: 0.4064  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 15140] train loss: 1.3932, train acc: 0.3974, val loss: 1.3702, val acc: 0.4101  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 15160] train loss: 1.3988, train acc: 0.3919, val loss: 1.3701, val acc: 0.4105  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 15180] train loss: 1.3957, train acc: 0.3952, val loss: 1.3705, val acc: 0.4044  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 15200] train loss: 1.3953, train acc: 0.3965, val loss: 1.3701, val acc: 0.4135  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 15220] train loss: 1.3982, train acc: 0.3888, val loss: 1.3711, val acc: 0.4074  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 15240] train loss: 1.3956, train acc: 0.3973, val loss: 1.3703, val acc: 0.4074  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 15260] train loss: 1.3974, train acc: 0.3986, val loss: 1.3698, val acc: 0.4108  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 15280] train loss: 1.3948, train acc: 0.3950, val loss: 1.3710, val acc: 0.4118  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 15300] train loss: 1.3956, train acc: 0.3963, val loss: 1.3701, val acc: 0.4071  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 15320] train loss: 1.3942, train acc: 0.3939, val loss: 1.3705, val acc: 0.4084  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 15340] train loss: 1.3986, train acc: 0.3974, val loss: 1.3695, val acc: 0.4088  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 15360] train loss: 1.3994, train acc: 0.3932, val loss: 1.3709, val acc: 0.4091  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 15380] train loss: 1.3953, train acc: 0.3978, val loss: 1.3696, val acc: 0.4054  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 15400] train loss: 1.3973, train acc: 0.3941, val loss: 1.3691, val acc: 0.4091  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 15420] train loss: 1.3967, train acc: 0.3928, val loss: 1.3708, val acc: 0.4108  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 15440] train loss: 1.3959, train acc: 0.3981, val loss: 1.3695, val acc: 0.4105  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 15460] train loss: 1.3961, train acc: 0.3966, val loss: 1.3695, val acc: 0.4067  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 15480] train loss: 1.3989, train acc: 0.3986, val loss: 1.3692, val acc: 0.4084  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 15500] train loss: 1.3968, train acc: 0.3974, val loss: 1.3683, val acc: 0.4037  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 15520] train loss: 1.3962, train acc: 0.3986, val loss: 1.3692, val acc: 0.4084  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 15540] train loss: 1.3982, train acc: 0.3961, val loss: 1.3687, val acc: 0.4101  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 15560] train loss: 1.3962, train acc: 0.3916, val loss: 1.3684, val acc: 0.4034  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 15580] train loss: 1.3983, train acc: 0.3978, val loss: 1.3687, val acc: 0.4101  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 15600] train loss: 1.3964, train acc: 0.3957, val loss: 1.3684, val acc: 0.3949  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 15620] train loss: 1.3938, train acc: 0.3957, val loss: 1.3674, val acc: 0.4074  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 15640] train loss: 1.3926, train acc: 0.3964, val loss: 1.3661, val acc: 0.4057  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 15660] train loss: 1.3924, train acc: 0.3809, val loss: 1.3666, val acc: 0.3997  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 15680] train loss: 1.3958, train acc: 0.3776, val loss: 1.3664, val acc: 0.4064  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 15700] train loss: 1.3959, train acc: 0.3804, val loss: 1.3665, val acc: 0.4071  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 15720] train loss: 1.3948, train acc: 0.3782, val loss: 1.3672, val acc: 0.4078  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 15740] train loss: 1.3946, train acc: 0.3793, val loss: 1.3662, val acc: 0.4067  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 15760] train loss: 1.3979, train acc: 0.3872, val loss: 1.3696, val acc: 0.4000  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 15780] train loss: 1.3953, train acc: 0.3744, val loss: 1.3705, val acc: 0.4081  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 15800] train loss: 1.3934, train acc: 0.3755, val loss: 1.3669, val acc: 0.4064  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 15820] train loss: 1.3965, train acc: 0.3968, val loss: 1.3659, val acc: 0.4067  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 15840] train loss: 1.3954, train acc: 0.3978, val loss: 1.3661, val acc: 0.4054  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 15860] train loss: 1.3927, train acc: 0.3939, val loss: 1.3661, val acc: 0.4051  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 15880] train loss: 1.3923, train acc: 0.3994, val loss: 1.3667, val acc: 0.4061  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 15900] train loss: 1.3913, train acc: 0.3992, val loss: 1.3652, val acc: 0.4084  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 15920] train loss: 1.3894, train acc: 0.3979, val loss: 1.3657, val acc: 0.4061  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 15940] train loss: 1.3919, train acc: 0.3987, val loss: 1.3650, val acc: 0.4094  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 15960] train loss: 1.3952, train acc: 0.3947, val loss: 1.3661, val acc: 0.4037  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 15980] train loss: 1.3935, train acc: 0.3797, val loss: 1.3670, val acc: 0.3970  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 16000] train loss: 1.3936, train acc: 0.3986, val loss: 1.3658, val acc: 0.4101  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 16020] train loss: 1.3942, train acc: 0.3958, val loss: 1.3656, val acc: 0.4081  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 16040] train loss: 1.3880, train acc: 0.3787, val loss: 1.3667, val acc: 0.4013  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 16060] train loss: 1.3965, train acc: 0.3786, val loss: 1.3655, val acc: 0.4020  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 16080] train loss: 1.3888, train acc: 0.3780, val loss: 1.3655, val acc: 0.4024  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 16100] train loss: 1.3915, train acc: 0.3875, val loss: 1.3683, val acc: 0.4128  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 16120] train loss: 1.4011, train acc: 0.3754, val loss: 1.3763, val acc: 0.4057  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 16140] train loss: 1.3980, train acc: 0.3665, val loss: 1.3670, val acc: 0.4000  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 16160] train loss: 1.3939, train acc: 0.3755, val loss: 1.3664, val acc: 0.4098  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 16180] train loss: 1.3923, train acc: 0.3736, val loss: 1.3665, val acc: 0.3990  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 16200] train loss: 1.3936, train acc: 0.3775, val loss: 1.3687, val acc: 0.3926  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 16220] train loss: 1.3911, train acc: 0.3801, val loss: 1.3658, val acc: 0.3926  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 16240] train loss: 1.3900, train acc: 0.3822, val loss: 1.3643, val acc: 0.3993  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 16260] train loss: 1.3900, train acc: 0.3835, val loss: 1.3651, val acc: 0.3936  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 16280] train loss: 1.3891, train acc: 0.3851, val loss: 1.3640, val acc: 0.4027  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 16300] train loss: 1.3848, train acc: 0.4072, val loss: 1.3744, val acc: 0.3939  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 16320] train loss: 1.3919, train acc: 0.3792, val loss: 1.3653, val acc: 0.3987  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 16340] train loss: 1.3935, train acc: 0.4007, val loss: 1.3635, val acc: 0.4010  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 16360] train loss: 1.3895, train acc: 0.4049, val loss: 1.3632, val acc: 0.3990  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 16380] train loss: 1.3905, train acc: 0.4017, val loss: 1.3630, val acc: 0.4047  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 16400] train loss: 1.3913, train acc: 0.4033, val loss: 1.3641, val acc: 0.3993  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 16420] train loss: 1.3905, train acc: 0.4064, val loss: 1.3691, val acc: 0.3929  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 16440] train loss: 1.3924, train acc: 0.3736, val loss: 1.3638, val acc: 0.4040  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 16460] train loss: 1.3890, train acc: 0.3790, val loss: 1.3621, val acc: 0.3949  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 16480] train loss: 1.3941, train acc: 0.3855, val loss: 1.3711, val acc: 0.4010  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 16500] train loss: 1.3898, train acc: 0.3847, val loss: 1.3680, val acc: 0.3976  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 16520] train loss: 1.3910, train acc: 0.3817, val loss: 1.3664, val acc: 0.3858  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 16540] train loss: 1.3816, train acc: 0.3864, val loss: 1.3640, val acc: 0.3926  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 16560] train loss: 1.3789, train acc: 0.3846, val loss: 1.3628, val acc: 0.3895  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 16580] train loss: 1.3803, train acc: 0.3861, val loss: 1.3622, val acc: 0.3912  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 16600] train loss: 1.3828, train acc: 0.3793, val loss: 1.3603, val acc: 0.3933  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 16620] train loss: 1.3852, train acc: 0.3796, val loss: 1.3610, val acc: 0.3889  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 16640] train loss: 1.3795, train acc: 0.3837, val loss: 1.3610, val acc: 0.3929  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 16660] train loss: 1.3803, train acc: 0.3840, val loss: 1.3623, val acc: 0.3946  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 16680] train loss: 1.3818, train acc: 0.3793, val loss: 1.3626, val acc: 0.3926  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 16700] train loss: 1.3802, train acc: 0.3914, val loss: 1.3609, val acc: 0.3993  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 16720] train loss: 1.3823, train acc: 0.3845, val loss: 1.3596, val acc: 0.4034  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 16740] train loss: 1.3853, train acc: 0.3870, val loss: 1.3596, val acc: 0.4040  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 16760] train loss: 1.3781, train acc: 0.3919, val loss: 1.3608, val acc: 0.3987  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 16780] train loss: 1.3991, train acc: 0.3850, val loss: 1.3731, val acc: 0.4040  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 16800] train loss: 1.3797, train acc: 0.3953, val loss: 1.3563, val acc: 0.4115  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 16820] train loss: 1.3798, train acc: 0.3982, val loss: 1.3520, val acc: 0.4078  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 16840] train loss: 1.3752, train acc: 0.4021, val loss: 1.3564, val acc: 0.4057  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 16860] train loss: 1.3747, train acc: 0.4043, val loss: 1.3495, val acc: 0.4162  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 16880] train loss: 1.3702, train acc: 0.4067, val loss: 1.3536, val acc: 0.4253  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 16900] train loss: 1.3795, train acc: 0.3991, val loss: 1.3611, val acc: 0.4125  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 16920] train loss: 1.3757, train acc: 0.4014, val loss: 1.3583, val acc: 0.4067  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3682  @ epoch 3660 )\n",
      "[Epoch: 16940] train loss: 1.3771, train acc: 0.4004, val loss: 1.3512, val acc: 0.4172  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3646  @ epoch 16934 )\n",
      "[Epoch: 16960] train loss: 1.3536, train acc: 0.4111, val loss: 1.3372, val acc: 0.4132  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3536  @ epoch 16960 )\n",
      "[Epoch: 16980] train loss: 1.3535, train acc: 0.4044, val loss: 1.3290, val acc: 0.4260  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3404  @ epoch 16970 )\n",
      "[Epoch: 17000] train loss: 1.3375, train acc: 0.4211, val loss: 1.3146, val acc: 0.4253  (best train acc: 0.4283, best val acc: 0.4445, best train loss: 1.3367  @ epoch 16999 )\n",
      "[Epoch: 17020] train loss: 1.3578, train acc: 0.4087, val loss: 1.3615, val acc: 0.4064  (best train acc: 0.4375, best val acc: 0.4742, best train loss: 1.3203  @ epoch 17019 )\n",
      "[Epoch: 17040] train loss: 1.4732, train acc: 0.3396, val loss: 1.4568, val acc: 0.3450  (best train acc: 0.4375, best val acc: 0.4742, best train loss: 1.3203  @ epoch 17019 )\n",
      "[Epoch: 17060] train loss: 1.4158, train acc: 0.3626, val loss: 1.4075, val acc: 0.3548  (best train acc: 0.4375, best val acc: 0.4742, best train loss: 1.3203  @ epoch 17019 )\n",
      "[Epoch: 17080] train loss: 1.3972, train acc: 0.3828, val loss: 1.3894, val acc: 0.3825  (best train acc: 0.4375, best val acc: 0.4742, best train loss: 1.3203  @ epoch 17019 )\n",
      "[Epoch: 17100] train loss: 1.3768, train acc: 0.3942, val loss: 1.3625, val acc: 0.3875  (best train acc: 0.4375, best val acc: 0.4742, best train loss: 1.3203  @ epoch 17019 )\n",
      "[Epoch: 17120] train loss: 1.3772, train acc: 0.3954, val loss: 1.3527, val acc: 0.4020  (best train acc: 0.4375, best val acc: 0.4742, best train loss: 1.3203  @ epoch 17019 )\n",
      "[Epoch: 17140] train loss: 1.3777, train acc: 0.3949, val loss: 1.3645, val acc: 0.4013  (best train acc: 0.4375, best val acc: 0.4742, best train loss: 1.3203  @ epoch 17019 )\n",
      "[Epoch: 17160] train loss: 1.3772, train acc: 0.3923, val loss: 1.3590, val acc: 0.3926  (best train acc: 0.4375, best val acc: 0.4742, best train loss: 1.3203  @ epoch 17019 )\n",
      "[Epoch: 17180] train loss: 1.3575, train acc: 0.4013, val loss: 1.3447, val acc: 0.3990  (best train acc: 0.4375, best val acc: 0.4742, best train loss: 1.3203  @ epoch 17019 )\n",
      "[Epoch: 17200] train loss: 1.3549, train acc: 0.3998, val loss: 1.3342, val acc: 0.4223  (best train acc: 0.4375, best val acc: 0.4742, best train loss: 1.3203  @ epoch 17019 )\n",
      "[Epoch: 17220] train loss: 1.3589, train acc: 0.4017, val loss: 1.3509, val acc: 0.3960  (best train acc: 0.4375, best val acc: 0.4742, best train loss: 1.3203  @ epoch 17019 )\n",
      "[Epoch: 17240] train loss: 1.3907, train acc: 0.3898, val loss: 1.3802, val acc: 0.3828  (best train acc: 0.4375, best val acc: 0.4742, best train loss: 1.3203  @ epoch 17019 )\n",
      "[Epoch: 17260] train loss: 1.3751, train acc: 0.3865, val loss: 1.3620, val acc: 0.3906  (best train acc: 0.4375, best val acc: 0.4742, best train loss: 1.3203  @ epoch 17019 )\n",
      "[Epoch: 17280] train loss: 1.3653, train acc: 0.3989, val loss: 1.3501, val acc: 0.4024  (best train acc: 0.4375, best val acc: 0.4742, best train loss: 1.3203  @ epoch 17019 )\n",
      "[Epoch: 17300] train loss: 1.3559, train acc: 0.4033, val loss: 1.3450, val acc: 0.4047  (best train acc: 0.4375, best val acc: 0.4742, best train loss: 1.3203  @ epoch 17019 )\n",
      "[Epoch: 17320] train loss: 1.3453, train acc: 0.4088, val loss: 1.3402, val acc: 0.4091  (best train acc: 0.4375, best val acc: 0.4742, best train loss: 1.3203  @ epoch 17019 )\n",
      "[Epoch: 17340] train loss: 1.3317, train acc: 0.4204, val loss: 1.3083, val acc: 0.4341  (best train acc: 0.4375, best val acc: 0.4742, best train loss: 1.3203  @ epoch 17019 )\n",
      "[Epoch: 17360] train loss: 1.3810, train acc: 0.3869, val loss: 1.3498, val acc: 0.3939  (best train acc: 0.4375, best val acc: 0.4742, best train loss: 1.3203  @ epoch 17019 )\n",
      "[Epoch: 17380] train loss: 1.3364, train acc: 0.4123, val loss: 1.3190, val acc: 0.4209  (best train acc: 0.4418, best val acc: 0.4742, best train loss: 1.3194  @ epoch 17376 )\n",
      "[Epoch: 17400] train loss: 1.3245, train acc: 0.4357, val loss: 1.2995, val acc: 0.4489  (best train acc: 0.4497, best val acc: 0.4742, best train loss: 1.3056  @ epoch 17392 )\n",
      "[Epoch: 17420] train loss: 1.3156, train acc: 0.4278, val loss: 1.2874, val acc: 0.4503  (best train acc: 0.4497, best val acc: 0.4742, best train loss: 1.3003  @ epoch 17405 )\n",
      "[Epoch: 17440] train loss: 1.2974, train acc: 0.4419, val loss: 1.2713, val acc: 0.4472  (best train acc: 0.4497, best val acc: 0.4742, best train loss: 1.2943  @ epoch 17438 )\n",
      "[Epoch: 17460] train loss: 1.2903, train acc: 0.4487, val loss: 1.2619, val acc: 0.4573  (best train acc: 0.4514, best val acc: 0.4742, best train loss: 1.2903  @ epoch 17460 )\n",
      "[Epoch: 17480] train loss: 1.2911, train acc: 0.4475, val loss: 1.2562, val acc: 0.4607  (best train acc: 0.4539, best val acc: 0.4742, best train loss: 1.2831  @ epoch 17479 )\n",
      "[Epoch: 17500] train loss: 1.2838, train acc: 0.4566, val loss: 1.2509, val acc: 0.4651  (best train acc: 0.4566, best val acc: 0.4742, best train loss: 1.2822  @ epoch 17493 )\n",
      "[Epoch: 17520] train loss: 1.2802, train acc: 0.4537, val loss: 1.2480, val acc: 0.4678  (best train acc: 0.4591, best val acc: 0.4742, best train loss: 1.2763  @ epoch 17516 )\n",
      "[Epoch: 17540] train loss: 1.2813, train acc: 0.4576, val loss: 1.2450, val acc: 0.4722  (best train acc: 0.4614, best val acc: 0.4742, best train loss: 1.2763  @ epoch 17516 )\n",
      "[Epoch: 17560] train loss: 1.2777, train acc: 0.4607, val loss: 1.2423, val acc: 0.4715  (best train acc: 0.4654, best val acc: 0.4793, best train loss: 1.2751  @ epoch 17554 )\n",
      "[Epoch: 17580] train loss: 1.2740, train acc: 0.4686, val loss: 1.2449, val acc: 0.4678  (best train acc: 0.4686, best val acc: 0.4793, best train loss: 1.2740  @ epoch 17580 )\n",
      "[Epoch: 17600] train loss: 1.2735, train acc: 0.4648, val loss: 1.2391, val acc: 0.4772  (best train acc: 0.4686, best val acc: 0.4809, best train loss: 1.2735  @ epoch 17600 )\n",
      "[Epoch: 17620] train loss: 1.2716, train acc: 0.4618, val loss: 1.2416, val acc: 0.4776  (best train acc: 0.4686, best val acc: 0.4809, best train loss: 1.2716  @ epoch 17620 )\n",
      "[Epoch: 17640] train loss: 1.2728, train acc: 0.4650, val loss: 1.2364, val acc: 0.4759  (best train acc: 0.4686, best val acc: 0.4809, best train loss: 1.2699  @ epoch 17639 )\n",
      "[Epoch: 17660] train loss: 1.2743, train acc: 0.4636, val loss: 1.2370, val acc: 0.4823  (best train acc: 0.4686, best val acc: 0.4823, best train loss: 1.2669  @ epoch 17659 )\n",
      "[Epoch: 17680] train loss: 1.2692, train acc: 0.4641, val loss: 1.2349, val acc: 0.4803  (best train acc: 0.4711, best val acc: 0.4850, best train loss: 1.2669  @ epoch 17659 )\n",
      "[Epoch: 17700] train loss: 1.2709, train acc: 0.4643, val loss: 1.2326, val acc: 0.4830  (best train acc: 0.4711, best val acc: 0.4850, best train loss: 1.2669  @ epoch 17659 )\n",
      "[Epoch: 17720] train loss: 1.2682, train acc: 0.4667, val loss: 1.2330, val acc: 0.4863  (best train acc: 0.4711, best val acc: 0.4880, best train loss: 1.2669  @ epoch 17659 )\n",
      "[Epoch: 17740] train loss: 1.2693, train acc: 0.4569, val loss: 1.2317, val acc: 0.4793  (best train acc: 0.4711, best val acc: 0.4880, best train loss: 1.2665  @ epoch 17722 )\n",
      "[Epoch: 17760] train loss: 1.3153, train acc: 0.4393, val loss: 1.3039, val acc: 0.4438  (best train acc: 0.4711, best val acc: 0.4931, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 17780] train loss: 1.3184, train acc: 0.4275, val loss: 1.2794, val acc: 0.4550  (best train acc: 0.4711, best val acc: 0.4931, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 17800] train loss: 1.3778, train acc: 0.3942, val loss: 1.3314, val acc: 0.4371  (best train acc: 0.4711, best val acc: 0.4931, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 17820] train loss: 1.3086, train acc: 0.4305, val loss: 1.2654, val acc: 0.4543  (best train acc: 0.4711, best val acc: 0.4931, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 17840] train loss: 1.3108, train acc: 0.4359, val loss: 1.2782, val acc: 0.4560  (best train acc: 0.4711, best val acc: 0.4931, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 17860] train loss: 1.3464, train acc: 0.4333, val loss: 1.3086, val acc: 0.4351  (best train acc: 0.4711, best val acc: 0.4931, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 17880] train loss: 2.0068, train acc: 0.2392, val loss: 2.0570, val acc: 0.2381  (best train acc: 0.4711, best val acc: 0.4931, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 17900] train loss: 1.6310, train acc: 0.2645, val loss: 1.5573, val acc: 0.2843  (best train acc: 0.4711, best val acc: 0.4931, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 17920] train loss: 1.4671, train acc: 0.2733, val loss: 1.4343, val acc: 0.3116  (best train acc: 0.4711, best val acc: 0.4931, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 17940] train loss: 1.4337, train acc: 0.3467, val loss: 1.4047, val acc: 0.3690  (best train acc: 0.4711, best val acc: 0.4931, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 17960] train loss: 1.3979, train acc: 0.3817, val loss: 1.3612, val acc: 0.4331  (best train acc: 0.4711, best val acc: 0.4931, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 17980] train loss: 1.3913, train acc: 0.3826, val loss: 1.3489, val acc: 0.4489  (best train acc: 0.4711, best val acc: 0.4931, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 18000] train loss: 1.3895, train acc: 0.3840, val loss: 1.3533, val acc: 0.4287  (best train acc: 0.4711, best val acc: 0.4931, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 18020] train loss: 1.4176, train acc: 0.3678, val loss: 1.3721, val acc: 0.4084  (best train acc: 0.4711, best val acc: 0.4931, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 18040] train loss: 1.3805, train acc: 0.3843, val loss: 1.3408, val acc: 0.4408  (best train acc: 0.4711, best val acc: 0.4931, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 18060] train loss: 1.3692, train acc: 0.3986, val loss: 1.3301, val acc: 0.4422  (best train acc: 0.4711, best val acc: 0.4931, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 18080] train loss: 1.3700, train acc: 0.3890, val loss: 1.3336, val acc: 0.4391  (best train acc: 0.4711, best val acc: 0.4931, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 18100] train loss: 1.3686, train acc: 0.3952, val loss: 1.3374, val acc: 0.4442  (best train acc: 0.4711, best val acc: 0.4931, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 18120] train loss: 1.3571, train acc: 0.4057, val loss: 1.3302, val acc: 0.4395  (best train acc: 0.4711, best val acc: 0.4931, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 18140] train loss: 1.3496, train acc: 0.4006, val loss: 1.3113, val acc: 0.4449  (best train acc: 0.4711, best val acc: 0.4931, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 18160] train loss: 1.3486, train acc: 0.3999, val loss: 1.3068, val acc: 0.4560  (best train acc: 0.4711, best val acc: 0.4931, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 18180] train loss: 1.3500, train acc: 0.4031, val loss: 1.3047, val acc: 0.4550  (best train acc: 0.4711, best val acc: 0.4931, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 18200] train loss: 1.3430, train acc: 0.4069, val loss: 1.3048, val acc: 0.4556  (best train acc: 0.4711, best val acc: 0.4931, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 18220] train loss: 1.3467, train acc: 0.4032, val loss: 1.3275, val acc: 0.4307  (best train acc: 0.4711, best val acc: 0.4931, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 18240] train loss: 1.3573, train acc: 0.3894, val loss: 1.3205, val acc: 0.4411  (best train acc: 0.4711, best val acc: 0.4931, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 18260] train loss: 1.3395, train acc: 0.4065, val loss: 1.2972, val acc: 0.4445  (best train acc: 0.4711, best val acc: 0.4931, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 18280] train loss: 1.3359, train acc: 0.4030, val loss: 1.2927, val acc: 0.4465  (best train acc: 0.4711, best val acc: 0.4931, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 18300] train loss: 1.3344, train acc: 0.4015, val loss: 1.2881, val acc: 0.4489  (best train acc: 0.4711, best val acc: 0.4931, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 18320] train loss: 1.3323, train acc: 0.4067, val loss: 1.2850, val acc: 0.4513  (best train acc: 0.4711, best val acc: 0.4931, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 18340] train loss: 1.3335, train acc: 0.4021, val loss: 1.2893, val acc: 0.4476  (best train acc: 0.4711, best val acc: 0.4931, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 18360] train loss: 1.3294, train acc: 0.4079, val loss: 1.2854, val acc: 0.4519  (best train acc: 0.4711, best val acc: 0.4931, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 18380] train loss: 1.3228, train acc: 0.4097, val loss: 1.2792, val acc: 0.4513  (best train acc: 0.4711, best val acc: 0.4931, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 18400] train loss: 1.3285, train acc: 0.4041, val loss: 1.2745, val acc: 0.4489  (best train acc: 0.4711, best val acc: 0.4931, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 18420] train loss: 1.3194, train acc: 0.4069, val loss: 1.2687, val acc: 0.4570  (best train acc: 0.4711, best val acc: 0.4931, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 18440] train loss: 1.3206, train acc: 0.4053, val loss: 1.2651, val acc: 0.4583  (best train acc: 0.4711, best val acc: 0.4931, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 18460] train loss: 1.3159, train acc: 0.4092, val loss: 1.2619, val acc: 0.4435  (best train acc: 0.4711, best val acc: 0.4931, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 18480] train loss: 1.3151, train acc: 0.4064, val loss: 1.2590, val acc: 0.4513  (best train acc: 0.4711, best val acc: 0.4931, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 18500] train loss: 1.3774, train acc: 0.3853, val loss: 1.3600, val acc: 0.4155  (best train acc: 0.4711, best val acc: 0.4931, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 18520] train loss: 1.3801, train acc: 0.3856, val loss: 1.3480, val acc: 0.4260  (best train acc: 0.4711, best val acc: 0.4931, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 18540] train loss: 1.3772, train acc: 0.3905, val loss: 1.3446, val acc: 0.4236  (best train acc: 0.4711, best val acc: 0.4931, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 18560] train loss: 1.3725, train acc: 0.3960, val loss: 1.3381, val acc: 0.4364  (best train acc: 0.4711, best val acc: 0.4931, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 18580] train loss: 1.3685, train acc: 0.3961, val loss: 1.3380, val acc: 0.4320  (best train acc: 0.4711, best val acc: 0.4931, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 18600] train loss: 1.3708, train acc: 0.3936, val loss: 1.3357, val acc: 0.4293  (best train acc: 0.4711, best val acc: 0.4931, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 18620] train loss: 1.3668, train acc: 0.4015, val loss: 1.3347, val acc: 0.4337  (best train acc: 0.4711, best val acc: 0.4931, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 18640] train loss: 1.3668, train acc: 0.3986, val loss: 1.3347, val acc: 0.4293  (best train acc: 0.4711, best val acc: 0.4931, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 18660] train loss: 1.3674, train acc: 0.3957, val loss: 1.3357, val acc: 0.4324  (best train acc: 0.4711, best val acc: 0.4931, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 18680] train loss: 1.3645, train acc: 0.4015, val loss: 1.3348, val acc: 0.4277  (best train acc: 0.4711, best val acc: 0.4931, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 18700] train loss: 1.3698, train acc: 0.3978, val loss: 1.3396, val acc: 0.4256  (best train acc: 0.4711, best val acc: 0.4931, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 18720] train loss: 1.3719, train acc: 0.3933, val loss: 1.3351, val acc: 0.4243  (best train acc: 0.4711, best val acc: 0.4931, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 18740] train loss: 1.3663, train acc: 0.3947, val loss: 1.3350, val acc: 0.4263  (best train acc: 0.4711, best val acc: 0.4931, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 18760] train loss: 1.3682, train acc: 0.3928, val loss: 1.3334, val acc: 0.4212  (best train acc: 0.4711, best val acc: 0.4931, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 18780] train loss: 1.3652, train acc: 0.3971, val loss: 1.3300, val acc: 0.4337  (best train acc: 0.4711, best val acc: 0.4931, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 18800] train loss: 1.3583, train acc: 0.3984, val loss: 1.3232, val acc: 0.4368  (best train acc: 0.4711, best val acc: 0.4931, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 18820] train loss: 1.3549, train acc: 0.4025, val loss: 1.3229, val acc: 0.4341  (best train acc: 0.4711, best val acc: 0.4931, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 18840] train loss: 1.3068, train acc: 0.4441, val loss: 1.2540, val acc: 0.4944  (best train acc: 0.4711, best val acc: 0.4944, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 18860] train loss: 1.3042, train acc: 0.4380, val loss: 1.2408, val acc: 0.4948  (best train acc: 0.4711, best val acc: 0.5022, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 18880] train loss: 1.3352, train acc: 0.4318, val loss: 1.2811, val acc: 0.4715  (best train acc: 0.4711, best val acc: 0.5022, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 18900] train loss: 1.2788, train acc: 0.4557, val loss: 1.2349, val acc: 0.5184  (best train acc: 0.4711, best val acc: 0.5184, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 18920] train loss: 1.3748, train acc: 0.3611, val loss: 1.3483, val acc: 0.3454  (best train acc: 0.4711, best val acc: 0.5184, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 18940] train loss: 1.2921, train acc: 0.4370, val loss: 1.2237, val acc: 0.4863  (best train acc: 0.4711, best val acc: 0.5184, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 18960] train loss: 1.2828, train acc: 0.4448, val loss: 1.2116, val acc: 0.5069  (best train acc: 0.4711, best val acc: 0.5184, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 18980] train loss: 1.3918, train acc: 0.3697, val loss: 1.3569, val acc: 0.4027  (best train acc: 0.4711, best val acc: 0.5184, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 19000] train loss: 1.2892, train acc: 0.4541, val loss: 1.2407, val acc: 0.4654  (best train acc: 0.4711, best val acc: 0.5228, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 19020] train loss: 1.4599, train acc: 0.2881, val loss: 1.4389, val acc: 0.3430  (best train acc: 0.4711, best val acc: 0.5228, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 19040] train loss: 1.4291, train acc: 0.3182, val loss: 1.4088, val acc: 0.3258  (best train acc: 0.4711, best val acc: 0.5228, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 19060] train loss: 1.3929, train acc: 0.3509, val loss: 1.3410, val acc: 0.3966  (best train acc: 0.4711, best val acc: 0.5228, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 19080] train loss: 1.3061, train acc: 0.4625, val loss: 1.2680, val acc: 0.5012  (best train acc: 0.4711, best val acc: 0.5228, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 19100] train loss: 1.2683, train acc: 0.4508, val loss: 1.2512, val acc: 0.5467  (best train acc: 0.4717, best val acc: 0.5467, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 19120] train loss: 1.6037, train acc: 0.3167, val loss: 1.5620, val acc: 0.3406  (best train acc: 0.4760, best val acc: 0.5467, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 19140] train loss: 1.5052, train acc: 0.3219, val loss: 1.4689, val acc: 0.3349  (best train acc: 0.4760, best val acc: 0.5467, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 19160] train loss: 1.4491, train acc: 0.3664, val loss: 1.4290, val acc: 0.3572  (best train acc: 0.4760, best val acc: 0.5467, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 19180] train loss: 1.4425, train acc: 0.3895, val loss: 1.4234, val acc: 0.3734  (best train acc: 0.4760, best val acc: 0.5467, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 19200] train loss: 1.4356, train acc: 0.3872, val loss: 1.4192, val acc: 0.3727  (best train acc: 0.4760, best val acc: 0.5467, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 19220] train loss: 1.4354, train acc: 0.3862, val loss: 1.4183, val acc: 0.3713  (best train acc: 0.4760, best val acc: 0.5467, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 19240] train loss: 1.4334, train acc: 0.3858, val loss: 1.4167, val acc: 0.3700  (best train acc: 0.4760, best val acc: 0.5467, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 19260] train loss: 1.4319, train acc: 0.3803, val loss: 1.4162, val acc: 0.3666  (best train acc: 0.4760, best val acc: 0.5467, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 19280] train loss: 1.4295, train acc: 0.3824, val loss: 1.4152, val acc: 0.3690  (best train acc: 0.4760, best val acc: 0.5467, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 19300] train loss: 1.4311, train acc: 0.3782, val loss: 1.4123, val acc: 0.3646  (best train acc: 0.4760, best val acc: 0.5467, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 19320] train loss: 1.4236, train acc: 0.3733, val loss: 1.4096, val acc: 0.3669  (best train acc: 0.4760, best val acc: 0.5467, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 19340] train loss: 1.4233, train acc: 0.3765, val loss: 1.4070, val acc: 0.3659  (best train acc: 0.4760, best val acc: 0.5467, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 19360] train loss: 1.4247, train acc: 0.3717, val loss: 1.4050, val acc: 0.3676  (best train acc: 0.4760, best val acc: 0.5467, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 19380] train loss: 1.4186, train acc: 0.3741, val loss: 1.4026, val acc: 0.3703  (best train acc: 0.4760, best val acc: 0.5467, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 19400] train loss: 1.4156, train acc: 0.3667, val loss: 1.3964, val acc: 0.3781  (best train acc: 0.4760, best val acc: 0.5467, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 19420] train loss: 1.4088, train acc: 0.3660, val loss: 1.3865, val acc: 0.3815  (best train acc: 0.4760, best val acc: 0.5467, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 19440] train loss: 1.4018, train acc: 0.3704, val loss: 1.3760, val acc: 0.3855  (best train acc: 0.4760, best val acc: 0.5467, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 19460] train loss: 1.3808, train acc: 0.3823, val loss: 1.3446, val acc: 0.3970  (best train acc: 0.4760, best val acc: 0.5467, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 19480] train loss: 1.3724, train acc: 0.3886, val loss: 1.3369, val acc: 0.3943  (best train acc: 0.4760, best val acc: 0.5467, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 19500] train loss: 1.3296, train acc: 0.4379, val loss: 1.2796, val acc: 0.4843  (best train acc: 0.4760, best val acc: 0.5467, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 19520] train loss: 1.2868, train acc: 0.4580, val loss: 1.2257, val acc: 0.5089  (best train acc: 0.4760, best val acc: 0.5467, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 19540] train loss: 1.2661, train acc: 0.4660, val loss: 1.2029, val acc: 0.5197  (best train acc: 0.4760, best val acc: 0.5467, best train loss: 1.2623  @ epoch 17745 )\n",
      "[Epoch: 19560] train loss: 1.2521, train acc: 0.4654, val loss: 1.1915, val acc: 0.5211  (best train acc: 0.4760, best val acc: 0.5467, best train loss: 1.2509  @ epoch 19559 )\n",
      "[Epoch: 19580] train loss: 1.2498, train acc: 0.4772, val loss: 1.1827, val acc: 0.5133  (best train acc: 0.4912, best val acc: 0.5467, best train loss: 1.2433  @ epoch 19579 )\n",
      "[Epoch: 19600] train loss: 1.2433, train acc: 0.4923, val loss: 1.1854, val acc: 0.5309  (best train acc: 0.4923, best val acc: 0.5467, best train loss: 1.2423  @ epoch 19591 )\n",
      "[Epoch: 19620] train loss: 1.2518, train acc: 0.4832, val loss: 1.1903, val acc: 0.5218  (best train acc: 0.4944, best val acc: 0.5467, best train loss: 1.2423  @ epoch 19591 )\n",
      "[Epoch: 19640] train loss: 1.2727, train acc: 0.4680, val loss: 1.2138, val acc: 0.5123  (best train acc: 0.4944, best val acc: 0.5467, best train loss: 1.2423  @ epoch 19591 )\n",
      "[Epoch: 19660] train loss: 1.4498, train acc: 0.3991, val loss: 1.2826, val acc: 0.4594  (best train acc: 0.4944, best val acc: 0.5467, best train loss: 1.2423  @ epoch 19591 )\n",
      "[Epoch: 19680] train loss: 1.2731, train acc: 0.4561, val loss: 1.1907, val acc: 0.5150  (best train acc: 0.4944, best val acc: 0.5467, best train loss: 1.2423  @ epoch 19591 )\n",
      "[Epoch: 19700] train loss: 1.2526, train acc: 0.4749, val loss: 1.1926, val acc: 0.5029  (best train acc: 0.4944, best val acc: 0.5467, best train loss: 1.2423  @ epoch 19591 )\n",
      "[Epoch: 19720] train loss: 1.2454, train acc: 0.4761, val loss: 1.1884, val acc: 0.5039  (best train acc: 0.4944, best val acc: 0.5467, best train loss: 1.2406  @ epoch 19715 )\n",
      "[Epoch: 19740] train loss: 1.2469, train acc: 0.4777, val loss: 1.1821, val acc: 0.5312  (best train acc: 0.4944, best val acc: 0.5467, best train loss: 1.2406  @ epoch 19736 )\n",
      "[Epoch: 19760] train loss: 1.2324, train acc: 0.4717, val loss: 1.1674, val acc: 0.5110  (best train acc: 0.4944, best val acc: 0.5467, best train loss: 1.2311  @ epoch 19759 )\n",
      "[Epoch: 19780] train loss: 1.2248, train acc: 0.5026, val loss: 1.1638, val acc: 0.5295  (best train acc: 0.5031, best val acc: 0.5467, best train loss: 1.2191  @ epoch 19779 )\n",
      "[Epoch: 19800] train loss: 1.2203, train acc: 0.5038, val loss: 1.1510, val acc: 0.5501  (best train acc: 0.5074, best val acc: 0.5508, best train loss: 1.2162  @ epoch 19793 )\n",
      "[Epoch: 19820] train loss: 1.2176, train acc: 0.5033, val loss: 1.1477, val acc: 0.5511  (best train acc: 0.5074, best val acc: 0.5541, best train loss: 1.2122  @ epoch 19815 )\n",
      "[Epoch: 19840] train loss: 1.2179, train acc: 0.5025, val loss: 1.1455, val acc: 0.5467  (best train acc: 0.5074, best val acc: 0.5541, best train loss: 1.2114  @ epoch 19835 )\n",
      "[Epoch: 19860] train loss: 1.2128, train acc: 0.5039, val loss: 1.1441, val acc: 0.5491  (best train acc: 0.5084, best val acc: 0.5541, best train loss: 1.2084  @ epoch 19841 )\n",
      "[Epoch: 19880] train loss: 1.2139, train acc: 0.5039, val loss: 1.1406, val acc: 0.5494  (best train acc: 0.5084, best val acc: 0.5541, best train loss: 1.2079  @ epoch 19878 )\n",
      "[Epoch: 19900] train loss: 1.2239, train acc: 0.5048, val loss: 1.1569, val acc: 0.5373  (best train acc: 0.5084, best val acc: 0.5541, best train loss: 1.2079  @ epoch 19878 )\n",
      "[Epoch: 19920] train loss: 1.2198, train acc: 0.5041, val loss: 1.1524, val acc: 0.5393  (best train acc: 0.5084, best val acc: 0.5541, best train loss: 1.2079  @ epoch 19878 )\n",
      "[Epoch: 19940] train loss: 1.2209, train acc: 0.4999, val loss: 1.1466, val acc: 0.5440  (best train acc: 0.5084, best val acc: 0.5541, best train loss: 1.2075  @ epoch 19939 )\n",
      "[Epoch: 19960] train loss: 1.2164, train acc: 0.4980, val loss: 1.1453, val acc: 0.5393  (best train acc: 0.5084, best val acc: 0.5541, best train loss: 1.2075  @ epoch 19939 )\n",
      "[Epoch: 19980] train loss: 1.2098, train acc: 0.4996, val loss: 1.1422, val acc: 0.5437  (best train acc: 0.5084, best val acc: 0.5541, best train loss: 1.2075  @ epoch 19939 )\n",
      "[Epoch: 20000] train loss: 1.2086, train acc: 0.5007, val loss: 1.1423, val acc: 0.5417  (best train acc: 0.5084, best val acc: 0.5541, best train loss: 1.2052  @ epoch 19989 )\n",
      "[Epoch: 20020] train loss: 1.2153, train acc: 0.4990, val loss: 1.1403, val acc: 0.5410  (best train acc: 0.5084, best val acc: 0.5541, best train loss: 1.2052  @ epoch 19989 )\n",
      "[Epoch: 20040] train loss: 1.2127, train acc: 0.4980, val loss: 1.1397, val acc: 0.5413  (best train acc: 0.5084, best val acc: 0.5541, best train loss: 1.2052  @ epoch 19989 )\n",
      "[Epoch: 20060] train loss: 1.2127, train acc: 0.5013, val loss: 1.1403, val acc: 0.5396  (best train acc: 0.5084, best val acc: 0.5541, best train loss: 1.2049  @ epoch 20058 )\n",
      "[Epoch: 20080] train loss: 1.2063, train acc: 0.5022, val loss: 1.1385, val acc: 0.5457  (best train acc: 0.5084, best val acc: 0.5541, best train loss: 1.2029  @ epoch 20066 )\n",
      "[Epoch: 20100] train loss: 1.2071, train acc: 0.5024, val loss: 1.1372, val acc: 0.5406  (best train acc: 0.5084, best val acc: 0.5541, best train loss: 1.2029  @ epoch 20066 )\n",
      "[Epoch: 20120] train loss: 1.2080, train acc: 0.4975, val loss: 1.1362, val acc: 0.5400  (best train acc: 0.5084, best val acc: 0.5541, best train loss: 1.2020  @ epoch 20105 )\n",
      "[Epoch: 20140] train loss: 1.2083, train acc: 0.4994, val loss: 1.1385, val acc: 0.5427  (best train acc: 0.5084, best val acc: 0.5541, best train loss: 1.2020  @ epoch 20105 )\n",
      "[Epoch: 20160] train loss: 1.2108, train acc: 0.4935, val loss: 1.1374, val acc: 0.5278  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.2020  @ epoch 20105 )\n",
      "[Epoch: 20180] train loss: 1.1991, train acc: 0.4866, val loss: 1.1366, val acc: 0.5464  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1991  @ epoch 20180 )\n",
      "[Epoch: 20200] train loss: 1.1981, train acc: 0.4857, val loss: 1.1348, val acc: 0.5309  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1956  @ epoch 20195 )\n",
      "[Epoch: 20220] train loss: 1.2040, train acc: 0.4840, val loss: 1.1351, val acc: 0.5302  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 20240] train loss: 1.3484, train acc: 0.4316, val loss: 1.2902, val acc: 0.5012  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 20260] train loss: 1.4128, train acc: 0.3863, val loss: 1.3374, val acc: 0.4378  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 20280] train loss: 1.3984, train acc: 0.3782, val loss: 1.3827, val acc: 0.3818  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 20300] train loss: 1.3937, train acc: 0.3754, val loss: 1.3651, val acc: 0.3970  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 20320] train loss: 1.3866, train acc: 0.3790, val loss: 1.3622, val acc: 0.4027  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 20340] train loss: 1.3848, train acc: 0.3818, val loss: 1.3586, val acc: 0.4037  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 20360] train loss: 1.3887, train acc: 0.3733, val loss: 1.3575, val acc: 0.4020  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 20380] train loss: 1.3826, train acc: 0.3784, val loss: 1.3573, val acc: 0.4034  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 20400] train loss: 1.3838, train acc: 0.3782, val loss: 1.3559, val acc: 0.4047  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 20420] train loss: 1.3824, train acc: 0.3776, val loss: 1.3558, val acc: 0.4024  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 20440] train loss: 1.3827, train acc: 0.3816, val loss: 1.3553, val acc: 0.4054  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 20460] train loss: 1.3817, train acc: 0.3791, val loss: 1.3547, val acc: 0.4054  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 20480] train loss: 1.3801, train acc: 0.3785, val loss: 1.3538, val acc: 0.4054  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 20500] train loss: 1.3812, train acc: 0.3798, val loss: 1.3548, val acc: 0.4064  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 20520] train loss: 1.3856, train acc: 0.3776, val loss: 1.3534, val acc: 0.4051  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 20540] train loss: 1.3748, train acc: 0.3826, val loss: 1.3534, val acc: 0.4081  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 20560] train loss: 1.3796, train acc: 0.3842, val loss: 1.3537, val acc: 0.4074  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 20580] train loss: 1.3796, train acc: 0.3820, val loss: 1.3530, val acc: 0.4081  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 20600] train loss: 1.3810, train acc: 0.3894, val loss: 1.3527, val acc: 0.4064  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 20620] train loss: 1.3760, train acc: 0.3874, val loss: 1.3519, val acc: 0.4054  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 20640] train loss: 1.3815, train acc: 0.3884, val loss: 1.3522, val acc: 0.4061  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 20660] train loss: 1.3826, train acc: 0.3908, val loss: 1.3524, val acc: 0.4064  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 20680] train loss: 1.3833, train acc: 0.3882, val loss: 1.3531, val acc: 0.4071  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 20700] train loss: 1.3749, train acc: 0.3928, val loss: 1.3519, val acc: 0.4078  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 20720] train loss: 1.3777, train acc: 0.3919, val loss: 1.3520, val acc: 0.4064  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 20740] train loss: 1.3725, train acc: 0.3953, val loss: 1.3512, val acc: 0.4044  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 20760] train loss: 1.3737, train acc: 0.3949, val loss: 1.3514, val acc: 0.4071  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 20780] train loss: 1.3793, train acc: 0.4156, val loss: 1.3500, val acc: 0.4105  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 20800] train loss: 1.3790, train acc: 0.4102, val loss: 1.3496, val acc: 0.4121  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 20820] train loss: 1.3784, train acc: 0.4095, val loss: 1.3494, val acc: 0.4108  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 20840] train loss: 1.3726, train acc: 0.4148, val loss: 1.3490, val acc: 0.4101  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 20860] train loss: 1.3786, train acc: 0.4122, val loss: 1.3491, val acc: 0.4108  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 20880] train loss: 1.3762, train acc: 0.4098, val loss: 1.3488, val acc: 0.4067  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 20900] train loss: 1.3774, train acc: 0.4152, val loss: 1.3491, val acc: 0.4054  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 20920] train loss: 1.3766, train acc: 0.4109, val loss: 1.3493, val acc: 0.4067  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 20940] train loss: 1.3772, train acc: 0.4085, val loss: 1.3474, val acc: 0.4132  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 20960] train loss: 1.3776, train acc: 0.4115, val loss: 1.3468, val acc: 0.4067  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 20980] train loss: 1.3714, train acc: 0.4128, val loss: 1.3440, val acc: 0.4128  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 21000] train loss: 1.3697, train acc: 0.4126, val loss: 1.3322, val acc: 0.4074  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 21020] train loss: 1.3605, train acc: 0.4164, val loss: 1.3277, val acc: 0.4128  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 21040] train loss: 1.3572, train acc: 0.4119, val loss: 1.3249, val acc: 0.4165  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 21060] train loss: 1.3590, train acc: 0.4090, val loss: 1.3241, val acc: 0.4142  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 21080] train loss: 1.3547, train acc: 0.4091, val loss: 1.3224, val acc: 0.4159  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 21100] train loss: 1.3559, train acc: 0.4100, val loss: 1.3198, val acc: 0.4189  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 21120] train loss: 1.3579, train acc: 0.4148, val loss: 1.3181, val acc: 0.4196  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 21140] train loss: 1.3514, train acc: 0.4101, val loss: 1.3180, val acc: 0.4202  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 21160] train loss: 1.3488, train acc: 0.4130, val loss: 1.3151, val acc: 0.4196  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 21180] train loss: 1.3478, train acc: 0.4143, val loss: 1.3142, val acc: 0.4185  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 21200] train loss: 1.3482, train acc: 0.4097, val loss: 1.3123, val acc: 0.4189  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 21220] train loss: 1.3465, train acc: 0.4110, val loss: 1.3090, val acc: 0.4206  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 21240] train loss: 1.3394, train acc: 0.4154, val loss: 1.3078, val acc: 0.4277  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 21260] train loss: 1.3426, train acc: 0.4130, val loss: 1.3119, val acc: 0.4192  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 21280] train loss: 1.3395, train acc: 0.4156, val loss: 1.3090, val acc: 0.4239  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 21300] train loss: 1.3390, train acc: 0.4173, val loss: 1.3079, val acc: 0.4428  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 21320] train loss: 1.3415, train acc: 0.4246, val loss: 1.3039, val acc: 0.4401  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 21340] train loss: 1.3392, train acc: 0.4319, val loss: 1.3060, val acc: 0.4425  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 21360] train loss: 1.3355, train acc: 0.4313, val loss: 1.3039, val acc: 0.4260  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 21380] train loss: 1.3388, train acc: 0.4138, val loss: 1.3014, val acc: 0.4422  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 21400] train loss: 1.3389, train acc: 0.4276, val loss: 1.3023, val acc: 0.4516  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 21420] train loss: 1.3347, train acc: 0.4320, val loss: 1.3018, val acc: 0.4509  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 21440] train loss: 1.3329, train acc: 0.4359, val loss: 1.2987, val acc: 0.4563  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 21460] train loss: 1.3341, train acc: 0.4395, val loss: 1.2980, val acc: 0.4621  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 21480] train loss: 1.3332, train acc: 0.4385, val loss: 1.2959, val acc: 0.4621  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 21500] train loss: 1.3344, train acc: 0.4375, val loss: 1.2947, val acc: 0.4614  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 21520] train loss: 1.3388, train acc: 0.4316, val loss: 1.3031, val acc: 0.4516  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 21540] train loss: 1.3332, train acc: 0.4367, val loss: 1.3011, val acc: 0.4452  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 21560] train loss: 1.3375, train acc: 0.4308, val loss: 1.2996, val acc: 0.4543  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 21580] train loss: 1.3342, train acc: 0.4317, val loss: 1.3002, val acc: 0.4543  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 21600] train loss: 1.3318, train acc: 0.4327, val loss: 1.3001, val acc: 0.4499  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 21620] train loss: 1.3360, train acc: 0.4232, val loss: 1.2983, val acc: 0.4472  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 21640] train loss: 1.3301, train acc: 0.4352, val loss: 1.2943, val acc: 0.4513  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 21660] train loss: 1.3328, train acc: 0.4372, val loss: 1.2971, val acc: 0.4442  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 21680] train loss: 1.3369, train acc: 0.4315, val loss: 1.2978, val acc: 0.4499  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 21700] train loss: 1.3302, train acc: 0.4291, val loss: 1.2962, val acc: 0.4513  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 21720] train loss: 1.3316, train acc: 0.4333, val loss: 1.2955, val acc: 0.4482  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 21740] train loss: 1.3309, train acc: 0.4388, val loss: 1.2939, val acc: 0.4452  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 21760] train loss: 1.3243, train acc: 0.4427, val loss: 1.2945, val acc: 0.4509  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 21780] train loss: 1.3276, train acc: 0.4487, val loss: 1.2911, val acc: 0.4607  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 21800] train loss: 1.3260, train acc: 0.4458, val loss: 1.2889, val acc: 0.4658  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 21820] train loss: 1.3239, train acc: 0.4452, val loss: 1.2890, val acc: 0.4546  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 21840] train loss: 1.3220, train acc: 0.4453, val loss: 1.2871, val acc: 0.4607  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 21860] train loss: 1.3190, train acc: 0.4506, val loss: 1.2875, val acc: 0.4556  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 21880] train loss: 1.3206, train acc: 0.4487, val loss: 1.2842, val acc: 0.4570  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 21900] train loss: 1.3179, train acc: 0.4505, val loss: 1.3059, val acc: 0.4543  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 21920] train loss: 1.3519, train acc: 0.4338, val loss: 1.3161, val acc: 0.4341  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 21940] train loss: 1.3314, train acc: 0.4448, val loss: 1.2978, val acc: 0.4449  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 21960] train loss: 1.3240, train acc: 0.4539, val loss: 1.2902, val acc: 0.4644  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 21980] train loss: 1.3192, train acc: 0.4470, val loss: 1.2864, val acc: 0.4607  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 22000] train loss: 1.3169, train acc: 0.4547, val loss: 1.2797, val acc: 0.4782  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 22020] train loss: 1.3078, train acc: 0.4587, val loss: 1.2767, val acc: 0.4742  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 22040] train loss: 1.3190, train acc: 0.4558, val loss: 1.2824, val acc: 0.4675  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 22060] train loss: 1.3121, train acc: 0.4571, val loss: 1.2716, val acc: 0.4759  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 22080] train loss: 1.3042, train acc: 0.4604, val loss: 1.2684, val acc: 0.4715  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 22100] train loss: 1.3001, train acc: 0.4567, val loss: 1.2725, val acc: 0.4688  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 22120] train loss: 1.3124, train acc: 0.4521, val loss: 1.2683, val acc: 0.4610  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 22140] train loss: 1.2969, train acc: 0.4614, val loss: 1.2604, val acc: 0.4725  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 22160] train loss: 1.2946, train acc: 0.4594, val loss: 1.2551, val acc: 0.4735  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 22180] train loss: 1.6973, train acc: 0.2650, val loss: 1.6197, val acc: 0.2570  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 22200] train loss: 1.8550, train acc: 0.2547, val loss: 1.8212, val acc: 0.2371  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 22220] train loss: 1.5253, train acc: 0.2308, val loss: 1.5225, val acc: 0.2327  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 22240] train loss: 1.5222, train acc: 0.2308, val loss: 1.5216, val acc: 0.2354  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 22260] train loss: 1.5194, train acc: 0.2640, val loss: 1.5180, val acc: 0.2688  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 22280] train loss: 1.5181, train acc: 0.2538, val loss: 1.5178, val acc: 0.2317  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 22300] train loss: 1.5181, train acc: 0.2038, val loss: 1.5178, val acc: 0.2317  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 22320] train loss: 1.5181, train acc: 0.2097, val loss: 1.5178, val acc: 0.2314  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 22340] train loss: 1.5183, train acc: 0.2067, val loss: 1.5178, val acc: 0.2317  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 22360] train loss: 1.5177, train acc: 0.2177, val loss: 1.5178, val acc: 0.2304  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 22380] train loss: 1.5181, train acc: 0.2099, val loss: 1.5177, val acc: 0.2320  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 22400] train loss: 1.5176, train acc: 0.2107, val loss: 1.5177, val acc: 0.2317  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 22420] train loss: 1.5175, train acc: 0.2173, val loss: 1.5177, val acc: 0.2334  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 22440] train loss: 1.5174, train acc: 0.2184, val loss: 1.5177, val acc: 0.2381  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 22460] train loss: 1.5178, train acc: 0.2154, val loss: 1.5176, val acc: 0.2331  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 22480] train loss: 1.5175, train acc: 0.2180, val loss: 1.5176, val acc: 0.2327  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 22500] train loss: 1.5178, train acc: 0.2230, val loss: 1.5175, val acc: 0.2334  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 22520] train loss: 1.5173, train acc: 0.2159, val loss: 1.5174, val acc: 0.2307  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 22540] train loss: 1.5175, train acc: 0.2169, val loss: 1.5172, val acc: 0.2334  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 22560] train loss: 1.5175, train acc: 0.2143, val loss: 1.5171, val acc: 0.2304  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 22580] train loss: 1.5173, train acc: 0.2265, val loss: 1.5170, val acc: 0.2401  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 22600] train loss: 1.5167, train acc: 0.2353, val loss: 1.5164, val acc: 0.2364  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 22620] train loss: 1.5149, train acc: 0.2460, val loss: 1.5149, val acc: 0.2583  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 22640] train loss: 1.5146, train acc: 0.2785, val loss: 1.5140, val acc: 0.2786  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 22660] train loss: 1.5135, train acc: 0.2228, val loss: 1.5140, val acc: 0.2034  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 22680] train loss: 1.5133, train acc: 0.2809, val loss: 1.5135, val acc: 0.3039  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 22700] train loss: 1.5176, train acc: 0.2707, val loss: 1.5175, val acc: 0.2600  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 22720] train loss: 1.5169, train acc: 0.2084, val loss: 1.5171, val acc: 0.2121  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 22740] train loss: 1.5171, train acc: 0.2177, val loss: 1.5169, val acc: 0.2270  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 22760] train loss: 1.5162, train acc: 0.2161, val loss: 1.5168, val acc: 0.2253  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 22780] train loss: 1.5166, train acc: 0.2287, val loss: 1.5168, val acc: 0.2246  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 22800] train loss: 1.5168, train acc: 0.2196, val loss: 1.5167, val acc: 0.2250  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 22820] train loss: 1.5165, train acc: 0.2371, val loss: 1.5166, val acc: 0.2462  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 22840] train loss: 1.5159, train acc: 0.2313, val loss: 1.5165, val acc: 0.2384  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 22860] train loss: 1.5160, train acc: 0.2222, val loss: 1.5164, val acc: 0.2250  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 22880] train loss: 1.5155, train acc: 0.2449, val loss: 1.5162, val acc: 0.2587  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 22900] train loss: 1.5150, train acc: 0.2338, val loss: 1.5161, val acc: 0.2411  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 22920] train loss: 1.5155, train acc: 0.2337, val loss: 1.5159, val acc: 0.2418  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 22940] train loss: 1.5159, train acc: 0.2424, val loss: 1.5154, val acc: 0.2550  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 22960] train loss: 1.5146, train acc: 0.2451, val loss: 1.5149, val acc: 0.2567  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 22980] train loss: 1.6699, train acc: 0.2542, val loss: 1.6584, val acc: 0.2374  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 23000] train loss: 1.5263, train acc: 0.2633, val loss: 1.5268, val acc: 0.2577  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 23020] train loss: 1.5390, train acc: 0.2821, val loss: 1.5363, val acc: 0.2691  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 23040] train loss: 1.5228, train acc: 0.2243, val loss: 1.5217, val acc: 0.2344  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 23060] train loss: 1.5153, train acc: 0.2172, val loss: 1.5147, val acc: 0.2175  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 23080] train loss: 1.5144, train acc: 0.2534, val loss: 1.5141, val acc: 0.2556  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 23100] train loss: 1.5139, train acc: 0.2544, val loss: 1.5135, val acc: 0.2567  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 23120] train loss: 1.5140, train acc: 0.2458, val loss: 1.5132, val acc: 0.2499  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 23140] train loss: 1.5131, train acc: 0.2599, val loss: 1.5130, val acc: 0.2644  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 23160] train loss: 1.5131, train acc: 0.2621, val loss: 1.5128, val acc: 0.2641  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 23180] train loss: 1.5127, train acc: 0.2594, val loss: 1.5126, val acc: 0.2614  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 23200] train loss: 1.5120, train acc: 0.2627, val loss: 1.5124, val acc: 0.2671  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 23220] train loss: 1.5121, train acc: 0.2568, val loss: 1.5122, val acc: 0.2627  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 23240] train loss: 1.5123, train acc: 0.2658, val loss: 1.5120, val acc: 0.2614  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 23260] train loss: 1.5112, train acc: 0.2660, val loss: 1.5117, val acc: 0.2651  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 23280] train loss: 1.5120, train acc: 0.2661, val loss: 1.5115, val acc: 0.2627  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 23300] train loss: 1.5109, train acc: 0.2667, val loss: 1.5112, val acc: 0.2661  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 23320] train loss: 1.5114, train acc: 0.2668, val loss: 1.5109, val acc: 0.2722  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 23340] train loss: 1.5112, train acc: 0.2681, val loss: 1.5106, val acc: 0.2749  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 23360] train loss: 1.5100, train acc: 0.2777, val loss: 1.5103, val acc: 0.2806  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 23380] train loss: 1.5104, train acc: 0.2745, val loss: 1.5099, val acc: 0.2793  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 23400] train loss: 1.5096, train acc: 0.2763, val loss: 1.5095, val acc: 0.2833  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 23420] train loss: 1.5097, train acc: 0.2840, val loss: 1.5090, val acc: 0.2887  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 23440] train loss: 1.5092, train acc: 0.2786, val loss: 1.5085, val acc: 0.2931  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 23460] train loss: 1.5084, train acc: 0.2822, val loss: 1.5079, val acc: 0.2874  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 23480] train loss: 1.5087, train acc: 0.2854, val loss: 1.5067, val acc: 0.2887  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 23500] train loss: 1.5056, train acc: 0.2854, val loss: 1.5044, val acc: 0.2793  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 23520] train loss: 1.5030, train acc: 0.2996, val loss: 1.5020, val acc: 0.3042  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 23540] train loss: 1.5032, train acc: 0.2971, val loss: 1.5002, val acc: 0.3032  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 23560] train loss: 1.5027, train acc: 0.2980, val loss: 1.4987, val acc: 0.3035  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 23580] train loss: 1.5012, train acc: 0.2989, val loss: 1.4973, val acc: 0.3062  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 23600] train loss: 1.4991, train acc: 0.2997, val loss: 1.4955, val acc: 0.3049  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 23620] train loss: 1.4963, train acc: 0.3089, val loss: 1.4921, val acc: 0.3056  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 23640] train loss: 1.4947, train acc: 0.3091, val loss: 1.4878, val acc: 0.3140  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 23660] train loss: 1.4919, train acc: 0.3146, val loss: 1.4843, val acc: 0.3056  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 23680] train loss: 1.4913, train acc: 0.3094, val loss: 1.4826, val acc: 0.3133  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 23700] train loss: 1.4891, train acc: 0.3169, val loss: 1.4814, val acc: 0.3133  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 23720] train loss: 1.4892, train acc: 0.3172, val loss: 1.4808, val acc: 0.3201  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 23740] train loss: 1.4894, train acc: 0.3135, val loss: 1.4800, val acc: 0.3207  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 23760] train loss: 1.4875, train acc: 0.3206, val loss: 1.4796, val acc: 0.3248  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 23780] train loss: 1.4879, train acc: 0.3198, val loss: 1.4785, val acc: 0.3245  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 23800] train loss: 1.4870, train acc: 0.3194, val loss: 1.4768, val acc: 0.3241  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 23820] train loss: 1.4814, train acc: 0.3270, val loss: 1.4738, val acc: 0.3315  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 23840] train loss: 1.4791, train acc: 0.3361, val loss: 1.4697, val acc: 0.3396  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 23860] train loss: 1.4721, train acc: 0.3379, val loss: 1.4630, val acc: 0.3366  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 23880] train loss: 1.4603, train acc: 0.3547, val loss: 1.4464, val acc: 0.3582  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 23900] train loss: 1.4517, train acc: 0.3648, val loss: 1.4327, val acc: 0.3636  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 23920] train loss: 1.8682, train acc: 0.2371, val loss: 1.8019, val acc: 0.2371  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 23940] train loss: 1.5375, train acc: 0.2371, val loss: 1.5333, val acc: 0.2691  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 23960] train loss: 1.5108, train acc: 0.2593, val loss: 1.5106, val acc: 0.2445  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 23980] train loss: 1.5034, train acc: 0.3263, val loss: 1.5023, val acc: 0.2911  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 24000] train loss: 1.4982, train acc: 0.3142, val loss: 1.4996, val acc: 0.2934  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 24020] train loss: 1.4964, train acc: 0.3008, val loss: 1.4985, val acc: 0.2236  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 24040] train loss: 1.4900, train acc: 0.3237, val loss: 1.4909, val acc: 0.3147  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 24060] train loss: 1.4857, train acc: 0.3310, val loss: 1.4868, val acc: 0.3265  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 24080] train loss: 1.5006, train acc: 0.3265, val loss: 1.4972, val acc: 0.3707  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 24100] train loss: 1.4832, train acc: 0.3156, val loss: 1.4789, val acc: 0.3228  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 24120] train loss: 1.4746, train acc: 0.3082, val loss: 1.4651, val acc: 0.3194  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 24140] train loss: 1.4616, train acc: 0.3483, val loss: 1.4509, val acc: 0.3241  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 24160] train loss: 1.4530, train acc: 0.3258, val loss: 1.4442, val acc: 0.3396  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 24180] train loss: 1.4467, train acc: 0.3455, val loss: 1.4352, val acc: 0.3680  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 24200] train loss: 1.4385, train acc: 0.3480, val loss: 1.4285, val acc: 0.3720  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 24220] train loss: 1.4354, train acc: 0.3513, val loss: 1.4222, val acc: 0.4236  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 24240] train loss: 1.4303, train acc: 0.3921, val loss: 1.4171, val acc: 0.4236  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 24260] train loss: 1.4268, train acc: 0.3858, val loss: 1.4130, val acc: 0.4250  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 24280] train loss: 1.4249, train acc: 0.3865, val loss: 1.4103, val acc: 0.4165  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 24300] train loss: 1.4238, train acc: 0.3898, val loss: 1.4080, val acc: 0.4111  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 24320] train loss: 1.4213, train acc: 0.3905, val loss: 1.4071, val acc: 0.4135  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 24340] train loss: 1.4183, train acc: 0.3903, val loss: 1.4053, val acc: 0.4165  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 24360] train loss: 1.4175, train acc: 0.3903, val loss: 1.4036, val acc: 0.4159  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 24380] train loss: 1.4172, train acc: 0.3913, val loss: 1.4023, val acc: 0.4179  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 24400] train loss: 1.4238, train acc: 0.3859, val loss: 1.4041, val acc: 0.4121  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 24420] train loss: 1.4193, train acc: 0.3871, val loss: 1.3982, val acc: 0.4287  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 24440] train loss: 1.4158, train acc: 0.3918, val loss: 1.3955, val acc: 0.4226  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 24460] train loss: 1.4141, train acc: 0.3916, val loss: 1.3950, val acc: 0.4273  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 24480] train loss: 1.4116, train acc: 0.3955, val loss: 1.3929, val acc: 0.4223  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 24500] train loss: 1.4110, train acc: 0.3924, val loss: 1.3909, val acc: 0.4250  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 24520] train loss: 1.4085, train acc: 0.3942, val loss: 1.3898, val acc: 0.4209  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 24540] train loss: 1.4098, train acc: 0.3957, val loss: 1.3892, val acc: 0.4250  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 24560] train loss: 1.4062, train acc: 0.3974, val loss: 1.3878, val acc: 0.4253  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 24580] train loss: 1.4082, train acc: 0.3897, val loss: 1.3866, val acc: 0.4243  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 24600] train loss: 1.4065, train acc: 0.3946, val loss: 1.3853, val acc: 0.4236  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 24620] train loss: 1.4051, train acc: 0.3961, val loss: 1.3842, val acc: 0.4260  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 24640] train loss: 1.4028, train acc: 0.3987, val loss: 1.3839, val acc: 0.4260  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 24660] train loss: 1.4040, train acc: 0.3937, val loss: 1.3827, val acc: 0.4253  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 24680] train loss: 1.4003, train acc: 0.3967, val loss: 1.3812, val acc: 0.4250  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 24700] train loss: 1.4024, train acc: 0.3955, val loss: 1.3817, val acc: 0.4243  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 24720] train loss: 1.4015, train acc: 0.3904, val loss: 1.3811, val acc: 0.4233  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 24740] train loss: 1.4021, train acc: 0.3946, val loss: 1.3802, val acc: 0.4243  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 24760] train loss: 1.3983, train acc: 0.3980, val loss: 1.3790, val acc: 0.4256  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 24780] train loss: 1.3988, train acc: 0.3973, val loss: 1.3798, val acc: 0.4246  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 24800] train loss: 1.4002, train acc: 0.3935, val loss: 1.3785, val acc: 0.4253  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 24820] train loss: 1.4000, train acc: 0.3959, val loss: 1.3791, val acc: 0.4212  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 24840] train loss: 1.3982, train acc: 0.3959, val loss: 1.3774, val acc: 0.4263  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 24860] train loss: 1.3989, train acc: 0.3960, val loss: 1.3769, val acc: 0.4256  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 24880] train loss: 1.4012, train acc: 0.3945, val loss: 1.3766, val acc: 0.4239  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 24900] train loss: 1.3983, train acc: 0.3971, val loss: 1.3760, val acc: 0.4246  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 24920] train loss: 1.3960, train acc: 0.3955, val loss: 1.3745, val acc: 0.4260  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 24940] train loss: 1.3988, train acc: 0.3948, val loss: 1.3743, val acc: 0.4246  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 24960] train loss: 1.3962, train acc: 0.3995, val loss: 1.3749, val acc: 0.4229  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 24980] train loss: 1.3928, train acc: 0.3971, val loss: 1.3735, val acc: 0.4253  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 25000] train loss: 1.3976, train acc: 0.3940, val loss: 1.3742, val acc: 0.4219  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 25020] train loss: 1.3959, train acc: 0.3997, val loss: 1.3734, val acc: 0.4236  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 25040] train loss: 1.3944, train acc: 0.3981, val loss: 1.3730, val acc: 0.4229  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 25060] train loss: 1.3933, train acc: 0.3955, val loss: 1.3717, val acc: 0.4270  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 25080] train loss: 1.3945, train acc: 0.3953, val loss: 1.3718, val acc: 0.4253  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 25100] train loss: 1.3958, train acc: 0.3929, val loss: 1.3719, val acc: 0.4233  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 25120] train loss: 1.3939, train acc: 0.3970, val loss: 1.3713, val acc: 0.4266  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 25140] train loss: 1.3910, train acc: 0.3985, val loss: 1.3704, val acc: 0.4233  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 25160] train loss: 1.3942, train acc: 0.3952, val loss: 1.3715, val acc: 0.4246  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 25180] train loss: 1.3929, train acc: 0.3958, val loss: 1.3737, val acc: 0.4233  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 25200] train loss: 1.3970, train acc: 0.3861, val loss: 1.3697, val acc: 0.4236  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 25220] train loss: 1.3950, train acc: 0.3942, val loss: 1.3690, val acc: 0.4236  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 25240] train loss: 1.3898, train acc: 0.4007, val loss: 1.3690, val acc: 0.4223  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 25260] train loss: 1.3907, train acc: 0.3978, val loss: 1.3687, val acc: 0.4223  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 25280] train loss: 1.3890, train acc: 0.3962, val loss: 1.3681, val acc: 0.4266  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 25300] train loss: 1.3877, train acc: 0.3994, val loss: 1.3698, val acc: 0.4243  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 25320] train loss: 1.3905, train acc: 0.3956, val loss: 1.3679, val acc: 0.4212  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 25340] train loss: 1.3909, train acc: 0.3981, val loss: 1.3689, val acc: 0.4233  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 25360] train loss: 1.3930, train acc: 0.3946, val loss: 1.3681, val acc: 0.4226  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 25380] train loss: 1.3911, train acc: 0.3962, val loss: 1.3675, val acc: 0.4236  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 25400] train loss: 1.3891, train acc: 0.3953, val loss: 1.3673, val acc: 0.4226  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 25420] train loss: 1.3899, train acc: 0.3959, val loss: 1.3664, val acc: 0.4239  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 25440] train loss: 1.3900, train acc: 0.3952, val loss: 1.3667, val acc: 0.4223  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 25460] train loss: 1.3914, train acc: 0.3924, val loss: 1.3664, val acc: 0.4226  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 25480] train loss: 1.3898, train acc: 0.3976, val loss: 1.3671, val acc: 0.4250  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 25500] train loss: 1.3864, train acc: 0.4001, val loss: 1.3662, val acc: 0.4226  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 25520] train loss: 1.3863, train acc: 0.3978, val loss: 1.3662, val acc: 0.4223  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 25540] train loss: 1.3895, train acc: 0.3934, val loss: 1.3657, val acc: 0.4239  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 25560] train loss: 1.3874, train acc: 0.3937, val loss: 1.3652, val acc: 0.4226  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 25580] train loss: 1.3883, train acc: 0.3961, val loss: 1.3655, val acc: 0.4246  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 25600] train loss: 1.3882, train acc: 0.3933, val loss: 1.3659, val acc: 0.4216  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 25620] train loss: 1.3837, train acc: 0.3978, val loss: 1.3644, val acc: 0.4250  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 25640] train loss: 1.3875, train acc: 0.3966, val loss: 1.3652, val acc: 0.4209  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 25660] train loss: 1.3873, train acc: 0.3958, val loss: 1.3650, val acc: 0.4202  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 25680] train loss: 1.3894, train acc: 0.3933, val loss: 1.3643, val acc: 0.4236  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 25700] train loss: 1.3884, train acc: 0.4030, val loss: 1.3646, val acc: 0.4206  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 25720] train loss: 1.3860, train acc: 0.4021, val loss: 1.3648, val acc: 0.4226  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 25740] train loss: 1.3828, train acc: 0.4049, val loss: 1.3636, val acc: 0.4226  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 25760] train loss: 1.3844, train acc: 0.4030, val loss: 1.3641, val acc: 0.4233  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 25780] train loss: 1.3850, train acc: 0.4062, val loss: 1.3642, val acc: 0.4223  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 25800] train loss: 1.3824, train acc: 0.4081, val loss: 1.3633, val acc: 0.4209  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 25820] train loss: 1.3837, train acc: 0.4071, val loss: 1.3650, val acc: 0.4250  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 25840] train loss: 1.3855, train acc: 0.4041, val loss: 1.3633, val acc: 0.4199  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 25860] train loss: 1.3845, train acc: 0.4044, val loss: 1.3626, val acc: 0.4206  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 25880] train loss: 1.3852, train acc: 0.4031, val loss: 1.3636, val acc: 0.4226  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 25900] train loss: 1.3842, train acc: 0.4106, val loss: 1.3646, val acc: 0.4280  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 25920] train loss: 1.3812, train acc: 0.4119, val loss: 1.3628, val acc: 0.4206  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 25940] train loss: 1.3823, train acc: 0.4113, val loss: 1.3625, val acc: 0.4196  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 25960] train loss: 1.3812, train acc: 0.4132, val loss: 1.3619, val acc: 0.4219  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 25980] train loss: 1.3821, train acc: 0.4142, val loss: 1.3623, val acc: 0.4185  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 26000] train loss: 1.3834, train acc: 0.4155, val loss: 1.3649, val acc: 0.4250  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 26020] train loss: 1.3827, train acc: 0.4157, val loss: 1.3636, val acc: 0.4260  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 26040] train loss: 1.3866, train acc: 0.4158, val loss: 1.3656, val acc: 0.4273  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 26060] train loss: 1.3838, train acc: 0.4169, val loss: 1.3625, val acc: 0.4229  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 26080] train loss: 1.3821, train acc: 0.4169, val loss: 1.3621, val acc: 0.4202  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 26100] train loss: 1.3823, train acc: 0.4153, val loss: 1.3618, val acc: 0.4219  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 26120] train loss: 1.3845, train acc: 0.4176, val loss: 1.3616, val acc: 0.4209  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 26140] train loss: 1.3810, train acc: 0.4160, val loss: 1.3626, val acc: 0.4192  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 26160] train loss: 1.3799, train acc: 0.4214, val loss: 1.3620, val acc: 0.4223  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 26180] train loss: 1.3794, train acc: 0.4190, val loss: 1.3620, val acc: 0.4239  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 26200] train loss: 1.3809, train acc: 0.4187, val loss: 1.3612, val acc: 0.4189  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 26220] train loss: 1.3828, train acc: 0.4168, val loss: 1.3625, val acc: 0.4273  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 26240] train loss: 1.3778, train acc: 0.4231, val loss: 1.3617, val acc: 0.4223  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 26260] train loss: 1.3800, train acc: 0.4200, val loss: 1.3608, val acc: 0.4219  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 26280] train loss: 1.3805, train acc: 0.4183, val loss: 1.3628, val acc: 0.4243  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 26300] train loss: 1.3832, train acc: 0.4180, val loss: 1.3610, val acc: 0.4196  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 26320] train loss: 1.3797, train acc: 0.4199, val loss: 1.3609, val acc: 0.4196  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 26340] train loss: 1.3773, train acc: 0.4203, val loss: 1.3598, val acc: 0.4226  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 26360] train loss: 1.3797, train acc: 0.4184, val loss: 1.3604, val acc: 0.4212  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 26380] train loss: 1.3778, train acc: 0.4200, val loss: 1.3600, val acc: 0.4199  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 26400] train loss: 1.3759, train acc: 0.4208, val loss: 1.3606, val acc: 0.4239  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 26420] train loss: 1.3800, train acc: 0.4216, val loss: 1.3607, val acc: 0.4212  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 26440] train loss: 1.3854, train acc: 0.4262, val loss: 1.3639, val acc: 0.4270  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 26460] train loss: 1.3816, train acc: 0.4228, val loss: 1.3608, val acc: 0.4236  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 26480] train loss: 1.3756, train acc: 0.4240, val loss: 1.3601, val acc: 0.4226  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 26500] train loss: 1.3757, train acc: 0.4247, val loss: 1.3599, val acc: 0.4229  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 26520] train loss: 1.3794, train acc: 0.4214, val loss: 1.3605, val acc: 0.4229  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 26540] train loss: 1.3766, train acc: 0.4235, val loss: 1.3611, val acc: 0.4236  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 26560] train loss: 1.3765, train acc: 0.4263, val loss: 1.3600, val acc: 0.4229  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 26580] train loss: 1.3762, train acc: 0.4255, val loss: 1.3587, val acc: 0.4206  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 26600] train loss: 1.3746, train acc: 0.4243, val loss: 1.3615, val acc: 0.4280  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 26620] train loss: 1.3777, train acc: 0.4211, val loss: 1.3587, val acc: 0.4236  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 26640] train loss: 1.3758, train acc: 0.4198, val loss: 1.3581, val acc: 0.4229  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 26660] train loss: 1.3786, train acc: 0.4216, val loss: 1.3585, val acc: 0.4209  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 26680] train loss: 1.3770, train acc: 0.4220, val loss: 1.3596, val acc: 0.4229  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 26700] train loss: 1.3761, train acc: 0.4213, val loss: 1.3603, val acc: 0.4256  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 26720] train loss: 1.3781, train acc: 0.4234, val loss: 1.3588, val acc: 0.4239  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 26740] train loss: 1.3751, train acc: 0.4252, val loss: 1.3579, val acc: 0.4223  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 26760] train loss: 1.3741, train acc: 0.4234, val loss: 1.3577, val acc: 0.4226  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 26780] train loss: 1.3769, train acc: 0.4255, val loss: 1.3571, val acc: 0.4223  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 26800] train loss: 1.3732, train acc: 0.4219, val loss: 1.3575, val acc: 0.4223  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 26820] train loss: 1.3780, train acc: 0.4274, val loss: 1.3607, val acc: 0.4246  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 26840] train loss: 1.3766, train acc: 0.4304, val loss: 1.3620, val acc: 0.4287  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 26860] train loss: 1.3762, train acc: 0.4255, val loss: 1.3578, val acc: 0.4223  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 26880] train loss: 1.3761, train acc: 0.4265, val loss: 1.3576, val acc: 0.4253  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 26900] train loss: 1.3750, train acc: 0.4272, val loss: 1.3581, val acc: 0.4266  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 26920] train loss: 1.3721, train acc: 0.4274, val loss: 1.3572, val acc: 0.4196  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 26940] train loss: 1.3742, train acc: 0.4299, val loss: 1.3588, val acc: 0.4233  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 26960] train loss: 1.3719, train acc: 0.4255, val loss: 1.3579, val acc: 0.4239  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 26980] train loss: 1.3710, train acc: 0.4250, val loss: 1.3571, val acc: 0.4229  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 27000] train loss: 1.3746, train acc: 0.4273, val loss: 1.3566, val acc: 0.4229  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 27020] train loss: 1.3731, train acc: 0.4278, val loss: 1.3552, val acc: 0.4243  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 27040] train loss: 1.3734, train acc: 0.4252, val loss: 1.3539, val acc: 0.4219  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 27060] train loss: 1.3716, train acc: 0.4279, val loss: 1.3542, val acc: 0.4236  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 27080] train loss: 1.3726, train acc: 0.4234, val loss: 1.3540, val acc: 0.4250  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 27100] train loss: 1.3707, train acc: 0.4278, val loss: 1.3549, val acc: 0.4253  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 27120] train loss: 1.3728, train acc: 0.4290, val loss: 1.3568, val acc: 0.4273  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 27140] train loss: 1.3757, train acc: 0.4239, val loss: 1.3543, val acc: 0.4239  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 27160] train loss: 1.3731, train acc: 0.4249, val loss: 1.3541, val acc: 0.4209  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 27180] train loss: 1.3718, train acc: 0.4297, val loss: 1.3562, val acc: 0.4256  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 27200] train loss: 1.3720, train acc: 0.4246, val loss: 1.3537, val acc: 0.4236  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 27220] train loss: 1.3686, train acc: 0.4293, val loss: 1.3572, val acc: 0.4260  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 27240] train loss: 1.3705, train acc: 0.4216, val loss: 1.3560, val acc: 0.4236  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 27260] train loss: 1.3713, train acc: 0.4229, val loss: 1.3541, val acc: 0.4229  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 27280] train loss: 1.3689, train acc: 0.4276, val loss: 1.3536, val acc: 0.4229  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 27300] train loss: 1.3716, train acc: 0.4261, val loss: 1.3552, val acc: 0.4250  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 27320] train loss: 1.3702, train acc: 0.4281, val loss: 1.3544, val acc: 0.4229  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 27340] train loss: 1.3682, train acc: 0.4297, val loss: 1.3545, val acc: 0.4226  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 27360] train loss: 1.3725, train acc: 0.4211, val loss: 1.3537, val acc: 0.4229  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 27380] train loss: 1.3696, train acc: 0.4255, val loss: 1.3552, val acc: 0.4246  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 27400] train loss: 1.3702, train acc: 0.4230, val loss: 1.3533, val acc: 0.4202  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 27420] train loss: 1.3702, train acc: 0.4295, val loss: 1.3543, val acc: 0.4229  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 27440] train loss: 1.3670, train acc: 0.4268, val loss: 1.3535, val acc: 0.4196  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 27460] train loss: 1.3705, train acc: 0.4268, val loss: 1.3556, val acc: 0.4280  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 27480] train loss: 1.3685, train acc: 0.4289, val loss: 1.3533, val acc: 0.4233  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 27500] train loss: 1.3660, train acc: 0.4248, val loss: 1.3529, val acc: 0.4229  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 27520] train loss: 1.3706, train acc: 0.4271, val loss: 1.3529, val acc: 0.4219  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 27540] train loss: 1.3750, train acc: 0.4214, val loss: 1.3530, val acc: 0.4212  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 27560] train loss: 1.3675, train acc: 0.4275, val loss: 1.3539, val acc: 0.4233  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 27580] train loss: 1.3693, train acc: 0.4268, val loss: 1.3530, val acc: 0.4243  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 27600] train loss: 1.3706, train acc: 0.4268, val loss: 1.3523, val acc: 0.4223  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 27620] train loss: 1.3654, train acc: 0.4282, val loss: 1.3524, val acc: 0.4233  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 27640] train loss: 1.3714, train acc: 0.4294, val loss: 1.3531, val acc: 0.4239  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 27660] train loss: 1.3680, train acc: 0.4246, val loss: 1.3532, val acc: 0.4223  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 27680] train loss: 1.3665, train acc: 0.4277, val loss: 1.3525, val acc: 0.4226  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 27700] train loss: 1.3690, train acc: 0.4257, val loss: 1.3532, val acc: 0.4216  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 27720] train loss: 1.3672, train acc: 0.4263, val loss: 1.3523, val acc: 0.4223  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 27740] train loss: 1.3705, train acc: 0.4254, val loss: 1.3522, val acc: 0.4233  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 27760] train loss: 1.3693, train acc: 0.4296, val loss: 1.3530, val acc: 0.4256  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 27780] train loss: 1.3668, train acc: 0.4280, val loss: 1.3523, val acc: 0.4239  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 27800] train loss: 1.3706, train acc: 0.4265, val loss: 1.3525, val acc: 0.4233  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 27820] train loss: 1.3695, train acc: 0.4260, val loss: 1.3536, val acc: 0.4209  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 27840] train loss: 1.3697, train acc: 0.4235, val loss: 1.3532, val acc: 0.4243  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 27860] train loss: 1.3640, train acc: 0.4278, val loss: 1.3538, val acc: 0.4223  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 27880] train loss: 1.3677, train acc: 0.4260, val loss: 1.3521, val acc: 0.4229  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 27900] train loss: 1.3649, train acc: 0.4287, val loss: 1.3531, val acc: 0.4236  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 27920] train loss: 1.3721, train acc: 0.4276, val loss: 1.3519, val acc: 0.4219  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 27940] train loss: 1.3702, train acc: 0.4242, val loss: 1.3529, val acc: 0.4216  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 27960] train loss: 1.3675, train acc: 0.4264, val loss: 1.3518, val acc: 0.4216  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 27980] train loss: 1.3662, train acc: 0.4292, val loss: 1.3530, val acc: 0.4216  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 28000] train loss: 1.3653, train acc: 0.4266, val loss: 1.3533, val acc: 0.4266  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 28020] train loss: 1.3680, train acc: 0.4259, val loss: 1.3540, val acc: 0.4300  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 28040] train loss: 1.3679, train acc: 0.4260, val loss: 1.3568, val acc: 0.4310  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 28060] train loss: 1.3697, train acc: 0.4240, val loss: 1.3512, val acc: 0.4239  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 28080] train loss: 1.3680, train acc: 0.4269, val loss: 1.3513, val acc: 0.4212  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 28100] train loss: 1.3654, train acc: 0.4266, val loss: 1.3512, val acc: 0.4243  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 28120] train loss: 1.3654, train acc: 0.4260, val loss: 1.3512, val acc: 0.4219  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 28140] train loss: 1.3646, train acc: 0.4284, val loss: 1.3520, val acc: 0.4253  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 28160] train loss: 1.3706, train acc: 0.4237, val loss: 1.3520, val acc: 0.4233  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 28180] train loss: 1.3636, train acc: 0.4278, val loss: 1.3507, val acc: 0.4226  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 28200] train loss: 1.3662, train acc: 0.4241, val loss: 1.3507, val acc: 0.4239  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 28220] train loss: 1.3656, train acc: 0.4237, val loss: 1.3507, val acc: 0.4236  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 28240] train loss: 1.3611, train acc: 0.4263, val loss: 1.3508, val acc: 0.4216  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 28260] train loss: 1.3689, train acc: 0.4250, val loss: 1.3509, val acc: 0.4243  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 28280] train loss: 1.3673, train acc: 0.4313, val loss: 1.3529, val acc: 0.4256  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 28300] train loss: 1.3644, train acc: 0.4266, val loss: 1.3504, val acc: 0.4246  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 28320] train loss: 1.3679, train acc: 0.4238, val loss: 1.3507, val acc: 0.4239  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 28340] train loss: 1.3641, train acc: 0.4265, val loss: 1.3528, val acc: 0.4199  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 28360] train loss: 1.3647, train acc: 0.4263, val loss: 1.3513, val acc: 0.4243  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 28380] train loss: 1.3730, train acc: 0.4311, val loss: 1.3583, val acc: 0.4300  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 28400] train loss: 1.3625, train acc: 0.4280, val loss: 1.3514, val acc: 0.4246  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 28420] train loss: 1.3640, train acc: 0.4289, val loss: 1.3509, val acc: 0.4219  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 28440] train loss: 1.3670, train acc: 0.4258, val loss: 1.3511, val acc: 0.4263  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 28460] train loss: 1.3628, train acc: 0.4314, val loss: 1.3533, val acc: 0.4266  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 28480] train loss: 1.3651, train acc: 0.4245, val loss: 1.3515, val acc: 0.4199  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 28500] train loss: 1.3609, train acc: 0.4297, val loss: 1.3504, val acc: 0.4253  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 28520] train loss: 1.3648, train acc: 0.4258, val loss: 1.3508, val acc: 0.4196  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 28540] train loss: 1.3644, train acc: 0.4299, val loss: 1.3501, val acc: 0.4239  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 28560] train loss: 1.3685, train acc: 0.4266, val loss: 1.3504, val acc: 0.4212  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 28580] train loss: 1.3672, train acc: 0.4268, val loss: 1.3506, val acc: 0.4253  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 28600] train loss: 1.3655, train acc: 0.4250, val loss: 1.3503, val acc: 0.4246  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 28620] train loss: 1.3640, train acc: 0.4247, val loss: 1.3501, val acc: 0.4212  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 28640] train loss: 1.3640, train acc: 0.4296, val loss: 1.3505, val acc: 0.4199  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 28660] train loss: 1.3670, train acc: 0.4283, val loss: 1.3504, val acc: 0.4223  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 28680] train loss: 1.3646, train acc: 0.4281, val loss: 1.3529, val acc: 0.4297  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 28700] train loss: 1.3647, train acc: 0.4277, val loss: 1.3515, val acc: 0.4219  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 28720] train loss: 1.3629, train acc: 0.4278, val loss: 1.3507, val acc: 0.4263  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 28740] train loss: 1.3674, train acc: 0.4276, val loss: 1.3506, val acc: 0.4219  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 28760] train loss: 1.3642, train acc: 0.4295, val loss: 1.3506, val acc: 0.4206  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 28780] train loss: 1.3666, train acc: 0.4243, val loss: 1.3497, val acc: 0.4243  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 28800] train loss: 1.3652, train acc: 0.4247, val loss: 1.3514, val acc: 0.4293  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 28820] train loss: 1.3657, train acc: 0.4254, val loss: 1.3497, val acc: 0.4223  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 28840] train loss: 1.3660, train acc: 0.4260, val loss: 1.3500, val acc: 0.4226  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 28860] train loss: 1.3635, train acc: 0.4262, val loss: 1.3497, val acc: 0.4250  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 28880] train loss: 1.3659, train acc: 0.4246, val loss: 1.3496, val acc: 0.4192  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 28900] train loss: 1.3645, train acc: 0.4277, val loss: 1.3497, val acc: 0.4229  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 28920] train loss: 1.3671, train acc: 0.4242, val loss: 1.3504, val acc: 0.4212  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 28940] train loss: 1.3635, train acc: 0.4280, val loss: 1.3498, val acc: 0.4196  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 28960] train loss: 1.3675, train acc: 0.4250, val loss: 1.3515, val acc: 0.4172  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 28980] train loss: 1.3643, train acc: 0.4259, val loss: 1.3508, val acc: 0.4226  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 29000] train loss: 1.3681, train acc: 0.4274, val loss: 1.3514, val acc: 0.4212  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 29020] train loss: 1.3618, train acc: 0.4284, val loss: 1.3498, val acc: 0.4250  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 29040] train loss: 1.3653, train acc: 0.4258, val loss: 1.3485, val acc: 0.4256  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 29060] train loss: 1.3627, train acc: 0.4242, val loss: 1.3490, val acc: 0.4233  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 29080] train loss: 1.3649, train acc: 0.4252, val loss: 1.3502, val acc: 0.4206  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 29100] train loss: 1.3622, train acc: 0.4279, val loss: 1.3483, val acc: 0.4243  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 29120] train loss: 1.3656, train acc: 0.4305, val loss: 1.3494, val acc: 0.4233  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 29140] train loss: 1.3628, train acc: 0.4294, val loss: 1.3488, val acc: 0.4253  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 29160] train loss: 1.3639, train acc: 0.4242, val loss: 1.3487, val acc: 0.4273  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 29180] train loss: 1.3631, train acc: 0.4247, val loss: 1.3489, val acc: 0.4287  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 29200] train loss: 1.3607, train acc: 0.4282, val loss: 1.3485, val acc: 0.4209  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 29220] train loss: 1.3693, train acc: 0.4289, val loss: 1.3504, val acc: 0.4185  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 29240] train loss: 1.3632, train acc: 0.4278, val loss: 1.3477, val acc: 0.4250  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 29260] train loss: 1.3642, train acc: 0.4294, val loss: 1.3485, val acc: 0.4253  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 29280] train loss: 1.3633, train acc: 0.4264, val loss: 1.3489, val acc: 0.4256  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 29300] train loss: 1.3655, train acc: 0.4299, val loss: 1.3484, val acc: 0.4196  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 29320] train loss: 1.3610, train acc: 0.4280, val loss: 1.3486, val acc: 0.4250  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 29340] train loss: 1.3644, train acc: 0.4282, val loss: 1.3513, val acc: 0.4273  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 29360] train loss: 1.3639, train acc: 0.4270, val loss: 1.3480, val acc: 0.4246  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 29380] train loss: 1.3668, train acc: 0.4231, val loss: 1.3496, val acc: 0.4253  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 29400] train loss: 1.3671, train acc: 0.4257, val loss: 1.3519, val acc: 0.4253  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 29420] train loss: 1.3617, train acc: 0.4261, val loss: 1.3483, val acc: 0.4250  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 29440] train loss: 1.3636, train acc: 0.4296, val loss: 1.3496, val acc: 0.4226  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 29460] train loss: 1.3698, train acc: 0.4247, val loss: 1.3501, val acc: 0.4307  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 29480] train loss: 1.3621, train acc: 0.4286, val loss: 1.3474, val acc: 0.4250  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 29500] train loss: 1.3648, train acc: 0.4268, val loss: 1.3480, val acc: 0.4216  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 29520] train loss: 1.3616, train acc: 0.4291, val loss: 1.3479, val acc: 0.4250  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 29540] train loss: 1.3608, train acc: 0.4278, val loss: 1.3480, val acc: 0.4216  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 29560] train loss: 1.3611, train acc: 0.4272, val loss: 1.3479, val acc: 0.4290  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 29580] train loss: 1.3678, train acc: 0.4227, val loss: 1.3480, val acc: 0.4189  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 29600] train loss: 1.3642, train acc: 0.4268, val loss: 1.3476, val acc: 0.4256  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 29620] train loss: 1.3628, train acc: 0.4284, val loss: 1.3479, val acc: 0.4246  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 29640] train loss: 1.3622, train acc: 0.4266, val loss: 1.3473, val acc: 0.4250  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 29660] train loss: 1.3600, train acc: 0.4302, val loss: 1.3477, val acc: 0.4219  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 29680] train loss: 1.3609, train acc: 0.4276, val loss: 1.3473, val acc: 0.4263  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 29700] train loss: 1.3607, train acc: 0.4291, val loss: 1.3478, val acc: 0.4196  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 29720] train loss: 1.3628, train acc: 0.4248, val loss: 1.3472, val acc: 0.4263  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 29740] train loss: 1.3595, train acc: 0.4273, val loss: 1.3472, val acc: 0.4256  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 29760] train loss: 1.3612, train acc: 0.4258, val loss: 1.3482, val acc: 0.4206  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 29780] train loss: 1.3630, train acc: 0.4298, val loss: 1.3473, val acc: 0.4260  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 29800] train loss: 1.3655, train acc: 0.4267, val loss: 1.3499, val acc: 0.4209  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 29820] train loss: 1.3667, train acc: 0.4286, val loss: 1.3480, val acc: 0.4209  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 29840] train loss: 1.3621, train acc: 0.4262, val loss: 1.3474, val acc: 0.4243  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 29860] train loss: 1.3625, train acc: 0.4286, val loss: 1.3470, val acc: 0.4239  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 29880] train loss: 1.3647, train acc: 0.4283, val loss: 1.3483, val acc: 0.4189  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 29900] train loss: 1.3626, train acc: 0.4282, val loss: 1.3473, val acc: 0.4263  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 29920] train loss: 1.3617, train acc: 0.4264, val loss: 1.3474, val acc: 0.4256  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 29940] train loss: 1.3593, train acc: 0.4321, val loss: 1.3471, val acc: 0.4253  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 29960] train loss: 1.3614, train acc: 0.4292, val loss: 1.3472, val acc: 0.4260  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 29980] train loss: 1.3639, train acc: 0.4297, val loss: 1.3471, val acc: 0.4209  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 30000] train loss: 1.3619, train acc: 0.4286, val loss: 1.3478, val acc: 0.4253  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 30020] train loss: 1.3652, train acc: 0.4287, val loss: 1.3515, val acc: 0.4246  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 30040] train loss: 1.3685, train acc: 0.4299, val loss: 1.3530, val acc: 0.4307  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 30060] train loss: 1.3624, train acc: 0.4314, val loss: 1.3468, val acc: 0.4223  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 30080] train loss: 1.3624, train acc: 0.4294, val loss: 1.3540, val acc: 0.4287  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 30100] train loss: 1.3652, train acc: 0.4286, val loss: 1.3478, val acc: 0.4233  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 30120] train loss: 1.3636, train acc: 0.4291, val loss: 1.3471, val acc: 0.4212  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 30140] train loss: 1.3623, train acc: 0.4247, val loss: 1.3473, val acc: 0.4226  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 30160] train loss: 1.3609, train acc: 0.4279, val loss: 1.3477, val acc: 0.4219  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 30180] train loss: 1.3684, train acc: 0.4255, val loss: 1.3476, val acc: 0.4209  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 30200] train loss: 1.3622, train acc: 0.4263, val loss: 1.3500, val acc: 0.4293  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 30220] train loss: 1.3615, train acc: 0.4284, val loss: 1.3470, val acc: 0.4253  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 30240] train loss: 1.3594, train acc: 0.4307, val loss: 1.3495, val acc: 0.4233  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 30260] train loss: 1.3624, train acc: 0.4280, val loss: 1.3507, val acc: 0.4277  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 30280] train loss: 1.3627, train acc: 0.4297, val loss: 1.3496, val acc: 0.4196  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 30300] train loss: 1.3636, train acc: 0.4280, val loss: 1.3486, val acc: 0.4250  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 30320] train loss: 1.3624, train acc: 0.4292, val loss: 1.3510, val acc: 0.4280  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 30340] train loss: 1.3668, train acc: 0.4263, val loss: 1.3488, val acc: 0.4256  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 30360] train loss: 1.3629, train acc: 0.4289, val loss: 1.3501, val acc: 0.4233  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 30380] train loss: 1.3644, train acc: 0.4256, val loss: 1.3498, val acc: 0.4209  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 30400] train loss: 1.3634, train acc: 0.4267, val loss: 1.3509, val acc: 0.4310  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 30420] train loss: 1.3611, train acc: 0.4299, val loss: 1.3510, val acc: 0.4212  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 30440] train loss: 1.3638, train acc: 0.4291, val loss: 1.3477, val acc: 0.4263  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 30460] train loss: 1.3658, train acc: 0.4276, val loss: 1.3491, val acc: 0.4202  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 30480] train loss: 1.3587, train acc: 0.4263, val loss: 1.3485, val acc: 0.4199  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 30500] train loss: 1.3630, train acc: 0.4242, val loss: 1.3488, val acc: 0.4209  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 30520] train loss: 1.3605, train acc: 0.4270, val loss: 1.3486, val acc: 0.4243  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 30540] train loss: 1.3615, train acc: 0.4250, val loss: 1.3485, val acc: 0.4206  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 30560] train loss: 1.3607, train acc: 0.4310, val loss: 1.3492, val acc: 0.4223  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 30580] train loss: 1.3599, train acc: 0.4304, val loss: 1.3483, val acc: 0.4202  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 30600] train loss: 1.3633, train acc: 0.4266, val loss: 1.3481, val acc: 0.4223  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 30620] train loss: 1.3658, train acc: 0.4284, val loss: 1.3490, val acc: 0.4199  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 30640] train loss: 1.3646, train acc: 0.4285, val loss: 1.3477, val acc: 0.4266  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 30660] train loss: 1.3613, train acc: 0.4292, val loss: 1.3482, val acc: 0.4273  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 30680] train loss: 1.3633, train acc: 0.4271, val loss: 1.3494, val acc: 0.4223  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 30700] train loss: 1.3626, train acc: 0.4268, val loss: 1.3495, val acc: 0.4192  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 30720] train loss: 1.3642, train acc: 0.4243, val loss: 1.3480, val acc: 0.4243  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 30740] train loss: 1.3660, train acc: 0.4280, val loss: 1.3525, val acc: 0.4260  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 30760] train loss: 1.3615, train acc: 0.4268, val loss: 1.3493, val acc: 0.4216  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 30780] train loss: 1.3595, train acc: 0.4286, val loss: 1.3488, val acc: 0.4246  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 30800] train loss: 1.3591, train acc: 0.4297, val loss: 1.3493, val acc: 0.4256  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 30820] train loss: 1.3647, train acc: 0.4285, val loss: 1.3523, val acc: 0.4277  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 30840] train loss: 1.3643, train acc: 0.4271, val loss: 1.3495, val acc: 0.4236  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 30860] train loss: 1.3693, train acc: 0.4202, val loss: 1.3515, val acc: 0.4320  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 30880] train loss: 1.3657, train acc: 0.4286, val loss: 1.3479, val acc: 0.4226  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 30900] train loss: 1.3617, train acc: 0.4255, val loss: 1.3486, val acc: 0.4212  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 30920] train loss: 1.3625, train acc: 0.4302, val loss: 1.3475, val acc: 0.4233  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 30940] train loss: 1.3638, train acc: 0.4266, val loss: 1.3501, val acc: 0.4243  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 30960] train loss: 1.3661, train acc: 0.4281, val loss: 1.3500, val acc: 0.4206  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 30980] train loss: 1.3599, train acc: 0.4289, val loss: 1.3478, val acc: 0.4253  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 31000] train loss: 1.3633, train acc: 0.4273, val loss: 1.3491, val acc: 0.4206  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 31020] train loss: 1.3604, train acc: 0.4296, val loss: 1.3479, val acc: 0.4202  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 31040] train loss: 1.3627, train acc: 0.4282, val loss: 1.3476, val acc: 0.4270  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 31060] train loss: 1.3587, train acc: 0.4262, val loss: 1.3470, val acc: 0.4253  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 31080] train loss: 1.3586, train acc: 0.4289, val loss: 1.3481, val acc: 0.4233  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 31100] train loss: 1.3610, train acc: 0.4242, val loss: 1.3471, val acc: 0.4253  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 31120] train loss: 1.3624, train acc: 0.4265, val loss: 1.3484, val acc: 0.4256  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 31140] train loss: 1.3617, train acc: 0.4261, val loss: 1.3473, val acc: 0.4260  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 31160] train loss: 1.3603, train acc: 0.4274, val loss: 1.3472, val acc: 0.4253  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 31180] train loss: 1.3643, train acc: 0.4263, val loss: 1.3479, val acc: 0.4219  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 31200] train loss: 1.3615, train acc: 0.4299, val loss: 1.3472, val acc: 0.4233  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 31220] train loss: 1.3654, train acc: 0.4281, val loss: 1.3489, val acc: 0.4216  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 31240] train loss: 1.3673, train acc: 0.4236, val loss: 1.3531, val acc: 0.4202  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 31260] train loss: 1.3880, train acc: 0.4177, val loss: 1.3676, val acc: 0.4192  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 31280] train loss: 1.3733, train acc: 0.4227, val loss: 1.3609, val acc: 0.4277  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 31300] train loss: 1.3649, train acc: 0.4248, val loss: 1.3561, val acc: 0.4243  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 31320] train loss: 1.3684, train acc: 0.4260, val loss: 1.3547, val acc: 0.4246  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 31340] train loss: 1.3659, train acc: 0.4232, val loss: 1.3553, val acc: 0.4260  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 31360] train loss: 1.3655, train acc: 0.4228, val loss: 1.3567, val acc: 0.4266  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 31380] train loss: 1.3681, train acc: 0.4278, val loss: 1.3552, val acc: 0.4233  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 31400] train loss: 1.3676, train acc: 0.4272, val loss: 1.3539, val acc: 0.4256  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 31420] train loss: 1.3661, train acc: 0.4290, val loss: 1.3530, val acc: 0.4226  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 31440] train loss: 1.3690, train acc: 0.4258, val loss: 1.3529, val acc: 0.4239  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 31460] train loss: 1.3639, train acc: 0.4299, val loss: 1.3533, val acc: 0.4223  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 31480] train loss: 1.3647, train acc: 0.4282, val loss: 1.3525, val acc: 0.4206  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 31500] train loss: 1.3679, train acc: 0.4224, val loss: 1.3518, val acc: 0.4236  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 31520] train loss: 1.3650, train acc: 0.4246, val loss: 1.3520, val acc: 0.4246  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 31540] train loss: 1.3637, train acc: 0.4268, val loss: 1.3515, val acc: 0.4266  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 31560] train loss: 1.3611, train acc: 0.4265, val loss: 1.3516, val acc: 0.4216  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 31580] train loss: 1.3605, train acc: 0.4243, val loss: 1.3507, val acc: 0.4226  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 31600] train loss: 1.3648, train acc: 0.4284, val loss: 1.3538, val acc: 0.4317  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 31620] train loss: 1.3623, train acc: 0.4257, val loss: 1.3507, val acc: 0.4223  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 31640] train loss: 1.3655, train acc: 0.4271, val loss: 1.3497, val acc: 0.4260  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 31660] train loss: 1.3617, train acc: 0.4278, val loss: 1.3501, val acc: 0.4280  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 31680] train loss: 1.3633, train acc: 0.4312, val loss: 1.3495, val acc: 0.4243  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 31700] train loss: 1.3620, train acc: 0.4249, val loss: 1.3500, val acc: 0.4212  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 31720] train loss: 1.3652, train acc: 0.4295, val loss: 1.3496, val acc: 0.4229  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 31740] train loss: 1.3619, train acc: 0.4258, val loss: 1.3490, val acc: 0.4206  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 31760] train loss: 1.3633, train acc: 0.4223, val loss: 1.3497, val acc: 0.4266  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 31780] train loss: 1.3609, train acc: 0.4250, val loss: 1.3495, val acc: 0.4196  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 31800] train loss: 1.3636, train acc: 0.4276, val loss: 1.3503, val acc: 0.4270  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 31820] train loss: 1.3618, train acc: 0.4275, val loss: 1.3490, val acc: 0.4202  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 31840] train loss: 1.3590, train acc: 0.4287, val loss: 1.3490, val acc: 0.4209  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 31860] train loss: 1.3651, train acc: 0.4271, val loss: 1.3494, val acc: 0.4192  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 31880] train loss: 1.3649, train acc: 0.4271, val loss: 1.3494, val acc: 0.4239  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 31900] train loss: 1.3649, train acc: 0.4299, val loss: 1.3498, val acc: 0.4185  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 31920] train loss: 1.3625, train acc: 0.4264, val loss: 1.3510, val acc: 0.4243  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 31940] train loss: 1.3630, train acc: 0.4245, val loss: 1.3484, val acc: 0.4253  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 31960] train loss: 1.3618, train acc: 0.4258, val loss: 1.3500, val acc: 0.4216  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 31980] train loss: 1.3578, train acc: 0.4289, val loss: 1.3487, val acc: 0.4229  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 32000] train loss: 1.3611, train acc: 0.4254, val loss: 1.3480, val acc: 0.4212  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 32020] train loss: 1.3686, train acc: 0.4224, val loss: 1.3592, val acc: 0.4307  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 32040] train loss: 1.3614, train acc: 0.4302, val loss: 1.3506, val acc: 0.4263  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 32060] train loss: 1.3615, train acc: 0.4290, val loss: 1.3478, val acc: 0.4202  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 32080] train loss: 1.3633, train acc: 0.4279, val loss: 1.3484, val acc: 0.4246  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 32100] train loss: 1.3628, train acc: 0.4290, val loss: 1.3477, val acc: 0.4202  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 32120] train loss: 1.3603, train acc: 0.4263, val loss: 1.3508, val acc: 0.4246  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 32140] train loss: 1.3690, train acc: 0.4308, val loss: 1.3483, val acc: 0.4199  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 32160] train loss: 1.3612, train acc: 0.4293, val loss: 1.3487, val acc: 0.4212  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 32180] train loss: 1.3624, train acc: 0.4293, val loss: 1.3497, val acc: 0.4236  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 32200] train loss: 1.3602, train acc: 0.4258, val loss: 1.3495, val acc: 0.4239  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 32220] train loss: 1.3618, train acc: 0.4244, val loss: 1.3532, val acc: 0.4236  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 32240] train loss: 1.3615, train acc: 0.4271, val loss: 1.3480, val acc: 0.4196  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 32260] train loss: 1.3601, train acc: 0.4271, val loss: 1.3475, val acc: 0.4233  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 32280] train loss: 1.3615, train acc: 0.4274, val loss: 1.3475, val acc: 0.4260  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 32300] train loss: 1.3636, train acc: 0.4291, val loss: 1.3485, val acc: 0.4189  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 32320] train loss: 1.3627, train acc: 0.4278, val loss: 1.3491, val acc: 0.4246  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 32340] train loss: 1.3621, train acc: 0.4280, val loss: 1.3499, val acc: 0.4256  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 32360] train loss: 1.3623, train acc: 0.4269, val loss: 1.3496, val acc: 0.4250  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 32380] train loss: 1.3626, train acc: 0.4293, val loss: 1.3473, val acc: 0.4239  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 32400] train loss: 1.3599, train acc: 0.4297, val loss: 1.3468, val acc: 0.4256  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 32420] train loss: 1.3600, train acc: 0.4303, val loss: 1.3479, val acc: 0.4246  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 32440] train loss: 1.3582, train acc: 0.4279, val loss: 1.3470, val acc: 0.4223  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 32460] train loss: 1.3621, train acc: 0.4298, val loss: 1.3487, val acc: 0.4185  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 32480] train loss: 1.3590, train acc: 0.4283, val loss: 1.3468, val acc: 0.4246  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 32500] train loss: 1.3615, train acc: 0.4280, val loss: 1.3477, val acc: 0.4233  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 32520] train loss: 1.3613, train acc: 0.4289, val loss: 1.3466, val acc: 0.4192  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 32540] train loss: 1.3599, train acc: 0.4303, val loss: 1.3463, val acc: 0.4253  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 32560] train loss: 1.3583, train acc: 0.4278, val loss: 1.3463, val acc: 0.4196  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 32580] train loss: 1.3583, train acc: 0.4302, val loss: 1.3461, val acc: 0.4226  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 32600] train loss: 1.3639, train acc: 0.4260, val loss: 1.3463, val acc: 0.4243  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 32620] train loss: 1.3619, train acc: 0.4290, val loss: 1.3474, val acc: 0.4189  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 32640] train loss: 1.3628, train acc: 0.4244, val loss: 1.3471, val acc: 0.4212  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 32660] train loss: 1.3601, train acc: 0.4295, val loss: 1.3537, val acc: 0.4320  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 32680] train loss: 1.3603, train acc: 0.4263, val loss: 1.3464, val acc: 0.4209  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 32700] train loss: 1.3605, train acc: 0.4242, val loss: 1.3468, val acc: 0.4243  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 32720] train loss: 1.3624, train acc: 0.4286, val loss: 1.3463, val acc: 0.4206  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 32740] train loss: 1.3609, train acc: 0.4289, val loss: 1.3460, val acc: 0.4199  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 32760] train loss: 1.3557, train acc: 0.4293, val loss: 1.3456, val acc: 0.4202  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 32780] train loss: 1.3584, train acc: 0.4294, val loss: 1.3455, val acc: 0.4209  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 32800] train loss: 1.3619, train acc: 0.4255, val loss: 1.3459, val acc: 0.4260  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 32820] train loss: 1.3583, train acc: 0.4268, val loss: 1.3460, val acc: 0.4175  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 32840] train loss: 1.3597, train acc: 0.4305, val loss: 1.3455, val acc: 0.4179  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 32860] train loss: 1.3585, train acc: 0.4265, val loss: 1.3452, val acc: 0.4192  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 32880] train loss: 1.3626, train acc: 0.4279, val loss: 1.3470, val acc: 0.4246  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 32900] train loss: 1.3636, train acc: 0.4278, val loss: 1.3457, val acc: 0.4236  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 32920] train loss: 1.3581, train acc: 0.4283, val loss: 1.3454, val acc: 0.4196  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 32940] train loss: 1.3640, train acc: 0.4266, val loss: 1.3458, val acc: 0.4216  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 32960] train loss: 1.3749, train acc: 0.4196, val loss: 1.3566, val acc: 0.4314  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 32980] train loss: 1.3631, train acc: 0.4272, val loss: 1.3493, val acc: 0.4132  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 33000] train loss: 1.3696, train acc: 0.4206, val loss: 1.3547, val acc: 0.4273  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 33020] train loss: 1.3674, train acc: 0.4284, val loss: 1.3521, val acc: 0.4250  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 33040] train loss: 1.3670, train acc: 0.4251, val loss: 1.3527, val acc: 0.4270  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 33060] train loss: 1.3654, train acc: 0.4295, val loss: 1.3514, val acc: 0.4297  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 33080] train loss: 1.3688, train acc: 0.4234, val loss: 1.3519, val acc: 0.4239  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 33100] train loss: 1.3646, train acc: 0.4249, val loss: 1.3518, val acc: 0.4290  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 33120] train loss: 1.3646, train acc: 0.4271, val loss: 1.3515, val acc: 0.4277  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 33140] train loss: 1.3648, train acc: 0.4265, val loss: 1.3513, val acc: 0.4229  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 33160] train loss: 1.3624, train acc: 0.4278, val loss: 1.3508, val acc: 0.4273  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 33180] train loss: 1.3625, train acc: 0.4265, val loss: 1.3509, val acc: 0.4277  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 33200] train loss: 1.3654, train acc: 0.4245, val loss: 1.3523, val acc: 0.4287  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 33220] train loss: 1.3652, train acc: 0.4296, val loss: 1.3510, val acc: 0.4277  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 33240] train loss: 1.3671, train acc: 0.4270, val loss: 1.3516, val acc: 0.4236  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 33260] train loss: 1.3685, train acc: 0.4211, val loss: 1.3508, val acc: 0.4290  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 33280] train loss: 1.3639, train acc: 0.4289, val loss: 1.3506, val acc: 0.4270  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 33300] train loss: 1.3633, train acc: 0.4277, val loss: 1.3521, val acc: 0.4266  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 33320] train loss: 1.3658, train acc: 0.4261, val loss: 1.3508, val acc: 0.4256  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 33340] train loss: 1.3648, train acc: 0.4226, val loss: 1.3503, val acc: 0.4277  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 33360] train loss: 1.3611, train acc: 0.4276, val loss: 1.3508, val acc: 0.4293  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 33380] train loss: 1.3641, train acc: 0.4257, val loss: 1.3507, val acc: 0.4250  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 33400] train loss: 1.3660, train acc: 0.4252, val loss: 1.3511, val acc: 0.4239  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 33420] train loss: 1.3643, train acc: 0.4271, val loss: 1.3508, val acc: 0.4229  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 33440] train loss: 1.3619, train acc: 0.4286, val loss: 1.3505, val acc: 0.4236  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 33460] train loss: 1.3638, train acc: 0.4265, val loss: 1.3504, val acc: 0.4266  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 33480] train loss: 1.3644, train acc: 0.4245, val loss: 1.3540, val acc: 0.4246  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 33500] train loss: 1.3630, train acc: 0.4283, val loss: 1.3499, val acc: 0.4256  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 33520] train loss: 1.3622, train acc: 0.4263, val loss: 1.3509, val acc: 0.4246  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 33540] train loss: 1.3634, train acc: 0.4254, val loss: 1.3509, val acc: 0.4266  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 33560] train loss: 1.3613, train acc: 0.4278, val loss: 1.3501, val acc: 0.4290  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 33580] train loss: 1.3623, train acc: 0.4279, val loss: 1.3510, val acc: 0.4270  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 33600] train loss: 1.3674, train acc: 0.4236, val loss: 1.3501, val acc: 0.4229  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 33620] train loss: 1.3604, train acc: 0.4270, val loss: 1.3498, val acc: 0.4233  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 33640] train loss: 1.3565, train acc: 0.4312, val loss: 1.3430, val acc: 0.4358  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 33660] train loss: 1.3643, train acc: 0.4223, val loss: 1.3392, val acc: 0.4358  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 33680] train loss: 1.3569, train acc: 0.4369, val loss: 1.3365, val acc: 0.4331  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 33700] train loss: 1.3486, train acc: 0.4357, val loss: 1.3371, val acc: 0.4273  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 33720] train loss: 1.3511, train acc: 0.4386, val loss: 1.3347, val acc: 0.4293  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 33740] train loss: 1.3518, train acc: 0.4404, val loss: 1.3399, val acc: 0.4202  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 33760] train loss: 1.3516, train acc: 0.4431, val loss: 1.3347, val acc: 0.4368  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 33780] train loss: 1.3506, train acc: 0.4338, val loss: 1.3340, val acc: 0.4347  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 33800] train loss: 1.3591, train acc: 0.4234, val loss: 1.3406, val acc: 0.4307  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 33820] train loss: 1.3504, train acc: 0.4385, val loss: 1.3342, val acc: 0.4398  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 33840] train loss: 1.3428, train acc: 0.4406, val loss: 1.3302, val acc: 0.4432  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 33860] train loss: 1.3469, train acc: 0.4409, val loss: 1.3301, val acc: 0.4435  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 33880] train loss: 1.3512, train acc: 0.4312, val loss: 1.3336, val acc: 0.4341  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 33900] train loss: 1.3463, train acc: 0.4396, val loss: 1.3296, val acc: 0.4465  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 33920] train loss: 1.3460, train acc: 0.4372, val loss: 1.3289, val acc: 0.4465  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 33940] train loss: 1.3458, train acc: 0.4345, val loss: 1.3291, val acc: 0.4418  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 33960] train loss: 1.3449, train acc: 0.4371, val loss: 1.3292, val acc: 0.4408  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 33980] train loss: 1.3453, train acc: 0.4353, val loss: 1.3296, val acc: 0.4435  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 34000] train loss: 1.3505, train acc: 0.4409, val loss: 1.3293, val acc: 0.4435  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 34020] train loss: 1.3465, train acc: 0.4414, val loss: 1.3279, val acc: 0.4492  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 34040] train loss: 1.3432, train acc: 0.4363, val loss: 1.3303, val acc: 0.4499  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 34060] train loss: 1.3501, train acc: 0.4410, val loss: 1.3366, val acc: 0.4415  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 34080] train loss: 1.3488, train acc: 0.4385, val loss: 1.3290, val acc: 0.4422  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 34100] train loss: 1.3463, train acc: 0.4359, val loss: 1.3271, val acc: 0.4445  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 34120] train loss: 1.3434, train acc: 0.4419, val loss: 1.3272, val acc: 0.4432  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 34140] train loss: 1.3438, train acc: 0.4398, val loss: 1.3292, val acc: 0.4405  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 34160] train loss: 1.3440, train acc: 0.4398, val loss: 1.3342, val acc: 0.4320  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 34180] train loss: 1.3425, train acc: 0.4396, val loss: 1.3279, val acc: 0.4465  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 34200] train loss: 1.3457, train acc: 0.4380, val loss: 1.3273, val acc: 0.4482  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 34220] train loss: 1.3441, train acc: 0.4436, val loss: 1.3264, val acc: 0.4449  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 34240] train loss: 1.3472, train acc: 0.4419, val loss: 1.3284, val acc: 0.4378  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 34260] train loss: 1.3475, train acc: 0.4346, val loss: 1.3281, val acc: 0.4438  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 34280] train loss: 1.3461, train acc: 0.4426, val loss: 1.3266, val acc: 0.4432  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 34300] train loss: 1.3425, train acc: 0.4398, val loss: 1.3296, val acc: 0.4496  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 34320] train loss: 1.3499, train acc: 0.4336, val loss: 1.3381, val acc: 0.4415  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 34340] train loss: 1.3526, train acc: 0.4392, val loss: 1.3385, val acc: 0.4395  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 34360] train loss: 1.6056, train acc: 0.2343, val loss: 1.5563, val acc: 0.2563  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 34380] train loss: 1.4759, train acc: 0.3302, val loss: 1.4656, val acc: 0.3575  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 34400] train loss: 1.4498, train acc: 0.3793, val loss: 1.4480, val acc: 0.3862  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 34420] train loss: 1.4497, train acc: 0.3659, val loss: 1.4437, val acc: 0.3757  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 34440] train loss: 1.4473, train acc: 0.3573, val loss: 1.4418, val acc: 0.3599  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 34460] train loss: 1.4438, train acc: 0.3702, val loss: 1.4413, val acc: 0.3653  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 34480] train loss: 1.4413, train acc: 0.3725, val loss: 1.4397, val acc: 0.3666  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 34500] train loss: 1.4418, train acc: 0.3610, val loss: 1.4400, val acc: 0.3568  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 34520] train loss: 1.4412, train acc: 0.3540, val loss: 1.4380, val acc: 0.3568  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 34540] train loss: 1.4388, train acc: 0.3490, val loss: 1.4373, val acc: 0.3548  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 34560] train loss: 1.4408, train acc: 0.3464, val loss: 1.4367, val acc: 0.3848  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 34580] train loss: 1.4374, train acc: 0.3452, val loss: 1.4363, val acc: 0.3646  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 34600] train loss: 1.4360, train acc: 0.3563, val loss: 1.4360, val acc: 0.3565  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 34620] train loss: 1.4368, train acc: 0.3399, val loss: 1.4347, val acc: 0.3696  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 34640] train loss: 1.4028, train acc: 0.3931, val loss: 1.5988, val acc: 0.2411  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 34660] train loss: 1.5462, train acc: 0.3024, val loss: 1.4722, val acc: 0.2921  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 34680] train loss: 1.4513, train acc: 0.3686, val loss: 1.4145, val acc: 0.4098  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 34700] train loss: 1.3773, train acc: 0.4146, val loss: 1.3626, val acc: 0.4229  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 34720] train loss: 1.3611, train acc: 0.4383, val loss: 1.3463, val acc: 0.4293  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 34740] train loss: 1.3558, train acc: 0.4401, val loss: 1.3395, val acc: 0.4428  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 34760] train loss: 1.3532, train acc: 0.4393, val loss: 1.3431, val acc: 0.4398  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 34780] train loss: 1.3558, train acc: 0.4343, val loss: 1.3392, val acc: 0.4388  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 34800] train loss: 1.3527, train acc: 0.4388, val loss: 1.3358, val acc: 0.4425  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 34820] train loss: 1.3493, train acc: 0.4420, val loss: 1.3368, val acc: 0.4465  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 34840] train loss: 1.3458, train acc: 0.4409, val loss: 1.3341, val acc: 0.4476  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 34860] train loss: 1.3439, train acc: 0.4417, val loss: 1.3315, val acc: 0.4449  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 34880] train loss: 1.3436, train acc: 0.4448, val loss: 1.3302, val acc: 0.4509  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 34900] train loss: 1.3492, train acc: 0.4409, val loss: 1.3310, val acc: 0.4540  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 34920] train loss: 1.3465, train acc: 0.4426, val loss: 1.3325, val acc: 0.4509  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 34940] train loss: 1.3412, train acc: 0.4429, val loss: 1.3335, val acc: 0.4492  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 34960] train loss: 1.3454, train acc: 0.4411, val loss: 1.3308, val acc: 0.4489  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 34980] train loss: 1.3457, train acc: 0.4477, val loss: 1.3278, val acc: 0.4506  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 35000] train loss: 1.3408, train acc: 0.4489, val loss: 1.3244, val acc: 0.4519  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 35020] train loss: 1.3411, train acc: 0.4448, val loss: 1.3218, val acc: 0.4506  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 35040] train loss: 1.3405, train acc: 0.4452, val loss: 1.3221, val acc: 0.4499  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 35060] train loss: 1.3368, train acc: 0.4487, val loss: 1.3222, val acc: 0.4540  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 35080] train loss: 1.3349, train acc: 0.4454, val loss: 1.3218, val acc: 0.4533  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 35100] train loss: 1.3329, train acc: 0.4469, val loss: 1.3200, val acc: 0.4560  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 35120] train loss: 1.3363, train acc: 0.4437, val loss: 1.3190, val acc: 0.4533  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 35140] train loss: 1.3329, train acc: 0.4498, val loss: 1.3195, val acc: 0.4560  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 35160] train loss: 1.3368, train acc: 0.4448, val loss: 1.3191, val acc: 0.4543  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 35180] train loss: 1.3345, train acc: 0.4495, val loss: 1.3211, val acc: 0.4590  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 35200] train loss: 1.3357, train acc: 0.4465, val loss: 1.3179, val acc: 0.4489  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 35220] train loss: 1.3331, train acc: 0.4458, val loss: 1.3179, val acc: 0.4486  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 35240] train loss: 1.3348, train acc: 0.4460, val loss: 1.3225, val acc: 0.4543  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 35260] train loss: 1.3355, train acc: 0.4482, val loss: 1.3162, val acc: 0.4513  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 35280] train loss: 1.3345, train acc: 0.4458, val loss: 1.3207, val acc: 0.4526  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 35300] train loss: 1.3335, train acc: 0.4466, val loss: 1.3182, val acc: 0.4503  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 35320] train loss: 1.3344, train acc: 0.4470, val loss: 1.3178, val acc: 0.4509  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 35340] train loss: 1.3380, train acc: 0.4495, val loss: 1.3191, val acc: 0.4553  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 35360] train loss: 1.3285, train acc: 0.4463, val loss: 1.3186, val acc: 0.4418  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 35380] train loss: 1.3310, train acc: 0.4486, val loss: 1.3263, val acc: 0.4526  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 35400] train loss: 1.3291, train acc: 0.4476, val loss: 1.3211, val acc: 0.4556  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 35420] train loss: 1.3342, train acc: 0.4453, val loss: 1.3177, val acc: 0.4503  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 35440] train loss: 1.3298, train acc: 0.4466, val loss: 1.3164, val acc: 0.4449  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 35460] train loss: 1.3332, train acc: 0.4459, val loss: 1.3151, val acc: 0.4563  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 35480] train loss: 1.3263, train acc: 0.4500, val loss: 1.3175, val acc: 0.4614  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 35500] train loss: 1.3320, train acc: 0.4499, val loss: 1.3167, val acc: 0.4482  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 35520] train loss: 1.3277, train acc: 0.4477, val loss: 1.3138, val acc: 0.4550  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 35540] train loss: 1.3276, train acc: 0.4452, val loss: 1.3132, val acc: 0.4563  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 35560] train loss: 1.3304, train acc: 0.4479, val loss: 1.3150, val acc: 0.4415  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 35580] train loss: 1.3451, train acc: 0.4392, val loss: 1.3225, val acc: 0.4374  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 35600] train loss: 1.3348, train acc: 0.4438, val loss: 1.3139, val acc: 0.4503  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 35620] train loss: 1.3456, train acc: 0.4258, val loss: 1.3253, val acc: 0.4476  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 35640] train loss: 1.5076, train acc: 0.2984, val loss: 1.4601, val acc: 0.3788  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 35660] train loss: 1.3987, train acc: 0.4078, val loss: 1.3691, val acc: 0.4317  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 35680] train loss: 1.3600, train acc: 0.4331, val loss: 1.3391, val acc: 0.4344  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 35700] train loss: 1.3496, train acc: 0.4349, val loss: 1.3310, val acc: 0.4523  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 35720] train loss: 1.3345, train acc: 0.4448, val loss: 1.3113, val acc: 0.4644  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 35740] train loss: 1.3317, train acc: 0.4460, val loss: 1.3100, val acc: 0.4610  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 35760] train loss: 1.3283, train acc: 0.4475, val loss: 1.3097, val acc: 0.4624  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 35780] train loss: 1.3262, train acc: 0.4495, val loss: 1.3101, val acc: 0.4631  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 35800] train loss: 1.3260, train acc: 0.4511, val loss: 1.3081, val acc: 0.4668  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 35820] train loss: 1.3240, train acc: 0.4504, val loss: 1.3082, val acc: 0.4648  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 35840] train loss: 1.3262, train acc: 0.4486, val loss: 1.3078, val acc: 0.4654  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 35860] train loss: 1.3279, train acc: 0.4506, val loss: 1.3075, val acc: 0.4641  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 35880] train loss: 1.3273, train acc: 0.4464, val loss: 1.3072, val acc: 0.4614  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 35900] train loss: 1.3247, train acc: 0.4479, val loss: 1.3102, val acc: 0.4617  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 35920] train loss: 1.3254, train acc: 0.4500, val loss: 1.3086, val acc: 0.4583  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 35940] train loss: 1.3290, train acc: 0.4489, val loss: 1.3067, val acc: 0.4634  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 35960] train loss: 1.3255, train acc: 0.4488, val loss: 1.3072, val acc: 0.4590  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 35980] train loss: 1.3257, train acc: 0.4467, val loss: 1.3083, val acc: 0.4617  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 36000] train loss: 1.3224, train acc: 0.4473, val loss: 1.3090, val acc: 0.4590  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 36020] train loss: 1.5261, train acc: 0.2967, val loss: 1.5378, val acc: 0.2556  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 36040] train loss: 1.3813, train acc: 0.3757, val loss: 1.3436, val acc: 0.4388  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 36060] train loss: 1.3590, train acc: 0.4050, val loss: 1.3378, val acc: 0.4243  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 36080] train loss: 1.3564, train acc: 0.4106, val loss: 1.3360, val acc: 0.4277  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 36100] train loss: 1.3650, train acc: 0.3947, val loss: 1.3576, val acc: 0.4084  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 36120] train loss: 1.3555, train acc: 0.4272, val loss: 1.3400, val acc: 0.4293  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 36140] train loss: 1.3808, train acc: 0.3860, val loss: 1.3652, val acc: 0.4226  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 36160] train loss: 1.3594, train acc: 0.3992, val loss: 1.3395, val acc: 0.4094  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 36180] train loss: 1.3440, train acc: 0.4091, val loss: 1.3324, val acc: 0.4152  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 36200] train loss: 1.3509, train acc: 0.4227, val loss: 1.3343, val acc: 0.4283  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 36220] train loss: 1.4536, train acc: 0.3368, val loss: 1.4426, val acc: 0.3204  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 36240] train loss: 1.4288, train acc: 0.3761, val loss: 1.3978, val acc: 0.3781  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 36260] train loss: 1.3853, train acc: 0.4104, val loss: 1.3627, val acc: 0.4280  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 36280] train loss: 1.3767, train acc: 0.4231, val loss: 1.3522, val acc: 0.4290  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 36300] train loss: 1.3746, train acc: 0.4250, val loss: 1.3472, val acc: 0.4287  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 36320] train loss: 1.3682, train acc: 0.4277, val loss: 1.3451, val acc: 0.4320  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 36340] train loss: 1.3658, train acc: 0.4267, val loss: 1.3450, val acc: 0.4344  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 36360] train loss: 1.3629, train acc: 0.4289, val loss: 1.3453, val acc: 0.4341  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 36380] train loss: 1.3669, train acc: 0.4273, val loss: 1.3445, val acc: 0.4314  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 36400] train loss: 1.3643, train acc: 0.4318, val loss: 1.3440, val acc: 0.4314  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 36420] train loss: 1.3624, train acc: 0.4273, val loss: 1.3446, val acc: 0.4320  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 36440] train loss: 1.3636, train acc: 0.4310, val loss: 1.3432, val acc: 0.4334  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 36460] train loss: 1.3640, train acc: 0.4278, val loss: 1.3440, val acc: 0.4337  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 36480] train loss: 1.3669, train acc: 0.4268, val loss: 1.3434, val acc: 0.4331  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 36500] train loss: 1.3609, train acc: 0.4286, val loss: 1.3439, val acc: 0.4331  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 36520] train loss: 1.3623, train acc: 0.4282, val loss: 1.3431, val acc: 0.4337  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 36540] train loss: 1.3589, train acc: 0.4286, val loss: 1.3434, val acc: 0.4337  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 36560] train loss: 1.3608, train acc: 0.4286, val loss: 1.3430, val acc: 0.4317  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 36580] train loss: 1.3592, train acc: 0.4319, val loss: 1.3435, val acc: 0.4347  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 36600] train loss: 1.3576, train acc: 0.4305, val loss: 1.3432, val acc: 0.4324  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 36620] train loss: 1.3633, train acc: 0.4267, val loss: 1.3430, val acc: 0.4327  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 36640] train loss: 1.3622, train acc: 0.4315, val loss: 1.3427, val acc: 0.4317  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 36660] train loss: 1.3596, train acc: 0.4309, val loss: 1.3430, val acc: 0.4361  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 36680] train loss: 1.3588, train acc: 0.4329, val loss: 1.3419, val acc: 0.4317  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 36700] train loss: 1.3612, train acc: 0.4322, val loss: 1.3426, val acc: 0.4344  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 36720] train loss: 1.3630, train acc: 0.4291, val loss: 1.3419, val acc: 0.4300  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 36740] train loss: 1.3623, train acc: 0.4301, val loss: 1.3424, val acc: 0.4341  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 36760] train loss: 1.3588, train acc: 0.4295, val loss: 1.3425, val acc: 0.4354  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 36780] train loss: 1.3588, train acc: 0.4323, val loss: 1.3422, val acc: 0.4320  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 36800] train loss: 1.3591, train acc: 0.4328, val loss: 1.3419, val acc: 0.4307  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 36820] train loss: 1.3625, train acc: 0.4298, val loss: 1.3426, val acc: 0.4341  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 36840] train loss: 1.3573, train acc: 0.4325, val loss: 1.3433, val acc: 0.4341  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 36860] train loss: 1.3613, train acc: 0.4330, val loss: 1.3416, val acc: 0.4304  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 36880] train loss: 1.3614, train acc: 0.4274, val loss: 1.3419, val acc: 0.4324  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 36900] train loss: 1.3568, train acc: 0.4311, val loss: 1.3416, val acc: 0.4300  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 36920] train loss: 1.3608, train acc: 0.4287, val loss: 1.3423, val acc: 0.4331  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 36940] train loss: 1.3606, train acc: 0.4312, val loss: 1.3427, val acc: 0.4334  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 36960] train loss: 1.3606, train acc: 0.4278, val loss: 1.3425, val acc: 0.4351  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 36980] train loss: 1.3600, train acc: 0.4321, val loss: 1.3410, val acc: 0.4331  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 37000] train loss: 1.3623, train acc: 0.4271, val loss: 1.3419, val acc: 0.4334  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 37020] train loss: 1.3567, train acc: 0.4291, val loss: 1.3416, val acc: 0.4331  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 37040] train loss: 1.3571, train acc: 0.4313, val loss: 1.3436, val acc: 0.4341  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 37060] train loss: 1.3596, train acc: 0.4328, val loss: 1.3415, val acc: 0.4347  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 37080] train loss: 1.3601, train acc: 0.4328, val loss: 1.3413, val acc: 0.4344  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 37100] train loss: 1.3594, train acc: 0.4321, val loss: 1.3414, val acc: 0.4361  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 37120] train loss: 1.3582, train acc: 0.4309, val loss: 1.3427, val acc: 0.4307  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 37140] train loss: 1.3586, train acc: 0.4308, val loss: 1.3423, val acc: 0.4337  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 37160] train loss: 1.3582, train acc: 0.4320, val loss: 1.3415, val acc: 0.4317  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 37180] train loss: 1.3647, train acc: 0.4315, val loss: 1.3421, val acc: 0.4347  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 37200] train loss: 1.3569, train acc: 0.4334, val loss: 1.3407, val acc: 0.4280  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 37220] train loss: 1.3605, train acc: 0.4307, val loss: 1.3408, val acc: 0.4277  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 37240] train loss: 1.3558, train acc: 0.4312, val loss: 1.3415, val acc: 0.4327  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 37260] train loss: 1.3579, train acc: 0.4311, val loss: 1.3425, val acc: 0.4307  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 37280] train loss: 1.3610, train acc: 0.4343, val loss: 1.3412, val acc: 0.4277  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 37300] train loss: 1.3617, train acc: 0.4333, val loss: 1.3414, val acc: 0.4283  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 37320] train loss: 1.3562, train acc: 0.4324, val loss: 1.3414, val acc: 0.4337  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 37340] train loss: 1.3584, train acc: 0.4312, val loss: 1.3435, val acc: 0.4304  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 37360] train loss: 1.3581, train acc: 0.4320, val loss: 1.3408, val acc: 0.4304  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 37380] train loss: 1.3577, train acc: 0.4358, val loss: 1.3408, val acc: 0.4320  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 37400] train loss: 1.3575, train acc: 0.4315, val loss: 1.3410, val acc: 0.4324  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 37420] train loss: 1.3558, train acc: 0.4341, val loss: 1.3424, val acc: 0.4358  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 37440] train loss: 1.3548, train acc: 0.4323, val loss: 1.3413, val acc: 0.4297  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 37460] train loss: 1.3581, train acc: 0.4305, val loss: 1.3412, val acc: 0.4277  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 37480] train loss: 1.3590, train acc: 0.4320, val loss: 1.3413, val acc: 0.4273  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 37500] train loss: 1.3587, train acc: 0.4316, val loss: 1.3434, val acc: 0.4347  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 37520] train loss: 1.3599, train acc: 0.4318, val loss: 1.3412, val acc: 0.4260  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 37540] train loss: 1.3565, train acc: 0.4351, val loss: 1.3411, val acc: 0.4334  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 37560] train loss: 1.3570, train acc: 0.4349, val loss: 1.3400, val acc: 0.4334  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 37580] train loss: 1.3546, train acc: 0.4330, val loss: 1.3404, val acc: 0.4307  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 37600] train loss: 1.3578, train acc: 0.4296, val loss: 1.3418, val acc: 0.4331  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 37620] train loss: 1.3572, train acc: 0.4284, val loss: 1.3411, val acc: 0.4331  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 37640] train loss: 1.3555, train acc: 0.4320, val loss: 1.3410, val acc: 0.4317  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 37660] train loss: 1.3594, train acc: 0.4292, val loss: 1.3418, val acc: 0.4324  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 37680] train loss: 1.3607, train acc: 0.4322, val loss: 1.3477, val acc: 0.4337  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 37700] train loss: 1.3567, train acc: 0.4339, val loss: 1.3415, val acc: 0.4287  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 37720] train loss: 1.3549, train acc: 0.4332, val loss: 1.3404, val acc: 0.4341  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 37740] train loss: 1.3530, train acc: 0.4336, val loss: 1.3409, val acc: 0.4327  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 37760] train loss: 1.3653, train acc: 0.4307, val loss: 1.3437, val acc: 0.4290  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 37780] train loss: 1.3661, train acc: 0.4312, val loss: 1.3608, val acc: 0.4236  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 37800] train loss: 1.3571, train acc: 0.4284, val loss: 1.3423, val acc: 0.4354  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 37820] train loss: 1.3581, train acc: 0.4290, val loss: 1.3425, val acc: 0.4347  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 37840] train loss: 1.3556, train acc: 0.4284, val loss: 1.3417, val acc: 0.4401  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 37860] train loss: 1.3579, train acc: 0.4276, val loss: 1.3405, val acc: 0.4354  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 37880] train loss: 1.3499, train acc: 0.4339, val loss: 1.3399, val acc: 0.4317  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 37900] train loss: 1.3414, train acc: 0.4351, val loss: 1.3236, val acc: 0.4513  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 37920] train loss: 1.3398, train acc: 0.4334, val loss: 1.3237, val acc: 0.4435  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 37940] train loss: 1.3352, train acc: 0.4377, val loss: 1.3231, val acc: 0.4428  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 37960] train loss: 1.3379, train acc: 0.4361, val loss: 1.3220, val acc: 0.4445  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 37980] train loss: 1.3355, train acc: 0.4369, val loss: 1.3228, val acc: 0.4425  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 38000] train loss: 1.3354, train acc: 0.4361, val loss: 1.3205, val acc: 0.4432  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 38020] train loss: 1.3347, train acc: 0.4375, val loss: 1.3209, val acc: 0.4445  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 38040] train loss: 1.3368, train acc: 0.4333, val loss: 1.3214, val acc: 0.4438  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 38060] train loss: 1.3360, train acc: 0.4370, val loss: 1.3209, val acc: 0.4435  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 38080] train loss: 1.3332, train acc: 0.4359, val loss: 1.3221, val acc: 0.4347  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 38100] train loss: 1.3409, train acc: 0.4313, val loss: 1.2984, val acc: 0.4550  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 38120] train loss: 1.3066, train acc: 0.4500, val loss: 1.2918, val acc: 0.4550  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 38140] train loss: 1.3180, train acc: 0.4540, val loss: 1.3073, val acc: 0.4553  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 38160] train loss: 1.5104, train acc: 0.2252, val loss: 1.5385, val acc: 0.2351  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 38180] train loss: 1.4502, train acc: 0.3316, val loss: 1.4277, val acc: 0.2998  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 38200] train loss: 1.4358, train acc: 0.3573, val loss: 1.4108, val acc: 0.3612  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 38220] train loss: 1.4164, train acc: 0.3461, val loss: 1.4028, val acc: 0.3342  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 38240] train loss: 1.4156, train acc: 0.3676, val loss: 1.4008, val acc: 0.3575  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 38260] train loss: 1.4117, train acc: 0.3697, val loss: 1.4001, val acc: 0.3164  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 38280] train loss: 1.4087, train acc: 0.3581, val loss: 1.4007, val acc: 0.3646  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 38300] train loss: 1.3844, train acc: 0.3953, val loss: 1.3737, val acc: 0.3929  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 38320] train loss: 1.3777, train acc: 0.3971, val loss: 1.3648, val acc: 0.3963  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 38340] train loss: 1.3808, train acc: 0.4007, val loss: 1.3636, val acc: 0.3990  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 38360] train loss: 1.3602, train acc: 0.4051, val loss: 1.3455, val acc: 0.4047  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 38380] train loss: 1.3360, train acc: 0.4190, val loss: 1.3214, val acc: 0.4290  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 38400] train loss: 1.3456, train acc: 0.4129, val loss: 1.3287, val acc: 0.4256  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 38420] train loss: 1.3379, train acc: 0.4192, val loss: 1.3263, val acc: 0.4226  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 38440] train loss: 1.3228, train acc: 0.4209, val loss: 1.3152, val acc: 0.4381  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 38460] train loss: 1.3162, train acc: 0.4280, val loss: 1.2992, val acc: 0.4307  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 38480] train loss: 1.3091, train acc: 0.4247, val loss: 1.2916, val acc: 0.4320  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 38500] train loss: 1.3295, train acc: 0.4285, val loss: 1.2851, val acc: 0.4378  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 38520] train loss: 1.3643, train acc: 0.4190, val loss: 1.3422, val acc: 0.4337  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 38540] train loss: 1.3137, train acc: 0.4247, val loss: 1.2976, val acc: 0.4337  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 38560] train loss: 1.3037, train acc: 0.4280, val loss: 1.2903, val acc: 0.4368  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 38580] train loss: 1.2953, train acc: 0.4320, val loss: 1.2853, val acc: 0.4384  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 38600] train loss: 1.2906, train acc: 0.4376, val loss: 1.2788, val acc: 0.4395  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 38620] train loss: 1.2870, train acc: 0.4401, val loss: 1.2756, val acc: 0.4405  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 38640] train loss: 1.2884, train acc: 0.4371, val loss: 1.2759, val acc: 0.4391  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 38660] train loss: 1.2872, train acc: 0.4387, val loss: 1.2764, val acc: 0.4445  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 38680] train loss: 1.2855, train acc: 0.4403, val loss: 1.2719, val acc: 0.4398  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 38700] train loss: 1.2844, train acc: 0.4409, val loss: 1.2720, val acc: 0.4459  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 38720] train loss: 1.2837, train acc: 0.4414, val loss: 1.2717, val acc: 0.4452  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 38740] train loss: 1.2840, train acc: 0.4470, val loss: 1.2706, val acc: 0.4442  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 38760] train loss: 1.2821, train acc: 0.4467, val loss: 1.2706, val acc: 0.4489  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 38780] train loss: 1.2832, train acc: 0.4439, val loss: 1.2735, val acc: 0.4405  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 38800] train loss: 1.2841, train acc: 0.4389, val loss: 1.2775, val acc: 0.4553  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 38820] train loss: 1.2863, train acc: 0.4521, val loss: 1.2677, val acc: 0.4543  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 38840] train loss: 1.2761, train acc: 0.4559, val loss: 1.2601, val acc: 0.4627  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 38860] train loss: 1.2753, train acc: 0.4625, val loss: 1.2557, val acc: 0.4631  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 38880] train loss: 1.2709, train acc: 0.4734, val loss: 1.2533, val acc: 0.4742  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 38900] train loss: 1.2660, train acc: 0.4725, val loss: 1.2510, val acc: 0.4668  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 38920] train loss: 1.2749, train acc: 0.4761, val loss: 1.2585, val acc: 0.4735  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 38940] train loss: 1.2654, train acc: 0.4766, val loss: 1.3157, val acc: 0.4243  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 38960] train loss: 1.2931, train acc: 0.4635, val loss: 1.3887, val acc: 0.4287  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 38980] train loss: 1.2795, train acc: 0.4562, val loss: 1.2489, val acc: 0.4637  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 39000] train loss: 1.2711, train acc: 0.4777, val loss: 1.2394, val acc: 0.4641  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 39020] train loss: 1.2247, train acc: 0.4756, val loss: 1.2086, val acc: 0.5066  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 39040] train loss: 1.2213, train acc: 0.4834, val loss: 1.2013, val acc: 0.4890  (best train acc: 0.5084, best val acc: 0.5568, best train loss: 1.1912  @ epoch 20210 )\n",
      "[Epoch: 39060] train loss: 1.1740, train acc: 0.5258, val loss: 1.1637, val acc: 0.5265  (best train acc: 0.5266, best val acc: 0.5568, best train loss: 1.1740  @ epoch 39060 )\n",
      "[Epoch: 39080] train loss: 1.1624, train acc: 0.5312, val loss: 1.1590, val acc: 0.5120  (best train acc: 0.5385, best val acc: 0.5568, best train loss: 1.1545  @ epoch 39079 )\n",
      "[Epoch: 39100] train loss: 1.3047, train acc: 0.4519, val loss: 1.2332, val acc: 0.4631  (best train acc: 0.5420, best val acc: 0.5568, best train loss: 1.1545  @ epoch 39079 )\n",
      "[Epoch: 39120] train loss: 1.1882, train acc: 0.5231, val loss: 1.1988, val acc: 0.5201  (best train acc: 0.5420, best val acc: 0.5568, best train loss: 1.1545  @ epoch 39079 )\n",
      "[Epoch: 39140] train loss: 1.1764, train acc: 0.5086, val loss: 1.1653, val acc: 0.5042  (best train acc: 0.5420, best val acc: 0.5568, best train loss: 1.1545  @ epoch 39079 )\n",
      "[Epoch: 39160] train loss: 1.1522, train acc: 0.5408, val loss: 1.1447, val acc: 0.5420  (best train acc: 0.5420, best val acc: 0.5568, best train loss: 1.1522  @ epoch 39160 )\n",
      "[Epoch: 39180] train loss: 1.1447, train acc: 0.5437, val loss: 1.1339, val acc: 0.5454  (best train acc: 0.5437, best val acc: 0.5568, best train loss: 1.1423  @ epoch 39173 )\n",
      "[Epoch: 39200] train loss: 1.1394, train acc: 0.5375, val loss: 1.1307, val acc: 0.5470  (best train acc: 0.5481, best val acc: 0.5568, best train loss: 1.1372  @ epoch 39187 )\n",
      "[Epoch: 39220] train loss: 1.1348, train acc: 0.5405, val loss: 1.1265, val acc: 0.5484  (best train acc: 0.5481, best val acc: 0.5568, best train loss: 1.1348  @ epoch 39220 )\n",
      "[Epoch: 39240] train loss: 1.4603, train acc: 0.4437, val loss: 1.6213, val acc: 0.4165  (best train acc: 0.5481, best val acc: 0.5568, best train loss: 1.1319  @ epoch 39234 )\n",
      "[Epoch: 39260] train loss: 1.2538, train acc: 0.4832, val loss: 1.2121, val acc: 0.4830  (best train acc: 0.5481, best val acc: 0.5568, best train loss: 1.1319  @ epoch 39234 )\n",
      "[Epoch: 39280] train loss: 1.2211, train acc: 0.4903, val loss: 1.1969, val acc: 0.4847  (best train acc: 0.5481, best val acc: 0.5568, best train loss: 1.1319  @ epoch 39234 )\n",
      "[Epoch: 39300] train loss: 1.1997, train acc: 0.5000, val loss: 1.1747, val acc: 0.5130  (best train acc: 0.5481, best val acc: 0.5568, best train loss: 1.1319  @ epoch 39234 )\n",
      "[Epoch: 39320] train loss: 1.1979, train acc: 0.5080, val loss: 1.1710, val acc: 0.5133  (best train acc: 0.5481, best val acc: 0.5568, best train loss: 1.1319  @ epoch 39234 )\n",
      "[Epoch: 39340] train loss: 1.1926, train acc: 0.5120, val loss: 1.1678, val acc: 0.5218  (best train acc: 0.5481, best val acc: 0.5568, best train loss: 1.1319  @ epoch 39234 )\n",
      "[Epoch: 39360] train loss: 1.1901, train acc: 0.5081, val loss: 1.1758, val acc: 0.5099  (best train acc: 0.5481, best val acc: 0.5568, best train loss: 1.1319  @ epoch 39234 )\n",
      "[Epoch: 39380] train loss: 1.1932, train acc: 0.5034, val loss: 1.1666, val acc: 0.5184  (best train acc: 0.5481, best val acc: 0.5568, best train loss: 1.1319  @ epoch 39234 )\n",
      "[Epoch: 39400] train loss: 1.1933, train acc: 0.5058, val loss: 1.1713, val acc: 0.5079  (best train acc: 0.5481, best val acc: 0.5568, best train loss: 1.1319  @ epoch 39234 )\n",
      "[Epoch: 39420] train loss: 1.1833, train acc: 0.5043, val loss: 1.1632, val acc: 0.5177  (best train acc: 0.5481, best val acc: 0.5568, best train loss: 1.1319  @ epoch 39234 )\n",
      "[Epoch: 39440] train loss: 1.1811, train acc: 0.5079, val loss: 1.1612, val acc: 0.5191  (best train acc: 0.5481, best val acc: 0.5568, best train loss: 1.1319  @ epoch 39234 )\n",
      "[Epoch: 39460] train loss: 1.1798, train acc: 0.5102, val loss: 1.1585, val acc: 0.5234  (best train acc: 0.5481, best val acc: 0.5568, best train loss: 1.1319  @ epoch 39234 )\n",
      "[Epoch: 39480] train loss: 1.1790, train acc: 0.5088, val loss: 1.1607, val acc: 0.5170  (best train acc: 0.5481, best val acc: 0.5568, best train loss: 1.1319  @ epoch 39234 )\n",
      "[Epoch: 39500] train loss: 1.1806, train acc: 0.5069, val loss: 1.1632, val acc: 0.5069  (best train acc: 0.5481, best val acc: 0.5568, best train loss: 1.1319  @ epoch 39234 )\n",
      "[Epoch: 39520] train loss: 1.1821, train acc: 0.5089, val loss: 1.1769, val acc: 0.4938  (best train acc: 0.5481, best val acc: 0.5568, best train loss: 1.1319  @ epoch 39234 )\n",
      "[Epoch: 39540] train loss: 1.1857, train acc: 0.4932, val loss: 1.1574, val acc: 0.5356  (best train acc: 0.5481, best val acc: 0.5568, best train loss: 1.1319  @ epoch 39234 )\n",
      "[Epoch: 39560] train loss: 1.1761, train acc: 0.5006, val loss: 1.1564, val acc: 0.5396  (best train acc: 0.5481, best val acc: 0.5568, best train loss: 1.1319  @ epoch 39234 )\n",
      "[Epoch: 39580] train loss: 1.1718, train acc: 0.5098, val loss: 1.1595, val acc: 0.5268  (best train acc: 0.5481, best val acc: 0.5568, best train loss: 1.1319  @ epoch 39234 )\n",
      "[Epoch: 39600] train loss: 1.1751, train acc: 0.5085, val loss: 1.1657, val acc: 0.5035  (best train acc: 0.5481, best val acc: 0.5568, best train loss: 1.1319  @ epoch 39234 )\n",
      "[Epoch: 39620] train loss: 1.1775, train acc: 0.5048, val loss: 1.1600, val acc: 0.5234  (best train acc: 0.5481, best val acc: 0.5568, best train loss: 1.1319  @ epoch 39234 )\n",
      "[Epoch: 39640] train loss: 1.1712, train acc: 0.5097, val loss: 1.1528, val acc: 0.5298  (best train acc: 0.5481, best val acc: 0.5568, best train loss: 1.1319  @ epoch 39234 )\n",
      "[Epoch: 39660] train loss: 1.1728, train acc: 0.4983, val loss: 1.1589, val acc: 0.5255  (best train acc: 0.5481, best val acc: 0.5568, best train loss: 1.1319  @ epoch 39234 )\n",
      "[Epoch: 39680] train loss: 1.1739, train acc: 0.5103, val loss: 1.1531, val acc: 0.5339  (best train acc: 0.5481, best val acc: 0.5568, best train loss: 1.1319  @ epoch 39234 )\n",
      "[Epoch: 39700] train loss: 1.1712, train acc: 0.5042, val loss: 1.1515, val acc: 0.5278  (best train acc: 0.5481, best val acc: 0.5568, best train loss: 1.1319  @ epoch 39234 )\n",
      "[Epoch: 39720] train loss: 1.1696, train acc: 0.4993, val loss: 1.1512, val acc: 0.5376  (best train acc: 0.5481, best val acc: 0.5568, best train loss: 1.1319  @ epoch 39234 )\n",
      "[Epoch: 39740] train loss: 1.1700, train acc: 0.5046, val loss: 1.1517, val acc: 0.5339  (best train acc: 0.5481, best val acc: 0.5568, best train loss: 1.1319  @ epoch 39234 )\n",
      "[Epoch: 39760] train loss: 1.1743, train acc: 0.4939, val loss: 1.1519, val acc: 0.5417  (best train acc: 0.5481, best val acc: 0.5568, best train loss: 1.1319  @ epoch 39234 )\n",
      "[Epoch: 39780] train loss: 1.2152, train acc: 0.4915, val loss: 1.1806, val acc: 0.5073  (best train acc: 0.5481, best val acc: 0.5568, best train loss: 1.1319  @ epoch 39234 )\n",
      "[Epoch: 39800] train loss: 1.1829, train acc: 0.4766, val loss: 1.1574, val acc: 0.5288  (best train acc: 0.5481, best val acc: 0.5568, best train loss: 1.1319  @ epoch 39234 )\n",
      "[Epoch: 39820] train loss: 1.2216, train acc: 0.4502, val loss: 1.2877, val acc: 0.4553  (best train acc: 0.5481, best val acc: 0.5568, best train loss: 1.1319  @ epoch 39234 )\n",
      "[Epoch: 39840] train loss: 1.1801, train acc: 0.5032, val loss: 1.1647, val acc: 0.5201  (best train acc: 0.5481, best val acc: 0.5568, best train loss: 1.1319  @ epoch 39234 )\n",
      "[Epoch: 39860] train loss: 1.1660, train acc: 0.5154, val loss: 1.1527, val acc: 0.5272  (best train acc: 0.5481, best val acc: 0.5568, best train loss: 1.1319  @ epoch 39234 )\n",
      "[Epoch: 39880] train loss: 1.1635, train acc: 0.5115, val loss: 1.1519, val acc: 0.5302  (best train acc: 0.5481, best val acc: 0.5568, best train loss: 1.1319  @ epoch 39234 )\n",
      "[Epoch: 39900] train loss: 1.1577, train acc: 0.5096, val loss: 1.1447, val acc: 0.5464  (best train acc: 0.5481, best val acc: 0.5568, best train loss: 1.1319  @ epoch 39234 )\n",
      "[Epoch: 39920] train loss: 1.1571, train acc: 0.5077, val loss: 1.1447, val acc: 0.5433  (best train acc: 0.5481, best val acc: 0.5568, best train loss: 1.1319  @ epoch 39234 )\n",
      "[Epoch: 39940] train loss: 1.1526, train acc: 0.5209, val loss: 1.1386, val acc: 0.5477  (best train acc: 0.5481, best val acc: 0.5568, best train loss: 1.1319  @ epoch 39234 )\n",
      "[Epoch: 39960] train loss: 1.1521, train acc: 0.5164, val loss: 1.1371, val acc: 0.5524  (best train acc: 0.5481, best val acc: 0.5568, best train loss: 1.1319  @ epoch 39234 )\n",
      "[Epoch: 39980] train loss: 1.1523, train acc: 0.5252, val loss: 1.1390, val acc: 0.5450  (best train acc: 0.5481, best val acc: 0.5568, best train loss: 1.1319  @ epoch 39234 )\n",
      "[Epoch: 40000] train loss: 1.1505, train acc: 0.5265, val loss: 1.1387, val acc: 0.5484  (best train acc: 0.5481, best val acc: 0.5568, best train loss: 1.1319  @ epoch 39234 )\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtQAAAGpCAYAAACteaFyAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAAleklEQVR4nO3dfbRkZ10n+u/vnNPdSUhCEtLBkASTYMSLXgzYk1FRFoKMgLNAnCsmS+ZyHWaiM3LV4Y4zMN41okvWdRxBx+UIEyS8OPI6yBKRURgUWIxI6EASEiDmhQAhId1JIO/p7nPOc/+oXd3V3XW6z6nddeqc5PNZq1btempX1a+e89Sub+2z66lqrQUAAJjM3KwLAACAzUygBgCAHgRqAADoQaAGAIAeBGoAAOhhYdYF9HH66ae3c889d9ZlAADwCHfllVfe2VrbPu66TR2ozz333OzcuXPWZQAA8AhXVV9Z6TqHfAAAQA8CNQAA9CBQAwBADwI1AAD0IFADAEAPAjUAAPQgUAMAQA8CNQAA9CBQAwBADwI1AAD0IFADAEAPAjUAAPQgUAMAQA8CNQAA9CBQT+D6b9yXXfc9POsyAADYAATqCTz/P38if/ypr8y6DAAANgCBGgAAehCoAQCgB4EaAAB6EKgBAKAHgRoAAHoQqCfU2qwrAABgIxCoJ1BVsy4BAIANQqAGAIAeBGoAAOhBoAYAgB4EagAA6EGgBgCAHgTqCbWYNw8AAIF6IibNAwBgSKAGAIAeBGoAAOhBoAYAgB6mFqir6vKq2lVV1460vbuqrupOt1TVVV37uVX10Mh1b5xWXQAAcCwtTPG+35rkD5K8fdjQWvvp4XJVvS7JPSPr39Rau3CK9QAAwDE3tUDdWvtEVZ077rqqqiQvSfLsaT3+tDWz5gEAkNkdQ/3DSe5ord0w0nZeVX2uqj5eVT+80g2r6tKq2llVO3fv3j39SsfWMJOHBQBgA5pVoL4kyTtHLt+e5ImttacleWWSd1TVyeNu2Fq7rLW2o7W2Y/v27etQKgAArGzdA3VVLST5ySTvHra11va01u7qlq9MclOS71zv2gAAYK1msYf6R5N8qbV267ChqrZX1Xy3fH6SC5LcPIPaAABgTaY5bd47k3wqyZOr6taqenl31cU5+HCPJHlmkmuq6uok/z3Jz7fW7p5WbQAAcKxMc5aPS1Zo/7/GtL0vyfumVQsAAEyLX0qckFnzAABIBOqJVMybBwDAgEANAAA9CNQAANCDQA0AAD0I1AAA0INADQAAPQjUE2rmzQMAIAL1ZMyaBwBAR6AGAIAeBGoAAOhBoAYAgB4EagAA6EGgBgCAHgTqCbWYNw8AAIF6ImbNAwBgSKAGAIAeBGoAAOhBoAYAgB4EagAA6EGgBgCAHgTqSZk1DwCACNQTKfPmAQDQEagBAKAHgRoAAHoQqAEAoAeBGgAAehCoAQCgB4F6QmbNAwAgEagnUjFvHgAAAwI1AAD0IFADAEAPAjUAAPQgUAMAQA8CNQAA9CBQT6g1E+cBACBQT6TMmgcAQEegBgCAHqYWqKvq8qraVVXXjrS9pqq+XlVXdacXjFz36qq6saqur6ofm1ZdAABwLE1zD/VbkzxvTPvvttYu7E4fSpKqekqSi5N8d3ebP6yq+SnWBgAAx8TUAnVr7RNJ7l7l6i9K8q7W2p7W2peT3JjkomnVBgAAx8osjqF+RVVd0x0ScmrXdlaSr42sc2vXdpiqurSqdlbVzt27d0+7VgAAOKL1DtRvSPKkJBcmuT3J67r2cfNmjJ2XrrV2WWttR2ttx/bt26dS5GqYNQ8AgGSdA3Vr7Y7W2lJrbTnJm3LgsI5bk5wzsurZSW5bz9rWwqx5AAAMrWugrqozRy6+OMlwBpAPJLm4qrZV1XlJLkhyxXrWBgAAk1iY1h1X1TuTPCvJ6VV1a5JfS/Ksqrowg8M5bknyc0nSWruuqt6T5AtJFpP8QmttaVq1AQDAsTK1QN1au2RM85uPsP5rk7x2WvUAAMA0+KVEAADoQaAGAIAeBOoJmTUPAIBEoJ5IlYnzAAAYEKgBAKAHgRoAAHoQqAEAoAeBGgAAehCoAQCgB4F6Qs28eQAARKCeiEnzAAAYEqgBAKAHgRoAAHoQqAEAoAeBGgAAehCoAQCgB4F6Qi3mzQMAQKCejHnzAADoCNQAANCDQA0AAD0I1AAA0INADQAAPQjUAADQg0A9oWbWPAAAIlBPxKx5AAAMCdQAANCDQA0AAD0I1AAA0INADQAAPQjUAADQg0ANAAA9CNQTqDJxHgAAAwI1AAD0IFADAEAPAjUAAPQgUAMAQA8CNQAA9DC1QF1Vl1fVrqq6dqTtP1XVl6rqmqp6f1Wd0rWfW1UPVdVV3emN06rrWGmtzboEAAA2gGnuoX5rkucd0vaRJN/TWntqkr9P8uqR625qrV3YnX5+inX1ZtY8AACGphaoW2ufSHL3IW0fbq0tdhf/LsnZ03p8AABYD7M8hvqfJfkfI5fPq6rPVdXHq+qHV7pRVV1aVTuraufu3bunXyUAABzBTAJ1Vf1qksUkf9I13Z7kia21pyV5ZZJ3VNXJ427bWrustbajtbZj+/bt61MwAACsYN0DdVW9LMk/TvIzrftmX2ttT2vtrm75yiQ3JfnO9a4NAADWal0DdVU9L8m/S/LC1tqDI+3bq2q+Wz4/yQVJbl7P2gAAYBIL07rjqnpnkmclOb2qbk3yaxnM6rEtyUdqMFXG33UzejwzyW9U1WKSpSQ/31q7e+wdbxAmzQMAIJlioG6tXTKm+c0rrPu+JO+bVi3HmlnzAAAY8kuJAADQg0ANAAA9CNQAANCDQA0AAD0I1AAA0INAPaFm3jwAACJQT6SbQxsAAARqAADoQ6AGAIAeBGoAAOhBoAYAgB4EagAA6EGgnlCLefMAABCoJ2LSPAAAhgRqAADoQaAGAIAeBGoAAOhBoAYAgB4EagAA6EGgnlAzax4AABGoJ1LmzQMAoCNQAwBADwI1AAD0IFADAEAPAjUAAPQgUAMAQA8C9YTMmgcAQCJQT8i8eQAADAjUAADQg0ANAAA9CNQAANDDUQN1VT2pqrZ1y8+qql+sqlOmXhkAAGwCq9lD/b4kS1X1HUnenOS8JO+YalUAALBJrCZQL7fWFpO8OMnvtdb+dZIzp1vWxtfMmwcAQFYXqPdV1SVJXpbkg13blumVtPGVWfMAAOisJlD/bJIfSPLa1tqXq+q8JP9tumUBAMDmsHC0FVprX0jyi0lSVacmOam19lvTLgwAADaD1czy8bGqOrmqTktydZK3VNXrp18aAABsfKs55OOxrbV7k/xkkre01r4vyY8e7UZVdXlV7aqqa0faTquqj1TVDd35qSPXvbqqbqyq66vqxyZ5MgAAsN5WE6gXqurMJC/JgS8lrsZbkzzvkLZXJfloa+2CJB/tLqeqnpLk4iTf3d3mD6tqfg2PBQAAM7GaQP0bSf4qyU2ttc9U1flJbjjajVprn0hy9yHNL0rytm75bUl+YqT9Xa21Pa21Lye5MclFq6hthsybBwDA6r6U+N4k7x25fHOSfzLh4z2+tXZ7dz+3V9UZXftZSf5uZL1bu7bDVNWlSS5Nkic+8YkTltGPWfMAABhazZcSz66q93fHQ99RVe+rqrOPcR3jMurYXcCttctaaztaazu2b99+jMsAAIC1Wc0hH29J8oEkT8hgr/Gfd22TuKM7Hjvd+a6u/dYk54ysd3aS2yZ8DAAAWDerCdTbW2tvaa0tdqe3Jpl01/AHMvjFxXTnfzbSfnFVbet+OOaCJFdM+BgAALBuVhOo76yql1bVfHd6aZK7jnajqnpnkk8leXJV3VpVL0/yW0meW1U3JHludzmtteuSvCfJF5L8ZZJfaK0tTfaUAABg/Rz1S4lJ/lmSP0jyuxkc1/y3Gfwc+RG11i5Z4arnrLD+a5O8dhX1AADAhrGaWT6+muSFo21V9TtJ/s20itoMmlnzAADI6g75GOclx7SKTabMmwcAQGfSQC1SAgBAjnDIR1WdttJVEagBACDJkY+hvjKDLyGOC897p1MOAABsLisG6tbaeetZCAAAbEaTHkMNAABEoJ6YafMAAEgE6omU72QCANBZzS8lpqrmkzx+dP3uB18AAOBR7aiBuqr+7yS/luSOJMtdc0vy1CnWBQAAm8Jq9lD/UpInt9bumnYxAACw2azmGOqvJbln2oUAAMBmtJo91Dcn+VhV/UWSPcPG1trrp1YVAABsEqsJ1F/tTlu7E0lazJsHAMAqAnVr7dfXo5DNpMyaBwBAZ8VAXVW/11r75ar68+Tw3bGttRdOtTIAANgEjrSH+o+7899Zj0IAAGAzWjFQt9au7M4/vn7lAADA5rKaH3a5IMn/l+QpSY4btrfWzp9iXQAAsCmsZh7qtyR5Q5LFJD+S5O05cDgIAAA8qq0mUB/fWvtokmqtfaW19pokz55uWRtfM2seAABZ3TzUD1fVXJIbquoVSb6e5IzplrWxmTUPAICh1eyh/uUkJyT5xSTfl+SlSV42xZoAAGDTOOIe6qqaT/KS1tqvJLk/yc+uS1UAALBJrLiHuqoWWmtLSb6vym8DAgDAOEfaQ31Fkqcn+VySP6uq9yZ5YHhla+1Pp1wbAABseKv5UuJpSe7KYGaPlsF38loSgRoAgEe9IwXqM6rqlUmuzYEgPfSonzTuUd8BAAAkOXKgnk9yYsbPEveozpMOKQcAYOhIgfr21tpvrFslAACwCR1pHmq7YQEA4CiOFKifs25VAADAJrVioG6t3b2ehQAAwGa0mp8eBwAAViBQT6g9quc5AQBgSKAGAIAeBGoAAOhhNT89fkxV1ZOTvHuk6fwk/yHJKUn+RZLdXfu/b619aH2rAwCAtVn3QN1auz7JhUlSVfNJvp7k/Ul+NsnvttZ+Z71rAgCASc36kI/nJLmptfaVGdcBAAATmXWgvjjJO0cuv6Kqrqmqy6vq1HE3qKpLq2pnVe3cvXv3uFUAAGDdzCxQV9XWJC9M8t6u6Q1JnpTB4SC3J3nduNu11i5rre1ore3Yvn37epQ6Vot58wAAmO0e6ucn+Wxr7Y4kaa3d0Vpbaq0tJ3lTkotmWNsRVc26AgAANopZBupLMnK4R1WdOXLdi5Ncu+4VAQDAGq37LB9JUlUnJHlukp8baf7tqrowSUtyyyHXAQDAhjSTQN1aezDJ4w5p+6ezqAUAAPqY9SwfAACwqQnUAADQg0A9KbPmAQAQgXoips0DAGBIoAYAgB4EagAA6EGgBgCAHgRqAADoQaAGAIAeBOoJmTUPAIBEoJ5Ixbx5AAAMCNQAANCDQA0AAD0I1AAA0INADQAAPQjUAADQg0A9odZMnAcAgEA9kTJrHgAAHYEaAAB6EKgBAKAHgRoAAHoQqAEAoAeBGgAAehCoJ2TSPAAAEoF6ImbNAwBgSKAGAIAeBGoAAOhBoAYAgB4EagAA6EGgBgCAHgTqCTXz5gEAEIF6IlUmzgMAYECgBgCAHgRqAADoQaAGAIAeBGoAAOhBoAYAgB4E6gmZNQ8AgCRZmMWDVtUtSe5LspRksbW2o6pOS/LuJOcmuSXJS1pr35xFfUdj0jwAAIZmuYf6R1prF7bWdnSXX5Xko621C5J8tLsMAAAb2kY65ONFSd7WLb8tyU/MrhQAAFidWQXqluTDVXVlVV3atT2+tXZ7knTnZ4y7YVVdWlU7q2rn7t2716lcAAAYbybHUCd5Rmvttqo6I8lHqupLq71ha+2yJJclyY4dO3w3EACAmZrJHurW2m3d+a4k709yUZI7qurMJOnOd82iNgAAWIt1D9RV9ZiqOmm4nOQfJbk2yQeSvKxb7WVJ/my9a1uL1uwcBwBgNod8PD7J+6tq+PjvaK39ZVV9Jsl7qurlSb6a5KdmUNvqmDcPAIDOugfq1trNSb53TPtdSZ6z3vUAAEAfG2naPAAA2HQEagAA6EGgBgCAHgRqAADoQaCekEnzAABIBOqJmDUPAIAhgRoAAHoQqAEAoAeBGgAAehCoAQCgB4EaAAB6EKgnZd48AAAiUE+kysR5AAAMCNQAANCDQA0AAD0I1AAA0INADQAAPQjUAADQg0A9oWbePAAAIlBPxKR5AAAMCdQAANCDQA0AAD0I1AAA0INADQAAPQjUAADQg0A9oWbWPAAAIlBPpMybBwBAR6AGAIAeBGoAAOhBoAYAgB4EagAA6EGgBgCAHgTqCZk2DwCARKCeSMW8eQAADAjUAADQg0ANAAA9CNQAANCDQA0AAD2se6CuqnOq6m+q6otVdV1V/VLX/pqq+npVXdWdXrDetQEAwFotzOAxF5P8P621z1bVSUmurKqPdNf9bmvtd2ZQ05q1mDcPAIAZBOrW2u1Jbu+W76uqLyY5a73r6KPMmgcAQGemx1BX1blJnpbk013TK6rqmqq6vKpOXeE2l1bVzqrauXv37vUqFQAAxppZoK6qE5O8L8kvt9buTfKGJE9KcmEGe7BfN+52rbXLWms7Wms7tm/fvl7lAgDAWDMJ1FW1JYMw/SettT9NktbaHa21pdbacpI3JbloFrUBAMBazGKWj0ry5iRfbK29fqT9zJHVXpzk2vWuDQAA1moWs3w8I8k/TfL5qrqqa/v3SS6pqguTtCS3JPm5GdQGAABrMotZPj6ZZNw8GR9a71r6aGbNAwAgfikRAAB6EagBAKAHgRoAAHoQqAEAoAeBGgAAehCoAQCgB4F6QmbNAwAgEagnMvixRwAAEKgBAKCXWfz0+KZ38+77s7i0POsyAADYAOyhnsCexeXcsOv+WZcBbEK/+cEv5DO33D3rMgA4hgRqgHX0R5/8cn7qjZ+adRkAHEMCNQAA9CBQAwBADwI1AAD0IFADAEAPAjUAAPQgUAMAQA8CNQAA9CBQs+n8zy/ckffu/NqsywAASOKnx9mE/vnbdyZJfmrHOTOuBADAHuqJPOM7HpcnPPa4WZcBAMAGIFBP4NQTtua4LfOzLgMAgA1AoJ7A/FxlqbVZlwEAwAYgUE9gvipLywI1AAAC9UTm5irLAjUAABGoJzJfDvkAAGBAoJ7A3FxlaXnWVQBwJEvLLb/+59fltm89NOtSgEc4gXoC83PJsj3UABvaZ7/6zbzlf92Sf/3uq2ZdCvAIJ1BPwJcSATa+6s73+ZciMGUC9QR8KRGYRPOfrXU1NzeI1DbXwLQJ1BPwpUSA9fWHH7sx1912z5puM1fDQG17DUyXQD2B+bnKol0eABN5aO9S/teNd67pNr/9l9fnx3//k2u6zbxADawTgXoCJ2xdyN7F5Sw6Lg9YA7lu4Fff//n8zB99Ol++84GpPk6Xp83KBEydQD2Bk49fSJLc9/DijCsB2Hxu2n1/kuSeh/ZN9XHmh8dQ+48iMGUC9QROPm5LkuTeh6f7ZsCj113378kVX7571mXAVAyD7rT/y7dlfvAWt9cuamDKBOoJbF0YdNt1t90740qSO+59eOIfLVhcWs49Dx74ULDr3oezZ3Ep37jn4fzKe69Okjy4dzFv/PhNG/LwltXUdM+D+/Jf/ubGTTe7wj9/+8685L9+Kg/vW5p1KWxA37jn4dy0+/4847f+Orvue/iI6x5t7H/zgb3rvgf3W9125/Z7jlx7X8PgPu1DS2CcB/ce/F/sXfc+fNB77lrcuOu+fPar3zys/brb7sld9++Z6D6Phbsf2JvLP/nlid5jr/jy3bl/z2JuufOB3DnD53Cs1GYLGqN27NjRdu7cue6P+8kb7sxL3/zpJMmJ2xaydWEuC3OV+bnKXFUW5ivzNbg8bKsaHM9X6ZaTpCqV7L9cB10eNM4NlzP4Ys3wvpLkgT2LufrWwbfed3z7qVmYr/3rDn3lrgdy8vFb8qVv3Jck+d5zTsnVX/vWQes89ezH5vgt8/n0UfaIfvcTTj7oOSQ56NGqamT5wPXD9jrsupGV2iF3NmJ5uWW5tSy1ZM++pf3PJUkuOOPE/fc56JvBvW6Zr2yZn8vOrxzYAJ13+mPGP8RoYxuUMu6qFW936EtozI1G++HA5UP6sluYqwMf1k57zNY84ZTjutt0/Thy/4eUPjhvSUsbnHeNdz2wJ3fcO9hgbevG61w3PrctzGXrwly2zHft3dgd1jc3V137Ic+p6oh/t5W6o8b2Tx1xncMuH2X9sXUcstLq6lr5Puaq8j+/eEeS5DnfdcZg7NVgm7BlvvKYbQvZtjCf+bnBl+M+c8s386mb79p/+x980uNy3Jb5zM/V/vG6MDeXLfOV5dZy1/17u5oOjIsPf+GOsc/tu77tpMzPVRbm57JtYS6nn7g120/clrd96itJkv/3x/+3zNXgfpeWW5Zay9JSyzVfvycf6e7zmd+5ff+4HH1bGD7luRqOjW65GytzdeDLf6Ov5eEMG5Xkwe6D4cnHbcmW+crbu7qS7B9zK71ukmTP4oEPz9sW5g76W811r/kD29lBTXNVueuBvfvXO+0xW7NtYS7HbZnPcVvmD4zpkfX3b2/3b5OHddQRx+T46w9/QqPbwOE24dD34HG3G73t2OtWvHKF+xqz/U4ObEMO3bQNX/vjxsVKhuuu5rV5/57FnLB1IXPd2BuOwZZk7+Jyti7MrVjTpIZ/s+H2cmm5HfReVWOe8/D5zM8N3muXW8u+pZbF5eUsLrXsW1rO4nLL4tJy7t+zmJt2Dz7Inb/9MVmYq/z9HYNDnZ7/Pd+WubnaPwaG46+618/i8qCexeXl7rzlY9fvTpL85NPOOugP96ef/XqS5MefemZu+9ZDeWjvUs4+9YQx2+uDn/dozjgwLsdnkoy8Vx08hivv3vm1JIM++YfnnZbjt8zn+K3z+eA1tydJLn3m+Qfdx198/rZ87e6H8gPnP+6g7WGSvP4l39u999RB2ecDV389f3XdHfmVH3vy/m3dheecklNO2LqaP/UxVVVXttZ2jLtuYb2LOZqqel6S/5xkPskftdZ+a8YlHeaHLjg9F/+Dc3LclvnMVWXv0lIWlw68US13L4Dl1rK4NDgfbhxaa935gcvJ4SFouLy8nHSXUlVZXF7ef9t9Swde6VsX5rK41HJouhsN00lywpb5nH3q8bn1mwf2ai91tR5qYWQ2k2d/1xn73yuHz+FA3YMX2XJrh71BDBeHt9h/+dD2Qx57dGO2dWHuwIvshAPP54e+4/ScdNzCQf233Ab17es2bk858+R84fZ789ynPD7HbZk/7DmO+0A5N/IOMHxOo+se7U1wubXDg24bvXz43/mg/mwtjz1+S/72prvy9CeeOjJmDn7M0UvDN6FxG8ak8oRTjs9y+1Ze8D3fluO2zmdpaTBWF5da9i4uZ+/S4DRsH+6xHH6QWVpezvLywWFrubXU3JHfOA/t3jam7dAOaoeMhnH3cfD1h/8ND1/nyNeP+7bg0e5j9MdCbuv2tLbW8sDexexbbHlgz2IeXlzqXl+H3X0e2reUex/el8Wltv9NeDhu9y0t58TjFnLSti37X5sr7fs446RtOee0E7K83LJvueXhfUu5/hv35RP3HphF4zf/4ovjbzzinof2HZ6YRi4PxsWB7cVwu7bUhmEkB42R0b/L4nLLtx7cl5OP35Kl5QP99gs/8qTBdm7Mc2sHv2jyXz9xc37wSY/L/37WY0eb09rwdZ+upsHl4fb3/Vd9Pd9//uPy7aedkD2LS3lw71Ie3recpeXltGT/NqO1wXMbfU22bpPacvB/w9rBpe1/roc+j9GAuv+qkfeA1Tr0NbFSLatqz6DecduxQw3XGW5f1lLD6DZ8OCZWuo+9S8u5efcD+a5vO+mg2ySDcfnY47eMfcxD7+9odY6uN/w7H7pjYzW3XerGWdVgZ8OW+bkszFe2zA0+0D5m20JOP3Fbbtr9QJ72xFNy1inHZ3Gp7Q/UN+y6f/C67sbZ8PU0fP1smR+85w0/bC7MHyjuilvG7/j60u337g/wK9Wd5JDx3Q7a/B6eUQ7eMXP4e/+BP/w/OPfU7Flczjcf3Jc9I/9Zfdvf3rL/vpPBB6Qkh4XpJHnle65esf4k+U9/df3+5ff83A/kovNOO+L6621D7aGuqvkkf5/kuUluTfKZJJe01r4wbv1Z7aEG2AyWuj1dD+1dSksb/FeiDvzn7N6H9+WErfM5YeuG27cCPEK11nLvw4v52t0P5q4H9mbr/Fwef/K2/VMSj35Ibhl8ML73oeEH8sFOnwvOODEnHXf4B61p20x7qC9KcmNr7eYkqap3JXlRkrGBGoCVDQ87G37v41Cnn7htnSsCHu2qKo89fkseO/LfpkeCjfalxLOSfG3k8q1d235VdWlV7ayqnbt3717X4gAA4FAbLVCPO4rpoGNSWmuXtdZ2tNZ2bN++fZ3KAgCA8TZaoL41yTkjl89OctuMagEAgKPaaIH6M0kuqKrzqmprkouTfGDGNQEAwIo21JcSW2uLVfWKJH+VwbR5l7fWrptxWQAAsKINFaiTpLX2oSQfmnUdAACwGhvtkA8AANhUBGoAAOhBoAYAgB4EagAA6EGgBgCAHgRqAADoQaAGAIAeBGoAAOhBoAYAgB6qtTbrGiZWVbuTfGVGD396kjtn9Nibkf5aG/21NvprbfTX2uivtdFfa6O/1maW/fXtrbXt467Y1IF6lqpqZ2ttx6zr2Cz019ror7XRX2ujv9ZGf62N/lob/bU2G7W/HPIBAAA9CNQAANCDQD25y2ZdwCajv9ZGf62N/lob/bU2+mtt9Nfa6K+12ZD95RhqAADowR5qAADoQaAGAIAeBOo1qqrnVdX1VXVjVb1q1vXMUlXdUlWfr6qrqmpn13ZaVX2kqm7ozk8dWf/VXb9dX1U/NtL+fd393FhVv19VNYvnc6xV1eVVtauqrh1pO2b9U1XbqurdXfunq+rcdX2Cx9gK/fWaqvp6N8auqqoXjFz3aO+vc6rqb6rqi1V1XVX9UtdujI1xhP4yxsaoquOq6oqqurrrr1/v2o2vMY7QX8bXEVTVfFV9rqo+2F3evOOrtea0ylOS+SQ3JTk/ydYkVyd5yqzrmmF/3JLk9EPafjvJq7rlVyX5j93yU7r+2pbkvK4f57vrrkjyA0kqyf9I8vxZP7dj1D/PTPL0JNdOo3+S/Kskb+yWL07y7lk/5yn012uS/Jsx6+qv5MwkT++WT0ry912/GGNr6y9jbHx/VZITu+UtST6d5PuNrzX3l/F15H57ZZJ3JPlgd3nTji97qNfmoiQ3ttZubq3tTfKuJC+acU0bzYuSvK1bfluSnxhpf1drbU9r7ctJbkxyUVWdmeTk1tqn2mDUv33kNptaa+0TSe4+pPlY9s/off33JM8ZfjLfjFbor5Xor9Zub619tlu+L8kXk5wVY2ysI/TXSh7t/dVaa/d3F7d0pxbja6wj9NdKHtX9lSRVdXaSH0/yRyPNm3Z8CdRrc1aSr41cvjVH3iA/0rUkH66qK6vq0q7t8a2125PBG1iSM7r2lfrurG750PZHqmPZP/tv01pbTHJPksdNrfLZeUVVXVODQ0KG//7TXyO6f2U+LYO9YsbYURzSX4kxNlb37/irkuxK8pHWmvF1BCv0V2J8reT3kvzbJMsjbZt2fAnUazPuk82jed7BZ7TWnp7k+Ul+oaqeeYR1V+o7fTowSf88GvruDUmelOTCJLcneV3Xrr86VXVikvcl+eXW2r1HWnVM26Ouz8b0lzG2gtbaUmvtwiRnZ7A38HuOsLr+Gt9fxtcYVfWPk+xqrV252puMadtQ/SVQr82tSc4ZuXx2kttmVMvMtdZu6853JXl/BofE3NH9Cybd+a5u9ZX67tZu+dD2R6pj2T/7b1NVC0kem9UfMrEptNbu6N6klpO8KYMxluivJElVbckgHP5Ja+1Pu2ZjbAXj+ssYO7rW2reSfCzJ82J8HdVofxlfK3pGkhdW1S0ZHD777Kr6b9nE40ugXpvPJLmgqs6rqq0ZHOT+gRnXNBNV9ZiqOmm4nOQfJbk2g/54Wbfay5L8Wbf8gSQXd9+6PS/JBUmu6P6lc19VfX93bNP/OXKbR6Jj2T+j9/V/JPnr7hiyR4zhhrXz4gzGWKK/0j2/Nyf5Ymvt9SNXGWNjrNRfxth4VbW9qk7plo9P8qNJvhTja6yV+sv4Gq+19urW2tmttXMzyFJ/3Vp7aTbz+Gob4Fuem+mU5AUZfDv8piS/Out6ZtgP52fwjdurk1w37IsMjk/6aJIbuvPTRm7zq12/XZ+RmTyS7MhgI3NTkj9I9wuem/2U5J0Z/ItvXwaflF9+LPsnyXFJ3pvBlzOuSHL+rJ/zFPrrj5N8Psk1GWwcz9Rf+5/nD2Xw78trklzVnV5gjK25v4yx8f311CSf6/rl2iT/oWs3vtbWX8bX0fvuWTkwy8emHV9+ehwAAHpwyAcAAPQgUAMAQA8CNQAA9CBQAwBADwI1AAD0IFADbFJVtVRVV42cXnUM7/vcqrr26GsCsDDrAgCY2ENt8FPHAMyQPdQAjzBVdUtV/cequqI7fUfX/u1V9dGquqY7f2LX/viqen9VXd2dfrC7q/mqelNVXVdVH+5+AQ6AQwjUAJvX8Ycc8vHTI9fd21q7KINfDvu9ru0Pkry9tfbUJH+S5Pe79t9P8vHW2vcmeXoGv36aDH7e97+01r47ybeS/JOpPhuATcovJQJsUlV1f2vtxDHttyR5dmvt5qrakuQbrbXHVdWdGfz08b6u/fbW2ulVtTvJ2a21PSP3cW6Sj7TWLugu/7skW1prv7kOTw1gU7GHGuCRqa2wvNI64+wZWV6K790AjCVQAzwy/fTI+ae65b9NcnG3/DNJPtktfzTJv0ySqpqvqpPXq0iARwJ7GwA2r+Or6qqRy3/ZWhtOnbetqj6dwY6TS7q2X0xyeVX9SpLdSX62a/+lJJdV1csz2BP9L5PcPu3iAR4pHEMN8AjTHUO9o7V256xrAXg0cMgHAAD0YA81AAD0YA81AAD0IFADAEAPAjUAAPQgUAMAQA8CNQAA9PD/AwXDqTbXgLhWAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 864x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pmaur\\miniconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\pmaur\\miniconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\pmaur\\miniconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\pmaur\\miniconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\pmaur\\miniconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\pmaur\\miniconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       154\n",
      "           1       0.65      0.82      0.72       703\n",
      "           2       0.39      0.45      0.42       702\n",
      "           3       0.42      0.39      0.40       703\n",
      "           4       0.66      0.57      0.61       702\n",
      "\n",
      "    accuracy                           0.53      2964\n",
      "   macro avg       0.42      0.45      0.43      2964\n",
      "weighted avg       0.50      0.53      0.51      2964\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 504x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 504x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "gnn_model = GNN7L_GAT(data_with_nedbit).to(device)\n",
    "pred = train(gnn_model, data_with_nedbit, epochs, lr, weight_decay, classes, 'GAT')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GraphCONV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9d81132e8dc4c33a88cc7ab1fffae84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40001 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch: 0000] train loss: 104.5635, train acc: 0.1295, val loss: 68.5135, val acc: 0.0560  (best train acc: 0.1295, best val acc: 0.0560, best train loss: 104.5635  @ epoch 0 )\n",
      "[Epoch: 0020] train loss: 5.3364, train acc: 0.2971, val loss: 1.9966, val acc: 0.4233  (best train acc: 0.2971, best val acc: 0.4233, best train loss: 5.3364  @ epoch 20 )\n",
      "[Epoch: 0040] train loss: 1.4747, train acc: 0.4511, val loss: 1.1729, val acc: 0.5956  (best train acc: 0.4511, best val acc: 0.5956, best train loss: 1.4747  @ epoch 40 )\n",
      "[Epoch: 0060] train loss: 1.0889, train acc: 0.5657, val loss: 0.8806, val acc: 0.7056  (best train acc: 0.5695, best val acc: 0.7056, best train loss: 1.0889  @ epoch 60 )\n",
      "[Epoch: 0080] train loss: 0.9302, train acc: 0.6601, val loss: 0.7392, val acc: 0.7936  (best train acc: 0.6601, best val acc: 0.7936, best train loss: 0.9213  @ epoch 79 )\n",
      "[Epoch: 0100] train loss: 0.8352, train acc: 0.6979, val loss: 0.6636, val acc: 0.8037  (best train acc: 0.6979, best val acc: 0.8040, best train loss: 0.8352  @ epoch 100 )\n",
      "[Epoch: 0120] train loss: 0.7980, train acc: 0.7009, val loss: 0.6131, val acc: 0.8148  (best train acc: 0.7133, best val acc: 0.8148, best train loss: 0.7979  @ epoch 116 )\n",
      "[Epoch: 0140] train loss: 0.7444, train acc: 0.7345, val loss: 0.5652, val acc: 0.8307  (best train acc: 0.7345, best val acc: 0.8317, best train loss: 0.7401  @ epoch 139 )\n",
      "[Epoch: 0160] train loss: 0.7022, train acc: 0.7465, val loss: 0.5218, val acc: 0.8452  (best train acc: 0.7465, best val acc: 0.8452, best train loss: 0.6980  @ epoch 159 )\n",
      "[Epoch: 0180] train loss: 0.6777, train acc: 0.7587, val loss: 0.4977, val acc: 0.8486  (best train acc: 0.7594, best val acc: 0.8523, best train loss: 0.6741  @ epoch 178 )\n",
      "[Epoch: 0200] train loss: 0.6525, train acc: 0.7711, val loss: 0.4626, val acc: 0.8698  (best train acc: 0.7729, best val acc: 0.8698, best train loss: 0.6423  @ epoch 198 )\n",
      "[Epoch: 0220] train loss: 0.6310, train acc: 0.7775, val loss: 0.4421, val acc: 0.8749  (best train acc: 0.7833, best val acc: 0.8776, best train loss: 0.6243  @ epoch 216 )\n",
      "[Epoch: 0240] train loss: 0.6140, train acc: 0.7901, val loss: 0.4238, val acc: 0.8870  (best train acc: 0.7916, best val acc: 0.8870, best train loss: 0.6096  @ epoch 232 )\n",
      "[Epoch: 0260] train loss: 0.5824, train acc: 0.8010, val loss: 0.4057, val acc: 0.8907  (best train acc: 0.8010, best val acc: 0.8907, best train loss: 0.5824  @ epoch 260 )\n",
      "[Epoch: 0280] train loss: 0.5786, train acc: 0.8073, val loss: 0.3809, val acc: 0.8897  (best train acc: 0.8151, best val acc: 0.8931, best train loss: 0.5635  @ epoch 279 )\n",
      "[Epoch: 0300] train loss: 0.5517, train acc: 0.8123, val loss: 0.3664, val acc: 0.8894  (best train acc: 0.8182, best val acc: 0.8954, best train loss: 0.5373  @ epoch 289 )\n",
      "[Epoch: 0320] train loss: 0.5357, train acc: 0.8169, val loss: 0.3526, val acc: 0.8924  (best train acc: 0.8232, best val acc: 0.8968, best train loss: 0.5203  @ epoch 315 )\n",
      "[Epoch: 0340] train loss: 0.5110, train acc: 0.8192, val loss: 0.3431, val acc: 0.8948  (best train acc: 0.8279, best val acc: 0.8981, best train loss: 0.5068  @ epoch 336 )\n",
      "[Epoch: 0360] train loss: 0.4995, train acc: 0.8316, val loss: 0.3369, val acc: 0.8958  (best train acc: 0.8316, best val acc: 0.8981, best train loss: 0.4972  @ epoch 345 )\n",
      "[Epoch: 0380] train loss: 0.5049, train acc: 0.8224, val loss: 0.3319, val acc: 0.8998  (best train acc: 0.8334, best val acc: 0.9005, best train loss: 0.4883  @ epoch 375 )\n",
      "[Epoch: 0400] train loss: 0.4897, train acc: 0.8323, val loss: 0.3272, val acc: 0.8965  (best train acc: 0.8370, best val acc: 0.9015, best train loss: 0.4825  @ epoch 399 )\n",
      "[Epoch: 0420] train loss: 0.4835, train acc: 0.8330, val loss: 0.3209, val acc: 0.8995  (best train acc: 0.8391, best val acc: 0.9029, best train loss: 0.4789  @ epoch 410 )\n",
      "[Epoch: 0440] train loss: 0.4929, train acc: 0.8299, val loss: 0.3182, val acc: 0.9035  (best train acc: 0.8391, best val acc: 0.9035, best train loss: 0.4710  @ epoch 436 )\n",
      "[Epoch: 0460] train loss: 0.4663, train acc: 0.8404, val loss: 0.3094, val acc: 0.9052  (best train acc: 0.8436, best val acc: 0.9069, best train loss: 0.4547  @ epoch 456 )\n",
      "[Epoch: 0480] train loss: 0.4662, train acc: 0.8398, val loss: 0.3125, val acc: 0.9029  (best train acc: 0.8436, best val acc: 0.9076, best train loss: 0.4547  @ epoch 456 )\n",
      "[Epoch: 0500] train loss: 0.4689, train acc: 0.8398, val loss: 0.3038, val acc: 0.9069  (best train acc: 0.8436, best val acc: 0.9083, best train loss: 0.4547  @ epoch 456 )\n",
      "[Epoch: 0520] train loss: 0.4507, train acc: 0.8437, val loss: 0.3010, val acc: 0.9059  (best train acc: 0.8468, best val acc: 0.9086, best train loss: 0.4504  @ epoch 515 )\n",
      "[Epoch: 0540] train loss: 0.4612, train acc: 0.8444, val loss: 0.3023, val acc: 0.9066  (best train acc: 0.8477, best val acc: 0.9106, best train loss: 0.4423  @ epoch 534 )\n",
      "[Epoch: 0560] train loss: 0.4526, train acc: 0.8433, val loss: 0.2952, val acc: 0.9093  (best train acc: 0.8506, best val acc: 0.9106, best train loss: 0.4423  @ epoch 534 )\n",
      "[Epoch: 0580] train loss: 0.4547, train acc: 0.8389, val loss: 0.2923, val acc: 0.9076  (best train acc: 0.8506, best val acc: 0.9106, best train loss: 0.4403  @ epoch 573 )\n",
      "[Epoch: 0600] train loss: 0.4494, train acc: 0.8386, val loss: 0.2869, val acc: 0.9110  (best train acc: 0.8531, best val acc: 0.9120, best train loss: 0.4333  @ epoch 586 )\n",
      "[Epoch: 0620] train loss: 0.4211, train acc: 0.8528, val loss: 0.2856, val acc: 0.9089  (best train acc: 0.8537, best val acc: 0.9130, best train loss: 0.4211  @ epoch 620 )\n",
      "[Epoch: 0640] train loss: 0.4320, train acc: 0.8503, val loss: 0.2812, val acc: 0.9123  (best train acc: 0.8539, best val acc: 0.9130, best train loss: 0.4211  @ epoch 620 )\n",
      "[Epoch: 0660] train loss: 0.4349, train acc: 0.8456, val loss: 0.2991, val acc: 0.8954  (best train acc: 0.8539, best val acc: 0.9130, best train loss: 0.4211  @ epoch 620 )\n",
      "[Epoch: 0680] train loss: 0.4307, train acc: 0.8448, val loss: 0.2869, val acc: 0.9093  (best train acc: 0.8539, best val acc: 0.9130, best train loss: 0.4211  @ epoch 620 )\n",
      "[Epoch: 0700] train loss: 0.4374, train acc: 0.8428, val loss: 0.2809, val acc: 0.9059  (best train acc: 0.8539, best val acc: 0.9140, best train loss: 0.4189  @ epoch 691 )\n",
      "[Epoch: 0720] train loss: 0.4281, train acc: 0.8472, val loss: 0.2724, val acc: 0.9120  (best train acc: 0.8545, best val acc: 0.9143, best train loss: 0.4159  @ epoch 703 )\n",
      "[Epoch: 0740] train loss: 0.4204, train acc: 0.8493, val loss: 0.2719, val acc: 0.9103  (best train acc: 0.8545, best val acc: 0.9143, best train loss: 0.4159  @ epoch 703 )\n",
      "[Epoch: 0760] train loss: 0.4129, train acc: 0.8560, val loss: 0.2666, val acc: 0.9133  (best train acc: 0.8597, best val acc: 0.9150, best train loss: 0.4078  @ epoch 753 )\n",
      "[Epoch: 0780] train loss: 0.4143, train acc: 0.8592, val loss: 0.2661, val acc: 0.9113  (best train acc: 0.8597, best val acc: 0.9153, best train loss: 0.4069  @ epoch 776 )\n",
      "[Epoch: 0800] train loss: 0.4094, train acc: 0.8557, val loss: 0.2645, val acc: 0.9140  (best train acc: 0.8602, best val acc: 0.9153, best train loss: 0.3980  @ epoch 792 )\n",
      "[Epoch: 0820] train loss: 0.4172, train acc: 0.8565, val loss: 0.2785, val acc: 0.9039  (best train acc: 0.8602, best val acc: 0.9160, best train loss: 0.3971  @ epoch 806 )\n",
      "[Epoch: 0840] train loss: 0.4071, train acc: 0.8548, val loss: 0.2624, val acc: 0.9120  (best train acc: 0.8602, best val acc: 0.9160, best train loss: 0.3971  @ epoch 806 )\n",
      "[Epoch: 0860] train loss: 0.3963, train acc: 0.8590, val loss: 0.2583, val acc: 0.9150  (best train acc: 0.8628, best val acc: 0.9170, best train loss: 0.3928  @ epoch 855 )\n",
      "[Epoch: 0880] train loss: 0.4106, train acc: 0.8542, val loss: 0.2562, val acc: 0.9126  (best train acc: 0.8628, best val acc: 0.9170, best train loss: 0.3928  @ epoch 855 )\n",
      "[Epoch: 0900] train loss: 0.3997, train acc: 0.8589, val loss: 0.2737, val acc: 0.9049  (best train acc: 0.8628, best val acc: 0.9174, best train loss: 0.3928  @ epoch 855 )\n",
      "[Epoch: 0920] train loss: 0.4144, train acc: 0.8492, val loss: 0.2640, val acc: 0.9052  (best train acc: 0.8628, best val acc: 0.9174, best train loss: 0.3928  @ epoch 855 )\n",
      "[Epoch: 0940] train loss: 0.4068, train acc: 0.8587, val loss: 0.2525, val acc: 0.9137  (best train acc: 0.8638, best val acc: 0.9174, best train loss: 0.3884  @ epoch 931 )\n",
      "[Epoch: 0960] train loss: 0.3977, train acc: 0.8580, val loss: 0.2506, val acc: 0.9164  (best train acc: 0.8638, best val acc: 0.9177, best train loss: 0.3859  @ epoch 959 )\n",
      "[Epoch: 0980] train loss: 0.4162, train acc: 0.8489, val loss: 0.2686, val acc: 0.8978  (best train acc: 0.8639, best val acc: 0.9177, best train loss: 0.3851  @ epoch 967 )\n",
      "[Epoch: 1000] train loss: 0.3985, train acc: 0.8594, val loss: 0.2560, val acc: 0.9059  (best train acc: 0.8639, best val acc: 0.9177, best train loss: 0.3843  @ epoch 994 )\n",
      "[Epoch: 1020] train loss: 0.3974, train acc: 0.8565, val loss: 0.2477, val acc: 0.9133  (best train acc: 0.8639, best val acc: 0.9184, best train loss: 0.3843  @ epoch 994 )\n",
      "[Epoch: 1040] train loss: 0.4087, train acc: 0.8577, val loss: 0.2464, val acc: 0.9147  (best train acc: 0.8665, best val acc: 0.9194, best train loss: 0.3749  @ epoch 1033 )\n",
      "[Epoch: 1060] train loss: 0.3824, train acc: 0.8635, val loss: 0.2591, val acc: 0.9019  (best train acc: 0.8691, best val acc: 0.9194, best train loss: 0.3749  @ epoch 1033 )\n",
      "[Epoch: 1080] train loss: 0.3900, train acc: 0.8539, val loss: 0.2460, val acc: 0.9164  (best train acc: 0.8691, best val acc: 0.9194, best train loss: 0.3749  @ epoch 1033 )\n",
      "[Epoch: 1100] train loss: 0.4021, train acc: 0.8540, val loss: 0.2523, val acc: 0.9184  (best train acc: 0.8691, best val acc: 0.9194, best train loss: 0.3740  @ epoch 1088 )\n",
      "[Epoch: 1120] train loss: 0.3896, train acc: 0.8587, val loss: 0.2516, val acc: 0.9110  (best train acc: 0.8691, best val acc: 0.9194, best train loss: 0.3740  @ epoch 1088 )\n",
      "[Epoch: 1140] train loss: 0.3837, train acc: 0.8599, val loss: 0.2499, val acc: 0.9093  (best train acc: 0.8691, best val acc: 0.9197, best train loss: 0.3731  @ epoch 1138 )\n",
      "[Epoch: 1160] train loss: 0.3980, train acc: 0.8576, val loss: 0.2443, val acc: 0.9160  (best train acc: 0.8691, best val acc: 0.9204, best train loss: 0.3731  @ epoch 1138 )\n",
      "[Epoch: 1180] train loss: 0.3809, train acc: 0.8693, val loss: 0.2406, val acc: 0.9191  (best train acc: 0.8694, best val acc: 0.9204, best train loss: 0.3685  @ epoch 1177 )\n",
      "[Epoch: 1200] train loss: 0.3678, train acc: 0.8671, val loss: 0.2442, val acc: 0.9180  (best train acc: 0.8728, best val acc: 0.9204, best train loss: 0.3637  @ epoch 1188 )\n",
      "[Epoch: 1220] train loss: 0.3840, train acc: 0.8665, val loss: 0.2548, val acc: 0.9046  (best train acc: 0.8728, best val acc: 0.9211, best train loss: 0.3579  @ epoch 1206 )\n",
      "[Epoch: 1240] train loss: 0.3725, train acc: 0.8681, val loss: 0.2528, val acc: 0.9130  (best train acc: 0.8728, best val acc: 0.9211, best train loss: 0.3579  @ epoch 1206 )\n",
      "[Epoch: 1260] train loss: 0.3628, train acc: 0.8728, val loss: 0.2446, val acc: 0.9086  (best train acc: 0.8728, best val acc: 0.9211, best train loss: 0.3579  @ epoch 1206 )\n",
      "[Epoch: 1280] train loss: 0.3717, train acc: 0.8687, val loss: 0.2452, val acc: 0.9153  (best train acc: 0.8762, best val acc: 0.9211, best train loss: 0.3579  @ epoch 1206 )\n",
      "[Epoch: 1300] train loss: 0.3711, train acc: 0.8673, val loss: 0.2408, val acc: 0.9160  (best train acc: 0.8762, best val acc: 0.9211, best train loss: 0.3579  @ epoch 1206 )\n",
      "[Epoch: 1320] train loss: 0.3728, train acc: 0.8610, val loss: 0.2402, val acc: 0.9184  (best train acc: 0.8762, best val acc: 0.9211, best train loss: 0.3579  @ epoch 1206 )\n",
      "[Epoch: 1340] train loss: 0.3673, train acc: 0.8691, val loss: 0.2400, val acc: 0.9164  (best train acc: 0.8762, best val acc: 0.9211, best train loss: 0.3571  @ epoch 1326 )\n",
      "[Epoch: 1360] train loss: 0.3660, train acc: 0.8705, val loss: 0.2404, val acc: 0.9164  (best train acc: 0.8762, best val acc: 0.9211, best train loss: 0.3571  @ epoch 1326 )\n",
      "[Epoch: 1380] train loss: 0.3961, train acc: 0.8569, val loss: 0.2415, val acc: 0.9218  (best train acc: 0.8777, best val acc: 0.9218, best train loss: 0.3512  @ epoch 1363 )\n",
      "[Epoch: 1400] train loss: 0.3632, train acc: 0.8691, val loss: 0.2350, val acc: 0.9207  (best train acc: 0.8777, best val acc: 0.9218, best train loss: 0.3512  @ epoch 1363 )\n",
      "[Epoch: 1420] train loss: 0.3607, train acc: 0.8705, val loss: 0.2429, val acc: 0.9153  (best train acc: 0.8777, best val acc: 0.9224, best train loss: 0.3512  @ epoch 1363 )\n",
      "[Epoch: 1440] train loss: 0.3597, train acc: 0.8681, val loss: 0.2366, val acc: 0.9218  (best train acc: 0.8777, best val acc: 0.9224, best train loss: 0.3512  @ epoch 1363 )\n",
      "[Epoch: 1460] train loss: 0.3571, train acc: 0.8737, val loss: 0.2327, val acc: 0.9211  (best train acc: 0.8777, best val acc: 0.9228, best train loss: 0.3512  @ epoch 1363 )\n",
      "[Epoch: 1480] train loss: 0.3769, train acc: 0.8571, val loss: 0.2489, val acc: 0.9137  (best train acc: 0.8777, best val acc: 0.9228, best train loss: 0.3512  @ epoch 1363 )\n",
      "[Epoch: 1500] train loss: 0.3726, train acc: 0.8710, val loss: 0.2510, val acc: 0.9069  (best train acc: 0.8777, best val acc: 0.9231, best train loss: 0.3487  @ epoch 1496 )\n",
      "[Epoch: 1520] train loss: 0.3623, train acc: 0.8709, val loss: 0.2315, val acc: 0.9228  (best train acc: 0.8777, best val acc: 0.9241, best train loss: 0.3484  @ epoch 1513 )\n",
      "[Epoch: 1540] train loss: 0.3416, train acc: 0.8837, val loss: 0.2330, val acc: 0.9201  (best train acc: 0.8837, best val acc: 0.9241, best train loss: 0.3416  @ epoch 1540 )\n",
      "[Epoch: 1560] train loss: 0.3571, train acc: 0.8686, val loss: 0.2387, val acc: 0.9140  (best train acc: 0.8837, best val acc: 0.9241, best train loss: 0.3390  @ epoch 1547 )\n",
      "[Epoch: 1580] train loss: 0.3578, train acc: 0.8772, val loss: 0.2320, val acc: 0.9197  (best train acc: 0.8837, best val acc: 0.9248, best train loss: 0.3390  @ epoch 1547 )\n",
      "[Epoch: 1600] train loss: 0.3471, train acc: 0.8803, val loss: 0.2302, val acc: 0.9218  (best train acc: 0.8837, best val acc: 0.9248, best train loss: 0.3390  @ epoch 1547 )\n",
      "[Epoch: 1620] train loss: 0.3485, train acc: 0.8764, val loss: 0.2328, val acc: 0.9187  (best train acc: 0.8837, best val acc: 0.9248, best train loss: 0.3361  @ epoch 1611 )\n",
      "[Epoch: 1640] train loss: 0.3489, train acc: 0.8751, val loss: 0.2311, val acc: 0.9221  (best train acc: 0.8837, best val acc: 0.9248, best train loss: 0.3361  @ epoch 1611 )\n",
      "[Epoch: 1660] train loss: 0.3600, train acc: 0.8648, val loss: 0.2324, val acc: 0.9224  (best train acc: 0.8837, best val acc: 0.9248, best train loss: 0.3361  @ epoch 1611 )\n",
      "[Epoch: 1680] train loss: 0.3465, train acc: 0.8772, val loss: 0.2316, val acc: 0.9177  (best train acc: 0.8837, best val acc: 0.9248, best train loss: 0.3361  @ epoch 1611 )\n",
      "[Epoch: 1700] train loss: 0.3618, train acc: 0.8649, val loss: 0.2412, val acc: 0.9231  (best train acc: 0.8837, best val acc: 0.9248, best train loss: 0.3361  @ epoch 1611 )\n",
      "[Epoch: 1720] train loss: 0.3507, train acc: 0.8793, val loss: 0.2335, val acc: 0.9157  (best train acc: 0.8837, best val acc: 0.9248, best train loss: 0.3361  @ epoch 1611 )\n",
      "[Epoch: 1740] train loss: 0.3412, train acc: 0.8807, val loss: 0.2287, val acc: 0.9207  (best train acc: 0.8837, best val acc: 0.9248, best train loss: 0.3361  @ epoch 1611 )\n",
      "[Epoch: 1760] train loss: 0.3366, train acc: 0.8821, val loss: 0.2287, val acc: 0.9221  (best train acc: 0.8837, best val acc: 0.9248, best train loss: 0.3361  @ epoch 1611 )\n",
      "[Epoch: 1780] train loss: 0.3373, train acc: 0.8794, val loss: 0.2389, val acc: 0.9113  (best train acc: 0.8837, best val acc: 0.9248, best train loss: 0.3361  @ epoch 1611 )\n",
      "[Epoch: 1800] train loss: 0.3463, train acc: 0.8770, val loss: 0.2256, val acc: 0.9201  (best train acc: 0.8844, best val acc: 0.9248, best train loss: 0.3304  @ epoch 1798 )\n",
      "[Epoch: 1820] train loss: 0.3395, train acc: 0.8798, val loss: 0.2246, val acc: 0.9228  (best train acc: 0.8875, best val acc: 0.9248, best train loss: 0.3260  @ epoch 1818 )\n",
      "[Epoch: 1840] train loss: 0.3458, train acc: 0.8776, val loss: 0.2537, val acc: 0.9086  (best train acc: 0.8875, best val acc: 0.9248, best train loss: 0.3260  @ epoch 1818 )\n",
      "[Epoch: 1860] train loss: 0.3436, train acc: 0.8757, val loss: 0.2351, val acc: 0.9140  (best train acc: 0.8875, best val acc: 0.9248, best train loss: 0.3260  @ epoch 1818 )\n",
      "[Epoch: 1880] train loss: 0.3372, train acc: 0.8799, val loss: 0.2460, val acc: 0.9143  (best train acc: 0.8875, best val acc: 0.9248, best train loss: 0.3260  @ epoch 1818 )\n",
      "[Epoch: 1900] train loss: 0.3802, train acc: 0.8646, val loss: 0.2312, val acc: 0.9174  (best train acc: 0.8875, best val acc: 0.9248, best train loss: 0.3260  @ epoch 1818 )\n",
      "[Epoch: 1920] train loss: 0.3369, train acc: 0.8811, val loss: 0.2240, val acc: 0.9191  (best train acc: 0.8880, best val acc: 0.9248, best train loss: 0.3221  @ epoch 1917 )\n",
      "[Epoch: 1940] train loss: 0.3370, train acc: 0.8801, val loss: 0.2282, val acc: 0.9177  (best train acc: 0.8897, best val acc: 0.9248, best train loss: 0.3221  @ epoch 1917 )\n",
      "[Epoch: 1960] train loss: 0.3339, train acc: 0.8826, val loss: 0.2296, val acc: 0.9170  (best train acc: 0.8898, best val acc: 0.9248, best train loss: 0.3207  @ epoch 1959 )\n",
      "[Epoch: 1980] train loss: 0.3264, train acc: 0.8860, val loss: 0.2197, val acc: 0.9218  (best train acc: 0.8898, best val acc: 0.9248, best train loss: 0.3207  @ epoch 1959 )\n",
      "[Epoch: 2000] train loss: 0.3316, train acc: 0.8820, val loss: 0.2270, val acc: 0.9207  (best train acc: 0.8898, best val acc: 0.9248, best train loss: 0.3207  @ epoch 1959 )\n",
      "[Epoch: 2020] train loss: 0.3211, train acc: 0.8899, val loss: 0.2201, val acc: 0.9218  (best train acc: 0.8899, best val acc: 0.9248, best train loss: 0.3194  @ epoch 2016 )\n",
      "[Epoch: 2040] train loss: 0.3256, train acc: 0.8843, val loss: 0.2218, val acc: 0.9211  (best train acc: 0.8899, best val acc: 0.9248, best train loss: 0.3194  @ epoch 2016 )\n",
      "[Epoch: 2060] train loss: 0.3428, train acc: 0.8815, val loss: 0.2234, val acc: 0.9204  (best train acc: 0.8899, best val acc: 0.9248, best train loss: 0.3194  @ epoch 2016 )\n",
      "[Epoch: 2080] train loss: 0.3267, train acc: 0.8843, val loss: 0.2287, val acc: 0.9204  (best train acc: 0.8900, best val acc: 0.9248, best train loss: 0.3145  @ epoch 2076 )\n",
      "[Epoch: 2100] train loss: 0.3303, train acc: 0.8837, val loss: 0.2283, val acc: 0.9174  (best train acc: 0.8903, best val acc: 0.9248, best train loss: 0.3145  @ epoch 2076 )\n",
      "[Epoch: 2120] train loss: 0.3153, train acc: 0.8884, val loss: 0.2233, val acc: 0.9228  (best train acc: 0.8908, best val acc: 0.9251, best train loss: 0.3145  @ epoch 2076 )\n",
      "[Epoch: 2140] train loss: 0.3462, train acc: 0.8785, val loss: 0.2208, val acc: 0.9204  (best train acc: 0.8934, best val acc: 0.9251, best train loss: 0.3071  @ epoch 2130 )\n",
      "[Epoch: 2160] train loss: 0.3207, train acc: 0.8886, val loss: 0.2221, val acc: 0.9214  (best train acc: 0.8934, best val acc: 0.9251, best train loss: 0.3071  @ epoch 2130 )\n",
      "[Epoch: 2180] train loss: 0.3227, train acc: 0.8848, val loss: 0.2218, val acc: 0.9204  (best train acc: 0.8934, best val acc: 0.9251, best train loss: 0.3071  @ epoch 2130 )\n",
      "[Epoch: 2200] train loss: 0.3303, train acc: 0.8832, val loss: 0.2323, val acc: 0.9150  (best train acc: 0.8934, best val acc: 0.9251, best train loss: 0.3071  @ epoch 2130 )\n",
      "[Epoch: 2220] train loss: 0.3305, train acc: 0.8833, val loss: 0.2495, val acc: 0.9096  (best train acc: 0.8934, best val acc: 0.9251, best train loss: 0.3071  @ epoch 2130 )\n",
      "[Epoch: 2240] train loss: 0.3180, train acc: 0.8900, val loss: 0.2205, val acc: 0.9214  (best train acc: 0.8934, best val acc: 0.9251, best train loss: 0.3071  @ epoch 2130 )\n",
      "[Epoch: 2260] train loss: 0.3191, train acc: 0.8812, val loss: 0.2239, val acc: 0.9218  (best train acc: 0.8934, best val acc: 0.9251, best train loss: 0.3071  @ epoch 2130 )\n",
      "[Epoch: 2280] train loss: 0.3311, train acc: 0.8824, val loss: 0.2291, val acc: 0.9170  (best train acc: 0.8934, best val acc: 0.9251, best train loss: 0.3071  @ epoch 2130 )\n",
      "[Epoch: 2300] train loss: 0.3215, train acc: 0.8868, val loss: 0.2289, val acc: 0.9191  (best train acc: 0.8934, best val acc: 0.9251, best train loss: 0.3071  @ epoch 2130 )\n",
      "[Epoch: 2320] train loss: 0.3109, train acc: 0.8861, val loss: 0.2283, val acc: 0.9164  (best train acc: 0.8934, best val acc: 0.9251, best train loss: 0.3071  @ epoch 2130 )\n",
      "[Epoch: 2340] train loss: 0.3435, train acc: 0.8707, val loss: 0.2596, val acc: 0.8958  (best train acc: 0.8934, best val acc: 0.9258, best train loss: 0.3071  @ epoch 2130 )\n",
      "[Epoch: 2360] train loss: 0.3242, train acc: 0.8893, val loss: 0.2531, val acc: 0.9073  (best train acc: 0.8934, best val acc: 0.9258, best train loss: 0.3071  @ epoch 2130 )\n",
      "[Epoch: 2380] train loss: 0.3243, train acc: 0.8858, val loss: 0.2244, val acc: 0.9231  (best train acc: 0.8934, best val acc: 0.9258, best train loss: 0.3071  @ epoch 2130 )\n",
      "[Epoch: 2400] train loss: 0.3084, train acc: 0.8924, val loss: 0.2202, val acc: 0.9224  (best train acc: 0.8936, best val acc: 0.9258, best train loss: 0.3065  @ epoch 2394 )\n",
      "[Epoch: 2420] train loss: 0.3092, train acc: 0.8922, val loss: 0.2188, val acc: 0.9234  (best train acc: 0.8944, best val acc: 0.9258, best train loss: 0.3057  @ epoch 2411 )\n",
      "[Epoch: 2440] train loss: 0.3089, train acc: 0.8906, val loss: 0.2358, val acc: 0.9153  (best train acc: 0.8944, best val acc: 0.9258, best train loss: 0.3043  @ epoch 2437 )\n",
      "[Epoch: 2460] train loss: 0.3112, train acc: 0.8902, val loss: 0.2199, val acc: 0.9177  (best train acc: 0.8958, best val acc: 0.9258, best train loss: 0.3028  @ epoch 2453 )\n",
      "[Epoch: 2480] train loss: 0.3194, train acc: 0.8857, val loss: 0.2168, val acc: 0.9221  (best train acc: 0.8966, best val acc: 0.9258, best train loss: 0.2973  @ epoch 2479 )\n",
      "[Epoch: 2500] train loss: 0.3099, train acc: 0.8937, val loss: 0.2272, val acc: 0.9157  (best train acc: 0.8999, best val acc: 0.9258, best train loss: 0.2973  @ epoch 2479 )\n",
      "[Epoch: 2520] train loss: 0.3102, train acc: 0.8893, val loss: 0.2210, val acc: 0.9207  (best train acc: 0.8999, best val acc: 0.9258, best train loss: 0.2973  @ epoch 2479 )\n",
      "[Epoch: 2540] train loss: 0.3105, train acc: 0.8932, val loss: 0.2211, val acc: 0.9204  (best train acc: 0.8999, best val acc: 0.9258, best train loss: 0.2962  @ epoch 2531 )\n",
      "[Epoch: 2560] train loss: 0.3105, train acc: 0.8902, val loss: 0.2358, val acc: 0.9140  (best train acc: 0.8999, best val acc: 0.9258, best train loss: 0.2962  @ epoch 2531 )\n",
      "[Epoch: 2580] train loss: 0.3115, train acc: 0.8916, val loss: 0.2248, val acc: 0.9180  (best train acc: 0.8999, best val acc: 0.9258, best train loss: 0.2962  @ epoch 2531 )\n",
      "[Epoch: 2600] train loss: 0.3104, train acc: 0.8928, val loss: 0.2248, val acc: 0.9191  (best train acc: 0.8999, best val acc: 0.9258, best train loss: 0.2942  @ epoch 2591 )\n",
      "[Epoch: 2620] train loss: 0.3081, train acc: 0.8928, val loss: 0.2380, val acc: 0.9130  (best train acc: 0.8999, best val acc: 0.9258, best train loss: 0.2942  @ epoch 2591 )\n",
      "[Epoch: 2640] train loss: 0.2947, train acc: 0.8987, val loss: 0.2164, val acc: 0.9231  (best train acc: 0.8999, best val acc: 0.9258, best train loss: 0.2942  @ epoch 2591 )\n",
      "[Epoch: 2660] train loss: 0.3017, train acc: 0.8932, val loss: 0.2179, val acc: 0.9221  (best train acc: 0.8999, best val acc: 0.9261, best train loss: 0.2942  @ epoch 2591 )\n",
      "[Epoch: 2680] train loss: 0.3345, train acc: 0.8817, val loss: 0.2244, val acc: 0.9177  (best train acc: 0.8999, best val acc: 0.9261, best train loss: 0.2942  @ epoch 2591 )\n",
      "[Epoch: 2700] train loss: 0.2957, train acc: 0.8953, val loss: 0.2180, val acc: 0.9255  (best train acc: 0.8999, best val acc: 0.9261, best train loss: 0.2942  @ epoch 2591 )\n",
      "[Epoch: 2720] train loss: 0.3036, train acc: 0.8922, val loss: 0.2189, val acc: 0.9251  (best train acc: 0.8999, best val acc: 0.9261, best train loss: 0.2942  @ epoch 2591 )\n",
      "[Epoch: 2740] train loss: 0.3081, train acc: 0.8916, val loss: 0.2278, val acc: 0.9201  (best train acc: 0.8999, best val acc: 0.9261, best train loss: 0.2942  @ epoch 2591 )\n",
      "[Epoch: 2760] train loss: 0.2909, train acc: 0.8991, val loss: 0.2243, val acc: 0.9211  (best train acc: 0.8999, best val acc: 0.9268, best train loss: 0.2909  @ epoch 2760 )\n",
      "[Epoch: 2780] train loss: 0.3038, train acc: 0.8927, val loss: 0.2137, val acc: 0.9238  (best train acc: 0.8999, best val acc: 0.9268, best train loss: 0.2909  @ epoch 2760 )\n",
      "[Epoch: 2800] train loss: 0.2918, train acc: 0.8976, val loss: 0.2165, val acc: 0.9228  (best train acc: 0.8999, best val acc: 0.9268, best train loss: 0.2909  @ epoch 2760 )\n",
      "[Epoch: 2820] train loss: 0.3015, train acc: 0.8911, val loss: 0.2195, val acc: 0.9231  (best train acc: 0.8999, best val acc: 0.9268, best train loss: 0.2870  @ epoch 2815 )\n",
      "[Epoch: 2840] train loss: 0.2944, train acc: 0.8971, val loss: 0.2150, val acc: 0.9255  (best train acc: 0.9029, best val acc: 0.9268, best train loss: 0.2835  @ epoch 2833 )\n",
      "[Epoch: 2860] train loss: 0.2935, train acc: 0.8981, val loss: 0.2157, val acc: 0.9248  (best train acc: 0.9029, best val acc: 0.9268, best train loss: 0.2832  @ epoch 2853 )\n",
      "[Epoch: 2880] train loss: 0.2823, train acc: 0.8986, val loss: 0.2165, val acc: 0.9241  (best train acc: 0.9029, best val acc: 0.9268, best train loss: 0.2823  @ epoch 2880 )\n",
      "[Epoch: 2900] train loss: 0.2831, train acc: 0.9018, val loss: 0.2170, val acc: 0.9228  (best train acc: 0.9029, best val acc: 0.9275, best train loss: 0.2823  @ epoch 2880 )\n",
      "[Epoch: 2920] train loss: 0.3037, train acc: 0.8921, val loss: 0.2186, val acc: 0.9214  (best train acc: 0.9029, best val acc: 0.9275, best train loss: 0.2823  @ epoch 2880 )\n",
      "[Epoch: 2940] train loss: 0.3066, train acc: 0.8918, val loss: 0.2127, val acc: 0.9275  (best train acc: 0.9034, best val acc: 0.9275, best train loss: 0.2823  @ epoch 2880 )\n",
      "[Epoch: 2960] train loss: 0.3074, train acc: 0.8880, val loss: 0.2148, val acc: 0.9228  (best train acc: 0.9034, best val acc: 0.9275, best train loss: 0.2823  @ epoch 2880 )\n",
      "[Epoch: 2980] train loss: 0.2889, train acc: 0.8999, val loss: 0.2145, val acc: 0.9261  (best train acc: 0.9034, best val acc: 0.9275, best train loss: 0.2804  @ epoch 2977 )\n",
      "[Epoch: 3000] train loss: 0.2942, train acc: 0.8950, val loss: 0.2126, val acc: 0.9255  (best train acc: 0.9044, best val acc: 0.9275, best train loss: 0.2797  @ epoch 2990 )\n",
      "[Epoch: 3020] train loss: 0.2914, train acc: 0.8973, val loss: 0.2156, val acc: 0.9248  (best train acc: 0.9058, best val acc: 0.9275, best train loss: 0.2769  @ epoch 3004 )\n",
      "[Epoch: 3040] train loss: 0.2846, train acc: 0.9007, val loss: 0.2204, val acc: 0.9248  (best train acc: 0.9058, best val acc: 0.9275, best train loss: 0.2769  @ epoch 3004 )\n",
      "[Epoch: 3060] train loss: 0.2905, train acc: 0.8984, val loss: 0.2160, val acc: 0.9265  (best train acc: 0.9058, best val acc: 0.9275, best train loss: 0.2769  @ epoch 3004 )\n",
      "[Epoch: 3080] train loss: 0.2915, train acc: 0.8940, val loss: 0.2263, val acc: 0.9221  (best train acc: 0.9058, best val acc: 0.9275, best train loss: 0.2769  @ epoch 3004 )\n",
      "[Epoch: 3100] train loss: 0.3287, train acc: 0.8785, val loss: 0.2522, val acc: 0.9022  (best train acc: 0.9058, best val acc: 0.9275, best train loss: 0.2769  @ epoch 3004 )\n",
      "[Epoch: 3120] train loss: 0.2844, train acc: 0.9007, val loss: 0.2131, val acc: 0.9268  (best train acc: 0.9058, best val acc: 0.9285, best train loss: 0.2769  @ epoch 3004 )\n",
      "[Epoch: 3140] train loss: 0.2768, train acc: 0.9003, val loss: 0.2230, val acc: 0.9218  (best train acc: 0.9058, best val acc: 0.9285, best train loss: 0.2768  @ epoch 3140 )\n",
      "[Epoch: 3160] train loss: 0.3117, train acc: 0.8855, val loss: 0.2214, val acc: 0.9231  (best train acc: 0.9060, best val acc: 0.9288, best train loss: 0.2749  @ epoch 3154 )\n",
      "[Epoch: 3180] train loss: 0.2920, train acc: 0.8968, val loss: 0.2145, val acc: 0.9261  (best train acc: 0.9060, best val acc: 0.9302, best train loss: 0.2749  @ epoch 3154 )\n",
      "[Epoch: 3200] train loss: 0.2864, train acc: 0.8978, val loss: 0.2258, val acc: 0.9224  (best train acc: 0.9091, best val acc: 0.9302, best train loss: 0.2728  @ epoch 3199 )\n",
      "[Epoch: 3220] train loss: 0.2788, train acc: 0.9037, val loss: 0.2170, val acc: 0.9268  (best train acc: 0.9091, best val acc: 0.9302, best train loss: 0.2717  @ epoch 3214 )\n",
      "[Epoch: 3240] train loss: 0.2952, train acc: 0.8963, val loss: 0.2231, val acc: 0.9251  (best train acc: 0.9091, best val acc: 0.9302, best train loss: 0.2717  @ epoch 3214 )\n",
      "[Epoch: 3260] train loss: 0.2925, train acc: 0.8974, val loss: 0.2313, val acc: 0.9174  (best train acc: 0.9091, best val acc: 0.9302, best train loss: 0.2717  @ epoch 3214 )\n",
      "[Epoch: 3280] train loss: 0.2816, train acc: 0.8997, val loss: 0.2108, val acc: 0.9288  (best train acc: 0.9091, best val acc: 0.9302, best train loss: 0.2717  @ epoch 3214 )\n",
      "[Epoch: 3300] train loss: 0.2875, train acc: 0.9017, val loss: 0.2297, val acc: 0.9207  (best train acc: 0.9091, best val acc: 0.9305, best train loss: 0.2714  @ epoch 3289 )\n",
      "[Epoch: 3320] train loss: 0.2822, train acc: 0.9005, val loss: 0.2206, val acc: 0.9248  (best train acc: 0.9091, best val acc: 0.9305, best train loss: 0.2695  @ epoch 3310 )\n",
      "[Epoch: 3340] train loss: 0.2831, train acc: 0.9012, val loss: 0.2115, val acc: 0.9282  (best train acc: 0.9091, best val acc: 0.9305, best train loss: 0.2695  @ epoch 3310 )\n",
      "[Epoch: 3360] train loss: 0.2776, train acc: 0.9043, val loss: 0.2405, val acc: 0.9157  (best train acc: 0.9091, best val acc: 0.9305, best train loss: 0.2671  @ epoch 3347 )\n",
      "[Epoch: 3380] train loss: 0.2875, train acc: 0.8926, val loss: 0.2132, val acc: 0.9285  (best train acc: 0.9091, best val acc: 0.9305, best train loss: 0.2671  @ epoch 3347 )\n",
      "[Epoch: 3400] train loss: 0.2854, train acc: 0.8997, val loss: 0.2100, val acc: 0.9298  (best train acc: 0.9091, best val acc: 0.9305, best train loss: 0.2671  @ epoch 3347 )\n",
      "[Epoch: 3420] train loss: 0.2803, train acc: 0.8974, val loss: 0.2084, val acc: 0.9288  (best train acc: 0.9091, best val acc: 0.9305, best train loss: 0.2658  @ epoch 3412 )\n",
      "[Epoch: 3440] train loss: 0.2847, train acc: 0.8957, val loss: 0.2149, val acc: 0.9261  (best train acc: 0.9091, best val acc: 0.9305, best train loss: 0.2658  @ epoch 3412 )\n",
      "[Epoch: 3460] train loss: 0.2809, train acc: 0.9025, val loss: 0.2147, val acc: 0.9261  (best train acc: 0.9091, best val acc: 0.9315, best train loss: 0.2658  @ epoch 3412 )\n",
      "[Epoch: 3480] train loss: 0.2918, train acc: 0.8936, val loss: 0.2099, val acc: 0.9309  (best train acc: 0.9091, best val acc: 0.9315, best train loss: 0.2658  @ epoch 3412 )\n",
      "[Epoch: 3500] train loss: 0.2758, train acc: 0.9033, val loss: 0.2152, val acc: 0.9275  (best train acc: 0.9091, best val acc: 0.9315, best train loss: 0.2658  @ epoch 3412 )\n",
      "[Epoch: 3520] train loss: 0.2566, train acc: 0.9093, val loss: 0.2104, val acc: 0.9295  (best train acc: 0.9093, best val acc: 0.9315, best train loss: 0.2566  @ epoch 3520 )\n",
      "[Epoch: 3540] train loss: 0.2656, train acc: 0.9050, val loss: 0.2088, val acc: 0.9282  (best train acc: 0.9093, best val acc: 0.9325, best train loss: 0.2566  @ epoch 3520 )\n",
      "[Epoch: 3560] train loss: 0.2679, train acc: 0.9050, val loss: 0.2098, val acc: 0.9285  (best train acc: 0.9093, best val acc: 0.9325, best train loss: 0.2566  @ epoch 3520 )\n",
      "[Epoch: 3580] train loss: 0.2830, train acc: 0.9017, val loss: 0.2106, val acc: 0.9275  (best train acc: 0.9117, best val acc: 0.9325, best train loss: 0.2566  @ epoch 3520 )\n",
      "[Epoch: 3600] train loss: 0.2658, train acc: 0.9078, val loss: 0.2073, val acc: 0.9305  (best train acc: 0.9117, best val acc: 0.9325, best train loss: 0.2566  @ epoch 3520 )\n",
      "[Epoch: 3620] train loss: 0.2720, train acc: 0.9058, val loss: 0.2075, val acc: 0.9298  (best train acc: 0.9117, best val acc: 0.9325, best train loss: 0.2566  @ epoch 3520 )\n",
      "[Epoch: 3640] train loss: 0.2658, train acc: 0.9071, val loss: 0.2122, val acc: 0.9285  (best train acc: 0.9117, best val acc: 0.9325, best train loss: 0.2566  @ epoch 3520 )\n",
      "[Epoch: 3660] train loss: 0.3461, train acc: 0.8784, val loss: 0.2439, val acc: 0.9066  (best train acc: 0.9117, best val acc: 0.9325, best train loss: 0.2566  @ epoch 3520 )\n",
      "[Epoch: 3680] train loss: 0.2723, train acc: 0.9016, val loss: 0.2359, val acc: 0.9194  (best train acc: 0.9117, best val acc: 0.9325, best train loss: 0.2566  @ epoch 3520 )\n",
      "[Epoch: 3700] train loss: 0.2780, train acc: 0.8960, val loss: 0.2107, val acc: 0.9295  (best train acc: 0.9117, best val acc: 0.9325, best train loss: 0.2566  @ epoch 3520 )\n",
      "[Epoch: 3720] train loss: 0.2882, train acc: 0.8950, val loss: 0.2141, val acc: 0.9272  (best train acc: 0.9117, best val acc: 0.9325, best train loss: 0.2566  @ epoch 3520 )\n",
      "[Epoch: 3740] train loss: 0.2657, train acc: 0.9075, val loss: 0.2051, val acc: 0.9302  (best train acc: 0.9131, best val acc: 0.9336, best train loss: 0.2565  @ epoch 3727 )\n",
      "[Epoch: 3760] train loss: 0.2702, train acc: 0.9062, val loss: 0.2068, val acc: 0.9332  (best train acc: 0.9131, best val acc: 0.9336, best train loss: 0.2565  @ epoch 3727 )\n",
      "[Epoch: 3780] train loss: 0.2624, train acc: 0.9053, val loss: 0.2079, val acc: 0.9298  (best train acc: 0.9131, best val acc: 0.9336, best train loss: 0.2565  @ epoch 3727 )\n",
      "[Epoch: 3800] train loss: 0.2592, train acc: 0.9071, val loss: 0.2055, val acc: 0.9312  (best train acc: 0.9131, best val acc: 0.9336, best train loss: 0.2564  @ epoch 3796 )\n",
      "[Epoch: 3820] train loss: 0.2667, train acc: 0.9125, val loss: 0.2056, val acc: 0.9312  (best train acc: 0.9131, best val acc: 0.9336, best train loss: 0.2529  @ epoch 3805 )\n",
      "[Epoch: 3840] train loss: 0.2663, train acc: 0.9076, val loss: 0.2124, val acc: 0.9295  (best train acc: 0.9143, best val acc: 0.9336, best train loss: 0.2520  @ epoch 3839 )\n",
      "[Epoch: 3860] train loss: 0.2584, train acc: 0.9102, val loss: 0.2078, val acc: 0.9309  (best train acc: 0.9143, best val acc: 0.9336, best train loss: 0.2520  @ epoch 3839 )\n",
      "[Epoch: 3880] train loss: 0.2521, train acc: 0.9114, val loss: 0.2101, val acc: 0.9302  (best train acc: 0.9143, best val acc: 0.9336, best train loss: 0.2516  @ epoch 3874 )\n",
      "[Epoch: 3900] train loss: 0.2756, train acc: 0.8968, val loss: 0.2090, val acc: 0.9315  (best train acc: 0.9143, best val acc: 0.9336, best train loss: 0.2516  @ epoch 3874 )\n",
      "[Epoch: 3920] train loss: 0.2628, train acc: 0.9050, val loss: 0.2182, val acc: 0.9261  (best train acc: 0.9143, best val acc: 0.9336, best train loss: 0.2516  @ epoch 3874 )\n",
      "[Epoch: 3940] train loss: 0.2664, train acc: 0.9061, val loss: 0.2100, val acc: 0.9302  (best train acc: 0.9143, best val acc: 0.9336, best train loss: 0.2516  @ epoch 3874 )\n",
      "[Epoch: 3960] train loss: 0.2561, train acc: 0.9104, val loss: 0.2060, val acc: 0.9325  (best train acc: 0.9143, best val acc: 0.9339, best train loss: 0.2516  @ epoch 3874 )\n",
      "[Epoch: 3980] train loss: 0.3129, train acc: 0.8890, val loss: 0.2263, val acc: 0.9197  (best train acc: 0.9143, best val acc: 0.9339, best train loss: 0.2506  @ epoch 3962 )\n",
      "[Epoch: 4000] train loss: 0.2600, train acc: 0.9119, val loss: 0.2056, val acc: 0.9325  (best train acc: 0.9143, best val acc: 0.9339, best train loss: 0.2506  @ epoch 3962 )\n",
      "[Epoch: 4020] train loss: 0.2761, train acc: 0.9028, val loss: 0.2118, val acc: 0.9292  (best train acc: 0.9143, best val acc: 0.9339, best train loss: 0.2506  @ epoch 3962 )\n",
      "[Epoch: 4040] train loss: 0.2580, train acc: 0.9088, val loss: 0.2229, val acc: 0.9228  (best train acc: 0.9143, best val acc: 0.9339, best train loss: 0.2506  @ epoch 3962 )\n",
      "[Epoch: 4060] train loss: 0.2803, train acc: 0.9028, val loss: 0.2315, val acc: 0.9201  (best train acc: 0.9143, best val acc: 0.9342, best train loss: 0.2504  @ epoch 4051 )\n",
      "[Epoch: 4080] train loss: 0.2622, train acc: 0.9076, val loss: 0.2186, val acc: 0.9295  (best train acc: 0.9143, best val acc: 0.9342, best train loss: 0.2504  @ epoch 4051 )\n",
      "[Epoch: 4100] train loss: 0.2577, train acc: 0.9073, val loss: 0.2151, val acc: 0.9272  (best train acc: 0.9143, best val acc: 0.9346, best train loss: 0.2504  @ epoch 4051 )\n",
      "[Epoch: 4120] train loss: 0.2705, train acc: 0.9051, val loss: 0.2087, val acc: 0.9315  (best train acc: 0.9144, best val acc: 0.9346, best train loss: 0.2494  @ epoch 4107 )\n",
      "[Epoch: 4140] train loss: 0.2539, train acc: 0.9132, val loss: 0.2371, val acc: 0.9238  (best train acc: 0.9144, best val acc: 0.9346, best train loss: 0.2486  @ epoch 4121 )\n",
      "[Epoch: 4160] train loss: 0.2752, train acc: 0.8972, val loss: 0.2179, val acc: 0.9268  (best train acc: 0.9144, best val acc: 0.9346, best train loss: 0.2486  @ epoch 4121 )\n",
      "[Epoch: 4180] train loss: 0.2776, train acc: 0.8944, val loss: 0.2188, val acc: 0.9295  (best train acc: 0.9144, best val acc: 0.9346, best train loss: 0.2486  @ epoch 4121 )\n",
      "[Epoch: 4200] train loss: 0.2570, train acc: 0.9114, val loss: 0.2253, val acc: 0.9228  (best train acc: 0.9144, best val acc: 0.9346, best train loss: 0.2467  @ epoch 4199 )\n",
      "[Epoch: 4220] train loss: 0.2585, train acc: 0.9106, val loss: 0.2336, val acc: 0.9234  (best train acc: 0.9144, best val acc: 0.9346, best train loss: 0.2467  @ epoch 4199 )\n",
      "[Epoch: 4240] train loss: 0.2498, train acc: 0.9124, val loss: 0.2074, val acc: 0.9342  (best train acc: 0.9145, best val acc: 0.9352, best train loss: 0.2467  @ epoch 4199 )\n",
      "[Epoch: 4260] train loss: 0.2619, train acc: 0.9090, val loss: 0.2216, val acc: 0.9258  (best train acc: 0.9145, best val acc: 0.9352, best train loss: 0.2467  @ epoch 4199 )\n",
      "[Epoch: 4280] train loss: 0.2500, train acc: 0.9128, val loss: 0.2098, val acc: 0.9329  (best train acc: 0.9145, best val acc: 0.9352, best train loss: 0.2467  @ epoch 4199 )\n",
      "[Epoch: 4300] train loss: 0.2618, train acc: 0.9032, val loss: 0.2588, val acc: 0.9120  (best train acc: 0.9158, best val acc: 0.9352, best train loss: 0.2425  @ epoch 4288 )\n",
      "[Epoch: 4320] train loss: 0.2499, train acc: 0.9098, val loss: 0.2103, val acc: 0.9302  (best train acc: 0.9158, best val acc: 0.9352, best train loss: 0.2425  @ epoch 4288 )\n",
      "[Epoch: 4340] train loss: 0.2493, train acc: 0.9108, val loss: 0.2164, val acc: 0.9305  (best train acc: 0.9158, best val acc: 0.9352, best train loss: 0.2425  @ epoch 4288 )\n",
      "[Epoch: 4360] train loss: 0.2484, train acc: 0.9132, val loss: 0.2122, val acc: 0.9315  (best train acc: 0.9158, best val acc: 0.9352, best train loss: 0.2396  @ epoch 4345 )\n",
      "[Epoch: 4380] train loss: 0.2528, train acc: 0.9111, val loss: 0.2095, val acc: 0.9292  (best train acc: 0.9171, best val acc: 0.9356, best train loss: 0.2396  @ epoch 4345 )\n",
      "[Epoch: 4400] train loss: 0.2528, train acc: 0.9134, val loss: 0.2243, val acc: 0.9268  (best train acc: 0.9171, best val acc: 0.9356, best train loss: 0.2396  @ epoch 4345 )\n",
      "[Epoch: 4420] train loss: 0.2864, train acc: 0.9002, val loss: 0.2167, val acc: 0.9248  (best train acc: 0.9171, best val acc: 0.9356, best train loss: 0.2396  @ epoch 4345 )\n",
      "[Epoch: 4440] train loss: 0.2511, train acc: 0.9120, val loss: 0.2135, val acc: 0.9298  (best train acc: 0.9171, best val acc: 0.9356, best train loss: 0.2396  @ epoch 4345 )\n",
      "[Epoch: 4460] train loss: 0.2431, train acc: 0.9156, val loss: 0.2096, val acc: 0.9346  (best train acc: 0.9171, best val acc: 0.9363, best train loss: 0.2388  @ epoch 4451 )\n",
      "[Epoch: 4480] train loss: 0.2888, train acc: 0.8930, val loss: 0.2114, val acc: 0.9292  (best train acc: 0.9171, best val acc: 0.9363, best train loss: 0.2372  @ epoch 4464 )\n",
      "[Epoch: 4500] train loss: 0.2583, train acc: 0.9075, val loss: 0.2054, val acc: 0.9309  (best train acc: 0.9171, best val acc: 0.9363, best train loss: 0.2372  @ epoch 4464 )\n",
      "[Epoch: 4520] train loss: 0.2446, train acc: 0.9132, val loss: 0.2091, val acc: 0.9309  (best train acc: 0.9171, best val acc: 0.9366, best train loss: 0.2372  @ epoch 4464 )\n",
      "[Epoch: 4540] train loss: 0.2470, train acc: 0.9140, val loss: 0.2060, val acc: 0.9346  (best train acc: 0.9171, best val acc: 0.9366, best train loss: 0.2372  @ epoch 4464 )\n",
      "[Epoch: 4560] train loss: 0.2659, train acc: 0.9008, val loss: 0.2467, val acc: 0.9201  (best train acc: 0.9171, best val acc: 0.9366, best train loss: 0.2372  @ epoch 4464 )\n",
      "[Epoch: 4580] train loss: 0.2734, train acc: 0.9023, val loss: 0.2317, val acc: 0.9207  (best train acc: 0.9171, best val acc: 0.9366, best train loss: 0.2372  @ epoch 4464 )\n",
      "[Epoch: 4600] train loss: 0.2666, train acc: 0.9046, val loss: 0.2086, val acc: 0.9349  (best train acc: 0.9178, best val acc: 0.9366, best train loss: 0.2372  @ epoch 4464 )\n",
      "[Epoch: 4620] train loss: 0.2447, train acc: 0.9138, val loss: 0.2067, val acc: 0.9315  (best train acc: 0.9178, best val acc: 0.9366, best train loss: 0.2372  @ epoch 4464 )\n",
      "[Epoch: 4640] train loss: 0.2460, train acc: 0.9134, val loss: 0.2111, val acc: 0.9325  (best train acc: 0.9178, best val acc: 0.9366, best train loss: 0.2372  @ epoch 4464 )\n",
      "[Epoch: 4660] train loss: 0.2428, train acc: 0.9137, val loss: 0.2056, val acc: 0.9319  (best train acc: 0.9178, best val acc: 0.9379, best train loss: 0.2372  @ epoch 4464 )\n",
      "[Epoch: 4680] train loss: 0.2403, train acc: 0.9164, val loss: 0.2070, val acc: 0.9322  (best train acc: 0.9178, best val acc: 0.9379, best train loss: 0.2355  @ epoch 4675 )\n",
      "[Epoch: 4700] train loss: 0.2946, train acc: 0.8891, val loss: 0.2074, val acc: 0.9322  (best train acc: 0.9182, best val acc: 0.9379, best train loss: 0.2355  @ epoch 4675 )\n",
      "[Epoch: 4720] train loss: 0.2662, train acc: 0.9027, val loss: 0.2149, val acc: 0.9278  (best train acc: 0.9182, best val acc: 0.9379, best train loss: 0.2355  @ epoch 4675 )\n",
      "[Epoch: 4740] train loss: 0.2584, train acc: 0.8999, val loss: 0.2071, val acc: 0.9315  (best train acc: 0.9182, best val acc: 0.9379, best train loss: 0.2355  @ epoch 4675 )\n",
      "[Epoch: 4760] train loss: 0.2496, train acc: 0.9111, val loss: 0.2062, val acc: 0.9339  (best train acc: 0.9182, best val acc: 0.9379, best train loss: 0.2355  @ epoch 4675 )\n",
      "[Epoch: 4780] train loss: 0.2519, train acc: 0.9088, val loss: 0.2073, val acc: 0.9319  (best train acc: 0.9182, best val acc: 0.9379, best train loss: 0.2354  @ epoch 4764 )\n",
      "[Epoch: 4800] train loss: 0.2471, train acc: 0.9135, val loss: 0.2038, val acc: 0.9363  (best train acc: 0.9211, best val acc: 0.9379, best train loss: 0.2352  @ epoch 4798 )\n",
      "[Epoch: 4820] train loss: 0.2407, train acc: 0.9125, val loss: 0.2101, val acc: 0.9349  (best train acc: 0.9217, best val acc: 0.9379, best train loss: 0.2335  @ epoch 4819 )\n",
      "[Epoch: 4840] train loss: 0.2543, train acc: 0.9087, val loss: 0.2065, val acc: 0.9339  (best train acc: 0.9217, best val acc: 0.9379, best train loss: 0.2334  @ epoch 4822 )\n",
      "[Epoch: 4860] train loss: 0.2560, train acc: 0.9083, val loss: 0.2080, val acc: 0.9322  (best train acc: 0.9217, best val acc: 0.9379, best train loss: 0.2334  @ epoch 4822 )\n",
      "[Epoch: 4880] train loss: 0.2459, train acc: 0.9151, val loss: 0.2112, val acc: 0.9309  (best train acc: 0.9217, best val acc: 0.9379, best train loss: 0.2334  @ epoch 4822 )\n",
      "[Epoch: 4900] train loss: 0.2413, train acc: 0.9171, val loss: 0.2035, val acc: 0.9349  (best train acc: 0.9217, best val acc: 0.9379, best train loss: 0.2334  @ epoch 4822 )\n",
      "[Epoch: 4920] train loss: 0.2487, train acc: 0.9098, val loss: 0.2121, val acc: 0.9312  (best train acc: 0.9217, best val acc: 0.9379, best train loss: 0.2334  @ epoch 4822 )\n",
      "[Epoch: 4940] train loss: 0.2437, train acc: 0.9142, val loss: 0.2108, val acc: 0.9292  (best train acc: 0.9217, best val acc: 0.9379, best train loss: 0.2334  @ epoch 4822 )\n",
      "[Epoch: 4960] train loss: 0.2398, train acc: 0.9109, val loss: 0.2719, val acc: 0.9120  (best train acc: 0.9217, best val acc: 0.9379, best train loss: 0.2334  @ epoch 4822 )\n",
      "[Epoch: 4980] train loss: 0.2442, train acc: 0.9091, val loss: 0.2136, val acc: 0.9339  (best train acc: 0.9217, best val acc: 0.9379, best train loss: 0.2334  @ epoch 4822 )\n",
      "[Epoch: 5000] train loss: 0.3015, train acc: 0.8966, val loss: 0.2538, val acc: 0.9110  (best train acc: 0.9217, best val acc: 0.9379, best train loss: 0.2334  @ epoch 4822 )\n",
      "[Epoch: 5020] train loss: 0.2421, train acc: 0.9130, val loss: 0.2082, val acc: 0.9336  (best train acc: 0.9217, best val acc: 0.9379, best train loss: 0.2334  @ epoch 4822 )\n",
      "[Epoch: 5040] train loss: 0.2408, train acc: 0.9145, val loss: 0.2062, val acc: 0.9356  (best train acc: 0.9217, best val acc: 0.9386, best train loss: 0.2315  @ epoch 5033 )\n",
      "[Epoch: 5060] train loss: 0.2367, train acc: 0.9166, val loss: 0.2059, val acc: 0.9369  (best train acc: 0.9217, best val acc: 0.9386, best train loss: 0.2291  @ epoch 5059 )\n",
      "[Epoch: 5080] train loss: 0.2461, train acc: 0.9105, val loss: 0.2092, val acc: 0.9339  (best train acc: 0.9217, best val acc: 0.9386, best train loss: 0.2291  @ epoch 5059 )\n",
      "[Epoch: 5100] train loss: 0.2445, train acc: 0.9074, val loss: 0.2322, val acc: 0.9184  (best train acc: 0.9217, best val acc: 0.9386, best train loss: 0.2291  @ epoch 5059 )\n",
      "[Epoch: 5120] train loss: 0.2801, train acc: 0.9020, val loss: 0.2215, val acc: 0.9298  (best train acc: 0.9217, best val acc: 0.9386, best train loss: 0.2291  @ epoch 5059 )\n",
      "[Epoch: 5140] train loss: 0.2554, train acc: 0.9077, val loss: 0.2040, val acc: 0.9383  (best train acc: 0.9217, best val acc: 0.9386, best train loss: 0.2291  @ epoch 5059 )\n",
      "[Epoch: 5160] train loss: 0.2575, train acc: 0.9075, val loss: 0.2228, val acc: 0.9288  (best train acc: 0.9217, best val acc: 0.9386, best train loss: 0.2291  @ epoch 5059 )\n",
      "[Epoch: 5180] train loss: 0.2346, train acc: 0.9182, val loss: 0.2122, val acc: 0.9315  (best train acc: 0.9217, best val acc: 0.9386, best train loss: 0.2291  @ epoch 5059 )\n",
      "[Epoch: 5200] train loss: 0.2401, train acc: 0.9143, val loss: 0.2102, val acc: 0.9349  (best train acc: 0.9217, best val acc: 0.9386, best train loss: 0.2285  @ epoch 5197 )\n",
      "[Epoch: 5220] train loss: 0.2628, train acc: 0.9098, val loss: 0.2161, val acc: 0.9305  (best train acc: 0.9217, best val acc: 0.9386, best train loss: 0.2285  @ epoch 5197 )\n",
      "[Epoch: 5240] train loss: 0.2332, train acc: 0.9169, val loss: 0.2198, val acc: 0.9309  (best train acc: 0.9217, best val acc: 0.9390, best train loss: 0.2285  @ epoch 5197 )\n",
      "[Epoch: 5260] train loss: 0.2419, train acc: 0.9118, val loss: 0.2173, val acc: 0.9356  (best train acc: 0.9217, best val acc: 0.9390, best train loss: 0.2285  @ epoch 5197 )\n",
      "[Epoch: 5280] train loss: 0.2435, train acc: 0.9148, val loss: 0.2093, val acc: 0.9349  (best train acc: 0.9217, best val acc: 0.9393, best train loss: 0.2284  @ epoch 5277 )\n",
      "[Epoch: 5300] train loss: 0.2370, train acc: 0.9168, val loss: 0.2114, val acc: 0.9315  (best train acc: 0.9218, best val acc: 0.9393, best train loss: 0.2266  @ epoch 5288 )\n",
      "[Epoch: 5320] train loss: 0.2353, train acc: 0.9166, val loss: 0.2129, val acc: 0.9356  (best train acc: 0.9218, best val acc: 0.9393, best train loss: 0.2266  @ epoch 5288 )\n",
      "[Epoch: 5340] train loss: 0.2407, train acc: 0.9136, val loss: 0.2148, val acc: 0.9349  (best train acc: 0.9218, best val acc: 0.9393, best train loss: 0.2259  @ epoch 5337 )\n",
      "[Epoch: 5360] train loss: 0.2562, train acc: 0.9091, val loss: 0.2175, val acc: 0.9288  (best train acc: 0.9218, best val acc: 0.9393, best train loss: 0.2259  @ epoch 5337 )\n",
      "[Epoch: 5380] train loss: 0.2622, train acc: 0.9023, val loss: 0.2102, val acc: 0.9329  (best train acc: 0.9218, best val acc: 0.9393, best train loss: 0.2259  @ epoch 5337 )\n",
      "[Epoch: 5400] train loss: 0.2400, train acc: 0.9160, val loss: 0.2123, val acc: 0.9352  (best train acc: 0.9218, best val acc: 0.9393, best train loss: 0.2259  @ epoch 5337 )\n",
      "[Epoch: 5420] train loss: 0.2275, train acc: 0.9189, val loss: 0.2084, val acc: 0.9379  (best train acc: 0.9218, best val acc: 0.9400, best train loss: 0.2259  @ epoch 5337 )\n",
      "[Epoch: 5440] train loss: 0.2332, train acc: 0.9211, val loss: 0.2071, val acc: 0.9376  (best train acc: 0.9218, best val acc: 0.9413, best train loss: 0.2234  @ epoch 5439 )\n",
      "[Epoch: 5460] train loss: 0.2582, train acc: 0.9039, val loss: 0.2070, val acc: 0.9376  (best train acc: 0.9245, best val acc: 0.9413, best train loss: 0.2215  @ epoch 5451 )\n",
      "[Epoch: 5480] train loss: 0.2296, train acc: 0.9206, val loss: 0.2084, val acc: 0.9349  (best train acc: 0.9245, best val acc: 0.9413, best train loss: 0.2215  @ epoch 5451 )\n",
      "[Epoch: 5500] train loss: 0.2279, train acc: 0.9181, val loss: 0.2053, val acc: 0.9379  (best train acc: 0.9245, best val acc: 0.9413, best train loss: 0.2215  @ epoch 5451 )\n",
      "[Epoch: 5520] train loss: 0.2417, train acc: 0.9157, val loss: 0.2148, val acc: 0.9322  (best train acc: 0.9245, best val acc: 0.9417, best train loss: 0.2187  @ epoch 5508 )\n",
      "[Epoch: 5540] train loss: 0.2353, train acc: 0.9155, val loss: 0.2128, val acc: 0.9315  (best train acc: 0.9245, best val acc: 0.9417, best train loss: 0.2187  @ epoch 5508 )\n",
      "[Epoch: 5560] train loss: 0.2290, train acc: 0.9161, val loss: 0.2006, val acc: 0.9369  (best train acc: 0.9245, best val acc: 0.9417, best train loss: 0.2187  @ epoch 5508 )\n",
      "[Epoch: 5580] train loss: 0.2432, train acc: 0.9177, val loss: 0.2058, val acc: 0.9359  (best train acc: 0.9245, best val acc: 0.9417, best train loss: 0.2187  @ epoch 5508 )\n",
      "[Epoch: 5600] train loss: 0.2322, train acc: 0.9162, val loss: 0.2009, val acc: 0.9390  (best train acc: 0.9245, best val acc: 0.9417, best train loss: 0.2187  @ epoch 5508 )\n",
      "[Epoch: 5620] train loss: 0.2413, train acc: 0.9143, val loss: 0.2072, val acc: 0.9352  (best train acc: 0.9245, best val acc: 0.9417, best train loss: 0.2187  @ epoch 5508 )\n",
      "[Epoch: 5640] train loss: 0.2254, train acc: 0.9240, val loss: 0.2069, val acc: 0.9393  (best train acc: 0.9245, best val acc: 0.9417, best train loss: 0.2187  @ epoch 5508 )\n",
      "[Epoch: 5660] train loss: 0.2395, train acc: 0.9147, val loss: 0.2047, val acc: 0.9379  (best train acc: 0.9245, best val acc: 0.9417, best train loss: 0.2187  @ epoch 5508 )\n",
      "[Epoch: 5680] train loss: 0.2251, train acc: 0.9221, val loss: 0.1996, val acc: 0.9413  (best train acc: 0.9248, best val acc: 0.9417, best train loss: 0.2187  @ epoch 5668 )\n",
      "[Epoch: 5700] train loss: 0.2336, train acc: 0.9178, val loss: 0.2073, val acc: 0.9396  (best train acc: 0.9248, best val acc: 0.9420, best train loss: 0.2187  @ epoch 5668 )\n",
      "[Epoch: 5720] train loss: 0.2222, train acc: 0.9207, val loss: 0.2115, val acc: 0.9349  (best train acc: 0.9260, best val acc: 0.9420, best train loss: 0.2144  @ epoch 5702 )\n",
      "[Epoch: 5740] train loss: 0.2400, train acc: 0.9152, val loss: 0.2072, val acc: 0.9356  (best train acc: 0.9260, best val acc: 0.9420, best train loss: 0.2144  @ epoch 5702 )\n",
      "[Epoch: 5760] train loss: 0.2445, train acc: 0.9116, val loss: 0.2052, val acc: 0.9423  (best train acc: 0.9260, best val acc: 0.9423, best train loss: 0.2144  @ epoch 5702 )\n",
      "[Epoch: 5780] train loss: 0.2367, train acc: 0.9132, val loss: 0.2067, val acc: 0.9400  (best train acc: 0.9260, best val acc: 0.9423, best train loss: 0.2144  @ epoch 5702 )\n",
      "[Epoch: 5800] train loss: 0.2233, train acc: 0.9229, val loss: 0.2162, val acc: 0.9339  (best train acc: 0.9260, best val acc: 0.9423, best train loss: 0.2144  @ epoch 5702 )\n",
      "[Epoch: 5820] train loss: 0.2447, train acc: 0.9066, val loss: 0.2093, val acc: 0.9359  (best train acc: 0.9260, best val acc: 0.9423, best train loss: 0.2144  @ epoch 5702 )\n",
      "[Epoch: 5840] train loss: 0.2321, train acc: 0.9212, val loss: 0.2156, val acc: 0.9373  (best train acc: 0.9260, best val acc: 0.9423, best train loss: 0.2144  @ epoch 5702 )\n",
      "[Epoch: 5860] train loss: 0.2329, train acc: 0.9156, val loss: 0.2303, val acc: 0.9292  (best train acc: 0.9260, best val acc: 0.9423, best train loss: 0.2144  @ epoch 5702 )\n",
      "[Epoch: 5880] train loss: 0.2405, train acc: 0.9155, val loss: 0.2097, val acc: 0.9400  (best train acc: 0.9260, best val acc: 0.9423, best train loss: 0.2144  @ epoch 5702 )\n",
      "[Epoch: 5900] train loss: 0.2327, train acc: 0.9160, val loss: 0.2071, val acc: 0.9393  (best train acc: 0.9270, best val acc: 0.9423, best train loss: 0.2142  @ epoch 5889 )\n",
      "[Epoch: 5920] train loss: 0.2162, train acc: 0.9218, val loss: 0.2083, val acc: 0.9390  (best train acc: 0.9270, best val acc: 0.9433, best train loss: 0.2142  @ epoch 5889 )\n",
      "[Epoch: 5940] train loss: 0.2184, train acc: 0.9217, val loss: 0.2050, val acc: 0.9433  (best train acc: 0.9270, best val acc: 0.9433, best train loss: 0.2142  @ epoch 5889 )\n",
      "[Epoch: 5960] train loss: 0.2355, train acc: 0.9169, val loss: 0.2228, val acc: 0.9305  (best train acc: 0.9270, best val acc: 0.9433, best train loss: 0.2142  @ epoch 5889 )\n",
      "[Epoch: 5980] train loss: 0.2257, train acc: 0.9186, val loss: 0.2223, val acc: 0.9309  (best train acc: 0.9270, best val acc: 0.9433, best train loss: 0.2142  @ epoch 5889 )\n",
      "[Epoch: 6000] train loss: 0.2300, train acc: 0.9214, val loss: 0.2075, val acc: 0.9413  (best train acc: 0.9270, best val acc: 0.9433, best train loss: 0.2142  @ epoch 5889 )\n",
      "[Epoch: 6020] train loss: 0.2158, train acc: 0.9263, val loss: 0.2154, val acc: 0.9329  (best train acc: 0.9270, best val acc: 0.9433, best train loss: 0.2142  @ epoch 5889 )\n",
      "[Epoch: 6040] train loss: 0.2286, train acc: 0.9174, val loss: 0.2124, val acc: 0.9379  (best train acc: 0.9270, best val acc: 0.9433, best train loss: 0.2142  @ epoch 5889 )\n",
      "[Epoch: 6060] train loss: 0.2138, train acc: 0.9239, val loss: 0.2066, val acc: 0.9393  (best train acc: 0.9277, best val acc: 0.9433, best train loss: 0.2079  @ epoch 6059 )\n",
      "[Epoch: 6080] train loss: 0.2284, train acc: 0.9184, val loss: 0.2090, val acc: 0.9386  (best train acc: 0.9293, best val acc: 0.9440, best train loss: 0.2052  @ epoch 6062 )\n",
      "[Epoch: 6100] train loss: 0.2347, train acc: 0.9161, val loss: 0.2074, val acc: 0.9403  (best train acc: 0.9293, best val acc: 0.9440, best train loss: 0.2052  @ epoch 6062 )\n",
      "[Epoch: 6120] train loss: 0.2313, train acc: 0.9171, val loss: 0.2335, val acc: 0.9309  (best train acc: 0.9293, best val acc: 0.9440, best train loss: 0.2052  @ epoch 6062 )\n",
      "[Epoch: 6140] train loss: 0.2204, train acc: 0.9211, val loss: 0.2135, val acc: 0.9413  (best train acc: 0.9293, best val acc: 0.9440, best train loss: 0.2052  @ epoch 6062 )\n",
      "[Epoch: 6160] train loss: 0.2194, train acc: 0.9223, val loss: 0.2522, val acc: 0.9248  (best train acc: 0.9293, best val acc: 0.9440, best train loss: 0.2052  @ epoch 6062 )\n",
      "[Epoch: 6180] train loss: 0.2182, train acc: 0.9242, val loss: 0.2130, val acc: 0.9396  (best train acc: 0.9293, best val acc: 0.9440, best train loss: 0.2052  @ epoch 6062 )\n",
      "[Epoch: 6200] train loss: 0.2217, train acc: 0.9202, val loss: 0.2092, val acc: 0.9437  (best train acc: 0.9293, best val acc: 0.9440, best train loss: 0.2052  @ epoch 6062 )\n",
      "[Epoch: 6220] train loss: 0.2170, train acc: 0.9239, val loss: 0.2061, val acc: 0.9433  (best train acc: 0.9293, best val acc: 0.9440, best train loss: 0.2052  @ epoch 6062 )\n",
      "[Epoch: 6240] train loss: 0.2215, train acc: 0.9239, val loss: 0.2092, val acc: 0.9427  (best train acc: 0.9294, best val acc: 0.9440, best train loss: 0.2052  @ epoch 6062 )\n",
      "[Epoch: 6260] train loss: 0.2193, train acc: 0.9236, val loss: 0.2155, val acc: 0.9373  (best train acc: 0.9300, best val acc: 0.9440, best train loss: 0.2025  @ epoch 6247 )\n",
      "[Epoch: 6280] train loss: 0.2078, train acc: 0.9260, val loss: 0.2116, val acc: 0.9386  (best train acc: 0.9300, best val acc: 0.9440, best train loss: 0.2025  @ epoch 6247 )\n",
      "[Epoch: 6300] train loss: 0.2358, train acc: 0.9114, val loss: 0.2077, val acc: 0.9363  (best train acc: 0.9300, best val acc: 0.9440, best train loss: 0.2025  @ epoch 6247 )\n",
      "[Epoch: 6320] train loss: 0.2479, train acc: 0.9091, val loss: 0.2124, val acc: 0.9413  (best train acc: 0.9300, best val acc: 0.9440, best train loss: 0.2025  @ epoch 6247 )\n",
      "[Epoch: 6340] train loss: 0.2193, train acc: 0.9242, val loss: 0.2061, val acc: 0.9423  (best train acc: 0.9300, best val acc: 0.9440, best train loss: 0.2025  @ epoch 6247 )\n",
      "[Epoch: 6360] train loss: 0.2125, train acc: 0.9249, val loss: 0.2095, val acc: 0.9413  (best train acc: 0.9305, best val acc: 0.9454, best train loss: 0.2009  @ epoch 6353 )\n",
      "[Epoch: 6380] train loss: 0.2188, train acc: 0.9223, val loss: 0.2041, val acc: 0.9437  (best train acc: 0.9305, best val acc: 0.9454, best train loss: 0.2009  @ epoch 6353 )\n",
      "[Epoch: 6400] train loss: 0.2261, train acc: 0.9174, val loss: 0.2052, val acc: 0.9410  (best train acc: 0.9305, best val acc: 0.9454, best train loss: 0.2009  @ epoch 6353 )\n",
      "[Epoch: 6420] train loss: 0.2379, train acc: 0.9187, val loss: 0.2408, val acc: 0.9258  (best train acc: 0.9305, best val acc: 0.9454, best train loss: 0.2009  @ epoch 6353 )\n",
      "[Epoch: 6440] train loss: 0.2252, train acc: 0.9213, val loss: 0.2130, val acc: 0.9332  (best train acc: 0.9305, best val acc: 0.9454, best train loss: 0.2009  @ epoch 6353 )\n",
      "[Epoch: 6460] train loss: 0.2097, train acc: 0.9268, val loss: 0.2028, val acc: 0.9400  (best train acc: 0.9305, best val acc: 0.9454, best train loss: 0.2009  @ epoch 6353 )\n",
      "[Epoch: 6480] train loss: 0.2381, train acc: 0.9102, val loss: 0.2200, val acc: 0.9339  (best train acc: 0.9305, best val acc: 0.9454, best train loss: 0.2009  @ epoch 6353 )\n",
      "[Epoch: 6500] train loss: 0.2162, train acc: 0.9228, val loss: 0.2081, val acc: 0.9427  (best train acc: 0.9305, best val acc: 0.9454, best train loss: 0.2009  @ epoch 6353 )\n",
      "[Epoch: 6520] train loss: 0.2606, train acc: 0.9064, val loss: 0.2286, val acc: 0.9268  (best train acc: 0.9318, best val acc: 0.9454, best train loss: 0.2009  @ epoch 6353 )\n",
      "[Epoch: 6540] train loss: 0.2137, train acc: 0.9268, val loss: 0.2148, val acc: 0.9376  (best train acc: 0.9318, best val acc: 0.9454, best train loss: 0.2009  @ epoch 6353 )\n",
      "[Epoch: 6560] train loss: 0.2241, train acc: 0.9195, val loss: 0.2290, val acc: 0.9302  (best train acc: 0.9318, best val acc: 0.9454, best train loss: 0.1997  @ epoch 6546 )\n",
      "[Epoch: 6580] train loss: 0.2264, train acc: 0.9216, val loss: 0.2049, val acc: 0.9447  (best train acc: 0.9318, best val acc: 0.9454, best train loss: 0.1997  @ epoch 6546 )\n",
      "[Epoch: 6600] train loss: 0.2105, train acc: 0.9258, val loss: 0.2208, val acc: 0.9373  (best train acc: 0.9318, best val acc: 0.9454, best train loss: 0.1997  @ epoch 6546 )\n",
      "[Epoch: 6620] train loss: 0.2070, train acc: 0.9286, val loss: 0.2092, val acc: 0.9413  (best train acc: 0.9318, best val acc: 0.9454, best train loss: 0.1997  @ epoch 6546 )\n",
      "[Epoch: 6640] train loss: 0.2017, train acc: 0.9285, val loss: 0.2065, val acc: 0.9437  (best train acc: 0.9336, best val acc: 0.9454, best train loss: 0.1974  @ epoch 6638 )\n",
      "[Epoch: 6660] train loss: 0.2117, train acc: 0.9256, val loss: 0.2071, val acc: 0.9423  (best train acc: 0.9336, best val acc: 0.9454, best train loss: 0.1961  @ epoch 6655 )\n",
      "[Epoch: 6680] train loss: 0.2064, train acc: 0.9281, val loss: 0.2066, val acc: 0.9410  (best train acc: 0.9336, best val acc: 0.9454, best train loss: 0.1961  @ epoch 6655 )\n",
      "[Epoch: 6700] train loss: 0.2021, train acc: 0.9291, val loss: 0.2130, val acc: 0.9410  (best train acc: 0.9336, best val acc: 0.9454, best train loss: 0.1961  @ epoch 6655 )\n",
      "[Epoch: 6720] train loss: 0.1962, train acc: 0.9346, val loss: 0.2117, val acc: 0.9420  (best train acc: 0.9346, best val acc: 0.9454, best train loss: 0.1961  @ epoch 6655 )\n",
      "[Epoch: 6740] train loss: 0.2101, train acc: 0.9284, val loss: 0.2054, val acc: 0.9427  (best train acc: 0.9346, best val acc: 0.9454, best train loss: 0.1961  @ epoch 6655 )\n",
      "[Epoch: 6760] train loss: 0.1978, train acc: 0.9320, val loss: 0.2110, val acc: 0.9413  (best train acc: 0.9346, best val acc: 0.9454, best train loss: 0.1945  @ epoch 6747 )\n",
      "[Epoch: 6780] train loss: 0.2147, train acc: 0.9233, val loss: 0.2063, val acc: 0.9413  (best train acc: 0.9346, best val acc: 0.9454, best train loss: 0.1945  @ epoch 6747 )\n",
      "[Epoch: 6800] train loss: 0.2503, train acc: 0.9162, val loss: 0.2225, val acc: 0.9352  (best train acc: 0.9346, best val acc: 0.9454, best train loss: 0.1945  @ epoch 6747 )\n",
      "[Epoch: 6820] train loss: 0.2873, train acc: 0.9041, val loss: 0.2401, val acc: 0.9201  (best train acc: 0.9346, best val acc: 0.9454, best train loss: 0.1945  @ epoch 6747 )\n",
      "[Epoch: 6840] train loss: 0.2270, train acc: 0.9189, val loss: 0.2126, val acc: 0.9400  (best train acc: 0.9346, best val acc: 0.9454, best train loss: 0.1945  @ epoch 6747 )\n",
      "[Epoch: 6860] train loss: 0.2258, train acc: 0.9171, val loss: 0.2056, val acc: 0.9427  (best train acc: 0.9346, best val acc: 0.9454, best train loss: 0.1945  @ epoch 6747 )\n",
      "[Epoch: 6880] train loss: 0.2197, train acc: 0.9217, val loss: 0.2233, val acc: 0.9400  (best train acc: 0.9346, best val acc: 0.9454, best train loss: 0.1945  @ epoch 6747 )\n",
      "[Epoch: 6900] train loss: 0.2081, train acc: 0.9250, val loss: 0.2116, val acc: 0.9420  (best train acc: 0.9346, best val acc: 0.9457, best train loss: 0.1945  @ epoch 6747 )\n",
      "[Epoch: 6920] train loss: 0.2159, train acc: 0.9256, val loss: 0.2070, val acc: 0.9396  (best train acc: 0.9346, best val acc: 0.9457, best train loss: 0.1945  @ epoch 6747 )\n",
      "[Epoch: 6940] train loss: 0.2100, train acc: 0.9237, val loss: 0.2091, val acc: 0.9437  (best train acc: 0.9346, best val acc: 0.9457, best train loss: 0.1945  @ epoch 6747 )\n",
      "[Epoch: 6960] train loss: 0.2259, train acc: 0.9203, val loss: 0.2186, val acc: 0.9390  (best train acc: 0.9346, best val acc: 0.9457, best train loss: 0.1945  @ epoch 6747 )\n",
      "[Epoch: 6980] train loss: 0.2167, train acc: 0.9206, val loss: 0.2079, val acc: 0.9433  (best train acc: 0.9346, best val acc: 0.9457, best train loss: 0.1945  @ epoch 6747 )\n",
      "[Epoch: 7000] train loss: 0.1972, train acc: 0.9313, val loss: 0.2038, val acc: 0.9413  (best train acc: 0.9352, best val acc: 0.9457, best train loss: 0.1916  @ epoch 6996 )\n",
      "[Epoch: 7020] train loss: 0.1999, train acc: 0.9315, val loss: 0.2068, val acc: 0.9410  (best train acc: 0.9352, best val acc: 0.9457, best train loss: 0.1886  @ epoch 7015 )\n",
      "[Epoch: 7040] train loss: 0.2142, train acc: 0.9258, val loss: 0.2088, val acc: 0.9386  (best train acc: 0.9352, best val acc: 0.9460, best train loss: 0.1886  @ epoch 7015 )\n",
      "[Epoch: 7060] train loss: 0.2008, train acc: 0.9286, val loss: 0.2075, val acc: 0.9417  (best train acc: 0.9352, best val acc: 0.9460, best train loss: 0.1886  @ epoch 7015 )\n",
      "[Epoch: 7080] train loss: 0.2033, train acc: 0.9292, val loss: 0.2055, val acc: 0.9450  (best train acc: 0.9355, best val acc: 0.9460, best train loss: 0.1886  @ epoch 7015 )\n",
      "[Epoch: 7100] train loss: 0.2036, train acc: 0.9290, val loss: 0.2071, val acc: 0.9410  (best train acc: 0.9355, best val acc: 0.9460, best train loss: 0.1886  @ epoch 7015 )\n",
      "[Epoch: 7120] train loss: 0.2059, train acc: 0.9252, val loss: 0.2064, val acc: 0.9444  (best train acc: 0.9355, best val acc: 0.9460, best train loss: 0.1886  @ epoch 7015 )\n",
      "[Epoch: 7140] train loss: 0.1931, train acc: 0.9341, val loss: 0.2077, val acc: 0.9433  (best train acc: 0.9365, best val acc: 0.9460, best train loss: 0.1847  @ epoch 7132 )\n",
      "[Epoch: 7160] train loss: 0.2269, train acc: 0.9142, val loss: 0.3130, val acc: 0.9073  (best train acc: 0.9365, best val acc: 0.9460, best train loss: 0.1847  @ epoch 7132 )\n",
      "[Epoch: 7180] train loss: 0.2342, train acc: 0.9182, val loss: 0.2166, val acc: 0.9383  (best train acc: 0.9365, best val acc: 0.9460, best train loss: 0.1847  @ epoch 7132 )\n",
      "[Epoch: 7200] train loss: 0.2174, train acc: 0.9268, val loss: 0.2069, val acc: 0.9440  (best train acc: 0.9365, best val acc: 0.9460, best train loss: 0.1847  @ epoch 7132 )\n",
      "[Epoch: 7220] train loss: 0.1947, train acc: 0.9310, val loss: 0.2228, val acc: 0.9403  (best train acc: 0.9365, best val acc: 0.9460, best train loss: 0.1847  @ epoch 7132 )\n",
      "[Epoch: 7240] train loss: 0.2195, train acc: 0.9255, val loss: 0.2063, val acc: 0.9386  (best train acc: 0.9372, best val acc: 0.9460, best train loss: 0.1847  @ epoch 7132 )\n",
      "[Epoch: 7260] train loss: 0.2057, train acc: 0.9297, val loss: 0.2263, val acc: 0.9352  (best train acc: 0.9372, best val acc: 0.9460, best train loss: 0.1847  @ epoch 7132 )\n",
      "[Epoch: 7280] train loss: 0.2048, train acc: 0.9290, val loss: 0.2135, val acc: 0.9403  (best train acc: 0.9372, best val acc: 0.9467, best train loss: 0.1847  @ epoch 7132 )\n",
      "[Epoch: 7300] train loss: 0.1969, train acc: 0.9315, val loss: 0.2305, val acc: 0.9275  (best train acc: 0.9372, best val acc: 0.9467, best train loss: 0.1847  @ epoch 7132 )\n",
      "[Epoch: 7320] train loss: 0.2011, train acc: 0.9307, val loss: 0.2050, val acc: 0.9427  (best train acc: 0.9372, best val acc: 0.9467, best train loss: 0.1847  @ epoch 7132 )\n",
      "[Epoch: 7340] train loss: 0.1978, train acc: 0.9310, val loss: 0.2275, val acc: 0.9342  (best train acc: 0.9372, best val acc: 0.9467, best train loss: 0.1847  @ epoch 7132 )\n",
      "[Epoch: 7360] train loss: 0.2083, train acc: 0.9282, val loss: 0.2232, val acc: 0.9379  (best train acc: 0.9372, best val acc: 0.9467, best train loss: 0.1847  @ epoch 7132 )\n",
      "[Epoch: 7380] train loss: 0.1969, train acc: 0.9313, val loss: 0.2125, val acc: 0.9437  (best train acc: 0.9372, best val acc: 0.9467, best train loss: 0.1847  @ epoch 7132 )\n",
      "[Epoch: 7400] train loss: 0.1963, train acc: 0.9315, val loss: 0.2071, val acc: 0.9440  (best train acc: 0.9372, best val acc: 0.9467, best train loss: 0.1847  @ epoch 7132 )\n",
      "[Epoch: 7420] train loss: 0.1922, train acc: 0.9353, val loss: 0.2362, val acc: 0.9332  (best train acc: 0.9372, best val acc: 0.9467, best train loss: 0.1847  @ epoch 7132 )\n",
      "[Epoch: 7440] train loss: 0.2523, train acc: 0.9017, val loss: 0.2218, val acc: 0.9332  (best train acc: 0.9372, best val acc: 0.9467, best train loss: 0.1847  @ epoch 7132 )\n",
      "[Epoch: 7460] train loss: 0.2088, train acc: 0.9273, val loss: 0.2066, val acc: 0.9433  (best train acc: 0.9372, best val acc: 0.9467, best train loss: 0.1847  @ epoch 7132 )\n",
      "[Epoch: 7480] train loss: 0.1902, train acc: 0.9368, val loss: 0.2126, val acc: 0.9420  (best train acc: 0.9372, best val acc: 0.9467, best train loss: 0.1845  @ epoch 7469 )\n",
      "[Epoch: 7500] train loss: 0.1962, train acc: 0.9298, val loss: 0.2104, val acc: 0.9376  (best train acc: 0.9372, best val acc: 0.9467, best train loss: 0.1845  @ epoch 7469 )\n",
      "[Epoch: 7520] train loss: 0.2120, train acc: 0.9217, val loss: 0.2031, val acc: 0.9450  (best train acc: 0.9375, best val acc: 0.9467, best train loss: 0.1845  @ epoch 7469 )\n",
      "[Epoch: 7540] train loss: 0.1852, train acc: 0.9359, val loss: 0.2087, val acc: 0.9427  (best train acc: 0.9381, best val acc: 0.9467, best train loss: 0.1845  @ epoch 7469 )\n",
      "[Epoch: 7560] train loss: 0.1884, train acc: 0.9362, val loss: 0.2085, val acc: 0.9444  (best train acc: 0.9382, best val acc: 0.9467, best train loss: 0.1845  @ epoch 7555 )\n",
      "[Epoch: 7580] train loss: 0.1947, train acc: 0.9320, val loss: 0.2055, val acc: 0.9460  (best train acc: 0.9382, best val acc: 0.9467, best train loss: 0.1843  @ epoch 7563 )\n",
      "[Epoch: 7600] train loss: 0.2108, train acc: 0.9217, val loss: 0.2154, val acc: 0.9386  (best train acc: 0.9382, best val acc: 0.9467, best train loss: 0.1842  @ epoch 7581 )\n",
      "[Epoch: 7620] train loss: 0.2059, train acc: 0.9231, val loss: 0.2072, val acc: 0.9444  (best train acc: 0.9382, best val acc: 0.9467, best train loss: 0.1842  @ epoch 7581 )\n",
      "[Epoch: 7640] train loss: 0.1956, train acc: 0.9325, val loss: 0.2039, val acc: 0.9454  (best train acc: 0.9382, best val acc: 0.9474, best train loss: 0.1830  @ epoch 7626 )\n",
      "[Epoch: 7660] train loss: 0.2052, train acc: 0.9305, val loss: 0.2020, val acc: 0.9447  (best train acc: 0.9387, best val acc: 0.9474, best train loss: 0.1824  @ epoch 7659 )\n",
      "[Epoch: 7680] train loss: 0.1946, train acc: 0.9313, val loss: 0.2135, val acc: 0.9406  (best train acc: 0.9392, best val acc: 0.9477, best train loss: 0.1824  @ epoch 7659 )\n",
      "[Epoch: 7700] train loss: 0.2087, train acc: 0.9271, val loss: 0.2188, val acc: 0.9373  (best train acc: 0.9392, best val acc: 0.9477, best train loss: 0.1821  @ epoch 7682 )\n",
      "[Epoch: 7720] train loss: 0.2166, train acc: 0.9270, val loss: 0.2142, val acc: 0.9406  (best train acc: 0.9392, best val acc: 0.9477, best train loss: 0.1821  @ epoch 7682 )\n",
      "[Epoch: 7740] train loss: 0.2601, train acc: 0.8968, val loss: 0.2783, val acc: 0.9089  (best train acc: 0.9392, best val acc: 0.9477, best train loss: 0.1821  @ epoch 7682 )\n",
      "[Epoch: 7760] train loss: 0.2066, train acc: 0.9307, val loss: 0.2040, val acc: 0.9437  (best train acc: 0.9392, best val acc: 0.9481, best train loss: 0.1821  @ epoch 7682 )\n",
      "[Epoch: 7780] train loss: 0.1980, train acc: 0.9325, val loss: 0.2026, val acc: 0.9444  (best train acc: 0.9392, best val acc: 0.9481, best train loss: 0.1821  @ epoch 7682 )\n",
      "[Epoch: 7800] train loss: 0.1963, train acc: 0.9286, val loss: 0.2463, val acc: 0.9278  (best train acc: 0.9392, best val acc: 0.9494, best train loss: 0.1804  @ epoch 7782 )\n",
      "[Epoch: 7820] train loss: 0.2143, train acc: 0.9198, val loss: 0.2013, val acc: 0.9491  (best train acc: 0.9392, best val acc: 0.9494, best train loss: 0.1804  @ epoch 7782 )\n",
      "[Epoch: 7840] train loss: 0.2218, train acc: 0.9162, val loss: 0.2157, val acc: 0.9383  (best train acc: 0.9392, best val acc: 0.9494, best train loss: 0.1804  @ epoch 7782 )\n",
      "[Epoch: 7860] train loss: 0.1988, train acc: 0.9268, val loss: 0.2231, val acc: 0.9373  (best train acc: 0.9392, best val acc: 0.9494, best train loss: 0.1804  @ epoch 7782 )\n",
      "[Epoch: 7880] train loss: 0.2032, train acc: 0.9325, val loss: 0.2129, val acc: 0.9437  (best train acc: 0.9392, best val acc: 0.9494, best train loss: 0.1804  @ epoch 7782 )\n",
      "[Epoch: 7900] train loss: 0.1909, train acc: 0.9338, val loss: 0.2043, val acc: 0.9457  (best train acc: 0.9392, best val acc: 0.9494, best train loss: 0.1804  @ epoch 7782 )\n",
      "[Epoch: 7920] train loss: 0.1832, train acc: 0.9385, val loss: 0.2008, val acc: 0.9457  (best train acc: 0.9404, best val acc: 0.9494, best train loss: 0.1799  @ epoch 7918 )\n",
      "[Epoch: 7940] train loss: 0.1780, train acc: 0.9413, val loss: 0.2046, val acc: 0.9460  (best train acc: 0.9413, best val acc: 0.9504, best train loss: 0.1780  @ epoch 7940 )\n",
      "[Epoch: 7960] train loss: 0.1855, train acc: 0.9396, val loss: 0.2016, val acc: 0.9470  (best train acc: 0.9413, best val acc: 0.9504, best train loss: 0.1780  @ epoch 7940 )\n",
      "[Epoch: 7980] train loss: 0.1818, train acc: 0.9376, val loss: 0.2048, val acc: 0.9457  (best train acc: 0.9413, best val acc: 0.9504, best train loss: 0.1780  @ epoch 7940 )\n",
      "[Epoch: 8000] train loss: 0.2142, train acc: 0.9269, val loss: 0.2021, val acc: 0.9484  (best train acc: 0.9413, best val acc: 0.9504, best train loss: 0.1780  @ epoch 7940 )\n",
      "[Epoch: 8020] train loss: 0.1829, train acc: 0.9359, val loss: 0.2069, val acc: 0.9450  (best train acc: 0.9413, best val acc: 0.9508, best train loss: 0.1747  @ epoch 8007 )\n",
      "[Epoch: 8040] train loss: 0.1930, train acc: 0.9367, val loss: 0.2020, val acc: 0.9467  (best train acc: 0.9426, best val acc: 0.9508, best train loss: 0.1728  @ epoch 8039 )\n",
      "[Epoch: 8060] train loss: 0.1754, train acc: 0.9383, val loss: 0.2072, val acc: 0.9454  (best train acc: 0.9426, best val acc: 0.9508, best train loss: 0.1728  @ epoch 8039 )\n",
      "[Epoch: 8080] train loss: 0.2208, train acc: 0.9296, val loss: 0.2159, val acc: 0.9383  (best train acc: 0.9426, best val acc: 0.9508, best train loss: 0.1728  @ epoch 8039 )\n",
      "[Epoch: 8100] train loss: 0.1860, train acc: 0.9385, val loss: 0.2021, val acc: 0.9474  (best train acc: 0.9426, best val acc: 0.9508, best train loss: 0.1728  @ epoch 8039 )\n",
      "[Epoch: 8120] train loss: 0.1825, train acc: 0.9373, val loss: 0.2078, val acc: 0.9464  (best train acc: 0.9426, best val acc: 0.9508, best train loss: 0.1728  @ epoch 8039 )\n",
      "[Epoch: 8140] train loss: 0.1969, train acc: 0.9331, val loss: 0.2051, val acc: 0.9474  (best train acc: 0.9429, best val acc: 0.9508, best train loss: 0.1717  @ epoch 8134 )\n",
      "[Epoch: 8160] train loss: 0.1986, train acc: 0.9320, val loss: 0.1988, val acc: 0.9484  (best train acc: 0.9429, best val acc: 0.9508, best train loss: 0.1717  @ epoch 8134 )\n",
      "[Epoch: 8180] train loss: 0.1925, train acc: 0.9320, val loss: 0.2054, val acc: 0.9464  (best train acc: 0.9429, best val acc: 0.9508, best train loss: 0.1717  @ epoch 8134 )\n",
      "[Epoch: 8200] train loss: 0.1867, train acc: 0.9357, val loss: 0.2089, val acc: 0.9477  (best train acc: 0.9429, best val acc: 0.9518, best train loss: 0.1717  @ epoch 8134 )\n",
      "[Epoch: 8220] train loss: 0.1833, train acc: 0.9376, val loss: 0.2042, val acc: 0.9491  (best train acc: 0.9429, best val acc: 0.9518, best train loss: 0.1712  @ epoch 8205 )\n",
      "[Epoch: 8240] train loss: 0.1865, train acc: 0.9381, val loss: 0.2061, val acc: 0.9477  (best train acc: 0.9432, best val acc: 0.9518, best train loss: 0.1712  @ epoch 8205 )\n",
      "[Epoch: 8260] train loss: 0.2976, train acc: 0.8833, val loss: 0.2718, val acc: 0.9126  (best train acc: 0.9432, best val acc: 0.9518, best train loss: 0.1712  @ epoch 8205 )\n",
      "[Epoch: 8280] train loss: 0.2057, train acc: 0.9264, val loss: 0.2054, val acc: 0.9460  (best train acc: 0.9432, best val acc: 0.9518, best train loss: 0.1712  @ epoch 8205 )\n",
      "[Epoch: 8300] train loss: 0.2253, train acc: 0.9263, val loss: 0.2577, val acc: 0.9234  (best train acc: 0.9432, best val acc: 0.9518, best train loss: 0.1712  @ epoch 8205 )\n",
      "[Epoch: 8320] train loss: 0.1977, train acc: 0.9272, val loss: 0.2143, val acc: 0.9410  (best train acc: 0.9432, best val acc: 0.9518, best train loss: 0.1712  @ epoch 8205 )\n",
      "[Epoch: 8340] train loss: 0.1913, train acc: 0.9359, val loss: 0.2036, val acc: 0.9450  (best train acc: 0.9432, best val acc: 0.9518, best train loss: 0.1712  @ epoch 8205 )\n",
      "[Epoch: 8360] train loss: 0.2039, train acc: 0.9315, val loss: 0.2016, val acc: 0.9484  (best train acc: 0.9432, best val acc: 0.9518, best train loss: 0.1712  @ epoch 8205 )\n",
      "[Epoch: 8380] train loss: 0.1806, train acc: 0.9390, val loss: 0.1984, val acc: 0.9484  (best train acc: 0.9432, best val acc: 0.9518, best train loss: 0.1712  @ epoch 8205 )\n",
      "[Epoch: 8400] train loss: 0.1890, train acc: 0.9378, val loss: 0.2013, val acc: 0.9477  (best train acc: 0.9440, best val acc: 0.9518, best train loss: 0.1712  @ epoch 8205 )\n",
      "[Epoch: 8420] train loss: 0.1758, train acc: 0.9359, val loss: 0.1963, val acc: 0.9467  (best train acc: 0.9440, best val acc: 0.9518, best train loss: 0.1712  @ epoch 8205 )\n",
      "[Epoch: 8440] train loss: 0.1880, train acc: 0.9345, val loss: 0.2705, val acc: 0.9251  (best train acc: 0.9447, best val acc: 0.9518, best train loss: 0.1712  @ epoch 8205 )\n",
      "[Epoch: 8460] train loss: 0.1939, train acc: 0.9357, val loss: 0.2198, val acc: 0.9444  (best train acc: 0.9447, best val acc: 0.9518, best train loss: 0.1712  @ epoch 8205 )\n",
      "[Epoch: 8480] train loss: 0.1783, train acc: 0.9409, val loss: 0.2024, val acc: 0.9481  (best train acc: 0.9447, best val acc: 0.9518, best train loss: 0.1698  @ epoch 8479 )\n",
      "[Epoch: 8500] train loss: 0.1828, train acc: 0.9365, val loss: 0.1951, val acc: 0.9508  (best train acc: 0.9447, best val acc: 0.9518, best train loss: 0.1698  @ epoch 8479 )\n",
      "[Epoch: 8520] train loss: 0.1860, train acc: 0.9404, val loss: 0.1961, val acc: 0.9497  (best train acc: 0.9459, best val acc: 0.9518, best train loss: 0.1692  @ epoch 8505 )\n",
      "[Epoch: 8540] train loss: 0.1805, train acc: 0.9414, val loss: 0.1939, val acc: 0.9521  (best train acc: 0.9459, best val acc: 0.9521, best train loss: 0.1669  @ epoch 8530 )\n",
      "[Epoch: 8560] train loss: 0.1800, train acc: 0.9397, val loss: 0.1982, val acc: 0.9484  (best train acc: 0.9459, best val acc: 0.9521, best train loss: 0.1669  @ epoch 8530 )\n",
      "[Epoch: 8580] train loss: 0.2005, train acc: 0.9341, val loss: 0.1979, val acc: 0.9494  (best train acc: 0.9459, best val acc: 0.9521, best train loss: 0.1669  @ epoch 8530 )\n",
      "[Epoch: 8600] train loss: 0.1814, train acc: 0.9363, val loss: 0.1930, val acc: 0.9504  (best train acc: 0.9459, best val acc: 0.9521, best train loss: 0.1669  @ epoch 8530 )\n",
      "[Epoch: 8620] train loss: 0.1954, train acc: 0.9332, val loss: 0.2224, val acc: 0.9403  (best train acc: 0.9459, best val acc: 0.9521, best train loss: 0.1644  @ epoch 8607 )\n",
      "[Epoch: 8640] train loss: 0.1818, train acc: 0.9401, val loss: 0.1972, val acc: 0.9477  (best train acc: 0.9459, best val acc: 0.9521, best train loss: 0.1644  @ epoch 8607 )\n",
      "[Epoch: 8660] train loss: 0.1707, train acc: 0.9425, val loss: 0.1924, val acc: 0.9508  (best train acc: 0.9459, best val acc: 0.9524, best train loss: 0.1622  @ epoch 8649 )\n",
      "[Epoch: 8680] train loss: 0.2335, train acc: 0.9158, val loss: 0.1926, val acc: 0.9464  (best train acc: 0.9459, best val acc: 0.9524, best train loss: 0.1622  @ epoch 8649 )\n",
      "[Epoch: 8700] train loss: 0.2071, train acc: 0.9287, val loss: 0.1997, val acc: 0.9430  (best train acc: 0.9459, best val acc: 0.9524, best train loss: 0.1622  @ epoch 8649 )\n",
      "[Epoch: 8720] train loss: 0.1995, train acc: 0.9247, val loss: 0.2195, val acc: 0.9393  (best train acc: 0.9459, best val acc: 0.9524, best train loss: 0.1622  @ epoch 8649 )\n",
      "[Epoch: 8740] train loss: 0.1794, train acc: 0.9396, val loss: 0.1938, val acc: 0.9501  (best train acc: 0.9459, best val acc: 0.9524, best train loss: 0.1622  @ epoch 8649 )\n",
      "[Epoch: 8760] train loss: 0.1687, train acc: 0.9440, val loss: 0.1951, val acc: 0.9460  (best train acc: 0.9466, best val acc: 0.9524, best train loss: 0.1618  @ epoch 8747 )\n",
      "[Epoch: 8780] train loss: 0.1665, train acc: 0.9432, val loss: 0.1894, val acc: 0.9501  (best train acc: 0.9479, best val acc: 0.9531, best train loss: 0.1609  @ epoch 8773 )\n",
      "[Epoch: 8800] train loss: 0.1692, train acc: 0.9434, val loss: 0.2007, val acc: 0.9484  (best train acc: 0.9479, best val acc: 0.9531, best train loss: 0.1607  @ epoch 8786 )\n",
      "[Epoch: 8820] train loss: 0.1854, train acc: 0.9338, val loss: 0.2342, val acc: 0.9356  (best train acc: 0.9479, best val acc: 0.9531, best train loss: 0.1607  @ epoch 8786 )\n",
      "[Epoch: 8840] train loss: 0.1827, train acc: 0.9404, val loss: 0.2034, val acc: 0.9481  (best train acc: 0.9479, best val acc: 0.9531, best train loss: 0.1607  @ epoch 8786 )\n",
      "[Epoch: 8860] train loss: 0.2063, train acc: 0.9230, val loss: 0.1929, val acc: 0.9501  (best train acc: 0.9479, best val acc: 0.9531, best train loss: 0.1607  @ epoch 8786 )\n",
      "[Epoch: 8880] train loss: 0.1780, train acc: 0.9384, val loss: 0.2037, val acc: 0.9457  (best train acc: 0.9479, best val acc: 0.9531, best train loss: 0.1607  @ epoch 8786 )\n",
      "[Epoch: 8900] train loss: 0.1785, train acc: 0.9378, val loss: 0.1955, val acc: 0.9491  (best train acc: 0.9479, best val acc: 0.9531, best train loss: 0.1607  @ epoch 8786 )\n",
      "[Epoch: 8920] train loss: 0.1727, train acc: 0.9417, val loss: 0.1872, val acc: 0.9508  (best train acc: 0.9479, best val acc: 0.9538, best train loss: 0.1607  @ epoch 8786 )\n",
      "[Epoch: 8940] train loss: 0.1752, train acc: 0.9403, val loss: 0.2001, val acc: 0.9484  (best train acc: 0.9479, best val acc: 0.9538, best train loss: 0.1607  @ epoch 8786 )\n",
      "[Epoch: 8960] train loss: 0.1643, train acc: 0.9460, val loss: 0.1986, val acc: 0.9501  (best train acc: 0.9479, best val acc: 0.9538, best train loss: 0.1607  @ epoch 8786 )\n",
      "[Epoch: 8980] train loss: 0.1735, train acc: 0.9453, val loss: 0.1855, val acc: 0.9511  (best train acc: 0.9479, best val acc: 0.9538, best train loss: 0.1607  @ epoch 8786 )\n",
      "[Epoch: 9000] train loss: 0.1955, train acc: 0.9378, val loss: 0.1940, val acc: 0.9508  (best train acc: 0.9479, best val acc: 0.9538, best train loss: 0.1607  @ epoch 8786 )\n",
      "[Epoch: 9020] train loss: 0.1756, train acc: 0.9445, val loss: 0.1968, val acc: 0.9487  (best train acc: 0.9479, best val acc: 0.9538, best train loss: 0.1607  @ epoch 8786 )\n",
      "[Epoch: 9040] train loss: 0.1929, train acc: 0.9370, val loss: 0.1964, val acc: 0.9501  (best train acc: 0.9479, best val acc: 0.9538, best train loss: 0.1607  @ epoch 8786 )\n",
      "[Epoch: 9060] train loss: 0.1727, train acc: 0.9443, val loss: 0.1889, val acc: 0.9514  (best train acc: 0.9479, best val acc: 0.9538, best train loss: 0.1607  @ epoch 8786 )\n",
      "[Epoch: 9080] train loss: 0.1705, train acc: 0.9425, val loss: 0.1830, val acc: 0.9528  (best train acc: 0.9501, best val acc: 0.9538, best train loss: 0.1589  @ epoch 9073 )\n",
      "[Epoch: 9100] train loss: 0.1695, train acc: 0.9438, val loss: 0.1802, val acc: 0.9508  (best train acc: 0.9501, best val acc: 0.9538, best train loss: 0.1589  @ epoch 9073 )\n",
      "[Epoch: 9120] train loss: 0.1743, train acc: 0.9445, val loss: 0.1817, val acc: 0.9538  (best train acc: 0.9501, best val acc: 0.9538, best train loss: 0.1589  @ epoch 9073 )\n",
      "[Epoch: 9140] train loss: 0.1665, train acc: 0.9445, val loss: 0.1873, val acc: 0.9524  (best train acc: 0.9501, best val acc: 0.9541, best train loss: 0.1589  @ epoch 9073 )\n",
      "[Epoch: 9160] train loss: 0.1847, train acc: 0.9431, val loss: 0.1900, val acc: 0.9497  (best train acc: 0.9501, best val acc: 0.9541, best train loss: 0.1587  @ epoch 9143 )\n",
      "[Epoch: 9180] train loss: 0.2244, train acc: 0.9263, val loss: 0.2306, val acc: 0.9349  (best train acc: 0.9501, best val acc: 0.9541, best train loss: 0.1587  @ epoch 9143 )\n",
      "[Epoch: 9200] train loss: 0.1873, train acc: 0.9384, val loss: 0.1984, val acc: 0.9474  (best train acc: 0.9501, best val acc: 0.9541, best train loss: 0.1587  @ epoch 9143 )\n",
      "[Epoch: 9220] train loss: 0.1760, train acc: 0.9466, val loss: 0.1869, val acc: 0.9518  (best train acc: 0.9501, best val acc: 0.9541, best train loss: 0.1587  @ epoch 9143 )\n",
      "[Epoch: 9240] train loss: 0.2060, train acc: 0.9345, val loss: 0.2523, val acc: 0.9221  (best train acc: 0.9501, best val acc: 0.9545, best train loss: 0.1554  @ epoch 9233 )\n",
      "[Epoch: 9260] train loss: 0.1906, train acc: 0.9302, val loss: 0.1852, val acc: 0.9491  (best train acc: 0.9501, best val acc: 0.9545, best train loss: 0.1554  @ epoch 9233 )\n",
      "[Epoch: 9280] train loss: 0.2072, train acc: 0.9278, val loss: 0.1951, val acc: 0.9454  (best train acc: 0.9501, best val acc: 0.9545, best train loss: 0.1554  @ epoch 9233 )\n",
      "[Epoch: 9300] train loss: 0.1715, train acc: 0.9425, val loss: 0.1870, val acc: 0.9528  (best train acc: 0.9501, best val acc: 0.9545, best train loss: 0.1554  @ epoch 9233 )\n",
      "[Epoch: 9320] train loss: 0.1661, train acc: 0.9453, val loss: 0.1849, val acc: 0.9518  (best train acc: 0.9501, best val acc: 0.9558, best train loss: 0.1529  @ epoch 9310 )\n",
      "[Epoch: 9340] train loss: 0.1613, train acc: 0.9469, val loss: 0.1807, val acc: 0.9538  (best train acc: 0.9501, best val acc: 0.9558, best train loss: 0.1529  @ epoch 9310 )\n",
      "[Epoch: 9360] train loss: 0.1684, train acc: 0.9422, val loss: 0.1822, val acc: 0.9538  (best train acc: 0.9501, best val acc: 0.9558, best train loss: 0.1529  @ epoch 9310 )\n",
      "[Epoch: 9380] train loss: 0.1705, train acc: 0.9470, val loss: 0.1813, val acc: 0.9541  (best train acc: 0.9503, best val acc: 0.9558, best train loss: 0.1529  @ epoch 9310 )\n",
      "[Epoch: 9400] train loss: 0.2294, train acc: 0.9164, val loss: 0.2084, val acc: 0.9342  (best train acc: 0.9503, best val acc: 0.9558, best train loss: 0.1529  @ epoch 9310 )\n",
      "[Epoch: 9420] train loss: 0.1696, train acc: 0.9456, val loss: 0.1882, val acc: 0.9541  (best train acc: 0.9503, best val acc: 0.9558, best train loss: 0.1529  @ epoch 9310 )\n",
      "[Epoch: 9440] train loss: 0.1596, train acc: 0.9505, val loss: 0.1813, val acc: 0.9521  (best train acc: 0.9505, best val acc: 0.9558, best train loss: 0.1529  @ epoch 9310 )\n",
      "[Epoch: 9460] train loss: 0.1687, train acc: 0.9446, val loss: 0.1849, val acc: 0.9538  (best train acc: 0.9505, best val acc: 0.9558, best train loss: 0.1529  @ epoch 9310 )\n",
      "[Epoch: 9480] train loss: 0.1583, train acc: 0.9475, val loss: 0.1759, val acc: 0.9562  (best train acc: 0.9505, best val acc: 0.9562, best train loss: 0.1529  @ epoch 9310 )\n",
      "[Epoch: 9500] train loss: 0.1657, train acc: 0.9454, val loss: 0.1799, val acc: 0.9541  (best train acc: 0.9511, best val acc: 0.9562, best train loss: 0.1522  @ epoch 9490 )\n",
      "[Epoch: 9520] train loss: 0.1644, train acc: 0.9468, val loss: 0.1887, val acc: 0.9531  (best train acc: 0.9511, best val acc: 0.9562, best train loss: 0.1522  @ epoch 9490 )\n",
      "[Epoch: 9540] train loss: 0.1608, train acc: 0.9473, val loss: 0.1853, val acc: 0.9555  (best train acc: 0.9511, best val acc: 0.9562, best train loss: 0.1509  @ epoch 9527 )\n",
      "[Epoch: 9560] train loss: 0.1599, train acc: 0.9484, val loss: 0.1829, val acc: 0.9531  (best train acc: 0.9511, best val acc: 0.9568, best train loss: 0.1509  @ epoch 9527 )\n",
      "[Epoch: 9580] train loss: 0.1675, train acc: 0.9414, val loss: 0.1816, val acc: 0.9548  (best train acc: 0.9511, best val acc: 0.9568, best train loss: 0.1509  @ epoch 9527 )\n",
      "[Epoch: 9600] train loss: 0.2147, train acc: 0.9256, val loss: 0.1957, val acc: 0.9444  (best train acc: 0.9511, best val acc: 0.9568, best train loss: 0.1509  @ epoch 9527 )\n",
      "[Epoch: 9620] train loss: 0.1893, train acc: 0.9398, val loss: 0.1860, val acc: 0.9518  (best train acc: 0.9511, best val acc: 0.9568, best train loss: 0.1509  @ epoch 9527 )\n",
      "[Epoch: 9640] train loss: 0.1602, train acc: 0.9481, val loss: 0.1809, val acc: 0.9541  (best train acc: 0.9511, best val acc: 0.9568, best train loss: 0.1509  @ epoch 9527 )\n",
      "[Epoch: 9660] train loss: 0.1585, train acc: 0.9488, val loss: 0.1899, val acc: 0.9514  (best train acc: 0.9511, best val acc: 0.9568, best train loss: 0.1509  @ epoch 9527 )\n",
      "[Epoch: 9680] train loss: 0.1731, train acc: 0.9386, val loss: 0.2290, val acc: 0.9369  (best train acc: 0.9511, best val acc: 0.9568, best train loss: 0.1509  @ epoch 9527 )\n",
      "[Epoch: 9700] train loss: 0.1642, train acc: 0.9456, val loss: 0.2081, val acc: 0.9501  (best train acc: 0.9511, best val acc: 0.9568, best train loss: 0.1509  @ epoch 9527 )\n",
      "[Epoch: 9720] train loss: 0.1714, train acc: 0.9429, val loss: 0.1756, val acc: 0.9545  (best train acc: 0.9511, best val acc: 0.9568, best train loss: 0.1509  @ epoch 9527 )\n",
      "[Epoch: 9740] train loss: 0.1560, train acc: 0.9493, val loss: 0.1814, val acc: 0.9541  (best train acc: 0.9519, best val acc: 0.9568, best train loss: 0.1509  @ epoch 9527 )\n",
      "[Epoch: 9760] train loss: 0.1682, train acc: 0.9459, val loss: 0.1776, val acc: 0.9531  (best train acc: 0.9525, best val acc: 0.9568, best train loss: 0.1509  @ epoch 9527 )\n",
      "[Epoch: 9780] train loss: 0.1919, train acc: 0.9270, val loss: 0.2026, val acc: 0.9433  (best train acc: 0.9525, best val acc: 0.9568, best train loss: 0.1509  @ epoch 9527 )\n",
      "[Epoch: 9800] train loss: 0.1676, train acc: 0.9492, val loss: 0.1785, val acc: 0.9541  (best train acc: 0.9525, best val acc: 0.9568, best train loss: 0.1509  @ epoch 9527 )\n",
      "[Epoch: 9820] train loss: 0.1685, train acc: 0.9451, val loss: 0.1805, val acc: 0.9545  (best train acc: 0.9525, best val acc: 0.9578, best train loss: 0.1507  @ epoch 9801 )\n",
      "[Epoch: 9840] train loss: 0.1596, train acc: 0.9508, val loss: 0.1775, val acc: 0.9548  (best train acc: 0.9525, best val acc: 0.9578, best train loss: 0.1507  @ epoch 9801 )\n",
      "[Epoch: 9860] train loss: 0.1645, train acc: 0.9448, val loss: 0.1798, val acc: 0.9558  (best train acc: 0.9525, best val acc: 0.9578, best train loss: 0.1507  @ epoch 9801 )\n",
      "[Epoch: 9880] train loss: 0.1739, train acc: 0.9412, val loss: 0.1916, val acc: 0.9501  (best train acc: 0.9525, best val acc: 0.9578, best train loss: 0.1507  @ epoch 9801 )\n",
      "[Epoch: 9900] train loss: 0.1561, train acc: 0.9490, val loss: 0.1983, val acc: 0.9524  (best train acc: 0.9525, best val acc: 0.9582, best train loss: 0.1507  @ epoch 9801 )\n",
      "[Epoch: 9920] train loss: 0.1575, train acc: 0.9489, val loss: 0.2037, val acc: 0.9504  (best train acc: 0.9525, best val acc: 0.9582, best train loss: 0.1497  @ epoch 9904 )\n",
      "[Epoch: 9940] train loss: 0.1541, train acc: 0.9498, val loss: 0.1789, val acc: 0.9551  (best train acc: 0.9525, best val acc: 0.9582, best train loss: 0.1497  @ epoch 9904 )\n",
      "[Epoch: 9960] train loss: 0.1622, train acc: 0.9470, val loss: 0.1777, val acc: 0.9555  (best train acc: 0.9534, best val acc: 0.9582, best train loss: 0.1474  @ epoch 9949 )\n",
      "[Epoch: 9980] train loss: 0.1632, train acc: 0.9440, val loss: 0.1906, val acc: 0.9545  (best train acc: 0.9534, best val acc: 0.9582, best train loss: 0.1474  @ epoch 9949 )\n",
      "[Epoch: 10000] train loss: 0.1553, train acc: 0.9485, val loss: 0.1793, val acc: 0.9558  (best train acc: 0.9534, best val acc: 0.9582, best train loss: 0.1462  @ epoch 9996 )\n",
      "[Epoch: 10020] train loss: 0.1684, train acc: 0.9438, val loss: 0.1883, val acc: 0.9531  (best train acc: 0.9534, best val acc: 0.9582, best train loss: 0.1462  @ epoch 9996 )\n",
      "[Epoch: 10040] train loss: 0.1613, train acc: 0.9449, val loss: 0.1793, val acc: 0.9551  (best train acc: 0.9534, best val acc: 0.9582, best train loss: 0.1462  @ epoch 9996 )\n",
      "[Epoch: 10060] train loss: 0.1630, train acc: 0.9466, val loss: 0.1819, val acc: 0.9555  (best train acc: 0.9534, best val acc: 0.9582, best train loss: 0.1462  @ epoch 9996 )\n",
      "[Epoch: 10080] train loss: 0.1727, train acc: 0.9398, val loss: 0.1850, val acc: 0.9558  (best train acc: 0.9539, best val acc: 0.9582, best train loss: 0.1462  @ epoch 9996 )\n",
      "[Epoch: 10100] train loss: 0.1914, train acc: 0.9358, val loss: 0.1894, val acc: 0.9535  (best train acc: 0.9539, best val acc: 0.9582, best train loss: 0.1462  @ epoch 9996 )\n",
      "[Epoch: 10120] train loss: 0.1686, train acc: 0.9425, val loss: 0.1816, val acc: 0.9551  (best train acc: 0.9539, best val acc: 0.9582, best train loss: 0.1462  @ epoch 9996 )\n",
      "[Epoch: 10140] train loss: 0.1620, train acc: 0.9463, val loss: 0.1785, val acc: 0.9565  (best train acc: 0.9539, best val acc: 0.9582, best train loss: 0.1462  @ epoch 9996 )\n",
      "[Epoch: 10160] train loss: 0.1549, train acc: 0.9467, val loss: 0.1915, val acc: 0.9558  (best train acc: 0.9541, best val acc: 0.9582, best train loss: 0.1455  @ epoch 10156 )\n",
      "[Epoch: 10180] train loss: 0.1837, train acc: 0.9333, val loss: 0.2016, val acc: 0.9481  (best train acc: 0.9541, best val acc: 0.9582, best train loss: 0.1455  @ epoch 10156 )\n",
      "[Epoch: 10200] train loss: 0.1613, train acc: 0.9456, val loss: 0.1850, val acc: 0.9562  (best train acc: 0.9541, best val acc: 0.9582, best train loss: 0.1455  @ epoch 10156 )\n",
      "[Epoch: 10220] train loss: 0.1641, train acc: 0.9445, val loss: 0.1879, val acc: 0.9467  (best train acc: 0.9541, best val acc: 0.9582, best train loss: 0.1440  @ epoch 10215 )\n",
      "[Epoch: 10240] train loss: 0.1746, train acc: 0.9416, val loss: 0.1985, val acc: 0.9501  (best train acc: 0.9541, best val acc: 0.9582, best train loss: 0.1440  @ epoch 10215 )\n",
      "[Epoch: 10260] train loss: 0.1610, train acc: 0.9487, val loss: 0.1906, val acc: 0.9494  (best train acc: 0.9541, best val acc: 0.9582, best train loss: 0.1440  @ epoch 10215 )\n",
      "[Epoch: 10280] train loss: 0.1873, train acc: 0.9404, val loss: 0.1834, val acc: 0.9504  (best train acc: 0.9541, best val acc: 0.9582, best train loss: 0.1440  @ epoch 10215 )\n",
      "[Epoch: 10300] train loss: 0.1632, train acc: 0.9464, val loss: 0.1897, val acc: 0.9562  (best train acc: 0.9541, best val acc: 0.9582, best train loss: 0.1440  @ epoch 10215 )\n",
      "[Epoch: 10320] train loss: 0.1768, train acc: 0.9439, val loss: 0.1880, val acc: 0.9481  (best train acc: 0.9541, best val acc: 0.9582, best train loss: 0.1440  @ epoch 10215 )\n",
      "[Epoch: 10340] train loss: 0.1552, train acc: 0.9483, val loss: 0.1747, val acc: 0.9548  (best train acc: 0.9541, best val acc: 0.9582, best train loss: 0.1440  @ epoch 10215 )\n",
      "[Epoch: 10360] train loss: 0.1713, train acc: 0.9356, val loss: 0.1843, val acc: 0.9494  (best train acc: 0.9541, best val acc: 0.9582, best train loss: 0.1440  @ epoch 10215 )\n",
      "[Epoch: 10380] train loss: 0.1564, train acc: 0.9450, val loss: 0.1864, val acc: 0.9531  (best train acc: 0.9541, best val acc: 0.9582, best train loss: 0.1440  @ epoch 10215 )\n",
      "[Epoch: 10400] train loss: 0.1453, train acc: 0.9512, val loss: 0.1738, val acc: 0.9548  (best train acc: 0.9541, best val acc: 0.9582, best train loss: 0.1440  @ epoch 10215 )\n",
      "[Epoch: 10420] train loss: 0.1693, train acc: 0.9443, val loss: 0.1782, val acc: 0.9548  (best train acc: 0.9541, best val acc: 0.9582, best train loss: 0.1409  @ epoch 10417 )\n",
      "[Epoch: 10440] train loss: 0.1521, train acc: 0.9503, val loss: 0.1791, val acc: 0.9541  (best train acc: 0.9542, best val acc: 0.9582, best train loss: 0.1394  @ epoch 10429 )\n",
      "[Epoch: 10460] train loss: 0.2186, train acc: 0.9118, val loss: 0.1908, val acc: 0.9497  (best train acc: 0.9542, best val acc: 0.9589, best train loss: 0.1394  @ epoch 10429 )\n",
      "[Epoch: 10480] train loss: 0.1693, train acc: 0.9393, val loss: 0.1739, val acc: 0.9545  (best train acc: 0.9542, best val acc: 0.9589, best train loss: 0.1394  @ epoch 10429 )\n",
      "[Epoch: 10500] train loss: 0.1575, train acc: 0.9481, val loss: 0.1726, val acc: 0.9558  (best train acc: 0.9542, best val acc: 0.9589, best train loss: 0.1394  @ epoch 10429 )\n",
      "[Epoch: 10520] train loss: 0.1498, train acc: 0.9519, val loss: 0.1729, val acc: 0.9555  (best train acc: 0.9542, best val acc: 0.9589, best train loss: 0.1394  @ epoch 10429 )\n",
      "[Epoch: 10540] train loss: 0.1559, train acc: 0.9487, val loss: 0.1906, val acc: 0.9487  (best train acc: 0.9545, best val acc: 0.9589, best train loss: 0.1394  @ epoch 10429 )\n",
      "[Epoch: 10560] train loss: 0.1528, train acc: 0.9493, val loss: 0.1766, val acc: 0.9562  (best train acc: 0.9545, best val acc: 0.9589, best train loss: 0.1394  @ epoch 10429 )\n",
      "[Epoch: 10580] train loss: 0.1449, train acc: 0.9534, val loss: 0.1727, val acc: 0.9575  (best train acc: 0.9545, best val acc: 0.9592, best train loss: 0.1394  @ epoch 10429 )\n",
      "[Epoch: 10600] train loss: 0.1485, train acc: 0.9523, val loss: 0.1752, val acc: 0.9545  (best train acc: 0.9552, best val acc: 0.9592, best train loss: 0.1394  @ epoch 10429 )\n",
      "[Epoch: 10620] train loss: 0.1464, train acc: 0.9560, val loss: 0.1785, val acc: 0.9565  (best train acc: 0.9560, best val acc: 0.9595, best train loss: 0.1394  @ epoch 10429 )\n",
      "[Epoch: 10640] train loss: 0.1556, train acc: 0.9461, val loss: 0.1744, val acc: 0.9555  (best train acc: 0.9560, best val acc: 0.9599, best train loss: 0.1394  @ epoch 10429 )\n",
      "[Epoch: 10660] train loss: 0.1592, train acc: 0.9448, val loss: 0.1751, val acc: 0.9578  (best train acc: 0.9560, best val acc: 0.9599, best train loss: 0.1394  @ epoch 10429 )\n",
      "[Epoch: 10680] train loss: 0.1568, train acc: 0.9492, val loss: 0.1750, val acc: 0.9535  (best train acc: 0.9560, best val acc: 0.9599, best train loss: 0.1394  @ epoch 10429 )\n",
      "[Epoch: 10700] train loss: 0.1859, train acc: 0.9370, val loss: 0.2014, val acc: 0.9477  (best train acc: 0.9560, best val acc: 0.9599, best train loss: 0.1394  @ epoch 10429 )\n",
      "[Epoch: 10720] train loss: 0.1676, train acc: 0.9440, val loss: 0.1809, val acc: 0.9558  (best train acc: 0.9560, best val acc: 0.9599, best train loss: 0.1394  @ epoch 10429 )\n",
      "[Epoch: 10740] train loss: 0.1658, train acc: 0.9443, val loss: 0.1773, val acc: 0.9565  (best train acc: 0.9560, best val acc: 0.9599, best train loss: 0.1394  @ epoch 10429 )\n",
      "[Epoch: 10760] train loss: 0.1466, train acc: 0.9485, val loss: 0.1802, val acc: 0.9508  (best train acc: 0.9560, best val acc: 0.9599, best train loss: 0.1394  @ epoch 10429 )\n",
      "[Epoch: 10780] train loss: 0.1500, train acc: 0.9523, val loss: 0.1756, val acc: 0.9575  (best train acc: 0.9560, best val acc: 0.9599, best train loss: 0.1382  @ epoch 10778 )\n",
      "[Epoch: 10800] train loss: 0.1425, train acc: 0.9550, val loss: 0.1659, val acc: 0.9578  (best train acc: 0.9560, best val acc: 0.9599, best train loss: 0.1382  @ epoch 10778 )\n",
      "[Epoch: 10820] train loss: 0.1529, train acc: 0.9478, val loss: 0.1700, val acc: 0.9582  (best train acc: 0.9560, best val acc: 0.9599, best train loss: 0.1382  @ epoch 10778 )\n",
      "[Epoch: 10840] train loss: 0.1459, train acc: 0.9516, val loss: 0.1743, val acc: 0.9535  (best train acc: 0.9560, best val acc: 0.9599, best train loss: 0.1382  @ epoch 10778 )\n",
      "[Epoch: 10860] train loss: 0.1560, train acc: 0.9503, val loss: 0.1735, val acc: 0.9558  (best train acc: 0.9563, best val acc: 0.9599, best train loss: 0.1361  @ epoch 10856 )\n",
      "[Epoch: 10880] train loss: 0.1960, train acc: 0.9327, val loss: 0.1911, val acc: 0.9528  (best train acc: 0.9563, best val acc: 0.9599, best train loss: 0.1361  @ epoch 10856 )\n",
      "[Epoch: 10900] train loss: 0.1494, train acc: 0.9502, val loss: 0.1708, val acc: 0.9562  (best train acc: 0.9563, best val acc: 0.9599, best train loss: 0.1361  @ epoch 10856 )\n",
      "[Epoch: 10920] train loss: 0.1380, train acc: 0.9535, val loss: 0.1704, val acc: 0.9562  (best train acc: 0.9566, best val acc: 0.9599, best train loss: 0.1361  @ epoch 10856 )\n",
      "[Epoch: 10940] train loss: 0.1461, train acc: 0.9535, val loss: 0.1735, val acc: 0.9568  (best train acc: 0.9568, best val acc: 0.9602, best train loss: 0.1361  @ epoch 10856 )\n",
      "[Epoch: 10960] train loss: 0.1385, train acc: 0.9539, val loss: 0.1712, val acc: 0.9582  (best train acc: 0.9568, best val acc: 0.9602, best train loss: 0.1361  @ epoch 10856 )\n",
      "[Epoch: 10980] train loss: 0.1435, train acc: 0.9534, val loss: 0.1797, val acc: 0.9589  (best train acc: 0.9568, best val acc: 0.9609, best train loss: 0.1359  @ epoch 10964 )\n",
      "[Epoch: 11000] train loss: 0.1596, train acc: 0.9496, val loss: 0.1794, val acc: 0.9548  (best train acc: 0.9573, best val acc: 0.9609, best train loss: 0.1325  @ epoch 10988 )\n",
      "[Epoch: 11020] train loss: 0.1866, train acc: 0.9382, val loss: 0.2052, val acc: 0.9376  (best train acc: 0.9573, best val acc: 0.9609, best train loss: 0.1325  @ epoch 10988 )\n",
      "[Epoch: 11040] train loss: 0.1492, train acc: 0.9535, val loss: 0.1788, val acc: 0.9531  (best train acc: 0.9573, best val acc: 0.9609, best train loss: 0.1325  @ epoch 10988 )\n",
      "[Epoch: 11060] train loss: 0.1594, train acc: 0.9439, val loss: 0.1734, val acc: 0.9572  (best train acc: 0.9573, best val acc: 0.9609, best train loss: 0.1325  @ epoch 10988 )\n",
      "[Epoch: 11080] train loss: 0.1469, train acc: 0.9489, val loss: 0.1903, val acc: 0.9501  (best train acc: 0.9573, best val acc: 0.9609, best train loss: 0.1325  @ epoch 10988 )\n",
      "[Epoch: 11100] train loss: 0.1480, train acc: 0.9520, val loss: 0.1858, val acc: 0.9548  (best train acc: 0.9573, best val acc: 0.9609, best train loss: 0.1325  @ epoch 10988 )\n",
      "[Epoch: 11120] train loss: 0.1507, train acc: 0.9495, val loss: 0.1913, val acc: 0.9528  (best train acc: 0.9573, best val acc: 0.9609, best train loss: 0.1325  @ epoch 10988 )\n",
      "[Epoch: 11140] train loss: 0.1431, train acc: 0.9519, val loss: 0.1731, val acc: 0.9575  (best train acc: 0.9575, best val acc: 0.9609, best train loss: 0.1325  @ epoch 10988 )\n",
      "[Epoch: 11160] train loss: 0.1437, train acc: 0.9542, val loss: 0.1900, val acc: 0.9568  (best train acc: 0.9580, best val acc: 0.9616, best train loss: 0.1325  @ epoch 10988 )\n",
      "[Epoch: 11180] train loss: 0.1396, train acc: 0.9529, val loss: 0.1730, val acc: 0.9582  (best train acc: 0.9580, best val acc: 0.9616, best train loss: 0.1325  @ epoch 10988 )\n",
      "[Epoch: 11200] train loss: 0.1620, train acc: 0.9499, val loss: 0.1797, val acc: 0.9589  (best train acc: 0.9583, best val acc: 0.9616, best train loss: 0.1313  @ epoch 11193 )\n",
      "[Epoch: 11220] train loss: 0.1368, train acc: 0.9552, val loss: 0.1725, val acc: 0.9565  (best train acc: 0.9583, best val acc: 0.9616, best train loss: 0.1313  @ epoch 11193 )\n",
      "[Epoch: 11240] train loss: 0.1434, train acc: 0.9526, val loss: 0.2114, val acc: 0.9440  (best train acc: 0.9583, best val acc: 0.9616, best train loss: 0.1313  @ epoch 11193 )\n",
      "[Epoch: 11260] train loss: 0.1439, train acc: 0.9542, val loss: 0.1697, val acc: 0.9592  (best train acc: 0.9583, best val acc: 0.9616, best train loss: 0.1313  @ epoch 11193 )\n",
      "[Epoch: 11280] train loss: 0.1507, train acc: 0.9503, val loss: 0.1736, val acc: 0.9582  (best train acc: 0.9583, best val acc: 0.9616, best train loss: 0.1275  @ epoch 11270 )\n",
      "[Epoch: 11300] train loss: 0.1397, train acc: 0.9532, val loss: 0.1882, val acc: 0.9558  (best train acc: 0.9583, best val acc: 0.9616, best train loss: 0.1275  @ epoch 11270 )\n",
      "[Epoch: 11320] train loss: 0.1760, train acc: 0.9367, val loss: 0.1671, val acc: 0.9558  (best train acc: 0.9583, best val acc: 0.9616, best train loss: 0.1275  @ epoch 11270 )\n",
      "[Epoch: 11340] train loss: 0.1759, train acc: 0.9328, val loss: 0.2100, val acc: 0.9417  (best train acc: 0.9583, best val acc: 0.9616, best train loss: 0.1275  @ epoch 11270 )\n",
      "[Epoch: 11360] train loss: 0.1458, train acc: 0.9545, val loss: 0.1724, val acc: 0.9578  (best train acc: 0.9583, best val acc: 0.9616, best train loss: 0.1275  @ epoch 11270 )\n",
      "[Epoch: 11380] train loss: 0.1516, train acc: 0.9514, val loss: 0.1700, val acc: 0.9592  (best train acc: 0.9583, best val acc: 0.9616, best train loss: 0.1275  @ epoch 11270 )\n",
      "[Epoch: 11400] train loss: 0.1451, train acc: 0.9486, val loss: 0.1732, val acc: 0.9555  (best train acc: 0.9583, best val acc: 0.9616, best train loss: 0.1275  @ epoch 11270 )\n",
      "[Epoch: 11420] train loss: 0.1862, train acc: 0.9414, val loss: 0.1826, val acc: 0.9524  (best train acc: 0.9583, best val acc: 0.9616, best train loss: 0.1275  @ epoch 11270 )\n",
      "[Epoch: 11440] train loss: 0.1462, train acc: 0.9537, val loss: 0.1795, val acc: 0.9568  (best train acc: 0.9583, best val acc: 0.9616, best train loss: 0.1275  @ epoch 11270 )\n",
      "[Epoch: 11460] train loss: 0.1430, train acc: 0.9515, val loss: 0.1786, val acc: 0.9578  (best train acc: 0.9583, best val acc: 0.9616, best train loss: 0.1275  @ epoch 11270 )\n",
      "[Epoch: 11480] train loss: 0.1424, train acc: 0.9515, val loss: 0.1756, val acc: 0.9568  (best train acc: 0.9583, best val acc: 0.9616, best train loss: 0.1275  @ epoch 11270 )\n",
      "[Epoch: 11500] train loss: 0.1526, train acc: 0.9540, val loss: 0.1913, val acc: 0.9514  (best train acc: 0.9583, best val acc: 0.9616, best train loss: 0.1275  @ epoch 11270 )\n",
      "[Epoch: 11520] train loss: 0.1635, train acc: 0.9397, val loss: 0.1917, val acc: 0.9460  (best train acc: 0.9583, best val acc: 0.9616, best train loss: 0.1275  @ epoch 11270 )\n",
      "[Epoch: 11540] train loss: 0.1438, train acc: 0.9511, val loss: 0.1885, val acc: 0.9568  (best train acc: 0.9583, best val acc: 0.9616, best train loss: 0.1275  @ epoch 11270 )\n",
      "[Epoch: 11560] train loss: 0.1342, train acc: 0.9566, val loss: 0.1779, val acc: 0.9572  (best train acc: 0.9583, best val acc: 0.9616, best train loss: 0.1275  @ epoch 11270 )\n",
      "[Epoch: 11580] train loss: 0.1352, train acc: 0.9547, val loss: 0.1741, val acc: 0.9589  (best train acc: 0.9592, best val acc: 0.9616, best train loss: 0.1275  @ epoch 11270 )\n",
      "[Epoch: 11600] train loss: 0.1469, train acc: 0.9498, val loss: 0.1787, val acc: 0.9575  (best train acc: 0.9597, best val acc: 0.9616, best train loss: 0.1260  @ epoch 11595 )\n",
      "[Epoch: 11620] train loss: 0.1397, train acc: 0.9526, val loss: 0.1710, val acc: 0.9595  (best train acc: 0.9597, best val acc: 0.9616, best train loss: 0.1260  @ epoch 11595 )\n",
      "[Epoch: 11640] train loss: 0.1470, train acc: 0.9496, val loss: 0.1824, val acc: 0.9578  (best train acc: 0.9597, best val acc: 0.9616, best train loss: 0.1260  @ epoch 11595 )\n",
      "[Epoch: 11660] train loss: 0.1719, train acc: 0.9465, val loss: 0.2127, val acc: 0.9430  (best train acc: 0.9597, best val acc: 0.9616, best train loss: 0.1257  @ epoch 11656 )\n",
      "[Epoch: 11680] train loss: 0.1502, train acc: 0.9485, val loss: 0.1816, val acc: 0.9518  (best train acc: 0.9597, best val acc: 0.9616, best train loss: 0.1257  @ epoch 11656 )\n",
      "[Epoch: 11700] train loss: 0.1703, train acc: 0.9454, val loss: 0.1965, val acc: 0.9491  (best train acc: 0.9597, best val acc: 0.9616, best train loss: 0.1257  @ epoch 11656 )\n",
      "[Epoch: 11720] train loss: 0.1562, train acc: 0.9483, val loss: 0.1803, val acc: 0.9531  (best train acc: 0.9597, best val acc: 0.9616, best train loss: 0.1257  @ epoch 11656 )\n",
      "[Epoch: 11740] train loss: 0.1404, train acc: 0.9527, val loss: 0.1795, val acc: 0.9572  (best train acc: 0.9597, best val acc: 0.9616, best train loss: 0.1257  @ epoch 11656 )\n",
      "[Epoch: 11760] train loss: 0.1418, train acc: 0.9537, val loss: 0.1905, val acc: 0.9524  (best train acc: 0.9597, best val acc: 0.9619, best train loss: 0.1255  @ epoch 11755 )\n",
      "[Epoch: 11780] train loss: 0.1317, train acc: 0.9574, val loss: 0.1767, val acc: 0.9589  (best train acc: 0.9597, best val acc: 0.9619, best train loss: 0.1255  @ epoch 11755 )\n",
      "[Epoch: 11800] train loss: 0.1539, train acc: 0.9494, val loss: 0.1783, val acc: 0.9589  (best train acc: 0.9597, best val acc: 0.9619, best train loss: 0.1255  @ epoch 11799 )\n",
      "[Epoch: 11820] train loss: 0.1433, train acc: 0.9492, val loss: 0.1812, val acc: 0.9595  (best train acc: 0.9597, best val acc: 0.9619, best train loss: 0.1255  @ epoch 11799 )\n",
      "[Epoch: 11840] train loss: 0.1347, train acc: 0.9540, val loss: 0.1948, val acc: 0.9558  (best train acc: 0.9597, best val acc: 0.9619, best train loss: 0.1255  @ epoch 11799 )\n",
      "[Epoch: 11860] train loss: 0.1350, train acc: 0.9543, val loss: 0.2157, val acc: 0.9467  (best train acc: 0.9597, best val acc: 0.9619, best train loss: 0.1255  @ epoch 11799 )\n",
      "[Epoch: 11880] train loss: 0.1398, train acc: 0.9515, val loss: 0.1847, val acc: 0.9538  (best train acc: 0.9597, best val acc: 0.9619, best train loss: 0.1255  @ epoch 11799 )\n",
      "[Epoch: 11900] train loss: 0.1292, train acc: 0.9592, val loss: 0.1785, val acc: 0.9575  (best train acc: 0.9597, best val acc: 0.9619, best train loss: 0.1255  @ epoch 11799 )\n",
      "[Epoch: 11920] train loss: 0.1364, train acc: 0.9538, val loss: 0.1827, val acc: 0.9585  (best train acc: 0.9597, best val acc: 0.9619, best train loss: 0.1255  @ epoch 11799 )\n",
      "[Epoch: 11940] train loss: 0.1397, train acc: 0.9518, val loss: 0.2000, val acc: 0.9555  (best train acc: 0.9597, best val acc: 0.9619, best train loss: 0.1255  @ epoch 11799 )\n",
      "[Epoch: 11960] train loss: 0.1415, train acc: 0.9501, val loss: 0.1787, val acc: 0.9592  (best train acc: 0.9597, best val acc: 0.9619, best train loss: 0.1255  @ epoch 11799 )\n",
      "[Epoch: 11980] train loss: 0.1669, train acc: 0.9468, val loss: 0.2019, val acc: 0.9470  (best train acc: 0.9597, best val acc: 0.9619, best train loss: 0.1255  @ epoch 11799 )\n",
      "[Epoch: 12000] train loss: 0.1489, train acc: 0.9521, val loss: 0.1782, val acc: 0.9551  (best train acc: 0.9597, best val acc: 0.9619, best train loss: 0.1255  @ epoch 11799 )\n",
      "[Epoch: 12020] train loss: 0.1464, train acc: 0.9510, val loss: 0.1839, val acc: 0.9578  (best train acc: 0.9597, best val acc: 0.9619, best train loss: 0.1255  @ epoch 11799 )\n",
      "[Epoch: 12040] train loss: 0.1278, train acc: 0.9571, val loss: 0.1760, val acc: 0.9585  (best train acc: 0.9597, best val acc: 0.9619, best train loss: 0.1255  @ epoch 11799 )\n",
      "[Epoch: 12060] train loss: 0.1345, train acc: 0.9550, val loss: 0.1800, val acc: 0.9575  (best train acc: 0.9599, best val acc: 0.9619, best train loss: 0.1255  @ epoch 11799 )\n",
      "[Epoch: 12080] train loss: 0.1420, train acc: 0.9535, val loss: 0.1809, val acc: 0.9551  (best train acc: 0.9599, best val acc: 0.9619, best train loss: 0.1250  @ epoch 12069 )\n",
      "[Epoch: 12100] train loss: 0.1403, train acc: 0.9539, val loss: 0.1787, val acc: 0.9578  (best train acc: 0.9599, best val acc: 0.9619, best train loss: 0.1250  @ epoch 12069 )\n",
      "[Epoch: 12120] train loss: 0.1278, train acc: 0.9580, val loss: 0.1811, val acc: 0.9582  (best train acc: 0.9618, best val acc: 0.9619, best train loss: 0.1201  @ epoch 12117 )\n",
      "[Epoch: 12140] train loss: 0.1423, train acc: 0.9537, val loss: 0.1807, val acc: 0.9562  (best train acc: 0.9618, best val acc: 0.9629, best train loss: 0.1201  @ epoch 12117 )\n",
      "[Epoch: 12160] train loss: 0.1632, train acc: 0.9465, val loss: 0.1915, val acc: 0.9484  (best train acc: 0.9618, best val acc: 0.9629, best train loss: 0.1201  @ epoch 12117 )\n",
      "[Epoch: 12180] train loss: 0.1381, train acc: 0.9561, val loss: 0.1801, val acc: 0.9595  (best train acc: 0.9618, best val acc: 0.9629, best train loss: 0.1201  @ epoch 12117 )\n",
      "[Epoch: 12200] train loss: 0.1325, train acc: 0.9549, val loss: 0.1787, val acc: 0.9582  (best train acc: 0.9618, best val acc: 0.9629, best train loss: 0.1201  @ epoch 12117 )\n",
      "[Epoch: 12220] train loss: 0.1341, train acc: 0.9555, val loss: 0.1804, val acc: 0.9592  (best train acc: 0.9618, best val acc: 0.9629, best train loss: 0.1201  @ epoch 12117 )\n",
      "[Epoch: 12240] train loss: 0.1299, train acc: 0.9571, val loss: 0.1927, val acc: 0.9582  (best train acc: 0.9618, best val acc: 0.9629, best train loss: 0.1201  @ epoch 12117 )\n",
      "[Epoch: 12260] train loss: 0.1420, train acc: 0.9531, val loss: 0.1929, val acc: 0.9558  (best train acc: 0.9618, best val acc: 0.9629, best train loss: 0.1201  @ epoch 12117 )\n",
      "[Epoch: 12280] train loss: 0.1325, train acc: 0.9557, val loss: 0.1825, val acc: 0.9605  (best train acc: 0.9618, best val acc: 0.9629, best train loss: 0.1201  @ epoch 12117 )\n",
      "[Epoch: 12300] train loss: 0.1321, train acc: 0.9569, val loss: 0.1939, val acc: 0.9562  (best train acc: 0.9618, best val acc: 0.9629, best train loss: 0.1201  @ epoch 12117 )\n",
      "[Epoch: 12320] train loss: 0.1354, train acc: 0.9575, val loss: 0.1912, val acc: 0.9535  (best train acc: 0.9618, best val acc: 0.9629, best train loss: 0.1201  @ epoch 12117 )\n",
      "[Epoch: 12340] train loss: 0.1347, train acc: 0.9555, val loss: 0.2099, val acc: 0.9518  (best train acc: 0.9618, best val acc: 0.9629, best train loss: 0.1201  @ epoch 12117 )\n",
      "[Epoch: 12360] train loss: 0.1412, train acc: 0.9488, val loss: 0.2050, val acc: 0.9541  (best train acc: 0.9618, best val acc: 0.9629, best train loss: 0.1201  @ epoch 12117 )\n",
      "[Epoch: 12380] train loss: 0.1656, train acc: 0.9370, val loss: 0.2059, val acc: 0.9444  (best train acc: 0.9618, best val acc: 0.9629, best train loss: 0.1201  @ epoch 12117 )\n",
      "[Epoch: 12400] train loss: 0.1403, train acc: 0.9520, val loss: 0.1945, val acc: 0.9585  (best train acc: 0.9618, best val acc: 0.9629, best train loss: 0.1201  @ epoch 12117 )\n",
      "[Epoch: 12420] train loss: 0.1382, train acc: 0.9536, val loss: 0.1802, val acc: 0.9589  (best train acc: 0.9618, best val acc: 0.9629, best train loss: 0.1201  @ epoch 12117 )\n",
      "[Epoch: 12440] train loss: 0.1360, train acc: 0.9552, val loss: 0.1849, val acc: 0.9595  (best train acc: 0.9618, best val acc: 0.9629, best train loss: 0.1201  @ epoch 12117 )\n",
      "[Epoch: 12460] train loss: 0.1287, train acc: 0.9566, val loss: 0.1854, val acc: 0.9585  (best train acc: 0.9618, best val acc: 0.9629, best train loss: 0.1201  @ epoch 12117 )\n",
      "[Epoch: 12480] train loss: 0.1328, train acc: 0.9545, val loss: 0.2011, val acc: 0.9575  (best train acc: 0.9620, best val acc: 0.9629, best train loss: 0.1201  @ epoch 12117 )\n",
      "[Epoch: 12500] train loss: 0.1268, train acc: 0.9587, val loss: 0.1761, val acc: 0.9578  (best train acc: 0.9620, best val acc: 0.9629, best train loss: 0.1201  @ epoch 12117 )\n",
      "[Epoch: 12520] train loss: 0.1286, train acc: 0.9578, val loss: 0.1825, val acc: 0.9589  (best train acc: 0.9620, best val acc: 0.9629, best train loss: 0.1201  @ epoch 12516 )\n",
      "[Epoch: 12540] train loss: 0.1218, train acc: 0.9599, val loss: 0.1863, val acc: 0.9565  (best train acc: 0.9620, best val acc: 0.9629, best train loss: 0.1201  @ epoch 12516 )\n",
      "[Epoch: 12560] train loss: 0.1912, train acc: 0.9311, val loss: 0.1821, val acc: 0.9578  (best train acc: 0.9620, best val acc: 0.9629, best train loss: 0.1201  @ epoch 12516 )\n",
      "[Epoch: 12580] train loss: 0.1669, train acc: 0.9373, val loss: 0.2259, val acc: 0.9292  (best train acc: 0.9620, best val acc: 0.9629, best train loss: 0.1201  @ epoch 12516 )\n",
      "[Epoch: 12600] train loss: 0.1623, train acc: 0.9490, val loss: 0.2043, val acc: 0.9467  (best train acc: 0.9620, best val acc: 0.9629, best train loss: 0.1201  @ epoch 12516 )\n",
      "[Epoch: 12620] train loss: 0.1529, train acc: 0.9511, val loss: 0.1837, val acc: 0.9565  (best train acc: 0.9620, best val acc: 0.9629, best train loss: 0.1201  @ epoch 12516 )\n",
      "[Epoch: 12640] train loss: 0.1476, train acc: 0.9450, val loss: 0.2013, val acc: 0.9565  (best train acc: 0.9620, best val acc: 0.9629, best train loss: 0.1201  @ epoch 12516 )\n",
      "[Epoch: 12660] train loss: 0.1246, train acc: 0.9589, val loss: 0.1827, val acc: 0.9605  (best train acc: 0.9620, best val acc: 0.9629, best train loss: 0.1201  @ epoch 12516 )\n",
      "[Epoch: 12680] train loss: 0.1180, train acc: 0.9610, val loss: 0.1908, val acc: 0.9592  (best train acc: 0.9620, best val acc: 0.9629, best train loss: 0.1180  @ epoch 12680 )\n",
      "[Epoch: 12700] train loss: 0.1336, train acc: 0.9567, val loss: 0.1897, val acc: 0.9572  (best train acc: 0.9620, best val acc: 0.9629, best train loss: 0.1180  @ epoch 12680 )\n",
      "[Epoch: 12720] train loss: 0.1249, train acc: 0.9576, val loss: 0.1880, val acc: 0.9602  (best train acc: 0.9620, best val acc: 0.9629, best train loss: 0.1180  @ epoch 12680 )\n",
      "[Epoch: 12740] train loss: 0.1286, train acc: 0.9550, val loss: 0.2244, val acc: 0.9470  (best train acc: 0.9620, best val acc: 0.9629, best train loss: 0.1167  @ epoch 12728 )\n",
      "[Epoch: 12760] train loss: 0.1387, train acc: 0.9555, val loss: 0.1794, val acc: 0.9545  (best train acc: 0.9620, best val acc: 0.9629, best train loss: 0.1167  @ epoch 12728 )\n",
      "[Epoch: 12780] train loss: 0.1189, train acc: 0.9618, val loss: 0.1840, val acc: 0.9595  (best train acc: 0.9631, best val acc: 0.9629, best train loss: 0.1167  @ epoch 12728 )\n",
      "[Epoch: 12800] train loss: 0.1206, train acc: 0.9602, val loss: 0.2088, val acc: 0.9531  (best train acc: 0.9631, best val acc: 0.9629, best train loss: 0.1167  @ epoch 12728 )\n",
      "[Epoch: 12820] train loss: 0.1371, train acc: 0.9570, val loss: 0.1939, val acc: 0.9548  (best train acc: 0.9631, best val acc: 0.9629, best train loss: 0.1167  @ epoch 12728 )\n",
      "[Epoch: 12840] train loss: 0.1273, train acc: 0.9566, val loss: 0.1806, val acc: 0.9612  (best train acc: 0.9631, best val acc: 0.9629, best train loss: 0.1167  @ epoch 12728 )\n",
      "[Epoch: 12860] train loss: 0.1291, train acc: 0.9553, val loss: 0.1919, val acc: 0.9585  (best train acc: 0.9631, best val acc: 0.9629, best train loss: 0.1167  @ epoch 12728 )\n",
      "[Epoch: 12880] train loss: 0.1334, train acc: 0.9557, val loss: 0.1939, val acc: 0.9582  (best train acc: 0.9632, best val acc: 0.9629, best train loss: 0.1118  @ epoch 12864 )\n",
      "[Epoch: 12900] train loss: 0.1481, train acc: 0.9547, val loss: 0.2102, val acc: 0.9538  (best train acc: 0.9632, best val acc: 0.9629, best train loss: 0.1118  @ epoch 12864 )\n",
      "[Epoch: 12920] train loss: 0.1279, train acc: 0.9586, val loss: 0.1948, val acc: 0.9592  (best train acc: 0.9632, best val acc: 0.9629, best train loss: 0.1118  @ epoch 12864 )\n",
      "[Epoch: 12940] train loss: 0.1953, train acc: 0.9202, val loss: 0.1970, val acc: 0.9531  (best train acc: 0.9632, best val acc: 0.9629, best train loss: 0.1118  @ epoch 12864 )\n",
      "[Epoch: 12960] train loss: 0.1325, train acc: 0.9550, val loss: 0.1884, val acc: 0.9545  (best train acc: 0.9632, best val acc: 0.9629, best train loss: 0.1118  @ epoch 12864 )\n",
      "[Epoch: 12980] train loss: 0.1299, train acc: 0.9590, val loss: 0.1857, val acc: 0.9589  (best train acc: 0.9632, best val acc: 0.9629, best train loss: 0.1118  @ epoch 12864 )\n",
      "[Epoch: 13000] train loss: 0.1221, train acc: 0.9597, val loss: 0.1859, val acc: 0.9555  (best train acc: 0.9632, best val acc: 0.9629, best train loss: 0.1118  @ epoch 12864 )\n",
      "[Epoch: 13020] train loss: 0.1210, train acc: 0.9620, val loss: 0.1880, val acc: 0.9599  (best train acc: 0.9636, best val acc: 0.9629, best train loss: 0.1118  @ epoch 12864 )\n",
      "[Epoch: 13040] train loss: 0.1231, train acc: 0.9597, val loss: 0.1874, val acc: 0.9599  (best train acc: 0.9636, best val acc: 0.9629, best train loss: 0.1118  @ epoch 12864 )\n",
      "[Epoch: 13060] train loss: 0.1174, train acc: 0.9614, val loss: 0.1811, val acc: 0.9616  (best train acc: 0.9636, best val acc: 0.9629, best train loss: 0.1118  @ epoch 12864 )\n",
      "[Epoch: 13080] train loss: 0.1199, train acc: 0.9609, val loss: 0.1799, val acc: 0.9595  (best train acc: 0.9650, best val acc: 0.9629, best train loss: 0.1115  @ epoch 13065 )\n",
      "[Epoch: 13100] train loss: 0.1195, train acc: 0.9595, val loss: 0.1842, val acc: 0.9592  (best train acc: 0.9650, best val acc: 0.9629, best train loss: 0.1115  @ epoch 13065 )\n",
      "[Epoch: 13120] train loss: 0.1311, train acc: 0.9574, val loss: 0.1782, val acc: 0.9609  (best train acc: 0.9650, best val acc: 0.9632, best train loss: 0.1115  @ epoch 13065 )\n",
      "[Epoch: 13140] train loss: 0.1316, train acc: 0.9559, val loss: 0.1905, val acc: 0.9541  (best train acc: 0.9650, best val acc: 0.9632, best train loss: 0.1115  @ epoch 13065 )\n",
      "[Epoch: 13160] train loss: 0.1294, train acc: 0.9594, val loss: 0.1836, val acc: 0.9582  (best train acc: 0.9650, best val acc: 0.9632, best train loss: 0.1115  @ epoch 13065 )\n",
      "[Epoch: 13180] train loss: 0.1347, train acc: 0.9558, val loss: 0.1896, val acc: 0.9562  (best train acc: 0.9650, best val acc: 0.9632, best train loss: 0.1115  @ epoch 13065 )\n",
      "[Epoch: 13200] train loss: 0.1254, train acc: 0.9604, val loss: 0.1860, val acc: 0.9582  (best train acc: 0.9650, best val acc: 0.9632, best train loss: 0.1115  @ epoch 13065 )\n",
      "[Epoch: 13220] train loss: 0.1340, train acc: 0.9571, val loss: 0.1804, val acc: 0.9616  (best train acc: 0.9650, best val acc: 0.9632, best train loss: 0.1115  @ epoch 13065 )\n",
      "[Epoch: 13240] train loss: 0.1536, train acc: 0.9464, val loss: 0.1963, val acc: 0.9578  (best train acc: 0.9650, best val acc: 0.9632, best train loss: 0.1115  @ epoch 13065 )\n",
      "[Epoch: 13260] train loss: 0.1407, train acc: 0.9501, val loss: 0.2137, val acc: 0.9437  (best train acc: 0.9650, best val acc: 0.9632, best train loss: 0.1115  @ epoch 13065 )\n",
      "[Epoch: 13280] train loss: 0.1289, train acc: 0.9597, val loss: 0.1878, val acc: 0.9575  (best train acc: 0.9650, best val acc: 0.9632, best train loss: 0.1115  @ epoch 13065 )\n",
      "[Epoch: 13300] train loss: 0.1468, train acc: 0.9499, val loss: 0.2369, val acc: 0.9447  (best train acc: 0.9650, best val acc: 0.9632, best train loss: 0.1115  @ epoch 13065 )\n",
      "[Epoch: 13320] train loss: 0.1440, train acc: 0.9474, val loss: 0.1893, val acc: 0.9605  (best train acc: 0.9650, best val acc: 0.9632, best train loss: 0.1115  @ epoch 13065 )\n",
      "[Epoch: 13340] train loss: 0.1335, train acc: 0.9539, val loss: 0.1841, val acc: 0.9578  (best train acc: 0.9650, best val acc: 0.9632, best train loss: 0.1115  @ epoch 13065 )\n",
      "[Epoch: 13360] train loss: 0.1351, train acc: 0.9518, val loss: 0.1958, val acc: 0.9599  (best train acc: 0.9650, best val acc: 0.9632, best train loss: 0.1115  @ epoch 13065 )\n",
      "[Epoch: 13380] train loss: 0.1211, train acc: 0.9585, val loss: 0.2001, val acc: 0.9619  (best train acc: 0.9652, best val acc: 0.9632, best train loss: 0.1107  @ epoch 13376 )\n",
      "[Epoch: 13400] train loss: 0.1278, train acc: 0.9592, val loss: 0.1956, val acc: 0.9589  (best train acc: 0.9652, best val acc: 0.9632, best train loss: 0.1107  @ epoch 13376 )\n",
      "[Epoch: 13420] train loss: 0.1244, train acc: 0.9580, val loss: 0.2225, val acc: 0.9514  (best train acc: 0.9652, best val acc: 0.9642, best train loss: 0.1107  @ epoch 13376 )\n",
      "[Epoch: 13440] train loss: 0.1336, train acc: 0.9555, val loss: 0.1831, val acc: 0.9605  (best train acc: 0.9652, best val acc: 0.9642, best train loss: 0.1107  @ epoch 13376 )\n",
      "[Epoch: 13460] train loss: 0.1267, train acc: 0.9597, val loss: 0.1946, val acc: 0.9572  (best train acc: 0.9652, best val acc: 0.9642, best train loss: 0.1107  @ epoch 13376 )\n",
      "[Epoch: 13480] train loss: 0.1210, train acc: 0.9592, val loss: 0.1888, val acc: 0.9589  (best train acc: 0.9652, best val acc: 0.9642, best train loss: 0.1107  @ epoch 13376 )\n",
      "[Epoch: 13500] train loss: 0.1479, train acc: 0.9479, val loss: 0.1973, val acc: 0.9589  (best train acc: 0.9652, best val acc: 0.9642, best train loss: 0.1107  @ epoch 13376 )\n",
      "[Epoch: 13520] train loss: 0.1238, train acc: 0.9587, val loss: 0.1886, val acc: 0.9602  (best train acc: 0.9652, best val acc: 0.9642, best train loss: 0.1107  @ epoch 13376 )\n",
      "[Epoch: 13540] train loss: 0.1194, train acc: 0.9604, val loss: 0.1866, val acc: 0.9622  (best train acc: 0.9654, best val acc: 0.9642, best train loss: 0.1107  @ epoch 13376 )\n",
      "[Epoch: 13560] train loss: 0.1256, train acc: 0.9560, val loss: 0.1878, val acc: 0.9609  (best train acc: 0.9654, best val acc: 0.9646, best train loss: 0.1107  @ epoch 13376 )\n",
      "[Epoch: 13580] train loss: 0.2006, train acc: 0.9346, val loss: 0.1924, val acc: 0.9545  (best train acc: 0.9654, best val acc: 0.9646, best train loss: 0.1107  @ epoch 13376 )\n",
      "[Epoch: 13600] train loss: 0.1328, train acc: 0.9536, val loss: 0.1985, val acc: 0.9585  (best train acc: 0.9654, best val acc: 0.9646, best train loss: 0.1107  @ epoch 13376 )\n",
      "[Epoch: 13620] train loss: 0.1246, train acc: 0.9570, val loss: 0.1863, val acc: 0.9632  (best train acc: 0.9654, best val acc: 0.9646, best train loss: 0.1107  @ epoch 13376 )\n",
      "[Epoch: 13640] train loss: 0.1216, train acc: 0.9567, val loss: 0.1883, val acc: 0.9585  (best train acc: 0.9654, best val acc: 0.9646, best train loss: 0.1056  @ epoch 13639 )\n",
      "[Epoch: 13660] train loss: 0.1167, train acc: 0.9620, val loss: 0.1833, val acc: 0.9616  (best train acc: 0.9654, best val acc: 0.9646, best train loss: 0.1056  @ epoch 13639 )\n",
      "[Epoch: 13680] train loss: 0.1199, train acc: 0.9592, val loss: 0.1780, val acc: 0.9629  (best train acc: 0.9654, best val acc: 0.9646, best train loss: 0.1056  @ epoch 13639 )\n",
      "[Epoch: 13700] train loss: 0.1310, train acc: 0.9569, val loss: 0.1800, val acc: 0.9609  (best train acc: 0.9654, best val acc: 0.9646, best train loss: 0.1056  @ epoch 13639 )\n",
      "[Epoch: 13720] train loss: 0.1258, train acc: 0.9594, val loss: 0.1898, val acc: 0.9555  (best train acc: 0.9654, best val acc: 0.9646, best train loss: 0.1056  @ epoch 13639 )\n",
      "[Epoch: 13740] train loss: 0.1436, train acc: 0.9557, val loss: 0.1966, val acc: 0.9565  (best train acc: 0.9654, best val acc: 0.9646, best train loss: 0.1056  @ epoch 13639 )\n",
      "[Epoch: 13760] train loss: 0.1332, train acc: 0.9568, val loss: 0.1892, val acc: 0.9595  (best train acc: 0.9654, best val acc: 0.9646, best train loss: 0.1056  @ epoch 13639 )\n",
      "[Epoch: 13780] train loss: 0.1203, train acc: 0.9597, val loss: 0.1809, val acc: 0.9609  (best train acc: 0.9654, best val acc: 0.9646, best train loss: 0.1056  @ epoch 13639 )\n",
      "[Epoch: 13800] train loss: 0.1233, train acc: 0.9598, val loss: 0.1821, val acc: 0.9595  (best train acc: 0.9654, best val acc: 0.9646, best train loss: 0.1056  @ epoch 13639 )\n",
      "[Epoch: 13820] train loss: 0.1136, train acc: 0.9628, val loss: 0.1823, val acc: 0.9639  (best train acc: 0.9654, best val acc: 0.9646, best train loss: 0.1056  @ epoch 13639 )\n",
      "[Epoch: 13840] train loss: 0.1191, train acc: 0.9630, val loss: 0.1799, val acc: 0.9622  (best train acc: 0.9654, best val acc: 0.9646, best train loss: 0.1056  @ epoch 13639 )\n",
      "[Epoch: 13860] train loss: 0.1299, train acc: 0.9512, val loss: 0.2021, val acc: 0.9545  (best train acc: 0.9654, best val acc: 0.9646, best train loss: 0.1056  @ epoch 13639 )\n",
      "[Epoch: 13880] train loss: 0.1218, train acc: 0.9620, val loss: 0.1766, val acc: 0.9639  (best train acc: 0.9654, best val acc: 0.9646, best train loss: 0.1056  @ epoch 13639 )\n",
      "[Epoch: 13900] train loss: 0.1366, train acc: 0.9531, val loss: 0.1882, val acc: 0.9568  (best train acc: 0.9654, best val acc: 0.9646, best train loss: 0.1056  @ epoch 13639 )\n",
      "[Epoch: 13920] train loss: 0.1213, train acc: 0.9597, val loss: 0.1894, val acc: 0.9622  (best train acc: 0.9654, best val acc: 0.9646, best train loss: 0.1056  @ epoch 13639 )\n",
      "[Epoch: 13940] train loss: 0.1316, train acc: 0.9526, val loss: 0.1988, val acc: 0.9514  (best train acc: 0.9654, best val acc: 0.9646, best train loss: 0.1056  @ epoch 13639 )\n",
      "[Epoch: 13960] train loss: 0.1180, train acc: 0.9602, val loss: 0.1768, val acc: 0.9605  (best train acc: 0.9654, best val acc: 0.9646, best train loss: 0.1056  @ epoch 13639 )\n",
      "[Epoch: 13980] train loss: 0.1405, train acc: 0.9558, val loss: 0.1918, val acc: 0.9582  (best train acc: 0.9654, best val acc: 0.9646, best train loss: 0.1056  @ epoch 13639 )\n",
      "[Epoch: 14000] train loss: 0.1180, train acc: 0.9602, val loss: 0.1866, val acc: 0.9602  (best train acc: 0.9657, best val acc: 0.9646, best train loss: 0.1056  @ epoch 13639 )\n",
      "[Epoch: 14020] train loss: 0.1231, train acc: 0.9612, val loss: 0.1980, val acc: 0.9592  (best train acc: 0.9657, best val acc: 0.9646, best train loss: 0.1056  @ epoch 13639 )\n",
      "[Epoch: 14040] train loss: 0.1090, train acc: 0.9658, val loss: 0.1888, val acc: 0.9609  (best train acc: 0.9658, best val acc: 0.9646, best train loss: 0.1056  @ epoch 13639 )\n",
      "[Epoch: 14060] train loss: 0.1567, train acc: 0.9518, val loss: 0.2054, val acc: 0.9548  (best train acc: 0.9658, best val acc: 0.9646, best train loss: 0.1056  @ epoch 13639 )\n",
      "[Epoch: 14080] train loss: 0.1213, train acc: 0.9593, val loss: 0.1876, val acc: 0.9622  (best train acc: 0.9658, best val acc: 0.9646, best train loss: 0.1056  @ epoch 13639 )\n",
      "[Epoch: 14100] train loss: 0.1489, train acc: 0.9427, val loss: 0.2068, val acc: 0.9531  (best train acc: 0.9658, best val acc: 0.9646, best train loss: 0.1056  @ epoch 13639 )\n",
      "[Epoch: 14120] train loss: 0.1302, train acc: 0.9571, val loss: 0.1952, val acc: 0.9585  (best train acc: 0.9658, best val acc: 0.9646, best train loss: 0.1056  @ epoch 13639 )\n",
      "[Epoch: 14140] train loss: 0.1349, train acc: 0.9569, val loss: 0.1946, val acc: 0.9582  (best train acc: 0.9658, best val acc: 0.9646, best train loss: 0.1047  @ epoch 14128 )\n",
      "[Epoch: 14160] train loss: 0.1147, train acc: 0.9628, val loss: 0.1806, val acc: 0.9639  (best train acc: 0.9658, best val acc: 0.9649, best train loss: 0.1047  @ epoch 14128 )\n",
      "[Epoch: 14180] train loss: 0.1096, train acc: 0.9659, val loss: 0.1829, val acc: 0.9619  (best train acc: 0.9659, best val acc: 0.9649, best train loss: 0.1047  @ epoch 14128 )\n",
      "[Epoch: 14200] train loss: 0.1165, train acc: 0.9625, val loss: 0.1834, val acc: 0.9619  (best train acc: 0.9659, best val acc: 0.9649, best train loss: 0.1047  @ epoch 14128 )\n",
      "[Epoch: 14220] train loss: 0.1098, train acc: 0.9636, val loss: 0.1801, val acc: 0.9629  (best train acc: 0.9663, best val acc: 0.9649, best train loss: 0.1047  @ epoch 14128 )\n",
      "[Epoch: 14240] train loss: 0.1228, train acc: 0.9619, val loss: 0.1877, val acc: 0.9619  (best train acc: 0.9663, best val acc: 0.9653, best train loss: 0.1047  @ epoch 14128 )\n",
      "[Epoch: 14260] train loss: 0.1536, train acc: 0.9427, val loss: 0.1988, val acc: 0.9481  (best train acc: 0.9663, best val acc: 0.9653, best train loss: 0.1047  @ epoch 14128 )\n",
      "[Epoch: 14280] train loss: 0.1337, train acc: 0.9580, val loss: 0.1965, val acc: 0.9504  (best train acc: 0.9663, best val acc: 0.9653, best train loss: 0.1047  @ epoch 14128 )\n",
      "[Epoch: 14300] train loss: 0.1230, train acc: 0.9593, val loss: 0.1908, val acc: 0.9585  (best train acc: 0.9663, best val acc: 0.9653, best train loss: 0.1047  @ epoch 14128 )\n",
      "[Epoch: 14320] train loss: 0.1205, train acc: 0.9619, val loss: 0.1869, val acc: 0.9592  (best train acc: 0.9663, best val acc: 0.9653, best train loss: 0.1047  @ epoch 14128 )\n",
      "[Epoch: 14340] train loss: 0.1235, train acc: 0.9571, val loss: 0.2051, val acc: 0.9558  (best train acc: 0.9663, best val acc: 0.9653, best train loss: 0.1047  @ epoch 14128 )\n",
      "[Epoch: 14360] train loss: 0.1133, train acc: 0.9628, val loss: 0.1811, val acc: 0.9636  (best train acc: 0.9663, best val acc: 0.9653, best train loss: 0.1047  @ epoch 14128 )\n",
      "[Epoch: 14380] train loss: 0.1111, train acc: 0.9646, val loss: 0.1787, val acc: 0.9626  (best train acc: 0.9663, best val acc: 0.9653, best train loss: 0.1047  @ epoch 14128 )\n",
      "[Epoch: 14400] train loss: 0.1163, train acc: 0.9619, val loss: 0.1809, val acc: 0.9616  (best train acc: 0.9665, best val acc: 0.9653, best train loss: 0.1047  @ epoch 14128 )\n",
      "[Epoch: 14420] train loss: 0.1122, train acc: 0.9644, val loss: 0.1810, val acc: 0.9626  (best train acc: 0.9665, best val acc: 0.9653, best train loss: 0.1047  @ epoch 14128 )\n",
      "[Epoch: 14440] train loss: 0.1137, train acc: 0.9643, val loss: 0.1807, val acc: 0.9629  (best train acc: 0.9665, best val acc: 0.9653, best train loss: 0.1047  @ epoch 14128 )\n",
      "[Epoch: 14460] train loss: 0.1122, train acc: 0.9628, val loss: 0.1786, val acc: 0.9616  (best train acc: 0.9665, best val acc: 0.9653, best train loss: 0.1047  @ epoch 14128 )\n",
      "[Epoch: 14480] train loss: 0.1279, train acc: 0.9581, val loss: 0.1848, val acc: 0.9622  (best train acc: 0.9665, best val acc: 0.9653, best train loss: 0.1047  @ epoch 14128 )\n",
      "[Epoch: 14500] train loss: 0.1216, train acc: 0.9607, val loss: 0.2183, val acc: 0.9568  (best train acc: 0.9665, best val acc: 0.9653, best train loss: 0.1047  @ epoch 14128 )\n",
      "[Epoch: 14520] train loss: 0.1513, train acc: 0.9526, val loss: 0.1955, val acc: 0.9578  (best train acc: 0.9665, best val acc: 0.9653, best train loss: 0.1047  @ epoch 14128 )\n",
      "[Epoch: 14540] train loss: 0.1251, train acc: 0.9568, val loss: 0.1779, val acc: 0.9616  (best train acc: 0.9665, best val acc: 0.9653, best train loss: 0.1047  @ epoch 14128 )\n",
      "[Epoch: 14560] train loss: 0.1253, train acc: 0.9590, val loss: 0.1750, val acc: 0.9616  (best train acc: 0.9665, best val acc: 0.9653, best train loss: 0.1047  @ epoch 14128 )\n",
      "[Epoch: 14580] train loss: 0.1132, train acc: 0.9629, val loss: 0.1821, val acc: 0.9632  (best train acc: 0.9665, best val acc: 0.9653, best train loss: 0.1047  @ epoch 14128 )\n",
      "[Epoch: 14600] train loss: 0.1112, train acc: 0.9628, val loss: 0.1842, val acc: 0.9622  (best train acc: 0.9665, best val acc: 0.9653, best train loss: 0.1047  @ epoch 14128 )\n",
      "[Epoch: 14620] train loss: 0.1246, train acc: 0.9611, val loss: 0.1945, val acc: 0.9558  (best train acc: 0.9665, best val acc: 0.9653, best train loss: 0.1037  @ epoch 14610 )\n",
      "[Epoch: 14640] train loss: 0.1559, train acc: 0.9453, val loss: 0.1913, val acc: 0.9501  (best train acc: 0.9665, best val acc: 0.9653, best train loss: 0.1037  @ epoch 14610 )\n",
      "[Epoch: 14660] train loss: 0.1246, train acc: 0.9588, val loss: 0.1787, val acc: 0.9636  (best train acc: 0.9665, best val acc: 0.9653, best train loss: 0.1037  @ epoch 14610 )\n",
      "[Epoch: 14680] train loss: 0.1302, train acc: 0.9606, val loss: 0.1792, val acc: 0.9616  (best train acc: 0.9665, best val acc: 0.9653, best train loss: 0.1037  @ epoch 14610 )\n",
      "[Epoch: 14700] train loss: 0.1113, train acc: 0.9631, val loss: 0.1793, val acc: 0.9636  (best train acc: 0.9665, best val acc: 0.9653, best train loss: 0.1037  @ epoch 14610 )\n",
      "[Epoch: 14720] train loss: 0.1266, train acc: 0.9547, val loss: 0.1770, val acc: 0.9612  (best train acc: 0.9665, best val acc: 0.9653, best train loss: 0.1037  @ epoch 14610 )\n",
      "[Epoch: 14740] train loss: 0.1217, train acc: 0.9569, val loss: 0.1785, val acc: 0.9605  (best train acc: 0.9665, best val acc: 0.9653, best train loss: 0.1037  @ epoch 14610 )\n",
      "[Epoch: 14760] train loss: 0.1159, train acc: 0.9602, val loss: 0.1839, val acc: 0.9612  (best train acc: 0.9678, best val acc: 0.9653, best train loss: 0.1037  @ epoch 14610 )\n",
      "[Epoch: 14780] train loss: 0.1145, train acc: 0.9619, val loss: 0.1856, val acc: 0.9616  (best train acc: 0.9678, best val acc: 0.9653, best train loss: 0.1037  @ epoch 14610 )\n",
      "[Epoch: 14800] train loss: 0.1196, train acc: 0.9611, val loss: 0.1807, val acc: 0.9592  (best train acc: 0.9678, best val acc: 0.9653, best train loss: 0.1037  @ epoch 14610 )\n",
      "[Epoch: 14820] train loss: 0.1145, train acc: 0.9652, val loss: 0.1921, val acc: 0.9605  (best train acc: 0.9678, best val acc: 0.9653, best train loss: 0.1037  @ epoch 14610 )\n",
      "[Epoch: 14840] train loss: 0.1206, train acc: 0.9662, val loss: 0.1843, val acc: 0.9609  (best train acc: 0.9678, best val acc: 0.9653, best train loss: 0.1037  @ epoch 14610 )\n",
      "[Epoch: 14860] train loss: 0.1120, train acc: 0.9664, val loss: 0.1944, val acc: 0.9612  (best train acc: 0.9678, best val acc: 0.9653, best train loss: 0.1037  @ epoch 14610 )\n",
      "[Epoch: 14880] train loss: 0.1202, train acc: 0.9589, val loss: 0.1931, val acc: 0.9602  (best train acc: 0.9678, best val acc: 0.9653, best train loss: 0.1037  @ epoch 14610 )\n",
      "[Epoch: 14900] train loss: 0.1063, train acc: 0.9651, val loss: 0.2182, val acc: 0.9562  (best train acc: 0.9678, best val acc: 0.9653, best train loss: 0.1037  @ epoch 14610 )\n",
      "[Epoch: 14920] train loss: 0.1221, train acc: 0.9601, val loss: 0.1882, val acc: 0.9585  (best train acc: 0.9678, best val acc: 0.9653, best train loss: 0.1037  @ epoch 14610 )\n",
      "[Epoch: 14940] train loss: 0.1173, train acc: 0.9607, val loss: 0.1803, val acc: 0.9639  (best train acc: 0.9678, best val acc: 0.9653, best train loss: 0.1037  @ epoch 14610 )\n",
      "[Epoch: 14960] train loss: 0.1685, train acc: 0.9406, val loss: 0.1940, val acc: 0.9551  (best train acc: 0.9678, best val acc: 0.9653, best train loss: 0.1037  @ epoch 14610 )\n",
      "[Epoch: 14980] train loss: 0.1207, train acc: 0.9599, val loss: 0.1938, val acc: 0.9592  (best train acc: 0.9678, best val acc: 0.9653, best train loss: 0.1037  @ epoch 14610 )\n",
      "[Epoch: 15000] train loss: 0.1069, train acc: 0.9646, val loss: 0.1823, val acc: 0.9599  (best train acc: 0.9678, best val acc: 0.9653, best train loss: 0.1037  @ epoch 14610 )\n",
      "[Epoch: 15020] train loss: 0.1113, train acc: 0.9632, val loss: 0.1776, val acc: 0.9649  (best train acc: 0.9678, best val acc: 0.9653, best train loss: 0.1037  @ epoch 14610 )\n",
      "[Epoch: 15040] train loss: 0.1112, train acc: 0.9638, val loss: 0.1845, val acc: 0.9622  (best train acc: 0.9678, best val acc: 0.9653, best train loss: 0.1029  @ epoch 15036 )\n",
      "[Epoch: 15060] train loss: 0.1085, train acc: 0.9642, val loss: 0.1783, val acc: 0.9609  (best train acc: 0.9678, best val acc: 0.9653, best train loss: 0.1029  @ epoch 15036 )\n",
      "[Epoch: 15080] train loss: 0.1163, train acc: 0.9616, val loss: 0.1821, val acc: 0.9636  (best train acc: 0.9678, best val acc: 0.9653, best train loss: 0.1014  @ epoch 15072 )\n",
      "[Epoch: 15100] train loss: 0.1130, train acc: 0.9653, val loss: 0.1872, val acc: 0.9629  (best train acc: 0.9678, best val acc: 0.9653, best train loss: 0.1014  @ epoch 15072 )\n",
      "[Epoch: 15120] train loss: 0.1034, train acc: 0.9666, val loss: 0.1874, val acc: 0.9649  (best train acc: 0.9678, best val acc: 0.9656, best train loss: 0.1014  @ epoch 15072 )\n",
      "[Epoch: 15140] train loss: 0.1247, train acc: 0.9568, val loss: 0.1861, val acc: 0.9609  (best train acc: 0.9693, best val acc: 0.9656, best train loss: 0.0993  @ epoch 15121 )\n",
      "[Epoch: 15160] train loss: 0.1291, train acc: 0.9533, val loss: 0.2228, val acc: 0.9518  (best train acc: 0.9693, best val acc: 0.9656, best train loss: 0.0993  @ epoch 15121 )\n",
      "[Epoch: 15180] train loss: 0.1206, train acc: 0.9608, val loss: 0.1797, val acc: 0.9599  (best train acc: 0.9693, best val acc: 0.9656, best train loss: 0.0993  @ epoch 15121 )\n",
      "[Epoch: 15200] train loss: 0.1216, train acc: 0.9616, val loss: 0.1705, val acc: 0.9619  (best train acc: 0.9693, best val acc: 0.9656, best train loss: 0.0993  @ epoch 15121 )\n",
      "[Epoch: 15220] train loss: 0.1119, train acc: 0.9613, val loss: 0.1827, val acc: 0.9632  (best train acc: 0.9693, best val acc: 0.9656, best train loss: 0.0993  @ epoch 15121 )\n",
      "[Epoch: 15240] train loss: 0.1099, train acc: 0.9654, val loss: 0.1752, val acc: 0.9653  (best train acc: 0.9693, best val acc: 0.9656, best train loss: 0.0993  @ epoch 15121 )\n",
      "[Epoch: 15260] train loss: 0.1292, train acc: 0.9570, val loss: 0.1979, val acc: 0.9528  (best train acc: 0.9693, best val acc: 0.9656, best train loss: 0.0993  @ epoch 15121 )\n",
      "[Epoch: 15280] train loss: 0.1533, train acc: 0.9518, val loss: 0.2306, val acc: 0.9410  (best train acc: 0.9693, best val acc: 0.9656, best train loss: 0.0993  @ epoch 15121 )\n",
      "[Epoch: 15300] train loss: 0.1210, train acc: 0.9543, val loss: 0.1720, val acc: 0.9619  (best train acc: 0.9693, best val acc: 0.9656, best train loss: 0.0993  @ epoch 15121 )\n",
      "[Epoch: 15320] train loss: 0.1120, train acc: 0.9633, val loss: 0.2108, val acc: 0.9514  (best train acc: 0.9693, best val acc: 0.9656, best train loss: 0.0993  @ epoch 15121 )\n",
      "[Epoch: 15340] train loss: 0.1095, train acc: 0.9643, val loss: 0.1844, val acc: 0.9626  (best train acc: 0.9693, best val acc: 0.9656, best train loss: 0.0993  @ epoch 15121 )\n",
      "[Epoch: 15360] train loss: 0.1122, train acc: 0.9609, val loss: 0.1884, val acc: 0.9632  (best train acc: 0.9693, best val acc: 0.9659, best train loss: 0.0993  @ epoch 15121 )\n",
      "[Epoch: 15380] train loss: 0.1055, train acc: 0.9649, val loss: 0.1762, val acc: 0.9612  (best train acc: 0.9693, best val acc: 0.9663, best train loss: 0.0993  @ epoch 15121 )\n",
      "[Epoch: 15400] train loss: 0.1042, train acc: 0.9649, val loss: 0.1749, val acc: 0.9642  (best train acc: 0.9693, best val acc: 0.9663, best train loss: 0.0993  @ epoch 15121 )\n",
      "[Epoch: 15420] train loss: 0.1068, train acc: 0.9666, val loss: 0.1833, val acc: 0.9626  (best train acc: 0.9693, best val acc: 0.9663, best train loss: 0.0966  @ epoch 15415 )\n",
      "[Epoch: 15440] train loss: 0.1137, train acc: 0.9626, val loss: 0.1782, val acc: 0.9602  (best train acc: 0.9693, best val acc: 0.9663, best train loss: 0.0966  @ epoch 15415 )\n",
      "[Epoch: 15460] train loss: 0.1150, train acc: 0.9625, val loss: 0.1788, val acc: 0.9629  (best train acc: 0.9693, best val acc: 0.9663, best train loss: 0.0966  @ epoch 15415 )\n",
      "[Epoch: 15480] train loss: 0.1351, train acc: 0.9523, val loss: 0.2006, val acc: 0.9545  (best train acc: 0.9693, best val acc: 0.9663, best train loss: 0.0966  @ epoch 15415 )\n",
      "[Epoch: 15500] train loss: 0.2095, train acc: 0.9445, val loss: 0.1889, val acc: 0.9538  (best train acc: 0.9693, best val acc: 0.9663, best train loss: 0.0966  @ epoch 15415 )\n",
      "[Epoch: 15520] train loss: 0.1055, train acc: 0.9657, val loss: 0.1881, val acc: 0.9595  (best train acc: 0.9693, best val acc: 0.9663, best train loss: 0.0966  @ epoch 15415 )\n",
      "[Epoch: 15540] train loss: 0.1080, train acc: 0.9652, val loss: 0.1854, val acc: 0.9646  (best train acc: 0.9693, best val acc: 0.9663, best train loss: 0.0966  @ epoch 15415 )\n",
      "[Epoch: 15560] train loss: 0.1116, train acc: 0.9650, val loss: 0.1825, val acc: 0.9626  (best train acc: 0.9693, best val acc: 0.9663, best train loss: 0.0966  @ epoch 15415 )\n",
      "[Epoch: 15580] train loss: 0.1349, train acc: 0.9501, val loss: 0.1801, val acc: 0.9609  (best train acc: 0.9693, best val acc: 0.9663, best train loss: 0.0966  @ epoch 15415 )\n",
      "[Epoch: 15600] train loss: 0.1054, train acc: 0.9664, val loss: 0.1722, val acc: 0.9653  (best train acc: 0.9693, best val acc: 0.9663, best train loss: 0.0966  @ epoch 15415 )\n",
      "[Epoch: 15620] train loss: 0.1021, train acc: 0.9672, val loss: 0.1822, val acc: 0.9622  (best train acc: 0.9693, best val acc: 0.9663, best train loss: 0.0966  @ epoch 15415 )\n",
      "[Epoch: 15640] train loss: 0.1061, train acc: 0.9661, val loss: 0.1869, val acc: 0.9619  (best train acc: 0.9693, best val acc: 0.9663, best train loss: 0.0966  @ epoch 15415 )\n",
      "[Epoch: 15660] train loss: 0.1073, train acc: 0.9694, val loss: 0.1798, val acc: 0.9646  (best train acc: 0.9694, best val acc: 0.9663, best train loss: 0.0966  @ epoch 15415 )\n",
      "[Epoch: 15680] train loss: 0.1048, train acc: 0.9664, val loss: 0.1757, val acc: 0.9639  (best train acc: 0.9694, best val acc: 0.9663, best train loss: 0.0966  @ epoch 15415 )\n",
      "[Epoch: 15700] train loss: 0.1137, train acc: 0.9641, val loss: 0.1908, val acc: 0.9578  (best train acc: 0.9696, best val acc: 0.9663, best train loss: 0.0963  @ epoch 15696 )\n",
      "[Epoch: 15720] train loss: 0.1105, train acc: 0.9665, val loss: 0.1744, val acc: 0.9626  (best train acc: 0.9696, best val acc: 0.9663, best train loss: 0.0963  @ epoch 15696 )\n",
      "[Epoch: 15740] train loss: 0.1170, train acc: 0.9597, val loss: 0.2112, val acc: 0.9541  (best train acc: 0.9696, best val acc: 0.9663, best train loss: 0.0963  @ epoch 15696 )\n",
      "[Epoch: 15760] train loss: 0.1266, train acc: 0.9577, val loss: 0.1727, val acc: 0.9649  (best train acc: 0.9696, best val acc: 0.9663, best train loss: 0.0963  @ epoch 15696 )\n",
      "[Epoch: 15780] train loss: 0.1241, train acc: 0.9593, val loss: 0.1730, val acc: 0.9619  (best train acc: 0.9696, best val acc: 0.9663, best train loss: 0.0963  @ epoch 15696 )\n",
      "[Epoch: 15800] train loss: 0.1123, train acc: 0.9623, val loss: 0.1769, val acc: 0.9632  (best train acc: 0.9696, best val acc: 0.9663, best train loss: 0.0963  @ epoch 15696 )\n",
      "[Epoch: 15820] train loss: 0.1076, train acc: 0.9649, val loss: 0.1875, val acc: 0.9582  (best train acc: 0.9696, best val acc: 0.9663, best train loss: 0.0963  @ epoch 15814 )\n",
      "[Epoch: 15840] train loss: 0.1097, train acc: 0.9633, val loss: 0.1898, val acc: 0.9585  (best train acc: 0.9696, best val acc: 0.9663, best train loss: 0.0963  @ epoch 15814 )\n",
      "[Epoch: 15860] train loss: 0.1051, train acc: 0.9618, val loss: 0.2043, val acc: 0.9531  (best train acc: 0.9696, best val acc: 0.9663, best train loss: 0.0963  @ epoch 15814 )\n",
      "[Epoch: 15880] train loss: 0.1198, train acc: 0.9604, val loss: 0.1769, val acc: 0.9605  (best train acc: 0.9696, best val acc: 0.9663, best train loss: 0.0963  @ epoch 15814 )\n",
      "[Epoch: 15900] train loss: 0.1128, train acc: 0.9652, val loss: 0.1868, val acc: 0.9616  (best train acc: 0.9696, best val acc: 0.9663, best train loss: 0.0963  @ epoch 15814 )\n",
      "[Epoch: 15920] train loss: 0.1063, train acc: 0.9656, val loss: 0.1791, val acc: 0.9592  (best train acc: 0.9696, best val acc: 0.9663, best train loss: 0.0963  @ epoch 15814 )\n",
      "[Epoch: 15940] train loss: 0.1235, train acc: 0.9568, val loss: 0.1817, val acc: 0.9582  (best train acc: 0.9696, best val acc: 0.9663, best train loss: 0.0963  @ epoch 15814 )\n",
      "[Epoch: 15960] train loss: 0.1042, train acc: 0.9644, val loss: 0.1778, val acc: 0.9629  (best train acc: 0.9696, best val acc: 0.9663, best train loss: 0.0963  @ epoch 15814 )\n",
      "[Epoch: 15980] train loss: 0.0991, train acc: 0.9676, val loss: 0.1716, val acc: 0.9663  (best train acc: 0.9696, best val acc: 0.9663, best train loss: 0.0963  @ epoch 15814 )\n",
      "[Epoch: 16000] train loss: 0.1193, train acc: 0.9612, val loss: 0.1850, val acc: 0.9585  (best train acc: 0.9696, best val acc: 0.9666, best train loss: 0.0963  @ epoch 15814 )\n",
      "[Epoch: 16020] train loss: 0.1050, train acc: 0.9668, val loss: 0.2050, val acc: 0.9568  (best train acc: 0.9696, best val acc: 0.9666, best train loss: 0.0963  @ epoch 15814 )\n",
      "[Epoch: 16040] train loss: 0.1022, train acc: 0.9683, val loss: 0.1712, val acc: 0.9653  (best train acc: 0.9696, best val acc: 0.9666, best train loss: 0.0963  @ epoch 15814 )\n",
      "[Epoch: 16060] train loss: 0.1101, train acc: 0.9648, val loss: 0.1779, val acc: 0.9646  (best train acc: 0.9696, best val acc: 0.9666, best train loss: 0.0963  @ epoch 15814 )\n",
      "[Epoch: 16080] train loss: 0.1276, train acc: 0.9579, val loss: 0.1954, val acc: 0.9558  (best train acc: 0.9696, best val acc: 0.9666, best train loss: 0.0963  @ epoch 15814 )\n",
      "[Epoch: 16100] train loss: 0.1310, train acc: 0.9511, val loss: 0.1822, val acc: 0.9585  (best train acc: 0.9696, best val acc: 0.9666, best train loss: 0.0963  @ epoch 15814 )\n",
      "[Epoch: 16120] train loss: 0.1133, train acc: 0.9644, val loss: 0.1743, val acc: 0.9656  (best train acc: 0.9696, best val acc: 0.9666, best train loss: 0.0963  @ epoch 15814 )\n",
      "[Epoch: 16140] train loss: 0.1036, train acc: 0.9656, val loss: 0.1753, val acc: 0.9639  (best train acc: 0.9696, best val acc: 0.9666, best train loss: 0.0963  @ epoch 15814 )\n",
      "[Epoch: 16160] train loss: 0.1059, train acc: 0.9631, val loss: 0.1749, val acc: 0.9626  (best train acc: 0.9696, best val acc: 0.9666, best train loss: 0.0963  @ epoch 15814 )\n",
      "[Epoch: 16180] train loss: 0.1024, train acc: 0.9676, val loss: 0.1877, val acc: 0.9589  (best train acc: 0.9696, best val acc: 0.9666, best train loss: 0.0963  @ epoch 15814 )\n",
      "[Epoch: 16200] train loss: 0.1022, train acc: 0.9657, val loss: 0.1791, val acc: 0.9605  (best train acc: 0.9696, best val acc: 0.9666, best train loss: 0.0963  @ epoch 15814 )\n",
      "[Epoch: 16220] train loss: 0.1001, train acc: 0.9683, val loss: 0.1768, val acc: 0.9636  (best train acc: 0.9700, best val acc: 0.9666, best train loss: 0.0918  @ epoch 16209 )\n",
      "[Epoch: 16240] train loss: 0.1010, train acc: 0.9675, val loss: 0.1790, val acc: 0.9619  (best train acc: 0.9700, best val acc: 0.9666, best train loss: 0.0918  @ epoch 16209 )\n",
      "[Epoch: 16260] train loss: 0.1062, train acc: 0.9647, val loss: 0.1791, val acc: 0.9629  (best train acc: 0.9700, best val acc: 0.9666, best train loss: 0.0918  @ epoch 16209 )\n",
      "[Epoch: 16280] train loss: 0.1116, train acc: 0.9639, val loss: 0.1797, val acc: 0.9632  (best train acc: 0.9700, best val acc: 0.9666, best train loss: 0.0918  @ epoch 16209 )\n",
      "[Epoch: 16300] train loss: 0.1176, train acc: 0.9618, val loss: 0.1949, val acc: 0.9551  (best train acc: 0.9700, best val acc: 0.9666, best train loss: 0.0918  @ epoch 16209 )\n",
      "[Epoch: 16320] train loss: 0.1050, train acc: 0.9651, val loss: 0.2131, val acc: 0.9541  (best train acc: 0.9700, best val acc: 0.9666, best train loss: 0.0918  @ epoch 16209 )\n",
      "[Epoch: 16340] train loss: 0.1294, train acc: 0.9562, val loss: 0.1957, val acc: 0.9551  (best train acc: 0.9700, best val acc: 0.9666, best train loss: 0.0918  @ epoch 16209 )\n",
      "[Epoch: 16360] train loss: 0.1149, train acc: 0.9608, val loss: 0.1764, val acc: 0.9575  (best train acc: 0.9700, best val acc: 0.9666, best train loss: 0.0918  @ epoch 16209 )\n",
      "[Epoch: 16380] train loss: 0.1142, train acc: 0.9632, val loss: 0.1819, val acc: 0.9609  (best train acc: 0.9700, best val acc: 0.9666, best train loss: 0.0918  @ epoch 16209 )\n",
      "[Epoch: 16400] train loss: 0.1067, train acc: 0.9661, val loss: 0.1813, val acc: 0.9629  (best train acc: 0.9700, best val acc: 0.9676, best train loss: 0.0918  @ epoch 16209 )\n",
      "[Epoch: 16420] train loss: 0.1311, train acc: 0.9605, val loss: 0.1786, val acc: 0.9595  (best train acc: 0.9700, best val acc: 0.9676, best train loss: 0.0918  @ epoch 16209 )\n",
      "[Epoch: 16440] train loss: 0.1149, train acc: 0.9647, val loss: 0.1748, val acc: 0.9572  (best train acc: 0.9700, best val acc: 0.9676, best train loss: 0.0918  @ epoch 16209 )\n",
      "[Epoch: 16460] train loss: 0.1017, train acc: 0.9674, val loss: 0.1697, val acc: 0.9653  (best train acc: 0.9700, best val acc: 0.9676, best train loss: 0.0918  @ epoch 16209 )\n",
      "[Epoch: 16480] train loss: 0.1098, train acc: 0.9631, val loss: 0.1737, val acc: 0.9636  (best train acc: 0.9700, best val acc: 0.9676, best train loss: 0.0918  @ epoch 16209 )\n",
      "[Epoch: 16500] train loss: 0.1103, train acc: 0.9605, val loss: 0.1756, val acc: 0.9646  (best train acc: 0.9700, best val acc: 0.9676, best train loss: 0.0918  @ epoch 16209 )\n",
      "[Epoch: 16520] train loss: 0.1053, train acc: 0.9669, val loss: 0.1793, val acc: 0.9626  (best train acc: 0.9702, best val acc: 0.9676, best train loss: 0.0915  @ epoch 16509 )\n",
      "[Epoch: 16540] train loss: 0.0990, train acc: 0.9680, val loss: 0.1779, val acc: 0.9636  (best train acc: 0.9702, best val acc: 0.9676, best train loss: 0.0915  @ epoch 16509 )\n",
      "[Epoch: 16560] train loss: 0.1089, train acc: 0.9636, val loss: 0.1708, val acc: 0.9629  (best train acc: 0.9702, best val acc: 0.9676, best train loss: 0.0915  @ epoch 16509 )\n",
      "[Epoch: 16580] train loss: 0.1005, train acc: 0.9670, val loss: 0.1689, val acc: 0.9626  (best train acc: 0.9702, best val acc: 0.9676, best train loss: 0.0915  @ epoch 16509 )\n",
      "[Epoch: 16600] train loss: 0.1077, train acc: 0.9636, val loss: 0.1697, val acc: 0.9636  (best train acc: 0.9708, best val acc: 0.9676, best train loss: 0.0915  @ epoch 16509 )\n",
      "[Epoch: 16620] train loss: 0.1023, train acc: 0.9677, val loss: 0.1798, val acc: 0.9609  (best train acc: 0.9708, best val acc: 0.9676, best train loss: 0.0915  @ epoch 16509 )\n",
      "[Epoch: 16640] train loss: 0.1101, train acc: 0.9659, val loss: 0.1710, val acc: 0.9649  (best train acc: 0.9708, best val acc: 0.9676, best train loss: 0.0915  @ epoch 16509 )\n",
      "[Epoch: 16660] train loss: 0.1105, train acc: 0.9626, val loss: 0.1819, val acc: 0.9609  (best train acc: 0.9708, best val acc: 0.9676, best train loss: 0.0915  @ epoch 16509 )\n",
      "[Epoch: 16680] train loss: 0.1148, train acc: 0.9647, val loss: 0.1831, val acc: 0.9609  (best train acc: 0.9708, best val acc: 0.9676, best train loss: 0.0915  @ epoch 16509 )\n",
      "[Epoch: 16700] train loss: 0.1101, train acc: 0.9601, val loss: 0.1957, val acc: 0.9558  (best train acc: 0.9708, best val acc: 0.9676, best train loss: 0.0915  @ epoch 16509 )\n",
      "[Epoch: 16720] train loss: 0.1401, train acc: 0.9520, val loss: 0.1844, val acc: 0.9568  (best train acc: 0.9708, best val acc: 0.9676, best train loss: 0.0915  @ epoch 16509 )\n",
      "[Epoch: 16740] train loss: 0.1034, train acc: 0.9649, val loss: 0.1712, val acc: 0.9602  (best train acc: 0.9708, best val acc: 0.9676, best train loss: 0.0915  @ epoch 16509 )\n",
      "[Epoch: 16760] train loss: 0.0986, train acc: 0.9668, val loss: 0.1662, val acc: 0.9639  (best train acc: 0.9708, best val acc: 0.9676, best train loss: 0.0915  @ epoch 16509 )\n",
      "[Epoch: 16780] train loss: 0.1148, train acc: 0.9602, val loss: 0.1603, val acc: 0.9649  (best train acc: 0.9708, best val acc: 0.9676, best train loss: 0.0915  @ epoch 16509 )\n",
      "[Epoch: 16800] train loss: 0.1046, train acc: 0.9657, val loss: 0.1611, val acc: 0.9589  (best train acc: 0.9708, best val acc: 0.9676, best train loss: 0.0915  @ epoch 16509 )\n",
      "[Epoch: 16820] train loss: 0.1053, train acc: 0.9654, val loss: 0.1666, val acc: 0.9626  (best train acc: 0.9708, best val acc: 0.9676, best train loss: 0.0915  @ epoch 16509 )\n",
      "[Epoch: 16840] train loss: 0.0981, train acc: 0.9684, val loss: 0.1652, val acc: 0.9636  (best train acc: 0.9708, best val acc: 0.9676, best train loss: 0.0915  @ epoch 16509 )\n",
      "[Epoch: 16860] train loss: 0.1052, train acc: 0.9670, val loss: 0.1791, val acc: 0.9589  (best train acc: 0.9708, best val acc: 0.9676, best train loss: 0.0915  @ epoch 16509 )\n",
      "[Epoch: 16880] train loss: 0.1192, train acc: 0.9644, val loss: 0.1648, val acc: 0.9622  (best train acc: 0.9708, best val acc: 0.9676, best train loss: 0.0915  @ epoch 16509 )\n",
      "[Epoch: 16900] train loss: 0.1014, train acc: 0.9675, val loss: 0.1702, val acc: 0.9626  (best train acc: 0.9708, best val acc: 0.9676, best train loss: 0.0915  @ epoch 16509 )\n",
      "[Epoch: 16920] train loss: 0.1099, train acc: 0.9674, val loss: 0.1792, val acc: 0.9619  (best train acc: 0.9709, best val acc: 0.9676, best train loss: 0.0915  @ epoch 16509 )\n",
      "[Epoch: 16940] train loss: 0.1360, train acc: 0.9549, val loss: 0.2649, val acc: 0.9336  (best train acc: 0.9709, best val acc: 0.9676, best train loss: 0.0915  @ epoch 16509 )\n",
      "[Epoch: 16960] train loss: 0.1172, train acc: 0.9639, val loss: 0.1653, val acc: 0.9622  (best train acc: 0.9709, best val acc: 0.9676, best train loss: 0.0915  @ epoch 16509 )\n",
      "[Epoch: 16980] train loss: 0.1041, train acc: 0.9641, val loss: 0.1660, val acc: 0.9629  (best train acc: 0.9709, best val acc: 0.9676, best train loss: 0.0915  @ epoch 16509 )\n",
      "[Epoch: 17000] train loss: 0.0986, train acc: 0.9689, val loss: 0.1602, val acc: 0.9636  (best train acc: 0.9709, best val acc: 0.9676, best train loss: 0.0915  @ epoch 16509 )\n",
      "[Epoch: 17020] train loss: 0.0990, train acc: 0.9692, val loss: 0.1663, val acc: 0.9639  (best train acc: 0.9709, best val acc: 0.9676, best train loss: 0.0915  @ epoch 16509 )\n",
      "[Epoch: 17040] train loss: 0.1331, train acc: 0.9555, val loss: 0.3178, val acc: 0.9245  (best train acc: 0.9709, best val acc: 0.9676, best train loss: 0.0915  @ epoch 16509 )\n",
      "[Epoch: 17060] train loss: 0.1175, train acc: 0.9620, val loss: 0.1670, val acc: 0.9575  (best train acc: 0.9709, best val acc: 0.9676, best train loss: 0.0915  @ epoch 16509 )\n",
      "[Epoch: 17080] train loss: 0.1166, train acc: 0.9608, val loss: 0.1563, val acc: 0.9639  (best train acc: 0.9709, best val acc: 0.9676, best train loss: 0.0915  @ epoch 16509 )\n",
      "[Epoch: 17100] train loss: 0.1027, train acc: 0.9663, val loss: 0.1642, val acc: 0.9626  (best train acc: 0.9709, best val acc: 0.9676, best train loss: 0.0915  @ epoch 16509 )\n",
      "[Epoch: 17120] train loss: 0.0970, train acc: 0.9688, val loss: 0.1576, val acc: 0.9653  (best train acc: 0.9709, best val acc: 0.9676, best train loss: 0.0915  @ epoch 16509 )\n",
      "[Epoch: 17140] train loss: 0.1179, train acc: 0.9589, val loss: 0.1689, val acc: 0.9605  (best train acc: 0.9709, best val acc: 0.9680, best train loss: 0.0915  @ epoch 16509 )\n",
      "[Epoch: 17160] train loss: 0.1061, train acc: 0.9644, val loss: 0.1674, val acc: 0.9619  (best train acc: 0.9709, best val acc: 0.9680, best train loss: 0.0915  @ epoch 16509 )\n",
      "[Epoch: 17180] train loss: 0.1166, train acc: 0.9603, val loss: 0.1667, val acc: 0.9602  (best train acc: 0.9709, best val acc: 0.9680, best train loss: 0.0890  @ epoch 17176 )\n",
      "[Epoch: 17200] train loss: 0.1104, train acc: 0.9636, val loss: 0.1796, val acc: 0.9599  (best train acc: 0.9709, best val acc: 0.9680, best train loss: 0.0890  @ epoch 17176 )\n",
      "[Epoch: 17220] train loss: 0.0988, train acc: 0.9659, val loss: 0.1803, val acc: 0.9589  (best train acc: 0.9709, best val acc: 0.9680, best train loss: 0.0890  @ epoch 17176 )\n",
      "[Epoch: 17240] train loss: 0.1007, train acc: 0.9669, val loss: 0.1620, val acc: 0.9612  (best train acc: 0.9709, best val acc: 0.9680, best train loss: 0.0890  @ epoch 17176 )\n",
      "[Epoch: 17260] train loss: 0.1023, train acc: 0.9665, val loss: 0.1629, val acc: 0.9649  (best train acc: 0.9709, best val acc: 0.9680, best train loss: 0.0890  @ epoch 17176 )\n",
      "[Epoch: 17280] train loss: 0.1061, train acc: 0.9651, val loss: 0.1613, val acc: 0.9676  (best train acc: 0.9710, best val acc: 0.9680, best train loss: 0.0890  @ epoch 17176 )\n",
      "[Epoch: 17300] train loss: 0.1061, train acc: 0.9664, val loss: 0.1570, val acc: 0.9632  (best train acc: 0.9710, best val acc: 0.9680, best train loss: 0.0890  @ epoch 17176 )\n",
      "[Epoch: 17320] train loss: 0.0995, train acc: 0.9662, val loss: 0.1688, val acc: 0.9642  (best train acc: 0.9710, best val acc: 0.9680, best train loss: 0.0890  @ epoch 17176 )\n",
      "[Epoch: 17340] train loss: 0.1028, train acc: 0.9667, val loss: 0.1692, val acc: 0.9619  (best train acc: 0.9710, best val acc: 0.9680, best train loss: 0.0890  @ epoch 17176 )\n",
      "[Epoch: 17360] train loss: 0.1419, train acc: 0.9541, val loss: 0.1984, val acc: 0.9501  (best train acc: 0.9710, best val acc: 0.9680, best train loss: 0.0890  @ epoch 17176 )\n",
      "[Epoch: 17380] train loss: 0.1149, train acc: 0.9615, val loss: 0.1712, val acc: 0.9592  (best train acc: 0.9710, best val acc: 0.9680, best train loss: 0.0890  @ epoch 17176 )\n",
      "[Epoch: 17400] train loss: 0.1023, train acc: 0.9668, val loss: 0.1646, val acc: 0.9642  (best train acc: 0.9710, best val acc: 0.9680, best train loss: 0.0890  @ epoch 17176 )\n",
      "[Epoch: 17420] train loss: 0.1205, train acc: 0.9618, val loss: 0.1621, val acc: 0.9609  (best train acc: 0.9710, best val acc: 0.9680, best train loss: 0.0890  @ epoch 17176 )\n",
      "[Epoch: 17440] train loss: 0.1031, train acc: 0.9654, val loss: 0.1453, val acc: 0.9649  (best train acc: 0.9710, best val acc: 0.9680, best train loss: 0.0890  @ epoch 17176 )\n",
      "[Epoch: 17460] train loss: 0.1018, train acc: 0.9649, val loss: 0.1677, val acc: 0.9626  (best train acc: 0.9718, best val acc: 0.9680, best train loss: 0.0890  @ epoch 17176 )\n",
      "[Epoch: 17480] train loss: 0.1289, train acc: 0.9588, val loss: 0.1536, val acc: 0.9578  (best train acc: 0.9718, best val acc: 0.9680, best train loss: 0.0890  @ epoch 17176 )\n",
      "[Epoch: 17500] train loss: 0.1100, train acc: 0.9652, val loss: 0.1790, val acc: 0.9649  (best train acc: 0.9718, best val acc: 0.9680, best train loss: 0.0890  @ epoch 17176 )\n",
      "[Epoch: 17520] train loss: 0.1104, train acc: 0.9636, val loss: 0.1795, val acc: 0.9572  (best train acc: 0.9718, best val acc: 0.9680, best train loss: 0.0890  @ epoch 17176 )\n",
      "[Epoch: 17540] train loss: 0.0994, train acc: 0.9670, val loss: 0.1521, val acc: 0.9663  (best train acc: 0.9718, best val acc: 0.9680, best train loss: 0.0890  @ epoch 17176 )\n",
      "[Epoch: 17560] train loss: 0.0940, train acc: 0.9690, val loss: 0.1655, val acc: 0.9626  (best train acc: 0.9718, best val acc: 0.9680, best train loss: 0.0890  @ epoch 17176 )\n",
      "[Epoch: 17580] train loss: 0.1193, train acc: 0.9558, val loss: 0.1806, val acc: 0.9555  (best train acc: 0.9718, best val acc: 0.9680, best train loss: 0.0890  @ epoch 17176 )\n",
      "[Epoch: 17600] train loss: 0.1000, train acc: 0.9678, val loss: 0.1648, val acc: 0.9622  (best train acc: 0.9718, best val acc: 0.9680, best train loss: 0.0890  @ epoch 17176 )\n",
      "[Epoch: 17620] train loss: 0.0974, train acc: 0.9664, val loss: 0.1704, val acc: 0.9612  (best train acc: 0.9718, best val acc: 0.9680, best train loss: 0.0890  @ epoch 17176 )\n",
      "[Epoch: 17640] train loss: 0.0981, train acc: 0.9656, val loss: 0.1859, val acc: 0.9545  (best train acc: 0.9718, best val acc: 0.9680, best train loss: 0.0890  @ epoch 17176 )\n",
      "[Epoch: 17660] train loss: 0.1014, train acc: 0.9649, val loss: 0.1651, val acc: 0.9636  (best train acc: 0.9718, best val acc: 0.9680, best train loss: 0.0890  @ epoch 17176 )\n",
      "[Epoch: 17680] train loss: 0.0972, train acc: 0.9644, val loss: 0.1659, val acc: 0.9632  (best train acc: 0.9718, best val acc: 0.9680, best train loss: 0.0890  @ epoch 17176 )\n",
      "[Epoch: 17700] train loss: 0.1938, train acc: 0.9346, val loss: 0.2019, val acc: 0.9379  (best train acc: 0.9718, best val acc: 0.9680, best train loss: 0.0890  @ epoch 17176 )\n",
      "[Epoch: 17720] train loss: 0.1270, train acc: 0.9589, val loss: 0.1630, val acc: 0.9609  (best train acc: 0.9718, best val acc: 0.9680, best train loss: 0.0890  @ epoch 17176 )\n",
      "[Epoch: 17740] train loss: 0.1039, train acc: 0.9647, val loss: 0.1421, val acc: 0.9646  (best train acc: 0.9718, best val acc: 0.9680, best train loss: 0.0890  @ epoch 17176 )\n",
      "[Epoch: 17760] train loss: 0.1048, train acc: 0.9665, val loss: 0.1718, val acc: 0.9585  (best train acc: 0.9718, best val acc: 0.9680, best train loss: 0.0890  @ epoch 17176 )\n",
      "[Epoch: 17780] train loss: 0.1185, train acc: 0.9587, val loss: 0.1642, val acc: 0.9612  (best train acc: 0.9718, best val acc: 0.9680, best train loss: 0.0890  @ epoch 17176 )\n",
      "[Epoch: 17800] train loss: 0.0961, train acc: 0.9685, val loss: 0.1670, val acc: 0.9639  (best train acc: 0.9718, best val acc: 0.9680, best train loss: 0.0890  @ epoch 17176 )\n",
      "[Epoch: 17820] train loss: 0.0999, train acc: 0.9680, val loss: 0.1541, val acc: 0.9599  (best train acc: 0.9718, best val acc: 0.9680, best train loss: 0.0890  @ epoch 17176 )\n",
      "[Epoch: 17840] train loss: 0.1092, train acc: 0.9678, val loss: 0.1668, val acc: 0.9622  (best train acc: 0.9718, best val acc: 0.9680, best train loss: 0.0861  @ epoch 17825 )\n",
      "[Epoch: 17860] train loss: 0.0987, train acc: 0.9672, val loss: 0.1668, val acc: 0.9642  (best train acc: 0.9718, best val acc: 0.9680, best train loss: 0.0861  @ epoch 17825 )\n",
      "[Epoch: 17880] train loss: 0.1025, train acc: 0.9680, val loss: 0.1538, val acc: 0.9642  (best train acc: 0.9718, best val acc: 0.9680, best train loss: 0.0861  @ epoch 17825 )\n",
      "[Epoch: 17900] train loss: 0.1016, train acc: 0.9662, val loss: 0.1621, val acc: 0.9632  (best train acc: 0.9718, best val acc: 0.9680, best train loss: 0.0861  @ epoch 17825 )\n",
      "[Epoch: 17920] train loss: 0.1037, train acc: 0.9643, val loss: 0.1555, val acc: 0.9626  (best train acc: 0.9718, best val acc: 0.9680, best train loss: 0.0861  @ epoch 17825 )\n",
      "[Epoch: 17940] train loss: 0.0985, train acc: 0.9661, val loss: 0.1567, val acc: 0.9649  (best train acc: 0.9718, best val acc: 0.9680, best train loss: 0.0861  @ epoch 17825 )\n",
      "[Epoch: 17960] train loss: 0.0997, train acc: 0.9652, val loss: 0.1564, val acc: 0.9612  (best train acc: 0.9718, best val acc: 0.9680, best train loss: 0.0861  @ epoch 17825 )\n",
      "[Epoch: 17980] train loss: 0.1109, train acc: 0.9602, val loss: 0.1949, val acc: 0.9538  (best train acc: 0.9720, best val acc: 0.9680, best train loss: 0.0861  @ epoch 17825 )\n",
      "[Epoch: 18000] train loss: 0.1829, train acc: 0.9331, val loss: 0.1659, val acc: 0.9497  (best train acc: 0.9720, best val acc: 0.9680, best train loss: 0.0861  @ epoch 17825 )\n",
      "[Epoch: 18020] train loss: 0.1126, train acc: 0.9620, val loss: 0.1550, val acc: 0.9632  (best train acc: 0.9720, best val acc: 0.9680, best train loss: 0.0861  @ epoch 17825 )\n",
      "[Epoch: 18040] train loss: 0.1040, train acc: 0.9686, val loss: 0.1539, val acc: 0.9656  (best train acc: 0.9720, best val acc: 0.9680, best train loss: 0.0861  @ epoch 17825 )\n",
      "[Epoch: 18060] train loss: 0.0987, train acc: 0.9701, val loss: 0.1473, val acc: 0.9656  (best train acc: 0.9720, best val acc: 0.9680, best train loss: 0.0861  @ epoch 17825 )\n",
      "[Epoch: 18080] train loss: 0.1023, train acc: 0.9671, val loss: 0.1551, val acc: 0.9642  (best train acc: 0.9720, best val acc: 0.9680, best train loss: 0.0861  @ epoch 17825 )\n",
      "[Epoch: 18100] train loss: 0.0975, train acc: 0.9654, val loss: 0.1674, val acc: 0.9629  (best train acc: 0.9720, best val acc: 0.9680, best train loss: 0.0861  @ epoch 17825 )\n",
      "[Epoch: 18120] train loss: 0.1498, train acc: 0.9530, val loss: 0.2136, val acc: 0.9447  (best train acc: 0.9720, best val acc: 0.9680, best train loss: 0.0861  @ epoch 17825 )\n",
      "[Epoch: 18140] train loss: 0.1210, train acc: 0.9555, val loss: 0.1609, val acc: 0.9639  (best train acc: 0.9720, best val acc: 0.9680, best train loss: 0.0861  @ epoch 17825 )\n",
      "[Epoch: 18160] train loss: 0.0995, train acc: 0.9689, val loss: 0.1601, val acc: 0.9642  (best train acc: 0.9720, best val acc: 0.9680, best train loss: 0.0861  @ epoch 17825 )\n",
      "[Epoch: 18180] train loss: 0.0965, train acc: 0.9696, val loss: 0.1597, val acc: 0.9646  (best train acc: 0.9720, best val acc: 0.9680, best train loss: 0.0861  @ epoch 17825 )\n",
      "[Epoch: 18200] train loss: 0.0995, train acc: 0.9674, val loss: 0.1626, val acc: 0.9646  (best train acc: 0.9720, best val acc: 0.9680, best train loss: 0.0861  @ epoch 17825 )\n",
      "[Epoch: 18220] train loss: 0.1048, train acc: 0.9654, val loss: 0.1598, val acc: 0.9599  (best train acc: 0.9720, best val acc: 0.9680, best train loss: 0.0861  @ epoch 17825 )\n",
      "[Epoch: 18240] train loss: 0.1544, train acc: 0.9506, val loss: 0.1971, val acc: 0.9508  (best train acc: 0.9720, best val acc: 0.9680, best train loss: 0.0861  @ epoch 17825 )\n",
      "[Epoch: 18260] train loss: 0.1218, train acc: 0.9568, val loss: 0.1475, val acc: 0.9616  (best train acc: 0.9720, best val acc: 0.9680, best train loss: 0.0861  @ epoch 17825 )\n",
      "[Epoch: 18280] train loss: 0.0963, train acc: 0.9676, val loss: 0.1630, val acc: 0.9649  (best train acc: 0.9720, best val acc: 0.9680, best train loss: 0.0861  @ epoch 17825 )\n",
      "[Epoch: 18300] train loss: 0.0938, train acc: 0.9702, val loss: 0.1538, val acc: 0.9669  (best train acc: 0.9720, best val acc: 0.9680, best train loss: 0.0861  @ epoch 17825 )\n",
      "[Epoch: 18320] train loss: 0.0945, train acc: 0.9691, val loss: 0.1484, val acc: 0.9663  (best train acc: 0.9720, best val acc: 0.9680, best train loss: 0.0861  @ epoch 17825 )\n",
      "[Epoch: 18340] train loss: 0.0960, train acc: 0.9687, val loss: 0.1588, val acc: 0.9639  (best train acc: 0.9720, best val acc: 0.9680, best train loss: 0.0861  @ epoch 17825 )\n",
      "[Epoch: 18360] train loss: 0.0965, train acc: 0.9684, val loss: 0.1590, val acc: 0.9656  (best train acc: 0.9720, best val acc: 0.9680, best train loss: 0.0861  @ epoch 17825 )\n",
      "[Epoch: 18380] train loss: 0.1002, train acc: 0.9665, val loss: 0.1493, val acc: 0.9656  (best train acc: 0.9720, best val acc: 0.9680, best train loss: 0.0861  @ epoch 17825 )\n",
      "[Epoch: 18400] train loss: 0.0935, train acc: 0.9722, val loss: 0.1570, val acc: 0.9656  (best train acc: 0.9722, best val acc: 0.9680, best train loss: 0.0859  @ epoch 18386 )\n",
      "[Epoch: 18420] train loss: 0.1018, train acc: 0.9675, val loss: 0.1629, val acc: 0.9602  (best train acc: 0.9722, best val acc: 0.9680, best train loss: 0.0859  @ epoch 18386 )\n",
      "[Epoch: 18440] train loss: 0.0966, train acc: 0.9707, val loss: 0.1678, val acc: 0.9636  (best train acc: 0.9722, best val acc: 0.9680, best train loss: 0.0859  @ epoch 18386 )\n",
      "[Epoch: 18460] train loss: 0.1030, train acc: 0.9651, val loss: 0.1983, val acc: 0.9545  (best train acc: 0.9722, best val acc: 0.9680, best train loss: 0.0859  @ epoch 18386 )\n",
      "[Epoch: 18480] train loss: 0.1244, train acc: 0.9555, val loss: 0.1497, val acc: 0.9626  (best train acc: 0.9722, best val acc: 0.9680, best train loss: 0.0859  @ epoch 18386 )\n",
      "[Epoch: 18500] train loss: 0.0970, train acc: 0.9680, val loss: 0.1602, val acc: 0.9629  (best train acc: 0.9722, best val acc: 0.9680, best train loss: 0.0859  @ epoch 18386 )\n",
      "[Epoch: 18520] train loss: 0.1022, train acc: 0.9676, val loss: 0.1551, val acc: 0.9636  (best train acc: 0.9722, best val acc: 0.9680, best train loss: 0.0859  @ epoch 18386 )\n",
      "[Epoch: 18540] train loss: 0.0936, train acc: 0.9691, val loss: 0.1599, val acc: 0.9666  (best train acc: 0.9722, best val acc: 0.9680, best train loss: 0.0859  @ epoch 18386 )\n",
      "[Epoch: 18560] train loss: 0.0961, train acc: 0.9686, val loss: 0.1575, val acc: 0.9656  (best train acc: 0.9722, best val acc: 0.9680, best train loss: 0.0857  @ epoch 18557 )\n",
      "[Epoch: 18580] train loss: 0.1400, train acc: 0.9459, val loss: 0.2291, val acc: 0.9255  (best train acc: 0.9722, best val acc: 0.9680, best train loss: 0.0857  @ epoch 18557 )\n",
      "[Epoch: 18600] train loss: 0.1107, train acc: 0.9604, val loss: 0.1576, val acc: 0.9582  (best train acc: 0.9722, best val acc: 0.9680, best train loss: 0.0857  @ epoch 18557 )\n",
      "[Epoch: 18620] train loss: 0.1066, train acc: 0.9639, val loss: 0.1605, val acc: 0.9632  (best train acc: 0.9722, best val acc: 0.9683, best train loss: 0.0857  @ epoch 18557 )\n",
      "[Epoch: 18640] train loss: 0.1156, train acc: 0.9579, val loss: 0.1570, val acc: 0.9619  (best train acc: 0.9722, best val acc: 0.9683, best train loss: 0.0857  @ epoch 18557 )\n",
      "[Epoch: 18660] train loss: 0.0931, train acc: 0.9674, val loss: 0.1559, val acc: 0.9659  (best train acc: 0.9722, best val acc: 0.9683, best train loss: 0.0857  @ epoch 18557 )\n",
      "[Epoch: 18680] train loss: 0.1013, train acc: 0.9715, val loss: 0.1571, val acc: 0.9663  (best train acc: 0.9722, best val acc: 0.9683, best train loss: 0.0857  @ epoch 18557 )\n",
      "[Epoch: 18700] train loss: 0.0884, train acc: 0.9709, val loss: 0.1585, val acc: 0.9669  (best train acc: 0.9729, best val acc: 0.9683, best train loss: 0.0849  @ epoch 18686 )\n",
      "[Epoch: 18720] train loss: 0.0971, train acc: 0.9681, val loss: 0.1545, val acc: 0.9683  (best train acc: 0.9729, best val acc: 0.9683, best train loss: 0.0821  @ epoch 18704 )\n",
      "[Epoch: 18740] train loss: 0.0991, train acc: 0.9662, val loss: 0.1453, val acc: 0.9686  (best train acc: 0.9729, best val acc: 0.9686, best train loss: 0.0821  @ epoch 18704 )\n",
      "[Epoch: 18760] train loss: 0.0986, train acc: 0.9665, val loss: 0.1516, val acc: 0.9659  (best train acc: 0.9729, best val acc: 0.9686, best train loss: 0.0821  @ epoch 18704 )\n",
      "[Epoch: 18780] train loss: 0.0962, train acc: 0.9691, val loss: 0.1550, val acc: 0.9686  (best train acc: 0.9734, best val acc: 0.9686, best train loss: 0.0821  @ epoch 18704 )\n",
      "[Epoch: 18800] train loss: 0.1048, train acc: 0.9639, val loss: 0.1512, val acc: 0.9673  (best train acc: 0.9734, best val acc: 0.9686, best train loss: 0.0821  @ epoch 18704 )\n",
      "[Epoch: 18820] train loss: 0.0904, train acc: 0.9712, val loss: 0.1549, val acc: 0.9649  (best train acc: 0.9734, best val acc: 0.9686, best train loss: 0.0821  @ epoch 18704 )\n",
      "[Epoch: 18840] train loss: 0.0965, train acc: 0.9680, val loss: 0.1610, val acc: 0.9659  (best train acc: 0.9734, best val acc: 0.9686, best train loss: 0.0821  @ epoch 18704 )\n",
      "[Epoch: 18860] train loss: 0.1002, train acc: 0.9664, val loss: 0.1739, val acc: 0.9642  (best train acc: 0.9734, best val acc: 0.9686, best train loss: 0.0821  @ epoch 18704 )\n",
      "[Epoch: 18880] train loss: 0.1048, train acc: 0.9646, val loss: 0.1519, val acc: 0.9649  (best train acc: 0.9734, best val acc: 0.9686, best train loss: 0.0821  @ epoch 18704 )\n",
      "[Epoch: 18900] train loss: 0.0879, train acc: 0.9714, val loss: 0.1525, val acc: 0.9673  (best train acc: 0.9734, best val acc: 0.9686, best train loss: 0.0821  @ epoch 18704 )\n",
      "[Epoch: 18920] train loss: 0.0948, train acc: 0.9678, val loss: 0.1511, val acc: 0.9680  (best train acc: 0.9734, best val acc: 0.9686, best train loss: 0.0821  @ epoch 18704 )\n",
      "[Epoch: 18940] train loss: 0.0925, train acc: 0.9690, val loss: 0.1547, val acc: 0.9629  (best train acc: 0.9734, best val acc: 0.9686, best train loss: 0.0821  @ epoch 18704 )\n",
      "[Epoch: 18960] train loss: 0.0913, train acc: 0.9682, val loss: 0.1510, val acc: 0.9676  (best train acc: 0.9734, best val acc: 0.9686, best train loss: 0.0821  @ epoch 18704 )\n",
      "[Epoch: 18980] train loss: 0.1106, train acc: 0.9629, val loss: 0.1579, val acc: 0.9629  (best train acc: 0.9734, best val acc: 0.9686, best train loss: 0.0821  @ epoch 18704 )\n",
      "[Epoch: 19000] train loss: 0.1102, train acc: 0.9629, val loss: 0.1556, val acc: 0.9622  (best train acc: 0.9734, best val acc: 0.9686, best train loss: 0.0821  @ epoch 18704 )\n",
      "[Epoch: 19020] train loss: 0.0925, train acc: 0.9689, val loss: 0.1498, val acc: 0.9669  (best train acc: 0.9734, best val acc: 0.9686, best train loss: 0.0821  @ epoch 18704 )\n",
      "[Epoch: 19040] train loss: 0.1000, train acc: 0.9658, val loss: 0.1886, val acc: 0.9562  (best train acc: 0.9734, best val acc: 0.9686, best train loss: 0.0821  @ epoch 18704 )\n",
      "[Epoch: 19060] train loss: 0.1775, train acc: 0.9477, val loss: 0.1823, val acc: 0.9477  (best train acc: 0.9734, best val acc: 0.9686, best train loss: 0.0821  @ epoch 18704 )\n",
      "[Epoch: 19080] train loss: 0.1144, train acc: 0.9609, val loss: 0.1462, val acc: 0.9649  (best train acc: 0.9734, best val acc: 0.9686, best train loss: 0.0821  @ epoch 18704 )\n",
      "[Epoch: 19100] train loss: 0.0967, train acc: 0.9690, val loss: 0.1538, val acc: 0.9659  (best train acc: 0.9734, best val acc: 0.9686, best train loss: 0.0821  @ epoch 18704 )\n",
      "[Epoch: 19120] train loss: 0.0888, train acc: 0.9711, val loss: 0.1537, val acc: 0.9659  (best train acc: 0.9734, best val acc: 0.9686, best train loss: 0.0821  @ epoch 18704 )\n",
      "[Epoch: 19140] train loss: 0.1033, train acc: 0.9633, val loss: 0.1625, val acc: 0.9619  (best train acc: 0.9734, best val acc: 0.9686, best train loss: 0.0821  @ epoch 18704 )\n",
      "[Epoch: 19160] train loss: 0.1072, train acc: 0.9626, val loss: 0.1549, val acc: 0.9629  (best train acc: 0.9734, best val acc: 0.9686, best train loss: 0.0821  @ epoch 18704 )\n",
      "[Epoch: 19180] train loss: 0.0972, train acc: 0.9687, val loss: 0.1511, val acc: 0.9683  (best train acc: 0.9734, best val acc: 0.9686, best train loss: 0.0821  @ epoch 18704 )\n",
      "[Epoch: 19200] train loss: 0.1027, train acc: 0.9668, val loss: 0.1452, val acc: 0.9622  (best train acc: 0.9734, best val acc: 0.9686, best train loss: 0.0821  @ epoch 18704 )\n",
      "[Epoch: 19220] train loss: 0.1069, train acc: 0.9638, val loss: 0.1548, val acc: 0.9663  (best train acc: 0.9734, best val acc: 0.9686, best train loss: 0.0821  @ epoch 18704 )\n",
      "[Epoch: 19240] train loss: 0.0934, train acc: 0.9680, val loss: 0.1675, val acc: 0.9663  (best train acc: 0.9734, best val acc: 0.9690, best train loss: 0.0821  @ epoch 18704 )\n",
      "[Epoch: 19260] train loss: 0.1113, train acc: 0.9640, val loss: 0.1883, val acc: 0.9562  (best train acc: 0.9734, best val acc: 0.9690, best train loss: 0.0821  @ epoch 18704 )\n",
      "[Epoch: 19280] train loss: 0.1782, train acc: 0.9284, val loss: 0.2832, val acc: 0.9167  (best train acc: 0.9734, best val acc: 0.9690, best train loss: 0.0821  @ epoch 18704 )\n",
      "[Epoch: 19300] train loss: 0.1251, train acc: 0.9596, val loss: 0.1489, val acc: 0.9595  (best train acc: 0.9734, best val acc: 0.9690, best train loss: 0.0821  @ epoch 18704 )\n",
      "[Epoch: 19320] train loss: 0.1039, train acc: 0.9661, val loss: 0.1380, val acc: 0.9659  (best train acc: 0.9734, best val acc: 0.9690, best train loss: 0.0821  @ epoch 18704 )\n",
      "[Epoch: 19340] train loss: 0.1114, train acc: 0.9638, val loss: 0.1412, val acc: 0.9609  (best train acc: 0.9734, best val acc: 0.9690, best train loss: 0.0821  @ epoch 18704 )\n",
      "[Epoch: 19360] train loss: 0.0944, train acc: 0.9676, val loss: 0.1417, val acc: 0.9680  (best train acc: 0.9734, best val acc: 0.9690, best train loss: 0.0821  @ epoch 18704 )\n",
      "[Epoch: 19380] train loss: 0.0941, train acc: 0.9709, val loss: 0.1367, val acc: 0.9653  (best train acc: 0.9734, best val acc: 0.9690, best train loss: 0.0821  @ epoch 18704 )\n",
      "[Epoch: 19400] train loss: 0.0911, train acc: 0.9697, val loss: 0.1430, val acc: 0.9649  (best train acc: 0.9734, best val acc: 0.9690, best train loss: 0.0821  @ epoch 18704 )\n",
      "[Epoch: 19420] train loss: 0.0942, train acc: 0.9678, val loss: 0.1459, val acc: 0.9666  (best train acc: 0.9734, best val acc: 0.9690, best train loss: 0.0821  @ epoch 18704 )\n",
      "[Epoch: 19440] train loss: 0.1007, train acc: 0.9712, val loss: 0.1466, val acc: 0.9676  (best train acc: 0.9734, best val acc: 0.9690, best train loss: 0.0821  @ epoch 18704 )\n",
      "[Epoch: 19460] train loss: 0.0909, train acc: 0.9682, val loss: 0.1561, val acc: 0.9653  (best train acc: 0.9734, best val acc: 0.9690, best train loss: 0.0802  @ epoch 19459 )\n",
      "[Epoch: 19480] train loss: 0.1993, train acc: 0.9412, val loss: 0.1756, val acc: 0.9538  (best train acc: 0.9734, best val acc: 0.9690, best train loss: 0.0802  @ epoch 19459 )\n",
      "[Epoch: 19500] train loss: 0.1187, train acc: 0.9636, val loss: 0.1718, val acc: 0.9599  (best train acc: 0.9734, best val acc: 0.9690, best train loss: 0.0802  @ epoch 19459 )\n",
      "[Epoch: 19520] train loss: 0.1066, train acc: 0.9654, val loss: 0.1705, val acc: 0.9659  (best train acc: 0.9734, best val acc: 0.9690, best train loss: 0.0802  @ epoch 19459 )\n",
      "[Epoch: 19540] train loss: 0.0879, train acc: 0.9712, val loss: 0.1556, val acc: 0.9659  (best train acc: 0.9734, best val acc: 0.9690, best train loss: 0.0802  @ epoch 19459 )\n",
      "[Epoch: 19560] train loss: 0.1044, train acc: 0.9652, val loss: 0.1501, val acc: 0.9656  (best train acc: 0.9734, best val acc: 0.9690, best train loss: 0.0802  @ epoch 19459 )\n",
      "[Epoch: 19580] train loss: 0.0933, train acc: 0.9690, val loss: 0.1436, val acc: 0.9649  (best train acc: 0.9734, best val acc: 0.9690, best train loss: 0.0802  @ epoch 19459 )\n",
      "[Epoch: 19600] train loss: 0.1163, train acc: 0.9577, val loss: 0.1379, val acc: 0.9663  (best train acc: 0.9734, best val acc: 0.9690, best train loss: 0.0802  @ epoch 19459 )\n",
      "[Epoch: 19620] train loss: 0.0906, train acc: 0.9700, val loss: 0.1553, val acc: 0.9669  (best train acc: 0.9734, best val acc: 0.9690, best train loss: 0.0802  @ epoch 19459 )\n",
      "[Epoch: 19640] train loss: 0.1139, train acc: 0.9581, val loss: 0.1428, val acc: 0.9690  (best train acc: 0.9734, best val acc: 0.9690, best train loss: 0.0802  @ epoch 19459 )\n",
      "[Epoch: 19660] train loss: 0.0999, train acc: 0.9661, val loss: 0.1599, val acc: 0.9676  (best train acc: 0.9734, best val acc: 0.9690, best train loss: 0.0802  @ epoch 19459 )\n",
      "[Epoch: 19680] train loss: 0.1001, train acc: 0.9664, val loss: 0.1565, val acc: 0.9642  (best train acc: 0.9734, best val acc: 0.9690, best train loss: 0.0802  @ epoch 19459 )\n",
      "[Epoch: 19700] train loss: 0.0948, train acc: 0.9714, val loss: 0.1516, val acc: 0.9669  (best train acc: 0.9734, best val acc: 0.9690, best train loss: 0.0802  @ epoch 19459 )\n",
      "[Epoch: 19720] train loss: 0.0931, train acc: 0.9703, val loss: 0.1552, val acc: 0.9649  (best train acc: 0.9734, best val acc: 0.9690, best train loss: 0.0802  @ epoch 19459 )\n",
      "[Epoch: 19740] train loss: 0.0944, train acc: 0.9682, val loss: 0.1485, val acc: 0.9669  (best train acc: 0.9734, best val acc: 0.9690, best train loss: 0.0802  @ epoch 19459 )\n",
      "[Epoch: 19760] train loss: 0.0995, train acc: 0.9637, val loss: 0.1513, val acc: 0.9673  (best train acc: 0.9734, best val acc: 0.9690, best train loss: 0.0802  @ epoch 19459 )\n",
      "[Epoch: 19780] train loss: 0.1047, train acc: 0.9617, val loss: 0.1537, val acc: 0.9653  (best train acc: 0.9734, best val acc: 0.9690, best train loss: 0.0802  @ epoch 19459 )\n",
      "[Epoch: 19800] train loss: 0.0925, train acc: 0.9693, val loss: 0.1534, val acc: 0.9656  (best train acc: 0.9734, best val acc: 0.9690, best train loss: 0.0802  @ epoch 19459 )\n",
      "[Epoch: 19820] train loss: 0.0856, train acc: 0.9714, val loss: 0.1516, val acc: 0.9619  (best train acc: 0.9734, best val acc: 0.9690, best train loss: 0.0802  @ epoch 19459 )\n",
      "[Epoch: 19840] train loss: 0.1056, train acc: 0.9653, val loss: 0.1387, val acc: 0.9666  (best train acc: 0.9734, best val acc: 0.9690, best train loss: 0.0802  @ epoch 19459 )\n",
      "[Epoch: 19860] train loss: 0.0881, train acc: 0.9715, val loss: 0.1370, val acc: 0.9653  (best train acc: 0.9734, best val acc: 0.9690, best train loss: 0.0802  @ epoch 19459 )\n",
      "[Epoch: 19880] train loss: 0.0844, train acc: 0.9717, val loss: 0.1427, val acc: 0.9683  (best train acc: 0.9734, best val acc: 0.9690, best train loss: 0.0802  @ epoch 19459 )\n",
      "[Epoch: 19900] train loss: 0.1085, train acc: 0.9612, val loss: 0.1602, val acc: 0.9592  (best train acc: 0.9734, best val acc: 0.9693, best train loss: 0.0802  @ epoch 19459 )\n",
      "[Epoch: 19920] train loss: 0.0924, train acc: 0.9690, val loss: 0.1334, val acc: 0.9666  (best train acc: 0.9734, best val acc: 0.9693, best train loss: 0.0802  @ epoch 19459 )\n",
      "[Epoch: 19940] train loss: 0.1200, train acc: 0.9521, val loss: 0.1530, val acc: 0.9622  (best train acc: 0.9734, best val acc: 0.9693, best train loss: 0.0802  @ epoch 19459 )\n",
      "[Epoch: 19960] train loss: 0.0877, train acc: 0.9710, val loss: 0.1488, val acc: 0.9653  (best train acc: 0.9734, best val acc: 0.9693, best train loss: 0.0802  @ epoch 19459 )\n",
      "[Epoch: 19980] train loss: 0.0849, train acc: 0.9717, val loss: 0.1421, val acc: 0.9680  (best train acc: 0.9734, best val acc: 0.9693, best train loss: 0.0802  @ epoch 19459 )\n",
      "[Epoch: 20000] train loss: 0.0892, train acc: 0.9682, val loss: 0.1413, val acc: 0.9680  (best train acc: 0.9747, best val acc: 0.9693, best train loss: 0.0802  @ epoch 19459 )\n",
      "[Epoch: 20020] train loss: 0.1180, train acc: 0.9602, val loss: 0.1545, val acc: 0.9605  (best train acc: 0.9747, best val acc: 0.9693, best train loss: 0.0787  @ epoch 20004 )\n",
      "[Epoch: 20040] train loss: 0.0930, train acc: 0.9671, val loss: 0.1436, val acc: 0.9686  (best train acc: 0.9747, best val acc: 0.9693, best train loss: 0.0787  @ epoch 20004 )\n",
      "[Epoch: 20060] train loss: 0.0848, train acc: 0.9712, val loss: 0.1459, val acc: 0.9690  (best train acc: 0.9747, best val acc: 0.9693, best train loss: 0.0787  @ epoch 20004 )\n",
      "[Epoch: 20080] train loss: 0.0849, train acc: 0.9725, val loss: 0.1506, val acc: 0.9676  (best train acc: 0.9750, best val acc: 0.9696, best train loss: 0.0775  @ epoch 20073 )\n",
      "[Epoch: 20100] train loss: 0.0868, train acc: 0.9693, val loss: 0.1707, val acc: 0.9592  (best train acc: 0.9750, best val acc: 0.9696, best train loss: 0.0775  @ epoch 20073 )\n",
      "[Epoch: 20120] train loss: 0.0837, train acc: 0.9720, val loss: 0.1510, val acc: 0.9666  (best train acc: 0.9750, best val acc: 0.9696, best train loss: 0.0775  @ epoch 20073 )\n",
      "[Epoch: 20140] train loss: 0.0846, train acc: 0.9720, val loss: 0.1488, val acc: 0.9659  (best train acc: 0.9750, best val acc: 0.9696, best train loss: 0.0775  @ epoch 20073 )\n",
      "[Epoch: 20160] train loss: 0.0919, train acc: 0.9679, val loss: 0.1327, val acc: 0.9673  (best train acc: 0.9750, best val acc: 0.9696, best train loss: 0.0775  @ epoch 20073 )\n",
      "[Epoch: 20180] train loss: 0.0935, train acc: 0.9654, val loss: 0.1580, val acc: 0.9659  (best train acc: 0.9750, best val acc: 0.9696, best train loss: 0.0775  @ epoch 20073 )\n",
      "[Epoch: 20200] train loss: 0.1034, train acc: 0.9629, val loss: 0.1555, val acc: 0.9646  (best train acc: 0.9750, best val acc: 0.9696, best train loss: 0.0775  @ epoch 20073 )\n",
      "[Epoch: 20220] train loss: 0.0904, train acc: 0.9674, val loss: 0.1415, val acc: 0.9669  (best train acc: 0.9750, best val acc: 0.9696, best train loss: 0.0775  @ epoch 20073 )\n",
      "[Epoch: 20240] train loss: 0.0929, train acc: 0.9665, val loss: 0.1368, val acc: 0.9673  (best train acc: 0.9750, best val acc: 0.9696, best train loss: 0.0775  @ epoch 20073 )\n",
      "[Epoch: 20260] train loss: 0.0852, train acc: 0.9712, val loss: 0.1382, val acc: 0.9686  (best train acc: 0.9750, best val acc: 0.9696, best train loss: 0.0775  @ epoch 20073 )\n",
      "[Epoch: 20280] train loss: 0.0930, train acc: 0.9664, val loss: 0.1496, val acc: 0.9656  (best train acc: 0.9750, best val acc: 0.9696, best train loss: 0.0775  @ epoch 20073 )\n",
      "[Epoch: 20300] train loss: 0.0775, train acc: 0.9736, val loss: 0.1443, val acc: 0.9656  (best train acc: 0.9750, best val acc: 0.9696, best train loss: 0.0759  @ epoch 20294 )\n",
      "[Epoch: 20320] train loss: 0.0819, train acc: 0.9725, val loss: 0.1572, val acc: 0.9659  (best train acc: 0.9757, best val acc: 0.9696, best train loss: 0.0759  @ epoch 20294 )\n",
      "[Epoch: 20340] train loss: 0.0959, train acc: 0.9680, val loss: 0.1908, val acc: 0.9511  (best train acc: 0.9757, best val acc: 0.9696, best train loss: 0.0759  @ epoch 20294 )\n",
      "[Epoch: 20360] train loss: 0.0957, train acc: 0.9660, val loss: 0.1390, val acc: 0.9683  (best train acc: 0.9757, best val acc: 0.9696, best train loss: 0.0759  @ epoch 20294 )\n",
      "[Epoch: 20380] train loss: 0.0941, train acc: 0.9659, val loss: 0.1486, val acc: 0.9649  (best train acc: 0.9757, best val acc: 0.9696, best train loss: 0.0759  @ epoch 20294 )\n",
      "[Epoch: 20400] train loss: 0.0930, train acc: 0.9676, val loss: 0.1455, val acc: 0.9642  (best train acc: 0.9757, best val acc: 0.9696, best train loss: 0.0759  @ epoch 20294 )\n",
      "[Epoch: 20420] train loss: 0.0821, train acc: 0.9714, val loss: 0.1503, val acc: 0.9659  (best train acc: 0.9757, best val acc: 0.9696, best train loss: 0.0759  @ epoch 20294 )\n",
      "[Epoch: 20440] train loss: 0.1088, train acc: 0.9592, val loss: 0.2303, val acc: 0.9514  (best train acc: 0.9757, best val acc: 0.9696, best train loss: 0.0759  @ epoch 20294 )\n",
      "[Epoch: 20460] train loss: 0.0973, train acc: 0.9680, val loss: 0.1448, val acc: 0.9632  (best train acc: 0.9757, best val acc: 0.9696, best train loss: 0.0759  @ epoch 20294 )\n",
      "[Epoch: 20480] train loss: 0.0860, train acc: 0.9706, val loss: 0.1522, val acc: 0.9676  (best train acc: 0.9757, best val acc: 0.9696, best train loss: 0.0759  @ epoch 20294 )\n",
      "[Epoch: 20500] train loss: 0.0976, train acc: 0.9646, val loss: 0.1538, val acc: 0.9636  (best train acc: 0.9757, best val acc: 0.9696, best train loss: 0.0759  @ epoch 20294 )\n",
      "[Epoch: 20520] train loss: 0.0991, train acc: 0.9635, val loss: 0.1487, val acc: 0.9612  (best train acc: 0.9757, best val acc: 0.9696, best train loss: 0.0759  @ epoch 20294 )\n",
      "[Epoch: 20540] train loss: 0.1194, train acc: 0.9613, val loss: 0.1539, val acc: 0.9612  (best train acc: 0.9757, best val acc: 0.9696, best train loss: 0.0759  @ epoch 20294 )\n",
      "[Epoch: 20560] train loss: 0.0976, train acc: 0.9649, val loss: 0.1685, val acc: 0.9636  (best train acc: 0.9757, best val acc: 0.9696, best train loss: 0.0759  @ epoch 20294 )\n",
      "[Epoch: 20580] train loss: 0.1086, train acc: 0.9605, val loss: 0.1382, val acc: 0.9619  (best train acc: 0.9757, best val acc: 0.9696, best train loss: 0.0759  @ epoch 20294 )\n",
      "[Epoch: 20600] train loss: 0.0849, train acc: 0.9691, val loss: 0.1667, val acc: 0.9642  (best train acc: 0.9757, best val acc: 0.9696, best train loss: 0.0759  @ epoch 20294 )\n",
      "[Epoch: 20620] train loss: 0.1016, train acc: 0.9643, val loss: 0.1685, val acc: 0.9636  (best train acc: 0.9757, best val acc: 0.9696, best train loss: 0.0759  @ epoch 20294 )\n",
      "[Epoch: 20640] train loss: 0.0930, train acc: 0.9720, val loss: 0.1543, val acc: 0.9649  (best train acc: 0.9757, best val acc: 0.9696, best train loss: 0.0759  @ epoch 20294 )\n",
      "[Epoch: 20660] train loss: 0.0919, train acc: 0.9695, val loss: 0.1394, val acc: 0.9653  (best train acc: 0.9757, best val acc: 0.9696, best train loss: 0.0759  @ epoch 20294 )\n",
      "[Epoch: 20680] train loss: 0.0821, train acc: 0.9718, val loss: 0.1468, val acc: 0.9669  (best train acc: 0.9757, best val acc: 0.9696, best train loss: 0.0759  @ epoch 20294 )\n",
      "[Epoch: 20700] train loss: 0.0775, train acc: 0.9756, val loss: 0.1391, val acc: 0.9673  (best train acc: 0.9757, best val acc: 0.9696, best train loss: 0.0759  @ epoch 20294 )\n",
      "[Epoch: 20720] train loss: 0.0884, train acc: 0.9695, val loss: 0.1594, val acc: 0.9649  (best train acc: 0.9757, best val acc: 0.9696, best train loss: 0.0725  @ epoch 20714 )\n",
      "[Epoch: 20740] train loss: 0.0931, train acc: 0.9687, val loss: 0.1534, val acc: 0.9673  (best train acc: 0.9757, best val acc: 0.9696, best train loss: 0.0725  @ epoch 20714 )\n",
      "[Epoch: 20760] train loss: 0.1139, train acc: 0.9602, val loss: 0.1836, val acc: 0.9551  (best train acc: 0.9757, best val acc: 0.9696, best train loss: 0.0725  @ epoch 20714 )\n",
      "[Epoch: 20780] train loss: 0.0937, train acc: 0.9675, val loss: 0.1598, val acc: 0.9659  (best train acc: 0.9757, best val acc: 0.9696, best train loss: 0.0725  @ epoch 20714 )\n",
      "[Epoch: 20800] train loss: 0.0916, train acc: 0.9670, val loss: 0.1671, val acc: 0.9622  (best train acc: 0.9757, best val acc: 0.9696, best train loss: 0.0725  @ epoch 20714 )\n",
      "[Epoch: 20820] train loss: 0.0924, train acc: 0.9671, val loss: 0.1568, val acc: 0.9656  (best train acc: 0.9757, best val acc: 0.9696, best train loss: 0.0725  @ epoch 20714 )\n",
      "[Epoch: 20840] train loss: 0.0865, train acc: 0.9706, val loss: 0.1488, val acc: 0.9622  (best train acc: 0.9757, best val acc: 0.9696, best train loss: 0.0725  @ epoch 20714 )\n",
      "[Epoch: 20860] train loss: 0.0829, train acc: 0.9704, val loss: 0.1612, val acc: 0.9642  (best train acc: 0.9757, best val acc: 0.9696, best train loss: 0.0725  @ epoch 20714 )\n",
      "[Epoch: 20880] train loss: 0.0886, train acc: 0.9699, val loss: 0.1518, val acc: 0.9676  (best train acc: 0.9757, best val acc: 0.9696, best train loss: 0.0725  @ epoch 20714 )\n",
      "[Epoch: 20900] train loss: 0.0895, train acc: 0.9696, val loss: 0.1511, val acc: 0.9663  (best train acc: 0.9758, best val acc: 0.9696, best train loss: 0.0725  @ epoch 20714 )\n",
      "[Epoch: 20920] train loss: 0.0857, train acc: 0.9717, val loss: 0.1723, val acc: 0.9609  (best train acc: 0.9758, best val acc: 0.9696, best train loss: 0.0725  @ epoch 20714 )\n",
      "[Epoch: 20940] train loss: 0.0800, train acc: 0.9733, val loss: 0.1504, val acc: 0.9666  (best train acc: 0.9758, best val acc: 0.9696, best train loss: 0.0725  @ epoch 20714 )\n",
      "[Epoch: 20960] train loss: 0.0771, train acc: 0.9735, val loss: 0.1484, val acc: 0.9680  (best train acc: 0.9758, best val acc: 0.9696, best train loss: 0.0725  @ epoch 20714 )\n",
      "[Epoch: 20980] train loss: 0.0860, train acc: 0.9704, val loss: 0.1510, val acc: 0.9622  (best train acc: 0.9758, best val acc: 0.9696, best train loss: 0.0725  @ epoch 20714 )\n",
      "[Epoch: 21000] train loss: 0.1193, train acc: 0.9541, val loss: 0.1772, val acc: 0.9565  (best train acc: 0.9758, best val acc: 0.9696, best train loss: 0.0725  @ epoch 20714 )\n",
      "[Epoch: 21020] train loss: 0.0871, train acc: 0.9689, val loss: 0.1472, val acc: 0.9646  (best train acc: 0.9758, best val acc: 0.9696, best train loss: 0.0725  @ epoch 20714 )\n",
      "[Epoch: 21040] train loss: 0.0941, train acc: 0.9677, val loss: 0.1454, val acc: 0.9659  (best train acc: 0.9758, best val acc: 0.9696, best train loss: 0.0725  @ epoch 20714 )\n",
      "[Epoch: 21060] train loss: 0.0848, train acc: 0.9730, val loss: 0.1425, val acc: 0.9669  (best train acc: 0.9758, best val acc: 0.9696, best train loss: 0.0725  @ epoch 20714 )\n",
      "[Epoch: 21080] train loss: 0.0934, train acc: 0.9698, val loss: 0.1530, val acc: 0.9589  (best train acc: 0.9758, best val acc: 0.9696, best train loss: 0.0725  @ epoch 20714 )\n",
      "[Epoch: 21100] train loss: 0.0933, train acc: 0.9683, val loss: 0.1484, val acc: 0.9629  (best train acc: 0.9758, best val acc: 0.9696, best train loss: 0.0725  @ epoch 20714 )\n",
      "[Epoch: 21120] train loss: 0.0918, train acc: 0.9671, val loss: 0.1689, val acc: 0.9582  (best train acc: 0.9758, best val acc: 0.9696, best train loss: 0.0725  @ epoch 20714 )\n",
      "[Epoch: 21140] train loss: 0.0851, train acc: 0.9722, val loss: 0.1458, val acc: 0.9639  (best train acc: 0.9758, best val acc: 0.9696, best train loss: 0.0725  @ epoch 20714 )\n",
      "[Epoch: 21160] train loss: 0.0832, train acc: 0.9714, val loss: 0.1525, val acc: 0.9666  (best train acc: 0.9758, best val acc: 0.9696, best train loss: 0.0725  @ epoch 20714 )\n",
      "[Epoch: 21180] train loss: 0.0848, train acc: 0.9736, val loss: 0.1467, val acc: 0.9666  (best train acc: 0.9758, best val acc: 0.9696, best train loss: 0.0725  @ epoch 20714 )\n",
      "[Epoch: 21200] train loss: 0.0881, train acc: 0.9695, val loss: 0.1465, val acc: 0.9663  (best train acc: 0.9758, best val acc: 0.9696, best train loss: 0.0725  @ epoch 20714 )\n",
      "[Epoch: 21220] train loss: 0.0940, train acc: 0.9678, val loss: 0.1471, val acc: 0.9649  (best train acc: 0.9758, best val acc: 0.9696, best train loss: 0.0725  @ epoch 20714 )\n",
      "[Epoch: 21240] train loss: 0.0916, train acc: 0.9730, val loss: 0.1478, val acc: 0.9669  (best train acc: 0.9758, best val acc: 0.9696, best train loss: 0.0725  @ epoch 20714 )\n",
      "[Epoch: 21260] train loss: 0.0956, train acc: 0.9643, val loss: 0.1769, val acc: 0.9531  (best train acc: 0.9758, best val acc: 0.9696, best train loss: 0.0725  @ epoch 20714 )\n",
      "[Epoch: 21280] train loss: 0.0824, train acc: 0.9720, val loss: 0.1655, val acc: 0.9666  (best train acc: 0.9758, best val acc: 0.9696, best train loss: 0.0725  @ epoch 20714 )\n",
      "[Epoch: 21300] train loss: 0.0845, train acc: 0.9743, val loss: 0.1575, val acc: 0.9639  (best train acc: 0.9758, best val acc: 0.9696, best train loss: 0.0725  @ epoch 20714 )\n",
      "[Epoch: 21320] train loss: 0.0836, train acc: 0.9719, val loss: 0.1619, val acc: 0.9653  (best train acc: 0.9764, best val acc: 0.9696, best train loss: 0.0719  @ epoch 21316 )\n",
      "[Epoch: 21340] train loss: 0.0871, train acc: 0.9704, val loss: 0.1641, val acc: 0.9636  (best train acc: 0.9764, best val acc: 0.9696, best train loss: 0.0719  @ epoch 21316 )\n",
      "[Epoch: 21360] train loss: 0.0826, train acc: 0.9718, val loss: 0.1652, val acc: 0.9656  (best train acc: 0.9764, best val acc: 0.9700, best train loss: 0.0719  @ epoch 21316 )\n",
      "[Epoch: 21380] train loss: 0.1066, train acc: 0.9682, val loss: 0.1665, val acc: 0.9616  (best train acc: 0.9764, best val acc: 0.9700, best train loss: 0.0719  @ epoch 21316 )\n",
      "[Epoch: 21400] train loss: 0.1335, train acc: 0.9506, val loss: 0.1518, val acc: 0.9592  (best train acc: 0.9764, best val acc: 0.9700, best train loss: 0.0719  @ epoch 21316 )\n",
      "[Epoch: 21420] train loss: 0.0938, train acc: 0.9690, val loss: 0.1496, val acc: 0.9616  (best train acc: 0.9764, best val acc: 0.9700, best train loss: 0.0719  @ epoch 21316 )\n",
      "[Epoch: 21440] train loss: 0.0875, train acc: 0.9694, val loss: 0.1404, val acc: 0.9680  (best train acc: 0.9764, best val acc: 0.9700, best train loss: 0.0719  @ epoch 21316 )\n",
      "[Epoch: 21460] train loss: 0.0781, train acc: 0.9745, val loss: 0.1548, val acc: 0.9649  (best train acc: 0.9764, best val acc: 0.9700, best train loss: 0.0719  @ epoch 21316 )\n",
      "[Epoch: 21480] train loss: 0.0797, train acc: 0.9730, val loss: 0.1516, val acc: 0.9669  (best train acc: 0.9764, best val acc: 0.9703, best train loss: 0.0719  @ epoch 21316 )\n",
      "[Epoch: 21500] train loss: 0.0965, train acc: 0.9675, val loss: 0.1413, val acc: 0.9639  (best train acc: 0.9764, best val acc: 0.9703, best train loss: 0.0719  @ epoch 21316 )\n",
      "[Epoch: 21520] train loss: 0.0856, train acc: 0.9704, val loss: 0.1420, val acc: 0.9653  (best train acc: 0.9764, best val acc: 0.9703, best train loss: 0.0719  @ epoch 21316 )\n",
      "[Epoch: 21540] train loss: 0.0800, train acc: 0.9725, val loss: 0.1517, val acc: 0.9649  (best train acc: 0.9764, best val acc: 0.9703, best train loss: 0.0719  @ epoch 21316 )\n",
      "[Epoch: 21560] train loss: 0.0804, train acc: 0.9717, val loss: 0.1631, val acc: 0.9599  (best train acc: 0.9766, best val acc: 0.9703, best train loss: 0.0719  @ epoch 21316 )\n",
      "[Epoch: 21580] train loss: 0.0881, train acc: 0.9683, val loss: 0.1583, val acc: 0.9649  (best train acc: 0.9766, best val acc: 0.9703, best train loss: 0.0719  @ epoch 21316 )\n",
      "[Epoch: 21600] train loss: 0.0780, train acc: 0.9724, val loss: 0.1544, val acc: 0.9649  (best train acc: 0.9766, best val acc: 0.9703, best train loss: 0.0719  @ epoch 21316 )\n",
      "[Epoch: 21620] train loss: 0.0863, train acc: 0.9725, val loss: 0.1607, val acc: 0.9649  (best train acc: 0.9766, best val acc: 0.9703, best train loss: 0.0719  @ epoch 21316 )\n",
      "[Epoch: 21640] train loss: 0.0761, train acc: 0.9747, val loss: 0.1531, val acc: 0.9666  (best train acc: 0.9766, best val acc: 0.9703, best train loss: 0.0719  @ epoch 21316 )\n",
      "[Epoch: 21660] train loss: 0.0802, train acc: 0.9722, val loss: 0.1473, val acc: 0.9646  (best train acc: 0.9766, best val acc: 0.9703, best train loss: 0.0719  @ epoch 21316 )\n",
      "[Epoch: 21680] train loss: 0.0826, train acc: 0.9721, val loss: 0.1493, val acc: 0.9659  (best train acc: 0.9766, best val acc: 0.9703, best train loss: 0.0719  @ epoch 21316 )\n",
      "[Epoch: 21700] train loss: 0.0905, train acc: 0.9698, val loss: 0.1523, val acc: 0.9609  (best train acc: 0.9766, best val acc: 0.9703, best train loss: 0.0719  @ epoch 21316 )\n",
      "[Epoch: 21720] train loss: 0.0849, train acc: 0.9722, val loss: 0.1548, val acc: 0.9669  (best train acc: 0.9766, best val acc: 0.9703, best train loss: 0.0719  @ epoch 21316 )\n",
      "[Epoch: 21740] train loss: 0.0844, train acc: 0.9721, val loss: 0.1521, val acc: 0.9666  (best train acc: 0.9766, best val acc: 0.9703, best train loss: 0.0719  @ epoch 21316 )\n",
      "[Epoch: 21760] train loss: 0.1165, train acc: 0.9558, val loss: 0.1576, val acc: 0.9629  (best train acc: 0.9766, best val acc: 0.9703, best train loss: 0.0719  @ epoch 21316 )\n",
      "[Epoch: 21780] train loss: 0.0925, train acc: 0.9680, val loss: 0.1522, val acc: 0.9649  (best train acc: 0.9766, best val acc: 0.9703, best train loss: 0.0719  @ epoch 21316 )\n",
      "[Epoch: 21800] train loss: 0.0866, train acc: 0.9711, val loss: 0.1561, val acc: 0.9656  (best train acc: 0.9766, best val acc: 0.9703, best train loss: 0.0719  @ epoch 21316 )\n",
      "[Epoch: 21820] train loss: 0.0815, train acc: 0.9720, val loss: 0.1468, val acc: 0.9656  (best train acc: 0.9766, best val acc: 0.9703, best train loss: 0.0719  @ epoch 21316 )\n",
      "[Epoch: 21840] train loss: 0.0849, train acc: 0.9704, val loss: 0.1492, val acc: 0.9646  (best train acc: 0.9766, best val acc: 0.9703, best train loss: 0.0719  @ epoch 21316 )\n",
      "[Epoch: 21860] train loss: 0.1435, train acc: 0.9486, val loss: 0.1532, val acc: 0.9622  (best train acc: 0.9766, best val acc: 0.9703, best train loss: 0.0719  @ epoch 21316 )\n",
      "[Epoch: 21880] train loss: 0.0842, train acc: 0.9711, val loss: 0.1580, val acc: 0.9636  (best train acc: 0.9766, best val acc: 0.9703, best train loss: 0.0719  @ epoch 21316 )\n",
      "[Epoch: 21900] train loss: 0.0853, train acc: 0.9709, val loss: 0.1574, val acc: 0.9653  (best train acc: 0.9766, best val acc: 0.9703, best train loss: 0.0719  @ epoch 21316 )\n",
      "[Epoch: 21920] train loss: 0.0847, train acc: 0.9722, val loss: 0.1601, val acc: 0.9659  (best train acc: 0.9766, best val acc: 0.9703, best train loss: 0.0719  @ epoch 21316 )\n",
      "[Epoch: 21940] train loss: 0.0962, train acc: 0.9706, val loss: 0.1606, val acc: 0.9676  (best train acc: 0.9766, best val acc: 0.9703, best train loss: 0.0719  @ epoch 21316 )\n",
      "[Epoch: 21960] train loss: 0.0800, train acc: 0.9719, val loss: 0.1576, val acc: 0.9680  (best train acc: 0.9766, best val acc: 0.9703, best train loss: 0.0719  @ epoch 21316 )\n",
      "[Epoch: 21980] train loss: 0.1054, train acc: 0.9617, val loss: 0.1851, val acc: 0.9589  (best train acc: 0.9766, best val acc: 0.9703, best train loss: 0.0719  @ epoch 21316 )\n",
      "[Epoch: 22000] train loss: 0.0786, train acc: 0.9736, val loss: 0.1438, val acc: 0.9666  (best train acc: 0.9766, best val acc: 0.9703, best train loss: 0.0719  @ epoch 21316 )\n",
      "[Epoch: 22020] train loss: 0.0847, train acc: 0.9724, val loss: 0.1578, val acc: 0.9642  (best train acc: 0.9766, best val acc: 0.9703, best train loss: 0.0719  @ epoch 21316 )\n",
      "[Epoch: 22040] train loss: 0.0757, train acc: 0.9741, val loss: 0.1679, val acc: 0.9656  (best train acc: 0.9766, best val acc: 0.9703, best train loss: 0.0719  @ epoch 21316 )\n",
      "[Epoch: 22060] train loss: 0.0947, train acc: 0.9698, val loss: 0.1656, val acc: 0.9609  (best train acc: 0.9766, best val acc: 0.9703, best train loss: 0.0719  @ epoch 21316 )\n",
      "[Epoch: 22080] train loss: 0.0914, train acc: 0.9701, val loss: 0.1734, val acc: 0.9609  (best train acc: 0.9766, best val acc: 0.9703, best train loss: 0.0719  @ epoch 21316 )\n",
      "[Epoch: 22100] train loss: 0.0941, train acc: 0.9648, val loss: 0.1474, val acc: 0.9659  (best train acc: 0.9766, best val acc: 0.9703, best train loss: 0.0719  @ epoch 21316 )\n",
      "[Epoch: 22120] train loss: 0.0792, train acc: 0.9727, val loss: 0.1783, val acc: 0.9636  (best train acc: 0.9766, best val acc: 0.9703, best train loss: 0.0719  @ epoch 21316 )\n",
      "[Epoch: 22140] train loss: 0.0750, train acc: 0.9757, val loss: 0.1562, val acc: 0.9669  (best train acc: 0.9766, best val acc: 0.9703, best train loss: 0.0719  @ epoch 21316 )\n",
      "[Epoch: 22160] train loss: 0.0825, train acc: 0.9729, val loss: 0.1545, val acc: 0.9649  (best train acc: 0.9766, best val acc: 0.9703, best train loss: 0.0719  @ epoch 21316 )\n",
      "[Epoch: 22180] train loss: 0.1023, train acc: 0.9624, val loss: 0.1648, val acc: 0.9649  (best train acc: 0.9766, best val acc: 0.9703, best train loss: 0.0719  @ epoch 21316 )\n",
      "[Epoch: 22200] train loss: 0.0967, train acc: 0.9720, val loss: 0.1566, val acc: 0.9642  (best train acc: 0.9766, best val acc: 0.9703, best train loss: 0.0719  @ epoch 21316 )\n",
      "[Epoch: 22220] train loss: 0.0969, train acc: 0.9672, val loss: 0.1729, val acc: 0.9602  (best train acc: 0.9766, best val acc: 0.9703, best train loss: 0.0719  @ epoch 21316 )\n",
      "[Epoch: 22240] train loss: 0.0765, train acc: 0.9730, val loss: 0.1545, val acc: 0.9642  (best train acc: 0.9766, best val acc: 0.9703, best train loss: 0.0719  @ epoch 21316 )\n",
      "[Epoch: 22260] train loss: 0.0912, train acc: 0.9648, val loss: 0.1579, val acc: 0.9646  (best train acc: 0.9766, best val acc: 0.9703, best train loss: 0.0719  @ epoch 21316 )\n",
      "[Epoch: 22280] train loss: 0.0822, train acc: 0.9706, val loss: 0.1673, val acc: 0.9632  (best train acc: 0.9766, best val acc: 0.9703, best train loss: 0.0719  @ epoch 21316 )\n",
      "[Epoch: 22300] train loss: 0.0911, train acc: 0.9684, val loss: 0.1558, val acc: 0.9636  (best train acc: 0.9766, best val acc: 0.9703, best train loss: 0.0719  @ epoch 21316 )\n",
      "[Epoch: 22320] train loss: 0.1001, train acc: 0.9699, val loss: 0.1790, val acc: 0.9629  (best train acc: 0.9766, best val acc: 0.9703, best train loss: 0.0719  @ epoch 21316 )\n",
      "[Epoch: 22340] train loss: 0.0844, train acc: 0.9713, val loss: 0.1634, val acc: 0.9649  (best train acc: 0.9766, best val acc: 0.9703, best train loss: 0.0719  @ epoch 21316 )\n",
      "[Epoch: 22360] train loss: 0.0963, train acc: 0.9644, val loss: 0.1806, val acc: 0.9609  (best train acc: 0.9766, best val acc: 0.9703, best train loss: 0.0719  @ epoch 21316 )\n",
      "[Epoch: 22380] train loss: 0.0975, train acc: 0.9686, val loss: 0.1440, val acc: 0.9656  (best train acc: 0.9766, best val acc: 0.9703, best train loss: 0.0719  @ epoch 21316 )\n",
      "[Epoch: 22400] train loss: 0.0868, train acc: 0.9711, val loss: 0.1463, val acc: 0.9663  (best train acc: 0.9766, best val acc: 0.9703, best train loss: 0.0719  @ epoch 21316 )\n",
      "[Epoch: 22420] train loss: 0.0969, train acc: 0.9637, val loss: 0.1749, val acc: 0.9619  (best train acc: 0.9766, best val acc: 0.9703, best train loss: 0.0719  @ epoch 21316 )\n",
      "[Epoch: 22440] train loss: 0.0770, train acc: 0.9731, val loss: 0.1602, val acc: 0.9669  (best train acc: 0.9766, best val acc: 0.9703, best train loss: 0.0719  @ epoch 21316 )\n",
      "[Epoch: 22460] train loss: 0.0857, train acc: 0.9737, val loss: 0.1558, val acc: 0.9676  (best train acc: 0.9774, best val acc: 0.9703, best train loss: 0.0694  @ epoch 22459 )\n",
      "[Epoch: 22480] train loss: 0.0784, train acc: 0.9747, val loss: 0.1557, val acc: 0.9649  (best train acc: 0.9774, best val acc: 0.9703, best train loss: 0.0694  @ epoch 22459 )\n",
      "[Epoch: 22500] train loss: 0.0819, train acc: 0.9718, val loss: 0.1579, val acc: 0.9649  (best train acc: 0.9774, best val acc: 0.9703, best train loss: 0.0694  @ epoch 22459 )\n",
      "[Epoch: 22520] train loss: 0.0791, train acc: 0.9730, val loss: 0.1521, val acc: 0.9669  (best train acc: 0.9774, best val acc: 0.9703, best train loss: 0.0694  @ epoch 22459 )\n",
      "[Epoch: 22540] train loss: 0.1898, train acc: 0.9477, val loss: 0.2857, val acc: 0.9234  (best train acc: 0.9774, best val acc: 0.9703, best train loss: 0.0694  @ epoch 22459 )\n",
      "[Epoch: 22560] train loss: 0.1187, train acc: 0.9595, val loss: 0.1790, val acc: 0.9572  (best train acc: 0.9774, best val acc: 0.9703, best train loss: 0.0694  @ epoch 22459 )\n",
      "[Epoch: 22580] train loss: 0.0910, train acc: 0.9675, val loss: 0.1620, val acc: 0.9622  (best train acc: 0.9774, best val acc: 0.9703, best train loss: 0.0694  @ epoch 22459 )\n",
      "[Epoch: 22600] train loss: 0.0860, train acc: 0.9721, val loss: 0.1463, val acc: 0.9676  (best train acc: 0.9774, best val acc: 0.9703, best train loss: 0.0694  @ epoch 22459 )\n",
      "[Epoch: 22620] train loss: 0.0831, train acc: 0.9724, val loss: 0.1333, val acc: 0.9653  (best train acc: 0.9774, best val acc: 0.9703, best train loss: 0.0694  @ epoch 22459 )\n",
      "[Epoch: 22640] train loss: 0.0805, train acc: 0.9714, val loss: 0.1543, val acc: 0.9653  (best train acc: 0.9774, best val acc: 0.9703, best train loss: 0.0694  @ epoch 22459 )\n",
      "[Epoch: 22660] train loss: 0.0830, train acc: 0.9712, val loss: 0.1660, val acc: 0.9659  (best train acc: 0.9774, best val acc: 0.9703, best train loss: 0.0694  @ epoch 22459 )\n",
      "[Epoch: 22680] train loss: 0.0857, train acc: 0.9710, val loss: 0.1505, val acc: 0.9626  (best train acc: 0.9774, best val acc: 0.9703, best train loss: 0.0694  @ epoch 22459 )\n",
      "[Epoch: 22700] train loss: 0.0896, train acc: 0.9693, val loss: 0.1452, val acc: 0.9646  (best train acc: 0.9774, best val acc: 0.9703, best train loss: 0.0694  @ epoch 22459 )\n",
      "[Epoch: 22720] train loss: 0.0832, train acc: 0.9717, val loss: 0.1690, val acc: 0.9642  (best train acc: 0.9774, best val acc: 0.9703, best train loss: 0.0694  @ epoch 22459 )\n",
      "[Epoch: 22740] train loss: 0.0887, train acc: 0.9718, val loss: 0.1623, val acc: 0.9639  (best train acc: 0.9774, best val acc: 0.9703, best train loss: 0.0694  @ epoch 22459 )\n",
      "[Epoch: 22760] train loss: 0.0868, train acc: 0.9695, val loss: 0.1592, val acc: 0.9673  (best train acc: 0.9774, best val acc: 0.9703, best train loss: 0.0694  @ epoch 22459 )\n",
      "[Epoch: 22780] train loss: 0.0778, train acc: 0.9740, val loss: 0.1457, val acc: 0.9666  (best train acc: 0.9777, best val acc: 0.9703, best train loss: 0.0694  @ epoch 22459 )\n",
      "[Epoch: 22800] train loss: 0.0852, train acc: 0.9704, val loss: 0.1641, val acc: 0.9649  (best train acc: 0.9777, best val acc: 0.9703, best train loss: 0.0694  @ epoch 22459 )\n",
      "[Epoch: 22820] train loss: 0.0808, train acc: 0.9721, val loss: 0.1655, val acc: 0.9669  (best train acc: 0.9777, best val acc: 0.9703, best train loss: 0.0694  @ epoch 22459 )\n",
      "[Epoch: 22840] train loss: 0.0974, train acc: 0.9695, val loss: 0.1359, val acc: 0.9622  (best train acc: 0.9777, best val acc: 0.9703, best train loss: 0.0694  @ epoch 22459 )\n",
      "[Epoch: 22860] train loss: 0.1104, train acc: 0.9573, val loss: 0.1970, val acc: 0.9501  (best train acc: 0.9777, best val acc: 0.9703, best train loss: 0.0694  @ epoch 22459 )\n",
      "[Epoch: 22880] train loss: 0.1034, train acc: 0.9641, val loss: 0.1595, val acc: 0.9612  (best train acc: 0.9777, best val acc: 0.9703, best train loss: 0.0694  @ epoch 22459 )\n",
      "[Epoch: 22900] train loss: 0.0986, train acc: 0.9664, val loss: 0.1666, val acc: 0.9609  (best train acc: 0.9777, best val acc: 0.9703, best train loss: 0.0694  @ epoch 22459 )\n",
      "[Epoch: 22920] train loss: 0.0750, train acc: 0.9760, val loss: 0.1737, val acc: 0.9653  (best train acc: 0.9777, best val acc: 0.9703, best train loss: 0.0694  @ epoch 22459 )\n",
      "[Epoch: 22940] train loss: 0.0756, train acc: 0.9752, val loss: 0.1566, val acc: 0.9666  (best train acc: 0.9777, best val acc: 0.9703, best train loss: 0.0694  @ epoch 22459 )\n",
      "[Epoch: 22960] train loss: 0.0752, train acc: 0.9765, val loss: 0.1582, val acc: 0.9683  (best train acc: 0.9777, best val acc: 0.9703, best train loss: 0.0694  @ epoch 22459 )\n",
      "[Epoch: 22980] train loss: 0.0803, train acc: 0.9714, val loss: 0.1491, val acc: 0.9642  (best train acc: 0.9777, best val acc: 0.9703, best train loss: 0.0694  @ epoch 22459 )\n",
      "[Epoch: 23000] train loss: 0.0889, train acc: 0.9692, val loss: 0.1526, val acc: 0.9669  (best train acc: 0.9777, best val acc: 0.9703, best train loss: 0.0694  @ epoch 22459 )\n",
      "[Epoch: 23020] train loss: 0.0808, train acc: 0.9716, val loss: 0.1485, val acc: 0.9666  (best train acc: 0.9777, best val acc: 0.9703, best train loss: 0.0694  @ epoch 22459 )\n",
      "[Epoch: 23040] train loss: 0.1127, train acc: 0.9628, val loss: 0.1717, val acc: 0.9562  (best train acc: 0.9777, best val acc: 0.9703, best train loss: 0.0694  @ epoch 22459 )\n",
      "[Epoch: 23060] train loss: 0.0770, train acc: 0.9740, val loss: 0.1398, val acc: 0.9646  (best train acc: 0.9777, best val acc: 0.9703, best train loss: 0.0694  @ epoch 22459 )\n",
      "[Epoch: 23080] train loss: 0.0723, train acc: 0.9773, val loss: 0.1616, val acc: 0.9639  (best train acc: 0.9777, best val acc: 0.9703, best train loss: 0.0694  @ epoch 22459 )\n",
      "[Epoch: 23100] train loss: 0.0734, train acc: 0.9753, val loss: 0.1525, val acc: 0.9653  (best train acc: 0.9777, best val acc: 0.9703, best train loss: 0.0694  @ epoch 22459 )\n",
      "[Epoch: 23120] train loss: 0.0792, train acc: 0.9737, val loss: 0.1761, val acc: 0.9653  (best train acc: 0.9777, best val acc: 0.9703, best train loss: 0.0694  @ epoch 22459 )\n",
      "[Epoch: 23140] train loss: 0.0760, train acc: 0.9754, val loss: 0.1618, val acc: 0.9642  (best train acc: 0.9777, best val acc: 0.9703, best train loss: 0.0694  @ epoch 22459 )\n",
      "[Epoch: 23160] train loss: 0.0766, train acc: 0.9744, val loss: 0.1623, val acc: 0.9666  (best train acc: 0.9777, best val acc: 0.9703, best train loss: 0.0694  @ epoch 22459 )\n",
      "[Epoch: 23180] train loss: 0.0737, train acc: 0.9756, val loss: 0.1450, val acc: 0.9632  (best train acc: 0.9777, best val acc: 0.9703, best train loss: 0.0694  @ epoch 22459 )\n",
      "[Epoch: 23200] train loss: 0.0779, train acc: 0.9729, val loss: 0.1560, val acc: 0.9653  (best train acc: 0.9777, best val acc: 0.9703, best train loss: 0.0694  @ epoch 22459 )\n",
      "[Epoch: 23220] train loss: 0.0785, train acc: 0.9740, val loss: 0.1616, val acc: 0.9663  (best train acc: 0.9777, best val acc: 0.9703, best train loss: 0.0694  @ epoch 22459 )\n",
      "[Epoch: 23240] train loss: 0.0926, train acc: 0.9696, val loss: 0.1397, val acc: 0.9639  (best train acc: 0.9777, best val acc: 0.9703, best train loss: 0.0694  @ epoch 22459 )\n",
      "[Epoch: 23260] train loss: 0.0789, train acc: 0.9707, val loss: 0.1540, val acc: 0.9680  (best train acc: 0.9777, best val acc: 0.9703, best train loss: 0.0694  @ epoch 22459 )\n",
      "[Epoch: 23280] train loss: 0.0945, train acc: 0.9680, val loss: 0.1659, val acc: 0.9602  (best train acc: 0.9777, best val acc: 0.9703, best train loss: 0.0694  @ epoch 22459 )\n",
      "[Epoch: 23300] train loss: 0.0765, train acc: 0.9743, val loss: 0.1635, val acc: 0.9659  (best train acc: 0.9777, best val acc: 0.9703, best train loss: 0.0694  @ epoch 22459 )\n",
      "[Epoch: 23320] train loss: 0.0797, train acc: 0.9738, val loss: 0.1536, val acc: 0.9663  (best train acc: 0.9777, best val acc: 0.9703, best train loss: 0.0694  @ epoch 22459 )\n",
      "[Epoch: 23340] train loss: 0.0750, train acc: 0.9751, val loss: 0.1531, val acc: 0.9680  (best train acc: 0.9777, best val acc: 0.9703, best train loss: 0.0694  @ epoch 22459 )\n",
      "[Epoch: 23360] train loss: 0.0772, train acc: 0.9740, val loss: 0.1491, val acc: 0.9666  (best train acc: 0.9777, best val acc: 0.9703, best train loss: 0.0694  @ epoch 22459 )\n",
      "[Epoch: 23380] train loss: 0.0798, train acc: 0.9712, val loss: 0.1609, val acc: 0.9663  (best train acc: 0.9780, best val acc: 0.9703, best train loss: 0.0693  @ epoch 23371 )\n",
      "[Epoch: 23400] train loss: 0.0718, train acc: 0.9759, val loss: 0.1674, val acc: 0.9639  (best train acc: 0.9780, best val acc: 0.9703, best train loss: 0.0693  @ epoch 23371 )\n",
      "[Epoch: 23420] train loss: 0.0813, train acc: 0.9718, val loss: 0.1515, val acc: 0.9642  (best train acc: 0.9780, best val acc: 0.9703, best train loss: 0.0693  @ epoch 23371 )\n",
      "[Epoch: 23440] train loss: 0.1691, train acc: 0.9299, val loss: 0.1414, val acc: 0.9646  (best train acc: 0.9780, best val acc: 0.9703, best train loss: 0.0693  @ epoch 23371 )\n",
      "[Epoch: 23460] train loss: 0.1316, train acc: 0.9580, val loss: 0.1439, val acc: 0.9609  (best train acc: 0.9780, best val acc: 0.9703, best train loss: 0.0693  @ epoch 23371 )\n",
      "[Epoch: 23480] train loss: 0.0951, train acc: 0.9657, val loss: 0.1468, val acc: 0.9616  (best train acc: 0.9780, best val acc: 0.9703, best train loss: 0.0693  @ epoch 23371 )\n",
      "[Epoch: 23500] train loss: 0.0893, train acc: 0.9712, val loss: 0.1400, val acc: 0.9653  (best train acc: 0.9780, best val acc: 0.9703, best train loss: 0.0693  @ epoch 23371 )\n",
      "[Epoch: 23520] train loss: 0.0769, train acc: 0.9730, val loss: 0.1512, val acc: 0.9666  (best train acc: 0.9780, best val acc: 0.9703, best train loss: 0.0693  @ epoch 23371 )\n",
      "[Epoch: 23540] train loss: 0.0848, train acc: 0.9737, val loss: 0.1490, val acc: 0.9626  (best train acc: 0.9780, best val acc: 0.9703, best train loss: 0.0693  @ epoch 23371 )\n",
      "[Epoch: 23560] train loss: 0.0850, train acc: 0.9720, val loss: 0.1527, val acc: 0.9659  (best train acc: 0.9780, best val acc: 0.9703, best train loss: 0.0693  @ epoch 23371 )\n",
      "[Epoch: 23580] train loss: 0.0773, train acc: 0.9783, val loss: 0.1488, val acc: 0.9649  (best train acc: 0.9783, best val acc: 0.9703, best train loss: 0.0693  @ epoch 23371 )\n",
      "[Epoch: 23600] train loss: 0.0837, train acc: 0.9702, val loss: 0.1412, val acc: 0.9659  (best train acc: 0.9783, best val acc: 0.9703, best train loss: 0.0693  @ epoch 23371 )\n",
      "[Epoch: 23620] train loss: 0.0825, train acc: 0.9731, val loss: 0.1552, val acc: 0.9680  (best train acc: 0.9783, best val acc: 0.9703, best train loss: 0.0693  @ epoch 23371 )\n",
      "[Epoch: 23640] train loss: 0.0876, train acc: 0.9699, val loss: 0.1423, val acc: 0.9646  (best train acc: 0.9783, best val acc: 0.9703, best train loss: 0.0693  @ epoch 23371 )\n",
      "[Epoch: 23660] train loss: 0.0763, train acc: 0.9730, val loss: 0.1431, val acc: 0.9676  (best train acc: 0.9783, best val acc: 0.9703, best train loss: 0.0693  @ epoch 23371 )\n",
      "[Epoch: 23680] train loss: 0.0776, train acc: 0.9736, val loss: 0.1433, val acc: 0.9663  (best train acc: 0.9783, best val acc: 0.9703, best train loss: 0.0693  @ epoch 23371 )\n",
      "[Epoch: 23700] train loss: 0.1037, train acc: 0.9624, val loss: 0.1340, val acc: 0.9622  (best train acc: 0.9783, best val acc: 0.9703, best train loss: 0.0693  @ epoch 23371 )\n",
      "[Epoch: 23720] train loss: 0.0797, train acc: 0.9738, val loss: 0.1582, val acc: 0.9642  (best train acc: 0.9783, best val acc: 0.9703, best train loss: 0.0693  @ epoch 23371 )\n",
      "[Epoch: 23740] train loss: 0.0695, train acc: 0.9776, val loss: 0.1566, val acc: 0.9656  (best train acc: 0.9783, best val acc: 0.9703, best train loss: 0.0693  @ epoch 23371 )\n",
      "[Epoch: 23760] train loss: 0.0757, train acc: 0.9753, val loss: 0.1507, val acc: 0.9663  (best train acc: 0.9783, best val acc: 0.9703, best train loss: 0.0693  @ epoch 23371 )\n",
      "[Epoch: 23780] train loss: 0.0995, train acc: 0.9670, val loss: 0.1474, val acc: 0.9632  (best train acc: 0.9783, best val acc: 0.9703, best train loss: 0.0693  @ epoch 23371 )\n",
      "[Epoch: 23800] train loss: 0.0892, train acc: 0.9704, val loss: 0.1399, val acc: 0.9653  (best train acc: 0.9783, best val acc: 0.9703, best train loss: 0.0693  @ epoch 23371 )\n",
      "[Epoch: 23820] train loss: 0.0849, train acc: 0.9684, val loss: 0.1793, val acc: 0.9619  (best train acc: 0.9783, best val acc: 0.9703, best train loss: 0.0693  @ epoch 23371 )\n",
      "[Epoch: 23840] train loss: 0.0906, train acc: 0.9701, val loss: 0.1586, val acc: 0.9656  (best train acc: 0.9783, best val acc: 0.9703, best train loss: 0.0693  @ epoch 23371 )\n",
      "[Epoch: 23860] train loss: 0.0993, train acc: 0.9638, val loss: 0.2635, val acc: 0.9403  (best train acc: 0.9783, best val acc: 0.9703, best train loss: 0.0693  @ epoch 23371 )\n",
      "[Epoch: 23880] train loss: 0.1043, train acc: 0.9644, val loss: 0.1861, val acc: 0.9622  (best train acc: 0.9783, best val acc: 0.9703, best train loss: 0.0693  @ epoch 23371 )\n",
      "[Epoch: 23900] train loss: 0.0833, train acc: 0.9714, val loss: 0.1475, val acc: 0.9663  (best train acc: 0.9783, best val acc: 0.9703, best train loss: 0.0693  @ epoch 23371 )\n",
      "[Epoch: 23920] train loss: 0.0926, train acc: 0.9673, val loss: 0.1501, val acc: 0.9663  (best train acc: 0.9783, best val acc: 0.9703, best train loss: 0.0693  @ epoch 23371 )\n",
      "[Epoch: 23940] train loss: 0.0867, train acc: 0.9689, val loss: 0.1471, val acc: 0.9673  (best train acc: 0.9783, best val acc: 0.9703, best train loss: 0.0693  @ epoch 23371 )\n",
      "[Epoch: 23960] train loss: 0.0803, train acc: 0.9741, val loss: 0.1512, val acc: 0.9656  (best train acc: 0.9783, best val acc: 0.9703, best train loss: 0.0693  @ epoch 23371 )\n",
      "[Epoch: 23980] train loss: 0.0830, train acc: 0.9725, val loss: 0.1526, val acc: 0.9686  (best train acc: 0.9783, best val acc: 0.9703, best train loss: 0.0693  @ epoch 23371 )\n",
      "[Epoch: 24000] train loss: 0.0790, train acc: 0.9727, val loss: 0.1440, val acc: 0.9656  (best train acc: 0.9783, best val acc: 0.9703, best train loss: 0.0693  @ epoch 23371 )\n",
      "[Epoch: 24020] train loss: 0.0723, train acc: 0.9761, val loss: 0.1461, val acc: 0.9656  (best train acc: 0.9783, best val acc: 0.9703, best train loss: 0.0693  @ epoch 23371 )\n",
      "[Epoch: 24040] train loss: 0.0726, train acc: 0.9751, val loss: 0.1471, val acc: 0.9666  (best train acc: 0.9783, best val acc: 0.9703, best train loss: 0.0693  @ epoch 23371 )\n",
      "[Epoch: 24060] train loss: 0.1328, train acc: 0.9592, val loss: 0.1928, val acc: 0.9558  (best train acc: 0.9783, best val acc: 0.9703, best train loss: 0.0693  @ epoch 23371 )\n",
      "[Epoch: 24080] train loss: 0.0830, train acc: 0.9712, val loss: 0.1719, val acc: 0.9653  (best train acc: 0.9783, best val acc: 0.9703, best train loss: 0.0693  @ epoch 23371 )\n",
      "[Epoch: 24100] train loss: 0.0786, train acc: 0.9718, val loss: 0.1689, val acc: 0.9636  (best train acc: 0.9783, best val acc: 0.9703, best train loss: 0.0693  @ epoch 23371 )\n",
      "[Epoch: 24120] train loss: 0.0745, train acc: 0.9733, val loss: 0.1544, val acc: 0.9673  (best train acc: 0.9783, best val acc: 0.9703, best train loss: 0.0693  @ epoch 23371 )\n",
      "[Epoch: 24140] train loss: 0.0747, train acc: 0.9759, val loss: 0.1529, val acc: 0.9669  (best train acc: 0.9783, best val acc: 0.9703, best train loss: 0.0693  @ epoch 23371 )\n",
      "[Epoch: 24160] train loss: 0.0765, train acc: 0.9735, val loss: 0.1528, val acc: 0.9666  (best train acc: 0.9783, best val acc: 0.9703, best train loss: 0.0693  @ epoch 23371 )\n",
      "[Epoch: 24180] train loss: 0.0725, train acc: 0.9766, val loss: 0.1433, val acc: 0.9676  (best train acc: 0.9783, best val acc: 0.9703, best train loss: 0.0693  @ epoch 23371 )\n",
      "[Epoch: 24200] train loss: 0.0942, train acc: 0.9676, val loss: 0.1695, val acc: 0.9582  (best train acc: 0.9783, best val acc: 0.9703, best train loss: 0.0693  @ epoch 23371 )\n",
      "[Epoch: 24220] train loss: 0.0727, train acc: 0.9743, val loss: 0.1604, val acc: 0.9673  (best train acc: 0.9783, best val acc: 0.9703, best train loss: 0.0693  @ epoch 23371 )\n",
      "[Epoch: 24240] train loss: 0.0778, train acc: 0.9725, val loss: 0.1455, val acc: 0.9656  (best train acc: 0.9783, best val acc: 0.9703, best train loss: 0.0693  @ epoch 23371 )\n",
      "[Epoch: 24260] train loss: 0.0751, train acc: 0.9758, val loss: 0.1501, val acc: 0.9676  (best train acc: 0.9783, best val acc: 0.9703, best train loss: 0.0693  @ epoch 23371 )\n",
      "[Epoch: 24280] train loss: 0.0709, train acc: 0.9774, val loss: 0.1505, val acc: 0.9663  (best train acc: 0.9783, best val acc: 0.9703, best train loss: 0.0673  @ epoch 24273 )\n",
      "[Epoch: 24300] train loss: 0.0821, train acc: 0.9707, val loss: 0.1508, val acc: 0.9663  (best train acc: 0.9783, best val acc: 0.9703, best train loss: 0.0673  @ epoch 24273 )\n",
      "[Epoch: 24320] train loss: 0.0729, train acc: 0.9762, val loss: 0.1682, val acc: 0.9666  (best train acc: 0.9783, best val acc: 0.9703, best train loss: 0.0673  @ epoch 24273 )\n",
      "[Epoch: 24340] train loss: 0.0726, train acc: 0.9764, val loss: 0.1521, val acc: 0.9656  (best train acc: 0.9783, best val acc: 0.9703, best train loss: 0.0673  @ epoch 24273 )\n",
      "[Epoch: 24360] train loss: 0.0690, train acc: 0.9769, val loss: 0.1612, val acc: 0.9649  (best train acc: 0.9783, best val acc: 0.9703, best train loss: 0.0673  @ epoch 24273 )\n",
      "[Epoch: 24380] train loss: 0.1026, train acc: 0.9667, val loss: 0.1428, val acc: 0.9612  (best train acc: 0.9783, best val acc: 0.9703, best train loss: 0.0673  @ epoch 24273 )\n",
      "[Epoch: 24400] train loss: 0.0820, train acc: 0.9704, val loss: 0.1581, val acc: 0.9659  (best train acc: 0.9783, best val acc: 0.9703, best train loss: 0.0673  @ epoch 24273 )\n",
      "[Epoch: 24420] train loss: 0.0797, train acc: 0.9724, val loss: 0.1452, val acc: 0.9683  (best train acc: 0.9783, best val acc: 0.9703, best train loss: 0.0673  @ epoch 24273 )\n",
      "[Epoch: 24440] train loss: 0.0991, train acc: 0.9626, val loss: 0.1531, val acc: 0.9632  (best train acc: 0.9783, best val acc: 0.9703, best train loss: 0.0673  @ epoch 24273 )\n",
      "[Epoch: 24460] train loss: 0.0846, train acc: 0.9690, val loss: 0.1637, val acc: 0.9636  (best train acc: 0.9783, best val acc: 0.9703, best train loss: 0.0673  @ epoch 24273 )\n",
      "[Epoch: 24480] train loss: 0.0795, train acc: 0.9712, val loss: 0.1528, val acc: 0.9622  (best train acc: 0.9783, best val acc: 0.9703, best train loss: 0.0673  @ epoch 24273 )\n",
      "[Epoch: 24500] train loss: 0.0778, train acc: 0.9748, val loss: 0.1376, val acc: 0.9649  (best train acc: 0.9783, best val acc: 0.9703, best train loss: 0.0673  @ epoch 24273 )\n",
      "[Epoch: 24520] train loss: 0.0812, train acc: 0.9719, val loss: 0.1721, val acc: 0.9609  (best train acc: 0.9783, best val acc: 0.9703, best train loss: 0.0673  @ epoch 24273 )\n",
      "[Epoch: 24540] train loss: 0.0783, train acc: 0.9759, val loss: 0.1521, val acc: 0.9683  (best train acc: 0.9783, best val acc: 0.9703, best train loss: 0.0673  @ epoch 24273 )\n",
      "[Epoch: 24560] train loss: 0.0719, train acc: 0.9758, val loss: 0.1457, val acc: 0.9646  (best train acc: 0.9783, best val acc: 0.9703, best train loss: 0.0673  @ epoch 24273 )\n",
      "[Epoch: 24580] train loss: 0.0759, train acc: 0.9728, val loss: 0.1580, val acc: 0.9669  (best train acc: 0.9783, best val acc: 0.9703, best train loss: 0.0673  @ epoch 24273 )\n",
      "[Epoch: 24600] train loss: 0.0781, train acc: 0.9734, val loss: 0.1578, val acc: 0.9666  (best train acc: 0.9787, best val acc: 0.9703, best train loss: 0.0663  @ epoch 24583 )\n",
      "[Epoch: 24620] train loss: 0.1034, train acc: 0.9621, val loss: 0.1418, val acc: 0.9663  (best train acc: 0.9787, best val acc: 0.9703, best train loss: 0.0663  @ epoch 24583 )\n",
      "[Epoch: 24640] train loss: 0.0792, train acc: 0.9733, val loss: 0.1647, val acc: 0.9632  (best train acc: 0.9787, best val acc: 0.9703, best train loss: 0.0663  @ epoch 24583 )\n",
      "[Epoch: 24660] train loss: 0.0740, train acc: 0.9745, val loss: 0.1573, val acc: 0.9656  (best train acc: 0.9787, best val acc: 0.9703, best train loss: 0.0663  @ epoch 24583 )\n",
      "[Epoch: 24680] train loss: 0.0761, train acc: 0.9756, val loss: 0.1537, val acc: 0.9632  (best train acc: 0.9787, best val acc: 0.9703, best train loss: 0.0663  @ epoch 24583 )\n",
      "[Epoch: 24700] train loss: 0.0886, train acc: 0.9662, val loss: 0.1701, val acc: 0.9632  (best train acc: 0.9787, best val acc: 0.9703, best train loss: 0.0663  @ epoch 24583 )\n",
      "[Epoch: 24720] train loss: 0.0822, train acc: 0.9748, val loss: 0.1597, val acc: 0.9646  (best train acc: 0.9787, best val acc: 0.9703, best train loss: 0.0663  @ epoch 24583 )\n",
      "[Epoch: 24740] train loss: 0.0811, train acc: 0.9734, val loss: 0.1441, val acc: 0.9646  (best train acc: 0.9787, best val acc: 0.9703, best train loss: 0.0663  @ epoch 24583 )\n",
      "[Epoch: 24760] train loss: 0.0781, train acc: 0.9749, val loss: 0.1593, val acc: 0.9666  (best train acc: 0.9787, best val acc: 0.9703, best train loss: 0.0663  @ epoch 24583 )\n",
      "[Epoch: 24780] train loss: 0.0741, train acc: 0.9756, val loss: 0.1513, val acc: 0.9666  (best train acc: 0.9792, best val acc: 0.9703, best train loss: 0.0645  @ epoch 24767 )\n",
      "[Epoch: 24800] train loss: 0.0748, train acc: 0.9749, val loss: 0.1753, val acc: 0.9639  (best train acc: 0.9792, best val acc: 0.9703, best train loss: 0.0642  @ epoch 24796 )\n",
      "[Epoch: 24820] train loss: 0.0767, train acc: 0.9738, val loss: 0.1613, val acc: 0.9653  (best train acc: 0.9792, best val acc: 0.9703, best train loss: 0.0642  @ epoch 24796 )\n",
      "[Epoch: 24840] train loss: 0.0697, train acc: 0.9761, val loss: 0.1574, val acc: 0.9666  (best train acc: 0.9792, best val acc: 0.9703, best train loss: 0.0642  @ epoch 24796 )\n",
      "[Epoch: 24860] train loss: 0.0730, train acc: 0.9766, val loss: 0.1902, val acc: 0.9639  (best train acc: 0.9792, best val acc: 0.9703, best train loss: 0.0642  @ epoch 24796 )\n",
      "[Epoch: 24880] train loss: 0.0947, train acc: 0.9616, val loss: 0.1592, val acc: 0.9649  (best train acc: 0.9792, best val acc: 0.9703, best train loss: 0.0642  @ epoch 24796 )\n",
      "[Epoch: 24900] train loss: 0.0771, train acc: 0.9738, val loss: 0.1518, val acc: 0.9663  (best train acc: 0.9792, best val acc: 0.9703, best train loss: 0.0642  @ epoch 24796 )\n",
      "[Epoch: 24920] train loss: 0.0726, train acc: 0.9769, val loss: 0.1455, val acc: 0.9666  (best train acc: 0.9792, best val acc: 0.9703, best train loss: 0.0642  @ epoch 24796 )\n",
      "[Epoch: 24940] train loss: 0.0757, train acc: 0.9765, val loss: 0.1585, val acc: 0.9642  (best train acc: 0.9792, best val acc: 0.9703, best train loss: 0.0642  @ epoch 24796 )\n",
      "[Epoch: 24960] train loss: 0.1334, train acc: 0.9477, val loss: 0.1393, val acc: 0.9575  (best train acc: 0.9792, best val acc: 0.9703, best train loss: 0.0642  @ epoch 24796 )\n",
      "[Epoch: 24980] train loss: 0.1188, train acc: 0.9622, val loss: 0.1438, val acc: 0.9659  (best train acc: 0.9792, best val acc: 0.9703, best train loss: 0.0642  @ epoch 24796 )\n",
      "[Epoch: 25000] train loss: 0.0769, train acc: 0.9721, val loss: 0.1565, val acc: 0.9622  (best train acc: 0.9792, best val acc: 0.9703, best train loss: 0.0642  @ epoch 24796 )\n",
      "[Epoch: 25020] train loss: 0.0745, train acc: 0.9750, val loss: 0.1484, val acc: 0.9646  (best train acc: 0.9792, best val acc: 0.9703, best train loss: 0.0642  @ epoch 24796 )\n",
      "[Epoch: 25040] train loss: 0.0793, train acc: 0.9732, val loss: 0.1671, val acc: 0.9592  (best train acc: 0.9792, best val acc: 0.9703, best train loss: 0.0642  @ epoch 24796 )\n",
      "[Epoch: 25060] train loss: 0.0788, train acc: 0.9740, val loss: 0.1298, val acc: 0.9659  (best train acc: 0.9792, best val acc: 0.9703, best train loss: 0.0642  @ epoch 24796 )\n",
      "[Epoch: 25080] train loss: 0.1028, train acc: 0.9696, val loss: 0.1615, val acc: 0.9646  (best train acc: 0.9792, best val acc: 0.9703, best train loss: 0.0642  @ epoch 24796 )\n",
      "[Epoch: 25100] train loss: 0.0777, train acc: 0.9756, val loss: 0.1464, val acc: 0.9626  (best train acc: 0.9792, best val acc: 0.9703, best train loss: 0.0642  @ epoch 24796 )\n",
      "[Epoch: 25120] train loss: 0.0682, train acc: 0.9775, val loss: 0.1548, val acc: 0.9646  (best train acc: 0.9792, best val acc: 0.9703, best train loss: 0.0642  @ epoch 24796 )\n",
      "[Epoch: 25140] train loss: 0.0716, train acc: 0.9769, val loss: 0.1405, val acc: 0.9646  (best train acc: 0.9792, best val acc: 0.9703, best train loss: 0.0642  @ epoch 24796 )\n",
      "[Epoch: 25160] train loss: 0.0876, train acc: 0.9702, val loss: 0.1517, val acc: 0.9669  (best train acc: 0.9792, best val acc: 0.9703, best train loss: 0.0642  @ epoch 24796 )\n",
      "[Epoch: 25180] train loss: 0.0771, train acc: 0.9714, val loss: 0.1578, val acc: 0.9653  (best train acc: 0.9793, best val acc: 0.9703, best train loss: 0.0640  @ epoch 25173 )\n",
      "[Epoch: 25200] train loss: 0.0728, train acc: 0.9753, val loss: 0.1496, val acc: 0.9673  (best train acc: 0.9793, best val acc: 0.9703, best train loss: 0.0640  @ epoch 25173 )\n",
      "[Epoch: 25220] train loss: 0.0737, train acc: 0.9764, val loss: 0.1635, val acc: 0.9649  (best train acc: 0.9793, best val acc: 0.9703, best train loss: 0.0640  @ epoch 25173 )\n",
      "[Epoch: 25240] train loss: 0.0703, train acc: 0.9776, val loss: 0.1571, val acc: 0.9666  (best train acc: 0.9793, best val acc: 0.9703, best train loss: 0.0640  @ epoch 25173 )\n",
      "[Epoch: 25260] train loss: 0.0693, train acc: 0.9775, val loss: 0.1684, val acc: 0.9649  (best train acc: 0.9793, best val acc: 0.9703, best train loss: 0.0640  @ epoch 25173 )\n",
      "[Epoch: 25280] train loss: 0.0751, train acc: 0.9752, val loss: 0.1446, val acc: 0.9676  (best train acc: 0.9793, best val acc: 0.9703, best train loss: 0.0640  @ epoch 25173 )\n",
      "[Epoch: 25300] train loss: 0.0759, train acc: 0.9760, val loss: 0.1572, val acc: 0.9676  (best train acc: 0.9793, best val acc: 0.9703, best train loss: 0.0640  @ epoch 25173 )\n",
      "[Epoch: 25320] train loss: 0.0729, train acc: 0.9751, val loss: 0.1663, val acc: 0.9646  (best train acc: 0.9793, best val acc: 0.9703, best train loss: 0.0640  @ epoch 25173 )\n",
      "[Epoch: 25340] train loss: 0.1114, train acc: 0.9663, val loss: 0.1665, val acc: 0.9602  (best train acc: 0.9793, best val acc: 0.9703, best train loss: 0.0640  @ epoch 25173 )\n",
      "[Epoch: 25360] train loss: 0.0765, train acc: 0.9740, val loss: 0.1675, val acc: 0.9632  (best train acc: 0.9793, best val acc: 0.9703, best train loss: 0.0640  @ epoch 25173 )\n",
      "[Epoch: 25380] train loss: 0.0839, train acc: 0.9722, val loss: 0.1544, val acc: 0.9616  (best train acc: 0.9793, best val acc: 0.9703, best train loss: 0.0640  @ epoch 25173 )\n",
      "[Epoch: 25400] train loss: 0.0795, train acc: 0.9718, val loss: 0.1590, val acc: 0.9663  (best train acc: 0.9793, best val acc: 0.9703, best train loss: 0.0640  @ epoch 25173 )\n",
      "[Epoch: 25420] train loss: 0.0794, train acc: 0.9740, val loss: 0.1514, val acc: 0.9669  (best train acc: 0.9793, best val acc: 0.9703, best train loss: 0.0640  @ epoch 25173 )\n",
      "[Epoch: 25440] train loss: 0.0793, train acc: 0.9733, val loss: 0.1555, val acc: 0.9669  (best train acc: 0.9793, best val acc: 0.9703, best train loss: 0.0640  @ epoch 25173 )\n",
      "[Epoch: 25460] train loss: 0.0716, train acc: 0.9775, val loss: 0.1523, val acc: 0.9666  (best train acc: 0.9793, best val acc: 0.9703, best train loss: 0.0640  @ epoch 25173 )\n",
      "[Epoch: 25480] train loss: 0.0865, train acc: 0.9686, val loss: 0.1606, val acc: 0.9653  (best train acc: 0.9793, best val acc: 0.9703, best train loss: 0.0640  @ epoch 25173 )\n",
      "[Epoch: 25500] train loss: 0.0838, train acc: 0.9751, val loss: 0.1576, val acc: 0.9659  (best train acc: 0.9793, best val acc: 0.9703, best train loss: 0.0638  @ epoch 25485 )\n",
      "[Epoch: 25520] train loss: 0.0677, train acc: 0.9789, val loss: 0.1662, val acc: 0.9669  (best train acc: 0.9793, best val acc: 0.9703, best train loss: 0.0638  @ epoch 25485 )\n",
      "[Epoch: 25540] train loss: 0.0768, train acc: 0.9742, val loss: 0.1450, val acc: 0.9669  (best train acc: 0.9793, best val acc: 0.9703, best train loss: 0.0638  @ epoch 25485 )\n",
      "[Epoch: 25560] train loss: 0.0785, train acc: 0.9739, val loss: 0.1425, val acc: 0.9629  (best train acc: 0.9793, best val acc: 0.9703, best train loss: 0.0638  @ epoch 25485 )\n",
      "[Epoch: 25580] train loss: 0.0714, train acc: 0.9755, val loss: 0.1719, val acc: 0.9636  (best train acc: 0.9793, best val acc: 0.9703, best train loss: 0.0638  @ epoch 25485 )\n",
      "[Epoch: 25600] train loss: 0.0675, train acc: 0.9778, val loss: 0.1445, val acc: 0.9676  (best train acc: 0.9793, best val acc: 0.9703, best train loss: 0.0638  @ epoch 25485 )\n",
      "[Epoch: 25620] train loss: 0.0699, train acc: 0.9788, val loss: 0.1536, val acc: 0.9642  (best train acc: 0.9793, best val acc: 0.9703, best train loss: 0.0636  @ epoch 25619 )\n",
      "[Epoch: 25640] train loss: 0.1036, train acc: 0.9675, val loss: 0.1512, val acc: 0.9599  (best train acc: 0.9793, best val acc: 0.9703, best train loss: 0.0636  @ epoch 25619 )\n",
      "[Epoch: 25660] train loss: 0.0805, train acc: 0.9735, val loss: 0.1598, val acc: 0.9666  (best train acc: 0.9793, best val acc: 0.9703, best train loss: 0.0636  @ epoch 25619 )\n",
      "[Epoch: 25680] train loss: 0.0790, train acc: 0.9727, val loss: 0.1535, val acc: 0.9666  (best train acc: 0.9793, best val acc: 0.9703, best train loss: 0.0636  @ epoch 25619 )\n",
      "[Epoch: 25700] train loss: 0.0708, train acc: 0.9756, val loss: 0.1804, val acc: 0.9659  (best train acc: 0.9793, best val acc: 0.9703, best train loss: 0.0636  @ epoch 25619 )\n",
      "[Epoch: 25720] train loss: 0.0712, train acc: 0.9774, val loss: 0.1547, val acc: 0.9663  (best train acc: 0.9793, best val acc: 0.9703, best train loss: 0.0636  @ epoch 25619 )\n",
      "[Epoch: 25740] train loss: 0.0666, train acc: 0.9771, val loss: 0.1716, val acc: 0.9629  (best train acc: 0.9793, best val acc: 0.9703, best train loss: 0.0636  @ epoch 25619 )\n",
      "[Epoch: 25760] train loss: 0.0790, train acc: 0.9723, val loss: 0.1652, val acc: 0.9649  (best train acc: 0.9793, best val acc: 0.9703, best train loss: 0.0636  @ epoch 25619 )\n",
      "[Epoch: 25780] train loss: 0.0715, train acc: 0.9774, val loss: 0.1609, val acc: 0.9686  (best train acc: 0.9793, best val acc: 0.9703, best train loss: 0.0636  @ epoch 25619 )\n",
      "[Epoch: 25800] train loss: 0.0686, train acc: 0.9785, val loss: 0.1619, val acc: 0.9659  (best train acc: 0.9793, best val acc: 0.9703, best train loss: 0.0636  @ epoch 25619 )\n",
      "[Epoch: 25820] train loss: 0.0736, train acc: 0.9743, val loss: 0.1775, val acc: 0.9673  (best train acc: 0.9793, best val acc: 0.9703, best train loss: 0.0636  @ epoch 25619 )\n",
      "[Epoch: 25840] train loss: 0.0674, train acc: 0.9769, val loss: 0.1596, val acc: 0.9659  (best train acc: 0.9796, best val acc: 0.9703, best train loss: 0.0636  @ epoch 25619 )\n",
      "[Epoch: 25860] train loss: 0.0658, train acc: 0.9792, val loss: 0.1627, val acc: 0.9659  (best train acc: 0.9796, best val acc: 0.9703, best train loss: 0.0632  @ epoch 25858 )\n",
      "[Epoch: 25880] train loss: 0.0660, train acc: 0.9776, val loss: 0.1630, val acc: 0.9649  (best train acc: 0.9796, best val acc: 0.9703, best train loss: 0.0630  @ epoch 25865 )\n",
      "[Epoch: 25900] train loss: 0.0754, train acc: 0.9751, val loss: 0.1635, val acc: 0.9653  (best train acc: 0.9796, best val acc: 0.9703, best train loss: 0.0630  @ epoch 25865 )\n",
      "[Epoch: 25920] train loss: 0.0668, train acc: 0.9767, val loss: 0.1756, val acc: 0.9626  (best train acc: 0.9796, best val acc: 0.9703, best train loss: 0.0630  @ epoch 25865 )\n",
      "[Epoch: 25940] train loss: 0.0987, train acc: 0.9639, val loss: 0.1685, val acc: 0.9545  (best train acc: 0.9796, best val acc: 0.9703, best train loss: 0.0630  @ epoch 25865 )\n",
      "[Epoch: 25960] train loss: 0.0876, train acc: 0.9719, val loss: 0.1497, val acc: 0.9582  (best train acc: 0.9796, best val acc: 0.9703, best train loss: 0.0630  @ epoch 25865 )\n",
      "[Epoch: 25980] train loss: 0.0741, train acc: 0.9750, val loss: 0.1471, val acc: 0.9632  (best train acc: 0.9796, best val acc: 0.9703, best train loss: 0.0630  @ epoch 25865 )\n",
      "[Epoch: 26000] train loss: 0.0727, train acc: 0.9762, val loss: 0.1612, val acc: 0.9659  (best train acc: 0.9796, best val acc: 0.9703, best train loss: 0.0630  @ epoch 25865 )\n",
      "[Epoch: 26020] train loss: 0.0738, train acc: 0.9727, val loss: 0.1488, val acc: 0.9663  (best train acc: 0.9796, best val acc: 0.9703, best train loss: 0.0630  @ epoch 25865 )\n",
      "[Epoch: 26040] train loss: 0.0810, train acc: 0.9717, val loss: 0.1705, val acc: 0.9649  (best train acc: 0.9796, best val acc: 0.9703, best train loss: 0.0630  @ epoch 25865 )\n",
      "[Epoch: 26060] train loss: 0.0752, train acc: 0.9737, val loss: 0.1525, val acc: 0.9676  (best train acc: 0.9796, best val acc: 0.9703, best train loss: 0.0630  @ epoch 25865 )\n",
      "[Epoch: 26080] train loss: 0.0991, train acc: 0.9628, val loss: 0.1587, val acc: 0.9669  (best train acc: 0.9796, best val acc: 0.9703, best train loss: 0.0630  @ epoch 25865 )\n",
      "[Epoch: 26100] train loss: 0.0695, train acc: 0.9758, val loss: 0.1835, val acc: 0.9639  (best train acc: 0.9798, best val acc: 0.9703, best train loss: 0.0630  @ epoch 25865 )\n",
      "[Epoch: 26120] train loss: 0.0670, train acc: 0.9765, val loss: 0.1629, val acc: 0.9649  (best train acc: 0.9798, best val acc: 0.9703, best train loss: 0.0630  @ epoch 25865 )\n",
      "[Epoch: 26140] train loss: 0.0731, train acc: 0.9782, val loss: 0.1756, val acc: 0.9666  (best train acc: 0.9798, best val acc: 0.9703, best train loss: 0.0630  @ epoch 25865 )\n",
      "[Epoch: 26160] train loss: 0.0690, train acc: 0.9767, val loss: 0.1580, val acc: 0.9653  (best train acc: 0.9798, best val acc: 0.9703, best train loss: 0.0630  @ epoch 25865 )\n",
      "[Epoch: 26180] train loss: 0.0665, train acc: 0.9780, val loss: 0.1646, val acc: 0.9646  (best train acc: 0.9798, best val acc: 0.9703, best train loss: 0.0630  @ epoch 26162 )\n",
      "[Epoch: 26200] train loss: 0.1158, train acc: 0.9687, val loss: 0.1567, val acc: 0.9612  (best train acc: 0.9798, best val acc: 0.9703, best train loss: 0.0630  @ epoch 26162 )\n",
      "[Epoch: 26220] train loss: 0.0780, train acc: 0.9726, val loss: 0.1634, val acc: 0.9612  (best train acc: 0.9798, best val acc: 0.9703, best train loss: 0.0630  @ epoch 26162 )\n",
      "[Epoch: 26240] train loss: 0.0863, train acc: 0.9667, val loss: 0.1570, val acc: 0.9653  (best train acc: 0.9798, best val acc: 0.9703, best train loss: 0.0630  @ epoch 26162 )\n",
      "[Epoch: 26260] train loss: 0.0792, train acc: 0.9734, val loss: 0.1588, val acc: 0.9649  (best train acc: 0.9798, best val acc: 0.9703, best train loss: 0.0630  @ epoch 26162 )\n",
      "[Epoch: 26280] train loss: 0.0685, train acc: 0.9775, val loss: 0.1609, val acc: 0.9646  (best train acc: 0.9798, best val acc: 0.9703, best train loss: 0.0630  @ epoch 26162 )\n",
      "[Epoch: 26300] train loss: 0.0750, train acc: 0.9740, val loss: 0.1655, val acc: 0.9653  (best train acc: 0.9800, best val acc: 0.9703, best train loss: 0.0630  @ epoch 26162 )\n",
      "[Epoch: 26320] train loss: 0.0845, train acc: 0.9697, val loss: 0.1723, val acc: 0.9649  (best train acc: 0.9800, best val acc: 0.9703, best train loss: 0.0630  @ epoch 26162 )\n",
      "[Epoch: 26340] train loss: 0.0982, train acc: 0.9696, val loss: 0.1425, val acc: 0.9639  (best train acc: 0.9800, best val acc: 0.9703, best train loss: 0.0630  @ epoch 26162 )\n",
      "[Epoch: 26360] train loss: 0.0694, train acc: 0.9758, val loss: 0.1575, val acc: 0.9666  (best train acc: 0.9800, best val acc: 0.9703, best train loss: 0.0630  @ epoch 26162 )\n",
      "[Epoch: 26380] train loss: 0.0900, train acc: 0.9686, val loss: 0.1479, val acc: 0.9636  (best train acc: 0.9800, best val acc: 0.9703, best train loss: 0.0630  @ epoch 26162 )\n",
      "[Epoch: 26400] train loss: 0.0805, train acc: 0.9751, val loss: 0.1619, val acc: 0.9649  (best train acc: 0.9800, best val acc: 0.9703, best train loss: 0.0630  @ epoch 26162 )\n",
      "[Epoch: 26420] train loss: 0.0722, train acc: 0.9773, val loss: 0.1672, val acc: 0.9656  (best train acc: 0.9800, best val acc: 0.9703, best train loss: 0.0630  @ epoch 26162 )\n",
      "[Epoch: 26440] train loss: 0.0748, train acc: 0.9774, val loss: 0.1637, val acc: 0.9649  (best train acc: 0.9800, best val acc: 0.9703, best train loss: 0.0630  @ epoch 26162 )\n",
      "[Epoch: 26460] train loss: 0.0749, train acc: 0.9750, val loss: 0.1727, val acc: 0.9642  (best train acc: 0.9801, best val acc: 0.9703, best train loss: 0.0630  @ epoch 26162 )\n",
      "[Epoch: 26480] train loss: 0.0754, train acc: 0.9753, val loss: 0.1562, val acc: 0.9659  (best train acc: 0.9801, best val acc: 0.9703, best train loss: 0.0630  @ epoch 26162 )\n",
      "[Epoch: 26500] train loss: 0.0871, train acc: 0.9729, val loss: 0.1835, val acc: 0.9622  (best train acc: 0.9801, best val acc: 0.9703, best train loss: 0.0630  @ epoch 26162 )\n",
      "[Epoch: 26520] train loss: 0.0938, train acc: 0.9688, val loss: 0.1589, val acc: 0.9653  (best train acc: 0.9801, best val acc: 0.9703, best train loss: 0.0630  @ epoch 26162 )\n",
      "[Epoch: 26540] train loss: 0.0722, train acc: 0.9761, val loss: 0.1629, val acc: 0.9656  (best train acc: 0.9801, best val acc: 0.9703, best train loss: 0.0630  @ epoch 26162 )\n",
      "[Epoch: 26560] train loss: 0.0857, train acc: 0.9743, val loss: 0.1565, val acc: 0.9622  (best train acc: 0.9801, best val acc: 0.9703, best train loss: 0.0630  @ epoch 26162 )\n",
      "[Epoch: 26580] train loss: 0.0773, train acc: 0.9733, val loss: 0.1667, val acc: 0.9642  (best train acc: 0.9801, best val acc: 0.9703, best train loss: 0.0630  @ epoch 26162 )\n",
      "[Epoch: 26600] train loss: 0.0666, train acc: 0.9783, val loss: 0.1628, val acc: 0.9673  (best train acc: 0.9801, best val acc: 0.9703, best train loss: 0.0630  @ epoch 26162 )\n",
      "[Epoch: 26620] train loss: 0.0687, train acc: 0.9774, val loss: 0.1590, val acc: 0.9656  (best train acc: 0.9807, best val acc: 0.9703, best train loss: 0.0612  @ epoch 26614 )\n",
      "[Epoch: 26640] train loss: 0.0737, train acc: 0.9733, val loss: 0.1647, val acc: 0.9656  (best train acc: 0.9807, best val acc: 0.9703, best train loss: 0.0612  @ epoch 26614 )\n",
      "[Epoch: 26660] train loss: 0.0701, train acc: 0.9770, val loss: 0.1598, val acc: 0.9663  (best train acc: 0.9807, best val acc: 0.9703, best train loss: 0.0612  @ epoch 26614 )\n",
      "[Epoch: 26680] train loss: 0.0716, train acc: 0.9771, val loss: 0.1627, val acc: 0.9659  (best train acc: 0.9807, best val acc: 0.9703, best train loss: 0.0612  @ epoch 26614 )\n",
      "[Epoch: 26700] train loss: 0.0734, train acc: 0.9772, val loss: 0.1658, val acc: 0.9642  (best train acc: 0.9807, best val acc: 0.9703, best train loss: 0.0612  @ epoch 26614 )\n",
      "[Epoch: 26720] train loss: 0.0743, train acc: 0.9748, val loss: 0.1784, val acc: 0.9636  (best train acc: 0.9807, best val acc: 0.9703, best train loss: 0.0612  @ epoch 26614 )\n",
      "[Epoch: 26740] train loss: 0.0664, train acc: 0.9779, val loss: 0.1520, val acc: 0.9663  (best train acc: 0.9807, best val acc: 0.9703, best train loss: 0.0612  @ epoch 26614 )\n",
      "[Epoch: 26760] train loss: 0.0876, train acc: 0.9708, val loss: 0.1475, val acc: 0.9602  (best train acc: 0.9807, best val acc: 0.9703, best train loss: 0.0612  @ epoch 26614 )\n",
      "[Epoch: 26780] train loss: 0.0775, train acc: 0.9728, val loss: 0.1596, val acc: 0.9656  (best train acc: 0.9807, best val acc: 0.9703, best train loss: 0.0612  @ epoch 26614 )\n",
      "[Epoch: 26800] train loss: 0.0718, train acc: 0.9768, val loss: 0.1491, val acc: 0.9659  (best train acc: 0.9807, best val acc: 0.9703, best train loss: 0.0612  @ epoch 26614 )\n",
      "[Epoch: 26820] train loss: 0.0719, train acc: 0.9743, val loss: 0.1509, val acc: 0.9669  (best train acc: 0.9807, best val acc: 0.9703, best train loss: 0.0612  @ epoch 26614 )\n",
      "[Epoch: 26840] train loss: 0.0738, train acc: 0.9730, val loss: 0.1630, val acc: 0.9666  (best train acc: 0.9807, best val acc: 0.9703, best train loss: 0.0612  @ epoch 26614 )\n",
      "[Epoch: 26860] train loss: 0.0747, train acc: 0.9742, val loss: 0.1682, val acc: 0.9636  (best train acc: 0.9807, best val acc: 0.9703, best train loss: 0.0612  @ epoch 26614 )\n",
      "[Epoch: 26880] train loss: 0.0756, train acc: 0.9738, val loss: 0.1777, val acc: 0.9663  (best train acc: 0.9807, best val acc: 0.9703, best train loss: 0.0612  @ epoch 26614 )\n",
      "[Epoch: 26900] train loss: 0.0691, train acc: 0.9787, val loss: 0.1814, val acc: 0.9642  (best train acc: 0.9807, best val acc: 0.9703, best train loss: 0.0612  @ epoch 26614 )\n",
      "[Epoch: 26920] train loss: 0.0855, train acc: 0.9724, val loss: 0.1806, val acc: 0.9632  (best train acc: 0.9807, best val acc: 0.9703, best train loss: 0.0612  @ epoch 26614 )\n",
      "[Epoch: 26940] train loss: 0.0998, train acc: 0.9665, val loss: 0.1616, val acc: 0.9521  (best train acc: 0.9807, best val acc: 0.9703, best train loss: 0.0612  @ epoch 26614 )\n",
      "[Epoch: 26960] train loss: 0.0814, train acc: 0.9706, val loss: 0.1513, val acc: 0.9653  (best train acc: 0.9807, best val acc: 0.9703, best train loss: 0.0612  @ epoch 26614 )\n",
      "[Epoch: 26980] train loss: 0.0663, train acc: 0.9781, val loss: 0.1568, val acc: 0.9656  (best train acc: 0.9807, best val acc: 0.9703, best train loss: 0.0612  @ epoch 26614 )\n",
      "[Epoch: 27000] train loss: 0.0701, train acc: 0.9779, val loss: 0.1604, val acc: 0.9663  (best train acc: 0.9807, best val acc: 0.9703, best train loss: 0.0612  @ epoch 26614 )\n",
      "[Epoch: 27020] train loss: 0.0762, train acc: 0.9755, val loss: 0.1472, val acc: 0.9676  (best train acc: 0.9807, best val acc: 0.9703, best train loss: 0.0612  @ epoch 26614 )\n",
      "[Epoch: 27040] train loss: 0.0794, train acc: 0.9744, val loss: 0.1604, val acc: 0.9632  (best train acc: 0.9807, best val acc: 0.9703, best train loss: 0.0612  @ epoch 26614 )\n",
      "[Epoch: 27060] train loss: 0.0744, train acc: 0.9741, val loss: 0.1507, val acc: 0.9646  (best train acc: 0.9807, best val acc: 0.9703, best train loss: 0.0612  @ epoch 26614 )\n",
      "[Epoch: 27080] train loss: 0.0764, train acc: 0.9743, val loss: 0.1617, val acc: 0.9646  (best train acc: 0.9807, best val acc: 0.9703, best train loss: 0.0612  @ epoch 26614 )\n",
      "[Epoch: 27100] train loss: 0.0661, train acc: 0.9768, val loss: 0.1658, val acc: 0.9656  (best train acc: 0.9807, best val acc: 0.9703, best train loss: 0.0612  @ epoch 26614 )\n",
      "[Epoch: 27120] train loss: 0.0675, train acc: 0.9782, val loss: 0.1623, val acc: 0.9642  (best train acc: 0.9807, best val acc: 0.9703, best train loss: 0.0612  @ epoch 26614 )\n",
      "[Epoch: 27140] train loss: 0.1211, train acc: 0.9602, val loss: 0.1438, val acc: 0.9589  (best train acc: 0.9807, best val acc: 0.9703, best train loss: 0.0612  @ epoch 26614 )\n",
      "[Epoch: 27160] train loss: 0.0770, train acc: 0.9734, val loss: 0.1427, val acc: 0.9673  (best train acc: 0.9807, best val acc: 0.9703, best train loss: 0.0612  @ epoch 26614 )\n",
      "[Epoch: 27180] train loss: 0.0750, train acc: 0.9735, val loss: 0.1605, val acc: 0.9646  (best train acc: 0.9807, best val acc: 0.9703, best train loss: 0.0612  @ epoch 26614 )\n",
      "[Epoch: 27200] train loss: 0.0716, train acc: 0.9757, val loss: 0.1590, val acc: 0.9649  (best train acc: 0.9807, best val acc: 0.9703, best train loss: 0.0612  @ epoch 26614 )\n",
      "[Epoch: 27220] train loss: 0.0824, train acc: 0.9715, val loss: 0.1806, val acc: 0.9622  (best train acc: 0.9807, best val acc: 0.9703, best train loss: 0.0612  @ epoch 26614 )\n",
      "[Epoch: 27240] train loss: 0.0722, train acc: 0.9777, val loss: 0.1432, val acc: 0.9659  (best train acc: 0.9807, best val acc: 0.9703, best train loss: 0.0612  @ epoch 26614 )\n",
      "[Epoch: 27260] train loss: 0.1135, train acc: 0.9622, val loss: 0.1544, val acc: 0.9599  (best train acc: 0.9807, best val acc: 0.9703, best train loss: 0.0612  @ epoch 26614 )\n",
      "[Epoch: 27280] train loss: 0.0788, train acc: 0.9729, val loss: 0.1412, val acc: 0.9653  (best train acc: 0.9807, best val acc: 0.9703, best train loss: 0.0612  @ epoch 26614 )\n",
      "[Epoch: 27300] train loss: 0.0890, train acc: 0.9709, val loss: 0.1641, val acc: 0.9616  (best train acc: 0.9807, best val acc: 0.9703, best train loss: 0.0612  @ epoch 26614 )\n",
      "[Epoch: 27320] train loss: 0.0966, train acc: 0.9646, val loss: 0.1611, val acc: 0.9666  (best train acc: 0.9807, best val acc: 0.9703, best train loss: 0.0612  @ epoch 26614 )\n",
      "[Epoch: 27340] train loss: 0.0666, train acc: 0.9777, val loss: 0.1481, val acc: 0.9653  (best train acc: 0.9807, best val acc: 0.9703, best train loss: 0.0612  @ epoch 26614 )\n",
      "[Epoch: 27360] train loss: 0.0663, train acc: 0.9795, val loss: 0.1620, val acc: 0.9663  (best train acc: 0.9807, best val acc: 0.9703, best train loss: 0.0612  @ epoch 26614 )\n",
      "[Epoch: 27380] train loss: 0.0694, train acc: 0.9766, val loss: 0.1822, val acc: 0.9639  (best train acc: 0.9807, best val acc: 0.9703, best train loss: 0.0612  @ epoch 26614 )\n",
      "[Epoch: 27400] train loss: 0.0903, train acc: 0.9702, val loss: 0.1966, val acc: 0.9612  (best train acc: 0.9807, best val acc: 0.9703, best train loss: 0.0612  @ epoch 26614 )\n",
      "[Epoch: 27420] train loss: 0.0671, train acc: 0.9791, val loss: 0.1735, val acc: 0.9646  (best train acc: 0.9807, best val acc: 0.9703, best train loss: 0.0612  @ epoch 26614 )\n",
      "[Epoch: 27440] train loss: 0.0673, train acc: 0.9793, val loss: 0.1786, val acc: 0.9653  (best train acc: 0.9807, best val acc: 0.9703, best train loss: 0.0612  @ epoch 26614 )\n",
      "[Epoch: 27460] train loss: 0.0975, train acc: 0.9695, val loss: 0.1847, val acc: 0.9565  (best train acc: 0.9807, best val acc: 0.9703, best train loss: 0.0612  @ epoch 26614 )\n",
      "[Epoch: 27480] train loss: 0.0995, train acc: 0.9649, val loss: 0.1743, val acc: 0.9626  (best train acc: 0.9807, best val acc: 0.9703, best train loss: 0.0612  @ epoch 26614 )\n",
      "[Epoch: 27500] train loss: 0.0802, train acc: 0.9748, val loss: 0.1435, val acc: 0.9663  (best train acc: 0.9807, best val acc: 0.9703, best train loss: 0.0612  @ epoch 26614 )\n",
      "[Epoch: 27520] train loss: 0.0758, train acc: 0.9744, val loss: 0.1699, val acc: 0.9636  (best train acc: 0.9807, best val acc: 0.9703, best train loss: 0.0612  @ epoch 26614 )\n",
      "[Epoch: 27540] train loss: 0.0689, train acc: 0.9771, val loss: 0.1588, val acc: 0.9659  (best train acc: 0.9807, best val acc: 0.9703, best train loss: 0.0612  @ epoch 26614 )\n",
      "[Epoch: 27560] train loss: 0.0860, train acc: 0.9774, val loss: 0.1632, val acc: 0.9649  (best train acc: 0.9807, best val acc: 0.9703, best train loss: 0.0612  @ epoch 26614 )\n",
      "[Epoch: 27580] train loss: 0.0637, train acc: 0.9785, val loss: 0.1700, val acc: 0.9666  (best train acc: 0.9807, best val acc: 0.9703, best train loss: 0.0612  @ epoch 26614 )\n",
      "[Epoch: 27600] train loss: 0.0663, train acc: 0.9786, val loss: 0.1866, val acc: 0.9649  (best train acc: 0.9807, best val acc: 0.9703, best train loss: 0.0612  @ epoch 26614 )\n",
      "[Epoch: 27620] train loss: 0.0638, train acc: 0.9777, val loss: 0.1627, val acc: 0.9666  (best train acc: 0.9807, best val acc: 0.9703, best train loss: 0.0612  @ epoch 26614 )\n",
      "[Epoch: 27640] train loss: 0.0651, train acc: 0.9785, val loss: 0.1657, val acc: 0.9639  (best train acc: 0.9807, best val acc: 0.9703, best train loss: 0.0612  @ epoch 26614 )\n",
      "[Epoch: 27660] train loss: 0.0665, train acc: 0.9769, val loss: 0.1707, val acc: 0.9656  (best train acc: 0.9807, best val acc: 0.9703, best train loss: 0.0602  @ epoch 27647 )\n",
      "[Epoch: 27680] train loss: 0.0697, train acc: 0.9766, val loss: 0.1679, val acc: 0.9649  (best train acc: 0.9807, best val acc: 0.9703, best train loss: 0.0602  @ epoch 27647 )\n",
      "[Epoch: 27700] train loss: 0.0715, train acc: 0.9754, val loss: 0.1702, val acc: 0.9646  (best train acc: 0.9807, best val acc: 0.9703, best train loss: 0.0602  @ epoch 27647 )\n",
      "[Epoch: 27720] train loss: 0.0760, train acc: 0.9748, val loss: 0.1550, val acc: 0.9646  (best train acc: 0.9807, best val acc: 0.9703, best train loss: 0.0602  @ epoch 27647 )\n",
      "[Epoch: 27740] train loss: 0.0889, train acc: 0.9704, val loss: 0.1444, val acc: 0.9659  (best train acc: 0.9807, best val acc: 0.9703, best train loss: 0.0602  @ epoch 27647 )\n",
      "[Epoch: 27760] train loss: 0.0725, train acc: 0.9739, val loss: 0.1597, val acc: 0.9642  (best train acc: 0.9807, best val acc: 0.9703, best train loss: 0.0602  @ epoch 27647 )\n",
      "[Epoch: 27780] train loss: 0.1167, train acc: 0.9568, val loss: 0.1617, val acc: 0.9653  (best train acc: 0.9807, best val acc: 0.9703, best train loss: 0.0602  @ epoch 27647 )\n",
      "[Epoch: 27800] train loss: 0.0713, train acc: 0.9771, val loss: 0.1948, val acc: 0.9619  (best train acc: 0.9807, best val acc: 0.9703, best train loss: 0.0602  @ epoch 27647 )\n",
      "[Epoch: 27820] train loss: 0.0817, train acc: 0.9712, val loss: 0.1363, val acc: 0.9649  (best train acc: 0.9807, best val acc: 0.9703, best train loss: 0.0602  @ epoch 27647 )\n",
      "[Epoch: 27840] train loss: 0.0766, train acc: 0.9735, val loss: 0.1709, val acc: 0.9673  (best train acc: 0.9807, best val acc: 0.9703, best train loss: 0.0602  @ epoch 27647 )\n",
      "[Epoch: 27860] train loss: 0.0731, train acc: 0.9754, val loss: 0.1600, val acc: 0.9642  (best train acc: 0.9807, best val acc: 0.9703, best train loss: 0.0602  @ epoch 27647 )\n",
      "[Epoch: 27880] train loss: 0.0749, train acc: 0.9779, val loss: 0.1726, val acc: 0.9680  (best train acc: 0.9807, best val acc: 0.9703, best train loss: 0.0602  @ epoch 27647 )\n",
      "[Epoch: 27900] train loss: 0.0833, train acc: 0.9729, val loss: 0.1650, val acc: 0.9646  (best train acc: 0.9807, best val acc: 0.9703, best train loss: 0.0602  @ epoch 27647 )\n",
      "[Epoch: 27920] train loss: 0.0650, train acc: 0.9784, val loss: 0.1573, val acc: 0.9659  (best train acc: 0.9807, best val acc: 0.9703, best train loss: 0.0602  @ epoch 27647 )\n",
      "[Epoch: 27940] train loss: 0.0839, train acc: 0.9717, val loss: 0.1659, val acc: 0.9649  (best train acc: 0.9807, best val acc: 0.9703, best train loss: 0.0602  @ epoch 27647 )\n",
      "[Epoch: 27960] train loss: 0.0685, train acc: 0.9761, val loss: 0.1782, val acc: 0.9649  (best train acc: 0.9807, best val acc: 0.9703, best train loss: 0.0602  @ epoch 27647 )\n",
      "[Epoch: 27980] train loss: 0.0978, train acc: 0.9669, val loss: 0.1747, val acc: 0.9599  (best train acc: 0.9807, best val acc: 0.9703, best train loss: 0.0602  @ epoch 27647 )\n",
      "[Epoch: 28000] train loss: 0.0950, train acc: 0.9704, val loss: 0.1627, val acc: 0.9646  (best train acc: 0.9807, best val acc: 0.9703, best train loss: 0.0602  @ epoch 27647 )\n",
      "[Epoch: 28020] train loss: 0.0889, train acc: 0.9715, val loss: 0.1493, val acc: 0.9666  (best train acc: 0.9807, best val acc: 0.9703, best train loss: 0.0602  @ epoch 27647 )\n",
      "[Epoch: 28040] train loss: 0.0779, train acc: 0.9721, val loss: 0.1649, val acc: 0.9659  (best train acc: 0.9807, best val acc: 0.9703, best train loss: 0.0602  @ epoch 27647 )\n",
      "[Epoch: 28060] train loss: 0.0831, train acc: 0.9751, val loss: 0.1680, val acc: 0.9639  (best train acc: 0.9807, best val acc: 0.9703, best train loss: 0.0602  @ epoch 27647 )\n",
      "[Epoch: 28080] train loss: 0.0656, train acc: 0.9790, val loss: 0.1447, val acc: 0.9666  (best train acc: 0.9807, best val acc: 0.9703, best train loss: 0.0602  @ epoch 27647 )\n",
      "[Epoch: 28100] train loss: 0.0664, train acc: 0.9779, val loss: 0.1594, val acc: 0.9669  (best train acc: 0.9807, best val acc: 0.9703, best train loss: 0.0602  @ epoch 27647 )\n",
      "[Epoch: 28120] train loss: 0.0675, train acc: 0.9788, val loss: 0.1606, val acc: 0.9669  (best train acc: 0.9807, best val acc: 0.9703, best train loss: 0.0602  @ epoch 27647 )\n",
      "[Epoch: 28140] train loss: 0.0720, train acc: 0.9762, val loss: 0.1667, val acc: 0.9676  (best train acc: 0.9809, best val acc: 0.9703, best train loss: 0.0602  @ epoch 27647 )\n",
      "[Epoch: 28160] train loss: 0.0814, train acc: 0.9717, val loss: 0.1739, val acc: 0.9636  (best train acc: 0.9809, best val acc: 0.9703, best train loss: 0.0598  @ epoch 28152 )\n",
      "[Epoch: 28180] train loss: 0.0708, train acc: 0.9763, val loss: 0.1605, val acc: 0.9666  (best train acc: 0.9809, best val acc: 0.9703, best train loss: 0.0598  @ epoch 28152 )\n",
      "[Epoch: 28200] train loss: 0.0703, train acc: 0.9756, val loss: 0.1609, val acc: 0.9653  (best train acc: 0.9809, best val acc: 0.9703, best train loss: 0.0598  @ epoch 28152 )\n",
      "[Epoch: 28220] train loss: 0.0670, train acc: 0.9793, val loss: 0.1620, val acc: 0.9680  (best train acc: 0.9809, best val acc: 0.9703, best train loss: 0.0598  @ epoch 28152 )\n",
      "[Epoch: 28240] train loss: 0.0652, train acc: 0.9782, val loss: 0.1641, val acc: 0.9666  (best train acc: 0.9809, best val acc: 0.9703, best train loss: 0.0598  @ epoch 28152 )\n",
      "[Epoch: 28260] train loss: 0.0682, train acc: 0.9779, val loss: 0.1645, val acc: 0.9680  (best train acc: 0.9809, best val acc: 0.9703, best train loss: 0.0598  @ epoch 28152 )\n",
      "[Epoch: 28280] train loss: 0.0677, train acc: 0.9779, val loss: 0.1841, val acc: 0.9663  (best train acc: 0.9809, best val acc: 0.9703, best train loss: 0.0598  @ epoch 28152 )\n",
      "[Epoch: 28300] train loss: 0.0634, train acc: 0.9787, val loss: 0.1713, val acc: 0.9673  (best train acc: 0.9809, best val acc: 0.9703, best train loss: 0.0598  @ epoch 28152 )\n",
      "[Epoch: 28320] train loss: 0.0718, train acc: 0.9753, val loss: 0.1696, val acc: 0.9629  (best train acc: 0.9809, best val acc: 0.9703, best train loss: 0.0598  @ epoch 28152 )\n",
      "[Epoch: 28340] train loss: 0.0664, train acc: 0.9777, val loss: 0.1605, val acc: 0.9649  (best train acc: 0.9809, best val acc: 0.9703, best train loss: 0.0598  @ epoch 28152 )\n",
      "[Epoch: 28360] train loss: 0.0795, train acc: 0.9727, val loss: 0.1678, val acc: 0.9599  (best train acc: 0.9809, best val acc: 0.9703, best train loss: 0.0598  @ epoch 28152 )\n",
      "[Epoch: 28380] train loss: 0.0685, train acc: 0.9779, val loss: 0.1709, val acc: 0.9629  (best train acc: 0.9809, best val acc: 0.9703, best train loss: 0.0598  @ epoch 28152 )\n",
      "[Epoch: 28400] train loss: 0.0712, train acc: 0.9759, val loss: 0.1555, val acc: 0.9639  (best train acc: 0.9809, best val acc: 0.9703, best train loss: 0.0598  @ epoch 28152 )\n",
      "[Epoch: 28420] train loss: 0.0745, train acc: 0.9747, val loss: 0.1446, val acc: 0.9659  (best train acc: 0.9809, best val acc: 0.9703, best train loss: 0.0598  @ epoch 28152 )\n",
      "[Epoch: 28440] train loss: 0.0886, train acc: 0.9697, val loss: 0.1474, val acc: 0.9619  (best train acc: 0.9809, best val acc: 0.9703, best train loss: 0.0598  @ epoch 28152 )\n",
      "[Epoch: 28460] train loss: 0.0730, train acc: 0.9748, val loss: 0.1507, val acc: 0.9653  (best train acc: 0.9809, best val acc: 0.9703, best train loss: 0.0598  @ epoch 28152 )\n",
      "[Epoch: 28480] train loss: 0.0876, train acc: 0.9712, val loss: 0.1978, val acc: 0.9545  (best train acc: 0.9809, best val acc: 0.9703, best train loss: 0.0598  @ epoch 28152 )\n",
      "[Epoch: 28500] train loss: 0.0733, train acc: 0.9751, val loss: 0.1694, val acc: 0.9642  (best train acc: 0.9809, best val acc: 0.9703, best train loss: 0.0598  @ epoch 28152 )\n",
      "[Epoch: 28520] train loss: 0.0809, train acc: 0.9744, val loss: 0.1618, val acc: 0.9680  (best train acc: 0.9809, best val acc: 0.9703, best train loss: 0.0598  @ epoch 28152 )\n",
      "[Epoch: 28540] train loss: 0.0807, train acc: 0.9741, val loss: 0.1700, val acc: 0.9639  (best train acc: 0.9809, best val acc: 0.9703, best train loss: 0.0598  @ epoch 28152 )\n",
      "[Epoch: 28560] train loss: 0.0641, train acc: 0.9791, val loss: 0.1598, val acc: 0.9669  (best train acc: 0.9809, best val acc: 0.9703, best train loss: 0.0598  @ epoch 28152 )\n",
      "[Epoch: 28580] train loss: 0.0817, train acc: 0.9697, val loss: 0.1442, val acc: 0.9673  (best train acc: 0.9809, best val acc: 0.9703, best train loss: 0.0598  @ epoch 28152 )\n",
      "[Epoch: 28600] train loss: 0.0703, train acc: 0.9764, val loss: 0.1619, val acc: 0.9646  (best train acc: 0.9809, best val acc: 0.9703, best train loss: 0.0598  @ epoch 28152 )\n",
      "[Epoch: 28620] train loss: 0.0669, train acc: 0.9778, val loss: 0.1586, val acc: 0.9642  (best train acc: 0.9809, best val acc: 0.9703, best train loss: 0.0598  @ epoch 28152 )\n",
      "[Epoch: 28640] train loss: 0.0717, train acc: 0.9798, val loss: 0.1571, val acc: 0.9669  (best train acc: 0.9809, best val acc: 0.9703, best train loss: 0.0598  @ epoch 28152 )\n",
      "[Epoch: 28660] train loss: 0.0786, train acc: 0.9731, val loss: 0.1695, val acc: 0.9666  (best train acc: 0.9820, best val acc: 0.9703, best train loss: 0.0598  @ epoch 28152 )\n",
      "[Epoch: 28680] train loss: 0.0739, train acc: 0.9758, val loss: 0.1650, val acc: 0.9649  (best train acc: 0.9820, best val acc: 0.9703, best train loss: 0.0598  @ epoch 28152 )\n",
      "[Epoch: 28700] train loss: 0.0865, train acc: 0.9665, val loss: 0.1613, val acc: 0.9653  (best train acc: 0.9820, best val acc: 0.9703, best train loss: 0.0598  @ epoch 28152 )\n",
      "[Epoch: 28720] train loss: 0.0669, train acc: 0.9782, val loss: 0.1590, val acc: 0.9656  (best train acc: 0.9820, best val acc: 0.9703, best train loss: 0.0598  @ epoch 28152 )\n",
      "[Epoch: 28740] train loss: 0.0685, train acc: 0.9774, val loss: 0.1617, val acc: 0.9676  (best train acc: 0.9820, best val acc: 0.9703, best train loss: 0.0598  @ epoch 28152 )\n",
      "[Epoch: 28760] train loss: 0.0897, train acc: 0.9702, val loss: 0.1624, val acc: 0.9612  (best train acc: 0.9820, best val acc: 0.9703, best train loss: 0.0598  @ epoch 28152 )\n",
      "[Epoch: 28780] train loss: 0.0852, train acc: 0.9738, val loss: 0.1845, val acc: 0.9622  (best train acc: 0.9820, best val acc: 0.9703, best train loss: 0.0598  @ epoch 28152 )\n",
      "[Epoch: 28800] train loss: 0.0772, train acc: 0.9733, val loss: 0.1592, val acc: 0.9680  (best train acc: 0.9820, best val acc: 0.9703, best train loss: 0.0598  @ epoch 28152 )\n",
      "[Epoch: 28820] train loss: 0.0684, train acc: 0.9769, val loss: 0.1588, val acc: 0.9669  (best train acc: 0.9820, best val acc: 0.9703, best train loss: 0.0598  @ epoch 28152 )\n",
      "[Epoch: 28840] train loss: 0.0827, train acc: 0.9730, val loss: 0.1440, val acc: 0.9632  (best train acc: 0.9820, best val acc: 0.9703, best train loss: 0.0598  @ epoch 28152 )\n",
      "[Epoch: 28860] train loss: 0.0687, train acc: 0.9760, val loss: 0.1717, val acc: 0.9646  (best train acc: 0.9820, best val acc: 0.9703, best train loss: 0.0598  @ epoch 28152 )\n",
      "[Epoch: 28880] train loss: 0.0740, train acc: 0.9754, val loss: 0.1821, val acc: 0.9653  (best train acc: 0.9820, best val acc: 0.9703, best train loss: 0.0598  @ epoch 28152 )\n",
      "[Epoch: 28900] train loss: 0.0689, train acc: 0.9751, val loss: 0.1804, val acc: 0.9653  (best train acc: 0.9820, best val acc: 0.9703, best train loss: 0.0598  @ epoch 28152 )\n",
      "[Epoch: 28920] train loss: 0.0717, train acc: 0.9746, val loss: 0.1680, val acc: 0.9642  (best train acc: 0.9820, best val acc: 0.9703, best train loss: 0.0597  @ epoch 28914 )\n",
      "[Epoch: 28940] train loss: 0.0818, train acc: 0.9707, val loss: 0.1719, val acc: 0.9659  (best train acc: 0.9820, best val acc: 0.9703, best train loss: 0.0597  @ epoch 28914 )\n",
      "[Epoch: 28960] train loss: 0.0785, train acc: 0.9722, val loss: 0.1690, val acc: 0.9646  (best train acc: 0.9820, best val acc: 0.9703, best train loss: 0.0597  @ epoch 28914 )\n",
      "[Epoch: 28980] train loss: 0.0681, train acc: 0.9768, val loss: 0.1602, val acc: 0.9676  (best train acc: 0.9820, best val acc: 0.9703, best train loss: 0.0597  @ epoch 28914 )\n",
      "[Epoch: 29000] train loss: 0.0757, train acc: 0.9751, val loss: 0.1935, val acc: 0.9636  (best train acc: 0.9820, best val acc: 0.9703, best train loss: 0.0597  @ epoch 28914 )\n",
      "[Epoch: 29020] train loss: 0.0765, train acc: 0.9753, val loss: 0.1938, val acc: 0.9595  (best train acc: 0.9820, best val acc: 0.9703, best train loss: 0.0597  @ epoch 28914 )\n",
      "[Epoch: 29040] train loss: 0.0900, train acc: 0.9685, val loss: 0.1749, val acc: 0.9629  (best train acc: 0.9820, best val acc: 0.9703, best train loss: 0.0597  @ epoch 28914 )\n",
      "[Epoch: 29060] train loss: 0.0694, train acc: 0.9754, val loss: 0.1721, val acc: 0.9632  (best train acc: 0.9820, best val acc: 0.9703, best train loss: 0.0597  @ epoch 28914 )\n",
      "[Epoch: 29080] train loss: 0.0677, train acc: 0.9748, val loss: 0.1794, val acc: 0.9663  (best train acc: 0.9820, best val acc: 0.9703, best train loss: 0.0589  @ epoch 29068 )\n",
      "[Epoch: 29100] train loss: 0.0613, train acc: 0.9785, val loss: 0.1843, val acc: 0.9646  (best train acc: 0.9820, best val acc: 0.9703, best train loss: 0.0589  @ epoch 29068 )\n",
      "[Epoch: 29120] train loss: 0.0680, train acc: 0.9779, val loss: 0.1694, val acc: 0.9663  (best train acc: 0.9820, best val acc: 0.9703, best train loss: 0.0582  @ epoch 29115 )\n",
      "[Epoch: 29140] train loss: 0.0873, train acc: 0.9794, val loss: 0.1809, val acc: 0.9663  (best train acc: 0.9820, best val acc: 0.9703, best train loss: 0.0582  @ epoch 29115 )\n",
      "[Epoch: 29160] train loss: 0.0721, train acc: 0.9750, val loss: 0.1564, val acc: 0.9656  (best train acc: 0.9820, best val acc: 0.9703, best train loss: 0.0582  @ epoch 29115 )\n",
      "[Epoch: 29180] train loss: 0.0640, train acc: 0.9787, val loss: 0.1662, val acc: 0.9653  (best train acc: 0.9820, best val acc: 0.9703, best train loss: 0.0582  @ epoch 29115 )\n",
      "[Epoch: 29200] train loss: 0.0901, train acc: 0.9676, val loss: 0.2093, val acc: 0.9578  (best train acc: 0.9820, best val acc: 0.9703, best train loss: 0.0582  @ epoch 29115 )\n",
      "[Epoch: 29220] train loss: 0.0866, train acc: 0.9718, val loss: 0.1859, val acc: 0.9649  (best train acc: 0.9820, best val acc: 0.9703, best train loss: 0.0582  @ epoch 29115 )\n",
      "[Epoch: 29240] train loss: 0.0715, train acc: 0.9764, val loss: 0.1705, val acc: 0.9649  (best train acc: 0.9820, best val acc: 0.9703, best train loss: 0.0582  @ epoch 29115 )\n",
      "[Epoch: 29260] train loss: 0.0766, train acc: 0.9730, val loss: 0.1638, val acc: 0.9659  (best train acc: 0.9820, best val acc: 0.9703, best train loss: 0.0582  @ epoch 29115 )\n",
      "[Epoch: 29280] train loss: 0.0684, train acc: 0.9761, val loss: 0.1610, val acc: 0.9659  (best train acc: 0.9820, best val acc: 0.9703, best train loss: 0.0582  @ epoch 29115 )\n",
      "[Epoch: 29300] train loss: 0.0740, train acc: 0.9756, val loss: 0.1760, val acc: 0.9653  (best train acc: 0.9820, best val acc: 0.9703, best train loss: 0.0582  @ epoch 29115 )\n",
      "[Epoch: 29320] train loss: 0.0630, train acc: 0.9801, val loss: 0.1740, val acc: 0.9669  (best train acc: 0.9820, best val acc: 0.9703, best train loss: 0.0582  @ epoch 29115 )\n",
      "[Epoch: 29340] train loss: 0.0652, train acc: 0.9784, val loss: 0.1776, val acc: 0.9673  (best train acc: 0.9820, best val acc: 0.9703, best train loss: 0.0582  @ epoch 29115 )\n",
      "[Epoch: 29360] train loss: 0.0650, train acc: 0.9782, val loss: 0.1780, val acc: 0.9642  (best train acc: 0.9820, best val acc: 0.9703, best train loss: 0.0582  @ epoch 29115 )\n",
      "[Epoch: 29380] train loss: 0.0654, train acc: 0.9785, val loss: 0.1648, val acc: 0.9676  (best train acc: 0.9820, best val acc: 0.9703, best train loss: 0.0582  @ epoch 29115 )\n",
      "[Epoch: 29400] train loss: 0.0646, train acc: 0.9786, val loss: 0.1678, val acc: 0.9673  (best train acc: 0.9820, best val acc: 0.9703, best train loss: 0.0582  @ epoch 29115 )\n",
      "[Epoch: 29420] train loss: 0.0635, train acc: 0.9793, val loss: 0.1691, val acc: 0.9683  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0577  @ epoch 29408 )\n",
      "[Epoch: 29440] train loss: 0.0983, train acc: 0.9680, val loss: 0.2585, val acc: 0.9487  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0577  @ epoch 29408 )\n",
      "[Epoch: 29460] train loss: 0.0899, train acc: 0.9614, val loss: 0.1765, val acc: 0.9616  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0577  @ epoch 29408 )\n",
      "[Epoch: 29480] train loss: 0.0728, train acc: 0.9753, val loss: 0.1657, val acc: 0.9659  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0577  @ epoch 29408 )\n",
      "[Epoch: 29500] train loss: 0.0762, train acc: 0.9739, val loss: 0.1834, val acc: 0.9653  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0577  @ epoch 29408 )\n",
      "[Epoch: 29520] train loss: 0.0726, train acc: 0.9777, val loss: 0.1334, val acc: 0.9676  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0577  @ epoch 29408 )\n",
      "[Epoch: 29540] train loss: 0.0743, train acc: 0.9751, val loss: 0.1674, val acc: 0.9669  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0577  @ epoch 29408 )\n",
      "[Epoch: 29560] train loss: 0.0673, train acc: 0.9775, val loss: 0.1551, val acc: 0.9673  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0577  @ epoch 29408 )\n",
      "[Epoch: 29580] train loss: 0.0911, train acc: 0.9720, val loss: 0.1708, val acc: 0.9595  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0577  @ epoch 29408 )\n",
      "[Epoch: 29600] train loss: 0.0624, train acc: 0.9790, val loss: 0.1691, val acc: 0.9680  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0577  @ epoch 29408 )\n",
      "[Epoch: 29620] train loss: 0.0738, train acc: 0.9725, val loss: 0.1750, val acc: 0.9649  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0577  @ epoch 29408 )\n",
      "[Epoch: 29640] train loss: 0.0658, train acc: 0.9790, val loss: 0.1748, val acc: 0.9686  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0577  @ epoch 29408 )\n",
      "[Epoch: 29660] train loss: 0.0617, train acc: 0.9801, val loss: 0.1773, val acc: 0.9632  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0577  @ epoch 29408 )\n",
      "[Epoch: 29680] train loss: 0.0701, train acc: 0.9783, val loss: 0.1749, val acc: 0.9653  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0577  @ epoch 29408 )\n",
      "[Epoch: 29700] train loss: 0.0587, train acc: 0.9812, val loss: 0.1764, val acc: 0.9673  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0577  @ epoch 29408 )\n",
      "[Epoch: 29720] train loss: 0.0742, train acc: 0.9753, val loss: 0.1534, val acc: 0.9659  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0577  @ epoch 29408 )\n",
      "[Epoch: 29740] train loss: 0.0627, train acc: 0.9802, val loss: 0.1677, val acc: 0.9663  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0577  @ epoch 29408 )\n",
      "[Epoch: 29760] train loss: 0.0613, train acc: 0.9790, val loss: 0.1588, val acc: 0.9683  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0577  @ epoch 29408 )\n",
      "[Epoch: 29780] train loss: 0.1330, train acc: 0.9575, val loss: 0.1854, val acc: 0.9599  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0577  @ epoch 29408 )\n",
      "[Epoch: 29800] train loss: 0.0826, train acc: 0.9728, val loss: 0.1554, val acc: 0.9666  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0577  @ epoch 29408 )\n",
      "[Epoch: 29820] train loss: 0.0699, train acc: 0.9767, val loss: 0.1671, val acc: 0.9669  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0577  @ epoch 29408 )\n",
      "[Epoch: 29840] train loss: 0.1073, train acc: 0.9669, val loss: 0.2537, val acc: 0.9514  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0577  @ epoch 29408 )\n",
      "[Epoch: 29860] train loss: 0.0809, train acc: 0.9726, val loss: 0.1710, val acc: 0.9656  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0577  @ epoch 29408 )\n",
      "[Epoch: 29880] train loss: 0.0691, train acc: 0.9761, val loss: 0.1608, val acc: 0.9666  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0577  @ epoch 29408 )\n",
      "[Epoch: 29900] train loss: 0.0758, train acc: 0.9738, val loss: 0.1488, val acc: 0.9632  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0577  @ epoch 29408 )\n",
      "[Epoch: 29920] train loss: 0.0698, train acc: 0.9751, val loss: 0.1783, val acc: 0.9646  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0577  @ epoch 29408 )\n",
      "[Epoch: 29940] train loss: 0.0760, train acc: 0.9787, val loss: 0.1741, val acc: 0.9680  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0577  @ epoch 29408 )\n",
      "[Epoch: 29960] train loss: 0.0627, train acc: 0.9793, val loss: 0.1704, val acc: 0.9663  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0577  @ epoch 29408 )\n",
      "[Epoch: 29980] train loss: 0.0630, train acc: 0.9793, val loss: 0.1554, val acc: 0.9666  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0577  @ epoch 29408 )\n",
      "[Epoch: 30000] train loss: 0.0650, train acc: 0.9772, val loss: 0.1807, val acc: 0.9656  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0577  @ epoch 29408 )\n",
      "[Epoch: 30020] train loss: 0.0708, train acc: 0.9748, val loss: 0.1706, val acc: 0.9676  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0577  @ epoch 29408 )\n",
      "[Epoch: 30040] train loss: 0.0588, train acc: 0.9810, val loss: 0.1549, val acc: 0.9659  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0577  @ epoch 29408 )\n",
      "[Epoch: 30060] train loss: 0.0725, train acc: 0.9753, val loss: 0.1672, val acc: 0.9646  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0577  @ epoch 29408 )\n",
      "[Epoch: 30080] train loss: 0.0800, train acc: 0.9732, val loss: 0.1597, val acc: 0.9666  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0577  @ epoch 29408 )\n",
      "[Epoch: 30100] train loss: 0.0621, train acc: 0.9800, val loss: 0.1691, val acc: 0.9673  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0577  @ epoch 29408 )\n",
      "[Epoch: 30120] train loss: 0.0696, train acc: 0.9761, val loss: 0.1587, val acc: 0.9642  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0577  @ epoch 29408 )\n",
      "[Epoch: 30140] train loss: 0.0632, train acc: 0.9782, val loss: 0.1616, val acc: 0.9599  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0577  @ epoch 29408 )\n",
      "[Epoch: 30160] train loss: 0.1083, train acc: 0.9661, val loss: 0.1879, val acc: 0.9477  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0577  @ epoch 29408 )\n",
      "[Epoch: 30180] train loss: 0.1122, train acc: 0.9641, val loss: 0.1738, val acc: 0.9656  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0577  @ epoch 29408 )\n",
      "[Epoch: 30200] train loss: 0.0660, train acc: 0.9795, val loss: 0.1519, val acc: 0.9690  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0577  @ epoch 29408 )\n",
      "[Epoch: 30220] train loss: 0.0653, train acc: 0.9777, val loss: 0.1550, val acc: 0.9690  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0577  @ epoch 29408 )\n",
      "[Epoch: 30240] train loss: 0.0723, train acc: 0.9742, val loss: 0.1577, val acc: 0.9676  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0577  @ epoch 29408 )\n",
      "[Epoch: 30260] train loss: 0.0751, train acc: 0.9743, val loss: 0.1523, val acc: 0.9676  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0577  @ epoch 29408 )\n",
      "[Epoch: 30280] train loss: 0.0779, train acc: 0.9730, val loss: 0.1603, val acc: 0.9656  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0577  @ epoch 29408 )\n",
      "[Epoch: 30300] train loss: 0.0679, train acc: 0.9767, val loss: 0.1630, val acc: 0.9686  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0577  @ epoch 29408 )\n",
      "[Epoch: 30320] train loss: 0.0639, train acc: 0.9785, val loss: 0.1522, val acc: 0.9676  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0577  @ epoch 29408 )\n",
      "[Epoch: 30340] train loss: 0.0604, train acc: 0.9807, val loss: 0.1708, val acc: 0.9659  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0577  @ epoch 29408 )\n",
      "[Epoch: 30360] train loss: 0.0622, train acc: 0.9801, val loss: 0.1625, val acc: 0.9663  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0577  @ epoch 29408 )\n",
      "[Epoch: 30380] train loss: 0.0718, train acc: 0.9759, val loss: 0.1671, val acc: 0.9656  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0577  @ epoch 29408 )\n",
      "[Epoch: 30400] train loss: 0.0638, train acc: 0.9786, val loss: 0.1744, val acc: 0.9676  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0577  @ epoch 29408 )\n",
      "[Epoch: 30420] train loss: 0.0668, train acc: 0.9783, val loss: 0.1722, val acc: 0.9669  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0577  @ epoch 30413 )\n",
      "[Epoch: 30440] train loss: 0.0749, train acc: 0.9728, val loss: 0.1531, val acc: 0.9673  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0577  @ epoch 30413 )\n",
      "[Epoch: 30460] train loss: 0.0808, train acc: 0.9712, val loss: 0.1896, val acc: 0.9646  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0577  @ epoch 30413 )\n",
      "[Epoch: 30480] train loss: 0.0676, train acc: 0.9764, val loss: 0.1571, val acc: 0.9686  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0577  @ epoch 30413 )\n",
      "[Epoch: 30500] train loss: 0.0803, train acc: 0.9708, val loss: 0.1867, val acc: 0.9653  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0577  @ epoch 30413 )\n",
      "[Epoch: 30520] train loss: 0.0729, train acc: 0.9757, val loss: 0.1731, val acc: 0.9619  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0577  @ epoch 30413 )\n",
      "[Epoch: 30540] train loss: 0.0802, train acc: 0.9712, val loss: 0.1765, val acc: 0.9656  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0577  @ epoch 30413 )\n",
      "[Epoch: 30560] train loss: 0.0744, train acc: 0.9750, val loss: 0.1647, val acc: 0.9669  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0577  @ epoch 30413 )\n",
      "[Epoch: 30580] train loss: 0.0706, train acc: 0.9755, val loss: 0.1772, val acc: 0.9649  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0577  @ epoch 30413 )\n",
      "[Epoch: 30600] train loss: 0.0648, train acc: 0.9781, val loss: 0.1635, val acc: 0.9656  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0577  @ epoch 30413 )\n",
      "[Epoch: 30620] train loss: 0.0712, train acc: 0.9761, val loss: 0.1728, val acc: 0.9632  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0577  @ epoch 30413 )\n",
      "[Epoch: 30640] train loss: 0.0721, train acc: 0.9748, val loss: 0.1661, val acc: 0.9649  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0577  @ epoch 30413 )\n",
      "[Epoch: 30660] train loss: 0.0847, train acc: 0.9657, val loss: 0.2314, val acc: 0.9457  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0577  @ epoch 30413 )\n",
      "[Epoch: 30680] train loss: 0.0754, train acc: 0.9727, val loss: 0.1766, val acc: 0.9639  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0577  @ epoch 30413 )\n",
      "[Epoch: 30700] train loss: 0.0645, train acc: 0.9797, val loss: 0.1699, val acc: 0.9676  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0577  @ epoch 30413 )\n",
      "[Epoch: 30720] train loss: 0.0644, train acc: 0.9795, val loss: 0.1789, val acc: 0.9686  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0577  @ epoch 30413 )\n",
      "[Epoch: 30740] train loss: 0.0638, train acc: 0.9777, val loss: 0.1719, val acc: 0.9676  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0576  @ epoch 30738 )\n",
      "[Epoch: 30760] train loss: 0.0653, train acc: 0.9817, val loss: 0.1769, val acc: 0.9656  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0576  @ epoch 30738 )\n",
      "[Epoch: 30780] train loss: 0.0693, train acc: 0.9772, val loss: 0.1850, val acc: 0.9666  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0576  @ epoch 30738 )\n",
      "[Epoch: 30800] train loss: 0.0633, train acc: 0.9779, val loss: 0.1813, val acc: 0.9666  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0576  @ epoch 30738 )\n",
      "[Epoch: 30820] train loss: 0.0686, train acc: 0.9774, val loss: 0.1679, val acc: 0.9659  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0576  @ epoch 30738 )\n",
      "[Epoch: 30840] train loss: 0.0591, train acc: 0.9803, val loss: 0.1847, val acc: 0.9656  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0576  @ epoch 30738 )\n",
      "[Epoch: 30860] train loss: 0.0692, train acc: 0.9772, val loss: 0.1748, val acc: 0.9649  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0576  @ epoch 30738 )\n",
      "[Epoch: 30880] train loss: 0.0666, train acc: 0.9780, val loss: 0.1719, val acc: 0.9663  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0576  @ epoch 30738 )\n",
      "[Epoch: 30900] train loss: 0.0735, train acc: 0.9755, val loss: 0.1910, val acc: 0.9565  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0576  @ epoch 30738 )\n",
      "[Epoch: 30920] train loss: 0.0866, train acc: 0.9723, val loss: 0.1721, val acc: 0.9673  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0576  @ epoch 30738 )\n",
      "[Epoch: 30940] train loss: 0.0695, train acc: 0.9757, val loss: 0.1860, val acc: 0.9669  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0576  @ epoch 30738 )\n",
      "[Epoch: 30960] train loss: 0.0612, train acc: 0.9803, val loss: 0.1673, val acc: 0.9666  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0576  @ epoch 30738 )\n",
      "[Epoch: 30980] train loss: 0.0645, train acc: 0.9758, val loss: 0.1657, val acc: 0.9656  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0576  @ epoch 30738 )\n",
      "[Epoch: 31000] train loss: 0.0726, train acc: 0.9788, val loss: 0.1911, val acc: 0.9629  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0576  @ epoch 30738 )\n",
      "[Epoch: 31020] train loss: 0.0814, train acc: 0.9732, val loss: 0.1912, val acc: 0.9649  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0576  @ epoch 30738 )\n",
      "[Epoch: 31040] train loss: 0.0754, train acc: 0.9766, val loss: 0.1533, val acc: 0.9629  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0576  @ epoch 30738 )\n",
      "[Epoch: 31060] train loss: 0.0644, train acc: 0.9785, val loss: 0.1709, val acc: 0.9680  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0576  @ epoch 30738 )\n",
      "[Epoch: 31080] train loss: 0.0671, train acc: 0.9777, val loss: 0.1692, val acc: 0.9663  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0576  @ epoch 30738 )\n",
      "[Epoch: 31100] train loss: 0.0784, train acc: 0.9744, val loss: 0.2019, val acc: 0.9622  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0576  @ epoch 30738 )\n",
      "[Epoch: 31120] train loss: 0.0655, train acc: 0.9771, val loss: 0.1816, val acc: 0.9683  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0576  @ epoch 30738 )\n",
      "[Epoch: 31140] train loss: 0.0635, train acc: 0.9791, val loss: 0.1773, val acc: 0.9632  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0576  @ epoch 30738 )\n",
      "[Epoch: 31160] train loss: 0.0665, train acc: 0.9761, val loss: 0.1743, val acc: 0.9676  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0576  @ epoch 30738 )\n",
      "[Epoch: 31180] train loss: 0.0654, train acc: 0.9770, val loss: 0.1721, val acc: 0.9659  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0576  @ epoch 30738 )\n",
      "[Epoch: 31200] train loss: 0.0762, train acc: 0.9730, val loss: 0.1643, val acc: 0.9639  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0576  @ epoch 30738 )\n",
      "[Epoch: 31220] train loss: 0.0730, train acc: 0.9765, val loss: 0.1575, val acc: 0.9666  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0576  @ epoch 30738 )\n",
      "[Epoch: 31240] train loss: 0.0661, train acc: 0.9787, val loss: 0.1836, val acc: 0.9663  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0576  @ epoch 30738 )\n",
      "[Epoch: 31260] train loss: 0.0774, train acc: 0.9728, val loss: 0.1737, val acc: 0.9629  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0576  @ epoch 30738 )\n",
      "[Epoch: 31280] train loss: 0.0941, train acc: 0.9680, val loss: 0.1709, val acc: 0.9656  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0576  @ epoch 30738 )\n",
      "[Epoch: 31300] train loss: 0.0782, train acc: 0.9733, val loss: 0.1929, val acc: 0.9656  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0576  @ epoch 30738 )\n",
      "[Epoch: 31320] train loss: 0.0737, train acc: 0.9751, val loss: 0.1701, val acc: 0.9666  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0576  @ epoch 30738 )\n",
      "[Epoch: 31340] train loss: 0.0693, train acc: 0.9772, val loss: 0.1977, val acc: 0.9649  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0576  @ epoch 30738 )\n",
      "[Epoch: 31360] train loss: 0.0623, train acc: 0.9799, val loss: 0.1977, val acc: 0.9666  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0576  @ epoch 30738 )\n",
      "[Epoch: 31380] train loss: 0.0638, train acc: 0.9764, val loss: 0.1723, val acc: 0.9693  (best train acc: 0.9821, best val acc: 0.9703, best train loss: 0.0576  @ epoch 30738 )\n",
      "[Epoch: 31400] train loss: 0.0675, train acc: 0.9775, val loss: 0.1928, val acc: 0.9666  (best train acc: 0.9827, best val acc: 0.9703, best train loss: 0.0564  @ epoch 31398 )\n",
      "[Epoch: 31420] train loss: 0.0681, train acc: 0.9769, val loss: 0.1778, val acc: 0.9653  (best train acc: 0.9827, best val acc: 0.9703, best train loss: 0.0564  @ epoch 31398 )\n",
      "[Epoch: 31440] train loss: 0.0651, train acc: 0.9787, val loss: 0.1526, val acc: 0.9676  (best train acc: 0.9827, best val acc: 0.9703, best train loss: 0.0564  @ epoch 31398 )\n",
      "[Epoch: 31460] train loss: 0.0796, train acc: 0.9733, val loss: 0.1849, val acc: 0.9680  (best train acc: 0.9827, best val acc: 0.9703, best train loss: 0.0564  @ epoch 31398 )\n",
      "[Epoch: 31480] train loss: 0.0755, train acc: 0.9755, val loss: 0.1962, val acc: 0.9605  (best train acc: 0.9827, best val acc: 0.9703, best train loss: 0.0564  @ epoch 31398 )\n",
      "[Epoch: 31500] train loss: 0.0668, train acc: 0.9776, val loss: 0.1661, val acc: 0.9653  (best train acc: 0.9827, best val acc: 0.9703, best train loss: 0.0564  @ epoch 31398 )\n",
      "[Epoch: 31520] train loss: 0.0674, train acc: 0.9768, val loss: 0.1639, val acc: 0.9683  (best train acc: 0.9827, best val acc: 0.9703, best train loss: 0.0564  @ epoch 31398 )\n",
      "[Epoch: 31540] train loss: 0.0616, train acc: 0.9798, val loss: 0.2034, val acc: 0.9653  (best train acc: 0.9827, best val acc: 0.9703, best train loss: 0.0564  @ epoch 31398 )\n",
      "[Epoch: 31560] train loss: 0.0695, train acc: 0.9767, val loss: 0.1967, val acc: 0.9568  (best train acc: 0.9827, best val acc: 0.9703, best train loss: 0.0564  @ epoch 31398 )\n",
      "[Epoch: 31580] train loss: 0.0817, train acc: 0.9687, val loss: 0.1748, val acc: 0.9653  (best train acc: 0.9827, best val acc: 0.9703, best train loss: 0.0564  @ epoch 31398 )\n",
      "[Epoch: 31600] train loss: 0.0626, train acc: 0.9790, val loss: 0.1895, val acc: 0.9680  (best train acc: 0.9827, best val acc: 0.9703, best train loss: 0.0564  @ epoch 31398 )\n",
      "[Epoch: 31620] train loss: 0.0647, train acc: 0.9777, val loss: 0.2121, val acc: 0.9649  (best train acc: 0.9827, best val acc: 0.9703, best train loss: 0.0564  @ epoch 31398 )\n",
      "[Epoch: 31640] train loss: 0.0826, train acc: 0.9753, val loss: 0.1630, val acc: 0.9649  (best train acc: 0.9827, best val acc: 0.9703, best train loss: 0.0564  @ epoch 31398 )\n",
      "[Epoch: 31660] train loss: 0.0624, train acc: 0.9802, val loss: 0.1735, val acc: 0.9676  (best train acc: 0.9827, best val acc: 0.9703, best train loss: 0.0564  @ epoch 31398 )\n",
      "[Epoch: 31680] train loss: 0.0884, train acc: 0.9677, val loss: 0.1997, val acc: 0.9555  (best train acc: 0.9827, best val acc: 0.9703, best train loss: 0.0564  @ epoch 31398 )\n",
      "[Epoch: 31700] train loss: 0.0734, train acc: 0.9740, val loss: 0.2202, val acc: 0.9656  (best train acc: 0.9827, best val acc: 0.9703, best train loss: 0.0564  @ epoch 31398 )\n",
      "[Epoch: 31720] train loss: 0.0668, train acc: 0.9774, val loss: 0.1761, val acc: 0.9673  (best train acc: 0.9827, best val acc: 0.9703, best train loss: 0.0564  @ epoch 31398 )\n",
      "[Epoch: 31740] train loss: 0.0664, train acc: 0.9786, val loss: 0.1927, val acc: 0.9676  (best train acc: 0.9827, best val acc: 0.9703, best train loss: 0.0564  @ epoch 31398 )\n",
      "[Epoch: 31760] train loss: 0.0757, train acc: 0.9732, val loss: 0.2121, val acc: 0.9642  (best train acc: 0.9827, best val acc: 0.9703, best train loss: 0.0564  @ epoch 31398 )\n",
      "[Epoch: 31780] train loss: 0.0594, train acc: 0.9809, val loss: 0.1733, val acc: 0.9680  (best train acc: 0.9827, best val acc: 0.9703, best train loss: 0.0564  @ epoch 31398 )\n",
      "[Epoch: 31800] train loss: 0.0718, train acc: 0.9749, val loss: 0.1925, val acc: 0.9663  (best train acc: 0.9827, best val acc: 0.9703, best train loss: 0.0564  @ epoch 31398 )\n",
      "[Epoch: 31820] train loss: 0.0656, train acc: 0.9787, val loss: 0.1797, val acc: 0.9656  (best train acc: 0.9827, best val acc: 0.9703, best train loss: 0.0564  @ epoch 31398 )\n",
      "[Epoch: 31840] train loss: 0.0724, train acc: 0.9769, val loss: 0.1580, val acc: 0.9653  (best train acc: 0.9827, best val acc: 0.9703, best train loss: 0.0564  @ epoch 31398 )\n",
      "[Epoch: 31860] train loss: 0.0904, train acc: 0.9722, val loss: 0.1388, val acc: 0.9669  (best train acc: 0.9827, best val acc: 0.9703, best train loss: 0.0564  @ epoch 31398 )\n",
      "[Epoch: 31880] train loss: 0.0866, train acc: 0.9703, val loss: 0.1656, val acc: 0.9673  (best train acc: 0.9827, best val acc: 0.9703, best train loss: 0.0564  @ epoch 31398 )\n",
      "[Epoch: 31900] train loss: 0.0710, train acc: 0.9753, val loss: 0.1454, val acc: 0.9693  (best train acc: 0.9827, best val acc: 0.9703, best train loss: 0.0564  @ epoch 31398 )\n",
      "[Epoch: 31920] train loss: 0.0693, train acc: 0.9772, val loss: 0.1560, val acc: 0.9683  (best train acc: 0.9827, best val acc: 0.9703, best train loss: 0.0564  @ epoch 31398 )\n",
      "[Epoch: 31940] train loss: 0.0702, train acc: 0.9751, val loss: 0.1952, val acc: 0.9639  (best train acc: 0.9827, best val acc: 0.9703, best train loss: 0.0564  @ epoch 31398 )\n",
      "[Epoch: 31960] train loss: 0.0805, train acc: 0.9725, val loss: 0.1766, val acc: 0.9659  (best train acc: 0.9827, best val acc: 0.9703, best train loss: 0.0564  @ epoch 31398 )\n",
      "[Epoch: 31980] train loss: 0.0751, train acc: 0.9748, val loss: 0.1777, val acc: 0.9659  (best train acc: 0.9827, best val acc: 0.9703, best train loss: 0.0564  @ epoch 31398 )\n",
      "[Epoch: 32000] train loss: 0.0797, train acc: 0.9735, val loss: 0.1468, val acc: 0.9673  (best train acc: 0.9827, best val acc: 0.9703, best train loss: 0.0564  @ epoch 31398 )\n",
      "[Epoch: 32020] train loss: 0.0593, train acc: 0.9799, val loss: 0.1805, val acc: 0.9676  (best train acc: 0.9827, best val acc: 0.9703, best train loss: 0.0564  @ epoch 31398 )\n",
      "[Epoch: 32040] train loss: 0.0778, train acc: 0.9697, val loss: 0.1550, val acc: 0.9676  (best train acc: 0.9827, best val acc: 0.9703, best train loss: 0.0564  @ epoch 31398 )\n",
      "[Epoch: 32060] train loss: 0.0610, train acc: 0.9808, val loss: 0.1578, val acc: 0.9690  (best train acc: 0.9827, best val acc: 0.9703, best train loss: 0.0564  @ epoch 31398 )\n",
      "[Epoch: 32080] train loss: 0.0657, train acc: 0.9771, val loss: 0.1872, val acc: 0.9676  (best train acc: 0.9827, best val acc: 0.9703, best train loss: 0.0564  @ epoch 31398 )\n",
      "[Epoch: 32100] train loss: 0.0644, train acc: 0.9787, val loss: 0.1694, val acc: 0.9690  (best train acc: 0.9827, best val acc: 0.9703, best train loss: 0.0564  @ epoch 31398 )\n",
      "[Epoch: 32120] train loss: 0.0727, train acc: 0.9779, val loss: 0.1798, val acc: 0.9669  (best train acc: 0.9827, best val acc: 0.9703, best train loss: 0.0558  @ epoch 32116 )\n",
      "[Epoch: 32140] train loss: 0.0656, train acc: 0.9793, val loss: 0.1849, val acc: 0.9690  (best train acc: 0.9827, best val acc: 0.9703, best train loss: 0.0558  @ epoch 32116 )\n",
      "[Epoch: 32160] train loss: 0.0628, train acc: 0.9787, val loss: 0.1745, val acc: 0.9649  (best train acc: 0.9827, best val acc: 0.9703, best train loss: 0.0558  @ epoch 32116 )\n",
      "[Epoch: 32180] train loss: 0.1247, train acc: 0.9511, val loss: 0.1884, val acc: 0.9518  (best train acc: 0.9827, best val acc: 0.9703, best train loss: 0.0558  @ epoch 32116 )\n",
      "[Epoch: 32200] train loss: 0.0817, train acc: 0.9706, val loss: 0.1681, val acc: 0.9686  (best train acc: 0.9827, best val acc: 0.9703, best train loss: 0.0558  @ epoch 32116 )\n",
      "[Epoch: 32220] train loss: 0.1038, train acc: 0.9587, val loss: 0.2046, val acc: 0.9352  (best train acc: 0.9827, best val acc: 0.9703, best train loss: 0.0558  @ epoch 32116 )\n",
      "[Epoch: 32240] train loss: 0.0694, train acc: 0.9788, val loss: 0.1651, val acc: 0.9666  (best train acc: 0.9827, best val acc: 0.9703, best train loss: 0.0558  @ epoch 32116 )\n",
      "[Epoch: 32260] train loss: 0.0615, train acc: 0.9803, val loss: 0.1838, val acc: 0.9666  (best train acc: 0.9827, best val acc: 0.9713, best train loss: 0.0558  @ epoch 32116 )\n",
      "[Epoch: 32280] train loss: 0.0787, train acc: 0.9751, val loss: 0.1743, val acc: 0.9673  (best train acc: 0.9827, best val acc: 0.9713, best train loss: 0.0558  @ epoch 32116 )\n",
      "[Epoch: 32300] train loss: 0.0652, train acc: 0.9783, val loss: 0.1629, val acc: 0.9700  (best train acc: 0.9827, best val acc: 0.9713, best train loss: 0.0558  @ epoch 32116 )\n",
      "[Epoch: 32320] train loss: 0.0629, train acc: 0.9790, val loss: 0.1899, val acc: 0.9659  (best train acc: 0.9827, best val acc: 0.9713, best train loss: 0.0558  @ epoch 32116 )\n",
      "[Epoch: 32340] train loss: 0.0686, train acc: 0.9752, val loss: 0.1698, val acc: 0.9669  (best train acc: 0.9827, best val acc: 0.9713, best train loss: 0.0558  @ epoch 32116 )\n",
      "[Epoch: 32360] train loss: 0.0690, train acc: 0.9773, val loss: 0.1676, val acc: 0.9666  (best train acc: 0.9827, best val acc: 0.9713, best train loss: 0.0558  @ epoch 32116 )\n",
      "[Epoch: 32380] train loss: 0.0634, train acc: 0.9790, val loss: 0.1895, val acc: 0.9690  (best train acc: 0.9827, best val acc: 0.9713, best train loss: 0.0558  @ epoch 32116 )\n",
      "[Epoch: 32400] train loss: 0.0672, train acc: 0.9774, val loss: 0.1799, val acc: 0.9659  (best train acc: 0.9827, best val acc: 0.9713, best train loss: 0.0550  @ epoch 32395 )\n",
      "[Epoch: 32420] train loss: 0.0649, train acc: 0.9789, val loss: 0.1962, val acc: 0.9673  (best train acc: 0.9827, best val acc: 0.9713, best train loss: 0.0550  @ epoch 32395 )\n",
      "[Epoch: 32440] train loss: 0.0836, train acc: 0.9727, val loss: 0.1905, val acc: 0.9669  (best train acc: 0.9827, best val acc: 0.9713, best train loss: 0.0550  @ epoch 32395 )\n",
      "[Epoch: 32460] train loss: 0.0818, train acc: 0.9730, val loss: 0.1570, val acc: 0.9666  (best train acc: 0.9827, best val acc: 0.9713, best train loss: 0.0550  @ epoch 32395 )\n",
      "[Epoch: 32480] train loss: 0.0805, train acc: 0.9759, val loss: 0.1857, val acc: 0.9669  (best train acc: 0.9827, best val acc: 0.9713, best train loss: 0.0550  @ epoch 32395 )\n",
      "[Epoch: 32500] train loss: 0.0613, train acc: 0.9792, val loss: 0.1743, val acc: 0.9690  (best train acc: 0.9827, best val acc: 0.9713, best train loss: 0.0550  @ epoch 32395 )\n",
      "[Epoch: 32520] train loss: 0.0721, train acc: 0.9765, val loss: 0.1860, val acc: 0.9683  (best train acc: 0.9827, best val acc: 0.9713, best train loss: 0.0550  @ epoch 32395 )\n",
      "[Epoch: 32540] train loss: 0.0815, train acc: 0.9720, val loss: 0.1873, val acc: 0.9659  (best train acc: 0.9827, best val acc: 0.9713, best train loss: 0.0550  @ epoch 32395 )\n",
      "[Epoch: 32560] train loss: 0.0751, train acc: 0.9721, val loss: 0.1704, val acc: 0.9673  (best train acc: 0.9827, best val acc: 0.9713, best train loss: 0.0550  @ epoch 32395 )\n",
      "[Epoch: 32580] train loss: 0.0636, train acc: 0.9784, val loss: 0.1829, val acc: 0.9673  (best train acc: 0.9827, best val acc: 0.9713, best train loss: 0.0550  @ epoch 32395 )\n",
      "[Epoch: 32600] train loss: 0.0850, train acc: 0.9715, val loss: 0.1781, val acc: 0.9639  (best train acc: 0.9827, best val acc: 0.9713, best train loss: 0.0550  @ epoch 32395 )\n",
      "[Epoch: 32620] train loss: 0.0641, train acc: 0.9785, val loss: 0.1901, val acc: 0.9653  (best train acc: 0.9827, best val acc: 0.9713, best train loss: 0.0550  @ epoch 32395 )\n",
      "[Epoch: 32640] train loss: 0.0578, train acc: 0.9814, val loss: 0.1850, val acc: 0.9656  (best train acc: 0.9827, best val acc: 0.9713, best train loss: 0.0550  @ epoch 32395 )\n",
      "[Epoch: 32660] train loss: 0.0628, train acc: 0.9785, val loss: 0.1954, val acc: 0.9669  (best train acc: 0.9827, best val acc: 0.9713, best train loss: 0.0550  @ epoch 32395 )\n",
      "[Epoch: 32680] train loss: 0.0977, train acc: 0.9685, val loss: 0.1923, val acc: 0.9545  (best train acc: 0.9827, best val acc: 0.9713, best train loss: 0.0550  @ epoch 32395 )\n",
      "[Epoch: 32700] train loss: 0.1025, train acc: 0.9675, val loss: 0.2593, val acc: 0.9548  (best train acc: 0.9827, best val acc: 0.9713, best train loss: 0.0550  @ epoch 32395 )\n",
      "[Epoch: 32720] train loss: 0.0716, train acc: 0.9772, val loss: 0.1652, val acc: 0.9683  (best train acc: 0.9827, best val acc: 0.9713, best train loss: 0.0550  @ epoch 32395 )\n",
      "[Epoch: 32740] train loss: 0.0901, train acc: 0.9706, val loss: 0.2001, val acc: 0.9639  (best train acc: 0.9827, best val acc: 0.9713, best train loss: 0.0550  @ epoch 32395 )\n",
      "[Epoch: 32760] train loss: 0.0654, train acc: 0.9790, val loss: 0.1902, val acc: 0.9659  (best train acc: 0.9827, best val acc: 0.9713, best train loss: 0.0550  @ epoch 32395 )\n",
      "[Epoch: 32780] train loss: 0.0666, train acc: 0.9785, val loss: 0.1790, val acc: 0.9663  (best train acc: 0.9827, best val acc: 0.9713, best train loss: 0.0550  @ epoch 32395 )\n",
      "[Epoch: 32800] train loss: 0.0779, train acc: 0.9703, val loss: 0.1831, val acc: 0.9666  (best train acc: 0.9827, best val acc: 0.9713, best train loss: 0.0550  @ epoch 32395 )\n",
      "[Epoch: 32820] train loss: 0.0603, train acc: 0.9808, val loss: 0.2004, val acc: 0.9676  (best train acc: 0.9827, best val acc: 0.9713, best train loss: 0.0550  @ epoch 32395 )\n",
      "[Epoch: 32840] train loss: 0.0908, train acc: 0.9691, val loss: 0.1974, val acc: 0.9649  (best train acc: 0.9827, best val acc: 0.9713, best train loss: 0.0550  @ epoch 32395 )\n",
      "[Epoch: 32860] train loss: 0.0850, train acc: 0.9727, val loss: 0.1935, val acc: 0.9599  (best train acc: 0.9827, best val acc: 0.9713, best train loss: 0.0550  @ epoch 32395 )\n",
      "[Epoch: 32880] train loss: 0.0680, train acc: 0.9779, val loss: 0.2034, val acc: 0.9659  (best train acc: 0.9827, best val acc: 0.9713, best train loss: 0.0550  @ epoch 32395 )\n",
      "[Epoch: 32900] train loss: 0.0591, train acc: 0.9811, val loss: 0.1852, val acc: 0.9676  (best train acc: 0.9827, best val acc: 0.9713, best train loss: 0.0550  @ epoch 32395 )\n",
      "[Epoch: 32920] train loss: 0.0735, train acc: 0.9727, val loss: 0.1870, val acc: 0.9622  (best train acc: 0.9827, best val acc: 0.9713, best train loss: 0.0550  @ epoch 32395 )\n",
      "[Epoch: 32940] train loss: 0.0707, train acc: 0.9759, val loss: 0.1802, val acc: 0.9690  (best train acc: 0.9827, best val acc: 0.9713, best train loss: 0.0550  @ epoch 32395 )\n",
      "[Epoch: 32960] train loss: 0.0714, train acc: 0.9748, val loss: 0.1640, val acc: 0.9649  (best train acc: 0.9827, best val acc: 0.9713, best train loss: 0.0550  @ epoch 32395 )\n",
      "[Epoch: 32980] train loss: 0.0636, train acc: 0.9788, val loss: 0.1740, val acc: 0.9676  (best train acc: 0.9827, best val acc: 0.9713, best train loss: 0.0550  @ epoch 32395 )\n",
      "[Epoch: 33000] train loss: 0.0570, train acc: 0.9804, val loss: 0.1858, val acc: 0.9673  (best train acc: 0.9827, best val acc: 0.9713, best train loss: 0.0550  @ epoch 32395 )\n",
      "[Epoch: 33020] train loss: 0.0758, train acc: 0.9704, val loss: 0.1815, val acc: 0.9659  (best train acc: 0.9827, best val acc: 0.9713, best train loss: 0.0550  @ epoch 32395 )\n",
      "[Epoch: 33040] train loss: 0.0686, train acc: 0.9751, val loss: 0.1845, val acc: 0.9649  (best train acc: 0.9827, best val acc: 0.9713, best train loss: 0.0550  @ epoch 32395 )\n",
      "[Epoch: 33060] train loss: 0.0642, train acc: 0.9799, val loss: 0.1825, val acc: 0.9669  (best train acc: 0.9827, best val acc: 0.9713, best train loss: 0.0550  @ epoch 32395 )\n",
      "[Epoch: 33080] train loss: 0.0705, train acc: 0.9768, val loss: 0.1690, val acc: 0.9656  (best train acc: 0.9827, best val acc: 0.9713, best train loss: 0.0550  @ epoch 32395 )\n",
      "[Epoch: 33100] train loss: 0.0801, train acc: 0.9813, val loss: 0.1697, val acc: 0.9676  (best train acc: 0.9827, best val acc: 0.9713, best train loss: 0.0550  @ epoch 32395 )\n",
      "[Epoch: 33120] train loss: 0.0639, train acc: 0.9787, val loss: 0.1849, val acc: 0.9639  (best train acc: 0.9827, best val acc: 0.9713, best train loss: 0.0550  @ epoch 32395 )\n",
      "[Epoch: 33140] train loss: 0.0829, train acc: 0.9727, val loss: 0.1656, val acc: 0.9629  (best train acc: 0.9827, best val acc: 0.9713, best train loss: 0.0550  @ epoch 32395 )\n",
      "[Epoch: 33160] train loss: 0.0652, train acc: 0.9763, val loss: 0.1830, val acc: 0.9653  (best train acc: 0.9827, best val acc: 0.9713, best train loss: 0.0550  @ epoch 32395 )\n",
      "[Epoch: 33180] train loss: 0.0627, train acc: 0.9795, val loss: 0.2017, val acc: 0.9663  (best train acc: 0.9827, best val acc: 0.9713, best train loss: 0.0550  @ epoch 32395 )\n",
      "[Epoch: 33200] train loss: 0.0640, train acc: 0.9775, val loss: 0.1774, val acc: 0.9676  (best train acc: 0.9827, best val acc: 0.9713, best train loss: 0.0550  @ epoch 32395 )\n",
      "[Epoch: 33220] train loss: 0.0563, train acc: 0.9813, val loss: 0.1837, val acc: 0.9653  (best train acc: 0.9827, best val acc: 0.9713, best train loss: 0.0550  @ epoch 32395 )\n",
      "[Epoch: 33240] train loss: 0.1014, train acc: 0.9675, val loss: 0.1796, val acc: 0.9609  (best train acc: 0.9827, best val acc: 0.9713, best train loss: 0.0550  @ epoch 32395 )\n",
      "[Epoch: 33260] train loss: 0.0629, train acc: 0.9808, val loss: 0.1630, val acc: 0.9683  (best train acc: 0.9827, best val acc: 0.9713, best train loss: 0.0550  @ epoch 32395 )\n",
      "[Epoch: 33280] train loss: 0.0622, train acc: 0.9787, val loss: 0.1733, val acc: 0.9659  (best train acc: 0.9827, best val acc: 0.9713, best train loss: 0.0550  @ epoch 32395 )\n",
      "[Epoch: 33300] train loss: 0.0593, train acc: 0.9816, val loss: 0.1765, val acc: 0.9686  (best train acc: 0.9828, best val acc: 0.9713, best train loss: 0.0550  @ epoch 32395 )\n",
      "[Epoch: 33320] train loss: 0.0612, train acc: 0.9802, val loss: 0.1699, val acc: 0.9666  (best train acc: 0.9828, best val acc: 0.9713, best train loss: 0.0550  @ epoch 32395 )\n",
      "[Epoch: 33340] train loss: 0.0600, train acc: 0.9798, val loss: 0.1687, val acc: 0.9680  (best train acc: 0.9828, best val acc: 0.9713, best train loss: 0.0550  @ epoch 32395 )\n",
      "[Epoch: 33360] train loss: 0.0713, train acc: 0.9774, val loss: 0.1578, val acc: 0.9663  (best train acc: 0.9828, best val acc: 0.9713, best train loss: 0.0550  @ epoch 32395 )\n",
      "[Epoch: 33380] train loss: 0.0718, train acc: 0.9764, val loss: 0.1954, val acc: 0.9636  (best train acc: 0.9828, best val acc: 0.9713, best train loss: 0.0550  @ epoch 32395 )\n",
      "[Epoch: 33400] train loss: 0.0724, train acc: 0.9760, val loss: 0.1777, val acc: 0.9666  (best train acc: 0.9828, best val acc: 0.9713, best train loss: 0.0550  @ epoch 32395 )\n",
      "[Epoch: 33420] train loss: 0.0779, train acc: 0.9752, val loss: 0.1817, val acc: 0.9659  (best train acc: 0.9828, best val acc: 0.9713, best train loss: 0.0550  @ epoch 32395 )\n",
      "[Epoch: 33440] train loss: 0.0853, train acc: 0.9654, val loss: 0.2014, val acc: 0.9494  (best train acc: 0.9828, best val acc: 0.9713, best train loss: 0.0550  @ epoch 32395 )\n",
      "[Epoch: 33460] train loss: 0.1174, train acc: 0.9550, val loss: 0.2170, val acc: 0.9359  (best train acc: 0.9828, best val acc: 0.9713, best train loss: 0.0550  @ epoch 32395 )\n",
      "[Epoch: 33480] train loss: 0.0658, train acc: 0.9785, val loss: 0.2016, val acc: 0.9632  (best train acc: 0.9828, best val acc: 0.9713, best train loss: 0.0550  @ epoch 32395 )\n",
      "[Epoch: 33500] train loss: 0.0660, train acc: 0.9777, val loss: 0.1784, val acc: 0.9663  (best train acc: 0.9828, best val acc: 0.9713, best train loss: 0.0550  @ epoch 32395 )\n",
      "[Epoch: 33520] train loss: 0.0607, train acc: 0.9813, val loss: 0.1993, val acc: 0.9669  (best train acc: 0.9828, best val acc: 0.9713, best train loss: 0.0550  @ epoch 32395 )\n",
      "[Epoch: 33540] train loss: 0.0797, train acc: 0.9671, val loss: 0.1798, val acc: 0.9673  (best train acc: 0.9828, best val acc: 0.9713, best train loss: 0.0550  @ epoch 32395 )\n",
      "[Epoch: 33560] train loss: 0.0722, train acc: 0.9733, val loss: 0.1775, val acc: 0.9666  (best train acc: 0.9828, best val acc: 0.9713, best train loss: 0.0550  @ epoch 32395 )\n",
      "[Epoch: 33580] train loss: 0.0783, train acc: 0.9710, val loss: 0.1728, val acc: 0.9669  (best train acc: 0.9828, best val acc: 0.9713, best train loss: 0.0550  @ epoch 32395 )\n",
      "[Epoch: 33600] train loss: 0.0630, train acc: 0.9778, val loss: 0.2124, val acc: 0.9629  (best train acc: 0.9828, best val acc: 0.9713, best train loss: 0.0550  @ epoch 32395 )\n",
      "[Epoch: 33620] train loss: 0.0679, train acc: 0.9787, val loss: 0.1806, val acc: 0.9642  (best train acc: 0.9828, best val acc: 0.9713, best train loss: 0.0550  @ epoch 32395 )\n",
      "[Epoch: 33640] train loss: 0.0598, train acc: 0.9795, val loss: 0.1739, val acc: 0.9666  (best train acc: 0.9828, best val acc: 0.9713, best train loss: 0.0550  @ epoch 32395 )\n",
      "[Epoch: 33660] train loss: 0.0632, train acc: 0.9785, val loss: 0.1976, val acc: 0.9663  (best train acc: 0.9828, best val acc: 0.9713, best train loss: 0.0550  @ epoch 32395 )\n",
      "[Epoch: 33680] train loss: 0.0638, train acc: 0.9791, val loss: 0.2107, val acc: 0.9663  (best train acc: 0.9828, best val acc: 0.9713, best train loss: 0.0550  @ epoch 32395 )\n",
      "[Epoch: 33700] train loss: 0.0662, train acc: 0.9760, val loss: 0.1707, val acc: 0.9696  (best train acc: 0.9828, best val acc: 0.9713, best train loss: 0.0550  @ epoch 32395 )\n",
      "[Epoch: 33720] train loss: 0.0590, train acc: 0.9798, val loss: 0.1830, val acc: 0.9666  (best train acc: 0.9828, best val acc: 0.9713, best train loss: 0.0550  @ epoch 32395 )\n",
      "[Epoch: 33740] train loss: 0.0629, train acc: 0.9786, val loss: 0.1832, val acc: 0.9676  (best train acc: 0.9828, best val acc: 0.9713, best train loss: 0.0550  @ epoch 32395 )\n",
      "[Epoch: 33760] train loss: 0.0687, train acc: 0.9764, val loss: 0.1797, val acc: 0.9659  (best train acc: 0.9828, best val acc: 0.9713, best train loss: 0.0550  @ epoch 32395 )\n",
      "[Epoch: 33780] train loss: 0.0752, train acc: 0.9753, val loss: 0.1565, val acc: 0.9666  (best train acc: 0.9828, best val acc: 0.9713, best train loss: 0.0545  @ epoch 33774 )\n",
      "[Epoch: 33800] train loss: 0.0676, train acc: 0.9759, val loss: 0.1862, val acc: 0.9646  (best train acc: 0.9828, best val acc: 0.9713, best train loss: 0.0545  @ epoch 33774 )\n",
      "[Epoch: 33820] train loss: 0.0701, train acc: 0.9777, val loss: 0.1764, val acc: 0.9683  (best train acc: 0.9828, best val acc: 0.9713, best train loss: 0.0545  @ epoch 33774 )\n",
      "[Epoch: 33840] train loss: 0.0673, train acc: 0.9785, val loss: 0.1787, val acc: 0.9693  (best train acc: 0.9828, best val acc: 0.9713, best train loss: 0.0545  @ epoch 33774 )\n",
      "[Epoch: 33860] train loss: 0.0704, train acc: 0.9777, val loss: 0.1705, val acc: 0.9659  (best train acc: 0.9828, best val acc: 0.9713, best train loss: 0.0545  @ epoch 33774 )\n",
      "[Epoch: 33880] train loss: 0.1435, train acc: 0.9533, val loss: 0.2016, val acc: 0.9602  (best train acc: 0.9828, best val acc: 0.9713, best train loss: 0.0545  @ epoch 33774 )\n",
      "[Epoch: 33900] train loss: 0.0944, train acc: 0.9665, val loss: 0.2011, val acc: 0.9619  (best train acc: 0.9828, best val acc: 0.9713, best train loss: 0.0545  @ epoch 33774 )\n",
      "[Epoch: 33920] train loss: 0.0738, train acc: 0.9741, val loss: 0.1962, val acc: 0.9649  (best train acc: 0.9828, best val acc: 0.9713, best train loss: 0.0545  @ epoch 33774 )\n",
      "[Epoch: 33940] train loss: 0.0671, train acc: 0.9783, val loss: 0.1883, val acc: 0.9673  (best train acc: 0.9828, best val acc: 0.9713, best train loss: 0.0545  @ epoch 33774 )\n",
      "[Epoch: 33960] train loss: 0.0685, train acc: 0.9772, val loss: 0.1918, val acc: 0.9680  (best train acc: 0.9828, best val acc: 0.9713, best train loss: 0.0545  @ epoch 33774 )\n",
      "[Epoch: 33980] train loss: 0.0637, train acc: 0.9785, val loss: 0.1802, val acc: 0.9659  (best train acc: 0.9828, best val acc: 0.9713, best train loss: 0.0545  @ epoch 33774 )\n",
      "[Epoch: 34000] train loss: 0.0663, train acc: 0.9779, val loss: 0.1933, val acc: 0.9663  (best train acc: 0.9828, best val acc: 0.9713, best train loss: 0.0545  @ epoch 33774 )\n",
      "[Epoch: 34020] train loss: 0.0834, train acc: 0.9806, val loss: 0.1839, val acc: 0.9676  (best train acc: 0.9828, best val acc: 0.9713, best train loss: 0.0545  @ epoch 33774 )\n",
      "[Epoch: 34040] train loss: 0.0634, train acc: 0.9779, val loss: 0.1614, val acc: 0.9669  (best train acc: 0.9828, best val acc: 0.9713, best train loss: 0.0545  @ epoch 33774 )\n",
      "[Epoch: 34060] train loss: 0.0859, train acc: 0.9738, val loss: 0.1971, val acc: 0.9659  (best train acc: 0.9828, best val acc: 0.9713, best train loss: 0.0545  @ epoch 33774 )\n",
      "[Epoch: 34080] train loss: 0.0648, train acc: 0.9777, val loss: 0.1812, val acc: 0.9669  (best train acc: 0.9828, best val acc: 0.9713, best train loss: 0.0545  @ epoch 33774 )\n",
      "[Epoch: 34100] train loss: 0.0644, train acc: 0.9804, val loss: 0.1920, val acc: 0.9666  (best train acc: 0.9828, best val acc: 0.9713, best train loss: 0.0545  @ epoch 33774 )\n",
      "[Epoch: 34120] train loss: 0.0601, train acc: 0.9803, val loss: 0.1941, val acc: 0.9639  (best train acc: 0.9828, best val acc: 0.9713, best train loss: 0.0545  @ epoch 33774 )\n",
      "[Epoch: 34140] train loss: 0.0605, train acc: 0.9807, val loss: 0.1799, val acc: 0.9663  (best train acc: 0.9828, best val acc: 0.9713, best train loss: 0.0545  @ epoch 33774 )\n",
      "[Epoch: 34160] train loss: 0.0936, train acc: 0.9624, val loss: 0.1721, val acc: 0.9646  (best train acc: 0.9828, best val acc: 0.9713, best train loss: 0.0545  @ epoch 33774 )\n",
      "[Epoch: 34180] train loss: 0.0669, train acc: 0.9782, val loss: 0.2037, val acc: 0.9646  (best train acc: 0.9828, best val acc: 0.9713, best train loss: 0.0545  @ epoch 33774 )\n",
      "[Epoch: 34200] train loss: 0.0697, train acc: 0.9779, val loss: 0.1724, val acc: 0.9666  (best train acc: 0.9828, best val acc: 0.9713, best train loss: 0.0545  @ epoch 33774 )\n",
      "[Epoch: 34220] train loss: 0.0647, train acc: 0.9778, val loss: 0.1651, val acc: 0.9669  (best train acc: 0.9828, best val acc: 0.9713, best train loss: 0.0545  @ epoch 33774 )\n",
      "[Epoch: 34240] train loss: 0.0621, train acc: 0.9800, val loss: 0.1859, val acc: 0.9683  (best train acc: 0.9828, best val acc: 0.9713, best train loss: 0.0545  @ epoch 33774 )\n",
      "[Epoch: 34260] train loss: 0.0632, train acc: 0.9804, val loss: 0.1878, val acc: 0.9653  (best train acc: 0.9828, best val acc: 0.9713, best train loss: 0.0545  @ epoch 33774 )\n",
      "[Epoch: 34280] train loss: 0.0615, train acc: 0.9790, val loss: 0.1725, val acc: 0.9676  (best train acc: 0.9828, best val acc: 0.9713, best train loss: 0.0545  @ epoch 33774 )\n",
      "[Epoch: 34300] train loss: 0.0647, train acc: 0.9775, val loss: 0.1883, val acc: 0.9669  (best train acc: 0.9828, best val acc: 0.9713, best train loss: 0.0545  @ epoch 33774 )\n",
      "[Epoch: 34320] train loss: 0.0816, train acc: 0.9725, val loss: 0.2009, val acc: 0.9656  (best train acc: 0.9828, best val acc: 0.9713, best train loss: 0.0545  @ epoch 33774 )\n",
      "[Epoch: 34340] train loss: 0.0603, train acc: 0.9789, val loss: 0.1791, val acc: 0.9669  (best train acc: 0.9828, best val acc: 0.9713, best train loss: 0.0545  @ epoch 33774 )\n",
      "[Epoch: 34360] train loss: 0.0611, train acc: 0.9784, val loss: 0.2014, val acc: 0.9676  (best train acc: 0.9834, best val acc: 0.9713, best train loss: 0.0545  @ epoch 33774 )\n",
      "[Epoch: 34380] train loss: 0.0813, train acc: 0.9722, val loss: 0.1853, val acc: 0.9666  (best train acc: 0.9834, best val acc: 0.9713, best train loss: 0.0545  @ epoch 33774 )\n",
      "[Epoch: 34400] train loss: 0.1615, train acc: 0.9534, val loss: 0.2236, val acc: 0.9531  (best train acc: 0.9834, best val acc: 0.9713, best train loss: 0.0545  @ epoch 33774 )\n",
      "[Epoch: 34420] train loss: 0.1043, train acc: 0.9660, val loss: 0.2106, val acc: 0.9626  (best train acc: 0.9834, best val acc: 0.9713, best train loss: 0.0545  @ epoch 33774 )\n",
      "[Epoch: 34440] train loss: 0.0740, train acc: 0.9746, val loss: 0.1622, val acc: 0.9713  (best train acc: 0.9834, best val acc: 0.9713, best train loss: 0.0545  @ epoch 33774 )\n",
      "[Epoch: 34460] train loss: 0.0670, train acc: 0.9782, val loss: 0.1985, val acc: 0.9676  (best train acc: 0.9834, best val acc: 0.9713, best train loss: 0.0545  @ epoch 33774 )\n",
      "[Epoch: 34480] train loss: 0.0636, train acc: 0.9797, val loss: 0.1784, val acc: 0.9676  (best train acc: 0.9834, best val acc: 0.9713, best train loss: 0.0545  @ epoch 33774 )\n",
      "[Epoch: 34500] train loss: 0.0853, train acc: 0.9750, val loss: 0.2319, val acc: 0.9646  (best train acc: 0.9834, best val acc: 0.9713, best train loss: 0.0545  @ epoch 33774 )\n",
      "[Epoch: 34520] train loss: 0.0613, train acc: 0.9788, val loss: 0.1659, val acc: 0.9680  (best train acc: 0.9834, best val acc: 0.9713, best train loss: 0.0545  @ epoch 33774 )\n",
      "[Epoch: 34540] train loss: 0.0628, train acc: 0.9800, val loss: 0.1882, val acc: 0.9686  (best train acc: 0.9834, best val acc: 0.9713, best train loss: 0.0545  @ epoch 33774 )\n",
      "[Epoch: 34560] train loss: 0.0686, train acc: 0.9780, val loss: 0.1811, val acc: 0.9666  (best train acc: 0.9834, best val acc: 0.9713, best train loss: 0.0545  @ epoch 33774 )\n",
      "[Epoch: 34580] train loss: 0.0639, train acc: 0.9782, val loss: 0.1701, val acc: 0.9673  (best train acc: 0.9834, best val acc: 0.9713, best train loss: 0.0545  @ epoch 33774 )\n",
      "[Epoch: 34600] train loss: 0.0630, train acc: 0.9793, val loss: 0.1754, val acc: 0.9690  (best train acc: 0.9834, best val acc: 0.9713, best train loss: 0.0539  @ epoch 34595 )\n",
      "[Epoch: 34620] train loss: 0.0757, train acc: 0.9719, val loss: 0.1812, val acc: 0.9666  (best train acc: 0.9834, best val acc: 0.9713, best train loss: 0.0539  @ epoch 34595 )\n",
      "[Epoch: 34640] train loss: 0.0971, train acc: 0.9665, val loss: 0.1874, val acc: 0.9646  (best train acc: 0.9834, best val acc: 0.9713, best train loss: 0.0539  @ epoch 34595 )\n",
      "[Epoch: 34660] train loss: 0.0735, train acc: 0.9739, val loss: 0.2016, val acc: 0.9642  (best train acc: 0.9834, best val acc: 0.9713, best train loss: 0.0539  @ epoch 34595 )\n",
      "[Epoch: 34680] train loss: 0.0763, train acc: 0.9748, val loss: 0.1636, val acc: 0.9659  (best train acc: 0.9834, best val acc: 0.9713, best train loss: 0.0539  @ epoch 34595 )\n",
      "[Epoch: 34700] train loss: 0.0621, train acc: 0.9803, val loss: 0.1828, val acc: 0.9649  (best train acc: 0.9834, best val acc: 0.9713, best train loss: 0.0539  @ epoch 34595 )\n",
      "[Epoch: 34720] train loss: 0.0617, train acc: 0.9813, val loss: 0.1767, val acc: 0.9666  (best train acc: 0.9834, best val acc: 0.9713, best train loss: 0.0539  @ epoch 34595 )\n",
      "[Epoch: 34740] train loss: 0.0635, train acc: 0.9780, val loss: 0.1811, val acc: 0.9676  (best train acc: 0.9834, best val acc: 0.9713, best train loss: 0.0539  @ epoch 34595 )\n",
      "[Epoch: 34760] train loss: 0.0598, train acc: 0.9804, val loss: 0.1740, val acc: 0.9663  (best train acc: 0.9834, best val acc: 0.9713, best train loss: 0.0539  @ epoch 34595 )\n",
      "[Epoch: 34780] train loss: 0.0681, train acc: 0.9767, val loss: 0.1874, val acc: 0.9659  (best train acc: 0.9834, best val acc: 0.9713, best train loss: 0.0539  @ epoch 34595 )\n",
      "[Epoch: 34800] train loss: 0.0583, train acc: 0.9804, val loss: 0.1772, val acc: 0.9663  (best train acc: 0.9834, best val acc: 0.9713, best train loss: 0.0539  @ epoch 34595 )\n",
      "[Epoch: 34820] train loss: 0.0671, train acc: 0.9776, val loss: 0.1786, val acc: 0.9656  (best train acc: 0.9834, best val acc: 0.9713, best train loss: 0.0539  @ epoch 34595 )\n",
      "[Epoch: 34840] train loss: 0.0616, train acc: 0.9785, val loss: 0.1946, val acc: 0.9656  (best train acc: 0.9834, best val acc: 0.9713, best train loss: 0.0539  @ epoch 34595 )\n",
      "[Epoch: 34860] train loss: 0.0576, train acc: 0.9786, val loss: 0.1909, val acc: 0.9663  (best train acc: 0.9834, best val acc: 0.9713, best train loss: 0.0539  @ epoch 34595 )\n",
      "[Epoch: 34880] train loss: 0.0632, train acc: 0.9793, val loss: 0.1848, val acc: 0.9663  (best train acc: 0.9834, best val acc: 0.9713, best train loss: 0.0539  @ epoch 34595 )\n",
      "[Epoch: 34900] train loss: 0.0654, train acc: 0.9788, val loss: 0.1504, val acc: 0.9680  (best train acc: 0.9834, best val acc: 0.9713, best train loss: 0.0539  @ epoch 34595 )\n",
      "[Epoch: 34920] train loss: 0.0656, train acc: 0.9774, val loss: 0.1950, val acc: 0.9653  (best train acc: 0.9834, best val acc: 0.9713, best train loss: 0.0539  @ epoch 34595 )\n",
      "[Epoch: 34940] train loss: 0.0700, train acc: 0.9792, val loss: 0.1861, val acc: 0.9656  (best train acc: 0.9834, best val acc: 0.9713, best train loss: 0.0539  @ epoch 34595 )\n",
      "[Epoch: 34960] train loss: 0.0609, train acc: 0.9802, val loss: 0.1866, val acc: 0.9673  (best train acc: 0.9834, best val acc: 0.9713, best train loss: 0.0539  @ epoch 34595 )\n",
      "[Epoch: 34980] train loss: 0.0642, train acc: 0.9769, val loss: 0.1931, val acc: 0.9673  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0539  @ epoch 34595 )\n",
      "[Epoch: 35000] train loss: 0.0703, train acc: 0.9761, val loss: 0.1632, val acc: 0.9663  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0539  @ epoch 34595 )\n",
      "[Epoch: 35020] train loss: 0.0615, train acc: 0.9793, val loss: 0.1653, val acc: 0.9653  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0539  @ epoch 34595 )\n",
      "[Epoch: 35040] train loss: 0.0673, train acc: 0.9761, val loss: 0.1909, val acc: 0.9663  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0539  @ epoch 34595 )\n",
      "[Epoch: 35060] train loss: 0.0644, train acc: 0.9806, val loss: 0.1897, val acc: 0.9663  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0539  @ epoch 34595 )\n",
      "[Epoch: 35080] train loss: 0.0659, train acc: 0.9761, val loss: 0.1806, val acc: 0.9642  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0539  @ epoch 34595 )\n",
      "[Epoch: 35100] train loss: 0.0905, train acc: 0.9644, val loss: 0.1708, val acc: 0.9646  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0539  @ epoch 34595 )\n",
      "[Epoch: 35120] train loss: 0.0691, train acc: 0.9774, val loss: 0.1920, val acc: 0.9609  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0539  @ epoch 34595 )\n",
      "[Epoch: 35140] train loss: 0.0659, train acc: 0.9778, val loss: 0.2126, val acc: 0.9680  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0539  @ epoch 34595 )\n",
      "[Epoch: 35160] train loss: 0.0774, train acc: 0.9746, val loss: 0.1724, val acc: 0.9669  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0539  @ epoch 34595 )\n",
      "[Epoch: 35180] train loss: 0.0613, train acc: 0.9798, val loss: 0.1962, val acc: 0.9653  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0539  @ epoch 34595 )\n",
      "[Epoch: 35200] train loss: 0.0953, train acc: 0.9620, val loss: 0.1637, val acc: 0.9653  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0539  @ epoch 34595 )\n",
      "[Epoch: 35220] train loss: 0.0802, train acc: 0.9724, val loss: 0.1834, val acc: 0.9659  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0539  @ epoch 34595 )\n",
      "[Epoch: 35240] train loss: 0.0754, train acc: 0.9729, val loss: 0.1544, val acc: 0.9636  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0539  @ epoch 34595 )\n",
      "[Epoch: 35260] train loss: 0.0865, train acc: 0.9730, val loss: 0.1712, val acc: 0.9632  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0539  @ epoch 34595 )\n",
      "[Epoch: 35280] train loss: 0.0652, train acc: 0.9766, val loss: 0.2051, val acc: 0.9599  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0539  @ epoch 34595 )\n",
      "[Epoch: 35300] train loss: 0.0730, train acc: 0.9752, val loss: 0.1671, val acc: 0.9653  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0539  @ epoch 34595 )\n",
      "[Epoch: 35320] train loss: 0.0609, train acc: 0.9799, val loss: 0.1898, val acc: 0.9653  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0539  @ epoch 34595 )\n",
      "[Epoch: 35340] train loss: 0.0958, train acc: 0.9693, val loss: 0.1510, val acc: 0.9619  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0539  @ epoch 34595 )\n",
      "[Epoch: 35360] train loss: 0.0719, train acc: 0.9764, val loss: 0.1907, val acc: 0.9669  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0539  @ epoch 34595 )\n",
      "[Epoch: 35380] train loss: 0.0762, train acc: 0.9746, val loss: 0.1803, val acc: 0.9659  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0539  @ epoch 34595 )\n",
      "[Epoch: 35400] train loss: 0.0651, train acc: 0.9786, val loss: 0.1796, val acc: 0.9663  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0539  @ epoch 34595 )\n",
      "[Epoch: 35420] train loss: 0.0745, train acc: 0.9751, val loss: 0.1707, val acc: 0.9666  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0539  @ epoch 34595 )\n",
      "[Epoch: 35440] train loss: 0.0700, train acc: 0.9768, val loss: 0.1474, val acc: 0.9673  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0539  @ epoch 34595 )\n",
      "[Epoch: 35460] train loss: 0.0577, train acc: 0.9816, val loss: 0.1808, val acc: 0.9676  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0539  @ epoch 34595 )\n",
      "[Epoch: 35480] train loss: 0.0669, train acc: 0.9766, val loss: 0.1796, val acc: 0.9676  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0539  @ epoch 34595 )\n",
      "[Epoch: 35500] train loss: 0.0792, train acc: 0.9685, val loss: 0.1779, val acc: 0.9636  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0539  @ epoch 34595 )\n",
      "[Epoch: 35520] train loss: 0.0613, train acc: 0.9786, val loss: 0.2104, val acc: 0.9649  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0539  @ epoch 34595 )\n",
      "[Epoch: 35540] train loss: 0.0641, train acc: 0.9785, val loss: 0.1866, val acc: 0.9653  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0539  @ epoch 34595 )\n",
      "[Epoch: 35560] train loss: 0.0565, train acc: 0.9821, val loss: 0.1729, val acc: 0.9656  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0539  @ epoch 34595 )\n",
      "[Epoch: 35580] train loss: 0.0720, train acc: 0.9731, val loss: 0.1600, val acc: 0.9680  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0539  @ epoch 34595 )\n",
      "[Epoch: 35600] train loss: 0.0741, train acc: 0.9746, val loss: 0.2202, val acc: 0.9497  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0539  @ epoch 34595 )\n",
      "[Epoch: 35620] train loss: 0.1021, train acc: 0.9623, val loss: 0.1896, val acc: 0.9521  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0539  @ epoch 34595 )\n",
      "[Epoch: 35640] train loss: 0.0673, train acc: 0.9772, val loss: 0.1509, val acc: 0.9663  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0539  @ epoch 34595 )\n",
      "[Epoch: 35660] train loss: 0.0614, train acc: 0.9785, val loss: 0.1782, val acc: 0.9666  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0539  @ epoch 34595 )\n",
      "[Epoch: 35680] train loss: 0.0575, train acc: 0.9807, val loss: 0.1812, val acc: 0.9653  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0539  @ epoch 34595 )\n",
      "[Epoch: 35700] train loss: 0.0575, train acc: 0.9799, val loss: 0.1737, val acc: 0.9683  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0539  @ epoch 34595 )\n",
      "[Epoch: 35720] train loss: 0.0610, train acc: 0.9794, val loss: 0.1739, val acc: 0.9666  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0537  @ epoch 35711 )\n",
      "[Epoch: 35740] train loss: 0.0625, train acc: 0.9797, val loss: 0.1864, val acc: 0.9659  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0537  @ epoch 35711 )\n",
      "[Epoch: 35760] train loss: 0.0603, train acc: 0.9793, val loss: 0.1775, val acc: 0.9669  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0537  @ epoch 35711 )\n",
      "[Epoch: 35780] train loss: 0.0770, train acc: 0.9756, val loss: 0.2003, val acc: 0.9642  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0533  @ epoch 35770 )\n",
      "[Epoch: 35800] train loss: 0.0790, train acc: 0.9727, val loss: 0.1579, val acc: 0.9673  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0533  @ epoch 35770 )\n",
      "[Epoch: 35820] train loss: 0.0603, train acc: 0.9805, val loss: 0.2003, val acc: 0.9666  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0533  @ epoch 35770 )\n",
      "[Epoch: 35840] train loss: 0.0618, train acc: 0.9800, val loss: 0.1838, val acc: 0.9683  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0533  @ epoch 35770 )\n",
      "[Epoch: 35860] train loss: 0.0744, train acc: 0.9763, val loss: 0.2056, val acc: 0.9639  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0533  @ epoch 35770 )\n",
      "[Epoch: 35880] train loss: 0.0753, train acc: 0.9733, val loss: 0.1823, val acc: 0.9646  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0533  @ epoch 35770 )\n",
      "[Epoch: 35900] train loss: 0.0673, train acc: 0.9755, val loss: 0.1612, val acc: 0.9656  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0533  @ epoch 35770 )\n",
      "[Epoch: 35920] train loss: 0.0857, train acc: 0.9721, val loss: 0.1879, val acc: 0.9639  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0533  @ epoch 35770 )\n",
      "[Epoch: 35940] train loss: 0.0593, train acc: 0.9812, val loss: 0.1881, val acc: 0.9659  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0533  @ epoch 35770 )\n",
      "[Epoch: 35960] train loss: 0.0642, train acc: 0.9780, val loss: 0.1811, val acc: 0.9686  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0533  @ epoch 35770 )\n",
      "[Epoch: 35980] train loss: 0.0660, train acc: 0.9785, val loss: 0.1505, val acc: 0.9663  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0533  @ epoch 35770 )\n",
      "[Epoch: 36000] train loss: 0.0586, train acc: 0.9802, val loss: 0.1928, val acc: 0.9663  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0533  @ epoch 35770 )\n",
      "[Epoch: 36020] train loss: 0.0694, train acc: 0.9783, val loss: 0.1993, val acc: 0.9659  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0533  @ epoch 35770 )\n",
      "[Epoch: 36040] train loss: 0.0596, train acc: 0.9809, val loss: 0.1782, val acc: 0.9673  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0533  @ epoch 35770 )\n",
      "[Epoch: 36060] train loss: 0.0642, train acc: 0.9784, val loss: 0.1982, val acc: 0.9636  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0533  @ epoch 35770 )\n",
      "[Epoch: 36080] train loss: 0.0611, train acc: 0.9791, val loss: 0.1780, val acc: 0.9669  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0533  @ epoch 35770 )\n",
      "[Epoch: 36100] train loss: 0.0920, train acc: 0.9717, val loss: 0.1888, val acc: 0.9619  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0533  @ epoch 35770 )\n",
      "[Epoch: 36120] train loss: 0.0622, train acc: 0.9805, val loss: 0.2008, val acc: 0.9669  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0533  @ epoch 35770 )\n",
      "[Epoch: 36140] train loss: 0.0641, train acc: 0.9787, val loss: 0.1894, val acc: 0.9669  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0533  @ epoch 35770 )\n",
      "[Epoch: 36160] train loss: 0.0662, train acc: 0.9793, val loss: 0.1658, val acc: 0.9663  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0533  @ epoch 35770 )\n",
      "[Epoch: 36180] train loss: 0.0604, train acc: 0.9803, val loss: 0.1891, val acc: 0.9669  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0533  @ epoch 35770 )\n",
      "[Epoch: 36200] train loss: 0.0780, train acc: 0.9701, val loss: 0.1847, val acc: 0.9666  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0533  @ epoch 35770 )\n",
      "[Epoch: 36220] train loss: 0.0620, train acc: 0.9787, val loss: 0.1767, val acc: 0.9693  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0533  @ epoch 35770 )\n",
      "[Epoch: 36240] train loss: 0.0568, train acc: 0.9820, val loss: 0.1963, val acc: 0.9656  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0533  @ epoch 35770 )\n",
      "[Epoch: 36260] train loss: 0.0642, train acc: 0.9795, val loss: 0.1742, val acc: 0.9683  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0533  @ epoch 35770 )\n",
      "[Epoch: 36280] train loss: 0.0632, train acc: 0.9773, val loss: 0.1780, val acc: 0.9669  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0533  @ epoch 35770 )\n",
      "[Epoch: 36300] train loss: 0.0613, train acc: 0.9791, val loss: 0.1676, val acc: 0.9676  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0533  @ epoch 35770 )\n",
      "[Epoch: 36320] train loss: 0.1022, train acc: 0.9675, val loss: 0.2296, val acc: 0.9589  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0533  @ epoch 35770 )\n",
      "[Epoch: 36340] train loss: 0.0809, train acc: 0.9716, val loss: 0.2036, val acc: 0.9592  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0533  @ epoch 35770 )\n",
      "[Epoch: 36360] train loss: 0.0681, train acc: 0.9767, val loss: 0.1475, val acc: 0.9653  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0533  @ epoch 35770 )\n",
      "[Epoch: 36380] train loss: 0.1117, train acc: 0.9598, val loss: 0.1633, val acc: 0.9656  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0533  @ epoch 35770 )\n",
      "[Epoch: 36400] train loss: 0.0622, train acc: 0.9787, val loss: 0.1721, val acc: 0.9663  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0533  @ epoch 35770 )\n",
      "[Epoch: 36420] train loss: 0.0680, train acc: 0.9775, val loss: 0.1639, val acc: 0.9649  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0533  @ epoch 35770 )\n",
      "[Epoch: 36440] train loss: 0.0761, train acc: 0.9746, val loss: 0.1801, val acc: 0.9619  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0533  @ epoch 35770 )\n",
      "[Epoch: 36460] train loss: 0.0600, train acc: 0.9800, val loss: 0.1676, val acc: 0.9676  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0533  @ epoch 35770 )\n",
      "[Epoch: 36480] train loss: 0.0640, train acc: 0.9775, val loss: 0.1826, val acc: 0.9619  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0517  @ epoch 36462 )\n",
      "[Epoch: 36500] train loss: 0.0585, train acc: 0.9798, val loss: 0.1638, val acc: 0.9656  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0517  @ epoch 36462 )\n",
      "[Epoch: 36520] train loss: 0.0568, train acc: 0.9813, val loss: 0.1773, val acc: 0.9669  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0517  @ epoch 36462 )\n",
      "[Epoch: 36540] train loss: 0.0610, train acc: 0.9806, val loss: 0.1630, val acc: 0.9686  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0517  @ epoch 36462 )\n",
      "[Epoch: 36560] train loss: 0.0618, train acc: 0.9798, val loss: 0.1731, val acc: 0.9666  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0517  @ epoch 36462 )\n",
      "[Epoch: 36580] train loss: 0.1218, train acc: 0.9534, val loss: 0.1437, val acc: 0.9632  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0517  @ epoch 36462 )\n",
      "[Epoch: 36600] train loss: 0.0659, train acc: 0.9788, val loss: 0.1567, val acc: 0.9683  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0517  @ epoch 36462 )\n",
      "[Epoch: 36620] train loss: 0.0636, train acc: 0.9791, val loss: 0.1608, val acc: 0.9666  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0517  @ epoch 36462 )\n",
      "[Epoch: 36640] train loss: 0.0620, train acc: 0.9793, val loss: 0.1742, val acc: 0.9653  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0517  @ epoch 36462 )\n",
      "[Epoch: 36660] train loss: 0.0776, train acc: 0.9722, val loss: 0.1578, val acc: 0.9669  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0517  @ epoch 36462 )\n",
      "[Epoch: 36680] train loss: 0.0585, train acc: 0.9814, val loss: 0.1727, val acc: 0.9666  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0517  @ epoch 36462 )\n",
      "[Epoch: 36700] train loss: 0.0663, train acc: 0.9790, val loss: 0.1629, val acc: 0.9666  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0517  @ epoch 36462 )\n",
      "[Epoch: 36720] train loss: 0.0681, train acc: 0.9761, val loss: 0.1745, val acc: 0.9683  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0517  @ epoch 36462 )\n",
      "[Epoch: 36740] train loss: 0.1026, train acc: 0.9594, val loss: 0.1722, val acc: 0.9592  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0517  @ epoch 36462 )\n",
      "[Epoch: 36760] train loss: 0.0699, train acc: 0.9738, val loss: 0.1681, val acc: 0.9676  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0517  @ epoch 36462 )\n",
      "[Epoch: 36780] train loss: 0.0781, train acc: 0.9705, val loss: 0.1647, val acc: 0.9653  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0517  @ epoch 36462 )\n",
      "[Epoch: 36800] train loss: 0.0716, train acc: 0.9724, val loss: 0.1584, val acc: 0.9656  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0517  @ epoch 36462 )\n",
      "[Epoch: 36820] train loss: 0.0638, train acc: 0.9809, val loss: 0.1557, val acc: 0.9663  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0517  @ epoch 36462 )\n",
      "[Epoch: 36840] train loss: 0.0643, train acc: 0.9779, val loss: 0.1625, val acc: 0.9669  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0517  @ epoch 36462 )\n",
      "[Epoch: 36860] train loss: 0.0614, train acc: 0.9797, val loss: 0.1646, val acc: 0.9656  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0517  @ epoch 36462 )\n",
      "[Epoch: 36880] train loss: 0.0606, train acc: 0.9802, val loss: 0.1752, val acc: 0.9663  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0517  @ epoch 36462 )\n",
      "[Epoch: 36900] train loss: 0.0634, train acc: 0.9785, val loss: 0.1763, val acc: 0.9669  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0517  @ epoch 36462 )\n",
      "[Epoch: 36920] train loss: 0.0567, train acc: 0.9832, val loss: 0.1500, val acc: 0.9663  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0517  @ epoch 36462 )\n",
      "[Epoch: 36940] train loss: 0.0588, train acc: 0.9804, val loss: 0.1921, val acc: 0.9669  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0517  @ epoch 36462 )\n",
      "[Epoch: 36960] train loss: 0.0584, train acc: 0.9803, val loss: 0.1666, val acc: 0.9666  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0517  @ epoch 36462 )\n",
      "[Epoch: 36980] train loss: 0.0648, train acc: 0.9778, val loss: 0.1713, val acc: 0.9646  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0517  @ epoch 36462 )\n",
      "[Epoch: 37000] train loss: 0.0616, train acc: 0.9800, val loss: 0.1622, val acc: 0.9663  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0517  @ epoch 36462 )\n",
      "[Epoch: 37020] train loss: 0.0830, train acc: 0.9699, val loss: 0.1492, val acc: 0.9649  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0517  @ epoch 36462 )\n",
      "[Epoch: 37040] train loss: 0.0599, train acc: 0.9811, val loss: 0.1710, val acc: 0.9676  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0517  @ epoch 36462 )\n",
      "[Epoch: 37060] train loss: 0.0606, train acc: 0.9789, val loss: 0.1763, val acc: 0.9656  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0517  @ epoch 36462 )\n",
      "[Epoch: 37080] train loss: 0.0576, train acc: 0.9806, val loss: 0.1765, val acc: 0.9656  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0517  @ epoch 36462 )\n",
      "[Epoch: 37100] train loss: 0.0726, train acc: 0.9740, val loss: 0.1676, val acc: 0.9686  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0517  @ epoch 36462 )\n",
      "[Epoch: 37120] train loss: 0.0596, train acc: 0.9805, val loss: 0.1730, val acc: 0.9703  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0517  @ epoch 36462 )\n",
      "[Epoch: 37140] train loss: 0.0740, train acc: 0.9725, val loss: 0.1745, val acc: 0.9669  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0517  @ epoch 37129 )\n",
      "[Epoch: 37160] train loss: 0.0661, train acc: 0.9778, val loss: 0.1725, val acc: 0.9656  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0517  @ epoch 37129 )\n",
      "[Epoch: 37180] train loss: 0.0987, train acc: 0.9690, val loss: 0.1597, val acc: 0.9572  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0517  @ epoch 37129 )\n",
      "[Epoch: 37200] train loss: 0.0629, train acc: 0.9782, val loss: 0.1716, val acc: 0.9659  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0517  @ epoch 37129 )\n",
      "[Epoch: 37220] train loss: 0.0605, train acc: 0.9802, val loss: 0.1662, val acc: 0.9669  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0517  @ epoch 37129 )\n",
      "[Epoch: 37240] train loss: 0.0722, train acc: 0.9759, val loss: 0.1698, val acc: 0.9669  (best train acc: 0.9838, best val acc: 0.9713, best train loss: 0.0517  @ epoch 37129 )\n",
      "[Epoch: 37260] train loss: 0.0552, train acc: 0.9839, val loss: 0.1800, val acc: 0.9659  (best train acc: 0.9839, best val acc: 0.9713, best train loss: 0.0517  @ epoch 37129 )\n",
      "[Epoch: 37280] train loss: 0.0694, train acc: 0.9758, val loss: 0.1936, val acc: 0.9605  (best train acc: 0.9839, best val acc: 0.9713, best train loss: 0.0517  @ epoch 37129 )\n",
      "[Epoch: 37300] train loss: 0.0954, train acc: 0.9698, val loss: 0.1537, val acc: 0.9653  (best train acc: 0.9839, best val acc: 0.9713, best train loss: 0.0517  @ epoch 37129 )\n",
      "[Epoch: 37320] train loss: 0.0641, train acc: 0.9782, val loss: 0.1509, val acc: 0.9683  (best train acc: 0.9839, best val acc: 0.9713, best train loss: 0.0517  @ epoch 37129 )\n",
      "[Epoch: 37340] train loss: 0.0586, train acc: 0.9807, val loss: 0.1658, val acc: 0.9683  (best train acc: 0.9839, best val acc: 0.9713, best train loss: 0.0517  @ epoch 37129 )\n",
      "[Epoch: 37360] train loss: 0.0573, train acc: 0.9813, val loss: 0.1817, val acc: 0.9659  (best train acc: 0.9839, best val acc: 0.9713, best train loss: 0.0517  @ epoch 37129 )\n",
      "[Epoch: 37380] train loss: 0.0646, train acc: 0.9780, val loss: 0.1796, val acc: 0.9629  (best train acc: 0.9839, best val acc: 0.9713, best train loss: 0.0517  @ epoch 37129 )\n",
      "[Epoch: 37400] train loss: 0.0789, train acc: 0.9787, val loss: 0.1700, val acc: 0.9686  (best train acc: 0.9839, best val acc: 0.9713, best train loss: 0.0517  @ epoch 37129 )\n",
      "[Epoch: 37420] train loss: 0.0671, train acc: 0.9768, val loss: 0.1793, val acc: 0.9659  (best train acc: 0.9839, best val acc: 0.9713, best train loss: 0.0517  @ epoch 37129 )\n",
      "[Epoch: 37440] train loss: 0.1112, train acc: 0.9568, val loss: 0.1530, val acc: 0.9487  (best train acc: 0.9839, best val acc: 0.9713, best train loss: 0.0517  @ epoch 37129 )\n",
      "[Epoch: 37460] train loss: 0.0780, train acc: 0.9698, val loss: 0.1600, val acc: 0.9690  (best train acc: 0.9839, best val acc: 0.9713, best train loss: 0.0517  @ epoch 37129 )\n",
      "[Epoch: 37480] train loss: 0.0654, train acc: 0.9772, val loss: 0.1690, val acc: 0.9656  (best train acc: 0.9839, best val acc: 0.9713, best train loss: 0.0517  @ epoch 37129 )\n",
      "[Epoch: 37500] train loss: 0.0567, train acc: 0.9813, val loss: 0.1753, val acc: 0.9663  (best train acc: 0.9839, best val acc: 0.9713, best train loss: 0.0517  @ epoch 37129 )\n",
      "[Epoch: 37520] train loss: 0.1117, train acc: 0.9592, val loss: 0.1729, val acc: 0.9589  (best train acc: 0.9839, best val acc: 0.9713, best train loss: 0.0517  @ epoch 37129 )\n",
      "[Epoch: 37540] train loss: 0.0657, train acc: 0.9770, val loss: 0.1533, val acc: 0.9642  (best train acc: 0.9839, best val acc: 0.9713, best train loss: 0.0517  @ epoch 37129 )\n",
      "[Epoch: 37560] train loss: 0.0626, train acc: 0.9787, val loss: 0.1646, val acc: 0.9666  (best train acc: 0.9839, best val acc: 0.9713, best train loss: 0.0517  @ epoch 37129 )\n",
      "[Epoch: 37580] train loss: 0.0528, train acc: 0.9831, val loss: 0.1711, val acc: 0.9676  (best train acc: 0.9844, best val acc: 0.9713, best train loss: 0.0516  @ epoch 37579 )\n",
      "[Epoch: 37600] train loss: 0.0623, train acc: 0.9807, val loss: 0.1856, val acc: 0.9656  (best train acc: 0.9844, best val acc: 0.9713, best train loss: 0.0516  @ epoch 37579 )\n",
      "[Epoch: 37620] train loss: 0.0676, train acc: 0.9756, val loss: 0.1520, val acc: 0.9673  (best train acc: 0.9844, best val acc: 0.9713, best train loss: 0.0516  @ epoch 37579 )\n",
      "[Epoch: 37640] train loss: 0.0532, train acc: 0.9832, val loss: 0.1511, val acc: 0.9676  (best train acc: 0.9844, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 37660] train loss: 0.0596, train acc: 0.9796, val loss: 0.1716, val acc: 0.9676  (best train acc: 0.9844, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 37680] train loss: 0.0608, train acc: 0.9813, val loss: 0.1747, val acc: 0.9676  (best train acc: 0.9844, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 37700] train loss: 0.0657, train acc: 0.9815, val loss: 0.1697, val acc: 0.9649  (best train acc: 0.9844, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 37720] train loss: 0.0556, train acc: 0.9815, val loss: 0.1674, val acc: 0.9649  (best train acc: 0.9844, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 37740] train loss: 0.0753, train acc: 0.9764, val loss: 0.1556, val acc: 0.9653  (best train acc: 0.9844, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 37760] train loss: 0.0712, train acc: 0.9763, val loss: 0.1805, val acc: 0.9690  (best train acc: 0.9844, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 37780] train loss: 0.0872, train acc: 0.9719, val loss: 0.1500, val acc: 0.9636  (best train acc: 0.9844, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 37800] train loss: 0.0736, train acc: 0.9766, val loss: 0.1688, val acc: 0.9653  (best train acc: 0.9844, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 37820] train loss: 0.0887, train acc: 0.9701, val loss: 0.1493, val acc: 0.9686  (best train acc: 0.9844, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 37840] train loss: 0.0707, train acc: 0.9793, val loss: 0.1693, val acc: 0.9653  (best train acc: 0.9844, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 37860] train loss: 0.0567, train acc: 0.9833, val loss: 0.1627, val acc: 0.9686  (best train acc: 0.9844, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 37880] train loss: 0.0637, train acc: 0.9803, val loss: 0.1656, val acc: 0.9666  (best train acc: 0.9844, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 37900] train loss: 0.0583, train acc: 0.9807, val loss: 0.1574, val acc: 0.9666  (best train acc: 0.9844, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 37920] train loss: 0.0598, train acc: 0.9804, val loss: 0.1610, val acc: 0.9669  (best train acc: 0.9844, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 37940] train loss: 0.0604, train acc: 0.9801, val loss: 0.1949, val acc: 0.9663  (best train acc: 0.9844, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 37960] train loss: 0.0740, train acc: 0.9735, val loss: 0.1975, val acc: 0.9589  (best train acc: 0.9844, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 37980] train loss: 0.0623, train acc: 0.9795, val loss: 0.1791, val acc: 0.9676  (best train acc: 0.9844, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 38000] train loss: 0.0673, train acc: 0.9754, val loss: 0.1966, val acc: 0.9686  (best train acc: 0.9844, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 38020] train loss: 0.0618, train acc: 0.9813, val loss: 0.1672, val acc: 0.9663  (best train acc: 0.9844, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 38040] train loss: 0.0624, train acc: 0.9790, val loss: 0.1694, val acc: 0.9622  (best train acc: 0.9844, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 38060] train loss: 0.0573, train acc: 0.9803, val loss: 0.1711, val acc: 0.9666  (best train acc: 0.9844, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 38080] train loss: 0.0562, train acc: 0.9816, val loss: 0.1576, val acc: 0.9686  (best train acc: 0.9844, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 38100] train loss: 0.0766, train acc: 0.9731, val loss: 0.1670, val acc: 0.9669  (best train acc: 0.9844, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 38120] train loss: 0.0599, train acc: 0.9816, val loss: 0.1879, val acc: 0.9673  (best train acc: 0.9844, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 38140] train loss: 0.0596, train acc: 0.9798, val loss: 0.1836, val acc: 0.9639  (best train acc: 0.9844, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 38160] train loss: 0.0563, train acc: 0.9806, val loss: 0.1682, val acc: 0.9683  (best train acc: 0.9844, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 38180] train loss: 0.0690, train acc: 0.9767, val loss: 0.1802, val acc: 0.9663  (best train acc: 0.9844, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 38200] train loss: 0.0671, train acc: 0.9741, val loss: 0.1598, val acc: 0.9680  (best train acc: 0.9844, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 38220] train loss: 0.1014, train acc: 0.9665, val loss: 0.1754, val acc: 0.9595  (best train acc: 0.9844, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 38240] train loss: 0.1007, train acc: 0.9705, val loss: 0.2008, val acc: 0.9602  (best train acc: 0.9844, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 38260] train loss: 0.0613, train acc: 0.9803, val loss: 0.1697, val acc: 0.9693  (best train acc: 0.9844, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 38280] train loss: 0.0628, train acc: 0.9786, val loss: 0.1698, val acc: 0.9700  (best train acc: 0.9844, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 38300] train loss: 0.0548, train acc: 0.9829, val loss: 0.1692, val acc: 0.9666  (best train acc: 0.9844, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 38320] train loss: 0.0593, train acc: 0.9811, val loss: 0.1846, val acc: 0.9663  (best train acc: 0.9844, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 38340] train loss: 0.0656, train acc: 0.9782, val loss: 0.1738, val acc: 0.9676  (best train acc: 0.9847, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 38360] train loss: 0.0610, train acc: 0.9789, val loss: 0.1914, val acc: 0.9646  (best train acc: 0.9847, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 38380] train loss: 0.0742, train acc: 0.9719, val loss: 0.1500, val acc: 0.9663  (best train acc: 0.9847, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 38400] train loss: 0.0666, train acc: 0.9768, val loss: 0.1819, val acc: 0.9673  (best train acc: 0.9847, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 38420] train loss: 0.0648, train acc: 0.9771, val loss: 0.1761, val acc: 0.9683  (best train acc: 0.9847, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 38440] train loss: 0.0557, train acc: 0.9821, val loss: 0.1871, val acc: 0.9663  (best train acc: 0.9847, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 38460] train loss: 0.0542, train acc: 0.9814, val loss: 0.1609, val acc: 0.9646  (best train acc: 0.9847, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 38480] train loss: 0.0583, train acc: 0.9800, val loss: 0.1726, val acc: 0.9669  (best train acc: 0.9847, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 38500] train loss: 0.0639, train acc: 0.9786, val loss: 0.1766, val acc: 0.9669  (best train acc: 0.9847, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 38520] train loss: 0.0803, train acc: 0.9672, val loss: 0.1909, val acc: 0.9551  (best train acc: 0.9847, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 38540] train loss: 0.0854, train acc: 0.9675, val loss: 0.1871, val acc: 0.9605  (best train acc: 0.9847, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 38560] train loss: 0.0591, train acc: 0.9805, val loss: 0.1732, val acc: 0.9669  (best train acc: 0.9847, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 38580] train loss: 0.0637, train acc: 0.9800, val loss: 0.1331, val acc: 0.9669  (best train acc: 0.9847, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 38600] train loss: 0.0602, train acc: 0.9797, val loss: 0.1570, val acc: 0.9666  (best train acc: 0.9847, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 38620] train loss: 0.0937, train acc: 0.9670, val loss: 0.1501, val acc: 0.9636  (best train acc: 0.9847, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 38640] train loss: 0.0886, train acc: 0.9647, val loss: 0.1796, val acc: 0.9494  (best train acc: 0.9847, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 38660] train loss: 0.0782, train acc: 0.9730, val loss: 0.1678, val acc: 0.9616  (best train acc: 0.9847, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 38680] train loss: 0.0633, train acc: 0.9777, val loss: 0.1473, val acc: 0.9666  (best train acc: 0.9847, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 38700] train loss: 0.0969, train acc: 0.9699, val loss: 0.1579, val acc: 0.9649  (best train acc: 0.9847, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 38720] train loss: 0.0732, train acc: 0.9754, val loss: 0.1552, val acc: 0.9642  (best train acc: 0.9847, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 38740] train loss: 0.0625, train acc: 0.9801, val loss: 0.1409, val acc: 0.9666  (best train acc: 0.9847, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 38760] train loss: 0.0649, train acc: 0.9785, val loss: 0.1707, val acc: 0.9646  (best train acc: 0.9847, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 38780] train loss: 0.0645, train acc: 0.9786, val loss: 0.1689, val acc: 0.9663  (best train acc: 0.9847, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 38800] train loss: 0.0599, train acc: 0.9793, val loss: 0.1737, val acc: 0.9669  (best train acc: 0.9847, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 38820] train loss: 0.0582, train acc: 0.9803, val loss: 0.1499, val acc: 0.9663  (best train acc: 0.9847, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 38840] train loss: 0.0645, train acc: 0.9770, val loss: 0.1628, val acc: 0.9663  (best train acc: 0.9847, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 38860] train loss: 0.0670, train acc: 0.9776, val loss: 0.1783, val acc: 0.9659  (best train acc: 0.9847, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 38880] train loss: 0.0632, train acc: 0.9789, val loss: 0.1664, val acc: 0.9666  (best train acc: 0.9847, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 38900] train loss: 0.0718, train acc: 0.9734, val loss: 0.1941, val acc: 0.9619  (best train acc: 0.9847, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 38920] train loss: 0.0660, train acc: 0.9783, val loss: 0.1852, val acc: 0.9646  (best train acc: 0.9847, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 38940] train loss: 0.0690, train acc: 0.9749, val loss: 0.1622, val acc: 0.9673  (best train acc: 0.9847, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 38960] train loss: 0.0756, train acc: 0.9753, val loss: 0.1724, val acc: 0.9629  (best train acc: 0.9847, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 38980] train loss: 0.0685, train acc: 0.9773, val loss: 0.1707, val acc: 0.9656  (best train acc: 0.9847, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 39000] train loss: 0.0607, train acc: 0.9798, val loss: 0.1650, val acc: 0.9676  (best train acc: 0.9847, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 39020] train loss: 0.0584, train acc: 0.9814, val loss: 0.1756, val acc: 0.9683  (best train acc: 0.9847, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 39040] train loss: 0.0661, train acc: 0.9771, val loss: 0.1775, val acc: 0.9669  (best train acc: 0.9847, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 39060] train loss: 0.0649, train acc: 0.9764, val loss: 0.1513, val acc: 0.9669  (best train acc: 0.9847, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 39080] train loss: 0.0637, train acc: 0.9777, val loss: 0.1623, val acc: 0.9673  (best train acc: 0.9847, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 39100] train loss: 0.0571, train acc: 0.9810, val loss: 0.1874, val acc: 0.9663  (best train acc: 0.9847, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 39120] train loss: 0.0737, train acc: 0.9743, val loss: 0.1836, val acc: 0.9680  (best train acc: 0.9847, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 39140] train loss: 0.0584, train acc: 0.9819, val loss: 0.1792, val acc: 0.9676  (best train acc: 0.9847, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 39160] train loss: 0.0619, train acc: 0.9780, val loss: 0.1894, val acc: 0.9666  (best train acc: 0.9847, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 39180] train loss: 0.0637, train acc: 0.9796, val loss: 0.1884, val acc: 0.9676  (best train acc: 0.9847, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 39200] train loss: 0.0696, train acc: 0.9757, val loss: 0.1890, val acc: 0.9686  (best train acc: 0.9847, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 39220] train loss: 0.0620, train acc: 0.9798, val loss: 0.1820, val acc: 0.9683  (best train acc: 0.9847, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 39240] train loss: 0.0547, train acc: 0.9818, val loss: 0.1616, val acc: 0.9663  (best train acc: 0.9847, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 39260] train loss: 0.0564, train acc: 0.9804, val loss: 0.1714, val acc: 0.9669  (best train acc: 0.9847, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 39280] train loss: 0.0651, train acc: 0.9781, val loss: 0.1801, val acc: 0.9649  (best train acc: 0.9847, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 39300] train loss: 0.0578, train acc: 0.9829, val loss: 0.1756, val acc: 0.9676  (best train acc: 0.9847, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 39320] train loss: 0.0952, train acc: 0.9665, val loss: 0.1635, val acc: 0.9622  (best train acc: 0.9847, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 39340] train loss: 0.0724, train acc: 0.9768, val loss: 0.1854, val acc: 0.9686  (best train acc: 0.9847, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 39360] train loss: 0.0603, train acc: 0.9803, val loss: 0.1603, val acc: 0.9673  (best train acc: 0.9847, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 39380] train loss: 0.0596, train acc: 0.9808, val loss: 0.1677, val acc: 0.9659  (best train acc: 0.9847, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 39400] train loss: 0.0781, train acc: 0.9714, val loss: 0.1679, val acc: 0.9612  (best train acc: 0.9847, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 39420] train loss: 0.0586, train acc: 0.9821, val loss: 0.1655, val acc: 0.9642  (best train acc: 0.9847, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 39440] train loss: 0.0544, train acc: 0.9819, val loss: 0.1741, val acc: 0.9659  (best train acc: 0.9847, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 39460] train loss: 0.0668, train acc: 0.9765, val loss: 0.1584, val acc: 0.9673  (best train acc: 0.9847, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 39480] train loss: 0.0748, train acc: 0.9726, val loss: 0.1565, val acc: 0.9683  (best train acc: 0.9847, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 39500] train loss: 0.0579, train acc: 0.9806, val loss: 0.1724, val acc: 0.9669  (best train acc: 0.9847, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 39520] train loss: 0.0610, train acc: 0.9806, val loss: 0.1593, val acc: 0.9669  (best train acc: 0.9847, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 39540] train loss: 0.0553, train acc: 0.9821, val loss: 0.1782, val acc: 0.9663  (best train acc: 0.9847, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 39560] train loss: 0.0543, train acc: 0.9824, val loss: 0.1874, val acc: 0.9680  (best train acc: 0.9847, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 39580] train loss: 0.0704, train acc: 0.9744, val loss: 0.1767, val acc: 0.9676  (best train acc: 0.9847, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 39600] train loss: 0.0533, train acc: 0.9830, val loss: 0.1855, val acc: 0.9673  (best train acc: 0.9847, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 39620] train loss: 0.0554, train acc: 0.9812, val loss: 0.1745, val acc: 0.9649  (best train acc: 0.9847, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 39640] train loss: 0.0588, train acc: 0.9800, val loss: 0.1869, val acc: 0.9673  (best train acc: 0.9847, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 39660] train loss: 0.0641, train acc: 0.9767, val loss: 0.1584, val acc: 0.9680  (best train acc: 0.9847, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 39680] train loss: 0.0552, train acc: 0.9825, val loss: 0.1915, val acc: 0.9663  (best train acc: 0.9847, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 39700] train loss: 0.0664, train acc: 0.9774, val loss: 0.1805, val acc: 0.9680  (best train acc: 0.9847, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 39720] train loss: 0.0735, train acc: 0.9733, val loss: 0.1658, val acc: 0.9680  (best train acc: 0.9847, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 39740] train loss: 0.1226, train acc: 0.9574, val loss: 0.1584, val acc: 0.9501  (best train acc: 0.9847, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 39760] train loss: 0.0762, train acc: 0.9739, val loss: 0.1686, val acc: 0.9663  (best train acc: 0.9847, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 39780] train loss: 0.0833, train acc: 0.9738, val loss: 0.1502, val acc: 0.9656  (best train acc: 0.9847, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 39800] train loss: 0.0567, train acc: 0.9826, val loss: 0.1997, val acc: 0.9663  (best train acc: 0.9847, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 39820] train loss: 0.0595, train acc: 0.9799, val loss: 0.1941, val acc: 0.9676  (best train acc: 0.9847, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 39840] train loss: 0.0576, train acc: 0.9814, val loss: 0.1777, val acc: 0.9659  (best train acc: 0.9847, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 39860] train loss: 0.0657, train acc: 0.9777, val loss: 0.1935, val acc: 0.9646  (best train acc: 0.9847, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 39880] train loss: 0.0653, train acc: 0.9756, val loss: 0.1712, val acc: 0.9696  (best train acc: 0.9847, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 39900] train loss: 0.0552, train acc: 0.9816, val loss: 0.1681, val acc: 0.9693  (best train acc: 0.9847, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 39920] train loss: 0.0678, train acc: 0.9765, val loss: 0.1645, val acc: 0.9669  (best train acc: 0.9847, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 39940] train loss: 0.0580, train acc: 0.9821, val loss: 0.1679, val acc: 0.9700  (best train acc: 0.9847, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 39960] train loss: 0.0618, train acc: 0.9782, val loss: 0.1729, val acc: 0.9673  (best train acc: 0.9847, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 39980] train loss: 0.0614, train acc: 0.9794, val loss: 0.1613, val acc: 0.9669  (best train acc: 0.9847, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n",
      "[Epoch: 40000] train loss: 0.0870, train acc: 0.9718, val loss: 0.1495, val acc: 0.9666  (best train acc: 0.9847, best val acc: 0.9713, best train loss: 0.0510  @ epoch 37637 )\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.93      0.93       154\n",
      "           1       0.98      0.98      0.98       703\n",
      "           2       0.95      0.96      0.96       702\n",
      "           3       0.97      0.95      0.96       703\n",
      "           4       0.99      1.00      0.99       702\n",
      "\n",
      "    accuracy                           0.97      2964\n",
      "   macro avg       0.96      0.96      0.96      2964\n",
      "weighted avg       0.97      0.97      0.97      2964\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 504x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 504x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "gnn_model = GNN7L_GraphConv(data_with_nedbit).to(device)\n",
    "pred = train(gnn_model, data_with_nedbit, epochs, lr, weight_decay, classes, 'GraphCONV')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GraphSAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f19ec33e673f482f871d8fd414ad8e01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40001 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch: 0000] train loss: 89.1047, train acc: 0.2359, val loss: 74.7601, val acc: 0.2368  (best train acc: 0.2359, best val acc: 0.2368, best train loss: 89.1047  @ epoch 0 )\n",
      "[Epoch: 0020] train loss: 5.1212, train acc: 0.2501, val loss: 2.3855, val acc: 0.3815  (best train acc: 0.2693, best val acc: 0.4445, best train loss: 5.1212  @ epoch 20 )\n",
      "[Epoch: 0040] train loss: 1.4736, train acc: 0.2976, val loss: 1.2375, val acc: 0.4681  (best train acc: 0.3991, best val acc: 0.5747, best train loss: 1.4736  @ epoch 40 )\n",
      "[Epoch: 0060] train loss: 1.3117, train acc: 0.4592, val loss: 1.1192, val acc: 0.6725  (best train acc: 0.4592, best val acc: 0.6725, best train loss: 1.3117  @ epoch 60 )\n",
      "[Epoch: 0080] train loss: 1.2965, train acc: 0.4680, val loss: 1.0616, val acc: 0.6793  (best train acc: 0.4727, best val acc: 0.6793, best train loss: 1.2876  @ epoch 77 )\n",
      "[Epoch: 0100] train loss: 1.2605, train acc: 0.4822, val loss: 1.0054, val acc: 0.7184  (best train acc: 0.4863, best val acc: 0.7184, best train loss: 1.2581  @ epoch 98 )\n",
      "[Epoch: 0120] train loss: 1.2424, train acc: 0.4853, val loss: 0.9756, val acc: 0.7612  (best train acc: 0.4920, best val acc: 0.7612, best train loss: 1.2422  @ epoch 116 )\n",
      "[Epoch: 0140] train loss: 1.2256, train acc: 0.4933, val loss: 0.9546, val acc: 0.7727  (best train acc: 0.4994, best val acc: 0.7727, best train loss: 1.2228  @ epoch 127 )\n",
      "[Epoch: 0160] train loss: 1.2207, train acc: 0.4911, val loss: 0.9416, val acc: 0.7801  (best train acc: 0.5017, best val acc: 0.7838, best train loss: 1.2051  @ epoch 155 )\n",
      "[Epoch: 0180] train loss: 1.1996, train acc: 0.5045, val loss: 0.9262, val acc: 0.7895  (best train acc: 0.5076, best val acc: 0.7906, best train loss: 1.1926  @ epoch 176 )\n",
      "[Epoch: 0200] train loss: 1.1858, train acc: 0.5105, val loss: 0.8983, val acc: 0.7956  (best train acc: 0.5192, best val acc: 0.7963, best train loss: 1.1735  @ epoch 191 )\n",
      "[Epoch: 0220] train loss: 1.1690, train acc: 0.5275, val loss: 0.8893, val acc: 0.8034  (best train acc: 0.5275, best val acc: 0.8051, best train loss: 1.1690  @ epoch 220 )\n",
      "[Epoch: 0240] train loss: 1.1563, train acc: 0.5218, val loss: 0.8777, val acc: 0.7949  (best train acc: 0.5312, best val acc: 0.8078, best train loss: 1.1561  @ epoch 236 )\n",
      "[Epoch: 0260] train loss: 1.1484, train acc: 0.5285, val loss: 0.8545, val acc: 0.8105  (best train acc: 0.5341, best val acc: 0.8132, best train loss: 1.1426  @ epoch 254 )\n",
      "[Epoch: 0280] train loss: 1.1340, train acc: 0.5293, val loss: 0.8443, val acc: 0.8145  (best train acc: 0.5344, best val acc: 0.8145, best train loss: 1.1259  @ epoch 275 )\n",
      "[Epoch: 0300] train loss: 1.1193, train acc: 0.5409, val loss: 0.8272, val acc: 0.8199  (best train acc: 0.5414, best val acc: 0.8199, best train loss: 1.1142  @ epoch 295 )\n",
      "[Epoch: 0320] train loss: 1.1037, train acc: 0.5398, val loss: 0.8014, val acc: 0.8250  (best train acc: 0.5495, best val acc: 0.8293, best train loss: 1.0978  @ epoch 317 )\n",
      "[Epoch: 0340] train loss: 1.0995, train acc: 0.5411, val loss: 0.7923, val acc: 0.8347  (best train acc: 0.5576, best val acc: 0.8361, best train loss: 1.0773  @ epoch 336 )\n",
      "[Epoch: 0360] train loss: 1.0801, train acc: 0.5504, val loss: 0.7636, val acc: 0.8371  (best train acc: 0.5576, best val acc: 0.8381, best train loss: 1.0764  @ epoch 349 )\n",
      "[Epoch: 0380] train loss: 1.0747, train acc: 0.5540, val loss: 0.7743, val acc: 0.8411  (best train acc: 0.5586, best val acc: 0.8445, best train loss: 1.0646  @ epoch 375 )\n",
      "[Epoch: 0400] train loss: 1.0587, train acc: 0.5622, val loss: 0.7319, val acc: 0.8503  (best train acc: 0.5667, best val acc: 0.8503, best train loss: 1.0539  @ epoch 397 )\n",
      "[Epoch: 0420] train loss: 1.0376, train acc: 0.5722, val loss: 0.7199, val acc: 0.8543  (best train acc: 0.5750, best val acc: 0.8567, best train loss: 1.0308  @ epoch 419 )\n",
      "[Epoch: 0440] train loss: 1.0162, train acc: 0.5893, val loss: 0.7001, val acc: 0.8614  (best train acc: 0.5893, best val acc: 0.8614, best train loss: 1.0162  @ epoch 440 )\n",
      "[Epoch: 0460] train loss: 1.0127, train acc: 0.5989, val loss: 0.6950, val acc: 0.8600  (best train acc: 0.6029, best val acc: 0.8637, best train loss: 1.0031  @ epoch 457 )\n",
      "[Epoch: 0480] train loss: 0.9961, train acc: 0.6108, val loss: 0.6817, val acc: 0.8631  (best train acc: 0.6123, best val acc: 0.8698, best train loss: 0.9961  @ epoch 480 )\n",
      "[Epoch: 0500] train loss: 0.9860, train acc: 0.6167, val loss: 0.6700, val acc: 0.8725  (best train acc: 0.6212, best val acc: 0.8732, best train loss: 0.9813  @ epoch 495 )\n",
      "[Epoch: 0520] train loss: 1.0000, train acc: 0.6048, val loss: 0.6521, val acc: 0.8749  (best train acc: 0.6242, best val acc: 0.8776, best train loss: 0.9685  @ epoch 517 )\n",
      "[Epoch: 0540] train loss: 0.9652, train acc: 0.6244, val loss: 0.6419, val acc: 0.8755  (best train acc: 0.6244, best val acc: 0.8779, best train loss: 0.9651  @ epoch 538 )\n",
      "[Epoch: 0560] train loss: 0.9668, train acc: 0.6163, val loss: 0.6344, val acc: 0.8799  (best train acc: 0.6244, best val acc: 0.8806, best train loss: 0.9601  @ epoch 552 )\n",
      "[Epoch: 0580] train loss: 0.9648, train acc: 0.6155, val loss: 0.6267, val acc: 0.8809  (best train acc: 0.6265, best val acc: 0.8823, best train loss: 0.9487  @ epoch 578 )\n",
      "[Epoch: 0600] train loss: 0.9591, train acc: 0.6145, val loss: 0.6225, val acc: 0.8840  (best train acc: 0.6272, best val acc: 0.8857, best train loss: 0.9487  @ epoch 578 )\n",
      "[Epoch: 0620] train loss: 0.9544, train acc: 0.6199, val loss: 0.6217, val acc: 0.8830  (best train acc: 0.6280, best val acc: 0.8877, best train loss: 0.9413  @ epoch 612 )\n",
      "[Epoch: 0640] train loss: 0.9515, train acc: 0.6183, val loss: 0.6066, val acc: 0.8874  (best train acc: 0.6280, best val acc: 0.8887, best train loss: 0.9397  @ epoch 636 )\n",
      "[Epoch: 0660] train loss: 0.9363, train acc: 0.6228, val loss: 0.5974, val acc: 0.8860  (best train acc: 0.6297, best val acc: 0.8914, best train loss: 0.9284  @ epoch 657 )\n",
      "[Epoch: 0680] train loss: 0.9388, train acc: 0.6210, val loss: 0.5870, val acc: 0.8901  (best train acc: 0.6297, best val acc: 0.8914, best train loss: 0.9222  @ epoch 678 )\n",
      "[Epoch: 0700] train loss: 0.9371, train acc: 0.6202, val loss: 0.5834, val acc: 0.8904  (best train acc: 0.6301, best val acc: 0.8917, best train loss: 0.9212  @ epoch 684 )\n",
      "[Epoch: 0720] train loss: 0.9264, train acc: 0.6282, val loss: 0.5805, val acc: 0.8907  (best train acc: 0.6318, best val acc: 0.8921, best train loss: 0.9169  @ epoch 709 )\n",
      "[Epoch: 0740] train loss: 0.9430, train acc: 0.6134, val loss: 0.5830, val acc: 0.8904  (best train acc: 0.6318, best val acc: 0.8921, best train loss: 0.9169  @ epoch 709 )\n",
      "[Epoch: 0760] train loss: 0.9232, train acc: 0.6292, val loss: 0.5681, val acc: 0.8931  (best train acc: 0.6318, best val acc: 0.8931, best train loss: 0.9169  @ epoch 709 )\n",
      "[Epoch: 0780] train loss: 0.9162, train acc: 0.6255, val loss: 0.5678, val acc: 0.8884  (best train acc: 0.6332, best val acc: 0.8931, best train loss: 0.9090  @ epoch 775 )\n",
      "[Epoch: 0800] train loss: 0.9134, train acc: 0.6254, val loss: 0.5634, val acc: 0.8887  (best train acc: 0.6332, best val acc: 0.8948, best train loss: 0.9034  @ epoch 793 )\n",
      "[Epoch: 0820] train loss: 0.9044, train acc: 0.6267, val loss: 0.5742, val acc: 0.8755  (best train acc: 0.6332, best val acc: 0.8951, best train loss: 0.9034  @ epoch 793 )\n",
      "[Epoch: 0840] train loss: 0.9137, train acc: 0.6257, val loss: 0.5602, val acc: 0.8921  (best train acc: 0.6332, best val acc: 0.8951, best train loss: 0.9034  @ epoch 793 )\n",
      "[Epoch: 0860] train loss: 0.8983, train acc: 0.6310, val loss: 0.5554, val acc: 0.8931  (best train acc: 0.6340, best val acc: 0.8951, best train loss: 0.8973  @ epoch 854 )\n",
      "[Epoch: 0880] train loss: 0.9286, train acc: 0.6153, val loss: 0.5764, val acc: 0.8607  (best train acc: 0.6340, best val acc: 0.8951, best train loss: 0.8973  @ epoch 854 )\n",
      "[Epoch: 0900] train loss: 0.9159, train acc: 0.6240, val loss: 0.5562, val acc: 0.8870  (best train acc: 0.6340, best val acc: 0.8951, best train loss: 0.8930  @ epoch 893 )\n",
      "[Epoch: 0920] train loss: 0.9154, train acc: 0.6199, val loss: 0.5416, val acc: 0.8938  (best train acc: 0.6340, best val acc: 0.8954, best train loss: 0.8889  @ epoch 919 )\n",
      "[Epoch: 0940] train loss: 0.9078, train acc: 0.6205, val loss: 0.5556, val acc: 0.8742  (best train acc: 0.6340, best val acc: 0.8954, best train loss: 0.8874  @ epoch 926 )\n",
      "[Epoch: 0960] train loss: 0.8975, train acc: 0.6196, val loss: 0.4956, val acc: 0.8911  (best train acc: 0.6340, best val acc: 0.8954, best train loss: 0.8862  @ epoch 958 )\n",
      "[Epoch: 0980] train loss: 0.8611, train acc: 0.6424, val loss: 0.5069, val acc: 0.8884  (best train acc: 0.6424, best val acc: 0.8954, best train loss: 0.8611  @ epoch 980 )\n",
      "[Epoch: 1000] train loss: 0.8713, train acc: 0.6266, val loss: 0.4908, val acc: 0.8897  (best train acc: 0.6424, best val acc: 0.8954, best train loss: 0.8457  @ epoch 998 )\n",
      "[Epoch: 1020] train loss: 0.8571, train acc: 0.6387, val loss: 0.5001, val acc: 0.8840  (best train acc: 0.6424, best val acc: 0.8954, best train loss: 0.8435  @ epoch 1010 )\n",
      "[Epoch: 1040] train loss: 0.8643, train acc: 0.6342, val loss: 0.5032, val acc: 0.8614  (best train acc: 0.6432, best val acc: 0.8954, best train loss: 0.8435  @ epoch 1010 )\n",
      "[Epoch: 1060] train loss: 0.8570, train acc: 0.6359, val loss: 0.4654, val acc: 0.8874  (best train acc: 0.6580, best val acc: 0.8954, best train loss: 0.8429  @ epoch 1049 )\n",
      "[Epoch: 1080] train loss: 0.8312, train acc: 0.6495, val loss: 0.4793, val acc: 0.8857  (best train acc: 0.6580, best val acc: 0.8954, best train loss: 0.8312  @ epoch 1080 )\n",
      "[Epoch: 1100] train loss: 0.8387, train acc: 0.6492, val loss: 0.4724, val acc: 0.8951  (best train acc: 0.6656, best val acc: 0.8965, best train loss: 0.8259  @ epoch 1098 )\n",
      "[Epoch: 1120] train loss: 0.8542, train acc: 0.6336, val loss: 0.4549, val acc: 0.8981  (best train acc: 0.6703, best val acc: 0.8985, best train loss: 0.8073  @ epoch 1104 )\n",
      "[Epoch: 1140] train loss: 0.8535, train acc: 0.6596, val loss: 0.4978, val acc: 0.8702  (best train acc: 0.6760, best val acc: 0.8985, best train loss: 0.8073  @ epoch 1104 )\n",
      "[Epoch: 1160] train loss: 0.8468, train acc: 0.6417, val loss: 0.4297, val acc: 0.8958  (best train acc: 0.6760, best val acc: 0.8985, best train loss: 0.8073  @ epoch 1104 )\n",
      "[Epoch: 1180] train loss: 0.8115, train acc: 0.6737, val loss: 0.4381, val acc: 0.8971  (best train acc: 0.6873, best val acc: 0.9002, best train loss: 0.8073  @ epoch 1104 )\n",
      "[Epoch: 1200] train loss: 0.8187, train acc: 0.6721, val loss: 0.4405, val acc: 0.8985  (best train acc: 0.6919, best val acc: 0.9015, best train loss: 0.7987  @ epoch 1194 )\n",
      "[Epoch: 1220] train loss: 0.8135, train acc: 0.6685, val loss: 0.4452, val acc: 0.8853  (best train acc: 0.6948, best val acc: 0.9015, best train loss: 0.7902  @ epoch 1214 )\n",
      "[Epoch: 1240] train loss: 0.8695, train acc: 0.6763, val loss: 0.5072, val acc: 0.8833  (best train acc: 0.6948, best val acc: 0.9025, best train loss: 0.7902  @ epoch 1214 )\n",
      "[Epoch: 1260] train loss: 0.7997, train acc: 0.6920, val loss: 0.4486, val acc: 0.8931  (best train acc: 0.6948, best val acc: 0.9025, best train loss: 0.7902  @ epoch 1214 )\n",
      "[Epoch: 1280] train loss: 0.7980, train acc: 0.6928, val loss: 0.4374, val acc: 0.8958  (best train acc: 0.7071, best val acc: 0.9029, best train loss: 0.7902  @ epoch 1214 )\n",
      "[Epoch: 1300] train loss: 0.8067, train acc: 0.6736, val loss: 0.4334, val acc: 0.9005  (best train acc: 0.7071, best val acc: 0.9029, best train loss: 0.7752  @ epoch 1285 )\n",
      "[Epoch: 1320] train loss: 0.7954, train acc: 0.7012, val loss: 0.4366, val acc: 0.8971  (best train acc: 0.7071, best val acc: 0.9029, best train loss: 0.7752  @ epoch 1285 )\n",
      "[Epoch: 1340] train loss: 0.7859, train acc: 0.6974, val loss: 0.4299, val acc: 0.8985  (best train acc: 0.7099, best val acc: 0.9035, best train loss: 0.7752  @ epoch 1285 )\n",
      "[Epoch: 1360] train loss: 0.8110, train acc: 0.6852, val loss: 0.4185, val acc: 0.8998  (best train acc: 0.7110, best val acc: 0.9035, best train loss: 0.7572  @ epoch 1347 )\n",
      "[Epoch: 1380] train loss: 0.7963, train acc: 0.6945, val loss: 0.4196, val acc: 0.9002  (best train acc: 0.7115, best val acc: 0.9046, best train loss: 0.7572  @ epoch 1347 )\n",
      "[Epoch: 1400] train loss: 0.7971, train acc: 0.6944, val loss: 0.4220, val acc: 0.9022  (best train acc: 0.7144, best val acc: 0.9046, best train loss: 0.7572  @ epoch 1347 )\n",
      "[Epoch: 1420] train loss: 0.8011, train acc: 0.6876, val loss: 0.4238, val acc: 0.8904  (best train acc: 0.7144, best val acc: 0.9046, best train loss: 0.7572  @ epoch 1347 )\n",
      "[Epoch: 1440] train loss: 0.8063, train acc: 0.6823, val loss: 0.4203, val acc: 0.8938  (best train acc: 0.7164, best val acc: 0.9046, best train loss: 0.7572  @ epoch 1347 )\n",
      "[Epoch: 1460] train loss: 0.7727, train acc: 0.7084, val loss: 0.4075, val acc: 0.9002  (best train acc: 0.7206, best val acc: 0.9046, best train loss: 0.7533  @ epoch 1448 )\n",
      "[Epoch: 1480] train loss: 0.7770, train acc: 0.7110, val loss: 0.4073, val acc: 0.9032  (best train acc: 0.7206, best val acc: 0.9046, best train loss: 0.7532  @ epoch 1463 )\n",
      "[Epoch: 1500] train loss: 0.7855, train acc: 0.7039, val loss: 0.4277, val acc: 0.8985  (best train acc: 0.7206, best val acc: 0.9046, best train loss: 0.7526  @ epoch 1494 )\n",
      "[Epoch: 1520] train loss: 0.7727, train acc: 0.7002, val loss: 0.4028, val acc: 0.9032  (best train acc: 0.7211, best val acc: 0.9059, best train loss: 0.7501  @ epoch 1507 )\n",
      "[Epoch: 1540] train loss: 0.7798, train acc: 0.7030, val loss: 0.4196, val acc: 0.8894  (best train acc: 0.7211, best val acc: 0.9059, best train loss: 0.7501  @ epoch 1507 )\n",
      "[Epoch: 1560] train loss: 0.7988, train acc: 0.6840, val loss: 0.3958, val acc: 0.9042  (best train acc: 0.7211, best val acc: 0.9062, best train loss: 0.7498  @ epoch 1551 )\n",
      "[Epoch: 1580] train loss: 0.7655, train acc: 0.7159, val loss: 0.4075, val acc: 0.9056  (best train acc: 0.7277, best val acc: 0.9062, best train loss: 0.7394  @ epoch 1570 )\n",
      "[Epoch: 1600] train loss: 0.8022, train acc: 0.6872, val loss: 0.3988, val acc: 0.9052  (best train acc: 0.7277, best val acc: 0.9062, best train loss: 0.7394  @ epoch 1570 )\n",
      "[Epoch: 1620] train loss: 0.7748, train acc: 0.7058, val loss: 0.4153, val acc: 0.9039  (best train acc: 0.7277, best val acc: 0.9062, best train loss: 0.7394  @ epoch 1570 )\n",
      "[Epoch: 1640] train loss: 0.7885, train acc: 0.6859, val loss: 0.3791, val acc: 0.9062  (best train acc: 0.7277, best val acc: 0.9079, best train loss: 0.7394  @ epoch 1570 )\n",
      "[Epoch: 1660] train loss: 0.7797, train acc: 0.6948, val loss: 0.3944, val acc: 0.9079  (best train acc: 0.7298, best val acc: 0.9079, best train loss: 0.7394  @ epoch 1570 )\n",
      "[Epoch: 1680] train loss: 0.7726, train acc: 0.7054, val loss: 0.4302, val acc: 0.8789  (best train acc: 0.7298, best val acc: 0.9079, best train loss: 0.7313  @ epoch 1662 )\n",
      "[Epoch: 1700] train loss: 0.7660, train acc: 0.6977, val loss: 0.3991, val acc: 0.9029  (best train acc: 0.7298, best val acc: 0.9079, best train loss: 0.7231  @ epoch 1697 )\n",
      "[Epoch: 1720] train loss: 0.7658, train acc: 0.7115, val loss: 0.4005, val acc: 0.8995  (best train acc: 0.7298, best val acc: 0.9089, best train loss: 0.7201  @ epoch 1717 )\n",
      "[Epoch: 1740] train loss: 0.7215, train acc: 0.7271, val loss: 0.4139, val acc: 0.9052  (best train acc: 0.7298, best val acc: 0.9106, best train loss: 0.7201  @ epoch 1717 )\n",
      "[Epoch: 1760] train loss: 0.7317, train acc: 0.7208, val loss: 0.3970, val acc: 0.9086  (best train acc: 0.7333, best val acc: 0.9106, best train loss: 0.7201  @ epoch 1717 )\n",
      "[Epoch: 1780] train loss: 0.7473, train acc: 0.7279, val loss: 0.4181, val acc: 0.9002  (best train acc: 0.7333, best val acc: 0.9110, best train loss: 0.7201  @ epoch 1717 )\n",
      "[Epoch: 1800] train loss: 0.7654, train acc: 0.7084, val loss: 0.3906, val acc: 0.9103  (best train acc: 0.7333, best val acc: 0.9110, best train loss: 0.7201  @ epoch 1717 )\n",
      "[Epoch: 1820] train loss: 0.7532, train acc: 0.7178, val loss: 0.4064, val acc: 0.8934  (best train acc: 0.7333, best val acc: 0.9110, best train loss: 0.7148  @ epoch 1819 )\n",
      "[Epoch: 1840] train loss: 0.7866, train acc: 0.6870, val loss: 0.3781, val acc: 0.9096  (best train acc: 0.7333, best val acc: 0.9116, best train loss: 0.7148  @ epoch 1819 )\n",
      "[Epoch: 1860] train loss: 0.7493, train acc: 0.7167, val loss: 0.3965, val acc: 0.9056  (best train acc: 0.7333, best val acc: 0.9116, best train loss: 0.7148  @ epoch 1819 )\n",
      "[Epoch: 1880] train loss: 0.7607, train acc: 0.6990, val loss: 0.3844, val acc: 0.9103  (best train acc: 0.7346, best val acc: 0.9116, best train loss: 0.7148  @ epoch 1819 )\n",
      "[Epoch: 1900] train loss: 0.7463, train acc: 0.7209, val loss: 0.3834, val acc: 0.9052  (best train acc: 0.7354, best val acc: 0.9130, best train loss: 0.7148  @ epoch 1819 )\n",
      "[Epoch: 1920] train loss: 0.7475, train acc: 0.7202, val loss: 0.3800, val acc: 0.9126  (best train acc: 0.7360, best val acc: 0.9130, best train loss: 0.7106  @ epoch 1903 )\n",
      "[Epoch: 1940] train loss: 0.7271, train acc: 0.7272, val loss: 0.3903, val acc: 0.9083  (best train acc: 0.7360, best val acc: 0.9140, best train loss: 0.7106  @ epoch 1903 )\n",
      "[Epoch: 1960] train loss: 0.7637, train acc: 0.6980, val loss: 0.3810, val acc: 0.9073  (best train acc: 0.7360, best val acc: 0.9140, best train loss: 0.7106  @ epoch 1903 )\n",
      "[Epoch: 1980] train loss: 0.7361, train acc: 0.7263, val loss: 0.3918, val acc: 0.9096  (best train acc: 0.7394, best val acc: 0.9140, best train loss: 0.7008  @ epoch 1975 )\n",
      "[Epoch: 2000] train loss: 0.7662, train acc: 0.6986, val loss: 0.3859, val acc: 0.9116  (best train acc: 0.7394, best val acc: 0.9140, best train loss: 0.7008  @ epoch 1975 )\n",
      "[Epoch: 2020] train loss: 0.7655, train acc: 0.7075, val loss: 0.4361, val acc: 0.8961  (best train acc: 0.7394, best val acc: 0.9140, best train loss: 0.7008  @ epoch 1975 )\n",
      "[Epoch: 2040] train loss: 0.7653, train acc: 0.7076, val loss: 0.3874, val acc: 0.9079  (best train acc: 0.7394, best val acc: 0.9140, best train loss: 0.7008  @ epoch 1975 )\n",
      "[Epoch: 2060] train loss: 0.7451, train acc: 0.7120, val loss: 0.3948, val acc: 0.8938  (best train acc: 0.7394, best val acc: 0.9140, best train loss: 0.7008  @ epoch 1975 )\n",
      "[Epoch: 2080] train loss: 0.7343, train acc: 0.7195, val loss: 0.4052, val acc: 0.9005  (best train acc: 0.7394, best val acc: 0.9140, best train loss: 0.6974  @ epoch 2073 )\n",
      "[Epoch: 2100] train loss: 0.7399, train acc: 0.7214, val loss: 0.4110, val acc: 0.8951  (best train acc: 0.7394, best val acc: 0.9140, best train loss: 0.6974  @ epoch 2073 )\n",
      "[Epoch: 2120] train loss: 0.7061, train acc: 0.7368, val loss: 0.4197, val acc: 0.9015  (best train acc: 0.7394, best val acc: 0.9140, best train loss: 0.6974  @ epoch 2073 )\n",
      "[Epoch: 2140] train loss: 0.7427, train acc: 0.7261, val loss: 0.3976, val acc: 0.9110  (best train acc: 0.7394, best val acc: 0.9140, best train loss: 0.6974  @ epoch 2073 )\n",
      "[Epoch: 2160] train loss: 0.7057, train acc: 0.7350, val loss: 0.4306, val acc: 0.9079  (best train acc: 0.7404, best val acc: 0.9140, best train loss: 0.6940  @ epoch 2141 )\n",
      "[Epoch: 2180] train loss: 0.7914, train acc: 0.6828, val loss: 0.4088, val acc: 0.9089  (best train acc: 0.7454, best val acc: 0.9140, best train loss: 0.6940  @ epoch 2141 )\n",
      "[Epoch: 2200] train loss: 0.7676, train acc: 0.6947, val loss: 0.3876, val acc: 0.9089  (best train acc: 0.7465, best val acc: 0.9140, best train loss: 0.6906  @ epoch 2199 )\n",
      "[Epoch: 2220] train loss: 0.7269, train acc: 0.7317, val loss: 0.4127, val acc: 0.9073  (best train acc: 0.7465, best val acc: 0.9140, best train loss: 0.6906  @ epoch 2199 )\n",
      "[Epoch: 2240] train loss: 0.7124, train acc: 0.7368, val loss: 0.4033, val acc: 0.9076  (best train acc: 0.7465, best val acc: 0.9140, best train loss: 0.6906  @ epoch 2199 )\n",
      "[Epoch: 2260] train loss: 0.7088, train acc: 0.7390, val loss: 0.4040, val acc: 0.9076  (best train acc: 0.7465, best val acc: 0.9147, best train loss: 0.6906  @ epoch 2199 )\n",
      "[Epoch: 2280] train loss: 0.7279, train acc: 0.7245, val loss: 0.4114, val acc: 0.8944  (best train acc: 0.7465, best val acc: 0.9147, best train loss: 0.6906  @ epoch 2199 )\n",
      "[Epoch: 2300] train loss: 0.7387, train acc: 0.7123, val loss: 0.3823, val acc: 0.9056  (best train acc: 0.7465, best val acc: 0.9147, best train loss: 0.6884  @ epoch 2295 )\n",
      "[Epoch: 2320] train loss: 0.7353, train acc: 0.7164, val loss: 0.3939, val acc: 0.9029  (best train acc: 0.7465, best val acc: 0.9147, best train loss: 0.6884  @ epoch 2295 )\n",
      "[Epoch: 2340] train loss: 0.7288, train acc: 0.7196, val loss: 0.3737, val acc: 0.9079  (best train acc: 0.7465, best val acc: 0.9147, best train loss: 0.6801  @ epoch 2338 )\n",
      "[Epoch: 2360] train loss: 0.7136, train acc: 0.7325, val loss: 0.3696, val acc: 0.9073  (best train acc: 0.7465, best val acc: 0.9147, best train loss: 0.6801  @ epoch 2338 )\n",
      "[Epoch: 2380] train loss: 0.7117, train acc: 0.7342, val loss: 0.3798, val acc: 0.9042  (best train acc: 0.7465, best val acc: 0.9147, best train loss: 0.6801  @ epoch 2338 )\n",
      "[Epoch: 2400] train loss: 0.7758, train acc: 0.7019, val loss: 0.3683, val acc: 0.9073  (best train acc: 0.7465, best val acc: 0.9147, best train loss: 0.6801  @ epoch 2338 )\n",
      "[Epoch: 2420] train loss: 0.7265, train acc: 0.7249, val loss: 0.3935, val acc: 0.9059  (best train acc: 0.7465, best val acc: 0.9147, best train loss: 0.6801  @ epoch 2338 )\n",
      "[Epoch: 2440] train loss: 0.7119, train acc: 0.7313, val loss: 0.3914, val acc: 0.9093  (best train acc: 0.7465, best val acc: 0.9147, best train loss: 0.6801  @ epoch 2338 )\n",
      "[Epoch: 2460] train loss: 0.7562, train acc: 0.7045, val loss: 0.3835, val acc: 0.9022  (best train acc: 0.7465, best val acc: 0.9147, best train loss: 0.6801  @ epoch 2338 )\n",
      "[Epoch: 2480] train loss: 0.7262, train acc: 0.7182, val loss: 0.3588, val acc: 0.9069  (best train acc: 0.7465, best val acc: 0.9147, best train loss: 0.6801  @ epoch 2338 )\n",
      "[Epoch: 2500] train loss: 0.7112, train acc: 0.7298, val loss: 0.3897, val acc: 0.9110  (best train acc: 0.7465, best val acc: 0.9147, best train loss: 0.6801  @ epoch 2338 )\n",
      "[Epoch: 2520] train loss: 0.6892, train acc: 0.7396, val loss: 0.3936, val acc: 0.9086  (best train acc: 0.7465, best val acc: 0.9147, best train loss: 0.6801  @ epoch 2338 )\n",
      "[Epoch: 2540] train loss: 0.7282, train acc: 0.7170, val loss: 0.3713, val acc: 0.9032  (best train acc: 0.7465, best val acc: 0.9147, best train loss: 0.6801  @ epoch 2338 )\n",
      "[Epoch: 2560] train loss: 0.7573, train acc: 0.7052, val loss: 0.3650, val acc: 0.9025  (best train acc: 0.7465, best val acc: 0.9147, best train loss: 0.6801  @ epoch 2338 )\n",
      "[Epoch: 2580] train loss: 0.7345, train acc: 0.7156, val loss: 0.3756, val acc: 0.9062  (best train acc: 0.7465, best val acc: 0.9147, best train loss: 0.6801  @ epoch 2338 )\n",
      "[Epoch: 2600] train loss: 0.7109, train acc: 0.7219, val loss: 0.3854, val acc: 0.9066  (best train acc: 0.7465, best val acc: 0.9147, best train loss: 0.6801  @ epoch 2338 )\n",
      "[Epoch: 2620] train loss: 0.7315, train acc: 0.7178, val loss: 0.3759, val acc: 0.9015  (best train acc: 0.7465, best val acc: 0.9147, best train loss: 0.6776  @ epoch 2608 )\n",
      "[Epoch: 2640] train loss: 0.7419, train acc: 0.7039, val loss: 0.3703, val acc: 0.9106  (best train acc: 0.7465, best val acc: 0.9147, best train loss: 0.6776  @ epoch 2608 )\n",
      "[Epoch: 2660] train loss: 0.6873, train acc: 0.7439, val loss: 0.3710, val acc: 0.9062  (best train acc: 0.7465, best val acc: 0.9147, best train loss: 0.6776  @ epoch 2608 )\n",
      "[Epoch: 2680] train loss: 0.7070, train acc: 0.7212, val loss: 0.3725, val acc: 0.9089  (best train acc: 0.7465, best val acc: 0.9147, best train loss: 0.6727  @ epoch 2664 )\n",
      "[Epoch: 2700] train loss: 0.7020, train acc: 0.7287, val loss: 0.3862, val acc: 0.9046  (best train acc: 0.7522, best val acc: 0.9147, best train loss: 0.6614  @ epoch 2688 )\n",
      "[Epoch: 2720] train loss: 0.6765, train acc: 0.7467, val loss: 0.4131, val acc: 0.9066  (best train acc: 0.7522, best val acc: 0.9147, best train loss: 0.6614  @ epoch 2688 )\n",
      "[Epoch: 2740] train loss: 0.7177, train acc: 0.7155, val loss: 0.3851, val acc: 0.9123  (best train acc: 0.7522, best val acc: 0.9147, best train loss: 0.6614  @ epoch 2688 )\n",
      "[Epoch: 2760] train loss: 0.7116, train acc: 0.7226, val loss: 0.3658, val acc: 0.9083  (best train acc: 0.7522, best val acc: 0.9147, best train loss: 0.6614  @ epoch 2688 )\n",
      "[Epoch: 2780] train loss: 0.7100, train acc: 0.7217, val loss: 0.3749, val acc: 0.9083  (best train acc: 0.7522, best val acc: 0.9147, best train loss: 0.6614  @ epoch 2688 )\n",
      "[Epoch: 2800] train loss: 0.7155, train acc: 0.7219, val loss: 0.3758, val acc: 0.9069  (best train acc: 0.7522, best val acc: 0.9147, best train loss: 0.6614  @ epoch 2688 )\n",
      "[Epoch: 2820] train loss: 0.7086, train acc: 0.7316, val loss: 0.3831, val acc: 0.9035  (best train acc: 0.7522, best val acc: 0.9147, best train loss: 0.6614  @ epoch 2688 )\n",
      "[Epoch: 2840] train loss: 0.6991, train acc: 0.7345, val loss: 0.3927, val acc: 0.9103  (best train acc: 0.7522, best val acc: 0.9147, best train loss: 0.6614  @ epoch 2688 )\n",
      "[Epoch: 2860] train loss: 0.7047, train acc: 0.7241, val loss: 0.3816, val acc: 0.9103  (best train acc: 0.7522, best val acc: 0.9147, best train loss: 0.6614  @ epoch 2688 )\n",
      "[Epoch: 2880] train loss: 0.7145, train acc: 0.7225, val loss: 0.3912, val acc: 0.8816  (best train acc: 0.7522, best val acc: 0.9147, best train loss: 0.6614  @ epoch 2688 )\n",
      "[Epoch: 2900] train loss: 0.6809, train acc: 0.7429, val loss: 0.3863, val acc: 0.9116  (best train acc: 0.7522, best val acc: 0.9147, best train loss: 0.6569  @ epoch 2887 )\n",
      "[Epoch: 2920] train loss: 0.7149, train acc: 0.7141, val loss: 0.3728, val acc: 0.9086  (best train acc: 0.7522, best val acc: 0.9147, best train loss: 0.6569  @ epoch 2887 )\n",
      "[Epoch: 2940] train loss: 0.7006, train acc: 0.7296, val loss: 0.3800, val acc: 0.9069  (best train acc: 0.7522, best val acc: 0.9147, best train loss: 0.6569  @ epoch 2887 )\n",
      "[Epoch: 2960] train loss: 0.7082, train acc: 0.7308, val loss: 0.3780, val acc: 0.8853  (best train acc: 0.7522, best val acc: 0.9147, best train loss: 0.6569  @ epoch 2887 )\n",
      "[Epoch: 2980] train loss: 0.6463, train acc: 0.7689, val loss: 0.3531, val acc: 0.9079  (best train acc: 0.7689, best val acc: 0.9147, best train loss: 0.6463  @ epoch 2980 )\n",
      "[Epoch: 3000] train loss: 0.6262, train acc: 0.7671, val loss: 0.3560, val acc: 0.9089  (best train acc: 0.7710, best val acc: 0.9147, best train loss: 0.6262  @ epoch 3000 )\n",
      "[Epoch: 3020] train loss: 0.6706, train acc: 0.7496, val loss: 0.3571, val acc: 0.9056  (best train acc: 0.7764, best val acc: 0.9147, best train loss: 0.6262  @ epoch 3000 )\n",
      "[Epoch: 3040] train loss: 0.6336, train acc: 0.7640, val loss: 0.3766, val acc: 0.8941  (best train acc: 0.7764, best val acc: 0.9147, best train loss: 0.6123  @ epoch 3027 )\n",
      "[Epoch: 3060] train loss: 0.6512, train acc: 0.7604, val loss: 0.3580, val acc: 0.9083  (best train acc: 0.7764, best val acc: 0.9147, best train loss: 0.6123  @ epoch 3027 )\n",
      "[Epoch: 3080] train loss: 0.6149, train acc: 0.7754, val loss: 0.3745, val acc: 0.9086  (best train acc: 0.7826, best val acc: 0.9147, best train loss: 0.6086  @ epoch 3064 )\n",
      "[Epoch: 3100] train loss: 0.6469, train acc: 0.7597, val loss: 0.3633, val acc: 0.9079  (best train acc: 0.7826, best val acc: 0.9147, best train loss: 0.6015  @ epoch 3088 )\n",
      "[Epoch: 3120] train loss: 0.6607, train acc: 0.7386, val loss: 0.3519, val acc: 0.9106  (best train acc: 0.7826, best val acc: 0.9147, best train loss: 0.6015  @ epoch 3088 )\n",
      "[Epoch: 3140] train loss: 0.6686, train acc: 0.7415, val loss: 0.3527, val acc: 0.9096  (best train acc: 0.7826, best val acc: 0.9147, best train loss: 0.6015  @ epoch 3088 )\n",
      "[Epoch: 3160] train loss: 0.6394, train acc: 0.7637, val loss: 0.3449, val acc: 0.9103  (best train acc: 0.7826, best val acc: 0.9147, best train loss: 0.6015  @ epoch 3088 )\n",
      "[Epoch: 3180] train loss: 0.6592, train acc: 0.7508, val loss: 0.3719, val acc: 0.8948  (best train acc: 0.7826, best val acc: 0.9147, best train loss: 0.6015  @ epoch 3088 )\n",
      "[Epoch: 3200] train loss: 0.6550, train acc: 0.7411, val loss: 0.3539, val acc: 0.9099  (best train acc: 0.7826, best val acc: 0.9147, best train loss: 0.6015  @ epoch 3088 )\n",
      "[Epoch: 3220] train loss: 0.6377, train acc: 0.7686, val loss: 0.3521, val acc: 0.9093  (best train acc: 0.7826, best val acc: 0.9147, best train loss: 0.6015  @ epoch 3088 )\n",
      "[Epoch: 3240] train loss: 0.6452, train acc: 0.7554, val loss: 0.3377, val acc: 0.9062  (best train acc: 0.7826, best val acc: 0.9147, best train loss: 0.6015  @ epoch 3088 )\n",
      "[Epoch: 3260] train loss: 0.6412, train acc: 0.7501, val loss: 0.3448, val acc: 0.9079  (best train acc: 0.7826, best val acc: 0.9147, best train loss: 0.6015  @ epoch 3088 )\n",
      "[Epoch: 3280] train loss: 0.6401, train acc: 0.7495, val loss: 0.3447, val acc: 0.9099  (best train acc: 0.7826, best val acc: 0.9147, best train loss: 0.6015  @ epoch 3088 )\n",
      "[Epoch: 3300] train loss: 0.6597, train acc: 0.7475, val loss: 0.3446, val acc: 0.9099  (best train acc: 0.7826, best val acc: 0.9147, best train loss: 0.6015  @ epoch 3088 )\n",
      "[Epoch: 3320] train loss: 0.6225, train acc: 0.7636, val loss: 0.3515, val acc: 0.8985  (best train acc: 0.7840, best val acc: 0.9147, best train loss: 0.5912  @ epoch 3317 )\n",
      "[Epoch: 3340] train loss: 0.6590, train acc: 0.7413, val loss: 0.3446, val acc: 0.9140  (best train acc: 0.7840, best val acc: 0.9147, best train loss: 0.5912  @ epoch 3317 )\n",
      "[Epoch: 3360] train loss: 0.6420, train acc: 0.7501, val loss: 0.3442, val acc: 0.9083  (best train acc: 0.7840, best val acc: 0.9147, best train loss: 0.5912  @ epoch 3317 )\n",
      "[Epoch: 3380] train loss: 0.6270, train acc: 0.7736, val loss: 0.3661, val acc: 0.9059  (best train acc: 0.7840, best val acc: 0.9147, best train loss: 0.5912  @ epoch 3317 )\n",
      "[Epoch: 3400] train loss: 0.6004, train acc: 0.7794, val loss: 0.3557, val acc: 0.8985  (best train acc: 0.7840, best val acc: 0.9147, best train loss: 0.5912  @ epoch 3317 )\n",
      "[Epoch: 3420] train loss: 0.6942, train acc: 0.7134, val loss: 0.3520, val acc: 0.9103  (best train acc: 0.7840, best val acc: 0.9147, best train loss: 0.5912  @ epoch 3317 )\n",
      "[Epoch: 3440] train loss: 0.6202, train acc: 0.7685, val loss: 0.3375, val acc: 0.8988  (best train acc: 0.7898, best val acc: 0.9147, best train loss: 0.5912  @ epoch 3317 )\n",
      "[Epoch: 3460] train loss: 0.6463, train acc: 0.7355, val loss: 0.3451, val acc: 0.8944  (best train acc: 0.7898, best val acc: 0.9147, best train loss: 0.5912  @ epoch 3317 )\n",
      "[Epoch: 3480] train loss: 0.6177, train acc: 0.7632, val loss: 0.3710, val acc: 0.9035  (best train acc: 0.7898, best val acc: 0.9147, best train loss: 0.5912  @ epoch 3317 )\n",
      "[Epoch: 3500] train loss: 0.5992, train acc: 0.7848, val loss: 0.3476, val acc: 0.9076  (best train acc: 0.7898, best val acc: 0.9147, best train loss: 0.5912  @ epoch 3317 )\n",
      "[Epoch: 3520] train loss: 0.6395, train acc: 0.7540, val loss: 0.3609, val acc: 0.8823  (best train acc: 0.7898, best val acc: 0.9147, best train loss: 0.5895  @ epoch 3517 )\n",
      "[Epoch: 3540] train loss: 0.6253, train acc: 0.7624, val loss: 0.3374, val acc: 0.9066  (best train acc: 0.7898, best val acc: 0.9147, best train loss: 0.5895  @ epoch 3517 )\n",
      "[Epoch: 3560] train loss: 0.6179, train acc: 0.7677, val loss: 0.3412, val acc: 0.9069  (best train acc: 0.7898, best val acc: 0.9147, best train loss: 0.5817  @ epoch 3559 )\n",
      "[Epoch: 3580] train loss: 0.5868, train acc: 0.7773, val loss: 0.3516, val acc: 0.9046  (best train acc: 0.7908, best val acc: 0.9147, best train loss: 0.5796  @ epoch 3575 )\n",
      "[Epoch: 3600] train loss: 0.5917, train acc: 0.7841, val loss: 0.3274, val acc: 0.9106  (best train acc: 0.7908, best val acc: 0.9147, best train loss: 0.5796  @ epoch 3575 )\n",
      "[Epoch: 3620] train loss: 0.5924, train acc: 0.7780, val loss: 0.3408, val acc: 0.9089  (best train acc: 0.7908, best val acc: 0.9147, best train loss: 0.5796  @ epoch 3575 )\n",
      "[Epoch: 3640] train loss: 0.5894, train acc: 0.7741, val loss: 0.3500, val acc: 0.9099  (best train acc: 0.7908, best val acc: 0.9147, best train loss: 0.5796  @ epoch 3575 )\n",
      "[Epoch: 3660] train loss: 0.5863, train acc: 0.7775, val loss: 0.3473, val acc: 0.9019  (best train acc: 0.7908, best val acc: 0.9147, best train loss: 0.5796  @ epoch 3575 )\n",
      "[Epoch: 3680] train loss: 0.5786, train acc: 0.7841, val loss: 0.3399, val acc: 0.9106  (best train acc: 0.7932, best val acc: 0.9147, best train loss: 0.5718  @ epoch 3675 )\n",
      "[Epoch: 3700] train loss: 0.6029, train acc: 0.7705, val loss: 0.3293, val acc: 0.9096  (best train acc: 0.7932, best val acc: 0.9147, best train loss: 0.5644  @ epoch 3694 )\n",
      "[Epoch: 3720] train loss: 0.6519, train acc: 0.7409, val loss: 0.3518, val acc: 0.8877  (best train acc: 0.7976, best val acc: 0.9147, best train loss: 0.5644  @ epoch 3694 )\n",
      "[Epoch: 3740] train loss: 0.5999, train acc: 0.7701, val loss: 0.3281, val acc: 0.9103  (best train acc: 0.7976, best val acc: 0.9147, best train loss: 0.5644  @ epoch 3694 )\n",
      "[Epoch: 3760] train loss: 0.5883, train acc: 0.7807, val loss: 0.3451, val acc: 0.9062  (best train acc: 0.7976, best val acc: 0.9147, best train loss: 0.5644  @ epoch 3694 )\n",
      "[Epoch: 3780] train loss: 0.5767, train acc: 0.7876, val loss: 0.3417, val acc: 0.9076  (best train acc: 0.7976, best val acc: 0.9147, best train loss: 0.5644  @ epoch 3694 )\n",
      "[Epoch: 3800] train loss: 0.5902, train acc: 0.7725, val loss: 0.3342, val acc: 0.9059  (best train acc: 0.7976, best val acc: 0.9147, best train loss: 0.5515  @ epoch 3788 )\n",
      "[Epoch: 3820] train loss: 0.5799, train acc: 0.7829, val loss: 0.3324, val acc: 0.9089  (best train acc: 0.7976, best val acc: 0.9153, best train loss: 0.5515  @ epoch 3788 )\n",
      "[Epoch: 3840] train loss: 0.6043, train acc: 0.7652, val loss: 0.3246, val acc: 0.9086  (best train acc: 0.7976, best val acc: 0.9153, best train loss: 0.5515  @ epoch 3788 )\n",
      "[Epoch: 3860] train loss: 0.5827, train acc: 0.7836, val loss: 0.3526, val acc: 0.9012  (best train acc: 0.7985, best val acc: 0.9153, best train loss: 0.5453  @ epoch 3843 )\n",
      "[Epoch: 3880] train loss: 0.6241, train acc: 0.7494, val loss: 0.3238, val acc: 0.9110  (best train acc: 0.7985, best val acc: 0.9153, best train loss: 0.5453  @ epoch 3843 )\n",
      "[Epoch: 3900] train loss: 0.5860, train acc: 0.7747, val loss: 0.3397, val acc: 0.9012  (best train acc: 0.7985, best val acc: 0.9153, best train loss: 0.5453  @ epoch 3843 )\n",
      "[Epoch: 3920] train loss: 0.5877, train acc: 0.7777, val loss: 0.3442, val acc: 0.8965  (best train acc: 0.8049, best val acc: 0.9153, best train loss: 0.5453  @ epoch 3843 )\n",
      "[Epoch: 3940] train loss: 0.5871, train acc: 0.7839, val loss: 0.3324, val acc: 0.9133  (best train acc: 0.8049, best val acc: 0.9153, best train loss: 0.5453  @ epoch 3843 )\n",
      "[Epoch: 3960] train loss: 0.5631, train acc: 0.7807, val loss: 0.3468, val acc: 0.9012  (best train acc: 0.8049, best val acc: 0.9153, best train loss: 0.5453  @ epoch 3843 )\n",
      "[Epoch: 3980] train loss: 0.5824, train acc: 0.7728, val loss: 0.3455, val acc: 0.9042  (best train acc: 0.8049, best val acc: 0.9153, best train loss: 0.5449  @ epoch 3979 )\n",
      "[Epoch: 4000] train loss: 0.5748, train acc: 0.7757, val loss: 0.3301, val acc: 0.9103  (best train acc: 0.8049, best val acc: 0.9153, best train loss: 0.5393  @ epoch 3998 )\n",
      "[Epoch: 4020] train loss: 0.5711, train acc: 0.7882, val loss: 0.3352, val acc: 0.9083  (best train acc: 0.8049, best val acc: 0.9153, best train loss: 0.5393  @ epoch 3998 )\n",
      "[Epoch: 4040] train loss: 0.5758, train acc: 0.7770, val loss: 0.3256, val acc: 0.9099  (best train acc: 0.8049, best val acc: 0.9153, best train loss: 0.5393  @ epoch 3998 )\n",
      "[Epoch: 4060] train loss: 0.5526, train acc: 0.7962, val loss: 0.3380, val acc: 0.9093  (best train acc: 0.8049, best val acc: 0.9153, best train loss: 0.5393  @ epoch 3998 )\n",
      "[Epoch: 4080] train loss: 0.5624, train acc: 0.7862, val loss: 0.3275, val acc: 0.9130  (best train acc: 0.8049, best val acc: 0.9153, best train loss: 0.5393  @ epoch 3998 )\n",
      "[Epoch: 4100] train loss: 0.5523, train acc: 0.7933, val loss: 0.3236, val acc: 0.9099  (best train acc: 0.8049, best val acc: 0.9157, best train loss: 0.5393  @ epoch 3998 )\n",
      "[Epoch: 4120] train loss: 0.5578, train acc: 0.7883, val loss: 0.3218, val acc: 0.9113  (best train acc: 0.8049, best val acc: 0.9157, best train loss: 0.5393  @ epoch 3998 )\n",
      "[Epoch: 4140] train loss: 0.5648, train acc: 0.7796, val loss: 0.3352, val acc: 0.9076  (best train acc: 0.8049, best val acc: 0.9157, best train loss: 0.5393  @ epoch 3998 )\n",
      "[Epoch: 4160] train loss: 0.5852, train acc: 0.7764, val loss: 0.3215, val acc: 0.9113  (best train acc: 0.8049, best val acc: 0.9157, best train loss: 0.5362  @ epoch 4156 )\n",
      "[Epoch: 4180] train loss: 0.5733, train acc: 0.7767, val loss: 0.3312, val acc: 0.9083  (best train acc: 0.8049, best val acc: 0.9157, best train loss: 0.5362  @ epoch 4156 )\n",
      "[Epoch: 4200] train loss: 0.5872, train acc: 0.7734, val loss: 0.3279, val acc: 0.9059  (best train acc: 0.8049, best val acc: 0.9157, best train loss: 0.5362  @ epoch 4156 )\n",
      "[Epoch: 4220] train loss: 0.5765, train acc: 0.7783, val loss: 0.3245, val acc: 0.9103  (best train acc: 0.8049, best val acc: 0.9157, best train loss: 0.5362  @ epoch 4156 )\n",
      "[Epoch: 4240] train loss: 0.5863, train acc: 0.7697, val loss: 0.3216, val acc: 0.9116  (best train acc: 0.8049, best val acc: 0.9157, best train loss: 0.5347  @ epoch 4239 )\n",
      "[Epoch: 4260] train loss: 0.6239, train acc: 0.7475, val loss: 0.3291, val acc: 0.9096  (best train acc: 0.8049, best val acc: 0.9157, best train loss: 0.5347  @ epoch 4239 )\n",
      "[Epoch: 4280] train loss: 0.5509, train acc: 0.7902, val loss: 0.3289, val acc: 0.9049  (best train acc: 0.8049, best val acc: 0.9157, best train loss: 0.5347  @ epoch 4239 )\n",
      "[Epoch: 4300] train loss: 0.5850, train acc: 0.7639, val loss: 0.3246, val acc: 0.9093  (best train acc: 0.8049, best val acc: 0.9157, best train loss: 0.5347  @ epoch 4239 )\n",
      "[Epoch: 4320] train loss: 0.5558, train acc: 0.7826, val loss: 0.3185, val acc: 0.9069  (best train acc: 0.8049, best val acc: 0.9157, best train loss: 0.5317  @ epoch 4308 )\n",
      "[Epoch: 4340] train loss: 0.5688, train acc: 0.7866, val loss: 0.3401, val acc: 0.9005  (best train acc: 0.8049, best val acc: 0.9157, best train loss: 0.5317  @ epoch 4308 )\n",
      "[Epoch: 4360] train loss: 0.5855, train acc: 0.7673, val loss: 0.3421, val acc: 0.8965  (best train acc: 0.8049, best val acc: 0.9157, best train loss: 0.5251  @ epoch 4345 )\n",
      "[Epoch: 4380] train loss: 0.5760, train acc: 0.7767, val loss: 0.3389, val acc: 0.9032  (best train acc: 0.8049, best val acc: 0.9157, best train loss: 0.5251  @ epoch 4345 )\n",
      "[Epoch: 4400] train loss: 0.5596, train acc: 0.7820, val loss: 0.3170, val acc: 0.9076  (best train acc: 0.8049, best val acc: 0.9157, best train loss: 0.5251  @ epoch 4345 )\n",
      "[Epoch: 4420] train loss: 0.5492, train acc: 0.7800, val loss: 0.3111, val acc: 0.9110  (best train acc: 0.8049, best val acc: 0.9157, best train loss: 0.5251  @ epoch 4345 )\n",
      "[Epoch: 4440] train loss: 0.5475, train acc: 0.7927, val loss: 0.3068, val acc: 0.9093  (best train acc: 0.8049, best val acc: 0.9157, best train loss: 0.5251  @ epoch 4345 )\n",
      "[Epoch: 4460] train loss: 0.5583, train acc: 0.7802, val loss: 0.3178, val acc: 0.9103  (best train acc: 0.8049, best val acc: 0.9157, best train loss: 0.5251  @ epoch 4345 )\n",
      "[Epoch: 4480] train loss: 0.5521, train acc: 0.7893, val loss: 0.3230, val acc: 0.9066  (best train acc: 0.8049, best val acc: 0.9157, best train loss: 0.5251  @ epoch 4345 )\n",
      "[Epoch: 4500] train loss: 0.5881, train acc: 0.7586, val loss: 0.2979, val acc: 0.9096  (best train acc: 0.8049, best val acc: 0.9157, best train loss: 0.5251  @ epoch 4345 )\n",
      "[Epoch: 4520] train loss: 0.5395, train acc: 0.7873, val loss: 0.3065, val acc: 0.9062  (best train acc: 0.8049, best val acc: 0.9157, best train loss: 0.5251  @ epoch 4345 )\n",
      "[Epoch: 4540] train loss: 0.5392, train acc: 0.7809, val loss: 0.3114, val acc: 0.9086  (best train acc: 0.8049, best val acc: 0.9157, best train loss: 0.5251  @ epoch 4345 )\n",
      "[Epoch: 4560] train loss: 0.5516, train acc: 0.7828, val loss: 0.3022, val acc: 0.9093  (best train acc: 0.8049, best val acc: 0.9157, best train loss: 0.5182  @ epoch 4547 )\n",
      "[Epoch: 4580] train loss: 0.5738, train acc: 0.7671, val loss: 0.3120, val acc: 0.9076  (best train acc: 0.8049, best val acc: 0.9157, best train loss: 0.5182  @ epoch 4547 )\n",
      "[Epoch: 4600] train loss: 0.5436, train acc: 0.7892, val loss: 0.3154, val acc: 0.9069  (best train acc: 0.8049, best val acc: 0.9157, best train loss: 0.5182  @ epoch 4547 )\n",
      "[Epoch: 4620] train loss: 0.5532, train acc: 0.7830, val loss: 0.2980, val acc: 0.9096  (best train acc: 0.8049, best val acc: 0.9157, best train loss: 0.5182  @ epoch 4547 )\n",
      "[Epoch: 4640] train loss: 0.5279, train acc: 0.7878, val loss: 0.3182, val acc: 0.9093  (best train acc: 0.8049, best val acc: 0.9157, best train loss: 0.5182  @ epoch 4547 )\n",
      "[Epoch: 4660] train loss: 0.5466, train acc: 0.7807, val loss: 0.3091, val acc: 0.9029  (best train acc: 0.8049, best val acc: 0.9157, best train loss: 0.5182  @ epoch 4547 )\n",
      "[Epoch: 4680] train loss: 0.5102, train acc: 0.8083, val loss: 0.3008, val acc: 0.9069  (best train acc: 0.8083, best val acc: 0.9157, best train loss: 0.5102  @ epoch 4680 )\n",
      "[Epoch: 4700] train loss: 0.5584, train acc: 0.7835, val loss: 0.3165, val acc: 0.9005  (best train acc: 0.8083, best val acc: 0.9157, best train loss: 0.5102  @ epoch 4680 )\n",
      "[Epoch: 4720] train loss: 0.5328, train acc: 0.7921, val loss: 0.3030, val acc: 0.9110  (best train acc: 0.8083, best val acc: 0.9157, best train loss: 0.5102  @ epoch 4680 )\n",
      "[Epoch: 4740] train loss: 0.5395, train acc: 0.7827, val loss: 0.3320, val acc: 0.8921  (best train acc: 0.8083, best val acc: 0.9157, best train loss: 0.5102  @ epoch 4680 )\n",
      "[Epoch: 4760] train loss: 0.5397, train acc: 0.7957, val loss: 0.2880, val acc: 0.9093  (best train acc: 0.8128, best val acc: 0.9157, best train loss: 0.5102  @ epoch 4680 )\n",
      "[Epoch: 4780] train loss: 0.5252, train acc: 0.8035, val loss: 0.3068, val acc: 0.9083  (best train acc: 0.8128, best val acc: 0.9157, best train loss: 0.5102  @ epoch 4680 )\n",
      "[Epoch: 4800] train loss: 0.5774, train acc: 0.7736, val loss: 0.2996, val acc: 0.9099  (best train acc: 0.8128, best val acc: 0.9157, best train loss: 0.5102  @ epoch 4680 )\n",
      "[Epoch: 4820] train loss: 0.5502, train acc: 0.7854, val loss: 0.2996, val acc: 0.9073  (best train acc: 0.8128, best val acc: 0.9157, best train loss: 0.5102  @ epoch 4680 )\n",
      "[Epoch: 4840] train loss: 0.5311, train acc: 0.7981, val loss: 0.3081, val acc: 0.9079  (best train acc: 0.8128, best val acc: 0.9157, best train loss: 0.5102  @ epoch 4680 )\n",
      "[Epoch: 4860] train loss: 0.5734, train acc: 0.7669, val loss: 0.3039, val acc: 0.9079  (best train acc: 0.8128, best val acc: 0.9157, best train loss: 0.5102  @ epoch 4680 )\n",
      "[Epoch: 4880] train loss: 0.5399, train acc: 0.7904, val loss: 0.3101, val acc: 0.9008  (best train acc: 0.8128, best val acc: 0.9157, best train loss: 0.5102  @ epoch 4680 )\n",
      "[Epoch: 4900] train loss: 0.5431, train acc: 0.7787, val loss: 0.2989, val acc: 0.9083  (best train acc: 0.8128, best val acc: 0.9157, best train loss: 0.5102  @ epoch 4680 )\n",
      "[Epoch: 4920] train loss: 0.5369, train acc: 0.7806, val loss: 0.2938, val acc: 0.9086  (best train acc: 0.8128, best val acc: 0.9157, best train loss: 0.5080  @ epoch 4917 )\n",
      "[Epoch: 4940] train loss: 0.5429, train acc: 0.7787, val loss: 0.3087, val acc: 0.9025  (best train acc: 0.8128, best val acc: 0.9157, best train loss: 0.5075  @ epoch 4938 )\n",
      "[Epoch: 4960] train loss: 0.5356, train acc: 0.7877, val loss: 0.3060, val acc: 0.9099  (best train acc: 0.8128, best val acc: 0.9157, best train loss: 0.5075  @ epoch 4938 )\n",
      "[Epoch: 4980] train loss: 0.5296, train acc: 0.7897, val loss: 0.3055, val acc: 0.9032  (best train acc: 0.8128, best val acc: 0.9157, best train loss: 0.5075  @ epoch 4938 )\n",
      "[Epoch: 5000] train loss: 0.5237, train acc: 0.7945, val loss: 0.3030, val acc: 0.9093  (best train acc: 0.8128, best val acc: 0.9157, best train loss: 0.5075  @ epoch 4938 )\n",
      "[Epoch: 5020] train loss: 0.5575, train acc: 0.7798, val loss: 0.3047, val acc: 0.9110  (best train acc: 0.8128, best val acc: 0.9157, best train loss: 0.5075  @ epoch 4938 )\n",
      "[Epoch: 5040] train loss: 0.5450, train acc: 0.7870, val loss: 0.3183, val acc: 0.8985  (best train acc: 0.8128, best val acc: 0.9157, best train loss: 0.5075  @ epoch 4938 )\n",
      "[Epoch: 5060] train loss: 0.5386, train acc: 0.7906, val loss: 0.2931, val acc: 0.9123  (best train acc: 0.8128, best val acc: 0.9157, best train loss: 0.5075  @ epoch 4938 )\n",
      "[Epoch: 5080] train loss: 0.5348, train acc: 0.7851, val loss: 0.3091, val acc: 0.9089  (best train acc: 0.8128, best val acc: 0.9157, best train loss: 0.5075  @ epoch 4938 )\n",
      "[Epoch: 5100] train loss: 0.5234, train acc: 0.7920, val loss: 0.2955, val acc: 0.9103  (best train acc: 0.8128, best val acc: 0.9157, best train loss: 0.5075  @ epoch 4938 )\n",
      "[Epoch: 5120] train loss: 0.5287, train acc: 0.7829, val loss: 0.2976, val acc: 0.9089  (best train acc: 0.8128, best val acc: 0.9157, best train loss: 0.5075  @ epoch 4938 )\n",
      "[Epoch: 5140] train loss: 0.5296, train acc: 0.7832, val loss: 0.3075, val acc: 0.9093  (best train acc: 0.8128, best val acc: 0.9157, best train loss: 0.5075  @ epoch 4938 )\n",
      "[Epoch: 5160] train loss: 0.5785, train acc: 0.7691, val loss: 0.3129, val acc: 0.8978  (best train acc: 0.8128, best val acc: 0.9157, best train loss: 0.5075  @ epoch 4938 )\n",
      "[Epoch: 5180] train loss: 0.5391, train acc: 0.7788, val loss: 0.3032, val acc: 0.9113  (best train acc: 0.8128, best val acc: 0.9157, best train loss: 0.5075  @ epoch 4938 )\n",
      "[Epoch: 5200] train loss: 0.5531, train acc: 0.7788, val loss: 0.3048, val acc: 0.9079  (best train acc: 0.8128, best val acc: 0.9157, best train loss: 0.5010  @ epoch 5190 )\n",
      "[Epoch: 5220] train loss: 0.5283, train acc: 0.7876, val loss: 0.3090, val acc: 0.9025  (best train acc: 0.8128, best val acc: 0.9157, best train loss: 0.5010  @ epoch 5190 )\n",
      "[Epoch: 5240] train loss: 0.5363, train acc: 0.7957, val loss: 0.3192, val acc: 0.9086  (best train acc: 0.8128, best val acc: 0.9157, best train loss: 0.5010  @ epoch 5190 )\n",
      "[Epoch: 5260] train loss: 0.5191, train acc: 0.7924, val loss: 0.3135, val acc: 0.9069  (best train acc: 0.8128, best val acc: 0.9157, best train loss: 0.5010  @ epoch 5190 )\n",
      "[Epoch: 5280] train loss: 0.5236, train acc: 0.7942, val loss: 0.3020, val acc: 0.9096  (best train acc: 0.8128, best val acc: 0.9157, best train loss: 0.5010  @ epoch 5190 )\n",
      "[Epoch: 5300] train loss: 0.5384, train acc: 0.7885, val loss: 0.3004, val acc: 0.9106  (best train acc: 0.8128, best val acc: 0.9157, best train loss: 0.4997  @ epoch 5294 )\n",
      "[Epoch: 5320] train loss: 0.5450, train acc: 0.7796, val loss: 0.3126, val acc: 0.9032  (best train acc: 0.8128, best val acc: 0.9157, best train loss: 0.4997  @ epoch 5294 )\n",
      "[Epoch: 5340] train loss: 0.5144, train acc: 0.8006, val loss: 0.3101, val acc: 0.9106  (best train acc: 0.8128, best val acc: 0.9157, best train loss: 0.4997  @ epoch 5294 )\n",
      "[Epoch: 5360] train loss: 0.5227, train acc: 0.7900, val loss: 0.3322, val acc: 0.8968  (best train acc: 0.8128, best val acc: 0.9157, best train loss: 0.4997  @ epoch 5294 )\n",
      "[Epoch: 5380] train loss: 0.5526, train acc: 0.7765, val loss: 0.3241, val acc: 0.9062  (best train acc: 0.8128, best val acc: 0.9157, best train loss: 0.4978  @ epoch 5364 )\n",
      "[Epoch: 5400] train loss: 0.5303, train acc: 0.7940, val loss: 0.2933, val acc: 0.9123  (best train acc: 0.8128, best val acc: 0.9157, best train loss: 0.4978  @ epoch 5364 )\n",
      "[Epoch: 5420] train loss: 0.5525, train acc: 0.7676, val loss: 0.3242, val acc: 0.9039  (best train acc: 0.8128, best val acc: 0.9157, best train loss: 0.4978  @ epoch 5364 )\n",
      "[Epoch: 5440] train loss: 0.5626, train acc: 0.7700, val loss: 0.3073, val acc: 0.9042  (best train acc: 0.8128, best val acc: 0.9157, best train loss: 0.4978  @ epoch 5364 )\n",
      "[Epoch: 5460] train loss: 0.5235, train acc: 0.7914, val loss: 0.3186, val acc: 0.9059  (best train acc: 0.8128, best val acc: 0.9157, best train loss: 0.4978  @ epoch 5364 )\n",
      "[Epoch: 5480] train loss: 0.5320, train acc: 0.7893, val loss: 0.3143, val acc: 0.9059  (best train acc: 0.8128, best val acc: 0.9157, best train loss: 0.4978  @ epoch 5364 )\n",
      "[Epoch: 5500] train loss: 0.5196, train acc: 0.7968, val loss: 0.3124, val acc: 0.9110  (best train acc: 0.8135, best val acc: 0.9157, best train loss: 0.4978  @ epoch 5364 )\n",
      "[Epoch: 5520] train loss: 0.5431, train acc: 0.7901, val loss: 0.3206, val acc: 0.9059  (best train acc: 0.8135, best val acc: 0.9157, best train loss: 0.4978  @ epoch 5364 )\n",
      "[Epoch: 5540] train loss: 0.5184, train acc: 0.7945, val loss: 0.3352, val acc: 0.8934  (best train acc: 0.8135, best val acc: 0.9157, best train loss: 0.4978  @ epoch 5364 )\n",
      "[Epoch: 5560] train loss: 0.5198, train acc: 0.7915, val loss: 0.3074, val acc: 0.9130  (best train acc: 0.8135, best val acc: 0.9157, best train loss: 0.4978  @ epoch 5364 )\n",
      "[Epoch: 5580] train loss: 0.5066, train acc: 0.8106, val loss: 0.3366, val acc: 0.8985  (best train acc: 0.8135, best val acc: 0.9157, best train loss: 0.4978  @ epoch 5364 )\n",
      "[Epoch: 5600] train loss: 0.5268, train acc: 0.7874, val loss: 0.3225, val acc: 0.9143  (best train acc: 0.8135, best val acc: 0.9157, best train loss: 0.4978  @ epoch 5364 )\n",
      "[Epoch: 5620] train loss: 0.5255, train acc: 0.7859, val loss: 0.3189, val acc: 0.9099  (best train acc: 0.8135, best val acc: 0.9157, best train loss: 0.4978  @ epoch 5364 )\n",
      "[Epoch: 5640] train loss: 0.5305, train acc: 0.7908, val loss: 0.3423, val acc: 0.8995  (best train acc: 0.8135, best val acc: 0.9157, best train loss: 0.4978  @ epoch 5364 )\n",
      "[Epoch: 5660] train loss: 0.5187, train acc: 0.7882, val loss: 0.3149, val acc: 0.9126  (best train acc: 0.8135, best val acc: 0.9157, best train loss: 0.4978  @ epoch 5364 )\n",
      "[Epoch: 5680] train loss: 0.5061, train acc: 0.8011, val loss: 0.3147, val acc: 0.9099  (best train acc: 0.8135, best val acc: 0.9157, best train loss: 0.4978  @ epoch 5364 )\n",
      "[Epoch: 5700] train loss: 0.5386, train acc: 0.7814, val loss: 0.3099, val acc: 0.9110  (best train acc: 0.8135, best val acc: 0.9157, best train loss: 0.4829  @ epoch 5686 )\n",
      "[Epoch: 5720] train loss: 0.5481, train acc: 0.7818, val loss: 0.3086, val acc: 0.9113  (best train acc: 0.8135, best val acc: 0.9157, best train loss: 0.4829  @ epoch 5686 )\n",
      "[Epoch: 5740] train loss: 0.5269, train acc: 0.7861, val loss: 0.3282, val acc: 0.9029  (best train acc: 0.8180, best val acc: 0.9157, best train loss: 0.4829  @ epoch 5686 )\n",
      "[Epoch: 5760] train loss: 0.5320, train acc: 0.7882, val loss: 0.3077, val acc: 0.9099  (best train acc: 0.8180, best val acc: 0.9157, best train loss: 0.4829  @ epoch 5686 )\n",
      "[Epoch: 5780] train loss: 0.5376, train acc: 0.7787, val loss: 0.3440, val acc: 0.9025  (best train acc: 0.8180, best val acc: 0.9157, best train loss: 0.4829  @ epoch 5686 )\n",
      "[Epoch: 5800] train loss: 0.5538, train acc: 0.7736, val loss: 0.3172, val acc: 0.9116  (best train acc: 0.8180, best val acc: 0.9157, best train loss: 0.4829  @ epoch 5686 )\n",
      "[Epoch: 5820] train loss: 0.5047, train acc: 0.7968, val loss: 0.3226, val acc: 0.9106  (best train acc: 0.8180, best val acc: 0.9157, best train loss: 0.4829  @ epoch 5686 )\n",
      "[Epoch: 5840] train loss: 0.5469, train acc: 0.7903, val loss: 0.3315, val acc: 0.8975  (best train acc: 0.8180, best val acc: 0.9157, best train loss: 0.4829  @ epoch 5686 )\n",
      "[Epoch: 5860] train loss: 0.5511, train acc: 0.7817, val loss: 0.3290, val acc: 0.9035  (best train acc: 0.8180, best val acc: 0.9167, best train loss: 0.4829  @ epoch 5686 )\n",
      "[Epoch: 5880] train loss: 0.5352, train acc: 0.7775, val loss: 0.3208, val acc: 0.9093  (best train acc: 0.8180, best val acc: 0.9167, best train loss: 0.4829  @ epoch 5686 )\n",
      "[Epoch: 5900] train loss: 0.5511, train acc: 0.7816, val loss: 0.3134, val acc: 0.9113  (best train acc: 0.8180, best val acc: 0.9167, best train loss: 0.4829  @ epoch 5686 )\n",
      "[Epoch: 5920] train loss: 0.5300, train acc: 0.7911, val loss: 0.3174, val acc: 0.9099  (best train acc: 0.8180, best val acc: 0.9167, best train loss: 0.4829  @ epoch 5686 )\n",
      "[Epoch: 5940] train loss: 0.5254, train acc: 0.7927, val loss: 0.3113, val acc: 0.9133  (best train acc: 0.8180, best val acc: 0.9167, best train loss: 0.4829  @ epoch 5686 )\n",
      "[Epoch: 5960] train loss: 0.5251, train acc: 0.7843, val loss: 0.3454, val acc: 0.8988  (best train acc: 0.8180, best val acc: 0.9167, best train loss: 0.4829  @ epoch 5686 )\n",
      "[Epoch: 5980] train loss: 0.5498, train acc: 0.7715, val loss: 0.3171, val acc: 0.9123  (best train acc: 0.8180, best val acc: 0.9167, best train loss: 0.4829  @ epoch 5686 )\n",
      "[Epoch: 6000] train loss: 0.5755, train acc: 0.7549, val loss: 0.3364, val acc: 0.8941  (best train acc: 0.8180, best val acc: 0.9167, best train loss: 0.4829  @ epoch 5686 )\n",
      "[Epoch: 6020] train loss: 0.5669, train acc: 0.7663, val loss: 0.3279, val acc: 0.9069  (best train acc: 0.8180, best val acc: 0.9167, best train loss: 0.4829  @ epoch 5686 )\n",
      "[Epoch: 6040] train loss: 0.4993, train acc: 0.8115, val loss: 0.3185, val acc: 0.9052  (best train acc: 0.8180, best val acc: 0.9167, best train loss: 0.4829  @ epoch 5686 )\n",
      "[Epoch: 6060] train loss: 0.4963, train acc: 0.8051, val loss: 0.3268, val acc: 0.9096  (best train acc: 0.8180, best val acc: 0.9167, best train loss: 0.4829  @ epoch 5686 )\n",
      "[Epoch: 6080] train loss: 0.5257, train acc: 0.7981, val loss: 0.3193, val acc: 0.9086  (best train acc: 0.8180, best val acc: 0.9167, best train loss: 0.4829  @ epoch 5686 )\n",
      "[Epoch: 6100] train loss: 0.5002, train acc: 0.8004, val loss: 0.3393, val acc: 0.9140  (best train acc: 0.8180, best val acc: 0.9167, best train loss: 0.4829  @ epoch 5686 )\n",
      "[Epoch: 6120] train loss: 0.5716, train acc: 0.7680, val loss: 0.3322, val acc: 0.9079  (best train acc: 0.8180, best val acc: 0.9167, best train loss: 0.4829  @ epoch 5686 )\n",
      "[Epoch: 6140] train loss: 0.5013, train acc: 0.8044, val loss: 0.3312, val acc: 0.9137  (best train acc: 0.8180, best val acc: 0.9167, best train loss: 0.4829  @ epoch 5686 )\n",
      "[Epoch: 6160] train loss: 0.5403, train acc: 0.7713, val loss: 0.3245, val acc: 0.9123  (best train acc: 0.8180, best val acc: 0.9167, best train loss: 0.4829  @ epoch 5686 )\n",
      "[Epoch: 6180] train loss: 0.5810, train acc: 0.7869, val loss: 0.3693, val acc: 0.8944  (best train acc: 0.8180, best val acc: 0.9167, best train loss: 0.4829  @ epoch 5686 )\n",
      "[Epoch: 6200] train loss: 0.5406, train acc: 0.7772, val loss: 0.3413, val acc: 0.9049  (best train acc: 0.8180, best val acc: 0.9167, best train loss: 0.4829  @ epoch 5686 )\n",
      "[Epoch: 6220] train loss: 0.5115, train acc: 0.7881, val loss: 0.3301, val acc: 0.9096  (best train acc: 0.8180, best val acc: 0.9167, best train loss: 0.4829  @ epoch 5686 )\n",
      "[Epoch: 6240] train loss: 0.4978, train acc: 0.8107, val loss: 0.3675, val acc: 0.8948  (best train acc: 0.8180, best val acc: 0.9167, best train loss: 0.4829  @ epoch 5686 )\n",
      "[Epoch: 6260] train loss: 0.5137, train acc: 0.7882, val loss: 0.3412, val acc: 0.9137  (best train acc: 0.8180, best val acc: 0.9167, best train loss: 0.4829  @ epoch 5686 )\n",
      "[Epoch: 6280] train loss: 0.5350, train acc: 0.7866, val loss: 0.3459, val acc: 0.9076  (best train acc: 0.8180, best val acc: 0.9167, best train loss: 0.4829  @ epoch 5686 )\n",
      "[Epoch: 6300] train loss: 0.5502, train acc: 0.7739, val loss: 0.3369, val acc: 0.9106  (best train acc: 0.8180, best val acc: 0.9167, best train loss: 0.4829  @ epoch 5686 )\n",
      "[Epoch: 6320] train loss: 0.5246, train acc: 0.7866, val loss: 0.3227, val acc: 0.9130  (best train acc: 0.8180, best val acc: 0.9167, best train loss: 0.4829  @ epoch 5686 )\n",
      "[Epoch: 6340] train loss: 0.5150, train acc: 0.8021, val loss: 0.3362, val acc: 0.9140  (best train acc: 0.8180, best val acc: 0.9167, best train loss: 0.4829  @ epoch 5686 )\n",
      "[Epoch: 6360] train loss: 0.5401, train acc: 0.7641, val loss: 0.3440, val acc: 0.9126  (best train acc: 0.8180, best val acc: 0.9167, best train loss: 0.4829  @ epoch 5686 )\n",
      "[Epoch: 6380] train loss: 0.5413, train acc: 0.7817, val loss: 0.3409, val acc: 0.9130  (best train acc: 0.8180, best val acc: 0.9167, best train loss: 0.4829  @ epoch 5686 )\n",
      "[Epoch: 6400] train loss: 0.5131, train acc: 0.7918, val loss: 0.3453, val acc: 0.9062  (best train acc: 0.8180, best val acc: 0.9167, best train loss: 0.4829  @ epoch 5686 )\n",
      "[Epoch: 6420] train loss: 0.4977, train acc: 0.8079, val loss: 0.3530, val acc: 0.9032  (best train acc: 0.8180, best val acc: 0.9167, best train loss: 0.4829  @ epoch 5686 )\n",
      "[Epoch: 6440] train loss: 0.5333, train acc: 0.7814, val loss: 0.3396, val acc: 0.9133  (best train acc: 0.8180, best val acc: 0.9167, best train loss: 0.4821  @ epoch 6428 )\n",
      "[Epoch: 6460] train loss: 0.5192, train acc: 0.7887, val loss: 0.3701, val acc: 0.8988  (best train acc: 0.8180, best val acc: 0.9167, best train loss: 0.4821  @ epoch 6428 )\n",
      "[Epoch: 6480] train loss: 0.5121, train acc: 0.7910, val loss: 0.3506, val acc: 0.9059  (best train acc: 0.8180, best val acc: 0.9167, best train loss: 0.4821  @ epoch 6428 )\n",
      "[Epoch: 6500] train loss: 0.5052, train acc: 0.7955, val loss: 0.3469, val acc: 0.9099  (best train acc: 0.8180, best val acc: 0.9167, best train loss: 0.4821  @ epoch 6428 )\n",
      "[Epoch: 6520] train loss: 0.5249, train acc: 0.8016, val loss: 0.3550, val acc: 0.9143  (best train acc: 0.8180, best val acc: 0.9167, best train loss: 0.4821  @ epoch 6428 )\n",
      "[Epoch: 6540] train loss: 0.5204, train acc: 0.7846, val loss: 0.3668, val acc: 0.9005  (best train acc: 0.8180, best val acc: 0.9167, best train loss: 0.4821  @ epoch 6428 )\n",
      "[Epoch: 6560] train loss: 0.5007, train acc: 0.8088, val loss: 0.3626, val acc: 0.9103  (best train acc: 0.8180, best val acc: 0.9167, best train loss: 0.4821  @ epoch 6428 )\n",
      "[Epoch: 6580] train loss: 0.5358, train acc: 0.7858, val loss: 0.3467, val acc: 0.9086  (best train acc: 0.8180, best val acc: 0.9167, best train loss: 0.4821  @ epoch 6428 )\n",
      "[Epoch: 6600] train loss: 0.5100, train acc: 0.7955, val loss: 0.3551, val acc: 0.9110  (best train acc: 0.8180, best val acc: 0.9167, best train loss: 0.4821  @ epoch 6428 )\n",
      "[Epoch: 6620] train loss: 0.5381, train acc: 0.7702, val loss: 0.3824, val acc: 0.9099  (best train acc: 0.8180, best val acc: 0.9167, best train loss: 0.4821  @ epoch 6428 )\n",
      "[Epoch: 6640] train loss: 0.5086, train acc: 0.7920, val loss: 0.3678, val acc: 0.9083  (best train acc: 0.8180, best val acc: 0.9167, best train loss: 0.4821  @ epoch 6428 )\n",
      "[Epoch: 6660] train loss: 0.5327, train acc: 0.7835, val loss: 0.3478, val acc: 0.9137  (best train acc: 0.8180, best val acc: 0.9167, best train loss: 0.4821  @ epoch 6428 )\n",
      "[Epoch: 6680] train loss: 0.5156, train acc: 0.7906, val loss: 0.3690, val acc: 0.9116  (best train acc: 0.8180, best val acc: 0.9167, best train loss: 0.4821  @ epoch 6428 )\n",
      "[Epoch: 6700] train loss: 0.5066, train acc: 0.8002, val loss: 0.3548, val acc: 0.9167  (best train acc: 0.8180, best val acc: 0.9167, best train loss: 0.4821  @ epoch 6428 )\n",
      "[Epoch: 6720] train loss: 0.5068, train acc: 0.8005, val loss: 0.3796, val acc: 0.9015  (best train acc: 0.8180, best val acc: 0.9167, best train loss: 0.4821  @ epoch 6428 )\n",
      "[Epoch: 6740] train loss: 0.5126, train acc: 0.8080, val loss: 0.3766, val acc: 0.9140  (best train acc: 0.8277, best val acc: 0.9167, best train loss: 0.4821  @ epoch 6428 )\n",
      "[Epoch: 6760] train loss: 0.5111, train acc: 0.8009, val loss: 0.4018, val acc: 0.9096  (best train acc: 0.8284, best val acc: 0.9167, best train loss: 0.4731  @ epoch 6754 )\n",
      "[Epoch: 6780] train loss: 0.4782, train acc: 0.8209, val loss: 0.4125, val acc: 0.9120  (best train acc: 0.8284, best val acc: 0.9167, best train loss: 0.4644  @ epoch 6779 )\n",
      "[Epoch: 6800] train loss: 0.5078, train acc: 0.7987, val loss: 0.3892, val acc: 0.9110  (best train acc: 0.8344, best val acc: 0.9167, best train loss: 0.4603  @ epoch 6799 )\n",
      "[Epoch: 6820] train loss: 0.5048, train acc: 0.8000, val loss: 0.3935, val acc: 0.9069  (best train acc: 0.8344, best val acc: 0.9167, best train loss: 0.4603  @ epoch 6799 )\n",
      "[Epoch: 6840] train loss: 0.4771, train acc: 0.8178, val loss: 0.3888, val acc: 0.9140  (best train acc: 0.8344, best val acc: 0.9167, best train loss: 0.4603  @ epoch 6799 )\n",
      "[Epoch: 6860] train loss: 0.4516, train acc: 0.8310, val loss: 0.3952, val acc: 0.9147  (best train acc: 0.8344, best val acc: 0.9167, best train loss: 0.4516  @ epoch 6860 )\n",
      "[Epoch: 6880] train loss: 0.4707, train acc: 0.8218, val loss: 0.4190, val acc: 0.9160  (best train acc: 0.8344, best val acc: 0.9170, best train loss: 0.4516  @ epoch 6860 )\n",
      "[Epoch: 6900] train loss: 0.4756, train acc: 0.8176, val loss: 0.4152, val acc: 0.9137  (best train acc: 0.8344, best val acc: 0.9170, best train loss: 0.4516  @ epoch 6860 )\n",
      "[Epoch: 6920] train loss: 0.4723, train acc: 0.8217, val loss: 0.4387, val acc: 0.9116  (best train acc: 0.8344, best val acc: 0.9180, best train loss: 0.4516  @ epoch 6860 )\n",
      "[Epoch: 6940] train loss: 0.4909, train acc: 0.8259, val loss: 0.4776, val acc: 0.8988  (best train acc: 0.8344, best val acc: 0.9180, best train loss: 0.4516  @ epoch 6860 )\n",
      "[Epoch: 6960] train loss: 0.4631, train acc: 0.8260, val loss: 0.4484, val acc: 0.9106  (best train acc: 0.8344, best val acc: 0.9180, best train loss: 0.4516  @ epoch 6860 )\n",
      "[Epoch: 6980] train loss: 0.4814, train acc: 0.8124, val loss: 0.4347, val acc: 0.9120  (best train acc: 0.8344, best val acc: 0.9180, best train loss: 0.4516  @ epoch 6860 )\n",
      "[Epoch: 7000] train loss: 0.4710, train acc: 0.8222, val loss: 0.4539, val acc: 0.9073  (best train acc: 0.8344, best val acc: 0.9180, best train loss: 0.4516  @ epoch 6860 )\n",
      "[Epoch: 7020] train loss: 0.4579, train acc: 0.8250, val loss: 0.4521, val acc: 0.9160  (best train acc: 0.8344, best val acc: 0.9180, best train loss: 0.4516  @ epoch 6860 )\n",
      "[Epoch: 7040] train loss: 0.4726, train acc: 0.8173, val loss: 0.4321, val acc: 0.9150  (best train acc: 0.8344, best val acc: 0.9180, best train loss: 0.4516  @ epoch 6860 )\n",
      "[Epoch: 7060] train loss: 0.4666, train acc: 0.8269, val loss: 0.4550, val acc: 0.9160  (best train acc: 0.8344, best val acc: 0.9180, best train loss: 0.4516  @ epoch 6860 )\n",
      "[Epoch: 7080] train loss: 0.4686, train acc: 0.8210, val loss: 0.4575, val acc: 0.9116  (best train acc: 0.8344, best val acc: 0.9180, best train loss: 0.4516  @ epoch 6860 )\n",
      "[Epoch: 7100] train loss: 0.4986, train acc: 0.8115, val loss: 0.4563, val acc: 0.9140  (best train acc: 0.8344, best val acc: 0.9180, best train loss: 0.4516  @ epoch 6860 )\n",
      "[Epoch: 7120] train loss: 0.4968, train acc: 0.8106, val loss: 0.4607, val acc: 0.9062  (best train acc: 0.8344, best val acc: 0.9180, best train loss: 0.4516  @ epoch 6860 )\n",
      "[Epoch: 7140] train loss: 0.4916, train acc: 0.8111, val loss: 0.4612, val acc: 0.9116  (best train acc: 0.8344, best val acc: 0.9180, best train loss: 0.4488  @ epoch 7126 )\n",
      "[Epoch: 7160] train loss: 0.4675, train acc: 0.8172, val loss: 0.4903, val acc: 0.8958  (best train acc: 0.8344, best val acc: 0.9180, best train loss: 0.4488  @ epoch 7126 )\n",
      "[Epoch: 7180] train loss: 0.4935, train acc: 0.8081, val loss: 0.4569, val acc: 0.9062  (best train acc: 0.8344, best val acc: 0.9180, best train loss: 0.4488  @ epoch 7126 )\n",
      "[Epoch: 7200] train loss: 0.4762, train acc: 0.8175, val loss: 0.4716, val acc: 0.9079  (best train acc: 0.8344, best val acc: 0.9180, best train loss: 0.4488  @ epoch 7126 )\n",
      "[Epoch: 7220] train loss: 0.5121, train acc: 0.7963, val loss: 0.4975, val acc: 0.9147  (best train acc: 0.8344, best val acc: 0.9180, best train loss: 0.4488  @ epoch 7126 )\n",
      "[Epoch: 7240] train loss: 0.4628, train acc: 0.8202, val loss: 0.5080, val acc: 0.9099  (best train acc: 0.8347, best val acc: 0.9180, best train loss: 0.4488  @ epoch 7126 )\n",
      "[Epoch: 7260] train loss: 0.4777, train acc: 0.8146, val loss: 0.4935, val acc: 0.9160  (best train acc: 0.8347, best val acc: 0.9180, best train loss: 0.4488  @ epoch 7126 )\n",
      "[Epoch: 7280] train loss: 0.4748, train acc: 0.8140, val loss: 0.4981, val acc: 0.9015  (best train acc: 0.8347, best val acc: 0.9180, best train loss: 0.4488  @ epoch 7126 )\n",
      "[Epoch: 7300] train loss: 0.4949, train acc: 0.8114, val loss: 0.4991, val acc: 0.9049  (best train acc: 0.8347, best val acc: 0.9180, best train loss: 0.4488  @ epoch 7126 )\n",
      "[Epoch: 7320] train loss: 0.4584, train acc: 0.8276, val loss: 0.4873, val acc: 0.9147  (best train acc: 0.8347, best val acc: 0.9180, best train loss: 0.4488  @ epoch 7126 )\n",
      "[Epoch: 7340] train loss: 0.4553, train acc: 0.8226, val loss: 0.4989, val acc: 0.9167  (best train acc: 0.8347, best val acc: 0.9180, best train loss: 0.4488  @ epoch 7126 )\n",
      "[Epoch: 7360] train loss: 0.4840, train acc: 0.8146, val loss: 0.4881, val acc: 0.9160  (best train acc: 0.8347, best val acc: 0.9180, best train loss: 0.4488  @ epoch 7126 )\n",
      "[Epoch: 7380] train loss: 0.4484, train acc: 0.8258, val loss: 0.4975, val acc: 0.9157  (best train acc: 0.8347, best val acc: 0.9180, best train loss: 0.4484  @ epoch 7380 )\n",
      "[Epoch: 7400] train loss: 0.4660, train acc: 0.8184, val loss: 0.4916, val acc: 0.9113  (best train acc: 0.8347, best val acc: 0.9184, best train loss: 0.4484  @ epoch 7380 )\n",
      "[Epoch: 7420] train loss: 0.4880, train acc: 0.8182, val loss: 0.5102, val acc: 0.9066  (best train acc: 0.8347, best val acc: 0.9187, best train loss: 0.4484  @ epoch 7380 )\n",
      "[Epoch: 7440] train loss: 0.4937, train acc: 0.8148, val loss: 0.5174, val acc: 0.9137  (best train acc: 0.8347, best val acc: 0.9187, best train loss: 0.4484  @ epoch 7380 )\n",
      "[Epoch: 7460] train loss: 0.4949, train acc: 0.7979, val loss: 0.4891, val acc: 0.9110  (best train acc: 0.8347, best val acc: 0.9187, best train loss: 0.4484  @ epoch 7380 )\n",
      "[Epoch: 7480] train loss: 0.4657, train acc: 0.8214, val loss: 0.5020, val acc: 0.9167  (best train acc: 0.8347, best val acc: 0.9187, best train loss: 0.4468  @ epoch 7477 )\n",
      "[Epoch: 7500] train loss: 0.4910, train acc: 0.8065, val loss: 0.5023, val acc: 0.9177  (best train acc: 0.8347, best val acc: 0.9187, best train loss: 0.4468  @ epoch 7477 )\n",
      "[Epoch: 7520] train loss: 0.4636, train acc: 0.8214, val loss: 0.4939, val acc: 0.9076  (best train acc: 0.8347, best val acc: 0.9187, best train loss: 0.4468  @ epoch 7477 )\n",
      "[Epoch: 7540] train loss: 0.4624, train acc: 0.8209, val loss: 0.5117, val acc: 0.9147  (best train acc: 0.8347, best val acc: 0.9191, best train loss: 0.4468  @ epoch 7477 )\n",
      "[Epoch: 7560] train loss: 0.4663, train acc: 0.8242, val loss: 0.5147, val acc: 0.9073  (best train acc: 0.8347, best val acc: 0.9191, best train loss: 0.4468  @ epoch 7477 )\n",
      "[Epoch: 7580] train loss: 0.4832, train acc: 0.8069, val loss: 0.5071, val acc: 0.9174  (best train acc: 0.8347, best val acc: 0.9191, best train loss: 0.4468  @ epoch 7477 )\n",
      "[Epoch: 7600] train loss: 0.4889, train acc: 0.8089, val loss: 0.5079, val acc: 0.8995  (best train acc: 0.8347, best val acc: 0.9191, best train loss: 0.4468  @ epoch 7477 )\n",
      "[Epoch: 7620] train loss: 0.4910, train acc: 0.7963, val loss: 0.5083, val acc: 0.9180  (best train acc: 0.8347, best val acc: 0.9191, best train loss: 0.4468  @ epoch 7477 )\n",
      "[Epoch: 7640] train loss: 0.4725, train acc: 0.8227, val loss: 0.4972, val acc: 0.9069  (best train acc: 0.8347, best val acc: 0.9191, best train loss: 0.4468  @ epoch 7477 )\n",
      "[Epoch: 7660] train loss: 0.4950, train acc: 0.7974, val loss: 0.4808, val acc: 0.9130  (best train acc: 0.8347, best val acc: 0.9197, best train loss: 0.4442  @ epoch 7656 )\n",
      "[Epoch: 7680] train loss: 0.4710, train acc: 0.8252, val loss: 0.5142, val acc: 0.9073  (best train acc: 0.8347, best val acc: 0.9197, best train loss: 0.4431  @ epoch 7671 )\n",
      "[Epoch: 7700] train loss: 0.4808, train acc: 0.8049, val loss: 0.4766, val acc: 0.9160  (best train acc: 0.8347, best val acc: 0.9197, best train loss: 0.4431  @ epoch 7671 )\n",
      "[Epoch: 7720] train loss: 0.4520, train acc: 0.8263, val loss: 0.5061, val acc: 0.9153  (best train acc: 0.8347, best val acc: 0.9197, best train loss: 0.4431  @ epoch 7671 )\n",
      "[Epoch: 7740] train loss: 0.4650, train acc: 0.8202, val loss: 0.5100, val acc: 0.9153  (best train acc: 0.8353, best val acc: 0.9224, best train loss: 0.4424  @ epoch 7726 )\n",
      "[Epoch: 7760] train loss: 0.4770, train acc: 0.8180, val loss: 0.4989, val acc: 0.9177  (best train acc: 0.8353, best val acc: 0.9224, best train loss: 0.4424  @ epoch 7726 )\n",
      "[Epoch: 7780] train loss: 0.4709, train acc: 0.8162, val loss: 0.4756, val acc: 0.9167  (best train acc: 0.8353, best val acc: 0.9224, best train loss: 0.4424  @ epoch 7726 )\n",
      "[Epoch: 7800] train loss: 0.4637, train acc: 0.8272, val loss: 0.4871, val acc: 0.9123  (best train acc: 0.8353, best val acc: 0.9224, best train loss: 0.4424  @ epoch 7726 )\n",
      "[Epoch: 7820] train loss: 0.4691, train acc: 0.8229, val loss: 0.5026, val acc: 0.9062  (best train acc: 0.8353, best val acc: 0.9224, best train loss: 0.4424  @ epoch 7726 )\n",
      "[Epoch: 7840] train loss: 0.4620, train acc: 0.8226, val loss: 0.4698, val acc: 0.9184  (best train acc: 0.8353, best val acc: 0.9224, best train loss: 0.4424  @ epoch 7726 )\n",
      "[Epoch: 7860] train loss: 0.4754, train acc: 0.8080, val loss: 0.4604, val acc: 0.9025  (best train acc: 0.8353, best val acc: 0.9224, best train loss: 0.4424  @ epoch 7726 )\n",
      "[Epoch: 7880] train loss: 0.4692, train acc: 0.8150, val loss: 0.4675, val acc: 0.9035  (best train acc: 0.8353, best val acc: 0.9224, best train loss: 0.4424  @ epoch 7726 )\n",
      "[Epoch: 7900] train loss: 0.4929, train acc: 0.8021, val loss: 0.3995, val acc: 0.9123  (best train acc: 0.8353, best val acc: 0.9224, best train loss: 0.4424  @ epoch 7726 )\n",
      "[Epoch: 7920] train loss: 0.4991, train acc: 0.8078, val loss: 0.4159, val acc: 0.9099  (best train acc: 0.8353, best val acc: 0.9224, best train loss: 0.4424  @ epoch 7726 )\n",
      "[Epoch: 7940] train loss: 0.5271, train acc: 0.7831, val loss: 0.4083, val acc: 0.9056  (best train acc: 0.8353, best val acc: 0.9224, best train loss: 0.4424  @ epoch 7726 )\n",
      "[Epoch: 7960] train loss: 0.4630, train acc: 0.8224, val loss: 0.4127, val acc: 0.9076  (best train acc: 0.8353, best val acc: 0.9224, best train loss: 0.4424  @ epoch 7726 )\n",
      "[Epoch: 7980] train loss: 0.5029, train acc: 0.7908, val loss: 0.4053, val acc: 0.9177  (best train acc: 0.8353, best val acc: 0.9224, best train loss: 0.4424  @ epoch 7726 )\n",
      "[Epoch: 8000] train loss: 0.4546, train acc: 0.8232, val loss: 0.4124, val acc: 0.9167  (best train acc: 0.8353, best val acc: 0.9224, best train loss: 0.4424  @ epoch 7726 )\n",
      "[Epoch: 8020] train loss: 0.4553, train acc: 0.8247, val loss: 0.3898, val acc: 0.9211  (best train acc: 0.8353, best val acc: 0.9224, best train loss: 0.4424  @ epoch 7726 )\n",
      "[Epoch: 8040] train loss: 0.4497, train acc: 0.8263, val loss: 0.4127, val acc: 0.9194  (best train acc: 0.8353, best val acc: 0.9224, best train loss: 0.4395  @ epoch 8032 )\n",
      "[Epoch: 8060] train loss: 0.4461, train acc: 0.8323, val loss: 0.3866, val acc: 0.9191  (best train acc: 0.8353, best val acc: 0.9224, best train loss: 0.4395  @ epoch 8032 )\n",
      "[Epoch: 8080] train loss: 0.5021, train acc: 0.8005, val loss: 0.4186, val acc: 0.9113  (best train acc: 0.8381, best val acc: 0.9224, best train loss: 0.4395  @ epoch 8032 )\n",
      "[Epoch: 8100] train loss: 0.4682, train acc: 0.8198, val loss: 0.4518, val acc: 0.9008  (best train acc: 0.8381, best val acc: 0.9224, best train loss: 0.4395  @ epoch 8032 )\n",
      "[Epoch: 8120] train loss: 0.4695, train acc: 0.8121, val loss: 0.3840, val acc: 0.9066  (best train acc: 0.8381, best val acc: 0.9224, best train loss: 0.4395  @ epoch 8032 )\n",
      "[Epoch: 8140] train loss: 0.4933, train acc: 0.8065, val loss: 0.4321, val acc: 0.9140  (best train acc: 0.8381, best val acc: 0.9224, best train loss: 0.4395  @ epoch 8032 )\n",
      "[Epoch: 8160] train loss: 0.4702, train acc: 0.8204, val loss: 0.3839, val acc: 0.9133  (best train acc: 0.8381, best val acc: 0.9224, best train loss: 0.4395  @ epoch 8032 )\n",
      "[Epoch: 8180] train loss: 0.4725, train acc: 0.8143, val loss: 0.4342, val acc: 0.9052  (best train acc: 0.8381, best val acc: 0.9224, best train loss: 0.4395  @ epoch 8032 )\n",
      "[Epoch: 8200] train loss: 0.4499, train acc: 0.8271, val loss: 0.4179, val acc: 0.9180  (best train acc: 0.8381, best val acc: 0.9224, best train loss: 0.4395  @ epoch 8032 )\n",
      "[Epoch: 8220] train loss: 0.4547, train acc: 0.8267, val loss: 0.4110, val acc: 0.9184  (best train acc: 0.8381, best val acc: 0.9224, best train loss: 0.4395  @ epoch 8032 )\n",
      "[Epoch: 8240] train loss: 0.4617, train acc: 0.8210, val loss: 0.4400, val acc: 0.8789  (best train acc: 0.8381, best val acc: 0.9224, best train loss: 0.4395  @ epoch 8032 )\n",
      "[Epoch: 8260] train loss: 0.4580, train acc: 0.8231, val loss: 0.4319, val acc: 0.9194  (best train acc: 0.8381, best val acc: 0.9224, best train loss: 0.4395  @ epoch 8032 )\n",
      "[Epoch: 8280] train loss: 0.4586, train acc: 0.8289, val loss: 0.4174, val acc: 0.9194  (best train acc: 0.8381, best val acc: 0.9224, best train loss: 0.4391  @ epoch 8272 )\n",
      "[Epoch: 8300] train loss: 0.4752, train acc: 0.8129, val loss: 0.4029, val acc: 0.9160  (best train acc: 0.8381, best val acc: 0.9234, best train loss: 0.4391  @ epoch 8272 )\n",
      "[Epoch: 8320] train loss: 0.4567, train acc: 0.8303, val loss: 0.4504, val acc: 0.9170  (best train acc: 0.8381, best val acc: 0.9234, best train loss: 0.4391  @ epoch 8272 )\n",
      "[Epoch: 8340] train loss: 0.4673, train acc: 0.8190, val loss: 0.4319, val acc: 0.9133  (best train acc: 0.8381, best val acc: 0.9238, best train loss: 0.4391  @ epoch 8272 )\n",
      "[Epoch: 8360] train loss: 0.4575, train acc: 0.8270, val loss: 0.4040, val acc: 0.9191  (best train acc: 0.8381, best val acc: 0.9238, best train loss: 0.4391  @ epoch 8272 )\n",
      "[Epoch: 8380] train loss: 0.4924, train acc: 0.8003, val loss: 0.4069, val acc: 0.9187  (best train acc: 0.8409, best val acc: 0.9238, best train loss: 0.4344  @ epoch 8369 )\n",
      "[Epoch: 8400] train loss: 0.4624, train acc: 0.8263, val loss: 0.4416, val acc: 0.9130  (best train acc: 0.8409, best val acc: 0.9238, best train loss: 0.4344  @ epoch 8369 )\n",
      "[Epoch: 8420] train loss: 0.4515, train acc: 0.8326, val loss: 0.4174, val acc: 0.9153  (best train acc: 0.8409, best val acc: 0.9238, best train loss: 0.4344  @ epoch 8369 )\n",
      "[Epoch: 8440] train loss: 0.4530, train acc: 0.8277, val loss: 0.4866, val acc: 0.8712  (best train acc: 0.8445, best val acc: 0.9238, best train loss: 0.4321  @ epoch 8431 )\n",
      "[Epoch: 8460] train loss: 0.4980, train acc: 0.8065, val loss: 0.2920, val acc: 0.9056  (best train acc: 0.8445, best val acc: 0.9238, best train loss: 0.4321  @ epoch 8431 )\n",
      "[Epoch: 8480] train loss: 0.4506, train acc: 0.8331, val loss: 0.2879, val acc: 0.9140  (best train acc: 0.8445, best val acc: 0.9238, best train loss: 0.4321  @ epoch 8431 )\n",
      "[Epoch: 8500] train loss: 0.4501, train acc: 0.8255, val loss: 0.2996, val acc: 0.9150  (best train acc: 0.8448, best val acc: 0.9238, best train loss: 0.4321  @ epoch 8485 )\n",
      "[Epoch: 8520] train loss: 0.4482, train acc: 0.8340, val loss: 0.3382, val acc: 0.9143  (best train acc: 0.8448, best val acc: 0.9238, best train loss: 0.4303  @ epoch 8501 )\n",
      "[Epoch: 8540] train loss: 0.4262, train acc: 0.8398, val loss: 0.3705, val acc: 0.9103  (best train acc: 0.8448, best val acc: 0.9238, best train loss: 0.4262  @ epoch 8540 )\n",
      "[Epoch: 8560] train loss: 0.4492, train acc: 0.8279, val loss: 0.3565, val acc: 0.9170  (best train acc: 0.8477, best val acc: 0.9238, best train loss: 0.4262  @ epoch 8540 )\n",
      "[Epoch: 8580] train loss: 0.4523, train acc: 0.8271, val loss: 0.3501, val acc: 0.9052  (best train acc: 0.8477, best val acc: 0.9238, best train loss: 0.4192  @ epoch 8562 )\n",
      "[Epoch: 8600] train loss: 0.4207, train acc: 0.8461, val loss: 0.4179, val acc: 0.9157  (best train acc: 0.8480, best val acc: 0.9238, best train loss: 0.4192  @ epoch 8562 )\n",
      "[Epoch: 8620] train loss: 0.4232, train acc: 0.8441, val loss: 0.4178, val acc: 0.9123  (best train acc: 0.8496, best val acc: 0.9238, best train loss: 0.4184  @ epoch 8615 )\n",
      "[Epoch: 8640] train loss: 0.4376, train acc: 0.8349, val loss: 0.4062, val acc: 0.9147  (best train acc: 0.8496, best val acc: 0.9238, best train loss: 0.4184  @ epoch 8615 )\n",
      "[Epoch: 8660] train loss: 0.4163, train acc: 0.8454, val loss: 0.3772, val acc: 0.9180  (best train acc: 0.8506, best val acc: 0.9238, best train loss: 0.4119  @ epoch 8652 )\n",
      "[Epoch: 8680] train loss: 0.4274, train acc: 0.8455, val loss: 0.4415, val acc: 0.8992  (best train acc: 0.8519, best val acc: 0.9238, best train loss: 0.4119  @ epoch 8652 )\n",
      "[Epoch: 8700] train loss: 0.4562, train acc: 0.8305, val loss: 0.4121, val acc: 0.8954  (best train acc: 0.8519, best val acc: 0.9238, best train loss: 0.4119  @ epoch 8652 )\n",
      "[Epoch: 8720] train loss: 0.4317, train acc: 0.8348, val loss: 0.3948, val acc: 0.9137  (best train acc: 0.8519, best val acc: 0.9238, best train loss: 0.4119  @ epoch 8652 )\n",
      "[Epoch: 8740] train loss: 0.4472, train acc: 0.8329, val loss: 0.3889, val acc: 0.9170  (best train acc: 0.8519, best val acc: 0.9238, best train loss: 0.4119  @ epoch 8652 )\n",
      "[Epoch: 8760] train loss: 0.4315, train acc: 0.8441, val loss: 0.4287, val acc: 0.9076  (best train acc: 0.8519, best val acc: 0.9238, best train loss: 0.4119  @ epoch 8652 )\n",
      "[Epoch: 8780] train loss: 0.4522, train acc: 0.8408, val loss: 0.4453, val acc: 0.9025  (best train acc: 0.8519, best val acc: 0.9238, best train loss: 0.4119  @ epoch 8652 )\n",
      "[Epoch: 8800] train loss: 0.4595, train acc: 0.8319, val loss: 0.4177, val acc: 0.9012  (best train acc: 0.8519, best val acc: 0.9238, best train loss: 0.4119  @ epoch 8652 )\n",
      "[Epoch: 8820] train loss: 0.4377, train acc: 0.8367, val loss: 0.4145, val acc: 0.9191  (best train acc: 0.8519, best val acc: 0.9238, best train loss: 0.4119  @ epoch 8652 )\n",
      "[Epoch: 8840] train loss: 0.4298, train acc: 0.8413, val loss: 0.3982, val acc: 0.9147  (best train acc: 0.8521, best val acc: 0.9238, best train loss: 0.4048  @ epoch 8836 )\n",
      "[Epoch: 8860] train loss: 0.4398, train acc: 0.8422, val loss: 0.4315, val acc: 0.9049  (best train acc: 0.8521, best val acc: 0.9238, best train loss: 0.4048  @ epoch 8836 )\n",
      "[Epoch: 8880] train loss: 0.4253, train acc: 0.8427, val loss: 0.4195, val acc: 0.9187  (best train acc: 0.8521, best val acc: 0.9238, best train loss: 0.4048  @ epoch 8836 )\n",
      "[Epoch: 8900] train loss: 0.4260, train acc: 0.8469, val loss: 0.4085, val acc: 0.9221  (best train acc: 0.8521, best val acc: 0.9238, best train loss: 0.4048  @ epoch 8836 )\n",
      "[Epoch: 8920] train loss: 0.4316, train acc: 0.8431, val loss: 0.4278, val acc: 0.9191  (best train acc: 0.8521, best val acc: 0.9241, best train loss: 0.4048  @ epoch 8836 )\n",
      "[Epoch: 8940] train loss: 0.4438, train acc: 0.8389, val loss: 0.4209, val acc: 0.9194  (best train acc: 0.8521, best val acc: 0.9241, best train loss: 0.4048  @ epoch 8836 )\n",
      "[Epoch: 8960] train loss: 0.4253, train acc: 0.8439, val loss: 0.4174, val acc: 0.9120  (best train acc: 0.8521, best val acc: 0.9241, best train loss: 0.4048  @ epoch 8836 )\n",
      "[Epoch: 8980] train loss: 0.4231, train acc: 0.8462, val loss: 0.4508, val acc: 0.9052  (best train acc: 0.8521, best val acc: 0.9241, best train loss: 0.4048  @ epoch 8836 )\n",
      "[Epoch: 9000] train loss: 0.4235, train acc: 0.8463, val loss: 0.4058, val acc: 0.9177  (best train acc: 0.8521, best val acc: 0.9241, best train loss: 0.4048  @ epoch 8836 )\n",
      "[Epoch: 9020] train loss: 0.4500, train acc: 0.8290, val loss: 0.4357, val acc: 0.9204  (best train acc: 0.8521, best val acc: 0.9241, best train loss: 0.4048  @ epoch 8836 )\n",
      "[Epoch: 9040] train loss: 0.4335, train acc: 0.8366, val loss: 0.4153, val acc: 0.9150  (best train acc: 0.8521, best val acc: 0.9241, best train loss: 0.4048  @ epoch 8836 )\n",
      "[Epoch: 9060] train loss: 0.4275, train acc: 0.8384, val loss: 0.4027, val acc: 0.9218  (best train acc: 0.8521, best val acc: 0.9241, best train loss: 0.4048  @ epoch 8836 )\n",
      "[Epoch: 9080] train loss: 0.4239, train acc: 0.8376, val loss: 0.4143, val acc: 0.9224  (best train acc: 0.8521, best val acc: 0.9241, best train loss: 0.4048  @ epoch 8836 )\n",
      "[Epoch: 9100] train loss: 0.4406, train acc: 0.8405, val loss: 0.4159, val acc: 0.9204  (best train acc: 0.8521, best val acc: 0.9241, best train loss: 0.4048  @ epoch 8836 )\n",
      "[Epoch: 9120] train loss: 0.4237, train acc: 0.8443, val loss: 0.4334, val acc: 0.9228  (best train acc: 0.8521, best val acc: 0.9241, best train loss: 0.4048  @ epoch 8836 )\n",
      "[Epoch: 9140] train loss: 0.4290, train acc: 0.8376, val loss: 0.4163, val acc: 0.9079  (best train acc: 0.8535, best val acc: 0.9241, best train loss: 0.4048  @ epoch 8836 )\n",
      "[Epoch: 9160] train loss: 0.4274, train acc: 0.8428, val loss: 0.4508, val acc: 0.8954  (best train acc: 0.8535, best val acc: 0.9241, best train loss: 0.4048  @ epoch 8836 )\n",
      "[Epoch: 9180] train loss: 0.4481, train acc: 0.8284, val loss: 0.4474, val acc: 0.9025  (best train acc: 0.8535, best val acc: 0.9241, best train loss: 0.4048  @ epoch 8836 )\n",
      "[Epoch: 9200] train loss: 0.4364, train acc: 0.8371, val loss: 0.4433, val acc: 0.9143  (best train acc: 0.8535, best val acc: 0.9241, best train loss: 0.4048  @ epoch 8836 )\n",
      "[Epoch: 9220] train loss: 0.4587, train acc: 0.8257, val loss: 0.4499, val acc: 0.9113  (best train acc: 0.8535, best val acc: 0.9241, best train loss: 0.4048  @ epoch 8836 )\n",
      "[Epoch: 9240] train loss: 0.4177, train acc: 0.8469, val loss: 0.4576, val acc: 0.9133  (best train acc: 0.8571, best val acc: 0.9241, best train loss: 0.4048  @ epoch 8836 )\n",
      "[Epoch: 9260] train loss: 0.4117, train acc: 0.8531, val loss: 0.4349, val acc: 0.9157  (best train acc: 0.8571, best val acc: 0.9241, best train loss: 0.4048  @ epoch 8836 )\n",
      "[Epoch: 9280] train loss: 0.4356, train acc: 0.8336, val loss: 0.4184, val acc: 0.9214  (best train acc: 0.8571, best val acc: 0.9241, best train loss: 0.4048  @ epoch 8836 )\n",
      "[Epoch: 9300] train loss: 0.4503, train acc: 0.8259, val loss: 0.4328, val acc: 0.9167  (best train acc: 0.8571, best val acc: 0.9241, best train loss: 0.4048  @ epoch 8836 )\n",
      "[Epoch: 9320] train loss: 0.4418, train acc: 0.8390, val loss: 0.4514, val acc: 0.9089  (best train acc: 0.8571, best val acc: 0.9241, best train loss: 0.4048  @ epoch 8836 )\n",
      "[Epoch: 9340] train loss: 0.4241, train acc: 0.8454, val loss: 0.4321, val acc: 0.9133  (best train acc: 0.8571, best val acc: 0.9241, best train loss: 0.4048  @ epoch 8836 )\n",
      "[Epoch: 9360] train loss: 0.4237, train acc: 0.8454, val loss: 0.4443, val acc: 0.9150  (best train acc: 0.8571, best val acc: 0.9241, best train loss: 0.4048  @ epoch 8836 )\n",
      "[Epoch: 9380] train loss: 0.4629, train acc: 0.8278, val loss: 0.4414, val acc: 0.9191  (best train acc: 0.8571, best val acc: 0.9241, best train loss: 0.4048  @ epoch 8836 )\n",
      "[Epoch: 9400] train loss: 0.4464, train acc: 0.8279, val loss: 0.4174, val acc: 0.9099  (best train acc: 0.8571, best val acc: 0.9241, best train loss: 0.4048  @ epoch 8836 )\n",
      "[Epoch: 9420] train loss: 0.4145, train acc: 0.8520, val loss: 0.4208, val acc: 0.9194  (best train acc: 0.8571, best val acc: 0.9241, best train loss: 0.4048  @ epoch 8836 )\n",
      "[Epoch: 9440] train loss: 0.4333, train acc: 0.8309, val loss: 0.4416, val acc: 0.9228  (best train acc: 0.8571, best val acc: 0.9241, best train loss: 0.4048  @ epoch 8836 )\n",
      "[Epoch: 9460] train loss: 0.4282, train acc: 0.8376, val loss: 0.4125, val acc: 0.9214  (best train acc: 0.8571, best val acc: 0.9241, best train loss: 0.4048  @ epoch 8836 )\n",
      "[Epoch: 9480] train loss: 0.4215, train acc: 0.8438, val loss: 0.4608, val acc: 0.9207  (best train acc: 0.8571, best val acc: 0.9241, best train loss: 0.4048  @ epoch 8836 )\n",
      "[Epoch: 9500] train loss: 0.4273, train acc: 0.8390, val loss: 0.4572, val acc: 0.9204  (best train acc: 0.8571, best val acc: 0.9241, best train loss: 0.3975  @ epoch 9486 )\n",
      "[Epoch: 9520] train loss: 0.4367, train acc: 0.8339, val loss: 0.4528, val acc: 0.9214  (best train acc: 0.8571, best val acc: 0.9241, best train loss: 0.3975  @ epoch 9486 )\n",
      "[Epoch: 9540] train loss: 0.4320, train acc: 0.8342, val loss: 0.4254, val acc: 0.9120  (best train acc: 0.8571, best val acc: 0.9241, best train loss: 0.3975  @ epoch 9486 )\n",
      "[Epoch: 9560] train loss: 0.4387, train acc: 0.8448, val loss: 0.4423, val acc: 0.9147  (best train acc: 0.8571, best val acc: 0.9241, best train loss: 0.3975  @ epoch 9486 )\n",
      "[Epoch: 9580] train loss: 0.4254, train acc: 0.8372, val loss: 0.4556, val acc: 0.9069  (best train acc: 0.8571, best val acc: 0.9241, best train loss: 0.3975  @ epoch 9486 )\n",
      "[Epoch: 9600] train loss: 0.4430, train acc: 0.8293, val loss: 0.3958, val acc: 0.9201  (best train acc: 0.8571, best val acc: 0.9241, best train loss: 0.3975  @ epoch 9486 )\n",
      "[Epoch: 9620] train loss: 0.4434, train acc: 0.8284, val loss: 0.4388, val acc: 0.9160  (best train acc: 0.8571, best val acc: 0.9241, best train loss: 0.3975  @ epoch 9486 )\n",
      "[Epoch: 9640] train loss: 0.4575, train acc: 0.8175, val loss: 0.4485, val acc: 0.9201  (best train acc: 0.8571, best val acc: 0.9241, best train loss: 0.3975  @ epoch 9486 )\n",
      "[Epoch: 9660] train loss: 0.4269, train acc: 0.8421, val loss: 0.4676, val acc: 0.9157  (best train acc: 0.8571, best val acc: 0.9241, best train loss: 0.3975  @ epoch 9486 )\n",
      "[Epoch: 9680] train loss: 0.4165, train acc: 0.8388, val loss: 0.4324, val acc: 0.9076  (best train acc: 0.8571, best val acc: 0.9241, best train loss: 0.3975  @ epoch 9486 )\n",
      "[Epoch: 9700] train loss: 0.4334, train acc: 0.8396, val loss: 0.4331, val acc: 0.9184  (best train acc: 0.8571, best val acc: 0.9241, best train loss: 0.3975  @ epoch 9486 )\n",
      "[Epoch: 9720] train loss: 0.4076, train acc: 0.8494, val loss: 0.4580, val acc: 0.9207  (best train acc: 0.8571, best val acc: 0.9241, best train loss: 0.3975  @ epoch 9486 )\n",
      "[Epoch: 9740] train loss: 0.4287, train acc: 0.8317, val loss: 0.4526, val acc: 0.9214  (best train acc: 0.8571, best val acc: 0.9251, best train loss: 0.3975  @ epoch 9486 )\n",
      "[Epoch: 9760] train loss: 0.4292, train acc: 0.8352, val loss: 0.4682, val acc: 0.9197  (best train acc: 0.8571, best val acc: 0.9251, best train loss: 0.3975  @ epoch 9486 )\n",
      "[Epoch: 9780] train loss: 0.4354, train acc: 0.8389, val loss: 0.4516, val acc: 0.9194  (best train acc: 0.8571, best val acc: 0.9251, best train loss: 0.3975  @ epoch 9486 )\n",
      "[Epoch: 9800] train loss: 0.4260, train acc: 0.8461, val loss: 0.4578, val acc: 0.9221  (best train acc: 0.8571, best val acc: 0.9251, best train loss: 0.3975  @ epoch 9486 )\n",
      "[Epoch: 9820] train loss: 0.4240, train acc: 0.8448, val loss: 0.4627, val acc: 0.9120  (best train acc: 0.8571, best val acc: 0.9251, best train loss: 0.3975  @ epoch 9486 )\n",
      "[Epoch: 9840] train loss: 0.4288, train acc: 0.8403, val loss: 0.4828, val acc: 0.9157  (best train acc: 0.8571, best val acc: 0.9251, best train loss: 0.3975  @ epoch 9486 )\n",
      "[Epoch: 9860] train loss: 0.4578, train acc: 0.8169, val loss: 0.4502, val acc: 0.9184  (best train acc: 0.8571, best val acc: 0.9251, best train loss: 0.3975  @ epoch 9486 )\n",
      "[Epoch: 9880] train loss: 0.4392, train acc: 0.8401, val loss: 0.3802, val acc: 0.9214  (best train acc: 0.8571, best val acc: 0.9251, best train loss: 0.3975  @ epoch 9486 )\n",
      "[Epoch: 9900] train loss: 0.4246, train acc: 0.8376, val loss: 0.4597, val acc: 0.9191  (best train acc: 0.8571, best val acc: 0.9251, best train loss: 0.3975  @ epoch 9486 )\n",
      "[Epoch: 9920] train loss: 0.4483, train acc: 0.8383, val loss: 0.4780, val acc: 0.9025  (best train acc: 0.8571, best val acc: 0.9251, best train loss: 0.3975  @ epoch 9486 )\n",
      "[Epoch: 9940] train loss: 0.4337, train acc: 0.8334, val loss: 0.4245, val acc: 0.9207  (best train acc: 0.8571, best val acc: 0.9251, best train loss: 0.3975  @ epoch 9486 )\n",
      "[Epoch: 9960] train loss: 0.4259, train acc: 0.8395, val loss: 0.4384, val acc: 0.9194  (best train acc: 0.8571, best val acc: 0.9251, best train loss: 0.3975  @ epoch 9486 )\n",
      "[Epoch: 9980] train loss: 0.4436, train acc: 0.8379, val loss: 0.4131, val acc: 0.9079  (best train acc: 0.8571, best val acc: 0.9251, best train loss: 0.3975  @ epoch 9486 )\n",
      "[Epoch: 10000] train loss: 0.4374, train acc: 0.8410, val loss: 0.4178, val acc: 0.9143  (best train acc: 0.8571, best val acc: 0.9251, best train loss: 0.3975  @ epoch 9486 )\n",
      "[Epoch: 10020] train loss: 0.4213, train acc: 0.8464, val loss: 0.5030, val acc: 0.9083  (best train acc: 0.8571, best val acc: 0.9251, best train loss: 0.3975  @ epoch 9486 )\n",
      "[Epoch: 10040] train loss: 0.4387, train acc: 0.8321, val loss: 0.4205, val acc: 0.9187  (best train acc: 0.8571, best val acc: 0.9251, best train loss: 0.3975  @ epoch 9486 )\n",
      "[Epoch: 10060] train loss: 0.4176, train acc: 0.8479, val loss: 0.4268, val acc: 0.9157  (best train acc: 0.8571, best val acc: 0.9251, best train loss: 0.3975  @ epoch 9486 )\n",
      "[Epoch: 10080] train loss: 0.4320, train acc: 0.8373, val loss: 0.4570, val acc: 0.9147  (best train acc: 0.8571, best val acc: 0.9251, best train loss: 0.3975  @ epoch 9486 )\n",
      "[Epoch: 10100] train loss: 0.4447, train acc: 0.8253, val loss: 0.4574, val acc: 0.9214  (best train acc: 0.8571, best val acc: 0.9251, best train loss: 0.3975  @ epoch 9486 )\n",
      "[Epoch: 10120] train loss: 0.4226, train acc: 0.8418, val loss: 0.4512, val acc: 0.9191  (best train acc: 0.8571, best val acc: 0.9251, best train loss: 0.3975  @ epoch 9486 )\n",
      "[Epoch: 10140] train loss: 0.4127, train acc: 0.8441, val loss: 0.4184, val acc: 0.9150  (best train acc: 0.8571, best val acc: 0.9251, best train loss: 0.3975  @ epoch 9486 )\n",
      "[Epoch: 10160] train loss: 0.4455, train acc: 0.8338, val loss: 0.4261, val acc: 0.9197  (best train acc: 0.8571, best val acc: 0.9251, best train loss: 0.3975  @ epoch 9486 )\n",
      "[Epoch: 10180] train loss: 0.4119, train acc: 0.8478, val loss: 0.3743, val acc: 0.9211  (best train acc: 0.8571, best val acc: 0.9251, best train loss: 0.3975  @ epoch 9486 )\n",
      "[Epoch: 10200] train loss: 0.4397, train acc: 0.8418, val loss: 0.4637, val acc: 0.9143  (best train acc: 0.8571, best val acc: 0.9251, best train loss: 0.3975  @ epoch 9486 )\n",
      "[Epoch: 10220] train loss: 0.4280, train acc: 0.8378, val loss: 0.4194, val acc: 0.9214  (best train acc: 0.8571, best val acc: 0.9251, best train loss: 0.3975  @ epoch 9486 )\n",
      "[Epoch: 10240] train loss: 0.4227, train acc: 0.8410, val loss: 0.4391, val acc: 0.9174  (best train acc: 0.8571, best val acc: 0.9251, best train loss: 0.3975  @ epoch 9486 )\n",
      "[Epoch: 10260] train loss: 0.4312, train acc: 0.8381, val loss: 0.4034, val acc: 0.9204  (best train acc: 0.8571, best val acc: 0.9251, best train loss: 0.3975  @ epoch 9486 )\n",
      "[Epoch: 10280] train loss: 0.4291, train acc: 0.8387, val loss: 0.3749, val acc: 0.9130  (best train acc: 0.8571, best val acc: 0.9251, best train loss: 0.3975  @ epoch 9486 )\n",
      "[Epoch: 10300] train loss: 0.4229, train acc: 0.8420, val loss: 0.4525, val acc: 0.9180  (best train acc: 0.8583, best val acc: 0.9251, best train loss: 0.3975  @ epoch 9486 )\n",
      "[Epoch: 10320] train loss: 0.4441, train acc: 0.8268, val loss: 0.4053, val acc: 0.9231  (best train acc: 0.8583, best val acc: 0.9251, best train loss: 0.3975  @ epoch 9486 )\n",
      "[Epoch: 10340] train loss: 0.4505, train acc: 0.8258, val loss: 0.4123, val acc: 0.9191  (best train acc: 0.8583, best val acc: 0.9251, best train loss: 0.3975  @ epoch 9486 )\n",
      "[Epoch: 10360] train loss: 0.4186, train acc: 0.8373, val loss: 0.4613, val acc: 0.9022  (best train acc: 0.8583, best val acc: 0.9251, best train loss: 0.3975  @ epoch 9486 )\n",
      "[Epoch: 10380] train loss: 0.4330, train acc: 0.8379, val loss: 0.3413, val acc: 0.9174  (best train acc: 0.8583, best val acc: 0.9251, best train loss: 0.3975  @ epoch 9486 )\n",
      "[Epoch: 10400] train loss: 0.4199, train acc: 0.8357, val loss: 0.3833, val acc: 0.9164  (best train acc: 0.8583, best val acc: 0.9251, best train loss: 0.3975  @ epoch 9486 )\n",
      "[Epoch: 10420] train loss: 0.4029, train acc: 0.8483, val loss: 0.4828, val acc: 0.9197  (best train acc: 0.8583, best val acc: 0.9251, best train loss: 0.3975  @ epoch 9486 )\n",
      "[Epoch: 10440] train loss: 0.4152, train acc: 0.8490, val loss: 0.3888, val acc: 0.9174  (best train acc: 0.8583, best val acc: 0.9251, best train loss: 0.3975  @ epoch 9486 )\n",
      "[Epoch: 10460] train loss: 0.4343, train acc: 0.8349, val loss: 0.4186, val acc: 0.9251  (best train acc: 0.8583, best val acc: 0.9251, best train loss: 0.3975  @ epoch 9486 )\n",
      "[Epoch: 10480] train loss: 0.4212, train acc: 0.8461, val loss: 0.3407, val acc: 0.9234  (best train acc: 0.8583, best val acc: 0.9251, best train loss: 0.3975  @ epoch 9486 )\n",
      "[Epoch: 10500] train loss: 0.4367, train acc: 0.8338, val loss: 0.4050, val acc: 0.9231  (best train acc: 0.8583, best val acc: 0.9251, best train loss: 0.3975  @ epoch 9486 )\n",
      "[Epoch: 10520] train loss: 0.4197, train acc: 0.8412, val loss: 0.3961, val acc: 0.9120  (best train acc: 0.8583, best val acc: 0.9251, best train loss: 0.3975  @ epoch 9486 )\n",
      "[Epoch: 10540] train loss: 0.4200, train acc: 0.8379, val loss: 0.4115, val acc: 0.9140  (best train acc: 0.8583, best val acc: 0.9251, best train loss: 0.3975  @ epoch 9486 )\n",
      "[Epoch: 10560] train loss: 0.4140, train acc: 0.8434, val loss: 0.4068, val acc: 0.9221  (best train acc: 0.8583, best val acc: 0.9251, best train loss: 0.3975  @ epoch 9486 )\n",
      "[Epoch: 10580] train loss: 0.4374, train acc: 0.8314, val loss: 0.3162, val acc: 0.9231  (best train acc: 0.8583, best val acc: 0.9251, best train loss: 0.3975  @ epoch 9486 )\n",
      "[Epoch: 10600] train loss: 0.4092, train acc: 0.8490, val loss: 0.3643, val acc: 0.9201  (best train acc: 0.8583, best val acc: 0.9251, best train loss: 0.3975  @ epoch 9486 )\n",
      "[Epoch: 10620] train loss: 0.4254, train acc: 0.8362, val loss: 0.3836, val acc: 0.9218  (best train acc: 0.8583, best val acc: 0.9251, best train loss: 0.3975  @ epoch 9486 )\n",
      "[Epoch: 10640] train loss: 0.4048, train acc: 0.8461, val loss: 0.3851, val acc: 0.9197  (best train acc: 0.8583, best val acc: 0.9251, best train loss: 0.3975  @ epoch 9486 )\n",
      "[Epoch: 10660] train loss: 0.4082, train acc: 0.8391, val loss: 0.3907, val acc: 0.9167  (best train acc: 0.8590, best val acc: 0.9251, best train loss: 0.3887  @ epoch 10651 )\n",
      "[Epoch: 10680] train loss: 0.4006, train acc: 0.8508, val loss: 0.4721, val acc: 0.8998  (best train acc: 0.8623, best val acc: 0.9251, best train loss: 0.3766  @ epoch 10677 )\n",
      "[Epoch: 10700] train loss: 0.3709, train acc: 0.8605, val loss: 0.4105, val acc: 0.9214  (best train acc: 0.8670, best val acc: 0.9251, best train loss: 0.3709  @ epoch 10700 )\n",
      "[Epoch: 10720] train loss: 0.3507, train acc: 0.8694, val loss: 0.4130, val acc: 0.9207  (best train acc: 0.8722, best val acc: 0.9251, best train loss: 0.3469  @ epoch 10719 )\n",
      "[Epoch: 10740] train loss: 0.3486, train acc: 0.8635, val loss: 0.4155, val acc: 0.9197  (best train acc: 0.8722, best val acc: 0.9251, best train loss: 0.3447  @ epoch 10739 )\n",
      "[Epoch: 10760] train loss: 0.3759, train acc: 0.8553, val loss: 0.4497, val acc: 0.9214  (best train acc: 0.8754, best val acc: 0.9251, best train loss: 0.3443  @ epoch 10754 )\n",
      "[Epoch: 10780] train loss: 0.3777, train acc: 0.8527, val loss: 0.4282, val acc: 0.9073  (best train acc: 0.8782, best val acc: 0.9251, best train loss: 0.3443  @ epoch 10754 )\n",
      "[Epoch: 10800] train loss: 0.3477, train acc: 0.8671, val loss: 0.3661, val acc: 0.9221  (best train acc: 0.8782, best val acc: 0.9251, best train loss: 0.3363  @ epoch 10791 )\n",
      "[Epoch: 10820] train loss: 0.3535, train acc: 0.8660, val loss: 0.3524, val acc: 0.9245  (best train acc: 0.8828, best val acc: 0.9261, best train loss: 0.3283  @ epoch 10811 )\n",
      "[Epoch: 10840] train loss: 0.3344, train acc: 0.8765, val loss: 0.3424, val acc: 0.9167  (best train acc: 0.8828, best val acc: 0.9261, best train loss: 0.3283  @ epoch 10811 )\n",
      "[Epoch: 10860] train loss: 0.3605, train acc: 0.8568, val loss: 0.3828, val acc: 0.9113  (best train acc: 0.8828, best val acc: 0.9261, best train loss: 0.3273  @ epoch 10846 )\n",
      "[Epoch: 10880] train loss: 0.3433, train acc: 0.8798, val loss: 0.3415, val acc: 0.9211  (best train acc: 0.8847, best val acc: 0.9275, best train loss: 0.3206  @ epoch 10873 )\n",
      "[Epoch: 10900] train loss: 0.3558, train acc: 0.8759, val loss: 0.3771, val acc: 0.9056  (best train acc: 0.8847, best val acc: 0.9275, best train loss: 0.3206  @ epoch 10873 )\n",
      "[Epoch: 10920] train loss: 0.3345, train acc: 0.8708, val loss: 0.4202, val acc: 0.9106  (best train acc: 0.8847, best val acc: 0.9275, best train loss: 0.3206  @ epoch 10873 )\n",
      "[Epoch: 10940] train loss: 0.3287, train acc: 0.8746, val loss: 0.3968, val acc: 0.9241  (best train acc: 0.8847, best val acc: 0.9275, best train loss: 0.3150  @ epoch 10927 )\n",
      "[Epoch: 10960] train loss: 0.3471, train acc: 0.8717, val loss: 0.4611, val acc: 0.9153  (best train acc: 0.8847, best val acc: 0.9275, best train loss: 0.3150  @ epoch 10927 )\n",
      "[Epoch: 10980] train loss: 0.3314, train acc: 0.8725, val loss: 0.4534, val acc: 0.9211  (best train acc: 0.8847, best val acc: 0.9275, best train loss: 0.3150  @ epoch 10927 )\n",
      "[Epoch: 11000] train loss: 0.3479, train acc: 0.8676, val loss: 0.4371, val acc: 0.9255  (best train acc: 0.8847, best val acc: 0.9275, best train loss: 0.3150  @ epoch 10927 )\n",
      "[Epoch: 11020] train loss: 0.3338, train acc: 0.8720, val loss: 0.5488, val acc: 0.9201  (best train acc: 0.8870, best val acc: 0.9302, best train loss: 0.3150  @ epoch 10927 )\n",
      "[Epoch: 11040] train loss: 0.3435, train acc: 0.8655, val loss: 0.5337, val acc: 0.9228  (best train acc: 0.8870, best val acc: 0.9309, best train loss: 0.3150  @ epoch 10927 )\n",
      "[Epoch: 11060] train loss: 0.3455, train acc: 0.8750, val loss: 0.4867, val acc: 0.9245  (best train acc: 0.8870, best val acc: 0.9309, best train loss: 0.3150  @ epoch 10927 )\n",
      "[Epoch: 11080] train loss: 0.3195, train acc: 0.8799, val loss: 0.5105, val acc: 0.9238  (best train acc: 0.8870, best val acc: 0.9309, best train loss: 0.3136  @ epoch 11076 )\n",
      "[Epoch: 11100] train loss: 0.3731, train acc: 0.8504, val loss: 0.5274, val acc: 0.9187  (best train acc: 0.8870, best val acc: 0.9309, best train loss: 0.3136  @ epoch 11076 )\n",
      "[Epoch: 11120] train loss: 0.3341, train acc: 0.8742, val loss: 0.5194, val acc: 0.9241  (best train acc: 0.8870, best val acc: 0.9309, best train loss: 0.3136  @ epoch 11076 )\n",
      "[Epoch: 11140] train loss: 0.3286, train acc: 0.8746, val loss: 0.5330, val acc: 0.9218  (best train acc: 0.8870, best val acc: 0.9309, best train loss: 0.3136  @ epoch 11076 )\n",
      "[Epoch: 11160] train loss: 0.3105, train acc: 0.8871, val loss: 0.5308, val acc: 0.9258  (best train acc: 0.8871, best val acc: 0.9309, best train loss: 0.3105  @ epoch 11160 )\n",
      "[Epoch: 11180] train loss: 0.3260, train acc: 0.8816, val loss: 0.5555, val acc: 0.9184  (best train acc: 0.8879, best val acc: 0.9309, best train loss: 0.3079  @ epoch 11174 )\n",
      "[Epoch: 11200] train loss: 0.3250, train acc: 0.8750, val loss: 0.5160, val acc: 0.9268  (best train acc: 0.8879, best val acc: 0.9309, best train loss: 0.3004  @ epoch 11192 )\n",
      "[Epoch: 11220] train loss: 0.3023, train acc: 0.8890, val loss: 0.5355, val acc: 0.9261  (best train acc: 0.8906, best val acc: 0.9309, best train loss: 0.2989  @ epoch 11206 )\n",
      "[Epoch: 11240] train loss: 0.3044, train acc: 0.8896, val loss: 0.5478, val acc: 0.9241  (best train acc: 0.8906, best val acc: 0.9309, best train loss: 0.2989  @ epoch 11206 )\n",
      "[Epoch: 11260] train loss: 0.3196, train acc: 0.8785, val loss: 0.5178, val acc: 0.9234  (best train acc: 0.8906, best val acc: 0.9309, best train loss: 0.2989  @ epoch 11206 )\n",
      "[Epoch: 11280] train loss: 0.3553, train acc: 0.8561, val loss: 0.4701, val acc: 0.9194  (best train acc: 0.8906, best val acc: 0.9309, best train loss: 0.2989  @ epoch 11206 )\n",
      "[Epoch: 11300] train loss: 0.3144, train acc: 0.8817, val loss: 0.5852, val acc: 0.9238  (best train acc: 0.8906, best val acc: 0.9309, best train loss: 0.2989  @ epoch 11206 )\n",
      "[Epoch: 11320] train loss: 0.3060, train acc: 0.8843, val loss: 0.6245, val acc: 0.9248  (best train acc: 0.8906, best val acc: 0.9315, best train loss: 0.2989  @ epoch 11206 )\n",
      "[Epoch: 11340] train loss: 0.3064, train acc: 0.8911, val loss: 0.5903, val acc: 0.9248  (best train acc: 0.8911, best val acc: 0.9315, best train loss: 0.2989  @ epoch 11206 )\n",
      "[Epoch: 11360] train loss: 0.3230, train acc: 0.8764, val loss: 0.5781, val acc: 0.9245  (best train acc: 0.8911, best val acc: 0.9315, best train loss: 0.2989  @ epoch 11206 )\n",
      "[Epoch: 11380] train loss: 0.3207, train acc: 0.8788, val loss: 0.5896, val acc: 0.9248  (best train acc: 0.8935, best val acc: 0.9315, best train loss: 0.2977  @ epoch 11369 )\n",
      "[Epoch: 11400] train loss: 0.3234, train acc: 0.8763, val loss: 0.6092, val acc: 0.9062  (best train acc: 0.8935, best val acc: 0.9315, best train loss: 0.2968  @ epoch 11394 )\n",
      "[Epoch: 11420] train loss: 0.3261, train acc: 0.8797, val loss: 0.6138, val acc: 0.9174  (best train acc: 0.8935, best val acc: 0.9315, best train loss: 0.2968  @ epoch 11394 )\n",
      "[Epoch: 11440] train loss: 0.3102, train acc: 0.8879, val loss: 0.5446, val acc: 0.9241  (best train acc: 0.8935, best val acc: 0.9315, best train loss: 0.2968  @ epoch 11394 )\n",
      "[Epoch: 11460] train loss: 0.3107, train acc: 0.8836, val loss: 0.5599, val acc: 0.9258  (best train acc: 0.8935, best val acc: 0.9315, best train loss: 0.2968  @ epoch 11394 )\n",
      "[Epoch: 11480] train loss: 0.3268, train acc: 0.8684, val loss: 0.5619, val acc: 0.9261  (best train acc: 0.8935, best val acc: 0.9315, best train loss: 0.2968  @ epoch 11394 )\n",
      "[Epoch: 11500] train loss: 0.3107, train acc: 0.8872, val loss: 0.5452, val acc: 0.9143  (best train acc: 0.8935, best val acc: 0.9315, best train loss: 0.2968  @ epoch 11394 )\n",
      "[Epoch: 11520] train loss: 0.3084, train acc: 0.8942, val loss: 0.5982, val acc: 0.9224  (best train acc: 0.8942, best val acc: 0.9315, best train loss: 0.2968  @ epoch 11394 )\n",
      "[Epoch: 11540] train loss: 0.3151, train acc: 0.8803, val loss: 0.6044, val acc: 0.9261  (best train acc: 0.8942, best val acc: 0.9315, best train loss: 0.2937  @ epoch 11528 )\n",
      "[Epoch: 11560] train loss: 0.3076, train acc: 0.8856, val loss: 0.5403, val acc: 0.9258  (best train acc: 0.8942, best val acc: 0.9315, best train loss: 0.2937  @ epoch 11528 )\n",
      "[Epoch: 11580] train loss: 0.3302, train acc: 0.8706, val loss: 0.6248, val acc: 0.9265  (best train acc: 0.8942, best val acc: 0.9315, best train loss: 0.2937  @ epoch 11528 )\n",
      "[Epoch: 11600] train loss: 0.3063, train acc: 0.8861, val loss: 0.6004, val acc: 0.9224  (best train acc: 0.8942, best val acc: 0.9315, best train loss: 0.2937  @ epoch 11528 )\n",
      "[Epoch: 11620] train loss: 0.3064, train acc: 0.8916, val loss: 0.6565, val acc: 0.9251  (best train acc: 0.8942, best val acc: 0.9315, best train loss: 0.2937  @ epoch 11528 )\n",
      "[Epoch: 11640] train loss: 0.3171, train acc: 0.8846, val loss: 0.6793, val acc: 0.9160  (best train acc: 0.8942, best val acc: 0.9315, best train loss: 0.2937  @ epoch 11528 )\n",
      "[Epoch: 11660] train loss: 0.3297, train acc: 0.8747, val loss: 0.6337, val acc: 0.9191  (best train acc: 0.8942, best val acc: 0.9315, best train loss: 0.2937  @ epoch 11528 )\n",
      "[Epoch: 11680] train loss: 0.3236, train acc: 0.8749, val loss: 0.6241, val acc: 0.9255  (best train acc: 0.8942, best val acc: 0.9315, best train loss: 0.2937  @ epoch 11528 )\n",
      "[Epoch: 11700] train loss: 0.3083, train acc: 0.8811, val loss: 0.6908, val acc: 0.9147  (best train acc: 0.8942, best val acc: 0.9315, best train loss: 0.2937  @ epoch 11528 )\n",
      "[Epoch: 11720] train loss: 0.3182, train acc: 0.8745, val loss: 0.6216, val acc: 0.9282  (best train acc: 0.8942, best val acc: 0.9315, best train loss: 0.2937  @ epoch 11528 )\n",
      "[Epoch: 11740] train loss: 0.3027, train acc: 0.8826, val loss: 0.6209, val acc: 0.9265  (best train acc: 0.8942, best val acc: 0.9315, best train loss: 0.2937  @ epoch 11528 )\n",
      "[Epoch: 11760] train loss: 0.3157, train acc: 0.8806, val loss: 0.6612, val acc: 0.9231  (best train acc: 0.8942, best val acc: 0.9315, best train loss: 0.2934  @ epoch 11754 )\n",
      "[Epoch: 11780] train loss: 0.3023, train acc: 0.8911, val loss: 0.6189, val acc: 0.9167  (best train acc: 0.8942, best val acc: 0.9315, best train loss: 0.2934  @ epoch 11754 )\n",
      "[Epoch: 11800] train loss: 0.3245, train acc: 0.8739, val loss: 0.5744, val acc: 0.9275  (best train acc: 0.8942, best val acc: 0.9315, best train loss: 0.2934  @ epoch 11754 )\n",
      "[Epoch: 11820] train loss: 0.3113, train acc: 0.8792, val loss: 0.6692, val acc: 0.9275  (best train acc: 0.8942, best val acc: 0.9315, best train loss: 0.2934  @ epoch 11754 )\n",
      "[Epoch: 11840] train loss: 0.3046, train acc: 0.8881, val loss: 0.6643, val acc: 0.9238  (best train acc: 0.8942, best val acc: 0.9315, best train loss: 0.2934  @ epoch 11754 )\n",
      "[Epoch: 11860] train loss: 0.3151, train acc: 0.8871, val loss: 0.6747, val acc: 0.9245  (best train acc: 0.8942, best val acc: 0.9315, best train loss: 0.2934  @ epoch 11754 )\n",
      "[Epoch: 11880] train loss: 0.3041, train acc: 0.8887, val loss: 0.6820, val acc: 0.9238  (best train acc: 0.8942, best val acc: 0.9315, best train loss: 0.2934  @ epoch 11754 )\n",
      "[Epoch: 11900] train loss: 0.2930, train acc: 0.8953, val loss: 0.6462, val acc: 0.9255  (best train acc: 0.8953, best val acc: 0.9315, best train loss: 0.2913  @ epoch 11890 )\n",
      "[Epoch: 11920] train loss: 0.3068, train acc: 0.8866, val loss: 0.6455, val acc: 0.9214  (best train acc: 0.8960, best val acc: 0.9315, best train loss: 0.2902  @ epoch 11903 )\n",
      "[Epoch: 11940] train loss: 0.3217, train acc: 0.8806, val loss: 0.6493, val acc: 0.9211  (best train acc: 0.8960, best val acc: 0.9315, best train loss: 0.2902  @ epoch 11903 )\n",
      "[Epoch: 11960] train loss: 0.3052, train acc: 0.8844, val loss: 0.6477, val acc: 0.9241  (best train acc: 0.8960, best val acc: 0.9315, best train loss: 0.2902  @ epoch 11903 )\n",
      "[Epoch: 11980] train loss: 0.3011, train acc: 0.8861, val loss: 0.6552, val acc: 0.9251  (best train acc: 0.8960, best val acc: 0.9315, best train loss: 0.2902  @ epoch 11903 )\n",
      "[Epoch: 12000] train loss: 0.3468, train acc: 0.8671, val loss: 0.6423, val acc: 0.9187  (best train acc: 0.8960, best val acc: 0.9315, best train loss: 0.2902  @ epoch 11903 )\n",
      "[Epoch: 12020] train loss: 0.3332, train acc: 0.8712, val loss: 0.7134, val acc: 0.9160  (best train acc: 0.8960, best val acc: 0.9315, best train loss: 0.2902  @ epoch 11903 )\n",
      "[Epoch: 12040] train loss: 0.3279, train acc: 0.8708, val loss: 0.6655, val acc: 0.9164  (best train acc: 0.8960, best val acc: 0.9315, best train loss: 0.2902  @ epoch 11903 )\n",
      "[Epoch: 12060] train loss: 0.3151, train acc: 0.8779, val loss: 0.6484, val acc: 0.9275  (best train acc: 0.8960, best val acc: 0.9315, best train loss: 0.2902  @ epoch 11903 )\n",
      "[Epoch: 12080] train loss: 0.2984, train acc: 0.8893, val loss: 0.6355, val acc: 0.9228  (best train acc: 0.8960, best val acc: 0.9315, best train loss: 0.2902  @ epoch 11903 )\n",
      "[Epoch: 12100] train loss: 0.2832, train acc: 0.8978, val loss: 0.6231, val acc: 0.9241  (best train acc: 0.9007, best val acc: 0.9315, best train loss: 0.2832  @ epoch 12100 )\n",
      "[Epoch: 12120] train loss: 0.2983, train acc: 0.8913, val loss: 0.6462, val acc: 0.9285  (best train acc: 0.9035, best val acc: 0.9315, best train loss: 0.2809  @ epoch 12109 )\n",
      "[Epoch: 12140] train loss: 0.3225, train acc: 0.8738, val loss: 0.6370, val acc: 0.9191  (best train acc: 0.9035, best val acc: 0.9332, best train loss: 0.2809  @ epoch 12109 )\n",
      "[Epoch: 12160] train loss: 0.3085, train acc: 0.8921, val loss: 0.5977, val acc: 0.9251  (best train acc: 0.9035, best val acc: 0.9332, best train loss: 0.2809  @ epoch 12109 )\n",
      "[Epoch: 12180] train loss: 0.3223, train acc: 0.8774, val loss: 0.6216, val acc: 0.9164  (best train acc: 0.9035, best val acc: 0.9332, best train loss: 0.2809  @ epoch 12109 )\n",
      "[Epoch: 12200] train loss: 0.2985, train acc: 0.8965, val loss: 0.6801, val acc: 0.9251  (best train acc: 0.9035, best val acc: 0.9332, best train loss: 0.2809  @ epoch 12109 )\n",
      "[Epoch: 12220] train loss: 0.2992, train acc: 0.8955, val loss: 0.6798, val acc: 0.9228  (best train acc: 0.9075, best val acc: 0.9332, best train loss: 0.2729  @ epoch 12218 )\n",
      "[Epoch: 12240] train loss: 0.2973, train acc: 0.8962, val loss: 0.6817, val acc: 0.9187  (best train acc: 0.9075, best val acc: 0.9332, best train loss: 0.2729  @ epoch 12218 )\n",
      "[Epoch: 12260] train loss: 0.3107, train acc: 0.8930, val loss: 0.6755, val acc: 0.9191  (best train acc: 0.9075, best val acc: 0.9332, best train loss: 0.2729  @ epoch 12218 )\n",
      "[Epoch: 12280] train loss: 0.3090, train acc: 0.8830, val loss: 0.6556, val acc: 0.9265  (best train acc: 0.9075, best val acc: 0.9332, best train loss: 0.2729  @ epoch 12218 )\n",
      "[Epoch: 12300] train loss: 0.2961, train acc: 0.8908, val loss: 0.6788, val acc: 0.9295  (best train acc: 0.9075, best val acc: 0.9332, best train loss: 0.2729  @ epoch 12218 )\n",
      "[Epoch: 12320] train loss: 0.2860, train acc: 0.8942, val loss: 0.6636, val acc: 0.9241  (best train acc: 0.9075, best val acc: 0.9332, best train loss: 0.2729  @ epoch 12218 )\n",
      "[Epoch: 12340] train loss: 0.3196, train acc: 0.8769, val loss: 0.6619, val acc: 0.9342  (best train acc: 0.9075, best val acc: 0.9342, best train loss: 0.2729  @ epoch 12218 )\n",
      "[Epoch: 12360] train loss: 0.2895, train acc: 0.8958, val loss: 0.6641, val acc: 0.9177  (best train acc: 0.9075, best val acc: 0.9342, best train loss: 0.2729  @ epoch 12218 )\n",
      "[Epoch: 12380] train loss: 0.3049, train acc: 0.8863, val loss: 0.6980, val acc: 0.9255  (best train acc: 0.9075, best val acc: 0.9342, best train loss: 0.2729  @ epoch 12218 )\n",
      "[Epoch: 12400] train loss: 0.2942, train acc: 0.8978, val loss: 0.7696, val acc: 0.9204  (best train acc: 0.9075, best val acc: 0.9342, best train loss: 0.2729  @ epoch 12218 )\n",
      "[Epoch: 12420] train loss: 0.2899, train acc: 0.8953, val loss: 0.6797, val acc: 0.9265  (best train acc: 0.9075, best val acc: 0.9342, best train loss: 0.2729  @ epoch 12218 )\n",
      "[Epoch: 12440] train loss: 0.2971, train acc: 0.8879, val loss: 0.6850, val acc: 0.9218  (best train acc: 0.9075, best val acc: 0.9342, best train loss: 0.2729  @ epoch 12218 )\n",
      "[Epoch: 12460] train loss: 0.3058, train acc: 0.8853, val loss: 0.6904, val acc: 0.9278  (best train acc: 0.9075, best val acc: 0.9342, best train loss: 0.2729  @ epoch 12218 )\n",
      "[Epoch: 12480] train loss: 0.2959, train acc: 0.8950, val loss: 0.6805, val acc: 0.9234  (best train acc: 0.9075, best val acc: 0.9342, best train loss: 0.2729  @ epoch 12218 )\n",
      "[Epoch: 12500] train loss: 0.2984, train acc: 0.8987, val loss: 0.7177, val acc: 0.9309  (best train acc: 0.9075, best val acc: 0.9342, best train loss: 0.2729  @ epoch 12218 )\n",
      "[Epoch: 12520] train loss: 0.2923, train acc: 0.8937, val loss: 0.7098, val acc: 0.9285  (best train acc: 0.9075, best val acc: 0.9342, best train loss: 0.2729  @ epoch 12218 )\n",
      "[Epoch: 12540] train loss: 0.2975, train acc: 0.8963, val loss: 0.7407, val acc: 0.9248  (best train acc: 0.9075, best val acc: 0.9342, best train loss: 0.2729  @ epoch 12218 )\n",
      "[Epoch: 12560] train loss: 0.2897, train acc: 0.8917, val loss: 0.7024, val acc: 0.9261  (best train acc: 0.9075, best val acc: 0.9342, best train loss: 0.2729  @ epoch 12218 )\n",
      "[Epoch: 12580] train loss: 0.2949, train acc: 0.9023, val loss: 0.7431, val acc: 0.9164  (best train acc: 0.9075, best val acc: 0.9342, best train loss: 0.2729  @ epoch 12218 )\n",
      "[Epoch: 12600] train loss: 0.3057, train acc: 0.8886, val loss: 0.6926, val acc: 0.9288  (best train acc: 0.9075, best val acc: 0.9342, best train loss: 0.2729  @ epoch 12218 )\n",
      "[Epoch: 12620] train loss: 0.3041, train acc: 0.8928, val loss: 0.7497, val acc: 0.9086  (best train acc: 0.9075, best val acc: 0.9342, best train loss: 0.2729  @ epoch 12218 )\n",
      "[Epoch: 12640] train loss: 0.3006, train acc: 0.8950, val loss: 0.7060, val acc: 0.9238  (best train acc: 0.9075, best val acc: 0.9342, best train loss: 0.2729  @ epoch 12218 )\n",
      "[Epoch: 12660] train loss: 0.2874, train acc: 0.8968, val loss: 0.6887, val acc: 0.9241  (best train acc: 0.9075, best val acc: 0.9342, best train loss: 0.2729  @ epoch 12218 )\n",
      "[Epoch: 12680] train loss: 0.2882, train acc: 0.8965, val loss: 0.6745, val acc: 0.9275  (best train acc: 0.9075, best val acc: 0.9342, best train loss: 0.2729  @ epoch 12218 )\n",
      "[Epoch: 12700] train loss: 0.2866, train acc: 0.8995, val loss: 0.6683, val acc: 0.9275  (best train acc: 0.9075, best val acc: 0.9342, best train loss: 0.2729  @ epoch 12218 )\n",
      "[Epoch: 12720] train loss: 0.3119, train acc: 0.8811, val loss: 0.6847, val acc: 0.9174  (best train acc: 0.9075, best val acc: 0.9342, best train loss: 0.2729  @ epoch 12218 )\n",
      "[Epoch: 12740] train loss: 0.2945, train acc: 0.8845, val loss: 0.7413, val acc: 0.9120  (best train acc: 0.9075, best val acc: 0.9342, best train loss: 0.2729  @ epoch 12218 )\n",
      "[Epoch: 12760] train loss: 0.3219, train acc: 0.8856, val loss: 0.7608, val acc: 0.9201  (best train acc: 0.9075, best val acc: 0.9342, best train loss: 0.2729  @ epoch 12218 )\n",
      "[Epoch: 12780] train loss: 0.3225, train acc: 0.8790, val loss: 0.7476, val acc: 0.9268  (best train acc: 0.9075, best val acc: 0.9342, best train loss: 0.2723  @ epoch 12774 )\n",
      "[Epoch: 12800] train loss: 0.3112, train acc: 0.8803, val loss: 0.7372, val acc: 0.9238  (best train acc: 0.9075, best val acc: 0.9342, best train loss: 0.2723  @ epoch 12774 )\n",
      "[Epoch: 12820] train loss: 0.2938, train acc: 0.8907, val loss: 0.7002, val acc: 0.9258  (best train acc: 0.9075, best val acc: 0.9342, best train loss: 0.2723  @ epoch 12774 )\n",
      "[Epoch: 12840] train loss: 0.2869, train acc: 0.8942, val loss: 0.7015, val acc: 0.9248  (best train acc: 0.9075, best val acc: 0.9342, best train loss: 0.2723  @ epoch 12774 )\n",
      "[Epoch: 12860] train loss: 0.2925, train acc: 0.8963, val loss: 0.7165, val acc: 0.9255  (best train acc: 0.9075, best val acc: 0.9342, best train loss: 0.2723  @ epoch 12774 )\n",
      "[Epoch: 12880] train loss: 0.2824, train acc: 0.9015, val loss: 0.6864, val acc: 0.9265  (best train acc: 0.9075, best val acc: 0.9342, best train loss: 0.2723  @ epoch 12774 )\n",
      "[Epoch: 12900] train loss: 0.3020, train acc: 0.8888, val loss: 0.6835, val acc: 0.9191  (best train acc: 0.9075, best val acc: 0.9342, best train loss: 0.2723  @ epoch 12774 )\n",
      "[Epoch: 12920] train loss: 0.2941, train acc: 0.8926, val loss: 0.7430, val acc: 0.9268  (best train acc: 0.9075, best val acc: 0.9342, best train loss: 0.2723  @ epoch 12774 )\n",
      "[Epoch: 12940] train loss: 0.2862, train acc: 0.8972, val loss: 0.6939, val acc: 0.9302  (best train acc: 0.9075, best val acc: 0.9342, best train loss: 0.2709  @ epoch 12932 )\n",
      "[Epoch: 12960] train loss: 0.2786, train acc: 0.8987, val loss: 0.6860, val acc: 0.9292  (best train acc: 0.9075, best val acc: 0.9342, best train loss: 0.2709  @ epoch 12932 )\n",
      "[Epoch: 12980] train loss: 0.2879, train acc: 0.8937, val loss: 0.7017, val acc: 0.9295  (best train acc: 0.9075, best val acc: 0.9342, best train loss: 0.2709  @ epoch 12932 )\n",
      "[Epoch: 13000] train loss: 0.2913, train acc: 0.8936, val loss: 0.7288, val acc: 0.9245  (best train acc: 0.9075, best val acc: 0.9342, best train loss: 0.2709  @ epoch 12932 )\n",
      "[Epoch: 13020] train loss: 0.2871, train acc: 0.9012, val loss: 0.7005, val acc: 0.9285  (best train acc: 0.9075, best val acc: 0.9342, best train loss: 0.2709  @ epoch 12932 )\n",
      "[Epoch: 13040] train loss: 0.2799, train acc: 0.8989, val loss: 0.6876, val acc: 0.9251  (best train acc: 0.9075, best val acc: 0.9342, best train loss: 0.2709  @ epoch 12932 )\n",
      "[Epoch: 13060] train loss: 0.2972, train acc: 0.8920, val loss: 0.7070, val acc: 0.9214  (best train acc: 0.9075, best val acc: 0.9342, best train loss: 0.2709  @ epoch 12932 )\n",
      "[Epoch: 13080] train loss: 0.2793, train acc: 0.9035, val loss: 0.6787, val acc: 0.9272  (best train acc: 0.9075, best val acc: 0.9342, best train loss: 0.2709  @ epoch 12932 )\n",
      "[Epoch: 13100] train loss: 0.2895, train acc: 0.8972, val loss: 0.6864, val acc: 0.9282  (best train acc: 0.9075, best val acc: 0.9342, best train loss: 0.2695  @ epoch 13097 )\n",
      "[Epoch: 13120] train loss: 0.2901, train acc: 0.8890, val loss: 0.6587, val acc: 0.9258  (best train acc: 0.9075, best val acc: 0.9342, best train loss: 0.2695  @ epoch 13097 )\n",
      "[Epoch: 13140] train loss: 0.2989, train acc: 0.8989, val loss: 0.6957, val acc: 0.9282  (best train acc: 0.9075, best val acc: 0.9342, best train loss: 0.2695  @ epoch 13097 )\n",
      "[Epoch: 13160] train loss: 0.2860, train acc: 0.8973, val loss: 0.6803, val acc: 0.9325  (best train acc: 0.9075, best val acc: 0.9342, best train loss: 0.2695  @ epoch 13097 )\n",
      "[Epoch: 13180] train loss: 0.2864, train acc: 0.9013, val loss: 0.6969, val acc: 0.9292  (best train acc: 0.9075, best val acc: 0.9342, best train loss: 0.2695  @ epoch 13097 )\n",
      "[Epoch: 13200] train loss: 0.3301, train acc: 0.8717, val loss: 0.6547, val acc: 0.9251  (best train acc: 0.9075, best val acc: 0.9342, best train loss: 0.2695  @ epoch 13097 )\n",
      "[Epoch: 13220] train loss: 0.2791, train acc: 0.8978, val loss: 0.6323, val acc: 0.9309  (best train acc: 0.9075, best val acc: 0.9342, best train loss: 0.2695  @ epoch 13097 )\n",
      "[Epoch: 13240] train loss: 0.2911, train acc: 0.8992, val loss: 0.7068, val acc: 0.9255  (best train acc: 0.9075, best val acc: 0.9342, best train loss: 0.2695  @ epoch 13097 )\n",
      "[Epoch: 13260] train loss: 0.2965, train acc: 0.8934, val loss: 0.6400, val acc: 0.9197  (best train acc: 0.9075, best val acc: 0.9342, best train loss: 0.2695  @ epoch 13097 )\n",
      "[Epoch: 13280] train loss: 0.2816, train acc: 0.8967, val loss: 0.7163, val acc: 0.9258  (best train acc: 0.9075, best val acc: 0.9342, best train loss: 0.2695  @ epoch 13097 )\n",
      "[Epoch: 13300] train loss: 0.2972, train acc: 0.8933, val loss: 0.6701, val acc: 0.9204  (best train acc: 0.9075, best val acc: 0.9342, best train loss: 0.2695  @ epoch 13097 )\n",
      "[Epoch: 13320] train loss: 0.2797, train acc: 0.9018, val loss: 0.6067, val acc: 0.9272  (best train acc: 0.9075, best val acc: 0.9342, best train loss: 0.2695  @ epoch 13097 )\n",
      "[Epoch: 13340] train loss: 0.2981, train acc: 0.8920, val loss: 0.7173, val acc: 0.9275  (best train acc: 0.9075, best val acc: 0.9342, best train loss: 0.2695  @ epoch 13097 )\n",
      "[Epoch: 13360] train loss: 0.2834, train acc: 0.8947, val loss: 0.6791, val acc: 0.9268  (best train acc: 0.9075, best val acc: 0.9342, best train loss: 0.2644  @ epoch 13358 )\n",
      "[Epoch: 13380] train loss: 0.2843, train acc: 0.8979, val loss: 0.6684, val acc: 0.9228  (best train acc: 0.9075, best val acc: 0.9342, best train loss: 0.2644  @ epoch 13358 )\n",
      "[Epoch: 13400] train loss: 0.2816, train acc: 0.8971, val loss: 0.6445, val acc: 0.9268  (best train acc: 0.9075, best val acc: 0.9342, best train loss: 0.2644  @ epoch 13358 )\n",
      "[Epoch: 13420] train loss: 0.2822, train acc: 0.9005, val loss: 0.6545, val acc: 0.9268  (best train acc: 0.9075, best val acc: 0.9342, best train loss: 0.2644  @ epoch 13358 )\n",
      "[Epoch: 13440] train loss: 0.3273, train acc: 0.8759, val loss: 0.6263, val acc: 0.9261  (best train acc: 0.9075, best val acc: 0.9342, best train loss: 0.2644  @ epoch 13358 )\n",
      "[Epoch: 13460] train loss: 0.2758, train acc: 0.9026, val loss: 0.7094, val acc: 0.9305  (best train acc: 0.9075, best val acc: 0.9342, best train loss: 0.2644  @ epoch 13358 )\n",
      "[Epoch: 13480] train loss: 0.2940, train acc: 0.8955, val loss: 0.6463, val acc: 0.9218  (best train acc: 0.9075, best val acc: 0.9342, best train loss: 0.2644  @ epoch 13358 )\n",
      "[Epoch: 13500] train loss: 0.3017, train acc: 0.8903, val loss: 0.6771, val acc: 0.9123  (best train acc: 0.9075, best val acc: 0.9342, best train loss: 0.2644  @ epoch 13358 )\n",
      "[Epoch: 13520] train loss: 0.3004, train acc: 0.8843, val loss: 0.6548, val acc: 0.9309  (best train acc: 0.9075, best val acc: 0.9342, best train loss: 0.2644  @ epoch 13358 )\n",
      "[Epoch: 13540] train loss: 0.2817, train acc: 0.9007, val loss: 0.6378, val acc: 0.9302  (best train acc: 0.9075, best val acc: 0.9342, best train loss: 0.2644  @ epoch 13358 )\n",
      "[Epoch: 13560] train loss: 0.2947, train acc: 0.8967, val loss: 0.6151, val acc: 0.9261  (best train acc: 0.9075, best val acc: 0.9342, best train loss: 0.2644  @ epoch 13358 )\n",
      "[Epoch: 13580] train loss: 0.2821, train acc: 0.8975, val loss: 0.6284, val acc: 0.9325  (best train acc: 0.9075, best val acc: 0.9342, best train loss: 0.2644  @ epoch 13358 )\n",
      "[Epoch: 13600] train loss: 0.2858, train acc: 0.8991, val loss: 0.6329, val acc: 0.9272  (best train acc: 0.9075, best val acc: 0.9342, best train loss: 0.2644  @ epoch 13358 )\n",
      "[Epoch: 13620] train loss: 0.2757, train acc: 0.8958, val loss: 0.6332, val acc: 0.9238  (best train acc: 0.9075, best val acc: 0.9342, best train loss: 0.2644  @ epoch 13358 )\n",
      "[Epoch: 13640] train loss: 0.2746, train acc: 0.9012, val loss: 0.6146, val acc: 0.9261  (best train acc: 0.9088, best val acc: 0.9342, best train loss: 0.2644  @ epoch 13358 )\n",
      "[Epoch: 13660] train loss: 0.2799, train acc: 0.9020, val loss: 0.6415, val acc: 0.9268  (best train acc: 0.9088, best val acc: 0.9342, best train loss: 0.2644  @ epoch 13358 )\n",
      "[Epoch: 13680] train loss: 0.2898, train acc: 0.8929, val loss: 0.6962, val acc: 0.9332  (best train acc: 0.9088, best val acc: 0.9342, best train loss: 0.2644  @ epoch 13358 )\n",
      "[Epoch: 13700] train loss: 0.2775, train acc: 0.9026, val loss: 0.5805, val acc: 0.9224  (best train acc: 0.9088, best val acc: 0.9342, best train loss: 0.2644  @ epoch 13358 )\n",
      "[Epoch: 13720] train loss: 0.2890, train acc: 0.8932, val loss: 0.6343, val acc: 0.9238  (best train acc: 0.9088, best val acc: 0.9342, best train loss: 0.2644  @ epoch 13358 )\n",
      "[Epoch: 13740] train loss: 0.2795, train acc: 0.8985, val loss: 0.6909, val acc: 0.9251  (best train acc: 0.9088, best val acc: 0.9342, best train loss: 0.2644  @ epoch 13358 )\n",
      "[Epoch: 13760] train loss: 0.2863, train acc: 0.8943, val loss: 0.7191, val acc: 0.9214  (best train acc: 0.9088, best val acc: 0.9342, best train loss: 0.2601  @ epoch 13757 )\n",
      "[Epoch: 13780] train loss: 0.2844, train acc: 0.8901, val loss: 0.7057, val acc: 0.9305  (best train acc: 0.9096, best val acc: 0.9342, best train loss: 0.2601  @ epoch 13757 )\n",
      "[Epoch: 13800] train loss: 0.2876, train acc: 0.8959, val loss: 0.6947, val acc: 0.9285  (best train acc: 0.9096, best val acc: 0.9342, best train loss: 0.2573  @ epoch 13795 )\n",
      "[Epoch: 13820] train loss: 0.2740, train acc: 0.9009, val loss: 0.7272, val acc: 0.9234  (best train acc: 0.9096, best val acc: 0.9342, best train loss: 0.2571  @ epoch 13804 )\n",
      "[Epoch: 13840] train loss: 0.2717, train acc: 0.9015, val loss: 0.6886, val acc: 0.9295  (best train acc: 0.9096, best val acc: 0.9342, best train loss: 0.2571  @ epoch 13804 )\n",
      "[Epoch: 13860] train loss: 0.2920, train acc: 0.8845, val loss: 0.6236, val acc: 0.9302  (best train acc: 0.9096, best val acc: 0.9342, best train loss: 0.2534  @ epoch 13845 )\n",
      "[Epoch: 13880] train loss: 0.2590, train acc: 0.9089, val loss: 0.6290, val acc: 0.9305  (best train acc: 0.9096, best val acc: 0.9342, best train loss: 0.2534  @ epoch 13845 )\n",
      "[Epoch: 13900] train loss: 0.2748, train acc: 0.9000, val loss: 0.6457, val acc: 0.9288  (best train acc: 0.9098, best val acc: 0.9342, best train loss: 0.2534  @ epoch 13845 )\n",
      "[Epoch: 13920] train loss: 0.2788, train acc: 0.8996, val loss: 0.6097, val acc: 0.9325  (best train acc: 0.9098, best val acc: 0.9342, best train loss: 0.2534  @ epoch 13845 )\n",
      "[Epoch: 13940] train loss: 0.2928, train acc: 0.8999, val loss: 0.6445, val acc: 0.9143  (best train acc: 0.9098, best val acc: 0.9342, best train loss: 0.2534  @ epoch 13845 )\n",
      "[Epoch: 13960] train loss: 0.2748, train acc: 0.8979, val loss: 0.6289, val acc: 0.9194  (best train acc: 0.9098, best val acc: 0.9342, best train loss: 0.2534  @ epoch 13845 )\n",
      "[Epoch: 13980] train loss: 0.2670, train acc: 0.9023, val loss: 0.6288, val acc: 0.9285  (best train acc: 0.9098, best val acc: 0.9342, best train loss: 0.2534  @ epoch 13845 )\n",
      "[Epoch: 14000] train loss: 0.2664, train acc: 0.9062, val loss: 0.6344, val acc: 0.9228  (best train acc: 0.9098, best val acc: 0.9342, best train loss: 0.2534  @ epoch 13845 )\n",
      "[Epoch: 14020] train loss: 0.2941, train acc: 0.8887, val loss: 0.6500, val acc: 0.9194  (best train acc: 0.9098, best val acc: 0.9349, best train loss: 0.2529  @ epoch 14011 )\n",
      "[Epoch: 14040] train loss: 0.2749, train acc: 0.8999, val loss: 0.6573, val acc: 0.9298  (best train acc: 0.9098, best val acc: 0.9349, best train loss: 0.2529  @ epoch 14011 )\n",
      "[Epoch: 14060] train loss: 0.2711, train acc: 0.9038, val loss: 0.5797, val acc: 0.9336  (best train acc: 0.9099, best val acc: 0.9359, best train loss: 0.2529  @ epoch 14011 )\n",
      "[Epoch: 14080] train loss: 0.2737, train acc: 0.8968, val loss: 0.6495, val acc: 0.9295  (best train acc: 0.9099, best val acc: 0.9359, best train loss: 0.2499  @ epoch 14065 )\n",
      "[Epoch: 14100] train loss: 0.2787, train acc: 0.8964, val loss: 0.6422, val acc: 0.9319  (best train acc: 0.9099, best val acc: 0.9359, best train loss: 0.2499  @ epoch 14065 )\n",
      "[Epoch: 14120] train loss: 0.2507, train acc: 0.9093, val loss: 0.6065, val acc: 0.9221  (best train acc: 0.9099, best val acc: 0.9359, best train loss: 0.2499  @ epoch 14065 )\n",
      "[Epoch: 14140] train loss: 0.2978, train acc: 0.8885, val loss: 0.6462, val acc: 0.9221  (best train acc: 0.9099, best val acc: 0.9359, best train loss: 0.2499  @ epoch 14065 )\n",
      "[Epoch: 14160] train loss: 0.3018, train acc: 0.8847, val loss: 0.6233, val acc: 0.9211  (best train acc: 0.9099, best val acc: 0.9359, best train loss: 0.2499  @ epoch 14065 )\n",
      "[Epoch: 14180] train loss: 0.2630, train acc: 0.9051, val loss: 0.6135, val acc: 0.9312  (best train acc: 0.9099, best val acc: 0.9359, best train loss: 0.2499  @ epoch 14065 )\n",
      "[Epoch: 14200] train loss: 0.2888, train acc: 0.8901, val loss: 0.6485, val acc: 0.9325  (best train acc: 0.9099, best val acc: 0.9359, best train loss: 0.2499  @ epoch 14065 )\n",
      "[Epoch: 14220] train loss: 0.2705, train acc: 0.8952, val loss: 0.6144, val acc: 0.9272  (best train acc: 0.9099, best val acc: 0.9359, best train loss: 0.2499  @ epoch 14065 )\n",
      "[Epoch: 14240] train loss: 0.2671, train acc: 0.9028, val loss: 0.5802, val acc: 0.9275  (best train acc: 0.9099, best val acc: 0.9363, best train loss: 0.2499  @ epoch 14065 )\n",
      "[Epoch: 14260] train loss: 0.2831, train acc: 0.8937, val loss: 0.6652, val acc: 0.9315  (best train acc: 0.9099, best val acc: 0.9369, best train loss: 0.2499  @ epoch 14065 )\n",
      "[Epoch: 14280] train loss: 0.2819, train acc: 0.8999, val loss: 0.6221, val acc: 0.9295  (best train acc: 0.9099, best val acc: 0.9369, best train loss: 0.2499  @ epoch 14065 )\n",
      "[Epoch: 14300] train loss: 0.2713, train acc: 0.8978, val loss: 0.5732, val acc: 0.9258  (best train acc: 0.9099, best val acc: 0.9369, best train loss: 0.2499  @ epoch 14065 )\n",
      "[Epoch: 14320] train loss: 0.2589, train acc: 0.9070, val loss: 0.5958, val acc: 0.9275  (best train acc: 0.9099, best val acc: 0.9369, best train loss: 0.2499  @ epoch 14065 )\n",
      "[Epoch: 14340] train loss: 0.2742, train acc: 0.8937, val loss: 0.6174, val acc: 0.9056  (best train acc: 0.9099, best val acc: 0.9373, best train loss: 0.2499  @ epoch 14065 )\n",
      "[Epoch: 14360] train loss: 0.2818, train acc: 0.8982, val loss: 0.5989, val acc: 0.9258  (best train acc: 0.9099, best val acc: 0.9373, best train loss: 0.2499  @ epoch 14065 )\n",
      "[Epoch: 14380] train loss: 0.2605, train acc: 0.9060, val loss: 0.5542, val acc: 0.9302  (best train acc: 0.9099, best val acc: 0.9373, best train loss: 0.2499  @ epoch 14065 )\n",
      "[Epoch: 14400] train loss: 0.2825, train acc: 0.8925, val loss: 0.5841, val acc: 0.9322  (best train acc: 0.9099, best val acc: 0.9373, best train loss: 0.2499  @ epoch 14065 )\n",
      "[Epoch: 14420] train loss: 0.2549, train acc: 0.9075, val loss: 0.5729, val acc: 0.9312  (best train acc: 0.9130, best val acc: 0.9386, best train loss: 0.2464  @ epoch 14408 )\n",
      "[Epoch: 14440] train loss: 0.3034, train acc: 0.8858, val loss: 0.6019, val acc: 0.9312  (best train acc: 0.9130, best val acc: 0.9386, best train loss: 0.2464  @ epoch 14408 )\n",
      "[Epoch: 14460] train loss: 0.2748, train acc: 0.8997, val loss: 0.6253, val acc: 0.9201  (best train acc: 0.9130, best val acc: 0.9386, best train loss: 0.2464  @ epoch 14408 )\n",
      "[Epoch: 14480] train loss: 0.2906, train acc: 0.8967, val loss: 0.6139, val acc: 0.9322  (best train acc: 0.9130, best val acc: 0.9386, best train loss: 0.2464  @ epoch 14408 )\n",
      "[Epoch: 14500] train loss: 0.2775, train acc: 0.8938, val loss: 0.6079, val acc: 0.9309  (best train acc: 0.9130, best val acc: 0.9386, best train loss: 0.2464  @ epoch 14408 )\n",
      "[Epoch: 14520] train loss: 0.2521, train acc: 0.9046, val loss: 0.6079, val acc: 0.9322  (best train acc: 0.9130, best val acc: 0.9393, best train loss: 0.2464  @ epoch 14408 )\n",
      "[Epoch: 14540] train loss: 0.2673, train acc: 0.9064, val loss: 0.5872, val acc: 0.9339  (best train acc: 0.9130, best val acc: 0.9393, best train loss: 0.2464  @ epoch 14408 )\n",
      "[Epoch: 14560] train loss: 0.2828, train acc: 0.9023, val loss: 0.5889, val acc: 0.9325  (best train acc: 0.9130, best val acc: 0.9393, best train loss: 0.2464  @ epoch 14408 )\n",
      "[Epoch: 14580] train loss: 0.2526, train acc: 0.9089, val loss: 0.5954, val acc: 0.9322  (best train acc: 0.9130, best val acc: 0.9393, best train loss: 0.2464  @ epoch 14408 )\n",
      "[Epoch: 14600] train loss: 0.2522, train acc: 0.9114, val loss: 0.5760, val acc: 0.9298  (best train acc: 0.9130, best val acc: 0.9393, best train loss: 0.2464  @ epoch 14408 )\n",
      "[Epoch: 14620] train loss: 0.2631, train acc: 0.9007, val loss: 0.5964, val acc: 0.9295  (best train acc: 0.9130, best val acc: 0.9393, best train loss: 0.2464  @ epoch 14408 )\n",
      "[Epoch: 14640] train loss: 0.2526, train acc: 0.9089, val loss: 0.5831, val acc: 0.9332  (best train acc: 0.9130, best val acc: 0.9393, best train loss: 0.2464  @ epoch 14408 )\n",
      "[Epoch: 14660] train loss: 0.2676, train acc: 0.8968, val loss: 0.6129, val acc: 0.9265  (best train acc: 0.9130, best val acc: 0.9393, best train loss: 0.2464  @ epoch 14408 )\n",
      "[Epoch: 14680] train loss: 0.2864, train acc: 0.8944, val loss: 0.6138, val acc: 0.9312  (best train acc: 0.9130, best val acc: 0.9393, best train loss: 0.2464  @ epoch 14408 )\n",
      "[Epoch: 14700] train loss: 0.2584, train acc: 0.9076, val loss: 0.5609, val acc: 0.9359  (best train acc: 0.9130, best val acc: 0.9400, best train loss: 0.2464  @ epoch 14408 )\n",
      "[Epoch: 14720] train loss: 0.2601, train acc: 0.9022, val loss: 0.5754, val acc: 0.9342  (best train acc: 0.9130, best val acc: 0.9400, best train loss: 0.2456  @ epoch 14710 )\n",
      "[Epoch: 14740] train loss: 0.2657, train acc: 0.9057, val loss: 0.5816, val acc: 0.9197  (best train acc: 0.9130, best val acc: 0.9400, best train loss: 0.2456  @ epoch 14710 )\n",
      "[Epoch: 14760] train loss: 0.2614, train acc: 0.9034, val loss: 0.5878, val acc: 0.9352  (best train acc: 0.9130, best val acc: 0.9400, best train loss: 0.2456  @ epoch 14710 )\n",
      "[Epoch: 14780] train loss: 0.2633, train acc: 0.9023, val loss: 0.5916, val acc: 0.9369  (best train acc: 0.9130, best val acc: 0.9400, best train loss: 0.2437  @ epoch 14776 )\n",
      "[Epoch: 14800] train loss: 0.2696, train acc: 0.9053, val loss: 0.6262, val acc: 0.9170  (best train acc: 0.9130, best val acc: 0.9400, best train loss: 0.2437  @ epoch 14776 )\n",
      "[Epoch: 14820] train loss: 0.2651, train acc: 0.9012, val loss: 0.5767, val acc: 0.9332  (best train acc: 0.9130, best val acc: 0.9400, best train loss: 0.2437  @ epoch 14776 )\n",
      "[Epoch: 14840] train loss: 0.2906, train acc: 0.8962, val loss: 0.6478, val acc: 0.9298  (best train acc: 0.9130, best val acc: 0.9400, best train loss: 0.2437  @ epoch 14776 )\n",
      "[Epoch: 14860] train loss: 0.2559, train acc: 0.9064, val loss: 0.5922, val acc: 0.9339  (best train acc: 0.9130, best val acc: 0.9400, best train loss: 0.2437  @ epoch 14776 )\n",
      "[Epoch: 14880] train loss: 0.2880, train acc: 0.8904, val loss: 0.5543, val acc: 0.9363  (best train acc: 0.9130, best val acc: 0.9400, best train loss: 0.2437  @ epoch 14776 )\n",
      "[Epoch: 14900] train loss: 0.2758, train acc: 0.8984, val loss: 0.5794, val acc: 0.9285  (best train acc: 0.9130, best val acc: 0.9400, best train loss: 0.2437  @ epoch 14776 )\n",
      "[Epoch: 14920] train loss: 0.2612, train acc: 0.9049, val loss: 0.5843, val acc: 0.9339  (best train acc: 0.9130, best val acc: 0.9400, best train loss: 0.2437  @ epoch 14776 )\n",
      "[Epoch: 14940] train loss: 0.2575, train acc: 0.9053, val loss: 0.6767, val acc: 0.9369  (best train acc: 0.9130, best val acc: 0.9400, best train loss: 0.2437  @ epoch 14776 )\n",
      "[Epoch: 14960] train loss: 0.2522, train acc: 0.9115, val loss: 0.5808, val acc: 0.9339  (best train acc: 0.9130, best val acc: 0.9400, best train loss: 0.2437  @ epoch 14776 )\n",
      "[Epoch: 14980] train loss: 0.2544, train acc: 0.9087, val loss: 0.5845, val acc: 0.9386  (best train acc: 0.9130, best val acc: 0.9400, best train loss: 0.2437  @ epoch 14776 )\n",
      "[Epoch: 15000] train loss: 0.2642, train acc: 0.9054, val loss: 0.5835, val acc: 0.9390  (best train acc: 0.9158, best val acc: 0.9400, best train loss: 0.2399  @ epoch 14999 )\n",
      "[Epoch: 15020] train loss: 0.2638, train acc: 0.9053, val loss: 0.5873, val acc: 0.9363  (best train acc: 0.9158, best val acc: 0.9400, best train loss: 0.2399  @ epoch 14999 )\n",
      "[Epoch: 15040] train loss: 0.2803, train acc: 0.8950, val loss: 0.5609, val acc: 0.9315  (best train acc: 0.9158, best val acc: 0.9400, best train loss: 0.2399  @ epoch 14999 )\n",
      "[Epoch: 15060] train loss: 0.2691, train acc: 0.8989, val loss: 0.5079, val acc: 0.9356  (best train acc: 0.9158, best val acc: 0.9400, best train loss: 0.2399  @ epoch 14999 )\n",
      "[Epoch: 15080] train loss: 0.2577, train acc: 0.9035, val loss: 0.5726, val acc: 0.9352  (best train acc: 0.9158, best val acc: 0.9400, best train loss: 0.2399  @ epoch 14999 )\n",
      "[Epoch: 15100] train loss: 0.2623, train acc: 0.9000, val loss: 0.5735, val acc: 0.9305  (best train acc: 0.9158, best val acc: 0.9400, best train loss: 0.2399  @ epoch 14999 )\n",
      "[Epoch: 15120] train loss: 0.2835, train acc: 0.9035, val loss: 0.5760, val acc: 0.9285  (best train acc: 0.9158, best val acc: 0.9400, best train loss: 0.2399  @ epoch 14999 )\n",
      "[Epoch: 15140] train loss: 0.2446, train acc: 0.9150, val loss: 0.5826, val acc: 0.9339  (best train acc: 0.9158, best val acc: 0.9400, best train loss: 0.2399  @ epoch 14999 )\n",
      "[Epoch: 15160] train loss: 0.2307, train acc: 0.9184, val loss: 0.5880, val acc: 0.9325  (best train acc: 0.9203, best val acc: 0.9400, best train loss: 0.2281  @ epoch 15150 )\n",
      "[Epoch: 15180] train loss: 0.2434, train acc: 0.9162, val loss: 0.5877, val acc: 0.9234  (best train acc: 0.9203, best val acc: 0.9400, best train loss: 0.2281  @ epoch 15150 )\n",
      "[Epoch: 15200] train loss: 0.2392, train acc: 0.9152, val loss: 0.5508, val acc: 0.9386  (best train acc: 0.9203, best val acc: 0.9403, best train loss: 0.2281  @ epoch 15150 )\n",
      "[Epoch: 15220] train loss: 0.2450, train acc: 0.9130, val loss: 0.5617, val acc: 0.9349  (best train acc: 0.9224, best val acc: 0.9403, best train loss: 0.2273  @ epoch 15217 )\n",
      "[Epoch: 15240] train loss: 0.2498, train acc: 0.9102, val loss: 0.5800, val acc: 0.9302  (best train acc: 0.9224, best val acc: 0.9403, best train loss: 0.2273  @ epoch 15217 )\n",
      "[Epoch: 15260] train loss: 0.2604, train acc: 0.9064, val loss: 0.5433, val acc: 0.9214  (best train acc: 0.9224, best val acc: 0.9403, best train loss: 0.2273  @ epoch 15217 )\n",
      "[Epoch: 15280] train loss: 0.2456, train acc: 0.9124, val loss: 0.5113, val acc: 0.9379  (best train acc: 0.9224, best val acc: 0.9403, best train loss: 0.2226  @ epoch 15277 )\n",
      "[Epoch: 15300] train loss: 0.2375, train acc: 0.9177, val loss: 0.5330, val acc: 0.9393  (best train acc: 0.9224, best val acc: 0.9403, best train loss: 0.2226  @ epoch 15277 )\n",
      "[Epoch: 15320] train loss: 0.2645, train acc: 0.9045, val loss: 0.4821, val acc: 0.9369  (best train acc: 0.9224, best val acc: 0.9403, best train loss: 0.2226  @ epoch 15277 )\n",
      "[Epoch: 15340] train loss: 0.2432, train acc: 0.9144, val loss: 0.5503, val acc: 0.9342  (best train acc: 0.9224, best val acc: 0.9403, best train loss: 0.2226  @ epoch 15277 )\n",
      "[Epoch: 15360] train loss: 0.2465, train acc: 0.9105, val loss: 0.5162, val acc: 0.9356  (best train acc: 0.9224, best val acc: 0.9403, best train loss: 0.2226  @ epoch 15277 )\n",
      "[Epoch: 15380] train loss: 0.2445, train acc: 0.9107, val loss: 0.5412, val acc: 0.9393  (best train acc: 0.9224, best val acc: 0.9403, best train loss: 0.2226  @ epoch 15277 )\n",
      "[Epoch: 15400] train loss: 0.2419, train acc: 0.9099, val loss: 0.5392, val acc: 0.9393  (best train acc: 0.9224, best val acc: 0.9420, best train loss: 0.2203  @ epoch 15386 )\n",
      "[Epoch: 15420] train loss: 0.2403, train acc: 0.9148, val loss: 0.4920, val acc: 0.9356  (best train acc: 0.9224, best val acc: 0.9420, best train loss: 0.2203  @ epoch 15386 )\n",
      "[Epoch: 15440] train loss: 0.2627, train acc: 0.9026, val loss: 0.5974, val acc: 0.9228  (best train acc: 0.9224, best val acc: 0.9420, best train loss: 0.2203  @ epoch 15386 )\n",
      "[Epoch: 15460] train loss: 0.2437, train acc: 0.9145, val loss: 0.5551, val acc: 0.9369  (best train acc: 0.9224, best val acc: 0.9420, best train loss: 0.2203  @ epoch 15386 )\n",
      "[Epoch: 15480] train loss: 0.2268, train acc: 0.9201, val loss: 0.5774, val acc: 0.9386  (best train acc: 0.9224, best val acc: 0.9420, best train loss: 0.2203  @ epoch 15386 )\n",
      "[Epoch: 15500] train loss: 0.2240, train acc: 0.9198, val loss: 0.5436, val acc: 0.9336  (best train acc: 0.9224, best val acc: 0.9420, best train loss: 0.2203  @ epoch 15386 )\n",
      "[Epoch: 15520] train loss: 0.2349, train acc: 0.9168, val loss: 0.5274, val acc: 0.9390  (best train acc: 0.9224, best val acc: 0.9420, best train loss: 0.2194  @ epoch 15518 )\n",
      "[Epoch: 15540] train loss: 0.2293, train acc: 0.9184, val loss: 0.5344, val acc: 0.9393  (best train acc: 0.9224, best val acc: 0.9420, best train loss: 0.2194  @ epoch 15518 )\n",
      "[Epoch: 15560] train loss: 0.2398, train acc: 0.9127, val loss: 0.5431, val acc: 0.9366  (best train acc: 0.9224, best val acc: 0.9420, best train loss: 0.2194  @ epoch 15518 )\n",
      "[Epoch: 15580] train loss: 0.2648, train acc: 0.9044, val loss: 0.6277, val acc: 0.9329  (best train acc: 0.9224, best val acc: 0.9420, best train loss: 0.2194  @ epoch 15518 )\n",
      "[Epoch: 15600] train loss: 0.2302, train acc: 0.9182, val loss: 0.5536, val acc: 0.9393  (best train acc: 0.9224, best val acc: 0.9420, best train loss: 0.2194  @ epoch 15518 )\n",
      "[Epoch: 15620] train loss: 0.2343, train acc: 0.9170, val loss: 0.5928, val acc: 0.9386  (best train acc: 0.9224, best val acc: 0.9420, best train loss: 0.2194  @ epoch 15518 )\n",
      "[Epoch: 15640] train loss: 0.2375, train acc: 0.9132, val loss: 0.5125, val acc: 0.9346  (best train acc: 0.9224, best val acc: 0.9430, best train loss: 0.2194  @ epoch 15518 )\n",
      "[Epoch: 15660] train loss: 0.2317, train acc: 0.9194, val loss: 0.5266, val acc: 0.9390  (best train acc: 0.9224, best val acc: 0.9430, best train loss: 0.2194  @ epoch 15518 )\n",
      "[Epoch: 15680] train loss: 0.2416, train acc: 0.9161, val loss: 0.5335, val acc: 0.9376  (best train acc: 0.9242, best val acc: 0.9430, best train loss: 0.2192  @ epoch 15675 )\n",
      "[Epoch: 15700] train loss: 0.2405, train acc: 0.9117, val loss: 0.5118, val acc: 0.9302  (best train acc: 0.9242, best val acc: 0.9430, best train loss: 0.2179  @ epoch 15682 )\n",
      "[Epoch: 15720] train loss: 0.2366, train acc: 0.9171, val loss: 0.4686, val acc: 0.9420  (best train acc: 0.9242, best val acc: 0.9430, best train loss: 0.2179  @ epoch 15682 )\n",
      "[Epoch: 15740] train loss: 0.2359, train acc: 0.9148, val loss: 0.5712, val acc: 0.9403  (best train acc: 0.9242, best val acc: 0.9430, best train loss: 0.2179  @ epoch 15682 )\n",
      "[Epoch: 15760] train loss: 0.2211, train acc: 0.9216, val loss: 0.6028, val acc: 0.9383  (best train acc: 0.9247, best val acc: 0.9430, best train loss: 0.2176  @ epoch 15759 )\n",
      "[Epoch: 15780] train loss: 0.2273, train acc: 0.9182, val loss: 0.5524, val acc: 0.9413  (best train acc: 0.9247, best val acc: 0.9430, best train loss: 0.2176  @ epoch 15759 )\n",
      "[Epoch: 15800] train loss: 0.2376, train acc: 0.9148, val loss: 0.5661, val acc: 0.9278  (best train acc: 0.9247, best val acc: 0.9430, best train loss: 0.2176  @ epoch 15759 )\n",
      "[Epoch: 15820] train loss: 0.2325, train acc: 0.9192, val loss: 0.5325, val acc: 0.9400  (best train acc: 0.9247, best val acc: 0.9430, best train loss: 0.2176  @ epoch 15759 )\n",
      "[Epoch: 15840] train loss: 0.2207, train acc: 0.9205, val loss: 0.5817, val acc: 0.9396  (best train acc: 0.9247, best val acc: 0.9430, best train loss: 0.2176  @ epoch 15759 )\n",
      "[Epoch: 15860] train loss: 0.2411, train acc: 0.9182, val loss: 0.5697, val acc: 0.9363  (best train acc: 0.9247, best val acc: 0.9430, best train loss: 0.2163  @ epoch 15856 )\n",
      "[Epoch: 15880] train loss: 0.2243, train acc: 0.9204, val loss: 0.5322, val acc: 0.9373  (best train acc: 0.9247, best val acc: 0.9430, best train loss: 0.2163  @ epoch 15856 )\n",
      "[Epoch: 15900] train loss: 0.2255, train acc: 0.9217, val loss: 0.5511, val acc: 0.9430  (best train acc: 0.9247, best val acc: 0.9430, best train loss: 0.2163  @ epoch 15856 )\n",
      "[Epoch: 15920] train loss: 0.2372, train acc: 0.9113, val loss: 0.5342, val acc: 0.9366  (best train acc: 0.9247, best val acc: 0.9430, best train loss: 0.2163  @ epoch 15856 )\n",
      "[Epoch: 15940] train loss: 0.2402, train acc: 0.9161, val loss: 0.5414, val acc: 0.9386  (best train acc: 0.9247, best val acc: 0.9430, best train loss: 0.2163  @ epoch 15856 )\n",
      "[Epoch: 15960] train loss: 0.2392, train acc: 0.9124, val loss: 0.4972, val acc: 0.9403  (best train acc: 0.9247, best val acc: 0.9440, best train loss: 0.2163  @ epoch 15856 )\n",
      "[Epoch: 15980] train loss: 0.2368, train acc: 0.9167, val loss: 0.5479, val acc: 0.9413  (best train acc: 0.9247, best val acc: 0.9440, best train loss: 0.2163  @ epoch 15856 )\n",
      "[Epoch: 16000] train loss: 0.2361, train acc: 0.9152, val loss: 0.5371, val acc: 0.9349  (best train acc: 0.9247, best val acc: 0.9440, best train loss: 0.2163  @ epoch 15856 )\n",
      "[Epoch: 16020] train loss: 0.2439, train acc: 0.9124, val loss: 0.5253, val acc: 0.9396  (best train acc: 0.9247, best val acc: 0.9457, best train loss: 0.2163  @ epoch 15856 )\n",
      "[Epoch: 16040] train loss: 0.2270, train acc: 0.9166, val loss: 0.5128, val acc: 0.9393  (best train acc: 0.9247, best val acc: 0.9457, best train loss: 0.2163  @ epoch 15856 )\n",
      "[Epoch: 16060] train loss: 0.2299, train acc: 0.9182, val loss: 0.5241, val acc: 0.9400  (best train acc: 0.9247, best val acc: 0.9457, best train loss: 0.2163  @ epoch 15856 )\n",
      "[Epoch: 16080] train loss: 0.2270, train acc: 0.9169, val loss: 0.5331, val acc: 0.9437  (best train acc: 0.9247, best val acc: 0.9457, best train loss: 0.2163  @ epoch 15856 )\n",
      "[Epoch: 16100] train loss: 0.2377, train acc: 0.9146, val loss: 0.5521, val acc: 0.9376  (best train acc: 0.9247, best val acc: 0.9457, best train loss: 0.2163  @ epoch 15856 )\n",
      "[Epoch: 16120] train loss: 0.2372, train acc: 0.9130, val loss: 0.5312, val acc: 0.9376  (best train acc: 0.9247, best val acc: 0.9457, best train loss: 0.2163  @ epoch 15856 )\n",
      "[Epoch: 16140] train loss: 0.2364, train acc: 0.9158, val loss: 0.5264, val acc: 0.9339  (best train acc: 0.9247, best val acc: 0.9457, best train loss: 0.2163  @ epoch 15856 )\n",
      "[Epoch: 16160] train loss: 0.2662, train acc: 0.9012, val loss: 0.5525, val acc: 0.9379  (best train acc: 0.9247, best val acc: 0.9457, best train loss: 0.2163  @ epoch 15856 )\n",
      "[Epoch: 16180] train loss: 0.2302, train acc: 0.9191, val loss: 0.5339, val acc: 0.9400  (best train acc: 0.9247, best val acc: 0.9457, best train loss: 0.2163  @ epoch 15856 )\n",
      "[Epoch: 16200] train loss: 0.2322, train acc: 0.9174, val loss: 0.5884, val acc: 0.9255  (best train acc: 0.9247, best val acc: 0.9457, best train loss: 0.2163  @ epoch 15856 )\n",
      "[Epoch: 16220] train loss: 0.2334, train acc: 0.9166, val loss: 0.5678, val acc: 0.9403  (best train acc: 0.9247, best val acc: 0.9457, best train loss: 0.2163  @ epoch 15856 )\n",
      "[Epoch: 16240] train loss: 0.2334, train acc: 0.9180, val loss: 0.5184, val acc: 0.9423  (best train acc: 0.9247, best val acc: 0.9457, best train loss: 0.2163  @ epoch 15856 )\n",
      "[Epoch: 16260] train loss: 0.2381, train acc: 0.9172, val loss: 0.4815, val acc: 0.9356  (best train acc: 0.9247, best val acc: 0.9457, best train loss: 0.2163  @ epoch 15856 )\n",
      "[Epoch: 16280] train loss: 0.2271, train acc: 0.9179, val loss: 0.5391, val acc: 0.9390  (best train acc: 0.9247, best val acc: 0.9457, best train loss: 0.2163  @ epoch 15856 )\n",
      "[Epoch: 16300] train loss: 0.2917, train acc: 0.8850, val loss: 0.5761, val acc: 0.9292  (best train acc: 0.9247, best val acc: 0.9457, best train loss: 0.2163  @ epoch 15856 )\n",
      "[Epoch: 16320] train loss: 0.2600, train acc: 0.9039, val loss: 0.5291, val acc: 0.9376  (best train acc: 0.9247, best val acc: 0.9457, best train loss: 0.2163  @ epoch 15856 )\n",
      "[Epoch: 16340] train loss: 0.2410, train acc: 0.9137, val loss: 0.5760, val acc: 0.9393  (best train acc: 0.9247, best val acc: 0.9457, best train loss: 0.2163  @ epoch 15856 )\n",
      "[Epoch: 16360] train loss: 0.2246, train acc: 0.9216, val loss: 0.5680, val acc: 0.9376  (best train acc: 0.9247, best val acc: 0.9457, best train loss: 0.2163  @ epoch 15856 )\n",
      "[Epoch: 16380] train loss: 0.2350, train acc: 0.9145, val loss: 0.5443, val acc: 0.9373  (best train acc: 0.9247, best val acc: 0.9457, best train loss: 0.2160  @ epoch 16363 )\n",
      "[Epoch: 16400] train loss: 0.2219, train acc: 0.9219, val loss: 0.5379, val acc: 0.9332  (best train acc: 0.9247, best val acc: 0.9457, best train loss: 0.2160  @ epoch 16363 )\n",
      "[Epoch: 16420] train loss: 0.2241, train acc: 0.9193, val loss: 0.5419, val acc: 0.9406  (best train acc: 0.9247, best val acc: 0.9457, best train loss: 0.2160  @ epoch 16363 )\n",
      "[Epoch: 16440] train loss: 0.2297, train acc: 0.9190, val loss: 0.5687, val acc: 0.9403  (best train acc: 0.9259, best val acc: 0.9457, best train loss: 0.2160  @ epoch 16363 )\n",
      "[Epoch: 16460] train loss: 0.2237, train acc: 0.9207, val loss: 0.6109, val acc: 0.9430  (best train acc: 0.9259, best val acc: 0.9457, best train loss: 0.2136  @ epoch 16441 )\n",
      "[Epoch: 16480] train loss: 0.2110, train acc: 0.9256, val loss: 0.6017, val acc: 0.9369  (best train acc: 0.9259, best val acc: 0.9457, best train loss: 0.2110  @ epoch 16480 )\n",
      "[Epoch: 16500] train loss: 0.2219, train acc: 0.9220, val loss: 0.5374, val acc: 0.9396  (best train acc: 0.9259, best val acc: 0.9457, best train loss: 0.2110  @ epoch 16480 )\n",
      "[Epoch: 16520] train loss: 0.2244, train acc: 0.9211, val loss: 0.5399, val acc: 0.9417  (best train acc: 0.9259, best val acc: 0.9457, best train loss: 0.2110  @ epoch 16480 )\n",
      "[Epoch: 16540] train loss: 0.2386, train acc: 0.9140, val loss: 0.5030, val acc: 0.9427  (best train acc: 0.9259, best val acc: 0.9457, best train loss: 0.2110  @ epoch 16480 )\n",
      "[Epoch: 16560] train loss: 0.2489, train acc: 0.9086, val loss: 0.5097, val acc: 0.9437  (best train acc: 0.9259, best val acc: 0.9457, best train loss: 0.2110  @ epoch 16480 )\n",
      "[Epoch: 16580] train loss: 0.2283, train acc: 0.9162, val loss: 0.5821, val acc: 0.9403  (best train acc: 0.9259, best val acc: 0.9457, best train loss: 0.2110  @ epoch 16480 )\n",
      "[Epoch: 16600] train loss: 0.2356, train acc: 0.9165, val loss: 0.5490, val acc: 0.9430  (best train acc: 0.9259, best val acc: 0.9457, best train loss: 0.2105  @ epoch 16593 )\n",
      "[Epoch: 16620] train loss: 0.2311, train acc: 0.9173, val loss: 0.5407, val acc: 0.9302  (best train acc: 0.9259, best val acc: 0.9457, best train loss: 0.2105  @ epoch 16593 )\n",
      "[Epoch: 16640] train loss: 0.2388, train acc: 0.9151, val loss: 0.5462, val acc: 0.9420  (best train acc: 0.9259, best val acc: 0.9457, best train loss: 0.2105  @ epoch 16593 )\n",
      "[Epoch: 16660] train loss: 0.2257, train acc: 0.9194, val loss: 0.5696, val acc: 0.9437  (best train acc: 0.9259, best val acc: 0.9457, best train loss: 0.2105  @ epoch 16593 )\n",
      "[Epoch: 16680] train loss: 0.2212, train acc: 0.9200, val loss: 0.5596, val acc: 0.9427  (best train acc: 0.9259, best val acc: 0.9464, best train loss: 0.2105  @ epoch 16593 )\n",
      "[Epoch: 16700] train loss: 0.2323, train acc: 0.9177, val loss: 0.5500, val acc: 0.9400  (best train acc: 0.9259, best val acc: 0.9464, best train loss: 0.2105  @ epoch 16593 )\n",
      "[Epoch: 16720] train loss: 0.2533, train acc: 0.9107, val loss: 0.5533, val acc: 0.9164  (best train acc: 0.9259, best val acc: 0.9464, best train loss: 0.2105  @ epoch 16593 )\n",
      "[Epoch: 16740] train loss: 0.2344, train acc: 0.9150, val loss: 0.5705, val acc: 0.9423  (best train acc: 0.9259, best val acc: 0.9464, best train loss: 0.2105  @ epoch 16593 )\n",
      "[Epoch: 16760] train loss: 0.2472, train acc: 0.9065, val loss: 0.5731, val acc: 0.9366  (best train acc: 0.9259, best val acc: 0.9464, best train loss: 0.2105  @ epoch 16593 )\n",
      "[Epoch: 16780] train loss: 0.2304, train acc: 0.9165, val loss: 0.5562, val acc: 0.9396  (best train acc: 0.9259, best val acc: 0.9464, best train loss: 0.2105  @ epoch 16593 )\n",
      "[Epoch: 16800] train loss: 0.2204, train acc: 0.9242, val loss: 0.5459, val acc: 0.9420  (best train acc: 0.9259, best val acc: 0.9464, best train loss: 0.2105  @ epoch 16593 )\n",
      "[Epoch: 16820] train loss: 0.2302, train acc: 0.9174, val loss: 0.5652, val acc: 0.9417  (best train acc: 0.9259, best val acc: 0.9464, best train loss: 0.2105  @ epoch 16593 )\n",
      "[Epoch: 16840] train loss: 0.2446, train acc: 0.9116, val loss: 0.5438, val acc: 0.9363  (best train acc: 0.9259, best val acc: 0.9464, best train loss: 0.2105  @ epoch 16593 )\n",
      "[Epoch: 16860] train loss: 0.2288, train acc: 0.9184, val loss: 0.5496, val acc: 0.9423  (best train acc: 0.9259, best val acc: 0.9464, best train loss: 0.2105  @ epoch 16593 )\n",
      "[Epoch: 16880] train loss: 0.2265, train acc: 0.9183, val loss: 0.5522, val acc: 0.9386  (best train acc: 0.9259, best val acc: 0.9464, best train loss: 0.2105  @ epoch 16593 )\n",
      "[Epoch: 16900] train loss: 0.2364, train acc: 0.9174, val loss: 0.5413, val acc: 0.9406  (best train acc: 0.9259, best val acc: 0.9464, best train loss: 0.2105  @ epoch 16593 )\n",
      "[Epoch: 16920] train loss: 0.2189, train acc: 0.9208, val loss: 0.5669, val acc: 0.9376  (best train acc: 0.9259, best val acc: 0.9464, best train loss: 0.2105  @ epoch 16593 )\n",
      "[Epoch: 16940] train loss: 0.2436, train acc: 0.9164, val loss: 0.5657, val acc: 0.9379  (best train acc: 0.9259, best val acc: 0.9464, best train loss: 0.2105  @ epoch 16593 )\n",
      "[Epoch: 16960] train loss: 0.2500, train acc: 0.9049, val loss: 0.5571, val acc: 0.9417  (best train acc: 0.9259, best val acc: 0.9464, best train loss: 0.2105  @ epoch 16593 )\n",
      "[Epoch: 16980] train loss: 0.2345, train acc: 0.9166, val loss: 0.5322, val acc: 0.9427  (best train acc: 0.9259, best val acc: 0.9464, best train loss: 0.2105  @ epoch 16593 )\n",
      "[Epoch: 17000] train loss: 0.2245, train acc: 0.9201, val loss: 0.5789, val acc: 0.9437  (best train acc: 0.9263, best val acc: 0.9464, best train loss: 0.2105  @ epoch 16593 )\n",
      "[Epoch: 17020] train loss: 0.2278, train acc: 0.9184, val loss: 0.5420, val acc: 0.9393  (best train acc: 0.9263, best val acc: 0.9464, best train loss: 0.2105  @ epoch 16593 )\n",
      "[Epoch: 17040] train loss: 0.2417, train acc: 0.9111, val loss: 0.4750, val acc: 0.9420  (best train acc: 0.9263, best val acc: 0.9464, best train loss: 0.2105  @ epoch 16593 )\n",
      "[Epoch: 17060] train loss: 0.2328, train acc: 0.9140, val loss: 0.5303, val acc: 0.9379  (best train acc: 0.9263, best val acc: 0.9464, best train loss: 0.2105  @ epoch 16593 )\n",
      "[Epoch: 17080] train loss: 0.2289, train acc: 0.9160, val loss: 0.5652, val acc: 0.9430  (best train acc: 0.9263, best val acc: 0.9464, best train loss: 0.2076  @ epoch 17073 )\n",
      "[Epoch: 17100] train loss: 0.2257, train acc: 0.9201, val loss: 0.4943, val acc: 0.9383  (best train acc: 0.9263, best val acc: 0.9464, best train loss: 0.2076  @ epoch 17073 )\n",
      "[Epoch: 17120] train loss: 0.2497, train acc: 0.9109, val loss: 0.4763, val acc: 0.9417  (best train acc: 0.9263, best val acc: 0.9464, best train loss: 0.2076  @ epoch 17073 )\n",
      "[Epoch: 17140] train loss: 0.2389, train acc: 0.9134, val loss: 0.4999, val acc: 0.9437  (best train acc: 0.9263, best val acc: 0.9464, best train loss: 0.2076  @ epoch 17073 )\n",
      "[Epoch: 17160] train loss: 0.2179, train acc: 0.9228, val loss: 0.4306, val acc: 0.9433  (best train acc: 0.9263, best val acc: 0.9464, best train loss: 0.2076  @ epoch 17073 )\n",
      "[Epoch: 17180] train loss: 0.2209, train acc: 0.9213, val loss: 0.4637, val acc: 0.9423  (best train acc: 0.9263, best val acc: 0.9464, best train loss: 0.2076  @ epoch 17073 )\n",
      "[Epoch: 17200] train loss: 0.2440, train acc: 0.9117, val loss: 0.4827, val acc: 0.9339  (best train acc: 0.9263, best val acc: 0.9464, best train loss: 0.2076  @ epoch 17073 )\n",
      "[Epoch: 17220] train loss: 0.2303, train acc: 0.9172, val loss: 0.4687, val acc: 0.9433  (best train acc: 0.9263, best val acc: 0.9464, best train loss: 0.2076  @ epoch 17073 )\n",
      "[Epoch: 17240] train loss: 0.2218, train acc: 0.9209, val loss: 0.4840, val acc: 0.9447  (best train acc: 0.9263, best val acc: 0.9464, best train loss: 0.2076  @ epoch 17073 )\n",
      "[Epoch: 17260] train loss: 0.2222, train acc: 0.9212, val loss: 0.4876, val acc: 0.9410  (best train acc: 0.9263, best val acc: 0.9464, best train loss: 0.2076  @ epoch 17073 )\n",
      "[Epoch: 17280] train loss: 0.2351, train acc: 0.9190, val loss: 0.4925, val acc: 0.9433  (best train acc: 0.9263, best val acc: 0.9464, best train loss: 0.2076  @ epoch 17073 )\n",
      "[Epoch: 17300] train loss: 0.2327, train acc: 0.9184, val loss: 0.5367, val acc: 0.9410  (best train acc: 0.9263, best val acc: 0.9467, best train loss: 0.2076  @ epoch 17073 )\n",
      "[Epoch: 17320] train loss: 0.2375, train acc: 0.9148, val loss: 0.5203, val acc: 0.9376  (best train acc: 0.9263, best val acc: 0.9467, best train loss: 0.2076  @ epoch 17073 )\n",
      "[Epoch: 17340] train loss: 0.2415, train acc: 0.9117, val loss: 0.5071, val acc: 0.9406  (best train acc: 0.9263, best val acc: 0.9467, best train loss: 0.2076  @ epoch 17073 )\n",
      "[Epoch: 17360] train loss: 0.2331, train acc: 0.9150, val loss: 0.5390, val acc: 0.9396  (best train acc: 0.9263, best val acc: 0.9470, best train loss: 0.2076  @ epoch 17073 )\n",
      "[Epoch: 17380] train loss: 0.2264, train acc: 0.9192, val loss: 0.5362, val acc: 0.9420  (best train acc: 0.9263, best val acc: 0.9470, best train loss: 0.2076  @ epoch 17073 )\n",
      "[Epoch: 17400] train loss: 0.2290, train acc: 0.9191, val loss: 0.5094, val acc: 0.9413  (best train acc: 0.9263, best val acc: 0.9470, best train loss: 0.2076  @ epoch 17073 )\n",
      "[Epoch: 17420] train loss: 0.2370, train acc: 0.9145, val loss: 0.4922, val acc: 0.9410  (best train acc: 0.9263, best val acc: 0.9470, best train loss: 0.2076  @ epoch 17073 )\n",
      "[Epoch: 17440] train loss: 0.2342, train acc: 0.9164, val loss: 0.5181, val acc: 0.9410  (best train acc: 0.9263, best val acc: 0.9470, best train loss: 0.2076  @ epoch 17073 )\n",
      "[Epoch: 17460] train loss: 0.2245, train acc: 0.9174, val loss: 0.5433, val acc: 0.9379  (best train acc: 0.9263, best val acc: 0.9470, best train loss: 0.2076  @ epoch 17073 )\n",
      "[Epoch: 17480] train loss: 0.2540, train acc: 0.9064, val loss: 0.5629, val acc: 0.9366  (best train acc: 0.9263, best val acc: 0.9470, best train loss: 0.2076  @ epoch 17073 )\n",
      "[Epoch: 17500] train loss: 0.2413, train acc: 0.9130, val loss: 0.5535, val acc: 0.9433  (best train acc: 0.9263, best val acc: 0.9470, best train loss: 0.2076  @ epoch 17073 )\n",
      "[Epoch: 17520] train loss: 0.2238, train acc: 0.9200, val loss: 0.5656, val acc: 0.9427  (best train acc: 0.9263, best val acc: 0.9470, best train loss: 0.2076  @ epoch 17073 )\n",
      "[Epoch: 17540] train loss: 0.2394, train acc: 0.9136, val loss: 0.5213, val acc: 0.9420  (best train acc: 0.9263, best val acc: 0.9470, best train loss: 0.2076  @ epoch 17073 )\n",
      "[Epoch: 17560] train loss: 0.2196, train acc: 0.9249, val loss: 0.5549, val acc: 0.9400  (best train acc: 0.9263, best val acc: 0.9470, best train loss: 0.2076  @ epoch 17073 )\n",
      "[Epoch: 17580] train loss: 0.2218, train acc: 0.9201, val loss: 0.5501, val acc: 0.9427  (best train acc: 0.9263, best val acc: 0.9470, best train loss: 0.2076  @ epoch 17073 )\n",
      "[Epoch: 17600] train loss: 0.2190, train acc: 0.9255, val loss: 0.5579, val acc: 0.9450  (best train acc: 0.9263, best val acc: 0.9470, best train loss: 0.2076  @ epoch 17073 )\n",
      "[Epoch: 17620] train loss: 0.2390, train acc: 0.9151, val loss: 0.5647, val acc: 0.9251  (best train acc: 0.9263, best val acc: 0.9470, best train loss: 0.2076  @ epoch 17073 )\n",
      "[Epoch: 17640] train loss: 0.2304, train acc: 0.9187, val loss: 0.5278, val acc: 0.9406  (best train acc: 0.9263, best val acc: 0.9470, best train loss: 0.2076  @ epoch 17073 )\n",
      "[Epoch: 17660] train loss: 0.2270, train acc: 0.9192, val loss: 0.5695, val acc: 0.9390  (best train acc: 0.9263, best val acc: 0.9470, best train loss: 0.2076  @ epoch 17073 )\n",
      "[Epoch: 17680] train loss: 0.2624, train acc: 0.9023, val loss: 0.5665, val acc: 0.9373  (best train acc: 0.9263, best val acc: 0.9470, best train loss: 0.2076  @ epoch 17073 )\n",
      "[Epoch: 17700] train loss: 0.2252, train acc: 0.9199, val loss: 0.5715, val acc: 0.9420  (best train acc: 0.9263, best val acc: 0.9470, best train loss: 0.2076  @ epoch 17073 )\n",
      "[Epoch: 17720] train loss: 0.2332, train acc: 0.9151, val loss: 0.5487, val acc: 0.9417  (best train acc: 0.9263, best val acc: 0.9470, best train loss: 0.2076  @ epoch 17073 )\n",
      "[Epoch: 17740] train loss: 0.2215, train acc: 0.9222, val loss: 0.5148, val acc: 0.9437  (best train acc: 0.9263, best val acc: 0.9470, best train loss: 0.2076  @ epoch 17073 )\n",
      "[Epoch: 17760] train loss: 0.2148, train acc: 0.9230, val loss: 0.5546, val acc: 0.9423  (best train acc: 0.9269, best val acc: 0.9470, best train loss: 0.2076  @ epoch 17073 )\n",
      "[Epoch: 17780] train loss: 0.2730, train acc: 0.8992, val loss: 0.5376, val acc: 0.9386  (best train acc: 0.9269, best val acc: 0.9470, best train loss: 0.2076  @ epoch 17073 )\n",
      "[Epoch: 17800] train loss: 0.2257, train acc: 0.9208, val loss: 0.5142, val acc: 0.9454  (best train acc: 0.9269, best val acc: 0.9470, best train loss: 0.2076  @ epoch 17073 )\n",
      "[Epoch: 17820] train loss: 0.2462, train acc: 0.9104, val loss: 0.5558, val acc: 0.9305  (best train acc: 0.9269, best val acc: 0.9470, best train loss: 0.2076  @ epoch 17073 )\n",
      "[Epoch: 17840] train loss: 0.2409, train acc: 0.9125, val loss: 0.5447, val acc: 0.9379  (best train acc: 0.9269, best val acc: 0.9470, best train loss: 0.2076  @ epoch 17073 )\n",
      "[Epoch: 17860] train loss: 0.2264, train acc: 0.9211, val loss: 0.5553, val acc: 0.9423  (best train acc: 0.9274, best val acc: 0.9470, best train loss: 0.2076  @ epoch 17073 )\n",
      "[Epoch: 17880] train loss: 0.2230, train acc: 0.9204, val loss: 0.5226, val acc: 0.9427  (best train acc: 0.9274, best val acc: 0.9470, best train loss: 0.2076  @ epoch 17073 )\n",
      "[Epoch: 17900] train loss: 0.2202, train acc: 0.9212, val loss: 0.5555, val acc: 0.9393  (best train acc: 0.9274, best val acc: 0.9470, best train loss: 0.2076  @ epoch 17073 )\n",
      "[Epoch: 17920] train loss: 0.2137, train acc: 0.9260, val loss: 0.6241, val acc: 0.9437  (best train acc: 0.9274, best val acc: 0.9470, best train loss: 0.2076  @ epoch 17073 )\n",
      "[Epoch: 17940] train loss: 0.2589, train acc: 0.9023, val loss: 0.5296, val acc: 0.9383  (best train acc: 0.9274, best val acc: 0.9470, best train loss: 0.2076  @ epoch 17073 )\n",
      "[Epoch: 17960] train loss: 0.2212, train acc: 0.9199, val loss: 0.5349, val acc: 0.9460  (best train acc: 0.9274, best val acc: 0.9484, best train loss: 0.2076  @ epoch 17073 )\n",
      "[Epoch: 17980] train loss: 0.2215, train acc: 0.9203, val loss: 0.5531, val acc: 0.9369  (best train acc: 0.9274, best val acc: 0.9484, best train loss: 0.2076  @ epoch 17073 )\n",
      "[Epoch: 18000] train loss: 0.2225, train acc: 0.9198, val loss: 0.5538, val acc: 0.9393  (best train acc: 0.9274, best val acc: 0.9484, best train loss: 0.2076  @ epoch 17073 )\n",
      "[Epoch: 18020] train loss: 0.2144, train acc: 0.9218, val loss: 0.5122, val acc: 0.9417  (best train acc: 0.9274, best val acc: 0.9484, best train loss: 0.2076  @ epoch 17073 )\n",
      "[Epoch: 18040] train loss: 0.2263, train acc: 0.9201, val loss: 0.5433, val acc: 0.9430  (best train acc: 0.9274, best val acc: 0.9484, best train loss: 0.2076  @ epoch 17073 )\n",
      "[Epoch: 18060] train loss: 0.2173, train acc: 0.9224, val loss: 0.5504, val acc: 0.9437  (best train acc: 0.9274, best val acc: 0.9484, best train loss: 0.2076  @ epoch 17073 )\n",
      "[Epoch: 18080] train loss: 0.2206, train acc: 0.9221, val loss: 0.5453, val acc: 0.9430  (best train acc: 0.9274, best val acc: 0.9484, best train loss: 0.2076  @ epoch 17073 )\n",
      "[Epoch: 18100] train loss: 0.2410, train acc: 0.9132, val loss: 0.5733, val acc: 0.9437  (best train acc: 0.9274, best val acc: 0.9484, best train loss: 0.2076  @ epoch 17073 )\n",
      "[Epoch: 18120] train loss: 0.2310, train acc: 0.9183, val loss: 0.5825, val acc: 0.9379  (best train acc: 0.9274, best val acc: 0.9484, best train loss: 0.2076  @ epoch 17073 )\n",
      "[Epoch: 18140] train loss: 0.2131, train acc: 0.9242, val loss: 0.5576, val acc: 0.9430  (best train acc: 0.9274, best val acc: 0.9484, best train loss: 0.2076  @ epoch 17073 )\n",
      "[Epoch: 18160] train loss: 0.2203, train acc: 0.9215, val loss: 0.5171, val acc: 0.9329  (best train acc: 0.9274, best val acc: 0.9484, best train loss: 0.2076  @ epoch 17073 )\n",
      "[Epoch: 18180] train loss: 0.2488, train acc: 0.9109, val loss: 0.5340, val acc: 0.9386  (best train acc: 0.9274, best val acc: 0.9484, best train loss: 0.2076  @ epoch 17073 )\n",
      "[Epoch: 18200] train loss: 0.2338, train acc: 0.9166, val loss: 0.5580, val acc: 0.9437  (best train acc: 0.9274, best val acc: 0.9484, best train loss: 0.2076  @ epoch 17073 )\n",
      "[Epoch: 18220] train loss: 0.2281, train acc: 0.9174, val loss: 0.5343, val acc: 0.9427  (best train acc: 0.9274, best val acc: 0.9484, best train loss: 0.2076  @ epoch 17073 )\n",
      "[Epoch: 18240] train loss: 0.2378, train acc: 0.9166, val loss: 0.5207, val acc: 0.9373  (best train acc: 0.9274, best val acc: 0.9484, best train loss: 0.2076  @ epoch 17073 )\n",
      "[Epoch: 18260] train loss: 0.2323, train acc: 0.9166, val loss: 0.5453, val acc: 0.9437  (best train acc: 0.9284, best val acc: 0.9484, best train loss: 0.2050  @ epoch 18246 )\n",
      "[Epoch: 18280] train loss: 0.2287, train acc: 0.9203, val loss: 0.5641, val acc: 0.9342  (best train acc: 0.9284, best val acc: 0.9484, best train loss: 0.2050  @ epoch 18246 )\n",
      "[Epoch: 18300] train loss: 0.2273, train acc: 0.9178, val loss: 0.5765, val acc: 0.9319  (best train acc: 0.9284, best val acc: 0.9484, best train loss: 0.2050  @ epoch 18246 )\n",
      "[Epoch: 18320] train loss: 0.2406, train acc: 0.9124, val loss: 0.5543, val acc: 0.9460  (best train acc: 0.9284, best val acc: 0.9484, best train loss: 0.2050  @ epoch 18246 )\n",
      "[Epoch: 18340] train loss: 0.2245, train acc: 0.9202, val loss: 0.5452, val acc: 0.9359  (best train acc: 0.9284, best val acc: 0.9484, best train loss: 0.2050  @ epoch 18246 )\n",
      "[Epoch: 18360] train loss: 0.2273, train acc: 0.9208, val loss: 0.5711, val acc: 0.9369  (best train acc: 0.9284, best val acc: 0.9484, best train loss: 0.2050  @ epoch 18246 )\n",
      "[Epoch: 18380] train loss: 0.2253, train acc: 0.9228, val loss: 0.5570, val acc: 0.9457  (best train acc: 0.9284, best val acc: 0.9484, best train loss: 0.2050  @ epoch 18246 )\n",
      "[Epoch: 18400] train loss: 0.2206, train acc: 0.9198, val loss: 0.5612, val acc: 0.9400  (best train acc: 0.9284, best val acc: 0.9484, best train loss: 0.2050  @ epoch 18246 )\n",
      "[Epoch: 18420] train loss: 0.2283, train acc: 0.9190, val loss: 0.5562, val acc: 0.9464  (best train acc: 0.9284, best val acc: 0.9484, best train loss: 0.2050  @ epoch 18246 )\n",
      "[Epoch: 18440] train loss: 0.2398, train acc: 0.9121, val loss: 0.5481, val acc: 0.9423  (best train acc: 0.9284, best val acc: 0.9484, best train loss: 0.2050  @ epoch 18246 )\n",
      "[Epoch: 18460] train loss: 0.2347, train acc: 0.9150, val loss: 0.4733, val acc: 0.9329  (best train acc: 0.9284, best val acc: 0.9484, best train loss: 0.2050  @ epoch 18246 )\n",
      "[Epoch: 18480] train loss: 0.2426, train acc: 0.9132, val loss: 0.5714, val acc: 0.9302  (best train acc: 0.9284, best val acc: 0.9484, best train loss: 0.2050  @ epoch 18246 )\n",
      "[Epoch: 18500] train loss: 0.2317, train acc: 0.9187, val loss: 0.5449, val acc: 0.9268  (best train acc: 0.9284, best val acc: 0.9484, best train loss: 0.2050  @ epoch 18246 )\n",
      "[Epoch: 18520] train loss: 0.2335, train acc: 0.9175, val loss: 0.5573, val acc: 0.9406  (best train acc: 0.9284, best val acc: 0.9484, best train loss: 0.2050  @ epoch 18246 )\n",
      "[Epoch: 18540] train loss: 0.2179, train acc: 0.9236, val loss: 0.5261, val acc: 0.9430  (best train acc: 0.9284, best val acc: 0.9484, best train loss: 0.2050  @ epoch 18246 )\n",
      "[Epoch: 18560] train loss: 0.2170, train acc: 0.9245, val loss: 0.5334, val acc: 0.9464  (best train acc: 0.9284, best val acc: 0.9484, best train loss: 0.2050  @ epoch 18246 )\n",
      "[Epoch: 18580] train loss: 0.2135, train acc: 0.9252, val loss: 0.5294, val acc: 0.9457  (best train acc: 0.9284, best val acc: 0.9484, best train loss: 0.2050  @ epoch 18246 )\n",
      "[Epoch: 18600] train loss: 0.2298, train acc: 0.9168, val loss: 0.5541, val acc: 0.9413  (best train acc: 0.9284, best val acc: 0.9484, best train loss: 0.2050  @ epoch 18246 )\n",
      "[Epoch: 18620] train loss: 0.2195, train acc: 0.9211, val loss: 0.5568, val acc: 0.9447  (best train acc: 0.9284, best val acc: 0.9484, best train loss: 0.2050  @ epoch 18246 )\n",
      "[Epoch: 18640] train loss: 0.2198, train acc: 0.9226, val loss: 0.5526, val acc: 0.9457  (best train acc: 0.9284, best val acc: 0.9484, best train loss: 0.2050  @ epoch 18246 )\n",
      "[Epoch: 18660] train loss: 0.2219, train acc: 0.9224, val loss: 0.5685, val acc: 0.9454  (best train acc: 0.9284, best val acc: 0.9484, best train loss: 0.2050  @ epoch 18246 )\n",
      "[Epoch: 18680] train loss: 0.2318, train acc: 0.9164, val loss: 0.5735, val acc: 0.9363  (best train acc: 0.9284, best val acc: 0.9484, best train loss: 0.2050  @ epoch 18246 )\n",
      "[Epoch: 18700] train loss: 0.2351, train acc: 0.9121, val loss: 0.4991, val acc: 0.9437  (best train acc: 0.9284, best val acc: 0.9484, best train loss: 0.2050  @ epoch 18246 )\n",
      "[Epoch: 18720] train loss: 0.2292, train acc: 0.9181, val loss: 0.5474, val acc: 0.9420  (best train acc: 0.9284, best val acc: 0.9484, best train loss: 0.2050  @ epoch 18246 )\n",
      "[Epoch: 18740] train loss: 0.2314, train acc: 0.9167, val loss: 0.5792, val acc: 0.9427  (best train acc: 0.9284, best val acc: 0.9484, best train loss: 0.2050  @ epoch 18246 )\n",
      "[Epoch: 18760] train loss: 0.2390, train acc: 0.9130, val loss: 0.5423, val acc: 0.9406  (best train acc: 0.9284, best val acc: 0.9484, best train loss: 0.2050  @ epoch 18246 )\n",
      "[Epoch: 18780] train loss: 0.2281, train acc: 0.9195, val loss: 0.5413, val acc: 0.9467  (best train acc: 0.9284, best val acc: 0.9484, best train loss: 0.2050  @ epoch 18246 )\n",
      "[Epoch: 18800] train loss: 0.2280, train acc: 0.9177, val loss: 0.5081, val acc: 0.9460  (best train acc: 0.9284, best val acc: 0.9484, best train loss: 0.2050  @ epoch 18246 )\n",
      "[Epoch: 18820] train loss: 0.2351, train acc: 0.9153, val loss: 0.5606, val acc: 0.9359  (best train acc: 0.9284, best val acc: 0.9484, best train loss: 0.2050  @ epoch 18246 )\n",
      "[Epoch: 18840] train loss: 0.2163, train acc: 0.9229, val loss: 0.5604, val acc: 0.9423  (best train acc: 0.9284, best val acc: 0.9484, best train loss: 0.2050  @ epoch 18246 )\n",
      "[Epoch: 18860] train loss: 0.2136, train acc: 0.9239, val loss: 0.5526, val acc: 0.9454  (best train acc: 0.9284, best val acc: 0.9484, best train loss: 0.2050  @ epoch 18246 )\n",
      "[Epoch: 18880] train loss: 0.2206, train acc: 0.9242, val loss: 0.5414, val acc: 0.9437  (best train acc: 0.9284, best val acc: 0.9484, best train loss: 0.2050  @ epoch 18246 )\n",
      "[Epoch: 18900] train loss: 0.2184, train acc: 0.9216, val loss: 0.5510, val acc: 0.9430  (best train acc: 0.9284, best val acc: 0.9484, best train loss: 0.2050  @ epoch 18246 )\n",
      "[Epoch: 18920] train loss: 0.2290, train acc: 0.9175, val loss: 0.5469, val acc: 0.9477  (best train acc: 0.9284, best val acc: 0.9494, best train loss: 0.2050  @ epoch 18246 )\n",
      "[Epoch: 18940] train loss: 0.2202, train acc: 0.9218, val loss: 0.5551, val acc: 0.9390  (best train acc: 0.9288, best val acc: 0.9494, best train loss: 0.2034  @ epoch 18925 )\n",
      "[Epoch: 18960] train loss: 0.2251, train acc: 0.9153, val loss: 0.5208, val acc: 0.9447  (best train acc: 0.9288, best val acc: 0.9494, best train loss: 0.2034  @ epoch 18925 )\n",
      "[Epoch: 18980] train loss: 0.2566, train acc: 0.9119, val loss: 0.6027, val acc: 0.9204  (best train acc: 0.9288, best val acc: 0.9494, best train loss: 0.2034  @ epoch 18925 )\n",
      "[Epoch: 19000] train loss: 0.2299, train acc: 0.9180, val loss: 0.6230, val acc: 0.9464  (best train acc: 0.9288, best val acc: 0.9494, best train loss: 0.2034  @ epoch 18925 )\n",
      "[Epoch: 19020] train loss: 0.2269, train acc: 0.9204, val loss: 0.5895, val acc: 0.9396  (best train acc: 0.9288, best val acc: 0.9494, best train loss: 0.2034  @ epoch 18925 )\n",
      "[Epoch: 19040] train loss: 0.2263, train acc: 0.9195, val loss: 0.5768, val acc: 0.9342  (best train acc: 0.9288, best val acc: 0.9494, best train loss: 0.2034  @ epoch 18925 )\n",
      "[Epoch: 19060] train loss: 0.2220, train acc: 0.9202, val loss: 0.5897, val acc: 0.9450  (best train acc: 0.9288, best val acc: 0.9494, best train loss: 0.2034  @ epoch 18925 )\n",
      "[Epoch: 19080] train loss: 0.2327, train acc: 0.9157, val loss: 0.5554, val acc: 0.9396  (best train acc: 0.9288, best val acc: 0.9494, best train loss: 0.2034  @ epoch 18925 )\n",
      "[Epoch: 19100] train loss: 0.2181, train acc: 0.9223, val loss: 0.5290, val acc: 0.9467  (best train acc: 0.9288, best val acc: 0.9494, best train loss: 0.2034  @ epoch 18925 )\n",
      "[Epoch: 19120] train loss: 0.2198, train acc: 0.9226, val loss: 0.5243, val acc: 0.9447  (best train acc: 0.9288, best val acc: 0.9494, best train loss: 0.2034  @ epoch 18925 )\n",
      "[Epoch: 19140] train loss: 0.2278, train acc: 0.9221, val loss: 0.5290, val acc: 0.9447  (best train acc: 0.9288, best val acc: 0.9494, best train loss: 0.2034  @ epoch 18925 )\n",
      "[Epoch: 19160] train loss: 0.2237, train acc: 0.9195, val loss: 0.5295, val acc: 0.9460  (best train acc: 0.9288, best val acc: 0.9494, best train loss: 0.2034  @ epoch 18925 )\n",
      "[Epoch: 19180] train loss: 0.2164, train acc: 0.9221, val loss: 0.5254, val acc: 0.9430  (best train acc: 0.9288, best val acc: 0.9494, best train loss: 0.2034  @ epoch 18925 )\n",
      "[Epoch: 19200] train loss: 0.2455, train acc: 0.9102, val loss: 0.5403, val acc: 0.9464  (best train acc: 0.9288, best val acc: 0.9494, best train loss: 0.2034  @ epoch 18925 )\n",
      "[Epoch: 19220] train loss: 0.2172, train acc: 0.9236, val loss: 0.5284, val acc: 0.9440  (best train acc: 0.9288, best val acc: 0.9494, best train loss: 0.2034  @ epoch 18925 )\n",
      "[Epoch: 19240] train loss: 0.2245, train acc: 0.9188, val loss: 0.5067, val acc: 0.9487  (best train acc: 0.9288, best val acc: 0.9494, best train loss: 0.2034  @ epoch 18925 )\n",
      "[Epoch: 19260] train loss: 0.2312, train acc: 0.9137, val loss: 0.5476, val acc: 0.9440  (best train acc: 0.9288, best val acc: 0.9494, best train loss: 0.2034  @ epoch 18925 )\n",
      "[Epoch: 19280] train loss: 0.2170, train acc: 0.9233, val loss: 0.5359, val acc: 0.9433  (best train acc: 0.9288, best val acc: 0.9504, best train loss: 0.2034  @ epoch 18925 )\n",
      "[Epoch: 19300] train loss: 0.2086, train acc: 0.9249, val loss: 0.5294, val acc: 0.9460  (best train acc: 0.9288, best val acc: 0.9504, best train loss: 0.2034  @ epoch 18925 )\n",
      "[Epoch: 19320] train loss: 0.2182, train acc: 0.9222, val loss: 0.5385, val acc: 0.9460  (best train acc: 0.9288, best val acc: 0.9504, best train loss: 0.2034  @ epoch 18925 )\n",
      "[Epoch: 19340] train loss: 0.2297, train acc: 0.9177, val loss: 0.4966, val acc: 0.9403  (best train acc: 0.9288, best val acc: 0.9504, best train loss: 0.2034  @ epoch 18925 )\n",
      "[Epoch: 19360] train loss: 0.2140, train acc: 0.9241, val loss: 0.5280, val acc: 0.9477  (best train acc: 0.9288, best val acc: 0.9504, best train loss: 0.2034  @ epoch 18925 )\n",
      "[Epoch: 19380] train loss: 0.2327, train acc: 0.9151, val loss: 0.5221, val acc: 0.9477  (best train acc: 0.9288, best val acc: 0.9504, best train loss: 0.2034  @ epoch 18925 )\n",
      "[Epoch: 19400] train loss: 0.2246, train acc: 0.9240, val loss: 0.5317, val acc: 0.9390  (best train acc: 0.9288, best val acc: 0.9504, best train loss: 0.2034  @ epoch 18925 )\n",
      "[Epoch: 19420] train loss: 0.2177, train acc: 0.9217, val loss: 0.5312, val acc: 0.9356  (best train acc: 0.9305, best val acc: 0.9504, best train loss: 0.2022  @ epoch 19416 )\n",
      "[Epoch: 19440] train loss: 0.2092, train acc: 0.9245, val loss: 0.5364, val acc: 0.9487  (best train acc: 0.9305, best val acc: 0.9504, best train loss: 0.2022  @ epoch 19416 )\n",
      "[Epoch: 19460] train loss: 0.2424, train acc: 0.9105, val loss: 0.5221, val acc: 0.9460  (best train acc: 0.9305, best val acc: 0.9504, best train loss: 0.2022  @ epoch 19416 )\n",
      "[Epoch: 19480] train loss: 0.2278, train acc: 0.9170, val loss: 0.5176, val acc: 0.9474  (best train acc: 0.9305, best val acc: 0.9504, best train loss: 0.2022  @ epoch 19416 )\n",
      "[Epoch: 19500] train loss: 0.2153, train acc: 0.9229, val loss: 0.5370, val acc: 0.9467  (best train acc: 0.9305, best val acc: 0.9504, best train loss: 0.2022  @ epoch 19416 )\n",
      "[Epoch: 19520] train loss: 0.2122, train acc: 0.9253, val loss: 0.5295, val acc: 0.9474  (best train acc: 0.9305, best val acc: 0.9504, best train loss: 0.2022  @ epoch 19416 )\n",
      "[Epoch: 19540] train loss: 0.2170, train acc: 0.9218, val loss: 0.5459, val acc: 0.9440  (best train acc: 0.9305, best val acc: 0.9504, best train loss: 0.2022  @ epoch 19416 )\n",
      "[Epoch: 19560] train loss: 0.2323, train acc: 0.9172, val loss: 0.5636, val acc: 0.9349  (best train acc: 0.9305, best val acc: 0.9504, best train loss: 0.2022  @ epoch 19416 )\n",
      "[Epoch: 19580] train loss: 0.2340, train acc: 0.9145, val loss: 0.5339, val acc: 0.9460  (best train acc: 0.9305, best val acc: 0.9504, best train loss: 0.2022  @ epoch 19416 )\n",
      "[Epoch: 19600] train loss: 0.2271, train acc: 0.9181, val loss: 0.5077, val acc: 0.9400  (best train acc: 0.9305, best val acc: 0.9504, best train loss: 0.2022  @ epoch 19416 )\n",
      "[Epoch: 19620] train loss: 0.2111, train acc: 0.9231, val loss: 0.5368, val acc: 0.9467  (best train acc: 0.9305, best val acc: 0.9504, best train loss: 0.2022  @ epoch 19416 )\n",
      "[Epoch: 19640] train loss: 0.2375, train acc: 0.9105, val loss: 0.5123, val acc: 0.9457  (best train acc: 0.9305, best val acc: 0.9504, best train loss: 0.2022  @ epoch 19416 )\n",
      "[Epoch: 19660] train loss: 0.2421, train acc: 0.9093, val loss: 0.5611, val acc: 0.9494  (best train acc: 0.9305, best val acc: 0.9504, best train loss: 0.2022  @ epoch 19416 )\n",
      "[Epoch: 19680] train loss: 0.2206, train acc: 0.9198, val loss: 0.5278, val acc: 0.9379  (best train acc: 0.9305, best val acc: 0.9504, best train loss: 0.2022  @ epoch 19416 )\n",
      "[Epoch: 19700] train loss: 0.2056, train acc: 0.9260, val loss: 0.5503, val acc: 0.9460  (best train acc: 0.9305, best val acc: 0.9504, best train loss: 0.2022  @ epoch 19416 )\n",
      "[Epoch: 19720] train loss: 0.2233, train acc: 0.9182, val loss: 0.5381, val acc: 0.9427  (best train acc: 0.9305, best val acc: 0.9504, best train loss: 0.2022  @ epoch 19416 )\n",
      "[Epoch: 19740] train loss: 0.2181, train acc: 0.9204, val loss: 0.5107, val acc: 0.9450  (best train acc: 0.9305, best val acc: 0.9504, best train loss: 0.2022  @ epoch 19416 )\n",
      "[Epoch: 19760] train loss: 0.2196, train acc: 0.9219, val loss: 0.5137, val acc: 0.9477  (best train acc: 0.9305, best val acc: 0.9504, best train loss: 0.2022  @ epoch 19416 )\n",
      "[Epoch: 19780] train loss: 0.2278, train acc: 0.9203, val loss: 0.5321, val acc: 0.9430  (best train acc: 0.9305, best val acc: 0.9504, best train loss: 0.2022  @ epoch 19416 )\n",
      "[Epoch: 19800] train loss: 0.2534, train acc: 0.9051, val loss: 0.5198, val acc: 0.9349  (best train acc: 0.9305, best val acc: 0.9504, best train loss: 0.2022  @ epoch 19416 )\n",
      "[Epoch: 19820] train loss: 0.2273, train acc: 0.9174, val loss: 0.5394, val acc: 0.9437  (best train acc: 0.9305, best val acc: 0.9504, best train loss: 0.2022  @ epoch 19416 )\n",
      "[Epoch: 19840] train loss: 0.2090, train acc: 0.9242, val loss: 0.5472, val acc: 0.9417  (best train acc: 0.9305, best val acc: 0.9504, best train loss: 0.2022  @ epoch 19416 )\n",
      "[Epoch: 19860] train loss: 0.2080, train acc: 0.9232, val loss: 0.5493, val acc: 0.9474  (best train acc: 0.9305, best val acc: 0.9504, best train loss: 0.2005  @ epoch 19856 )\n",
      "[Epoch: 19880] train loss: 0.2128, train acc: 0.9229, val loss: 0.5463, val acc: 0.9417  (best train acc: 0.9305, best val acc: 0.9504, best train loss: 0.2005  @ epoch 19856 )\n",
      "[Epoch: 19900] train loss: 0.2207, train acc: 0.9184, val loss: 0.5182, val acc: 0.9460  (best train acc: 0.9305, best val acc: 0.9508, best train loss: 0.2005  @ epoch 19856 )\n",
      "[Epoch: 19920] train loss: 0.2258, train acc: 0.9145, val loss: 0.4873, val acc: 0.9450  (best train acc: 0.9305, best val acc: 0.9508, best train loss: 0.2005  @ epoch 19856 )\n",
      "[Epoch: 19940] train loss: 0.2311, train acc: 0.9124, val loss: 0.5411, val acc: 0.9420  (best train acc: 0.9305, best val acc: 0.9508, best train loss: 0.2005  @ epoch 19856 )\n",
      "[Epoch: 19960] train loss: 0.2171, train acc: 0.9238, val loss: 0.5513, val acc: 0.9444  (best train acc: 0.9305, best val acc: 0.9508, best train loss: 0.2001  @ epoch 19957 )\n",
      "[Epoch: 19980] train loss: 0.2306, train acc: 0.9142, val loss: 0.5814, val acc: 0.9379  (best train acc: 0.9305, best val acc: 0.9508, best train loss: 0.2001  @ epoch 19957 )\n",
      "[Epoch: 20000] train loss: 0.2407, train acc: 0.9151, val loss: 0.5330, val acc: 0.9427  (best train acc: 0.9305, best val acc: 0.9508, best train loss: 0.2001  @ epoch 19957 )\n",
      "[Epoch: 20020] train loss: 0.2209, train acc: 0.9199, val loss: 0.4883, val acc: 0.9366  (best train acc: 0.9305, best val acc: 0.9508, best train loss: 0.2001  @ epoch 19957 )\n",
      "[Epoch: 20040] train loss: 0.2123, train acc: 0.9278, val loss: 0.5583, val acc: 0.9487  (best train acc: 0.9305, best val acc: 0.9508, best train loss: 0.2001  @ epoch 19957 )\n",
      "[Epoch: 20060] train loss: 0.2084, train acc: 0.9284, val loss: 0.5406, val acc: 0.9481  (best train acc: 0.9305, best val acc: 0.9508, best train loss: 0.2001  @ epoch 19957 )\n",
      "[Epoch: 20080] train loss: 0.2105, train acc: 0.9259, val loss: 0.5307, val acc: 0.9484  (best train acc: 0.9305, best val acc: 0.9508, best train loss: 0.2001  @ epoch 19957 )\n",
      "[Epoch: 20100] train loss: 0.2025, train acc: 0.9283, val loss: 0.5445, val acc: 0.9386  (best train acc: 0.9305, best val acc: 0.9508, best train loss: 0.2001  @ epoch 19957 )\n",
      "[Epoch: 20120] train loss: 0.2303, train acc: 0.9195, val loss: 0.5404, val acc: 0.9403  (best train acc: 0.9305, best val acc: 0.9508, best train loss: 0.2001  @ epoch 19957 )\n",
      "[Epoch: 20140] train loss: 0.2190, train acc: 0.9198, val loss: 0.5631, val acc: 0.9470  (best train acc: 0.9305, best val acc: 0.9508, best train loss: 0.2001  @ epoch 19957 )\n",
      "[Epoch: 20160] train loss: 0.2264, train acc: 0.9145, val loss: 0.5411, val acc: 0.9413  (best train acc: 0.9305, best val acc: 0.9508, best train loss: 0.2001  @ epoch 19957 )\n",
      "[Epoch: 20180] train loss: 0.2152, train acc: 0.9216, val loss: 0.5277, val acc: 0.9396  (best train acc: 0.9305, best val acc: 0.9508, best train loss: 0.1998  @ epoch 20166 )\n",
      "[Epoch: 20200] train loss: 0.2212, train acc: 0.9200, val loss: 0.5057, val acc: 0.9484  (best train acc: 0.9305, best val acc: 0.9508, best train loss: 0.1998  @ epoch 20166 )\n",
      "[Epoch: 20220] train loss: 0.2197, train acc: 0.9187, val loss: 0.5350, val acc: 0.9497  (best train acc: 0.9305, best val acc: 0.9508, best train loss: 0.1998  @ epoch 20166 )\n",
      "[Epoch: 20240] train loss: 0.2206, train acc: 0.9200, val loss: 0.5642, val acc: 0.9501  (best train acc: 0.9305, best val acc: 0.9508, best train loss: 0.1995  @ epoch 20221 )\n",
      "[Epoch: 20260] train loss: 0.2157, train acc: 0.9194, val loss: 0.5598, val acc: 0.9410  (best train acc: 0.9305, best val acc: 0.9508, best train loss: 0.1995  @ epoch 20221 )\n",
      "[Epoch: 20280] train loss: 0.2180, train acc: 0.9221, val loss: 0.5672, val acc: 0.9251  (best train acc: 0.9305, best val acc: 0.9508, best train loss: 0.1995  @ epoch 20221 )\n",
      "[Epoch: 20300] train loss: 0.2326, train acc: 0.9176, val loss: 0.4804, val acc: 0.9383  (best train acc: 0.9305, best val acc: 0.9508, best train loss: 0.1995  @ epoch 20221 )\n",
      "[Epoch: 20320] train loss: 0.2012, train acc: 0.9285, val loss: 0.5405, val acc: 0.9487  (best train acc: 0.9305, best val acc: 0.9511, best train loss: 0.1995  @ epoch 20221 )\n",
      "[Epoch: 20340] train loss: 0.2367, train acc: 0.9159, val loss: 0.5715, val acc: 0.9444  (best train acc: 0.9305, best val acc: 0.9514, best train loss: 0.1995  @ epoch 20221 )\n",
      "[Epoch: 20360] train loss: 0.2241, train acc: 0.9190, val loss: 0.5605, val acc: 0.9440  (best train acc: 0.9305, best val acc: 0.9514, best train loss: 0.1995  @ epoch 20221 )\n",
      "[Epoch: 20380] train loss: 0.2216, train acc: 0.9181, val loss: 0.5651, val acc: 0.9464  (best train acc: 0.9305, best val acc: 0.9514, best train loss: 0.1995  @ epoch 20221 )\n",
      "[Epoch: 20400] train loss: 0.2261, train acc: 0.9180, val loss: 0.5536, val acc: 0.9444  (best train acc: 0.9305, best val acc: 0.9514, best train loss: 0.1995  @ epoch 20221 )\n",
      "[Epoch: 20420] train loss: 0.2332, train acc: 0.9124, val loss: 0.5746, val acc: 0.9396  (best train acc: 0.9305, best val acc: 0.9514, best train loss: 0.1995  @ epoch 20221 )\n",
      "[Epoch: 20440] train loss: 0.2111, train acc: 0.9231, val loss: 0.5905, val acc: 0.9322  (best train acc: 0.9305, best val acc: 0.9514, best train loss: 0.1995  @ epoch 20221 )\n",
      "[Epoch: 20460] train loss: 0.2097, train acc: 0.9234, val loss: 0.5366, val acc: 0.9417  (best train acc: 0.9305, best val acc: 0.9514, best train loss: 0.1995  @ epoch 20221 )\n",
      "[Epoch: 20480] train loss: 0.2121, train acc: 0.9233, val loss: 0.5552, val acc: 0.9467  (best train acc: 0.9305, best val acc: 0.9514, best train loss: 0.1980  @ epoch 20465 )\n",
      "[Epoch: 20500] train loss: 0.2106, train acc: 0.9262, val loss: 0.5362, val acc: 0.9481  (best train acc: 0.9305, best val acc: 0.9518, best train loss: 0.1980  @ epoch 20465 )\n",
      "[Epoch: 20520] train loss: 0.2169, train acc: 0.9220, val loss: 0.5346, val acc: 0.9467  (best train acc: 0.9305, best val acc: 0.9518, best train loss: 0.1980  @ epoch 20465 )\n",
      "[Epoch: 20540] train loss: 0.2100, train acc: 0.9263, val loss: 0.5430, val acc: 0.9467  (best train acc: 0.9315, best val acc: 0.9518, best train loss: 0.1977  @ epoch 20528 )\n",
      "[Epoch: 20560] train loss: 0.2102, train acc: 0.9252, val loss: 0.5630, val acc: 0.9427  (best train acc: 0.9315, best val acc: 0.9518, best train loss: 0.1977  @ epoch 20528 )\n",
      "[Epoch: 20580] train loss: 0.2200, train acc: 0.9195, val loss: 0.5686, val acc: 0.9423  (best train acc: 0.9315, best val acc: 0.9518, best train loss: 0.1977  @ epoch 20528 )\n",
      "[Epoch: 20600] train loss: 0.2180, train acc: 0.9196, val loss: 0.5636, val acc: 0.9363  (best train acc: 0.9315, best val acc: 0.9518, best train loss: 0.1977  @ epoch 20528 )\n",
      "[Epoch: 20620] train loss: 0.2086, train acc: 0.9264, val loss: 0.5699, val acc: 0.9464  (best train acc: 0.9315, best val acc: 0.9518, best train loss: 0.1977  @ epoch 20528 )\n",
      "[Epoch: 20640] train loss: 0.2197, train acc: 0.9184, val loss: 0.5622, val acc: 0.9450  (best train acc: 0.9315, best val acc: 0.9518, best train loss: 0.1977  @ epoch 20528 )\n",
      "[Epoch: 20660] train loss: 0.2088, train acc: 0.9229, val loss: 0.5764, val acc: 0.9464  (best train acc: 0.9315, best val acc: 0.9518, best train loss: 0.1974  @ epoch 20658 )\n",
      "[Epoch: 20680] train loss: 0.2360, train acc: 0.9109, val loss: 0.5191, val acc: 0.9373  (best train acc: 0.9315, best val acc: 0.9518, best train loss: 0.1974  @ epoch 20658 )\n",
      "[Epoch: 20700] train loss: 0.2254, train acc: 0.9190, val loss: 0.5505, val acc: 0.9450  (best train acc: 0.9315, best val acc: 0.9518, best train loss: 0.1974  @ epoch 20658 )\n",
      "[Epoch: 20720] train loss: 0.2057, train acc: 0.9243, val loss: 0.5351, val acc: 0.9464  (best train acc: 0.9315, best val acc: 0.9518, best train loss: 0.1974  @ epoch 20658 )\n",
      "[Epoch: 20740] train loss: 0.2149, train acc: 0.9217, val loss: 0.5332, val acc: 0.9464  (best train acc: 0.9315, best val acc: 0.9518, best train loss: 0.1974  @ epoch 20658 )\n",
      "[Epoch: 20760] train loss: 0.2172, train acc: 0.9215, val loss: 0.5368, val acc: 0.9427  (best train acc: 0.9315, best val acc: 0.9521, best train loss: 0.1964  @ epoch 20749 )\n",
      "[Epoch: 20780] train loss: 0.2137, train acc: 0.9232, val loss: 0.5645, val acc: 0.9319  (best train acc: 0.9315, best val acc: 0.9521, best train loss: 0.1964  @ epoch 20749 )\n",
      "[Epoch: 20800] train loss: 0.2283, train acc: 0.9168, val loss: 0.5756, val acc: 0.9417  (best train acc: 0.9315, best val acc: 0.9521, best train loss: 0.1964  @ epoch 20749 )\n",
      "[Epoch: 20820] train loss: 0.2190, train acc: 0.9210, val loss: 0.5210, val acc: 0.9497  (best train acc: 0.9315, best val acc: 0.9521, best train loss: 0.1964  @ epoch 20749 )\n",
      "[Epoch: 20840] train loss: 0.2035, train acc: 0.9263, val loss: 0.5312, val acc: 0.9430  (best train acc: 0.9315, best val acc: 0.9521, best train loss: 0.1964  @ epoch 20749 )\n",
      "[Epoch: 20860] train loss: 0.2200, train acc: 0.9200, val loss: 0.5680, val acc: 0.9379  (best train acc: 0.9315, best val acc: 0.9521, best train loss: 0.1964  @ epoch 20749 )\n",
      "[Epoch: 20880] train loss: 0.2085, train acc: 0.9254, val loss: 0.5575, val acc: 0.9450  (best train acc: 0.9315, best val acc: 0.9521, best train loss: 0.1964  @ epoch 20749 )\n",
      "[Epoch: 20900] train loss: 0.2099, train acc: 0.9280, val loss: 0.5443, val acc: 0.9450  (best train acc: 0.9315, best val acc: 0.9521, best train loss: 0.1964  @ epoch 20749 )\n",
      "[Epoch: 20920] train loss: 0.2248, train acc: 0.9173, val loss: 0.5365, val acc: 0.9481  (best train acc: 0.9315, best val acc: 0.9521, best train loss: 0.1964  @ epoch 20749 )\n",
      "[Epoch: 20940] train loss: 0.2138, train acc: 0.9252, val loss: 0.5577, val acc: 0.9464  (best train acc: 0.9315, best val acc: 0.9521, best train loss: 0.1964  @ epoch 20749 )\n",
      "[Epoch: 20960] train loss: 0.2006, train acc: 0.9263, val loss: 0.5138, val acc: 0.9467  (best train acc: 0.9315, best val acc: 0.9521, best train loss: 0.1955  @ epoch 20958 )\n",
      "[Epoch: 20980] train loss: 0.2103, train acc: 0.9226, val loss: 0.5283, val acc: 0.9423  (best train acc: 0.9315, best val acc: 0.9521, best train loss: 0.1955  @ epoch 20958 )\n",
      "[Epoch: 21000] train loss: 0.2293, train acc: 0.9163, val loss: 0.5314, val acc: 0.9481  (best train acc: 0.9315, best val acc: 0.9521, best train loss: 0.1926  @ epoch 20991 )\n",
      "[Epoch: 21020] train loss: 0.2087, train acc: 0.9223, val loss: 0.4997, val acc: 0.9457  (best train acc: 0.9315, best val acc: 0.9521, best train loss: 0.1926  @ epoch 20991 )\n",
      "[Epoch: 21040] train loss: 0.2215, train acc: 0.9198, val loss: 0.5229, val acc: 0.9450  (best train acc: 0.9315, best val acc: 0.9521, best train loss: 0.1926  @ epoch 20991 )\n",
      "[Epoch: 21060] train loss: 0.2208, train acc: 0.9190, val loss: 0.5056, val acc: 0.9430  (best train acc: 0.9315, best val acc: 0.9521, best train loss: 0.1926  @ epoch 20991 )\n",
      "[Epoch: 21080] train loss: 0.2156, train acc: 0.9202, val loss: 0.5361, val acc: 0.9477  (best train acc: 0.9315, best val acc: 0.9521, best train loss: 0.1926  @ epoch 20991 )\n",
      "[Epoch: 21100] train loss: 0.2227, train acc: 0.9221, val loss: 0.5336, val acc: 0.9420  (best train acc: 0.9315, best val acc: 0.9521, best train loss: 0.1926  @ epoch 20991 )\n",
      "[Epoch: 21120] train loss: 0.2044, train acc: 0.9273, val loss: 0.5439, val acc: 0.9346  (best train acc: 0.9315, best val acc: 0.9521, best train loss: 0.1926  @ epoch 20991 )\n",
      "[Epoch: 21140] train loss: 0.2072, train acc: 0.9251, val loss: 0.5391, val acc: 0.9484  (best train acc: 0.9315, best val acc: 0.9521, best train loss: 0.1926  @ epoch 20991 )\n",
      "[Epoch: 21160] train loss: 0.2078, train acc: 0.9261, val loss: 0.5438, val acc: 0.9470  (best train acc: 0.9315, best val acc: 0.9521, best train loss: 0.1926  @ epoch 20991 )\n",
      "[Epoch: 21180] train loss: 0.2089, train acc: 0.9257, val loss: 0.5123, val acc: 0.9460  (best train acc: 0.9315, best val acc: 0.9521, best train loss: 0.1926  @ epoch 20991 )\n",
      "[Epoch: 21200] train loss: 0.2081, train acc: 0.9246, val loss: 0.5689, val acc: 0.9477  (best train acc: 0.9315, best val acc: 0.9521, best train loss: 0.1926  @ epoch 20991 )\n",
      "[Epoch: 21220] train loss: 0.2084, train acc: 0.9247, val loss: 0.5137, val acc: 0.9454  (best train acc: 0.9315, best val acc: 0.9521, best train loss: 0.1926  @ epoch 20991 )\n",
      "[Epoch: 21240] train loss: 0.2083, train acc: 0.9243, val loss: 0.5317, val acc: 0.9444  (best train acc: 0.9315, best val acc: 0.9521, best train loss: 0.1926  @ epoch 20991 )\n",
      "[Epoch: 21260] train loss: 0.2007, train acc: 0.9266, val loss: 0.5090, val acc: 0.9457  (best train acc: 0.9315, best val acc: 0.9521, best train loss: 0.1926  @ epoch 20991 )\n",
      "[Epoch: 21280] train loss: 0.2094, train acc: 0.9265, val loss: 0.5457, val acc: 0.9437  (best train acc: 0.9315, best val acc: 0.9521, best train loss: 0.1926  @ epoch 20991 )\n",
      "[Epoch: 21300] train loss: 0.2139, train acc: 0.9247, val loss: 0.5230, val acc: 0.9474  (best train acc: 0.9315, best val acc: 0.9521, best train loss: 0.1926  @ epoch 20991 )\n",
      "[Epoch: 21320] train loss: 0.2153, train acc: 0.9206, val loss: 0.4683, val acc: 0.9403  (best train acc: 0.9315, best val acc: 0.9521, best train loss: 0.1926  @ epoch 20991 )\n",
      "[Epoch: 21340] train loss: 0.2178, train acc: 0.9212, val loss: 0.5594, val acc: 0.9433  (best train acc: 0.9315, best val acc: 0.9521, best train loss: 0.1926  @ epoch 20991 )\n",
      "[Epoch: 21360] train loss: 0.2193, train acc: 0.9192, val loss: 0.5105, val acc: 0.9413  (best train acc: 0.9315, best val acc: 0.9521, best train loss: 0.1926  @ epoch 20991 )\n",
      "[Epoch: 21380] train loss: 0.2152, train acc: 0.9235, val loss: 0.5200, val acc: 0.9467  (best train acc: 0.9315, best val acc: 0.9521, best train loss: 0.1926  @ epoch 20991 )\n",
      "[Epoch: 21400] train loss: 0.2036, train acc: 0.9249, val loss: 0.5301, val acc: 0.9440  (best train acc: 0.9315, best val acc: 0.9521, best train loss: 0.1926  @ epoch 20991 )\n",
      "[Epoch: 21420] train loss: 0.2074, train acc: 0.9258, val loss: 0.5498, val acc: 0.9373  (best train acc: 0.9315, best val acc: 0.9521, best train loss: 0.1926  @ epoch 20991 )\n",
      "[Epoch: 21440] train loss: 0.2169, train acc: 0.9205, val loss: 0.5239, val acc: 0.9447  (best train acc: 0.9315, best val acc: 0.9521, best train loss: 0.1926  @ epoch 20991 )\n",
      "[Epoch: 21460] train loss: 0.2280, train acc: 0.9205, val loss: 0.5267, val acc: 0.9427  (best train acc: 0.9315, best val acc: 0.9521, best train loss: 0.1926  @ epoch 20991 )\n",
      "[Epoch: 21480] train loss: 0.2223, train acc: 0.9209, val loss: 0.5475, val acc: 0.9501  (best train acc: 0.9315, best val acc: 0.9521, best train loss: 0.1926  @ epoch 20991 )\n",
      "[Epoch: 21500] train loss: 0.2062, train acc: 0.9276, val loss: 0.5413, val acc: 0.9413  (best train acc: 0.9318, best val acc: 0.9521, best train loss: 0.1926  @ epoch 20991 )\n",
      "[Epoch: 21520] train loss: 0.2335, train acc: 0.9109, val loss: 0.5350, val acc: 0.9460  (best train acc: 0.9318, best val acc: 0.9521, best train loss: 0.1926  @ epoch 20991 )\n",
      "[Epoch: 21540] train loss: 0.2154, train acc: 0.9244, val loss: 0.4826, val acc: 0.9460  (best train acc: 0.9318, best val acc: 0.9521, best train loss: 0.1926  @ epoch 20991 )\n",
      "[Epoch: 21560] train loss: 0.2074, train acc: 0.9268, val loss: 0.5502, val acc: 0.9481  (best train acc: 0.9318, best val acc: 0.9521, best train loss: 0.1926  @ epoch 20991 )\n",
      "[Epoch: 21580] train loss: 0.2196, train acc: 0.9213, val loss: 0.5178, val acc: 0.9460  (best train acc: 0.9318, best val acc: 0.9521, best train loss: 0.1926  @ epoch 20991 )\n",
      "[Epoch: 21600] train loss: 0.2083, train acc: 0.9245, val loss: 0.4968, val acc: 0.9423  (best train acc: 0.9318, best val acc: 0.9521, best train loss: 0.1926  @ epoch 20991 )\n",
      "[Epoch: 21620] train loss: 0.2010, train acc: 0.9263, val loss: 0.5029, val acc: 0.9447  (best train acc: 0.9318, best val acc: 0.9521, best train loss: 0.1926  @ epoch 20991 )\n",
      "[Epoch: 21640] train loss: 0.2087, train acc: 0.9221, val loss: 0.5471, val acc: 0.9464  (best train acc: 0.9318, best val acc: 0.9528, best train loss: 0.1924  @ epoch 21633 )\n",
      "[Epoch: 21660] train loss: 0.2085, train acc: 0.9276, val loss: 0.5431, val acc: 0.9470  (best train acc: 0.9318, best val acc: 0.9528, best train loss: 0.1924  @ epoch 21633 )\n",
      "[Epoch: 21680] train loss: 0.2013, train acc: 0.9284, val loss: 0.5458, val acc: 0.9477  (best train acc: 0.9318, best val acc: 0.9528, best train loss: 0.1924  @ epoch 21633 )\n",
      "[Epoch: 21700] train loss: 0.2079, train acc: 0.9247, val loss: 0.5291, val acc: 0.9481  (best train acc: 0.9318, best val acc: 0.9528, best train loss: 0.1924  @ epoch 21633 )\n",
      "[Epoch: 21720] train loss: 0.2187, train acc: 0.9211, val loss: 0.5260, val acc: 0.9413  (best train acc: 0.9318, best val acc: 0.9528, best train loss: 0.1924  @ epoch 21633 )\n",
      "[Epoch: 21740] train loss: 0.2137, train acc: 0.9227, val loss: 0.4956, val acc: 0.9437  (best train acc: 0.9318, best val acc: 0.9528, best train loss: 0.1924  @ epoch 21633 )\n",
      "[Epoch: 21760] train loss: 0.2108, train acc: 0.9218, val loss: 0.5368, val acc: 0.9454  (best train acc: 0.9318, best val acc: 0.9528, best train loss: 0.1924  @ epoch 21633 )\n",
      "[Epoch: 21780] train loss: 0.2082, train acc: 0.9255, val loss: 0.5533, val acc: 0.9470  (best train acc: 0.9318, best val acc: 0.9528, best train loss: 0.1924  @ epoch 21633 )\n",
      "[Epoch: 21800] train loss: 0.2034, train acc: 0.9294, val loss: 0.5287, val acc: 0.9497  (best train acc: 0.9318, best val acc: 0.9528, best train loss: 0.1924  @ epoch 21633 )\n",
      "[Epoch: 21820] train loss: 0.2022, train acc: 0.9284, val loss: 0.5376, val acc: 0.9447  (best train acc: 0.9318, best val acc: 0.9528, best train loss: 0.1924  @ epoch 21633 )\n",
      "[Epoch: 21840] train loss: 0.2130, train acc: 0.9209, val loss: 0.5468, val acc: 0.9447  (best train acc: 0.9318, best val acc: 0.9528, best train loss: 0.1924  @ epoch 21633 )\n",
      "[Epoch: 21860] train loss: 0.2267, train acc: 0.9166, val loss: 0.4756, val acc: 0.9376  (best train acc: 0.9318, best val acc: 0.9528, best train loss: 0.1907  @ epoch 21841 )\n",
      "[Epoch: 21880] train loss: 0.2037, train acc: 0.9265, val loss: 0.5120, val acc: 0.9477  (best train acc: 0.9318, best val acc: 0.9528, best train loss: 0.1907  @ epoch 21841 )\n",
      "[Epoch: 21900] train loss: 0.2272, train acc: 0.9139, val loss: 0.5437, val acc: 0.9464  (best train acc: 0.9318, best val acc: 0.9528, best train loss: 0.1907  @ epoch 21841 )\n",
      "[Epoch: 21920] train loss: 0.2270, train acc: 0.9250, val loss: 0.4825, val acc: 0.9390  (best train acc: 0.9318, best val acc: 0.9528, best train loss: 0.1907  @ epoch 21841 )\n",
      "[Epoch: 21940] train loss: 0.2057, train acc: 0.9263, val loss: 0.3884, val acc: 0.9457  (best train acc: 0.9318, best val acc: 0.9528, best train loss: 0.1907  @ epoch 21841 )\n",
      "[Epoch: 21960] train loss: 0.2163, train acc: 0.9211, val loss: 0.4924, val acc: 0.9477  (best train acc: 0.9318, best val acc: 0.9528, best train loss: 0.1907  @ epoch 21841 )\n",
      "[Epoch: 21980] train loss: 0.1964, train acc: 0.9293, val loss: 0.4949, val acc: 0.9467  (best train acc: 0.9321, best val acc: 0.9528, best train loss: 0.1907  @ epoch 21841 )\n",
      "[Epoch: 22000] train loss: 0.2221, train acc: 0.9188, val loss: 0.5453, val acc: 0.9474  (best train acc: 0.9323, best val acc: 0.9528, best train loss: 0.1905  @ epoch 21988 )\n",
      "[Epoch: 22020] train loss: 0.2124, train acc: 0.9250, val loss: 0.5275, val acc: 0.9450  (best train acc: 0.9323, best val acc: 0.9541, best train loss: 0.1905  @ epoch 21988 )\n",
      "[Epoch: 22040] train loss: 0.2073, train acc: 0.9262, val loss: 0.4844, val acc: 0.9491  (best train acc: 0.9323, best val acc: 0.9541, best train loss: 0.1905  @ epoch 21988 )\n",
      "[Epoch: 22060] train loss: 0.2083, train acc: 0.9253, val loss: 0.5373, val acc: 0.9504  (best train acc: 0.9325, best val acc: 0.9541, best train loss: 0.1905  @ epoch 21988 )\n",
      "[Epoch: 22080] train loss: 0.1936, train acc: 0.9330, val loss: 0.5253, val acc: 0.9437  (best train acc: 0.9330, best val acc: 0.9541, best train loss: 0.1905  @ epoch 21988 )\n",
      "[Epoch: 22100] train loss: 0.2038, train acc: 0.9271, val loss: 0.4929, val acc: 0.9433  (best train acc: 0.9330, best val acc: 0.9541, best train loss: 0.1905  @ epoch 21988 )\n",
      "[Epoch: 22120] train loss: 0.2040, train acc: 0.9246, val loss: 0.5480, val acc: 0.9444  (best train acc: 0.9339, best val acc: 0.9541, best train loss: 0.1886  @ epoch 22118 )\n",
      "[Epoch: 22140] train loss: 0.2280, train acc: 0.9181, val loss: 0.5308, val acc: 0.9450  (best train acc: 0.9339, best val acc: 0.9541, best train loss: 0.1886  @ epoch 22118 )\n",
      "[Epoch: 22160] train loss: 0.2091, train acc: 0.9229, val loss: 0.5085, val acc: 0.9481  (best train acc: 0.9339, best val acc: 0.9541, best train loss: 0.1886  @ epoch 22118 )\n",
      "[Epoch: 22180] train loss: 0.1993, train acc: 0.9315, val loss: 0.5051, val acc: 0.9447  (best train acc: 0.9339, best val acc: 0.9541, best train loss: 0.1886  @ epoch 22118 )\n",
      "[Epoch: 22200] train loss: 0.1968, train acc: 0.9331, val loss: 0.5413, val acc: 0.9494  (best train acc: 0.9362, best val acc: 0.9541, best train loss: 0.1855  @ epoch 22198 )\n",
      "[Epoch: 22220] train loss: 0.2077, train acc: 0.9259, val loss: 0.5496, val acc: 0.9491  (best train acc: 0.9374, best val acc: 0.9541, best train loss: 0.1780  @ epoch 22204 )\n",
      "[Epoch: 22240] train loss: 0.2039, train acc: 0.9305, val loss: 0.5242, val acc: 0.9406  (best train acc: 0.9381, best val acc: 0.9541, best train loss: 0.1780  @ epoch 22204 )\n",
      "[Epoch: 22260] train loss: 0.1883, train acc: 0.9336, val loss: 0.5608, val acc: 0.9477  (best train acc: 0.9381, best val acc: 0.9541, best train loss: 0.1780  @ epoch 22204 )\n",
      "[Epoch: 22280] train loss: 0.1908, train acc: 0.9356, val loss: 0.5210, val acc: 0.9487  (best train acc: 0.9381, best val acc: 0.9541, best train loss: 0.1780  @ epoch 22204 )\n",
      "[Epoch: 22300] train loss: 0.1876, train acc: 0.9349, val loss: 0.5424, val acc: 0.9491  (best train acc: 0.9389, best val acc: 0.9541, best train loss: 0.1780  @ epoch 22204 )\n",
      "[Epoch: 22320] train loss: 0.2016, train acc: 0.9288, val loss: 0.5043, val acc: 0.9427  (best train acc: 0.9398, best val acc: 0.9541, best train loss: 0.1779  @ epoch 22301 )\n",
      "[Epoch: 22340] train loss: 0.2009, train acc: 0.9307, val loss: 0.4963, val acc: 0.9417  (best train acc: 0.9398, best val acc: 0.9541, best train loss: 0.1779  @ epoch 22301 )\n",
      "[Epoch: 22360] train loss: 0.1790, train acc: 0.9372, val loss: 0.5176, val acc: 0.9518  (best train acc: 0.9398, best val acc: 0.9541, best train loss: 0.1779  @ epoch 22301 )\n",
      "[Epoch: 22380] train loss: 0.2104, train acc: 0.9224, val loss: 0.5239, val acc: 0.9491  (best train acc: 0.9398, best val acc: 0.9541, best train loss: 0.1779  @ epoch 22301 )\n",
      "[Epoch: 22400] train loss: 0.1887, train acc: 0.9325, val loss: 0.5391, val acc: 0.9437  (best train acc: 0.9398, best val acc: 0.9541, best train loss: 0.1779  @ epoch 22301 )\n",
      "[Epoch: 22420] train loss: 0.1844, train acc: 0.9339, val loss: 0.5277, val acc: 0.9474  (best train acc: 0.9398, best val acc: 0.9541, best train loss: 0.1779  @ epoch 22301 )\n",
      "[Epoch: 22440] train loss: 0.1926, train acc: 0.9294, val loss: 0.5099, val acc: 0.9494  (best train acc: 0.9398, best val acc: 0.9541, best train loss: 0.1779  @ epoch 22301 )\n",
      "[Epoch: 22460] train loss: 0.1906, train acc: 0.9311, val loss: 0.4921, val acc: 0.9497  (best train acc: 0.9398, best val acc: 0.9541, best train loss: 0.1755  @ epoch 22449 )\n",
      "[Epoch: 22480] train loss: 0.1821, train acc: 0.9333, val loss: 0.5036, val acc: 0.9497  (best train acc: 0.9398, best val acc: 0.9541, best train loss: 0.1755  @ epoch 22449 )\n",
      "[Epoch: 22500] train loss: 0.1908, train acc: 0.9308, val loss: 0.5213, val acc: 0.9437  (best train acc: 0.9398, best val acc: 0.9541, best train loss: 0.1755  @ epoch 22449 )\n",
      "[Epoch: 22520] train loss: 0.1922, train acc: 0.9322, val loss: 0.5109, val acc: 0.9524  (best train acc: 0.9398, best val acc: 0.9541, best train loss: 0.1755  @ epoch 22449 )\n",
      "[Epoch: 22540] train loss: 0.2000, train acc: 0.9291, val loss: 0.5040, val acc: 0.9457  (best train acc: 0.9398, best val acc: 0.9541, best train loss: 0.1755  @ epoch 22449 )\n",
      "[Epoch: 22560] train loss: 0.1898, train acc: 0.9312, val loss: 0.5276, val acc: 0.9383  (best train acc: 0.9398, best val acc: 0.9541, best train loss: 0.1746  @ epoch 22547 )\n",
      "[Epoch: 22580] train loss: 0.2045, train acc: 0.9268, val loss: 0.4917, val acc: 0.9403  (best train acc: 0.9399, best val acc: 0.9541, best train loss: 0.1746  @ epoch 22547 )\n",
      "[Epoch: 22600] train loss: 0.2009, train acc: 0.9277, val loss: 0.5311, val acc: 0.9545  (best train acc: 0.9399, best val acc: 0.9545, best train loss: 0.1746  @ epoch 22547 )\n",
      "[Epoch: 22620] train loss: 0.1956, train acc: 0.9292, val loss: 0.5026, val acc: 0.9487  (best train acc: 0.9399, best val acc: 0.9545, best train loss: 0.1746  @ epoch 22547 )\n",
      "[Epoch: 22640] train loss: 0.1840, train acc: 0.9374, val loss: 0.5310, val acc: 0.9511  (best train acc: 0.9399, best val acc: 0.9545, best train loss: 0.1721  @ epoch 22632 )\n",
      "[Epoch: 22660] train loss: 0.1772, train acc: 0.9393, val loss: 0.5258, val acc: 0.9467  (best train acc: 0.9399, best val acc: 0.9545, best train loss: 0.1721  @ epoch 22632 )\n",
      "[Epoch: 22680] train loss: 0.1939, train acc: 0.9299, val loss: 0.5444, val acc: 0.9423  (best train acc: 0.9399, best val acc: 0.9545, best train loss: 0.1721  @ epoch 22632 )\n",
      "[Epoch: 22700] train loss: 0.1776, train acc: 0.9388, val loss: 0.5188, val acc: 0.9440  (best train acc: 0.9399, best val acc: 0.9545, best train loss: 0.1721  @ epoch 22632 )\n",
      "[Epoch: 22720] train loss: 0.1873, train acc: 0.9340, val loss: 0.5607, val acc: 0.9467  (best train acc: 0.9399, best val acc: 0.9545, best train loss: 0.1721  @ epoch 22632 )\n",
      "[Epoch: 22740] train loss: 0.1802, train acc: 0.9357, val loss: 0.5347, val acc: 0.9464  (best train acc: 0.9399, best val acc: 0.9545, best train loss: 0.1721  @ epoch 22632 )\n",
      "[Epoch: 22760] train loss: 0.1825, train acc: 0.9348, val loss: 0.5275, val acc: 0.9508  (best train acc: 0.9410, best val acc: 0.9545, best train loss: 0.1721  @ epoch 22632 )\n",
      "[Epoch: 22780] train loss: 0.1822, train acc: 0.9376, val loss: 0.5088, val acc: 0.9491  (best train acc: 0.9410, best val acc: 0.9545, best train loss: 0.1721  @ epoch 22632 )\n",
      "[Epoch: 22800] train loss: 0.1914, train acc: 0.9341, val loss: 0.5300, val acc: 0.9373  (best train acc: 0.9410, best val acc: 0.9545, best train loss: 0.1721  @ epoch 22632 )\n",
      "[Epoch: 22820] train loss: 0.1870, train acc: 0.9323, val loss: 0.5384, val acc: 0.9470  (best train acc: 0.9410, best val acc: 0.9545, best train loss: 0.1721  @ epoch 22632 )\n",
      "[Epoch: 22840] train loss: 0.1918, train acc: 0.9313, val loss: 0.5164, val acc: 0.9477  (best train acc: 0.9410, best val acc: 0.9545, best train loss: 0.1721  @ epoch 22632 )\n",
      "[Epoch: 22860] train loss: 0.1906, train acc: 0.9319, val loss: 0.5009, val acc: 0.9491  (best train acc: 0.9410, best val acc: 0.9545, best train loss: 0.1721  @ epoch 22632 )\n",
      "[Epoch: 22880] train loss: 0.1730, train acc: 0.9405, val loss: 0.5056, val acc: 0.9450  (best train acc: 0.9410, best val acc: 0.9545, best train loss: 0.1721  @ epoch 22632 )\n",
      "[Epoch: 22900] train loss: 0.2248, train acc: 0.9181, val loss: 0.5048, val acc: 0.9413  (best train acc: 0.9410, best val acc: 0.9545, best train loss: 0.1721  @ epoch 22632 )\n",
      "[Epoch: 22920] train loss: 0.1924, train acc: 0.9309, val loss: 0.5512, val acc: 0.9464  (best train acc: 0.9410, best val acc: 0.9545, best train loss: 0.1721  @ epoch 22632 )\n",
      "[Epoch: 22940] train loss: 0.1790, train acc: 0.9364, val loss: 0.5121, val acc: 0.9477  (best train acc: 0.9410, best val acc: 0.9545, best train loss: 0.1721  @ epoch 22632 )\n",
      "[Epoch: 22960] train loss: 0.2081, train acc: 0.9258, val loss: 0.5096, val acc: 0.9487  (best train acc: 0.9410, best val acc: 0.9545, best train loss: 0.1721  @ epoch 22632 )\n",
      "[Epoch: 22980] train loss: 0.1876, train acc: 0.9339, val loss: 0.5118, val acc: 0.9470  (best train acc: 0.9410, best val acc: 0.9545, best train loss: 0.1721  @ epoch 22632 )\n",
      "[Epoch: 23000] train loss: 0.1945, train acc: 0.9320, val loss: 0.5133, val acc: 0.9491  (best train acc: 0.9410, best val acc: 0.9545, best train loss: 0.1721  @ epoch 22632 )\n",
      "[Epoch: 23020] train loss: 0.1746, train acc: 0.9401, val loss: 0.5171, val acc: 0.9514  (best train acc: 0.9410, best val acc: 0.9545, best train loss: 0.1719  @ epoch 23006 )\n",
      "[Epoch: 23040] train loss: 0.1833, train acc: 0.9374, val loss: 0.5317, val acc: 0.9430  (best train acc: 0.9416, best val acc: 0.9545, best train loss: 0.1669  @ epoch 23025 )\n",
      "[Epoch: 23060] train loss: 0.1968, train acc: 0.9310, val loss: 0.5361, val acc: 0.9454  (best train acc: 0.9416, best val acc: 0.9545, best train loss: 0.1669  @ epoch 23025 )\n",
      "[Epoch: 23080] train loss: 0.1905, train acc: 0.9336, val loss: 0.5165, val acc: 0.9444  (best train acc: 0.9416, best val acc: 0.9545, best train loss: 0.1669  @ epoch 23025 )\n",
      "[Epoch: 23100] train loss: 0.1793, train acc: 0.9357, val loss: 0.5180, val acc: 0.9487  (best train acc: 0.9421, best val acc: 0.9545, best train loss: 0.1669  @ epoch 23025 )\n",
      "[Epoch: 23120] train loss: 0.1792, train acc: 0.9360, val loss: 0.5169, val acc: 0.9491  (best train acc: 0.9421, best val acc: 0.9545, best train loss: 0.1669  @ epoch 23025 )\n",
      "[Epoch: 23140] train loss: 0.1878, train acc: 0.9326, val loss: 0.5222, val acc: 0.9494  (best train acc: 0.9421, best val acc: 0.9545, best train loss: 0.1669  @ epoch 23025 )\n",
      "[Epoch: 23160] train loss: 0.1858, train acc: 0.9345, val loss: 0.5454, val acc: 0.9511  (best train acc: 0.9421, best val acc: 0.9545, best train loss: 0.1669  @ epoch 23025 )\n",
      "[Epoch: 23180] train loss: 0.1844, train acc: 0.9350, val loss: 0.5559, val acc: 0.9518  (best train acc: 0.9421, best val acc: 0.9545, best train loss: 0.1669  @ epoch 23025 )\n",
      "[Epoch: 23200] train loss: 0.1868, train acc: 0.9362, val loss: 0.5044, val acc: 0.9481  (best train acc: 0.9421, best val acc: 0.9545, best train loss: 0.1669  @ epoch 23025 )\n",
      "[Epoch: 23220] train loss: 0.2052, train acc: 0.9280, val loss: 0.5013, val acc: 0.9417  (best train acc: 0.9421, best val acc: 0.9545, best train loss: 0.1669  @ epoch 23025 )\n",
      "[Epoch: 23240] train loss: 0.2005, train acc: 0.9288, val loss: 0.5193, val acc: 0.9457  (best train acc: 0.9421, best val acc: 0.9545, best train loss: 0.1669  @ epoch 23025 )\n",
      "[Epoch: 23260] train loss: 0.1762, train acc: 0.9372, val loss: 0.5406, val acc: 0.9487  (best train acc: 0.9421, best val acc: 0.9545, best train loss: 0.1669  @ epoch 23025 )\n",
      "[Epoch: 23280] train loss: 0.1865, train acc: 0.9338, val loss: 0.5187, val acc: 0.9481  (best train acc: 0.9421, best val acc: 0.9545, best train loss: 0.1669  @ epoch 23025 )\n",
      "[Epoch: 23300] train loss: 0.1921, train acc: 0.9272, val loss: 0.4838, val acc: 0.9390  (best train acc: 0.9421, best val acc: 0.9545, best train loss: 0.1669  @ epoch 23025 )\n",
      "[Epoch: 23320] train loss: 0.2053, train acc: 0.9259, val loss: 0.5242, val acc: 0.9444  (best train acc: 0.9421, best val acc: 0.9545, best train loss: 0.1669  @ epoch 23025 )\n",
      "[Epoch: 23340] train loss: 0.2146, train acc: 0.9198, val loss: 0.5025, val acc: 0.9491  (best train acc: 0.9421, best val acc: 0.9545, best train loss: 0.1669  @ epoch 23025 )\n",
      "[Epoch: 23360] train loss: 0.1914, train acc: 0.9294, val loss: 0.5073, val acc: 0.9487  (best train acc: 0.9421, best val acc: 0.9545, best train loss: 0.1669  @ epoch 23025 )\n",
      "[Epoch: 23380] train loss: 0.1923, train acc: 0.9310, val loss: 0.5338, val acc: 0.9460  (best train acc: 0.9421, best val acc: 0.9545, best train loss: 0.1669  @ epoch 23025 )\n",
      "[Epoch: 23400] train loss: 0.1765, train acc: 0.9404, val loss: 0.5536, val acc: 0.9440  (best train acc: 0.9421, best val acc: 0.9545, best train loss: 0.1669  @ epoch 23025 )\n",
      "[Epoch: 23420] train loss: 0.1883, train acc: 0.9369, val loss: 0.5453, val acc: 0.9474  (best train acc: 0.9421, best val acc: 0.9545, best train loss: 0.1669  @ epoch 23025 )\n",
      "[Epoch: 23440] train loss: 0.1898, train acc: 0.9306, val loss: 0.5444, val acc: 0.9494  (best train acc: 0.9421, best val acc: 0.9545, best train loss: 0.1669  @ epoch 23025 )\n",
      "[Epoch: 23460] train loss: 0.1871, train acc: 0.9325, val loss: 0.5320, val acc: 0.9470  (best train acc: 0.9421, best val acc: 0.9545, best train loss: 0.1669  @ epoch 23025 )\n",
      "[Epoch: 23480] train loss: 0.1816, train acc: 0.9346, val loss: 0.5384, val acc: 0.9504  (best train acc: 0.9421, best val acc: 0.9545, best train loss: 0.1669  @ epoch 23025 )\n",
      "[Epoch: 23500] train loss: 0.1900, train acc: 0.9338, val loss: 0.5455, val acc: 0.9504  (best train acc: 0.9421, best val acc: 0.9545, best train loss: 0.1669  @ epoch 23025 )\n",
      "[Epoch: 23520] train loss: 0.1865, train acc: 0.9320, val loss: 0.5108, val acc: 0.9477  (best train acc: 0.9421, best val acc: 0.9545, best train loss: 0.1669  @ epoch 23025 )\n",
      "[Epoch: 23540] train loss: 0.1937, train acc: 0.9336, val loss: 0.5138, val acc: 0.9417  (best train acc: 0.9421, best val acc: 0.9545, best train loss: 0.1669  @ epoch 23025 )\n",
      "[Epoch: 23560] train loss: 0.1980, train acc: 0.9275, val loss: 0.5250, val acc: 0.9406  (best train acc: 0.9421, best val acc: 0.9545, best train loss: 0.1669  @ epoch 23025 )\n",
      "[Epoch: 23580] train loss: 0.1806, train acc: 0.9363, val loss: 0.5244, val acc: 0.9504  (best train acc: 0.9421, best val acc: 0.9545, best train loss: 0.1669  @ epoch 23025 )\n",
      "[Epoch: 23600] train loss: 0.1865, train acc: 0.9319, val loss: 0.5157, val acc: 0.9504  (best train acc: 0.9421, best val acc: 0.9545, best train loss: 0.1669  @ epoch 23025 )\n",
      "[Epoch: 23620] train loss: 0.1826, train acc: 0.9322, val loss: 0.5333, val acc: 0.9504  (best train acc: 0.9421, best val acc: 0.9545, best train loss: 0.1669  @ epoch 23025 )\n",
      "[Epoch: 23640] train loss: 0.1916, train acc: 0.9328, val loss: 0.5339, val acc: 0.9356  (best train acc: 0.9421, best val acc: 0.9545, best train loss: 0.1669  @ epoch 23025 )\n",
      "[Epoch: 23660] train loss: 0.1807, train acc: 0.9362, val loss: 0.5239, val acc: 0.9508  (best train acc: 0.9421, best val acc: 0.9545, best train loss: 0.1669  @ epoch 23025 )\n",
      "[Epoch: 23680] train loss: 0.1900, train acc: 0.9324, val loss: 0.5290, val acc: 0.9497  (best train acc: 0.9421, best val acc: 0.9545, best train loss: 0.1669  @ epoch 23025 )\n",
      "[Epoch: 23700] train loss: 0.1768, train acc: 0.9374, val loss: 0.5221, val acc: 0.9470  (best train acc: 0.9421, best val acc: 0.9545, best train loss: 0.1669  @ epoch 23025 )\n",
      "[Epoch: 23720] train loss: 0.1886, train acc: 0.9331, val loss: 0.5487, val acc: 0.9501  (best train acc: 0.9421, best val acc: 0.9545, best train loss: 0.1669  @ epoch 23025 )\n",
      "[Epoch: 23740] train loss: 0.1903, train acc: 0.9329, val loss: 0.5421, val acc: 0.9477  (best train acc: 0.9421, best val acc: 0.9545, best train loss: 0.1669  @ epoch 23025 )\n",
      "[Epoch: 23760] train loss: 0.1889, train acc: 0.9347, val loss: 0.5567, val acc: 0.9406  (best train acc: 0.9421, best val acc: 0.9545, best train loss: 0.1669  @ epoch 23025 )\n",
      "[Epoch: 23780] train loss: 0.1827, train acc: 0.9366, val loss: 0.5612, val acc: 0.9494  (best train acc: 0.9421, best val acc: 0.9545, best train loss: 0.1669  @ epoch 23025 )\n",
      "[Epoch: 23800] train loss: 0.1877, train acc: 0.9324, val loss: 0.5447, val acc: 0.9501  (best train acc: 0.9421, best val acc: 0.9545, best train loss: 0.1669  @ epoch 23025 )\n",
      "[Epoch: 23820] train loss: 0.1915, train acc: 0.9298, val loss: 0.5635, val acc: 0.9474  (best train acc: 0.9421, best val acc: 0.9545, best train loss: 0.1669  @ epoch 23025 )\n",
      "[Epoch: 23840] train loss: 0.2008, train acc: 0.9250, val loss: 0.5043, val acc: 0.9454  (best train acc: 0.9421, best val acc: 0.9545, best train loss: 0.1669  @ epoch 23025 )\n",
      "[Epoch: 23860] train loss: 0.1880, train acc: 0.9349, val loss: 0.5375, val acc: 0.9406  (best train acc: 0.9421, best val acc: 0.9545, best train loss: 0.1669  @ epoch 23025 )\n",
      "[Epoch: 23880] train loss: 0.1837, train acc: 0.9369, val loss: 0.5274, val acc: 0.9497  (best train acc: 0.9421, best val acc: 0.9545, best train loss: 0.1669  @ epoch 23025 )\n",
      "[Epoch: 23900] train loss: 0.1875, train acc: 0.9327, val loss: 0.5429, val acc: 0.9491  (best train acc: 0.9421, best val acc: 0.9545, best train loss: 0.1669  @ epoch 23025 )\n",
      "[Epoch: 23920] train loss: 0.1856, train acc: 0.9352, val loss: 0.5539, val acc: 0.9481  (best train acc: 0.9421, best val acc: 0.9545, best train loss: 0.1669  @ epoch 23025 )\n",
      "[Epoch: 23940] train loss: 0.1926, train acc: 0.9289, val loss: 0.5319, val acc: 0.9464  (best train acc: 0.9421, best val acc: 0.9545, best train loss: 0.1669  @ epoch 23025 )\n",
      "[Epoch: 23960] train loss: 0.1816, train acc: 0.9326, val loss: 0.5321, val acc: 0.9487  (best train acc: 0.9421, best val acc: 0.9545, best train loss: 0.1669  @ epoch 23025 )\n",
      "[Epoch: 23980] train loss: 0.1931, train acc: 0.9313, val loss: 0.5387, val acc: 0.9460  (best train acc: 0.9421, best val acc: 0.9545, best train loss: 0.1669  @ epoch 23025 )\n",
      "[Epoch: 24000] train loss: 0.1842, train acc: 0.9357, val loss: 0.5415, val acc: 0.9427  (best train acc: 0.9423, best val acc: 0.9545, best train loss: 0.1668  @ epoch 23998 )\n",
      "[Epoch: 24020] train loss: 0.1916, train acc: 0.9312, val loss: 0.5399, val acc: 0.9474  (best train acc: 0.9423, best val acc: 0.9545, best train loss: 0.1668  @ epoch 23998 )\n",
      "[Epoch: 24040] train loss: 0.2066, train acc: 0.9250, val loss: 0.5508, val acc: 0.9218  (best train acc: 0.9423, best val acc: 0.9545, best train loss: 0.1668  @ epoch 23998 )\n",
      "[Epoch: 24060] train loss: 0.1877, train acc: 0.9339, val loss: 0.5672, val acc: 0.9481  (best train acc: 0.9423, best val acc: 0.9545, best train loss: 0.1668  @ epoch 23998 )\n",
      "[Epoch: 24080] train loss: 0.1711, train acc: 0.9401, val loss: 0.5224, val acc: 0.9501  (best train acc: 0.9423, best val acc: 0.9545, best train loss: 0.1668  @ epoch 23998 )\n",
      "[Epoch: 24100] train loss: 0.1754, train acc: 0.9394, val loss: 0.5583, val acc: 0.9511  (best train acc: 0.9423, best val acc: 0.9545, best train loss: 0.1668  @ epoch 23998 )\n",
      "[Epoch: 24120] train loss: 0.1839, train acc: 0.9352, val loss: 0.5420, val acc: 0.9470  (best train acc: 0.9423, best val acc: 0.9545, best train loss: 0.1668  @ epoch 23998 )\n",
      "[Epoch: 24140] train loss: 0.1993, train acc: 0.9273, val loss: 0.5232, val acc: 0.9457  (best train acc: 0.9423, best val acc: 0.9545, best train loss: 0.1668  @ epoch 23998 )\n",
      "[Epoch: 24160] train loss: 0.1963, train acc: 0.9331, val loss: 0.5376, val acc: 0.9430  (best train acc: 0.9423, best val acc: 0.9545, best train loss: 0.1668  @ epoch 23998 )\n",
      "[Epoch: 24180] train loss: 0.2024, train acc: 0.9287, val loss: 0.5010, val acc: 0.9477  (best train acc: 0.9423, best val acc: 0.9545, best train loss: 0.1668  @ epoch 23998 )\n",
      "[Epoch: 24200] train loss: 0.1930, train acc: 0.9295, val loss: 0.4579, val acc: 0.9491  (best train acc: 0.9423, best val acc: 0.9545, best train loss: 0.1668  @ epoch 23998 )\n",
      "[Epoch: 24220] train loss: 0.1927, train acc: 0.9307, val loss: 0.4956, val acc: 0.9437  (best train acc: 0.9423, best val acc: 0.9545, best train loss: 0.1668  @ epoch 23998 )\n",
      "[Epoch: 24240] train loss: 0.1922, train acc: 0.9326, val loss: 0.5459, val acc: 0.9454  (best train acc: 0.9423, best val acc: 0.9545, best train loss: 0.1668  @ epoch 23998 )\n",
      "[Epoch: 24260] train loss: 0.1764, train acc: 0.9375, val loss: 0.5253, val acc: 0.9481  (best train acc: 0.9423, best val acc: 0.9545, best train loss: 0.1668  @ epoch 23998 )\n",
      "[Epoch: 24280] train loss: 0.1992, train acc: 0.9297, val loss: 0.5501, val acc: 0.9487  (best train acc: 0.9423, best val acc: 0.9545, best train loss: 0.1668  @ epoch 23998 )\n",
      "[Epoch: 24300] train loss: 0.1818, train acc: 0.9359, val loss: 0.5700, val acc: 0.9545  (best train acc: 0.9423, best val acc: 0.9545, best train loss: 0.1668  @ epoch 23998 )\n",
      "[Epoch: 24320] train loss: 0.1690, train acc: 0.9393, val loss: 0.5434, val acc: 0.9518  (best train acc: 0.9423, best val acc: 0.9545, best train loss: 0.1668  @ epoch 23998 )\n",
      "[Epoch: 24340] train loss: 0.1841, train acc: 0.9341, val loss: 0.5444, val acc: 0.9460  (best train acc: 0.9434, best val acc: 0.9545, best train loss: 0.1668  @ epoch 23998 )\n",
      "[Epoch: 24360] train loss: 0.1754, train acc: 0.9370, val loss: 0.5705, val acc: 0.9508  (best train acc: 0.9434, best val acc: 0.9545, best train loss: 0.1668  @ epoch 23998 )\n",
      "[Epoch: 24380] train loss: 0.1776, train acc: 0.9396, val loss: 0.5817, val acc: 0.9514  (best train acc: 0.9434, best val acc: 0.9545, best train loss: 0.1668  @ epoch 23998 )\n",
      "[Epoch: 24400] train loss: 0.1850, train acc: 0.9338, val loss: 0.5976, val acc: 0.9514  (best train acc: 0.9434, best val acc: 0.9545, best train loss: 0.1668  @ epoch 23998 )\n",
      "[Epoch: 24420] train loss: 0.1840, train acc: 0.9339, val loss: 0.5354, val acc: 0.9487  (best train acc: 0.9434, best val acc: 0.9545, best train loss: 0.1668  @ epoch 23998 )\n",
      "[Epoch: 24440] train loss: 0.1661, train acc: 0.9398, val loss: 0.5581, val acc: 0.9437  (best train acc: 0.9434, best val acc: 0.9545, best train loss: 0.1661  @ epoch 24440 )\n",
      "[Epoch: 24460] train loss: 0.1786, train acc: 0.9373, val loss: 0.5640, val acc: 0.9501  (best train acc: 0.9434, best val acc: 0.9545, best train loss: 0.1661  @ epoch 24458 )\n",
      "[Epoch: 24480] train loss: 0.1849, train acc: 0.9346, val loss: 0.5584, val acc: 0.9420  (best train acc: 0.9434, best val acc: 0.9545, best train loss: 0.1661  @ epoch 24458 )\n",
      "[Epoch: 24500] train loss: 0.1802, train acc: 0.9349, val loss: 0.5260, val acc: 0.9484  (best train acc: 0.9434, best val acc: 0.9545, best train loss: 0.1661  @ epoch 24458 )\n",
      "[Epoch: 24520] train loss: 0.1842, train acc: 0.9356, val loss: 0.5505, val acc: 0.9508  (best train acc: 0.9434, best val acc: 0.9545, best train loss: 0.1661  @ epoch 24458 )\n",
      "[Epoch: 24540] train loss: 0.1889, train acc: 0.9308, val loss: 0.5548, val acc: 0.9494  (best train acc: 0.9434, best val acc: 0.9545, best train loss: 0.1661  @ epoch 24458 )\n",
      "[Epoch: 24560] train loss: 0.1922, train acc: 0.9305, val loss: 0.5481, val acc: 0.9501  (best train acc: 0.9434, best val acc: 0.9545, best train loss: 0.1661  @ epoch 24458 )\n",
      "[Epoch: 24580] train loss: 0.1831, train acc: 0.9352, val loss: 0.5748, val acc: 0.9400  (best train acc: 0.9434, best val acc: 0.9545, best train loss: 0.1661  @ epoch 24458 )\n",
      "[Epoch: 24600] train loss: 0.1900, train acc: 0.9289, val loss: 0.5664, val acc: 0.9484  (best train acc: 0.9434, best val acc: 0.9545, best train loss: 0.1661  @ epoch 24458 )\n",
      "[Epoch: 24620] train loss: 0.1760, train acc: 0.9383, val loss: 0.5643, val acc: 0.9484  (best train acc: 0.9434, best val acc: 0.9545, best train loss: 0.1661  @ epoch 24458 )\n",
      "[Epoch: 24640] train loss: 0.1862, train acc: 0.9314, val loss: 0.5812, val acc: 0.9481  (best train acc: 0.9434, best val acc: 0.9545, best train loss: 0.1661  @ epoch 24458 )\n",
      "[Epoch: 24660] train loss: 0.1907, train acc: 0.9305, val loss: 0.6188, val acc: 0.9457  (best train acc: 0.9434, best val acc: 0.9545, best train loss: 0.1661  @ epoch 24458 )\n",
      "[Epoch: 24680] train loss: 0.1963, train acc: 0.9273, val loss: 0.5001, val acc: 0.9497  (best train acc: 0.9434, best val acc: 0.9545, best train loss: 0.1661  @ epoch 24458 )\n",
      "[Epoch: 24700] train loss: 0.1698, train acc: 0.9378, val loss: 0.5902, val acc: 0.9484  (best train acc: 0.9434, best val acc: 0.9545, best train loss: 0.1661  @ epoch 24458 )\n",
      "[Epoch: 24720] train loss: 0.1910, train acc: 0.9310, val loss: 0.5697, val acc: 0.9467  (best train acc: 0.9434, best val acc: 0.9545, best train loss: 0.1661  @ epoch 24458 )\n",
      "[Epoch: 24740] train loss: 0.1871, train acc: 0.9336, val loss: 0.6104, val acc: 0.9524  (best train acc: 0.9434, best val acc: 0.9545, best train loss: 0.1661  @ epoch 24458 )\n",
      "[Epoch: 24760] train loss: 0.1811, train acc: 0.9333, val loss: 0.5877, val acc: 0.9528  (best train acc: 0.9434, best val acc: 0.9545, best train loss: 0.1661  @ epoch 24458 )\n",
      "[Epoch: 24780] train loss: 0.2050, train acc: 0.9242, val loss: 0.6032, val acc: 0.9474  (best train acc: 0.9434, best val acc: 0.9545, best train loss: 0.1661  @ epoch 24458 )\n",
      "[Epoch: 24800] train loss: 0.1843, train acc: 0.9374, val loss: 0.5406, val acc: 0.9491  (best train acc: 0.9434, best val acc: 0.9545, best train loss: 0.1661  @ epoch 24458 )\n",
      "[Epoch: 24820] train loss: 0.1970, train acc: 0.9310, val loss: 0.5347, val acc: 0.9467  (best train acc: 0.9434, best val acc: 0.9545, best train loss: 0.1661  @ epoch 24458 )\n",
      "[Epoch: 24840] train loss: 0.1883, train acc: 0.9320, val loss: 0.6026, val acc: 0.9477  (best train acc: 0.9434, best val acc: 0.9545, best train loss: 0.1661  @ epoch 24458 )\n",
      "[Epoch: 24860] train loss: 0.2327, train acc: 0.9218, val loss: 0.2310, val acc: 0.9497  (best train acc: 0.9434, best val acc: 0.9545, best train loss: 0.1661  @ epoch 24458 )\n",
      "[Epoch: 24880] train loss: 0.1978, train acc: 0.9281, val loss: 0.3715, val acc: 0.9484  (best train acc: 0.9434, best val acc: 0.9545, best train loss: 0.1661  @ epoch 24458 )\n",
      "[Epoch: 24900] train loss: 0.2118, train acc: 0.9239, val loss: 0.6068, val acc: 0.9467  (best train acc: 0.9434, best val acc: 0.9545, best train loss: 0.1661  @ epoch 24458 )\n",
      "[Epoch: 24920] train loss: 0.2010, train acc: 0.9284, val loss: 0.5452, val acc: 0.9420  (best train acc: 0.9434, best val acc: 0.9545, best train loss: 0.1661  @ epoch 24458 )\n",
      "[Epoch: 24940] train loss: 0.1897, train acc: 0.9307, val loss: 0.5106, val acc: 0.9504  (best train acc: 0.9434, best val acc: 0.9545, best train loss: 0.1661  @ epoch 24458 )\n",
      "[Epoch: 24960] train loss: 0.1888, train acc: 0.9343, val loss: 0.5093, val acc: 0.9420  (best train acc: 0.9434, best val acc: 0.9545, best train loss: 0.1661  @ epoch 24458 )\n",
      "[Epoch: 24980] train loss: 0.1810, train acc: 0.9341, val loss: 0.5264, val acc: 0.9481  (best train acc: 0.9434, best val acc: 0.9545, best train loss: 0.1661  @ epoch 24458 )\n",
      "[Epoch: 25000] train loss: 0.1851, train acc: 0.9350, val loss: 0.5193, val acc: 0.9423  (best train acc: 0.9434, best val acc: 0.9545, best train loss: 0.1661  @ epoch 24458 )\n",
      "[Epoch: 25020] train loss: 0.1888, train acc: 0.9311, val loss: 0.5297, val acc: 0.9487  (best train acc: 0.9434, best val acc: 0.9545, best train loss: 0.1661  @ epoch 24458 )\n",
      "[Epoch: 25040] train loss: 0.2018, train acc: 0.9281, val loss: 0.5013, val acc: 0.9390  (best train acc: 0.9434, best val acc: 0.9545, best train loss: 0.1661  @ epoch 24458 )\n",
      "[Epoch: 25060] train loss: 0.1841, train acc: 0.9333, val loss: 0.5407, val acc: 0.9524  (best train acc: 0.9434, best val acc: 0.9545, best train loss: 0.1661  @ epoch 24458 )\n",
      "[Epoch: 25080] train loss: 0.1731, train acc: 0.9384, val loss: 0.5185, val acc: 0.9467  (best train acc: 0.9434, best val acc: 0.9545, best train loss: 0.1661  @ epoch 24458 )\n",
      "[Epoch: 25100] train loss: 0.1767, train acc: 0.9383, val loss: 0.5486, val acc: 0.9481  (best train acc: 0.9434, best val acc: 0.9545, best train loss: 0.1661  @ epoch 24458 )\n",
      "[Epoch: 25120] train loss: 0.1699, train acc: 0.9375, val loss: 0.5236, val acc: 0.9423  (best train acc: 0.9434, best val acc: 0.9545, best train loss: 0.1661  @ epoch 24458 )\n",
      "[Epoch: 25140] train loss: 0.1719, train acc: 0.9401, val loss: 0.5396, val acc: 0.9494  (best train acc: 0.9434, best val acc: 0.9545, best train loss: 0.1661  @ epoch 24458 )\n",
      "[Epoch: 25160] train loss: 0.1777, train acc: 0.9367, val loss: 0.5022, val acc: 0.9477  (best train acc: 0.9434, best val acc: 0.9545, best train loss: 0.1661  @ epoch 24458 )\n",
      "[Epoch: 25180] train loss: 0.1826, train acc: 0.9370, val loss: 0.5662, val acc: 0.9504  (best train acc: 0.9434, best val acc: 0.9545, best train loss: 0.1661  @ epoch 24458 )\n",
      "[Epoch: 25200] train loss: 0.1753, train acc: 0.9370, val loss: 0.5718, val acc: 0.9464  (best train acc: 0.9434, best val acc: 0.9545, best train loss: 0.1661  @ epoch 24458 )\n",
      "[Epoch: 25220] train loss: 0.1867, train acc: 0.9323, val loss: 0.5334, val acc: 0.9518  (best train acc: 0.9434, best val acc: 0.9545, best train loss: 0.1661  @ epoch 24458 )\n",
      "[Epoch: 25240] train loss: 0.1804, train acc: 0.9363, val loss: 0.5677, val acc: 0.9406  (best train acc: 0.9434, best val acc: 0.9545, best train loss: 0.1661  @ epoch 24458 )\n",
      "[Epoch: 25260] train loss: 0.1786, train acc: 0.9383, val loss: 0.5645, val acc: 0.9504  (best train acc: 0.9434, best val acc: 0.9545, best train loss: 0.1661  @ epoch 24458 )\n",
      "[Epoch: 25280] train loss: 0.1830, train acc: 0.9348, val loss: 0.5498, val acc: 0.9511  (best train acc: 0.9434, best val acc: 0.9545, best train loss: 0.1661  @ epoch 24458 )\n",
      "[Epoch: 25300] train loss: 0.1869, train acc: 0.9312, val loss: 0.5491, val acc: 0.9501  (best train acc: 0.9434, best val acc: 0.9545, best train loss: 0.1661  @ epoch 24458 )\n",
      "[Epoch: 25320] train loss: 0.1736, train acc: 0.9401, val loss: 0.5688, val acc: 0.9373  (best train acc: 0.9434, best val acc: 0.9545, best train loss: 0.1661  @ epoch 24458 )\n",
      "[Epoch: 25340] train loss: 0.2024, train acc: 0.9292, val loss: 0.5582, val acc: 0.9400  (best train acc: 0.9434, best val acc: 0.9545, best train loss: 0.1661  @ epoch 24458 )\n",
      "[Epoch: 25360] train loss: 0.1769, train acc: 0.9386, val loss: 0.5212, val acc: 0.9460  (best train acc: 0.9434, best val acc: 0.9545, best train loss: 0.1661  @ epoch 24458 )\n",
      "[Epoch: 25380] train loss: 0.1716, train acc: 0.9415, val loss: 0.5595, val acc: 0.9538  (best train acc: 0.9434, best val acc: 0.9545, best train loss: 0.1661  @ epoch 24458 )\n",
      "[Epoch: 25400] train loss: 0.1795, train acc: 0.9370, val loss: 0.5596, val acc: 0.9470  (best train acc: 0.9434, best val acc: 0.9545, best train loss: 0.1661  @ epoch 24458 )\n",
      "[Epoch: 25420] train loss: 0.1884, train acc: 0.9315, val loss: 0.5497, val acc: 0.9501  (best train acc: 0.9434, best val acc: 0.9545, best train loss: 0.1661  @ epoch 24458 )\n",
      "[Epoch: 25440] train loss: 0.1771, train acc: 0.9409, val loss: 0.5727, val acc: 0.9457  (best train acc: 0.9434, best val acc: 0.9545, best train loss: 0.1661  @ epoch 24458 )\n",
      "[Epoch: 25460] train loss: 0.1903, train acc: 0.9320, val loss: 0.5624, val acc: 0.9342  (best train acc: 0.9434, best val acc: 0.9548, best train loss: 0.1647  @ epoch 25454 )\n",
      "[Epoch: 25480] train loss: 0.1912, train acc: 0.9295, val loss: 0.5905, val acc: 0.9487  (best train acc: 0.9434, best val acc: 0.9548, best train loss: 0.1647  @ epoch 25454 )\n",
      "[Epoch: 25500] train loss: 0.1894, train acc: 0.9334, val loss: 0.6160, val acc: 0.9467  (best train acc: 0.9434, best val acc: 0.9548, best train loss: 0.1647  @ epoch 25454 )\n",
      "[Epoch: 25520] train loss: 0.2001, train acc: 0.9260, val loss: 0.5736, val acc: 0.9447  (best train acc: 0.9434, best val acc: 0.9548, best train loss: 0.1647  @ epoch 25454 )\n",
      "[Epoch: 25540] train loss: 0.1892, train acc: 0.9312, val loss: 0.6108, val acc: 0.9511  (best train acc: 0.9434, best val acc: 0.9548, best train loss: 0.1647  @ epoch 25454 )\n",
      "[Epoch: 25560] train loss: 0.1758, train acc: 0.9359, val loss: 0.5700, val acc: 0.9447  (best train acc: 0.9434, best val acc: 0.9548, best train loss: 0.1647  @ epoch 25454 )\n",
      "[Epoch: 25580] train loss: 0.1876, train acc: 0.9340, val loss: 0.5724, val acc: 0.9501  (best train acc: 0.9434, best val acc: 0.9548, best train loss: 0.1647  @ epoch 25454 )\n",
      "[Epoch: 25600] train loss: 0.1898, train acc: 0.9320, val loss: 0.5698, val acc: 0.9440  (best train acc: 0.9434, best val acc: 0.9548, best train loss: 0.1647  @ epoch 25454 )\n",
      "[Epoch: 25620] train loss: 0.1976, train acc: 0.9307, val loss: 0.6003, val acc: 0.9504  (best train acc: 0.9434, best val acc: 0.9548, best train loss: 0.1647  @ epoch 25454 )\n",
      "[Epoch: 25640] train loss: 0.1891, train acc: 0.9324, val loss: 0.6041, val acc: 0.9470  (best train acc: 0.9434, best val acc: 0.9548, best train loss: 0.1647  @ epoch 25454 )\n",
      "[Epoch: 25660] train loss: 0.1878, train acc: 0.9284, val loss: 0.6031, val acc: 0.9484  (best train acc: 0.9434, best val acc: 0.9548, best train loss: 0.1647  @ epoch 25454 )\n",
      "[Epoch: 25680] train loss: 0.1856, train acc: 0.9344, val loss: 0.5736, val acc: 0.9464  (best train acc: 0.9434, best val acc: 0.9548, best train loss: 0.1647  @ epoch 25454 )\n",
      "[Epoch: 25700] train loss: 0.1749, train acc: 0.9391, val loss: 0.5863, val acc: 0.9494  (best train acc: 0.9434, best val acc: 0.9548, best train loss: 0.1647  @ epoch 25454 )\n",
      "[Epoch: 25720] train loss: 0.1775, train acc: 0.9346, val loss: 0.6056, val acc: 0.9460  (best train acc: 0.9434, best val acc: 0.9548, best train loss: 0.1647  @ epoch 25454 )\n",
      "[Epoch: 25740] train loss: 0.2011, train acc: 0.9284, val loss: 0.6138, val acc: 0.9454  (best train acc: 0.9434, best val acc: 0.9548, best train loss: 0.1647  @ epoch 25454 )\n",
      "[Epoch: 25760] train loss: 0.1787, train acc: 0.9325, val loss: 0.6255, val acc: 0.9477  (best train acc: 0.9434, best val acc: 0.9548, best train loss: 0.1647  @ epoch 25454 )\n",
      "[Epoch: 25780] train loss: 0.1788, train acc: 0.9357, val loss: 0.6172, val acc: 0.9491  (best train acc: 0.9434, best val acc: 0.9548, best train loss: 0.1647  @ epoch 25454 )\n",
      "[Epoch: 25800] train loss: 0.1826, train acc: 0.9369, val loss: 0.6393, val acc: 0.9440  (best train acc: 0.9434, best val acc: 0.9548, best train loss: 0.1647  @ epoch 25454 )\n",
      "[Epoch: 25820] train loss: 0.1783, train acc: 0.9370, val loss: 0.6247, val acc: 0.9528  (best train acc: 0.9434, best val acc: 0.9548, best train loss: 0.1647  @ epoch 25454 )\n",
      "[Epoch: 25840] train loss: 0.1987, train acc: 0.9280, val loss: 0.5912, val acc: 0.9363  (best train acc: 0.9434, best val acc: 0.9548, best train loss: 0.1647  @ epoch 25454 )\n",
      "[Epoch: 25860] train loss: 0.1810, train acc: 0.9356, val loss: 0.6293, val acc: 0.9484  (best train acc: 0.9434, best val acc: 0.9548, best train loss: 0.1647  @ epoch 25454 )\n",
      "[Epoch: 25880] train loss: 0.1693, train acc: 0.9398, val loss: 0.6041, val acc: 0.9524  (best train acc: 0.9434, best val acc: 0.9548, best train loss: 0.1647  @ epoch 25454 )\n",
      "[Epoch: 25900] train loss: 0.2055, train acc: 0.9252, val loss: 0.6441, val acc: 0.9433  (best train acc: 0.9434, best val acc: 0.9548, best train loss: 0.1647  @ epoch 25454 )\n",
      "[Epoch: 25920] train loss: 0.1730, train acc: 0.9386, val loss: 0.6107, val acc: 0.9501  (best train acc: 0.9434, best val acc: 0.9548, best train loss: 0.1647  @ epoch 25454 )\n",
      "[Epoch: 25940] train loss: 0.1861, train acc: 0.9347, val loss: 0.5568, val acc: 0.9420  (best train acc: 0.9434, best val acc: 0.9548, best train loss: 0.1647  @ epoch 25454 )\n",
      "[Epoch: 25960] train loss: 0.1727, train acc: 0.9391, val loss: 0.5849, val acc: 0.9477  (best train acc: 0.9434, best val acc: 0.9548, best train loss: 0.1647  @ epoch 25454 )\n",
      "[Epoch: 25980] train loss: 0.1965, train acc: 0.9276, val loss: 0.5847, val acc: 0.9501  (best train acc: 0.9434, best val acc: 0.9548, best train loss: 0.1647  @ epoch 25454 )\n",
      "[Epoch: 26000] train loss: 0.1840, train acc: 0.9331, val loss: 0.6102, val acc: 0.9467  (best train acc: 0.9434, best val acc: 0.9548, best train loss: 0.1647  @ epoch 25454 )\n",
      "[Epoch: 26020] train loss: 0.1774, train acc: 0.9354, val loss: 0.5709, val acc: 0.9467  (best train acc: 0.9434, best val acc: 0.9548, best train loss: 0.1647  @ epoch 25454 )\n",
      "[Epoch: 26040] train loss: 0.1818, train acc: 0.9312, val loss: 0.5983, val acc: 0.9487  (best train acc: 0.9434, best val acc: 0.9548, best train loss: 0.1647  @ epoch 25454 )\n",
      "[Epoch: 26060] train loss: 0.1796, train acc: 0.9360, val loss: 0.6253, val acc: 0.9494  (best train acc: 0.9434, best val acc: 0.9548, best train loss: 0.1647  @ epoch 25454 )\n",
      "[Epoch: 26080] train loss: 0.1791, train acc: 0.9324, val loss: 0.6509, val acc: 0.9467  (best train acc: 0.9434, best val acc: 0.9548, best train loss: 0.1647  @ epoch 25454 )\n",
      "[Epoch: 26100] train loss: 0.1924, train acc: 0.9277, val loss: 0.6361, val acc: 0.9454  (best train acc: 0.9434, best val acc: 0.9548, best train loss: 0.1647  @ epoch 25454 )\n",
      "[Epoch: 26120] train loss: 0.1837, train acc: 0.9330, val loss: 0.6163, val acc: 0.9511  (best train acc: 0.9434, best val acc: 0.9548, best train loss: 0.1647  @ epoch 25454 )\n",
      "[Epoch: 26140] train loss: 0.1764, train acc: 0.9389, val loss: 0.6020, val acc: 0.9524  (best train acc: 0.9434, best val acc: 0.9548, best train loss: 0.1647  @ epoch 25454 )\n",
      "[Epoch: 26160] train loss: 0.1833, train acc: 0.9360, val loss: 0.6229, val acc: 0.9474  (best train acc: 0.9434, best val acc: 0.9548, best train loss: 0.1647  @ epoch 25454 )\n",
      "[Epoch: 26180] train loss: 0.1647, train acc: 0.9423, val loss: 0.6155, val acc: 0.9538  (best train acc: 0.9434, best val acc: 0.9548, best train loss: 0.1647  @ epoch 26180 )\n",
      "[Epoch: 26200] train loss: 0.2016, train acc: 0.9288, val loss: 0.6246, val acc: 0.9501  (best train acc: 0.9434, best val acc: 0.9548, best train loss: 0.1647  @ epoch 26180 )\n",
      "[Epoch: 26220] train loss: 0.2157, train acc: 0.9235, val loss: 0.6289, val acc: 0.9433  (best train acc: 0.9434, best val acc: 0.9548, best train loss: 0.1647  @ epoch 26180 )\n",
      "[Epoch: 26240] train loss: 0.1994, train acc: 0.9294, val loss: 0.5959, val acc: 0.9524  (best train acc: 0.9434, best val acc: 0.9548, best train loss: 0.1647  @ epoch 26180 )\n",
      "[Epoch: 26260] train loss: 0.1818, train acc: 0.9350, val loss: 0.6108, val acc: 0.9417  (best train acc: 0.9434, best val acc: 0.9548, best train loss: 0.1647  @ epoch 26180 )\n",
      "[Epoch: 26280] train loss: 0.1715, train acc: 0.9398, val loss: 0.6195, val acc: 0.9504  (best train acc: 0.9434, best val acc: 0.9548, best train loss: 0.1647  @ epoch 26180 )\n",
      "[Epoch: 26300] train loss: 0.1803, train acc: 0.9355, val loss: 0.5990, val acc: 0.9457  (best train acc: 0.9453, best val acc: 0.9548, best train loss: 0.1581  @ epoch 26281 )\n",
      "[Epoch: 26320] train loss: 0.1724, train acc: 0.9417, val loss: 0.5916, val acc: 0.9518  (best train acc: 0.9453, best val acc: 0.9558, best train loss: 0.1581  @ epoch 26281 )\n",
      "[Epoch: 26340] train loss: 0.1962, train acc: 0.9285, val loss: 0.6381, val acc: 0.9491  (best train acc: 0.9453, best val acc: 0.9558, best train loss: 0.1581  @ epoch 26281 )\n",
      "[Epoch: 26360] train loss: 0.1705, train acc: 0.9398, val loss: 0.6179, val acc: 0.9518  (best train acc: 0.9453, best val acc: 0.9558, best train loss: 0.1581  @ epoch 26281 )\n",
      "[Epoch: 26380] train loss: 0.1774, train acc: 0.9383, val loss: 0.6196, val acc: 0.9511  (best train acc: 0.9453, best val acc: 0.9558, best train loss: 0.1581  @ epoch 26281 )\n",
      "[Epoch: 26400] train loss: 0.1836, train acc: 0.9355, val loss: 0.6126, val acc: 0.9481  (best train acc: 0.9453, best val acc: 0.9558, best train loss: 0.1581  @ epoch 26281 )\n",
      "[Epoch: 26420] train loss: 0.1725, train acc: 0.9365, val loss: 0.6195, val acc: 0.9464  (best train acc: 0.9453, best val acc: 0.9558, best train loss: 0.1581  @ epoch 26281 )\n",
      "[Epoch: 26440] train loss: 0.1886, train acc: 0.9308, val loss: 0.6132, val acc: 0.9501  (best train acc: 0.9453, best val acc: 0.9558, best train loss: 0.1581  @ epoch 26281 )\n",
      "[Epoch: 26460] train loss: 0.1865, train acc: 0.9336, val loss: 0.6315, val acc: 0.9511  (best train acc: 0.9453, best val acc: 0.9558, best train loss: 0.1581  @ epoch 26281 )\n",
      "[Epoch: 26480] train loss: 0.1860, train acc: 0.9322, val loss: 0.5927, val acc: 0.9484  (best train acc: 0.9453, best val acc: 0.9558, best train loss: 0.1581  @ epoch 26281 )\n",
      "[Epoch: 26500] train loss: 0.1976, train acc: 0.9324, val loss: 0.6143, val acc: 0.9491  (best train acc: 0.9453, best val acc: 0.9558, best train loss: 0.1581  @ epoch 26281 )\n",
      "[Epoch: 26520] train loss: 0.1806, train acc: 0.9352, val loss: 0.5788, val acc: 0.9481  (best train acc: 0.9453, best val acc: 0.9558, best train loss: 0.1581  @ epoch 26281 )\n",
      "[Epoch: 26540] train loss: 0.1813, train acc: 0.9343, val loss: 0.6170, val acc: 0.9504  (best train acc: 0.9453, best val acc: 0.9558, best train loss: 0.1581  @ epoch 26281 )\n",
      "[Epoch: 26560] train loss: 0.1732, train acc: 0.9367, val loss: 0.6219, val acc: 0.9430  (best train acc: 0.9453, best val acc: 0.9558, best train loss: 0.1581  @ epoch 26281 )\n",
      "[Epoch: 26580] train loss: 0.1774, train acc: 0.9377, val loss: 0.6509, val acc: 0.9477  (best train acc: 0.9453, best val acc: 0.9558, best train loss: 0.1581  @ epoch 26281 )\n",
      "[Epoch: 26600] train loss: 0.1816, train acc: 0.9341, val loss: 0.6269, val acc: 0.9484  (best train acc: 0.9453, best val acc: 0.9558, best train loss: 0.1581  @ epoch 26281 )\n",
      "[Epoch: 26620] train loss: 0.1835, train acc: 0.9339, val loss: 0.5886, val acc: 0.9420  (best train acc: 0.9453, best val acc: 0.9558, best train loss: 0.1581  @ epoch 26281 )\n",
      "[Epoch: 26640] train loss: 0.1761, train acc: 0.9368, val loss: 0.6411, val acc: 0.9508  (best train acc: 0.9453, best val acc: 0.9558, best train loss: 0.1581  @ epoch 26281 )\n",
      "[Epoch: 26660] train loss: 0.1709, train acc: 0.9396, val loss: 0.6121, val acc: 0.9477  (best train acc: 0.9453, best val acc: 0.9558, best train loss: 0.1581  @ epoch 26281 )\n",
      "[Epoch: 26680] train loss: 0.1677, train acc: 0.9383, val loss: 0.6445, val acc: 0.9508  (best train acc: 0.9453, best val acc: 0.9558, best train loss: 0.1581  @ epoch 26281 )\n",
      "[Epoch: 26700] train loss: 0.1732, train acc: 0.9393, val loss: 0.6643, val acc: 0.9514  (best train acc: 0.9453, best val acc: 0.9558, best train loss: 0.1581  @ epoch 26281 )\n",
      "[Epoch: 26720] train loss: 0.1771, train acc: 0.9362, val loss: 0.6556, val acc: 0.9528  (best train acc: 0.9453, best val acc: 0.9558, best train loss: 0.1581  @ epoch 26281 )\n",
      "[Epoch: 26740] train loss: 0.1714, train acc: 0.9373, val loss: 0.6240, val acc: 0.9504  (best train acc: 0.9453, best val acc: 0.9558, best train loss: 0.1581  @ epoch 26281 )\n",
      "[Epoch: 26760] train loss: 0.2084, train acc: 0.9234, val loss: 0.5883, val acc: 0.9454  (best train acc: 0.9453, best val acc: 0.9558, best train loss: 0.1581  @ epoch 26281 )\n",
      "[Epoch: 26780] train loss: 0.1826, train acc: 0.9317, val loss: 0.6254, val acc: 0.9396  (best train acc: 0.9453, best val acc: 0.9558, best train loss: 0.1581  @ epoch 26281 )\n",
      "[Epoch: 26800] train loss: 0.1790, train acc: 0.9360, val loss: 0.6182, val acc: 0.9491  (best train acc: 0.9453, best val acc: 0.9558, best train loss: 0.1581  @ epoch 26281 )\n",
      "[Epoch: 26820] train loss: 0.1876, train acc: 0.9320, val loss: 0.6127, val acc: 0.9460  (best train acc: 0.9453, best val acc: 0.9558, best train loss: 0.1581  @ epoch 26281 )\n",
      "[Epoch: 26840] train loss: 0.1750, train acc: 0.9392, val loss: 0.6234, val acc: 0.9494  (best train acc: 0.9453, best val acc: 0.9558, best train loss: 0.1581  @ epoch 26281 )\n",
      "[Epoch: 26860] train loss: 0.1775, train acc: 0.9347, val loss: 0.6246, val acc: 0.9457  (best train acc: 0.9453, best val acc: 0.9558, best train loss: 0.1581  @ epoch 26281 )\n",
      "[Epoch: 26880] train loss: 0.1799, train acc: 0.9380, val loss: 0.5871, val acc: 0.9447  (best train acc: 0.9453, best val acc: 0.9558, best train loss: 0.1581  @ epoch 26281 )\n",
      "[Epoch: 26900] train loss: 0.1764, train acc: 0.9344, val loss: 0.6587, val acc: 0.9447  (best train acc: 0.9453, best val acc: 0.9558, best train loss: 0.1581  @ epoch 26281 )\n",
      "[Epoch: 26920] train loss: 0.1790, train acc: 0.9325, val loss: 0.6217, val acc: 0.9430  (best train acc: 0.9453, best val acc: 0.9558, best train loss: 0.1581  @ epoch 26281 )\n",
      "[Epoch: 26940] train loss: 0.1782, train acc: 0.9359, val loss: 0.6551, val acc: 0.9521  (best train acc: 0.9453, best val acc: 0.9558, best train loss: 0.1581  @ epoch 26281 )\n",
      "[Epoch: 26960] train loss: 0.1727, train acc: 0.9379, val loss: 0.6496, val acc: 0.9491  (best train acc: 0.9453, best val acc: 0.9558, best train loss: 0.1581  @ epoch 26281 )\n",
      "[Epoch: 26980] train loss: 0.1805, train acc: 0.9344, val loss: 0.6469, val acc: 0.9403  (best train acc: 0.9453, best val acc: 0.9558, best train loss: 0.1581  @ epoch 26281 )\n",
      "[Epoch: 27000] train loss: 0.1859, train acc: 0.9312, val loss: 0.6532, val acc: 0.9508  (best train acc: 0.9453, best val acc: 0.9558, best train loss: 0.1581  @ epoch 26281 )\n",
      "[Epoch: 27020] train loss: 0.1981, train acc: 0.9307, val loss: 0.6655, val acc: 0.9444  (best train acc: 0.9453, best val acc: 0.9558, best train loss: 0.1567  @ epoch 27008 )\n",
      "[Epoch: 27040] train loss: 0.1749, train acc: 0.9368, val loss: 0.6565, val acc: 0.9521  (best train acc: 0.9453, best val acc: 0.9558, best train loss: 0.1567  @ epoch 27008 )\n",
      "[Epoch: 27060] train loss: 0.1882, train acc: 0.9315, val loss: 0.6783, val acc: 0.9417  (best train acc: 0.9453, best val acc: 0.9558, best train loss: 0.1567  @ epoch 27008 )\n",
      "[Epoch: 27080] train loss: 0.1818, train acc: 0.9315, val loss: 0.6506, val acc: 0.9474  (best train acc: 0.9453, best val acc: 0.9558, best train loss: 0.1567  @ epoch 27008 )\n",
      "[Epoch: 27100] train loss: 0.1881, train acc: 0.9298, val loss: 0.6436, val acc: 0.9491  (best train acc: 0.9453, best val acc: 0.9558, best train loss: 0.1567  @ epoch 27008 )\n",
      "[Epoch: 27120] train loss: 0.1779, train acc: 0.9336, val loss: 0.6648, val acc: 0.9511  (best train acc: 0.9453, best val acc: 0.9558, best train loss: 0.1567  @ epoch 27008 )\n",
      "[Epoch: 27140] train loss: 0.1781, train acc: 0.9365, val loss: 0.6496, val acc: 0.9464  (best train acc: 0.9453, best val acc: 0.9558, best train loss: 0.1567  @ epoch 27008 )\n",
      "[Epoch: 27160] train loss: 0.1710, train acc: 0.9395, val loss: 0.6610, val acc: 0.9427  (best train acc: 0.9453, best val acc: 0.9558, best train loss: 0.1567  @ epoch 27008 )\n",
      "[Epoch: 27180] train loss: 0.1790, train acc: 0.9371, val loss: 0.6737, val acc: 0.9481  (best train acc: 0.9453, best val acc: 0.9558, best train loss: 0.1567  @ epoch 27008 )\n",
      "[Epoch: 27200] train loss: 0.1831, train acc: 0.9327, val loss: 0.6307, val acc: 0.9511  (best train acc: 0.9453, best val acc: 0.9558, best train loss: 0.1567  @ epoch 27008 )\n",
      "[Epoch: 27220] train loss: 0.1888, train acc: 0.9298, val loss: 0.6561, val acc: 0.9383  (best train acc: 0.9453, best val acc: 0.9558, best train loss: 0.1567  @ epoch 27008 )\n",
      "[Epoch: 27240] train loss: 0.1757, train acc: 0.9364, val loss: 0.6652, val acc: 0.9508  (best train acc: 0.9453, best val acc: 0.9558, best train loss: 0.1567  @ epoch 27008 )\n",
      "[Epoch: 27260] train loss: 0.1935, train acc: 0.9301, val loss: 0.6887, val acc: 0.9373  (best train acc: 0.9453, best val acc: 0.9558, best train loss: 0.1567  @ epoch 27008 )\n",
      "[Epoch: 27280] train loss: 0.1786, train acc: 0.9344, val loss: 0.5968, val acc: 0.9454  (best train acc: 0.9453, best val acc: 0.9558, best train loss: 0.1567  @ epoch 27008 )\n",
      "[Epoch: 27300] train loss: 0.1787, train acc: 0.9321, val loss: 0.6286, val acc: 0.9454  (best train acc: 0.9453, best val acc: 0.9558, best train loss: 0.1567  @ epoch 27008 )\n",
      "[Epoch: 27320] train loss: 0.1656, train acc: 0.9408, val loss: 0.6904, val acc: 0.9514  (best train acc: 0.9453, best val acc: 0.9558, best train loss: 0.1567  @ epoch 27008 )\n",
      "[Epoch: 27340] train loss: 0.1731, train acc: 0.9393, val loss: 0.6514, val acc: 0.9501  (best train acc: 0.9453, best val acc: 0.9558, best train loss: 0.1567  @ epoch 27008 )\n",
      "[Epoch: 27360] train loss: 0.1949, train acc: 0.9271, val loss: 0.6358, val acc: 0.9386  (best train acc: 0.9453, best val acc: 0.9558, best train loss: 0.1567  @ epoch 27008 )\n",
      "[Epoch: 27380] train loss: 0.1780, train acc: 0.9325, val loss: 0.6547, val acc: 0.9487  (best train acc: 0.9453, best val acc: 0.9558, best train loss: 0.1567  @ epoch 27008 )\n",
      "[Epoch: 27400] train loss: 0.1801, train acc: 0.9358, val loss: 0.5796, val acc: 0.9420  (best train acc: 0.9453, best val acc: 0.9558, best train loss: 0.1567  @ epoch 27008 )\n",
      "[Epoch: 27420] train loss: 0.1772, train acc: 0.9355, val loss: 0.6514, val acc: 0.9497  (best train acc: 0.9453, best val acc: 0.9558, best train loss: 0.1567  @ epoch 27008 )\n",
      "[Epoch: 27440] train loss: 0.1814, train acc: 0.9354, val loss: 0.6561, val acc: 0.9437  (best train acc: 0.9453, best val acc: 0.9558, best train loss: 0.1567  @ epoch 27008 )\n",
      "[Epoch: 27460] train loss: 0.1864, train acc: 0.9338, val loss: 0.6533, val acc: 0.9497  (best train acc: 0.9453, best val acc: 0.9558, best train loss: 0.1567  @ epoch 27008 )\n",
      "[Epoch: 27480] train loss: 0.1805, train acc: 0.9341, val loss: 0.6622, val acc: 0.9484  (best train acc: 0.9453, best val acc: 0.9558, best train loss: 0.1567  @ epoch 27008 )\n",
      "[Epoch: 27500] train loss: 0.1753, train acc: 0.9349, val loss: 0.6619, val acc: 0.9444  (best train acc: 0.9453, best val acc: 0.9558, best train loss: 0.1567  @ epoch 27008 )\n",
      "[Epoch: 27520] train loss: 0.1903, train acc: 0.9302, val loss: 0.6704, val acc: 0.9494  (best train acc: 0.9453, best val acc: 0.9558, best train loss: 0.1567  @ epoch 27008 )\n",
      "[Epoch: 27540] train loss: 0.1914, train acc: 0.9286, val loss: 0.6860, val acc: 0.9484  (best train acc: 0.9453, best val acc: 0.9558, best train loss: 0.1567  @ epoch 27008 )\n",
      "[Epoch: 27560] train loss: 0.1776, train acc: 0.9346, val loss: 0.6415, val acc: 0.9430  (best train acc: 0.9453, best val acc: 0.9558, best train loss: 0.1567  @ epoch 27008 )\n",
      "[Epoch: 27580] train loss: 0.1736, train acc: 0.9362, val loss: 0.6930, val acc: 0.9356  (best train acc: 0.9453, best val acc: 0.9558, best train loss: 0.1567  @ epoch 27008 )\n",
      "[Epoch: 27600] train loss: 0.1767, train acc: 0.9349, val loss: 0.6483, val acc: 0.9450  (best train acc: 0.9453, best val acc: 0.9558, best train loss: 0.1567  @ epoch 27008 )\n",
      "[Epoch: 27620] train loss: 0.1842, train acc: 0.9315, val loss: 0.6403, val acc: 0.9464  (best train acc: 0.9453, best val acc: 0.9558, best train loss: 0.1567  @ epoch 27008 )\n",
      "[Epoch: 27640] train loss: 0.1833, train acc: 0.9335, val loss: 0.6732, val acc: 0.9514  (best train acc: 0.9453, best val acc: 0.9558, best train loss: 0.1567  @ epoch 27008 )\n",
      "[Epoch: 27660] train loss: 0.1848, train acc: 0.9344, val loss: 0.6567, val acc: 0.9430  (best train acc: 0.9453, best val acc: 0.9558, best train loss: 0.1567  @ epoch 27008 )\n",
      "[Epoch: 27680] train loss: 0.1854, train acc: 0.9331, val loss: 0.6790, val acc: 0.9369  (best train acc: 0.9453, best val acc: 0.9558, best train loss: 0.1567  @ epoch 27008 )\n",
      "[Epoch: 27700] train loss: 0.2043, train acc: 0.9206, val loss: 0.6452, val acc: 0.9497  (best train acc: 0.9453, best val acc: 0.9558, best train loss: 0.1567  @ epoch 27008 )\n",
      "[Epoch: 27720] train loss: 0.1773, train acc: 0.9336, val loss: 0.6664, val acc: 0.9511  (best train acc: 0.9453, best val acc: 0.9558, best train loss: 0.1567  @ epoch 27008 )\n",
      "[Epoch: 27740] train loss: 0.1796, train acc: 0.9357, val loss: 0.7031, val acc: 0.9524  (best train acc: 0.9453, best val acc: 0.9558, best train loss: 0.1567  @ epoch 27008 )\n",
      "[Epoch: 27760] train loss: 0.1714, train acc: 0.9397, val loss: 0.6721, val acc: 0.9518  (best train acc: 0.9453, best val acc: 0.9558, best train loss: 0.1567  @ epoch 27008 )\n",
      "[Epoch: 27780] train loss: 0.1657, train acc: 0.9395, val loss: 0.6603, val acc: 0.9460  (best train acc: 0.9453, best val acc: 0.9558, best train loss: 0.1567  @ epoch 27008 )\n",
      "[Epoch: 27800] train loss: 0.1774, train acc: 0.9320, val loss: 0.6786, val acc: 0.9481  (best train acc: 0.9453, best val acc: 0.9558, best train loss: 0.1567  @ epoch 27008 )\n",
      "[Epoch: 27820] train loss: 0.1910, train acc: 0.9293, val loss: 0.6822, val acc: 0.9501  (best train acc: 0.9453, best val acc: 0.9558, best train loss: 0.1567  @ epoch 27008 )\n",
      "[Epoch: 27840] train loss: 0.1781, train acc: 0.9394, val loss: 0.6089, val acc: 0.9470  (best train acc: 0.9453, best val acc: 0.9558, best train loss: 0.1567  @ epoch 27008 )\n",
      "[Epoch: 27860] train loss: 0.1727, train acc: 0.9388, val loss: 0.6780, val acc: 0.9487  (best train acc: 0.9453, best val acc: 0.9558, best train loss: 0.1567  @ epoch 27008 )\n",
      "[Epoch: 27880] train loss: 0.1791, train acc: 0.9329, val loss: 0.6781, val acc: 0.9444  (best train acc: 0.9453, best val acc: 0.9558, best train loss: 0.1567  @ epoch 27008 )\n",
      "[Epoch: 27900] train loss: 0.1609, train acc: 0.9427, val loss: 0.6612, val acc: 0.9494  (best train acc: 0.9459, best val acc: 0.9558, best train loss: 0.1567  @ epoch 27008 )\n",
      "[Epoch: 27920] train loss: 0.2096, train acc: 0.9246, val loss: 0.6565, val acc: 0.9302  (best train acc: 0.9459, best val acc: 0.9558, best train loss: 0.1567  @ epoch 27008 )\n",
      "[Epoch: 27940] train loss: 0.1914, train acc: 0.9276, val loss: 0.6916, val acc: 0.9481  (best train acc: 0.9459, best val acc: 0.9558, best train loss: 0.1567  @ epoch 27008 )\n",
      "[Epoch: 27960] train loss: 0.1716, train acc: 0.9378, val loss: 0.6016, val acc: 0.9487  (best train acc: 0.9459, best val acc: 0.9558, best train loss: 0.1567  @ epoch 27008 )\n",
      "[Epoch: 27980] train loss: 0.1788, train acc: 0.9338, val loss: 0.6555, val acc: 0.9494  (best train acc: 0.9459, best val acc: 0.9558, best train loss: 0.1567  @ epoch 27008 )\n",
      "[Epoch: 28000] train loss: 0.1863, train acc: 0.9337, val loss: 0.6590, val acc: 0.9514  (best train acc: 0.9459, best val acc: 0.9558, best train loss: 0.1567  @ epoch 27008 )\n",
      "[Epoch: 28020] train loss: 0.1867, train acc: 0.9297, val loss: 0.6769, val acc: 0.9497  (best train acc: 0.9459, best val acc: 0.9558, best train loss: 0.1567  @ epoch 27008 )\n",
      "[Epoch: 28040] train loss: 0.1707, train acc: 0.9360, val loss: 0.6679, val acc: 0.9501  (best train acc: 0.9459, best val acc: 0.9558, best train loss: 0.1567  @ epoch 27008 )\n",
      "[Epoch: 28060] train loss: 0.1762, train acc: 0.9359, val loss: 0.6811, val acc: 0.9521  (best train acc: 0.9459, best val acc: 0.9558, best train loss: 0.1567  @ epoch 27008 )\n",
      "[Epoch: 28080] train loss: 0.1822, train acc: 0.9325, val loss: 0.6583, val acc: 0.9504  (best train acc: 0.9459, best val acc: 0.9558, best train loss: 0.1567  @ epoch 27008 )\n",
      "[Epoch: 28100] train loss: 0.1804, train acc: 0.9341, val loss: 0.7050, val acc: 0.9447  (best train acc: 0.9459, best val acc: 0.9558, best train loss: 0.1567  @ epoch 27008 )\n",
      "[Epoch: 28120] train loss: 0.1718, train acc: 0.9405, val loss: 0.6832, val acc: 0.9494  (best train acc: 0.9459, best val acc: 0.9558, best train loss: 0.1567  @ epoch 27008 )\n",
      "[Epoch: 28140] train loss: 0.1781, train acc: 0.9366, val loss: 0.7117, val acc: 0.9481  (best train acc: 0.9459, best val acc: 0.9558, best train loss: 0.1567  @ epoch 27008 )\n",
      "[Epoch: 28160] train loss: 0.1775, train acc: 0.9355, val loss: 0.6854, val acc: 0.9491  (best train acc: 0.9459, best val acc: 0.9558, best train loss: 0.1567  @ epoch 27008 )\n",
      "[Epoch: 28180] train loss: 0.1823, train acc: 0.9310, val loss: 0.7094, val acc: 0.9501  (best train acc: 0.9459, best val acc: 0.9558, best train loss: 0.1567  @ epoch 27008 )\n",
      "[Epoch: 28200] train loss: 0.1843, train acc: 0.9332, val loss: 0.6902, val acc: 0.9528  (best train acc: 0.9459, best val acc: 0.9558, best train loss: 0.1567  @ epoch 27008 )\n",
      "[Epoch: 28220] train loss: 0.1692, train acc: 0.9378, val loss: 0.7282, val acc: 0.9528  (best train acc: 0.9459, best val acc: 0.9558, best train loss: 0.1567  @ epoch 27008 )\n",
      "[Epoch: 28240] train loss: 0.1748, train acc: 0.9368, val loss: 0.6910, val acc: 0.9400  (best train acc: 0.9459, best val acc: 0.9558, best train loss: 0.1567  @ epoch 27008 )\n",
      "[Epoch: 28260] train loss: 0.1858, train acc: 0.9312, val loss: 0.7016, val acc: 0.9430  (best train acc: 0.9459, best val acc: 0.9558, best train loss: 0.1567  @ epoch 27008 )\n",
      "[Epoch: 28280] train loss: 0.1831, train acc: 0.9285, val loss: 0.7070, val acc: 0.9508  (best train acc: 0.9459, best val acc: 0.9558, best train loss: 0.1567  @ epoch 27008 )\n",
      "[Epoch: 28300] train loss: 0.1713, train acc: 0.9408, val loss: 0.7267, val acc: 0.9524  (best train acc: 0.9459, best val acc: 0.9558, best train loss: 0.1567  @ epoch 27008 )\n",
      "[Epoch: 28320] train loss: 0.1754, train acc: 0.9344, val loss: 0.6792, val acc: 0.9477  (best train acc: 0.9459, best val acc: 0.9558, best train loss: 0.1567  @ epoch 27008 )\n",
      "[Epoch: 28340] train loss: 0.1756, train acc: 0.9361, val loss: 0.6781, val acc: 0.9491  (best train acc: 0.9459, best val acc: 0.9558, best train loss: 0.1567  @ epoch 27008 )\n",
      "[Epoch: 28360] train loss: 0.1710, train acc: 0.9374, val loss: 0.7052, val acc: 0.9514  (best train acc: 0.9459, best val acc: 0.9558, best train loss: 0.1567  @ epoch 27008 )\n",
      "[Epoch: 28380] train loss: 0.1703, train acc: 0.9371, val loss: 0.7195, val acc: 0.9491  (best train acc: 0.9459, best val acc: 0.9558, best train loss: 0.1567  @ epoch 27008 )\n",
      "[Epoch: 28400] train loss: 0.1919, train acc: 0.9297, val loss: 0.7121, val acc: 0.9420  (best train acc: 0.9459, best val acc: 0.9558, best train loss: 0.1567  @ epoch 27008 )\n",
      "[Epoch: 28420] train loss: 0.1944, train acc: 0.9289, val loss: 0.7179, val acc: 0.9444  (best train acc: 0.9459, best val acc: 0.9558, best train loss: 0.1567  @ epoch 27008 )\n",
      "[Epoch: 28440] train loss: 0.1779, train acc: 0.9341, val loss: 0.7448, val acc: 0.9494  (best train acc: 0.9459, best val acc: 0.9558, best train loss: 0.1567  @ epoch 27008 )\n",
      "[Epoch: 28460] train loss: 0.1761, train acc: 0.9362, val loss: 0.7309, val acc: 0.9494  (best train acc: 0.9459, best val acc: 0.9558, best train loss: 0.1567  @ epoch 27008 )\n",
      "[Epoch: 28480] train loss: 0.1778, train acc: 0.9339, val loss: 0.7298, val acc: 0.9518  (best train acc: 0.9459, best val acc: 0.9558, best train loss: 0.1567  @ epoch 27008 )\n",
      "[Epoch: 28500] train loss: 0.1702, train acc: 0.9391, val loss: 0.7298, val acc: 0.9518  (best train acc: 0.9459, best val acc: 0.9558, best train loss: 0.1567  @ epoch 27008 )\n",
      "[Epoch: 28520] train loss: 0.2028, train acc: 0.9228, val loss: 0.7468, val acc: 0.9464  (best train acc: 0.9459, best val acc: 0.9558, best train loss: 0.1567  @ epoch 27008 )\n",
      "[Epoch: 28540] train loss: 0.1704, train acc: 0.9393, val loss: 0.6837, val acc: 0.9518  (best train acc: 0.9459, best val acc: 0.9558, best train loss: 0.1567  @ epoch 27008 )\n",
      "[Epoch: 28560] train loss: 0.1607, train acc: 0.9438, val loss: 0.7042, val acc: 0.9460  (best train acc: 0.9459, best val acc: 0.9558, best train loss: 0.1567  @ epoch 27008 )\n",
      "[Epoch: 28580] train loss: 0.1669, train acc: 0.9408, val loss: 0.6928, val acc: 0.9457  (best train acc: 0.9459, best val acc: 0.9558, best train loss: 0.1567  @ epoch 27008 )\n",
      "[Epoch: 28600] train loss: 0.1781, train acc: 0.9353, val loss: 0.6682, val acc: 0.9373  (best train acc: 0.9459, best val acc: 0.9558, best train loss: 0.1567  @ epoch 27008 )\n",
      "[Epoch: 28620] train loss: 0.1632, train acc: 0.9427, val loss: 0.7110, val acc: 0.9524  (best train acc: 0.9459, best val acc: 0.9558, best train loss: 0.1567  @ epoch 27008 )\n",
      "[Epoch: 28640] train loss: 0.1698, train acc: 0.9413, val loss: 0.7324, val acc: 0.9535  (best train acc: 0.9459, best val acc: 0.9558, best train loss: 0.1567  @ epoch 27008 )\n",
      "[Epoch: 28660] train loss: 0.1667, train acc: 0.9398, val loss: 0.7124, val acc: 0.9528  (best train acc: 0.9459, best val acc: 0.9558, best train loss: 0.1567  @ epoch 27008 )\n",
      "[Epoch: 28680] train loss: 0.1731, train acc: 0.9390, val loss: 0.7091, val acc: 0.9541  (best train acc: 0.9459, best val acc: 0.9558, best train loss: 0.1567  @ epoch 27008 )\n",
      "[Epoch: 28700] train loss: 0.1860, train acc: 0.9292, val loss: 0.6624, val acc: 0.9427  (best train acc: 0.9459, best val acc: 0.9558, best train loss: 0.1567  @ epoch 27008 )\n",
      "[Epoch: 28720] train loss: 0.1757, train acc: 0.9373, val loss: 0.7063, val acc: 0.9538  (best train acc: 0.9459, best val acc: 0.9558, best train loss: 0.1567  @ epoch 27008 )\n",
      "[Epoch: 28740] train loss: 0.1656, train acc: 0.9380, val loss: 0.6788, val acc: 0.9464  (best train acc: 0.9459, best val acc: 0.9558, best train loss: 0.1567  @ epoch 27008 )\n",
      "[Epoch: 28760] train loss: 0.1623, train acc: 0.9413, val loss: 0.7078, val acc: 0.9524  (best train acc: 0.9459, best val acc: 0.9558, best train loss: 0.1567  @ epoch 27008 )\n",
      "[Epoch: 28780] train loss: 0.1768, train acc: 0.9338, val loss: 0.6839, val acc: 0.9514  (best train acc: 0.9459, best val acc: 0.9558, best train loss: 0.1567  @ epoch 27008 )\n",
      "[Epoch: 28800] train loss: 0.1713, train acc: 0.9380, val loss: 0.7014, val acc: 0.9481  (best train acc: 0.9459, best val acc: 0.9558, best train loss: 0.1567  @ epoch 27008 )\n",
      "[Epoch: 28820] train loss: 0.1752, train acc: 0.9346, val loss: 0.7008, val acc: 0.9501  (best train acc: 0.9459, best val acc: 0.9558, best train loss: 0.1567  @ epoch 27008 )\n",
      "[Epoch: 28840] train loss: 0.1737, train acc: 0.9377, val loss: 0.7393, val acc: 0.9531  (best train acc: 0.9459, best val acc: 0.9558, best train loss: 0.1567  @ epoch 27008 )\n",
      "[Epoch: 28860] train loss: 0.1775, train acc: 0.9361, val loss: 0.6775, val acc: 0.9497  (best train acc: 0.9459, best val acc: 0.9558, best train loss: 0.1567  @ epoch 27008 )\n",
      "[Epoch: 28880] train loss: 0.1983, train acc: 0.9299, val loss: 0.6937, val acc: 0.9504  (best train acc: 0.9459, best val acc: 0.9558, best train loss: 0.1567  @ epoch 27008 )\n",
      "[Epoch: 28900] train loss: 0.1668, train acc: 0.9403, val loss: 0.6805, val acc: 0.9548  (best train acc: 0.9459, best val acc: 0.9558, best train loss: 0.1567  @ epoch 27008 )\n",
      "[Epoch: 28920] train loss: 0.1762, train acc: 0.9331, val loss: 0.6031, val acc: 0.9379  (best train acc: 0.9459, best val acc: 0.9558, best train loss: 0.1567  @ epoch 27008 )\n",
      "[Epoch: 28940] train loss: 0.1796, train acc: 0.9355, val loss: 0.6920, val acc: 0.9487  (best train acc: 0.9459, best val acc: 0.9558, best train loss: 0.1567  @ epoch 27008 )\n",
      "[Epoch: 28960] train loss: 0.1717, train acc: 0.9391, val loss: 0.7492, val acc: 0.9484  (best train acc: 0.9459, best val acc: 0.9558, best train loss: 0.1567  @ epoch 27008 )\n",
      "[Epoch: 28980] train loss: 0.1702, train acc: 0.9409, val loss: 0.7312, val acc: 0.9511  (best train acc: 0.9459, best val acc: 0.9558, best train loss: 0.1567  @ epoch 27008 )\n",
      "[Epoch: 29000] train loss: 0.1838, train acc: 0.9316, val loss: 0.6705, val acc: 0.9467  (best train acc: 0.9459, best val acc: 0.9558, best train loss: 0.1567  @ epoch 27008 )\n",
      "[Epoch: 29020] train loss: 0.1700, train acc: 0.9373, val loss: 0.7073, val acc: 0.9470  (best train acc: 0.9459, best val acc: 0.9558, best train loss: 0.1567  @ epoch 27008 )\n",
      "[Epoch: 29040] train loss: 0.1739, train acc: 0.9367, val loss: 0.7158, val acc: 0.9474  (best train acc: 0.9459, best val acc: 0.9558, best train loss: 0.1567  @ epoch 27008 )\n",
      "[Epoch: 29060] train loss: 0.1748, train acc: 0.9355, val loss: 0.7089, val acc: 0.9349  (best train acc: 0.9459, best val acc: 0.9558, best train loss: 0.1567  @ epoch 27008 )\n",
      "[Epoch: 29080] train loss: 0.1765, train acc: 0.9328, val loss: 0.7265, val acc: 0.9521  (best train acc: 0.9459, best val acc: 0.9558, best train loss: 0.1567  @ epoch 27008 )\n",
      "[Epoch: 29100] train loss: 0.1627, train acc: 0.9412, val loss: 0.7339, val acc: 0.9528  (best train acc: 0.9459, best val acc: 0.9558, best train loss: 0.1567  @ epoch 27008 )\n",
      "[Epoch: 29120] train loss: 0.1783, train acc: 0.9335, val loss: 0.7814, val acc: 0.9524  (best train acc: 0.9459, best val acc: 0.9558, best train loss: 0.1567  @ epoch 27008 )\n",
      "[Epoch: 29140] train loss: 0.2027, train acc: 0.9223, val loss: 0.7252, val acc: 0.9403  (best train acc: 0.9459, best val acc: 0.9558, best train loss: 0.1567  @ epoch 27008 )\n",
      "[Epoch: 29160] train loss: 0.1851, train acc: 0.9282, val loss: 0.7430, val acc: 0.9427  (best train acc: 0.9459, best val acc: 0.9558, best train loss: 0.1567  @ epoch 27008 )\n",
      "[Epoch: 29180] train loss: 0.1743, train acc: 0.9357, val loss: 0.7235, val acc: 0.9524  (best train acc: 0.9459, best val acc: 0.9558, best train loss: 0.1567  @ epoch 27008 )\n",
      "[Epoch: 29200] train loss: 0.1725, train acc: 0.9370, val loss: 0.7139, val acc: 0.9444  (best train acc: 0.9459, best val acc: 0.9558, best train loss: 0.1567  @ epoch 27008 )\n",
      "[Epoch: 29220] train loss: 0.1789, train acc: 0.9352, val loss: 0.7077, val acc: 0.9497  (best train acc: 0.9459, best val acc: 0.9558, best train loss: 0.1567  @ epoch 27008 )\n",
      "[Epoch: 29240] train loss: 0.1849, train acc: 0.9320, val loss: 0.7147, val acc: 0.9457  (best train acc: 0.9459, best val acc: 0.9562, best train loss: 0.1567  @ epoch 27008 )\n",
      "[Epoch: 29260] train loss: 0.1769, train acc: 0.9346, val loss: 0.7467, val acc: 0.9535  (best train acc: 0.9459, best val acc: 0.9562, best train loss: 0.1566  @ epoch 29254 )\n",
      "[Epoch: 29280] train loss: 0.2028, train acc: 0.9273, val loss: 0.7027, val acc: 0.9484  (best train acc: 0.9459, best val acc: 0.9562, best train loss: 0.1566  @ epoch 29254 )\n",
      "[Epoch: 29300] train loss: 0.1889, train acc: 0.9305, val loss: 0.7350, val acc: 0.9447  (best train acc: 0.9459, best val acc: 0.9562, best train loss: 0.1566  @ epoch 29254 )\n",
      "[Epoch: 29320] train loss: 0.1644, train acc: 0.9400, val loss: 0.6912, val acc: 0.9487  (best train acc: 0.9459, best val acc: 0.9562, best train loss: 0.1566  @ epoch 29254 )\n",
      "[Epoch: 29340] train loss: 0.1762, train acc: 0.9345, val loss: 0.6919, val acc: 0.9514  (best train acc: 0.9459, best val acc: 0.9562, best train loss: 0.1566  @ epoch 29254 )\n",
      "[Epoch: 29360] train loss: 0.1717, train acc: 0.9353, val loss: 0.7260, val acc: 0.9524  (best train acc: 0.9459, best val acc: 0.9562, best train loss: 0.1566  @ epoch 29254 )\n",
      "[Epoch: 29380] train loss: 0.1698, train acc: 0.9396, val loss: 0.7113, val acc: 0.9518  (best train acc: 0.9459, best val acc: 0.9562, best train loss: 0.1566  @ epoch 29254 )\n",
      "[Epoch: 29400] train loss: 0.1723, train acc: 0.9378, val loss: 0.7320, val acc: 0.9494  (best train acc: 0.9459, best val acc: 0.9562, best train loss: 0.1566  @ epoch 29254 )\n",
      "[Epoch: 29420] train loss: 0.1678, train acc: 0.9375, val loss: 0.7117, val acc: 0.9535  (best train acc: 0.9459, best val acc: 0.9562, best train loss: 0.1566  @ epoch 29254 )\n",
      "[Epoch: 29440] train loss: 0.1836, train acc: 0.9370, val loss: 0.7723, val acc: 0.9501  (best train acc: 0.9459, best val acc: 0.9562, best train loss: 0.1566  @ epoch 29254 )\n",
      "[Epoch: 29460] train loss: 0.1697, train acc: 0.9376, val loss: 0.7345, val acc: 0.9548  (best train acc: 0.9459, best val acc: 0.9562, best train loss: 0.1566  @ epoch 29254 )\n",
      "[Epoch: 29480] train loss: 0.1658, train acc: 0.9395, val loss: 0.7459, val acc: 0.9545  (best train acc: 0.9459, best val acc: 0.9562, best train loss: 0.1566  @ epoch 29254 )\n",
      "[Epoch: 29500] train loss: 0.1700, train acc: 0.9391, val loss: 0.7491, val acc: 0.9531  (best train acc: 0.9459, best val acc: 0.9562, best train loss: 0.1566  @ epoch 29254 )\n",
      "[Epoch: 29520] train loss: 0.1706, train acc: 0.9367, val loss: 0.7287, val acc: 0.9541  (best train acc: 0.9459, best val acc: 0.9562, best train loss: 0.1566  @ epoch 29254 )\n",
      "[Epoch: 29540] train loss: 0.1676, train acc: 0.9419, val loss: 0.7344, val acc: 0.9541  (best train acc: 0.9459, best val acc: 0.9562, best train loss: 0.1566  @ epoch 29254 )\n",
      "[Epoch: 29560] train loss: 0.1769, train acc: 0.9328, val loss: 0.7301, val acc: 0.9491  (best train acc: 0.9459, best val acc: 0.9562, best train loss: 0.1566  @ epoch 29254 )\n",
      "[Epoch: 29580] train loss: 0.1970, train acc: 0.9287, val loss: 0.7400, val acc: 0.9497  (best train acc: 0.9459, best val acc: 0.9562, best train loss: 0.1566  @ epoch 29254 )\n",
      "[Epoch: 29600] train loss: 0.1845, train acc: 0.9333, val loss: 0.7150, val acc: 0.9393  (best train acc: 0.9459, best val acc: 0.9562, best train loss: 0.1566  @ epoch 29254 )\n",
      "[Epoch: 29620] train loss: 0.1719, train acc: 0.9345, val loss: 0.7609, val acc: 0.9541  (best train acc: 0.9459, best val acc: 0.9562, best train loss: 0.1566  @ epoch 29254 )\n",
      "[Epoch: 29640] train loss: 0.1665, train acc: 0.9417, val loss: 0.7518, val acc: 0.9524  (best train acc: 0.9459, best val acc: 0.9562, best train loss: 0.1566  @ epoch 29254 )\n",
      "[Epoch: 29660] train loss: 0.1683, train acc: 0.9382, val loss: 0.7497, val acc: 0.9477  (best train acc: 0.9459, best val acc: 0.9562, best train loss: 0.1566  @ epoch 29254 )\n",
      "[Epoch: 29680] train loss: 0.1806, train acc: 0.9338, val loss: 0.7263, val acc: 0.9467  (best train acc: 0.9459, best val acc: 0.9562, best train loss: 0.1566  @ epoch 29254 )\n",
      "[Epoch: 29700] train loss: 0.1746, train acc: 0.9370, val loss: 0.7597, val acc: 0.9538  (best train acc: 0.9459, best val acc: 0.9562, best train loss: 0.1566  @ epoch 29254 )\n",
      "[Epoch: 29720] train loss: 0.1798, train acc: 0.9300, val loss: 0.7560, val acc: 0.9477  (best train acc: 0.9459, best val acc: 0.9562, best train loss: 0.1566  @ epoch 29254 )\n",
      "[Epoch: 29740] train loss: 0.1743, train acc: 0.9374, val loss: 0.7603, val acc: 0.9464  (best train acc: 0.9459, best val acc: 0.9562, best train loss: 0.1566  @ epoch 29254 )\n",
      "[Epoch: 29760] train loss: 0.1712, train acc: 0.9391, val loss: 0.7451, val acc: 0.9504  (best train acc: 0.9459, best val acc: 0.9562, best train loss: 0.1566  @ epoch 29254 )\n",
      "[Epoch: 29780] train loss: 0.1681, train acc: 0.9391, val loss: 0.7511, val acc: 0.9376  (best train acc: 0.9459, best val acc: 0.9562, best train loss: 0.1566  @ epoch 29254 )\n",
      "[Epoch: 29800] train loss: 0.1858, train acc: 0.9357, val loss: 0.7147, val acc: 0.9487  (best train acc: 0.9459, best val acc: 0.9562, best train loss: 0.1566  @ epoch 29254 )\n",
      "[Epoch: 29820] train loss: 0.1736, train acc: 0.9377, val loss: 0.7392, val acc: 0.9484  (best train acc: 0.9459, best val acc: 0.9562, best train loss: 0.1566  @ epoch 29254 )\n",
      "[Epoch: 29840] train loss: 0.1738, train acc: 0.9357, val loss: 0.7590, val acc: 0.9508  (best train acc: 0.9459, best val acc: 0.9562, best train loss: 0.1566  @ epoch 29254 )\n",
      "[Epoch: 29860] train loss: 0.1759, train acc: 0.9372, val loss: 0.7727, val acc: 0.9440  (best train acc: 0.9459, best val acc: 0.9562, best train loss: 0.1566  @ epoch 29254 )\n",
      "[Epoch: 29880] train loss: 0.1636, train acc: 0.9394, val loss: 0.7203, val acc: 0.9464  (best train acc: 0.9459, best val acc: 0.9562, best train loss: 0.1566  @ epoch 29254 )\n",
      "[Epoch: 29900] train loss: 0.1723, train acc: 0.9362, val loss: 0.7230, val acc: 0.9363  (best train acc: 0.9459, best val acc: 0.9562, best train loss: 0.1566  @ epoch 29254 )\n",
      "[Epoch: 29920] train loss: 0.1808, train acc: 0.9318, val loss: 0.7820, val acc: 0.9538  (best train acc: 0.9459, best val acc: 0.9562, best train loss: 0.1566  @ epoch 29254 )\n",
      "[Epoch: 29940] train loss: 0.1779, train acc: 0.9310, val loss: 0.7409, val acc: 0.9457  (best train acc: 0.9459, best val acc: 0.9562, best train loss: 0.1566  @ epoch 29254 )\n",
      "[Epoch: 29960] train loss: 0.1761, train acc: 0.9355, val loss: 0.7370, val acc: 0.9524  (best train acc: 0.9459, best val acc: 0.9562, best train loss: 0.1566  @ epoch 29254 )\n",
      "[Epoch: 29980] train loss: 0.1628, train acc: 0.9402, val loss: 0.7220, val acc: 0.9474  (best train acc: 0.9459, best val acc: 0.9562, best train loss: 0.1566  @ epoch 29254 )\n",
      "[Epoch: 30000] train loss: 0.1736, train acc: 0.9341, val loss: 0.7044, val acc: 0.9474  (best train acc: 0.9459, best val acc: 0.9562, best train loss: 0.1566  @ epoch 29254 )\n",
      "[Epoch: 30020] train loss: 0.1642, train acc: 0.9406, val loss: 0.7473, val acc: 0.9551  (best train acc: 0.9459, best val acc: 0.9562, best train loss: 0.1566  @ epoch 29254 )\n",
      "[Epoch: 30040] train loss: 0.1812, train acc: 0.9321, val loss: 0.7007, val acc: 0.9535  (best train acc: 0.9459, best val acc: 0.9562, best train loss: 0.1566  @ epoch 29254 )\n",
      "[Epoch: 30060] train loss: 0.1885, train acc: 0.9299, val loss: 0.7614, val acc: 0.9508  (best train acc: 0.9459, best val acc: 0.9562, best train loss: 0.1566  @ epoch 29254 )\n",
      "[Epoch: 30080] train loss: 0.1737, train acc: 0.9370, val loss: 0.7451, val acc: 0.9417  (best train acc: 0.9459, best val acc: 0.9562, best train loss: 0.1566  @ epoch 29254 )\n",
      "[Epoch: 30100] train loss: 0.1678, train acc: 0.9401, val loss: 0.6773, val acc: 0.9518  (best train acc: 0.9459, best val acc: 0.9562, best train loss: 0.1566  @ epoch 29254 )\n",
      "[Epoch: 30120] train loss: 0.1782, train acc: 0.9374, val loss: 0.7127, val acc: 0.9518  (best train acc: 0.9459, best val acc: 0.9562, best train loss: 0.1566  @ epoch 29254 )\n",
      "[Epoch: 30140] train loss: 0.1755, train acc: 0.9353, val loss: 0.7260, val acc: 0.9298  (best train acc: 0.9459, best val acc: 0.9562, best train loss: 0.1566  @ epoch 29254 )\n",
      "[Epoch: 30160] train loss: 0.1710, train acc: 0.9374, val loss: 0.7009, val acc: 0.9528  (best train acc: 0.9459, best val acc: 0.9562, best train loss: 0.1566  @ epoch 29254 )\n",
      "[Epoch: 30180] train loss: 0.1722, train acc: 0.9396, val loss: 0.7042, val acc: 0.9528  (best train acc: 0.9459, best val acc: 0.9562, best train loss: 0.1566  @ epoch 29254 )\n",
      "[Epoch: 30200] train loss: 0.1658, train acc: 0.9394, val loss: 0.7076, val acc: 0.9437  (best train acc: 0.9459, best val acc: 0.9562, best train loss: 0.1566  @ epoch 29254 )\n",
      "[Epoch: 30220] train loss: 0.1664, train acc: 0.9372, val loss: 0.7395, val acc: 0.9484  (best train acc: 0.9459, best val acc: 0.9562, best train loss: 0.1566  @ epoch 29254 )\n",
      "[Epoch: 30240] train loss: 0.1712, train acc: 0.9401, val loss: 0.7127, val acc: 0.9491  (best train acc: 0.9459, best val acc: 0.9562, best train loss: 0.1566  @ epoch 29254 )\n",
      "[Epoch: 30260] train loss: 0.1711, train acc: 0.9387, val loss: 0.7290, val acc: 0.9470  (best train acc: 0.9459, best val acc: 0.9562, best train loss: 0.1566  @ epoch 29254 )\n",
      "[Epoch: 30280] train loss: 0.1658, train acc: 0.9408, val loss: 0.7201, val acc: 0.9487  (best train acc: 0.9459, best val acc: 0.9562, best train loss: 0.1566  @ epoch 29254 )\n",
      "[Epoch: 30300] train loss: 0.1733, train acc: 0.9377, val loss: 0.7402, val acc: 0.9551  (best train acc: 0.9459, best val acc: 0.9562, best train loss: 0.1566  @ epoch 29254 )\n",
      "[Epoch: 30320] train loss: 0.1686, train acc: 0.9358, val loss: 0.7208, val acc: 0.9528  (best train acc: 0.9459, best val acc: 0.9562, best train loss: 0.1566  @ epoch 29254 )\n",
      "[Epoch: 30340] train loss: 0.1974, train acc: 0.9244, val loss: 0.6990, val acc: 0.9420  (best train acc: 0.9459, best val acc: 0.9562, best train loss: 0.1566  @ epoch 29254 )\n",
      "[Epoch: 30360] train loss: 0.2075, train acc: 0.9196, val loss: 0.7979, val acc: 0.9444  (best train acc: 0.9459, best val acc: 0.9562, best train loss: 0.1566  @ epoch 29254 )\n",
      "[Epoch: 30380] train loss: 0.1822, train acc: 0.9355, val loss: 0.6545, val acc: 0.9514  (best train acc: 0.9459, best val acc: 0.9562, best train loss: 0.1566  @ epoch 29254 )\n",
      "[Epoch: 30400] train loss: 0.1762, train acc: 0.9344, val loss: 0.7457, val acc: 0.9481  (best train acc: 0.9459, best val acc: 0.9562, best train loss: 0.1566  @ epoch 29254 )\n",
      "[Epoch: 30420] train loss: 0.1662, train acc: 0.9384, val loss: 0.7560, val acc: 0.9518  (best train acc: 0.9459, best val acc: 0.9562, best train loss: 0.1566  @ epoch 29254 )\n",
      "[Epoch: 30440] train loss: 0.1679, train acc: 0.9383, val loss: 0.7661, val acc: 0.9521  (best train acc: 0.9459, best val acc: 0.9562, best train loss: 0.1566  @ epoch 29254 )\n",
      "[Epoch: 30460] train loss: 0.1667, train acc: 0.9373, val loss: 0.7273, val acc: 0.9450  (best train acc: 0.9459, best val acc: 0.9562, best train loss: 0.1566  @ epoch 29254 )\n",
      "[Epoch: 30480] train loss: 0.1676, train acc: 0.9404, val loss: 0.7445, val acc: 0.9477  (best train acc: 0.9459, best val acc: 0.9562, best train loss: 0.1566  @ epoch 29254 )\n",
      "[Epoch: 30500] train loss: 0.1605, train acc: 0.9414, val loss: 0.7381, val acc: 0.9484  (best train acc: 0.9459, best val acc: 0.9562, best train loss: 0.1557  @ epoch 30494 )\n",
      "[Epoch: 30520] train loss: 0.1711, train acc: 0.9352, val loss: 0.7437, val acc: 0.9497  (best train acc: 0.9459, best val acc: 0.9562, best train loss: 0.1557  @ epoch 30494 )\n",
      "[Epoch: 30540] train loss: 0.1899, train acc: 0.9288, val loss: 0.7555, val acc: 0.9535  (best train acc: 0.9459, best val acc: 0.9562, best train loss: 0.1557  @ epoch 30494 )\n",
      "[Epoch: 30560] train loss: 0.1766, train acc: 0.9340, val loss: 0.7351, val acc: 0.9524  (best train acc: 0.9459, best val acc: 0.9562, best train loss: 0.1557  @ epoch 30494 )\n",
      "[Epoch: 30580] train loss: 0.1733, train acc: 0.9348, val loss: 0.7431, val acc: 0.9474  (best train acc: 0.9459, best val acc: 0.9572, best train loss: 0.1534  @ epoch 30567 )\n",
      "[Epoch: 30600] train loss: 0.1709, train acc: 0.9372, val loss: 0.7649, val acc: 0.9477  (best train acc: 0.9459, best val acc: 0.9572, best train loss: 0.1534  @ epoch 30567 )\n",
      "[Epoch: 30620] train loss: 0.1698, train acc: 0.9385, val loss: 0.7620, val acc: 0.9491  (best train acc: 0.9459, best val acc: 0.9572, best train loss: 0.1534  @ epoch 30567 )\n",
      "[Epoch: 30640] train loss: 0.1653, train acc: 0.9412, val loss: 0.7263, val acc: 0.9538  (best train acc: 0.9459, best val acc: 0.9572, best train loss: 0.1534  @ epoch 30567 )\n",
      "[Epoch: 30660] train loss: 0.1695, train acc: 0.9386, val loss: 0.7191, val acc: 0.9521  (best train acc: 0.9459, best val acc: 0.9572, best train loss: 0.1534  @ epoch 30567 )\n",
      "[Epoch: 30680] train loss: 0.1682, train acc: 0.9402, val loss: 0.7460, val acc: 0.9551  (best train acc: 0.9459, best val acc: 0.9572, best train loss: 0.1534  @ epoch 30567 )\n",
      "[Epoch: 30700] train loss: 0.1719, train acc: 0.9373, val loss: 0.7577, val acc: 0.9518  (best train acc: 0.9459, best val acc: 0.9572, best train loss: 0.1534  @ epoch 30567 )\n",
      "[Epoch: 30720] train loss: 0.1707, train acc: 0.9394, val loss: 0.7427, val acc: 0.9548  (best train acc: 0.9459, best val acc: 0.9572, best train loss: 0.1534  @ epoch 30567 )\n",
      "[Epoch: 30740] train loss: 0.1670, train acc: 0.9413, val loss: 0.7583, val acc: 0.9545  (best train acc: 0.9459, best val acc: 0.9572, best train loss: 0.1534  @ epoch 30567 )\n",
      "[Epoch: 30760] train loss: 0.1707, train acc: 0.9352, val loss: 0.7963, val acc: 0.9555  (best train acc: 0.9459, best val acc: 0.9572, best train loss: 0.1534  @ epoch 30567 )\n",
      "[Epoch: 30780] train loss: 0.1645, train acc: 0.9404, val loss: 0.7989, val acc: 0.9518  (best train acc: 0.9459, best val acc: 0.9572, best train loss: 0.1534  @ epoch 30567 )\n",
      "[Epoch: 30800] train loss: 0.1820, train acc: 0.9288, val loss: 0.7947, val acc: 0.9518  (best train acc: 0.9459, best val acc: 0.9572, best train loss: 0.1534  @ epoch 30567 )\n",
      "[Epoch: 30820] train loss: 0.1957, train acc: 0.9252, val loss: 0.7431, val acc: 0.9508  (best train acc: 0.9459, best val acc: 0.9572, best train loss: 0.1534  @ epoch 30567 )\n",
      "[Epoch: 30840] train loss: 0.1739, train acc: 0.9376, val loss: 0.7394, val acc: 0.9470  (best train acc: 0.9459, best val acc: 0.9572, best train loss: 0.1534  @ epoch 30567 )\n",
      "[Epoch: 30860] train loss: 0.1615, train acc: 0.9401, val loss: 0.7791, val acc: 0.9528  (best train acc: 0.9459, best val acc: 0.9572, best train loss: 0.1534  @ epoch 30567 )\n",
      "[Epoch: 30880] train loss: 0.1623, train acc: 0.9414, val loss: 0.7925, val acc: 0.9518  (best train acc: 0.9459, best val acc: 0.9572, best train loss: 0.1534  @ epoch 30567 )\n",
      "[Epoch: 30900] train loss: 0.1915, train acc: 0.9296, val loss: 0.7882, val acc: 0.9430  (best train acc: 0.9459, best val acc: 0.9572, best train loss: 0.1534  @ epoch 30567 )\n",
      "[Epoch: 30920] train loss: 0.1848, train acc: 0.9312, val loss: 0.6564, val acc: 0.9497  (best train acc: 0.9459, best val acc: 0.9572, best train loss: 0.1534  @ epoch 30567 )\n",
      "[Epoch: 30940] train loss: 0.1877, train acc: 0.9305, val loss: 0.6555, val acc: 0.9400  (best train acc: 0.9459, best val acc: 0.9572, best train loss: 0.1534  @ epoch 30567 )\n",
      "[Epoch: 30960] train loss: 0.1839, train acc: 0.9328, val loss: 0.6568, val acc: 0.9484  (best train acc: 0.9459, best val acc: 0.9572, best train loss: 0.1534  @ epoch 30567 )\n",
      "[Epoch: 30980] train loss: 0.1841, train acc: 0.9316, val loss: 0.6950, val acc: 0.9518  (best train acc: 0.9459, best val acc: 0.9572, best train loss: 0.1534  @ epoch 30567 )\n",
      "[Epoch: 31000] train loss: 0.1671, train acc: 0.9409, val loss: 0.7219, val acc: 0.9521  (best train acc: 0.9459, best val acc: 0.9572, best train loss: 0.1534  @ epoch 30567 )\n",
      "[Epoch: 31020] train loss: 0.1633, train acc: 0.9388, val loss: 0.7453, val acc: 0.9524  (best train acc: 0.9459, best val acc: 0.9572, best train loss: 0.1534  @ epoch 30567 )\n",
      "[Epoch: 31040] train loss: 0.1813, train acc: 0.9312, val loss: 0.7576, val acc: 0.9538  (best train acc: 0.9459, best val acc: 0.9572, best train loss: 0.1534  @ epoch 30567 )\n",
      "[Epoch: 31060] train loss: 0.1678, train acc: 0.9379, val loss: 0.7912, val acc: 0.9514  (best train acc: 0.9459, best val acc: 0.9572, best train loss: 0.1534  @ epoch 30567 )\n",
      "[Epoch: 31080] train loss: 0.1733, train acc: 0.9370, val loss: 0.7434, val acc: 0.9497  (best train acc: 0.9459, best val acc: 0.9572, best train loss: 0.1534  @ epoch 30567 )\n",
      "[Epoch: 31100] train loss: 0.1766, train acc: 0.9361, val loss: 0.7636, val acc: 0.9460  (best train acc: 0.9459, best val acc: 0.9572, best train loss: 0.1534  @ epoch 30567 )\n",
      "[Epoch: 31120] train loss: 0.1816, train acc: 0.9336, val loss: 0.7710, val acc: 0.9521  (best train acc: 0.9459, best val acc: 0.9572, best train loss: 0.1530  @ epoch 31113 )\n",
      "[Epoch: 31140] train loss: 0.1639, train acc: 0.9377, val loss: 0.7288, val acc: 0.9541  (best train acc: 0.9459, best val acc: 0.9572, best train loss: 0.1530  @ epoch 31113 )\n",
      "[Epoch: 31160] train loss: 0.1789, train acc: 0.9323, val loss: 0.7426, val acc: 0.9494  (best train acc: 0.9459, best val acc: 0.9572, best train loss: 0.1530  @ epoch 31113 )\n",
      "[Epoch: 31180] train loss: 0.1689, train acc: 0.9399, val loss: 0.7874, val acc: 0.9514  (best train acc: 0.9459, best val acc: 0.9572, best train loss: 0.1530  @ epoch 31113 )\n",
      "[Epoch: 31200] train loss: 0.1793, train acc: 0.9336, val loss: 0.7882, val acc: 0.9518  (best train acc: 0.9459, best val acc: 0.9572, best train loss: 0.1530  @ epoch 31113 )\n",
      "[Epoch: 31220] train loss: 0.1780, train acc: 0.9352, val loss: 0.8156, val acc: 0.9410  (best train acc: 0.9459, best val acc: 0.9572, best train loss: 0.1530  @ epoch 31113 )\n",
      "[Epoch: 31240] train loss: 0.1687, train acc: 0.9383, val loss: 0.8127, val acc: 0.9541  (best train acc: 0.9459, best val acc: 0.9572, best train loss: 0.1530  @ epoch 31113 )\n",
      "[Epoch: 31260] train loss: 0.1807, train acc: 0.9359, val loss: 0.7746, val acc: 0.9528  (best train acc: 0.9459, best val acc: 0.9572, best train loss: 0.1530  @ epoch 31113 )\n",
      "[Epoch: 31280] train loss: 0.1814, train acc: 0.9328, val loss: 0.7994, val acc: 0.9437  (best train acc: 0.9459, best val acc: 0.9572, best train loss: 0.1530  @ epoch 31113 )\n",
      "[Epoch: 31300] train loss: 0.1721, train acc: 0.9367, val loss: 0.7984, val acc: 0.9487  (best train acc: 0.9459, best val acc: 0.9572, best train loss: 0.1530  @ epoch 31113 )\n",
      "[Epoch: 31320] train loss: 0.1799, train acc: 0.9374, val loss: 0.8134, val acc: 0.9383  (best train acc: 0.9459, best val acc: 0.9572, best train loss: 0.1530  @ epoch 31113 )\n",
      "[Epoch: 31340] train loss: 0.1599, train acc: 0.9401, val loss: 0.8360, val acc: 0.9417  (best train acc: 0.9459, best val acc: 0.9572, best train loss: 0.1530  @ epoch 31113 )\n",
      "[Epoch: 31360] train loss: 0.1633, train acc: 0.9420, val loss: 0.8548, val acc: 0.9528  (best train acc: 0.9459, best val acc: 0.9572, best train loss: 0.1530  @ epoch 31113 )\n",
      "[Epoch: 31380] train loss: 0.1770, train acc: 0.9325, val loss: 0.8625, val acc: 0.9514  (best train acc: 0.9459, best val acc: 0.9572, best train loss: 0.1530  @ epoch 31113 )\n",
      "[Epoch: 31400] train loss: 0.1704, train acc: 0.9398, val loss: 0.8686, val acc: 0.9535  (best train acc: 0.9464, best val acc: 0.9572, best train loss: 0.1517  @ epoch 31398 )\n",
      "[Epoch: 31420] train loss: 0.1907, train acc: 0.9289, val loss: 0.8246, val acc: 0.9487  (best train acc: 0.9464, best val acc: 0.9572, best train loss: 0.1517  @ epoch 31398 )\n",
      "[Epoch: 31440] train loss: 0.1589, train acc: 0.9425, val loss: 0.8525, val acc: 0.9514  (best train acc: 0.9464, best val acc: 0.9572, best train loss: 0.1517  @ epoch 31398 )\n",
      "[Epoch: 31460] train loss: 0.1684, train acc: 0.9393, val loss: 0.8660, val acc: 0.9484  (best train acc: 0.9464, best val acc: 0.9572, best train loss: 0.1517  @ epoch 31398 )\n",
      "[Epoch: 31480] train loss: 0.1719, train acc: 0.9389, val loss: 0.8609, val acc: 0.9545  (best train acc: 0.9464, best val acc: 0.9578, best train loss: 0.1517  @ epoch 31398 )\n",
      "[Epoch: 31500] train loss: 0.1920, train acc: 0.9276, val loss: 0.7885, val acc: 0.9433  (best train acc: 0.9464, best val acc: 0.9578, best train loss: 0.1517  @ epoch 31398 )\n",
      "[Epoch: 31520] train loss: 0.1696, train acc: 0.9381, val loss: 0.8394, val acc: 0.9558  (best train acc: 0.9464, best val acc: 0.9578, best train loss: 0.1517  @ epoch 31398 )\n",
      "[Epoch: 31540] train loss: 0.1749, train acc: 0.9364, val loss: 0.8617, val acc: 0.9474  (best train acc: 0.9464, best val acc: 0.9578, best train loss: 0.1517  @ epoch 31398 )\n",
      "[Epoch: 31560] train loss: 0.1661, train acc: 0.9388, val loss: 0.8532, val acc: 0.9524  (best train acc: 0.9464, best val acc: 0.9578, best train loss: 0.1517  @ epoch 31398 )\n",
      "[Epoch: 31580] train loss: 0.1894, train acc: 0.9324, val loss: 0.8663, val acc: 0.9511  (best train acc: 0.9464, best val acc: 0.9578, best train loss: 0.1517  @ epoch 31398 )\n",
      "[Epoch: 31600] train loss: 0.1759, train acc: 0.9326, val loss: 0.8611, val acc: 0.9531  (best train acc: 0.9464, best val acc: 0.9578, best train loss: 0.1517  @ epoch 31398 )\n",
      "[Epoch: 31620] train loss: 0.1784, train acc: 0.9316, val loss: 0.8819, val acc: 0.9538  (best train acc: 0.9464, best val acc: 0.9578, best train loss: 0.1517  @ epoch 31398 )\n",
      "[Epoch: 31640] train loss: 0.1752, train acc: 0.9346, val loss: 0.9069, val acc: 0.9545  (best train acc: 0.9464, best val acc: 0.9578, best train loss: 0.1517  @ epoch 31398 )\n",
      "[Epoch: 31660] train loss: 0.1681, train acc: 0.9388, val loss: 0.8415, val acc: 0.9545  (best train acc: 0.9464, best val acc: 0.9578, best train loss: 0.1517  @ epoch 31398 )\n",
      "[Epoch: 31680] train loss: 0.1694, train acc: 0.9367, val loss: 0.7945, val acc: 0.9444  (best train acc: 0.9464, best val acc: 0.9578, best train loss: 0.1517  @ epoch 31398 )\n",
      "[Epoch: 31700] train loss: 0.1614, train acc: 0.9406, val loss: 0.8452, val acc: 0.9545  (best train acc: 0.9464, best val acc: 0.9578, best train loss: 0.1517  @ epoch 31398 )\n",
      "[Epoch: 31720] train loss: 0.1655, train acc: 0.9401, val loss: 0.8354, val acc: 0.9457  (best train acc: 0.9464, best val acc: 0.9578, best train loss: 0.1517  @ epoch 31398 )\n",
      "[Epoch: 31740] train loss: 0.1793, train acc: 0.9343, val loss: 0.7545, val acc: 0.9464  (best train acc: 0.9464, best val acc: 0.9578, best train loss: 0.1517  @ epoch 31398 )\n",
      "[Epoch: 31760] train loss: 0.1682, train acc: 0.9376, val loss: 0.8789, val acc: 0.9474  (best train acc: 0.9464, best val acc: 0.9578, best train loss: 0.1517  @ epoch 31398 )\n",
      "[Epoch: 31780] train loss: 0.1824, train acc: 0.9321, val loss: 0.8580, val acc: 0.9437  (best train acc: 0.9464, best val acc: 0.9578, best train loss: 0.1517  @ epoch 31398 )\n",
      "[Epoch: 31800] train loss: 0.1597, train acc: 0.9404, val loss: 0.8758, val acc: 0.9454  (best train acc: 0.9464, best val acc: 0.9578, best train loss: 0.1517  @ epoch 31398 )\n",
      "[Epoch: 31820] train loss: 0.1646, train acc: 0.9395, val loss: 0.8468, val acc: 0.9484  (best train acc: 0.9464, best val acc: 0.9578, best train loss: 0.1517  @ epoch 31398 )\n",
      "[Epoch: 31840] train loss: 0.1664, train acc: 0.9416, val loss: 0.8097, val acc: 0.9514  (best train acc: 0.9464, best val acc: 0.9578, best train loss: 0.1517  @ epoch 31398 )\n",
      "[Epoch: 31860] train loss: 0.1681, train acc: 0.9343, val loss: 0.8327, val acc: 0.9538  (best train acc: 0.9464, best val acc: 0.9578, best train loss: 0.1517  @ epoch 31398 )\n",
      "[Epoch: 31880] train loss: 0.1761, train acc: 0.9352, val loss: 0.8378, val acc: 0.9477  (best train acc: 0.9464, best val acc: 0.9578, best train loss: 0.1517  @ epoch 31398 )\n",
      "[Epoch: 31900] train loss: 0.1766, train acc: 0.9331, val loss: 0.8266, val acc: 0.9272  (best train acc: 0.9464, best val acc: 0.9578, best train loss: 0.1517  @ epoch 31398 )\n",
      "[Epoch: 31920] train loss: 0.1937, train acc: 0.9349, val loss: 0.8298, val acc: 0.9386  (best train acc: 0.9464, best val acc: 0.9578, best train loss: 0.1517  @ epoch 31398 )\n",
      "[Epoch: 31940] train loss: 0.1810, train acc: 0.9344, val loss: 0.7898, val acc: 0.9521  (best train acc: 0.9464, best val acc: 0.9578, best train loss: 0.1517  @ epoch 31398 )\n",
      "[Epoch: 31960] train loss: 0.1779, train acc: 0.9343, val loss: 0.8395, val acc: 0.9474  (best train acc: 0.9464, best val acc: 0.9578, best train loss: 0.1517  @ epoch 31398 )\n",
      "[Epoch: 31980] train loss: 0.1709, train acc: 0.9357, val loss: 0.8316, val acc: 0.9528  (best train acc: 0.9464, best val acc: 0.9578, best train loss: 0.1517  @ epoch 31398 )\n",
      "[Epoch: 32000] train loss: 0.1693, train acc: 0.9376, val loss: 0.8511, val acc: 0.9555  (best train acc: 0.9464, best val acc: 0.9578, best train loss: 0.1517  @ epoch 31398 )\n",
      "[Epoch: 32020] train loss: 0.1625, train acc: 0.9426, val loss: 0.8596, val acc: 0.9535  (best train acc: 0.9464, best val acc: 0.9578, best train loss: 0.1517  @ epoch 31398 )\n",
      "[Epoch: 32040] train loss: 0.1691, train acc: 0.9378, val loss: 0.8790, val acc: 0.9518  (best train acc: 0.9464, best val acc: 0.9578, best train loss: 0.1517  @ epoch 31398 )\n",
      "[Epoch: 32060] train loss: 0.1647, train acc: 0.9387, val loss: 0.8530, val acc: 0.9518  (best train acc: 0.9464, best val acc: 0.9578, best train loss: 0.1517  @ epoch 31398 )\n",
      "[Epoch: 32080] train loss: 0.1619, train acc: 0.9406, val loss: 0.9226, val acc: 0.9555  (best train acc: 0.9464, best val acc: 0.9578, best train loss: 0.1517  @ epoch 31398 )\n",
      "[Epoch: 32100] train loss: 0.1870, train acc: 0.9289, val loss: 0.8135, val acc: 0.9524  (best train acc: 0.9464, best val acc: 0.9578, best train loss: 0.1517  @ epoch 31398 )\n",
      "[Epoch: 32120] train loss: 0.1957, train acc: 0.9258, val loss: 0.7809, val acc: 0.9302  (best train acc: 0.9464, best val acc: 0.9578, best train loss: 0.1517  @ epoch 31398 )\n",
      "[Epoch: 32140] train loss: 0.1748, train acc: 0.9355, val loss: 0.8283, val acc: 0.9511  (best train acc: 0.9464, best val acc: 0.9578, best train loss: 0.1517  @ epoch 31398 )\n",
      "[Epoch: 32160] train loss: 0.1683, train acc: 0.9373, val loss: 0.7454, val acc: 0.9504  (best train acc: 0.9464, best val acc: 0.9578, best train loss: 0.1517  @ epoch 31398 )\n",
      "[Epoch: 32180] train loss: 0.1653, train acc: 0.9394, val loss: 0.8454, val acc: 0.9518  (best train acc: 0.9464, best val acc: 0.9578, best train loss: 0.1517  @ epoch 31398 )\n",
      "[Epoch: 32200] train loss: 0.1682, train acc: 0.9379, val loss: 0.8051, val acc: 0.9413  (best train acc: 0.9464, best val acc: 0.9578, best train loss: 0.1517  @ epoch 31398 )\n",
      "[Epoch: 32220] train loss: 0.1794, train acc: 0.9318, val loss: 0.8352, val acc: 0.9484  (best train acc: 0.9464, best val acc: 0.9578, best train loss: 0.1517  @ epoch 31398 )\n",
      "[Epoch: 32240] train loss: 0.1605, train acc: 0.9411, val loss: 0.8157, val acc: 0.9528  (best train acc: 0.9464, best val acc: 0.9578, best train loss: 0.1517  @ epoch 31398 )\n",
      "[Epoch: 32260] train loss: 0.1779, train acc: 0.9317, val loss: 0.8321, val acc: 0.9541  (best train acc: 0.9464, best val acc: 0.9578, best train loss: 0.1517  @ epoch 31398 )\n",
      "[Epoch: 32280] train loss: 0.1788, train acc: 0.9345, val loss: 0.8672, val acc: 0.9501  (best train acc: 0.9464, best val acc: 0.9578, best train loss: 0.1517  @ epoch 31398 )\n",
      "[Epoch: 32300] train loss: 0.1693, train acc: 0.9389, val loss: 0.8339, val acc: 0.9565  (best train acc: 0.9471, best val acc: 0.9578, best train loss: 0.1517  @ epoch 31398 )\n",
      "[Epoch: 32320] train loss: 0.1613, train acc: 0.9418, val loss: 0.8861, val acc: 0.9551  (best train acc: 0.9471, best val acc: 0.9578, best train loss: 0.1517  @ epoch 31398 )\n",
      "[Epoch: 32340] train loss: 0.1653, train acc: 0.9398, val loss: 0.8567, val acc: 0.9531  (best train acc: 0.9471, best val acc: 0.9578, best train loss: 0.1517  @ epoch 31398 )\n",
      "[Epoch: 32360] train loss: 0.1809, train acc: 0.9333, val loss: 0.8791, val acc: 0.9444  (best train acc: 0.9471, best val acc: 0.9578, best train loss: 0.1517  @ epoch 31398 )\n",
      "[Epoch: 32380] train loss: 0.1745, train acc: 0.9344, val loss: 0.8923, val acc: 0.9410  (best train acc: 0.9471, best val acc: 0.9578, best train loss: 0.1517  @ epoch 31398 )\n",
      "[Epoch: 32400] train loss: 0.1802, train acc: 0.9357, val loss: 0.8842, val acc: 0.9481  (best train acc: 0.9471, best val acc: 0.9578, best train loss: 0.1517  @ epoch 31398 )\n",
      "[Epoch: 32420] train loss: 0.1727, train acc: 0.9365, val loss: 0.9120, val acc: 0.9545  (best train acc: 0.9471, best val acc: 0.9578, best train loss: 0.1517  @ epoch 31398 )\n",
      "[Epoch: 32440] train loss: 0.1639, train acc: 0.9410, val loss: 0.8928, val acc: 0.9545  (best train acc: 0.9471, best val acc: 0.9578, best train loss: 0.1517  @ epoch 31398 )\n",
      "[Epoch: 32460] train loss: 0.1802, train acc: 0.9341, val loss: 0.9438, val acc: 0.9538  (best train acc: 0.9471, best val acc: 0.9578, best train loss: 0.1517  @ epoch 31398 )\n",
      "[Epoch: 32480] train loss: 0.1677, train acc: 0.9378, val loss: 0.8518, val acc: 0.9511  (best train acc: 0.9471, best val acc: 0.9578, best train loss: 0.1517  @ epoch 31398 )\n",
      "[Epoch: 32500] train loss: 0.1615, train acc: 0.9410, val loss: 0.8867, val acc: 0.9531  (best train acc: 0.9471, best val acc: 0.9578, best train loss: 0.1517  @ epoch 31398 )\n",
      "[Epoch: 32520] train loss: 0.1800, train acc: 0.9338, val loss: 0.8713, val acc: 0.9494  (best train acc: 0.9471, best val acc: 0.9578, best train loss: 0.1517  @ epoch 31398 )\n",
      "[Epoch: 32540] train loss: 0.1598, train acc: 0.9433, val loss: 0.8578, val acc: 0.9504  (best train acc: 0.9471, best val acc: 0.9578, best train loss: 0.1511  @ epoch 32538 )\n",
      "[Epoch: 32560] train loss: 0.1855, train acc: 0.9301, val loss: 0.8530, val acc: 0.9295  (best train acc: 0.9471, best val acc: 0.9578, best train loss: 0.1511  @ epoch 32538 )\n",
      "[Epoch: 32580] train loss: 0.2014, train acc: 0.9250, val loss: 0.8229, val acc: 0.9430  (best train acc: 0.9471, best val acc: 0.9578, best train loss: 0.1511  @ epoch 32538 )\n",
      "[Epoch: 32600] train loss: 0.1839, train acc: 0.9290, val loss: 0.8326, val acc: 0.9514  (best train acc: 0.9471, best val acc: 0.9578, best train loss: 0.1511  @ epoch 32538 )\n",
      "[Epoch: 32620] train loss: 0.2068, train acc: 0.9218, val loss: 0.9191, val acc: 0.9508  (best train acc: 0.9471, best val acc: 0.9578, best train loss: 0.1511  @ epoch 32538 )\n",
      "[Epoch: 32640] train loss: 0.1837, train acc: 0.9372, val loss: 0.7275, val acc: 0.9494  (best train acc: 0.9471, best val acc: 0.9578, best train loss: 0.1511  @ epoch 32538 )\n",
      "[Epoch: 32660] train loss: 0.1672, train acc: 0.9391, val loss: 0.7388, val acc: 0.9535  (best train acc: 0.9471, best val acc: 0.9578, best train loss: 0.1511  @ epoch 32538 )\n",
      "[Epoch: 32680] train loss: 0.1694, train acc: 0.9376, val loss: 0.8032, val acc: 0.9541  (best train acc: 0.9471, best val acc: 0.9578, best train loss: 0.1511  @ epoch 32538 )\n",
      "[Epoch: 32700] train loss: 0.1653, train acc: 0.9407, val loss: 0.8326, val acc: 0.9535  (best train acc: 0.9471, best val acc: 0.9578, best train loss: 0.1511  @ epoch 32538 )\n",
      "[Epoch: 32720] train loss: 0.1694, train acc: 0.9357, val loss: 0.8529, val acc: 0.9545  (best train acc: 0.9471, best val acc: 0.9578, best train loss: 0.1511  @ epoch 32538 )\n",
      "[Epoch: 32740] train loss: 0.1774, train acc: 0.9354, val loss: 0.8508, val acc: 0.9491  (best train acc: 0.9471, best val acc: 0.9578, best train loss: 0.1511  @ epoch 32538 )\n",
      "[Epoch: 32760] train loss: 0.1613, train acc: 0.9426, val loss: 0.8774, val acc: 0.9538  (best train acc: 0.9471, best val acc: 0.9578, best train loss: 0.1511  @ epoch 32538 )\n",
      "[Epoch: 32780] train loss: 0.1760, train acc: 0.9342, val loss: 0.9098, val acc: 0.9494  (best train acc: 0.9471, best val acc: 0.9578, best train loss: 0.1511  @ epoch 32538 )\n",
      "[Epoch: 32800] train loss: 0.1719, train acc: 0.9367, val loss: 0.8442, val acc: 0.9504  (best train acc: 0.9471, best val acc: 0.9578, best train loss: 0.1511  @ epoch 32538 )\n",
      "[Epoch: 32820] train loss: 0.1661, train acc: 0.9396, val loss: 0.8411, val acc: 0.9524  (best train acc: 0.9471, best val acc: 0.9578, best train loss: 0.1511  @ epoch 32538 )\n",
      "[Epoch: 32840] train loss: 0.1719, train acc: 0.9371, val loss: 0.8987, val acc: 0.9562  (best train acc: 0.9471, best val acc: 0.9578, best train loss: 0.1511  @ epoch 32538 )\n",
      "[Epoch: 32860] train loss: 0.1628, train acc: 0.9399, val loss: 0.8874, val acc: 0.9508  (best train acc: 0.9471, best val acc: 0.9578, best train loss: 0.1511  @ epoch 32538 )\n",
      "[Epoch: 32880] train loss: 0.1775, train acc: 0.9360, val loss: 0.8697, val acc: 0.9460  (best train acc: 0.9471, best val acc: 0.9592, best train loss: 0.1511  @ epoch 32538 )\n",
      "[Epoch: 32900] train loss: 0.1817, train acc: 0.9321, val loss: 0.8410, val acc: 0.9511  (best train acc: 0.9471, best val acc: 0.9592, best train loss: 0.1511  @ epoch 32538 )\n",
      "[Epoch: 32920] train loss: 0.1639, train acc: 0.9411, val loss: 0.8888, val acc: 0.9538  (best train acc: 0.9471, best val acc: 0.9592, best train loss: 0.1511  @ epoch 32538 )\n",
      "[Epoch: 32940] train loss: 0.1630, train acc: 0.9392, val loss: 0.8891, val acc: 0.9538  (best train acc: 0.9471, best val acc: 0.9592, best train loss: 0.1511  @ epoch 32538 )\n",
      "[Epoch: 32960] train loss: 0.1728, train acc: 0.9350, val loss: 0.8453, val acc: 0.9521  (best train acc: 0.9471, best val acc: 0.9592, best train loss: 0.1511  @ epoch 32538 )\n",
      "[Epoch: 32980] train loss: 0.1627, train acc: 0.9409, val loss: 0.8705, val acc: 0.9521  (best train acc: 0.9471, best val acc: 0.9592, best train loss: 0.1511  @ epoch 32538 )\n",
      "[Epoch: 33000] train loss: 0.1592, train acc: 0.9424, val loss: 0.9053, val acc: 0.9558  (best train acc: 0.9471, best val acc: 0.9592, best train loss: 0.1511  @ epoch 32538 )\n",
      "[Epoch: 33020] train loss: 0.1661, train acc: 0.9414, val loss: 0.8987, val acc: 0.9470  (best train acc: 0.9471, best val acc: 0.9592, best train loss: 0.1511  @ epoch 32538 )\n",
      "[Epoch: 33040] train loss: 0.1796, train acc: 0.9371, val loss: 0.8789, val acc: 0.9558  (best train acc: 0.9471, best val acc: 0.9592, best train loss: 0.1511  @ epoch 32538 )\n",
      "[Epoch: 33060] train loss: 0.1867, train acc: 0.9295, val loss: 0.8840, val acc: 0.9511  (best train acc: 0.9471, best val acc: 0.9592, best train loss: 0.1511  @ epoch 32538 )\n",
      "[Epoch: 33080] train loss: 0.1647, train acc: 0.9400, val loss: 0.8893, val acc: 0.9545  (best train acc: 0.9471, best val acc: 0.9592, best train loss: 0.1511  @ epoch 32538 )\n",
      "[Epoch: 33100] train loss: 0.1717, train acc: 0.9383, val loss: 0.9043, val acc: 0.9562  (best train acc: 0.9471, best val acc: 0.9592, best train loss: 0.1511  @ epoch 32538 )\n",
      "[Epoch: 33120] train loss: 0.1814, train acc: 0.9289, val loss: 0.9105, val acc: 0.9524  (best train acc: 0.9471, best val acc: 0.9592, best train loss: 0.1511  @ epoch 32538 )\n",
      "[Epoch: 33140] train loss: 0.1690, train acc: 0.9349, val loss: 0.9044, val acc: 0.9501  (best train acc: 0.9471, best val acc: 0.9592, best train loss: 0.1511  @ epoch 32538 )\n",
      "[Epoch: 33160] train loss: 0.1809, train acc: 0.9385, val loss: 0.8553, val acc: 0.9464  (best train acc: 0.9471, best val acc: 0.9592, best train loss: 0.1511  @ epoch 32538 )\n",
      "[Epoch: 33180] train loss: 0.1780, train acc: 0.9333, val loss: 0.8374, val acc: 0.9467  (best train acc: 0.9471, best val acc: 0.9592, best train loss: 0.1511  @ epoch 32538 )\n",
      "[Epoch: 33200] train loss: 0.1594, train acc: 0.9424, val loss: 0.8313, val acc: 0.9514  (best train acc: 0.9471, best val acc: 0.9592, best train loss: 0.1511  @ epoch 32538 )\n",
      "[Epoch: 33220] train loss: 0.1647, train acc: 0.9408, val loss: 0.8248, val acc: 0.9514  (best train acc: 0.9471, best val acc: 0.9592, best train loss: 0.1511  @ epoch 32538 )\n",
      "[Epoch: 33240] train loss: 0.1744, train acc: 0.9362, val loss: 0.8715, val acc: 0.9535  (best train acc: 0.9471, best val acc: 0.9592, best train loss: 0.1511  @ epoch 32538 )\n",
      "[Epoch: 33260] train loss: 0.1772, train acc: 0.9362, val loss: 0.8914, val acc: 0.9555  (best train acc: 0.9471, best val acc: 0.9592, best train loss: 0.1511  @ epoch 32538 )\n",
      "[Epoch: 33280] train loss: 0.1843, train acc: 0.9299, val loss: 0.8889, val acc: 0.9535  (best train acc: 0.9471, best val acc: 0.9592, best train loss: 0.1511  @ epoch 32538 )\n",
      "[Epoch: 33300] train loss: 0.1681, train acc: 0.9401, val loss: 0.8928, val acc: 0.9484  (best train acc: 0.9471, best val acc: 0.9592, best train loss: 0.1511  @ epoch 32538 )\n",
      "[Epoch: 33320] train loss: 0.1662, train acc: 0.9409, val loss: 0.9074, val acc: 0.9514  (best train acc: 0.9471, best val acc: 0.9592, best train loss: 0.1511  @ epoch 32538 )\n",
      "[Epoch: 33340] train loss: 0.1723, train acc: 0.9357, val loss: 0.8997, val acc: 0.9467  (best train acc: 0.9471, best val acc: 0.9592, best train loss: 0.1511  @ epoch 32538 )\n",
      "[Epoch: 33360] train loss: 0.1625, train acc: 0.9426, val loss: 0.9082, val acc: 0.9528  (best train acc: 0.9471, best val acc: 0.9592, best train loss: 0.1511  @ epoch 32538 )\n",
      "[Epoch: 33380] train loss: 0.1653, train acc: 0.9383, val loss: 0.8990, val acc: 0.9562  (best train acc: 0.9471, best val acc: 0.9592, best train loss: 0.1511  @ epoch 32538 )\n",
      "[Epoch: 33400] train loss: 0.1706, train acc: 0.9365, val loss: 0.8208, val acc: 0.9494  (best train acc: 0.9471, best val acc: 0.9592, best train loss: 0.1511  @ epoch 32538 )\n",
      "[Epoch: 33420] train loss: 0.1805, train acc: 0.9337, val loss: 0.9260, val acc: 0.9528  (best train acc: 0.9471, best val acc: 0.9592, best train loss: 0.1511  @ epoch 32538 )\n",
      "[Epoch: 33440] train loss: 0.1653, train acc: 0.9394, val loss: 0.8614, val acc: 0.9521  (best train acc: 0.9471, best val acc: 0.9592, best train loss: 0.1511  @ epoch 32538 )\n",
      "[Epoch: 33460] train loss: 0.1709, train acc: 0.9367, val loss: 0.8952, val acc: 0.9528  (best train acc: 0.9471, best val acc: 0.9592, best train loss: 0.1511  @ epoch 32538 )\n",
      "[Epoch: 33480] train loss: 0.1672, train acc: 0.9401, val loss: 0.9092, val acc: 0.9565  (best train acc: 0.9471, best val acc: 0.9592, best train loss: 0.1511  @ epoch 32538 )\n",
      "[Epoch: 33500] train loss: 0.1600, train acc: 0.9417, val loss: 0.8795, val acc: 0.9545  (best train acc: 0.9471, best val acc: 0.9592, best train loss: 0.1511  @ epoch 32538 )\n",
      "[Epoch: 33520] train loss: 0.1733, train acc: 0.9369, val loss: 0.8815, val acc: 0.9494  (best train acc: 0.9471, best val acc: 0.9592, best train loss: 0.1511  @ epoch 32538 )\n",
      "[Epoch: 33540] train loss: 0.1699, train acc: 0.9354, val loss: 0.8831, val acc: 0.9427  (best train acc: 0.9471, best val acc: 0.9592, best train loss: 0.1511  @ epoch 32538 )\n",
      "[Epoch: 33560] train loss: 0.1723, train acc: 0.9347, val loss: 0.8507, val acc: 0.9518  (best train acc: 0.9471, best val acc: 0.9592, best train loss: 0.1511  @ epoch 32538 )\n",
      "[Epoch: 33580] train loss: 0.1857, train acc: 0.9330, val loss: 0.7395, val acc: 0.9484  (best train acc: 0.9471, best val acc: 0.9592, best train loss: 0.1511  @ epoch 32538 )\n",
      "[Epoch: 33600] train loss: 0.2109, train acc: 0.9195, val loss: 0.7589, val acc: 0.9450  (best train acc: 0.9471, best val acc: 0.9592, best train loss: 0.1511  @ epoch 32538 )\n",
      "[Epoch: 33620] train loss: 0.1781, train acc: 0.9322, val loss: 0.7701, val acc: 0.9531  (best train acc: 0.9471, best val acc: 0.9592, best train loss: 0.1511  @ epoch 32538 )\n",
      "[Epoch: 33640] train loss: 0.1801, train acc: 0.9314, val loss: 0.8358, val acc: 0.9555  (best train acc: 0.9471, best val acc: 0.9592, best train loss: 0.1511  @ epoch 32538 )\n",
      "[Epoch: 33660] train loss: 0.1670, train acc: 0.9372, val loss: 0.8433, val acc: 0.9531  (best train acc: 0.9471, best val acc: 0.9592, best train loss: 0.1511  @ epoch 32538 )\n",
      "[Epoch: 33680] train loss: 0.1617, train acc: 0.9414, val loss: 0.8003, val acc: 0.9535  (best train acc: 0.9471, best val acc: 0.9592, best train loss: 0.1511  @ epoch 32538 )\n",
      "[Epoch: 33700] train loss: 0.1653, train acc: 0.9402, val loss: 0.8455, val acc: 0.9518  (best train acc: 0.9471, best val acc: 0.9592, best train loss: 0.1511  @ epoch 32538 )\n",
      "[Epoch: 33720] train loss: 0.1684, train acc: 0.9412, val loss: 0.8739, val acc: 0.9558  (best train acc: 0.9471, best val acc: 0.9592, best train loss: 0.1511  @ epoch 32538 )\n",
      "[Epoch: 33740] train loss: 0.2053, train acc: 0.9208, val loss: 0.8817, val acc: 0.9551  (best train acc: 0.9471, best val acc: 0.9592, best train loss: 0.1511  @ epoch 32538 )\n",
      "[Epoch: 33760] train loss: 0.1799, train acc: 0.9323, val loss: 0.8645, val acc: 0.9541  (best train acc: 0.9471, best val acc: 0.9592, best train loss: 0.1511  @ epoch 32538 )\n",
      "[Epoch: 33780] train loss: 0.1694, train acc: 0.9337, val loss: 0.8742, val acc: 0.9551  (best train acc: 0.9471, best val acc: 0.9592, best train loss: 0.1511  @ epoch 32538 )\n",
      "[Epoch: 33800] train loss: 0.1651, train acc: 0.9381, val loss: 0.8795, val acc: 0.9514  (best train acc: 0.9471, best val acc: 0.9592, best train loss: 0.1511  @ epoch 32538 )\n",
      "[Epoch: 33820] train loss: 0.1720, train acc: 0.9379, val loss: 0.8737, val acc: 0.9514  (best train acc: 0.9471, best val acc: 0.9592, best train loss: 0.1511  @ epoch 32538 )\n",
      "[Epoch: 33840] train loss: 0.1871, train acc: 0.9291, val loss: 0.8541, val acc: 0.9460  (best train acc: 0.9471, best val acc: 0.9592, best train loss: 0.1511  @ epoch 32538 )\n",
      "[Epoch: 33860] train loss: 0.1636, train acc: 0.9388, val loss: 0.8308, val acc: 0.9541  (best train acc: 0.9471, best val acc: 0.9592, best train loss: 0.1511  @ epoch 32538 )\n",
      "[Epoch: 33880] train loss: 0.1775, train acc: 0.9348, val loss: 0.8839, val acc: 0.9558  (best train acc: 0.9471, best val acc: 0.9592, best train loss: 0.1511  @ epoch 32538 )\n",
      "[Epoch: 33900] train loss: 0.1750, train acc: 0.9336, val loss: 0.8678, val acc: 0.9555  (best train acc: 0.9471, best val acc: 0.9592, best train loss: 0.1511  @ epoch 32538 )\n",
      "[Epoch: 33920] train loss: 0.1692, train acc: 0.9369, val loss: 0.8679, val acc: 0.9548  (best train acc: 0.9471, best val acc: 0.9592, best train loss: 0.1511  @ epoch 32538 )\n",
      "[Epoch: 33940] train loss: 0.1723, train acc: 0.9352, val loss: 0.8279, val acc: 0.9545  (best train acc: 0.9471, best val acc: 0.9592, best train loss: 0.1511  @ epoch 32538 )\n",
      "[Epoch: 33960] train loss: 0.1620, train acc: 0.9415, val loss: 0.8498, val acc: 0.9497  (best train acc: 0.9471, best val acc: 0.9592, best train loss: 0.1511  @ epoch 32538 )\n",
      "[Epoch: 33980] train loss: 0.1655, train acc: 0.9372, val loss: 0.8617, val acc: 0.9497  (best train acc: 0.9471, best val acc: 0.9592, best train loss: 0.1511  @ epoch 32538 )\n",
      "[Epoch: 34000] train loss: 0.1869, train acc: 0.9345, val loss: 0.8532, val acc: 0.9514  (best train acc: 0.9471, best val acc: 0.9592, best train loss: 0.1511  @ epoch 32538 )\n",
      "[Epoch: 34020] train loss: 0.1621, train acc: 0.9414, val loss: 0.8775, val acc: 0.9514  (best train acc: 0.9471, best val acc: 0.9592, best train loss: 0.1511  @ epoch 32538 )\n",
      "[Epoch: 34040] train loss: 0.1693, train acc: 0.9387, val loss: 0.9108, val acc: 0.9504  (best train acc: 0.9471, best val acc: 0.9592, best train loss: 0.1511  @ epoch 32538 )\n",
      "[Epoch: 34060] train loss: 0.1694, train acc: 0.9369, val loss: 0.8494, val acc: 0.9545  (best train acc: 0.9471, best val acc: 0.9592, best train loss: 0.1511  @ epoch 32538 )\n",
      "[Epoch: 34080] train loss: 0.1594, train acc: 0.9413, val loss: 0.8653, val acc: 0.9524  (best train acc: 0.9471, best val acc: 0.9592, best train loss: 0.1511  @ epoch 32538 )\n",
      "[Epoch: 34100] train loss: 0.1719, train acc: 0.9341, val loss: 0.8730, val acc: 0.9508  (best train acc: 0.9471, best val acc: 0.9592, best train loss: 0.1511  @ epoch 32538 )\n",
      "[Epoch: 34120] train loss: 0.1578, train acc: 0.9433, val loss: 0.8669, val acc: 0.9508  (best train acc: 0.9471, best val acc: 0.9592, best train loss: 0.1511  @ epoch 32538 )\n",
      "[Epoch: 34140] train loss: 0.1657, train acc: 0.9423, val loss: 0.8715, val acc: 0.9548  (best train acc: 0.9471, best val acc: 0.9592, best train loss: 0.1511  @ epoch 32538 )\n",
      "[Epoch: 34160] train loss: 0.1612, train acc: 0.9398, val loss: 0.8713, val acc: 0.9501  (best train acc: 0.9471, best val acc: 0.9592, best train loss: 0.1511  @ epoch 32538 )\n",
      "[Epoch: 34180] train loss: 0.1793, train acc: 0.9340, val loss: 0.8547, val acc: 0.9528  (best train acc: 0.9471, best val acc: 0.9592, best train loss: 0.1511  @ epoch 32538 )\n",
      "[Epoch: 34200] train loss: 0.1745, train acc: 0.9376, val loss: 0.8996, val acc: 0.9535  (best train acc: 0.9471, best val acc: 0.9592, best train loss: 0.1511  @ epoch 32538 )\n",
      "[Epoch: 34220] train loss: 0.1582, train acc: 0.9417, val loss: 0.8714, val acc: 0.9545  (best train acc: 0.9471, best val acc: 0.9592, best train loss: 0.1511  @ epoch 32538 )\n",
      "[Epoch: 34240] train loss: 0.1705, train acc: 0.9378, val loss: 0.8455, val acc: 0.9568  (best train acc: 0.9471, best val acc: 0.9592, best train loss: 0.1511  @ epoch 32538 )\n",
      "[Epoch: 34260] train loss: 0.1566, train acc: 0.9432, val loss: 0.8827, val acc: 0.9518  (best train acc: 0.9471, best val acc: 0.9592, best train loss: 0.1511  @ epoch 32538 )\n",
      "[Epoch: 34280] train loss: 0.1808, train acc: 0.9310, val loss: 0.8401, val acc: 0.9508  (best train acc: 0.9471, best val acc: 0.9592, best train loss: 0.1511  @ epoch 32538 )\n",
      "[Epoch: 34300] train loss: 0.1969, train acc: 0.9266, val loss: 0.8823, val acc: 0.9528  (best train acc: 0.9471, best val acc: 0.9592, best train loss: 0.1511  @ epoch 32538 )\n",
      "[Epoch: 34320] train loss: 0.1604, train acc: 0.9414, val loss: 0.8946, val acc: 0.9420  (best train acc: 0.9471, best val acc: 0.9592, best train loss: 0.1511  @ epoch 32538 )\n",
      "[Epoch: 34340] train loss: 0.1776, train acc: 0.9349, val loss: 0.8869, val acc: 0.9551  (best train acc: 0.9471, best val acc: 0.9592, best train loss: 0.1511  @ epoch 32538 )\n",
      "[Epoch: 34360] train loss: 0.1688, train acc: 0.9361, val loss: 0.8815, val acc: 0.9433  (best train acc: 0.9471, best val acc: 0.9592, best train loss: 0.1511  @ epoch 32538 )\n",
      "[Epoch: 34380] train loss: 0.1600, train acc: 0.9440, val loss: 0.8456, val acc: 0.9535  (best train acc: 0.9471, best val acc: 0.9592, best train loss: 0.1511  @ epoch 32538 )\n",
      "[Epoch: 34400] train loss: 0.1743, train acc: 0.9353, val loss: 0.8513, val acc: 0.9484  (best train acc: 0.9471, best val acc: 0.9592, best train loss: 0.1511  @ epoch 32538 )\n",
      "[Epoch: 34420] train loss: 0.1576, train acc: 0.9423, val loss: 0.8441, val acc: 0.9551  (best train acc: 0.9471, best val acc: 0.9592, best train loss: 0.1511  @ epoch 32538 )\n",
      "[Epoch: 34440] train loss: 0.1689, train acc: 0.9377, val loss: 0.8484, val acc: 0.9531  (best train acc: 0.9471, best val acc: 0.9592, best train loss: 0.1511  @ epoch 32538 )\n",
      "[Epoch: 34460] train loss: 0.1667, train acc: 0.9356, val loss: 0.8735, val acc: 0.9555  (best train acc: 0.9471, best val acc: 0.9592, best train loss: 0.1511  @ epoch 32538 )\n",
      "[Epoch: 34480] train loss: 0.1642, train acc: 0.9379, val loss: 0.8789, val acc: 0.9551  (best train acc: 0.9471, best val acc: 0.9592, best train loss: 0.1511  @ epoch 32538 )\n",
      "[Epoch: 34500] train loss: 0.1668, train acc: 0.9349, val loss: 0.8871, val acc: 0.9562  (best train acc: 0.9471, best val acc: 0.9592, best train loss: 0.1511  @ epoch 32538 )\n",
      "[Epoch: 34520] train loss: 0.1563, train acc: 0.9431, val loss: 0.8529, val acc: 0.9555  (best train acc: 0.9471, best val acc: 0.9592, best train loss: 0.1511  @ epoch 32538 )\n",
      "[Epoch: 34540] train loss: 0.1606, train acc: 0.9401, val loss: 0.8853, val acc: 0.9551  (best train acc: 0.9471, best val acc: 0.9592, best train loss: 0.1511  @ epoch 32538 )\n",
      "[Epoch: 34560] train loss: 0.1582, train acc: 0.9451, val loss: 0.8167, val acc: 0.9528  (best train acc: 0.9471, best val acc: 0.9592, best train loss: 0.1511  @ epoch 32538 )\n",
      "[Epoch: 34580] train loss: 0.1653, train acc: 0.9413, val loss: 0.8557, val acc: 0.9511  (best train acc: 0.9471, best val acc: 0.9592, best train loss: 0.1511  @ epoch 32538 )\n",
      "[Epoch: 34600] train loss: 0.1754, train acc: 0.9335, val loss: 0.8755, val acc: 0.9562  (best train acc: 0.9475, best val acc: 0.9592, best train loss: 0.1511  @ epoch 32538 )\n",
      "[Epoch: 34620] train loss: 0.1681, train acc: 0.9372, val loss: 0.8681, val acc: 0.9585  (best train acc: 0.9475, best val acc: 0.9592, best train loss: 0.1511  @ epoch 32538 )\n",
      "[Epoch: 34640] train loss: 0.1696, train acc: 0.9378, val loss: 0.8642, val acc: 0.9541  (best train acc: 0.9475, best val acc: 0.9592, best train loss: 0.1511  @ epoch 32538 )\n",
      "[Epoch: 34660] train loss: 0.1770, train acc: 0.9328, val loss: 0.7844, val acc: 0.9531  (best train acc: 0.9475, best val acc: 0.9592, best train loss: 0.1511  @ epoch 32538 )\n",
      "[Epoch: 34680] train loss: 0.1753, train acc: 0.9344, val loss: 0.7872, val acc: 0.9578  (best train acc: 0.9475, best val acc: 0.9592, best train loss: 0.1511  @ epoch 32538 )\n",
      "[Epoch: 34700] train loss: 0.1946, train acc: 0.9284, val loss: 0.8539, val acc: 0.9565  (best train acc: 0.9475, best val acc: 0.9592, best train loss: 0.1511  @ epoch 32538 )\n",
      "[Epoch: 34720] train loss: 0.1521, train acc: 0.9427, val loss: 0.8439, val acc: 0.9548  (best train acc: 0.9475, best val acc: 0.9592, best train loss: 0.1511  @ epoch 32538 )\n",
      "[Epoch: 34740] train loss: 0.1596, train acc: 0.9419, val loss: 0.8562, val acc: 0.9538  (best train acc: 0.9475, best val acc: 0.9592, best train loss: 0.1511  @ epoch 32538 )\n",
      "[Epoch: 34760] train loss: 0.1774, train acc: 0.9345, val loss: 0.8439, val acc: 0.9568  (best train acc: 0.9475, best val acc: 0.9592, best train loss: 0.1511  @ epoch 32538 )\n",
      "[Epoch: 34780] train loss: 0.1840, train acc: 0.9336, val loss: 0.8351, val acc: 0.9535  (best train acc: 0.9475, best val acc: 0.9592, best train loss: 0.1511  @ epoch 32538 )\n",
      "[Epoch: 34800] train loss: 0.1776, train acc: 0.9330, val loss: 0.6550, val acc: 0.9511  (best train acc: 0.9475, best val acc: 0.9592, best train loss: 0.1511  @ epoch 32538 )\n",
      "[Epoch: 34820] train loss: 0.1719, train acc: 0.9366, val loss: 0.8959, val acc: 0.9504  (best train acc: 0.9475, best val acc: 0.9592, best train loss: 0.1511  @ epoch 32538 )\n",
      "[Epoch: 34840] train loss: 0.1648, train acc: 0.9398, val loss: 0.7496, val acc: 0.9531  (best train acc: 0.9475, best val acc: 0.9592, best train loss: 0.1511  @ epoch 32538 )\n",
      "[Epoch: 34860] train loss: 0.1669, train acc: 0.9367, val loss: 0.7482, val acc: 0.9541  (best train acc: 0.9475, best val acc: 0.9592, best train loss: 0.1511  @ epoch 32538 )\n",
      "[Epoch: 34880] train loss: 0.1757, train acc: 0.9351, val loss: 0.8256, val acc: 0.9454  (best train acc: 0.9475, best val acc: 0.9592, best train loss: 0.1511  @ epoch 32538 )\n",
      "[Epoch: 34900] train loss: 0.1753, train acc: 0.9356, val loss: 0.8489, val acc: 0.9551  (best train acc: 0.9475, best val acc: 0.9592, best train loss: 0.1511  @ epoch 32538 )\n",
      "[Epoch: 34920] train loss: 0.1654, train acc: 0.9363, val loss: 0.8737, val acc: 0.9494  (best train acc: 0.9475, best val acc: 0.9592, best train loss: 0.1511  @ epoch 32538 )\n",
      "[Epoch: 34940] train loss: 0.1881, train acc: 0.9284, val loss: 0.8266, val acc: 0.9545  (best train acc: 0.9475, best val acc: 0.9592, best train loss: 0.1511  @ epoch 32538 )\n",
      "[Epoch: 34960] train loss: 0.1727, train acc: 0.9363, val loss: 0.8559, val acc: 0.9464  (best train acc: 0.9475, best val acc: 0.9592, best train loss: 0.1511  @ epoch 32538 )\n",
      "[Epoch: 34980] train loss: 0.1721, train acc: 0.9352, val loss: 0.8849, val acc: 0.9460  (best train acc: 0.9475, best val acc: 0.9592, best train loss: 0.1511  @ epoch 32538 )\n",
      "[Epoch: 35000] train loss: 0.1777, train acc: 0.9315, val loss: 0.8417, val acc: 0.9494  (best train acc: 0.9475, best val acc: 0.9592, best train loss: 0.1511  @ epoch 32538 )\n",
      "[Epoch: 35020] train loss: 0.1569, train acc: 0.9435, val loss: 0.8564, val acc: 0.9558  (best train acc: 0.9475, best val acc: 0.9592, best train loss: 0.1511  @ epoch 32538 )\n",
      "[Epoch: 35040] train loss: 0.1704, train acc: 0.9385, val loss: 0.8844, val acc: 0.9504  (best train acc: 0.9475, best val acc: 0.9592, best train loss: 0.1511  @ epoch 32538 )\n",
      "[Epoch: 35060] train loss: 0.1725, train acc: 0.9373, val loss: 0.8876, val acc: 0.9558  (best train acc: 0.9475, best val acc: 0.9592, best train loss: 0.1511  @ epoch 32538 )\n",
      "[Epoch: 35080] train loss: 0.1578, train acc: 0.9413, val loss: 0.8450, val acc: 0.9491  (best train acc: 0.9475, best val acc: 0.9592, best train loss: 0.1511  @ epoch 32538 )\n",
      "[Epoch: 35100] train loss: 0.1618, train acc: 0.9410, val loss: 0.9257, val acc: 0.9487  (best train acc: 0.9475, best val acc: 0.9592, best train loss: 0.1511  @ epoch 32538 )\n",
      "[Epoch: 35120] train loss: 0.1540, train acc: 0.9446, val loss: 0.8259, val acc: 0.9531  (best train acc: 0.9475, best val acc: 0.9592, best train loss: 0.1511  @ epoch 32538 )\n",
      "[Epoch: 35140] train loss: 0.1764, train acc: 0.9380, val loss: 0.8692, val acc: 0.9538  (best train acc: 0.9475, best val acc: 0.9592, best train loss: 0.1511  @ epoch 32538 )\n",
      "[Epoch: 35160] train loss: 0.1756, train acc: 0.9347, val loss: 0.8910, val acc: 0.9508  (best train acc: 0.9475, best val acc: 0.9592, best train loss: 0.1511  @ epoch 32538 )\n",
      "[Epoch: 35180] train loss: 0.1695, train acc: 0.9385, val loss: 0.8525, val acc: 0.9541  (best train acc: 0.9475, best val acc: 0.9592, best train loss: 0.1511  @ epoch 32538 )\n",
      "[Epoch: 35200] train loss: 0.2019, train acc: 0.9199, val loss: 0.8286, val acc: 0.9255  (best train acc: 0.9475, best val acc: 0.9592, best train loss: 0.1511  @ epoch 32538 )\n",
      "[Epoch: 35220] train loss: 0.1700, train acc: 0.9352, val loss: 0.8039, val acc: 0.9497  (best train acc: 0.9475, best val acc: 0.9592, best train loss: 0.1511  @ epoch 32538 )\n",
      "[Epoch: 35240] train loss: 0.1746, train acc: 0.9337, val loss: 0.8374, val acc: 0.9545  (best train acc: 0.9475, best val acc: 0.9592, best train loss: 0.1511  @ epoch 32538 )\n",
      "[Epoch: 35260] train loss: 0.1628, train acc: 0.9398, val loss: 0.8579, val acc: 0.9538  (best train acc: 0.9475, best val acc: 0.9592, best train loss: 0.1511  @ epoch 32538 )\n",
      "[Epoch: 35280] train loss: 0.1751, train acc: 0.9315, val loss: 0.8548, val acc: 0.9558  (best train acc: 0.9475, best val acc: 0.9592, best train loss: 0.1511  @ epoch 32538 )\n",
      "[Epoch: 35300] train loss: 0.1633, train acc: 0.9394, val loss: 0.8741, val acc: 0.9551  (best train acc: 0.9475, best val acc: 0.9592, best train loss: 0.1511  @ epoch 32538 )\n",
      "[Epoch: 35320] train loss: 0.1762, train acc: 0.9300, val loss: 0.8413, val acc: 0.9427  (best train acc: 0.9475, best val acc: 0.9592, best train loss: 0.1511  @ epoch 32538 )\n",
      "[Epoch: 35340] train loss: 0.1652, train acc: 0.9390, val loss: 0.9207, val acc: 0.9514  (best train acc: 0.9475, best val acc: 0.9592, best train loss: 0.1511  @ epoch 32538 )\n",
      "[Epoch: 35360] train loss: 0.1804, train acc: 0.9320, val loss: 0.8462, val acc: 0.9477  (best train acc: 0.9475, best val acc: 0.9592, best train loss: 0.1511  @ epoch 32538 )\n",
      "[Epoch: 35380] train loss: 0.1668, train acc: 0.9355, val loss: 0.7952, val acc: 0.9497  (best train acc: 0.9475, best val acc: 0.9592, best train loss: 0.1511  @ epoch 32538 )\n",
      "[Epoch: 35400] train loss: 0.1600, train acc: 0.9438, val loss: 0.8228, val acc: 0.9551  (best train acc: 0.9475, best val acc: 0.9592, best train loss: 0.1511  @ epoch 32538 )\n",
      "[Epoch: 35420] train loss: 0.1628, train acc: 0.9443, val loss: 0.8208, val acc: 0.9568  (best train acc: 0.9475, best val acc: 0.9592, best train loss: 0.1511  @ epoch 32538 )\n",
      "[Epoch: 35440] train loss: 0.1620, train acc: 0.9414, val loss: 0.8384, val acc: 0.9504  (best train acc: 0.9475, best val acc: 0.9592, best train loss: 0.1511  @ epoch 32538 )\n",
      "[Epoch: 35460] train loss: 0.1720, train acc: 0.9381, val loss: 0.8447, val acc: 0.9545  (best train acc: 0.9475, best val acc: 0.9592, best train loss: 0.1511  @ epoch 32538 )\n",
      "[Epoch: 35480] train loss: 0.1606, train acc: 0.9399, val loss: 0.8405, val acc: 0.9555  (best train acc: 0.9475, best val acc: 0.9592, best train loss: 0.1511  @ epoch 32538 )\n",
      "[Epoch: 35500] train loss: 0.1554, train acc: 0.9443, val loss: 0.8685, val acc: 0.9491  (best train acc: 0.9475, best val acc: 0.9592, best train loss: 0.1511  @ epoch 32538 )\n",
      "[Epoch: 35520] train loss: 0.1588, train acc: 0.9412, val loss: 0.8712, val acc: 0.9481  (best train acc: 0.9475, best val acc: 0.9592, best train loss: 0.1511  @ epoch 32538 )\n",
      "[Epoch: 35540] train loss: 0.1800, train acc: 0.9306, val loss: 0.8576, val acc: 0.9538  (best train acc: 0.9475, best val acc: 0.9592, best train loss: 0.1511  @ epoch 32538 )\n",
      "[Epoch: 35560] train loss: 0.1647, train acc: 0.9399, val loss: 0.8231, val acc: 0.9524  (best train acc: 0.9475, best val acc: 0.9592, best train loss: 0.1511  @ epoch 32538 )\n",
      "[Epoch: 35580] train loss: 0.1566, train acc: 0.9410, val loss: 0.9347, val acc: 0.9524  (best train acc: 0.9475, best val acc: 0.9592, best train loss: 0.1511  @ epoch 32538 )\n",
      "[Epoch: 35600] train loss: 0.1543, train acc: 0.9448, val loss: 0.8898, val acc: 0.9535  (best train acc: 0.9475, best val acc: 0.9592, best train loss: 0.1511  @ epoch 32538 )\n",
      "[Epoch: 35620] train loss: 0.1610, train acc: 0.9390, val loss: 0.8825, val acc: 0.9518  (best train acc: 0.9475, best val acc: 0.9592, best train loss: 0.1511  @ epoch 32538 )\n",
      "[Epoch: 35640] train loss: 0.1629, train acc: 0.9412, val loss: 0.9068, val acc: 0.9545  (best train acc: 0.9475, best val acc: 0.9592, best train loss: 0.1511  @ epoch 32538 )\n",
      "[Epoch: 35660] train loss: 0.2066, train acc: 0.9229, val loss: 0.8411, val acc: 0.9423  (best train acc: 0.9475, best val acc: 0.9592, best train loss: 0.1511  @ epoch 32538 )\n",
      "[Epoch: 35680] train loss: 0.1766, train acc: 0.9383, val loss: 0.7683, val acc: 0.9481  (best train acc: 0.9475, best val acc: 0.9592, best train loss: 0.1511  @ epoch 32538 )\n",
      "[Epoch: 35700] train loss: 0.1717, train acc: 0.9376, val loss: 0.8621, val acc: 0.9562  (best train acc: 0.9475, best val acc: 0.9592, best train loss: 0.1511  @ epoch 32538 )\n",
      "[Epoch: 35720] train loss: 0.1630, train acc: 0.9426, val loss: 0.9198, val acc: 0.9568  (best train acc: 0.9475, best val acc: 0.9592, best train loss: 0.1511  @ epoch 32538 )\n",
      "[Epoch: 35740] train loss: 0.1631, train acc: 0.9407, val loss: 0.8669, val acc: 0.9555  (best train acc: 0.9475, best val acc: 0.9592, best train loss: 0.1511  @ epoch 32538 )\n",
      "[Epoch: 35760] train loss: 0.1899, train acc: 0.9292, val loss: 0.8472, val acc: 0.9504  (best train acc: 0.9475, best val acc: 0.9592, best train loss: 0.1491  @ epoch 35746 )\n",
      "[Epoch: 35780] train loss: 0.1795, train acc: 0.9339, val loss: 0.8905, val acc: 0.9504  (best train acc: 0.9475, best val acc: 0.9592, best train loss: 0.1491  @ epoch 35746 )\n",
      "[Epoch: 35800] train loss: 0.1783, train acc: 0.9323, val loss: 0.8019, val acc: 0.9474  (best train acc: 0.9475, best val acc: 0.9592, best train loss: 0.1491  @ epoch 35746 )\n",
      "[Epoch: 35820] train loss: 0.1601, train acc: 0.9425, val loss: 0.8338, val acc: 0.9514  (best train acc: 0.9475, best val acc: 0.9592, best train loss: 0.1491  @ epoch 35746 )\n",
      "[Epoch: 35840] train loss: 0.1676, train acc: 0.9359, val loss: 0.8806, val acc: 0.9484  (best train acc: 0.9475, best val acc: 0.9592, best train loss: 0.1491  @ epoch 35746 )\n",
      "[Epoch: 35860] train loss: 0.1725, train acc: 0.9352, val loss: 0.8885, val acc: 0.9558  (best train acc: 0.9475, best val acc: 0.9592, best train loss: 0.1491  @ epoch 35746 )\n",
      "[Epoch: 35880] train loss: 0.1832, train acc: 0.9318, val loss: 0.9119, val acc: 0.9562  (best train acc: 0.9475, best val acc: 0.9592, best train loss: 0.1491  @ epoch 35746 )\n",
      "[Epoch: 35900] train loss: 0.1812, train acc: 0.9311, val loss: 0.7544, val acc: 0.9400  (best train acc: 0.9475, best val acc: 0.9592, best train loss: 0.1491  @ epoch 35746 )\n",
      "[Epoch: 35920] train loss: 0.1668, train acc: 0.9408, val loss: 0.8930, val acc: 0.9548  (best train acc: 0.9475, best val acc: 0.9592, best train loss: 0.1491  @ epoch 35746 )\n",
      "[Epoch: 35940] train loss: 0.1612, train acc: 0.9406, val loss: 0.8653, val acc: 0.9545  (best train acc: 0.9475, best val acc: 0.9592, best train loss: 0.1491  @ epoch 35746 )\n",
      "[Epoch: 35960] train loss: 0.1571, train acc: 0.9403, val loss: 0.8674, val acc: 0.9504  (best train acc: 0.9475, best val acc: 0.9592, best train loss: 0.1491  @ epoch 35746 )\n",
      "[Epoch: 35980] train loss: 0.1721, train acc: 0.9335, val loss: 0.8345, val acc: 0.9491  (best train acc: 0.9475, best val acc: 0.9592, best train loss: 0.1491  @ epoch 35746 )\n",
      "[Epoch: 36000] train loss: 0.1612, train acc: 0.9398, val loss: 0.8481, val acc: 0.9528  (best train acc: 0.9475, best val acc: 0.9592, best train loss: 0.1491  @ epoch 35746 )\n",
      "[Epoch: 36020] train loss: 0.1689, train acc: 0.9363, val loss: 0.8987, val acc: 0.9558  (best train acc: 0.9475, best val acc: 0.9592, best train loss: 0.1491  @ epoch 35746 )\n",
      "[Epoch: 36040] train loss: 0.1621, train acc: 0.9400, val loss: 0.9125, val acc: 0.9572  (best train acc: 0.9475, best val acc: 0.9592, best train loss: 0.1491  @ epoch 35746 )\n",
      "[Epoch: 36060] train loss: 0.2208, train acc: 0.9176, val loss: 0.8100, val acc: 0.9477  (best train acc: 0.9475, best val acc: 0.9592, best train loss: 0.1491  @ epoch 35746 )\n",
      "[Epoch: 36080] train loss: 0.1732, train acc: 0.9370, val loss: 0.7917, val acc: 0.9562  (best train acc: 0.9475, best val acc: 0.9592, best train loss: 0.1491  @ epoch 35746 )\n",
      "[Epoch: 36100] train loss: 0.1667, train acc: 0.9401, val loss: 0.8261, val acc: 0.9545  (best train acc: 0.9475, best val acc: 0.9592, best train loss: 0.1491  @ epoch 35746 )\n",
      "[Epoch: 36120] train loss: 0.1804, train acc: 0.9356, val loss: 0.8134, val acc: 0.9548  (best train acc: 0.9475, best val acc: 0.9592, best train loss: 0.1491  @ epoch 35746 )\n",
      "[Epoch: 36140] train loss: 0.1668, train acc: 0.9384, val loss: 0.8845, val acc: 0.9538  (best train acc: 0.9475, best val acc: 0.9592, best train loss: 0.1491  @ epoch 35746 )\n",
      "[Epoch: 36160] train loss: 0.1605, train acc: 0.9401, val loss: 0.8908, val acc: 0.9562  (best train acc: 0.9475, best val acc: 0.9592, best train loss: 0.1491  @ epoch 35746 )\n",
      "[Epoch: 36180] train loss: 0.1670, train acc: 0.9396, val loss: 0.8921, val acc: 0.9508  (best train acc: 0.9475, best val acc: 0.9592, best train loss: 0.1491  @ epoch 35746 )\n",
      "[Epoch: 36200] train loss: 0.1599, train acc: 0.9430, val loss: 0.8910, val acc: 0.9558  (best train acc: 0.9475, best val acc: 0.9592, best train loss: 0.1491  @ epoch 35746 )\n",
      "[Epoch: 36220] train loss: 0.1598, train acc: 0.9438, val loss: 0.8994, val acc: 0.9514  (best train acc: 0.9475, best val acc: 0.9592, best train loss: 0.1491  @ epoch 35746 )\n",
      "[Epoch: 36240] train loss: 0.1611, train acc: 0.9413, val loss: 0.9352, val acc: 0.9545  (best train acc: 0.9475, best val acc: 0.9592, best train loss: 0.1491  @ epoch 35746 )\n",
      "[Epoch: 36260] train loss: 0.1848, train acc: 0.9333, val loss: 0.9192, val acc: 0.9551  (best train acc: 0.9475, best val acc: 0.9592, best train loss: 0.1491  @ epoch 35746 )\n",
      "[Epoch: 36280] train loss: 0.1725, train acc: 0.9374, val loss: 0.8457, val acc: 0.9454  (best train acc: 0.9475, best val acc: 0.9592, best train loss: 0.1491  @ epoch 35746 )\n",
      "[Epoch: 36300] train loss: 0.1665, train acc: 0.9390, val loss: 0.7933, val acc: 0.9508  (best train acc: 0.9475, best val acc: 0.9592, best train loss: 0.1491  @ epoch 35746 )\n",
      "[Epoch: 36320] train loss: 0.1585, train acc: 0.9404, val loss: 0.8324, val acc: 0.9555  (best train acc: 0.9475, best val acc: 0.9592, best train loss: 0.1491  @ epoch 35746 )\n",
      "[Epoch: 36340] train loss: 0.1624, train acc: 0.9406, val loss: 0.8555, val acc: 0.9508  (best train acc: 0.9475, best val acc: 0.9592, best train loss: 0.1491  @ epoch 35746 )\n",
      "[Epoch: 36360] train loss: 0.1690, train acc: 0.9383, val loss: 0.8764, val acc: 0.9568  (best train acc: 0.9475, best val acc: 0.9592, best train loss: 0.1491  @ epoch 35746 )\n",
      "[Epoch: 36380] train loss: 0.1638, train acc: 0.9387, val loss: 0.8540, val acc: 0.9562  (best train acc: 0.9475, best val acc: 0.9592, best train loss: 0.1491  @ epoch 35746 )\n",
      "[Epoch: 36400] train loss: 0.1804, train acc: 0.9315, val loss: 0.8137, val acc: 0.9514  (best train acc: 0.9475, best val acc: 0.9592, best train loss: 0.1491  @ epoch 35746 )\n",
      "[Epoch: 36420] train loss: 0.1740, train acc: 0.9346, val loss: 0.9169, val acc: 0.9538  (best train acc: 0.9475, best val acc: 0.9592, best train loss: 0.1491  @ epoch 35746 )\n",
      "[Epoch: 36440] train loss: 0.1631, train acc: 0.9396, val loss: 0.8929, val acc: 0.9464  (best train acc: 0.9475, best val acc: 0.9592, best train loss: 0.1491  @ epoch 35746 )\n",
      "[Epoch: 36460] train loss: 0.1598, train acc: 0.9428, val loss: 0.8561, val acc: 0.9562  (best train acc: 0.9475, best val acc: 0.9592, best train loss: 0.1491  @ epoch 35746 )\n",
      "[Epoch: 36480] train loss: 0.1841, train acc: 0.9290, val loss: 0.8817, val acc: 0.9551  (best train acc: 0.9475, best val acc: 0.9592, best train loss: 0.1491  @ epoch 35746 )\n",
      "[Epoch: 36500] train loss: 0.1577, train acc: 0.9435, val loss: 0.9025, val acc: 0.9562  (best train acc: 0.9475, best val acc: 0.9592, best train loss: 0.1491  @ epoch 35746 )\n",
      "[Epoch: 36520] train loss: 0.1627, train acc: 0.9377, val loss: 0.8690, val acc: 0.9555  (best train acc: 0.9475, best val acc: 0.9592, best train loss: 0.1491  @ epoch 35746 )\n",
      "[Epoch: 36540] train loss: 0.1763, train acc: 0.9328, val loss: 0.9292, val acc: 0.9501  (best train acc: 0.9475, best val acc: 0.9592, best train loss: 0.1491  @ epoch 35746 )\n",
      "[Epoch: 36560] train loss: 0.1761, train acc: 0.9336, val loss: 0.8754, val acc: 0.9484  (best train acc: 0.9475, best val acc: 0.9592, best train loss: 0.1491  @ epoch 35746 )\n",
      "[Epoch: 36580] train loss: 0.1742, train acc: 0.9351, val loss: 0.7957, val acc: 0.9474  (best train acc: 0.9475, best val acc: 0.9592, best train loss: 0.1491  @ epoch 35746 )\n",
      "[Epoch: 36600] train loss: 0.1647, train acc: 0.9391, val loss: 0.8547, val acc: 0.9494  (best train acc: 0.9475, best val acc: 0.9592, best train loss: 0.1491  @ epoch 35746 )\n",
      "[Epoch: 36620] train loss: 0.1559, train acc: 0.9446, val loss: 0.8726, val acc: 0.9535  (best train acc: 0.9475, best val acc: 0.9592, best train loss: 0.1491  @ epoch 35746 )\n",
      "[Epoch: 36640] train loss: 0.1623, train acc: 0.9401, val loss: 0.9081, val acc: 0.9568  (best train acc: 0.9475, best val acc: 0.9592, best train loss: 0.1491  @ epoch 35746 )\n",
      "[Epoch: 36660] train loss: 0.1755, train acc: 0.9337, val loss: 0.9091, val acc: 0.9477  (best train acc: 0.9475, best val acc: 0.9592, best train loss: 0.1491  @ epoch 35746 )\n",
      "[Epoch: 36680] train loss: 0.1923, train acc: 0.9291, val loss: 0.8078, val acc: 0.9460  (best train acc: 0.9475, best val acc: 0.9592, best train loss: 0.1491  @ epoch 35746 )\n",
      "[Epoch: 36700] train loss: 0.1842, train acc: 0.9302, val loss: 0.8712, val acc: 0.9511  (best train acc: 0.9475, best val acc: 0.9592, best train loss: 0.1491  @ epoch 35746 )\n",
      "[Epoch: 36720] train loss: 0.1647, train acc: 0.9412, val loss: 0.9195, val acc: 0.9548  (best train acc: 0.9475, best val acc: 0.9592, best train loss: 0.1491  @ epoch 35746 )\n",
      "[Epoch: 36740] train loss: 0.1719, train acc: 0.9357, val loss: 0.8716, val acc: 0.9521  (best train acc: 0.9475, best val acc: 0.9592, best train loss: 0.1491  @ epoch 35746 )\n",
      "[Epoch: 36760] train loss: 0.1655, train acc: 0.9393, val loss: 0.9430, val acc: 0.9497  (best train acc: 0.9475, best val acc: 0.9592, best train loss: 0.1491  @ epoch 35746 )\n",
      "[Epoch: 36780] train loss: 0.1748, train acc: 0.9360, val loss: 0.8951, val acc: 0.9565  (best train acc: 0.9475, best val acc: 0.9592, best train loss: 0.1491  @ epoch 35746 )\n",
      "[Epoch: 36800] train loss: 0.1677, train acc: 0.9390, val loss: 0.9093, val acc: 0.9541  (best train acc: 0.9475, best val acc: 0.9592, best train loss: 0.1491  @ epoch 35746 )\n",
      "[Epoch: 36820] train loss: 0.1644, train acc: 0.9420, val loss: 0.8898, val acc: 0.9444  (best train acc: 0.9475, best val acc: 0.9592, best train loss: 0.1491  @ epoch 35746 )\n",
      "[Epoch: 36840] train loss: 0.1643, train acc: 0.9396, val loss: 0.9022, val acc: 0.9363  (best train acc: 0.9475, best val acc: 0.9592, best train loss: 0.1491  @ epoch 35746 )\n",
      "[Epoch: 36860] train loss: 0.1697, train acc: 0.9360, val loss: 0.8232, val acc: 0.9518  (best train acc: 0.9475, best val acc: 0.9592, best train loss: 0.1491  @ epoch 35746 )\n",
      "[Epoch: 36880] train loss: 0.1639, train acc: 0.9414, val loss: 0.8306, val acc: 0.9538  (best train acc: 0.9475, best val acc: 0.9592, best train loss: 0.1491  @ epoch 35746 )\n",
      "[Epoch: 36900] train loss: 0.1608, train acc: 0.9421, val loss: 0.8243, val acc: 0.9558  (best train acc: 0.9475, best val acc: 0.9592, best train loss: 0.1491  @ epoch 35746 )\n",
      "[Epoch: 36920] train loss: 0.1813, train acc: 0.9338, val loss: 0.7985, val acc: 0.9444  (best train acc: 0.9475, best val acc: 0.9592, best train loss: 0.1491  @ epoch 35746 )\n",
      "[Epoch: 36940] train loss: 0.1646, train acc: 0.9384, val loss: 0.8862, val acc: 0.9541  (best train acc: 0.9475, best val acc: 0.9592, best train loss: 0.1491  @ epoch 35746 )\n",
      "[Epoch: 36960] train loss: 0.1590, train acc: 0.9417, val loss: 0.8947, val acc: 0.9491  (best train acc: 0.9475, best val acc: 0.9592, best train loss: 0.1491  @ epoch 35746 )\n",
      "[Epoch: 36980] train loss: 0.1560, train acc: 0.9439, val loss: 0.8844, val acc: 0.9467  (best train acc: 0.9475, best val acc: 0.9592, best train loss: 0.1491  @ epoch 35746 )\n",
      "[Epoch: 37000] train loss: 0.1621, train acc: 0.9431, val loss: 0.9211, val acc: 0.9545  (best train acc: 0.9475, best val acc: 0.9602, best train loss: 0.1491  @ epoch 35746 )\n",
      "[Epoch: 37020] train loss: 0.1541, train acc: 0.9462, val loss: 0.9087, val acc: 0.9541  (best train acc: 0.9475, best val acc: 0.9602, best train loss: 0.1491  @ epoch 35746 )\n",
      "[Epoch: 37040] train loss: 0.1571, train acc: 0.9411, val loss: 0.8996, val acc: 0.9555  (best train acc: 0.9475, best val acc: 0.9602, best train loss: 0.1491  @ epoch 35746 )\n",
      "[Epoch: 37060] train loss: 0.1647, train acc: 0.9398, val loss: 0.9076, val acc: 0.9497  (best train acc: 0.9475, best val acc: 0.9602, best train loss: 0.1485  @ epoch 37048 )\n",
      "[Epoch: 37080] train loss: 0.1670, train acc: 0.9396, val loss: 0.9479, val acc: 0.9541  (best train acc: 0.9475, best val acc: 0.9602, best train loss: 0.1485  @ epoch 37048 )\n",
      "[Epoch: 37100] train loss: 0.1652, train acc: 0.9378, val loss: 0.9288, val acc: 0.9531  (best train acc: 0.9475, best val acc: 0.9602, best train loss: 0.1485  @ epoch 37048 )\n",
      "[Epoch: 37120] train loss: 0.1657, train acc: 0.9424, val loss: 0.8989, val acc: 0.9467  (best train acc: 0.9475, best val acc: 0.9602, best train loss: 0.1485  @ epoch 37048 )\n",
      "[Epoch: 37140] train loss: 0.1795, train acc: 0.9328, val loss: 0.8758, val acc: 0.9437  (best train acc: 0.9475, best val acc: 0.9602, best train loss: 0.1485  @ epoch 37048 )\n",
      "[Epoch: 37160] train loss: 0.1627, train acc: 0.9391, val loss: 0.8111, val acc: 0.9454  (best train acc: 0.9475, best val acc: 0.9602, best train loss: 0.1485  @ epoch 37048 )\n",
      "[Epoch: 37180] train loss: 0.1587, train acc: 0.9396, val loss: 0.8872, val acc: 0.9538  (best train acc: 0.9475, best val acc: 0.9602, best train loss: 0.1485  @ epoch 37048 )\n",
      "[Epoch: 37200] train loss: 0.1635, train acc: 0.9376, val loss: 0.8686, val acc: 0.9538  (best train acc: 0.9475, best val acc: 0.9602, best train loss: 0.1485  @ epoch 37048 )\n",
      "[Epoch: 37220] train loss: 0.1911, train acc: 0.9294, val loss: 0.8661, val acc: 0.9548  (best train acc: 0.9475, best val acc: 0.9602, best train loss: 0.1485  @ epoch 37048 )\n",
      "[Epoch: 37240] train loss: 0.1699, train acc: 0.9390, val loss: 0.8805, val acc: 0.9555  (best train acc: 0.9475, best val acc: 0.9602, best train loss: 0.1485  @ epoch 37048 )\n",
      "[Epoch: 37260] train loss: 0.1597, train acc: 0.9417, val loss: 0.8249, val acc: 0.9555  (best train acc: 0.9475, best val acc: 0.9602, best train loss: 0.1485  @ epoch 37048 )\n",
      "[Epoch: 37280] train loss: 0.1605, train acc: 0.9425, val loss: 0.8321, val acc: 0.9555  (best train acc: 0.9475, best val acc: 0.9602, best train loss: 0.1485  @ epoch 37048 )\n",
      "[Epoch: 37300] train loss: 0.1667, train acc: 0.9400, val loss: 0.7880, val acc: 0.9487  (best train acc: 0.9475, best val acc: 0.9602, best train loss: 0.1485  @ epoch 37048 )\n",
      "[Epoch: 37320] train loss: 0.1703, train acc: 0.9356, val loss: 0.8185, val acc: 0.9464  (best train acc: 0.9475, best val acc: 0.9602, best train loss: 0.1485  @ epoch 37048 )\n",
      "[Epoch: 37340] train loss: 0.1549, train acc: 0.9393, val loss: 0.8427, val acc: 0.9541  (best train acc: 0.9475, best val acc: 0.9602, best train loss: 0.1485  @ epoch 37048 )\n",
      "[Epoch: 37360] train loss: 0.1570, train acc: 0.9433, val loss: 0.8782, val acc: 0.9524  (best train acc: 0.9475, best val acc: 0.9602, best train loss: 0.1485  @ epoch 37048 )\n",
      "[Epoch: 37380] train loss: 0.1652, train acc: 0.9408, val loss: 0.8418, val acc: 0.9551  (best train acc: 0.9475, best val acc: 0.9602, best train loss: 0.1485  @ epoch 37048 )\n",
      "[Epoch: 37400] train loss: 0.1642, train acc: 0.9410, val loss: 0.8789, val acc: 0.9531  (best train acc: 0.9475, best val acc: 0.9602, best train loss: 0.1485  @ epoch 37048 )\n",
      "[Epoch: 37420] train loss: 0.1739, train acc: 0.9368, val loss: 0.9047, val acc: 0.9555  (best train acc: 0.9475, best val acc: 0.9602, best train loss: 0.1485  @ epoch 37048 )\n",
      "[Epoch: 37440] train loss: 0.1600, train acc: 0.9396, val loss: 0.8720, val acc: 0.9467  (best train acc: 0.9475, best val acc: 0.9602, best train loss: 0.1485  @ epoch 37048 )\n",
      "[Epoch: 37460] train loss: 0.1806, train acc: 0.9330, val loss: 0.7779, val acc: 0.9521  (best train acc: 0.9478, best val acc: 0.9602, best train loss: 0.1485  @ epoch 37048 )\n",
      "[Epoch: 37480] train loss: 0.1783, train acc: 0.9364, val loss: 0.8378, val acc: 0.9454  (best train acc: 0.9478, best val acc: 0.9602, best train loss: 0.1485  @ epoch 37048 )\n",
      "[Epoch: 37500] train loss: 0.1665, train acc: 0.9388, val loss: 0.8759, val acc: 0.9545  (best train acc: 0.9478, best val acc: 0.9602, best train loss: 0.1485  @ epoch 37048 )\n",
      "[Epoch: 37520] train loss: 0.1681, train acc: 0.9380, val loss: 0.8583, val acc: 0.9524  (best train acc: 0.9478, best val acc: 0.9602, best train loss: 0.1485  @ epoch 37048 )\n",
      "[Epoch: 37540] train loss: 0.1679, train acc: 0.9370, val loss: 0.8072, val acc: 0.9487  (best train acc: 0.9478, best val acc: 0.9602, best train loss: 0.1485  @ epoch 37048 )\n",
      "[Epoch: 37560] train loss: 0.1700, train acc: 0.9393, val loss: 0.8029, val acc: 0.9562  (best train acc: 0.9478, best val acc: 0.9602, best train loss: 0.1485  @ epoch 37543 )\n",
      "[Epoch: 37580] train loss: 0.1557, train acc: 0.9444, val loss: 0.8200, val acc: 0.9494  (best train acc: 0.9478, best val acc: 0.9602, best train loss: 0.1485  @ epoch 37543 )\n",
      "[Epoch: 37600] train loss: 0.1579, train acc: 0.9428, val loss: 0.8218, val acc: 0.9518  (best train acc: 0.9478, best val acc: 0.9602, best train loss: 0.1485  @ epoch 37543 )\n",
      "[Epoch: 37620] train loss: 0.1593, train acc: 0.9412, val loss: 0.8260, val acc: 0.9541  (best train acc: 0.9478, best val acc: 0.9602, best train loss: 0.1484  @ epoch 37613 )\n",
      "[Epoch: 37640] train loss: 0.1513, train acc: 0.9461, val loss: 0.8075, val acc: 0.9518  (best train acc: 0.9478, best val acc: 0.9602, best train loss: 0.1484  @ epoch 37613 )\n",
      "[Epoch: 37660] train loss: 0.1567, train acc: 0.9421, val loss: 0.8255, val acc: 0.9524  (best train acc: 0.9478, best val acc: 0.9602, best train loss: 0.1484  @ epoch 37613 )\n",
      "[Epoch: 37680] train loss: 0.1544, train acc: 0.9416, val loss: 0.8194, val acc: 0.9491  (best train acc: 0.9478, best val acc: 0.9602, best train loss: 0.1484  @ epoch 37613 )\n",
      "[Epoch: 37700] train loss: 0.1614, train acc: 0.9386, val loss: 0.8248, val acc: 0.9491  (best train acc: 0.9478, best val acc: 0.9602, best train loss: 0.1484  @ epoch 37613 )\n",
      "[Epoch: 37720] train loss: 0.1656, train acc: 0.9380, val loss: 0.8576, val acc: 0.9460  (best train acc: 0.9478, best val acc: 0.9602, best train loss: 0.1484  @ epoch 37613 )\n",
      "[Epoch: 37740] train loss: 0.1563, train acc: 0.9432, val loss: 0.8155, val acc: 0.9524  (best train acc: 0.9478, best val acc: 0.9602, best train loss: 0.1478  @ epoch 37737 )\n",
      "[Epoch: 37760] train loss: 0.1649, train acc: 0.9386, val loss: 0.8299, val acc: 0.9538  (best train acc: 0.9478, best val acc: 0.9602, best train loss: 0.1478  @ epoch 37737 )\n",
      "[Epoch: 37780] train loss: 0.1668, train acc: 0.9372, val loss: 0.7875, val acc: 0.9504  (best train acc: 0.9478, best val acc: 0.9602, best train loss: 0.1478  @ epoch 37737 )\n",
      "[Epoch: 37800] train loss: 0.1641, train acc: 0.9393, val loss: 0.8183, val acc: 0.9484  (best train acc: 0.9478, best val acc: 0.9602, best train loss: 0.1478  @ epoch 37737 )\n",
      "[Epoch: 37820] train loss: 0.1600, train acc: 0.9419, val loss: 0.7570, val acc: 0.9511  (best train acc: 0.9478, best val acc: 0.9602, best train loss: 0.1478  @ epoch 37737 )\n",
      "[Epoch: 37840] train loss: 0.1561, train acc: 0.9460, val loss: 0.7781, val acc: 0.9497  (best train acc: 0.9478, best val acc: 0.9602, best train loss: 0.1478  @ epoch 37737 )\n",
      "[Epoch: 37860] train loss: 0.1586, train acc: 0.9417, val loss: 0.8135, val acc: 0.9528  (best train acc: 0.9478, best val acc: 0.9602, best train loss: 0.1478  @ epoch 37737 )\n",
      "[Epoch: 37880] train loss: 0.1621, train acc: 0.9414, val loss: 0.7823, val acc: 0.9528  (best train acc: 0.9478, best val acc: 0.9602, best train loss: 0.1478  @ epoch 37737 )\n",
      "[Epoch: 37900] train loss: 0.1570, train acc: 0.9389, val loss: 0.8068, val acc: 0.9511  (best train acc: 0.9478, best val acc: 0.9602, best train loss: 0.1478  @ epoch 37737 )\n",
      "[Epoch: 37920] train loss: 0.1539, train acc: 0.9422, val loss: 0.8225, val acc: 0.9531  (best train acc: 0.9478, best val acc: 0.9602, best train loss: 0.1478  @ epoch 37737 )\n",
      "[Epoch: 37940] train loss: 0.1730, train acc: 0.9375, val loss: 0.8113, val acc: 0.9501  (best train acc: 0.9478, best val acc: 0.9602, best train loss: 0.1478  @ epoch 37737 )\n",
      "[Epoch: 37960] train loss: 0.1710, train acc: 0.9380, val loss: 0.8178, val acc: 0.9518  (best train acc: 0.9478, best val acc: 0.9602, best train loss: 0.1478  @ epoch 37737 )\n",
      "[Epoch: 37980] train loss: 0.1737, train acc: 0.9337, val loss: 0.7504, val acc: 0.9427  (best train acc: 0.9478, best val acc: 0.9602, best train loss: 0.1478  @ epoch 37737 )\n",
      "[Epoch: 38000] train loss: 0.1606, train acc: 0.9425, val loss: 0.8101, val acc: 0.9427  (best train acc: 0.9478, best val acc: 0.9602, best train loss: 0.1478  @ epoch 37737 )\n",
      "[Epoch: 38020] train loss: 0.1749, train acc: 0.9325, val loss: 0.7388, val acc: 0.9545  (best train acc: 0.9478, best val acc: 0.9602, best train loss: 0.1478  @ epoch 37737 )\n",
      "[Epoch: 38040] train loss: 0.1558, train acc: 0.9440, val loss: 0.7280, val acc: 0.9491  (best train acc: 0.9478, best val acc: 0.9602, best train loss: 0.1478  @ epoch 37737 )\n",
      "[Epoch: 38060] train loss: 0.1586, train acc: 0.9417, val loss: 0.7850, val acc: 0.9501  (best train acc: 0.9478, best val acc: 0.9602, best train loss: 0.1478  @ epoch 37737 )\n",
      "[Epoch: 38080] train loss: 0.1569, train acc: 0.9430, val loss: 0.7894, val acc: 0.9548  (best train acc: 0.9478, best val acc: 0.9602, best train loss: 0.1478  @ epoch 37737 )\n",
      "[Epoch: 38100] train loss: 0.1661, train acc: 0.9403, val loss: 0.7629, val acc: 0.9504  (best train acc: 0.9478, best val acc: 0.9602, best train loss: 0.1478  @ epoch 37737 )\n",
      "[Epoch: 38120] train loss: 0.1568, train acc: 0.9414, val loss: 0.7951, val acc: 0.9484  (best train acc: 0.9478, best val acc: 0.9602, best train loss: 0.1478  @ epoch 37737 )\n",
      "[Epoch: 38140] train loss: 0.1934, train acc: 0.9304, val loss: 0.7735, val acc: 0.9437  (best train acc: 0.9478, best val acc: 0.9602, best train loss: 0.1478  @ epoch 37737 )\n",
      "[Epoch: 38160] train loss: 0.1627, train acc: 0.9379, val loss: 0.8794, val acc: 0.9508  (best train acc: 0.9478, best val acc: 0.9602, best train loss: 0.1478  @ epoch 37737 )\n",
      "[Epoch: 38180] train loss: 0.1597, train acc: 0.9398, val loss: 0.7932, val acc: 0.9420  (best train acc: 0.9478, best val acc: 0.9602, best train loss: 0.1453  @ epoch 38161 )\n",
      "[Epoch: 38200] train loss: 0.1573, train acc: 0.9420, val loss: 0.7638, val acc: 0.9481  (best train acc: 0.9478, best val acc: 0.9602, best train loss: 0.1453  @ epoch 38161 )\n",
      "[Epoch: 38220] train loss: 0.1667, train acc: 0.9378, val loss: 0.8113, val acc: 0.9548  (best train acc: 0.9478, best val acc: 0.9602, best train loss: 0.1453  @ epoch 38161 )\n",
      "[Epoch: 38240] train loss: 0.1655, train acc: 0.9406, val loss: 0.7851, val acc: 0.9477  (best train acc: 0.9478, best val acc: 0.9602, best train loss: 0.1453  @ epoch 38161 )\n",
      "[Epoch: 38260] train loss: 0.1653, train acc: 0.9399, val loss: 0.7871, val acc: 0.9508  (best train acc: 0.9478, best val acc: 0.9602, best train loss: 0.1453  @ epoch 38161 )\n",
      "[Epoch: 38280] train loss: 0.1699, train acc: 0.9373, val loss: 0.8263, val acc: 0.9528  (best train acc: 0.9478, best val acc: 0.9602, best train loss: 0.1453  @ epoch 38161 )\n",
      "[Epoch: 38300] train loss: 0.1535, train acc: 0.9427, val loss: 0.8362, val acc: 0.9504  (best train acc: 0.9478, best val acc: 0.9602, best train loss: 0.1453  @ epoch 38161 )\n",
      "[Epoch: 38320] train loss: 0.1520, train acc: 0.9468, val loss: 0.8405, val acc: 0.9555  (best train acc: 0.9478, best val acc: 0.9602, best train loss: 0.1453  @ epoch 38161 )\n",
      "[Epoch: 38340] train loss: 0.1731, train acc: 0.9362, val loss: 0.9017, val acc: 0.9562  (best train acc: 0.9478, best val acc: 0.9602, best train loss: 0.1453  @ epoch 38161 )\n",
      "[Epoch: 38360] train loss: 0.1620, train acc: 0.9417, val loss: 0.7491, val acc: 0.9545  (best train acc: 0.9478, best val acc: 0.9602, best train loss: 0.1453  @ epoch 38161 )\n",
      "[Epoch: 38380] train loss: 0.1673, train acc: 0.9365, val loss: 0.7502, val acc: 0.9470  (best train acc: 0.9478, best val acc: 0.9602, best train loss: 0.1453  @ epoch 38161 )\n",
      "[Epoch: 38400] train loss: 0.1642, train acc: 0.9371, val loss: 0.8300, val acc: 0.9487  (best train acc: 0.9478, best val acc: 0.9602, best train loss: 0.1453  @ epoch 38161 )\n",
      "[Epoch: 38420] train loss: 0.1583, train acc: 0.9431, val loss: 0.8623, val acc: 0.9555  (best train acc: 0.9478, best val acc: 0.9602, best train loss: 0.1453  @ epoch 38161 )\n",
      "[Epoch: 38440] train loss: 0.1648, train acc: 0.9410, val loss: 0.8569, val acc: 0.9548  (best train acc: 0.9478, best val acc: 0.9602, best train loss: 0.1453  @ epoch 38161 )\n",
      "[Epoch: 38460] train loss: 0.1591, train acc: 0.9411, val loss: 0.8551, val acc: 0.9497  (best train acc: 0.9478, best val acc: 0.9602, best train loss: 0.1453  @ epoch 38161 )\n",
      "[Epoch: 38480] train loss: 0.1550, train acc: 0.9422, val loss: 0.8190, val acc: 0.9535  (best train acc: 0.9478, best val acc: 0.9602, best train loss: 0.1453  @ epoch 38161 )\n",
      "[Epoch: 38500] train loss: 0.1608, train acc: 0.9404, val loss: 0.8013, val acc: 0.9558  (best train acc: 0.9478, best val acc: 0.9602, best train loss: 0.1453  @ epoch 38161 )\n",
      "[Epoch: 38520] train loss: 0.1584, train acc: 0.9402, val loss: 0.8587, val acc: 0.9521  (best train acc: 0.9478, best val acc: 0.9602, best train loss: 0.1453  @ epoch 38161 )\n",
      "[Epoch: 38540] train loss: 0.1642, train acc: 0.9387, val loss: 0.7434, val acc: 0.9497  (best train acc: 0.9478, best val acc: 0.9602, best train loss: 0.1453  @ epoch 38161 )\n",
      "[Epoch: 38560] train loss: 0.1503, train acc: 0.9444, val loss: 0.7825, val acc: 0.9524  (best train acc: 0.9478, best val acc: 0.9602, best train loss: 0.1453  @ epoch 38161 )\n",
      "[Epoch: 38580] train loss: 0.1637, train acc: 0.9406, val loss: 0.8168, val acc: 0.9531  (best train acc: 0.9478, best val acc: 0.9602, best train loss: 0.1453  @ epoch 38161 )\n",
      "[Epoch: 38600] train loss: 0.1630, train acc: 0.9397, val loss: 0.8366, val acc: 0.9548  (best train acc: 0.9478, best val acc: 0.9602, best train loss: 0.1453  @ epoch 38161 )\n",
      "[Epoch: 38620] train loss: 0.1667, train acc: 0.9390, val loss: 0.8354, val acc: 0.9518  (best train acc: 0.9478, best val acc: 0.9602, best train loss: 0.1453  @ epoch 38161 )\n",
      "[Epoch: 38640] train loss: 0.1782, train acc: 0.9328, val loss: 0.7675, val acc: 0.9396  (best train acc: 0.9478, best val acc: 0.9602, best train loss: 0.1453  @ epoch 38161 )\n",
      "[Epoch: 38660] train loss: 0.1828, train acc: 0.9281, val loss: 0.8299, val acc: 0.9474  (best train acc: 0.9478, best val acc: 0.9602, best train loss: 0.1453  @ epoch 38161 )\n",
      "[Epoch: 38680] train loss: 0.1550, train acc: 0.9438, val loss: 0.7536, val acc: 0.9521  (best train acc: 0.9478, best val acc: 0.9602, best train loss: 0.1453  @ epoch 38161 )\n",
      "[Epoch: 38700] train loss: 0.1745, train acc: 0.9372, val loss: 0.7270, val acc: 0.9504  (best train acc: 0.9478, best val acc: 0.9602, best train loss: 0.1453  @ epoch 38161 )\n",
      "[Epoch: 38720] train loss: 0.1607, train acc: 0.9416, val loss: 0.7704, val acc: 0.9541  (best train acc: 0.9478, best val acc: 0.9602, best train loss: 0.1453  @ epoch 38161 )\n",
      "[Epoch: 38740] train loss: 0.1683, train acc: 0.9387, val loss: 0.8031, val acc: 0.9545  (best train acc: 0.9478, best val acc: 0.9602, best train loss: 0.1453  @ epoch 38161 )\n",
      "[Epoch: 38760] train loss: 0.1717, train acc: 0.9368, val loss: 0.8000, val acc: 0.9531  (best train acc: 0.9478, best val acc: 0.9602, best train loss: 0.1453  @ epoch 38161 )\n",
      "[Epoch: 38780] train loss: 0.1552, train acc: 0.9438, val loss: 0.8357, val acc: 0.9558  (best train acc: 0.9478, best val acc: 0.9602, best train loss: 0.1453  @ epoch 38161 )\n",
      "[Epoch: 38800] train loss: 0.1549, train acc: 0.9440, val loss: 0.8502, val acc: 0.9531  (best train acc: 0.9478, best val acc: 0.9602, best train loss: 0.1453  @ epoch 38161 )\n",
      "[Epoch: 38820] train loss: 0.1630, train acc: 0.9407, val loss: 0.8642, val acc: 0.9477  (best train acc: 0.9487, best val acc: 0.9602, best train loss: 0.1453  @ epoch 38161 )\n",
      "[Epoch: 38840] train loss: 0.1665, train acc: 0.9383, val loss: 0.8670, val acc: 0.9535  (best train acc: 0.9487, best val acc: 0.9602, best train loss: 0.1453  @ epoch 38161 )\n",
      "[Epoch: 38860] train loss: 0.1602, train acc: 0.9397, val loss: 0.8783, val acc: 0.9535  (best train acc: 0.9487, best val acc: 0.9602, best train loss: 0.1453  @ epoch 38161 )\n",
      "[Epoch: 38880] train loss: 0.1499, train acc: 0.9467, val loss: 0.8579, val acc: 0.9518  (best train acc: 0.9487, best val acc: 0.9602, best train loss: 0.1453  @ epoch 38161 )\n",
      "[Epoch: 38900] train loss: 0.1577, train acc: 0.9422, val loss: 0.8174, val acc: 0.9514  (best train acc: 0.9487, best val acc: 0.9602, best train loss: 0.1453  @ epoch 38161 )\n",
      "[Epoch: 38920] train loss: 0.1599, train acc: 0.9422, val loss: 0.8622, val acc: 0.9524  (best train acc: 0.9487, best val acc: 0.9602, best train loss: 0.1453  @ epoch 38161 )\n",
      "[Epoch: 38940] train loss: 0.1567, train acc: 0.9438, val loss: 0.8524, val acc: 0.9545  (best train acc: 0.9487, best val acc: 0.9602, best train loss: 0.1453  @ epoch 38161 )\n",
      "[Epoch: 38960] train loss: 0.1598, train acc: 0.9429, val loss: 0.7870, val acc: 0.9491  (best train acc: 0.9487, best val acc: 0.9602, best train loss: 0.1453  @ epoch 38161 )\n",
      "[Epoch: 38980] train loss: 0.1687, train acc: 0.9396, val loss: 0.8303, val acc: 0.9524  (best train acc: 0.9487, best val acc: 0.9602, best train loss: 0.1453  @ epoch 38161 )\n",
      "[Epoch: 39000] train loss: 0.1636, train acc: 0.9411, val loss: 0.7989, val acc: 0.9531  (best train acc: 0.9487, best val acc: 0.9602, best train loss: 0.1453  @ epoch 38161 )\n",
      "[Epoch: 39020] train loss: 0.1604, train acc: 0.9431, val loss: 0.7910, val acc: 0.9481  (best train acc: 0.9487, best val acc: 0.9602, best train loss: 0.1453  @ epoch 38161 )\n",
      "[Epoch: 39040] train loss: 0.1765, train acc: 0.9349, val loss: 0.7784, val acc: 0.9538  (best train acc: 0.9487, best val acc: 0.9602, best train loss: 0.1453  @ epoch 38161 )\n",
      "[Epoch: 39060] train loss: 0.1593, train acc: 0.9415, val loss: 0.7983, val acc: 0.9555  (best train acc: 0.9487, best val acc: 0.9602, best train loss: 0.1453  @ epoch 38161 )\n",
      "[Epoch: 39080] train loss: 0.1673, train acc: 0.9371, val loss: 0.8294, val acc: 0.9508  (best train acc: 0.9487, best val acc: 0.9602, best train loss: 0.1453  @ epoch 38161 )\n",
      "[Epoch: 39100] train loss: 0.1593, train acc: 0.9418, val loss: 0.8788, val acc: 0.9541  (best train acc: 0.9487, best val acc: 0.9602, best train loss: 0.1453  @ epoch 38161 )\n",
      "[Epoch: 39120] train loss: 0.1589, train acc: 0.9419, val loss: 0.9095, val acc: 0.9551  (best train acc: 0.9487, best val acc: 0.9602, best train loss: 0.1453  @ epoch 38161 )\n",
      "[Epoch: 39140] train loss: 0.1874, train acc: 0.9305, val loss: 0.6388, val acc: 0.9501  (best train acc: 0.9487, best val acc: 0.9602, best train loss: 0.1453  @ epoch 38161 )\n",
      "[Epoch: 39160] train loss: 0.1803, train acc: 0.9341, val loss: 0.6959, val acc: 0.9521  (best train acc: 0.9487, best val acc: 0.9602, best train loss: 0.1453  @ epoch 38161 )\n",
      "[Epoch: 39180] train loss: 0.1613, train acc: 0.9412, val loss: 0.6284, val acc: 0.9531  (best train acc: 0.9487, best val acc: 0.9602, best train loss: 0.1453  @ epoch 38161 )\n",
      "[Epoch: 39200] train loss: 0.1630, train acc: 0.9406, val loss: 0.6821, val acc: 0.9535  (best train acc: 0.9487, best val acc: 0.9602, best train loss: 0.1453  @ epoch 38161 )\n",
      "[Epoch: 39220] train loss: 0.1611, train acc: 0.9413, val loss: 0.7091, val acc: 0.9524  (best train acc: 0.9487, best val acc: 0.9602, best train loss: 0.1453  @ epoch 38161 )\n",
      "[Epoch: 39240] train loss: 0.1536, train acc: 0.9452, val loss: 0.7168, val acc: 0.9558  (best train acc: 0.9487, best val acc: 0.9602, best train loss: 0.1453  @ epoch 38161 )\n",
      "[Epoch: 39260] train loss: 0.1630, train acc: 0.9402, val loss: 0.6953, val acc: 0.9508  (best train acc: 0.9487, best val acc: 0.9602, best train loss: 0.1453  @ epoch 38161 )\n",
      "[Epoch: 39280] train loss: 0.1598, train acc: 0.9396, val loss: 0.7370, val acc: 0.9514  (best train acc: 0.9487, best val acc: 0.9602, best train loss: 0.1453  @ epoch 38161 )\n",
      "[Epoch: 39300] train loss: 0.1498, train acc: 0.9468, val loss: 0.7725, val acc: 0.9528  (best train acc: 0.9487, best val acc: 0.9602, best train loss: 0.1453  @ epoch 38161 )\n",
      "[Epoch: 39320] train loss: 0.1577, train acc: 0.9425, val loss: 0.7529, val acc: 0.9504  (best train acc: 0.9487, best val acc: 0.9602, best train loss: 0.1453  @ epoch 38161 )\n",
      "[Epoch: 39340] train loss: 0.1593, train acc: 0.9419, val loss: 0.7998, val acc: 0.9548  (best train acc: 0.9487, best val acc: 0.9602, best train loss: 0.1453  @ epoch 38161 )\n",
      "[Epoch: 39360] train loss: 0.1829, train acc: 0.9330, val loss: 0.7663, val acc: 0.9305  (best train acc: 0.9487, best val acc: 0.9602, best train loss: 0.1453  @ epoch 38161 )\n",
      "[Epoch: 39380] train loss: 0.1685, train acc: 0.9417, val loss: 0.7378, val acc: 0.9413  (best train acc: 0.9487, best val acc: 0.9602, best train loss: 0.1453  @ epoch 38161 )\n",
      "[Epoch: 39400] train loss: 0.1707, train acc: 0.9386, val loss: 0.7794, val acc: 0.9541  (best train acc: 0.9487, best val acc: 0.9602, best train loss: 0.1453  @ epoch 38161 )\n",
      "[Epoch: 39420] train loss: 0.1608, train acc: 0.9414, val loss: 0.7858, val acc: 0.9562  (best train acc: 0.9487, best val acc: 0.9602, best train loss: 0.1453  @ epoch 38161 )\n",
      "[Epoch: 39440] train loss: 0.1598, train acc: 0.9422, val loss: 0.7886, val acc: 0.9487  (best train acc: 0.9487, best val acc: 0.9602, best train loss: 0.1453  @ epoch 38161 )\n",
      "[Epoch: 39460] train loss: 0.1546, train acc: 0.9438, val loss: 0.7714, val acc: 0.9535  (best train acc: 0.9487, best val acc: 0.9602, best train loss: 0.1453  @ epoch 38161 )\n",
      "[Epoch: 39480] train loss: 0.1553, train acc: 0.9422, val loss: 0.7896, val acc: 0.9545  (best train acc: 0.9487, best val acc: 0.9602, best train loss: 0.1453  @ epoch 38161 )\n",
      "[Epoch: 39500] train loss: 0.1640, train acc: 0.9411, val loss: 0.7605, val acc: 0.9518  (best train acc: 0.9487, best val acc: 0.9602, best train loss: 0.1453  @ epoch 38161 )\n",
      "[Epoch: 39520] train loss: 0.1566, train acc: 0.9426, val loss: 0.7848, val acc: 0.9474  (best train acc: 0.9487, best val acc: 0.9602, best train loss: 0.1453  @ epoch 38161 )\n",
      "[Epoch: 39540] train loss: 0.1494, train acc: 0.9469, val loss: 0.7901, val acc: 0.9531  (best train acc: 0.9487, best val acc: 0.9602, best train loss: 0.1453  @ epoch 38161 )\n",
      "[Epoch: 39560] train loss: 0.1671, train acc: 0.9366, val loss: 0.7774, val acc: 0.9494  (best train acc: 0.9487, best val acc: 0.9602, best train loss: 0.1453  @ epoch 38161 )\n",
      "[Epoch: 39580] train loss: 0.1529, train acc: 0.9461, val loss: 0.8255, val acc: 0.9541  (best train acc: 0.9487, best val acc: 0.9602, best train loss: 0.1453  @ epoch 38161 )\n",
      "[Epoch: 39600] train loss: 0.2107, train acc: 0.9216, val loss: 0.7552, val acc: 0.9467  (best train acc: 0.9487, best val acc: 0.9602, best train loss: 0.1453  @ epoch 38161 )\n",
      "[Epoch: 39620] train loss: 0.1814, train acc: 0.9357, val loss: 0.8119, val acc: 0.9440  (best train acc: 0.9487, best val acc: 0.9602, best train loss: 0.1453  @ epoch 38161 )\n",
      "[Epoch: 39640] train loss: 0.1562, train acc: 0.9426, val loss: 0.8443, val acc: 0.9504  (best train acc: 0.9487, best val acc: 0.9602, best train loss: 0.1453  @ epoch 38161 )\n",
      "[Epoch: 39660] train loss: 0.1577, train acc: 0.9419, val loss: 0.8923, val acc: 0.9521  (best train acc: 0.9487, best val acc: 0.9602, best train loss: 0.1453  @ epoch 38161 )\n",
      "[Epoch: 39680] train loss: 0.1585, train acc: 0.9400, val loss: 0.8162, val acc: 0.9504  (best train acc: 0.9487, best val acc: 0.9602, best train loss: 0.1453  @ epoch 38161 )\n",
      "[Epoch: 39700] train loss: 0.1626, train acc: 0.9396, val loss: 0.8300, val acc: 0.9531  (best train acc: 0.9487, best val acc: 0.9602, best train loss: 0.1453  @ epoch 38161 )\n",
      "[Epoch: 39720] train loss: 0.1655, train acc: 0.9401, val loss: 0.8370, val acc: 0.9535  (best train acc: 0.9487, best val acc: 0.9602, best train loss: 0.1453  @ epoch 38161 )\n",
      "[Epoch: 39740] train loss: 0.1565, train acc: 0.9427, val loss: 0.8458, val acc: 0.9514  (best train acc: 0.9487, best val acc: 0.9602, best train loss: 0.1453  @ epoch 38161 )\n",
      "[Epoch: 39760] train loss: 0.1521, train acc: 0.9440, val loss: 0.8305, val acc: 0.9531  (best train acc: 0.9487, best val acc: 0.9602, best train loss: 0.1453  @ epoch 38161 )\n",
      "[Epoch: 39780] train loss: 0.1544, train acc: 0.9446, val loss: 0.8561, val acc: 0.9541  (best train acc: 0.9487, best val acc: 0.9602, best train loss: 0.1447  @ epoch 39773 )\n",
      "[Epoch: 39800] train loss: 0.1637, train acc: 0.9392, val loss: 0.8391, val acc: 0.9521  (best train acc: 0.9487, best val acc: 0.9602, best train loss: 0.1447  @ epoch 39773 )\n",
      "[Epoch: 39820] train loss: 0.1583, train acc: 0.9445, val loss: 0.8035, val acc: 0.9440  (best train acc: 0.9487, best val acc: 0.9602, best train loss: 0.1447  @ epoch 39773 )\n",
      "[Epoch: 39840] train loss: 0.1564, train acc: 0.9435, val loss: 0.8506, val acc: 0.9518  (best train acc: 0.9487, best val acc: 0.9602, best train loss: 0.1447  @ epoch 39773 )\n",
      "[Epoch: 39860] train loss: 0.1552, train acc: 0.9434, val loss: 0.8666, val acc: 0.9545  (best train acc: 0.9487, best val acc: 0.9602, best train loss: 0.1447  @ epoch 39773 )\n",
      "[Epoch: 39880] train loss: 0.1567, train acc: 0.9440, val loss: 0.8470, val acc: 0.9440  (best train acc: 0.9487, best val acc: 0.9602, best train loss: 0.1447  @ epoch 39773 )\n",
      "[Epoch: 39900] train loss: 0.1616, train acc: 0.9383, val loss: 0.9066, val acc: 0.9551  (best train acc: 0.9487, best val acc: 0.9602, best train loss: 0.1445  @ epoch 39894 )\n",
      "[Epoch: 39920] train loss: 0.1861, train acc: 0.9331, val loss: 0.8742, val acc: 0.9484  (best train acc: 0.9487, best val acc: 0.9602, best train loss: 0.1445  @ epoch 39894 )\n",
      "[Epoch: 39940] train loss: 0.1654, train acc: 0.9385, val loss: 0.8297, val acc: 0.9494  (best train acc: 0.9487, best val acc: 0.9602, best train loss: 0.1445  @ epoch 39894 )\n",
      "[Epoch: 39960] train loss: 0.1686, train acc: 0.9398, val loss: 0.8664, val acc: 0.9524  (best train acc: 0.9487, best val acc: 0.9602, best train loss: 0.1445  @ epoch 39894 )\n",
      "[Epoch: 39980] train loss: 0.1631, train acc: 0.9383, val loss: 0.8977, val acc: 0.9511  (best train acc: 0.9487, best val acc: 0.9602, best train loss: 0.1445  @ epoch 39894 )\n",
      "[Epoch: 40000] train loss: 0.1541, train acc: 0.9445, val loss: 0.8565, val acc: 0.9521  (best train acc: 0.9487, best val acc: 0.9602, best train loss: 0.1445  @ epoch 39894 )\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.75      0.82       154\n",
      "           1       0.97      0.98      0.97       703\n",
      "           2       0.93      0.94      0.94       702\n",
      "           3       0.93      0.95      0.94       703\n",
      "           4       0.99      1.00      0.99       702\n",
      "\n",
      "    accuracy                           0.95      2964\n",
      "   macro avg       0.95      0.92      0.93      2964\n",
      "weighted avg       0.95      0.95      0.95      2964\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 504x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 504x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "gnn_model = GNN7L_Sage(data_with_nedbit).to(device)\n",
    "pred = train(gnn_model, data_with_nedbit, epochs, lr, weight_decay, classes, 'GraphSAGE')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6aeea6fe202f5c7201d5940e4573c0a76b23e4e16f0e3784ac81597546f2b3b3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
