{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from GNNTrain import train, predict_from_saved_model\n",
    "from CreateDatasetv2 import get_dataset_from_graph\n",
    "from Paths import PATH_TO_GRAPHS, PATH_TO_RANKINGS\n",
    "from GDARanking import get_ranking, get_ranking_from_all_positives, validate_with_extended_dataset\n",
    "from GraphSageModel import GNN7L_Sage\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "disease_Id  = 'C0006142'\n",
    "classes     = ['P', 'LP', 'WN', 'LN', 'RN']\n",
    "model_name  = 'GraphSAGE_' + disease_Id + '_new_rankings'\n",
    "graph_path  = PATH_TO_GRAPHS + 'grafo_nedbit_' + disease_Id + '.gml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Reading graph...ok\n",
      "[+] Creating dataset...ok\n",
      "[i] Elapsed time: 40.96\n"
     ]
    }
   ],
   "source": [
    "dataset, G = get_dataset_from_graph(graph_path, disease_Id, quartile=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54af9d3efc1e4855bbec58bbfcae7aa6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40001 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch: 0000] train loss: 279.5172, train acc: 0.2495, val loss: 131.6520, val acc: 0.2496  (best train acc: 0.2495, best val acc: 0.2496, best train loss: 279.5172  @ epoch 0 )\n",
      "[Epoch: 0020] train loss: 16.7397, train acc: 0.2350, val loss: 5.2775, val acc: 0.4027  (best train acc: 0.2610, best val acc: 0.4108, best train loss: 16.7397  @ epoch 20 )\n",
      "[Epoch: 0040] train loss: 4.1308, train acc: 0.2804, val loss: 1.6959, val acc: 0.3302  (best train acc: 0.2984, best val acc: 0.4108, best train loss: 4.1308  @ epoch 40 )\n",
      "[Epoch: 0060] train loss: 1.9809, train acc: 0.2763, val loss: 1.5841, val acc: 0.2914  (best train acc: 0.3094, best val acc: 0.4108, best train loss: 1.9809  @ epoch 60 )\n",
      "[Epoch: 0080] train loss: 1.6136, train acc: 0.2720, val loss: 1.5000, val acc: 0.2934  (best train acc: 0.3094, best val acc: 0.4108, best train loss: 1.6136  @ epoch 80 )\n",
      "[Epoch: 0100] train loss: 1.5400, train acc: 0.2962, val loss: 1.4786, val acc: 0.3103  (best train acc: 0.3094, best val acc: 0.4108, best train loss: 1.5368  @ epoch 99 )\n",
      "[Epoch: 0120] train loss: 1.5065, train acc: 0.2986, val loss: 1.4633, val acc: 0.3204  (best train acc: 0.3117, best val acc: 0.4108, best train loss: 1.4932  @ epoch 116 )\n",
      "[Epoch: 0140] train loss: 1.4709, train acc: 0.3115, val loss: 1.4513, val acc: 0.3349  (best train acc: 0.3118, best val acc: 0.4108, best train loss: 1.4709  @ epoch 140 )\n",
      "[Epoch: 0160] train loss: 1.4686, train acc: 0.3122, val loss: 1.4355, val acc: 0.3467  (best train acc: 0.3167, best val acc: 0.4108, best train loss: 1.4612  @ epoch 152 )\n",
      "[Epoch: 0180] train loss: 1.4446, train acc: 0.3166, val loss: 1.4272, val acc: 0.3440  (best train acc: 0.3235, best val acc: 0.4108, best train loss: 1.4390  @ epoch 172 )\n",
      "[Epoch: 0200] train loss: 1.4404, train acc: 0.3206, val loss: 1.4161, val acc: 0.3508  (best train acc: 0.3249, best val acc: 0.4108, best train loss: 1.4377  @ epoch 187 )\n",
      "[Epoch: 0220] train loss: 1.4293, train acc: 0.3130, val loss: 1.4074, val acc: 0.3545  (best train acc: 0.3300, best val acc: 0.4108, best train loss: 1.4197  @ epoch 216 )\n",
      "[Epoch: 0240] train loss: 1.4230, train acc: 0.3137, val loss: 1.3978, val acc: 0.3595  (best train acc: 0.3300, best val acc: 0.4108, best train loss: 1.4159  @ epoch 238 )\n",
      "[Epoch: 0260] train loss: 1.4082, train acc: 0.3239, val loss: 1.3921, val acc: 0.3666  (best train acc: 0.3303, best val acc: 0.4108, best train loss: 1.4063  @ epoch 246 )\n",
      "[Epoch: 0280] train loss: 1.4061, train acc: 0.3391, val loss: 1.3813, val acc: 0.3845  (best train acc: 0.3485, best val acc: 0.4108, best train loss: 1.3967  @ epoch 274 )\n",
      "[Epoch: 0300] train loss: 1.3923, train acc: 0.3438, val loss: 1.3695, val acc: 0.3794  (best train acc: 0.3485, best val acc: 0.4108, best train loss: 1.3895  @ epoch 296 )\n",
      "[Epoch: 0320] train loss: 1.3994, train acc: 0.3443, val loss: 1.3686, val acc: 0.3936  (best train acc: 0.3535, best val acc: 0.4108, best train loss: 1.3820  @ epoch 305 )\n",
      "[Epoch: 0340] train loss: 1.3883, train acc: 0.3619, val loss: 1.3569, val acc: 0.3933  (best train acc: 0.3619, best val acc: 0.4108, best train loss: 1.3782  @ epoch 321 )\n",
      "[Epoch: 0360] train loss: 1.3657, train acc: 0.3629, val loss: 1.3491, val acc: 0.4132  (best train acc: 0.3704, best val acc: 0.4138, best train loss: 1.3657  @ epoch 360 )\n",
      "[Epoch: 0380] train loss: 1.3591, train acc: 0.4044, val loss: 1.3336, val acc: 0.4310  (best train acc: 0.4066, best val acc: 0.4317, best train loss: 1.3524  @ epoch 378 )\n",
      "[Epoch: 0400] train loss: 1.3483, train acc: 0.4103, val loss: 1.3367, val acc: 0.4509  (best train acc: 0.4136, best val acc: 0.4509, best train loss: 1.3410  @ epoch 393 )\n",
      "[Epoch: 0420] train loss: 1.3367, train acc: 0.4146, val loss: 1.3143, val acc: 0.4516  (best train acc: 0.4276, best val acc: 0.4553, best train loss: 1.3367  @ epoch 420 )\n",
      "[Epoch: 0440] train loss: 1.3313, train acc: 0.4321, val loss: 1.3135, val acc: 0.4533  (best train acc: 0.4325, best val acc: 0.4661, best train loss: 1.3202  @ epoch 432 )\n",
      "[Epoch: 0460] train loss: 1.3276, train acc: 0.4352, val loss: 1.3111, val acc: 0.4668  (best train acc: 0.4411, best val acc: 0.4718, best train loss: 1.3190  @ epoch 448 )\n",
      "[Epoch: 0480] train loss: 1.3167, train acc: 0.4391, val loss: 1.2974, val acc: 0.4702  (best train acc: 0.4474, best val acc: 0.4826, best train loss: 1.3042  @ epoch 475 )\n",
      "[Epoch: 0500] train loss: 1.3100, train acc: 0.4524, val loss: 1.2924, val acc: 0.4948  (best train acc: 0.4623, best val acc: 0.4948, best train loss: 1.2870  @ epoch 495 )\n",
      "[Epoch: 0520] train loss: 1.2852, train acc: 0.4573, val loss: 1.2762, val acc: 0.4927  (best train acc: 0.4676, best val acc: 0.4971, best train loss: 1.2808  @ epoch 505 )\n",
      "[Epoch: 0540] train loss: 1.2797, train acc: 0.4708, val loss: 1.2729, val acc: 0.5005  (best train acc: 0.4806, best val acc: 0.5008, best train loss: 1.2709  @ epoch 539 )\n",
      "[Epoch: 0560] train loss: 1.2910, train acc: 0.4663, val loss: 1.2619, val acc: 0.4988  (best train acc: 0.4883, best val acc: 0.5046, best train loss: 1.2579  @ epoch 551 )\n",
      "[Epoch: 0580] train loss: 1.2655, train acc: 0.4703, val loss: 1.2558, val acc: 0.4877  (best train acc: 0.4883, best val acc: 0.5123, best train loss: 1.2579  @ epoch 551 )\n",
      "[Epoch: 0600] train loss: 1.2523, train acc: 0.4868, val loss: 1.2446, val acc: 0.4968  (best train acc: 0.4937, best val acc: 0.5275, best train loss: 1.2458  @ epoch 590 )\n",
      "[Epoch: 0620] train loss: 1.2447, train acc: 0.4856, val loss: 1.2253, val acc: 0.5120  (best train acc: 0.5021, best val acc: 0.5275, best train loss: 1.2283  @ epoch 618 )\n",
      "[Epoch: 0640] train loss: 1.2212, train acc: 0.5026, val loss: 1.2176, val acc: 0.5295  (best train acc: 0.5093, best val acc: 0.5376, best train loss: 1.2212  @ epoch 640 )\n",
      "[Epoch: 0660] train loss: 1.2190, train acc: 0.5112, val loss: 1.2012, val acc: 0.5305  (best train acc: 0.5408, best val acc: 0.5467, best train loss: 1.1943  @ epoch 657 )\n",
      "[Epoch: 0680] train loss: 1.1994, train acc: 0.5223, val loss: 1.1967, val acc: 0.5582  (best train acc: 0.5408, best val acc: 0.5582, best train loss: 1.1830  @ epoch 679 )\n",
      "[Epoch: 0700] train loss: 1.1878, train acc: 0.5364, val loss: 1.1689, val acc: 0.5629  (best train acc: 0.5484, best val acc: 0.5656, best train loss: 1.1638  @ epoch 692 )\n",
      "[Epoch: 0720] train loss: 1.1973, train acc: 0.5192, val loss: 1.1475, val acc: 0.5690  (best train acc: 0.5596, best val acc: 0.5885, best train loss: 1.1548  @ epoch 717 )\n",
      "[Epoch: 0740] train loss: 1.1583, train acc: 0.5484, val loss: 1.1370, val acc: 0.5899  (best train acc: 0.5917, best val acc: 0.5949, best train loss: 1.1204  @ epoch 736 )\n",
      "[Epoch: 0760] train loss: 1.1215, train acc: 0.5863, val loss: 1.1093, val acc: 0.5949  (best train acc: 0.6030, best val acc: 0.5949, best train loss: 1.0876  @ epoch 759 )\n",
      "[Epoch: 0780] train loss: 1.2235, train acc: 0.5148, val loss: 1.1114, val acc: 0.5949  (best train acc: 0.6145, best val acc: 0.6037, best train loss: 1.0578  @ epoch 772 )\n",
      "[Epoch: 0800] train loss: 1.1701, train acc: 0.5387, val loss: 1.0824, val acc: 0.6027  (best train acc: 0.6145, best val acc: 0.6037, best train loss: 1.0578  @ epoch 772 )\n",
      "[Epoch: 0820] train loss: 1.1074, train acc: 0.5729, val loss: 1.0946, val acc: 0.5906  (best train acc: 0.6202, best val acc: 0.6094, best train loss: 1.0438  @ epoch 811 )\n",
      "[Epoch: 0840] train loss: 1.1102, train acc: 0.5719, val loss: 1.0597, val acc: 0.6084  (best train acc: 0.6300, best val acc: 0.6155, best train loss: 1.0253  @ epoch 839 )\n",
      "[Epoch: 0860] train loss: 1.0154, train acc: 0.6242, val loss: 1.0382, val acc: 0.6169  (best train acc: 0.6300, best val acc: 0.6179, best train loss: 1.0154  @ epoch 860 )\n",
      "[Epoch: 0880] train loss: 1.1019, train acc: 0.5721, val loss: 1.0338, val acc: 0.6128  (best train acc: 0.6309, best val acc: 0.6202, best train loss: 0.9929  @ epoch 872 )\n",
      "[Epoch: 0900] train loss: 1.1209, train acc: 0.5499, val loss: 1.0250, val acc: 0.6051  (best train acc: 0.6309, best val acc: 0.6246, best train loss: 0.9929  @ epoch 872 )\n",
      "[Epoch: 0920] train loss: 1.0274, train acc: 0.6071, val loss: 1.0155, val acc: 0.6219  (best train acc: 0.6309, best val acc: 0.6246, best train loss: 0.9929  @ epoch 872 )\n",
      "[Epoch: 0940] train loss: 1.0447, train acc: 0.5939, val loss: 1.0027, val acc: 0.6152  (best train acc: 0.6337, best val acc: 0.6317, best train loss: 0.9811  @ epoch 937 )\n",
      "[Epoch: 0960] train loss: 1.0512, train acc: 0.5860, val loss: 1.0170, val acc: 0.5862  (best train acc: 0.6478, best val acc: 0.6331, best train loss: 0.9644  @ epoch 956 )\n",
      "[Epoch: 0980] train loss: 1.0088, train acc: 0.6176, val loss: 0.9930, val acc: 0.6324  (best train acc: 0.6546, best val acc: 0.6422, best train loss: 0.9588  @ epoch 975 )\n",
      "[Epoch: 1000] train loss: 1.0147, train acc: 0.6001, val loss: 0.9692, val acc: 0.6243  (best train acc: 0.6632, best val acc: 0.6442, best train loss: 0.9405  @ epoch 987 )\n",
      "[Epoch: 1020] train loss: 0.9842, train acc: 0.6357, val loss: 0.9629, val acc: 0.6519  (best train acc: 0.6731, best val acc: 0.6567, best train loss: 0.9255  @ epoch 1003 )\n",
      "[Epoch: 1040] train loss: 0.9807, train acc: 0.6266, val loss: 0.9460, val acc: 0.6631  (best train acc: 0.6731, best val acc: 0.6631, best train loss: 0.9255  @ epoch 1003 )\n",
      "[Epoch: 1060] train loss: 0.9777, train acc: 0.6335, val loss: 0.9498, val acc: 0.6614  (best train acc: 0.6731, best val acc: 0.6651, best train loss: 0.9111  @ epoch 1051 )\n",
      "[Epoch: 1080] train loss: 0.9384, train acc: 0.6327, val loss: 0.9369, val acc: 0.6499  (best train acc: 0.6731, best val acc: 0.6742, best train loss: 0.9111  @ epoch 1051 )\n",
      "[Epoch: 1100] train loss: 1.0129, train acc: 0.6274, val loss: 0.9178, val acc: 0.6874  (best train acc: 0.6879, best val acc: 0.6874, best train loss: 0.8957  @ epoch 1098 )\n",
      "[Epoch: 1120] train loss: 0.9206, train acc: 0.6400, val loss: 0.9278, val acc: 0.6874  (best train acc: 0.6891, best val acc: 0.6938, best train loss: 0.8957  @ epoch 1098 )\n",
      "[Epoch: 1140] train loss: 0.9310, train acc: 0.6691, val loss: 0.9006, val acc: 0.6958  (best train acc: 0.6901, best val acc: 0.7039, best train loss: 0.8890  @ epoch 1131 )\n",
      "[Epoch: 1160] train loss: 0.9575, train acc: 0.6455, val loss: 0.8736, val acc: 0.6948  (best train acc: 0.7133, best val acc: 0.7106, best train loss: 0.8430  @ epoch 1145 )\n",
      "[Epoch: 1180] train loss: 0.8840, train acc: 0.6580, val loss: 0.8688, val acc: 0.7123  (best train acc: 0.7133, best val acc: 0.7241, best train loss: 0.8430  @ epoch 1145 )\n",
      "[Epoch: 1200] train loss: 0.8571, train acc: 0.6938, val loss: 0.8620, val acc: 0.7073  (best train acc: 0.7133, best val acc: 0.7285, best train loss: 0.8430  @ epoch 1145 )\n",
      "[Epoch: 1220] train loss: 0.8415, train acc: 0.6632, val loss: 0.8285, val acc: 0.7116  (best train acc: 0.7133, best val acc: 0.7298, best train loss: 0.8186  @ epoch 1219 )\n",
      "[Epoch: 1240] train loss: 0.8583, train acc: 0.6588, val loss: 0.8106, val acc: 0.7066  (best train acc: 0.7133, best val acc: 0.7427, best train loss: 0.7965  @ epoch 1236 )\n",
      "[Epoch: 1260] train loss: 0.9202, train acc: 0.6181, val loss: 0.7914, val acc: 0.7157  (best train acc: 0.7133, best val acc: 0.7595, best train loss: 0.7727  @ epoch 1245 )\n",
      "[Epoch: 1280] train loss: 0.8619, train acc: 0.6382, val loss: 0.7675, val acc: 0.7417  (best train acc: 0.7133, best val acc: 0.7595, best train loss: 0.7664  @ epoch 1269 )\n",
      "[Epoch: 1300] train loss: 0.7914, train acc: 0.6888, val loss: 0.7669, val acc: 0.7187  (best train acc: 0.7162, best val acc: 0.7595, best train loss: 0.7503  @ epoch 1298 )\n",
      "[Epoch: 1320] train loss: 0.8649, train acc: 0.6415, val loss: 0.8252, val acc: 0.6735  (best train acc: 0.7251, best val acc: 0.7595, best train loss: 0.7185  @ epoch 1305 )\n",
      "[Epoch: 1340] train loss: 0.8023, train acc: 0.6817, val loss: 0.7638, val acc: 0.7207  (best train acc: 0.7251, best val acc: 0.7595, best train loss: 0.7185  @ epoch 1305 )\n",
      "[Epoch: 1360] train loss: 0.7545, train acc: 0.7065, val loss: 0.7475, val acc: 0.7302  (best train acc: 0.7300, best val acc: 0.7595, best train loss: 0.7185  @ epoch 1305 )\n",
      "[Epoch: 1380] train loss: 0.8112, train acc: 0.6690, val loss: 0.7414, val acc: 0.7369  (best train acc: 0.7300, best val acc: 0.7595, best train loss: 0.7185  @ epoch 1305 )\n",
      "[Epoch: 1400] train loss: 0.7541, train acc: 0.7080, val loss: 0.7209, val acc: 0.7508  (best train acc: 0.7300, best val acc: 0.7646, best train loss: 0.7185  @ epoch 1305 )\n",
      "[Epoch: 1420] train loss: 0.9146, train acc: 0.6230, val loss: 0.7680, val acc: 0.7008  (best train acc: 0.7300, best val acc: 0.7646, best train loss: 0.7185  @ epoch 1305 )\n",
      "[Epoch: 1440] train loss: 0.8167, train acc: 0.6737, val loss: 0.7347, val acc: 0.7558  (best train acc: 0.7300, best val acc: 0.7649, best train loss: 0.7185  @ epoch 1305 )\n",
      "[Epoch: 1460] train loss: 0.7803, train acc: 0.6932, val loss: 0.7197, val acc: 0.7460  (best train acc: 0.7338, best val acc: 0.7649, best train loss: 0.7129  @ epoch 1458 )\n",
      "[Epoch: 1480] train loss: 0.8438, train acc: 0.6597, val loss: 0.7217, val acc: 0.7420  (best train acc: 0.7338, best val acc: 0.7649, best train loss: 0.7129  @ epoch 1458 )\n",
      "[Epoch: 1500] train loss: 0.7144, train acc: 0.7230, val loss: 0.7368, val acc: 0.7349  (best train acc: 0.7338, best val acc: 0.7649, best train loss: 0.7129  @ epoch 1458 )\n",
      "[Epoch: 1520] train loss: 0.8431, train acc: 0.6618, val loss: 0.7169, val acc: 0.7535  (best train acc: 0.7338, best val acc: 0.7649, best train loss: 0.7129  @ epoch 1458 )\n",
      "[Epoch: 1540] train loss: 0.7317, train acc: 0.7146, val loss: 0.7042, val acc: 0.7555  (best train acc: 0.7338, best val acc: 0.7649, best train loss: 0.7129  @ epoch 1458 )\n",
      "[Epoch: 1560] train loss: 0.7286, train acc: 0.7204, val loss: 0.6979, val acc: 0.7605  (best train acc: 0.7338, best val acc: 0.7761, best train loss: 0.7111  @ epoch 1550 )\n",
      "[Epoch: 1580] train loss: 0.8009, train acc: 0.6796, val loss: 0.6913, val acc: 0.7653  (best train acc: 0.7338, best val acc: 0.7761, best train loss: 0.7109  @ epoch 1577 )\n",
      "[Epoch: 1600] train loss: 0.7326, train acc: 0.7138, val loss: 0.6932, val acc: 0.7578  (best train acc: 0.7338, best val acc: 0.7761, best train loss: 0.7044  @ epoch 1595 )\n",
      "[Epoch: 1620] train loss: 0.8025, train acc: 0.6671, val loss: 0.6922, val acc: 0.7619  (best train acc: 0.7338, best val acc: 0.7761, best train loss: 0.7026  @ epoch 1602 )\n",
      "[Epoch: 1640] train loss: 0.7427, train acc: 0.7024, val loss: 0.6872, val acc: 0.7622  (best train acc: 0.7338, best val acc: 0.7761, best train loss: 0.7017  @ epoch 1622 )\n",
      "[Epoch: 1660] train loss: 0.7791, train acc: 0.6863, val loss: 0.6774, val acc: 0.7605  (best train acc: 0.7423, best val acc: 0.7761, best train loss: 0.6885  @ epoch 1642 )\n",
      "[Epoch: 1680] train loss: 0.7692, train acc: 0.6921, val loss: 0.6762, val acc: 0.7683  (best train acc: 0.7423, best val acc: 0.7784, best train loss: 0.6885  @ epoch 1642 )\n",
      "[Epoch: 1700] train loss: 0.7578, train acc: 0.6860, val loss: 0.6710, val acc: 0.7713  (best train acc: 0.7423, best val acc: 0.7875, best train loss: 0.6885  @ epoch 1642 )\n",
      "[Epoch: 1720] train loss: 0.8154, train acc: 0.6721, val loss: 0.6712, val acc: 0.7639  (best train acc: 0.7441, best val acc: 0.7875, best train loss: 0.6815  @ epoch 1713 )\n",
      "[Epoch: 1740] train loss: 0.7731, train acc: 0.6876, val loss: 0.6623, val acc: 0.7707  (best train acc: 0.7441, best val acc: 0.7902, best train loss: 0.6683  @ epoch 1725 )\n",
      "[Epoch: 1760] train loss: 0.8499, train acc: 0.6423, val loss: 0.6656, val acc: 0.7707  (best train acc: 0.7441, best val acc: 0.7939, best train loss: 0.6683  @ epoch 1725 )\n",
      "[Epoch: 1780] train loss: 0.8126, train acc: 0.6729, val loss: 0.6527, val acc: 0.7754  (best train acc: 0.7441, best val acc: 0.7939, best train loss: 0.6683  @ epoch 1725 )\n",
      "[Epoch: 1800] train loss: 0.7230, train acc: 0.7084, val loss: 0.6515, val acc: 0.7872  (best train acc: 0.7441, best val acc: 0.7939, best train loss: 0.6683  @ epoch 1725 )\n",
      "[Epoch: 1820] train loss: 0.7153, train acc: 0.7218, val loss: 0.6560, val acc: 0.7723  (best train acc: 0.7441, best val acc: 0.7939, best train loss: 0.6683  @ epoch 1725 )\n",
      "[Epoch: 1840] train loss: 0.7110, train acc: 0.7138, val loss: 0.6394, val acc: 0.7922  (best train acc: 0.7441, best val acc: 0.7939, best train loss: 0.6683  @ epoch 1725 )\n",
      "[Epoch: 1860] train loss: 0.7399, train acc: 0.7070, val loss: 0.6349, val acc: 0.7872  (best train acc: 0.7480, best val acc: 0.8010, best train loss: 0.6614  @ epoch 1849 )\n",
      "[Epoch: 1880] train loss: 0.6711, train acc: 0.7363, val loss: 0.6301, val acc: 0.8007  (best train acc: 0.7480, best val acc: 0.8010, best train loss: 0.6614  @ epoch 1849 )\n",
      "[Epoch: 1900] train loss: 0.6754, train acc: 0.7419, val loss: 0.6230, val acc: 0.8017  (best train acc: 0.7612, best val acc: 0.8061, best train loss: 0.6521  @ epoch 1892 )\n",
      "[Epoch: 1920] train loss: 0.8092, train acc: 0.6665, val loss: 0.6413, val acc: 0.7835  (best train acc: 0.7612, best val acc: 0.8061, best train loss: 0.6521  @ epoch 1892 )\n",
      "[Epoch: 1940] train loss: 0.7321, train acc: 0.7111, val loss: 0.6212, val acc: 0.7956  (best train acc: 0.7612, best val acc: 0.8061, best train loss: 0.6521  @ epoch 1892 )\n",
      "[Epoch: 1960] train loss: 0.6975, train acc: 0.7260, val loss: 0.6207, val acc: 0.7922  (best train acc: 0.7612, best val acc: 0.8074, best train loss: 0.6521  @ epoch 1892 )\n",
      "[Epoch: 1980] train loss: 0.7698, train acc: 0.6882, val loss: 0.6126, val acc: 0.8064  (best train acc: 0.7612, best val acc: 0.8074, best train loss: 0.6521  @ epoch 1892 )\n",
      "[Epoch: 2000] train loss: 0.6505, train acc: 0.7594, val loss: 0.6102, val acc: 0.8044  (best train acc: 0.7612, best val acc: 0.8101, best train loss: 0.6503  @ epoch 1992 )\n",
      "[Epoch: 2020] train loss: 0.7082, train acc: 0.7178, val loss: 0.6234, val acc: 0.7791  (best train acc: 0.7612, best val acc: 0.8132, best train loss: 0.6503  @ epoch 1992 )\n",
      "[Epoch: 2040] train loss: 0.7961, train acc: 0.6729, val loss: 0.6165, val acc: 0.8067  (best train acc: 0.7612, best val acc: 0.8132, best train loss: 0.6503  @ epoch 1992 )\n",
      "[Epoch: 2060] train loss: 0.7262, train acc: 0.7141, val loss: 0.6236, val acc: 0.7794  (best train acc: 0.7612, best val acc: 0.8162, best train loss: 0.6410  @ epoch 2047 )\n",
      "[Epoch: 2080] train loss: 0.6665, train acc: 0.7434, val loss: 0.6021, val acc: 0.8003  (best train acc: 0.7616, best val acc: 0.8162, best train loss: 0.6292  @ epoch 2066 )\n",
      "[Epoch: 2100] train loss: 0.6638, train acc: 0.7493, val loss: 0.5956, val acc: 0.8132  (best train acc: 0.7616, best val acc: 0.8162, best train loss: 0.6292  @ epoch 2066 )\n",
      "[Epoch: 2120] train loss: 0.6847, train acc: 0.7299, val loss: 0.5908, val acc: 0.8125  (best train acc: 0.7616, best val acc: 0.8162, best train loss: 0.6292  @ epoch 2066 )\n",
      "[Epoch: 2140] train loss: 0.6516, train acc: 0.7563, val loss: 0.5922, val acc: 0.8128  (best train acc: 0.7616, best val acc: 0.8165, best train loss: 0.6292  @ epoch 2066 )\n",
      "[Epoch: 2160] train loss: 0.6658, train acc: 0.7386, val loss: 0.5844, val acc: 0.8094  (best train acc: 0.7616, best val acc: 0.8169, best train loss: 0.6292  @ epoch 2066 )\n",
      "[Epoch: 2180] train loss: 0.6358, train acc: 0.7567, val loss: 0.6034, val acc: 0.7781  (best train acc: 0.7616, best val acc: 0.8175, best train loss: 0.6292  @ epoch 2066 )\n",
      "[Epoch: 2200] train loss: 0.7255, train acc: 0.7139, val loss: 0.5865, val acc: 0.8145  (best train acc: 0.7627, best val acc: 0.8216, best train loss: 0.6292  @ epoch 2066 )\n",
      "[Epoch: 2220] train loss: 0.6409, train acc: 0.7593, val loss: 0.5818, val acc: 0.8132  (best train acc: 0.7627, best val acc: 0.8216, best train loss: 0.6292  @ epoch 2066 )\n",
      "[Epoch: 2240] train loss: 0.6839, train acc: 0.7355, val loss: 0.5945, val acc: 0.7936  (best train acc: 0.7627, best val acc: 0.8216, best train loss: 0.6292  @ epoch 2066 )\n",
      "[Epoch: 2260] train loss: 0.7228, train acc: 0.7199, val loss: 0.6085, val acc: 0.7929  (best train acc: 0.7627, best val acc: 0.8216, best train loss: 0.6292  @ epoch 2066 )\n",
      "[Epoch: 2280] train loss: 0.6924, train acc: 0.7222, val loss: 0.5727, val acc: 0.8128  (best train acc: 0.7662, best val acc: 0.8239, best train loss: 0.6292  @ epoch 2066 )\n",
      "[Epoch: 2300] train loss: 0.6536, train acc: 0.7527, val loss: 0.5702, val acc: 0.8229  (best train acc: 0.7662, best val acc: 0.8246, best train loss: 0.6282  @ epoch 2283 )\n",
      "[Epoch: 2320] train loss: 0.7024, train acc: 0.7240, val loss: 0.5874, val acc: 0.8040  (best train acc: 0.7667, best val acc: 0.8246, best train loss: 0.6077  @ epoch 2313 )\n",
      "[Epoch: 2340] train loss: 0.6871, train acc: 0.7294, val loss: 0.5821, val acc: 0.8027  (best train acc: 0.7667, best val acc: 0.8246, best train loss: 0.6077  @ epoch 2313 )\n",
      "[Epoch: 2360] train loss: 0.7233, train acc: 0.7008, val loss: 0.5592, val acc: 0.8189  (best train acc: 0.7667, best val acc: 0.8246, best train loss: 0.6077  @ epoch 2313 )\n",
      "[Epoch: 2380] train loss: 0.7091, train acc: 0.7143, val loss: 0.6003, val acc: 0.7939  (best train acc: 0.7715, best val acc: 0.8246, best train loss: 0.6077  @ epoch 2313 )\n",
      "[Epoch: 2400] train loss: 0.6782, train acc: 0.7225, val loss: 0.5596, val acc: 0.8179  (best train acc: 0.7715, best val acc: 0.8246, best train loss: 0.6077  @ epoch 2313 )\n",
      "[Epoch: 2420] train loss: 0.7131, train acc: 0.7081, val loss: 0.5582, val acc: 0.8189  (best train acc: 0.7739, best val acc: 0.8246, best train loss: 0.6036  @ epoch 2409 )\n",
      "[Epoch: 2440] train loss: 0.6258, train acc: 0.7564, val loss: 0.5573, val acc: 0.8054  (best train acc: 0.7781, best val acc: 0.8250, best train loss: 0.6036  @ epoch 2409 )\n",
      "[Epoch: 2460] train loss: 0.7115, train acc: 0.7145, val loss: 0.5502, val acc: 0.8118  (best train acc: 0.7781, best val acc: 0.8250, best train loss: 0.6036  @ epoch 2409 )\n",
      "[Epoch: 2480] train loss: 0.7199, train acc: 0.7070, val loss: 0.5475, val acc: 0.8233  (best train acc: 0.7781, best val acc: 0.8256, best train loss: 0.6002  @ epoch 2479 )\n",
      "[Epoch: 2500] train loss: 0.6404, train acc: 0.7501, val loss: 0.5573, val acc: 0.8162  (best train acc: 0.7781, best val acc: 0.8307, best train loss: 0.5929  @ epoch 2485 )\n",
      "[Epoch: 2520] train loss: 0.6165, train acc: 0.7641, val loss: 0.5409, val acc: 0.8270  (best train acc: 0.7781, best val acc: 0.8317, best train loss: 0.5929  @ epoch 2485 )\n",
      "[Epoch: 2540] train loss: 0.6209, train acc: 0.7653, val loss: 0.5415, val acc: 0.8250  (best train acc: 0.7781, best val acc: 0.8317, best train loss: 0.5929  @ epoch 2485 )\n",
      "[Epoch: 2560] train loss: 0.7305, train acc: 0.6959, val loss: 0.5427, val acc: 0.8175  (best train acc: 0.7794, best val acc: 0.8327, best train loss: 0.5878  @ epoch 2552 )\n",
      "[Epoch: 2580] train loss: 0.6460, train acc: 0.7475, val loss: 0.5240, val acc: 0.8354  (best train acc: 0.7794, best val acc: 0.8354, best train loss: 0.5878  @ epoch 2552 )\n",
      "[Epoch: 2600] train loss: 0.6217, train acc: 0.7602, val loss: 0.5380, val acc: 0.8236  (best train acc: 0.7794, best val acc: 0.8364, best train loss: 0.5846  @ epoch 2584 )\n",
      "[Epoch: 2620] train loss: 0.5978, train acc: 0.7734, val loss: 0.5338, val acc: 0.8250  (best train acc: 0.7794, best val acc: 0.8364, best train loss: 0.5846  @ epoch 2584 )\n",
      "[Epoch: 2640] train loss: 0.5984, train acc: 0.7664, val loss: 0.5170, val acc: 0.8314  (best train acc: 0.7794, best val acc: 0.8364, best train loss: 0.5846  @ epoch 2584 )\n",
      "[Epoch: 2660] train loss: 0.6470, train acc: 0.7517, val loss: 0.5258, val acc: 0.8219  (best train acc: 0.7846, best val acc: 0.8364, best train loss: 0.5747  @ epoch 2659 )\n",
      "[Epoch: 2680] train loss: 0.7125, train acc: 0.7068, val loss: 0.5302, val acc: 0.8266  (best train acc: 0.7846, best val acc: 0.8364, best train loss: 0.5747  @ epoch 2659 )\n",
      "[Epoch: 2700] train loss: 0.6115, train acc: 0.7621, val loss: 0.5355, val acc: 0.8250  (best train acc: 0.7846, best val acc: 0.8364, best train loss: 0.5747  @ epoch 2659 )\n",
      "[Epoch: 2720] train loss: 0.6013, train acc: 0.7653, val loss: 0.5310, val acc: 0.8108  (best train acc: 0.7846, best val acc: 0.8384, best train loss: 0.5747  @ epoch 2659 )\n",
      "[Epoch: 2740] train loss: 0.6180, train acc: 0.7614, val loss: 0.5200, val acc: 0.8324  (best train acc: 0.7889, best val acc: 0.8391, best train loss: 0.5747  @ epoch 2659 )\n",
      "[Epoch: 2760] train loss: 0.6506, train acc: 0.7389, val loss: 0.5114, val acc: 0.8337  (best train acc: 0.7889, best val acc: 0.8405, best train loss: 0.5747  @ epoch 2659 )\n",
      "[Epoch: 2780] train loss: 0.6342, train acc: 0.7453, val loss: 0.5093, val acc: 0.8358  (best train acc: 0.7889, best val acc: 0.8442, best train loss: 0.5747  @ epoch 2659 )\n",
      "[Epoch: 2800] train loss: 0.6345, train acc: 0.7413, val loss: 0.5342, val acc: 0.8138  (best train acc: 0.7898, best val acc: 0.8442, best train loss: 0.5628  @ epoch 2787 )\n",
      "[Epoch: 2820] train loss: 0.6899, train acc: 0.7115, val loss: 0.5008, val acc: 0.8411  (best train acc: 0.7898, best val acc: 0.8442, best train loss: 0.5628  @ epoch 2787 )\n",
      "[Epoch: 2840] train loss: 0.5884, train acc: 0.7733, val loss: 0.5030, val acc: 0.8442  (best train acc: 0.7906, best val acc: 0.8442, best train loss: 0.5628  @ epoch 2787 )\n",
      "[Epoch: 2860] train loss: 0.5717, train acc: 0.7897, val loss: 0.4962, val acc: 0.8418  (best train acc: 0.7906, best val acc: 0.8442, best train loss: 0.5604  @ epoch 2855 )\n",
      "[Epoch: 2880] train loss: 0.6211, train acc: 0.7600, val loss: 0.5014, val acc: 0.8384  (best train acc: 0.7906, best val acc: 0.8442, best train loss: 0.5604  @ epoch 2855 )\n",
      "[Epoch: 2900] train loss: 0.5780, train acc: 0.7833, val loss: 0.4899, val acc: 0.8408  (best train acc: 0.7906, best val acc: 0.8459, best train loss: 0.5604  @ epoch 2855 )\n",
      "[Epoch: 2920] train loss: 0.5904, train acc: 0.7697, val loss: 0.4958, val acc: 0.8344  (best train acc: 0.7906, best val acc: 0.8459, best train loss: 0.5604  @ epoch 2855 )\n",
      "[Epoch: 2940] train loss: 0.5950, train acc: 0.7714, val loss: 0.4866, val acc: 0.8398  (best train acc: 0.7912, best val acc: 0.8459, best train loss: 0.5540  @ epoch 2925 )\n",
      "[Epoch: 2960] train loss: 0.6494, train acc: 0.7403, val loss: 0.4851, val acc: 0.8398  (best train acc: 0.7984, best val acc: 0.8459, best train loss: 0.5465  @ epoch 2941 )\n",
      "[Epoch: 2980] train loss: 0.5701, train acc: 0.7811, val loss: 0.4917, val acc: 0.8337  (best train acc: 0.7984, best val acc: 0.8459, best train loss: 0.5465  @ epoch 2941 )\n",
      "[Epoch: 3000] train loss: 0.6395, train acc: 0.7468, val loss: 0.4792, val acc: 0.8398  (best train acc: 0.7984, best val acc: 0.8459, best train loss: 0.5459  @ epoch 2983 )\n",
      "[Epoch: 3020] train loss: 0.6032, train acc: 0.7606, val loss: 0.4765, val acc: 0.8435  (best train acc: 0.7988, best val acc: 0.8459, best train loss: 0.5434  @ epoch 3010 )\n",
      "[Epoch: 3040] train loss: 0.6755, train acc: 0.7303, val loss: 0.4747, val acc: 0.8442  (best train acc: 0.8038, best val acc: 0.8459, best train loss: 0.5312  @ epoch 3036 )\n",
      "[Epoch: 3060] train loss: 0.5962, train acc: 0.7606, val loss: 0.4867, val acc: 0.8310  (best train acc: 0.8038, best val acc: 0.8459, best train loss: 0.5312  @ epoch 3036 )\n",
      "[Epoch: 3080] train loss: 0.6073, train acc: 0.7616, val loss: 0.4666, val acc: 0.8449  (best train acc: 0.8038, best val acc: 0.8465, best train loss: 0.5294  @ epoch 3063 )\n",
      "[Epoch: 3100] train loss: 0.6629, train acc: 0.7234, val loss: 0.4673, val acc: 0.8445  (best train acc: 0.8038, best val acc: 0.8472, best train loss: 0.5294  @ epoch 3063 )\n",
      "[Epoch: 3120] train loss: 0.5667, train acc: 0.7753, val loss: 0.4697, val acc: 0.8425  (best train acc: 0.8038, best val acc: 0.8472, best train loss: 0.5294  @ epoch 3063 )\n",
      "[Epoch: 3140] train loss: 0.5859, train acc: 0.7687, val loss: 0.4687, val acc: 0.8472  (best train acc: 0.8082, best val acc: 0.8472, best train loss: 0.5181  @ epoch 3135 )\n",
      "[Epoch: 3160] train loss: 0.5767, train acc: 0.7765, val loss: 0.4692, val acc: 0.8425  (best train acc: 0.8082, best val acc: 0.8492, best train loss: 0.5181  @ epoch 3135 )\n",
      "[Epoch: 3180] train loss: 0.6136, train acc: 0.7637, val loss: 0.4635, val acc: 0.8479  (best train acc: 0.8082, best val acc: 0.8503, best train loss: 0.5181  @ epoch 3135 )\n",
      "[Epoch: 3200] train loss: 0.5163, train acc: 0.8105, val loss: 0.4670, val acc: 0.8405  (best train acc: 0.8105, best val acc: 0.8503, best train loss: 0.5163  @ epoch 3200 )\n",
      "[Epoch: 3220] train loss: 0.5599, train acc: 0.7836, val loss: 0.4652, val acc: 0.8425  (best train acc: 0.8105, best val acc: 0.8523, best train loss: 0.5163  @ epoch 3200 )\n",
      "[Epoch: 3240] train loss: 0.5387, train acc: 0.7962, val loss: 0.4666, val acc: 0.8486  (best train acc: 0.8105, best val acc: 0.8523, best train loss: 0.5163  @ epoch 3200 )\n",
      "[Epoch: 3260] train loss: 0.5615, train acc: 0.7807, val loss: 0.4590, val acc: 0.8432  (best train acc: 0.8105, best val acc: 0.8523, best train loss: 0.5163  @ epoch 3200 )\n",
      "[Epoch: 3280] train loss: 0.5432, train acc: 0.7919, val loss: 0.4559, val acc: 0.8469  (best train acc: 0.8105, best val acc: 0.8523, best train loss: 0.5163  @ epoch 3200 )\n",
      "[Epoch: 3300] train loss: 0.5373, train acc: 0.7987, val loss: 0.4568, val acc: 0.8513  (best train acc: 0.8105, best val acc: 0.8540, best train loss: 0.5163  @ epoch 3200 )\n",
      "[Epoch: 3320] train loss: 0.5383, train acc: 0.7984, val loss: 0.4539, val acc: 0.8449  (best train acc: 0.8105, best val acc: 0.8540, best train loss: 0.5077  @ epoch 3311 )\n",
      "[Epoch: 3340] train loss: 0.6314, train acc: 0.7421, val loss: 0.4716, val acc: 0.8418  (best train acc: 0.8105, best val acc: 0.8540, best train loss: 0.5077  @ epoch 3311 )\n",
      "[Epoch: 3360] train loss: 0.5139, train acc: 0.8022, val loss: 0.4485, val acc: 0.8476  (best train acc: 0.8111, best val acc: 0.8540, best train loss: 0.5046  @ epoch 3343 )\n",
      "[Epoch: 3380] train loss: 0.5172, train acc: 0.8034, val loss: 0.4503, val acc: 0.8459  (best train acc: 0.8111, best val acc: 0.8540, best train loss: 0.5046  @ epoch 3343 )\n",
      "[Epoch: 3400] train loss: 0.5271, train acc: 0.8001, val loss: 0.4473, val acc: 0.8438  (best train acc: 0.8111, best val acc: 0.8540, best train loss: 0.5046  @ epoch 3343 )\n",
      "[Epoch: 3420] train loss: 0.5158, train acc: 0.8078, val loss: 0.4545, val acc: 0.8459  (best train acc: 0.8126, best val acc: 0.8540, best train loss: 0.5046  @ epoch 3343 )\n",
      "[Epoch: 3440] train loss: 0.5336, train acc: 0.7921, val loss: 0.4385, val acc: 0.8479  (best train acc: 0.8126, best val acc: 0.8540, best train loss: 0.5046  @ epoch 3343 )\n",
      "[Epoch: 3460] train loss: 0.5083, train acc: 0.8084, val loss: 0.4460, val acc: 0.8472  (best train acc: 0.8142, best val acc: 0.8546, best train loss: 0.5015  @ epoch 3448 )\n",
      "[Epoch: 3480] train loss: 0.5227, train acc: 0.7951, val loss: 0.4527, val acc: 0.8405  (best train acc: 0.8159, best val acc: 0.8546, best train loss: 0.5011  @ epoch 3462 )\n",
      "[Epoch: 3500] train loss: 0.5055, train acc: 0.8117, val loss: 0.4446, val acc: 0.8519  (best train acc: 0.8159, best val acc: 0.8577, best train loss: 0.5011  @ epoch 3462 )\n",
      "[Epoch: 3520] train loss: 0.5262, train acc: 0.7934, val loss: 0.4396, val acc: 0.8395  (best train acc: 0.8159, best val acc: 0.8577, best train loss: 0.4947  @ epoch 3506 )\n",
      "[Epoch: 3540] train loss: 0.6282, train acc: 0.7392, val loss: 0.4561, val acc: 0.8411  (best train acc: 0.8159, best val acc: 0.8577, best train loss: 0.4887  @ epoch 3539 )\n",
      "[Epoch: 3560] train loss: 0.5210, train acc: 0.7978, val loss: 0.4343, val acc: 0.8489  (best train acc: 0.8175, best val acc: 0.8583, best train loss: 0.4887  @ epoch 3539 )\n",
      "[Epoch: 3580] train loss: 0.5461, train acc: 0.7833, val loss: 0.4362, val acc: 0.8472  (best train acc: 0.8175, best val acc: 0.8583, best train loss: 0.4887  @ epoch 3539 )\n",
      "[Epoch: 3600] train loss: 0.5250, train acc: 0.7971, val loss: 0.4405, val acc: 0.8482  (best train acc: 0.8235, best val acc: 0.8583, best train loss: 0.4862  @ epoch 3595 )\n",
      "[Epoch: 3620] train loss: 0.4955, train acc: 0.8112, val loss: 0.4375, val acc: 0.8489  (best train acc: 0.8235, best val acc: 0.8583, best train loss: 0.4820  @ epoch 3618 )\n",
      "[Epoch: 3640] train loss: 0.5332, train acc: 0.7855, val loss: 0.4271, val acc: 0.8489  (best train acc: 0.8235, best val acc: 0.8583, best train loss: 0.4820  @ epoch 3618 )\n",
      "[Epoch: 3660] train loss: 0.5474, train acc: 0.7726, val loss: 0.4373, val acc: 0.8449  (best train acc: 0.8235, best val acc: 0.8583, best train loss: 0.4820  @ epoch 3618 )\n",
      "[Epoch: 3680] train loss: 0.5224, train acc: 0.8047, val loss: 0.4426, val acc: 0.8486  (best train acc: 0.8235, best val acc: 0.8583, best train loss: 0.4820  @ epoch 3618 )\n",
      "[Epoch: 3700] train loss: 0.5428, train acc: 0.7817, val loss: 0.4388, val acc: 0.8459  (best train acc: 0.8235, best val acc: 0.8583, best train loss: 0.4820  @ epoch 3618 )\n",
      "[Epoch: 3720] train loss: 0.5041, train acc: 0.7981, val loss: 0.4314, val acc: 0.8479  (best train acc: 0.8235, best val acc: 0.8583, best train loss: 0.4820  @ epoch 3618 )\n",
      "[Epoch: 3740] train loss: 0.5860, train acc: 0.7637, val loss: 0.4320, val acc: 0.8523  (best train acc: 0.8235, best val acc: 0.8583, best train loss: 0.4820  @ epoch 3618 )\n",
      "[Epoch: 3760] train loss: 0.5088, train acc: 0.8019, val loss: 0.4278, val acc: 0.8472  (best train acc: 0.8235, best val acc: 0.8583, best train loss: 0.4820  @ epoch 3618 )\n",
      "[Epoch: 3780] train loss: 0.5212, train acc: 0.7948, val loss: 0.4335, val acc: 0.8465  (best train acc: 0.8235, best val acc: 0.8590, best train loss: 0.4776  @ epoch 3779 )\n",
      "[Epoch: 3800] train loss: 0.5510, train acc: 0.7901, val loss: 0.4271, val acc: 0.8428  (best train acc: 0.8235, best val acc: 0.8590, best train loss: 0.4776  @ epoch 3779 )\n",
      "[Epoch: 3820] train loss: 0.5009, train acc: 0.8088, val loss: 0.4228, val acc: 0.8563  (best train acc: 0.8245, best val acc: 0.8590, best train loss: 0.4684  @ epoch 3816 )\n",
      "[Epoch: 3840] train loss: 0.5002, train acc: 0.7984, val loss: 0.4220, val acc: 0.8526  (best train acc: 0.8271, best val acc: 0.8590, best train loss: 0.4637  @ epoch 3829 )\n",
      "[Epoch: 3860] train loss: 0.5676, train acc: 0.7749, val loss: 0.4304, val acc: 0.8486  (best train acc: 0.8271, best val acc: 0.8590, best train loss: 0.4637  @ epoch 3829 )\n",
      "[Epoch: 3880] train loss: 0.5115, train acc: 0.8000, val loss: 0.4267, val acc: 0.8492  (best train acc: 0.8271, best val acc: 0.8590, best train loss: 0.4637  @ epoch 3829 )\n",
      "[Epoch: 3900] train loss: 0.5295, train acc: 0.7909, val loss: 0.4232, val acc: 0.8509  (best train acc: 0.8271, best val acc: 0.8594, best train loss: 0.4637  @ epoch 3829 )\n",
      "[Epoch: 3920] train loss: 0.5071, train acc: 0.8055, val loss: 0.4178, val acc: 0.8509  (best train acc: 0.8271, best val acc: 0.8607, best train loss: 0.4637  @ epoch 3829 )\n",
      "[Epoch: 3940] train loss: 0.4760, train acc: 0.8183, val loss: 0.4202, val acc: 0.8546  (best train acc: 0.8271, best val acc: 0.8607, best train loss: 0.4637  @ epoch 3829 )\n",
      "[Epoch: 3960] train loss: 0.4827, train acc: 0.8170, val loss: 0.4176, val acc: 0.8526  (best train acc: 0.8271, best val acc: 0.8607, best train loss: 0.4637  @ epoch 3829 )\n",
      "[Epoch: 3980] train loss: 0.4816, train acc: 0.8128, val loss: 0.4231, val acc: 0.8506  (best train acc: 0.8292, best val acc: 0.8607, best train loss: 0.4622  @ epoch 3970 )\n",
      "[Epoch: 4000] train loss: 0.4900, train acc: 0.8203, val loss: 0.4146, val acc: 0.8543  (best train acc: 0.8292, best val acc: 0.8607, best train loss: 0.4601  @ epoch 3999 )\n",
      "[Epoch: 4020] train loss: 0.5078, train acc: 0.7962, val loss: 0.4158, val acc: 0.8526  (best train acc: 0.8292, best val acc: 0.8607, best train loss: 0.4601  @ epoch 3999 )\n",
      "[Epoch: 4040] train loss: 0.4874, train acc: 0.8115, val loss: 0.4125, val acc: 0.8536  (best train acc: 0.8292, best val acc: 0.8607, best train loss: 0.4601  @ epoch 3999 )\n",
      "[Epoch: 4060] train loss: 0.5230, train acc: 0.7940, val loss: 0.4261, val acc: 0.8594  (best train acc: 0.8292, best val acc: 0.8607, best train loss: 0.4601  @ epoch 3999 )\n",
      "[Epoch: 4080] train loss: 0.4988, train acc: 0.8028, val loss: 0.4178, val acc: 0.8496  (best train acc: 0.8304, best val acc: 0.8607, best train loss: 0.4601  @ epoch 3999 )\n",
      "[Epoch: 4100] train loss: 0.4855, train acc: 0.8182, val loss: 0.4157, val acc: 0.8556  (best train acc: 0.8304, best val acc: 0.8607, best train loss: 0.4542  @ epoch 4098 )\n",
      "[Epoch: 4120] train loss: 0.5165, train acc: 0.8029, val loss: 0.4095, val acc: 0.8530  (best train acc: 0.8304, best val acc: 0.8607, best train loss: 0.4542  @ epoch 4098 )\n",
      "[Epoch: 4140] train loss: 0.4768, train acc: 0.8158, val loss: 0.4027, val acc: 0.8536  (best train acc: 0.8304, best val acc: 0.8607, best train loss: 0.4542  @ epoch 4098 )\n",
      "[Epoch: 4160] train loss: 0.4733, train acc: 0.8233, val loss: 0.4026, val acc: 0.8580  (best train acc: 0.8371, best val acc: 0.8607, best train loss: 0.4504  @ epoch 4159 )\n",
      "[Epoch: 4180] train loss: 0.4765, train acc: 0.8211, val loss: 0.4023, val acc: 0.8560  (best train acc: 0.8371, best val acc: 0.8607, best train loss: 0.4504  @ epoch 4159 )\n",
      "[Epoch: 4200] train loss: 0.4583, train acc: 0.8323, val loss: 0.4039, val acc: 0.8590  (best train acc: 0.8389, best val acc: 0.8607, best train loss: 0.4402  @ epoch 4184 )\n",
      "[Epoch: 4220] train loss: 0.4678, train acc: 0.8245, val loss: 0.4301, val acc: 0.8307  (best train acc: 0.8389, best val acc: 0.8607, best train loss: 0.4402  @ epoch 4184 )\n",
      "[Epoch: 4240] train loss: 0.4735, train acc: 0.8211, val loss: 0.4003, val acc: 0.8580  (best train acc: 0.8389, best val acc: 0.8607, best train loss: 0.4402  @ epoch 4184 )\n",
      "[Epoch: 4260] train loss: 0.4371, train acc: 0.8435, val loss: 0.4006, val acc: 0.8543  (best train acc: 0.8435, best val acc: 0.8607, best train loss: 0.4368  @ epoch 4251 )\n",
      "[Epoch: 4280] train loss: 0.4676, train acc: 0.8200, val loss: 0.3975, val acc: 0.8583  (best train acc: 0.8435, best val acc: 0.8607, best train loss: 0.4368  @ epoch 4251 )\n",
      "[Epoch: 4300] train loss: 0.4508, train acc: 0.8349, val loss: 0.3977, val acc: 0.8573  (best train acc: 0.8435, best val acc: 0.8607, best train loss: 0.4261  @ epoch 4293 )\n",
      "[Epoch: 4320] train loss: 0.4313, train acc: 0.8388, val loss: 0.4075, val acc: 0.8567  (best train acc: 0.8435, best val acc: 0.8610, best train loss: 0.4261  @ epoch 4293 )\n",
      "[Epoch: 4340] train loss: 0.4427, train acc: 0.8357, val loss: 0.3936, val acc: 0.8553  (best train acc: 0.8435, best val acc: 0.8610, best train loss: 0.4261  @ epoch 4293 )\n",
      "[Epoch: 4360] train loss: 0.4449, train acc: 0.8315, val loss: 0.3950, val acc: 0.8624  (best train acc: 0.8435, best val acc: 0.8624, best train loss: 0.4261  @ epoch 4293 )\n",
      "[Epoch: 4380] train loss: 0.4943, train acc: 0.8005, val loss: 0.3998, val acc: 0.8556  (best train acc: 0.8435, best val acc: 0.8624, best train loss: 0.4261  @ epoch 4293 )\n",
      "[Epoch: 4400] train loss: 0.4867, train acc: 0.8056, val loss: 0.4255, val acc: 0.8503  (best train acc: 0.8435, best val acc: 0.8624, best train loss: 0.4251  @ epoch 4388 )\n",
      "[Epoch: 4420] train loss: 0.4493, train acc: 0.8335, val loss: 0.3911, val acc: 0.8580  (best train acc: 0.8435, best val acc: 0.8624, best train loss: 0.4251  @ epoch 4388 )\n",
      "[Epoch: 4440] train loss: 0.4805, train acc: 0.8169, val loss: 0.3909, val acc: 0.8543  (best train acc: 0.8495, best val acc: 0.8624, best train loss: 0.4251  @ epoch 4388 )\n",
      "[Epoch: 4460] train loss: 0.4436, train acc: 0.8297, val loss: 0.4015, val acc: 0.8567  (best train acc: 0.8495, best val acc: 0.8624, best train loss: 0.4160  @ epoch 4456 )\n",
      "[Epoch: 4480] train loss: 0.4401, train acc: 0.8324, val loss: 0.4056, val acc: 0.8526  (best train acc: 0.8495, best val acc: 0.8624, best train loss: 0.4160  @ epoch 4456 )\n",
      "[Epoch: 4500] train loss: 0.4540, train acc: 0.8292, val loss: 0.4131, val acc: 0.8486  (best train acc: 0.8495, best val acc: 0.8624, best train loss: 0.4160  @ epoch 4456 )\n",
      "[Epoch: 4520] train loss: 0.5046, train acc: 0.8055, val loss: 0.3946, val acc: 0.8577  (best train acc: 0.8495, best val acc: 0.8637, best train loss: 0.4160  @ epoch 4456 )\n",
      "[Epoch: 4540] train loss: 0.4297, train acc: 0.8327, val loss: 0.3902, val acc: 0.8570  (best train acc: 0.8495, best val acc: 0.8637, best train loss: 0.4160  @ epoch 4456 )\n",
      "[Epoch: 4560] train loss: 0.4632, train acc: 0.8214, val loss: 0.3904, val acc: 0.8587  (best train acc: 0.8495, best val acc: 0.8637, best train loss: 0.4160  @ epoch 4456 )\n",
      "[Epoch: 4580] train loss: 0.4324, train acc: 0.8380, val loss: 0.3942, val acc: 0.8536  (best train acc: 0.8495, best val acc: 0.8637, best train loss: 0.4160  @ epoch 4572 )\n",
      "[Epoch: 4600] train loss: 0.4534, train acc: 0.8325, val loss: 0.3886, val acc: 0.8597  (best train acc: 0.8502, best val acc: 0.8644, best train loss: 0.4107  @ epoch 4586 )\n",
      "[Epoch: 4620] train loss: 0.4289, train acc: 0.8387, val loss: 0.3898, val acc: 0.8597  (best train acc: 0.8502, best val acc: 0.8644, best train loss: 0.4074  @ epoch 4607 )\n",
      "[Epoch: 4640] train loss: 0.4288, train acc: 0.8422, val loss: 0.3809, val acc: 0.8573  (best train acc: 0.8502, best val acc: 0.8648, best train loss: 0.4074  @ epoch 4607 )\n",
      "[Epoch: 4660] train loss: 0.4330, train acc: 0.8358, val loss: 0.3875, val acc: 0.8583  (best train acc: 0.8502, best val acc: 0.8648, best train loss: 0.4074  @ epoch 4607 )\n",
      "[Epoch: 4680] train loss: 0.4193, train acc: 0.8401, val loss: 0.3816, val acc: 0.8587  (best train acc: 0.8537, best val acc: 0.8648, best train loss: 0.3976  @ epoch 4679 )\n",
      "[Epoch: 4700] train loss: 0.4317, train acc: 0.8360, val loss: 0.3837, val acc: 0.8573  (best train acc: 0.8537, best val acc: 0.8648, best train loss: 0.3976  @ epoch 4679 )\n",
      "[Epoch: 4720] train loss: 0.4312, train acc: 0.8376, val loss: 0.3847, val acc: 0.8604  (best train acc: 0.8537, best val acc: 0.8648, best train loss: 0.3976  @ epoch 4679 )\n",
      "[Epoch: 4740] train loss: 0.4254, train acc: 0.8378, val loss: 0.3882, val acc: 0.8556  (best train acc: 0.8537, best val acc: 0.8648, best train loss: 0.3976  @ epoch 4679 )\n",
      "[Epoch: 4760] train loss: 0.4088, train acc: 0.8440, val loss: 0.3790, val acc: 0.8583  (best train acc: 0.8537, best val acc: 0.8648, best train loss: 0.3976  @ epoch 4679 )\n",
      "[Epoch: 4780] train loss: 0.4392, train acc: 0.8289, val loss: 0.3825, val acc: 0.8573  (best train acc: 0.8537, best val acc: 0.8648, best train loss: 0.3976  @ epoch 4679 )\n",
      "[Epoch: 4800] train loss: 0.4011, train acc: 0.8458, val loss: 0.3765, val acc: 0.8567  (best train acc: 0.8537, best val acc: 0.8648, best train loss: 0.3972  @ epoch 4795 )\n",
      "[Epoch: 4820] train loss: 0.4130, train acc: 0.8427, val loss: 0.3840, val acc: 0.8624  (best train acc: 0.8537, best val acc: 0.8648, best train loss: 0.3972  @ epoch 4795 )\n",
      "[Epoch: 4840] train loss: 0.4602, train acc: 0.8276, val loss: 0.3751, val acc: 0.8560  (best train acc: 0.8537, best val acc: 0.8648, best train loss: 0.3972  @ epoch 4795 )\n",
      "[Epoch: 4860] train loss: 0.4609, train acc: 0.8179, val loss: 0.3731, val acc: 0.8580  (best train acc: 0.8537, best val acc: 0.8648, best train loss: 0.3972  @ epoch 4795 )\n",
      "[Epoch: 4880] train loss: 0.4306, train acc: 0.8339, val loss: 0.3706, val acc: 0.8648  (best train acc: 0.8537, best val acc: 0.8648, best train loss: 0.3972  @ epoch 4795 )\n",
      "[Epoch: 4900] train loss: 0.4200, train acc: 0.8430, val loss: 0.3762, val acc: 0.8590  (best train acc: 0.8543, best val acc: 0.8648, best train loss: 0.3951  @ epoch 4896 )\n",
      "[Epoch: 4920] train loss: 0.4630, train acc: 0.8195, val loss: 0.3797, val acc: 0.8580  (best train acc: 0.8543, best val acc: 0.8651, best train loss: 0.3951  @ epoch 4896 )\n",
      "[Epoch: 4940] train loss: 0.4407, train acc: 0.8273, val loss: 0.3761, val acc: 0.8631  (best train acc: 0.8543, best val acc: 0.8661, best train loss: 0.3951  @ epoch 4896 )\n",
      "[Epoch: 4960] train loss: 0.4232, train acc: 0.8344, val loss: 0.3733, val acc: 0.8614  (best train acc: 0.8543, best val acc: 0.8664, best train loss: 0.3951  @ epoch 4896 )\n",
      "[Epoch: 4980] train loss: 0.4293, train acc: 0.8249, val loss: 0.3718, val acc: 0.8604  (best train acc: 0.8552, best val acc: 0.8671, best train loss: 0.3907  @ epoch 4972 )\n",
      "[Epoch: 5000] train loss: 0.4093, train acc: 0.8494, val loss: 0.3686, val acc: 0.8631  (best train acc: 0.8552, best val acc: 0.8671, best train loss: 0.3890  @ epoch 4982 )\n",
      "[Epoch: 5020] train loss: 0.4221, train acc: 0.8323, val loss: 0.3711, val acc: 0.8621  (best train acc: 0.8552, best val acc: 0.8675, best train loss: 0.3890  @ epoch 4982 )\n",
      "[Epoch: 5040] train loss: 0.4192, train acc: 0.8314, val loss: 0.3688, val acc: 0.8597  (best train acc: 0.8552, best val acc: 0.8681, best train loss: 0.3890  @ epoch 4982 )\n",
      "[Epoch: 5060] train loss: 0.4034, train acc: 0.8462, val loss: 0.3670, val acc: 0.8634  (best train acc: 0.8552, best val acc: 0.8722, best train loss: 0.3887  @ epoch 5056 )\n",
      "[Epoch: 5080] train loss: 0.4076, train acc: 0.8419, val loss: 0.3591, val acc: 0.8624  (best train acc: 0.8552, best val acc: 0.8722, best train loss: 0.3887  @ epoch 5056 )\n",
      "[Epoch: 5100] train loss: 0.4305, train acc: 0.8232, val loss: 0.3646, val acc: 0.8641  (best train acc: 0.8552, best val acc: 0.8722, best train loss: 0.3877  @ epoch 5097 )\n",
      "[Epoch: 5120] train loss: 0.4135, train acc: 0.8397, val loss: 0.3624, val acc: 0.8624  (best train acc: 0.8552, best val acc: 0.8722, best train loss: 0.3818  @ epoch 5113 )\n",
      "[Epoch: 5140] train loss: 0.4127, train acc: 0.8496, val loss: 0.3526, val acc: 0.8681  (best train acc: 0.8556, best val acc: 0.8722, best train loss: 0.3818  @ epoch 5113 )\n",
      "[Epoch: 5160] train loss: 0.3868, train acc: 0.8483, val loss: 0.3665, val acc: 0.8597  (best train acc: 0.8556, best val acc: 0.8725, best train loss: 0.3818  @ epoch 5113 )\n",
      "[Epoch: 5180] train loss: 0.3883, train acc: 0.8560, val loss: 0.3542, val acc: 0.8695  (best train acc: 0.8565, best val acc: 0.8752, best train loss: 0.3818  @ epoch 5113 )\n",
      "[Epoch: 5200] train loss: 0.3950, train acc: 0.8459, val loss: 0.3536, val acc: 0.8705  (best train acc: 0.8565, best val acc: 0.8752, best train loss: 0.3818  @ epoch 5113 )\n",
      "[Epoch: 5220] train loss: 0.3839, train acc: 0.8519, val loss: 0.3567, val acc: 0.8675  (best train acc: 0.8565, best val acc: 0.8752, best train loss: 0.3818  @ epoch 5113 )\n",
      "[Epoch: 5240] train loss: 0.3971, train acc: 0.8443, val loss: 0.3545, val acc: 0.8742  (best train acc: 0.8565, best val acc: 0.8793, best train loss: 0.3818  @ epoch 5113 )\n",
      "[Epoch: 5260] train loss: 0.4028, train acc: 0.8420, val loss: 0.3543, val acc: 0.8671  (best train acc: 0.8605, best val acc: 0.8793, best train loss: 0.3787  @ epoch 5246 )\n",
      "[Epoch: 5280] train loss: 0.4122, train acc: 0.8461, val loss: 0.3477, val acc: 0.8759  (best train acc: 0.8605, best val acc: 0.8793, best train loss: 0.3787  @ epoch 5246 )\n",
      "[Epoch: 5300] train loss: 0.3869, train acc: 0.8487, val loss: 0.3481, val acc: 0.8742  (best train acc: 0.8605, best val acc: 0.8793, best train loss: 0.3787  @ epoch 5246 )\n",
      "[Epoch: 5320] train loss: 0.3883, train acc: 0.8474, val loss: 0.3777, val acc: 0.8560  (best train acc: 0.8605, best val acc: 0.8793, best train loss: 0.3787  @ epoch 5246 )\n",
      "[Epoch: 5340] train loss: 0.4110, train acc: 0.8376, val loss: 0.3463, val acc: 0.8752  (best train acc: 0.8605, best val acc: 0.8793, best train loss: 0.3730  @ epoch 5338 )\n",
      "[Epoch: 5360] train loss: 0.3926, train acc: 0.8496, val loss: 0.3571, val acc: 0.8678  (best train acc: 0.8644, best val acc: 0.8793, best train loss: 0.3668  @ epoch 5349 )\n",
      "[Epoch: 5380] train loss: 0.3931, train acc: 0.8491, val loss: 0.3474, val acc: 0.8789  (best train acc: 0.8644, best val acc: 0.8793, best train loss: 0.3668  @ epoch 5349 )\n",
      "[Epoch: 5400] train loss: 0.3910, train acc: 0.8506, val loss: 0.3539, val acc: 0.8651  (best train acc: 0.8644, best val acc: 0.8793, best train loss: 0.3668  @ epoch 5349 )\n",
      "[Epoch: 5420] train loss: 0.3885, train acc: 0.8535, val loss: 0.3559, val acc: 0.8732  (best train acc: 0.8644, best val acc: 0.8793, best train loss: 0.3668  @ epoch 5349 )\n",
      "[Epoch: 5440] train loss: 0.4060, train acc: 0.8372, val loss: 0.3646, val acc: 0.8637  (best train acc: 0.8644, best val acc: 0.8793, best train loss: 0.3668  @ epoch 5349 )\n",
      "[Epoch: 5460] train loss: 0.4011, train acc: 0.8404, val loss: 0.3427, val acc: 0.8786  (best train acc: 0.8644, best val acc: 0.8793, best train loss: 0.3668  @ epoch 5349 )\n",
      "[Epoch: 5480] train loss: 0.3829, train acc: 0.8555, val loss: 0.3465, val acc: 0.8789  (best train acc: 0.8644, best val acc: 0.8799, best train loss: 0.3668  @ epoch 5349 )\n",
      "[Epoch: 5500] train loss: 0.3859, train acc: 0.8511, val loss: 0.3374, val acc: 0.8806  (best train acc: 0.8644, best val acc: 0.8806, best train loss: 0.3668  @ epoch 5349 )\n",
      "[Epoch: 5520] train loss: 0.3870, train acc: 0.8496, val loss: 0.3479, val acc: 0.8725  (best train acc: 0.8644, best val acc: 0.8806, best train loss: 0.3668  @ epoch 5349 )\n",
      "[Epoch: 5540] train loss: 0.3740, train acc: 0.8608, val loss: 0.3396, val acc: 0.8739  (best train acc: 0.8644, best val acc: 0.8806, best train loss: 0.3668  @ epoch 5349 )\n",
      "[Epoch: 5560] train loss: 0.3784, train acc: 0.8516, val loss: 0.3462, val acc: 0.8739  (best train acc: 0.8644, best val acc: 0.8806, best train loss: 0.3668  @ epoch 5349 )\n",
      "[Epoch: 5580] train loss: 0.4263, train acc: 0.8317, val loss: 0.3433, val acc: 0.8779  (best train acc: 0.8644, best val acc: 0.8806, best train loss: 0.3668  @ epoch 5349 )\n",
      "[Epoch: 5600] train loss: 0.3828, train acc: 0.8522, val loss: 0.3952, val acc: 0.8411  (best train acc: 0.8644, best val acc: 0.8806, best train loss: 0.3668  @ epoch 5349 )\n",
      "[Epoch: 5620] train loss: 0.3987, train acc: 0.8435, val loss: 0.3506, val acc: 0.8654  (best train acc: 0.8644, best val acc: 0.8806, best train loss: 0.3668  @ epoch 5349 )\n",
      "[Epoch: 5640] train loss: 0.4407, train acc: 0.8236, val loss: 0.3504, val acc: 0.8735  (best train acc: 0.8644, best val acc: 0.8813, best train loss: 0.3668  @ epoch 5349 )\n",
      "[Epoch: 5660] train loss: 0.3869, train acc: 0.8519, val loss: 0.3622, val acc: 0.8654  (best train acc: 0.8644, best val acc: 0.8826, best train loss: 0.3668  @ epoch 5349 )\n",
      "[Epoch: 5680] train loss: 0.4319, train acc: 0.8309, val loss: 0.3435, val acc: 0.8830  (best train acc: 0.8644, best val acc: 0.8830, best train loss: 0.3668  @ epoch 5349 )\n",
      "[Epoch: 5700] train loss: 0.3864, train acc: 0.8527, val loss: 0.3447, val acc: 0.8712  (best train acc: 0.8644, best val acc: 0.8830, best train loss: 0.3668  @ epoch 5349 )\n",
      "[Epoch: 5720] train loss: 0.3883, train acc: 0.8553, val loss: 0.3319, val acc: 0.8853  (best train acc: 0.8644, best val acc: 0.8853, best train loss: 0.3650  @ epoch 5713 )\n",
      "[Epoch: 5740] train loss: 0.3980, train acc: 0.8411, val loss: 0.3426, val acc: 0.8745  (best train acc: 0.8644, best val acc: 0.8853, best train loss: 0.3650  @ epoch 5713 )\n",
      "[Epoch: 5760] train loss: 0.3814, train acc: 0.8595, val loss: 0.3417, val acc: 0.8705  (best train acc: 0.8663, best val acc: 0.8880, best train loss: 0.3650  @ epoch 5713 )\n",
      "[Epoch: 5780] train loss: 0.4154, train acc: 0.8356, val loss: 0.3711, val acc: 0.8587  (best train acc: 0.8663, best val acc: 0.8880, best train loss: 0.3577  @ epoch 5771 )\n",
      "[Epoch: 5800] train loss: 0.3802, train acc: 0.8512, val loss: 0.3366, val acc: 0.8755  (best train acc: 0.8663, best val acc: 0.8880, best train loss: 0.3577  @ epoch 5771 )\n",
      "[Epoch: 5820] train loss: 0.4075, train acc: 0.8389, val loss: 0.3326, val acc: 0.8823  (best train acc: 0.8663, best val acc: 0.8880, best train loss: 0.3577  @ epoch 5771 )\n",
      "[Epoch: 5840] train loss: 0.3826, train acc: 0.8544, val loss: 0.3440, val acc: 0.8732  (best train acc: 0.8700, best val acc: 0.8880, best train loss: 0.3457  @ epoch 5836 )\n",
      "[Epoch: 5860] train loss: 0.4290, train acc: 0.8310, val loss: 0.3442, val acc: 0.8772  (best train acc: 0.8700, best val acc: 0.8880, best train loss: 0.3457  @ epoch 5836 )\n",
      "[Epoch: 5880] train loss: 0.3849, train acc: 0.8523, val loss: 0.3355, val acc: 0.8745  (best train acc: 0.8700, best val acc: 0.8880, best train loss: 0.3457  @ epoch 5836 )\n",
      "[Epoch: 5900] train loss: 0.3898, train acc: 0.8501, val loss: 0.3440, val acc: 0.8735  (best train acc: 0.8700, best val acc: 0.8880, best train loss: 0.3457  @ epoch 5836 )\n",
      "[Epoch: 5920] train loss: 0.3934, train acc: 0.8477, val loss: 0.3559, val acc: 0.8712  (best train acc: 0.8700, best val acc: 0.8880, best train loss: 0.3457  @ epoch 5836 )\n",
      "[Epoch: 5940] train loss: 0.3913, train acc: 0.8477, val loss: 0.3363, val acc: 0.8722  (best train acc: 0.8700, best val acc: 0.8880, best train loss: 0.3457  @ epoch 5836 )\n",
      "[Epoch: 5960] train loss: 0.3954, train acc: 0.8475, val loss: 0.3393, val acc: 0.8705  (best train acc: 0.8700, best val acc: 0.8880, best train loss: 0.3457  @ epoch 5836 )\n",
      "[Epoch: 5980] train loss: 0.3871, train acc: 0.8457, val loss: 0.3610, val acc: 0.8685  (best train acc: 0.8700, best val acc: 0.8880, best train loss: 0.3457  @ epoch 5836 )\n",
      "[Epoch: 6000] train loss: 0.3843, train acc: 0.8508, val loss: 0.3406, val acc: 0.8776  (best train acc: 0.8700, best val acc: 0.8880, best train loss: 0.3457  @ epoch 5836 )\n",
      "[Epoch: 6020] train loss: 0.3734, train acc: 0.8572, val loss: 0.3276, val acc: 0.8836  (best train acc: 0.8700, best val acc: 0.8880, best train loss: 0.3457  @ epoch 5836 )\n",
      "[Epoch: 6040] train loss: 0.3853, train acc: 0.8577, val loss: 0.3416, val acc: 0.8691  (best train acc: 0.8700, best val acc: 0.8880, best train loss: 0.3457  @ epoch 5836 )\n",
      "[Epoch: 6060] train loss: 0.3514, train acc: 0.8686, val loss: 0.3314, val acc: 0.8766  (best train acc: 0.8700, best val acc: 0.8880, best train loss: 0.3457  @ epoch 5836 )\n",
      "[Epoch: 6080] train loss: 0.3780, train acc: 0.8518, val loss: 0.3318, val acc: 0.8769  (best train acc: 0.8712, best val acc: 0.8880, best train loss: 0.3457  @ epoch 5836 )\n",
      "[Epoch: 6100] train loss: 0.3738, train acc: 0.8572, val loss: 0.3235, val acc: 0.8853  (best train acc: 0.8712, best val acc: 0.8880, best train loss: 0.3457  @ epoch 5836 )\n",
      "[Epoch: 6120] train loss: 0.3713, train acc: 0.8590, val loss: 0.3877, val acc: 0.8597  (best train acc: 0.8767, best val acc: 0.8880, best train loss: 0.3457  @ epoch 5836 )\n",
      "[Epoch: 6140] train loss: 0.4372, train acc: 0.8314, val loss: 0.3300, val acc: 0.8786  (best train acc: 0.8767, best val acc: 0.8880, best train loss: 0.3457  @ epoch 5836 )\n",
      "[Epoch: 6160] train loss: 0.3684, train acc: 0.8609, val loss: 0.3332, val acc: 0.8739  (best train acc: 0.8767, best val acc: 0.8880, best train loss: 0.3457  @ epoch 5836 )\n",
      "[Epoch: 6180] train loss: 0.3467, train acc: 0.8728, val loss: 0.3331, val acc: 0.8782  (best train acc: 0.8767, best val acc: 0.8880, best train loss: 0.3417  @ epoch 6175 )\n",
      "[Epoch: 6200] train loss: 0.3653, train acc: 0.8545, val loss: 0.3282, val acc: 0.8779  (best train acc: 0.8767, best val acc: 0.8880, best train loss: 0.3417  @ epoch 6175 )\n",
      "[Epoch: 6220] train loss: 0.4001, train acc: 0.8433, val loss: 0.3867, val acc: 0.8496  (best train acc: 0.8767, best val acc: 0.8880, best train loss: 0.3417  @ epoch 6175 )\n",
      "[Epoch: 6240] train loss: 0.3662, train acc: 0.8592, val loss: 0.3232, val acc: 0.8813  (best train acc: 0.8767, best val acc: 0.8880, best train loss: 0.3417  @ epoch 6175 )\n",
      "[Epoch: 6260] train loss: 0.3755, train acc: 0.8506, val loss: 0.3254, val acc: 0.8813  (best train acc: 0.8767, best val acc: 0.8880, best train loss: 0.3417  @ epoch 6175 )\n",
      "[Epoch: 6280] train loss: 0.3650, train acc: 0.8584, val loss: 0.3392, val acc: 0.8745  (best train acc: 0.8767, best val acc: 0.8880, best train loss: 0.3384  @ epoch 6278 )\n",
      "[Epoch: 6300] train loss: 0.3461, train acc: 0.8681, val loss: 0.3274, val acc: 0.8809  (best train acc: 0.8767, best val acc: 0.8901, best train loss: 0.3384  @ epoch 6278 )\n",
      "[Epoch: 6320] train loss: 0.3881, train acc: 0.8519, val loss: 0.3188, val acc: 0.8820  (best train acc: 0.8768, best val acc: 0.8901, best train loss: 0.3384  @ epoch 6278 )\n",
      "[Epoch: 6340] train loss: 0.3440, train acc: 0.8697, val loss: 0.3193, val acc: 0.8867  (best train acc: 0.8768, best val acc: 0.8901, best train loss: 0.3384  @ epoch 6278 )\n",
      "[Epoch: 6360] train loss: 0.3894, train acc: 0.8455, val loss: 0.3196, val acc: 0.8857  (best train acc: 0.8768, best val acc: 0.8901, best train loss: 0.3384  @ epoch 6278 )\n",
      "[Epoch: 6380] train loss: 0.3503, train acc: 0.8657, val loss: 0.3346, val acc: 0.8766  (best train acc: 0.8768, best val acc: 0.8901, best train loss: 0.3360  @ epoch 6372 )\n",
      "[Epoch: 6400] train loss: 0.3493, train acc: 0.8722, val loss: 0.3266, val acc: 0.8803  (best train acc: 0.8768, best val acc: 0.8901, best train loss: 0.3360  @ epoch 6372 )\n",
      "[Epoch: 6420] train loss: 0.3454, train acc: 0.8670, val loss: 0.3167, val acc: 0.8867  (best train acc: 0.8768, best val acc: 0.8901, best train loss: 0.3360  @ epoch 6372 )\n",
      "[Epoch: 6440] train loss: 0.3634, train acc: 0.8579, val loss: 0.3358, val acc: 0.8725  (best train acc: 0.8768, best val acc: 0.8901, best train loss: 0.3360  @ epoch 6372 )\n",
      "[Epoch: 6460] train loss: 0.3999, train acc: 0.8492, val loss: 0.3188, val acc: 0.8877  (best train acc: 0.8768, best val acc: 0.8901, best train loss: 0.3360  @ epoch 6372 )\n",
      "[Epoch: 6480] train loss: 0.3510, train acc: 0.8683, val loss: 0.3185, val acc: 0.8860  (best train acc: 0.8768, best val acc: 0.8901, best train loss: 0.3360  @ epoch 6372 )\n",
      "[Epoch: 6500] train loss: 0.3633, train acc: 0.8540, val loss: 0.3378, val acc: 0.8712  (best train acc: 0.8768, best val acc: 0.8901, best train loss: 0.3360  @ epoch 6372 )\n",
      "[Epoch: 6520] train loss: 0.3710, train acc: 0.8567, val loss: 0.3206, val acc: 0.8863  (best train acc: 0.8788, best val acc: 0.8901, best train loss: 0.3265  @ epoch 6518 )\n",
      "[Epoch: 6540] train loss: 0.3701, train acc: 0.8559, val loss: 0.3209, val acc: 0.8813  (best train acc: 0.8788, best val acc: 0.8901, best train loss: 0.3265  @ epoch 6518 )\n",
      "[Epoch: 6560] train loss: 0.3689, train acc: 0.8582, val loss: 0.3365, val acc: 0.8718  (best train acc: 0.8788, best val acc: 0.8901, best train loss: 0.3265  @ epoch 6518 )\n",
      "[Epoch: 6580] train loss: 0.3597, train acc: 0.8677, val loss: 0.3236, val acc: 0.8776  (best train acc: 0.8788, best val acc: 0.8901, best train loss: 0.3265  @ epoch 6518 )\n",
      "[Epoch: 6600] train loss: 0.3618, train acc: 0.8613, val loss: 0.3154, val acc: 0.8840  (best train acc: 0.8788, best val acc: 0.8901, best train loss: 0.3265  @ epoch 6518 )\n",
      "[Epoch: 6620] train loss: 0.3405, train acc: 0.8686, val loss: 0.3127, val acc: 0.8850  (best train acc: 0.8788, best val acc: 0.8901, best train loss: 0.3265  @ epoch 6518 )\n",
      "[Epoch: 6640] train loss: 0.3528, train acc: 0.8683, val loss: 0.3292, val acc: 0.8796  (best train acc: 0.8788, best val acc: 0.8901, best train loss: 0.3265  @ epoch 6518 )\n",
      "[Epoch: 6660] train loss: 0.3939, train acc: 0.8446, val loss: 0.3132, val acc: 0.8843  (best train acc: 0.8788, best val acc: 0.8901, best train loss: 0.3209  @ epoch 6646 )\n",
      "[Epoch: 6680] train loss: 0.3545, train acc: 0.8612, val loss: 0.3235, val acc: 0.8766  (best train acc: 0.8792, best val acc: 0.8901, best train loss: 0.3209  @ epoch 6646 )\n",
      "[Epoch: 6700] train loss: 0.3589, train acc: 0.8608, val loss: 0.3168, val acc: 0.8772  (best train acc: 0.8792, best val acc: 0.8901, best train loss: 0.3209  @ epoch 6646 )\n",
      "[Epoch: 6720] train loss: 0.3503, train acc: 0.8667, val loss: 0.3189, val acc: 0.8806  (best train acc: 0.8792, best val acc: 0.8901, best train loss: 0.3209  @ epoch 6646 )\n",
      "[Epoch: 6740] train loss: 0.3514, train acc: 0.8603, val loss: 0.3121, val acc: 0.8826  (best train acc: 0.8792, best val acc: 0.8901, best train loss: 0.3209  @ epoch 6646 )\n",
      "[Epoch: 6760] train loss: 0.3343, train acc: 0.8765, val loss: 0.3076, val acc: 0.8860  (best train acc: 0.8792, best val acc: 0.8901, best train loss: 0.3209  @ epoch 6646 )\n",
      "[Epoch: 6780] train loss: 0.3378, train acc: 0.8724, val loss: 0.3139, val acc: 0.8850  (best train acc: 0.8792, best val acc: 0.8907, best train loss: 0.3209  @ epoch 6646 )\n",
      "[Epoch: 6800] train loss: 0.3467, train acc: 0.8674, val loss: 0.3093, val acc: 0.8823  (best train acc: 0.8792, best val acc: 0.8907, best train loss: 0.3209  @ epoch 6646 )\n",
      "[Epoch: 6820] train loss: 0.3515, train acc: 0.8639, val loss: 0.3072, val acc: 0.8874  (best train acc: 0.8803, best val acc: 0.8907, best train loss: 0.3209  @ epoch 6646 )\n",
      "[Epoch: 6840] train loss: 0.3482, train acc: 0.8666, val loss: 0.3146, val acc: 0.8823  (best train acc: 0.8803, best val acc: 0.8907, best train loss: 0.3209  @ epoch 6646 )\n",
      "[Epoch: 6860] train loss: 0.3594, train acc: 0.8617, val loss: 0.3217, val acc: 0.8779  (best train acc: 0.8803, best val acc: 0.8907, best train loss: 0.3209  @ epoch 6646 )\n",
      "[Epoch: 6880] train loss: 0.3868, train acc: 0.8511, val loss: 0.3192, val acc: 0.8793  (best train acc: 0.8803, best val acc: 0.8907, best train loss: 0.3209  @ epoch 6646 )\n",
      "[Epoch: 6900] train loss: 0.3445, train acc: 0.8626, val loss: 0.3144, val acc: 0.8836  (best train acc: 0.8803, best val acc: 0.8907, best train loss: 0.3209  @ epoch 6646 )\n",
      "[Epoch: 6920] train loss: 0.3582, train acc: 0.8656, val loss: 0.3120, val acc: 0.8836  (best train acc: 0.8803, best val acc: 0.8907, best train loss: 0.3209  @ epoch 6646 )\n",
      "[Epoch: 6940] train loss: 0.3333, train acc: 0.8719, val loss: 0.3228, val acc: 0.8769  (best train acc: 0.8814, best val acc: 0.8907, best train loss: 0.3209  @ epoch 6646 )\n",
      "[Epoch: 6960] train loss: 0.3537, train acc: 0.8692, val loss: 0.3202, val acc: 0.8779  (best train acc: 0.8814, best val acc: 0.8907, best train loss: 0.3209  @ epoch 6646 )\n",
      "[Epoch: 6980] train loss: 0.3715, train acc: 0.8585, val loss: 0.3379, val acc: 0.8732  (best train acc: 0.8814, best val acc: 0.8907, best train loss: 0.3209  @ epoch 6646 )\n",
      "[Epoch: 7000] train loss: 0.3495, train acc: 0.8595, val loss: 0.3098, val acc: 0.8799  (best train acc: 0.8814, best val acc: 0.8907, best train loss: 0.3209  @ epoch 6646 )\n",
      "[Epoch: 7020] train loss: 0.3448, train acc: 0.8675, val loss: 0.3023, val acc: 0.8867  (best train acc: 0.8814, best val acc: 0.8907, best train loss: 0.3208  @ epoch 7005 )\n",
      "[Epoch: 7040] train loss: 0.3320, train acc: 0.8757, val loss: 0.3225, val acc: 0.8772  (best train acc: 0.8814, best val acc: 0.8907, best train loss: 0.3208  @ epoch 7005 )\n",
      "[Epoch: 7060] train loss: 0.3565, train acc: 0.8664, val loss: 0.3330, val acc: 0.8728  (best train acc: 0.8814, best val acc: 0.8907, best train loss: 0.3176  @ epoch 7052 )\n",
      "[Epoch: 7080] train loss: 0.3721, train acc: 0.8569, val loss: 0.3083, val acc: 0.8840  (best train acc: 0.8814, best val acc: 0.8907, best train loss: 0.3176  @ epoch 7052 )\n",
      "[Epoch: 7100] train loss: 0.3760, train acc: 0.8564, val loss: 0.3187, val acc: 0.8799  (best train acc: 0.8814, best val acc: 0.8907, best train loss: 0.3176  @ epoch 7052 )\n",
      "[Epoch: 7120] train loss: 0.3579, train acc: 0.8670, val loss: 0.3077, val acc: 0.8877  (best train acc: 0.8814, best val acc: 0.8907, best train loss: 0.3176  @ epoch 7052 )\n",
      "[Epoch: 7140] train loss: 0.3480, train acc: 0.8689, val loss: 0.3425, val acc: 0.8749  (best train acc: 0.8814, best val acc: 0.8914, best train loss: 0.3176  @ epoch 7052 )\n",
      "[Epoch: 7160] train loss: 0.3476, train acc: 0.8608, val loss: 0.3213, val acc: 0.8786  (best train acc: 0.8814, best val acc: 0.8914, best train loss: 0.3176  @ epoch 7052 )\n",
      "[Epoch: 7180] train loss: 0.3612, train acc: 0.8629, val loss: 0.3135, val acc: 0.8799  (best train acc: 0.8814, best val acc: 0.8914, best train loss: 0.3176  @ epoch 7052 )\n",
      "[Epoch: 7200] train loss: 0.3677, train acc: 0.8604, val loss: 0.3044, val acc: 0.8826  (best train acc: 0.8814, best val acc: 0.8914, best train loss: 0.3176  @ epoch 7052 )\n",
      "[Epoch: 7220] train loss: 0.3309, train acc: 0.8732, val loss: 0.2990, val acc: 0.8850  (best train acc: 0.8831, best val acc: 0.8914, best train loss: 0.3176  @ epoch 7052 )\n",
      "[Epoch: 7240] train loss: 0.3306, train acc: 0.8752, val loss: 0.3020, val acc: 0.8880  (best train acc: 0.8831, best val acc: 0.8914, best train loss: 0.3176  @ epoch 7052 )\n",
      "[Epoch: 7260] train loss: 0.3559, train acc: 0.8619, val loss: 0.3023, val acc: 0.8857  (best train acc: 0.8831, best val acc: 0.8914, best train loss: 0.3176  @ epoch 7052 )\n",
      "[Epoch: 7280] train loss: 0.3404, train acc: 0.8668, val loss: 0.2982, val acc: 0.8880  (best train acc: 0.8831, best val acc: 0.8914, best train loss: 0.3176  @ epoch 7052 )\n",
      "[Epoch: 7300] train loss: 0.3232, train acc: 0.8757, val loss: 0.2987, val acc: 0.8867  (best train acc: 0.8850, best val acc: 0.8914, best train loss: 0.3093  @ epoch 7284 )\n",
      "[Epoch: 7320] train loss: 0.3804, train acc: 0.8520, val loss: 0.3071, val acc: 0.8877  (best train acc: 0.8850, best val acc: 0.8914, best train loss: 0.3093  @ epoch 7284 )\n",
      "[Epoch: 7340] train loss: 0.3222, train acc: 0.8830, val loss: 0.2987, val acc: 0.8877  (best train acc: 0.8850, best val acc: 0.8914, best train loss: 0.3093  @ epoch 7284 )\n",
      "[Epoch: 7360] train loss: 0.3378, train acc: 0.8638, val loss: 0.3038, val acc: 0.8877  (best train acc: 0.8850, best val acc: 0.8914, best train loss: 0.3093  @ epoch 7284 )\n",
      "[Epoch: 7380] train loss: 0.3409, train acc: 0.8651, val loss: 0.3057, val acc: 0.8860  (best train acc: 0.8856, best val acc: 0.8914, best train loss: 0.3050  @ epoch 7375 )\n",
      "[Epoch: 7400] train loss: 0.3609, train acc: 0.8540, val loss: 0.3245, val acc: 0.8789  (best train acc: 0.8856, best val acc: 0.8914, best train loss: 0.3050  @ epoch 7375 )\n",
      "[Epoch: 7420] train loss: 0.3624, train acc: 0.8593, val loss: 0.3183, val acc: 0.8809  (best train acc: 0.8856, best val acc: 0.8927, best train loss: 0.3050  @ epoch 7375 )\n",
      "[Epoch: 7440] train loss: 0.3382, train acc: 0.8660, val loss: 0.3004, val acc: 0.8904  (best train acc: 0.8856, best val acc: 0.8927, best train loss: 0.3050  @ epoch 7375 )\n",
      "[Epoch: 7460] train loss: 0.3402, train acc: 0.8660, val loss: 0.3011, val acc: 0.8884  (best train acc: 0.8856, best val acc: 0.8927, best train loss: 0.3050  @ epoch 7375 )\n",
      "[Epoch: 7480] train loss: 0.3571, train acc: 0.8600, val loss: 0.3269, val acc: 0.8772  (best train acc: 0.8856, best val acc: 0.8927, best train loss: 0.3050  @ epoch 7375 )\n",
      "[Epoch: 7500] train loss: 0.3538, train acc: 0.8626, val loss: 0.2954, val acc: 0.8890  (best train acc: 0.8856, best val acc: 0.8927, best train loss: 0.3050  @ epoch 7375 )\n",
      "[Epoch: 7520] train loss: 0.3658, train acc: 0.8562, val loss: 0.3161, val acc: 0.8766  (best train acc: 0.8856, best val acc: 0.8931, best train loss: 0.3050  @ epoch 7375 )\n",
      "[Epoch: 7540] train loss: 0.3492, train acc: 0.8623, val loss: 0.3093, val acc: 0.8833  (best train acc: 0.8856, best val acc: 0.8931, best train loss: 0.3050  @ epoch 7375 )\n",
      "[Epoch: 7560] train loss: 0.3539, train acc: 0.8584, val loss: 0.3222, val acc: 0.8809  (best train acc: 0.8856, best val acc: 0.8958, best train loss: 0.3050  @ epoch 7375 )\n",
      "[Epoch: 7580] train loss: 0.3516, train acc: 0.8580, val loss: 0.3123, val acc: 0.8782  (best train acc: 0.8856, best val acc: 0.8958, best train loss: 0.3050  @ epoch 7375 )\n",
      "[Epoch: 7600] train loss: 0.3520, train acc: 0.8621, val loss: 0.3007, val acc: 0.8914  (best train acc: 0.8856, best val acc: 0.8958, best train loss: 0.3050  @ epoch 7375 )\n",
      "[Epoch: 7620] train loss: 0.3570, train acc: 0.8634, val loss: 0.2999, val acc: 0.8823  (best train acc: 0.8856, best val acc: 0.8958, best train loss: 0.3050  @ epoch 7375 )\n",
      "[Epoch: 7640] train loss: 0.3523, train acc: 0.8587, val loss: 0.3061, val acc: 0.8850  (best train acc: 0.8856, best val acc: 0.8958, best train loss: 0.3050  @ epoch 7375 )\n",
      "[Epoch: 7660] train loss: 0.3425, train acc: 0.8567, val loss: 0.3034, val acc: 0.8874  (best train acc: 0.8856, best val acc: 0.8958, best train loss: 0.3050  @ epoch 7375 )\n",
      "[Epoch: 7680] train loss: 0.3569, train acc: 0.8595, val loss: 0.2994, val acc: 0.8914  (best train acc: 0.8856, best val acc: 0.8958, best train loss: 0.3050  @ epoch 7375 )\n",
      "[Epoch: 7700] train loss: 0.3583, train acc: 0.8630, val loss: 0.2960, val acc: 0.8890  (best train acc: 0.8856, best val acc: 0.8958, best train loss: 0.3050  @ epoch 7375 )\n",
      "[Epoch: 7720] train loss: 0.3365, train acc: 0.8644, val loss: 0.3064, val acc: 0.8840  (best train acc: 0.8856, best val acc: 0.8958, best train loss: 0.3050  @ epoch 7375 )\n",
      "[Epoch: 7740] train loss: 0.3453, train acc: 0.8638, val loss: 0.3059, val acc: 0.8857  (best train acc: 0.8856, best val acc: 0.8958, best train loss: 0.3050  @ epoch 7375 )\n",
      "[Epoch: 7760] train loss: 0.3554, train acc: 0.8591, val loss: 0.2993, val acc: 0.8894  (best train acc: 0.8856, best val acc: 0.8958, best train loss: 0.3050  @ epoch 7375 )\n",
      "[Epoch: 7780] train loss: 0.3282, train acc: 0.8730, val loss: 0.3045, val acc: 0.8843  (best train acc: 0.8877, best val acc: 0.8958, best train loss: 0.3031  @ epoch 7764 )\n",
      "[Epoch: 7800] train loss: 0.3596, train acc: 0.8603, val loss: 0.2934, val acc: 0.8884  (best train acc: 0.8877, best val acc: 0.8958, best train loss: 0.3031  @ epoch 7764 )\n",
      "[Epoch: 7820] train loss: 0.3261, train acc: 0.8753, val loss: 0.3003, val acc: 0.8887  (best train acc: 0.8877, best val acc: 0.8958, best train loss: 0.3031  @ epoch 7764 )\n",
      "[Epoch: 7840] train loss: 0.3564, train acc: 0.8535, val loss: 0.3002, val acc: 0.8884  (best train acc: 0.8877, best val acc: 0.8958, best train loss: 0.3031  @ epoch 7764 )\n",
      "[Epoch: 7860] train loss: 0.3206, train acc: 0.8735, val loss: 0.3076, val acc: 0.8833  (best train acc: 0.8877, best val acc: 0.8958, best train loss: 0.3031  @ epoch 7764 )\n",
      "[Epoch: 7880] train loss: 0.3431, train acc: 0.8683, val loss: 0.2976, val acc: 0.8887  (best train acc: 0.8877, best val acc: 0.8958, best train loss: 0.3031  @ epoch 7764 )\n",
      "[Epoch: 7900] train loss: 0.3448, train acc: 0.8645, val loss: 0.2968, val acc: 0.8894  (best train acc: 0.8877, best val acc: 0.8958, best train loss: 0.3031  @ epoch 7764 )\n",
      "[Epoch: 7920] train loss: 0.3655, train acc: 0.8615, val loss: 0.3019, val acc: 0.8863  (best train acc: 0.8877, best val acc: 0.8958, best train loss: 0.3031  @ epoch 7764 )\n",
      "[Epoch: 7940] train loss: 0.3328, train acc: 0.8678, val loss: 0.3029, val acc: 0.8843  (best train acc: 0.8877, best val acc: 0.8958, best train loss: 0.3031  @ epoch 7764 )\n",
      "[Epoch: 7960] train loss: 0.3443, train acc: 0.8658, val loss: 0.2908, val acc: 0.8877  (best train acc: 0.8877, best val acc: 0.8958, best train loss: 0.3031  @ epoch 7764 )\n",
      "[Epoch: 7980] train loss: 0.3105, train acc: 0.8810, val loss: 0.2916, val acc: 0.8890  (best train acc: 0.8877, best val acc: 0.8958, best train loss: 0.3031  @ epoch 7764 )\n",
      "[Epoch: 8000] train loss: 0.3194, train acc: 0.8753, val loss: 0.3210, val acc: 0.8752  (best train acc: 0.8877, best val acc: 0.8958, best train loss: 0.3031  @ epoch 7764 )\n",
      "[Epoch: 8020] train loss: 0.3207, train acc: 0.8767, val loss: 0.2965, val acc: 0.8874  (best train acc: 0.8877, best val acc: 0.8958, best train loss: 0.3031  @ epoch 7764 )\n",
      "[Epoch: 8040] train loss: 0.3116, train acc: 0.8836, val loss: 0.2986, val acc: 0.8847  (best train acc: 0.8877, best val acc: 0.8958, best train loss: 0.3031  @ epoch 7764 )\n",
      "[Epoch: 8060] train loss: 0.3286, train acc: 0.8707, val loss: 0.3001, val acc: 0.8853  (best train acc: 0.8877, best val acc: 0.8958, best train loss: 0.3031  @ epoch 7764 )\n",
      "[Epoch: 8080] train loss: 0.3343, train acc: 0.8689, val loss: 0.3031, val acc: 0.8843  (best train acc: 0.8877, best val acc: 0.8958, best train loss: 0.3031  @ epoch 7764 )\n",
      "[Epoch: 8100] train loss: 0.3797, train acc: 0.8492, val loss: 0.3177, val acc: 0.8772  (best train acc: 0.8877, best val acc: 0.8958, best train loss: 0.3031  @ epoch 7764 )\n",
      "[Epoch: 8120] train loss: 0.3299, train acc: 0.8760, val loss: 0.3057, val acc: 0.8843  (best train acc: 0.8877, best val acc: 0.8958, best train loss: 0.3031  @ epoch 7764 )\n",
      "[Epoch: 8140] train loss: 0.3399, train acc: 0.8661, val loss: 0.2973, val acc: 0.8870  (best train acc: 0.8877, best val acc: 0.8958, best train loss: 0.3031  @ epoch 7764 )\n",
      "[Epoch: 8160] train loss: 0.3498, train acc: 0.8569, val loss: 0.3020, val acc: 0.8857  (best train acc: 0.8877, best val acc: 0.8958, best train loss: 0.3031  @ epoch 7764 )\n",
      "[Epoch: 8180] train loss: 0.3257, train acc: 0.8741, val loss: 0.3170, val acc: 0.8735  (best train acc: 0.8877, best val acc: 0.8958, best train loss: 0.3031  @ epoch 7764 )\n",
      "[Epoch: 8200] train loss: 0.3117, train acc: 0.8821, val loss: 0.2864, val acc: 0.8907  (best train acc: 0.8877, best val acc: 0.8958, best train loss: 0.3031  @ epoch 7764 )\n",
      "[Epoch: 8220] train loss: 0.3431, train acc: 0.8647, val loss: 0.2992, val acc: 0.8863  (best train acc: 0.8877, best val acc: 0.8958, best train loss: 0.3031  @ epoch 7764 )\n",
      "[Epoch: 8240] train loss: 0.3212, train acc: 0.8743, val loss: 0.2908, val acc: 0.8904  (best train acc: 0.8877, best val acc: 0.8958, best train loss: 0.3031  @ epoch 7764 )\n",
      "[Epoch: 8260] train loss: 0.3136, train acc: 0.8802, val loss: 0.2861, val acc: 0.8901  (best train acc: 0.8877, best val acc: 0.8958, best train loss: 0.3031  @ epoch 7764 )\n",
      "[Epoch: 8280] train loss: 0.3102, train acc: 0.8846, val loss: 0.2939, val acc: 0.8927  (best train acc: 0.8910, best val acc: 0.8958, best train loss: 0.3004  @ epoch 8268 )\n",
      "[Epoch: 8300] train loss: 0.3107, train acc: 0.8789, val loss: 0.2881, val acc: 0.8914  (best train acc: 0.8910, best val acc: 0.8958, best train loss: 0.3004  @ epoch 8268 )\n",
      "[Epoch: 8320] train loss: 0.3190, train acc: 0.8828, val loss: 0.2884, val acc: 0.8870  (best train acc: 0.8910, best val acc: 0.8958, best train loss: 0.3004  @ epoch 8268 )\n",
      "[Epoch: 8340] train loss: 0.3234, train acc: 0.8798, val loss: 0.2900, val acc: 0.8924  (best train acc: 0.8910, best val acc: 0.8958, best train loss: 0.3004  @ epoch 8268 )\n",
      "[Epoch: 8360] train loss: 0.3167, train acc: 0.8759, val loss: 0.2908, val acc: 0.8917  (best train acc: 0.8910, best val acc: 0.8958, best train loss: 0.2968  @ epoch 8355 )\n",
      "[Epoch: 8380] train loss: 0.3386, train acc: 0.8648, val loss: 0.2895, val acc: 0.8958  (best train acc: 0.8910, best val acc: 0.8958, best train loss: 0.2968  @ epoch 8355 )\n",
      "[Epoch: 8400] train loss: 0.3149, train acc: 0.8780, val loss: 0.2934, val acc: 0.8884  (best train acc: 0.8910, best val acc: 0.8958, best train loss: 0.2968  @ epoch 8355 )\n",
      "[Epoch: 8420] train loss: 0.2990, train acc: 0.8856, val loss: 0.2875, val acc: 0.8901  (best train acc: 0.8910, best val acc: 0.8958, best train loss: 0.2968  @ epoch 8355 )\n",
      "[Epoch: 8440] train loss: 0.3523, train acc: 0.8582, val loss: 0.2985, val acc: 0.8870  (best train acc: 0.8916, best val acc: 0.8958, best train loss: 0.2938  @ epoch 8433 )\n",
      "[Epoch: 8460] train loss: 0.3036, train acc: 0.8832, val loss: 0.2907, val acc: 0.8894  (best train acc: 0.8916, best val acc: 0.8958, best train loss: 0.2938  @ epoch 8433 )\n",
      "[Epoch: 8480] train loss: 0.3227, train acc: 0.8712, val loss: 0.2894, val acc: 0.8890  (best train acc: 0.8916, best val acc: 0.8958, best train loss: 0.2936  @ epoch 8461 )\n",
      "[Epoch: 8500] train loss: 0.3219, train acc: 0.8728, val loss: 0.2956, val acc: 0.8890  (best train acc: 0.8916, best val acc: 0.8958, best train loss: 0.2936  @ epoch 8461 )\n",
      "[Epoch: 8520] train loss: 0.3317, train acc: 0.8661, val loss: 0.2966, val acc: 0.8901  (best train acc: 0.8916, best val acc: 0.8958, best train loss: 0.2936  @ epoch 8461 )\n",
      "[Epoch: 8540] train loss: 0.3144, train acc: 0.8738, val loss: 0.2987, val acc: 0.8894  (best train acc: 0.8916, best val acc: 0.8958, best train loss: 0.2936  @ epoch 8461 )\n",
      "[Epoch: 8560] train loss: 0.3035, train acc: 0.8842, val loss: 0.2998, val acc: 0.8874  (best train acc: 0.8916, best val acc: 0.8958, best train loss: 0.2932  @ epoch 8547 )\n",
      "[Epoch: 8580] train loss: 0.3130, train acc: 0.8806, val loss: 0.2909, val acc: 0.8907  (best train acc: 0.8916, best val acc: 0.8958, best train loss: 0.2932  @ epoch 8547 )\n",
      "[Epoch: 8600] train loss: 0.3180, train acc: 0.8776, val loss: 0.2904, val acc: 0.8924  (best train acc: 0.8916, best val acc: 0.8958, best train loss: 0.2932  @ epoch 8547 )\n",
      "[Epoch: 8620] train loss: 0.3089, train acc: 0.8826, val loss: 0.2882, val acc: 0.8938  (best train acc: 0.8916, best val acc: 0.8958, best train loss: 0.2932  @ epoch 8547 )\n",
      "[Epoch: 8640] train loss: 0.3031, train acc: 0.8845, val loss: 0.2947, val acc: 0.8911  (best train acc: 0.8916, best val acc: 0.8958, best train loss: 0.2932  @ epoch 8547 )\n",
      "[Epoch: 8660] train loss: 0.3211, train acc: 0.8772, val loss: 0.2895, val acc: 0.8890  (best train acc: 0.8916, best val acc: 0.8958, best train loss: 0.2932  @ epoch 8547 )\n",
      "[Epoch: 8680] train loss: 0.3222, train acc: 0.8728, val loss: 0.2882, val acc: 0.8914  (best train acc: 0.8916, best val acc: 0.8958, best train loss: 0.2932  @ epoch 8547 )\n",
      "[Epoch: 8700] train loss: 0.3178, train acc: 0.8770, val loss: 0.2852, val acc: 0.8907  (best train acc: 0.8916, best val acc: 0.8958, best train loss: 0.2932  @ epoch 8547 )\n",
      "[Epoch: 8720] train loss: 0.3149, train acc: 0.8745, val loss: 0.2947, val acc: 0.8894  (best train acc: 0.8916, best val acc: 0.8958, best train loss: 0.2932  @ epoch 8547 )\n",
      "[Epoch: 8740] train loss: 0.3098, train acc: 0.8801, val loss: 0.2901, val acc: 0.8924  (best train acc: 0.8916, best val acc: 0.8958, best train loss: 0.2932  @ epoch 8547 )\n",
      "[Epoch: 8760] train loss: 0.3098, train acc: 0.8819, val loss: 0.3003, val acc: 0.8880  (best train acc: 0.8916, best val acc: 0.8958, best train loss: 0.2858  @ epoch 8747 )\n",
      "[Epoch: 8780] train loss: 0.3065, train acc: 0.8798, val loss: 0.2941, val acc: 0.8890  (best train acc: 0.8916, best val acc: 0.8958, best train loss: 0.2858  @ epoch 8747 )\n",
      "[Epoch: 8800] train loss: 0.3047, train acc: 0.8806, val loss: 0.2852, val acc: 0.8934  (best train acc: 0.8916, best val acc: 0.8961, best train loss: 0.2858  @ epoch 8747 )\n",
      "[Epoch: 8820] train loss: 0.3142, train acc: 0.8795, val loss: 0.2887, val acc: 0.8931  (best train acc: 0.8916, best val acc: 0.8961, best train loss: 0.2858  @ epoch 8747 )\n",
      "[Epoch: 8840] train loss: 0.3553, train acc: 0.8579, val loss: 0.2933, val acc: 0.8914  (best train acc: 0.8916, best val acc: 0.8961, best train loss: 0.2858  @ epoch 8747 )\n",
      "[Epoch: 8860] train loss: 0.3166, train acc: 0.8776, val loss: 0.3038, val acc: 0.8853  (best train acc: 0.8916, best val acc: 0.8961, best train loss: 0.2858  @ epoch 8747 )\n",
      "[Epoch: 8880] train loss: 0.3325, train acc: 0.8728, val loss: 0.2908, val acc: 0.8887  (best train acc: 0.8916, best val acc: 0.8961, best train loss: 0.2858  @ epoch 8747 )\n",
      "[Epoch: 8900] train loss: 0.3142, train acc: 0.8744, val loss: 0.2813, val acc: 0.8954  (best train acc: 0.8916, best val acc: 0.8961, best train loss: 0.2858  @ epoch 8747 )\n",
      "[Epoch: 8920] train loss: 0.2952, train acc: 0.8867, val loss: 0.2888, val acc: 0.8897  (best train acc: 0.8916, best val acc: 0.8961, best train loss: 0.2858  @ epoch 8747 )\n",
      "[Epoch: 8940] train loss: 0.3251, train acc: 0.8674, val loss: 0.3009, val acc: 0.8840  (best train acc: 0.8916, best val acc: 0.8981, best train loss: 0.2858  @ epoch 8747 )\n",
      "[Epoch: 8960] train loss: 0.3140, train acc: 0.8752, val loss: 0.3162, val acc: 0.8772  (best train acc: 0.8916, best val acc: 0.8981, best train loss: 0.2858  @ epoch 8747 )\n",
      "[Epoch: 8980] train loss: 0.2975, train acc: 0.8823, val loss: 0.3072, val acc: 0.8809  (best train acc: 0.8916, best val acc: 0.8981, best train loss: 0.2858  @ epoch 8747 )\n",
      "[Epoch: 9000] train loss: 0.2977, train acc: 0.8889, val loss: 0.2824, val acc: 0.8965  (best train acc: 0.8920, best val acc: 0.8981, best train loss: 0.2858  @ epoch 8747 )\n",
      "[Epoch: 9020] train loss: 0.3565, train acc: 0.8558, val loss: 0.3035, val acc: 0.8887  (best train acc: 0.8920, best val acc: 0.8981, best train loss: 0.2858  @ epoch 8747 )\n",
      "[Epoch: 9040] train loss: 0.3106, train acc: 0.8823, val loss: 0.2888, val acc: 0.8897  (best train acc: 0.8920, best val acc: 0.8981, best train loss: 0.2858  @ epoch 8747 )\n",
      "[Epoch: 9060] train loss: 0.3326, train acc: 0.8674, val loss: 0.2960, val acc: 0.8890  (best train acc: 0.8920, best val acc: 0.8981, best train loss: 0.2858  @ epoch 8747 )\n",
      "[Epoch: 9080] train loss: 0.3338, train acc: 0.8610, val loss: 0.3137, val acc: 0.8830  (best train acc: 0.8920, best val acc: 0.8981, best train loss: 0.2858  @ epoch 8747 )\n",
      "[Epoch: 9100] train loss: 0.3330, train acc: 0.8698, val loss: 0.3112, val acc: 0.8809  (best train acc: 0.8920, best val acc: 0.8981, best train loss: 0.2858  @ epoch 8747 )\n",
      "[Epoch: 9120] train loss: 0.3178, train acc: 0.8743, val loss: 0.3034, val acc: 0.8823  (best train acc: 0.8920, best val acc: 0.8981, best train loss: 0.2858  @ epoch 8747 )\n",
      "[Epoch: 9140] train loss: 0.3117, train acc: 0.8824, val loss: 0.2942, val acc: 0.8917  (best train acc: 0.8920, best val acc: 0.8981, best train loss: 0.2858  @ epoch 8747 )\n",
      "[Epoch: 9160] train loss: 0.3163, train acc: 0.8775, val loss: 0.2963, val acc: 0.8897  (best train acc: 0.8920, best val acc: 0.8981, best train loss: 0.2858  @ epoch 8747 )\n",
      "[Epoch: 9180] train loss: 0.3456, train acc: 0.8571, val loss: 0.2852, val acc: 0.8904  (best train acc: 0.8935, best val acc: 0.8981, best train loss: 0.2858  @ epoch 8747 )\n",
      "[Epoch: 9200] train loss: 0.3443, train acc: 0.8608, val loss: 0.2859, val acc: 0.8911  (best train acc: 0.8935, best val acc: 0.8981, best train loss: 0.2858  @ epoch 8747 )\n",
      "[Epoch: 9220] train loss: 0.3156, train acc: 0.8812, val loss: 0.3019, val acc: 0.8877  (best train acc: 0.8935, best val acc: 0.8981, best train loss: 0.2858  @ epoch 8747 )\n",
      "[Epoch: 9240] train loss: 0.3215, train acc: 0.8693, val loss: 0.2945, val acc: 0.8867  (best train acc: 0.8935, best val acc: 0.8981, best train loss: 0.2858  @ epoch 8747 )\n",
      "[Epoch: 9260] train loss: 0.3105, train acc: 0.8842, val loss: 0.2952, val acc: 0.8867  (best train acc: 0.8935, best val acc: 0.8985, best train loss: 0.2858  @ epoch 8747 )\n",
      "[Epoch: 9280] train loss: 0.3093, train acc: 0.8756, val loss: 0.2847, val acc: 0.8958  (best train acc: 0.8935, best val acc: 0.8985, best train loss: 0.2858  @ epoch 8747 )\n",
      "[Epoch: 9300] train loss: 0.3371, train acc: 0.8750, val loss: 0.3104, val acc: 0.8897  (best train acc: 0.8935, best val acc: 0.8985, best train loss: 0.2858  @ epoch 8747 )\n",
      "[Epoch: 9320] train loss: 0.3195, train acc: 0.8699, val loss: 0.3251, val acc: 0.8745  (best train acc: 0.8935, best val acc: 0.8985, best train loss: 0.2858  @ epoch 8747 )\n",
      "[Epoch: 9340] train loss: 0.3665, train acc: 0.8515, val loss: 0.3045, val acc: 0.8887  (best train acc: 0.8935, best val acc: 0.8985, best train loss: 0.2858  @ epoch 8747 )\n",
      "[Epoch: 9360] train loss: 0.3044, train acc: 0.8783, val loss: 0.2849, val acc: 0.8894  (best train acc: 0.8935, best val acc: 0.8985, best train loss: 0.2858  @ epoch 8747 )\n",
      "[Epoch: 9380] train loss: 0.3275, train acc: 0.8666, val loss: 0.2803, val acc: 0.8961  (best train acc: 0.8935, best val acc: 0.8985, best train loss: 0.2789  @ epoch 9371 )\n",
      "[Epoch: 9400] train loss: 0.3085, train acc: 0.8805, val loss: 0.3259, val acc: 0.8823  (best train acc: 0.8939, best val acc: 0.8985, best train loss: 0.2789  @ epoch 9371 )\n",
      "[Epoch: 9420] train loss: 0.2912, train acc: 0.8899, val loss: 0.2902, val acc: 0.8887  (best train acc: 0.8939, best val acc: 0.8985, best train loss: 0.2789  @ epoch 9371 )\n",
      "[Epoch: 9440] train loss: 0.3628, train acc: 0.8499, val loss: 0.2993, val acc: 0.8880  (best train acc: 0.8939, best val acc: 0.8985, best train loss: 0.2789  @ epoch 9371 )\n",
      "[Epoch: 9460] train loss: 0.3080, train acc: 0.8834, val loss: 0.2871, val acc: 0.8921  (best train acc: 0.8939, best val acc: 0.8985, best train loss: 0.2789  @ epoch 9371 )\n",
      "[Epoch: 9480] train loss: 0.3048, train acc: 0.8777, val loss: 0.2915, val acc: 0.8907  (best train acc: 0.8939, best val acc: 0.8985, best train loss: 0.2789  @ epoch 9371 )\n",
      "[Epoch: 9500] train loss: 0.3035, train acc: 0.8784, val loss: 0.2864, val acc: 0.8951  (best train acc: 0.8939, best val acc: 0.8992, best train loss: 0.2789  @ epoch 9371 )\n",
      "[Epoch: 9520] train loss: 0.2947, train acc: 0.8824, val loss: 0.2830, val acc: 0.8938  (best train acc: 0.8939, best val acc: 0.8995, best train loss: 0.2789  @ epoch 9371 )\n",
      "[Epoch: 9540] train loss: 0.3375, train acc: 0.8611, val loss: 0.3016, val acc: 0.8884  (best train acc: 0.8939, best val acc: 0.8995, best train loss: 0.2789  @ epoch 9371 )\n",
      "[Epoch: 9560] train loss: 0.3003, train acc: 0.8806, val loss: 0.2815, val acc: 0.8944  (best train acc: 0.8939, best val acc: 0.8995, best train loss: 0.2789  @ epoch 9371 )\n",
      "[Epoch: 9580] train loss: 0.2955, train acc: 0.8849, val loss: 0.2960, val acc: 0.8860  (best train acc: 0.8939, best val acc: 0.8995, best train loss: 0.2789  @ epoch 9371 )\n",
      "[Epoch: 9600] train loss: 0.3305, train acc: 0.8636, val loss: 0.2878, val acc: 0.8914  (best train acc: 0.8939, best val acc: 0.8995, best train loss: 0.2789  @ epoch 9371 )\n",
      "[Epoch: 9620] train loss: 0.3051, train acc: 0.8771, val loss: 0.2868, val acc: 0.8931  (best train acc: 0.8939, best val acc: 0.8995, best train loss: 0.2789  @ epoch 9371 )\n",
      "[Epoch: 9640] train loss: 0.3412, train acc: 0.8689, val loss: 0.3143, val acc: 0.8840  (best train acc: 0.8939, best val acc: 0.8995, best train loss: 0.2789  @ epoch 9371 )\n",
      "[Epoch: 9660] train loss: 0.3015, train acc: 0.8816, val loss: 0.2838, val acc: 0.8897  (best train acc: 0.8949, best val acc: 0.8998, best train loss: 0.2789  @ epoch 9371 )\n",
      "[Epoch: 9680] train loss: 0.3004, train acc: 0.8798, val loss: 0.2868, val acc: 0.8931  (best train acc: 0.8949, best val acc: 0.9019, best train loss: 0.2789  @ epoch 9371 )\n",
      "[Epoch: 9700] train loss: 0.2942, train acc: 0.8839, val loss: 0.2935, val acc: 0.8931  (best train acc: 0.8949, best val acc: 0.9019, best train loss: 0.2736  @ epoch 9692 )\n",
      "[Epoch: 9720] train loss: 0.3111, train acc: 0.8775, val loss: 0.2958, val acc: 0.8894  (best train acc: 0.8949, best val acc: 0.9019, best train loss: 0.2736  @ epoch 9692 )\n",
      "[Epoch: 9740] train loss: 0.2884, train acc: 0.8871, val loss: 0.2810, val acc: 0.8975  (best train acc: 0.8949, best val acc: 0.9019, best train loss: 0.2736  @ epoch 9692 )\n",
      "[Epoch: 9760] train loss: 0.2856, train acc: 0.8901, val loss: 0.2864, val acc: 0.8965  (best train acc: 0.8949, best val acc: 0.9019, best train loss: 0.2736  @ epoch 9692 )\n",
      "[Epoch: 9780] train loss: 0.2873, train acc: 0.8847, val loss: 0.2884, val acc: 0.8863  (best train acc: 0.8949, best val acc: 0.9019, best train loss: 0.2736  @ epoch 9692 )\n",
      "[Epoch: 9800] train loss: 0.2976, train acc: 0.8853, val loss: 0.2863, val acc: 0.8901  (best train acc: 0.8949, best val acc: 0.9019, best train loss: 0.2736  @ epoch 9692 )\n",
      "[Epoch: 9820] train loss: 0.3120, train acc: 0.8738, val loss: 0.2897, val acc: 0.8894  (best train acc: 0.8949, best val acc: 0.9019, best train loss: 0.2736  @ epoch 9692 )\n",
      "[Epoch: 9840] train loss: 0.3221, train acc: 0.8686, val loss: 0.3124, val acc: 0.8826  (best train acc: 0.8949, best val acc: 0.9019, best train loss: 0.2736  @ epoch 9692 )\n",
      "[Epoch: 9860] train loss: 0.3504, train acc: 0.8514, val loss: 0.3176, val acc: 0.8779  (best train acc: 0.8949, best val acc: 0.9019, best train loss: 0.2736  @ epoch 9692 )\n",
      "[Epoch: 9880] train loss: 0.3053, train acc: 0.8855, val loss: 0.2943, val acc: 0.8884  (best train acc: 0.8949, best val acc: 0.9019, best train loss: 0.2736  @ epoch 9692 )\n",
      "[Epoch: 9900] train loss: 0.2910, train acc: 0.8891, val loss: 0.2968, val acc: 0.8877  (best train acc: 0.8949, best val acc: 0.9019, best train loss: 0.2736  @ epoch 9692 )\n",
      "[Epoch: 9920] train loss: 0.3543, train acc: 0.8565, val loss: 0.2831, val acc: 0.8941  (best train acc: 0.8949, best val acc: 0.9019, best train loss: 0.2736  @ epoch 9692 )\n",
      "[Epoch: 9940] train loss: 0.2886, train acc: 0.8884, val loss: 0.2757, val acc: 0.8971  (best train acc: 0.8949, best val acc: 0.9019, best train loss: 0.2736  @ epoch 9692 )\n",
      "[Epoch: 9960] train loss: 0.3143, train acc: 0.8743, val loss: 0.2802, val acc: 0.8938  (best train acc: 0.8949, best val acc: 0.9019, best train loss: 0.2736  @ epoch 9692 )\n",
      "[Epoch: 9980] train loss: 0.2949, train acc: 0.8850, val loss: 0.2825, val acc: 0.8877  (best train acc: 0.8949, best val acc: 0.9019, best train loss: 0.2736  @ epoch 9692 )\n",
      "[Epoch: 10000] train loss: 0.2869, train acc: 0.8895, val loss: 0.2759, val acc: 0.8975  (best train acc: 0.8949, best val acc: 0.9019, best train loss: 0.2736  @ epoch 9692 )\n",
      "[Epoch: 10020] train loss: 0.2963, train acc: 0.8796, val loss: 0.3003, val acc: 0.8809  (best train acc: 0.8949, best val acc: 0.9019, best train loss: 0.2736  @ epoch 9692 )\n",
      "[Epoch: 10040] train loss: 0.2937, train acc: 0.8897, val loss: 0.2829, val acc: 0.8927  (best train acc: 0.8949, best val acc: 0.9019, best train loss: 0.2736  @ epoch 9692 )\n",
      "[Epoch: 10060] train loss: 0.3272, train acc: 0.8764, val loss: 0.2860, val acc: 0.8870  (best train acc: 0.8949, best val acc: 0.9019, best train loss: 0.2736  @ epoch 9692 )\n",
      "[Epoch: 10080] train loss: 0.3040, train acc: 0.8830, val loss: 0.2938, val acc: 0.8826  (best train acc: 0.8961, best val acc: 0.9019, best train loss: 0.2736  @ epoch 9692 )\n",
      "[Epoch: 10100] train loss: 0.3106, train acc: 0.8767, val loss: 0.2836, val acc: 0.8998  (best train acc: 0.8961, best val acc: 0.9019, best train loss: 0.2736  @ epoch 9692 )\n",
      "[Epoch: 10120] train loss: 0.3084, train acc: 0.8821, val loss: 0.2897, val acc: 0.8884  (best train acc: 0.8961, best val acc: 0.9019, best train loss: 0.2736  @ epoch 9692 )\n",
      "[Epoch: 10140] train loss: 0.2782, train acc: 0.8926, val loss: 0.2787, val acc: 0.8934  (best train acc: 0.8961, best val acc: 0.9019, best train loss: 0.2736  @ epoch 9692 )\n",
      "[Epoch: 10160] train loss: 0.2785, train acc: 0.8912, val loss: 0.2799, val acc: 0.8965  (best train acc: 0.8961, best val acc: 0.9019, best train loss: 0.2736  @ epoch 9692 )\n",
      "[Epoch: 10180] train loss: 0.2980, train acc: 0.8809, val loss: 0.2845, val acc: 0.8931  (best train acc: 0.8961, best val acc: 0.9019, best train loss: 0.2704  @ epoch 10166 )\n",
      "[Epoch: 10200] train loss: 0.2946, train acc: 0.8816, val loss: 0.2895, val acc: 0.8887  (best train acc: 0.8961, best val acc: 0.9019, best train loss: 0.2704  @ epoch 10166 )\n",
      "[Epoch: 10220] train loss: 0.3298, train acc: 0.8584, val loss: 0.2845, val acc: 0.8907  (best train acc: 0.8961, best val acc: 0.9019, best train loss: 0.2704  @ epoch 10166 )\n",
      "[Epoch: 10240] train loss: 0.2818, train acc: 0.8913, val loss: 0.2763, val acc: 0.8917  (best train acc: 0.8961, best val acc: 0.9019, best train loss: 0.2704  @ epoch 10166 )\n",
      "[Epoch: 10260] train loss: 0.3191, train acc: 0.8739, val loss: 0.2880, val acc: 0.8890  (best train acc: 0.8961, best val acc: 0.9019, best train loss: 0.2704  @ epoch 10166 )\n",
      "[Epoch: 10280] train loss: 0.3185, train acc: 0.8679, val loss: 0.3131, val acc: 0.8830  (best train acc: 0.8961, best val acc: 0.9019, best train loss: 0.2704  @ epoch 10166 )\n",
      "[Epoch: 10300] train loss: 0.2933, train acc: 0.8789, val loss: 0.2771, val acc: 0.8958  (best train acc: 0.8961, best val acc: 0.9019, best train loss: 0.2704  @ epoch 10166 )\n",
      "[Epoch: 10320] train loss: 0.3034, train acc: 0.8780, val loss: 0.2735, val acc: 0.8904  (best train acc: 0.8961, best val acc: 0.9019, best train loss: 0.2704  @ epoch 10166 )\n",
      "[Epoch: 10340] train loss: 0.2995, train acc: 0.8879, val loss: 0.2746, val acc: 0.8968  (best train acc: 0.8984, best val acc: 0.9019, best train loss: 0.2686  @ epoch 10332 )\n",
      "[Epoch: 10360] train loss: 0.2917, train acc: 0.8879, val loss: 0.2891, val acc: 0.8877  (best train acc: 0.8984, best val acc: 0.9019, best train loss: 0.2686  @ epoch 10332 )\n",
      "[Epoch: 10380] train loss: 0.3188, train acc: 0.8795, val loss: 0.2903, val acc: 0.8874  (best train acc: 0.8984, best val acc: 0.9019, best train loss: 0.2686  @ epoch 10332 )\n",
      "[Epoch: 10400] train loss: 0.2914, train acc: 0.8880, val loss: 0.2804, val acc: 0.8931  (best train acc: 0.8984, best val acc: 0.9019, best train loss: 0.2686  @ epoch 10332 )\n",
      "[Epoch: 10420] train loss: 0.2733, train acc: 0.8949, val loss: 0.2772, val acc: 0.8921  (best train acc: 0.8984, best val acc: 0.9019, best train loss: 0.2686  @ epoch 10332 )\n",
      "[Epoch: 10440] train loss: 0.3217, train acc: 0.8668, val loss: 0.2888, val acc: 0.8867  (best train acc: 0.8984, best val acc: 0.9019, best train loss: 0.2686  @ epoch 10332 )\n",
      "[Epoch: 10460] train loss: 0.2851, train acc: 0.8903, val loss: 0.2977, val acc: 0.8874  (best train acc: 0.8984, best val acc: 0.9019, best train loss: 0.2686  @ epoch 10332 )\n",
      "[Epoch: 10480] train loss: 0.3272, train acc: 0.8678, val loss: 0.2785, val acc: 0.8941  (best train acc: 0.8984, best val acc: 0.9019, best train loss: 0.2686  @ epoch 10332 )\n",
      "[Epoch: 10500] train loss: 0.3303, train acc: 0.8612, val loss: 0.3098, val acc: 0.8833  (best train acc: 0.8984, best val acc: 0.9019, best train loss: 0.2686  @ epoch 10332 )\n",
      "[Epoch: 10520] train loss: 0.2918, train acc: 0.8815, val loss: 0.2881, val acc: 0.8894  (best train acc: 0.8984, best val acc: 0.9019, best train loss: 0.2686  @ epoch 10332 )\n",
      "[Epoch: 10540] train loss: 0.2993, train acc: 0.8798, val loss: 0.2841, val acc: 0.8948  (best train acc: 0.8984, best val acc: 0.9019, best train loss: 0.2686  @ epoch 10332 )\n",
      "[Epoch: 10560] train loss: 0.2813, train acc: 0.8882, val loss: 0.2784, val acc: 0.8917  (best train acc: 0.8984, best val acc: 0.9019, best train loss: 0.2686  @ epoch 10332 )\n",
      "[Epoch: 10580] train loss: 0.3035, train acc: 0.8868, val loss: 0.2822, val acc: 0.8968  (best train acc: 0.8984, best val acc: 0.9022, best train loss: 0.2686  @ epoch 10332 )\n",
      "[Epoch: 10600] train loss: 0.2785, train acc: 0.8915, val loss: 0.2771, val acc: 0.8911  (best train acc: 0.8984, best val acc: 0.9022, best train loss: 0.2686  @ epoch 10332 )\n",
      "[Epoch: 10620] train loss: 0.2773, train acc: 0.8931, val loss: 0.2749, val acc: 0.8951  (best train acc: 0.8990, best val acc: 0.9022, best train loss: 0.2686  @ epoch 10332 )\n",
      "[Epoch: 10640] train loss: 0.2644, train acc: 0.8971, val loss: 0.2739, val acc: 0.8961  (best train acc: 0.8990, best val acc: 0.9022, best train loss: 0.2644  @ epoch 10640 )\n",
      "[Epoch: 10660] train loss: 0.2803, train acc: 0.8898, val loss: 0.2773, val acc: 0.8934  (best train acc: 0.8990, best val acc: 0.9022, best train loss: 0.2644  @ epoch 10640 )\n",
      "[Epoch: 10680] train loss: 0.3228, train acc: 0.8655, val loss: 0.2941, val acc: 0.8877  (best train acc: 0.8990, best val acc: 0.9022, best train loss: 0.2644  @ epoch 10640 )\n",
      "[Epoch: 10700] train loss: 0.3092, train acc: 0.8869, val loss: 0.2863, val acc: 0.8911  (best train acc: 0.8990, best val acc: 0.9022, best train loss: 0.2644  @ epoch 10640 )\n",
      "[Epoch: 10720] train loss: 0.2987, train acc: 0.8895, val loss: 0.2724, val acc: 0.8968  (best train acc: 0.8990, best val acc: 0.9022, best train loss: 0.2644  @ epoch 10640 )\n",
      "[Epoch: 10740] train loss: 0.2903, train acc: 0.8811, val loss: 0.2872, val acc: 0.8897  (best train acc: 0.8990, best val acc: 0.9022, best train loss: 0.2644  @ epoch 10640 )\n",
      "[Epoch: 10760] train loss: 0.2861, train acc: 0.8861, val loss: 0.2848, val acc: 0.8914  (best train acc: 0.8990, best val acc: 0.9022, best train loss: 0.2644  @ epoch 10640 )\n",
      "[Epoch: 10780] train loss: 0.3286, train acc: 0.8678, val loss: 0.2944, val acc: 0.8863  (best train acc: 0.8990, best val acc: 0.9022, best train loss: 0.2640  @ epoch 10777 )\n",
      "[Epoch: 10800] train loss: 0.2857, train acc: 0.8882, val loss: 0.2995, val acc: 0.8820  (best train acc: 0.8990, best val acc: 0.9022, best train loss: 0.2640  @ epoch 10777 )\n",
      "[Epoch: 10820] train loss: 0.2786, train acc: 0.8918, val loss: 0.2800, val acc: 0.8938  (best train acc: 0.8990, best val acc: 0.9022, best train loss: 0.2640  @ epoch 10777 )\n",
      "[Epoch: 10840] train loss: 0.2896, train acc: 0.8882, val loss: 0.3074, val acc: 0.8836  (best train acc: 0.8990, best val acc: 0.9022, best train loss: 0.2640  @ epoch 10777 )\n",
      "[Epoch: 10860] train loss: 0.2775, train acc: 0.8890, val loss: 0.2923, val acc: 0.8904  (best train acc: 0.8990, best val acc: 0.9022, best train loss: 0.2640  @ epoch 10777 )\n",
      "[Epoch: 10880] train loss: 0.2983, train acc: 0.8840, val loss: 0.2921, val acc: 0.8884  (best train acc: 0.8990, best val acc: 0.9022, best train loss: 0.2640  @ epoch 10777 )\n",
      "[Epoch: 10900] train loss: 0.2935, train acc: 0.8821, val loss: 0.2838, val acc: 0.8961  (best train acc: 0.8990, best val acc: 0.9022, best train loss: 0.2634  @ epoch 10888 )\n",
      "[Epoch: 10920] train loss: 0.2844, train acc: 0.8848, val loss: 0.2964, val acc: 0.8853  (best train acc: 0.8990, best val acc: 0.9022, best train loss: 0.2634  @ epoch 10888 )\n",
      "[Epoch: 10940] train loss: 0.2726, train acc: 0.8929, val loss: 0.2688, val acc: 0.8938  (best train acc: 0.8990, best val acc: 0.9022, best train loss: 0.2573  @ epoch 10931 )\n",
      "[Epoch: 10960] train loss: 0.2945, train acc: 0.8787, val loss: 0.2763, val acc: 0.8941  (best train acc: 0.8993, best val acc: 0.9022, best train loss: 0.2573  @ epoch 10931 )\n",
      "[Epoch: 10980] train loss: 0.2787, train acc: 0.8920, val loss: 0.2730, val acc: 0.8944  (best train acc: 0.8993, best val acc: 0.9022, best train loss: 0.2573  @ epoch 10931 )\n",
      "[Epoch: 11000] train loss: 0.2819, train acc: 0.8883, val loss: 0.2793, val acc: 0.8944  (best train acc: 0.8993, best val acc: 0.9022, best train loss: 0.2573  @ epoch 10931 )\n",
      "[Epoch: 11020] train loss: 0.2914, train acc: 0.8795, val loss: 0.2736, val acc: 0.8998  (best train acc: 0.9015, best val acc: 0.9022, best train loss: 0.2573  @ epoch 10931 )\n",
      "[Epoch: 11040] train loss: 0.2871, train acc: 0.8866, val loss: 0.2726, val acc: 0.8958  (best train acc: 0.9015, best val acc: 0.9022, best train loss: 0.2573  @ epoch 10931 )\n",
      "[Epoch: 11060] train loss: 0.2677, train acc: 0.8944, val loss: 0.2737, val acc: 0.8954  (best train acc: 0.9015, best val acc: 0.9022, best train loss: 0.2573  @ epoch 10931 )\n",
      "[Epoch: 11080] train loss: 0.2702, train acc: 0.8934, val loss: 0.2885, val acc: 0.8941  (best train acc: 0.9015, best val acc: 0.9022, best train loss: 0.2573  @ epoch 10931 )\n",
      "[Epoch: 11100] train loss: 0.2627, train acc: 0.8939, val loss: 0.2741, val acc: 0.8941  (best train acc: 0.9015, best val acc: 0.9022, best train loss: 0.2573  @ epoch 10931 )\n",
      "[Epoch: 11120] train loss: 0.2742, train acc: 0.8970, val loss: 0.2700, val acc: 0.8965  (best train acc: 0.9015, best val acc: 0.9022, best train loss: 0.2573  @ epoch 10931 )\n",
      "[Epoch: 11140] train loss: 0.2859, train acc: 0.8937, val loss: 0.2719, val acc: 0.8971  (best train acc: 0.9015, best val acc: 0.9022, best train loss: 0.2573  @ epoch 10931 )\n",
      "[Epoch: 11160] train loss: 0.2856, train acc: 0.8853, val loss: 0.2694, val acc: 0.8938  (best train acc: 0.9015, best val acc: 0.9022, best train loss: 0.2573  @ epoch 10931 )\n",
      "[Epoch: 11180] train loss: 0.2801, train acc: 0.8869, val loss: 0.2861, val acc: 0.8914  (best train acc: 0.9015, best val acc: 0.9022, best train loss: 0.2573  @ epoch 10931 )\n",
      "[Epoch: 11200] train loss: 0.2796, train acc: 0.8848, val loss: 0.3233, val acc: 0.8816  (best train acc: 0.9015, best val acc: 0.9022, best train loss: 0.2573  @ epoch 10931 )\n",
      "[Epoch: 11220] train loss: 0.2592, train acc: 0.8968, val loss: 0.2893, val acc: 0.8857  (best train acc: 0.9015, best val acc: 0.9022, best train loss: 0.2573  @ epoch 10931 )\n",
      "[Epoch: 11240] train loss: 0.3503, train acc: 0.8538, val loss: 0.2972, val acc: 0.8836  (best train acc: 0.9015, best val acc: 0.9022, best train loss: 0.2573  @ epoch 10931 )\n",
      "[Epoch: 11260] train loss: 0.2858, train acc: 0.8835, val loss: 0.2811, val acc: 0.8938  (best train acc: 0.9015, best val acc: 0.9022, best train loss: 0.2573  @ epoch 10931 )\n",
      "[Epoch: 11280] train loss: 0.2941, train acc: 0.8903, val loss: 0.2771, val acc: 0.8961  (best train acc: 0.9015, best val acc: 0.9022, best train loss: 0.2573  @ epoch 10931 )\n",
      "[Epoch: 11300] train loss: 0.2844, train acc: 0.8919, val loss: 0.2825, val acc: 0.8917  (best train acc: 0.9015, best val acc: 0.9022, best train loss: 0.2573  @ epoch 10931 )\n",
      "[Epoch: 11320] train loss: 0.2982, train acc: 0.8911, val loss: 0.2675, val acc: 0.8988  (best train acc: 0.9015, best val acc: 0.9022, best train loss: 0.2561  @ epoch 11309 )\n",
      "[Epoch: 11340] train loss: 0.2908, train acc: 0.8931, val loss: 0.2736, val acc: 0.8954  (best train acc: 0.9015, best val acc: 0.9022, best train loss: 0.2561  @ epoch 11309 )\n",
      "[Epoch: 11360] train loss: 0.2662, train acc: 0.8960, val loss: 0.2812, val acc: 0.8934  (best train acc: 0.9015, best val acc: 0.9022, best train loss: 0.2561  @ epoch 11309 )\n",
      "[Epoch: 11380] train loss: 0.3337, train acc: 0.8748, val loss: 0.2828, val acc: 0.8944  (best train acc: 0.9015, best val acc: 0.9022, best train loss: 0.2561  @ epoch 11309 )\n",
      "[Epoch: 11400] train loss: 0.3103, train acc: 0.8755, val loss: 0.2857, val acc: 0.8975  (best train acc: 0.9015, best val acc: 0.9022, best train loss: 0.2561  @ epoch 11309 )\n",
      "[Epoch: 11420] train loss: 0.2828, train acc: 0.8926, val loss: 0.2830, val acc: 0.8944  (best train acc: 0.9015, best val acc: 0.9022, best train loss: 0.2561  @ epoch 11309 )\n",
      "[Epoch: 11440] train loss: 0.2971, train acc: 0.8798, val loss: 0.2805, val acc: 0.8981  (best train acc: 0.9015, best val acc: 0.9022, best train loss: 0.2561  @ epoch 11309 )\n",
      "[Epoch: 11460] train loss: 0.2622, train acc: 0.8982, val loss: 0.2666, val acc: 0.8995  (best train acc: 0.9015, best val acc: 0.9022, best train loss: 0.2561  @ epoch 11309 )\n",
      "[Epoch: 11480] train loss: 0.2831, train acc: 0.8825, val loss: 0.2785, val acc: 0.8924  (best train acc: 0.9015, best val acc: 0.9022, best train loss: 0.2561  @ epoch 11309 )\n",
      "[Epoch: 11500] train loss: 0.3324, train acc: 0.8681, val loss: 0.2834, val acc: 0.8965  (best train acc: 0.9015, best val acc: 0.9022, best train loss: 0.2561  @ epoch 11309 )\n",
      "[Epoch: 11520] train loss: 0.2820, train acc: 0.8901, val loss: 0.2861, val acc: 0.8948  (best train acc: 0.9015, best val acc: 0.9022, best train loss: 0.2561  @ epoch 11309 )\n",
      "[Epoch: 11540] train loss: 0.2754, train acc: 0.8922, val loss: 0.2720, val acc: 0.8988  (best train acc: 0.9015, best val acc: 0.9022, best train loss: 0.2561  @ epoch 11309 )\n",
      "[Epoch: 11560] train loss: 0.2733, train acc: 0.8945, val loss: 0.2844, val acc: 0.8975  (best train acc: 0.9016, best val acc: 0.9022, best train loss: 0.2520  @ epoch 11541 )\n",
      "[Epoch: 11580] train loss: 0.2873, train acc: 0.8874, val loss: 0.2831, val acc: 0.8890  (best train acc: 0.9016, best val acc: 0.9022, best train loss: 0.2520  @ epoch 11541 )\n",
      "[Epoch: 11600] train loss: 0.2893, train acc: 0.8973, val loss: 0.2648, val acc: 0.8921  (best train acc: 0.9016, best val acc: 0.9022, best train loss: 0.2520  @ epoch 11541 )\n",
      "[Epoch: 11620] train loss: 0.2659, train acc: 0.8950, val loss: 0.2691, val acc: 0.8941  (best train acc: 0.9016, best val acc: 0.9022, best train loss: 0.2520  @ epoch 11541 )\n",
      "[Epoch: 11640] train loss: 0.2674, train acc: 0.8949, val loss: 0.2694, val acc: 0.9008  (best train acc: 0.9016, best val acc: 0.9022, best train loss: 0.2520  @ epoch 11541 )\n",
      "[Epoch: 11660] train loss: 0.2787, train acc: 0.8908, val loss: 0.2746, val acc: 0.8948  (best train acc: 0.9016, best val acc: 0.9022, best train loss: 0.2520  @ epoch 11541 )\n",
      "[Epoch: 11680] train loss: 0.3000, train acc: 0.8798, val loss: 0.2834, val acc: 0.8897  (best train acc: 0.9016, best val acc: 0.9022, best train loss: 0.2520  @ epoch 11541 )\n",
      "[Epoch: 11700] train loss: 0.2705, train acc: 0.8979, val loss: 0.2695, val acc: 0.8948  (best train acc: 0.9020, best val acc: 0.9022, best train loss: 0.2520  @ epoch 11541 )\n",
      "[Epoch: 11720] train loss: 0.2757, train acc: 0.8909, val loss: 0.2707, val acc: 0.8965  (best train acc: 0.9020, best val acc: 0.9022, best train loss: 0.2520  @ epoch 11541 )\n",
      "[Epoch: 11740] train loss: 0.2681, train acc: 0.8986, val loss: 0.2653, val acc: 0.8965  (best train acc: 0.9020, best val acc: 0.9022, best train loss: 0.2520  @ epoch 11541 )\n",
      "[Epoch: 11760] train loss: 0.2871, train acc: 0.8796, val loss: 0.2829, val acc: 0.8941  (best train acc: 0.9020, best val acc: 0.9022, best train loss: 0.2520  @ epoch 11541 )\n",
      "[Epoch: 11780] train loss: 0.2665, train acc: 0.8895, val loss: 0.2702, val acc: 0.8927  (best train acc: 0.9020, best val acc: 0.9022, best train loss: 0.2494  @ epoch 11774 )\n",
      "[Epoch: 11800] train loss: 0.2800, train acc: 0.8882, val loss: 0.2687, val acc: 0.8965  (best train acc: 0.9054, best val acc: 0.9022, best train loss: 0.2489  @ epoch 11798 )\n",
      "[Epoch: 11820] train loss: 0.2695, train acc: 0.8931, val loss: 0.2700, val acc: 0.8998  (best train acc: 0.9054, best val acc: 0.9022, best train loss: 0.2486  @ epoch 11815 )\n",
      "[Epoch: 11840] train loss: 0.2865, train acc: 0.8811, val loss: 0.2740, val acc: 0.8988  (best train acc: 0.9054, best val acc: 0.9022, best train loss: 0.2486  @ epoch 11815 )\n",
      "[Epoch: 11860] train loss: 0.2535, train acc: 0.9007, val loss: 0.2660, val acc: 0.8968  (best train acc: 0.9054, best val acc: 0.9022, best train loss: 0.2486  @ epoch 11815 )\n",
      "[Epoch: 11880] train loss: 0.2680, train acc: 0.8939, val loss: 0.2805, val acc: 0.8931  (best train acc: 0.9054, best val acc: 0.9022, best train loss: 0.2486  @ epoch 11815 )\n",
      "[Epoch: 11900] train loss: 0.2731, train acc: 0.8915, val loss: 0.2689, val acc: 0.8965  (best train acc: 0.9054, best val acc: 0.9022, best train loss: 0.2486  @ epoch 11815 )\n",
      "[Epoch: 11920] train loss: 0.2775, train acc: 0.8902, val loss: 0.2678, val acc: 0.8975  (best train acc: 0.9054, best val acc: 0.9022, best train loss: 0.2486  @ epoch 11815 )\n",
      "[Epoch: 11940] train loss: 0.2721, train acc: 0.8939, val loss: 0.2723, val acc: 0.8887  (best train acc: 0.9054, best val acc: 0.9022, best train loss: 0.2486  @ epoch 11815 )\n",
      "[Epoch: 11960] train loss: 0.3094, train acc: 0.8761, val loss: 0.2957, val acc: 0.8857  (best train acc: 0.9054, best val acc: 0.9022, best train loss: 0.2486  @ epoch 11815 )\n",
      "[Epoch: 11980] train loss: 0.2921, train acc: 0.8807, val loss: 0.2782, val acc: 0.8985  (best train acc: 0.9054, best val acc: 0.9022, best train loss: 0.2486  @ epoch 11815 )\n",
      "[Epoch: 12000] train loss: 0.2702, train acc: 0.8937, val loss: 0.2758, val acc: 0.8927  (best train acc: 0.9054, best val acc: 0.9022, best train loss: 0.2486  @ epoch 11815 )\n",
      "[Epoch: 12020] train loss: 0.2676, train acc: 0.8922, val loss: 0.2721, val acc: 0.8941  (best train acc: 0.9054, best val acc: 0.9022, best train loss: 0.2486  @ epoch 11815 )\n",
      "[Epoch: 12040] train loss: 0.2739, train acc: 0.8925, val loss: 0.2812, val acc: 0.8941  (best train acc: 0.9054, best val acc: 0.9022, best train loss: 0.2486  @ epoch 11815 )\n",
      "[Epoch: 12060] train loss: 0.2594, train acc: 0.8938, val loss: 0.2637, val acc: 0.8995  (best train acc: 0.9054, best val acc: 0.9022, best train loss: 0.2486  @ epoch 11815 )\n",
      "[Epoch: 12080] train loss: 0.2676, train acc: 0.8949, val loss: 0.2834, val acc: 0.8975  (best train acc: 0.9054, best val acc: 0.9022, best train loss: 0.2486  @ epoch 11815 )\n",
      "[Epoch: 12100] train loss: 0.2756, train acc: 0.8905, val loss: 0.2786, val acc: 0.8914  (best train acc: 0.9054, best val acc: 0.9022, best train loss: 0.2486  @ epoch 11815 )\n",
      "[Epoch: 12120] train loss: 0.2626, train acc: 0.8992, val loss: 0.2670, val acc: 0.8971  (best train acc: 0.9054, best val acc: 0.9022, best train loss: 0.2486  @ epoch 11815 )\n",
      "[Epoch: 12140] train loss: 0.2709, train acc: 0.8929, val loss: 0.2796, val acc: 0.8927  (best train acc: 0.9054, best val acc: 0.9029, best train loss: 0.2486  @ epoch 11815 )\n",
      "[Epoch: 12160] train loss: 0.2722, train acc: 0.8891, val loss: 0.2728, val acc: 0.8954  (best train acc: 0.9054, best val acc: 0.9029, best train loss: 0.2486  @ epoch 11815 )\n",
      "[Epoch: 12180] train loss: 0.2632, train acc: 0.8926, val loss: 0.2713, val acc: 0.8934  (best train acc: 0.9054, best val acc: 0.9029, best train loss: 0.2486  @ epoch 11815 )\n",
      "[Epoch: 12200] train loss: 0.2662, train acc: 0.8957, val loss: 0.2667, val acc: 0.8954  (best train acc: 0.9054, best val acc: 0.9029, best train loss: 0.2486  @ epoch 11815 )\n",
      "[Epoch: 12220] train loss: 0.2689, train acc: 0.8952, val loss: 0.2982, val acc: 0.8874  (best train acc: 0.9054, best val acc: 0.9029, best train loss: 0.2486  @ epoch 11815 )\n",
      "[Epoch: 12240] train loss: 0.2753, train acc: 0.8901, val loss: 0.2744, val acc: 0.8961  (best train acc: 0.9054, best val acc: 0.9029, best train loss: 0.2486  @ epoch 11815 )\n",
      "[Epoch: 12260] train loss: 0.2787, train acc: 0.8904, val loss: 0.2671, val acc: 0.8992  (best train acc: 0.9054, best val acc: 0.9029, best train loss: 0.2486  @ epoch 11815 )\n",
      "[Epoch: 12280] train loss: 0.2570, train acc: 0.8982, val loss: 0.2673, val acc: 0.8995  (best train acc: 0.9054, best val acc: 0.9029, best train loss: 0.2486  @ epoch 11815 )\n",
      "[Epoch: 12300] train loss: 0.2647, train acc: 0.8984, val loss: 0.2659, val acc: 0.8954  (best train acc: 0.9054, best val acc: 0.9029, best train loss: 0.2413  @ epoch 12293 )\n",
      "[Epoch: 12320] train loss: 0.2753, train acc: 0.8857, val loss: 0.2814, val acc: 0.8877  (best train acc: 0.9054, best val acc: 0.9029, best train loss: 0.2413  @ epoch 12293 )\n",
      "[Epoch: 12340] train loss: 0.2644, train acc: 0.8943, val loss: 0.2698, val acc: 0.8992  (best train acc: 0.9054, best val acc: 0.9029, best train loss: 0.2413  @ epoch 12293 )\n",
      "[Epoch: 12360] train loss: 0.2762, train acc: 0.8937, val loss: 0.2670, val acc: 0.8965  (best train acc: 0.9054, best val acc: 0.9029, best train loss: 0.2413  @ epoch 12293 )\n",
      "[Epoch: 12380] train loss: 0.2592, train acc: 0.8947, val loss: 0.2788, val acc: 0.8971  (best train acc: 0.9054, best val acc: 0.9029, best train loss: 0.2413  @ epoch 12293 )\n",
      "[Epoch: 12400] train loss: 0.3080, train acc: 0.8812, val loss: 0.2621, val acc: 0.8985  (best train acc: 0.9054, best val acc: 0.9029, best train loss: 0.2413  @ epoch 12293 )\n",
      "[Epoch: 12420] train loss: 0.2819, train acc: 0.8873, val loss: 0.2697, val acc: 0.8988  (best train acc: 0.9054, best val acc: 0.9029, best train loss: 0.2413  @ epoch 12293 )\n",
      "[Epoch: 12440] train loss: 0.2757, train acc: 0.8913, val loss: 0.2703, val acc: 0.8944  (best train acc: 0.9054, best val acc: 0.9029, best train loss: 0.2413  @ epoch 12293 )\n",
      "[Epoch: 12460] train loss: 0.2780, train acc: 0.8866, val loss: 0.2747, val acc: 0.8924  (best train acc: 0.9054, best val acc: 0.9029, best train loss: 0.2413  @ epoch 12293 )\n",
      "[Epoch: 12480] train loss: 0.2600, train acc: 0.8944, val loss: 0.2593, val acc: 0.8971  (best train acc: 0.9054, best val acc: 0.9029, best train loss: 0.2413  @ epoch 12293 )\n",
      "[Epoch: 12500] train loss: 0.2869, train acc: 0.8813, val loss: 0.2614, val acc: 0.8971  (best train acc: 0.9054, best val acc: 0.9029, best train loss: 0.2413  @ epoch 12293 )\n",
      "[Epoch: 12520] train loss: 0.2621, train acc: 0.8977, val loss: 0.2705, val acc: 0.8961  (best train acc: 0.9054, best val acc: 0.9029, best train loss: 0.2413  @ epoch 12293 )\n",
      "[Epoch: 12540] train loss: 0.2735, train acc: 0.8944, val loss: 0.2658, val acc: 0.8975  (best train acc: 0.9054, best val acc: 0.9029, best train loss: 0.2413  @ epoch 12293 )\n",
      "[Epoch: 12560] train loss: 0.2835, train acc: 0.8902, val loss: 0.2699, val acc: 0.8961  (best train acc: 0.9054, best val acc: 0.9029, best train loss: 0.2413  @ epoch 12293 )\n",
      "[Epoch: 12580] train loss: 0.2787, train acc: 0.8937, val loss: 0.2724, val acc: 0.8965  (best train acc: 0.9054, best val acc: 0.9029, best train loss: 0.2413  @ epoch 12293 )\n",
      "[Epoch: 12600] train loss: 0.2638, train acc: 0.8956, val loss: 0.2625, val acc: 0.8975  (best train acc: 0.9054, best val acc: 0.9029, best train loss: 0.2413  @ epoch 12293 )\n",
      "[Epoch: 12620] train loss: 0.2751, train acc: 0.8875, val loss: 0.2647, val acc: 0.8968  (best train acc: 0.9054, best val acc: 0.9029, best train loss: 0.2413  @ epoch 12293 )\n",
      "[Epoch: 12640] train loss: 0.2879, train acc: 0.8765, val loss: 0.2896, val acc: 0.8860  (best train acc: 0.9054, best val acc: 0.9029, best train loss: 0.2413  @ epoch 12293 )\n",
      "[Epoch: 12660] train loss: 0.2762, train acc: 0.8859, val loss: 0.2748, val acc: 0.8965  (best train acc: 0.9054, best val acc: 0.9029, best train loss: 0.2413  @ epoch 12293 )\n",
      "[Epoch: 12680] train loss: 0.2972, train acc: 0.8791, val loss: 0.2794, val acc: 0.8941  (best train acc: 0.9054, best val acc: 0.9029, best train loss: 0.2413  @ epoch 12293 )\n",
      "[Epoch: 12700] train loss: 0.2875, train acc: 0.8892, val loss: 0.2832, val acc: 0.8874  (best train acc: 0.9054, best val acc: 0.9029, best train loss: 0.2413  @ epoch 12293 )\n",
      "[Epoch: 12720] train loss: 0.2916, train acc: 0.8817, val loss: 0.2967, val acc: 0.8850  (best train acc: 0.9054, best val acc: 0.9029, best train loss: 0.2413  @ epoch 12293 )\n",
      "[Epoch: 12740] train loss: 0.2790, train acc: 0.8800, val loss: 0.2743, val acc: 0.8921  (best train acc: 0.9054, best val acc: 0.9029, best train loss: 0.2413  @ epoch 12293 )\n",
      "[Epoch: 12760] train loss: 0.2740, train acc: 0.8824, val loss: 0.2598, val acc: 0.8961  (best train acc: 0.9054, best val acc: 0.9029, best train loss: 0.2413  @ epoch 12293 )\n",
      "[Epoch: 12780] train loss: 0.2629, train acc: 0.9028, val loss: 0.2816, val acc: 0.8934  (best train acc: 0.9054, best val acc: 0.9029, best train loss: 0.2413  @ epoch 12293 )\n",
      "[Epoch: 12800] train loss: 0.2711, train acc: 0.8919, val loss: 0.2575, val acc: 0.8981  (best train acc: 0.9054, best val acc: 0.9029, best train loss: 0.2413  @ epoch 12293 )\n",
      "[Epoch: 12820] train loss: 0.2787, train acc: 0.8887, val loss: 0.2890, val acc: 0.8904  (best train acc: 0.9054, best val acc: 0.9029, best train loss: 0.2413  @ epoch 12293 )\n",
      "[Epoch: 12840] train loss: 0.2678, train acc: 0.8933, val loss: 0.2692, val acc: 0.8988  (best train acc: 0.9054, best val acc: 0.9035, best train loss: 0.2413  @ epoch 12293 )\n",
      "[Epoch: 12860] train loss: 0.2601, train acc: 0.8977, val loss: 0.2589, val acc: 0.8988  (best train acc: 0.9054, best val acc: 0.9035, best train loss: 0.2413  @ epoch 12293 )\n",
      "[Epoch: 12880] train loss: 0.2654, train acc: 0.8918, val loss: 0.2694, val acc: 0.8954  (best train acc: 0.9054, best val acc: 0.9035, best train loss: 0.2413  @ epoch 12293 )\n",
      "[Epoch: 12900] train loss: 0.2543, train acc: 0.8999, val loss: 0.2618, val acc: 0.8971  (best train acc: 0.9056, best val acc: 0.9039, best train loss: 0.2413  @ epoch 12293 )\n",
      "[Epoch: 12920] train loss: 0.2619, train acc: 0.8958, val loss: 0.2733, val acc: 0.8941  (best train acc: 0.9056, best val acc: 0.9039, best train loss: 0.2413  @ epoch 12293 )\n",
      "[Epoch: 12940] train loss: 0.2598, train acc: 0.9026, val loss: 0.2740, val acc: 0.8941  (best train acc: 0.9056, best val acc: 0.9039, best train loss: 0.2413  @ epoch 12293 )\n",
      "[Epoch: 12960] train loss: 0.2815, train acc: 0.8944, val loss: 0.2649, val acc: 0.8978  (best train acc: 0.9056, best val acc: 0.9039, best train loss: 0.2413  @ epoch 12293 )\n",
      "[Epoch: 12980] train loss: 0.2531, train acc: 0.9049, val loss: 0.2593, val acc: 0.8965  (best train acc: 0.9056, best val acc: 0.9039, best train loss: 0.2413  @ epoch 12293 )\n",
      "[Epoch: 13000] train loss: 0.2480, train acc: 0.9032, val loss: 0.2638, val acc: 0.8965  (best train acc: 0.9056, best val acc: 0.9039, best train loss: 0.2413  @ epoch 12293 )\n",
      "[Epoch: 13020] train loss: 0.2918, train acc: 0.8817, val loss: 0.2781, val acc: 0.8901  (best train acc: 0.9056, best val acc: 0.9039, best train loss: 0.2413  @ epoch 12293 )\n",
      "[Epoch: 13040] train loss: 0.2679, train acc: 0.8982, val loss: 0.2643, val acc: 0.8965  (best train acc: 0.9056, best val acc: 0.9039, best train loss: 0.2413  @ epoch 12293 )\n",
      "[Epoch: 13060] train loss: 0.2921, train acc: 0.8878, val loss: 0.2742, val acc: 0.8965  (best train acc: 0.9056, best val acc: 0.9039, best train loss: 0.2413  @ epoch 12293 )\n",
      "[Epoch: 13080] train loss: 0.2704, train acc: 0.8938, val loss: 0.2699, val acc: 0.8981  (best train acc: 0.9056, best val acc: 0.9039, best train loss: 0.2413  @ epoch 12293 )\n",
      "[Epoch: 13100] train loss: 0.2752, train acc: 0.8877, val loss: 0.2716, val acc: 0.8968  (best train acc: 0.9056, best val acc: 0.9039, best train loss: 0.2413  @ epoch 12293 )\n",
      "[Epoch: 13120] train loss: 0.2647, train acc: 0.8931, val loss: 0.2598, val acc: 0.8934  (best train acc: 0.9059, best val acc: 0.9039, best train loss: 0.2413  @ epoch 12293 )\n",
      "[Epoch: 13140] train loss: 0.2600, train acc: 0.8999, val loss: 0.2618, val acc: 0.8938  (best train acc: 0.9059, best val acc: 0.9039, best train loss: 0.2413  @ epoch 12293 )\n",
      "[Epoch: 13160] train loss: 0.2689, train acc: 0.8938, val loss: 0.2606, val acc: 0.9019  (best train acc: 0.9083, best val acc: 0.9039, best train loss: 0.2413  @ epoch 12293 )\n",
      "[Epoch: 13180] train loss: 0.2684, train acc: 0.8903, val loss: 0.2596, val acc: 0.8958  (best train acc: 0.9083, best val acc: 0.9039, best train loss: 0.2413  @ epoch 12293 )\n",
      "[Epoch: 13200] train loss: 0.2562, train acc: 0.8994, val loss: 0.2646, val acc: 0.8968  (best train acc: 0.9083, best val acc: 0.9039, best train loss: 0.2413  @ epoch 12293 )\n",
      "[Epoch: 13220] train loss: 0.2575, train acc: 0.9004, val loss: 0.2660, val acc: 0.8995  (best train acc: 0.9098, best val acc: 0.9039, best train loss: 0.2410  @ epoch 13214 )\n",
      "[Epoch: 13240] train loss: 0.2647, train acc: 0.8910, val loss: 0.2637, val acc: 0.8981  (best train acc: 0.9098, best val acc: 0.9039, best train loss: 0.2410  @ epoch 13214 )\n",
      "[Epoch: 13260] train loss: 0.2555, train acc: 0.8972, val loss: 0.2659, val acc: 0.8971  (best train acc: 0.9098, best val acc: 0.9039, best train loss: 0.2410  @ epoch 13214 )\n",
      "[Epoch: 13280] train loss: 0.2568, train acc: 0.8952, val loss: 0.2747, val acc: 0.8931  (best train acc: 0.9098, best val acc: 0.9039, best train loss: 0.2410  @ epoch 13214 )\n",
      "[Epoch: 13300] train loss: 0.2621, train acc: 0.8937, val loss: 0.2566, val acc: 0.8998  (best train acc: 0.9098, best val acc: 0.9039, best train loss: 0.2410  @ epoch 13214 )\n",
      "[Epoch: 13320] train loss: 0.2719, train acc: 0.8877, val loss: 0.2787, val acc: 0.8917  (best train acc: 0.9098, best val acc: 0.9039, best train loss: 0.2410  @ epoch 13214 )\n",
      "[Epoch: 13340] train loss: 0.2556, train acc: 0.8993, val loss: 0.2598, val acc: 0.8961  (best train acc: 0.9098, best val acc: 0.9039, best train loss: 0.2410  @ epoch 13214 )\n",
      "[Epoch: 13360] train loss: 0.2731, train acc: 0.8945, val loss: 0.2885, val acc: 0.8961  (best train acc: 0.9098, best val acc: 0.9039, best train loss: 0.2410  @ epoch 13214 )\n",
      "[Epoch: 13380] train loss: 0.2389, train acc: 0.9044, val loss: 0.2717, val acc: 0.8978  (best train acc: 0.9098, best val acc: 0.9039, best train loss: 0.2389  @ epoch 13380 )\n",
      "[Epoch: 13400] train loss: 0.2604, train acc: 0.8957, val loss: 0.2713, val acc: 0.8971  (best train acc: 0.9098, best val acc: 0.9039, best train loss: 0.2389  @ epoch 13380 )\n",
      "[Epoch: 13420] train loss: 0.2555, train acc: 0.8998, val loss: 0.2722, val acc: 0.8975  (best train acc: 0.9098, best val acc: 0.9039, best train loss: 0.2389  @ epoch 13380 )\n",
      "[Epoch: 13440] train loss: 0.2456, train acc: 0.9012, val loss: 0.2731, val acc: 0.8961  (best train acc: 0.9098, best val acc: 0.9039, best train loss: 0.2389  @ epoch 13380 )\n",
      "[Epoch: 13460] train loss: 0.2516, train acc: 0.9007, val loss: 0.2686, val acc: 0.8965  (best train acc: 0.9098, best val acc: 0.9039, best train loss: 0.2332  @ epoch 13450 )\n",
      "[Epoch: 13480] train loss: 0.2709, train acc: 0.8915, val loss: 0.2584, val acc: 0.8951  (best train acc: 0.9098, best val acc: 0.9039, best train loss: 0.2332  @ epoch 13450 )\n",
      "[Epoch: 13500] train loss: 0.2731, train acc: 0.8947, val loss: 0.2616, val acc: 0.9019  (best train acc: 0.9098, best val acc: 0.9039, best train loss: 0.2332  @ epoch 13450 )\n",
      "[Epoch: 13520] train loss: 0.2462, train acc: 0.9036, val loss: 0.2675, val acc: 0.8971  (best train acc: 0.9098, best val acc: 0.9039, best train loss: 0.2332  @ epoch 13450 )\n",
      "[Epoch: 13540] train loss: 0.2504, train acc: 0.9024, val loss: 0.2610, val acc: 0.8978  (best train acc: 0.9098, best val acc: 0.9039, best train loss: 0.2332  @ epoch 13450 )\n",
      "[Epoch: 13560] train loss: 0.2568, train acc: 0.8943, val loss: 0.2634, val acc: 0.8975  (best train acc: 0.9098, best val acc: 0.9039, best train loss: 0.2332  @ epoch 13450 )\n",
      "[Epoch: 13580] train loss: 0.2539, train acc: 0.9012, val loss: 0.2662, val acc: 0.8931  (best train acc: 0.9098, best val acc: 0.9039, best train loss: 0.2332  @ epoch 13450 )\n",
      "[Epoch: 13600] train loss: 0.2627, train acc: 0.8980, val loss: 0.2590, val acc: 0.8978  (best train acc: 0.9098, best val acc: 0.9039, best train loss: 0.2332  @ epoch 13450 )\n",
      "[Epoch: 13620] train loss: 0.2713, train acc: 0.8938, val loss: 0.2937, val acc: 0.8877  (best train acc: 0.9098, best val acc: 0.9046, best train loss: 0.2332  @ epoch 13450 )\n",
      "[Epoch: 13640] train loss: 0.2543, train acc: 0.8973, val loss: 0.2696, val acc: 0.8901  (best train acc: 0.9098, best val acc: 0.9046, best train loss: 0.2332  @ epoch 13450 )\n",
      "[Epoch: 13660] train loss: 0.2679, train acc: 0.8914, val loss: 0.2938, val acc: 0.8904  (best train acc: 0.9098, best val acc: 0.9046, best train loss: 0.2332  @ epoch 13450 )\n",
      "[Epoch: 13680] train loss: 0.2576, train acc: 0.8964, val loss: 0.2813, val acc: 0.8931  (best train acc: 0.9098, best val acc: 0.9046, best train loss: 0.2332  @ epoch 13450 )\n",
      "[Epoch: 13700] train loss: 0.2460, train acc: 0.9027, val loss: 0.2557, val acc: 0.9008  (best train acc: 0.9098, best val acc: 0.9046, best train loss: 0.2332  @ epoch 13450 )\n",
      "[Epoch: 13720] train loss: 0.2438, train acc: 0.9033, val loss: 0.2565, val acc: 0.8985  (best train acc: 0.9098, best val acc: 0.9046, best train loss: 0.2332  @ epoch 13450 )\n",
      "[Epoch: 13740] train loss: 0.2840, train acc: 0.8848, val loss: 0.2721, val acc: 0.8941  (best train acc: 0.9098, best val acc: 0.9046, best train loss: 0.2332  @ epoch 13450 )\n",
      "[Epoch: 13760] train loss: 0.2791, train acc: 0.8870, val loss: 0.2678, val acc: 0.9008  (best train acc: 0.9098, best val acc: 0.9046, best train loss: 0.2332  @ epoch 13450 )\n",
      "[Epoch: 13780] train loss: 0.2762, train acc: 0.8857, val loss: 0.2638, val acc: 0.9015  (best train acc: 0.9098, best val acc: 0.9046, best train loss: 0.2332  @ epoch 13450 )\n",
      "[Epoch: 13800] train loss: 0.2597, train acc: 0.8989, val loss: 0.2576, val acc: 0.8971  (best train acc: 0.9098, best val acc: 0.9046, best train loss: 0.2332  @ epoch 13450 )\n",
      "[Epoch: 13820] train loss: 0.2540, train acc: 0.8971, val loss: 0.2589, val acc: 0.8954  (best train acc: 0.9098, best val acc: 0.9046, best train loss: 0.2332  @ epoch 13450 )\n",
      "[Epoch: 13840] train loss: 0.2589, train acc: 0.8956, val loss: 0.2673, val acc: 0.8978  (best train acc: 0.9098, best val acc: 0.9046, best train loss: 0.2332  @ epoch 13450 )\n",
      "[Epoch: 13860] train loss: 0.2569, train acc: 0.8962, val loss: 0.2784, val acc: 0.8944  (best train acc: 0.9098, best val acc: 0.9046, best train loss: 0.2332  @ epoch 13450 )\n",
      "[Epoch: 13880] train loss: 0.2537, train acc: 0.8989, val loss: 0.2635, val acc: 0.8978  (best train acc: 0.9098, best val acc: 0.9046, best train loss: 0.2332  @ epoch 13450 )\n",
      "[Epoch: 13900] train loss: 0.2704, train acc: 0.8937, val loss: 0.2844, val acc: 0.8965  (best train acc: 0.9098, best val acc: 0.9046, best train loss: 0.2332  @ epoch 13450 )\n",
      "[Epoch: 13920] train loss: 0.2458, train acc: 0.8977, val loss: 0.2691, val acc: 0.8927  (best train acc: 0.9098, best val acc: 0.9046, best train loss: 0.2332  @ epoch 13450 )\n",
      "[Epoch: 13940] train loss: 0.2579, train acc: 0.8990, val loss: 0.2680, val acc: 0.8992  (best train acc: 0.9098, best val acc: 0.9046, best train loss: 0.2332  @ epoch 13450 )\n",
      "[Epoch: 13960] train loss: 0.2518, train acc: 0.9007, val loss: 0.2589, val acc: 0.8961  (best train acc: 0.9098, best val acc: 0.9046, best train loss: 0.2332  @ epoch 13450 )\n",
      "[Epoch: 13980] train loss: 0.2609, train acc: 0.8944, val loss: 0.2588, val acc: 0.9005  (best train acc: 0.9098, best val acc: 0.9046, best train loss: 0.2332  @ epoch 13450 )\n",
      "[Epoch: 14000] train loss: 0.2577, train acc: 0.8957, val loss: 0.2618, val acc: 0.9002  (best train acc: 0.9098, best val acc: 0.9056, best train loss: 0.2332  @ epoch 13450 )\n",
      "[Epoch: 14020] train loss: 0.2704, train acc: 0.8964, val loss: 0.2553, val acc: 0.8961  (best train acc: 0.9098, best val acc: 0.9056, best train loss: 0.2332  @ epoch 13450 )\n",
      "[Epoch: 14040] train loss: 0.2474, train acc: 0.8989, val loss: 0.2540, val acc: 0.9002  (best train acc: 0.9098, best val acc: 0.9056, best train loss: 0.2325  @ epoch 14038 )\n",
      "[Epoch: 14060] train loss: 0.2582, train acc: 0.8929, val loss: 0.2581, val acc: 0.8998  (best train acc: 0.9098, best val acc: 0.9056, best train loss: 0.2281  @ epoch 14041 )\n",
      "[Epoch: 14080] train loss: 0.2459, train acc: 0.9045, val loss: 0.2937, val acc: 0.8907  (best train acc: 0.9098, best val acc: 0.9056, best train loss: 0.2281  @ epoch 14041 )\n",
      "[Epoch: 14100] train loss: 0.2429, train acc: 0.9012, val loss: 0.2630, val acc: 0.8938  (best train acc: 0.9098, best val acc: 0.9056, best train loss: 0.2281  @ epoch 14041 )\n",
      "[Epoch: 14120] train loss: 0.2609, train acc: 0.8950, val loss: 0.2628, val acc: 0.8978  (best train acc: 0.9098, best val acc: 0.9056, best train loss: 0.2281  @ epoch 14041 )\n",
      "[Epoch: 14140] train loss: 0.2695, train acc: 0.8903, val loss: 0.2648, val acc: 0.8985  (best train acc: 0.9098, best val acc: 0.9056, best train loss: 0.2281  @ epoch 14041 )\n",
      "[Epoch: 14160] train loss: 0.2442, train acc: 0.9039, val loss: 0.2822, val acc: 0.8975  (best train acc: 0.9098, best val acc: 0.9056, best train loss: 0.2281  @ epoch 14041 )\n",
      "[Epoch: 14180] train loss: 0.2692, train acc: 0.8881, val loss: 0.2738, val acc: 0.9015  (best train acc: 0.9098, best val acc: 0.9056, best train loss: 0.2281  @ epoch 14041 )\n",
      "[Epoch: 14200] train loss: 0.2616, train acc: 0.8996, val loss: 0.2640, val acc: 0.8988  (best train acc: 0.9098, best val acc: 0.9056, best train loss: 0.2281  @ epoch 14041 )\n",
      "[Epoch: 14220] train loss: 0.2656, train acc: 0.8969, val loss: 0.2589, val acc: 0.8992  (best train acc: 0.9098, best val acc: 0.9056, best train loss: 0.2277  @ epoch 14212 )\n",
      "[Epoch: 14240] train loss: 0.2564, train acc: 0.8980, val loss: 0.2645, val acc: 0.9012  (best train acc: 0.9098, best val acc: 0.9056, best train loss: 0.2277  @ epoch 14212 )\n",
      "[Epoch: 14260] train loss: 0.2529, train acc: 0.8973, val loss: 0.2778, val acc: 0.8965  (best train acc: 0.9098, best val acc: 0.9056, best train loss: 0.2277  @ epoch 14212 )\n",
      "[Epoch: 14280] train loss: 0.2401, train acc: 0.9035, val loss: 0.2796, val acc: 0.8951  (best train acc: 0.9098, best val acc: 0.9056, best train loss: 0.2277  @ epoch 14212 )\n",
      "[Epoch: 14300] train loss: 0.2597, train acc: 0.8912, val loss: 0.2507, val acc: 0.9002  (best train acc: 0.9098, best val acc: 0.9056, best train loss: 0.2277  @ epoch 14212 )\n",
      "[Epoch: 14320] train loss: 0.2435, train acc: 0.9083, val loss: 0.2538, val acc: 0.8975  (best train acc: 0.9098, best val acc: 0.9062, best train loss: 0.2277  @ epoch 14212 )\n",
      "[Epoch: 14340] train loss: 0.2648, train acc: 0.8930, val loss: 0.2549, val acc: 0.8961  (best train acc: 0.9101, best val acc: 0.9062, best train loss: 0.2277  @ epoch 14212 )\n",
      "[Epoch: 14360] train loss: 0.2521, train acc: 0.8949, val loss: 0.2548, val acc: 0.8988  (best train acc: 0.9101, best val acc: 0.9062, best train loss: 0.2277  @ epoch 14212 )\n",
      "[Epoch: 14380] train loss: 0.2805, train acc: 0.8892, val loss: 0.2890, val acc: 0.8924  (best train acc: 0.9101, best val acc: 0.9062, best train loss: 0.2277  @ epoch 14212 )\n",
      "[Epoch: 14400] train loss: 0.2488, train acc: 0.8998, val loss: 0.2511, val acc: 0.8981  (best train acc: 0.9101, best val acc: 0.9062, best train loss: 0.2277  @ epoch 14212 )\n",
      "[Epoch: 14420] train loss: 0.2451, train acc: 0.9049, val loss: 0.2537, val acc: 0.9015  (best train acc: 0.9101, best val acc: 0.9062, best train loss: 0.2277  @ epoch 14212 )\n",
      "[Epoch: 14440] train loss: 0.2428, train acc: 0.9003, val loss: 0.2554, val acc: 0.8978  (best train acc: 0.9101, best val acc: 0.9062, best train loss: 0.2277  @ epoch 14212 )\n",
      "[Epoch: 14460] train loss: 0.2536, train acc: 0.8978, val loss: 0.2522, val acc: 0.9012  (best train acc: 0.9101, best val acc: 0.9062, best train loss: 0.2277  @ epoch 14212 )\n",
      "[Epoch: 14480] train loss: 0.2302, train acc: 0.9091, val loss: 0.2509, val acc: 0.9029  (best train acc: 0.9101, best val acc: 0.9062, best train loss: 0.2269  @ epoch 14470 )\n",
      "[Epoch: 14500] train loss: 0.2635, train acc: 0.8913, val loss: 0.2592, val acc: 0.9019  (best train acc: 0.9101, best val acc: 0.9073, best train loss: 0.2269  @ epoch 14470 )\n",
      "[Epoch: 14520] train loss: 0.2346, train acc: 0.9071, val loss: 0.2571, val acc: 0.8998  (best train acc: 0.9101, best val acc: 0.9073, best train loss: 0.2269  @ epoch 14470 )\n",
      "[Epoch: 14540] train loss: 0.2365, train acc: 0.9062, val loss: 0.2500, val acc: 0.9022  (best train acc: 0.9101, best val acc: 0.9073, best train loss: 0.2269  @ epoch 14470 )\n",
      "[Epoch: 14560] train loss: 0.2356, train acc: 0.9054, val loss: 0.2585, val acc: 0.9025  (best train acc: 0.9101, best val acc: 0.9073, best train loss: 0.2240  @ epoch 14559 )\n",
      "[Epoch: 14580] train loss: 0.2608, train acc: 0.8936, val loss: 0.2424, val acc: 0.9019  (best train acc: 0.9114, best val acc: 0.9073, best train loss: 0.2240  @ epoch 14559 )\n",
      "[Epoch: 14600] train loss: 0.2439, train acc: 0.9044, val loss: 0.2494, val acc: 0.9019  (best train acc: 0.9114, best val acc: 0.9073, best train loss: 0.2240  @ epoch 14559 )\n",
      "[Epoch: 14620] train loss: 0.2414, train acc: 0.9048, val loss: 0.2600, val acc: 0.8951  (best train acc: 0.9114, best val acc: 0.9073, best train loss: 0.2240  @ epoch 14559 )\n",
      "[Epoch: 14640] train loss: 0.2315, train acc: 0.9078, val loss: 0.2481, val acc: 0.9025  (best train acc: 0.9118, best val acc: 0.9073, best train loss: 0.2240  @ epoch 14559 )\n",
      "[Epoch: 14660] train loss: 0.2398, train acc: 0.8999, val loss: 0.2501, val acc: 0.9015  (best train acc: 0.9118, best val acc: 0.9073, best train loss: 0.2240  @ epoch 14559 )\n",
      "[Epoch: 14680] train loss: 0.2583, train acc: 0.8989, val loss: 0.2785, val acc: 0.8968  (best train acc: 0.9118, best val acc: 0.9073, best train loss: 0.2240  @ epoch 14559 )\n",
      "[Epoch: 14700] train loss: 0.2402, train acc: 0.9011, val loss: 0.2550, val acc: 0.9015  (best train acc: 0.9118, best val acc: 0.9079, best train loss: 0.2240  @ epoch 14559 )\n",
      "[Epoch: 14720] train loss: 0.2398, train acc: 0.9020, val loss: 0.2652, val acc: 0.9002  (best train acc: 0.9118, best val acc: 0.9079, best train loss: 0.2240  @ epoch 14559 )\n",
      "[Epoch: 14740] train loss: 0.2380, train acc: 0.9049, val loss: 0.2476, val acc: 0.9039  (best train acc: 0.9118, best val acc: 0.9079, best train loss: 0.2240  @ epoch 14559 )\n",
      "[Epoch: 14760] train loss: 0.2400, train acc: 0.9061, val loss: 0.2397, val acc: 0.9076  (best train acc: 0.9118, best val acc: 0.9079, best train loss: 0.2216  @ epoch 14749 )\n",
      "[Epoch: 14780] train loss: 0.2586, train acc: 0.8950, val loss: 0.2605, val acc: 0.9022  (best train acc: 0.9118, best val acc: 0.9079, best train loss: 0.2216  @ epoch 14749 )\n",
      "[Epoch: 14800] train loss: 0.2333, train acc: 0.9071, val loss: 0.2493, val acc: 0.9042  (best train acc: 0.9118, best val acc: 0.9079, best train loss: 0.2216  @ epoch 14749 )\n",
      "[Epoch: 14820] train loss: 0.2409, train acc: 0.8991, val loss: 0.2516, val acc: 0.9012  (best train acc: 0.9118, best val acc: 0.9079, best train loss: 0.2216  @ epoch 14749 )\n",
      "[Epoch: 14840] train loss: 0.2405, train acc: 0.9069, val loss: 0.2504, val acc: 0.9019  (best train acc: 0.9118, best val acc: 0.9079, best train loss: 0.2216  @ epoch 14749 )\n",
      "[Epoch: 14860] train loss: 0.2468, train acc: 0.8985, val loss: 0.2465, val acc: 0.9022  (best train acc: 0.9124, best val acc: 0.9083, best train loss: 0.2216  @ epoch 14749 )\n",
      "[Epoch: 14880] train loss: 0.2315, train acc: 0.9106, val loss: 0.2483, val acc: 0.9015  (best train acc: 0.9128, best val acc: 0.9083, best train loss: 0.2211  @ epoch 14872 )\n",
      "[Epoch: 14900] train loss: 0.2286, train acc: 0.9073, val loss: 0.2537, val acc: 0.9035  (best train acc: 0.9128, best val acc: 0.9083, best train loss: 0.2211  @ epoch 14872 )\n",
      "[Epoch: 14920] train loss: 0.2754, train acc: 0.8935, val loss: 0.2457, val acc: 0.9056  (best train acc: 0.9128, best val acc: 0.9083, best train loss: 0.2211  @ epoch 14872 )\n",
      "[Epoch: 14940] train loss: 0.2405, train acc: 0.9082, val loss: 0.2437, val acc: 0.8978  (best train acc: 0.9128, best val acc: 0.9083, best train loss: 0.2205  @ epoch 14930 )\n",
      "[Epoch: 14960] train loss: 0.2638, train acc: 0.8950, val loss: 0.2608, val acc: 0.9022  (best train acc: 0.9128, best val acc: 0.9083, best train loss: 0.2205  @ epoch 14930 )\n",
      "[Epoch: 14980] train loss: 0.2471, train acc: 0.8959, val loss: 0.2584, val acc: 0.9029  (best train acc: 0.9128, best val acc: 0.9093, best train loss: 0.2205  @ epoch 14930 )\n",
      "[Epoch: 15000] train loss: 0.2836, train acc: 0.8796, val loss: 0.2464, val acc: 0.9066  (best train acc: 0.9128, best val acc: 0.9093, best train loss: 0.2205  @ epoch 14930 )\n",
      "[Epoch: 15020] train loss: 0.2587, train acc: 0.8957, val loss: 0.2488, val acc: 0.9015  (best train acc: 0.9128, best val acc: 0.9093, best train loss: 0.2205  @ epoch 14930 )\n",
      "[Epoch: 15040] train loss: 0.2535, train acc: 0.8974, val loss: 0.2506, val acc: 0.9083  (best train acc: 0.9128, best val acc: 0.9093, best train loss: 0.2205  @ epoch 14930 )\n",
      "[Epoch: 15060] train loss: 0.3051, train acc: 0.8701, val loss: 0.2669, val acc: 0.8981  (best train acc: 0.9128, best val acc: 0.9093, best train loss: 0.2205  @ epoch 14930 )\n",
      "[Epoch: 15080] train loss: 0.2497, train acc: 0.8986, val loss: 0.2523, val acc: 0.9069  (best train acc: 0.9128, best val acc: 0.9093, best train loss: 0.2205  @ epoch 14930 )\n",
      "[Epoch: 15100] train loss: 0.2475, train acc: 0.8985, val loss: 0.2439, val acc: 0.9076  (best train acc: 0.9128, best val acc: 0.9093, best train loss: 0.2205  @ epoch 14930 )\n",
      "[Epoch: 15120] train loss: 0.2437, train acc: 0.9013, val loss: 0.2470, val acc: 0.9086  (best train acc: 0.9128, best val acc: 0.9093, best train loss: 0.2205  @ epoch 14930 )\n",
      "[Epoch: 15140] train loss: 0.2444, train acc: 0.8997, val loss: 0.2455, val acc: 0.9046  (best train acc: 0.9128, best val acc: 0.9093, best train loss: 0.2205  @ epoch 14930 )\n",
      "[Epoch: 15160] train loss: 0.2285, train acc: 0.9116, val loss: 0.2451, val acc: 0.9039  (best train acc: 0.9140, best val acc: 0.9099, best train loss: 0.2205  @ epoch 14930 )\n",
      "[Epoch: 15180] train loss: 0.2355, train acc: 0.9009, val loss: 0.2588, val acc: 0.9035  (best train acc: 0.9141, best val acc: 0.9099, best train loss: 0.2205  @ epoch 14930 )\n",
      "[Epoch: 15200] train loss: 0.2489, train acc: 0.8941, val loss: 0.2478, val acc: 0.8985  (best train acc: 0.9141, best val acc: 0.9099, best train loss: 0.2205  @ epoch 14930 )\n",
      "[Epoch: 15220] train loss: 0.2550, train acc: 0.8973, val loss: 0.2429, val acc: 0.9066  (best train acc: 0.9141, best val acc: 0.9099, best train loss: 0.2205  @ epoch 14930 )\n",
      "[Epoch: 15240] train loss: 0.2389, train acc: 0.9036, val loss: 0.2523, val acc: 0.9025  (best train acc: 0.9141, best val acc: 0.9099, best train loss: 0.2205  @ epoch 14930 )\n",
      "[Epoch: 15260] train loss: 0.2486, train acc: 0.8982, val loss: 0.2488, val acc: 0.8988  (best train acc: 0.9141, best val acc: 0.9099, best train loss: 0.2205  @ epoch 14930 )\n",
      "[Epoch: 15280] train loss: 0.2796, train acc: 0.8791, val loss: 0.2445, val acc: 0.8941  (best train acc: 0.9148, best val acc: 0.9099, best train loss: 0.2175  @ epoch 15262 )\n",
      "[Epoch: 15300] train loss: 0.2493, train acc: 0.8971, val loss: 0.2598, val acc: 0.9046  (best train acc: 0.9148, best val acc: 0.9099, best train loss: 0.2175  @ epoch 15262 )\n",
      "[Epoch: 15320] train loss: 0.2399, train acc: 0.9001, val loss: 0.2437, val acc: 0.9046  (best train acc: 0.9148, best val acc: 0.9099, best train loss: 0.2175  @ epoch 15262 )\n",
      "[Epoch: 15340] train loss: 0.2393, train acc: 0.9080, val loss: 0.2518, val acc: 0.9059  (best train acc: 0.9148, best val acc: 0.9099, best train loss: 0.2175  @ epoch 15262 )\n",
      "[Epoch: 15360] train loss: 0.2663, train acc: 0.8884, val loss: 0.2618, val acc: 0.9042  (best train acc: 0.9148, best val acc: 0.9099, best train loss: 0.2175  @ epoch 15262 )\n",
      "[Epoch: 15380] train loss: 0.2409, train acc: 0.9052, val loss: 0.2477, val acc: 0.9066  (best train acc: 0.9148, best val acc: 0.9099, best train loss: 0.2175  @ epoch 15262 )\n",
      "[Epoch: 15400] train loss: 0.2468, train acc: 0.9036, val loss: 0.2668, val acc: 0.8961  (best train acc: 0.9148, best val acc: 0.9099, best train loss: 0.2175  @ epoch 15262 )\n",
      "[Epoch: 15420] train loss: 0.2291, train acc: 0.9085, val loss: 0.2457, val acc: 0.8992  (best train acc: 0.9148, best val acc: 0.9099, best train loss: 0.2175  @ epoch 15262 )\n",
      "[Epoch: 15440] train loss: 0.2263, train acc: 0.9098, val loss: 0.2383, val acc: 0.9046  (best train acc: 0.9148, best val acc: 0.9099, best train loss: 0.2175  @ epoch 15262 )\n",
      "[Epoch: 15460] train loss: 0.2429, train acc: 0.9005, val loss: 0.2444, val acc: 0.9049  (best train acc: 0.9148, best val acc: 0.9099, best train loss: 0.2175  @ epoch 15262 )\n",
      "[Epoch: 15480] train loss: 0.2432, train acc: 0.9046, val loss: 0.2568, val acc: 0.9046  (best train acc: 0.9148, best val acc: 0.9099, best train loss: 0.2147  @ epoch 15465 )\n",
      "[Epoch: 15500] train loss: 0.2353, train acc: 0.9049, val loss: 0.2486, val acc: 0.9019  (best train acc: 0.9148, best val acc: 0.9099, best train loss: 0.2147  @ epoch 15465 )\n",
      "[Epoch: 15520] train loss: 0.2751, train acc: 0.8853, val loss: 0.2490, val acc: 0.8992  (best train acc: 0.9148, best val acc: 0.9099, best train loss: 0.2147  @ epoch 15465 )\n",
      "[Epoch: 15540] train loss: 0.2561, train acc: 0.8965, val loss: 0.2614, val acc: 0.9049  (best train acc: 0.9148, best val acc: 0.9099, best train loss: 0.2147  @ epoch 15465 )\n",
      "[Epoch: 15560] train loss: 0.2275, train acc: 0.9078, val loss: 0.2527, val acc: 0.9056  (best train acc: 0.9148, best val acc: 0.9099, best train loss: 0.2147  @ epoch 15465 )\n",
      "[Epoch: 15580] train loss: 0.2523, train acc: 0.8993, val loss: 0.2549, val acc: 0.9083  (best train acc: 0.9148, best val acc: 0.9099, best train loss: 0.2147  @ epoch 15465 )\n",
      "[Epoch: 15600] train loss: 0.2263, train acc: 0.9095, val loss: 0.2515, val acc: 0.9046  (best train acc: 0.9148, best val acc: 0.9099, best train loss: 0.2147  @ epoch 15465 )\n",
      "[Epoch: 15620] train loss: 0.2407, train acc: 0.9011, val loss: 0.2544, val acc: 0.9035  (best train acc: 0.9148, best val acc: 0.9099, best train loss: 0.2147  @ epoch 15465 )\n",
      "[Epoch: 15640] train loss: 0.2275, train acc: 0.9086, val loss: 0.2512, val acc: 0.8998  (best train acc: 0.9148, best val acc: 0.9099, best train loss: 0.2147  @ epoch 15465 )\n",
      "[Epoch: 15660] train loss: 0.2343, train acc: 0.9080, val loss: 0.2460, val acc: 0.8931  (best train acc: 0.9148, best val acc: 0.9099, best train loss: 0.2147  @ epoch 15465 )\n",
      "[Epoch: 15680] train loss: 0.2640, train acc: 0.8921, val loss: 0.2490, val acc: 0.9042  (best train acc: 0.9148, best val acc: 0.9103, best train loss: 0.2147  @ epoch 15465 )\n",
      "[Epoch: 15700] train loss: 0.2398, train acc: 0.9017, val loss: 0.2519, val acc: 0.9005  (best train acc: 0.9148, best val acc: 0.9103, best train loss: 0.2147  @ epoch 15465 )\n",
      "[Epoch: 15720] train loss: 0.2310, train acc: 0.9036, val loss: 0.2472, val acc: 0.9086  (best train acc: 0.9148, best val acc: 0.9103, best train loss: 0.2147  @ epoch 15465 )\n",
      "[Epoch: 15740] train loss: 0.2338, train acc: 0.9090, val loss: 0.2446, val acc: 0.9059  (best train acc: 0.9148, best val acc: 0.9103, best train loss: 0.2147  @ epoch 15465 )\n",
      "[Epoch: 15760] train loss: 0.2495, train acc: 0.8993, val loss: 0.2417, val acc: 0.9059  (best train acc: 0.9148, best val acc: 0.9103, best train loss: 0.2147  @ epoch 15465 )\n",
      "[Epoch: 15780] train loss: 0.2427, train acc: 0.9045, val loss: 0.2524, val acc: 0.9073  (best train acc: 0.9148, best val acc: 0.9103, best train loss: 0.2147  @ epoch 15465 )\n",
      "[Epoch: 15800] train loss: 0.2314, train acc: 0.9066, val loss: 0.2410, val acc: 0.9079  (best train acc: 0.9148, best val acc: 0.9103, best train loss: 0.2147  @ epoch 15465 )\n",
      "[Epoch: 15820] train loss: 0.2409, train acc: 0.8989, val loss: 0.2432, val acc: 0.9049  (best train acc: 0.9148, best val acc: 0.9103, best train loss: 0.2147  @ epoch 15465 )\n",
      "[Epoch: 15840] train loss: 0.2375, train acc: 0.9073, val loss: 0.2525, val acc: 0.9056  (best train acc: 0.9148, best val acc: 0.9113, best train loss: 0.2147  @ epoch 15465 )\n",
      "[Epoch: 15860] train loss: 0.2337, train acc: 0.9065, val loss: 0.2476, val acc: 0.9049  (best train acc: 0.9148, best val acc: 0.9113, best train loss: 0.2147  @ epoch 15465 )\n",
      "[Epoch: 15880] train loss: 0.2671, train acc: 0.8944, val loss: 0.2512, val acc: 0.9022  (best train acc: 0.9148, best val acc: 0.9113, best train loss: 0.2147  @ epoch 15465 )\n",
      "[Epoch: 15900] train loss: 0.2509, train acc: 0.8965, val loss: 0.2444, val acc: 0.9086  (best train acc: 0.9148, best val acc: 0.9113, best train loss: 0.2147  @ epoch 15465 )\n",
      "[Epoch: 15920] train loss: 0.2383, train acc: 0.9059, val loss: 0.2505, val acc: 0.9059  (best train acc: 0.9148, best val acc: 0.9113, best train loss: 0.2147  @ epoch 15465 )\n",
      "[Epoch: 15940] train loss: 0.2180, train acc: 0.9146, val loss: 0.2454, val acc: 0.9073  (best train acc: 0.9148, best val acc: 0.9113, best train loss: 0.2147  @ epoch 15465 )\n",
      "[Epoch: 15960] train loss: 0.2315, train acc: 0.9065, val loss: 0.2566, val acc: 0.9042  (best train acc: 0.9148, best val acc: 0.9113, best train loss: 0.2147  @ epoch 15465 )\n",
      "[Epoch: 15980] train loss: 0.2319, train acc: 0.9089, val loss: 0.2541, val acc: 0.9022  (best train acc: 0.9148, best val acc: 0.9113, best train loss: 0.2147  @ epoch 15465 )\n",
      "[Epoch: 16000] train loss: 0.2270, train acc: 0.9060, val loss: 0.2489, val acc: 0.9039  (best train acc: 0.9148, best val acc: 0.9116, best train loss: 0.2147  @ epoch 15465 )\n",
      "[Epoch: 16020] train loss: 0.2170, train acc: 0.9123, val loss: 0.2396, val acc: 0.9096  (best train acc: 0.9148, best val acc: 0.9116, best train loss: 0.2147  @ epoch 15465 )\n",
      "[Epoch: 16040] train loss: 0.2347, train acc: 0.9028, val loss: 0.2374, val acc: 0.9083  (best train acc: 0.9148, best val acc: 0.9116, best train loss: 0.2147  @ epoch 15465 )\n",
      "[Epoch: 16060] train loss: 0.2229, train acc: 0.9116, val loss: 0.2465, val acc: 0.9066  (best train acc: 0.9156, best val acc: 0.9116, best train loss: 0.2135  @ epoch 16059 )\n",
      "[Epoch: 16080] train loss: 0.2358, train acc: 0.9062, val loss: 0.2615, val acc: 0.9025  (best train acc: 0.9156, best val acc: 0.9116, best train loss: 0.2135  @ epoch 16059 )\n",
      "[Epoch: 16100] train loss: 0.2214, train acc: 0.9102, val loss: 0.2397, val acc: 0.9052  (best train acc: 0.9156, best val acc: 0.9116, best train loss: 0.2135  @ epoch 16059 )\n",
      "[Epoch: 16120] train loss: 0.2242, train acc: 0.9096, val loss: 0.2436, val acc: 0.9042  (best train acc: 0.9156, best val acc: 0.9116, best train loss: 0.2135  @ epoch 16059 )\n",
      "[Epoch: 16140] train loss: 0.2225, train acc: 0.9089, val loss: 0.2365, val acc: 0.9086  (best train acc: 0.9156, best val acc: 0.9116, best train loss: 0.2135  @ epoch 16059 )\n",
      "[Epoch: 16160] train loss: 0.2263, train acc: 0.9083, val loss: 0.2444, val acc: 0.9049  (best train acc: 0.9156, best val acc: 0.9116, best train loss: 0.2135  @ epoch 16059 )\n",
      "[Epoch: 16180] train loss: 0.2231, train acc: 0.9067, val loss: 0.2434, val acc: 0.9025  (best train acc: 0.9156, best val acc: 0.9120, best train loss: 0.2135  @ epoch 16059 )\n",
      "[Epoch: 16200] train loss: 0.2383, train acc: 0.9013, val loss: 0.2418, val acc: 0.9005  (best train acc: 0.9156, best val acc: 0.9120, best train loss: 0.2135  @ epoch 16059 )\n",
      "[Epoch: 16220] train loss: 0.2533, train acc: 0.8955, val loss: 0.2401, val acc: 0.9052  (best train acc: 0.9156, best val acc: 0.9120, best train loss: 0.2135  @ epoch 16059 )\n",
      "[Epoch: 16240] train loss: 0.2465, train acc: 0.9043, val loss: 0.2412, val acc: 0.9099  (best train acc: 0.9156, best val acc: 0.9120, best train loss: 0.2135  @ epoch 16059 )\n",
      "[Epoch: 16260] train loss: 0.2302, train acc: 0.9079, val loss: 0.2360, val acc: 0.9029  (best train acc: 0.9156, best val acc: 0.9120, best train loss: 0.2135  @ epoch 16059 )\n",
      "[Epoch: 16280] train loss: 0.2290, train acc: 0.9071, val loss: 0.2546, val acc: 0.9056  (best train acc: 0.9156, best val acc: 0.9120, best train loss: 0.2135  @ epoch 16059 )\n",
      "[Epoch: 16300] train loss: 0.2242, train acc: 0.9050, val loss: 0.2401, val acc: 0.9025  (best train acc: 0.9156, best val acc: 0.9120, best train loss: 0.2135  @ epoch 16059 )\n",
      "[Epoch: 16320] train loss: 0.2418, train acc: 0.9016, val loss: 0.2442, val acc: 0.9106  (best train acc: 0.9156, best val acc: 0.9120, best train loss: 0.2135  @ epoch 16059 )\n",
      "[Epoch: 16340] train loss: 0.2302, train acc: 0.9046, val loss: 0.2653, val acc: 0.8975  (best train acc: 0.9156, best val acc: 0.9120, best train loss: 0.2135  @ epoch 16059 )\n",
      "[Epoch: 16360] train loss: 0.2277, train acc: 0.9085, val loss: 0.2386, val acc: 0.8998  (best train acc: 0.9156, best val acc: 0.9120, best train loss: 0.2135  @ epoch 16059 )\n",
      "[Epoch: 16380] train loss: 0.2277, train acc: 0.9083, val loss: 0.2527, val acc: 0.9076  (best train acc: 0.9156, best val acc: 0.9120, best train loss: 0.2135  @ epoch 16059 )\n",
      "[Epoch: 16400] train loss: 0.2296, train acc: 0.9065, val loss: 0.2520, val acc: 0.9005  (best train acc: 0.9156, best val acc: 0.9120, best train loss: 0.2135  @ epoch 16059 )\n",
      "[Epoch: 16420] train loss: 0.2270, train acc: 0.9078, val loss: 0.2462, val acc: 0.9019  (best train acc: 0.9156, best val acc: 0.9120, best train loss: 0.2135  @ epoch 16059 )\n",
      "[Epoch: 16440] train loss: 0.2321, train acc: 0.9075, val loss: 0.2516, val acc: 0.9042  (best train acc: 0.9156, best val acc: 0.9120, best train loss: 0.2135  @ epoch 16059 )\n",
      "[Epoch: 16460] train loss: 0.2583, train acc: 0.8934, val loss: 0.2782, val acc: 0.8924  (best train acc: 0.9156, best val acc: 0.9120, best train loss: 0.2135  @ epoch 16059 )\n",
      "[Epoch: 16480] train loss: 0.2437, train acc: 0.9103, val loss: 0.2446, val acc: 0.9069  (best train acc: 0.9156, best val acc: 0.9120, best train loss: 0.2135  @ epoch 16059 )\n",
      "[Epoch: 16500] train loss: 0.2617, train acc: 0.8963, val loss: 0.2447, val acc: 0.9059  (best train acc: 0.9156, best val acc: 0.9120, best train loss: 0.2135  @ epoch 16059 )\n",
      "[Epoch: 16520] train loss: 0.2261, train acc: 0.9041, val loss: 0.2378, val acc: 0.9059  (best train acc: 0.9156, best val acc: 0.9120, best train loss: 0.2135  @ epoch 16059 )\n",
      "[Epoch: 16540] train loss: 0.2411, train acc: 0.8989, val loss: 0.2540, val acc: 0.8998  (best train acc: 0.9156, best val acc: 0.9120, best train loss: 0.2135  @ epoch 16059 )\n",
      "[Epoch: 16560] train loss: 0.2419, train acc: 0.8967, val loss: 0.2565, val acc: 0.9056  (best train acc: 0.9156, best val acc: 0.9120, best train loss: 0.2135  @ epoch 16059 )\n",
      "[Epoch: 16580] train loss: 0.2201, train acc: 0.9095, val loss: 0.2591, val acc: 0.9049  (best train acc: 0.9156, best val acc: 0.9120, best train loss: 0.2135  @ epoch 16059 )\n",
      "[Epoch: 16600] train loss: 0.2581, train acc: 0.8945, val loss: 0.2730, val acc: 0.8978  (best train acc: 0.9156, best val acc: 0.9120, best train loss: 0.2135  @ epoch 16059 )\n",
      "[Epoch: 16620] train loss: 0.2332, train acc: 0.9065, val loss: 0.2514, val acc: 0.9069  (best train acc: 0.9156, best val acc: 0.9120, best train loss: 0.2135  @ epoch 16059 )\n",
      "[Epoch: 16640] train loss: 0.2271, train acc: 0.9076, val loss: 0.2482, val acc: 0.9066  (best train acc: 0.9162, best val acc: 0.9120, best train loss: 0.2135  @ epoch 16059 )\n",
      "[Epoch: 16660] train loss: 0.2371, train acc: 0.9025, val loss: 0.2477, val acc: 0.9025  (best train acc: 0.9162, best val acc: 0.9120, best train loss: 0.2135  @ epoch 16059 )\n",
      "[Epoch: 16680] train loss: 0.2279, train acc: 0.9024, val loss: 0.2393, val acc: 0.9002  (best train acc: 0.9162, best val acc: 0.9120, best train loss: 0.2135  @ epoch 16059 )\n",
      "[Epoch: 16700] train loss: 0.2301, train acc: 0.9064, val loss: 0.2472, val acc: 0.9073  (best train acc: 0.9162, best val acc: 0.9120, best train loss: 0.2135  @ epoch 16059 )\n",
      "[Epoch: 16720] train loss: 0.2262, train acc: 0.9049, val loss: 0.2431, val acc: 0.9083  (best train acc: 0.9162, best val acc: 0.9120, best train loss: 0.2135  @ epoch 16059 )\n",
      "[Epoch: 16740] train loss: 0.2278, train acc: 0.9095, val loss: 0.2435, val acc: 0.9062  (best train acc: 0.9162, best val acc: 0.9120, best train loss: 0.2135  @ epoch 16059 )\n",
      "[Epoch: 16760] train loss: 0.2267, train acc: 0.9081, val loss: 0.2480, val acc: 0.9076  (best train acc: 0.9162, best val acc: 0.9120, best train loss: 0.2135  @ epoch 16059 )\n",
      "[Epoch: 16780] train loss: 0.2292, train acc: 0.9069, val loss: 0.2339, val acc: 0.9042  (best train acc: 0.9162, best val acc: 0.9120, best train loss: 0.2134  @ epoch 16778 )\n",
      "[Epoch: 16800] train loss: 0.2302, train acc: 0.9108, val loss: 0.2425, val acc: 0.9046  (best train acc: 0.9162, best val acc: 0.9120, best train loss: 0.2134  @ epoch 16778 )\n",
      "[Epoch: 16820] train loss: 0.2277, train acc: 0.9040, val loss: 0.2417, val acc: 0.9019  (best train acc: 0.9162, best val acc: 0.9120, best train loss: 0.2134  @ epoch 16778 )\n",
      "[Epoch: 16840] train loss: 0.2270, train acc: 0.9064, val loss: 0.2346, val acc: 0.9079  (best train acc: 0.9162, best val acc: 0.9120, best train loss: 0.2134  @ epoch 16778 )\n",
      "[Epoch: 16860] train loss: 0.2391, train acc: 0.8981, val loss: 0.2356, val acc: 0.9049  (best train acc: 0.9162, best val acc: 0.9120, best train loss: 0.2134  @ epoch 16778 )\n",
      "[Epoch: 16880] train loss: 0.2347, train acc: 0.9073, val loss: 0.2441, val acc: 0.9079  (best train acc: 0.9162, best val acc: 0.9120, best train loss: 0.2134  @ epoch 16778 )\n",
      "[Epoch: 16900] train loss: 0.2212, train acc: 0.9093, val loss: 0.2366, val acc: 0.9086  (best train acc: 0.9162, best val acc: 0.9120, best train loss: 0.2134  @ epoch 16778 )\n",
      "[Epoch: 16920] train loss: 0.3034, train acc: 0.8861, val loss: 0.2562, val acc: 0.9079  (best train acc: 0.9170, best val acc: 0.9120, best train loss: 0.2112  @ epoch 16901 )\n",
      "[Epoch: 16940] train loss: 0.2308, train acc: 0.9068, val loss: 0.2374, val acc: 0.9062  (best train acc: 0.9170, best val acc: 0.9120, best train loss: 0.2112  @ epoch 16901 )\n",
      "[Epoch: 16960] train loss: 0.2311, train acc: 0.9027, val loss: 0.2537, val acc: 0.9035  (best train acc: 0.9170, best val acc: 0.9120, best train loss: 0.2112  @ epoch 16901 )\n",
      "[Epoch: 16980] train loss: 0.2359, train acc: 0.8994, val loss: 0.2578, val acc: 0.9042  (best train acc: 0.9170, best val acc: 0.9120, best train loss: 0.2112  @ epoch 16901 )\n",
      "[Epoch: 17000] train loss: 0.2229, train acc: 0.9110, val loss: 0.2348, val acc: 0.9029  (best train acc: 0.9170, best val acc: 0.9120, best train loss: 0.2112  @ epoch 16901 )\n",
      "[Epoch: 17020] train loss: 0.2373, train acc: 0.9035, val loss: 0.2322, val acc: 0.9066  (best train acc: 0.9170, best val acc: 0.9120, best train loss: 0.2112  @ epoch 16901 )\n",
      "[Epoch: 17040] train loss: 0.2329, train acc: 0.9043, val loss: 0.2507, val acc: 0.9032  (best train acc: 0.9170, best val acc: 0.9120, best train loss: 0.2112  @ epoch 17022 )\n",
      "[Epoch: 17060] train loss: 0.2190, train acc: 0.9117, val loss: 0.2450, val acc: 0.9002  (best train acc: 0.9170, best val acc: 0.9130, best train loss: 0.2112  @ epoch 17022 )\n",
      "[Epoch: 17080] train loss: 0.2235, train acc: 0.9097, val loss: 0.2409, val acc: 0.9049  (best train acc: 0.9170, best val acc: 0.9130, best train loss: 0.2112  @ epoch 17022 )\n",
      "[Epoch: 17100] train loss: 0.2279, train acc: 0.9098, val loss: 0.2453, val acc: 0.9025  (best train acc: 0.9170, best val acc: 0.9130, best train loss: 0.2112  @ epoch 17022 )\n",
      "[Epoch: 17120] train loss: 0.2200, train acc: 0.9123, val loss: 0.2351, val acc: 0.9059  (best train acc: 0.9170, best val acc: 0.9130, best train loss: 0.2112  @ epoch 17022 )\n",
      "[Epoch: 17140] train loss: 0.2330, train acc: 0.9069, val loss: 0.2384, val acc: 0.9005  (best train acc: 0.9170, best val acc: 0.9130, best train loss: 0.2112  @ epoch 17022 )\n",
      "[Epoch: 17160] train loss: 0.2379, train acc: 0.9033, val loss: 0.2426, val acc: 0.9035  (best train acc: 0.9170, best val acc: 0.9130, best train loss: 0.2112  @ epoch 17022 )\n",
      "[Epoch: 17180] train loss: 0.2593, train acc: 0.9001, val loss: 0.2425, val acc: 0.9022  (best train acc: 0.9170, best val acc: 0.9130, best train loss: 0.2112  @ epoch 17022 )\n",
      "[Epoch: 17200] train loss: 0.2882, train acc: 0.8856, val loss: 0.2623, val acc: 0.9076  (best train acc: 0.9170, best val acc: 0.9130, best train loss: 0.2112  @ epoch 17022 )\n",
      "[Epoch: 17220] train loss: 0.2159, train acc: 0.9135, val loss: 0.2388, val acc: 0.9046  (best train acc: 0.9170, best val acc: 0.9130, best train loss: 0.2112  @ epoch 17022 )\n",
      "[Epoch: 17240] train loss: 0.2183, train acc: 0.9130, val loss: 0.2442, val acc: 0.9022  (best train acc: 0.9170, best val acc: 0.9130, best train loss: 0.2112  @ epoch 17022 )\n",
      "[Epoch: 17260] train loss: 0.2361, train acc: 0.9073, val loss: 0.2354, val acc: 0.9049  (best train acc: 0.9170, best val acc: 0.9130, best train loss: 0.2112  @ epoch 17022 )\n",
      "[Epoch: 17280] train loss: 0.2639, train acc: 0.8931, val loss: 0.2549, val acc: 0.9056  (best train acc: 0.9170, best val acc: 0.9130, best train loss: 0.2112  @ epoch 17022 )\n",
      "[Epoch: 17300] train loss: 0.2323, train acc: 0.9079, val loss: 0.2371, val acc: 0.9113  (best train acc: 0.9170, best val acc: 0.9130, best train loss: 0.2112  @ epoch 17022 )\n",
      "[Epoch: 17320] train loss: 0.2389, train acc: 0.9068, val loss: 0.2354, val acc: 0.9032  (best train acc: 0.9170, best val acc: 0.9130, best train loss: 0.2112  @ epoch 17022 )\n",
      "[Epoch: 17340] train loss: 0.2244, train acc: 0.9062, val loss: 0.2748, val acc: 0.8971  (best train acc: 0.9170, best val acc: 0.9130, best train loss: 0.2112  @ epoch 17022 )\n",
      "[Epoch: 17360] train loss: 0.2185, train acc: 0.9136, val loss: 0.2396, val acc: 0.9029  (best train acc: 0.9170, best val acc: 0.9130, best train loss: 0.2112  @ epoch 17022 )\n",
      "[Epoch: 17380] train loss: 0.2176, train acc: 0.9122, val loss: 0.2384, val acc: 0.9046  (best train acc: 0.9170, best val acc: 0.9130, best train loss: 0.2112  @ epoch 17022 )\n",
      "[Epoch: 17400] train loss: 0.2138, train acc: 0.9135, val loss: 0.2386, val acc: 0.9049  (best train acc: 0.9170, best val acc: 0.9130, best train loss: 0.2112  @ epoch 17022 )\n",
      "[Epoch: 17420] train loss: 0.2409, train acc: 0.9075, val loss: 0.2439, val acc: 0.9019  (best train acc: 0.9170, best val acc: 0.9130, best train loss: 0.2112  @ epoch 17022 )\n",
      "[Epoch: 17440] train loss: 0.2618, train acc: 0.8923, val loss: 0.2439, val acc: 0.9113  (best train acc: 0.9170, best val acc: 0.9130, best train loss: 0.2112  @ epoch 17022 )\n",
      "[Epoch: 17460] train loss: 0.2284, train acc: 0.9149, val loss: 0.2668, val acc: 0.8985  (best train acc: 0.9170, best val acc: 0.9130, best train loss: 0.2112  @ epoch 17022 )\n",
      "[Epoch: 17480] train loss: 0.2680, train acc: 0.8872, val loss: 0.2641, val acc: 0.9005  (best train acc: 0.9170, best val acc: 0.9130, best train loss: 0.2112  @ epoch 17022 )\n",
      "[Epoch: 17500] train loss: 0.2219, train acc: 0.9104, val loss: 0.2380, val acc: 0.9110  (best train acc: 0.9170, best val acc: 0.9130, best train loss: 0.2112  @ epoch 17022 )\n",
      "[Epoch: 17520] train loss: 0.2272, train acc: 0.9046, val loss: 0.2414, val acc: 0.9073  (best train acc: 0.9170, best val acc: 0.9130, best train loss: 0.2112  @ epoch 17022 )\n",
      "[Epoch: 17540] train loss: 0.2316, train acc: 0.9051, val loss: 0.2347, val acc: 0.9062  (best train acc: 0.9170, best val acc: 0.9130, best train loss: 0.2112  @ epoch 17022 )\n",
      "[Epoch: 17560] train loss: 0.2269, train acc: 0.9099, val loss: 0.2314, val acc: 0.9069  (best train acc: 0.9170, best val acc: 0.9130, best train loss: 0.2112  @ epoch 17022 )\n",
      "[Epoch: 17580] train loss: 0.2400, train acc: 0.9007, val loss: 0.2449, val acc: 0.9073  (best train acc: 0.9170, best val acc: 0.9130, best train loss: 0.2112  @ epoch 17022 )\n",
      "[Epoch: 17600] train loss: 0.2387, train acc: 0.9030, val loss: 0.2529, val acc: 0.9042  (best train acc: 0.9170, best val acc: 0.9130, best train loss: 0.2112  @ epoch 17022 )\n",
      "[Epoch: 17620] train loss: 0.2319, train acc: 0.9082, val loss: 0.2466, val acc: 0.9106  (best train acc: 0.9170, best val acc: 0.9130, best train loss: 0.2112  @ epoch 17022 )\n",
      "[Epoch: 17640] train loss: 0.2258, train acc: 0.9083, val loss: 0.2350, val acc: 0.9083  (best train acc: 0.9170, best val acc: 0.9130, best train loss: 0.2112  @ epoch 17022 )\n",
      "[Epoch: 17660] train loss: 0.2198, train acc: 0.9104, val loss: 0.2437, val acc: 0.9083  (best train acc: 0.9170, best val acc: 0.9130, best train loss: 0.2112  @ epoch 17022 )\n",
      "[Epoch: 17680] train loss: 0.2343, train acc: 0.9064, val loss: 0.2543, val acc: 0.9062  (best train acc: 0.9180, best val acc: 0.9130, best train loss: 0.2112  @ epoch 17022 )\n",
      "[Epoch: 17700] train loss: 0.2145, train acc: 0.9135, val loss: 0.2394, val acc: 0.9049  (best train acc: 0.9180, best val acc: 0.9130, best train loss: 0.2112  @ epoch 17022 )\n",
      "[Epoch: 17720] train loss: 0.2237, train acc: 0.9076, val loss: 0.2305, val acc: 0.9099  (best train acc: 0.9180, best val acc: 0.9130, best train loss: 0.2112  @ epoch 17022 )\n",
      "[Epoch: 17740] train loss: 0.2251, train acc: 0.9117, val loss: 0.2418, val acc: 0.9093  (best train acc: 0.9180, best val acc: 0.9130, best train loss: 0.2086  @ epoch 17733 )\n",
      "[Epoch: 17760] train loss: 0.2117, train acc: 0.9175, val loss: 0.2364, val acc: 0.9123  (best train acc: 0.9180, best val acc: 0.9143, best train loss: 0.2086  @ epoch 17733 )\n",
      "[Epoch: 17780] train loss: 0.2224, train acc: 0.9122, val loss: 0.2467, val acc: 0.9076  (best train acc: 0.9180, best val acc: 0.9143, best train loss: 0.2086  @ epoch 17733 )\n",
      "[Epoch: 17800] train loss: 0.2384, train acc: 0.9062, val loss: 0.2396, val acc: 0.9086  (best train acc: 0.9180, best val acc: 0.9143, best train loss: 0.2086  @ epoch 17733 )\n",
      "[Epoch: 17820] train loss: 0.2272, train acc: 0.9114, val loss: 0.2446, val acc: 0.9073  (best train acc: 0.9180, best val acc: 0.9143, best train loss: 0.2086  @ epoch 17733 )\n",
      "[Epoch: 17840] train loss: 0.2407, train acc: 0.8981, val loss: 0.2528, val acc: 0.9062  (best train acc: 0.9180, best val acc: 0.9147, best train loss: 0.2086  @ epoch 17733 )\n",
      "[Epoch: 17860] train loss: 0.2250, train acc: 0.9097, val loss: 0.2560, val acc: 0.9039  (best train acc: 0.9180, best val acc: 0.9147, best train loss: 0.2086  @ epoch 17733 )\n",
      "[Epoch: 17880] train loss: 0.2250, train acc: 0.9128, val loss: 0.2421, val acc: 0.9106  (best train acc: 0.9180, best val acc: 0.9147, best train loss: 0.2086  @ epoch 17733 )\n",
      "[Epoch: 17900] train loss: 0.2169, train acc: 0.9117, val loss: 0.2309, val acc: 0.9103  (best train acc: 0.9180, best val acc: 0.9147, best train loss: 0.2086  @ epoch 17733 )\n",
      "[Epoch: 17920] train loss: 0.2207, train acc: 0.9107, val loss: 0.2396, val acc: 0.9073  (best train acc: 0.9180, best val acc: 0.9147, best train loss: 0.2086  @ epoch 17733 )\n",
      "[Epoch: 17940] train loss: 0.2251, train acc: 0.9102, val loss: 0.2585, val acc: 0.9005  (best train acc: 0.9180, best val acc: 0.9147, best train loss: 0.2086  @ epoch 17733 )\n",
      "[Epoch: 17960] train loss: 0.2377, train acc: 0.9075, val loss: 0.2390, val acc: 0.9103  (best train acc: 0.9180, best val acc: 0.9147, best train loss: 0.2062  @ epoch 17949 )\n",
      "[Epoch: 17980] train loss: 0.2271, train acc: 0.9068, val loss: 0.2387, val acc: 0.9099  (best train acc: 0.9180, best val acc: 0.9147, best train loss: 0.2062  @ epoch 17949 )\n",
      "[Epoch: 18000] train loss: 0.2337, train acc: 0.9038, val loss: 0.2389, val acc: 0.9049  (best train acc: 0.9180, best val acc: 0.9147, best train loss: 0.2062  @ epoch 17949 )\n",
      "[Epoch: 18020] train loss: 0.2278, train acc: 0.9093, val loss: 0.2377, val acc: 0.9083  (best train acc: 0.9180, best val acc: 0.9147, best train loss: 0.2062  @ epoch 17949 )\n",
      "[Epoch: 18040] train loss: 0.2281, train acc: 0.9083, val loss: 0.2642, val acc: 0.9002  (best train acc: 0.9180, best val acc: 0.9147, best train loss: 0.2062  @ epoch 17949 )\n",
      "[Epoch: 18060] train loss: 0.2392, train acc: 0.9058, val loss: 0.2568, val acc: 0.9052  (best train acc: 0.9180, best val acc: 0.9147, best train loss: 0.2062  @ epoch 17949 )\n",
      "[Epoch: 18080] train loss: 0.2254, train acc: 0.9107, val loss: 0.2393, val acc: 0.9096  (best train acc: 0.9180, best val acc: 0.9147, best train loss: 0.2062  @ epoch 17949 )\n",
      "[Epoch: 18100] train loss: 0.2324, train acc: 0.9088, val loss: 0.2315, val acc: 0.9113  (best train acc: 0.9180, best val acc: 0.9147, best train loss: 0.2062  @ epoch 17949 )\n",
      "[Epoch: 18120] train loss: 0.2263, train acc: 0.9092, val loss: 0.2408, val acc: 0.9096  (best train acc: 0.9180, best val acc: 0.9147, best train loss: 0.2062  @ epoch 17949 )\n",
      "[Epoch: 18140] train loss: 0.2251, train acc: 0.9070, val loss: 0.2464, val acc: 0.9069  (best train acc: 0.9180, best val acc: 0.9147, best train loss: 0.2062  @ epoch 17949 )\n",
      "[Epoch: 18160] train loss: 0.2329, train acc: 0.9037, val loss: 0.2350, val acc: 0.9089  (best train acc: 0.9180, best val acc: 0.9147, best train loss: 0.2062  @ epoch 17949 )\n",
      "[Epoch: 18180] train loss: 0.2496, train acc: 0.8970, val loss: 0.2520, val acc: 0.8965  (best train acc: 0.9180, best val acc: 0.9147, best train loss: 0.2062  @ epoch 17949 )\n",
      "[Epoch: 18200] train loss: 0.2204, train acc: 0.9130, val loss: 0.2360, val acc: 0.9103  (best train acc: 0.9180, best val acc: 0.9147, best train loss: 0.2062  @ epoch 17949 )\n",
      "[Epoch: 18220] train loss: 0.2302, train acc: 0.9071, val loss: 0.2478, val acc: 0.9056  (best train acc: 0.9180, best val acc: 0.9147, best train loss: 0.2062  @ epoch 17949 )\n",
      "[Epoch: 18240] train loss: 0.2390, train acc: 0.9020, val loss: 0.2515, val acc: 0.9049  (best train acc: 0.9180, best val acc: 0.9147, best train loss: 0.2062  @ epoch 17949 )\n",
      "[Epoch: 18260] train loss: 0.2196, train acc: 0.9115, val loss: 0.2368, val acc: 0.9069  (best train acc: 0.9180, best val acc: 0.9147, best train loss: 0.2062  @ epoch 17949 )\n",
      "[Epoch: 18280] train loss: 0.2508, train acc: 0.8989, val loss: 0.2505, val acc: 0.9032  (best train acc: 0.9180, best val acc: 0.9147, best train loss: 0.2062  @ epoch 17949 )\n",
      "[Epoch: 18300] train loss: 0.2149, train acc: 0.9135, val loss: 0.2421, val acc: 0.9110  (best train acc: 0.9180, best val acc: 0.9147, best train loss: 0.2062  @ epoch 17949 )\n",
      "[Epoch: 18320] train loss: 0.2174, train acc: 0.9109, val loss: 0.2360, val acc: 0.9069  (best train acc: 0.9180, best val acc: 0.9147, best train loss: 0.2062  @ epoch 17949 )\n",
      "[Epoch: 18340] train loss: 0.2161, train acc: 0.9122, val loss: 0.2406, val acc: 0.9083  (best train acc: 0.9180, best val acc: 0.9147, best train loss: 0.2062  @ epoch 17949 )\n",
      "[Epoch: 18360] train loss: 0.2104, train acc: 0.9160, val loss: 0.2351, val acc: 0.9069  (best train acc: 0.9180, best val acc: 0.9147, best train loss: 0.2062  @ epoch 17949 )\n",
      "[Epoch: 18380] train loss: 0.2223, train acc: 0.9074, val loss: 0.2446, val acc: 0.9083  (best train acc: 0.9180, best val acc: 0.9147, best train loss: 0.2062  @ epoch 17949 )\n",
      "[Epoch: 18400] train loss: 0.2415, train acc: 0.9072, val loss: 0.2373, val acc: 0.9059  (best train acc: 0.9180, best val acc: 0.9147, best train loss: 0.2062  @ epoch 17949 )\n",
      "[Epoch: 18420] train loss: 0.2229, train acc: 0.9099, val loss: 0.2444, val acc: 0.9049  (best train acc: 0.9180, best val acc: 0.9147, best train loss: 0.2062  @ epoch 17949 )\n",
      "[Epoch: 18440] train loss: 0.2288, train acc: 0.9078, val loss: 0.2450, val acc: 0.9073  (best train acc: 0.9180, best val acc: 0.9147, best train loss: 0.2062  @ epoch 17949 )\n",
      "[Epoch: 18460] train loss: 0.2100, train acc: 0.9166, val loss: 0.2538, val acc: 0.9025  (best train acc: 0.9180, best val acc: 0.9147, best train loss: 0.2062  @ epoch 17949 )\n",
      "[Epoch: 18480] train loss: 0.2359, train acc: 0.9023, val loss: 0.2376, val acc: 0.9103  (best train acc: 0.9180, best val acc: 0.9147, best train loss: 0.2062  @ epoch 17949 )\n",
      "[Epoch: 18500] train loss: 0.2332, train acc: 0.9037, val loss: 0.2354, val acc: 0.9086  (best train acc: 0.9180, best val acc: 0.9147, best train loss: 0.2062  @ epoch 17949 )\n",
      "[Epoch: 18520] train loss: 0.2409, train acc: 0.9015, val loss: 0.2426, val acc: 0.9076  (best train acc: 0.9180, best val acc: 0.9147, best train loss: 0.2062  @ epoch 17949 )\n",
      "[Epoch: 18540] train loss: 0.2138, train acc: 0.9124, val loss: 0.2516, val acc: 0.9049  (best train acc: 0.9180, best val acc: 0.9147, best train loss: 0.2062  @ epoch 17949 )\n",
      "[Epoch: 18560] train loss: 0.2283, train acc: 0.9041, val loss: 0.2381, val acc: 0.9120  (best train acc: 0.9180, best val acc: 0.9147, best train loss: 0.2062  @ epoch 17949 )\n",
      "[Epoch: 18580] train loss: 0.2274, train acc: 0.9074, val loss: 0.2528, val acc: 0.9056  (best train acc: 0.9180, best val acc: 0.9147, best train loss: 0.2046  @ epoch 18570 )\n",
      "[Epoch: 18600] train loss: 0.2230, train acc: 0.9098, val loss: 0.2381, val acc: 0.9096  (best train acc: 0.9180, best val acc: 0.9147, best train loss: 0.2046  @ epoch 18570 )\n",
      "[Epoch: 18620] train loss: 0.2296, train acc: 0.9063, val loss: 0.2364, val acc: 0.9069  (best train acc: 0.9180, best val acc: 0.9147, best train loss: 0.2046  @ epoch 18570 )\n",
      "[Epoch: 18640] train loss: 0.2170, train acc: 0.9138, val loss: 0.2397, val acc: 0.9096  (best train acc: 0.9180, best val acc: 0.9147, best train loss: 0.2046  @ epoch 18570 )\n",
      "[Epoch: 18660] train loss: 0.2160, train acc: 0.9134, val loss: 0.2345, val acc: 0.9137  (best train acc: 0.9180, best val acc: 0.9147, best train loss: 0.2046  @ epoch 18570 )\n",
      "[Epoch: 18680] train loss: 0.2283, train acc: 0.9080, val loss: 0.2532, val acc: 0.8992  (best train acc: 0.9180, best val acc: 0.9147, best train loss: 0.2046  @ epoch 18570 )\n",
      "[Epoch: 18700] train loss: 0.2233, train acc: 0.9107, val loss: 0.2430, val acc: 0.9039  (best train acc: 0.9180, best val acc: 0.9147, best train loss: 0.2046  @ epoch 18570 )\n",
      "[Epoch: 18720] train loss: 0.2195, train acc: 0.9114, val loss: 0.2341, val acc: 0.9089  (best train acc: 0.9180, best val acc: 0.9147, best train loss: 0.2046  @ epoch 18570 )\n",
      "[Epoch: 18740] train loss: 0.2304, train acc: 0.9070, val loss: 0.2353, val acc: 0.9069  (best train acc: 0.9180, best val acc: 0.9147, best train loss: 0.2046  @ epoch 18570 )\n",
      "[Epoch: 18760] train loss: 0.2296, train acc: 0.9025, val loss: 0.2614, val acc: 0.9012  (best train acc: 0.9198, best val acc: 0.9147, best train loss: 0.2046  @ epoch 18570 )\n",
      "[Epoch: 18780] train loss: 0.2346, train acc: 0.9059, val loss: 0.2783, val acc: 0.8944  (best train acc: 0.9198, best val acc: 0.9147, best train loss: 0.2046  @ epoch 18570 )\n",
      "[Epoch: 18800] train loss: 0.2231, train acc: 0.9091, val loss: 0.2451, val acc: 0.9049  (best train acc: 0.9198, best val acc: 0.9147, best train loss: 0.2046  @ epoch 18570 )\n",
      "[Epoch: 18820] train loss: 0.2586, train acc: 0.8941, val loss: 0.2440, val acc: 0.9052  (best train acc: 0.9198, best val acc: 0.9147, best train loss: 0.2046  @ epoch 18570 )\n",
      "[Epoch: 18840] train loss: 0.2146, train acc: 0.9139, val loss: 0.2406, val acc: 0.9099  (best train acc: 0.9198, best val acc: 0.9153, best train loss: 0.2046  @ epoch 18570 )\n",
      "[Epoch: 18860] train loss: 0.2372, train acc: 0.9055, val loss: 0.2359, val acc: 0.9086  (best train acc: 0.9198, best val acc: 0.9153, best train loss: 0.2046  @ epoch 18570 )\n",
      "[Epoch: 18880] train loss: 0.2224, train acc: 0.9135, val loss: 0.2428, val acc: 0.9106  (best train acc: 0.9198, best val acc: 0.9153, best train loss: 0.2046  @ epoch 18570 )\n",
      "[Epoch: 18900] train loss: 0.2207, train acc: 0.9096, val loss: 0.2276, val acc: 0.9113  (best train acc: 0.9198, best val acc: 0.9153, best train loss: 0.2046  @ epoch 18570 )\n",
      "[Epoch: 18920] train loss: 0.2479, train acc: 0.8978, val loss: 0.2606, val acc: 0.9046  (best train acc: 0.9198, best val acc: 0.9153, best train loss: 0.2046  @ epoch 18570 )\n",
      "[Epoch: 18940] train loss: 0.2424, train acc: 0.9027, val loss: 0.2372, val acc: 0.9089  (best train acc: 0.9198, best val acc: 0.9153, best train loss: 0.2046  @ epoch 18570 )\n",
      "[Epoch: 18960] train loss: 0.2332, train acc: 0.9052, val loss: 0.2337, val acc: 0.9049  (best train acc: 0.9198, best val acc: 0.9153, best train loss: 0.2046  @ epoch 18570 )\n",
      "[Epoch: 18980] train loss: 0.2350, train acc: 0.9091, val loss: 0.2417, val acc: 0.9089  (best train acc: 0.9198, best val acc: 0.9153, best train loss: 0.2046  @ epoch 18570 )\n",
      "[Epoch: 19000] train loss: 0.2275, train acc: 0.9098, val loss: 0.2427, val acc: 0.9099  (best train acc: 0.9198, best val acc: 0.9153, best train loss: 0.2046  @ epoch 18570 )\n",
      "[Epoch: 19020] train loss: 0.2236, train acc: 0.9096, val loss: 0.2392, val acc: 0.9066  (best train acc: 0.9198, best val acc: 0.9153, best train loss: 0.2046  @ epoch 18570 )\n",
      "[Epoch: 19040] train loss: 0.2215, train acc: 0.9085, val loss: 0.2391, val acc: 0.9066  (best train acc: 0.9198, best val acc: 0.9153, best train loss: 0.2046  @ epoch 18570 )\n",
      "[Epoch: 19060] train loss: 0.2106, train acc: 0.9172, val loss: 0.2306, val acc: 0.9099  (best train acc: 0.9198, best val acc: 0.9153, best train loss: 0.2046  @ epoch 18570 )\n",
      "[Epoch: 19080] train loss: 0.2182, train acc: 0.9161, val loss: 0.2344, val acc: 0.9133  (best train acc: 0.9198, best val acc: 0.9153, best train loss: 0.2046  @ epoch 18570 )\n",
      "[Epoch: 19100] train loss: 0.2440, train acc: 0.8987, val loss: 0.2521, val acc: 0.9073  (best train acc: 0.9198, best val acc: 0.9153, best train loss: 0.2046  @ epoch 18570 )\n",
      "[Epoch: 19120] train loss: 0.2210, train acc: 0.9138, val loss: 0.2432, val acc: 0.9066  (best train acc: 0.9198, best val acc: 0.9153, best train loss: 0.2046  @ epoch 18570 )\n",
      "[Epoch: 19140] train loss: 0.2301, train acc: 0.9079, val loss: 0.2295, val acc: 0.9116  (best train acc: 0.9198, best val acc: 0.9153, best train loss: 0.2046  @ epoch 18570 )\n",
      "[Epoch: 19160] train loss: 0.2451, train acc: 0.9028, val loss: 0.2333, val acc: 0.9089  (best train acc: 0.9198, best val acc: 0.9153, best train loss: 0.2046  @ epoch 18570 )\n",
      "[Epoch: 19180] train loss: 0.2392, train acc: 0.9044, val loss: 0.2392, val acc: 0.9022  (best train acc: 0.9198, best val acc: 0.9153, best train loss: 0.2046  @ epoch 18570 )\n",
      "[Epoch: 19200] train loss: 0.2416, train acc: 0.9035, val loss: 0.2798, val acc: 0.8958  (best train acc: 0.9198, best val acc: 0.9153, best train loss: 0.2046  @ epoch 18570 )\n",
      "[Epoch: 19220] train loss: 0.2284, train acc: 0.9046, val loss: 0.2428, val acc: 0.9076  (best train acc: 0.9198, best val acc: 0.9153, best train loss: 0.2046  @ epoch 18570 )\n",
      "[Epoch: 19240] train loss: 0.2271, train acc: 0.9086, val loss: 0.2398, val acc: 0.9089  (best train acc: 0.9198, best val acc: 0.9153, best train loss: 0.2046  @ epoch 18570 )\n",
      "[Epoch: 19260] train loss: 0.2182, train acc: 0.9159, val loss: 0.2426, val acc: 0.9052  (best train acc: 0.9198, best val acc: 0.9153, best train loss: 0.2046  @ epoch 18570 )\n",
      "[Epoch: 19280] train loss: 0.2322, train acc: 0.9049, val loss: 0.2460, val acc: 0.9069  (best train acc: 0.9198, best val acc: 0.9153, best train loss: 0.2046  @ epoch 18570 )\n",
      "[Epoch: 19300] train loss: 0.2580, train acc: 0.8973, val loss: 0.2609, val acc: 0.8981  (best train acc: 0.9200, best val acc: 0.9153, best train loss: 0.2045  @ epoch 19289 )\n",
      "[Epoch: 19320] train loss: 0.2097, train acc: 0.9151, val loss: 0.2439, val acc: 0.9015  (best train acc: 0.9200, best val acc: 0.9153, best train loss: 0.2045  @ epoch 19289 )\n",
      "[Epoch: 19340] train loss: 0.2205, train acc: 0.9115, val loss: 0.2472, val acc: 0.9062  (best train acc: 0.9200, best val acc: 0.9153, best train loss: 0.2045  @ epoch 19289 )\n",
      "[Epoch: 19360] train loss: 0.2219, train acc: 0.9120, val loss: 0.2305, val acc: 0.9116  (best train acc: 0.9200, best val acc: 0.9153, best train loss: 0.2045  @ epoch 19289 )\n",
      "[Epoch: 19380] train loss: 0.2153, train acc: 0.9140, val loss: 0.2382, val acc: 0.9073  (best train acc: 0.9200, best val acc: 0.9153, best train loss: 0.2041  @ epoch 19361 )\n",
      "[Epoch: 19400] train loss: 0.2290, train acc: 0.9055, val loss: 0.2335, val acc: 0.9076  (best train acc: 0.9200, best val acc: 0.9153, best train loss: 0.2041  @ epoch 19361 )\n",
      "[Epoch: 19420] train loss: 0.2188, train acc: 0.9066, val loss: 0.2367, val acc: 0.9062  (best train acc: 0.9200, best val acc: 0.9153, best train loss: 0.2041  @ epoch 19361 )\n",
      "[Epoch: 19440] train loss: 0.2237, train acc: 0.9102, val loss: 0.2432, val acc: 0.9046  (best train acc: 0.9200, best val acc: 0.9153, best train loss: 0.2041  @ epoch 19361 )\n",
      "[Epoch: 19460] train loss: 0.2289, train acc: 0.9036, val loss: 0.2332, val acc: 0.9066  (best train acc: 0.9200, best val acc: 0.9153, best train loss: 0.2041  @ epoch 19361 )\n",
      "[Epoch: 19480] train loss: 0.2292, train acc: 0.9078, val loss: 0.2431, val acc: 0.9022  (best train acc: 0.9200, best val acc: 0.9153, best train loss: 0.2041  @ epoch 19361 )\n",
      "[Epoch: 19500] train loss: 0.2295, train acc: 0.9040, val loss: 0.2386, val acc: 0.9093  (best train acc: 0.9200, best val acc: 0.9153, best train loss: 0.2015  @ epoch 19494 )\n",
      "[Epoch: 19520] train loss: 0.2201, train acc: 0.9155, val loss: 0.2285, val acc: 0.9062  (best train acc: 0.9200, best val acc: 0.9153, best train loss: 0.2015  @ epoch 19494 )\n",
      "[Epoch: 19540] train loss: 0.2134, train acc: 0.9128, val loss: 0.2364, val acc: 0.9110  (best train acc: 0.9200, best val acc: 0.9153, best train loss: 0.2015  @ epoch 19494 )\n",
      "[Epoch: 19560] train loss: 0.2171, train acc: 0.9113, val loss: 0.2538, val acc: 0.9035  (best train acc: 0.9200, best val acc: 0.9153, best train loss: 0.2015  @ epoch 19494 )\n",
      "[Epoch: 19580] train loss: 0.2417, train acc: 0.9042, val loss: 0.2284, val acc: 0.9076  (best train acc: 0.9200, best val acc: 0.9153, best train loss: 0.2015  @ epoch 19494 )\n",
      "[Epoch: 19600] train loss: 0.2549, train acc: 0.8935, val loss: 0.2464, val acc: 0.9059  (best train acc: 0.9200, best val acc: 0.9153, best train loss: 0.2015  @ epoch 19494 )\n",
      "[Epoch: 19620] train loss: 0.2367, train acc: 0.9101, val loss: 0.2512, val acc: 0.9059  (best train acc: 0.9200, best val acc: 0.9153, best train loss: 0.2015  @ epoch 19494 )\n",
      "[Epoch: 19640] train loss: 0.2220, train acc: 0.9155, val loss: 0.2341, val acc: 0.9130  (best train acc: 0.9200, best val acc: 0.9153, best train loss: 0.2015  @ epoch 19494 )\n",
      "[Epoch: 19660] train loss: 0.2275, train acc: 0.9057, val loss: 0.2696, val acc: 0.9022  (best train acc: 0.9200, best val acc: 0.9153, best train loss: 0.2015  @ epoch 19494 )\n",
      "[Epoch: 19680] train loss: 0.2176, train acc: 0.9160, val loss: 0.2318, val acc: 0.9103  (best train acc: 0.9200, best val acc: 0.9153, best train loss: 0.2015  @ epoch 19494 )\n",
      "[Epoch: 19700] train loss: 0.2206, train acc: 0.9119, val loss: 0.2317, val acc: 0.9120  (best train acc: 0.9200, best val acc: 0.9153, best train loss: 0.2015  @ epoch 19494 )\n",
      "[Epoch: 19720] train loss: 0.2186, train acc: 0.9112, val loss: 0.2313, val acc: 0.9116  (best train acc: 0.9200, best val acc: 0.9153, best train loss: 0.2015  @ epoch 19494 )\n",
      "[Epoch: 19740] train loss: 0.2459, train acc: 0.9043, val loss: 0.2401, val acc: 0.9056  (best train acc: 0.9200, best val acc: 0.9153, best train loss: 0.2015  @ epoch 19494 )\n",
      "[Epoch: 19760] train loss: 0.2258, train acc: 0.9117, val loss: 0.2383, val acc: 0.9066  (best train acc: 0.9200, best val acc: 0.9153, best train loss: 0.2015  @ epoch 19494 )\n",
      "[Epoch: 19780] train loss: 0.2149, train acc: 0.9135, val loss: 0.2385, val acc: 0.9083  (best train acc: 0.9200, best val acc: 0.9153, best train loss: 0.2015  @ epoch 19494 )\n",
      "[Epoch: 19800] train loss: 0.2258, train acc: 0.9091, val loss: 0.2396, val acc: 0.9066  (best train acc: 0.9200, best val acc: 0.9153, best train loss: 0.2015  @ epoch 19494 )\n",
      "[Epoch: 19820] train loss: 0.2176, train acc: 0.9148, val loss: 0.2387, val acc: 0.9099  (best train acc: 0.9200, best val acc: 0.9153, best train loss: 0.2015  @ epoch 19494 )\n",
      "[Epoch: 19840] train loss: 0.2268, train acc: 0.9114, val loss: 0.2380, val acc: 0.9093  (best train acc: 0.9200, best val acc: 0.9153, best train loss: 0.2015  @ epoch 19494 )\n",
      "[Epoch: 19860] train loss: 0.2183, train acc: 0.9147, val loss: 0.2338, val acc: 0.9106  (best train acc: 0.9200, best val acc: 0.9153, best train loss: 0.2015  @ epoch 19494 )\n",
      "[Epoch: 19880] train loss: 0.2171, train acc: 0.9142, val loss: 0.2406, val acc: 0.9093  (best train acc: 0.9200, best val acc: 0.9153, best train loss: 0.2015  @ epoch 19494 )\n",
      "[Epoch: 19900] train loss: 0.2305, train acc: 0.9108, val loss: 0.2384, val acc: 0.9083  (best train acc: 0.9200, best val acc: 0.9153, best train loss: 0.2015  @ epoch 19494 )\n",
      "[Epoch: 19920] train loss: 0.2182, train acc: 0.9113, val loss: 0.2343, val acc: 0.9076  (best train acc: 0.9200, best val acc: 0.9153, best train loss: 0.2015  @ epoch 19494 )\n",
      "[Epoch: 19940] train loss: 0.2113, train acc: 0.9145, val loss: 0.2534, val acc: 0.9035  (best train acc: 0.9200, best val acc: 0.9153, best train loss: 0.2015  @ epoch 19494 )\n",
      "[Epoch: 19960] train loss: 0.2208, train acc: 0.9109, val loss: 0.2370, val acc: 0.9066  (best train acc: 0.9200, best val acc: 0.9153, best train loss: 0.2015  @ epoch 19494 )\n",
      "[Epoch: 19980] train loss: 0.2087, train acc: 0.9175, val loss: 0.2402, val acc: 0.9110  (best train acc: 0.9200, best val acc: 0.9153, best train loss: 0.2015  @ epoch 19494 )\n",
      "[Epoch: 20000] train loss: 0.2278, train acc: 0.9112, val loss: 0.2446, val acc: 0.9015  (best train acc: 0.9200, best val acc: 0.9153, best train loss: 0.2015  @ epoch 19494 )\n",
      "[Epoch: 20020] train loss: 0.2218, train acc: 0.9093, val loss: 0.2491, val acc: 0.9120  (best train acc: 0.9200, best val acc: 0.9153, best train loss: 0.2015  @ epoch 19494 )\n",
      "[Epoch: 20040] train loss: 0.2336, train acc: 0.9072, val loss: 0.2493, val acc: 0.9099  (best train acc: 0.9200, best val acc: 0.9153, best train loss: 0.2015  @ epoch 19494 )\n",
      "[Epoch: 20060] train loss: 0.2265, train acc: 0.9080, val loss: 0.2373, val acc: 0.9106  (best train acc: 0.9200, best val acc: 0.9153, best train loss: 0.2015  @ epoch 19494 )\n",
      "[Epoch: 20080] train loss: 0.2219, train acc: 0.9116, val loss: 0.2343, val acc: 0.9093  (best train acc: 0.9200, best val acc: 0.9153, best train loss: 0.2015  @ epoch 19494 )\n",
      "[Epoch: 20100] train loss: 0.2109, train acc: 0.9167, val loss: 0.2304, val acc: 0.9113  (best train acc: 0.9200, best val acc: 0.9153, best train loss: 0.2015  @ epoch 19494 )\n",
      "[Epoch: 20120] train loss: 0.2146, train acc: 0.9141, val loss: 0.2343, val acc: 0.9096  (best train acc: 0.9200, best val acc: 0.9153, best train loss: 0.2015  @ epoch 19494 )\n",
      "[Epoch: 20140] train loss: 0.2272, train acc: 0.9080, val loss: 0.2285, val acc: 0.9113  (best train acc: 0.9200, best val acc: 0.9153, best train loss: 0.2015  @ epoch 19494 )\n",
      "[Epoch: 20160] train loss: 0.2175, train acc: 0.9129, val loss: 0.2339, val acc: 0.9083  (best train acc: 0.9200, best val acc: 0.9153, best train loss: 0.2015  @ epoch 19494 )\n",
      "[Epoch: 20180] train loss: 0.2225, train acc: 0.9051, val loss: 0.2536, val acc: 0.9069  (best train acc: 0.9200, best val acc: 0.9153, best train loss: 0.2015  @ epoch 19494 )\n",
      "[Epoch: 20200] train loss: 0.2259, train acc: 0.9054, val loss: 0.2359, val acc: 0.9079  (best train acc: 0.9200, best val acc: 0.9153, best train loss: 0.2015  @ epoch 19494 )\n",
      "[Epoch: 20220] train loss: 0.2134, train acc: 0.9131, val loss: 0.2383, val acc: 0.9076  (best train acc: 0.9200, best val acc: 0.9153, best train loss: 0.2015  @ epoch 19494 )\n",
      "[Epoch: 20240] train loss: 0.2344, train acc: 0.9028, val loss: 0.2342, val acc: 0.9106  (best train acc: 0.9200, best val acc: 0.9153, best train loss: 0.2015  @ epoch 19494 )\n",
      "[Epoch: 20260] train loss: 0.2320, train acc: 0.9024, val loss: 0.2312, val acc: 0.9083  (best train acc: 0.9200, best val acc: 0.9153, best train loss: 0.2015  @ epoch 19494 )\n",
      "[Epoch: 20280] train loss: 0.2316, train acc: 0.9039, val loss: 0.2366, val acc: 0.9052  (best train acc: 0.9200, best val acc: 0.9153, best train loss: 0.2015  @ epoch 19494 )\n",
      "[Epoch: 20300] train loss: 0.2308, train acc: 0.9021, val loss: 0.2452, val acc: 0.9069  (best train acc: 0.9200, best val acc: 0.9153, best train loss: 0.2015  @ epoch 19494 )\n",
      "[Epoch: 20320] train loss: 0.2183, train acc: 0.9135, val loss: 0.2401, val acc: 0.9083  (best train acc: 0.9200, best val acc: 0.9153, best train loss: 0.2015  @ epoch 19494 )\n",
      "[Epoch: 20340] train loss: 0.2272, train acc: 0.9062, val loss: 0.2281, val acc: 0.9086  (best train acc: 0.9200, best val acc: 0.9153, best train loss: 0.2015  @ epoch 19494 )\n",
      "[Epoch: 20360] train loss: 0.2245, train acc: 0.9091, val loss: 0.2477, val acc: 0.9083  (best train acc: 0.9200, best val acc: 0.9153, best train loss: 0.2015  @ epoch 19494 )\n",
      "[Epoch: 20380] train loss: 0.2245, train acc: 0.9087, val loss: 0.2334, val acc: 0.9086  (best train acc: 0.9200, best val acc: 0.9153, best train loss: 0.2015  @ epoch 19494 )\n",
      "[Epoch: 20400] train loss: 0.2169, train acc: 0.9091, val loss: 0.2335, val acc: 0.9106  (best train acc: 0.9200, best val acc: 0.9153, best train loss: 0.2015  @ epoch 19494 )\n",
      "[Epoch: 20420] train loss: 0.2173, train acc: 0.9107, val loss: 0.2372, val acc: 0.9066  (best train acc: 0.9200, best val acc: 0.9153, best train loss: 0.2015  @ epoch 19494 )\n",
      "[Epoch: 20440] train loss: 0.2283, train acc: 0.9070, val loss: 0.2346, val acc: 0.9106  (best train acc: 0.9200, best val acc: 0.9153, best train loss: 0.2015  @ epoch 19494 )\n",
      "[Epoch: 20460] train loss: 0.2209, train acc: 0.9104, val loss: 0.2513, val acc: 0.9052  (best train acc: 0.9200, best val acc: 0.9153, best train loss: 0.2015  @ epoch 19494 )\n",
      "[Epoch: 20480] train loss: 0.2323, train acc: 0.9049, val loss: 0.2336, val acc: 0.9066  (best train acc: 0.9200, best val acc: 0.9153, best train loss: 0.2015  @ epoch 19494 )\n",
      "[Epoch: 20500] train loss: 0.2115, train acc: 0.9156, val loss: 0.2644, val acc: 0.8978  (best train acc: 0.9200, best val acc: 0.9153, best train loss: 0.2015  @ epoch 19494 )\n",
      "[Epoch: 20520] train loss: 0.2164, train acc: 0.9146, val loss: 0.2361, val acc: 0.9120  (best train acc: 0.9200, best val acc: 0.9153, best train loss: 0.2015  @ epoch 19494 )\n",
      "[Epoch: 20540] train loss: 0.2173, train acc: 0.9068, val loss: 0.2342, val acc: 0.9096  (best train acc: 0.9200, best val acc: 0.9153, best train loss: 0.2015  @ epoch 19494 )\n",
      "[Epoch: 20560] train loss: 0.2118, train acc: 0.9156, val loss: 0.2346, val acc: 0.9113  (best train acc: 0.9212, best val acc: 0.9153, best train loss: 0.2009  @ epoch 20557 )\n",
      "[Epoch: 20580] train loss: 0.2498, train acc: 0.8961, val loss: 0.2438, val acc: 0.9066  (best train acc: 0.9212, best val acc: 0.9153, best train loss: 0.2009  @ epoch 20557 )\n",
      "[Epoch: 20600] train loss: 0.2168, train acc: 0.9143, val loss: 0.2378, val acc: 0.9049  (best train acc: 0.9212, best val acc: 0.9153, best train loss: 0.2009  @ epoch 20557 )\n",
      "[Epoch: 20620] train loss: 0.2411, train acc: 0.8987, val loss: 0.2516, val acc: 0.9035  (best train acc: 0.9212, best val acc: 0.9153, best train loss: 0.2009  @ epoch 20557 )\n",
      "[Epoch: 20640] train loss: 0.2146, train acc: 0.9138, val loss: 0.2378, val acc: 0.9073  (best train acc: 0.9212, best val acc: 0.9153, best train loss: 0.2009  @ epoch 20557 )\n",
      "[Epoch: 20660] train loss: 0.2154, train acc: 0.9160, val loss: 0.2354, val acc: 0.9056  (best train acc: 0.9212, best val acc: 0.9153, best train loss: 0.2009  @ epoch 20557 )\n",
      "[Epoch: 20680] train loss: 0.2133, train acc: 0.9112, val loss: 0.2344, val acc: 0.9019  (best train acc: 0.9212, best val acc: 0.9153, best train loss: 0.2009  @ epoch 20557 )\n",
      "[Epoch: 20700] train loss: 0.2137, train acc: 0.9132, val loss: 0.2387, val acc: 0.9103  (best train acc: 0.9212, best val acc: 0.9153, best train loss: 0.2009  @ epoch 20557 )\n",
      "[Epoch: 20720] train loss: 0.2471, train acc: 0.9001, val loss: 0.2489, val acc: 0.9073  (best train acc: 0.9212, best val acc: 0.9153, best train loss: 0.2009  @ epoch 20557 )\n",
      "[Epoch: 20740] train loss: 0.2201, train acc: 0.9110, val loss: 0.2391, val acc: 0.9093  (best train acc: 0.9212, best val acc: 0.9153, best train loss: 0.2009  @ epoch 20557 )\n",
      "[Epoch: 20760] train loss: 0.2203, train acc: 0.9156, val loss: 0.2418, val acc: 0.9056  (best train acc: 0.9212, best val acc: 0.9153, best train loss: 0.2009  @ epoch 20557 )\n",
      "[Epoch: 20780] train loss: 0.2154, train acc: 0.9131, val loss: 0.2459, val acc: 0.9076  (best train acc: 0.9212, best val acc: 0.9153, best train loss: 0.2009  @ epoch 20557 )\n",
      "[Epoch: 20800] train loss: 0.2048, train acc: 0.9191, val loss: 0.2388, val acc: 0.9025  (best train acc: 0.9212, best val acc: 0.9153, best train loss: 0.2009  @ epoch 20557 )\n",
      "[Epoch: 20820] train loss: 0.2260, train acc: 0.9021, val loss: 0.2376, val acc: 0.9103  (best train acc: 0.9212, best val acc: 0.9153, best train loss: 0.2009  @ epoch 20557 )\n",
      "[Epoch: 20840] train loss: 0.2189, train acc: 0.9079, val loss: 0.2441, val acc: 0.9056  (best train acc: 0.9212, best val acc: 0.9153, best train loss: 0.2009  @ epoch 20557 )\n",
      "[Epoch: 20860] train loss: 0.2225, train acc: 0.9161, val loss: 0.2373, val acc: 0.9073  (best train acc: 0.9212, best val acc: 0.9153, best train loss: 0.2009  @ epoch 20557 )\n",
      "[Epoch: 20880] train loss: 0.2320, train acc: 0.9055, val loss: 0.2407, val acc: 0.9069  (best train acc: 0.9212, best val acc: 0.9153, best train loss: 0.2009  @ epoch 20557 )\n",
      "[Epoch: 20900] train loss: 0.2276, train acc: 0.9091, val loss: 0.2401, val acc: 0.9049  (best train acc: 0.9212, best val acc: 0.9153, best train loss: 0.2009  @ epoch 20557 )\n",
      "[Epoch: 20920] train loss: 0.2274, train acc: 0.9054, val loss: 0.2392, val acc: 0.9099  (best train acc: 0.9212, best val acc: 0.9153, best train loss: 0.2009  @ epoch 20557 )\n",
      "[Epoch: 20940] train loss: 0.2216, train acc: 0.9087, val loss: 0.2318, val acc: 0.9079  (best train acc: 0.9212, best val acc: 0.9153, best train loss: 0.2009  @ epoch 20557 )\n",
      "[Epoch: 20960] train loss: 0.2208, train acc: 0.9096, val loss: 0.2437, val acc: 0.9073  (best train acc: 0.9216, best val acc: 0.9153, best train loss: 0.2008  @ epoch 20948 )\n",
      "[Epoch: 20980] train loss: 0.2023, train acc: 0.9172, val loss: 0.2348, val acc: 0.9052  (best train acc: 0.9216, best val acc: 0.9153, best train loss: 0.2008  @ epoch 20948 )\n",
      "[Epoch: 21000] train loss: 0.2252, train acc: 0.9094, val loss: 0.2480, val acc: 0.9089  (best train acc: 0.9216, best val acc: 0.9153, best train loss: 0.2008  @ epoch 20948 )\n",
      "[Epoch: 21020] train loss: 0.2304, train acc: 0.9038, val loss: 0.2289, val acc: 0.9110  (best train acc: 0.9216, best val acc: 0.9153, best train loss: 0.2008  @ epoch 20948 )\n",
      "[Epoch: 21040] train loss: 0.2226, train acc: 0.9086, val loss: 0.2399, val acc: 0.9099  (best train acc: 0.9227, best val acc: 0.9153, best train loss: 0.2008  @ epoch 20948 )\n",
      "[Epoch: 21060] train loss: 0.2095, train acc: 0.9176, val loss: 0.2388, val acc: 0.9113  (best train acc: 0.9227, best val acc: 0.9153, best train loss: 0.2008  @ epoch 20948 )\n",
      "[Epoch: 21080] train loss: 0.2374, train acc: 0.9020, val loss: 0.2557, val acc: 0.9059  (best train acc: 0.9227, best val acc: 0.9153, best train loss: 0.1995  @ epoch 21068 )\n",
      "[Epoch: 21100] train loss: 0.2355, train acc: 0.9072, val loss: 0.2361, val acc: 0.9073  (best train acc: 0.9227, best val acc: 0.9153, best train loss: 0.1995  @ epoch 21068 )\n",
      "[Epoch: 21120] train loss: 0.2169, train acc: 0.9133, val loss: 0.2607, val acc: 0.9012  (best train acc: 0.9227, best val acc: 0.9153, best train loss: 0.1995  @ epoch 21068 )\n",
      "[Epoch: 21140] train loss: 0.2264, train acc: 0.9084, val loss: 0.2525, val acc: 0.9056  (best train acc: 0.9227, best val acc: 0.9153, best train loss: 0.1995  @ epoch 21068 )\n",
      "[Epoch: 21160] train loss: 0.2189, train acc: 0.9112, val loss: 0.2376, val acc: 0.9076  (best train acc: 0.9227, best val acc: 0.9153, best train loss: 0.1995  @ epoch 21068 )\n",
      "[Epoch: 21180] train loss: 0.2792, train acc: 0.8858, val loss: 0.2395, val acc: 0.9056  (best train acc: 0.9227, best val acc: 0.9153, best train loss: 0.1995  @ epoch 21068 )\n",
      "[Epoch: 21200] train loss: 0.2216, train acc: 0.9114, val loss: 0.2433, val acc: 0.9083  (best train acc: 0.9227, best val acc: 0.9153, best train loss: 0.1995  @ epoch 21068 )\n",
      "[Epoch: 21220] train loss: 0.2101, train acc: 0.9166, val loss: 0.2323, val acc: 0.9110  (best train acc: 0.9227, best val acc: 0.9153, best train loss: 0.1995  @ epoch 21068 )\n",
      "[Epoch: 21240] train loss: 0.2267, train acc: 0.9083, val loss: 0.2402, val acc: 0.9039  (best train acc: 0.9227, best val acc: 0.9153, best train loss: 0.1995  @ epoch 21068 )\n",
      "[Epoch: 21260] train loss: 0.2499, train acc: 0.8897, val loss: 0.2344, val acc: 0.9062  (best train acc: 0.9227, best val acc: 0.9153, best train loss: 0.1995  @ epoch 21068 )\n",
      "[Epoch: 21280] train loss: 0.2260, train acc: 0.9091, val loss: 0.2433, val acc: 0.9076  (best train acc: 0.9227, best val acc: 0.9153, best train loss: 0.1995  @ epoch 21068 )\n",
      "[Epoch: 21300] train loss: 0.2082, train acc: 0.9151, val loss: 0.2410, val acc: 0.9066  (best train acc: 0.9227, best val acc: 0.9153, best train loss: 0.1995  @ epoch 21068 )\n",
      "[Epoch: 21320] train loss: 0.2274, train acc: 0.9067, val loss: 0.2532, val acc: 0.9042  (best train acc: 0.9227, best val acc: 0.9153, best train loss: 0.1995  @ epoch 21068 )\n",
      "[Epoch: 21340] train loss: 0.2119, train acc: 0.9148, val loss: 0.2347, val acc: 0.9066  (best train acc: 0.9227, best val acc: 0.9153, best train loss: 0.1995  @ epoch 21068 )\n",
      "[Epoch: 21360] train loss: 0.2547, train acc: 0.8992, val loss: 0.2366, val acc: 0.9093  (best train acc: 0.9227, best val acc: 0.9153, best train loss: 0.1995  @ epoch 21068 )\n",
      "[Epoch: 21380] train loss: 0.2494, train acc: 0.8973, val loss: 0.2516, val acc: 0.9086  (best train acc: 0.9227, best val acc: 0.9153, best train loss: 0.1995  @ epoch 21068 )\n",
      "[Epoch: 21400] train loss: 0.2292, train acc: 0.9071, val loss: 0.2428, val acc: 0.9096  (best train acc: 0.9227, best val acc: 0.9153, best train loss: 0.1995  @ epoch 21068 )\n",
      "[Epoch: 21420] train loss: 0.2154, train acc: 0.9056, val loss: 0.2354, val acc: 0.9110  (best train acc: 0.9227, best val acc: 0.9153, best train loss: 0.1995  @ epoch 21068 )\n",
      "[Epoch: 21440] train loss: 0.2280, train acc: 0.9117, val loss: 0.2408, val acc: 0.9059  (best train acc: 0.9227, best val acc: 0.9153, best train loss: 0.1995  @ epoch 21068 )\n",
      "[Epoch: 21460] train loss: 0.2287, train acc: 0.9067, val loss: 0.2386, val acc: 0.9083  (best train acc: 0.9227, best val acc: 0.9153, best train loss: 0.1995  @ epoch 21068 )\n",
      "[Epoch: 21480] train loss: 0.2073, train acc: 0.9161, val loss: 0.2377, val acc: 0.9093  (best train acc: 0.9227, best val acc: 0.9153, best train loss: 0.1995  @ epoch 21068 )\n",
      "[Epoch: 21500] train loss: 0.2372, train acc: 0.9025, val loss: 0.2491, val acc: 0.9093  (best train acc: 0.9227, best val acc: 0.9153, best train loss: 0.1995  @ epoch 21482 )\n",
      "[Epoch: 21520] train loss: 0.2255, train acc: 0.9106, val loss: 0.2401, val acc: 0.9069  (best train acc: 0.9227, best val acc: 0.9153, best train loss: 0.1995  @ epoch 21482 )\n",
      "[Epoch: 21540] train loss: 0.2122, train acc: 0.9129, val loss: 0.2439, val acc: 0.9076  (best train acc: 0.9227, best val acc: 0.9153, best train loss: 0.1995  @ epoch 21482 )\n",
      "[Epoch: 21560] train loss: 0.2334, train acc: 0.9078, val loss: 0.2452, val acc: 0.9110  (best train acc: 0.9227, best val acc: 0.9153, best train loss: 0.1995  @ epoch 21482 )\n",
      "[Epoch: 21580] train loss: 0.2456, train acc: 0.8993, val loss: 0.2372, val acc: 0.9106  (best train acc: 0.9227, best val acc: 0.9153, best train loss: 0.1995  @ epoch 21482 )\n",
      "[Epoch: 21600] train loss: 0.2268, train acc: 0.9079, val loss: 0.2436, val acc: 0.9096  (best train acc: 0.9227, best val acc: 0.9153, best train loss: 0.1995  @ epoch 21482 )\n",
      "[Epoch: 21620] train loss: 0.2175, train acc: 0.9088, val loss: 0.2409, val acc: 0.9059  (best train acc: 0.9227, best val acc: 0.9153, best train loss: 0.1995  @ epoch 21482 )\n",
      "[Epoch: 21640] train loss: 0.2417, train acc: 0.9007, val loss: 0.2376, val acc: 0.9130  (best train acc: 0.9227, best val acc: 0.9153, best train loss: 0.1995  @ epoch 21482 )\n",
      "[Epoch: 21660] train loss: 0.2079, train acc: 0.9134, val loss: 0.2495, val acc: 0.9049  (best train acc: 0.9227, best val acc: 0.9153, best train loss: 0.1995  @ epoch 21482 )\n",
      "[Epoch: 21680] train loss: 0.2121, train acc: 0.9131, val loss: 0.2437, val acc: 0.9046  (best train acc: 0.9227, best val acc: 0.9153, best train loss: 0.1995  @ epoch 21482 )\n",
      "[Epoch: 21700] train loss: 0.2100, train acc: 0.9167, val loss: 0.2401, val acc: 0.9079  (best train acc: 0.9227, best val acc: 0.9153, best train loss: 0.1995  @ epoch 21482 )\n",
      "[Epoch: 21720] train loss: 0.2259, train acc: 0.9084, val loss: 0.2621, val acc: 0.9052  (best train acc: 0.9227, best val acc: 0.9153, best train loss: 0.1995  @ epoch 21482 )\n",
      "[Epoch: 21740] train loss: 0.2179, train acc: 0.9085, val loss: 0.2411, val acc: 0.9059  (best train acc: 0.9227, best val acc: 0.9153, best train loss: 0.1995  @ epoch 21482 )\n",
      "[Epoch: 21760] train loss: 0.2150, train acc: 0.9133, val loss: 0.2361, val acc: 0.9035  (best train acc: 0.9227, best val acc: 0.9153, best train loss: 0.1995  @ epoch 21482 )\n",
      "[Epoch: 21780] train loss: 0.2322, train acc: 0.9070, val loss: 0.2396, val acc: 0.9076  (best train acc: 0.9227, best val acc: 0.9153, best train loss: 0.1995  @ epoch 21482 )\n",
      "[Epoch: 21800] train loss: 0.2222, train acc: 0.9123, val loss: 0.2609, val acc: 0.9029  (best train acc: 0.9227, best val acc: 0.9153, best train loss: 0.1995  @ epoch 21482 )\n",
      "[Epoch: 21820] train loss: 0.2063, train acc: 0.9161, val loss: 0.2417, val acc: 0.9039  (best train acc: 0.9227, best val acc: 0.9153, best train loss: 0.1995  @ epoch 21482 )\n",
      "[Epoch: 21840] train loss: 0.2115, train acc: 0.9167, val loss: 0.2404, val acc: 0.9073  (best train acc: 0.9227, best val acc: 0.9153, best train loss: 0.1995  @ epoch 21482 )\n",
      "[Epoch: 21860] train loss: 0.2119, train acc: 0.9134, val loss: 0.2309, val acc: 0.9079  (best train acc: 0.9227, best val acc: 0.9153, best train loss: 0.1995  @ epoch 21482 )\n",
      "[Epoch: 21880] train loss: 0.2100, train acc: 0.9173, val loss: 0.2453, val acc: 0.9086  (best train acc: 0.9227, best val acc: 0.9153, best train loss: 0.1995  @ epoch 21482 )\n",
      "[Epoch: 21900] train loss: 0.2200, train acc: 0.9080, val loss: 0.2367, val acc: 0.9096  (best train acc: 0.9227, best val acc: 0.9153, best train loss: 0.1995  @ epoch 21482 )\n",
      "[Epoch: 21920] train loss: 0.2062, train acc: 0.9158, val loss: 0.2404, val acc: 0.9062  (best train acc: 0.9227, best val acc: 0.9153, best train loss: 0.1995  @ epoch 21482 )\n",
      "[Epoch: 21940] train loss: 0.2204, train acc: 0.9095, val loss: 0.2427, val acc: 0.9039  (best train acc: 0.9227, best val acc: 0.9153, best train loss: 0.1995  @ epoch 21482 )\n",
      "[Epoch: 21960] train loss: 0.2154, train acc: 0.9164, val loss: 0.2416, val acc: 0.9083  (best train acc: 0.9227, best val acc: 0.9153, best train loss: 0.1995  @ epoch 21482 )\n",
      "[Epoch: 21980] train loss: 0.2206, train acc: 0.9067, val loss: 0.2357, val acc: 0.9086  (best train acc: 0.9227, best val acc: 0.9153, best train loss: 0.1995  @ epoch 21482 )\n",
      "[Epoch: 22000] train loss: 0.2578, train acc: 0.9004, val loss: 0.2467, val acc: 0.9052  (best train acc: 0.9227, best val acc: 0.9153, best train loss: 0.1995  @ epoch 21482 )\n",
      "[Epoch: 22020] train loss: 0.2184, train acc: 0.9113, val loss: 0.2355, val acc: 0.9123  (best train acc: 0.9227, best val acc: 0.9153, best train loss: 0.1995  @ epoch 21482 )\n",
      "[Epoch: 22040] train loss: 0.2241, train acc: 0.9125, val loss: 0.2465, val acc: 0.9073  (best train acc: 0.9227, best val acc: 0.9153, best train loss: 0.1995  @ epoch 21482 )\n",
      "[Epoch: 22060] train loss: 0.2302, train acc: 0.9049, val loss: 0.2490, val acc: 0.9052  (best train acc: 0.9227, best val acc: 0.9153, best train loss: 0.1995  @ epoch 21482 )\n",
      "[Epoch: 22080] train loss: 0.2088, train acc: 0.9114, val loss: 0.2421, val acc: 0.9103  (best train acc: 0.9227, best val acc: 0.9153, best train loss: 0.1995  @ epoch 21482 )\n",
      "[Epoch: 22100] train loss: 0.2294, train acc: 0.9046, val loss: 0.2494, val acc: 0.9073  (best train acc: 0.9227, best val acc: 0.9153, best train loss: 0.1995  @ epoch 21482 )\n",
      "[Epoch: 22120] train loss: 0.2095, train acc: 0.9115, val loss: 0.2445, val acc: 0.9106  (best train acc: 0.9227, best val acc: 0.9153, best train loss: 0.1995  @ epoch 21482 )\n",
      "[Epoch: 22140] train loss: 0.2365, train acc: 0.9028, val loss: 0.2426, val acc: 0.9066  (best train acc: 0.9227, best val acc: 0.9153, best train loss: 0.1995  @ epoch 21482 )\n",
      "[Epoch: 22160] train loss: 0.2533, train acc: 0.8968, val loss: 0.2487, val acc: 0.9073  (best train acc: 0.9227, best val acc: 0.9153, best train loss: 0.1995  @ epoch 21482 )\n",
      "[Epoch: 22180] train loss: 0.2439, train acc: 0.9041, val loss: 0.2472, val acc: 0.9046  (best train acc: 0.9227, best val acc: 0.9153, best train loss: 0.1995  @ epoch 21482 )\n",
      "[Epoch: 22200] train loss: 0.2521, train acc: 0.8956, val loss: 0.2565, val acc: 0.9083  (best train acc: 0.9227, best val acc: 0.9153, best train loss: 0.1995  @ epoch 21482 )\n",
      "[Epoch: 22220] train loss: 0.2267, train acc: 0.9058, val loss: 0.2324, val acc: 0.9046  (best train acc: 0.9227, best val acc: 0.9153, best train loss: 0.1995  @ epoch 21482 )\n",
      "[Epoch: 22240] train loss: 0.2181, train acc: 0.9109, val loss: 0.2347, val acc: 0.9106  (best train acc: 0.9227, best val acc: 0.9153, best train loss: 0.1995  @ epoch 21482 )\n",
      "[Epoch: 22260] train loss: 0.2261, train acc: 0.9054, val loss: 0.2451, val acc: 0.9062  (best train acc: 0.9227, best val acc: 0.9153, best train loss: 0.1995  @ epoch 21482 )\n",
      "[Epoch: 22280] train loss: 0.2152, train acc: 0.9132, val loss: 0.2398, val acc: 0.9076  (best train acc: 0.9227, best val acc: 0.9153, best train loss: 0.1995  @ epoch 21482 )\n",
      "[Epoch: 22300] train loss: 0.2422, train acc: 0.8947, val loss: 0.2439, val acc: 0.9042  (best train acc: 0.9227, best val acc: 0.9153, best train loss: 0.1995  @ epoch 21482 )\n",
      "[Epoch: 22320] train loss: 0.2156, train acc: 0.9111, val loss: 0.2411, val acc: 0.9113  (best train acc: 0.9227, best val acc: 0.9153, best train loss: 0.1995  @ epoch 21482 )\n",
      "[Epoch: 22340] train loss: 0.2139, train acc: 0.9178, val loss: 0.2402, val acc: 0.9106  (best train acc: 0.9227, best val acc: 0.9153, best train loss: 0.1995  @ epoch 21482 )\n",
      "[Epoch: 22360] train loss: 0.2173, train acc: 0.9105, val loss: 0.2494, val acc: 0.9083  (best train acc: 0.9227, best val acc: 0.9153, best train loss: 0.1995  @ epoch 21482 )\n",
      "[Epoch: 22380] train loss: 0.2146, train acc: 0.9132, val loss: 0.2465, val acc: 0.9099  (best train acc: 0.9227, best val acc: 0.9153, best train loss: 0.1995  @ epoch 21482 )\n",
      "[Epoch: 22400] train loss: 0.2243, train acc: 0.9045, val loss: 0.2369, val acc: 0.9096  (best train acc: 0.9227, best val acc: 0.9153, best train loss: 0.1995  @ epoch 21482 )\n",
      "[Epoch: 22420] train loss: 0.2239, train acc: 0.9057, val loss: 0.2474, val acc: 0.9089  (best train acc: 0.9227, best val acc: 0.9153, best train loss: 0.1995  @ epoch 21482 )\n",
      "[Epoch: 22440] train loss: 0.2320, train acc: 0.9067, val loss: 0.2427, val acc: 0.9083  (best train acc: 0.9227, best val acc: 0.9153, best train loss: 0.1995  @ epoch 21482 )\n",
      "[Epoch: 22460] train loss: 0.2129, train acc: 0.9156, val loss: 0.2416, val acc: 0.9103  (best train acc: 0.9227, best val acc: 0.9153, best train loss: 0.1995  @ epoch 21482 )\n",
      "[Epoch: 22480] train loss: 0.2137, train acc: 0.9150, val loss: 0.2388, val acc: 0.9035  (best train acc: 0.9227, best val acc: 0.9153, best train loss: 0.1995  @ epoch 21482 )\n",
      "[Epoch: 22500] train loss: 0.2291, train acc: 0.9040, val loss: 0.2491, val acc: 0.9056  (best train acc: 0.9227, best val acc: 0.9153, best train loss: 0.1995  @ epoch 21482 )\n",
      "[Epoch: 22520] train loss: 0.2174, train acc: 0.9145, val loss: 0.2452, val acc: 0.9130  (best train acc: 0.9227, best val acc: 0.9153, best train loss: 0.1995  @ epoch 21482 )\n",
      "[Epoch: 22540] train loss: 0.2226, train acc: 0.9096, val loss: 0.2455, val acc: 0.9099  (best train acc: 0.9227, best val acc: 0.9153, best train loss: 0.1995  @ epoch 21482 )\n",
      "[Epoch: 22560] train loss: 0.2197, train acc: 0.9101, val loss: 0.2436, val acc: 0.9086  (best train acc: 0.9227, best val acc: 0.9153, best train loss: 0.1995  @ epoch 21482 )\n",
      "[Epoch: 22580] train loss: 0.2280, train acc: 0.9042, val loss: 0.2428, val acc: 0.9083  (best train acc: 0.9227, best val acc: 0.9153, best train loss: 0.1995  @ epoch 21482 )\n",
      "[Epoch: 22600] train loss: 0.2119, train acc: 0.9133, val loss: 0.2388, val acc: 0.9099  (best train acc: 0.9227, best val acc: 0.9153, best train loss: 0.1995  @ epoch 21482 )\n",
      "[Epoch: 22620] train loss: 0.2375, train acc: 0.8992, val loss: 0.2502, val acc: 0.9083  (best train acc: 0.9227, best val acc: 0.9153, best train loss: 0.1995  @ epoch 21482 )\n",
      "[Epoch: 22640] train loss: 0.2040, train acc: 0.9153, val loss: 0.2467, val acc: 0.9079  (best train acc: 0.9227, best val acc: 0.9153, best train loss: 0.1995  @ epoch 21482 )\n",
      "[Epoch: 22660] train loss: 0.2187, train acc: 0.9130, val loss: 0.2455, val acc: 0.9069  (best train acc: 0.9227, best val acc: 0.9153, best train loss: 0.1995  @ epoch 21482 )\n",
      "[Epoch: 22680] train loss: 0.2131, train acc: 0.9165, val loss: 0.2525, val acc: 0.9025  (best train acc: 0.9227, best val acc: 0.9153, best train loss: 0.1995  @ epoch 21482 )\n",
      "[Epoch: 22700] train loss: 0.2194, train acc: 0.9092, val loss: 0.2386, val acc: 0.9099  (best train acc: 0.9227, best val acc: 0.9153, best train loss: 0.1995  @ epoch 21482 )\n",
      "[Epoch: 22720] train loss: 0.2179, train acc: 0.9058, val loss: 0.2383, val acc: 0.9110  (best train acc: 0.9227, best val acc: 0.9153, best train loss: 0.1995  @ epoch 21482 )\n",
      "[Epoch: 22740] train loss: 0.2018, train acc: 0.9180, val loss: 0.2423, val acc: 0.9123  (best train acc: 0.9227, best val acc: 0.9153, best train loss: 0.1973  @ epoch 22738 )\n",
      "[Epoch: 22760] train loss: 0.2074, train acc: 0.9138, val loss: 0.2437, val acc: 0.9052  (best train acc: 0.9227, best val acc: 0.9153, best train loss: 0.1973  @ epoch 22738 )\n",
      "[Epoch: 22780] train loss: 0.2106, train acc: 0.9130, val loss: 0.2431, val acc: 0.9069  (best train acc: 0.9227, best val acc: 0.9153, best train loss: 0.1973  @ epoch 22738 )\n",
      "[Epoch: 22800] train loss: 0.2263, train acc: 0.9070, val loss: 0.2404, val acc: 0.9042  (best train acc: 0.9227, best val acc: 0.9153, best train loss: 0.1973  @ epoch 22738 )\n",
      "[Epoch: 22820] train loss: 0.2275, train acc: 0.9139, val loss: 0.2589, val acc: 0.9083  (best train acc: 0.9227, best val acc: 0.9153, best train loss: 0.1973  @ epoch 22738 )\n",
      "[Epoch: 22840] train loss: 0.2346, train acc: 0.9053, val loss: 0.2401, val acc: 0.9120  (best train acc: 0.9227, best val acc: 0.9153, best train loss: 0.1973  @ epoch 22738 )\n",
      "[Epoch: 22860] train loss: 0.2047, train acc: 0.9130, val loss: 0.2415, val acc: 0.9123  (best train acc: 0.9227, best val acc: 0.9153, best train loss: 0.1973  @ epoch 22738 )\n",
      "[Epoch: 22880] train loss: 0.2194, train acc: 0.9111, val loss: 0.2301, val acc: 0.9103  (best train acc: 0.9227, best val acc: 0.9153, best train loss: 0.1973  @ epoch 22738 )\n",
      "[Epoch: 22900] train loss: 0.2074, train acc: 0.9164, val loss: 0.2315, val acc: 0.9079  (best train acc: 0.9227, best val acc: 0.9153, best train loss: 0.1973  @ epoch 22738 )\n",
      "[Epoch: 22920] train loss: 0.2206, train acc: 0.9080, val loss: 0.2455, val acc: 0.9066  (best train acc: 0.9227, best val acc: 0.9153, best train loss: 0.1973  @ epoch 22738 )\n",
      "[Epoch: 22940] train loss: 0.2206, train acc: 0.9088, val loss: 0.2428, val acc: 0.9086  (best train acc: 0.9227, best val acc: 0.9153, best train loss: 0.1973  @ epoch 22738 )\n",
      "[Epoch: 22960] train loss: 0.2235, train acc: 0.9103, val loss: 0.2428, val acc: 0.9110  (best train acc: 0.9227, best val acc: 0.9153, best train loss: 0.1973  @ epoch 22738 )\n",
      "[Epoch: 22980] train loss: 0.2112, train acc: 0.9115, val loss: 0.2450, val acc: 0.9059  (best train acc: 0.9227, best val acc: 0.9153, best train loss: 0.1973  @ epoch 22738 )\n",
      "[Epoch: 23000] train loss: 0.2448, train acc: 0.9017, val loss: 0.2412, val acc: 0.9066  (best train acc: 0.9227, best val acc: 0.9153, best train loss: 0.1973  @ epoch 22738 )\n",
      "[Epoch: 23020] train loss: 0.2110, train acc: 0.9148, val loss: 0.2334, val acc: 0.9076  (best train acc: 0.9227, best val acc: 0.9153, best train loss: 0.1973  @ epoch 22738 )\n",
      "[Epoch: 23040] train loss: 0.2082, train acc: 0.9193, val loss: 0.2421, val acc: 0.9106  (best train acc: 0.9227, best val acc: 0.9153, best train loss: 0.1973  @ epoch 22738 )\n",
      "[Epoch: 23060] train loss: 0.2301, train acc: 0.9092, val loss: 0.2795, val acc: 0.9025  (best train acc: 0.9227, best val acc: 0.9153, best train loss: 0.1973  @ epoch 22738 )\n",
      "[Epoch: 23080] train loss: 0.2369, train acc: 0.9046, val loss: 0.2446, val acc: 0.9076  (best train acc: 0.9227, best val acc: 0.9153, best train loss: 0.1973  @ epoch 22738 )\n",
      "[Epoch: 23100] train loss: 0.2202, train acc: 0.9078, val loss: 0.2503, val acc: 0.9049  (best train acc: 0.9227, best val acc: 0.9153, best train loss: 0.1973  @ epoch 22738 )\n",
      "[Epoch: 23120] train loss: 0.2103, train acc: 0.9158, val loss: 0.2539, val acc: 0.9093  (best train acc: 0.9227, best val acc: 0.9153, best train loss: 0.1973  @ epoch 22738 )\n",
      "[Epoch: 23140] train loss: 0.2130, train acc: 0.9126, val loss: 0.2436, val acc: 0.9093  (best train acc: 0.9227, best val acc: 0.9153, best train loss: 0.1973  @ epoch 22738 )\n",
      "[Epoch: 23160] train loss: 0.2246, train acc: 0.9057, val loss: 0.2582, val acc: 0.9079  (best train acc: 0.9227, best val acc: 0.9153, best train loss: 0.1973  @ epoch 22738 )\n",
      "[Epoch: 23180] train loss: 0.2149, train acc: 0.9108, val loss: 0.2490, val acc: 0.9116  (best train acc: 0.9227, best val acc: 0.9153, best train loss: 0.1973  @ epoch 22738 )\n",
      "[Epoch: 23200] train loss: 0.2251, train acc: 0.9122, val loss: 0.2617, val acc: 0.9042  (best train acc: 0.9227, best val acc: 0.9153, best train loss: 0.1973  @ epoch 22738 )\n",
      "[Epoch: 23220] train loss: 0.2128, train acc: 0.9109, val loss: 0.2620, val acc: 0.9008  (best train acc: 0.9227, best val acc: 0.9153, best train loss: 0.1973  @ epoch 22738 )\n",
      "[Epoch: 23240] train loss: 0.2465, train acc: 0.9007, val loss: 0.2359, val acc: 0.9106  (best train acc: 0.9227, best val acc: 0.9153, best train loss: 0.1973  @ epoch 22738 )\n",
      "[Epoch: 23260] train loss: 0.2110, train acc: 0.9148, val loss: 0.2425, val acc: 0.9096  (best train acc: 0.9227, best val acc: 0.9153, best train loss: 0.1973  @ epoch 22738 )\n",
      "[Epoch: 23280] train loss: 0.2212, train acc: 0.9126, val loss: 0.2471, val acc: 0.9113  (best train acc: 0.9227, best val acc: 0.9153, best train loss: 0.1973  @ epoch 22738 )\n",
      "[Epoch: 23300] train loss: 0.2104, train acc: 0.9101, val loss: 0.2377, val acc: 0.9089  (best train acc: 0.9227, best val acc: 0.9153, best train loss: 0.1973  @ epoch 22738 )\n",
      "[Epoch: 23320] train loss: 0.2281, train acc: 0.9072, val loss: 0.2550, val acc: 0.9046  (best train acc: 0.9227, best val acc: 0.9153, best train loss: 0.1973  @ epoch 22738 )\n",
      "[Epoch: 23340] train loss: 0.2080, train acc: 0.9161, val loss: 0.2395, val acc: 0.9035  (best train acc: 0.9227, best val acc: 0.9153, best train loss: 0.1973  @ epoch 22738 )\n",
      "[Epoch: 23360] train loss: 0.2095, train acc: 0.9142, val loss: 0.2362, val acc: 0.9073  (best train acc: 0.9227, best val acc: 0.9153, best train loss: 0.1973  @ epoch 22738 )\n",
      "[Epoch: 23380] train loss: 0.2012, train acc: 0.9194, val loss: 0.2457, val acc: 0.9083  (best train acc: 0.9227, best val acc: 0.9153, best train loss: 0.1973  @ epoch 22738 )\n",
      "[Epoch: 23400] train loss: 0.2146, train acc: 0.9112, val loss: 0.2392, val acc: 0.9049  (best train acc: 0.9227, best val acc: 0.9153, best train loss: 0.1973  @ epoch 22738 )\n",
      "[Epoch: 23420] train loss: 0.1969, train acc: 0.9215, val loss: 0.2357, val acc: 0.9066  (best train acc: 0.9227, best val acc: 0.9153, best train loss: 0.1969  @ epoch 23420 )\n",
      "[Epoch: 23440] train loss: 0.2083, train acc: 0.9158, val loss: 0.2434, val acc: 0.9046  (best train acc: 0.9227, best val acc: 0.9153, best train loss: 0.1939  @ epoch 23430 )\n",
      "[Epoch: 23460] train loss: 0.2232, train acc: 0.9112, val loss: 0.2452, val acc: 0.9062  (best train acc: 0.9227, best val acc: 0.9153, best train loss: 0.1939  @ epoch 23430 )\n",
      "[Epoch: 23480] train loss: 0.2137, train acc: 0.9112, val loss: 0.2363, val acc: 0.9076  (best train acc: 0.9227, best val acc: 0.9153, best train loss: 0.1939  @ epoch 23430 )\n",
      "[Epoch: 23500] train loss: 0.2050, train acc: 0.9166, val loss: 0.2381, val acc: 0.9096  (best train acc: 0.9227, best val acc: 0.9153, best train loss: 0.1939  @ epoch 23430 )\n",
      "[Epoch: 23520] train loss: 0.2287, train acc: 0.9033, val loss: 0.2387, val acc: 0.9069  (best train acc: 0.9227, best val acc: 0.9153, best train loss: 0.1939  @ epoch 23430 )\n",
      "[Epoch: 23540] train loss: 0.1994, train acc: 0.9216, val loss: 0.2351, val acc: 0.9116  (best train acc: 0.9227, best val acc: 0.9153, best train loss: 0.1939  @ epoch 23430 )\n",
      "[Epoch: 23560] train loss: 0.2198, train acc: 0.9124, val loss: 0.2399, val acc: 0.9113  (best train acc: 0.9227, best val acc: 0.9153, best train loss: 0.1939  @ epoch 23430 )\n",
      "[Epoch: 23580] train loss: 0.2187, train acc: 0.9093, val loss: 0.2445, val acc: 0.9083  (best train acc: 0.9227, best val acc: 0.9153, best train loss: 0.1939  @ epoch 23430 )\n",
      "[Epoch: 23600] train loss: 0.2106, train acc: 0.9139, val loss: 0.2433, val acc: 0.9052  (best train acc: 0.9227, best val acc: 0.9153, best train loss: 0.1939  @ epoch 23430 )\n",
      "[Epoch: 23620] train loss: 0.2128, train acc: 0.9111, val loss: 0.2361, val acc: 0.9103  (best train acc: 0.9227, best val acc: 0.9153, best train loss: 0.1939  @ epoch 23430 )\n",
      "[Epoch: 23640] train loss: 0.1987, train acc: 0.9189, val loss: 0.2413, val acc: 0.9137  (best train acc: 0.9227, best val acc: 0.9153, best train loss: 0.1939  @ epoch 23430 )\n",
      "[Epoch: 23660] train loss: 0.2237, train acc: 0.9070, val loss: 0.2369, val acc: 0.9103  (best train acc: 0.9227, best val acc: 0.9157, best train loss: 0.1939  @ epoch 23430 )\n",
      "[Epoch: 23680] train loss: 0.2113, train acc: 0.9127, val loss: 0.2419, val acc: 0.9073  (best train acc: 0.9227, best val acc: 0.9157, best train loss: 0.1939  @ epoch 23430 )\n",
      "[Epoch: 23700] train loss: 0.2177, train acc: 0.9091, val loss: 0.2571, val acc: 0.9089  (best train acc: 0.9227, best val acc: 0.9157, best train loss: 0.1939  @ epoch 23430 )\n",
      "[Epoch: 23720] train loss: 0.2102, train acc: 0.9154, val loss: 0.2434, val acc: 0.9083  (best train acc: 0.9227, best val acc: 0.9157, best train loss: 0.1939  @ epoch 23430 )\n",
      "[Epoch: 23740] train loss: 0.2158, train acc: 0.9064, val loss: 0.2481, val acc: 0.9113  (best train acc: 0.9227, best val acc: 0.9157, best train loss: 0.1939  @ epoch 23430 )\n",
      "[Epoch: 23760] train loss: 0.2239, train acc: 0.9111, val loss: 0.2602, val acc: 0.9086  (best train acc: 0.9227, best val acc: 0.9157, best train loss: 0.1939  @ epoch 23430 )\n",
      "[Epoch: 23780] train loss: 0.2345, train acc: 0.9070, val loss: 0.2471, val acc: 0.9073  (best train acc: 0.9227, best val acc: 0.9157, best train loss: 0.1939  @ epoch 23430 )\n",
      "[Epoch: 23800] train loss: 0.2069, train acc: 0.9163, val loss: 0.2402, val acc: 0.9113  (best train acc: 0.9227, best val acc: 0.9157, best train loss: 0.1939  @ epoch 23430 )\n",
      "[Epoch: 23820] train loss: 0.2150, train acc: 0.9121, val loss: 0.2425, val acc: 0.9059  (best train acc: 0.9227, best val acc: 0.9157, best train loss: 0.1939  @ epoch 23430 )\n",
      "[Epoch: 23840] train loss: 0.2003, train acc: 0.9174, val loss: 0.2679, val acc: 0.9052  (best train acc: 0.9227, best val acc: 0.9157, best train loss: 0.1939  @ epoch 23430 )\n",
      "[Epoch: 23860] train loss: 0.2088, train acc: 0.9106, val loss: 0.2492, val acc: 0.9073  (best train acc: 0.9227, best val acc: 0.9157, best train loss: 0.1939  @ epoch 23430 )\n",
      "[Epoch: 23880] train loss: 0.2076, train acc: 0.9150, val loss: 0.2557, val acc: 0.9106  (best train acc: 0.9227, best val acc: 0.9157, best train loss: 0.1939  @ epoch 23430 )\n",
      "[Epoch: 23900] train loss: 0.2332, train acc: 0.9059, val loss: 0.2490, val acc: 0.9096  (best train acc: 0.9227, best val acc: 0.9157, best train loss: 0.1939  @ epoch 23430 )\n",
      "[Epoch: 23920] train loss: 0.2195, train acc: 0.9158, val loss: 0.2397, val acc: 0.9096  (best train acc: 0.9227, best val acc: 0.9157, best train loss: 0.1939  @ epoch 23430 )\n",
      "[Epoch: 23940] train loss: 0.2172, train acc: 0.9147, val loss: 0.2486, val acc: 0.9089  (best train acc: 0.9227, best val acc: 0.9157, best train loss: 0.1939  @ epoch 23430 )\n",
      "[Epoch: 23960] train loss: 0.2244, train acc: 0.9129, val loss: 0.2411, val acc: 0.9099  (best train acc: 0.9227, best val acc: 0.9157, best train loss: 0.1939  @ epoch 23430 )\n",
      "[Epoch: 23980] train loss: 0.2168, train acc: 0.9107, val loss: 0.2463, val acc: 0.9073  (best train acc: 0.9227, best val acc: 0.9157, best train loss: 0.1939  @ epoch 23430 )\n",
      "[Epoch: 24000] train loss: 0.2167, train acc: 0.9132, val loss: 0.2363, val acc: 0.9116  (best train acc: 0.9227, best val acc: 0.9157, best train loss: 0.1939  @ epoch 23430 )\n",
      "[Epoch: 24020] train loss: 0.2020, train acc: 0.9182, val loss: 0.2464, val acc: 0.9083  (best train acc: 0.9227, best val acc: 0.9157, best train loss: 0.1939  @ epoch 23430 )\n",
      "[Epoch: 24040] train loss: 0.2148, train acc: 0.9100, val loss: 0.2421, val acc: 0.9123  (best train acc: 0.9227, best val acc: 0.9157, best train loss: 0.1939  @ epoch 23430 )\n",
      "[Epoch: 24060] train loss: 0.2329, train acc: 0.9070, val loss: 0.2521, val acc: 0.9120  (best train acc: 0.9227, best val acc: 0.9157, best train loss: 0.1939  @ epoch 23430 )\n",
      "[Epoch: 24080] train loss: 0.2268, train acc: 0.9067, val loss: 0.2454, val acc: 0.9103  (best train acc: 0.9227, best val acc: 0.9157, best train loss: 0.1939  @ epoch 23430 )\n",
      "[Epoch: 24100] train loss: 0.2155, train acc: 0.9099, val loss: 0.2418, val acc: 0.9099  (best train acc: 0.9227, best val acc: 0.9157, best train loss: 0.1939  @ epoch 23430 )\n",
      "[Epoch: 24120] train loss: 0.2211, train acc: 0.9057, val loss: 0.2390, val acc: 0.9113  (best train acc: 0.9227, best val acc: 0.9157, best train loss: 0.1939  @ epoch 23430 )\n",
      "[Epoch: 24140] train loss: 0.2107, train acc: 0.9164, val loss: 0.2482, val acc: 0.9076  (best train acc: 0.9227, best val acc: 0.9157, best train loss: 0.1911  @ epoch 24126 )\n",
      "[Epoch: 24160] train loss: 0.2723, train acc: 0.8950, val loss: 0.2567, val acc: 0.9052  (best train acc: 0.9227, best val acc: 0.9157, best train loss: 0.1911  @ epoch 24126 )\n",
      "[Epoch: 24180] train loss: 0.2164, train acc: 0.9101, val loss: 0.2459, val acc: 0.9089  (best train acc: 0.9227, best val acc: 0.9157, best train loss: 0.1911  @ epoch 24126 )\n",
      "[Epoch: 24200] train loss: 0.2100, train acc: 0.9124, val loss: 0.2409, val acc: 0.9103  (best train acc: 0.9227, best val acc: 0.9157, best train loss: 0.1911  @ epoch 24126 )\n",
      "[Epoch: 24220] train loss: 0.2147, train acc: 0.9075, val loss: 0.2375, val acc: 0.9116  (best train acc: 0.9227, best val acc: 0.9157, best train loss: 0.1911  @ epoch 24126 )\n",
      "[Epoch: 24240] train loss: 0.1994, train acc: 0.9188, val loss: 0.2408, val acc: 0.9089  (best train acc: 0.9227, best val acc: 0.9157, best train loss: 0.1911  @ epoch 24126 )\n",
      "[Epoch: 24260] train loss: 0.2260, train acc: 0.9126, val loss: 0.2748, val acc: 0.8998  (best train acc: 0.9227, best val acc: 0.9157, best train loss: 0.1911  @ epoch 24126 )\n",
      "[Epoch: 24280] train loss: 0.2278, train acc: 0.9054, val loss: 0.2586, val acc: 0.9046  (best train acc: 0.9227, best val acc: 0.9157, best train loss: 0.1911  @ epoch 24126 )\n",
      "[Epoch: 24300] train loss: 0.2222, train acc: 0.9101, val loss: 0.2370, val acc: 0.9056  (best train acc: 0.9227, best val acc: 0.9157, best train loss: 0.1911  @ epoch 24126 )\n",
      "[Epoch: 24320] train loss: 0.2126, train acc: 0.9135, val loss: 0.2399, val acc: 0.9096  (best train acc: 0.9227, best val acc: 0.9157, best train loss: 0.1911  @ epoch 24126 )\n",
      "[Epoch: 24340] train loss: 0.2270, train acc: 0.9051, val loss: 0.2429, val acc: 0.9086  (best train acc: 0.9227, best val acc: 0.9157, best train loss: 0.1911  @ epoch 24126 )\n",
      "[Epoch: 24360] train loss: 0.2037, train acc: 0.9184, val loss: 0.2527, val acc: 0.9076  (best train acc: 0.9227, best val acc: 0.9157, best train loss: 0.1911  @ epoch 24126 )\n",
      "[Epoch: 24380] train loss: 0.2044, train acc: 0.9153, val loss: 0.2364, val acc: 0.9093  (best train acc: 0.9227, best val acc: 0.9157, best train loss: 0.1911  @ epoch 24126 )\n",
      "[Epoch: 24400] train loss: 0.2019, train acc: 0.9183, val loss: 0.2425, val acc: 0.9086  (best train acc: 0.9227, best val acc: 0.9157, best train loss: 0.1911  @ epoch 24126 )\n",
      "[Epoch: 24420] train loss: 0.2076, train acc: 0.9151, val loss: 0.2331, val acc: 0.9103  (best train acc: 0.9227, best val acc: 0.9157, best train loss: 0.1911  @ epoch 24126 )\n",
      "[Epoch: 24440] train loss: 0.2117, train acc: 0.9137, val loss: 0.2495, val acc: 0.9046  (best train acc: 0.9227, best val acc: 0.9157, best train loss: 0.1911  @ epoch 24126 )\n",
      "[Epoch: 24460] train loss: 0.2006, train acc: 0.9171, val loss: 0.2389, val acc: 0.9076  (best train acc: 0.9227, best val acc: 0.9157, best train loss: 0.1911  @ epoch 24126 )\n",
      "[Epoch: 24480] train loss: 0.2001, train acc: 0.9182, val loss: 0.2411, val acc: 0.9089  (best train acc: 0.9227, best val acc: 0.9157, best train loss: 0.1911  @ epoch 24126 )\n",
      "[Epoch: 24500] train loss: 0.2134, train acc: 0.9134, val loss: 0.2517, val acc: 0.9046  (best train acc: 0.9228, best val acc: 0.9157, best train loss: 0.1911  @ epoch 24126 )\n",
      "[Epoch: 24520] train loss: 0.2022, train acc: 0.9132, val loss: 0.2351, val acc: 0.9096  (best train acc: 0.9228, best val acc: 0.9157, best train loss: 0.1911  @ epoch 24126 )\n",
      "[Epoch: 24540] train loss: 0.2016, train acc: 0.9175, val loss: 0.2622, val acc: 0.9079  (best train acc: 0.9228, best val acc: 0.9157, best train loss: 0.1911  @ epoch 24126 )\n",
      "[Epoch: 24560] train loss: 0.2081, train acc: 0.9160, val loss: 0.2421, val acc: 0.9120  (best train acc: 0.9228, best val acc: 0.9157, best train loss: 0.1911  @ epoch 24126 )\n",
      "[Epoch: 24580] train loss: 0.2129, train acc: 0.9180, val loss: 0.2348, val acc: 0.9106  (best train acc: 0.9228, best val acc: 0.9157, best train loss: 0.1911  @ epoch 24126 )\n",
      "[Epoch: 24600] train loss: 0.2272, train acc: 0.9047, val loss: 0.2651, val acc: 0.9056  (best train acc: 0.9228, best val acc: 0.9157, best train loss: 0.1911  @ epoch 24126 )\n",
      "[Epoch: 24620] train loss: 0.2053, train acc: 0.9138, val loss: 0.2482, val acc: 0.9089  (best train acc: 0.9228, best val acc: 0.9157, best train loss: 0.1911  @ epoch 24126 )\n",
      "[Epoch: 24640] train loss: 0.2220, train acc: 0.9038, val loss: 0.2532, val acc: 0.9083  (best train acc: 0.9228, best val acc: 0.9157, best train loss: 0.1911  @ epoch 24126 )\n",
      "[Epoch: 24660] train loss: 0.2165, train acc: 0.9074, val loss: 0.2517, val acc: 0.9126  (best train acc: 0.9228, best val acc: 0.9157, best train loss: 0.1911  @ epoch 24126 )\n",
      "[Epoch: 24680] train loss: 0.2021, train acc: 0.9162, val loss: 0.2474, val acc: 0.9120  (best train acc: 0.9228, best val acc: 0.9157, best train loss: 0.1911  @ epoch 24126 )\n",
      "[Epoch: 24700] train loss: 0.2179, train acc: 0.9102, val loss: 0.2436, val acc: 0.9123  (best train acc: 0.9228, best val acc: 0.9157, best train loss: 0.1911  @ epoch 24126 )\n",
      "[Epoch: 24720] train loss: 0.1998, train acc: 0.9174, val loss: 0.2442, val acc: 0.9096  (best train acc: 0.9228, best val acc: 0.9157, best train loss: 0.1911  @ epoch 24126 )\n",
      "[Epoch: 24740] train loss: 0.2258, train acc: 0.9078, val loss: 0.2583, val acc: 0.9049  (best train acc: 0.9228, best val acc: 0.9157, best train loss: 0.1911  @ epoch 24126 )\n",
      "[Epoch: 24760] train loss: 0.2156, train acc: 0.9131, val loss: 0.2358, val acc: 0.9089  (best train acc: 0.9228, best val acc: 0.9157, best train loss: 0.1911  @ epoch 24126 )\n",
      "[Epoch: 24780] train loss: 0.2205, train acc: 0.9063, val loss: 0.2418, val acc: 0.9073  (best train acc: 0.9228, best val acc: 0.9157, best train loss: 0.1911  @ epoch 24126 )\n",
      "[Epoch: 24800] train loss: 0.2060, train acc: 0.9138, val loss: 0.2450, val acc: 0.9056  (best train acc: 0.9228, best val acc: 0.9157, best train loss: 0.1911  @ epoch 24126 )\n",
      "[Epoch: 24820] train loss: 0.1999, train acc: 0.9158, val loss: 0.2401, val acc: 0.9113  (best train acc: 0.9228, best val acc: 0.9157, best train loss: 0.1911  @ epoch 24126 )\n",
      "[Epoch: 24840] train loss: 0.2258, train acc: 0.9065, val loss: 0.2354, val acc: 0.9062  (best train acc: 0.9228, best val acc: 0.9157, best train loss: 0.1911  @ epoch 24126 )\n",
      "[Epoch: 24860] train loss: 0.2043, train acc: 0.9171, val loss: 0.2386, val acc: 0.9113  (best train acc: 0.9228, best val acc: 0.9157, best train loss: 0.1911  @ epoch 24126 )\n",
      "[Epoch: 24880] train loss: 0.2272, train acc: 0.9067, val loss: 0.2574, val acc: 0.9113  (best train acc: 0.9228, best val acc: 0.9157, best train loss: 0.1911  @ epoch 24126 )\n",
      "[Epoch: 24900] train loss: 0.2054, train acc: 0.9160, val loss: 0.2417, val acc: 0.9086  (best train acc: 0.9229, best val acc: 0.9157, best train loss: 0.1909  @ epoch 24892 )\n",
      "[Epoch: 24920] train loss: 0.2449, train acc: 0.9010, val loss: 0.2621, val acc: 0.9039  (best train acc: 0.9229, best val acc: 0.9157, best train loss: 0.1909  @ epoch 24892 )\n",
      "[Epoch: 24940] train loss: 0.2271, train acc: 0.9067, val loss: 0.2425, val acc: 0.9099  (best train acc: 0.9241, best val acc: 0.9157, best train loss: 0.1904  @ epoch 24931 )\n",
      "[Epoch: 24960] train loss: 0.2040, train acc: 0.9143, val loss: 0.2532, val acc: 0.9066  (best train acc: 0.9241, best val acc: 0.9157, best train loss: 0.1904  @ epoch 24931 )\n",
      "[Epoch: 24980] train loss: 0.2039, train acc: 0.9154, val loss: 0.2427, val acc: 0.9113  (best train acc: 0.9241, best val acc: 0.9157, best train loss: 0.1904  @ epoch 24931 )\n",
      "[Epoch: 25000] train loss: 0.2151, train acc: 0.9095, val loss: 0.2497, val acc: 0.9099  (best train acc: 0.9241, best val acc: 0.9157, best train loss: 0.1904  @ epoch 24931 )\n",
      "[Epoch: 25020] train loss: 0.2175, train acc: 0.9137, val loss: 0.2672, val acc: 0.9069  (best train acc: 0.9241, best val acc: 0.9157, best train loss: 0.1904  @ epoch 24931 )\n",
      "[Epoch: 25040] train loss: 0.2136, train acc: 0.9089, val loss: 0.2408, val acc: 0.9079  (best train acc: 0.9241, best val acc: 0.9157, best train loss: 0.1904  @ epoch 24931 )\n",
      "[Epoch: 25060] train loss: 0.2105, train acc: 0.9132, val loss: 0.2360, val acc: 0.9086  (best train acc: 0.9241, best val acc: 0.9157, best train loss: 0.1904  @ epoch 24931 )\n",
      "[Epoch: 25080] train loss: 0.1968, train acc: 0.9179, val loss: 0.2348, val acc: 0.9113  (best train acc: 0.9241, best val acc: 0.9157, best train loss: 0.1904  @ epoch 24931 )\n",
      "[Epoch: 25100] train loss: 0.2078, train acc: 0.9131, val loss: 0.2531, val acc: 0.9076  (best train acc: 0.9241, best val acc: 0.9157, best train loss: 0.1904  @ epoch 24931 )\n",
      "[Epoch: 25120] train loss: 0.2277, train acc: 0.9079, val loss: 0.2523, val acc: 0.9113  (best train acc: 0.9241, best val acc: 0.9157, best train loss: 0.1904  @ epoch 24931 )\n",
      "[Epoch: 25140] train loss: 0.2078, train acc: 0.9162, val loss: 0.2494, val acc: 0.9062  (best train acc: 0.9241, best val acc: 0.9157, best train loss: 0.1904  @ epoch 24931 )\n",
      "[Epoch: 25160] train loss: 0.2028, train acc: 0.9152, val loss: 0.2412, val acc: 0.9116  (best train acc: 0.9241, best val acc: 0.9157, best train loss: 0.1904  @ epoch 24931 )\n",
      "[Epoch: 25180] train loss: 0.2036, train acc: 0.9158, val loss: 0.2459, val acc: 0.9106  (best train acc: 0.9241, best val acc: 0.9157, best train loss: 0.1904  @ epoch 24931 )\n",
      "[Epoch: 25200] train loss: 0.2094, train acc: 0.9168, val loss: 0.2416, val acc: 0.9113  (best train acc: 0.9241, best val acc: 0.9157, best train loss: 0.1904  @ epoch 24931 )\n",
      "[Epoch: 25220] train loss: 0.2231, train acc: 0.9091, val loss: 0.2537, val acc: 0.9089  (best train acc: 0.9241, best val acc: 0.9157, best train loss: 0.1904  @ epoch 24931 )\n",
      "[Epoch: 25240] train loss: 0.1950, train acc: 0.9206, val loss: 0.2351, val acc: 0.9086  (best train acc: 0.9241, best val acc: 0.9157, best train loss: 0.1904  @ epoch 24931 )\n",
      "[Epoch: 25260] train loss: 0.2119, train acc: 0.9122, val loss: 0.2474, val acc: 0.9062  (best train acc: 0.9241, best val acc: 0.9157, best train loss: 0.1904  @ epoch 24931 )\n",
      "[Epoch: 25280] train loss: 0.2361, train acc: 0.9023, val loss: 0.2454, val acc: 0.9046  (best train acc: 0.9241, best val acc: 0.9157, best train loss: 0.1904  @ epoch 24931 )\n",
      "[Epoch: 25300] train loss: 0.2031, train acc: 0.9140, val loss: 0.2419, val acc: 0.9096  (best train acc: 0.9241, best val acc: 0.9157, best train loss: 0.1904  @ epoch 24931 )\n",
      "[Epoch: 25320] train loss: 0.2092, train acc: 0.9137, val loss: 0.2317, val acc: 0.9126  (best train acc: 0.9241, best val acc: 0.9157, best train loss: 0.1904  @ epoch 24931 )\n",
      "[Epoch: 25340] train loss: 0.2039, train acc: 0.9179, val loss: 0.2535, val acc: 0.9099  (best train acc: 0.9241, best val acc: 0.9157, best train loss: 0.1904  @ epoch 24931 )\n",
      "[Epoch: 25360] train loss: 0.2044, train acc: 0.9132, val loss: 0.2357, val acc: 0.9113  (best train acc: 0.9241, best val acc: 0.9157, best train loss: 0.1904  @ epoch 24931 )\n",
      "[Epoch: 25380] train loss: 0.2082, train acc: 0.9136, val loss: 0.2641, val acc: 0.9083  (best train acc: 0.9241, best val acc: 0.9157, best train loss: 0.1904  @ epoch 24931 )\n",
      "[Epoch: 25400] train loss: 0.2213, train acc: 0.9106, val loss: 0.2779, val acc: 0.9046  (best train acc: 0.9241, best val acc: 0.9157, best train loss: 0.1904  @ epoch 24931 )\n",
      "[Epoch: 25420] train loss: 0.2122, train acc: 0.9124, val loss: 0.2384, val acc: 0.9147  (best train acc: 0.9241, best val acc: 0.9157, best train loss: 0.1904  @ epoch 24931 )\n",
      "[Epoch: 25440] train loss: 0.2010, train acc: 0.9194, val loss: 0.2364, val acc: 0.9110  (best train acc: 0.9241, best val acc: 0.9157, best train loss: 0.1904  @ epoch 24931 )\n",
      "[Epoch: 25460] train loss: 0.2110, train acc: 0.9124, val loss: 0.2459, val acc: 0.9089  (best train acc: 0.9241, best val acc: 0.9157, best train loss: 0.1904  @ epoch 24931 )\n",
      "[Epoch: 25480] train loss: 0.2079, train acc: 0.9117, val loss: 0.2536, val acc: 0.9099  (best train acc: 0.9241, best val acc: 0.9157, best train loss: 0.1904  @ epoch 24931 )\n",
      "[Epoch: 25500] train loss: 0.2155, train acc: 0.9138, val loss: 0.2556, val acc: 0.9089  (best train acc: 0.9241, best val acc: 0.9157, best train loss: 0.1904  @ epoch 24931 )\n",
      "[Epoch: 25520] train loss: 0.2179, train acc: 0.9054, val loss: 0.2400, val acc: 0.9126  (best train acc: 0.9241, best val acc: 0.9157, best train loss: 0.1904  @ epoch 24931 )\n",
      "[Epoch: 25540] train loss: 0.2034, train acc: 0.9199, val loss: 0.2381, val acc: 0.9116  (best train acc: 0.9241, best val acc: 0.9157, best train loss: 0.1904  @ epoch 24931 )\n",
      "[Epoch: 25560] train loss: 0.2178, train acc: 0.9085, val loss: 0.2463, val acc: 0.9086  (best train acc: 0.9241, best val acc: 0.9157, best train loss: 0.1904  @ epoch 24931 )\n",
      "[Epoch: 25580] train loss: 0.2317, train acc: 0.9021, val loss: 0.2513, val acc: 0.9076  (best train acc: 0.9241, best val acc: 0.9157, best train loss: 0.1904  @ epoch 24931 )\n",
      "[Epoch: 25600] train loss: 0.2180, train acc: 0.9091, val loss: 0.2442, val acc: 0.9110  (best train acc: 0.9241, best val acc: 0.9157, best train loss: 0.1904  @ epoch 24931 )\n",
      "[Epoch: 25620] train loss: 0.2076, train acc: 0.9120, val loss: 0.2476, val acc: 0.9059  (best train acc: 0.9241, best val acc: 0.9157, best train loss: 0.1904  @ epoch 24931 )\n",
      "[Epoch: 25640] train loss: 0.2109, train acc: 0.9139, val loss: 0.2459, val acc: 0.9103  (best train acc: 0.9241, best val acc: 0.9157, best train loss: 0.1904  @ epoch 24931 )\n",
      "[Epoch: 25660] train loss: 0.2023, train acc: 0.9209, val loss: 0.2365, val acc: 0.9130  (best train acc: 0.9241, best val acc: 0.9157, best train loss: 0.1904  @ epoch 24931 )\n",
      "[Epoch: 25680] train loss: 0.2035, train acc: 0.9189, val loss: 0.2356, val acc: 0.9093  (best train acc: 0.9241, best val acc: 0.9157, best train loss: 0.1904  @ epoch 24931 )\n",
      "[Epoch: 25700] train loss: 0.2100, train acc: 0.9130, val loss: 0.2401, val acc: 0.9089  (best train acc: 0.9241, best val acc: 0.9157, best train loss: 0.1904  @ epoch 24931 )\n",
      "[Epoch: 25720] train loss: 0.2022, train acc: 0.9154, val loss: 0.2483, val acc: 0.9073  (best train acc: 0.9241, best val acc: 0.9157, best train loss: 0.1904  @ epoch 24931 )\n",
      "[Epoch: 25740] train loss: 0.1985, train acc: 0.9174, val loss: 0.2475, val acc: 0.9069  (best train acc: 0.9241, best val acc: 0.9157, best train loss: 0.1904  @ epoch 24931 )\n",
      "[Epoch: 25760] train loss: 0.2159, train acc: 0.9111, val loss: 0.2384, val acc: 0.9089  (best train acc: 0.9241, best val acc: 0.9157, best train loss: 0.1904  @ epoch 24931 )\n",
      "[Epoch: 25780] train loss: 0.1949, train acc: 0.9181, val loss: 0.2418, val acc: 0.9096  (best train acc: 0.9241, best val acc: 0.9157, best train loss: 0.1904  @ epoch 24931 )\n",
      "[Epoch: 25800] train loss: 0.2061, train acc: 0.9161, val loss: 0.2440, val acc: 0.9079  (best train acc: 0.9241, best val acc: 0.9157, best train loss: 0.1890  @ epoch 25788 )\n",
      "[Epoch: 25820] train loss: 0.2176, train acc: 0.9084, val loss: 0.2411, val acc: 0.9113  (best train acc: 0.9241, best val acc: 0.9157, best train loss: 0.1890  @ epoch 25788 )\n",
      "[Epoch: 25840] train loss: 0.2033, train acc: 0.9135, val loss: 0.2377, val acc: 0.9099  (best train acc: 0.9241, best val acc: 0.9157, best train loss: 0.1890  @ epoch 25788 )\n",
      "[Epoch: 25860] train loss: 0.2521, train acc: 0.8982, val loss: 0.2451, val acc: 0.9069  (best train acc: 0.9245, best val acc: 0.9157, best train loss: 0.1890  @ epoch 25788 )\n",
      "[Epoch: 25880] train loss: 0.2094, train acc: 0.9130, val loss: 0.2478, val acc: 0.9099  (best train acc: 0.9253, best val acc: 0.9157, best train loss: 0.1890  @ epoch 25788 )\n",
      "[Epoch: 25900] train loss: 0.2079, train acc: 0.9122, val loss: 0.2460, val acc: 0.9086  (best train acc: 0.9253, best val acc: 0.9157, best train loss: 0.1890  @ epoch 25788 )\n",
      "[Epoch: 25920] train loss: 0.2096, train acc: 0.9149, val loss: 0.2382, val acc: 0.9079  (best train acc: 0.9253, best val acc: 0.9157, best train loss: 0.1890  @ epoch 25788 )\n",
      "[Epoch: 25940] train loss: 0.2162, train acc: 0.9073, val loss: 0.2452, val acc: 0.9096  (best train acc: 0.9253, best val acc: 0.9157, best train loss: 0.1890  @ epoch 25788 )\n",
      "[Epoch: 25960] train loss: 0.2011, train acc: 0.9184, val loss: 0.2378, val acc: 0.9116  (best train acc: 0.9253, best val acc: 0.9157, best train loss: 0.1890  @ epoch 25788 )\n",
      "[Epoch: 25980] train loss: 0.2006, train acc: 0.9184, val loss: 0.2395, val acc: 0.9079  (best train acc: 0.9253, best val acc: 0.9157, best train loss: 0.1890  @ epoch 25788 )\n",
      "[Epoch: 26000] train loss: 0.2006, train acc: 0.9166, val loss: 0.2391, val acc: 0.9110  (best train acc: 0.9253, best val acc: 0.9157, best train loss: 0.1890  @ epoch 25788 )\n",
      "[Epoch: 26020] train loss: 0.1973, train acc: 0.9221, val loss: 0.2444, val acc: 0.9083  (best train acc: 0.9253, best val acc: 0.9157, best train loss: 0.1890  @ epoch 25788 )\n",
      "[Epoch: 26040] train loss: 0.2136, train acc: 0.9143, val loss: 0.2321, val acc: 0.9062  (best train acc: 0.9253, best val acc: 0.9157, best train loss: 0.1890  @ epoch 25788 )\n",
      "[Epoch: 26060] train loss: 0.2114, train acc: 0.9175, val loss: 0.2345, val acc: 0.9120  (best train acc: 0.9253, best val acc: 0.9157, best train loss: 0.1890  @ epoch 25788 )\n",
      "[Epoch: 26080] train loss: 0.1978, train acc: 0.9179, val loss: 0.2457, val acc: 0.9103  (best train acc: 0.9253, best val acc: 0.9157, best train loss: 0.1890  @ epoch 25788 )\n",
      "[Epoch: 26100] train loss: 0.1966, train acc: 0.9225, val loss: 0.2412, val acc: 0.9096  (best train acc: 0.9253, best val acc: 0.9157, best train loss: 0.1890  @ epoch 25788 )\n",
      "[Epoch: 26120] train loss: 0.2181, train acc: 0.9119, val loss: 0.2377, val acc: 0.9096  (best train acc: 0.9253, best val acc: 0.9157, best train loss: 0.1880  @ epoch 26102 )\n",
      "[Epoch: 26140] train loss: 0.2176, train acc: 0.9070, val loss: 0.2624, val acc: 0.9049  (best train acc: 0.9253, best val acc: 0.9157, best train loss: 0.1880  @ epoch 26102 )\n",
      "[Epoch: 26160] train loss: 0.2496, train acc: 0.9050, val loss: 0.2415, val acc: 0.9086  (best train acc: 0.9253, best val acc: 0.9157, best train loss: 0.1880  @ epoch 26102 )\n",
      "[Epoch: 26180] train loss: 0.2045, train acc: 0.9171, val loss: 0.2429, val acc: 0.9093  (best train acc: 0.9253, best val acc: 0.9157, best train loss: 0.1880  @ epoch 26102 )\n",
      "[Epoch: 26200] train loss: 0.2111, train acc: 0.9107, val loss: 0.2401, val acc: 0.9116  (best train acc: 0.9253, best val acc: 0.9157, best train loss: 0.1880  @ epoch 26102 )\n",
      "[Epoch: 26220] train loss: 0.2042, train acc: 0.9177, val loss: 0.2429, val acc: 0.9099  (best train acc: 0.9253, best val acc: 0.9157, best train loss: 0.1880  @ epoch 26102 )\n",
      "[Epoch: 26240] train loss: 0.2141, train acc: 0.9127, val loss: 0.2341, val acc: 0.9099  (best train acc: 0.9253, best val acc: 0.9157, best train loss: 0.1880  @ epoch 26102 )\n",
      "[Epoch: 26260] train loss: 0.2118, train acc: 0.9141, val loss: 0.2398, val acc: 0.9099  (best train acc: 0.9253, best val acc: 0.9157, best train loss: 0.1880  @ epoch 26102 )\n",
      "[Epoch: 26280] train loss: 0.2267, train acc: 0.9093, val loss: 0.2429, val acc: 0.9062  (best train acc: 0.9253, best val acc: 0.9157, best train loss: 0.1880  @ epoch 26102 )\n",
      "[Epoch: 26300] train loss: 0.2282, train acc: 0.9091, val loss: 0.2502, val acc: 0.9083  (best train acc: 0.9253, best val acc: 0.9157, best train loss: 0.1880  @ epoch 26102 )\n",
      "[Epoch: 26320] train loss: 0.2161, train acc: 0.9114, val loss: 0.2422, val acc: 0.9099  (best train acc: 0.9253, best val acc: 0.9157, best train loss: 0.1880  @ epoch 26102 )\n",
      "[Epoch: 26340] train loss: 0.2156, train acc: 0.9094, val loss: 0.2487, val acc: 0.9062  (best train acc: 0.9253, best val acc: 0.9157, best train loss: 0.1880  @ epoch 26102 )\n",
      "[Epoch: 26360] train loss: 0.1968, train acc: 0.9201, val loss: 0.2476, val acc: 0.9073  (best train acc: 0.9253, best val acc: 0.9157, best train loss: 0.1880  @ epoch 26102 )\n",
      "[Epoch: 26380] train loss: 0.2096, train acc: 0.9144, val loss: 0.2390, val acc: 0.9073  (best train acc: 0.9253, best val acc: 0.9157, best train loss: 0.1880  @ epoch 26102 )\n",
      "[Epoch: 26400] train loss: 0.2103, train acc: 0.9117, val loss: 0.2472, val acc: 0.9096  (best train acc: 0.9253, best val acc: 0.9157, best train loss: 0.1880  @ epoch 26102 )\n",
      "[Epoch: 26420] train loss: 0.2175, train acc: 0.9072, val loss: 0.2392, val acc: 0.9086  (best train acc: 0.9253, best val acc: 0.9157, best train loss: 0.1880  @ epoch 26102 )\n",
      "[Epoch: 26440] train loss: 0.1996, train acc: 0.9185, val loss: 0.2320, val acc: 0.9086  (best train acc: 0.9253, best val acc: 0.9157, best train loss: 0.1880  @ epoch 26102 )\n",
      "[Epoch: 26460] train loss: 0.2163, train acc: 0.9099, val loss: 0.2551, val acc: 0.9039  (best train acc: 0.9253, best val acc: 0.9157, best train loss: 0.1880  @ epoch 26102 )\n",
      "[Epoch: 26480] train loss: 0.2113, train acc: 0.9137, val loss: 0.2349, val acc: 0.9049  (best train acc: 0.9253, best val acc: 0.9157, best train loss: 0.1880  @ epoch 26102 )\n",
      "[Epoch: 26500] train loss: 0.2353, train acc: 0.9022, val loss: 0.2404, val acc: 0.9083  (best train acc: 0.9253, best val acc: 0.9157, best train loss: 0.1880  @ epoch 26102 )\n",
      "[Epoch: 26520] train loss: 0.2100, train acc: 0.9193, val loss: 0.2385, val acc: 0.9076  (best train acc: 0.9253, best val acc: 0.9157, best train loss: 0.1880  @ epoch 26102 )\n",
      "[Epoch: 26540] train loss: 0.2205, train acc: 0.9095, val loss: 0.2480, val acc: 0.9069  (best train acc: 0.9253, best val acc: 0.9157, best train loss: 0.1880  @ epoch 26102 )\n",
      "[Epoch: 26560] train loss: 0.1949, train acc: 0.9187, val loss: 0.2492, val acc: 0.9046  (best train acc: 0.9253, best val acc: 0.9157, best train loss: 0.1880  @ epoch 26102 )\n",
      "[Epoch: 26580] train loss: 0.2279, train acc: 0.9082, val loss: 0.2470, val acc: 0.9073  (best train acc: 0.9253, best val acc: 0.9157, best train loss: 0.1880  @ epoch 26102 )\n",
      "[Epoch: 26600] train loss: 0.2153, train acc: 0.9111, val loss: 0.2434, val acc: 0.9093  (best train acc: 0.9253, best val acc: 0.9157, best train loss: 0.1880  @ epoch 26102 )\n",
      "[Epoch: 26620] train loss: 0.2163, train acc: 0.9103, val loss: 0.2553, val acc: 0.9052  (best train acc: 0.9253, best val acc: 0.9157, best train loss: 0.1880  @ epoch 26102 )\n",
      "[Epoch: 26640] train loss: 0.1969, train acc: 0.9182, val loss: 0.2380, val acc: 0.9103  (best train acc: 0.9253, best val acc: 0.9157, best train loss: 0.1880  @ epoch 26102 )\n",
      "[Epoch: 26660] train loss: 0.2023, train acc: 0.9166, val loss: 0.2556, val acc: 0.9052  (best train acc: 0.9253, best val acc: 0.9157, best train loss: 0.1880  @ epoch 26102 )\n",
      "[Epoch: 26680] train loss: 0.2226, train acc: 0.9101, val loss: 0.2544, val acc: 0.9052  (best train acc: 0.9253, best val acc: 0.9157, best train loss: 0.1880  @ epoch 26102 )\n",
      "[Epoch: 26700] train loss: 0.2121, train acc: 0.9119, val loss: 0.2341, val acc: 0.9110  (best train acc: 0.9253, best val acc: 0.9157, best train loss: 0.1880  @ epoch 26102 )\n",
      "[Epoch: 26720] train loss: 0.2120, train acc: 0.9153, val loss: 0.2490, val acc: 0.9076  (best train acc: 0.9253, best val acc: 0.9157, best train loss: 0.1880  @ epoch 26102 )\n",
      "[Epoch: 26740] train loss: 0.1993, train acc: 0.9169, val loss: 0.2446, val acc: 0.9079  (best train acc: 0.9253, best val acc: 0.9157, best train loss: 0.1880  @ epoch 26102 )\n",
      "[Epoch: 26760] train loss: 0.2046, train acc: 0.9179, val loss: 0.2446, val acc: 0.9096  (best train acc: 0.9253, best val acc: 0.9157, best train loss: 0.1880  @ epoch 26102 )\n",
      "[Epoch: 26780] train loss: 0.2021, train acc: 0.9170, val loss: 0.2296, val acc: 0.9096  (best train acc: 0.9253, best val acc: 0.9157, best train loss: 0.1880  @ epoch 26102 )\n",
      "[Epoch: 26800] train loss: 0.2023, train acc: 0.9140, val loss: 0.2374, val acc: 0.9089  (best train acc: 0.9253, best val acc: 0.9157, best train loss: 0.1880  @ epoch 26102 )\n",
      "[Epoch: 26820] train loss: 0.1919, train acc: 0.9198, val loss: 0.2522, val acc: 0.9019  (best train acc: 0.9253, best val acc: 0.9157, best train loss: 0.1880  @ epoch 26102 )\n",
      "[Epoch: 26840] train loss: 0.2016, train acc: 0.9151, val loss: 0.2417, val acc: 0.9079  (best train acc: 0.9253, best val acc: 0.9157, best train loss: 0.1880  @ epoch 26102 )\n",
      "[Epoch: 26860] train loss: 0.2085, train acc: 0.9146, val loss: 0.2443, val acc: 0.9086  (best train acc: 0.9253, best val acc: 0.9157, best train loss: 0.1880  @ epoch 26102 )\n",
      "[Epoch: 26880] train loss: 0.2064, train acc: 0.9153, val loss: 0.2430, val acc: 0.9066  (best train acc: 0.9253, best val acc: 0.9157, best train loss: 0.1880  @ epoch 26102 )\n",
      "[Epoch: 26900] train loss: 0.2272, train acc: 0.9056, val loss: 0.2398, val acc: 0.9113  (best train acc: 0.9253, best val acc: 0.9157, best train loss: 0.1880  @ epoch 26102 )\n",
      "[Epoch: 26920] train loss: 0.2241, train acc: 0.9091, val loss: 0.2496, val acc: 0.9059  (best train acc: 0.9253, best val acc: 0.9157, best train loss: 0.1880  @ epoch 26102 )\n",
      "[Epoch: 26940] train loss: 0.1968, train acc: 0.9179, val loss: 0.2627, val acc: 0.9056  (best train acc: 0.9253, best val acc: 0.9157, best train loss: 0.1880  @ epoch 26102 )\n",
      "[Epoch: 26960] train loss: 0.2137, train acc: 0.9103, val loss: 0.2471, val acc: 0.9069  (best train acc: 0.9253, best val acc: 0.9157, best train loss: 0.1880  @ epoch 26102 )\n",
      "[Epoch: 26980] train loss: 0.2175, train acc: 0.9088, val loss: 0.2383, val acc: 0.9099  (best train acc: 0.9253, best val acc: 0.9157, best train loss: 0.1880  @ epoch 26102 )\n",
      "[Epoch: 27000] train loss: 0.2088, train acc: 0.9117, val loss: 0.2300, val acc: 0.9096  (best train acc: 0.9253, best val acc: 0.9157, best train loss: 0.1880  @ epoch 26102 )\n",
      "[Epoch: 27020] train loss: 0.2079, train acc: 0.9111, val loss: 0.2452, val acc: 0.9073  (best train acc: 0.9253, best val acc: 0.9157, best train loss: 0.1880  @ epoch 26102 )\n",
      "[Epoch: 27040] train loss: 0.2073, train acc: 0.9140, val loss: 0.2398, val acc: 0.9116  (best train acc: 0.9253, best val acc: 0.9157, best train loss: 0.1880  @ epoch 26102 )\n",
      "[Epoch: 27060] train loss: 0.2076, train acc: 0.9162, val loss: 0.2318, val acc: 0.9120  (best train acc: 0.9253, best val acc: 0.9157, best train loss: 0.1880  @ epoch 26102 )\n",
      "[Epoch: 27080] train loss: 0.2059, train acc: 0.9195, val loss: 0.2490, val acc: 0.9106  (best train acc: 0.9253, best val acc: 0.9157, best train loss: 0.1880  @ epoch 26102 )\n",
      "[Epoch: 27100] train loss: 0.2049, train acc: 0.9138, val loss: 0.2343, val acc: 0.9103  (best train acc: 0.9253, best val acc: 0.9157, best train loss: 0.1880  @ epoch 26102 )\n",
      "[Epoch: 27120] train loss: 0.2060, train acc: 0.9162, val loss: 0.2423, val acc: 0.9110  (best train acc: 0.9253, best val acc: 0.9157, best train loss: 0.1880  @ epoch 26102 )\n",
      "[Epoch: 27140] train loss: 0.2105, train acc: 0.9121, val loss: 0.2439, val acc: 0.9089  (best train acc: 0.9253, best val acc: 0.9157, best train loss: 0.1880  @ epoch 26102 )\n",
      "[Epoch: 27160] train loss: 0.2109, train acc: 0.9119, val loss: 0.2439, val acc: 0.9076  (best train acc: 0.9253, best val acc: 0.9157, best train loss: 0.1880  @ epoch 26102 )\n",
      "[Epoch: 27180] train loss: 0.2057, train acc: 0.9135, val loss: 0.2387, val acc: 0.9096  (best train acc: 0.9253, best val acc: 0.9157, best train loss: 0.1880  @ epoch 26102 )\n",
      "[Epoch: 27200] train loss: 0.2060, train acc: 0.9188, val loss: 0.2495, val acc: 0.9066  (best train acc: 0.9253, best val acc: 0.9157, best train loss: 0.1880  @ epoch 26102 )\n",
      "[Epoch: 27220] train loss: 0.2189, train acc: 0.9127, val loss: 0.2372, val acc: 0.9099  (best train acc: 0.9253, best val acc: 0.9157, best train loss: 0.1880  @ epoch 26102 )\n",
      "[Epoch: 27240] train loss: 0.2131, train acc: 0.9093, val loss: 0.2336, val acc: 0.9103  (best train acc: 0.9253, best val acc: 0.9157, best train loss: 0.1880  @ epoch 26102 )\n",
      "[Epoch: 27260] train loss: 0.2033, train acc: 0.9189, val loss: 0.2431, val acc: 0.9116  (best train acc: 0.9253, best val acc: 0.9157, best train loss: 0.1876  @ epoch 27245 )\n",
      "[Epoch: 27280] train loss: 0.1966, train acc: 0.9190, val loss: 0.2489, val acc: 0.9086  (best train acc: 0.9253, best val acc: 0.9157, best train loss: 0.1876  @ epoch 27245 )\n",
      "[Epoch: 27300] train loss: 0.2116, train acc: 0.9126, val loss: 0.2415, val acc: 0.9099  (best train acc: 0.9253, best val acc: 0.9157, best train loss: 0.1876  @ epoch 27245 )\n",
      "[Epoch: 27320] train loss: 0.2055, train acc: 0.9135, val loss: 0.2382, val acc: 0.9096  (best train acc: 0.9253, best val acc: 0.9157, best train loss: 0.1876  @ epoch 27245 )\n",
      "[Epoch: 27340] train loss: 0.2050, train acc: 0.9173, val loss: 0.2506, val acc: 0.9059  (best train acc: 0.9253, best val acc: 0.9157, best train loss: 0.1876  @ epoch 27245 )\n",
      "[Epoch: 27360] train loss: 0.2145, train acc: 0.9098, val loss: 0.2467, val acc: 0.9106  (best train acc: 0.9253, best val acc: 0.9157, best train loss: 0.1876  @ epoch 27245 )\n",
      "[Epoch: 27380] train loss: 0.1987, train acc: 0.9232, val loss: 0.2359, val acc: 0.9076  (best train acc: 0.9253, best val acc: 0.9157, best train loss: 0.1876  @ epoch 27245 )\n",
      "[Epoch: 27400] train loss: 0.2093, train acc: 0.9174, val loss: 0.2353, val acc: 0.9069  (best train acc: 0.9253, best val acc: 0.9157, best train loss: 0.1876  @ epoch 27245 )\n",
      "[Epoch: 27420] train loss: 0.2057, train acc: 0.9161, val loss: 0.2408, val acc: 0.9096  (best train acc: 0.9253, best val acc: 0.9157, best train loss: 0.1876  @ epoch 27245 )\n",
      "[Epoch: 27440] train loss: 0.2081, train acc: 0.9119, val loss: 0.2463, val acc: 0.9096  (best train acc: 0.9253, best val acc: 0.9157, best train loss: 0.1876  @ epoch 27245 )\n",
      "[Epoch: 27460] train loss: 0.1986, train acc: 0.9187, val loss: 0.2400, val acc: 0.9106  (best train acc: 0.9253, best val acc: 0.9157, best train loss: 0.1876  @ epoch 27245 )\n",
      "[Epoch: 27480] train loss: 0.2079, train acc: 0.9134, val loss: 0.2470, val acc: 0.9059  (best train acc: 0.9253, best val acc: 0.9157, best train loss: 0.1876  @ epoch 27245 )\n",
      "[Epoch: 27500] train loss: 0.2176, train acc: 0.9153, val loss: 0.2347, val acc: 0.9093  (best train acc: 0.9253, best val acc: 0.9157, best train loss: 0.1876  @ epoch 27245 )\n",
      "[Epoch: 27520] train loss: 0.2042, train acc: 0.9201, val loss: 0.2405, val acc: 0.9086  (best train acc: 0.9253, best val acc: 0.9157, best train loss: 0.1876  @ epoch 27245 )\n",
      "[Epoch: 27540] train loss: 0.2180, train acc: 0.9080, val loss: 0.2619, val acc: 0.9008  (best train acc: 0.9253, best val acc: 0.9157, best train loss: 0.1876  @ epoch 27245 )\n",
      "[Epoch: 27560] train loss: 0.1976, train acc: 0.9200, val loss: 0.2386, val acc: 0.9089  (best train acc: 0.9253, best val acc: 0.9157, best train loss: 0.1876  @ epoch 27245 )\n",
      "[Epoch: 27580] train loss: 0.1976, train acc: 0.9202, val loss: 0.2416, val acc: 0.9086  (best train acc: 0.9253, best val acc: 0.9157, best train loss: 0.1876  @ epoch 27245 )\n",
      "[Epoch: 27600] train loss: 0.2136, train acc: 0.9143, val loss: 0.2592, val acc: 0.9099  (best train acc: 0.9253, best val acc: 0.9157, best train loss: 0.1876  @ epoch 27245 )\n",
      "[Epoch: 27620] train loss: 0.2093, train acc: 0.9117, val loss: 0.2391, val acc: 0.9086  (best train acc: 0.9253, best val acc: 0.9157, best train loss: 0.1876  @ epoch 27245 )\n",
      "[Epoch: 27640] train loss: 0.2370, train acc: 0.9031, val loss: 0.2485, val acc: 0.9059  (best train acc: 0.9253, best val acc: 0.9157, best train loss: 0.1876  @ epoch 27245 )\n",
      "[Epoch: 27660] train loss: 0.2066, train acc: 0.9156, val loss: 0.2413, val acc: 0.9113  (best train acc: 0.9253, best val acc: 0.9157, best train loss: 0.1876  @ epoch 27245 )\n",
      "[Epoch: 27680] train loss: 0.2073, train acc: 0.9198, val loss: 0.2450, val acc: 0.9086  (best train acc: 0.9253, best val acc: 0.9157, best train loss: 0.1876  @ epoch 27245 )\n",
      "[Epoch: 27700] train loss: 0.2218, train acc: 0.9059, val loss: 0.2393, val acc: 0.9096  (best train acc: 0.9253, best val acc: 0.9157, best train loss: 0.1876  @ epoch 27245 )\n",
      "[Epoch: 27720] train loss: 0.2099, train acc: 0.9127, val loss: 0.2529, val acc: 0.9062  (best train acc: 0.9253, best val acc: 0.9157, best train loss: 0.1876  @ epoch 27245 )\n",
      "[Epoch: 27740] train loss: 0.2115, train acc: 0.9098, val loss: 0.2487, val acc: 0.9079  (best train acc: 0.9253, best val acc: 0.9157, best train loss: 0.1876  @ epoch 27245 )\n",
      "[Epoch: 27760] train loss: 0.2035, train acc: 0.9189, val loss: 0.2475, val acc: 0.9012  (best train acc: 0.9253, best val acc: 0.9157, best train loss: 0.1876  @ epoch 27245 )\n",
      "[Epoch: 27780] train loss: 0.2227, train acc: 0.9089, val loss: 0.2383, val acc: 0.9089  (best train acc: 0.9253, best val acc: 0.9157, best train loss: 0.1876  @ epoch 27245 )\n",
      "[Epoch: 27800] train loss: 0.1996, train acc: 0.9174, val loss: 0.2452, val acc: 0.9039  (best train acc: 0.9253, best val acc: 0.9157, best train loss: 0.1876  @ epoch 27245 )\n",
      "[Epoch: 27820] train loss: 0.2047, train acc: 0.9173, val loss: 0.2426, val acc: 0.9106  (best train acc: 0.9253, best val acc: 0.9157, best train loss: 0.1876  @ epoch 27245 )\n",
      "[Epoch: 27840] train loss: 0.2071, train acc: 0.9164, val loss: 0.2320, val acc: 0.9089  (best train acc: 0.9253, best val acc: 0.9157, best train loss: 0.1876  @ epoch 27245 )\n",
      "[Epoch: 27860] train loss: 0.2076, train acc: 0.9132, val loss: 0.2401, val acc: 0.9113  (best train acc: 0.9253, best val acc: 0.9157, best train loss: 0.1876  @ epoch 27245 )\n",
      "[Epoch: 27880] train loss: 0.1999, train acc: 0.9199, val loss: 0.2467, val acc: 0.9062  (best train acc: 0.9253, best val acc: 0.9157, best train loss: 0.1876  @ epoch 27245 )\n",
      "[Epoch: 27900] train loss: 0.2049, train acc: 0.9133, val loss: 0.2360, val acc: 0.9116  (best train acc: 0.9265, best val acc: 0.9157, best train loss: 0.1874  @ epoch 27896 )\n",
      "[Epoch: 27920] train loss: 0.2009, train acc: 0.9204, val loss: 0.2367, val acc: 0.9029  (best train acc: 0.9265, best val acc: 0.9157, best train loss: 0.1874  @ epoch 27896 )\n",
      "[Epoch: 27940] train loss: 0.2237, train acc: 0.9023, val loss: 0.2562, val acc: 0.8988  (best train acc: 0.9265, best val acc: 0.9157, best train loss: 0.1874  @ epoch 27896 )\n",
      "[Epoch: 27960] train loss: 0.2055, train acc: 0.9166, val loss: 0.2403, val acc: 0.9062  (best train acc: 0.9265, best val acc: 0.9157, best train loss: 0.1874  @ epoch 27896 )\n",
      "[Epoch: 27980] train loss: 0.1970, train acc: 0.9169, val loss: 0.2480, val acc: 0.9052  (best train acc: 0.9265, best val acc: 0.9157, best train loss: 0.1874  @ epoch 27896 )\n",
      "[Epoch: 28000] train loss: 0.1957, train acc: 0.9192, val loss: 0.2448, val acc: 0.9096  (best train acc: 0.9265, best val acc: 0.9157, best train loss: 0.1874  @ epoch 27896 )\n",
      "[Epoch: 28020] train loss: 0.2404, train acc: 0.9049, val loss: 0.2627, val acc: 0.8965  (best train acc: 0.9265, best val acc: 0.9157, best train loss: 0.1874  @ epoch 27896 )\n",
      "[Epoch: 28040] train loss: 0.2213, train acc: 0.9149, val loss: 0.2455, val acc: 0.9086  (best train acc: 0.9265, best val acc: 0.9157, best train loss: 0.1874  @ epoch 27896 )\n",
      "[Epoch: 28060] train loss: 0.2163, train acc: 0.9091, val loss: 0.2477, val acc: 0.9086  (best train acc: 0.9265, best val acc: 0.9157, best train loss: 0.1874  @ epoch 27896 )\n",
      "[Epoch: 28080] train loss: 0.1981, train acc: 0.9156, val loss: 0.2334, val acc: 0.9116  (best train acc: 0.9265, best val acc: 0.9157, best train loss: 0.1874  @ epoch 27896 )\n",
      "[Epoch: 28100] train loss: 0.2018, train acc: 0.9155, val loss: 0.2330, val acc: 0.9093  (best train acc: 0.9265, best val acc: 0.9157, best train loss: 0.1874  @ epoch 27896 )\n",
      "[Epoch: 28120] train loss: 0.2205, train acc: 0.9078, val loss: 0.3147, val acc: 0.8887  (best train acc: 0.9265, best val acc: 0.9157, best train loss: 0.1874  @ epoch 27896 )\n",
      "[Epoch: 28140] train loss: 0.2066, train acc: 0.9184, val loss: 0.2443, val acc: 0.9076  (best train acc: 0.9265, best val acc: 0.9157, best train loss: 0.1874  @ epoch 27896 )\n",
      "[Epoch: 28160] train loss: 0.2033, train acc: 0.9169, val loss: 0.2463, val acc: 0.9099  (best train acc: 0.9265, best val acc: 0.9157, best train loss: 0.1874  @ epoch 27896 )\n",
      "[Epoch: 28180] train loss: 0.2127, train acc: 0.9101, val loss: 0.2445, val acc: 0.9106  (best train acc: 0.9265, best val acc: 0.9157, best train loss: 0.1874  @ epoch 27896 )\n",
      "[Epoch: 28200] train loss: 0.2136, train acc: 0.9108, val loss: 0.2515, val acc: 0.9083  (best train acc: 0.9265, best val acc: 0.9157, best train loss: 0.1874  @ epoch 27896 )\n",
      "[Epoch: 28220] train loss: 0.2089, train acc: 0.9127, val loss: 0.2407, val acc: 0.9079  (best train acc: 0.9265, best val acc: 0.9157, best train loss: 0.1874  @ epoch 27896 )\n",
      "[Epoch: 28240] train loss: 0.1980, train acc: 0.9187, val loss: 0.2363, val acc: 0.9110  (best train acc: 0.9265, best val acc: 0.9157, best train loss: 0.1874  @ epoch 27896 )\n",
      "[Epoch: 28260] train loss: 0.2005, train acc: 0.9198, val loss: 0.2486, val acc: 0.9079  (best train acc: 0.9265, best val acc: 0.9157, best train loss: 0.1874  @ epoch 27896 )\n",
      "[Epoch: 28280] train loss: 0.2252, train acc: 0.9057, val loss: 0.2457, val acc: 0.9096  (best train acc: 0.9265, best val acc: 0.9157, best train loss: 0.1874  @ epoch 27896 )\n",
      "[Epoch: 28300] train loss: 0.2028, train acc: 0.9192, val loss: 0.2546, val acc: 0.9086  (best train acc: 0.9265, best val acc: 0.9157, best train loss: 0.1874  @ epoch 27896 )\n",
      "[Epoch: 28320] train loss: 0.2028, train acc: 0.9163, val loss: 0.2427, val acc: 0.9052  (best train acc: 0.9265, best val acc: 0.9157, best train loss: 0.1874  @ epoch 27896 )\n",
      "[Epoch: 28340] train loss: 0.2121, train acc: 0.9104, val loss: 0.2408, val acc: 0.9066  (best train acc: 0.9265, best val acc: 0.9157, best train loss: 0.1874  @ epoch 27896 )\n",
      "[Epoch: 28360] train loss: 0.2048, train acc: 0.9136, val loss: 0.2412, val acc: 0.9079  (best train acc: 0.9265, best val acc: 0.9157, best train loss: 0.1874  @ epoch 27896 )\n",
      "[Epoch: 28380] train loss: 0.1995, train acc: 0.9192, val loss: 0.2370, val acc: 0.9059  (best train acc: 0.9265, best val acc: 0.9157, best train loss: 0.1874  @ epoch 27896 )\n",
      "[Epoch: 28400] train loss: 0.2021, train acc: 0.9169, val loss: 0.2337, val acc: 0.9093  (best train acc: 0.9265, best val acc: 0.9157, best train loss: 0.1874  @ epoch 27896 )\n",
      "[Epoch: 28420] train loss: 0.2040, train acc: 0.9149, val loss: 0.2648, val acc: 0.9022  (best train acc: 0.9265, best val acc: 0.9157, best train loss: 0.1874  @ epoch 27896 )\n",
      "[Epoch: 28440] train loss: 0.2057, train acc: 0.9165, val loss: 0.2405, val acc: 0.9099  (best train acc: 0.9265, best val acc: 0.9157, best train loss: 0.1874  @ epoch 27896 )\n",
      "[Epoch: 28460] train loss: 0.1996, train acc: 0.9163, val loss: 0.2360, val acc: 0.9086  (best train acc: 0.9265, best val acc: 0.9157, best train loss: 0.1874  @ epoch 27896 )\n",
      "[Epoch: 28480] train loss: 0.1976, train acc: 0.9165, val loss: 0.2334, val acc: 0.9069  (best train acc: 0.9265, best val acc: 0.9157, best train loss: 0.1874  @ epoch 27896 )\n",
      "[Epoch: 28500] train loss: 0.2211, train acc: 0.9091, val loss: 0.2361, val acc: 0.9093  (best train acc: 0.9265, best val acc: 0.9157, best train loss: 0.1874  @ epoch 27896 )\n",
      "[Epoch: 28520] train loss: 0.2287, train acc: 0.9062, val loss: 0.2577, val acc: 0.9046  (best train acc: 0.9265, best val acc: 0.9157, best train loss: 0.1874  @ epoch 27896 )\n",
      "[Epoch: 28540] train loss: 0.1987, train acc: 0.9179, val loss: 0.2455, val acc: 0.9089  (best train acc: 0.9265, best val acc: 0.9157, best train loss: 0.1874  @ epoch 27896 )\n",
      "[Epoch: 28560] train loss: 0.1870, train acc: 0.9237, val loss: 0.2378, val acc: 0.9066  (best train acc: 0.9265, best val acc: 0.9157, best train loss: 0.1870  @ epoch 28560 )\n",
      "[Epoch: 28580] train loss: 0.2109, train acc: 0.9094, val loss: 0.2421, val acc: 0.9106  (best train acc: 0.9265, best val acc: 0.9157, best train loss: 0.1870  @ epoch 28560 )\n",
      "[Epoch: 28600] train loss: 0.2092, train acc: 0.9138, val loss: 0.2510, val acc: 0.9086  (best train acc: 0.9265, best val acc: 0.9157, best train loss: 0.1870  @ epoch 28560 )\n",
      "[Epoch: 28620] train loss: 0.2065, train acc: 0.9156, val loss: 0.2434, val acc: 0.9096  (best train acc: 0.9265, best val acc: 0.9157, best train loss: 0.1870  @ epoch 28560 )\n",
      "[Epoch: 28640] train loss: 0.2032, train acc: 0.9130, val loss: 0.2493, val acc: 0.9032  (best train acc: 0.9265, best val acc: 0.9157, best train loss: 0.1870  @ epoch 28560 )\n",
      "[Epoch: 28660] train loss: 0.2097, train acc: 0.9163, val loss: 0.2354, val acc: 0.9106  (best train acc: 0.9265, best val acc: 0.9157, best train loss: 0.1870  @ epoch 28560 )\n",
      "[Epoch: 28680] train loss: 0.1917, train acc: 0.9220, val loss: 0.2452, val acc: 0.9083  (best train acc: 0.9265, best val acc: 0.9157, best train loss: 0.1870  @ epoch 28560 )\n",
      "[Epoch: 28700] train loss: 0.2041, train acc: 0.9163, val loss: 0.2359, val acc: 0.9096  (best train acc: 0.9265, best val acc: 0.9157, best train loss: 0.1870  @ epoch 28560 )\n",
      "[Epoch: 28720] train loss: 0.2095, train acc: 0.9172, val loss: 0.2379, val acc: 0.9106  (best train acc: 0.9265, best val acc: 0.9157, best train loss: 0.1870  @ epoch 28560 )\n",
      "[Epoch: 28740] train loss: 0.2122, train acc: 0.9138, val loss: 0.2490, val acc: 0.9039  (best train acc: 0.9265, best val acc: 0.9157, best train loss: 0.1870  @ epoch 28560 )\n",
      "[Epoch: 28760] train loss: 0.2366, train acc: 0.9088, val loss: 0.2448, val acc: 0.9059  (best train acc: 0.9265, best val acc: 0.9157, best train loss: 0.1870  @ epoch 28560 )\n",
      "[Epoch: 28780] train loss: 0.1999, train acc: 0.9179, val loss: 0.2368, val acc: 0.9089  (best train acc: 0.9265, best val acc: 0.9157, best train loss: 0.1870  @ epoch 28560 )\n",
      "[Epoch: 28800] train loss: 0.1950, train acc: 0.9200, val loss: 0.2518, val acc: 0.9056  (best train acc: 0.9265, best val acc: 0.9157, best train loss: 0.1870  @ epoch 28560 )\n",
      "[Epoch: 28820] train loss: 0.2094, train acc: 0.9138, val loss: 0.2486, val acc: 0.9089  (best train acc: 0.9265, best val acc: 0.9157, best train loss: 0.1870  @ epoch 28560 )\n",
      "[Epoch: 28840] train loss: 0.1932, train acc: 0.9234, val loss: 0.2499, val acc: 0.9099  (best train acc: 0.9265, best val acc: 0.9157, best train loss: 0.1870  @ epoch 28560 )\n",
      "[Epoch: 28860] train loss: 0.2080, train acc: 0.9151, val loss: 0.2380, val acc: 0.9079  (best train acc: 0.9265, best val acc: 0.9157, best train loss: 0.1870  @ epoch 28560 )\n",
      "[Epoch: 28880] train loss: 0.2054, train acc: 0.9127, val loss: 0.2471, val acc: 0.9073  (best train acc: 0.9265, best val acc: 0.9157, best train loss: 0.1870  @ epoch 28560 )\n",
      "[Epoch: 28900] train loss: 0.2118, train acc: 0.9135, val loss: 0.2490, val acc: 0.9066  (best train acc: 0.9265, best val acc: 0.9157, best train loss: 0.1870  @ epoch 28560 )\n",
      "[Epoch: 28920] train loss: 0.2055, train acc: 0.9156, val loss: 0.2426, val acc: 0.9083  (best train acc: 0.9265, best val acc: 0.9157, best train loss: 0.1870  @ epoch 28560 )\n",
      "[Epoch: 28940] train loss: 0.2017, train acc: 0.9205, val loss: 0.2488, val acc: 0.9076  (best train acc: 0.9265, best val acc: 0.9157, best train loss: 0.1870  @ epoch 28560 )\n",
      "[Epoch: 28960] train loss: 0.1981, train acc: 0.9157, val loss: 0.2447, val acc: 0.9056  (best train acc: 0.9265, best val acc: 0.9157, best train loss: 0.1858  @ epoch 28956 )\n",
      "[Epoch: 28980] train loss: 0.2093, train acc: 0.9149, val loss: 0.2412, val acc: 0.9086  (best train acc: 0.9265, best val acc: 0.9157, best train loss: 0.1858  @ epoch 28956 )\n",
      "[Epoch: 29000] train loss: 0.1950, train acc: 0.9207, val loss: 0.2487, val acc: 0.9083  (best train acc: 0.9265, best val acc: 0.9157, best train loss: 0.1858  @ epoch 28956 )\n",
      "[Epoch: 29020] train loss: 0.2010, train acc: 0.9144, val loss: 0.2375, val acc: 0.9076  (best train acc: 0.9265, best val acc: 0.9157, best train loss: 0.1858  @ epoch 28956 )\n",
      "[Epoch: 29040] train loss: 0.2361, train acc: 0.9003, val loss: 0.2438, val acc: 0.9099  (best train acc: 0.9265, best val acc: 0.9157, best train loss: 0.1858  @ epoch 28956 )\n",
      "[Epoch: 29060] train loss: 0.1953, train acc: 0.9172, val loss: 0.2520, val acc: 0.9059  (best train acc: 0.9265, best val acc: 0.9157, best train loss: 0.1858  @ epoch 28956 )\n",
      "[Epoch: 29080] train loss: 0.2259, train acc: 0.9057, val loss: 0.2695, val acc: 0.9032  (best train acc: 0.9265, best val acc: 0.9157, best train loss: 0.1858  @ epoch 28956 )\n",
      "[Epoch: 29100] train loss: 0.2002, train acc: 0.9204, val loss: 0.2475, val acc: 0.9089  (best train acc: 0.9265, best val acc: 0.9157, best train loss: 0.1858  @ epoch 28956 )\n",
      "[Epoch: 29120] train loss: 0.1953, train acc: 0.9199, val loss: 0.2513, val acc: 0.9032  (best train acc: 0.9265, best val acc: 0.9157, best train loss: 0.1858  @ epoch 28956 )\n",
      "[Epoch: 29140] train loss: 0.2059, train acc: 0.9144, val loss: 0.2345, val acc: 0.9083  (best train acc: 0.9265, best val acc: 0.9157, best train loss: 0.1858  @ epoch 28956 )\n",
      "[Epoch: 29160] train loss: 0.2102, train acc: 0.9126, val loss: 0.2442, val acc: 0.9056  (best train acc: 0.9265, best val acc: 0.9157, best train loss: 0.1858  @ epoch 28956 )\n",
      "[Epoch: 29180] train loss: 0.1987, train acc: 0.9171, val loss: 0.2421, val acc: 0.9093  (best train acc: 0.9265, best val acc: 0.9157, best train loss: 0.1858  @ epoch 28956 )\n",
      "[Epoch: 29200] train loss: 0.2136, train acc: 0.9121, val loss: 0.2529, val acc: 0.9076  (best train acc: 0.9265, best val acc: 0.9157, best train loss: 0.1858  @ epoch 28956 )\n",
      "[Epoch: 29220] train loss: 0.2105, train acc: 0.9132, val loss: 0.2506, val acc: 0.9086  (best train acc: 0.9265, best val acc: 0.9157, best train loss: 0.1858  @ epoch 28956 )\n",
      "[Epoch: 29240] train loss: 0.2113, train acc: 0.9116, val loss: 0.2490, val acc: 0.9110  (best train acc: 0.9265, best val acc: 0.9157, best train loss: 0.1858  @ epoch 28956 )\n",
      "[Epoch: 29260] train loss: 0.1947, train acc: 0.9209, val loss: 0.2511, val acc: 0.9046  (best train acc: 0.9265, best val acc: 0.9157, best train loss: 0.1858  @ epoch 28956 )\n",
      "[Epoch: 29280] train loss: 0.2259, train acc: 0.9068, val loss: 0.2387, val acc: 0.9086  (best train acc: 0.9265, best val acc: 0.9157, best train loss: 0.1858  @ epoch 28956 )\n",
      "[Epoch: 29300] train loss: 0.2053, train acc: 0.9164, val loss: 0.2610, val acc: 0.9032  (best train acc: 0.9265, best val acc: 0.9157, best train loss: 0.1858  @ epoch 28956 )\n",
      "[Epoch: 29320] train loss: 0.2181, train acc: 0.9114, val loss: 0.2503, val acc: 0.9069  (best train acc: 0.9265, best val acc: 0.9157, best train loss: 0.1858  @ epoch 28956 )\n",
      "[Epoch: 29340] train loss: 0.1965, train acc: 0.9171, val loss: 0.2440, val acc: 0.9073  (best train acc: 0.9265, best val acc: 0.9157, best train loss: 0.1858  @ epoch 28956 )\n",
      "[Epoch: 29360] train loss: 0.2045, train acc: 0.9166, val loss: 0.2463, val acc: 0.9083  (best train acc: 0.9265, best val acc: 0.9157, best train loss: 0.1858  @ epoch 28956 )\n",
      "[Epoch: 29380] train loss: 0.1958, train acc: 0.9218, val loss: 0.2365, val acc: 0.9083  (best train acc: 0.9265, best val acc: 0.9157, best train loss: 0.1858  @ epoch 28956 )\n",
      "[Epoch: 29400] train loss: 0.2389, train acc: 0.8983, val loss: 0.2565, val acc: 0.9076  (best train acc: 0.9265, best val acc: 0.9157, best train loss: 0.1858  @ epoch 28956 )\n",
      "[Epoch: 29420] train loss: 0.2059, train acc: 0.9173, val loss: 0.2439, val acc: 0.9083  (best train acc: 0.9265, best val acc: 0.9157, best train loss: 0.1858  @ epoch 28956 )\n",
      "[Epoch: 29440] train loss: 0.2010, train acc: 0.9180, val loss: 0.2447, val acc: 0.9096  (best train acc: 0.9265, best val acc: 0.9157, best train loss: 0.1858  @ epoch 28956 )\n",
      "[Epoch: 29460] train loss: 0.2113, train acc: 0.9148, val loss: 0.2464, val acc: 0.9093  (best train acc: 0.9265, best val acc: 0.9157, best train loss: 0.1858  @ epoch 28956 )\n",
      "[Epoch: 29480] train loss: 0.2103, train acc: 0.9139, val loss: 0.2573, val acc: 0.9049  (best train acc: 0.9265, best val acc: 0.9157, best train loss: 0.1858  @ epoch 28956 )\n",
      "[Epoch: 29500] train loss: 0.1996, train acc: 0.9190, val loss: 0.2419, val acc: 0.9083  (best train acc: 0.9265, best val acc: 0.9157, best train loss: 0.1858  @ epoch 28956 )\n",
      "[Epoch: 29520] train loss: 0.2092, train acc: 0.9172, val loss: 0.2382, val acc: 0.9073  (best train acc: 0.9265, best val acc: 0.9157, best train loss: 0.1858  @ epoch 28956 )\n",
      "[Epoch: 29540] train loss: 0.2068, train acc: 0.9184, val loss: 0.2479, val acc: 0.9106  (best train acc: 0.9265, best val acc: 0.9157, best train loss: 0.1858  @ epoch 28956 )\n",
      "[Epoch: 29560] train loss: 0.2095, train acc: 0.9162, val loss: 0.2467, val acc: 0.9089  (best train acc: 0.9265, best val acc: 0.9157, best train loss: 0.1858  @ epoch 28956 )\n",
      "[Epoch: 29580] train loss: 0.1924, train acc: 0.9233, val loss: 0.2460, val acc: 0.9079  (best train acc: 0.9265, best val acc: 0.9157, best train loss: 0.1858  @ epoch 28956 )\n",
      "[Epoch: 29600] train loss: 0.2360, train acc: 0.9031, val loss: 0.2490, val acc: 0.9079  (best train acc: 0.9265, best val acc: 0.9157, best train loss: 0.1858  @ epoch 28956 )\n",
      "[Epoch: 29620] train loss: 0.2266, train acc: 0.9083, val loss: 0.2409, val acc: 0.9133  (best train acc: 0.9265, best val acc: 0.9157, best train loss: 0.1858  @ epoch 28956 )\n",
      "[Epoch: 29640] train loss: 0.2040, train acc: 0.9142, val loss: 0.2462, val acc: 0.9059  (best train acc: 0.9265, best val acc: 0.9157, best train loss: 0.1858  @ epoch 28956 )\n",
      "[Epoch: 29660] train loss: 0.2018, train acc: 0.9138, val loss: 0.2389, val acc: 0.9083  (best train acc: 0.9265, best val acc: 0.9157, best train loss: 0.1858  @ epoch 28956 )\n",
      "[Epoch: 29680] train loss: 0.2131, train acc: 0.9137, val loss: 0.2480, val acc: 0.9086  (best train acc: 0.9265, best val acc: 0.9157, best train loss: 0.1858  @ epoch 28956 )\n",
      "[Epoch: 29700] train loss: 0.1956, train acc: 0.9223, val loss: 0.2355, val acc: 0.9116  (best train acc: 0.9265, best val acc: 0.9157, best train loss: 0.1858  @ epoch 28956 )\n",
      "[Epoch: 29720] train loss: 0.1973, train acc: 0.9202, val loss: 0.2459, val acc: 0.9103  (best train acc: 0.9265, best val acc: 0.9157, best train loss: 0.1858  @ epoch 28956 )\n",
      "[Epoch: 29740] train loss: 0.2081, train acc: 0.9118, val loss: 0.2483, val acc: 0.9096  (best train acc: 0.9265, best val acc: 0.9157, best train loss: 0.1858  @ epoch 28956 )\n",
      "[Epoch: 29760] train loss: 0.2005, train acc: 0.9195, val loss: 0.2421, val acc: 0.9076  (best train acc: 0.9265, best val acc: 0.9157, best train loss: 0.1858  @ epoch 28956 )\n",
      "[Epoch: 29780] train loss: 0.1969, train acc: 0.9218, val loss: 0.2413, val acc: 0.9079  (best train acc: 0.9265, best val acc: 0.9157, best train loss: 0.1858  @ epoch 28956 )\n",
      "[Epoch: 29800] train loss: 0.1916, train acc: 0.9210, val loss: 0.2465, val acc: 0.9103  (best train acc: 0.9265, best val acc: 0.9157, best train loss: 0.1858  @ epoch 28956 )\n",
      "[Epoch: 29820] train loss: 0.1910, train acc: 0.9226, val loss: 0.2467, val acc: 0.9083  (best train acc: 0.9265, best val acc: 0.9157, best train loss: 0.1858  @ epoch 28956 )\n",
      "[Epoch: 29840] train loss: 0.1984, train acc: 0.9130, val loss: 0.2471, val acc: 0.9083  (best train acc: 0.9265, best val acc: 0.9157, best train loss: 0.1858  @ epoch 28956 )\n",
      "[Epoch: 29860] train loss: 0.1972, train acc: 0.9206, val loss: 0.2396, val acc: 0.9076  (best train acc: 0.9265, best val acc: 0.9157, best train loss: 0.1858  @ epoch 28956 )\n",
      "[Epoch: 29880] train loss: 0.1991, train acc: 0.9192, val loss: 0.2443, val acc: 0.9096  (best train acc: 0.9265, best val acc: 0.9157, best train loss: 0.1858  @ epoch 28956 )\n",
      "[Epoch: 29900] train loss: 0.1929, train acc: 0.9202, val loss: 0.2475, val acc: 0.9083  (best train acc: 0.9265, best val acc: 0.9157, best train loss: 0.1858  @ epoch 28956 )\n",
      "[Epoch: 29920] train loss: 0.2068, train acc: 0.9109, val loss: 0.2408, val acc: 0.9069  (best train acc: 0.9265, best val acc: 0.9157, best train loss: 0.1858  @ epoch 28956 )\n",
      "[Epoch: 29940] train loss: 0.1973, train acc: 0.9197, val loss: 0.2364, val acc: 0.9096  (best train acc: 0.9265, best val acc: 0.9157, best train loss: 0.1858  @ epoch 28956 )\n",
      "[Epoch: 29960] train loss: 0.1925, train acc: 0.9203, val loss: 0.2523, val acc: 0.9073  (best train acc: 0.9265, best val acc: 0.9157, best train loss: 0.1858  @ epoch 28956 )\n",
      "[Epoch: 29980] train loss: 0.2181, train acc: 0.9075, val loss: 0.2391, val acc: 0.9069  (best train acc: 0.9265, best val acc: 0.9157, best train loss: 0.1858  @ epoch 28956 )\n",
      "[Epoch: 30000] train loss: 0.2024, train acc: 0.9169, val loss: 0.2382, val acc: 0.9052  (best train acc: 0.9265, best val acc: 0.9157, best train loss: 0.1858  @ epoch 28956 )\n",
      "[Epoch: 30020] train loss: 0.1967, train acc: 0.9194, val loss: 0.2438, val acc: 0.9079  (best train acc: 0.9265, best val acc: 0.9157, best train loss: 0.1858  @ epoch 28956 )\n",
      "[Epoch: 30040] train loss: 0.2100, train acc: 0.9130, val loss: 0.2426, val acc: 0.9086  (best train acc: 0.9265, best val acc: 0.9157, best train loss: 0.1858  @ epoch 28956 )\n",
      "[Epoch: 30060] train loss: 0.1966, train acc: 0.9208, val loss: 0.2391, val acc: 0.9093  (best train acc: 0.9267, best val acc: 0.9157, best train loss: 0.1822  @ epoch 30050 )\n",
      "[Epoch: 30080] train loss: 0.2044, train acc: 0.9159, val loss: 0.2485, val acc: 0.9049  (best train acc: 0.9267, best val acc: 0.9157, best train loss: 0.1822  @ epoch 30050 )\n",
      "[Epoch: 30100] train loss: 0.2032, train acc: 0.9169, val loss: 0.2393, val acc: 0.9089  (best train acc: 0.9267, best val acc: 0.9157, best train loss: 0.1822  @ epoch 30050 )\n",
      "[Epoch: 30120] train loss: 0.2008, train acc: 0.9182, val loss: 0.2483, val acc: 0.9089  (best train acc: 0.9267, best val acc: 0.9157, best train loss: 0.1822  @ epoch 30050 )\n",
      "[Epoch: 30140] train loss: 0.1972, train acc: 0.9196, val loss: 0.2349, val acc: 0.9083  (best train acc: 0.9267, best val acc: 0.9157, best train loss: 0.1822  @ epoch 30050 )\n",
      "[Epoch: 30160] train loss: 0.2101, train acc: 0.9130, val loss: 0.2483, val acc: 0.9042  (best train acc: 0.9267, best val acc: 0.9157, best train loss: 0.1822  @ epoch 30050 )\n",
      "[Epoch: 30180] train loss: 0.2028, train acc: 0.9136, val loss: 0.2446, val acc: 0.9066  (best train acc: 0.9267, best val acc: 0.9157, best train loss: 0.1822  @ epoch 30050 )\n",
      "[Epoch: 30200] train loss: 0.2174, train acc: 0.9114, val loss: 0.2414, val acc: 0.9073  (best train acc: 0.9267, best val acc: 0.9157, best train loss: 0.1822  @ epoch 30050 )\n",
      "[Epoch: 30220] train loss: 0.1997, train acc: 0.9168, val loss: 0.2447, val acc: 0.9076  (best train acc: 0.9267, best val acc: 0.9157, best train loss: 0.1822  @ epoch 30050 )\n",
      "[Epoch: 30240] train loss: 0.2085, train acc: 0.9173, val loss: 0.2584, val acc: 0.8998  (best train acc: 0.9267, best val acc: 0.9157, best train loss: 0.1822  @ epoch 30050 )\n",
      "[Epoch: 30260] train loss: 0.1995, train acc: 0.9199, val loss: 0.2524, val acc: 0.9069  (best train acc: 0.9267, best val acc: 0.9157, best train loss: 0.1822  @ epoch 30050 )\n",
      "[Epoch: 30280] train loss: 0.2001, train acc: 0.9190, val loss: 0.2387, val acc: 0.9073  (best train acc: 0.9267, best val acc: 0.9157, best train loss: 0.1822  @ epoch 30050 )\n",
      "[Epoch: 30300] train loss: 0.2294, train acc: 0.9062, val loss: 0.2352, val acc: 0.9076  (best train acc: 0.9267, best val acc: 0.9157, best train loss: 0.1822  @ epoch 30050 )\n",
      "[Epoch: 30320] train loss: 0.1910, train acc: 0.9199, val loss: 0.2468, val acc: 0.9066  (best train acc: 0.9267, best val acc: 0.9157, best train loss: 0.1822  @ epoch 30050 )\n",
      "[Epoch: 30340] train loss: 0.2024, train acc: 0.9156, val loss: 0.2444, val acc: 0.9116  (best train acc: 0.9267, best val acc: 0.9157, best train loss: 0.1822  @ epoch 30050 )\n",
      "[Epoch: 30360] train loss: 0.2148, train acc: 0.9081, val loss: 0.2504, val acc: 0.9096  (best train acc: 0.9267, best val acc: 0.9157, best train loss: 0.1822  @ epoch 30050 )\n",
      "[Epoch: 30380] train loss: 0.2044, train acc: 0.9123, val loss: 0.2351, val acc: 0.9083  (best train acc: 0.9267, best val acc: 0.9157, best train loss: 0.1822  @ epoch 30050 )\n",
      "[Epoch: 30400] train loss: 0.1941, train acc: 0.9216, val loss: 0.2385, val acc: 0.9086  (best train acc: 0.9267, best val acc: 0.9157, best train loss: 0.1822  @ epoch 30050 )\n",
      "[Epoch: 30420] train loss: 0.1992, train acc: 0.9181, val loss: 0.2406, val acc: 0.9089  (best train acc: 0.9267, best val acc: 0.9157, best train loss: 0.1822  @ epoch 30050 )\n",
      "[Epoch: 30440] train loss: 0.1949, train acc: 0.9190, val loss: 0.2446, val acc: 0.9096  (best train acc: 0.9267, best val acc: 0.9157, best train loss: 0.1822  @ epoch 30050 )\n",
      "[Epoch: 30460] train loss: 0.1984, train acc: 0.9160, val loss: 0.2335, val acc: 0.9086  (best train acc: 0.9267, best val acc: 0.9157, best train loss: 0.1822  @ epoch 30050 )\n",
      "[Epoch: 30480] train loss: 0.1994, train acc: 0.9203, val loss: 0.2494, val acc: 0.9099  (best train acc: 0.9267, best val acc: 0.9157, best train loss: 0.1822  @ epoch 30050 )\n",
      "[Epoch: 30500] train loss: 0.1961, train acc: 0.9211, val loss: 0.2413, val acc: 0.9089  (best train acc: 0.9267, best val acc: 0.9157, best train loss: 0.1822  @ epoch 30050 )\n",
      "[Epoch: 30520] train loss: 0.2100, train acc: 0.9104, val loss: 0.2582, val acc: 0.9046  (best train acc: 0.9267, best val acc: 0.9157, best train loss: 0.1822  @ epoch 30050 )\n",
      "[Epoch: 30540] train loss: 0.1985, train acc: 0.9203, val loss: 0.2356, val acc: 0.9093  (best train acc: 0.9267, best val acc: 0.9157, best train loss: 0.1822  @ epoch 30050 )\n",
      "[Epoch: 30560] train loss: 0.1925, train acc: 0.9220, val loss: 0.2383, val acc: 0.9073  (best train acc: 0.9267, best val acc: 0.9157, best train loss: 0.1822  @ epoch 30050 )\n",
      "[Epoch: 30580] train loss: 0.1975, train acc: 0.9217, val loss: 0.2364, val acc: 0.9056  (best train acc: 0.9267, best val acc: 0.9157, best train loss: 0.1822  @ epoch 30050 )\n",
      "[Epoch: 30600] train loss: 0.2060, train acc: 0.9101, val loss: 0.2363, val acc: 0.9086  (best train acc: 0.9267, best val acc: 0.9157, best train loss: 0.1822  @ epoch 30050 )\n",
      "[Epoch: 30620] train loss: 0.2130, train acc: 0.9171, val loss: 0.2581, val acc: 0.9079  (best train acc: 0.9267, best val acc: 0.9157, best train loss: 0.1822  @ epoch 30050 )\n",
      "[Epoch: 30640] train loss: 0.2045, train acc: 0.9117, val loss: 0.2398, val acc: 0.9083  (best train acc: 0.9267, best val acc: 0.9157, best train loss: 0.1822  @ epoch 30050 )\n",
      "[Epoch: 30660] train loss: 0.2002, train acc: 0.9191, val loss: 0.2496, val acc: 0.9079  (best train acc: 0.9267, best val acc: 0.9157, best train loss: 0.1822  @ epoch 30050 )\n",
      "[Epoch: 30680] train loss: 0.2062, train acc: 0.9167, val loss: 0.2562, val acc: 0.9052  (best train acc: 0.9267, best val acc: 0.9157, best train loss: 0.1822  @ epoch 30050 )\n",
      "[Epoch: 30700] train loss: 0.1948, train acc: 0.9211, val loss: 0.2484, val acc: 0.9059  (best train acc: 0.9267, best val acc: 0.9157, best train loss: 0.1822  @ epoch 30050 )\n",
      "[Epoch: 30720] train loss: 0.2043, train acc: 0.9153, val loss: 0.2402, val acc: 0.9073  (best train acc: 0.9267, best val acc: 0.9157, best train loss: 0.1822  @ epoch 30050 )\n",
      "[Epoch: 30740] train loss: 0.1981, train acc: 0.9170, val loss: 0.2451, val acc: 0.9059  (best train acc: 0.9267, best val acc: 0.9157, best train loss: 0.1822  @ epoch 30050 )\n",
      "[Epoch: 30760] train loss: 0.2120, train acc: 0.9127, val loss: 0.2694, val acc: 0.8992  (best train acc: 0.9267, best val acc: 0.9157, best train loss: 0.1822  @ epoch 30050 )\n",
      "[Epoch: 30780] train loss: 0.2043, train acc: 0.9186, val loss: 0.2378, val acc: 0.9049  (best train acc: 0.9267, best val acc: 0.9157, best train loss: 0.1822  @ epoch 30050 )\n",
      "[Epoch: 30800] train loss: 0.2162, train acc: 0.9083, val loss: 0.2573, val acc: 0.9062  (best train acc: 0.9267, best val acc: 0.9157, best train loss: 0.1822  @ epoch 30050 )\n",
      "[Epoch: 30820] train loss: 0.1897, train acc: 0.9223, val loss: 0.2452, val acc: 0.9073  (best train acc: 0.9267, best val acc: 0.9157, best train loss: 0.1822  @ epoch 30050 )\n",
      "[Epoch: 30840] train loss: 0.2009, train acc: 0.9166, val loss: 0.2457, val acc: 0.9073  (best train acc: 0.9267, best val acc: 0.9157, best train loss: 0.1822  @ epoch 30050 )\n",
      "[Epoch: 30860] train loss: 0.2042, train acc: 0.9137, val loss: 0.2321, val acc: 0.9076  (best train acc: 0.9267, best val acc: 0.9157, best train loss: 0.1822  @ epoch 30050 )\n",
      "[Epoch: 30880] train loss: 0.2028, train acc: 0.9163, val loss: 0.2374, val acc: 0.9079  (best train acc: 0.9267, best val acc: 0.9157, best train loss: 0.1822  @ epoch 30050 )\n",
      "[Epoch: 30900] train loss: 0.1975, train acc: 0.9174, val loss: 0.2364, val acc: 0.9083  (best train acc: 0.9267, best val acc: 0.9157, best train loss: 0.1822  @ epoch 30050 )\n",
      "[Epoch: 30920] train loss: 0.1975, train acc: 0.9179, val loss: 0.2522, val acc: 0.9069  (best train acc: 0.9276, best val acc: 0.9157, best train loss: 0.1822  @ epoch 30050 )\n",
      "[Epoch: 30940] train loss: 0.1997, train acc: 0.9161, val loss: 0.2371, val acc: 0.9056  (best train acc: 0.9276, best val acc: 0.9157, best train loss: 0.1822  @ epoch 30050 )\n",
      "[Epoch: 30960] train loss: 0.2063, train acc: 0.9130, val loss: 0.2409, val acc: 0.9066  (best train acc: 0.9276, best val acc: 0.9157, best train loss: 0.1822  @ epoch 30050 )\n",
      "[Epoch: 30980] train loss: 0.2206, train acc: 0.9084, val loss: 0.2758, val acc: 0.9015  (best train acc: 0.9276, best val acc: 0.9157, best train loss: 0.1822  @ epoch 30050 )\n",
      "[Epoch: 31000] train loss: 0.2005, train acc: 0.9201, val loss: 0.2562, val acc: 0.9008  (best train acc: 0.9276, best val acc: 0.9157, best train loss: 0.1822  @ epoch 30050 )\n",
      "[Epoch: 31020] train loss: 0.1915, train acc: 0.9218, val loss: 0.2478, val acc: 0.9083  (best train acc: 0.9276, best val acc: 0.9157, best train loss: 0.1822  @ epoch 30050 )\n",
      "[Epoch: 31040] train loss: 0.2001, train acc: 0.9161, val loss: 0.2570, val acc: 0.9083  (best train acc: 0.9276, best val acc: 0.9157, best train loss: 0.1822  @ epoch 30050 )\n",
      "[Epoch: 31060] train loss: 0.1999, train acc: 0.9167, val loss: 0.2443, val acc: 0.9083  (best train acc: 0.9276, best val acc: 0.9157, best train loss: 0.1822  @ epoch 30050 )\n",
      "[Epoch: 31080] train loss: 0.2054, train acc: 0.9172, val loss: 0.2466, val acc: 0.9056  (best train acc: 0.9276, best val acc: 0.9157, best train loss: 0.1822  @ epoch 30050 )\n",
      "[Epoch: 31100] train loss: 0.1924, train acc: 0.9211, val loss: 0.2487, val acc: 0.9069  (best train acc: 0.9276, best val acc: 0.9157, best train loss: 0.1822  @ epoch 30050 )\n",
      "[Epoch: 31120] train loss: 0.1920, train acc: 0.9240, val loss: 0.2624, val acc: 0.9059  (best train acc: 0.9276, best val acc: 0.9157, best train loss: 0.1822  @ epoch 30050 )\n",
      "[Epoch: 31140] train loss: 0.2181, train acc: 0.9152, val loss: 0.2401, val acc: 0.9069  (best train acc: 0.9276, best val acc: 0.9157, best train loss: 0.1822  @ epoch 30050 )\n",
      "[Epoch: 31160] train loss: 0.1959, train acc: 0.9199, val loss: 0.2390, val acc: 0.9069  (best train acc: 0.9276, best val acc: 0.9157, best train loss: 0.1822  @ epoch 30050 )\n",
      "[Epoch: 31180] train loss: 0.1972, train acc: 0.9186, val loss: 0.2418, val acc: 0.9062  (best train acc: 0.9276, best val acc: 0.9157, best train loss: 0.1822  @ epoch 30050 )\n",
      "[Epoch: 31200] train loss: 0.2209, train acc: 0.9057, val loss: 0.2457, val acc: 0.9073  (best train acc: 0.9276, best val acc: 0.9157, best train loss: 0.1822  @ epoch 30050 )\n",
      "[Epoch: 31220] train loss: 0.2058, train acc: 0.9153, val loss: 0.2559, val acc: 0.9059  (best train acc: 0.9276, best val acc: 0.9157, best train loss: 0.1822  @ epoch 30050 )\n",
      "[Epoch: 31240] train loss: 0.2027, train acc: 0.9182, val loss: 0.2443, val acc: 0.9049  (best train acc: 0.9276, best val acc: 0.9157, best train loss: 0.1822  @ epoch 30050 )\n",
      "[Epoch: 31260] train loss: 0.1922, train acc: 0.9195, val loss: 0.2371, val acc: 0.9086  (best train acc: 0.9276, best val acc: 0.9157, best train loss: 0.1822  @ epoch 30050 )\n",
      "[Epoch: 31280] train loss: 0.2106, train acc: 0.9142, val loss: 0.2360, val acc: 0.9029  (best train acc: 0.9276, best val acc: 0.9157, best train loss: 0.1822  @ epoch 30050 )\n",
      "[Epoch: 31300] train loss: 0.1840, train acc: 0.9247, val loss: 0.2455, val acc: 0.9035  (best train acc: 0.9276, best val acc: 0.9157, best train loss: 0.1822  @ epoch 30050 )\n",
      "[Epoch: 31320] train loss: 0.2082, train acc: 0.9166, val loss: 0.2501, val acc: 0.9035  (best train acc: 0.9276, best val acc: 0.9157, best train loss: 0.1822  @ epoch 30050 )\n",
      "[Epoch: 31340] train loss: 0.2084, train acc: 0.9132, val loss: 0.2436, val acc: 0.9069  (best train acc: 0.9276, best val acc: 0.9157, best train loss: 0.1822  @ epoch 30050 )\n",
      "[Epoch: 31360] train loss: 0.2111, train acc: 0.9169, val loss: 0.2345, val acc: 0.9096  (best train acc: 0.9276, best val acc: 0.9157, best train loss: 0.1822  @ epoch 30050 )\n",
      "[Epoch: 31380] train loss: 0.1941, train acc: 0.9199, val loss: 0.2316, val acc: 0.9093  (best train acc: 0.9276, best val acc: 0.9157, best train loss: 0.1822  @ epoch 30050 )\n",
      "[Epoch: 31400] train loss: 0.1949, train acc: 0.9191, val loss: 0.2594, val acc: 0.9062  (best train acc: 0.9276, best val acc: 0.9157, best train loss: 0.1822  @ epoch 30050 )\n",
      "[Epoch: 31420] train loss: 0.2166, train acc: 0.9136, val loss: 0.2505, val acc: 0.9066  (best train acc: 0.9276, best val acc: 0.9157, best train loss: 0.1822  @ epoch 30050 )\n",
      "[Epoch: 31440] train loss: 0.2087, train acc: 0.9117, val loss: 0.2435, val acc: 0.9076  (best train acc: 0.9276, best val acc: 0.9157, best train loss: 0.1822  @ epoch 30050 )\n",
      "[Epoch: 31460] train loss: 0.1989, train acc: 0.9151, val loss: 0.2405, val acc: 0.9079  (best train acc: 0.9276, best val acc: 0.9157, best train loss: 0.1822  @ epoch 30050 )\n",
      "[Epoch: 31480] train loss: 0.1849, train acc: 0.9225, val loss: 0.2413, val acc: 0.9076  (best train acc: 0.9276, best val acc: 0.9157, best train loss: 0.1822  @ epoch 30050 )\n",
      "[Epoch: 31500] train loss: 0.2003, train acc: 0.9169, val loss: 0.2452, val acc: 0.9079  (best train acc: 0.9276, best val acc: 0.9157, best train loss: 0.1822  @ epoch 30050 )\n",
      "[Epoch: 31520] train loss: 0.2032, train acc: 0.9160, val loss: 0.2470, val acc: 0.9130  (best train acc: 0.9276, best val acc: 0.9157, best train loss: 0.1822  @ epoch 30050 )\n",
      "[Epoch: 31540] train loss: 0.1939, train acc: 0.9179, val loss: 0.2378, val acc: 0.9079  (best train acc: 0.9276, best val acc: 0.9157, best train loss: 0.1822  @ epoch 30050 )\n",
      "[Epoch: 31560] train loss: 0.1938, train acc: 0.9189, val loss: 0.2487, val acc: 0.9103  (best train acc: 0.9276, best val acc: 0.9157, best train loss: 0.1822  @ epoch 30050 )\n",
      "[Epoch: 31580] train loss: 0.1947, train acc: 0.9203, val loss: 0.2623, val acc: 0.9076  (best train acc: 0.9276, best val acc: 0.9157, best train loss: 0.1822  @ epoch 31567 )\n",
      "[Epoch: 31600] train loss: 0.2053, train acc: 0.9161, val loss: 0.2482, val acc: 0.9052  (best train acc: 0.9276, best val acc: 0.9157, best train loss: 0.1822  @ epoch 31567 )\n",
      "[Epoch: 31620] train loss: 0.1920, train acc: 0.9202, val loss: 0.2414, val acc: 0.9069  (best train acc: 0.9276, best val acc: 0.9157, best train loss: 0.1822  @ epoch 31567 )\n",
      "[Epoch: 31640] train loss: 0.1981, train acc: 0.9173, val loss: 0.2429, val acc: 0.9113  (best train acc: 0.9276, best val acc: 0.9157, best train loss: 0.1822  @ epoch 31567 )\n",
      "[Epoch: 31660] train loss: 0.2002, train acc: 0.9201, val loss: 0.2366, val acc: 0.9076  (best train acc: 0.9276, best val acc: 0.9157, best train loss: 0.1822  @ epoch 31567 )\n",
      "[Epoch: 31680] train loss: 0.1898, train acc: 0.9237, val loss: 0.2491, val acc: 0.9066  (best train acc: 0.9276, best val acc: 0.9157, best train loss: 0.1822  @ epoch 31567 )\n",
      "[Epoch: 31700] train loss: 0.1955, train acc: 0.9159, val loss: 0.2375, val acc: 0.9083  (best train acc: 0.9276, best val acc: 0.9157, best train loss: 0.1822  @ epoch 31567 )\n",
      "[Epoch: 31720] train loss: 0.1937, train acc: 0.9188, val loss: 0.2392, val acc: 0.9083  (best train acc: 0.9276, best val acc: 0.9157, best train loss: 0.1822  @ epoch 31567 )\n",
      "[Epoch: 31740] train loss: 0.1957, train acc: 0.9169, val loss: 0.2416, val acc: 0.9089  (best train acc: 0.9276, best val acc: 0.9157, best train loss: 0.1822  @ epoch 31567 )\n",
      "[Epoch: 31760] train loss: 0.2039, train acc: 0.9166, val loss: 0.2486, val acc: 0.9093  (best train acc: 0.9276, best val acc: 0.9157, best train loss: 0.1822  @ epoch 31567 )\n",
      "[Epoch: 31780] train loss: 0.2109, train acc: 0.9107, val loss: 0.2491, val acc: 0.9073  (best train acc: 0.9276, best val acc: 0.9157, best train loss: 0.1822  @ epoch 31567 )\n",
      "[Epoch: 31800] train loss: 0.1895, train acc: 0.9257, val loss: 0.2415, val acc: 0.9052  (best train acc: 0.9276, best val acc: 0.9157, best train loss: 0.1822  @ epoch 31567 )\n",
      "[Epoch: 31820] train loss: 0.2162, train acc: 0.9100, val loss: 0.2312, val acc: 0.9083  (best train acc: 0.9276, best val acc: 0.9157, best train loss: 0.1822  @ epoch 31567 )\n",
      "[Epoch: 31840] train loss: 0.1940, train acc: 0.9203, val loss: 0.2472, val acc: 0.9076  (best train acc: 0.9276, best val acc: 0.9157, best train loss: 0.1822  @ epoch 31567 )\n",
      "[Epoch: 31860] train loss: 0.2147, train acc: 0.9107, val loss: 0.2332, val acc: 0.9083  (best train acc: 0.9276, best val acc: 0.9157, best train loss: 0.1822  @ epoch 31567 )\n",
      "[Epoch: 31880] train loss: 0.1898, train acc: 0.9202, val loss: 0.2395, val acc: 0.9039  (best train acc: 0.9276, best val acc: 0.9157, best train loss: 0.1822  @ epoch 31567 )\n",
      "[Epoch: 31900] train loss: 0.2033, train acc: 0.9215, val loss: 0.2396, val acc: 0.9086  (best train acc: 0.9276, best val acc: 0.9157, best train loss: 0.1822  @ epoch 31567 )\n",
      "[Epoch: 31920] train loss: 0.2043, train acc: 0.9120, val loss: 0.2325, val acc: 0.9076  (best train acc: 0.9276, best val acc: 0.9157, best train loss: 0.1822  @ epoch 31567 )\n",
      "[Epoch: 31940] train loss: 0.1986, train acc: 0.9167, val loss: 0.2389, val acc: 0.9089  (best train acc: 0.9276, best val acc: 0.9157, best train loss: 0.1822  @ epoch 31567 )\n",
      "[Epoch: 31960] train loss: 0.1977, train acc: 0.9189, val loss: 0.2520, val acc: 0.9069  (best train acc: 0.9276, best val acc: 0.9157, best train loss: 0.1822  @ epoch 31567 )\n",
      "[Epoch: 31980] train loss: 0.1984, train acc: 0.9216, val loss: 0.2354, val acc: 0.9093  (best train acc: 0.9276, best val acc: 0.9157, best train loss: 0.1822  @ epoch 31567 )\n",
      "[Epoch: 32000] train loss: 0.2073, train acc: 0.9152, val loss: 0.2279, val acc: 0.9116  (best train acc: 0.9276, best val acc: 0.9157, best train loss: 0.1822  @ epoch 31567 )\n",
      "[Epoch: 32020] train loss: 0.2006, train acc: 0.9177, val loss: 0.2423, val acc: 0.9052  (best train acc: 0.9276, best val acc: 0.9157, best train loss: 0.1822  @ epoch 31567 )\n",
      "[Epoch: 32040] train loss: 0.2092, train acc: 0.9121, val loss: 0.2390, val acc: 0.9086  (best train acc: 0.9276, best val acc: 0.9157, best train loss: 0.1822  @ epoch 31567 )\n",
      "[Epoch: 32060] train loss: 0.1922, train acc: 0.9216, val loss: 0.2352, val acc: 0.9079  (best train acc: 0.9276, best val acc: 0.9157, best train loss: 0.1820  @ epoch 32050 )\n",
      "[Epoch: 32080] train loss: 0.2210, train acc: 0.9102, val loss: 0.2398, val acc: 0.9069  (best train acc: 0.9276, best val acc: 0.9157, best train loss: 0.1820  @ epoch 32050 )\n",
      "[Epoch: 32100] train loss: 0.2092, train acc: 0.9161, val loss: 0.2400, val acc: 0.9076  (best train acc: 0.9276, best val acc: 0.9157, best train loss: 0.1820  @ epoch 32050 )\n",
      "[Epoch: 32120] train loss: 0.2049, train acc: 0.9158, val loss: 0.2515, val acc: 0.9059  (best train acc: 0.9276, best val acc: 0.9157, best train loss: 0.1820  @ epoch 32050 )\n",
      "[Epoch: 32140] train loss: 0.2068, train acc: 0.9146, val loss: 0.2536, val acc: 0.9049  (best train acc: 0.9276, best val acc: 0.9157, best train loss: 0.1820  @ epoch 32050 )\n",
      "[Epoch: 32160] train loss: 0.1960, train acc: 0.9182, val loss: 0.2552, val acc: 0.9046  (best train acc: 0.9276, best val acc: 0.9157, best train loss: 0.1820  @ epoch 32050 )\n",
      "[Epoch: 32180] train loss: 0.2021, train acc: 0.9148, val loss: 0.2336, val acc: 0.9089  (best train acc: 0.9276, best val acc: 0.9157, best train loss: 0.1820  @ epoch 32050 )\n",
      "[Epoch: 32200] train loss: 0.1834, train acc: 0.9255, val loss: 0.2499, val acc: 0.9062  (best train acc: 0.9276, best val acc: 0.9157, best train loss: 0.1820  @ epoch 32050 )\n",
      "[Epoch: 32220] train loss: 0.2187, train acc: 0.9072, val loss: 0.2579, val acc: 0.9035  (best train acc: 0.9286, best val acc: 0.9157, best train loss: 0.1818  @ epoch 32202 )\n",
      "[Epoch: 32240] train loss: 0.2086, train acc: 0.9107, val loss: 0.2505, val acc: 0.9076  (best train acc: 0.9286, best val acc: 0.9157, best train loss: 0.1818  @ epoch 32202 )\n",
      "[Epoch: 32260] train loss: 0.1938, train acc: 0.9222, val loss: 0.2511, val acc: 0.9076  (best train acc: 0.9286, best val acc: 0.9157, best train loss: 0.1818  @ epoch 32202 )\n",
      "[Epoch: 32280] train loss: 0.2026, train acc: 0.9140, val loss: 0.2453, val acc: 0.9096  (best train acc: 0.9286, best val acc: 0.9157, best train loss: 0.1818  @ epoch 32202 )\n",
      "[Epoch: 32300] train loss: 0.2060, train acc: 0.9163, val loss: 0.2498, val acc: 0.9046  (best train acc: 0.9286, best val acc: 0.9157, best train loss: 0.1818  @ epoch 32202 )\n",
      "[Epoch: 32320] train loss: 0.2038, train acc: 0.9148, val loss: 0.2327, val acc: 0.9106  (best train acc: 0.9286, best val acc: 0.9157, best train loss: 0.1818  @ epoch 32202 )\n",
      "[Epoch: 32340] train loss: 0.1905, train acc: 0.9225, val loss: 0.2405, val acc: 0.9076  (best train acc: 0.9286, best val acc: 0.9157, best train loss: 0.1818  @ epoch 32202 )\n",
      "[Epoch: 32360] train loss: 0.1886, train acc: 0.9241, val loss: 0.2345, val acc: 0.9103  (best train acc: 0.9286, best val acc: 0.9157, best train loss: 0.1818  @ epoch 32202 )\n",
      "[Epoch: 32380] train loss: 0.1985, train acc: 0.9199, val loss: 0.2470, val acc: 0.9056  (best train acc: 0.9286, best val acc: 0.9157, best train loss: 0.1818  @ epoch 32202 )\n",
      "[Epoch: 32400] train loss: 0.1900, train acc: 0.9243, val loss: 0.2370, val acc: 0.9073  (best train acc: 0.9286, best val acc: 0.9157, best train loss: 0.1818  @ epoch 32202 )\n",
      "[Epoch: 32420] train loss: 0.2161, train acc: 0.9111, val loss: 0.2395, val acc: 0.9056  (best train acc: 0.9286, best val acc: 0.9157, best train loss: 0.1818  @ epoch 32202 )\n",
      "[Epoch: 32440] train loss: 0.1920, train acc: 0.9192, val loss: 0.2400, val acc: 0.9073  (best train acc: 0.9286, best val acc: 0.9157, best train loss: 0.1818  @ epoch 32202 )\n",
      "[Epoch: 32460] train loss: 0.1918, train acc: 0.9227, val loss: 0.2456, val acc: 0.9089  (best train acc: 0.9286, best val acc: 0.9157, best train loss: 0.1818  @ epoch 32202 )\n",
      "[Epoch: 32480] train loss: 0.1848, train acc: 0.9247, val loss: 0.2406, val acc: 0.9110  (best train acc: 0.9286, best val acc: 0.9157, best train loss: 0.1818  @ epoch 32202 )\n",
      "[Epoch: 32500] train loss: 0.1898, train acc: 0.9215, val loss: 0.2407, val acc: 0.9106  (best train acc: 0.9286, best val acc: 0.9157, best train loss: 0.1818  @ epoch 32202 )\n",
      "[Epoch: 32520] train loss: 0.1909, train acc: 0.9247, val loss: 0.2372, val acc: 0.9076  (best train acc: 0.9286, best val acc: 0.9157, best train loss: 0.1818  @ epoch 32202 )\n",
      "[Epoch: 32540] train loss: 0.2125, train acc: 0.9136, val loss: 0.2458, val acc: 0.9052  (best train acc: 0.9286, best val acc: 0.9157, best train loss: 0.1818  @ epoch 32202 )\n",
      "[Epoch: 32560] train loss: 0.2062, train acc: 0.9175, val loss: 0.2454, val acc: 0.9096  (best train acc: 0.9286, best val acc: 0.9157, best train loss: 0.1818  @ epoch 32202 )\n",
      "[Epoch: 32580] train loss: 0.2103, train acc: 0.9143, val loss: 0.2416, val acc: 0.9062  (best train acc: 0.9286, best val acc: 0.9157, best train loss: 0.1818  @ epoch 32202 )\n",
      "[Epoch: 32600] train loss: 0.1893, train acc: 0.9218, val loss: 0.2357, val acc: 0.9099  (best train acc: 0.9286, best val acc: 0.9157, best train loss: 0.1818  @ epoch 32202 )\n",
      "[Epoch: 32620] train loss: 0.1986, train acc: 0.9170, val loss: 0.2436, val acc: 0.9056  (best train acc: 0.9286, best val acc: 0.9157, best train loss: 0.1818  @ epoch 32202 )\n",
      "[Epoch: 32640] train loss: 0.2439, train acc: 0.9049, val loss: 0.2480, val acc: 0.9083  (best train acc: 0.9286, best val acc: 0.9157, best train loss: 0.1818  @ epoch 32202 )\n",
      "[Epoch: 32660] train loss: 0.2149, train acc: 0.9050, val loss: 0.2636, val acc: 0.9052  (best train acc: 0.9286, best val acc: 0.9157, best train loss: 0.1818  @ epoch 32202 )\n",
      "[Epoch: 32680] train loss: 0.2164, train acc: 0.9079, val loss: 0.2380, val acc: 0.9066  (best train acc: 0.9286, best val acc: 0.9157, best train loss: 0.1818  @ epoch 32202 )\n",
      "[Epoch: 32700] train loss: 0.2107, train acc: 0.9150, val loss: 0.2558, val acc: 0.9042  (best train acc: 0.9286, best val acc: 0.9157, best train loss: 0.1818  @ epoch 32202 )\n",
      "[Epoch: 32720] train loss: 0.1942, train acc: 0.9203, val loss: 0.2349, val acc: 0.9073  (best train acc: 0.9286, best val acc: 0.9157, best train loss: 0.1818  @ epoch 32202 )\n",
      "[Epoch: 32740] train loss: 0.2108, train acc: 0.9168, val loss: 0.2523, val acc: 0.9069  (best train acc: 0.9286, best val acc: 0.9157, best train loss: 0.1818  @ epoch 32202 )\n",
      "[Epoch: 32760] train loss: 0.1893, train acc: 0.9228, val loss: 0.2356, val acc: 0.9093  (best train acc: 0.9286, best val acc: 0.9157, best train loss: 0.1818  @ epoch 32202 )\n",
      "[Epoch: 32780] train loss: 0.1970, train acc: 0.9195, val loss: 0.2560, val acc: 0.9103  (best train acc: 0.9286, best val acc: 0.9157, best train loss: 0.1818  @ epoch 32202 )\n",
      "[Epoch: 32800] train loss: 0.2028, train acc: 0.9189, val loss: 0.2490, val acc: 0.9086  (best train acc: 0.9286, best val acc: 0.9157, best train loss: 0.1818  @ epoch 32202 )\n",
      "[Epoch: 32820] train loss: 0.1954, train acc: 0.9189, val loss: 0.2367, val acc: 0.9086  (best train acc: 0.9286, best val acc: 0.9157, best train loss: 0.1818  @ epoch 32202 )\n",
      "[Epoch: 32840] train loss: 0.1967, train acc: 0.9197, val loss: 0.2539, val acc: 0.9052  (best train acc: 0.9286, best val acc: 0.9157, best train loss: 0.1818  @ epoch 32202 )\n",
      "[Epoch: 32860] train loss: 0.1983, train acc: 0.9211, val loss: 0.2397, val acc: 0.9079  (best train acc: 0.9286, best val acc: 0.9157, best train loss: 0.1818  @ epoch 32202 )\n",
      "[Epoch: 32880] train loss: 0.1906, train acc: 0.9208, val loss: 0.2327, val acc: 0.9110  (best train acc: 0.9286, best val acc: 0.9157, best train loss: 0.1818  @ epoch 32202 )\n",
      "[Epoch: 32900] train loss: 0.1936, train acc: 0.9182, val loss: 0.2393, val acc: 0.9103  (best train acc: 0.9286, best val acc: 0.9157, best train loss: 0.1818  @ epoch 32202 )\n",
      "[Epoch: 32920] train loss: 0.2299, train acc: 0.9074, val loss: 0.2379, val acc: 0.9046  (best train acc: 0.9286, best val acc: 0.9157, best train loss: 0.1818  @ epoch 32202 )\n",
      "[Epoch: 32940] train loss: 0.1933, train acc: 0.9191, val loss: 0.2346, val acc: 0.9079  (best train acc: 0.9286, best val acc: 0.9157, best train loss: 0.1818  @ epoch 32202 )\n",
      "[Epoch: 32960] train loss: 0.1887, train acc: 0.9230, val loss: 0.2500, val acc: 0.9079  (best train acc: 0.9286, best val acc: 0.9157, best train loss: 0.1818  @ epoch 32202 )\n",
      "[Epoch: 32980] train loss: 0.1957, train acc: 0.9210, val loss: 0.2422, val acc: 0.9089  (best train acc: 0.9286, best val acc: 0.9157, best train loss: 0.1818  @ epoch 32202 )\n",
      "[Epoch: 33000] train loss: 0.1908, train acc: 0.9218, val loss: 0.2473, val acc: 0.9062  (best train acc: 0.9286, best val acc: 0.9157, best train loss: 0.1818  @ epoch 32202 )\n",
      "[Epoch: 33020] train loss: 0.1887, train acc: 0.9237, val loss: 0.2353, val acc: 0.9089  (best train acc: 0.9286, best val acc: 0.9157, best train loss: 0.1818  @ epoch 32202 )\n",
      "[Epoch: 33040] train loss: 0.2013, train acc: 0.9182, val loss: 0.2404, val acc: 0.9123  (best train acc: 0.9286, best val acc: 0.9157, best train loss: 0.1818  @ epoch 32202 )\n",
      "[Epoch: 33060] train loss: 0.2524, train acc: 0.8937, val loss: 0.2566, val acc: 0.9029  (best train acc: 0.9286, best val acc: 0.9157, best train loss: 0.1818  @ epoch 32202 )\n",
      "[Epoch: 33080] train loss: 0.2028, train acc: 0.9190, val loss: 0.2428, val acc: 0.9069  (best train acc: 0.9286, best val acc: 0.9157, best train loss: 0.1818  @ epoch 32202 )\n",
      "[Epoch: 33100] train loss: 0.1995, train acc: 0.9178, val loss: 0.2707, val acc: 0.9069  (best train acc: 0.9286, best val acc: 0.9157, best train loss: 0.1818  @ epoch 32202 )\n",
      "[Epoch: 33120] train loss: 0.1895, train acc: 0.9215, val loss: 0.2399, val acc: 0.9076  (best train acc: 0.9286, best val acc: 0.9157, best train loss: 0.1818  @ epoch 32202 )\n",
      "[Epoch: 33140] train loss: 0.1933, train acc: 0.9190, val loss: 0.2382, val acc: 0.9103  (best train acc: 0.9286, best val acc: 0.9157, best train loss: 0.1818  @ epoch 32202 )\n",
      "[Epoch: 33160] train loss: 0.1991, train acc: 0.9171, val loss: 0.2606, val acc: 0.9086  (best train acc: 0.9286, best val acc: 0.9157, best train loss: 0.1818  @ epoch 32202 )\n",
      "[Epoch: 33180] train loss: 0.1997, train acc: 0.9199, val loss: 0.2447, val acc: 0.9052  (best train acc: 0.9286, best val acc: 0.9157, best train loss: 0.1818  @ epoch 32202 )\n",
      "[Epoch: 33200] train loss: 0.1926, train acc: 0.9204, val loss: 0.2499, val acc: 0.9083  (best train acc: 0.9286, best val acc: 0.9157, best train loss: 0.1818  @ epoch 32202 )\n",
      "[Epoch: 33220] train loss: 0.2043, train acc: 0.9200, val loss: 0.2502, val acc: 0.9076  (best train acc: 0.9286, best val acc: 0.9157, best train loss: 0.1818  @ epoch 32202 )\n",
      "[Epoch: 33240] train loss: 0.1966, train acc: 0.9198, val loss: 0.2324, val acc: 0.9113  (best train acc: 0.9286, best val acc: 0.9157, best train loss: 0.1809  @ epoch 33222 )\n",
      "[Epoch: 33260] train loss: 0.2158, train acc: 0.9074, val loss: 0.2382, val acc: 0.9059  (best train acc: 0.9286, best val acc: 0.9157, best train loss: 0.1809  @ epoch 33222 )\n",
      "[Epoch: 33280] train loss: 0.2141, train acc: 0.9107, val loss: 0.2337, val acc: 0.9056  (best train acc: 0.9286, best val acc: 0.9157, best train loss: 0.1809  @ epoch 33222 )\n",
      "[Epoch: 33300] train loss: 0.2005, train acc: 0.9189, val loss: 0.2369, val acc: 0.9062  (best train acc: 0.9286, best val acc: 0.9157, best train loss: 0.1809  @ epoch 33222 )\n",
      "[Epoch: 33320] train loss: 0.2025, train acc: 0.9176, val loss: 0.2391, val acc: 0.9093  (best train acc: 0.9286, best val acc: 0.9157, best train loss: 0.1809  @ epoch 33222 )\n",
      "[Epoch: 33340] train loss: 0.2025, train acc: 0.9146, val loss: 0.2652, val acc: 0.8988  (best train acc: 0.9286, best val acc: 0.9157, best train loss: 0.1809  @ epoch 33222 )\n",
      "[Epoch: 33360] train loss: 0.1969, train acc: 0.9175, val loss: 0.2400, val acc: 0.9079  (best train acc: 0.9286, best val acc: 0.9157, best train loss: 0.1809  @ epoch 33222 )\n",
      "[Epoch: 33380] train loss: 0.1956, train acc: 0.9161, val loss: 0.2420, val acc: 0.9073  (best train acc: 0.9286, best val acc: 0.9157, best train loss: 0.1809  @ epoch 33222 )\n",
      "[Epoch: 33400] train loss: 0.1865, train acc: 0.9247, val loss: 0.2450, val acc: 0.9103  (best train acc: 0.9286, best val acc: 0.9157, best train loss: 0.1809  @ epoch 33222 )\n",
      "[Epoch: 33420] train loss: 0.1924, train acc: 0.9197, val loss: 0.2495, val acc: 0.9079  (best train acc: 0.9286, best val acc: 0.9157, best train loss: 0.1809  @ epoch 33222 )\n",
      "[Epoch: 33440] train loss: 0.2037, train acc: 0.9148, val loss: 0.2587, val acc: 0.9059  (best train acc: 0.9286, best val acc: 0.9157, best train loss: 0.1809  @ epoch 33222 )\n",
      "[Epoch: 33460] train loss: 0.1925, train acc: 0.9195, val loss: 0.2418, val acc: 0.9096  (best train acc: 0.9286, best val acc: 0.9157, best train loss: 0.1809  @ epoch 33222 )\n",
      "[Epoch: 33480] train loss: 0.1898, train acc: 0.9205, val loss: 0.2442, val acc: 0.9106  (best train acc: 0.9286, best val acc: 0.9157, best train loss: 0.1809  @ epoch 33222 )\n",
      "[Epoch: 33500] train loss: 0.1999, train acc: 0.9177, val loss: 0.2564, val acc: 0.9046  (best train acc: 0.9286, best val acc: 0.9157, best train loss: 0.1809  @ epoch 33222 )\n",
      "[Epoch: 33520] train loss: 0.2069, train acc: 0.9196, val loss: 0.2373, val acc: 0.9089  (best train acc: 0.9286, best val acc: 0.9157, best train loss: 0.1809  @ epoch 33222 )\n",
      "[Epoch: 33540] train loss: 0.1960, train acc: 0.9168, val loss: 0.2405, val acc: 0.9079  (best train acc: 0.9286, best val acc: 0.9157, best train loss: 0.1809  @ epoch 33222 )\n",
      "[Epoch: 33560] train loss: 0.2160, train acc: 0.9094, val loss: 0.2312, val acc: 0.9099  (best train acc: 0.9286, best val acc: 0.9157, best train loss: 0.1809  @ epoch 33222 )\n",
      "[Epoch: 33580] train loss: 0.1944, train acc: 0.9186, val loss: 0.2400, val acc: 0.9086  (best train acc: 0.9286, best val acc: 0.9157, best train loss: 0.1809  @ epoch 33222 )\n",
      "[Epoch: 33600] train loss: 0.2093, train acc: 0.9135, val loss: 0.2515, val acc: 0.9056  (best train acc: 0.9286, best val acc: 0.9157, best train loss: 0.1809  @ epoch 33222 )\n",
      "[Epoch: 33620] train loss: 0.1926, train acc: 0.9208, val loss: 0.2423, val acc: 0.9046  (best train acc: 0.9286, best val acc: 0.9157, best train loss: 0.1809  @ epoch 33222 )\n",
      "[Epoch: 33640] train loss: 0.1965, train acc: 0.9180, val loss: 0.2466, val acc: 0.9052  (best train acc: 0.9286, best val acc: 0.9157, best train loss: 0.1809  @ epoch 33222 )\n",
      "[Epoch: 33660] train loss: 0.1823, train acc: 0.9221, val loss: 0.2475, val acc: 0.9099  (best train acc: 0.9286, best val acc: 0.9157, best train loss: 0.1796  @ epoch 33650 )\n",
      "[Epoch: 33680] train loss: 0.1956, train acc: 0.9179, val loss: 0.2499, val acc: 0.9069  (best train acc: 0.9286, best val acc: 0.9157, best train loss: 0.1796  @ epoch 33650 )\n",
      "[Epoch: 33700] train loss: 0.1926, train acc: 0.9191, val loss: 0.2369, val acc: 0.9089  (best train acc: 0.9286, best val acc: 0.9157, best train loss: 0.1796  @ epoch 33650 )\n",
      "[Epoch: 33720] train loss: 0.1968, train acc: 0.9168, val loss: 0.2641, val acc: 0.9025  (best train acc: 0.9286, best val acc: 0.9157, best train loss: 0.1796  @ epoch 33650 )\n",
      "[Epoch: 33740] train loss: 0.1903, train acc: 0.9235, val loss: 0.2392, val acc: 0.9110  (best train acc: 0.9286, best val acc: 0.9157, best train loss: 0.1796  @ epoch 33650 )\n",
      "[Epoch: 33760] train loss: 0.1790, train acc: 0.9248, val loss: 0.2454, val acc: 0.9015  (best train acc: 0.9286, best val acc: 0.9157, best train loss: 0.1790  @ epoch 33760 )\n",
      "[Epoch: 33780] train loss: 0.1895, train acc: 0.9224, val loss: 0.2418, val acc: 0.9079  (best train acc: 0.9286, best val acc: 0.9157, best train loss: 0.1790  @ epoch 33760 )\n",
      "[Epoch: 33800] train loss: 0.2011, train acc: 0.9201, val loss: 0.2627, val acc: 0.9005  (best train acc: 0.9286, best val acc: 0.9157, best train loss: 0.1790  @ epoch 33760 )\n",
      "[Epoch: 33820] train loss: 0.1950, train acc: 0.9170, val loss: 0.2562, val acc: 0.9066  (best train acc: 0.9286, best val acc: 0.9157, best train loss: 0.1790  @ epoch 33760 )\n",
      "[Epoch: 33840] train loss: 0.2052, train acc: 0.9154, val loss: 0.2450, val acc: 0.9079  (best train acc: 0.9286, best val acc: 0.9157, best train loss: 0.1790  @ epoch 33760 )\n",
      "[Epoch: 33860] train loss: 0.1838, train acc: 0.9225, val loss: 0.2442, val acc: 0.9113  (best train acc: 0.9286, best val acc: 0.9157, best train loss: 0.1790  @ epoch 33760 )\n",
      "[Epoch: 33880] train loss: 0.1932, train acc: 0.9246, val loss: 0.2416, val acc: 0.9083  (best train acc: 0.9286, best val acc: 0.9157, best train loss: 0.1790  @ epoch 33760 )\n",
      "[Epoch: 33900] train loss: 0.2127, train acc: 0.9104, val loss: 0.2473, val acc: 0.9046  (best train acc: 0.9286, best val acc: 0.9157, best train loss: 0.1790  @ epoch 33760 )\n",
      "[Epoch: 33920] train loss: 0.2127, train acc: 0.9120, val loss: 0.2517, val acc: 0.9012  (best train acc: 0.9286, best val acc: 0.9157, best train loss: 0.1790  @ epoch 33760 )\n",
      "[Epoch: 33940] train loss: 0.1990, train acc: 0.9209, val loss: 0.2443, val acc: 0.9056  (best train acc: 0.9286, best val acc: 0.9157, best train loss: 0.1790  @ epoch 33760 )\n",
      "[Epoch: 33960] train loss: 0.2141, train acc: 0.9101, val loss: 0.2446, val acc: 0.9019  (best train acc: 0.9286, best val acc: 0.9157, best train loss: 0.1790  @ epoch 33760 )\n",
      "[Epoch: 33980] train loss: 0.2036, train acc: 0.9140, val loss: 0.2407, val acc: 0.9089  (best train acc: 0.9286, best val acc: 0.9157, best train loss: 0.1790  @ epoch 33760 )\n",
      "[Epoch: 34000] train loss: 0.1970, train acc: 0.9177, val loss: 0.2268, val acc: 0.9086  (best train acc: 0.9286, best val acc: 0.9157, best train loss: 0.1790  @ epoch 33760 )\n",
      "[Epoch: 34020] train loss: 0.2029, train acc: 0.9152, val loss: 0.2378, val acc: 0.9086  (best train acc: 0.9286, best val acc: 0.9157, best train loss: 0.1790  @ epoch 33760 )\n",
      "[Epoch: 34040] train loss: 0.2200, train acc: 0.9047, val loss: 0.2393, val acc: 0.9083  (best train acc: 0.9286, best val acc: 0.9157, best train loss: 0.1790  @ epoch 33760 )\n",
      "[Epoch: 34060] train loss: 0.2044, train acc: 0.9190, val loss: 0.2373, val acc: 0.9073  (best train acc: 0.9286, best val acc: 0.9157, best train loss: 0.1790  @ epoch 33760 )\n",
      "[Epoch: 34080] train loss: 0.2012, train acc: 0.9148, val loss: 0.2342, val acc: 0.9103  (best train acc: 0.9286, best val acc: 0.9157, best train loss: 0.1790  @ epoch 33760 )\n",
      "[Epoch: 34100] train loss: 0.2106, train acc: 0.9086, val loss: 0.2437, val acc: 0.9066  (best train acc: 0.9286, best val acc: 0.9157, best train loss: 0.1790  @ epoch 33760 )\n",
      "[Epoch: 34120] train loss: 0.2289, train acc: 0.9077, val loss: 0.2623, val acc: 0.9035  (best train acc: 0.9286, best val acc: 0.9157, best train loss: 0.1790  @ epoch 33760 )\n",
      "[Epoch: 34140] train loss: 0.1956, train acc: 0.9205, val loss: 0.2449, val acc: 0.9069  (best train acc: 0.9286, best val acc: 0.9157, best train loss: 0.1790  @ epoch 33760 )\n",
      "[Epoch: 34160] train loss: 0.2014, train acc: 0.9188, val loss: 0.2397, val acc: 0.9113  (best train acc: 0.9286, best val acc: 0.9157, best train loss: 0.1790  @ epoch 33760 )\n",
      "[Epoch: 34180] train loss: 0.1984, train acc: 0.9182, val loss: 0.2719, val acc: 0.9035  (best train acc: 0.9286, best val acc: 0.9157, best train loss: 0.1790  @ epoch 33760 )\n",
      "[Epoch: 34200] train loss: 0.1889, train acc: 0.9242, val loss: 0.2400, val acc: 0.9093  (best train acc: 0.9286, best val acc: 0.9157, best train loss: 0.1790  @ epoch 33760 )\n",
      "[Epoch: 34220] train loss: 0.1968, train acc: 0.9207, val loss: 0.2398, val acc: 0.9099  (best train acc: 0.9286, best val acc: 0.9157, best train loss: 0.1790  @ epoch 33760 )\n",
      "[Epoch: 34240] train loss: 0.2044, train acc: 0.9155, val loss: 0.2329, val acc: 0.9079  (best train acc: 0.9286, best val acc: 0.9157, best train loss: 0.1790  @ epoch 33760 )\n",
      "[Epoch: 34260] train loss: 0.1853, train acc: 0.9222, val loss: 0.2549, val acc: 0.9056  (best train acc: 0.9286, best val acc: 0.9157, best train loss: 0.1790  @ epoch 33760 )\n",
      "[Epoch: 34280] train loss: 0.2109, train acc: 0.9102, val loss: 0.2341, val acc: 0.9099  (best train acc: 0.9286, best val acc: 0.9157, best train loss: 0.1790  @ epoch 33760 )\n",
      "[Epoch: 34300] train loss: 0.1968, train acc: 0.9188, val loss: 0.2420, val acc: 0.9076  (best train acc: 0.9286, best val acc: 0.9157, best train loss: 0.1790  @ epoch 33760 )\n",
      "[Epoch: 34320] train loss: 0.1927, train acc: 0.9214, val loss: 0.2360, val acc: 0.9110  (best train acc: 0.9286, best val acc: 0.9157, best train loss: 0.1790  @ epoch 33760 )\n",
      "[Epoch: 34340] train loss: 0.2004, train acc: 0.9138, val loss: 0.2434, val acc: 0.9083  (best train acc: 0.9286, best val acc: 0.9157, best train loss: 0.1790  @ epoch 33760 )\n",
      "[Epoch: 34360] train loss: 0.1956, train acc: 0.9223, val loss: 0.2363, val acc: 0.9083  (best train acc: 0.9286, best val acc: 0.9157, best train loss: 0.1790  @ epoch 33760 )\n",
      "[Epoch: 34380] train loss: 0.2121, train acc: 0.9090, val loss: 0.2479, val acc: 0.9025  (best train acc: 0.9286, best val acc: 0.9157, best train loss: 0.1790  @ epoch 33760 )\n",
      "[Epoch: 34400] train loss: 0.2173, train acc: 0.9103, val loss: 0.2553, val acc: 0.9049  (best train acc: 0.9286, best val acc: 0.9157, best train loss: 0.1790  @ epoch 33760 )\n",
      "[Epoch: 34420] train loss: 0.1893, train acc: 0.9246, val loss: 0.2426, val acc: 0.9052  (best train acc: 0.9286, best val acc: 0.9157, best train loss: 0.1790  @ epoch 33760 )\n",
      "[Epoch: 34440] train loss: 0.1918, train acc: 0.9205, val loss: 0.2342, val acc: 0.9096  (best train acc: 0.9286, best val acc: 0.9157, best train loss: 0.1790  @ epoch 33760 )\n",
      "[Epoch: 34460] train loss: 0.1981, train acc: 0.9174, val loss: 0.2442, val acc: 0.9069  (best train acc: 0.9286, best val acc: 0.9157, best train loss: 0.1790  @ epoch 33760 )\n",
      "[Epoch: 34480] train loss: 0.1985, train acc: 0.9184, val loss: 0.2544, val acc: 0.9052  (best train acc: 0.9286, best val acc: 0.9157, best train loss: 0.1790  @ epoch 33760 )\n",
      "[Epoch: 34500] train loss: 0.1858, train acc: 0.9225, val loss: 0.2429, val acc: 0.9066  (best train acc: 0.9286, best val acc: 0.9157, best train loss: 0.1790  @ epoch 33760 )\n",
      "[Epoch: 34520] train loss: 0.1986, train acc: 0.9142, val loss: 0.2465, val acc: 0.9083  (best train acc: 0.9286, best val acc: 0.9157, best train loss: 0.1790  @ epoch 33760 )\n",
      "[Epoch: 34540] train loss: 0.1916, train acc: 0.9216, val loss: 0.2485, val acc: 0.9062  (best train acc: 0.9286, best val acc: 0.9157, best train loss: 0.1790  @ epoch 33760 )\n",
      "[Epoch: 34560] train loss: 0.2079, train acc: 0.9154, val loss: 0.2312, val acc: 0.9073  (best train acc: 0.9286, best val acc: 0.9157, best train loss: 0.1790  @ epoch 33760 )\n",
      "[Epoch: 34580] train loss: 0.1925, train acc: 0.9204, val loss: 0.2320, val acc: 0.9106  (best train acc: 0.9286, best val acc: 0.9157, best train loss: 0.1790  @ epoch 33760 )\n",
      "[Epoch: 34600] train loss: 0.2291, train acc: 0.9006, val loss: 0.2568, val acc: 0.9059  (best train acc: 0.9286, best val acc: 0.9157, best train loss: 0.1790  @ epoch 33760 )\n",
      "[Epoch: 34620] train loss: 0.2004, train acc: 0.9185, val loss: 0.2753, val acc: 0.9039  (best train acc: 0.9286, best val acc: 0.9157, best train loss: 0.1790  @ epoch 33760 )\n",
      "[Epoch: 34640] train loss: 0.2037, train acc: 0.9178, val loss: 0.2480, val acc: 0.9066  (best train acc: 0.9286, best val acc: 0.9157, best train loss: 0.1790  @ epoch 33760 )\n",
      "[Epoch: 34660] train loss: 0.1930, train acc: 0.9213, val loss: 0.2531, val acc: 0.9019  (best train acc: 0.9286, best val acc: 0.9157, best train loss: 0.1790  @ epoch 33760 )\n",
      "[Epoch: 34680] train loss: 0.1923, train acc: 0.9181, val loss: 0.2358, val acc: 0.9083  (best train acc: 0.9286, best val acc: 0.9157, best train loss: 0.1790  @ epoch 33760 )\n",
      "[Epoch: 34700] train loss: 0.1999, train acc: 0.9162, val loss: 0.2320, val acc: 0.9096  (best train acc: 0.9286, best val acc: 0.9157, best train loss: 0.1790  @ epoch 33760 )\n",
      "[Epoch: 34720] train loss: 0.2144, train acc: 0.9107, val loss: 0.2622, val acc: 0.9029  (best train acc: 0.9286, best val acc: 0.9157, best train loss: 0.1790  @ epoch 33760 )\n",
      "[Epoch: 34740] train loss: 0.2019, train acc: 0.9175, val loss: 0.2489, val acc: 0.9029  (best train acc: 0.9286, best val acc: 0.9157, best train loss: 0.1790  @ epoch 33760 )\n",
      "[Epoch: 34760] train loss: 0.1977, train acc: 0.9193, val loss: 0.2401, val acc: 0.9076  (best train acc: 0.9286, best val acc: 0.9157, best train loss: 0.1790  @ epoch 33760 )\n",
      "[Epoch: 34780] train loss: 0.2132, train acc: 0.9090, val loss: 0.2774, val acc: 0.8971  (best train acc: 0.9286, best val acc: 0.9157, best train loss: 0.1790  @ epoch 33760 )\n",
      "[Epoch: 34800] train loss: 0.2006, train acc: 0.9167, val loss: 0.2451, val acc: 0.9069  (best train acc: 0.9286, best val acc: 0.9157, best train loss: 0.1790  @ epoch 33760 )\n",
      "[Epoch: 34820] train loss: 0.1989, train acc: 0.9200, val loss: 0.2453, val acc: 0.9069  (best train acc: 0.9286, best val acc: 0.9157, best train loss: 0.1790  @ epoch 33760 )\n",
      "[Epoch: 34840] train loss: 0.2214, train acc: 0.9140, val loss: 0.2431, val acc: 0.9099  (best train acc: 0.9286, best val acc: 0.9157, best train loss: 0.1790  @ epoch 33760 )\n",
      "[Epoch: 34860] train loss: 0.1947, train acc: 0.9194, val loss: 0.2526, val acc: 0.9049  (best train acc: 0.9286, best val acc: 0.9157, best train loss: 0.1790  @ epoch 33760 )\n",
      "[Epoch: 34880] train loss: 0.2027, train acc: 0.9160, val loss: 0.2516, val acc: 0.9103  (best train acc: 0.9286, best val acc: 0.9157, best train loss: 0.1790  @ epoch 33760 )\n",
      "[Epoch: 34900] train loss: 0.2157, train acc: 0.9098, val loss: 0.2490, val acc: 0.9069  (best train acc: 0.9286, best val acc: 0.9157, best train loss: 0.1790  @ epoch 33760 )\n",
      "[Epoch: 34920] train loss: 0.2067, train acc: 0.9166, val loss: 0.2513, val acc: 0.9059  (best train acc: 0.9286, best val acc: 0.9157, best train loss: 0.1790  @ epoch 33760 )\n",
      "[Epoch: 34940] train loss: 0.2050, train acc: 0.9171, val loss: 0.2483, val acc: 0.9066  (best train acc: 0.9286, best val acc: 0.9157, best train loss: 0.1790  @ epoch 33760 )\n",
      "[Epoch: 34960] train loss: 0.2026, train acc: 0.9171, val loss: 0.2479, val acc: 0.9093  (best train acc: 0.9286, best val acc: 0.9157, best train loss: 0.1790  @ epoch 33760 )\n",
      "[Epoch: 34980] train loss: 0.1947, train acc: 0.9203, val loss: 0.2383, val acc: 0.9096  (best train acc: 0.9286, best val acc: 0.9157, best train loss: 0.1790  @ epoch 33760 )\n",
      "[Epoch: 35000] train loss: 0.1918, train acc: 0.9174, val loss: 0.2521, val acc: 0.9096  (best train acc: 0.9286, best val acc: 0.9157, best train loss: 0.1790  @ epoch 33760 )\n",
      "[Epoch: 35020] train loss: 0.2113, train acc: 0.9132, val loss: 0.2474, val acc: 0.9059  (best train acc: 0.9286, best val acc: 0.9157, best train loss: 0.1790  @ epoch 33760 )\n",
      "[Epoch: 35040] train loss: 0.2161, train acc: 0.9054, val loss: 0.2710, val acc: 0.9022  (best train acc: 0.9286, best val acc: 0.9157, best train loss: 0.1790  @ epoch 33760 )\n",
      "[Epoch: 35060] train loss: 0.1908, train acc: 0.9263, val loss: 0.2385, val acc: 0.9073  (best train acc: 0.9286, best val acc: 0.9157, best train loss: 0.1790  @ epoch 33760 )\n",
      "[Epoch: 35080] train loss: 0.2016, train acc: 0.9209, val loss: 0.2436, val acc: 0.9089  (best train acc: 0.9286, best val acc: 0.9157, best train loss: 0.1790  @ epoch 33760 )\n",
      "[Epoch: 35100] train loss: 0.2182, train acc: 0.9092, val loss: 0.2476, val acc: 0.9103  (best train acc: 0.9286, best val acc: 0.9157, best train loss: 0.1790  @ epoch 33760 )\n",
      "[Epoch: 35120] train loss: 0.1938, train acc: 0.9220, val loss: 0.2316, val acc: 0.9086  (best train acc: 0.9287, best val acc: 0.9157, best train loss: 0.1790  @ epoch 33760 )\n",
      "[Epoch: 35140] train loss: 0.2084, train acc: 0.9146, val loss: 0.2493, val acc: 0.9096  (best train acc: 0.9287, best val acc: 0.9157, best train loss: 0.1790  @ epoch 33760 )\n",
      "[Epoch: 35160] train loss: 0.1837, train acc: 0.9254, val loss: 0.2399, val acc: 0.9103  (best train acc: 0.9287, best val acc: 0.9157, best train loss: 0.1790  @ epoch 33760 )\n",
      "[Epoch: 35180] train loss: 0.1923, train acc: 0.9211, val loss: 0.2413, val acc: 0.9103  (best train acc: 0.9287, best val acc: 0.9157, best train loss: 0.1790  @ epoch 33760 )\n",
      "[Epoch: 35200] train loss: 0.2214, train acc: 0.9072, val loss: 0.2403, val acc: 0.9079  (best train acc: 0.9287, best val acc: 0.9157, best train loss: 0.1790  @ epoch 33760 )\n",
      "[Epoch: 35220] train loss: 0.2016, train acc: 0.9132, val loss: 0.2502, val acc: 0.9079  (best train acc: 0.9287, best val acc: 0.9157, best train loss: 0.1770  @ epoch 35204 )\n",
      "[Epoch: 35240] train loss: 0.1941, train acc: 0.9225, val loss: 0.2412, val acc: 0.9093  (best train acc: 0.9287, best val acc: 0.9157, best train loss: 0.1770  @ epoch 35204 )\n",
      "[Epoch: 35260] train loss: 0.2048, train acc: 0.9158, val loss: 0.2437, val acc: 0.9069  (best train acc: 0.9287, best val acc: 0.9157, best train loss: 0.1770  @ epoch 35204 )\n",
      "[Epoch: 35280] train loss: 0.1904, train acc: 0.9196, val loss: 0.2482, val acc: 0.9059  (best train acc: 0.9287, best val acc: 0.9157, best train loss: 0.1770  @ epoch 35204 )\n",
      "[Epoch: 35300] train loss: 0.2018, train acc: 0.9169, val loss: 0.2402, val acc: 0.9093  (best train acc: 0.9288, best val acc: 0.9157, best train loss: 0.1770  @ epoch 35204 )\n",
      "[Epoch: 35320] train loss: 0.2002, train acc: 0.9193, val loss: 0.2562, val acc: 0.9049  (best train acc: 0.9288, best val acc: 0.9157, best train loss: 0.1770  @ epoch 35204 )\n",
      "[Epoch: 35340] train loss: 0.1879, train acc: 0.9250, val loss: 0.2420, val acc: 0.9103  (best train acc: 0.9288, best val acc: 0.9157, best train loss: 0.1770  @ epoch 35204 )\n",
      "[Epoch: 35360] train loss: 0.2068, train acc: 0.9161, val loss: 0.2345, val acc: 0.9066  (best train acc: 0.9288, best val acc: 0.9157, best train loss: 0.1770  @ epoch 35204 )\n",
      "[Epoch: 35380] train loss: 0.1993, train acc: 0.9194, val loss: 0.2349, val acc: 0.9093  (best train acc: 0.9288, best val acc: 0.9157, best train loss: 0.1770  @ epoch 35204 )\n",
      "[Epoch: 35400] train loss: 0.2034, train acc: 0.9169, val loss: 0.2473, val acc: 0.9086  (best train acc: 0.9288, best val acc: 0.9157, best train loss: 0.1770  @ epoch 35204 )\n",
      "[Epoch: 35420] train loss: 0.2187, train acc: 0.9073, val loss: 0.2361, val acc: 0.9089  (best train acc: 0.9288, best val acc: 0.9157, best train loss: 0.1770  @ epoch 35204 )\n",
      "[Epoch: 35440] train loss: 0.1967, train acc: 0.9151, val loss: 0.2354, val acc: 0.9089  (best train acc: 0.9288, best val acc: 0.9157, best train loss: 0.1770  @ epoch 35204 )\n",
      "[Epoch: 35460] train loss: 0.1954, train acc: 0.9179, val loss: 0.2509, val acc: 0.9079  (best train acc: 0.9288, best val acc: 0.9157, best train loss: 0.1770  @ epoch 35204 )\n",
      "[Epoch: 35480] train loss: 0.1877, train acc: 0.9253, val loss: 0.2543, val acc: 0.9069  (best train acc: 0.9288, best val acc: 0.9157, best train loss: 0.1770  @ epoch 35204 )\n",
      "[Epoch: 35500] train loss: 0.1884, train acc: 0.9246, val loss: 0.2459, val acc: 0.9083  (best train acc: 0.9288, best val acc: 0.9157, best train loss: 0.1770  @ epoch 35204 )\n",
      "[Epoch: 35520] train loss: 0.2044, train acc: 0.9147, val loss: 0.2376, val acc: 0.9079  (best train acc: 0.9288, best val acc: 0.9157, best train loss: 0.1770  @ epoch 35204 )\n",
      "[Epoch: 35540] train loss: 0.1987, train acc: 0.9200, val loss: 0.2410, val acc: 0.9086  (best train acc: 0.9288, best val acc: 0.9157, best train loss: 0.1770  @ epoch 35204 )\n",
      "[Epoch: 35560] train loss: 0.1871, train acc: 0.9229, val loss: 0.2324, val acc: 0.9059  (best train acc: 0.9288, best val acc: 0.9157, best train loss: 0.1770  @ epoch 35204 )\n",
      "[Epoch: 35580] train loss: 0.2136, train acc: 0.9126, val loss: 0.2685, val acc: 0.9029  (best train acc: 0.9288, best val acc: 0.9157, best train loss: 0.1770  @ epoch 35204 )\n",
      "[Epoch: 35600] train loss: 0.1892, train acc: 0.9238, val loss: 0.2418, val acc: 0.9096  (best train acc: 0.9288, best val acc: 0.9157, best train loss: 0.1770  @ epoch 35204 )\n",
      "[Epoch: 35620] train loss: 0.1941, train acc: 0.9201, val loss: 0.2496, val acc: 0.9086  (best train acc: 0.9288, best val acc: 0.9157, best train loss: 0.1770  @ epoch 35204 )\n",
      "[Epoch: 35640] train loss: 0.2070, train acc: 0.9190, val loss: 0.2379, val acc: 0.9089  (best train acc: 0.9288, best val acc: 0.9157, best train loss: 0.1770  @ epoch 35204 )\n",
      "[Epoch: 35660] train loss: 0.1906, train acc: 0.9188, val loss: 0.2436, val acc: 0.9079  (best train acc: 0.9288, best val acc: 0.9157, best train loss: 0.1770  @ epoch 35204 )\n",
      "[Epoch: 35680] train loss: 0.1939, train acc: 0.9193, val loss: 0.2380, val acc: 0.9083  (best train acc: 0.9288, best val acc: 0.9157, best train loss: 0.1770  @ epoch 35204 )\n",
      "[Epoch: 35700] train loss: 0.1864, train acc: 0.9231, val loss: 0.2481, val acc: 0.9083  (best train acc: 0.9288, best val acc: 0.9157, best train loss: 0.1770  @ epoch 35204 )\n",
      "[Epoch: 35720] train loss: 0.2056, train acc: 0.9170, val loss: 0.2435, val acc: 0.9083  (best train acc: 0.9288, best val acc: 0.9157, best train loss: 0.1770  @ epoch 35204 )\n",
      "[Epoch: 35740] train loss: 0.1990, train acc: 0.9197, val loss: 0.2518, val acc: 0.9083  (best train acc: 0.9288, best val acc: 0.9157, best train loss: 0.1770  @ epoch 35204 )\n",
      "[Epoch: 35760] train loss: 0.2013, train acc: 0.9175, val loss: 0.2404, val acc: 0.9086  (best train acc: 0.9288, best val acc: 0.9157, best train loss: 0.1770  @ epoch 35204 )\n",
      "[Epoch: 35780] train loss: 0.2020, train acc: 0.9131, val loss: 0.2305, val acc: 0.9096  (best train acc: 0.9288, best val acc: 0.9157, best train loss: 0.1770  @ epoch 35204 )\n",
      "[Epoch: 35800] train loss: 0.1861, train acc: 0.9221, val loss: 0.2654, val acc: 0.9066  (best train acc: 0.9288, best val acc: 0.9157, best train loss: 0.1770  @ epoch 35204 )\n",
      "[Epoch: 35820] train loss: 0.1910, train acc: 0.9196, val loss: 0.2348, val acc: 0.9120  (best train acc: 0.9288, best val acc: 0.9157, best train loss: 0.1770  @ epoch 35204 )\n",
      "[Epoch: 35840] train loss: 0.1861, train acc: 0.9229, val loss: 0.2548, val acc: 0.9022  (best train acc: 0.9288, best val acc: 0.9157, best train loss: 0.1770  @ epoch 35204 )\n",
      "[Epoch: 35860] train loss: 0.1844, train acc: 0.9234, val loss: 0.2398, val acc: 0.9110  (best train acc: 0.9288, best val acc: 0.9157, best train loss: 0.1770  @ epoch 35204 )\n",
      "[Epoch: 35880] train loss: 0.2248, train acc: 0.9063, val loss: 0.2730, val acc: 0.8985  (best train acc: 0.9288, best val acc: 0.9157, best train loss: 0.1770  @ epoch 35204 )\n",
      "[Epoch: 35900] train loss: 0.2159, train acc: 0.9133, val loss: 0.2568, val acc: 0.9073  (best train acc: 0.9288, best val acc: 0.9157, best train loss: 0.1770  @ epoch 35204 )\n",
      "[Epoch: 35920] train loss: 0.2017, train acc: 0.9195, val loss: 0.2431, val acc: 0.9073  (best train acc: 0.9288, best val acc: 0.9157, best train loss: 0.1770  @ epoch 35204 )\n",
      "[Epoch: 35940] train loss: 0.1978, train acc: 0.9183, val loss: 0.2549, val acc: 0.9052  (best train acc: 0.9288, best val acc: 0.9157, best train loss: 0.1770  @ epoch 35204 )\n",
      "[Epoch: 35960] train loss: 0.2044, train acc: 0.9125, val loss: 0.2569, val acc: 0.9062  (best train acc: 0.9288, best val acc: 0.9157, best train loss: 0.1770  @ epoch 35204 )\n",
      "[Epoch: 35980] train loss: 0.1913, train acc: 0.9211, val loss: 0.2420, val acc: 0.9113  (best train acc: 0.9288, best val acc: 0.9157, best train loss: 0.1770  @ epoch 35204 )\n",
      "[Epoch: 36000] train loss: 0.1919, train acc: 0.9215, val loss: 0.2412, val acc: 0.9069  (best train acc: 0.9288, best val acc: 0.9157, best train loss: 0.1770  @ epoch 35204 )\n",
      "[Epoch: 36020] train loss: 0.1847, train acc: 0.9231, val loss: 0.2430, val acc: 0.9062  (best train acc: 0.9288, best val acc: 0.9157, best train loss: 0.1770  @ epoch 35204 )\n",
      "[Epoch: 36040] train loss: 0.2043, train acc: 0.9156, val loss: 0.2412, val acc: 0.9096  (best train acc: 0.9288, best val acc: 0.9157, best train loss: 0.1770  @ epoch 35204 )\n",
      "[Epoch: 36060] train loss: 0.1972, train acc: 0.9190, val loss: 0.2655, val acc: 0.9002  (best train acc: 0.9288, best val acc: 0.9157, best train loss: 0.1770  @ epoch 35204 )\n",
      "[Epoch: 36080] train loss: 0.1985, train acc: 0.9158, val loss: 0.2349, val acc: 0.9103  (best train acc: 0.9288, best val acc: 0.9157, best train loss: 0.1770  @ epoch 35204 )\n",
      "[Epoch: 36100] train loss: 0.1943, train acc: 0.9214, val loss: 0.2454, val acc: 0.9110  (best train acc: 0.9288, best val acc: 0.9157, best train loss: 0.1770  @ epoch 35204 )\n",
      "[Epoch: 36120] train loss: 0.1938, train acc: 0.9182, val loss: 0.2500, val acc: 0.9079  (best train acc: 0.9288, best val acc: 0.9157, best train loss: 0.1770  @ epoch 35204 )\n",
      "[Epoch: 36140] train loss: 0.1932, train acc: 0.9205, val loss: 0.2427, val acc: 0.9069  (best train acc: 0.9288, best val acc: 0.9157, best train loss: 0.1770  @ epoch 35204 )\n",
      "[Epoch: 36160] train loss: 0.1986, train acc: 0.9203, val loss: 0.2411, val acc: 0.9062  (best train acc: 0.9288, best val acc: 0.9157, best train loss: 0.1770  @ epoch 35204 )\n",
      "[Epoch: 36180] train loss: 0.1893, train acc: 0.9225, val loss: 0.2322, val acc: 0.9089  (best train acc: 0.9288, best val acc: 0.9157, best train loss: 0.1770  @ epoch 35204 )\n",
      "[Epoch: 36200] train loss: 0.1995, train acc: 0.9169, val loss: 0.2451, val acc: 0.9110  (best train acc: 0.9288, best val acc: 0.9157, best train loss: 0.1770  @ epoch 35204 )\n",
      "[Epoch: 36220] train loss: 0.1981, train acc: 0.9210, val loss: 0.2531, val acc: 0.9059  (best train acc: 0.9288, best val acc: 0.9157, best train loss: 0.1770  @ epoch 35204 )\n",
      "[Epoch: 36240] train loss: 0.1938, train acc: 0.9227, val loss: 0.2337, val acc: 0.9130  (best train acc: 0.9288, best val acc: 0.9157, best train loss: 0.1770  @ epoch 35204 )\n",
      "[Epoch: 36260] train loss: 0.2070, train acc: 0.9172, val loss: 0.2544, val acc: 0.9056  (best train acc: 0.9288, best val acc: 0.9157, best train loss: 0.1770  @ epoch 35204 )\n",
      "[Epoch: 36280] train loss: 0.1870, train acc: 0.9250, val loss: 0.2486, val acc: 0.9069  (best train acc: 0.9288, best val acc: 0.9157, best train loss: 0.1770  @ epoch 35204 )\n",
      "[Epoch: 36300] train loss: 0.1917, train acc: 0.9217, val loss: 0.2350, val acc: 0.9113  (best train acc: 0.9288, best val acc: 0.9157, best train loss: 0.1770  @ epoch 35204 )\n",
      "[Epoch: 36320] train loss: 0.1898, train acc: 0.9226, val loss: 0.2455, val acc: 0.9083  (best train acc: 0.9288, best val acc: 0.9157, best train loss: 0.1770  @ epoch 35204 )\n",
      "[Epoch: 36340] train loss: 0.1948, train acc: 0.9229, val loss: 0.2465, val acc: 0.9103  (best train acc: 0.9288, best val acc: 0.9157, best train loss: 0.1770  @ epoch 35204 )\n",
      "[Epoch: 36360] train loss: 0.1915, train acc: 0.9189, val loss: 0.2518, val acc: 0.9062  (best train acc: 0.9288, best val acc: 0.9157, best train loss: 0.1770  @ epoch 35204 )\n",
      "[Epoch: 36380] train loss: 0.1859, train acc: 0.9227, val loss: 0.2343, val acc: 0.9093  (best train acc: 0.9288, best val acc: 0.9157, best train loss: 0.1770  @ epoch 35204 )\n",
      "[Epoch: 36400] train loss: 0.1899, train acc: 0.9234, val loss: 0.2475, val acc: 0.9083  (best train acc: 0.9288, best val acc: 0.9157, best train loss: 0.1770  @ epoch 35204 )\n",
      "[Epoch: 36420] train loss: 0.2201, train acc: 0.9069, val loss: 0.2360, val acc: 0.9046  (best train acc: 0.9288, best val acc: 0.9157, best train loss: 0.1770  @ epoch 35204 )\n",
      "[Epoch: 36440] train loss: 0.2023, train acc: 0.9156, val loss: 0.2444, val acc: 0.9076  (best train acc: 0.9288, best val acc: 0.9157, best train loss: 0.1770  @ epoch 35204 )\n",
      "[Epoch: 36460] train loss: 0.1867, train acc: 0.9227, val loss: 0.2379, val acc: 0.9116  (best train acc: 0.9288, best val acc: 0.9157, best train loss: 0.1770  @ epoch 35204 )\n",
      "[Epoch: 36480] train loss: 0.1865, train acc: 0.9240, val loss: 0.2664, val acc: 0.9056  (best train acc: 0.9288, best val acc: 0.9157, best train loss: 0.1765  @ epoch 36471 )\n",
      "[Epoch: 36500] train loss: 0.2043, train acc: 0.9148, val loss: 0.2510, val acc: 0.9059  (best train acc: 0.9288, best val acc: 0.9157, best train loss: 0.1765  @ epoch 36471 )\n",
      "[Epoch: 36520] train loss: 0.2009, train acc: 0.9143, val loss: 0.2481, val acc: 0.9069  (best train acc: 0.9288, best val acc: 0.9157, best train loss: 0.1765  @ epoch 36471 )\n",
      "[Epoch: 36540] train loss: 0.1932, train acc: 0.9207, val loss: 0.2398, val acc: 0.9083  (best train acc: 0.9288, best val acc: 0.9157, best train loss: 0.1765  @ epoch 36471 )\n",
      "[Epoch: 36560] train loss: 0.1967, train acc: 0.9209, val loss: 0.2491, val acc: 0.9069  (best train acc: 0.9288, best val acc: 0.9157, best train loss: 0.1765  @ epoch 36471 )\n",
      "[Epoch: 36580] train loss: 0.1946, train acc: 0.9226, val loss: 0.2462, val acc: 0.9076  (best train acc: 0.9288, best val acc: 0.9157, best train loss: 0.1765  @ epoch 36471 )\n",
      "[Epoch: 36600] train loss: 0.1996, train acc: 0.9203, val loss: 0.2554, val acc: 0.9076  (best train acc: 0.9288, best val acc: 0.9157, best train loss: 0.1765  @ epoch 36471 )\n",
      "[Epoch: 36620] train loss: 0.2024, train acc: 0.9163, val loss: 0.2485, val acc: 0.9076  (best train acc: 0.9288, best val acc: 0.9157, best train loss: 0.1765  @ epoch 36471 )\n",
      "[Epoch: 36640] train loss: 0.2100, train acc: 0.9173, val loss: 0.2608, val acc: 0.9025  (best train acc: 0.9288, best val acc: 0.9157, best train loss: 0.1765  @ epoch 36471 )\n",
      "[Epoch: 36660] train loss: 0.2006, train acc: 0.9156, val loss: 0.2268, val acc: 0.9089  (best train acc: 0.9288, best val acc: 0.9157, best train loss: 0.1765  @ epoch 36471 )\n",
      "[Epoch: 36680] train loss: 0.2041, train acc: 0.9166, val loss: 0.2405, val acc: 0.9086  (best train acc: 0.9288, best val acc: 0.9157, best train loss: 0.1765  @ epoch 36471 )\n",
      "[Epoch: 36700] train loss: 0.2062, train acc: 0.9089, val loss: 0.2444, val acc: 0.9062  (best train acc: 0.9288, best val acc: 0.9157, best train loss: 0.1765  @ epoch 36471 )\n",
      "[Epoch: 36720] train loss: 0.1900, train acc: 0.9238, val loss: 0.2403, val acc: 0.9113  (best train acc: 0.9288, best val acc: 0.9157, best train loss: 0.1765  @ epoch 36471 )\n",
      "[Epoch: 36740] train loss: 0.2117, train acc: 0.9127, val loss: 0.2483, val acc: 0.9093  (best train acc: 0.9288, best val acc: 0.9157, best train loss: 0.1765  @ epoch 36471 )\n",
      "[Epoch: 36760] train loss: 0.1930, train acc: 0.9200, val loss: 0.2446, val acc: 0.9116  (best train acc: 0.9288, best val acc: 0.9157, best train loss: 0.1765  @ epoch 36471 )\n",
      "[Epoch: 36780] train loss: 0.1899, train acc: 0.9250, val loss: 0.2405, val acc: 0.9106  (best train acc: 0.9288, best val acc: 0.9157, best train loss: 0.1765  @ epoch 36471 )\n",
      "[Epoch: 36800] train loss: 0.1958, train acc: 0.9175, val loss: 0.2348, val acc: 0.9086  (best train acc: 0.9288, best val acc: 0.9157, best train loss: 0.1765  @ epoch 36471 )\n",
      "[Epoch: 36820] train loss: 0.2146, train acc: 0.9142, val loss: 0.2605, val acc: 0.9066  (best train acc: 0.9288, best val acc: 0.9157, best train loss: 0.1765  @ epoch 36471 )\n",
      "[Epoch: 36840] train loss: 0.1870, train acc: 0.9234, val loss: 0.2399, val acc: 0.9113  (best train acc: 0.9288, best val acc: 0.9157, best train loss: 0.1765  @ epoch 36471 )\n",
      "[Epoch: 36860] train loss: 0.2088, train acc: 0.9093, val loss: 0.2862, val acc: 0.8941  (best train acc: 0.9288, best val acc: 0.9157, best train loss: 0.1765  @ epoch 36471 )\n",
      "[Epoch: 36880] train loss: 0.1936, train acc: 0.9211, val loss: 0.2347, val acc: 0.9110  (best train acc: 0.9288, best val acc: 0.9157, best train loss: 0.1765  @ epoch 36471 )\n",
      "[Epoch: 36900] train loss: 0.1976, train acc: 0.9158, val loss: 0.2415, val acc: 0.9120  (best train acc: 0.9288, best val acc: 0.9157, best train loss: 0.1765  @ epoch 36471 )\n",
      "[Epoch: 36920] train loss: 0.1933, train acc: 0.9189, val loss: 0.2352, val acc: 0.9096  (best train acc: 0.9288, best val acc: 0.9157, best train loss: 0.1765  @ epoch 36471 )\n",
      "[Epoch: 36940] train loss: 0.2009, train acc: 0.9198, val loss: 0.2351, val acc: 0.9093  (best train acc: 0.9288, best val acc: 0.9157, best train loss: 0.1765  @ epoch 36471 )\n",
      "[Epoch: 36960] train loss: 0.1958, train acc: 0.9174, val loss: 0.2368, val acc: 0.9083  (best train acc: 0.9299, best val acc: 0.9157, best train loss: 0.1765  @ epoch 36471 )\n",
      "[Epoch: 36980] train loss: 0.1873, train acc: 0.9217, val loss: 0.2468, val acc: 0.9076  (best train acc: 0.9299, best val acc: 0.9157, best train loss: 0.1765  @ epoch 36471 )\n",
      "[Epoch: 37000] train loss: 0.2007, train acc: 0.9211, val loss: 0.2484, val acc: 0.9049  (best train acc: 0.9299, best val acc: 0.9157, best train loss: 0.1765  @ epoch 36471 )\n",
      "[Epoch: 37020] train loss: 0.1835, train acc: 0.9258, val loss: 0.2417, val acc: 0.9123  (best train acc: 0.9299, best val acc: 0.9157, best train loss: 0.1765  @ epoch 36471 )\n",
      "[Epoch: 37040] train loss: 0.1803, train acc: 0.9263, val loss: 0.2447, val acc: 0.9096  (best train acc: 0.9299, best val acc: 0.9157, best train loss: 0.1765  @ epoch 36471 )\n",
      "[Epoch: 37060] train loss: 0.2126, train acc: 0.9119, val loss: 0.2483, val acc: 0.9073  (best train acc: 0.9299, best val acc: 0.9157, best train loss: 0.1765  @ epoch 36471 )\n",
      "[Epoch: 37080] train loss: 0.2078, train acc: 0.9151, val loss: 0.2617, val acc: 0.9046  (best train acc: 0.9299, best val acc: 0.9157, best train loss: 0.1765  @ epoch 36471 )\n",
      "[Epoch: 37100] train loss: 0.2002, train acc: 0.9186, val loss: 0.2517, val acc: 0.9042  (best train acc: 0.9299, best val acc: 0.9157, best train loss: 0.1765  @ epoch 36471 )\n",
      "[Epoch: 37120] train loss: 0.2085, train acc: 0.9171, val loss: 0.2344, val acc: 0.9093  (best train acc: 0.9299, best val acc: 0.9157, best train loss: 0.1765  @ epoch 36471 )\n",
      "[Epoch: 37140] train loss: 0.2004, train acc: 0.9171, val loss: 0.2468, val acc: 0.9096  (best train acc: 0.9299, best val acc: 0.9157, best train loss: 0.1765  @ epoch 36471 )\n",
      "[Epoch: 37160] train loss: 0.2073, train acc: 0.9166, val loss: 0.2394, val acc: 0.9116  (best train acc: 0.9299, best val acc: 0.9157, best train loss: 0.1765  @ epoch 36471 )\n",
      "[Epoch: 37180] train loss: 0.2019, train acc: 0.9220, val loss: 0.2339, val acc: 0.9062  (best train acc: 0.9299, best val acc: 0.9157, best train loss: 0.1765  @ epoch 36471 )\n",
      "[Epoch: 37200] train loss: 0.2006, train acc: 0.9157, val loss: 0.2561, val acc: 0.9062  (best train acc: 0.9299, best val acc: 0.9157, best train loss: 0.1765  @ epoch 36471 )\n",
      "[Epoch: 37220] train loss: 0.2109, train acc: 0.9131, val loss: 0.2473, val acc: 0.9083  (best train acc: 0.9299, best val acc: 0.9157, best train loss: 0.1765  @ epoch 36471 )\n",
      "[Epoch: 37240] train loss: 0.1916, train acc: 0.9192, val loss: 0.2378, val acc: 0.9096  (best train acc: 0.9299, best val acc: 0.9157, best train loss: 0.1765  @ epoch 36471 )\n",
      "[Epoch: 37260] train loss: 0.2012, train acc: 0.9162, val loss: 0.2349, val acc: 0.9120  (best train acc: 0.9299, best val acc: 0.9157, best train loss: 0.1765  @ epoch 36471 )\n",
      "[Epoch: 37280] train loss: 0.1831, train acc: 0.9240, val loss: 0.2427, val acc: 0.9059  (best train acc: 0.9299, best val acc: 0.9157, best train loss: 0.1765  @ epoch 36471 )\n",
      "[Epoch: 37300] train loss: 0.1944, train acc: 0.9233, val loss: 0.2521, val acc: 0.9083  (best train acc: 0.9299, best val acc: 0.9157, best train loss: 0.1765  @ epoch 36471 )\n",
      "[Epoch: 37320] train loss: 0.1806, train acc: 0.9226, val loss: 0.2341, val acc: 0.9126  (best train acc: 0.9299, best val acc: 0.9157, best train loss: 0.1765  @ epoch 36471 )\n",
      "[Epoch: 37340] train loss: 0.2075, train acc: 0.9127, val loss: 0.2631, val acc: 0.9035  (best train acc: 0.9299, best val acc: 0.9157, best train loss: 0.1765  @ epoch 36471 )\n",
      "[Epoch: 37360] train loss: 0.1958, train acc: 0.9214, val loss: 0.2425, val acc: 0.9093  (best train acc: 0.9299, best val acc: 0.9157, best train loss: 0.1765  @ epoch 36471 )\n",
      "[Epoch: 37380] train loss: 0.2036, train acc: 0.9151, val loss: 0.2398, val acc: 0.9083  (best train acc: 0.9299, best val acc: 0.9157, best train loss: 0.1765  @ epoch 36471 )\n",
      "[Epoch: 37400] train loss: 0.1972, train acc: 0.9177, val loss: 0.2386, val acc: 0.9126  (best train acc: 0.9299, best val acc: 0.9157, best train loss: 0.1765  @ epoch 36471 )\n",
      "[Epoch: 37420] train loss: 0.1930, train acc: 0.9206, val loss: 0.2431, val acc: 0.9042  (best train acc: 0.9299, best val acc: 0.9157, best train loss: 0.1765  @ epoch 36471 )\n",
      "[Epoch: 37440] train loss: 0.1965, train acc: 0.9184, val loss: 0.2523, val acc: 0.9096  (best train acc: 0.9299, best val acc: 0.9157, best train loss: 0.1765  @ epoch 36471 )\n",
      "[Epoch: 37460] train loss: 0.1990, train acc: 0.9173, val loss: 0.2487, val acc: 0.9103  (best train acc: 0.9299, best val acc: 0.9157, best train loss: 0.1765  @ epoch 36471 )\n",
      "[Epoch: 37480] train loss: 0.1880, train acc: 0.9222, val loss: 0.2271, val acc: 0.9110  (best train acc: 0.9299, best val acc: 0.9157, best train loss: 0.1765  @ epoch 36471 )\n",
      "[Epoch: 37500] train loss: 0.1924, train acc: 0.9232, val loss: 0.2403, val acc: 0.9039  (best train acc: 0.9299, best val acc: 0.9157, best train loss: 0.1765  @ epoch 36471 )\n",
      "[Epoch: 37520] train loss: 0.2050, train acc: 0.9147, val loss: 0.2377, val acc: 0.9066  (best train acc: 0.9299, best val acc: 0.9157, best train loss: 0.1765  @ epoch 36471 )\n",
      "[Epoch: 37540] train loss: 0.1886, train acc: 0.9234, val loss: 0.2379, val acc: 0.9103  (best train acc: 0.9299, best val acc: 0.9157, best train loss: 0.1765  @ epoch 36471 )\n",
      "[Epoch: 37560] train loss: 0.2136, train acc: 0.9138, val loss: 0.2548, val acc: 0.9073  (best train acc: 0.9299, best val acc: 0.9157, best train loss: 0.1765  @ epoch 36471 )\n",
      "[Epoch: 37580] train loss: 0.1870, train acc: 0.9227, val loss: 0.2483, val acc: 0.9073  (best train acc: 0.9299, best val acc: 0.9157, best train loss: 0.1765  @ epoch 36471 )\n",
      "[Epoch: 37600] train loss: 0.1879, train acc: 0.9221, val loss: 0.2357, val acc: 0.9076  (best train acc: 0.9299, best val acc: 0.9157, best train loss: 0.1765  @ epoch 36471 )\n",
      "[Epoch: 37620] train loss: 0.2015, train acc: 0.9186, val loss: 0.2405, val acc: 0.9089  (best train acc: 0.9299, best val acc: 0.9157, best train loss: 0.1765  @ epoch 36471 )\n",
      "[Epoch: 37640] train loss: 0.1895, train acc: 0.9205, val loss: 0.2416, val acc: 0.9110  (best train acc: 0.9299, best val acc: 0.9157, best train loss: 0.1765  @ epoch 36471 )\n",
      "[Epoch: 37660] train loss: 0.1919, train acc: 0.9206, val loss: 0.2404, val acc: 0.9093  (best train acc: 0.9299, best val acc: 0.9157, best train loss: 0.1765  @ epoch 36471 )\n",
      "[Epoch: 37680] train loss: 0.1918, train acc: 0.9211, val loss: 0.2376, val acc: 0.9089  (best train acc: 0.9299, best val acc: 0.9157, best train loss: 0.1765  @ epoch 36471 )\n",
      "[Epoch: 37700] train loss: 0.1899, train acc: 0.9248, val loss: 0.2534, val acc: 0.9049  (best train acc: 0.9299, best val acc: 0.9157, best train loss: 0.1765  @ epoch 36471 )\n",
      "[Epoch: 37720] train loss: 0.2043, train acc: 0.9183, val loss: 0.2374, val acc: 0.9099  (best train acc: 0.9299, best val acc: 0.9157, best train loss: 0.1765  @ epoch 36471 )\n",
      "[Epoch: 37740] train loss: 0.2020, train acc: 0.9199, val loss: 0.2365, val acc: 0.9066  (best train acc: 0.9299, best val acc: 0.9157, best train loss: 0.1765  @ epoch 36471 )\n",
      "[Epoch: 37760] train loss: 0.2092, train acc: 0.9130, val loss: 0.2588, val acc: 0.9025  (best train acc: 0.9299, best val acc: 0.9157, best train loss: 0.1765  @ epoch 36471 )\n",
      "[Epoch: 37780] train loss: 0.2001, train acc: 0.9140, val loss: 0.2569, val acc: 0.9039  (best train acc: 0.9299, best val acc: 0.9157, best train loss: 0.1765  @ epoch 36471 )\n",
      "[Epoch: 37800] train loss: 0.2096, train acc: 0.9116, val loss: 0.2513, val acc: 0.9083  (best train acc: 0.9299, best val acc: 0.9157, best train loss: 0.1765  @ epoch 36471 )\n",
      "[Epoch: 37820] train loss: 0.1993, train acc: 0.9190, val loss: 0.2416, val acc: 0.9052  (best train acc: 0.9299, best val acc: 0.9157, best train loss: 0.1765  @ epoch 36471 )\n",
      "[Epoch: 37840] train loss: 0.2003, train acc: 0.9169, val loss: 0.2341, val acc: 0.9116  (best train acc: 0.9299, best val acc: 0.9157, best train loss: 0.1765  @ epoch 36471 )\n",
      "[Epoch: 37860] train loss: 0.1894, train acc: 0.9205, val loss: 0.2407, val acc: 0.9079  (best train acc: 0.9299, best val acc: 0.9157, best train loss: 0.1765  @ epoch 36471 )\n",
      "[Epoch: 37880] train loss: 0.1949, train acc: 0.9169, val loss: 0.2398, val acc: 0.9089  (best train acc: 0.9299, best val acc: 0.9157, best train loss: 0.1765  @ epoch 36471 )\n",
      "[Epoch: 37900] train loss: 0.1910, train acc: 0.9202, val loss: 0.2310, val acc: 0.9089  (best train acc: 0.9299, best val acc: 0.9157, best train loss: 0.1765  @ epoch 36471 )\n",
      "[Epoch: 37920] train loss: 0.2000, train acc: 0.9200, val loss: 0.2403, val acc: 0.9140  (best train acc: 0.9299, best val acc: 0.9157, best train loss: 0.1765  @ epoch 36471 )\n",
      "[Epoch: 37940] train loss: 0.1852, train acc: 0.9220, val loss: 0.2378, val acc: 0.9110  (best train acc: 0.9299, best val acc: 0.9157, best train loss: 0.1765  @ epoch 36471 )\n",
      "[Epoch: 37960] train loss: 0.1957, train acc: 0.9249, val loss: 0.2514, val acc: 0.9099  (best train acc: 0.9299, best val acc: 0.9157, best train loss: 0.1765  @ epoch 36471 )\n",
      "[Epoch: 37980] train loss: 0.1919, train acc: 0.9198, val loss: 0.2415, val acc: 0.9093  (best train acc: 0.9299, best val acc: 0.9157, best train loss: 0.1765  @ epoch 36471 )\n",
      "[Epoch: 38000] train loss: 0.1948, train acc: 0.9186, val loss: 0.2599, val acc: 0.9079  (best train acc: 0.9299, best val acc: 0.9157, best train loss: 0.1765  @ epoch 36471 )\n",
      "[Epoch: 38020] train loss: 0.1859, train acc: 0.9217, val loss: 0.2483, val acc: 0.9059  (best train acc: 0.9299, best val acc: 0.9157, best train loss: 0.1765  @ epoch 36471 )\n",
      "[Epoch: 38040] train loss: 0.1912, train acc: 0.9222, val loss: 0.2592, val acc: 0.9035  (best train acc: 0.9299, best val acc: 0.9157, best train loss: 0.1765  @ epoch 36471 )\n",
      "[Epoch: 38060] train loss: 0.2110, train acc: 0.9088, val loss: 0.2465, val acc: 0.9046  (best train acc: 0.9299, best val acc: 0.9157, best train loss: 0.1765  @ epoch 36471 )\n",
      "[Epoch: 38080] train loss: 0.1910, train acc: 0.9218, val loss: 0.2506, val acc: 0.9079  (best train acc: 0.9299, best val acc: 0.9157, best train loss: 0.1765  @ epoch 36471 )\n",
      "[Epoch: 38100] train loss: 0.1820, train acc: 0.9261, val loss: 0.2449, val acc: 0.9083  (best train acc: 0.9299, best val acc: 0.9157, best train loss: 0.1765  @ epoch 36471 )\n",
      "[Epoch: 38120] train loss: 0.1924, train acc: 0.9233, val loss: 0.2512, val acc: 0.9083  (best train acc: 0.9299, best val acc: 0.9157, best train loss: 0.1738  @ epoch 38118 )\n",
      "[Epoch: 38140] train loss: 0.2025, train acc: 0.9156, val loss: 0.2469, val acc: 0.9086  (best train acc: 0.9299, best val acc: 0.9157, best train loss: 0.1738  @ epoch 38118 )\n",
      "[Epoch: 38160] train loss: 0.1910, train acc: 0.9208, val loss: 0.2539, val acc: 0.9089  (best train acc: 0.9299, best val acc: 0.9157, best train loss: 0.1738  @ epoch 38118 )\n",
      "[Epoch: 38180] train loss: 0.2022, train acc: 0.9179, val loss: 0.2434, val acc: 0.9113  (best train acc: 0.9299, best val acc: 0.9157, best train loss: 0.1738  @ epoch 38118 )\n",
      "[Epoch: 38200] train loss: 0.1865, train acc: 0.9253, val loss: 0.2513, val acc: 0.9076  (best train acc: 0.9299, best val acc: 0.9157, best train loss: 0.1738  @ epoch 38118 )\n",
      "[Epoch: 38220] train loss: 0.2007, train acc: 0.9153, val loss: 0.2411, val acc: 0.9120  (best train acc: 0.9299, best val acc: 0.9157, best train loss: 0.1738  @ epoch 38118 )\n",
      "[Epoch: 38240] train loss: 0.2028, train acc: 0.9166, val loss: 0.2396, val acc: 0.9099  (best train acc: 0.9299, best val acc: 0.9157, best train loss: 0.1738  @ epoch 38118 )\n",
      "[Epoch: 38260] train loss: 0.1982, train acc: 0.9171, val loss: 0.2444, val acc: 0.9059  (best train acc: 0.9299, best val acc: 0.9157, best train loss: 0.1738  @ epoch 38118 )\n",
      "[Epoch: 38280] train loss: 0.1948, train acc: 0.9193, val loss: 0.2366, val acc: 0.9110  (best train acc: 0.9299, best val acc: 0.9157, best train loss: 0.1738  @ epoch 38118 )\n",
      "[Epoch: 38300] train loss: 0.1854, train acc: 0.9233, val loss: 0.2372, val acc: 0.9083  (best train acc: 0.9299, best val acc: 0.9157, best train loss: 0.1738  @ epoch 38118 )\n",
      "[Epoch: 38320] train loss: 0.2057, train acc: 0.9184, val loss: 0.2533, val acc: 0.9083  (best train acc: 0.9299, best val acc: 0.9157, best train loss: 0.1738  @ epoch 38118 )\n",
      "[Epoch: 38340] train loss: 0.2036, train acc: 0.9114, val loss: 0.2553, val acc: 0.9069  (best train acc: 0.9299, best val acc: 0.9157, best train loss: 0.1738  @ epoch 38118 )\n",
      "[Epoch: 38360] train loss: 0.1821, train acc: 0.9263, val loss: 0.2499, val acc: 0.9103  (best train acc: 0.9299, best val acc: 0.9157, best train loss: 0.1738  @ epoch 38118 )\n",
      "[Epoch: 38380] train loss: 0.1906, train acc: 0.9211, val loss: 0.2450, val acc: 0.9106  (best train acc: 0.9299, best val acc: 0.9157, best train loss: 0.1738  @ epoch 38118 )\n",
      "[Epoch: 38400] train loss: 0.1927, train acc: 0.9201, val loss: 0.2415, val acc: 0.9056  (best train acc: 0.9299, best val acc: 0.9157, best train loss: 0.1738  @ epoch 38118 )\n",
      "[Epoch: 38420] train loss: 0.1951, train acc: 0.9197, val loss: 0.2566, val acc: 0.9042  (best train acc: 0.9299, best val acc: 0.9157, best train loss: 0.1738  @ epoch 38118 )\n",
      "[Epoch: 38440] train loss: 0.1757, train acc: 0.9276, val loss: 0.2350, val acc: 0.9116  (best train acc: 0.9299, best val acc: 0.9157, best train loss: 0.1738  @ epoch 38118 )\n",
      "[Epoch: 38460] train loss: 0.1842, train acc: 0.9258, val loss: 0.2527, val acc: 0.9089  (best train acc: 0.9299, best val acc: 0.9157, best train loss: 0.1738  @ epoch 38118 )\n",
      "[Epoch: 38480] train loss: 0.1889, train acc: 0.9242, val loss: 0.2415, val acc: 0.9093  (best train acc: 0.9299, best val acc: 0.9157, best train loss: 0.1738  @ epoch 38118 )\n",
      "[Epoch: 38500] train loss: 0.1945, train acc: 0.9221, val loss: 0.2373, val acc: 0.9099  (best train acc: 0.9299, best val acc: 0.9157, best train loss: 0.1738  @ epoch 38118 )\n",
      "[Epoch: 38520] train loss: 0.1779, train acc: 0.9287, val loss: 0.2515, val acc: 0.9093  (best train acc: 0.9299, best val acc: 0.9157, best train loss: 0.1738  @ epoch 38118 )\n",
      "[Epoch: 38540] train loss: 0.1890, train acc: 0.9225, val loss: 0.2426, val acc: 0.9096  (best train acc: 0.9299, best val acc: 0.9157, best train loss: 0.1738  @ epoch 38118 )\n",
      "[Epoch: 38560] train loss: 0.1854, train acc: 0.9270, val loss: 0.2424, val acc: 0.9140  (best train acc: 0.9299, best val acc: 0.9157, best train loss: 0.1738  @ epoch 38118 )\n",
      "[Epoch: 38580] train loss: 0.1865, train acc: 0.9278, val loss: 0.2414, val acc: 0.9089  (best train acc: 0.9299, best val acc: 0.9157, best train loss: 0.1738  @ epoch 38118 )\n",
      "[Epoch: 38600] train loss: 0.2121, train acc: 0.9145, val loss: 0.2432, val acc: 0.9035  (best train acc: 0.9299, best val acc: 0.9157, best train loss: 0.1738  @ epoch 38118 )\n",
      "[Epoch: 38620] train loss: 0.1993, train acc: 0.9119, val loss: 0.2438, val acc: 0.9120  (best train acc: 0.9299, best val acc: 0.9157, best train loss: 0.1738  @ epoch 38118 )\n",
      "[Epoch: 38640] train loss: 0.1849, train acc: 0.9237, val loss: 0.2509, val acc: 0.9059  (best train acc: 0.9299, best val acc: 0.9157, best train loss: 0.1738  @ epoch 38118 )\n",
      "[Epoch: 38660] train loss: 0.1854, train acc: 0.9223, val loss: 0.2358, val acc: 0.9059  (best train acc: 0.9299, best val acc: 0.9157, best train loss: 0.1738  @ epoch 38118 )\n",
      "[Epoch: 38680] train loss: 0.1868, train acc: 0.9245, val loss: 0.2352, val acc: 0.9130  (best train acc: 0.9299, best val acc: 0.9157, best train loss: 0.1738  @ epoch 38118 )\n",
      "[Epoch: 38700] train loss: 0.1948, train acc: 0.9140, val loss: 0.2520, val acc: 0.9052  (best train acc: 0.9299, best val acc: 0.9157, best train loss: 0.1738  @ epoch 38118 )\n",
      "[Epoch: 38720] train loss: 0.1838, train acc: 0.9239, val loss: 0.2557, val acc: 0.9056  (best train acc: 0.9299, best val acc: 0.9157, best train loss: 0.1738  @ epoch 38118 )\n",
      "[Epoch: 38740] train loss: 0.1983, train acc: 0.9186, val loss: 0.2484, val acc: 0.9086  (best train acc: 0.9299, best val acc: 0.9164, best train loss: 0.1738  @ epoch 38118 )\n",
      "[Epoch: 38760] train loss: 0.2066, train acc: 0.9204, val loss: 0.2491, val acc: 0.9130  (best train acc: 0.9299, best val acc: 0.9164, best train loss: 0.1738  @ epoch 38118 )\n",
      "[Epoch: 38780] train loss: 0.2109, train acc: 0.9135, val loss: 0.2487, val acc: 0.9079  (best train acc: 0.9299, best val acc: 0.9164, best train loss: 0.1738  @ epoch 38118 )\n",
      "[Epoch: 38800] train loss: 0.1975, train acc: 0.9184, val loss: 0.2418, val acc: 0.9099  (best train acc: 0.9299, best val acc: 0.9164, best train loss: 0.1738  @ epoch 38118 )\n",
      "[Epoch: 38820] train loss: 0.1974, train acc: 0.9178, val loss: 0.2440, val acc: 0.9086  (best train acc: 0.9299, best val acc: 0.9164, best train loss: 0.1738  @ epoch 38118 )\n",
      "[Epoch: 38840] train loss: 0.1932, train acc: 0.9181, val loss: 0.2361, val acc: 0.9126  (best train acc: 0.9299, best val acc: 0.9164, best train loss: 0.1738  @ epoch 38118 )\n",
      "[Epoch: 38860] train loss: 0.1910, train acc: 0.9195, val loss: 0.2447, val acc: 0.9086  (best train acc: 0.9299, best val acc: 0.9164, best train loss: 0.1738  @ epoch 38118 )\n",
      "[Epoch: 38880] train loss: 0.1774, train acc: 0.9267, val loss: 0.2306, val acc: 0.9116  (best train acc: 0.9299, best val acc: 0.9164, best train loss: 0.1738  @ epoch 38118 )\n",
      "[Epoch: 38900] train loss: 0.1931, train acc: 0.9218, val loss: 0.2410, val acc: 0.9079  (best train acc: 0.9299, best val acc: 0.9164, best train loss: 0.1738  @ epoch 38118 )\n",
      "[Epoch: 38920] train loss: 0.1983, train acc: 0.9222, val loss: 0.2547, val acc: 0.9106  (best train acc: 0.9299, best val acc: 0.9164, best train loss: 0.1738  @ epoch 38118 )\n",
      "[Epoch: 38940] train loss: 0.1870, train acc: 0.9224, val loss: 0.2487, val acc: 0.9056  (best train acc: 0.9299, best val acc: 0.9164, best train loss: 0.1738  @ epoch 38118 )\n",
      "[Epoch: 38960] train loss: 0.1931, train acc: 0.9195, val loss: 0.2342, val acc: 0.9143  (best train acc: 0.9306, best val acc: 0.9164, best train loss: 0.1738  @ epoch 38118 )\n",
      "[Epoch: 38980] train loss: 0.1881, train acc: 0.9270, val loss: 0.2426, val acc: 0.9096  (best train acc: 0.9306, best val acc: 0.9164, best train loss: 0.1738  @ epoch 38118 )\n",
      "[Epoch: 39000] train loss: 0.1939, train acc: 0.9192, val loss: 0.2451, val acc: 0.9066  (best train acc: 0.9306, best val acc: 0.9164, best train loss: 0.1738  @ epoch 38118 )\n",
      "[Epoch: 39020] train loss: 0.2047, train acc: 0.9156, val loss: 0.2638, val acc: 0.9079  (best train acc: 0.9306, best val acc: 0.9164, best train loss: 0.1738  @ epoch 38118 )\n",
      "[Epoch: 39040] train loss: 0.2033, train acc: 0.9207, val loss: 0.2772, val acc: 0.9005  (best train acc: 0.9306, best val acc: 0.9164, best train loss: 0.1738  @ epoch 38118 )\n",
      "[Epoch: 39060] train loss: 0.2027, train acc: 0.9175, val loss: 0.2349, val acc: 0.9116  (best train acc: 0.9306, best val acc: 0.9164, best train loss: 0.1738  @ epoch 38118 )\n",
      "[Epoch: 39080] train loss: 0.1996, train acc: 0.9130, val loss: 0.2319, val acc: 0.9113  (best train acc: 0.9306, best val acc: 0.9164, best train loss: 0.1738  @ epoch 38118 )\n",
      "[Epoch: 39100] train loss: 0.2144, train acc: 0.9153, val loss: 0.2525, val acc: 0.9096  (best train acc: 0.9306, best val acc: 0.9164, best train loss: 0.1738  @ epoch 38118 )\n",
      "[Epoch: 39120] train loss: 0.1853, train acc: 0.9234, val loss: 0.2399, val acc: 0.9076  (best train acc: 0.9306, best val acc: 0.9164, best train loss: 0.1738  @ epoch 38118 )\n",
      "[Epoch: 39140] train loss: 0.1888, train acc: 0.9218, val loss: 0.2418, val acc: 0.9086  (best train acc: 0.9306, best val acc: 0.9164, best train loss: 0.1738  @ epoch 38118 )\n",
      "[Epoch: 39160] train loss: 0.2025, train acc: 0.9179, val loss: 0.2301, val acc: 0.9133  (best train acc: 0.9306, best val acc: 0.9164, best train loss: 0.1738  @ epoch 38118 )\n",
      "[Epoch: 39180] train loss: 0.1886, train acc: 0.9226, val loss: 0.2472, val acc: 0.9083  (best train acc: 0.9306, best val acc: 0.9164, best train loss: 0.1738  @ epoch 38118 )\n",
      "[Epoch: 39200] train loss: 0.1930, train acc: 0.9188, val loss: 0.2470, val acc: 0.9083  (best train acc: 0.9306, best val acc: 0.9164, best train loss: 0.1738  @ epoch 38118 )\n",
      "[Epoch: 39220] train loss: 0.1932, train acc: 0.9220, val loss: 0.2472, val acc: 0.9086  (best train acc: 0.9306, best val acc: 0.9164, best train loss: 0.1738  @ epoch 38118 )\n",
      "[Epoch: 39240] train loss: 0.2058, train acc: 0.9138, val loss: 0.2458, val acc: 0.9066  (best train acc: 0.9315, best val acc: 0.9164, best train loss: 0.1734  @ epoch 39229 )\n",
      "[Epoch: 39260] train loss: 0.1887, train acc: 0.9231, val loss: 0.2396, val acc: 0.9126  (best train acc: 0.9315, best val acc: 0.9164, best train loss: 0.1734  @ epoch 39229 )\n",
      "[Epoch: 39280] train loss: 0.1838, train acc: 0.9260, val loss: 0.2442, val acc: 0.9130  (best train acc: 0.9315, best val acc: 0.9164, best train loss: 0.1734  @ epoch 39229 )\n",
      "[Epoch: 39300] train loss: 0.1918, train acc: 0.9189, val loss: 0.2316, val acc: 0.9143  (best train acc: 0.9315, best val acc: 0.9164, best train loss: 0.1734  @ epoch 39229 )\n",
      "[Epoch: 39320] train loss: 0.1998, train acc: 0.9157, val loss: 0.2693, val acc: 0.9039  (best train acc: 0.9315, best val acc: 0.9164, best train loss: 0.1734  @ epoch 39229 )\n",
      "[Epoch: 39340] train loss: 0.1917, train acc: 0.9206, val loss: 0.2390, val acc: 0.9143  (best train acc: 0.9315, best val acc: 0.9164, best train loss: 0.1734  @ epoch 39229 )\n",
      "[Epoch: 39360] train loss: 0.1805, train acc: 0.9265, val loss: 0.2371, val acc: 0.9096  (best train acc: 0.9315, best val acc: 0.9164, best train loss: 0.1734  @ epoch 39229 )\n",
      "[Epoch: 39380] train loss: 0.1874, train acc: 0.9248, val loss: 0.2428, val acc: 0.9103  (best train acc: 0.9315, best val acc: 0.9164, best train loss: 0.1734  @ epoch 39229 )\n",
      "[Epoch: 39400] train loss: 0.2035, train acc: 0.9155, val loss: 0.2516, val acc: 0.9126  (best train acc: 0.9315, best val acc: 0.9164, best train loss: 0.1734  @ epoch 39229 )\n",
      "[Epoch: 39420] train loss: 0.2053, train acc: 0.9150, val loss: 0.2353, val acc: 0.9120  (best train acc: 0.9315, best val acc: 0.9164, best train loss: 0.1734  @ epoch 39229 )\n",
      "[Epoch: 39440] train loss: 0.1964, train acc: 0.9231, val loss: 0.2510, val acc: 0.9096  (best train acc: 0.9315, best val acc: 0.9164, best train loss: 0.1734  @ epoch 39229 )\n",
      "[Epoch: 39460] train loss: 0.1937, train acc: 0.9200, val loss: 0.2280, val acc: 0.9160  (best train acc: 0.9315, best val acc: 0.9164, best train loss: 0.1734  @ epoch 39229 )\n",
      "[Epoch: 39480] train loss: 0.1850, train acc: 0.9217, val loss: 0.2584, val acc: 0.9046  (best train acc: 0.9315, best val acc: 0.9174, best train loss: 0.1734  @ epoch 39229 )\n",
      "[Epoch: 39500] train loss: 0.1960, train acc: 0.9190, val loss: 0.2618, val acc: 0.9046  (best train acc: 0.9315, best val acc: 0.9174, best train loss: 0.1734  @ epoch 39229 )\n",
      "[Epoch: 39520] train loss: 0.1836, train acc: 0.9262, val loss: 0.2324, val acc: 0.9120  (best train acc: 0.9315, best val acc: 0.9174, best train loss: 0.1734  @ epoch 39229 )\n",
      "[Epoch: 39540] train loss: 0.1913, train acc: 0.9187, val loss: 0.2335, val acc: 0.9106  (best train acc: 0.9315, best val acc: 0.9174, best train loss: 0.1734  @ epoch 39229 )\n",
      "[Epoch: 39560] train loss: 0.1803, train acc: 0.9254, val loss: 0.2377, val acc: 0.9147  (best train acc: 0.9315, best val acc: 0.9174, best train loss: 0.1734  @ epoch 39229 )\n",
      "[Epoch: 39580] train loss: 0.1859, train acc: 0.9241, val loss: 0.2440, val acc: 0.9083  (best train acc: 0.9315, best val acc: 0.9174, best train loss: 0.1734  @ epoch 39229 )\n",
      "[Epoch: 39600] train loss: 0.1927, train acc: 0.9220, val loss: 0.2394, val acc: 0.9133  (best train acc: 0.9315, best val acc: 0.9174, best train loss: 0.1734  @ epoch 39229 )\n",
      "[Epoch: 39620] train loss: 0.1932, train acc: 0.9195, val loss: 0.2415, val acc: 0.9099  (best train acc: 0.9315, best val acc: 0.9174, best train loss: 0.1734  @ epoch 39229 )\n",
      "[Epoch: 39640] train loss: 0.1883, train acc: 0.9219, val loss: 0.2367, val acc: 0.9079  (best train acc: 0.9315, best val acc: 0.9174, best train loss: 0.1734  @ epoch 39229 )\n",
      "[Epoch: 39660] train loss: 0.1865, train acc: 0.9258, val loss: 0.2530, val acc: 0.9059  (best train acc: 0.9315, best val acc: 0.9174, best train loss: 0.1734  @ epoch 39229 )\n",
      "[Epoch: 39680] train loss: 0.1777, train acc: 0.9273, val loss: 0.2481, val acc: 0.9046  (best train acc: 0.9315, best val acc: 0.9174, best train loss: 0.1734  @ epoch 39229 )\n",
      "[Epoch: 39700] train loss: 0.1945, train acc: 0.9171, val loss: 0.2376, val acc: 0.9110  (best train acc: 0.9315, best val acc: 0.9174, best train loss: 0.1734  @ epoch 39229 )\n",
      "[Epoch: 39720] train loss: 0.1855, train acc: 0.9231, val loss: 0.2460, val acc: 0.9116  (best train acc: 0.9315, best val acc: 0.9174, best train loss: 0.1734  @ epoch 39229 )\n",
      "[Epoch: 39740] train loss: 0.1881, train acc: 0.9240, val loss: 0.2579, val acc: 0.9089  (best train acc: 0.9315, best val acc: 0.9174, best train loss: 0.1734  @ epoch 39229 )\n",
      "[Epoch: 39760] train loss: 0.1842, train acc: 0.9231, val loss: 0.2488, val acc: 0.9079  (best train acc: 0.9315, best val acc: 0.9174, best train loss: 0.1734  @ epoch 39229 )\n",
      "[Epoch: 39780] train loss: 0.1870, train acc: 0.9206, val loss: 0.2455, val acc: 0.9076  (best train acc: 0.9315, best val acc: 0.9174, best train loss: 0.1734  @ epoch 39229 )\n",
      "[Epoch: 39800] train loss: 0.1923, train acc: 0.9177, val loss: 0.2511, val acc: 0.9069  (best train acc: 0.9315, best val acc: 0.9174, best train loss: 0.1734  @ epoch 39229 )\n",
      "[Epoch: 39820] train loss: 0.1948, train acc: 0.9194, val loss: 0.2526, val acc: 0.9066  (best train acc: 0.9315, best val acc: 0.9174, best train loss: 0.1734  @ epoch 39229 )\n",
      "[Epoch: 39840] train loss: 0.1881, train acc: 0.9221, val loss: 0.2568, val acc: 0.9052  (best train acc: 0.9315, best val acc: 0.9174, best train loss: 0.1734  @ epoch 39229 )\n",
      "[Epoch: 39860] train loss: 0.1831, train acc: 0.9239, val loss: 0.2466, val acc: 0.9049  (best train acc: 0.9315, best val acc: 0.9174, best train loss: 0.1734  @ epoch 39229 )\n",
      "[Epoch: 39880] train loss: 0.1979, train acc: 0.9203, val loss: 0.2350, val acc: 0.9106  (best train acc: 0.9315, best val acc: 0.9174, best train loss: 0.1734  @ epoch 39229 )\n",
      "[Epoch: 39900] train loss: 0.1830, train acc: 0.9244, val loss: 0.2323, val acc: 0.9140  (best train acc: 0.9315, best val acc: 0.9174, best train loss: 0.1734  @ epoch 39229 )\n",
      "[Epoch: 39920] train loss: 0.1984, train acc: 0.9160, val loss: 0.2457, val acc: 0.9069  (best train acc: 0.9315, best val acc: 0.9174, best train loss: 0.1734  @ epoch 39229 )\n",
      "[Epoch: 39940] train loss: 0.1933, train acc: 0.9184, val loss: 0.2605, val acc: 0.9052  (best train acc: 0.9315, best val acc: 0.9174, best train loss: 0.1734  @ epoch 39229 )\n",
      "[Epoch: 39960] train loss: 0.1914, train acc: 0.9174, val loss: 0.2431, val acc: 0.9042  (best train acc: 0.9315, best val acc: 0.9174, best train loss: 0.1734  @ epoch 39229 )\n",
      "[Epoch: 39980] train loss: 0.1795, train acc: 0.9251, val loss: 0.2497, val acc: 0.9089  (best train acc: 0.9315, best val acc: 0.9174, best train loss: 0.1734  @ epoch 39229 )\n",
      "[Epoch: 40000] train loss: 0.1929, train acc: 0.9216, val loss: 0.2478, val acc: 0.9096  (best train acc: 0.9315, best val acc: 0.9174, best train loss: 0.1734  @ epoch 39229 )\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.98       154\n",
      "           1       0.93      0.96      0.94       739\n",
      "           2       0.90      0.89      0.90       739\n",
      "           3       0.93      0.84      0.89       739\n",
      "           4       0.89      0.97      0.93       593\n",
      "\n",
      "    accuracy                           0.92      2964\n",
      "   macro avg       0.92      0.93      0.93      2964\n",
      "weighted avg       0.92      0.92      0.92      2964\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 504x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 504x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "lr              = 0.001\n",
    "epochs          = 40000\n",
    "weight_decay    = 0.0005\n",
    "classes         = ['P', 'LP', 'WN', 'LN', 'RN']\n",
    "\n",
    "model = GNN7L_Sage(dataset)\n",
    "preds = train(model, dataset, epochs, lr, weight_decay, classes, model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get predictions from trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.98       154\n",
      "           1       0.93      0.96      0.94       739\n",
      "           2       0.90      0.89      0.90       739\n",
      "           3       0.93      0.84      0.89       739\n",
      "           4       0.89      0.97      0.93       593\n",
      "\n",
      "    accuracy                           0.92      2964\n",
      "   macro avg       0.92      0.93      0.93      2964\n",
      "weighted avg       0.92      0.92      0.92      2964\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 504x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 504x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "preds, probs, model = predict_from_saved_model(model_name + '_40000_0_0005', dataset, classes, save_to_file=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47bd2b979c604e9888de9f7e42a5cf66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Precision on top 25 : 0.96\n",
      "[+] Precision on top 50 : 0.8\n",
      "[+] Precision on top 100 : 0.7\n",
      "[+] Precision on top 200 : 0.67\n",
      "[+] Precision on top 500 : 0.618\n"
     ]
    }
   ],
   "source": [
    "n_positives = 10\n",
    "\n",
    "preds, probs, model = predict_from_saved_model(model_name+'_40000_0_0005', dataset, classes, save_to_file=False, plot_results=False)\n",
    "\n",
    "ranking = get_ranking(model, dataset, preds, probs, disease_Id, n_positive=n_positives, explanation_nodes_ratio=1, masks_for_seed=10, G=G)\n",
    "\n",
    "### Save ranking to file\n",
    "filename = PATH_TO_RANKINGS + disease_Id + '_' + str(n_positives) + '_new_rankings.txt'\n",
    "with open(filename, 'w') as f:\n",
    "    for line in ranking:\n",
    "        f.write(line + '\\n')\n",
    "\n",
    "cuts = [25, 50, 100, 200, 500]\n",
    "for k in cuts:\n",
    "    precision = validate_with_extended_dataset(ranking[:k], disease_Id, save_ranking_to_file=False)\n",
    "    print('[+] Precision on top', k, ':', precision/k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All positive genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+]  1025 positive nodes found in the graph\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6197afd6fad3437a86b45f7e4843bdfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1025 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ranking = get_ranking_from_all_positives(model, dataset, preds, disease_Id, explanation_nodes_ratio=1, masks_for_seed=5, G=G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Save ranking to file\n",
    "filename = PATH_TO_RANKINGS + disease_Id + '_all_positives_new_ranking.txt'\n",
    "with open(filename, 'w') as f:\n",
    "    for line in ranking:\n",
    "        f.write(line + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Precision on top 25 : 0.8\n",
      "[+] Precision on top 50 : 0.72\n",
      "[+] Precision on top 100 : 0.75\n",
      "[+] Precision on top 200 : 0.71\n",
      "[+] Precision on top 500 : 0.638\n"
     ]
    }
   ],
   "source": [
    "cuts = [25, 50, 100, 200, 500]\n",
    "for k in cuts:\n",
    "    precision = validate_with_extended_dataset(ranking[:k], disease_Id, save_ranking_to_file=False)\n",
    "    print('[+] Precision on top', k, ':', precision/k)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6aeea6fe202f5c7201d5940e4573c0a76b23e4e16f0e3784ac81597546f2b3b3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
