{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from GNNTrain import train, predict_from_saved_model\n",
    "from CreateDatasetv2 import get_dataset_from_graph\n",
    "from Paths import PATH_TO_GRAPHS, PATH_TO_RANKINGS\n",
    "from GDARanking import get_ranking, get_ranking_from_all_positives, validate_with_extended_dataset\n",
    "from GraphSageModel import GNN7L_Sage\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "disease_Id  = 'C0009402'\n",
    "classes     = ['P', 'LP', 'WN', 'LN', 'RN']\n",
    "model_name  = 'GraphSAGE_' + disease_Id + '_new_rankings'\n",
    "graph_path  = PATH_TO_GRAPHS + 'grafo_nedbit_' + disease_Id + '.gml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Reading graph...ok\n",
      "[+] Creating dataset...ok\n",
      "[i] Elapsed time: 37.263\n"
     ]
    }
   ],
   "source": [
    "dataset, G = get_dataset_from_graph(graph_path, disease_Id, quartile=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54f25810dcad4ef18ddf34891b255920",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40001 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch: 0000] train loss: 186.2212, train acc: 0.2546, val loss: 82.7106, val acc: 0.2553  (best train acc: 0.2546, best val acc: 0.2553, best train loss: 186.2212  @ epoch 0 )\n",
      "[Epoch: 0020] train loss: 9.3580, train acc: 0.1886, val loss: 3.6631, val acc: 0.3457  (best train acc: 0.2662, best val acc: 0.3457, best train loss: 9.3580  @ epoch 20 )\n",
      "[Epoch: 0040] train loss: 3.0321, train acc: 0.3016, val loss: 2.0005, val acc: 0.2890  (best train acc: 0.3016, best val acc: 0.3484, best train loss: 3.0321  @ epoch 40 )\n",
      "[Epoch: 0060] train loss: 1.5873, train acc: 0.3253, val loss: 1.4831, val acc: 0.3589  (best train acc: 0.3253, best val acc: 0.3592, best train loss: 1.5873  @ epoch 60 )\n",
      "[Epoch: 0080] train loss: 1.5415, train acc: 0.3220, val loss: 1.4574, val acc: 0.3808  (best train acc: 0.3274, best val acc: 0.3808, best train loss: 1.5331  @ epoch 78 )\n",
      "[Epoch: 0100] train loss: 1.4987, train acc: 0.3227, val loss: 1.4195, val acc: 0.3815  (best train acc: 0.3314, best val acc: 0.3825, best train loss: 1.4894  @ epoch 98 )\n",
      "[Epoch: 0120] train loss: 1.4727, train acc: 0.3334, val loss: 1.3820, val acc: 0.3970  (best train acc: 0.3441, best val acc: 0.3997, best train loss: 1.4490  @ epoch 116 )\n",
      "[Epoch: 0140] train loss: 1.4200, train acc: 0.3554, val loss: 1.3655, val acc: 0.3936  (best train acc: 0.3554, best val acc: 0.3997, best train loss: 1.4157  @ epoch 139 )\n",
      "[Epoch: 0160] train loss: 1.3992, train acc: 0.3572, val loss: 1.3434, val acc: 0.4040  (best train acc: 0.3582, best val acc: 0.4040, best train loss: 1.3992  @ epoch 160 )\n",
      "[Epoch: 0180] train loss: 1.3810, train acc: 0.3621, val loss: 1.3302, val acc: 0.4051  (best train acc: 0.3621, best val acc: 0.4088, best train loss: 1.3693  @ epoch 172 )\n",
      "[Epoch: 0200] train loss: 1.3677, train acc: 0.3647, val loss: 1.3205, val acc: 0.4115  (best train acc: 0.3680, best val acc: 0.4175, best train loss: 1.3645  @ epoch 198 )\n",
      "[Epoch: 0220] train loss: 1.3607, train acc: 0.3606, val loss: 1.3095, val acc: 0.4229  (best train acc: 0.3705, best val acc: 0.4229, best train loss: 1.3541  @ epoch 209 )\n",
      "[Epoch: 0240] train loss: 1.3469, train acc: 0.3629, val loss: 1.3089, val acc: 0.4216  (best train acc: 0.3705, best val acc: 0.4297, best train loss: 1.3469  @ epoch 240 )\n",
      "[Epoch: 0260] train loss: 1.3352, train acc: 0.3767, val loss: 1.2880, val acc: 0.4320  (best train acc: 0.3767, best val acc: 0.4334, best train loss: 1.3328  @ epoch 250 )\n",
      "[Epoch: 0280] train loss: 1.3260, train acc: 0.3778, val loss: 1.2768, val acc: 0.4371  (best train acc: 0.3786, best val acc: 0.4381, best train loss: 1.3220  @ epoch 278 )\n",
      "[Epoch: 0300] train loss: 1.3186, train acc: 0.3929, val loss: 1.2745, val acc: 0.4432  (best train acc: 0.3931, best val acc: 0.4432, best train loss: 1.3186  @ epoch 300 )\n",
      "[Epoch: 0320] train loss: 1.3304, train acc: 0.3963, val loss: 1.2724, val acc: 0.4455  (best train acc: 0.4052, best val acc: 0.4469, best train loss: 1.3022  @ epoch 317 )\n",
      "[Epoch: 0340] train loss: 1.3293, train acc: 0.3908, val loss: 1.2624, val acc: 0.4540  (best train acc: 0.4057, best val acc: 0.4583, best train loss: 1.2932  @ epoch 336 )\n",
      "[Epoch: 0360] train loss: 1.3060, train acc: 0.4070, val loss: 1.2516, val acc: 0.4607  (best train acc: 0.4174, best val acc: 0.4627, best train loss: 1.2840  @ epoch 351 )\n",
      "[Epoch: 0380] train loss: 1.3061, train acc: 0.4124, val loss: 1.2388, val acc: 0.4718  (best train acc: 0.4207, best val acc: 0.4725, best train loss: 1.2840  @ epoch 351 )\n",
      "[Epoch: 0400] train loss: 1.2972, train acc: 0.4166, val loss: 1.2279, val acc: 0.4806  (best train acc: 0.4218, best val acc: 0.4887, best train loss: 1.2785  @ epoch 399 )\n",
      "[Epoch: 0420] train loss: 1.2868, train acc: 0.4231, val loss: 1.2190, val acc: 0.4911  (best train acc: 0.4282, best val acc: 0.4924, best train loss: 1.2579  @ epoch 418 )\n",
      "[Epoch: 0440] train loss: 1.2626, train acc: 0.4414, val loss: 1.1953, val acc: 0.5147  (best train acc: 0.4466, best val acc: 0.5147, best train loss: 1.2454  @ epoch 435 )\n",
      "[Epoch: 0460] train loss: 1.2229, train acc: 0.4626, val loss: 1.1519, val acc: 0.5325  (best train acc: 0.4633, best val acc: 0.5342, best train loss: 1.2221  @ epoch 458 )\n",
      "[Epoch: 0480] train loss: 1.1956, train acc: 0.4697, val loss: 1.1283, val acc: 0.5386  (best train acc: 0.4800, best val acc: 0.5454, best train loss: 1.1914  @ epoch 477 )\n",
      "[Epoch: 0500] train loss: 1.1521, train acc: 0.4975, val loss: 1.0990, val acc: 0.5450  (best train acc: 0.5053, best val acc: 0.5504, best train loss: 1.1462  @ epoch 497 )\n",
      "[Epoch: 0520] train loss: 1.1535, train acc: 0.4986, val loss: 1.0699, val acc: 0.5464  (best train acc: 0.5053, best val acc: 0.5568, best train loss: 1.1409  @ epoch 516 )\n",
      "[Epoch: 0540] train loss: 1.1363, train acc: 0.5041, val loss: 1.0379, val acc: 0.5558  (best train acc: 0.5176, best val acc: 0.5568, best train loss: 1.1235  @ epoch 524 )\n",
      "[Epoch: 0560] train loss: 1.1375, train acc: 0.4927, val loss: 1.0402, val acc: 0.5599  (best train acc: 0.5192, best val acc: 0.5653, best train loss: 1.1000  @ epoch 558 )\n",
      "[Epoch: 0580] train loss: 1.1197, train acc: 0.5093, val loss: 1.0140, val acc: 0.5616  (best train acc: 0.5192, best val acc: 0.5673, best train loss: 1.0987  @ epoch 568 )\n",
      "[Epoch: 0600] train loss: 1.1143, train acc: 0.5061, val loss: 1.0184, val acc: 0.5619  (best train acc: 0.5192, best val acc: 0.5707, best train loss: 1.0823  @ epoch 588 )\n",
      "[Epoch: 0620] train loss: 1.0970, train acc: 0.5069, val loss: 0.9879, val acc: 0.5656  (best train acc: 0.5226, best val acc: 0.5707, best train loss: 1.0823  @ epoch 588 )\n",
      "[Epoch: 0640] train loss: 1.0656, train acc: 0.5238, val loss: 0.9764, val acc: 0.5616  (best train acc: 0.5318, best val acc: 0.5727, best train loss: 1.0615  @ epoch 637 )\n",
      "[Epoch: 0660] train loss: 1.0487, train acc: 0.5356, val loss: 0.9330, val acc: 0.5963  (best train acc: 0.5526, best val acc: 0.6024, best train loss: 1.0300  @ epoch 654 )\n",
      "[Epoch: 0680] train loss: 1.0163, train acc: 0.5654, val loss: 0.9165, val acc: 0.6212  (best train acc: 0.5700, best val acc: 0.6212, best train loss: 1.0048  @ epoch 678 )\n",
      "[Epoch: 0700] train loss: 1.0183, train acc: 0.5481, val loss: 0.8999, val acc: 0.6216  (best train acc: 0.5700, best val acc: 0.6216, best train loss: 1.0004  @ epoch 695 )\n",
      "[Epoch: 0720] train loss: 1.0011, train acc: 0.5583, val loss: 0.9159, val acc: 0.5926  (best train acc: 0.5709, best val acc: 0.6233, best train loss: 0.9802  @ epoch 712 )\n",
      "[Epoch: 0740] train loss: 0.9634, train acc: 0.5767, val loss: 0.8818, val acc: 0.6165  (best train acc: 0.5786, best val acc: 0.6233, best train loss: 0.9618  @ epoch 739 )\n",
      "[Epoch: 0760] train loss: 0.9646, train acc: 0.5896, val loss: 0.8697, val acc: 0.6182  (best train acc: 0.5941, best val acc: 0.6283, best train loss: 0.9614  @ epoch 750 )\n",
      "[Epoch: 0780] train loss: 0.9722, train acc: 0.5804, val loss: 0.8582, val acc: 0.6250  (best train acc: 0.6023, best val acc: 0.6334, best train loss: 0.9503  @ epoch 776 )\n",
      "[Epoch: 0800] train loss: 0.9802, train acc: 0.5785, val loss: 0.8449, val acc: 0.6293  (best train acc: 0.6080, best val acc: 0.6374, best train loss: 0.9425  @ epoch 788 )\n",
      "[Epoch: 0820] train loss: 0.9596, train acc: 0.5825, val loss: 0.8568, val acc: 0.6145  (best train acc: 0.6088, best val acc: 0.6445, best train loss: 0.9358  @ epoch 818 )\n",
      "[Epoch: 0840] train loss: 0.9405, train acc: 0.5976, val loss: 0.8484, val acc: 0.6196  (best train acc: 0.6108, best val acc: 0.6499, best train loss: 0.9281  @ epoch 837 )\n",
      "[Epoch: 0860] train loss: 0.9477, train acc: 0.5965, val loss: 0.8320, val acc: 0.6415  (best train acc: 0.6123, best val acc: 0.6533, best train loss: 0.9194  @ epoch 855 )\n",
      "[Epoch: 0880] train loss: 0.9433, train acc: 0.5994, val loss: 0.8496, val acc: 0.6000  (best train acc: 0.6207, best val acc: 0.6607, best train loss: 0.9149  @ epoch 865 )\n",
      "[Epoch: 0900] train loss: 0.9215, train acc: 0.6046, val loss: 0.8149, val acc: 0.6600  (best train acc: 0.6207, best val acc: 0.6641, best train loss: 0.9149  @ epoch 865 )\n",
      "[Epoch: 0920] train loss: 0.9395, train acc: 0.5854, val loss: 0.8129, val acc: 0.6587  (best train acc: 0.6207, best val acc: 0.6678, best train loss: 0.9104  @ epoch 907 )\n",
      "[Epoch: 0940] train loss: 0.9173, train acc: 0.6060, val loss: 0.8194, val acc: 0.6401  (best train acc: 0.6251, best val acc: 0.6698, best train loss: 0.8984  @ epoch 936 )\n",
      "[Epoch: 0960] train loss: 0.9102, train acc: 0.6138, val loss: 0.8038, val acc: 0.6668  (best train acc: 0.6251, best val acc: 0.6698, best train loss: 0.8984  @ epoch 936 )\n",
      "[Epoch: 0980] train loss: 0.9044, train acc: 0.6123, val loss: 0.8088, val acc: 0.6506  (best train acc: 0.6251, best val acc: 0.6789, best train loss: 0.8983  @ epoch 978 )\n",
      "[Epoch: 1000] train loss: 0.9072, train acc: 0.6105, val loss: 0.8086, val acc: 0.6374  (best train acc: 0.6251, best val acc: 0.6789, best train loss: 0.8983  @ epoch 978 )\n",
      "[Epoch: 1020] train loss: 0.8874, train acc: 0.6293, val loss: 0.7933, val acc: 0.6715  (best train acc: 0.6348, best val acc: 0.6789, best train loss: 0.8780  @ epoch 1019 )\n",
      "[Epoch: 1040] train loss: 0.8957, train acc: 0.6243, val loss: 0.7857, val acc: 0.6793  (best train acc: 0.6348, best val acc: 0.6823, best train loss: 0.8780  @ epoch 1019 )\n",
      "[Epoch: 1060] train loss: 0.8785, train acc: 0.6235, val loss: 0.7892, val acc: 0.6772  (best train acc: 0.6362, best val acc: 0.6836, best train loss: 0.8715  @ epoch 1055 )\n",
      "[Epoch: 1080] train loss: 0.8798, train acc: 0.6311, val loss: 0.7983, val acc: 0.6300  (best train acc: 0.6507, best val acc: 0.6857, best train loss: 0.8586  @ epoch 1077 )\n",
      "[Epoch: 1100] train loss: 0.8537, train acc: 0.6440, val loss: 0.7748, val acc: 0.6874  (best train acc: 0.6507, best val acc: 0.6884, best train loss: 0.8537  @ epoch 1100 )\n",
      "[Epoch: 1120] train loss: 0.8573, train acc: 0.6396, val loss: 0.7750, val acc: 0.6759  (best train acc: 0.6507, best val acc: 0.6897, best train loss: 0.8537  @ epoch 1100 )\n",
      "[Epoch: 1140] train loss: 0.8714, train acc: 0.6351, val loss: 0.7789, val acc: 0.6556  (best train acc: 0.6532, best val acc: 0.6897, best train loss: 0.8456  @ epoch 1132 )\n",
      "[Epoch: 1160] train loss: 0.8482, train acc: 0.6444, val loss: 0.7716, val acc: 0.6675  (best train acc: 0.6532, best val acc: 0.6975, best train loss: 0.8448  @ epoch 1148 )\n",
      "[Epoch: 1180] train loss: 0.8661, train acc: 0.6306, val loss: 0.7540, val acc: 0.6992  (best train acc: 0.6532, best val acc: 0.7008, best train loss: 0.8357  @ epoch 1170 )\n",
      "[Epoch: 1200] train loss: 0.8327, train acc: 0.6491, val loss: 0.7667, val acc: 0.6597  (best train acc: 0.6564, best val acc: 0.7025, best train loss: 0.8193  @ epoch 1186 )\n",
      "[Epoch: 1220] train loss: 0.8336, train acc: 0.6527, val loss: 0.7480, val acc: 0.6978  (best train acc: 0.6591, best val acc: 0.7025, best train loss: 0.8193  @ epoch 1186 )\n",
      "[Epoch: 1240] train loss: 0.8365, train acc: 0.6471, val loss: 0.7671, val acc: 0.6405  (best train acc: 0.6612, best val acc: 0.7046, best train loss: 0.8174  @ epoch 1225 )\n",
      "[Epoch: 1260] train loss: 0.8483, train acc: 0.6364, val loss: 0.7470, val acc: 0.6803  (best train acc: 0.6612, best val acc: 0.7062, best train loss: 0.8174  @ epoch 1225 )\n",
      "[Epoch: 1280] train loss: 0.8384, train acc: 0.6405, val loss: 0.7396, val acc: 0.6887  (best train acc: 0.6612, best val acc: 0.7086, best train loss: 0.8174  @ epoch 1225 )\n",
      "[Epoch: 1300] train loss: 0.8384, train acc: 0.6427, val loss: 0.7485, val acc: 0.6675  (best train acc: 0.6612, best val acc: 0.7086, best train loss: 0.8064  @ epoch 1296 )\n",
      "[Epoch: 1320] train loss: 0.8286, train acc: 0.6475, val loss: 0.7410, val acc: 0.6901  (best train acc: 0.6633, best val acc: 0.7086, best train loss: 0.8064  @ epoch 1296 )\n",
      "[Epoch: 1340] train loss: 0.8099, train acc: 0.6546, val loss: 0.7431, val acc: 0.6813  (best train acc: 0.6633, best val acc: 0.7086, best train loss: 0.8058  @ epoch 1330 )\n",
      "[Epoch: 1360] train loss: 0.8250, train acc: 0.6470, val loss: 0.7438, val acc: 0.6712  (best train acc: 0.6651, best val acc: 0.7086, best train loss: 0.7940  @ epoch 1346 )\n",
      "[Epoch: 1380] train loss: 0.8038, train acc: 0.6587, val loss: 0.7454, val acc: 0.6664  (best train acc: 0.6677, best val acc: 0.7086, best train loss: 0.7940  @ epoch 1346 )\n",
      "[Epoch: 1400] train loss: 0.8258, train acc: 0.6426, val loss: 0.7434, val acc: 0.6654  (best train acc: 0.6700, best val acc: 0.7086, best train loss: 0.7811  @ epoch 1397 )\n",
      "[Epoch: 1420] train loss: 0.8017, train acc: 0.6569, val loss: 0.7401, val acc: 0.6698  (best train acc: 0.6700, best val acc: 0.7086, best train loss: 0.7811  @ epoch 1397 )\n",
      "[Epoch: 1440] train loss: 0.7847, train acc: 0.6569, val loss: 0.7340, val acc: 0.6759  (best train acc: 0.6755, best val acc: 0.7086, best train loss: 0.7767  @ epoch 1428 )\n",
      "[Epoch: 1460] train loss: 0.7883, train acc: 0.6638, val loss: 0.7228, val acc: 0.7069  (best train acc: 0.6755, best val acc: 0.7086, best train loss: 0.7767  @ epoch 1428 )\n",
      "[Epoch: 1480] train loss: 0.7948, train acc: 0.6603, val loss: 0.7315, val acc: 0.6772  (best train acc: 0.6755, best val acc: 0.7086, best train loss: 0.7767  @ epoch 1428 )\n",
      "[Epoch: 1500] train loss: 0.7974, train acc: 0.6538, val loss: 0.7199, val acc: 0.6894  (best train acc: 0.6755, best val acc: 0.7086, best train loss: 0.7728  @ epoch 1486 )\n",
      "[Epoch: 1520] train loss: 0.8044, train acc: 0.6457, val loss: 0.7162, val acc: 0.7069  (best train acc: 0.6755, best val acc: 0.7116, best train loss: 0.7699  @ epoch 1507 )\n",
      "[Epoch: 1540] train loss: 0.7752, train acc: 0.6667, val loss: 0.7269, val acc: 0.6698  (best train acc: 0.6757, best val acc: 0.7116, best train loss: 0.7557  @ epoch 1526 )\n",
      "[Epoch: 1560] train loss: 0.7828, train acc: 0.6596, val loss: 0.7372, val acc: 0.6533  (best train acc: 0.6757, best val acc: 0.7116, best train loss: 0.7557  @ epoch 1526 )\n",
      "[Epoch: 1580] train loss: 0.7844, train acc: 0.6591, val loss: 0.7242, val acc: 0.6637  (best train acc: 0.6757, best val acc: 0.7116, best train loss: 0.7557  @ epoch 1526 )\n",
      "[Epoch: 1600] train loss: 0.7664, train acc: 0.6662, val loss: 0.7361, val acc: 0.6381  (best train acc: 0.6757, best val acc: 0.7116, best train loss: 0.7499  @ epoch 1597 )\n",
      "[Epoch: 1620] train loss: 0.7813, train acc: 0.6491, val loss: 0.7075, val acc: 0.6961  (best train acc: 0.6757, best val acc: 0.7116, best train loss: 0.7499  @ epoch 1597 )\n",
      "[Epoch: 1640] train loss: 0.7834, train acc: 0.6525, val loss: 0.7190, val acc: 0.6702  (best train acc: 0.6757, best val acc: 0.7116, best train loss: 0.7499  @ epoch 1597 )\n",
      "[Epoch: 1660] train loss: 0.7682, train acc: 0.6676, val loss: 0.7189, val acc: 0.6563  (best train acc: 0.6807, best val acc: 0.7150, best train loss: 0.7399  @ epoch 1657 )\n",
      "[Epoch: 1680] train loss: 0.7819, train acc: 0.6410, val loss: 0.7168, val acc: 0.6654  (best train acc: 0.6807, best val acc: 0.7150, best train loss: 0.7399  @ epoch 1657 )\n",
      "[Epoch: 1700] train loss: 0.7565, train acc: 0.6670, val loss: 0.7085, val acc: 0.6843  (best train acc: 0.6807, best val acc: 0.7150, best train loss: 0.7385  @ epoch 1687 )\n",
      "[Epoch: 1720] train loss: 0.7626, train acc: 0.6578, val loss: 0.7142, val acc: 0.6654  (best train acc: 0.6807, best val acc: 0.7150, best train loss: 0.7385  @ epoch 1687 )\n",
      "[Epoch: 1740] train loss: 0.7614, train acc: 0.6622, val loss: 0.7096, val acc: 0.6594  (best train acc: 0.6807, best val acc: 0.7150, best train loss: 0.7385  @ epoch 1687 )\n",
      "[Epoch: 1760] train loss: 0.7677, train acc: 0.6679, val loss: 0.7202, val acc: 0.6482  (best train acc: 0.6833, best val acc: 0.7150, best train loss: 0.7274  @ epoch 1756 )\n",
      "[Epoch: 1780] train loss: 0.7374, train acc: 0.6731, val loss: 0.7072, val acc: 0.6526  (best train acc: 0.6833, best val acc: 0.7150, best train loss: 0.7273  @ epoch 1777 )\n",
      "[Epoch: 1800] train loss: 0.7251, train acc: 0.6782, val loss: 0.6915, val acc: 0.6745  (best train acc: 0.6851, best val acc: 0.7150, best train loss: 0.7251  @ epoch 1800 )\n",
      "[Epoch: 1820] train loss: 0.7237, train acc: 0.6810, val loss: 0.6985, val acc: 0.6648  (best train acc: 0.6899, best val acc: 0.7150, best train loss: 0.7156  @ epoch 1808 )\n",
      "[Epoch: 1840] train loss: 0.7376, train acc: 0.6731, val loss: 0.6842, val acc: 0.6816  (best train acc: 0.6899, best val acc: 0.7150, best train loss: 0.7129  @ epoch 1827 )\n",
      "[Epoch: 1860] train loss: 0.7338, train acc: 0.6778, val loss: 0.7002, val acc: 0.6563  (best train acc: 0.6899, best val acc: 0.7150, best train loss: 0.7129  @ epoch 1827 )\n",
      "[Epoch: 1880] train loss: 0.7588, train acc: 0.6651, val loss: 0.6938, val acc: 0.6523  (best train acc: 0.6900, best val acc: 0.7150, best train loss: 0.7079  @ epoch 1864 )\n",
      "[Epoch: 1900] train loss: 0.7111, train acc: 0.6868, val loss: 0.6941, val acc: 0.6516  (best train acc: 0.6900, best val acc: 0.7150, best train loss: 0.7079  @ epoch 1864 )\n",
      "[Epoch: 1920] train loss: 0.7213, train acc: 0.6781, val loss: 0.6856, val acc: 0.6567  (best train acc: 0.6921, best val acc: 0.7150, best train loss: 0.7050  @ epoch 1908 )\n",
      "[Epoch: 1940] train loss: 0.7121, train acc: 0.6842, val loss: 0.6737, val acc: 0.6739  (best train acc: 0.6921, best val acc: 0.7150, best train loss: 0.6956  @ epoch 1935 )\n",
      "[Epoch: 1960] train loss: 0.7200, train acc: 0.6828, val loss: 0.6760, val acc: 0.6685  (best train acc: 0.6921, best val acc: 0.7150, best train loss: 0.6956  @ epoch 1935 )\n",
      "[Epoch: 1980] train loss: 0.7324, train acc: 0.6732, val loss: 0.6705, val acc: 0.6772  (best train acc: 0.6921, best val acc: 0.7150, best train loss: 0.6917  @ epoch 1965 )\n",
      "[Epoch: 2000] train loss: 0.7035, train acc: 0.6890, val loss: 0.6914, val acc: 0.6449  (best train acc: 0.6949, best val acc: 0.7150, best train loss: 0.6917  @ epoch 1965 )\n",
      "[Epoch: 2020] train loss: 0.7273, train acc: 0.6801, val loss: 0.6779, val acc: 0.6543  (best train acc: 0.6949, best val acc: 0.7150, best train loss: 0.6909  @ epoch 2007 )\n",
      "[Epoch: 2040] train loss: 0.7424, train acc: 0.6658, val loss: 0.6633, val acc: 0.6735  (best train acc: 0.6949, best val acc: 0.7150, best train loss: 0.6877  @ epoch 2033 )\n",
      "[Epoch: 2060] train loss: 0.7000, train acc: 0.6867, val loss: 0.6652, val acc: 0.6695  (best train acc: 0.6962, best val acc: 0.7150, best train loss: 0.6874  @ epoch 2055 )\n",
      "[Epoch: 2080] train loss: 0.7239, train acc: 0.6785, val loss: 0.6622, val acc: 0.6718  (best train acc: 0.6974, best val acc: 0.7150, best train loss: 0.6860  @ epoch 2074 )\n",
      "[Epoch: 2100] train loss: 0.6957, train acc: 0.6859, val loss: 0.6684, val acc: 0.6634  (best train acc: 0.7016, best val acc: 0.7150, best train loss: 0.6805  @ epoch 2086 )\n",
      "[Epoch: 2120] train loss: 0.7551, train acc: 0.6598, val loss: 0.6513, val acc: 0.6786  (best train acc: 0.7016, best val acc: 0.7150, best train loss: 0.6804  @ epoch 2110 )\n",
      "[Epoch: 2140] train loss: 0.7251, train acc: 0.6791, val loss: 0.6862, val acc: 0.6270  (best train acc: 0.7016, best val acc: 0.7150, best train loss: 0.6804  @ epoch 2110 )\n",
      "[Epoch: 2160] train loss: 0.7358, train acc: 0.6682, val loss: 0.6733, val acc: 0.6570  (best train acc: 0.7016, best val acc: 0.7150, best train loss: 0.6804  @ epoch 2110 )\n",
      "[Epoch: 2180] train loss: 0.7083, train acc: 0.6820, val loss: 0.6486, val acc: 0.6782  (best train acc: 0.7031, best val acc: 0.7150, best train loss: 0.6692  @ epoch 2177 )\n",
      "[Epoch: 2200] train loss: 0.7014, train acc: 0.6896, val loss: 0.6539, val acc: 0.6823  (best train acc: 0.7034, best val acc: 0.7150, best train loss: 0.6692  @ epoch 2177 )\n",
      "[Epoch: 2220] train loss: 0.6990, train acc: 0.6906, val loss: 0.6627, val acc: 0.6691  (best train acc: 0.7034, best val acc: 0.7150, best train loss: 0.6692  @ epoch 2177 )\n",
      "[Epoch: 2240] train loss: 0.6830, train acc: 0.6906, val loss: 0.6295, val acc: 0.6961  (best train acc: 0.7034, best val acc: 0.7150, best train loss: 0.6692  @ epoch 2177 )\n",
      "[Epoch: 2260] train loss: 0.7000, train acc: 0.6867, val loss: 0.6384, val acc: 0.6860  (best train acc: 0.7034, best val acc: 0.7150, best train loss: 0.6692  @ epoch 2177 )\n",
      "[Epoch: 2280] train loss: 0.7191, train acc: 0.6726, val loss: 0.6554, val acc: 0.6610  (best train acc: 0.7054, best val acc: 0.7150, best train loss: 0.6692  @ epoch 2177 )\n",
      "[Epoch: 2300] train loss: 0.6958, train acc: 0.6854, val loss: 0.6388, val acc: 0.6880  (best train acc: 0.7099, best val acc: 0.7150, best train loss: 0.6674  @ epoch 2284 )\n",
      "[Epoch: 2320] train loss: 0.6941, train acc: 0.6899, val loss: 0.6419, val acc: 0.6981  (best train acc: 0.7099, best val acc: 0.7150, best train loss: 0.6674  @ epoch 2284 )\n",
      "[Epoch: 2340] train loss: 0.7066, train acc: 0.6805, val loss: 0.6466, val acc: 0.6820  (best train acc: 0.7099, best val acc: 0.7150, best train loss: 0.6642  @ epoch 2337 )\n",
      "[Epoch: 2360] train loss: 0.7092, train acc: 0.6815, val loss: 0.6504, val acc: 0.6728  (best train acc: 0.7099, best val acc: 0.7150, best train loss: 0.6642  @ epoch 2337 )\n",
      "[Epoch: 2380] train loss: 0.7123, train acc: 0.6864, val loss: 0.6687, val acc: 0.6381  (best train acc: 0.7099, best val acc: 0.7150, best train loss: 0.6642  @ epoch 2337 )\n",
      "[Epoch: 2400] train loss: 0.6810, train acc: 0.6970, val loss: 0.6385, val acc: 0.6793  (best train acc: 0.7099, best val acc: 0.7150, best train loss: 0.6642  @ epoch 2337 )\n",
      "[Epoch: 2420] train loss: 0.7004, train acc: 0.6851, val loss: 0.6628, val acc: 0.6364  (best train acc: 0.7159, best val acc: 0.7150, best train loss: 0.6456  @ epoch 2411 )\n",
      "[Epoch: 2440] train loss: 0.6839, train acc: 0.6887, val loss: 0.6340, val acc: 0.6870  (best train acc: 0.7159, best val acc: 0.7150, best train loss: 0.6456  @ epoch 2411 )\n",
      "[Epoch: 2460] train loss: 0.6928, train acc: 0.6936, val loss: 0.6331, val acc: 0.6884  (best train acc: 0.7159, best val acc: 0.7150, best train loss: 0.6456  @ epoch 2411 )\n",
      "[Epoch: 2480] train loss: 0.6577, train acc: 0.7046, val loss: 0.6162, val acc: 0.7079  (best train acc: 0.7159, best val acc: 0.7150, best train loss: 0.6456  @ epoch 2411 )\n",
      "[Epoch: 2500] train loss: 0.6582, train acc: 0.7039, val loss: 0.6518, val acc: 0.6570  (best train acc: 0.7159, best val acc: 0.7150, best train loss: 0.6456  @ epoch 2411 )\n",
      "[Epoch: 2520] train loss: 0.6922, train acc: 0.6900, val loss: 0.6430, val acc: 0.6577  (best train acc: 0.7159, best val acc: 0.7150, best train loss: 0.6451  @ epoch 2519 )\n",
      "[Epoch: 2540] train loss: 0.6786, train acc: 0.6975, val loss: 0.6401, val acc: 0.6705  (best train acc: 0.7159, best val acc: 0.7167, best train loss: 0.6451  @ epoch 2519 )\n",
      "[Epoch: 2560] train loss: 0.6780, train acc: 0.6904, val loss: 0.6189, val acc: 0.7015  (best train acc: 0.7163, best val acc: 0.7167, best train loss: 0.6429  @ epoch 2553 )\n",
      "[Epoch: 2580] train loss: 0.6588, train acc: 0.7081, val loss: 0.6508, val acc: 0.6479  (best train acc: 0.7163, best val acc: 0.7167, best train loss: 0.6429  @ epoch 2553 )\n",
      "[Epoch: 2600] train loss: 0.6522, train acc: 0.7047, val loss: 0.6223, val acc: 0.6914  (best train acc: 0.7205, best val acc: 0.7167, best train loss: 0.6429  @ epoch 2553 )\n",
      "[Epoch: 2620] train loss: 0.6930, train acc: 0.6916, val loss: 0.6569, val acc: 0.6368  (best train acc: 0.7205, best val acc: 0.7167, best train loss: 0.6420  @ epoch 2615 )\n",
      "[Epoch: 2640] train loss: 0.6691, train acc: 0.6963, val loss: 0.6137, val acc: 0.7049  (best train acc: 0.7205, best val acc: 0.7167, best train loss: 0.6416  @ epoch 2636 )\n",
      "[Epoch: 2660] train loss: 0.6710, train acc: 0.7008, val loss: 0.6101, val acc: 0.7116  (best train acc: 0.7205, best val acc: 0.7241, best train loss: 0.6416  @ epoch 2636 )\n",
      "[Epoch: 2680] train loss: 0.7013, train acc: 0.6878, val loss: 0.6261, val acc: 0.6813  (best train acc: 0.7212, best val acc: 0.7241, best train loss: 0.6358  @ epoch 2673 )\n",
      "[Epoch: 2700] train loss: 0.6532, train acc: 0.7016, val loss: 0.6132, val acc: 0.6968  (best train acc: 0.7212, best val acc: 0.7241, best train loss: 0.6358  @ epoch 2673 )\n",
      "[Epoch: 2720] train loss: 0.6556, train acc: 0.7097, val loss: 0.6133, val acc: 0.6992  (best train acc: 0.7264, best val acc: 0.7241, best train loss: 0.6298  @ epoch 2708 )\n",
      "[Epoch: 2740] train loss: 0.6389, train acc: 0.7141, val loss: 0.6158, val acc: 0.6934  (best train acc: 0.7264, best val acc: 0.7241, best train loss: 0.6298  @ epoch 2708 )\n",
      "[Epoch: 2760] train loss: 0.6587, train acc: 0.7060, val loss: 0.6424, val acc: 0.6452  (best train acc: 0.7264, best val acc: 0.7245, best train loss: 0.6298  @ epoch 2708 )\n",
      "[Epoch: 2780] train loss: 0.6683, train acc: 0.7032, val loss: 0.6288, val acc: 0.6749  (best train acc: 0.7264, best val acc: 0.7245, best train loss: 0.6298  @ epoch 2708 )\n",
      "[Epoch: 2800] train loss: 0.6653, train acc: 0.7018, val loss: 0.6242, val acc: 0.6826  (best train acc: 0.7264, best val acc: 0.7245, best train loss: 0.6298  @ epoch 2708 )\n",
      "[Epoch: 2820] train loss: 0.6583, train acc: 0.7096, val loss: 0.6191, val acc: 0.6968  (best train acc: 0.7264, best val acc: 0.7245, best train loss: 0.6298  @ epoch 2708 )\n",
      "[Epoch: 2840] train loss: 0.6262, train acc: 0.7197, val loss: 0.6310, val acc: 0.6641  (best train acc: 0.7264, best val acc: 0.7245, best train loss: 0.6262  @ epoch 2840 )\n",
      "[Epoch: 2860] train loss: 0.6458, train acc: 0.7110, val loss: 0.6307, val acc: 0.6563  (best train acc: 0.7264, best val acc: 0.7245, best train loss: 0.6262  @ epoch 2840 )\n",
      "[Epoch: 2880] train loss: 0.6358, train acc: 0.7188, val loss: 0.6006, val acc: 0.7116  (best train acc: 0.7264, best val acc: 0.7245, best train loss: 0.6262  @ epoch 2840 )\n",
      "[Epoch: 2900] train loss: 0.6629, train acc: 0.7025, val loss: 0.6240, val acc: 0.6634  (best train acc: 0.7264, best val acc: 0.7245, best train loss: 0.6262  @ epoch 2840 )\n",
      "[Epoch: 2920] train loss: 0.6404, train acc: 0.7108, val loss: 0.6280, val acc: 0.6644  (best train acc: 0.7264, best val acc: 0.7245, best train loss: 0.6262  @ epoch 2840 )\n",
      "[Epoch: 2940] train loss: 0.6383, train acc: 0.7193, val loss: 0.5987, val acc: 0.7187  (best train acc: 0.7287, best val acc: 0.7245, best train loss: 0.6195  @ epoch 2928 )\n",
      "[Epoch: 2960] train loss: 0.6386, train acc: 0.7093, val loss: 0.6239, val acc: 0.6708  (best train acc: 0.7287, best val acc: 0.7245, best train loss: 0.6195  @ epoch 2928 )\n",
      "[Epoch: 2980] train loss: 0.6643, train acc: 0.7039, val loss: 0.6080, val acc: 0.6917  (best train acc: 0.7287, best val acc: 0.7268, best train loss: 0.6195  @ epoch 2928 )\n",
      "[Epoch: 3000] train loss: 0.6323, train acc: 0.7177, val loss: 0.6087, val acc: 0.6867  (best train acc: 0.7287, best val acc: 0.7268, best train loss: 0.6195  @ epoch 2928 )\n",
      "[Epoch: 3020] train loss: 0.6608, train acc: 0.7047, val loss: 0.6045, val acc: 0.6944  (best train acc: 0.7287, best val acc: 0.7336, best train loss: 0.6195  @ epoch 2928 )\n",
      "[Epoch: 3040] train loss: 0.6576, train acc: 0.7079, val loss: 0.6239, val acc: 0.6745  (best train acc: 0.7287, best val acc: 0.7336, best train loss: 0.6195  @ epoch 2928 )\n",
      "[Epoch: 3060] train loss: 0.6754, train acc: 0.7000, val loss: 0.6159, val acc: 0.6809  (best train acc: 0.7287, best val acc: 0.7336, best train loss: 0.6195  @ epoch 2928 )\n",
      "[Epoch: 3080] train loss: 0.6710, train acc: 0.7011, val loss: 0.6257, val acc: 0.6594  (best train acc: 0.7287, best val acc: 0.7390, best train loss: 0.6195  @ epoch 2928 )\n",
      "[Epoch: 3100] train loss: 0.6205, train acc: 0.7222, val loss: 0.6219, val acc: 0.6661  (best train acc: 0.7305, best val acc: 0.7390, best train loss: 0.6195  @ epoch 2928 )\n",
      "[Epoch: 3120] train loss: 0.6396, train acc: 0.7130, val loss: 0.6287, val acc: 0.6678  (best train acc: 0.7305, best val acc: 0.7390, best train loss: 0.6195  @ epoch 2928 )\n",
      "[Epoch: 3140] train loss: 0.6672, train acc: 0.6972, val loss: 0.6404, val acc: 0.6476  (best train acc: 0.7305, best val acc: 0.7390, best train loss: 0.6195  @ epoch 2928 )\n",
      "[Epoch: 3160] train loss: 0.6335, train acc: 0.7208, val loss: 0.6030, val acc: 0.6975  (best train acc: 0.7305, best val acc: 0.7390, best train loss: 0.6195  @ epoch 2928 )\n",
      "[Epoch: 3180] train loss: 0.6634, train acc: 0.7044, val loss: 0.6161, val acc: 0.6695  (best train acc: 0.7305, best val acc: 0.7390, best train loss: 0.6195  @ epoch 2928 )\n",
      "[Epoch: 3200] train loss: 0.6287, train acc: 0.7279, val loss: 0.6086, val acc: 0.6833  (best train acc: 0.7305, best val acc: 0.7390, best train loss: 0.6163  @ epoch 3191 )\n",
      "[Epoch: 3220] train loss: 0.6243, train acc: 0.7194, val loss: 0.6337, val acc: 0.6465  (best train acc: 0.7305, best val acc: 0.7390, best train loss: 0.6163  @ epoch 3191 )\n",
      "[Epoch: 3240] train loss: 0.6611, train acc: 0.7047, val loss: 0.6246, val acc: 0.6607  (best train acc: 0.7305, best val acc: 0.7390, best train loss: 0.6163  @ epoch 3191 )\n",
      "[Epoch: 3260] train loss: 0.6487, train acc: 0.7081, val loss: 0.6066, val acc: 0.6914  (best train acc: 0.7305, best val acc: 0.7390, best train loss: 0.6101  @ epoch 3258 )\n",
      "[Epoch: 3280] train loss: 0.6317, train acc: 0.7227, val loss: 0.6198, val acc: 0.6631  (best train acc: 0.7305, best val acc: 0.7390, best train loss: 0.6101  @ epoch 3258 )\n",
      "[Epoch: 3300] train loss: 0.6491, train acc: 0.7071, val loss: 0.6235, val acc: 0.6671  (best train acc: 0.7305, best val acc: 0.7390, best train loss: 0.6101  @ epoch 3258 )\n",
      "[Epoch: 3320] train loss: 0.6458, train acc: 0.7219, val loss: 0.6240, val acc: 0.6556  (best train acc: 0.7305, best val acc: 0.7390, best train loss: 0.6101  @ epoch 3258 )\n",
      "[Epoch: 3340] train loss: 0.6292, train acc: 0.7240, val loss: 0.6244, val acc: 0.6523  (best train acc: 0.7305, best val acc: 0.7390, best train loss: 0.6101  @ epoch 3258 )\n",
      "[Epoch: 3360] train loss: 0.6297, train acc: 0.7224, val loss: 0.6220, val acc: 0.6580  (best train acc: 0.7318, best val acc: 0.7390, best train loss: 0.6092  @ epoch 3344 )\n",
      "[Epoch: 3380] train loss: 0.6299, train acc: 0.7183, val loss: 0.5911, val acc: 0.6961  (best train acc: 0.7353, best val acc: 0.7390, best train loss: 0.6049  @ epoch 3379 )\n",
      "[Epoch: 3400] train loss: 0.6430, train acc: 0.7154, val loss: 0.5942, val acc: 0.7008  (best train acc: 0.7353, best val acc: 0.7390, best train loss: 0.6033  @ epoch 3388 )\n",
      "[Epoch: 3420] train loss: 0.6525, train acc: 0.7055, val loss: 0.6015, val acc: 0.6870  (best train acc: 0.7353, best val acc: 0.7390, best train loss: 0.6033  @ epoch 3388 )\n",
      "[Epoch: 3440] train loss: 0.6305, train acc: 0.7172, val loss: 0.6180, val acc: 0.6567  (best train acc: 0.7353, best val acc: 0.7390, best train loss: 0.6033  @ epoch 3388 )\n",
      "[Epoch: 3460] train loss: 0.6272, train acc: 0.7193, val loss: 0.6153, val acc: 0.6708  (best train acc: 0.7353, best val acc: 0.7390, best train loss: 0.6033  @ epoch 3388 )\n",
      "[Epoch: 3480] train loss: 0.6337, train acc: 0.7213, val loss: 0.5951, val acc: 0.7029  (best train acc: 0.7353, best val acc: 0.7390, best train loss: 0.6033  @ epoch 3388 )\n",
      "[Epoch: 3500] train loss: 0.6237, train acc: 0.7192, val loss: 0.5825, val acc: 0.7224  (best train acc: 0.7353, best val acc: 0.7390, best train loss: 0.6033  @ epoch 3388 )\n",
      "[Epoch: 3520] train loss: 0.6175, train acc: 0.7252, val loss: 0.5923, val acc: 0.7096  (best train acc: 0.7373, best val acc: 0.7390, best train loss: 0.6033  @ epoch 3388 )\n",
      "[Epoch: 3540] train loss: 0.6608, train acc: 0.7032, val loss: 0.6026, val acc: 0.6927  (best train acc: 0.7373, best val acc: 0.7390, best train loss: 0.6033  @ epoch 3388 )\n",
      "[Epoch: 3560] train loss: 0.6083, train acc: 0.7269, val loss: 0.6089, val acc: 0.6715  (best train acc: 0.7373, best val acc: 0.7390, best train loss: 0.6033  @ epoch 3388 )\n",
      "[Epoch: 3580] train loss: 0.6158, train acc: 0.7297, val loss: 0.6212, val acc: 0.6567  (best train acc: 0.7373, best val acc: 0.7390, best train loss: 0.6033  @ epoch 3388 )\n",
      "[Epoch: 3600] train loss: 0.6409, train acc: 0.7149, val loss: 0.6263, val acc: 0.6516  (best train acc: 0.7373, best val acc: 0.7390, best train loss: 0.6011  @ epoch 3598 )\n",
      "[Epoch: 3620] train loss: 0.6442, train acc: 0.7053, val loss: 0.6226, val acc: 0.6587  (best train acc: 0.7373, best val acc: 0.7390, best train loss: 0.6011  @ epoch 3598 )\n",
      "[Epoch: 3640] train loss: 0.6531, train acc: 0.7141, val loss: 0.6296, val acc: 0.6479  (best train acc: 0.7373, best val acc: 0.7390, best train loss: 0.6011  @ epoch 3598 )\n",
      "[Epoch: 3660] train loss: 0.6287, train acc: 0.7113, val loss: 0.6111, val acc: 0.6907  (best train acc: 0.7373, best val acc: 0.7390, best train loss: 0.6011  @ epoch 3598 )\n",
      "[Epoch: 3680] train loss: 0.6251, train acc: 0.7154, val loss: 0.5939, val acc: 0.6968  (best train acc: 0.7373, best val acc: 0.7390, best train loss: 0.6011  @ epoch 3598 )\n",
      "[Epoch: 3700] train loss: 0.6264, train acc: 0.7187, val loss: 0.5892, val acc: 0.6998  (best train acc: 0.7373, best val acc: 0.7390, best train loss: 0.6011  @ epoch 3598 )\n",
      "[Epoch: 3720] train loss: 0.6501, train acc: 0.7138, val loss: 0.6031, val acc: 0.6816  (best train acc: 0.7373, best val acc: 0.7390, best train loss: 0.6011  @ epoch 3598 )\n",
      "[Epoch: 3740] train loss: 0.6836, train acc: 0.6891, val loss: 0.5903, val acc: 0.7140  (best train acc: 0.7373, best val acc: 0.7390, best train loss: 0.5960  @ epoch 3733 )\n",
      "[Epoch: 3760] train loss: 0.6132, train acc: 0.7215, val loss: 0.6176, val acc: 0.6614  (best train acc: 0.7373, best val acc: 0.7390, best train loss: 0.5960  @ epoch 3733 )\n",
      "[Epoch: 3780] train loss: 0.6296, train acc: 0.7277, val loss: 0.6093, val acc: 0.6678  (best train acc: 0.7418, best val acc: 0.7390, best train loss: 0.5851  @ epoch 3765 )\n",
      "[Epoch: 3800] train loss: 0.6267, train acc: 0.7186, val loss: 0.6140, val acc: 0.6590  (best train acc: 0.7418, best val acc: 0.7390, best train loss: 0.5851  @ epoch 3765 )\n",
      "[Epoch: 3820] train loss: 0.6190, train acc: 0.7175, val loss: 0.6141, val acc: 0.6631  (best train acc: 0.7418, best val acc: 0.7390, best train loss: 0.5851  @ epoch 3765 )\n",
      "[Epoch: 3840] train loss: 0.6169, train acc: 0.7271, val loss: 0.6126, val acc: 0.6600  (best train acc: 0.7418, best val acc: 0.7390, best train loss: 0.5851  @ epoch 3765 )\n",
      "[Epoch: 3860] train loss: 0.6099, train acc: 0.7296, val loss: 0.6001, val acc: 0.6789  (best train acc: 0.7418, best val acc: 0.7390, best train loss: 0.5851  @ epoch 3765 )\n",
      "[Epoch: 3880] train loss: 0.6179, train acc: 0.7238, val loss: 0.5958, val acc: 0.6806  (best train acc: 0.7418, best val acc: 0.7390, best train loss: 0.5851  @ epoch 3765 )\n",
      "[Epoch: 3900] train loss: 0.6090, train acc: 0.7269, val loss: 0.5868, val acc: 0.7015  (best train acc: 0.7418, best val acc: 0.7390, best train loss: 0.5851  @ epoch 3765 )\n",
      "[Epoch: 3920] train loss: 0.5971, train acc: 0.7270, val loss: 0.5918, val acc: 0.6914  (best train acc: 0.7418, best val acc: 0.7390, best train loss: 0.5851  @ epoch 3765 )\n",
      "[Epoch: 3940] train loss: 0.6038, train acc: 0.7351, val loss: 0.6152, val acc: 0.6604  (best train acc: 0.7418, best val acc: 0.7390, best train loss: 0.5851  @ epoch 3765 )\n",
      "[Epoch: 3960] train loss: 0.6403, train acc: 0.7074, val loss: 0.6202, val acc: 0.6503  (best train acc: 0.7418, best val acc: 0.7390, best train loss: 0.5851  @ epoch 3765 )\n",
      "[Epoch: 3980] train loss: 0.6051, train acc: 0.7251, val loss: 0.6110, val acc: 0.6671  (best train acc: 0.7418, best val acc: 0.7390, best train loss: 0.5851  @ epoch 3765 )\n",
      "[Epoch: 4000] train loss: 0.6238, train acc: 0.7218, val loss: 0.5974, val acc: 0.6806  (best train acc: 0.7418, best val acc: 0.7390, best train loss: 0.5851  @ epoch 3765 )\n",
      "[Epoch: 4020] train loss: 0.6293, train acc: 0.7165, val loss: 0.5885, val acc: 0.6981  (best train acc: 0.7418, best val acc: 0.7390, best train loss: 0.5851  @ epoch 3765 )\n",
      "[Epoch: 4040] train loss: 0.6404, train acc: 0.7095, val loss: 0.6298, val acc: 0.6374  (best train acc: 0.7418, best val acc: 0.7390, best train loss: 0.5851  @ epoch 3765 )\n",
      "[Epoch: 4060] train loss: 0.6225, train acc: 0.7256, val loss: 0.6048, val acc: 0.6793  (best train acc: 0.7418, best val acc: 0.7390, best train loss: 0.5851  @ epoch 3765 )\n",
      "[Epoch: 4080] train loss: 0.6169, train acc: 0.7258, val loss: 0.5851, val acc: 0.6995  (best train acc: 0.7418, best val acc: 0.7390, best train loss: 0.5851  @ epoch 3765 )\n",
      "[Epoch: 4100] train loss: 0.6122, train acc: 0.7231, val loss: 0.5854, val acc: 0.7042  (best train acc: 0.7418, best val acc: 0.7390, best train loss: 0.5851  @ epoch 3765 )\n",
      "[Epoch: 4120] train loss: 0.6270, train acc: 0.7254, val loss: 0.5985, val acc: 0.6853  (best train acc: 0.7418, best val acc: 0.7390, best train loss: 0.5851  @ epoch 3765 )\n",
      "[Epoch: 4140] train loss: 0.6125, train acc: 0.7237, val loss: 0.5868, val acc: 0.6981  (best train acc: 0.7418, best val acc: 0.7390, best train loss: 0.5851  @ epoch 3765 )\n",
      "[Epoch: 4160] train loss: 0.6080, train acc: 0.7291, val loss: 0.6045, val acc: 0.6651  (best train acc: 0.7418, best val acc: 0.7390, best train loss: 0.5851  @ epoch 3765 )\n",
      "[Epoch: 4180] train loss: 0.6107, train acc: 0.7250, val loss: 0.5986, val acc: 0.6799  (best train acc: 0.7418, best val acc: 0.7390, best train loss: 0.5851  @ epoch 3765 )\n",
      "[Epoch: 4200] train loss: 0.6233, train acc: 0.7161, val loss: 0.5958, val acc: 0.6860  (best train acc: 0.7418, best val acc: 0.7390, best train loss: 0.5851  @ epoch 3765 )\n",
      "[Epoch: 4220] train loss: 0.5965, train acc: 0.7337, val loss: 0.5869, val acc: 0.6897  (best train acc: 0.7418, best val acc: 0.7390, best train loss: 0.5851  @ epoch 3765 )\n",
      "[Epoch: 4240] train loss: 0.6224, train acc: 0.7201, val loss: 0.5736, val acc: 0.7211  (best train acc: 0.7418, best val acc: 0.7390, best train loss: 0.5851  @ epoch 3765 )\n",
      "[Epoch: 4260] train loss: 0.6210, train acc: 0.7217, val loss: 0.5772, val acc: 0.7083  (best train acc: 0.7418, best val acc: 0.7390, best train loss: 0.5851  @ epoch 3765 )\n",
      "[Epoch: 4280] train loss: 0.6191, train acc: 0.7224, val loss: 0.5883, val acc: 0.6887  (best train acc: 0.7418, best val acc: 0.7390, best train loss: 0.5851  @ epoch 3765 )\n",
      "[Epoch: 4300] train loss: 0.6054, train acc: 0.7282, val loss: 0.5919, val acc: 0.6863  (best train acc: 0.7418, best val acc: 0.7390, best train loss: 0.5799  @ epoch 4297 )\n",
      "[Epoch: 4320] train loss: 0.6231, train acc: 0.7222, val loss: 0.6142, val acc: 0.6526  (best train acc: 0.7418, best val acc: 0.7390, best train loss: 0.5799  @ epoch 4297 )\n",
      "[Epoch: 4340] train loss: 0.6121, train acc: 0.7267, val loss: 0.6005, val acc: 0.6742  (best train acc: 0.7418, best val acc: 0.7390, best train loss: 0.5799  @ epoch 4297 )\n",
      "[Epoch: 4360] train loss: 0.5972, train acc: 0.7307, val loss: 0.6109, val acc: 0.6553  (best train acc: 0.7418, best val acc: 0.7390, best train loss: 0.5799  @ epoch 4297 )\n",
      "[Epoch: 4380] train loss: 0.6122, train acc: 0.7287, val loss: 0.5949, val acc: 0.6809  (best train acc: 0.7425, best val acc: 0.7390, best train loss: 0.5799  @ epoch 4297 )\n",
      "[Epoch: 4400] train loss: 0.5905, train acc: 0.7324, val loss: 0.5764, val acc: 0.7204  (best train acc: 0.7425, best val acc: 0.7390, best train loss: 0.5799  @ epoch 4297 )\n",
      "[Epoch: 4420] train loss: 0.6152, train acc: 0.7304, val loss: 0.5709, val acc: 0.7170  (best train acc: 0.7425, best val acc: 0.7518, best train loss: 0.5799  @ epoch 4297 )\n",
      "[Epoch: 4440] train loss: 0.6053, train acc: 0.7253, val loss: 0.6092, val acc: 0.6607  (best train acc: 0.7425, best val acc: 0.7518, best train loss: 0.5799  @ epoch 4297 )\n",
      "[Epoch: 4460] train loss: 0.6029, train acc: 0.7233, val loss: 0.5668, val acc: 0.7184  (best train acc: 0.7425, best val acc: 0.7518, best train loss: 0.5799  @ epoch 4297 )\n",
      "[Epoch: 4480] train loss: 0.6076, train acc: 0.7334, val loss: 0.5907, val acc: 0.7042  (best train acc: 0.7425, best val acc: 0.7518, best train loss: 0.5793  @ epoch 4465 )\n",
      "[Epoch: 4500] train loss: 0.6019, train acc: 0.7345, val loss: 0.5971, val acc: 0.6786  (best train acc: 0.7425, best val acc: 0.7518, best train loss: 0.5793  @ epoch 4465 )\n",
      "[Epoch: 4520] train loss: 0.5967, train acc: 0.7305, val loss: 0.5872, val acc: 0.7083  (best train acc: 0.7425, best val acc: 0.7518, best train loss: 0.5793  @ epoch 4465 )\n",
      "[Epoch: 4540] train loss: 0.5833, train acc: 0.7383, val loss: 0.5734, val acc: 0.7093  (best train acc: 0.7451, best val acc: 0.7518, best train loss: 0.5793  @ epoch 4465 )\n",
      "[Epoch: 4560] train loss: 0.6278, train acc: 0.7140, val loss: 0.5868, val acc: 0.6887  (best train acc: 0.7451, best val acc: 0.7518, best train loss: 0.5793  @ epoch 4465 )\n",
      "[Epoch: 4580] train loss: 0.6120, train acc: 0.7208, val loss: 0.5802, val acc: 0.6944  (best train acc: 0.7451, best val acc: 0.7518, best train loss: 0.5793  @ epoch 4465 )\n",
      "[Epoch: 4600] train loss: 0.6425, train acc: 0.7101, val loss: 0.5983, val acc: 0.6772  (best train acc: 0.7451, best val acc: 0.7518, best train loss: 0.5793  @ epoch 4465 )\n",
      "[Epoch: 4620] train loss: 0.5868, train acc: 0.7385, val loss: 0.5731, val acc: 0.7089  (best train acc: 0.7467, best val acc: 0.7518, best train loss: 0.5793  @ epoch 4465 )\n",
      "[Epoch: 4640] train loss: 0.6086, train acc: 0.7319, val loss: 0.5921, val acc: 0.6847  (best train acc: 0.7467, best val acc: 0.7518, best train loss: 0.5793  @ epoch 4465 )\n",
      "[Epoch: 4660] train loss: 0.6061, train acc: 0.7265, val loss: 0.5819, val acc: 0.6924  (best train acc: 0.7467, best val acc: 0.7518, best train loss: 0.5766  @ epoch 4642 )\n",
      "[Epoch: 4680] train loss: 0.6139, train acc: 0.7259, val loss: 0.5885, val acc: 0.6836  (best train acc: 0.7467, best val acc: 0.7518, best train loss: 0.5766  @ epoch 4642 )\n",
      "[Epoch: 4700] train loss: 0.6023, train acc: 0.7290, val loss: 0.5898, val acc: 0.6766  (best train acc: 0.7467, best val acc: 0.7518, best train loss: 0.5766  @ epoch 4642 )\n",
      "[Epoch: 4720] train loss: 0.6139, train acc: 0.7248, val loss: 0.5890, val acc: 0.6806  (best train acc: 0.7467, best val acc: 0.7518, best train loss: 0.5766  @ epoch 4642 )\n",
      "[Epoch: 4740] train loss: 0.5932, train acc: 0.7334, val loss: 0.5734, val acc: 0.7089  (best train acc: 0.7467, best val acc: 0.7518, best train loss: 0.5766  @ epoch 4642 )\n",
      "[Epoch: 4760] train loss: 0.6114, train acc: 0.7303, val loss: 0.5748, val acc: 0.7049  (best train acc: 0.7467, best val acc: 0.7518, best train loss: 0.5766  @ epoch 4642 )\n",
      "[Epoch: 4780] train loss: 0.6454, train acc: 0.7139, val loss: 0.5822, val acc: 0.6944  (best train acc: 0.7467, best val acc: 0.7518, best train loss: 0.5735  @ epoch 4769 )\n",
      "[Epoch: 4800] train loss: 0.6110, train acc: 0.7261, val loss: 0.5914, val acc: 0.6769  (best train acc: 0.7467, best val acc: 0.7518, best train loss: 0.5735  @ epoch 4769 )\n",
      "[Epoch: 4820] train loss: 0.6033, train acc: 0.7284, val loss: 0.5749, val acc: 0.6995  (best train acc: 0.7467, best val acc: 0.7518, best train loss: 0.5735  @ epoch 4769 )\n",
      "[Epoch: 4840] train loss: 0.6133, train acc: 0.7273, val loss: 0.6136, val acc: 0.6415  (best train acc: 0.7467, best val acc: 0.7518, best train loss: 0.5735  @ epoch 4769 )\n",
      "[Epoch: 4860] train loss: 0.5980, train acc: 0.7347, val loss: 0.5812, val acc: 0.6890  (best train acc: 0.7467, best val acc: 0.7518, best train loss: 0.5735  @ epoch 4769 )\n",
      "[Epoch: 4880] train loss: 0.6393, train acc: 0.7127, val loss: 0.5940, val acc: 0.6998  (best train acc: 0.7467, best val acc: 0.7518, best train loss: 0.5735  @ epoch 4769 )\n",
      "[Epoch: 4900] train loss: 0.5960, train acc: 0.7308, val loss: 0.5706, val acc: 0.7366  (best train acc: 0.7467, best val acc: 0.7518, best train loss: 0.5735  @ epoch 4769 )\n",
      "[Epoch: 4920] train loss: 0.5832, train acc: 0.7333, val loss: 0.5616, val acc: 0.7164  (best train acc: 0.7467, best val acc: 0.7518, best train loss: 0.5735  @ epoch 4769 )\n",
      "[Epoch: 4940] train loss: 0.6059, train acc: 0.7303, val loss: 0.5848, val acc: 0.6793  (best train acc: 0.7467, best val acc: 0.7518, best train loss: 0.5735  @ epoch 4769 )\n",
      "[Epoch: 4960] train loss: 0.5932, train acc: 0.7345, val loss: 0.5843, val acc: 0.6853  (best train acc: 0.7467, best val acc: 0.7518, best train loss: 0.5731  @ epoch 4943 )\n",
      "[Epoch: 4980] train loss: 0.5795, train acc: 0.7476, val loss: 0.5762, val acc: 0.7029  (best train acc: 0.7476, best val acc: 0.7518, best train loss: 0.5731  @ epoch 4943 )\n",
      "[Epoch: 5000] train loss: 0.5935, train acc: 0.7335, val loss: 0.5540, val acc: 0.7346  (best train acc: 0.7476, best val acc: 0.7518, best train loss: 0.5731  @ epoch 4943 )\n",
      "[Epoch: 5020] train loss: 0.6128, train acc: 0.7303, val loss: 0.6002, val acc: 0.6702  (best train acc: 0.7476, best val acc: 0.7518, best train loss: 0.5731  @ epoch 4943 )\n",
      "[Epoch: 5040] train loss: 0.5784, train acc: 0.7391, val loss: 0.5806, val acc: 0.6904  (best train acc: 0.7476, best val acc: 0.7518, best train loss: 0.5731  @ epoch 4943 )\n",
      "[Epoch: 5060] train loss: 0.5947, train acc: 0.7376, val loss: 0.5714, val acc: 0.7120  (best train acc: 0.7514, best val acc: 0.7518, best train loss: 0.5690  @ epoch 5051 )\n",
      "[Epoch: 5080] train loss: 0.5890, train acc: 0.7372, val loss: 0.5677, val acc: 0.7096  (best train acc: 0.7514, best val acc: 0.7518, best train loss: 0.5670  @ epoch 5064 )\n",
      "[Epoch: 5100] train loss: 0.5765, train acc: 0.7405, val loss: 0.5609, val acc: 0.7211  (best train acc: 0.7514, best val acc: 0.7518, best train loss: 0.5641  @ epoch 5093 )\n",
      "[Epoch: 5120] train loss: 0.5987, train acc: 0.7309, val loss: 0.5693, val acc: 0.7180  (best train acc: 0.7514, best val acc: 0.7518, best train loss: 0.5641  @ epoch 5093 )\n",
      "[Epoch: 5140] train loss: 0.6144, train acc: 0.7259, val loss: 0.5861, val acc: 0.6830  (best train acc: 0.7514, best val acc: 0.7524, best train loss: 0.5641  @ epoch 5093 )\n",
      "[Epoch: 5160] train loss: 0.5725, train acc: 0.7486, val loss: 0.5668, val acc: 0.7012  (best train acc: 0.7514, best val acc: 0.7524, best train loss: 0.5641  @ epoch 5093 )\n",
      "[Epoch: 5180] train loss: 0.5949, train acc: 0.7374, val loss: 0.5776, val acc: 0.6927  (best train acc: 0.7514, best val acc: 0.7524, best train loss: 0.5641  @ epoch 5093 )\n",
      "[Epoch: 5200] train loss: 0.5955, train acc: 0.7349, val loss: 0.5638, val acc: 0.7099  (best train acc: 0.7529, best val acc: 0.7524, best train loss: 0.5640  @ epoch 5190 )\n",
      "[Epoch: 5220] train loss: 0.5836, train acc: 0.7439, val loss: 0.5700, val acc: 0.6998  (best train acc: 0.7529, best val acc: 0.7524, best train loss: 0.5640  @ epoch 5190 )\n",
      "[Epoch: 5240] train loss: 0.5737, train acc: 0.7501, val loss: 0.5669, val acc: 0.7126  (best train acc: 0.7529, best val acc: 0.7524, best train loss: 0.5640  @ epoch 5190 )\n",
      "[Epoch: 5260] train loss: 0.5748, train acc: 0.7452, val loss: 0.5673, val acc: 0.6971  (best train acc: 0.7545, best val acc: 0.7524, best train loss: 0.5639  @ epoch 5256 )\n",
      "[Epoch: 5280] train loss: 0.6003, train acc: 0.7388, val loss: 0.5622, val acc: 0.7123  (best train acc: 0.7545, best val acc: 0.7524, best train loss: 0.5606  @ epoch 5264 )\n",
      "[Epoch: 5300] train loss: 0.6325, train acc: 0.7186, val loss: 0.5585, val acc: 0.7272  (best train acc: 0.7561, best val acc: 0.7524, best train loss: 0.5586  @ epoch 5292 )\n",
      "[Epoch: 5320] train loss: 0.5947, train acc: 0.7346, val loss: 0.5544, val acc: 0.7251  (best train acc: 0.7570, best val acc: 0.7524, best train loss: 0.5586  @ epoch 5292 )\n",
      "[Epoch: 5340] train loss: 0.5717, train acc: 0.7423, val loss: 0.5664, val acc: 0.7039  (best train acc: 0.7570, best val acc: 0.7524, best train loss: 0.5586  @ epoch 5292 )\n",
      "[Epoch: 5360] train loss: 0.6218, train acc: 0.7088, val loss: 0.5602, val acc: 0.7234  (best train acc: 0.7570, best val acc: 0.7524, best train loss: 0.5544  @ epoch 5341 )\n",
      "[Epoch: 5380] train loss: 0.6340, train acc: 0.7197, val loss: 0.5678, val acc: 0.7039  (best train acc: 0.7570, best val acc: 0.7524, best train loss: 0.5544  @ epoch 5341 )\n",
      "[Epoch: 5400] train loss: 0.6029, train acc: 0.7341, val loss: 0.5914, val acc: 0.6840  (best train acc: 0.7570, best val acc: 0.7524, best train loss: 0.5544  @ epoch 5341 )\n",
      "[Epoch: 5420] train loss: 0.5974, train acc: 0.7331, val loss: 0.5608, val acc: 0.7059  (best train acc: 0.7570, best val acc: 0.7524, best train loss: 0.5544  @ epoch 5341 )\n",
      "[Epoch: 5440] train loss: 0.5848, train acc: 0.7406, val loss: 0.5728, val acc: 0.6931  (best train acc: 0.7570, best val acc: 0.7524, best train loss: 0.5544  @ epoch 5341 )\n",
      "[Epoch: 5460] train loss: 0.5683, train acc: 0.7497, val loss: 0.5704, val acc: 0.6938  (best train acc: 0.7570, best val acc: 0.7524, best train loss: 0.5544  @ epoch 5341 )\n",
      "[Epoch: 5480] train loss: 0.5931, train acc: 0.7390, val loss: 0.5751, val acc: 0.7113  (best train acc: 0.7570, best val acc: 0.7524, best train loss: 0.5544  @ epoch 5341 )\n",
      "[Epoch: 5500] train loss: 0.6180, train acc: 0.7145, val loss: 0.5414, val acc: 0.7400  (best train acc: 0.7570, best val acc: 0.7524, best train loss: 0.5544  @ epoch 5341 )\n",
      "[Epoch: 5520] train loss: 0.5876, train acc: 0.7398, val loss: 0.5495, val acc: 0.7218  (best train acc: 0.7570, best val acc: 0.7524, best train loss: 0.5544  @ epoch 5341 )\n",
      "[Epoch: 5540] train loss: 0.5911, train acc: 0.7371, val loss: 0.5618, val acc: 0.7066  (best train acc: 0.7570, best val acc: 0.7524, best train loss: 0.5544  @ epoch 5341 )\n",
      "[Epoch: 5560] train loss: 0.5630, train acc: 0.7465, val loss: 0.5611, val acc: 0.7076  (best train acc: 0.7570, best val acc: 0.7524, best train loss: 0.5544  @ epoch 5341 )\n",
      "[Epoch: 5580] train loss: 0.5763, train acc: 0.7491, val loss: 0.5556, val acc: 0.7194  (best train acc: 0.7570, best val acc: 0.7524, best train loss: 0.5544  @ epoch 5341 )\n",
      "[Epoch: 5600] train loss: 0.5862, train acc: 0.7377, val loss: 0.5543, val acc: 0.7255  (best train acc: 0.7597, best val acc: 0.7524, best train loss: 0.5544  @ epoch 5341 )\n",
      "[Epoch: 5620] train loss: 0.5570, train acc: 0.7540, val loss: 0.5425, val acc: 0.7447  (best train acc: 0.7597, best val acc: 0.7524, best train loss: 0.5544  @ epoch 5341 )\n",
      "[Epoch: 5640] train loss: 0.6023, train acc: 0.7336, val loss: 0.5539, val acc: 0.7305  (best train acc: 0.7597, best val acc: 0.7524, best train loss: 0.5544  @ epoch 5341 )\n",
      "[Epoch: 5660] train loss: 0.6019, train acc: 0.7301, val loss: 0.5517, val acc: 0.7241  (best train acc: 0.7597, best val acc: 0.7524, best train loss: 0.5544  @ epoch 5341 )\n",
      "[Epoch: 5680] train loss: 0.5950, train acc: 0.7332, val loss: 0.5485, val acc: 0.7292  (best train acc: 0.7597, best val acc: 0.7524, best train loss: 0.5544  @ epoch 5341 )\n",
      "[Epoch: 5700] train loss: 0.6105, train acc: 0.7290, val loss: 0.5514, val acc: 0.7164  (best train acc: 0.7597, best val acc: 0.7524, best train loss: 0.5544  @ epoch 5341 )\n",
      "[Epoch: 5720] train loss: 0.5634, train acc: 0.7473, val loss: 0.5665, val acc: 0.6887  (best train acc: 0.7597, best val acc: 0.7524, best train loss: 0.5507  @ epoch 5701 )\n",
      "[Epoch: 5740] train loss: 0.5640, train acc: 0.7543, val loss: 0.5736, val acc: 0.6850  (best train acc: 0.7606, best val acc: 0.7524, best train loss: 0.5507  @ epoch 5701 )\n",
      "[Epoch: 5760] train loss: 0.5623, train acc: 0.7502, val loss: 0.5579, val acc: 0.7103  (best train acc: 0.7606, best val acc: 0.7538, best train loss: 0.5507  @ epoch 5701 )\n",
      "[Epoch: 5780] train loss: 0.5845, train acc: 0.7460, val loss: 0.5420, val acc: 0.7403  (best train acc: 0.7606, best val acc: 0.7538, best train loss: 0.5507  @ epoch 5701 )\n",
      "[Epoch: 5800] train loss: 0.5704, train acc: 0.7423, val loss: 0.5469, val acc: 0.7298  (best train acc: 0.7606, best val acc: 0.7565, best train loss: 0.5507  @ epoch 5701 )\n",
      "[Epoch: 5820] train loss: 0.6036, train acc: 0.7351, val loss: 0.5707, val acc: 0.6890  (best train acc: 0.7606, best val acc: 0.7565, best train loss: 0.5496  @ epoch 5809 )\n",
      "[Epoch: 5840] train loss: 0.5989, train acc: 0.7309, val loss: 0.5752, val acc: 0.6816  (best train acc: 0.7606, best val acc: 0.7565, best train loss: 0.5496  @ epoch 5809 )\n",
      "[Epoch: 5860] train loss: 0.5780, train acc: 0.7465, val loss: 0.5534, val acc: 0.7180  (best train acc: 0.7606, best val acc: 0.7565, best train loss: 0.5496  @ epoch 5809 )\n",
      "[Epoch: 5880] train loss: 0.5999, train acc: 0.7369, val loss: 0.5532, val acc: 0.7174  (best train acc: 0.7606, best val acc: 0.7565, best train loss: 0.5496  @ epoch 5809 )\n",
      "[Epoch: 5900] train loss: 0.5658, train acc: 0.7507, val loss: 0.5474, val acc: 0.7204  (best train acc: 0.7606, best val acc: 0.7565, best train loss: 0.5496  @ epoch 5809 )\n",
      "[Epoch: 5920] train loss: 0.5821, train acc: 0.7417, val loss: 0.5754, val acc: 0.6904  (best train acc: 0.7608, best val acc: 0.7565, best train loss: 0.5493  @ epoch 5906 )\n",
      "[Epoch: 5940] train loss: 0.5636, train acc: 0.7464, val loss: 0.5589, val acc: 0.7170  (best train acc: 0.7608, best val acc: 0.7565, best train loss: 0.5493  @ epoch 5906 )\n",
      "[Epoch: 5960] train loss: 0.5780, train acc: 0.7468, val loss: 0.5866, val acc: 0.6617  (best train acc: 0.7608, best val acc: 0.7565, best train loss: 0.5467  @ epoch 5955 )\n",
      "[Epoch: 5980] train loss: 0.5671, train acc: 0.7530, val loss: 0.5484, val acc: 0.7207  (best train acc: 0.7608, best val acc: 0.7585, best train loss: 0.5467  @ epoch 5955 )\n",
      "[Epoch: 6000] train loss: 0.5742, train acc: 0.7433, val loss: 0.5753, val acc: 0.6830  (best train acc: 0.7608, best val acc: 0.7585, best train loss: 0.5467  @ epoch 5955 )\n",
      "[Epoch: 6020] train loss: 0.5569, train acc: 0.7597, val loss: 0.5757, val acc: 0.6752  (best train acc: 0.7608, best val acc: 0.7585, best train loss: 0.5467  @ epoch 5955 )\n",
      "[Epoch: 6040] train loss: 0.5587, train acc: 0.7494, val loss: 0.5602, val acc: 0.7012  (best train acc: 0.7608, best val acc: 0.7585, best train loss: 0.5467  @ epoch 5955 )\n",
      "[Epoch: 6060] train loss: 0.5510, train acc: 0.7595, val loss: 0.5367, val acc: 0.7336  (best train acc: 0.7608, best val acc: 0.7585, best train loss: 0.5467  @ epoch 5955 )\n",
      "[Epoch: 6080] train loss: 0.5707, train acc: 0.7535, val loss: 0.5489, val acc: 0.7231  (best train acc: 0.7608, best val acc: 0.7585, best train loss: 0.5467  @ epoch 5955 )\n",
      "[Epoch: 6100] train loss: 0.5858, train acc: 0.7368, val loss: 0.5441, val acc: 0.7268  (best train acc: 0.7608, best val acc: 0.7585, best train loss: 0.5467  @ epoch 5955 )\n",
      "[Epoch: 6120] train loss: 0.5754, train acc: 0.7437, val loss: 0.5661, val acc: 0.6985  (best train acc: 0.7608, best val acc: 0.7669, best train loss: 0.5467  @ epoch 5955 )\n",
      "[Epoch: 6140] train loss: 0.5799, train acc: 0.7368, val loss: 0.5369, val acc: 0.7440  (best train acc: 0.7608, best val acc: 0.7669, best train loss: 0.5462  @ epoch 6124 )\n",
      "[Epoch: 6160] train loss: 0.5858, train acc: 0.7389, val loss: 0.5313, val acc: 0.7444  (best train acc: 0.7608, best val acc: 0.7669, best train loss: 0.5462  @ epoch 6124 )\n",
      "[Epoch: 6180] train loss: 0.5836, train acc: 0.7386, val loss: 0.5385, val acc: 0.7430  (best train acc: 0.7608, best val acc: 0.7669, best train loss: 0.5457  @ epoch 6161 )\n",
      "[Epoch: 6200] train loss: 0.5786, train acc: 0.7457, val loss: 0.5622, val acc: 0.6998  (best train acc: 0.7612, best val acc: 0.7669, best train loss: 0.5457  @ epoch 6161 )\n",
      "[Epoch: 6220] train loss: 0.5656, train acc: 0.7460, val loss: 0.5427, val acc: 0.7241  (best train acc: 0.7612, best val acc: 0.7669, best train loss: 0.5449  @ epoch 6211 )\n",
      "[Epoch: 6240] train loss: 0.6010, train acc: 0.7334, val loss: 0.5596, val acc: 0.7309  (best train acc: 0.7612, best val acc: 0.7669, best train loss: 0.5449  @ epoch 6211 )\n",
      "[Epoch: 6260] train loss: 0.5623, train acc: 0.7509, val loss: 0.5537, val acc: 0.7248  (best train acc: 0.7612, best val acc: 0.7669, best train loss: 0.5432  @ epoch 6257 )\n",
      "[Epoch: 6280] train loss: 0.5624, train acc: 0.7483, val loss: 0.5545, val acc: 0.7170  (best train acc: 0.7626, best val acc: 0.7669, best train loss: 0.5432  @ epoch 6257 )\n",
      "[Epoch: 6300] train loss: 0.5664, train acc: 0.7545, val loss: 0.5476, val acc: 0.7248  (best train acc: 0.7626, best val acc: 0.7669, best train loss: 0.5432  @ epoch 6257 )\n",
      "[Epoch: 6320] train loss: 0.5851, train acc: 0.7412, val loss: 0.5586, val acc: 0.7069  (best train acc: 0.7626, best val acc: 0.7669, best train loss: 0.5432  @ epoch 6257 )\n",
      "[Epoch: 6340] train loss: 0.5602, train acc: 0.7457, val loss: 0.5311, val acc: 0.7450  (best train acc: 0.7626, best val acc: 0.7669, best train loss: 0.5432  @ epoch 6257 )\n",
      "[Epoch: 6360] train loss: 0.5675, train acc: 0.7466, val loss: 0.5443, val acc: 0.7336  (best train acc: 0.7626, best val acc: 0.7669, best train loss: 0.5432  @ epoch 6257 )\n",
      "[Epoch: 6380] train loss: 0.5993, train acc: 0.7364, val loss: 0.5736, val acc: 0.6867  (best train acc: 0.7626, best val acc: 0.7669, best train loss: 0.5432  @ epoch 6257 )\n",
      "[Epoch: 6400] train loss: 0.5611, train acc: 0.7506, val loss: 0.5530, val acc: 0.7224  (best train acc: 0.7626, best val acc: 0.7669, best train loss: 0.5432  @ epoch 6257 )\n",
      "[Epoch: 6420] train loss: 0.5779, train acc: 0.7422, val loss: 0.5418, val acc: 0.7481  (best train acc: 0.7626, best val acc: 0.7669, best train loss: 0.5432  @ epoch 6257 )\n",
      "[Epoch: 6440] train loss: 0.5767, train acc: 0.7480, val loss: 0.5361, val acc: 0.7427  (best train acc: 0.7626, best val acc: 0.7669, best train loss: 0.5432  @ epoch 6257 )\n",
      "[Epoch: 6460] train loss: 0.5726, train acc: 0.7492, val loss: 0.5628, val acc: 0.7150  (best train acc: 0.7626, best val acc: 0.7669, best train loss: 0.5380  @ epoch 6458 )\n",
      "[Epoch: 6480] train loss: 0.5535, train acc: 0.7546, val loss: 0.5500, val acc: 0.7282  (best train acc: 0.7626, best val acc: 0.7669, best train loss: 0.5380  @ epoch 6458 )\n",
      "[Epoch: 6500] train loss: 0.5573, train acc: 0.7569, val loss: 0.5487, val acc: 0.7278  (best train acc: 0.7632, best val acc: 0.7669, best train loss: 0.5377  @ epoch 6487 )\n",
      "[Epoch: 6520] train loss: 0.5592, train acc: 0.7593, val loss: 0.5561, val acc: 0.7106  (best train acc: 0.7639, best val acc: 0.7669, best train loss: 0.5377  @ epoch 6487 )\n",
      "[Epoch: 6540] train loss: 0.5509, train acc: 0.7567, val loss: 0.5989, val acc: 0.6742  (best train acc: 0.7639, best val acc: 0.7879, best train loss: 0.5377  @ epoch 6487 )\n",
      "[Epoch: 6560] train loss: 0.5503, train acc: 0.7530, val loss: 0.5444, val acc: 0.7423  (best train acc: 0.7708, best val acc: 0.7879, best train loss: 0.5364  @ epoch 6547 )\n",
      "[Epoch: 6580] train loss: 0.5541, train acc: 0.7483, val loss: 0.5221, val acc: 0.7676  (best train acc: 0.7708, best val acc: 0.7879, best train loss: 0.5363  @ epoch 6573 )\n",
      "[Epoch: 6600] train loss: 0.5592, train acc: 0.7644, val loss: 0.5357, val acc: 0.7626  (best train acc: 0.7708, best val acc: 0.7879, best train loss: 0.5363  @ epoch 6573 )\n",
      "[Epoch: 6620] train loss: 0.5647, train acc: 0.7519, val loss: 0.5147, val acc: 0.7764  (best train acc: 0.7708, best val acc: 0.7879, best train loss: 0.5313  @ epoch 6602 )\n",
      "[Epoch: 6640] train loss: 0.5548, train acc: 0.7545, val loss: 0.5397, val acc: 0.7332  (best train acc: 0.7708, best val acc: 0.7879, best train loss: 0.5313  @ epoch 6602 )\n",
      "[Epoch: 6660] train loss: 0.5807, train acc: 0.7414, val loss: 0.5292, val acc: 0.7518  (best train acc: 0.7708, best val acc: 0.7879, best train loss: 0.5313  @ epoch 6602 )\n",
      "[Epoch: 6680] train loss: 0.5604, train acc: 0.7533, val loss: 0.5485, val acc: 0.7238  (best train acc: 0.7708, best val acc: 0.7879, best train loss: 0.5313  @ epoch 6602 )\n",
      "[Epoch: 6700] train loss: 0.5635, train acc: 0.7468, val loss: 0.5393, val acc: 0.7309  (best train acc: 0.7708, best val acc: 0.7879, best train loss: 0.5307  @ epoch 6687 )\n",
      "[Epoch: 6720] train loss: 0.5366, train acc: 0.7702, val loss: 0.5312, val acc: 0.7423  (best train acc: 0.7708, best val acc: 0.7879, best train loss: 0.5293  @ epoch 6705 )\n",
      "[Epoch: 6740] train loss: 0.5473, train acc: 0.7577, val loss: 0.5366, val acc: 0.7379  (best train acc: 0.7708, best val acc: 0.7879, best train loss: 0.5293  @ epoch 6705 )\n",
      "[Epoch: 6760] train loss: 0.5713, train acc: 0.7435, val loss: 0.5206, val acc: 0.7622  (best train acc: 0.7708, best val acc: 0.7879, best train loss: 0.5293  @ epoch 6705 )\n",
      "[Epoch: 6780] train loss: 0.5629, train acc: 0.7543, val loss: 0.5433, val acc: 0.7214  (best train acc: 0.7730, best val acc: 0.7879, best train loss: 0.5293  @ epoch 6705 )\n",
      "[Epoch: 6800] train loss: 0.5484, train acc: 0.7578, val loss: 0.5146, val acc: 0.7700  (best train acc: 0.7730, best val acc: 0.7879, best train loss: 0.5293  @ epoch 6705 )\n",
      "[Epoch: 6820] train loss: 0.5590, train acc: 0.7538, val loss: 0.5590, val acc: 0.7012  (best train acc: 0.7730, best val acc: 0.7879, best train loss: 0.5293  @ epoch 6705 )\n",
      "[Epoch: 6840] train loss: 0.5953, train acc: 0.7360, val loss: 0.5557, val acc: 0.7042  (best train acc: 0.7730, best val acc: 0.7879, best train loss: 0.5293  @ epoch 6705 )\n",
      "[Epoch: 6860] train loss: 0.5478, train acc: 0.7578, val loss: 0.5334, val acc: 0.7457  (best train acc: 0.7730, best val acc: 0.7879, best train loss: 0.5293  @ epoch 6705 )\n",
      "[Epoch: 6880] train loss: 0.5560, train acc: 0.7574, val loss: 0.5402, val acc: 0.7420  (best train acc: 0.7730, best val acc: 0.7879, best train loss: 0.5229  @ epoch 6872 )\n",
      "[Epoch: 6900] train loss: 0.5394, train acc: 0.7653, val loss: 0.5440, val acc: 0.7272  (best train acc: 0.7730, best val acc: 0.7879, best train loss: 0.5190  @ epoch 6889 )\n",
      "[Epoch: 6920] train loss: 0.5333, train acc: 0.7694, val loss: 0.5261, val acc: 0.7558  (best train acc: 0.7730, best val acc: 0.7879, best train loss: 0.5190  @ epoch 6889 )\n",
      "[Epoch: 6940] train loss: 0.5253, train acc: 0.7746, val loss: 0.5295, val acc: 0.7487  (best train acc: 0.7746, best val acc: 0.7879, best train loss: 0.5190  @ epoch 6889 )\n",
      "[Epoch: 6960] train loss: 0.5177, train acc: 0.7747, val loss: 0.5353, val acc: 0.7329  (best train acc: 0.7767, best val acc: 0.7879, best train loss: 0.5176  @ epoch 6949 )\n",
      "[Epoch: 6980] train loss: 0.5326, train acc: 0.7666, val loss: 0.5225, val acc: 0.7639  (best train acc: 0.7767, best val acc: 0.7879, best train loss: 0.5176  @ epoch 6949 )\n",
      "[Epoch: 7000] train loss: 0.5294, train acc: 0.7741, val loss: 0.5236, val acc: 0.7497  (best train acc: 0.7791, best val acc: 0.7879, best train loss: 0.5061  @ epoch 6998 )\n",
      "[Epoch: 7020] train loss: 0.5531, train acc: 0.7556, val loss: 0.5263, val acc: 0.7514  (best train acc: 0.7791, best val acc: 0.7879, best train loss: 0.5061  @ epoch 6998 )\n",
      "[Epoch: 7040] train loss: 0.5319, train acc: 0.7723, val loss: 0.5833, val acc: 0.6874  (best train acc: 0.7791, best val acc: 0.7879, best train loss: 0.5061  @ epoch 6998 )\n",
      "[Epoch: 7060] train loss: 0.5499, train acc: 0.7596, val loss: 0.5582, val acc: 0.7258  (best train acc: 0.7800, best val acc: 0.7879, best train loss: 0.5061  @ epoch 6998 )\n",
      "[Epoch: 7080] train loss: 0.5268, train acc: 0.7658, val loss: 0.5427, val acc: 0.7211  (best train acc: 0.7800, best val acc: 0.7879, best train loss: 0.5061  @ epoch 6998 )\n",
      "[Epoch: 7100] train loss: 0.5439, train acc: 0.7611, val loss: 0.5121, val acc: 0.7703  (best train acc: 0.7800, best val acc: 0.7879, best train loss: 0.5061  @ epoch 6998 )\n",
      "[Epoch: 7120] train loss: 0.5445, train acc: 0.7621, val loss: 0.5369, val acc: 0.7417  (best train acc: 0.7800, best val acc: 0.7879, best train loss: 0.5061  @ epoch 6998 )\n",
      "[Epoch: 7140] train loss: 0.5433, train acc: 0.7632, val loss: 0.5238, val acc: 0.7636  (best train acc: 0.7800, best val acc: 0.7879, best train loss: 0.5053  @ epoch 7127 )\n",
      "[Epoch: 7160] train loss: 0.5182, train acc: 0.7715, val loss: 0.5241, val acc: 0.7720  (best train acc: 0.7834, best val acc: 0.7879, best train loss: 0.5053  @ epoch 7127 )\n",
      "[Epoch: 7180] train loss: 0.5293, train acc: 0.7649, val loss: 0.5249, val acc: 0.7595  (best train acc: 0.7834, best val acc: 0.7879, best train loss: 0.5053  @ epoch 7127 )\n",
      "[Epoch: 7200] train loss: 0.5184, train acc: 0.7743, val loss: 0.5483, val acc: 0.7191  (best train acc: 0.7834, best val acc: 0.7879, best train loss: 0.5050  @ epoch 7197 )\n",
      "[Epoch: 7220] train loss: 0.5099, train acc: 0.7717, val loss: 0.5441, val acc: 0.7221  (best train acc: 0.7834, best val acc: 0.7879, best train loss: 0.5050  @ epoch 7197 )\n",
      "[Epoch: 7240] train loss: 0.5176, train acc: 0.7700, val loss: 0.5277, val acc: 0.7481  (best train acc: 0.7834, best val acc: 0.7879, best train loss: 0.5050  @ epoch 7197 )\n",
      "[Epoch: 7260] train loss: 0.5340, train acc: 0.7673, val loss: 0.5396, val acc: 0.7541  (best train acc: 0.7834, best val acc: 0.7879, best train loss: 0.5050  @ epoch 7197 )\n",
      "[Epoch: 7280] train loss: 0.5318, train acc: 0.7731, val loss: 0.5325, val acc: 0.7470  (best train acc: 0.7834, best val acc: 0.7879, best train loss: 0.5050  @ epoch 7197 )\n",
      "[Epoch: 7300] train loss: 0.5196, train acc: 0.7739, val loss: 0.5310, val acc: 0.7484  (best train acc: 0.7834, best val acc: 0.7879, best train loss: 0.5037  @ epoch 7293 )\n",
      "[Epoch: 7320] train loss: 0.5400, train acc: 0.7595, val loss: 0.5159, val acc: 0.7433  (best train acc: 0.7834, best val acc: 0.7879, best train loss: 0.5020  @ epoch 7304 )\n",
      "[Epoch: 7340] train loss: 0.5089, train acc: 0.7760, val loss: 0.5234, val acc: 0.7528  (best train acc: 0.7834, best val acc: 0.7879, best train loss: 0.5020  @ epoch 7304 )\n",
      "[Epoch: 7360] train loss: 0.5312, train acc: 0.7623, val loss: 0.5256, val acc: 0.7349  (best train acc: 0.7834, best val acc: 0.7879, best train loss: 0.5020  @ epoch 7304 )\n",
      "[Epoch: 7380] train loss: 0.5432, train acc: 0.7596, val loss: 0.5175, val acc: 0.7504  (best train acc: 0.7834, best val acc: 0.7879, best train loss: 0.5020  @ epoch 7304 )\n",
      "[Epoch: 7400] train loss: 0.5168, train acc: 0.7700, val loss: 0.5362, val acc: 0.7450  (best train acc: 0.7834, best val acc: 0.7879, best train loss: 0.5020  @ epoch 7304 )\n",
      "[Epoch: 7420] train loss: 0.5093, train acc: 0.7707, val loss: 0.5301, val acc: 0.7302  (best train acc: 0.7877, best val acc: 0.7879, best train loss: 0.4929  @ epoch 7406 )\n",
      "[Epoch: 7440] train loss: 0.5066, train acc: 0.7731, val loss: 0.5303, val acc: 0.7329  (best train acc: 0.7877, best val acc: 0.7879, best train loss: 0.4929  @ epoch 7406 )\n",
      "[Epoch: 7460] train loss: 0.5014, train acc: 0.7825, val loss: 0.5263, val acc: 0.7430  (best train acc: 0.7877, best val acc: 0.7879, best train loss: 0.4917  @ epoch 7456 )\n",
      "[Epoch: 7480] train loss: 0.5169, train acc: 0.7740, val loss: 0.5279, val acc: 0.7427  (best train acc: 0.7877, best val acc: 0.7879, best train loss: 0.4893  @ epoch 7478 )\n",
      "[Epoch: 7500] train loss: 0.4983, train acc: 0.7867, val loss: 0.5170, val acc: 0.7585  (best train acc: 0.7888, best val acc: 0.7879, best train loss: 0.4862  @ epoch 7498 )\n",
      "[Epoch: 7520] train loss: 0.5207, train acc: 0.7707, val loss: 0.5130, val acc: 0.7575  (best train acc: 0.7896, best val acc: 0.7879, best train loss: 0.4862  @ epoch 7498 )\n",
      "[Epoch: 7540] train loss: 0.5239, train acc: 0.7699, val loss: 0.5296, val acc: 0.7352  (best train acc: 0.7896, best val acc: 0.7879, best train loss: 0.4861  @ epoch 7533 )\n",
      "[Epoch: 7560] train loss: 0.4939, train acc: 0.7825, val loss: 0.5381, val acc: 0.7423  (best train acc: 0.7896, best val acc: 0.7879, best train loss: 0.4856  @ epoch 7547 )\n",
      "[Epoch: 7580] train loss: 0.5153, train acc: 0.7765, val loss: 0.5388, val acc: 0.7251  (best train acc: 0.7896, best val acc: 0.7879, best train loss: 0.4845  @ epoch 7566 )\n",
      "[Epoch: 7600] train loss: 0.5113, train acc: 0.7692, val loss: 0.5304, val acc: 0.7339  (best train acc: 0.7896, best val acc: 0.7879, best train loss: 0.4845  @ epoch 7566 )\n",
      "[Epoch: 7620] train loss: 0.4952, train acc: 0.7846, val loss: 0.5309, val acc: 0.7396  (best train acc: 0.7896, best val acc: 0.7879, best train loss: 0.4845  @ epoch 7566 )\n",
      "[Epoch: 7640] train loss: 0.4950, train acc: 0.7820, val loss: 0.5327, val acc: 0.7339  (best train acc: 0.7896, best val acc: 0.7879, best train loss: 0.4840  @ epoch 7624 )\n",
      "[Epoch: 7660] train loss: 0.4770, train acc: 0.7900, val loss: 0.5158, val acc: 0.7609  (best train acc: 0.7900, best val acc: 0.7879, best train loss: 0.4770  @ epoch 7660 )\n",
      "[Epoch: 7680] train loss: 0.5055, train acc: 0.7760, val loss: 0.5219, val acc: 0.7605  (best train acc: 0.7900, best val acc: 0.7879, best train loss: 0.4770  @ epoch 7660 )\n",
      "[Epoch: 7700] train loss: 0.5044, train acc: 0.7817, val loss: 0.5198, val acc: 0.7474  (best train acc: 0.7974, best val acc: 0.7879, best train loss: 0.4770  @ epoch 7660 )\n",
      "[Epoch: 7720] train loss: 0.5101, train acc: 0.7702, val loss: 0.5303, val acc: 0.7403  (best train acc: 0.7974, best val acc: 0.7879, best train loss: 0.4770  @ epoch 7660 )\n",
      "[Epoch: 7740] train loss: 0.5025, train acc: 0.7772, val loss: 0.5113, val acc: 0.7707  (best train acc: 0.7974, best val acc: 0.7879, best train loss: 0.4770  @ epoch 7660 )\n",
      "[Epoch: 7760] train loss: 0.4960, train acc: 0.7801, val loss: 0.4986, val acc: 0.7730  (best train acc: 0.7974, best val acc: 0.7879, best train loss: 0.4770  @ epoch 7660 )\n",
      "[Epoch: 7780] train loss: 0.4940, train acc: 0.7825, val loss: 0.5174, val acc: 0.7521  (best train acc: 0.7974, best val acc: 0.7879, best train loss: 0.4770  @ epoch 7660 )\n",
      "[Epoch: 7800] train loss: 0.5204, train acc: 0.7658, val loss: 0.5257, val acc: 0.7383  (best train acc: 0.7974, best val acc: 0.7879, best train loss: 0.4716  @ epoch 7786 )\n",
      "[Epoch: 7820] train loss: 0.4938, train acc: 0.7768, val loss: 0.5254, val acc: 0.7322  (best train acc: 0.7974, best val acc: 0.7879, best train loss: 0.4716  @ epoch 7786 )\n",
      "[Epoch: 7840] train loss: 0.5008, train acc: 0.7724, val loss: 0.5316, val acc: 0.7339  (best train acc: 0.7974, best val acc: 0.7879, best train loss: 0.4716  @ epoch 7786 )\n",
      "[Epoch: 7860] train loss: 0.5166, train acc: 0.7711, val loss: 0.5580, val acc: 0.7241  (best train acc: 0.7974, best val acc: 0.7879, best train loss: 0.4716  @ epoch 7786 )\n",
      "[Epoch: 7880] train loss: 0.4963, train acc: 0.7825, val loss: 0.5142, val acc: 0.7518  (best train acc: 0.7974, best val acc: 0.7879, best train loss: 0.4716  @ epoch 7786 )\n",
      "[Epoch: 7900] train loss: 0.5068, train acc: 0.7794, val loss: 0.5070, val acc: 0.7649  (best train acc: 0.7974, best val acc: 0.7879, best train loss: 0.4656  @ epoch 7889 )\n",
      "[Epoch: 7920] train loss: 0.5146, train acc: 0.7647, val loss: 0.5027, val acc: 0.7690  (best train acc: 0.7974, best val acc: 0.7879, best train loss: 0.4656  @ epoch 7889 )\n",
      "[Epoch: 7940] train loss: 0.4945, train acc: 0.7856, val loss: 0.5212, val acc: 0.7440  (best train acc: 0.7974, best val acc: 0.7879, best train loss: 0.4656  @ epoch 7889 )\n",
      "[Epoch: 7960] train loss: 0.5041, train acc: 0.7727, val loss: 0.5151, val acc: 0.7511  (best train acc: 0.7974, best val acc: 0.7879, best train loss: 0.4656  @ epoch 7889 )\n",
      "[Epoch: 7980] train loss: 0.4853, train acc: 0.7856, val loss: 0.5142, val acc: 0.7565  (best train acc: 0.7974, best val acc: 0.7879, best train loss: 0.4656  @ epoch 7889 )\n",
      "[Epoch: 8000] train loss: 0.5109, train acc: 0.7693, val loss: 0.5193, val acc: 0.7413  (best train acc: 0.7974, best val acc: 0.7879, best train loss: 0.4656  @ epoch 7889 )\n",
      "[Epoch: 8020] train loss: 0.4886, train acc: 0.7817, val loss: 0.5235, val acc: 0.7504  (best train acc: 0.7974, best val acc: 0.7879, best train loss: 0.4656  @ epoch 7889 )\n",
      "[Epoch: 8040] train loss: 0.4987, train acc: 0.7846, val loss: 0.5098, val acc: 0.7568  (best train acc: 0.7974, best val acc: 0.7879, best train loss: 0.4656  @ epoch 7889 )\n",
      "[Epoch: 8060] train loss: 0.4939, train acc: 0.7859, val loss: 0.5005, val acc: 0.7717  (best train acc: 0.7974, best val acc: 0.7879, best train loss: 0.4656  @ epoch 7889 )\n",
      "[Epoch: 8080] train loss: 0.5065, train acc: 0.7739, val loss: 0.5199, val acc: 0.7508  (best train acc: 0.7974, best val acc: 0.7879, best train loss: 0.4656  @ epoch 7889 )\n",
      "[Epoch: 8100] train loss: 0.5093, train acc: 0.7747, val loss: 0.5140, val acc: 0.7609  (best train acc: 0.7974, best val acc: 0.7879, best train loss: 0.4656  @ epoch 7889 )\n",
      "[Epoch: 8120] train loss: 0.4870, train acc: 0.7804, val loss: 0.5401, val acc: 0.7278  (best train acc: 0.7974, best val acc: 0.7879, best train loss: 0.4656  @ epoch 7889 )\n",
      "[Epoch: 8140] train loss: 0.4781, train acc: 0.7899, val loss: 0.5095, val acc: 0.7572  (best train acc: 0.7974, best val acc: 0.7879, best train loss: 0.4656  @ epoch 7889 )\n",
      "[Epoch: 8160] train loss: 0.4987, train acc: 0.7869, val loss: 0.5249, val acc: 0.7447  (best train acc: 0.7974, best val acc: 0.7879, best train loss: 0.4656  @ epoch 7889 )\n",
      "[Epoch: 8180] train loss: 0.5184, train acc: 0.7739, val loss: 0.5189, val acc: 0.7460  (best train acc: 0.7974, best val acc: 0.7879, best train loss: 0.4646  @ epoch 8177 )\n",
      "[Epoch: 8200] train loss: 0.4857, train acc: 0.7822, val loss: 0.5079, val acc: 0.7528  (best train acc: 0.7974, best val acc: 0.7912, best train loss: 0.4646  @ epoch 8177 )\n",
      "[Epoch: 8220] train loss: 0.4873, train acc: 0.7849, val loss: 0.5173, val acc: 0.7551  (best train acc: 0.7974, best val acc: 0.7912, best train loss: 0.4646  @ epoch 8177 )\n",
      "[Epoch: 8240] train loss: 0.4798, train acc: 0.7849, val loss: 0.5328, val acc: 0.7265  (best train acc: 0.7974, best val acc: 0.7912, best train loss: 0.4646  @ epoch 8177 )\n",
      "[Epoch: 8260] train loss: 0.4850, train acc: 0.7896, val loss: 0.5232, val acc: 0.7417  (best train acc: 0.7974, best val acc: 0.7912, best train loss: 0.4646  @ epoch 8177 )\n",
      "[Epoch: 8280] train loss: 0.4812, train acc: 0.7916, val loss: 0.5094, val acc: 0.7602  (best train acc: 0.7974, best val acc: 0.7912, best train loss: 0.4646  @ epoch 8177 )\n",
      "[Epoch: 8300] train loss: 0.5097, train acc: 0.7742, val loss: 0.5330, val acc: 0.7207  (best train acc: 0.7974, best val acc: 0.7912, best train loss: 0.4646  @ epoch 8177 )\n",
      "[Epoch: 8320] train loss: 0.4636, train acc: 0.7987, val loss: 0.5141, val acc: 0.7572  (best train acc: 0.7987, best val acc: 0.7912, best train loss: 0.4636  @ epoch 8320 )\n",
      "[Epoch: 8340] train loss: 0.4814, train acc: 0.7895, val loss: 0.5334, val acc: 0.7356  (best train acc: 0.7987, best val acc: 0.7912, best train loss: 0.4634  @ epoch 8330 )\n",
      "[Epoch: 8360] train loss: 0.4994, train acc: 0.7841, val loss: 0.5300, val acc: 0.7305  (best train acc: 0.7987, best val acc: 0.7912, best train loss: 0.4634  @ epoch 8330 )\n",
      "[Epoch: 8380] train loss: 0.4841, train acc: 0.7824, val loss: 0.5135, val acc: 0.7538  (best train acc: 0.7987, best val acc: 0.7912, best train loss: 0.4634  @ epoch 8330 )\n",
      "[Epoch: 8400] train loss: 0.4763, train acc: 0.7916, val loss: 0.5424, val acc: 0.7194  (best train acc: 0.7987, best val acc: 0.7912, best train loss: 0.4634  @ epoch 8330 )\n",
      "[Epoch: 8420] train loss: 0.4991, train acc: 0.7804, val loss: 0.5236, val acc: 0.7356  (best train acc: 0.7987, best val acc: 0.7956, best train loss: 0.4634  @ epoch 8330 )\n",
      "[Epoch: 8440] train loss: 0.5213, train acc: 0.7685, val loss: 0.5185, val acc: 0.7595  (best train acc: 0.7987, best val acc: 0.7956, best train loss: 0.4631  @ epoch 8437 )\n",
      "[Epoch: 8460] train loss: 0.4893, train acc: 0.7783, val loss: 0.5105, val acc: 0.7622  (best train acc: 0.7987, best val acc: 0.7956, best train loss: 0.4631  @ epoch 8437 )\n",
      "[Epoch: 8480] train loss: 0.4900, train acc: 0.7786, val loss: 0.5412, val acc: 0.7305  (best train acc: 0.7987, best val acc: 0.7956, best train loss: 0.4631  @ epoch 8437 )\n",
      "[Epoch: 8500] train loss: 0.5022, train acc: 0.7786, val loss: 0.5007, val acc: 0.7669  (best train acc: 0.7987, best val acc: 0.7956, best train loss: 0.4631  @ epoch 8437 )\n",
      "[Epoch: 8520] train loss: 0.5013, train acc: 0.7734, val loss: 0.5157, val acc: 0.7477  (best train acc: 0.7989, best val acc: 0.7956, best train loss: 0.4631  @ epoch 8437 )\n",
      "[Epoch: 8540] train loss: 0.5053, train acc: 0.7812, val loss: 0.5043, val acc: 0.7659  (best train acc: 0.7989, best val acc: 0.7956, best train loss: 0.4631  @ epoch 8437 )\n",
      "[Epoch: 8560] train loss: 0.4863, train acc: 0.7884, val loss: 0.5142, val acc: 0.7508  (best train acc: 0.7989, best val acc: 0.7956, best train loss: 0.4631  @ epoch 8437 )\n",
      "[Epoch: 8580] train loss: 0.4800, train acc: 0.7883, val loss: 0.5175, val acc: 0.7541  (best train acc: 0.7990, best val acc: 0.7956, best train loss: 0.4631  @ epoch 8437 )\n",
      "[Epoch: 8600] train loss: 0.4863, train acc: 0.7838, val loss: 0.5118, val acc: 0.7595  (best train acc: 0.7990, best val acc: 0.7956, best train loss: 0.4631  @ epoch 8437 )\n",
      "[Epoch: 8620] train loss: 0.4968, train acc: 0.7773, val loss: 0.5371, val acc: 0.7302  (best train acc: 0.7990, best val acc: 0.7956, best train loss: 0.4631  @ epoch 8437 )\n",
      "[Epoch: 8640] train loss: 0.4833, train acc: 0.7882, val loss: 0.5155, val acc: 0.7578  (best train acc: 0.7990, best val acc: 0.7956, best train loss: 0.4631  @ epoch 8437 )\n",
      "[Epoch: 8660] train loss: 0.4810, train acc: 0.7903, val loss: 0.5011, val acc: 0.7642  (best train acc: 0.7990, best val acc: 0.7956, best train loss: 0.4631  @ epoch 8437 )\n",
      "[Epoch: 8680] train loss: 0.4865, train acc: 0.7885, val loss: 0.5069, val acc: 0.7538  (best train acc: 0.7990, best val acc: 0.7956, best train loss: 0.4598  @ epoch 8668 )\n",
      "[Epoch: 8700] train loss: 0.4721, train acc: 0.7946, val loss: 0.5285, val acc: 0.7423  (best train acc: 0.7990, best val acc: 0.7956, best train loss: 0.4588  @ epoch 8686 )\n",
      "[Epoch: 8720] train loss: 0.5102, train acc: 0.7698, val loss: 0.5153, val acc: 0.7599  (best train acc: 0.7990, best val acc: 0.7956, best train loss: 0.4588  @ epoch 8686 )\n",
      "[Epoch: 8740] train loss: 0.4720, train acc: 0.7940, val loss: 0.5141, val acc: 0.7521  (best train acc: 0.7990, best val acc: 0.7956, best train loss: 0.4588  @ epoch 8686 )\n",
      "[Epoch: 8760] train loss: 0.4850, train acc: 0.7873, val loss: 0.5352, val acc: 0.7305  (best train acc: 0.7990, best val acc: 0.7956, best train loss: 0.4588  @ epoch 8686 )\n",
      "[Epoch: 8780] train loss: 0.4756, train acc: 0.7895, val loss: 0.5113, val acc: 0.7605  (best train acc: 0.7990, best val acc: 0.7956, best train loss: 0.4588  @ epoch 8686 )\n",
      "[Epoch: 8800] train loss: 0.5056, train acc: 0.7786, val loss: 0.5444, val acc: 0.7211  (best train acc: 0.7990, best val acc: 0.7956, best train loss: 0.4588  @ epoch 8686 )\n",
      "[Epoch: 8820] train loss: 0.4653, train acc: 0.7931, val loss: 0.5133, val acc: 0.7460  (best train acc: 0.7990, best val acc: 0.7956, best train loss: 0.4588  @ epoch 8686 )\n",
      "[Epoch: 8840] train loss: 0.4792, train acc: 0.7921, val loss: 0.4928, val acc: 0.7734  (best train acc: 0.7990, best val acc: 0.7956, best train loss: 0.4588  @ epoch 8686 )\n",
      "[Epoch: 8860] train loss: 0.4803, train acc: 0.7912, val loss: 0.5038, val acc: 0.7663  (best train acc: 0.7990, best val acc: 0.7956, best train loss: 0.4588  @ epoch 8686 )\n",
      "[Epoch: 8880] train loss: 0.5037, train acc: 0.7836, val loss: 0.5092, val acc: 0.7572  (best train acc: 0.7990, best val acc: 0.7956, best train loss: 0.4588  @ epoch 8686 )\n",
      "[Epoch: 8900] train loss: 0.4821, train acc: 0.7867, val loss: 0.5050, val acc: 0.7558  (best train acc: 0.7990, best val acc: 0.7956, best train loss: 0.4584  @ epoch 8893 )\n",
      "[Epoch: 8920] train loss: 0.4734, train acc: 0.7893, val loss: 0.5012, val acc: 0.7686  (best train acc: 0.7990, best val acc: 0.7956, best train loss: 0.4584  @ epoch 8893 )\n",
      "[Epoch: 8940] train loss: 0.4830, train acc: 0.7812, val loss: 0.5198, val acc: 0.7646  (best train acc: 0.7990, best val acc: 0.7956, best train loss: 0.4584  @ epoch 8893 )\n",
      "[Epoch: 8960] train loss: 0.4776, train acc: 0.7885, val loss: 0.5002, val acc: 0.7673  (best train acc: 0.8002, best val acc: 0.7956, best train loss: 0.4584  @ epoch 8893 )\n",
      "[Epoch: 8980] train loss: 0.4927, train acc: 0.7766, val loss: 0.5056, val acc: 0.7663  (best train acc: 0.8002, best val acc: 0.7956, best train loss: 0.4584  @ epoch 8893 )\n",
      "[Epoch: 9000] train loss: 0.4715, train acc: 0.7918, val loss: 0.5018, val acc: 0.7612  (best train acc: 0.8007, best val acc: 0.7956, best train loss: 0.4494  @ epoch 8995 )\n",
      "[Epoch: 9020] train loss: 0.4643, train acc: 0.7937, val loss: 0.4998, val acc: 0.7757  (best train acc: 0.8007, best val acc: 0.7956, best train loss: 0.4494  @ epoch 8995 )\n",
      "[Epoch: 9040] train loss: 0.4789, train acc: 0.7836, val loss: 0.5179, val acc: 0.7383  (best train acc: 0.8007, best val acc: 0.7956, best train loss: 0.4494  @ epoch 8995 )\n",
      "[Epoch: 9060] train loss: 0.4729, train acc: 0.7930, val loss: 0.5013, val acc: 0.7632  (best train acc: 0.8007, best val acc: 0.7956, best train loss: 0.4494  @ epoch 8995 )\n",
      "[Epoch: 9080] train loss: 0.4685, train acc: 0.7935, val loss: 0.5195, val acc: 0.7457  (best train acc: 0.8007, best val acc: 0.7956, best train loss: 0.4494  @ epoch 8995 )\n",
      "[Epoch: 9100] train loss: 0.4795, train acc: 0.7859, val loss: 0.5020, val acc: 0.7676  (best train acc: 0.8007, best val acc: 0.7956, best train loss: 0.4494  @ epoch 8995 )\n",
      "[Epoch: 9120] train loss: 0.4896, train acc: 0.7835, val loss: 0.5092, val acc: 0.7595  (best train acc: 0.8007, best val acc: 0.7956, best train loss: 0.4494  @ epoch 8995 )\n",
      "[Epoch: 9140] train loss: 0.5084, train acc: 0.7813, val loss: 0.5062, val acc: 0.7535  (best train acc: 0.8007, best val acc: 0.7956, best train loss: 0.4494  @ epoch 8995 )\n",
      "[Epoch: 9160] train loss: 0.4700, train acc: 0.7901, val loss: 0.5019, val acc: 0.7676  (best train acc: 0.8007, best val acc: 0.7956, best train loss: 0.4494  @ epoch 8995 )\n",
      "[Epoch: 9180] train loss: 0.5076, train acc: 0.7814, val loss: 0.5117, val acc: 0.7551  (best train acc: 0.8007, best val acc: 0.7956, best train loss: 0.4494  @ epoch 8995 )\n",
      "[Epoch: 9200] train loss: 0.4887, train acc: 0.7846, val loss: 0.4968, val acc: 0.7720  (best train acc: 0.8010, best val acc: 0.7956, best train loss: 0.4494  @ epoch 8995 )\n",
      "[Epoch: 9220] train loss: 0.4813, train acc: 0.7846, val loss: 0.5060, val acc: 0.7612  (best train acc: 0.8010, best val acc: 0.7956, best train loss: 0.4494  @ epoch 8995 )\n",
      "[Epoch: 9240] train loss: 0.4679, train acc: 0.7854, val loss: 0.5098, val acc: 0.7528  (best train acc: 0.8010, best val acc: 0.7956, best train loss: 0.4494  @ epoch 8995 )\n",
      "[Epoch: 9260] train loss: 0.4884, train acc: 0.7821, val loss: 0.5034, val acc: 0.7622  (best train acc: 0.8010, best val acc: 0.7956, best train loss: 0.4494  @ epoch 8995 )\n",
      "[Epoch: 9280] train loss: 0.4646, train acc: 0.7942, val loss: 0.5234, val acc: 0.7511  (best train acc: 0.8010, best val acc: 0.7956, best train loss: 0.4494  @ epoch 8995 )\n",
      "[Epoch: 9300] train loss: 0.4767, train acc: 0.7851, val loss: 0.5058, val acc: 0.7605  (best train acc: 0.8010, best val acc: 0.7956, best train loss: 0.4494  @ epoch 8995 )\n",
      "[Epoch: 9320] train loss: 0.4782, train acc: 0.7833, val loss: 0.5278, val acc: 0.7379  (best train acc: 0.8010, best val acc: 0.7956, best train loss: 0.4494  @ epoch 8995 )\n",
      "[Epoch: 9340] train loss: 0.4723, train acc: 0.7888, val loss: 0.5218, val acc: 0.7535  (best train acc: 0.8064, best val acc: 0.7956, best train loss: 0.4449  @ epoch 9331 )\n",
      "[Epoch: 9360] train loss: 0.4627, train acc: 0.7967, val loss: 0.5072, val acc: 0.7504  (best train acc: 0.8064, best val acc: 0.7956, best train loss: 0.4449  @ epoch 9331 )\n",
      "[Epoch: 9380] train loss: 0.4792, train acc: 0.7886, val loss: 0.4992, val acc: 0.7632  (best train acc: 0.8064, best val acc: 0.7956, best train loss: 0.4449  @ epoch 9331 )\n",
      "[Epoch: 9400] train loss: 0.5216, train acc: 0.7708, val loss: 0.4858, val acc: 0.7811  (best train acc: 0.8064, best val acc: 0.7956, best train loss: 0.4449  @ epoch 9331 )\n",
      "[Epoch: 9420] train loss: 0.5049, train acc: 0.7752, val loss: 0.4952, val acc: 0.7696  (best train acc: 0.8064, best val acc: 0.7956, best train loss: 0.4449  @ epoch 9331 )\n",
      "[Epoch: 9440] train loss: 0.4871, train acc: 0.7741, val loss: 0.4897, val acc: 0.7761  (best train acc: 0.8064, best val acc: 0.7956, best train loss: 0.4449  @ epoch 9331 )\n",
      "[Epoch: 9460] train loss: 0.5005, train acc: 0.7807, val loss: 0.5265, val acc: 0.7457  (best train acc: 0.8064, best val acc: 0.7956, best train loss: 0.4449  @ epoch 9331 )\n",
      "[Epoch: 9480] train loss: 0.4866, train acc: 0.7828, val loss: 0.5033, val acc: 0.7676  (best train acc: 0.8064, best val acc: 0.7956, best train loss: 0.4449  @ epoch 9331 )\n",
      "[Epoch: 9500] train loss: 0.4840, train acc: 0.7858, val loss: 0.4977, val acc: 0.7696  (best train acc: 0.8064, best val acc: 0.7956, best train loss: 0.4449  @ epoch 9331 )\n",
      "[Epoch: 9520] train loss: 0.4815, train acc: 0.7887, val loss: 0.4870, val acc: 0.7841  (best train acc: 0.8064, best val acc: 0.7956, best train loss: 0.4449  @ epoch 9331 )\n",
      "[Epoch: 9540] train loss: 0.4771, train acc: 0.7816, val loss: 0.4902, val acc: 0.7723  (best train acc: 0.8064, best val acc: 0.7963, best train loss: 0.4449  @ epoch 9331 )\n",
      "[Epoch: 9560] train loss: 0.4779, train acc: 0.7890, val loss: 0.5062, val acc: 0.7642  (best train acc: 0.8064, best val acc: 0.7963, best train loss: 0.4449  @ epoch 9331 )\n",
      "[Epoch: 9580] train loss: 0.4749, train acc: 0.7921, val loss: 0.5117, val acc: 0.7551  (best train acc: 0.8064, best val acc: 0.7963, best train loss: 0.4449  @ epoch 9331 )\n",
      "[Epoch: 9600] train loss: 0.4855, train acc: 0.7877, val loss: 0.5010, val acc: 0.7592  (best train acc: 0.8064, best val acc: 0.7963, best train loss: 0.4449  @ epoch 9331 )\n",
      "[Epoch: 9620] train loss: 0.4962, train acc: 0.7816, val loss: 0.5137, val acc: 0.7474  (best train acc: 0.8064, best val acc: 0.7963, best train loss: 0.4449  @ epoch 9331 )\n",
      "[Epoch: 9640] train loss: 0.4822, train acc: 0.7909, val loss: 0.5302, val acc: 0.7319  (best train acc: 0.8064, best val acc: 0.7963, best train loss: 0.4449  @ epoch 9331 )\n",
      "[Epoch: 9660] train loss: 0.4754, train acc: 0.7934, val loss: 0.5049, val acc: 0.7639  (best train acc: 0.8064, best val acc: 0.7963, best train loss: 0.4449  @ epoch 9331 )\n",
      "[Epoch: 9680] train loss: 0.4737, train acc: 0.7919, val loss: 0.5091, val acc: 0.7642  (best train acc: 0.8064, best val acc: 0.7963, best train loss: 0.4449  @ epoch 9331 )\n",
      "[Epoch: 9700] train loss: 0.4785, train acc: 0.7858, val loss: 0.4962, val acc: 0.7727  (best train acc: 0.8064, best val acc: 0.7963, best train loss: 0.4449  @ epoch 9331 )\n",
      "[Epoch: 9720] train loss: 0.4876, train acc: 0.7896, val loss: 0.4976, val acc: 0.7750  (best train acc: 0.8064, best val acc: 0.7963, best train loss: 0.4449  @ epoch 9331 )\n",
      "[Epoch: 9740] train loss: 0.4921, train acc: 0.7791, val loss: 0.4853, val acc: 0.7767  (best train acc: 0.8064, best val acc: 0.7963, best train loss: 0.4449  @ epoch 9331 )\n",
      "[Epoch: 9760] train loss: 0.4800, train acc: 0.7809, val loss: 0.4935, val acc: 0.7659  (best train acc: 0.8064, best val acc: 0.7963, best train loss: 0.4449  @ epoch 9331 )\n",
      "[Epoch: 9780] train loss: 0.4707, train acc: 0.7968, val loss: 0.5106, val acc: 0.7565  (best train acc: 0.8064, best val acc: 0.7963, best train loss: 0.4449  @ epoch 9331 )\n",
      "[Epoch: 9800] train loss: 0.4765, train acc: 0.7916, val loss: 0.5023, val acc: 0.7575  (best train acc: 0.8064, best val acc: 0.7963, best train loss: 0.4449  @ epoch 9331 )\n",
      "[Epoch: 9820] train loss: 0.4895, train acc: 0.7878, val loss: 0.4900, val acc: 0.7825  (best train acc: 0.8064, best val acc: 0.7963, best train loss: 0.4449  @ epoch 9331 )\n",
      "[Epoch: 9840] train loss: 0.4721, train acc: 0.7869, val loss: 0.5087, val acc: 0.7582  (best train acc: 0.8064, best val acc: 0.7963, best train loss: 0.4449  @ epoch 9331 )\n",
      "[Epoch: 9860] train loss: 0.4639, train acc: 0.7893, val loss: 0.4903, val acc: 0.7801  (best train acc: 0.8064, best val acc: 0.7963, best train loss: 0.4449  @ epoch 9331 )\n",
      "[Epoch: 9880] train loss: 0.4619, train acc: 0.7949, val loss: 0.5003, val acc: 0.7612  (best train acc: 0.8064, best val acc: 0.7963, best train loss: 0.4449  @ epoch 9331 )\n",
      "[Epoch: 9900] train loss: 0.4969, train acc: 0.7815, val loss: 0.4858, val acc: 0.7767  (best train acc: 0.8064, best val acc: 0.7963, best train loss: 0.4449  @ epoch 9331 )\n",
      "[Epoch: 9920] train loss: 0.4765, train acc: 0.7958, val loss: 0.5119, val acc: 0.7622  (best train acc: 0.8064, best val acc: 0.7963, best train loss: 0.4449  @ epoch 9331 )\n",
      "[Epoch: 9940] train loss: 0.4600, train acc: 0.7963, val loss: 0.5016, val acc: 0.7622  (best train acc: 0.8064, best val acc: 0.7963, best train loss: 0.4449  @ epoch 9331 )\n",
      "[Epoch: 9960] train loss: 0.4723, train acc: 0.7934, val loss: 0.4839, val acc: 0.7777  (best train acc: 0.8064, best val acc: 0.7963, best train loss: 0.4449  @ epoch 9331 )\n",
      "[Epoch: 9980] train loss: 0.4632, train acc: 0.7968, val loss: 0.4886, val acc: 0.7707  (best train acc: 0.8064, best val acc: 0.7963, best train loss: 0.4449  @ epoch 9331 )\n",
      "[Epoch: 10000] train loss: 0.4605, train acc: 0.7991, val loss: 0.4864, val acc: 0.7788  (best train acc: 0.8064, best val acc: 0.7963, best train loss: 0.4388  @ epoch 9990 )\n",
      "[Epoch: 10020] train loss: 0.4551, train acc: 0.7964, val loss: 0.4905, val acc: 0.7723  (best train acc: 0.8064, best val acc: 0.7963, best train loss: 0.4388  @ epoch 9990 )\n",
      "[Epoch: 10040] train loss: 0.4651, train acc: 0.7948, val loss: 0.4759, val acc: 0.7929  (best train acc: 0.8064, best val acc: 0.7963, best train loss: 0.4388  @ epoch 9990 )\n",
      "[Epoch: 10060] train loss: 0.4885, train acc: 0.7814, val loss: 0.5019, val acc: 0.7562  (best train acc: 0.8064, best val acc: 0.7963, best train loss: 0.4388  @ epoch 9990 )\n",
      "[Epoch: 10080] train loss: 0.5067, train acc: 0.7735, val loss: 0.5237, val acc: 0.7349  (best train acc: 0.8064, best val acc: 0.7963, best train loss: 0.4388  @ epoch 9990 )\n",
      "[Epoch: 10100] train loss: 0.4890, train acc: 0.7828, val loss: 0.5069, val acc: 0.7666  (best train acc: 0.8064, best val acc: 0.7963, best train loss: 0.4388  @ epoch 9990 )\n",
      "[Epoch: 10120] train loss: 0.4659, train acc: 0.7935, val loss: 0.4931, val acc: 0.7841  (best train acc: 0.8064, best val acc: 0.7963, best train loss: 0.4388  @ epoch 9990 )\n",
      "[Epoch: 10140] train loss: 0.4882, train acc: 0.7896, val loss: 0.4919, val acc: 0.7707  (best train acc: 0.8064, best val acc: 0.7963, best train loss: 0.4388  @ epoch 9990 )\n",
      "[Epoch: 10160] train loss: 0.4687, train acc: 0.7890, val loss: 0.5143, val acc: 0.7447  (best train acc: 0.8064, best val acc: 0.7963, best train loss: 0.4388  @ epoch 9990 )\n",
      "[Epoch: 10180] train loss: 0.4983, train acc: 0.7770, val loss: 0.4887, val acc: 0.7730  (best train acc: 0.8064, best val acc: 0.7973, best train loss: 0.4388  @ epoch 9990 )\n",
      "[Epoch: 10200] train loss: 0.4979, train acc: 0.7820, val loss: 0.4874, val acc: 0.7777  (best train acc: 0.8064, best val acc: 0.7973, best train loss: 0.4388  @ epoch 9990 )\n",
      "[Epoch: 10220] train loss: 0.4672, train acc: 0.7925, val loss: 0.4863, val acc: 0.7872  (best train acc: 0.8064, best val acc: 0.7973, best train loss: 0.4388  @ epoch 9990 )\n",
      "[Epoch: 10240] train loss: 0.4859, train acc: 0.7826, val loss: 0.4893, val acc: 0.7727  (best train acc: 0.8064, best val acc: 0.7973, best train loss: 0.4388  @ epoch 9990 )\n",
      "[Epoch: 10260] train loss: 0.4676, train acc: 0.7930, val loss: 0.4961, val acc: 0.7683  (best train acc: 0.8064, best val acc: 0.7973, best train loss: 0.4388  @ epoch 9990 )\n",
      "[Epoch: 10280] train loss: 0.5242, train acc: 0.7713, val loss: 0.4990, val acc: 0.7737  (best train acc: 0.8064, best val acc: 0.7973, best train loss: 0.4388  @ epoch 9990 )\n",
      "[Epoch: 10300] train loss: 0.5081, train acc: 0.7731, val loss: 0.5150, val acc: 0.7531  (best train acc: 0.8064, best val acc: 0.7993, best train loss: 0.4388  @ epoch 9990 )\n",
      "[Epoch: 10320] train loss: 0.4546, train acc: 0.7968, val loss: 0.4851, val acc: 0.7774  (best train acc: 0.8064, best val acc: 0.7993, best train loss: 0.4388  @ epoch 9990 )\n",
      "[Epoch: 10340] train loss: 0.4631, train acc: 0.7992, val loss: 0.4965, val acc: 0.7629  (best train acc: 0.8064, best val acc: 0.7993, best train loss: 0.4388  @ epoch 9990 )\n",
      "[Epoch: 10360] train loss: 0.4554, train acc: 0.7994, val loss: 0.4890, val acc: 0.7757  (best train acc: 0.8064, best val acc: 0.7993, best train loss: 0.4388  @ epoch 9990 )\n",
      "[Epoch: 10380] train loss: 0.4976, train acc: 0.7794, val loss: 0.5172, val acc: 0.7467  (best train acc: 0.8064, best val acc: 0.7993, best train loss: 0.4388  @ epoch 9990 )\n",
      "[Epoch: 10400] train loss: 0.4605, train acc: 0.7911, val loss: 0.4953, val acc: 0.7656  (best train acc: 0.8064, best val acc: 0.7993, best train loss: 0.4388  @ epoch 9990 )\n",
      "[Epoch: 10420] train loss: 0.4571, train acc: 0.7984, val loss: 0.4890, val acc: 0.7784  (best train acc: 0.8064, best val acc: 0.7993, best train loss: 0.4388  @ epoch 9990 )\n",
      "[Epoch: 10440] train loss: 0.4650, train acc: 0.7935, val loss: 0.4859, val acc: 0.7798  (best train acc: 0.8064, best val acc: 0.7993, best train loss: 0.4388  @ epoch 9990 )\n",
      "[Epoch: 10460] train loss: 0.4682, train acc: 0.7948, val loss: 0.5062, val acc: 0.7656  (best train acc: 0.8064, best val acc: 0.7993, best train loss: 0.4388  @ epoch 9990 )\n",
      "[Epoch: 10480] train loss: 0.4715, train acc: 0.7904, val loss: 0.4930, val acc: 0.7730  (best train acc: 0.8064, best val acc: 0.7993, best train loss: 0.4388  @ epoch 9990 )\n",
      "[Epoch: 10500] train loss: 0.4774, train acc: 0.7861, val loss: 0.4818, val acc: 0.7838  (best train acc: 0.8064, best val acc: 0.7993, best train loss: 0.4388  @ epoch 9990 )\n",
      "[Epoch: 10520] train loss: 0.4626, train acc: 0.7890, val loss: 0.4739, val acc: 0.7980  (best train acc: 0.8064, best val acc: 0.7993, best train loss: 0.4388  @ epoch 9990 )\n",
      "[Epoch: 10540] train loss: 0.4649, train acc: 0.7976, val loss: 0.5066, val acc: 0.7626  (best train acc: 0.8064, best val acc: 0.7993, best train loss: 0.4388  @ epoch 9990 )\n",
      "[Epoch: 10560] train loss: 0.4718, train acc: 0.7917, val loss: 0.5127, val acc: 0.7528  (best train acc: 0.8064, best val acc: 0.7993, best train loss: 0.4388  @ epoch 9990 )\n",
      "[Epoch: 10580] train loss: 0.4487, train acc: 0.7992, val loss: 0.4872, val acc: 0.7771  (best train acc: 0.8064, best val acc: 0.7993, best train loss: 0.4388  @ epoch 9990 )\n",
      "[Epoch: 10600] train loss: 0.4597, train acc: 0.7962, val loss: 0.4825, val acc: 0.7838  (best train acc: 0.8064, best val acc: 0.7993, best train loss: 0.4388  @ epoch 9990 )\n",
      "[Epoch: 10620] train loss: 0.4648, train acc: 0.7980, val loss: 0.5011, val acc: 0.7646  (best train acc: 0.8064, best val acc: 0.7993, best train loss: 0.4388  @ epoch 9990 )\n",
      "[Epoch: 10640] train loss: 0.4531, train acc: 0.7986, val loss: 0.4917, val acc: 0.7609  (best train acc: 0.8064, best val acc: 0.7993, best train loss: 0.4388  @ epoch 9990 )\n",
      "[Epoch: 10660] train loss: 0.4770, train acc: 0.7859, val loss: 0.4834, val acc: 0.7838  (best train acc: 0.8064, best val acc: 0.7993, best train loss: 0.4388  @ epoch 9990 )\n",
      "[Epoch: 10680] train loss: 0.4924, train acc: 0.7807, val loss: 0.5176, val acc: 0.7548  (best train acc: 0.8064, best val acc: 0.7993, best train loss: 0.4388  @ epoch 9990 )\n",
      "[Epoch: 10700] train loss: 0.4897, train acc: 0.7856, val loss: 0.5121, val acc: 0.7659  (best train acc: 0.8064, best val acc: 0.7993, best train loss: 0.4388  @ epoch 9990 )\n",
      "[Epoch: 10720] train loss: 0.4577, train acc: 0.7968, val loss: 0.4891, val acc: 0.7727  (best train acc: 0.8064, best val acc: 0.7993, best train loss: 0.4388  @ epoch 9990 )\n",
      "[Epoch: 10740] train loss: 0.4783, train acc: 0.7876, val loss: 0.4905, val acc: 0.7757  (best train acc: 0.8064, best val acc: 0.7993, best train loss: 0.4388  @ epoch 9990 )\n",
      "[Epoch: 10760] train loss: 0.4689, train acc: 0.7921, val loss: 0.4929, val acc: 0.7710  (best train acc: 0.8064, best val acc: 0.7993, best train loss: 0.4388  @ epoch 9990 )\n",
      "[Epoch: 10780] train loss: 0.4667, train acc: 0.7918, val loss: 0.4887, val acc: 0.7825  (best train acc: 0.8064, best val acc: 0.7993, best train loss: 0.4388  @ epoch 9990 )\n",
      "[Epoch: 10800] train loss: 0.4732, train acc: 0.7926, val loss: 0.5112, val acc: 0.7447  (best train acc: 0.8064, best val acc: 0.7993, best train loss: 0.4388  @ epoch 9990 )\n",
      "[Epoch: 10820] train loss: 0.4537, train acc: 0.7996, val loss: 0.4903, val acc: 0.7825  (best train acc: 0.8064, best val acc: 0.8003, best train loss: 0.4388  @ epoch 9990 )\n",
      "[Epoch: 10840] train loss: 0.4798, train acc: 0.7853, val loss: 0.4765, val acc: 0.7966  (best train acc: 0.8064, best val acc: 0.8024, best train loss: 0.4388  @ epoch 9990 )\n",
      "[Epoch: 10860] train loss: 0.4982, train acc: 0.7857, val loss: 0.5146, val acc: 0.7393  (best train acc: 0.8064, best val acc: 0.8024, best train loss: 0.4388  @ epoch 9990 )\n",
      "[Epoch: 10880] train loss: 0.4710, train acc: 0.7908, val loss: 0.5298, val acc: 0.7305  (best train acc: 0.8064, best val acc: 0.8024, best train loss: 0.4388  @ epoch 9990 )\n",
      "[Epoch: 10900] train loss: 0.4662, train acc: 0.7902, val loss: 0.4832, val acc: 0.7862  (best train acc: 0.8064, best val acc: 0.8024, best train loss: 0.4388  @ epoch 9990 )\n",
      "[Epoch: 10920] train loss: 0.4559, train acc: 0.7976, val loss: 0.4806, val acc: 0.7838  (best train acc: 0.8064, best val acc: 0.8024, best train loss: 0.4388  @ epoch 9990 )\n",
      "[Epoch: 10940] train loss: 0.4692, train acc: 0.7927, val loss: 0.4756, val acc: 0.7868  (best train acc: 0.8066, best val acc: 0.8024, best train loss: 0.4388  @ epoch 9990 )\n",
      "[Epoch: 10960] train loss: 0.4626, train acc: 0.7990, val loss: 0.5013, val acc: 0.7663  (best train acc: 0.8066, best val acc: 0.8024, best train loss: 0.4388  @ epoch 9990 )\n",
      "[Epoch: 10980] train loss: 0.4623, train acc: 0.7981, val loss: 0.4885, val acc: 0.7727  (best train acc: 0.8066, best val acc: 0.8024, best train loss: 0.4388  @ epoch 9990 )\n",
      "[Epoch: 11000] train loss: 0.4655, train acc: 0.7911, val loss: 0.4998, val acc: 0.7727  (best train acc: 0.8066, best val acc: 0.8024, best train loss: 0.4388  @ epoch 9990 )\n",
      "[Epoch: 11020] train loss: 0.4884, train acc: 0.7906, val loss: 0.4952, val acc: 0.7649  (best train acc: 0.8066, best val acc: 0.8024, best train loss: 0.4388  @ epoch 9990 )\n",
      "[Epoch: 11040] train loss: 0.4684, train acc: 0.7877, val loss: 0.5084, val acc: 0.7551  (best train acc: 0.8066, best val acc: 0.8024, best train loss: 0.4388  @ epoch 9990 )\n",
      "[Epoch: 11060] train loss: 0.4580, train acc: 0.7971, val loss: 0.5007, val acc: 0.7622  (best train acc: 0.8066, best val acc: 0.8024, best train loss: 0.4388  @ epoch 9990 )\n",
      "[Epoch: 11080] train loss: 0.4784, train acc: 0.7891, val loss: 0.4895, val acc: 0.7835  (best train acc: 0.8080, best val acc: 0.8088, best train loss: 0.4388  @ epoch 9990 )\n",
      "[Epoch: 11100] train loss: 0.4581, train acc: 0.7984, val loss: 0.5104, val acc: 0.7497  (best train acc: 0.8080, best val acc: 0.8088, best train loss: 0.4388  @ epoch 9990 )\n",
      "[Epoch: 11120] train loss: 0.4788, train acc: 0.7828, val loss: 0.4750, val acc: 0.7855  (best train acc: 0.8080, best val acc: 0.8088, best train loss: 0.4388  @ epoch 9990 )\n",
      "[Epoch: 11140] train loss: 0.4753, train acc: 0.7920, val loss: 0.4995, val acc: 0.7612  (best train acc: 0.8080, best val acc: 0.8088, best train loss: 0.4388  @ epoch 9990 )\n",
      "[Epoch: 11160] train loss: 0.4826, train acc: 0.7895, val loss: 0.5095, val acc: 0.7535  (best train acc: 0.8083, best val acc: 0.8088, best train loss: 0.4388  @ epoch 9990 )\n",
      "[Epoch: 11180] train loss: 0.4555, train acc: 0.7969, val loss: 0.4899, val acc: 0.7686  (best train acc: 0.8083, best val acc: 0.8088, best train loss: 0.4388  @ epoch 9990 )\n",
      "[Epoch: 11200] train loss: 0.4559, train acc: 0.7972, val loss: 0.4815, val acc: 0.7774  (best train acc: 0.8083, best val acc: 0.8088, best train loss: 0.4388  @ epoch 9990 )\n",
      "[Epoch: 11220] train loss: 0.4744, train acc: 0.7971, val loss: 0.4889, val acc: 0.7676  (best train acc: 0.8083, best val acc: 0.8088, best train loss: 0.4388  @ epoch 9990 )\n",
      "[Epoch: 11240] train loss: 0.4526, train acc: 0.8005, val loss: 0.4810, val acc: 0.7791  (best train acc: 0.8083, best val acc: 0.8088, best train loss: 0.4388  @ epoch 9990 )\n",
      "[Epoch: 11260] train loss: 0.5076, train acc: 0.7767, val loss: 0.5024, val acc: 0.7642  (best train acc: 0.8083, best val acc: 0.8088, best train loss: 0.4388  @ epoch 9990 )\n",
      "[Epoch: 11280] train loss: 0.4757, train acc: 0.7828, val loss: 0.5014, val acc: 0.7514  (best train acc: 0.8083, best val acc: 0.8088, best train loss: 0.4388  @ epoch 9990 )\n",
      "[Epoch: 11300] train loss: 0.4612, train acc: 0.7977, val loss: 0.4941, val acc: 0.7683  (best train acc: 0.8083, best val acc: 0.8088, best train loss: 0.4388  @ epoch 9990 )\n",
      "[Epoch: 11320] train loss: 0.4548, train acc: 0.7981, val loss: 0.4780, val acc: 0.7828  (best train acc: 0.8083, best val acc: 0.8088, best train loss: 0.4388  @ epoch 9990 )\n",
      "[Epoch: 11340] train loss: 0.4397, train acc: 0.8080, val loss: 0.4886, val acc: 0.7680  (best train acc: 0.8083, best val acc: 0.8088, best train loss: 0.4388  @ epoch 9990 )\n",
      "[Epoch: 11360] train loss: 0.4538, train acc: 0.7975, val loss: 0.4984, val acc: 0.7602  (best train acc: 0.8083, best val acc: 0.8088, best train loss: 0.4388  @ epoch 9990 )\n",
      "[Epoch: 11380] train loss: 0.4734, train acc: 0.7927, val loss: 0.4917, val acc: 0.7761  (best train acc: 0.8083, best val acc: 0.8088, best train loss: 0.4388  @ epoch 9990 )\n",
      "[Epoch: 11400] train loss: 0.4637, train acc: 0.7971, val loss: 0.4975, val acc: 0.7578  (best train acc: 0.8083, best val acc: 0.8088, best train loss: 0.4388  @ epoch 9990 )\n",
      "[Epoch: 11420] train loss: 0.4438, train acc: 0.8036, val loss: 0.4923, val acc: 0.7764  (best train acc: 0.8083, best val acc: 0.8088, best train loss: 0.4388  @ epoch 9990 )\n",
      "[Epoch: 11440] train loss: 0.4568, train acc: 0.7956, val loss: 0.4765, val acc: 0.7804  (best train acc: 0.8083, best val acc: 0.8088, best train loss: 0.4388  @ epoch 9990 )\n",
      "[Epoch: 11460] train loss: 0.4737, train acc: 0.7890, val loss: 0.4876, val acc: 0.7788  (best train acc: 0.8083, best val acc: 0.8088, best train loss: 0.4388  @ epoch 9990 )\n",
      "[Epoch: 11480] train loss: 0.4622, train acc: 0.7935, val loss: 0.4890, val acc: 0.7669  (best train acc: 0.8083, best val acc: 0.8088, best train loss: 0.4388  @ epoch 9990 )\n",
      "[Epoch: 11500] train loss: 0.4584, train acc: 0.7943, val loss: 0.4854, val acc: 0.7696  (best train acc: 0.8083, best val acc: 0.8088, best train loss: 0.4388  @ epoch 9990 )\n",
      "[Epoch: 11520] train loss: 0.4493, train acc: 0.8005, val loss: 0.4965, val acc: 0.7626  (best train acc: 0.8083, best val acc: 0.8088, best train loss: 0.4388  @ epoch 9990 )\n",
      "[Epoch: 11540] train loss: 0.4536, train acc: 0.8013, val loss: 0.4927, val acc: 0.7720  (best train acc: 0.8083, best val acc: 0.8088, best train loss: 0.4388  @ epoch 9990 )\n",
      "[Epoch: 11560] train loss: 0.4523, train acc: 0.8001, val loss: 0.4946, val acc: 0.7622  (best train acc: 0.8083, best val acc: 0.8088, best train loss: 0.4388  @ epoch 9990 )\n",
      "[Epoch: 11580] train loss: 0.4772, train acc: 0.7893, val loss: 0.4958, val acc: 0.7693  (best train acc: 0.8083, best val acc: 0.8088, best train loss: 0.4387  @ epoch 11573 )\n",
      "[Epoch: 11600] train loss: 0.4533, train acc: 0.7945, val loss: 0.4713, val acc: 0.7939  (best train acc: 0.8083, best val acc: 0.8088, best train loss: 0.4387  @ epoch 11573 )\n",
      "[Epoch: 11620] train loss: 0.4476, train acc: 0.8031, val loss: 0.4835, val acc: 0.7720  (best train acc: 0.8083, best val acc: 0.8088, best train loss: 0.4387  @ epoch 11573 )\n",
      "[Epoch: 11640] train loss: 0.4520, train acc: 0.8025, val loss: 0.4889, val acc: 0.7710  (best train acc: 0.8099, best val acc: 0.8088, best train loss: 0.4387  @ epoch 11573 )\n",
      "[Epoch: 11660] train loss: 0.4633, train acc: 0.7950, val loss: 0.4867, val acc: 0.7723  (best train acc: 0.8099, best val acc: 0.8088, best train loss: 0.4387  @ epoch 11573 )\n",
      "[Epoch: 11680] train loss: 0.4664, train acc: 0.7916, val loss: 0.4949, val acc: 0.7592  (best train acc: 0.8099, best val acc: 0.8088, best train loss: 0.4387  @ epoch 11573 )\n",
      "[Epoch: 11700] train loss: 0.4548, train acc: 0.7973, val loss: 0.4837, val acc: 0.7750  (best train acc: 0.8099, best val acc: 0.8088, best train loss: 0.4387  @ epoch 11573 )\n",
      "[Epoch: 11720] train loss: 0.4576, train acc: 0.7976, val loss: 0.5071, val acc: 0.7508  (best train acc: 0.8099, best val acc: 0.8088, best train loss: 0.4387  @ epoch 11573 )\n",
      "[Epoch: 11740] train loss: 0.4496, train acc: 0.8013, val loss: 0.4925, val acc: 0.7626  (best train acc: 0.8099, best val acc: 0.8088, best train loss: 0.4387  @ epoch 11573 )\n",
      "[Epoch: 11760] train loss: 0.4877, train acc: 0.7783, val loss: 0.4869, val acc: 0.7673  (best train acc: 0.8099, best val acc: 0.8088, best train loss: 0.4387  @ epoch 11573 )\n",
      "[Epoch: 11780] train loss: 0.4727, train acc: 0.7899, val loss: 0.4733, val acc: 0.7835  (best train acc: 0.8099, best val acc: 0.8088, best train loss: 0.4387  @ epoch 11573 )\n",
      "[Epoch: 11800] train loss: 0.4505, train acc: 0.8041, val loss: 0.4940, val acc: 0.7750  (best train acc: 0.8099, best val acc: 0.8088, best train loss: 0.4387  @ epoch 11573 )\n",
      "[Epoch: 11820] train loss: 0.4647, train acc: 0.7943, val loss: 0.5066, val acc: 0.7548  (best train acc: 0.8099, best val acc: 0.8088, best train loss: 0.4378  @ epoch 11808 )\n",
      "[Epoch: 11840] train loss: 0.4532, train acc: 0.8004, val loss: 0.5091, val acc: 0.7582  (best train acc: 0.8099, best val acc: 0.8088, best train loss: 0.4378  @ epoch 11808 )\n",
      "[Epoch: 11860] train loss: 0.4951, train acc: 0.7772, val loss: 0.4937, val acc: 0.7669  (best train acc: 0.8099, best val acc: 0.8088, best train loss: 0.4378  @ epoch 11808 )\n",
      "[Epoch: 11880] train loss: 0.4577, train acc: 0.7950, val loss: 0.4934, val acc: 0.7737  (best train acc: 0.8099, best val acc: 0.8088, best train loss: 0.4378  @ epoch 11808 )\n",
      "[Epoch: 11900] train loss: 0.4455, train acc: 0.8035, val loss: 0.4947, val acc: 0.7690  (best train acc: 0.8099, best val acc: 0.8088, best train loss: 0.4378  @ epoch 11808 )\n",
      "[Epoch: 11920] train loss: 0.4515, train acc: 0.7990, val loss: 0.5017, val acc: 0.7481  (best train acc: 0.8099, best val acc: 0.8088, best train loss: 0.4378  @ epoch 11808 )\n",
      "[Epoch: 11940] train loss: 0.4656, train acc: 0.7923, val loss: 0.5245, val acc: 0.7521  (best train acc: 0.8099, best val acc: 0.8088, best train loss: 0.4378  @ epoch 11808 )\n",
      "[Epoch: 11960] train loss: 0.4647, train acc: 0.7939, val loss: 0.4961, val acc: 0.7609  (best train acc: 0.8099, best val acc: 0.8088, best train loss: 0.4378  @ epoch 11808 )\n",
      "[Epoch: 11980] train loss: 0.4586, train acc: 0.7939, val loss: 0.4702, val acc: 0.8034  (best train acc: 0.8099, best val acc: 0.8088, best train loss: 0.4378  @ epoch 11808 )\n",
      "[Epoch: 12000] train loss: 0.4571, train acc: 0.7971, val loss: 0.4797, val acc: 0.7696  (best train acc: 0.8099, best val acc: 0.8088, best train loss: 0.4378  @ epoch 11808 )\n",
      "[Epoch: 12020] train loss: 0.4730, train acc: 0.7916, val loss: 0.4993, val acc: 0.7616  (best train acc: 0.8099, best val acc: 0.8088, best train loss: 0.4378  @ epoch 11808 )\n",
      "[Epoch: 12040] train loss: 0.4948, train acc: 0.7790, val loss: 0.4881, val acc: 0.7717  (best train acc: 0.8099, best val acc: 0.8088, best train loss: 0.4373  @ epoch 12039 )\n",
      "[Epoch: 12060] train loss: 0.4709, train acc: 0.7922, val loss: 0.4817, val acc: 0.7723  (best train acc: 0.8099, best val acc: 0.8088, best train loss: 0.4373  @ epoch 12039 )\n",
      "[Epoch: 12080] train loss: 0.4598, train acc: 0.8016, val loss: 0.4784, val acc: 0.7811  (best train acc: 0.8099, best val acc: 0.8088, best train loss: 0.4373  @ epoch 12039 )\n",
      "[Epoch: 12100] train loss: 0.4613, train acc: 0.7976, val loss: 0.4808, val acc: 0.7794  (best train acc: 0.8099, best val acc: 0.8088, best train loss: 0.4373  @ epoch 12039 )\n",
      "[Epoch: 12120] train loss: 0.4881, train acc: 0.7911, val loss: 0.4967, val acc: 0.7653  (best train acc: 0.8099, best val acc: 0.8088, best train loss: 0.4373  @ epoch 12039 )\n",
      "[Epoch: 12140] train loss: 0.4585, train acc: 0.7968, val loss: 0.4902, val acc: 0.7700  (best train acc: 0.8099, best val acc: 0.8088, best train loss: 0.4373  @ epoch 12039 )\n",
      "[Epoch: 12160] train loss: 0.4472, train acc: 0.8010, val loss: 0.4878, val acc: 0.7764  (best train acc: 0.8132, best val acc: 0.8088, best train loss: 0.4358  @ epoch 12141 )\n",
      "[Epoch: 12180] train loss: 0.4852, train acc: 0.7852, val loss: 0.4745, val acc: 0.7794  (best train acc: 0.8132, best val acc: 0.8088, best train loss: 0.4358  @ epoch 12141 )\n",
      "[Epoch: 12200] train loss: 0.4558, train acc: 0.7966, val loss: 0.4841, val acc: 0.7686  (best train acc: 0.8132, best val acc: 0.8088, best train loss: 0.4353  @ epoch 12189 )\n",
      "[Epoch: 12220] train loss: 0.4793, train acc: 0.7934, val loss: 0.5007, val acc: 0.7595  (best train acc: 0.8132, best val acc: 0.8088, best train loss: 0.4353  @ epoch 12189 )\n",
      "[Epoch: 12240] train loss: 0.4558, train acc: 0.7964, val loss: 0.4947, val acc: 0.7646  (best train acc: 0.8132, best val acc: 0.8088, best train loss: 0.4353  @ epoch 12189 )\n",
      "[Epoch: 12260] train loss: 0.4556, train acc: 0.7960, val loss: 0.4731, val acc: 0.7815  (best train acc: 0.8132, best val acc: 0.8088, best train loss: 0.4344  @ epoch 12252 )\n",
      "[Epoch: 12280] train loss: 0.4488, train acc: 0.8002, val loss: 0.4779, val acc: 0.7737  (best train acc: 0.8132, best val acc: 0.8088, best train loss: 0.4344  @ epoch 12252 )\n",
      "[Epoch: 12300] train loss: 0.4384, train acc: 0.8044, val loss: 0.4817, val acc: 0.7764  (best train acc: 0.8132, best val acc: 0.8088, best train loss: 0.4344  @ epoch 12252 )\n",
      "[Epoch: 12320] train loss: 0.4608, train acc: 0.7950, val loss: 0.5089, val acc: 0.7575  (best train acc: 0.8132, best val acc: 0.8088, best train loss: 0.4340  @ epoch 12309 )\n",
      "[Epoch: 12340] train loss: 0.4964, train acc: 0.7756, val loss: 0.4866, val acc: 0.7723  (best train acc: 0.8132, best val acc: 0.8088, best train loss: 0.4340  @ epoch 12309 )\n",
      "[Epoch: 12360] train loss: 0.4497, train acc: 0.8026, val loss: 0.4747, val acc: 0.7804  (best train acc: 0.8132, best val acc: 0.8088, best train loss: 0.4337  @ epoch 12359 )\n",
      "[Epoch: 12380] train loss: 0.4631, train acc: 0.7943, val loss: 0.4833, val acc: 0.7794  (best train acc: 0.8132, best val acc: 0.8088, best train loss: 0.4337  @ epoch 12359 )\n",
      "[Epoch: 12400] train loss: 0.4695, train acc: 0.7917, val loss: 0.5071, val acc: 0.7605  (best train acc: 0.8132, best val acc: 0.8088, best train loss: 0.4337  @ epoch 12359 )\n",
      "[Epoch: 12420] train loss: 0.4549, train acc: 0.8006, val loss: 0.4758, val acc: 0.7811  (best train acc: 0.8132, best val acc: 0.8088, best train loss: 0.4337  @ epoch 12359 )\n",
      "[Epoch: 12440] train loss: 0.4673, train acc: 0.7966, val loss: 0.4877, val acc: 0.7690  (best train acc: 0.8132, best val acc: 0.8088, best train loss: 0.4337  @ epoch 12359 )\n",
      "[Epoch: 12460] train loss: 0.4717, train acc: 0.7974, val loss: 0.5011, val acc: 0.7602  (best train acc: 0.8132, best val acc: 0.8088, best train loss: 0.4337  @ epoch 12359 )\n",
      "[Epoch: 12480] train loss: 0.4497, train acc: 0.8078, val loss: 0.4896, val acc: 0.7683  (best train acc: 0.8132, best val acc: 0.8088, best train loss: 0.4337  @ epoch 12359 )\n",
      "[Epoch: 12500] train loss: 0.4562, train acc: 0.7890, val loss: 0.4705, val acc: 0.7879  (best train acc: 0.8132, best val acc: 0.8088, best train loss: 0.4337  @ epoch 12359 )\n",
      "[Epoch: 12520] train loss: 0.4547, train acc: 0.7979, val loss: 0.4686, val acc: 0.7875  (best train acc: 0.8132, best val acc: 0.8088, best train loss: 0.4337  @ epoch 12359 )\n",
      "[Epoch: 12540] train loss: 0.4538, train acc: 0.7948, val loss: 0.4788, val acc: 0.7821  (best train acc: 0.8132, best val acc: 0.8088, best train loss: 0.4337  @ epoch 12359 )\n",
      "[Epoch: 12560] train loss: 0.4814, train acc: 0.7837, val loss: 0.4717, val acc: 0.7882  (best train acc: 0.8132, best val acc: 0.8088, best train loss: 0.4337  @ epoch 12359 )\n",
      "[Epoch: 12580] train loss: 0.4553, train acc: 0.7984, val loss: 0.4980, val acc: 0.7693  (best train acc: 0.8132, best val acc: 0.8088, best train loss: 0.4288  @ epoch 12565 )\n",
      "[Epoch: 12600] train loss: 0.4664, train acc: 0.7956, val loss: 0.4768, val acc: 0.7835  (best train acc: 0.8132, best val acc: 0.8088, best train loss: 0.4288  @ epoch 12565 )\n",
      "[Epoch: 12620] train loss: 0.4555, train acc: 0.7978, val loss: 0.5073, val acc: 0.7551  (best train acc: 0.8132, best val acc: 0.8135, best train loss: 0.4288  @ epoch 12565 )\n",
      "[Epoch: 12640] train loss: 0.4645, train acc: 0.7868, val loss: 0.4863, val acc: 0.7740  (best train acc: 0.8132, best val acc: 0.8135, best train loss: 0.4288  @ epoch 12565 )\n",
      "[Epoch: 12660] train loss: 0.4408, train acc: 0.8109, val loss: 0.4893, val acc: 0.7642  (best train acc: 0.8132, best val acc: 0.8135, best train loss: 0.4288  @ epoch 12565 )\n",
      "[Epoch: 12680] train loss: 0.4489, train acc: 0.7997, val loss: 0.4876, val acc: 0.7764  (best train acc: 0.8132, best val acc: 0.8135, best train loss: 0.4288  @ epoch 12565 )\n",
      "[Epoch: 12700] train loss: 0.4452, train acc: 0.8006, val loss: 0.4795, val acc: 0.7771  (best train acc: 0.8132, best val acc: 0.8135, best train loss: 0.4288  @ epoch 12565 )\n",
      "[Epoch: 12720] train loss: 0.4657, train acc: 0.7952, val loss: 0.4958, val acc: 0.7663  (best train acc: 0.8149, best val acc: 0.8135, best train loss: 0.4237  @ epoch 12707 )\n",
      "[Epoch: 12740] train loss: 0.4608, train acc: 0.7929, val loss: 0.4884, val acc: 0.7666  (best train acc: 0.8149, best val acc: 0.8135, best train loss: 0.4237  @ epoch 12707 )\n",
      "[Epoch: 12760] train loss: 0.4474, train acc: 0.8010, val loss: 0.4759, val acc: 0.7828  (best train acc: 0.8149, best val acc: 0.8135, best train loss: 0.4237  @ epoch 12707 )\n",
      "[Epoch: 12780] train loss: 0.4458, train acc: 0.7989, val loss: 0.4729, val acc: 0.7831  (best train acc: 0.8149, best val acc: 0.8135, best train loss: 0.4237  @ epoch 12707 )\n",
      "[Epoch: 12800] train loss: 0.4564, train acc: 0.7997, val loss: 0.4808, val acc: 0.7713  (best train acc: 0.8149, best val acc: 0.8135, best train loss: 0.4237  @ epoch 12707 )\n",
      "[Epoch: 12820] train loss: 0.4637, train acc: 0.7996, val loss: 0.4968, val acc: 0.7673  (best train acc: 0.8149, best val acc: 0.8135, best train loss: 0.4237  @ epoch 12707 )\n",
      "[Epoch: 12840] train loss: 0.4467, train acc: 0.8051, val loss: 0.4859, val acc: 0.7673  (best train acc: 0.8149, best val acc: 0.8135, best train loss: 0.4237  @ epoch 12707 )\n",
      "[Epoch: 12860] train loss: 0.4552, train acc: 0.7973, val loss: 0.4761, val acc: 0.7757  (best train acc: 0.8149, best val acc: 0.8135, best train loss: 0.4237  @ epoch 12707 )\n",
      "[Epoch: 12880] train loss: 0.4519, train acc: 0.7984, val loss: 0.4812, val acc: 0.7831  (best train acc: 0.8149, best val acc: 0.8135, best train loss: 0.4237  @ epoch 12707 )\n",
      "[Epoch: 12900] train loss: 0.4687, train acc: 0.7911, val loss: 0.4842, val acc: 0.7740  (best train acc: 0.8149, best val acc: 0.8135, best train loss: 0.4237  @ epoch 12707 )\n",
      "[Epoch: 12920] train loss: 0.4339, train acc: 0.8062, val loss: 0.4671, val acc: 0.7906  (best train acc: 0.8149, best val acc: 0.8135, best train loss: 0.4237  @ epoch 12707 )\n",
      "[Epoch: 12940] train loss: 0.4436, train acc: 0.8015, val loss: 0.4884, val acc: 0.7750  (best train acc: 0.8149, best val acc: 0.8135, best train loss: 0.4237  @ epoch 12707 )\n",
      "[Epoch: 12960] train loss: 0.4549, train acc: 0.8012, val loss: 0.4858, val acc: 0.7707  (best train acc: 0.8149, best val acc: 0.8135, best train loss: 0.4237  @ epoch 12707 )\n",
      "[Epoch: 12980] train loss: 0.4356, train acc: 0.8060, val loss: 0.4782, val acc: 0.7727  (best train acc: 0.8149, best val acc: 0.8135, best train loss: 0.4237  @ epoch 12707 )\n",
      "[Epoch: 13000] train loss: 0.4600, train acc: 0.7941, val loss: 0.4890, val acc: 0.7696  (best train acc: 0.8149, best val acc: 0.8135, best train loss: 0.4237  @ epoch 12707 )\n",
      "[Epoch: 13020] train loss: 0.4404, train acc: 0.8075, val loss: 0.4705, val acc: 0.7835  (best train acc: 0.8149, best val acc: 0.8135, best train loss: 0.4237  @ epoch 12707 )\n",
      "[Epoch: 13040] train loss: 0.4508, train acc: 0.7964, val loss: 0.4726, val acc: 0.7858  (best train acc: 0.8149, best val acc: 0.8135, best train loss: 0.4237  @ epoch 12707 )\n",
      "[Epoch: 13060] train loss: 0.4654, train acc: 0.7947, val loss: 0.4737, val acc: 0.7794  (best train acc: 0.8149, best val acc: 0.8135, best train loss: 0.4237  @ epoch 12707 )\n",
      "[Epoch: 13080] train loss: 0.4593, train acc: 0.8010, val loss: 0.4831, val acc: 0.7761  (best train acc: 0.8149, best val acc: 0.8135, best train loss: 0.4237  @ epoch 12707 )\n",
      "[Epoch: 13100] train loss: 0.4488, train acc: 0.8010, val loss: 0.4796, val acc: 0.7744  (best train acc: 0.8149, best val acc: 0.8135, best train loss: 0.4237  @ epoch 12707 )\n",
      "[Epoch: 13120] train loss: 0.4399, train acc: 0.8060, val loss: 0.4613, val acc: 0.8047  (best train acc: 0.8149, best val acc: 0.8135, best train loss: 0.4237  @ epoch 12707 )\n",
      "[Epoch: 13140] train loss: 0.4403, train acc: 0.8061, val loss: 0.4764, val acc: 0.7781  (best train acc: 0.8149, best val acc: 0.8135, best train loss: 0.4237  @ epoch 12707 )\n",
      "[Epoch: 13160] train loss: 0.4458, train acc: 0.8023, val loss: 0.4785, val acc: 0.7794  (best train acc: 0.8149, best val acc: 0.8135, best train loss: 0.4237  @ epoch 12707 )\n",
      "[Epoch: 13180] train loss: 0.4533, train acc: 0.7995, val loss: 0.4635, val acc: 0.7919  (best train acc: 0.8164, best val acc: 0.8135, best train loss: 0.4237  @ epoch 12707 )\n",
      "[Epoch: 13200] train loss: 0.4497, train acc: 0.8039, val loss: 0.4780, val acc: 0.7841  (best train acc: 0.8164, best val acc: 0.8135, best train loss: 0.4237  @ epoch 12707 )\n",
      "[Epoch: 13220] train loss: 0.4414, train acc: 0.7982, val loss: 0.4708, val acc: 0.7862  (best train acc: 0.8164, best val acc: 0.8135, best train loss: 0.4237  @ epoch 12707 )\n",
      "[Epoch: 13240] train loss: 0.4556, train acc: 0.8006, val loss: 0.4665, val acc: 0.7862  (best train acc: 0.8164, best val acc: 0.8135, best train loss: 0.4237  @ epoch 12707 )\n",
      "[Epoch: 13260] train loss: 0.4433, train acc: 0.8043, val loss: 0.4736, val acc: 0.7838  (best train acc: 0.8164, best val acc: 0.8135, best train loss: 0.4237  @ epoch 12707 )\n",
      "[Epoch: 13280] train loss: 0.4386, train acc: 0.8042, val loss: 0.4943, val acc: 0.7632  (best train acc: 0.8164, best val acc: 0.8135, best train loss: 0.4237  @ epoch 12707 )\n",
      "[Epoch: 13300] train loss: 0.4687, train acc: 0.7870, val loss: 0.4734, val acc: 0.7835  (best train acc: 0.8164, best val acc: 0.8135, best train loss: 0.4237  @ epoch 12707 )\n",
      "[Epoch: 13320] train loss: 0.4590, train acc: 0.7937, val loss: 0.4679, val acc: 0.7899  (best train acc: 0.8164, best val acc: 0.8135, best train loss: 0.4237  @ epoch 12707 )\n",
      "[Epoch: 13340] train loss: 0.4595, train acc: 0.8015, val loss: 0.4564, val acc: 0.8037  (best train acc: 0.8164, best val acc: 0.8135, best train loss: 0.4237  @ epoch 12707 )\n",
      "[Epoch: 13360] train loss: 0.4333, train acc: 0.8126, val loss: 0.4861, val acc: 0.7703  (best train acc: 0.8164, best val acc: 0.8135, best train loss: 0.4237  @ epoch 12707 )\n",
      "[Epoch: 13380] train loss: 0.4359, train acc: 0.8039, val loss: 0.4650, val acc: 0.7976  (best train acc: 0.8164, best val acc: 0.8135, best train loss: 0.4237  @ epoch 12707 )\n",
      "[Epoch: 13400] train loss: 0.4249, train acc: 0.8082, val loss: 0.4697, val acc: 0.7882  (best train acc: 0.8164, best val acc: 0.8135, best train loss: 0.4237  @ epoch 12707 )\n",
      "[Epoch: 13420] train loss: 0.4555, train acc: 0.7961, val loss: 0.4814, val acc: 0.7767  (best train acc: 0.8164, best val acc: 0.8135, best train loss: 0.4237  @ epoch 12707 )\n",
      "[Epoch: 13440] train loss: 0.4613, train acc: 0.7975, val loss: 0.4841, val acc: 0.7757  (best train acc: 0.8164, best val acc: 0.8135, best train loss: 0.4237  @ epoch 12707 )\n",
      "[Epoch: 13460] train loss: 0.4468, train acc: 0.8021, val loss: 0.4753, val acc: 0.7808  (best train acc: 0.8164, best val acc: 0.8135, best train loss: 0.4237  @ epoch 12707 )\n",
      "[Epoch: 13480] train loss: 0.4304, train acc: 0.8120, val loss: 0.4541, val acc: 0.8040  (best train acc: 0.8164, best val acc: 0.8135, best train loss: 0.4237  @ epoch 12707 )\n",
      "[Epoch: 13500] train loss: 0.4362, train acc: 0.8061, val loss: 0.4642, val acc: 0.7919  (best train acc: 0.8164, best val acc: 0.8135, best train loss: 0.4237  @ epoch 12707 )\n",
      "[Epoch: 13520] train loss: 0.4411, train acc: 0.8048, val loss: 0.4739, val acc: 0.7791  (best train acc: 0.8164, best val acc: 0.8135, best train loss: 0.4237  @ epoch 12707 )\n",
      "[Epoch: 13540] train loss: 0.4437, train acc: 0.8062, val loss: 0.4683, val acc: 0.7909  (best train acc: 0.8164, best val acc: 0.8135, best train loss: 0.4237  @ epoch 12707 )\n",
      "[Epoch: 13560] train loss: 0.4533, train acc: 0.8023, val loss: 0.4786, val acc: 0.7750  (best train acc: 0.8164, best val acc: 0.8135, best train loss: 0.4237  @ epoch 12707 )\n",
      "[Epoch: 13580] train loss: 0.4731, train acc: 0.8013, val loss: 0.5039, val acc: 0.7477  (best train acc: 0.8164, best val acc: 0.8135, best train loss: 0.4237  @ epoch 12707 )\n",
      "[Epoch: 13600] train loss: 0.4293, train acc: 0.8090, val loss: 0.4611, val acc: 0.7987  (best train acc: 0.8164, best val acc: 0.8135, best train loss: 0.4237  @ epoch 12707 )\n",
      "[Epoch: 13620] train loss: 0.4417, train acc: 0.8044, val loss: 0.4774, val acc: 0.7723  (best train acc: 0.8164, best val acc: 0.8135, best train loss: 0.4235  @ epoch 13617 )\n",
      "[Epoch: 13640] train loss: 0.4914, train acc: 0.7813, val loss: 0.5027, val acc: 0.7612  (best train acc: 0.8164, best val acc: 0.8135, best train loss: 0.4235  @ epoch 13617 )\n",
      "[Epoch: 13660] train loss: 0.4485, train acc: 0.8031, val loss: 0.4799, val acc: 0.7781  (best train acc: 0.8164, best val acc: 0.8135, best train loss: 0.4235  @ epoch 13617 )\n",
      "[Epoch: 13680] train loss: 0.4244, train acc: 0.8116, val loss: 0.4598, val acc: 0.7936  (best train acc: 0.8164, best val acc: 0.8135, best train loss: 0.4235  @ epoch 13617 )\n",
      "[Epoch: 13700] train loss: 0.4366, train acc: 0.8055, val loss: 0.4892, val acc: 0.7669  (best train acc: 0.8164, best val acc: 0.8135, best train loss: 0.4235  @ epoch 13617 )\n",
      "[Epoch: 13720] train loss: 0.4393, train acc: 0.8042, val loss: 0.4716, val acc: 0.7868  (best train acc: 0.8164, best val acc: 0.8135, best train loss: 0.4235  @ epoch 13617 )\n",
      "[Epoch: 13740] train loss: 0.4371, train acc: 0.8095, val loss: 0.4668, val acc: 0.7892  (best train acc: 0.8164, best val acc: 0.8135, best train loss: 0.4235  @ epoch 13617 )\n",
      "[Epoch: 13760] train loss: 0.4303, train acc: 0.8088, val loss: 0.4887, val acc: 0.7737  (best train acc: 0.8164, best val acc: 0.8135, best train loss: 0.4235  @ epoch 13617 )\n",
      "[Epoch: 13780] train loss: 0.4556, train acc: 0.8008, val loss: 0.4875, val acc: 0.7703  (best train acc: 0.8164, best val acc: 0.8135, best train loss: 0.4209  @ epoch 13772 )\n",
      "[Epoch: 13800] train loss: 0.4518, train acc: 0.7992, val loss: 0.4774, val acc: 0.7818  (best train acc: 0.8164, best val acc: 0.8135, best train loss: 0.4209  @ epoch 13772 )\n",
      "[Epoch: 13820] train loss: 0.4356, train acc: 0.8061, val loss: 0.4758, val acc: 0.7811  (best train acc: 0.8164, best val acc: 0.8135, best train loss: 0.4209  @ epoch 13772 )\n",
      "[Epoch: 13840] train loss: 0.4379, train acc: 0.8091, val loss: 0.4862, val acc: 0.7686  (best train acc: 0.8164, best val acc: 0.8135, best train loss: 0.4209  @ epoch 13772 )\n",
      "[Epoch: 13860] train loss: 0.4418, train acc: 0.7998, val loss: 0.5044, val acc: 0.7646  (best train acc: 0.8164, best val acc: 0.8135, best train loss: 0.4209  @ epoch 13772 )\n",
      "[Epoch: 13880] train loss: 0.4630, train acc: 0.7980, val loss: 0.4895, val acc: 0.7696  (best train acc: 0.8164, best val acc: 0.8135, best train loss: 0.4209  @ epoch 13772 )\n",
      "[Epoch: 13900] train loss: 0.4403, train acc: 0.8073, val loss: 0.5014, val acc: 0.7558  (best train acc: 0.8164, best val acc: 0.8135, best train loss: 0.4209  @ epoch 13772 )\n",
      "[Epoch: 13920] train loss: 0.4694, train acc: 0.7927, val loss: 0.4681, val acc: 0.7858  (best train acc: 0.8164, best val acc: 0.8135, best train loss: 0.4209  @ epoch 13772 )\n",
      "[Epoch: 13940] train loss: 0.4418, train acc: 0.8012, val loss: 0.4647, val acc: 0.7953  (best train acc: 0.8164, best val acc: 0.8135, best train loss: 0.4209  @ epoch 13772 )\n",
      "[Epoch: 13960] train loss: 0.4483, train acc: 0.7997, val loss: 0.4810, val acc: 0.7723  (best train acc: 0.8164, best val acc: 0.8135, best train loss: 0.4209  @ epoch 13772 )\n",
      "[Epoch: 13980] train loss: 0.4449, train acc: 0.8034, val loss: 0.4766, val acc: 0.7815  (best train acc: 0.8164, best val acc: 0.8135, best train loss: 0.4209  @ epoch 13772 )\n",
      "[Epoch: 14000] train loss: 0.4431, train acc: 0.8023, val loss: 0.4881, val acc: 0.7676  (best train acc: 0.8164, best val acc: 0.8135, best train loss: 0.4209  @ epoch 13772 )\n",
      "[Epoch: 14020] train loss: 0.4363, train acc: 0.8088, val loss: 0.4801, val acc: 0.7818  (best train acc: 0.8164, best val acc: 0.8135, best train loss: 0.4209  @ epoch 13772 )\n",
      "[Epoch: 14040] train loss: 0.4375, train acc: 0.8094, val loss: 0.4856, val acc: 0.7686  (best train acc: 0.8164, best val acc: 0.8135, best train loss: 0.4209  @ epoch 13772 )\n",
      "[Epoch: 14060] train loss: 0.4290, train acc: 0.8104, val loss: 0.4649, val acc: 0.7916  (best train acc: 0.8164, best val acc: 0.8135, best train loss: 0.4209  @ epoch 13772 )\n",
      "[Epoch: 14080] train loss: 0.4394, train acc: 0.8117, val loss: 0.4689, val acc: 0.7838  (best train acc: 0.8171, best val acc: 0.8135, best train loss: 0.4205  @ epoch 14062 )\n",
      "[Epoch: 14100] train loss: 0.4311, train acc: 0.8109, val loss: 0.4641, val acc: 0.7889  (best train acc: 0.8171, best val acc: 0.8135, best train loss: 0.4205  @ epoch 14062 )\n",
      "[Epoch: 14120] train loss: 0.4411, train acc: 0.8029, val loss: 0.4865, val acc: 0.7781  (best train acc: 0.8171, best val acc: 0.8135, best train loss: 0.4205  @ epoch 14062 )\n",
      "[Epoch: 14140] train loss: 0.4459, train acc: 0.8041, val loss: 0.4833, val acc: 0.7707  (best train acc: 0.8171, best val acc: 0.8135, best train loss: 0.4205  @ epoch 14062 )\n",
      "[Epoch: 14160] train loss: 0.4475, train acc: 0.7983, val loss: 0.4697, val acc: 0.7933  (best train acc: 0.8171, best val acc: 0.8135, best train loss: 0.4205  @ epoch 14062 )\n",
      "[Epoch: 14180] train loss: 0.4433, train acc: 0.8061, val loss: 0.4666, val acc: 0.7852  (best train acc: 0.8171, best val acc: 0.8135, best train loss: 0.4205  @ epoch 14062 )\n",
      "[Epoch: 14200] train loss: 0.4455, train acc: 0.8017, val loss: 0.4674, val acc: 0.7865  (best train acc: 0.8171, best val acc: 0.8135, best train loss: 0.4205  @ epoch 14062 )\n",
      "[Epoch: 14220] train loss: 0.4434, train acc: 0.8067, val loss: 0.4858, val acc: 0.7727  (best train acc: 0.8171, best val acc: 0.8135, best train loss: 0.4205  @ epoch 14062 )\n",
      "[Epoch: 14240] train loss: 0.4440, train acc: 0.8028, val loss: 0.4585, val acc: 0.7973  (best train acc: 0.8171, best val acc: 0.8135, best train loss: 0.4205  @ epoch 14062 )\n",
      "[Epoch: 14260] train loss: 0.4503, train acc: 0.7981, val loss: 0.4609, val acc: 0.8020  (best train acc: 0.8171, best val acc: 0.8135, best train loss: 0.4205  @ epoch 14062 )\n",
      "[Epoch: 14280] train loss: 0.4470, train acc: 0.8003, val loss: 0.4633, val acc: 0.7865  (best train acc: 0.8171, best val acc: 0.8135, best train loss: 0.4205  @ epoch 14062 )\n",
      "[Epoch: 14300] train loss: 0.4471, train acc: 0.8042, val loss: 0.4897, val acc: 0.7693  (best train acc: 0.8171, best val acc: 0.8135, best train loss: 0.4205  @ epoch 14062 )\n",
      "[Epoch: 14320] train loss: 0.4350, train acc: 0.8061, val loss: 0.4740, val acc: 0.7879  (best train acc: 0.8171, best val acc: 0.8135, best train loss: 0.4205  @ epoch 14062 )\n",
      "[Epoch: 14340] train loss: 0.4416, train acc: 0.8090, val loss: 0.4683, val acc: 0.7899  (best train acc: 0.8171, best val acc: 0.8135, best train loss: 0.4205  @ epoch 14062 )\n",
      "[Epoch: 14360] train loss: 0.4297, train acc: 0.8091, val loss: 0.4589, val acc: 0.7963  (best train acc: 0.8171, best val acc: 0.8135, best train loss: 0.4205  @ epoch 14062 )\n",
      "[Epoch: 14380] train loss: 0.4486, train acc: 0.8067, val loss: 0.4740, val acc: 0.7774  (best train acc: 0.8171, best val acc: 0.8135, best train loss: 0.4205  @ epoch 14062 )\n",
      "[Epoch: 14400] train loss: 0.4563, train acc: 0.8022, val loss: 0.5018, val acc: 0.7626  (best train acc: 0.8171, best val acc: 0.8135, best train loss: 0.4205  @ epoch 14062 )\n",
      "[Epoch: 14420] train loss: 0.4569, train acc: 0.7923, val loss: 0.4729, val acc: 0.7845  (best train acc: 0.8171, best val acc: 0.8135, best train loss: 0.4205  @ epoch 14062 )\n",
      "[Epoch: 14440] train loss: 0.4384, train acc: 0.8083, val loss: 0.4780, val acc: 0.7757  (best train acc: 0.8171, best val acc: 0.8135, best train loss: 0.4205  @ epoch 14062 )\n",
      "[Epoch: 14460] train loss: 0.4402, train acc: 0.8049, val loss: 0.4665, val acc: 0.7841  (best train acc: 0.8171, best val acc: 0.8135, best train loss: 0.4205  @ epoch 14062 )\n",
      "[Epoch: 14480] train loss: 0.4478, train acc: 0.8028, val loss: 0.4735, val acc: 0.7794  (best train acc: 0.8171, best val acc: 0.8135, best train loss: 0.4205  @ epoch 14062 )\n",
      "[Epoch: 14500] train loss: 0.4652, train acc: 0.8028, val loss: 0.4820, val acc: 0.7717  (best train acc: 0.8171, best val acc: 0.8135, best train loss: 0.4205  @ epoch 14062 )\n",
      "[Epoch: 14520] train loss: 0.4362, train acc: 0.8037, val loss: 0.4693, val acc: 0.7865  (best train acc: 0.8171, best val acc: 0.8135, best train loss: 0.4205  @ epoch 14062 )\n",
      "[Epoch: 14540] train loss: 0.4590, train acc: 0.7957, val loss: 0.4784, val acc: 0.7720  (best train acc: 0.8171, best val acc: 0.8159, best train loss: 0.4205  @ epoch 14062 )\n",
      "[Epoch: 14560] train loss: 0.4549, train acc: 0.7946, val loss: 0.4847, val acc: 0.7750  (best train acc: 0.8171, best val acc: 0.8159, best train loss: 0.4205  @ epoch 14062 )\n",
      "[Epoch: 14580] train loss: 0.4766, train acc: 0.7817, val loss: 0.4691, val acc: 0.7906  (best train acc: 0.8171, best val acc: 0.8172, best train loss: 0.4205  @ epoch 14062 )\n",
      "[Epoch: 14600] train loss: 0.4363, train acc: 0.8081, val loss: 0.4722, val acc: 0.7808  (best train acc: 0.8171, best val acc: 0.8172, best train loss: 0.4205  @ epoch 14062 )\n",
      "[Epoch: 14620] train loss: 0.4317, train acc: 0.8103, val loss: 0.4707, val acc: 0.7858  (best train acc: 0.8171, best val acc: 0.8172, best train loss: 0.4183  @ epoch 14604 )\n",
      "[Epoch: 14640] train loss: 0.4407, train acc: 0.8000, val loss: 0.4604, val acc: 0.7939  (best train acc: 0.8171, best val acc: 0.8172, best train loss: 0.4183  @ epoch 14604 )\n",
      "[Epoch: 14660] train loss: 0.4335, train acc: 0.8101, val loss: 0.4575, val acc: 0.7956  (best train acc: 0.8171, best val acc: 0.8172, best train loss: 0.4183  @ epoch 14648 )\n",
      "[Epoch: 14680] train loss: 0.4512, train acc: 0.8028, val loss: 0.4758, val acc: 0.7815  (best train acc: 0.8171, best val acc: 0.8172, best train loss: 0.4183  @ epoch 14648 )\n",
      "[Epoch: 14700] train loss: 0.4640, train acc: 0.7934, val loss: 0.4787, val acc: 0.7788  (best train acc: 0.8171, best val acc: 0.8172, best train loss: 0.4183  @ epoch 14648 )\n",
      "[Epoch: 14720] train loss: 0.4589, train acc: 0.7949, val loss: 0.4658, val acc: 0.7872  (best train acc: 0.8171, best val acc: 0.8172, best train loss: 0.4183  @ epoch 14648 )\n",
      "[Epoch: 14740] train loss: 0.4442, train acc: 0.8060, val loss: 0.4616, val acc: 0.7868  (best train acc: 0.8171, best val acc: 0.8172, best train loss: 0.4183  @ epoch 14648 )\n",
      "[Epoch: 14760] train loss: 0.4383, train acc: 0.8047, val loss: 0.4615, val acc: 0.7912  (best train acc: 0.8171, best val acc: 0.8172, best train loss: 0.4183  @ epoch 14648 )\n",
      "[Epoch: 14780] train loss: 0.4574, train acc: 0.7982, val loss: 0.4868, val acc: 0.7707  (best train acc: 0.8171, best val acc: 0.8172, best train loss: 0.4183  @ epoch 14648 )\n",
      "[Epoch: 14800] train loss: 0.4703, train acc: 0.7893, val loss: 0.4857, val acc: 0.7696  (best train acc: 0.8171, best val acc: 0.8172, best train loss: 0.4183  @ epoch 14648 )\n",
      "[Epoch: 14820] train loss: 0.4407, train acc: 0.7989, val loss: 0.4561, val acc: 0.7960  (best train acc: 0.8171, best val acc: 0.8172, best train loss: 0.4183  @ epoch 14648 )\n",
      "[Epoch: 14840] train loss: 0.4417, train acc: 0.8010, val loss: 0.4579, val acc: 0.7943  (best train acc: 0.8171, best val acc: 0.8172, best train loss: 0.4183  @ epoch 14648 )\n",
      "[Epoch: 14860] train loss: 0.4416, train acc: 0.8070, val loss: 0.4706, val acc: 0.7848  (best train acc: 0.8171, best val acc: 0.8172, best train loss: 0.4183  @ epoch 14648 )\n",
      "[Epoch: 14880] train loss: 0.4211, train acc: 0.8154, val loss: 0.4621, val acc: 0.7912  (best train acc: 0.8171, best val acc: 0.8172, best train loss: 0.4183  @ epoch 14648 )\n",
      "[Epoch: 14900] train loss: 0.4395, train acc: 0.8090, val loss: 0.4628, val acc: 0.7916  (best train acc: 0.8171, best val acc: 0.8172, best train loss: 0.4183  @ epoch 14648 )\n",
      "[Epoch: 14920] train loss: 0.4634, train acc: 0.7877, val loss: 0.4804, val acc: 0.7828  (best train acc: 0.8171, best val acc: 0.8172, best train loss: 0.4183  @ epoch 14648 )\n",
      "[Epoch: 14940] train loss: 0.4410, train acc: 0.8027, val loss: 0.4918, val acc: 0.7720  (best train acc: 0.8171, best val acc: 0.8172, best train loss: 0.4183  @ epoch 14648 )\n",
      "[Epoch: 14960] train loss: 0.4504, train acc: 0.8004, val loss: 0.4693, val acc: 0.7848  (best train acc: 0.8171, best val acc: 0.8172, best train loss: 0.4183  @ epoch 14648 )\n",
      "[Epoch: 14980] train loss: 0.4454, train acc: 0.8063, val loss: 0.4690, val acc: 0.7892  (best train acc: 0.8171, best val acc: 0.8172, best train loss: 0.4183  @ epoch 14648 )\n",
      "[Epoch: 15000] train loss: 0.4489, train acc: 0.8018, val loss: 0.4819, val acc: 0.7673  (best train acc: 0.8171, best val acc: 0.8172, best train loss: 0.4183  @ epoch 14648 )\n",
      "[Epoch: 15020] train loss: 0.4351, train acc: 0.8115, val loss: 0.4761, val acc: 0.7777  (best train acc: 0.8171, best val acc: 0.8172, best train loss: 0.4183  @ epoch 14648 )\n",
      "[Epoch: 15040] train loss: 0.4471, train acc: 0.8055, val loss: 0.4787, val acc: 0.7794  (best train acc: 0.8171, best val acc: 0.8172, best train loss: 0.4183  @ epoch 14648 )\n",
      "[Epoch: 15060] train loss: 0.4278, train acc: 0.8124, val loss: 0.4788, val acc: 0.7747  (best train acc: 0.8171, best val acc: 0.8172, best train loss: 0.4183  @ epoch 14648 )\n",
      "[Epoch: 15080] train loss: 0.4460, train acc: 0.8034, val loss: 0.4695, val acc: 0.7801  (best train acc: 0.8171, best val acc: 0.8172, best train loss: 0.4183  @ epoch 14648 )\n",
      "[Epoch: 15100] train loss: 0.4326, train acc: 0.8088, val loss: 0.4721, val acc: 0.7838  (best train acc: 0.8171, best val acc: 0.8172, best train loss: 0.4183  @ epoch 14648 )\n",
      "[Epoch: 15120] train loss: 0.4616, train acc: 0.7974, val loss: 0.4733, val acc: 0.7845  (best train acc: 0.8171, best val acc: 0.8172, best train loss: 0.4183  @ epoch 14648 )\n",
      "[Epoch: 15140] train loss: 0.4385, train acc: 0.8051, val loss: 0.4560, val acc: 0.7983  (best train acc: 0.8171, best val acc: 0.8172, best train loss: 0.4183  @ epoch 14648 )\n",
      "[Epoch: 15160] train loss: 0.4269, train acc: 0.8158, val loss: 0.4592, val acc: 0.8044  (best train acc: 0.8171, best val acc: 0.8172, best train loss: 0.4183  @ epoch 14648 )\n",
      "[Epoch: 15180] train loss: 0.4259, train acc: 0.8127, val loss: 0.4576, val acc: 0.7963  (best train acc: 0.8171, best val acc: 0.8172, best train loss: 0.4183  @ epoch 14648 )\n",
      "[Epoch: 15200] train loss: 0.4522, train acc: 0.7940, val loss: 0.4730, val acc: 0.7811  (best train acc: 0.8171, best val acc: 0.8172, best train loss: 0.4174  @ epoch 15187 )\n",
      "[Epoch: 15220] train loss: 0.4450, train acc: 0.8039, val loss: 0.4701, val acc: 0.7841  (best train acc: 0.8171, best val acc: 0.8172, best train loss: 0.4174  @ epoch 15187 )\n",
      "[Epoch: 15240] train loss: 0.4263, train acc: 0.8152, val loss: 0.4568, val acc: 0.7946  (best train acc: 0.8171, best val acc: 0.8172, best train loss: 0.4174  @ epoch 15187 )\n",
      "[Epoch: 15260] train loss: 0.4236, train acc: 0.8116, val loss: 0.4490, val acc: 0.8105  (best train acc: 0.8171, best val acc: 0.8172, best train loss: 0.4174  @ epoch 15187 )\n",
      "[Epoch: 15280] train loss: 0.4555, train acc: 0.7934, val loss: 0.4703, val acc: 0.7845  (best train acc: 0.8171, best val acc: 0.8172, best train loss: 0.4174  @ epoch 15187 )\n",
      "[Epoch: 15300] train loss: 0.4485, train acc: 0.8006, val loss: 0.4670, val acc: 0.7879  (best train acc: 0.8171, best val acc: 0.8172, best train loss: 0.4174  @ epoch 15187 )\n",
      "[Epoch: 15320] train loss: 0.4349, train acc: 0.8075, val loss: 0.4636, val acc: 0.7875  (best train acc: 0.8171, best val acc: 0.8172, best train loss: 0.4174  @ epoch 15187 )\n",
      "[Epoch: 15340] train loss: 0.4243, train acc: 0.8156, val loss: 0.4703, val acc: 0.7926  (best train acc: 0.8171, best val acc: 0.8172, best train loss: 0.4174  @ epoch 15187 )\n",
      "[Epoch: 15360] train loss: 0.4408, train acc: 0.8057, val loss: 0.4991, val acc: 0.7605  (best train acc: 0.8171, best val acc: 0.8172, best train loss: 0.4174  @ epoch 15187 )\n",
      "[Epoch: 15380] train loss: 0.4493, train acc: 0.8056, val loss: 0.5012, val acc: 0.7555  (best train acc: 0.8171, best val acc: 0.8172, best train loss: 0.4174  @ epoch 15187 )\n",
      "[Epoch: 15400] train loss: 0.4372, train acc: 0.8054, val loss: 0.4789, val acc: 0.7740  (best train acc: 0.8171, best val acc: 0.8172, best train loss: 0.4174  @ epoch 15187 )\n",
      "[Epoch: 15420] train loss: 0.4321, train acc: 0.8065, val loss: 0.4622, val acc: 0.7909  (best train acc: 0.8171, best val acc: 0.8172, best train loss: 0.4174  @ epoch 15187 )\n",
      "[Epoch: 15440] train loss: 0.4258, train acc: 0.8123, val loss: 0.4617, val acc: 0.7980  (best train acc: 0.8171, best val acc: 0.8172, best train loss: 0.4174  @ epoch 15187 )\n",
      "[Epoch: 15460] train loss: 0.4462, train acc: 0.8041, val loss: 0.4726, val acc: 0.7798  (best train acc: 0.8171, best val acc: 0.8172, best train loss: 0.4143  @ epoch 15447 )\n",
      "[Epoch: 15480] train loss: 0.4285, train acc: 0.8090, val loss: 0.4656, val acc: 0.7872  (best train acc: 0.8171, best val acc: 0.8172, best train loss: 0.4143  @ epoch 15447 )\n",
      "[Epoch: 15500] train loss: 0.4722, train acc: 0.7929, val loss: 0.4719, val acc: 0.7865  (best train acc: 0.8171, best val acc: 0.8172, best train loss: 0.4143  @ epoch 15447 )\n",
      "[Epoch: 15520] train loss: 0.4652, train acc: 0.7958, val loss: 0.4615, val acc: 0.7946  (best train acc: 0.8171, best val acc: 0.8172, best train loss: 0.4143  @ epoch 15447 )\n",
      "[Epoch: 15540] train loss: 0.4346, train acc: 0.8076, val loss: 0.4685, val acc: 0.7828  (best train acc: 0.8171, best val acc: 0.8172, best train loss: 0.4143  @ epoch 15447 )\n",
      "[Epoch: 15560] train loss: 0.4228, train acc: 0.8138, val loss: 0.4887, val acc: 0.7683  (best train acc: 0.8171, best val acc: 0.8172, best train loss: 0.4143  @ epoch 15447 )\n",
      "[Epoch: 15580] train loss: 0.4284, train acc: 0.8093, val loss: 0.4710, val acc: 0.7838  (best train acc: 0.8171, best val acc: 0.8172, best train loss: 0.4143  @ epoch 15447 )\n",
      "[Epoch: 15600] train loss: 0.4279, train acc: 0.8114, val loss: 0.4698, val acc: 0.7862  (best train acc: 0.8175, best val acc: 0.8172, best train loss: 0.4143  @ epoch 15447 )\n",
      "[Epoch: 15620] train loss: 0.4332, train acc: 0.8082, val loss: 0.4706, val acc: 0.7798  (best train acc: 0.8175, best val acc: 0.8172, best train loss: 0.4143  @ epoch 15447 )\n",
      "[Epoch: 15640] train loss: 0.4459, train acc: 0.8009, val loss: 0.4712, val acc: 0.7821  (best train acc: 0.8175, best val acc: 0.8172, best train loss: 0.4143  @ epoch 15447 )\n",
      "[Epoch: 15660] train loss: 0.4463, train acc: 0.8024, val loss: 0.4573, val acc: 0.7976  (best train acc: 0.8175, best val acc: 0.8172, best train loss: 0.4143  @ epoch 15447 )\n",
      "[Epoch: 15680] train loss: 0.4733, train acc: 0.7884, val loss: 0.4703, val acc: 0.7889  (best train acc: 0.8175, best val acc: 0.8172, best train loss: 0.4143  @ epoch 15447 )\n",
      "[Epoch: 15700] train loss: 0.4461, train acc: 0.8015, val loss: 0.4637, val acc: 0.7933  (best train acc: 0.8175, best val acc: 0.8172, best train loss: 0.4143  @ epoch 15447 )\n",
      "[Epoch: 15720] train loss: 0.4345, train acc: 0.8086, val loss: 0.4717, val acc: 0.7791  (best train acc: 0.8175, best val acc: 0.8172, best train loss: 0.4143  @ epoch 15447 )\n",
      "[Epoch: 15740] train loss: 0.4482, train acc: 0.8053, val loss: 0.4676, val acc: 0.7922  (best train acc: 0.8175, best val acc: 0.8172, best train loss: 0.4143  @ epoch 15447 )\n",
      "[Epoch: 15760] train loss: 0.4322, train acc: 0.8062, val loss: 0.4664, val acc: 0.7916  (best train acc: 0.8175, best val acc: 0.8172, best train loss: 0.4143  @ epoch 15447 )\n",
      "[Epoch: 15780] train loss: 0.4346, train acc: 0.8068, val loss: 0.4823, val acc: 0.7734  (best train acc: 0.8194, best val acc: 0.8172, best train loss: 0.4143  @ epoch 15447 )\n",
      "[Epoch: 15800] train loss: 0.4229, train acc: 0.8105, val loss: 0.4738, val acc: 0.7858  (best train acc: 0.8194, best val acc: 0.8172, best train loss: 0.4132  @ epoch 15791 )\n",
      "[Epoch: 15820] train loss: 0.4462, train acc: 0.7966, val loss: 0.4604, val acc: 0.7939  (best train acc: 0.8194, best val acc: 0.8172, best train loss: 0.4132  @ epoch 15791 )\n",
      "[Epoch: 15840] train loss: 0.4333, train acc: 0.8057, val loss: 0.4607, val acc: 0.7970  (best train acc: 0.8194, best val acc: 0.8172, best train loss: 0.4132  @ epoch 15791 )\n",
      "[Epoch: 15860] train loss: 0.4273, train acc: 0.8088, val loss: 0.4537, val acc: 0.8034  (best train acc: 0.8194, best val acc: 0.8172, best train loss: 0.4132  @ epoch 15791 )\n",
      "[Epoch: 15880] train loss: 0.4450, train acc: 0.8031, val loss: 0.4728, val acc: 0.7788  (best train acc: 0.8194, best val acc: 0.8172, best train loss: 0.4132  @ epoch 15791 )\n",
      "[Epoch: 15900] train loss: 0.4306, train acc: 0.8094, val loss: 0.5145, val acc: 0.7497  (best train acc: 0.8194, best val acc: 0.8172, best train loss: 0.4132  @ epoch 15791 )\n",
      "[Epoch: 15920] train loss: 0.4735, train acc: 0.7803, val loss: 0.4640, val acc: 0.7939  (best train acc: 0.8194, best val acc: 0.8179, best train loss: 0.4132  @ epoch 15791 )\n",
      "[Epoch: 15940] train loss: 0.4515, train acc: 0.7958, val loss: 0.4646, val acc: 0.7875  (best train acc: 0.8194, best val acc: 0.8179, best train loss: 0.4132  @ epoch 15791 )\n",
      "[Epoch: 15960] train loss: 0.4291, train acc: 0.8068, val loss: 0.4661, val acc: 0.7899  (best train acc: 0.8196, best val acc: 0.8179, best train loss: 0.4132  @ epoch 15791 )\n",
      "[Epoch: 15980] train loss: 0.4428, train acc: 0.8065, val loss: 0.4681, val acc: 0.7852  (best train acc: 0.8196, best val acc: 0.8179, best train loss: 0.4132  @ epoch 15791 )\n",
      "[Epoch: 16000] train loss: 0.4399, train acc: 0.8065, val loss: 0.4718, val acc: 0.7862  (best train acc: 0.8196, best val acc: 0.8179, best train loss: 0.4132  @ epoch 15791 )\n",
      "[Epoch: 16020] train loss: 0.4588, train acc: 0.7906, val loss: 0.4722, val acc: 0.7855  (best train acc: 0.8196, best val acc: 0.8179, best train loss: 0.4132  @ epoch 15791 )\n",
      "[Epoch: 16040] train loss: 0.4300, train acc: 0.8098, val loss: 0.4498, val acc: 0.8101  (best train acc: 0.8196, best val acc: 0.8179, best train loss: 0.4132  @ epoch 15791 )\n",
      "[Epoch: 16060] train loss: 0.4442, train acc: 0.8088, val loss: 0.4584, val acc: 0.7933  (best train acc: 0.8196, best val acc: 0.8189, best train loss: 0.4132  @ epoch 15791 )\n",
      "[Epoch: 16080] train loss: 0.4472, train acc: 0.8017, val loss: 0.4653, val acc: 0.7841  (best train acc: 0.8196, best val acc: 0.8189, best train loss: 0.4132  @ epoch 15791 )\n",
      "[Epoch: 16100] train loss: 0.4301, train acc: 0.8083, val loss: 0.4683, val acc: 0.7855  (best train acc: 0.8196, best val acc: 0.8189, best train loss: 0.4132  @ epoch 15791 )\n",
      "[Epoch: 16120] train loss: 0.4402, train acc: 0.8086, val loss: 0.4612, val acc: 0.7902  (best train acc: 0.8196, best val acc: 0.8189, best train loss: 0.4132  @ epoch 15791 )\n",
      "[Epoch: 16140] train loss: 0.4344, train acc: 0.8042, val loss: 0.4524, val acc: 0.7983  (best train acc: 0.8196, best val acc: 0.8189, best train loss: 0.4132  @ epoch 15791 )\n",
      "[Epoch: 16160] train loss: 0.4533, train acc: 0.8041, val loss: 0.4589, val acc: 0.7895  (best train acc: 0.8196, best val acc: 0.8189, best train loss: 0.4132  @ epoch 15791 )\n",
      "[Epoch: 16180] train loss: 0.4192, train acc: 0.8145, val loss: 0.4633, val acc: 0.7906  (best train acc: 0.8196, best val acc: 0.8189, best train loss: 0.4132  @ epoch 15791 )\n",
      "[Epoch: 16200] train loss: 0.4408, train acc: 0.8104, val loss: 0.4761, val acc: 0.7801  (best train acc: 0.8196, best val acc: 0.8189, best train loss: 0.4120  @ epoch 16184 )\n",
      "[Epoch: 16220] train loss: 0.4550, train acc: 0.8015, val loss: 0.4519, val acc: 0.8007  (best train acc: 0.8196, best val acc: 0.8189, best train loss: 0.4120  @ epoch 16184 )\n",
      "[Epoch: 16240] train loss: 0.4275, train acc: 0.8108, val loss: 0.4600, val acc: 0.7936  (best train acc: 0.8196, best val acc: 0.8189, best train loss: 0.4120  @ epoch 16184 )\n",
      "[Epoch: 16260] train loss: 0.4227, train acc: 0.8133, val loss: 0.4732, val acc: 0.7848  (best train acc: 0.8196, best val acc: 0.8189, best train loss: 0.4120  @ epoch 16184 )\n",
      "[Epoch: 16280] train loss: 0.4365, train acc: 0.8075, val loss: 0.4582, val acc: 0.7946  (best train acc: 0.8196, best val acc: 0.8189, best train loss: 0.4120  @ epoch 16184 )\n",
      "[Epoch: 16300] train loss: 0.4149, train acc: 0.8145, val loss: 0.4858, val acc: 0.7713  (best train acc: 0.8196, best val acc: 0.8189, best train loss: 0.4120  @ epoch 16184 )\n",
      "[Epoch: 16320] train loss: 0.4508, train acc: 0.8006, val loss: 0.4603, val acc: 0.7882  (best train acc: 0.8196, best val acc: 0.8189, best train loss: 0.4120  @ epoch 16184 )\n",
      "[Epoch: 16340] train loss: 0.4371, train acc: 0.8096, val loss: 0.4578, val acc: 0.7990  (best train acc: 0.8196, best val acc: 0.8189, best train loss: 0.4120  @ epoch 16184 )\n",
      "[Epoch: 16360] train loss: 0.4249, train acc: 0.8128, val loss: 0.4604, val acc: 0.7970  (best train acc: 0.8235, best val acc: 0.8189, best train loss: 0.4120  @ epoch 16184 )\n",
      "[Epoch: 16380] train loss: 0.4175, train acc: 0.8172, val loss: 0.4552, val acc: 0.8037  (best train acc: 0.8235, best val acc: 0.8189, best train loss: 0.4120  @ epoch 16184 )\n",
      "[Epoch: 16400] train loss: 0.4469, train acc: 0.8051, val loss: 0.4573, val acc: 0.7966  (best train acc: 0.8235, best val acc: 0.8189, best train loss: 0.4105  @ epoch 16381 )\n",
      "[Epoch: 16420] train loss: 0.4525, train acc: 0.8000, val loss: 0.4773, val acc: 0.7767  (best train acc: 0.8235, best val acc: 0.8189, best train loss: 0.4098  @ epoch 16419 )\n",
      "[Epoch: 16440] train loss: 0.4539, train acc: 0.7987, val loss: 0.4564, val acc: 0.8091  (best train acc: 0.8235, best val acc: 0.8189, best train loss: 0.4098  @ epoch 16419 )\n",
      "[Epoch: 16460] train loss: 0.4433, train acc: 0.8073, val loss: 0.4532, val acc: 0.8003  (best train acc: 0.8235, best val acc: 0.8189, best train loss: 0.4098  @ epoch 16419 )\n",
      "[Epoch: 16480] train loss: 0.4504, train acc: 0.8021, val loss: 0.4638, val acc: 0.7872  (best train acc: 0.8235, best val acc: 0.8189, best train loss: 0.4098  @ epoch 16419 )\n",
      "[Epoch: 16500] train loss: 0.4289, train acc: 0.8078, val loss: 0.4707, val acc: 0.7879  (best train acc: 0.8235, best val acc: 0.8189, best train loss: 0.4098  @ epoch 16419 )\n",
      "[Epoch: 16520] train loss: 0.4493, train acc: 0.8035, val loss: 0.4720, val acc: 0.7798  (best train acc: 0.8235, best val acc: 0.8189, best train loss: 0.4098  @ epoch 16419 )\n",
      "[Epoch: 16540] train loss: 0.4545, train acc: 0.7945, val loss: 0.4684, val acc: 0.7855  (best train acc: 0.8235, best val acc: 0.8189, best train loss: 0.4098  @ epoch 16419 )\n",
      "[Epoch: 16560] train loss: 0.4367, train acc: 0.8059, val loss: 0.4669, val acc: 0.7916  (best train acc: 0.8235, best val acc: 0.8189, best train loss: 0.4098  @ epoch 16419 )\n",
      "[Epoch: 16580] train loss: 0.4518, train acc: 0.7979, val loss: 0.4468, val acc: 0.8101  (best train acc: 0.8235, best val acc: 0.8189, best train loss: 0.4098  @ epoch 16419 )\n",
      "[Epoch: 16600] train loss: 0.4310, train acc: 0.8060, val loss: 0.4475, val acc: 0.7990  (best train acc: 0.8235, best val acc: 0.8189, best train loss: 0.4098  @ epoch 16419 )\n",
      "[Epoch: 16620] train loss: 0.4440, train acc: 0.8039, val loss: 0.4661, val acc: 0.7815  (best train acc: 0.8235, best val acc: 0.8189, best train loss: 0.4098  @ epoch 16419 )\n",
      "[Epoch: 16640] train loss: 0.4327, train acc: 0.8081, val loss: 0.4558, val acc: 0.7960  (best train acc: 0.8235, best val acc: 0.8189, best train loss: 0.4098  @ epoch 16419 )\n",
      "[Epoch: 16660] train loss: 0.4462, train acc: 0.8027, val loss: 0.4624, val acc: 0.7912  (best train acc: 0.8235, best val acc: 0.8189, best train loss: 0.4098  @ epoch 16419 )\n",
      "[Epoch: 16680] train loss: 0.4222, train acc: 0.8137, val loss: 0.4643, val acc: 0.7943  (best train acc: 0.8235, best val acc: 0.8189, best train loss: 0.4098  @ epoch 16419 )\n",
      "[Epoch: 16700] train loss: 0.4245, train acc: 0.8144, val loss: 0.4692, val acc: 0.7960  (best train acc: 0.8235, best val acc: 0.8189, best train loss: 0.4098  @ epoch 16419 )\n",
      "[Epoch: 16720] train loss: 0.4448, train acc: 0.8011, val loss: 0.4597, val acc: 0.7939  (best train acc: 0.8235, best val acc: 0.8189, best train loss: 0.4098  @ epoch 16419 )\n",
      "[Epoch: 16740] train loss: 0.4159, train acc: 0.8142, val loss: 0.4566, val acc: 0.7926  (best train acc: 0.8235, best val acc: 0.8189, best train loss: 0.4098  @ epoch 16419 )\n",
      "[Epoch: 16760] train loss: 0.4281, train acc: 0.8081, val loss: 0.4538, val acc: 0.7912  (best train acc: 0.8235, best val acc: 0.8189, best train loss: 0.4098  @ epoch 16419 )\n",
      "[Epoch: 16780] train loss: 0.4666, train acc: 0.7934, val loss: 0.4675, val acc: 0.7838  (best train acc: 0.8235, best val acc: 0.8189, best train loss: 0.4098  @ epoch 16419 )\n",
      "[Epoch: 16800] train loss: 0.4321, train acc: 0.8068, val loss: 0.4708, val acc: 0.7862  (best train acc: 0.8235, best val acc: 0.8189, best train loss: 0.4098  @ epoch 16419 )\n",
      "[Epoch: 16820] train loss: 0.4285, train acc: 0.8086, val loss: 0.4553, val acc: 0.7976  (best train acc: 0.8235, best val acc: 0.8189, best train loss: 0.4098  @ epoch 16419 )\n",
      "[Epoch: 16840] train loss: 0.4461, train acc: 0.8010, val loss: 0.4813, val acc: 0.7707  (best train acc: 0.8235, best val acc: 0.8189, best train loss: 0.4098  @ epoch 16419 )\n",
      "[Epoch: 16860] train loss: 0.4360, train acc: 0.8051, val loss: 0.4433, val acc: 0.8088  (best train acc: 0.8235, best val acc: 0.8189, best train loss: 0.4098  @ epoch 16419 )\n",
      "[Epoch: 16880] train loss: 0.4400, train acc: 0.8028, val loss: 0.4607, val acc: 0.7919  (best train acc: 0.8235, best val acc: 0.8189, best train loss: 0.4098  @ epoch 16419 )\n",
      "[Epoch: 16900] train loss: 0.4416, train acc: 0.8031, val loss: 0.4794, val acc: 0.7744  (best train acc: 0.8235, best val acc: 0.8189, best train loss: 0.4098  @ epoch 16419 )\n",
      "[Epoch: 16920] train loss: 0.4455, train acc: 0.8037, val loss: 0.4782, val acc: 0.7710  (best train acc: 0.8235, best val acc: 0.8189, best train loss: 0.4098  @ epoch 16419 )\n",
      "[Epoch: 16940] train loss: 0.4306, train acc: 0.8098, val loss: 0.4632, val acc: 0.7909  (best train acc: 0.8235, best val acc: 0.8189, best train loss: 0.4098  @ epoch 16419 )\n",
      "[Epoch: 16960] train loss: 0.4370, train acc: 0.8112, val loss: 0.4674, val acc: 0.7892  (best train acc: 0.8235, best val acc: 0.8189, best train loss: 0.4098  @ epoch 16419 )\n",
      "[Epoch: 16980] train loss: 0.4257, train acc: 0.8125, val loss: 0.4689, val acc: 0.7987  (best train acc: 0.8235, best val acc: 0.8189, best train loss: 0.4098  @ epoch 16419 )\n",
      "[Epoch: 17000] train loss: 0.4464, train acc: 0.8041, val loss: 0.5029, val acc: 0.7548  (best train acc: 0.8235, best val acc: 0.8189, best train loss: 0.4098  @ epoch 16419 )\n",
      "[Epoch: 17020] train loss: 0.4253, train acc: 0.8098, val loss: 0.4460, val acc: 0.8084  (best train acc: 0.8235, best val acc: 0.8189, best train loss: 0.4098  @ epoch 16419 )\n",
      "[Epoch: 17040] train loss: 0.4229, train acc: 0.8154, val loss: 0.4493, val acc: 0.8078  (best train acc: 0.8235, best val acc: 0.8189, best train loss: 0.4098  @ epoch 16419 )\n",
      "[Epoch: 17060] train loss: 0.4217, train acc: 0.8115, val loss: 0.4568, val acc: 0.7899  (best train acc: 0.8235, best val acc: 0.8189, best train loss: 0.4098  @ epoch 16419 )\n",
      "[Epoch: 17080] train loss: 0.4510, train acc: 0.7980, val loss: 0.4484, val acc: 0.8020  (best train acc: 0.8235, best val acc: 0.8189, best train loss: 0.4098  @ epoch 16419 )\n",
      "[Epoch: 17100] train loss: 0.4417, train acc: 0.8023, val loss: 0.5106, val acc: 0.7602  (best train acc: 0.8235, best val acc: 0.8189, best train loss: 0.4098  @ epoch 16419 )\n",
      "[Epoch: 17120] train loss: 0.4485, train acc: 0.8007, val loss: 0.4605, val acc: 0.7976  (best train acc: 0.8235, best val acc: 0.8189, best train loss: 0.4098  @ epoch 16419 )\n",
      "[Epoch: 17140] train loss: 0.4474, train acc: 0.8014, val loss: 0.4487, val acc: 0.8061  (best train acc: 0.8235, best val acc: 0.8196, best train loss: 0.4098  @ epoch 16419 )\n",
      "[Epoch: 17160] train loss: 0.4423, train acc: 0.8023, val loss: 0.4720, val acc: 0.7828  (best train acc: 0.8235, best val acc: 0.8196, best train loss: 0.4098  @ epoch 16419 )\n",
      "[Epoch: 17180] train loss: 0.4388, train acc: 0.8064, val loss: 0.4715, val acc: 0.7848  (best train acc: 0.8235, best val acc: 0.8196, best train loss: 0.4098  @ epoch 16419 )\n",
      "[Epoch: 17200] train loss: 0.4704, train acc: 0.7972, val loss: 0.4639, val acc: 0.7929  (best train acc: 0.8235, best val acc: 0.8196, best train loss: 0.4098  @ epoch 16419 )\n",
      "[Epoch: 17220] train loss: 0.4149, train acc: 0.8150, val loss: 0.4581, val acc: 0.7922  (best train acc: 0.8235, best val acc: 0.8196, best train loss: 0.4098  @ epoch 16419 )\n",
      "[Epoch: 17240] train loss: 0.4392, train acc: 0.8068, val loss: 0.4768, val acc: 0.7804  (best train acc: 0.8235, best val acc: 0.8196, best train loss: 0.4098  @ epoch 16419 )\n",
      "[Epoch: 17260] train loss: 0.4393, train acc: 0.8068, val loss: 0.4676, val acc: 0.7798  (best train acc: 0.8235, best val acc: 0.8196, best train loss: 0.4057  @ epoch 17250 )\n",
      "[Epoch: 17280] train loss: 0.4300, train acc: 0.8159, val loss: 0.4716, val acc: 0.7879  (best train acc: 0.8235, best val acc: 0.8196, best train loss: 0.4057  @ epoch 17250 )\n",
      "[Epoch: 17300] train loss: 0.4578, train acc: 0.7950, val loss: 0.4631, val acc: 0.7868  (best train acc: 0.8235, best val acc: 0.8196, best train loss: 0.4057  @ epoch 17250 )\n",
      "[Epoch: 17320] train loss: 0.4658, train acc: 0.7976, val loss: 0.4530, val acc: 0.7919  (best train acc: 0.8235, best val acc: 0.8196, best train loss: 0.4057  @ epoch 17250 )\n",
      "[Epoch: 17340] train loss: 0.4462, train acc: 0.8035, val loss: 0.4478, val acc: 0.7966  (best train acc: 0.8235, best val acc: 0.8196, best train loss: 0.4057  @ epoch 17250 )\n",
      "[Epoch: 17360] train loss: 0.4235, train acc: 0.8139, val loss: 0.4477, val acc: 0.8067  (best train acc: 0.8235, best val acc: 0.8196, best train loss: 0.4057  @ epoch 17250 )\n",
      "[Epoch: 17380] train loss: 0.4215, train acc: 0.8139, val loss: 0.4597, val acc: 0.7956  (best train acc: 0.8235, best val acc: 0.8196, best train loss: 0.4057  @ epoch 17250 )\n",
      "[Epoch: 17400] train loss: 0.4443, train acc: 0.8032, val loss: 0.4515, val acc: 0.8098  (best train acc: 0.8235, best val acc: 0.8196, best train loss: 0.4057  @ epoch 17250 )\n",
      "[Epoch: 17420] train loss: 0.4166, train acc: 0.8181, val loss: 0.4526, val acc: 0.7973  (best train acc: 0.8235, best val acc: 0.8196, best train loss: 0.4057  @ epoch 17250 )\n",
      "[Epoch: 17440] train loss: 0.4194, train acc: 0.8180, val loss: 0.4642, val acc: 0.7838  (best train acc: 0.8235, best val acc: 0.8196, best train loss: 0.4057  @ epoch 17250 )\n",
      "[Epoch: 17460] train loss: 0.4367, train acc: 0.8093, val loss: 0.4522, val acc: 0.7983  (best train acc: 0.8235, best val acc: 0.8196, best train loss: 0.4057  @ epoch 17250 )\n",
      "[Epoch: 17480] train loss: 0.4266, train acc: 0.8107, val loss: 0.4696, val acc: 0.7841  (best train acc: 0.8235, best val acc: 0.8196, best train loss: 0.4057  @ epoch 17250 )\n",
      "[Epoch: 17500] train loss: 0.4374, train acc: 0.8058, val loss: 0.4651, val acc: 0.7828  (best train acc: 0.8235, best val acc: 0.8196, best train loss: 0.4057  @ epoch 17250 )\n",
      "[Epoch: 17520] train loss: 0.4240, train acc: 0.8105, val loss: 0.4651, val acc: 0.7852  (best train acc: 0.8235, best val acc: 0.8196, best train loss: 0.4057  @ epoch 17250 )\n",
      "[Epoch: 17540] train loss: 0.4344, train acc: 0.8110, val loss: 0.4580, val acc: 0.7943  (best train acc: 0.8235, best val acc: 0.8196, best train loss: 0.4057  @ epoch 17250 )\n",
      "[Epoch: 17560] train loss: 0.4297, train acc: 0.8124, val loss: 0.4801, val acc: 0.7750  (best train acc: 0.8235, best val acc: 0.8196, best train loss: 0.4057  @ epoch 17250 )\n",
      "[Epoch: 17580] train loss: 0.4313, train acc: 0.8102, val loss: 0.4577, val acc: 0.7912  (best train acc: 0.8235, best val acc: 0.8196, best train loss: 0.4057  @ epoch 17250 )\n",
      "[Epoch: 17600] train loss: 0.4253, train acc: 0.8100, val loss: 0.4667, val acc: 0.7862  (best train acc: 0.8235, best val acc: 0.8196, best train loss: 0.4057  @ epoch 17250 )\n",
      "[Epoch: 17620] train loss: 0.4247, train acc: 0.8124, val loss: 0.4573, val acc: 0.7845  (best train acc: 0.8235, best val acc: 0.8196, best train loss: 0.4057  @ epoch 17250 )\n",
      "[Epoch: 17640] train loss: 0.4343, train acc: 0.8087, val loss: 0.4544, val acc: 0.8020  (best train acc: 0.8235, best val acc: 0.8196, best train loss: 0.4057  @ epoch 17250 )\n",
      "[Epoch: 17660] train loss: 0.4420, train acc: 0.8057, val loss: 0.4438, val acc: 0.8030  (best train acc: 0.8235, best val acc: 0.8196, best train loss: 0.4057  @ epoch 17250 )\n",
      "[Epoch: 17680] train loss: 0.4430, train acc: 0.8047, val loss: 0.4623, val acc: 0.7865  (best train acc: 0.8235, best val acc: 0.8196, best train loss: 0.4057  @ epoch 17250 )\n",
      "[Epoch: 17700] train loss: 0.4227, train acc: 0.8125, val loss: 0.4611, val acc: 0.7791  (best train acc: 0.8235, best val acc: 0.8196, best train loss: 0.4057  @ epoch 17250 )\n",
      "[Epoch: 17720] train loss: 0.4206, train acc: 0.8081, val loss: 0.4305, val acc: 0.8233  (best train acc: 0.8235, best val acc: 0.8233, best train loss: 0.4057  @ epoch 17250 )\n",
      "[Epoch: 17740] train loss: 0.4449, train acc: 0.7994, val loss: 0.4616, val acc: 0.7774  (best train acc: 0.8235, best val acc: 0.8233, best train loss: 0.4057  @ epoch 17250 )\n",
      "[Epoch: 17760] train loss: 0.4285, train acc: 0.8099, val loss: 0.4492, val acc: 0.8054  (best train acc: 0.8235, best val acc: 0.8233, best train loss: 0.4057  @ epoch 17250 )\n",
      "[Epoch: 17780] train loss: 0.4235, train acc: 0.8161, val loss: 0.4768, val acc: 0.7710  (best train acc: 0.8235, best val acc: 0.8233, best train loss: 0.4057  @ epoch 17250 )\n",
      "[Epoch: 17800] train loss: 0.4349, train acc: 0.8133, val loss: 0.4467, val acc: 0.7960  (best train acc: 0.8235, best val acc: 0.8233, best train loss: 0.4057  @ epoch 17250 )\n",
      "[Epoch: 17820] train loss: 0.4230, train acc: 0.8150, val loss: 0.4521, val acc: 0.7875  (best train acc: 0.8235, best val acc: 0.8233, best train loss: 0.4057  @ epoch 17250 )\n",
      "[Epoch: 17840] train loss: 0.4581, train acc: 0.7984, val loss: 0.4503, val acc: 0.7997  (best train acc: 0.8235, best val acc: 0.8233, best train loss: 0.4057  @ epoch 17250 )\n",
      "[Epoch: 17860] train loss: 0.4283, train acc: 0.8081, val loss: 0.4571, val acc: 0.7892  (best train acc: 0.8235, best val acc: 0.8233, best train loss: 0.4057  @ epoch 17250 )\n",
      "[Epoch: 17880] train loss: 0.4436, train acc: 0.7995, val loss: 0.4641, val acc: 0.7906  (best train acc: 0.8235, best val acc: 0.8233, best train loss: 0.4057  @ epoch 17250 )\n",
      "[Epoch: 17900] train loss: 0.4261, train acc: 0.8130, val loss: 0.4535, val acc: 0.7922  (best train acc: 0.8235, best val acc: 0.8233, best train loss: 0.4057  @ epoch 17250 )\n",
      "[Epoch: 17920] train loss: 0.4366, train acc: 0.8028, val loss: 0.4494, val acc: 0.7926  (best train acc: 0.8235, best val acc: 0.8233, best train loss: 0.4057  @ epoch 17250 )\n",
      "[Epoch: 17940] train loss: 0.4375, train acc: 0.8087, val loss: 0.4589, val acc: 0.7889  (best train acc: 0.8235, best val acc: 0.8233, best train loss: 0.4057  @ epoch 17250 )\n",
      "[Epoch: 17960] train loss: 0.4152, train acc: 0.8160, val loss: 0.4401, val acc: 0.8061  (best train acc: 0.8235, best val acc: 0.8233, best train loss: 0.4057  @ epoch 17250 )\n",
      "[Epoch: 17980] train loss: 0.4270, train acc: 0.8122, val loss: 0.4529, val acc: 0.7912  (best train acc: 0.8235, best val acc: 0.8233, best train loss: 0.4057  @ epoch 17250 )\n",
      "[Epoch: 18000] train loss: 0.4311, train acc: 0.8107, val loss: 0.4635, val acc: 0.7889  (best train acc: 0.8235, best val acc: 0.8233, best train loss: 0.4057  @ epoch 17250 )\n",
      "[Epoch: 18020] train loss: 0.4521, train acc: 0.7985, val loss: 0.4537, val acc: 0.7922  (best train acc: 0.8235, best val acc: 0.8233, best train loss: 0.4057  @ epoch 17250 )\n",
      "[Epoch: 18040] train loss: 0.4456, train acc: 0.8013, val loss: 0.4456, val acc: 0.7936  (best train acc: 0.8235, best val acc: 0.8233, best train loss: 0.4057  @ epoch 17250 )\n",
      "[Epoch: 18060] train loss: 0.4172, train acc: 0.8154, val loss: 0.4456, val acc: 0.8020  (best train acc: 0.8235, best val acc: 0.8233, best train loss: 0.4057  @ epoch 17250 )\n",
      "[Epoch: 18080] train loss: 0.4304, train acc: 0.8094, val loss: 0.4597, val acc: 0.7818  (best train acc: 0.8235, best val acc: 0.8233, best train loss: 0.4057  @ epoch 17250 )\n",
      "[Epoch: 18100] train loss: 0.4262, train acc: 0.8091, val loss: 0.4744, val acc: 0.7767  (best train acc: 0.8235, best val acc: 0.8233, best train loss: 0.4057  @ epoch 17250 )\n",
      "[Epoch: 18120] train loss: 0.4231, train acc: 0.8127, val loss: 0.4602, val acc: 0.7868  (best train acc: 0.8235, best val acc: 0.8233, best train loss: 0.4057  @ epoch 17250 )\n",
      "[Epoch: 18140] train loss: 0.4228, train acc: 0.8119, val loss: 0.4628, val acc: 0.7791  (best train acc: 0.8235, best val acc: 0.8233, best train loss: 0.4057  @ epoch 17250 )\n",
      "[Epoch: 18160] train loss: 0.4378, train acc: 0.7984, val loss: 0.4402, val acc: 0.8128  (best train acc: 0.8235, best val acc: 0.8233, best train loss: 0.4057  @ epoch 17250 )\n",
      "[Epoch: 18180] train loss: 0.4404, train acc: 0.8048, val loss: 0.4530, val acc: 0.7811  (best train acc: 0.8235, best val acc: 0.8260, best train loss: 0.4057  @ epoch 17250 )\n",
      "[Epoch: 18200] train loss: 0.4486, train acc: 0.8062, val loss: 0.4504, val acc: 0.7987  (best train acc: 0.8235, best val acc: 0.8260, best train loss: 0.4057  @ epoch 17250 )\n",
      "[Epoch: 18220] train loss: 0.4225, train acc: 0.8129, val loss: 0.4488, val acc: 0.7946  (best train acc: 0.8235, best val acc: 0.8260, best train loss: 0.4057  @ epoch 17250 )\n",
      "[Epoch: 18240] train loss: 0.4212, train acc: 0.8120, val loss: 0.4443, val acc: 0.7970  (best train acc: 0.8235, best val acc: 0.8260, best train loss: 0.4057  @ epoch 17250 )\n",
      "[Epoch: 18260] train loss: 0.4239, train acc: 0.8108, val loss: 0.4515, val acc: 0.7956  (best train acc: 0.8235, best val acc: 0.8260, best train loss: 0.4057  @ epoch 17250 )\n",
      "[Epoch: 18280] train loss: 0.4314, train acc: 0.8083, val loss: 0.4367, val acc: 0.8118  (best train acc: 0.8235, best val acc: 0.8260, best train loss: 0.4057  @ epoch 17250 )\n",
      "[Epoch: 18300] train loss: 0.4269, train acc: 0.8106, val loss: 0.4526, val acc: 0.7909  (best train acc: 0.8235, best val acc: 0.8260, best train loss: 0.4057  @ epoch 17250 )\n",
      "[Epoch: 18320] train loss: 0.4136, train acc: 0.8171, val loss: 0.4355, val acc: 0.8101  (best train acc: 0.8235, best val acc: 0.8260, best train loss: 0.4057  @ epoch 17250 )\n",
      "[Epoch: 18340] train loss: 0.4458, train acc: 0.8075, val loss: 0.4555, val acc: 0.7848  (best train acc: 0.8235, best val acc: 0.8260, best train loss: 0.4057  @ epoch 17250 )\n",
      "[Epoch: 18360] train loss: 0.4229, train acc: 0.8075, val loss: 0.4457, val acc: 0.8040  (best train acc: 0.8235, best val acc: 0.8260, best train loss: 0.4054  @ epoch 18347 )\n",
      "[Epoch: 18380] train loss: 0.4243, train acc: 0.8097, val loss: 0.4664, val acc: 0.7754  (best train acc: 0.8235, best val acc: 0.8260, best train loss: 0.4054  @ epoch 18347 )\n",
      "[Epoch: 18400] train loss: 0.4391, train acc: 0.8012, val loss: 0.4395, val acc: 0.8030  (best train acc: 0.8235, best val acc: 0.8260, best train loss: 0.4047  @ epoch 18393 )\n",
      "[Epoch: 18420] train loss: 0.4412, train acc: 0.8120, val loss: 0.4499, val acc: 0.7916  (best train acc: 0.8235, best val acc: 0.8260, best train loss: 0.4047  @ epoch 18393 )\n",
      "[Epoch: 18440] train loss: 0.4338, train acc: 0.8082, val loss: 0.4454, val acc: 0.7973  (best train acc: 0.8235, best val acc: 0.8260, best train loss: 0.4047  @ epoch 18393 )\n",
      "[Epoch: 18460] train loss: 0.4408, train acc: 0.8060, val loss: 0.4500, val acc: 0.8017  (best train acc: 0.8235, best val acc: 0.8260, best train loss: 0.4047  @ epoch 18393 )\n",
      "[Epoch: 18480] train loss: 0.4327, train acc: 0.8117, val loss: 0.4665, val acc: 0.7838  (best train acc: 0.8235, best val acc: 0.8260, best train loss: 0.4047  @ epoch 18393 )\n",
      "[Epoch: 18500] train loss: 0.4160, train acc: 0.8171, val loss: 0.4640, val acc: 0.7804  (best train acc: 0.8235, best val acc: 0.8260, best train loss: 0.4047  @ epoch 18393 )\n",
      "[Epoch: 18520] train loss: 0.4145, train acc: 0.8127, val loss: 0.4335, val acc: 0.8074  (best train acc: 0.8235, best val acc: 0.8260, best train loss: 0.4047  @ epoch 18393 )\n",
      "[Epoch: 18540] train loss: 0.4384, train acc: 0.8057, val loss: 0.4430, val acc: 0.8047  (best train acc: 0.8235, best val acc: 0.8260, best train loss: 0.4047  @ epoch 18393 )\n",
      "[Epoch: 18560] train loss: 0.4360, train acc: 0.8068, val loss: 0.4519, val acc: 0.7882  (best train acc: 0.8235, best val acc: 0.8260, best train loss: 0.4047  @ epoch 18393 )\n",
      "[Epoch: 18580] train loss: 0.4424, train acc: 0.8052, val loss: 0.4469, val acc: 0.7963  (best train acc: 0.8248, best val acc: 0.8260, best train loss: 0.4047  @ epoch 18393 )\n",
      "[Epoch: 18600] train loss: 0.4197, train acc: 0.8154, val loss: 0.4459, val acc: 0.8003  (best train acc: 0.8248, best val acc: 0.8260, best train loss: 0.4047  @ epoch 18393 )\n",
      "[Epoch: 18620] train loss: 0.4410, train acc: 0.8076, val loss: 0.4428, val acc: 0.8017  (best train acc: 0.8248, best val acc: 0.8260, best train loss: 0.4047  @ epoch 18393 )\n",
      "[Epoch: 18640] train loss: 0.4259, train acc: 0.8114, val loss: 0.4448, val acc: 0.7936  (best train acc: 0.8248, best val acc: 0.8260, best train loss: 0.4047  @ epoch 18393 )\n",
      "[Epoch: 18660] train loss: 0.4176, train acc: 0.8172, val loss: 0.4424, val acc: 0.7960  (best train acc: 0.8248, best val acc: 0.8260, best train loss: 0.4040  @ epoch 18642 )\n",
      "[Epoch: 18680] train loss: 0.4309, train acc: 0.8091, val loss: 0.4669, val acc: 0.7801  (best train acc: 0.8248, best val acc: 0.8260, best train loss: 0.4040  @ epoch 18642 )\n",
      "[Epoch: 18700] train loss: 0.4202, train acc: 0.8120, val loss: 0.4420, val acc: 0.8013  (best train acc: 0.8248, best val acc: 0.8260, best train loss: 0.4040  @ epoch 18642 )\n",
      "[Epoch: 18720] train loss: 0.4344, train acc: 0.8094, val loss: 0.4370, val acc: 0.8027  (best train acc: 0.8248, best val acc: 0.8260, best train loss: 0.4040  @ epoch 18642 )\n",
      "[Epoch: 18740] train loss: 0.4147, train acc: 0.8147, val loss: 0.4282, val acc: 0.8185  (best train acc: 0.8248, best val acc: 0.8260, best train loss: 0.4040  @ epoch 18642 )\n",
      "[Epoch: 18760] train loss: 0.4466, train acc: 0.8044, val loss: 0.4572, val acc: 0.7852  (best train acc: 0.8248, best val acc: 0.8260, best train loss: 0.4040  @ epoch 18642 )\n",
      "[Epoch: 18780] train loss: 0.4180, train acc: 0.8172, val loss: 0.4430, val acc: 0.8034  (best train acc: 0.8248, best val acc: 0.8260, best train loss: 0.4040  @ epoch 18642 )\n",
      "[Epoch: 18800] train loss: 0.4369, train acc: 0.8064, val loss: 0.4311, val acc: 0.8199  (best train acc: 0.8248, best val acc: 0.8260, best train loss: 0.4040  @ epoch 18642 )\n",
      "[Epoch: 18820] train loss: 0.4105, train acc: 0.8177, val loss: 0.4351, val acc: 0.8074  (best train acc: 0.8248, best val acc: 0.8260, best train loss: 0.4040  @ epoch 18642 )\n",
      "[Epoch: 18840] train loss: 0.4320, train acc: 0.8036, val loss: 0.4566, val acc: 0.7879  (best train acc: 0.8248, best val acc: 0.8260, best train loss: 0.4040  @ epoch 18642 )\n",
      "[Epoch: 18860] train loss: 0.4514, train acc: 0.7950, val loss: 0.4508, val acc: 0.7922  (best train acc: 0.8248, best val acc: 0.8260, best train loss: 0.4040  @ epoch 18642 )\n",
      "[Epoch: 18880] train loss: 0.4582, train acc: 0.7992, val loss: 0.4392, val acc: 0.8054  (best train acc: 0.8248, best val acc: 0.8260, best train loss: 0.4040  @ epoch 18642 )\n",
      "[Epoch: 18900] train loss: 0.4111, train acc: 0.8149, val loss: 0.4393, val acc: 0.8078  (best train acc: 0.8248, best val acc: 0.8260, best train loss: 0.4021  @ epoch 18892 )\n",
      "[Epoch: 18920] train loss: 0.4187, train acc: 0.8165, val loss: 0.4356, val acc: 0.8111  (best train acc: 0.8248, best val acc: 0.8260, best train loss: 0.4012  @ epoch 18914 )\n",
      "[Epoch: 18940] train loss: 0.4316, train acc: 0.8124, val loss: 0.4489, val acc: 0.7943  (best train acc: 0.8248, best val acc: 0.8260, best train loss: 0.4012  @ epoch 18914 )\n",
      "[Epoch: 18960] train loss: 0.4131, train acc: 0.8170, val loss: 0.4572, val acc: 0.7815  (best train acc: 0.8248, best val acc: 0.8260, best train loss: 0.4012  @ epoch 18914 )\n",
      "[Epoch: 18980] train loss: 0.4205, train acc: 0.8122, val loss: 0.4429, val acc: 0.7963  (best train acc: 0.8248, best val acc: 0.8260, best train loss: 0.4012  @ epoch 18914 )\n",
      "[Epoch: 19000] train loss: 0.4185, train acc: 0.8129, val loss: 0.4326, val acc: 0.8000  (best train acc: 0.8248, best val acc: 0.8260, best train loss: 0.4012  @ epoch 18914 )\n",
      "[Epoch: 19020] train loss: 0.4250, train acc: 0.8088, val loss: 0.4320, val acc: 0.8064  (best train acc: 0.8248, best val acc: 0.8260, best train loss: 0.4012  @ epoch 18914 )\n",
      "[Epoch: 19040] train loss: 0.4124, train acc: 0.8213, val loss: 0.4514, val acc: 0.7916  (best train acc: 0.8250, best val acc: 0.8260, best train loss: 0.4012  @ epoch 18914 )\n",
      "[Epoch: 19060] train loss: 0.4178, train acc: 0.8130, val loss: 0.4394, val acc: 0.8037  (best train acc: 0.8250, best val acc: 0.8260, best train loss: 0.4012  @ epoch 18914 )\n",
      "[Epoch: 19080] train loss: 0.4239, train acc: 0.8089, val loss: 0.4487, val acc: 0.7936  (best train acc: 0.8250, best val acc: 0.8260, best train loss: 0.4012  @ epoch 18914 )\n",
      "[Epoch: 19100] train loss: 0.4573, train acc: 0.7954, val loss: 0.4495, val acc: 0.7980  (best train acc: 0.8250, best val acc: 0.8260, best train loss: 0.4012  @ epoch 18914 )\n",
      "[Epoch: 19120] train loss: 0.4156, train acc: 0.8173, val loss: 0.4307, val acc: 0.8057  (best train acc: 0.8250, best val acc: 0.8260, best train loss: 0.4012  @ epoch 18914 )\n",
      "[Epoch: 19140] train loss: 0.4204, train acc: 0.8138, val loss: 0.4401, val acc: 0.8061  (best train acc: 0.8250, best val acc: 0.8260, best train loss: 0.4012  @ epoch 18914 )\n",
      "[Epoch: 19160] train loss: 0.4370, train acc: 0.8023, val loss: 0.4271, val acc: 0.8132  (best train acc: 0.8255, best val acc: 0.8260, best train loss: 0.3952  @ epoch 19143 )\n",
      "[Epoch: 19180] train loss: 0.4497, train acc: 0.7985, val loss: 0.4443, val acc: 0.7922  (best train acc: 0.8255, best val acc: 0.8260, best train loss: 0.3952  @ epoch 19143 )\n",
      "[Epoch: 19200] train loss: 0.4138, train acc: 0.8142, val loss: 0.4348, val acc: 0.8071  (best train acc: 0.8255, best val acc: 0.8260, best train loss: 0.3952  @ epoch 19143 )\n",
      "[Epoch: 19220] train loss: 0.4072, train acc: 0.8179, val loss: 0.4284, val acc: 0.8165  (best train acc: 0.8255, best val acc: 0.8260, best train loss: 0.3952  @ epoch 19143 )\n",
      "[Epoch: 19240] train loss: 0.4433, train acc: 0.8021, val loss: 0.4618, val acc: 0.7838  (best train acc: 0.8255, best val acc: 0.8260, best train loss: 0.3952  @ epoch 19143 )\n",
      "[Epoch: 19260] train loss: 0.4283, train acc: 0.8112, val loss: 0.4400, val acc: 0.7970  (best train acc: 0.8255, best val acc: 0.8260, best train loss: 0.3952  @ epoch 19143 )\n",
      "[Epoch: 19280] train loss: 0.4306, train acc: 0.8114, val loss: 0.4318, val acc: 0.8054  (best train acc: 0.8255, best val acc: 0.8260, best train loss: 0.3952  @ epoch 19143 )\n",
      "[Epoch: 19300] train loss: 0.4216, train acc: 0.8102, val loss: 0.4203, val acc: 0.8148  (best train acc: 0.8255, best val acc: 0.8260, best train loss: 0.3952  @ epoch 19143 )\n",
      "[Epoch: 19320] train loss: 0.4077, train acc: 0.8205, val loss: 0.4296, val acc: 0.8047  (best train acc: 0.8255, best val acc: 0.8260, best train loss: 0.3952  @ epoch 19143 )\n",
      "[Epoch: 19340] train loss: 0.4187, train acc: 0.8121, val loss: 0.4225, val acc: 0.8246  (best train acc: 0.8255, best val acc: 0.8260, best train loss: 0.3952  @ epoch 19143 )\n",
      "[Epoch: 19360] train loss: 0.4330, train acc: 0.8013, val loss: 0.4375, val acc: 0.7987  (best train acc: 0.8255, best val acc: 0.8287, best train loss: 0.3952  @ epoch 19143 )\n",
      "[Epoch: 19380] train loss: 0.4188, train acc: 0.8155, val loss: 0.4350, val acc: 0.7983  (best train acc: 0.8255, best val acc: 0.8287, best train loss: 0.3952  @ epoch 19143 )\n",
      "[Epoch: 19400] train loss: 0.4095, train acc: 0.8177, val loss: 0.4251, val acc: 0.8108  (best train acc: 0.8255, best val acc: 0.8287, best train loss: 0.3952  @ epoch 19143 )\n",
      "[Epoch: 19420] train loss: 0.4125, train acc: 0.8184, val loss: 0.4238, val acc: 0.8132  (best train acc: 0.8255, best val acc: 0.8287, best train loss: 0.3952  @ epoch 19143 )\n",
      "[Epoch: 19440] train loss: 0.4388, train acc: 0.8044, val loss: 0.4356, val acc: 0.7997  (best train acc: 0.8255, best val acc: 0.8287, best train loss: 0.3952  @ epoch 19143 )\n",
      "[Epoch: 19460] train loss: 0.4169, train acc: 0.8154, val loss: 0.4257, val acc: 0.8165  (best train acc: 0.8255, best val acc: 0.8287, best train loss: 0.3952  @ epoch 19143 )\n",
      "[Epoch: 19480] train loss: 0.4107, train acc: 0.8149, val loss: 0.4302, val acc: 0.8020  (best train acc: 0.8255, best val acc: 0.8287, best train loss: 0.3952  @ epoch 19143 )\n",
      "[Epoch: 19500] train loss: 0.4206, train acc: 0.8082, val loss: 0.4493, val acc: 0.7919  (best train acc: 0.8255, best val acc: 0.8287, best train loss: 0.3952  @ epoch 19143 )\n",
      "[Epoch: 19520] train loss: 0.4266, train acc: 0.8102, val loss: 0.4308, val acc: 0.7966  (best train acc: 0.8255, best val acc: 0.8287, best train loss: 0.3952  @ epoch 19143 )\n",
      "[Epoch: 19540] train loss: 0.4081, train acc: 0.8174, val loss: 0.4236, val acc: 0.8105  (best train acc: 0.8255, best val acc: 0.8287, best train loss: 0.3952  @ epoch 19143 )\n",
      "[Epoch: 19560] train loss: 0.4186, train acc: 0.8125, val loss: 0.4432, val acc: 0.7960  (best train acc: 0.8255, best val acc: 0.8287, best train loss: 0.3952  @ epoch 19143 )\n",
      "[Epoch: 19580] train loss: 0.4428, train acc: 0.8053, val loss: 0.4431, val acc: 0.7868  (best train acc: 0.8255, best val acc: 0.8287, best train loss: 0.3952  @ epoch 19143 )\n",
      "[Epoch: 19600] train loss: 0.4140, train acc: 0.8190, val loss: 0.4283, val acc: 0.8121  (best train acc: 0.8255, best val acc: 0.8287, best train loss: 0.3952  @ epoch 19143 )\n",
      "[Epoch: 19620] train loss: 0.4186, train acc: 0.8109, val loss: 0.4261, val acc: 0.8047  (best train acc: 0.8255, best val acc: 0.8287, best train loss: 0.3952  @ epoch 19143 )\n",
      "[Epoch: 19640] train loss: 0.4352, train acc: 0.8050, val loss: 0.4230, val acc: 0.8121  (best train acc: 0.8255, best val acc: 0.8287, best train loss: 0.3952  @ epoch 19143 )\n",
      "[Epoch: 19660] train loss: 0.4385, train acc: 0.8092, val loss: 0.4417, val acc: 0.7919  (best train acc: 0.8255, best val acc: 0.8287, best train loss: 0.3952  @ epoch 19143 )\n",
      "[Epoch: 19680] train loss: 0.4134, train acc: 0.8195, val loss: 0.4259, val acc: 0.8094  (best train acc: 0.8255, best val acc: 0.8287, best train loss: 0.3952  @ epoch 19143 )\n",
      "[Epoch: 19700] train loss: 0.4255, train acc: 0.8088, val loss: 0.4223, val acc: 0.8155  (best train acc: 0.8255, best val acc: 0.8287, best train loss: 0.3952  @ epoch 19143 )\n",
      "[Epoch: 19720] train loss: 0.4105, train acc: 0.8154, val loss: 0.4351, val acc: 0.7987  (best train acc: 0.8255, best val acc: 0.8287, best train loss: 0.3952  @ epoch 19143 )\n",
      "[Epoch: 19740] train loss: 0.4189, train acc: 0.8128, val loss: 0.4323, val acc: 0.8067  (best train acc: 0.8255, best val acc: 0.8287, best train loss: 0.3952  @ epoch 19143 )\n",
      "[Epoch: 19760] train loss: 0.4316, train acc: 0.8105, val loss: 0.4280, val acc: 0.8040  (best train acc: 0.8255, best val acc: 0.8287, best train loss: 0.3952  @ epoch 19143 )\n",
      "[Epoch: 19780] train loss: 0.4570, train acc: 0.7984, val loss: 0.4294, val acc: 0.8159  (best train acc: 0.8255, best val acc: 0.8287, best train loss: 0.3952  @ epoch 19143 )\n",
      "[Epoch: 19800] train loss: 0.4025, train acc: 0.8229, val loss: 0.4302, val acc: 0.8148  (best train acc: 0.8255, best val acc: 0.8287, best train loss: 0.3952  @ epoch 19143 )\n",
      "[Epoch: 19820] train loss: 0.4338, train acc: 0.8094, val loss: 0.4306, val acc: 0.8030  (best train acc: 0.8255, best val acc: 0.8287, best train loss: 0.3952  @ epoch 19143 )\n",
      "[Epoch: 19840] train loss: 0.4285, train acc: 0.8096, val loss: 0.4300, val acc: 0.8024  (best train acc: 0.8255, best val acc: 0.8287, best train loss: 0.3952  @ epoch 19143 )\n",
      "[Epoch: 19860] train loss: 0.4219, train acc: 0.8122, val loss: 0.4448, val acc: 0.7885  (best train acc: 0.8255, best val acc: 0.8287, best train loss: 0.3952  @ epoch 19143 )\n",
      "[Epoch: 19880] train loss: 0.4093, train acc: 0.8167, val loss: 0.4263, val acc: 0.8091  (best train acc: 0.8255, best val acc: 0.8287, best train loss: 0.3952  @ epoch 19143 )\n",
      "[Epoch: 19900] train loss: 0.4121, train acc: 0.8190, val loss: 0.4261, val acc: 0.8030  (best train acc: 0.8255, best val acc: 0.8287, best train loss: 0.3952  @ epoch 19143 )\n",
      "[Epoch: 19920] train loss: 0.4086, train acc: 0.8201, val loss: 0.4283, val acc: 0.8061  (best train acc: 0.8255, best val acc: 0.8287, best train loss: 0.3952  @ epoch 19143 )\n",
      "[Epoch: 19940] train loss: 0.4310, train acc: 0.8071, val loss: 0.4546, val acc: 0.7761  (best train acc: 0.8255, best val acc: 0.8287, best train loss: 0.3952  @ epoch 19143 )\n",
      "[Epoch: 19960] train loss: 0.4231, train acc: 0.8151, val loss: 0.4399, val acc: 0.7970  (best train acc: 0.8255, best val acc: 0.8287, best train loss: 0.3952  @ epoch 19143 )\n",
      "[Epoch: 19980] train loss: 0.4222, train acc: 0.8079, val loss: 0.4242, val acc: 0.8118  (best train acc: 0.8255, best val acc: 0.8287, best train loss: 0.3952  @ epoch 19143 )\n",
      "[Epoch: 20000] train loss: 0.4208, train acc: 0.8071, val loss: 0.4163, val acc: 0.8202  (best train acc: 0.8259, best val acc: 0.8287, best train loss: 0.3952  @ epoch 19143 )\n",
      "[Epoch: 20020] train loss: 0.4085, train acc: 0.8163, val loss: 0.4336, val acc: 0.8000  (best train acc: 0.8259, best val acc: 0.8287, best train loss: 0.3952  @ epoch 19143 )\n",
      "[Epoch: 20040] train loss: 0.4534, train acc: 0.7989, val loss: 0.4658, val acc: 0.7777  (best train acc: 0.8259, best val acc: 0.8287, best train loss: 0.3952  @ epoch 19143 )\n",
      "[Epoch: 20060] train loss: 0.4301, train acc: 0.8095, val loss: 0.4399, val acc: 0.8064  (best train acc: 0.8259, best val acc: 0.8287, best train loss: 0.3952  @ epoch 19143 )\n",
      "[Epoch: 20080] train loss: 0.4309, train acc: 0.8100, val loss: 0.4394, val acc: 0.7922  (best train acc: 0.8259, best val acc: 0.8287, best train loss: 0.3952  @ epoch 19143 )\n",
      "[Epoch: 20100] train loss: 0.4172, train acc: 0.8104, val loss: 0.4391, val acc: 0.7970  (best train acc: 0.8259, best val acc: 0.8287, best train loss: 0.3952  @ epoch 19143 )\n",
      "[Epoch: 20120] train loss: 0.4058, train acc: 0.8216, val loss: 0.4283, val acc: 0.8064  (best train acc: 0.8259, best val acc: 0.8287, best train loss: 0.3952  @ epoch 19143 )\n",
      "[Epoch: 20140] train loss: 0.4091, train acc: 0.8146, val loss: 0.4270, val acc: 0.8027  (best train acc: 0.8294, best val acc: 0.8287, best train loss: 0.3952  @ epoch 19143 )\n",
      "[Epoch: 20160] train loss: 0.4077, train acc: 0.8187, val loss: 0.4226, val acc: 0.8074  (best train acc: 0.8294, best val acc: 0.8287, best train loss: 0.3952  @ epoch 19143 )\n",
      "[Epoch: 20180] train loss: 0.4131, train acc: 0.8165, val loss: 0.4246, val acc: 0.8051  (best train acc: 0.8294, best val acc: 0.8287, best train loss: 0.3952  @ epoch 19143 )\n",
      "[Epoch: 20200] train loss: 0.4057, train acc: 0.8225, val loss: 0.4267, val acc: 0.8061  (best train acc: 0.8294, best val acc: 0.8287, best train loss: 0.3952  @ epoch 19143 )\n",
      "[Epoch: 20220] train loss: 0.4111, train acc: 0.8185, val loss: 0.4211, val acc: 0.8128  (best train acc: 0.8294, best val acc: 0.8287, best train loss: 0.3952  @ epoch 19143 )\n",
      "[Epoch: 20240] train loss: 0.4462, train acc: 0.8017, val loss: 0.4534, val acc: 0.7909  (best train acc: 0.8294, best val acc: 0.8287, best train loss: 0.3952  @ epoch 19143 )\n",
      "[Epoch: 20260] train loss: 0.4123, train acc: 0.8178, val loss: 0.4293, val acc: 0.8013  (best train acc: 0.8294, best val acc: 0.8287, best train loss: 0.3952  @ epoch 19143 )\n",
      "[Epoch: 20280] train loss: 0.4181, train acc: 0.8206, val loss: 0.4366, val acc: 0.7946  (best train acc: 0.8294, best val acc: 0.8287, best train loss: 0.3952  @ epoch 19143 )\n",
      "[Epoch: 20300] train loss: 0.4023, train acc: 0.8188, val loss: 0.4273, val acc: 0.8078  (best train acc: 0.8294, best val acc: 0.8287, best train loss: 0.3910  @ epoch 20292 )\n",
      "[Epoch: 20320] train loss: 0.4271, train acc: 0.8031, val loss: 0.4514, val acc: 0.7953  (best train acc: 0.8294, best val acc: 0.8287, best train loss: 0.3910  @ epoch 20292 )\n",
      "[Epoch: 20340] train loss: 0.4230, train acc: 0.8122, val loss: 0.4187, val acc: 0.8135  (best train acc: 0.8294, best val acc: 0.8287, best train loss: 0.3910  @ epoch 20292 )\n",
      "[Epoch: 20360] train loss: 0.4151, train acc: 0.8128, val loss: 0.4241, val acc: 0.8121  (best train acc: 0.8294, best val acc: 0.8287, best train loss: 0.3910  @ epoch 20292 )\n",
      "[Epoch: 20380] train loss: 0.4060, train acc: 0.8169, val loss: 0.4206, val acc: 0.8101  (best train acc: 0.8294, best val acc: 0.8287, best train loss: 0.3910  @ epoch 20292 )\n",
      "[Epoch: 20400] train loss: 0.4112, train acc: 0.8148, val loss: 0.4213, val acc: 0.8111  (best train acc: 0.8294, best val acc: 0.8287, best train loss: 0.3910  @ epoch 20292 )\n",
      "[Epoch: 20420] train loss: 0.4317, train acc: 0.8095, val loss: 0.4216, val acc: 0.8088  (best train acc: 0.8294, best val acc: 0.8287, best train loss: 0.3910  @ epoch 20292 )\n",
      "[Epoch: 20440] train loss: 0.4053, train acc: 0.8179, val loss: 0.4292, val acc: 0.8020  (best train acc: 0.8294, best val acc: 0.8287, best train loss: 0.3910  @ epoch 20292 )\n",
      "[Epoch: 20460] train loss: 0.4295, train acc: 0.8104, val loss: 0.4384, val acc: 0.7966  (best train acc: 0.8294, best val acc: 0.8287, best train loss: 0.3910  @ epoch 20292 )\n",
      "[Epoch: 20480] train loss: 0.4276, train acc: 0.8102, val loss: 0.4325, val acc: 0.8057  (best train acc: 0.8294, best val acc: 0.8287, best train loss: 0.3910  @ epoch 20292 )\n",
      "[Epoch: 20500] train loss: 0.4114, train acc: 0.8183, val loss: 0.4372, val acc: 0.7956  (best train acc: 0.8294, best val acc: 0.8287, best train loss: 0.3910  @ epoch 20292 )\n",
      "[Epoch: 20520] train loss: 0.4171, train acc: 0.8166, val loss: 0.4398, val acc: 0.7926  (best train acc: 0.8294, best val acc: 0.8287, best train loss: 0.3910  @ epoch 20292 )\n",
      "[Epoch: 20540] train loss: 0.4148, train acc: 0.8135, val loss: 0.4212, val acc: 0.8101  (best train acc: 0.8294, best val acc: 0.8287, best train loss: 0.3910  @ epoch 20292 )\n",
      "[Epoch: 20560] train loss: 0.3993, train acc: 0.8248, val loss: 0.4323, val acc: 0.8067  (best train acc: 0.8294, best val acc: 0.8287, best train loss: 0.3910  @ epoch 20292 )\n",
      "[Epoch: 20580] train loss: 0.4211, train acc: 0.8105, val loss: 0.4299, val acc: 0.8003  (best train acc: 0.8294, best val acc: 0.8287, best train loss: 0.3910  @ epoch 20292 )\n",
      "[Epoch: 20600] train loss: 0.4210, train acc: 0.8142, val loss: 0.4314, val acc: 0.8024  (best train acc: 0.8294, best val acc: 0.8287, best train loss: 0.3910  @ epoch 20292 )\n",
      "[Epoch: 20620] train loss: 0.4259, train acc: 0.8096, val loss: 0.4605, val acc: 0.7794  (best train acc: 0.8294, best val acc: 0.8287, best train loss: 0.3910  @ epoch 20292 )\n",
      "[Epoch: 20640] train loss: 0.3981, train acc: 0.8226, val loss: 0.4218, val acc: 0.8044  (best train acc: 0.8294, best val acc: 0.8287, best train loss: 0.3910  @ epoch 20292 )\n",
      "[Epoch: 20660] train loss: 0.4053, train acc: 0.8215, val loss: 0.4190, val acc: 0.8145  (best train acc: 0.8294, best val acc: 0.8287, best train loss: 0.3910  @ epoch 20292 )\n",
      "[Epoch: 20680] train loss: 0.4248, train acc: 0.8128, val loss: 0.4325, val acc: 0.8091  (best train acc: 0.8294, best val acc: 0.8287, best train loss: 0.3910  @ epoch 20292 )\n",
      "[Epoch: 20700] train loss: 0.4208, train acc: 0.8122, val loss: 0.4382, val acc: 0.7929  (best train acc: 0.8294, best val acc: 0.8287, best train loss: 0.3910  @ epoch 20292 )\n",
      "[Epoch: 20720] train loss: 0.4351, train acc: 0.8042, val loss: 0.4520, val acc: 0.7916  (best train acc: 0.8294, best val acc: 0.8287, best train loss: 0.3910  @ epoch 20292 )\n",
      "[Epoch: 20740] train loss: 0.4352, train acc: 0.8061, val loss: 0.4387, val acc: 0.7963  (best train acc: 0.8294, best val acc: 0.8287, best train loss: 0.3910  @ epoch 20292 )\n",
      "[Epoch: 20760] train loss: 0.4119, train acc: 0.8251, val loss: 0.4343, val acc: 0.8064  (best train acc: 0.8294, best val acc: 0.8287, best train loss: 0.3910  @ epoch 20292 )\n",
      "[Epoch: 20780] train loss: 0.4131, train acc: 0.8152, val loss: 0.4435, val acc: 0.7987  (best train acc: 0.8294, best val acc: 0.8287, best train loss: 0.3910  @ epoch 20292 )\n",
      "[Epoch: 20800] train loss: 0.4282, train acc: 0.8111, val loss: 0.4245, val acc: 0.8084  (best train acc: 0.8294, best val acc: 0.8287, best train loss: 0.3910  @ epoch 20292 )\n",
      "[Epoch: 20820] train loss: 0.4234, train acc: 0.8115, val loss: 0.4300, val acc: 0.8057  (best train acc: 0.8294, best val acc: 0.8287, best train loss: 0.3910  @ epoch 20292 )\n",
      "[Epoch: 20840] train loss: 0.4250, train acc: 0.8096, val loss: 0.4409, val acc: 0.7902  (best train acc: 0.8294, best val acc: 0.8287, best train loss: 0.3910  @ epoch 20292 )\n",
      "[Epoch: 20860] train loss: 0.3987, train acc: 0.8258, val loss: 0.4377, val acc: 0.7983  (best train acc: 0.8294, best val acc: 0.8287, best train loss: 0.3910  @ epoch 20292 )\n",
      "[Epoch: 20880] train loss: 0.4122, train acc: 0.8167, val loss: 0.4356, val acc: 0.7960  (best train acc: 0.8294, best val acc: 0.8287, best train loss: 0.3910  @ epoch 20292 )\n",
      "[Epoch: 20900] train loss: 0.4097, train acc: 0.8164, val loss: 0.4230, val acc: 0.8159  (best train acc: 0.8294, best val acc: 0.8317, best train loss: 0.3910  @ epoch 20292 )\n",
      "[Epoch: 20920] train loss: 0.4150, train acc: 0.8135, val loss: 0.4368, val acc: 0.7933  (best train acc: 0.8294, best val acc: 0.8317, best train loss: 0.3910  @ epoch 20292 )\n",
      "[Epoch: 20940] train loss: 0.4246, train acc: 0.8112, val loss: 0.4160, val acc: 0.8216  (best train acc: 0.8294, best val acc: 0.8317, best train loss: 0.3910  @ epoch 20292 )\n",
      "[Epoch: 20960] train loss: 0.4144, train acc: 0.8121, val loss: 0.4296, val acc: 0.8051  (best train acc: 0.8294, best val acc: 0.8317, best train loss: 0.3910  @ epoch 20292 )\n",
      "[Epoch: 20980] train loss: 0.4180, train acc: 0.8113, val loss: 0.4396, val acc: 0.7949  (best train acc: 0.8294, best val acc: 0.8317, best train loss: 0.3910  @ epoch 20292 )\n",
      "[Epoch: 21000] train loss: 0.4243, train acc: 0.8071, val loss: 0.4354, val acc: 0.8034  (best train acc: 0.8294, best val acc: 0.8317, best train loss: 0.3910  @ epoch 20292 )\n",
      "[Epoch: 21020] train loss: 0.4270, train acc: 0.8122, val loss: 0.4367, val acc: 0.7987  (best train acc: 0.8294, best val acc: 0.8317, best train loss: 0.3910  @ epoch 20292 )\n",
      "[Epoch: 21040] train loss: 0.4138, train acc: 0.8159, val loss: 0.4232, val acc: 0.8138  (best train acc: 0.8294, best val acc: 0.8317, best train loss: 0.3910  @ epoch 20292 )\n",
      "[Epoch: 21060] train loss: 0.3929, train acc: 0.8237, val loss: 0.4317, val acc: 0.8000  (best train acc: 0.8294, best val acc: 0.8317, best train loss: 0.3910  @ epoch 20292 )\n",
      "[Epoch: 21080] train loss: 0.4121, train acc: 0.8145, val loss: 0.4282, val acc: 0.8088  (best train acc: 0.8294, best val acc: 0.8317, best train loss: 0.3910  @ epoch 20292 )\n",
      "[Epoch: 21100] train loss: 0.4098, train acc: 0.8138, val loss: 0.4343, val acc: 0.7993  (best train acc: 0.8294, best val acc: 0.8317, best train loss: 0.3910  @ epoch 20292 )\n",
      "[Epoch: 21120] train loss: 0.4078, train acc: 0.8162, val loss: 0.4412, val acc: 0.7943  (best train acc: 0.8294, best val acc: 0.8317, best train loss: 0.3910  @ epoch 20292 )\n",
      "[Epoch: 21140] train loss: 0.4035, train acc: 0.8222, val loss: 0.4287, val acc: 0.8027  (best train acc: 0.8294, best val acc: 0.8317, best train loss: 0.3910  @ epoch 20292 )\n",
      "[Epoch: 21160] train loss: 0.4310, train acc: 0.8102, val loss: 0.4836, val acc: 0.7622  (best train acc: 0.8294, best val acc: 0.8317, best train loss: 0.3910  @ epoch 20292 )\n",
      "[Epoch: 21180] train loss: 0.4110, train acc: 0.8135, val loss: 0.4339, val acc: 0.7929  (best train acc: 0.8294, best val acc: 0.8317, best train loss: 0.3910  @ epoch 20292 )\n",
      "[Epoch: 21200] train loss: 0.4183, train acc: 0.8125, val loss: 0.4297, val acc: 0.8074  (best train acc: 0.8294, best val acc: 0.8317, best train loss: 0.3910  @ epoch 20292 )\n",
      "[Epoch: 21220] train loss: 0.4115, train acc: 0.8209, val loss: 0.4233, val acc: 0.8202  (best train acc: 0.8294, best val acc: 0.8317, best train loss: 0.3910  @ epoch 20292 )\n",
      "[Epoch: 21240] train loss: 0.4354, train acc: 0.8075, val loss: 0.4355, val acc: 0.8007  (best train acc: 0.8294, best val acc: 0.8317, best train loss: 0.3910  @ epoch 20292 )\n",
      "[Epoch: 21260] train loss: 0.4087, train acc: 0.8189, val loss: 0.4199, val acc: 0.8175  (best train acc: 0.8294, best val acc: 0.8317, best train loss: 0.3910  @ epoch 20292 )\n",
      "[Epoch: 21280] train loss: 0.4215, train acc: 0.8133, val loss: 0.4312, val acc: 0.7956  (best train acc: 0.8294, best val acc: 0.8317, best train loss: 0.3910  @ epoch 20292 )\n",
      "[Epoch: 21300] train loss: 0.4158, train acc: 0.8144, val loss: 0.4320, val acc: 0.7943  (best train acc: 0.8294, best val acc: 0.8317, best train loss: 0.3910  @ epoch 20292 )\n",
      "[Epoch: 21320] train loss: 0.4063, train acc: 0.8205, val loss: 0.4357, val acc: 0.7976  (best train acc: 0.8294, best val acc: 0.8317, best train loss: 0.3910  @ epoch 20292 )\n",
      "[Epoch: 21340] train loss: 0.4025, train acc: 0.8200, val loss: 0.4269, val acc: 0.8098  (best train acc: 0.8294, best val acc: 0.8317, best train loss: 0.3910  @ epoch 20292 )\n",
      "[Epoch: 21360] train loss: 0.4240, train acc: 0.8135, val loss: 0.4295, val acc: 0.8159  (best train acc: 0.8294, best val acc: 0.8317, best train loss: 0.3910  @ epoch 20292 )\n",
      "[Epoch: 21380] train loss: 0.4145, train acc: 0.8148, val loss: 0.4318, val acc: 0.7976  (best train acc: 0.8294, best val acc: 0.8317, best train loss: 0.3910  @ epoch 20292 )\n",
      "[Epoch: 21400] train loss: 0.4116, train acc: 0.8165, val loss: 0.4134, val acc: 0.8202  (best train acc: 0.8294, best val acc: 0.8317, best train loss: 0.3910  @ epoch 20292 )\n",
      "[Epoch: 21420] train loss: 0.4036, train acc: 0.8196, val loss: 0.4429, val acc: 0.7919  (best train acc: 0.8294, best val acc: 0.8317, best train loss: 0.3910  @ epoch 20292 )\n",
      "[Epoch: 21440] train loss: 0.4078, train acc: 0.8232, val loss: 0.4575, val acc: 0.7784  (best train acc: 0.8294, best val acc: 0.8317, best train loss: 0.3910  @ epoch 20292 )\n",
      "[Epoch: 21460] train loss: 0.4115, train acc: 0.8130, val loss: 0.4218, val acc: 0.8084  (best train acc: 0.8294, best val acc: 0.8317, best train loss: 0.3910  @ epoch 20292 )\n",
      "[Epoch: 21480] train loss: 0.4107, train acc: 0.8165, val loss: 0.4408, val acc: 0.7939  (best train acc: 0.8294, best val acc: 0.8317, best train loss: 0.3910  @ epoch 20292 )\n",
      "[Epoch: 21500] train loss: 0.4145, train acc: 0.8189, val loss: 0.4350, val acc: 0.7926  (best train acc: 0.8294, best val acc: 0.8317, best train loss: 0.3910  @ epoch 20292 )\n",
      "[Epoch: 21520] train loss: 0.4083, train acc: 0.8205, val loss: 0.4238, val acc: 0.8084  (best train acc: 0.8294, best val acc: 0.8317, best train loss: 0.3910  @ epoch 20292 )\n",
      "[Epoch: 21540] train loss: 0.4216, train acc: 0.8120, val loss: 0.4330, val acc: 0.7976  (best train acc: 0.8294, best val acc: 0.8317, best train loss: 0.3910  @ epoch 20292 )\n",
      "[Epoch: 21560] train loss: 0.4040, train acc: 0.8167, val loss: 0.4214, val acc: 0.8142  (best train acc: 0.8294, best val acc: 0.8317, best train loss: 0.3910  @ epoch 20292 )\n",
      "[Epoch: 21580] train loss: 0.4091, train acc: 0.8190, val loss: 0.4315, val acc: 0.8061  (best train acc: 0.8294, best val acc: 0.8317, best train loss: 0.3910  @ epoch 20292 )\n",
      "[Epoch: 21600] train loss: 0.4059, train acc: 0.8181, val loss: 0.4300, val acc: 0.8037  (best train acc: 0.8294, best val acc: 0.8317, best train loss: 0.3910  @ epoch 20292 )\n",
      "[Epoch: 21620] train loss: 0.4222, train acc: 0.8095, val loss: 0.4233, val acc: 0.8094  (best train acc: 0.8294, best val acc: 0.8317, best train loss: 0.3910  @ epoch 20292 )\n",
      "[Epoch: 21640] train loss: 0.4161, train acc: 0.8132, val loss: 0.4141, val acc: 0.8216  (best train acc: 0.8294, best val acc: 0.8317, best train loss: 0.3910  @ epoch 20292 )\n",
      "[Epoch: 21660] train loss: 0.4041, train acc: 0.8217, val loss: 0.4122, val acc: 0.8172  (best train acc: 0.8294, best val acc: 0.8317, best train loss: 0.3910  @ epoch 20292 )\n",
      "[Epoch: 21680] train loss: 0.4115, train acc: 0.8158, val loss: 0.4199, val acc: 0.8179  (best train acc: 0.8294, best val acc: 0.8317, best train loss: 0.3910  @ epoch 20292 )\n",
      "[Epoch: 21700] train loss: 0.4100, train acc: 0.8159, val loss: 0.4279, val acc: 0.8030  (best train acc: 0.8294, best val acc: 0.8317, best train loss: 0.3910  @ epoch 20292 )\n",
      "[Epoch: 21720] train loss: 0.3994, train acc: 0.8224, val loss: 0.4334, val acc: 0.8047  (best train acc: 0.8294, best val acc: 0.8317, best train loss: 0.3910  @ epoch 20292 )\n",
      "[Epoch: 21740] train loss: 0.4126, train acc: 0.8156, val loss: 0.4160, val acc: 0.8206  (best train acc: 0.8294, best val acc: 0.8317, best train loss: 0.3910  @ epoch 20292 )\n",
      "[Epoch: 21760] train loss: 0.4051, train acc: 0.8197, val loss: 0.4187, val acc: 0.8135  (best train acc: 0.8294, best val acc: 0.8317, best train loss: 0.3910  @ epoch 20292 )\n",
      "[Epoch: 21780] train loss: 0.3970, train acc: 0.8239, val loss: 0.4167, val acc: 0.8229  (best train acc: 0.8294, best val acc: 0.8317, best train loss: 0.3910  @ epoch 20292 )\n",
      "[Epoch: 21800] train loss: 0.4052, train acc: 0.8191, val loss: 0.4408, val acc: 0.7902  (best train acc: 0.8294, best val acc: 0.8317, best train loss: 0.3910  @ epoch 20292 )\n",
      "[Epoch: 21820] train loss: 0.4021, train acc: 0.8232, val loss: 0.4209, val acc: 0.8132  (best train acc: 0.8294, best val acc: 0.8317, best train loss: 0.3910  @ epoch 20292 )\n",
      "[Epoch: 21840] train loss: 0.4015, train acc: 0.8203, val loss: 0.4285, val acc: 0.8064  (best train acc: 0.8294, best val acc: 0.8317, best train loss: 0.3910  @ epoch 20292 )\n",
      "[Epoch: 21860] train loss: 0.4086, train acc: 0.8177, val loss: 0.4285, val acc: 0.8020  (best train acc: 0.8296, best val acc: 0.8317, best train loss: 0.3910  @ epoch 20292 )\n",
      "[Epoch: 21880] train loss: 0.4074, train acc: 0.8186, val loss: 0.4276, val acc: 0.8051  (best train acc: 0.8296, best val acc: 0.8317, best train loss: 0.3910  @ epoch 20292 )\n",
      "[Epoch: 21900] train loss: 0.3994, train acc: 0.8241, val loss: 0.4277, val acc: 0.8091  (best train acc: 0.8296, best val acc: 0.8317, best train loss: 0.3910  @ epoch 20292 )\n",
      "[Epoch: 21920] train loss: 0.4409, train acc: 0.8080, val loss: 0.4339, val acc: 0.8064  (best train acc: 0.8296, best val acc: 0.8317, best train loss: 0.3910  @ epoch 20292 )\n",
      "[Epoch: 21940] train loss: 0.4072, train acc: 0.8200, val loss: 0.4485, val acc: 0.7983  (best train acc: 0.8296, best val acc: 0.8317, best train loss: 0.3910  @ epoch 20292 )\n",
      "[Epoch: 21960] train loss: 0.4184, train acc: 0.8151, val loss: 0.4537, val acc: 0.7831  (best train acc: 0.8296, best val acc: 0.8317, best train loss: 0.3910  @ epoch 20292 )\n",
      "[Epoch: 21980] train loss: 0.3996, train acc: 0.8226, val loss: 0.4483, val acc: 0.7875  (best train acc: 0.8296, best val acc: 0.8317, best train loss: 0.3910  @ epoch 20292 )\n",
      "[Epoch: 22000] train loss: 0.4380, train acc: 0.8121, val loss: 0.4488, val acc: 0.7791  (best train acc: 0.8296, best val acc: 0.8317, best train loss: 0.3910  @ epoch 20292 )\n",
      "[Epoch: 22020] train loss: 0.4213, train acc: 0.8172, val loss: 0.4291, val acc: 0.8108  (best train acc: 0.8296, best val acc: 0.8317, best train loss: 0.3910  @ epoch 20292 )\n",
      "[Epoch: 22040] train loss: 0.4081, train acc: 0.8205, val loss: 0.4508, val acc: 0.7862  (best train acc: 0.8296, best val acc: 0.8317, best train loss: 0.3910  @ epoch 20292 )\n",
      "[Epoch: 22060] train loss: 0.4127, train acc: 0.8177, val loss: 0.4226, val acc: 0.8101  (best train acc: 0.8296, best val acc: 0.8331, best train loss: 0.3910  @ epoch 20292 )\n",
      "[Epoch: 22080] train loss: 0.4009, train acc: 0.8174, val loss: 0.4238, val acc: 0.8017  (best train acc: 0.8296, best val acc: 0.8331, best train loss: 0.3910  @ epoch 20292 )\n",
      "[Epoch: 22100] train loss: 0.4255, train acc: 0.8146, val loss: 0.4246, val acc: 0.8081  (best train acc: 0.8296, best val acc: 0.8331, best train loss: 0.3901  @ epoch 22095 )\n",
      "[Epoch: 22120] train loss: 0.3995, train acc: 0.8258, val loss: 0.4194, val acc: 0.8118  (best train acc: 0.8296, best val acc: 0.8331, best train loss: 0.3901  @ epoch 22095 )\n",
      "[Epoch: 22140] train loss: 0.4054, train acc: 0.8198, val loss: 0.4256, val acc: 0.8034  (best train acc: 0.8296, best val acc: 0.8331, best train loss: 0.3884  @ epoch 22123 )\n",
      "[Epoch: 22160] train loss: 0.4172, train acc: 0.8139, val loss: 0.4283, val acc: 0.8037  (best train acc: 0.8296, best val acc: 0.8331, best train loss: 0.3884  @ epoch 22123 )\n",
      "[Epoch: 22180] train loss: 0.4009, train acc: 0.8204, val loss: 0.4151, val acc: 0.8138  (best train acc: 0.8296, best val acc: 0.8331, best train loss: 0.3884  @ epoch 22123 )\n",
      "[Epoch: 22200] train loss: 0.4229, train acc: 0.8094, val loss: 0.4163, val acc: 0.8061  (best train acc: 0.8296, best val acc: 0.8331, best train loss: 0.3884  @ epoch 22123 )\n",
      "[Epoch: 22220] train loss: 0.4183, train acc: 0.8146, val loss: 0.4105, val acc: 0.8155  (best train acc: 0.8296, best val acc: 0.8331, best train loss: 0.3884  @ epoch 22123 )\n",
      "[Epoch: 22240] train loss: 0.3972, train acc: 0.8246, val loss: 0.4091, val acc: 0.8179  (best train acc: 0.8296, best val acc: 0.8331, best train loss: 0.3884  @ epoch 22123 )\n",
      "[Epoch: 22260] train loss: 0.4051, train acc: 0.8205, val loss: 0.4264, val acc: 0.8064  (best train acc: 0.8328, best val acc: 0.8331, best train loss: 0.3846  @ epoch 22242 )\n",
      "[Epoch: 22280] train loss: 0.3946, train acc: 0.8237, val loss: 0.4239, val acc: 0.8084  (best train acc: 0.8328, best val acc: 0.8331, best train loss: 0.3846  @ epoch 22242 )\n",
      "[Epoch: 22300] train loss: 0.4078, train acc: 0.8238, val loss: 0.4197, val acc: 0.8091  (best train acc: 0.8328, best val acc: 0.8331, best train loss: 0.3846  @ epoch 22242 )\n",
      "[Epoch: 22320] train loss: 0.4273, train acc: 0.8083, val loss: 0.4267, val acc: 0.8067  (best train acc: 0.8328, best val acc: 0.8331, best train loss: 0.3846  @ epoch 22242 )\n",
      "[Epoch: 22340] train loss: 0.4104, train acc: 0.8227, val loss: 0.4378, val acc: 0.7933  (best train acc: 0.8328, best val acc: 0.8331, best train loss: 0.3846  @ epoch 22242 )\n",
      "[Epoch: 22360] train loss: 0.3868, train acc: 0.8301, val loss: 0.4389, val acc: 0.7899  (best train acc: 0.8328, best val acc: 0.8331, best train loss: 0.3846  @ epoch 22242 )\n",
      "[Epoch: 22380] train loss: 0.4133, train acc: 0.8130, val loss: 0.4254, val acc: 0.8071  (best train acc: 0.8328, best val acc: 0.8331, best train loss: 0.3846  @ epoch 22242 )\n",
      "[Epoch: 22400] train loss: 0.4032, train acc: 0.8220, val loss: 0.4078, val acc: 0.8226  (best train acc: 0.8328, best val acc: 0.8331, best train loss: 0.3846  @ epoch 22242 )\n",
      "[Epoch: 22420] train loss: 0.3950, train acc: 0.8266, val loss: 0.4202, val acc: 0.8081  (best train acc: 0.8328, best val acc: 0.8331, best train loss: 0.3846  @ epoch 22242 )\n",
      "[Epoch: 22440] train loss: 0.4187, train acc: 0.8135, val loss: 0.4294, val acc: 0.8084  (best train acc: 0.8328, best val acc: 0.8331, best train loss: 0.3846  @ epoch 22242 )\n",
      "[Epoch: 22460] train loss: 0.3924, train acc: 0.8295, val loss: 0.4319, val acc: 0.8057  (best train acc: 0.8328, best val acc: 0.8331, best train loss: 0.3846  @ epoch 22242 )\n",
      "[Epoch: 22480] train loss: 0.4001, train acc: 0.8280, val loss: 0.4270, val acc: 0.8061  (best train acc: 0.8328, best val acc: 0.8331, best train loss: 0.3846  @ epoch 22242 )\n",
      "[Epoch: 22500] train loss: 0.3966, train acc: 0.8238, val loss: 0.4190, val acc: 0.8192  (best train acc: 0.8328, best val acc: 0.8331, best train loss: 0.3811  @ epoch 22493 )\n",
      "[Epoch: 22520] train loss: 0.3859, train acc: 0.8287, val loss: 0.4264, val acc: 0.8098  (best train acc: 0.8328, best val acc: 0.8331, best train loss: 0.3811  @ epoch 22493 )\n",
      "[Epoch: 22540] train loss: 0.4013, train acc: 0.8243, val loss: 0.4342, val acc: 0.7980  (best train acc: 0.8328, best val acc: 0.8331, best train loss: 0.3811  @ epoch 22493 )\n",
      "[Epoch: 22560] train loss: 0.4180, train acc: 0.8166, val loss: 0.4122, val acc: 0.8189  (best train acc: 0.8328, best val acc: 0.8331, best train loss: 0.3811  @ epoch 22493 )\n",
      "[Epoch: 22580] train loss: 0.3925, train acc: 0.8232, val loss: 0.4503, val acc: 0.7825  (best train acc: 0.8328, best val acc: 0.8331, best train loss: 0.3811  @ epoch 22493 )\n",
      "[Epoch: 22600] train loss: 0.4067, train acc: 0.8269, val loss: 0.4299, val acc: 0.7997  (best train acc: 0.8328, best val acc: 0.8331, best train loss: 0.3811  @ epoch 22493 )\n",
      "[Epoch: 22620] train loss: 0.4129, train acc: 0.8172, val loss: 0.4405, val acc: 0.7983  (best train acc: 0.8328, best val acc: 0.8331, best train loss: 0.3811  @ epoch 22493 )\n",
      "[Epoch: 22640] train loss: 0.3923, train acc: 0.8292, val loss: 0.4461, val acc: 0.7879  (best train acc: 0.8328, best val acc: 0.8331, best train loss: 0.3811  @ epoch 22493 )\n",
      "[Epoch: 22660] train loss: 0.3882, train acc: 0.8304, val loss: 0.4175, val acc: 0.8165  (best train acc: 0.8328, best val acc: 0.8331, best train loss: 0.3811  @ epoch 22493 )\n",
      "[Epoch: 22680] train loss: 0.3937, train acc: 0.8224, val loss: 0.4100, val acc: 0.8236  (best train acc: 0.8328, best val acc: 0.8331, best train loss: 0.3806  @ epoch 22662 )\n",
      "[Epoch: 22700] train loss: 0.4002, train acc: 0.8225, val loss: 0.4232, val acc: 0.8074  (best train acc: 0.8328, best val acc: 0.8331, best train loss: 0.3806  @ epoch 22662 )\n",
      "[Epoch: 22720] train loss: 0.3856, train acc: 0.8302, val loss: 0.4291, val acc: 0.7990  (best train acc: 0.8328, best val acc: 0.8331, best train loss: 0.3806  @ epoch 22662 )\n",
      "[Epoch: 22740] train loss: 0.4453, train acc: 0.8044, val loss: 0.4476, val acc: 0.7788  (best train acc: 0.8328, best val acc: 0.8331, best train loss: 0.3794  @ epoch 22726 )\n",
      "[Epoch: 22760] train loss: 0.4073, train acc: 0.8186, val loss: 0.4382, val acc: 0.7933  (best train acc: 0.8328, best val acc: 0.8331, best train loss: 0.3794  @ epoch 22726 )\n",
      "[Epoch: 22780] train loss: 0.4252, train acc: 0.8140, val loss: 0.4390, val acc: 0.7960  (best train acc: 0.8328, best val acc: 0.8331, best train loss: 0.3794  @ epoch 22726 )\n",
      "[Epoch: 22800] train loss: 0.3925, train acc: 0.8247, val loss: 0.4185, val acc: 0.8155  (best train acc: 0.8328, best val acc: 0.8331, best train loss: 0.3794  @ epoch 22726 )\n",
      "[Epoch: 22820] train loss: 0.3896, train acc: 0.8258, val loss: 0.4195, val acc: 0.8189  (best train acc: 0.8328, best val acc: 0.8331, best train loss: 0.3794  @ epoch 22726 )\n",
      "[Epoch: 22840] train loss: 0.4012, train acc: 0.8255, val loss: 0.4208, val acc: 0.8132  (best train acc: 0.8328, best val acc: 0.8331, best train loss: 0.3794  @ epoch 22726 )\n",
      "[Epoch: 22860] train loss: 0.3938, train acc: 0.8271, val loss: 0.4145, val acc: 0.8148  (best train acc: 0.8328, best val acc: 0.8331, best train loss: 0.3794  @ epoch 22726 )\n",
      "[Epoch: 22880] train loss: 0.4122, train acc: 0.8141, val loss: 0.4157, val acc: 0.8212  (best train acc: 0.8328, best val acc: 0.8331, best train loss: 0.3794  @ epoch 22726 )\n",
      "[Epoch: 22900] train loss: 0.3969, train acc: 0.8234, val loss: 0.4199, val acc: 0.8128  (best train acc: 0.8334, best val acc: 0.8331, best train loss: 0.3794  @ epoch 22726 )\n",
      "[Epoch: 22920] train loss: 0.3972, train acc: 0.8227, val loss: 0.4153, val acc: 0.8081  (best train acc: 0.8334, best val acc: 0.8331, best train loss: 0.3794  @ epoch 22726 )\n",
      "[Epoch: 22940] train loss: 0.4093, train acc: 0.8176, val loss: 0.4250, val acc: 0.8071  (best train acc: 0.8334, best val acc: 0.8331, best train loss: 0.3794  @ epoch 22726 )\n",
      "[Epoch: 22960] train loss: 0.3952, train acc: 0.8244, val loss: 0.4226, val acc: 0.8152  (best train acc: 0.8334, best val acc: 0.8331, best train loss: 0.3794  @ epoch 22726 )\n",
      "[Epoch: 22980] train loss: 0.3903, train acc: 0.8269, val loss: 0.4199, val acc: 0.8091  (best train acc: 0.8363, best val acc: 0.8331, best train loss: 0.3764  @ epoch 22973 )\n",
      "[Epoch: 23000] train loss: 0.4044, train acc: 0.8248, val loss: 0.4132, val acc: 0.8101  (best train acc: 0.8363, best val acc: 0.8331, best train loss: 0.3764  @ epoch 22973 )\n",
      "[Epoch: 23020] train loss: 0.3856, train acc: 0.8287, val loss: 0.4288, val acc: 0.8034  (best train acc: 0.8363, best val acc: 0.8331, best train loss: 0.3764  @ epoch 22973 )\n",
      "[Epoch: 23040] train loss: 0.3920, train acc: 0.8242, val loss: 0.4145, val acc: 0.8108  (best train acc: 0.8363, best val acc: 0.8331, best train loss: 0.3764  @ epoch 22973 )\n",
      "[Epoch: 23060] train loss: 0.4262, train acc: 0.8146, val loss: 0.4390, val acc: 0.7882  (best train acc: 0.8363, best val acc: 0.8331, best train loss: 0.3764  @ epoch 22973 )\n",
      "[Epoch: 23080] train loss: 0.4193, train acc: 0.8139, val loss: 0.4251, val acc: 0.8044  (best train acc: 0.8363, best val acc: 0.8331, best train loss: 0.3764  @ epoch 22973 )\n",
      "[Epoch: 23100] train loss: 0.3990, train acc: 0.8199, val loss: 0.4207, val acc: 0.8108  (best train acc: 0.8363, best val acc: 0.8331, best train loss: 0.3764  @ epoch 22973 )\n",
      "[Epoch: 23120] train loss: 0.4188, train acc: 0.8149, val loss: 0.4254, val acc: 0.8128  (best train acc: 0.8363, best val acc: 0.8331, best train loss: 0.3764  @ epoch 22973 )\n",
      "[Epoch: 23140] train loss: 0.4070, train acc: 0.8151, val loss: 0.4132, val acc: 0.8159  (best train acc: 0.8363, best val acc: 0.8331, best train loss: 0.3764  @ epoch 22973 )\n",
      "[Epoch: 23160] train loss: 0.4131, train acc: 0.8180, val loss: 0.4320, val acc: 0.7960  (best train acc: 0.8363, best val acc: 0.8331, best train loss: 0.3764  @ epoch 22973 )\n",
      "[Epoch: 23180] train loss: 0.3883, train acc: 0.8322, val loss: 0.4076, val acc: 0.8216  (best train acc: 0.8363, best val acc: 0.8331, best train loss: 0.3764  @ epoch 22973 )\n",
      "[Epoch: 23200] train loss: 0.4202, train acc: 0.8112, val loss: 0.4127, val acc: 0.8142  (best train acc: 0.8363, best val acc: 0.8331, best train loss: 0.3764  @ epoch 22973 )\n",
      "[Epoch: 23220] train loss: 0.4064, train acc: 0.8180, val loss: 0.4145, val acc: 0.8138  (best train acc: 0.8363, best val acc: 0.8331, best train loss: 0.3764  @ epoch 22973 )\n",
      "[Epoch: 23240] train loss: 0.3996, train acc: 0.8226, val loss: 0.4305, val acc: 0.7990  (best train acc: 0.8363, best val acc: 0.8331, best train loss: 0.3764  @ epoch 22973 )\n",
      "[Epoch: 23260] train loss: 0.4010, train acc: 0.8187, val loss: 0.4305, val acc: 0.7993  (best train acc: 0.8363, best val acc: 0.8331, best train loss: 0.3764  @ epoch 22973 )\n",
      "[Epoch: 23280] train loss: 0.4122, train acc: 0.8169, val loss: 0.4310, val acc: 0.8020  (best train acc: 0.8363, best val acc: 0.8331, best train loss: 0.3764  @ epoch 22973 )\n",
      "[Epoch: 23300] train loss: 0.3871, train acc: 0.8271, val loss: 0.4334, val acc: 0.7973  (best train acc: 0.8363, best val acc: 0.8331, best train loss: 0.3764  @ epoch 22973 )\n",
      "[Epoch: 23320] train loss: 0.4038, train acc: 0.8206, val loss: 0.4251, val acc: 0.8030  (best train acc: 0.8363, best val acc: 0.8331, best train loss: 0.3764  @ epoch 22973 )\n",
      "[Epoch: 23340] train loss: 0.3986, train acc: 0.8236, val loss: 0.4267, val acc: 0.8084  (best train acc: 0.8363, best val acc: 0.8331, best train loss: 0.3764  @ epoch 22973 )\n",
      "[Epoch: 23360] train loss: 0.3928, train acc: 0.8277, val loss: 0.4101, val acc: 0.8138  (best train acc: 0.8363, best val acc: 0.8331, best train loss: 0.3764  @ epoch 22973 )\n",
      "[Epoch: 23380] train loss: 0.3949, train acc: 0.8216, val loss: 0.4583, val acc: 0.7720  (best train acc: 0.8363, best val acc: 0.8331, best train loss: 0.3764  @ epoch 22973 )\n",
      "[Epoch: 23400] train loss: 0.4008, train acc: 0.8179, val loss: 0.4193, val acc: 0.8159  (best train acc: 0.8363, best val acc: 0.8331, best train loss: 0.3764  @ epoch 22973 )\n",
      "[Epoch: 23420] train loss: 0.3965, train acc: 0.8229, val loss: 0.4122, val acc: 0.8192  (best train acc: 0.8363, best val acc: 0.8331, best train loss: 0.3764  @ epoch 22973 )\n",
      "[Epoch: 23440] train loss: 0.3933, train acc: 0.8215, val loss: 0.4042, val acc: 0.8182  (best train acc: 0.8363, best val acc: 0.8331, best train loss: 0.3764  @ epoch 22973 )\n",
      "[Epoch: 23460] train loss: 0.3860, train acc: 0.8330, val loss: 0.4083, val acc: 0.8223  (best train acc: 0.8363, best val acc: 0.8331, best train loss: 0.3764  @ epoch 22973 )\n",
      "[Epoch: 23480] train loss: 0.4108, train acc: 0.8162, val loss: 0.4253, val acc: 0.8037  (best train acc: 0.8363, best val acc: 0.8331, best train loss: 0.3764  @ epoch 22973 )\n",
      "[Epoch: 23500] train loss: 0.3885, train acc: 0.8279, val loss: 0.4149, val acc: 0.8128  (best train acc: 0.8363, best val acc: 0.8331, best train loss: 0.3764  @ epoch 22973 )\n",
      "[Epoch: 23520] train loss: 0.3870, train acc: 0.8276, val loss: 0.4181, val acc: 0.8078  (best train acc: 0.8363, best val acc: 0.8331, best train loss: 0.3764  @ epoch 22973 )\n",
      "[Epoch: 23540] train loss: 0.4231, train acc: 0.8122, val loss: 0.4364, val acc: 0.7970  (best train acc: 0.8363, best val acc: 0.8331, best train loss: 0.3764  @ epoch 22973 )\n",
      "[Epoch: 23560] train loss: 0.4043, train acc: 0.8198, val loss: 0.4255, val acc: 0.8196  (best train acc: 0.8363, best val acc: 0.8331, best train loss: 0.3764  @ epoch 22973 )\n",
      "[Epoch: 23580] train loss: 0.3797, train acc: 0.8305, val loss: 0.4086, val acc: 0.8256  (best train acc: 0.8363, best val acc: 0.8331, best train loss: 0.3764  @ epoch 22973 )\n",
      "[Epoch: 23600] train loss: 0.3928, train acc: 0.8269, val loss: 0.4162, val acc: 0.8155  (best train acc: 0.8363, best val acc: 0.8331, best train loss: 0.3764  @ epoch 22973 )\n",
      "[Epoch: 23620] train loss: 0.4057, train acc: 0.8151, val loss: 0.4101, val acc: 0.8138  (best train acc: 0.8363, best val acc: 0.8331, best train loss: 0.3764  @ epoch 22973 )\n",
      "[Epoch: 23640] train loss: 0.3893, train acc: 0.8269, val loss: 0.4155, val acc: 0.8121  (best train acc: 0.8363, best val acc: 0.8331, best train loss: 0.3764  @ epoch 22973 )\n",
      "[Epoch: 23660] train loss: 0.3868, train acc: 0.8290, val loss: 0.4056, val acc: 0.8182  (best train acc: 0.8363, best val acc: 0.8331, best train loss: 0.3764  @ epoch 22973 )\n",
      "[Epoch: 23680] train loss: 0.3861, train acc: 0.8277, val loss: 0.4096, val acc: 0.8189  (best train acc: 0.8363, best val acc: 0.8331, best train loss: 0.3764  @ epoch 22973 )\n",
      "[Epoch: 23700] train loss: 0.3987, train acc: 0.8259, val loss: 0.4171, val acc: 0.8115  (best train acc: 0.8363, best val acc: 0.8331, best train loss: 0.3764  @ epoch 22973 )\n",
      "[Epoch: 23720] train loss: 0.4076, train acc: 0.8190, val loss: 0.4155, val acc: 0.8121  (best train acc: 0.8363, best val acc: 0.8331, best train loss: 0.3764  @ epoch 22973 )\n",
      "[Epoch: 23740] train loss: 0.4117, train acc: 0.8154, val loss: 0.4145, val acc: 0.8152  (best train acc: 0.8363, best val acc: 0.8331, best train loss: 0.3764  @ epoch 22973 )\n",
      "[Epoch: 23760] train loss: 0.3968, train acc: 0.8196, val loss: 0.4360, val acc: 0.7899  (best train acc: 0.8363, best val acc: 0.8354, best train loss: 0.3764  @ epoch 22973 )\n",
      "[Epoch: 23780] train loss: 0.3857, train acc: 0.8271, val loss: 0.4065, val acc: 0.8236  (best train acc: 0.8363, best val acc: 0.8354, best train loss: 0.3764  @ epoch 22973 )\n",
      "[Epoch: 23800] train loss: 0.4006, train acc: 0.8220, val loss: 0.4052, val acc: 0.8260  (best train acc: 0.8363, best val acc: 0.8354, best train loss: 0.3764  @ epoch 22973 )\n",
      "[Epoch: 23820] train loss: 0.3852, train acc: 0.8310, val loss: 0.4168, val acc: 0.8094  (best train acc: 0.8363, best val acc: 0.8354, best train loss: 0.3764  @ epoch 22973 )\n",
      "[Epoch: 23840] train loss: 0.4071, train acc: 0.8201, val loss: 0.4325, val acc: 0.8034  (best train acc: 0.8363, best val acc: 0.8354, best train loss: 0.3764  @ epoch 22973 )\n",
      "[Epoch: 23860] train loss: 0.3992, train acc: 0.8229, val loss: 0.4191, val acc: 0.8098  (best train acc: 0.8363, best val acc: 0.8354, best train loss: 0.3764  @ epoch 22973 )\n",
      "[Epoch: 23880] train loss: 0.3865, train acc: 0.8284, val loss: 0.4271, val acc: 0.8007  (best train acc: 0.8363, best val acc: 0.8354, best train loss: 0.3764  @ epoch 22973 )\n",
      "[Epoch: 23900] train loss: 0.3997, train acc: 0.8210, val loss: 0.4124, val acc: 0.8185  (best train acc: 0.8363, best val acc: 0.8354, best train loss: 0.3764  @ epoch 22973 )\n",
      "[Epoch: 23920] train loss: 0.4001, train acc: 0.8263, val loss: 0.4204, val acc: 0.8152  (best train acc: 0.8363, best val acc: 0.8354, best train loss: 0.3764  @ epoch 22973 )\n",
      "[Epoch: 23940] train loss: 0.4037, train acc: 0.8242, val loss: 0.4254, val acc: 0.8064  (best train acc: 0.8363, best val acc: 0.8354, best train loss: 0.3764  @ epoch 22973 )\n",
      "[Epoch: 23960] train loss: 0.3978, train acc: 0.8246, val loss: 0.4198, val acc: 0.8101  (best train acc: 0.8363, best val acc: 0.8354, best train loss: 0.3764  @ epoch 22973 )\n",
      "[Epoch: 23980] train loss: 0.3816, train acc: 0.8306, val loss: 0.4154, val acc: 0.8175  (best train acc: 0.8363, best val acc: 0.8354, best train loss: 0.3764  @ epoch 22973 )\n",
      "[Epoch: 24000] train loss: 0.3871, train acc: 0.8293, val loss: 0.4117, val acc: 0.8196  (best train acc: 0.8363, best val acc: 0.8354, best train loss: 0.3764  @ epoch 22973 )\n",
      "[Epoch: 24020] train loss: 0.3833, train acc: 0.8300, val loss: 0.4210, val acc: 0.8071  (best train acc: 0.8363, best val acc: 0.8354, best train loss: 0.3764  @ epoch 22973 )\n",
      "[Epoch: 24040] train loss: 0.4084, train acc: 0.8146, val loss: 0.4425, val acc: 0.7936  (best train acc: 0.8363, best val acc: 0.8354, best train loss: 0.3758  @ epoch 24026 )\n",
      "[Epoch: 24060] train loss: 0.3881, train acc: 0.8288, val loss: 0.4447, val acc: 0.7916  (best train acc: 0.8363, best val acc: 0.8354, best train loss: 0.3758  @ epoch 24026 )\n",
      "[Epoch: 24080] train loss: 0.4323, train acc: 0.8062, val loss: 0.4209, val acc: 0.8054  (best train acc: 0.8363, best val acc: 0.8354, best train loss: 0.3736  @ epoch 24064 )\n",
      "[Epoch: 24100] train loss: 0.3876, train acc: 0.8301, val loss: 0.4186, val acc: 0.8118  (best train acc: 0.8363, best val acc: 0.8354, best train loss: 0.3736  @ epoch 24064 )\n",
      "[Epoch: 24120] train loss: 0.3833, train acc: 0.8286, val loss: 0.4166, val acc: 0.8111  (best train acc: 0.8363, best val acc: 0.8354, best train loss: 0.3736  @ epoch 24064 )\n",
      "[Epoch: 24140] train loss: 0.4083, train acc: 0.8175, val loss: 0.4181, val acc: 0.8138  (best train acc: 0.8363, best val acc: 0.8354, best train loss: 0.3736  @ epoch 24064 )\n",
      "[Epoch: 24160] train loss: 0.3885, train acc: 0.8277, val loss: 0.4245, val acc: 0.8067  (best train acc: 0.8363, best val acc: 0.8354, best train loss: 0.3736  @ epoch 24064 )\n",
      "[Epoch: 24180] train loss: 0.3876, train acc: 0.8271, val loss: 0.4076, val acc: 0.8155  (best train acc: 0.8363, best val acc: 0.8354, best train loss: 0.3736  @ epoch 24064 )\n",
      "[Epoch: 24200] train loss: 0.4076, train acc: 0.8164, val loss: 0.4236, val acc: 0.7970  (best train acc: 0.8363, best val acc: 0.8354, best train loss: 0.3736  @ epoch 24064 )\n",
      "[Epoch: 24220] train loss: 0.4014, train acc: 0.8230, val loss: 0.4261, val acc: 0.8064  (best train acc: 0.8363, best val acc: 0.8354, best train loss: 0.3736  @ epoch 24064 )\n",
      "[Epoch: 24240] train loss: 0.3921, train acc: 0.8285, val loss: 0.4150, val acc: 0.8152  (best train acc: 0.8363, best val acc: 0.8354, best train loss: 0.3736  @ epoch 24064 )\n",
      "[Epoch: 24260] train loss: 0.4349, train acc: 0.8082, val loss: 0.4223, val acc: 0.8108  (best train acc: 0.8363, best val acc: 0.8354, best train loss: 0.3736  @ epoch 24064 )\n",
      "[Epoch: 24280] train loss: 0.4097, train acc: 0.8191, val loss: 0.4365, val acc: 0.7990  (best train acc: 0.8363, best val acc: 0.8354, best train loss: 0.3736  @ epoch 24064 )\n",
      "[Epoch: 24300] train loss: 0.4078, train acc: 0.8150, val loss: 0.4239, val acc: 0.8037  (best train acc: 0.8363, best val acc: 0.8354, best train loss: 0.3736  @ epoch 24064 )\n",
      "[Epoch: 24320] train loss: 0.3858, train acc: 0.8308, val loss: 0.4142, val acc: 0.8159  (best train acc: 0.8363, best val acc: 0.8354, best train loss: 0.3736  @ epoch 24064 )\n",
      "[Epoch: 24340] train loss: 0.3786, train acc: 0.8326, val loss: 0.4104, val acc: 0.8159  (best train acc: 0.8363, best val acc: 0.8354, best train loss: 0.3736  @ epoch 24064 )\n",
      "[Epoch: 24360] train loss: 0.3894, train acc: 0.8280, val loss: 0.4144, val acc: 0.8138  (best train acc: 0.8363, best val acc: 0.8354, best train loss: 0.3736  @ epoch 24064 )\n",
      "[Epoch: 24380] train loss: 0.3888, train acc: 0.8273, val loss: 0.4052, val acc: 0.8223  (best train acc: 0.8363, best val acc: 0.8354, best train loss: 0.3736  @ epoch 24064 )\n",
      "[Epoch: 24400] train loss: 0.3827, train acc: 0.8308, val loss: 0.3998, val acc: 0.8229  (best train acc: 0.8363, best val acc: 0.8354, best train loss: 0.3736  @ epoch 24064 )\n",
      "[Epoch: 24420] train loss: 0.3936, train acc: 0.8251, val loss: 0.4075, val acc: 0.8206  (best train acc: 0.8363, best val acc: 0.8354, best train loss: 0.3736  @ epoch 24064 )\n",
      "[Epoch: 24440] train loss: 0.4001, train acc: 0.8175, val loss: 0.4238, val acc: 0.8081  (best train acc: 0.8363, best val acc: 0.8354, best train loss: 0.3736  @ epoch 24064 )\n",
      "[Epoch: 24460] train loss: 0.3988, train acc: 0.8250, val loss: 0.4182, val acc: 0.8084  (best train acc: 0.8363, best val acc: 0.8354, best train loss: 0.3700  @ epoch 24453 )\n",
      "[Epoch: 24480] train loss: 0.3946, train acc: 0.8219, val loss: 0.4079, val acc: 0.8236  (best train acc: 0.8363, best val acc: 0.8354, best train loss: 0.3700  @ epoch 24453 )\n",
      "[Epoch: 24500] train loss: 0.4011, train acc: 0.8225, val loss: 0.4223, val acc: 0.8132  (best train acc: 0.8363, best val acc: 0.8354, best train loss: 0.3700  @ epoch 24453 )\n",
      "[Epoch: 24520] train loss: 0.3762, train acc: 0.8331, val loss: 0.4200, val acc: 0.8169  (best train acc: 0.8363, best val acc: 0.8354, best train loss: 0.3700  @ epoch 24453 )\n",
      "[Epoch: 24540] train loss: 0.4038, train acc: 0.8208, val loss: 0.4142, val acc: 0.8155  (best train acc: 0.8363, best val acc: 0.8354, best train loss: 0.3700  @ epoch 24453 )\n",
      "[Epoch: 24560] train loss: 0.3872, train acc: 0.8313, val loss: 0.4169, val acc: 0.8115  (best train acc: 0.8363, best val acc: 0.8354, best train loss: 0.3700  @ epoch 24453 )\n",
      "[Epoch: 24580] train loss: 0.4225, train acc: 0.8112, val loss: 0.4236, val acc: 0.8067  (best train acc: 0.8363, best val acc: 0.8354, best train loss: 0.3700  @ epoch 24453 )\n",
      "[Epoch: 24600] train loss: 0.3938, train acc: 0.8250, val loss: 0.4225, val acc: 0.8094  (best train acc: 0.8363, best val acc: 0.8354, best train loss: 0.3700  @ epoch 24453 )\n",
      "[Epoch: 24620] train loss: 0.4112, train acc: 0.8217, val loss: 0.4248, val acc: 0.8054  (best train acc: 0.8363, best val acc: 0.8354, best train loss: 0.3700  @ epoch 24453 )\n",
      "[Epoch: 24640] train loss: 0.4075, train acc: 0.8188, val loss: 0.4260, val acc: 0.8064  (best train acc: 0.8363, best val acc: 0.8354, best train loss: 0.3700  @ epoch 24453 )\n",
      "[Epoch: 24660] train loss: 0.3895, train acc: 0.8282, val loss: 0.4224, val acc: 0.8071  (best train acc: 0.8363, best val acc: 0.8354, best train loss: 0.3700  @ epoch 24453 )\n",
      "[Epoch: 24680] train loss: 0.4055, train acc: 0.8190, val loss: 0.4224, val acc: 0.8030  (best train acc: 0.8363, best val acc: 0.8354, best train loss: 0.3700  @ epoch 24453 )\n",
      "[Epoch: 24700] train loss: 0.4134, train acc: 0.8191, val loss: 0.4286, val acc: 0.8030  (best train acc: 0.8363, best val acc: 0.8354, best train loss: 0.3700  @ epoch 24453 )\n",
      "[Epoch: 24720] train loss: 0.4241, train acc: 0.8141, val loss: 0.4439, val acc: 0.7912  (best train acc: 0.8363, best val acc: 0.8354, best train loss: 0.3700  @ epoch 24453 )\n",
      "[Epoch: 24740] train loss: 0.3976, train acc: 0.8246, val loss: 0.4243, val acc: 0.8064  (best train acc: 0.8363, best val acc: 0.8354, best train loss: 0.3700  @ epoch 24453 )\n",
      "[Epoch: 24760] train loss: 0.3902, train acc: 0.8237, val loss: 0.4213, val acc: 0.8088  (best train acc: 0.8363, best val acc: 0.8354, best train loss: 0.3700  @ epoch 24453 )\n",
      "[Epoch: 24780] train loss: 0.3757, train acc: 0.8365, val loss: 0.4181, val acc: 0.8115  (best train acc: 0.8365, best val acc: 0.8354, best train loss: 0.3700  @ epoch 24453 )\n",
      "[Epoch: 24800] train loss: 0.3912, train acc: 0.8272, val loss: 0.4028, val acc: 0.8277  (best train acc: 0.8365, best val acc: 0.8354, best train loss: 0.3700  @ epoch 24453 )\n",
      "[Epoch: 24820] train loss: 0.3956, train acc: 0.8220, val loss: 0.4140, val acc: 0.8121  (best train acc: 0.8365, best val acc: 0.8354, best train loss: 0.3700  @ epoch 24453 )\n",
      "[Epoch: 24840] train loss: 0.3888, train acc: 0.8274, val loss: 0.3995, val acc: 0.8307  (best train acc: 0.8365, best val acc: 0.8354, best train loss: 0.3700  @ epoch 24453 )\n",
      "[Epoch: 24860] train loss: 0.4013, train acc: 0.8264, val loss: 0.4136, val acc: 0.8115  (best train acc: 0.8365, best val acc: 0.8354, best train loss: 0.3700  @ epoch 24453 )\n",
      "[Epoch: 24880] train loss: 0.3794, train acc: 0.8315, val loss: 0.4249, val acc: 0.8091  (best train acc: 0.8365, best val acc: 0.8354, best train loss: 0.3700  @ epoch 24453 )\n",
      "[Epoch: 24900] train loss: 0.3938, train acc: 0.8279, val loss: 0.4131, val acc: 0.8145  (best train acc: 0.8376, best val acc: 0.8354, best train loss: 0.3700  @ epoch 24453 )\n",
      "[Epoch: 24920] train loss: 0.4255, train acc: 0.8114, val loss: 0.4418, val acc: 0.7879  (best train acc: 0.8376, best val acc: 0.8354, best train loss: 0.3700  @ epoch 24453 )\n",
      "[Epoch: 24940] train loss: 0.3980, train acc: 0.8203, val loss: 0.4295, val acc: 0.8027  (best train acc: 0.8376, best val acc: 0.8354, best train loss: 0.3700  @ epoch 24453 )\n",
      "[Epoch: 24960] train loss: 0.4268, train acc: 0.8096, val loss: 0.4237, val acc: 0.8061  (best train acc: 0.8376, best val acc: 0.8354, best train loss: 0.3700  @ epoch 24453 )\n",
      "[Epoch: 24980] train loss: 0.3900, train acc: 0.8315, val loss: 0.4105, val acc: 0.8155  (best train acc: 0.8376, best val acc: 0.8354, best train loss: 0.3700  @ epoch 24453 )\n",
      "[Epoch: 25000] train loss: 0.3931, train acc: 0.8244, val loss: 0.4106, val acc: 0.8121  (best train acc: 0.8376, best val acc: 0.8354, best train loss: 0.3700  @ epoch 24453 )\n",
      "[Epoch: 25020] train loss: 0.3894, train acc: 0.8253, val loss: 0.4078, val acc: 0.8283  (best train acc: 0.8376, best val acc: 0.8354, best train loss: 0.3700  @ epoch 24453 )\n",
      "[Epoch: 25040] train loss: 0.3939, train acc: 0.8235, val loss: 0.4091, val acc: 0.8162  (best train acc: 0.8376, best val acc: 0.8354, best train loss: 0.3700  @ epoch 24453 )\n",
      "[Epoch: 25060] train loss: 0.3900, train acc: 0.8253, val loss: 0.4062, val acc: 0.8270  (best train acc: 0.8376, best val acc: 0.8354, best train loss: 0.3700  @ epoch 24453 )\n",
      "[Epoch: 25080] train loss: 0.3915, train acc: 0.8288, val loss: 0.4267, val acc: 0.8024  (best train acc: 0.8376, best val acc: 0.8354, best train loss: 0.3700  @ epoch 24453 )\n",
      "[Epoch: 25100] train loss: 0.3910, train acc: 0.8265, val loss: 0.4067, val acc: 0.8152  (best train acc: 0.8376, best val acc: 0.8354, best train loss: 0.3700  @ epoch 24453 )\n",
      "[Epoch: 25120] train loss: 0.3908, train acc: 0.8292, val loss: 0.4314, val acc: 0.8040  (best train acc: 0.8376, best val acc: 0.8354, best train loss: 0.3700  @ epoch 24453 )\n",
      "[Epoch: 25140] train loss: 0.3785, train acc: 0.8298, val loss: 0.4157, val acc: 0.8145  (best train acc: 0.8376, best val acc: 0.8354, best train loss: 0.3700  @ epoch 24453 )\n",
      "[Epoch: 25160] train loss: 0.3893, train acc: 0.8287, val loss: 0.4434, val acc: 0.7821  (best train acc: 0.8376, best val acc: 0.8354, best train loss: 0.3700  @ epoch 24453 )\n",
      "[Epoch: 25180] train loss: 0.3990, train acc: 0.8282, val loss: 0.4214, val acc: 0.8108  (best train acc: 0.8376, best val acc: 0.8354, best train loss: 0.3700  @ epoch 24453 )\n",
      "[Epoch: 25200] train loss: 0.3895, train acc: 0.8227, val loss: 0.4357, val acc: 0.8047  (best train acc: 0.8376, best val acc: 0.8354, best train loss: 0.3700  @ epoch 24453 )\n",
      "[Epoch: 25220] train loss: 0.3948, train acc: 0.8229, val loss: 0.4002, val acc: 0.8273  (best train acc: 0.8376, best val acc: 0.8354, best train loss: 0.3700  @ epoch 24453 )\n",
      "[Epoch: 25240] train loss: 0.4160, train acc: 0.8172, val loss: 0.4377, val acc: 0.7929  (best train acc: 0.8376, best val acc: 0.8354, best train loss: 0.3700  @ epoch 24453 )\n",
      "[Epoch: 25260] train loss: 0.4082, train acc: 0.8161, val loss: 0.4063, val acc: 0.8219  (best train acc: 0.8376, best val acc: 0.8354, best train loss: 0.3700  @ epoch 24453 )\n",
      "[Epoch: 25280] train loss: 0.3867, train acc: 0.8271, val loss: 0.4060, val acc: 0.8175  (best train acc: 0.8376, best val acc: 0.8354, best train loss: 0.3700  @ epoch 24453 )\n",
      "[Epoch: 25300] train loss: 0.3821, train acc: 0.8302, val loss: 0.4141, val acc: 0.8236  (best train acc: 0.8376, best val acc: 0.8354, best train loss: 0.3700  @ epoch 24453 )\n",
      "[Epoch: 25320] train loss: 0.3963, train acc: 0.8238, val loss: 0.4006, val acc: 0.8199  (best train acc: 0.8376, best val acc: 0.8354, best train loss: 0.3700  @ epoch 24453 )\n",
      "[Epoch: 25340] train loss: 0.4157, train acc: 0.8199, val loss: 0.4188, val acc: 0.8057  (best train acc: 0.8376, best val acc: 0.8354, best train loss: 0.3700  @ epoch 24453 )\n",
      "[Epoch: 25360] train loss: 0.4041, train acc: 0.8186, val loss: 0.4031, val acc: 0.8226  (best train acc: 0.8376, best val acc: 0.8354, best train loss: 0.3700  @ epoch 24453 )\n",
      "[Epoch: 25380] train loss: 0.3850, train acc: 0.8267, val loss: 0.4165, val acc: 0.8067  (best train acc: 0.8382, best val acc: 0.8354, best train loss: 0.3700  @ epoch 24453 )\n",
      "[Epoch: 25400] train loss: 0.3826, train acc: 0.8295, val loss: 0.4067, val acc: 0.8162  (best train acc: 0.8382, best val acc: 0.8354, best train loss: 0.3700  @ epoch 24453 )\n",
      "[Epoch: 25420] train loss: 0.3823, train acc: 0.8326, val loss: 0.3986, val acc: 0.8239  (best train acc: 0.8382, best val acc: 0.8354, best train loss: 0.3700  @ epoch 24453 )\n",
      "[Epoch: 25440] train loss: 0.3871, train acc: 0.8303, val loss: 0.4163, val acc: 0.8051  (best train acc: 0.8382, best val acc: 0.8354, best train loss: 0.3700  @ epoch 24453 )\n",
      "[Epoch: 25460] train loss: 0.3885, train acc: 0.8308, val loss: 0.4058, val acc: 0.8152  (best train acc: 0.8382, best val acc: 0.8354, best train loss: 0.3700  @ epoch 24453 )\n",
      "[Epoch: 25480] train loss: 0.3834, train acc: 0.8308, val loss: 0.4217, val acc: 0.8165  (best train acc: 0.8382, best val acc: 0.8354, best train loss: 0.3700  @ epoch 24453 )\n",
      "[Epoch: 25500] train loss: 0.3925, train acc: 0.8267, val loss: 0.4212, val acc: 0.8044  (best train acc: 0.8382, best val acc: 0.8354, best train loss: 0.3700  @ epoch 24453 )\n",
      "[Epoch: 25520] train loss: 0.3702, train acc: 0.8360, val loss: 0.4043, val acc: 0.8277  (best train acc: 0.8382, best val acc: 0.8354, best train loss: 0.3700  @ epoch 24453 )\n",
      "[Epoch: 25540] train loss: 0.3824, train acc: 0.8329, val loss: 0.4033, val acc: 0.8185  (best train acc: 0.8382, best val acc: 0.8354, best train loss: 0.3700  @ epoch 24453 )\n",
      "[Epoch: 25560] train loss: 0.3793, train acc: 0.8336, val loss: 0.4067, val acc: 0.8196  (best train acc: 0.8382, best val acc: 0.8354, best train loss: 0.3700  @ epoch 24453 )\n",
      "[Epoch: 25580] train loss: 0.3919, train acc: 0.8287, val loss: 0.4239, val acc: 0.8007  (best train acc: 0.8382, best val acc: 0.8354, best train loss: 0.3700  @ epoch 24453 )\n",
      "[Epoch: 25600] train loss: 0.4059, train acc: 0.8219, val loss: 0.4313, val acc: 0.7963  (best train acc: 0.8382, best val acc: 0.8354, best train loss: 0.3700  @ epoch 24453 )\n",
      "[Epoch: 25620] train loss: 0.4014, train acc: 0.8216, val loss: 0.4120, val acc: 0.8088  (best train acc: 0.8382, best val acc: 0.8354, best train loss: 0.3700  @ epoch 24453 )\n",
      "[Epoch: 25640] train loss: 0.3820, train acc: 0.8337, val loss: 0.4275, val acc: 0.8054  (best train acc: 0.8382, best val acc: 0.8354, best train loss: 0.3700  @ epoch 24453 )\n",
      "[Epoch: 25660] train loss: 0.3840, train acc: 0.8308, val loss: 0.4073, val acc: 0.8219  (best train acc: 0.8382, best val acc: 0.8354, best train loss: 0.3700  @ epoch 24453 )\n",
      "[Epoch: 25680] train loss: 0.3959, train acc: 0.8236, val loss: 0.4126, val acc: 0.8121  (best train acc: 0.8386, best val acc: 0.8354, best train loss: 0.3688  @ epoch 25674 )\n",
      "[Epoch: 25700] train loss: 0.4111, train acc: 0.8182, val loss: 0.4165, val acc: 0.8067  (best train acc: 0.8386, best val acc: 0.8354, best train loss: 0.3688  @ epoch 25674 )\n",
      "[Epoch: 25720] train loss: 0.4049, train acc: 0.8182, val loss: 0.4171, val acc: 0.8064  (best train acc: 0.8386, best val acc: 0.8354, best train loss: 0.3688  @ epoch 25674 )\n",
      "[Epoch: 25740] train loss: 0.4007, train acc: 0.8227, val loss: 0.4211, val acc: 0.8061  (best train acc: 0.8386, best val acc: 0.8354, best train loss: 0.3688  @ epoch 25674 )\n",
      "[Epoch: 25760] train loss: 0.3954, train acc: 0.8266, val loss: 0.4238, val acc: 0.8054  (best train acc: 0.8386, best val acc: 0.8354, best train loss: 0.3688  @ epoch 25674 )\n",
      "[Epoch: 25780] train loss: 0.3902, train acc: 0.8224, val loss: 0.4164, val acc: 0.8047  (best train acc: 0.8386, best val acc: 0.8354, best train loss: 0.3688  @ epoch 25674 )\n",
      "[Epoch: 25800] train loss: 0.4153, train acc: 0.8171, val loss: 0.4288, val acc: 0.7973  (best train acc: 0.8386, best val acc: 0.8354, best train loss: 0.3688  @ epoch 25674 )\n",
      "[Epoch: 25820] train loss: 0.4267, train acc: 0.8106, val loss: 0.4307, val acc: 0.8040  (best train acc: 0.8386, best val acc: 0.8354, best train loss: 0.3688  @ epoch 25674 )\n",
      "[Epoch: 25840] train loss: 0.4214, train acc: 0.8133, val loss: 0.4243, val acc: 0.7997  (best train acc: 0.8386, best val acc: 0.8354, best train loss: 0.3688  @ epoch 25674 )\n",
      "[Epoch: 25860] train loss: 0.4585, train acc: 0.7940, val loss: 0.4465, val acc: 0.7899  (best train acc: 0.8386, best val acc: 0.8354, best train loss: 0.3688  @ epoch 25674 )\n",
      "[Epoch: 25880] train loss: 0.3764, train acc: 0.8356, val loss: 0.4365, val acc: 0.7868  (best train acc: 0.8386, best val acc: 0.8354, best train loss: 0.3688  @ epoch 25674 )\n",
      "[Epoch: 25900] train loss: 0.4026, train acc: 0.8211, val loss: 0.4105, val acc: 0.8185  (best train acc: 0.8386, best val acc: 0.8354, best train loss: 0.3688  @ epoch 25674 )\n",
      "[Epoch: 25920] train loss: 0.4222, train acc: 0.8119, val loss: 0.4237, val acc: 0.8047  (best train acc: 0.8386, best val acc: 0.8354, best train loss: 0.3688  @ epoch 25674 )\n",
      "[Epoch: 25940] train loss: 0.3772, train acc: 0.8315, val loss: 0.4159, val acc: 0.8121  (best train acc: 0.8386, best val acc: 0.8354, best train loss: 0.3688  @ epoch 25674 )\n",
      "[Epoch: 25960] train loss: 0.3780, train acc: 0.8347, val loss: 0.4019, val acc: 0.8290  (best train acc: 0.8392, best val acc: 0.8354, best train loss: 0.3648  @ epoch 25942 )\n",
      "[Epoch: 25980] train loss: 0.3848, train acc: 0.8348, val loss: 0.4197, val acc: 0.8067  (best train acc: 0.8392, best val acc: 0.8354, best train loss: 0.3648  @ epoch 25942 )\n",
      "[Epoch: 26000] train loss: 0.3983, train acc: 0.8206, val loss: 0.4267, val acc: 0.8007  (best train acc: 0.8392, best val acc: 0.8354, best train loss: 0.3648  @ epoch 25942 )\n",
      "[Epoch: 26020] train loss: 0.3998, train acc: 0.8219, val loss: 0.4092, val acc: 0.8165  (best train acc: 0.8392, best val acc: 0.8354, best train loss: 0.3648  @ epoch 25942 )\n",
      "[Epoch: 26040] train loss: 0.3778, train acc: 0.8349, val loss: 0.4037, val acc: 0.8135  (best train acc: 0.8392, best val acc: 0.8354, best train loss: 0.3648  @ epoch 25942 )\n",
      "[Epoch: 26060] train loss: 0.3995, train acc: 0.8242, val loss: 0.4221, val acc: 0.8024  (best train acc: 0.8392, best val acc: 0.8354, best train loss: 0.3648  @ epoch 25942 )\n",
      "[Epoch: 26080] train loss: 0.3740, train acc: 0.8376, val loss: 0.4266, val acc: 0.8071  (best train acc: 0.8392, best val acc: 0.8354, best train loss: 0.3648  @ epoch 25942 )\n",
      "[Epoch: 26100] train loss: 0.3981, train acc: 0.8238, val loss: 0.4131, val acc: 0.8165  (best train acc: 0.8392, best val acc: 0.8361, best train loss: 0.3648  @ epoch 25942 )\n",
      "[Epoch: 26120] train loss: 0.3880, train acc: 0.8289, val loss: 0.4110, val acc: 0.8128  (best train acc: 0.8392, best val acc: 0.8361, best train loss: 0.3648  @ epoch 25942 )\n",
      "[Epoch: 26140] train loss: 0.3926, train acc: 0.8253, val loss: 0.4081, val acc: 0.8111  (best train acc: 0.8392, best val acc: 0.8361, best train loss: 0.3648  @ epoch 25942 )\n",
      "[Epoch: 26160] train loss: 0.4284, train acc: 0.8128, val loss: 0.4329, val acc: 0.7939  (best train acc: 0.8392, best val acc: 0.8361, best train loss: 0.3648  @ epoch 25942 )\n",
      "[Epoch: 26180] train loss: 0.3946, train acc: 0.8274, val loss: 0.4237, val acc: 0.8047  (best train acc: 0.8392, best val acc: 0.8361, best train loss: 0.3648  @ epoch 25942 )\n",
      "[Epoch: 26200] train loss: 0.3966, train acc: 0.8287, val loss: 0.4046, val acc: 0.8233  (best train acc: 0.8392, best val acc: 0.8361, best train loss: 0.3648  @ epoch 25942 )\n",
      "[Epoch: 26220] train loss: 0.3775, train acc: 0.8332, val loss: 0.4143, val acc: 0.8169  (best train acc: 0.8392, best val acc: 0.8361, best train loss: 0.3648  @ epoch 25942 )\n",
      "[Epoch: 26240] train loss: 0.4000, train acc: 0.8206, val loss: 0.4278, val acc: 0.7997  (best train acc: 0.8392, best val acc: 0.8361, best train loss: 0.3648  @ epoch 25942 )\n",
      "[Epoch: 26260] train loss: 0.3962, train acc: 0.8180, val loss: 0.4090, val acc: 0.8179  (best train acc: 0.8420, best val acc: 0.8361, best train loss: 0.3648  @ epoch 25942 )\n",
      "[Epoch: 26280] train loss: 0.3863, train acc: 0.8297, val loss: 0.4127, val acc: 0.8132  (best train acc: 0.8420, best val acc: 0.8361, best train loss: 0.3648  @ epoch 25942 )\n",
      "[Epoch: 26300] train loss: 0.4587, train acc: 0.7835, val loss: 0.4361, val acc: 0.7946  (best train acc: 0.8420, best val acc: 0.8361, best train loss: 0.3648  @ epoch 25942 )\n",
      "[Epoch: 26320] train loss: 0.3835, train acc: 0.8327, val loss: 0.4188, val acc: 0.8101  (best train acc: 0.8420, best val acc: 0.8361, best train loss: 0.3648  @ epoch 25942 )\n",
      "[Epoch: 26340] train loss: 0.4336, train acc: 0.8120, val loss: 0.4483, val acc: 0.7963  (best train acc: 0.8420, best val acc: 0.8361, best train loss: 0.3648  @ epoch 25942 )\n",
      "[Epoch: 26360] train loss: 0.4116, train acc: 0.8157, val loss: 0.4328, val acc: 0.7966  (best train acc: 0.8420, best val acc: 0.8361, best train loss: 0.3648  @ epoch 25942 )\n",
      "[Epoch: 26380] train loss: 0.4017, train acc: 0.8229, val loss: 0.4331, val acc: 0.7956  (best train acc: 0.8420, best val acc: 0.8361, best train loss: 0.3648  @ epoch 25942 )\n",
      "[Epoch: 26400] train loss: 0.3853, train acc: 0.8329, val loss: 0.4039, val acc: 0.8189  (best train acc: 0.8420, best val acc: 0.8361, best train loss: 0.3648  @ epoch 25942 )\n",
      "[Epoch: 26420] train loss: 0.3731, train acc: 0.8341, val loss: 0.4039, val acc: 0.8175  (best train acc: 0.8420, best val acc: 0.8361, best train loss: 0.3648  @ epoch 25942 )\n",
      "[Epoch: 26440] train loss: 0.3838, train acc: 0.8286, val loss: 0.4114, val acc: 0.8098  (best train acc: 0.8420, best val acc: 0.8361, best train loss: 0.3648  @ epoch 25942 )\n",
      "[Epoch: 26460] train loss: 0.3835, train acc: 0.8289, val loss: 0.4156, val acc: 0.8132  (best train acc: 0.8420, best val acc: 0.8361, best train loss: 0.3648  @ epoch 25942 )\n",
      "[Epoch: 26480] train loss: 0.4043, train acc: 0.8164, val loss: 0.4156, val acc: 0.8125  (best train acc: 0.8420, best val acc: 0.8361, best train loss: 0.3648  @ epoch 25942 )\n",
      "[Epoch: 26500] train loss: 0.3967, train acc: 0.8266, val loss: 0.4158, val acc: 0.8145  (best train acc: 0.8420, best val acc: 0.8361, best train loss: 0.3648  @ epoch 25942 )\n",
      "[Epoch: 26520] train loss: 0.3856, train acc: 0.8266, val loss: 0.4110, val acc: 0.8182  (best train acc: 0.8420, best val acc: 0.8361, best train loss: 0.3648  @ epoch 25942 )\n",
      "[Epoch: 26540] train loss: 0.3858, train acc: 0.8266, val loss: 0.3980, val acc: 0.8280  (best train acc: 0.8420, best val acc: 0.8361, best train loss: 0.3648  @ epoch 25942 )\n",
      "[Epoch: 26560] train loss: 0.4085, train acc: 0.8164, val loss: 0.4098, val acc: 0.8142  (best train acc: 0.8420, best val acc: 0.8361, best train loss: 0.3648  @ epoch 25942 )\n",
      "[Epoch: 26580] train loss: 0.4034, train acc: 0.8199, val loss: 0.4043, val acc: 0.8169  (best train acc: 0.8420, best val acc: 0.8361, best train loss: 0.3648  @ epoch 25942 )\n",
      "[Epoch: 26600] train loss: 0.3938, train acc: 0.8276, val loss: 0.3998, val acc: 0.8216  (best train acc: 0.8420, best val acc: 0.8361, best train loss: 0.3648  @ epoch 25942 )\n",
      "[Epoch: 26620] train loss: 0.4048, train acc: 0.8224, val loss: 0.4364, val acc: 0.7933  (best train acc: 0.8420, best val acc: 0.8361, best train loss: 0.3648  @ epoch 25942 )\n",
      "[Epoch: 26640] train loss: 0.3835, train acc: 0.8339, val loss: 0.4055, val acc: 0.8199  (best train acc: 0.8420, best val acc: 0.8361, best train loss: 0.3648  @ epoch 25942 )\n",
      "[Epoch: 26660] train loss: 0.3800, train acc: 0.8319, val loss: 0.4068, val acc: 0.8189  (best train acc: 0.8420, best val acc: 0.8361, best train loss: 0.3648  @ epoch 25942 )\n",
      "[Epoch: 26680] train loss: 0.3824, train acc: 0.8260, val loss: 0.4053, val acc: 0.8199  (best train acc: 0.8420, best val acc: 0.8361, best train loss: 0.3648  @ epoch 25942 )\n",
      "[Epoch: 26700] train loss: 0.3969, train acc: 0.8229, val loss: 0.4064, val acc: 0.8226  (best train acc: 0.8420, best val acc: 0.8361, best train loss: 0.3648  @ epoch 25942 )\n",
      "[Epoch: 26720] train loss: 0.4075, train acc: 0.8205, val loss: 0.4090, val acc: 0.8135  (best train acc: 0.8420, best val acc: 0.8361, best train loss: 0.3648  @ epoch 25942 )\n",
      "[Epoch: 26740] train loss: 0.3936, train acc: 0.8248, val loss: 0.4059, val acc: 0.8135  (best train acc: 0.8420, best val acc: 0.8361, best train loss: 0.3648  @ epoch 25942 )\n",
      "[Epoch: 26760] train loss: 0.3962, train acc: 0.8263, val loss: 0.4109, val acc: 0.8105  (best train acc: 0.8420, best val acc: 0.8361, best train loss: 0.3648  @ epoch 25942 )\n",
      "[Epoch: 26780] train loss: 0.3841, train acc: 0.8308, val loss: 0.4177, val acc: 0.8054  (best train acc: 0.8420, best val acc: 0.8361, best train loss: 0.3648  @ epoch 25942 )\n",
      "[Epoch: 26800] train loss: 0.3996, train acc: 0.8269, val loss: 0.4251, val acc: 0.8081  (best train acc: 0.8420, best val acc: 0.8361, best train loss: 0.3648  @ epoch 25942 )\n",
      "[Epoch: 26820] train loss: 0.3901, train acc: 0.8277, val loss: 0.4017, val acc: 0.8142  (best train acc: 0.8420, best val acc: 0.8361, best train loss: 0.3648  @ epoch 25942 )\n",
      "[Epoch: 26840] train loss: 0.4076, train acc: 0.8188, val loss: 0.4133, val acc: 0.8202  (best train acc: 0.8420, best val acc: 0.8361, best train loss: 0.3648  @ epoch 25942 )\n",
      "[Epoch: 26860] train loss: 0.3929, train acc: 0.8277, val loss: 0.4058, val acc: 0.8165  (best train acc: 0.8420, best val acc: 0.8361, best train loss: 0.3648  @ epoch 25942 )\n",
      "[Epoch: 26880] train loss: 0.4044, train acc: 0.8206, val loss: 0.3994, val acc: 0.8179  (best train acc: 0.8420, best val acc: 0.8361, best train loss: 0.3648  @ epoch 25942 )\n",
      "[Epoch: 26900] train loss: 0.3861, train acc: 0.8317, val loss: 0.4061, val acc: 0.8165  (best train acc: 0.8420, best val acc: 0.8361, best train loss: 0.3648  @ epoch 25942 )\n",
      "[Epoch: 26920] train loss: 0.4096, train acc: 0.8154, val loss: 0.4455, val acc: 0.7983  (best train acc: 0.8420, best val acc: 0.8361, best train loss: 0.3648  @ epoch 25942 )\n",
      "[Epoch: 26940] train loss: 0.3910, train acc: 0.8256, val loss: 0.4184, val acc: 0.8159  (best train acc: 0.8420, best val acc: 0.8361, best train loss: 0.3648  @ epoch 25942 )\n",
      "[Epoch: 26960] train loss: 0.3815, train acc: 0.8316, val loss: 0.4116, val acc: 0.8162  (best train acc: 0.8420, best val acc: 0.8361, best train loss: 0.3648  @ epoch 25942 )\n",
      "[Epoch: 26980] train loss: 0.3850, train acc: 0.8294, val loss: 0.4162, val acc: 0.8061  (best train acc: 0.8420, best val acc: 0.8361, best train loss: 0.3648  @ epoch 25942 )\n",
      "[Epoch: 27000] train loss: 0.3877, train acc: 0.8283, val loss: 0.4009, val acc: 0.8189  (best train acc: 0.8420, best val acc: 0.8361, best train loss: 0.3648  @ epoch 25942 )\n",
      "[Epoch: 27020] train loss: 0.4195, train acc: 0.8156, val loss: 0.4273, val acc: 0.7980  (best train acc: 0.8420, best val acc: 0.8361, best train loss: 0.3648  @ epoch 25942 )\n",
      "[Epoch: 27040] train loss: 0.3829, train acc: 0.8316, val loss: 0.4206, val acc: 0.8054  (best train acc: 0.8420, best val acc: 0.8361, best train loss: 0.3648  @ epoch 25942 )\n",
      "[Epoch: 27060] train loss: 0.4033, train acc: 0.8225, val loss: 0.4109, val acc: 0.8202  (best train acc: 0.8420, best val acc: 0.8361, best train loss: 0.3648  @ epoch 25942 )\n",
      "[Epoch: 27080] train loss: 0.3906, train acc: 0.8219, val loss: 0.3884, val acc: 0.8304  (best train acc: 0.8420, best val acc: 0.8361, best train loss: 0.3648  @ epoch 25942 )\n",
      "[Epoch: 27100] train loss: 0.3845, train acc: 0.8311, val loss: 0.4192, val acc: 0.8094  (best train acc: 0.8420, best val acc: 0.8361, best train loss: 0.3648  @ epoch 25942 )\n",
      "[Epoch: 27120] train loss: 0.3885, train acc: 0.8262, val loss: 0.4099, val acc: 0.8152  (best train acc: 0.8420, best val acc: 0.8361, best train loss: 0.3648  @ epoch 25942 )\n",
      "[Epoch: 27140] train loss: 0.3981, train acc: 0.8237, val loss: 0.4170, val acc: 0.8202  (best train acc: 0.8420, best val acc: 0.8361, best train loss: 0.3648  @ epoch 25942 )\n",
      "[Epoch: 27160] train loss: 0.3867, train acc: 0.8317, val loss: 0.4030, val acc: 0.8155  (best train acc: 0.8420, best val acc: 0.8361, best train loss: 0.3648  @ epoch 25942 )\n",
      "[Epoch: 27180] train loss: 0.3903, train acc: 0.8295, val loss: 0.4218, val acc: 0.8017  (best train acc: 0.8420, best val acc: 0.8361, best train loss: 0.3648  @ epoch 25942 )\n",
      "[Epoch: 27200] train loss: 0.3807, train acc: 0.8286, val loss: 0.4025, val acc: 0.8206  (best train acc: 0.8420, best val acc: 0.8361, best train loss: 0.3648  @ epoch 25942 )\n",
      "[Epoch: 27220] train loss: 0.3777, train acc: 0.8318, val loss: 0.4102, val acc: 0.8165  (best train acc: 0.8420, best val acc: 0.8361, best train loss: 0.3648  @ epoch 25942 )\n",
      "[Epoch: 27240] train loss: 0.3995, train acc: 0.8182, val loss: 0.4540, val acc: 0.7774  (best train acc: 0.8420, best val acc: 0.8361, best train loss: 0.3648  @ epoch 25942 )\n",
      "[Epoch: 27260] train loss: 0.3999, train acc: 0.8231, val loss: 0.4198, val acc: 0.8152  (best train acc: 0.8420, best val acc: 0.8361, best train loss: 0.3648  @ epoch 25942 )\n",
      "[Epoch: 27280] train loss: 0.3967, train acc: 0.8220, val loss: 0.4103, val acc: 0.8189  (best train acc: 0.8420, best val acc: 0.8398, best train loss: 0.3648  @ epoch 25942 )\n",
      "[Epoch: 27300] train loss: 0.3819, train acc: 0.8279, val loss: 0.4190, val acc: 0.8118  (best train acc: 0.8420, best val acc: 0.8398, best train loss: 0.3648  @ epoch 25942 )\n",
      "[Epoch: 27320] train loss: 0.3982, train acc: 0.8222, val loss: 0.4356, val acc: 0.8074  (best train acc: 0.8420, best val acc: 0.8398, best train loss: 0.3648  @ epoch 25942 )\n",
      "[Epoch: 27340] train loss: 0.3896, train acc: 0.8233, val loss: 0.4104, val acc: 0.8135  (best train acc: 0.8420, best val acc: 0.8398, best train loss: 0.3648  @ epoch 25942 )\n",
      "[Epoch: 27360] train loss: 0.3753, train acc: 0.8373, val loss: 0.4272, val acc: 0.7997  (best train acc: 0.8420, best val acc: 0.8398, best train loss: 0.3648  @ epoch 25942 )\n",
      "[Epoch: 27380] train loss: 0.3967, train acc: 0.8225, val loss: 0.4112, val acc: 0.8162  (best train acc: 0.8420, best val acc: 0.8398, best train loss: 0.3648  @ epoch 25942 )\n",
      "[Epoch: 27400] train loss: 0.3835, train acc: 0.8258, val loss: 0.4024, val acc: 0.8243  (best train acc: 0.8420, best val acc: 0.8398, best train loss: 0.3648  @ epoch 25942 )\n",
      "[Epoch: 27420] train loss: 0.3800, train acc: 0.8339, val loss: 0.4150, val acc: 0.8111  (best train acc: 0.8420, best val acc: 0.8398, best train loss: 0.3648  @ epoch 25942 )\n",
      "[Epoch: 27440] train loss: 0.3736, train acc: 0.8345, val loss: 0.3979, val acc: 0.8280  (best train acc: 0.8420, best val acc: 0.8398, best train loss: 0.3648  @ epoch 25942 )\n",
      "[Epoch: 27460] train loss: 0.3914, train acc: 0.8263, val loss: 0.4099, val acc: 0.8212  (best train acc: 0.8420, best val acc: 0.8398, best train loss: 0.3648  @ epoch 25942 )\n",
      "[Epoch: 27480] train loss: 0.3935, train acc: 0.8211, val loss: 0.4209, val acc: 0.8020  (best train acc: 0.8420, best val acc: 0.8398, best train loss: 0.3648  @ epoch 25942 )\n",
      "[Epoch: 27500] train loss: 0.3996, train acc: 0.8221, val loss: 0.4003, val acc: 0.8236  (best train acc: 0.8420, best val acc: 0.8398, best train loss: 0.3648  @ epoch 25942 )\n",
      "[Epoch: 27520] train loss: 0.3858, train acc: 0.8305, val loss: 0.4155, val acc: 0.8121  (best train acc: 0.8420, best val acc: 0.8398, best train loss: 0.3648  @ epoch 25942 )\n",
      "[Epoch: 27540] train loss: 0.3797, train acc: 0.8301, val loss: 0.4063, val acc: 0.8162  (best train acc: 0.8420, best val acc: 0.8398, best train loss: 0.3648  @ epoch 25942 )\n",
      "[Epoch: 27560] train loss: 0.3788, train acc: 0.8325, val loss: 0.4107, val acc: 0.8256  (best train acc: 0.8420, best val acc: 0.8398, best train loss: 0.3648  @ epoch 25942 )\n",
      "[Epoch: 27580] train loss: 0.4013, train acc: 0.8246, val loss: 0.4242, val acc: 0.8030  (best train acc: 0.8420, best val acc: 0.8398, best train loss: 0.3648  @ epoch 25942 )\n",
      "[Epoch: 27600] train loss: 0.3920, train acc: 0.8231, val loss: 0.3942, val acc: 0.8263  (best train acc: 0.8420, best val acc: 0.8398, best train loss: 0.3648  @ epoch 25942 )\n",
      "[Epoch: 27620] train loss: 0.3932, train acc: 0.8255, val loss: 0.4248, val acc: 0.8024  (best train acc: 0.8420, best val acc: 0.8398, best train loss: 0.3648  @ epoch 25942 )\n",
      "[Epoch: 27640] train loss: 0.3988, train acc: 0.8242, val loss: 0.4267, val acc: 0.8132  (best train acc: 0.8420, best val acc: 0.8398, best train loss: 0.3648  @ epoch 25942 )\n",
      "[Epoch: 27660] train loss: 0.3911, train acc: 0.8254, val loss: 0.4209, val acc: 0.8054  (best train acc: 0.8420, best val acc: 0.8398, best train loss: 0.3648  @ epoch 25942 )\n",
      "[Epoch: 27680] train loss: 0.3792, train acc: 0.8334, val loss: 0.4151, val acc: 0.8111  (best train acc: 0.8420, best val acc: 0.8398, best train loss: 0.3648  @ epoch 25942 )\n",
      "[Epoch: 27700] train loss: 0.3973, train acc: 0.8261, val loss: 0.4125, val acc: 0.8101  (best train acc: 0.8420, best val acc: 0.8398, best train loss: 0.3648  @ epoch 25942 )\n",
      "[Epoch: 27720] train loss: 0.3763, train acc: 0.8345, val loss: 0.4054, val acc: 0.8250  (best train acc: 0.8420, best val acc: 0.8398, best train loss: 0.3648  @ epoch 25942 )\n",
      "[Epoch: 27740] train loss: 0.3785, train acc: 0.8321, val loss: 0.4033, val acc: 0.8185  (best train acc: 0.8420, best val acc: 0.8398, best train loss: 0.3648  @ epoch 25942 )\n",
      "[Epoch: 27760] train loss: 0.3987, train acc: 0.8240, val loss: 0.4119, val acc: 0.8121  (best train acc: 0.8420, best val acc: 0.8398, best train loss: 0.3648  @ epoch 25942 )\n",
      "[Epoch: 27780] train loss: 0.3751, train acc: 0.8354, val loss: 0.4123, val acc: 0.8084  (best train acc: 0.8420, best val acc: 0.8398, best train loss: 0.3648  @ epoch 25942 )\n",
      "[Epoch: 27800] train loss: 0.3834, train acc: 0.8293, val loss: 0.4118, val acc: 0.8071  (best train acc: 0.8420, best val acc: 0.8398, best train loss: 0.3648  @ epoch 25942 )\n",
      "[Epoch: 27820] train loss: 0.3978, train acc: 0.8256, val loss: 0.4048, val acc: 0.8159  (best train acc: 0.8420, best val acc: 0.8398, best train loss: 0.3648  @ epoch 25942 )\n",
      "[Epoch: 27840] train loss: 0.3942, train acc: 0.8253, val loss: 0.4131, val acc: 0.8098  (best train acc: 0.8420, best val acc: 0.8398, best train loss: 0.3648  @ epoch 25942 )\n",
      "[Epoch: 27860] train loss: 0.3885, train acc: 0.8309, val loss: 0.4151, val acc: 0.8057  (best train acc: 0.8420, best val acc: 0.8398, best train loss: 0.3648  @ epoch 25942 )\n",
      "[Epoch: 27880] train loss: 0.3791, train acc: 0.8313, val loss: 0.4040, val acc: 0.8169  (best train acc: 0.8420, best val acc: 0.8398, best train loss: 0.3648  @ epoch 25942 )\n",
      "[Epoch: 27900] train loss: 0.4022, train acc: 0.8217, val loss: 0.4114, val acc: 0.8101  (best train acc: 0.8420, best val acc: 0.8398, best train loss: 0.3648  @ epoch 25942 )\n",
      "[Epoch: 27920] train loss: 0.3876, train acc: 0.8278, val loss: 0.4182, val acc: 0.8020  (best train acc: 0.8420, best val acc: 0.8398, best train loss: 0.3648  @ epoch 25942 )\n",
      "[Epoch: 27940] train loss: 0.3880, train acc: 0.8302, val loss: 0.3983, val acc: 0.8202  (best train acc: 0.8420, best val acc: 0.8398, best train loss: 0.3634  @ epoch 27927 )\n",
      "[Epoch: 27960] train loss: 0.4088, train acc: 0.8225, val loss: 0.4035, val acc: 0.8179  (best train acc: 0.8420, best val acc: 0.8398, best train loss: 0.3634  @ epoch 27927 )\n",
      "[Epoch: 27980] train loss: 0.3732, train acc: 0.8326, val loss: 0.4043, val acc: 0.8199  (best train acc: 0.8420, best val acc: 0.8398, best train loss: 0.3634  @ epoch 27927 )\n",
      "[Epoch: 28000] train loss: 0.3994, train acc: 0.8237, val loss: 0.4031, val acc: 0.8226  (best train acc: 0.8420, best val acc: 0.8398, best train loss: 0.3634  @ epoch 27927 )\n",
      "[Epoch: 28020] train loss: 0.3956, train acc: 0.8245, val loss: 0.4058, val acc: 0.8185  (best train acc: 0.8420, best val acc: 0.8398, best train loss: 0.3634  @ epoch 27927 )\n",
      "[Epoch: 28040] train loss: 0.3878, train acc: 0.8242, val loss: 0.4236, val acc: 0.8064  (best train acc: 0.8420, best val acc: 0.8398, best train loss: 0.3634  @ epoch 27927 )\n",
      "[Epoch: 28060] train loss: 0.3913, train acc: 0.8284, val loss: 0.4320, val acc: 0.8051  (best train acc: 0.8420, best val acc: 0.8398, best train loss: 0.3634  @ epoch 27927 )\n",
      "[Epoch: 28080] train loss: 0.3817, train acc: 0.8277, val loss: 0.4048, val acc: 0.8229  (best train acc: 0.8420, best val acc: 0.8398, best train loss: 0.3634  @ epoch 27927 )\n",
      "[Epoch: 28100] train loss: 0.3786, train acc: 0.8355, val loss: 0.4148, val acc: 0.8125  (best train acc: 0.8420, best val acc: 0.8398, best train loss: 0.3634  @ epoch 27927 )\n",
      "[Epoch: 28120] train loss: 0.3811, train acc: 0.8289, val loss: 0.4096, val acc: 0.8098  (best train acc: 0.8420, best val acc: 0.8398, best train loss: 0.3634  @ epoch 27927 )\n",
      "[Epoch: 28140] train loss: 0.3882, train acc: 0.8269, val loss: 0.4052, val acc: 0.8199  (best train acc: 0.8420, best val acc: 0.8398, best train loss: 0.3634  @ epoch 27927 )\n",
      "[Epoch: 28160] train loss: 0.4018, train acc: 0.8217, val loss: 0.4072, val acc: 0.8128  (best train acc: 0.8420, best val acc: 0.8398, best train loss: 0.3634  @ epoch 27927 )\n",
      "[Epoch: 28180] train loss: 0.4115, train acc: 0.8167, val loss: 0.4265, val acc: 0.8030  (best train acc: 0.8420, best val acc: 0.8398, best train loss: 0.3634  @ epoch 27927 )\n",
      "[Epoch: 28200] train loss: 0.3940, train acc: 0.8266, val loss: 0.4347, val acc: 0.7970  (best train acc: 0.8420, best val acc: 0.8398, best train loss: 0.3634  @ epoch 27927 )\n",
      "[Epoch: 28220] train loss: 0.3913, train acc: 0.8262, val loss: 0.4112, val acc: 0.8226  (best train acc: 0.8420, best val acc: 0.8398, best train loss: 0.3634  @ epoch 27927 )\n",
      "[Epoch: 28240] train loss: 0.3768, train acc: 0.8322, val loss: 0.4061, val acc: 0.8152  (best train acc: 0.8420, best val acc: 0.8398, best train loss: 0.3634  @ epoch 27927 )\n",
      "[Epoch: 28260] train loss: 0.4040, train acc: 0.8205, val loss: 0.4347, val acc: 0.7946  (best train acc: 0.8420, best val acc: 0.8398, best train loss: 0.3634  @ epoch 27927 )\n",
      "[Epoch: 28280] train loss: 0.3888, train acc: 0.8266, val loss: 0.4083, val acc: 0.8179  (best train acc: 0.8420, best val acc: 0.8398, best train loss: 0.3634  @ epoch 27927 )\n",
      "[Epoch: 28300] train loss: 0.3903, train acc: 0.8252, val loss: 0.4161, val acc: 0.8091  (best train acc: 0.8420, best val acc: 0.8398, best train loss: 0.3634  @ epoch 27927 )\n",
      "[Epoch: 28320] train loss: 0.4008, train acc: 0.8240, val loss: 0.4024, val acc: 0.8175  (best train acc: 0.8420, best val acc: 0.8398, best train loss: 0.3634  @ epoch 27927 )\n",
      "[Epoch: 28340] train loss: 0.3743, train acc: 0.8362, val loss: 0.3896, val acc: 0.8381  (best train acc: 0.8420, best val acc: 0.8398, best train loss: 0.3634  @ epoch 27927 )\n",
      "[Epoch: 28360] train loss: 0.3926, train acc: 0.8261, val loss: 0.4080, val acc: 0.8148  (best train acc: 0.8420, best val acc: 0.8398, best train loss: 0.3634  @ epoch 27927 )\n",
      "[Epoch: 28380] train loss: 0.3798, train acc: 0.8312, val loss: 0.4084, val acc: 0.8199  (best train acc: 0.8420, best val acc: 0.8398, best train loss: 0.3634  @ epoch 27927 )\n",
      "[Epoch: 28400] train loss: 0.3799, train acc: 0.8316, val loss: 0.3963, val acc: 0.8239  (best train acc: 0.8420, best val acc: 0.8398, best train loss: 0.3634  @ epoch 27927 )\n",
      "[Epoch: 28420] train loss: 0.3895, train acc: 0.8263, val loss: 0.4372, val acc: 0.7970  (best train acc: 0.8420, best val acc: 0.8398, best train loss: 0.3634  @ epoch 27927 )\n",
      "[Epoch: 28440] train loss: 0.3844, train acc: 0.8254, val loss: 0.4140, val acc: 0.8084  (best train acc: 0.8420, best val acc: 0.8398, best train loss: 0.3634  @ epoch 27927 )\n",
      "[Epoch: 28460] train loss: 0.3844, train acc: 0.8275, val loss: 0.4164, val acc: 0.8064  (best train acc: 0.8420, best val acc: 0.8398, best train loss: 0.3634  @ epoch 27927 )\n",
      "[Epoch: 28480] train loss: 0.3869, train acc: 0.8286, val loss: 0.4206, val acc: 0.8057  (best train acc: 0.8420, best val acc: 0.8398, best train loss: 0.3634  @ epoch 27927 )\n",
      "[Epoch: 28500] train loss: 0.4172, train acc: 0.8112, val loss: 0.4226, val acc: 0.8020  (best train acc: 0.8420, best val acc: 0.8398, best train loss: 0.3618  @ epoch 28499 )\n",
      "[Epoch: 28520] train loss: 0.3844, train acc: 0.8258, val loss: 0.4069, val acc: 0.8128  (best train acc: 0.8420, best val acc: 0.8398, best train loss: 0.3618  @ epoch 28499 )\n",
      "[Epoch: 28540] train loss: 0.4037, train acc: 0.8191, val loss: 0.4017, val acc: 0.8196  (best train acc: 0.8420, best val acc: 0.8398, best train loss: 0.3618  @ epoch 28499 )\n",
      "[Epoch: 28560] train loss: 0.3797, train acc: 0.8323, val loss: 0.4027, val acc: 0.8263  (best train acc: 0.8420, best val acc: 0.8398, best train loss: 0.3618  @ epoch 28499 )\n",
      "[Epoch: 28580] train loss: 0.4005, train acc: 0.8232, val loss: 0.4005, val acc: 0.8209  (best train acc: 0.8420, best val acc: 0.8398, best train loss: 0.3618  @ epoch 28499 )\n",
      "[Epoch: 28600] train loss: 0.3740, train acc: 0.8343, val loss: 0.4138, val acc: 0.8088  (best train acc: 0.8420, best val acc: 0.8398, best train loss: 0.3618  @ epoch 28499 )\n",
      "[Epoch: 28620] train loss: 0.3736, train acc: 0.8346, val loss: 0.3999, val acc: 0.8250  (best train acc: 0.8438, best val acc: 0.8398, best train loss: 0.3618  @ epoch 28499 )\n",
      "[Epoch: 28640] train loss: 0.3784, train acc: 0.8316, val loss: 0.3943, val acc: 0.8283  (best train acc: 0.8438, best val acc: 0.8398, best train loss: 0.3618  @ epoch 28499 )\n",
      "[Epoch: 28660] train loss: 0.3801, train acc: 0.8293, val loss: 0.4098, val acc: 0.8098  (best train acc: 0.8438, best val acc: 0.8398, best train loss: 0.3618  @ epoch 28499 )\n",
      "[Epoch: 28680] train loss: 0.3971, train acc: 0.8252, val loss: 0.4133, val acc: 0.8138  (best train acc: 0.8438, best val acc: 0.8398, best train loss: 0.3618  @ epoch 28499 )\n",
      "[Epoch: 28700] train loss: 0.4155, train acc: 0.8186, val loss: 0.4066, val acc: 0.8074  (best train acc: 0.8438, best val acc: 0.8405, best train loss: 0.3618  @ epoch 28499 )\n",
      "[Epoch: 28720] train loss: 0.3973, train acc: 0.8250, val loss: 0.4113, val acc: 0.8219  (best train acc: 0.8438, best val acc: 0.8405, best train loss: 0.3618  @ epoch 28499 )\n",
      "[Epoch: 28740] train loss: 0.4041, train acc: 0.8191, val loss: 0.4044, val acc: 0.8172  (best train acc: 0.8438, best val acc: 0.8405, best train loss: 0.3615  @ epoch 28739 )\n",
      "[Epoch: 28760] train loss: 0.3906, train acc: 0.8262, val loss: 0.4186, val acc: 0.8078  (best train acc: 0.8438, best val acc: 0.8405, best train loss: 0.3615  @ epoch 28739 )\n",
      "[Epoch: 28780] train loss: 0.3787, train acc: 0.8317, val loss: 0.4140, val acc: 0.8101  (best train acc: 0.8438, best val acc: 0.8405, best train loss: 0.3615  @ epoch 28739 )\n",
      "[Epoch: 28800] train loss: 0.3788, train acc: 0.8319, val loss: 0.3939, val acc: 0.8243  (best train acc: 0.8438, best val acc: 0.8405, best train loss: 0.3615  @ epoch 28739 )\n",
      "[Epoch: 28820] train loss: 0.3721, train acc: 0.8354, val loss: 0.4110, val acc: 0.8091  (best train acc: 0.8438, best val acc: 0.8405, best train loss: 0.3615  @ epoch 28739 )\n",
      "[Epoch: 28840] train loss: 0.3854, train acc: 0.8266, val loss: 0.4117, val acc: 0.8142  (best train acc: 0.8438, best val acc: 0.8405, best train loss: 0.3615  @ epoch 28739 )\n",
      "[Epoch: 28860] train loss: 0.3826, train acc: 0.8344, val loss: 0.3915, val acc: 0.8327  (best train acc: 0.8438, best val acc: 0.8405, best train loss: 0.3615  @ epoch 28739 )\n",
      "[Epoch: 28880] train loss: 0.3831, train acc: 0.8259, val loss: 0.3958, val acc: 0.8371  (best train acc: 0.8438, best val acc: 0.8405, best train loss: 0.3615  @ epoch 28739 )\n",
      "[Epoch: 28900] train loss: 0.3765, train acc: 0.8369, val loss: 0.4186, val acc: 0.8061  (best train acc: 0.8438, best val acc: 0.8405, best train loss: 0.3615  @ epoch 28739 )\n",
      "[Epoch: 28920] train loss: 0.3874, train acc: 0.8278, val loss: 0.4135, val acc: 0.8064  (best train acc: 0.8438, best val acc: 0.8405, best train loss: 0.3615  @ epoch 28739 )\n",
      "[Epoch: 28940] train loss: 0.3665, train acc: 0.8370, val loss: 0.4123, val acc: 0.8179  (best train acc: 0.8438, best val acc: 0.8405, best train loss: 0.3615  @ epoch 28739 )\n",
      "[Epoch: 28960] train loss: 0.3901, train acc: 0.8220, val loss: 0.4189, val acc: 0.8064  (best train acc: 0.8438, best val acc: 0.8405, best train loss: 0.3615  @ epoch 28739 )\n",
      "[Epoch: 28980] train loss: 0.3750, train acc: 0.8340, val loss: 0.4018, val acc: 0.8192  (best train acc: 0.8438, best val acc: 0.8405, best train loss: 0.3615  @ epoch 28739 )\n",
      "[Epoch: 29000] train loss: 0.3720, train acc: 0.8347, val loss: 0.3992, val acc: 0.8266  (best train acc: 0.8438, best val acc: 0.8405, best train loss: 0.3615  @ epoch 28739 )\n",
      "[Epoch: 29020] train loss: 0.3929, train acc: 0.8287, val loss: 0.4099, val acc: 0.8111  (best train acc: 0.8438, best val acc: 0.8405, best train loss: 0.3615  @ epoch 28739 )\n",
      "[Epoch: 29040] train loss: 0.3810, train acc: 0.8302, val loss: 0.4205, val acc: 0.8037  (best train acc: 0.8438, best val acc: 0.8405, best train loss: 0.3615  @ epoch 28739 )\n",
      "[Epoch: 29060] train loss: 0.3759, train acc: 0.8326, val loss: 0.4210, val acc: 0.8054  (best train acc: 0.8438, best val acc: 0.8405, best train loss: 0.3615  @ epoch 28739 )\n",
      "[Epoch: 29080] train loss: 0.3829, train acc: 0.8354, val loss: 0.4209, val acc: 0.8078  (best train acc: 0.8438, best val acc: 0.8405, best train loss: 0.3615  @ epoch 28739 )\n",
      "[Epoch: 29100] train loss: 0.4075, train acc: 0.8226, val loss: 0.4111, val acc: 0.8256  (best train acc: 0.8438, best val acc: 0.8405, best train loss: 0.3615  @ epoch 28739 )\n",
      "[Epoch: 29120] train loss: 0.3718, train acc: 0.8369, val loss: 0.4082, val acc: 0.8091  (best train acc: 0.8438, best val acc: 0.8405, best train loss: 0.3615  @ epoch 28739 )\n",
      "[Epoch: 29140] train loss: 0.3640, train acc: 0.8415, val loss: 0.4104, val acc: 0.8263  (best train acc: 0.8438, best val acc: 0.8405, best train loss: 0.3615  @ epoch 28739 )\n",
      "[Epoch: 29160] train loss: 0.3907, train acc: 0.8250, val loss: 0.3954, val acc: 0.8280  (best train acc: 0.8438, best val acc: 0.8405, best train loss: 0.3615  @ epoch 28739 )\n",
      "[Epoch: 29180] train loss: 0.3877, train acc: 0.8285, val loss: 0.4151, val acc: 0.8105  (best train acc: 0.8438, best val acc: 0.8405, best train loss: 0.3615  @ epoch 28739 )\n",
      "[Epoch: 29200] train loss: 0.4456, train acc: 0.8049, val loss: 0.4300, val acc: 0.7970  (best train acc: 0.8438, best val acc: 0.8405, best train loss: 0.3615  @ epoch 28739 )\n",
      "[Epoch: 29220] train loss: 0.3970, train acc: 0.8248, val loss: 0.4418, val acc: 0.7936  (best train acc: 0.8438, best val acc: 0.8405, best train loss: 0.3615  @ epoch 28739 )\n",
      "[Epoch: 29240] train loss: 0.3932, train acc: 0.8289, val loss: 0.4234, val acc: 0.8017  (best train acc: 0.8438, best val acc: 0.8405, best train loss: 0.3615  @ epoch 28739 )\n",
      "[Epoch: 29260] train loss: 0.3909, train acc: 0.8266, val loss: 0.4159, val acc: 0.8024  (best train acc: 0.8438, best val acc: 0.8405, best train loss: 0.3615  @ epoch 28739 )\n",
      "[Epoch: 29280] train loss: 0.3845, train acc: 0.8277, val loss: 0.3978, val acc: 0.8223  (best train acc: 0.8438, best val acc: 0.8405, best train loss: 0.3615  @ epoch 28739 )\n",
      "[Epoch: 29300] train loss: 0.3767, train acc: 0.8316, val loss: 0.4002, val acc: 0.8135  (best train acc: 0.8438, best val acc: 0.8405, best train loss: 0.3615  @ epoch 28739 )\n",
      "[Epoch: 29320] train loss: 0.3758, train acc: 0.8356, val loss: 0.4124, val acc: 0.8091  (best train acc: 0.8438, best val acc: 0.8405, best train loss: 0.3615  @ epoch 28739 )\n",
      "[Epoch: 29340] train loss: 0.3950, train acc: 0.8229, val loss: 0.3874, val acc: 0.8307  (best train acc: 0.8438, best val acc: 0.8405, best train loss: 0.3615  @ epoch 28739 )\n",
      "[Epoch: 29360] train loss: 0.3831, train acc: 0.8299, val loss: 0.4175, val acc: 0.8040  (best train acc: 0.8438, best val acc: 0.8405, best train loss: 0.3615  @ epoch 28739 )\n",
      "[Epoch: 29380] train loss: 0.3719, train acc: 0.8352, val loss: 0.4046, val acc: 0.8145  (best train acc: 0.8438, best val acc: 0.8405, best train loss: 0.3615  @ epoch 28739 )\n",
      "[Epoch: 29400] train loss: 0.3897, train acc: 0.8274, val loss: 0.4137, val acc: 0.8057  (best train acc: 0.8438, best val acc: 0.8405, best train loss: 0.3615  @ epoch 28739 )\n",
      "[Epoch: 29420] train loss: 0.3874, train acc: 0.8315, val loss: 0.4130, val acc: 0.8179  (best train acc: 0.8438, best val acc: 0.8405, best train loss: 0.3615  @ epoch 28739 )\n",
      "[Epoch: 29440] train loss: 0.3880, train acc: 0.8262, val loss: 0.4337, val acc: 0.7956  (best train acc: 0.8438, best val acc: 0.8405, best train loss: 0.3615  @ epoch 28739 )\n",
      "[Epoch: 29460] train loss: 0.3722, train acc: 0.8352, val loss: 0.4125, val acc: 0.8064  (best train acc: 0.8438, best val acc: 0.8405, best train loss: 0.3615  @ epoch 28739 )\n",
      "[Epoch: 29480] train loss: 0.3733, train acc: 0.8326, val loss: 0.3944, val acc: 0.8324  (best train acc: 0.8438, best val acc: 0.8405, best train loss: 0.3615  @ epoch 28739 )\n",
      "[Epoch: 29500] train loss: 0.3748, train acc: 0.8373, val loss: 0.4161, val acc: 0.7997  (best train acc: 0.8438, best val acc: 0.8405, best train loss: 0.3615  @ epoch 28739 )\n",
      "[Epoch: 29520] train loss: 0.3929, train acc: 0.8281, val loss: 0.4146, val acc: 0.8013  (best train acc: 0.8438, best val acc: 0.8405, best train loss: 0.3615  @ epoch 28739 )\n",
      "[Epoch: 29540] train loss: 0.3930, train acc: 0.8286, val loss: 0.4134, val acc: 0.8091  (best train acc: 0.8438, best val acc: 0.8405, best train loss: 0.3615  @ epoch 28739 )\n",
      "[Epoch: 29560] train loss: 0.3991, train acc: 0.8229, val loss: 0.4098, val acc: 0.8135  (best train acc: 0.8438, best val acc: 0.8405, best train loss: 0.3615  @ epoch 28739 )\n",
      "[Epoch: 29580] train loss: 0.3760, train acc: 0.8324, val loss: 0.3957, val acc: 0.8314  (best train acc: 0.8438, best val acc: 0.8405, best train loss: 0.3615  @ epoch 28739 )\n",
      "[Epoch: 29600] train loss: 0.3858, train acc: 0.8245, val loss: 0.3898, val acc: 0.8364  (best train acc: 0.8438, best val acc: 0.8405, best train loss: 0.3615  @ epoch 28739 )\n",
      "[Epoch: 29620] train loss: 0.3967, train acc: 0.8219, val loss: 0.4098, val acc: 0.8094  (best train acc: 0.8438, best val acc: 0.8405, best train loss: 0.3615  @ epoch 28739 )\n",
      "[Epoch: 29640] train loss: 0.3838, train acc: 0.8304, val loss: 0.3925, val acc: 0.8270  (best train acc: 0.8438, best val acc: 0.8405, best train loss: 0.3615  @ epoch 28739 )\n",
      "[Epoch: 29660] train loss: 0.3968, train acc: 0.8226, val loss: 0.4182, val acc: 0.8067  (best train acc: 0.8438, best val acc: 0.8405, best train loss: 0.3615  @ epoch 28739 )\n",
      "[Epoch: 29680] train loss: 0.4175, train acc: 0.8188, val loss: 0.3999, val acc: 0.8283  (best train acc: 0.8438, best val acc: 0.8405, best train loss: 0.3615  @ epoch 28739 )\n",
      "[Epoch: 29700] train loss: 0.3798, train acc: 0.8332, val loss: 0.4170, val acc: 0.8051  (best train acc: 0.8438, best val acc: 0.8405, best train loss: 0.3615  @ epoch 28739 )\n",
      "[Epoch: 29720] train loss: 0.3715, train acc: 0.8363, val loss: 0.4247, val acc: 0.8071  (best train acc: 0.8438, best val acc: 0.8405, best train loss: 0.3615  @ epoch 28739 )\n",
      "[Epoch: 29740] train loss: 0.3874, train acc: 0.8284, val loss: 0.4450, val acc: 0.7902  (best train acc: 0.8438, best val acc: 0.8405, best train loss: 0.3615  @ epoch 28739 )\n",
      "[Epoch: 29760] train loss: 0.4011, train acc: 0.8185, val loss: 0.4195, val acc: 0.8088  (best train acc: 0.8438, best val acc: 0.8405, best train loss: 0.3615  @ epoch 28739 )\n",
      "[Epoch: 29780] train loss: 0.3661, train acc: 0.8377, val loss: 0.3968, val acc: 0.8280  (best train acc: 0.8438, best val acc: 0.8405, best train loss: 0.3615  @ epoch 28739 )\n",
      "[Epoch: 29800] train loss: 0.3864, train acc: 0.8287, val loss: 0.4075, val acc: 0.8290  (best train acc: 0.8438, best val acc: 0.8405, best train loss: 0.3615  @ epoch 28739 )\n",
      "[Epoch: 29820] train loss: 0.3824, train acc: 0.8280, val loss: 0.3984, val acc: 0.8327  (best train acc: 0.8438, best val acc: 0.8405, best train loss: 0.3615  @ epoch 28739 )\n",
      "[Epoch: 29840] train loss: 0.3932, train acc: 0.8235, val loss: 0.4562, val acc: 0.7845  (best train acc: 0.8438, best val acc: 0.8405, best train loss: 0.3615  @ epoch 28739 )\n",
      "[Epoch: 29860] train loss: 0.4081, train acc: 0.8201, val loss: 0.4281, val acc: 0.8034  (best train acc: 0.8438, best val acc: 0.8405, best train loss: 0.3615  @ epoch 28739 )\n",
      "[Epoch: 29880] train loss: 0.3867, train acc: 0.8325, val loss: 0.4095, val acc: 0.8152  (best train acc: 0.8438, best val acc: 0.8405, best train loss: 0.3615  @ epoch 28739 )\n",
      "[Epoch: 29900] train loss: 0.3924, train acc: 0.8263, val loss: 0.4209, val acc: 0.8071  (best train acc: 0.8438, best val acc: 0.8405, best train loss: 0.3615  @ epoch 28739 )\n",
      "[Epoch: 29920] train loss: 0.3885, train acc: 0.8291, val loss: 0.4110, val acc: 0.8175  (best train acc: 0.8438, best val acc: 0.8405, best train loss: 0.3615  @ epoch 28739 )\n",
      "[Epoch: 29940] train loss: 0.3775, train acc: 0.8331, val loss: 0.4028, val acc: 0.8165  (best train acc: 0.8438, best val acc: 0.8405, best train loss: 0.3615  @ epoch 28739 )\n",
      "[Epoch: 29960] train loss: 0.3989, train acc: 0.8206, val loss: 0.4323, val acc: 0.7943  (best train acc: 0.8438, best val acc: 0.8405, best train loss: 0.3615  @ epoch 28739 )\n",
      "[Epoch: 29980] train loss: 0.3775, train acc: 0.8310, val loss: 0.4039, val acc: 0.8223  (best train acc: 0.8438, best val acc: 0.8405, best train loss: 0.3615  @ epoch 28739 )\n",
      "[Epoch: 30000] train loss: 0.3808, train acc: 0.8357, val loss: 0.3998, val acc: 0.8229  (best train acc: 0.8438, best val acc: 0.8405, best train loss: 0.3615  @ epoch 28739 )\n",
      "[Epoch: 30020] train loss: 0.3909, train acc: 0.8245, val loss: 0.4159, val acc: 0.8067  (best train acc: 0.8438, best val acc: 0.8405, best train loss: 0.3615  @ epoch 28739 )\n",
      "[Epoch: 30040] train loss: 0.3875, train acc: 0.8280, val loss: 0.4238, val acc: 0.8044  (best train acc: 0.8438, best val acc: 0.8405, best train loss: 0.3615  @ epoch 28739 )\n",
      "[Epoch: 30060] train loss: 0.3710, train acc: 0.8325, val loss: 0.4088, val acc: 0.8155  (best train acc: 0.8438, best val acc: 0.8405, best train loss: 0.3615  @ epoch 28739 )\n",
      "[Epoch: 30080] train loss: 0.3665, train acc: 0.8347, val loss: 0.4116, val acc: 0.8071  (best train acc: 0.8438, best val acc: 0.8405, best train loss: 0.3615  @ epoch 28739 )\n",
      "[Epoch: 30100] train loss: 0.4008, train acc: 0.8250, val loss: 0.3979, val acc: 0.8331  (best train acc: 0.8438, best val acc: 0.8405, best train loss: 0.3615  @ epoch 28739 )\n",
      "[Epoch: 30120] train loss: 0.3783, train acc: 0.8294, val loss: 0.3934, val acc: 0.8250  (best train acc: 0.8438, best val acc: 0.8405, best train loss: 0.3615  @ epoch 28739 )\n",
      "[Epoch: 30140] train loss: 0.3896, train acc: 0.8300, val loss: 0.3938, val acc: 0.8185  (best train acc: 0.8438, best val acc: 0.8405, best train loss: 0.3615  @ epoch 28739 )\n",
      "[Epoch: 30160] train loss: 0.3794, train acc: 0.8306, val loss: 0.4063, val acc: 0.8175  (best train acc: 0.8438, best val acc: 0.8405, best train loss: 0.3615  @ epoch 28739 )\n",
      "[Epoch: 30180] train loss: 0.3928, train acc: 0.8269, val loss: 0.4143, val acc: 0.8128  (best train acc: 0.8438, best val acc: 0.8405, best train loss: 0.3615  @ epoch 28739 )\n",
      "[Epoch: 30200] train loss: 0.3816, train acc: 0.8314, val loss: 0.4216, val acc: 0.8064  (best train acc: 0.8438, best val acc: 0.8405, best train loss: 0.3615  @ epoch 28739 )\n",
      "[Epoch: 30220] train loss: 0.3751, train acc: 0.8357, val loss: 0.4020, val acc: 0.8236  (best train acc: 0.8438, best val acc: 0.8405, best train loss: 0.3615  @ epoch 28739 )\n",
      "[Epoch: 30240] train loss: 0.3846, train acc: 0.8311, val loss: 0.4148, val acc: 0.8013  (best train acc: 0.8438, best val acc: 0.8405, best train loss: 0.3615  @ epoch 28739 )\n",
      "[Epoch: 30260] train loss: 0.4070, train acc: 0.8229, val loss: 0.4175, val acc: 0.8078  (best train acc: 0.8438, best val acc: 0.8405, best train loss: 0.3615  @ epoch 28739 )\n",
      "[Epoch: 30280] train loss: 0.3893, train acc: 0.8312, val loss: 0.4062, val acc: 0.8182  (best train acc: 0.8438, best val acc: 0.8405, best train loss: 0.3615  @ epoch 28739 )\n",
      "[Epoch: 30300] train loss: 0.3748, train acc: 0.8372, val loss: 0.4037, val acc: 0.8165  (best train acc: 0.8438, best val acc: 0.8405, best train loss: 0.3613  @ epoch 30295 )\n",
      "[Epoch: 30320] train loss: 0.3702, train acc: 0.8374, val loss: 0.4151, val acc: 0.8061  (best train acc: 0.8452, best val acc: 0.8405, best train loss: 0.3569  @ epoch 30312 )\n",
      "[Epoch: 30340] train loss: 0.3646, train acc: 0.8386, val loss: 0.3940, val acc: 0.8266  (best train acc: 0.8452, best val acc: 0.8405, best train loss: 0.3569  @ epoch 30312 )\n",
      "[Epoch: 30360] train loss: 0.3715, train acc: 0.8334, val loss: 0.3977, val acc: 0.8226  (best train acc: 0.8452, best val acc: 0.8405, best train loss: 0.3569  @ epoch 30312 )\n",
      "[Epoch: 30380] train loss: 0.3954, train acc: 0.8205, val loss: 0.4216, val acc: 0.8078  (best train acc: 0.8452, best val acc: 0.8405, best train loss: 0.3569  @ epoch 30312 )\n",
      "[Epoch: 30400] train loss: 0.3737, train acc: 0.8359, val loss: 0.4020, val acc: 0.8260  (best train acc: 0.8452, best val acc: 0.8405, best train loss: 0.3569  @ epoch 30312 )\n",
      "[Epoch: 30420] train loss: 0.3748, train acc: 0.8334, val loss: 0.3946, val acc: 0.8337  (best train acc: 0.8452, best val acc: 0.8408, best train loss: 0.3569  @ epoch 30312 )\n",
      "[Epoch: 30440] train loss: 0.3814, train acc: 0.8319, val loss: 0.4112, val acc: 0.8125  (best train acc: 0.8452, best val acc: 0.8408, best train loss: 0.3569  @ epoch 30312 )\n",
      "[Epoch: 30460] train loss: 0.3892, train acc: 0.8308, val loss: 0.4182, val acc: 0.8034  (best train acc: 0.8452, best val acc: 0.8408, best train loss: 0.3569  @ epoch 30312 )\n",
      "[Epoch: 30480] train loss: 0.3709, train acc: 0.8390, val loss: 0.4178, val acc: 0.8030  (best train acc: 0.8452, best val acc: 0.8408, best train loss: 0.3569  @ epoch 30312 )\n",
      "[Epoch: 30500] train loss: 0.4015, train acc: 0.8188, val loss: 0.4035, val acc: 0.8189  (best train acc: 0.8452, best val acc: 0.8408, best train loss: 0.3569  @ epoch 30312 )\n",
      "[Epoch: 30520] train loss: 0.3915, train acc: 0.8280, val loss: 0.4033, val acc: 0.8246  (best train acc: 0.8452, best val acc: 0.8408, best train loss: 0.3569  @ epoch 30312 )\n",
      "[Epoch: 30540] train loss: 0.3788, train acc: 0.8312, val loss: 0.4194, val acc: 0.8040  (best train acc: 0.8452, best val acc: 0.8408, best train loss: 0.3569  @ epoch 30312 )\n",
      "[Epoch: 30560] train loss: 0.3781, train acc: 0.8301, val loss: 0.4201, val acc: 0.8017  (best train acc: 0.8452, best val acc: 0.8408, best train loss: 0.3569  @ epoch 30312 )\n",
      "[Epoch: 30580] train loss: 0.3734, train acc: 0.8323, val loss: 0.4017, val acc: 0.8212  (best train acc: 0.8452, best val acc: 0.8408, best train loss: 0.3569  @ epoch 30312 )\n",
      "[Epoch: 30600] train loss: 0.3597, train acc: 0.8376, val loss: 0.4072, val acc: 0.8219  (best train acc: 0.8452, best val acc: 0.8408, best train loss: 0.3569  @ epoch 30312 )\n",
      "[Epoch: 30620] train loss: 0.3769, train acc: 0.8358, val loss: 0.4197, val acc: 0.8165  (best train acc: 0.8452, best val acc: 0.8408, best train loss: 0.3569  @ epoch 30312 )\n",
      "[Epoch: 30640] train loss: 0.3788, train acc: 0.8339, val loss: 0.3973, val acc: 0.8219  (best train acc: 0.8452, best val acc: 0.8408, best train loss: 0.3569  @ epoch 30312 )\n",
      "[Epoch: 30660] train loss: 0.3850, train acc: 0.8328, val loss: 0.3923, val acc: 0.8307  (best train acc: 0.8452, best val acc: 0.8408, best train loss: 0.3569  @ epoch 30312 )\n",
      "[Epoch: 30680] train loss: 0.3792, train acc: 0.8320, val loss: 0.4107, val acc: 0.8105  (best train acc: 0.8452, best val acc: 0.8408, best train loss: 0.3569  @ epoch 30312 )\n",
      "[Epoch: 30700] train loss: 0.3788, train acc: 0.8288, val loss: 0.3929, val acc: 0.8212  (best train acc: 0.8452, best val acc: 0.8408, best train loss: 0.3569  @ epoch 30312 )\n",
      "[Epoch: 30720] train loss: 0.3672, train acc: 0.8412, val loss: 0.3903, val acc: 0.8341  (best train acc: 0.8452, best val acc: 0.8408, best train loss: 0.3569  @ epoch 30312 )\n",
      "[Epoch: 30740] train loss: 0.3713, train acc: 0.8345, val loss: 0.4024, val acc: 0.8206  (best train acc: 0.8452, best val acc: 0.8408, best train loss: 0.3569  @ epoch 30312 )\n",
      "[Epoch: 30760] train loss: 0.3705, train acc: 0.8344, val loss: 0.4107, val acc: 0.8135  (best train acc: 0.8452, best val acc: 0.8408, best train loss: 0.3569  @ epoch 30312 )\n",
      "[Epoch: 30780] train loss: 0.4076, train acc: 0.8195, val loss: 0.4082, val acc: 0.8236  (best train acc: 0.8452, best val acc: 0.8408, best train loss: 0.3569  @ epoch 30312 )\n",
      "[Epoch: 30800] train loss: 0.3911, train acc: 0.8262, val loss: 0.4090, val acc: 0.8101  (best train acc: 0.8452, best val acc: 0.8408, best train loss: 0.3569  @ epoch 30312 )\n",
      "[Epoch: 30820] train loss: 0.3681, train acc: 0.8337, val loss: 0.4038, val acc: 0.8182  (best train acc: 0.8452, best val acc: 0.8408, best train loss: 0.3569  @ epoch 30312 )\n",
      "[Epoch: 30840] train loss: 0.3834, train acc: 0.8284, val loss: 0.4167, val acc: 0.8040  (best train acc: 0.8452, best val acc: 0.8408, best train loss: 0.3569  @ epoch 30312 )\n",
      "[Epoch: 30860] train loss: 0.3693, train acc: 0.8376, val loss: 0.4031, val acc: 0.8239  (best train acc: 0.8452, best val acc: 0.8408, best train loss: 0.3569  @ epoch 30312 )\n",
      "[Epoch: 30880] train loss: 0.3787, train acc: 0.8287, val loss: 0.3964, val acc: 0.8297  (best train acc: 0.8452, best val acc: 0.8408, best train loss: 0.3569  @ epoch 30312 )\n",
      "[Epoch: 30900] train loss: 0.3864, train acc: 0.8266, val loss: 0.4090, val acc: 0.8098  (best train acc: 0.8452, best val acc: 0.8408, best train loss: 0.3569  @ epoch 30312 )\n",
      "[Epoch: 30920] train loss: 0.3800, train acc: 0.8318, val loss: 0.4014, val acc: 0.8159  (best train acc: 0.8452, best val acc: 0.8408, best train loss: 0.3569  @ epoch 30312 )\n",
      "[Epoch: 30940] train loss: 0.3691, train acc: 0.8337, val loss: 0.4092, val acc: 0.8155  (best train acc: 0.8452, best val acc: 0.8408, best train loss: 0.3569  @ epoch 30312 )\n",
      "[Epoch: 30960] train loss: 0.3802, train acc: 0.8339, val loss: 0.3901, val acc: 0.8358  (best train acc: 0.8452, best val acc: 0.8408, best train loss: 0.3569  @ epoch 30312 )\n",
      "[Epoch: 30980] train loss: 0.3864, train acc: 0.8252, val loss: 0.3935, val acc: 0.8260  (best train acc: 0.8452, best val acc: 0.8408, best train loss: 0.3569  @ epoch 30312 )\n",
      "[Epoch: 31000] train loss: 0.3827, train acc: 0.8314, val loss: 0.4073, val acc: 0.8145  (best train acc: 0.8452, best val acc: 0.8408, best train loss: 0.3569  @ epoch 30312 )\n",
      "[Epoch: 31020] train loss: 0.3685, train acc: 0.8382, val loss: 0.4075, val acc: 0.8125  (best train acc: 0.8452, best val acc: 0.8408, best train loss: 0.3569  @ epoch 30312 )\n",
      "[Epoch: 31040] train loss: 0.3900, train acc: 0.8281, val loss: 0.4123, val acc: 0.8138  (best train acc: 0.8452, best val acc: 0.8408, best train loss: 0.3569  @ epoch 30312 )\n",
      "[Epoch: 31060] train loss: 0.3776, train acc: 0.8340, val loss: 0.4023, val acc: 0.8250  (best train acc: 0.8452, best val acc: 0.8408, best train loss: 0.3569  @ epoch 30312 )\n",
      "[Epoch: 31080] train loss: 0.3664, train acc: 0.8357, val loss: 0.4252, val acc: 0.8051  (best train acc: 0.8452, best val acc: 0.8408, best train loss: 0.3569  @ epoch 30312 )\n",
      "[Epoch: 31100] train loss: 0.3825, train acc: 0.8336, val loss: 0.4068, val acc: 0.8165  (best train acc: 0.8452, best val acc: 0.8408, best train loss: 0.3569  @ epoch 30312 )\n",
      "[Epoch: 31120] train loss: 0.3703, train acc: 0.8371, val loss: 0.4149, val acc: 0.8078  (best train acc: 0.8452, best val acc: 0.8408, best train loss: 0.3569  @ epoch 30312 )\n",
      "[Epoch: 31140] train loss: 0.3970, train acc: 0.8290, val loss: 0.4111, val acc: 0.8132  (best train acc: 0.8452, best val acc: 0.8408, best train loss: 0.3569  @ epoch 30312 )\n",
      "[Epoch: 31160] train loss: 0.3751, train acc: 0.8329, val loss: 0.4074, val acc: 0.8148  (best train acc: 0.8452, best val acc: 0.8408, best train loss: 0.3569  @ epoch 30312 )\n",
      "[Epoch: 31180] train loss: 0.3928, train acc: 0.8276, val loss: 0.3928, val acc: 0.8239  (best train acc: 0.8452, best val acc: 0.8408, best train loss: 0.3569  @ epoch 30312 )\n",
      "[Epoch: 31200] train loss: 0.3965, train acc: 0.8271, val loss: 0.4198, val acc: 0.8064  (best train acc: 0.8452, best val acc: 0.8408, best train loss: 0.3569  @ epoch 30312 )\n",
      "[Epoch: 31220] train loss: 0.3760, train acc: 0.8316, val loss: 0.4195, val acc: 0.8017  (best train acc: 0.8452, best val acc: 0.8408, best train loss: 0.3569  @ epoch 30312 )\n",
      "[Epoch: 31240] train loss: 0.3658, train acc: 0.8382, val loss: 0.4102, val acc: 0.8239  (best train acc: 0.8452, best val acc: 0.8408, best train loss: 0.3569  @ epoch 30312 )\n",
      "[Epoch: 31260] train loss: 0.3706, train acc: 0.8357, val loss: 0.4005, val acc: 0.8280  (best train acc: 0.8452, best val acc: 0.8408, best train loss: 0.3569  @ epoch 30312 )\n",
      "[Epoch: 31280] train loss: 0.3721, train acc: 0.8366, val loss: 0.3994, val acc: 0.8310  (best train acc: 0.8452, best val acc: 0.8408, best train loss: 0.3569  @ epoch 30312 )\n",
      "[Epoch: 31300] train loss: 0.3797, train acc: 0.8345, val loss: 0.3863, val acc: 0.8314  (best train acc: 0.8452, best val acc: 0.8408, best train loss: 0.3569  @ epoch 30312 )\n",
      "[Epoch: 31320] train loss: 0.3670, train acc: 0.8336, val loss: 0.4116, val acc: 0.8057  (best train acc: 0.8452, best val acc: 0.8408, best train loss: 0.3569  @ epoch 30312 )\n",
      "[Epoch: 31340] train loss: 0.3903, train acc: 0.8296, val loss: 0.4162, val acc: 0.8061  (best train acc: 0.8452, best val acc: 0.8408, best train loss: 0.3569  @ epoch 30312 )\n",
      "[Epoch: 31360] train loss: 0.3631, train acc: 0.8359, val loss: 0.4091, val acc: 0.8135  (best train acc: 0.8452, best val acc: 0.8408, best train loss: 0.3569  @ epoch 30312 )\n",
      "[Epoch: 31380] train loss: 0.3742, train acc: 0.8292, val loss: 0.3967, val acc: 0.8277  (best train acc: 0.8452, best val acc: 0.8408, best train loss: 0.3569  @ epoch 30312 )\n",
      "[Epoch: 31400] train loss: 0.3948, train acc: 0.8303, val loss: 0.4097, val acc: 0.8081  (best train acc: 0.8452, best val acc: 0.8408, best train loss: 0.3569  @ epoch 30312 )\n",
      "[Epoch: 31420] train loss: 0.4034, train acc: 0.8261, val loss: 0.4075, val acc: 0.8226  (best train acc: 0.8452, best val acc: 0.8408, best train loss: 0.3569  @ epoch 30312 )\n",
      "[Epoch: 31440] train loss: 0.3693, train acc: 0.8363, val loss: 0.4048, val acc: 0.8304  (best train acc: 0.8452, best val acc: 0.8408, best train loss: 0.3569  @ epoch 30312 )\n",
      "[Epoch: 31460] train loss: 0.3931, train acc: 0.8250, val loss: 0.4252, val acc: 0.7997  (best train acc: 0.8452, best val acc: 0.8422, best train loss: 0.3569  @ epoch 30312 )\n",
      "[Epoch: 31480] train loss: 0.3644, train acc: 0.8355, val loss: 0.4143, val acc: 0.8179  (best train acc: 0.8452, best val acc: 0.8422, best train loss: 0.3569  @ epoch 30312 )\n",
      "[Epoch: 31500] train loss: 0.3608, train acc: 0.8387, val loss: 0.4133, val acc: 0.8236  (best train acc: 0.8454, best val acc: 0.8422, best train loss: 0.3569  @ epoch 30312 )\n",
      "[Epoch: 31520] train loss: 0.3894, train acc: 0.8300, val loss: 0.4070, val acc: 0.8152  (best train acc: 0.8454, best val acc: 0.8422, best train loss: 0.3569  @ epoch 30312 )\n",
      "[Epoch: 31540] train loss: 0.3951, train acc: 0.8230, val loss: 0.4412, val acc: 0.7960  (best train acc: 0.8469, best val acc: 0.8422, best train loss: 0.3534  @ epoch 31526 )\n",
      "[Epoch: 31560] train loss: 0.3761, train acc: 0.8352, val loss: 0.4150, val acc: 0.8061  (best train acc: 0.8469, best val acc: 0.8422, best train loss: 0.3534  @ epoch 31526 )\n",
      "[Epoch: 31580] train loss: 0.3664, train acc: 0.8406, val loss: 0.3990, val acc: 0.8283  (best train acc: 0.8469, best val acc: 0.8422, best train loss: 0.3534  @ epoch 31526 )\n",
      "[Epoch: 31600] train loss: 0.3716, train acc: 0.8358, val loss: 0.4018, val acc: 0.8300  (best train acc: 0.8469, best val acc: 0.8422, best train loss: 0.3534  @ epoch 31526 )\n",
      "[Epoch: 31620] train loss: 0.4004, train acc: 0.8229, val loss: 0.4489, val acc: 0.7835  (best train acc: 0.8469, best val acc: 0.8422, best train loss: 0.3534  @ epoch 31526 )\n",
      "[Epoch: 31640] train loss: 0.3883, train acc: 0.8302, val loss: 0.4414, val acc: 0.8007  (best train acc: 0.8469, best val acc: 0.8422, best train loss: 0.3534  @ epoch 31526 )\n",
      "[Epoch: 31660] train loss: 0.3754, train acc: 0.8349, val loss: 0.4040, val acc: 0.8270  (best train acc: 0.8469, best val acc: 0.8422, best train loss: 0.3534  @ epoch 31526 )\n",
      "[Epoch: 31680] train loss: 0.3791, train acc: 0.8355, val loss: 0.4079, val acc: 0.8263  (best train acc: 0.8469, best val acc: 0.8422, best train loss: 0.3534  @ epoch 31526 )\n",
      "[Epoch: 31700] train loss: 0.3871, train acc: 0.8317, val loss: 0.4040, val acc: 0.8226  (best train acc: 0.8469, best val acc: 0.8422, best train loss: 0.3534  @ epoch 31526 )\n",
      "[Epoch: 31720] train loss: 0.3773, train acc: 0.8316, val loss: 0.4019, val acc: 0.8239  (best train acc: 0.8469, best val acc: 0.8422, best train loss: 0.3534  @ epoch 31526 )\n",
      "[Epoch: 31740] train loss: 0.3893, train acc: 0.8335, val loss: 0.4123, val acc: 0.8111  (best train acc: 0.8469, best val acc: 0.8422, best train loss: 0.3534  @ epoch 31526 )\n",
      "[Epoch: 31760] train loss: 0.3995, train acc: 0.8219, val loss: 0.4343, val acc: 0.8020  (best train acc: 0.8469, best val acc: 0.8422, best train loss: 0.3534  @ epoch 31526 )\n",
      "[Epoch: 31780] train loss: 0.3697, train acc: 0.8386, val loss: 0.3975, val acc: 0.8256  (best train acc: 0.8469, best val acc: 0.8422, best train loss: 0.3534  @ epoch 31526 )\n",
      "[Epoch: 31800] train loss: 0.3909, train acc: 0.8261, val loss: 0.4055, val acc: 0.8175  (best train acc: 0.8469, best val acc: 0.8422, best train loss: 0.3534  @ epoch 31526 )\n",
      "[Epoch: 31820] train loss: 0.3637, train acc: 0.8386, val loss: 0.4041, val acc: 0.8250  (best train acc: 0.8469, best val acc: 0.8422, best train loss: 0.3534  @ epoch 31526 )\n",
      "[Epoch: 31840] train loss: 0.3925, train acc: 0.8241, val loss: 0.3988, val acc: 0.8250  (best train acc: 0.8469, best val acc: 0.8422, best train loss: 0.3534  @ epoch 31526 )\n",
      "[Epoch: 31860] train loss: 0.3815, train acc: 0.8342, val loss: 0.4022, val acc: 0.8246  (best train acc: 0.8469, best val acc: 0.8422, best train loss: 0.3534  @ epoch 31526 )\n",
      "[Epoch: 31880] train loss: 0.3810, train acc: 0.8319, val loss: 0.4081, val acc: 0.8192  (best train acc: 0.8469, best val acc: 0.8422, best train loss: 0.3534  @ epoch 31526 )\n",
      "[Epoch: 31900] train loss: 0.4225, train acc: 0.8145, val loss: 0.4200, val acc: 0.8007  (best train acc: 0.8469, best val acc: 0.8422, best train loss: 0.3534  @ epoch 31526 )\n",
      "[Epoch: 31920] train loss: 0.3810, train acc: 0.8313, val loss: 0.4228, val acc: 0.8054  (best train acc: 0.8469, best val acc: 0.8422, best train loss: 0.3534  @ epoch 31526 )\n",
      "[Epoch: 31940] train loss: 0.3688, train acc: 0.8379, val loss: 0.4326, val acc: 0.7963  (best train acc: 0.8469, best val acc: 0.8422, best train loss: 0.3534  @ epoch 31526 )\n",
      "[Epoch: 31960] train loss: 0.3731, train acc: 0.8333, val loss: 0.3969, val acc: 0.8250  (best train acc: 0.8469, best val acc: 0.8422, best train loss: 0.3534  @ epoch 31526 )\n",
      "[Epoch: 31980] train loss: 0.3778, train acc: 0.8376, val loss: 0.4029, val acc: 0.8239  (best train acc: 0.8469, best val acc: 0.8422, best train loss: 0.3534  @ epoch 31526 )\n",
      "[Epoch: 32000] train loss: 0.3900, train acc: 0.8305, val loss: 0.4121, val acc: 0.8067  (best train acc: 0.8469, best val acc: 0.8422, best train loss: 0.3534  @ epoch 31526 )\n",
      "[Epoch: 32020] train loss: 0.3737, train acc: 0.8348, val loss: 0.4120, val acc: 0.8128  (best train acc: 0.8469, best val acc: 0.8422, best train loss: 0.3534  @ epoch 31526 )\n",
      "[Epoch: 32040] train loss: 0.3936, train acc: 0.8329, val loss: 0.4130, val acc: 0.8108  (best train acc: 0.8469, best val acc: 0.8422, best train loss: 0.3534  @ epoch 31526 )\n",
      "[Epoch: 32060] train loss: 0.3863, train acc: 0.8251, val loss: 0.4248, val acc: 0.8132  (best train acc: 0.8469, best val acc: 0.8422, best train loss: 0.3534  @ epoch 31526 )\n",
      "[Epoch: 32080] train loss: 0.3839, train acc: 0.8296, val loss: 0.4259, val acc: 0.8020  (best train acc: 0.8469, best val acc: 0.8422, best train loss: 0.3534  @ epoch 31526 )\n",
      "[Epoch: 32100] train loss: 0.3718, train acc: 0.8352, val loss: 0.4023, val acc: 0.8152  (best train acc: 0.8469, best val acc: 0.8422, best train loss: 0.3534  @ epoch 31526 )\n",
      "[Epoch: 32120] train loss: 0.3841, train acc: 0.8289, val loss: 0.4006, val acc: 0.8226  (best train acc: 0.8469, best val acc: 0.8422, best train loss: 0.3534  @ epoch 31526 )\n",
      "[Epoch: 32140] train loss: 0.3971, train acc: 0.8251, val loss: 0.4171, val acc: 0.8030  (best train acc: 0.8469, best val acc: 0.8422, best train loss: 0.3534  @ epoch 31526 )\n",
      "[Epoch: 32160] train loss: 0.3869, train acc: 0.8300, val loss: 0.4022, val acc: 0.8142  (best train acc: 0.8469, best val acc: 0.8422, best train loss: 0.3534  @ epoch 31526 )\n",
      "[Epoch: 32180] train loss: 0.3845, train acc: 0.8305, val loss: 0.4316, val acc: 0.8020  (best train acc: 0.8469, best val acc: 0.8422, best train loss: 0.3534  @ epoch 31526 )\n",
      "[Epoch: 32200] train loss: 0.3869, train acc: 0.8250, val loss: 0.4078, val acc: 0.8250  (best train acc: 0.8486, best val acc: 0.8422, best train loss: 0.3514  @ epoch 32189 )\n",
      "[Epoch: 32220] train loss: 0.3695, train acc: 0.8382, val loss: 0.4060, val acc: 0.8199  (best train acc: 0.8486, best val acc: 0.8422, best train loss: 0.3514  @ epoch 32189 )\n",
      "[Epoch: 32240] train loss: 0.3848, train acc: 0.8302, val loss: 0.4266, val acc: 0.8027  (best train acc: 0.8486, best val acc: 0.8422, best train loss: 0.3514  @ epoch 32189 )\n",
      "[Epoch: 32260] train loss: 0.3675, train acc: 0.8378, val loss: 0.4012, val acc: 0.8314  (best train acc: 0.8486, best val acc: 0.8422, best train loss: 0.3514  @ epoch 32189 )\n",
      "[Epoch: 32280] train loss: 0.3574, train acc: 0.8412, val loss: 0.4055, val acc: 0.8250  (best train acc: 0.8486, best val acc: 0.8422, best train loss: 0.3514  @ epoch 32189 )\n",
      "[Epoch: 32300] train loss: 0.3670, train acc: 0.8374, val loss: 0.3965, val acc: 0.8283  (best train acc: 0.8486, best val acc: 0.8422, best train loss: 0.3514  @ epoch 32189 )\n",
      "[Epoch: 32320] train loss: 0.3695, train acc: 0.8400, val loss: 0.4107, val acc: 0.8189  (best train acc: 0.8486, best val acc: 0.8422, best train loss: 0.3514  @ epoch 32189 )\n",
      "[Epoch: 32340] train loss: 0.3728, train acc: 0.8359, val loss: 0.4098, val acc: 0.8074  (best train acc: 0.8486, best val acc: 0.8422, best train loss: 0.3514  @ epoch 32189 )\n",
      "[Epoch: 32360] train loss: 0.3716, train acc: 0.8349, val loss: 0.3973, val acc: 0.8297  (best train acc: 0.8486, best val acc: 0.8422, best train loss: 0.3514  @ epoch 32189 )\n",
      "[Epoch: 32380] train loss: 0.3820, train acc: 0.8339, val loss: 0.4073, val acc: 0.8105  (best train acc: 0.8486, best val acc: 0.8422, best train loss: 0.3514  @ epoch 32189 )\n",
      "[Epoch: 32400] train loss: 0.3860, train acc: 0.8281, val loss: 0.4076, val acc: 0.8162  (best train acc: 0.8486, best val acc: 0.8422, best train loss: 0.3514  @ epoch 32189 )\n",
      "[Epoch: 32420] train loss: 0.3725, train acc: 0.8368, val loss: 0.4207, val acc: 0.8047  (best train acc: 0.8486, best val acc: 0.8422, best train loss: 0.3514  @ epoch 32189 )\n",
      "[Epoch: 32440] train loss: 0.3681, train acc: 0.8384, val loss: 0.4105, val acc: 0.8182  (best train acc: 0.8486, best val acc: 0.8422, best train loss: 0.3514  @ epoch 32189 )\n",
      "[Epoch: 32460] train loss: 0.3762, train acc: 0.8319, val loss: 0.4054, val acc: 0.8317  (best train acc: 0.8486, best val acc: 0.8422, best train loss: 0.3514  @ epoch 32189 )\n",
      "[Epoch: 32480] train loss: 0.3855, train acc: 0.8313, val loss: 0.4361, val acc: 0.7987  (best train acc: 0.8486, best val acc: 0.8422, best train loss: 0.3514  @ epoch 32189 )\n",
      "[Epoch: 32500] train loss: 0.3634, train acc: 0.8389, val loss: 0.4138, val acc: 0.8152  (best train acc: 0.8486, best val acc: 0.8422, best train loss: 0.3514  @ epoch 32189 )\n",
      "[Epoch: 32520] train loss: 0.3776, train acc: 0.8308, val loss: 0.4216, val acc: 0.8111  (best train acc: 0.8486, best val acc: 0.8422, best train loss: 0.3514  @ epoch 32189 )\n",
      "[Epoch: 32540] train loss: 0.3862, train acc: 0.8273, val loss: 0.4031, val acc: 0.8270  (best train acc: 0.8486, best val acc: 0.8422, best train loss: 0.3514  @ epoch 32189 )\n",
      "[Epoch: 32560] train loss: 0.3832, train acc: 0.8327, val loss: 0.4057, val acc: 0.8148  (best train acc: 0.8486, best val acc: 0.8422, best train loss: 0.3514  @ epoch 32189 )\n",
      "[Epoch: 32580] train loss: 0.3671, train acc: 0.8352, val loss: 0.4318, val acc: 0.7997  (best train acc: 0.8486, best val acc: 0.8422, best train loss: 0.3514  @ epoch 32189 )\n",
      "[Epoch: 32600] train loss: 0.3635, train acc: 0.8408, val loss: 0.4037, val acc: 0.8152  (best train acc: 0.8486, best val acc: 0.8422, best train loss: 0.3514  @ epoch 32189 )\n",
      "[Epoch: 32620] train loss: 0.3691, train acc: 0.8349, val loss: 0.4101, val acc: 0.8202  (best train acc: 0.8486, best val acc: 0.8422, best train loss: 0.3514  @ epoch 32189 )\n",
      "[Epoch: 32640] train loss: 0.3880, train acc: 0.8300, val loss: 0.3913, val acc: 0.8253  (best train acc: 0.8486, best val acc: 0.8422, best train loss: 0.3514  @ epoch 32189 )\n",
      "[Epoch: 32660] train loss: 0.3734, train acc: 0.8352, val loss: 0.3938, val acc: 0.8260  (best train acc: 0.8486, best val acc: 0.8422, best train loss: 0.3514  @ epoch 32189 )\n",
      "[Epoch: 32680] train loss: 0.3830, train acc: 0.8284, val loss: 0.4124, val acc: 0.8054  (best train acc: 0.8486, best val acc: 0.8422, best train loss: 0.3514  @ epoch 32189 )\n",
      "[Epoch: 32700] train loss: 0.3671, train acc: 0.8381, val loss: 0.4274, val acc: 0.8027  (best train acc: 0.8486, best val acc: 0.8422, best train loss: 0.3514  @ epoch 32189 )\n",
      "[Epoch: 32720] train loss: 0.3708, train acc: 0.8349, val loss: 0.4160, val acc: 0.8253  (best train acc: 0.8486, best val acc: 0.8422, best train loss: 0.3514  @ epoch 32189 )\n",
      "[Epoch: 32740] train loss: 0.3640, train acc: 0.8406, val loss: 0.4143, val acc: 0.8199  (best train acc: 0.8486, best val acc: 0.8422, best train loss: 0.3514  @ epoch 32189 )\n",
      "[Epoch: 32760] train loss: 0.3744, train acc: 0.8359, val loss: 0.4066, val acc: 0.8199  (best train acc: 0.8486, best val acc: 0.8422, best train loss: 0.3514  @ epoch 32189 )\n",
      "[Epoch: 32780] train loss: 0.3719, train acc: 0.8337, val loss: 0.4160, val acc: 0.8219  (best train acc: 0.8486, best val acc: 0.8422, best train loss: 0.3514  @ epoch 32189 )\n",
      "[Epoch: 32800] train loss: 0.3839, train acc: 0.8348, val loss: 0.3900, val acc: 0.8250  (best train acc: 0.8486, best val acc: 0.8422, best train loss: 0.3514  @ epoch 32189 )\n",
      "[Epoch: 32820] train loss: 0.3859, train acc: 0.8301, val loss: 0.4258, val acc: 0.8010  (best train acc: 0.8486, best val acc: 0.8422, best train loss: 0.3514  @ epoch 32189 )\n",
      "[Epoch: 32840] train loss: 0.3859, train acc: 0.8242, val loss: 0.4189, val acc: 0.8098  (best train acc: 0.8486, best val acc: 0.8422, best train loss: 0.3514  @ epoch 32189 )\n",
      "[Epoch: 32860] train loss: 0.3718, train acc: 0.8376, val loss: 0.4208, val acc: 0.8128  (best train acc: 0.8486, best val acc: 0.8422, best train loss: 0.3514  @ epoch 32189 )\n",
      "[Epoch: 32880] train loss: 0.3791, train acc: 0.8387, val loss: 0.4160, val acc: 0.8098  (best train acc: 0.8486, best val acc: 0.8422, best train loss: 0.3514  @ epoch 32189 )\n",
      "[Epoch: 32900] train loss: 0.3954, train acc: 0.8252, val loss: 0.4094, val acc: 0.8118  (best train acc: 0.8486, best val acc: 0.8422, best train loss: 0.3514  @ epoch 32189 )\n",
      "[Epoch: 32920] train loss: 0.4169, train acc: 0.8160, val loss: 0.4214, val acc: 0.8020  (best train acc: 0.8486, best val acc: 0.8422, best train loss: 0.3514  @ epoch 32189 )\n",
      "[Epoch: 32940] train loss: 0.3940, train acc: 0.8221, val loss: 0.3955, val acc: 0.8337  (best train acc: 0.8486, best val acc: 0.8422, best train loss: 0.3514  @ epoch 32189 )\n",
      "[Epoch: 32960] train loss: 0.3856, train acc: 0.8305, val loss: 0.4000, val acc: 0.8182  (best train acc: 0.8486, best val acc: 0.8422, best train loss: 0.3514  @ epoch 32189 )\n",
      "[Epoch: 32980] train loss: 0.3806, train acc: 0.8344, val loss: 0.4007, val acc: 0.8263  (best train acc: 0.8486, best val acc: 0.8422, best train loss: 0.3514  @ epoch 32189 )\n",
      "[Epoch: 33000] train loss: 0.3750, train acc: 0.8368, val loss: 0.4025, val acc: 0.8206  (best train acc: 0.8486, best val acc: 0.8422, best train loss: 0.3514  @ epoch 32189 )\n",
      "[Epoch: 33020] train loss: 0.3728, train acc: 0.8357, val loss: 0.4001, val acc: 0.8192  (best train acc: 0.8486, best val acc: 0.8422, best train loss: 0.3514  @ epoch 32189 )\n",
      "[Epoch: 33040] train loss: 0.3884, train acc: 0.8297, val loss: 0.4046, val acc: 0.8202  (best train acc: 0.8486, best val acc: 0.8422, best train loss: 0.3514  @ epoch 32189 )\n",
      "[Epoch: 33060] train loss: 0.3752, train acc: 0.8352, val loss: 0.4313, val acc: 0.8057  (best train acc: 0.8486, best val acc: 0.8422, best train loss: 0.3514  @ epoch 32189 )\n",
      "[Epoch: 33080] train loss: 0.3620, train acc: 0.8399, val loss: 0.4239, val acc: 0.8081  (best train acc: 0.8486, best val acc: 0.8422, best train loss: 0.3514  @ epoch 32189 )\n",
      "[Epoch: 33100] train loss: 0.3782, train acc: 0.8349, val loss: 0.4131, val acc: 0.8121  (best train acc: 0.8486, best val acc: 0.8422, best train loss: 0.3514  @ epoch 32189 )\n",
      "[Epoch: 33120] train loss: 0.3920, train acc: 0.8300, val loss: 0.4078, val acc: 0.8196  (best train acc: 0.8490, best val acc: 0.8422, best train loss: 0.3514  @ epoch 32189 )\n",
      "[Epoch: 33140] train loss: 0.4027, train acc: 0.8250, val loss: 0.4078, val acc: 0.8162  (best train acc: 0.8490, best val acc: 0.8422, best train loss: 0.3514  @ epoch 32189 )\n",
      "[Epoch: 33160] train loss: 0.3851, train acc: 0.8351, val loss: 0.4100, val acc: 0.8165  (best train acc: 0.8490, best val acc: 0.8422, best train loss: 0.3514  @ epoch 32189 )\n",
      "[Epoch: 33180] train loss: 0.3926, train acc: 0.8297, val loss: 0.4155, val acc: 0.8111  (best train acc: 0.8490, best val acc: 0.8422, best train loss: 0.3514  @ epoch 32189 )\n",
      "[Epoch: 33200] train loss: 0.3709, train acc: 0.8360, val loss: 0.4191, val acc: 0.8253  (best train acc: 0.8490, best val acc: 0.8422, best train loss: 0.3514  @ epoch 32189 )\n",
      "[Epoch: 33220] train loss: 0.3726, train acc: 0.8338, val loss: 0.4137, val acc: 0.8159  (best train acc: 0.8490, best val acc: 0.8422, best train loss: 0.3514  @ epoch 32189 )\n",
      "[Epoch: 33240] train loss: 0.3726, train acc: 0.8397, val loss: 0.4379, val acc: 0.7943  (best train acc: 0.8490, best val acc: 0.8422, best train loss: 0.3514  @ epoch 32189 )\n",
      "[Epoch: 33260] train loss: 0.3801, train acc: 0.8307, val loss: 0.4084, val acc: 0.8256  (best train acc: 0.8490, best val acc: 0.8422, best train loss: 0.3514  @ epoch 32189 )\n",
      "[Epoch: 33280] train loss: 0.3650, train acc: 0.8391, val loss: 0.3951, val acc: 0.8391  (best train acc: 0.8490, best val acc: 0.8422, best train loss: 0.3514  @ epoch 32189 )\n",
      "[Epoch: 33300] train loss: 0.3798, train acc: 0.8305, val loss: 0.4101, val acc: 0.8159  (best train acc: 0.8490, best val acc: 0.8422, best train loss: 0.3514  @ epoch 32189 )\n",
      "[Epoch: 33320] train loss: 0.3596, train acc: 0.8422, val loss: 0.4092, val acc: 0.8212  (best train acc: 0.8490, best val acc: 0.8422, best train loss: 0.3514  @ epoch 32189 )\n",
      "[Epoch: 33340] train loss: 0.3954, train acc: 0.8263, val loss: 0.4130, val acc: 0.8084  (best train acc: 0.8490, best val acc: 0.8422, best train loss: 0.3514  @ epoch 32189 )\n",
      "[Epoch: 33360] train loss: 0.3561, train acc: 0.8445, val loss: 0.4211, val acc: 0.8064  (best train acc: 0.8490, best val acc: 0.8422, best train loss: 0.3514  @ epoch 32189 )\n",
      "[Epoch: 33380] train loss: 0.3619, train acc: 0.8421, val loss: 0.4035, val acc: 0.8226  (best train acc: 0.8490, best val acc: 0.8422, best train loss: 0.3514  @ epoch 32189 )\n",
      "[Epoch: 33400] train loss: 0.3748, train acc: 0.8366, val loss: 0.4123, val acc: 0.8155  (best train acc: 0.8490, best val acc: 0.8422, best train loss: 0.3514  @ epoch 32189 )\n",
      "[Epoch: 33420] train loss: 0.3548, train acc: 0.8448, val loss: 0.4101, val acc: 0.8172  (best train acc: 0.8490, best val acc: 0.8422, best train loss: 0.3514  @ epoch 32189 )\n",
      "[Epoch: 33440] train loss: 0.3761, train acc: 0.8355, val loss: 0.4124, val acc: 0.8196  (best train acc: 0.8490, best val acc: 0.8422, best train loss: 0.3514  @ epoch 32189 )\n",
      "[Epoch: 33460] train loss: 0.3695, train acc: 0.8376, val loss: 0.4124, val acc: 0.8081  (best train acc: 0.8490, best val acc: 0.8422, best train loss: 0.3514  @ epoch 32189 )\n",
      "[Epoch: 33480] train loss: 0.3670, train acc: 0.8383, val loss: 0.3916, val acc: 0.8253  (best train acc: 0.8490, best val acc: 0.8422, best train loss: 0.3514  @ epoch 32189 )\n",
      "[Epoch: 33500] train loss: 0.3808, train acc: 0.8344, val loss: 0.4403, val acc: 0.7889  (best train acc: 0.8490, best val acc: 0.8422, best train loss: 0.3514  @ epoch 32189 )\n",
      "[Epoch: 33520] train loss: 0.4005, train acc: 0.8180, val loss: 0.4065, val acc: 0.8253  (best train acc: 0.8490, best val acc: 0.8422, best train loss: 0.3514  @ epoch 32189 )\n",
      "[Epoch: 33540] train loss: 0.3704, train acc: 0.8390, val loss: 0.4066, val acc: 0.8219  (best train acc: 0.8490, best val acc: 0.8422, best train loss: 0.3514  @ epoch 32189 )\n",
      "[Epoch: 33560] train loss: 0.3637, train acc: 0.8388, val loss: 0.4055, val acc: 0.8266  (best train acc: 0.8490, best val acc: 0.8422, best train loss: 0.3514  @ epoch 32189 )\n",
      "[Epoch: 33580] train loss: 0.3750, train acc: 0.8383, val loss: 0.4535, val acc: 0.7906  (best train acc: 0.8490, best val acc: 0.8422, best train loss: 0.3514  @ epoch 32189 )\n",
      "[Epoch: 33600] train loss: 0.3717, train acc: 0.8344, val loss: 0.4322, val acc: 0.8020  (best train acc: 0.8490, best val acc: 0.8422, best train loss: 0.3514  @ epoch 32189 )\n",
      "[Epoch: 33620] train loss: 0.3801, train acc: 0.8341, val loss: 0.4146, val acc: 0.8189  (best train acc: 0.8490, best val acc: 0.8422, best train loss: 0.3514  @ epoch 32189 )\n",
      "[Epoch: 33640] train loss: 0.3837, train acc: 0.8343, val loss: 0.4066, val acc: 0.8155  (best train acc: 0.8490, best val acc: 0.8422, best train loss: 0.3514  @ epoch 32189 )\n",
      "[Epoch: 33660] train loss: 0.3661, train acc: 0.8384, val loss: 0.4050, val acc: 0.8206  (best train acc: 0.8490, best val acc: 0.8422, best train loss: 0.3514  @ epoch 32189 )\n",
      "[Epoch: 33680] train loss: 0.3859, train acc: 0.8310, val loss: 0.4043, val acc: 0.8250  (best train acc: 0.8490, best val acc: 0.8422, best train loss: 0.3514  @ epoch 32189 )\n",
      "[Epoch: 33700] train loss: 0.3618, train acc: 0.8409, val loss: 0.3951, val acc: 0.8212  (best train acc: 0.8490, best val acc: 0.8422, best train loss: 0.3514  @ epoch 32189 )\n",
      "[Epoch: 33720] train loss: 0.3700, train acc: 0.8349, val loss: 0.4074, val acc: 0.8206  (best train acc: 0.8490, best val acc: 0.8422, best train loss: 0.3514  @ epoch 32189 )\n",
      "[Epoch: 33740] train loss: 0.3788, train acc: 0.8302, val loss: 0.4015, val acc: 0.8182  (best train acc: 0.8490, best val acc: 0.8422, best train loss: 0.3514  @ epoch 32189 )\n",
      "[Epoch: 33760] train loss: 0.3854, train acc: 0.8329, val loss: 0.3937, val acc: 0.8358  (best train acc: 0.8490, best val acc: 0.8422, best train loss: 0.3514  @ epoch 32189 )\n",
      "[Epoch: 33780] train loss: 0.4026, train acc: 0.8217, val loss: 0.4089, val acc: 0.8212  (best train acc: 0.8490, best val acc: 0.8422, best train loss: 0.3514  @ epoch 32189 )\n",
      "[Epoch: 33800] train loss: 0.3645, train acc: 0.8365, val loss: 0.4169, val acc: 0.8162  (best train acc: 0.8490, best val acc: 0.8422, best train loss: 0.3514  @ epoch 32189 )\n",
      "[Epoch: 33820] train loss: 0.3754, train acc: 0.8383, val loss: 0.3988, val acc: 0.8243  (best train acc: 0.8490, best val acc: 0.8422, best train loss: 0.3514  @ epoch 32189 )\n",
      "[Epoch: 33840] train loss: 0.3681, train acc: 0.8370, val loss: 0.4008, val acc: 0.8236  (best train acc: 0.8490, best val acc: 0.8422, best train loss: 0.3514  @ epoch 32189 )\n",
      "[Epoch: 33860] train loss: 0.3544, train acc: 0.8470, val loss: 0.3988, val acc: 0.8290  (best train acc: 0.8490, best val acc: 0.8422, best train loss: 0.3514  @ epoch 32189 )\n",
      "[Epoch: 33880] train loss: 0.3720, train acc: 0.8351, val loss: 0.4074, val acc: 0.8165  (best train acc: 0.8490, best val acc: 0.8422, best train loss: 0.3514  @ epoch 32189 )\n",
      "[Epoch: 33900] train loss: 0.3815, train acc: 0.8320, val loss: 0.4252, val acc: 0.8115  (best train acc: 0.8490, best val acc: 0.8422, best train loss: 0.3514  @ epoch 32189 )\n",
      "[Epoch: 33920] train loss: 0.3673, train acc: 0.8396, val loss: 0.3975, val acc: 0.8266  (best train acc: 0.8490, best val acc: 0.8422, best train loss: 0.3514  @ epoch 32189 )\n",
      "[Epoch: 33940] train loss: 0.3555, train acc: 0.8476, val loss: 0.4048, val acc: 0.8202  (best train acc: 0.8490, best val acc: 0.8422, best train loss: 0.3514  @ epoch 32189 )\n",
      "[Epoch: 33960] train loss: 0.3686, train acc: 0.8353, val loss: 0.3962, val acc: 0.8266  (best train acc: 0.8490, best val acc: 0.8422, best train loss: 0.3514  @ epoch 32189 )\n",
      "[Epoch: 33980] train loss: 0.3725, train acc: 0.8399, val loss: 0.4117, val acc: 0.8192  (best train acc: 0.8490, best val acc: 0.8422, best train loss: 0.3514  @ epoch 32189 )\n",
      "[Epoch: 34000] train loss: 0.3705, train acc: 0.8356, val loss: 0.4127, val acc: 0.8287  (best train acc: 0.8490, best val acc: 0.8422, best train loss: 0.3514  @ epoch 32189 )\n",
      "[Epoch: 34020] train loss: 0.3703, train acc: 0.8352, val loss: 0.4090, val acc: 0.8233  (best train acc: 0.8490, best val acc: 0.8422, best train loss: 0.3514  @ epoch 32189 )\n",
      "[Epoch: 34040] train loss: 0.3611, train acc: 0.8387, val loss: 0.4164, val acc: 0.8155  (best train acc: 0.8490, best val acc: 0.8422, best train loss: 0.3514  @ epoch 32189 )\n",
      "[Epoch: 34060] train loss: 0.3724, train acc: 0.8387, val loss: 0.4104, val acc: 0.8293  (best train acc: 0.8490, best val acc: 0.8422, best train loss: 0.3514  @ epoch 32189 )\n",
      "[Epoch: 34080] train loss: 0.3634, train acc: 0.8384, val loss: 0.4008, val acc: 0.8263  (best train acc: 0.8490, best val acc: 0.8422, best train loss: 0.3514  @ epoch 32189 )\n",
      "[Epoch: 34100] train loss: 0.3829, train acc: 0.8347, val loss: 0.4400, val acc: 0.7956  (best train acc: 0.8490, best val acc: 0.8422, best train loss: 0.3514  @ epoch 32189 )\n",
      "[Epoch: 34120] train loss: 0.3769, train acc: 0.8326, val loss: 0.4255, val acc: 0.8051  (best train acc: 0.8490, best val acc: 0.8422, best train loss: 0.3514  @ epoch 32189 )\n",
      "[Epoch: 34140] train loss: 0.3716, train acc: 0.8381, val loss: 0.4147, val acc: 0.8162  (best train acc: 0.8490, best val acc: 0.8422, best train loss: 0.3514  @ epoch 32189 )\n",
      "[Epoch: 34160] train loss: 0.3732, train acc: 0.8313, val loss: 0.4051, val acc: 0.8297  (best train acc: 0.8490, best val acc: 0.8422, best train loss: 0.3514  @ epoch 32189 )\n",
      "[Epoch: 34180] train loss: 0.3809, train acc: 0.8344, val loss: 0.4126, val acc: 0.8145  (best train acc: 0.8490, best val acc: 0.8422, best train loss: 0.3514  @ epoch 32189 )\n",
      "[Epoch: 34200] train loss: 0.3591, train acc: 0.8431, val loss: 0.4075, val acc: 0.8266  (best train acc: 0.8490, best val acc: 0.8422, best train loss: 0.3514  @ epoch 32189 )\n",
      "[Epoch: 34220] train loss: 0.3641, train acc: 0.8396, val loss: 0.4037, val acc: 0.8175  (best train acc: 0.8490, best val acc: 0.8422, best train loss: 0.3514  @ epoch 32189 )\n",
      "[Epoch: 34240] train loss: 0.3791, train acc: 0.8290, val loss: 0.4049, val acc: 0.8175  (best train acc: 0.8490, best val acc: 0.8422, best train loss: 0.3514  @ epoch 32189 )\n",
      "[Epoch: 34260] train loss: 0.3706, train acc: 0.8365, val loss: 0.4037, val acc: 0.8196  (best train acc: 0.8490, best val acc: 0.8422, best train loss: 0.3514  @ epoch 32189 )\n",
      "[Epoch: 34280] train loss: 0.3770, train acc: 0.8306, val loss: 0.4201, val acc: 0.8192  (best train acc: 0.8490, best val acc: 0.8422, best train loss: 0.3514  @ epoch 32189 )\n",
      "[Epoch: 34300] train loss: 0.3548, train acc: 0.8431, val loss: 0.3992, val acc: 0.8280  (best train acc: 0.8490, best val acc: 0.8422, best train loss: 0.3514  @ epoch 32189 )\n",
      "[Epoch: 34320] train loss: 0.3610, train acc: 0.8420, val loss: 0.3977, val acc: 0.8253  (best train acc: 0.8490, best val acc: 0.8422, best train loss: 0.3503  @ epoch 34313 )\n",
      "[Epoch: 34340] train loss: 0.3699, train acc: 0.8385, val loss: 0.4084, val acc: 0.8229  (best train acc: 0.8490, best val acc: 0.8422, best train loss: 0.3503  @ epoch 34313 )\n",
      "[Epoch: 34360] train loss: 0.3515, train acc: 0.8458, val loss: 0.4008, val acc: 0.8202  (best train acc: 0.8490, best val acc: 0.8422, best train loss: 0.3503  @ epoch 34313 )\n",
      "[Epoch: 34380] train loss: 0.3600, train acc: 0.8428, val loss: 0.4024, val acc: 0.8287  (best train acc: 0.8490, best val acc: 0.8422, best train loss: 0.3503  @ epoch 34313 )\n",
      "[Epoch: 34400] train loss: 0.3912, train acc: 0.8295, val loss: 0.4712, val acc: 0.7801  (best train acc: 0.8490, best val acc: 0.8422, best train loss: 0.3503  @ epoch 34313 )\n",
      "[Epoch: 34420] train loss: 0.3580, train acc: 0.8431, val loss: 0.4009, val acc: 0.8209  (best train acc: 0.8490, best val acc: 0.8422, best train loss: 0.3503  @ epoch 34313 )\n",
      "[Epoch: 34440] train loss: 0.3671, train acc: 0.8385, val loss: 0.4188, val acc: 0.8094  (best train acc: 0.8490, best val acc: 0.8422, best train loss: 0.3503  @ epoch 34313 )\n",
      "[Epoch: 34460] train loss: 0.3794, train acc: 0.8313, val loss: 0.4100, val acc: 0.8212  (best train acc: 0.8490, best val acc: 0.8422, best train loss: 0.3503  @ epoch 34313 )\n",
      "[Epoch: 34480] train loss: 0.3743, train acc: 0.8350, val loss: 0.3925, val acc: 0.8293  (best train acc: 0.8490, best val acc: 0.8422, best train loss: 0.3503  @ epoch 34313 )\n",
      "[Epoch: 34500] train loss: 0.3656, train acc: 0.8401, val loss: 0.4194, val acc: 0.8175  (best train acc: 0.8490, best val acc: 0.8422, best train loss: 0.3503  @ epoch 34313 )\n",
      "[Epoch: 34520] train loss: 0.3741, train acc: 0.8368, val loss: 0.4069, val acc: 0.8229  (best train acc: 0.8490, best val acc: 0.8422, best train loss: 0.3503  @ epoch 34313 )\n",
      "[Epoch: 34540] train loss: 0.3885, train acc: 0.8308, val loss: 0.4040, val acc: 0.8206  (best train acc: 0.8490, best val acc: 0.8422, best train loss: 0.3503  @ epoch 34313 )\n",
      "[Epoch: 34560] train loss: 0.3725, train acc: 0.8355, val loss: 0.3960, val acc: 0.8246  (best train acc: 0.8490, best val acc: 0.8422, best train loss: 0.3503  @ epoch 34313 )\n",
      "[Epoch: 34580] train loss: 0.3830, train acc: 0.8338, val loss: 0.3988, val acc: 0.8226  (best train acc: 0.8490, best val acc: 0.8422, best train loss: 0.3503  @ epoch 34313 )\n",
      "[Epoch: 34600] train loss: 0.3642, train acc: 0.8352, val loss: 0.4091, val acc: 0.8212  (best train acc: 0.8490, best val acc: 0.8422, best train loss: 0.3503  @ epoch 34313 )\n",
      "[Epoch: 34620] train loss: 0.3811, train acc: 0.8326, val loss: 0.4051, val acc: 0.8246  (best train acc: 0.8490, best val acc: 0.8422, best train loss: 0.3503  @ epoch 34313 )\n",
      "[Epoch: 34640] train loss: 0.3666, train acc: 0.8412, val loss: 0.3990, val acc: 0.8290  (best train acc: 0.8490, best val acc: 0.8422, best train loss: 0.3469  @ epoch 34633 )\n",
      "[Epoch: 34660] train loss: 0.3723, train acc: 0.8338, val loss: 0.4042, val acc: 0.8233  (best train acc: 0.8490, best val acc: 0.8422, best train loss: 0.3469  @ epoch 34633 )\n",
      "[Epoch: 34680] train loss: 0.3603, train acc: 0.8412, val loss: 0.3947, val acc: 0.8307  (best train acc: 0.8490, best val acc: 0.8422, best train loss: 0.3469  @ epoch 34633 )\n",
      "[Epoch: 34700] train loss: 0.3794, train acc: 0.8357, val loss: 0.4199, val acc: 0.8115  (best train acc: 0.8490, best val acc: 0.8422, best train loss: 0.3469  @ epoch 34633 )\n",
      "[Epoch: 34720] train loss: 0.3740, train acc: 0.8374, val loss: 0.4097, val acc: 0.8199  (best train acc: 0.8490, best val acc: 0.8422, best train loss: 0.3469  @ epoch 34633 )\n",
      "[Epoch: 34740] train loss: 0.3707, train acc: 0.8346, val loss: 0.4184, val acc: 0.8212  (best train acc: 0.8490, best val acc: 0.8422, best train loss: 0.3469  @ epoch 34633 )\n",
      "[Epoch: 34760] train loss: 0.3641, train acc: 0.8388, val loss: 0.3948, val acc: 0.8287  (best train acc: 0.8490, best val acc: 0.8422, best train loss: 0.3469  @ epoch 34633 )\n",
      "[Epoch: 34780] train loss: 0.3845, train acc: 0.8311, val loss: 0.4136, val acc: 0.8236  (best train acc: 0.8490, best val acc: 0.8422, best train loss: 0.3469  @ epoch 34633 )\n",
      "[Epoch: 34800] train loss: 0.3674, train acc: 0.8343, val loss: 0.4040, val acc: 0.8219  (best train acc: 0.8490, best val acc: 0.8422, best train loss: 0.3469  @ epoch 34633 )\n",
      "[Epoch: 34820] train loss: 0.3793, train acc: 0.8287, val loss: 0.4376, val acc: 0.7976  (best train acc: 0.8490, best val acc: 0.8422, best train loss: 0.3469  @ epoch 34633 )\n",
      "[Epoch: 34840] train loss: 0.3791, train acc: 0.8314, val loss: 0.4237, val acc: 0.8253  (best train acc: 0.8490, best val acc: 0.8422, best train loss: 0.3469  @ epoch 34633 )\n",
      "[Epoch: 34860] train loss: 0.3631, train acc: 0.8370, val loss: 0.4070, val acc: 0.8260  (best train acc: 0.8490, best val acc: 0.8422, best train loss: 0.3469  @ epoch 34633 )\n",
      "[Epoch: 34880] train loss: 0.3784, train acc: 0.8278, val loss: 0.4047, val acc: 0.8206  (best train acc: 0.8490, best val acc: 0.8422, best train loss: 0.3469  @ epoch 34633 )\n",
      "[Epoch: 34900] train loss: 0.4069, train acc: 0.8233, val loss: 0.4242, val acc: 0.8040  (best train acc: 0.8490, best val acc: 0.8422, best train loss: 0.3469  @ epoch 34633 )\n",
      "[Epoch: 34920] train loss: 0.4119, train acc: 0.8207, val loss: 0.4254, val acc: 0.8081  (best train acc: 0.8490, best val acc: 0.8422, best train loss: 0.3469  @ epoch 34633 )\n",
      "[Epoch: 34940] train loss: 0.3926, train acc: 0.8287, val loss: 0.4037, val acc: 0.8256  (best train acc: 0.8490, best val acc: 0.8422, best train loss: 0.3469  @ epoch 34633 )\n",
      "[Epoch: 34960] train loss: 0.3871, train acc: 0.8247, val loss: 0.4040, val acc: 0.8209  (best train acc: 0.8490, best val acc: 0.8422, best train loss: 0.3469  @ epoch 34633 )\n",
      "[Epoch: 34980] train loss: 0.3808, train acc: 0.8308, val loss: 0.4143, val acc: 0.8206  (best train acc: 0.8490, best val acc: 0.8422, best train loss: 0.3469  @ epoch 34633 )\n",
      "[Epoch: 35000] train loss: 0.3683, train acc: 0.8380, val loss: 0.4084, val acc: 0.8138  (best train acc: 0.8490, best val acc: 0.8422, best train loss: 0.3469  @ epoch 34633 )\n",
      "[Epoch: 35020] train loss: 0.3619, train acc: 0.8383, val loss: 0.4016, val acc: 0.8229  (best train acc: 0.8490, best val acc: 0.8422, best train loss: 0.3469  @ epoch 34633 )\n",
      "[Epoch: 35040] train loss: 0.4168, train acc: 0.8182, val loss: 0.4428, val acc: 0.7936  (best train acc: 0.8490, best val acc: 0.8422, best train loss: 0.3469  @ epoch 34633 )\n",
      "[Epoch: 35060] train loss: 0.3643, train acc: 0.8402, val loss: 0.4019, val acc: 0.8250  (best train acc: 0.8490, best val acc: 0.8422, best train loss: 0.3469  @ epoch 34633 )\n",
      "[Epoch: 35080] train loss: 0.3805, train acc: 0.8276, val loss: 0.4003, val acc: 0.8270  (best train acc: 0.8490, best val acc: 0.8422, best train loss: 0.3469  @ epoch 34633 )\n",
      "[Epoch: 35100] train loss: 0.3781, train acc: 0.8348, val loss: 0.4132, val acc: 0.8277  (best train acc: 0.8490, best val acc: 0.8422, best train loss: 0.3469  @ epoch 34633 )\n",
      "[Epoch: 35120] train loss: 0.3666, train acc: 0.8404, val loss: 0.4129, val acc: 0.8115  (best train acc: 0.8490, best val acc: 0.8422, best train loss: 0.3469  @ epoch 34633 )\n",
      "[Epoch: 35140] train loss: 0.3614, train acc: 0.8421, val loss: 0.4096, val acc: 0.8283  (best train acc: 0.8490, best val acc: 0.8422, best train loss: 0.3469  @ epoch 34633 )\n",
      "[Epoch: 35160] train loss: 0.3628, train acc: 0.8436, val loss: 0.3958, val acc: 0.8304  (best train acc: 0.8490, best val acc: 0.8422, best train loss: 0.3469  @ epoch 34633 )\n",
      "[Epoch: 35180] train loss: 0.3732, train acc: 0.8388, val loss: 0.4122, val acc: 0.8152  (best train acc: 0.8490, best val acc: 0.8422, best train loss: 0.3469  @ epoch 34633 )\n",
      "[Epoch: 35200] train loss: 0.3788, train acc: 0.8366, val loss: 0.4120, val acc: 0.8125  (best train acc: 0.8490, best val acc: 0.8422, best train loss: 0.3469  @ epoch 34633 )\n",
      "[Epoch: 35220] train loss: 0.3739, train acc: 0.8343, val loss: 0.4060, val acc: 0.8266  (best train acc: 0.8490, best val acc: 0.8422, best train loss: 0.3469  @ epoch 34633 )\n",
      "[Epoch: 35240] train loss: 0.3734, train acc: 0.8323, val loss: 0.4103, val acc: 0.8273  (best train acc: 0.8490, best val acc: 0.8422, best train loss: 0.3469  @ epoch 34633 )\n",
      "[Epoch: 35260] train loss: 0.3892, train acc: 0.8269, val loss: 0.4039, val acc: 0.8216  (best train acc: 0.8490, best val acc: 0.8422, best train loss: 0.3469  @ epoch 34633 )\n",
      "[Epoch: 35280] train loss: 0.3653, train acc: 0.8404, val loss: 0.4166, val acc: 0.8185  (best train acc: 0.8490, best val acc: 0.8422, best train loss: 0.3469  @ epoch 34633 )\n",
      "[Epoch: 35300] train loss: 0.3605, train acc: 0.8412, val loss: 0.4063, val acc: 0.8202  (best train acc: 0.8490, best val acc: 0.8422, best train loss: 0.3469  @ epoch 34633 )\n",
      "[Epoch: 35320] train loss: 0.3586, train acc: 0.8376, val loss: 0.4048, val acc: 0.8260  (best train acc: 0.8490, best val acc: 0.8422, best train loss: 0.3469  @ epoch 34633 )\n",
      "[Epoch: 35340] train loss: 0.3868, train acc: 0.8299, val loss: 0.4088, val acc: 0.8189  (best train acc: 0.8490, best val acc: 0.8422, best train loss: 0.3469  @ epoch 34633 )\n",
      "[Epoch: 35360] train loss: 0.3713, train acc: 0.8356, val loss: 0.4043, val acc: 0.8236  (best train acc: 0.8490, best val acc: 0.8422, best train loss: 0.3469  @ epoch 34633 )\n",
      "[Epoch: 35380] train loss: 0.3529, train acc: 0.8442, val loss: 0.4041, val acc: 0.8216  (best train acc: 0.8490, best val acc: 0.8422, best train loss: 0.3469  @ epoch 34633 )\n",
      "[Epoch: 35400] train loss: 0.3632, train acc: 0.8400, val loss: 0.4009, val acc: 0.8273  (best train acc: 0.8490, best val acc: 0.8422, best train loss: 0.3469  @ epoch 34633 )\n",
      "[Epoch: 35420] train loss: 0.3740, train acc: 0.8360, val loss: 0.4072, val acc: 0.8226  (best train acc: 0.8490, best val acc: 0.8422, best train loss: 0.3469  @ epoch 34633 )\n",
      "[Epoch: 35440] train loss: 0.3742, train acc: 0.8371, val loss: 0.4090, val acc: 0.8185  (best train acc: 0.8490, best val acc: 0.8422, best train loss: 0.3469  @ epoch 34633 )\n",
      "[Epoch: 35460] train loss: 0.3796, train acc: 0.8335, val loss: 0.4126, val acc: 0.8243  (best train acc: 0.8497, best val acc: 0.8422, best train loss: 0.3469  @ epoch 34633 )\n",
      "[Epoch: 35480] train loss: 0.3690, train acc: 0.8396, val loss: 0.4079, val acc: 0.8287  (best train acc: 0.8497, best val acc: 0.8422, best train loss: 0.3469  @ epoch 34633 )\n",
      "[Epoch: 35500] train loss: 0.3882, train acc: 0.8302, val loss: 0.4370, val acc: 0.7990  (best train acc: 0.8497, best val acc: 0.8422, best train loss: 0.3469  @ epoch 34633 )\n",
      "[Epoch: 35520] train loss: 0.3688, train acc: 0.8417, val loss: 0.4104, val acc: 0.8290  (best train acc: 0.8497, best val acc: 0.8422, best train loss: 0.3469  @ epoch 34633 )\n",
      "[Epoch: 35540] train loss: 0.3783, train acc: 0.8342, val loss: 0.4108, val acc: 0.8175  (best train acc: 0.8497, best val acc: 0.8422, best train loss: 0.3469  @ epoch 34633 )\n",
      "[Epoch: 35560] train loss: 0.3698, train acc: 0.8390, val loss: 0.3965, val acc: 0.8246  (best train acc: 0.8497, best val acc: 0.8422, best train loss: 0.3469  @ epoch 34633 )\n",
      "[Epoch: 35580] train loss: 0.3791, train acc: 0.8314, val loss: 0.4071, val acc: 0.8212  (best train acc: 0.8497, best val acc: 0.8422, best train loss: 0.3469  @ epoch 34633 )\n",
      "[Epoch: 35600] train loss: 0.3661, train acc: 0.8347, val loss: 0.4074, val acc: 0.8179  (best train acc: 0.8497, best val acc: 0.8422, best train loss: 0.3469  @ epoch 34633 )\n",
      "[Epoch: 35620] train loss: 0.3700, train acc: 0.8386, val loss: 0.3972, val acc: 0.8273  (best train acc: 0.8497, best val acc: 0.8422, best train loss: 0.3469  @ epoch 34633 )\n",
      "[Epoch: 35640] train loss: 0.3749, train acc: 0.8325, val loss: 0.4001, val acc: 0.8270  (best train acc: 0.8497, best val acc: 0.8422, best train loss: 0.3469  @ epoch 34633 )\n",
      "[Epoch: 35660] train loss: 0.3561, train acc: 0.8425, val loss: 0.3890, val acc: 0.8297  (best train acc: 0.8497, best val acc: 0.8422, best train loss: 0.3469  @ epoch 34633 )\n",
      "[Epoch: 35680] train loss: 0.3693, train acc: 0.8355, val loss: 0.4104, val acc: 0.8243  (best train acc: 0.8497, best val acc: 0.8422, best train loss: 0.3469  @ epoch 34633 )\n",
      "[Epoch: 35700] train loss: 0.3650, train acc: 0.8417, val loss: 0.4245, val acc: 0.8091  (best train acc: 0.8497, best val acc: 0.8422, best train loss: 0.3469  @ epoch 34633 )\n",
      "[Epoch: 35720] train loss: 0.3659, train acc: 0.8425, val loss: 0.3994, val acc: 0.8202  (best train acc: 0.8497, best val acc: 0.8422, best train loss: 0.3469  @ epoch 34633 )\n",
      "[Epoch: 35740] train loss: 0.3869, train acc: 0.8232, val loss: 0.4156, val acc: 0.8202  (best train acc: 0.8497, best val acc: 0.8422, best train loss: 0.3469  @ epoch 34633 )\n",
      "[Epoch: 35760] train loss: 0.3774, train acc: 0.8324, val loss: 0.4120, val acc: 0.8266  (best train acc: 0.8497, best val acc: 0.8422, best train loss: 0.3469  @ epoch 34633 )\n",
      "[Epoch: 35780] train loss: 0.3797, train acc: 0.8383, val loss: 0.4041, val acc: 0.8243  (best train acc: 0.8497, best val acc: 0.8422, best train loss: 0.3469  @ epoch 34633 )\n",
      "[Epoch: 35800] train loss: 0.3739, train acc: 0.8348, val loss: 0.4258, val acc: 0.8236  (best train acc: 0.8497, best val acc: 0.8422, best train loss: 0.3469  @ epoch 34633 )\n",
      "[Epoch: 35820] train loss: 0.3766, train acc: 0.8408, val loss: 0.3967, val acc: 0.8290  (best train acc: 0.8497, best val acc: 0.8422, best train loss: 0.3469  @ epoch 34633 )\n",
      "[Epoch: 35840] train loss: 0.3640, train acc: 0.8404, val loss: 0.4111, val acc: 0.8253  (best train acc: 0.8497, best val acc: 0.8422, best train loss: 0.3469  @ epoch 34633 )\n",
      "[Epoch: 35860] train loss: 0.3634, train acc: 0.8394, val loss: 0.3939, val acc: 0.8243  (best train acc: 0.8497, best val acc: 0.8422, best train loss: 0.3469  @ epoch 34633 )\n",
      "[Epoch: 35880] train loss: 0.3690, train acc: 0.8381, val loss: 0.4256, val acc: 0.8088  (best train acc: 0.8497, best val acc: 0.8422, best train loss: 0.3469  @ epoch 34633 )\n",
      "[Epoch: 35900] train loss: 0.3616, train acc: 0.8396, val loss: 0.3982, val acc: 0.8273  (best train acc: 0.8497, best val acc: 0.8422, best train loss: 0.3469  @ epoch 34633 )\n",
      "[Epoch: 35920] train loss: 0.3920, train acc: 0.8279, val loss: 0.4042, val acc: 0.8270  (best train acc: 0.8497, best val acc: 0.8422, best train loss: 0.3469  @ epoch 34633 )\n",
      "[Epoch: 35940] train loss: 0.3676, train acc: 0.8378, val loss: 0.4011, val acc: 0.8277  (best train acc: 0.8497, best val acc: 0.8422, best train loss: 0.3469  @ epoch 34633 )\n",
      "[Epoch: 35960] train loss: 0.4095, train acc: 0.8241, val loss: 0.4349, val acc: 0.8017  (best train acc: 0.8497, best val acc: 0.8422, best train loss: 0.3469  @ epoch 34633 )\n",
      "[Epoch: 35980] train loss: 0.3531, train acc: 0.8443, val loss: 0.4063, val acc: 0.8229  (best train acc: 0.8497, best val acc: 0.8422, best train loss: 0.3469  @ epoch 34633 )\n",
      "[Epoch: 36000] train loss: 0.3650, train acc: 0.8399, val loss: 0.4549, val acc: 0.7906  (best train acc: 0.8497, best val acc: 0.8422, best train loss: 0.3469  @ epoch 34633 )\n",
      "[Epoch: 36020] train loss: 0.3607, train acc: 0.8438, val loss: 0.4074, val acc: 0.8196  (best train acc: 0.8497, best val acc: 0.8422, best train loss: 0.3469  @ epoch 34633 )\n",
      "[Epoch: 36040] train loss: 0.3816, train acc: 0.8357, val loss: 0.4101, val acc: 0.8209  (best train acc: 0.8497, best val acc: 0.8422, best train loss: 0.3469  @ epoch 34633 )\n",
      "[Epoch: 36060] train loss: 0.3749, train acc: 0.8358, val loss: 0.4082, val acc: 0.8226  (best train acc: 0.8497, best val acc: 0.8422, best train loss: 0.3469  @ epoch 34633 )\n",
      "[Epoch: 36080] train loss: 0.3554, train acc: 0.8425, val loss: 0.4027, val acc: 0.8331  (best train acc: 0.8497, best val acc: 0.8422, best train loss: 0.3469  @ epoch 34633 )\n",
      "[Epoch: 36100] train loss: 0.3717, train acc: 0.8396, val loss: 0.4102, val acc: 0.8290  (best train acc: 0.8500, best val acc: 0.8422, best train loss: 0.3469  @ epoch 34633 )\n",
      "[Epoch: 36120] train loss: 0.3915, train acc: 0.8226, val loss: 0.4070, val acc: 0.8233  (best train acc: 0.8500, best val acc: 0.8422, best train loss: 0.3469  @ epoch 34633 )\n",
      "[Epoch: 36140] train loss: 0.3650, train acc: 0.8388, val loss: 0.4061, val acc: 0.8263  (best train acc: 0.8500, best val acc: 0.8422, best train loss: 0.3469  @ epoch 34633 )\n",
      "[Epoch: 36160] train loss: 0.3939, train acc: 0.8257, val loss: 0.4072, val acc: 0.8239  (best train acc: 0.8500, best val acc: 0.8422, best train loss: 0.3469  @ epoch 34633 )\n",
      "[Epoch: 36180] train loss: 0.3561, train acc: 0.8420, val loss: 0.4039, val acc: 0.8263  (best train acc: 0.8500, best val acc: 0.8422, best train loss: 0.3469  @ epoch 34633 )\n",
      "[Epoch: 36200] train loss: 0.3642, train acc: 0.8386, val loss: 0.4158, val acc: 0.8155  (best train acc: 0.8500, best val acc: 0.8422, best train loss: 0.3469  @ epoch 34633 )\n",
      "[Epoch: 36220] train loss: 0.3969, train acc: 0.8209, val loss: 0.3973, val acc: 0.8287  (best train acc: 0.8500, best val acc: 0.8422, best train loss: 0.3469  @ epoch 34633 )\n",
      "[Epoch: 36240] train loss: 0.3925, train acc: 0.8316, val loss: 0.4054, val acc: 0.8250  (best train acc: 0.8500, best val acc: 0.8422, best train loss: 0.3469  @ epoch 34633 )\n",
      "[Epoch: 36260] train loss: 0.3670, train acc: 0.8407, val loss: 0.3994, val acc: 0.8246  (best train acc: 0.8500, best val acc: 0.8422, best train loss: 0.3469  @ epoch 34633 )\n",
      "[Epoch: 36280] train loss: 0.3628, train acc: 0.8367, val loss: 0.4248, val acc: 0.8078  (best train acc: 0.8500, best val acc: 0.8422, best train loss: 0.3469  @ epoch 34633 )\n",
      "[Epoch: 36300] train loss: 0.3534, train acc: 0.8454, val loss: 0.4082, val acc: 0.8196  (best train acc: 0.8500, best val acc: 0.8422, best train loss: 0.3469  @ epoch 34633 )\n",
      "[Epoch: 36320] train loss: 0.3816, train acc: 0.8287, val loss: 0.4016, val acc: 0.8246  (best train acc: 0.8500, best val acc: 0.8422, best train loss: 0.3469  @ epoch 34633 )\n",
      "[Epoch: 36340] train loss: 0.3638, train acc: 0.8405, val loss: 0.4320, val acc: 0.8000  (best train acc: 0.8500, best val acc: 0.8422, best train loss: 0.3469  @ epoch 34633 )\n",
      "[Epoch: 36360] train loss: 0.3647, train acc: 0.8365, val loss: 0.4009, val acc: 0.8202  (best train acc: 0.8500, best val acc: 0.8422, best train loss: 0.3437  @ epoch 36347 )\n",
      "[Epoch: 36380] train loss: 0.3668, train acc: 0.8368, val loss: 0.4205, val acc: 0.8236  (best train acc: 0.8500, best val acc: 0.8422, best train loss: 0.3437  @ epoch 36347 )\n",
      "[Epoch: 36400] train loss: 0.3653, train acc: 0.8435, val loss: 0.4219, val acc: 0.8081  (best train acc: 0.8501, best val acc: 0.8422, best train loss: 0.3437  @ epoch 36347 )\n",
      "[Epoch: 36420] train loss: 0.3736, train acc: 0.8347, val loss: 0.3965, val acc: 0.8236  (best train acc: 0.8501, best val acc: 0.8422, best train loss: 0.3437  @ epoch 36347 )\n",
      "[Epoch: 36440] train loss: 0.3488, train acc: 0.8467, val loss: 0.4137, val acc: 0.8209  (best train acc: 0.8503, best val acc: 0.8422, best train loss: 0.3437  @ epoch 36347 )\n",
      "[Epoch: 36460] train loss: 0.3610, train acc: 0.8418, val loss: 0.4105, val acc: 0.8138  (best train acc: 0.8503, best val acc: 0.8422, best train loss: 0.3437  @ epoch 36347 )\n",
      "[Epoch: 36480] train loss: 0.3840, train acc: 0.8323, val loss: 0.4080, val acc: 0.8246  (best train acc: 0.8503, best val acc: 0.8422, best train loss: 0.3437  @ epoch 36347 )\n",
      "[Epoch: 36500] train loss: 0.3624, train acc: 0.8381, val loss: 0.4084, val acc: 0.8270  (best train acc: 0.8503, best val acc: 0.8422, best train loss: 0.3437  @ epoch 36347 )\n",
      "[Epoch: 36520] train loss: 0.3718, train acc: 0.8378, val loss: 0.4208, val acc: 0.8162  (best train acc: 0.8503, best val acc: 0.8422, best train loss: 0.3437  @ epoch 36347 )\n",
      "[Epoch: 36540] train loss: 0.3807, train acc: 0.8389, val loss: 0.4237, val acc: 0.8125  (best train acc: 0.8503, best val acc: 0.8422, best train loss: 0.3437  @ epoch 36347 )\n",
      "[Epoch: 36560] train loss: 0.3674, train acc: 0.8387, val loss: 0.3931, val acc: 0.8300  (best train acc: 0.8503, best val acc: 0.8422, best train loss: 0.3437  @ epoch 36347 )\n",
      "[Epoch: 36580] train loss: 0.3709, train acc: 0.8389, val loss: 0.4175, val acc: 0.8152  (best train acc: 0.8503, best val acc: 0.8422, best train loss: 0.3437  @ epoch 36347 )\n",
      "[Epoch: 36600] train loss: 0.3717, train acc: 0.8315, val loss: 0.4118, val acc: 0.8199  (best train acc: 0.8503, best val acc: 0.8422, best train loss: 0.3437  @ epoch 36347 )\n",
      "[Epoch: 36620] train loss: 0.3568, train acc: 0.8409, val loss: 0.3930, val acc: 0.8283  (best train acc: 0.8503, best val acc: 0.8422, best train loss: 0.3437  @ epoch 36347 )\n",
      "[Epoch: 36640] train loss: 0.3767, train acc: 0.8376, val loss: 0.3963, val acc: 0.8246  (best train acc: 0.8503, best val acc: 0.8422, best train loss: 0.3437  @ epoch 36347 )\n",
      "[Epoch: 36660] train loss: 0.3626, train acc: 0.8423, val loss: 0.4006, val acc: 0.8334  (best train acc: 0.8503, best val acc: 0.8422, best train loss: 0.3437  @ epoch 36347 )\n",
      "[Epoch: 36680] train loss: 0.3649, train acc: 0.8402, val loss: 0.4018, val acc: 0.8331  (best train acc: 0.8503, best val acc: 0.8422, best train loss: 0.3437  @ epoch 36347 )\n",
      "[Epoch: 36700] train loss: 0.3639, train acc: 0.8376, val loss: 0.4160, val acc: 0.8155  (best train acc: 0.8503, best val acc: 0.8422, best train loss: 0.3437  @ epoch 36347 )\n",
      "[Epoch: 36720] train loss: 0.3609, train acc: 0.8357, val loss: 0.4087, val acc: 0.8209  (best train acc: 0.8503, best val acc: 0.8422, best train loss: 0.3437  @ epoch 36347 )\n",
      "[Epoch: 36740] train loss: 0.3905, train acc: 0.8302, val loss: 0.4266, val acc: 0.8094  (best train acc: 0.8503, best val acc: 0.8422, best train loss: 0.3437  @ epoch 36347 )\n",
      "[Epoch: 36760] train loss: 0.3803, train acc: 0.8348, val loss: 0.4048, val acc: 0.8273  (best train acc: 0.8503, best val acc: 0.8422, best train loss: 0.3437  @ epoch 36347 )\n",
      "[Epoch: 36780] train loss: 0.3577, train acc: 0.8412, val loss: 0.4156, val acc: 0.8239  (best train acc: 0.8503, best val acc: 0.8422, best train loss: 0.3437  @ epoch 36347 )\n",
      "[Epoch: 36800] train loss: 0.3634, train acc: 0.8425, val loss: 0.3945, val acc: 0.8277  (best train acc: 0.8503, best val acc: 0.8422, best train loss: 0.3437  @ epoch 36347 )\n",
      "[Epoch: 36820] train loss: 0.3602, train acc: 0.8373, val loss: 0.4161, val acc: 0.8202  (best train acc: 0.8503, best val acc: 0.8422, best train loss: 0.3437  @ epoch 36347 )\n",
      "[Epoch: 36840] train loss: 0.3724, train acc: 0.8328, val loss: 0.4090, val acc: 0.8243  (best train acc: 0.8503, best val acc: 0.8422, best train loss: 0.3437  @ epoch 36347 )\n",
      "[Epoch: 36860] train loss: 0.3715, train acc: 0.8337, val loss: 0.4145, val acc: 0.8175  (best train acc: 0.8503, best val acc: 0.8422, best train loss: 0.3437  @ epoch 36347 )\n",
      "[Epoch: 36880] train loss: 0.3689, train acc: 0.8428, val loss: 0.4048, val acc: 0.8307  (best train acc: 0.8503, best val acc: 0.8422, best train loss: 0.3437  @ epoch 36347 )\n",
      "[Epoch: 36900] train loss: 0.3721, train acc: 0.8318, val loss: 0.4038, val acc: 0.8253  (best train acc: 0.8503, best val acc: 0.8422, best train loss: 0.3437  @ epoch 36347 )\n",
      "[Epoch: 36920] train loss: 0.3658, train acc: 0.8405, val loss: 0.4037, val acc: 0.8277  (best train acc: 0.8503, best val acc: 0.8422, best train loss: 0.3437  @ epoch 36347 )\n",
      "[Epoch: 36940] train loss: 0.3619, train acc: 0.8409, val loss: 0.4179, val acc: 0.8199  (best train acc: 0.8503, best val acc: 0.8422, best train loss: 0.3437  @ epoch 36347 )\n",
      "[Epoch: 36960] train loss: 0.3551, train acc: 0.8449, val loss: 0.4016, val acc: 0.8236  (best train acc: 0.8503, best val acc: 0.8422, best train loss: 0.3437  @ epoch 36347 )\n",
      "[Epoch: 36980] train loss: 0.3691, train acc: 0.8360, val loss: 0.4274, val acc: 0.8051  (best train acc: 0.8503, best val acc: 0.8422, best train loss: 0.3437  @ epoch 36347 )\n",
      "[Epoch: 37000] train loss: 0.3603, train acc: 0.8389, val loss: 0.4115, val acc: 0.8297  (best train acc: 0.8503, best val acc: 0.8422, best train loss: 0.3437  @ epoch 36347 )\n",
      "[Epoch: 37020] train loss: 0.3816, train acc: 0.8310, val loss: 0.4125, val acc: 0.8260  (best train acc: 0.8503, best val acc: 0.8422, best train loss: 0.3437  @ epoch 36347 )\n",
      "[Epoch: 37040] train loss: 0.3607, train acc: 0.8422, val loss: 0.4163, val acc: 0.8175  (best train acc: 0.8503, best val acc: 0.8422, best train loss: 0.3437  @ epoch 36347 )\n",
      "[Epoch: 37060] train loss: 0.3578, train acc: 0.8415, val loss: 0.4049, val acc: 0.8196  (best train acc: 0.8503, best val acc: 0.8422, best train loss: 0.3437  @ epoch 36347 )\n",
      "[Epoch: 37080] train loss: 0.3746, train acc: 0.8349, val loss: 0.4174, val acc: 0.8101  (best train acc: 0.8503, best val acc: 0.8422, best train loss: 0.3396  @ epoch 37064 )\n",
      "[Epoch: 37100] train loss: 0.3986, train acc: 0.8204, val loss: 0.4026, val acc: 0.8206  (best train acc: 0.8503, best val acc: 0.8422, best train loss: 0.3396  @ epoch 37064 )\n",
      "[Epoch: 37120] train loss: 0.3640, train acc: 0.8413, val loss: 0.3973, val acc: 0.8280  (best train acc: 0.8503, best val acc: 0.8422, best train loss: 0.3396  @ epoch 37064 )\n",
      "[Epoch: 37140] train loss: 0.3672, train acc: 0.8393, val loss: 0.4013, val acc: 0.8212  (best train acc: 0.8503, best val acc: 0.8422, best train loss: 0.3396  @ epoch 37064 )\n",
      "[Epoch: 37160] train loss: 0.3650, train acc: 0.8392, val loss: 0.3997, val acc: 0.8280  (best train acc: 0.8503, best val acc: 0.8422, best train loss: 0.3396  @ epoch 37064 )\n",
      "[Epoch: 37180] train loss: 0.3651, train acc: 0.8362, val loss: 0.4170, val acc: 0.8175  (best train acc: 0.8503, best val acc: 0.8422, best train loss: 0.3396  @ epoch 37064 )\n",
      "[Epoch: 37200] train loss: 0.3493, train acc: 0.8493, val loss: 0.4020, val acc: 0.8202  (best train acc: 0.8503, best val acc: 0.8422, best train loss: 0.3396  @ epoch 37064 )\n",
      "[Epoch: 37220] train loss: 0.3669, train acc: 0.8379, val loss: 0.4105, val acc: 0.8266  (best train acc: 0.8503, best val acc: 0.8422, best train loss: 0.3396  @ epoch 37064 )\n",
      "[Epoch: 37240] train loss: 0.3608, train acc: 0.8433, val loss: 0.4118, val acc: 0.8199  (best train acc: 0.8503, best val acc: 0.8422, best train loss: 0.3396  @ epoch 37064 )\n",
      "[Epoch: 37260] train loss: 0.3743, train acc: 0.8394, val loss: 0.3987, val acc: 0.8236  (best train acc: 0.8518, best val acc: 0.8422, best train loss: 0.3396  @ epoch 37064 )\n",
      "[Epoch: 37280] train loss: 0.3752, train acc: 0.8362, val loss: 0.3961, val acc: 0.8297  (best train acc: 0.8518, best val acc: 0.8422, best train loss: 0.3396  @ epoch 37064 )\n",
      "[Epoch: 37300] train loss: 0.3604, train acc: 0.8436, val loss: 0.4150, val acc: 0.8273  (best train acc: 0.8518, best val acc: 0.8422, best train loss: 0.3396  @ epoch 37064 )\n",
      "[Epoch: 37320] train loss: 0.3645, train acc: 0.8393, val loss: 0.4038, val acc: 0.8256  (best train acc: 0.8518, best val acc: 0.8422, best train loss: 0.3396  @ epoch 37064 )\n",
      "[Epoch: 37340] train loss: 0.3765, train acc: 0.8363, val loss: 0.4244, val acc: 0.8202  (best train acc: 0.8518, best val acc: 0.8422, best train loss: 0.3396  @ epoch 37064 )\n",
      "[Epoch: 37360] train loss: 0.3726, train acc: 0.8373, val loss: 0.4243, val acc: 0.8263  (best train acc: 0.8518, best val acc: 0.8422, best train loss: 0.3396  @ epoch 37064 )\n",
      "[Epoch: 37380] train loss: 0.3805, train acc: 0.8339, val loss: 0.3990, val acc: 0.8260  (best train acc: 0.8518, best val acc: 0.8422, best train loss: 0.3396  @ epoch 37064 )\n",
      "[Epoch: 37400] train loss: 0.3656, train acc: 0.8401, val loss: 0.3905, val acc: 0.8266  (best train acc: 0.8518, best val acc: 0.8422, best train loss: 0.3396  @ epoch 37064 )\n",
      "[Epoch: 37420] train loss: 0.3537, train acc: 0.8447, val loss: 0.3993, val acc: 0.8233  (best train acc: 0.8518, best val acc: 0.8422, best train loss: 0.3396  @ epoch 37064 )\n",
      "[Epoch: 37440] train loss: 0.3659, train acc: 0.8433, val loss: 0.4075, val acc: 0.8229  (best train acc: 0.8518, best val acc: 0.8422, best train loss: 0.3396  @ epoch 37064 )\n",
      "[Epoch: 37460] train loss: 0.3629, train acc: 0.8423, val loss: 0.4192, val acc: 0.8135  (best train acc: 0.8518, best val acc: 0.8422, best train loss: 0.3396  @ epoch 37064 )\n",
      "[Epoch: 37480] train loss: 0.3705, train acc: 0.8365, val loss: 0.4077, val acc: 0.8202  (best train acc: 0.8518, best val acc: 0.8422, best train loss: 0.3396  @ epoch 37064 )\n",
      "[Epoch: 37500] train loss: 0.3540, train acc: 0.8407, val loss: 0.4149, val acc: 0.8263  (best train acc: 0.8518, best val acc: 0.8422, best train loss: 0.3396  @ epoch 37064 )\n",
      "[Epoch: 37520] train loss: 0.3622, train acc: 0.8388, val loss: 0.4138, val acc: 0.8135  (best train acc: 0.8518, best val acc: 0.8422, best train loss: 0.3396  @ epoch 37064 )\n",
      "[Epoch: 37540] train loss: 0.3799, train acc: 0.8339, val loss: 0.4167, val acc: 0.8189  (best train acc: 0.8518, best val acc: 0.8422, best train loss: 0.3396  @ epoch 37064 )\n",
      "[Epoch: 37560] train loss: 0.3591, train acc: 0.8432, val loss: 0.4226, val acc: 0.8037  (best train acc: 0.8518, best val acc: 0.8422, best train loss: 0.3396  @ epoch 37064 )\n",
      "[Epoch: 37580] train loss: 0.3502, train acc: 0.8454, val loss: 0.4152, val acc: 0.8239  (best train acc: 0.8518, best val acc: 0.8422, best train loss: 0.3396  @ epoch 37064 )\n",
      "[Epoch: 37600] train loss: 0.3638, train acc: 0.8412, val loss: 0.4265, val acc: 0.8179  (best train acc: 0.8518, best val acc: 0.8422, best train loss: 0.3396  @ epoch 37064 )\n",
      "[Epoch: 37620] train loss: 0.3771, train acc: 0.8292, val loss: 0.4180, val acc: 0.8182  (best train acc: 0.8518, best val acc: 0.8422, best train loss: 0.3396  @ epoch 37064 )\n",
      "[Epoch: 37640] train loss: 0.3751, train acc: 0.8342, val loss: 0.4066, val acc: 0.8246  (best train acc: 0.8518, best val acc: 0.8422, best train loss: 0.3396  @ epoch 37064 )\n",
      "[Epoch: 37660] train loss: 0.4022, train acc: 0.8282, val loss: 0.4011, val acc: 0.8280  (best train acc: 0.8518, best val acc: 0.8422, best train loss: 0.3396  @ epoch 37064 )\n",
      "[Epoch: 37680] train loss: 0.3553, train acc: 0.8490, val loss: 0.4129, val acc: 0.8219  (best train acc: 0.8518, best val acc: 0.8422, best train loss: 0.3396  @ epoch 37064 )\n",
      "[Epoch: 37700] train loss: 0.3638, train acc: 0.8422, val loss: 0.4252, val acc: 0.8226  (best train acc: 0.8518, best val acc: 0.8422, best train loss: 0.3396  @ epoch 37064 )\n",
      "[Epoch: 37720] train loss: 0.3610, train acc: 0.8395, val loss: 0.4043, val acc: 0.8229  (best train acc: 0.8518, best val acc: 0.8422, best train loss: 0.3396  @ epoch 37064 )\n",
      "[Epoch: 37740] train loss: 0.3615, train acc: 0.8413, val loss: 0.4093, val acc: 0.8256  (best train acc: 0.8518, best val acc: 0.8422, best train loss: 0.3396  @ epoch 37064 )\n",
      "[Epoch: 37760] train loss: 0.3928, train acc: 0.8308, val loss: 0.4118, val acc: 0.8206  (best train acc: 0.8518, best val acc: 0.8422, best train loss: 0.3396  @ epoch 37064 )\n",
      "[Epoch: 37780] train loss: 0.3748, train acc: 0.8347, val loss: 0.4460, val acc: 0.8051  (best train acc: 0.8518, best val acc: 0.8422, best train loss: 0.3396  @ epoch 37064 )\n",
      "[Epoch: 37800] train loss: 0.3616, train acc: 0.8376, val loss: 0.4001, val acc: 0.8256  (best train acc: 0.8518, best val acc: 0.8422, best train loss: 0.3396  @ epoch 37064 )\n",
      "[Epoch: 37820] train loss: 0.3679, train acc: 0.8410, val loss: 0.4036, val acc: 0.8236  (best train acc: 0.8522, best val acc: 0.8422, best train loss: 0.3396  @ epoch 37064 )\n",
      "[Epoch: 37840] train loss: 0.3692, train acc: 0.8389, val loss: 0.4127, val acc: 0.8250  (best train acc: 0.8522, best val acc: 0.8422, best train loss: 0.3396  @ epoch 37064 )\n",
      "[Epoch: 37860] train loss: 0.3646, train acc: 0.8395, val loss: 0.4161, val acc: 0.8226  (best train acc: 0.8522, best val acc: 0.8422, best train loss: 0.3396  @ epoch 37064 )\n",
      "[Epoch: 37880] train loss: 0.3571, train acc: 0.8415, val loss: 0.4206, val acc: 0.8256  (best train acc: 0.8522, best val acc: 0.8422, best train loss: 0.3396  @ epoch 37064 )\n",
      "[Epoch: 37900] train loss: 0.3512, train acc: 0.8439, val loss: 0.4087, val acc: 0.8317  (best train acc: 0.8522, best val acc: 0.8422, best train loss: 0.3396  @ epoch 37064 )\n",
      "[Epoch: 37920] train loss: 0.3781, train acc: 0.8386, val loss: 0.4018, val acc: 0.8270  (best train acc: 0.8522, best val acc: 0.8422, best train loss: 0.3396  @ epoch 37064 )\n",
      "[Epoch: 37940] train loss: 0.3561, train acc: 0.8433, val loss: 0.4092, val acc: 0.8256  (best train acc: 0.8522, best val acc: 0.8422, best train loss: 0.3396  @ epoch 37064 )\n",
      "[Epoch: 37960] train loss: 0.3562, train acc: 0.8432, val loss: 0.4248, val acc: 0.8162  (best train acc: 0.8522, best val acc: 0.8422, best train loss: 0.3396  @ epoch 37064 )\n",
      "[Epoch: 37980] train loss: 0.3798, train acc: 0.8351, val loss: 0.4124, val acc: 0.8172  (best train acc: 0.8522, best val acc: 0.8422, best train loss: 0.3396  @ epoch 37064 )\n",
      "[Epoch: 38000] train loss: 0.3647, train acc: 0.8451, val loss: 0.4066, val acc: 0.8310  (best train acc: 0.8522, best val acc: 0.8422, best train loss: 0.3396  @ epoch 37064 )\n",
      "[Epoch: 38020] train loss: 0.3500, train acc: 0.8480, val loss: 0.4069, val acc: 0.8253  (best train acc: 0.8522, best val acc: 0.8422, best train loss: 0.3396  @ epoch 37064 )\n",
      "[Epoch: 38040] train loss: 0.3641, train acc: 0.8420, val loss: 0.4034, val acc: 0.8307  (best train acc: 0.8522, best val acc: 0.8422, best train loss: 0.3396  @ epoch 37064 )\n",
      "[Epoch: 38060] train loss: 0.3734, train acc: 0.8391, val loss: 0.4097, val acc: 0.8209  (best train acc: 0.8522, best val acc: 0.8422, best train loss: 0.3396  @ epoch 37064 )\n",
      "[Epoch: 38080] train loss: 0.3668, train acc: 0.8375, val loss: 0.3924, val acc: 0.8290  (best train acc: 0.8522, best val acc: 0.8422, best train loss: 0.3396  @ epoch 37064 )\n",
      "[Epoch: 38100] train loss: 0.3632, train acc: 0.8428, val loss: 0.4236, val acc: 0.8118  (best train acc: 0.8522, best val acc: 0.8422, best train loss: 0.3396  @ epoch 37064 )\n",
      "[Epoch: 38120] train loss: 0.3591, train acc: 0.8410, val loss: 0.4079, val acc: 0.8182  (best train acc: 0.8522, best val acc: 0.8422, best train loss: 0.3396  @ epoch 37064 )\n",
      "[Epoch: 38140] train loss: 0.3798, train acc: 0.8347, val loss: 0.4168, val acc: 0.8155  (best train acc: 0.8522, best val acc: 0.8422, best train loss: 0.3396  @ epoch 37064 )\n",
      "[Epoch: 38160] train loss: 0.3573, train acc: 0.8451, val loss: 0.4023, val acc: 0.8236  (best train acc: 0.8522, best val acc: 0.8422, best train loss: 0.3396  @ epoch 37064 )\n",
      "[Epoch: 38180] train loss: 0.3570, train acc: 0.8441, val loss: 0.4072, val acc: 0.8277  (best train acc: 0.8522, best val acc: 0.8422, best train loss: 0.3396  @ epoch 37064 )\n",
      "[Epoch: 38200] train loss: 0.3731, train acc: 0.8415, val loss: 0.3980, val acc: 0.8206  (best train acc: 0.8522, best val acc: 0.8422, best train loss: 0.3396  @ epoch 37064 )\n",
      "[Epoch: 38220] train loss: 0.3661, train acc: 0.8417, val loss: 0.4125, val acc: 0.8209  (best train acc: 0.8522, best val acc: 0.8422, best train loss: 0.3396  @ epoch 37064 )\n",
      "[Epoch: 38240] train loss: 0.3675, train acc: 0.8375, val loss: 0.4215, val acc: 0.8280  (best train acc: 0.8522, best val acc: 0.8422, best train loss: 0.3396  @ epoch 37064 )\n",
      "[Epoch: 38260] train loss: 0.3608, train acc: 0.8454, val loss: 0.4146, val acc: 0.8155  (best train acc: 0.8522, best val acc: 0.8422, best train loss: 0.3396  @ epoch 37064 )\n",
      "[Epoch: 38280] train loss: 0.3498, train acc: 0.8437, val loss: 0.4001, val acc: 0.8297  (best train acc: 0.8522, best val acc: 0.8422, best train loss: 0.3396  @ epoch 37064 )\n",
      "[Epoch: 38300] train loss: 0.3522, train acc: 0.8443, val loss: 0.4106, val acc: 0.8243  (best train acc: 0.8522, best val acc: 0.8422, best train loss: 0.3396  @ epoch 37064 )\n",
      "[Epoch: 38320] train loss: 0.3645, train acc: 0.8390, val loss: 0.4136, val acc: 0.8256  (best train acc: 0.8522, best val acc: 0.8422, best train loss: 0.3396  @ epoch 37064 )\n",
      "[Epoch: 38340] train loss: 0.3603, train acc: 0.8407, val loss: 0.4308, val acc: 0.8084  (best train acc: 0.8522, best val acc: 0.8422, best train loss: 0.3396  @ epoch 37064 )\n",
      "[Epoch: 38360] train loss: 0.3683, train acc: 0.8394, val loss: 0.4215, val acc: 0.8132  (best train acc: 0.8522, best val acc: 0.8422, best train loss: 0.3396  @ epoch 37064 )\n",
      "[Epoch: 38380] train loss: 0.3555, train acc: 0.8470, val loss: 0.4086, val acc: 0.8273  (best train acc: 0.8522, best val acc: 0.8422, best train loss: 0.3396  @ epoch 37064 )\n",
      "[Epoch: 38400] train loss: 0.3547, train acc: 0.8415, val loss: 0.4099, val acc: 0.8246  (best train acc: 0.8522, best val acc: 0.8422, best train loss: 0.3396  @ epoch 37064 )\n",
      "[Epoch: 38420] train loss: 0.3520, train acc: 0.8469, val loss: 0.3994, val acc: 0.8280  (best train acc: 0.8522, best val acc: 0.8422, best train loss: 0.3396  @ epoch 37064 )\n",
      "[Epoch: 38440] train loss: 0.3661, train acc: 0.8435, val loss: 0.4012, val acc: 0.8277  (best train acc: 0.8522, best val acc: 0.8422, best train loss: 0.3396  @ epoch 37064 )\n",
      "[Epoch: 38460] train loss: 0.3578, train acc: 0.8471, val loss: 0.4024, val acc: 0.8293  (best train acc: 0.8522, best val acc: 0.8422, best train loss: 0.3396  @ epoch 37064 )\n",
      "[Epoch: 38480] train loss: 0.3779, train acc: 0.8362, val loss: 0.3992, val acc: 0.8236  (best train acc: 0.8522, best val acc: 0.8422, best train loss: 0.3396  @ epoch 37064 )\n",
      "[Epoch: 38500] train loss: 0.3662, train acc: 0.8351, val loss: 0.3878, val acc: 0.8300  (best train acc: 0.8522, best val acc: 0.8422, best train loss: 0.3396  @ epoch 37064 )\n",
      "[Epoch: 38520] train loss: 0.3517, train acc: 0.8459, val loss: 0.4223, val acc: 0.8300  (best train acc: 0.8522, best val acc: 0.8422, best train loss: 0.3396  @ epoch 37064 )\n",
      "[Epoch: 38540] train loss: 0.3754, train acc: 0.8340, val loss: 0.4079, val acc: 0.8212  (best train acc: 0.8522, best val acc: 0.8422, best train loss: 0.3396  @ epoch 37064 )\n",
      "[Epoch: 38560] train loss: 0.3661, train acc: 0.8386, val loss: 0.3928, val acc: 0.8297  (best train acc: 0.8522, best val acc: 0.8422, best train loss: 0.3396  @ epoch 37064 )\n",
      "[Epoch: 38580] train loss: 0.3616, train acc: 0.8433, val loss: 0.4093, val acc: 0.8169  (best train acc: 0.8522, best val acc: 0.8422, best train loss: 0.3396  @ epoch 37064 )\n",
      "[Epoch: 38600] train loss: 0.3596, train acc: 0.8427, val loss: 0.4170, val acc: 0.8192  (best train acc: 0.8522, best val acc: 0.8422, best train loss: 0.3396  @ epoch 37064 )\n",
      "[Epoch: 38620] train loss: 0.3489, train acc: 0.8481, val loss: 0.4158, val acc: 0.8297  (best train acc: 0.8522, best val acc: 0.8422, best train loss: 0.3396  @ epoch 37064 )\n",
      "[Epoch: 38640] train loss: 0.3630, train acc: 0.8405, val loss: 0.4014, val acc: 0.8209  (best train acc: 0.8522, best val acc: 0.8422, best train loss: 0.3396  @ epoch 37064 )\n",
      "[Epoch: 38660] train loss: 0.3748, train acc: 0.8383, val loss: 0.4006, val acc: 0.8327  (best train acc: 0.8522, best val acc: 0.8422, best train loss: 0.3396  @ epoch 37064 )\n",
      "[Epoch: 38680] train loss: 0.3582, train acc: 0.8442, val loss: 0.4160, val acc: 0.8189  (best train acc: 0.8522, best val acc: 0.8422, best train loss: 0.3396  @ epoch 37064 )\n",
      "[Epoch: 38700] train loss: 0.3743, train acc: 0.8407, val loss: 0.4037, val acc: 0.8300  (best train acc: 0.8522, best val acc: 0.8422, best train loss: 0.3396  @ epoch 37064 )\n",
      "[Epoch: 38720] train loss: 0.3688, train acc: 0.8396, val loss: 0.4034, val acc: 0.8216  (best train acc: 0.8522, best val acc: 0.8422, best train loss: 0.3396  @ epoch 37064 )\n",
      "[Epoch: 38740] train loss: 0.3570, train acc: 0.8441, val loss: 0.4196, val acc: 0.8179  (best train acc: 0.8522, best val acc: 0.8422, best train loss: 0.3396  @ epoch 37064 )\n",
      "[Epoch: 38760] train loss: 0.3622, train acc: 0.8409, val loss: 0.4235, val acc: 0.8152  (best train acc: 0.8522, best val acc: 0.8422, best train loss: 0.3396  @ epoch 37064 )\n",
      "[Epoch: 38780] train loss: 0.3618, train acc: 0.8440, val loss: 0.4026, val acc: 0.8290  (best train acc: 0.8522, best val acc: 0.8422, best train loss: 0.3396  @ epoch 37064 )\n",
      "[Epoch: 38800] train loss: 0.3590, train acc: 0.8454, val loss: 0.4010, val acc: 0.8273  (best train acc: 0.8522, best val acc: 0.8422, best train loss: 0.3396  @ epoch 37064 )\n",
      "[Epoch: 38820] train loss: 0.3656, train acc: 0.8372, val loss: 0.4201, val acc: 0.8179  (best train acc: 0.8522, best val acc: 0.8422, best train loss: 0.3396  @ epoch 37064 )\n",
      "[Epoch: 38840] train loss: 0.3682, train acc: 0.8393, val loss: 0.4212, val acc: 0.8159  (best train acc: 0.8522, best val acc: 0.8422, best train loss: 0.3396  @ epoch 37064 )\n",
      "[Epoch: 38860] train loss: 0.3516, train acc: 0.8444, val loss: 0.4072, val acc: 0.8239  (best train acc: 0.8522, best val acc: 0.8422, best train loss: 0.3396  @ epoch 37064 )\n",
      "[Epoch: 38880] train loss: 0.3686, train acc: 0.8372, val loss: 0.4083, val acc: 0.8239  (best train acc: 0.8522, best val acc: 0.8422, best train loss: 0.3396  @ epoch 37064 )\n",
      "[Epoch: 38900] train loss: 0.3545, train acc: 0.8472, val loss: 0.3941, val acc: 0.8253  (best train acc: 0.8522, best val acc: 0.8422, best train loss: 0.3396  @ epoch 37064 )\n",
      "[Epoch: 38920] train loss: 0.3588, train acc: 0.8437, val loss: 0.4059, val acc: 0.8263  (best train acc: 0.8522, best val acc: 0.8422, best train loss: 0.3396  @ epoch 37064 )\n",
      "[Epoch: 38940] train loss: 0.3474, train acc: 0.8512, val loss: 0.4119, val acc: 0.8233  (best train acc: 0.8522, best val acc: 0.8422, best train loss: 0.3396  @ epoch 37064 )\n",
      "[Epoch: 38960] train loss: 0.3539, train acc: 0.8440, val loss: 0.4282, val acc: 0.8037  (best train acc: 0.8522, best val acc: 0.8422, best train loss: 0.3396  @ epoch 37064 )\n",
      "[Epoch: 38980] train loss: 0.3657, train acc: 0.8429, val loss: 0.4154, val acc: 0.8246  (best train acc: 0.8522, best val acc: 0.8422, best train loss: 0.3396  @ epoch 37064 )\n",
      "[Epoch: 39000] train loss: 0.3646, train acc: 0.8383, val loss: 0.4222, val acc: 0.8253  (best train acc: 0.8522, best val acc: 0.8422, best train loss: 0.3396  @ epoch 37064 )\n",
      "[Epoch: 39020] train loss: 0.3583, train acc: 0.8457, val loss: 0.4222, val acc: 0.8165  (best train acc: 0.8522, best val acc: 0.8422, best train loss: 0.3396  @ epoch 37064 )\n",
      "[Epoch: 39040] train loss: 0.3589, train acc: 0.8417, val loss: 0.4109, val acc: 0.8138  (best train acc: 0.8522, best val acc: 0.8422, best train loss: 0.3396  @ epoch 37064 )\n",
      "[Epoch: 39060] train loss: 0.3567, train acc: 0.8450, val loss: 0.3998, val acc: 0.8253  (best train acc: 0.8522, best val acc: 0.8422, best train loss: 0.3396  @ epoch 37064 )\n",
      "[Epoch: 39080] train loss: 0.3522, train acc: 0.8458, val loss: 0.4194, val acc: 0.8179  (best train acc: 0.8522, best val acc: 0.8422, best train loss: 0.3396  @ epoch 37064 )\n",
      "[Epoch: 39100] train loss: 0.3577, train acc: 0.8462, val loss: 0.3961, val acc: 0.8327  (best train acc: 0.8522, best val acc: 0.8422, best train loss: 0.3396  @ epoch 37064 )\n",
      "[Epoch: 39120] train loss: 0.3753, train acc: 0.8336, val loss: 0.4217, val acc: 0.8266  (best train acc: 0.8522, best val acc: 0.8422, best train loss: 0.3396  @ epoch 37064 )\n",
      "[Epoch: 39140] train loss: 0.3664, train acc: 0.8380, val loss: 0.3963, val acc: 0.8236  (best train acc: 0.8522, best val acc: 0.8422, best train loss: 0.3396  @ epoch 37064 )\n",
      "[Epoch: 39160] train loss: 0.3589, train acc: 0.8422, val loss: 0.4367, val acc: 0.8094  (best train acc: 0.8522, best val acc: 0.8422, best train loss: 0.3396  @ epoch 37064 )\n",
      "[Epoch: 39180] train loss: 0.3723, train acc: 0.8391, val loss: 0.4260, val acc: 0.8128  (best train acc: 0.8522, best val acc: 0.8422, best train loss: 0.3396  @ epoch 37064 )\n",
      "[Epoch: 39200] train loss: 0.3543, train acc: 0.8438, val loss: 0.4004, val acc: 0.8290  (best train acc: 0.8522, best val acc: 0.8422, best train loss: 0.3396  @ epoch 37064 )\n",
      "[Epoch: 39220] train loss: 0.3553, train acc: 0.8458, val loss: 0.4111, val acc: 0.8263  (best train acc: 0.8522, best val acc: 0.8422, best train loss: 0.3396  @ epoch 37064 )\n",
      "[Epoch: 39240] train loss: 0.3553, train acc: 0.8433, val loss: 0.4160, val acc: 0.8256  (best train acc: 0.8526, best val acc: 0.8422, best train loss: 0.3396  @ epoch 37064 )\n",
      "[Epoch: 39260] train loss: 0.3456, train acc: 0.8451, val loss: 0.3991, val acc: 0.8300  (best train acc: 0.8526, best val acc: 0.8422, best train loss: 0.3396  @ epoch 37064 )\n",
      "[Epoch: 39280] train loss: 0.3570, train acc: 0.8433, val loss: 0.4295, val acc: 0.8192  (best train acc: 0.8526, best val acc: 0.8422, best train loss: 0.3396  @ epoch 37064 )\n",
      "[Epoch: 39300] train loss: 0.3500, train acc: 0.8452, val loss: 0.4152, val acc: 0.8189  (best train acc: 0.8556, best val acc: 0.8422, best train loss: 0.3396  @ epoch 37064 )\n",
      "[Epoch: 39320] train loss: 0.3478, train acc: 0.8486, val loss: 0.3969, val acc: 0.8314  (best train acc: 0.8556, best val acc: 0.8422, best train loss: 0.3396  @ epoch 37064 )\n",
      "[Epoch: 39340] train loss: 0.3501, train acc: 0.8481, val loss: 0.4128, val acc: 0.8223  (best train acc: 0.8556, best val acc: 0.8422, best train loss: 0.3396  @ epoch 37064 )\n",
      "[Epoch: 39360] train loss: 0.3630, train acc: 0.8404, val loss: 0.4208, val acc: 0.8250  (best train acc: 0.8556, best val acc: 0.8422, best train loss: 0.3396  @ epoch 37064 )\n",
      "[Epoch: 39380] train loss: 0.3496, train acc: 0.8442, val loss: 0.4210, val acc: 0.8236  (best train acc: 0.8556, best val acc: 0.8422, best train loss: 0.3396  @ epoch 37064 )\n",
      "[Epoch: 39400] train loss: 0.3518, train acc: 0.8422, val loss: 0.4008, val acc: 0.8297  (best train acc: 0.8556, best val acc: 0.8422, best train loss: 0.3396  @ epoch 37064 )\n",
      "[Epoch: 39420] train loss: 0.3483, train acc: 0.8488, val loss: 0.4038, val acc: 0.8347  (best train acc: 0.8556, best val acc: 0.8422, best train loss: 0.3396  @ epoch 37064 )\n",
      "[Epoch: 39440] train loss: 0.3653, train acc: 0.8433, val loss: 0.4109, val acc: 0.8223  (best train acc: 0.8556, best val acc: 0.8422, best train loss: 0.3396  @ epoch 37064 )\n",
      "[Epoch: 39460] train loss: 0.3486, train acc: 0.8480, val loss: 0.3995, val acc: 0.8358  (best train acc: 0.8556, best val acc: 0.8422, best train loss: 0.3396  @ epoch 37064 )\n",
      "[Epoch: 39480] train loss: 0.3542, train acc: 0.8432, val loss: 0.4253, val acc: 0.8159  (best train acc: 0.8556, best val acc: 0.8422, best train loss: 0.3396  @ epoch 37064 )\n",
      "[Epoch: 39500] train loss: 0.3556, train acc: 0.8476, val loss: 0.4084, val acc: 0.8273  (best train acc: 0.8556, best val acc: 0.8422, best train loss: 0.3385  @ epoch 39482 )\n",
      "[Epoch: 39520] train loss: 0.3812, train acc: 0.8386, val loss: 0.4015, val acc: 0.8304  (best train acc: 0.8556, best val acc: 0.8422, best train loss: 0.3385  @ epoch 39482 )\n",
      "[Epoch: 39540] train loss: 0.3672, train acc: 0.8412, val loss: 0.4091, val acc: 0.8297  (best train acc: 0.8556, best val acc: 0.8422, best train loss: 0.3375  @ epoch 39533 )\n",
      "[Epoch: 39560] train loss: 0.3541, train acc: 0.8431, val loss: 0.4156, val acc: 0.8172  (best train acc: 0.8556, best val acc: 0.8422, best train loss: 0.3375  @ epoch 39533 )\n",
      "[Epoch: 39580] train loss: 0.3663, train acc: 0.8351, val loss: 0.4096, val acc: 0.8233  (best train acc: 0.8556, best val acc: 0.8422, best train loss: 0.3375  @ epoch 39533 )\n",
      "[Epoch: 39600] train loss: 0.3466, train acc: 0.8490, val loss: 0.4192, val acc: 0.8202  (best train acc: 0.8556, best val acc: 0.8422, best train loss: 0.3375  @ epoch 39533 )\n",
      "[Epoch: 39620] train loss: 0.3471, train acc: 0.8475, val loss: 0.4117, val acc: 0.8239  (best train acc: 0.8556, best val acc: 0.8422, best train loss: 0.3361  @ epoch 39616 )\n",
      "[Epoch: 39640] train loss: 0.3443, train acc: 0.8476, val loss: 0.4037, val acc: 0.8320  (best train acc: 0.8556, best val acc: 0.8422, best train loss: 0.3361  @ epoch 39616 )\n",
      "[Epoch: 39660] train loss: 0.3508, train acc: 0.8443, val loss: 0.4047, val acc: 0.8297  (best train acc: 0.8556, best val acc: 0.8422, best train loss: 0.3361  @ epoch 39616 )\n",
      "[Epoch: 39680] train loss: 0.3589, train acc: 0.8433, val loss: 0.4202, val acc: 0.8263  (best train acc: 0.8556, best val acc: 0.8422, best train loss: 0.3361  @ epoch 39616 )\n",
      "[Epoch: 39700] train loss: 0.3707, train acc: 0.8354, val loss: 0.4196, val acc: 0.8229  (best train acc: 0.8556, best val acc: 0.8422, best train loss: 0.3361  @ epoch 39616 )\n",
      "[Epoch: 39720] train loss: 0.3604, train acc: 0.8376, val loss: 0.4076, val acc: 0.8287  (best train acc: 0.8556, best val acc: 0.8422, best train loss: 0.3361  @ epoch 39616 )\n",
      "[Epoch: 39740] train loss: 0.3426, train acc: 0.8508, val loss: 0.4331, val acc: 0.8125  (best train acc: 0.8556, best val acc: 0.8422, best train loss: 0.3361  @ epoch 39616 )\n",
      "[Epoch: 39760] train loss: 0.3547, train acc: 0.8467, val loss: 0.4185, val acc: 0.8135  (best train acc: 0.8556, best val acc: 0.8422, best train loss: 0.3361  @ epoch 39616 )\n",
      "[Epoch: 39780] train loss: 0.3657, train acc: 0.8405, val loss: 0.4161, val acc: 0.8175  (best train acc: 0.8556, best val acc: 0.8422, best train loss: 0.3361  @ epoch 39616 )\n",
      "[Epoch: 39800] train loss: 0.3664, train acc: 0.8366, val loss: 0.4349, val acc: 0.8196  (best train acc: 0.8556, best val acc: 0.8422, best train loss: 0.3361  @ epoch 39616 )\n",
      "[Epoch: 39820] train loss: 0.3811, train acc: 0.8331, val loss: 0.4160, val acc: 0.8256  (best train acc: 0.8556, best val acc: 0.8422, best train loss: 0.3361  @ epoch 39616 )\n",
      "[Epoch: 39840] train loss: 0.3409, train acc: 0.8477, val loss: 0.3950, val acc: 0.8358  (best train acc: 0.8556, best val acc: 0.8422, best train loss: 0.3361  @ epoch 39616 )\n",
      "[Epoch: 39860] train loss: 0.3777, train acc: 0.8357, val loss: 0.4057, val acc: 0.8314  (best train acc: 0.8556, best val acc: 0.8422, best train loss: 0.3361  @ epoch 39616 )\n",
      "[Epoch: 39880] train loss: 0.3683, train acc: 0.8398, val loss: 0.4155, val acc: 0.8199  (best train acc: 0.8556, best val acc: 0.8422, best train loss: 0.3361  @ epoch 39616 )\n",
      "[Epoch: 39900] train loss: 0.3459, train acc: 0.8514, val loss: 0.4248, val acc: 0.8219  (best train acc: 0.8561, best val acc: 0.8422, best train loss: 0.3361  @ epoch 39616 )\n",
      "[Epoch: 39920] train loss: 0.3570, train acc: 0.8421, val loss: 0.4223, val acc: 0.8239  (best train acc: 0.8561, best val acc: 0.8422, best train loss: 0.3361  @ epoch 39616 )\n",
      "[Epoch: 39940] train loss: 0.3590, train acc: 0.8370, val loss: 0.4360, val acc: 0.8189  (best train acc: 0.8561, best val acc: 0.8422, best train loss: 0.3361  @ epoch 39616 )\n",
      "[Epoch: 39960] train loss: 0.3527, train acc: 0.8435, val loss: 0.4043, val acc: 0.8253  (best train acc: 0.8561, best val acc: 0.8422, best train loss: 0.3361  @ epoch 39616 )\n",
      "[Epoch: 39980] train loss: 0.3534, train acc: 0.8434, val loss: 0.4004, val acc: 0.8304  (best train acc: 0.8561, best val acc: 0.8422, best train loss: 0.3361  @ epoch 39616 )\n",
      "[Epoch: 40000] train loss: 0.3520, train acc: 0.8463, val loss: 0.4189, val acc: 0.8263  (best train acc: 0.8561, best val acc: 0.8422, best train loss: 0.3361  @ epoch 39616 )\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.99      0.97       100\n",
      "           1       0.72      0.95      0.82       757\n",
      "           2       0.88      0.59      0.71       757\n",
      "           3       0.87      0.86      0.86       757\n",
      "           4       0.87      0.90      0.88       593\n",
      "\n",
      "    accuracy                           0.83      2964\n",
      "   macro avg       0.86      0.86      0.85      2964\n",
      "weighted avg       0.84      0.83      0.82      2964\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 504x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 504x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "lr              = 0.001\n",
    "epochs          = 40000\n",
    "weight_decay    = 0.0005\n",
    "classes         = ['P', 'LP', 'WN', 'LN', 'RN']\n",
    "\n",
    "model = GNN7L_Sage(dataset)\n",
    "preds = train(model, dataset, epochs, lr, weight_decay, classes, model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get predictions from trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.99      0.97       100\n",
      "           1       0.72      0.95      0.82       757\n",
      "           2       0.88      0.59      0.71       757\n",
      "           3       0.87      0.86      0.86       757\n",
      "           4       0.87      0.90      0.88       593\n",
      "\n",
      "    accuracy                           0.83      2964\n",
      "   macro avg       0.86      0.86      0.85      2964\n",
      "weighted avg       0.84      0.83      0.82      2964\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 504x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 504x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "preds, probs, model = predict_from_saved_model(model_name + '_40000_0_0005', dataset, classes, save_to_file=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad3e417715d34ce8ad701b19feadb755",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Precision on top 25 : 0.72\n",
      "[+] Precision on top 50 : 0.66\n",
      "[+] Precision on top 100 : 0.67\n",
      "[+] Precision on top 200 : 0.625\n",
      "[+] Precision on top 500 : 0.53\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('precision_positive_genes.csv') \n",
    "n_positives = n_positives = df['n_positives'][df[disease_Id].idxmax()]\n",
    "\n",
    "preds, probs, model = predict_from_saved_model(model_name+'_40000_0_0005', dataset, classes, save_to_file=False, plot_results=False)\n",
    "\n",
    "ranking = get_ranking(model, dataset, preds, probs, disease_Id, n_positive=n_positives, explanation_nodes_ratio=1, masks_for_seed=10, G=G)\n",
    "\n",
    "### Save ranking to file\n",
    "filename = PATH_TO_RANKINGS + disease_Id + '_' + str(n_positives) + '_new_rankings.txt'\n",
    "with open(filename, 'w') as f:\n",
    "    for line in ranking:\n",
    "        f.write(line + '\\n')\n",
    "\n",
    "cuts = [25, 50, 100, 200, 500]\n",
    "for k in cuts:\n",
    "    precision = validate_with_extended_dataset(ranking[:k], disease_Id, save_ranking_to_file=False)\n",
    "    print('[+] Precision on top', k, ':', precision/k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All positive genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] 672 positive nodes found in the graph\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd130a172fbd4c00b4a6a1f89200cf8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/672 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ranking = get_ranking_from_all_positives(model, dataset, preds, disease_Id, explanation_nodes_ratio=1, masks_for_seed=5, G=G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Save ranking to file\n",
    "filename = PATH_TO_RANKINGS + disease_Id + '_all_positives_new_ranking.txt'\n",
    "with open(filename, 'w') as f:\n",
    "     for line in ranking:\n",
    "        f.write(line + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Precision on top 25 : 0.68\n",
      "[+] Precision on top 50 : 0.72\n",
      "[+] Precision on top 100 : 0.66\n",
      "[+] Precision on top 200 : 0.645\n",
      "[+] Precision on top 500 : 0.574\n"
     ]
    }
   ],
   "source": [
    "cuts = [25, 50, 100, 200, 500]\n",
    "for k in cuts:\n",
    "    precision = validate_with_extended_dataset(ranking[:k], disease_Id, save_ranking_to_file=False)\n",
    "    print('[+] Precision on top', k, ':', precision/k)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6aeea6fe202f5c7201d5940e4573c0a76b23e4e16f0e3784ac81597546f2b3b3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
