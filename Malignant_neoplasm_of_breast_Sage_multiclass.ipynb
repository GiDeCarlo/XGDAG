{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.11.0\n",
      "2.0.4\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "import networkx as nx\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric\n",
    "from torch_geometric.nn.conv import SAGEConv\n",
    "from torch_geometric.utils import from_networkx\n",
    "from torch_geometric.data import InMemoryDataset\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(torch.__version__)\n",
    "print(torch_geometric.__version__)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN4L_Sage (nn.Module):\n",
    "    def __init__(self, data):\n",
    "        super().__init__()\n",
    "        self.conv1 = SAGEConv(data.num_features, 16)\n",
    "        self.conv2 = SAGEConv(16, 16)\n",
    "        self.conv3 = SAGEConv(16, 16)\n",
    "        self.conv4 = SAGEConv(16, int(data.num_classes))\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        x = F.relu(self.conv3(x, edge_index))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv4(x, edge_index)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "class GNN7L_Sage (nn.Module):\n",
    "    def __init__(self, data):\n",
    "        super().__init__()\n",
    "        self.conv1 = SAGEConv(data.num_features, 16)\n",
    "        self.conv2 = SAGEConv(16, 16)\n",
    "        self.conv3 = SAGEConv(16, 16)\n",
    "        self.conv4 = SAGEConv(16, 16)\n",
    "        self.conv5 = SAGEConv(16, 16)\n",
    "        self.conv6 = SAGEConv(16, 16)\n",
    "        self.conv7 = SAGEConv(16, int(data.num_classes))\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        x = F.relu(self.conv3(x, edge_index))\n",
    "        x = F.relu(self.conv4(x, edge_index))\n",
    "        x = F.relu(self.conv5(x, edge_index))\n",
    "        x = F.relu(self.conv6(x, edge_index))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv7(x, edge_index)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset class: it allows to translate a vector (Graph, Attributes, Labels)\n",
    "# into a dataset compatible with the PyTorch models.\n",
    "# \n",
    "# Parameters:\n",
    "# - G: NetworkX graph\n",
    "# - Labels: of the nodes used for classification\n",
    "# - attributes: List of the nodes' attributes\n",
    "\n",
    "class MyDataset(InMemoryDataset):\n",
    "  def __init__(self, G, labels, attributes, num_classes=2):\n",
    "    super(MyDataset, self).__init__('.', None, None, None)\n",
    "\n",
    "    # import data from the networkx graph with the attributes of the nodes\n",
    "    data = from_networkx(G, attributes)\n",
    "      \n",
    "    y = torch.from_numpy(labels).type(torch.long)\n",
    "\n",
    "    data.x = data.x.float()\n",
    "    data.y = y.clone().detach()\n",
    "    data.num_classes = num_classes\n",
    "\n",
    "    # Using train_test_split function from sklearn to stratify train/test/val sets\n",
    "    indices = range(G.number_of_nodes())\n",
    "    # Stratified split of train/test/val sets. Returned indices are used to create the masks\n",
    "    X_train, X_test, y_train, y_test, train_idx, test_idx = train_test_split(data.x, data.y, indices, test_size=0.3, stratify=labels, random_state=42)\n",
    "    # To create validation set, test set is splitted in half\n",
    "    X_test, X_val, y_test, y_val, test_idx, val_idx = train_test_split(X_test, y_test, test_idx, test_size=0.5, stratify=y_test, random_state=42)\n",
    "\n",
    "    n_nodes = G.number_of_nodes()\n",
    "    train_mask  = torch.zeros(n_nodes, dtype=torch.bool)\n",
    "    test_mask   = torch.zeros(n_nodes, dtype=torch.bool)\n",
    "    val_mask    = torch.zeros(n_nodes, dtype=torch.bool)\n",
    "    \n",
    "    for idx in train_idx:\n",
    "      train_mask[idx] = True\n",
    "\n",
    "    for idx in test_idx:\n",
    "      test_mask[idx] = True\n",
    "    \n",
    "    for idx in val_idx:\n",
    "      val_mask[idx] = True\n",
    "\n",
    "    data['train_mask']  = train_mask\n",
    "    data['test_mask']   = test_mask\n",
    "    data['val_mask']    = val_mask\n",
    "\n",
    "    self.data, self.slices = self.collate([data])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data, epochs = 200, classes = ['0','1'], lr = 0.001, weight_decay=0, cm_title = 'GNN'):\n",
    "    title = cm_title + '_' + str(epochs) + '_' + str(weight_decay).replace('.', '_')\n",
    "\n",
    "    model_path  = 'Models/' + title\n",
    "    image_path  = 'Images/' + title\n",
    "    report_path = 'Reports/' + title + '.csv'\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    train_mask  = data['train_mask']\n",
    "    test_mask   = data['test_mask']\n",
    "    val_mask    = data['val_mask']\n",
    "\n",
    "    labels    = data.y\n",
    "    output = ''\n",
    "\n",
    "    # list to plot the train accuracy\n",
    "    train_acc_curve = []\n",
    "    train_lss_curve = []\n",
    "\n",
    "    best_train_acc  = 0\n",
    "    best_val_acc    = 0\n",
    "    # best_test_acc   = 0\n",
    "    best_train_lss  = 999\n",
    "\n",
    "    for e in tqdm(range(epochs+1)):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        logits      = model(data)\n",
    "        output      = logits.argmax(1)\n",
    "        #Â train_loss  = F.cross_entropy(logits[train_mask], labels[train_mask])\n",
    "        train_loss  = F.nll_loss(logits[train_mask], labels[train_mask])\n",
    "        train_acc   = (output[train_mask] == labels[train_mask]).float().mean()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Append train acc. to plot curve later\n",
    "        train_acc_curve.append(train_acc.item())\n",
    "        train_lss_curve.append(train_loss.item())\n",
    "\n",
    "        if train_acc > best_train_acc:\n",
    "            best_train_acc = train_acc\n",
    "\n",
    "        # Evaluation and test\n",
    "        model.eval()\n",
    "        logits      = model(data)\n",
    "        output      = logits.argmax(1)\n",
    "        # val_loss    = F.cross_entropy(logits[val_mask], labels[val_mask])\n",
    "        val_loss    = F.nll_loss(logits[val_mask], labels[val_mask])\n",
    "        val_acc     = (output[val_mask] == labels[val_mask]).float().mean()\n",
    "        # test_acc    = (output[test_mask] == labels[test_mask]).float().mean()\n",
    "\n",
    "        # Update best test/val acc.\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "        \n",
    "        # if test_acc > best_test_acc:\n",
    "        #     best_test_acc = test_acc\n",
    "        \n",
    "        # Save model with best train loss\n",
    "        if train_loss < best_train_lss:\n",
    "            best_train_lss = train_loss\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "\n",
    "        if e % 20 == 0 or e == epochs:\n",
    "            print('[Epoch: {:04d}]'.format(e),\n",
    "            'train loss: {:.4f},'.format(train_loss.item()),\n",
    "            'train acc: {:.4f},'.format(train_acc.item()),\n",
    "            'val loss: {:.4f},'.format(val_loss.item()),\n",
    "            'val acc: {:.4f} '.format(val_acc.item()),\n",
    "            '(best train acc: {:.4f},'.format(best_train_acc.item()),\n",
    "            'best val acc: {:.4f},'.format(best_val_acc.item()),\n",
    "            'best train loss: {:.4f})'.format(best_train_lss.item()))\n",
    "    \n",
    "    # Plot training accuracy curve\n",
    "    plt.figure(figsize = (12,7))\n",
    "    plt.plot(train_acc_curve)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Train Accuracy')\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize = (12,7))\n",
    "    plt.plot(train_lss_curve)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Train Loss')\n",
    "    plt.show()\n",
    "\n",
    "    # Load best model\n",
    "    loaded_model = GNN7L_Sage(data).to(device)\n",
    "    loaded_model.load_state_dict(torch.load(model_path))\n",
    "    loaded_model.eval()\n",
    "    logits = loaded_model(data)\n",
    "    output = logits.argmax(1)\n",
    "\n",
    "    print(classification_report(labels[test_mask].to('cpu'), output[test_mask].to('cpu')))\n",
    "\n",
    "    class_report = classification_report(labels[test_mask].to('cpu'), output[test_mask].to('cpu'), output_dict=True)\n",
    "    classification_report_dataframe = pd.DataFrame(class_report)\n",
    "    classification_report_dataframe.to_csv(report_path)\n",
    "\n",
    "    #Confusion Matrix\n",
    "    norms = [None, \"true\"]\n",
    "    for norm in norms:\n",
    "        cm = confusion_matrix(labels[test_mask].to('cpu'), output[test_mask].to('cpu'), normalize=norm)\n",
    "\n",
    "        plt.figure(figsize=(7,7))\n",
    "        \n",
    "        if norm == \"true\":\n",
    "            sn.heatmap(cm, annot=True, fmt=\".3f\", linewidths=.5, square = True, cmap = 'BuPu', xticklabels = classes, yticklabels = classes)\n",
    "        else:\n",
    "            sn.heatmap(cm, annot=True, fmt=\".0f\", linewidths=.5, square = True, cmap = 'BuPu', xticklabels = classes, yticklabels = classes)\n",
    "        plt.title(cm_title)\n",
    "        plt.ylabel('Actual label')\n",
    "        plt.xlabel('Predicted label')\n",
    "\n",
    "        if norm == None:\n",
    "            plt.savefig(image_path + '_notNorm.png')\n",
    "        else:\n",
    "            plt.savefig(image_path + '_Norm.png')\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NeDBIT Multiclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18736, 2)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "G = nx.read_gml('Graphs/graph_with_normalized_nedbit.gml')\n",
    "\n",
    "seed_genes          = pd.read_csv('Datasets/C0006142_Malignant_neoplasm_of_breast_seed_genes.txt', header=None, sep=' ')\n",
    "seed_genes.columns  = [\"name\", \"GDA Score\"]\n",
    "seeds_list          = seed_genes[\"name\"].values.tolist()\n",
    "\n",
    "nedbit_scores = pd.read_csv('Datasets/C0006142_Malignant_neoplasm_of_breast_features_Score.csv')\n",
    "\n",
    "# Remove seed genes\n",
    "nedbit_scores_not_seed = nedbit_scores[~nedbit_scores['name'].isin(seeds_list)]\n",
    "print(nedbit_scores_not_seed.shape)\n",
    "\n",
    "# Sort scores for quartile division\n",
    "nedbit_scores_not_seed = nedbit_scores_not_seed.sort_values(by = \"out\", ascending = False)\n",
    "pseudo_labels = pd.qcut(x = nedbit_scores_not_seed[\"out\"], q = 4, labels = [\"RN\", \"LN\", \"WN\", \"LP\"])\n",
    "\n",
    "plt.hist(pseudo_labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19761"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nedbit_scores_not_seed['label'] = pseudo_labels\n",
    "\n",
    "nedbit_scores_seed = nedbit_scores[nedbit_scores['name'].isin(seeds_list)]\n",
    "nedbit_scores_seed = nedbit_scores_seed.assign(label = 'P')\n",
    "\n",
    "# Convert dataframe to dict for searching nodes and their labels\n",
    "not_seed_labels = dict(zip(nedbit_scores_not_seed['name'], nedbit_scores_not_seed['label']))\n",
    "seed_labels     = dict(zip(nedbit_scores_seed['name'], nedbit_scores_seed['label']))\n",
    "\n",
    "labels_dict = {'P':0, 'LP': 1, 'WN': 2, 'LN': 3, 'RN': 4}\n",
    "labels = []\n",
    "\n",
    "for node in G:\n",
    "    if node in not_seed_labels:\n",
    "        labels.append(labels_dict[not_seed_labels[node]])\n",
    "    else:\n",
    "        labels.append(labels_dict[seed_labels[node]])\n",
    "\n",
    "labels = np.asarray(labels)\n",
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "attributes = ['degree', 'ring', 'NetRank', 'NetShort', 'HeatDiff', 'InfoDiff']\n",
    "\n",
    "dataset_with_nedbit = MyDataset(G, labels, attributes, num_classes=5)\n",
    "data_with_nedbit = dataset_with_nedbit[0]\n",
    "\n",
    "plt.hist(labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnn_sage = GNN4L_Sage(data_with_nedbit).to(device)\n",
    "pred = train(gnn_sage, data_with_nedbit.to(device), 1000, cm_title='SAGE4L_multiclass_16HC_LSTM', classes=['P', 'LP', 'WN', 'LN', 'RN'], weight_decay=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pmaur\\miniconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\pmaur\\miniconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\pmaur\\miniconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\pmaur\\miniconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\pmaur\\miniconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\pmaur\\miniconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       154\n",
      "           1       0.54      0.75      0.63       703\n",
      "           2       0.37      0.27      0.31       702\n",
      "           3       0.48      0.50      0.49       703\n",
      "           4       0.67      0.71      0.69       702\n",
      "\n",
      "    accuracy                           0.53      2964\n",
      "   macro avg       0.41      0.45      0.42      2964\n",
      "weighted avg       0.49      0.53      0.50      2964\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 504x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 504x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs = 40000\n",
    "cm_title='SAGE4L_multiclass_16HC'\n",
    "classes=['P', 'LP', 'WN', 'LN', 'RN']\n",
    "weight_decay=0.0005\n",
    "model = gnn_sage\n",
    "data = data_with_nedbit\n",
    "lr = 0.001\n",
    "labels    = data_with_nedbit.y\n",
    "\n",
    "title = cm_title + '_' + str(epochs) + '_' + str(weight_decay).replace('.', '_')\n",
    "\n",
    "model_path  = 'Models/' + title\n",
    "image_path  = 'Images/' + title\n",
    "report_path = 'Reports/' + title + '.csv'\n",
    "\n",
    "train_mask  = data['train_mask']\n",
    "test_mask   = data['test_mask']\n",
    "val_mask    = data['val_mask']\n",
    "\n",
    "# Load best model\n",
    "loaded_model = GNN4L_Sage(data).to(device)\n",
    "loaded_model.load_state_dict(torch.load(model_path))\n",
    "loaded_model.eval()\n",
    "logits = loaded_model(data)\n",
    "output = logits.argmax(1)\n",
    "\n",
    "print(classification_report(labels[test_mask].to('cpu'), output[test_mask].to('cpu')))\n",
    "\n",
    "class_report = classification_report(labels[test_mask].to('cpu'), output[test_mask].to('cpu'), output_dict=True)\n",
    "classification_report_dataframe = pd.DataFrame(class_report)\n",
    "classification_report_dataframe.to_csv(report_path)\n",
    "\n",
    "#Confusion Matrix\n",
    "norms = [None, \"true\"]\n",
    "for norm in norms:\n",
    "    cm = confusion_matrix(labels[test_mask].to('cpu'), output[test_mask].to('cpu'), normalize=norm)\n",
    "\n",
    "    plt.figure(figsize=(7,7))\n",
    "    \n",
    "    if norm == \"true\":\n",
    "        sn.heatmap(cm, annot=True, fmt=\".3f\", linewidths=.5, square = True, cmap = 'BuPu', xticklabels = classes, yticklabels = classes)\n",
    "    else:\n",
    "        sn.heatmap(cm, annot=True, fmt=\".0f\", linewidths=.5, square = True, cmap = 'BuPu', xticklabels = classes, yticklabels = classes)\n",
    "    plt.title(cm_title)\n",
    "    plt.ylabel('Actual label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "    if norm == None:\n",
    "        plt.savefig(image_path + '_notNorm.png')\n",
    "    else:\n",
    "        plt.savefig(image_path + '_Norm.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7 Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88e077f0ba2444b3a78e01a659ab5347",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40001 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch: 0000] train loss: 89.1047, train acc: 0.2359, val loss: 74.7601, val acc: 0.2368  (best train acc: 0.2359, best val acc: 0.2368, best train loss: 89.1047)\n",
      "[Epoch: 0020] train loss: 5.1212, train acc: 0.2501, val loss: 2.3855, val acc: 0.3815  (best train acc: 0.2693, best val acc: 0.4445, best train loss: 5.1212)\n",
      "[Epoch: 0040] train loss: 1.4737, train acc: 0.2976, val loss: 1.2376, val acc: 0.4685  (best train acc: 0.3991, best val acc: 0.5747, best train loss: 1.4737)\n",
      "[Epoch: 0060] train loss: 1.3117, train acc: 0.4592, val loss: 1.1191, val acc: 0.6725  (best train acc: 0.4592, best val acc: 0.6725, best train loss: 1.3117)\n",
      "[Epoch: 0080] train loss: 1.2965, train acc: 0.4680, val loss: 1.0616, val acc: 0.6789  (best train acc: 0.4727, best val acc: 0.6789, best train loss: 1.2877)\n",
      "[Epoch: 0100] train loss: 1.2605, train acc: 0.4823, val loss: 1.0054, val acc: 0.7184  (best train acc: 0.4862, best val acc: 0.7184, best train loss: 1.2581)\n",
      "[Epoch: 0120] train loss: 1.2424, train acc: 0.4853, val loss: 0.9756, val acc: 0.7605  (best train acc: 0.4920, best val acc: 0.7605, best train loss: 1.2422)\n",
      "[Epoch: 0140] train loss: 1.2256, train acc: 0.4933, val loss: 0.9546, val acc: 0.7730  (best train acc: 0.4993, best val acc: 0.7730, best train loss: 1.2229)\n",
      "[Epoch: 0160] train loss: 1.2207, train acc: 0.4910, val loss: 0.9417, val acc: 0.7804  (best train acc: 0.5019, best val acc: 0.7825, best train loss: 1.2051)\n",
      "[Epoch: 0180] train loss: 1.1996, train acc: 0.5044, val loss: 0.9262, val acc: 0.7885  (best train acc: 0.5076, best val acc: 0.7892, best train loss: 1.1926)\n",
      "[Epoch: 0200] train loss: 1.1858, train acc: 0.5100, val loss: 0.8983, val acc: 0.7953  (best train acc: 0.5194, best val acc: 0.7966, best train loss: 1.1735)\n",
      "[Epoch: 0220] train loss: 1.1690, train acc: 0.5278, val loss: 0.8894, val acc: 0.8040  (best train acc: 0.5278, best val acc: 0.8047, best train loss: 1.1690)\n",
      "[Epoch: 0240] train loss: 1.1563, train acc: 0.5221, val loss: 0.8763, val acc: 0.7953  (best train acc: 0.5312, best val acc: 0.8078, best train loss: 1.1561)\n",
      "[Epoch: 0260] train loss: 1.1483, train acc: 0.5278, val loss: 0.8552, val acc: 0.8118  (best train acc: 0.5347, best val acc: 0.8135, best train loss: 1.1426)\n",
      "[Epoch: 0280] train loss: 1.1338, train acc: 0.5294, val loss: 0.8437, val acc: 0.8135  (best train acc: 0.5347, best val acc: 0.8138, best train loss: 1.1258)\n",
      "[Epoch: 0300] train loss: 1.1188, train acc: 0.5420, val loss: 0.8249, val acc: 0.8196  (best train acc: 0.5420, best val acc: 0.8196, best train loss: 1.1137)\n",
      "[Epoch: 0320] train loss: 1.1029, train acc: 0.5419, val loss: 0.8004, val acc: 0.8290  (best train acc: 0.5487, best val acc: 0.8290, best train loss: 1.0980)\n",
      "[Epoch: 0340] train loss: 1.0989, train acc: 0.5436, val loss: 0.7988, val acc: 0.8297  (best train acc: 0.5599, best val acc: 0.8364, best train loss: 1.0754)\n",
      "[Epoch: 0360] train loss: 1.0794, train acc: 0.5502, val loss: 0.7593, val acc: 0.8374  (best train acc: 0.5599, best val acc: 0.8384, best train loss: 1.0754)\n",
      "[Epoch: 0380] train loss: 1.0742, train acc: 0.5544, val loss: 0.7758, val acc: 0.8408  (best train acc: 0.5599, best val acc: 0.8455, best train loss: 1.0646)\n",
      "[Epoch: 0400] train loss: 1.0580, train acc: 0.5631, val loss: 0.7398, val acc: 0.8533  (best train acc: 0.5687, best val acc: 0.8533, best train loss: 1.0526)\n",
      "[Epoch: 0420] train loss: 1.0348, train acc: 0.5722, val loss: 0.7221, val acc: 0.8607  (best train acc: 0.5819, best val acc: 0.8614, best train loss: 1.0283)\n",
      "[Epoch: 0440] train loss: 1.0232, train acc: 0.5790, val loss: 0.6984, val acc: 0.8577  (best train acc: 0.5869, best val acc: 0.8631, best train loss: 1.0220)\n",
      "[Epoch: 0460] train loss: 1.0108, train acc: 0.6012, val loss: 0.6932, val acc: 0.8644  (best train acc: 0.6014, best val acc: 0.8654, best train loss: 1.0020)\n",
      "[Epoch: 0480] train loss: 0.9954, train acc: 0.6119, val loss: 0.6812, val acc: 0.8637  (best train acc: 0.6119, best val acc: 0.8705, best train loss: 0.9954)\n",
      "[Epoch: 0500] train loss: 0.9852, train acc: 0.6207, val loss: 0.6670, val acc: 0.8742  (best train acc: 0.6207, best val acc: 0.8742, best train loss: 0.9805)\n",
      "[Epoch: 0520] train loss: 0.9987, train acc: 0.6060, val loss: 0.6490, val acc: 0.8776  (best train acc: 0.6260, best val acc: 0.8806, best train loss: 0.9671)\n",
      "[Epoch: 0540] train loss: 0.9686, train acc: 0.6168, val loss: 0.6388, val acc: 0.8806  (best train acc: 0.6260, best val acc: 0.8806, best train loss: 0.9655)\n",
      "[Epoch: 0560] train loss: 0.9672, train acc: 0.6170, val loss: 0.6368, val acc: 0.8793  (best train acc: 0.6260, best val acc: 0.8813, best train loss: 0.9590)\n",
      "[Epoch: 0580] train loss: 0.9636, train acc: 0.6160, val loss: 0.6209, val acc: 0.8853  (best train acc: 0.6275, best val acc: 0.8863, best train loss: 0.9474)\n",
      "[Epoch: 0600] train loss: 0.9598, train acc: 0.6170, val loss: 0.6221, val acc: 0.8826  (best train acc: 0.6275, best val acc: 0.8877, best train loss: 0.9452)\n",
      "[Epoch: 0620] train loss: 0.9540, train acc: 0.6200, val loss: 0.6202, val acc: 0.8857  (best train acc: 0.6280, best val acc: 0.8887, best train loss: 0.9403)\n",
      "[Epoch: 0640] train loss: 0.9596, train acc: 0.6109, val loss: 0.6046, val acc: 0.8803  (best train acc: 0.6283, best val acc: 0.8887, best train loss: 0.9403)\n",
      "[Epoch: 0660] train loss: 0.9378, train acc: 0.6220, val loss: 0.6166, val acc: 0.8664  (best train acc: 0.6283, best val acc: 0.8911, best train loss: 0.9335)\n",
      "[Epoch: 0680] train loss: 0.9410, train acc: 0.6187, val loss: 0.5877, val acc: 0.8901  (best train acc: 0.6290, best val acc: 0.8931, best train loss: 0.9203)\n",
      "[Epoch: 0700] train loss: 0.9360, train acc: 0.6212, val loss: 0.5806, val acc: 0.8890  (best train acc: 0.6298, best val acc: 0.8934, best train loss: 0.9203)\n",
      "[Epoch: 0720] train loss: 0.9267, train acc: 0.6292, val loss: 0.5789, val acc: 0.8931  (best train acc: 0.6312, best val acc: 0.8934, best train loss: 0.9184)\n",
      "[Epoch: 0740] train loss: 0.9309, train acc: 0.6183, val loss: 0.5726, val acc: 0.8927  (best train acc: 0.6313, best val acc: 0.8934, best train loss: 0.9124)\n",
      "[Epoch: 0760] train loss: 0.9485, train acc: 0.6107, val loss: 0.6263, val acc: 0.8455  (best train acc: 0.6313, best val acc: 0.8934, best train loss: 0.9124)\n",
      "[Epoch: 0780] train loss: 0.9210, train acc: 0.6225, val loss: 0.5930, val acc: 0.8762  (best train acc: 0.6313, best val acc: 0.8934, best train loss: 0.9124)\n",
      "[Epoch: 0800] train loss: 0.9155, train acc: 0.6227, val loss: 0.5658, val acc: 0.8907  (best train acc: 0.6313, best val acc: 0.8941, best train loss: 0.9072)\n",
      "[Epoch: 0820] train loss: 0.8987, train acc: 0.6321, val loss: 0.5606, val acc: 0.8931  (best train acc: 0.6321, best val acc: 0.8941, best train loss: 0.8987)\n",
      "[Epoch: 0840] train loss: 0.9124, train acc: 0.6249, val loss: 0.5610, val acc: 0.8958  (best train acc: 0.6321, best val acc: 0.8958, best train loss: 0.8987)\n",
      "[Epoch: 0860] train loss: 0.8993, train acc: 0.6302, val loss: 0.5461, val acc: 0.8911  (best train acc: 0.6340, best val acc: 0.8968, best train loss: 0.8973)\n",
      "[Epoch: 0880] train loss: 0.8964, train acc: 0.6298, val loss: 0.5480, val acc: 0.8914  (best train acc: 0.6340, best val acc: 0.8968, best train loss: 0.8964)\n",
      "[Epoch: 0900] train loss: 0.9267, train acc: 0.6165, val loss: 0.5582, val acc: 0.8847  (best train acc: 0.6340, best val acc: 0.8968, best train loss: 0.8896)\n",
      "[Epoch: 0920] train loss: 0.9156, train acc: 0.6190, val loss: 0.5436, val acc: 0.8897  (best train acc: 0.6340, best val acc: 0.8968, best train loss: 0.8892)\n",
      "[Epoch: 0940] train loss: 0.9048, train acc: 0.6223, val loss: 0.5605, val acc: 0.8702  (best train acc: 0.6340, best val acc: 0.8968, best train loss: 0.8892)\n",
      "[Epoch: 0960] train loss: 0.9014, train acc: 0.6205, val loss: 0.5337, val acc: 0.8887  (best train acc: 0.6340, best val acc: 0.8968, best train loss: 0.8891)\n",
      "[Epoch: 0980] train loss: 0.8761, train acc: 0.6318, val loss: 0.5288, val acc: 0.8914  (best train acc: 0.6340, best val acc: 0.8968, best train loss: 0.8761)\n",
      "[Epoch: 1000] train loss: 0.8810, train acc: 0.6231, val loss: 0.5047, val acc: 0.8890  (best train acc: 0.6343, best val acc: 0.8968, best train loss: 0.8548)\n",
      "[Epoch: 1020] train loss: 0.8577, train acc: 0.6362, val loss: 0.4894, val acc: 0.8874  (best train acc: 0.6413, best val acc: 0.8968, best train loss: 0.8503)\n",
      "[Epoch: 1040] train loss: 0.8364, train acc: 0.6510, val loss: 0.4956, val acc: 0.8927  (best train acc: 0.6510, best val acc: 0.8968, best train loss: 0.8364)\n",
      "[Epoch: 1060] train loss: 0.8532, train acc: 0.6437, val loss: 0.4785, val acc: 0.8941  (best train acc: 0.6601, best val acc: 0.8968, best train loss: 0.8328)\n",
      "[Epoch: 1080] train loss: 0.8326, train acc: 0.6541, val loss: 0.4764, val acc: 0.8897  (best train acc: 0.6601, best val acc: 0.8968, best train loss: 0.8326)\n",
      "[Epoch: 1100] train loss: 0.8434, train acc: 0.6443, val loss: 0.4882, val acc: 0.8874  (best train acc: 0.6630, best val acc: 0.8998, best train loss: 0.8300)\n",
      "[Epoch: 1120] train loss: 0.8558, train acc: 0.6322, val loss: 0.4621, val acc: 0.8958  (best train acc: 0.6739, best val acc: 0.8998, best train loss: 0.8119)\n",
      "[Epoch: 1140] train loss: 0.8441, train acc: 0.6649, val loss: 0.4648, val acc: 0.8907  (best train acc: 0.6799, best val acc: 0.8998, best train loss: 0.8119)\n",
      "[Epoch: 1160] train loss: 0.8414, train acc: 0.6492, val loss: 0.4648, val acc: 0.9002  (best train acc: 0.6842, best val acc: 0.9015, best train loss: 0.8073)\n",
      "[Epoch: 1180] train loss: 0.8145, train acc: 0.6859, val loss: 0.4681, val acc: 0.8803  (best train acc: 0.6881, best val acc: 0.9015, best train loss: 0.8056)\n",
      "[Epoch: 1200] train loss: 0.8283, train acc: 0.6570, val loss: 0.4336, val acc: 0.8975  (best train acc: 0.6911, best val acc: 0.9015, best train loss: 0.8056)\n",
      "[Epoch: 1220] train loss: 0.8153, train acc: 0.6664, val loss: 0.4541, val acc: 0.8971  (best train acc: 0.6911, best val acc: 0.9029, best train loss: 0.7902)\n",
      "[Epoch: 1240] train loss: 0.8033, train acc: 0.6818, val loss: 0.4597, val acc: 0.8830  (best train acc: 0.7029, best val acc: 0.9029, best train loss: 0.7897)\n",
      "[Epoch: 1260] train loss: 0.7980, train acc: 0.6898, val loss: 0.4500, val acc: 0.8975  (best train acc: 0.7039, best val acc: 0.9035, best train loss: 0.7871)\n",
      "[Epoch: 1280] train loss: 0.7975, train acc: 0.6953, val loss: 0.4449, val acc: 0.8954  (best train acc: 0.7043, best val acc: 0.9035, best train loss: 0.7871)\n",
      "[Epoch: 1300] train loss: 0.8043, train acc: 0.6805, val loss: 0.4362, val acc: 0.9005  (best train acc: 0.7058, best val acc: 0.9035, best train loss: 0.7744)\n",
      "[Epoch: 1320] train loss: 0.7987, train acc: 0.7034, val loss: 0.4394, val acc: 0.8988  (best train acc: 0.7098, best val acc: 0.9035, best train loss: 0.7744)\n",
      "[Epoch: 1340] train loss: 0.7877, train acc: 0.6972, val loss: 0.4377, val acc: 0.8988  (best train acc: 0.7098, best val acc: 0.9035, best train loss: 0.7744)\n",
      "[Epoch: 1360] train loss: 0.8114, train acc: 0.6855, val loss: 0.4233, val acc: 0.8975  (best train acc: 0.7178, best val acc: 0.9039, best train loss: 0.7564)\n",
      "[Epoch: 1380] train loss: 0.8431, train acc: 0.6635, val loss: 0.4736, val acc: 0.8793  (best train acc: 0.7178, best val acc: 0.9039, best train loss: 0.7564)\n",
      "[Epoch: 1400] train loss: 0.8000, train acc: 0.6959, val loss: 0.4504, val acc: 0.8762  (best train acc: 0.7178, best val acc: 0.9039, best train loss: 0.7564)\n",
      "[Epoch: 1420] train loss: 0.7996, train acc: 0.6943, val loss: 0.4037, val acc: 0.9029  (best train acc: 0.7178, best val acc: 0.9039, best train loss: 0.7564)\n",
      "[Epoch: 1440] train loss: 0.7987, train acc: 0.6878, val loss: 0.4156, val acc: 0.9029  (best train acc: 0.7178, best val acc: 0.9039, best train loss: 0.7564)\n",
      "[Epoch: 1460] train loss: 0.7718, train acc: 0.7109, val loss: 0.4161, val acc: 0.9008  (best train acc: 0.7274, best val acc: 0.9039, best train loss: 0.7454)\n",
      "[Epoch: 1480] train loss: 0.7748, train acc: 0.7135, val loss: 0.4182, val acc: 0.8954  (best train acc: 0.7274, best val acc: 0.9042, best train loss: 0.7454)\n",
      "[Epoch: 1500] train loss: 0.7804, train acc: 0.7090, val loss: 0.4232, val acc: 0.9042  (best train acc: 0.7274, best val acc: 0.9042, best train loss: 0.7454)\n",
      "[Epoch: 1520] train loss: 0.7655, train acc: 0.7058, val loss: 0.3990, val acc: 0.9032  (best train acc: 0.7274, best val acc: 0.9059, best train loss: 0.7454)\n",
      "[Epoch: 1540] train loss: 0.7757, train acc: 0.7095, val loss: 0.4187, val acc: 0.9035  (best train acc: 0.7274, best val acc: 0.9059, best train loss: 0.7454)\n",
      "[Epoch: 1560] train loss: 0.8074, train acc: 0.6882, val loss: 0.4218, val acc: 0.8951  (best train acc: 0.7274, best val acc: 0.9059, best train loss: 0.7454)\n",
      "[Epoch: 1580] train loss: 0.7643, train acc: 0.7202, val loss: 0.4090, val acc: 0.9049  (best train acc: 0.7282, best val acc: 0.9059, best train loss: 0.7397)\n",
      "[Epoch: 1600] train loss: 0.8060, train acc: 0.6842, val loss: 0.4097, val acc: 0.8992  (best train acc: 0.7283, best val acc: 0.9073, best train loss: 0.7397)\n",
      "[Epoch: 1620] train loss: 0.7650, train acc: 0.7106, val loss: 0.3975, val acc: 0.9073  (best train acc: 0.7326, best val acc: 0.9073, best train loss: 0.7392)\n",
      "[Epoch: 1640] train loss: 0.7930, train acc: 0.6849, val loss: 0.3885, val acc: 0.9032  (best train acc: 0.7326, best val acc: 0.9073, best train loss: 0.7392)\n",
      "[Epoch: 1660] train loss: 0.7803, train acc: 0.7009, val loss: 0.3969, val acc: 0.9073  (best train acc: 0.7327, best val acc: 0.9073, best train loss: 0.7392)\n",
      "[Epoch: 1680] train loss: 0.7565, train acc: 0.7244, val loss: 0.4145, val acc: 0.9046  (best train acc: 0.7332, best val acc: 0.9076, best train loss: 0.7308)\n",
      "[Epoch: 1700] train loss: 0.7633, train acc: 0.7055, val loss: 0.3993, val acc: 0.9022  (best train acc: 0.7332, best val acc: 0.9076, best train loss: 0.7198)\n",
      "[Epoch: 1720] train loss: 0.7635, train acc: 0.7219, val loss: 0.4106, val acc: 0.9056  (best train acc: 0.7340, best val acc: 0.9076, best train loss: 0.7186)\n",
      "[Epoch: 1740] train loss: 0.7174, train acc: 0.7351, val loss: 0.4062, val acc: 0.9062  (best train acc: 0.7351, best val acc: 0.9079, best train loss: 0.7174)\n",
      "[Epoch: 1760] train loss: 0.7308, train acc: 0.7325, val loss: 0.4042, val acc: 0.9069  (best train acc: 0.7418, best val acc: 0.9083, best train loss: 0.7174)\n",
      "[Epoch: 1780] train loss: 0.7436, train acc: 0.7367, val loss: 0.4182, val acc: 0.9059  (best train acc: 0.7418, best val acc: 0.9083, best train loss: 0.7174)\n",
      "[Epoch: 1800] train loss: 0.7676, train acc: 0.7141, val loss: 0.3897, val acc: 0.9019  (best train acc: 0.7418, best val acc: 0.9083, best train loss: 0.7174)\n",
      "[Epoch: 1820] train loss: 0.7501, train acc: 0.7209, val loss: 0.4101, val acc: 0.9059  (best train acc: 0.7432, best val acc: 0.9083, best train loss: 0.7063)\n",
      "[Epoch: 1840] train loss: 0.7864, train acc: 0.6953, val loss: 0.3853, val acc: 0.9025  (best train acc: 0.7432, best val acc: 0.9096, best train loss: 0.7063)\n",
      "[Epoch: 1860] train loss: 0.7452, train acc: 0.7272, val loss: 0.3949, val acc: 0.8944  (best train acc: 0.7432, best val acc: 0.9096, best train loss: 0.7063)\n",
      "[Epoch: 1880] train loss: 0.7618, train acc: 0.7136, val loss: 0.3836, val acc: 0.9083  (best train acc: 0.7434, best val acc: 0.9096, best train loss: 0.7063)\n",
      "[Epoch: 1900] train loss: 0.7453, train acc: 0.7319, val loss: 0.3858, val acc: 0.9052  (best train acc: 0.7498, best val acc: 0.9103, best train loss: 0.7063)\n",
      "[Epoch: 1920] train loss: 0.7605, train acc: 0.7267, val loss: 0.3937, val acc: 0.9089  (best train acc: 0.7498, best val acc: 0.9110, best train loss: 0.7010)\n",
      "[Epoch: 1940] train loss: 0.7331, train acc: 0.7313, val loss: 0.4128, val acc: 0.9069  (best train acc: 0.7498, best val acc: 0.9110, best train loss: 0.7010)\n",
      "[Epoch: 1960] train loss: 0.7573, train acc: 0.7102, val loss: 0.3984, val acc: 0.9052  (best train acc: 0.7498, best val acc: 0.9110, best train loss: 0.7010)\n",
      "[Epoch: 1980] train loss: 0.7310, train acc: 0.7357, val loss: 0.3949, val acc: 0.9029  (best train acc: 0.7500, best val acc: 0.9110, best train loss: 0.6910)\n",
      "[Epoch: 2000] train loss: 0.7691, train acc: 0.7063, val loss: 0.3922, val acc: 0.9056  (best train acc: 0.7500, best val acc: 0.9110, best train loss: 0.6910)\n",
      "[Epoch: 2020] train loss: 0.7524, train acc: 0.7238, val loss: 0.4047, val acc: 0.8911  (best train acc: 0.7512, best val acc: 0.9110, best train loss: 0.6910)\n",
      "[Epoch: 2040] train loss: 0.7508, train acc: 0.7198, val loss: 0.3899, val acc: 0.8897  (best train acc: 0.7512, best val acc: 0.9110, best train loss: 0.6910)\n",
      "[Epoch: 2060] train loss: 0.7382, train acc: 0.7217, val loss: 0.3945, val acc: 0.8998  (best train acc: 0.7512, best val acc: 0.9110, best train loss: 0.6910)\n",
      "[Epoch: 2080] train loss: 0.7325, train acc: 0.7309, val loss: 0.3976, val acc: 0.9083  (best train acc: 0.7512, best val acc: 0.9110, best train loss: 0.6905)\n",
      "[Epoch: 2100] train loss: 0.7402, train acc: 0.7331, val loss: 0.4136, val acc: 0.8948  (best train acc: 0.7512, best val acc: 0.9110, best train loss: 0.6905)\n",
      "[Epoch: 2120] train loss: 0.7005, train acc: 0.7520, val loss: 0.4154, val acc: 0.9012  (best train acc: 0.7520, best val acc: 0.9113, best train loss: 0.6905)\n",
      "[Epoch: 2140] train loss: 0.7419, train acc: 0.7335, val loss: 0.3962, val acc: 0.9073  (best train acc: 0.7520, best val acc: 0.9113, best train loss: 0.6905)\n",
      "[Epoch: 2160] train loss: 0.6936, train acc: 0.7510, val loss: 0.4152, val acc: 0.9035  (best train acc: 0.7575, best val acc: 0.9113, best train loss: 0.6840)\n",
      "[Epoch: 2180] train loss: 0.7979, train acc: 0.6880, val loss: 0.3883, val acc: 0.9042  (best train acc: 0.7575, best val acc: 0.9113, best train loss: 0.6840)\n",
      "[Epoch: 2200] train loss: 0.7654, train acc: 0.7049, val loss: 0.3853, val acc: 0.9056  (best train acc: 0.7581, best val acc: 0.9113, best train loss: 0.6840)\n",
      "[Epoch: 2220] train loss: 0.7211, train acc: 0.7394, val loss: 0.4028, val acc: 0.9083  (best train acc: 0.7581, best val acc: 0.9113, best train loss: 0.6840)\n",
      "[Epoch: 2240] train loss: 0.7066, train acc: 0.7496, val loss: 0.4086, val acc: 0.9103  (best train acc: 0.7581, best val acc: 0.9113, best train loss: 0.6812)\n",
      "[Epoch: 2260] train loss: 0.7020, train acc: 0.7527, val loss: 0.4014, val acc: 0.9106  (best train acc: 0.7581, best val acc: 0.9116, best train loss: 0.6812)\n",
      "[Epoch: 2280] train loss: 0.7271, train acc: 0.7333, val loss: 0.4145, val acc: 0.9008  (best train acc: 0.7581, best val acc: 0.9116, best train loss: 0.6812)\n",
      "[Epoch: 2300] train loss: 0.7276, train acc: 0.7323, val loss: 0.3946, val acc: 0.9042  (best train acc: 0.7581, best val acc: 0.9116, best train loss: 0.6791)\n",
      "[Epoch: 2320] train loss: 0.7283, train acc: 0.7282, val loss: 0.3983, val acc: 0.9029  (best train acc: 0.7581, best val acc: 0.9116, best train loss: 0.6791)\n",
      "[Epoch: 2340] train loss: 0.7237, train acc: 0.7322, val loss: 0.3783, val acc: 0.9052  (best train acc: 0.7581, best val acc: 0.9116, best train loss: 0.6773)\n",
      "[Epoch: 2360] train loss: 0.7093, train acc: 0.7436, val loss: 0.3961, val acc: 0.8975  (best train acc: 0.7581, best val acc: 0.9116, best train loss: 0.6753)\n",
      "[Epoch: 2380] train loss: 0.7072, train acc: 0.7423, val loss: 0.3890, val acc: 0.8978  (best train acc: 0.7581, best val acc: 0.9116, best train loss: 0.6726)\n",
      "[Epoch: 2400] train loss: 0.7753, train acc: 0.7132, val loss: 0.3838, val acc: 0.9069  (best train acc: 0.7581, best val acc: 0.9116, best train loss: 0.6726)\n",
      "[Epoch: 2420] train loss: 0.7196, train acc: 0.7347, val loss: 0.3948, val acc: 0.9046  (best train acc: 0.7650, best val acc: 0.9116, best train loss: 0.6602)\n",
      "[Epoch: 2440] train loss: 0.6988, train acc: 0.7509, val loss: 0.3954, val acc: 0.9079  (best train acc: 0.7650, best val acc: 0.9116, best train loss: 0.6602)\n",
      "[Epoch: 2460] train loss: 0.7544, train acc: 0.7141, val loss: 0.3879, val acc: 0.8965  (best train acc: 0.7650, best val acc: 0.9116, best train loss: 0.6602)\n",
      "[Epoch: 2480] train loss: 0.7368, train acc: 0.7247, val loss: 0.3884, val acc: 0.9042  (best train acc: 0.7650, best val acc: 0.9116, best train loss: 0.6602)\n",
      "[Epoch: 2500] train loss: 0.7037, train acc: 0.7441, val loss: 0.3960, val acc: 0.9079  (best train acc: 0.7650, best val acc: 0.9116, best train loss: 0.6602)\n",
      "[Epoch: 2520] train loss: 0.6772, train acc: 0.7539, val loss: 0.3832, val acc: 0.9069  (best train acc: 0.7650, best val acc: 0.9116, best train loss: 0.6602)\n",
      "[Epoch: 2540] train loss: 0.7185, train acc: 0.7373, val loss: 0.3788, val acc: 0.9039  (best train acc: 0.7650, best val acc: 0.9116, best train loss: 0.6602)\n",
      "[Epoch: 2560] train loss: 0.7500, train acc: 0.7157, val loss: 0.3866, val acc: 0.8978  (best train acc: 0.7650, best val acc: 0.9116, best train loss: 0.6602)\n",
      "[Epoch: 2580] train loss: 0.7255, train acc: 0.7349, val loss: 0.3862, val acc: 0.9035  (best train acc: 0.7650, best val acc: 0.9116, best train loss: 0.6602)\n",
      "[Epoch: 2600] train loss: 0.6999, train acc: 0.7375, val loss: 0.3872, val acc: 0.9069  (best train acc: 0.7650, best val acc: 0.9116, best train loss: 0.6602)\n",
      "[Epoch: 2620] train loss: 0.7252, train acc: 0.7277, val loss: 0.3923, val acc: 0.9086  (best train acc: 0.7650, best val acc: 0.9116, best train loss: 0.6602)\n",
      "[Epoch: 2640] train loss: 0.7264, train acc: 0.7258, val loss: 0.3830, val acc: 0.9059  (best train acc: 0.7650, best val acc: 0.9116, best train loss: 0.6602)\n",
      "[Epoch: 2660] train loss: 0.6761, train acc: 0.7593, val loss: 0.3739, val acc: 0.9042  (best train acc: 0.7650, best val acc: 0.9116, best train loss: 0.6602)\n",
      "[Epoch: 2680] train loss: 0.6931, train acc: 0.7412, val loss: 0.3932, val acc: 0.9005  (best train acc: 0.7650, best val acc: 0.9116, best train loss: 0.6572)\n",
      "[Epoch: 2700] train loss: 0.6908, train acc: 0.7381, val loss: 0.3729, val acc: 0.9083  (best train acc: 0.7653, best val acc: 0.9116, best train loss: 0.6521)\n",
      "[Epoch: 2720] train loss: 0.6692, train acc: 0.7626, val loss: 0.3981, val acc: 0.9035  (best train acc: 0.7653, best val acc: 0.9116, best train loss: 0.6521)\n",
      "[Epoch: 2740] train loss: 0.7070, train acc: 0.7348, val loss: 0.3769, val acc: 0.9035  (best train acc: 0.7653, best val acc: 0.9116, best train loss: 0.6521)\n",
      "[Epoch: 2760] train loss: 0.7093, train acc: 0.7303, val loss: 0.3892, val acc: 0.9073  (best train acc: 0.7653, best val acc: 0.9116, best train loss: 0.6521)\n",
      "[Epoch: 2780] train loss: 0.7006, train acc: 0.7368, val loss: 0.3818, val acc: 0.9093  (best train acc: 0.7653, best val acc: 0.9116, best train loss: 0.6521)\n",
      "[Epoch: 2800] train loss: 0.7077, train acc: 0.7379, val loss: 0.3871, val acc: 0.9073  (best train acc: 0.7653, best val acc: 0.9116, best train loss: 0.6521)\n",
      "[Epoch: 2820] train loss: 0.7063, train acc: 0.7431, val loss: 0.3651, val acc: 0.9089  (best train acc: 0.7653, best val acc: 0.9116, best train loss: 0.6521)\n",
      "[Epoch: 2840] train loss: 0.7006, train acc: 0.7421, val loss: 0.4177, val acc: 0.9052  (best train acc: 0.7653, best val acc: 0.9116, best train loss: 0.6521)\n",
      "[Epoch: 2860] train loss: 0.6953, train acc: 0.7429, val loss: 0.3754, val acc: 0.9059  (best train acc: 0.7653, best val acc: 0.9116, best train loss: 0.6521)\n",
      "[Epoch: 2880] train loss: 0.6898, train acc: 0.7479, val loss: 0.3757, val acc: 0.9062  (best train acc: 0.7653, best val acc: 0.9116, best train loss: 0.6521)\n",
      "[Epoch: 2900] train loss: 0.6719, train acc: 0.7556, val loss: 0.4016, val acc: 0.9056  (best train acc: 0.7726, best val acc: 0.9116, best train loss: 0.6363)\n",
      "[Epoch: 2920] train loss: 0.7040, train acc: 0.7327, val loss: 0.3719, val acc: 0.9096  (best train acc: 0.7726, best val acc: 0.9116, best train loss: 0.6363)\n",
      "[Epoch: 2940] train loss: 0.6918, train acc: 0.7433, val loss: 0.3758, val acc: 0.9096  (best train acc: 0.7726, best val acc: 0.9116, best train loss: 0.6363)\n",
      "[Epoch: 2960] train loss: 0.7085, train acc: 0.7334, val loss: 0.4062, val acc: 0.8887  (best train acc: 0.7726, best val acc: 0.9116, best train loss: 0.6363)\n",
      "[Epoch: 2980] train loss: 0.6553, train acc: 0.7635, val loss: 0.3833, val acc: 0.9039  (best train acc: 0.7726, best val acc: 0.9116, best train loss: 0.6363)\n",
      "[Epoch: 3000] train loss: 0.6798, train acc: 0.7506, val loss: 0.3658, val acc: 0.9089  (best train acc: 0.7726, best val acc: 0.9116, best train loss: 0.6363)\n",
      "[Epoch: 3020] train loss: 0.7198, train acc: 0.7321, val loss: 0.3918, val acc: 0.9056  (best train acc: 0.7726, best val acc: 0.9116, best train loss: 0.6363)\n",
      "[Epoch: 3040] train loss: 0.6829, train acc: 0.7439, val loss: 0.3804, val acc: 0.9056  (best train acc: 0.7726, best val acc: 0.9133, best train loss: 0.6363)\n",
      "[Epoch: 3060] train loss: 0.7041, train acc: 0.7423, val loss: 0.3710, val acc: 0.9029  (best train acc: 0.7726, best val acc: 0.9133, best train loss: 0.6363)\n",
      "[Epoch: 3080] train loss: 0.6565, train acc: 0.7590, val loss: 0.3741, val acc: 0.9083  (best train acc: 0.7726, best val acc: 0.9133, best train loss: 0.6363)\n",
      "[Epoch: 3100] train loss: 0.6877, train acc: 0.7460, val loss: 0.3654, val acc: 0.9110  (best train acc: 0.7726, best val acc: 0.9133, best train loss: 0.6363)\n",
      "[Epoch: 3120] train loss: 0.7178, train acc: 0.7321, val loss: 0.3808, val acc: 0.9113  (best train acc: 0.7726, best val acc: 0.9133, best train loss: 0.6363)\n",
      "[Epoch: 3140] train loss: 0.7092, train acc: 0.7316, val loss: 0.3652, val acc: 0.9106  (best train acc: 0.7726, best val acc: 0.9133, best train loss: 0.6363)\n",
      "[Epoch: 3160] train loss: 0.6861, train acc: 0.7525, val loss: 0.3748, val acc: 0.9083  (best train acc: 0.7726, best val acc: 0.9133, best train loss: 0.6363)\n",
      "[Epoch: 3180] train loss: 0.6947, train acc: 0.7424, val loss: 0.3691, val acc: 0.9093  (best train acc: 0.7726, best val acc: 0.9133, best train loss: 0.6363)\n",
      "[Epoch: 3200] train loss: 0.7060, train acc: 0.7291, val loss: 0.3601, val acc: 0.9113  (best train acc: 0.7726, best val acc: 0.9133, best train loss: 0.6363)\n",
      "[Epoch: 3220] train loss: 0.6998, train acc: 0.7475, val loss: 0.3658, val acc: 0.9126  (best train acc: 0.7726, best val acc: 0.9133, best train loss: 0.6363)\n",
      "[Epoch: 3240] train loss: 0.7056, train acc: 0.7329, val loss: 0.3575, val acc: 0.9069  (best train acc: 0.7726, best val acc: 0.9137, best train loss: 0.6363)\n",
      "[Epoch: 3260] train loss: 0.6989, train acc: 0.7317, val loss: 0.3612, val acc: 0.9123  (best train acc: 0.7726, best val acc: 0.9137, best train loss: 0.6363)\n",
      "[Epoch: 3280] train loss: 0.6957, train acc: 0.7337, val loss: 0.3786, val acc: 0.9086  (best train acc: 0.7726, best val acc: 0.9137, best train loss: 0.6363)\n",
      "[Epoch: 3300] train loss: 0.7112, train acc: 0.7342, val loss: 0.3653, val acc: 0.9079  (best train acc: 0.7726, best val acc: 0.9137, best train loss: 0.6363)\n",
      "[Epoch: 3320] train loss: 0.6725, train acc: 0.7499, val loss: 0.3620, val acc: 0.9120  (best train acc: 0.7726, best val acc: 0.9137, best train loss: 0.6363)\n",
      "[Epoch: 3340] train loss: 0.7178, train acc: 0.7233, val loss: 0.3640, val acc: 0.9116  (best train acc: 0.7726, best val acc: 0.9137, best train loss: 0.6363)\n",
      "[Epoch: 3360] train loss: 0.6994, train acc: 0.7360, val loss: 0.3595, val acc: 0.9099  (best train acc: 0.7726, best val acc: 0.9137, best train loss: 0.6363)\n",
      "[Epoch: 3380] train loss: 0.6859, train acc: 0.7479, val loss: 0.3958, val acc: 0.9079  (best train acc: 0.7726, best val acc: 0.9137, best train loss: 0.6363)\n",
      "[Epoch: 3400] train loss: 0.6422, train acc: 0.7564, val loss: 0.3750, val acc: 0.9049  (best train acc: 0.7726, best val acc: 0.9137, best train loss: 0.6363)\n",
      "[Epoch: 3420] train loss: 0.7601, train acc: 0.6960, val loss: 0.3731, val acc: 0.9015  (best train acc: 0.7726, best val acc: 0.9137, best train loss: 0.6363)\n",
      "[Epoch: 3440] train loss: 0.6795, train acc: 0.7512, val loss: 0.3615, val acc: 0.9130  (best train acc: 0.7726, best val acc: 0.9140, best train loss: 0.6341)\n",
      "[Epoch: 3460] train loss: 0.7024, train acc: 0.7272, val loss: 0.3544, val acc: 0.9133  (best train acc: 0.7726, best val acc: 0.9140, best train loss: 0.6341)\n",
      "[Epoch: 3480] train loss: 0.6680, train acc: 0.7532, val loss: 0.3745, val acc: 0.9096  (best train acc: 0.7726, best val acc: 0.9140, best train loss: 0.6307)\n",
      "[Epoch: 3500] train loss: 0.6657, train acc: 0.7575, val loss: 0.3865, val acc: 0.9106  (best train acc: 0.7726, best val acc: 0.9140, best train loss: 0.6307)\n",
      "[Epoch: 3520] train loss: 0.7375, train acc: 0.7170, val loss: 0.3687, val acc: 0.9103  (best train acc: 0.7726, best val acc: 0.9140, best train loss: 0.6307)\n",
      "[Epoch: 3540] train loss: 0.6902, train acc: 0.7422, val loss: 0.3415, val acc: 0.9093  (best train acc: 0.7914, best val acc: 0.9147, best train loss: 0.6079)\n",
      "[Epoch: 3560] train loss: 0.6436, train acc: 0.7723, val loss: 0.3445, val acc: 0.9103  (best train acc: 0.7914, best val acc: 0.9147, best train loss: 0.6042)\n",
      "[Epoch: 3580] train loss: 0.6186, train acc: 0.7801, val loss: 0.3437, val acc: 0.9093  (best train acc: 0.7914, best val acc: 0.9147, best train loss: 0.6042)\n",
      "[Epoch: 3600] train loss: 0.6092, train acc: 0.7847, val loss: 0.3445, val acc: 0.9120  (best train acc: 0.7914, best val acc: 0.9147, best train loss: 0.6042)\n",
      "[Epoch: 3620] train loss: 0.6411, train acc: 0.7718, val loss: 0.3496, val acc: 0.9113  (best train acc: 0.7914, best val acc: 0.9147, best train loss: 0.6012)\n",
      "[Epoch: 3640] train loss: 0.6315, train acc: 0.7663, val loss: 0.3456, val acc: 0.9106  (best train acc: 0.7914, best val acc: 0.9147, best train loss: 0.6012)\n",
      "[Epoch: 3660] train loss: 0.6232, train acc: 0.7790, val loss: 0.3550, val acc: 0.9056  (best train acc: 0.7914, best val acc: 0.9147, best train loss: 0.6012)\n",
      "[Epoch: 3680] train loss: 0.6453, train acc: 0.7622, val loss: 0.3377, val acc: 0.9126  (best train acc: 0.7940, best val acc: 0.9147, best train loss: 0.5997)\n",
      "[Epoch: 3700] train loss: 0.6785, train acc: 0.7479, val loss: 0.3358, val acc: 0.9099  (best train acc: 0.7940, best val acc: 0.9147, best train loss: 0.5997)\n",
      "[Epoch: 3720] train loss: 0.6777, train acc: 0.7482, val loss: 0.3399, val acc: 0.9120  (best train acc: 0.7992, best val acc: 0.9147, best train loss: 0.5886)\n",
      "[Epoch: 3740] train loss: 0.6389, train acc: 0.7629, val loss: 0.3351, val acc: 0.9096  (best train acc: 0.7992, best val acc: 0.9147, best train loss: 0.5886)\n",
      "[Epoch: 3760] train loss: 0.6139, train acc: 0.7848, val loss: 0.3458, val acc: 0.9052  (best train acc: 0.7992, best val acc: 0.9147, best train loss: 0.5886)\n",
      "[Epoch: 3780] train loss: 0.6044, train acc: 0.7898, val loss: 0.3439, val acc: 0.9093  (best train acc: 0.7992, best val acc: 0.9147, best train loss: 0.5886)\n",
      "[Epoch: 3800] train loss: 0.6297, train acc: 0.7747, val loss: 0.3367, val acc: 0.9140  (best train acc: 0.7992, best val acc: 0.9147, best train loss: 0.5778)\n",
      "[Epoch: 3820] train loss: 0.6345, train acc: 0.7728, val loss: 0.3424, val acc: 0.9126  (best train acc: 0.7992, best val acc: 0.9147, best train loss: 0.5778)\n",
      "[Epoch: 3840] train loss: 0.7027, train acc: 0.7302, val loss: 0.3376, val acc: 0.9083  (best train acc: 0.7992, best val acc: 0.9147, best train loss: 0.5778)\n",
      "[Epoch: 3860] train loss: 0.6621, train acc: 0.7555, val loss: 0.3339, val acc: 0.9089  (best train acc: 0.7992, best val acc: 0.9147, best train loss: 0.5778)\n",
      "[Epoch: 3880] train loss: 0.6809, train acc: 0.7444, val loss: 0.3434, val acc: 0.9052  (best train acc: 0.7992, best val acc: 0.9147, best train loss: 0.5778)\n",
      "[Epoch: 3900] train loss: 0.6797, train acc: 0.7470, val loss: 0.3319, val acc: 0.9110  (best train acc: 0.7992, best val acc: 0.9147, best train loss: 0.5778)\n",
      "[Epoch: 3920] train loss: 0.6199, train acc: 0.7820, val loss: 0.3476, val acc: 0.9089  (best train acc: 0.7992, best val acc: 0.9147, best train loss: 0.5778)\n",
      "[Epoch: 3940] train loss: 0.6268, train acc: 0.7795, val loss: 0.3543, val acc: 0.9123  (best train acc: 0.7992, best val acc: 0.9147, best train loss: 0.5778)\n",
      "[Epoch: 3960] train loss: 0.6201, train acc: 0.7750, val loss: 0.3370, val acc: 0.9116  (best train acc: 0.7992, best val acc: 0.9147, best train loss: 0.5778)\n",
      "[Epoch: 3980] train loss: 0.6185, train acc: 0.7708, val loss: 0.3392, val acc: 0.9147  (best train acc: 0.7992, best val acc: 0.9147, best train loss: 0.5778)\n",
      "[Epoch: 4000] train loss: 0.6367, train acc: 0.7658, val loss: 0.3358, val acc: 0.9126  (best train acc: 0.8031, best val acc: 0.9147, best train loss: 0.5625)\n",
      "[Epoch: 4020] train loss: 0.6258, train acc: 0.7788, val loss: 0.3369, val acc: 0.9130  (best train acc: 0.8031, best val acc: 0.9147, best train loss: 0.5625)\n",
      "[Epoch: 4040] train loss: 0.6388, train acc: 0.7684, val loss: 0.3366, val acc: 0.9106  (best train acc: 0.8031, best val acc: 0.9147, best train loss: 0.5625)\n",
      "[Epoch: 4060] train loss: 0.5836, train acc: 0.7921, val loss: 0.3413, val acc: 0.9086  (best train acc: 0.8031, best val acc: 0.9147, best train loss: 0.5625)\n",
      "[Epoch: 4080] train loss: 0.6201, train acc: 0.7796, val loss: 0.3403, val acc: 0.9103  (best train acc: 0.8031, best val acc: 0.9147, best train loss: 0.5625)\n",
      "[Epoch: 4100] train loss: 0.5975, train acc: 0.7936, val loss: 0.3634, val acc: 0.8897  (best train acc: 0.8031, best val acc: 0.9147, best train loss: 0.5625)\n",
      "[Epoch: 4120] train loss: 0.6441, train acc: 0.7639, val loss: 0.3332, val acc: 0.9086  (best train acc: 0.8031, best val acc: 0.9147, best train loss: 0.5625)\n",
      "[Epoch: 4140] train loss: 0.6289, train acc: 0.7734, val loss: 0.3346, val acc: 0.9079  (best train acc: 0.8031, best val acc: 0.9147, best train loss: 0.5625)\n",
      "[Epoch: 4160] train loss: 0.6219, train acc: 0.7750, val loss: 0.3419, val acc: 0.9032  (best train acc: 0.8031, best val acc: 0.9147, best train loss: 0.5625)\n",
      "[Epoch: 4180] train loss: 0.6215, train acc: 0.7763, val loss: 0.3443, val acc: 0.9089  (best train acc: 0.8031, best val acc: 0.9147, best train loss: 0.5625)\n",
      "[Epoch: 4200] train loss: 0.6504, train acc: 0.7624, val loss: 0.3285, val acc: 0.9099  (best train acc: 0.8031, best val acc: 0.9147, best train loss: 0.5625)\n",
      "[Epoch: 4220] train loss: 0.6444, train acc: 0.7614, val loss: 0.3317, val acc: 0.9069  (best train acc: 0.8031, best val acc: 0.9147, best train loss: 0.5625)\n",
      "[Epoch: 4240] train loss: 0.6386, train acc: 0.7753, val loss: 0.3557, val acc: 0.9035  (best train acc: 0.8031, best val acc: 0.9147, best train loss: 0.5625)\n",
      "[Epoch: 4260] train loss: 0.6616, train acc: 0.7515, val loss: 0.3344, val acc: 0.9106  (best train acc: 0.8031, best val acc: 0.9147, best train loss: 0.5625)\n",
      "[Epoch: 4280] train loss: 0.5975, train acc: 0.7879, val loss: 0.3589, val acc: 0.9002  (best train acc: 0.8031, best val acc: 0.9147, best train loss: 0.5625)\n",
      "[Epoch: 4300] train loss: 0.6644, train acc: 0.7511, val loss: 0.3299, val acc: 0.9093  (best train acc: 0.8031, best val acc: 0.9147, best train loss: 0.5625)\n",
      "[Epoch: 4320] train loss: 0.6039, train acc: 0.7818, val loss: 0.3594, val acc: 0.8998  (best train acc: 0.8031, best val acc: 0.9147, best train loss: 0.5625)\n",
      "[Epoch: 4340] train loss: 0.6557, train acc: 0.7575, val loss: 0.3359, val acc: 0.9083  (best train acc: 0.8031, best val acc: 0.9147, best train loss: 0.5625)\n",
      "[Epoch: 4360] train loss: 0.6085, train acc: 0.7835, val loss: 0.3799, val acc: 0.8907  (best train acc: 0.8031, best val acc: 0.9147, best train loss: 0.5625)\n",
      "[Epoch: 4380] train loss: 0.6444, train acc: 0.7615, val loss: 0.3425, val acc: 0.9069  (best train acc: 0.8031, best val acc: 0.9147, best train loss: 0.5625)\n",
      "[Epoch: 4400] train loss: 0.6333, train acc: 0.7656, val loss: 0.3397, val acc: 0.9073  (best train acc: 0.8031, best val acc: 0.9147, best train loss: 0.5625)\n",
      "[Epoch: 4420] train loss: 0.6356, train acc: 0.7646, val loss: 0.3630, val acc: 0.8927  (best train acc: 0.8031, best val acc: 0.9147, best train loss: 0.5625)\n",
      "[Epoch: 4440] train loss: 0.5893, train acc: 0.7960, val loss: 0.3310, val acc: 0.9133  (best train acc: 0.8031, best val acc: 0.9147, best train loss: 0.5625)\n",
      "[Epoch: 4460] train loss: 0.6798, train acc: 0.7345, val loss: 0.3395, val acc: 0.9025  (best train acc: 0.8031, best val acc: 0.9147, best train loss: 0.5625)\n",
      "[Epoch: 4480] train loss: 0.6263, train acc: 0.7791, val loss: 0.3595, val acc: 0.9046  (best train acc: 0.8031, best val acc: 0.9147, best train loss: 0.5625)\n",
      "[Epoch: 4500] train loss: 0.6740, train acc: 0.7476, val loss: 0.3465, val acc: 0.9076  (best train acc: 0.8031, best val acc: 0.9147, best train loss: 0.5625)\n",
      "[Epoch: 4520] train loss: 0.6160, train acc: 0.7713, val loss: 0.3814, val acc: 0.8843  (best train acc: 0.8031, best val acc: 0.9147, best train loss: 0.5625)\n",
      "[Epoch: 4540] train loss: 0.5887, train acc: 0.7901, val loss: 0.3485, val acc: 0.9032  (best train acc: 0.8031, best val acc: 0.9147, best train loss: 0.5625)\n",
      "[Epoch: 4560] train loss: 0.6577, train acc: 0.7471, val loss: 0.3423, val acc: 0.9106  (best train acc: 0.8031, best val acc: 0.9147, best train loss: 0.5625)\n",
      "[Epoch: 4580] train loss: 0.6609, train acc: 0.7504, val loss: 0.3396, val acc: 0.9059  (best train acc: 0.8031, best val acc: 0.9147, best train loss: 0.5625)\n",
      "[Epoch: 4600] train loss: 0.6079, train acc: 0.7822, val loss: 0.3663, val acc: 0.9029  (best train acc: 0.8031, best val acc: 0.9147, best train loss: 0.5625)\n",
      "[Epoch: 4620] train loss: 0.6241, train acc: 0.7729, val loss: 0.3511, val acc: 0.9103  (best train acc: 0.8031, best val acc: 0.9147, best train loss: 0.5625)\n",
      "[Epoch: 4640] train loss: 0.5791, train acc: 0.7934, val loss: 0.3737, val acc: 0.8965  (best train acc: 0.8031, best val acc: 0.9147, best train loss: 0.5625)\n",
      "[Epoch: 4660] train loss: 0.6413, train acc: 0.7623, val loss: 0.3548, val acc: 0.9052  (best train acc: 0.8031, best val acc: 0.9147, best train loss: 0.5625)\n",
      "[Epoch: 4680] train loss: 0.5778, train acc: 0.7984, val loss: 0.3619, val acc: 0.9056  (best train acc: 0.8031, best val acc: 0.9147, best train loss: 0.5625)\n",
      "[Epoch: 4700] train loss: 0.6393, train acc: 0.7611, val loss: 0.3538, val acc: 0.9066  (best train acc: 0.8031, best val acc: 0.9147, best train loss: 0.5625)\n",
      "[Epoch: 4720] train loss: 0.6420, train acc: 0.7629, val loss: 0.3485, val acc: 0.9096  (best train acc: 0.8031, best val acc: 0.9147, best train loss: 0.5625)\n",
      "[Epoch: 4740] train loss: 0.5786, train acc: 0.7931, val loss: 0.3556, val acc: 0.9069  (best train acc: 0.8031, best val acc: 0.9147, best train loss: 0.5625)\n",
      "[Epoch: 4760] train loss: 0.6160, train acc: 0.7778, val loss: 0.3478, val acc: 0.9103  (best train acc: 0.8031, best val acc: 0.9147, best train loss: 0.5625)\n",
      "[Epoch: 4780] train loss: 0.5977, train acc: 0.7862, val loss: 0.3572, val acc: 0.9116  (best train acc: 0.8031, best val acc: 0.9147, best train loss: 0.5625)\n",
      "[Epoch: 4800] train loss: 0.6578, train acc: 0.7513, val loss: 0.3528, val acc: 0.9086  (best train acc: 0.8031, best val acc: 0.9147, best train loss: 0.5625)\n",
      "[Epoch: 4820] train loss: 0.6091, train acc: 0.7807, val loss: 0.3731, val acc: 0.9008  (best train acc: 0.8031, best val acc: 0.9147, best train loss: 0.5625)\n",
      "[Epoch: 4840] train loss: 0.6015, train acc: 0.7798, val loss: 0.3726, val acc: 0.9049  (best train acc: 0.8031, best val acc: 0.9147, best train loss: 0.5625)\n",
      "[Epoch: 4860] train loss: 0.6401, train acc: 0.7556, val loss: 0.3580, val acc: 0.9069  (best train acc: 0.8031, best val acc: 0.9147, best train loss: 0.5625)\n",
      "[Epoch: 4880] train loss: 0.6661, train acc: 0.7454, val loss: 0.3563, val acc: 0.9059  (best train acc: 0.8031, best val acc: 0.9147, best train loss: 0.5625)\n",
      "[Epoch: 4900] train loss: 0.6523, train acc: 0.7557, val loss: 0.3755, val acc: 0.9015  (best train acc: 0.8031, best val acc: 0.9147, best train loss: 0.5625)\n",
      "[Epoch: 4920] train loss: 0.6220, train acc: 0.7657, val loss: 0.3462, val acc: 0.9113  (best train acc: 0.8031, best val acc: 0.9147, best train loss: 0.5625)\n",
      "[Epoch: 4940] train loss: 0.6147, train acc: 0.7679, val loss: 0.3622, val acc: 0.9046  (best train acc: 0.8031, best val acc: 0.9147, best train loss: 0.5625)\n",
      "[Epoch: 4960] train loss: 0.6104, train acc: 0.7771, val loss: 0.3529, val acc: 0.9099  (best train acc: 0.8031, best val acc: 0.9147, best train loss: 0.5625)\n",
      "[Epoch: 4980] train loss: 0.6964, train acc: 0.7238, val loss: 0.3578, val acc: 0.9079  (best train acc: 0.8031, best val acc: 0.9147, best train loss: 0.5625)\n",
      "[Epoch: 5000] train loss: 0.6391, train acc: 0.7584, val loss: 0.3578, val acc: 0.9113  (best train acc: 0.8031, best val acc: 0.9147, best train loss: 0.5625)\n",
      "[Epoch: 5020] train loss: 0.6211, train acc: 0.7705, val loss: 0.3637, val acc: 0.9113  (best train acc: 0.8031, best val acc: 0.9147, best train loss: 0.5625)\n",
      "[Epoch: 5040] train loss: 0.5967, train acc: 0.7842, val loss: 0.3487, val acc: 0.9126  (best train acc: 0.8041, best val acc: 0.9147, best train loss: 0.5489)\n",
      "[Epoch: 5060] train loss: 0.6553, train acc: 0.7456, val loss: 0.3527, val acc: 0.9093  (best train acc: 0.8041, best val acc: 0.9147, best train loss: 0.5489)\n",
      "[Epoch: 5080] train loss: 0.6539, train acc: 0.7442, val loss: 0.3550, val acc: 0.9130  (best train acc: 0.8041, best val acc: 0.9147, best train loss: 0.5489)\n",
      "[Epoch: 5100] train loss: 0.5734, train acc: 0.7914, val loss: 0.3584, val acc: 0.9025  (best train acc: 0.8041, best val acc: 0.9147, best train loss: 0.5489)\n",
      "[Epoch: 5120] train loss: 0.6238, train acc: 0.7675, val loss: 0.3516, val acc: 0.9113  (best train acc: 0.8041, best val acc: 0.9147, best train loss: 0.5489)\n",
      "[Epoch: 5140] train loss: 0.6231, train acc: 0.7635, val loss: 0.3735, val acc: 0.9005  (best train acc: 0.8041, best val acc: 0.9147, best train loss: 0.5489)\n",
      "[Epoch: 5160] train loss: 0.6408, train acc: 0.7590, val loss: 0.3516, val acc: 0.9110  (best train acc: 0.8041, best val acc: 0.9147, best train loss: 0.5489)\n",
      "[Epoch: 5180] train loss: 0.6277, train acc: 0.7568, val loss: 0.3566, val acc: 0.9110  (best train acc: 0.8041, best val acc: 0.9147, best train loss: 0.5489)\n",
      "[Epoch: 5200] train loss: 0.6363, train acc: 0.7596, val loss: 0.3578, val acc: 0.9116  (best train acc: 0.8041, best val acc: 0.9147, best train loss: 0.5489)\n",
      "[Epoch: 5220] train loss: 0.6274, train acc: 0.7646, val loss: 0.3648, val acc: 0.9052  (best train acc: 0.8041, best val acc: 0.9147, best train loss: 0.5489)\n",
      "[Epoch: 5240] train loss: 0.6104, train acc: 0.7715, val loss: 0.3505, val acc: 0.9079  (best train acc: 0.8041, best val acc: 0.9147, best train loss: 0.5489)\n",
      "[Epoch: 5260] train loss: 0.5963, train acc: 0.7813, val loss: 0.3649, val acc: 0.9035  (best train acc: 0.8041, best val acc: 0.9147, best train loss: 0.5489)\n",
      "[Epoch: 5280] train loss: 0.6181, train acc: 0.7671, val loss: 0.3789, val acc: 0.9005  (best train acc: 0.8041, best val acc: 0.9147, best train loss: 0.5489)\n",
      "[Epoch: 5300] train loss: 0.5860, train acc: 0.7874, val loss: 0.3601, val acc: 0.9066  (best train acc: 0.8041, best val acc: 0.9147, best train loss: 0.5489)\n",
      "[Epoch: 5320] train loss: 0.6166, train acc: 0.7679, val loss: 0.3518, val acc: 0.9103  (best train acc: 0.8041, best val acc: 0.9147, best train loss: 0.5489)\n",
      "[Epoch: 5340] train loss: 0.5744, train acc: 0.7925, val loss: 0.3628, val acc: 0.9012  (best train acc: 0.8041, best val acc: 0.9147, best train loss: 0.5489)\n",
      "[Epoch: 5360] train loss: 0.6446, train acc: 0.7527, val loss: 0.3826, val acc: 0.8924  (best train acc: 0.8041, best val acc: 0.9147, best train loss: 0.5489)\n",
      "[Epoch: 5380] train loss: 0.6511, train acc: 0.7540, val loss: 0.3595, val acc: 0.8921  (best train acc: 0.8041, best val acc: 0.9147, best train loss: 0.5489)\n",
      "[Epoch: 5400] train loss: 0.6358, train acc: 0.7556, val loss: 0.3633, val acc: 0.9046  (best train acc: 0.8041, best val acc: 0.9147, best train loss: 0.5489)\n",
      "[Epoch: 5420] train loss: 0.6242, train acc: 0.7659, val loss: 0.3577, val acc: 0.9029  (best train acc: 0.8041, best val acc: 0.9147, best train loss: 0.5489)\n",
      "[Epoch: 5440] train loss: 0.6504, train acc: 0.7439, val loss: 0.3550, val acc: 0.9093  (best train acc: 0.8041, best val acc: 0.9147, best train loss: 0.5489)\n",
      "[Epoch: 5460] train loss: 0.5809, train acc: 0.7835, val loss: 0.3555, val acc: 0.9032  (best train acc: 0.8041, best val acc: 0.9147, best train loss: 0.5489)\n",
      "[Epoch: 5480] train loss: 0.5712, train acc: 0.7861, val loss: 0.3387, val acc: 0.9133  (best train acc: 0.8041, best val acc: 0.9147, best train loss: 0.5489)\n",
      "[Epoch: 5500] train loss: 0.5516, train acc: 0.7956, val loss: 0.3351, val acc: 0.9113  (best train acc: 0.8095, best val acc: 0.9147, best train loss: 0.5319)\n",
      "[Epoch: 5520] train loss: 0.5663, train acc: 0.7876, val loss: 0.3450, val acc: 0.9099  (best train acc: 0.8095, best val acc: 0.9147, best train loss: 0.5285)\n",
      "[Epoch: 5540] train loss: 0.5493, train acc: 0.7970, val loss: 0.3409, val acc: 0.9099  (best train acc: 0.8095, best val acc: 0.9147, best train loss: 0.5285)\n",
      "[Epoch: 5560] train loss: 0.5394, train acc: 0.7964, val loss: 0.3482, val acc: 0.9089  (best train acc: 0.8132, best val acc: 0.9147, best train loss: 0.5208)\n",
      "[Epoch: 5580] train loss: 0.5551, train acc: 0.7899, val loss: 0.3454, val acc: 0.9116  (best train acc: 0.8132, best val acc: 0.9147, best train loss: 0.5208)\n",
      "[Epoch: 5600] train loss: 0.5559, train acc: 0.7904, val loss: 0.3395, val acc: 0.9126  (best train acc: 0.8132, best val acc: 0.9147, best train loss: 0.5208)\n",
      "[Epoch: 5620] train loss: 0.5579, train acc: 0.7787, val loss: 0.3441, val acc: 0.9066  (best train acc: 0.8132, best val acc: 0.9160, best train loss: 0.5121)\n",
      "[Epoch: 5640] train loss: 0.5737, train acc: 0.7777, val loss: 0.3410, val acc: 0.9113  (best train acc: 0.8132, best val acc: 0.9160, best train loss: 0.5121)\n",
      "[Epoch: 5660] train loss: 0.5724, train acc: 0.7760, val loss: 0.3907, val acc: 0.8847  (best train acc: 0.8132, best val acc: 0.9160, best train loss: 0.5121)\n",
      "[Epoch: 5680] train loss: 0.5776, train acc: 0.7862, val loss: 0.3810, val acc: 0.8793  (best train acc: 0.8132, best val acc: 0.9160, best train loss: 0.5121)\n",
      "[Epoch: 5700] train loss: 0.6109, train acc: 0.7517, val loss: 0.3494, val acc: 0.8958  (best train acc: 0.8132, best val acc: 0.9160, best train loss: 0.5121)\n",
      "[Epoch: 5720] train loss: 0.6052, train acc: 0.7533, val loss: 0.3419, val acc: 0.9086  (best train acc: 0.8132, best val acc: 0.9160, best train loss: 0.5121)\n",
      "[Epoch: 5740] train loss: 0.5404, train acc: 0.7967, val loss: 0.3410, val acc: 0.9096  (best train acc: 0.8132, best val acc: 0.9160, best train loss: 0.5121)\n",
      "[Epoch: 5760] train loss: 0.5517, train acc: 0.7819, val loss: 0.3510, val acc: 0.9012  (best train acc: 0.8132, best val acc: 0.9160, best train loss: 0.5121)\n",
      "[Epoch: 5780] train loss: 0.5753, train acc: 0.7782, val loss: 0.3419, val acc: 0.9120  (best train acc: 0.8132, best val acc: 0.9160, best train loss: 0.5121)\n",
      "[Epoch: 5800] train loss: 0.5628, train acc: 0.7765, val loss: 0.3493, val acc: 0.9099  (best train acc: 0.8132, best val acc: 0.9160, best train loss: 0.5121)\n",
      "[Epoch: 5820] train loss: 0.5490, train acc: 0.7858, val loss: 0.3462, val acc: 0.9049  (best train acc: 0.8132, best val acc: 0.9160, best train loss: 0.5121)\n",
      "[Epoch: 5840] train loss: 0.5324, train acc: 0.8053, val loss: 0.3459, val acc: 0.9147  (best train acc: 0.8132, best val acc: 0.9160, best train loss: 0.5113)\n",
      "[Epoch: 5860] train loss: 0.5540, train acc: 0.7821, val loss: 0.3464, val acc: 0.9103  (best train acc: 0.8132, best val acc: 0.9160, best train loss: 0.5113)\n",
      "[Epoch: 5880] train loss: 0.5529, train acc: 0.7919, val loss: 0.3367, val acc: 0.9137  (best train acc: 0.8132, best val acc: 0.9160, best train loss: 0.5113)\n",
      "[Epoch: 5900] train loss: 0.5723, train acc: 0.7745, val loss: 0.3516, val acc: 0.9086  (best train acc: 0.8132, best val acc: 0.9160, best train loss: 0.5113)\n",
      "[Epoch: 5920] train loss: 0.5466, train acc: 0.7901, val loss: 0.3473, val acc: 0.9059  (best train acc: 0.8132, best val acc: 0.9160, best train loss: 0.5113)\n",
      "[Epoch: 5940] train loss: 0.5527, train acc: 0.7872, val loss: 0.3461, val acc: 0.9096  (best train acc: 0.8132, best val acc: 0.9160, best train loss: 0.5113)\n",
      "[Epoch: 5960] train loss: 0.5384, train acc: 0.7981, val loss: 0.3589, val acc: 0.9059  (best train acc: 0.8132, best val acc: 0.9160, best train loss: 0.5113)\n",
      "[Epoch: 5980] train loss: 0.5640, train acc: 0.7858, val loss: 0.3561, val acc: 0.9073  (best train acc: 0.8132, best val acc: 0.9160, best train loss: 0.5113)\n",
      "[Epoch: 6000] train loss: 0.5928, train acc: 0.7563, val loss: 0.3483, val acc: 0.9093  (best train acc: 0.8132, best val acc: 0.9160, best train loss: 0.5113)\n",
      "[Epoch: 6020] train loss: 0.5685, train acc: 0.7721, val loss: 0.3567, val acc: 0.9096  (best train acc: 0.8132, best val acc: 0.9160, best train loss: 0.5113)\n",
      "[Epoch: 6040] train loss: 0.5175, train acc: 0.8064, val loss: 0.3483, val acc: 0.9133  (best train acc: 0.8132, best val acc: 0.9160, best train loss: 0.5113)\n",
      "[Epoch: 6060] train loss: 0.5207, train acc: 0.8086, val loss: 0.3438, val acc: 0.9116  (best train acc: 0.8132, best val acc: 0.9160, best train loss: 0.5113)\n",
      "[Epoch: 6080] train loss: 0.5342, train acc: 0.7962, val loss: 0.3763, val acc: 0.8985  (best train acc: 0.8132, best val acc: 0.9160, best train loss: 0.5113)\n",
      "[Epoch: 6100] train loss: 0.5486, train acc: 0.7892, val loss: 0.3562, val acc: 0.9110  (best train acc: 0.8132, best val acc: 0.9160, best train loss: 0.5113)\n",
      "[Epoch: 6120] train loss: 0.6127, train acc: 0.7627, val loss: 0.3540, val acc: 0.9089  (best train acc: 0.8132, best val acc: 0.9160, best train loss: 0.5113)\n",
      "[Epoch: 6140] train loss: 0.5408, train acc: 0.7870, val loss: 0.3578, val acc: 0.9035  (best train acc: 0.8190, best val acc: 0.9160, best train loss: 0.5113)\n",
      "[Epoch: 6160] train loss: 0.5936, train acc: 0.7609, val loss: 0.3567, val acc: 0.9110  (best train acc: 0.8190, best val acc: 0.9160, best train loss: 0.5113)\n",
      "[Epoch: 6180] train loss: 0.5712, train acc: 0.7791, val loss: 0.3685, val acc: 0.9022  (best train acc: 0.8190, best val acc: 0.9160, best train loss: 0.5113)\n",
      "[Epoch: 6200] train loss: 0.5703, train acc: 0.7754, val loss: 0.3700, val acc: 0.9032  (best train acc: 0.8190, best val acc: 0.9160, best train loss: 0.5113)\n",
      "[Epoch: 6220] train loss: 0.5524, train acc: 0.7794, val loss: 0.3530, val acc: 0.9099  (best train acc: 0.8190, best val acc: 0.9160, best train loss: 0.5113)\n",
      "[Epoch: 6240] train loss: 0.5390, train acc: 0.7889, val loss: 0.3650, val acc: 0.9089  (best train acc: 0.8190, best val acc: 0.9160, best train loss: 0.5110)\n",
      "[Epoch: 6260] train loss: 0.5497, train acc: 0.7849, val loss: 0.3555, val acc: 0.9106  (best train acc: 0.8190, best val acc: 0.9160, best train loss: 0.5104)\n",
      "[Epoch: 6280] train loss: 0.5596, train acc: 0.7802, val loss: 0.3635, val acc: 0.9066  (best train acc: 0.8190, best val acc: 0.9160, best train loss: 0.5104)\n",
      "[Epoch: 6300] train loss: 0.5616, train acc: 0.7770, val loss: 0.3491, val acc: 0.9103  (best train acc: 0.8191, best val acc: 0.9160, best train loss: 0.5066)\n",
      "[Epoch: 6320] train loss: 0.5714, train acc: 0.7710, val loss: 0.3600, val acc: 0.9042  (best train acc: 0.8191, best val acc: 0.9160, best train loss: 0.5066)\n",
      "[Epoch: 6340] train loss: 0.5765, train acc: 0.7694, val loss: 0.3967, val acc: 0.8927  (best train acc: 0.8191, best val acc: 0.9160, best train loss: 0.5066)\n",
      "[Epoch: 6360] train loss: 0.6010, train acc: 0.7520, val loss: 0.3543, val acc: 0.9079  (best train acc: 0.8191, best val acc: 0.9160, best train loss: 0.5066)\n",
      "[Epoch: 6380] train loss: 0.5631, train acc: 0.7709, val loss: 0.3610, val acc: 0.9052  (best train acc: 0.8191, best val acc: 0.9160, best train loss: 0.5066)\n",
      "[Epoch: 6400] train loss: 0.5307, train acc: 0.7898, val loss: 0.3530, val acc: 0.9106  (best train acc: 0.8191, best val acc: 0.9160, best train loss: 0.5066)\n",
      "[Epoch: 6420] train loss: 0.5427, train acc: 0.7924, val loss: 0.3558, val acc: 0.9089  (best train acc: 0.8191, best val acc: 0.9160, best train loss: 0.5066)\n",
      "[Epoch: 6440] train loss: 0.5343, train acc: 0.7938, val loss: 0.3614, val acc: 0.9140  (best train acc: 0.8191, best val acc: 0.9160, best train loss: 0.5054)\n",
      "[Epoch: 6460] train loss: 0.5398, train acc: 0.7854, val loss: 0.3676, val acc: 0.9143  (best train acc: 0.8191, best val acc: 0.9160, best train loss: 0.5054)\n",
      "[Epoch: 6480] train loss: 0.5290, train acc: 0.7938, val loss: 0.3775, val acc: 0.9029  (best train acc: 0.8191, best val acc: 0.9160, best train loss: 0.5054)\n",
      "[Epoch: 6500] train loss: 0.5250, train acc: 0.7978, val loss: 0.3586, val acc: 0.9116  (best train acc: 0.8191, best val acc: 0.9160, best train loss: 0.5054)\n",
      "[Epoch: 6520] train loss: 0.5607, train acc: 0.7835, val loss: 0.3609, val acc: 0.9076  (best train acc: 0.8191, best val acc: 0.9160, best train loss: 0.5054)\n",
      "[Epoch: 6540] train loss: 0.5482, train acc: 0.7822, val loss: 0.3633, val acc: 0.9076  (best train acc: 0.8191, best val acc: 0.9174, best train loss: 0.5054)\n",
      "[Epoch: 6560] train loss: 0.5368, train acc: 0.8010, val loss: 0.3716, val acc: 0.9046  (best train acc: 0.8191, best val acc: 0.9174, best train loss: 0.5054)\n",
      "[Epoch: 6580] train loss: 0.5774, train acc: 0.7578, val loss: 0.4059, val acc: 0.8840  (best train acc: 0.8191, best val acc: 0.9174, best train loss: 0.5054)\n",
      "[Epoch: 6600] train loss: 0.6116, train acc: 0.7528, val loss: 0.3666, val acc: 0.9066  (best train acc: 0.8191, best val acc: 0.9174, best train loss: 0.5054)\n",
      "[Epoch: 6620] train loss: 0.5497, train acc: 0.7775, val loss: 0.4036, val acc: 0.8766  (best train acc: 0.8191, best val acc: 0.9174, best train loss: 0.5054)\n",
      "[Epoch: 6640] train loss: 0.5523, train acc: 0.7830, val loss: 0.3568, val acc: 0.9153  (best train acc: 0.8191, best val acc: 0.9184, best train loss: 0.5037)\n",
      "[Epoch: 6660] train loss: 0.5067, train acc: 0.8065, val loss: 0.3526, val acc: 0.9167  (best train acc: 0.8202, best val acc: 0.9184, best train loss: 0.4919)\n",
      "[Epoch: 6680] train loss: 0.5324, train acc: 0.7874, val loss: 0.3684, val acc: 0.9116  (best train acc: 0.8202, best val acc: 0.9184, best train loss: 0.4919)\n",
      "[Epoch: 6700] train loss: 0.4826, train acc: 0.8219, val loss: 0.3587, val acc: 0.9170  (best train acc: 0.8257, best val acc: 0.9184, best train loss: 0.4780)\n",
      "[Epoch: 6720] train loss: 0.4857, train acc: 0.8173, val loss: 0.3646, val acc: 0.9120  (best train acc: 0.8257, best val acc: 0.9184, best train loss: 0.4780)\n",
      "[Epoch: 6740] train loss: 0.4884, train acc: 0.8178, val loss: 0.3534, val acc: 0.9184  (best train acc: 0.8257, best val acc: 0.9191, best train loss: 0.4780)\n",
      "[Epoch: 6760] train loss: 0.5100, train acc: 0.8054, val loss: 0.3567, val acc: 0.9191  (best train acc: 0.8257, best val acc: 0.9191, best train loss: 0.4780)\n",
      "[Epoch: 6780] train loss: 0.4925, train acc: 0.8120, val loss: 0.3618, val acc: 0.9056  (best train acc: 0.8257, best val acc: 0.9191, best train loss: 0.4780)\n",
      "[Epoch: 6800] train loss: 0.5140, train acc: 0.8020, val loss: 0.3605, val acc: 0.9147  (best train acc: 0.8257, best val acc: 0.9191, best train loss: 0.4729)\n",
      "[Epoch: 6820] train loss: 0.5136, train acc: 0.7992, val loss: 0.3529, val acc: 0.9194  (best train acc: 0.8257, best val acc: 0.9194, best train loss: 0.4729)\n",
      "[Epoch: 6840] train loss: 0.5136, train acc: 0.7971, val loss: 0.3594, val acc: 0.9130  (best train acc: 0.8257, best val acc: 0.9194, best train loss: 0.4729)\n",
      "[Epoch: 6860] train loss: 0.5030, train acc: 0.7992, val loss: 0.3622, val acc: 0.9099  (best train acc: 0.8257, best val acc: 0.9194, best train loss: 0.4625)\n",
      "[Epoch: 6880] train loss: 0.5162, train acc: 0.8049, val loss: 0.3565, val acc: 0.9150  (best train acc: 0.8257, best val acc: 0.9201, best train loss: 0.4625)\n",
      "[Epoch: 6900] train loss: 0.5188, train acc: 0.7895, val loss: 0.3696, val acc: 0.9093  (best train acc: 0.8257, best val acc: 0.9201, best train loss: 0.4625)\n",
      "[Epoch: 6920] train loss: 0.4755, train acc: 0.8213, val loss: 0.3539, val acc: 0.9143  (best train acc: 0.8257, best val acc: 0.9207, best train loss: 0.4625)\n",
      "[Epoch: 6940] train loss: 0.4847, train acc: 0.8206, val loss: 0.3610, val acc: 0.9083  (best train acc: 0.8261, best val acc: 0.9214, best train loss: 0.4625)\n",
      "[Epoch: 6960] train loss: 0.4773, train acc: 0.8206, val loss: 0.3599, val acc: 0.9177  (best train acc: 0.8263, best val acc: 0.9214, best train loss: 0.4625)\n",
      "[Epoch: 6980] train loss: 0.4994, train acc: 0.8009, val loss: 0.3739, val acc: 0.9126  (best train acc: 0.8263, best val acc: 0.9214, best train loss: 0.4625)\n",
      "[Epoch: 7000] train loss: 0.4984, train acc: 0.8194, val loss: 0.3612, val acc: 0.9116  (best train acc: 0.8263, best val acc: 0.9214, best train loss: 0.4625)\n",
      "[Epoch: 7020] train loss: 0.4938, train acc: 0.8031, val loss: 0.3574, val acc: 0.9126  (best train acc: 0.8263, best val acc: 0.9214, best train loss: 0.4625)\n",
      "[Epoch: 7040] train loss: 0.4725, train acc: 0.8231, val loss: 0.3607, val acc: 0.9140  (best train acc: 0.8263, best val acc: 0.9214, best train loss: 0.4625)\n",
      "[Epoch: 7060] train loss: 0.4943, train acc: 0.8055, val loss: 0.3607, val acc: 0.9133  (best train acc: 0.8263, best val acc: 0.9214, best train loss: 0.4625)\n",
      "[Epoch: 7080] train loss: 0.4787, train acc: 0.8124, val loss: 0.3529, val acc: 0.9164  (best train acc: 0.8263, best val acc: 0.9214, best train loss: 0.4615)\n",
      "[Epoch: 7100] train loss: 0.4849, train acc: 0.8134, val loss: 0.3775, val acc: 0.9032  (best train acc: 0.8263, best val acc: 0.9214, best train loss: 0.4615)\n",
      "[Epoch: 7120] train loss: 0.4761, train acc: 0.8158, val loss: 0.4078, val acc: 0.8938  (best train acc: 0.8263, best val acc: 0.9214, best train loss: 0.4615)\n",
      "[Epoch: 7140] train loss: 0.4997, train acc: 0.8039, val loss: 0.3866, val acc: 0.8934  (best train acc: 0.8263, best val acc: 0.9214, best train loss: 0.4615)\n",
      "[Epoch: 7160] train loss: 0.4755, train acc: 0.8161, val loss: 0.3545, val acc: 0.9211  (best train acc: 0.8263, best val acc: 0.9214, best train loss: 0.4615)\n",
      "[Epoch: 7180] train loss: 0.4855, train acc: 0.8172, val loss: 0.3525, val acc: 0.9194  (best train acc: 0.8325, best val acc: 0.9214, best train loss: 0.4556)\n",
      "[Epoch: 7200] train loss: 0.4786, train acc: 0.8188, val loss: 0.3523, val acc: 0.9170  (best train acc: 0.8325, best val acc: 0.9214, best train loss: 0.4556)\n",
      "[Epoch: 7220] train loss: 0.4768, train acc: 0.8111, val loss: 0.3570, val acc: 0.9130  (best train acc: 0.8325, best val acc: 0.9214, best train loss: 0.4556)\n",
      "[Epoch: 7240] train loss: 0.4865, train acc: 0.8098, val loss: 0.3525, val acc: 0.9160  (best train acc: 0.8325, best val acc: 0.9214, best train loss: 0.4552)\n",
      "[Epoch: 7260] train loss: 0.4794, train acc: 0.8182, val loss: 0.3569, val acc: 0.9140  (best train acc: 0.8325, best val acc: 0.9214, best train loss: 0.4529)\n",
      "[Epoch: 7280] train loss: 0.4641, train acc: 0.8213, val loss: 0.3578, val acc: 0.9164  (best train acc: 0.8325, best val acc: 0.9214, best train loss: 0.4529)\n",
      "[Epoch: 7300] train loss: 0.4802, train acc: 0.8070, val loss: 0.3575, val acc: 0.9164  (best train acc: 0.8363, best val acc: 0.9214, best train loss: 0.4376)\n",
      "[Epoch: 7320] train loss: 0.4575, train acc: 0.8254, val loss: 0.3513, val acc: 0.9167  (best train acc: 0.8363, best val acc: 0.9214, best train loss: 0.4376)\n",
      "[Epoch: 7340] train loss: 0.4738, train acc: 0.8133, val loss: 0.3425, val acc: 0.9170  (best train acc: 0.8363, best val acc: 0.9214, best train loss: 0.4376)\n",
      "[Epoch: 7360] train loss: 0.4577, train acc: 0.8233, val loss: 0.3510, val acc: 0.9150  (best train acc: 0.8363, best val acc: 0.9214, best train loss: 0.4376)\n",
      "[Epoch: 7380] train loss: 0.4559, train acc: 0.8194, val loss: 0.3591, val acc: 0.9184  (best train acc: 0.8363, best val acc: 0.9214, best train loss: 0.4376)\n",
      "[Epoch: 7400] train loss: 0.4543, train acc: 0.8240, val loss: 0.3544, val acc: 0.9187  (best train acc: 0.8368, best val acc: 0.9214, best train loss: 0.4376)\n",
      "[Epoch: 7420] train loss: 0.5002, train acc: 0.8092, val loss: 0.3523, val acc: 0.9180  (best train acc: 0.8368, best val acc: 0.9214, best train loss: 0.4342)\n",
      "[Epoch: 7440] train loss: 0.4908, train acc: 0.8145, val loss: 0.3531, val acc: 0.9167  (best train acc: 0.8368, best val acc: 0.9214, best train loss: 0.4306)\n",
      "[Epoch: 7460] train loss: 0.4727, train acc: 0.8127, val loss: 0.3433, val acc: 0.9194  (best train acc: 0.8368, best val acc: 0.9214, best train loss: 0.4306)\n",
      "[Epoch: 7480] train loss: 0.4733, train acc: 0.8187, val loss: 0.3543, val acc: 0.9174  (best train acc: 0.8368, best val acc: 0.9214, best train loss: 0.4306)\n",
      "[Epoch: 7500] train loss: 0.4825, train acc: 0.8128, val loss: 0.3653, val acc: 0.9137  (best train acc: 0.8368, best val acc: 0.9218, best train loss: 0.4306)\n",
      "[Epoch: 7520] train loss: 0.4502, train acc: 0.8297, val loss: 0.3496, val acc: 0.9191  (best train acc: 0.8368, best val acc: 0.9218, best train loss: 0.4306)\n",
      "[Epoch: 7540] train loss: 0.4585, train acc: 0.8232, val loss: 0.3537, val acc: 0.9184  (best train acc: 0.8370, best val acc: 0.9231, best train loss: 0.4306)\n",
      "[Epoch: 7560] train loss: 0.4503, train acc: 0.8316, val loss: 0.3638, val acc: 0.9194  (best train acc: 0.8381, best val acc: 0.9231, best train loss: 0.4306)\n",
      "[Epoch: 7580] train loss: 0.5099, train acc: 0.8056, val loss: 0.3639, val acc: 0.9110  (best train acc: 0.8381, best val acc: 0.9231, best train loss: 0.4306)\n",
      "[Epoch: 7600] train loss: 0.4707, train acc: 0.8175, val loss: 0.3523, val acc: 0.9187  (best train acc: 0.8381, best val acc: 0.9231, best train loss: 0.4306)\n",
      "[Epoch: 7620] train loss: 0.4557, train acc: 0.8258, val loss: 0.3416, val acc: 0.9191  (best train acc: 0.8381, best val acc: 0.9231, best train loss: 0.4306)\n",
      "[Epoch: 7640] train loss: 0.4574, train acc: 0.8344, val loss: 0.3625, val acc: 0.9137  (best train acc: 0.8381, best val acc: 0.9231, best train loss: 0.4283)\n",
      "[Epoch: 7660] train loss: 0.4582, train acc: 0.8258, val loss: 0.3456, val acc: 0.9180  (best train acc: 0.8382, best val acc: 0.9231, best train loss: 0.4283)\n",
      "[Epoch: 7680] train loss: 0.4511, train acc: 0.8287, val loss: 0.3474, val acc: 0.9207  (best train acc: 0.8382, best val acc: 0.9231, best train loss: 0.4283)\n",
      "[Epoch: 7700] train loss: 0.4667, train acc: 0.8202, val loss: 0.3614, val acc: 0.9116  (best train acc: 0.8383, best val acc: 0.9231, best train loss: 0.4283)\n",
      "[Epoch: 7720] train loss: 0.4507, train acc: 0.8343, val loss: 0.3603, val acc: 0.9103  (best train acc: 0.8446, best val acc: 0.9231, best train loss: 0.4283)\n",
      "[Epoch: 7740] train loss: 0.4465, train acc: 0.8362, val loss: 0.3575, val acc: 0.9184  (best train acc: 0.8446, best val acc: 0.9231, best train loss: 0.4283)\n",
      "[Epoch: 7760] train loss: 0.4486, train acc: 0.8347, val loss: 0.3515, val acc: 0.9197  (best train acc: 0.8446, best val acc: 0.9231, best train loss: 0.4283)\n",
      "[Epoch: 7780] train loss: 0.4937, train acc: 0.8129, val loss: 0.3504, val acc: 0.9194  (best train acc: 0.8446, best val acc: 0.9231, best train loss: 0.4283)\n",
      "[Epoch: 7800] train loss: 0.4633, train acc: 0.8296, val loss: 0.3552, val acc: 0.9164  (best train acc: 0.8446, best val acc: 0.9231, best train loss: 0.4283)\n",
      "[Epoch: 7820] train loss: 0.4691, train acc: 0.8237, val loss: 0.3535, val acc: 0.9170  (best train acc: 0.8446, best val acc: 0.9231, best train loss: 0.4227)\n",
      "[Epoch: 7840] train loss: 0.4509, train acc: 0.8281, val loss: 0.3536, val acc: 0.9140  (best train acc: 0.8446, best val acc: 0.9231, best train loss: 0.4227)\n",
      "[Epoch: 7860] train loss: 0.4404, train acc: 0.8388, val loss: 0.3585, val acc: 0.9197  (best train acc: 0.8446, best val acc: 0.9231, best train loss: 0.4227)\n",
      "[Epoch: 7880] train loss: 0.4501, train acc: 0.8294, val loss: 0.3599, val acc: 0.9130  (best train acc: 0.8446, best val acc: 0.9231, best train loss: 0.4227)\n",
      "[Epoch: 7900] train loss: 0.4839, train acc: 0.8142, val loss: 0.3374, val acc: 0.9221  (best train acc: 0.8446, best val acc: 0.9231, best train loss: 0.4227)\n",
      "[Epoch: 7920] train loss: 0.4438, train acc: 0.8326, val loss: 0.3385, val acc: 0.9211  (best train acc: 0.8446, best val acc: 0.9231, best train loss: 0.4227)\n",
      "[Epoch: 7940] train loss: 0.4491, train acc: 0.8290, val loss: 0.3460, val acc: 0.9221  (best train acc: 0.8446, best val acc: 0.9231, best train loss: 0.4227)\n",
      "[Epoch: 7960] train loss: 0.4470, train acc: 0.8288, val loss: 0.3527, val acc: 0.9177  (best train acc: 0.8446, best val acc: 0.9231, best train loss: 0.4227)\n",
      "[Epoch: 7980] train loss: 0.4850, train acc: 0.8112, val loss: 0.3549, val acc: 0.9137  (best train acc: 0.8446, best val acc: 0.9234, best train loss: 0.4227)\n",
      "[Epoch: 8000] train loss: 0.4475, train acc: 0.8368, val loss: 0.3505, val acc: 0.9218  (best train acc: 0.8446, best val acc: 0.9234, best train loss: 0.4227)\n",
      "[Epoch: 8020] train loss: 0.4230, train acc: 0.8351, val loss: 0.3457, val acc: 0.9180  (best train acc: 0.8509, best val acc: 0.9234, best train loss: 0.4145)\n",
      "[Epoch: 8040] train loss: 0.4176, train acc: 0.8402, val loss: 0.3410, val acc: 0.9187  (best train acc: 0.8509, best val acc: 0.9234, best train loss: 0.4073)\n",
      "[Epoch: 8060] train loss: 0.4096, train acc: 0.8407, val loss: 0.3493, val acc: 0.9204  (best train acc: 0.8509, best val acc: 0.9234, best train loss: 0.4036)\n",
      "[Epoch: 8080] train loss: 0.4056, train acc: 0.8381, val loss: 0.3534, val acc: 0.9083  (best train acc: 0.8509, best val acc: 0.9234, best train loss: 0.3970)\n",
      "[Epoch: 8100] train loss: 0.4088, train acc: 0.8406, val loss: 0.3437, val acc: 0.9207  (best train acc: 0.8509, best val acc: 0.9234, best train loss: 0.3908)\n",
      "[Epoch: 8120] train loss: 0.4297, train acc: 0.8271, val loss: 0.3663, val acc: 0.9103  (best train acc: 0.8509, best val acc: 0.9234, best train loss: 0.3906)\n",
      "[Epoch: 8140] train loss: 0.4269, train acc: 0.8269, val loss: 0.3529, val acc: 0.9180  (best train acc: 0.8509, best val acc: 0.9238, best train loss: 0.3906)\n",
      "[Epoch: 8160] train loss: 0.3994, train acc: 0.8412, val loss: 0.3410, val acc: 0.9197  (best train acc: 0.8509, best val acc: 0.9251, best train loss: 0.3896)\n",
      "[Epoch: 8180] train loss: 0.4157, train acc: 0.8321, val loss: 0.3345, val acc: 0.9241  (best train acc: 0.8509, best val acc: 0.9251, best train loss: 0.3896)\n",
      "[Epoch: 8200] train loss: 0.4061, train acc: 0.8342, val loss: 0.3446, val acc: 0.9211  (best train acc: 0.8509, best val acc: 0.9251, best train loss: 0.3896)\n",
      "[Epoch: 8220] train loss: 0.4075, train acc: 0.8389, val loss: 0.3329, val acc: 0.9228  (best train acc: 0.8509, best val acc: 0.9251, best train loss: 0.3896)\n",
      "[Epoch: 8240] train loss: 0.4027, train acc: 0.8407, val loss: 0.3382, val acc: 0.9201  (best train acc: 0.8509, best val acc: 0.9255, best train loss: 0.3896)\n",
      "[Epoch: 8260] train loss: 0.3973, train acc: 0.8437, val loss: 0.3403, val acc: 0.9170  (best train acc: 0.8509, best val acc: 0.9255, best train loss: 0.3896)\n",
      "[Epoch: 8280] train loss: 0.4206, train acc: 0.8308, val loss: 0.3337, val acc: 0.9231  (best train acc: 0.8509, best val acc: 0.9255, best train loss: 0.3874)\n",
      "[Epoch: 8300] train loss: 0.4069, train acc: 0.8337, val loss: 0.3300, val acc: 0.9170  (best train acc: 0.8509, best val acc: 0.9255, best train loss: 0.3874)\n",
      "[Epoch: 8320] train loss: 0.4035, train acc: 0.8479, val loss: 0.3366, val acc: 0.9160  (best train acc: 0.8509, best val acc: 0.9255, best train loss: 0.3874)\n",
      "[Epoch: 8340] train loss: 0.3982, train acc: 0.8404, val loss: 0.3246, val acc: 0.9224  (best train acc: 0.8509, best val acc: 0.9258, best train loss: 0.3874)\n",
      "[Epoch: 8360] train loss: 0.4193, train acc: 0.8320, val loss: 0.3246, val acc: 0.9221  (best train acc: 0.8509, best val acc: 0.9258, best train loss: 0.3867)\n",
      "[Epoch: 8380] train loss: 0.3850, train acc: 0.8550, val loss: 0.3194, val acc: 0.9231  (best train acc: 0.8550, best val acc: 0.9258, best train loss: 0.3850)\n",
      "[Epoch: 8400] train loss: 0.4017, train acc: 0.8412, val loss: 0.3233, val acc: 0.9221  (best train acc: 0.8550, best val acc: 0.9261, best train loss: 0.3850)\n",
      "[Epoch: 8420] train loss: 0.4037, train acc: 0.8398, val loss: 0.3198, val acc: 0.9255  (best train acc: 0.8555, best val acc: 0.9261, best train loss: 0.3832)\n",
      "[Epoch: 8440] train loss: 0.3983, train acc: 0.8433, val loss: 0.3226, val acc: 0.9241  (best train acc: 0.8555, best val acc: 0.9272, best train loss: 0.3832)\n",
      "[Epoch: 8460] train loss: 0.4124, train acc: 0.8376, val loss: 0.3612, val acc: 0.9039  (best train acc: 0.8555, best val acc: 0.9272, best train loss: 0.3832)\n",
      "[Epoch: 8480] train loss: 0.4070, train acc: 0.8433, val loss: 0.3520, val acc: 0.9035  (best train acc: 0.8555, best val acc: 0.9272, best train loss: 0.3832)\n",
      "[Epoch: 8500] train loss: 0.3967, train acc: 0.8425, val loss: 0.3169, val acc: 0.9204  (best train acc: 0.8555, best val acc: 0.9272, best train loss: 0.3832)\n",
      "[Epoch: 8520] train loss: 0.3983, train acc: 0.8427, val loss: 0.3144, val acc: 0.9214  (best train acc: 0.8555, best val acc: 0.9272, best train loss: 0.3814)\n",
      "[Epoch: 8540] train loss: 0.3994, train acc: 0.8450, val loss: 0.3288, val acc: 0.9207  (best train acc: 0.8558, best val acc: 0.9282, best train loss: 0.3748)\n",
      "[Epoch: 8560] train loss: 0.4147, train acc: 0.8342, val loss: 0.3326, val acc: 0.9261  (best train acc: 0.8558, best val acc: 0.9282, best train loss: 0.3748)\n",
      "[Epoch: 8580] train loss: 0.4030, train acc: 0.8412, val loss: 0.3153, val acc: 0.9255  (best train acc: 0.8558, best val acc: 0.9282, best train loss: 0.3748)\n",
      "[Epoch: 8600] train loss: 0.3834, train acc: 0.8477, val loss: 0.3120, val acc: 0.9238  (best train acc: 0.8558, best val acc: 0.9282, best train loss: 0.3748)\n",
      "[Epoch: 8620] train loss: 0.3957, train acc: 0.8446, val loss: 0.3199, val acc: 0.9191  (best train acc: 0.8558, best val acc: 0.9282, best train loss: 0.3748)\n",
      "[Epoch: 8640] train loss: 0.4125, train acc: 0.8354, val loss: 0.3213, val acc: 0.9234  (best train acc: 0.8558, best val acc: 0.9282, best train loss: 0.3748)\n",
      "[Epoch: 8660] train loss: 0.3838, train acc: 0.8518, val loss: 0.3201, val acc: 0.9224  (best train acc: 0.8558, best val acc: 0.9282, best train loss: 0.3748)\n",
      "[Epoch: 8680] train loss: 0.4123, train acc: 0.8327, val loss: 0.3159, val acc: 0.9214  (best train acc: 0.8558, best val acc: 0.9292, best train loss: 0.3748)\n",
      "[Epoch: 8700] train loss: 0.4071, train acc: 0.8365, val loss: 0.3217, val acc: 0.9248  (best train acc: 0.8558, best val acc: 0.9292, best train loss: 0.3748)\n",
      "[Epoch: 8720] train loss: 0.3899, train acc: 0.8440, val loss: 0.3289, val acc: 0.9255  (best train acc: 0.8558, best val acc: 0.9292, best train loss: 0.3748)\n",
      "[Epoch: 8740] train loss: 0.4046, train acc: 0.8449, val loss: 0.3349, val acc: 0.9180  (best train acc: 0.8558, best val acc: 0.9292, best train loss: 0.3748)\n",
      "[Epoch: 8760] train loss: 0.4001, train acc: 0.8392, val loss: 0.3132, val acc: 0.9268  (best train acc: 0.8558, best val acc: 0.9292, best train loss: 0.3748)\n",
      "[Epoch: 8780] train loss: 0.3899, train acc: 0.8495, val loss: 0.3301, val acc: 0.9211  (best train acc: 0.8558, best val acc: 0.9292, best train loss: 0.3748)\n",
      "[Epoch: 8800] train loss: 0.4411, train acc: 0.8302, val loss: 0.3341, val acc: 0.9133  (best train acc: 0.8558, best val acc: 0.9292, best train loss: 0.3748)\n",
      "[Epoch: 8820] train loss: 0.3854, train acc: 0.8500, val loss: 0.3143, val acc: 0.9245  (best train acc: 0.8558, best val acc: 0.9292, best train loss: 0.3748)\n",
      "[Epoch: 8840] train loss: 0.3974, train acc: 0.8380, val loss: 0.3088, val acc: 0.9261  (best train acc: 0.8558, best val acc: 0.9292, best train loss: 0.3748)\n",
      "[Epoch: 8860] train loss: 0.4048, train acc: 0.8354, val loss: 0.3121, val acc: 0.9187  (best train acc: 0.8558, best val acc: 0.9292, best train loss: 0.3748)\n",
      "[Epoch: 8880] train loss: 0.3902, train acc: 0.8444, val loss: 0.3130, val acc: 0.9268  (best train acc: 0.8558, best val acc: 0.9292, best train loss: 0.3748)\n",
      "[Epoch: 8900] train loss: 0.3937, train acc: 0.8455, val loss: 0.3203, val acc: 0.9248  (best train acc: 0.8558, best val acc: 0.9292, best train loss: 0.3748)\n",
      "[Epoch: 8920] train loss: 0.3842, train acc: 0.8553, val loss: 0.3183, val acc: 0.9261  (best train acc: 0.8558, best val acc: 0.9292, best train loss: 0.3746)\n",
      "[Epoch: 8940] train loss: 0.3969, train acc: 0.8414, val loss: 0.3541, val acc: 0.9066  (best train acc: 0.8558, best val acc: 0.9292, best train loss: 0.3746)\n",
      "[Epoch: 8960] train loss: 0.4027, train acc: 0.8389, val loss: 0.3175, val acc: 0.9275  (best train acc: 0.8558, best val acc: 0.9292, best train loss: 0.3746)\n",
      "[Epoch: 8980] train loss: 0.3963, train acc: 0.8411, val loss: 0.3429, val acc: 0.9062  (best train acc: 0.8558, best val acc: 0.9292, best train loss: 0.3746)\n",
      "[Epoch: 9000] train loss: 0.4093, train acc: 0.8326, val loss: 0.3376, val acc: 0.9218  (best train acc: 0.8558, best val acc: 0.9292, best train loss: 0.3746)\n",
      "[Epoch: 9020] train loss: 0.4285, train acc: 0.8305, val loss: 0.3185, val acc: 0.9234  (best train acc: 0.8558, best val acc: 0.9292, best train loss: 0.3746)\n",
      "[Epoch: 9040] train loss: 0.4070, train acc: 0.8365, val loss: 0.3088, val acc: 0.9218  (best train acc: 0.8558, best val acc: 0.9292, best train loss: 0.3746)\n",
      "[Epoch: 9060] train loss: 0.3972, train acc: 0.8417, val loss: 0.3171, val acc: 0.9258  (best train acc: 0.8558, best val acc: 0.9292, best train loss: 0.3693)\n",
      "[Epoch: 9080] train loss: 0.3987, train acc: 0.8425, val loss: 0.3220, val acc: 0.9187  (best train acc: 0.8558, best val acc: 0.9292, best train loss: 0.3693)\n",
      "[Epoch: 9100] train loss: 0.3829, train acc: 0.8507, val loss: 0.3226, val acc: 0.9268  (best train acc: 0.8558, best val acc: 0.9292, best train loss: 0.3693)\n",
      "[Epoch: 9120] train loss: 0.3894, train acc: 0.8459, val loss: 0.3131, val acc: 0.9234  (best train acc: 0.8558, best val acc: 0.9292, best train loss: 0.3693)\n",
      "[Epoch: 9140] train loss: 0.4074, train acc: 0.8378, val loss: 0.3141, val acc: 0.9275  (best train acc: 0.8558, best val acc: 0.9292, best train loss: 0.3693)\n",
      "[Epoch: 9160] train loss: 0.3929, train acc: 0.8458, val loss: 0.3279, val acc: 0.9164  (best train acc: 0.8558, best val acc: 0.9292, best train loss: 0.3693)\n",
      "[Epoch: 9180] train loss: 0.4181, train acc: 0.8407, val loss: 0.3181, val acc: 0.9251  (best train acc: 0.8558, best val acc: 0.9295, best train loss: 0.3693)\n",
      "[Epoch: 9200] train loss: 0.4326, train acc: 0.8274, val loss: 0.3249, val acc: 0.9241  (best train acc: 0.8558, best val acc: 0.9295, best train loss: 0.3693)\n",
      "[Epoch: 9220] train loss: 0.3851, train acc: 0.8459, val loss: 0.3204, val acc: 0.9238  (best train acc: 0.8558, best val acc: 0.9295, best train loss: 0.3681)\n",
      "[Epoch: 9240] train loss: 0.3745, train acc: 0.8485, val loss: 0.3415, val acc: 0.9153  (best train acc: 0.8558, best val acc: 0.9295, best train loss: 0.3681)\n",
      "[Epoch: 9260] train loss: 0.3836, train acc: 0.8461, val loss: 0.3344, val acc: 0.9214  (best train acc: 0.8558, best val acc: 0.9295, best train loss: 0.3681)\n",
      "[Epoch: 9280] train loss: 0.3861, train acc: 0.8469, val loss: 0.3273, val acc: 0.9285  (best train acc: 0.8558, best val acc: 0.9295, best train loss: 0.3681)\n",
      "[Epoch: 9300] train loss: 0.3757, train acc: 0.8501, val loss: 0.3268, val acc: 0.9255  (best train acc: 0.8558, best val acc: 0.9295, best train loss: 0.3681)\n",
      "[Epoch: 9320] train loss: 0.3995, train acc: 0.8461, val loss: 0.3279, val acc: 0.9268  (best train acc: 0.8617, best val acc: 0.9295, best train loss: 0.3657)\n",
      "[Epoch: 9340] train loss: 0.4077, train acc: 0.8373, val loss: 0.3506, val acc: 0.9153  (best train acc: 0.8617, best val acc: 0.9295, best train loss: 0.3657)\n",
      "[Epoch: 9360] train loss: 0.4068, train acc: 0.8359, val loss: 0.3415, val acc: 0.9258  (best train acc: 0.8617, best val acc: 0.9295, best train loss: 0.3657)\n",
      "[Epoch: 9380] train loss: 0.4087, train acc: 0.8392, val loss: 0.3379, val acc: 0.9157  (best train acc: 0.8617, best val acc: 0.9295, best train loss: 0.3657)\n",
      "[Epoch: 9400] train loss: 0.4049, train acc: 0.8429, val loss: 0.3301, val acc: 0.9275  (best train acc: 0.8617, best val acc: 0.9295, best train loss: 0.3657)\n",
      "[Epoch: 9420] train loss: 0.3820, train acc: 0.8483, val loss: 0.3323, val acc: 0.9231  (best train acc: 0.8617, best val acc: 0.9295, best train loss: 0.3657)\n",
      "[Epoch: 9440] train loss: 0.3884, train acc: 0.8442, val loss: 0.3393, val acc: 0.9204  (best train acc: 0.8617, best val acc: 0.9295, best train loss: 0.3657)\n",
      "[Epoch: 9460] train loss: 0.3963, train acc: 0.8446, val loss: 0.3289, val acc: 0.9275  (best train acc: 0.8617, best val acc: 0.9295, best train loss: 0.3657)\n",
      "[Epoch: 9480] train loss: 0.4027, train acc: 0.8391, val loss: 0.3246, val acc: 0.9278  (best train acc: 0.8617, best val acc: 0.9295, best train loss: 0.3657)\n",
      "[Epoch: 9500] train loss: 0.3872, train acc: 0.8462, val loss: 0.3279, val acc: 0.9245  (best train acc: 0.8617, best val acc: 0.9295, best train loss: 0.3657)\n",
      "[Epoch: 9520] train loss: 0.4041, train acc: 0.8370, val loss: 0.3387, val acc: 0.9261  (best train acc: 0.8617, best val acc: 0.9312, best train loss: 0.3657)\n",
      "[Epoch: 9540] train loss: 0.4028, train acc: 0.8362, val loss: 0.3241, val acc: 0.9275  (best train acc: 0.8617, best val acc: 0.9312, best train loss: 0.3657)\n",
      "[Epoch: 9560] train loss: 0.3875, train acc: 0.8383, val loss: 0.3557, val acc: 0.9197  (best train acc: 0.8617, best val acc: 0.9312, best train loss: 0.3657)\n",
      "[Epoch: 9580] train loss: 0.3850, train acc: 0.8423, val loss: 0.3472, val acc: 0.9130  (best train acc: 0.8617, best val acc: 0.9312, best train loss: 0.3657)\n",
      "[Epoch: 9600] train loss: 0.4095, train acc: 0.8334, val loss: 0.3356, val acc: 0.9265  (best train acc: 0.8617, best val acc: 0.9312, best train loss: 0.3657)\n",
      "[Epoch: 9620] train loss: 0.3995, train acc: 0.8355, val loss: 0.3381, val acc: 0.9268  (best train acc: 0.8617, best val acc: 0.9312, best train loss: 0.3657)\n",
      "[Epoch: 9640] train loss: 0.4357, train acc: 0.8226, val loss: 0.3383, val acc: 0.9201  (best train acc: 0.8617, best val acc: 0.9312, best train loss: 0.3657)\n",
      "[Epoch: 9660] train loss: 0.4011, train acc: 0.8405, val loss: 0.3236, val acc: 0.9298  (best train acc: 0.8617, best val acc: 0.9312, best train loss: 0.3657)\n",
      "[Epoch: 9680] train loss: 0.3747, train acc: 0.8534, val loss: 0.3196, val acc: 0.9245  (best train acc: 0.8617, best val acc: 0.9312, best train loss: 0.3657)\n",
      "[Epoch: 9700] train loss: 0.4169, train acc: 0.8383, val loss: 0.3331, val acc: 0.9285  (best train acc: 0.8617, best val acc: 0.9312, best train loss: 0.3657)\n",
      "[Epoch: 9720] train loss: 0.3776, train acc: 0.8457, val loss: 0.3368, val acc: 0.9272  (best train acc: 0.8617, best val acc: 0.9312, best train loss: 0.3650)\n",
      "[Epoch: 9740] train loss: 0.3591, train acc: 0.8608, val loss: 0.3411, val acc: 0.9265  (best train acc: 0.8617, best val acc: 0.9312, best train loss: 0.3566)\n",
      "[Epoch: 9760] train loss: 0.3603, train acc: 0.8621, val loss: 0.3491, val acc: 0.9218  (best train acc: 0.8644, best val acc: 0.9312, best train loss: 0.3499)\n",
      "[Epoch: 9780] train loss: 0.3667, train acc: 0.8522, val loss: 0.3436, val acc: 0.9261  (best train acc: 0.8644, best val acc: 0.9312, best train loss: 0.3499)\n",
      "[Epoch: 9800] train loss: 0.3615, train acc: 0.8595, val loss: 0.3336, val acc: 0.9275  (best train acc: 0.8652, best val acc: 0.9312, best train loss: 0.3499)\n",
      "[Epoch: 9820] train loss: 0.3759, train acc: 0.8504, val loss: 0.3502, val acc: 0.9298  (best train acc: 0.8652, best val acc: 0.9312, best train loss: 0.3499)\n",
      "[Epoch: 9840] train loss: 0.3717, train acc: 0.8522, val loss: 0.3479, val acc: 0.9265  (best train acc: 0.8688, best val acc: 0.9312, best train loss: 0.3434)\n",
      "[Epoch: 9860] train loss: 0.3957, train acc: 0.8452, val loss: 0.3604, val acc: 0.9251  (best train acc: 0.8688, best val acc: 0.9312, best train loss: 0.3434)\n",
      "[Epoch: 9880] train loss: 0.3421, train acc: 0.8700, val loss: 0.3522, val acc: 0.9268  (best train acc: 0.8700, best val acc: 0.9312, best train loss: 0.3421)\n",
      "[Epoch: 9900] train loss: 0.3632, train acc: 0.8557, val loss: 0.3498, val acc: 0.9278  (best train acc: 0.8700, best val acc: 0.9312, best train loss: 0.3421)\n",
      "[Epoch: 9920] train loss: 0.3692, train acc: 0.8572, val loss: 0.3471, val acc: 0.9282  (best train acc: 0.8700, best val acc: 0.9312, best train loss: 0.3421)\n",
      "[Epoch: 9940] train loss: 0.3581, train acc: 0.8614, val loss: 0.3494, val acc: 0.9255  (best train acc: 0.8700, best val acc: 0.9312, best train loss: 0.3421)\n",
      "[Epoch: 9960] train loss: 0.3686, train acc: 0.8499, val loss: 0.3509, val acc: 0.9292  (best train acc: 0.8700, best val acc: 0.9312, best train loss: 0.3421)\n",
      "[Epoch: 9980] train loss: 0.3940, train acc: 0.8441, val loss: 0.3572, val acc: 0.9261  (best train acc: 0.8700, best val acc: 0.9312, best train loss: 0.3418)\n",
      "[Epoch: 10000] train loss: 0.3983, train acc: 0.8420, val loss: 0.3793, val acc: 0.9079  (best train acc: 0.8700, best val acc: 0.9312, best train loss: 0.3418)\n",
      "[Epoch: 10020] train loss: 0.3566, train acc: 0.8643, val loss: 0.3378, val acc: 0.9258  (best train acc: 0.8700, best val acc: 0.9312, best train loss: 0.3418)\n",
      "[Epoch: 10040] train loss: 0.3602, train acc: 0.8581, val loss: 0.3441, val acc: 0.9275  (best train acc: 0.8700, best val acc: 0.9312, best train loss: 0.3418)\n",
      "[Epoch: 10060] train loss: 0.3601, train acc: 0.8607, val loss: 0.3455, val acc: 0.9288  (best train acc: 0.8700, best val acc: 0.9312, best train loss: 0.3418)\n",
      "[Epoch: 10080] train loss: 0.3806, train acc: 0.8522, val loss: 0.3770, val acc: 0.9079  (best train acc: 0.8705, best val acc: 0.9312, best train loss: 0.3418)\n",
      "[Epoch: 10100] train loss: 0.3686, train acc: 0.8506, val loss: 0.3491, val acc: 0.9268  (best train acc: 0.8705, best val acc: 0.9312, best train loss: 0.3418)\n",
      "[Epoch: 10120] train loss: 0.3636, train acc: 0.8561, val loss: 0.3501, val acc: 0.9295  (best train acc: 0.8705, best val acc: 0.9315, best train loss: 0.3418)\n",
      "[Epoch: 10140] train loss: 0.3541, train acc: 0.8625, val loss: 0.3510, val acc: 0.9295  (best train acc: 0.8705, best val acc: 0.9315, best train loss: 0.3418)\n",
      "[Epoch: 10160] train loss: 0.3668, train acc: 0.8540, val loss: 0.3404, val acc: 0.9288  (best train acc: 0.8705, best val acc: 0.9315, best train loss: 0.3418)\n",
      "[Epoch: 10180] train loss: 0.3227, train acc: 0.8791, val loss: 0.3513, val acc: 0.9288  (best train acc: 0.8791, best val acc: 0.9315, best train loss: 0.3227)\n",
      "[Epoch: 10200] train loss: 0.3247, train acc: 0.8754, val loss: 0.3477, val acc: 0.9228  (best train acc: 0.8841, best val acc: 0.9315, best train loss: 0.3151)\n",
      "[Epoch: 10220] train loss: 0.3478, train acc: 0.8632, val loss: 0.3379, val acc: 0.9285  (best train acc: 0.8856, best val acc: 0.9315, best train loss: 0.3123)\n",
      "[Epoch: 10240] train loss: 0.3581, train acc: 0.8665, val loss: 0.3424, val acc: 0.9083  (best train acc: 0.8856, best val acc: 0.9315, best train loss: 0.3123)\n",
      "[Epoch: 10260] train loss: 0.3224, train acc: 0.8738, val loss: 0.3199, val acc: 0.9245  (best train acc: 0.8856, best val acc: 0.9315, best train loss: 0.3123)\n",
      "[Epoch: 10280] train loss: 0.3130, train acc: 0.8817, val loss: 0.3321, val acc: 0.9261  (best train acc: 0.8856, best val acc: 0.9315, best train loss: 0.3012)\n",
      "[Epoch: 10300] train loss: 0.3129, train acc: 0.8759, val loss: 0.3315, val acc: 0.9295  (best train acc: 0.8877, best val acc: 0.9315, best train loss: 0.2994)\n",
      "[Epoch: 10320] train loss: 0.3383, train acc: 0.8701, val loss: 0.3263, val acc: 0.9265  (best train acc: 0.8887, best val acc: 0.9342, best train loss: 0.2954)\n",
      "[Epoch: 10340] train loss: 0.3160, train acc: 0.8762, val loss: 0.3347, val acc: 0.9218  (best train acc: 0.8906, best val acc: 0.9342, best train loss: 0.2954)\n",
      "[Epoch: 10360] train loss: 0.3006, train acc: 0.8897, val loss: 0.3203, val acc: 0.9325  (best train acc: 0.8906, best val acc: 0.9342, best train loss: 0.2954)\n",
      "[Epoch: 10380] train loss: 0.3092, train acc: 0.8882, val loss: 0.3283, val acc: 0.9275  (best train acc: 0.8916, best val acc: 0.9342, best train loss: 0.2954)\n",
      "[Epoch: 10400] train loss: 0.2882, train acc: 0.8940, val loss: 0.3283, val acc: 0.9325  (best train acc: 0.8940, best val acc: 0.9342, best train loss: 0.2882)\n",
      "[Epoch: 10420] train loss: 0.2916, train acc: 0.8948, val loss: 0.3286, val acc: 0.9292  (best train acc: 0.8948, best val acc: 0.9342, best train loss: 0.2882)\n",
      "[Epoch: 10440] train loss: 0.3256, train acc: 0.8679, val loss: 0.3079, val acc: 0.9218  (best train acc: 0.8948, best val acc: 0.9342, best train loss: 0.2882)\n",
      "[Epoch: 10460] train loss: 0.3202, train acc: 0.8828, val loss: 0.3147, val acc: 0.9288  (best train acc: 0.8948, best val acc: 0.9342, best train loss: 0.2882)\n",
      "[Epoch: 10480] train loss: 0.3202, train acc: 0.8774, val loss: 0.3034, val acc: 0.9248  (best train acc: 0.8948, best val acc: 0.9342, best train loss: 0.2882)\n",
      "[Epoch: 10500] train loss: 0.3135, train acc: 0.8764, val loss: 0.3191, val acc: 0.9298  (best train acc: 0.8948, best val acc: 0.9342, best train loss: 0.2862)\n",
      "[Epoch: 10520] train loss: 0.2967, train acc: 0.8875, val loss: 0.3094, val acc: 0.9309  (best train acc: 0.8948, best val acc: 0.9342, best train loss: 0.2831)\n",
      "[Epoch: 10540] train loss: 0.2959, train acc: 0.8913, val loss: 0.3217, val acc: 0.9339  (best train acc: 0.8948, best val acc: 0.9342, best train loss: 0.2831)\n",
      "[Epoch: 10560] train loss: 0.2902, train acc: 0.8890, val loss: 0.3252, val acc: 0.9305  (best train acc: 0.8948, best val acc: 0.9342, best train loss: 0.2831)\n",
      "[Epoch: 10580] train loss: 0.3140, train acc: 0.8800, val loss: 0.3119, val acc: 0.9322  (best train acc: 0.8963, best val acc: 0.9342, best train loss: 0.2793)\n",
      "[Epoch: 10600] train loss: 0.2840, train acc: 0.8919, val loss: 0.3398, val acc: 0.9319  (best train acc: 0.8963, best val acc: 0.9342, best train loss: 0.2793)\n",
      "[Epoch: 10620] train loss: 0.3099, train acc: 0.8811, val loss: 0.3268, val acc: 0.9272  (best train acc: 0.8963, best val acc: 0.9342, best train loss: 0.2793)\n",
      "[Epoch: 10640] train loss: 0.2950, train acc: 0.8874, val loss: 0.3368, val acc: 0.9265  (best train acc: 0.8970, best val acc: 0.9342, best train loss: 0.2793)\n",
      "[Epoch: 10660] train loss: 0.2875, train acc: 0.8920, val loss: 0.3842, val acc: 0.9056  (best train acc: 0.8970, best val acc: 0.9342, best train loss: 0.2793)\n",
      "[Epoch: 10680] train loss: 0.3126, train acc: 0.8822, val loss: 0.3187, val acc: 0.9305  (best train acc: 0.8970, best val acc: 0.9342, best train loss: 0.2793)\n",
      "[Epoch: 10700] train loss: 0.3232, train acc: 0.8775, val loss: 0.3174, val acc: 0.9066  (best train acc: 0.8970, best val acc: 0.9342, best train loss: 0.2793)\n",
      "[Epoch: 10720] train loss: 0.2954, train acc: 0.8894, val loss: 0.2852, val acc: 0.9312  (best train acc: 0.8970, best val acc: 0.9342, best train loss: 0.2793)\n",
      "[Epoch: 10740] train loss: 0.2990, train acc: 0.8875, val loss: 0.3023, val acc: 0.9305  (best train acc: 0.8970, best val acc: 0.9342, best train loss: 0.2793)\n",
      "[Epoch: 10760] train loss: 0.3071, train acc: 0.8856, val loss: 0.3155, val acc: 0.9288  (best train acc: 0.8970, best val acc: 0.9342, best train loss: 0.2793)\n",
      "[Epoch: 10780] train loss: 0.3070, train acc: 0.8813, val loss: 0.3227, val acc: 0.9177  (best train acc: 0.8970, best val acc: 0.9342, best train loss: 0.2793)\n",
      "[Epoch: 10800] train loss: 0.2856, train acc: 0.8939, val loss: 0.3152, val acc: 0.9282  (best train acc: 0.8970, best val acc: 0.9342, best train loss: 0.2793)\n",
      "[Epoch: 10820] train loss: 0.3059, train acc: 0.8854, val loss: 0.3336, val acc: 0.9201  (best train acc: 0.8970, best val acc: 0.9342, best train loss: 0.2793)\n",
      "[Epoch: 10840] train loss: 0.2928, train acc: 0.8939, val loss: 0.3204, val acc: 0.9288  (best train acc: 0.8970, best val acc: 0.9342, best train loss: 0.2793)\n",
      "[Epoch: 10860] train loss: 0.2847, train acc: 0.8930, val loss: 0.3446, val acc: 0.9238  (best train acc: 0.8970, best val acc: 0.9342, best train loss: 0.2788)\n",
      "[Epoch: 10880] train loss: 0.2800, train acc: 0.8960, val loss: 0.3214, val acc: 0.9305  (best train acc: 0.9007, best val acc: 0.9342, best train loss: 0.2686)\n",
      "[Epoch: 10900] train loss: 0.2788, train acc: 0.8978, val loss: 0.3396, val acc: 0.9251  (best train acc: 0.9007, best val acc: 0.9342, best train loss: 0.2686)\n",
      "[Epoch: 10920] train loss: 0.2933, train acc: 0.8859, val loss: 0.3271, val acc: 0.9305  (best train acc: 0.9007, best val acc: 0.9342, best train loss: 0.2686)\n",
      "[Epoch: 10940] train loss: 0.2770, train acc: 0.8950, val loss: 0.3378, val acc: 0.9261  (best train acc: 0.9007, best val acc: 0.9342, best train loss: 0.2679)\n",
      "[Epoch: 10960] train loss: 0.2789, train acc: 0.8942, val loss: 0.3235, val acc: 0.9288  (best train acc: 0.9017, best val acc: 0.9342, best train loss: 0.2679)\n",
      "[Epoch: 10980] train loss: 0.2691, train acc: 0.8982, val loss: 0.3175, val acc: 0.9319  (best train acc: 0.9021, best val acc: 0.9342, best train loss: 0.2659)\n",
      "[Epoch: 11000] train loss: 0.2793, train acc: 0.8985, val loss: 0.3286, val acc: 0.9285  (best train acc: 0.9021, best val acc: 0.9342, best train loss: 0.2653)\n",
      "[Epoch: 11020] train loss: 0.3089, train acc: 0.8876, val loss: 0.3658, val acc: 0.9049  (best train acc: 0.9021, best val acc: 0.9342, best train loss: 0.2653)\n",
      "[Epoch: 11040] train loss: 0.2956, train acc: 0.8861, val loss: 0.3239, val acc: 0.9298  (best train acc: 0.9021, best val acc: 0.9342, best train loss: 0.2653)\n",
      "[Epoch: 11060] train loss: 0.2754, train acc: 0.8967, val loss: 0.3240, val acc: 0.9268  (best train acc: 0.9021, best val acc: 0.9349, best train loss: 0.2653)\n",
      "[Epoch: 11080] train loss: 0.2674, train acc: 0.9000, val loss: 0.3318, val acc: 0.9278  (best train acc: 0.9036, best val acc: 0.9349, best train loss: 0.2653)\n",
      "[Epoch: 11100] train loss: 0.3001, train acc: 0.8882, val loss: 0.3129, val acc: 0.9285  (best train acc: 0.9037, best val acc: 0.9349, best train loss: 0.2646)\n",
      "[Epoch: 11120] train loss: 0.2774, train acc: 0.8931, val loss: 0.3237, val acc: 0.9285  (best train acc: 0.9037, best val acc: 0.9349, best train loss: 0.2630)\n",
      "[Epoch: 11140] train loss: 0.2724, train acc: 0.8986, val loss: 0.3257, val acc: 0.9282  (best train acc: 0.9037, best val acc: 0.9349, best train loss: 0.2630)\n",
      "[Epoch: 11160] train loss: 0.2935, train acc: 0.8825, val loss: 0.3261, val acc: 0.9187  (best train acc: 0.9037, best val acc: 0.9349, best train loss: 0.2630)\n",
      "[Epoch: 11180] train loss: 0.2928, train acc: 0.8924, val loss: 0.3248, val acc: 0.9268  (best train acc: 0.9037, best val acc: 0.9349, best train loss: 0.2630)\n",
      "[Epoch: 11200] train loss: 0.2835, train acc: 0.8918, val loss: 0.3204, val acc: 0.9319  (best train acc: 0.9037, best val acc: 0.9349, best train loss: 0.2630)\n",
      "[Epoch: 11220] train loss: 0.2747, train acc: 0.8989, val loss: 0.3195, val acc: 0.9302  (best train acc: 0.9037, best val acc: 0.9349, best train loss: 0.2630)\n",
      "[Epoch: 11240] train loss: 0.3044, train acc: 0.8874, val loss: 0.3016, val acc: 0.9278  (best train acc: 0.9037, best val acc: 0.9349, best train loss: 0.2630)\n",
      "[Epoch: 11260] train loss: 0.3146, train acc: 0.8789, val loss: 0.3202, val acc: 0.9251  (best train acc: 0.9037, best val acc: 0.9349, best train loss: 0.2630)\n",
      "[Epoch: 11280] train loss: 0.2818, train acc: 0.8935, val loss: 0.3214, val acc: 0.9322  (best train acc: 0.9037, best val acc: 0.9349, best train loss: 0.2630)\n",
      "[Epoch: 11300] train loss: 0.2848, train acc: 0.8942, val loss: 0.3340, val acc: 0.9302  (best train acc: 0.9037, best val acc: 0.9349, best train loss: 0.2630)\n",
      "[Epoch: 11320] train loss: 0.2772, train acc: 0.8932, val loss: 0.3168, val acc: 0.9342  (best train acc: 0.9037, best val acc: 0.9349, best train loss: 0.2630)\n",
      "[Epoch: 11340] train loss: 0.2664, train acc: 0.8999, val loss: 0.3136, val acc: 0.9319  (best train acc: 0.9037, best val acc: 0.9349, best train loss: 0.2630)\n",
      "[Epoch: 11360] train loss: 0.2750, train acc: 0.8936, val loss: 0.3269, val acc: 0.9329  (best train acc: 0.9040, best val acc: 0.9349, best train loss: 0.2622)\n",
      "[Epoch: 11380] train loss: 0.3164, train acc: 0.8736, val loss: 0.3400, val acc: 0.9224  (best train acc: 0.9040, best val acc: 0.9349, best train loss: 0.2622)\n",
      "[Epoch: 11400] train loss: 0.2880, train acc: 0.8900, val loss: 0.3046, val acc: 0.9302  (best train acc: 0.9040, best val acc: 0.9349, best train loss: 0.2622)\n",
      "[Epoch: 11420] train loss: 0.2696, train acc: 0.8987, val loss: 0.3058, val acc: 0.9336  (best train acc: 0.9040, best val acc: 0.9349, best train loss: 0.2622)\n",
      "[Epoch: 11440] train loss: 0.2815, train acc: 0.8960, val loss: 0.3190, val acc: 0.9305  (best train acc: 0.9040, best val acc: 0.9349, best train loss: 0.2622)\n",
      "[Epoch: 11460] train loss: 0.2732, train acc: 0.8940, val loss: 0.3203, val acc: 0.9322  (best train acc: 0.9040, best val acc: 0.9356, best train loss: 0.2622)\n",
      "[Epoch: 11480] train loss: 0.2727, train acc: 0.8962, val loss: 0.3222, val acc: 0.9302  (best train acc: 0.9051, best val acc: 0.9356, best train loss: 0.2586)\n",
      "[Epoch: 11500] train loss: 0.2796, train acc: 0.8945, val loss: 0.3165, val acc: 0.9278  (best train acc: 0.9051, best val acc: 0.9356, best train loss: 0.2586)\n",
      "[Epoch: 11520] train loss: 0.2776, train acc: 0.8972, val loss: 0.3147, val acc: 0.9282  (best train acc: 0.9051, best val acc: 0.9356, best train loss: 0.2586)\n",
      "[Epoch: 11540] train loss: 0.2750, train acc: 0.8991, val loss: 0.3194, val acc: 0.9305  (best train acc: 0.9051, best val acc: 0.9356, best train loss: 0.2586)\n",
      "[Epoch: 11560] train loss: 0.2702, train acc: 0.8951, val loss: 0.3166, val acc: 0.9329  (best train acc: 0.9051, best val acc: 0.9356, best train loss: 0.2586)\n",
      "[Epoch: 11580] train loss: 0.2834, train acc: 0.8916, val loss: 0.3166, val acc: 0.9302  (best train acc: 0.9054, best val acc: 0.9363, best train loss: 0.2586)\n",
      "[Epoch: 11600] train loss: 0.2692, train acc: 0.8976, val loss: 0.3240, val acc: 0.9332  (best train acc: 0.9059, best val acc: 0.9363, best train loss: 0.2586)\n",
      "[Epoch: 11620] train loss: 0.2866, train acc: 0.8992, val loss: 0.3442, val acc: 0.9157  (best train acc: 0.9059, best val acc: 0.9363, best train loss: 0.2586)\n",
      "[Epoch: 11640] train loss: 0.3012, train acc: 0.8883, val loss: 0.3211, val acc: 0.9231  (best train acc: 0.9059, best val acc: 0.9363, best train loss: 0.2586)\n",
      "[Epoch: 11660] train loss: 0.2881, train acc: 0.8921, val loss: 0.3289, val acc: 0.9184  (best train acc: 0.9059, best val acc: 0.9363, best train loss: 0.2586)\n",
      "[Epoch: 11680] train loss: 0.2745, train acc: 0.8961, val loss: 0.2995, val acc: 0.9295  (best train acc: 0.9059, best val acc: 0.9363, best train loss: 0.2586)\n",
      "[Epoch: 11700] train loss: 0.2665, train acc: 0.9015, val loss: 0.3176, val acc: 0.9312  (best train acc: 0.9059, best val acc: 0.9363, best train loss: 0.2586)\n",
      "[Epoch: 11720] train loss: 0.2756, train acc: 0.8966, val loss: 0.3210, val acc: 0.9329  (best train acc: 0.9059, best val acc: 0.9363, best train loss: 0.2586)\n",
      "[Epoch: 11740] train loss: 0.2559, train acc: 0.9049, val loss: 0.3112, val acc: 0.9292  (best train acc: 0.9059, best val acc: 0.9363, best train loss: 0.2559)\n",
      "[Epoch: 11760] train loss: 0.2746, train acc: 0.8963, val loss: 0.3172, val acc: 0.9319  (best train acc: 0.9059, best val acc: 0.9363, best train loss: 0.2559)\n",
      "[Epoch: 11780] train loss: 0.2655, train acc: 0.9032, val loss: 0.3055, val acc: 0.9332  (best train acc: 0.9059, best val acc: 0.9363, best train loss: 0.2559)\n",
      "[Epoch: 11800] train loss: 0.2664, train acc: 0.8999, val loss: 0.3200, val acc: 0.9359  (best train acc: 0.9059, best val acc: 0.9363, best train loss: 0.2559)\n",
      "[Epoch: 11820] train loss: 0.2826, train acc: 0.8902, val loss: 0.3214, val acc: 0.9278  (best train acc: 0.9059, best val acc: 0.9363, best train loss: 0.2559)\n",
      "[Epoch: 11840] train loss: 0.2749, train acc: 0.8977, val loss: 0.2985, val acc: 0.9339  (best train acc: 0.9059, best val acc: 0.9363, best train loss: 0.2559)\n",
      "[Epoch: 11860] train loss: 0.2665, train acc: 0.9005, val loss: 0.3151, val acc: 0.9319  (best train acc: 0.9059, best val acc: 0.9363, best train loss: 0.2559)\n",
      "[Epoch: 11880] train loss: 0.2677, train acc: 0.8990, val loss: 0.2980, val acc: 0.9342  (best train acc: 0.9059, best val acc: 0.9363, best train loss: 0.2559)\n",
      "[Epoch: 11900] train loss: 0.2902, train acc: 0.8942, val loss: 0.3271, val acc: 0.9325  (best train acc: 0.9059, best val acc: 0.9363, best train loss: 0.2559)\n",
      "[Epoch: 11920] train loss: 0.2720, train acc: 0.8971, val loss: 0.3294, val acc: 0.9315  (best train acc: 0.9059, best val acc: 0.9369, best train loss: 0.2559)\n",
      "[Epoch: 11940] train loss: 0.2732, train acc: 0.8976, val loss: 0.3178, val acc: 0.9376  (best train acc: 0.9059, best val acc: 0.9376, best train loss: 0.2559)\n",
      "[Epoch: 11960] train loss: 0.2930, train acc: 0.8885, val loss: 0.3288, val acc: 0.9302  (best train acc: 0.9059, best val acc: 0.9376, best train loss: 0.2559)\n",
      "[Epoch: 11980] train loss: 0.2907, train acc: 0.8861, val loss: 0.3705, val acc: 0.9005  (best train acc: 0.9059, best val acc: 0.9376, best train loss: 0.2559)\n",
      "[Epoch: 12000] train loss: 0.2844, train acc: 0.8928, val loss: 0.3261, val acc: 0.9302  (best train acc: 0.9059, best val acc: 0.9376, best train loss: 0.2559)\n",
      "[Epoch: 12020] train loss: 0.2743, train acc: 0.8956, val loss: 0.3138, val acc: 0.9352  (best train acc: 0.9059, best val acc: 0.9393, best train loss: 0.2559)\n",
      "[Epoch: 12040] train loss: 0.2990, train acc: 0.8776, val loss: 0.3700, val acc: 0.8874  (best train acc: 0.9059, best val acc: 0.9393, best train loss: 0.2559)\n",
      "[Epoch: 12060] train loss: 0.2953, train acc: 0.8809, val loss: 0.3364, val acc: 0.9110  (best train acc: 0.9059, best val acc: 0.9393, best train loss: 0.2559)\n",
      "[Epoch: 12080] train loss: 0.2719, train acc: 0.9009, val loss: 0.3040, val acc: 0.9312  (best train acc: 0.9059, best val acc: 0.9393, best train loss: 0.2559)\n",
      "[Epoch: 12100] train loss: 0.2665, train acc: 0.8974, val loss: 0.3122, val acc: 0.9342  (best train acc: 0.9059, best val acc: 0.9393, best train loss: 0.2559)\n",
      "[Epoch: 12120] train loss: 0.2628, train acc: 0.9006, val loss: 0.3027, val acc: 0.9315  (best train acc: 0.9059, best val acc: 0.9393, best train loss: 0.2559)\n",
      "[Epoch: 12140] train loss: 0.2694, train acc: 0.8999, val loss: 0.3116, val acc: 0.9336  (best train acc: 0.9086, best val acc: 0.9393, best train loss: 0.2559)\n",
      "[Epoch: 12160] train loss: 0.2707, train acc: 0.8972, val loss: 0.3183, val acc: 0.9315  (best train acc: 0.9086, best val acc: 0.9393, best train loss: 0.2559)\n",
      "[Epoch: 12180] train loss: 0.2738, train acc: 0.8997, val loss: 0.3054, val acc: 0.9336  (best train acc: 0.9086, best val acc: 0.9393, best train loss: 0.2559)\n",
      "[Epoch: 12200] train loss: 0.2711, train acc: 0.8972, val loss: 0.3198, val acc: 0.9352  (best train acc: 0.9086, best val acc: 0.9393, best train loss: 0.2559)\n",
      "[Epoch: 12220] train loss: 0.2815, train acc: 0.8957, val loss: 0.3122, val acc: 0.9346  (best train acc: 0.9096, best val acc: 0.9393, best train loss: 0.2506)\n",
      "[Epoch: 12240] train loss: 0.2677, train acc: 0.8976, val loss: 0.3189, val acc: 0.9329  (best train acc: 0.9096, best val acc: 0.9393, best train loss: 0.2506)\n",
      "[Epoch: 12260] train loss: 0.2572, train acc: 0.9020, val loss: 0.3256, val acc: 0.9346  (best train acc: 0.9096, best val acc: 0.9393, best train loss: 0.2506)\n",
      "[Epoch: 12280] train loss: 0.2754, train acc: 0.8924, val loss: 0.3271, val acc: 0.9329  (best train acc: 0.9096, best val acc: 0.9393, best train loss: 0.2506)\n",
      "[Epoch: 12300] train loss: 0.2624, train acc: 0.9004, val loss: 0.3169, val acc: 0.9336  (best train acc: 0.9096, best val acc: 0.9393, best train loss: 0.2506)\n",
      "[Epoch: 12320] train loss: 0.2571, train acc: 0.9013, val loss: 0.3019, val acc: 0.9339  (best train acc: 0.9096, best val acc: 0.9393, best train loss: 0.2506)\n",
      "[Epoch: 12340] train loss: 0.2827, train acc: 0.8925, val loss: 0.3174, val acc: 0.9339  (best train acc: 0.9096, best val acc: 0.9393, best train loss: 0.2506)\n",
      "[Epoch: 12360] train loss: 0.2753, train acc: 0.8927, val loss: 0.3201, val acc: 0.9329  (best train acc: 0.9096, best val acc: 0.9393, best train loss: 0.2506)\n",
      "[Epoch: 12380] train loss: 0.2889, train acc: 0.8929, val loss: 0.3029, val acc: 0.9346  (best train acc: 0.9096, best val acc: 0.9393, best train loss: 0.2506)\n",
      "[Epoch: 12400] train loss: 0.2695, train acc: 0.8962, val loss: 0.3132, val acc: 0.9322  (best train acc: 0.9096, best val acc: 0.9393, best train loss: 0.2506)\n",
      "[Epoch: 12420] train loss: 0.2885, train acc: 0.8869, val loss: 0.3117, val acc: 0.9292  (best train acc: 0.9096, best val acc: 0.9393, best train loss: 0.2506)\n",
      "[Epoch: 12440] train loss: 0.2691, train acc: 0.8981, val loss: 0.3240, val acc: 0.9309  (best train acc: 0.9096, best val acc: 0.9393, best train loss: 0.2506)\n",
      "[Epoch: 12460] train loss: 0.2815, train acc: 0.8931, val loss: 0.3256, val acc: 0.9292  (best train acc: 0.9096, best val acc: 0.9393, best train loss: 0.2506)\n",
      "[Epoch: 12480] train loss: 0.2681, train acc: 0.8995, val loss: 0.3197, val acc: 0.9315  (best train acc: 0.9096, best val acc: 0.9393, best train loss: 0.2506)\n",
      "[Epoch: 12500] train loss: 0.2566, train acc: 0.9048, val loss: 0.3233, val acc: 0.9329  (best train acc: 0.9096, best val acc: 0.9393, best train loss: 0.2506)\n",
      "[Epoch: 12520] train loss: 0.2788, train acc: 0.8938, val loss: 0.3095, val acc: 0.9319  (best train acc: 0.9096, best val acc: 0.9393, best train loss: 0.2506)\n",
      "[Epoch: 12540] train loss: 0.2664, train acc: 0.9010, val loss: 0.3071, val acc: 0.9332  (best train acc: 0.9096, best val acc: 0.9393, best train loss: 0.2506)\n",
      "[Epoch: 12560] train loss: 0.2781, train acc: 0.8893, val loss: 0.3383, val acc: 0.9174  (best train acc: 0.9096, best val acc: 0.9393, best train loss: 0.2506)\n",
      "[Epoch: 12580] train loss: 0.2683, train acc: 0.8975, val loss: 0.3128, val acc: 0.9295  (best train acc: 0.9096, best val acc: 0.9393, best train loss: 0.2506)\n",
      "[Epoch: 12600] train loss: 0.2743, train acc: 0.9003, val loss: 0.3145, val acc: 0.9278  (best train acc: 0.9096, best val acc: 0.9393, best train loss: 0.2506)\n",
      "[Epoch: 12620] train loss: 0.2637, train acc: 0.9041, val loss: 0.3085, val acc: 0.9352  (best train acc: 0.9096, best val acc: 0.9393, best train loss: 0.2506)\n",
      "[Epoch: 12640] train loss: 0.2668, train acc: 0.9015, val loss: 0.2934, val acc: 0.9359  (best train acc: 0.9096, best val acc: 0.9393, best train loss: 0.2506)\n",
      "[Epoch: 12660] train loss: 0.2700, train acc: 0.8978, val loss: 0.3175, val acc: 0.9322  (best train acc: 0.9096, best val acc: 0.9393, best train loss: 0.2506)\n",
      "[Epoch: 12680] train loss: 0.2874, train acc: 0.8925, val loss: 0.3141, val acc: 0.9319  (best train acc: 0.9096, best val acc: 0.9393, best train loss: 0.2506)\n",
      "[Epoch: 12700] train loss: 0.2581, train acc: 0.8998, val loss: 0.3049, val acc: 0.9349  (best train acc: 0.9096, best val acc: 0.9393, best train loss: 0.2506)\n",
      "[Epoch: 12720] train loss: 0.2859, train acc: 0.8898, val loss: 0.3239, val acc: 0.9329  (best train acc: 0.9096, best val acc: 0.9393, best train loss: 0.2506)\n",
      "[Epoch: 12740] train loss: 0.2607, train acc: 0.9033, val loss: 0.3112, val acc: 0.9346  (best train acc: 0.9096, best val acc: 0.9393, best train loss: 0.2506)\n",
      "[Epoch: 12760] train loss: 0.2690, train acc: 0.8964, val loss: 0.3103, val acc: 0.9319  (best train acc: 0.9096, best val acc: 0.9393, best train loss: 0.2506)\n",
      "[Epoch: 12780] train loss: 0.2826, train acc: 0.8951, val loss: 0.3032, val acc: 0.9312  (best train acc: 0.9096, best val acc: 0.9393, best train loss: 0.2506)\n",
      "[Epoch: 12800] train loss: 0.2746, train acc: 0.8929, val loss: 0.3402, val acc: 0.9177  (best train acc: 0.9096, best val acc: 0.9393, best train loss: 0.2506)\n",
      "[Epoch: 12820] train loss: 0.2904, train acc: 0.8850, val loss: 0.3154, val acc: 0.9336  (best train acc: 0.9096, best val acc: 0.9393, best train loss: 0.2506)\n",
      "[Epoch: 12840] train loss: 0.2745, train acc: 0.8956, val loss: 0.3140, val acc: 0.9339  (best train acc: 0.9096, best val acc: 0.9393, best train loss: 0.2506)\n",
      "[Epoch: 12860] train loss: 0.2726, train acc: 0.8929, val loss: 0.3145, val acc: 0.9272  (best train acc: 0.9096, best val acc: 0.9393, best train loss: 0.2506)\n",
      "[Epoch: 12880] train loss: 0.2624, train acc: 0.9003, val loss: 0.3007, val acc: 0.9302  (best train acc: 0.9096, best val acc: 0.9393, best train loss: 0.2506)\n",
      "[Epoch: 12900] train loss: 0.2574, train acc: 0.9033, val loss: 0.3019, val acc: 0.9349  (best train acc: 0.9096, best val acc: 0.9393, best train loss: 0.2506)\n",
      "[Epoch: 12920] train loss: 0.2659, train acc: 0.8987, val loss: 0.3107, val acc: 0.9349  (best train acc: 0.9096, best val acc: 0.9393, best train loss: 0.2506)\n",
      "[Epoch: 12940] train loss: 0.2663, train acc: 0.9007, val loss: 0.3194, val acc: 0.9339  (best train acc: 0.9096, best val acc: 0.9393, best train loss: 0.2506)\n",
      "[Epoch: 12960] train loss: 0.2699, train acc: 0.8964, val loss: 0.3319, val acc: 0.9309  (best train acc: 0.9101, best val acc: 0.9393, best train loss: 0.2506)\n",
      "[Epoch: 12980] train loss: 0.2614, train acc: 0.9019, val loss: 0.3263, val acc: 0.9356  (best train acc: 0.9101, best val acc: 0.9393, best train loss: 0.2506)\n",
      "[Epoch: 13000] train loss: 0.2629, train acc: 0.8976, val loss: 0.3267, val acc: 0.9332  (best train acc: 0.9101, best val acc: 0.9393, best train loss: 0.2506)\n",
      "[Epoch: 13020] train loss: 0.2681, train acc: 0.9001, val loss: 0.3297, val acc: 0.9339  (best train acc: 0.9101, best val acc: 0.9393, best train loss: 0.2506)\n",
      "[Epoch: 13040] train loss: 0.2658, train acc: 0.9030, val loss: 0.3131, val acc: 0.9315  (best train acc: 0.9101, best val acc: 0.9393, best train loss: 0.2506)\n",
      "[Epoch: 13060] train loss: 0.2595, train acc: 0.9040, val loss: 0.3176, val acc: 0.9342  (best train acc: 0.9101, best val acc: 0.9393, best train loss: 0.2506)\n",
      "[Epoch: 13080] train loss: 0.2620, train acc: 0.9030, val loss: 0.3198, val acc: 0.9325  (best train acc: 0.9101, best val acc: 0.9393, best train loss: 0.2506)\n",
      "[Epoch: 13100] train loss: 0.2727, train acc: 0.8940, val loss: 0.3093, val acc: 0.9366  (best train acc: 0.9101, best val acc: 0.9393, best train loss: 0.2506)\n",
      "[Epoch: 13120] train loss: 0.2692, train acc: 0.8995, val loss: 0.3138, val acc: 0.9305  (best train acc: 0.9101, best val acc: 0.9393, best train loss: 0.2506)\n",
      "[Epoch: 13140] train loss: 0.2708, train acc: 0.8973, val loss: 0.3196, val acc: 0.9309  (best train acc: 0.9101, best val acc: 0.9393, best train loss: 0.2506)\n",
      "[Epoch: 13160] train loss: 0.2589, train acc: 0.9053, val loss: 0.3208, val acc: 0.9373  (best train acc: 0.9104, best val acc: 0.9393, best train loss: 0.2506)\n",
      "[Epoch: 13180] train loss: 0.3102, train acc: 0.8897, val loss: 0.3488, val acc: 0.9029  (best train acc: 0.9104, best val acc: 0.9393, best train loss: 0.2506)\n",
      "[Epoch: 13200] train loss: 0.2752, train acc: 0.8933, val loss: 0.2982, val acc: 0.9298  (best train acc: 0.9104, best val acc: 0.9393, best train loss: 0.2506)\n",
      "[Epoch: 13220] train loss: 0.2559, train acc: 0.9055, val loss: 0.3157, val acc: 0.9359  (best train acc: 0.9104, best val acc: 0.9393, best train loss: 0.2464)\n",
      "[Epoch: 13240] train loss: 0.2560, train acc: 0.9042, val loss: 0.3059, val acc: 0.9312  (best train acc: 0.9104, best val acc: 0.9393, best train loss: 0.2464)\n",
      "[Epoch: 13260] train loss: 0.2647, train acc: 0.9017, val loss: 0.3269, val acc: 0.9319  (best train acc: 0.9104, best val acc: 0.9393, best train loss: 0.2464)\n",
      "[Epoch: 13280] train loss: 0.2742, train acc: 0.8993, val loss: 0.3359, val acc: 0.9282  (best train acc: 0.9104, best val acc: 0.9393, best train loss: 0.2464)\n",
      "[Epoch: 13300] train loss: 0.2592, train acc: 0.9030, val loss: 0.3378, val acc: 0.9312  (best train acc: 0.9104, best val acc: 0.9393, best train loss: 0.2464)\n",
      "[Epoch: 13320] train loss: 0.2664, train acc: 0.8970, val loss: 0.3255, val acc: 0.9342  (best train acc: 0.9104, best val acc: 0.9393, best train loss: 0.2458)\n",
      "[Epoch: 13340] train loss: 0.2651, train acc: 0.8991, val loss: 0.3298, val acc: 0.9255  (best train acc: 0.9104, best val acc: 0.9393, best train loss: 0.2458)\n",
      "[Epoch: 13360] train loss: 0.2550, train acc: 0.9040, val loss: 0.3160, val acc: 0.9312  (best train acc: 0.9104, best val acc: 0.9393, best train loss: 0.2458)\n",
      "[Epoch: 13380] train loss: 0.2635, train acc: 0.8984, val loss: 0.3327, val acc: 0.9352  (best train acc: 0.9104, best val acc: 0.9393, best train loss: 0.2458)\n",
      "[Epoch: 13400] train loss: 0.3111, train acc: 0.8753, val loss: 0.3391, val acc: 0.9197  (best train acc: 0.9104, best val acc: 0.9393, best train loss: 0.2458)\n",
      "[Epoch: 13420] train loss: 0.2731, train acc: 0.8914, val loss: 0.3313, val acc: 0.9332  (best train acc: 0.9104, best val acc: 0.9393, best train loss: 0.2458)\n",
      "[Epoch: 13440] train loss: 0.2751, train acc: 0.8950, val loss: 0.3236, val acc: 0.9363  (best train acc: 0.9104, best val acc: 0.9393, best train loss: 0.2458)\n",
      "[Epoch: 13460] train loss: 0.2538, train acc: 0.9058, val loss: 0.3253, val acc: 0.9346  (best train acc: 0.9104, best val acc: 0.9393, best train loss: 0.2458)\n",
      "[Epoch: 13480] train loss: 0.2710, train acc: 0.8986, val loss: 0.3218, val acc: 0.9325  (best train acc: 0.9104, best val acc: 0.9393, best train loss: 0.2458)\n",
      "[Epoch: 13500] train loss: 0.2911, train acc: 0.8848, val loss: 0.2995, val acc: 0.9319  (best train acc: 0.9104, best val acc: 0.9393, best train loss: 0.2458)\n",
      "[Epoch: 13520] train loss: 0.2532, train acc: 0.9053, val loss: 0.3429, val acc: 0.9309  (best train acc: 0.9104, best val acc: 0.9393, best train loss: 0.2458)\n",
      "[Epoch: 13540] train loss: 0.2574, train acc: 0.8997, val loss: 0.3175, val acc: 0.9349  (best train acc: 0.9104, best val acc: 0.9393, best train loss: 0.2371)\n",
      "[Epoch: 13560] train loss: 0.2471, train acc: 0.9059, val loss: 0.3206, val acc: 0.9359  (best train acc: 0.9149, best val acc: 0.9393, best train loss: 0.2338)\n",
      "[Epoch: 13580] train loss: 0.2577, train acc: 0.9084, val loss: 0.3218, val acc: 0.9268  (best train acc: 0.9149, best val acc: 0.9393, best train loss: 0.2338)\n",
      "[Epoch: 13600] train loss: 0.2452, train acc: 0.9114, val loss: 0.3205, val acc: 0.9339  (best train acc: 0.9149, best val acc: 0.9393, best train loss: 0.2338)\n",
      "[Epoch: 13620] train loss: 0.2574, train acc: 0.9023, val loss: 0.3233, val acc: 0.9312  (best train acc: 0.9149, best val acc: 0.9393, best train loss: 0.2338)\n",
      "[Epoch: 13640] train loss: 0.2514, train acc: 0.9001, val loss: 0.3229, val acc: 0.9288  (best train acc: 0.9149, best val acc: 0.9393, best train loss: 0.2338)\n",
      "[Epoch: 13660] train loss: 0.2347, train acc: 0.9117, val loss: 0.3289, val acc: 0.9352  (best train acc: 0.9149, best val acc: 0.9393, best train loss: 0.2338)\n",
      "[Epoch: 13680] train loss: 0.2522, train acc: 0.9020, val loss: 0.3289, val acc: 0.9349  (best train acc: 0.9149, best val acc: 0.9393, best train loss: 0.2338)\n",
      "[Epoch: 13700] train loss: 0.2335, train acc: 0.9092, val loss: 0.3298, val acc: 0.9332  (best train acc: 0.9149, best val acc: 0.9393, best train loss: 0.2335)\n",
      "[Epoch: 13720] train loss: 0.2368, train acc: 0.9086, val loss: 0.3280, val acc: 0.9265  (best train acc: 0.9149, best val acc: 0.9393, best train loss: 0.2335)\n",
      "[Epoch: 13740] train loss: 0.2422, train acc: 0.9065, val loss: 0.3140, val acc: 0.9400  (best train acc: 0.9149, best val acc: 0.9400, best train loss: 0.2335)\n",
      "[Epoch: 13760] train loss: 0.2474, train acc: 0.9043, val loss: 0.3333, val acc: 0.9329  (best train acc: 0.9149, best val acc: 0.9400, best train loss: 0.2317)\n",
      "[Epoch: 13780] train loss: 0.2360, train acc: 0.9127, val loss: 0.3211, val acc: 0.9346  (best train acc: 0.9149, best val acc: 0.9400, best train loss: 0.2317)\n",
      "[Epoch: 13800] train loss: 0.2577, train acc: 0.9051, val loss: 0.3255, val acc: 0.9373  (best train acc: 0.9149, best val acc: 0.9400, best train loss: 0.2317)\n",
      "[Epoch: 13820] train loss: 0.2473, train acc: 0.9054, val loss: 0.3238, val acc: 0.9359  (best train acc: 0.9175, best val acc: 0.9400, best train loss: 0.2294)\n",
      "[Epoch: 13840] train loss: 0.2394, train acc: 0.9130, val loss: 0.3232, val acc: 0.9329  (best train acc: 0.9175, best val acc: 0.9400, best train loss: 0.2294)\n",
      "[Epoch: 13860] train loss: 0.2598, train acc: 0.9040, val loss: 0.3139, val acc: 0.9366  (best train acc: 0.9182, best val acc: 0.9400, best train loss: 0.2294)\n",
      "[Epoch: 13880] train loss: 0.2534, train acc: 0.9079, val loss: 0.3162, val acc: 0.9336  (best train acc: 0.9182, best val acc: 0.9400, best train loss: 0.2287)\n",
      "[Epoch: 13900] train loss: 0.2514, train acc: 0.9097, val loss: 0.3122, val acc: 0.9336  (best train acc: 0.9182, best val acc: 0.9400, best train loss: 0.2287)\n",
      "[Epoch: 13920] train loss: 0.2415, train acc: 0.9108, val loss: 0.3157, val acc: 0.9332  (best train acc: 0.9182, best val acc: 0.9400, best train loss: 0.2287)\n",
      "[Epoch: 13940] train loss: 0.2526, train acc: 0.9082, val loss: 0.3212, val acc: 0.9339  (best train acc: 0.9187, best val acc: 0.9400, best train loss: 0.2287)\n",
      "[Epoch: 13960] train loss: 0.2382, train acc: 0.9152, val loss: 0.3104, val acc: 0.9339  (best train acc: 0.9187, best val acc: 0.9400, best train loss: 0.2287)\n",
      "[Epoch: 13980] train loss: 0.2494, train acc: 0.9081, val loss: 0.3136, val acc: 0.9359  (best train acc: 0.9245, best val acc: 0.9400, best train loss: 0.2245)\n",
      "[Epoch: 14000] train loss: 0.2360, train acc: 0.9169, val loss: 0.3197, val acc: 0.9339  (best train acc: 0.9245, best val acc: 0.9400, best train loss: 0.2245)\n",
      "[Epoch: 14020] train loss: 0.2547, train acc: 0.9127, val loss: 0.2868, val acc: 0.9373  (best train acc: 0.9245, best val acc: 0.9400, best train loss: 0.2245)\n",
      "[Epoch: 14040] train loss: 0.2693, train acc: 0.9034, val loss: 0.2848, val acc: 0.9356  (best train acc: 0.9245, best val acc: 0.9400, best train loss: 0.2245)\n",
      "[Epoch: 14060] train loss: 0.2385, train acc: 0.9145, val loss: 0.2973, val acc: 0.9363  (best train acc: 0.9245, best val acc: 0.9403, best train loss: 0.2245)\n",
      "[Epoch: 14080] train loss: 0.2424, train acc: 0.9093, val loss: 0.3113, val acc: 0.9315  (best train acc: 0.9245, best val acc: 0.9403, best train loss: 0.2245)\n",
      "[Epoch: 14100] train loss: 0.2458, train acc: 0.9058, val loss: 0.3114, val acc: 0.9319  (best train acc: 0.9245, best val acc: 0.9403, best train loss: 0.2245)\n",
      "[Epoch: 14120] train loss: 0.2323, train acc: 0.9165, val loss: 0.3140, val acc: 0.9390  (best train acc: 0.9245, best val acc: 0.9403, best train loss: 0.2245)\n",
      "[Epoch: 14140] train loss: 0.2283, train acc: 0.9194, val loss: 0.3085, val acc: 0.9342  (best train acc: 0.9245, best val acc: 0.9403, best train loss: 0.2230)\n",
      "[Epoch: 14160] train loss: 0.2522, train acc: 0.9050, val loss: 0.3071, val acc: 0.9369  (best train acc: 0.9245, best val acc: 0.9403, best train loss: 0.2202)\n",
      "[Epoch: 14180] train loss: 0.2380, train acc: 0.9130, val loss: 0.3115, val acc: 0.9342  (best train acc: 0.9245, best val acc: 0.9403, best train loss: 0.2202)\n",
      "[Epoch: 14200] train loss: 0.2327, train acc: 0.9156, val loss: 0.3186, val acc: 0.9356  (best train acc: 0.9245, best val acc: 0.9403, best train loss: 0.2202)\n",
      "[Epoch: 14220] train loss: 0.2449, train acc: 0.9132, val loss: 0.3129, val acc: 0.9325  (best train acc: 0.9245, best val acc: 0.9403, best train loss: 0.2202)\n",
      "[Epoch: 14240] train loss: 0.2672, train acc: 0.9034, val loss: 0.3526, val acc: 0.9265  (best train acc: 0.9245, best val acc: 0.9403, best train loss: 0.2202)\n",
      "[Epoch: 14260] train loss: 0.2532, train acc: 0.9054, val loss: 0.2942, val acc: 0.9369  (best train acc: 0.9245, best val acc: 0.9403, best train loss: 0.2202)\n",
      "[Epoch: 14280] train loss: 0.2272, train acc: 0.9224, val loss: 0.3232, val acc: 0.9332  (best train acc: 0.9245, best val acc: 0.9413, best train loss: 0.2202)\n",
      "[Epoch: 14300] train loss: 0.2366, train acc: 0.9169, val loss: 0.3304, val acc: 0.9339  (best train acc: 0.9245, best val acc: 0.9413, best train loss: 0.2202)\n",
      "[Epoch: 14320] train loss: 0.2335, train acc: 0.9162, val loss: 0.3135, val acc: 0.9379  (best train acc: 0.9245, best val acc: 0.9413, best train loss: 0.2202)\n",
      "[Epoch: 14340] train loss: 0.2189, train acc: 0.9250, val loss: 0.3202, val acc: 0.9356  (best train acc: 0.9250, best val acc: 0.9413, best train loss: 0.2189)\n",
      "[Epoch: 14360] train loss: 0.2405, train acc: 0.9135, val loss: 0.3298, val acc: 0.9315  (best train acc: 0.9250, best val acc: 0.9413, best train loss: 0.2189)\n",
      "[Epoch: 14380] train loss: 0.2328, train acc: 0.9124, val loss: 0.3206, val acc: 0.9346  (best train acc: 0.9250, best val acc: 0.9413, best train loss: 0.2189)\n",
      "[Epoch: 14400] train loss: 0.2439, train acc: 0.9085, val loss: 0.3338, val acc: 0.9339  (best train acc: 0.9250, best val acc: 0.9413, best train loss: 0.2189)\n",
      "[Epoch: 14420] train loss: 0.2245, train acc: 0.9191, val loss: 0.3243, val acc: 0.9373  (best train acc: 0.9252, best val acc: 0.9413, best train loss: 0.2180)\n",
      "[Epoch: 14440] train loss: 0.2399, train acc: 0.9092, val loss: 0.3310, val acc: 0.9363  (best train acc: 0.9252, best val acc: 0.9413, best train loss: 0.2155)\n",
      "[Epoch: 14460] train loss: 0.2344, train acc: 0.9138, val loss: 0.3475, val acc: 0.9228  (best train acc: 0.9252, best val acc: 0.9413, best train loss: 0.2155)\n",
      "[Epoch: 14480] train loss: 0.2547, train acc: 0.9083, val loss: 0.3283, val acc: 0.9373  (best train acc: 0.9252, best val acc: 0.9413, best train loss: 0.2155)\n",
      "[Epoch: 14500] train loss: 0.2451, train acc: 0.9092, val loss: 0.3071, val acc: 0.9339  (best train acc: 0.9252, best val acc: 0.9413, best train loss: 0.2155)\n",
      "[Epoch: 14520] train loss: 0.2355, train acc: 0.9135, val loss: 0.3147, val acc: 0.9349  (best train acc: 0.9252, best val acc: 0.9413, best train loss: 0.2155)\n",
      "[Epoch: 14540] train loss: 0.2233, train acc: 0.9181, val loss: 0.3414, val acc: 0.9363  (best train acc: 0.9260, best val acc: 0.9413, best train loss: 0.2155)\n",
      "[Epoch: 14560] train loss: 0.2307, train acc: 0.9184, val loss: 0.3164, val acc: 0.9363  (best train acc: 0.9260, best val acc: 0.9413, best train loss: 0.2155)\n",
      "[Epoch: 14580] train loss: 0.2277, train acc: 0.9196, val loss: 0.3511, val acc: 0.9275  (best train acc: 0.9260, best val acc: 0.9413, best train loss: 0.2155)\n",
      "[Epoch: 14600] train loss: 0.2381, train acc: 0.9140, val loss: 0.3193, val acc: 0.9349  (best train acc: 0.9260, best val acc: 0.9413, best train loss: 0.2155)\n",
      "[Epoch: 14620] train loss: 0.2327, train acc: 0.9167, val loss: 0.3313, val acc: 0.9390  (best train acc: 0.9260, best val acc: 0.9413, best train loss: 0.2155)\n",
      "[Epoch: 14640] train loss: 0.2324, train acc: 0.9199, val loss: 0.3501, val acc: 0.9309  (best train acc: 0.9260, best val acc: 0.9413, best train loss: 0.2155)\n",
      "[Epoch: 14660] train loss: 0.2202, train acc: 0.9220, val loss: 0.3109, val acc: 0.9383  (best train acc: 0.9260, best val acc: 0.9413, best train loss: 0.2155)\n",
      "[Epoch: 14680] train loss: 0.2355, train acc: 0.9155, val loss: 0.3061, val acc: 0.9366  (best train acc: 0.9260, best val acc: 0.9413, best train loss: 0.2155)\n",
      "[Epoch: 14700] train loss: 0.2734, train acc: 0.9031, val loss: 0.3254, val acc: 0.9204  (best train acc: 0.9260, best val acc: 0.9413, best train loss: 0.2155)\n",
      "[Epoch: 14720] train loss: 0.2689, train acc: 0.9003, val loss: 0.2957, val acc: 0.9275  (best train acc: 0.9260, best val acc: 0.9413, best train loss: 0.2155)\n",
      "[Epoch: 14740] train loss: 0.2758, train acc: 0.8974, val loss: 0.3126, val acc: 0.9245  (best train acc: 0.9260, best val acc: 0.9413, best train loss: 0.2155)\n",
      "[Epoch: 14760] train loss: 0.2356, train acc: 0.9133, val loss: 0.3101, val acc: 0.9349  (best train acc: 0.9260, best val acc: 0.9413, best train loss: 0.2155)\n",
      "[Epoch: 14780] train loss: 0.2297, train acc: 0.9203, val loss: 0.3241, val acc: 0.9359  (best train acc: 0.9260, best val acc: 0.9413, best train loss: 0.2155)\n",
      "[Epoch: 14800] train loss: 0.2239, train acc: 0.9243, val loss: 0.3258, val acc: 0.9329  (best train acc: 0.9260, best val acc: 0.9413, best train loss: 0.2155)\n",
      "[Epoch: 14820] train loss: 0.2523, train acc: 0.9027, val loss: 0.3160, val acc: 0.9363  (best train acc: 0.9260, best val acc: 0.9413, best train loss: 0.2155)\n",
      "[Epoch: 14840] train loss: 0.2372, train acc: 0.9139, val loss: 0.3248, val acc: 0.9268  (best train acc: 0.9260, best val acc: 0.9413, best train loss: 0.2155)\n",
      "[Epoch: 14860] train loss: 0.2265, train acc: 0.9169, val loss: 0.2876, val acc: 0.9349  (best train acc: 0.9260, best val acc: 0.9413, best train loss: 0.2155)\n",
      "[Epoch: 14880] train loss: 0.2403, train acc: 0.9121, val loss: 0.3044, val acc: 0.9339  (best train acc: 0.9260, best val acc: 0.9413, best train loss: 0.2155)\n",
      "[Epoch: 14900] train loss: 0.2476, train acc: 0.9069, val loss: 0.3280, val acc: 0.9174  (best train acc: 0.9260, best val acc: 0.9413, best train loss: 0.2155)\n",
      "[Epoch: 14920] train loss: 0.2362, train acc: 0.9121, val loss: 0.3161, val acc: 0.9359  (best train acc: 0.9260, best val acc: 0.9413, best train loss: 0.2155)\n",
      "[Epoch: 14940] train loss: 0.2371, train acc: 0.9151, val loss: 0.3270, val acc: 0.9342  (best train acc: 0.9260, best val acc: 0.9413, best train loss: 0.2155)\n",
      "[Epoch: 14960] train loss: 0.2248, train acc: 0.9206, val loss: 0.3270, val acc: 0.9363  (best train acc: 0.9260, best val acc: 0.9413, best train loss: 0.2155)\n",
      "[Epoch: 14980] train loss: 0.2237, train acc: 0.9230, val loss: 0.2893, val acc: 0.9373  (best train acc: 0.9260, best val acc: 0.9413, best train loss: 0.2155)\n",
      "[Epoch: 15000] train loss: 0.2364, train acc: 0.9114, val loss: 0.3309, val acc: 0.9379  (best train acc: 0.9284, best val acc: 0.9413, best train loss: 0.2127)\n",
      "[Epoch: 15020] train loss: 0.2351, train acc: 0.9114, val loss: 0.3326, val acc: 0.9349  (best train acc: 0.9284, best val acc: 0.9413, best train loss: 0.2127)\n",
      "[Epoch: 15040] train loss: 0.2662, train acc: 0.8984, val loss: 0.3331, val acc: 0.9309  (best train acc: 0.9284, best val acc: 0.9413, best train loss: 0.2127)\n",
      "[Epoch: 15060] train loss: 0.2973, train acc: 0.8928, val loss: 0.3327, val acc: 0.9332  (best train acc: 0.9284, best val acc: 0.9413, best train loss: 0.2127)\n",
      "[Epoch: 15080] train loss: 0.2213, train acc: 0.9199, val loss: 0.3132, val acc: 0.9336  (best train acc: 0.9284, best val acc: 0.9420, best train loss: 0.2127)\n",
      "[Epoch: 15100] train loss: 0.2385, train acc: 0.9160, val loss: 0.3199, val acc: 0.9376  (best train acc: 0.9284, best val acc: 0.9420, best train loss: 0.2114)\n",
      "[Epoch: 15120] train loss: 0.2298, train acc: 0.9151, val loss: 0.3349, val acc: 0.9363  (best train acc: 0.9284, best val acc: 0.9420, best train loss: 0.2114)\n",
      "[Epoch: 15140] train loss: 0.2347, train acc: 0.9151, val loss: 0.3256, val acc: 0.9396  (best train acc: 0.9284, best val acc: 0.9420, best train loss: 0.2114)\n",
      "[Epoch: 15160] train loss: 0.2221, train acc: 0.9228, val loss: 0.3359, val acc: 0.9386  (best train acc: 0.9284, best val acc: 0.9420, best train loss: 0.2114)\n",
      "[Epoch: 15180] train loss: 0.2365, train acc: 0.9155, val loss: 0.3483, val acc: 0.9346  (best train acc: 0.9284, best val acc: 0.9420, best train loss: 0.2114)\n",
      "[Epoch: 15200] train loss: 0.2333, train acc: 0.9136, val loss: 0.3177, val acc: 0.9376  (best train acc: 0.9284, best val acc: 0.9420, best train loss: 0.2114)\n",
      "[Epoch: 15220] train loss: 0.2220, train acc: 0.9200, val loss: 0.3200, val acc: 0.9393  (best train acc: 0.9284, best val acc: 0.9420, best train loss: 0.2114)\n",
      "[Epoch: 15240] train loss: 0.2281, train acc: 0.9202, val loss: 0.3425, val acc: 0.9393  (best train acc: 0.9284, best val acc: 0.9420, best train loss: 0.2114)\n",
      "[Epoch: 15260] train loss: 0.2376, train acc: 0.9143, val loss: 0.3298, val acc: 0.9396  (best train acc: 0.9284, best val acc: 0.9420, best train loss: 0.2114)\n",
      "[Epoch: 15280] train loss: 0.2475, train acc: 0.9065, val loss: 0.3503, val acc: 0.9305  (best train acc: 0.9284, best val acc: 0.9420, best train loss: 0.2114)\n",
      "[Epoch: 15300] train loss: 0.2539, train acc: 0.9147, val loss: 0.3013, val acc: 0.9346  (best train acc: 0.9284, best val acc: 0.9420, best train loss: 0.2114)\n",
      "[Epoch: 15320] train loss: 0.2321, train acc: 0.9122, val loss: 0.3207, val acc: 0.9373  (best train acc: 0.9284, best val acc: 0.9420, best train loss: 0.2114)\n",
      "[Epoch: 15340] train loss: 0.2279, train acc: 0.9197, val loss: 0.3224, val acc: 0.9386  (best train acc: 0.9284, best val acc: 0.9420, best train loss: 0.2114)\n",
      "[Epoch: 15360] train loss: 0.2232, train acc: 0.9211, val loss: 0.3366, val acc: 0.9329  (best train acc: 0.9284, best val acc: 0.9420, best train loss: 0.2114)\n",
      "[Epoch: 15380] train loss: 0.2371, train acc: 0.9109, val loss: 0.3804, val acc: 0.8998  (best train acc: 0.9284, best val acc: 0.9420, best train loss: 0.2114)\n",
      "[Epoch: 15400] train loss: 0.2342, train acc: 0.9192, val loss: 0.3206, val acc: 0.9342  (best train acc: 0.9284, best val acc: 0.9420, best train loss: 0.2114)\n",
      "[Epoch: 15420] train loss: 0.2411, train acc: 0.9122, val loss: 0.3183, val acc: 0.9390  (best train acc: 0.9284, best val acc: 0.9420, best train loss: 0.2114)\n",
      "[Epoch: 15440] train loss: 0.2268, train acc: 0.9169, val loss: 0.3322, val acc: 0.9363  (best train acc: 0.9284, best val acc: 0.9420, best train loss: 0.2114)\n",
      "[Epoch: 15460] train loss: 0.2449, train acc: 0.9163, val loss: 0.3019, val acc: 0.9400  (best train acc: 0.9284, best val acc: 0.9420, best train loss: 0.2114)\n",
      "[Epoch: 15480] train loss: 0.2210, train acc: 0.9203, val loss: 0.3264, val acc: 0.9379  (best train acc: 0.9284, best val acc: 0.9420, best train loss: 0.2114)\n",
      "[Epoch: 15500] train loss: 0.2211, train acc: 0.9208, val loss: 0.3193, val acc: 0.9403  (best train acc: 0.9284, best val acc: 0.9420, best train loss: 0.2062)\n",
      "[Epoch: 15520] train loss: 0.2163, train acc: 0.9229, val loss: 0.3321, val acc: 0.9376  (best train acc: 0.9284, best val acc: 0.9420, best train loss: 0.2062)\n",
      "[Epoch: 15540] train loss: 0.2243, train acc: 0.9177, val loss: 0.3527, val acc: 0.9288  (best train acc: 0.9289, best val acc: 0.9420, best train loss: 0.2052)\n",
      "[Epoch: 15560] train loss: 0.2106, train acc: 0.9245, val loss: 0.3366, val acc: 0.9342  (best train acc: 0.9289, best val acc: 0.9420, best train loss: 0.2052)\n",
      "[Epoch: 15580] train loss: 0.2040, train acc: 0.9291, val loss: 0.3297, val acc: 0.9383  (best train acc: 0.9299, best val acc: 0.9420, best train loss: 0.2010)\n",
      "[Epoch: 15600] train loss: 0.2065, train acc: 0.9284, val loss: 0.3151, val acc: 0.9390  (best train acc: 0.9299, best val acc: 0.9420, best train loss: 0.2010)\n",
      "[Epoch: 15620] train loss: 0.2130, train acc: 0.9229, val loss: 0.3161, val acc: 0.9356  (best train acc: 0.9299, best val acc: 0.9420, best train loss: 0.2006)\n",
      "[Epoch: 15640] train loss: 0.2201, train acc: 0.9206, val loss: 0.3218, val acc: 0.9265  (best train acc: 0.9317, best val acc: 0.9420, best train loss: 0.2006)\n",
      "[Epoch: 15660] train loss: 0.2094, train acc: 0.9244, val loss: 0.3162, val acc: 0.9352  (best train acc: 0.9317, best val acc: 0.9427, best train loss: 0.2006)\n",
      "[Epoch: 15680] train loss: 0.2248, train acc: 0.9153, val loss: 0.3513, val acc: 0.9315  (best train acc: 0.9317, best val acc: 0.9427, best train loss: 0.2006)\n",
      "[Epoch: 15700] train loss: 0.2092, train acc: 0.9236, val loss: 0.3357, val acc: 0.9376  (best train acc: 0.9317, best val acc: 0.9427, best train loss: 0.2006)\n",
      "[Epoch: 15720] train loss: 0.2394, train acc: 0.9147, val loss: 0.3266, val acc: 0.9356  (best train acc: 0.9317, best val acc: 0.9427, best train loss: 0.2006)\n",
      "[Epoch: 15740] train loss: 0.2110, train acc: 0.9253, val loss: 0.3128, val acc: 0.9352  (best train acc: 0.9317, best val acc: 0.9427, best train loss: 0.2006)\n",
      "[Epoch: 15760] train loss: 0.2143, train acc: 0.9221, val loss: 0.3356, val acc: 0.9268  (best train acc: 0.9317, best val acc: 0.9427, best train loss: 0.2006)\n",
      "[Epoch: 15780] train loss: 0.2136, train acc: 0.9236, val loss: 0.3170, val acc: 0.9275  (best train acc: 0.9317, best val acc: 0.9427, best train loss: 0.2006)\n",
      "[Epoch: 15800] train loss: 0.2134, train acc: 0.9266, val loss: 0.3397, val acc: 0.9312  (best train acc: 0.9317, best val acc: 0.9427, best train loss: 0.2006)\n",
      "[Epoch: 15820] train loss: 0.2092, train acc: 0.9248, val loss: 0.3181, val acc: 0.9390  (best train acc: 0.9317, best val acc: 0.9427, best train loss: 0.2006)\n",
      "[Epoch: 15840] train loss: 0.2174, train acc: 0.9209, val loss: 0.3250, val acc: 0.9356  (best train acc: 0.9319, best val acc: 0.9440, best train loss: 0.2006)\n",
      "[Epoch: 15860] train loss: 0.2165, train acc: 0.9219, val loss: 0.3277, val acc: 0.9366  (best train acc: 0.9319, best val acc: 0.9440, best train loss: 0.2006)\n",
      "[Epoch: 15880] train loss: 0.2130, train acc: 0.9188, val loss: 0.3459, val acc: 0.9312  (best train acc: 0.9319, best val acc: 0.9440, best train loss: 0.2006)\n",
      "[Epoch: 15900] train loss: 0.2145, train acc: 0.9226, val loss: 0.3187, val acc: 0.9386  (best train acc: 0.9319, best val acc: 0.9440, best train loss: 0.2006)\n",
      "[Epoch: 15920] train loss: 0.2259, train acc: 0.9166, val loss: 0.3598, val acc: 0.9221  (best train acc: 0.9319, best val acc: 0.9440, best train loss: 0.2006)\n",
      "[Epoch: 15940] train loss: 0.2137, train acc: 0.9216, val loss: 0.3312, val acc: 0.9390  (best train acc: 0.9319, best val acc: 0.9440, best train loss: 0.2006)\n",
      "[Epoch: 15960] train loss: 0.2274, train acc: 0.9150, val loss: 0.3383, val acc: 0.9386  (best train acc: 0.9319, best val acc: 0.9440, best train loss: 0.1979)\n",
      "[Epoch: 15980] train loss: 0.2065, train acc: 0.9261, val loss: 0.3409, val acc: 0.9332  (best train acc: 0.9319, best val acc: 0.9440, best train loss: 0.1979)\n",
      "[Epoch: 16000] train loss: 0.2440, train acc: 0.9127, val loss: 0.3298, val acc: 0.9390  (best train acc: 0.9319, best val acc: 0.9440, best train loss: 0.1979)\n",
      "[Epoch: 16020] train loss: 0.2078, train acc: 0.9250, val loss: 0.3384, val acc: 0.9359  (best train acc: 0.9319, best val acc: 0.9440, best train loss: 0.1979)\n",
      "[Epoch: 16040] train loss: 0.2073, train acc: 0.9290, val loss: 0.3533, val acc: 0.9363  (best train acc: 0.9319, best val acc: 0.9440, best train loss: 0.1975)\n",
      "[Epoch: 16060] train loss: 0.2163, train acc: 0.9241, val loss: 0.3351, val acc: 0.9386  (best train acc: 0.9319, best val acc: 0.9440, best train loss: 0.1975)\n",
      "[Epoch: 16080] train loss: 0.2023, train acc: 0.9308, val loss: 0.3360, val acc: 0.9417  (best train acc: 0.9319, best val acc: 0.9440, best train loss: 0.1975)\n",
      "[Epoch: 16100] train loss: 0.2077, train acc: 0.9280, val loss: 0.3414, val acc: 0.9390  (best train acc: 0.9319, best val acc: 0.9440, best train loss: 0.1975)\n",
      "[Epoch: 16120] train loss: 0.2230, train acc: 0.9190, val loss: 0.3376, val acc: 0.9383  (best train acc: 0.9319, best val acc: 0.9440, best train loss: 0.1950)\n",
      "[Epoch: 16140] train loss: 0.2210, train acc: 0.9192, val loss: 0.3552, val acc: 0.9329  (best train acc: 0.9319, best val acc: 0.9440, best train loss: 0.1950)\n",
      "[Epoch: 16160] train loss: 0.1994, train acc: 0.9306, val loss: 0.3189, val acc: 0.9386  (best train acc: 0.9319, best val acc: 0.9440, best train loss: 0.1950)\n",
      "[Epoch: 16180] train loss: 0.2051, train acc: 0.9268, val loss: 0.3379, val acc: 0.9406  (best train acc: 0.9319, best val acc: 0.9440, best train loss: 0.1950)\n",
      "[Epoch: 16200] train loss: 0.2193, train acc: 0.9229, val loss: 0.3430, val acc: 0.9352  (best train acc: 0.9319, best val acc: 0.9440, best train loss: 0.1950)\n",
      "[Epoch: 16220] train loss: 0.2246, train acc: 0.9156, val loss: 0.3432, val acc: 0.9356  (best train acc: 0.9319, best val acc: 0.9440, best train loss: 0.1950)\n",
      "[Epoch: 16240] train loss: 0.2049, train acc: 0.9295, val loss: 0.3441, val acc: 0.9366  (best train acc: 0.9319, best val acc: 0.9440, best train loss: 0.1950)\n",
      "[Epoch: 16260] train loss: 0.2213, train acc: 0.9182, val loss: 0.3622, val acc: 0.9295  (best train acc: 0.9319, best val acc: 0.9440, best train loss: 0.1950)\n",
      "[Epoch: 16280] train loss: 0.2448, train acc: 0.9109, val loss: 0.3528, val acc: 0.9245  (best train acc: 0.9319, best val acc: 0.9440, best train loss: 0.1950)\n",
      "[Epoch: 16300] train loss: 0.2497, train acc: 0.9064, val loss: 0.3150, val acc: 0.9302  (best train acc: 0.9319, best val acc: 0.9440, best train loss: 0.1950)\n",
      "[Epoch: 16320] train loss: 0.2345, train acc: 0.9106, val loss: 0.3458, val acc: 0.9386  (best train acc: 0.9319, best val acc: 0.9440, best train loss: 0.1950)\n",
      "[Epoch: 16340] train loss: 0.2091, train acc: 0.9273, val loss: 0.3340, val acc: 0.9396  (best train acc: 0.9319, best val acc: 0.9440, best train loss: 0.1950)\n",
      "[Epoch: 16360] train loss: 0.2128, train acc: 0.9226, val loss: 0.3340, val acc: 0.9400  (best train acc: 0.9344, best val acc: 0.9440, best train loss: 0.1950)\n",
      "[Epoch: 16380] train loss: 0.2060, train acc: 0.9263, val loss: 0.3407, val acc: 0.9379  (best train acc: 0.9344, best val acc: 0.9440, best train loss: 0.1922)\n",
      "[Epoch: 16400] train loss: 0.1970, train acc: 0.9328, val loss: 0.3460, val acc: 0.9373  (best train acc: 0.9344, best val acc: 0.9440, best train loss: 0.1922)\n",
      "[Epoch: 16420] train loss: 0.2047, train acc: 0.9269, val loss: 0.3470, val acc: 0.9292  (best train acc: 0.9344, best val acc: 0.9440, best train loss: 0.1922)\n",
      "[Epoch: 16440] train loss: 0.2195, train acc: 0.9199, val loss: 0.3432, val acc: 0.9420  (best train acc: 0.9344, best val acc: 0.9440, best train loss: 0.1922)\n",
      "[Epoch: 16460] train loss: 0.2097, train acc: 0.9250, val loss: 0.3494, val acc: 0.9396  (best train acc: 0.9344, best val acc: 0.9440, best train loss: 0.1922)\n",
      "[Epoch: 16480] train loss: 0.1941, train acc: 0.9310, val loss: 0.3500, val acc: 0.9383  (best train acc: 0.9344, best val acc: 0.9440, best train loss: 0.1922)\n",
      "[Epoch: 16500] train loss: 0.2034, train acc: 0.9264, val loss: 0.3163, val acc: 0.9366  (best train acc: 0.9344, best val acc: 0.9440, best train loss: 0.1922)\n",
      "[Epoch: 16520] train loss: 0.2029, train acc: 0.9287, val loss: 0.3510, val acc: 0.9329  (best train acc: 0.9344, best val acc: 0.9440, best train loss: 0.1922)\n",
      "[Epoch: 16540] train loss: 0.2390, train acc: 0.9125, val loss: 0.3689, val acc: 0.9231  (best train acc: 0.9344, best val acc: 0.9440, best train loss: 0.1922)\n",
      "[Epoch: 16560] train loss: 0.2094, train acc: 0.9243, val loss: 0.3388, val acc: 0.9342  (best train acc: 0.9344, best val acc: 0.9440, best train loss: 0.1922)\n",
      "[Epoch: 16580] train loss: 0.2130, train acc: 0.9218, val loss: 0.3465, val acc: 0.9376  (best train acc: 0.9344, best val acc: 0.9440, best train loss: 0.1922)\n",
      "[Epoch: 16600] train loss: 0.2172, train acc: 0.9187, val loss: 0.3399, val acc: 0.9410  (best train acc: 0.9344, best val acc: 0.9440, best train loss: 0.1922)\n",
      "[Epoch: 16620] train loss: 0.2030, train acc: 0.9276, val loss: 0.3356, val acc: 0.9379  (best train acc: 0.9344, best val acc: 0.9440, best train loss: 0.1922)\n",
      "[Epoch: 16640] train loss: 0.2085, train acc: 0.9224, val loss: 0.3435, val acc: 0.9393  (best train acc: 0.9344, best val acc: 0.9440, best train loss: 0.1922)\n",
      "[Epoch: 16660] train loss: 0.1970, train acc: 0.9306, val loss: 0.3329, val acc: 0.9342  (best train acc: 0.9344, best val acc: 0.9440, best train loss: 0.1922)\n",
      "[Epoch: 16680] train loss: 0.2084, train acc: 0.9258, val loss: 0.3434, val acc: 0.9390  (best train acc: 0.9344, best val acc: 0.9440, best train loss: 0.1922)\n",
      "[Epoch: 16700] train loss: 0.2229, train acc: 0.9190, val loss: 0.3375, val acc: 0.9305  (best train acc: 0.9344, best val acc: 0.9440, best train loss: 0.1922)\n",
      "[Epoch: 16720] train loss: 0.2079, train acc: 0.9265, val loss: 0.3425, val acc: 0.9349  (best train acc: 0.9344, best val acc: 0.9440, best train loss: 0.1922)\n",
      "[Epoch: 16740] train loss: 0.1971, train acc: 0.9308, val loss: 0.3433, val acc: 0.9369  (best train acc: 0.9344, best val acc: 0.9440, best train loss: 0.1922)\n",
      "[Epoch: 16760] train loss: 0.2017, train acc: 0.9317, val loss: 0.3454, val acc: 0.9406  (best train acc: 0.9344, best val acc: 0.9440, best train loss: 0.1922)\n",
      "[Epoch: 16780] train loss: 0.2039, train acc: 0.9276, val loss: 0.3600, val acc: 0.9349  (best train acc: 0.9344, best val acc: 0.9440, best train loss: 0.1922)\n",
      "[Epoch: 16800] train loss: 0.2102, train acc: 0.9266, val loss: 0.3373, val acc: 0.9376  (best train acc: 0.9344, best val acc: 0.9440, best train loss: 0.1922)\n",
      "[Epoch: 16820] train loss: 0.2067, train acc: 0.9260, val loss: 0.3303, val acc: 0.9420  (best train acc: 0.9344, best val acc: 0.9440, best train loss: 0.1922)\n",
      "[Epoch: 16840] train loss: 0.1994, train acc: 0.9298, val loss: 0.3281, val acc: 0.9403  (best train acc: 0.9346, best val acc: 0.9440, best train loss: 0.1910)\n",
      "[Epoch: 16860] train loss: 0.2346, train acc: 0.9151, val loss: 0.3257, val acc: 0.9363  (best train acc: 0.9346, best val acc: 0.9440, best train loss: 0.1910)\n",
      "[Epoch: 16880] train loss: 0.2075, train acc: 0.9280, val loss: 0.3465, val acc: 0.9369  (best train acc: 0.9346, best val acc: 0.9440, best train loss: 0.1910)\n",
      "[Epoch: 16900] train loss: 0.2144, train acc: 0.9242, val loss: 0.3519, val acc: 0.9346  (best train acc: 0.9346, best val acc: 0.9440, best train loss: 0.1910)\n",
      "[Epoch: 16920] train loss: 0.2291, train acc: 0.9176, val loss: 0.3335, val acc: 0.9383  (best train acc: 0.9346, best val acc: 0.9440, best train loss: 0.1910)\n",
      "[Epoch: 16940] train loss: 0.2261, train acc: 0.9174, val loss: 0.3235, val acc: 0.9356  (best train acc: 0.9346, best val acc: 0.9440, best train loss: 0.1910)\n",
      "[Epoch: 16960] train loss: 0.2198, train acc: 0.9170, val loss: 0.3298, val acc: 0.9319  (best train acc: 0.9346, best val acc: 0.9440, best train loss: 0.1910)\n",
      "[Epoch: 16980] train loss: 0.2135, train acc: 0.9223, val loss: 0.3099, val acc: 0.9396  (best train acc: 0.9346, best val acc: 0.9440, best train loss: 0.1910)\n",
      "[Epoch: 17000] train loss: 0.2121, train acc: 0.9250, val loss: 0.3306, val acc: 0.9400  (best train acc: 0.9346, best val acc: 0.9440, best train loss: 0.1910)\n",
      "[Epoch: 17020] train loss: 0.2137, train acc: 0.9224, val loss: 0.3259, val acc: 0.9396  (best train acc: 0.9346, best val acc: 0.9440, best train loss: 0.1910)\n",
      "[Epoch: 17040] train loss: 0.2139, train acc: 0.9235, val loss: 0.3344, val acc: 0.9373  (best train acc: 0.9346, best val acc: 0.9440, best train loss: 0.1910)\n",
      "[Epoch: 17060] train loss: 0.2103, train acc: 0.9221, val loss: 0.3392, val acc: 0.9413  (best train acc: 0.9346, best val acc: 0.9440, best train loss: 0.1910)\n",
      "[Epoch: 17080] train loss: 0.2029, train acc: 0.9268, val loss: 0.3190, val acc: 0.9400  (best train acc: 0.9346, best val acc: 0.9440, best train loss: 0.1910)\n",
      "[Epoch: 17100] train loss: 0.2015, train acc: 0.9284, val loss: 0.3443, val acc: 0.9369  (best train acc: 0.9346, best val acc: 0.9440, best train loss: 0.1910)\n",
      "[Epoch: 17120] train loss: 0.2144, train acc: 0.9260, val loss: 0.3234, val acc: 0.9346  (best train acc: 0.9346, best val acc: 0.9440, best train loss: 0.1910)\n",
      "[Epoch: 17140] train loss: 0.2150, train acc: 0.9255, val loss: 0.3261, val acc: 0.9359  (best train acc: 0.9346, best val acc: 0.9440, best train loss: 0.1910)\n",
      "[Epoch: 17160] train loss: 0.2239, train acc: 0.9187, val loss: 0.3467, val acc: 0.9309  (best train acc: 0.9346, best val acc: 0.9440, best train loss: 0.1910)\n",
      "[Epoch: 17180] train loss: 0.2176, train acc: 0.9226, val loss: 0.3393, val acc: 0.9383  (best train acc: 0.9346, best val acc: 0.9440, best train loss: 0.1910)\n",
      "[Epoch: 17200] train loss: 0.2155, train acc: 0.9223, val loss: 0.3538, val acc: 0.9379  (best train acc: 0.9346, best val acc: 0.9440, best train loss: 0.1908)\n",
      "[Epoch: 17220] train loss: 0.2303, train acc: 0.9137, val loss: 0.3279, val acc: 0.9406  (best train acc: 0.9346, best val acc: 0.9440, best train loss: 0.1908)\n",
      "[Epoch: 17240] train loss: 0.2087, train acc: 0.9220, val loss: 0.3216, val acc: 0.9339  (best train acc: 0.9346, best val acc: 0.9440, best train loss: 0.1908)\n",
      "[Epoch: 17260] train loss: 0.2064, train acc: 0.9272, val loss: 0.3450, val acc: 0.9302  (best train acc: 0.9346, best val acc: 0.9444, best train loss: 0.1908)\n",
      "[Epoch: 17280] train loss: 0.2241, train acc: 0.9143, val loss: 0.3228, val acc: 0.9373  (best train acc: 0.9346, best val acc: 0.9444, best train loss: 0.1908)\n",
      "[Epoch: 17300] train loss: 0.2018, train acc: 0.9296, val loss: 0.3096, val acc: 0.9396  (best train acc: 0.9346, best val acc: 0.9444, best train loss: 0.1908)\n",
      "[Epoch: 17320] train loss: 0.2132, train acc: 0.9234, val loss: 0.3314, val acc: 0.9406  (best train acc: 0.9346, best val acc: 0.9444, best train loss: 0.1908)\n",
      "[Epoch: 17340] train loss: 0.2267, train acc: 0.9127, val loss: 0.3362, val acc: 0.9373  (best train acc: 0.9346, best val acc: 0.9444, best train loss: 0.1908)\n",
      "[Epoch: 17360] train loss: 0.2171, train acc: 0.9202, val loss: 0.3496, val acc: 0.9352  (best train acc: 0.9346, best val acc: 0.9444, best train loss: 0.1908)\n",
      "[Epoch: 17380] train loss: 0.2000, train acc: 0.9307, val loss: 0.3426, val acc: 0.9400  (best train acc: 0.9346, best val acc: 0.9444, best train loss: 0.1908)\n",
      "[Epoch: 17400] train loss: 0.2186, train acc: 0.9134, val loss: 0.3487, val acc: 0.9298  (best train acc: 0.9346, best val acc: 0.9444, best train loss: 0.1908)\n",
      "[Epoch: 17420] train loss: 0.2107, train acc: 0.9215, val loss: 0.3471, val acc: 0.9369  (best train acc: 0.9346, best val acc: 0.9444, best train loss: 0.1908)\n",
      "[Epoch: 17440] train loss: 0.2132, train acc: 0.9215, val loss: 0.3368, val acc: 0.9406  (best train acc: 0.9346, best val acc: 0.9444, best train loss: 0.1908)\n",
      "[Epoch: 17460] train loss: 0.1990, train acc: 0.9293, val loss: 0.3347, val acc: 0.9410  (best train acc: 0.9346, best val acc: 0.9444, best train loss: 0.1908)\n",
      "[Epoch: 17480] train loss: 0.2144, train acc: 0.9212, val loss: 0.3365, val acc: 0.9255  (best train acc: 0.9346, best val acc: 0.9444, best train loss: 0.1908)\n",
      "[Epoch: 17500] train loss: 0.2148, train acc: 0.9270, val loss: 0.3727, val acc: 0.9342  (best train acc: 0.9346, best val acc: 0.9444, best train loss: 0.1908)\n",
      "[Epoch: 17520] train loss: 0.2128, train acc: 0.9213, val loss: 0.3431, val acc: 0.9346  (best train acc: 0.9346, best val acc: 0.9444, best train loss: 0.1908)\n",
      "[Epoch: 17540] train loss: 0.2136, train acc: 0.9247, val loss: 0.3527, val acc: 0.9349  (best train acc: 0.9346, best val acc: 0.9444, best train loss: 0.1908)\n",
      "[Epoch: 17560] train loss: 0.1965, train acc: 0.9307, val loss: 0.3482, val acc: 0.9413  (best train acc: 0.9346, best val acc: 0.9444, best train loss: 0.1908)\n",
      "[Epoch: 17580] train loss: 0.1974, train acc: 0.9294, val loss: 0.3462, val acc: 0.9396  (best train acc: 0.9346, best val acc: 0.9444, best train loss: 0.1872)\n",
      "[Epoch: 17600] train loss: 0.1984, train acc: 0.9289, val loss: 0.3553, val acc: 0.9359  (best train acc: 0.9346, best val acc: 0.9444, best train loss: 0.1872)\n",
      "[Epoch: 17620] train loss: 0.2121, train acc: 0.9215, val loss: 0.3529, val acc: 0.9379  (best train acc: 0.9346, best val acc: 0.9444, best train loss: 0.1872)\n",
      "[Epoch: 17640] train loss: 0.2484, train acc: 0.9106, val loss: 0.3568, val acc: 0.9339  (best train acc: 0.9346, best val acc: 0.9444, best train loss: 0.1872)\n",
      "[Epoch: 17660] train loss: 0.2145, train acc: 0.9200, val loss: 0.3277, val acc: 0.9393  (best train acc: 0.9346, best val acc: 0.9444, best train loss: 0.1872)\n",
      "[Epoch: 17680] train loss: 0.1962, train acc: 0.9297, val loss: 0.3368, val acc: 0.9373  (best train acc: 0.9346, best val acc: 0.9444, best train loss: 0.1872)\n",
      "[Epoch: 17700] train loss: 0.2056, train acc: 0.9239, val loss: 0.3375, val acc: 0.9383  (best train acc: 0.9346, best val acc: 0.9444, best train loss: 0.1872)\n",
      "[Epoch: 17720] train loss: 0.2070, train acc: 0.9245, val loss: 0.3466, val acc: 0.9413  (best train acc: 0.9346, best val acc: 0.9444, best train loss: 0.1872)\n",
      "[Epoch: 17740] train loss: 0.2026, train acc: 0.9234, val loss: 0.3575, val acc: 0.9359  (best train acc: 0.9346, best val acc: 0.9444, best train loss: 0.1872)\n",
      "[Epoch: 17760] train loss: 0.2077, train acc: 0.9271, val loss: 0.3591, val acc: 0.9363  (best train acc: 0.9346, best val acc: 0.9444, best train loss: 0.1872)\n",
      "[Epoch: 17780] train loss: 0.2001, train acc: 0.9271, val loss: 0.3478, val acc: 0.9376  (best train acc: 0.9346, best val acc: 0.9444, best train loss: 0.1872)\n",
      "[Epoch: 17800] train loss: 0.1987, train acc: 0.9304, val loss: 0.3486, val acc: 0.9406  (best train acc: 0.9346, best val acc: 0.9444, best train loss: 0.1872)\n",
      "[Epoch: 17820] train loss: 0.2136, train acc: 0.9194, val loss: 0.3631, val acc: 0.9383  (best train acc: 0.9346, best val acc: 0.9444, best train loss: 0.1872)\n",
      "[Epoch: 17840] train loss: 0.2045, train acc: 0.9264, val loss: 0.3418, val acc: 0.9423  (best train acc: 0.9346, best val acc: 0.9444, best train loss: 0.1872)\n",
      "[Epoch: 17860] train loss: 0.2087, train acc: 0.9261, val loss: 0.3265, val acc: 0.9393  (best train acc: 0.9346, best val acc: 0.9444, best train loss: 0.1872)\n",
      "[Epoch: 17880] train loss: 0.2162, train acc: 0.9193, val loss: 0.3347, val acc: 0.9423  (best train acc: 0.9346, best val acc: 0.9444, best train loss: 0.1872)\n",
      "[Epoch: 17900] train loss: 0.1973, train acc: 0.9317, val loss: 0.3375, val acc: 0.9396  (best train acc: 0.9346, best val acc: 0.9444, best train loss: 0.1872)\n",
      "[Epoch: 17920] train loss: 0.1925, train acc: 0.9329, val loss: 0.3430, val acc: 0.9413  (best train acc: 0.9346, best val acc: 0.9444, best train loss: 0.1868)\n",
      "[Epoch: 17940] train loss: 0.2023, train acc: 0.9284, val loss: 0.3463, val acc: 0.9417  (best train acc: 0.9346, best val acc: 0.9444, best train loss: 0.1868)\n",
      "[Epoch: 17960] train loss: 0.1970, train acc: 0.9302, val loss: 0.3646, val acc: 0.9417  (best train acc: 0.9346, best val acc: 0.9444, best train loss: 0.1868)\n",
      "[Epoch: 17980] train loss: 0.1999, train acc: 0.9286, val loss: 0.3249, val acc: 0.9413  (best train acc: 0.9346, best val acc: 0.9444, best train loss: 0.1868)\n",
      "[Epoch: 18000] train loss: 0.2021, train acc: 0.9245, val loss: 0.3486, val acc: 0.9396  (best train acc: 0.9346, best val acc: 0.9444, best train loss: 0.1868)\n",
      "[Epoch: 18020] train loss: 0.2043, train acc: 0.9268, val loss: 0.3387, val acc: 0.9379  (best train acc: 0.9346, best val acc: 0.9444, best train loss: 0.1868)\n",
      "[Epoch: 18040] train loss: 0.2046, train acc: 0.9266, val loss: 0.3716, val acc: 0.9379  (best train acc: 0.9346, best val acc: 0.9444, best train loss: 0.1868)\n",
      "[Epoch: 18060] train loss: 0.2116, train acc: 0.9226, val loss: 0.3612, val acc: 0.9396  (best train acc: 0.9346, best val acc: 0.9444, best train loss: 0.1868)\n",
      "[Epoch: 18080] train loss: 0.1987, train acc: 0.9305, val loss: 0.3536, val acc: 0.9403  (best train acc: 0.9346, best val acc: 0.9444, best train loss: 0.1868)\n",
      "[Epoch: 18100] train loss: 0.2018, train acc: 0.9280, val loss: 0.3637, val acc: 0.9379  (best train acc: 0.9346, best val acc: 0.9444, best train loss: 0.1868)\n",
      "[Epoch: 18120] train loss: 0.2138, train acc: 0.9233, val loss: 0.3611, val acc: 0.9366  (best train acc: 0.9346, best val acc: 0.9444, best train loss: 0.1868)\n",
      "[Epoch: 18140] train loss: 0.1914, train acc: 0.9328, val loss: 0.3564, val acc: 0.9417  (best train acc: 0.9346, best val acc: 0.9444, best train loss: 0.1868)\n",
      "[Epoch: 18160] train loss: 0.2150, train acc: 0.9192, val loss: 0.3676, val acc: 0.9383  (best train acc: 0.9346, best val acc: 0.9444, best train loss: 0.1868)\n",
      "[Epoch: 18180] train loss: 0.2271, train acc: 0.9166, val loss: 0.3944, val acc: 0.9329  (best train acc: 0.9346, best val acc: 0.9444, best train loss: 0.1868)\n",
      "[Epoch: 18200] train loss: 0.2124, train acc: 0.9187, val loss: 0.4016, val acc: 0.9332  (best train acc: 0.9346, best val acc: 0.9444, best train loss: 0.1868)\n",
      "[Epoch: 18220] train loss: 0.1965, train acc: 0.9275, val loss: 0.3395, val acc: 0.9396  (best train acc: 0.9346, best val acc: 0.9444, best train loss: 0.1868)\n",
      "[Epoch: 18240] train loss: 0.2034, train acc: 0.9269, val loss: 0.3501, val acc: 0.9417  (best train acc: 0.9346, best val acc: 0.9444, best train loss: 0.1868)\n",
      "[Epoch: 18260] train loss: 0.2144, train acc: 0.9213, val loss: 0.3593, val acc: 0.9430  (best train acc: 0.9346, best val acc: 0.9444, best train loss: 0.1868)\n",
      "[Epoch: 18280] train loss: 0.2092, train acc: 0.9220, val loss: 0.3559, val acc: 0.9417  (best train acc: 0.9346, best val acc: 0.9444, best train loss: 0.1868)\n",
      "[Epoch: 18300] train loss: 0.2280, train acc: 0.9166, val loss: 0.3498, val acc: 0.9430  (best train acc: 0.9346, best val acc: 0.9444, best train loss: 0.1868)\n",
      "[Epoch: 18320] train loss: 0.2291, train acc: 0.9124, val loss: 0.3828, val acc: 0.9305  (best train acc: 0.9346, best val acc: 0.9444, best train loss: 0.1868)\n",
      "[Epoch: 18340] train loss: 0.2073, train acc: 0.9265, val loss: 0.3602, val acc: 0.9369  (best train acc: 0.9346, best val acc: 0.9444, best train loss: 0.1868)\n",
      "[Epoch: 18360] train loss: 0.2196, train acc: 0.9226, val loss: 0.3562, val acc: 0.9406  (best train acc: 0.9346, best val acc: 0.9444, best train loss: 0.1868)\n",
      "[Epoch: 18380] train loss: 0.2069, train acc: 0.9258, val loss: 0.3801, val acc: 0.9376  (best train acc: 0.9346, best val acc: 0.9444, best train loss: 0.1868)\n",
      "[Epoch: 18400] train loss: 0.2003, train acc: 0.9276, val loss: 0.3365, val acc: 0.9433  (best train acc: 0.9346, best val acc: 0.9444, best train loss: 0.1868)\n",
      "[Epoch: 18420] train loss: 0.2142, train acc: 0.9218, val loss: 0.3442, val acc: 0.9379  (best train acc: 0.9348, best val acc: 0.9444, best train loss: 0.1868)\n",
      "[Epoch: 18440] train loss: 0.1940, train acc: 0.9333, val loss: 0.3564, val acc: 0.9410  (best train acc: 0.9348, best val acc: 0.9444, best train loss: 0.1868)\n",
      "[Epoch: 18460] train loss: 0.2069, train acc: 0.9247, val loss: 0.3495, val acc: 0.9386  (best train acc: 0.9348, best val acc: 0.9444, best train loss: 0.1868)\n",
      "[Epoch: 18480] train loss: 0.2249, train acc: 0.9224, val loss: 0.3644, val acc: 0.9369  (best train acc: 0.9348, best val acc: 0.9444, best train loss: 0.1868)\n",
      "[Epoch: 18500] train loss: 0.2126, train acc: 0.9195, val loss: 0.3514, val acc: 0.9366  (best train acc: 0.9348, best val acc: 0.9444, best train loss: 0.1868)\n",
      "[Epoch: 18520] train loss: 0.1931, train acc: 0.9343, val loss: 0.3559, val acc: 0.9386  (best train acc: 0.9354, best val acc: 0.9444, best train loss: 0.1868)\n",
      "[Epoch: 18540] train loss: 0.2089, train acc: 0.9265, val loss: 0.3834, val acc: 0.9413  (best train acc: 0.9358, best val acc: 0.9444, best train loss: 0.1868)\n",
      "[Epoch: 18560] train loss: 0.2030, train acc: 0.9263, val loss: 0.3416, val acc: 0.9390  (best train acc: 0.9358, best val acc: 0.9444, best train loss: 0.1868)\n",
      "[Epoch: 18580] train loss: 0.2027, train acc: 0.9283, val loss: 0.3608, val acc: 0.9383  (best train acc: 0.9358, best val acc: 0.9444, best train loss: 0.1868)\n",
      "[Epoch: 18600] train loss: 0.1992, train acc: 0.9276, val loss: 0.3605, val acc: 0.9433  (best train acc: 0.9358, best val acc: 0.9444, best train loss: 0.1868)\n",
      "[Epoch: 18620] train loss: 0.2042, train acc: 0.9252, val loss: 0.3641, val acc: 0.9410  (best train acc: 0.9358, best val acc: 0.9444, best train loss: 0.1868)\n",
      "[Epoch: 18640] train loss: 0.2142, train acc: 0.9224, val loss: 0.3706, val acc: 0.9366  (best train acc: 0.9358, best val acc: 0.9444, best train loss: 0.1868)\n",
      "[Epoch: 18660] train loss: 0.1976, train acc: 0.9291, val loss: 0.3537, val acc: 0.9376  (best train acc: 0.9358, best val acc: 0.9444, best train loss: 0.1868)\n",
      "[Epoch: 18680] train loss: 0.2230, train acc: 0.9192, val loss: 0.3662, val acc: 0.9400  (best train acc: 0.9358, best val acc: 0.9444, best train loss: 0.1868)\n",
      "[Epoch: 18700] train loss: 0.2038, train acc: 0.9295, val loss: 0.3479, val acc: 0.9383  (best train acc: 0.9358, best val acc: 0.9447, best train loss: 0.1868)\n",
      "[Epoch: 18720] train loss: 0.2050, train acc: 0.9275, val loss: 0.3610, val acc: 0.9410  (best train acc: 0.9358, best val acc: 0.9447, best train loss: 0.1868)\n",
      "[Epoch: 18740] train loss: 0.2127, train acc: 0.9219, val loss: 0.3658, val acc: 0.9288  (best train acc: 0.9358, best val acc: 0.9447, best train loss: 0.1868)\n",
      "[Epoch: 18760] train loss: 0.2138, train acc: 0.9251, val loss: 0.3794, val acc: 0.9298  (best train acc: 0.9358, best val acc: 0.9447, best train loss: 0.1868)\n",
      "[Epoch: 18780] train loss: 0.2156, train acc: 0.9201, val loss: 0.3644, val acc: 0.9356  (best train acc: 0.9358, best val acc: 0.9447, best train loss: 0.1868)\n",
      "[Epoch: 18800] train loss: 0.2085, train acc: 0.9217, val loss: 0.3465, val acc: 0.9410  (best train acc: 0.9358, best val acc: 0.9447, best train loss: 0.1868)\n",
      "[Epoch: 18820] train loss: 0.2336, train acc: 0.9088, val loss: 0.3475, val acc: 0.9336  (best train acc: 0.9358, best val acc: 0.9447, best train loss: 0.1868)\n",
      "[Epoch: 18840] train loss: 0.1972, train acc: 0.9276, val loss: 0.3396, val acc: 0.9403  (best train acc: 0.9358, best val acc: 0.9447, best train loss: 0.1868)\n",
      "[Epoch: 18860] train loss: 0.1906, train acc: 0.9341, val loss: 0.3625, val acc: 0.9390  (best train acc: 0.9358, best val acc: 0.9447, best train loss: 0.1868)\n",
      "[Epoch: 18880] train loss: 0.1979, train acc: 0.9275, val loss: 0.3798, val acc: 0.9417  (best train acc: 0.9358, best val acc: 0.9447, best train loss: 0.1868)\n",
      "[Epoch: 18900] train loss: 0.2002, train acc: 0.9273, val loss: 0.3662, val acc: 0.9373  (best train acc: 0.9358, best val acc: 0.9447, best train loss: 0.1868)\n",
      "[Epoch: 18920] train loss: 0.1915, train acc: 0.9315, val loss: 0.3361, val acc: 0.9410  (best train acc: 0.9358, best val acc: 0.9447, best train loss: 0.1868)\n",
      "[Epoch: 18940] train loss: 0.1970, train acc: 0.9318, val loss: 0.3570, val acc: 0.9413  (best train acc: 0.9358, best val acc: 0.9447, best train loss: 0.1868)\n",
      "[Epoch: 18960] train loss: 0.2025, train acc: 0.9281, val loss: 0.3617, val acc: 0.9400  (best train acc: 0.9358, best val acc: 0.9447, best train loss: 0.1868)\n",
      "[Epoch: 18980] train loss: 0.1957, train acc: 0.9305, val loss: 0.3575, val acc: 0.9390  (best train acc: 0.9358, best val acc: 0.9447, best train loss: 0.1868)\n",
      "[Epoch: 19000] train loss: 0.2079, train acc: 0.9243, val loss: 0.3660, val acc: 0.9390  (best train acc: 0.9358, best val acc: 0.9447, best train loss: 0.1868)\n",
      "[Epoch: 19020] train loss: 0.1950, train acc: 0.9302, val loss: 0.3691, val acc: 0.9379  (best train acc: 0.9358, best val acc: 0.9447, best train loss: 0.1868)\n",
      "[Epoch: 19040] train loss: 0.2099, train acc: 0.9194, val loss: 0.3843, val acc: 0.9356  (best train acc: 0.9358, best val acc: 0.9447, best train loss: 0.1868)\n",
      "[Epoch: 19060] train loss: 0.2239, train acc: 0.9128, val loss: 0.3804, val acc: 0.9325  (best train acc: 0.9358, best val acc: 0.9447, best train loss: 0.1868)\n",
      "[Epoch: 19080] train loss: 0.1957, train acc: 0.9314, val loss: 0.3532, val acc: 0.9376  (best train acc: 0.9358, best val acc: 0.9447, best train loss: 0.1868)\n",
      "[Epoch: 19100] train loss: 0.2128, train acc: 0.9208, val loss: 0.3362, val acc: 0.9423  (best train acc: 0.9358, best val acc: 0.9447, best train loss: 0.1868)\n",
      "[Epoch: 19120] train loss: 0.2106, train acc: 0.9229, val loss: 0.3718, val acc: 0.9406  (best train acc: 0.9358, best val acc: 0.9447, best train loss: 0.1868)\n",
      "[Epoch: 19140] train loss: 0.2020, train acc: 0.9288, val loss: 0.3661, val acc: 0.9400  (best train acc: 0.9358, best val acc: 0.9447, best train loss: 0.1868)\n",
      "[Epoch: 19160] train loss: 0.2175, train acc: 0.9156, val loss: 0.3599, val acc: 0.9241  (best train acc: 0.9358, best val acc: 0.9447, best train loss: 0.1868)\n",
      "[Epoch: 19180] train loss: 0.2249, train acc: 0.9203, val loss: 0.3392, val acc: 0.9363  (best train acc: 0.9358, best val acc: 0.9450, best train loss: 0.1868)\n",
      "[Epoch: 19200] train loss: 0.2146, train acc: 0.9218, val loss: 0.3435, val acc: 0.9406  (best train acc: 0.9358, best val acc: 0.9450, best train loss: 0.1868)\n",
      "[Epoch: 19220] train loss: 0.2039, train acc: 0.9270, val loss: 0.3683, val acc: 0.9376  (best train acc: 0.9378, best val acc: 0.9450, best train loss: 0.1852)\n",
      "[Epoch: 19240] train loss: 0.1979, train acc: 0.9298, val loss: 0.3829, val acc: 0.9433  (best train acc: 0.9378, best val acc: 0.9450, best train loss: 0.1852)\n",
      "[Epoch: 19260] train loss: 0.2007, train acc: 0.9268, val loss: 0.3667, val acc: 0.9417  (best train acc: 0.9378, best val acc: 0.9450, best train loss: 0.1852)\n",
      "[Epoch: 19280] train loss: 0.2146, train acc: 0.9187, val loss: 0.3899, val acc: 0.9312  (best train acc: 0.9378, best val acc: 0.9450, best train loss: 0.1840)\n",
      "[Epoch: 19300] train loss: 0.2228, train acc: 0.9133, val loss: 0.3631, val acc: 0.9339  (best train acc: 0.9378, best val acc: 0.9450, best train loss: 0.1840)\n",
      "[Epoch: 19320] train loss: 0.2003, train acc: 0.9295, val loss: 0.3537, val acc: 0.9410  (best train acc: 0.9378, best val acc: 0.9450, best train loss: 0.1840)\n",
      "[Epoch: 19340] train loss: 0.2041, train acc: 0.9250, val loss: 0.3720, val acc: 0.9420  (best train acc: 0.9378, best val acc: 0.9450, best train loss: 0.1840)\n",
      "[Epoch: 19360] train loss: 0.1954, train acc: 0.9305, val loss: 0.3673, val acc: 0.9423  (best train acc: 0.9378, best val acc: 0.9450, best train loss: 0.1840)\n",
      "[Epoch: 19380] train loss: 0.2147, train acc: 0.9218, val loss: 0.3643, val acc: 0.9352  (best train acc: 0.9378, best val acc: 0.9450, best train loss: 0.1828)\n",
      "[Epoch: 19400] train loss: 0.2061, train acc: 0.9293, val loss: 0.3546, val acc: 0.9420  (best train acc: 0.9378, best val acc: 0.9450, best train loss: 0.1825)\n",
      "[Epoch: 19420] train loss: 0.2035, train acc: 0.9231, val loss: 0.3694, val acc: 0.9413  (best train acc: 0.9378, best val acc: 0.9450, best train loss: 0.1825)\n",
      "[Epoch: 19440] train loss: 0.1953, train acc: 0.9264, val loss: 0.3845, val acc: 0.9406  (best train acc: 0.9378, best val acc: 0.9450, best train loss: 0.1825)\n",
      "[Epoch: 19460] train loss: 0.2196, train acc: 0.9208, val loss: 0.3741, val acc: 0.9305  (best train acc: 0.9378, best val acc: 0.9450, best train loss: 0.1825)\n",
      "[Epoch: 19480] train loss: 0.2419, train acc: 0.9100, val loss: 0.3790, val acc: 0.9393  (best train acc: 0.9378, best val acc: 0.9450, best train loss: 0.1825)\n",
      "[Epoch: 19500] train loss: 0.2078, train acc: 0.9250, val loss: 0.3491, val acc: 0.9393  (best train acc: 0.9378, best val acc: 0.9450, best train loss: 0.1825)\n",
      "[Epoch: 19520] train loss: 0.2204, train acc: 0.9195, val loss: 0.3327, val acc: 0.9417  (best train acc: 0.9378, best val acc: 0.9450, best train loss: 0.1825)\n",
      "[Epoch: 19540] train loss: 0.2191, train acc: 0.9190, val loss: 0.3641, val acc: 0.9268  (best train acc: 0.9378, best val acc: 0.9450, best train loss: 0.1825)\n",
      "[Epoch: 19560] train loss: 0.2003, train acc: 0.9302, val loss: 0.3349, val acc: 0.9410  (best train acc: 0.9378, best val acc: 0.9450, best train loss: 0.1825)\n",
      "[Epoch: 19580] train loss: 0.2233, train acc: 0.9214, val loss: 0.3627, val acc: 0.9413  (best train acc: 0.9378, best val acc: 0.9450, best train loss: 0.1825)\n",
      "[Epoch: 19600] train loss: 0.1914, train acc: 0.9317, val loss: 0.3545, val acc: 0.9406  (best train acc: 0.9378, best val acc: 0.9450, best train loss: 0.1825)\n",
      "[Epoch: 19620] train loss: 0.1897, train acc: 0.9339, val loss: 0.3555, val acc: 0.9410  (best train acc: 0.9378, best val acc: 0.9450, best train loss: 0.1825)\n",
      "[Epoch: 19640] train loss: 0.2154, train acc: 0.9258, val loss: 0.3557, val acc: 0.9386  (best train acc: 0.9378, best val acc: 0.9450, best train loss: 0.1825)\n",
      "[Epoch: 19660] train loss: 0.2072, train acc: 0.9268, val loss: 0.3589, val acc: 0.9410  (best train acc: 0.9378, best val acc: 0.9450, best train loss: 0.1825)\n",
      "[Epoch: 19680] train loss: 0.1952, train acc: 0.9291, val loss: 0.3636, val acc: 0.9400  (best train acc: 0.9378, best val acc: 0.9450, best train loss: 0.1825)\n",
      "[Epoch: 19700] train loss: 0.2218, train acc: 0.9185, val loss: 0.3798, val acc: 0.9346  (best train acc: 0.9378, best val acc: 0.9450, best train loss: 0.1825)\n",
      "[Epoch: 19720] train loss: 0.2105, train acc: 0.9216, val loss: 0.3650, val acc: 0.9423  (best train acc: 0.9378, best val acc: 0.9450, best train loss: 0.1825)\n",
      "[Epoch: 19740] train loss: 0.1948, train acc: 0.9305, val loss: 0.3591, val acc: 0.9410  (best train acc: 0.9378, best val acc: 0.9450, best train loss: 0.1825)\n",
      "[Epoch: 19760] train loss: 0.1928, train acc: 0.9324, val loss: 0.3600, val acc: 0.9366  (best train acc: 0.9378, best val acc: 0.9450, best train loss: 0.1825)\n",
      "[Epoch: 19780] train loss: 0.2240, train acc: 0.9161, val loss: 0.3627, val acc: 0.9417  (best train acc: 0.9378, best val acc: 0.9450, best train loss: 0.1825)\n",
      "[Epoch: 19800] train loss: 0.1971, train acc: 0.9305, val loss: 0.3673, val acc: 0.9406  (best train acc: 0.9378, best val acc: 0.9450, best train loss: 0.1825)\n",
      "[Epoch: 19820] train loss: 0.2014, train acc: 0.9239, val loss: 0.3715, val acc: 0.9393  (best train acc: 0.9378, best val acc: 0.9450, best train loss: 0.1825)\n",
      "[Epoch: 19840] train loss: 0.1971, train acc: 0.9276, val loss: 0.3661, val acc: 0.9396  (best train acc: 0.9378, best val acc: 0.9450, best train loss: 0.1825)\n",
      "[Epoch: 19860] train loss: 0.2093, train acc: 0.9269, val loss: 0.3603, val acc: 0.9410  (best train acc: 0.9378, best val acc: 0.9450, best train loss: 0.1825)\n",
      "[Epoch: 19880] train loss: 0.2139, train acc: 0.9184, val loss: 0.3593, val acc: 0.9336  (best train acc: 0.9378, best val acc: 0.9450, best train loss: 0.1825)\n",
      "[Epoch: 19900] train loss: 0.2001, train acc: 0.9295, val loss: 0.3627, val acc: 0.9413  (best train acc: 0.9378, best val acc: 0.9450, best train loss: 0.1825)\n",
      "[Epoch: 19920] train loss: 0.2186, train acc: 0.9231, val loss: 0.3834, val acc: 0.9386  (best train acc: 0.9378, best val acc: 0.9450, best train loss: 0.1825)\n",
      "[Epoch: 19940] train loss: 0.1915, train acc: 0.9328, val loss: 0.3644, val acc: 0.9430  (best train acc: 0.9378, best val acc: 0.9450, best train loss: 0.1825)\n",
      "[Epoch: 19960] train loss: 0.1966, train acc: 0.9291, val loss: 0.3754, val acc: 0.9403  (best train acc: 0.9378, best val acc: 0.9450, best train loss: 0.1825)\n",
      "[Epoch: 19980] train loss: 0.1930, train acc: 0.9315, val loss: 0.3701, val acc: 0.9383  (best train acc: 0.9378, best val acc: 0.9450, best train loss: 0.1825)\n",
      "[Epoch: 20000] train loss: 0.1985, train acc: 0.9271, val loss: 0.3637, val acc: 0.9410  (best train acc: 0.9378, best val acc: 0.9450, best train loss: 0.1825)\n",
      "[Epoch: 20020] train loss: 0.1988, train acc: 0.9260, val loss: 0.3880, val acc: 0.9410  (best train acc: 0.9378, best val acc: 0.9450, best train loss: 0.1825)\n",
      "[Epoch: 20040] train loss: 0.1912, train acc: 0.9302, val loss: 0.3947, val acc: 0.9403  (best train acc: 0.9378, best val acc: 0.9450, best train loss: 0.1825)\n",
      "[Epoch: 20060] train loss: 0.2049, train acc: 0.9284, val loss: 0.3898, val acc: 0.9403  (best train acc: 0.9378, best val acc: 0.9450, best train loss: 0.1825)\n",
      "[Epoch: 20080] train loss: 0.2085, train acc: 0.9221, val loss: 0.3730, val acc: 0.9373  (best train acc: 0.9378, best val acc: 0.9450, best train loss: 0.1825)\n",
      "[Epoch: 20100] train loss: 0.2067, train acc: 0.9245, val loss: 0.3688, val acc: 0.9390  (best train acc: 0.9378, best val acc: 0.9450, best train loss: 0.1825)\n",
      "[Epoch: 20120] train loss: 0.2046, train acc: 0.9237, val loss: 0.4394, val acc: 0.9180  (best train acc: 0.9378, best val acc: 0.9450, best train loss: 0.1825)\n",
      "[Epoch: 20140] train loss: 0.2210, train acc: 0.9175, val loss: 0.3665, val acc: 0.9288  (best train acc: 0.9378, best val acc: 0.9450, best train loss: 0.1825)\n",
      "[Epoch: 20160] train loss: 0.1988, train acc: 0.9284, val loss: 0.3497, val acc: 0.9433  (best train acc: 0.9378, best val acc: 0.9450, best train loss: 0.1825)\n",
      "[Epoch: 20180] train loss: 0.2024, train acc: 0.9265, val loss: 0.3424, val acc: 0.9359  (best train acc: 0.9378, best val acc: 0.9450, best train loss: 0.1825)\n",
      "[Epoch: 20200] train loss: 0.2090, train acc: 0.9219, val loss: 0.3585, val acc: 0.9383  (best train acc: 0.9378, best val acc: 0.9450, best train loss: 0.1825)\n",
      "[Epoch: 20220] train loss: 0.2098, train acc: 0.9269, val loss: 0.3641, val acc: 0.9309  (best train acc: 0.9378, best val acc: 0.9450, best train loss: 0.1825)\n",
      "[Epoch: 20240] train loss: 0.1973, train acc: 0.9315, val loss: 0.3660, val acc: 0.9406  (best train acc: 0.9378, best val acc: 0.9450, best train loss: 0.1825)\n",
      "[Epoch: 20260] train loss: 0.1942, train acc: 0.9320, val loss: 0.3580, val acc: 0.9396  (best train acc: 0.9378, best val acc: 0.9450, best train loss: 0.1825)\n",
      "[Epoch: 20280] train loss: 0.2228, train acc: 0.9185, val loss: 0.3617, val acc: 0.9417  (best train acc: 0.9378, best val acc: 0.9450, best train loss: 0.1825)\n",
      "[Epoch: 20300] train loss: 0.1931, train acc: 0.9331, val loss: 0.3627, val acc: 0.9396  (best train acc: 0.9378, best val acc: 0.9450, best train loss: 0.1825)\n",
      "[Epoch: 20320] train loss: 0.1980, train acc: 0.9276, val loss: 0.3716, val acc: 0.9423  (best train acc: 0.9378, best val acc: 0.9450, best train loss: 0.1825)\n",
      "[Epoch: 20340] train loss: 0.1921, train acc: 0.9327, val loss: 0.3699, val acc: 0.9413  (best train acc: 0.9378, best val acc: 0.9450, best train loss: 0.1809)\n",
      "[Epoch: 20360] train loss: 0.2009, train acc: 0.9273, val loss: 0.3591, val acc: 0.9423  (best train acc: 0.9378, best val acc: 0.9450, best train loss: 0.1809)\n",
      "[Epoch: 20380] train loss: 0.2101, train acc: 0.9238, val loss: 0.3733, val acc: 0.9433  (best train acc: 0.9378, best val acc: 0.9450, best train loss: 0.1809)\n",
      "[Epoch: 20400] train loss: 0.1901, train acc: 0.9325, val loss: 0.3575, val acc: 0.9427  (best train acc: 0.9378, best val acc: 0.9450, best train loss: 0.1809)\n",
      "[Epoch: 20420] train loss: 0.1874, train acc: 0.9354, val loss: 0.3683, val acc: 0.9420  (best train acc: 0.9378, best val acc: 0.9450, best train loss: 0.1809)\n",
      "[Epoch: 20440] train loss: 0.1966, train acc: 0.9291, val loss: 0.3835, val acc: 0.9349  (best train acc: 0.9378, best val acc: 0.9450, best train loss: 0.1809)\n",
      "[Epoch: 20460] train loss: 0.1969, train acc: 0.9301, val loss: 0.4006, val acc: 0.9406  (best train acc: 0.9378, best val acc: 0.9450, best train loss: 0.1809)\n",
      "[Epoch: 20480] train loss: 0.2044, train acc: 0.9247, val loss: 0.3850, val acc: 0.9197  (best train acc: 0.9378, best val acc: 0.9450, best train loss: 0.1809)\n",
      "[Epoch: 20500] train loss: 0.2204, train acc: 0.9162, val loss: 0.3775, val acc: 0.9356  (best train acc: 0.9378, best val acc: 0.9450, best train loss: 0.1809)\n",
      "[Epoch: 20520] train loss: 0.1995, train acc: 0.9295, val loss: 0.3413, val acc: 0.9420  (best train acc: 0.9378, best val acc: 0.9450, best train loss: 0.1809)\n",
      "[Epoch: 20540] train loss: 0.1926, train acc: 0.9315, val loss: 0.3785, val acc: 0.9423  (best train acc: 0.9378, best val acc: 0.9454, best train loss: 0.1809)\n",
      "[Epoch: 20560] train loss: 0.1998, train acc: 0.9264, val loss: 0.3594, val acc: 0.9427  (best train acc: 0.9378, best val acc: 0.9454, best train loss: 0.1809)\n",
      "[Epoch: 20580] train loss: 0.1964, train acc: 0.9323, val loss: 0.3785, val acc: 0.9454  (best train acc: 0.9378, best val acc: 0.9454, best train loss: 0.1809)\n",
      "[Epoch: 20600] train loss: 0.2176, train acc: 0.9169, val loss: 0.3838, val acc: 0.9373  (best train acc: 0.9378, best val acc: 0.9454, best train loss: 0.1809)\n",
      "[Epoch: 20620] train loss: 0.1900, train acc: 0.9323, val loss: 0.3795, val acc: 0.9447  (best train acc: 0.9378, best val acc: 0.9454, best train loss: 0.1809)\n",
      "[Epoch: 20640] train loss: 0.2042, train acc: 0.9295, val loss: 0.3830, val acc: 0.9400  (best train acc: 0.9378, best val acc: 0.9454, best train loss: 0.1809)\n",
      "[Epoch: 20660] train loss: 0.1898, train acc: 0.9325, val loss: 0.3734, val acc: 0.9417  (best train acc: 0.9378, best val acc: 0.9454, best train loss: 0.1809)\n",
      "[Epoch: 20680] train loss: 0.2015, train acc: 0.9259, val loss: 0.3773, val acc: 0.9376  (best train acc: 0.9378, best val acc: 0.9454, best train loss: 0.1809)\n",
      "[Epoch: 20700] train loss: 0.2038, train acc: 0.9245, val loss: 0.3625, val acc: 0.9366  (best train acc: 0.9378, best val acc: 0.9454, best train loss: 0.1809)\n",
      "[Epoch: 20720] train loss: 0.2684, train acc: 0.8994, val loss: 0.3689, val acc: 0.9177  (best train acc: 0.9378, best val acc: 0.9454, best train loss: 0.1809)\n",
      "[Epoch: 20740] train loss: 0.1966, train acc: 0.9320, val loss: 0.3664, val acc: 0.9325  (best train acc: 0.9378, best val acc: 0.9454, best train loss: 0.1809)\n",
      "[Epoch: 20760] train loss: 0.1950, train acc: 0.9290, val loss: 0.3531, val acc: 0.9437  (best train acc: 0.9378, best val acc: 0.9454, best train loss: 0.1809)\n",
      "[Epoch: 20780] train loss: 0.1895, train acc: 0.9315, val loss: 0.3625, val acc: 0.9413  (best train acc: 0.9378, best val acc: 0.9454, best train loss: 0.1809)\n",
      "[Epoch: 20800] train loss: 0.1937, train acc: 0.9267, val loss: 0.3700, val acc: 0.9410  (best train acc: 0.9378, best val acc: 0.9454, best train loss: 0.1809)\n",
      "[Epoch: 20820] train loss: 0.1943, train acc: 0.9330, val loss: 0.3732, val acc: 0.9433  (best train acc: 0.9378, best val acc: 0.9454, best train loss: 0.1809)\n",
      "[Epoch: 20840] train loss: 0.1890, train acc: 0.9312, val loss: 0.3640, val acc: 0.9420  (best train acc: 0.9378, best val acc: 0.9454, best train loss: 0.1809)\n",
      "[Epoch: 20860] train loss: 0.2162, train acc: 0.9236, val loss: 0.3765, val acc: 0.9400  (best train acc: 0.9378, best val acc: 0.9454, best train loss: 0.1809)\n",
      "[Epoch: 20880] train loss: 0.1980, train acc: 0.9325, val loss: 0.3730, val acc: 0.9379  (best train acc: 0.9378, best val acc: 0.9454, best train loss: 0.1809)\n",
      "[Epoch: 20900] train loss: 0.1959, train acc: 0.9344, val loss: 0.3594, val acc: 0.9433  (best train acc: 0.9378, best val acc: 0.9454, best train loss: 0.1809)\n",
      "[Epoch: 20920] train loss: 0.1943, train acc: 0.9283, val loss: 0.3570, val acc: 0.9437  (best train acc: 0.9378, best val acc: 0.9454, best train loss: 0.1809)\n",
      "[Epoch: 20940] train loss: 0.1891, train acc: 0.9327, val loss: 0.3786, val acc: 0.9410  (best train acc: 0.9378, best val acc: 0.9454, best train loss: 0.1797)\n",
      "[Epoch: 20960] train loss: 0.1888, train acc: 0.9331, val loss: 0.3743, val acc: 0.9403  (best train acc: 0.9378, best val acc: 0.9454, best train loss: 0.1797)\n",
      "[Epoch: 20980] train loss: 0.2149, train acc: 0.9245, val loss: 0.3643, val acc: 0.9329  (best train acc: 0.9378, best val acc: 0.9454, best train loss: 0.1797)\n",
      "[Epoch: 21000] train loss: 0.2053, train acc: 0.9265, val loss: 0.3523, val acc: 0.9379  (best train acc: 0.9378, best val acc: 0.9454, best train loss: 0.1797)\n",
      "[Epoch: 21020] train loss: 0.1843, train acc: 0.9357, val loss: 0.3659, val acc: 0.9420  (best train acc: 0.9378, best val acc: 0.9454, best train loss: 0.1797)\n",
      "[Epoch: 21040] train loss: 0.1933, train acc: 0.9292, val loss: 0.3668, val acc: 0.9396  (best train acc: 0.9378, best val acc: 0.9454, best train loss: 0.1797)\n",
      "[Epoch: 21060] train loss: 0.1885, train acc: 0.9333, val loss: 0.3767, val acc: 0.9430  (best train acc: 0.9378, best val acc: 0.9454, best train loss: 0.1797)\n",
      "[Epoch: 21080] train loss: 0.2133, train acc: 0.9226, val loss: 0.3660, val acc: 0.9393  (best train acc: 0.9378, best val acc: 0.9454, best train loss: 0.1797)\n",
      "[Epoch: 21100] train loss: 0.1883, train acc: 0.9318, val loss: 0.3665, val acc: 0.9433  (best train acc: 0.9378, best val acc: 0.9454, best train loss: 0.1797)\n",
      "[Epoch: 21120] train loss: 0.2018, train acc: 0.9247, val loss: 0.3767, val acc: 0.9413  (best train acc: 0.9378, best val acc: 0.9454, best train loss: 0.1797)\n",
      "[Epoch: 21140] train loss: 0.1880, train acc: 0.9358, val loss: 0.3682, val acc: 0.9447  (best train acc: 0.9378, best val acc: 0.9454, best train loss: 0.1797)\n",
      "[Epoch: 21160] train loss: 0.1858, train acc: 0.9344, val loss: 0.3647, val acc: 0.9437  (best train acc: 0.9378, best val acc: 0.9454, best train loss: 0.1797)\n",
      "[Epoch: 21180] train loss: 0.1951, train acc: 0.9278, val loss: 0.3700, val acc: 0.9410  (best train acc: 0.9378, best val acc: 0.9454, best train loss: 0.1797)\n",
      "[Epoch: 21200] train loss: 0.2038, train acc: 0.9237, val loss: 0.3613, val acc: 0.9410  (best train acc: 0.9378, best val acc: 0.9454, best train loss: 0.1797)\n",
      "[Epoch: 21220] train loss: 0.1818, train acc: 0.9352, val loss: 0.3512, val acc: 0.9390  (best train acc: 0.9378, best val acc: 0.9454, best train loss: 0.1797)\n",
      "[Epoch: 21240] train loss: 0.1923, train acc: 0.9333, val loss: 0.3863, val acc: 0.9400  (best train acc: 0.9386, best val acc: 0.9454, best train loss: 0.1741)\n",
      "[Epoch: 21260] train loss: 0.1834, train acc: 0.9336, val loss: 0.3711, val acc: 0.9417  (best train acc: 0.9386, best val acc: 0.9454, best train loss: 0.1741)\n",
      "[Epoch: 21280] train loss: 0.1985, train acc: 0.9250, val loss: 0.3697, val acc: 0.9420  (best train acc: 0.9386, best val acc: 0.9454, best train loss: 0.1741)\n",
      "[Epoch: 21300] train loss: 0.1900, train acc: 0.9323, val loss: 0.3796, val acc: 0.9390  (best train acc: 0.9386, best val acc: 0.9454, best train loss: 0.1741)\n",
      "[Epoch: 21320] train loss: 0.1791, train acc: 0.9362, val loss: 0.3580, val acc: 0.9420  (best train acc: 0.9386, best val acc: 0.9454, best train loss: 0.1741)\n",
      "[Epoch: 21340] train loss: 0.1890, train acc: 0.9302, val loss: 0.3789, val acc: 0.9410  (best train acc: 0.9386, best val acc: 0.9454, best train loss: 0.1741)\n",
      "[Epoch: 21360] train loss: 0.1901, train acc: 0.9336, val loss: 0.3929, val acc: 0.9288  (best train acc: 0.9386, best val acc: 0.9454, best train loss: 0.1741)\n",
      "[Epoch: 21380] train loss: 0.1948, train acc: 0.9302, val loss: 0.3818, val acc: 0.9393  (best train acc: 0.9386, best val acc: 0.9454, best train loss: 0.1741)\n",
      "[Epoch: 21400] train loss: 0.1897, train acc: 0.9294, val loss: 0.3802, val acc: 0.9383  (best train acc: 0.9386, best val acc: 0.9454, best train loss: 0.1741)\n",
      "[Epoch: 21420] train loss: 0.1876, train acc: 0.9336, val loss: 0.3503, val acc: 0.9363  (best train acc: 0.9386, best val acc: 0.9454, best train loss: 0.1741)\n",
      "[Epoch: 21440] train loss: 0.1903, train acc: 0.9301, val loss: 0.3646, val acc: 0.9396  (best train acc: 0.9386, best val acc: 0.9460, best train loss: 0.1741)\n",
      "[Epoch: 21460] train loss: 0.1888, train acc: 0.9341, val loss: 0.3681, val acc: 0.9410  (best train acc: 0.9386, best val acc: 0.9460, best train loss: 0.1741)\n",
      "[Epoch: 21480] train loss: 0.1908, train acc: 0.9320, val loss: 0.3675, val acc: 0.9376  (best train acc: 0.9386, best val acc: 0.9460, best train loss: 0.1741)\n",
      "[Epoch: 21500] train loss: 0.1837, train acc: 0.9325, val loss: 0.3719, val acc: 0.9440  (best train acc: 0.9386, best val acc: 0.9460, best train loss: 0.1741)\n",
      "[Epoch: 21520] train loss: 0.1980, train acc: 0.9270, val loss: 0.3539, val acc: 0.9406  (best train acc: 0.9386, best val acc: 0.9460, best train loss: 0.1741)\n",
      "[Epoch: 21540] train loss: 0.1878, train acc: 0.9315, val loss: 0.3671, val acc: 0.9430  (best train acc: 0.9387, best val acc: 0.9460, best train loss: 0.1738)\n",
      "[Epoch: 21560] train loss: 0.1832, train acc: 0.9344, val loss: 0.3513, val acc: 0.9433  (best train acc: 0.9387, best val acc: 0.9460, best train loss: 0.1738)\n",
      "[Epoch: 21580] train loss: 0.1899, train acc: 0.9335, val loss: 0.3384, val acc: 0.9430  (best train acc: 0.9391, best val acc: 0.9460, best train loss: 0.1738)\n",
      "[Epoch: 21600] train loss: 0.1954, train acc: 0.9308, val loss: 0.3617, val acc: 0.9440  (best train acc: 0.9391, best val acc: 0.9460, best train loss: 0.1735)\n",
      "[Epoch: 21620] train loss: 0.1968, train acc: 0.9305, val loss: 0.3811, val acc: 0.9356  (best train acc: 0.9391, best val acc: 0.9460, best train loss: 0.1735)\n",
      "[Epoch: 21640] train loss: 0.1956, train acc: 0.9291, val loss: 0.3577, val acc: 0.9373  (best train acc: 0.9391, best val acc: 0.9460, best train loss: 0.1735)\n",
      "[Epoch: 21660] train loss: 0.1765, train acc: 0.9374, val loss: 0.3581, val acc: 0.9427  (best train acc: 0.9391, best val acc: 0.9460, best train loss: 0.1735)\n",
      "[Epoch: 21680] train loss: 0.1855, train acc: 0.9338, val loss: 0.3744, val acc: 0.9444  (best train acc: 0.9391, best val acc: 0.9460, best train loss: 0.1735)\n",
      "[Epoch: 21700] train loss: 0.1909, train acc: 0.9304, val loss: 0.3689, val acc: 0.9332  (best train acc: 0.9391, best val acc: 0.9460, best train loss: 0.1735)\n",
      "[Epoch: 21720] train loss: 0.1898, train acc: 0.9284, val loss: 0.3693, val acc: 0.9396  (best train acc: 0.9391, best val acc: 0.9460, best train loss: 0.1735)\n",
      "[Epoch: 21740] train loss: 0.1939, train acc: 0.9309, val loss: 0.3669, val acc: 0.9369  (best train acc: 0.9391, best val acc: 0.9464, best train loss: 0.1735)\n",
      "[Epoch: 21760] train loss: 0.1908, train acc: 0.9291, val loss: 0.3278, val acc: 0.9379  (best train acc: 0.9391, best val acc: 0.9464, best train loss: 0.1735)\n",
      "[Epoch: 21780] train loss: 0.2064, train acc: 0.9224, val loss: 0.3421, val acc: 0.9356  (best train acc: 0.9391, best val acc: 0.9464, best train loss: 0.1735)\n",
      "[Epoch: 21800] train loss: 0.2137, train acc: 0.9246, val loss: 0.3437, val acc: 0.9410  (best train acc: 0.9391, best val acc: 0.9464, best train loss: 0.1735)\n",
      "[Epoch: 21820] train loss: 0.1866, train acc: 0.9306, val loss: 0.3704, val acc: 0.9430  (best train acc: 0.9391, best val acc: 0.9464, best train loss: 0.1735)\n",
      "[Epoch: 21840] train loss: 0.2064, train acc: 0.9224, val loss: 0.3675, val acc: 0.9420  (best train acc: 0.9420, best val acc: 0.9464, best train loss: 0.1680)\n",
      "[Epoch: 21860] train loss: 0.1797, train acc: 0.9353, val loss: 0.3677, val acc: 0.9447  (best train acc: 0.9420, best val acc: 0.9464, best train loss: 0.1680)\n",
      "[Epoch: 21880] train loss: 0.1855, train acc: 0.9336, val loss: 0.3626, val acc: 0.9423  (best train acc: 0.9420, best val acc: 0.9464, best train loss: 0.1680)\n",
      "[Epoch: 21900] train loss: 0.1831, train acc: 0.9362, val loss: 0.3719, val acc: 0.9423  (best train acc: 0.9420, best val acc: 0.9464, best train loss: 0.1680)\n",
      "[Epoch: 21920] train loss: 0.1837, train acc: 0.9321, val loss: 0.3881, val acc: 0.9410  (best train acc: 0.9420, best val acc: 0.9464, best train loss: 0.1680)\n",
      "[Epoch: 21940] train loss: 0.1897, train acc: 0.9315, val loss: 0.3782, val acc: 0.9413  (best train acc: 0.9420, best val acc: 0.9464, best train loss: 0.1680)\n",
      "[Epoch: 21960] train loss: 0.1862, train acc: 0.9314, val loss: 0.3791, val acc: 0.9427  (best train acc: 0.9420, best val acc: 0.9464, best train loss: 0.1680)\n",
      "[Epoch: 21980] train loss: 0.1895, train acc: 0.9350, val loss: 0.3747, val acc: 0.9420  (best train acc: 0.9420, best val acc: 0.9464, best train loss: 0.1680)\n",
      "[Epoch: 22000] train loss: 0.2381, train acc: 0.9167, val loss: 0.3695, val acc: 0.9278  (best train acc: 0.9420, best val acc: 0.9464, best train loss: 0.1680)\n",
      "[Epoch: 22020] train loss: 0.2332, train acc: 0.9142, val loss: 0.4175, val acc: 0.9312  (best train acc: 0.9420, best val acc: 0.9464, best train loss: 0.1680)\n",
      "[Epoch: 22040] train loss: 0.1910, train acc: 0.9328, val loss: 0.3490, val acc: 0.9454  (best train acc: 0.9420, best val acc: 0.9464, best train loss: 0.1680)\n",
      "[Epoch: 22060] train loss: 0.1841, train acc: 0.9339, val loss: 0.3627, val acc: 0.9430  (best train acc: 0.9420, best val acc: 0.9464, best train loss: 0.1680)\n",
      "[Epoch: 22080] train loss: 0.1769, train acc: 0.9362, val loss: 0.3623, val acc: 0.9427  (best train acc: 0.9420, best val acc: 0.9464, best train loss: 0.1680)\n",
      "[Epoch: 22100] train loss: 0.1849, train acc: 0.9305, val loss: 0.3817, val acc: 0.9410  (best train acc: 0.9420, best val acc: 0.9464, best train loss: 0.1680)\n",
      "[Epoch: 22120] train loss: 0.2086, train acc: 0.9222, val loss: 0.3746, val acc: 0.9403  (best train acc: 0.9420, best val acc: 0.9464, best train loss: 0.1680)\n",
      "[Epoch: 22140] train loss: 0.2100, train acc: 0.9236, val loss: 0.3725, val acc: 0.9363  (best train acc: 0.9420, best val acc: 0.9464, best train loss: 0.1680)\n",
      "[Epoch: 22160] train loss: 0.1847, train acc: 0.9359, val loss: 0.3608, val acc: 0.9406  (best train acc: 0.9420, best val acc: 0.9464, best train loss: 0.1680)\n",
      "[Epoch: 22180] train loss: 0.1774, train acc: 0.9347, val loss: 0.3762, val acc: 0.9376  (best train acc: 0.9420, best val acc: 0.9464, best train loss: 0.1680)\n",
      "[Epoch: 22200] train loss: 0.1957, train acc: 0.9297, val loss: 0.3705, val acc: 0.9417  (best train acc: 0.9420, best val acc: 0.9464, best train loss: 0.1680)\n",
      "[Epoch: 22220] train loss: 0.1882, train acc: 0.9342, val loss: 0.3611, val acc: 0.9410  (best train acc: 0.9420, best val acc: 0.9464, best train loss: 0.1680)\n",
      "[Epoch: 22240] train loss: 0.2124, train acc: 0.9203, val loss: 0.3765, val acc: 0.9437  (best train acc: 0.9420, best val acc: 0.9464, best train loss: 0.1680)\n",
      "[Epoch: 22260] train loss: 0.1982, train acc: 0.9296, val loss: 0.3275, val acc: 0.9400  (best train acc: 0.9420, best val acc: 0.9464, best train loss: 0.1680)\n",
      "[Epoch: 22280] train loss: 0.2377, train acc: 0.9164, val loss: 0.3222, val acc: 0.9386  (best train acc: 0.9420, best val acc: 0.9464, best train loss: 0.1680)\n",
      "[Epoch: 22300] train loss: 0.2153, train acc: 0.9207, val loss: 0.3445, val acc: 0.9430  (best train acc: 0.9420, best val acc: 0.9474, best train loss: 0.1680)\n",
      "[Epoch: 22320] train loss: 0.1796, train acc: 0.9354, val loss: 0.3565, val acc: 0.9440  (best train acc: 0.9420, best val acc: 0.9474, best train loss: 0.1680)\n",
      "[Epoch: 22340] train loss: 0.1866, train acc: 0.9357, val loss: 0.3791, val acc: 0.9383  (best train acc: 0.9420, best val acc: 0.9474, best train loss: 0.1680)\n",
      "[Epoch: 22360] train loss: 0.1803, train acc: 0.9336, val loss: 0.3938, val acc: 0.9349  (best train acc: 0.9420, best val acc: 0.9474, best train loss: 0.1680)\n",
      "[Epoch: 22380] train loss: 0.1846, train acc: 0.9338, val loss: 0.3823, val acc: 0.9417  (best train acc: 0.9420, best val acc: 0.9474, best train loss: 0.1680)\n",
      "[Epoch: 22400] train loss: 0.1921, train acc: 0.9303, val loss: 0.3522, val acc: 0.9427  (best train acc: 0.9420, best val acc: 0.9474, best train loss: 0.1680)\n",
      "[Epoch: 22420] train loss: 0.1839, train acc: 0.9357, val loss: 0.3666, val acc: 0.9406  (best train acc: 0.9420, best val acc: 0.9474, best train loss: 0.1680)\n",
      "[Epoch: 22440] train loss: 0.1946, train acc: 0.9302, val loss: 0.3490, val acc: 0.9386  (best train acc: 0.9420, best val acc: 0.9474, best train loss: 0.1680)\n",
      "[Epoch: 22460] train loss: 0.1891, train acc: 0.9313, val loss: 0.3743, val acc: 0.9450  (best train acc: 0.9420, best val acc: 0.9474, best train loss: 0.1680)\n",
      "[Epoch: 22480] train loss: 0.1825, train acc: 0.9354, val loss: 0.3721, val acc: 0.9423  (best train acc: 0.9420, best val acc: 0.9474, best train loss: 0.1680)\n",
      "[Epoch: 22500] train loss: 0.1878, train acc: 0.9341, val loss: 0.3736, val acc: 0.9437  (best train acc: 0.9420, best val acc: 0.9474, best train loss: 0.1680)\n",
      "[Epoch: 22520] train loss: 0.1992, train acc: 0.9278, val loss: 0.3814, val acc: 0.9406  (best train acc: 0.9420, best val acc: 0.9474, best train loss: 0.1680)\n",
      "[Epoch: 22540] train loss: 0.2174, train acc: 0.9231, val loss: 0.3598, val acc: 0.9309  (best train acc: 0.9420, best val acc: 0.9474, best train loss: 0.1680)\n",
      "[Epoch: 22560] train loss: 0.1965, train acc: 0.9289, val loss: 0.3570, val acc: 0.9386  (best train acc: 0.9420, best val acc: 0.9474, best train loss: 0.1680)\n",
      "[Epoch: 22580] train loss: 0.2100, train acc: 0.9238, val loss: 0.3677, val acc: 0.9447  (best train acc: 0.9420, best val acc: 0.9474, best train loss: 0.1680)\n",
      "[Epoch: 22600] train loss: 0.1771, train acc: 0.9377, val loss: 0.3524, val acc: 0.9444  (best train acc: 0.9420, best val acc: 0.9474, best train loss: 0.1680)\n",
      "[Epoch: 22620] train loss: 0.1778, train acc: 0.9383, val loss: 0.3667, val acc: 0.9417  (best train acc: 0.9420, best val acc: 0.9474, best train loss: 0.1680)\n",
      "[Epoch: 22640] train loss: 0.1814, train acc: 0.9330, val loss: 0.3805, val acc: 0.9430  (best train acc: 0.9420, best val acc: 0.9474, best train loss: 0.1680)\n",
      "[Epoch: 22660] train loss: 0.2078, train acc: 0.9296, val loss: 0.3945, val acc: 0.9349  (best train acc: 0.9420, best val acc: 0.9474, best train loss: 0.1680)\n",
      "[Epoch: 22680] train loss: 0.1799, train acc: 0.9359, val loss: 0.3808, val acc: 0.9393  (best train acc: 0.9420, best val acc: 0.9474, best train loss: 0.1680)\n",
      "[Epoch: 22700] train loss: 0.1758, train acc: 0.9354, val loss: 0.3692, val acc: 0.9423  (best train acc: 0.9420, best val acc: 0.9474, best train loss: 0.1680)\n",
      "[Epoch: 22720] train loss: 0.1883, train acc: 0.9293, val loss: 0.3685, val acc: 0.9417  (best train acc: 0.9420, best val acc: 0.9474, best train loss: 0.1680)\n",
      "[Epoch: 22740] train loss: 0.1777, train acc: 0.9399, val loss: 0.3773, val acc: 0.9433  (best train acc: 0.9420, best val acc: 0.9474, best train loss: 0.1680)\n",
      "[Epoch: 22760] train loss: 0.2138, train acc: 0.9231, val loss: 0.3502, val acc: 0.9447  (best train acc: 0.9420, best val acc: 0.9474, best train loss: 0.1680)\n",
      "[Epoch: 22780] train loss: 0.1937, train acc: 0.9318, val loss: 0.3652, val acc: 0.9423  (best train acc: 0.9420, best val acc: 0.9474, best train loss: 0.1680)\n",
      "[Epoch: 22800] train loss: 0.1879, train acc: 0.9333, val loss: 0.3755, val acc: 0.9417  (best train acc: 0.9420, best val acc: 0.9474, best train loss: 0.1680)\n",
      "[Epoch: 22820] train loss: 0.1872, train acc: 0.9305, val loss: 0.3753, val acc: 0.9433  (best train acc: 0.9420, best val acc: 0.9474, best train loss: 0.1680)\n",
      "[Epoch: 22840] train loss: 0.2150, train acc: 0.9206, val loss: 0.3641, val acc: 0.9430  (best train acc: 0.9420, best val acc: 0.9477, best train loss: 0.1680)\n",
      "[Epoch: 22860] train loss: 0.1868, train acc: 0.9313, val loss: 0.3728, val acc: 0.9406  (best train acc: 0.9420, best val acc: 0.9477, best train loss: 0.1680)\n",
      "[Epoch: 22880] train loss: 0.1843, train acc: 0.9345, val loss: 0.3896, val acc: 0.9396  (best train acc: 0.9420, best val acc: 0.9477, best train loss: 0.1680)\n",
      "[Epoch: 22900] train loss: 0.1879, train acc: 0.9317, val loss: 0.3218, val acc: 0.9325  (best train acc: 0.9420, best val acc: 0.9477, best train loss: 0.1680)\n",
      "[Epoch: 22920] train loss: 0.1880, train acc: 0.9341, val loss: 0.3404, val acc: 0.9423  (best train acc: 0.9420, best val acc: 0.9477, best train loss: 0.1680)\n",
      "[Epoch: 22940] train loss: 0.1769, train acc: 0.9355, val loss: 0.3591, val acc: 0.9430  (best train acc: 0.9420, best val acc: 0.9484, best train loss: 0.1680)\n",
      "[Epoch: 22960] train loss: 0.1859, train acc: 0.9333, val loss: 0.3708, val acc: 0.9417  (best train acc: 0.9420, best val acc: 0.9484, best train loss: 0.1663)\n",
      "[Epoch: 22980] train loss: 0.1783, train acc: 0.9361, val loss: 0.3664, val acc: 0.9444  (best train acc: 0.9420, best val acc: 0.9484, best train loss: 0.1663)\n",
      "[Epoch: 23000] train loss: 0.1984, train acc: 0.9325, val loss: 0.4005, val acc: 0.9251  (best train acc: 0.9420, best val acc: 0.9484, best train loss: 0.1663)\n",
      "[Epoch: 23020] train loss: 0.1955, train acc: 0.9314, val loss: 0.3592, val acc: 0.9352  (best train acc: 0.9420, best val acc: 0.9484, best train loss: 0.1663)\n",
      "[Epoch: 23040] train loss: 0.1852, train acc: 0.9359, val loss: 0.3503, val acc: 0.9413  (best train acc: 0.9420, best val acc: 0.9484, best train loss: 0.1663)\n",
      "[Epoch: 23060] train loss: 0.1975, train acc: 0.9281, val loss: 0.3401, val acc: 0.9420  (best train acc: 0.9420, best val acc: 0.9484, best train loss: 0.1663)\n",
      "[Epoch: 23080] train loss: 0.1797, train acc: 0.9357, val loss: 0.3577, val acc: 0.9403  (best train acc: 0.9420, best val acc: 0.9484, best train loss: 0.1663)\n",
      "[Epoch: 23100] train loss: 0.1734, train acc: 0.9385, val loss: 0.3610, val acc: 0.9433  (best train acc: 0.9420, best val acc: 0.9484, best train loss: 0.1663)\n",
      "[Epoch: 23120] train loss: 0.1752, train acc: 0.9341, val loss: 0.3523, val acc: 0.9420  (best train acc: 0.9420, best val acc: 0.9484, best train loss: 0.1663)\n",
      "[Epoch: 23140] train loss: 0.1940, train acc: 0.9281, val loss: 0.3827, val acc: 0.9346  (best train acc: 0.9420, best val acc: 0.9484, best train loss: 0.1663)\n",
      "[Epoch: 23160] train loss: 0.1888, train acc: 0.9344, val loss: 0.3548, val acc: 0.9413  (best train acc: 0.9420, best val acc: 0.9484, best train loss: 0.1663)\n",
      "[Epoch: 23180] train loss: 0.1866, train acc: 0.9337, val loss: 0.3869, val acc: 0.9356  (best train acc: 0.9420, best val acc: 0.9484, best train loss: 0.1663)\n",
      "[Epoch: 23200] train loss: 0.1776, train acc: 0.9357, val loss: 0.3930, val acc: 0.9410  (best train acc: 0.9420, best val acc: 0.9484, best train loss: 0.1663)\n",
      "[Epoch: 23220] train loss: 0.1964, train acc: 0.9278, val loss: 0.3771, val acc: 0.9376  (best train acc: 0.9420, best val acc: 0.9484, best train loss: 0.1663)\n",
      "[Epoch: 23240] train loss: 0.1792, train acc: 0.9347, val loss: 0.3697, val acc: 0.9444  (best train acc: 0.9426, best val acc: 0.9484, best train loss: 0.1617)\n",
      "[Epoch: 23260] train loss: 0.1988, train acc: 0.9281, val loss: 0.3517, val acc: 0.9417  (best train acc: 0.9426, best val acc: 0.9484, best train loss: 0.1617)\n",
      "[Epoch: 23280] train loss: 0.1830, train acc: 0.9365, val loss: 0.3373, val acc: 0.9349  (best train acc: 0.9426, best val acc: 0.9484, best train loss: 0.1617)\n",
      "[Epoch: 23300] train loss: 0.1776, train acc: 0.9349, val loss: 0.3400, val acc: 0.9454  (best train acc: 0.9426, best val acc: 0.9484, best train loss: 0.1617)\n",
      "[Epoch: 23320] train loss: 0.1923, train acc: 0.9268, val loss: 0.3422, val acc: 0.9440  (best train acc: 0.9426, best val acc: 0.9484, best train loss: 0.1617)\n",
      "[Epoch: 23340] train loss: 0.1768, train acc: 0.9384, val loss: 0.3381, val acc: 0.9440  (best train acc: 0.9426, best val acc: 0.9484, best train loss: 0.1617)\n",
      "[Epoch: 23360] train loss: 0.1796, train acc: 0.9364, val loss: 0.3662, val acc: 0.9413  (best train acc: 0.9426, best val acc: 0.9484, best train loss: 0.1617)\n",
      "[Epoch: 23380] train loss: 0.1710, train acc: 0.9388, val loss: 0.3544, val acc: 0.9413  (best train acc: 0.9426, best val acc: 0.9494, best train loss: 0.1617)\n",
      "[Epoch: 23400] train loss: 0.1736, train acc: 0.9362, val loss: 0.3669, val acc: 0.9450  (best train acc: 0.9426, best val acc: 0.9494, best train loss: 0.1617)\n",
      "[Epoch: 23420] train loss: 0.1929, train acc: 0.9301, val loss: 0.3733, val acc: 0.9390  (best train acc: 0.9426, best val acc: 0.9494, best train loss: 0.1617)\n",
      "[Epoch: 23440] train loss: 0.1749, train acc: 0.9352, val loss: 0.3649, val acc: 0.9447  (best train acc: 0.9426, best val acc: 0.9494, best train loss: 0.1617)\n",
      "[Epoch: 23460] train loss: 0.1710, train acc: 0.9386, val loss: 0.3680, val acc: 0.9464  (best train acc: 0.9426, best val acc: 0.9494, best train loss: 0.1617)\n",
      "[Epoch: 23480] train loss: 0.1773, train acc: 0.9352, val loss: 0.3794, val acc: 0.9450  (best train acc: 0.9426, best val acc: 0.9494, best train loss: 0.1617)\n",
      "[Epoch: 23500] train loss: 0.1809, train acc: 0.9342, val loss: 0.3647, val acc: 0.9430  (best train acc: 0.9426, best val acc: 0.9494, best train loss: 0.1617)\n",
      "[Epoch: 23520] train loss: 0.1895, train acc: 0.9291, val loss: 0.3676, val acc: 0.9447  (best train acc: 0.9426, best val acc: 0.9494, best train loss: 0.1617)\n",
      "[Epoch: 23540] train loss: 0.1959, train acc: 0.9294, val loss: 0.3893, val acc: 0.9430  (best train acc: 0.9426, best val acc: 0.9494, best train loss: 0.1617)\n",
      "[Epoch: 23560] train loss: 0.1772, train acc: 0.9385, val loss: 0.3719, val acc: 0.9460  (best train acc: 0.9426, best val acc: 0.9494, best train loss: 0.1617)\n",
      "[Epoch: 23580] train loss: 0.1837, train acc: 0.9362, val loss: 0.3805, val acc: 0.9460  (best train acc: 0.9426, best val acc: 0.9494, best train loss: 0.1617)\n",
      "[Epoch: 23600] train loss: 0.1777, train acc: 0.9363, val loss: 0.3972, val acc: 0.9440  (best train acc: 0.9426, best val acc: 0.9494, best train loss: 0.1617)\n",
      "[Epoch: 23620] train loss: 0.1821, train acc: 0.9344, val loss: 0.3728, val acc: 0.9427  (best train acc: 0.9426, best val acc: 0.9494, best train loss: 0.1617)\n",
      "[Epoch: 23640] train loss: 0.1930, train acc: 0.9320, val loss: 0.3856, val acc: 0.9390  (best train acc: 0.9426, best val acc: 0.9494, best train loss: 0.1617)\n",
      "[Epoch: 23660] train loss: 0.1838, train acc: 0.9352, val loss: 0.3670, val acc: 0.9454  (best train acc: 0.9426, best val acc: 0.9494, best train loss: 0.1617)\n",
      "[Epoch: 23680] train loss: 0.1807, train acc: 0.9343, val loss: 0.3662, val acc: 0.9413  (best train acc: 0.9427, best val acc: 0.9494, best train loss: 0.1617)\n",
      "[Epoch: 23700] train loss: 0.1777, train acc: 0.9372, val loss: 0.3659, val acc: 0.9433  (best train acc: 0.9427, best val acc: 0.9497, best train loss: 0.1617)\n",
      "[Epoch: 23720] train loss: 0.1813, train acc: 0.9344, val loss: 0.3453, val acc: 0.9292  (best train acc: 0.9427, best val acc: 0.9497, best train loss: 0.1617)\n",
      "[Epoch: 23740] train loss: 0.1752, train acc: 0.9373, val loss: 0.3368, val acc: 0.9460  (best train acc: 0.9427, best val acc: 0.9497, best train loss: 0.1617)\n",
      "[Epoch: 23760] train loss: 0.1813, train acc: 0.9341, val loss: 0.3691, val acc: 0.9420  (best train acc: 0.9427, best val acc: 0.9497, best train loss: 0.1617)\n",
      "[Epoch: 23780] train loss: 0.1770, train acc: 0.9380, val loss: 0.3871, val acc: 0.9444  (best train acc: 0.9427, best val acc: 0.9497, best train loss: 0.1617)\n",
      "[Epoch: 23800] train loss: 0.1895, train acc: 0.9291, val loss: 0.3903, val acc: 0.9440  (best train acc: 0.9427, best val acc: 0.9497, best train loss: 0.1617)\n",
      "[Epoch: 23820] train loss: 0.1801, train acc: 0.9359, val loss: 0.4017, val acc: 0.9386  (best train acc: 0.9427, best val acc: 0.9497, best train loss: 0.1617)\n",
      "[Epoch: 23840] train loss: 0.1889, train acc: 0.9320, val loss: 0.3500, val acc: 0.9457  (best train acc: 0.9427, best val acc: 0.9497, best train loss: 0.1617)\n",
      "[Epoch: 23860] train loss: 0.1792, train acc: 0.9298, val loss: 0.3609, val acc: 0.9417  (best train acc: 0.9427, best val acc: 0.9497, best train loss: 0.1617)\n",
      "[Epoch: 23880] train loss: 0.1878, train acc: 0.9315, val loss: 0.3466, val acc: 0.9440  (best train acc: 0.9427, best val acc: 0.9497, best train loss: 0.1617)\n",
      "[Epoch: 23900] train loss: 0.1792, train acc: 0.9346, val loss: 0.3836, val acc: 0.9403  (best train acc: 0.9427, best val acc: 0.9497, best train loss: 0.1617)\n",
      "[Epoch: 23920] train loss: 0.1895, train acc: 0.9325, val loss: 0.3726, val acc: 0.9410  (best train acc: 0.9427, best val acc: 0.9497, best train loss: 0.1617)\n",
      "[Epoch: 23940] train loss: 0.1749, train acc: 0.9367, val loss: 0.3876, val acc: 0.9450  (best train acc: 0.9427, best val acc: 0.9497, best train loss: 0.1617)\n",
      "[Epoch: 23960] train loss: 0.1819, train acc: 0.9361, val loss: 0.3883, val acc: 0.9427  (best train acc: 0.9427, best val acc: 0.9497, best train loss: 0.1617)\n",
      "[Epoch: 23980] train loss: 0.1738, train acc: 0.9388, val loss: 0.3926, val acc: 0.9433  (best train acc: 0.9427, best val acc: 0.9497, best train loss: 0.1617)\n",
      "[Epoch: 24000] train loss: 0.1678, train acc: 0.9420, val loss: 0.3952, val acc: 0.9440  (best train acc: 0.9431, best val acc: 0.9497, best train loss: 0.1617)\n",
      "[Epoch: 24020] train loss: 0.1768, train acc: 0.9379, val loss: 0.3953, val acc: 0.9444  (best train acc: 0.9431, best val acc: 0.9497, best train loss: 0.1617)\n",
      "[Epoch: 24040] train loss: 0.1905, train acc: 0.9286, val loss: 0.4090, val acc: 0.9457  (best train acc: 0.9431, best val acc: 0.9497, best train loss: 0.1617)\n",
      "[Epoch: 24060] train loss: 0.1813, train acc: 0.9376, val loss: 0.3792, val acc: 0.9430  (best train acc: 0.9431, best val acc: 0.9497, best train loss: 0.1617)\n",
      "[Epoch: 24080] train loss: 0.1776, train acc: 0.9372, val loss: 0.3584, val acc: 0.9420  (best train acc: 0.9431, best val acc: 0.9497, best train loss: 0.1617)\n",
      "[Epoch: 24100] train loss: 0.1761, train acc: 0.9380, val loss: 0.3897, val acc: 0.9481  (best train acc: 0.9431, best val acc: 0.9497, best train loss: 0.1617)\n",
      "[Epoch: 24120] train loss: 0.1814, train acc: 0.9349, val loss: 0.4014, val acc: 0.9467  (best train acc: 0.9431, best val acc: 0.9497, best train loss: 0.1617)\n",
      "[Epoch: 24140] train loss: 0.1727, train acc: 0.9432, val loss: 0.3897, val acc: 0.9460  (best train acc: 0.9437, best val acc: 0.9497, best train loss: 0.1613)\n",
      "[Epoch: 24160] train loss: 0.1912, train acc: 0.9283, val loss: 0.3889, val acc: 0.9447  (best train acc: 0.9437, best val acc: 0.9497, best train loss: 0.1613)\n",
      "[Epoch: 24180] train loss: 0.1775, train acc: 0.9356, val loss: 0.4077, val acc: 0.9433  (best train acc: 0.9437, best val acc: 0.9497, best train loss: 0.1613)\n",
      "[Epoch: 24200] train loss: 0.1751, train acc: 0.9359, val loss: 0.3960, val acc: 0.9433  (best train acc: 0.9437, best val acc: 0.9497, best train loss: 0.1613)\n",
      "[Epoch: 24220] train loss: 0.2025, train acc: 0.9245, val loss: 0.3993, val acc: 0.9470  (best train acc: 0.9437, best val acc: 0.9497, best train loss: 0.1613)\n",
      "[Epoch: 24240] train loss: 0.1771, train acc: 0.9390, val loss: 0.3978, val acc: 0.9447  (best train acc: 0.9437, best val acc: 0.9497, best train loss: 0.1613)\n",
      "[Epoch: 24260] train loss: 0.1954, train acc: 0.9258, val loss: 0.3836, val acc: 0.9410  (best train acc: 0.9437, best val acc: 0.9497, best train loss: 0.1613)\n",
      "[Epoch: 24280] train loss: 0.1837, train acc: 0.9324, val loss: 0.3733, val acc: 0.9433  (best train acc: 0.9437, best val acc: 0.9497, best train loss: 0.1613)\n",
      "[Epoch: 24300] train loss: 0.1948, train acc: 0.9277, val loss: 0.3695, val acc: 0.9454  (best train acc: 0.9437, best val acc: 0.9497, best train loss: 0.1613)\n",
      "[Epoch: 24320] train loss: 0.1825, train acc: 0.9315, val loss: 0.3985, val acc: 0.9444  (best train acc: 0.9437, best val acc: 0.9497, best train loss: 0.1613)\n",
      "[Epoch: 24340] train loss: 0.1799, train acc: 0.9365, val loss: 0.4045, val acc: 0.9410  (best train acc: 0.9437, best val acc: 0.9497, best train loss: 0.1613)\n",
      "[Epoch: 24360] train loss: 0.1753, train acc: 0.9362, val loss: 0.4170, val acc: 0.9430  (best train acc: 0.9437, best val acc: 0.9497, best train loss: 0.1613)\n",
      "[Epoch: 24380] train loss: 0.1712, train acc: 0.9367, val loss: 0.4144, val acc: 0.9460  (best train acc: 0.9437, best val acc: 0.9497, best train loss: 0.1613)\n",
      "[Epoch: 24400] train loss: 0.1722, train acc: 0.9412, val loss: 0.4008, val acc: 0.9390  (best train acc: 0.9437, best val acc: 0.9497, best train loss: 0.1613)\n",
      "[Epoch: 24420] train loss: 0.1823, train acc: 0.9370, val loss: 0.3610, val acc: 0.9423  (best train acc: 0.9437, best val acc: 0.9497, best train loss: 0.1613)\n",
      "[Epoch: 24440] train loss: 0.1734, train acc: 0.9399, val loss: 0.4249, val acc: 0.9417  (best train acc: 0.9437, best val acc: 0.9497, best train loss: 0.1613)\n",
      "[Epoch: 24460] train loss: 0.1786, train acc: 0.9397, val loss: 0.3780, val acc: 0.9467  (best train acc: 0.9437, best val acc: 0.9497, best train loss: 0.1613)\n",
      "[Epoch: 24480] train loss: 0.1611, train acc: 0.9421, val loss: 0.3897, val acc: 0.9477  (best train acc: 0.9437, best val acc: 0.9497, best train loss: 0.1611)\n",
      "[Epoch: 24500] train loss: 0.1775, train acc: 0.9351, val loss: 0.3839, val acc: 0.9457  (best train acc: 0.9437, best val acc: 0.9497, best train loss: 0.1611)\n",
      "[Epoch: 24520] train loss: 0.1760, train acc: 0.9382, val loss: 0.3952, val acc: 0.9433  (best train acc: 0.9437, best val acc: 0.9497, best train loss: 0.1611)\n",
      "[Epoch: 24540] train loss: 0.1755, train acc: 0.9369, val loss: 0.4073, val acc: 0.9302  (best train acc: 0.9437, best val acc: 0.9497, best train loss: 0.1611)\n",
      "[Epoch: 24560] train loss: 0.1783, train acc: 0.9353, val loss: 0.3915, val acc: 0.9433  (best train acc: 0.9437, best val acc: 0.9497, best train loss: 0.1611)\n",
      "[Epoch: 24580] train loss: 0.1708, train acc: 0.9401, val loss: 0.3977, val acc: 0.9470  (best train acc: 0.9437, best val acc: 0.9497, best train loss: 0.1611)\n",
      "[Epoch: 24600] train loss: 0.2002, train acc: 0.9221, val loss: 0.3960, val acc: 0.9450  (best train acc: 0.9437, best val acc: 0.9497, best train loss: 0.1611)\n",
      "[Epoch: 24620] train loss: 0.1956, train acc: 0.9324, val loss: 0.4270, val acc: 0.9410  (best train acc: 0.9437, best val acc: 0.9497, best train loss: 0.1611)\n",
      "[Epoch: 24640] train loss: 0.1645, train acc: 0.9412, val loss: 0.4099, val acc: 0.9464  (best train acc: 0.9437, best val acc: 0.9497, best train loss: 0.1611)\n",
      "[Epoch: 24660] train loss: 0.1722, train acc: 0.9396, val loss: 0.3936, val acc: 0.9417  (best train acc: 0.9442, best val acc: 0.9508, best train loss: 0.1611)\n",
      "[Epoch: 24680] train loss: 0.1871, train acc: 0.9323, val loss: 0.3967, val acc: 0.9460  (best train acc: 0.9442, best val acc: 0.9508, best train loss: 0.1611)\n",
      "[Epoch: 24700] train loss: 0.1743, train acc: 0.9391, val loss: 0.4067, val acc: 0.9413  (best train acc: 0.9442, best val acc: 0.9508, best train loss: 0.1611)\n",
      "[Epoch: 24720] train loss: 0.1644, train acc: 0.9412, val loss: 0.4023, val acc: 0.9447  (best train acc: 0.9442, best val acc: 0.9508, best train loss: 0.1611)\n",
      "[Epoch: 24740] train loss: 0.1827, train acc: 0.9326, val loss: 0.3593, val acc: 0.9450  (best train acc: 0.9442, best val acc: 0.9508, best train loss: 0.1611)\n",
      "[Epoch: 24760] train loss: 0.1677, train acc: 0.9417, val loss: 0.3759, val acc: 0.9467  (best train acc: 0.9442, best val acc: 0.9508, best train loss: 0.1611)\n",
      "[Epoch: 24780] train loss: 0.1945, train acc: 0.9326, val loss: 0.3785, val acc: 0.9376  (best train acc: 0.9442, best val acc: 0.9508, best train loss: 0.1596)\n",
      "[Epoch: 24800] train loss: 0.1756, train acc: 0.9372, val loss: 0.3850, val acc: 0.9464  (best train acc: 0.9442, best val acc: 0.9508, best train loss: 0.1596)\n",
      "[Epoch: 24820] train loss: 0.1785, train acc: 0.9381, val loss: 0.3868, val acc: 0.9467  (best train acc: 0.9457, best val acc: 0.9508, best train loss: 0.1589)\n",
      "[Epoch: 24840] train loss: 0.1726, train acc: 0.9380, val loss: 0.3967, val acc: 0.9430  (best train acc: 0.9457, best val acc: 0.9508, best train loss: 0.1589)\n",
      "[Epoch: 24860] train loss: 0.1745, train acc: 0.9349, val loss: 0.3951, val acc: 0.9440  (best train acc: 0.9457, best val acc: 0.9508, best train loss: 0.1589)\n",
      "[Epoch: 24880] train loss: 0.1655, train acc: 0.9428, val loss: 0.3937, val acc: 0.9464  (best train acc: 0.9457, best val acc: 0.9508, best train loss: 0.1589)\n",
      "[Epoch: 24900] train loss: 0.1763, train acc: 0.9376, val loss: 0.3952, val acc: 0.9457  (best train acc: 0.9457, best val acc: 0.9508, best train loss: 0.1589)\n",
      "[Epoch: 24920] train loss: 0.1791, train acc: 0.9337, val loss: 0.3985, val acc: 0.9470  (best train acc: 0.9457, best val acc: 0.9508, best train loss: 0.1589)\n",
      "[Epoch: 24940] train loss: 0.1704, train acc: 0.9363, val loss: 0.3956, val acc: 0.9413  (best train acc: 0.9457, best val acc: 0.9508, best train loss: 0.1589)\n",
      "[Epoch: 24960] train loss: 0.1847, train acc: 0.9334, val loss: 0.4147, val acc: 0.9460  (best train acc: 0.9457, best val acc: 0.9508, best train loss: 0.1589)\n",
      "[Epoch: 24980] train loss: 0.1733, train acc: 0.9372, val loss: 0.4145, val acc: 0.9474  (best train acc: 0.9457, best val acc: 0.9508, best train loss: 0.1589)\n",
      "[Epoch: 25000] train loss: 0.1699, train acc: 0.9385, val loss: 0.4059, val acc: 0.9470  (best train acc: 0.9457, best val acc: 0.9508, best train loss: 0.1589)\n",
      "[Epoch: 25020] train loss: 0.1955, train acc: 0.9303, val loss: 0.4036, val acc: 0.9410  (best train acc: 0.9457, best val acc: 0.9508, best train loss: 0.1589)\n",
      "[Epoch: 25040] train loss: 0.1949, train acc: 0.9289, val loss: 0.4220, val acc: 0.9467  (best train acc: 0.9457, best val acc: 0.9508, best train loss: 0.1589)\n",
      "[Epoch: 25060] train loss: 0.1791, train acc: 0.9349, val loss: 0.4032, val acc: 0.9481  (best train acc: 0.9457, best val acc: 0.9508, best train loss: 0.1589)\n",
      "[Epoch: 25080] train loss: 0.1685, train acc: 0.9406, val loss: 0.3994, val acc: 0.9491  (best train acc: 0.9457, best val acc: 0.9508, best train loss: 0.1589)\n",
      "[Epoch: 25100] train loss: 0.1668, train acc: 0.9403, val loss: 0.3963, val acc: 0.9457  (best train acc: 0.9457, best val acc: 0.9508, best train loss: 0.1589)\n",
      "[Epoch: 25120] train loss: 0.1661, train acc: 0.9396, val loss: 0.4074, val acc: 0.9470  (best train acc: 0.9457, best val acc: 0.9508, best train loss: 0.1589)\n",
      "[Epoch: 25140] train loss: 0.1871, train acc: 0.9336, val loss: 0.4130, val acc: 0.9383  (best train acc: 0.9457, best val acc: 0.9508, best train loss: 0.1589)\n",
      "[Epoch: 25160] train loss: 0.1870, train acc: 0.9346, val loss: 0.4505, val acc: 0.9430  (best train acc: 0.9457, best val acc: 0.9508, best train loss: 0.1589)\n",
      "[Epoch: 25180] train loss: 0.1874, train acc: 0.9302, val loss: 0.4347, val acc: 0.9390  (best train acc: 0.9457, best val acc: 0.9508, best train loss: 0.1589)\n",
      "[Epoch: 25200] train loss: 0.1642, train acc: 0.9419, val loss: 0.4269, val acc: 0.9454  (best train acc: 0.9457, best val acc: 0.9508, best train loss: 0.1589)\n",
      "[Epoch: 25220] train loss: 0.1732, train acc: 0.9414, val loss: 0.4268, val acc: 0.9481  (best train acc: 0.9457, best val acc: 0.9511, best train loss: 0.1589)\n",
      "[Epoch: 25240] train loss: 0.1782, train acc: 0.9343, val loss: 0.4342, val acc: 0.9460  (best train acc: 0.9457, best val acc: 0.9511, best train loss: 0.1589)\n",
      "[Epoch: 25260] train loss: 0.2595, train acc: 0.8907, val loss: 0.3932, val acc: 0.9133  (best train acc: 0.9457, best val acc: 0.9511, best train loss: 0.1589)\n",
      "[Epoch: 25280] train loss: 0.1893, train acc: 0.9305, val loss: 0.4123, val acc: 0.9444  (best train acc: 0.9457, best val acc: 0.9511, best train loss: 0.1589)\n",
      "[Epoch: 25300] train loss: 0.1780, train acc: 0.9359, val loss: 0.3877, val acc: 0.9454  (best train acc: 0.9457, best val acc: 0.9511, best train loss: 0.1589)\n",
      "[Epoch: 25320] train loss: 0.1808, train acc: 0.9335, val loss: 0.3797, val acc: 0.9427  (best train acc: 0.9457, best val acc: 0.9511, best train loss: 0.1589)\n",
      "[Epoch: 25340] train loss: 0.1727, train acc: 0.9392, val loss: 0.4031, val acc: 0.9504  (best train acc: 0.9457, best val acc: 0.9511, best train loss: 0.1589)\n",
      "[Epoch: 25360] train loss: 0.1740, train acc: 0.9410, val loss: 0.4020, val acc: 0.9487  (best train acc: 0.9457, best val acc: 0.9511, best train loss: 0.1589)\n",
      "[Epoch: 25380] train loss: 0.1746, train acc: 0.9379, val loss: 0.4088, val acc: 0.9474  (best train acc: 0.9457, best val acc: 0.9511, best train loss: 0.1589)\n",
      "[Epoch: 25400] train loss: 0.1778, train acc: 0.9376, val loss: 0.4284, val acc: 0.9477  (best train acc: 0.9457, best val acc: 0.9511, best train loss: 0.1589)\n",
      "[Epoch: 25420] train loss: 0.1819, train acc: 0.9331, val loss: 0.3462, val acc: 0.9481  (best train acc: 0.9457, best val acc: 0.9511, best train loss: 0.1589)\n",
      "[Epoch: 25440] train loss: 0.1690, train acc: 0.9423, val loss: 0.3817, val acc: 0.9477  (best train acc: 0.9457, best val acc: 0.9511, best train loss: 0.1589)\n",
      "[Epoch: 25460] train loss: 0.1828, train acc: 0.9347, val loss: 0.3915, val acc: 0.9437  (best train acc: 0.9457, best val acc: 0.9511, best train loss: 0.1589)\n",
      "[Epoch: 25480] train loss: 0.1772, train acc: 0.9370, val loss: 0.4048, val acc: 0.9467  (best train acc: 0.9457, best val acc: 0.9511, best train loss: 0.1589)\n",
      "[Epoch: 25500] train loss: 0.1688, train acc: 0.9401, val loss: 0.3858, val acc: 0.9433  (best train acc: 0.9457, best val acc: 0.9511, best train loss: 0.1589)\n",
      "[Epoch: 25520] train loss: 0.1751, train acc: 0.9395, val loss: 0.3932, val acc: 0.9417  (best train acc: 0.9457, best val acc: 0.9511, best train loss: 0.1589)\n",
      "[Epoch: 25540] train loss: 0.1764, train acc: 0.9370, val loss: 0.3887, val acc: 0.9487  (best train acc: 0.9457, best val acc: 0.9511, best train loss: 0.1589)\n",
      "[Epoch: 25560] train loss: 0.1746, train acc: 0.9372, val loss: 0.3841, val acc: 0.9437  (best train acc: 0.9457, best val acc: 0.9511, best train loss: 0.1589)\n",
      "[Epoch: 25580] train loss: 0.1651, train acc: 0.9420, val loss: 0.4039, val acc: 0.9481  (best train acc: 0.9457, best val acc: 0.9511, best train loss: 0.1589)\n",
      "[Epoch: 25600] train loss: 0.1714, train acc: 0.9382, val loss: 0.4228, val acc: 0.9447  (best train acc: 0.9457, best val acc: 0.9511, best train loss: 0.1589)\n",
      "[Epoch: 25620] train loss: 0.1653, train acc: 0.9425, val loss: 0.4079, val acc: 0.9464  (best train acc: 0.9457, best val acc: 0.9511, best train loss: 0.1589)\n",
      "[Epoch: 25640] train loss: 0.1653, train acc: 0.9420, val loss: 0.4099, val acc: 0.9464  (best train acc: 0.9457, best val acc: 0.9511, best train loss: 0.1589)\n",
      "[Epoch: 25660] train loss: 0.1675, train acc: 0.9392, val loss: 0.4223, val acc: 0.9477  (best train acc: 0.9457, best val acc: 0.9511, best train loss: 0.1589)\n",
      "[Epoch: 25680] train loss: 0.1828, train acc: 0.9380, val loss: 0.4683, val acc: 0.9413  (best train acc: 0.9457, best val acc: 0.9511, best train loss: 0.1589)\n",
      "[Epoch: 25700] train loss: 0.1774, train acc: 0.9401, val loss: 0.4298, val acc: 0.9417  (best train acc: 0.9457, best val acc: 0.9511, best train loss: 0.1589)\n",
      "[Epoch: 25720] train loss: 0.1887, train acc: 0.9328, val loss: 0.3803, val acc: 0.9457  (best train acc: 0.9457, best val acc: 0.9511, best train loss: 0.1589)\n",
      "[Epoch: 25740] train loss: 0.1722, train acc: 0.9398, val loss: 0.4341, val acc: 0.9477  (best train acc: 0.9457, best val acc: 0.9511, best train loss: 0.1589)\n",
      "[Epoch: 25760] train loss: 0.1774, train acc: 0.9341, val loss: 0.4183, val acc: 0.9457  (best train acc: 0.9457, best val acc: 0.9511, best train loss: 0.1589)\n",
      "[Epoch: 25780] train loss: 0.1780, train acc: 0.9375, val loss: 0.4078, val acc: 0.9420  (best train acc: 0.9457, best val acc: 0.9511, best train loss: 0.1589)\n",
      "[Epoch: 25800] train loss: 0.1651, train acc: 0.9410, val loss: 0.4184, val acc: 0.9467  (best train acc: 0.9457, best val acc: 0.9511, best train loss: 0.1589)\n",
      "[Epoch: 25820] train loss: 0.1733, train acc: 0.9401, val loss: 0.4295, val acc: 0.9467  (best train acc: 0.9457, best val acc: 0.9511, best train loss: 0.1589)\n",
      "[Epoch: 25840] train loss: 0.1645, train acc: 0.9426, val loss: 0.4089, val acc: 0.9454  (best train acc: 0.9457, best val acc: 0.9511, best train loss: 0.1589)\n",
      "[Epoch: 25860] train loss: 0.1830, train acc: 0.9340, val loss: 0.4159, val acc: 0.9464  (best train acc: 0.9457, best val acc: 0.9511, best train loss: 0.1589)\n",
      "[Epoch: 25880] train loss: 0.1720, train acc: 0.9367, val loss: 0.4200, val acc: 0.9450  (best train acc: 0.9457, best val acc: 0.9511, best train loss: 0.1589)\n",
      "[Epoch: 25900] train loss: 0.1672, train acc: 0.9415, val loss: 0.4250, val acc: 0.9501  (best train acc: 0.9457, best val acc: 0.9511, best train loss: 0.1589)\n",
      "[Epoch: 25920] train loss: 0.1885, train acc: 0.9326, val loss: 0.4180, val acc: 0.9417  (best train acc: 0.9457, best val acc: 0.9511, best train loss: 0.1589)\n",
      "[Epoch: 25940] train loss: 0.2096, train acc: 0.9149, val loss: 0.3699, val acc: 0.9315  (best train acc: 0.9457, best val acc: 0.9511, best train loss: 0.1589)\n",
      "[Epoch: 25960] train loss: 0.1720, train acc: 0.9394, val loss: 0.3943, val acc: 0.9464  (best train acc: 0.9457, best val acc: 0.9511, best train loss: 0.1589)\n",
      "[Epoch: 25980] train loss: 0.1684, train acc: 0.9391, val loss: 0.3957, val acc: 0.9319  (best train acc: 0.9457, best val acc: 0.9511, best train loss: 0.1589)\n",
      "[Epoch: 26000] train loss: 0.1834, train acc: 0.9365, val loss: 0.4378, val acc: 0.9450  (best train acc: 0.9457, best val acc: 0.9511, best train loss: 0.1589)\n",
      "[Epoch: 26020] train loss: 0.1694, train acc: 0.9378, val loss: 0.3937, val acc: 0.9467  (best train acc: 0.9457, best val acc: 0.9514, best train loss: 0.1589)\n",
      "[Epoch: 26040] train loss: 0.1828, train acc: 0.9323, val loss: 0.4045, val acc: 0.9470  (best train acc: 0.9457, best val acc: 0.9514, best train loss: 0.1589)\n",
      "[Epoch: 26060] train loss: 0.1853, train acc: 0.9307, val loss: 0.4031, val acc: 0.9491  (best train acc: 0.9459, best val acc: 0.9514, best train loss: 0.1531)\n",
      "[Epoch: 26080] train loss: 0.1717, train acc: 0.9381, val loss: 0.4192, val acc: 0.9450  (best train acc: 0.9478, best val acc: 0.9514, best train loss: 0.1531)\n",
      "[Epoch: 26100] train loss: 0.1676, train acc: 0.9433, val loss: 0.4033, val acc: 0.9460  (best train acc: 0.9478, best val acc: 0.9514, best train loss: 0.1531)\n",
      "[Epoch: 26120] train loss: 0.1738, train acc: 0.9385, val loss: 0.4421, val acc: 0.9470  (best train acc: 0.9478, best val acc: 0.9514, best train loss: 0.1531)\n",
      "[Epoch: 26140] train loss: 0.1944, train acc: 0.9252, val loss: 0.4308, val acc: 0.9484  (best train acc: 0.9478, best val acc: 0.9514, best train loss: 0.1531)\n",
      "[Epoch: 26160] train loss: 0.1762, train acc: 0.9378, val loss: 0.3922, val acc: 0.9410  (best train acc: 0.9478, best val acc: 0.9514, best train loss: 0.1531)\n",
      "[Epoch: 26180] train loss: 0.1643, train acc: 0.9438, val loss: 0.4003, val acc: 0.9501  (best train acc: 0.9478, best val acc: 0.9514, best train loss: 0.1531)\n",
      "[Epoch: 26200] train loss: 0.2034, train acc: 0.9294, val loss: 0.4004, val acc: 0.9454  (best train acc: 0.9478, best val acc: 0.9514, best train loss: 0.1531)\n",
      "[Epoch: 26220] train loss: 0.1743, train acc: 0.9395, val loss: 0.4152, val acc: 0.9497  (best train acc: 0.9478, best val acc: 0.9514, best train loss: 0.1531)\n",
      "[Epoch: 26240] train loss: 0.1730, train acc: 0.9398, val loss: 0.4231, val acc: 0.9497  (best train acc: 0.9478, best val acc: 0.9514, best train loss: 0.1531)\n",
      "[Epoch: 26260] train loss: 0.1699, train acc: 0.9386, val loss: 0.4243, val acc: 0.9474  (best train acc: 0.9478, best val acc: 0.9514, best train loss: 0.1531)\n",
      "[Epoch: 26280] train loss: 0.1697, train acc: 0.9411, val loss: 0.4411, val acc: 0.9470  (best train acc: 0.9478, best val acc: 0.9514, best train loss: 0.1531)\n",
      "[Epoch: 26300] train loss: 0.1708, train acc: 0.9404, val loss: 0.4407, val acc: 0.9484  (best train acc: 0.9478, best val acc: 0.9514, best train loss: 0.1531)\n",
      "[Epoch: 26320] train loss: 0.2089, train acc: 0.9287, val loss: 0.4483, val acc: 0.9356  (best train acc: 0.9478, best val acc: 0.9514, best train loss: 0.1531)\n",
      "[Epoch: 26340] train loss: 0.1752, train acc: 0.9376, val loss: 0.3937, val acc: 0.9481  (best train acc: 0.9478, best val acc: 0.9514, best train loss: 0.1531)\n",
      "[Epoch: 26360] train loss: 0.1668, train acc: 0.9410, val loss: 0.4177, val acc: 0.9427  (best train acc: 0.9478, best val acc: 0.9514, best train loss: 0.1531)\n",
      "[Epoch: 26380] train loss: 0.1646, train acc: 0.9435, val loss: 0.4099, val acc: 0.9484  (best train acc: 0.9478, best val acc: 0.9514, best train loss: 0.1531)\n",
      "[Epoch: 26400] train loss: 0.1721, train acc: 0.9427, val loss: 0.4226, val acc: 0.9491  (best train acc: 0.9478, best val acc: 0.9514, best train loss: 0.1531)\n",
      "[Epoch: 26420] train loss: 0.1631, train acc: 0.9399, val loss: 0.4181, val acc: 0.9423  (best train acc: 0.9478, best val acc: 0.9514, best train loss: 0.1531)\n",
      "[Epoch: 26440] train loss: 0.1819, train acc: 0.9342, val loss: 0.3955, val acc: 0.9427  (best train acc: 0.9478, best val acc: 0.9514, best train loss: 0.1531)\n",
      "[Epoch: 26460] train loss: 0.1920, train acc: 0.9294, val loss: 0.3741, val acc: 0.9440  (best train acc: 0.9478, best val acc: 0.9514, best train loss: 0.1531)\n",
      "[Epoch: 26480] train loss: 0.1776, train acc: 0.9364, val loss: 0.3893, val acc: 0.9460  (best train acc: 0.9478, best val acc: 0.9514, best train loss: 0.1531)\n",
      "[Epoch: 26500] train loss: 0.1755, train acc: 0.9359, val loss: 0.4197, val acc: 0.9427  (best train acc: 0.9478, best val acc: 0.9514, best train loss: 0.1531)\n",
      "[Epoch: 26520] train loss: 0.1745, train acc: 0.9383, val loss: 0.3885, val acc: 0.9406  (best train acc: 0.9478, best val acc: 0.9514, best train loss: 0.1531)\n",
      "[Epoch: 26540] train loss: 0.1698, train acc: 0.9397, val loss: 0.4303, val acc: 0.9484  (best train acc: 0.9478, best val acc: 0.9514, best train loss: 0.1531)\n",
      "[Epoch: 26560] train loss: 0.1684, train acc: 0.9426, val loss: 0.3971, val acc: 0.9474  (best train acc: 0.9478, best val acc: 0.9514, best train loss: 0.1531)\n",
      "[Epoch: 26580] train loss: 0.1710, train acc: 0.9353, val loss: 0.4014, val acc: 0.9481  (best train acc: 0.9478, best val acc: 0.9514, best train loss: 0.1531)\n",
      "[Epoch: 26600] train loss: 0.1734, train acc: 0.9351, val loss: 0.4283, val acc: 0.9481  (best train acc: 0.9478, best val acc: 0.9514, best train loss: 0.1531)\n",
      "[Epoch: 26620] train loss: 0.1933, train acc: 0.9258, val loss: 0.4127, val acc: 0.9494  (best train acc: 0.9478, best val acc: 0.9514, best train loss: 0.1531)\n",
      "[Epoch: 26640] train loss: 0.1883, train acc: 0.9359, val loss: 0.4006, val acc: 0.9487  (best train acc: 0.9478, best val acc: 0.9514, best train loss: 0.1531)\n",
      "[Epoch: 26660] train loss: 0.1669, train acc: 0.9414, val loss: 0.4270, val acc: 0.9477  (best train acc: 0.9478, best val acc: 0.9514, best train loss: 0.1531)\n",
      "[Epoch: 26680] train loss: 0.1619, train acc: 0.9418, val loss: 0.4289, val acc: 0.9494  (best train acc: 0.9478, best val acc: 0.9514, best train loss: 0.1531)\n",
      "[Epoch: 26700] train loss: 0.1710, train acc: 0.9389, val loss: 0.4302, val acc: 0.9494  (best train acc: 0.9478, best val acc: 0.9514, best train loss: 0.1531)\n",
      "[Epoch: 26720] train loss: 0.1809, train acc: 0.9356, val loss: 0.4218, val acc: 0.9487  (best train acc: 0.9478, best val acc: 0.9514, best train loss: 0.1531)\n",
      "[Epoch: 26740] train loss: 0.1744, train acc: 0.9385, val loss: 0.4111, val acc: 0.9460  (best train acc: 0.9478, best val acc: 0.9514, best train loss: 0.1531)\n",
      "[Epoch: 26760] train loss: 0.1827, train acc: 0.9344, val loss: 0.4072, val acc: 0.9464  (best train acc: 0.9478, best val acc: 0.9514, best train loss: 0.1531)\n",
      "[Epoch: 26780] train loss: 0.1778, train acc: 0.9340, val loss: 0.3670, val acc: 0.9484  (best train acc: 0.9478, best val acc: 0.9514, best train loss: 0.1531)\n",
      "[Epoch: 26800] train loss: 0.1771, train acc: 0.9377, val loss: 0.4291, val acc: 0.9430  (best train acc: 0.9478, best val acc: 0.9514, best train loss: 0.1531)\n",
      "[Epoch: 26820] train loss: 0.1728, train acc: 0.9355, val loss: 0.4267, val acc: 0.9481  (best train acc: 0.9478, best val acc: 0.9514, best train loss: 0.1531)\n",
      "[Epoch: 26840] train loss: 0.1684, train acc: 0.9388, val loss: 0.3988, val acc: 0.9477  (best train acc: 0.9478, best val acc: 0.9514, best train loss: 0.1531)\n",
      "[Epoch: 26860] train loss: 0.1630, train acc: 0.9426, val loss: 0.4241, val acc: 0.9467  (best train acc: 0.9478, best val acc: 0.9514, best train loss: 0.1531)\n",
      "[Epoch: 26880] train loss: 0.1802, train acc: 0.9344, val loss: 0.4243, val acc: 0.9481  (best train acc: 0.9478, best val acc: 0.9514, best train loss: 0.1531)\n",
      "[Epoch: 26900] train loss: 0.1560, train acc: 0.9432, val loss: 0.4293, val acc: 0.9491  (best train acc: 0.9478, best val acc: 0.9514, best train loss: 0.1531)\n",
      "[Epoch: 26920] train loss: 0.1843, train acc: 0.9331, val loss: 0.4240, val acc: 0.9427  (best train acc: 0.9478, best val acc: 0.9514, best train loss: 0.1531)\n",
      "[Epoch: 26940] train loss: 0.1712, train acc: 0.9413, val loss: 0.4089, val acc: 0.9470  (best train acc: 0.9478, best val acc: 0.9514, best train loss: 0.1531)\n",
      "[Epoch: 26960] train loss: 0.1744, train acc: 0.9394, val loss: 0.3966, val acc: 0.9484  (best train acc: 0.9478, best val acc: 0.9514, best train loss: 0.1531)\n",
      "[Epoch: 26980] train loss: 0.1863, train acc: 0.9346, val loss: 0.3915, val acc: 0.9427  (best train acc: 0.9478, best val acc: 0.9514, best train loss: 0.1531)\n",
      "[Epoch: 27000] train loss: 0.1606, train acc: 0.9440, val loss: 0.4099, val acc: 0.9497  (best train acc: 0.9478, best val acc: 0.9514, best train loss: 0.1531)\n",
      "[Epoch: 27020] train loss: 0.1717, train acc: 0.9372, val loss: 0.4192, val acc: 0.9497  (best train acc: 0.9478, best val acc: 0.9514, best train loss: 0.1531)\n",
      "[Epoch: 27040] train loss: 0.1716, train acc: 0.9423, val loss: 0.4204, val acc: 0.9467  (best train acc: 0.9478, best val acc: 0.9518, best train loss: 0.1531)\n",
      "[Epoch: 27060] train loss: 0.1737, train acc: 0.9413, val loss: 0.3878, val acc: 0.9440  (best train acc: 0.9478, best val acc: 0.9518, best train loss: 0.1531)\n",
      "[Epoch: 27080] train loss: 0.1606, train acc: 0.9441, val loss: 0.3989, val acc: 0.9484  (best train acc: 0.9478, best val acc: 0.9518, best train loss: 0.1531)\n",
      "[Epoch: 27100] train loss: 0.1706, train acc: 0.9395, val loss: 0.4054, val acc: 0.9420  (best train acc: 0.9478, best val acc: 0.9518, best train loss: 0.1531)\n",
      "[Epoch: 27120] train loss: 0.1660, train acc: 0.9415, val loss: 0.3713, val acc: 0.9447  (best train acc: 0.9478, best val acc: 0.9518, best train loss: 0.1531)\n",
      "[Epoch: 27140] train loss: 0.1720, train acc: 0.9373, val loss: 0.3924, val acc: 0.9484  (best train acc: 0.9478, best val acc: 0.9518, best train loss: 0.1531)\n",
      "[Epoch: 27160] train loss: 0.1625, train acc: 0.9429, val loss: 0.4041, val acc: 0.9504  (best train acc: 0.9478, best val acc: 0.9524, best train loss: 0.1531)\n",
      "[Epoch: 27180] train loss: 0.1734, train acc: 0.9389, val loss: 0.4236, val acc: 0.9474  (best train acc: 0.9478, best val acc: 0.9524, best train loss: 0.1531)\n",
      "[Epoch: 27200] train loss: 0.1742, train acc: 0.9383, val loss: 0.4255, val acc: 0.9481  (best train acc: 0.9478, best val acc: 0.9524, best train loss: 0.1531)\n",
      "[Epoch: 27220] train loss: 0.1765, train acc: 0.9366, val loss: 0.4191, val acc: 0.9497  (best train acc: 0.9478, best val acc: 0.9524, best train loss: 0.1531)\n",
      "[Epoch: 27240] train loss: 0.1666, train acc: 0.9413, val loss: 0.4099, val acc: 0.9487  (best train acc: 0.9478, best val acc: 0.9524, best train loss: 0.1531)\n",
      "[Epoch: 27260] train loss: 0.2005, train acc: 0.9258, val loss: 0.4128, val acc: 0.9373  (best train acc: 0.9478, best val acc: 0.9524, best train loss: 0.1531)\n",
      "[Epoch: 27280] train loss: 0.1613, train acc: 0.9440, val loss: 0.4385, val acc: 0.9484  (best train acc: 0.9478, best val acc: 0.9524, best train loss: 0.1531)\n",
      "[Epoch: 27300] train loss: 0.1815, train acc: 0.9365, val loss: 0.4319, val acc: 0.9467  (best train acc: 0.9478, best val acc: 0.9524, best train loss: 0.1531)\n",
      "[Epoch: 27320] train loss: 0.1756, train acc: 0.9345, val loss: 0.4185, val acc: 0.9484  (best train acc: 0.9478, best val acc: 0.9524, best train loss: 0.1531)\n",
      "[Epoch: 27340] train loss: 0.1766, train acc: 0.9344, val loss: 0.4236, val acc: 0.9454  (best train acc: 0.9478, best val acc: 0.9524, best train loss: 0.1531)\n",
      "[Epoch: 27360] train loss: 0.1710, train acc: 0.9394, val loss: 0.4039, val acc: 0.9484  (best train acc: 0.9478, best val acc: 0.9524, best train loss: 0.1531)\n",
      "[Epoch: 27380] train loss: 0.1749, train acc: 0.9357, val loss: 0.4155, val acc: 0.9487  (best train acc: 0.9478, best val acc: 0.9524, best train loss: 0.1531)\n",
      "[Epoch: 27400] train loss: 0.1724, train acc: 0.9346, val loss: 0.3864, val acc: 0.9484  (best train acc: 0.9478, best val acc: 0.9524, best train loss: 0.1531)\n",
      "[Epoch: 27420] train loss: 0.2489, train acc: 0.8968, val loss: 0.3927, val acc: 0.9218  (best train acc: 0.9478, best val acc: 0.9524, best train loss: 0.1531)\n",
      "[Epoch: 27440] train loss: 0.1949, train acc: 0.9286, val loss: 0.3545, val acc: 0.9464  (best train acc: 0.9478, best val acc: 0.9524, best train loss: 0.1531)\n",
      "[Epoch: 27460] train loss: 0.1753, train acc: 0.9375, val loss: 0.4094, val acc: 0.9511  (best train acc: 0.9478, best val acc: 0.9524, best train loss: 0.1531)\n",
      "[Epoch: 27480] train loss: 0.1641, train acc: 0.9405, val loss: 0.4093, val acc: 0.9491  (best train acc: 0.9478, best val acc: 0.9524, best train loss: 0.1531)\n",
      "[Epoch: 27500] train loss: 0.1666, train acc: 0.9420, val loss: 0.4110, val acc: 0.9460  (best train acc: 0.9478, best val acc: 0.9524, best train loss: 0.1531)\n",
      "[Epoch: 27520] train loss: 0.1715, train acc: 0.9358, val loss: 0.4100, val acc: 0.9454  (best train acc: 0.9478, best val acc: 0.9524, best train loss: 0.1531)\n",
      "[Epoch: 27540] train loss: 0.1983, train acc: 0.9291, val loss: 0.4203, val acc: 0.9410  (best train acc: 0.9478, best val acc: 0.9524, best train loss: 0.1531)\n",
      "[Epoch: 27560] train loss: 0.1622, train acc: 0.9414, val loss: 0.4350, val acc: 0.9450  (best train acc: 0.9478, best val acc: 0.9524, best train loss: 0.1531)\n",
      "[Epoch: 27580] train loss: 0.1777, train acc: 0.9377, val loss: 0.3911, val acc: 0.9423  (best train acc: 0.9478, best val acc: 0.9524, best train loss: 0.1531)\n",
      "[Epoch: 27600] train loss: 0.1732, train acc: 0.9371, val loss: 0.4262, val acc: 0.9494  (best train acc: 0.9478, best val acc: 0.9524, best train loss: 0.1531)\n",
      "[Epoch: 27620] train loss: 0.1657, train acc: 0.9451, val loss: 0.3931, val acc: 0.9484  (best train acc: 0.9478, best val acc: 0.9524, best train loss: 0.1531)\n",
      "[Epoch: 27640] train loss: 0.1805, train acc: 0.9350, val loss: 0.4017, val acc: 0.9464  (best train acc: 0.9478, best val acc: 0.9524, best train loss: 0.1531)\n",
      "[Epoch: 27660] train loss: 0.1581, train acc: 0.9460, val loss: 0.4044, val acc: 0.9508  (best train acc: 0.9478, best val acc: 0.9524, best train loss: 0.1531)\n",
      "[Epoch: 27680] train loss: 0.1847, train acc: 0.9330, val loss: 0.3889, val acc: 0.9474  (best train acc: 0.9478, best val acc: 0.9524, best train loss: 0.1531)\n",
      "[Epoch: 27700] train loss: 0.1936, train acc: 0.9318, val loss: 0.3476, val acc: 0.9440  (best train acc: 0.9478, best val acc: 0.9524, best train loss: 0.1531)\n",
      "[Epoch: 27720] train loss: 0.1667, train acc: 0.9405, val loss: 0.3895, val acc: 0.9491  (best train acc: 0.9478, best val acc: 0.9524, best train loss: 0.1531)\n",
      "[Epoch: 27740] train loss: 0.1738, train acc: 0.9358, val loss: 0.3941, val acc: 0.9396  (best train acc: 0.9478, best val acc: 0.9524, best train loss: 0.1531)\n",
      "[Epoch: 27760] train loss: 0.1754, train acc: 0.9383, val loss: 0.4210, val acc: 0.9440  (best train acc: 0.9478, best val acc: 0.9524, best train loss: 0.1531)\n",
      "[Epoch: 27780] train loss: 0.1581, train acc: 0.9433, val loss: 0.4093, val acc: 0.9497  (best train acc: 0.9478, best val acc: 0.9524, best train loss: 0.1531)\n",
      "[Epoch: 27800] train loss: 0.1710, train acc: 0.9390, val loss: 0.4177, val acc: 0.9484  (best train acc: 0.9478, best val acc: 0.9524, best train loss: 0.1531)\n",
      "[Epoch: 27820] train loss: 0.1684, train acc: 0.9412, val loss: 0.3951, val acc: 0.9477  (best train acc: 0.9478, best val acc: 0.9524, best train loss: 0.1531)\n",
      "[Epoch: 27840] train loss: 0.1671, train acc: 0.9427, val loss: 0.3942, val acc: 0.9477  (best train acc: 0.9478, best val acc: 0.9524, best train loss: 0.1531)\n",
      "[Epoch: 27860] train loss: 0.1684, train acc: 0.9421, val loss: 0.4340, val acc: 0.9477  (best train acc: 0.9478, best val acc: 0.9524, best train loss: 0.1531)\n",
      "[Epoch: 27880] train loss: 0.1716, train acc: 0.9400, val loss: 0.4117, val acc: 0.9437  (best train acc: 0.9478, best val acc: 0.9524, best train loss: 0.1531)\n",
      "[Epoch: 27900] train loss: 0.1691, train acc: 0.9359, val loss: 0.4171, val acc: 0.9470  (best train acc: 0.9478, best val acc: 0.9524, best train loss: 0.1531)\n",
      "[Epoch: 27920] train loss: 0.1725, train acc: 0.9370, val loss: 0.4224, val acc: 0.9501  (best train acc: 0.9478, best val acc: 0.9524, best train loss: 0.1531)\n",
      "[Epoch: 27940] train loss: 0.1802, train acc: 0.9354, val loss: 0.4068, val acc: 0.9433  (best train acc: 0.9478, best val acc: 0.9524, best train loss: 0.1531)\n",
      "[Epoch: 27960] train loss: 0.1661, train acc: 0.9372, val loss: 0.4060, val acc: 0.9497  (best train acc: 0.9478, best val acc: 0.9524, best train loss: 0.1531)\n",
      "[Epoch: 27980] train loss: 0.1750, train acc: 0.9387, val loss: 0.4301, val acc: 0.9508  (best train acc: 0.9478, best val acc: 0.9524, best train loss: 0.1531)\n",
      "[Epoch: 28000] train loss: 0.1705, train acc: 0.9391, val loss: 0.4344, val acc: 0.9494  (best train acc: 0.9478, best val acc: 0.9524, best train loss: 0.1531)\n",
      "[Epoch: 28020] train loss: 0.1611, train acc: 0.9434, val loss: 0.4330, val acc: 0.9501  (best train acc: 0.9478, best val acc: 0.9524, best train loss: 0.1531)\n",
      "[Epoch: 28040] train loss: 0.1802, train acc: 0.9345, val loss: 0.4156, val acc: 0.9484  (best train acc: 0.9478, best val acc: 0.9524, best train loss: 0.1531)\n",
      "[Epoch: 28060] train loss: 0.1705, train acc: 0.9405, val loss: 0.4367, val acc: 0.9396  (best train acc: 0.9478, best val acc: 0.9524, best train loss: 0.1531)\n",
      "[Epoch: 28080] train loss: 0.1734, train acc: 0.9384, val loss: 0.4219, val acc: 0.9433  (best train acc: 0.9478, best val acc: 0.9524, best train loss: 0.1531)\n",
      "[Epoch: 28100] train loss: 0.1594, train acc: 0.9438, val loss: 0.4367, val acc: 0.9501  (best train acc: 0.9478, best val acc: 0.9524, best train loss: 0.1531)\n",
      "[Epoch: 28120] train loss: 0.2057, train acc: 0.9288, val loss: 0.4606, val acc: 0.9322  (best train acc: 0.9478, best val acc: 0.9524, best train loss: 0.1531)\n",
      "[Epoch: 28140] train loss: 0.1723, train acc: 0.9374, val loss: 0.4121, val acc: 0.9379  (best train acc: 0.9478, best val acc: 0.9524, best train loss: 0.1531)\n",
      "[Epoch: 28160] train loss: 0.2063, train acc: 0.9297, val loss: 0.4579, val acc: 0.9325  (best train acc: 0.9478, best val acc: 0.9524, best train loss: 0.1531)\n",
      "[Epoch: 28180] train loss: 0.1684, train acc: 0.9400, val loss: 0.4086, val acc: 0.9457  (best train acc: 0.9478, best val acc: 0.9524, best train loss: 0.1531)\n",
      "[Epoch: 28200] train loss: 0.1663, train acc: 0.9392, val loss: 0.3830, val acc: 0.9470  (best train acc: 0.9478, best val acc: 0.9524, best train loss: 0.1531)\n",
      "[Epoch: 28220] train loss: 0.1688, train acc: 0.9396, val loss: 0.4528, val acc: 0.9514  (best train acc: 0.9478, best val acc: 0.9524, best train loss: 0.1531)\n",
      "[Epoch: 28240] train loss: 0.1683, train acc: 0.9366, val loss: 0.3910, val acc: 0.9474  (best train acc: 0.9478, best val acc: 0.9524, best train loss: 0.1531)\n",
      "[Epoch: 28260] train loss: 0.1677, train acc: 0.9398, val loss: 0.4201, val acc: 0.9501  (best train acc: 0.9478, best val acc: 0.9524, best train loss: 0.1531)\n",
      "[Epoch: 28280] train loss: 0.1793, train acc: 0.9349, val loss: 0.3970, val acc: 0.9484  (best train acc: 0.9478, best val acc: 0.9524, best train loss: 0.1531)\n",
      "[Epoch: 28300] train loss: 0.1831, train acc: 0.9338, val loss: 0.4104, val acc: 0.9497  (best train acc: 0.9478, best val acc: 0.9524, best train loss: 0.1531)\n",
      "[Epoch: 28320] train loss: 0.1734, train acc: 0.9388, val loss: 0.4307, val acc: 0.9491  (best train acc: 0.9478, best val acc: 0.9524, best train loss: 0.1508)\n",
      "[Epoch: 28340] train loss: 0.1766, train acc: 0.9350, val loss: 0.4022, val acc: 0.9444  (best train acc: 0.9478, best val acc: 0.9524, best train loss: 0.1508)\n",
      "[Epoch: 28360] train loss: 0.1776, train acc: 0.9345, val loss: 0.4406, val acc: 0.9447  (best train acc: 0.9478, best val acc: 0.9524, best train loss: 0.1508)\n",
      "[Epoch: 28380] train loss: 0.1726, train acc: 0.9388, val loss: 0.4131, val acc: 0.9393  (best train acc: 0.9478, best val acc: 0.9524, best train loss: 0.1508)\n",
      "[Epoch: 28400] train loss: 0.1657, train acc: 0.9388, val loss: 0.3934, val acc: 0.9390  (best train acc: 0.9478, best val acc: 0.9524, best train loss: 0.1508)\n",
      "[Epoch: 28420] train loss: 0.1787, train acc: 0.9380, val loss: 0.3933, val acc: 0.9467  (best train acc: 0.9478, best val acc: 0.9524, best train loss: 0.1508)\n",
      "[Epoch: 28440] train loss: 0.1651, train acc: 0.9432, val loss: 0.4049, val acc: 0.9504  (best train acc: 0.9478, best val acc: 0.9524, best train loss: 0.1508)\n",
      "[Epoch: 28460] train loss: 0.1618, train acc: 0.9427, val loss: 0.4143, val acc: 0.9487  (best train acc: 0.9478, best val acc: 0.9524, best train loss: 0.1508)\n",
      "[Epoch: 28480] train loss: 0.1765, train acc: 0.9388, val loss: 0.3899, val acc: 0.9403  (best train acc: 0.9478, best val acc: 0.9524, best train loss: 0.1508)\n",
      "[Epoch: 28500] train loss: 0.1609, train acc: 0.9434, val loss: 0.3965, val acc: 0.9474  (best train acc: 0.9478, best val acc: 0.9524, best train loss: 0.1508)\n",
      "[Epoch: 28520] train loss: 0.1860, train acc: 0.9354, val loss: 0.4159, val acc: 0.9447  (best train acc: 0.9478, best val acc: 0.9524, best train loss: 0.1508)\n",
      "[Epoch: 28540] train loss: 0.1664, train acc: 0.9423, val loss: 0.3957, val acc: 0.9487  (best train acc: 0.9478, best val acc: 0.9524, best train loss: 0.1508)\n",
      "[Epoch: 28560] train loss: 0.1727, train acc: 0.9390, val loss: 0.4208, val acc: 0.9491  (best train acc: 0.9478, best val acc: 0.9524, best train loss: 0.1508)\n",
      "[Epoch: 28580] train loss: 0.1635, train acc: 0.9396, val loss: 0.4093, val acc: 0.9450  (best train acc: 0.9478, best val acc: 0.9524, best train loss: 0.1508)\n",
      "[Epoch: 28600] train loss: 0.1686, train acc: 0.9391, val loss: 0.4090, val acc: 0.9363  (best train acc: 0.9478, best val acc: 0.9524, best train loss: 0.1508)\n",
      "[Epoch: 28620] train loss: 0.1605, train acc: 0.9427, val loss: 0.4433, val acc: 0.9423  (best train acc: 0.9478, best val acc: 0.9524, best train loss: 0.1508)\n",
      "[Epoch: 28640] train loss: 0.1680, train acc: 0.9409, val loss: 0.3841, val acc: 0.9467  (best train acc: 0.9478, best val acc: 0.9524, best train loss: 0.1508)\n",
      "[Epoch: 28660] train loss: 0.1659, train acc: 0.9415, val loss: 0.4232, val acc: 0.9447  (best train acc: 0.9478, best val acc: 0.9524, best train loss: 0.1508)\n",
      "[Epoch: 28680] train loss: 0.1635, train acc: 0.9419, val loss: 0.4225, val acc: 0.9511  (best train acc: 0.9478, best val acc: 0.9524, best train loss: 0.1508)\n",
      "[Epoch: 28700] train loss: 0.1722, train acc: 0.9380, val loss: 0.4068, val acc: 0.9379  (best train acc: 0.9478, best val acc: 0.9524, best train loss: 0.1508)\n",
      "[Epoch: 28720] train loss: 0.1813, train acc: 0.9327, val loss: 0.3946, val acc: 0.9352  (best train acc: 0.9478, best val acc: 0.9524, best train loss: 0.1508)\n",
      "[Epoch: 28740] train loss: 0.1758, train acc: 0.9396, val loss: 0.4052, val acc: 0.9501  (best train acc: 0.9478, best val acc: 0.9524, best train loss: 0.1508)\n",
      "[Epoch: 28760] train loss: 0.1589, train acc: 0.9448, val loss: 0.4158, val acc: 0.9417  (best train acc: 0.9478, best val acc: 0.9524, best train loss: 0.1508)\n",
      "[Epoch: 28780] train loss: 0.1637, train acc: 0.9411, val loss: 0.4212, val acc: 0.9410  (best train acc: 0.9478, best val acc: 0.9524, best train loss: 0.1508)\n",
      "[Epoch: 28800] train loss: 0.1667, train acc: 0.9434, val loss: 0.3894, val acc: 0.9497  (best train acc: 0.9478, best val acc: 0.9524, best train loss: 0.1508)\n",
      "[Epoch: 28820] train loss: 0.1591, train acc: 0.9447, val loss: 0.4293, val acc: 0.9494  (best train acc: 0.9478, best val acc: 0.9524, best train loss: 0.1508)\n",
      "[Epoch: 28840] train loss: 0.1663, train acc: 0.9419, val loss: 0.3979, val acc: 0.9504  (best train acc: 0.9478, best val acc: 0.9524, best train loss: 0.1508)\n",
      "[Epoch: 28860] train loss: 0.1707, train acc: 0.9364, val loss: 0.4171, val acc: 0.9491  (best train acc: 0.9478, best val acc: 0.9524, best train loss: 0.1508)\n",
      "[Epoch: 28880] train loss: 0.1637, train acc: 0.9434, val loss: 0.4166, val acc: 0.9501  (best train acc: 0.9478, best val acc: 0.9524, best train loss: 0.1508)\n",
      "[Epoch: 28900] train loss: 0.2082, train acc: 0.9255, val loss: 0.4127, val acc: 0.9417  (best train acc: 0.9478, best val acc: 0.9524, best train loss: 0.1508)\n",
      "[Epoch: 28920] train loss: 0.1667, train acc: 0.9404, val loss: 0.3864, val acc: 0.9447  (best train acc: 0.9478, best val acc: 0.9528, best train loss: 0.1508)\n",
      "[Epoch: 28940] train loss: 0.1757, train acc: 0.9359, val loss: 0.4023, val acc: 0.9467  (best train acc: 0.9478, best val acc: 0.9528, best train loss: 0.1508)\n",
      "[Epoch: 28960] train loss: 0.1663, train acc: 0.9404, val loss: 0.4086, val acc: 0.9457  (best train acc: 0.9478, best val acc: 0.9528, best train loss: 0.1508)\n",
      "[Epoch: 28980] train loss: 0.1686, train acc: 0.9414, val loss: 0.4117, val acc: 0.9487  (best train acc: 0.9478, best val acc: 0.9528, best train loss: 0.1508)\n",
      "[Epoch: 29000] train loss: 0.1878, train acc: 0.9297, val loss: 0.4069, val acc: 0.9491  (best train acc: 0.9478, best val acc: 0.9528, best train loss: 0.1508)\n",
      "[Epoch: 29020] train loss: 0.1679, train acc: 0.9393, val loss: 0.4100, val acc: 0.9487  (best train acc: 0.9478, best val acc: 0.9528, best train loss: 0.1508)\n",
      "[Epoch: 29040] train loss: 0.1792, train acc: 0.9322, val loss: 0.4173, val acc: 0.9454  (best train acc: 0.9478, best val acc: 0.9528, best train loss: 0.1508)\n",
      "[Epoch: 29060] train loss: 0.1656, train acc: 0.9400, val loss: 0.4363, val acc: 0.9487  (best train acc: 0.9478, best val acc: 0.9528, best train loss: 0.1508)\n",
      "[Epoch: 29080] train loss: 0.1597, train acc: 0.9426, val loss: 0.3948, val acc: 0.9511  (best train acc: 0.9478, best val acc: 0.9528, best train loss: 0.1508)\n",
      "[Epoch: 29100] train loss: 0.1609, train acc: 0.9432, val loss: 0.4152, val acc: 0.9511  (best train acc: 0.9478, best val acc: 0.9531, best train loss: 0.1508)\n",
      "[Epoch: 29120] train loss: 0.1659, train acc: 0.9408, val loss: 0.4307, val acc: 0.9484  (best train acc: 0.9478, best val acc: 0.9531, best train loss: 0.1508)\n",
      "[Epoch: 29140] train loss: 0.1786, train acc: 0.9333, val loss: 0.4366, val acc: 0.9356  (best train acc: 0.9478, best val acc: 0.9531, best train loss: 0.1508)\n",
      "[Epoch: 29160] train loss: 0.1839, train acc: 0.9285, val loss: 0.3217, val acc: 0.9450  (best train acc: 0.9478, best val acc: 0.9531, best train loss: 0.1508)\n",
      "[Epoch: 29180] train loss: 0.1610, train acc: 0.9433, val loss: 0.3563, val acc: 0.9491  (best train acc: 0.9478, best val acc: 0.9531, best train loss: 0.1508)\n",
      "[Epoch: 29200] train loss: 0.1559, train acc: 0.9446, val loss: 0.3878, val acc: 0.9481  (best train acc: 0.9478, best val acc: 0.9531, best train loss: 0.1508)\n",
      "[Epoch: 29220] train loss: 0.1761, train acc: 0.9345, val loss: 0.3606, val acc: 0.9450  (best train acc: 0.9478, best val acc: 0.9531, best train loss: 0.1508)\n",
      "[Epoch: 29240] train loss: 0.1950, train acc: 0.9247, val loss: 0.3646, val acc: 0.9470  (best train acc: 0.9478, best val acc: 0.9531, best train loss: 0.1508)\n",
      "[Epoch: 29260] train loss: 0.1741, train acc: 0.9383, val loss: 0.3893, val acc: 0.9487  (best train acc: 0.9478, best val acc: 0.9531, best train loss: 0.1508)\n",
      "[Epoch: 29280] train loss: 0.1782, train acc: 0.9375, val loss: 0.4104, val acc: 0.9464  (best train acc: 0.9478, best val acc: 0.9531, best train loss: 0.1508)\n",
      "[Epoch: 29300] train loss: 0.1672, train acc: 0.9406, val loss: 0.3920, val acc: 0.9470  (best train acc: 0.9478, best val acc: 0.9531, best train loss: 0.1508)\n",
      "[Epoch: 29320] train loss: 0.1655, train acc: 0.9427, val loss: 0.4092, val acc: 0.9504  (best train acc: 0.9478, best val acc: 0.9531, best train loss: 0.1508)\n",
      "[Epoch: 29340] train loss: 0.1652, train acc: 0.9408, val loss: 0.4189, val acc: 0.9501  (best train acc: 0.9478, best val acc: 0.9531, best train loss: 0.1508)\n",
      "[Epoch: 29360] train loss: 0.1742, train acc: 0.9396, val loss: 0.4135, val acc: 0.9474  (best train acc: 0.9478, best val acc: 0.9531, best train loss: 0.1508)\n",
      "[Epoch: 29380] train loss: 0.1659, train acc: 0.9402, val loss: 0.4078, val acc: 0.9437  (best train acc: 0.9478, best val acc: 0.9531, best train loss: 0.1508)\n",
      "[Epoch: 29400] train loss: 0.1667, train acc: 0.9380, val loss: 0.3898, val acc: 0.9427  (best train acc: 0.9478, best val acc: 0.9531, best train loss: 0.1508)\n",
      "[Epoch: 29420] train loss: 0.1589, train acc: 0.9426, val loss: 0.3902, val acc: 0.9494  (best train acc: 0.9478, best val acc: 0.9531, best train loss: 0.1508)\n",
      "[Epoch: 29440] train loss: 0.1657, train acc: 0.9401, val loss: 0.4049, val acc: 0.9514  (best train acc: 0.9478, best val acc: 0.9531, best train loss: 0.1508)\n",
      "[Epoch: 29460] train loss: 0.1993, train acc: 0.9218, val loss: 0.4472, val acc: 0.9298  (best train acc: 0.9478, best val acc: 0.9531, best train loss: 0.1508)\n",
      "[Epoch: 29480] train loss: 0.1673, train acc: 0.9426, val loss: 0.3879, val acc: 0.9518  (best train acc: 0.9478, best val acc: 0.9531, best train loss: 0.1508)\n",
      "[Epoch: 29500] train loss: 0.1639, train acc: 0.9402, val loss: 0.4100, val acc: 0.9484  (best train acc: 0.9478, best val acc: 0.9531, best train loss: 0.1508)\n",
      "[Epoch: 29520] train loss: 0.1616, train acc: 0.9405, val loss: 0.4156, val acc: 0.9383  (best train acc: 0.9478, best val acc: 0.9531, best train loss: 0.1508)\n",
      "[Epoch: 29540] train loss: 0.1784, train acc: 0.9398, val loss: 0.4244, val acc: 0.9444  (best train acc: 0.9478, best val acc: 0.9531, best train loss: 0.1508)\n",
      "[Epoch: 29560] train loss: 0.1645, train acc: 0.9412, val loss: 0.4041, val acc: 0.9494  (best train acc: 0.9478, best val acc: 0.9531, best train loss: 0.1508)\n",
      "[Epoch: 29580] train loss: 0.1710, train acc: 0.9404, val loss: 0.4068, val acc: 0.9491  (best train acc: 0.9478, best val acc: 0.9531, best train loss: 0.1508)\n",
      "[Epoch: 29600] train loss: 0.2057, train acc: 0.9240, val loss: 0.4003, val acc: 0.9481  (best train acc: 0.9478, best val acc: 0.9531, best train loss: 0.1508)\n",
      "[Epoch: 29620] train loss: 0.1672, train acc: 0.9398, val loss: 0.4205, val acc: 0.9410  (best train acc: 0.9478, best val acc: 0.9531, best train loss: 0.1508)\n",
      "[Epoch: 29640] train loss: 0.1696, train acc: 0.9380, val loss: 0.3955, val acc: 0.9457  (best train acc: 0.9478, best val acc: 0.9531, best train loss: 0.1508)\n",
      "[Epoch: 29660] train loss: 0.1597, train acc: 0.9419, val loss: 0.3859, val acc: 0.9470  (best train acc: 0.9478, best val acc: 0.9531, best train loss: 0.1508)\n",
      "[Epoch: 29680] train loss: 0.1750, train acc: 0.9336, val loss: 0.3852, val acc: 0.9450  (best train acc: 0.9478, best val acc: 0.9531, best train loss: 0.1508)\n",
      "[Epoch: 29700] train loss: 0.1859, train acc: 0.9335, val loss: 0.3543, val acc: 0.9457  (best train acc: 0.9478, best val acc: 0.9531, best train loss: 0.1508)\n",
      "[Epoch: 29720] train loss: 0.1595, train acc: 0.9406, val loss: 0.4311, val acc: 0.9494  (best train acc: 0.9478, best val acc: 0.9531, best train loss: 0.1508)\n",
      "[Epoch: 29740] train loss: 0.1700, train acc: 0.9401, val loss: 0.3845, val acc: 0.9423  (best train acc: 0.9478, best val acc: 0.9531, best train loss: 0.1508)\n",
      "[Epoch: 29760] train loss: 0.1636, train acc: 0.9456, val loss: 0.3908, val acc: 0.9454  (best train acc: 0.9478, best val acc: 0.9531, best train loss: 0.1508)\n",
      "[Epoch: 29780] train loss: 0.1666, train acc: 0.9422, val loss: 0.3962, val acc: 0.9511  (best train acc: 0.9478, best val acc: 0.9531, best train loss: 0.1508)\n",
      "[Epoch: 29800] train loss: 0.1799, train acc: 0.9351, val loss: 0.3842, val acc: 0.9454  (best train acc: 0.9478, best val acc: 0.9531, best train loss: 0.1508)\n",
      "[Epoch: 29820] train loss: 0.1602, train acc: 0.9441, val loss: 0.3996, val acc: 0.9504  (best train acc: 0.9478, best val acc: 0.9531, best train loss: 0.1508)\n",
      "[Epoch: 29840] train loss: 0.1683, train acc: 0.9414, val loss: 0.3904, val acc: 0.9494  (best train acc: 0.9478, best val acc: 0.9531, best train loss: 0.1508)\n",
      "[Epoch: 29860] train loss: 0.1710, train acc: 0.9425, val loss: 0.4221, val acc: 0.9474  (best train acc: 0.9478, best val acc: 0.9531, best train loss: 0.1508)\n",
      "[Epoch: 29880] train loss: 0.1619, train acc: 0.9427, val loss: 0.3794, val acc: 0.9460  (best train acc: 0.9478, best val acc: 0.9531, best train loss: 0.1508)\n",
      "[Epoch: 29900] train loss: 0.1685, train acc: 0.9426, val loss: 0.3539, val acc: 0.9484  (best train acc: 0.9478, best val acc: 0.9531, best train loss: 0.1508)\n",
      "[Epoch: 29920] train loss: 0.1686, train acc: 0.9407, val loss: 0.4019, val acc: 0.9484  (best train acc: 0.9478, best val acc: 0.9531, best train loss: 0.1508)\n",
      "[Epoch: 29940] train loss: 0.1689, train acc: 0.9402, val loss: 0.3703, val acc: 0.9494  (best train acc: 0.9478, best val acc: 0.9531, best train loss: 0.1508)\n",
      "[Epoch: 29960] train loss: 0.1668, train acc: 0.9400, val loss: 0.4573, val acc: 0.9454  (best train acc: 0.9478, best val acc: 0.9531, best train loss: 0.1508)\n",
      "[Epoch: 29980] train loss: 0.1662, train acc: 0.9400, val loss: 0.4144, val acc: 0.9464  (best train acc: 0.9478, best val acc: 0.9531, best train loss: 0.1508)\n",
      "[Epoch: 30000] train loss: 0.1939, train acc: 0.9254, val loss: 0.3671, val acc: 0.9440  (best train acc: 0.9478, best val acc: 0.9531, best train loss: 0.1508)\n",
      "[Epoch: 30020] train loss: 0.1714, train acc: 0.9390, val loss: 0.3786, val acc: 0.9501  (best train acc: 0.9478, best val acc: 0.9531, best train loss: 0.1508)\n",
      "[Epoch: 30040] train loss: 0.1729, train acc: 0.9369, val loss: 0.4090, val acc: 0.9484  (best train acc: 0.9478, best val acc: 0.9541, best train loss: 0.1508)\n",
      "[Epoch: 30060] train loss: 0.1659, train acc: 0.9444, val loss: 0.4117, val acc: 0.9491  (best train acc: 0.9478, best val acc: 0.9541, best train loss: 0.1508)\n",
      "[Epoch: 30080] train loss: 0.1693, train acc: 0.9362, val loss: 0.4086, val acc: 0.9497  (best train acc: 0.9478, best val acc: 0.9541, best train loss: 0.1508)\n",
      "[Epoch: 30100] train loss: 0.1614, train acc: 0.9406, val loss: 0.4125, val acc: 0.9467  (best train acc: 0.9478, best val acc: 0.9541, best train loss: 0.1508)\n",
      "[Epoch: 30120] train loss: 0.1687, train acc: 0.9393, val loss: 0.4238, val acc: 0.9494  (best train acc: 0.9478, best val acc: 0.9541, best train loss: 0.1508)\n",
      "[Epoch: 30140] train loss: 0.1722, train acc: 0.9394, val loss: 0.4037, val acc: 0.9504  (best train acc: 0.9478, best val acc: 0.9541, best train loss: 0.1508)\n",
      "[Epoch: 30160] train loss: 0.1714, train acc: 0.9402, val loss: 0.4118, val acc: 0.9430  (best train acc: 0.9478, best val acc: 0.9541, best train loss: 0.1508)\n",
      "[Epoch: 30180] train loss: 0.1680, train acc: 0.9417, val loss: 0.3956, val acc: 0.9413  (best train acc: 0.9478, best val acc: 0.9541, best train loss: 0.1508)\n",
      "[Epoch: 30200] train loss: 0.1634, train acc: 0.9439, val loss: 0.4348, val acc: 0.9470  (best train acc: 0.9478, best val acc: 0.9541, best train loss: 0.1508)\n",
      "[Epoch: 30220] train loss: 0.1609, train acc: 0.9425, val loss: 0.4090, val acc: 0.9379  (best train acc: 0.9478, best val acc: 0.9541, best train loss: 0.1508)\n",
      "[Epoch: 30240] train loss: 0.1742, train acc: 0.9354, val loss: 0.4418, val acc: 0.9444  (best train acc: 0.9478, best val acc: 0.9541, best train loss: 0.1508)\n",
      "[Epoch: 30260] train loss: 0.1666, train acc: 0.9414, val loss: 0.3719, val acc: 0.9487  (best train acc: 0.9478, best val acc: 0.9541, best train loss: 0.1508)\n",
      "[Epoch: 30280] train loss: 0.1539, train acc: 0.9441, val loss: 0.4120, val acc: 0.9504  (best train acc: 0.9478, best val acc: 0.9541, best train loss: 0.1508)\n",
      "[Epoch: 30300] train loss: 0.1793, train acc: 0.9350, val loss: 0.3823, val acc: 0.9494  (best train acc: 0.9478, best val acc: 0.9541, best train loss: 0.1508)\n",
      "[Epoch: 30320] train loss: 0.1696, train acc: 0.9367, val loss: 0.2963, val acc: 0.9497  (best train acc: 0.9478, best val acc: 0.9541, best train loss: 0.1508)\n",
      "[Epoch: 30340] train loss: 0.1588, train acc: 0.9453, val loss: 0.3783, val acc: 0.9504  (best train acc: 0.9478, best val acc: 0.9541, best train loss: 0.1508)\n",
      "[Epoch: 30360] train loss: 0.1682, train acc: 0.9410, val loss: 0.3437, val acc: 0.9494  (best train acc: 0.9478, best val acc: 0.9541, best train loss: 0.1508)\n",
      "[Epoch: 30380] train loss: 0.1638, train acc: 0.9419, val loss: 0.3591, val acc: 0.9474  (best train acc: 0.9482, best val acc: 0.9541, best train loss: 0.1508)\n",
      "[Epoch: 30400] train loss: 0.1662, train acc: 0.9396, val loss: 0.3593, val acc: 0.9511  (best train acc: 0.9482, best val acc: 0.9541, best train loss: 0.1508)\n",
      "[Epoch: 30420] train loss: 0.1555, train acc: 0.9462, val loss: 0.3769, val acc: 0.9491  (best train acc: 0.9482, best val acc: 0.9541, best train loss: 0.1508)\n",
      "[Epoch: 30440] train loss: 0.1606, train acc: 0.9437, val loss: 0.3872, val acc: 0.9440  (best train acc: 0.9482, best val acc: 0.9541, best train loss: 0.1508)\n",
      "[Epoch: 30460] train loss: 0.1693, train acc: 0.9371, val loss: 0.3836, val acc: 0.9481  (best train acc: 0.9482, best val acc: 0.9541, best train loss: 0.1508)\n",
      "[Epoch: 30480] train loss: 0.1703, train acc: 0.9387, val loss: 0.3946, val acc: 0.9444  (best train acc: 0.9482, best val acc: 0.9541, best train loss: 0.1508)\n",
      "[Epoch: 30500] train loss: 0.2007, train acc: 0.9232, val loss: 0.3830, val acc: 0.9464  (best train acc: 0.9482, best val acc: 0.9541, best train loss: 0.1508)\n",
      "[Epoch: 30520] train loss: 0.1699, train acc: 0.9417, val loss: 0.3669, val acc: 0.9444  (best train acc: 0.9482, best val acc: 0.9541, best train loss: 0.1508)\n",
      "[Epoch: 30540] train loss: 0.1597, train acc: 0.9431, val loss: 0.3921, val acc: 0.9497  (best train acc: 0.9482, best val acc: 0.9541, best train loss: 0.1508)\n",
      "[Epoch: 30560] train loss: 0.1644, train acc: 0.9434, val loss: 0.4051, val acc: 0.9501  (best train acc: 0.9482, best val acc: 0.9541, best train loss: 0.1508)\n",
      "[Epoch: 30580] train loss: 0.1679, train acc: 0.9381, val loss: 0.4014, val acc: 0.9511  (best train acc: 0.9482, best val acc: 0.9541, best train loss: 0.1508)\n",
      "[Epoch: 30600] train loss: 0.1662, train acc: 0.9411, val loss: 0.3651, val acc: 0.9494  (best train acc: 0.9482, best val acc: 0.9541, best train loss: 0.1508)\n",
      "[Epoch: 30620] train loss: 0.1811, train acc: 0.9382, val loss: 0.4331, val acc: 0.9390  (best train acc: 0.9482, best val acc: 0.9541, best train loss: 0.1508)\n",
      "[Epoch: 30640] train loss: 0.1646, train acc: 0.9444, val loss: 0.4141, val acc: 0.9511  (best train acc: 0.9482, best val acc: 0.9541, best train loss: 0.1508)\n",
      "[Epoch: 30660] train loss: 0.1641, train acc: 0.9420, val loss: 0.3988, val acc: 0.9484  (best train acc: 0.9482, best val acc: 0.9541, best train loss: 0.1508)\n",
      "[Epoch: 30680] train loss: 0.1741, train acc: 0.9383, val loss: 0.3992, val acc: 0.9413  (best train acc: 0.9482, best val acc: 0.9541, best train loss: 0.1508)\n",
      "[Epoch: 30700] train loss: 0.1658, train acc: 0.9385, val loss: 0.3974, val acc: 0.9359  (best train acc: 0.9482, best val acc: 0.9541, best train loss: 0.1508)\n",
      "[Epoch: 30720] train loss: 0.1729, train acc: 0.9378, val loss: 0.4014, val acc: 0.9494  (best train acc: 0.9482, best val acc: 0.9541, best train loss: 0.1508)\n",
      "[Epoch: 30740] train loss: 0.1627, train acc: 0.9425, val loss: 0.3956, val acc: 0.9504  (best train acc: 0.9482, best val acc: 0.9541, best train loss: 0.1508)\n",
      "[Epoch: 30760] train loss: 0.1704, train acc: 0.9380, val loss: 0.4227, val acc: 0.9508  (best train acc: 0.9482, best val acc: 0.9541, best train loss: 0.1508)\n",
      "[Epoch: 30780] train loss: 0.1595, train acc: 0.9438, val loss: 0.4139, val acc: 0.9491  (best train acc: 0.9482, best val acc: 0.9541, best train loss: 0.1508)\n",
      "[Epoch: 30800] train loss: 0.1722, train acc: 0.9375, val loss: 0.3834, val acc: 0.9497  (best train acc: 0.9482, best val acc: 0.9541, best train loss: 0.1508)\n",
      "[Epoch: 30820] train loss: 0.1776, train acc: 0.9365, val loss: 0.4249, val acc: 0.9501  (best train acc: 0.9482, best val acc: 0.9541, best train loss: 0.1508)\n",
      "[Epoch: 30840] train loss: 0.1618, train acc: 0.9435, val loss: 0.4203, val acc: 0.9511  (best train acc: 0.9482, best val acc: 0.9541, best train loss: 0.1508)\n",
      "[Epoch: 30860] train loss: 0.1610, train acc: 0.9447, val loss: 0.3980, val acc: 0.9487  (best train acc: 0.9482, best val acc: 0.9541, best train loss: 0.1508)\n",
      "[Epoch: 30880] train loss: 0.1635, train acc: 0.9410, val loss: 0.3996, val acc: 0.9481  (best train acc: 0.9482, best val acc: 0.9541, best train loss: 0.1508)\n",
      "[Epoch: 30900] train loss: 0.1704, train acc: 0.9382, val loss: 0.3942, val acc: 0.9487  (best train acc: 0.9482, best val acc: 0.9541, best train loss: 0.1508)\n",
      "[Epoch: 30920] train loss: 0.1562, train acc: 0.9443, val loss: 0.4211, val acc: 0.9491  (best train acc: 0.9482, best val acc: 0.9541, best train loss: 0.1508)\n",
      "[Epoch: 30940] train loss: 0.2469, train acc: 0.9106, val loss: 0.3761, val acc: 0.9363  (best train acc: 0.9482, best val acc: 0.9541, best train loss: 0.1508)\n",
      "[Epoch: 30960] train loss: 0.1797, train acc: 0.9389, val loss: 0.3963, val acc: 0.9420  (best train acc: 0.9482, best val acc: 0.9541, best train loss: 0.1508)\n",
      "[Epoch: 30980] train loss: 0.1854, train acc: 0.9329, val loss: 0.3468, val acc: 0.9504  (best train acc: 0.9482, best val acc: 0.9541, best train loss: 0.1508)\n",
      "[Epoch: 31000] train loss: 0.1592, train acc: 0.9435, val loss: 0.4012, val acc: 0.9497  (best train acc: 0.9482, best val acc: 0.9541, best train loss: 0.1508)\n",
      "[Epoch: 31020] train loss: 0.1617, train acc: 0.9413, val loss: 0.3888, val acc: 0.9491  (best train acc: 0.9482, best val acc: 0.9541, best train loss: 0.1508)\n",
      "[Epoch: 31040] train loss: 0.1696, train acc: 0.9393, val loss: 0.4106, val acc: 0.9433  (best train acc: 0.9482, best val acc: 0.9541, best train loss: 0.1508)\n",
      "[Epoch: 31060] train loss: 0.1700, train acc: 0.9388, val loss: 0.3579, val acc: 0.9393  (best train acc: 0.9482, best val acc: 0.9541, best train loss: 0.1508)\n",
      "[Epoch: 31080] train loss: 0.1803, train acc: 0.9351, val loss: 0.3942, val acc: 0.9501  (best train acc: 0.9482, best val acc: 0.9541, best train loss: 0.1508)\n",
      "[Epoch: 31100] train loss: 0.1555, train acc: 0.9459, val loss: 0.3674, val acc: 0.9474  (best train acc: 0.9482, best val acc: 0.9541, best train loss: 0.1508)\n",
      "[Epoch: 31120] train loss: 0.1643, train acc: 0.9409, val loss: 0.4051, val acc: 0.9460  (best train acc: 0.9482, best val acc: 0.9541, best train loss: 0.1508)\n",
      "[Epoch: 31140] train loss: 0.1803, train acc: 0.9344, val loss: 0.3418, val acc: 0.9400  (best train acc: 0.9482, best val acc: 0.9541, best train loss: 0.1508)\n",
      "[Epoch: 31160] train loss: 0.1660, train acc: 0.9402, val loss: 0.3775, val acc: 0.9501  (best train acc: 0.9482, best val acc: 0.9541, best train loss: 0.1508)\n",
      "[Epoch: 31180] train loss: 0.1652, train acc: 0.9432, val loss: 0.3821, val acc: 0.9511  (best train acc: 0.9482, best val acc: 0.9541, best train loss: 0.1508)\n",
      "[Epoch: 31200] train loss: 0.1609, train acc: 0.9450, val loss: 0.4053, val acc: 0.9474  (best train acc: 0.9482, best val acc: 0.9541, best train loss: 0.1508)\n",
      "[Epoch: 31220] train loss: 0.1725, train acc: 0.9367, val loss: 0.3685, val acc: 0.9470  (best train acc: 0.9482, best val acc: 0.9541, best train loss: 0.1508)\n",
      "[Epoch: 31240] train loss: 0.1586, train acc: 0.9432, val loss: 0.3985, val acc: 0.9521  (best train acc: 0.9482, best val acc: 0.9545, best train loss: 0.1508)\n",
      "[Epoch: 31260] train loss: 0.1639, train acc: 0.9401, val loss: 0.3655, val acc: 0.9511  (best train acc: 0.9482, best val acc: 0.9545, best train loss: 0.1508)\n",
      "[Epoch: 31280] train loss: 0.1610, train acc: 0.9451, val loss: 0.4017, val acc: 0.9491  (best train acc: 0.9482, best val acc: 0.9545, best train loss: 0.1508)\n",
      "[Epoch: 31300] train loss: 0.1740, train acc: 0.9352, val loss: 0.3781, val acc: 0.9494  (best train acc: 0.9482, best val acc: 0.9545, best train loss: 0.1508)\n",
      "[Epoch: 31320] train loss: 0.1733, train acc: 0.9406, val loss: 0.3782, val acc: 0.9494  (best train acc: 0.9482, best val acc: 0.9545, best train loss: 0.1508)\n",
      "[Epoch: 31340] train loss: 0.1628, train acc: 0.9409, val loss: 0.3815, val acc: 0.9501  (best train acc: 0.9482, best val acc: 0.9545, best train loss: 0.1508)\n",
      "[Epoch: 31360] train loss: 0.1668, train acc: 0.9420, val loss: 0.4000, val acc: 0.9447  (best train acc: 0.9482, best val acc: 0.9545, best train loss: 0.1508)\n",
      "[Epoch: 31380] train loss: 0.1622, train acc: 0.9426, val loss: 0.3842, val acc: 0.9514  (best train acc: 0.9482, best val acc: 0.9545, best train loss: 0.1508)\n",
      "[Epoch: 31400] train loss: 0.1706, train acc: 0.9391, val loss: 0.3869, val acc: 0.9494  (best train acc: 0.9482, best val acc: 0.9545, best train loss: 0.1508)\n",
      "[Epoch: 31420] train loss: 0.1726, train acc: 0.9357, val loss: 0.3927, val acc: 0.9460  (best train acc: 0.9482, best val acc: 0.9545, best train loss: 0.1508)\n",
      "[Epoch: 31440] train loss: 0.1603, train acc: 0.9445, val loss: 0.3850, val acc: 0.9504  (best train acc: 0.9482, best val acc: 0.9545, best train loss: 0.1508)\n",
      "[Epoch: 31460] train loss: 0.1698, train acc: 0.9397, val loss: 0.4060, val acc: 0.9457  (best train acc: 0.9482, best val acc: 0.9545, best train loss: 0.1508)\n",
      "[Epoch: 31480] train loss: 0.1615, train acc: 0.9429, val loss: 0.3833, val acc: 0.9470  (best train acc: 0.9482, best val acc: 0.9545, best train loss: 0.1508)\n",
      "[Epoch: 31500] train loss: 0.1663, train acc: 0.9417, val loss: 0.4224, val acc: 0.9511  (best train acc: 0.9482, best val acc: 0.9545, best train loss: 0.1508)\n",
      "[Epoch: 31520] train loss: 0.2539, train acc: 0.9332, val loss: 0.4122, val acc: 0.9106  (best train acc: 0.9482, best val acc: 0.9545, best train loss: 0.1508)\n",
      "[Epoch: 31540] train loss: 0.1902, train acc: 0.9331, val loss: 0.2961, val acc: 0.9437  (best train acc: 0.9482, best val acc: 0.9545, best train loss: 0.1508)\n",
      "[Epoch: 31560] train loss: 0.1721, train acc: 0.9401, val loss: 0.3608, val acc: 0.9504  (best train acc: 0.9482, best val acc: 0.9545, best train loss: 0.1508)\n",
      "[Epoch: 31580] train loss: 0.1656, train acc: 0.9398, val loss: 0.3465, val acc: 0.9501  (best train acc: 0.9482, best val acc: 0.9545, best train loss: 0.1508)\n",
      "[Epoch: 31600] train loss: 0.1661, train acc: 0.9409, val loss: 0.3767, val acc: 0.9477  (best train acc: 0.9482, best val acc: 0.9545, best train loss: 0.1508)\n",
      "[Epoch: 31620] train loss: 0.1825, train acc: 0.9360, val loss: 0.3859, val acc: 0.9484  (best train acc: 0.9482, best val acc: 0.9545, best train loss: 0.1508)\n",
      "[Epoch: 31640] train loss: 0.1698, train acc: 0.9401, val loss: 0.4065, val acc: 0.9460  (best train acc: 0.9482, best val acc: 0.9545, best train loss: 0.1508)\n",
      "[Epoch: 31660] train loss: 0.1645, train acc: 0.9444, val loss: 0.3648, val acc: 0.9511  (best train acc: 0.9482, best val acc: 0.9545, best train loss: 0.1508)\n",
      "[Epoch: 31680] train loss: 0.1700, train acc: 0.9392, val loss: 0.3760, val acc: 0.9491  (best train acc: 0.9482, best val acc: 0.9545, best train loss: 0.1508)\n",
      "[Epoch: 31700] train loss: 0.1591, train acc: 0.9452, val loss: 0.3408, val acc: 0.9514  (best train acc: 0.9482, best val acc: 0.9545, best train loss: 0.1508)\n",
      "[Epoch: 31720] train loss: 0.1677, train acc: 0.9397, val loss: 0.3898, val acc: 0.9504  (best train acc: 0.9482, best val acc: 0.9545, best train loss: 0.1508)\n",
      "[Epoch: 31740] train loss: 0.1871, train acc: 0.9365, val loss: 0.3855, val acc: 0.9352  (best train acc: 0.9482, best val acc: 0.9545, best train loss: 0.1508)\n",
      "[Epoch: 31760] train loss: 0.1877, train acc: 0.9337, val loss: 0.3651, val acc: 0.9460  (best train acc: 0.9482, best val acc: 0.9545, best train loss: 0.1508)\n",
      "[Epoch: 31780] train loss: 0.1699, train acc: 0.9370, val loss: 0.3952, val acc: 0.9467  (best train acc: 0.9482, best val acc: 0.9545, best train loss: 0.1508)\n",
      "[Epoch: 31800] train loss: 0.1741, train acc: 0.9352, val loss: 0.3359, val acc: 0.9460  (best train acc: 0.9482, best val acc: 0.9545, best train loss: 0.1508)\n",
      "[Epoch: 31820] train loss: 0.1692, train acc: 0.9414, val loss: 0.3535, val acc: 0.9491  (best train acc: 0.9482, best val acc: 0.9545, best train loss: 0.1508)\n",
      "[Epoch: 31840] train loss: 0.1611, train acc: 0.9443, val loss: 0.3785, val acc: 0.9508  (best train acc: 0.9482, best val acc: 0.9545, best train loss: 0.1508)\n",
      "[Epoch: 31860] train loss: 0.1640, train acc: 0.9421, val loss: 0.3916, val acc: 0.9521  (best train acc: 0.9482, best val acc: 0.9545, best train loss: 0.1508)\n",
      "[Epoch: 31880] train loss: 0.1732, train acc: 0.9376, val loss: 0.3688, val acc: 0.9474  (best train acc: 0.9482, best val acc: 0.9545, best train loss: 0.1508)\n",
      "[Epoch: 31900] train loss: 0.1804, train acc: 0.9290, val loss: 0.3818, val acc: 0.9460  (best train acc: 0.9482, best val acc: 0.9545, best train loss: 0.1508)\n",
      "[Epoch: 31920] train loss: 0.1642, train acc: 0.9432, val loss: 0.3919, val acc: 0.9521  (best train acc: 0.9482, best val acc: 0.9545, best train loss: 0.1508)\n",
      "[Epoch: 31940] train loss: 0.1664, train acc: 0.9393, val loss: 0.3937, val acc: 0.9497  (best train acc: 0.9482, best val acc: 0.9545, best train loss: 0.1508)\n",
      "[Epoch: 31960] train loss: 0.1634, train acc: 0.9401, val loss: 0.3859, val acc: 0.9491  (best train acc: 0.9482, best val acc: 0.9545, best train loss: 0.1508)\n",
      "[Epoch: 31980] train loss: 0.1575, train acc: 0.9448, val loss: 0.4010, val acc: 0.9524  (best train acc: 0.9484, best val acc: 0.9545, best train loss: 0.1508)\n",
      "[Epoch: 32000] train loss: 0.1645, train acc: 0.9392, val loss: 0.3811, val acc: 0.9497  (best train acc: 0.9484, best val acc: 0.9545, best train loss: 0.1494)\n",
      "[Epoch: 32020] train loss: 0.1639, train acc: 0.9405, val loss: 0.3596, val acc: 0.9504  (best train acc: 0.9484, best val acc: 0.9545, best train loss: 0.1494)\n",
      "[Epoch: 32040] train loss: 0.1635, train acc: 0.9425, val loss: 0.3958, val acc: 0.9497  (best train acc: 0.9484, best val acc: 0.9545, best train loss: 0.1494)\n",
      "[Epoch: 32060] train loss: 0.1854, train acc: 0.9301, val loss: 0.3981, val acc: 0.9497  (best train acc: 0.9484, best val acc: 0.9545, best train loss: 0.1494)\n",
      "[Epoch: 32080] train loss: 0.1698, train acc: 0.9357, val loss: 0.3701, val acc: 0.9511  (best train acc: 0.9484, best val acc: 0.9545, best train loss: 0.1494)\n",
      "[Epoch: 32100] train loss: 0.1734, train acc: 0.9401, val loss: 0.3917, val acc: 0.9484  (best train acc: 0.9484, best val acc: 0.9545, best train loss: 0.1494)\n",
      "[Epoch: 32120] train loss: 0.1662, train acc: 0.9433, val loss: 0.3793, val acc: 0.9487  (best train acc: 0.9484, best val acc: 0.9545, best train loss: 0.1494)\n",
      "[Epoch: 32140] train loss: 0.1602, train acc: 0.9431, val loss: 0.3659, val acc: 0.9514  (best train acc: 0.9484, best val acc: 0.9545, best train loss: 0.1494)\n",
      "[Epoch: 32160] train loss: 0.1574, train acc: 0.9458, val loss: 0.3577, val acc: 0.9494  (best train acc: 0.9484, best val acc: 0.9545, best train loss: 0.1494)\n",
      "[Epoch: 32180] train loss: 0.1670, train acc: 0.9424, val loss: 0.3794, val acc: 0.9518  (best train acc: 0.9484, best val acc: 0.9545, best train loss: 0.1494)\n",
      "[Epoch: 32200] train loss: 0.1839, train acc: 0.9276, val loss: 0.3260, val acc: 0.9464  (best train acc: 0.9484, best val acc: 0.9545, best train loss: 0.1494)\n",
      "[Epoch: 32220] train loss: 0.1837, train acc: 0.9326, val loss: 0.4290, val acc: 0.9470  (best train acc: 0.9484, best val acc: 0.9545, best train loss: 0.1494)\n",
      "[Epoch: 32240] train loss: 0.1799, train acc: 0.9317, val loss: 0.3585, val acc: 0.9487  (best train acc: 0.9484, best val acc: 0.9545, best train loss: 0.1494)\n",
      "[Epoch: 32260] train loss: 0.1708, train acc: 0.9390, val loss: 0.3780, val acc: 0.9467  (best train acc: 0.9484, best val acc: 0.9545, best train loss: 0.1494)\n",
      "[Epoch: 32280] train loss: 0.1732, train acc: 0.9403, val loss: 0.3856, val acc: 0.9487  (best train acc: 0.9484, best val acc: 0.9545, best train loss: 0.1494)\n",
      "[Epoch: 32300] train loss: 0.1668, train acc: 0.9389, val loss: 0.3428, val acc: 0.9501  (best train acc: 0.9484, best val acc: 0.9545, best train loss: 0.1494)\n",
      "[Epoch: 32320] train loss: 0.1609, train acc: 0.9439, val loss: 0.3778, val acc: 0.9504  (best train acc: 0.9484, best val acc: 0.9545, best train loss: 0.1494)\n",
      "[Epoch: 32340] train loss: 0.1538, train acc: 0.9466, val loss: 0.3731, val acc: 0.9450  (best train acc: 0.9484, best val acc: 0.9545, best train loss: 0.1494)\n",
      "[Epoch: 32360] train loss: 0.1832, train acc: 0.9318, val loss: 0.3808, val acc: 0.9511  (best train acc: 0.9484, best val acc: 0.9545, best train loss: 0.1494)\n",
      "[Epoch: 32380] train loss: 0.1605, train acc: 0.9404, val loss: 0.3764, val acc: 0.9541  (best train acc: 0.9484, best val acc: 0.9545, best train loss: 0.1494)\n",
      "[Epoch: 32400] train loss: 0.1701, train acc: 0.9358, val loss: 0.3962, val acc: 0.9514  (best train acc: 0.9484, best val acc: 0.9545, best train loss: 0.1494)\n",
      "[Epoch: 32420] train loss: 0.1857, train acc: 0.9331, val loss: 0.3708, val acc: 0.9454  (best train acc: 0.9484, best val acc: 0.9545, best train loss: 0.1494)\n",
      "[Epoch: 32440] train loss: 0.1734, train acc: 0.9355, val loss: 0.3683, val acc: 0.9521  (best train acc: 0.9484, best val acc: 0.9545, best train loss: 0.1494)\n",
      "[Epoch: 32460] train loss: 0.1688, train acc: 0.9392, val loss: 0.3836, val acc: 0.9491  (best train acc: 0.9484, best val acc: 0.9545, best train loss: 0.1494)\n",
      "[Epoch: 32480] train loss: 0.1697, train acc: 0.9387, val loss: 0.4085, val acc: 0.9531  (best train acc: 0.9484, best val acc: 0.9545, best train loss: 0.1494)\n",
      "[Epoch: 32500] train loss: 0.1624, train acc: 0.9417, val loss: 0.3542, val acc: 0.9450  (best train acc: 0.9484, best val acc: 0.9545, best train loss: 0.1494)\n",
      "[Epoch: 32520] train loss: 0.1660, train acc: 0.9440, val loss: 0.4104, val acc: 0.9521  (best train acc: 0.9484, best val acc: 0.9545, best train loss: 0.1494)\n",
      "[Epoch: 32540] train loss: 0.1675, train acc: 0.9419, val loss: 0.4154, val acc: 0.9427  (best train acc: 0.9484, best val acc: 0.9545, best train loss: 0.1494)\n",
      "[Epoch: 32560] train loss: 0.1706, train acc: 0.9349, val loss: 0.3789, val acc: 0.9376  (best train acc: 0.9484, best val acc: 0.9545, best train loss: 0.1494)\n",
      "[Epoch: 32580] train loss: 0.1958, train acc: 0.9286, val loss: 0.3629, val acc: 0.9447  (best train acc: 0.9484, best val acc: 0.9545, best train loss: 0.1494)\n",
      "[Epoch: 32600] train loss: 0.1908, train acc: 0.9258, val loss: 0.3307, val acc: 0.9457  (best train acc: 0.9484, best val acc: 0.9548, best train loss: 0.1494)\n",
      "[Epoch: 32620] train loss: 0.1706, train acc: 0.9382, val loss: 0.3353, val acc: 0.9477  (best train acc: 0.9484, best val acc: 0.9548, best train loss: 0.1494)\n",
      "[Epoch: 32640] train loss: 0.1620, train acc: 0.9431, val loss: 0.3839, val acc: 0.9538  (best train acc: 0.9484, best val acc: 0.9548, best train loss: 0.1494)\n",
      "[Epoch: 32660] train loss: 0.1575, train acc: 0.9417, val loss: 0.3743, val acc: 0.9528  (best train acc: 0.9484, best val acc: 0.9548, best train loss: 0.1494)\n",
      "[Epoch: 32680] train loss: 0.1781, train acc: 0.9327, val loss: 0.3701, val acc: 0.9491  (best train acc: 0.9484, best val acc: 0.9548, best train loss: 0.1494)\n",
      "[Epoch: 32700] train loss: 0.1644, train acc: 0.9393, val loss: 0.3795, val acc: 0.9508  (best train acc: 0.9484, best val acc: 0.9548, best train loss: 0.1494)\n",
      "[Epoch: 32720] train loss: 0.1588, train acc: 0.9417, val loss: 0.3808, val acc: 0.9528  (best train acc: 0.9484, best val acc: 0.9548, best train loss: 0.1485)\n",
      "[Epoch: 32740] train loss: 0.1788, train acc: 0.9309, val loss: 0.3462, val acc: 0.9390  (best train acc: 0.9484, best val acc: 0.9548, best train loss: 0.1485)\n",
      "[Epoch: 32760] train loss: 0.1578, train acc: 0.9444, val loss: 0.4049, val acc: 0.9524  (best train acc: 0.9484, best val acc: 0.9548, best train loss: 0.1485)\n",
      "[Epoch: 32780] train loss: 0.1726, train acc: 0.9354, val loss: 0.3629, val acc: 0.9521  (best train acc: 0.9484, best val acc: 0.9548, best train loss: 0.1485)\n",
      "[Epoch: 32800] train loss: 0.1729, train acc: 0.9403, val loss: 0.3665, val acc: 0.9454  (best train acc: 0.9485, best val acc: 0.9548, best train loss: 0.1485)\n",
      "[Epoch: 32820] train loss: 0.1594, train acc: 0.9443, val loss: 0.3662, val acc: 0.9514  (best train acc: 0.9485, best val acc: 0.9548, best train loss: 0.1485)\n",
      "[Epoch: 32840] train loss: 0.1723, train acc: 0.9373, val loss: 0.3869, val acc: 0.9518  (best train acc: 0.9485, best val acc: 0.9548, best train loss: 0.1485)\n",
      "[Epoch: 32860] train loss: 0.1613, train acc: 0.9393, val loss: 0.3488, val acc: 0.9501  (best train acc: 0.9485, best val acc: 0.9548, best train loss: 0.1485)\n",
      "[Epoch: 32880] train loss: 0.1793, train acc: 0.9356, val loss: 0.3625, val acc: 0.9504  (best train acc: 0.9485, best val acc: 0.9548, best train loss: 0.1485)\n",
      "[Epoch: 32900] train loss: 0.1662, train acc: 0.9443, val loss: 0.4190, val acc: 0.9464  (best train acc: 0.9485, best val acc: 0.9548, best train loss: 0.1485)\n",
      "[Epoch: 32920] train loss: 0.1518, train acc: 0.9440, val loss: 0.3894, val acc: 0.9501  (best train acc: 0.9485, best val acc: 0.9548, best train loss: 0.1485)\n",
      "[Epoch: 32940] train loss: 0.1844, train acc: 0.9369, val loss: 0.3895, val acc: 0.9379  (best train acc: 0.9485, best val acc: 0.9548, best train loss: 0.1485)\n",
      "[Epoch: 32960] train loss: 0.1851, train acc: 0.9299, val loss: 0.3648, val acc: 0.9444  (best train acc: 0.9485, best val acc: 0.9548, best train loss: 0.1485)\n",
      "[Epoch: 32980] train loss: 0.1661, train acc: 0.9409, val loss: 0.3514, val acc: 0.9504  (best train acc: 0.9485, best val acc: 0.9548, best train loss: 0.1485)\n",
      "[Epoch: 33000] train loss: 0.1708, train acc: 0.9408, val loss: 0.4122, val acc: 0.9538  (best train acc: 0.9485, best val acc: 0.9548, best train loss: 0.1485)\n",
      "[Epoch: 33020] train loss: 0.1605, train acc: 0.9435, val loss: 0.3401, val acc: 0.9531  (best train acc: 0.9485, best val acc: 0.9548, best train loss: 0.1485)\n",
      "[Epoch: 33040] train loss: 0.1622, train acc: 0.9381, val loss: 0.4104, val acc: 0.9352  (best train acc: 0.9485, best val acc: 0.9548, best train loss: 0.1485)\n",
      "[Epoch: 33060] train loss: 0.1808, train acc: 0.9370, val loss: 0.3923, val acc: 0.9481  (best train acc: 0.9485, best val acc: 0.9548, best train loss: 0.1485)\n",
      "[Epoch: 33080] train loss: 0.1589, train acc: 0.9427, val loss: 0.3878, val acc: 0.9518  (best train acc: 0.9485, best val acc: 0.9548, best train loss: 0.1485)\n",
      "[Epoch: 33100] train loss: 0.1611, train acc: 0.9429, val loss: 0.4067, val acc: 0.9518  (best train acc: 0.9485, best val acc: 0.9548, best train loss: 0.1485)\n",
      "[Epoch: 33120] train loss: 0.1551, train acc: 0.9458, val loss: 0.3630, val acc: 0.9511  (best train acc: 0.9485, best val acc: 0.9548, best train loss: 0.1485)\n",
      "[Epoch: 33140] train loss: 0.1658, train acc: 0.9408, val loss: 0.4015, val acc: 0.9477  (best train acc: 0.9485, best val acc: 0.9548, best train loss: 0.1485)\n",
      "[Epoch: 33160] train loss: 0.1598, train acc: 0.9445, val loss: 0.3825, val acc: 0.9528  (best train acc: 0.9485, best val acc: 0.9548, best train loss: 0.1485)\n",
      "[Epoch: 33180] train loss: 0.1552, train acc: 0.9422, val loss: 0.3777, val acc: 0.9514  (best train acc: 0.9485, best val acc: 0.9548, best train loss: 0.1485)\n",
      "[Epoch: 33200] train loss: 0.1590, train acc: 0.9426, val loss: 0.3813, val acc: 0.9524  (best train acc: 0.9485, best val acc: 0.9548, best train loss: 0.1465)\n",
      "[Epoch: 33220] train loss: 0.1603, train acc: 0.9434, val loss: 0.3729, val acc: 0.9508  (best train acc: 0.9485, best val acc: 0.9548, best train loss: 0.1465)\n",
      "[Epoch: 33240] train loss: 0.1575, train acc: 0.9432, val loss: 0.4054, val acc: 0.9535  (best train acc: 0.9485, best val acc: 0.9548, best train loss: 0.1465)\n",
      "[Epoch: 33260] train loss: 0.1642, train acc: 0.9429, val loss: 0.4094, val acc: 0.9538  (best train acc: 0.9485, best val acc: 0.9548, best train loss: 0.1465)\n",
      "[Epoch: 33280] train loss: 0.1936, train acc: 0.9318, val loss: 0.3853, val acc: 0.9406  (best train acc: 0.9485, best val acc: 0.9548, best train loss: 0.1465)\n",
      "[Epoch: 33300] train loss: 0.1672, train acc: 0.9408, val loss: 0.3301, val acc: 0.9484  (best train acc: 0.9485, best val acc: 0.9548, best train loss: 0.1465)\n",
      "[Epoch: 33320] train loss: 0.1673, train acc: 0.9388, val loss: 0.3638, val acc: 0.9504  (best train acc: 0.9485, best val acc: 0.9548, best train loss: 0.1465)\n",
      "[Epoch: 33340] train loss: 0.1600, train acc: 0.9427, val loss: 0.3935, val acc: 0.9497  (best train acc: 0.9485, best val acc: 0.9548, best train loss: 0.1465)\n",
      "[Epoch: 33360] train loss: 0.1543, train acc: 0.9453, val loss: 0.3771, val acc: 0.9538  (best train acc: 0.9485, best val acc: 0.9548, best train loss: 0.1465)\n",
      "[Epoch: 33380] train loss: 0.1728, train acc: 0.9387, val loss: 0.4068, val acc: 0.9538  (best train acc: 0.9485, best val acc: 0.9548, best train loss: 0.1465)\n",
      "[Epoch: 33400] train loss: 0.1461, train acc: 0.9480, val loss: 0.4140, val acc: 0.9545  (best train acc: 0.9485, best val acc: 0.9548, best train loss: 0.1461)\n",
      "[Epoch: 33420] train loss: 0.1609, train acc: 0.9443, val loss: 0.4011, val acc: 0.9494  (best train acc: 0.9485, best val acc: 0.9548, best train loss: 0.1461)\n",
      "[Epoch: 33440] train loss: 0.1596, train acc: 0.9419, val loss: 0.3685, val acc: 0.9494  (best train acc: 0.9485, best val acc: 0.9548, best train loss: 0.1461)\n",
      "[Epoch: 33460] train loss: 0.1608, train acc: 0.9402, val loss: 0.3878, val acc: 0.9444  (best train acc: 0.9485, best val acc: 0.9548, best train loss: 0.1461)\n",
      "[Epoch: 33480] train loss: 0.1846, train acc: 0.9377, val loss: 0.3950, val acc: 0.9410  (best train acc: 0.9485, best val acc: 0.9548, best train loss: 0.1461)\n",
      "[Epoch: 33500] train loss: 0.1875, train acc: 0.9358, val loss: 0.4047, val acc: 0.9376  (best train acc: 0.9485, best val acc: 0.9548, best train loss: 0.1461)\n",
      "[Epoch: 33520] train loss: 0.1765, train acc: 0.9373, val loss: 0.3514, val acc: 0.9491  (best train acc: 0.9485, best val acc: 0.9548, best train loss: 0.1461)\n",
      "[Epoch: 33540] train loss: 0.1608, train acc: 0.9396, val loss: 0.3746, val acc: 0.9444  (best train acc: 0.9485, best val acc: 0.9548, best train loss: 0.1461)\n",
      "[Epoch: 33560] train loss: 0.1524, train acc: 0.9461, val loss: 0.3690, val acc: 0.9508  (best train acc: 0.9485, best val acc: 0.9548, best train loss: 0.1461)\n",
      "[Epoch: 33580] train loss: 0.1546, train acc: 0.9440, val loss: 0.3586, val acc: 0.9528  (best train acc: 0.9485, best val acc: 0.9548, best train loss: 0.1461)\n",
      "[Epoch: 33600] train loss: 0.1691, train acc: 0.9364, val loss: 0.3679, val acc: 0.9406  (best train acc: 0.9485, best val acc: 0.9548, best train loss: 0.1461)\n",
      "[Epoch: 33620] train loss: 0.1723, train acc: 0.9353, val loss: 0.3299, val acc: 0.9524  (best train acc: 0.9485, best val acc: 0.9548, best train loss: 0.1461)\n",
      "[Epoch: 33640] train loss: 0.1619, train acc: 0.9407, val loss: 0.3842, val acc: 0.9504  (best train acc: 0.9485, best val acc: 0.9551, best train loss: 0.1461)\n",
      "[Epoch: 33660] train loss: 0.1651, train acc: 0.9412, val loss: 0.3341, val acc: 0.9531  (best train acc: 0.9485, best val acc: 0.9551, best train loss: 0.1461)\n",
      "[Epoch: 33680] train loss: 0.1590, train acc: 0.9433, val loss: 0.3583, val acc: 0.9545  (best train acc: 0.9485, best val acc: 0.9551, best train loss: 0.1461)\n",
      "[Epoch: 33700] train loss: 0.1525, train acc: 0.9459, val loss: 0.3632, val acc: 0.9514  (best train acc: 0.9485, best val acc: 0.9551, best train loss: 0.1461)\n",
      "[Epoch: 33720] train loss: 0.1653, train acc: 0.9404, val loss: 0.4072, val acc: 0.9494  (best train acc: 0.9487, best val acc: 0.9551, best train loss: 0.1461)\n",
      "[Epoch: 33740] train loss: 0.2023, train acc: 0.9200, val loss: 0.3252, val acc: 0.9410  (best train acc: 0.9487, best val acc: 0.9551, best train loss: 0.1461)\n",
      "[Epoch: 33760] train loss: 0.2004, train acc: 0.9317, val loss: 0.3942, val acc: 0.9491  (best train acc: 0.9487, best val acc: 0.9551, best train loss: 0.1461)\n",
      "[Epoch: 33780] train loss: 0.1599, train acc: 0.9419, val loss: 0.3417, val acc: 0.9518  (best train acc: 0.9487, best val acc: 0.9551, best train loss: 0.1461)\n",
      "[Epoch: 33800] train loss: 0.1619, train acc: 0.9452, val loss: 0.3457, val acc: 0.9514  (best train acc: 0.9487, best val acc: 0.9551, best train loss: 0.1461)\n",
      "[Epoch: 33820] train loss: 0.1563, train acc: 0.9450, val loss: 0.3360, val acc: 0.9514  (best train acc: 0.9487, best val acc: 0.9551, best train loss: 0.1461)\n",
      "[Epoch: 33840] train loss: 0.1526, train acc: 0.9464, val loss: 0.4019, val acc: 0.9545  (best train acc: 0.9487, best val acc: 0.9551, best train loss: 0.1461)\n",
      "[Epoch: 33860] train loss: 0.1651, train acc: 0.9422, val loss: 0.3828, val acc: 0.9477  (best train acc: 0.9487, best val acc: 0.9551, best train loss: 0.1461)\n",
      "[Epoch: 33880] train loss: 0.1673, train acc: 0.9393, val loss: 0.3871, val acc: 0.9514  (best train acc: 0.9487, best val acc: 0.9551, best train loss: 0.1461)\n",
      "[Epoch: 33900] train loss: 0.1994, train acc: 0.9243, val loss: 0.3902, val acc: 0.9491  (best train acc: 0.9487, best val acc: 0.9551, best train loss: 0.1461)\n",
      "[Epoch: 33920] train loss: 0.1621, train acc: 0.9420, val loss: 0.3589, val acc: 0.9548  (best train acc: 0.9487, best val acc: 0.9551, best train loss: 0.1461)\n",
      "[Epoch: 33940] train loss: 0.1685, train acc: 0.9407, val loss: 0.3623, val acc: 0.9511  (best train acc: 0.9487, best val acc: 0.9551, best train loss: 0.1461)\n",
      "[Epoch: 33960] train loss: 0.1596, train acc: 0.9448, val loss: 0.3973, val acc: 0.9518  (best train acc: 0.9487, best val acc: 0.9551, best train loss: 0.1461)\n",
      "[Epoch: 33980] train loss: 0.1639, train acc: 0.9401, val loss: 0.3582, val acc: 0.9494  (best train acc: 0.9487, best val acc: 0.9551, best train loss: 0.1461)\n",
      "[Epoch: 34000] train loss: 0.1802, train acc: 0.9346, val loss: 0.4073, val acc: 0.9491  (best train acc: 0.9487, best val acc: 0.9551, best train loss: 0.1461)\n",
      "[Epoch: 34020] train loss: 0.1510, train acc: 0.9472, val loss: 0.3677, val acc: 0.9535  (best train acc: 0.9487, best val acc: 0.9558, best train loss: 0.1461)\n",
      "[Epoch: 34040] train loss: 0.1671, train acc: 0.9374, val loss: 0.3667, val acc: 0.9521  (best train acc: 0.9487, best val acc: 0.9558, best train loss: 0.1461)\n",
      "[Epoch: 34060] train loss: 0.1705, train acc: 0.9408, val loss: 0.3872, val acc: 0.9514  (best train acc: 0.9487, best val acc: 0.9558, best train loss: 0.1461)\n",
      "[Epoch: 34080] train loss: 0.1664, train acc: 0.9379, val loss: 0.3815, val acc: 0.9511  (best train acc: 0.9487, best val acc: 0.9558, best train loss: 0.1461)\n",
      "[Epoch: 34100] train loss: 0.1632, train acc: 0.9432, val loss: 0.3779, val acc: 0.9535  (best train acc: 0.9487, best val acc: 0.9558, best train loss: 0.1461)\n",
      "[Epoch: 34120] train loss: 0.1548, train acc: 0.9438, val loss: 0.3674, val acc: 0.9538  (best train acc: 0.9487, best val acc: 0.9562, best train loss: 0.1461)\n",
      "[Epoch: 34140] train loss: 0.1654, train acc: 0.9457, val loss: 0.3701, val acc: 0.9551  (best train acc: 0.9487, best val acc: 0.9562, best train loss: 0.1461)\n",
      "[Epoch: 34160] train loss: 0.1552, train acc: 0.9434, val loss: 0.3543, val acc: 0.9511  (best train acc: 0.9487, best val acc: 0.9562, best train loss: 0.1461)\n",
      "[Epoch: 34180] train loss: 0.1872, train acc: 0.9333, val loss: 0.3993, val acc: 0.9528  (best train acc: 0.9487, best val acc: 0.9562, best train loss: 0.1461)\n",
      "[Epoch: 34200] train loss: 0.1724, train acc: 0.9382, val loss: 0.3415, val acc: 0.9518  (best train acc: 0.9487, best val acc: 0.9562, best train loss: 0.1461)\n",
      "[Epoch: 34220] train loss: 0.1622, train acc: 0.9419, val loss: 0.3457, val acc: 0.9514  (best train acc: 0.9487, best val acc: 0.9562, best train loss: 0.1461)\n",
      "[Epoch: 34240] train loss: 0.1532, train acc: 0.9457, val loss: 0.3937, val acc: 0.9555  (best train acc: 0.9487, best val acc: 0.9562, best train loss: 0.1461)\n",
      "[Epoch: 34260] train loss: 0.1566, train acc: 0.9440, val loss: 0.3721, val acc: 0.9531  (best train acc: 0.9487, best val acc: 0.9562, best train loss: 0.1461)\n",
      "[Epoch: 34280] train loss: 0.1659, train acc: 0.9382, val loss: 0.4013, val acc: 0.9541  (best train acc: 0.9487, best val acc: 0.9562, best train loss: 0.1461)\n",
      "[Epoch: 34300] train loss: 0.1590, train acc: 0.9433, val loss: 0.3482, val acc: 0.9528  (best train acc: 0.9487, best val acc: 0.9562, best train loss: 0.1461)\n",
      "[Epoch: 34320] train loss: 0.1607, train acc: 0.9402, val loss: 0.3525, val acc: 0.9528  (best train acc: 0.9487, best val acc: 0.9562, best train loss: 0.1461)\n",
      "[Epoch: 34340] train loss: 0.1538, train acc: 0.9459, val loss: 0.3633, val acc: 0.9524  (best train acc: 0.9487, best val acc: 0.9562, best train loss: 0.1461)\n",
      "[Epoch: 34360] train loss: 0.1655, train acc: 0.9372, val loss: 0.3848, val acc: 0.9501  (best train acc: 0.9487, best val acc: 0.9562, best train loss: 0.1461)\n",
      "[Epoch: 34380] train loss: 0.1543, train acc: 0.9432, val loss: 0.3565, val acc: 0.9511  (best train acc: 0.9487, best val acc: 0.9562, best train loss: 0.1461)\n",
      "[Epoch: 34400] train loss: 0.1671, train acc: 0.9368, val loss: 0.3598, val acc: 0.9501  (best train acc: 0.9487, best val acc: 0.9562, best train loss: 0.1461)\n",
      "[Epoch: 34420] train loss: 0.1543, train acc: 0.9440, val loss: 0.3337, val acc: 0.9427  (best train acc: 0.9487, best val acc: 0.9562, best train loss: 0.1461)\n",
      "[Epoch: 34440] train loss: 0.1540, train acc: 0.9467, val loss: 0.3756, val acc: 0.9551  (best train acc: 0.9499, best val acc: 0.9562, best train loss: 0.1461)\n",
      "[Epoch: 34460] train loss: 0.1606, train acc: 0.9419, val loss: 0.3870, val acc: 0.9541  (best train acc: 0.9499, best val acc: 0.9562, best train loss: 0.1461)\n",
      "[Epoch: 34480] train loss: 0.1701, train acc: 0.9393, val loss: 0.4065, val acc: 0.9514  (best train acc: 0.9499, best val acc: 0.9562, best train loss: 0.1461)\n",
      "[Epoch: 34500] train loss: 0.1543, train acc: 0.9451, val loss: 0.3754, val acc: 0.9518  (best train acc: 0.9499, best val acc: 0.9562, best train loss: 0.1461)\n",
      "[Epoch: 34520] train loss: 0.1551, train acc: 0.9451, val loss: 0.3665, val acc: 0.9528  (best train acc: 0.9503, best val acc: 0.9562, best train loss: 0.1457)\n",
      "[Epoch: 34540] train loss: 0.2062, train acc: 0.9181, val loss: 0.3390, val acc: 0.9460  (best train acc: 0.9503, best val acc: 0.9562, best train loss: 0.1457)\n",
      "[Epoch: 34560] train loss: 0.1522, train acc: 0.9460, val loss: 0.3365, val acc: 0.9528  (best train acc: 0.9503, best val acc: 0.9562, best train loss: 0.1457)\n",
      "[Epoch: 34580] train loss: 0.1567, train acc: 0.9448, val loss: 0.4001, val acc: 0.9528  (best train acc: 0.9503, best val acc: 0.9565, best train loss: 0.1446)\n",
      "[Epoch: 34600] train loss: 0.1641, train acc: 0.9388, val loss: 0.3329, val acc: 0.9467  (best train acc: 0.9503, best val acc: 0.9565, best train loss: 0.1446)\n",
      "[Epoch: 34620] train loss: 0.1776, train acc: 0.9378, val loss: 0.3353, val acc: 0.9514  (best train acc: 0.9503, best val acc: 0.9565, best train loss: 0.1446)\n",
      "[Epoch: 34640] train loss: 0.1642, train acc: 0.9390, val loss: 0.3729, val acc: 0.9484  (best train acc: 0.9503, best val acc: 0.9565, best train loss: 0.1446)\n",
      "[Epoch: 34660] train loss: 0.1745, train acc: 0.9341, val loss: 0.3237, val acc: 0.9366  (best train acc: 0.9503, best val acc: 0.9565, best train loss: 0.1446)\n",
      "[Epoch: 34680] train loss: 0.1720, train acc: 0.9403, val loss: 0.3575, val acc: 0.9508  (best train acc: 0.9503, best val acc: 0.9565, best train loss: 0.1446)\n",
      "[Epoch: 34700] train loss: 0.1716, train acc: 0.9369, val loss: 0.3476, val acc: 0.9531  (best train acc: 0.9503, best val acc: 0.9572, best train loss: 0.1446)\n",
      "[Epoch: 34720] train loss: 0.1524, train acc: 0.9450, val loss: 0.3683, val acc: 0.9504  (best train acc: 0.9503, best val acc: 0.9572, best train loss: 0.1446)\n",
      "[Epoch: 34740] train loss: 0.1512, train acc: 0.9479, val loss: 0.3482, val acc: 0.9548  (best train acc: 0.9503, best val acc: 0.9572, best train loss: 0.1446)\n",
      "[Epoch: 34760] train loss: 0.1789, train acc: 0.9364, val loss: 0.4138, val acc: 0.9538  (best train acc: 0.9503, best val acc: 0.9572, best train loss: 0.1446)\n",
      "[Epoch: 34780] train loss: 0.1576, train acc: 0.9445, val loss: 0.4000, val acc: 0.9481  (best train acc: 0.9503, best val acc: 0.9572, best train loss: 0.1446)\n",
      "[Epoch: 34800] train loss: 0.1515, train acc: 0.9438, val loss: 0.3688, val acc: 0.9535  (best train acc: 0.9503, best val acc: 0.9572, best train loss: 0.1446)\n",
      "[Epoch: 34820] train loss: 0.1622, train acc: 0.9432, val loss: 0.4109, val acc: 0.9562  (best train acc: 0.9503, best val acc: 0.9572, best train loss: 0.1446)\n",
      "[Epoch: 34840] train loss: 0.1539, train acc: 0.9461, val loss: 0.4101, val acc: 0.9504  (best train acc: 0.9503, best val acc: 0.9572, best train loss: 0.1446)\n",
      "[Epoch: 34860] train loss: 0.1545, train acc: 0.9466, val loss: 0.3666, val acc: 0.9551  (best train acc: 0.9503, best val acc: 0.9572, best train loss: 0.1446)\n",
      "[Epoch: 34880] train loss: 0.1478, train acc: 0.9494, val loss: 0.4003, val acc: 0.9548  (best train acc: 0.9503, best val acc: 0.9572, best train loss: 0.1418)\n",
      "[Epoch: 34900] train loss: 0.1705, train acc: 0.9366, val loss: 0.3514, val acc: 0.9481  (best train acc: 0.9503, best val acc: 0.9572, best train loss: 0.1418)\n",
      "[Epoch: 34920] train loss: 0.1704, train acc: 0.9385, val loss: 0.3680, val acc: 0.9511  (best train acc: 0.9503, best val acc: 0.9572, best train loss: 0.1418)\n",
      "[Epoch: 34940] train loss: 0.1894, train acc: 0.9349, val loss: 0.3374, val acc: 0.9487  (best train acc: 0.9503, best val acc: 0.9572, best train loss: 0.1418)\n",
      "[Epoch: 34960] train loss: 0.1640, train acc: 0.9408, val loss: 0.3707, val acc: 0.9531  (best train acc: 0.9503, best val acc: 0.9572, best train loss: 0.1418)\n",
      "[Epoch: 34980] train loss: 0.1501, train acc: 0.9482, val loss: 0.4043, val acc: 0.9535  (best train acc: 0.9503, best val acc: 0.9572, best train loss: 0.1418)\n",
      "[Epoch: 35000] train loss: 0.1490, train acc: 0.9488, val loss: 0.4078, val acc: 0.9558  (best train acc: 0.9503, best val acc: 0.9572, best train loss: 0.1418)\n",
      "[Epoch: 35020] train loss: 0.1670, train acc: 0.9393, val loss: 0.3420, val acc: 0.9444  (best train acc: 0.9503, best val acc: 0.9572, best train loss: 0.1418)\n",
      "[Epoch: 35040] train loss: 0.1721, train acc: 0.9355, val loss: 0.4116, val acc: 0.9531  (best train acc: 0.9503, best val acc: 0.9578, best train loss: 0.1418)\n",
      "[Epoch: 35060] train loss: 0.1668, train acc: 0.9364, val loss: 0.3118, val acc: 0.9467  (best train acc: 0.9503, best val acc: 0.9578, best train loss: 0.1418)\n",
      "[Epoch: 35080] train loss: 0.1524, train acc: 0.9469, val loss: 0.3884, val acc: 0.9555  (best train acc: 0.9503, best val acc: 0.9578, best train loss: 0.1418)\n",
      "[Epoch: 35100] train loss: 0.1559, train acc: 0.9450, val loss: 0.3974, val acc: 0.9535  (best train acc: 0.9503, best val acc: 0.9578, best train loss: 0.1418)\n",
      "[Epoch: 35120] train loss: 0.1691, train acc: 0.9380, val loss: 0.3682, val acc: 0.9497  (best train acc: 0.9503, best val acc: 0.9578, best train loss: 0.1418)\n",
      "[Epoch: 35140] train loss: 0.1538, train acc: 0.9480, val loss: 0.3620, val acc: 0.9558  (best train acc: 0.9503, best val acc: 0.9578, best train loss: 0.1418)\n",
      "[Epoch: 35160] train loss: 0.1996, train acc: 0.9320, val loss: 0.3506, val acc: 0.9376  (best train acc: 0.9503, best val acc: 0.9578, best train loss: 0.1418)\n",
      "[Epoch: 35180] train loss: 0.1584, train acc: 0.9443, val loss: 0.3520, val acc: 0.9518  (best train acc: 0.9503, best val acc: 0.9578, best train loss: 0.1418)\n",
      "[Epoch: 35200] train loss: 0.1488, train acc: 0.9481, val loss: 0.3878, val acc: 0.9548  (best train acc: 0.9503, best val acc: 0.9578, best train loss: 0.1418)\n",
      "[Epoch: 35220] train loss: 0.1505, train acc: 0.9459, val loss: 0.3984, val acc: 0.9551  (best train acc: 0.9513, best val acc: 0.9578, best train loss: 0.1411)\n",
      "[Epoch: 35240] train loss: 0.1606, train acc: 0.9419, val loss: 0.3437, val acc: 0.9548  (best train acc: 0.9513, best val acc: 0.9578, best train loss: 0.1411)\n",
      "[Epoch: 35260] train loss: 0.1736, train acc: 0.9376, val loss: 0.4119, val acc: 0.9501  (best train acc: 0.9513, best val acc: 0.9578, best train loss: 0.1411)\n",
      "[Epoch: 35280] train loss: 0.1535, train acc: 0.9466, val loss: 0.3863, val acc: 0.9538  (best train acc: 0.9513, best val acc: 0.9578, best train loss: 0.1411)\n",
      "[Epoch: 35300] train loss: 0.1473, train acc: 0.9481, val loss: 0.4020, val acc: 0.9538  (best train acc: 0.9513, best val acc: 0.9578, best train loss: 0.1411)\n",
      "[Epoch: 35320] train loss: 0.1668, train acc: 0.9398, val loss: 0.3729, val acc: 0.9538  (best train acc: 0.9513, best val acc: 0.9578, best train loss: 0.1411)\n",
      "[Epoch: 35340] train loss: 0.1462, train acc: 0.9481, val loss: 0.4233, val acc: 0.9572  (best train acc: 0.9513, best val acc: 0.9578, best train loss: 0.1411)\n",
      "[Epoch: 35360] train loss: 0.1795, train acc: 0.9348, val loss: 0.4354, val acc: 0.9487  (best train acc: 0.9515, best val acc: 0.9578, best train loss: 0.1409)\n",
      "[Epoch: 35380] train loss: 0.1567, train acc: 0.9447, val loss: 0.3804, val acc: 0.9545  (best train acc: 0.9515, best val acc: 0.9578, best train loss: 0.1409)\n",
      "[Epoch: 35400] train loss: 0.1571, train acc: 0.9474, val loss: 0.3975, val acc: 0.9558  (best train acc: 0.9515, best val acc: 0.9578, best train loss: 0.1409)\n",
      "[Epoch: 35420] train loss: 0.1893, train acc: 0.9354, val loss: 0.4484, val acc: 0.9491  (best train acc: 0.9515, best val acc: 0.9578, best train loss: 0.1409)\n",
      "[Epoch: 35440] train loss: 0.1514, train acc: 0.9466, val loss: 0.4053, val acc: 0.9521  (best train acc: 0.9515, best val acc: 0.9578, best train loss: 0.1409)\n",
      "[Epoch: 35460] train loss: 0.1618, train acc: 0.9438, val loss: 0.3928, val acc: 0.9511  (best train acc: 0.9515, best val acc: 0.9578, best train loss: 0.1409)\n",
      "[Epoch: 35480] train loss: 0.1615, train acc: 0.9436, val loss: 0.3850, val acc: 0.9390  (best train acc: 0.9515, best val acc: 0.9578, best train loss: 0.1409)\n",
      "[Epoch: 35500] train loss: 0.1693, train acc: 0.9376, val loss: 0.3470, val acc: 0.9551  (best train acc: 0.9515, best val acc: 0.9578, best train loss: 0.1409)\n",
      "[Epoch: 35520] train loss: 0.1532, train acc: 0.9443, val loss: 0.4042, val acc: 0.9548  (best train acc: 0.9515, best val acc: 0.9578, best train loss: 0.1409)\n",
      "[Epoch: 35540] train loss: 0.1575, train acc: 0.9430, val loss: 0.4015, val acc: 0.9548  (best train acc: 0.9515, best val acc: 0.9578, best train loss: 0.1409)\n",
      "[Epoch: 35560] train loss: 0.1466, train acc: 0.9485, val loss: 0.4285, val acc: 0.9545  (best train acc: 0.9515, best val acc: 0.9578, best train loss: 0.1409)\n",
      "[Epoch: 35580] train loss: 0.1621, train acc: 0.9424, val loss: 0.3565, val acc: 0.9447  (best train acc: 0.9515, best val acc: 0.9578, best train loss: 0.1409)\n",
      "[Epoch: 35600] train loss: 0.1506, train acc: 0.9476, val loss: 0.4310, val acc: 0.9551  (best train acc: 0.9515, best val acc: 0.9578, best train loss: 0.1409)\n",
      "[Epoch: 35620] train loss: 0.1489, train acc: 0.9488, val loss: 0.4037, val acc: 0.9548  (best train acc: 0.9515, best val acc: 0.9578, best train loss: 0.1409)\n",
      "[Epoch: 35640] train loss: 0.1573, train acc: 0.9475, val loss: 0.3973, val acc: 0.9551  (best train acc: 0.9515, best val acc: 0.9578, best train loss: 0.1409)\n",
      "[Epoch: 35660] train loss: 0.1497, train acc: 0.9500, val loss: 0.3679, val acc: 0.9504  (best train acc: 0.9515, best val acc: 0.9578, best train loss: 0.1409)\n",
      "[Epoch: 35680] train loss: 0.1526, train acc: 0.9475, val loss: 0.4173, val acc: 0.9558  (best train acc: 0.9515, best val acc: 0.9578, best train loss: 0.1409)\n",
      "[Epoch: 35700] train loss: 0.1588, train acc: 0.9452, val loss: 0.4163, val acc: 0.9558  (best train acc: 0.9515, best val acc: 0.9578, best train loss: 0.1409)\n",
      "[Epoch: 35720] train loss: 0.1555, train acc: 0.9436, val loss: 0.4366, val acc: 0.9531  (best train acc: 0.9515, best val acc: 0.9578, best train loss: 0.1409)\n",
      "[Epoch: 35740] train loss: 0.1775, train acc: 0.9354, val loss: 0.3952, val acc: 0.9531  (best train acc: 0.9515, best val acc: 0.9578, best train loss: 0.1409)\n",
      "[Epoch: 35760] train loss: 0.1793, train acc: 0.9376, val loss: 0.3551, val acc: 0.9497  (best train acc: 0.9515, best val acc: 0.9578, best train loss: 0.1409)\n",
      "[Epoch: 35780] train loss: 0.1831, train acc: 0.9369, val loss: 0.4299, val acc: 0.9541  (best train acc: 0.9515, best val acc: 0.9578, best train loss: 0.1409)\n",
      "[Epoch: 35800] train loss: 0.1519, train acc: 0.9473, val loss: 0.4067, val acc: 0.9551  (best train acc: 0.9515, best val acc: 0.9578, best train loss: 0.1409)\n",
      "[Epoch: 35820] train loss: 0.1515, train acc: 0.9419, val loss: 0.4128, val acc: 0.9548  (best train acc: 0.9515, best val acc: 0.9578, best train loss: 0.1409)\n",
      "[Epoch: 35840] train loss: 0.1458, train acc: 0.9487, val loss: 0.4405, val acc: 0.9474  (best train acc: 0.9515, best val acc: 0.9578, best train loss: 0.1403)\n",
      "[Epoch: 35860] train loss: 0.1512, train acc: 0.9458, val loss: 0.4121, val acc: 0.9548  (best train acc: 0.9515, best val acc: 0.9578, best train loss: 0.1403)\n",
      "[Epoch: 35880] train loss: 0.1780, train acc: 0.9392, val loss: 0.4391, val acc: 0.9403  (best train acc: 0.9515, best val acc: 0.9578, best train loss: 0.1403)\n",
      "[Epoch: 35900] train loss: 0.1507, train acc: 0.9461, val loss: 0.4010, val acc: 0.9538  (best train acc: 0.9515, best val acc: 0.9578, best train loss: 0.1403)\n",
      "[Epoch: 35920] train loss: 0.1489, train acc: 0.9469, val loss: 0.4172, val acc: 0.9524  (best train acc: 0.9515, best val acc: 0.9578, best train loss: 0.1403)\n",
      "[Epoch: 35940] train loss: 0.1502, train acc: 0.9464, val loss: 0.4495, val acc: 0.9470  (best train acc: 0.9515, best val acc: 0.9578, best train loss: 0.1403)\n",
      "[Epoch: 35960] train loss: 0.1847, train acc: 0.9350, val loss: 0.4125, val acc: 0.9457  (best train acc: 0.9515, best val acc: 0.9578, best train loss: 0.1403)\n",
      "[Epoch: 35980] train loss: 0.1749, train acc: 0.9365, val loss: 0.3382, val acc: 0.9538  (best train acc: 0.9515, best val acc: 0.9578, best train loss: 0.1403)\n",
      "[Epoch: 36000] train loss: 0.1610, train acc: 0.9430, val loss: 0.3377, val acc: 0.9518  (best train acc: 0.9515, best val acc: 0.9578, best train loss: 0.1403)\n",
      "[Epoch: 36020] train loss: 0.1572, train acc: 0.9469, val loss: 0.4172, val acc: 0.9524  (best train acc: 0.9515, best val acc: 0.9578, best train loss: 0.1403)\n",
      "[Epoch: 36040] train loss: 0.1577, train acc: 0.9417, val loss: 0.4144, val acc: 0.9551  (best train acc: 0.9516, best val acc: 0.9578, best train loss: 0.1403)\n",
      "[Epoch: 36060] train loss: 0.1533, train acc: 0.9472, val loss: 0.4207, val acc: 0.9551  (best train acc: 0.9516, best val acc: 0.9578, best train loss: 0.1403)\n",
      "[Epoch: 36080] train loss: 0.1458, train acc: 0.9500, val loss: 0.3923, val acc: 0.9548  (best train acc: 0.9516, best val acc: 0.9578, best train loss: 0.1403)\n",
      "[Epoch: 36100] train loss: 0.1531, train acc: 0.9459, val loss: 0.4153, val acc: 0.9541  (best train acc: 0.9520, best val acc: 0.9578, best train loss: 0.1383)\n",
      "[Epoch: 36120] train loss: 0.1467, train acc: 0.9469, val loss: 0.3727, val acc: 0.9514  (best train acc: 0.9520, best val acc: 0.9582, best train loss: 0.1383)\n",
      "[Epoch: 36140] train loss: 0.1658, train acc: 0.9390, val loss: 0.3493, val acc: 0.9551  (best train acc: 0.9520, best val acc: 0.9582, best train loss: 0.1383)\n",
      "[Epoch: 36160] train loss: 0.1546, train acc: 0.9497, val loss: 0.3630, val acc: 0.9528  (best train acc: 0.9520, best val acc: 0.9582, best train loss: 0.1383)\n",
      "[Epoch: 36180] train loss: 0.1562, train acc: 0.9461, val loss: 0.3844, val acc: 0.9562  (best train acc: 0.9520, best val acc: 0.9582, best train loss: 0.1383)\n",
      "[Epoch: 36200] train loss: 0.1545, train acc: 0.9464, val loss: 0.4092, val acc: 0.9545  (best train acc: 0.9520, best val acc: 0.9582, best train loss: 0.1383)\n",
      "[Epoch: 36220] train loss: 0.2715, train acc: 0.9181, val loss: 0.3718, val acc: 0.9241  (best train acc: 0.9520, best val acc: 0.9582, best train loss: 0.1383)\n",
      "[Epoch: 36240] train loss: 0.2257, train acc: 0.9239, val loss: 0.4503, val acc: 0.9511  (best train acc: 0.9520, best val acc: 0.9582, best train loss: 0.1383)\n",
      "[Epoch: 36260] train loss: 0.1596, train acc: 0.9443, val loss: 0.3203, val acc: 0.9551  (best train acc: 0.9520, best val acc: 0.9582, best train loss: 0.1383)\n",
      "[Epoch: 36280] train loss: 0.1465, train acc: 0.9485, val loss: 0.3680, val acc: 0.9551  (best train acc: 0.9520, best val acc: 0.9582, best train loss: 0.1383)\n",
      "[Epoch: 36300] train loss: 0.1502, train acc: 0.9485, val loss: 0.3986, val acc: 0.9578  (best train acc: 0.9535, best val acc: 0.9582, best train loss: 0.1383)\n",
      "[Epoch: 36320] train loss: 0.1418, train acc: 0.9505, val loss: 0.4237, val acc: 0.9555  (best train acc: 0.9535, best val acc: 0.9582, best train loss: 0.1383)\n",
      "[Epoch: 36340] train loss: 0.1603, train acc: 0.9430, val loss: 0.4039, val acc: 0.9528  (best train acc: 0.9535, best val acc: 0.9582, best train loss: 0.1383)\n",
      "[Epoch: 36360] train loss: 0.1558, train acc: 0.9456, val loss: 0.4684, val acc: 0.9501  (best train acc: 0.9535, best val acc: 0.9582, best train loss: 0.1383)\n",
      "[Epoch: 36380] train loss: 0.1650, train acc: 0.9396, val loss: 0.3534, val acc: 0.9531  (best train acc: 0.9535, best val acc: 0.9582, best train loss: 0.1383)\n",
      "[Epoch: 36400] train loss: 0.1571, train acc: 0.9427, val loss: 0.3546, val acc: 0.9548  (best train acc: 0.9535, best val acc: 0.9582, best train loss: 0.1383)\n",
      "[Epoch: 36420] train loss: 0.1476, train acc: 0.9496, val loss: 0.4276, val acc: 0.9538  (best train acc: 0.9535, best val acc: 0.9582, best train loss: 0.1383)\n",
      "[Epoch: 36440] train loss: 0.1486, train acc: 0.9472, val loss: 0.4230, val acc: 0.9548  (best train acc: 0.9535, best val acc: 0.9582, best train loss: 0.1383)\n",
      "[Epoch: 36460] train loss: 0.1562, train acc: 0.9424, val loss: 0.3801, val acc: 0.9521  (best train acc: 0.9535, best val acc: 0.9582, best train loss: 0.1383)\n",
      "[Epoch: 36480] train loss: 0.1723, train acc: 0.9382, val loss: 0.4119, val acc: 0.9535  (best train acc: 0.9535, best val acc: 0.9582, best train loss: 0.1383)\n",
      "[Epoch: 36500] train loss: 0.1632, train acc: 0.9437, val loss: 0.4059, val acc: 0.9538  (best train acc: 0.9535, best val acc: 0.9582, best train loss: 0.1383)\n",
      "[Epoch: 36520] train loss: 0.1585, train acc: 0.9444, val loss: 0.4366, val acc: 0.9565  (best train acc: 0.9535, best val acc: 0.9582, best train loss: 0.1383)\n",
      "[Epoch: 36540] train loss: 0.1631, train acc: 0.9406, val loss: 0.4136, val acc: 0.9508  (best train acc: 0.9535, best val acc: 0.9582, best train loss: 0.1383)\n",
      "[Epoch: 36560] train loss: 0.1613, train acc: 0.9451, val loss: 0.3775, val acc: 0.9440  (best train acc: 0.9535, best val acc: 0.9582, best train loss: 0.1383)\n",
      "[Epoch: 36580] train loss: 0.1573, train acc: 0.9448, val loss: 0.3764, val acc: 0.9541  (best train acc: 0.9535, best val acc: 0.9582, best train loss: 0.1383)\n",
      "[Epoch: 36600] train loss: 0.1589, train acc: 0.9400, val loss: 0.3445, val acc: 0.9504  (best train acc: 0.9535, best val acc: 0.9582, best train loss: 0.1383)\n",
      "[Epoch: 36620] train loss: 0.1662, train acc: 0.9412, val loss: 0.4091, val acc: 0.9504  (best train acc: 0.9535, best val acc: 0.9582, best train loss: 0.1383)\n",
      "[Epoch: 36640] train loss: 0.1516, train acc: 0.9451, val loss: 0.4439, val acc: 0.9585  (best train acc: 0.9535, best val acc: 0.9585, best train loss: 0.1383)\n",
      "[Epoch: 36660] train loss: 0.1552, train acc: 0.9435, val loss: 0.3351, val acc: 0.9491  (best train acc: 0.9535, best val acc: 0.9585, best train loss: 0.1383)\n",
      "[Epoch: 36680] train loss: 0.1486, train acc: 0.9498, val loss: 0.3878, val acc: 0.9541  (best train acc: 0.9535, best val acc: 0.9585, best train loss: 0.1383)\n",
      "[Epoch: 36700] train loss: 0.1622, train acc: 0.9409, val loss: 0.4194, val acc: 0.9565  (best train acc: 0.9535, best val acc: 0.9585, best train loss: 0.1383)\n",
      "[Epoch: 36720] train loss: 0.1542, train acc: 0.9473, val loss: 0.3955, val acc: 0.9565  (best train acc: 0.9535, best val acc: 0.9585, best train loss: 0.1383)\n",
      "[Epoch: 36740] train loss: 0.1642, train acc: 0.9393, val loss: 0.4146, val acc: 0.9568  (best train acc: 0.9535, best val acc: 0.9585, best train loss: 0.1383)\n",
      "[Epoch: 36760] train loss: 0.1586, train acc: 0.9451, val loss: 0.4111, val acc: 0.9531  (best train acc: 0.9535, best val acc: 0.9585, best train loss: 0.1383)\n",
      "[Epoch: 36780] train loss: 0.1443, train acc: 0.9519, val loss: 0.4275, val acc: 0.9555  (best train acc: 0.9535, best val acc: 0.9585, best train loss: 0.1383)\n",
      "[Epoch: 36800] train loss: 0.1599, train acc: 0.9421, val loss: 0.4550, val acc: 0.9457  (best train acc: 0.9535, best val acc: 0.9585, best train loss: 0.1383)\n",
      "[Epoch: 36820] train loss: 0.1534, train acc: 0.9461, val loss: 0.4185, val acc: 0.9551  (best train acc: 0.9535, best val acc: 0.9585, best train loss: 0.1383)\n",
      "[Epoch: 36840] train loss: 0.1686, train acc: 0.9418, val loss: 0.4147, val acc: 0.9518  (best train acc: 0.9535, best val acc: 0.9585, best train loss: 0.1383)\n",
      "[Epoch: 36860] train loss: 0.1603, train acc: 0.9432, val loss: 0.4151, val acc: 0.9555  (best train acc: 0.9535, best val acc: 0.9585, best train loss: 0.1383)\n",
      "[Epoch: 36880] train loss: 0.1500, train acc: 0.9461, val loss: 0.4093, val acc: 0.9558  (best train acc: 0.9535, best val acc: 0.9585, best train loss: 0.1383)\n",
      "[Epoch: 36900] train loss: 0.1604, train acc: 0.9454, val loss: 0.4067, val acc: 0.9551  (best train acc: 0.9535, best val acc: 0.9585, best train loss: 0.1383)\n",
      "[Epoch: 36920] train loss: 0.1635, train acc: 0.9459, val loss: 0.3659, val acc: 0.9508  (best train acc: 0.9535, best val acc: 0.9585, best train loss: 0.1383)\n",
      "[Epoch: 36940] train loss: 0.1562, train acc: 0.9473, val loss: 0.3286, val acc: 0.9535  (best train acc: 0.9535, best val acc: 0.9585, best train loss: 0.1383)\n",
      "[Epoch: 36960] train loss: 0.1445, train acc: 0.9482, val loss: 0.4032, val acc: 0.9575  (best train acc: 0.9535, best val acc: 0.9585, best train loss: 0.1383)\n",
      "[Epoch: 36980] train loss: 0.1554, train acc: 0.9480, val loss: 0.4387, val acc: 0.9565  (best train acc: 0.9535, best val acc: 0.9585, best train loss: 0.1383)\n",
      "[Epoch: 37000] train loss: 0.1529, train acc: 0.9455, val loss: 0.3838, val acc: 0.9551  (best train acc: 0.9535, best val acc: 0.9585, best train loss: 0.1383)\n",
      "[Epoch: 37020] train loss: 0.1493, train acc: 0.9493, val loss: 0.3765, val acc: 0.9558  (best train acc: 0.9535, best val acc: 0.9585, best train loss: 0.1383)\n",
      "[Epoch: 37040] train loss: 0.1613, train acc: 0.9437, val loss: 0.4083, val acc: 0.9427  (best train acc: 0.9535, best val acc: 0.9585, best train loss: 0.1383)\n",
      "[Epoch: 37060] train loss: 0.1477, train acc: 0.9473, val loss: 0.4255, val acc: 0.9555  (best train acc: 0.9535, best val acc: 0.9585, best train loss: 0.1383)\n",
      "[Epoch: 37080] train loss: 0.1590, train acc: 0.9427, val loss: 0.4193, val acc: 0.9531  (best train acc: 0.9535, best val acc: 0.9585, best train loss: 0.1383)\n",
      "[Epoch: 37100] train loss: 0.1425, train acc: 0.9503, val loss: 0.3867, val acc: 0.9531  (best train acc: 0.9535, best val acc: 0.9585, best train loss: 0.1383)\n",
      "[Epoch: 37120] train loss: 0.1474, train acc: 0.9485, val loss: 0.4372, val acc: 0.9535  (best train acc: 0.9535, best val acc: 0.9585, best train loss: 0.1383)\n",
      "[Epoch: 37140] train loss: 0.1521, train acc: 0.9461, val loss: 0.4104, val acc: 0.9464  (best train acc: 0.9535, best val acc: 0.9585, best train loss: 0.1383)\n",
      "[Epoch: 37160] train loss: 0.1507, train acc: 0.9445, val loss: 0.4401, val acc: 0.9562  (best train acc: 0.9535, best val acc: 0.9585, best train loss: 0.1383)\n",
      "[Epoch: 37180] train loss: 0.1562, train acc: 0.9403, val loss: 0.4255, val acc: 0.9514  (best train acc: 0.9535, best val acc: 0.9585, best train loss: 0.1383)\n",
      "[Epoch: 37200] train loss: 0.1447, train acc: 0.9482, val loss: 0.3805, val acc: 0.9531  (best train acc: 0.9535, best val acc: 0.9585, best train loss: 0.1383)\n",
      "[Epoch: 37220] train loss: 0.1609, train acc: 0.9422, val loss: 0.4285, val acc: 0.9562  (best train acc: 0.9535, best val acc: 0.9585, best train loss: 0.1383)\n",
      "[Epoch: 37240] train loss: 0.1538, train acc: 0.9474, val loss: 0.4377, val acc: 0.9565  (best train acc: 0.9535, best val acc: 0.9585, best train loss: 0.1383)\n",
      "[Epoch: 37260] train loss: 0.1539, train acc: 0.9428, val loss: 0.4288, val acc: 0.9538  (best train acc: 0.9535, best val acc: 0.9585, best train loss: 0.1383)\n",
      "[Epoch: 37280] train loss: 0.1537, train acc: 0.9468, val loss: 0.4635, val acc: 0.9538  (best train acc: 0.9535, best val acc: 0.9585, best train loss: 0.1383)\n",
      "[Epoch: 37300] train loss: 0.1591, train acc: 0.9429, val loss: 0.3560, val acc: 0.9572  (best train acc: 0.9535, best val acc: 0.9585, best train loss: 0.1383)\n",
      "[Epoch: 37320] train loss: 0.1554, train acc: 0.9445, val loss: 0.4437, val acc: 0.9555  (best train acc: 0.9535, best val acc: 0.9585, best train loss: 0.1383)\n",
      "[Epoch: 37340] train loss: 0.1486, train acc: 0.9479, val loss: 0.4293, val acc: 0.9548  (best train acc: 0.9535, best val acc: 0.9585, best train loss: 0.1377)\n",
      "[Epoch: 37360] train loss: 0.1610, train acc: 0.9414, val loss: 0.3475, val acc: 0.9518  (best train acc: 0.9535, best val acc: 0.9595, best train loss: 0.1377)\n",
      "[Epoch: 37380] train loss: 0.1652, train acc: 0.9396, val loss: 0.4123, val acc: 0.9524  (best train acc: 0.9535, best val acc: 0.9595, best train loss: 0.1377)\n",
      "[Epoch: 37400] train loss: 0.1524, train acc: 0.9492, val loss: 0.4257, val acc: 0.9565  (best train acc: 0.9535, best val acc: 0.9595, best train loss: 0.1377)\n",
      "[Epoch: 37420] train loss: 0.1569, train acc: 0.9454, val loss: 0.4176, val acc: 0.9575  (best train acc: 0.9535, best val acc: 0.9595, best train loss: 0.1377)\n",
      "[Epoch: 37440] train loss: 0.1502, train acc: 0.9495, val loss: 0.4215, val acc: 0.9558  (best train acc: 0.9535, best val acc: 0.9599, best train loss: 0.1373)\n",
      "[Epoch: 37460] train loss: 0.1746, train acc: 0.9370, val loss: 0.4480, val acc: 0.9420  (best train acc: 0.9535, best val acc: 0.9599, best train loss: 0.1373)\n",
      "[Epoch: 37480] train loss: 0.1887, train acc: 0.9367, val loss: 0.4121, val acc: 0.9322  (best train acc: 0.9535, best val acc: 0.9599, best train loss: 0.1373)\n",
      "[Epoch: 37500] train loss: 0.1826, train acc: 0.9366, val loss: 0.3548, val acc: 0.9524  (best train acc: 0.9535, best val acc: 0.9599, best train loss: 0.1373)\n",
      "[Epoch: 37520] train loss: 0.1563, train acc: 0.9450, val loss: 0.4101, val acc: 0.9585  (best train acc: 0.9535, best val acc: 0.9599, best train loss: 0.1373)\n",
      "[Epoch: 37540] train loss: 0.1560, train acc: 0.9466, val loss: 0.4423, val acc: 0.9541  (best train acc: 0.9535, best val acc: 0.9599, best train loss: 0.1373)\n",
      "[Epoch: 37560] train loss: 0.1477, train acc: 0.9471, val loss: 0.4207, val acc: 0.9491  (best train acc: 0.9535, best val acc: 0.9599, best train loss: 0.1373)\n",
      "[Epoch: 37580] train loss: 0.1563, train acc: 0.9453, val loss: 0.4322, val acc: 0.9578  (best train acc: 0.9535, best val acc: 0.9599, best train loss: 0.1373)\n",
      "[Epoch: 37600] train loss: 0.1451, train acc: 0.9497, val loss: 0.4358, val acc: 0.9548  (best train acc: 0.9535, best val acc: 0.9599, best train loss: 0.1373)\n",
      "[Epoch: 37620] train loss: 0.1566, train acc: 0.9462, val loss: 0.4746, val acc: 0.9487  (best train acc: 0.9535, best val acc: 0.9599, best train loss: 0.1373)\n",
      "[Epoch: 37640] train loss: 0.1819, train acc: 0.9382, val loss: 0.4080, val acc: 0.9555  (best train acc: 0.9535, best val acc: 0.9599, best train loss: 0.1373)\n",
      "[Epoch: 37660] train loss: 0.1494, train acc: 0.9477, val loss: 0.4340, val acc: 0.9555  (best train acc: 0.9535, best val acc: 0.9599, best train loss: 0.1373)\n",
      "[Epoch: 37680] train loss: 0.1482, train acc: 0.9464, val loss: 0.4467, val acc: 0.9575  (best train acc: 0.9535, best val acc: 0.9599, best train loss: 0.1373)\n",
      "[Epoch: 37700] train loss: 0.1406, train acc: 0.9516, val loss: 0.4301, val acc: 0.9572  (best train acc: 0.9535, best val acc: 0.9599, best train loss: 0.1373)\n",
      "[Epoch: 37720] train loss: 0.1556, train acc: 0.9456, val loss: 0.4314, val acc: 0.9575  (best train acc: 0.9535, best val acc: 0.9599, best train loss: 0.1373)\n",
      "[Epoch: 37740] train loss: 0.1464, train acc: 0.9488, val loss: 0.3908, val acc: 0.9494  (best train acc: 0.9535, best val acc: 0.9599, best train loss: 0.1373)\n",
      "[Epoch: 37760] train loss: 0.1528, train acc: 0.9453, val loss: 0.3989, val acc: 0.9528  (best train acc: 0.9535, best val acc: 0.9599, best train loss: 0.1373)\n",
      "[Epoch: 37780] train loss: 0.1568, train acc: 0.9425, val loss: 0.4800, val acc: 0.9551  (best train acc: 0.9535, best val acc: 0.9599, best train loss: 0.1373)\n",
      "[Epoch: 37800] train loss: 0.1535, train acc: 0.9477, val loss: 0.4375, val acc: 0.9484  (best train acc: 0.9535, best val acc: 0.9599, best train loss: 0.1373)\n",
      "[Epoch: 37820] train loss: 0.1477, train acc: 0.9474, val loss: 0.3862, val acc: 0.9541  (best train acc: 0.9535, best val acc: 0.9599, best train loss: 0.1373)\n",
      "[Epoch: 37840] train loss: 0.1546, train acc: 0.9435, val loss: 0.3708, val acc: 0.9555  (best train acc: 0.9535, best val acc: 0.9599, best train loss: 0.1373)\n",
      "[Epoch: 37860] train loss: 0.1608, train acc: 0.9412, val loss: 0.4377, val acc: 0.9491  (best train acc: 0.9535, best val acc: 0.9599, best train loss: 0.1373)\n",
      "[Epoch: 37880] train loss: 0.1693, train acc: 0.9413, val loss: 0.3548, val acc: 0.9484  (best train acc: 0.9535, best val acc: 0.9599, best train loss: 0.1373)\n",
      "[Epoch: 37900] train loss: 0.1525, train acc: 0.9478, val loss: 0.4078, val acc: 0.9585  (best train acc: 0.9535, best val acc: 0.9599, best train loss: 0.1373)\n",
      "[Epoch: 37920] train loss: 0.1473, train acc: 0.9464, val loss: 0.3786, val acc: 0.9578  (best train acc: 0.9535, best val acc: 0.9599, best train loss: 0.1358)\n",
      "[Epoch: 37940] train loss: 0.1548, train acc: 0.9463, val loss: 0.3739, val acc: 0.9558  (best train acc: 0.9535, best val acc: 0.9599, best train loss: 0.1358)\n",
      "[Epoch: 37960] train loss: 0.1523, train acc: 0.9457, val loss: 0.4221, val acc: 0.9585  (best train acc: 0.9535, best val acc: 0.9599, best train loss: 0.1358)\n",
      "[Epoch: 37980] train loss: 0.1693, train acc: 0.9395, val loss: 0.3547, val acc: 0.9531  (best train acc: 0.9535, best val acc: 0.9599, best train loss: 0.1358)\n",
      "[Epoch: 38000] train loss: 0.1397, train acc: 0.9532, val loss: 0.4267, val acc: 0.9521  (best train acc: 0.9535, best val acc: 0.9599, best train loss: 0.1358)\n",
      "[Epoch: 38020] train loss: 0.1617, train acc: 0.9430, val loss: 0.4101, val acc: 0.9454  (best train acc: 0.9535, best val acc: 0.9599, best train loss: 0.1358)\n",
      "[Epoch: 38040] train loss: 0.1604, train acc: 0.9397, val loss: 0.3723, val acc: 0.9551  (best train acc: 0.9535, best val acc: 0.9599, best train loss: 0.1358)\n",
      "[Epoch: 38060] train loss: 0.1447, train acc: 0.9508, val loss: 0.4025, val acc: 0.9585  (best train acc: 0.9535, best val acc: 0.9599, best train loss: 0.1358)\n",
      "[Epoch: 38080] train loss: 0.1492, train acc: 0.9471, val loss: 0.3580, val acc: 0.9551  (best train acc: 0.9535, best val acc: 0.9599, best train loss: 0.1358)\n",
      "[Epoch: 38100] train loss: 0.1799, train acc: 0.9366, val loss: 0.4138, val acc: 0.9568  (best train acc: 0.9535, best val acc: 0.9599, best train loss: 0.1358)\n",
      "[Epoch: 38120] train loss: 0.1565, train acc: 0.9454, val loss: 0.4002, val acc: 0.9535  (best train acc: 0.9535, best val acc: 0.9599, best train loss: 0.1358)\n",
      "[Epoch: 38140] train loss: 0.1758, train acc: 0.9341, val loss: 0.3986, val acc: 0.9514  (best train acc: 0.9535, best val acc: 0.9599, best train loss: 0.1358)\n",
      "[Epoch: 38160] train loss: 0.1418, train acc: 0.9505, val loss: 0.4377, val acc: 0.9568  (best train acc: 0.9535, best val acc: 0.9599, best train loss: 0.1358)\n",
      "[Epoch: 38180] train loss: 0.1525, train acc: 0.9464, val loss: 0.4591, val acc: 0.9568  (best train acc: 0.9535, best val acc: 0.9599, best train loss: 0.1358)\n",
      "[Epoch: 38200] train loss: 0.1511, train acc: 0.9455, val loss: 0.3860, val acc: 0.9575  (best train acc: 0.9535, best val acc: 0.9599, best train loss: 0.1358)\n",
      "[Epoch: 38220] train loss: 0.1467, train acc: 0.9503, val loss: 0.4371, val acc: 0.9582  (best train acc: 0.9535, best val acc: 0.9599, best train loss: 0.1358)\n",
      "[Epoch: 38240] train loss: 0.1435, train acc: 0.9488, val loss: 0.4243, val acc: 0.9572  (best train acc: 0.9535, best val acc: 0.9599, best train loss: 0.1358)\n",
      "[Epoch: 38260] train loss: 0.1491, train acc: 0.9468, val loss: 0.4303, val acc: 0.9535  (best train acc: 0.9535, best val acc: 0.9599, best train loss: 0.1358)\n",
      "[Epoch: 38280] train loss: 0.1574, train acc: 0.9418, val loss: 0.4117, val acc: 0.9541  (best train acc: 0.9535, best val acc: 0.9599, best train loss: 0.1358)\n",
      "[Epoch: 38300] train loss: 0.1514, train acc: 0.9455, val loss: 0.4291, val acc: 0.9578  (best train acc: 0.9535, best val acc: 0.9599, best train loss: 0.1358)\n",
      "[Epoch: 38320] train loss: 0.1507, train acc: 0.9495, val loss: 0.4775, val acc: 0.9508  (best train acc: 0.9539, best val acc: 0.9599, best train loss: 0.1358)\n",
      "[Epoch: 38340] train loss: 0.1657, train acc: 0.9391, val loss: 0.3502, val acc: 0.9528  (best train acc: 0.9539, best val acc: 0.9599, best train loss: 0.1358)\n",
      "[Epoch: 38360] train loss: 0.1428, train acc: 0.9502, val loss: 0.4481, val acc: 0.9518  (best train acc: 0.9539, best val acc: 0.9599, best train loss: 0.1358)\n",
      "[Epoch: 38380] train loss: 0.1922, train acc: 0.9323, val loss: 0.3742, val acc: 0.9531  (best train acc: 0.9539, best val acc: 0.9599, best train loss: 0.1358)\n",
      "[Epoch: 38400] train loss: 0.1731, train acc: 0.9412, val loss: 0.3808, val acc: 0.9545  (best train acc: 0.9539, best val acc: 0.9599, best train loss: 0.1358)\n",
      "[Epoch: 38420] train loss: 0.1737, train acc: 0.9370, val loss: 0.4126, val acc: 0.9501  (best train acc: 0.9539, best val acc: 0.9599, best train loss: 0.1358)\n",
      "[Epoch: 38440] train loss: 0.1518, train acc: 0.9474, val loss: 0.3866, val acc: 0.9528  (best train acc: 0.9539, best val acc: 0.9599, best train loss: 0.1358)\n",
      "[Epoch: 38460] train loss: 0.1603, train acc: 0.9442, val loss: 0.4228, val acc: 0.9565  (best train acc: 0.9539, best val acc: 0.9599, best train loss: 0.1358)\n",
      "[Epoch: 38480] train loss: 0.1465, train acc: 0.9461, val loss: 0.4345, val acc: 0.9545  (best train acc: 0.9539, best val acc: 0.9599, best train loss: 0.1358)\n",
      "[Epoch: 38500] train loss: 0.1487, train acc: 0.9467, val loss: 0.4621, val acc: 0.9568  (best train acc: 0.9539, best val acc: 0.9599, best train loss: 0.1358)\n",
      "[Epoch: 38520] train loss: 0.1471, train acc: 0.9481, val loss: 0.4342, val acc: 0.9572  (best train acc: 0.9539, best val acc: 0.9599, best train loss: 0.1358)\n",
      "[Epoch: 38540] train loss: 0.1430, train acc: 0.9478, val loss: 0.3718, val acc: 0.9541  (best train acc: 0.9539, best val acc: 0.9599, best train loss: 0.1358)\n",
      "[Epoch: 38560] train loss: 0.1500, train acc: 0.9451, val loss: 0.4770, val acc: 0.9548  (best train acc: 0.9539, best val acc: 0.9599, best train loss: 0.1358)\n",
      "[Epoch: 38580] train loss: 0.1396, train acc: 0.9510, val loss: 0.4324, val acc: 0.9528  (best train acc: 0.9539, best val acc: 0.9599, best train loss: 0.1358)\n",
      "[Epoch: 38600] train loss: 0.1593, train acc: 0.9453, val loss: 0.4522, val acc: 0.9548  (best train acc: 0.9539, best val acc: 0.9599, best train loss: 0.1358)\n",
      "[Epoch: 38620] train loss: 0.1557, train acc: 0.9455, val loss: 0.4579, val acc: 0.9558  (best train acc: 0.9539, best val acc: 0.9599, best train loss: 0.1355)\n",
      "[Epoch: 38640] train loss: 0.1536, train acc: 0.9478, val loss: 0.4132, val acc: 0.9562  (best train acc: 0.9539, best val acc: 0.9599, best train loss: 0.1355)\n",
      "[Epoch: 38660] train loss: 0.1646, train acc: 0.9408, val loss: 0.3833, val acc: 0.9497  (best train acc: 0.9539, best val acc: 0.9599, best train loss: 0.1355)\n",
      "[Epoch: 38680] train loss: 0.1493, train acc: 0.9472, val loss: 0.4602, val acc: 0.9582  (best train acc: 0.9539, best val acc: 0.9602, best train loss: 0.1355)\n",
      "[Epoch: 38700] train loss: 0.1679, train acc: 0.9370, val loss: 0.3810, val acc: 0.9528  (best train acc: 0.9539, best val acc: 0.9602, best train loss: 0.1355)\n",
      "[Epoch: 38720] train loss: 0.1548, train acc: 0.9435, val loss: 0.4212, val acc: 0.9467  (best train acc: 0.9539, best val acc: 0.9602, best train loss: 0.1355)\n",
      "[Epoch: 38740] train loss: 0.1495, train acc: 0.9479, val loss: 0.4136, val acc: 0.9572  (best train acc: 0.9539, best val acc: 0.9602, best train loss: 0.1355)\n",
      "[Epoch: 38760] train loss: 0.1401, train acc: 0.9510, val loss: 0.4474, val acc: 0.9568  (best train acc: 0.9539, best val acc: 0.9605, best train loss: 0.1355)\n",
      "[Epoch: 38780] train loss: 0.1604, train acc: 0.9440, val loss: 0.4742, val acc: 0.9592  (best train acc: 0.9539, best val acc: 0.9605, best train loss: 0.1355)\n",
      "[Epoch: 38800] train loss: 0.1476, train acc: 0.9488, val loss: 0.3956, val acc: 0.9474  (best train acc: 0.9539, best val acc: 0.9605, best train loss: 0.1355)\n",
      "[Epoch: 38820] train loss: 0.1557, train acc: 0.9435, val loss: 0.4361, val acc: 0.9568  (best train acc: 0.9539, best val acc: 0.9605, best train loss: 0.1355)\n",
      "[Epoch: 38840] train loss: 0.1625, train acc: 0.9425, val loss: 0.3986, val acc: 0.9545  (best train acc: 0.9539, best val acc: 0.9605, best train loss: 0.1355)\n",
      "[Epoch: 38860] train loss: 0.1717, train acc: 0.9392, val loss: 0.3890, val acc: 0.9447  (best train acc: 0.9539, best val acc: 0.9605, best train loss: 0.1355)\n",
      "[Epoch: 38880] train loss: 0.1480, train acc: 0.9468, val loss: 0.4460, val acc: 0.9568  (best train acc: 0.9539, best val acc: 0.9605, best train loss: 0.1355)\n",
      "[Epoch: 38900] train loss: 0.1541, train acc: 0.9420, val loss: 0.4662, val acc: 0.9548  (best train acc: 0.9539, best val acc: 0.9605, best train loss: 0.1355)\n",
      "[Epoch: 38920] train loss: 0.1533, train acc: 0.9458, val loss: 0.4514, val acc: 0.9572  (best train acc: 0.9539, best val acc: 0.9605, best train loss: 0.1332)\n",
      "[Epoch: 38940] train loss: 0.1642, train acc: 0.9371, val loss: 0.4231, val acc: 0.9477  (best train acc: 0.9539, best val acc: 0.9605, best train loss: 0.1332)\n",
      "[Epoch: 38960] train loss: 0.1430, train acc: 0.9500, val loss: 0.4640, val acc: 0.9555  (best train acc: 0.9539, best val acc: 0.9605, best train loss: 0.1332)\n",
      "[Epoch: 38980] train loss: 0.1487, train acc: 0.9489, val loss: 0.4352, val acc: 0.9541  (best train acc: 0.9539, best val acc: 0.9605, best train loss: 0.1332)\n",
      "[Epoch: 39000] train loss: 0.1548, train acc: 0.9452, val loss: 0.4180, val acc: 0.9548  (best train acc: 0.9539, best val acc: 0.9605, best train loss: 0.1332)\n",
      "[Epoch: 39020] train loss: 0.1605, train acc: 0.9409, val loss: 0.4234, val acc: 0.9538  (best train acc: 0.9539, best val acc: 0.9605, best train loss: 0.1332)\n",
      "[Epoch: 39040] train loss: 0.1579, train acc: 0.9422, val loss: 0.3464, val acc: 0.9541  (best train acc: 0.9539, best val acc: 0.9605, best train loss: 0.1332)\n",
      "[Epoch: 39060] train loss: 0.1514, train acc: 0.9453, val loss: 0.3840, val acc: 0.9548  (best train acc: 0.9539, best val acc: 0.9605, best train loss: 0.1332)\n",
      "[Epoch: 39080] train loss: 0.1535, train acc: 0.9463, val loss: 0.4130, val acc: 0.9565  (best train acc: 0.9539, best val acc: 0.9605, best train loss: 0.1332)\n",
      "[Epoch: 39100] train loss: 0.1472, train acc: 0.9455, val loss: 0.4088, val acc: 0.9575  (best train acc: 0.9539, best val acc: 0.9605, best train loss: 0.1332)\n",
      "[Epoch: 39120] train loss: 0.1411, train acc: 0.9513, val loss: 0.3900, val acc: 0.9565  (best train acc: 0.9539, best val acc: 0.9605, best train loss: 0.1332)\n",
      "[Epoch: 39140] train loss: 0.1487, train acc: 0.9465, val loss: 0.4382, val acc: 0.9457  (best train acc: 0.9539, best val acc: 0.9605, best train loss: 0.1332)\n",
      "[Epoch: 39160] train loss: 0.1576, train acc: 0.9435, val loss: 0.3649, val acc: 0.9528  (best train acc: 0.9539, best val acc: 0.9605, best train loss: 0.1332)\n",
      "[Epoch: 39180] train loss: 0.1541, train acc: 0.9459, val loss: 0.4074, val acc: 0.9548  (best train acc: 0.9539, best val acc: 0.9605, best train loss: 0.1332)\n",
      "[Epoch: 39200] train loss: 0.1662, train acc: 0.9377, val loss: 0.3837, val acc: 0.9541  (best train acc: 0.9539, best val acc: 0.9605, best train loss: 0.1332)\n",
      "[Epoch: 39220] train loss: 0.1573, train acc: 0.9439, val loss: 0.4124, val acc: 0.9545  (best train acc: 0.9539, best val acc: 0.9605, best train loss: 0.1332)\n",
      "[Epoch: 39240] train loss: 0.1455, train acc: 0.9487, val loss: 0.4225, val acc: 0.9582  (best train acc: 0.9545, best val acc: 0.9605, best train loss: 0.1332)\n",
      "[Epoch: 39260] train loss: 0.1488, train acc: 0.9483, val loss: 0.4381, val acc: 0.9568  (best train acc: 0.9545, best val acc: 0.9605, best train loss: 0.1332)\n",
      "[Epoch: 39280] train loss: 0.1434, train acc: 0.9503, val loss: 0.4372, val acc: 0.9558  (best train acc: 0.9545, best val acc: 0.9605, best train loss: 0.1332)\n",
      "[Epoch: 39300] train loss: 0.1828, train acc: 0.9371, val loss: 0.3435, val acc: 0.9477  (best train acc: 0.9545, best val acc: 0.9605, best train loss: 0.1332)\n",
      "[Epoch: 39320] train loss: 0.1543, train acc: 0.9466, val loss: 0.4278, val acc: 0.9528  (best train acc: 0.9545, best val acc: 0.9605, best train loss: 0.1332)\n",
      "[Epoch: 39340] train loss: 0.2368, train acc: 0.9125, val loss: 0.2509, val acc: 0.9430  (best train acc: 0.9545, best val acc: 0.9605, best train loss: 0.1332)\n",
      "[Epoch: 39360] train loss: 0.1506, train acc: 0.9452, val loss: 0.3769, val acc: 0.9555  (best train acc: 0.9545, best val acc: 0.9605, best train loss: 0.1332)\n",
      "[Epoch: 39380] train loss: 0.1409, train acc: 0.9493, val loss: 0.4046, val acc: 0.9582  (best train acc: 0.9545, best val acc: 0.9605, best train loss: 0.1332)\n",
      "[Epoch: 39400] train loss: 0.1415, train acc: 0.9498, val loss: 0.4199, val acc: 0.9585  (best train acc: 0.9545, best val acc: 0.9605, best train loss: 0.1332)\n",
      "[Epoch: 39420] train loss: 0.1381, train acc: 0.9536, val loss: 0.4430, val acc: 0.9589  (best train acc: 0.9545, best val acc: 0.9605, best train loss: 0.1332)\n",
      "[Epoch: 39440] train loss: 0.1686, train acc: 0.9456, val loss: 0.4848, val acc: 0.9454  (best train acc: 0.9545, best val acc: 0.9605, best train loss: 0.1332)\n",
      "[Epoch: 39460] train loss: 0.1520, train acc: 0.9448, val loss: 0.4431, val acc: 0.9535  (best train acc: 0.9545, best val acc: 0.9605, best train loss: 0.1332)\n",
      "[Epoch: 39480] train loss: 0.1407, train acc: 0.9485, val loss: 0.4709, val acc: 0.9585  (best train acc: 0.9545, best val acc: 0.9605, best train loss: 0.1332)\n",
      "[Epoch: 39500] train loss: 0.1589, train acc: 0.9399, val loss: 0.4233, val acc: 0.9582  (best train acc: 0.9545, best val acc: 0.9605, best train loss: 0.1332)\n",
      "[Epoch: 39520] train loss: 0.1616, train acc: 0.9441, val loss: 0.4672, val acc: 0.9562  (best train acc: 0.9545, best val acc: 0.9605, best train loss: 0.1332)\n",
      "[Epoch: 39540] train loss: 0.1464, train acc: 0.9504, val loss: 0.4547, val acc: 0.9582  (best train acc: 0.9545, best val acc: 0.9605, best train loss: 0.1332)\n",
      "[Epoch: 39560] train loss: 0.1428, train acc: 0.9492, val loss: 0.4541, val acc: 0.9578  (best train acc: 0.9545, best val acc: 0.9605, best train loss: 0.1326)\n",
      "[Epoch: 39580] train loss: 0.1414, train acc: 0.9492, val loss: 0.4470, val acc: 0.9572  (best train acc: 0.9545, best val acc: 0.9605, best train loss: 0.1326)\n",
      "[Epoch: 39600] train loss: 0.1514, train acc: 0.9459, val loss: 0.4528, val acc: 0.9555  (best train acc: 0.9545, best val acc: 0.9605, best train loss: 0.1326)\n",
      "[Epoch: 39620] train loss: 0.1581, train acc: 0.9445, val loss: 0.4404, val acc: 0.9521  (best train acc: 0.9545, best val acc: 0.9605, best train loss: 0.1326)\n",
      "[Epoch: 39640] train loss: 0.1453, train acc: 0.9485, val loss: 0.4550, val acc: 0.9528  (best train acc: 0.9545, best val acc: 0.9605, best train loss: 0.1326)\n",
      "[Epoch: 39660] train loss: 0.1448, train acc: 0.9507, val loss: 0.4841, val acc: 0.9555  (best train acc: 0.9545, best val acc: 0.9605, best train loss: 0.1326)\n",
      "[Epoch: 39680] train loss: 0.1639, train acc: 0.9407, val loss: 0.3309, val acc: 0.9538  (best train acc: 0.9545, best val acc: 0.9605, best train loss: 0.1326)\n",
      "[Epoch: 39700] train loss: 0.1490, train acc: 0.9481, val loss: 0.4475, val acc: 0.9541  (best train acc: 0.9545, best val acc: 0.9605, best train loss: 0.1326)\n",
      "[Epoch: 39720] train loss: 0.1532, train acc: 0.9432, val loss: 0.4687, val acc: 0.9572  (best train acc: 0.9545, best val acc: 0.9605, best train loss: 0.1326)\n",
      "[Epoch: 39740] train loss: 0.1467, train acc: 0.9499, val loss: 0.4670, val acc: 0.9565  (best train acc: 0.9545, best val acc: 0.9605, best train loss: 0.1326)\n",
      "[Epoch: 39760] train loss: 0.1460, train acc: 0.9474, val loss: 0.4316, val acc: 0.9541  (best train acc: 0.9545, best val acc: 0.9605, best train loss: 0.1326)\n",
      "[Epoch: 39780] train loss: 0.1537, train acc: 0.9477, val loss: 0.4619, val acc: 0.9528  (best train acc: 0.9545, best val acc: 0.9605, best train loss: 0.1326)\n",
      "[Epoch: 39800] train loss: 0.1713, train acc: 0.9380, val loss: 0.4309, val acc: 0.9548  (best train acc: 0.9545, best val acc: 0.9605, best train loss: 0.1326)\n",
      "[Epoch: 39820] train loss: 0.1547, train acc: 0.9429, val loss: 0.4641, val acc: 0.9373  (best train acc: 0.9545, best val acc: 0.9605, best train loss: 0.1326)\n",
      "[Epoch: 39840] train loss: 0.1576, train acc: 0.9419, val loss: 0.3656, val acc: 0.9558  (best train acc: 0.9545, best val acc: 0.9605, best train loss: 0.1326)\n",
      "[Epoch: 39860] train loss: 0.1456, train acc: 0.9492, val loss: 0.4172, val acc: 0.9562  (best train acc: 0.9545, best val acc: 0.9605, best train loss: 0.1326)\n",
      "[Epoch: 39880] train loss: 0.1531, train acc: 0.9469, val loss: 0.4846, val acc: 0.9548  (best train acc: 0.9545, best val acc: 0.9605, best train loss: 0.1326)\n",
      "[Epoch: 39900] train loss: 0.1476, train acc: 0.9495, val loss: 0.4635, val acc: 0.9558  (best train acc: 0.9545, best val acc: 0.9605, best train loss: 0.1326)\n",
      "[Epoch: 39920] train loss: 0.1504, train acc: 0.9445, val loss: 0.4696, val acc: 0.9575  (best train acc: 0.9545, best val acc: 0.9605, best train loss: 0.1326)\n",
      "[Epoch: 39940] train loss: 0.1417, train acc: 0.9479, val loss: 0.4771, val acc: 0.9575  (best train acc: 0.9556, best val acc: 0.9605, best train loss: 0.1326)\n",
      "[Epoch: 39960] train loss: 0.1493, train acc: 0.9467, val loss: 0.4618, val acc: 0.9467  (best train acc: 0.9556, best val acc: 0.9605, best train loss: 0.1326)\n",
      "[Epoch: 39980] train loss: 0.1457, train acc: 0.9506, val loss: 0.4525, val acc: 0.9514  (best train acc: 0.9556, best val acc: 0.9605, best train loss: 0.1326)\n",
      "[Epoch: 40000] train loss: 0.1558, train acc: 0.9432, val loss: 0.4498, val acc: 0.9551  (best train acc: 0.9556, best val acc: 0.9605, best train loss: 0.1326)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.77      0.86       154\n",
      "           1       0.97      0.98      0.98       703\n",
      "           2       0.94      0.94      0.94       702\n",
      "           3       0.93      0.96      0.94       703\n",
      "           4       0.99      1.00      0.99       702\n",
      "\n",
      "    accuracy                           0.96      2964\n",
      "   macro avg       0.96      0.93      0.94      2964\n",
      "weighted avg       0.96      0.96      0.96      2964\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 504x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 504x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "gnn_sage = GNN7L_Sage(data_with_nedbit).to(device)\n",
    "pred = train(gnn_sage, data_with_nedbit.to(device), 40000, cm_title='SAGE7L_multiclass_16HC_v2', classes=['P', 'LP', 'WN', 'LN', 'RN'], weight_decay=0.0005)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6aeea6fe202f5c7201d5940e4573c0a76b23e4e16f0e3784ac81597546f2b3b3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
