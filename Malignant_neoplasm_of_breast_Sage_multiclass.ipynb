{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.11.0\n",
      "2.0.4\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "import networkx as nx\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric\n",
    "from torch_geometric.nn.conv import SAGEConv\n",
    "from torch_geometric.utils import from_networkx\n",
    "from torch_geometric.data import InMemoryDataset\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(torch.__version__)\n",
    "print(torch_geometric.__version__)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN2L_Sage (nn.Module):\n",
    "    def __init__(self, data):\n",
    "        super().__init__()\n",
    "        self.conv1 = SAGEConv(data.num_features, 16, aggr='max')\n",
    "        self.conv2 = SAGEConv(16, int(data.num_classes), aggr='max')\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "class GNN4L_Sage (nn.Module):\n",
    "    def __init__(self, data):\n",
    "        super().__init__()\n",
    "        self.conv1 = SAGEConv(data.num_features, 16, aggr='max')\n",
    "        self.conv2 = SAGEConv(16, 16, aggr='max')\n",
    "        self.conv3 = SAGEConv(16, 16, aggr='max')\n",
    "        self.conv4 = SAGEConv(16, int(data.num_classes), aggr='max')\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        x = F.relu(self.conv3(x, edge_index))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv4(x, edge_index)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "class GNN7L_Sage (nn.Module):\n",
    "    def __init__(self, data):\n",
    "        super().__init__()\n",
    "        self.conv1 = SAGEConv(data.num_features, 16)\n",
    "        self.conv2 = SAGEConv(16, 16)\n",
    "        self.conv3 = SAGEConv(16, 16)\n",
    "        self.conv4 = SAGEConv(16, 16)\n",
    "        self.conv5 = SAGEConv(16, 16)\n",
    "        self.conv6 = SAGEConv(16, 16)\n",
    "        self.conv7 = SAGEConv(16, int(data.num_classes))\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        x = F.relu(self.conv3(x, edge_index))\n",
    "        x = F.relu(self.conv4(x, edge_index))\n",
    "        x = F.relu(self.conv5(x, edge_index))\n",
    "        x = F.relu(self.conv6(x, edge_index))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv7(x, edge_index)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset class: it allows to translate a vector (Graph, Attributes, Labels)\n",
    "# into a dataset compatible with the PyTorch models.\n",
    "# \n",
    "# Parameters:\n",
    "# - G: NetworkX graph\n",
    "# - Labels: of the nodes used for classification\n",
    "# - attributes: List of the nodes' attributes\n",
    "\n",
    "class MyDataset(InMemoryDataset):\n",
    "  def __init__(self, G, labels, attributes, num_classes=2):\n",
    "    super(MyDataset, self).__init__('.', None, None, None)\n",
    "\n",
    "    # import data from the networkx graph with the attributes of the nodes\n",
    "    data = from_networkx(G, attributes)\n",
    "      \n",
    "    y = torch.from_numpy(labels).type(torch.long)\n",
    "\n",
    "    data.x = data.x.float()\n",
    "    data.y = y.clone().detach()\n",
    "    data.num_classes = num_classes\n",
    "\n",
    "    # Using train_test_split function from sklearn to stratify train/test/val sets\n",
    "    indices = range(G.number_of_nodes())\n",
    "    # Stratified split of train/test/val sets. Returned indices are used to create the masks\n",
    "    X_train, X_test, y_train, y_test, train_idx, test_idx = train_test_split(data.x, data.y, indices, test_size=0.3, stratify=labels, random_state=42)\n",
    "    # To create validation set, test set is splitted in half\n",
    "    X_test, X_val, y_test, y_val, test_idx, val_idx = train_test_split(X_test, y_test, test_idx, test_size=0.5, stratify=y_test, random_state=42)\n",
    "\n",
    "    n_nodes = G.number_of_nodes()\n",
    "    train_mask  = torch.zeros(n_nodes, dtype=torch.bool)\n",
    "    test_mask   = torch.zeros(n_nodes, dtype=torch.bool)\n",
    "    val_mask    = torch.zeros(n_nodes, dtype=torch.bool)\n",
    "    \n",
    "    for idx in train_idx:\n",
    "      train_mask[idx] = True\n",
    "\n",
    "    for idx in test_idx:\n",
    "      test_mask[idx] = True\n",
    "    \n",
    "    for idx in val_idx:\n",
    "      val_mask[idx] = True\n",
    "\n",
    "    data['train_mask']  = train_mask\n",
    "    data['test_mask']   = test_mask\n",
    "    data['val_mask']    = val_mask\n",
    "\n",
    "    self.data, self.slices = self.collate([data])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data, epochs = 200, classes = ['0','1'], lr = 0.001, weight_decay=0, cm_title = 'GNN', num_layers = 7):\n",
    "    title = cm_title + '_' + str(epochs) + '_' + str(weight_decay).replace('.', '_')\n",
    "\n",
    "    model_path  = 'Models/' + title\n",
    "    image_path  = 'Images/' + title\n",
    "    report_path = 'Reports/' + title + '.csv'\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    # optimizer = torch.optim.RMSprop(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    train_mask  = data['train_mask']\n",
    "    test_mask   = data['test_mask']\n",
    "    val_mask    = data['val_mask']\n",
    "\n",
    "    labels    = data.y\n",
    "    output = ''\n",
    "\n",
    "    # list to plot the train accuracy\n",
    "    train_acc_curve = []\n",
    "    train_lss_curve = []\n",
    "\n",
    "    best_train_acc  = 0\n",
    "    best_val_acc    = 0\n",
    "    # best_test_acc   = 0\n",
    "    best_train_lss  = 999\n",
    "    best_loss_epoch = 0\n",
    "\n",
    "    for e in tqdm(range(epochs+1)):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        logits      = model(data)\n",
    "        output      = logits.argmax(1)\n",
    "        #Â train_loss  = F.cross_entropy(logits[train_mask], labels[train_mask])\n",
    "        train_loss  = F.nll_loss(logits[train_mask], labels[train_mask])\n",
    "        train_acc   = (output[train_mask] == labels[train_mask]).float().mean()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Append train acc. to plot curve later\n",
    "        train_acc_curve.append(train_acc.item())\n",
    "        train_lss_curve.append(train_loss.item())\n",
    "\n",
    "        if train_acc > best_train_acc:\n",
    "            best_train_acc = train_acc\n",
    "\n",
    "        # Evaluation and test\n",
    "        model.eval()\n",
    "        logits      = model(data)\n",
    "        output      = logits.argmax(1)\n",
    "        # val_loss    = F.cross_entropy(logits[val_mask], labels[val_mask])\n",
    "        val_loss    = F.nll_loss(logits[val_mask], labels[val_mask])\n",
    "        val_acc     = (output[val_mask] == labels[val_mask]).float().mean()\n",
    "        # test_acc    = (output[test_mask] == labels[test_mask]).float().mean()\n",
    "\n",
    "        # Update best test/val acc.\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "        \n",
    "        # if test_acc > best_test_acc:\n",
    "        #     best_test_acc = test_acc\n",
    "        \n",
    "        # Save model with best train loss\n",
    "        if train_loss < best_train_lss:\n",
    "            best_train_lss = train_loss\n",
    "            best_loss_epoch = e\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "\n",
    "        if e % 20 == 0 or e == epochs:\n",
    "            print('[Epoch: {:04d}]'.format(e),\n",
    "            'train loss: {:.4f},'.format(train_loss.item()),\n",
    "            'train acc: {:.4f},'.format(train_acc.item()),\n",
    "            'val loss: {:.4f},'.format(val_loss.item()),\n",
    "            'val acc: {:.4f} '.format(val_acc.item()),\n",
    "            '(best train acc: {:.4f},'.format(best_train_acc.item()),\n",
    "            'best val acc: {:.4f},'.format(best_val_acc.item()),\n",
    "            'best train loss: {:.4f} '.format(best_train_lss),\n",
    "            '@ epoch', best_loss_epoch ,')')\n",
    "    \n",
    "    # Plot training accuracy curve\n",
    "    plt.figure(figsize = (12,7))\n",
    "    plt.plot(train_acc_curve)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Train Accuracy')\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize = (12,7))\n",
    "    plt.plot(train_lss_curve)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Train Loss')\n",
    "    plt.show()\n",
    "\n",
    "    # Load best model\n",
    "    loaded_model = None\n",
    "    if num_layers == 2:\n",
    "        loaded_model = GNN2L_Sage(data).to(device)\n",
    "    elif num_layers == 4:\n",
    "        loaded_model = GNN4L_Sage(data).to(device)\n",
    "    elif num_layers == 7:\n",
    "        loaded_model = GNN7L_Sage(data).to(device)\n",
    "    else:\n",
    "        print('Wrong number of layer. No model available with', num_layers, 'layers')\n",
    "        return -1\n",
    "\n",
    "    loaded_model.load_state_dict(torch.load(model_path))\n",
    "    loaded_model.eval()\n",
    "    logits = loaded_model(data)\n",
    "    output = logits.argmax(1)\n",
    "\n",
    "    print(classification_report(labels[test_mask].to('cpu'), output[test_mask].to('cpu')))\n",
    "\n",
    "    class_report = classification_report(labels[test_mask].to('cpu'), output[test_mask].to('cpu'), output_dict=True)\n",
    "    classification_report_dataframe = pd.DataFrame(class_report)\n",
    "    classification_report_dataframe.to_csv(report_path)\n",
    "\n",
    "    #Confusion Matrix\n",
    "    norms = [None, \"true\"]\n",
    "    for norm in norms:\n",
    "        cm = confusion_matrix(labels[test_mask].to('cpu'), output[test_mask].to('cpu'), normalize=norm)\n",
    "\n",
    "        plt.figure(figsize=(7,7))\n",
    "        \n",
    "        if norm == \"true\":\n",
    "            sn.heatmap(cm, annot=True, fmt=\".3f\", linewidths=.5, square = True, cmap = 'BuPu', xticklabels = classes, yticklabels = classes)\n",
    "        else:\n",
    "            sn.heatmap(cm, annot=True, fmt=\".0f\", linewidths=.5, square = True, cmap = 'BuPu', xticklabels = classes, yticklabels = classes)\n",
    "        plt.title(cm_title)\n",
    "        plt.ylabel('Actual label')\n",
    "        plt.xlabel('Predicted label')\n",
    "\n",
    "        if norm == None:\n",
    "            plt.savefig(image_path + '_notNorm.png')\n",
    "        else:\n",
    "            plt.savefig(image_path + '_Norm.png')\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NeDBIT Multiclass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18736, 2)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "G = nx.read_gml('Graphs/graph_with_normalized_nedbit.gml')\n",
    "\n",
    "seed_genes          = pd.read_csv('Datasets/C0006142_Malignant_neoplasm_of_breast_seed_genes.txt', header=None, sep=' ')\n",
    "seed_genes.columns  = [\"name\", \"GDA Score\"]\n",
    "seeds_list          = seed_genes[\"name\"].values.tolist()\n",
    "\n",
    "nedbit_scores = pd.read_csv('Datasets/C0006142_Malignant_neoplasm_of_breast_features_Score.csv')\n",
    "\n",
    "# Remove seed genes\n",
    "nedbit_scores_not_seed = nedbit_scores[~nedbit_scores['name'].isin(seeds_list)]\n",
    "print(nedbit_scores_not_seed.shape)\n",
    "\n",
    "# Sort scores for quartile division\n",
    "nedbit_scores_not_seed = nedbit_scores_not_seed.sort_values(by = \"out\", ascending = False)\n",
    "pseudo_labels = pd.qcut(x = nedbit_scores_not_seed[\"out\"], q = 4, labels = [\"RN\", \"LN\", \"WN\", \"LP\"])\n",
    "\n",
    "plt.hist(pseudo_labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19761"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nedbit_scores_not_seed['label'] = pseudo_labels\n",
    "\n",
    "nedbit_scores_seed = nedbit_scores[nedbit_scores['name'].isin(seeds_list)]\n",
    "nedbit_scores_seed = nedbit_scores_seed.assign(label = 'P')\n",
    "\n",
    "# Convert dataframe to dict for searching nodes and their labels\n",
    "not_seed_labels = dict(zip(nedbit_scores_not_seed['name'], nedbit_scores_not_seed['label']))\n",
    "seed_labels     = dict(zip(nedbit_scores_seed['name'], nedbit_scores_seed['label']))\n",
    "\n",
    "labels_dict = {'P':0, 'LP': 1, 'WN': 2, 'LN': 3, 'RN': 4}\n",
    "labels = []\n",
    "\n",
    "for node in G:\n",
    "    if node in not_seed_labels:\n",
    "        labels.append(labels_dict[not_seed_labels[node]])\n",
    "    else:\n",
    "        labels.append(labels_dict[seed_labels[node]])\n",
    "\n",
    "labels = np.asarray(labels)\n",
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "attributes = ['degree', 'ring', 'NetRank', 'NetShort', 'HeatDiff', 'InfoDiff']\n",
    "\n",
    "dataset_with_nedbit = MyDataset(G, labels, attributes, num_classes=5)\n",
    "data_with_nedbit = dataset_with_nedbit[0]\n",
    "\n",
    "plt.hist(labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ae1067506584a1d8db9a803d65dcc96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40001 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch: 0000] train loss: 1416.6721, train acc: 0.2316, val loss: 1313.3909, val acc: 0.2371  (best train acc: 0.2316, best val acc: 0.2371, best train loss: 999.0000  @ epoch 0 )\n",
      "[Epoch: 0020] train loss: 935.5636, train acc: 0.2167, val loss: 577.3289, val acc: 0.2371  (best train acc: 0.2323, best val acc: 0.2371, best train loss: 935.5636  @ epoch 20 )\n",
      "[Epoch: 0040] train loss: 722.0756, train acc: 0.2214, val loss: 302.9381, val acc: 0.2388  (best train acc: 0.2323, best val acc: 0.2391, best train loss: 698.4193  @ epoch 39 )\n",
      "[Epoch: 0060] train loss: 532.3002, train acc: 0.2590, val loss: 235.6854, val acc: 0.2384  (best train acc: 0.2657, best val acc: 0.2391, best train loss: 528.3857  @ epoch 58 )\n",
      "[Epoch: 0080] train loss: 404.3631, train acc: 0.2972, val loss: 140.5530, val acc: 0.2465  (best train acc: 0.3055, best val acc: 0.2469, best train loss: 404.3631  @ epoch 80 )\n",
      "[Epoch: 0100] train loss: 319.5390, train acc: 0.3247, val loss: 93.1488, val acc: 0.3926  (best train acc: 0.3487, best val acc: 0.3926, best train loss: 291.8306  @ epoch 99 )\n",
      "[Epoch: 0120] train loss: 239.6490, train acc: 0.3596, val loss: 71.8004, val acc: 0.5272  (best train acc: 0.3601, best val acc: 0.5272, best train loss: 238.9482  @ epoch 118 )\n",
      "[Epoch: 0140] train loss: 188.8058, train acc: 0.3810, val loss: 58.6777, val acc: 0.5761  (best train acc: 0.3904, best val acc: 0.5983, best train loss: 179.2319  @ epoch 138 )\n",
      "[Epoch: 0160] train loss: 124.0255, train acc: 0.4252, val loss: 44.1857, val acc: 0.6148  (best train acc: 0.4252, best val acc: 0.6155, best train loss: 124.0255  @ epoch 160 )\n",
      "[Epoch: 0180] train loss: 98.2134, train acc: 0.4438, val loss: 33.1585, val acc: 0.6088  (best train acc: 0.4556, best val acc: 0.6199, best train loss: 93.7564  @ epoch 179 )\n",
      "[Epoch: 0200] train loss: 71.1042, train acc: 0.4667, val loss: 22.0898, val acc: 0.6341  (best train acc: 0.4719, best val acc: 0.6384, best train loss: 71.1042  @ epoch 200 )\n",
      "[Epoch: 0220] train loss: 57.8914, train acc: 0.5014, val loss: 15.7889, val acc: 0.6867  (best train acc: 0.5024, best val acc: 0.6907, best train loss: 56.6457  @ epoch 216 )\n",
      "[Epoch: 0240] train loss: 48.8302, train acc: 0.5021, val loss: 12.0940, val acc: 0.6452  (best train acc: 0.5244, best val acc: 0.7035, best train loss: 45.3095  @ epoch 237 )\n",
      "[Epoch: 0260] train loss: 39.5286, train acc: 0.4892, val loss: 9.9154, val acc: 0.6223  (best train acc: 0.5244, best val acc: 0.7035, best train loss: 37.5269  @ epoch 255 )\n",
      "[Epoch: 0280] train loss: 31.8336, train acc: 0.5090, val loss: 8.1265, val acc: 0.6273  (best train acc: 0.5244, best val acc: 0.7035, best train loss: 31.8336  @ epoch 280 )\n",
      "[Epoch: 0300] train loss: 29.1205, train acc: 0.4967, val loss: 6.9524, val acc: 0.6283  (best train acc: 0.5244, best val acc: 0.7035, best train loss: 26.0734  @ epoch 294 )\n",
      "[Epoch: 0320] train loss: 23.9133, train acc: 0.4964, val loss: 5.7758, val acc: 0.6712  (best train acc: 0.5244, best val acc: 0.7093, best train loss: 23.5833  @ epoch 312 )\n",
      "[Epoch: 0340] train loss: 22.3313, train acc: 0.4948, val loss: 5.0158, val acc: 0.6708  (best train acc: 0.5301, best val acc: 0.7201, best train loss: 20.6926  @ epoch 333 )\n",
      "[Epoch: 0360] train loss: 21.0621, train acc: 0.5131, val loss: 4.6809, val acc: 0.6806  (best train acc: 0.5301, best val acc: 0.7201, best train loss: 19.7168  @ epoch 357 )\n",
      "[Epoch: 0380] train loss: 19.7675, train acc: 0.5094, val loss: 4.2656, val acc: 0.6219  (best train acc: 0.5301, best val acc: 0.7201, best train loss: 16.3157  @ epoch 378 )\n",
      "[Epoch: 0400] train loss: 17.9402, train acc: 0.5184, val loss: 3.7840, val acc: 0.6479  (best train acc: 0.5301, best val acc: 0.7201, best train loss: 15.6663  @ epoch 386 )\n",
      "[Epoch: 0420] train loss: 16.3031, train acc: 0.5134, val loss: 3.6341, val acc: 0.6894  (best train acc: 0.5301, best val acc: 0.7201, best train loss: 13.6597  @ epoch 413 )\n",
      "[Epoch: 0440] train loss: 13.4462, train acc: 0.5212, val loss: 3.0795, val acc: 0.7056  (best train acc: 0.5332, best val acc: 0.7204, best train loss: 12.8399  @ epoch 433 )\n",
      "[Epoch: 0460] train loss: 13.3116, train acc: 0.5160, val loss: 2.8124, val acc: 0.6465  (best train acc: 0.5364, best val acc: 0.7204, best train loss: 10.6796  @ epoch 456 )\n",
      "[Epoch: 0480] train loss: 12.3622, train acc: 0.5263, val loss: 2.7329, val acc: 0.6722  (best train acc: 0.5390, best val acc: 0.7204, best train loss: 10.6796  @ epoch 456 )\n",
      "[Epoch: 0500] train loss: 10.5253, train acc: 0.5223, val loss: 2.3964, val acc: 0.6695  (best train acc: 0.5390, best val acc: 0.7204, best train loss: 9.7637  @ epoch 496 )\n",
      "[Epoch: 0520] train loss: 10.6233, train acc: 0.5223, val loss: 2.1625, val acc: 0.6968  (best train acc: 0.5390, best val acc: 0.7204, best train loss: 8.5987  @ epoch 517 )\n",
      "[Epoch: 0540] train loss: 9.3796, train acc: 0.5111, val loss: 1.9025, val acc: 0.6938  (best train acc: 0.5390, best val acc: 0.7204, best train loss: 8.1110  @ epoch 539 )\n",
      "[Epoch: 0560] train loss: 9.5924, train acc: 0.5103, val loss: 1.8928, val acc: 0.6341  (best train acc: 0.5480, best val acc: 0.7204, best train loss: 8.0981  @ epoch 551 )\n",
      "[Epoch: 0580] train loss: 8.1457, train acc: 0.5083, val loss: 1.7489, val acc: 0.6415  (best train acc: 0.5480, best val acc: 0.7204, best train loss: 7.3567  @ epoch 566 )\n",
      "[Epoch: 0600] train loss: 7.5652, train acc: 0.5333, val loss: 1.6850, val acc: 0.6449  (best train acc: 0.5480, best val acc: 0.7204, best train loss: 6.3109  @ epoch 598 )\n",
      "[Epoch: 0620] train loss: 6.2735, train acc: 0.5213, val loss: 1.6465, val acc: 0.6175  (best train acc: 0.5480, best val acc: 0.7204, best train loss: 6.0184  @ epoch 619 )\n",
      "[Epoch: 0640] train loss: 6.1315, train acc: 0.5367, val loss: 1.6122, val acc: 0.6260  (best train acc: 0.5480, best val acc: 0.7204, best train loss: 5.9272  @ epoch 628 )\n",
      "[Epoch: 0660] train loss: 6.1082, train acc: 0.5254, val loss: 1.6116, val acc: 0.6057  (best train acc: 0.5480, best val acc: 0.7204, best train loss: 5.5281  @ epoch 642 )\n",
      "[Epoch: 0680] train loss: 5.0377, train acc: 0.5252, val loss: 1.5617, val acc: 0.6597  (best train acc: 0.5480, best val acc: 0.7204, best train loss: 5.0377  @ epoch 680 )\n",
      "[Epoch: 0700] train loss: 5.5032, train acc: 0.5271, val loss: 1.5664, val acc: 0.5987  (best train acc: 0.5480, best val acc: 0.7204, best train loss: 4.8209  @ epoch 695 )\n",
      "[Epoch: 0720] train loss: 5.1972, train acc: 0.5414, val loss: 1.5221, val acc: 0.6668  (best train acc: 0.5480, best val acc: 0.7204, best train loss: 4.5408  @ epoch 712 )\n",
      "[Epoch: 0740] train loss: 4.7237, train acc: 0.5278, val loss: 1.5016, val acc: 0.6715  (best train acc: 0.5480, best val acc: 0.7204, best train loss: 4.3889  @ epoch 738 )\n",
      "[Epoch: 0760] train loss: 4.6649, train acc: 0.5119, val loss: 1.4814, val acc: 0.6482  (best train acc: 0.5480, best val acc: 0.7204, best train loss: 4.3010  @ epoch 758 )\n",
      "[Epoch: 0780] train loss: 4.4688, train acc: 0.5257, val loss: 1.4732, val acc: 0.6762  (best train acc: 0.5480, best val acc: 0.7204, best train loss: 3.7690  @ epoch 777 )\n",
      "[Epoch: 0800] train loss: 3.7407, train acc: 0.5304, val loss: 1.4635, val acc: 0.6550  (best train acc: 0.5580, best val acc: 0.7204, best train loss: 3.7385  @ epoch 782 )\n",
      "[Epoch: 0820] train loss: 3.5627, train acc: 0.5445, val loss: 1.4316, val acc: 0.6121  (best train acc: 0.5580, best val acc: 0.7204, best train loss: 3.5627  @ epoch 820 )\n",
      "[Epoch: 0840] train loss: 3.5201, train acc: 0.5265, val loss: 1.4087, val acc: 0.6172  (best train acc: 0.5580, best val acc: 0.7204, best train loss: 3.4964  @ epoch 830 )\n",
      "[Epoch: 0860] train loss: 3.4040, train acc: 0.5207, val loss: 1.3992, val acc: 0.5980  (best train acc: 0.5580, best val acc: 0.7204, best train loss: 3.2592  @ epoch 841 )\n",
      "[Epoch: 0880] train loss: 3.4593, train acc: 0.5330, val loss: 1.3705, val acc: 0.6081  (best train acc: 0.5580, best val acc: 0.7204, best train loss: 3.1333  @ epoch 876 )\n",
      "[Epoch: 0900] train loss: 2.9731, train acc: 0.5502, val loss: 1.3331, val acc: 0.6516  (best train acc: 0.5580, best val acc: 0.7204, best train loss: 2.9731  @ epoch 900 )\n",
      "[Epoch: 0920] train loss: 3.0723, train acc: 0.5458, val loss: 1.3135, val acc: 0.6253  (best train acc: 0.5580, best val acc: 0.7204, best train loss: 2.7999  @ epoch 919 )\n",
      "[Epoch: 0940] train loss: 2.8829, train acc: 0.5480, val loss: 1.2835, val acc: 0.6010  (best train acc: 0.5619, best val acc: 0.7204, best train loss: 2.6386  @ epoch 931 )\n",
      "[Epoch: 0960] train loss: 2.7336, train acc: 0.5542, val loss: 1.2396, val acc: 0.6344  (best train acc: 0.5619, best val acc: 0.7204, best train loss: 2.5361  @ epoch 952 )\n",
      "[Epoch: 0980] train loss: 2.6106, train acc: 0.5562, val loss: 1.2073, val acc: 0.6341  (best train acc: 0.5704, best val acc: 0.7204, best train loss: 2.4535  @ epoch 976 )\n",
      "[Epoch: 1000] train loss: 2.3425, train acc: 0.5511, val loss: 1.1803, val acc: 0.6563  (best train acc: 0.5704, best val acc: 0.7204, best train loss: 2.3425  @ epoch 1000 )\n",
      "[Epoch: 1020] train loss: 2.4114, train acc: 0.5539, val loss: 1.1620, val acc: 0.6074  (best train acc: 0.5704, best val acc: 0.7204, best train loss: 2.3219  @ epoch 1018 )\n",
      "[Epoch: 1040] train loss: 2.1841, train acc: 0.5529, val loss: 1.1296, val acc: 0.6354  (best train acc: 0.5704, best val acc: 0.7204, best train loss: 2.1841  @ epoch 1040 )\n",
      "[Epoch: 1060] train loss: 2.3082, train acc: 0.5575, val loss: 1.1326, val acc: 0.5939  (best train acc: 0.5704, best val acc: 0.7204, best train loss: 2.1047  @ epoch 1057 )\n",
      "[Epoch: 1080] train loss: 2.2312, train acc: 0.5495, val loss: 1.0751, val acc: 0.7046  (best train acc: 0.5704, best val acc: 0.7204, best train loss: 2.0169  @ epoch 1079 )\n",
      "[Epoch: 1100] train loss: 2.0811, train acc: 0.5521, val loss: 1.0617, val acc: 0.6334  (best train acc: 0.5704, best val acc: 0.7204, best train loss: 1.9707  @ epoch 1098 )\n",
      "[Epoch: 1120] train loss: 1.9655, train acc: 0.5558, val loss: 1.0286, val acc: 0.6536  (best train acc: 0.5704, best val acc: 0.7204, best train loss: 1.8991  @ epoch 1109 )\n",
      "[Epoch: 1140] train loss: 1.8748, train acc: 0.5747, val loss: 1.0192, val acc: 0.6196  (best train acc: 0.5747, best val acc: 0.7204, best train loss: 1.7909  @ epoch 1135 )\n",
      "[Epoch: 1160] train loss: 1.7690, train acc: 0.5654, val loss: 0.9823, val acc: 0.6455  (best train acc: 0.5747, best val acc: 0.7204, best train loss: 1.7126  @ epoch 1154 )\n",
      "[Epoch: 1180] train loss: 1.7388, train acc: 0.5686, val loss: 0.9412, val acc: 0.6594  (best train acc: 0.5747, best val acc: 0.7204, best train loss: 1.6154  @ epoch 1176 )\n",
      "[Epoch: 1200] train loss: 1.5723, train acc: 0.5617, val loss: 0.9406, val acc: 0.6806  (best train acc: 0.5747, best val acc: 0.7204, best train loss: 1.5526  @ epoch 1193 )\n",
      "[Epoch: 1220] train loss: 1.5533, train acc: 0.5401, val loss: 0.9257, val acc: 0.6503  (best train acc: 0.5747, best val acc: 0.7204, best train loss: 1.4579  @ epoch 1207 )\n",
      "[Epoch: 1240] train loss: 1.4741, train acc: 0.5527, val loss: 0.8955, val acc: 0.7120  (best train acc: 0.5752, best val acc: 0.7204, best train loss: 1.4229  @ epoch 1236 )\n",
      "[Epoch: 1260] train loss: 1.3921, train acc: 0.5508, val loss: 0.8807, val acc: 0.6735  (best train acc: 0.5752, best val acc: 0.7248, best train loss: 1.3361  @ epoch 1252 )\n",
      "[Epoch: 1280] train loss: 1.3697, train acc: 0.5771, val loss: 0.8643, val acc: 0.6728  (best train acc: 0.5795, best val acc: 0.7312, best train loss: 1.2935  @ epoch 1272 )\n",
      "[Epoch: 1300] train loss: 1.2337, train acc: 0.5644, val loss: 0.8389, val acc: 0.6911  (best train acc: 0.5823, best val acc: 0.7312, best train loss: 1.2069  @ epoch 1294 )\n",
      "[Epoch: 1320] train loss: 1.2623, train acc: 0.5712, val loss: 0.8239, val acc: 0.7228  (best train acc: 0.5861, best val acc: 0.7467, best train loss: 1.2026  @ epoch 1314 )\n",
      "[Epoch: 1340] train loss: 1.2114, train acc: 0.5784, val loss: 0.8100, val acc: 0.6853  (best train acc: 0.5873, best val acc: 0.7467, best train loss: 1.1460  @ epoch 1339 )\n",
      "[Epoch: 1360] train loss: 1.1569, train acc: 0.5899, val loss: 0.7915, val acc: 0.7221  (best train acc: 0.5914, best val acc: 0.7501, best train loss: 1.1190  @ epoch 1353 )\n",
      "[Epoch: 1380] train loss: 1.1122, train acc: 0.5837, val loss: 0.7819, val acc: 0.6880  (best train acc: 0.5914, best val acc: 0.7501, best train loss: 1.0928  @ epoch 1362 )\n",
      "[Epoch: 1400] train loss: 1.0893, train acc: 0.5788, val loss: 0.7799, val acc: 0.7305  (best train acc: 0.6001, best val acc: 0.7636, best train loss: 1.0366  @ epoch 1391 )\n",
      "[Epoch: 1420] train loss: 1.0279, train acc: 0.5617, val loss: 0.7715, val acc: 0.7160  (best train acc: 0.6021, best val acc: 0.7636, best train loss: 1.0245  @ epoch 1418 )\n",
      "[Epoch: 1440] train loss: 1.0749, train acc: 0.5907, val loss: 0.7849, val acc: 0.6961  (best train acc: 0.6090, best val acc: 0.7636, best train loss: 1.0087  @ epoch 1438 )\n",
      "[Epoch: 1460] train loss: 0.9975, train acc: 0.6149, val loss: 0.7742, val acc: 0.7383  (best train acc: 0.6149, best val acc: 0.7636, best train loss: 0.9692  @ epoch 1451 )\n",
      "[Epoch: 1480] train loss: 0.9923, train acc: 0.6026, val loss: 0.7728, val acc: 0.7460  (best train acc: 0.6156, best val acc: 0.7636, best train loss: 0.9555  @ epoch 1467 )\n",
      "[Epoch: 1500] train loss: 1.0072, train acc: 0.6119, val loss: 0.7953, val acc: 0.6739  (best train acc: 0.6162, best val acc: 0.7723, best train loss: 0.9555  @ epoch 1467 )\n",
      "[Epoch: 1520] train loss: 1.0037, train acc: 0.5870, val loss: 0.7638, val acc: 0.7140  (best train acc: 0.6162, best val acc: 0.7723, best train loss: 0.9373  @ epoch 1514 )\n",
      "[Epoch: 1540] train loss: 0.9679, train acc: 0.6070, val loss: 0.7641, val acc: 0.7629  (best train acc: 0.6196, best val acc: 0.7727, best train loss: 0.9320  @ epoch 1528 )\n",
      "[Epoch: 1560] train loss: 1.0227, train acc: 0.5858, val loss: 0.7652, val acc: 0.7686  (best train acc: 0.6196, best val acc: 0.7727, best train loss: 0.9320  @ epoch 1528 )\n",
      "[Epoch: 1580] train loss: 0.9619, train acc: 0.5852, val loss: 0.7683, val acc: 0.7180  (best train acc: 0.6230, best val acc: 0.7747, best train loss: 0.9320  @ epoch 1528 )\n",
      "[Epoch: 1600] train loss: 0.9494, train acc: 0.6259, val loss: 0.7618, val acc: 0.7764  (best train acc: 0.6316, best val acc: 0.7771, best train loss: 0.9171  @ epoch 1587 )\n",
      "[Epoch: 1620] train loss: 0.9561, train acc: 0.6200, val loss: 0.7538, val acc: 0.7791  (best train acc: 0.6358, best val acc: 0.7794, best train loss: 0.9021  @ epoch 1616 )\n",
      "[Epoch: 1640] train loss: 0.9512, train acc: 0.5568, val loss: 0.7534, val acc: 0.7339  (best train acc: 0.6358, best val acc: 0.7798, best train loss: 0.9021  @ epoch 1616 )\n",
      "[Epoch: 1660] train loss: 0.9396, train acc: 0.6047, val loss: 0.7518, val acc: 0.7481  (best train acc: 0.6358, best val acc: 0.7845, best train loss: 0.9021  @ epoch 1616 )\n",
      "[Epoch: 1680] train loss: 0.9768, train acc: 0.6317, val loss: 0.7580, val acc: 0.7676  (best train acc: 0.6358, best val acc: 0.7845, best train loss: 0.9021  @ epoch 1616 )\n",
      "[Epoch: 1700] train loss: 0.9440, train acc: 0.5967, val loss: 0.7634, val acc: 0.7352  (best train acc: 0.6358, best val acc: 0.7845, best train loss: 0.9021  @ epoch 1616 )\n",
      "[Epoch: 1720] train loss: 0.9593, train acc: 0.5905, val loss: 0.7390, val acc: 0.7454  (best train acc: 0.6358, best val acc: 0.7845, best train loss: 0.9021  @ epoch 1616 )\n",
      "[Epoch: 1740] train loss: 0.9283, train acc: 0.6178, val loss: 0.7409, val acc: 0.7727  (best train acc: 0.6358, best val acc: 0.7845, best train loss: 0.9021  @ epoch 1616 )\n",
      "[Epoch: 1760] train loss: 1.0229, train acc: 0.5688, val loss: 0.7418, val acc: 0.7821  (best train acc: 0.6370, best val acc: 0.7845, best train loss: 0.9021  @ epoch 1616 )\n",
      "[Epoch: 1780] train loss: 0.9532, train acc: 0.5841, val loss: 0.7522, val acc: 0.7042  (best train acc: 0.6378, best val acc: 0.7845, best train loss: 0.8994  @ epoch 1763 )\n",
      "[Epoch: 1800] train loss: 0.9497, train acc: 0.6189, val loss: 0.7365, val acc: 0.7757  (best train acc: 0.6437, best val acc: 0.7845, best train loss: 0.8994  @ epoch 1763 )\n",
      "[Epoch: 1820] train loss: 0.9555, train acc: 0.6219, val loss: 0.7325, val acc: 0.7649  (best train acc: 0.6437, best val acc: 0.7845, best train loss: 0.8994  @ epoch 1763 )\n",
      "[Epoch: 1840] train loss: 0.9585, train acc: 0.6134, val loss: 0.7303, val acc: 0.7680  (best train acc: 0.6437, best val acc: 0.7845, best train loss: 0.8994  @ epoch 1763 )\n",
      "[Epoch: 1860] train loss: 0.9645, train acc: 0.5751, val loss: 0.7412, val acc: 0.7147  (best train acc: 0.6437, best val acc: 0.7845, best train loss: 0.8994  @ epoch 1763 )\n",
      "[Epoch: 1880] train loss: 0.9261, train acc: 0.6175, val loss: 0.7628, val acc: 0.6489  (best train acc: 0.6437, best val acc: 0.7845, best train loss: 0.8916  @ epoch 1873 )\n",
      "[Epoch: 1900] train loss: 0.9996, train acc: 0.6204, val loss: 0.7648, val acc: 0.6634  (best train acc: 0.6437, best val acc: 0.7845, best train loss: 0.8916  @ epoch 1873 )\n",
      "[Epoch: 1920] train loss: 0.9701, train acc: 0.6166, val loss: 0.7379, val acc: 0.7447  (best train acc: 0.6437, best val acc: 0.7845, best train loss: 0.8916  @ epoch 1873 )\n",
      "[Epoch: 1940] train loss: 0.9283, train acc: 0.5895, val loss: 0.7541, val acc: 0.6988  (best train acc: 0.6437, best val acc: 0.7845, best train loss: 0.8916  @ epoch 1873 )\n",
      "[Epoch: 1960] train loss: 0.8907, train acc: 0.6358, val loss: 0.7485, val acc: 0.6813  (best train acc: 0.6437, best val acc: 0.7845, best train loss: 0.8907  @ epoch 1960 )\n",
      "[Epoch: 1980] train loss: 0.9492, train acc: 0.5848, val loss: 0.7231, val acc: 0.7828  (best train acc: 0.6468, best val acc: 0.7845, best train loss: 0.8907  @ epoch 1960 )\n",
      "[Epoch: 2000] train loss: 0.9795, train acc: 0.6302, val loss: 0.7924, val acc: 0.6044  (best train acc: 0.6468, best val acc: 0.7879, best train loss: 0.8907  @ epoch 1960 )\n",
      "[Epoch: 2020] train loss: 0.9548, train acc: 0.6178, val loss: 0.7351, val acc: 0.7096  (best train acc: 0.6468, best val acc: 0.7879, best train loss: 0.8907  @ epoch 1960 )\n",
      "[Epoch: 2040] train loss: 0.8958, train acc: 0.6440, val loss: 0.7350, val acc: 0.7302  (best train acc: 0.6468, best val acc: 0.7879, best train loss: 0.8907  @ epoch 1960 )\n",
      "[Epoch: 2060] train loss: 0.8978, train acc: 0.6275, val loss: 0.7263, val acc: 0.7669  (best train acc: 0.6468, best val acc: 0.7879, best train loss: 0.8886  @ epoch 2049 )\n",
      "[Epoch: 2080] train loss: 0.9753, train acc: 0.5917, val loss: 0.7202, val acc: 0.7319  (best train acc: 0.6468, best val acc: 0.7879, best train loss: 0.8829  @ epoch 2063 )\n",
      "[Epoch: 2100] train loss: 0.9517, train acc: 0.6110, val loss: 0.7142, val acc: 0.7626  (best train acc: 0.6468, best val acc: 0.7879, best train loss: 0.8829  @ epoch 2063 )\n",
      "[Epoch: 2120] train loss: 0.9408, train acc: 0.6333, val loss: 0.7877, val acc: 0.5841  (best train acc: 0.6468, best val acc: 0.7879, best train loss: 0.8829  @ epoch 2063 )\n",
      "[Epoch: 2140] train loss: 0.9855, train acc: 0.5995, val loss: 0.7200, val acc: 0.7234  (best train acc: 0.6468, best val acc: 0.7879, best train loss: 0.8829  @ epoch 2063 )\n",
      "[Epoch: 2160] train loss: 0.9355, train acc: 0.6125, val loss: 0.7242, val acc: 0.7258  (best train acc: 0.6468, best val acc: 0.7879, best train loss: 0.8717  @ epoch 2154 )\n",
      "[Epoch: 2180] train loss: 0.9425, train acc: 0.5962, val loss: 0.7234, val acc: 0.7191  (best train acc: 0.6468, best val acc: 0.7879, best train loss: 0.8717  @ epoch 2154 )\n",
      "[Epoch: 2200] train loss: 0.9169, train acc: 0.6332, val loss: 0.7123, val acc: 0.7642  (best train acc: 0.6468, best val acc: 0.7879, best train loss: 0.8717  @ epoch 2154 )\n",
      "[Epoch: 2220] train loss: 0.9627, train acc: 0.6317, val loss: 0.7162, val acc: 0.7737  (best train acc: 0.6515, best val acc: 0.7879, best train loss: 0.8717  @ epoch 2154 )\n",
      "[Epoch: 2240] train loss: 0.8922, train acc: 0.6459, val loss: 0.7393, val acc: 0.7423  (best train acc: 0.6515, best val acc: 0.7879, best train loss: 0.8717  @ epoch 2154 )\n",
      "[Epoch: 2260] train loss: 0.9235, train acc: 0.6188, val loss: 0.7438, val acc: 0.6678  (best train acc: 0.6515, best val acc: 0.7879, best train loss: 0.8717  @ epoch 2154 )\n",
      "[Epoch: 2280] train loss: 0.9259, train acc: 0.6214, val loss: 0.7203, val acc: 0.7710  (best train acc: 0.6515, best val acc: 0.7879, best train loss: 0.8717  @ epoch 2154 )\n",
      "[Epoch: 2300] train loss: 0.8993, train acc: 0.6431, val loss: 0.7168, val acc: 0.7771  (best train acc: 0.6515, best val acc: 0.7879, best train loss: 0.8717  @ epoch 2154 )\n",
      "[Epoch: 2320] train loss: 0.9498, train acc: 0.6219, val loss: 0.7403, val acc: 0.6809  (best train acc: 0.6515, best val acc: 0.7879, best train loss: 0.8717  @ epoch 2154 )\n",
      "[Epoch: 2340] train loss: 0.9408, train acc: 0.6208, val loss: 0.7554, val acc: 0.6948  (best train acc: 0.6515, best val acc: 0.7879, best train loss: 0.8717  @ epoch 2154 )\n",
      "[Epoch: 2360] train loss: 0.9231, train acc: 0.6019, val loss: 0.7188, val acc: 0.7541  (best train acc: 0.6515, best val acc: 0.7879, best train loss: 0.8717  @ epoch 2154 )\n",
      "[Epoch: 2380] train loss: 0.9614, train acc: 0.6303, val loss: 0.7107, val acc: 0.7808  (best train acc: 0.6515, best val acc: 0.7879, best train loss: 0.8717  @ epoch 2154 )\n",
      "[Epoch: 2400] train loss: 0.9333, train acc: 0.6400, val loss: 0.7200, val acc: 0.7720  (best train acc: 0.6515, best val acc: 0.7879, best train loss: 0.8717  @ epoch 2154 )\n",
      "[Epoch: 2420] train loss: 0.9044, train acc: 0.6384, val loss: 0.7079, val acc: 0.7642  (best train acc: 0.6515, best val acc: 0.7879, best train loss: 0.8717  @ epoch 2154 )\n",
      "[Epoch: 2440] train loss: 0.9234, train acc: 0.6010, val loss: 0.7534, val acc: 0.6938  (best train acc: 0.6515, best val acc: 0.7879, best train loss: 0.8717  @ epoch 2154 )\n",
      "[Epoch: 2460] train loss: 0.9560, train acc: 0.6415, val loss: 0.7016, val acc: 0.7612  (best train acc: 0.6515, best val acc: 0.7879, best train loss: 0.8717  @ epoch 2154 )\n",
      "[Epoch: 2480] train loss: 0.9041, train acc: 0.6540, val loss: 0.7001, val acc: 0.7477  (best train acc: 0.6540, best val acc: 0.7879, best train loss: 0.8717  @ epoch 2154 )\n",
      "[Epoch: 2500] train loss: 0.9212, train acc: 0.6434, val loss: 0.7155, val acc: 0.7582  (best train acc: 0.6540, best val acc: 0.7879, best train loss: 0.8717  @ epoch 2154 )\n",
      "[Epoch: 2520] train loss: 0.9160, train acc: 0.6109, val loss: 0.7241, val acc: 0.7049  (best train acc: 0.6540, best val acc: 0.7879, best train loss: 0.8664  @ epoch 2509 )\n",
      "[Epoch: 2540] train loss: 0.8612, train acc: 0.6505, val loss: 0.6961, val acc: 0.7663  (best train acc: 0.6540, best val acc: 0.7879, best train loss: 0.8612  @ epoch 2540 )\n",
      "[Epoch: 2560] train loss: 0.8976, train acc: 0.6452, val loss: 0.7375, val acc: 0.6779  (best train acc: 0.6540, best val acc: 0.7879, best train loss: 0.8612  @ epoch 2540 )\n",
      "[Epoch: 2580] train loss: 0.9527, train acc: 0.6022, val loss: 0.6986, val acc: 0.7470  (best train acc: 0.6540, best val acc: 0.7879, best train loss: 0.8612  @ epoch 2540 )\n",
      "[Epoch: 2600] train loss: 0.9072, train acc: 0.6217, val loss: 0.7027, val acc: 0.7551  (best train acc: 0.6540, best val acc: 0.7879, best train loss: 0.8612  @ epoch 2540 )\n",
      "[Epoch: 2620] train loss: 0.9019, train acc: 0.6270, val loss: 0.7038, val acc: 0.7160  (best train acc: 0.6540, best val acc: 0.7879, best train loss: 0.8612  @ epoch 2540 )\n",
      "[Epoch: 2640] train loss: 0.9061, train acc: 0.6467, val loss: 0.6945, val acc: 0.7440  (best train acc: 0.6540, best val acc: 0.7879, best train loss: 0.8612  @ epoch 2540 )\n",
      "[Epoch: 2660] train loss: 0.8800, train acc: 0.6382, val loss: 0.6941, val acc: 0.7366  (best train acc: 0.6596, best val acc: 0.7879, best train loss: 0.8612  @ epoch 2540 )\n",
      "[Epoch: 2680] train loss: 0.8726, train acc: 0.6403, val loss: 0.6973, val acc: 0.7656  (best train acc: 0.6596, best val acc: 0.7879, best train loss: 0.8612  @ epoch 2540 )\n",
      "[Epoch: 2700] train loss: 0.9259, train acc: 0.6376, val loss: 0.7104, val acc: 0.7676  (best train acc: 0.6596, best val acc: 0.7879, best train loss: 0.8612  @ epoch 2540 )\n",
      "[Epoch: 2720] train loss: 0.9513, train acc: 0.6139, val loss: 0.6931, val acc: 0.7680  (best train acc: 0.6596, best val acc: 0.7879, best train loss: 0.8612  @ epoch 2540 )\n",
      "[Epoch: 2740] train loss: 0.8874, train acc: 0.6133, val loss: 0.7023, val acc: 0.7083  (best train acc: 0.6596, best val acc: 0.7879, best train loss: 0.8482  @ epoch 2723 )\n",
      "[Epoch: 2760] train loss: 0.8805, train acc: 0.6501, val loss: 0.7029, val acc: 0.7487  (best train acc: 0.6596, best val acc: 0.7879, best train loss: 0.8482  @ epoch 2723 )\n",
      "[Epoch: 2780] train loss: 0.8823, train acc: 0.6408, val loss: 0.7189, val acc: 0.7049  (best train acc: 0.6596, best val acc: 0.7879, best train loss: 0.8482  @ epoch 2723 )\n",
      "[Epoch: 2800] train loss: 0.8816, train acc: 0.6450, val loss: 0.7188, val acc: 0.6951  (best train acc: 0.6596, best val acc: 0.7879, best train loss: 0.8482  @ epoch 2723 )\n",
      "[Epoch: 2820] train loss: 0.9151, train acc: 0.5850, val loss: 0.7100, val acc: 0.7288  (best train acc: 0.6596, best val acc: 0.7879, best train loss: 0.8482  @ epoch 2723 )\n",
      "[Epoch: 2840] train loss: 0.8841, train acc: 0.6195, val loss: 0.6965, val acc: 0.7150  (best train acc: 0.6596, best val acc: 0.7879, best train loss: 0.8482  @ epoch 2723 )\n",
      "[Epoch: 2860] train loss: 0.8934, train acc: 0.6261, val loss: 0.6831, val acc: 0.7555  (best train acc: 0.6596, best val acc: 0.7879, best train loss: 0.8482  @ epoch 2723 )\n",
      "[Epoch: 2880] train loss: 0.9420, train acc: 0.6207, val loss: 0.7080, val acc: 0.7450  (best train acc: 0.6596, best val acc: 0.7879, best train loss: 0.8482  @ epoch 2723 )\n",
      "[Epoch: 2900] train loss: 0.8574, train acc: 0.6491, val loss: 0.7068, val acc: 0.7423  (best train acc: 0.6596, best val acc: 0.7879, best train loss: 0.8482  @ epoch 2723 )\n",
      "[Epoch: 2920] train loss: 0.9049, train acc: 0.6399, val loss: 0.7110, val acc: 0.7342  (best train acc: 0.6596, best val acc: 0.7879, best train loss: 0.8482  @ epoch 2723 )\n",
      "[Epoch: 2940] train loss: 0.8802, train acc: 0.6325, val loss: 0.7156, val acc: 0.7133  (best train acc: 0.6596, best val acc: 0.7879, best train loss: 0.8482  @ epoch 2723 )\n",
      "[Epoch: 2960] train loss: 0.9034, train acc: 0.6464, val loss: 0.7140, val acc: 0.7322  (best train acc: 0.6596, best val acc: 0.7879, best train loss: 0.8482  @ epoch 2723 )\n",
      "[Epoch: 2980] train loss: 0.8957, train acc: 0.6516, val loss: 0.6982, val acc: 0.7676  (best train acc: 0.6596, best val acc: 0.7879, best train loss: 0.8482  @ epoch 2723 )\n",
      "[Epoch: 3000] train loss: 0.9080, train acc: 0.6348, val loss: 0.6852, val acc: 0.7342  (best train acc: 0.6596, best val acc: 0.7879, best train loss: 0.8482  @ epoch 2723 )\n",
      "[Epoch: 3020] train loss: 0.8887, train acc: 0.6305, val loss: 0.6915, val acc: 0.7710  (best train acc: 0.6596, best val acc: 0.7879, best train loss: 0.8482  @ epoch 2723 )\n",
      "[Epoch: 3040] train loss: 0.9236, train acc: 0.6220, val loss: 0.7464, val acc: 0.6121  (best train acc: 0.6596, best val acc: 0.7879, best train loss: 0.8482  @ epoch 2723 )\n",
      "[Epoch: 3060] train loss: 0.8759, train acc: 0.6510, val loss: 0.7165, val acc: 0.6887  (best train acc: 0.6596, best val acc: 0.7879, best train loss: 0.8482  @ epoch 2723 )\n",
      "[Epoch: 3080] train loss: 0.8970, train acc: 0.6311, val loss: 0.6784, val acc: 0.7565  (best train acc: 0.6596, best val acc: 0.7879, best train loss: 0.8482  @ epoch 2723 )\n",
      "[Epoch: 3100] train loss: 0.8957, train acc: 0.6251, val loss: 0.7434, val acc: 0.6422  (best train acc: 0.6596, best val acc: 0.7879, best train loss: 0.8482  @ epoch 2723 )\n",
      "[Epoch: 3120] train loss: 0.8725, train acc: 0.6557, val loss: 0.6928, val acc: 0.7558  (best train acc: 0.6596, best val acc: 0.7879, best train loss: 0.8482  @ epoch 2723 )\n",
      "[Epoch: 3140] train loss: 0.8808, train acc: 0.6322, val loss: 0.6880, val acc: 0.7707  (best train acc: 0.6596, best val acc: 0.7879, best train loss: 0.8482  @ epoch 2723 )\n",
      "[Epoch: 3160] train loss: 0.8816, train acc: 0.6282, val loss: 0.6853, val acc: 0.7339  (best train acc: 0.6596, best val acc: 0.7879, best train loss: 0.8482  @ epoch 2723 )\n",
      "[Epoch: 3180] train loss: 0.9265, train acc: 0.6126, val loss: 0.7051, val acc: 0.7359  (best train acc: 0.6596, best val acc: 0.7879, best train loss: 0.8482  @ epoch 2723 )\n",
      "[Epoch: 3200] train loss: 0.8940, train acc: 0.5993, val loss: 0.6886, val acc: 0.7535  (best train acc: 0.6596, best val acc: 0.7879, best train loss: 0.8482  @ epoch 2723 )\n",
      "[Epoch: 3220] train loss: 0.9013, train acc: 0.6201, val loss: 0.6877, val acc: 0.7433  (best train acc: 0.6596, best val acc: 0.7879, best train loss: 0.8482  @ epoch 2723 )\n",
      "[Epoch: 3240] train loss: 0.8905, train acc: 0.6518, val loss: 0.6783, val acc: 0.7696  (best train acc: 0.6596, best val acc: 0.7879, best train loss: 0.8482  @ epoch 2723 )\n",
      "[Epoch: 3260] train loss: 0.9058, train acc: 0.6254, val loss: 0.6869, val acc: 0.7720  (best train acc: 0.6596, best val acc: 0.7879, best train loss: 0.8482  @ epoch 2723 )\n",
      "[Epoch: 3280] train loss: 0.8871, train acc: 0.6427, val loss: 0.7026, val acc: 0.7379  (best train acc: 0.6596, best val acc: 0.7879, best train loss: 0.8482  @ epoch 2723 )\n",
      "[Epoch: 3300] train loss: 0.8513, train acc: 0.6544, val loss: 0.6999, val acc: 0.7126  (best train acc: 0.6596, best val acc: 0.7879, best train loss: 0.8482  @ epoch 2723 )\n",
      "[Epoch: 3320] train loss: 0.8858, train acc: 0.6399, val loss: 0.7078, val acc: 0.7008  (best train acc: 0.6596, best val acc: 0.7879, best train loss: 0.8482  @ epoch 2723 )\n",
      "[Epoch: 3340] train loss: 0.8922, train acc: 0.6328, val loss: 0.6741, val acc: 0.7312  (best train acc: 0.6596, best val acc: 0.7879, best train loss: 0.8482  @ epoch 2723 )\n",
      "[Epoch: 3360] train loss: 0.8892, train acc: 0.6048, val loss: 0.6860, val acc: 0.7096  (best train acc: 0.6596, best val acc: 0.7879, best train loss: 0.8482  @ epoch 2723 )\n",
      "[Epoch: 3380] train loss: 0.8844, train acc: 0.6523, val loss: 0.6943, val acc: 0.7093  (best train acc: 0.6596, best val acc: 0.7879, best train loss: 0.8482  @ epoch 2723 )\n",
      "[Epoch: 3400] train loss: 0.8687, train acc: 0.6346, val loss: 0.6907, val acc: 0.7632  (best train acc: 0.6596, best val acc: 0.7879, best train loss: 0.8482  @ epoch 2723 )\n",
      "[Epoch: 3420] train loss: 0.9023, train acc: 0.5928, val loss: 0.6820, val acc: 0.7686  (best train acc: 0.6596, best val acc: 0.7879, best train loss: 0.8482  @ epoch 2723 )\n",
      "[Epoch: 3440] train loss: 0.8896, train acc: 0.6518, val loss: 0.6783, val acc: 0.7487  (best train acc: 0.6596, best val acc: 0.7879, best train loss: 0.8466  @ epoch 3429 )\n",
      "[Epoch: 3460] train loss: 0.8731, train acc: 0.6455, val loss: 0.6721, val acc: 0.7740  (best train acc: 0.6596, best val acc: 0.7879, best train loss: 0.8466  @ epoch 3429 )\n",
      "[Epoch: 3480] train loss: 0.8636, train acc: 0.6491, val loss: 0.6644, val acc: 0.7538  (best train acc: 0.6601, best val acc: 0.7879, best train loss: 0.8429  @ epoch 3470 )\n",
      "[Epoch: 3500] train loss: 0.8744, train acc: 0.6512, val loss: 0.6660, val acc: 0.7491  (best train acc: 0.6601, best val acc: 0.7879, best train loss: 0.8404  @ epoch 3487 )\n",
      "[Epoch: 3520] train loss: 0.8898, train acc: 0.6343, val loss: 0.7067, val acc: 0.6877  (best train acc: 0.6601, best val acc: 0.7879, best train loss: 0.8404  @ epoch 3487 )\n",
      "[Epoch: 3540] train loss: 0.8770, train acc: 0.6083, val loss: 0.6721, val acc: 0.7801  (best train acc: 0.6601, best val acc: 0.7879, best train loss: 0.8404  @ epoch 3487 )\n",
      "[Epoch: 3560] train loss: 0.8686, train acc: 0.6429, val loss: 0.6706, val acc: 0.7201  (best train acc: 0.6601, best val acc: 0.7879, best train loss: 0.8404  @ epoch 3487 )\n",
      "[Epoch: 3580] train loss: 0.8844, train acc: 0.6315, val loss: 0.6877, val acc: 0.7727  (best train acc: 0.6601, best val acc: 0.7879, best train loss: 0.8404  @ epoch 3487 )\n",
      "[Epoch: 3600] train loss: 0.8694, train acc: 0.6372, val loss: 0.6752, val acc: 0.7767  (best train acc: 0.6601, best val acc: 0.7879, best train loss: 0.8404  @ epoch 3487 )\n",
      "[Epoch: 3620] train loss: 0.8405, train acc: 0.6505, val loss: 0.6676, val acc: 0.7771  (best train acc: 0.6601, best val acc: 0.7879, best train loss: 0.8404  @ epoch 3487 )\n",
      "[Epoch: 3640] train loss: 0.8726, train acc: 0.6390, val loss: 0.6650, val acc: 0.7656  (best train acc: 0.6619, best val acc: 0.7879, best train loss: 0.8317  @ epoch 3623 )\n",
      "[Epoch: 3660] train loss: 0.8528, train acc: 0.6531, val loss: 0.6783, val acc: 0.7585  (best train acc: 0.6619, best val acc: 0.7885, best train loss: 0.8317  @ epoch 3623 )\n",
      "[Epoch: 3680] train loss: 0.8569, train acc: 0.6472, val loss: 0.6814, val acc: 0.7504  (best train acc: 0.6632, best val acc: 0.7885, best train loss: 0.8317  @ epoch 3623 )\n",
      "[Epoch: 3700] train loss: 0.8762, train acc: 0.6372, val loss: 0.6855, val acc: 0.7406  (best train acc: 0.6632, best val acc: 0.7885, best train loss: 0.8317  @ epoch 3623 )\n",
      "[Epoch: 3720] train loss: 0.8563, train acc: 0.6535, val loss: 0.6676, val acc: 0.7737  (best train acc: 0.6632, best val acc: 0.7885, best train loss: 0.8317  @ epoch 3623 )\n",
      "[Epoch: 3740] train loss: 0.8685, train acc: 0.6417, val loss: 0.6721, val acc: 0.7285  (best train acc: 0.6632, best val acc: 0.7885, best train loss: 0.8317  @ epoch 3623 )\n",
      "[Epoch: 3760] train loss: 0.8629, train acc: 0.6399, val loss: 0.6757, val acc: 0.7781  (best train acc: 0.6632, best val acc: 0.7885, best train loss: 0.8317  @ epoch 3623 )\n",
      "[Epoch: 3780] train loss: 0.8684, train acc: 0.6446, val loss: 0.7090, val acc: 0.6954  (best train acc: 0.6632, best val acc: 0.7885, best train loss: 0.8317  @ epoch 3623 )\n",
      "[Epoch: 3800] train loss: 0.8555, train acc: 0.6497, val loss: 0.6862, val acc: 0.7329  (best train acc: 0.6632, best val acc: 0.7885, best train loss: 0.8288  @ epoch 3782 )\n",
      "[Epoch: 3820] train loss: 0.8625, train acc: 0.6353, val loss: 0.6825, val acc: 0.7272  (best train acc: 0.6632, best val acc: 0.7885, best train loss: 0.8288  @ epoch 3782 )\n",
      "[Epoch: 3840] train loss: 0.8599, train acc: 0.6498, val loss: 0.6767, val acc: 0.7707  (best train acc: 0.6632, best val acc: 0.7885, best train loss: 0.8288  @ epoch 3782 )\n",
      "[Epoch: 3860] train loss: 0.8389, train acc: 0.6593, val loss: 0.6930, val acc: 0.7400  (best train acc: 0.6632, best val acc: 0.7885, best train loss: 0.8288  @ epoch 3782 )\n",
      "[Epoch: 3880] train loss: 0.8336, train acc: 0.6523, val loss: 0.6645, val acc: 0.7774  (best train acc: 0.6632, best val acc: 0.7895, best train loss: 0.8288  @ epoch 3782 )\n",
      "[Epoch: 3900] train loss: 0.8554, train acc: 0.6484, val loss: 0.6766, val acc: 0.7683  (best train acc: 0.6632, best val acc: 0.7895, best train loss: 0.8238  @ epoch 3881 )\n",
      "[Epoch: 3920] train loss: 0.8599, train acc: 0.6434, val loss: 0.6929, val acc: 0.6836  (best train acc: 0.6635, best val acc: 0.7895, best train loss: 0.8238  @ epoch 3881 )\n",
      "[Epoch: 3940] train loss: 0.8516, train acc: 0.6593, val loss: 0.6575, val acc: 0.7666  (best train acc: 0.6635, best val acc: 0.7895, best train loss: 0.8238  @ epoch 3881 )\n",
      "[Epoch: 3960] train loss: 0.8689, train acc: 0.6261, val loss: 0.6612, val acc: 0.7298  (best train acc: 0.6635, best val acc: 0.7895, best train loss: 0.8238  @ epoch 3881 )\n",
      "[Epoch: 3980] train loss: 0.8363, train acc: 0.6572, val loss: 0.6659, val acc: 0.7828  (best train acc: 0.6635, best val acc: 0.7895, best train loss: 0.8238  @ epoch 3881 )\n",
      "[Epoch: 4000] train loss: 0.8648, train acc: 0.6356, val loss: 0.7111, val acc: 0.6553  (best train acc: 0.6635, best val acc: 0.7895, best train loss: 0.8238  @ epoch 3881 )\n",
      "[Epoch: 4020] train loss: 0.8754, train acc: 0.6365, val loss: 0.6682, val acc: 0.7258  (best train acc: 0.6635, best val acc: 0.7895, best train loss: 0.8238  @ epoch 3881 )\n",
      "[Epoch: 4040] train loss: 0.8539, train acc: 0.6491, val loss: 0.6611, val acc: 0.7521  (best train acc: 0.6635, best val acc: 0.7895, best train loss: 0.8238  @ epoch 3881 )\n",
      "[Epoch: 4060] train loss: 0.8542, train acc: 0.6485, val loss: 0.6621, val acc: 0.7676  (best train acc: 0.6635, best val acc: 0.7895, best train loss: 0.8238  @ epoch 3881 )\n",
      "[Epoch: 4080] train loss: 0.8605, train acc: 0.6475, val loss: 0.6988, val acc: 0.7403  (best train acc: 0.6635, best val acc: 0.7895, best train loss: 0.8238  @ epoch 3881 )\n",
      "[Epoch: 4100] train loss: 0.8343, train acc: 0.6517, val loss: 0.6658, val acc: 0.7592  (best train acc: 0.6635, best val acc: 0.7895, best train loss: 0.8238  @ epoch 3881 )\n",
      "[Epoch: 4120] train loss: 0.8470, train acc: 0.6449, val loss: 0.6794, val acc: 0.7565  (best train acc: 0.6635, best val acc: 0.7895, best train loss: 0.8238  @ epoch 3881 )\n",
      "[Epoch: 4140] train loss: 0.8447, train acc: 0.6560, val loss: 0.7034, val acc: 0.6816  (best train acc: 0.6635, best val acc: 0.7895, best train loss: 0.8238  @ epoch 3881 )\n",
      "[Epoch: 4160] train loss: 0.8637, train acc: 0.6390, val loss: 0.6964, val acc: 0.6789  (best train acc: 0.6635, best val acc: 0.7902, best train loss: 0.8238  @ epoch 3881 )\n",
      "[Epoch: 4180] train loss: 0.8973, train acc: 0.6141, val loss: 0.7560, val acc: 0.5821  (best train acc: 0.6661, best val acc: 0.7902, best train loss: 0.8224  @ epoch 4170 )\n",
      "[Epoch: 4200] train loss: 0.8649, train acc: 0.6422, val loss: 0.6977, val acc: 0.6809  (best train acc: 0.6661, best val acc: 0.7902, best train loss: 0.8224  @ epoch 4170 )\n",
      "[Epoch: 4220] train loss: 0.8746, train acc: 0.6379, val loss: 0.6512, val acc: 0.7693  (best train acc: 0.6661, best val acc: 0.7902, best train loss: 0.8224  @ epoch 4170 )\n",
      "[Epoch: 4240] train loss: 0.8362, train acc: 0.6476, val loss: 0.6854, val acc: 0.7086  (best train acc: 0.6664, best val acc: 0.7902, best train loss: 0.8183  @ epoch 4221 )\n",
      "[Epoch: 4260] train loss: 0.8639, train acc: 0.6340, val loss: 0.6493, val acc: 0.7656  (best train acc: 0.6664, best val acc: 0.7922, best train loss: 0.8183  @ epoch 4221 )\n",
      "[Epoch: 4280] train loss: 0.8459, train acc: 0.6361, val loss: 0.6631, val acc: 0.7906  (best train acc: 0.6664, best val acc: 0.7922, best train loss: 0.8177  @ epoch 4279 )\n",
      "[Epoch: 4300] train loss: 0.8401, train acc: 0.6483, val loss: 0.7049, val acc: 0.6691  (best train acc: 0.6664, best val acc: 0.7922, best train loss: 0.8177  @ epoch 4279 )\n",
      "[Epoch: 4320] train loss: 0.8436, train acc: 0.6580, val loss: 0.6694, val acc: 0.7430  (best train acc: 0.6664, best val acc: 0.7956, best train loss: 0.8177  @ epoch 4279 )\n",
      "[Epoch: 4340] train loss: 0.8403, train acc: 0.6648, val loss: 0.6584, val acc: 0.7767  (best train acc: 0.6665, best val acc: 0.7956, best train loss: 0.8177  @ epoch 4279 )\n",
      "[Epoch: 4360] train loss: 0.8335, train acc: 0.6343, val loss: 0.6522, val acc: 0.7417  (best train acc: 0.6723, best val acc: 0.7956, best train loss: 0.8177  @ epoch 4279 )\n",
      "[Epoch: 4380] train loss: 0.8319, train acc: 0.6418, val loss: 0.6841, val acc: 0.7265  (best train acc: 0.6723, best val acc: 0.7956, best train loss: 0.8177  @ epoch 4279 )\n",
      "[Epoch: 4400] train loss: 0.8425, train acc: 0.6526, val loss: 0.6491, val acc: 0.7720  (best train acc: 0.6723, best val acc: 0.7956, best train loss: 0.8177  @ epoch 4279 )\n",
      "[Epoch: 4420] train loss: 0.8547, train acc: 0.6511, val loss: 0.6556, val acc: 0.7339  (best train acc: 0.6723, best val acc: 0.7956, best train loss: 0.8177  @ epoch 4279 )\n",
      "[Epoch: 4440] train loss: 0.8484, train acc: 0.6522, val loss: 0.6770, val acc: 0.7578  (best train acc: 0.6723, best val acc: 0.7956, best train loss: 0.8139  @ epoch 4423 )\n",
      "[Epoch: 4460] train loss: 0.8209, train acc: 0.6624, val loss: 0.6612, val acc: 0.7686  (best train acc: 0.6723, best val acc: 0.7956, best train loss: 0.8139  @ epoch 4423 )\n",
      "[Epoch: 4480] train loss: 0.8481, train acc: 0.6338, val loss: 0.6747, val acc: 0.7578  (best train acc: 0.6723, best val acc: 0.7956, best train loss: 0.8139  @ epoch 4423 )\n",
      "[Epoch: 4500] train loss: 0.8743, train acc: 0.5986, val loss: 0.6544, val acc: 0.7663  (best train acc: 0.6723, best val acc: 0.7956, best train loss: 0.8139  @ epoch 4423 )\n",
      "[Epoch: 4520] train loss: 0.8374, train acc: 0.6661, val loss: 0.6460, val acc: 0.7511  (best train acc: 0.6723, best val acc: 0.7956, best train loss: 0.8139  @ epoch 4423 )\n",
      "[Epoch: 4540] train loss: 0.8332, train acc: 0.6533, val loss: 0.6488, val acc: 0.7730  (best train acc: 0.6782, best val acc: 0.7956, best train loss: 0.8037  @ epoch 4521 )\n",
      "[Epoch: 4560] train loss: 0.8286, train acc: 0.6489, val loss: 0.6972, val acc: 0.6759  (best train acc: 0.6782, best val acc: 0.7956, best train loss: 0.8037  @ epoch 4521 )\n",
      "[Epoch: 4580] train loss: 0.8346, train acc: 0.6444, val loss: 0.6640, val acc: 0.7841  (best train acc: 0.6782, best val acc: 0.7956, best train loss: 0.8037  @ epoch 4521 )\n",
      "[Epoch: 4600] train loss: 0.8167, train acc: 0.6504, val loss: 0.6799, val acc: 0.7039  (best train acc: 0.6782, best val acc: 0.7956, best train loss: 0.8037  @ epoch 4521 )\n",
      "[Epoch: 4620] train loss: 0.8276, train acc: 0.6579, val loss: 0.6442, val acc: 0.7626  (best train acc: 0.6782, best val acc: 0.7956, best train loss: 0.8037  @ epoch 4521 )\n",
      "[Epoch: 4640] train loss: 0.8417, train acc: 0.6276, val loss: 0.6785, val acc: 0.7275  (best train acc: 0.6782, best val acc: 0.7956, best train loss: 0.8037  @ epoch 4521 )\n",
      "[Epoch: 4660] train loss: 0.8414, train acc: 0.6400, val loss: 0.6510, val acc: 0.7858  (best train acc: 0.6782, best val acc: 0.7956, best train loss: 0.7993  @ epoch 4658 )\n",
      "[Epoch: 4680] train loss: 0.8318, train acc: 0.6523, val loss: 0.6356, val acc: 0.7609  (best train acc: 0.6782, best val acc: 0.7956, best train loss: 0.7993  @ epoch 4658 )\n",
      "[Epoch: 4700] train loss: 0.8277, train acc: 0.6434, val loss: 0.6484, val acc: 0.7710  (best train acc: 0.6782, best val acc: 0.7956, best train loss: 0.7993  @ epoch 4658 )\n",
      "[Epoch: 4720] train loss: 0.8298, train acc: 0.6598, val loss: 0.6707, val acc: 0.7541  (best train acc: 0.6782, best val acc: 0.7956, best train loss: 0.7930  @ epoch 4711 )\n",
      "[Epoch: 4740] train loss: 0.8208, train acc: 0.6585, val loss: 0.6519, val acc: 0.7858  (best train acc: 0.6782, best val acc: 0.7956, best train loss: 0.7930  @ epoch 4711 )\n",
      "[Epoch: 4760] train loss: 0.8283, train acc: 0.6673, val loss: 0.6433, val acc: 0.7794  (best train acc: 0.6782, best val acc: 0.7956, best train loss: 0.7930  @ epoch 4711 )\n",
      "[Epoch: 4780] train loss: 0.8205, train acc: 0.6682, val loss: 0.6575, val acc: 0.7639  (best train acc: 0.6782, best val acc: 0.7960, best train loss: 0.7930  @ epoch 4711 )\n",
      "[Epoch: 4800] train loss: 0.8275, train acc: 0.6656, val loss: 0.6508, val acc: 0.7835  (best train acc: 0.6782, best val acc: 0.7960, best train loss: 0.7930  @ epoch 4711 )\n",
      "[Epoch: 4820] train loss: 0.8153, train acc: 0.6566, val loss: 0.6372, val acc: 0.7801  (best train acc: 0.6782, best val acc: 0.7976, best train loss: 0.7930  @ epoch 4711 )\n",
      "[Epoch: 4840] train loss: 0.8124, train acc: 0.6742, val loss: 0.6436, val acc: 0.7764  (best train acc: 0.6782, best val acc: 0.7976, best train loss: 0.7907  @ epoch 4829 )\n",
      "[Epoch: 4860] train loss: 0.8294, train acc: 0.6464, val loss: 0.6341, val acc: 0.7656  (best train acc: 0.6782, best val acc: 0.7976, best train loss: 0.7907  @ epoch 4829 )\n",
      "[Epoch: 4880] train loss: 0.8156, train acc: 0.6666, val loss: 0.6494, val acc: 0.7494  (best train acc: 0.6782, best val acc: 0.7976, best train loss: 0.7907  @ epoch 4829 )\n",
      "[Epoch: 4900] train loss: 0.8092, train acc: 0.6601, val loss: 0.6518, val acc: 0.7777  (best train acc: 0.6810, best val acc: 0.7976, best train loss: 0.7907  @ epoch 4829 )\n",
      "[Epoch: 4920] train loss: 0.8275, train acc: 0.6288, val loss: 0.6404, val acc: 0.7531  (best train acc: 0.6810, best val acc: 0.7976, best train loss: 0.7907  @ epoch 4829 )\n",
      "[Epoch: 4940] train loss: 0.8186, train acc: 0.6658, val loss: 0.6492, val acc: 0.7578  (best train acc: 0.6810, best val acc: 0.7976, best train loss: 0.7907  @ epoch 4829 )\n",
      "[Epoch: 4960] train loss: 0.8041, train acc: 0.6756, val loss: 0.6343, val acc: 0.7673  (best train acc: 0.6810, best val acc: 0.7976, best train loss: 0.7907  @ epoch 4829 )\n",
      "[Epoch: 4980] train loss: 0.8376, train acc: 0.6482, val loss: 0.6571, val acc: 0.7720  (best train acc: 0.6810, best val acc: 0.7976, best train loss: 0.7907  @ epoch 4829 )\n",
      "[Epoch: 5000] train loss: 0.8090, train acc: 0.6642, val loss: 0.6537, val acc: 0.7848  (best train acc: 0.6810, best val acc: 0.7976, best train loss: 0.7893  @ epoch 4992 )\n",
      "[Epoch: 5020] train loss: 0.7994, train acc: 0.6767, val loss: 0.6451, val acc: 0.7936  (best train acc: 0.6812, best val acc: 0.7976, best train loss: 0.7890  @ epoch 5013 )\n",
      "[Epoch: 5040] train loss: 0.8183, train acc: 0.6461, val loss: 0.6659, val acc: 0.7535  (best train acc: 0.6812, best val acc: 0.7976, best train loss: 0.7890  @ epoch 5013 )\n",
      "[Epoch: 5060] train loss: 0.8165, train acc: 0.6474, val loss: 0.6431, val acc: 0.7636  (best train acc: 0.6812, best val acc: 0.7976, best train loss: 0.7890  @ epoch 5013 )\n",
      "[Epoch: 5080] train loss: 0.7994, train acc: 0.6755, val loss: 0.6411, val acc: 0.7943  (best train acc: 0.6812, best val acc: 0.7983, best train loss: 0.7890  @ epoch 5013 )\n",
      "[Epoch: 5100] train loss: 0.8319, train acc: 0.6428, val loss: 0.6525, val acc: 0.7315  (best train acc: 0.6812, best val acc: 0.7983, best train loss: 0.7890  @ epoch 5013 )\n",
      "[Epoch: 5120] train loss: 0.8085, train acc: 0.6623, val loss: 0.6342, val acc: 0.7497  (best train acc: 0.6812, best val acc: 0.7983, best train loss: 0.7890  @ epoch 5013 )\n",
      "[Epoch: 5140] train loss: 0.8052, train acc: 0.6640, val loss: 0.6563, val acc: 0.7410  (best train acc: 0.6812, best val acc: 0.7983, best train loss: 0.7890  @ epoch 5013 )\n",
      "[Epoch: 5160] train loss: 0.8021, train acc: 0.6602, val loss: 0.6541, val acc: 0.7798  (best train acc: 0.6812, best val acc: 0.7983, best train loss: 0.7890  @ epoch 5013 )\n",
      "[Epoch: 5180] train loss: 0.8158, train acc: 0.6516, val loss: 0.6336, val acc: 0.7852  (best train acc: 0.6812, best val acc: 0.7983, best train loss: 0.7890  @ epoch 5013 )\n",
      "[Epoch: 5200] train loss: 0.8522, train acc: 0.6426, val loss: 0.6679, val acc: 0.7039  (best train acc: 0.6812, best val acc: 0.7983, best train loss: 0.7890  @ epoch 5013 )\n",
      "[Epoch: 5220] train loss: 0.8433, train acc: 0.6408, val loss: 0.6401, val acc: 0.7980  (best train acc: 0.6842, best val acc: 0.7983, best train loss: 0.7837  @ epoch 5209 )\n",
      "[Epoch: 5240] train loss: 0.8050, train acc: 0.6578, val loss: 0.6372, val acc: 0.7437  (best train acc: 0.6842, best val acc: 0.7983, best train loss: 0.7837  @ epoch 5209 )\n",
      "[Epoch: 5260] train loss: 0.8119, train acc: 0.6517, val loss: 0.6238, val acc: 0.7508  (best train acc: 0.6842, best val acc: 0.7987, best train loss: 0.7836  @ epoch 5243 )\n",
      "[Epoch: 5280] train loss: 0.8034, train acc: 0.6730, val loss: 0.6275, val acc: 0.7703  (best train acc: 0.6842, best val acc: 0.7987, best train loss: 0.7836  @ epoch 5243 )\n",
      "[Epoch: 5300] train loss: 0.8246, train acc: 0.6443, val loss: 0.6362, val acc: 0.7639  (best train acc: 0.6842, best val acc: 0.7987, best train loss: 0.7836  @ epoch 5243 )\n",
      "[Epoch: 5320] train loss: 0.8432, train acc: 0.6243, val loss: 0.6342, val acc: 0.7363  (best train acc: 0.6842, best val acc: 0.8010, best train loss: 0.7836  @ epoch 5243 )\n",
      "[Epoch: 5340] train loss: 0.8140, train acc: 0.6640, val loss: 0.6433, val acc: 0.7852  (best train acc: 0.6842, best val acc: 0.8010, best train loss: 0.7836  @ epoch 5243 )\n",
      "[Epoch: 5360] train loss: 0.7741, train acc: 0.6776, val loss: 0.6233, val acc: 0.7744  (best train acc: 0.6842, best val acc: 0.8010, best train loss: 0.7741  @ epoch 5360 )\n",
      "[Epoch: 5380] train loss: 0.7872, train acc: 0.6643, val loss: 0.6397, val acc: 0.7818  (best train acc: 0.6876, best val acc: 0.8010, best train loss: 0.7741  @ epoch 5360 )\n",
      "[Epoch: 5400] train loss: 0.7816, train acc: 0.6786, val loss: 0.6354, val acc: 0.7902  (best train acc: 0.6896, best val acc: 0.8024, best train loss: 0.7741  @ epoch 5360 )\n",
      "[Epoch: 5420] train loss: 0.8058, train acc: 0.6598, val loss: 0.6395, val acc: 0.8030  (best train acc: 0.6896, best val acc: 0.8030, best train loss: 0.7741  @ epoch 5360 )\n",
      "[Epoch: 5440] train loss: 0.8146, train acc: 0.6481, val loss: 0.6299, val acc: 0.7966  (best train acc: 0.6896, best val acc: 0.8030, best train loss: 0.7741  @ epoch 5360 )\n",
      "[Epoch: 5460] train loss: 0.7992, train acc: 0.6757, val loss: 0.6422, val acc: 0.7926  (best train acc: 0.6896, best val acc: 0.8030, best train loss: 0.7741  @ epoch 5360 )\n",
      "[Epoch: 5480] train loss: 0.8077, train acc: 0.6739, val loss: 0.6284, val acc: 0.7535  (best train acc: 0.6896, best val acc: 0.8030, best train loss: 0.7741  @ epoch 5360 )\n",
      "[Epoch: 5500] train loss: 0.8086, train acc: 0.6612, val loss: 0.6455, val acc: 0.7275  (best train acc: 0.6896, best val acc: 0.8030, best train loss: 0.7741  @ epoch 5360 )\n",
      "[Epoch: 5520] train loss: 0.8114, train acc: 0.6381, val loss: 0.6330, val acc: 0.7848  (best train acc: 0.6896, best val acc: 0.8030, best train loss: 0.7741  @ epoch 5360 )\n",
      "[Epoch: 5540] train loss: 0.7891, train acc: 0.6651, val loss: 0.6482, val acc: 0.7885  (best train acc: 0.6900, best val acc: 0.8030, best train loss: 0.7741  @ epoch 5360 )\n",
      "[Epoch: 5560] train loss: 0.7918, train acc: 0.6799, val loss: 0.6291, val acc: 0.7973  (best train acc: 0.6900, best val acc: 0.8030, best train loss: 0.7741  @ epoch 5360 )\n",
      "[Epoch: 5580] train loss: 0.7961, train acc: 0.6707, val loss: 0.6196, val acc: 0.7899  (best train acc: 0.6900, best val acc: 0.8030, best train loss: 0.7741  @ epoch 5360 )\n",
      "[Epoch: 5600] train loss: 0.7773, train acc: 0.6737, val loss: 0.6355, val acc: 0.8017  (best train acc: 0.6946, best val acc: 0.8054, best train loss: 0.7741  @ epoch 5360 )\n",
      "[Epoch: 5620] train loss: 0.8091, train acc: 0.6803, val loss: 0.6510, val acc: 0.7707  (best train acc: 0.6946, best val acc: 0.8054, best train loss: 0.7741  @ epoch 5360 )\n",
      "[Epoch: 5640] train loss: 0.7893, train acc: 0.6680, val loss: 0.6263, val acc: 0.7993  (best train acc: 0.6946, best val acc: 0.8054, best train loss: 0.7741  @ epoch 5360 )\n",
      "[Epoch: 5660] train loss: 0.7864, train acc: 0.6752, val loss: 0.6340, val acc: 0.8003  (best train acc: 0.6980, best val acc: 0.8054, best train loss: 0.7708  @ epoch 5648 )\n",
      "[Epoch: 5680] train loss: 0.8007, train acc: 0.6687, val loss: 0.6146, val acc: 0.7757  (best train acc: 0.6980, best val acc: 0.8071, best train loss: 0.7679  @ epoch 5662 )\n",
      "[Epoch: 5700] train loss: 0.8153, train acc: 0.6703, val loss: 0.6304, val acc: 0.7815  (best train acc: 0.6980, best val acc: 0.8074, best train loss: 0.7679  @ epoch 5662 )\n",
      "[Epoch: 5720] train loss: 0.8183, train acc: 0.6350, val loss: 0.6275, val acc: 0.7963  (best train acc: 0.6980, best val acc: 0.8074, best train loss: 0.7554  @ epoch 5708 )\n",
      "[Epoch: 5740] train loss: 0.7804, train acc: 0.6806, val loss: 0.6157, val acc: 0.7943  (best train acc: 0.6980, best val acc: 0.8088, best train loss: 0.7554  @ epoch 5708 )\n",
      "[Epoch: 5760] train loss: 0.7810, train acc: 0.6896, val loss: 0.6238, val acc: 0.8091  (best train acc: 0.6980, best val acc: 0.8125, best train loss: 0.7554  @ epoch 5708 )\n",
      "[Epoch: 5780] train loss: 0.7761, train acc: 0.6760, val loss: 0.6564, val acc: 0.7201  (best train acc: 0.6980, best val acc: 0.8125, best train loss: 0.7554  @ epoch 5708 )\n",
      "[Epoch: 5800] train loss: 0.7946, train acc: 0.6662, val loss: 0.6327, val acc: 0.7646  (best train acc: 0.6980, best val acc: 0.8125, best train loss: 0.7554  @ epoch 5708 )\n",
      "[Epoch: 5820] train loss: 0.7874, train acc: 0.6735, val loss: 0.6159, val acc: 0.7761  (best train acc: 0.6980, best val acc: 0.8125, best train loss: 0.7554  @ epoch 5708 )\n",
      "[Epoch: 5840] train loss: 0.7715, train acc: 0.6949, val loss: 0.6125, val acc: 0.8034  (best train acc: 0.6980, best val acc: 0.8125, best train loss: 0.7554  @ epoch 5708 )\n",
      "[Epoch: 5860] train loss: 0.7633, train acc: 0.6880, val loss: 0.6187, val acc: 0.7956  (best train acc: 0.6980, best val acc: 0.8125, best train loss: 0.7554  @ epoch 5708 )\n",
      "[Epoch: 5880] train loss: 0.7792, train acc: 0.6763, val loss: 0.6313, val acc: 0.7771  (best train acc: 0.6980, best val acc: 0.8125, best train loss: 0.7554  @ epoch 5708 )\n",
      "[Epoch: 5900] train loss: 0.7846, train acc: 0.6580, val loss: 0.6210, val acc: 0.8047  (best train acc: 0.6980, best val acc: 0.8125, best train loss: 0.7554  @ epoch 5708 )\n",
      "[Epoch: 5920] train loss: 0.7851, train acc: 0.6745, val loss: 0.6126, val acc: 0.8034  (best train acc: 0.6980, best val acc: 0.8125, best train loss: 0.7554  @ epoch 5708 )\n",
      "[Epoch: 5940] train loss: 0.7888, train acc: 0.6836, val loss: 0.6106, val acc: 0.7592  (best train acc: 0.6980, best val acc: 0.8125, best train loss: 0.7554  @ epoch 5708 )\n",
      "[Epoch: 5960] train loss: 0.7706, train acc: 0.6740, val loss: 0.6135, val acc: 0.7963  (best train acc: 0.6980, best val acc: 0.8125, best train loss: 0.7554  @ epoch 5708 )\n",
      "[Epoch: 5980] train loss: 0.7575, train acc: 0.6908, val loss: 0.6177, val acc: 0.8054  (best train acc: 0.6980, best val acc: 0.8125, best train loss: 0.7554  @ epoch 5708 )\n",
      "[Epoch: 6000] train loss: 0.7878, train acc: 0.6679, val loss: 0.6219, val acc: 0.8071  (best train acc: 0.6993, best val acc: 0.8125, best train loss: 0.7554  @ epoch 5708 )\n",
      "[Epoch: 6020] train loss: 0.7732, train acc: 0.6860, val loss: 0.6292, val acc: 0.7798  (best train acc: 0.6993, best val acc: 0.8125, best train loss: 0.7554  @ epoch 5708 )\n",
      "[Epoch: 6040] train loss: 0.7797, train acc: 0.6811, val loss: 0.6108, val acc: 0.7879  (best train acc: 0.6993, best val acc: 0.8125, best train loss: 0.7554  @ epoch 5708 )\n",
      "[Epoch: 6060] train loss: 0.7712, train acc: 0.6802, val loss: 0.6143, val acc: 0.7889  (best train acc: 0.6993, best val acc: 0.8125, best train loss: 0.7554  @ epoch 5708 )\n",
      "[Epoch: 6080] train loss: 0.7735, train acc: 0.6928, val loss: 0.6179, val acc: 0.7960  (best train acc: 0.6993, best val acc: 0.8125, best train loss: 0.7554  @ epoch 5708 )\n",
      "[Epoch: 6100] train loss: 0.7846, train acc: 0.6752, val loss: 0.6243, val acc: 0.8074  (best train acc: 0.6993, best val acc: 0.8125, best train loss: 0.7515  @ epoch 6096 )\n",
      "[Epoch: 6120] train loss: 0.7732, train acc: 0.6795, val loss: 0.6402, val acc: 0.7707  (best train acc: 0.7003, best val acc: 0.8125, best train loss: 0.7515  @ epoch 6096 )\n",
      "[Epoch: 6140] train loss: 0.7743, train acc: 0.6833, val loss: 0.6146, val acc: 0.7669  (best train acc: 0.7003, best val acc: 0.8125, best train loss: 0.7515  @ epoch 6096 )\n",
      "[Epoch: 6160] train loss: 0.7840, train acc: 0.6851, val loss: 0.6157, val acc: 0.7943  (best train acc: 0.7003, best val acc: 0.8125, best train loss: 0.7515  @ epoch 6096 )\n",
      "[Epoch: 6180] train loss: 0.7696, train acc: 0.6818, val loss: 0.6130, val acc: 0.8054  (best train acc: 0.7003, best val acc: 0.8125, best train loss: 0.7515  @ epoch 6096 )\n",
      "[Epoch: 6200] train loss: 0.7644, train acc: 0.6708, val loss: 0.6130, val acc: 0.7831  (best train acc: 0.7003, best val acc: 0.8125, best train loss: 0.7515  @ epoch 6096 )\n",
      "[Epoch: 6220] train loss: 0.7652, train acc: 0.6909, val loss: 0.6052, val acc: 0.7906  (best train acc: 0.7014, best val acc: 0.8125, best train loss: 0.7515  @ epoch 6096 )\n",
      "[Epoch: 6240] train loss: 0.7804, train acc: 0.6748, val loss: 0.6349, val acc: 0.7828  (best train acc: 0.7014, best val acc: 0.8125, best train loss: 0.7515  @ epoch 6096 )\n",
      "[Epoch: 6260] train loss: 0.7696, train acc: 0.6803, val loss: 0.6181, val acc: 0.8010  (best train acc: 0.7014, best val acc: 0.8132, best train loss: 0.7464  @ epoch 6243 )\n",
      "[Epoch: 6280] train loss: 0.7733, train acc: 0.6875, val loss: 0.6123, val acc: 0.8152  (best train acc: 0.7014, best val acc: 0.8152, best train loss: 0.7464  @ epoch 6243 )\n",
      "[Epoch: 6300] train loss: 0.7849, train acc: 0.6610, val loss: 0.6287, val acc: 0.7841  (best train acc: 0.7014, best val acc: 0.8152, best train loss: 0.7464  @ epoch 6243 )\n",
      "[Epoch: 6320] train loss: 0.7764, train acc: 0.6786, val loss: 0.6086, val acc: 0.7761  (best train acc: 0.7014, best val acc: 0.8152, best train loss: 0.7464  @ epoch 6243 )\n",
      "[Epoch: 6340] train loss: 0.7871, train acc: 0.6731, val loss: 0.6084, val acc: 0.7619  (best train acc: 0.7014, best val acc: 0.8152, best train loss: 0.7464  @ epoch 6243 )\n",
      "[Epoch: 6360] train loss: 0.7683, train acc: 0.6746, val loss: 0.6090, val acc: 0.7673  (best train acc: 0.7014, best val acc: 0.8152, best train loss: 0.7464  @ epoch 6243 )\n",
      "[Epoch: 6380] train loss: 0.7647, train acc: 0.6762, val loss: 0.6063, val acc: 0.8128  (best train acc: 0.7014, best val acc: 0.8152, best train loss: 0.7464  @ epoch 6243 )\n",
      "[Epoch: 6400] train loss: 0.7655, train acc: 0.6923, val loss: 0.6148, val acc: 0.8071  (best train acc: 0.7042, best val acc: 0.8152, best train loss: 0.7464  @ epoch 6243 )\n",
      "[Epoch: 6420] train loss: 0.7909, train acc: 0.6668, val loss: 0.6225, val acc: 0.8098  (best train acc: 0.7042, best val acc: 0.8152, best train loss: 0.7464  @ epoch 6243 )\n",
      "[Epoch: 6440] train loss: 0.7658, train acc: 0.6931, val loss: 0.6194, val acc: 0.8074  (best train acc: 0.7042, best val acc: 0.8152, best train loss: 0.7464  @ epoch 6243 )\n",
      "[Epoch: 6460] train loss: 0.7747, train acc: 0.6852, val loss: 0.6148, val acc: 0.7821  (best train acc: 0.7042, best val acc: 0.8159, best train loss: 0.7464  @ epoch 6243 )\n",
      "[Epoch: 6480] train loss: 0.7624, train acc: 0.6992, val loss: 0.6164, val acc: 0.8111  (best train acc: 0.7046, best val acc: 0.8159, best train loss: 0.7464  @ epoch 6243 )\n",
      "[Epoch: 6500] train loss: 0.7675, train acc: 0.6888, val loss: 0.6217, val acc: 0.7889  (best train acc: 0.7046, best val acc: 0.8159, best train loss: 0.7464  @ epoch 6243 )\n",
      "[Epoch: 6520] train loss: 0.7783, train acc: 0.6824, val loss: 0.6021, val acc: 0.7892  (best train acc: 0.7046, best val acc: 0.8159, best train loss: 0.7464  @ epoch 6243 )\n",
      "[Epoch: 6540] train loss: 0.7549, train acc: 0.6872, val loss: 0.6153, val acc: 0.8084  (best train acc: 0.7046, best val acc: 0.8159, best train loss: 0.7464  @ epoch 6243 )\n",
      "[Epoch: 6560] train loss: 0.7874, train acc: 0.6713, val loss: 0.6188, val acc: 0.7953  (best train acc: 0.7046, best val acc: 0.8159, best train loss: 0.7464  @ epoch 6243 )\n",
      "[Epoch: 6580] train loss: 0.7531, train acc: 0.6878, val loss: 0.6212, val acc: 0.8108  (best train acc: 0.7046, best val acc: 0.8159, best train loss: 0.7464  @ epoch 6243 )\n",
      "[Epoch: 6600] train loss: 0.7853, train acc: 0.6559, val loss: 0.6023, val acc: 0.7777  (best train acc: 0.7046, best val acc: 0.8159, best train loss: 0.7464  @ epoch 6243 )\n",
      "[Epoch: 6620] train loss: 0.7642, train acc: 0.6791, val loss: 0.6317, val acc: 0.7717  (best train acc: 0.7046, best val acc: 0.8159, best train loss: 0.7410  @ epoch 6601 )\n",
      "[Epoch: 6640] train loss: 0.7796, train acc: 0.6733, val loss: 0.6052, val acc: 0.7943  (best train acc: 0.7085, best val acc: 0.8159, best train loss: 0.7410  @ epoch 6601 )\n",
      "[Epoch: 6660] train loss: 0.7543, train acc: 0.6931, val loss: 0.6043, val acc: 0.7993  (best train acc: 0.7085, best val acc: 0.8159, best train loss: 0.7410  @ epoch 6601 )\n",
      "[Epoch: 6680] train loss: 0.7661, train acc: 0.6700, val loss: 0.6039, val acc: 0.8094  (best train acc: 0.7085, best val acc: 0.8175, best train loss: 0.7410  @ epoch 6601 )\n",
      "[Epoch: 6700] train loss: 0.7794, train acc: 0.6759, val loss: 0.6068, val acc: 0.8125  (best train acc: 0.7085, best val acc: 0.8175, best train loss: 0.7410  @ epoch 6601 )\n",
      "[Epoch: 6720] train loss: 0.7705, train acc: 0.6883, val loss: 0.6300, val acc: 0.7764  (best train acc: 0.7085, best val acc: 0.8175, best train loss: 0.7410  @ epoch 6601 )\n",
      "[Epoch: 6740] train loss: 0.7545, train acc: 0.6746, val loss: 0.6177, val acc: 0.7683  (best train acc: 0.7085, best val acc: 0.8175, best train loss: 0.7410  @ epoch 6601 )\n",
      "[Epoch: 6760] train loss: 0.7737, train acc: 0.6948, val loss: 0.6075, val acc: 0.8078  (best train acc: 0.7085, best val acc: 0.8209, best train loss: 0.7410  @ epoch 6601 )\n",
      "[Epoch: 6780] train loss: 0.7650, train acc: 0.6960, val loss: 0.6026, val acc: 0.8162  (best train acc: 0.7085, best val acc: 0.8209, best train loss: 0.7410  @ epoch 6601 )\n",
      "[Epoch: 6800] train loss: 0.7748, train acc: 0.6917, val loss: 0.6081, val acc: 0.8061  (best train acc: 0.7085, best val acc: 0.8209, best train loss: 0.7410  @ epoch 6601 )\n",
      "[Epoch: 6820] train loss: 0.7615, train acc: 0.6872, val loss: 0.5991, val acc: 0.8027  (best train acc: 0.7085, best val acc: 0.8209, best train loss: 0.7410  @ epoch 6601 )\n",
      "[Epoch: 6840] train loss: 0.7616, train acc: 0.6745, val loss: 0.6065, val acc: 0.8192  (best train acc: 0.7085, best val acc: 0.8209, best train loss: 0.7410  @ epoch 6601 )\n",
      "[Epoch: 6860] train loss: 0.7546, train acc: 0.6852, val loss: 0.5918, val acc: 0.8030  (best train acc: 0.7085, best val acc: 0.8236, best train loss: 0.7408  @ epoch 6841 )\n",
      "[Epoch: 6880] train loss: 0.7534, train acc: 0.6970, val loss: 0.6062, val acc: 0.8162  (best train acc: 0.7123, best val acc: 0.8236, best train loss: 0.7407  @ epoch 6877 )\n",
      "[Epoch: 6900] train loss: 0.7556, train acc: 0.6985, val loss: 0.5986, val acc: 0.8145  (best train acc: 0.7128, best val acc: 0.8253, best train loss: 0.7378  @ epoch 6895 )\n",
      "[Epoch: 6920] train loss: 0.7392, train acc: 0.7181, val loss: 0.6030, val acc: 0.8256  (best train acc: 0.7181, best val acc: 0.8256, best train loss: 0.7378  @ epoch 6895 )\n",
      "[Epoch: 6940] train loss: 0.7424, train acc: 0.7094, val loss: 0.5974, val acc: 0.7933  (best train acc: 0.7181, best val acc: 0.8293, best train loss: 0.7270  @ epoch 6926 )\n",
      "[Epoch: 6960] train loss: 0.7275, train acc: 0.7140, val loss: 0.6038, val acc: 0.7936  (best train acc: 0.7181, best val acc: 0.8293, best train loss: 0.7270  @ epoch 6926 )\n",
      "[Epoch: 6980] train loss: 0.7338, train acc: 0.7125, val loss: 0.6043, val acc: 0.8165  (best train acc: 0.7201, best val acc: 0.8293, best train loss: 0.7214  @ epoch 6978 )\n",
      "[Epoch: 7000] train loss: 0.7426, train acc: 0.6992, val loss: 0.6045, val acc: 0.8152  (best train acc: 0.7201, best val acc: 0.8293, best train loss: 0.7214  @ epoch 6978 )\n",
      "[Epoch: 7020] train loss: 0.7453, train acc: 0.6938, val loss: 0.6071, val acc: 0.8223  (best train acc: 0.7201, best val acc: 0.8293, best train loss: 0.7214  @ epoch 6978 )\n",
      "[Epoch: 7040] train loss: 0.7448, train acc: 0.6958, val loss: 0.5953, val acc: 0.8155  (best train acc: 0.7212, best val acc: 0.8293, best train loss: 0.7214  @ epoch 6978 )\n",
      "[Epoch: 7060] train loss: 0.7409, train acc: 0.6999, val loss: 0.5970, val acc: 0.8047  (best train acc: 0.7217, best val acc: 0.8293, best train loss: 0.7190  @ epoch 7041 )\n",
      "[Epoch: 7080] train loss: 0.7431, train acc: 0.7053, val loss: 0.5938, val acc: 0.8057  (best train acc: 0.7226, best val acc: 0.8293, best train loss: 0.7190  @ epoch 7041 )\n",
      "[Epoch: 7100] train loss: 0.7285, train acc: 0.7148, val loss: 0.5960, val acc: 0.8037  (best train acc: 0.7226, best val acc: 0.8293, best train loss: 0.7190  @ epoch 7041 )\n",
      "[Epoch: 7120] train loss: 0.7485, train acc: 0.7101, val loss: 0.6119, val acc: 0.7987  (best train acc: 0.7226, best val acc: 0.8293, best train loss: 0.7190  @ epoch 7041 )\n",
      "[Epoch: 7140] train loss: 0.7283, train acc: 0.7212, val loss: 0.5900, val acc: 0.8061  (best train acc: 0.7226, best val acc: 0.8293, best train loss: 0.7190  @ epoch 7041 )\n",
      "[Epoch: 7160] train loss: 0.7544, train acc: 0.7005, val loss: 0.6090, val acc: 0.8155  (best train acc: 0.7228, best val acc: 0.8293, best train loss: 0.7190  @ epoch 7041 )\n",
      "[Epoch: 7180] train loss: 0.7343, train acc: 0.7202, val loss: 0.6042, val acc: 0.8074  (best train acc: 0.7240, best val acc: 0.8293, best train loss: 0.7131  @ epoch 7177 )\n",
      "[Epoch: 7200] train loss: 0.7492, train acc: 0.7089, val loss: 0.5904, val acc: 0.8098  (best train acc: 0.7240, best val acc: 0.8293, best train loss: 0.7131  @ epoch 7177 )\n",
      "[Epoch: 7220] train loss: 0.7165, train acc: 0.7203, val loss: 0.5922, val acc: 0.8064  (best train acc: 0.7255, best val acc: 0.8293, best train loss: 0.7081  @ epoch 7215 )\n",
      "[Epoch: 7240] train loss: 0.7222, train acc: 0.7176, val loss: 0.6038, val acc: 0.8128  (best train acc: 0.7272, best val acc: 0.8293, best train loss: 0.7081  @ epoch 7215 )\n",
      "[Epoch: 7260] train loss: 0.7427, train acc: 0.7190, val loss: 0.5949, val acc: 0.8138  (best train acc: 0.7272, best val acc: 0.8293, best train loss: 0.7081  @ epoch 7215 )\n",
      "[Epoch: 7280] train loss: 0.7262, train acc: 0.7196, val loss: 0.5966, val acc: 0.8105  (best train acc: 0.7272, best val acc: 0.8293, best train loss: 0.7081  @ epoch 7215 )\n",
      "[Epoch: 7300] train loss: 0.7363, train acc: 0.7119, val loss: 0.5933, val acc: 0.8101  (best train acc: 0.7272, best val acc: 0.8293, best train loss: 0.7081  @ epoch 7215 )\n",
      "[Epoch: 7320] train loss: 0.7431, train acc: 0.7200, val loss: 0.5857, val acc: 0.8003  (best train acc: 0.7277, best val acc: 0.8293, best train loss: 0.7081  @ epoch 7215 )\n",
      "[Epoch: 7340] train loss: 0.7333, train acc: 0.7189, val loss: 0.5906, val acc: 0.8020  (best train acc: 0.7277, best val acc: 0.8293, best train loss: 0.7081  @ epoch 7215 )\n",
      "[Epoch: 7360] train loss: 0.7278, train acc: 0.7190, val loss: 0.5956, val acc: 0.7960  (best train acc: 0.7277, best val acc: 0.8293, best train loss: 0.7081  @ epoch 7215 )\n",
      "[Epoch: 7380] train loss: 0.7324, train acc: 0.7177, val loss: 0.5902, val acc: 0.8037  (best train acc: 0.7277, best val acc: 0.8293, best train loss: 0.7081  @ epoch 7215 )\n",
      "[Epoch: 7400] train loss: 0.7231, train acc: 0.7175, val loss: 0.6020, val acc: 0.7990  (best train acc: 0.7277, best val acc: 0.8293, best train loss: 0.7081  @ epoch 7215 )\n",
      "[Epoch: 7420] train loss: 0.7301, train acc: 0.7207, val loss: 0.5807, val acc: 0.7858  (best train acc: 0.7277, best val acc: 0.8293, best train loss: 0.7081  @ epoch 7215 )\n",
      "[Epoch: 7440] train loss: 0.7486, train acc: 0.7120, val loss: 0.6022, val acc: 0.8142  (best train acc: 0.7277, best val acc: 0.8293, best train loss: 0.7081  @ epoch 7215 )\n",
      "[Epoch: 7460] train loss: 0.7410, train acc: 0.7192, val loss: 0.5963, val acc: 0.8138  (best train acc: 0.7277, best val acc: 0.8293, best train loss: 0.7081  @ epoch 7215 )\n",
      "[Epoch: 7480] train loss: 0.7282, train acc: 0.7131, val loss: 0.6067, val acc: 0.7909  (best train acc: 0.7277, best val acc: 0.8293, best train loss: 0.7081  @ epoch 7215 )\n",
      "[Epoch: 7500] train loss: 0.7321, train acc: 0.7197, val loss: 0.5991, val acc: 0.8081  (best train acc: 0.7277, best val acc: 0.8293, best train loss: 0.7081  @ epoch 7215 )\n",
      "[Epoch: 7520] train loss: 0.7388, train acc: 0.7124, val loss: 0.5956, val acc: 0.8159  (best train acc: 0.7277, best val acc: 0.8293, best train loss: 0.7081  @ epoch 7215 )\n",
      "[Epoch: 7540] train loss: 0.7205, train acc: 0.7202, val loss: 0.5994, val acc: 0.8132  (best train acc: 0.7277, best val acc: 0.8293, best train loss: 0.7081  @ epoch 7215 )\n",
      "[Epoch: 7560] train loss: 0.7355, train acc: 0.7194, val loss: 0.5859, val acc: 0.8128  (best train acc: 0.7277, best val acc: 0.8293, best train loss: 0.7027  @ epoch 7553 )\n",
      "[Epoch: 7580] train loss: 0.7222, train acc: 0.7240, val loss: 0.5860, val acc: 0.7825  (best train acc: 0.7277, best val acc: 0.8293, best train loss: 0.7027  @ epoch 7553 )\n",
      "[Epoch: 7600] train loss: 0.7406, train acc: 0.7159, val loss: 0.5902, val acc: 0.8145  (best train acc: 0.7277, best val acc: 0.8293, best train loss: 0.7027  @ epoch 7553 )\n",
      "[Epoch: 7620] train loss: 0.7356, train acc: 0.7163, val loss: 0.5978, val acc: 0.8078  (best train acc: 0.7277, best val acc: 0.8293, best train loss: 0.7027  @ epoch 7553 )\n",
      "[Epoch: 7640] train loss: 0.7444, train acc: 0.7102, val loss: 0.6112, val acc: 0.8111  (best train acc: 0.7277, best val acc: 0.8293, best train loss: 0.7027  @ epoch 7553 )\n",
      "[Epoch: 7660] train loss: 0.7456, train acc: 0.7077, val loss: 0.5912, val acc: 0.7757  (best train acc: 0.7277, best val acc: 0.8293, best train loss: 0.7027  @ epoch 7553 )\n",
      "[Epoch: 7680] train loss: 0.7501, train acc: 0.7042, val loss: 0.5926, val acc: 0.7970  (best train acc: 0.7277, best val acc: 0.8293, best train loss: 0.7027  @ epoch 7553 )\n",
      "[Epoch: 7700] train loss: 0.7604, train acc: 0.6971, val loss: 0.6008, val acc: 0.8138  (best train acc: 0.7277, best val acc: 0.8293, best train loss: 0.7027  @ epoch 7553 )\n",
      "[Epoch: 7720] train loss: 0.7220, train acc: 0.7191, val loss: 0.5907, val acc: 0.8115  (best train acc: 0.7277, best val acc: 0.8293, best train loss: 0.7027  @ epoch 7553 )\n",
      "[Epoch: 7740] train loss: 0.7214, train acc: 0.7222, val loss: 0.5976, val acc: 0.8132  (best train acc: 0.7277, best val acc: 0.8293, best train loss: 0.7027  @ epoch 7553 )\n",
      "[Epoch: 7760] train loss: 0.7278, train acc: 0.7149, val loss: 0.5869, val acc: 0.8118  (best train acc: 0.7277, best val acc: 0.8293, best train loss: 0.7027  @ epoch 7553 )\n",
      "[Epoch: 7780] train loss: 0.7185, train acc: 0.7234, val loss: 0.5878, val acc: 0.8024  (best train acc: 0.7285, best val acc: 0.8293, best train loss: 0.7027  @ epoch 7553 )\n",
      "[Epoch: 7800] train loss: 0.7197, train acc: 0.7211, val loss: 0.5880, val acc: 0.8047  (best train acc: 0.7285, best val acc: 0.8293, best train loss: 0.7027  @ epoch 7553 )\n",
      "[Epoch: 7820] train loss: 0.7311, train acc: 0.7118, val loss: 0.5801, val acc: 0.7993  (best train acc: 0.7308, best val acc: 0.8293, best train loss: 0.7027  @ epoch 7553 )\n",
      "[Epoch: 7840] train loss: 0.7313, train acc: 0.7146, val loss: 0.5825, val acc: 0.7892  (best train acc: 0.7308, best val acc: 0.8293, best train loss: 0.7027  @ epoch 7553 )\n",
      "[Epoch: 7860] train loss: 0.7129, train acc: 0.7253, val loss: 0.5877, val acc: 0.8175  (best train acc: 0.7308, best val acc: 0.8293, best train loss: 0.7027  @ epoch 7553 )\n",
      "[Epoch: 7880] train loss: 0.7245, train acc: 0.7150, val loss: 0.5847, val acc: 0.8024  (best train acc: 0.7308, best val acc: 0.8293, best train loss: 0.7027  @ epoch 7553 )\n",
      "[Epoch: 7900] train loss: 0.7088, train acc: 0.7222, val loss: 0.5900, val acc: 0.8148  (best train acc: 0.7308, best val acc: 0.8293, best train loss: 0.7027  @ epoch 7553 )\n",
      "[Epoch: 7920] train loss: 0.7193, train acc: 0.7240, val loss: 0.6030, val acc: 0.7852  (best train acc: 0.7308, best val acc: 0.8293, best train loss: 0.7027  @ epoch 7553 )\n",
      "[Epoch: 7940] train loss: 0.7416, train acc: 0.7142, val loss: 0.5823, val acc: 0.8007  (best train acc: 0.7308, best val acc: 0.8293, best train loss: 0.7027  @ epoch 7553 )\n",
      "[Epoch: 7960] train loss: 0.7364, train acc: 0.7131, val loss: 0.5817, val acc: 0.8017  (best train acc: 0.7308, best val acc: 0.8293, best train loss: 0.7027  @ epoch 7553 )\n",
      "[Epoch: 7980] train loss: 0.7261, train acc: 0.7170, val loss: 0.5912, val acc: 0.8162  (best train acc: 0.7308, best val acc: 0.8293, best train loss: 0.7027  @ epoch 7553 )\n",
      "[Epoch: 8000] train loss: 0.7421, train acc: 0.7123, val loss: 0.5952, val acc: 0.8108  (best train acc: 0.7308, best val acc: 0.8293, best train loss: 0.7027  @ epoch 7553 )\n",
      "[Epoch: 8020] train loss: 0.7382, train acc: 0.7148, val loss: 0.5902, val acc: 0.8105  (best train acc: 0.7308, best val acc: 0.8293, best train loss: 0.7027  @ epoch 7553 )\n",
      "[Epoch: 8040] train loss: 0.7355, train acc: 0.7180, val loss: 0.5787, val acc: 0.8098  (best train acc: 0.7308, best val acc: 0.8293, best train loss: 0.7027  @ epoch 7553 )\n",
      "[Epoch: 8060] train loss: 0.7298, train acc: 0.7190, val loss: 0.5875, val acc: 0.8148  (best train acc: 0.7308, best val acc: 0.8293, best train loss: 0.7027  @ epoch 7553 )\n",
      "[Epoch: 8080] train loss: 0.7304, train acc: 0.7174, val loss: 0.5921, val acc: 0.7980  (best train acc: 0.7308, best val acc: 0.8293, best train loss: 0.7027  @ epoch 7553 )\n",
      "[Epoch: 8100] train loss: 0.7257, train acc: 0.7200, val loss: 0.5794, val acc: 0.8192  (best train acc: 0.7308, best val acc: 0.8293, best train loss: 0.7027  @ epoch 7553 )\n",
      "[Epoch: 8120] train loss: 0.7354, train acc: 0.7164, val loss: 0.5823, val acc: 0.8159  (best train acc: 0.7308, best val acc: 0.8293, best train loss: 0.7027  @ epoch 7553 )\n",
      "[Epoch: 8140] train loss: 0.7285, train acc: 0.7147, val loss: 0.5807, val acc: 0.8071  (best train acc: 0.7308, best val acc: 0.8293, best train loss: 0.7027  @ epoch 7553 )\n",
      "[Epoch: 8160] train loss: 0.7375, train acc: 0.7186, val loss: 0.5771, val acc: 0.8064  (best train acc: 0.7308, best val acc: 0.8293, best train loss: 0.7027  @ epoch 7553 )\n",
      "[Epoch: 8180] train loss: 0.7264, train acc: 0.7141, val loss: 0.5827, val acc: 0.8138  (best train acc: 0.7308, best val acc: 0.8293, best train loss: 0.7027  @ epoch 7553 )\n",
      "[Epoch: 8200] train loss: 0.7290, train acc: 0.7273, val loss: 0.5930, val acc: 0.8175  (best train acc: 0.7308, best val acc: 0.8293, best train loss: 0.7027  @ epoch 7553 )\n",
      "[Epoch: 8220] train loss: 0.7161, train acc: 0.7282, val loss: 0.5843, val acc: 0.8132  (best train acc: 0.7308, best val acc: 0.8293, best train loss: 0.7027  @ epoch 7553 )\n",
      "[Epoch: 8240] train loss: 0.7299, train acc: 0.7157, val loss: 0.5894, val acc: 0.7990  (best train acc: 0.7329, best val acc: 0.8293, best train loss: 0.7027  @ epoch 7553 )\n",
      "[Epoch: 8260] train loss: 0.7190, train acc: 0.7214, val loss: 0.5897, val acc: 0.8067  (best train acc: 0.7329, best val acc: 0.8293, best train loss: 0.7027  @ epoch 7553 )\n",
      "[Epoch: 8280] train loss: 0.7236, train acc: 0.7201, val loss: 0.5853, val acc: 0.8206  (best train acc: 0.7329, best val acc: 0.8293, best train loss: 0.7027  @ epoch 7553 )\n",
      "[Epoch: 8300] train loss: 0.7396, train acc: 0.7188, val loss: 0.5925, val acc: 0.8209  (best train acc: 0.7329, best val acc: 0.8293, best train loss: 0.7027  @ epoch 7553 )\n",
      "[Epoch: 8320] train loss: 0.7275, train acc: 0.7171, val loss: 0.5832, val acc: 0.8182  (best train acc: 0.7329, best val acc: 0.8293, best train loss: 0.7027  @ epoch 7553 )\n",
      "[Epoch: 8340] train loss: 0.7192, train acc: 0.7217, val loss: 0.5811, val acc: 0.8132  (best train acc: 0.7329, best val acc: 0.8293, best train loss: 0.7027  @ epoch 7553 )\n",
      "[Epoch: 8360] train loss: 0.7241, train acc: 0.7235, val loss: 0.5921, val acc: 0.8192  (best train acc: 0.7329, best val acc: 0.8293, best train loss: 0.7027  @ epoch 7553 )\n",
      "[Epoch: 8380] train loss: 0.7197, train acc: 0.7240, val loss: 0.5840, val acc: 0.8196  (best train acc: 0.7329, best val acc: 0.8293, best train loss: 0.7027  @ epoch 7553 )\n",
      "[Epoch: 8400] train loss: 0.7146, train acc: 0.7285, val loss: 0.5842, val acc: 0.8212  (best train acc: 0.7329, best val acc: 0.8293, best train loss: 0.7027  @ epoch 7553 )\n",
      "[Epoch: 8420] train loss: 0.7148, train acc: 0.7265, val loss: 0.5833, val acc: 0.7960  (best train acc: 0.7329, best val acc: 0.8293, best train loss: 0.7027  @ epoch 7553 )\n",
      "[Epoch: 8440] train loss: 0.7341, train acc: 0.7171, val loss: 0.5849, val acc: 0.8206  (best train acc: 0.7329, best val acc: 0.8293, best train loss: 0.7027  @ epoch 7553 )\n",
      "[Epoch: 8460] train loss: 0.7321, train acc: 0.7194, val loss: 0.5772, val acc: 0.8067  (best train acc: 0.7329, best val acc: 0.8293, best train loss: 0.7027  @ epoch 7553 )\n",
      "[Epoch: 8480] train loss: 0.7299, train acc: 0.7182, val loss: 0.5880, val acc: 0.7956  (best train acc: 0.7329, best val acc: 0.8293, best train loss: 0.7027  @ epoch 7553 )\n",
      "[Epoch: 8500] train loss: 0.7289, train acc: 0.7089, val loss: 0.5787, val acc: 0.8007  (best train acc: 0.7329, best val acc: 0.8293, best train loss: 0.7027  @ epoch 7553 )\n",
      "[Epoch: 8520] train loss: 0.7296, train acc: 0.7073, val loss: 0.5748, val acc: 0.8145  (best train acc: 0.7329, best val acc: 0.8293, best train loss: 0.7027  @ epoch 7553 )\n",
      "[Epoch: 8540] train loss: 0.7333, train acc: 0.7180, val loss: 0.5944, val acc: 0.8067  (best train acc: 0.7332, best val acc: 0.8293, best train loss: 0.7027  @ epoch 7553 )\n",
      "[Epoch: 8560] train loss: 0.7305, train acc: 0.7212, val loss: 0.5822, val acc: 0.8067  (best train acc: 0.7343, best val acc: 0.8293, best train loss: 0.6939  @ epoch 8547 )\n",
      "[Epoch: 8580] train loss: 0.7272, train acc: 0.7199, val loss: 0.5778, val acc: 0.8172  (best train acc: 0.7343, best val acc: 0.8293, best train loss: 0.6939  @ epoch 8547 )\n",
      "[Epoch: 8600] train loss: 0.7181, train acc: 0.7230, val loss: 0.5879, val acc: 0.8115  (best train acc: 0.7343, best val acc: 0.8293, best train loss: 0.6939  @ epoch 8547 )\n",
      "[Epoch: 8620] train loss: 0.7255, train acc: 0.7171, val loss: 0.5822, val acc: 0.8108  (best train acc: 0.7343, best val acc: 0.8293, best train loss: 0.6939  @ epoch 8547 )\n",
      "[Epoch: 8640] train loss: 0.7406, train acc: 0.7152, val loss: 0.5882, val acc: 0.8145  (best train acc: 0.7343, best val acc: 0.8293, best train loss: 0.6939  @ epoch 8547 )\n",
      "[Epoch: 8660] train loss: 0.7307, train acc: 0.7182, val loss: 0.5888, val acc: 0.8017  (best train acc: 0.7343, best val acc: 0.8293, best train loss: 0.6939  @ epoch 8547 )\n",
      "[Epoch: 8680] train loss: 0.7144, train acc: 0.7262, val loss: 0.5865, val acc: 0.8081  (best train acc: 0.7343, best val acc: 0.8293, best train loss: 0.6939  @ epoch 8547 )\n",
      "[Epoch: 8700] train loss: 0.7183, train acc: 0.7304, val loss: 0.5917, val acc: 0.8179  (best train acc: 0.7343, best val acc: 0.8293, best train loss: 0.6939  @ epoch 8547 )\n",
      "[Epoch: 8720] train loss: 0.7161, train acc: 0.7198, val loss: 0.5872, val acc: 0.8155  (best train acc: 0.7343, best val acc: 0.8293, best train loss: 0.6939  @ epoch 8547 )\n",
      "[Epoch: 8740] train loss: 0.7348, train acc: 0.7143, val loss: 0.5817, val acc: 0.8128  (best train acc: 0.7343, best val acc: 0.8293, best train loss: 0.6939  @ epoch 8547 )\n",
      "[Epoch: 8760] train loss: 0.7199, train acc: 0.7220, val loss: 0.5766, val acc: 0.8121  (best train acc: 0.7343, best val acc: 0.8293, best train loss: 0.6939  @ epoch 8547 )\n",
      "[Epoch: 8780] train loss: 0.7350, train acc: 0.7147, val loss: 0.5773, val acc: 0.8148  (best train acc: 0.7343, best val acc: 0.8293, best train loss: 0.6939  @ epoch 8547 )\n",
      "[Epoch: 8800] train loss: 0.7195, train acc: 0.7335, val loss: 0.5771, val acc: 0.8138  (best train acc: 0.7343, best val acc: 0.8293, best train loss: 0.6939  @ epoch 8547 )\n",
      "[Epoch: 8820] train loss: 0.7153, train acc: 0.7238, val loss: 0.5798, val acc: 0.8125  (best train acc: 0.7343, best val acc: 0.8293, best train loss: 0.6939  @ epoch 8547 )\n",
      "[Epoch: 8840] train loss: 0.7326, train acc: 0.7193, val loss: 0.5886, val acc: 0.8185  (best train acc: 0.7343, best val acc: 0.8293, best train loss: 0.6939  @ epoch 8547 )\n",
      "[Epoch: 8860] train loss: 0.7220, train acc: 0.7298, val loss: 0.5815, val acc: 0.8175  (best train acc: 0.7343, best val acc: 0.8293, best train loss: 0.6939  @ epoch 8547 )\n",
      "[Epoch: 8880] train loss: 0.7175, train acc: 0.7268, val loss: 0.5799, val acc: 0.7983  (best train acc: 0.7343, best val acc: 0.8293, best train loss: 0.6939  @ epoch 8547 )\n",
      "[Epoch: 8900] train loss: 0.7150, train acc: 0.7204, val loss: 0.5853, val acc: 0.8189  (best train acc: 0.7359, best val acc: 0.8293, best train loss: 0.6939  @ epoch 8547 )\n",
      "[Epoch: 8920] train loss: 0.7209, train acc: 0.7246, val loss: 0.5892, val acc: 0.8229  (best train acc: 0.7359, best val acc: 0.8293, best train loss: 0.6939  @ epoch 8547 )\n",
      "[Epoch: 8940] train loss: 0.6931, train acc: 0.7291, val loss: 0.5815, val acc: 0.8081  (best train acc: 0.7359, best val acc: 0.8293, best train loss: 0.6931  @ epoch 8940 )\n",
      "[Epoch: 8960] train loss: 0.7139, train acc: 0.7323, val loss: 0.5748, val acc: 0.8162  (best train acc: 0.7359, best val acc: 0.8293, best train loss: 0.6931  @ epoch 8940 )\n",
      "[Epoch: 8980] train loss: 0.7098, train acc: 0.7233, val loss: 0.5801, val acc: 0.8105  (best train acc: 0.7359, best val acc: 0.8293, best train loss: 0.6931  @ epoch 8940 )\n",
      "[Epoch: 9000] train loss: 0.7221, train acc: 0.7218, val loss: 0.5921, val acc: 0.8209  (best train acc: 0.7359, best val acc: 0.8293, best train loss: 0.6920  @ epoch 8999 )\n",
      "[Epoch: 9020] train loss: 0.7160, train acc: 0.7215, val loss: 0.5776, val acc: 0.8010  (best train acc: 0.7359, best val acc: 0.8293, best train loss: 0.6920  @ epoch 8999 )\n",
      "[Epoch: 9040] train loss: 0.7261, train acc: 0.7287, val loss: 0.5799, val acc: 0.8236  (best train acc: 0.7359, best val acc: 0.8293, best train loss: 0.6920  @ epoch 8999 )\n",
      "[Epoch: 9060] train loss: 0.7281, train acc: 0.7262, val loss: 0.5767, val acc: 0.8155  (best train acc: 0.7359, best val acc: 0.8293, best train loss: 0.6920  @ epoch 8999 )\n",
      "[Epoch: 9080] train loss: 0.7046, train acc: 0.7284, val loss: 0.5845, val acc: 0.8148  (best train acc: 0.7359, best val acc: 0.8293, best train loss: 0.6920  @ epoch 8999 )\n",
      "[Epoch: 9100] train loss: 0.7124, train acc: 0.7267, val loss: 0.5850, val acc: 0.8105  (best train acc: 0.7359, best val acc: 0.8293, best train loss: 0.6920  @ epoch 8999 )\n",
      "[Epoch: 9120] train loss: 0.7198, train acc: 0.7211, val loss: 0.5693, val acc: 0.8121  (best train acc: 0.7359, best val acc: 0.8293, best train loss: 0.6920  @ epoch 8999 )\n",
      "[Epoch: 9140] train loss: 0.7227, train acc: 0.7196, val loss: 0.5795, val acc: 0.8202  (best train acc: 0.7359, best val acc: 0.8293, best train loss: 0.6920  @ epoch 8999 )\n",
      "[Epoch: 9160] train loss: 0.7277, train acc: 0.7248, val loss: 0.5796, val acc: 0.8277  (best train acc: 0.7359, best val acc: 0.8293, best train loss: 0.6920  @ epoch 8999 )\n",
      "[Epoch: 9180] train loss: 0.7093, train acc: 0.7279, val loss: 0.5736, val acc: 0.8169  (best train acc: 0.7373, best val acc: 0.8293, best train loss: 0.6920  @ epoch 8999 )\n",
      "[Epoch: 9200] train loss: 0.7083, train acc: 0.7317, val loss: 0.5804, val acc: 0.8155  (best train acc: 0.7373, best val acc: 0.8293, best train loss: 0.6920  @ epoch 8999 )\n",
      "[Epoch: 9220] train loss: 0.6983, train acc: 0.7303, val loss: 0.5736, val acc: 0.8175  (best train acc: 0.7373, best val acc: 0.8293, best train loss: 0.6920  @ epoch 8999 )\n",
      "[Epoch: 9240] train loss: 0.7085, train acc: 0.7227, val loss: 0.5753, val acc: 0.8192  (best train acc: 0.7373, best val acc: 0.8293, best train loss: 0.6920  @ epoch 8999 )\n",
      "[Epoch: 9260] train loss: 0.7233, train acc: 0.7177, val loss: 0.5720, val acc: 0.8098  (best train acc: 0.7373, best val acc: 0.8293, best train loss: 0.6920  @ epoch 8999 )\n",
      "[Epoch: 9280] train loss: 0.7178, train acc: 0.7302, val loss: 0.5752, val acc: 0.8148  (best train acc: 0.7373, best val acc: 0.8293, best train loss: 0.6920  @ epoch 8999 )\n",
      "[Epoch: 9300] train loss: 0.7036, train acc: 0.7264, val loss: 0.5762, val acc: 0.8216  (best train acc: 0.7373, best val acc: 0.8293, best train loss: 0.6920  @ epoch 8999 )\n",
      "[Epoch: 9320] train loss: 0.7115, train acc: 0.7316, val loss: 0.5788, val acc: 0.8182  (best train acc: 0.7373, best val acc: 0.8293, best train loss: 0.6920  @ epoch 8999 )\n",
      "[Epoch: 9340] train loss: 0.7243, train acc: 0.7168, val loss: 0.5790, val acc: 0.8179  (best train acc: 0.7373, best val acc: 0.8293, best train loss: 0.6920  @ epoch 8999 )\n",
      "[Epoch: 9360] train loss: 0.7231, train acc: 0.7218, val loss: 0.5677, val acc: 0.8206  (best train acc: 0.7373, best val acc: 0.8293, best train loss: 0.6920  @ epoch 8999 )\n",
      "[Epoch: 9380] train loss: 0.6860, train acc: 0.7349, val loss: 0.5723, val acc: 0.8199  (best train acc: 0.7373, best val acc: 0.8293, best train loss: 0.6850  @ epoch 9367 )\n",
      "[Epoch: 9400] train loss: 0.7037, train acc: 0.7252, val loss: 0.5744, val acc: 0.8192  (best train acc: 0.7373, best val acc: 0.8293, best train loss: 0.6850  @ epoch 9367 )\n",
      "[Epoch: 9420] train loss: 0.7153, train acc: 0.7207, val loss: 0.5717, val acc: 0.8233  (best train acc: 0.7373, best val acc: 0.8293, best train loss: 0.6850  @ epoch 9367 )\n",
      "[Epoch: 9440] train loss: 0.7287, train acc: 0.7225, val loss: 0.5660, val acc: 0.8226  (best train acc: 0.7373, best val acc: 0.8293, best train loss: 0.6850  @ epoch 9367 )\n",
      "[Epoch: 9460] train loss: 0.7237, train acc: 0.7243, val loss: 0.5644, val acc: 0.8206  (best train acc: 0.7420, best val acc: 0.8293, best train loss: 0.6850  @ epoch 9367 )\n",
      "[Epoch: 9480] train loss: 0.7004, train acc: 0.7272, val loss: 0.5714, val acc: 0.8209  (best train acc: 0.7420, best val acc: 0.8293, best train loss: 0.6850  @ epoch 9367 )\n",
      "[Epoch: 9500] train loss: 0.7181, train acc: 0.7264, val loss: 0.5683, val acc: 0.8196  (best train acc: 0.7420, best val acc: 0.8293, best train loss: 0.6850  @ epoch 9367 )\n",
      "[Epoch: 9520] train loss: 0.6926, train acc: 0.7308, val loss: 0.5623, val acc: 0.8199  (best train acc: 0.7420, best val acc: 0.8293, best train loss: 0.6838  @ epoch 9508 )\n",
      "[Epoch: 9540] train loss: 0.7028, train acc: 0.7239, val loss: 0.5614, val acc: 0.8260  (best train acc: 0.7420, best val acc: 0.8293, best train loss: 0.6838  @ epoch 9508 )\n",
      "[Epoch: 9560] train loss: 0.7005, train acc: 0.7284, val loss: 0.5621, val acc: 0.8229  (best train acc: 0.7420, best val acc: 0.8293, best train loss: 0.6838  @ epoch 9508 )\n",
      "[Epoch: 9580] train loss: 0.7099, train acc: 0.7230, val loss: 0.5540, val acc: 0.8243  (best train acc: 0.7420, best val acc: 0.8293, best train loss: 0.6769  @ epoch 9578 )\n",
      "[Epoch: 9600] train loss: 0.6872, train acc: 0.7331, val loss: 0.5587, val acc: 0.8293  (best train acc: 0.7420, best val acc: 0.8314, best train loss: 0.6769  @ epoch 9578 )\n",
      "[Epoch: 9620] train loss: 0.6964, train acc: 0.7279, val loss: 0.5557, val acc: 0.8277  (best train acc: 0.7420, best val acc: 0.8314, best train loss: 0.6769  @ epoch 9578 )\n",
      "[Epoch: 9640] train loss: 0.6921, train acc: 0.7300, val loss: 0.5509, val acc: 0.8239  (best train acc: 0.7420, best val acc: 0.8314, best train loss: 0.6728  @ epoch 9626 )\n",
      "[Epoch: 9660] train loss: 0.6818, train acc: 0.7312, val loss: 0.5497, val acc: 0.8297  (best train acc: 0.7420, best val acc: 0.8314, best train loss: 0.6728  @ epoch 9626 )\n",
      "[Epoch: 9680] train loss: 0.6937, train acc: 0.7313, val loss: 0.5509, val acc: 0.8246  (best train acc: 0.7420, best val acc: 0.8314, best train loss: 0.6728  @ epoch 9626 )\n",
      "[Epoch: 9700] train loss: 0.6812, train acc: 0.7319, val loss: 0.5558, val acc: 0.8283  (best train acc: 0.7420, best val acc: 0.8317, best train loss: 0.6686  @ epoch 9685 )\n",
      "[Epoch: 9720] train loss: 0.6810, train acc: 0.7277, val loss: 0.5529, val acc: 0.8216  (best train acc: 0.7420, best val acc: 0.8317, best train loss: 0.6686  @ epoch 9685 )\n",
      "[Epoch: 9740] train loss: 0.6773, train acc: 0.7312, val loss: 0.5532, val acc: 0.8277  (best train acc: 0.7420, best val acc: 0.8334, best train loss: 0.6686  @ epoch 9685 )\n",
      "[Epoch: 9760] train loss: 0.6774, train acc: 0.7400, val loss: 0.5504, val acc: 0.8280  (best train acc: 0.7420, best val acc: 0.8334, best train loss: 0.6680  @ epoch 9742 )\n",
      "[Epoch: 9780] train loss: 0.6787, train acc: 0.7386, val loss: 0.5568, val acc: 0.8317  (best train acc: 0.7433, best val acc: 0.8334, best train loss: 0.6680  @ epoch 9742 )\n",
      "[Epoch: 9800] train loss: 0.6914, train acc: 0.7324, val loss: 0.5555, val acc: 0.8266  (best train acc: 0.7433, best val acc: 0.8351, best train loss: 0.6680  @ epoch 9742 )\n",
      "[Epoch: 9820] train loss: 0.6704, train acc: 0.7404, val loss: 0.5510, val acc: 0.8300  (best train acc: 0.7433, best val acc: 0.8351, best train loss: 0.6680  @ epoch 9742 )\n",
      "[Epoch: 9840] train loss: 0.6876, train acc: 0.7376, val loss: 0.5540, val acc: 0.8324  (best train acc: 0.7433, best val acc: 0.8371, best train loss: 0.6680  @ epoch 9742 )\n",
      "[Epoch: 9860] train loss: 0.6736, train acc: 0.7390, val loss: 0.5568, val acc: 0.8331  (best train acc: 0.7433, best val acc: 0.8371, best train loss: 0.6680  @ epoch 9742 )\n",
      "[Epoch: 9880] train loss: 0.6775, train acc: 0.7353, val loss: 0.5547, val acc: 0.8314  (best train acc: 0.7435, best val acc: 0.8371, best train loss: 0.6680  @ epoch 9742 )\n",
      "[Epoch: 9900] train loss: 0.6875, train acc: 0.7342, val loss: 0.5485, val acc: 0.8304  (best train acc: 0.7442, best val acc: 0.8381, best train loss: 0.6680  @ epoch 9742 )\n",
      "[Epoch: 9920] train loss: 0.6887, train acc: 0.7273, val loss: 0.5465, val acc: 0.8250  (best train acc: 0.7442, best val acc: 0.8381, best train loss: 0.6680  @ epoch 9742 )\n",
      "[Epoch: 9940] train loss: 0.6774, train acc: 0.7382, val loss: 0.5515, val acc: 0.8293  (best train acc: 0.7442, best val acc: 0.8391, best train loss: 0.6680  @ epoch 9742 )\n",
      "[Epoch: 9960] train loss: 0.6785, train acc: 0.7415, val loss: 0.5532, val acc: 0.8354  (best train acc: 0.7442, best val acc: 0.8405, best train loss: 0.6680  @ epoch 9742 )\n",
      "[Epoch: 9980] train loss: 0.6808, train acc: 0.7350, val loss: 0.5485, val acc: 0.8314  (best train acc: 0.7449, best val acc: 0.8405, best train loss: 0.6680  @ epoch 9742 )\n",
      "[Epoch: 10000] train loss: 0.6761, train acc: 0.7436, val loss: 0.5507, val acc: 0.8347  (best train acc: 0.7465, best val acc: 0.8405, best train loss: 0.6657  @ epoch 9982 )\n",
      "[Epoch: 10020] train loss: 0.6806, train acc: 0.7403, val loss: 0.5490, val acc: 0.8341  (best train acc: 0.7467, best val acc: 0.8405, best train loss: 0.6656  @ epoch 10010 )\n",
      "[Epoch: 10040] train loss: 0.6935, train acc: 0.7336, val loss: 0.5494, val acc: 0.8358  (best train acc: 0.7467, best val acc: 0.8405, best train loss: 0.6656  @ epoch 10010 )\n",
      "[Epoch: 10060] train loss: 0.6673, train acc: 0.7433, val loss: 0.5481, val acc: 0.8374  (best train acc: 0.7473, best val acc: 0.8405, best train loss: 0.6570  @ epoch 10053 )\n",
      "[Epoch: 10080] train loss: 0.7070, train acc: 0.7340, val loss: 0.5491, val acc: 0.8364  (best train acc: 0.7473, best val acc: 0.8405, best train loss: 0.6570  @ epoch 10053 )\n",
      "[Epoch: 10100] train loss: 0.6896, train acc: 0.7400, val loss: 0.5468, val acc: 0.8334  (best train acc: 0.7473, best val acc: 0.8405, best train loss: 0.6570  @ epoch 10053 )\n",
      "[Epoch: 10120] train loss: 0.6799, train acc: 0.7358, val loss: 0.5503, val acc: 0.8310  (best train acc: 0.7473, best val acc: 0.8405, best train loss: 0.6570  @ epoch 10053 )\n",
      "[Epoch: 10140] train loss: 0.6770, train acc: 0.7338, val loss: 0.5465, val acc: 0.8368  (best train acc: 0.7475, best val acc: 0.8405, best train loss: 0.6570  @ epoch 10053 )\n",
      "[Epoch: 10160] train loss: 0.6899, train acc: 0.7378, val loss: 0.5468, val acc: 0.8334  (best train acc: 0.7507, best val acc: 0.8405, best train loss: 0.6570  @ epoch 10053 )\n",
      "[Epoch: 10180] train loss: 0.6933, train acc: 0.7350, val loss: 0.5467, val acc: 0.8280  (best train acc: 0.7507, best val acc: 0.8405, best train loss: 0.6570  @ epoch 10053 )\n",
      "[Epoch: 10200] train loss: 0.6856, train acc: 0.7407, val loss: 0.5467, val acc: 0.8405  (best train acc: 0.7507, best val acc: 0.8411, best train loss: 0.6570  @ epoch 10053 )\n",
      "[Epoch: 10220] train loss: 0.6698, train acc: 0.7369, val loss: 0.5439, val acc: 0.8293  (best train acc: 0.7507, best val acc: 0.8411, best train loss: 0.6570  @ epoch 10053 )\n",
      "[Epoch: 10240] train loss: 0.6683, train acc: 0.7419, val loss: 0.5491, val acc: 0.8364  (best train acc: 0.7507, best val acc: 0.8411, best train loss: 0.6564  @ epoch 10226 )\n",
      "[Epoch: 10260] train loss: 0.6671, train acc: 0.7431, val loss: 0.5502, val acc: 0.8344  (best train acc: 0.7507, best val acc: 0.8411, best train loss: 0.6564  @ epoch 10226 )\n",
      "[Epoch: 10280] train loss: 0.7040, train acc: 0.7387, val loss: 0.5454, val acc: 0.8314  (best train acc: 0.7507, best val acc: 0.8411, best train loss: 0.6564  @ epoch 10226 )\n",
      "[Epoch: 10300] train loss: 0.6704, train acc: 0.7391, val loss: 0.5435, val acc: 0.8361  (best train acc: 0.7507, best val acc: 0.8411, best train loss: 0.6564  @ epoch 10226 )\n",
      "[Epoch: 10320] train loss: 0.6742, train acc: 0.7469, val loss: 0.5469, val acc: 0.8351  (best train acc: 0.7507, best val acc: 0.8411, best train loss: 0.6564  @ epoch 10226 )\n",
      "[Epoch: 10340] train loss: 0.7036, train acc: 0.7295, val loss: 0.5425, val acc: 0.8331  (best train acc: 0.7507, best val acc: 0.8411, best train loss: 0.6564  @ epoch 10226 )\n",
      "[Epoch: 10360] train loss: 0.6687, train acc: 0.7406, val loss: 0.5447, val acc: 0.8368  (best train acc: 0.7507, best val acc: 0.8411, best train loss: 0.6564  @ epoch 10226 )\n",
      "[Epoch: 10380] train loss: 0.6961, train acc: 0.7342, val loss: 0.5453, val acc: 0.8233  (best train acc: 0.7507, best val acc: 0.8411, best train loss: 0.6564  @ epoch 10226 )\n",
      "[Epoch: 10400] train loss: 0.6764, train acc: 0.7393, val loss: 0.5432, val acc: 0.8368  (best train acc: 0.7507, best val acc: 0.8411, best train loss: 0.6564  @ epoch 10226 )\n",
      "[Epoch: 10420] train loss: 0.6822, train acc: 0.7417, val loss: 0.5466, val acc: 0.8256  (best train acc: 0.7507, best val acc: 0.8425, best train loss: 0.6552  @ epoch 10419 )\n",
      "[Epoch: 10440] train loss: 0.6603, train acc: 0.7462, val loss: 0.5456, val acc: 0.8391  (best train acc: 0.7543, best val acc: 0.8425, best train loss: 0.6442  @ epoch 10434 )\n",
      "[Epoch: 10460] train loss: 0.6754, train acc: 0.7430, val loss: 0.5443, val acc: 0.8327  (best train acc: 0.7543, best val acc: 0.8425, best train loss: 0.6442  @ epoch 10434 )\n",
      "[Epoch: 10480] train loss: 0.6775, train acc: 0.7431, val loss: 0.5403, val acc: 0.8314  (best train acc: 0.7543, best val acc: 0.8425, best train loss: 0.6442  @ epoch 10434 )\n",
      "[Epoch: 10500] train loss: 0.6806, train acc: 0.7405, val loss: 0.5421, val acc: 0.8381  (best train acc: 0.7543, best val acc: 0.8425, best train loss: 0.6442  @ epoch 10434 )\n",
      "[Epoch: 10520] train loss: 0.6850, train acc: 0.7400, val loss: 0.5457, val acc: 0.8351  (best train acc: 0.7543, best val acc: 0.8425, best train loss: 0.6442  @ epoch 10434 )\n",
      "[Epoch: 10540] train loss: 0.6637, train acc: 0.7445, val loss: 0.5403, val acc: 0.8314  (best train acc: 0.7543, best val acc: 0.8425, best train loss: 0.6442  @ epoch 10434 )\n",
      "[Epoch: 10560] train loss: 0.6648, train acc: 0.7436, val loss: 0.5414, val acc: 0.8290  (best train acc: 0.7543, best val acc: 0.8425, best train loss: 0.6442  @ epoch 10434 )\n",
      "[Epoch: 10580] train loss: 0.6575, train acc: 0.7396, val loss: 0.5408, val acc: 0.8391  (best train acc: 0.7543, best val acc: 0.8425, best train loss: 0.6442  @ epoch 10434 )\n",
      "[Epoch: 10600] train loss: 0.6960, train acc: 0.7363, val loss: 0.5429, val acc: 0.8344  (best train acc: 0.7543, best val acc: 0.8425, best train loss: 0.6442  @ epoch 10434 )\n",
      "[Epoch: 10620] train loss: 0.6752, train acc: 0.7426, val loss: 0.5376, val acc: 0.8310  (best train acc: 0.7543, best val acc: 0.8425, best train loss: 0.6442  @ epoch 10434 )\n",
      "[Epoch: 10640] train loss: 0.6854, train acc: 0.7334, val loss: 0.5389, val acc: 0.8358  (best train acc: 0.7543, best val acc: 0.8425, best train loss: 0.6442  @ epoch 10434 )\n",
      "[Epoch: 10660] train loss: 0.6893, train acc: 0.7391, val loss: 0.5462, val acc: 0.8347  (best train acc: 0.7543, best val acc: 0.8425, best train loss: 0.6442  @ epoch 10434 )\n",
      "[Epoch: 10680] train loss: 0.6661, train acc: 0.7405, val loss: 0.5411, val acc: 0.8314  (best train acc: 0.7543, best val acc: 0.8425, best train loss: 0.6442  @ epoch 10434 )\n",
      "[Epoch: 10700] train loss: 0.6660, train acc: 0.7483, val loss: 0.5435, val acc: 0.8395  (best train acc: 0.7543, best val acc: 0.8442, best train loss: 0.6442  @ epoch 10434 )\n",
      "[Epoch: 10720] train loss: 0.7025, train acc: 0.7313, val loss: 0.5403, val acc: 0.8351  (best train acc: 0.7543, best val acc: 0.8442, best train loss: 0.6442  @ epoch 10434 )\n",
      "[Epoch: 10740] train loss: 0.6687, train acc: 0.7408, val loss: 0.5390, val acc: 0.8368  (best train acc: 0.7543, best val acc: 0.8442, best train loss: 0.6442  @ epoch 10434 )\n",
      "[Epoch: 10760] train loss: 0.6787, train acc: 0.7378, val loss: 0.5411, val acc: 0.8344  (best train acc: 0.7543, best val acc: 0.8442, best train loss: 0.6442  @ epoch 10434 )\n",
      "[Epoch: 10780] train loss: 0.6679, train acc: 0.7384, val loss: 0.5462, val acc: 0.8418  (best train acc: 0.7543, best val acc: 0.8442, best train loss: 0.6442  @ epoch 10434 )\n",
      "[Epoch: 10800] train loss: 0.6728, train acc: 0.7377, val loss: 0.5449, val acc: 0.8300  (best train acc: 0.7543, best val acc: 0.8442, best train loss: 0.6442  @ epoch 10434 )\n",
      "[Epoch: 10820] train loss: 0.6774, train acc: 0.7340, val loss: 0.5440, val acc: 0.8324  (best train acc: 0.7543, best val acc: 0.8442, best train loss: 0.6442  @ epoch 10434 )\n",
      "[Epoch: 10840] train loss: 0.6604, train acc: 0.7481, val loss: 0.5439, val acc: 0.8354  (best train acc: 0.7543, best val acc: 0.8442, best train loss: 0.6442  @ epoch 10434 )\n",
      "[Epoch: 10860] train loss: 0.6814, train acc: 0.7407, val loss: 0.5444, val acc: 0.8395  (best train acc: 0.7543, best val acc: 0.8442, best train loss: 0.6442  @ epoch 10434 )\n",
      "[Epoch: 10880] train loss: 0.6948, train acc: 0.7383, val loss: 0.5363, val acc: 0.8270  (best train acc: 0.7543, best val acc: 0.8442, best train loss: 0.6442  @ epoch 10434 )\n",
      "[Epoch: 10900] train loss: 0.6732, train acc: 0.7424, val loss: 0.5344, val acc: 0.8320  (best train acc: 0.7543, best val acc: 0.8442, best train loss: 0.6442  @ epoch 10434 )\n",
      "[Epoch: 10920] train loss: 0.6818, train acc: 0.7443, val loss: 0.5420, val acc: 0.8374  (best train acc: 0.7543, best val acc: 0.8442, best train loss: 0.6442  @ epoch 10434 )\n",
      "[Epoch: 10940] train loss: 0.6732, train acc: 0.7423, val loss: 0.5352, val acc: 0.8354  (best train acc: 0.7543, best val acc: 0.8442, best train loss: 0.6442  @ epoch 10434 )\n",
      "[Epoch: 10960] train loss: 0.6847, train acc: 0.7304, val loss: 0.5388, val acc: 0.8280  (best train acc: 0.7543, best val acc: 0.8442, best train loss: 0.6442  @ epoch 10434 )\n",
      "[Epoch: 10980] train loss: 0.6813, train acc: 0.7389, val loss: 0.5415, val acc: 0.8452  (best train acc: 0.7543, best val acc: 0.8452, best train loss: 0.6442  @ epoch 10434 )\n",
      "[Epoch: 11000] train loss: 0.6829, train acc: 0.7379, val loss: 0.5442, val acc: 0.8415  (best train acc: 0.7543, best val acc: 0.8452, best train loss: 0.6442  @ epoch 10434 )\n",
      "[Epoch: 11020] train loss: 0.6763, train acc: 0.7363, val loss: 0.5355, val acc: 0.8324  (best train acc: 0.7543, best val acc: 0.8452, best train loss: 0.6442  @ epoch 10434 )\n",
      "[Epoch: 11040] train loss: 0.6751, train acc: 0.7410, val loss: 0.5345, val acc: 0.8334  (best train acc: 0.7543, best val acc: 0.8452, best train loss: 0.6442  @ epoch 10434 )\n",
      "[Epoch: 11060] train loss: 0.6734, train acc: 0.7392, val loss: 0.5418, val acc: 0.8344  (best train acc: 0.7543, best val acc: 0.8452, best train loss: 0.6442  @ epoch 10434 )\n",
      "[Epoch: 11080] train loss: 0.6607, train acc: 0.7486, val loss: 0.5405, val acc: 0.8337  (best train acc: 0.7543, best val acc: 0.8452, best train loss: 0.6442  @ epoch 10434 )\n",
      "[Epoch: 11100] train loss: 0.6738, train acc: 0.7358, val loss: 0.5312, val acc: 0.8378  (best train acc: 0.7543, best val acc: 0.8452, best train loss: 0.6442  @ epoch 10434 )\n",
      "[Epoch: 11120] train loss: 0.6744, train acc: 0.7382, val loss: 0.5345, val acc: 0.8411  (best train acc: 0.7543, best val acc: 0.8452, best train loss: 0.6442  @ epoch 10434 )\n",
      "[Epoch: 11140] train loss: 0.6676, train acc: 0.7482, val loss: 0.5367, val acc: 0.8347  (best train acc: 0.7543, best val acc: 0.8452, best train loss: 0.6442  @ epoch 10434 )\n",
      "[Epoch: 11160] train loss: 0.6623, train acc: 0.7426, val loss: 0.5373, val acc: 0.8364  (best train acc: 0.7543, best val acc: 0.8452, best train loss: 0.6442  @ epoch 10434 )\n",
      "[Epoch: 11180] train loss: 0.6608, train acc: 0.7409, val loss: 0.5382, val acc: 0.8358  (best train acc: 0.7543, best val acc: 0.8452, best train loss: 0.6442  @ epoch 10434 )\n",
      "[Epoch: 11200] train loss: 0.6698, train acc: 0.7398, val loss: 0.5409, val acc: 0.8358  (best train acc: 0.7543, best val acc: 0.8452, best train loss: 0.6442  @ epoch 10434 )\n",
      "[Epoch: 11220] train loss: 0.6741, train acc: 0.7412, val loss: 0.5448, val acc: 0.8344  (best train acc: 0.7543, best val acc: 0.8452, best train loss: 0.6442  @ epoch 10434 )\n",
      "[Epoch: 11240] train loss: 0.6657, train acc: 0.7417, val loss: 0.5353, val acc: 0.8381  (best train acc: 0.7543, best val acc: 0.8452, best train loss: 0.6442  @ epoch 10434 )\n",
      "[Epoch: 11260] train loss: 0.6708, train acc: 0.7389, val loss: 0.5385, val acc: 0.8314  (best train acc: 0.7543, best val acc: 0.8452, best train loss: 0.6442  @ epoch 10434 )\n",
      "[Epoch: 11280] train loss: 0.6426, train acc: 0.7473, val loss: 0.5334, val acc: 0.8358  (best train acc: 0.7543, best val acc: 0.8452, best train loss: 0.6426  @ epoch 11280 )\n",
      "[Epoch: 11300] train loss: 0.6929, train acc: 0.7349, val loss: 0.5390, val acc: 0.8432  (best train acc: 0.7543, best val acc: 0.8452, best train loss: 0.6426  @ epoch 11280 )\n",
      "[Epoch: 11320] train loss: 0.6677, train acc: 0.7410, val loss: 0.5349, val acc: 0.8337  (best train acc: 0.7543, best val acc: 0.8452, best train loss: 0.6426  @ epoch 11280 )\n",
      "[Epoch: 11340] train loss: 0.6796, train acc: 0.7363, val loss: 0.5333, val acc: 0.8361  (best train acc: 0.7543, best val acc: 0.8452, best train loss: 0.6426  @ epoch 11280 )\n",
      "[Epoch: 11360] train loss: 0.6802, train acc: 0.7418, val loss: 0.5340, val acc: 0.8374  (best train acc: 0.7543, best val acc: 0.8452, best train loss: 0.6426  @ epoch 11280 )\n",
      "[Epoch: 11380] train loss: 0.6815, train acc: 0.7405, val loss: 0.5328, val acc: 0.8344  (best train acc: 0.7543, best val acc: 0.8452, best train loss: 0.6426  @ epoch 11280 )\n",
      "[Epoch: 11400] train loss: 0.6659, train acc: 0.7421, val loss: 0.5361, val acc: 0.8405  (best train acc: 0.7543, best val acc: 0.8452, best train loss: 0.6426  @ epoch 11280 )\n",
      "[Epoch: 11420] train loss: 0.6779, train acc: 0.7415, val loss: 0.5350, val acc: 0.8378  (best train acc: 0.7543, best val acc: 0.8452, best train loss: 0.6426  @ epoch 11280 )\n",
      "[Epoch: 11440] train loss: 0.6630, train acc: 0.7412, val loss: 0.5328, val acc: 0.8384  (best train acc: 0.7543, best val acc: 0.8452, best train loss: 0.6426  @ epoch 11280 )\n",
      "[Epoch: 11460] train loss: 0.6742, train acc: 0.7381, val loss: 0.5402, val acc: 0.8391  (best train acc: 0.7543, best val acc: 0.8452, best train loss: 0.6426  @ epoch 11280 )\n",
      "[Epoch: 11480] train loss: 0.6672, train acc: 0.7444, val loss: 0.5358, val acc: 0.8371  (best train acc: 0.7543, best val acc: 0.8472, best train loss: 0.6426  @ epoch 11280 )\n",
      "[Epoch: 11500] train loss: 0.6684, train acc: 0.7366, val loss: 0.5376, val acc: 0.8331  (best train acc: 0.7543, best val acc: 0.8472, best train loss: 0.6426  @ epoch 11280 )\n",
      "[Epoch: 11520] train loss: 0.7035, train acc: 0.7282, val loss: 0.5376, val acc: 0.8364  (best train acc: 0.7543, best val acc: 0.8472, best train loss: 0.6426  @ epoch 11280 )\n",
      "[Epoch: 11540] train loss: 0.6848, train acc: 0.7423, val loss: 0.5385, val acc: 0.8361  (best train acc: 0.7543, best val acc: 0.8472, best train loss: 0.6426  @ epoch 11280 )\n",
      "[Epoch: 11560] train loss: 0.6604, train acc: 0.7423, val loss: 0.5269, val acc: 0.8391  (best train acc: 0.7543, best val acc: 0.8472, best train loss: 0.6426  @ epoch 11280 )\n",
      "[Epoch: 11580] train loss: 0.6683, train acc: 0.7465, val loss: 0.5337, val acc: 0.8411  (best train acc: 0.7543, best val acc: 0.8472, best train loss: 0.6426  @ epoch 11280 )\n",
      "[Epoch: 11600] train loss: 0.6806, train acc: 0.7381, val loss: 0.5316, val acc: 0.8391  (best train acc: 0.7543, best val acc: 0.8472, best train loss: 0.6426  @ epoch 11280 )\n",
      "[Epoch: 11620] train loss: 0.6646, train acc: 0.7334, val loss: 0.5342, val acc: 0.8361  (best train acc: 0.7543, best val acc: 0.8506, best train loss: 0.6426  @ epoch 11280 )\n",
      "[Epoch: 11640] train loss: 0.6927, train acc: 0.7345, val loss: 0.5295, val acc: 0.8425  (best train acc: 0.7543, best val acc: 0.8506, best train loss: 0.6426  @ epoch 11280 )\n",
      "[Epoch: 11660] train loss: 0.6802, train acc: 0.7350, val loss: 0.5389, val acc: 0.8351  (best train acc: 0.7543, best val acc: 0.8506, best train loss: 0.6426  @ epoch 11280 )\n",
      "[Epoch: 11680] train loss: 0.6804, train acc: 0.7454, val loss: 0.5305, val acc: 0.8425  (best train acc: 0.7543, best val acc: 0.8506, best train loss: 0.6426  @ epoch 11280 )\n",
      "[Epoch: 11700] train loss: 0.6772, train acc: 0.7413, val loss: 0.5343, val acc: 0.8432  (best train acc: 0.7543, best val acc: 0.8506, best train loss: 0.6426  @ epoch 11280 )\n",
      "[Epoch: 11720] train loss: 0.6718, train acc: 0.7404, val loss: 0.5327, val acc: 0.8452  (best train acc: 0.7543, best val acc: 0.8506, best train loss: 0.6426  @ epoch 11280 )\n",
      "[Epoch: 11740] train loss: 0.6687, train acc: 0.7449, val loss: 0.5382, val acc: 0.8398  (best train acc: 0.7543, best val acc: 0.8506, best train loss: 0.6426  @ epoch 11280 )\n",
      "[Epoch: 11760] train loss: 0.6674, train acc: 0.7450, val loss: 0.5271, val acc: 0.8418  (best train acc: 0.7543, best val acc: 0.8506, best train loss: 0.6426  @ epoch 11280 )\n",
      "[Epoch: 11780] train loss: 0.6740, train acc: 0.7359, val loss: 0.5284, val acc: 0.8354  (best train acc: 0.7543, best val acc: 0.8506, best train loss: 0.6426  @ epoch 11280 )\n",
      "[Epoch: 11800] train loss: 0.6687, train acc: 0.7361, val loss: 0.5302, val acc: 0.8395  (best train acc: 0.7543, best val acc: 0.8506, best train loss: 0.6426  @ epoch 11280 )\n",
      "[Epoch: 11820] train loss: 0.6665, train acc: 0.7450, val loss: 0.5362, val acc: 0.8445  (best train acc: 0.7543, best val acc: 0.8506, best train loss: 0.6395  @ epoch 11801 )\n",
      "[Epoch: 11840] train loss: 0.6704, train acc: 0.7355, val loss: 0.5270, val acc: 0.8465  (best train acc: 0.7543, best val acc: 0.8506, best train loss: 0.6395  @ epoch 11801 )\n",
      "[Epoch: 11860] train loss: 0.6721, train acc: 0.7408, val loss: 0.5322, val acc: 0.8358  (best train acc: 0.7543, best val acc: 0.8506, best train loss: 0.6395  @ epoch 11801 )\n",
      "[Epoch: 11880] train loss: 0.6682, train acc: 0.7379, val loss: 0.5433, val acc: 0.8159  (best train acc: 0.7543, best val acc: 0.8506, best train loss: 0.6395  @ epoch 11801 )\n",
      "[Epoch: 11900] train loss: 0.6593, train acc: 0.7434, val loss: 0.5388, val acc: 0.8351  (best train acc: 0.7543, best val acc: 0.8519, best train loss: 0.6395  @ epoch 11801 )\n",
      "[Epoch: 11920] train loss: 0.6624, train acc: 0.7444, val loss: 0.5311, val acc: 0.8401  (best train acc: 0.7543, best val acc: 0.8536, best train loss: 0.6395  @ epoch 11801 )\n",
      "[Epoch: 11940] train loss: 0.6541, train acc: 0.7432, val loss: 0.5284, val acc: 0.8499  (best train acc: 0.7551, best val acc: 0.8536, best train loss: 0.6395  @ epoch 11801 )\n",
      "[Epoch: 11960] train loss: 0.6663, train acc: 0.7429, val loss: 0.5354, val acc: 0.8418  (best train acc: 0.7551, best val acc: 0.8590, best train loss: 0.6359  @ epoch 11959 )\n",
      "[Epoch: 11980] train loss: 0.6612, train acc: 0.7542, val loss: 0.5295, val acc: 0.8580  (best train acc: 0.7551, best val acc: 0.8590, best train loss: 0.6359  @ epoch 11959 )\n",
      "[Epoch: 12000] train loss: 0.6530, train acc: 0.7579, val loss: 0.5240, val acc: 0.8499  (best train acc: 0.7579, best val acc: 0.8590, best train loss: 0.6359  @ epoch 11959 )\n",
      "[Epoch: 12020] train loss: 0.6579, train acc: 0.7478, val loss: 0.5262, val acc: 0.8543  (best train acc: 0.7579, best val acc: 0.8590, best train loss: 0.6359  @ epoch 11959 )\n",
      "[Epoch: 12040] train loss: 0.6744, train acc: 0.7438, val loss: 0.5258, val acc: 0.8358  (best train acc: 0.7579, best val acc: 0.8590, best train loss: 0.6359  @ epoch 11959 )\n",
      "[Epoch: 12060] train loss: 0.6537, train acc: 0.7346, val loss: 0.5214, val acc: 0.8395  (best train acc: 0.7579, best val acc: 0.8590, best train loss: 0.6359  @ epoch 11959 )\n",
      "[Epoch: 12080] train loss: 0.6671, train acc: 0.7420, val loss: 0.5299, val acc: 0.8459  (best train acc: 0.7579, best val acc: 0.8597, best train loss: 0.6359  @ epoch 11959 )\n",
      "[Epoch: 12100] train loss: 0.6808, train acc: 0.7381, val loss: 0.5270, val acc: 0.8519  (best train acc: 0.7579, best val acc: 0.8597, best train loss: 0.6359  @ epoch 11959 )\n",
      "[Epoch: 12120] train loss: 0.6706, train acc: 0.7349, val loss: 0.5197, val acc: 0.8580  (best train acc: 0.7582, best val acc: 0.8631, best train loss: 0.6359  @ epoch 11959 )\n",
      "[Epoch: 12140] train loss: 0.6519, train acc: 0.7524, val loss: 0.5221, val acc: 0.8496  (best train acc: 0.7590, best val acc: 0.8631, best train loss: 0.6359  @ epoch 11959 )\n",
      "[Epoch: 12160] train loss: 0.6565, train acc: 0.7449, val loss: 0.5188, val acc: 0.8479  (best train acc: 0.7590, best val acc: 0.8631, best train loss: 0.6359  @ epoch 11959 )\n",
      "[Epoch: 12180] train loss: 0.6666, train acc: 0.7491, val loss: 0.5232, val acc: 0.8408  (best train acc: 0.7590, best val acc: 0.8644, best train loss: 0.6359  @ epoch 11959 )\n",
      "[Epoch: 12200] train loss: 0.6585, train acc: 0.7389, val loss: 0.5282, val acc: 0.8442  (best train acc: 0.7590, best val acc: 0.8644, best train loss: 0.6359  @ epoch 11959 )\n",
      "[Epoch: 12220] train loss: 0.6697, train acc: 0.7426, val loss: 0.5217, val acc: 0.8523  (best train acc: 0.7590, best val acc: 0.8644, best train loss: 0.6359  @ epoch 11959 )\n",
      "[Epoch: 12240] train loss: 0.6569, train acc: 0.7492, val loss: 0.5129, val acc: 0.8476  (best train acc: 0.7622, best val acc: 0.8678, best train loss: 0.6359  @ epoch 11959 )\n",
      "[Epoch: 12260] train loss: 0.6494, train acc: 0.7574, val loss: 0.5222, val acc: 0.8462  (best train acc: 0.7622, best val acc: 0.8749, best train loss: 0.6359  @ epoch 11959 )\n",
      "[Epoch: 12280] train loss: 0.6479, train acc: 0.7616, val loss: 0.5168, val acc: 0.8594  (best train acc: 0.7622, best val acc: 0.8749, best train loss: 0.6359  @ epoch 11959 )\n",
      "[Epoch: 12300] train loss: 0.6591, train acc: 0.7382, val loss: 0.5094, val acc: 0.8617  (best train acc: 0.7667, best val acc: 0.8749, best train loss: 0.6359  @ epoch 11959 )\n",
      "[Epoch: 12320] train loss: 0.7088, train acc: 0.7134, val loss: 0.5271, val acc: 0.8449  (best train acc: 0.7667, best val acc: 0.8749, best train loss: 0.6359  @ epoch 11959 )\n",
      "[Epoch: 12340] train loss: 0.6728, train acc: 0.7460, val loss: 0.5100, val acc: 0.8614  (best train acc: 0.7667, best val acc: 0.8749, best train loss: 0.6359  @ epoch 11959 )\n",
      "[Epoch: 12360] train loss: 0.6591, train acc: 0.7459, val loss: 0.5030, val acc: 0.8577  (best train acc: 0.7667, best val acc: 0.8749, best train loss: 0.6269  @ epoch 12357 )\n",
      "[Epoch: 12380] train loss: 0.6470, train acc: 0.7624, val loss: 0.5123, val acc: 0.8472  (best train acc: 0.7667, best val acc: 0.8749, best train loss: 0.6269  @ epoch 12357 )\n",
      "[Epoch: 12400] train loss: 0.6521, train acc: 0.7478, val loss: 0.5183, val acc: 0.8435  (best train acc: 0.7667, best val acc: 0.8749, best train loss: 0.6269  @ epoch 12357 )\n",
      "[Epoch: 12420] train loss: 0.6487, train acc: 0.7533, val loss: 0.5085, val acc: 0.8796  (best train acc: 0.7667, best val acc: 0.8796, best train loss: 0.6269  @ epoch 12357 )\n",
      "[Epoch: 12440] train loss: 0.6612, train acc: 0.7441, val loss: 0.5157, val acc: 0.8573  (best train acc: 0.7667, best val acc: 0.8796, best train loss: 0.6269  @ epoch 12357 )\n",
      "[Epoch: 12460] train loss: 0.6420, train acc: 0.7544, val loss: 0.5034, val acc: 0.8580  (best train acc: 0.7667, best val acc: 0.8796, best train loss: 0.6269  @ epoch 12357 )\n",
      "[Epoch: 12480] train loss: 0.6699, train acc: 0.7517, val loss: 0.5025, val acc: 0.8594  (best train acc: 0.7667, best val acc: 0.8796, best train loss: 0.6269  @ epoch 12357 )\n",
      "[Epoch: 12500] train loss: 0.6383, train acc: 0.7600, val loss: 0.5122, val acc: 0.8476  (best train acc: 0.7667, best val acc: 0.8796, best train loss: 0.6269  @ epoch 12357 )\n",
      "[Epoch: 12520] train loss: 0.6580, train acc: 0.7475, val loss: 0.5625, val acc: 0.8115  (best train acc: 0.7667, best val acc: 0.8823, best train loss: 0.6269  @ epoch 12357 )\n",
      "[Epoch: 12540] train loss: 0.6793, train acc: 0.7486, val loss: 0.5108, val acc: 0.8631  (best train acc: 0.7667, best val acc: 0.8823, best train loss: 0.6269  @ epoch 12357 )\n",
      "[Epoch: 12560] train loss: 0.6429, train acc: 0.7561, val loss: 0.5023, val acc: 0.8624  (best train acc: 0.7709, best val acc: 0.8823, best train loss: 0.6249  @ epoch 12541 )\n",
      "[Epoch: 12580] train loss: 0.6483, train acc: 0.7679, val loss: 0.4989, val acc: 0.8496  (best train acc: 0.7709, best val acc: 0.8823, best train loss: 0.6249  @ epoch 12541 )\n",
      "[Epoch: 12600] train loss: 0.6488, train acc: 0.7530, val loss: 0.5047, val acc: 0.8577  (best train acc: 0.7709, best val acc: 0.8823, best train loss: 0.6249  @ epoch 12541 )\n",
      "[Epoch: 12620] train loss: 0.6529, train acc: 0.7578, val loss: 0.4891, val acc: 0.8685  (best train acc: 0.7709, best val acc: 0.8823, best train loss: 0.6249  @ epoch 12541 )\n",
      "[Epoch: 12640] train loss: 0.6621, train acc: 0.7477, val loss: 0.5146, val acc: 0.8381  (best train acc: 0.7709, best val acc: 0.8823, best train loss: 0.6249  @ epoch 12541 )\n",
      "[Epoch: 12660] train loss: 0.7001, train acc: 0.7178, val loss: 0.5275, val acc: 0.8118  (best train acc: 0.7709, best val acc: 0.8823, best train loss: 0.6249  @ epoch 12541 )\n",
      "[Epoch: 12680] train loss: 0.6539, train acc: 0.7590, val loss: 0.5014, val acc: 0.8553  (best train acc: 0.7709, best val acc: 0.8823, best train loss: 0.6249  @ epoch 12541 )\n",
      "[Epoch: 12700] train loss: 0.6500, train acc: 0.7553, val loss: 0.5000, val acc: 0.8664  (best train acc: 0.7709, best val acc: 0.8823, best train loss: 0.6232  @ epoch 12692 )\n",
      "[Epoch: 12720] train loss: 0.6588, train acc: 0.7584, val loss: 0.4975, val acc: 0.8570  (best train acc: 0.7709, best val acc: 0.8823, best train loss: 0.6232  @ epoch 12692 )\n",
      "[Epoch: 12740] train loss: 0.6328, train acc: 0.7603, val loss: 0.4972, val acc: 0.8516  (best train acc: 0.7712, best val acc: 0.8874, best train loss: 0.6232  @ epoch 12692 )\n",
      "[Epoch: 12760] train loss: 0.6300, train acc: 0.7545, val loss: 0.4926, val acc: 0.8513  (best train acc: 0.7712, best val acc: 0.8907, best train loss: 0.6232  @ epoch 12692 )\n",
      "[Epoch: 12780] train loss: 0.6939, train acc: 0.7084, val loss: 0.5291, val acc: 0.8206  (best train acc: 0.7725, best val acc: 0.8907, best train loss: 0.6232  @ epoch 12692 )\n",
      "[Epoch: 12800] train loss: 0.6429, train acc: 0.7579, val loss: 0.4864, val acc: 0.8877  (best train acc: 0.7725, best val acc: 0.8907, best train loss: 0.6232  @ epoch 12692 )\n",
      "[Epoch: 12820] train loss: 0.6472, train acc: 0.7515, val loss: 0.4959, val acc: 0.8621  (best train acc: 0.7733, best val acc: 0.8907, best train loss: 0.6150  @ epoch 12802 )\n",
      "[Epoch: 12840] train loss: 0.6682, train acc: 0.7408, val loss: 0.4878, val acc: 0.8604  (best train acc: 0.7733, best val acc: 0.8907, best train loss: 0.6150  @ epoch 12802 )\n",
      "[Epoch: 12860] train loss: 0.6452, train acc: 0.7632, val loss: 0.4825, val acc: 0.8725  (best train acc: 0.7733, best val acc: 0.8907, best train loss: 0.6150  @ epoch 12802 )\n",
      "[Epoch: 12880] train loss: 0.6434, train acc: 0.7572, val loss: 0.4860, val acc: 0.8779  (best train acc: 0.7751, best val acc: 0.8907, best train loss: 0.6150  @ epoch 12802 )\n",
      "[Epoch: 12900] train loss: 0.6454, train acc: 0.7575, val loss: 0.4879, val acc: 0.8644  (best train acc: 0.7764, best val acc: 0.8907, best train loss: 0.6150  @ epoch 12802 )\n",
      "[Epoch: 12920] train loss: 0.6332, train acc: 0.7691, val loss: 0.4848, val acc: 0.8526  (best train acc: 0.7764, best val acc: 0.8907, best train loss: 0.6150  @ epoch 12802 )\n",
      "[Epoch: 12940] train loss: 0.6522, train acc: 0.7454, val loss: 0.5033, val acc: 0.8405  (best train acc: 0.7764, best val acc: 0.8907, best train loss: 0.6150  @ epoch 12802 )\n",
      "[Epoch: 12960] train loss: 0.6457, train acc: 0.7632, val loss: 0.4839, val acc: 0.8583  (best train acc: 0.7764, best val acc: 0.8907, best train loss: 0.6150  @ epoch 12802 )\n",
      "[Epoch: 12980] train loss: 0.6450, train acc: 0.7672, val loss: 0.4924, val acc: 0.8614  (best train acc: 0.7764, best val acc: 0.8907, best train loss: 0.6150  @ epoch 12802 )\n",
      "[Epoch: 13000] train loss: 0.6364, train acc: 0.7576, val loss: 0.4929, val acc: 0.8546  (best train acc: 0.7764, best val acc: 0.8907, best train loss: 0.6150  @ epoch 12802 )\n",
      "[Epoch: 13020] train loss: 0.6327, train acc: 0.7758, val loss: 0.4780, val acc: 0.8853  (best train acc: 0.7764, best val acc: 0.8917, best train loss: 0.6150  @ epoch 12802 )\n",
      "[Epoch: 13040] train loss: 0.6367, train acc: 0.7619, val loss: 0.4915, val acc: 0.8523  (best train acc: 0.7764, best val acc: 0.8917, best train loss: 0.6150  @ epoch 12802 )\n",
      "[Epoch: 13060] train loss: 0.6613, train acc: 0.7410, val loss: 0.5206, val acc: 0.8307  (best train acc: 0.7764, best val acc: 0.8917, best train loss: 0.6150  @ epoch 12802 )\n",
      "[Epoch: 13080] train loss: 0.6533, train acc: 0.7603, val loss: 0.4974, val acc: 0.8506  (best train acc: 0.7764, best val acc: 0.8917, best train loss: 0.6150  @ epoch 12802 )\n",
      "[Epoch: 13100] train loss: 0.6381, train acc: 0.7587, val loss: 0.4757, val acc: 0.8799  (best train acc: 0.7764, best val acc: 0.8917, best train loss: 0.6150  @ epoch 12802 )\n",
      "[Epoch: 13120] train loss: 0.6690, train acc: 0.7403, val loss: 0.5106, val acc: 0.8253  (best train acc: 0.7764, best val acc: 0.8917, best train loss: 0.6150  @ epoch 12802 )\n",
      "[Epoch: 13140] train loss: 0.6427, train acc: 0.7491, val loss: 0.4870, val acc: 0.8715  (best train acc: 0.7764, best val acc: 0.8917, best train loss: 0.6150  @ epoch 12802 )\n",
      "[Epoch: 13160] train loss: 0.6363, train acc: 0.7690, val loss: 0.5463, val acc: 0.8307  (best train acc: 0.7764, best val acc: 0.8917, best train loss: 0.6150  @ epoch 12802 )\n",
      "[Epoch: 13180] train loss: 0.6444, train acc: 0.7578, val loss: 0.4847, val acc: 0.8516  (best train acc: 0.7764, best val acc: 0.8917, best train loss: 0.6150  @ epoch 12802 )\n",
      "[Epoch: 13200] train loss: 0.6429, train acc: 0.7652, val loss: 0.4901, val acc: 0.8472  (best train acc: 0.7764, best val acc: 0.8917, best train loss: 0.6150  @ epoch 12802 )\n",
      "[Epoch: 13220] train loss: 0.6379, train acc: 0.7637, val loss: 0.4728, val acc: 0.8728  (best train acc: 0.7775, best val acc: 0.8917, best train loss: 0.6150  @ epoch 12802 )\n",
      "[Epoch: 13240] train loss: 0.6550, train acc: 0.7594, val loss: 0.4745, val acc: 0.8688  (best train acc: 0.7775, best val acc: 0.8917, best train loss: 0.6092  @ epoch 13233 )\n",
      "[Epoch: 13260] train loss: 0.6292, train acc: 0.7729, val loss: 0.4792, val acc: 0.8877  (best train acc: 0.7775, best val acc: 0.8917, best train loss: 0.6092  @ epoch 13233 )\n",
      "[Epoch: 13280] train loss: 0.6568, train acc: 0.7450, val loss: 0.4742, val acc: 0.8732  (best train acc: 0.7775, best val acc: 0.8917, best train loss: 0.6092  @ epoch 13233 )\n",
      "[Epoch: 13300] train loss: 0.6256, train acc: 0.7657, val loss: 0.4791, val acc: 0.8621  (best train acc: 0.7775, best val acc: 0.8917, best train loss: 0.6092  @ epoch 13233 )\n",
      "[Epoch: 13320] train loss: 0.6518, train acc: 0.7475, val loss: 0.4863, val acc: 0.8570  (best train acc: 0.7775, best val acc: 0.8917, best train loss: 0.6092  @ epoch 13233 )\n",
      "[Epoch: 13340] train loss: 0.6263, train acc: 0.7680, val loss: 0.4726, val acc: 0.8698  (best train acc: 0.7775, best val acc: 0.8917, best train loss: 0.6092  @ epoch 13233 )\n",
      "[Epoch: 13360] train loss: 0.6238, train acc: 0.7679, val loss: 0.4706, val acc: 0.8661  (best train acc: 0.7817, best val acc: 0.8917, best train loss: 0.6092  @ epoch 13233 )\n",
      "[Epoch: 13380] train loss: 0.6288, train acc: 0.7611, val loss: 0.4704, val acc: 0.8786  (best train acc: 0.7817, best val acc: 0.8917, best train loss: 0.6092  @ epoch 13233 )\n",
      "[Epoch: 13400] train loss: 0.6768, train acc: 0.7441, val loss: 0.4963, val acc: 0.8486  (best train acc: 0.7817, best val acc: 0.8917, best train loss: 0.6092  @ epoch 13233 )\n",
      "[Epoch: 13420] train loss: 0.6494, train acc: 0.7507, val loss: 0.4931, val acc: 0.8411  (best train acc: 0.7817, best val acc: 0.8917, best train loss: 0.6092  @ epoch 13233 )\n",
      "[Epoch: 13440] train loss: 0.6381, train acc: 0.7595, val loss: 0.4762, val acc: 0.8509  (best train acc: 0.7817, best val acc: 0.8917, best train loss: 0.6092  @ epoch 13233 )\n",
      "[Epoch: 13460] train loss: 0.6402, train acc: 0.7619, val loss: 0.4692, val acc: 0.8826  (best train acc: 0.7817, best val acc: 0.8917, best train loss: 0.6092  @ epoch 13233 )\n",
      "[Epoch: 13480] train loss: 0.6424, train acc: 0.7643, val loss: 0.4794, val acc: 0.8594  (best train acc: 0.7817, best val acc: 0.8917, best train loss: 0.6092  @ epoch 13233 )\n",
      "[Epoch: 13500] train loss: 0.6350, train acc: 0.7639, val loss: 0.4803, val acc: 0.8648  (best train acc: 0.7817, best val acc: 0.8917, best train loss: 0.6067  @ epoch 13483 )\n",
      "[Epoch: 13520] train loss: 0.6374, train acc: 0.7666, val loss: 0.4653, val acc: 0.8749  (best train acc: 0.7817, best val acc: 0.8917, best train loss: 0.6067  @ epoch 13483 )\n",
      "[Epoch: 13540] train loss: 0.6143, train acc: 0.7755, val loss: 0.4694, val acc: 0.8725  (best train acc: 0.7817, best val acc: 0.8927, best train loss: 0.6067  @ epoch 13483 )\n",
      "[Epoch: 13560] train loss: 0.6175, train acc: 0.7691, val loss: 0.4719, val acc: 0.8847  (best train acc: 0.7817, best val acc: 0.8927, best train loss: 0.6067  @ epoch 13483 )\n",
      "[Epoch: 13580] train loss: 0.6337, train acc: 0.7644, val loss: 0.4857, val acc: 0.8614  (best train acc: 0.7817, best val acc: 0.8927, best train loss: 0.6067  @ epoch 13483 )\n",
      "[Epoch: 13600] train loss: 0.6319, train acc: 0.7635, val loss: 0.4814, val acc: 0.8735  (best train acc: 0.7817, best val acc: 0.8927, best train loss: 0.6067  @ epoch 13483 )\n",
      "[Epoch: 13620] train loss: 0.6318, train acc: 0.7610, val loss: 0.4825, val acc: 0.8702  (best train acc: 0.7817, best val acc: 0.8927, best train loss: 0.6067  @ epoch 13483 )\n",
      "[Epoch: 13640] train loss: 0.6367, train acc: 0.7650, val loss: 0.4813, val acc: 0.8634  (best train acc: 0.7817, best val acc: 0.8927, best train loss: 0.6067  @ epoch 13483 )\n",
      "[Epoch: 13660] train loss: 0.6528, train acc: 0.7561, val loss: 0.4891, val acc: 0.8523  (best train acc: 0.7817, best val acc: 0.8927, best train loss: 0.6067  @ epoch 13483 )\n",
      "[Epoch: 13680] train loss: 0.6376, train acc: 0.7676, val loss: 0.4656, val acc: 0.8789  (best train acc: 0.7817, best val acc: 0.8927, best train loss: 0.6067  @ epoch 13483 )\n",
      "[Epoch: 13700] train loss: 0.6504, train acc: 0.7415, val loss: 0.4994, val acc: 0.8550  (best train acc: 0.7822, best val acc: 0.8927, best train loss: 0.5980  @ epoch 13685 )\n",
      "[Epoch: 13720] train loss: 0.6425, train acc: 0.7604, val loss: 0.4710, val acc: 0.8782  (best train acc: 0.7822, best val acc: 0.8938, best train loss: 0.5980  @ epoch 13685 )\n",
      "[Epoch: 13740] train loss: 0.6214, train acc: 0.7595, val loss: 0.4797, val acc: 0.8570  (best train acc: 0.7822, best val acc: 0.8941, best train loss: 0.5980  @ epoch 13685 )\n",
      "[Epoch: 13760] train loss: 0.6339, train acc: 0.7705, val loss: 0.4771, val acc: 0.8540  (best train acc: 0.7822, best val acc: 0.8941, best train loss: 0.5980  @ epoch 13685 )\n",
      "[Epoch: 13780] train loss: 0.6395, train acc: 0.7653, val loss: 0.4967, val acc: 0.8368  (best train acc: 0.7822, best val acc: 0.8941, best train loss: 0.5980  @ epoch 13685 )\n",
      "[Epoch: 13800] train loss: 0.6535, train acc: 0.7384, val loss: 0.4927, val acc: 0.8519  (best train acc: 0.7822, best val acc: 0.8941, best train loss: 0.5980  @ epoch 13685 )\n",
      "[Epoch: 13820] train loss: 0.6580, train acc: 0.7590, val loss: 0.4746, val acc: 0.8523  (best train acc: 0.7822, best val acc: 0.8941, best train loss: 0.5980  @ epoch 13685 )\n",
      "[Epoch: 13840] train loss: 0.6172, train acc: 0.7721, val loss: 0.4699, val acc: 0.8779  (best train acc: 0.7822, best val acc: 0.8941, best train loss: 0.5980  @ epoch 13685 )\n",
      "[Epoch: 13860] train loss: 0.6145, train acc: 0.7761, val loss: 0.4749, val acc: 0.8769  (best train acc: 0.7822, best val acc: 0.8941, best train loss: 0.5980  @ epoch 13685 )\n",
      "[Epoch: 13880] train loss: 0.6313, train acc: 0.7693, val loss: 0.4608, val acc: 0.8826  (best train acc: 0.7822, best val acc: 0.8941, best train loss: 0.5980  @ epoch 13685 )\n",
      "[Epoch: 13900] train loss: 0.6314, train acc: 0.7667, val loss: 0.4717, val acc: 0.8553  (best train acc: 0.7822, best val acc: 0.8941, best train loss: 0.5980  @ epoch 13685 )\n",
      "[Epoch: 13920] train loss: 0.6292, train acc: 0.7644, val loss: 0.4668, val acc: 0.8762  (best train acc: 0.7822, best val acc: 0.8941, best train loss: 0.5980  @ epoch 13685 )\n",
      "[Epoch: 13940] train loss: 0.6233, train acc: 0.7655, val loss: 0.4609, val acc: 0.8728  (best train acc: 0.7822, best val acc: 0.8941, best train loss: 0.5980  @ epoch 13685 )\n",
      "[Epoch: 13960] train loss: 0.6199, train acc: 0.7753, val loss: 0.4618, val acc: 0.8793  (best train acc: 0.7822, best val acc: 0.8941, best train loss: 0.5980  @ epoch 13685 )\n",
      "[Epoch: 13980] train loss: 0.6285, train acc: 0.7735, val loss: 0.4550, val acc: 0.8722  (best train acc: 0.7822, best val acc: 0.8941, best train loss: 0.5980  @ epoch 13685 )\n",
      "[Epoch: 14000] train loss: 0.6241, train acc: 0.7731, val loss: 0.4622, val acc: 0.8823  (best train acc: 0.7822, best val acc: 0.8941, best train loss: 0.5980  @ epoch 13685 )\n",
      "[Epoch: 14020] train loss: 0.6353, train acc: 0.7579, val loss: 0.4740, val acc: 0.8607  (best train acc: 0.7822, best val acc: 0.8941, best train loss: 0.5980  @ epoch 13685 )\n",
      "[Epoch: 14040] train loss: 0.6416, train acc: 0.7497, val loss: 0.4763, val acc: 0.8479  (best train acc: 0.7822, best val acc: 0.8941, best train loss: 0.5980  @ epoch 13685 )\n",
      "[Epoch: 14060] train loss: 0.6315, train acc: 0.7624, val loss: 0.4767, val acc: 0.8678  (best train acc: 0.7822, best val acc: 0.8941, best train loss: 0.5980  @ epoch 13685 )\n",
      "[Epoch: 14080] train loss: 0.6559, train acc: 0.7569, val loss: 0.5061, val acc: 0.8465  (best train acc: 0.7822, best val acc: 0.8941, best train loss: 0.5980  @ epoch 13685 )\n",
      "[Epoch: 14100] train loss: 0.6354, train acc: 0.7663, val loss: 0.4680, val acc: 0.8857  (best train acc: 0.7822, best val acc: 0.8941, best train loss: 0.5980  @ epoch 13685 )\n",
      "[Epoch: 14120] train loss: 0.6484, train acc: 0.7605, val loss: 0.4995, val acc: 0.8408  (best train acc: 0.7822, best val acc: 0.8941, best train loss: 0.5980  @ epoch 13685 )\n",
      "[Epoch: 14140] train loss: 0.6239, train acc: 0.7599, val loss: 0.4799, val acc: 0.8631  (best train acc: 0.7822, best val acc: 0.8941, best train loss: 0.5980  @ epoch 13685 )\n",
      "[Epoch: 14160] train loss: 0.6447, train acc: 0.7504, val loss: 0.4795, val acc: 0.8651  (best train acc: 0.7822, best val acc: 0.8941, best train loss: 0.5980  @ epoch 13685 )\n",
      "[Epoch: 14180] train loss: 0.6411, train acc: 0.7611, val loss: 0.4834, val acc: 0.8449  (best train acc: 0.7822, best val acc: 0.8941, best train loss: 0.5980  @ epoch 13685 )\n",
      "[Epoch: 14200] train loss: 0.6421, train acc: 0.7625, val loss: 0.5073, val acc: 0.8408  (best train acc: 0.7822, best val acc: 0.8941, best train loss: 0.5980  @ epoch 13685 )\n",
      "[Epoch: 14220] train loss: 0.6564, train acc: 0.7364, val loss: 0.5014, val acc: 0.8418  (best train acc: 0.7822, best val acc: 0.8941, best train loss: 0.5980  @ epoch 13685 )\n",
      "[Epoch: 14240] train loss: 0.6378, train acc: 0.7657, val loss: 0.4667, val acc: 0.8769  (best train acc: 0.7822, best val acc: 0.8941, best train loss: 0.5980  @ epoch 13685 )\n",
      "[Epoch: 14260] train loss: 0.6127, train acc: 0.7693, val loss: 0.4599, val acc: 0.8850  (best train acc: 0.7822, best val acc: 0.8941, best train loss: 0.5980  @ epoch 13685 )\n",
      "[Epoch: 14280] train loss: 0.6456, train acc: 0.7486, val loss: 0.4878, val acc: 0.8644  (best train acc: 0.7822, best val acc: 0.8941, best train loss: 0.5980  @ epoch 13685 )\n",
      "[Epoch: 14300] train loss: 0.6227, train acc: 0.7698, val loss: 0.4686, val acc: 0.8661  (best train acc: 0.7822, best val acc: 0.8941, best train loss: 0.5980  @ epoch 13685 )\n",
      "[Epoch: 14320] train loss: 0.6216, train acc: 0.7712, val loss: 0.4552, val acc: 0.8863  (best train acc: 0.7822, best val acc: 0.8941, best train loss: 0.5980  @ epoch 13685 )\n",
      "[Epoch: 14340] train loss: 0.6242, train acc: 0.7655, val loss: 0.4631, val acc: 0.8681  (best train acc: 0.7822, best val acc: 0.8941, best train loss: 0.5980  @ epoch 13685 )\n",
      "[Epoch: 14360] train loss: 0.6293, train acc: 0.7705, val loss: 0.4512, val acc: 0.8911  (best train acc: 0.7833, best val acc: 0.8941, best train loss: 0.5980  @ epoch 13685 )\n",
      "[Epoch: 14380] train loss: 0.6345, train acc: 0.7625, val loss: 0.4563, val acc: 0.8766  (best train acc: 0.7833, best val acc: 0.8941, best train loss: 0.5980  @ epoch 13685 )\n",
      "[Epoch: 14400] train loss: 0.6199, train acc: 0.7801, val loss: 0.4559, val acc: 0.8816  (best train acc: 0.7833, best val acc: 0.8941, best train loss: 0.5980  @ epoch 13685 )\n",
      "[Epoch: 14420] train loss: 0.6478, train acc: 0.7425, val loss: 0.4728, val acc: 0.8499  (best train acc: 0.7833, best val acc: 0.8941, best train loss: 0.5980  @ epoch 13685 )\n",
      "[Epoch: 14440] train loss: 0.6120, train acc: 0.7715, val loss: 0.5195, val acc: 0.8277  (best train acc: 0.7833, best val acc: 0.8941, best train loss: 0.5980  @ epoch 13685 )\n",
      "[Epoch: 14460] train loss: 0.6388, train acc: 0.7585, val loss: 0.4734, val acc: 0.8590  (best train acc: 0.7833, best val acc: 0.8941, best train loss: 0.5980  @ epoch 13685 )\n",
      "[Epoch: 14480] train loss: 0.6112, train acc: 0.7715, val loss: 0.4725, val acc: 0.8418  (best train acc: 0.7833, best val acc: 0.8941, best train loss: 0.5980  @ epoch 13685 )\n",
      "[Epoch: 14500] train loss: 0.6181, train acc: 0.7654, val loss: 0.4576, val acc: 0.8762  (best train acc: 0.7833, best val acc: 0.8941, best train loss: 0.5980  @ epoch 13685 )\n",
      "[Epoch: 14520] train loss: 0.6326, train acc: 0.7734, val loss: 0.4660, val acc: 0.8735  (best train acc: 0.7833, best val acc: 0.8941, best train loss: 0.5980  @ epoch 13685 )\n",
      "[Epoch: 14540] train loss: 0.6246, train acc: 0.7744, val loss: 0.4614, val acc: 0.8860  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5980  @ epoch 13685 )\n",
      "[Epoch: 14560] train loss: 0.6408, train acc: 0.7606, val loss: 0.4815, val acc: 0.8556  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5961  @ epoch 14544 )\n",
      "[Epoch: 14580] train loss: 0.6290, train acc: 0.7681, val loss: 0.4652, val acc: 0.8735  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5961  @ epoch 14544 )\n",
      "[Epoch: 14600] train loss: 0.6365, train acc: 0.7667, val loss: 0.5406, val acc: 0.8246  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5961  @ epoch 14544 )\n",
      "[Epoch: 14620] train loss: 0.6264, train acc: 0.7580, val loss: 0.4618, val acc: 0.8509  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5961  @ epoch 14544 )\n",
      "[Epoch: 14640] train loss: 0.6227, train acc: 0.7703, val loss: 0.4533, val acc: 0.8772  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5961  @ epoch 14544 )\n",
      "[Epoch: 14660] train loss: 0.6252, train acc: 0.7660, val loss: 0.4513, val acc: 0.8715  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5961  @ epoch 14544 )\n",
      "[Epoch: 14680] train loss: 0.6498, train acc: 0.7545, val loss: 0.4613, val acc: 0.8627  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5961  @ epoch 14544 )\n",
      "[Epoch: 14700] train loss: 0.6337, train acc: 0.7516, val loss: 0.4850, val acc: 0.8624  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5961  @ epoch 14544 )\n",
      "[Epoch: 14720] train loss: 0.6586, train acc: 0.7606, val loss: 0.4730, val acc: 0.8398  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5961  @ epoch 14544 )\n",
      "[Epoch: 14740] train loss: 0.6385, train acc: 0.7659, val loss: 0.4493, val acc: 0.8850  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5961  @ epoch 14544 )\n",
      "[Epoch: 14760] train loss: 0.6124, train acc: 0.7749, val loss: 0.4550, val acc: 0.8661  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5961  @ epoch 14544 )\n",
      "[Epoch: 14780] train loss: 0.6239, train acc: 0.7680, val loss: 0.4573, val acc: 0.8826  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5961  @ epoch 14544 )\n",
      "[Epoch: 14800] train loss: 0.6426, train acc: 0.7541, val loss: 0.4567, val acc: 0.8695  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5961  @ epoch 14544 )\n",
      "[Epoch: 14820] train loss: 0.6416, train acc: 0.7679, val loss: 0.4767, val acc: 0.8644  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5961  @ epoch 14544 )\n",
      "[Epoch: 14840] train loss: 0.6345, train acc: 0.7645, val loss: 0.4764, val acc: 0.8793  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5961  @ epoch 14544 )\n",
      "[Epoch: 14860] train loss: 0.6327, train acc: 0.7725, val loss: 0.4655, val acc: 0.8641  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5961  @ epoch 14544 )\n",
      "[Epoch: 14880] train loss: 0.6325, train acc: 0.7720, val loss: 0.4515, val acc: 0.8803  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5961  @ epoch 14544 )\n",
      "[Epoch: 14900] train loss: 0.6271, train acc: 0.7693, val loss: 0.4568, val acc: 0.8830  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5961  @ epoch 14544 )\n",
      "[Epoch: 14920] train loss: 0.6088, train acc: 0.7734, val loss: 0.4570, val acc: 0.8847  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5961  @ epoch 14544 )\n",
      "[Epoch: 14940] train loss: 0.6306, train acc: 0.7635, val loss: 0.4630, val acc: 0.8732  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5961  @ epoch 14544 )\n",
      "[Epoch: 14960] train loss: 0.6294, train acc: 0.7702, val loss: 0.4612, val acc: 0.8715  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5961  @ epoch 14544 )\n",
      "[Epoch: 14980] train loss: 0.6144, train acc: 0.7789, val loss: 0.4553, val acc: 0.8789  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5961  @ epoch 14544 )\n",
      "[Epoch: 15000] train loss: 0.6134, train acc: 0.7687, val loss: 0.4540, val acc: 0.8833  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5961  @ epoch 14544 )\n",
      "[Epoch: 15020] train loss: 0.6254, train acc: 0.7778, val loss: 0.4525, val acc: 0.8809  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5961  @ epoch 14544 )\n",
      "[Epoch: 15040] train loss: 0.6189, train acc: 0.7697, val loss: 0.4699, val acc: 0.8637  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5961  @ epoch 14544 )\n",
      "[Epoch: 15060] train loss: 0.6902, train acc: 0.7263, val loss: 0.4853, val acc: 0.8209  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5961  @ epoch 14544 )\n",
      "[Epoch: 15080] train loss: 0.6264, train acc: 0.7585, val loss: 0.4613, val acc: 0.8560  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5961  @ epoch 14544 )\n",
      "[Epoch: 15100] train loss: 0.6210, train acc: 0.7784, val loss: 0.4475, val acc: 0.8668  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5961  @ epoch 14544 )\n",
      "[Epoch: 15120] train loss: 0.6446, train acc: 0.7415, val loss: 0.4775, val acc: 0.8570  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5961  @ epoch 14544 )\n",
      "[Epoch: 15140] train loss: 0.6254, train acc: 0.7676, val loss: 0.4748, val acc: 0.8543  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5961  @ epoch 14544 )\n",
      "[Epoch: 15160] train loss: 0.6184, train acc: 0.7700, val loss: 0.4652, val acc: 0.8631  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5961  @ epoch 14544 )\n",
      "[Epoch: 15180] train loss: 0.6260, train acc: 0.7592, val loss: 0.4650, val acc: 0.8725  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5961  @ epoch 14544 )\n",
      "[Epoch: 15200] train loss: 0.6413, train acc: 0.7388, val loss: 0.4922, val acc: 0.8384  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5961  @ epoch 14544 )\n",
      "[Epoch: 15220] train loss: 0.6604, train acc: 0.7408, val loss: 0.5259, val acc: 0.8084  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5961  @ epoch 14544 )\n",
      "[Epoch: 15240] train loss: 0.6180, train acc: 0.7707, val loss: 0.4634, val acc: 0.8728  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5961  @ epoch 14544 )\n",
      "[Epoch: 15260] train loss: 0.6354, train acc: 0.7673, val loss: 0.4524, val acc: 0.8830  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5961  @ epoch 14544 )\n",
      "[Epoch: 15280] train loss: 0.6091, train acc: 0.7768, val loss: 0.4560, val acc: 0.8644  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5961  @ epoch 14544 )\n",
      "[Epoch: 15300] train loss: 0.6144, train acc: 0.7741, val loss: 0.4682, val acc: 0.8668  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5961  @ epoch 14544 )\n",
      "[Epoch: 15320] train loss: 0.6515, train acc: 0.7500, val loss: 0.4771, val acc: 0.8364  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5961  @ epoch 14544 )\n",
      "[Epoch: 15340] train loss: 0.6380, train acc: 0.7513, val loss: 0.4824, val acc: 0.8341  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5961  @ epoch 14544 )\n",
      "[Epoch: 15360] train loss: 0.6214, train acc: 0.7616, val loss: 0.4584, val acc: 0.8749  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5961  @ epoch 14544 )\n",
      "[Epoch: 15380] train loss: 0.6280, train acc: 0.7505, val loss: 0.4559, val acc: 0.8786  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5961  @ epoch 14544 )\n",
      "[Epoch: 15400] train loss: 0.6239, train acc: 0.7537, val loss: 0.4813, val acc: 0.8486  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5961  @ epoch 14544 )\n",
      "[Epoch: 15420] train loss: 0.6320, train acc: 0.7585, val loss: 0.4650, val acc: 0.8833  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5961  @ epoch 14544 )\n",
      "[Epoch: 15440] train loss: 0.6270, train acc: 0.7611, val loss: 0.4578, val acc: 0.8465  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5961  @ epoch 14544 )\n",
      "[Epoch: 15460] train loss: 0.6142, train acc: 0.7637, val loss: 0.4671, val acc: 0.8604  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5961  @ epoch 14544 )\n",
      "[Epoch: 15480] train loss: 0.6248, train acc: 0.7728, val loss: 0.4529, val acc: 0.8847  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5961  @ epoch 14544 )\n",
      "[Epoch: 15500] train loss: 0.6097, train acc: 0.7785, val loss: 0.4471, val acc: 0.8681  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5961  @ epoch 14544 )\n",
      "[Epoch: 15520] train loss: 0.6254, train acc: 0.7704, val loss: 0.4589, val acc: 0.8745  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5961  @ epoch 14544 )\n",
      "[Epoch: 15540] train loss: 0.6276, train acc: 0.7608, val loss: 0.4510, val acc: 0.8718  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5961  @ epoch 14544 )\n",
      "[Epoch: 15560] train loss: 0.6202, train acc: 0.7717, val loss: 0.4487, val acc: 0.8600  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5961  @ epoch 14544 )\n",
      "[Epoch: 15580] train loss: 0.6312, train acc: 0.7700, val loss: 0.4496, val acc: 0.8816  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5961  @ epoch 14544 )\n",
      "[Epoch: 15600] train loss: 0.6356, train acc: 0.7662, val loss: 0.4532, val acc: 0.8806  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5961  @ epoch 14544 )\n",
      "[Epoch: 15620] train loss: 0.6119, train acc: 0.7745, val loss: 0.4436, val acc: 0.8776  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5961  @ epoch 14544 )\n",
      "[Epoch: 15640] train loss: 0.6226, train acc: 0.7648, val loss: 0.4557, val acc: 0.8671  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5961  @ epoch 14544 )\n",
      "[Epoch: 15660] train loss: 0.6367, train acc: 0.7681, val loss: 0.4526, val acc: 0.8621  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5961  @ epoch 14544 )\n",
      "[Epoch: 15680] train loss: 0.6286, train acc: 0.7715, val loss: 0.4639, val acc: 0.8728  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5961  @ epoch 14544 )\n",
      "[Epoch: 15700] train loss: 0.6358, train acc: 0.7537, val loss: 0.4618, val acc: 0.8658  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5961  @ epoch 14544 )\n",
      "[Epoch: 15720] train loss: 0.6271, train acc: 0.7652, val loss: 0.4478, val acc: 0.8631  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5961  @ epoch 14544 )\n",
      "[Epoch: 15740] train loss: 0.6216, train acc: 0.7767, val loss: 0.4550, val acc: 0.8762  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5961  @ epoch 14544 )\n",
      "[Epoch: 15760] train loss: 0.6371, train acc: 0.7645, val loss: 0.4654, val acc: 0.8489  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5961  @ epoch 14544 )\n",
      "[Epoch: 15780] train loss: 0.6283, train acc: 0.7613, val loss: 0.4932, val acc: 0.8519  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5961  @ epoch 14544 )\n",
      "[Epoch: 15800] train loss: 0.6242, train acc: 0.7667, val loss: 0.4743, val acc: 0.8570  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5961  @ epoch 14544 )\n",
      "[Epoch: 15820] train loss: 0.6300, train acc: 0.7628, val loss: 0.4603, val acc: 0.8725  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5961  @ epoch 14544 )\n",
      "[Epoch: 15840] train loss: 0.6542, train acc: 0.7433, val loss: 0.4793, val acc: 0.8496  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5961  @ epoch 14544 )\n",
      "[Epoch: 15860] train loss: 0.6301, train acc: 0.7612, val loss: 0.4540, val acc: 0.8722  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5961  @ epoch 14544 )\n",
      "[Epoch: 15880] train loss: 0.6206, train acc: 0.7770, val loss: 0.4604, val acc: 0.8651  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5961  @ epoch 14544 )\n",
      "[Epoch: 15900] train loss: 0.6437, train acc: 0.7570, val loss: 0.4449, val acc: 0.8907  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5933  @ epoch 15894 )\n",
      "[Epoch: 15920] train loss: 0.6721, train acc: 0.7240, val loss: 0.4769, val acc: 0.8435  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5933  @ epoch 15894 )\n",
      "[Epoch: 15940] train loss: 0.6257, train acc: 0.7672, val loss: 0.4488, val acc: 0.8577  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5933  @ epoch 15894 )\n",
      "[Epoch: 15960] train loss: 0.6051, train acc: 0.7725, val loss: 0.4488, val acc: 0.8610  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5933  @ epoch 15894 )\n",
      "[Epoch: 15980] train loss: 0.6151, train acc: 0.7666, val loss: 0.4526, val acc: 0.8705  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5933  @ epoch 15894 )\n",
      "[Epoch: 16000] train loss: 0.6370, train acc: 0.7629, val loss: 0.4588, val acc: 0.8718  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5933  @ epoch 15894 )\n",
      "[Epoch: 16020] train loss: 0.6132, train acc: 0.7801, val loss: 0.4608, val acc: 0.8745  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5933  @ epoch 15894 )\n",
      "[Epoch: 16040] train loss: 0.6098, train acc: 0.7780, val loss: 0.4459, val acc: 0.8789  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5933  @ epoch 15894 )\n",
      "[Epoch: 16060] train loss: 0.6333, train acc: 0.7640, val loss: 0.4639, val acc: 0.8614  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5933  @ epoch 15894 )\n",
      "[Epoch: 16080] train loss: 0.6725, train acc: 0.7394, val loss: 0.5224, val acc: 0.8121  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5896  @ epoch 16061 )\n",
      "[Epoch: 16100] train loss: 0.6217, train acc: 0.7604, val loss: 0.4664, val acc: 0.8658  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5896  @ epoch 16061 )\n",
      "[Epoch: 16120] train loss: 0.6453, train acc: 0.7576, val loss: 0.4641, val acc: 0.8799  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5896  @ epoch 16061 )\n",
      "[Epoch: 16140] train loss: 0.6147, train acc: 0.7714, val loss: 0.4427, val acc: 0.8867  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5896  @ epoch 16061 )\n",
      "[Epoch: 16160] train loss: 0.6264, train acc: 0.7580, val loss: 0.4601, val acc: 0.8664  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5896  @ epoch 16061 )\n",
      "[Epoch: 16180] train loss: 0.6089, train acc: 0.7704, val loss: 0.4527, val acc: 0.8762  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5896  @ epoch 16061 )\n",
      "[Epoch: 16200] train loss: 0.6236, train acc: 0.7598, val loss: 0.4631, val acc: 0.8725  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5896  @ epoch 16061 )\n",
      "[Epoch: 16220] train loss: 0.6090, train acc: 0.7757, val loss: 0.4513, val acc: 0.8725  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5896  @ epoch 16061 )\n",
      "[Epoch: 16240] train loss: 0.6228, train acc: 0.7618, val loss: 0.4548, val acc: 0.8664  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5896  @ epoch 16061 )\n",
      "[Epoch: 16260] train loss: 0.6185, train acc: 0.7667, val loss: 0.4637, val acc: 0.8587  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5896  @ epoch 16061 )\n",
      "[Epoch: 16280] train loss: 0.6321, train acc: 0.7684, val loss: 0.4530, val acc: 0.8776  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5896  @ epoch 16061 )\n",
      "[Epoch: 16300] train loss: 0.6329, train acc: 0.7590, val loss: 0.4729, val acc: 0.8364  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5896  @ epoch 16061 )\n",
      "[Epoch: 16320] train loss: 0.5945, train acc: 0.7739, val loss: 0.4501, val acc: 0.8796  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5896  @ epoch 16061 )\n",
      "[Epoch: 16340] train loss: 0.6135, train acc: 0.7726, val loss: 0.4501, val acc: 0.8860  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5896  @ epoch 16061 )\n",
      "[Epoch: 16360] train loss: 0.6267, train acc: 0.7680, val loss: 0.4488, val acc: 0.8712  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5896  @ epoch 16061 )\n",
      "[Epoch: 16380] train loss: 0.6190, train acc: 0.7761, val loss: 0.4444, val acc: 0.8830  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5896  @ epoch 16061 )\n",
      "[Epoch: 16400] train loss: 0.6212, train acc: 0.7712, val loss: 0.4669, val acc: 0.8651  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5896  @ epoch 16061 )\n",
      "[Epoch: 16420] train loss: 0.6331, train acc: 0.7729, val loss: 0.4520, val acc: 0.8590  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5896  @ epoch 16061 )\n",
      "[Epoch: 16440] train loss: 0.6113, train acc: 0.7686, val loss: 0.4456, val acc: 0.8745  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5896  @ epoch 16061 )\n",
      "[Epoch: 16460] train loss: 0.6165, train acc: 0.7644, val loss: 0.4440, val acc: 0.8847  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5896  @ epoch 16061 )\n",
      "[Epoch: 16480] train loss: 0.6170, train acc: 0.7687, val loss: 0.4480, val acc: 0.8546  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5896  @ epoch 16061 )\n",
      "[Epoch: 16500] train loss: 0.6325, train acc: 0.7493, val loss: 0.4709, val acc: 0.8739  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5896  @ epoch 16061 )\n",
      "[Epoch: 16520] train loss: 0.6384, train acc: 0.7637, val loss: 0.4527, val acc: 0.8546  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5896  @ epoch 16061 )\n",
      "[Epoch: 16540] train loss: 0.6354, train acc: 0.7632, val loss: 0.4606, val acc: 0.8705  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5896  @ epoch 16061 )\n",
      "[Epoch: 16560] train loss: 0.6216, train acc: 0.7735, val loss: 0.4460, val acc: 0.8806  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5896  @ epoch 16061 )\n",
      "[Epoch: 16580] train loss: 0.6029, train acc: 0.7713, val loss: 0.4756, val acc: 0.8523  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5896  @ epoch 16061 )\n",
      "[Epoch: 16600] train loss: 0.6336, train acc: 0.7622, val loss: 0.5080, val acc: 0.8219  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5896  @ epoch 16061 )\n",
      "[Epoch: 16620] train loss: 0.6420, train acc: 0.7533, val loss: 0.4626, val acc: 0.8637  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5896  @ epoch 16061 )\n",
      "[Epoch: 16640] train loss: 0.6329, train acc: 0.7633, val loss: 0.4523, val acc: 0.8691  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5896  @ epoch 16061 )\n",
      "[Epoch: 16660] train loss: 0.6272, train acc: 0.7668, val loss: 0.4580, val acc: 0.8759  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5896  @ epoch 16061 )\n",
      "[Epoch: 16680] train loss: 0.6211, train acc: 0.7713, val loss: 0.4451, val acc: 0.8796  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5896  @ epoch 16061 )\n",
      "[Epoch: 16700] train loss: 0.6046, train acc: 0.7727, val loss: 0.4525, val acc: 0.8705  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5896  @ epoch 16061 )\n",
      "[Epoch: 16720] train loss: 0.6373, train acc: 0.7660, val loss: 0.4629, val acc: 0.8826  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5896  @ epoch 16061 )\n",
      "[Epoch: 16740] train loss: 0.6205, train acc: 0.7755, val loss: 0.4474, val acc: 0.8840  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5896  @ epoch 16061 )\n",
      "[Epoch: 16760] train loss: 0.6269, train acc: 0.7606, val loss: 0.4696, val acc: 0.8624  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5896  @ epoch 16061 )\n",
      "[Epoch: 16780] train loss: 0.6113, train acc: 0.7760, val loss: 0.4615, val acc: 0.8624  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5896  @ epoch 16061 )\n",
      "[Epoch: 16800] train loss: 0.6095, train acc: 0.7762, val loss: 0.4551, val acc: 0.8782  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5896  @ epoch 16061 )\n",
      "[Epoch: 16820] train loss: 0.6285, train acc: 0.7660, val loss: 0.4478, val acc: 0.8799  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5896  @ epoch 16061 )\n",
      "[Epoch: 16840] train loss: 0.6353, train acc: 0.7596, val loss: 0.4480, val acc: 0.8745  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5896  @ epoch 16061 )\n",
      "[Epoch: 16860] train loss: 0.6416, train acc: 0.7655, val loss: 0.4735, val acc: 0.8614  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5896  @ epoch 16061 )\n",
      "[Epoch: 16880] train loss: 0.6234, train acc: 0.7592, val loss: 0.4489, val acc: 0.8654  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5896  @ epoch 16061 )\n",
      "[Epoch: 16900] train loss: 0.6030, train acc: 0.7755, val loss: 0.4556, val acc: 0.8597  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5896  @ epoch 16061 )\n",
      "[Epoch: 16920] train loss: 0.6210, train acc: 0.7723, val loss: 0.4679, val acc: 0.8449  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5896  @ epoch 16061 )\n",
      "[Epoch: 16940] train loss: 0.6080, train acc: 0.7759, val loss: 0.4569, val acc: 0.8661  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5896  @ epoch 16061 )\n",
      "[Epoch: 16960] train loss: 0.6136, train acc: 0.7711, val loss: 0.4618, val acc: 0.8766  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5896  @ epoch 16061 )\n",
      "[Epoch: 16980] train loss: 0.6118, train acc: 0.7801, val loss: 0.4490, val acc: 0.8732  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5896  @ epoch 16061 )\n",
      "[Epoch: 17000] train loss: 0.6149, train acc: 0.7771, val loss: 0.4512, val acc: 0.8695  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5896  @ epoch 16061 )\n",
      "[Epoch: 17020] train loss: 0.5978, train acc: 0.7796, val loss: 0.4524, val acc: 0.8762  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5896  @ epoch 16061 )\n",
      "[Epoch: 17040] train loss: 0.6524, train acc: 0.7284, val loss: 0.4751, val acc: 0.8658  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5896  @ epoch 16061 )\n",
      "[Epoch: 17060] train loss: 0.6203, train acc: 0.7605, val loss: 0.4641, val acc: 0.8637  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5896  @ epoch 16061 )\n",
      "[Epoch: 17080] train loss: 0.6192, train acc: 0.7663, val loss: 0.4875, val acc: 0.8212  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5896  @ epoch 16061 )\n",
      "[Epoch: 17100] train loss: 0.6381, train acc: 0.7627, val loss: 0.4636, val acc: 0.8610  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5896  @ epoch 16061 )\n",
      "[Epoch: 17120] train loss: 0.6517, train acc: 0.7640, val loss: 0.4559, val acc: 0.8556  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5896  @ epoch 16061 )\n",
      "[Epoch: 17140] train loss: 0.6290, train acc: 0.7601, val loss: 0.4932, val acc: 0.8347  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5896  @ epoch 16061 )\n",
      "[Epoch: 17160] train loss: 0.6299, train acc: 0.7605, val loss: 0.4439, val acc: 0.8695  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5896  @ epoch 16061 )\n",
      "[Epoch: 17180] train loss: 0.6159, train acc: 0.7692, val loss: 0.4634, val acc: 0.8594  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5896  @ epoch 16061 )\n",
      "[Epoch: 17200] train loss: 0.6111, train acc: 0.7703, val loss: 0.4494, val acc: 0.8772  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5896  @ epoch 16061 )\n",
      "[Epoch: 17220] train loss: 0.6075, train acc: 0.7723, val loss: 0.4496, val acc: 0.8675  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5896  @ epoch 16061 )\n",
      "[Epoch: 17240] train loss: 0.6193, train acc: 0.7708, val loss: 0.4530, val acc: 0.8614  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5896  @ epoch 16061 )\n",
      "[Epoch: 17260] train loss: 0.6189, train acc: 0.7711, val loss: 0.4487, val acc: 0.8732  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5896  @ epoch 16061 )\n",
      "[Epoch: 17280] train loss: 0.6178, train acc: 0.7721, val loss: 0.4465, val acc: 0.8695  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5896  @ epoch 16061 )\n",
      "[Epoch: 17300] train loss: 0.6109, train acc: 0.7732, val loss: 0.4547, val acc: 0.8678  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5896  @ epoch 16061 )\n",
      "[Epoch: 17320] train loss: 0.6041, train acc: 0.7725, val loss: 0.4462, val acc: 0.8853  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5896  @ epoch 16061 )\n",
      "[Epoch: 17340] train loss: 0.6443, train acc: 0.7407, val loss: 0.4618, val acc: 0.8570  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5896  @ epoch 16061 )\n",
      "[Epoch: 17360] train loss: 0.6278, train acc: 0.7657, val loss: 0.4580, val acc: 0.8637  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5896  @ epoch 16061 )\n",
      "[Epoch: 17380] train loss: 0.6235, train acc: 0.7668, val loss: 0.4443, val acc: 0.8594  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5896  @ epoch 16061 )\n",
      "[Epoch: 17400] train loss: 0.6134, train acc: 0.7728, val loss: 0.4420, val acc: 0.8749  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5896  @ epoch 16061 )\n",
      "[Epoch: 17420] train loss: 0.6223, train acc: 0.7717, val loss: 0.4600, val acc: 0.8776  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5896  @ epoch 16061 )\n",
      "[Epoch: 17440] train loss: 0.6167, train acc: 0.7671, val loss: 0.4758, val acc: 0.8435  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5896  @ epoch 16061 )\n",
      "[Epoch: 17460] train loss: 0.6113, train acc: 0.7694, val loss: 0.4845, val acc: 0.8368  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5896  @ epoch 16061 )\n",
      "[Epoch: 17480] train loss: 0.6284, train acc: 0.7620, val loss: 0.4625, val acc: 0.8597  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5896  @ epoch 16061 )\n",
      "[Epoch: 17500] train loss: 0.6222, train acc: 0.7645, val loss: 0.4555, val acc: 0.8728  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5896  @ epoch 16061 )\n",
      "[Epoch: 17520] train loss: 0.6032, train acc: 0.7660, val loss: 0.4864, val acc: 0.8425  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5896  @ epoch 16061 )\n",
      "[Epoch: 17540] train loss: 0.6038, train acc: 0.7718, val loss: 0.4498, val acc: 0.8732  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5896  @ epoch 16061 )\n",
      "[Epoch: 17560] train loss: 0.6180, train acc: 0.7761, val loss: 0.4481, val acc: 0.8661  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5896  @ epoch 16061 )\n",
      "[Epoch: 17580] train loss: 0.6405, train acc: 0.7721, val loss: 0.4484, val acc: 0.8675  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5896  @ epoch 16061 )\n",
      "[Epoch: 17600] train loss: 0.6232, train acc: 0.7637, val loss: 0.4487, val acc: 0.8675  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5896  @ epoch 16061 )\n",
      "[Epoch: 17620] train loss: 0.6171, train acc: 0.7723, val loss: 0.4358, val acc: 0.8880  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5896  @ epoch 16061 )\n",
      "[Epoch: 17640] train loss: 0.6194, train acc: 0.7694, val loss: 0.4457, val acc: 0.8715  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5896  @ epoch 16061 )\n",
      "[Epoch: 17660] train loss: 0.6250, train acc: 0.7715, val loss: 0.4636, val acc: 0.8513  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5896  @ epoch 16061 )\n",
      "[Epoch: 17680] train loss: 0.6149, train acc: 0.7613, val loss: 0.4509, val acc: 0.8735  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5896  @ epoch 16061 )\n",
      "[Epoch: 17700] train loss: 0.6129, train acc: 0.7648, val loss: 0.4550, val acc: 0.8735  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5896  @ epoch 16061 )\n",
      "[Epoch: 17720] train loss: 0.6239, train acc: 0.7736, val loss: 0.4438, val acc: 0.8728  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5896  @ epoch 16061 )\n",
      "[Epoch: 17740] train loss: 0.6371, train acc: 0.7668, val loss: 0.4584, val acc: 0.8675  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5896  @ epoch 16061 )\n",
      "[Epoch: 17760] train loss: 0.6170, train acc: 0.7665, val loss: 0.4548, val acc: 0.8597  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5896  @ epoch 16061 )\n",
      "[Epoch: 17780] train loss: 0.6074, train acc: 0.7705, val loss: 0.4373, val acc: 0.8759  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5896  @ epoch 16061 )\n",
      "[Epoch: 17800] train loss: 0.6235, train acc: 0.7681, val loss: 0.4921, val acc: 0.8175  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5896  @ epoch 16061 )\n",
      "[Epoch: 17820] train loss: 0.6370, train acc: 0.7651, val loss: 0.4698, val acc: 0.8624  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5896  @ epoch 16061 )\n",
      "[Epoch: 17840] train loss: 0.6230, train acc: 0.7668, val loss: 0.4552, val acc: 0.8486  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5896  @ epoch 16061 )\n",
      "[Epoch: 17860] train loss: 0.6022, train acc: 0.7707, val loss: 0.4703, val acc: 0.8462  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5896  @ epoch 16061 )\n",
      "[Epoch: 17880] train loss: 0.6322, train acc: 0.7658, val loss: 0.4436, val acc: 0.8712  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5896  @ epoch 16061 )\n",
      "[Epoch: 17900] train loss: 0.5978, train acc: 0.7830, val loss: 0.4330, val acc: 0.8836  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5896  @ epoch 16061 )\n",
      "[Epoch: 17920] train loss: 0.6069, train acc: 0.7736, val loss: 0.4396, val acc: 0.8739  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5896  @ epoch 16061 )\n",
      "[Epoch: 17940] train loss: 0.6038, train acc: 0.7709, val loss: 0.4422, val acc: 0.8813  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5896  @ epoch 16061 )\n",
      "[Epoch: 17960] train loss: 0.6139, train acc: 0.7726, val loss: 0.4422, val acc: 0.8867  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5896  @ epoch 16061 )\n",
      "[Epoch: 17980] train loss: 0.6391, train acc: 0.7597, val loss: 0.4454, val acc: 0.8755  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5896  @ epoch 16061 )\n",
      "[Epoch: 18000] train loss: 0.6137, train acc: 0.7713, val loss: 0.4456, val acc: 0.8782  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5896  @ epoch 16061 )\n",
      "[Epoch: 18020] train loss: 0.6243, train acc: 0.7667, val loss: 0.4628, val acc: 0.8533  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5896  @ epoch 16061 )\n",
      "[Epoch: 18040] train loss: 0.6430, train acc: 0.7609, val loss: 0.4545, val acc: 0.8540  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5896  @ epoch 16061 )\n",
      "[Epoch: 18060] train loss: 0.6213, train acc: 0.7618, val loss: 0.4715, val acc: 0.8472  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5896  @ epoch 16061 )\n",
      "[Epoch: 18080] train loss: 0.6228, train acc: 0.7589, val loss: 0.4425, val acc: 0.8870  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5896  @ epoch 16061 )\n",
      "[Epoch: 18100] train loss: 0.6408, train acc: 0.7582, val loss: 0.4638, val acc: 0.8513  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5896  @ epoch 16061 )\n",
      "[Epoch: 18120] train loss: 0.6462, train acc: 0.7580, val loss: 0.4525, val acc: 0.8607  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5896  @ epoch 16061 )\n",
      "[Epoch: 18140] train loss: 0.6048, train acc: 0.7760, val loss: 0.4387, val acc: 0.8786  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5896  @ epoch 16061 )\n",
      "[Epoch: 18160] train loss: 0.6177, train acc: 0.7707, val loss: 0.4413, val acc: 0.8722  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5896  @ epoch 16061 )\n",
      "[Epoch: 18180] train loss: 0.6047, train acc: 0.7796, val loss: 0.4386, val acc: 0.8796  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5896  @ epoch 16061 )\n",
      "[Epoch: 18200] train loss: 0.6139, train acc: 0.7654, val loss: 0.4398, val acc: 0.8796  (best train acc: 0.7847, best val acc: 0.8941, best train loss: 0.5896  @ epoch 16061 )\n",
      "[Epoch: 18220] train loss: 0.6387, train acc: 0.7669, val loss: 0.4450, val acc: 0.8722  (best train acc: 0.7847, best val acc: 0.8958, best train loss: 0.5896  @ epoch 16061 )\n",
      "[Epoch: 18240] train loss: 0.6062, train acc: 0.7710, val loss: 0.4508, val acc: 0.8533  (best train acc: 0.7858, best val acc: 0.8958, best train loss: 0.5896  @ epoch 16061 )\n",
      "[Epoch: 18260] train loss: 0.6163, train acc: 0.7698, val loss: 0.4389, val acc: 0.8766  (best train acc: 0.7858, best val acc: 0.8958, best train loss: 0.5896  @ epoch 16061 )\n",
      "[Epoch: 18280] train loss: 0.6166, train acc: 0.7757, val loss: 0.4399, val acc: 0.8836  (best train acc: 0.7858, best val acc: 0.8958, best train loss: 0.5896  @ epoch 16061 )\n",
      "[Epoch: 18300] train loss: 0.6225, train acc: 0.7718, val loss: 0.4612, val acc: 0.8540  (best train acc: 0.7858, best val acc: 0.8958, best train loss: 0.5896  @ epoch 16061 )\n",
      "[Epoch: 18320] train loss: 0.6255, train acc: 0.7689, val loss: 0.4766, val acc: 0.8546  (best train acc: 0.7858, best val acc: 0.8958, best train loss: 0.5896  @ epoch 16061 )\n",
      "[Epoch: 18340] train loss: 0.6039, train acc: 0.7734, val loss: 0.4515, val acc: 0.8718  (best train acc: 0.7858, best val acc: 0.8958, best train loss: 0.5896  @ epoch 16061 )\n",
      "[Epoch: 18360] train loss: 0.6245, train acc: 0.7715, val loss: 0.4430, val acc: 0.8644  (best train acc: 0.7858, best val acc: 0.8958, best train loss: 0.5896  @ epoch 16061 )\n",
      "[Epoch: 18380] train loss: 0.6190, train acc: 0.7791, val loss: 0.4366, val acc: 0.8813  (best train acc: 0.7858, best val acc: 0.8958, best train loss: 0.5896  @ epoch 16061 )\n",
      "[Epoch: 18400] train loss: 0.6148, train acc: 0.7757, val loss: 0.4383, val acc: 0.8762  (best train acc: 0.7858, best val acc: 0.8958, best train loss: 0.5896  @ epoch 16061 )\n",
      "[Epoch: 18420] train loss: 0.6102, train acc: 0.7775, val loss: 0.4482, val acc: 0.8796  (best train acc: 0.7858, best val acc: 0.8958, best train loss: 0.5896  @ epoch 16061 )\n",
      "[Epoch: 18440] train loss: 0.6052, train acc: 0.7781, val loss: 0.4688, val acc: 0.8381  (best train acc: 0.7858, best val acc: 0.8958, best train loss: 0.5896  @ epoch 16061 )\n",
      "[Epoch: 18460] train loss: 0.6377, train acc: 0.7616, val loss: 0.4558, val acc: 0.8577  (best train acc: 0.7858, best val acc: 0.8958, best train loss: 0.5896  @ epoch 16061 )\n",
      "[Epoch: 18480] train loss: 0.6202, train acc: 0.7684, val loss: 0.4712, val acc: 0.8422  (best train acc: 0.7858, best val acc: 0.8958, best train loss: 0.5896  @ epoch 16061 )\n",
      "[Epoch: 18500] train loss: 0.6348, train acc: 0.7536, val loss: 0.4485, val acc: 0.8671  (best train acc: 0.7858, best val acc: 0.8958, best train loss: 0.5896  @ epoch 16061 )\n",
      "[Epoch: 18520] train loss: 0.6059, train acc: 0.7707, val loss: 0.4500, val acc: 0.8786  (best train acc: 0.7858, best val acc: 0.8958, best train loss: 0.5896  @ epoch 16061 )\n",
      "[Epoch: 18540] train loss: 0.6196, train acc: 0.7662, val loss: 0.4608, val acc: 0.8664  (best train acc: 0.7858, best val acc: 0.8958, best train loss: 0.5896  @ epoch 16061 )\n",
      "[Epoch: 18560] train loss: 0.6053, train acc: 0.7751, val loss: 0.4455, val acc: 0.8631  (best train acc: 0.7858, best val acc: 0.8958, best train loss: 0.5896  @ epoch 16061 )\n",
      "[Epoch: 18580] train loss: 0.7683, train acc: 0.7035, val loss: 0.5459, val acc: 0.7987  (best train acc: 0.7858, best val acc: 0.8958, best train loss: 0.5896  @ epoch 16061 )\n",
      "[Epoch: 18600] train loss: 0.6329, train acc: 0.7499, val loss: 0.4353, val acc: 0.8742  (best train acc: 0.7858, best val acc: 0.8958, best train loss: 0.5896  @ epoch 16061 )\n",
      "[Epoch: 18620] train loss: 0.6302, train acc: 0.7669, val loss: 0.4482, val acc: 0.8648  (best train acc: 0.7858, best val acc: 0.8958, best train loss: 0.5896  @ epoch 16061 )\n",
      "[Epoch: 18640] train loss: 0.6290, train acc: 0.7620, val loss: 0.4895, val acc: 0.8361  (best train acc: 0.7858, best val acc: 0.8958, best train loss: 0.5896  @ epoch 16061 )\n",
      "[Epoch: 18660] train loss: 0.6290, train acc: 0.7563, val loss: 0.4546, val acc: 0.8641  (best train acc: 0.7858, best val acc: 0.8958, best train loss: 0.5896  @ epoch 16061 )\n",
      "[Epoch: 18680] train loss: 0.6128, train acc: 0.7656, val loss: 0.4662, val acc: 0.8499  (best train acc: 0.7858, best val acc: 0.8958, best train loss: 0.5896  @ epoch 16061 )\n",
      "[Epoch: 18700] train loss: 0.6099, train acc: 0.7641, val loss: 0.4401, val acc: 0.8766  (best train acc: 0.7858, best val acc: 0.8958, best train loss: 0.5896  @ epoch 16061 )\n",
      "[Epoch: 18720] train loss: 0.6039, train acc: 0.7801, val loss: 0.4501, val acc: 0.8847  (best train acc: 0.7858, best val acc: 0.8958, best train loss: 0.5896  @ epoch 16061 )\n",
      "[Epoch: 18740] train loss: 0.5991, train acc: 0.7718, val loss: 0.4430, val acc: 0.8664  (best train acc: 0.7858, best val acc: 0.8958, best train loss: 0.5896  @ epoch 16061 )\n",
      "[Epoch: 18760] train loss: 0.6358, train acc: 0.7603, val loss: 0.4460, val acc: 0.8809  (best train acc: 0.7858, best val acc: 0.8958, best train loss: 0.5896  @ epoch 16061 )\n",
      "[Epoch: 18780] train loss: 0.6132, train acc: 0.7718, val loss: 0.4429, val acc: 0.8830  (best train acc: 0.7858, best val acc: 0.8958, best train loss: 0.5896  @ epoch 16061 )\n",
      "[Epoch: 18800] train loss: 0.5971, train acc: 0.7797, val loss: 0.4421, val acc: 0.8648  (best train acc: 0.7858, best val acc: 0.8958, best train loss: 0.5896  @ epoch 16061 )\n",
      "[Epoch: 18820] train loss: 0.6260, train acc: 0.7679, val loss: 0.4423, val acc: 0.8789  (best train acc: 0.7858, best val acc: 0.8958, best train loss: 0.5896  @ epoch 16061 )\n",
      "[Epoch: 18840] train loss: 0.6093, train acc: 0.7679, val loss: 0.4393, val acc: 0.8769  (best train acc: 0.7858, best val acc: 0.8958, best train loss: 0.5896  @ epoch 16061 )\n",
      "[Epoch: 18860] train loss: 0.6216, train acc: 0.7661, val loss: 0.4484, val acc: 0.8860  (best train acc: 0.7858, best val acc: 0.8958, best train loss: 0.5896  @ epoch 16061 )\n",
      "[Epoch: 18880] train loss: 0.6060, train acc: 0.7689, val loss: 0.4415, val acc: 0.8668  (best train acc: 0.7858, best val acc: 0.8958, best train loss: 0.5896  @ epoch 16061 )\n",
      "[Epoch: 18900] train loss: 0.6161, train acc: 0.7741, val loss: 0.4476, val acc: 0.8705  (best train acc: 0.7858, best val acc: 0.8958, best train loss: 0.5896  @ epoch 16061 )\n",
      "[Epoch: 18920] train loss: 0.6172, train acc: 0.7650, val loss: 0.4720, val acc: 0.8499  (best train acc: 0.7858, best val acc: 0.8958, best train loss: 0.5896  @ epoch 16061 )\n",
      "[Epoch: 18940] train loss: 0.6106, train acc: 0.7681, val loss: 0.4486, val acc: 0.8695  (best train acc: 0.7858, best val acc: 0.8958, best train loss: 0.5896  @ epoch 16061 )\n",
      "[Epoch: 18960] train loss: 0.6115, train acc: 0.7726, val loss: 0.4404, val acc: 0.8816  (best train acc: 0.7858, best val acc: 0.8958, best train loss: 0.5896  @ epoch 16061 )\n",
      "[Epoch: 18980] train loss: 0.6063, train acc: 0.7668, val loss: 0.4461, val acc: 0.8745  (best train acc: 0.7858, best val acc: 0.8958, best train loss: 0.5896  @ epoch 16061 )\n",
      "[Epoch: 19000] train loss: 0.6260, train acc: 0.7624, val loss: 0.4776, val acc: 0.8519  (best train acc: 0.7858, best val acc: 0.8958, best train loss: 0.5896  @ epoch 16061 )\n",
      "[Epoch: 19020] train loss: 0.6090, train acc: 0.7707, val loss: 0.4413, val acc: 0.8836  (best train acc: 0.7858, best val acc: 0.8958, best train loss: 0.5896  @ epoch 16061 )\n",
      "[Epoch: 19040] train loss: 0.6137, train acc: 0.7713, val loss: 0.4338, val acc: 0.8860  (best train acc: 0.7858, best val acc: 0.8958, best train loss: 0.5896  @ epoch 16061 )\n",
      "[Epoch: 19060] train loss: 0.6213, train acc: 0.7703, val loss: 0.4407, val acc: 0.8860  (best train acc: 0.7862, best val acc: 0.8958, best train loss: 0.5896  @ epoch 16061 )\n",
      "[Epoch: 19080] train loss: 0.6195, train acc: 0.7654, val loss: 0.4417, val acc: 0.8712  (best train acc: 0.7862, best val acc: 0.8958, best train loss: 0.5896  @ epoch 16061 )\n",
      "[Epoch: 19100] train loss: 0.6166, train acc: 0.7721, val loss: 0.4462, val acc: 0.8718  (best train acc: 0.7862, best val acc: 0.8958, best train loss: 0.5896  @ epoch 16061 )\n",
      "[Epoch: 19120] train loss: 0.6310, train acc: 0.7652, val loss: 0.4642, val acc: 0.8486  (best train acc: 0.7862, best val acc: 0.8958, best train loss: 0.5896  @ epoch 16061 )\n",
      "[Epoch: 19140] train loss: 0.6003, train acc: 0.7768, val loss: 0.4512, val acc: 0.8641  (best train acc: 0.7862, best val acc: 0.8958, best train loss: 0.5896  @ epoch 16061 )\n",
      "[Epoch: 19160] train loss: 0.5980, train acc: 0.7775, val loss: 0.4386, val acc: 0.8857  (best train acc: 0.7862, best val acc: 0.8958, best train loss: 0.5896  @ epoch 16061 )\n",
      "[Epoch: 19180] train loss: 0.6297, train acc: 0.7710, val loss: 0.4369, val acc: 0.8779  (best train acc: 0.7862, best val acc: 0.8958, best train loss: 0.5864  @ epoch 19176 )\n",
      "[Epoch: 19200] train loss: 0.6194, train acc: 0.7689, val loss: 0.4382, val acc: 0.8776  (best train acc: 0.7862, best val acc: 0.8958, best train loss: 0.5864  @ epoch 19176 )\n",
      "[Epoch: 19220] train loss: 0.6156, train acc: 0.7676, val loss: 0.4534, val acc: 0.8894  (best train acc: 0.7862, best val acc: 0.8971, best train loss: 0.5864  @ epoch 19176 )\n",
      "[Epoch: 19240] train loss: 0.6094, train acc: 0.7786, val loss: 0.4313, val acc: 0.8702  (best train acc: 0.7862, best val acc: 0.8971, best train loss: 0.5864  @ epoch 19176 )\n",
      "[Epoch: 19260] train loss: 0.6197, train acc: 0.7689, val loss: 0.4569, val acc: 0.8695  (best train acc: 0.7862, best val acc: 0.8971, best train loss: 0.5864  @ epoch 19176 )\n",
      "[Epoch: 19280] train loss: 0.6120, train acc: 0.7697, val loss: 0.4453, val acc: 0.8705  (best train acc: 0.7862, best val acc: 0.8971, best train loss: 0.5864  @ epoch 19176 )\n",
      "[Epoch: 19300] train loss: 0.6323, train acc: 0.7509, val loss: 0.4649, val acc: 0.8725  (best train acc: 0.7862, best val acc: 0.8971, best train loss: 0.5864  @ epoch 19176 )\n",
      "[Epoch: 19320] train loss: 0.5983, train acc: 0.7739, val loss: 0.4472, val acc: 0.8806  (best train acc: 0.7862, best val acc: 0.8971, best train loss: 0.5864  @ epoch 19176 )\n",
      "[Epoch: 19340] train loss: 0.6370, train acc: 0.7596, val loss: 0.4435, val acc: 0.8617  (best train acc: 0.7862, best val acc: 0.8971, best train loss: 0.5864  @ epoch 19176 )\n",
      "[Epoch: 19360] train loss: 0.6123, train acc: 0.7696, val loss: 0.4438, val acc: 0.8843  (best train acc: 0.7862, best val acc: 0.8971, best train loss: 0.5864  @ epoch 19176 )\n",
      "[Epoch: 19380] train loss: 0.6150, train acc: 0.7625, val loss: 0.4484, val acc: 0.8718  (best train acc: 0.7862, best val acc: 0.8971, best train loss: 0.5864  @ epoch 19176 )\n",
      "[Epoch: 19400] train loss: 0.5950, train acc: 0.7741, val loss: 0.4332, val acc: 0.8830  (best train acc: 0.7862, best val acc: 0.8971, best train loss: 0.5864  @ epoch 19176 )\n",
      "[Epoch: 19420] train loss: 0.6042, train acc: 0.7792, val loss: 0.4417, val acc: 0.8917  (best train acc: 0.7862, best val acc: 0.8971, best train loss: 0.5864  @ epoch 19176 )\n",
      "[Epoch: 19440] train loss: 0.6074, train acc: 0.7753, val loss: 0.4370, val acc: 0.8728  (best train acc: 0.7862, best val acc: 0.8971, best train loss: 0.5864  @ epoch 19176 )\n",
      "[Epoch: 19460] train loss: 0.6193, train acc: 0.7606, val loss: 0.4338, val acc: 0.8836  (best train acc: 0.7862, best val acc: 0.8971, best train loss: 0.5864  @ epoch 19176 )\n",
      "[Epoch: 19480] train loss: 0.6130, train acc: 0.7694, val loss: 0.4592, val acc: 0.8621  (best train acc: 0.7862, best val acc: 0.8971, best train loss: 0.5864  @ epoch 19176 )\n",
      "[Epoch: 19500] train loss: 0.6190, train acc: 0.7597, val loss: 0.4444, val acc: 0.8820  (best train acc: 0.7862, best val acc: 0.8971, best train loss: 0.5864  @ epoch 19176 )\n",
      "[Epoch: 19520] train loss: 0.6101, train acc: 0.7684, val loss: 0.4480, val acc: 0.8678  (best train acc: 0.7862, best val acc: 0.8971, best train loss: 0.5864  @ epoch 19176 )\n",
      "[Epoch: 19540] train loss: 0.6296, train acc: 0.7688, val loss: 0.4386, val acc: 0.8739  (best train acc: 0.7862, best val acc: 0.8971, best train loss: 0.5864  @ epoch 19176 )\n",
      "[Epoch: 19560] train loss: 0.6281, train acc: 0.7650, val loss: 0.4493, val acc: 0.8641  (best train acc: 0.7862, best val acc: 0.8971, best train loss: 0.5864  @ epoch 19176 )\n",
      "[Epoch: 19580] train loss: 0.6306, train acc: 0.7671, val loss: 0.4436, val acc: 0.8826  (best train acc: 0.7862, best val acc: 0.8971, best train loss: 0.5864  @ epoch 19176 )\n",
      "[Epoch: 19600] train loss: 0.6188, train acc: 0.7733, val loss: 0.4402, val acc: 0.8671  (best train acc: 0.7862, best val acc: 0.8971, best train loss: 0.5864  @ epoch 19176 )\n",
      "[Epoch: 19620] train loss: 0.6495, train acc: 0.7425, val loss: 0.4532, val acc: 0.8408  (best train acc: 0.7862, best val acc: 0.8971, best train loss: 0.5864  @ epoch 19176 )\n",
      "[Epoch: 19640] train loss: 0.6241, train acc: 0.7684, val loss: 0.4387, val acc: 0.8671  (best train acc: 0.7862, best val acc: 0.8971, best train loss: 0.5864  @ epoch 19176 )\n",
      "[Epoch: 19660] train loss: 0.6217, train acc: 0.7724, val loss: 0.4538, val acc: 0.8769  (best train acc: 0.7862, best val acc: 0.8971, best train loss: 0.5864  @ epoch 19176 )\n",
      "[Epoch: 19680] train loss: 0.6110, train acc: 0.7691, val loss: 0.4404, val acc: 0.8809  (best train acc: 0.7862, best val acc: 0.8971, best train loss: 0.5864  @ epoch 19176 )\n",
      "[Epoch: 19700] train loss: 0.6097, train acc: 0.7672, val loss: 0.4340, val acc: 0.8755  (best train acc: 0.7862, best val acc: 0.8971, best train loss: 0.5864  @ epoch 19176 )\n",
      "[Epoch: 19720] train loss: 0.6120, train acc: 0.7679, val loss: 0.4519, val acc: 0.8691  (best train acc: 0.7862, best val acc: 0.8971, best train loss: 0.5864  @ epoch 19176 )\n",
      "[Epoch: 19740] train loss: 0.6050, train acc: 0.7697, val loss: 0.4361, val acc: 0.8678  (best train acc: 0.7888, best val acc: 0.8971, best train loss: 0.5820  @ epoch 19735 )\n",
      "[Epoch: 19760] train loss: 0.6090, train acc: 0.7706, val loss: 0.4478, val acc: 0.8847  (best train acc: 0.7888, best val acc: 0.8971, best train loss: 0.5820  @ epoch 19735 )\n",
      "[Epoch: 19780] train loss: 0.5964, train acc: 0.7781, val loss: 0.4434, val acc: 0.8675  (best train acc: 0.7888, best val acc: 0.8971, best train loss: 0.5820  @ epoch 19735 )\n",
      "[Epoch: 19800] train loss: 0.6292, train acc: 0.7653, val loss: 0.4346, val acc: 0.8833  (best train acc: 0.7888, best val acc: 0.8971, best train loss: 0.5820  @ epoch 19735 )\n",
      "[Epoch: 19820] train loss: 0.6030, train acc: 0.7713, val loss: 0.4394, val acc: 0.8944  (best train acc: 0.7888, best val acc: 0.8971, best train loss: 0.5820  @ epoch 19735 )\n",
      "[Epoch: 19840] train loss: 0.6040, train acc: 0.7757, val loss: 0.4440, val acc: 0.8793  (best train acc: 0.7888, best val acc: 0.8971, best train loss: 0.5820  @ epoch 19735 )\n",
      "[Epoch: 19860] train loss: 0.6036, train acc: 0.7786, val loss: 0.4340, val acc: 0.8752  (best train acc: 0.7888, best val acc: 0.8971, best train loss: 0.5820  @ epoch 19735 )\n",
      "[Epoch: 19880] train loss: 0.6127, train acc: 0.7759, val loss: 0.4644, val acc: 0.8533  (best train acc: 0.7888, best val acc: 0.8971, best train loss: 0.5820  @ epoch 19735 )\n",
      "[Epoch: 19900] train loss: 0.6065, train acc: 0.7752, val loss: 0.4478, val acc: 0.8624  (best train acc: 0.7888, best val acc: 0.8971, best train loss: 0.5820  @ epoch 19735 )\n",
      "[Epoch: 19920] train loss: 0.6079, train acc: 0.7762, val loss: 0.4381, val acc: 0.8816  (best train acc: 0.7888, best val acc: 0.8971, best train loss: 0.5820  @ epoch 19735 )\n",
      "[Epoch: 19940] train loss: 0.6099, train acc: 0.7699, val loss: 0.4462, val acc: 0.8705  (best train acc: 0.7888, best val acc: 0.8971, best train loss: 0.5820  @ epoch 19735 )\n",
      "[Epoch: 19960] train loss: 0.6140, train acc: 0.7726, val loss: 0.4782, val acc: 0.8418  (best train acc: 0.7888, best val acc: 0.8971, best train loss: 0.5820  @ epoch 19735 )\n",
      "[Epoch: 19980] train loss: 0.6096, train acc: 0.7665, val loss: 0.4620, val acc: 0.8600  (best train acc: 0.7888, best val acc: 0.8971, best train loss: 0.5820  @ epoch 19735 )\n",
      "[Epoch: 20000] train loss: 0.5968, train acc: 0.7759, val loss: 0.4476, val acc: 0.8752  (best train acc: 0.7888, best val acc: 0.8971, best train loss: 0.5820  @ epoch 19735 )\n",
      "[Epoch: 20020] train loss: 0.6031, train acc: 0.7734, val loss: 0.4381, val acc: 0.8813  (best train acc: 0.7888, best val acc: 0.8971, best train loss: 0.5820  @ epoch 19735 )\n",
      "[Epoch: 20040] train loss: 0.6209, train acc: 0.7691, val loss: 0.4406, val acc: 0.8702  (best train acc: 0.7888, best val acc: 0.8971, best train loss: 0.5820  @ epoch 19735 )\n",
      "[Epoch: 20060] train loss: 0.6040, train acc: 0.7657, val loss: 0.4367, val acc: 0.8840  (best train acc: 0.7888, best val acc: 0.8971, best train loss: 0.5820  @ epoch 19735 )\n",
      "[Epoch: 20080] train loss: 0.6245, train acc: 0.7695, val loss: 0.4431, val acc: 0.8705  (best train acc: 0.7888, best val acc: 0.8971, best train loss: 0.5820  @ epoch 19735 )\n",
      "[Epoch: 20100] train loss: 0.6269, train acc: 0.7638, val loss: 0.4495, val acc: 0.8651  (best train acc: 0.7888, best val acc: 0.8971, best train loss: 0.5820  @ epoch 19735 )\n",
      "[Epoch: 20120] train loss: 0.6184, train acc: 0.7762, val loss: 0.4431, val acc: 0.8806  (best train acc: 0.7888, best val acc: 0.8971, best train loss: 0.5820  @ epoch 19735 )\n",
      "[Epoch: 20140] train loss: 0.6083, train acc: 0.7707, val loss: 0.4407, val acc: 0.8772  (best train acc: 0.7888, best val acc: 0.8971, best train loss: 0.5820  @ epoch 19735 )\n",
      "[Epoch: 20160] train loss: 0.6282, train acc: 0.7603, val loss: 0.4398, val acc: 0.8759  (best train acc: 0.7888, best val acc: 0.8971, best train loss: 0.5820  @ epoch 19735 )\n",
      "[Epoch: 20180] train loss: 0.6202, train acc: 0.7689, val loss: 0.4450, val acc: 0.8776  (best train acc: 0.7888, best val acc: 0.8971, best train loss: 0.5802  @ epoch 20176 )\n",
      "[Epoch: 20200] train loss: 0.6113, train acc: 0.7671, val loss: 0.4399, val acc: 0.8813  (best train acc: 0.7888, best val acc: 0.8971, best train loss: 0.5802  @ epoch 20176 )\n",
      "[Epoch: 20220] train loss: 0.6156, train acc: 0.7711, val loss: 0.4417, val acc: 0.8722  (best train acc: 0.7888, best val acc: 0.8971, best train loss: 0.5802  @ epoch 20176 )\n",
      "[Epoch: 20240] train loss: 0.6015, train acc: 0.7696, val loss: 0.4369, val acc: 0.8857  (best train acc: 0.7888, best val acc: 0.8971, best train loss: 0.5802  @ epoch 20176 )\n",
      "[Epoch: 20260] train loss: 0.6002, train acc: 0.7722, val loss: 0.4375, val acc: 0.8911  (best train acc: 0.7888, best val acc: 0.8971, best train loss: 0.5802  @ epoch 20176 )\n",
      "[Epoch: 20280] train loss: 0.6370, train acc: 0.7501, val loss: 0.4726, val acc: 0.8428  (best train acc: 0.7888, best val acc: 0.8971, best train loss: 0.5802  @ epoch 20176 )\n",
      "[Epoch: 20300] train loss: 0.6236, train acc: 0.7660, val loss: 0.4593, val acc: 0.8718  (best train acc: 0.7888, best val acc: 0.8985, best train loss: 0.5802  @ epoch 20176 )\n",
      "[Epoch: 20320] train loss: 0.6139, train acc: 0.7723, val loss: 0.4537, val acc: 0.8712  (best train acc: 0.7888, best val acc: 0.8985, best train loss: 0.5802  @ epoch 20176 )\n",
      "[Epoch: 20340] train loss: 0.6022, train acc: 0.7729, val loss: 0.4450, val acc: 0.8806  (best train acc: 0.7888, best val acc: 0.8985, best train loss: 0.5802  @ epoch 20176 )\n",
      "[Epoch: 20360] train loss: 0.6387, train acc: 0.7664, val loss: 0.4471, val acc: 0.8695  (best train acc: 0.7888, best val acc: 0.8985, best train loss: 0.5802  @ epoch 20176 )\n",
      "[Epoch: 20380] train loss: 0.6164, train acc: 0.7576, val loss: 0.4472, val acc: 0.8843  (best train acc: 0.7888, best val acc: 0.8985, best train loss: 0.5802  @ epoch 20176 )\n",
      "[Epoch: 20400] train loss: 0.5999, train acc: 0.7715, val loss: 0.4354, val acc: 0.8799  (best train acc: 0.7888, best val acc: 0.8985, best train loss: 0.5802  @ epoch 20176 )\n",
      "[Epoch: 20420] train loss: 0.5996, train acc: 0.7760, val loss: 0.4441, val acc: 0.8651  (best train acc: 0.7888, best val acc: 0.8985, best train loss: 0.5802  @ epoch 20176 )\n",
      "[Epoch: 20440] train loss: 0.6374, train acc: 0.7625, val loss: 0.4495, val acc: 0.8712  (best train acc: 0.7888, best val acc: 0.8985, best train loss: 0.5802  @ epoch 20176 )\n",
      "[Epoch: 20460] train loss: 0.6232, train acc: 0.7629, val loss: 0.4332, val acc: 0.8712  (best train acc: 0.7888, best val acc: 0.8985, best train loss: 0.5802  @ epoch 20176 )\n",
      "[Epoch: 20480] train loss: 0.6128, train acc: 0.7721, val loss: 0.4340, val acc: 0.8799  (best train acc: 0.7888, best val acc: 0.8985, best train loss: 0.5802  @ epoch 20176 )\n",
      "[Epoch: 20500] train loss: 0.6236, train acc: 0.7657, val loss: 0.4438, val acc: 0.8749  (best train acc: 0.7888, best val acc: 0.9012, best train loss: 0.5802  @ epoch 20176 )\n",
      "[Epoch: 20520] train loss: 0.6047, train acc: 0.7757, val loss: 0.4483, val acc: 0.8712  (best train acc: 0.7888, best val acc: 0.9012, best train loss: 0.5802  @ epoch 20176 )\n",
      "[Epoch: 20540] train loss: 0.6136, train acc: 0.7702, val loss: 0.4454, val acc: 0.8820  (best train acc: 0.7888, best val acc: 0.9012, best train loss: 0.5802  @ epoch 20176 )\n",
      "[Epoch: 20560] train loss: 0.6096, train acc: 0.7734, val loss: 0.4564, val acc: 0.8624  (best train acc: 0.7888, best val acc: 0.9012, best train loss: 0.5802  @ epoch 20176 )\n",
      "[Epoch: 20580] train loss: 0.6192, train acc: 0.7660, val loss: 0.4473, val acc: 0.8769  (best train acc: 0.7888, best val acc: 0.9012, best train loss: 0.5802  @ epoch 20176 )\n",
      "[Epoch: 20600] train loss: 0.6031, train acc: 0.7719, val loss: 0.4367, val acc: 0.8772  (best train acc: 0.7888, best val acc: 0.9012, best train loss: 0.5802  @ epoch 20176 )\n",
      "[Epoch: 20620] train loss: 0.6046, train acc: 0.7711, val loss: 0.4378, val acc: 0.8786  (best train acc: 0.7888, best val acc: 0.9012, best train loss: 0.5802  @ epoch 20176 )\n",
      "[Epoch: 20640] train loss: 0.6248, train acc: 0.7662, val loss: 0.4488, val acc: 0.8681  (best train acc: 0.7888, best val acc: 0.9012, best train loss: 0.5802  @ epoch 20176 )\n",
      "[Epoch: 20660] train loss: 0.6019, train acc: 0.7764, val loss: 0.4377, val acc: 0.8843  (best train acc: 0.7888, best val acc: 0.9012, best train loss: 0.5802  @ epoch 20176 )\n",
      "[Epoch: 20680] train loss: 0.6277, train acc: 0.7668, val loss: 0.4408, val acc: 0.8826  (best train acc: 0.7888, best val acc: 0.9012, best train loss: 0.5802  @ epoch 20176 )\n",
      "[Epoch: 20700] train loss: 0.6038, train acc: 0.7716, val loss: 0.4470, val acc: 0.8857  (best train acc: 0.7888, best val acc: 0.9012, best train loss: 0.5802  @ epoch 20176 )\n",
      "[Epoch: 20720] train loss: 0.6029, train acc: 0.7685, val loss: 0.4452, val acc: 0.8793  (best train acc: 0.7888, best val acc: 0.9012, best train loss: 0.5802  @ epoch 20176 )\n",
      "[Epoch: 20740] train loss: 0.6045, train acc: 0.7658, val loss: 0.4469, val acc: 0.8782  (best train acc: 0.7888, best val acc: 0.9012, best train loss: 0.5802  @ epoch 20176 )\n",
      "[Epoch: 20760] train loss: 0.6045, train acc: 0.7693, val loss: 0.4492, val acc: 0.8769  (best train acc: 0.7888, best val acc: 0.9012, best train loss: 0.5802  @ epoch 20176 )\n",
      "[Epoch: 20780] train loss: 0.6145, train acc: 0.7669, val loss: 0.4380, val acc: 0.8927  (best train acc: 0.7888, best val acc: 0.9012, best train loss: 0.5802  @ epoch 20176 )\n",
      "[Epoch: 20800] train loss: 0.5956, train acc: 0.7743, val loss: 0.4348, val acc: 0.8890  (best train acc: 0.7888, best val acc: 0.9052, best train loss: 0.5802  @ epoch 20176 )\n",
      "[Epoch: 20820] train loss: 0.6170, train acc: 0.7624, val loss: 0.4394, val acc: 0.8786  (best train acc: 0.7888, best val acc: 0.9052, best train loss: 0.5802  @ epoch 20176 )\n",
      "[Epoch: 20840] train loss: 0.6168, train acc: 0.7627, val loss: 0.4422, val acc: 0.8894  (best train acc: 0.7888, best val acc: 0.9052, best train loss: 0.5802  @ epoch 20176 )\n",
      "[Epoch: 20860] train loss: 0.5958, train acc: 0.7781, val loss: 0.4433, val acc: 0.8901  (best train acc: 0.7888, best val acc: 0.9052, best train loss: 0.5802  @ epoch 20176 )\n",
      "[Epoch: 20880] train loss: 0.6131, train acc: 0.7716, val loss: 0.4430, val acc: 0.8914  (best train acc: 0.7888, best val acc: 0.9052, best train loss: 0.5802  @ epoch 20176 )\n",
      "[Epoch: 20900] train loss: 0.6262, train acc: 0.7614, val loss: 0.4348, val acc: 0.9005  (best train acc: 0.7888, best val acc: 0.9052, best train loss: 0.5802  @ epoch 20176 )\n",
      "[Epoch: 20920] train loss: 0.6005, train acc: 0.7751, val loss: 0.4353, val acc: 0.8934  (best train acc: 0.7888, best val acc: 0.9052, best train loss: 0.5802  @ epoch 20176 )\n",
      "[Epoch: 20940] train loss: 0.6160, train acc: 0.7796, val loss: 0.4365, val acc: 0.9022  (best train acc: 0.7888, best val acc: 0.9052, best train loss: 0.5802  @ epoch 20176 )\n",
      "[Epoch: 20960] train loss: 0.6049, train acc: 0.7741, val loss: 0.4445, val acc: 0.8884  (best train acc: 0.7888, best val acc: 0.9052, best train loss: 0.5802  @ epoch 20176 )\n",
      "[Epoch: 20980] train loss: 0.6070, train acc: 0.7696, val loss: 0.4316, val acc: 0.8917  (best train acc: 0.7888, best val acc: 0.9052, best train loss: 0.5802  @ epoch 20176 )\n",
      "[Epoch: 21000] train loss: 0.5982, train acc: 0.7734, val loss: 0.4253, val acc: 0.9035  (best train acc: 0.7888, best val acc: 0.9079, best train loss: 0.5802  @ epoch 20176 )\n",
      "[Epoch: 21020] train loss: 0.6054, train acc: 0.7624, val loss: 0.4301, val acc: 0.8938  (best train acc: 0.7888, best val acc: 0.9079, best train loss: 0.5802  @ epoch 20176 )\n",
      "[Epoch: 21040] train loss: 0.6048, train acc: 0.7664, val loss: 0.4358, val acc: 0.8971  (best train acc: 0.7888, best val acc: 0.9079, best train loss: 0.5802  @ epoch 20176 )\n",
      "[Epoch: 21060] train loss: 0.6235, train acc: 0.7602, val loss: 0.4331, val acc: 0.8931  (best train acc: 0.7888, best val acc: 0.9086, best train loss: 0.5802  @ epoch 20176 )\n",
      "[Epoch: 21080] train loss: 0.5907, train acc: 0.7787, val loss: 0.4367, val acc: 0.8843  (best train acc: 0.7888, best val acc: 0.9086, best train loss: 0.5802  @ epoch 20176 )\n",
      "[Epoch: 21100] train loss: 0.6069, train acc: 0.7728, val loss: 0.4290, val acc: 0.8988  (best train acc: 0.7888, best val acc: 0.9093, best train loss: 0.5802  @ epoch 20176 )\n",
      "[Epoch: 21120] train loss: 0.6250, train acc: 0.7636, val loss: 0.4302, val acc: 0.8981  (best train acc: 0.7888, best val acc: 0.9093, best train loss: 0.5802  @ epoch 20176 )\n",
      "[Epoch: 21140] train loss: 0.6091, train acc: 0.7677, val loss: 0.4322, val acc: 0.8796  (best train acc: 0.7888, best val acc: 0.9099, best train loss: 0.5802  @ epoch 20176 )\n",
      "[Epoch: 21160] train loss: 0.6106, train acc: 0.7778, val loss: 0.4271, val acc: 0.9029  (best train acc: 0.7888, best val acc: 0.9099, best train loss: 0.5802  @ epoch 20176 )\n",
      "[Epoch: 21180] train loss: 0.6341, train acc: 0.7650, val loss: 0.4376, val acc: 0.8971  (best train acc: 0.7888, best val acc: 0.9099, best train loss: 0.5802  @ epoch 20176 )\n",
      "[Epoch: 21200] train loss: 0.6121, train acc: 0.7725, val loss: 0.4425, val acc: 0.8954  (best train acc: 0.7888, best val acc: 0.9099, best train loss: 0.5802  @ epoch 20176 )\n",
      "[Epoch: 21220] train loss: 0.6307, train acc: 0.7660, val loss: 0.4396, val acc: 0.8769  (best train acc: 0.7888, best val acc: 0.9197, best train loss: 0.5802  @ epoch 20176 )\n",
      "[Epoch: 21240] train loss: 0.6043, train acc: 0.7721, val loss: 0.4315, val acc: 0.9046  (best train acc: 0.7888, best val acc: 0.9197, best train loss: 0.5802  @ epoch 20176 )\n",
      "[Epoch: 21260] train loss: 0.6036, train acc: 0.7757, val loss: 0.4211, val acc: 0.9042  (best train acc: 0.7888, best val acc: 0.9197, best train loss: 0.5802  @ epoch 20176 )\n",
      "[Epoch: 21280] train loss: 0.5984, train acc: 0.7748, val loss: 0.4228, val acc: 0.9076  (best train acc: 0.7888, best val acc: 0.9197, best train loss: 0.5802  @ epoch 20176 )\n",
      "[Epoch: 21300] train loss: 0.5989, train acc: 0.7809, val loss: 0.4255, val acc: 0.8965  (best train acc: 0.7888, best val acc: 0.9197, best train loss: 0.5768  @ epoch 21285 )\n",
      "[Epoch: 21320] train loss: 0.5743, train acc: 0.7777, val loss: 0.4174, val acc: 0.8890  (best train acc: 0.7888, best val acc: 0.9197, best train loss: 0.5724  @ epoch 21313 )\n",
      "[Epoch: 21340] train loss: 0.5885, train acc: 0.7782, val loss: 0.4235, val acc: 0.8884  (best train acc: 0.7889, best val acc: 0.9197, best train loss: 0.5654  @ epoch 21337 )\n",
      "[Epoch: 21360] train loss: 0.6008, train acc: 0.7748, val loss: 0.4276, val acc: 0.8617  (best train acc: 0.7889, best val acc: 0.9197, best train loss: 0.5654  @ epoch 21337 )\n",
      "[Epoch: 21380] train loss: 0.5637, train acc: 0.7783, val loss: 0.4206, val acc: 0.8803  (best train acc: 0.7889, best val acc: 0.9197, best train loss: 0.5637  @ epoch 21380 )\n",
      "[Epoch: 21400] train loss: 0.5847, train acc: 0.7723, val loss: 0.4114, val acc: 0.8857  (best train acc: 0.7889, best val acc: 0.9197, best train loss: 0.5637  @ epoch 21380 )\n",
      "[Epoch: 21420] train loss: 0.5896, train acc: 0.7728, val loss: 0.4323, val acc: 0.8759  (best train acc: 0.7889, best val acc: 0.9197, best train loss: 0.5637  @ epoch 21380 )\n",
      "[Epoch: 21440] train loss: 0.5951, train acc: 0.7643, val loss: 0.4164, val acc: 0.8941  (best train acc: 0.7889, best val acc: 0.9197, best train loss: 0.5637  @ epoch 21380 )\n",
      "[Epoch: 21460] train loss: 0.6012, train acc: 0.7713, val loss: 0.4082, val acc: 0.8971  (best train acc: 0.7894, best val acc: 0.9197, best train loss: 0.5570  @ epoch 21454 )\n",
      "[Epoch: 21480] train loss: 0.5678, train acc: 0.7885, val loss: 0.4111, val acc: 0.9019  (best train acc: 0.7894, best val acc: 0.9197, best train loss: 0.5570  @ epoch 21454 )\n",
      "[Epoch: 21500] train loss: 0.5570, train acc: 0.7848, val loss: 0.4139, val acc: 0.8941  (best train acc: 0.7903, best val acc: 0.9197, best train loss: 0.5570  @ epoch 21500 )\n",
      "[Epoch: 21520] train loss: 0.6005, train acc: 0.7717, val loss: 0.4313, val acc: 0.8752  (best train acc: 0.7903, best val acc: 0.9197, best train loss: 0.5543  @ epoch 21506 )\n",
      "[Epoch: 21540] train loss: 0.5806, train acc: 0.7778, val loss: 0.4266, val acc: 0.8742  (best train acc: 0.7913, best val acc: 0.9197, best train loss: 0.5505  @ epoch 21538 )\n",
      "[Epoch: 21560] train loss: 0.5972, train acc: 0.7641, val loss: 0.4184, val acc: 0.9035  (best train acc: 0.7913, best val acc: 0.9197, best train loss: 0.5505  @ epoch 21538 )\n",
      "[Epoch: 21580] train loss: 0.5779, train acc: 0.7744, val loss: 0.4220, val acc: 0.8793  (best train acc: 0.7913, best val acc: 0.9197, best train loss: 0.5505  @ epoch 21538 )\n",
      "[Epoch: 21600] train loss: 0.5763, train acc: 0.7831, val loss: 0.4095, val acc: 0.8863  (best train acc: 0.7923, best val acc: 0.9197, best train loss: 0.5505  @ epoch 21538 )\n",
      "[Epoch: 21620] train loss: 0.5653, train acc: 0.7840, val loss: 0.4044, val acc: 0.8992  (best train acc: 0.7923, best val acc: 0.9197, best train loss: 0.5505  @ epoch 21538 )\n",
      "[Epoch: 21640] train loss: 0.5573, train acc: 0.7867, val loss: 0.4198, val acc: 0.8874  (best train acc: 0.7923, best val acc: 0.9197, best train loss: 0.5505  @ epoch 21538 )\n",
      "[Epoch: 21660] train loss: 0.5635, train acc: 0.7849, val loss: 0.4247, val acc: 0.8958  (best train acc: 0.7923, best val acc: 0.9197, best train loss: 0.5426  @ epoch 21645 )\n",
      "[Epoch: 21680] train loss: 0.5684, train acc: 0.7846, val loss: 0.4073, val acc: 0.8971  (best train acc: 0.7923, best val acc: 0.9197, best train loss: 0.5426  @ epoch 21645 )\n",
      "[Epoch: 21700] train loss: 0.5733, train acc: 0.7794, val loss: 0.4050, val acc: 0.9008  (best train acc: 0.7941, best val acc: 0.9197, best train loss: 0.5426  @ epoch 21645 )\n",
      "[Epoch: 21720] train loss: 0.5643, train acc: 0.7911, val loss: 0.4184, val acc: 0.8782  (best train acc: 0.7967, best val acc: 0.9197, best train loss: 0.5426  @ epoch 21645 )\n",
      "[Epoch: 21740] train loss: 0.5598, train acc: 0.7851, val loss: 0.4025, val acc: 0.8894  (best train acc: 0.7967, best val acc: 0.9197, best train loss: 0.5426  @ epoch 21645 )\n",
      "[Epoch: 21760] train loss: 0.5720, train acc: 0.7882, val loss: 0.4037, val acc: 0.9039  (best train acc: 0.7967, best val acc: 0.9197, best train loss: 0.5426  @ epoch 21645 )\n",
      "[Epoch: 21780] train loss: 0.5669, train acc: 0.7830, val loss: 0.4052, val acc: 0.9046  (best train acc: 0.7973, best val acc: 0.9197, best train loss: 0.5426  @ epoch 21645 )\n",
      "[Epoch: 21800] train loss: 0.5799, train acc: 0.7709, val loss: 0.4153, val acc: 0.8799  (best train acc: 0.7973, best val acc: 0.9197, best train loss: 0.5426  @ epoch 21645 )\n",
      "[Epoch: 21820] train loss: 0.5520, train acc: 0.7987, val loss: 0.4099, val acc: 0.9008  (best train acc: 0.7987, best val acc: 0.9197, best train loss: 0.5426  @ epoch 21645 )\n",
      "[Epoch: 21840] train loss: 0.5739, train acc: 0.7877, val loss: 0.4072, val acc: 0.9046  (best train acc: 0.7987, best val acc: 0.9197, best train loss: 0.5426  @ epoch 21645 )\n",
      "[Epoch: 21860] train loss: 0.5636, train acc: 0.7915, val loss: 0.4035, val acc: 0.8927  (best train acc: 0.7987, best val acc: 0.9197, best train loss: 0.5426  @ epoch 21645 )\n",
      "[Epoch: 21880] train loss: 0.5941, train acc: 0.7717, val loss: 0.4211, val acc: 0.8803  (best train acc: 0.7987, best val acc: 0.9197, best train loss: 0.5426  @ epoch 21645 )\n",
      "[Epoch: 21900] train loss: 0.5508, train acc: 0.7899, val loss: 0.4080, val acc: 0.8769  (best train acc: 0.7987, best val acc: 0.9197, best train loss: 0.5426  @ epoch 21645 )\n",
      "[Epoch: 21920] train loss: 0.5626, train acc: 0.7897, val loss: 0.3964, val acc: 0.9150  (best train acc: 0.7987, best val acc: 0.9197, best train loss: 0.5426  @ epoch 21645 )\n",
      "[Epoch: 21940] train loss: 0.5674, train acc: 0.7846, val loss: 0.4053, val acc: 0.8998  (best train acc: 0.8011, best val acc: 0.9197, best train loss: 0.5426  @ epoch 21645 )\n",
      "[Epoch: 21960] train loss: 0.5690, train acc: 0.7863, val loss: 0.4136, val acc: 0.8816  (best train acc: 0.8011, best val acc: 0.9197, best train loss: 0.5426  @ epoch 21645 )\n",
      "[Epoch: 21980] train loss: 0.5477, train acc: 0.7880, val loss: 0.4114, val acc: 0.8806  (best train acc: 0.8011, best val acc: 0.9197, best train loss: 0.5426  @ epoch 21645 )\n",
      "[Epoch: 22000] train loss: 0.5876, train acc: 0.7704, val loss: 0.4441, val acc: 0.8614  (best train acc: 0.8017, best val acc: 0.9197, best train loss: 0.5390  @ epoch 21992 )\n",
      "[Epoch: 22020] train loss: 0.5637, train acc: 0.7874, val loss: 0.4089, val acc: 0.8884  (best train acc: 0.8017, best val acc: 0.9197, best train loss: 0.5390  @ epoch 21992 )\n",
      "[Epoch: 22040] train loss: 0.5669, train acc: 0.7875, val loss: 0.3966, val acc: 0.8951  (best train acc: 0.8017, best val acc: 0.9197, best train loss: 0.5390  @ epoch 21992 )\n",
      "[Epoch: 22060] train loss: 0.5702, train acc: 0.7856, val loss: 0.4033, val acc: 0.8948  (best train acc: 0.8017, best val acc: 0.9197, best train loss: 0.5390  @ epoch 21992 )\n",
      "[Epoch: 22080] train loss: 0.6152, train acc: 0.7584, val loss: 0.4051, val acc: 0.9059  (best train acc: 0.8017, best val acc: 0.9197, best train loss: 0.5390  @ epoch 21992 )\n",
      "[Epoch: 22100] train loss: 0.6405, train acc: 0.7571, val loss: 0.4872, val acc: 0.8526  (best train acc: 0.8017, best val acc: 0.9197, best train loss: 0.5390  @ epoch 21992 )\n",
      "[Epoch: 22120] train loss: 0.5917, train acc: 0.7762, val loss: 0.4128, val acc: 0.8914  (best train acc: 0.8017, best val acc: 0.9197, best train loss: 0.5390  @ epoch 21992 )\n",
      "[Epoch: 22140] train loss: 0.5662, train acc: 0.7929, val loss: 0.4020, val acc: 0.8992  (best train acc: 0.8017, best val acc: 0.9197, best train loss: 0.5390  @ epoch 21992 )\n",
      "[Epoch: 22160] train loss: 0.5539, train acc: 0.7937, val loss: 0.4025, val acc: 0.8961  (best train acc: 0.8017, best val acc: 0.9197, best train loss: 0.5390  @ epoch 21992 )\n",
      "[Epoch: 22180] train loss: 0.5571, train acc: 0.7930, val loss: 0.4066, val acc: 0.8927  (best train acc: 0.8017, best val acc: 0.9197, best train loss: 0.5390  @ epoch 21992 )\n",
      "[Epoch: 22200] train loss: 0.5546, train acc: 0.7901, val loss: 0.3951, val acc: 0.9025  (best train acc: 0.8017, best val acc: 0.9197, best train loss: 0.5390  @ epoch 21992 )\n",
      "[Epoch: 22220] train loss: 0.5668, train acc: 0.7853, val loss: 0.4026, val acc: 0.8985  (best train acc: 0.8017, best val acc: 0.9258, best train loss: 0.5390  @ epoch 21992 )\n",
      "[Epoch: 22240] train loss: 0.5784, train acc: 0.7879, val loss: 0.4044, val acc: 0.8975  (best train acc: 0.8017, best val acc: 0.9258, best train loss: 0.5390  @ epoch 21992 )\n",
      "[Epoch: 22260] train loss: 0.5564, train acc: 0.7942, val loss: 0.4006, val acc: 0.8988  (best train acc: 0.8017, best val acc: 0.9258, best train loss: 0.5390  @ epoch 21992 )\n",
      "[Epoch: 22280] train loss: 0.5629, train acc: 0.7869, val loss: 0.3984, val acc: 0.9066  (best train acc: 0.8017, best val acc: 0.9258, best train loss: 0.5390  @ epoch 21992 )\n",
      "[Epoch: 22300] train loss: 0.5596, train acc: 0.7926, val loss: 0.3940, val acc: 0.9113  (best train acc: 0.8017, best val acc: 0.9258, best train loss: 0.5390  @ epoch 21992 )\n",
      "[Epoch: 22320] train loss: 0.5513, train acc: 0.7932, val loss: 0.3956, val acc: 0.9096  (best train acc: 0.8017, best val acc: 0.9258, best train loss: 0.5390  @ epoch 21992 )\n",
      "[Epoch: 22340] train loss: 0.5376, train acc: 0.7949, val loss: 0.4135, val acc: 0.8944  (best train acc: 0.8017, best val acc: 0.9258, best train loss: 0.5376  @ epoch 22340 )\n",
      "[Epoch: 22360] train loss: 0.5687, train acc: 0.7849, val loss: 0.4010, val acc: 0.8917  (best train acc: 0.8036, best val acc: 0.9258, best train loss: 0.5376  @ epoch 22340 )\n",
      "[Epoch: 22380] train loss: 0.5528, train acc: 0.7852, val loss: 0.3922, val acc: 0.9079  (best train acc: 0.8036, best val acc: 0.9258, best train loss: 0.5369  @ epoch 22370 )\n",
      "[Epoch: 22400] train loss: 0.5881, train acc: 0.7752, val loss: 0.4029, val acc: 0.8985  (best train acc: 0.8036, best val acc: 0.9258, best train loss: 0.5369  @ epoch 22370 )\n",
      "[Epoch: 22420] train loss: 0.5699, train acc: 0.7728, val loss: 0.3962, val acc: 0.9062  (best train acc: 0.8036, best val acc: 0.9258, best train loss: 0.5369  @ epoch 22370 )\n",
      "[Epoch: 22440] train loss: 0.6449, train acc: 0.7463, val loss: 0.4905, val acc: 0.8462  (best train acc: 0.8036, best val acc: 0.9258, best train loss: 0.5369  @ epoch 22370 )\n",
      "[Epoch: 22460] train loss: 0.5797, train acc: 0.7732, val loss: 0.4375, val acc: 0.8742  (best train acc: 0.8036, best val acc: 0.9258, best train loss: 0.5369  @ epoch 22370 )\n",
      "[Epoch: 22480] train loss: 0.5591, train acc: 0.7958, val loss: 0.4213, val acc: 0.8820  (best train acc: 0.8036, best val acc: 0.9258, best train loss: 0.5369  @ epoch 22370 )\n",
      "[Epoch: 22500] train loss: 0.6052, train acc: 0.7809, val loss: 0.4111, val acc: 0.8971  (best train acc: 0.8036, best val acc: 0.9258, best train loss: 0.5369  @ epoch 22370 )\n",
      "[Epoch: 22520] train loss: 0.5809, train acc: 0.7723, val loss: 0.3987, val acc: 0.8965  (best train acc: 0.8036, best val acc: 0.9258, best train loss: 0.5369  @ epoch 22370 )\n",
      "[Epoch: 22540] train loss: 0.5710, train acc: 0.7801, val loss: 0.3844, val acc: 0.9069  (best train acc: 0.8036, best val acc: 0.9258, best train loss: 0.5369  @ epoch 22370 )\n",
      "[Epoch: 22560] train loss: 0.5497, train acc: 0.7992, val loss: 0.3896, val acc: 0.9140  (best train acc: 0.8036, best val acc: 0.9258, best train loss: 0.5326  @ epoch 22541 )\n",
      "[Epoch: 22580] train loss: 0.5586, train acc: 0.7895, val loss: 0.3905, val acc: 0.9170  (best train acc: 0.8036, best val acc: 0.9258, best train loss: 0.5326  @ epoch 22541 )\n",
      "[Epoch: 22600] train loss: 0.5474, train acc: 0.7985, val loss: 0.3918, val acc: 0.9150  (best train acc: 0.8036, best val acc: 0.9258, best train loss: 0.5326  @ epoch 22541 )\n",
      "[Epoch: 22620] train loss: 0.5540, train acc: 0.7971, val loss: 0.3873, val acc: 0.9005  (best train acc: 0.8036, best val acc: 0.9258, best train loss: 0.5326  @ epoch 22541 )\n",
      "[Epoch: 22640] train loss: 0.5502, train acc: 0.7874, val loss: 0.3837, val acc: 0.9099  (best train acc: 0.8036, best val acc: 0.9258, best train loss: 0.5326  @ epoch 22541 )\n",
      "[Epoch: 22660] train loss: 0.5746, train acc: 0.7861, val loss: 0.4001, val acc: 0.8870  (best train acc: 0.8036, best val acc: 0.9258, best train loss: 0.5326  @ epoch 22541 )\n",
      "[Epoch: 22680] train loss: 0.5505, train acc: 0.7942, val loss: 0.4007, val acc: 0.8961  (best train acc: 0.8036, best val acc: 0.9258, best train loss: 0.5326  @ epoch 22541 )\n",
      "[Epoch: 22700] train loss: 0.5545, train acc: 0.7958, val loss: 0.3982, val acc: 0.8965  (best train acc: 0.8036, best val acc: 0.9258, best train loss: 0.5326  @ epoch 22541 )\n",
      "[Epoch: 22720] train loss: 0.5523, train acc: 0.7864, val loss: 0.3910, val acc: 0.9066  (best train acc: 0.8036, best val acc: 0.9258, best train loss: 0.5326  @ epoch 22541 )\n",
      "[Epoch: 22740] train loss: 0.5354, train acc: 0.8081, val loss: 0.3933, val acc: 0.9123  (best train acc: 0.8081, best val acc: 0.9258, best train loss: 0.5326  @ epoch 22541 )\n",
      "[Epoch: 22760] train loss: 0.5753, train acc: 0.7877, val loss: 0.3947, val acc: 0.9002  (best train acc: 0.8081, best val acc: 0.9258, best train loss: 0.5326  @ epoch 22541 )\n",
      "[Epoch: 22780] train loss: 0.5534, train acc: 0.7906, val loss: 0.4038, val acc: 0.8890  (best train acc: 0.8081, best val acc: 0.9258, best train loss: 0.5326  @ epoch 22541 )\n",
      "[Epoch: 22800] train loss: 0.5561, train acc: 0.7899, val loss: 0.3898, val acc: 0.9167  (best train acc: 0.8081, best val acc: 0.9258, best train loss: 0.5326  @ epoch 22541 )\n",
      "[Epoch: 22820] train loss: 0.5443, train acc: 0.8010, val loss: 0.3872, val acc: 0.9073  (best train acc: 0.8081, best val acc: 0.9258, best train loss: 0.5326  @ epoch 22541 )\n",
      "[Epoch: 22840] train loss: 0.5556, train acc: 0.7934, val loss: 0.3975, val acc: 0.8951  (best train acc: 0.8081, best val acc: 0.9258, best train loss: 0.5326  @ epoch 22541 )\n",
      "[Epoch: 22860] train loss: 0.5809, train acc: 0.7848, val loss: 0.4178, val acc: 0.8742  (best train acc: 0.8081, best val acc: 0.9258, best train loss: 0.5326  @ epoch 22541 )\n",
      "[Epoch: 22880] train loss: 0.5560, train acc: 0.7960, val loss: 0.3921, val acc: 0.9049  (best train acc: 0.8081, best val acc: 0.9258, best train loss: 0.5326  @ epoch 22541 )\n",
      "[Epoch: 22900] train loss: 0.5484, train acc: 0.7916, val loss: 0.3940, val acc: 0.9019  (best train acc: 0.8081, best val acc: 0.9258, best train loss: 0.5326  @ epoch 22541 )\n",
      "[Epoch: 22920] train loss: 0.5710, train acc: 0.7752, val loss: 0.4124, val acc: 0.8809  (best train acc: 0.8081, best val acc: 0.9258, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 22940] train loss: 0.6591, train acc: 0.7511, val loss: 0.4631, val acc: 0.8516  (best train acc: 0.8081, best val acc: 0.9258, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 22960] train loss: 0.6092, train acc: 0.7650, val loss: 0.4455, val acc: 0.8739  (best train acc: 0.8081, best val acc: 0.9258, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 22980] train loss: 0.6285, train acc: 0.7653, val loss: 0.4250, val acc: 0.8847  (best train acc: 0.8081, best val acc: 0.9258, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 23000] train loss: 0.6143, train acc: 0.7677, val loss: 0.4768, val acc: 0.8411  (best train acc: 0.8081, best val acc: 0.9258, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 23020] train loss: 0.6134, train acc: 0.7780, val loss: 0.4227, val acc: 0.8860  (best train acc: 0.8081, best val acc: 0.9258, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 23040] train loss: 0.6089, train acc: 0.7632, val loss: 0.4000, val acc: 0.9089  (best train acc: 0.8081, best val acc: 0.9258, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 23060] train loss: 0.6040, train acc: 0.7811, val loss: 0.4100, val acc: 0.9073  (best train acc: 0.8081, best val acc: 0.9258, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 23080] train loss: 0.6089, train acc: 0.7744, val loss: 0.3964, val acc: 0.9153  (best train acc: 0.8081, best val acc: 0.9258, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 23100] train loss: 0.6057, train acc: 0.7723, val loss: 0.4154, val acc: 0.8931  (best train acc: 0.8081, best val acc: 0.9258, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 23120] train loss: 0.6004, train acc: 0.7810, val loss: 0.4140, val acc: 0.9123  (best train acc: 0.8081, best val acc: 0.9258, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 23140] train loss: 0.5913, train acc: 0.7789, val loss: 0.4005, val acc: 0.9113  (best train acc: 0.8081, best val acc: 0.9258, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 23160] train loss: 0.6059, train acc: 0.7767, val loss: 0.4042, val acc: 0.9113  (best train acc: 0.8081, best val acc: 0.9258, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 23180] train loss: 0.5832, train acc: 0.7804, val loss: 0.4113, val acc: 0.9150  (best train acc: 0.8081, best val acc: 0.9258, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 23200] train loss: 0.6204, train acc: 0.7711, val loss: 0.3988, val acc: 0.9049  (best train acc: 0.8081, best val acc: 0.9258, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 23220] train loss: 0.6016, train acc: 0.7803, val loss: 0.4011, val acc: 0.9157  (best train acc: 0.8081, best val acc: 0.9258, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 23240] train loss: 0.6225, train acc: 0.7677, val loss: 0.4173, val acc: 0.8971  (best train acc: 0.8081, best val acc: 0.9258, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 23260] train loss: 0.6285, train acc: 0.7718, val loss: 0.4390, val acc: 0.8816  (best train acc: 0.8081, best val acc: 0.9258, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 23280] train loss: 0.6925, train acc: 0.7365, val loss: 0.5070, val acc: 0.8644  (best train acc: 0.8081, best val acc: 0.9258, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 23300] train loss: 0.7048, train acc: 0.7201, val loss: 0.5192, val acc: 0.8610  (best train acc: 0.8081, best val acc: 0.9258, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 23320] train loss: 0.6019, train acc: 0.7827, val loss: 0.4278, val acc: 0.8779  (best train acc: 0.8081, best val acc: 0.9258, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 23340] train loss: 0.5886, train acc: 0.7782, val loss: 0.4077, val acc: 0.9066  (best train acc: 0.8081, best val acc: 0.9258, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 23360] train loss: 0.6147, train acc: 0.7679, val loss: 0.4169, val acc: 0.8998  (best train acc: 0.8081, best val acc: 0.9258, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 23380] train loss: 0.5995, train acc: 0.7775, val loss: 0.4084, val acc: 0.9137  (best train acc: 0.8081, best val acc: 0.9258, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 23400] train loss: 0.6000, train acc: 0.7862, val loss: 0.3963, val acc: 0.9096  (best train acc: 0.8081, best val acc: 0.9258, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 23420] train loss: 0.6117, train acc: 0.7790, val loss: 0.3925, val acc: 0.9211  (best train acc: 0.8081, best val acc: 0.9258, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 23440] train loss: 0.5932, train acc: 0.7756, val loss: 0.3999, val acc: 0.9150  (best train acc: 0.8081, best val acc: 0.9258, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 23460] train loss: 0.6346, train acc: 0.7600, val loss: 0.4189, val acc: 0.8961  (best train acc: 0.8081, best val acc: 0.9258, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 23480] train loss: 0.6328, train acc: 0.7616, val loss: 0.4553, val acc: 0.8789  (best train acc: 0.8081, best val acc: 0.9258, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 23500] train loss: 0.6326, train acc: 0.7658, val loss: 0.4071, val acc: 0.9177  (best train acc: 0.8081, best val acc: 0.9258, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 23520] train loss: 0.6135, train acc: 0.7775, val loss: 0.4051, val acc: 0.9137  (best train acc: 0.8081, best val acc: 0.9258, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 23540] train loss: 0.6057, train acc: 0.7760, val loss: 0.4179, val acc: 0.9029  (best train acc: 0.8081, best val acc: 0.9258, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 23560] train loss: 0.6049, train acc: 0.7832, val loss: 0.3968, val acc: 0.9177  (best train acc: 0.8081, best val acc: 0.9258, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 23580] train loss: 0.6008, train acc: 0.7739, val loss: 0.4088, val acc: 0.9140  (best train acc: 0.8081, best val acc: 0.9258, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 23600] train loss: 0.6769, train acc: 0.7392, val loss: 0.4423, val acc: 0.8857  (best train acc: 0.8081, best val acc: 0.9258, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 23620] train loss: 0.6043, train acc: 0.7733, val loss: 0.4020, val acc: 0.9140  (best train acc: 0.8081, best val acc: 0.9258, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 23640] train loss: 0.6140, train acc: 0.7671, val loss: 0.3964, val acc: 0.9164  (best train acc: 0.8081, best val acc: 0.9258, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 23660] train loss: 0.5959, train acc: 0.7775, val loss: 0.4003, val acc: 0.9032  (best train acc: 0.8081, best val acc: 0.9258, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 23680] train loss: 0.6280, train acc: 0.7629, val loss: 0.4007, val acc: 0.9157  (best train acc: 0.8081, best val acc: 0.9258, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 23700] train loss: 0.6138, train acc: 0.7754, val loss: 0.4231, val acc: 0.8924  (best train acc: 0.8081, best val acc: 0.9258, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 23720] train loss: 0.6822, train acc: 0.7349, val loss: 0.4868, val acc: 0.8664  (best train acc: 0.8081, best val acc: 0.9258, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 23740] train loss: 0.6175, train acc: 0.7721, val loss: 0.4149, val acc: 0.8924  (best train acc: 0.8081, best val acc: 0.9258, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 23760] train loss: 0.5929, train acc: 0.7775, val loss: 0.4079, val acc: 0.9035  (best train acc: 0.8081, best val acc: 0.9258, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 23780] train loss: 0.5878, train acc: 0.7869, val loss: 0.4077, val acc: 0.9164  (best train acc: 0.8081, best val acc: 0.9258, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 23800] train loss: 0.5896, train acc: 0.7794, val loss: 0.4062, val acc: 0.9147  (best train acc: 0.8081, best val acc: 0.9258, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 23820] train loss: 0.6102, train acc: 0.7748, val loss: 0.4080, val acc: 0.9157  (best train acc: 0.8081, best val acc: 0.9258, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 23840] train loss: 0.6166, train acc: 0.7723, val loss: 0.4120, val acc: 0.9069  (best train acc: 0.8081, best val acc: 0.9258, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 23860] train loss: 0.6063, train acc: 0.7701, val loss: 0.3998, val acc: 0.9170  (best train acc: 0.8081, best val acc: 0.9258, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 23880] train loss: 0.5743, train acc: 0.7832, val loss: 0.4186, val acc: 0.8809  (best train acc: 0.8081, best val acc: 0.9258, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 23900] train loss: 0.6355, train acc: 0.7472, val loss: 0.4035, val acc: 0.9086  (best train acc: 0.8081, best val acc: 0.9258, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 23920] train loss: 0.6161, train acc: 0.7668, val loss: 0.4046, val acc: 0.9133  (best train acc: 0.8081, best val acc: 0.9258, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 23940] train loss: 0.5886, train acc: 0.7851, val loss: 0.4122, val acc: 0.8998  (best train acc: 0.8081, best val acc: 0.9258, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 23960] train loss: 0.5847, train acc: 0.7798, val loss: 0.4011, val acc: 0.9160  (best train acc: 0.8081, best val acc: 0.9258, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 23980] train loss: 0.5851, train acc: 0.7762, val loss: 0.4020, val acc: 0.9164  (best train acc: 0.8081, best val acc: 0.9258, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 24000] train loss: 0.6461, train acc: 0.7546, val loss: 0.4351, val acc: 0.8809  (best train acc: 0.8081, best val acc: 0.9258, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 24020] train loss: 0.6137, train acc: 0.7719, val loss: 0.4770, val acc: 0.8408  (best train acc: 0.8081, best val acc: 0.9258, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 24040] train loss: 0.6119, train acc: 0.7735, val loss: 0.4272, val acc: 0.8951  (best train acc: 0.8081, best val acc: 0.9258, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 24060] train loss: 0.6004, train acc: 0.7818, val loss: 0.4061, val acc: 0.9103  (best train acc: 0.8081, best val acc: 0.9258, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 24080] train loss: 0.5909, train acc: 0.7767, val loss: 0.4038, val acc: 0.8934  (best train acc: 0.8081, best val acc: 0.9258, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 24100] train loss: 0.6074, train acc: 0.7701, val loss: 0.3946, val acc: 0.9160  (best train acc: 0.8081, best val acc: 0.9258, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 24120] train loss: 0.5883, train acc: 0.7788, val loss: 0.3983, val acc: 0.9187  (best train acc: 0.8081, best val acc: 0.9258, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 24140] train loss: 0.5993, train acc: 0.7765, val loss: 0.4007, val acc: 0.9089  (best train acc: 0.8081, best val acc: 0.9258, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 24160] train loss: 0.5895, train acc: 0.7795, val loss: 0.3974, val acc: 0.9177  (best train acc: 0.8081, best val acc: 0.9258, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 24180] train loss: 0.5793, train acc: 0.7830, val loss: 0.3991, val acc: 0.9035  (best train acc: 0.8081, best val acc: 0.9258, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 24200] train loss: 0.6127, train acc: 0.7723, val loss: 0.4140, val acc: 0.9194  (best train acc: 0.8081, best val acc: 0.9258, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 24220] train loss: 0.6134, train acc: 0.7702, val loss: 0.4074, val acc: 0.9089  (best train acc: 0.8081, best val acc: 0.9258, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 24240] train loss: 0.6087, train acc: 0.7776, val loss: 0.4017, val acc: 0.9099  (best train acc: 0.8081, best val acc: 0.9258, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 24260] train loss: 0.6135, train acc: 0.7689, val loss: 0.3910, val acc: 0.9130  (best train acc: 0.8081, best val acc: 0.9258, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 24280] train loss: 0.5980, train acc: 0.7770, val loss: 0.3939, val acc: 0.9133  (best train acc: 0.8081, best val acc: 0.9258, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 24300] train loss: 0.6210, train acc: 0.7764, val loss: 0.4083, val acc: 0.9191  (best train acc: 0.8081, best val acc: 0.9258, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 24320] train loss: 0.6053, train acc: 0.7728, val loss: 0.4181, val acc: 0.8887  (best train acc: 0.8081, best val acc: 0.9258, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 24340] train loss: 0.5975, train acc: 0.7778, val loss: 0.4041, val acc: 0.9076  (best train acc: 0.8081, best val acc: 0.9258, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 24360] train loss: 0.6137, train acc: 0.7754, val loss: 0.4269, val acc: 0.8830  (best train acc: 0.8081, best val acc: 0.9258, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 24380] train loss: 0.5962, train acc: 0.7809, val loss: 0.4221, val acc: 0.8954  (best train acc: 0.8081, best val acc: 0.9258, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 24400] train loss: 0.6013, train acc: 0.7720, val loss: 0.4035, val acc: 0.9130  (best train acc: 0.8081, best val acc: 0.9258, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 24420] train loss: 0.6095, train acc: 0.7685, val loss: 0.4086, val acc: 0.9147  (best train acc: 0.8081, best val acc: 0.9258, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 24440] train loss: 0.6029, train acc: 0.7724, val loss: 0.4152, val acc: 0.8992  (best train acc: 0.8081, best val acc: 0.9258, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 24460] train loss: 0.6102, train acc: 0.7739, val loss: 0.4133, val acc: 0.9153  (best train acc: 0.8081, best val acc: 0.9258, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 24480] train loss: 0.5969, train acc: 0.7659, val loss: 0.4084, val acc: 0.9039  (best train acc: 0.8081, best val acc: 0.9258, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 24500] train loss: 0.6094, train acc: 0.7666, val loss: 0.4157, val acc: 0.8907  (best train acc: 0.8081, best val acc: 0.9258, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 24520] train loss: 0.5928, train acc: 0.7767, val loss: 0.4019, val acc: 0.9079  (best train acc: 0.8081, best val acc: 0.9258, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 24540] train loss: 0.6257, train acc: 0.7633, val loss: 0.4037, val acc: 0.9221  (best train acc: 0.8081, best val acc: 0.9258, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 24560] train loss: 0.6813, train acc: 0.7401, val loss: 0.4595, val acc: 0.8644  (best train acc: 0.8081, best val acc: 0.9258, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 24580] train loss: 0.5994, train acc: 0.7733, val loss: 0.4192, val acc: 0.8853  (best train acc: 0.8081, best val acc: 0.9258, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 24600] train loss: 0.6000, train acc: 0.7755, val loss: 0.4154, val acc: 0.8934  (best train acc: 0.8081, best val acc: 0.9258, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 24620] train loss: 0.6112, train acc: 0.7719, val loss: 0.3987, val acc: 0.9069  (best train acc: 0.8081, best val acc: 0.9258, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 24640] train loss: 0.6010, train acc: 0.7734, val loss: 0.4035, val acc: 0.9133  (best train acc: 0.8081, best val acc: 0.9258, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 24660] train loss: 0.6586, train acc: 0.7415, val loss: 0.4431, val acc: 0.8755  (best train acc: 0.8081, best val acc: 0.9258, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 24680] train loss: 0.6161, train acc: 0.7707, val loss: 0.4132, val acc: 0.9089  (best train acc: 0.8081, best val acc: 0.9258, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 24700] train loss: 0.5915, train acc: 0.7813, val loss: 0.3958, val acc: 0.9093  (best train acc: 0.8081, best val acc: 0.9258, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 24720] train loss: 0.6085, train acc: 0.7792, val loss: 0.4086, val acc: 0.9126  (best train acc: 0.8081, best val acc: 0.9258, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 24740] train loss: 0.5950, train acc: 0.7810, val loss: 0.4105, val acc: 0.9164  (best train acc: 0.8081, best val acc: 0.9258, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 24760] train loss: 0.7033, train acc: 0.7299, val loss: 0.4556, val acc: 0.8695  (best train acc: 0.8081, best val acc: 0.9258, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 24780] train loss: 0.6310, train acc: 0.7660, val loss: 0.4254, val acc: 0.8884  (best train acc: 0.8081, best val acc: 0.9258, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 24800] train loss: 0.6285, train acc: 0.7649, val loss: 0.4244, val acc: 0.8968  (best train acc: 0.8081, best val acc: 0.9258, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 24820] train loss: 0.6100, train acc: 0.7658, val loss: 0.4062, val acc: 0.9062  (best train acc: 0.8081, best val acc: 0.9258, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 24840] train loss: 0.5955, train acc: 0.7766, val loss: 0.4056, val acc: 0.9126  (best train acc: 0.8081, best val acc: 0.9258, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 24860] train loss: 0.6210, train acc: 0.7651, val loss: 0.4321, val acc: 0.8809  (best train acc: 0.8081, best val acc: 0.9258, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 24880] train loss: 0.6117, train acc: 0.7667, val loss: 0.4046, val acc: 0.9012  (best train acc: 0.8081, best val acc: 0.9258, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 24900] train loss: 0.6060, train acc: 0.7792, val loss: 0.4073, val acc: 0.9049  (best train acc: 0.8081, best val acc: 0.9258, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 24920] train loss: 0.5920, train acc: 0.7825, val loss: 0.3962, val acc: 0.9150  (best train acc: 0.8081, best val acc: 0.9258, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 24940] train loss: 0.6067, train acc: 0.7775, val loss: 0.3999, val acc: 0.9167  (best train acc: 0.8081, best val acc: 0.9258, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 24960] train loss: 0.6024, train acc: 0.7730, val loss: 0.3978, val acc: 0.9137  (best train acc: 0.8081, best val acc: 0.9258, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 24980] train loss: 0.6128, train acc: 0.7726, val loss: 0.4067, val acc: 0.9120  (best train acc: 0.8081, best val acc: 0.9258, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 25000] train loss: 0.5821, train acc: 0.7891, val loss: 0.4075, val acc: 0.9180  (best train acc: 0.8081, best val acc: 0.9258, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 25020] train loss: 0.5899, train acc: 0.7835, val loss: 0.3970, val acc: 0.9143  (best train acc: 0.8081, best val acc: 0.9258, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 25040] train loss: 0.6048, train acc: 0.7755, val loss: 0.4595, val acc: 0.8556  (best train acc: 0.8081, best val acc: 0.9258, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 25060] train loss: 0.6258, train acc: 0.7681, val loss: 0.4262, val acc: 0.8887  (best train acc: 0.8081, best val acc: 0.9258, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 25080] train loss: 0.6062, train acc: 0.7700, val loss: 0.4155, val acc: 0.8948  (best train acc: 0.8081, best val acc: 0.9258, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 25100] train loss: 0.5799, train acc: 0.7865, val loss: 0.4059, val acc: 0.9143  (best train acc: 0.8081, best val acc: 0.9258, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 25120] train loss: 0.6454, train acc: 0.7645, val loss: 0.4368, val acc: 0.8779  (best train acc: 0.8081, best val acc: 0.9258, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 25140] train loss: 0.6320, train acc: 0.7615, val loss: 0.4223, val acc: 0.8870  (best train acc: 0.8081, best val acc: 0.9258, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 25160] train loss: 0.5960, train acc: 0.7773, val loss: 0.4182, val acc: 0.8988  (best train acc: 0.8081, best val acc: 0.9258, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 25180] train loss: 0.6125, train acc: 0.7790, val loss: 0.4159, val acc: 0.8884  (best train acc: 0.8081, best val acc: 0.9258, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 25200] train loss: 0.6002, train acc: 0.7786, val loss: 0.3997, val acc: 0.9130  (best train acc: 0.8081, best val acc: 0.9258, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 25220] train loss: 0.6129, train acc: 0.7725, val loss: 0.3986, val acc: 0.9116  (best train acc: 0.8081, best val acc: 0.9258, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 25240] train loss: 0.6052, train acc: 0.7760, val loss: 0.3984, val acc: 0.9133  (best train acc: 0.8081, best val acc: 0.9258, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 25260] train loss: 0.5924, train acc: 0.7794, val loss: 0.4035, val acc: 0.9066  (best train acc: 0.8081, best val acc: 0.9258, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 25280] train loss: 0.5917, train acc: 0.7799, val loss: 0.4001, val acc: 0.9116  (best train acc: 0.8081, best val acc: 0.9258, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 25300] train loss: 0.6108, train acc: 0.7721, val loss: 0.4921, val acc: 0.8560  (best train acc: 0.8081, best val acc: 0.9258, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 25320] train loss: 0.6169, train acc: 0.7621, val loss: 0.4553, val acc: 0.8675  (best train acc: 0.8081, best val acc: 0.9292, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 25340] train loss: 0.5946, train acc: 0.7746, val loss: 0.4053, val acc: 0.9110  (best train acc: 0.8081, best val acc: 0.9292, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 25360] train loss: 0.5970, train acc: 0.7833, val loss: 0.4070, val acc: 0.8988  (best train acc: 0.8081, best val acc: 0.9292, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 25380] train loss: 0.6148, train acc: 0.7718, val loss: 0.4063, val acc: 0.9096  (best train acc: 0.8081, best val acc: 0.9292, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 25400] train loss: 0.5952, train acc: 0.7809, val loss: 0.3982, val acc: 0.9170  (best train acc: 0.8081, best val acc: 0.9292, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 25420] train loss: 0.5940, train acc: 0.7854, val loss: 0.4014, val acc: 0.9177  (best train acc: 0.8081, best val acc: 0.9292, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 25440] train loss: 0.6007, train acc: 0.7817, val loss: 0.4018, val acc: 0.9103  (best train acc: 0.8081, best val acc: 0.9292, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 25460] train loss: 0.5853, train acc: 0.7816, val loss: 0.3995, val acc: 0.9126  (best train acc: 0.8081, best val acc: 0.9292, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 25480] train loss: 0.6021, train acc: 0.7760, val loss: 0.4060, val acc: 0.9005  (best train acc: 0.8081, best val acc: 0.9292, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 25500] train loss: 0.6087, train acc: 0.7723, val loss: 0.3990, val acc: 0.9140  (best train acc: 0.8081, best val acc: 0.9292, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 25520] train loss: 0.6464, train acc: 0.7499, val loss: 0.5207, val acc: 0.8459  (best train acc: 0.8081, best val acc: 0.9292, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 25540] train loss: 0.6175, train acc: 0.7645, val loss: 0.4924, val acc: 0.8519  (best train acc: 0.8081, best val acc: 0.9292, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 25560] train loss: 0.6508, train acc: 0.7454, val loss: 0.4220, val acc: 0.8968  (best train acc: 0.8081, best val acc: 0.9292, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 25580] train loss: 0.5973, train acc: 0.7792, val loss: 0.4062, val acc: 0.9126  (best train acc: 0.8081, best val acc: 0.9292, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 25600] train loss: 0.5974, train acc: 0.7771, val loss: 0.4031, val acc: 0.9187  (best train acc: 0.8081, best val acc: 0.9292, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 25620] train loss: 0.5958, train acc: 0.7822, val loss: 0.4148, val acc: 0.9022  (best train acc: 0.8081, best val acc: 0.9292, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 25640] train loss: 0.6083, train acc: 0.7737, val loss: 0.4058, val acc: 0.9083  (best train acc: 0.8081, best val acc: 0.9292, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 25660] train loss: 0.6039, train acc: 0.7783, val loss: 0.4071, val acc: 0.9133  (best train acc: 0.8081, best val acc: 0.9292, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 25680] train loss: 0.6005, train acc: 0.7707, val loss: 0.3992, val acc: 0.9137  (best train acc: 0.8081, best val acc: 0.9292, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 25700] train loss: 0.5976, train acc: 0.7814, val loss: 0.4105, val acc: 0.9180  (best train acc: 0.8081, best val acc: 0.9292, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 25720] train loss: 0.5922, train acc: 0.7850, val loss: 0.4013, val acc: 0.9191  (best train acc: 0.8081, best val acc: 0.9292, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 25740] train loss: 0.5925, train acc: 0.7852, val loss: 0.4005, val acc: 0.9153  (best train acc: 0.8081, best val acc: 0.9292, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 25760] train loss: 0.5977, train acc: 0.7739, val loss: 0.4005, val acc: 0.9167  (best train acc: 0.8081, best val acc: 0.9292, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 25780] train loss: 0.5958, train acc: 0.7819, val loss: 0.3992, val acc: 0.9140  (best train acc: 0.8081, best val acc: 0.9292, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 25800] train loss: 0.6090, train acc: 0.7775, val loss: 0.3973, val acc: 0.9150  (best train acc: 0.8081, best val acc: 0.9292, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 25820] train loss: 0.6176, train acc: 0.7637, val loss: 0.4682, val acc: 0.8658  (best train acc: 0.8081, best val acc: 0.9292, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 25840] train loss: 0.7088, train acc: 0.7296, val loss: 0.5289, val acc: 0.8755  (best train acc: 0.8081, best val acc: 0.9292, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 25860] train loss: 0.6620, train acc: 0.7579, val loss: 0.4379, val acc: 0.8732  (best train acc: 0.8081, best val acc: 0.9292, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 25880] train loss: 0.6832, train acc: 0.7363, val loss: 0.5037, val acc: 0.8718  (best train acc: 0.8081, best val acc: 0.9292, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 25900] train loss: 0.5983, train acc: 0.7839, val loss: 0.4124, val acc: 0.8921  (best train acc: 0.8081, best val acc: 0.9292, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 25920] train loss: 0.5819, train acc: 0.7826, val loss: 0.4026, val acc: 0.9093  (best train acc: 0.8081, best val acc: 0.9292, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 25940] train loss: 0.5820, train acc: 0.7799, val loss: 0.4009, val acc: 0.9157  (best train acc: 0.8081, best val acc: 0.9292, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 25960] train loss: 0.6086, train acc: 0.7698, val loss: 0.4099, val acc: 0.9153  (best train acc: 0.8081, best val acc: 0.9292, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 25980] train loss: 0.6073, train acc: 0.7808, val loss: 0.4217, val acc: 0.8776  (best train acc: 0.8081, best val acc: 0.9292, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 26000] train loss: 0.6093, train acc: 0.7778, val loss: 0.3996, val acc: 0.9002  (best train acc: 0.8081, best val acc: 0.9292, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 26020] train loss: 0.5773, train acc: 0.7838, val loss: 0.4014, val acc: 0.9180  (best train acc: 0.8081, best val acc: 0.9292, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 26040] train loss: 0.5813, train acc: 0.7791, val loss: 0.4001, val acc: 0.9150  (best train acc: 0.8081, best val acc: 0.9292, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 26060] train loss: 0.6197, train acc: 0.7629, val loss: 0.4001, val acc: 0.9049  (best train acc: 0.8081, best val acc: 0.9292, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 26080] train loss: 0.6116, train acc: 0.7683, val loss: 0.3975, val acc: 0.9174  (best train acc: 0.8081, best val acc: 0.9292, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 26100] train loss: 0.6244, train acc: 0.7736, val loss: 0.4008, val acc: 0.9157  (best train acc: 0.8081, best val acc: 0.9292, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 26120] train loss: 0.6187, train acc: 0.7734, val loss: 0.3992, val acc: 0.9083  (best train acc: 0.8081, best val acc: 0.9292, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 26140] train loss: 0.6106, train acc: 0.7763, val loss: 0.3940, val acc: 0.9153  (best train acc: 0.8081, best val acc: 0.9292, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 26160] train loss: 0.5864, train acc: 0.7815, val loss: 0.4011, val acc: 0.9123  (best train acc: 0.8081, best val acc: 0.9292, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 26180] train loss: 0.5973, train acc: 0.7784, val loss: 0.4047, val acc: 0.9059  (best train acc: 0.8081, best val acc: 0.9292, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 26200] train loss: 0.6025, train acc: 0.7818, val loss: 0.4059, val acc: 0.8988  (best train acc: 0.8081, best val acc: 0.9292, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 26220] train loss: 0.6010, train acc: 0.7762, val loss: 0.3917, val acc: 0.9191  (best train acc: 0.8081, best val acc: 0.9292, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 26240] train loss: 0.5887, train acc: 0.7836, val loss: 0.4031, val acc: 0.9160  (best train acc: 0.8081, best val acc: 0.9292, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 26260] train loss: 0.5982, train acc: 0.7818, val loss: 0.4110, val acc: 0.9056  (best train acc: 0.8081, best val acc: 0.9292, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 26280] train loss: 0.6147, train acc: 0.7647, val loss: 0.3969, val acc: 0.9103  (best train acc: 0.8081, best val acc: 0.9292, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 26300] train loss: 0.5886, train acc: 0.7864, val loss: 0.3998, val acc: 0.9106  (best train acc: 0.8081, best val acc: 0.9292, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 26320] train loss: 0.5978, train acc: 0.7770, val loss: 0.4004, val acc: 0.9221  (best train acc: 0.8081, best val acc: 0.9292, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 26340] train loss: 0.6151, train acc: 0.7743, val loss: 0.3957, val acc: 0.9103  (best train acc: 0.8081, best val acc: 0.9292, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 26360] train loss: 0.6606, train acc: 0.7418, val loss: 0.4145, val acc: 0.8887  (best train acc: 0.8081, best val acc: 0.9292, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 26380] train loss: 0.6168, train acc: 0.7667, val loss: 0.4042, val acc: 0.9083  (best train acc: 0.8081, best val acc: 0.9292, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 26400] train loss: 0.5958, train acc: 0.7786, val loss: 0.4015, val acc: 0.9008  (best train acc: 0.8081, best val acc: 0.9292, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 26420] train loss: 0.6247, train acc: 0.7690, val loss: 0.4019, val acc: 0.9137  (best train acc: 0.8081, best val acc: 0.9292, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 26440] train loss: 0.6490, train acc: 0.7574, val loss: 0.4059, val acc: 0.9130  (best train acc: 0.8081, best val acc: 0.9292, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 26460] train loss: 0.6523, train acc: 0.7538, val loss: 0.5661, val acc: 0.7909  (best train acc: 0.8081, best val acc: 0.9292, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 26480] train loss: 0.7047, train acc: 0.7195, val loss: 0.5045, val acc: 0.8695  (best train acc: 0.8081, best val acc: 0.9292, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 26500] train loss: 0.6044, train acc: 0.7709, val loss: 0.4188, val acc: 0.8884  (best train acc: 0.8081, best val acc: 0.9292, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 26520] train loss: 0.5866, train acc: 0.7874, val loss: 0.4008, val acc: 0.9012  (best train acc: 0.8081, best val acc: 0.9292, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 26540] train loss: 0.6134, train acc: 0.7711, val loss: 0.4175, val acc: 0.8826  (best train acc: 0.8081, best val acc: 0.9292, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 26560] train loss: 0.6396, train acc: 0.7653, val loss: 0.4133, val acc: 0.9086  (best train acc: 0.8081, best val acc: 0.9292, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 26580] train loss: 0.5939, train acc: 0.7713, val loss: 0.4218, val acc: 0.8843  (best train acc: 0.8081, best val acc: 0.9292, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 26600] train loss: 0.5978, train acc: 0.7845, val loss: 0.4034, val acc: 0.9012  (best train acc: 0.8081, best val acc: 0.9292, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 26620] train loss: 0.6067, train acc: 0.7751, val loss: 0.4043, val acc: 0.9042  (best train acc: 0.8081, best val acc: 0.9292, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 26640] train loss: 0.6163, train acc: 0.7757, val loss: 0.4037, val acc: 0.9150  (best train acc: 0.8081, best val acc: 0.9292, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 26660] train loss: 0.6089, train acc: 0.7676, val loss: 0.4152, val acc: 0.8988  (best train acc: 0.8081, best val acc: 0.9292, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 26680] train loss: 0.5908, train acc: 0.7816, val loss: 0.4083, val acc: 0.9029  (best train acc: 0.8081, best val acc: 0.9292, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 26700] train loss: 0.5929, train acc: 0.7807, val loss: 0.4394, val acc: 0.8796  (best train acc: 0.8081, best val acc: 0.9292, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 26720] train loss: 0.6162, train acc: 0.7625, val loss: 0.4108, val acc: 0.9096  (best train acc: 0.8081, best val acc: 0.9292, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 26740] train loss: 0.6011, train acc: 0.7761, val loss: 0.3978, val acc: 0.9079  (best train acc: 0.8081, best val acc: 0.9292, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 26760] train loss: 0.6365, train acc: 0.7621, val loss: 0.4376, val acc: 0.8782  (best train acc: 0.8081, best val acc: 0.9292, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 26780] train loss: 0.6278, train acc: 0.7575, val loss: 0.4369, val acc: 0.8917  (best train acc: 0.8081, best val acc: 0.9292, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 26800] train loss: 0.6185, train acc: 0.7697, val loss: 0.3981, val acc: 0.9167  (best train acc: 0.8081, best val acc: 0.9292, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 26820] train loss: 0.6014, train acc: 0.7755, val loss: 0.3967, val acc: 0.9113  (best train acc: 0.8081, best val acc: 0.9292, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 26840] train loss: 0.6094, train acc: 0.7718, val loss: 0.4008, val acc: 0.9083  (best train acc: 0.8081, best val acc: 0.9292, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 26860] train loss: 0.5945, train acc: 0.7817, val loss: 0.4161, val acc: 0.8958  (best train acc: 0.8081, best val acc: 0.9292, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 26880] train loss: 0.5874, train acc: 0.7765, val loss: 0.4057, val acc: 0.9110  (best train acc: 0.8081, best val acc: 0.9292, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 26900] train loss: 0.5958, train acc: 0.7808, val loss: 0.4088, val acc: 0.8934  (best train acc: 0.8081, best val acc: 0.9292, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 26920] train loss: 0.5975, train acc: 0.7868, val loss: 0.3994, val acc: 0.9211  (best train acc: 0.8081, best val acc: 0.9292, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 26940] train loss: 0.5985, train acc: 0.7744, val loss: 0.4020, val acc: 0.9143  (best train acc: 0.8081, best val acc: 0.9292, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 26960] train loss: 0.5870, train acc: 0.7841, val loss: 0.3917, val acc: 0.9153  (best train acc: 0.8081, best val acc: 0.9292, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 26980] train loss: 0.5854, train acc: 0.7854, val loss: 0.3972, val acc: 0.9130  (best train acc: 0.8081, best val acc: 0.9292, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 27000] train loss: 0.5987, train acc: 0.7715, val loss: 0.4158, val acc: 0.8904  (best train acc: 0.8081, best val acc: 0.9292, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 27020] train loss: 0.5935, train acc: 0.7830, val loss: 0.3971, val acc: 0.9184  (best train acc: 0.8081, best val acc: 0.9292, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 27040] train loss: 0.5916, train acc: 0.7864, val loss: 0.4033, val acc: 0.9059  (best train acc: 0.8081, best val acc: 0.9292, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 27060] train loss: 0.6016, train acc: 0.7711, val loss: 0.3987, val acc: 0.9089  (best train acc: 0.8081, best val acc: 0.9292, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 27080] train loss: 0.6324, train acc: 0.7619, val loss: 0.4143, val acc: 0.8941  (best train acc: 0.8081, best val acc: 0.9292, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 27100] train loss: 0.6750, train acc: 0.7404, val loss: 0.4823, val acc: 0.8577  (best train acc: 0.8081, best val acc: 0.9292, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 27120] train loss: 0.6147, train acc: 0.7629, val loss: 0.3985, val acc: 0.8981  (best train acc: 0.8081, best val acc: 0.9292, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 27140] train loss: 0.5962, train acc: 0.7749, val loss: 0.3947, val acc: 0.8998  (best train acc: 0.8081, best val acc: 0.9292, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 27160] train loss: 0.6042, train acc: 0.7778, val loss: 0.4027, val acc: 0.8958  (best train acc: 0.8081, best val acc: 0.9292, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 27180] train loss: 0.6305, train acc: 0.7554, val loss: 0.4058, val acc: 0.9133  (best train acc: 0.8081, best val acc: 0.9292, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 27200] train loss: 0.6242, train acc: 0.7629, val loss: 0.4166, val acc: 0.8870  (best train acc: 0.8081, best val acc: 0.9292, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 27220] train loss: 0.6124, train acc: 0.7716, val loss: 0.4118, val acc: 0.9069  (best train acc: 0.8081, best val acc: 0.9292, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 27240] train loss: 0.6244, train acc: 0.7616, val loss: 0.4179, val acc: 0.9218  (best train acc: 0.8081, best val acc: 0.9292, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 27260] train loss: 0.6189, train acc: 0.7726, val loss: 0.4242, val acc: 0.8803  (best train acc: 0.8081, best val acc: 0.9292, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 27280] train loss: 0.5942, train acc: 0.7809, val loss: 0.4112, val acc: 0.8944  (best train acc: 0.8081, best val acc: 0.9292, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 27300] train loss: 0.6589, train acc: 0.7420, val loss: 0.4301, val acc: 0.8904  (best train acc: 0.8081, best val acc: 0.9292, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 27320] train loss: 0.5915, train acc: 0.7833, val loss: 0.4018, val acc: 0.9194  (best train acc: 0.8081, best val acc: 0.9292, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 27340] train loss: 0.5933, train acc: 0.7811, val loss: 0.3962, val acc: 0.9221  (best train acc: 0.8081, best val acc: 0.9292, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 27360] train loss: 0.5955, train acc: 0.7771, val loss: 0.4050, val acc: 0.9073  (best train acc: 0.8081, best val acc: 0.9292, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 27380] train loss: 0.6021, train acc: 0.7719, val loss: 0.3954, val acc: 0.9180  (best train acc: 0.8081, best val acc: 0.9292, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 27400] train loss: 0.6002, train acc: 0.7784, val loss: 0.3931, val acc: 0.9177  (best train acc: 0.8081, best val acc: 0.9292, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 27420] train loss: 0.5985, train acc: 0.7704, val loss: 0.4033, val acc: 0.9106  (best train acc: 0.8081, best val acc: 0.9292, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 27440] train loss: 0.5934, train acc: 0.7773, val loss: 0.4083, val acc: 0.8978  (best train acc: 0.8081, best val acc: 0.9292, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 27460] train loss: 0.6187, train acc: 0.7695, val loss: 0.4286, val acc: 0.8911  (best train acc: 0.8081, best val acc: 0.9292, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 27480] train loss: 0.6055, train acc: 0.7776, val loss: 0.4047, val acc: 0.8985  (best train acc: 0.8081, best val acc: 0.9292, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 27500] train loss: 0.5875, train acc: 0.7864, val loss: 0.4098, val acc: 0.8985  (best train acc: 0.8081, best val acc: 0.9292, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 27520] train loss: 0.6421, train acc: 0.7587, val loss: 0.4076, val acc: 0.9126  (best train acc: 0.8081, best val acc: 0.9292, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 27540] train loss: 0.5869, train acc: 0.7808, val loss: 0.3959, val acc: 0.9150  (best train acc: 0.8081, best val acc: 0.9292, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 27560] train loss: 0.6042, train acc: 0.7699, val loss: 0.4007, val acc: 0.9076  (best train acc: 0.8081, best val acc: 0.9292, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 27580] train loss: 0.6031, train acc: 0.7809, val loss: 0.4001, val acc: 0.9157  (best train acc: 0.8081, best val acc: 0.9292, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 27600] train loss: 0.5949, train acc: 0.7770, val loss: 0.3946, val acc: 0.9120  (best train acc: 0.8081, best val acc: 0.9292, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 27620] train loss: 0.5966, train acc: 0.7820, val loss: 0.4104, val acc: 0.8941  (best train acc: 0.8081, best val acc: 0.9292, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 27640] train loss: 0.5888, train acc: 0.7798, val loss: 0.3956, val acc: 0.9218  (best train acc: 0.8081, best val acc: 0.9292, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 27660] train loss: 0.6053, train acc: 0.7726, val loss: 0.4343, val acc: 0.8860  (best train acc: 0.8081, best val acc: 0.9292, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 27680] train loss: 0.6876, train acc: 0.7290, val loss: 0.5154, val acc: 0.8637  (best train acc: 0.8081, best val acc: 0.9292, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 27700] train loss: 0.6223, train acc: 0.7580, val loss: 0.4439, val acc: 0.8718  (best train acc: 0.8081, best val acc: 0.9292, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 27720] train loss: 0.6207, train acc: 0.7616, val loss: 0.3935, val acc: 0.9184  (best train acc: 0.8081, best val acc: 0.9292, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 27740] train loss: 0.5946, train acc: 0.7880, val loss: 0.3966, val acc: 0.9150  (best train acc: 0.8081, best val acc: 0.9292, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 27760] train loss: 0.6007, train acc: 0.7786, val loss: 0.3976, val acc: 0.9079  (best train acc: 0.8081, best val acc: 0.9292, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 27780] train loss: 0.5802, train acc: 0.7821, val loss: 0.3950, val acc: 0.9218  (best train acc: 0.8081, best val acc: 0.9292, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 27800] train loss: 0.5898, train acc: 0.7800, val loss: 0.3983, val acc: 0.9137  (best train acc: 0.8081, best val acc: 0.9292, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 27820] train loss: 0.5881, train acc: 0.7734, val loss: 0.3998, val acc: 0.9046  (best train acc: 0.8081, best val acc: 0.9292, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 27840] train loss: 0.5961, train acc: 0.7732, val loss: 0.3929, val acc: 0.9191  (best train acc: 0.8081, best val acc: 0.9292, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 27860] train loss: 0.5897, train acc: 0.7809, val loss: 0.3933, val acc: 0.9228  (best train acc: 0.8081, best val acc: 0.9292, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 27880] train loss: 0.6091, train acc: 0.7743, val loss: 0.3914, val acc: 0.9187  (best train acc: 0.8081, best val acc: 0.9302, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 27900] train loss: 0.5901, train acc: 0.7787, val loss: 0.3975, val acc: 0.9059  (best train acc: 0.8081, best val acc: 0.9302, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 27920] train loss: 0.5906, train acc: 0.7838, val loss: 0.4033, val acc: 0.9221  (best train acc: 0.8081, best val acc: 0.9302, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 27940] train loss: 0.6170, train acc: 0.7511, val loss: 0.4477, val acc: 0.8550  (best train acc: 0.8081, best val acc: 0.9305, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 27960] train loss: 0.6032, train acc: 0.7723, val loss: 0.4138, val acc: 0.8904  (best train acc: 0.8081, best val acc: 0.9305, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 27980] train loss: 0.6061, train acc: 0.7692, val loss: 0.3856, val acc: 0.9079  (best train acc: 0.8081, best val acc: 0.9305, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 28000] train loss: 0.5947, train acc: 0.7713, val loss: 0.4161, val acc: 0.8907  (best train acc: 0.8081, best val acc: 0.9305, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 28020] train loss: 0.6897, train acc: 0.7347, val loss: 0.5137, val acc: 0.8583  (best train acc: 0.8081, best val acc: 0.9305, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 28040] train loss: 0.6087, train acc: 0.7747, val loss: 0.4222, val acc: 0.8927  (best train acc: 0.8081, best val acc: 0.9305, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 28060] train loss: 0.5761, train acc: 0.7851, val loss: 0.3843, val acc: 0.9251  (best train acc: 0.8081, best val acc: 0.9305, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 28080] train loss: 0.5887, train acc: 0.7854, val loss: 0.3940, val acc: 0.9113  (best train acc: 0.8081, best val acc: 0.9305, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 28100] train loss: 0.5704, train acc: 0.7895, val loss: 0.3932, val acc: 0.9167  (best train acc: 0.8081, best val acc: 0.9305, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 28120] train loss: 0.5710, train acc: 0.7840, val loss: 0.3890, val acc: 0.9251  (best train acc: 0.8081, best val acc: 0.9305, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 28140] train loss: 0.5808, train acc: 0.7843, val loss: 0.3929, val acc: 0.9251  (best train acc: 0.8081, best val acc: 0.9305, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 28160] train loss: 0.6016, train acc: 0.7778, val loss: 0.4114, val acc: 0.9022  (best train acc: 0.8081, best val acc: 0.9305, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 28180] train loss: 0.6079, train acc: 0.7711, val loss: 0.4105, val acc: 0.8927  (best train acc: 0.8081, best val acc: 0.9339, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 28200] train loss: 0.6091, train acc: 0.7709, val loss: 0.3938, val acc: 0.9191  (best train acc: 0.8081, best val acc: 0.9339, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 28220] train loss: 0.5980, train acc: 0.7723, val loss: 0.3834, val acc: 0.9285  (best train acc: 0.8081, best val acc: 0.9339, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 28240] train loss: 0.6104, train acc: 0.7563, val loss: 0.4445, val acc: 0.8570  (best train acc: 0.8081, best val acc: 0.9339, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 28260] train loss: 0.6031, train acc: 0.7663, val loss: 0.3846, val acc: 0.9035  (best train acc: 0.8081, best val acc: 0.9339, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 28280] train loss: 0.5905, train acc: 0.7821, val loss: 0.3825, val acc: 0.9255  (best train acc: 0.8081, best val acc: 0.9339, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 28300] train loss: 0.6076, train acc: 0.7795, val loss: 0.4005, val acc: 0.9187  (best train acc: 0.8081, best val acc: 0.9339, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 28320] train loss: 0.6422, train acc: 0.7624, val loss: 0.4716, val acc: 0.8344  (best train acc: 0.8081, best val acc: 0.9339, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 28340] train loss: 0.5970, train acc: 0.7897, val loss: 0.4170, val acc: 0.8884  (best train acc: 0.8081, best val acc: 0.9339, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 28360] train loss: 0.6056, train acc: 0.7780, val loss: 0.3864, val acc: 0.9170  (best train acc: 0.8081, best val acc: 0.9339, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 28380] train loss: 0.5955, train acc: 0.7764, val loss: 0.3730, val acc: 0.9197  (best train acc: 0.8081, best val acc: 0.9339, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 28400] train loss: 0.6572, train acc: 0.7487, val loss: 0.5099, val acc: 0.8169  (best train acc: 0.8081, best val acc: 0.9339, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 28420] train loss: 0.5966, train acc: 0.7857, val loss: 0.4140, val acc: 0.8998  (best train acc: 0.8081, best val acc: 0.9339, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 28440] train loss: 0.5852, train acc: 0.7891, val loss: 0.3865, val acc: 0.9069  (best train acc: 0.8081, best val acc: 0.9339, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 28460] train loss: 0.5763, train acc: 0.7930, val loss: 0.3719, val acc: 0.9170  (best train acc: 0.8081, best val acc: 0.9339, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 28480] train loss: 0.6201, train acc: 0.7461, val loss: 0.4294, val acc: 0.8951  (best train acc: 0.8081, best val acc: 0.9339, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 28500] train loss: 0.5669, train acc: 0.7837, val loss: 0.3823, val acc: 0.9160  (best train acc: 0.8081, best val acc: 0.9339, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 28520] train loss: 0.5831, train acc: 0.7793, val loss: 0.3755, val acc: 0.9241  (best train acc: 0.8081, best val acc: 0.9339, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 28540] train loss: 0.5751, train acc: 0.7879, val loss: 0.4003, val acc: 0.8944  (best train acc: 0.8081, best val acc: 0.9339, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 28560] train loss: 0.6038, train acc: 0.7753, val loss: 0.3699, val acc: 0.9309  (best train acc: 0.8081, best val acc: 0.9346, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 28580] train loss: 0.6103, train acc: 0.7679, val loss: 0.4102, val acc: 0.8735  (best train acc: 0.8081, best val acc: 0.9369, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 28600] train loss: 0.5791, train acc: 0.7760, val loss: 0.4164, val acc: 0.8678  (best train acc: 0.8081, best val acc: 0.9369, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 28620] train loss: 0.6013, train acc: 0.7762, val loss: 0.4053, val acc: 0.8853  (best train acc: 0.8081, best val acc: 0.9369, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 28640] train loss: 0.5829, train acc: 0.7872, val loss: 0.4051, val acc: 0.9093  (best train acc: 0.8081, best val acc: 0.9369, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 28660] train loss: 0.5885, train acc: 0.7882, val loss: 0.4250, val acc: 0.9002  (best train acc: 0.8081, best val acc: 0.9369, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 28680] train loss: 0.5897, train acc: 0.7749, val loss: 0.3778, val acc: 0.9170  (best train acc: 0.8081, best val acc: 0.9369, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 28700] train loss: 0.5837, train acc: 0.7759, val loss: 0.3802, val acc: 0.9147  (best train acc: 0.8081, best val acc: 0.9369, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 28720] train loss: 0.5753, train acc: 0.7955, val loss: 0.3764, val acc: 0.9234  (best train acc: 0.8081, best val acc: 0.9369, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 28740] train loss: 0.6354, train acc: 0.7606, val loss: 0.4902, val acc: 0.8199  (best train acc: 0.8081, best val acc: 0.9369, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 28760] train loss: 0.6997, train acc: 0.7264, val loss: 0.5625, val acc: 0.8283  (best train acc: 0.8081, best val acc: 0.9369, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 28780] train loss: 0.6347, train acc: 0.7407, val loss: 0.4023, val acc: 0.9029  (best train acc: 0.8081, best val acc: 0.9369, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 28800] train loss: 0.5784, train acc: 0.7856, val loss: 0.3862, val acc: 0.9150  (best train acc: 0.8081, best val acc: 0.9369, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 28820] train loss: 0.5940, train acc: 0.7816, val loss: 0.4104, val acc: 0.8927  (best train acc: 0.8081, best val acc: 0.9369, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 28840] train loss: 0.6857, train acc: 0.7376, val loss: 0.4970, val acc: 0.8664  (best train acc: 0.8081, best val acc: 0.9369, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 28860] train loss: 0.6147, train acc: 0.7662, val loss: 0.3982, val acc: 0.9066  (best train acc: 0.8081, best val acc: 0.9369, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 28880] train loss: 0.5898, train acc: 0.7812, val loss: 0.3762, val acc: 0.9177  (best train acc: 0.8081, best val acc: 0.9369, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 28900] train loss: 0.5898, train acc: 0.7837, val loss: 0.3664, val acc: 0.9329  (best train acc: 0.8081, best val acc: 0.9369, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 28920] train loss: 0.5926, train acc: 0.7634, val loss: 0.3964, val acc: 0.8981  (best train acc: 0.8081, best val acc: 0.9403, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 28940] train loss: 0.5973, train acc: 0.7823, val loss: 0.3841, val acc: 0.9103  (best train acc: 0.8081, best val acc: 0.9403, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 28960] train loss: 0.5758, train acc: 0.7932, val loss: 0.3763, val acc: 0.9298  (best train acc: 0.8081, best val acc: 0.9403, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 28980] train loss: 0.5822, train acc: 0.7812, val loss: 0.3745, val acc: 0.9093  (best train acc: 0.8081, best val acc: 0.9410, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 29000] train loss: 0.7090, train acc: 0.7285, val loss: 0.4809, val acc: 0.8563  (best train acc: 0.8081, best val acc: 0.9410, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 29020] train loss: 0.6495, train acc: 0.7422, val loss: 0.4100, val acc: 0.9224  (best train acc: 0.8081, best val acc: 0.9410, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 29040] train loss: 0.6008, train acc: 0.7686, val loss: 0.3738, val acc: 0.9177  (best train acc: 0.8081, best val acc: 0.9410, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 29060] train loss: 0.5783, train acc: 0.7805, val loss: 0.3627, val acc: 0.9390  (best train acc: 0.8081, best val acc: 0.9410, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 29080] train loss: 0.6086, train acc: 0.7593, val loss: 0.4055, val acc: 0.9184  (best train acc: 0.8081, best val acc: 0.9417, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 29100] train loss: 0.5609, train acc: 0.7997, val loss: 0.3763, val acc: 0.9359  (best train acc: 0.8081, best val acc: 0.9417, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 29120] train loss: 0.5885, train acc: 0.7818, val loss: 0.3726, val acc: 0.9130  (best train acc: 0.8081, best val acc: 0.9417, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 29140] train loss: 0.5837, train acc: 0.7789, val loss: 0.3689, val acc: 0.9238  (best train acc: 0.8081, best val acc: 0.9487, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 29160] train loss: 0.6195, train acc: 0.7572, val loss: 0.4504, val acc: 0.8961  (best train acc: 0.8081, best val acc: 0.9487, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 29180] train loss: 0.5908, train acc: 0.7775, val loss: 0.3720, val acc: 0.9194  (best train acc: 0.8081, best val acc: 0.9487, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 29200] train loss: 0.5727, train acc: 0.7910, val loss: 0.3664, val acc: 0.9339  (best train acc: 0.8081, best val acc: 0.9487, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 29220] train loss: 0.6322, train acc: 0.7636, val loss: 0.4168, val acc: 0.9022  (best train acc: 0.8081, best val acc: 0.9487, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 29240] train loss: 0.6913, train acc: 0.7008, val loss: 0.5079, val acc: 0.8617  (best train acc: 0.8081, best val acc: 0.9487, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 29260] train loss: 0.6373, train acc: 0.7641, val loss: 0.4109, val acc: 0.9224  (best train acc: 0.8081, best val acc: 0.9487, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 29280] train loss: 0.5972, train acc: 0.7754, val loss: 0.3897, val acc: 0.9025  (best train acc: 0.8081, best val acc: 0.9487, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 29300] train loss: 0.5794, train acc: 0.7861, val loss: 0.3716, val acc: 0.9224  (best train acc: 0.8081, best val acc: 0.9487, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 29320] train loss: 0.5500, train acc: 0.7950, val loss: 0.3681, val acc: 0.9369  (best train acc: 0.8081, best val acc: 0.9487, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 29340] train loss: 0.5588, train acc: 0.7942, val loss: 0.3760, val acc: 0.9346  (best train acc: 0.8081, best val acc: 0.9487, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 29360] train loss: 0.5623, train acc: 0.7914, val loss: 0.3825, val acc: 0.9207  (best train acc: 0.8081, best val acc: 0.9521, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 29380] train loss: 0.5966, train acc: 0.7802, val loss: 0.4515, val acc: 0.8476  (best train acc: 0.8081, best val acc: 0.9521, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 29400] train loss: 0.6798, train acc: 0.7438, val loss: 0.4890, val acc: 0.8229  (best train acc: 0.8081, best val acc: 0.9521, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 29420] train loss: 0.6085, train acc: 0.7698, val loss: 0.3984, val acc: 0.9251  (best train acc: 0.8081, best val acc: 0.9521, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 29440] train loss: 0.5782, train acc: 0.7835, val loss: 0.3790, val acc: 0.9103  (best train acc: 0.8081, best val acc: 0.9521, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 29460] train loss: 0.5876, train acc: 0.7838, val loss: 0.3714, val acc: 0.9089  (best train acc: 0.8081, best val acc: 0.9521, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 29480] train loss: 0.5748, train acc: 0.7871, val loss: 0.4145, val acc: 0.8728  (best train acc: 0.8081, best val acc: 0.9521, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 29500] train loss: 0.5798, train acc: 0.7811, val loss: 0.3661, val acc: 0.9302  (best train acc: 0.8081, best val acc: 0.9521, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 29520] train loss: 0.5686, train acc: 0.7929, val loss: 0.3696, val acc: 0.9322  (best train acc: 0.8081, best val acc: 0.9521, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 29540] train loss: 0.5781, train acc: 0.7805, val loss: 0.3707, val acc: 0.9447  (best train acc: 0.8081, best val acc: 0.9521, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 29560] train loss: 0.5815, train acc: 0.7716, val loss: 0.3878, val acc: 0.9012  (best train acc: 0.8081, best val acc: 0.9521, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 29580] train loss: 0.5848, train acc: 0.7791, val loss: 0.4827, val acc: 0.8216  (best train acc: 0.8081, best val acc: 0.9521, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 29600] train loss: 0.6904, train acc: 0.7369, val loss: 0.4955, val acc: 0.8644  (best train acc: 0.8081, best val acc: 0.9521, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 29620] train loss: 0.5700, train acc: 0.7880, val loss: 0.3929, val acc: 0.9120  (best train acc: 0.8081, best val acc: 0.9521, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 29640] train loss: 0.5832, train acc: 0.7863, val loss: 0.3572, val acc: 0.9275  (best train acc: 0.8081, best val acc: 0.9521, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 29660] train loss: 0.5584, train acc: 0.7961, val loss: 0.3653, val acc: 0.9275  (best train acc: 0.8081, best val acc: 0.9521, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 29680] train loss: 0.5833, train acc: 0.7757, val loss: 0.4107, val acc: 0.9123  (best train acc: 0.8081, best val acc: 0.9521, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 29700] train loss: 0.5811, train acc: 0.7875, val loss: 0.3743, val acc: 0.9113  (best train acc: 0.8081, best val acc: 0.9521, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 29720] train loss: 0.5981, train acc: 0.7671, val loss: 0.3980, val acc: 0.8877  (best train acc: 0.8081, best val acc: 0.9521, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 29740] train loss: 0.5899, train acc: 0.7676, val loss: 0.3763, val acc: 0.8968  (best train acc: 0.8081, best val acc: 0.9521, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 29760] train loss: 0.5625, train acc: 0.7888, val loss: 0.3760, val acc: 0.9126  (best train acc: 0.8081, best val acc: 0.9521, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 29780] train loss: 0.5764, train acc: 0.7572, val loss: 0.3776, val acc: 0.9167  (best train acc: 0.8081, best val acc: 0.9521, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 29800] train loss: 0.5752, train acc: 0.7922, val loss: 0.3975, val acc: 0.8951  (best train acc: 0.8081, best val acc: 0.9521, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 29820] train loss: 0.5984, train acc: 0.7745, val loss: 0.3959, val acc: 0.9066  (best train acc: 0.8081, best val acc: 0.9521, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 29840] train loss: 0.5838, train acc: 0.7840, val loss: 0.4135, val acc: 0.8762  (best train acc: 0.8081, best val acc: 0.9521, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 29860] train loss: 0.5628, train acc: 0.7861, val loss: 0.3725, val acc: 0.9126  (best train acc: 0.8081, best val acc: 0.9521, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 29880] train loss: 0.5755, train acc: 0.7933, val loss: 0.3853, val acc: 0.9137  (best train acc: 0.8081, best val acc: 0.9521, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 29900] train loss: 0.6945, train acc: 0.7312, val loss: 0.4168, val acc: 0.8806  (best train acc: 0.8081, best val acc: 0.9521, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 29920] train loss: 0.5788, train acc: 0.7804, val loss: 0.3832, val acc: 0.9184  (best train acc: 0.8081, best val acc: 0.9521, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 29940] train loss: 0.5719, train acc: 0.7831, val loss: 0.3643, val acc: 0.9180  (best train acc: 0.8081, best val acc: 0.9521, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 29960] train loss: 0.5659, train acc: 0.7774, val loss: 0.3582, val acc: 0.9191  (best train acc: 0.8081, best val acc: 0.9521, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 29980] train loss: 0.5984, train acc: 0.7616, val loss: 0.4544, val acc: 0.8664  (best train acc: 0.8081, best val acc: 0.9521, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 30000] train loss: 0.6640, train acc: 0.7367, val loss: 0.4909, val acc: 0.8637  (best train acc: 0.8081, best val acc: 0.9521, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 30020] train loss: 0.5875, train acc: 0.7740, val loss: 0.3827, val acc: 0.9339  (best train acc: 0.8081, best val acc: 0.9521, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 30040] train loss: 0.5728, train acc: 0.7799, val loss: 0.3669, val acc: 0.9113  (best train acc: 0.8081, best val acc: 0.9521, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 30060] train loss: 0.6248, train acc: 0.7353, val loss: 0.3965, val acc: 0.8941  (best train acc: 0.8081, best val acc: 0.9521, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 30080] train loss: 0.5552, train acc: 0.7883, val loss: 0.3856, val acc: 0.9012  (best train acc: 0.8081, best val acc: 0.9521, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 30100] train loss: 0.5832, train acc: 0.7877, val loss: 0.3839, val acc: 0.9403  (best train acc: 0.8081, best val acc: 0.9521, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 30120] train loss: 0.5609, train acc: 0.7888, val loss: 0.3832, val acc: 0.8934  (best train acc: 0.8081, best val acc: 0.9521, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 30140] train loss: 0.5759, train acc: 0.7886, val loss: 0.3813, val acc: 0.9110  (best train acc: 0.8081, best val acc: 0.9521, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 30160] train loss: 0.5544, train acc: 0.7946, val loss: 0.3822, val acc: 0.9238  (best train acc: 0.8081, best val acc: 0.9521, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 30180] train loss: 0.5784, train acc: 0.7693, val loss: 0.3800, val acc: 0.9042  (best train acc: 0.8081, best val acc: 0.9521, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 30200] train loss: 0.5677, train acc: 0.7903, val loss: 0.3732, val acc: 0.9008  (best train acc: 0.8081, best val acc: 0.9521, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 30220] train loss: 0.6748, train acc: 0.7317, val loss: 0.3867, val acc: 0.9153  (best train acc: 0.8081, best val acc: 0.9521, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 30240] train loss: 0.5985, train acc: 0.7739, val loss: 0.3858, val acc: 0.8954  (best train acc: 0.8081, best val acc: 0.9521, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 30260] train loss: 0.5625, train acc: 0.7904, val loss: 0.3723, val acc: 0.9123  (best train acc: 0.8081, best val acc: 0.9521, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 30280] train loss: 0.5891, train acc: 0.7734, val loss: 0.3517, val acc: 0.9214  (best train acc: 0.8081, best val acc: 0.9521, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 30300] train loss: 0.5944, train acc: 0.7691, val loss: 0.4220, val acc: 0.8924  (best train acc: 0.8081, best val acc: 0.9521, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 30320] train loss: 0.6205, train acc: 0.7723, val loss: 0.3770, val acc: 0.9346  (best train acc: 0.8081, best val acc: 0.9521, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 30340] train loss: 0.8173, train acc: 0.7041, val loss: 0.4960, val acc: 0.8378  (best train acc: 0.8081, best val acc: 0.9521, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 30360] train loss: 0.6212, train acc: 0.7651, val loss: 0.4233, val acc: 0.8961  (best train acc: 0.8081, best val acc: 0.9521, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 30380] train loss: 0.5596, train acc: 0.7909, val loss: 0.3631, val acc: 0.9403  (best train acc: 0.8081, best val acc: 0.9521, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 30400] train loss: 0.5598, train acc: 0.7979, val loss: 0.3492, val acc: 0.9487  (best train acc: 0.8081, best val acc: 0.9521, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 30420] train loss: 0.5673, train acc: 0.7835, val loss: 0.3912, val acc: 0.9245  (best train acc: 0.8081, best val acc: 0.9521, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 30440] train loss: 0.6126, train acc: 0.7582, val loss: 0.4144, val acc: 0.9113  (best train acc: 0.8081, best val acc: 0.9521, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 30460] train loss: 0.5991, train acc: 0.7740, val loss: 0.4155, val acc: 0.9032  (best train acc: 0.8081, best val acc: 0.9521, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 30480] train loss: 0.5833, train acc: 0.7928, val loss: 0.3554, val acc: 0.9336  (best train acc: 0.8081, best val acc: 0.9521, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 30500] train loss: 0.5633, train acc: 0.7893, val loss: 0.3491, val acc: 0.9514  (best train acc: 0.8081, best val acc: 0.9521, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 30520] train loss: 0.5898, train acc: 0.7854, val loss: 0.4154, val acc: 0.8755  (best train acc: 0.8081, best val acc: 0.9521, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 30540] train loss: 0.5807, train acc: 0.7799, val loss: 0.3808, val acc: 0.9052  (best train acc: 0.8081, best val acc: 0.9521, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 30560] train loss: 0.5439, train acc: 0.8034, val loss: 0.3508, val acc: 0.9383  (best train acc: 0.8081, best val acc: 0.9521, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 30580] train loss: 0.5895, train acc: 0.7760, val loss: 0.3603, val acc: 0.9062  (best train acc: 0.8081, best val acc: 0.9521, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 30600] train loss: 0.5908, train acc: 0.7788, val loss: 0.3664, val acc: 0.9066  (best train acc: 0.8081, best val acc: 0.9521, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 30620] train loss: 0.5706, train acc: 0.7999, val loss: 0.3813, val acc: 0.9197  (best train acc: 0.8081, best val acc: 0.9521, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 30640] train loss: 0.5917, train acc: 0.7510, val loss: 0.3925, val acc: 0.8826  (best train acc: 0.8081, best val acc: 0.9548, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 30660] train loss: 0.6081, train acc: 0.7571, val loss: 0.3873, val acc: 0.9150  (best train acc: 0.8081, best val acc: 0.9548, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 30680] train loss: 0.5687, train acc: 0.7960, val loss: 0.3496, val acc: 0.9356  (best train acc: 0.8081, best val acc: 0.9548, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 30700] train loss: 0.6391, train acc: 0.7285, val loss: 0.3916, val acc: 0.9025  (best train acc: 0.8081, best val acc: 0.9548, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 30720] train loss: 0.6529, train acc: 0.7384, val loss: 0.4306, val acc: 0.8779  (best train acc: 0.8081, best val acc: 0.9548, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 30740] train loss: 0.5925, train acc: 0.7880, val loss: 0.4042, val acc: 0.8914  (best train acc: 0.8081, best val acc: 0.9548, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 30760] train loss: 0.6047, train acc: 0.7735, val loss: 0.4300, val acc: 0.8830  (best train acc: 0.8081, best val acc: 0.9548, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 30780] train loss: 0.5834, train acc: 0.7637, val loss: 0.3794, val acc: 0.9086  (best train acc: 0.8081, best val acc: 0.9548, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 30800] train loss: 0.6077, train acc: 0.7436, val loss: 0.4054, val acc: 0.8863  (best train acc: 0.8081, best val acc: 0.9548, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 30820] train loss: 0.6014, train acc: 0.7668, val loss: 0.3686, val acc: 0.9255  (best train acc: 0.8081, best val acc: 0.9548, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 30840] train loss: 0.6097, train acc: 0.7701, val loss: 0.4001, val acc: 0.8863  (best train acc: 0.8081, best val acc: 0.9548, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 30860] train loss: 0.5754, train acc: 0.7885, val loss: 0.3863, val acc: 0.9214  (best train acc: 0.8081, best val acc: 0.9548, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 30880] train loss: 0.5698, train acc: 0.7889, val loss: 0.3617, val acc: 0.9302  (best train acc: 0.8081, best val acc: 0.9548, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 30900] train loss: 0.5668, train acc: 0.7866, val loss: 0.3707, val acc: 0.9288  (best train acc: 0.8081, best val acc: 0.9548, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 30920] train loss: 0.5828, train acc: 0.7704, val loss: 0.3961, val acc: 0.8793  (best train acc: 0.8081, best val acc: 0.9548, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 30940] train loss: 0.6568, train acc: 0.7381, val loss: 0.4512, val acc: 0.8755  (best train acc: 0.8081, best val acc: 0.9548, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 30960] train loss: 0.5743, train acc: 0.7918, val loss: 0.3775, val acc: 0.9238  (best train acc: 0.8081, best val acc: 0.9548, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 30980] train loss: 0.5815, train acc: 0.7820, val loss: 0.3644, val acc: 0.9106  (best train acc: 0.8081, best val acc: 0.9548, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 31000] train loss: 0.5803, train acc: 0.7716, val loss: 0.4873, val acc: 0.8580  (best train acc: 0.8081, best val acc: 0.9548, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 31020] train loss: 0.6077, train acc: 0.7530, val loss: 0.3806, val acc: 0.9245  (best train acc: 0.8081, best val acc: 0.9548, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 31040] train loss: 0.5773, train acc: 0.7862, val loss: 0.3724, val acc: 0.9130  (best train acc: 0.8081, best val acc: 0.9548, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 31060] train loss: 0.5750, train acc: 0.7901, val loss: 0.3635, val acc: 0.9049  (best train acc: 0.8081, best val acc: 0.9555, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 31080] train loss: 0.7081, train acc: 0.7376, val loss: 0.5003, val acc: 0.8492  (best train acc: 0.8081, best val acc: 0.9555, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 31100] train loss: 0.7797, train acc: 0.6709, val loss: 0.5366, val acc: 0.8695  (best train acc: 0.8081, best val acc: 0.9555, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 31120] train loss: 0.5785, train acc: 0.7777, val loss: 0.3863, val acc: 0.9066  (best train acc: 0.8081, best val acc: 0.9555, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 31140] train loss: 0.5785, train acc: 0.7890, val loss: 0.3533, val acc: 0.9207  (best train acc: 0.8081, best val acc: 0.9555, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 31160] train loss: 0.5713, train acc: 0.7793, val loss: 0.3498, val acc: 0.9393  (best train acc: 0.8081, best val acc: 0.9555, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 31180] train loss: 0.6201, train acc: 0.7358, val loss: 0.4134, val acc: 0.8614  (best train acc: 0.8081, best val acc: 0.9555, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 31200] train loss: 0.5629, train acc: 0.7923, val loss: 0.3529, val acc: 0.9467  (best train acc: 0.8081, best val acc: 0.9555, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 31220] train loss: 0.5708, train acc: 0.7892, val loss: 0.3728, val acc: 0.9160  (best train acc: 0.8081, best val acc: 0.9555, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 31240] train loss: 0.5919, train acc: 0.7787, val loss: 0.3620, val acc: 0.9184  (best train acc: 0.8081, best val acc: 0.9555, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 31260] train loss: 0.6722, train acc: 0.7427, val loss: 0.4161, val acc: 0.8759  (best train acc: 0.8081, best val acc: 0.9555, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 31280] train loss: 0.6693, train acc: 0.7533, val loss: 0.4768, val acc: 0.8708  (best train acc: 0.8081, best val acc: 0.9555, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 31300] train loss: 0.5953, train acc: 0.7712, val loss: 0.3965, val acc: 0.8877  (best train acc: 0.8081, best val acc: 0.9555, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 31320] train loss: 0.5859, train acc: 0.7662, val loss: 0.4072, val acc: 0.9191  (best train acc: 0.8081, best val acc: 0.9555, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 31340] train loss: 0.5743, train acc: 0.7971, val loss: 0.3545, val acc: 0.9241  (best train acc: 0.8081, best val acc: 0.9555, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 31360] train loss: 0.5821, train acc: 0.7872, val loss: 0.3708, val acc: 0.9086  (best train acc: 0.8081, best val acc: 0.9555, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 31380] train loss: 0.5948, train acc: 0.7641, val loss: 0.3832, val acc: 0.9234  (best train acc: 0.8081, best val acc: 0.9555, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 31400] train loss: 0.5888, train acc: 0.7818, val loss: 0.3828, val acc: 0.9325  (best train acc: 0.8081, best val acc: 0.9555, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 31420] train loss: 0.5905, train acc: 0.7444, val loss: 0.3886, val acc: 0.8934  (best train acc: 0.8081, best val acc: 0.9555, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 31440] train loss: 0.5830, train acc: 0.7814, val loss: 0.3700, val acc: 0.9248  (best train acc: 0.8081, best val acc: 0.9555, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 31460] train loss: 0.5669, train acc: 0.7992, val loss: 0.3619, val acc: 0.9275  (best train acc: 0.8081, best val acc: 0.9555, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 31480] train loss: 0.5886, train acc: 0.7623, val loss: 0.3836, val acc: 0.8975  (best train acc: 0.8081, best val acc: 0.9575, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 31500] train loss: 0.5762, train acc: 0.7779, val loss: 0.3847, val acc: 0.9032  (best train acc: 0.8081, best val acc: 0.9575, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 31520] train loss: 0.5700, train acc: 0.7909, val loss: 0.3637, val acc: 0.9191  (best train acc: 0.8081, best val acc: 0.9575, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 31540] train loss: 0.5616, train acc: 0.7976, val loss: 0.3433, val acc: 0.9427  (best train acc: 0.8081, best val acc: 0.9575, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 31560] train loss: 0.5842, train acc: 0.7736, val loss: 0.3700, val acc: 0.9123  (best train acc: 0.8081, best val acc: 0.9575, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 31580] train loss: 0.6746, train acc: 0.7410, val loss: 0.4411, val acc: 0.8782  (best train acc: 0.8081, best val acc: 0.9575, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 31600] train loss: 0.5894, train acc: 0.7637, val loss: 0.3626, val acc: 0.9218  (best train acc: 0.8081, best val acc: 0.9575, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 31620] train loss: 0.5623, train acc: 0.7848, val loss: 0.3788, val acc: 0.9437  (best train acc: 0.8081, best val acc: 0.9575, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 31640] train loss: 0.6235, train acc: 0.7329, val loss: 0.4579, val acc: 0.8671  (best train acc: 0.8081, best val acc: 0.9575, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 31660] train loss: 0.5875, train acc: 0.7655, val loss: 0.4178, val acc: 0.8944  (best train acc: 0.8081, best val acc: 0.9575, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 31680] train loss: 0.5691, train acc: 0.7904, val loss: 0.3399, val acc: 0.9494  (best train acc: 0.8081, best val acc: 0.9575, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 31700] train loss: 0.5684, train acc: 0.7963, val loss: 0.3727, val acc: 0.9258  (best train acc: 0.8081, best val acc: 0.9575, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 31720] train loss: 0.6033, train acc: 0.7507, val loss: 0.3730, val acc: 0.9083  (best train acc: 0.8081, best val acc: 0.9575, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 31740] train loss: 0.5518, train acc: 0.7986, val loss: 0.3491, val acc: 0.9201  (best train acc: 0.8081, best val acc: 0.9575, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 31760] train loss: 0.6277, train acc: 0.7643, val loss: 0.4277, val acc: 0.9005  (best train acc: 0.8081, best val acc: 0.9575, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 31780] train loss: 0.5911, train acc: 0.7778, val loss: 0.3718, val acc: 0.9133  (best train acc: 0.8081, best val acc: 0.9575, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 31800] train loss: 0.5827, train acc: 0.7839, val loss: 0.3703, val acc: 0.9430  (best train acc: 0.8081, best val acc: 0.9575, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 31820] train loss: 0.5734, train acc: 0.7851, val loss: 0.3868, val acc: 0.9103  (best train acc: 0.8081, best val acc: 0.9575, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 31840] train loss: 0.5752, train acc: 0.7879, val loss: 0.3715, val acc: 0.9339  (best train acc: 0.8081, best val acc: 0.9575, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 31860] train loss: 0.5943, train acc: 0.7743, val loss: 0.4131, val acc: 0.9170  (best train acc: 0.8081, best val acc: 0.9575, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 31880] train loss: 0.5821, train acc: 0.7796, val loss: 0.3928, val acc: 0.8799  (best train acc: 0.8081, best val acc: 0.9575, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 31900] train loss: 0.5498, train acc: 0.8009, val loss: 0.3392, val acc: 0.9477  (best train acc: 0.8081, best val acc: 0.9575, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 31920] train loss: 0.5977, train acc: 0.7572, val loss: 0.4102, val acc: 0.8691  (best train acc: 0.8081, best val acc: 0.9575, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 31940] train loss: 0.5645, train acc: 0.7974, val loss: 0.3605, val acc: 0.9376  (best train acc: 0.8081, best val acc: 0.9575, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 31960] train loss: 0.5757, train acc: 0.7788, val loss: 0.3687, val acc: 0.9032  (best train acc: 0.8081, best val acc: 0.9575, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 31980] train loss: 0.5680, train acc: 0.7736, val loss: 0.3960, val acc: 0.8887  (best train acc: 0.8081, best val acc: 0.9575, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 32000] train loss: 0.5989, train acc: 0.7755, val loss: 0.4138, val acc: 0.9137  (best train acc: 0.8081, best val acc: 0.9575, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 32020] train loss: 0.5689, train acc: 0.7883, val loss: 0.3827, val acc: 0.8796  (best train acc: 0.8081, best val acc: 0.9575, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 32040] train loss: 0.6030, train acc: 0.7629, val loss: 0.3837, val acc: 0.8961  (best train acc: 0.8081, best val acc: 0.9575, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 32060] train loss: 0.5728, train acc: 0.7825, val loss: 0.3736, val acc: 0.9403  (best train acc: 0.8081, best val acc: 0.9575, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 32080] train loss: 0.5696, train acc: 0.7962, val loss: 0.3593, val acc: 0.9295  (best train acc: 0.8081, best val acc: 0.9575, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 32100] train loss: 0.5337, train acc: 0.8060, val loss: 0.3368, val acc: 0.9538  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 32120] train loss: 0.6382, train acc: 0.7541, val loss: 0.4663, val acc: 0.8702  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 32140] train loss: 0.6968, train acc: 0.7299, val loss: 0.4349, val acc: 0.9025  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 32160] train loss: 0.6521, train acc: 0.7366, val loss: 0.4287, val acc: 0.9073  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 32180] train loss: 0.6416, train acc: 0.7502, val loss: 0.4123, val acc: 0.9099  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 32200] train loss: 0.5916, train acc: 0.7851, val loss: 0.3991, val acc: 0.9120  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 32220] train loss: 0.5668, train acc: 0.7923, val loss: 0.3649, val acc: 0.9234  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 32240] train loss: 0.5588, train acc: 0.7958, val loss: 0.3621, val acc: 0.9282  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 32260] train loss: 0.5723, train acc: 0.7874, val loss: 0.4050, val acc: 0.8681  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 32280] train loss: 0.5898, train acc: 0.7715, val loss: 0.4094, val acc: 0.8651  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 32300] train loss: 0.5699, train acc: 0.7793, val loss: 0.3750, val acc: 0.9012  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 32320] train loss: 0.5601, train acc: 0.7953, val loss: 0.3698, val acc: 0.9241  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 32340] train loss: 0.6064, train acc: 0.7420, val loss: 0.5266, val acc: 0.8273  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 32360] train loss: 0.6486, train acc: 0.7404, val loss: 0.3899, val acc: 0.9231  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 32380] train loss: 0.5781, train acc: 0.7821, val loss: 0.3755, val acc: 0.9342  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 32400] train loss: 0.5757, train acc: 0.7762, val loss: 0.3541, val acc: 0.9207  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 32420] train loss: 0.6040, train acc: 0.7447, val loss: 0.3885, val acc: 0.9221  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 32440] train loss: 0.5955, train acc: 0.7732, val loss: 0.3573, val acc: 0.9285  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 32460] train loss: 0.5829, train acc: 0.7806, val loss: 0.3829, val acc: 0.9005  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 32480] train loss: 0.5950, train acc: 0.7793, val loss: 0.3722, val acc: 0.9019  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 32500] train loss: 0.5712, train acc: 0.7846, val loss: 0.3654, val acc: 0.9126  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 32520] train loss: 0.5959, train acc: 0.7645, val loss: 0.3750, val acc: 0.9049  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 32540] train loss: 0.5724, train acc: 0.7861, val loss: 0.4180, val acc: 0.8553  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 32560] train loss: 0.7630, train acc: 0.6901, val loss: 0.5836, val acc: 0.7831  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 32580] train loss: 0.7011, train acc: 0.7324, val loss: 0.5170, val acc: 0.8482  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 32600] train loss: 0.6965, train acc: 0.7300, val loss: 0.5121, val acc: 0.8597  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 32620] train loss: 0.7014, train acc: 0.7288, val loss: 0.4955, val acc: 0.8634  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 32640] train loss: 0.7238, train acc: 0.6910, val loss: 0.5378, val acc: 0.8398  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 32660] train loss: 0.6918, train acc: 0.7355, val loss: 0.5165, val acc: 0.8543  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 32680] train loss: 0.7014, train acc: 0.7223, val loss: 0.4747, val acc: 0.8745  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 32700] train loss: 0.7143, train acc: 0.7056, val loss: 0.5292, val acc: 0.8600  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 32720] train loss: 0.7365, train acc: 0.7000, val loss: 0.5435, val acc: 0.8132  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 32740] train loss: 0.6806, train acc: 0.7312, val loss: 0.4884, val acc: 0.8786  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 32760] train loss: 0.6906, train acc: 0.7381, val loss: 0.4886, val acc: 0.8894  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 32780] train loss: 0.6925, train acc: 0.7312, val loss: 0.4976, val acc: 0.8661  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 32800] train loss: 0.7023, train acc: 0.7203, val loss: 0.4919, val acc: 0.8678  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 32820] train loss: 0.7023, train acc: 0.7206, val loss: 0.5503, val acc: 0.8132  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 32840] train loss: 0.6829, train acc: 0.7262, val loss: 0.4975, val acc: 0.8428  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 32860] train loss: 0.6959, train acc: 0.7237, val loss: 0.5020, val acc: 0.8573  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 32880] train loss: 0.7000, train acc: 0.7287, val loss: 0.5200, val acc: 0.8148  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 32900] train loss: 0.6905, train acc: 0.7269, val loss: 0.5019, val acc: 0.8469  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 32920] train loss: 0.7052, train acc: 0.7296, val loss: 0.4946, val acc: 0.8671  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 32940] train loss: 0.7287, train acc: 0.7060, val loss: 0.5395, val acc: 0.8061  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 32960] train loss: 0.7031, train acc: 0.7285, val loss: 0.4851, val acc: 0.9008  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 32980] train loss: 0.6805, train acc: 0.7305, val loss: 0.5078, val acc: 0.8371  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 33000] train loss: 0.7014, train acc: 0.7287, val loss: 0.4826, val acc: 0.8675  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 33020] train loss: 0.7180, train acc: 0.6961, val loss: 0.5362, val acc: 0.8128  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 33040] train loss: 0.6625, train acc: 0.7425, val loss: 0.4935, val acc: 0.8482  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 33060] train loss: 0.6839, train acc: 0.7324, val loss: 0.5082, val acc: 0.8476  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 33080] train loss: 0.6823, train acc: 0.7363, val loss: 0.4860, val acc: 0.8745  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 33100] train loss: 0.6991, train acc: 0.7274, val loss: 0.4758, val acc: 0.8745  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 33120] train loss: 0.6913, train acc: 0.7180, val loss: 0.5524, val acc: 0.7727  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 33140] train loss: 0.7334, train acc: 0.7000, val loss: 0.5586, val acc: 0.7626  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 33160] train loss: 0.7150, train acc: 0.7143, val loss: 0.4915, val acc: 0.8735  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 33180] train loss: 0.6916, train acc: 0.7285, val loss: 0.4914, val acc: 0.8513  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 33200] train loss: 0.6931, train acc: 0.7345, val loss: 0.4698, val acc: 0.8840  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 33220] train loss: 0.6870, train acc: 0.7326, val loss: 0.4968, val acc: 0.8641  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 33240] train loss: 0.7655, train acc: 0.6437, val loss: 0.5234, val acc: 0.8378  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 33260] train loss: 0.7073, train acc: 0.7219, val loss: 0.5098, val acc: 0.8425  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 33280] train loss: 0.6914, train acc: 0.7269, val loss: 0.4724, val acc: 0.8722  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 33300] train loss: 0.6777, train acc: 0.7299, val loss: 0.5137, val acc: 0.8449  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 33320] train loss: 0.6958, train acc: 0.7203, val loss: 0.4970, val acc: 0.8691  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 33340] train loss: 0.6883, train acc: 0.7151, val loss: 0.5535, val acc: 0.7953  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 33360] train loss: 0.6855, train acc: 0.7364, val loss: 0.4949, val acc: 0.8621  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 33380] train loss: 0.6847, train acc: 0.7280, val loss: 0.4845, val acc: 0.8681  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 33400] train loss: 0.6840, train acc: 0.7255, val loss: 0.4756, val acc: 0.8658  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 33420] train loss: 0.6830, train acc: 0.7366, val loss: 0.4791, val acc: 0.8708  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 33440] train loss: 0.7128, train acc: 0.6945, val loss: 0.4980, val acc: 0.8594  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 33460] train loss: 0.6995, train acc: 0.7240, val loss: 0.4854, val acc: 0.8604  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 33480] train loss: 0.7318, train acc: 0.6749, val loss: 0.5234, val acc: 0.8209  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 33500] train loss: 0.6786, train acc: 0.7326, val loss: 0.4899, val acc: 0.8658  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 33520] train loss: 0.6666, train acc: 0.7322, val loss: 0.4802, val acc: 0.8664  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 33540] train loss: 0.6966, train acc: 0.7295, val loss: 0.4866, val acc: 0.8722  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 33560] train loss: 0.6917, train acc: 0.7251, val loss: 0.5658, val acc: 0.7831  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 33580] train loss: 0.6781, train acc: 0.7366, val loss: 0.5373, val acc: 0.8209  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 33600] train loss: 0.6878, train acc: 0.7184, val loss: 0.4745, val acc: 0.8627  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 33620] train loss: 0.6834, train acc: 0.7300, val loss: 0.4613, val acc: 0.8843  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 33640] train loss: 0.6692, train acc: 0.7406, val loss: 0.4668, val acc: 0.8877  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 33660] train loss: 0.6926, train acc: 0.7325, val loss: 0.4723, val acc: 0.8897  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 33680] train loss: 0.6930, train acc: 0.7262, val loss: 0.4820, val acc: 0.8752  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 33700] train loss: 0.6838, train acc: 0.7277, val loss: 0.5417, val acc: 0.7970  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 33720] train loss: 0.7069, train acc: 0.7251, val loss: 0.4959, val acc: 0.8600  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 33740] train loss: 0.6815, train acc: 0.7246, val loss: 0.4769, val acc: 0.8631  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 33760] train loss: 0.6763, train acc: 0.7349, val loss: 0.5109, val acc: 0.8496  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 33780] train loss: 0.6610, train acc: 0.7373, val loss: 0.4847, val acc: 0.8607  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 33800] train loss: 0.6671, train acc: 0.7350, val loss: 0.4644, val acc: 0.8769  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 33820] train loss: 0.7280, train acc: 0.6604, val loss: 0.5524, val acc: 0.8034  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 33840] train loss: 0.6939, train acc: 0.7162, val loss: 0.5295, val acc: 0.8256  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 33860] train loss: 0.6908, train acc: 0.7247, val loss: 0.4904, val acc: 0.8567  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 33880] train loss: 0.6879, train acc: 0.7311, val loss: 0.4777, val acc: 0.8691  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 33900] train loss: 0.6675, train acc: 0.7392, val loss: 0.4714, val acc: 0.8782  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 33920] train loss: 0.6718, train acc: 0.7371, val loss: 0.4981, val acc: 0.8462  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 33940] train loss: 0.6749, train acc: 0.7334, val loss: 0.4872, val acc: 0.8658  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 33960] train loss: 0.6910, train acc: 0.7205, val loss: 0.6410, val acc: 0.7315  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 33980] train loss: 0.7026, train acc: 0.7038, val loss: 0.5077, val acc: 0.8492  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 34000] train loss: 0.6981, train acc: 0.7261, val loss: 0.4852, val acc: 0.8519  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 34020] train loss: 0.6750, train acc: 0.7328, val loss: 0.4826, val acc: 0.8604  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 34040] train loss: 0.6663, train acc: 0.7384, val loss: 0.4614, val acc: 0.8890  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 34060] train loss: 0.6761, train acc: 0.7368, val loss: 0.4733, val acc: 0.8833  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 34080] train loss: 0.6807, train acc: 0.7343, val loss: 0.4640, val acc: 0.8887  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 34100] train loss: 0.7973, train acc: 0.6558, val loss: 0.5212, val acc: 0.8560  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 34120] train loss: 0.7029, train acc: 0.7258, val loss: 0.5000, val acc: 0.8563  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 34140] train loss: 0.6777, train acc: 0.7326, val loss: 0.4978, val acc: 0.8445  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 34160] train loss: 0.6895, train acc: 0.7177, val loss: 0.4810, val acc: 0.8641  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 34180] train loss: 0.6781, train acc: 0.7332, val loss: 0.4769, val acc: 0.8766  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 34200] train loss: 0.6826, train acc: 0.7337, val loss: 0.4694, val acc: 0.8772  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 34220] train loss: 0.6796, train acc: 0.7255, val loss: 0.4814, val acc: 0.8695  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 34240] train loss: 0.6728, train acc: 0.7371, val loss: 0.4703, val acc: 0.8712  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 34260] train loss: 0.6827, train acc: 0.7319, val loss: 0.4737, val acc: 0.8718  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 34280] train loss: 0.6834, train acc: 0.7351, val loss: 0.4737, val acc: 0.8681  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 34300] train loss: 0.6981, train acc: 0.7227, val loss: 0.5020, val acc: 0.8371  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 34320] train loss: 0.6842, train acc: 0.7358, val loss: 0.4871, val acc: 0.8698  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 34340] train loss: 0.6670, train acc: 0.7411, val loss: 0.4771, val acc: 0.8725  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 34360] train loss: 0.6894, train acc: 0.7299, val loss: 0.4715, val acc: 0.8695  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 34380] train loss: 0.6566, train acc: 0.7469, val loss: 0.4787, val acc: 0.8654  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 34400] train loss: 0.6733, train acc: 0.7373, val loss: 0.4777, val acc: 0.8702  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 34420] train loss: 0.6802, train acc: 0.7334, val loss: 0.5328, val acc: 0.8088  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 34440] train loss: 0.6942, train acc: 0.7211, val loss: 0.4698, val acc: 0.8722  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 34460] train loss: 0.6774, train acc: 0.7302, val loss: 0.4639, val acc: 0.8853  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 34480] train loss: 0.6642, train acc: 0.7448, val loss: 0.4770, val acc: 0.8661  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 34500] train loss: 0.6753, train acc: 0.7297, val loss: 0.5119, val acc: 0.8263  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 34520] train loss: 0.6743, train acc: 0.7322, val loss: 0.4733, val acc: 0.8830  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 34540] train loss: 0.6829, train acc: 0.7287, val loss: 0.5078, val acc: 0.8337  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 34560] train loss: 0.6724, train acc: 0.7342, val loss: 0.4721, val acc: 0.8779  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 34580] train loss: 0.6747, train acc: 0.7311, val loss: 0.4645, val acc: 0.8826  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 34600] train loss: 0.6818, train acc: 0.7295, val loss: 0.4759, val acc: 0.8712  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 34620] train loss: 0.7468, train acc: 0.6878, val loss: 0.5590, val acc: 0.7322  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 34640] train loss: 0.6863, train acc: 0.7288, val loss: 0.4869, val acc: 0.8671  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 34660] train loss: 0.6669, train acc: 0.7369, val loss: 0.4797, val acc: 0.8567  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 34680] train loss: 0.6794, train acc: 0.7377, val loss: 0.4784, val acc: 0.8621  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 34700] train loss: 0.6995, train acc: 0.7212, val loss: 0.4699, val acc: 0.8735  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 34720] train loss: 0.7090, train acc: 0.7185, val loss: 0.4746, val acc: 0.8732  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 34740] train loss: 0.6875, train acc: 0.7248, val loss: 0.4734, val acc: 0.8668  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 34760] train loss: 0.6569, train acc: 0.7428, val loss: 0.4641, val acc: 0.8894  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 34780] train loss: 0.6706, train acc: 0.7336, val loss: 0.4658, val acc: 0.8766  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 34800] train loss: 0.6714, train acc: 0.7342, val loss: 0.4709, val acc: 0.8634  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 34820] train loss: 0.6560, train acc: 0.7438, val loss: 0.4725, val acc: 0.8725  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 34840] train loss: 0.6740, train acc: 0.7282, val loss: 0.4669, val acc: 0.8735  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 34860] train loss: 0.6759, train acc: 0.7321, val loss: 0.4825, val acc: 0.8654  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 34880] train loss: 0.6606, train acc: 0.7402, val loss: 0.4595, val acc: 0.8745  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 34900] train loss: 0.6715, train acc: 0.7339, val loss: 0.4704, val acc: 0.8745  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 34920] train loss: 0.6600, train acc: 0.7400, val loss: 0.4610, val acc: 0.8880  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 34940] train loss: 0.6725, train acc: 0.7410, val loss: 0.4757, val acc: 0.8661  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 34960] train loss: 0.6655, train acc: 0.7389, val loss: 0.4705, val acc: 0.8752  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 34980] train loss: 0.6638, train acc: 0.7409, val loss: 0.4797, val acc: 0.8580  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 35000] train loss: 0.6710, train acc: 0.7416, val loss: 0.4733, val acc: 0.8685  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 35020] train loss: 0.6871, train acc: 0.7354, val loss: 0.4610, val acc: 0.8826  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 35040] train loss: 0.6571, train acc: 0.7411, val loss: 0.4623, val acc: 0.8840  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 35060] train loss: 0.6815, train acc: 0.7337, val loss: 0.4716, val acc: 0.8739  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 35080] train loss: 0.6736, train acc: 0.7376, val loss: 0.4662, val acc: 0.8759  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 35100] train loss: 0.6613, train acc: 0.7378, val loss: 0.5166, val acc: 0.8209  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 35120] train loss: 0.6804, train acc: 0.7282, val loss: 0.4667, val acc: 0.8695  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 35140] train loss: 0.6711, train acc: 0.7353, val loss: 0.4687, val acc: 0.8702  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 35160] train loss: 0.6808, train acc: 0.7335, val loss: 0.4680, val acc: 0.8776  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 35180] train loss: 0.6707, train acc: 0.7352, val loss: 0.4696, val acc: 0.8789  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 35200] train loss: 0.6670, train acc: 0.7379, val loss: 0.4742, val acc: 0.8637  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 35220] train loss: 0.6678, train acc: 0.7346, val loss: 0.4644, val acc: 0.8776  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 35240] train loss: 0.6755, train acc: 0.7324, val loss: 0.4826, val acc: 0.8654  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 35260] train loss: 0.6968, train acc: 0.7164, val loss: 0.4898, val acc: 0.8422  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 35280] train loss: 0.6935, train acc: 0.7333, val loss: 0.4684, val acc: 0.8820  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 35300] train loss: 0.6817, train acc: 0.7303, val loss: 0.4781, val acc: 0.8526  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 35320] train loss: 0.6808, train acc: 0.7298, val loss: 0.4636, val acc: 0.8762  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 35340] train loss: 0.6600, train acc: 0.7360, val loss: 0.4652, val acc: 0.8786  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 35360] train loss: 0.6734, train acc: 0.7287, val loss: 0.4752, val acc: 0.8637  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 35380] train loss: 0.6982, train acc: 0.7349, val loss: 0.4742, val acc: 0.8782  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 35400] train loss: 0.6718, train acc: 0.7285, val loss: 0.4735, val acc: 0.8705  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 35420] train loss: 0.6749, train acc: 0.7353, val loss: 0.4611, val acc: 0.8749  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 35440] train loss: 0.6864, train acc: 0.7211, val loss: 0.4713, val acc: 0.8580  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 35460] train loss: 0.6754, train acc: 0.7343, val loss: 0.4756, val acc: 0.8688  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 35480] train loss: 0.6781, train acc: 0.7337, val loss: 0.4676, val acc: 0.8732  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 35500] train loss: 0.6792, train acc: 0.7314, val loss: 0.4585, val acc: 0.8796  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 35520] train loss: 0.6743, train acc: 0.7329, val loss: 0.4710, val acc: 0.8695  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 35540] train loss: 0.6591, train acc: 0.7372, val loss: 0.4737, val acc: 0.8755  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 35560] train loss: 0.6691, train acc: 0.7323, val loss: 0.4592, val acc: 0.8843  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 35580] train loss: 0.6839, train acc: 0.7360, val loss: 0.4657, val acc: 0.8762  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 35600] train loss: 0.6784, train acc: 0.7338, val loss: 0.4723, val acc: 0.8715  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 35620] train loss: 0.6546, train acc: 0.7319, val loss: 0.4673, val acc: 0.8742  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 35640] train loss: 0.6650, train acc: 0.7349, val loss: 0.4621, val acc: 0.8641  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 35660] train loss: 0.6724, train acc: 0.7333, val loss: 0.4632, val acc: 0.8759  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 35680] train loss: 0.6698, train acc: 0.7383, val loss: 0.4628, val acc: 0.8863  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 35700] train loss: 0.6864, train acc: 0.7192, val loss: 0.4751, val acc: 0.8735  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 35720] train loss: 0.6863, train acc: 0.7240, val loss: 0.4621, val acc: 0.8675  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 35740] train loss: 0.6743, train acc: 0.7301, val loss: 0.4683, val acc: 0.8712  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 35760] train loss: 0.6751, train acc: 0.7314, val loss: 0.4708, val acc: 0.8890  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 35780] train loss: 0.6861, train acc: 0.7300, val loss: 0.4616, val acc: 0.8735  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 35800] train loss: 0.7000, train acc: 0.7182, val loss: 0.4782, val acc: 0.8641  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 35820] train loss: 0.6815, train acc: 0.7318, val loss: 0.4742, val acc: 0.8725  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 35840] train loss: 0.6617, train acc: 0.7383, val loss: 0.4693, val acc: 0.8735  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 35860] train loss: 0.6797, train acc: 0.7325, val loss: 0.4878, val acc: 0.8526  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 35880] train loss: 0.6722, train acc: 0.7366, val loss: 0.4697, val acc: 0.8718  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 35900] train loss: 0.6692, train acc: 0.7325, val loss: 0.4597, val acc: 0.8803  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 35920] train loss: 0.6667, train acc: 0.7350, val loss: 0.4690, val acc: 0.8691  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 35940] train loss: 0.6731, train acc: 0.7363, val loss: 0.4686, val acc: 0.8779  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 35960] train loss: 0.6693, train acc: 0.7361, val loss: 0.4646, val acc: 0.8793  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 35980] train loss: 0.6749, train acc: 0.7367, val loss: 0.4920, val acc: 0.8479  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 36000] train loss: 0.6608, train acc: 0.7435, val loss: 0.4733, val acc: 0.8803  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 36020] train loss: 0.6655, train acc: 0.7329, val loss: 0.4638, val acc: 0.8728  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 36040] train loss: 0.6801, train acc: 0.7277, val loss: 0.4676, val acc: 0.8796  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 36060] train loss: 0.6573, train acc: 0.7403, val loss: 0.4661, val acc: 0.8796  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 36080] train loss: 0.6531, train acc: 0.7388, val loss: 0.4740, val acc: 0.8789  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 36100] train loss: 0.6884, train acc: 0.7293, val loss: 0.4692, val acc: 0.8820  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 36120] train loss: 0.6560, train acc: 0.7401, val loss: 0.4757, val acc: 0.8715  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 36140] train loss: 0.6711, train acc: 0.7317, val loss: 0.4552, val acc: 0.8820  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 36160] train loss: 0.6520, train acc: 0.7360, val loss: 0.4556, val acc: 0.8755  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 36180] train loss: 0.6556, train acc: 0.7445, val loss: 0.4657, val acc: 0.8941  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 36200] train loss: 0.6638, train acc: 0.7436, val loss: 0.4579, val acc: 0.8789  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 36220] train loss: 0.6683, train acc: 0.7340, val loss: 0.4543, val acc: 0.8961  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 36240] train loss: 0.6573, train acc: 0.7340, val loss: 0.4655, val acc: 0.8769  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 36260] train loss: 0.6695, train acc: 0.7412, val loss: 0.4796, val acc: 0.8668  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 36280] train loss: 0.6774, train acc: 0.7258, val loss: 0.4584, val acc: 0.8897  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 36300] train loss: 0.6608, train acc: 0.7433, val loss: 0.4648, val acc: 0.9012  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 36320] train loss: 0.6515, train acc: 0.7391, val loss: 0.4634, val acc: 0.8813  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 36340] train loss: 0.6723, train acc: 0.7308, val loss: 0.4559, val acc: 0.8857  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 36360] train loss: 0.6678, train acc: 0.7307, val loss: 0.4559, val acc: 0.8958  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 36380] train loss: 0.6673, train acc: 0.7374, val loss: 0.4625, val acc: 0.8998  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 36400] train loss: 0.6768, train acc: 0.7318, val loss: 0.4670, val acc: 0.8830  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 36420] train loss: 0.6681, train acc: 0.7392, val loss: 0.4612, val acc: 0.8894  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 36440] train loss: 0.6594, train acc: 0.7391, val loss: 0.4565, val acc: 0.8988  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 36460] train loss: 0.6494, train acc: 0.7397, val loss: 0.4617, val acc: 0.8877  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 36480] train loss: 0.6649, train acc: 0.7388, val loss: 0.4650, val acc: 0.8985  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 36500] train loss: 0.6659, train acc: 0.7321, val loss: 0.4577, val acc: 0.9012  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 36520] train loss: 0.6810, train acc: 0.7262, val loss: 0.4644, val acc: 0.8691  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 36540] train loss: 0.6736, train acc: 0.7303, val loss: 0.4583, val acc: 0.8887  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 36560] train loss: 0.6828, train acc: 0.7236, val loss: 0.4751, val acc: 0.8725  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 36580] train loss: 0.6421, train acc: 0.7460, val loss: 0.4582, val acc: 0.9106  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 36600] train loss: 0.6609, train acc: 0.7355, val loss: 0.4632, val acc: 0.8820  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 36620] train loss: 0.6596, train acc: 0.7402, val loss: 0.4653, val acc: 0.8958  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 36640] train loss: 0.6443, train acc: 0.7433, val loss: 0.4562, val acc: 0.8813  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 36660] train loss: 0.6562, train acc: 0.7403, val loss: 0.4444, val acc: 0.9029  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 36680] train loss: 0.6533, train acc: 0.7462, val loss: 0.4470, val acc: 0.9089  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 36700] train loss: 0.6562, train acc: 0.7413, val loss: 0.4577, val acc: 0.8938  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 36720] train loss: 0.6662, train acc: 0.7349, val loss: 0.4355, val acc: 0.8985  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 36740] train loss: 0.6462, train acc: 0.7447, val loss: 0.4381, val acc: 0.9032  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 36760] train loss: 0.6365, train acc: 0.7480, val loss: 0.4429, val acc: 0.8772  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 36780] train loss: 0.6427, train acc: 0.7418, val loss: 0.4338, val acc: 0.9008  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 36800] train loss: 0.6353, train acc: 0.7441, val loss: 0.4547, val acc: 0.8901  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 36820] train loss: 0.6792, train acc: 0.7410, val loss: 0.4527, val acc: 0.9187  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 36840] train loss: 0.6729, train acc: 0.7290, val loss: 0.4411, val acc: 0.9059  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 36860] train loss: 0.6266, train acc: 0.7506, val loss: 0.4531, val acc: 0.8914  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 36880] train loss: 0.6545, train acc: 0.7368, val loss: 0.4313, val acc: 0.9123  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 36900] train loss: 0.6638, train acc: 0.7353, val loss: 0.4457, val acc: 0.8830  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 36920] train loss: 0.6326, train acc: 0.7505, val loss: 0.4347, val acc: 0.9123  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 36940] train loss: 0.6489, train acc: 0.7380, val loss: 0.4441, val acc: 0.8965  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 36960] train loss: 0.6546, train acc: 0.7405, val loss: 0.4239, val acc: 0.9157  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 36980] train loss: 0.6365, train acc: 0.7452, val loss: 0.4334, val acc: 0.9174  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 37000] train loss: 0.6217, train acc: 0.7561, val loss: 0.4147, val acc: 0.9089  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 37020] train loss: 0.6390, train acc: 0.7528, val loss: 0.4249, val acc: 0.9126  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 37040] train loss: 0.6660, train acc: 0.7361, val loss: 0.4484, val acc: 0.9147  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 37060] train loss: 0.6695, train acc: 0.7385, val loss: 0.4255, val acc: 0.9046  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 37080] train loss: 0.6491, train acc: 0.7581, val loss: 0.4181, val acc: 0.9110  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 37100] train loss: 0.6343, train acc: 0.7593, val loss: 0.4459, val acc: 0.9214  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 37120] train loss: 0.6365, train acc: 0.7475, val loss: 0.4225, val acc: 0.9191  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 37140] train loss: 0.6181, train acc: 0.7602, val loss: 0.4316, val acc: 0.9231  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 37160] train loss: 0.6446, train acc: 0.7480, val loss: 0.4351, val acc: 0.9241  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 37180] train loss: 0.6658, train acc: 0.7411, val loss: 0.4750, val acc: 0.8496  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 37200] train loss: 0.6439, train acc: 0.7431, val loss: 0.4447, val acc: 0.8921  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 37220] train loss: 0.6305, train acc: 0.7505, val loss: 0.4126, val acc: 0.9207  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 37240] train loss: 0.6360, train acc: 0.7556, val loss: 0.4293, val acc: 0.9282  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 37260] train loss: 0.6404, train acc: 0.7503, val loss: 0.4280, val acc: 0.9069  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 37280] train loss: 0.6433, train acc: 0.7492, val loss: 0.4110, val acc: 0.9089  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 37300] train loss: 0.6904, train acc: 0.7190, val loss: 0.4494, val acc: 0.8664  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 37320] train loss: 0.6209, train acc: 0.7415, val loss: 0.4396, val acc: 0.8914  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 37340] train loss: 0.6280, train acc: 0.7540, val loss: 0.4550, val acc: 0.8590  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 37360] train loss: 0.6365, train acc: 0.7424, val loss: 0.4054, val acc: 0.9140  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 37380] train loss: 0.6273, train acc: 0.7567, val loss: 0.4206, val acc: 0.9278  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 37400] train loss: 0.6467, train acc: 0.7501, val loss: 0.4302, val acc: 0.8863  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 37420] train loss: 0.6668, train acc: 0.7333, val loss: 0.4306, val acc: 0.9025  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 37440] train loss: 0.6363, train acc: 0.7524, val loss: 0.4171, val acc: 0.9187  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 37460] train loss: 0.6431, train acc: 0.7399, val loss: 0.4166, val acc: 0.9103  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 37480] train loss: 0.6549, train acc: 0.7466, val loss: 0.4392, val acc: 0.8752  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 37500] train loss: 0.6208, train acc: 0.7567, val loss: 0.4079, val acc: 0.9025  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 37520] train loss: 0.6278, train acc: 0.7590, val loss: 0.4165, val acc: 0.9015  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 37540] train loss: 0.6910, train acc: 0.7311, val loss: 0.4001, val acc: 0.9305  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 37560] train loss: 0.6253, train acc: 0.7478, val loss: 0.4026, val acc: 0.9140  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 37580] train loss: 0.6410, train acc: 0.7493, val loss: 0.4219, val acc: 0.9150  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 37600] train loss: 0.6369, train acc: 0.7489, val loss: 0.4019, val acc: 0.9184  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 37620] train loss: 0.6257, train acc: 0.7551, val loss: 0.3983, val acc: 0.9113  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 37640] train loss: 0.6155, train acc: 0.7553, val loss: 0.4320, val acc: 0.8975  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 37660] train loss: 0.6263, train acc: 0.7529, val loss: 0.4055, val acc: 0.9258  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 37680] train loss: 0.6215, train acc: 0.7617, val loss: 0.3836, val acc: 0.9231  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 37700] train loss: 0.6260, train acc: 0.7593, val loss: 0.4119, val acc: 0.9160  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 37720] train loss: 0.6347, train acc: 0.7548, val loss: 0.4329, val acc: 0.8995  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 37740] train loss: 0.6452, train acc: 0.7395, val loss: 0.3897, val acc: 0.9191  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 37760] train loss: 0.6263, train acc: 0.7486, val loss: 0.4296, val acc: 0.8823  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 37780] train loss: 0.6464, train acc: 0.7467, val loss: 0.3986, val acc: 0.9180  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 37800] train loss: 0.6199, train acc: 0.7468, val loss: 0.4020, val acc: 0.9265  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 37820] train loss: 0.6351, train acc: 0.7465, val loss: 0.3967, val acc: 0.9268  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 37840] train loss: 0.6196, train acc: 0.7671, val loss: 0.4305, val acc: 0.8887  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 37860] train loss: 0.6140, train acc: 0.7535, val loss: 0.3891, val acc: 0.9221  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 37880] train loss: 0.6234, train acc: 0.7508, val loss: 0.4051, val acc: 0.9150  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 37900] train loss: 0.6094, train acc: 0.7611, val loss: 0.4027, val acc: 0.9002  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 37920] train loss: 0.6361, train acc: 0.7569, val loss: 0.4011, val acc: 0.9197  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 37940] train loss: 0.6307, train acc: 0.7514, val loss: 0.3901, val acc: 0.9228  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 37960] train loss: 0.6248, train acc: 0.7627, val loss: 0.3909, val acc: 0.9298  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 37980] train loss: 0.6322, train acc: 0.7483, val loss: 0.3966, val acc: 0.9312  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 38000] train loss: 0.6050, train acc: 0.7707, val loss: 0.4087, val acc: 0.9312  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 38020] train loss: 0.6250, train acc: 0.7497, val loss: 0.3998, val acc: 0.9046  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 38040] train loss: 0.6195, train acc: 0.7635, val loss: 0.4053, val acc: 0.9170  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 38060] train loss: 0.6084, train acc: 0.7741, val loss: 0.3910, val acc: 0.9228  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 38080] train loss: 0.6159, train acc: 0.7545, val loss: 0.3976, val acc: 0.8887  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 38100] train loss: 0.8905, train acc: 0.6383, val loss: 0.6028, val acc: 0.7272  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 38120] train loss: 0.8108, train acc: 0.6381, val loss: 0.5560, val acc: 0.8283  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 38140] train loss: 0.8014, train acc: 0.6735, val loss: 0.4537, val acc: 0.9042  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 38160] train loss: 0.7067, train acc: 0.7143, val loss: 0.4752, val acc: 0.8867  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 38180] train loss: 0.7619, train acc: 0.6776, val loss: 0.4898, val acc: 0.8543  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 38200] train loss: 0.7363, train acc: 0.6872, val loss: 0.4470, val acc: 0.9110  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 38220] train loss: 0.7232, train acc: 0.6894, val loss: 0.4508, val acc: 0.9187  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 38240] train loss: 0.7201, train acc: 0.7058, val loss: 0.4853, val acc: 0.8796  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 38260] train loss: 0.7396, train acc: 0.6904, val loss: 0.4906, val acc: 0.8442  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 38280] train loss: 0.6982, train acc: 0.7016, val loss: 0.4907, val acc: 0.8314  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 38300] train loss: 0.7506, train acc: 0.6794, val loss: 0.4283, val acc: 0.9174  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 38320] train loss: 0.7337, train acc: 0.6860, val loss: 0.4723, val acc: 0.9103  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 38340] train loss: 0.7341, train acc: 0.6966, val loss: 0.4504, val acc: 0.9052  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 38360] train loss: 0.7219, train acc: 0.6861, val loss: 0.4665, val acc: 0.9106  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 38380] train loss: 0.7362, train acc: 0.7028, val loss: 0.4771, val acc: 0.9052  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 38400] train loss: 0.7152, train acc: 0.6986, val loss: 0.4593, val acc: 0.9261  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 38420] train loss: 0.7146, train acc: 0.7059, val loss: 0.4613, val acc: 0.9089  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 38440] train loss: 0.7135, train acc: 0.6932, val loss: 0.4884, val acc: 0.9126  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 38460] train loss: 0.7288, train acc: 0.6888, val loss: 0.4716, val acc: 0.8951  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 38480] train loss: 0.7156, train acc: 0.6982, val loss: 0.4770, val acc: 0.9069  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 38500] train loss: 0.7287, train acc: 0.6940, val loss: 0.4479, val acc: 0.9224  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 38520] train loss: 0.7086, train acc: 0.6993, val loss: 0.4690, val acc: 0.9015  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 38540] train loss: 0.6935, train acc: 0.7071, val loss: 0.5220, val acc: 0.8223  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 38560] train loss: 0.7187, train acc: 0.7031, val loss: 0.4920, val acc: 0.8452  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 38580] train loss: 0.7051, train acc: 0.7002, val loss: 0.4668, val acc: 0.8648  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 38600] train loss: 0.6995, train acc: 0.6992, val loss: 0.4428, val acc: 0.9137  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 38620] train loss: 0.6821, train acc: 0.7073, val loss: 0.5106, val acc: 0.8398  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 38640] train loss: 0.7202, train acc: 0.6974, val loss: 0.4697, val acc: 0.8678  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 38660] train loss: 0.7007, train acc: 0.7066, val loss: 0.4656, val acc: 0.9103  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 38680] train loss: 0.7030, train acc: 0.7009, val loss: 0.4598, val acc: 0.8776  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 38700] train loss: 0.7186, train acc: 0.6909, val loss: 0.4619, val acc: 0.8772  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 38720] train loss: 0.6975, train acc: 0.7059, val loss: 0.4750, val acc: 0.9170  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 38740] train loss: 0.7121, train acc: 0.7022, val loss: 0.4828, val acc: 0.8823  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 38760] train loss: 0.7193, train acc: 0.6870, val loss: 0.4603, val acc: 0.9221  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 38780] train loss: 0.7018, train acc: 0.7019, val loss: 0.4494, val acc: 0.9160  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 38800] train loss: 0.7067, train acc: 0.7005, val loss: 0.4619, val acc: 0.8978  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 38820] train loss: 0.7186, train acc: 0.6876, val loss: 0.4559, val acc: 0.9133  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 38840] train loss: 0.7130, train acc: 0.7037, val loss: 0.4681, val acc: 0.8965  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 38860] train loss: 0.7761, train acc: 0.6612, val loss: 0.4847, val acc: 0.8870  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 38880] train loss: 0.7079, train acc: 0.7009, val loss: 0.4589, val acc: 0.8948  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 38900] train loss: 0.6929, train acc: 0.7046, val loss: 0.4706, val acc: 0.8927  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 38920] train loss: 0.7290, train acc: 0.6917, val loss: 0.4609, val acc: 0.9157  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 38940] train loss: 0.6941, train acc: 0.7050, val loss: 0.4466, val acc: 0.9211  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 38960] train loss: 0.7109, train acc: 0.6955, val loss: 0.4584, val acc: 0.8968  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 38980] train loss: 0.7095, train acc: 0.6947, val loss: 0.4467, val acc: 0.9150  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 39000] train loss: 0.7278, train acc: 0.6772, val loss: 0.4588, val acc: 0.9180  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 39020] train loss: 0.7176, train acc: 0.6857, val loss: 0.4763, val acc: 0.8995  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 39040] train loss: 0.7103, train acc: 0.6924, val loss: 0.4490, val acc: 0.9214  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 39060] train loss: 0.6958, train acc: 0.7108, val loss: 0.4500, val acc: 0.9218  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 39080] train loss: 0.7260, train acc: 0.6867, val loss: 0.4398, val acc: 0.9211  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 39100] train loss: 0.7001, train acc: 0.7068, val loss: 0.4632, val acc: 0.9251  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 39120] train loss: 0.7237, train acc: 0.6885, val loss: 0.4775, val acc: 0.8941  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 39140] train loss: 0.7325, train acc: 0.6753, val loss: 0.4930, val acc: 0.8708  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 39160] train loss: 0.7180, train acc: 0.6879, val loss: 0.4618, val acc: 0.9201  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 39180] train loss: 0.7074, train acc: 0.6945, val loss: 0.4734, val acc: 0.9039  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 39200] train loss: 0.7298, train acc: 0.6878, val loss: 0.4327, val acc: 0.9201  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 39220] train loss: 0.7084, train acc: 0.6943, val loss: 0.4742, val acc: 0.8931  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 39240] train loss: 0.7126, train acc: 0.6908, val loss: 0.4438, val acc: 0.9282  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 39260] train loss: 0.6987, train acc: 0.6995, val loss: 0.4552, val acc: 0.9194  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 39280] train loss: 0.7011, train acc: 0.7025, val loss: 0.4497, val acc: 0.9187  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 39300] train loss: 0.7097, train acc: 0.6955, val loss: 0.4444, val acc: 0.9042  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 39320] train loss: 0.7361, train acc: 0.6917, val loss: 0.5054, val acc: 0.8772  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 39340] train loss: 0.7403, train acc: 0.6753, val loss: 0.4756, val acc: 0.8867  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 39360] train loss: 0.7039, train acc: 0.6898, val loss: 0.4393, val acc: 0.9022  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 39380] train loss: 0.7056, train acc: 0.6991, val loss: 0.4330, val acc: 0.9184  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 39400] train loss: 0.7085, train acc: 0.6825, val loss: 0.4539, val acc: 0.9110  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 39420] train loss: 0.7189, train acc: 0.6852, val loss: 0.4589, val acc: 0.9278  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 39440] train loss: 0.7041, train acc: 0.7057, val loss: 0.4634, val acc: 0.9241  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 39460] train loss: 0.7062, train acc: 0.6923, val loss: 0.4633, val acc: 0.9184  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 39480] train loss: 0.6987, train acc: 0.6945, val loss: 0.4728, val acc: 0.9049  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 39500] train loss: 0.7172, train acc: 0.6890, val loss: 0.4669, val acc: 0.9012  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 39520] train loss: 0.7047, train acc: 0.7058, val loss: 0.4401, val acc: 0.8772  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 39540] train loss: 0.7225, train acc: 0.6719, val loss: 0.5552, val acc: 0.8223  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 39560] train loss: 0.7031, train acc: 0.6931, val loss: 0.4641, val acc: 0.8816  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 39580] train loss: 0.7004, train acc: 0.6900, val loss: 0.4539, val acc: 0.9029  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 39600] train loss: 0.7022, train acc: 0.6879, val loss: 0.4627, val acc: 0.9147  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 39620] train loss: 0.7129, train acc: 0.6948, val loss: 0.4429, val acc: 0.9224  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 39640] train loss: 0.7169, train acc: 0.6912, val loss: 0.4853, val acc: 0.8762  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 39660] train loss: 0.7131, train acc: 0.6898, val loss: 0.4305, val acc: 0.9231  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 39680] train loss: 0.7049, train acc: 0.6995, val loss: 0.4463, val acc: 0.9234  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 39700] train loss: 0.6986, train acc: 0.6901, val loss: 0.4557, val acc: 0.9032  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 39720] train loss: 0.6962, train acc: 0.6954, val loss: 0.4361, val acc: 0.9170  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 39740] train loss: 0.6975, train acc: 0.6953, val loss: 0.4261, val acc: 0.9191  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 39760] train loss: 0.6927, train acc: 0.7021, val loss: 0.4469, val acc: 0.9285  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 39780] train loss: 0.7232, train acc: 0.6860, val loss: 0.4568, val acc: 0.8847  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 39800] train loss: 0.7677, train acc: 0.6550, val loss: 0.5469, val acc: 0.8857  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 39820] train loss: 0.7236, train acc: 0.7045, val loss: 0.4603, val acc: 0.9167  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 39840] train loss: 0.7149, train acc: 0.6885, val loss: 0.4477, val acc: 0.9312  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 39860] train loss: 0.7083, train acc: 0.6922, val loss: 0.4466, val acc: 0.9228  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 39880] train loss: 0.7049, train acc: 0.7060, val loss: 0.4415, val acc: 0.9228  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 39900] train loss: 0.7032, train acc: 0.6986, val loss: 0.4460, val acc: 0.9298  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 39920] train loss: 0.6959, train acc: 0.7012, val loss: 0.4343, val acc: 0.9258  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 39940] train loss: 0.6979, train acc: 0.6966, val loss: 0.4403, val acc: 0.9194  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 39960] train loss: 0.6966, train acc: 0.6964, val loss: 0.4415, val acc: 0.9346  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 39980] train loss: 0.6909, train acc: 0.6973, val loss: 0.4509, val acc: 0.9197  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n",
      "[Epoch: 40000] train loss: 0.6992, train acc: 0.7021, val loss: 0.4213, val acc: 0.9170  (best train acc: 0.8081, best val acc: 0.9585, best train loss: 0.5317  @ epoch 22901 )\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.84      0.92       154\n",
      "           1       0.86      1.00      0.93       703\n",
      "           2       0.90      0.72      0.80       702\n",
      "           3       0.86      0.91      0.88       703\n",
      "           4       0.99      1.00      0.99       702\n",
      "\n",
      "    accuracy                           0.90      2964\n",
      "   macro avg       0.92      0.89      0.90      2964\n",
      "weighted avg       0.91      0.90      0.90      2964\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 504x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 504x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "gnn_sage = GNN2L_Sage(data_with_nedbit).to(device)\n",
    "pred = train(gnn_sage, data_with_nedbit.to(device), 40000, weight_decay=0.0005, cm_title='SAGE2L_multiclass_16HC_v2_max', classes=['P', 'LP', 'WN', 'LN', 'RN'], num_layers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd02f9dc34cf4cf69b86d4a60944df42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40001 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch: 0000] train loss: 461.4225, train acc: 0.1210, val loss: 288.6747, val acc: 0.1734  (best train acc: 0.1210, best val acc: 0.1734, best train loss: 461.4225  @ epoch 0 )\n",
      "[Epoch: 0020] train loss: 46.3379, train acc: 0.1542, val loss: 27.7432, val acc: 0.0664  (best train acc: 0.2320, best val acc: 0.2310, best train loss: 46.3379  @ epoch 20 )\n",
      "[Epoch: 0040] train loss: 2.7743, train acc: 0.1179, val loss: 2.3376, val acc: 0.1079  (best train acc: 0.2324, best val acc: 0.2482, best train loss: 2.7743  @ epoch 40 )\n",
      "[Epoch: 0060] train loss: 1.5460, train acc: 0.3465, val loss: 1.5191, val acc: 0.3460  (best train acc: 0.3490, best val acc: 0.3460, best train loss: 1.5460  @ epoch 60 )\n",
      "[Epoch: 0080] train loss: 1.5001, train acc: 0.3760, val loss: 1.4342, val acc: 0.4776  (best train acc: 0.3865, best val acc: 0.4826, best train loss: 1.4938  @ epoch 79 )\n",
      "[Epoch: 0100] train loss: 1.4236, train acc: 0.4009, val loss: 1.3129, val acc: 0.4948  (best train acc: 0.4135, best val acc: 0.4968, best train loss: 1.4208  @ epoch 96 )\n",
      "[Epoch: 0120] train loss: 1.3424, train acc: 0.4308, val loss: 1.2148, val acc: 0.5002  (best train acc: 0.4313, best val acc: 0.5008, best train loss: 1.3424  @ epoch 120 )\n",
      "[Epoch: 0140] train loss: 1.3141, train acc: 0.4267, val loss: 1.1804, val acc: 0.4894  (best train acc: 0.4448, best val acc: 0.5008, best train loss: 1.3093  @ epoch 138 )\n",
      "[Epoch: 0160] train loss: 1.2753, train acc: 0.3466, val loss: 1.1215, val acc: 0.4688  (best train acc: 0.4448, best val acc: 0.5022, best train loss: 1.2667  @ epoch 159 )\n",
      "[Epoch: 0180] train loss: 1.2283, train acc: 0.4666, val loss: 1.0411, val acc: 0.6098  (best train acc: 0.4666, best val acc: 0.6098, best train loss: 1.2170  @ epoch 176 )\n",
      "[Epoch: 0200] train loss: 1.2036, train acc: 0.4819, val loss: 1.0250, val acc: 0.6074  (best train acc: 0.4819, best val acc: 0.6216, best train loss: 1.2023  @ epoch 198 )\n",
      "[Epoch: 0220] train loss: 1.2012, train acc: 0.4895, val loss: 1.0043, val acc: 0.6438  (best train acc: 0.4943, best val acc: 0.6438, best train loss: 1.1991  @ epoch 216 )\n",
      "[Epoch: 0240] train loss: 1.1907, train acc: 0.5092, val loss: 1.0033, val acc: 0.6675  (best train acc: 0.5124, best val acc: 0.6675, best train loss: 1.1907  @ epoch 240 )\n",
      "[Epoch: 0260] train loss: 1.1895, train acc: 0.5219, val loss: 0.9932, val acc: 0.6786  (best train acc: 0.5219, best val acc: 0.6823, best train loss: 1.1849  @ epoch 254 )\n",
      "[Epoch: 0280] train loss: 1.1841, train acc: 0.5200, val loss: 0.9846, val acc: 0.6921  (best train acc: 0.5273, best val acc: 0.6921, best train loss: 1.1815  @ epoch 272 )\n",
      "[Epoch: 0300] train loss: 1.1857, train acc: 0.5210, val loss: 0.9837, val acc: 0.6951  (best train acc: 0.5320, best val acc: 0.6954, best train loss: 1.1755  @ epoch 298 )\n",
      "[Epoch: 0320] train loss: 1.1830, train acc: 0.5281, val loss: 0.9742, val acc: 0.6971  (best train acc: 0.5331, best val acc: 0.6985, best train loss: 1.1684  @ epoch 306 )\n",
      "[Epoch: 0340] train loss: 1.1711, train acc: 0.5249, val loss: 0.9647, val acc: 0.7008  (best train acc: 0.5348, best val acc: 0.7015, best train loss: 1.1633  @ epoch 322 )\n",
      "[Epoch: 0360] train loss: 1.1699, train acc: 0.5342, val loss: 0.9596, val acc: 0.7025  (best train acc: 0.5389, best val acc: 0.7039, best train loss: 1.1585  @ epoch 345 )\n",
      "[Epoch: 0380] train loss: 1.1654, train acc: 0.5314, val loss: 0.9517, val acc: 0.7046  (best train acc: 0.5389, best val acc: 0.7052, best train loss: 1.1530  @ epoch 375 )\n",
      "[Epoch: 0400] train loss: 1.1628, train acc: 0.5320, val loss: 0.9486, val acc: 0.7056  (best train acc: 0.5389, best val acc: 0.7066, best train loss: 1.1529  @ epoch 384 )\n",
      "[Epoch: 0420] train loss: 1.1577, train acc: 0.5284, val loss: 0.9465, val acc: 0.7052  (best train acc: 0.5389, best val acc: 0.7069, best train loss: 1.1489  @ epoch 415 )\n",
      "[Epoch: 0440] train loss: 1.1466, train acc: 0.5354, val loss: 0.9466, val acc: 0.7035  (best train acc: 0.5408, best val acc: 0.7079, best train loss: 1.1413  @ epoch 424 )\n",
      "[Epoch: 0460] train loss: 1.1484, train acc: 0.5356, val loss: 0.9352, val acc: 0.7086  (best train acc: 0.5408, best val acc: 0.7093, best train loss: 1.1391  @ epoch 450 )\n",
      "[Epoch: 0480] train loss: 1.1387, train acc: 0.5401, val loss: 0.9323, val acc: 0.7089  (best train acc: 0.5413, best val acc: 0.7093, best train loss: 1.1335  @ epoch 473 )\n",
      "[Epoch: 0500] train loss: 1.1366, train acc: 0.5415, val loss: 0.9279, val acc: 0.7103  (best train acc: 0.5433, best val acc: 0.7103, best train loss: 1.1294  @ epoch 498 )\n",
      "[Epoch: 0520] train loss: 1.1424, train acc: 0.5374, val loss: 0.9236, val acc: 0.7099  (best train acc: 0.5433, best val acc: 0.7110, best train loss: 1.1294  @ epoch 498 )\n",
      "[Epoch: 0540] train loss: 1.1248, train acc: 0.5392, val loss: 0.9180, val acc: 0.7126  (best train acc: 0.5433, best val acc: 0.7126, best train loss: 1.1248  @ epoch 540 )\n",
      "[Epoch: 0560] train loss: 1.1319, train acc: 0.5399, val loss: 0.9295, val acc: 0.7035  (best train acc: 0.5441, best val acc: 0.7126, best train loss: 1.1248  @ epoch 540 )\n",
      "[Epoch: 0580] train loss: 1.1466, train acc: 0.5343, val loss: 0.9159, val acc: 0.7126  (best train acc: 0.5441, best val acc: 0.7143, best train loss: 1.1248  @ epoch 540 )\n",
      "[Epoch: 0600] train loss: 1.1317, train acc: 0.5394, val loss: 0.9034, val acc: 0.7153  (best train acc: 0.5466, best val acc: 0.7153, best train loss: 1.1241  @ epoch 599 )\n",
      "[Epoch: 0620] train loss: 1.1325, train acc: 0.5393, val loss: 0.9063, val acc: 0.7143  (best train acc: 0.5466, best val acc: 0.7174, best train loss: 1.1204  @ epoch 617 )\n",
      "[Epoch: 0640] train loss: 1.1352, train acc: 0.5346, val loss: 0.8991, val acc: 0.7160  (best train acc: 0.5466, best val acc: 0.7177, best train loss: 1.1204  @ epoch 617 )\n",
      "[Epoch: 0660] train loss: 1.1195, train acc: 0.5438, val loss: 0.8933, val acc: 0.7187  (best train acc: 0.5466, best val acc: 0.7197, best train loss: 1.1132  @ epoch 656 )\n",
      "[Epoch: 0680] train loss: 1.1253, train acc: 0.5417, val loss: 0.8920, val acc: 0.7197  (best train acc: 0.5466, best val acc: 0.7197, best train loss: 1.1132  @ epoch 656 )\n",
      "[Epoch: 0700] train loss: 1.1269, train acc: 0.5387, val loss: 0.8896, val acc: 0.7197  (best train acc: 0.5476, best val acc: 0.7228, best train loss: 1.1132  @ epoch 656 )\n",
      "[Epoch: 0720] train loss: 1.1141, train acc: 0.5447, val loss: 0.9024, val acc: 0.7083  (best train acc: 0.5490, best val acc: 0.7228, best train loss: 1.1132  @ epoch 656 )\n",
      "[Epoch: 0740] train loss: 1.1211, train acc: 0.5418, val loss: 0.8961, val acc: 0.7133  (best train acc: 0.5505, best val acc: 0.7258, best train loss: 1.1116  @ epoch 739 )\n",
      "[Epoch: 0760] train loss: 1.1155, train acc: 0.5445, val loss: 0.8898, val acc: 0.7184  (best train acc: 0.5505, best val acc: 0.7258, best train loss: 1.1085  @ epoch 747 )\n",
      "[Epoch: 0780] train loss: 1.1206, train acc: 0.5432, val loss: 0.8790, val acc: 0.7261  (best train acc: 0.5505, best val acc: 0.7285, best train loss: 1.1085  @ epoch 747 )\n",
      "[Epoch: 0800] train loss: 1.1081, train acc: 0.5468, val loss: 0.8813, val acc: 0.7265  (best train acc: 0.5505, best val acc: 0.7319, best train loss: 1.1047  @ epoch 796 )\n",
      "[Epoch: 0820] train loss: 1.1156, train acc: 0.5424, val loss: 0.8937, val acc: 0.7022  (best train acc: 0.5520, best val acc: 0.7339, best train loss: 1.1037  @ epoch 817 )\n",
      "[Epoch: 0840] train loss: 1.1061, train acc: 0.5468, val loss: 0.8800, val acc: 0.7204  (best train acc: 0.5521, best val acc: 0.7339, best train loss: 1.1037  @ epoch 817 )\n",
      "[Epoch: 0860] train loss: 1.1127, train acc: 0.5479, val loss: 0.8661, val acc: 0.7285  (best train acc: 0.5529, best val acc: 0.7339, best train loss: 1.0970  @ epoch 850 )\n",
      "[Epoch: 0880] train loss: 1.1037, train acc: 0.5515, val loss: 0.8770, val acc: 0.7224  (best train acc: 0.5539, best val acc: 0.7339, best train loss: 1.0969  @ epoch 871 )\n",
      "[Epoch: 0900] train loss: 1.1091, train acc: 0.5436, val loss: 0.8642, val acc: 0.7275  (best train acc: 0.5539, best val acc: 0.7339, best train loss: 1.0969  @ epoch 871 )\n",
      "[Epoch: 0920] train loss: 1.1050, train acc: 0.5453, val loss: 0.8717, val acc: 0.7191  (best train acc: 0.5539, best val acc: 0.7339, best train loss: 1.0969  @ epoch 871 )\n",
      "[Epoch: 0940] train loss: 1.1059, train acc: 0.5470, val loss: 0.8648, val acc: 0.7288  (best train acc: 0.5539, best val acc: 0.7339, best train loss: 1.0958  @ epoch 937 )\n",
      "[Epoch: 0960] train loss: 1.0935, train acc: 0.5541, val loss: 0.8650, val acc: 0.7150  (best train acc: 0.5562, best val acc: 0.7339, best train loss: 1.0916  @ epoch 959 )\n",
      "[Epoch: 0980] train loss: 1.1002, train acc: 0.5468, val loss: 0.8627, val acc: 0.7214  (best train acc: 0.5562, best val acc: 0.7339, best train loss: 1.0916  @ epoch 959 )\n",
      "[Epoch: 1000] train loss: 1.1004, train acc: 0.5435, val loss: 0.8533, val acc: 0.7298  (best train acc: 0.5562, best val acc: 0.7339, best train loss: 1.0916  @ epoch 959 )\n",
      "[Epoch: 1020] train loss: 1.0889, train acc: 0.5528, val loss: 0.8535, val acc: 0.7234  (best train acc: 0.5562, best val acc: 0.7339, best train loss: 1.0889  @ epoch 1020 )\n",
      "[Epoch: 1040] train loss: 1.0990, train acc: 0.5465, val loss: 0.8470, val acc: 0.7272  (best train acc: 0.5562, best val acc: 0.7339, best train loss: 1.0889  @ epoch 1020 )\n",
      "[Epoch: 1060] train loss: 1.1108, train acc: 0.5413, val loss: 0.8673, val acc: 0.7066  (best train acc: 0.5562, best val acc: 0.7339, best train loss: 1.0889  @ epoch 1020 )\n",
      "[Epoch: 1080] train loss: 1.0958, train acc: 0.5472, val loss: 0.8472, val acc: 0.7268  (best train acc: 0.5562, best val acc: 0.7339, best train loss: 1.0860  @ epoch 1066 )\n",
      "[Epoch: 1100] train loss: 1.0937, train acc: 0.5521, val loss: 0.8510, val acc: 0.7248  (best train acc: 0.5562, best val acc: 0.7339, best train loss: 1.0860  @ epoch 1066 )\n",
      "[Epoch: 1120] train loss: 1.1182, train acc: 0.5432, val loss: 0.8539, val acc: 0.7164  (best train acc: 0.5562, best val acc: 0.7339, best train loss: 1.0809  @ epoch 1105 )\n",
      "[Epoch: 1140] train loss: 1.0960, train acc: 0.5434, val loss: 0.8466, val acc: 0.7224  (best train acc: 0.5562, best val acc: 0.7349, best train loss: 1.0809  @ epoch 1105 )\n",
      "[Epoch: 1160] train loss: 1.1097, train acc: 0.5267, val loss: 0.8320, val acc: 0.7292  (best train acc: 0.5562, best val acc: 0.7349, best train loss: 1.0809  @ epoch 1105 )\n",
      "[Epoch: 1180] train loss: 1.0983, train acc: 0.5467, val loss: 0.8394, val acc: 0.7285  (best train acc: 0.5562, best val acc: 0.7349, best train loss: 1.0809  @ epoch 1105 )\n",
      "[Epoch: 1200] train loss: 1.0903, train acc: 0.5515, val loss: 0.8418, val acc: 0.7258  (best train acc: 0.5562, best val acc: 0.7349, best train loss: 1.0809  @ epoch 1105 )\n",
      "[Epoch: 1220] train loss: 1.0952, train acc: 0.5504, val loss: 0.8486, val acc: 0.7147  (best train acc: 0.5578, best val acc: 0.7349, best train loss: 1.0809  @ epoch 1105 )\n",
      "[Epoch: 1240] train loss: 1.0943, train acc: 0.5468, val loss: 0.8271, val acc: 0.7302  (best train acc: 0.5578, best val acc: 0.7349, best train loss: 1.0809  @ epoch 1105 )\n",
      "[Epoch: 1260] train loss: 1.1047, train acc: 0.5411, val loss: 0.8356, val acc: 0.7265  (best train acc: 0.5578, best val acc: 0.7349, best train loss: 1.0766  @ epoch 1255 )\n",
      "[Epoch: 1280] train loss: 1.1014, train acc: 0.5414, val loss: 0.8317, val acc: 0.7285  (best train acc: 0.5578, best val acc: 0.7349, best train loss: 1.0766  @ epoch 1255 )\n",
      "[Epoch: 1300] train loss: 1.0903, train acc: 0.5458, val loss: 0.8416, val acc: 0.7218  (best train acc: 0.5578, best val acc: 0.7359, best train loss: 1.0766  @ epoch 1255 )\n",
      "[Epoch: 1320] train loss: 1.0871, train acc: 0.5470, val loss: 0.8336, val acc: 0.7272  (best train acc: 0.5578, best val acc: 0.7359, best train loss: 1.0766  @ epoch 1255 )\n",
      "[Epoch: 1340] train loss: 1.0871, train acc: 0.5497, val loss: 0.8242, val acc: 0.7322  (best train acc: 0.5578, best val acc: 0.7363, best train loss: 1.0766  @ epoch 1255 )\n",
      "[Epoch: 1360] train loss: 1.0964, train acc: 0.5451, val loss: 0.8223, val acc: 0.7346  (best train acc: 0.5578, best val acc: 0.7363, best train loss: 1.0766  @ epoch 1255 )\n",
      "[Epoch: 1380] train loss: 1.0978, train acc: 0.5445, val loss: 0.8274, val acc: 0.7282  (best train acc: 0.5578, best val acc: 0.7363, best train loss: 1.0766  @ epoch 1255 )\n",
      "[Epoch: 1400] train loss: 1.0867, train acc: 0.5458, val loss: 0.8258, val acc: 0.7268  (best train acc: 0.5578, best val acc: 0.7363, best train loss: 1.0741  @ epoch 1396 )\n",
      "[Epoch: 1420] train loss: 1.0743, train acc: 0.5547, val loss: 0.8257, val acc: 0.7265  (best train acc: 0.5578, best val acc: 0.7363, best train loss: 1.0741  @ epoch 1396 )\n",
      "[Epoch: 1440] train loss: 1.0794, train acc: 0.5463, val loss: 0.8247, val acc: 0.7261  (best train acc: 0.5578, best val acc: 0.7363, best train loss: 1.0741  @ epoch 1396 )\n",
      "[Epoch: 1460] train loss: 1.0988, train acc: 0.5437, val loss: 0.8178, val acc: 0.7319  (best train acc: 0.5578, best val acc: 0.7363, best train loss: 1.0741  @ epoch 1396 )\n",
      "[Epoch: 1480] train loss: 1.0875, train acc: 0.5476, val loss: 0.8375, val acc: 0.7103  (best train acc: 0.5578, best val acc: 0.7363, best train loss: 1.0741  @ epoch 1396 )\n",
      "[Epoch: 1500] train loss: 1.0784, train acc: 0.5501, val loss: 0.8199, val acc: 0.7275  (best train acc: 0.5578, best val acc: 0.7363, best train loss: 1.0741  @ epoch 1396 )\n",
      "[Epoch: 1520] train loss: 1.0792, train acc: 0.5562, val loss: 0.8179, val acc: 0.7309  (best train acc: 0.5578, best val acc: 0.7363, best train loss: 1.0741  @ epoch 1396 )\n",
      "[Epoch: 1540] train loss: 1.0902, train acc: 0.5456, val loss: 0.8174, val acc: 0.7272  (best train acc: 0.5578, best val acc: 0.7363, best train loss: 1.0701  @ epoch 1537 )\n",
      "[Epoch: 1560] train loss: 1.0829, train acc: 0.5538, val loss: 0.8446, val acc: 0.6867  (best train acc: 0.5578, best val acc: 0.7363, best train loss: 1.0701  @ epoch 1537 )\n",
      "[Epoch: 1580] train loss: 1.0753, train acc: 0.5531, val loss: 0.8092, val acc: 0.7285  (best train acc: 0.5578, best val acc: 0.7363, best train loss: 1.0701  @ epoch 1537 )\n",
      "[Epoch: 1600] train loss: 1.0837, train acc: 0.5521, val loss: 0.8560, val acc: 0.6641  (best train acc: 0.5596, best val acc: 0.7363, best train loss: 1.0701  @ epoch 1537 )\n",
      "[Epoch: 1620] train loss: 1.0712, train acc: 0.5558, val loss: 0.8165, val acc: 0.7322  (best train acc: 0.5596, best val acc: 0.7373, best train loss: 1.0701  @ epoch 1537 )\n",
      "[Epoch: 1640] train loss: 1.0769, train acc: 0.5524, val loss: 0.8402, val acc: 0.6931  (best train acc: 0.5596, best val acc: 0.7373, best train loss: 1.0701  @ epoch 1537 )\n",
      "[Epoch: 1660] train loss: 1.0806, train acc: 0.5469, val loss: 0.8101, val acc: 0.7339  (best train acc: 0.5596, best val acc: 0.7373, best train loss: 1.0701  @ epoch 1537 )\n",
      "[Epoch: 1680] train loss: 1.0938, train acc: 0.5411, val loss: 0.8087, val acc: 0.7312  (best train acc: 0.5596, best val acc: 0.7373, best train loss: 1.0701  @ epoch 1537 )\n",
      "[Epoch: 1700] train loss: 1.0872, train acc: 0.5458, val loss: 0.8097, val acc: 0.7359  (best train acc: 0.5596, best val acc: 0.7373, best train loss: 1.0701  @ epoch 1537 )\n",
      "[Epoch: 1720] train loss: 1.0795, train acc: 0.5512, val loss: 0.8169, val acc: 0.7261  (best train acc: 0.5596, best val acc: 0.7373, best train loss: 1.0701  @ epoch 1537 )\n",
      "[Epoch: 1740] train loss: 1.0258, train acc: 0.5764, val loss: 0.8151, val acc: 0.7275  (best train acc: 0.5764, best val acc: 0.7373, best train loss: 1.0258  @ epoch 1740 )\n",
      "[Epoch: 1760] train loss: 1.0047, train acc: 0.5781, val loss: 0.7896, val acc: 0.7251  (best train acc: 0.5854, best val acc: 0.7420, best train loss: 1.0008  @ epoch 1758 )\n",
      "[Epoch: 1780] train loss: 1.0048, train acc: 0.5798, val loss: 0.7953, val acc: 0.7315  (best train acc: 0.5867, best val acc: 0.7420, best train loss: 0.9942  @ epoch 1777 )\n",
      "[Epoch: 1800] train loss: 0.9865, train acc: 0.5949, val loss: 0.8126, val acc: 0.7309  (best train acc: 0.5996, best val acc: 0.7427, best train loss: 0.9763  @ epoch 1792 )\n",
      "[Epoch: 1820] train loss: 0.9766, train acc: 0.6038, val loss: 0.7950, val acc: 0.7352  (best train acc: 0.6128, best val acc: 0.7460, best train loss: 0.9609  @ epoch 1817 )\n",
      "[Epoch: 1840] train loss: 0.9617, train acc: 0.6237, val loss: 0.7843, val acc: 0.7315  (best train acc: 0.6307, best val acc: 0.7460, best train loss: 0.9508  @ epoch 1838 )\n",
      "[Epoch: 1860] train loss: 0.9432, train acc: 0.6682, val loss: 0.7504, val acc: 0.8199  (best train acc: 0.6682, best val acc: 0.8199, best train loss: 0.9416  @ epoch 1855 )\n",
      "[Epoch: 1880] train loss: 0.9428, train acc: 0.6721, val loss: 0.7269, val acc: 0.8351  (best train acc: 0.6724, best val acc: 0.8398, best train loss: 0.9372  @ epoch 1878 )\n",
      "[Epoch: 1900] train loss: 0.9316, train acc: 0.6722, val loss: 0.7141, val acc: 0.8482  (best train acc: 0.6790, best val acc: 0.8523, best train loss: 0.9297  @ epoch 1891 )\n",
      "[Epoch: 1920] train loss: 0.9310, train acc: 0.6801, val loss: 0.6985, val acc: 0.8617  (best train acc: 0.6861, best val acc: 0.8637, best train loss: 0.9275  @ epoch 1906 )\n",
      "[Epoch: 1940] train loss: 0.9449, train acc: 0.6677, val loss: 0.6988, val acc: 0.8482  (best train acc: 0.6887, best val acc: 0.8637, best train loss: 0.9225  @ epoch 1931 )\n",
      "[Epoch: 1960] train loss: 0.9289, train acc: 0.6747, val loss: 0.7020, val acc: 0.8449  (best train acc: 0.6887, best val acc: 0.8637, best train loss: 0.9208  @ epoch 1949 )\n",
      "[Epoch: 1980] train loss: 0.9272, train acc: 0.6816, val loss: 0.6962, val acc: 0.8587  (best train acc: 0.6887, best val acc: 0.8664, best train loss: 0.9159  @ epoch 1976 )\n",
      "[Epoch: 2000] train loss: 0.9403, train acc: 0.6776, val loss: 0.6846, val acc: 0.8644  (best train acc: 0.6887, best val acc: 0.8664, best train loss: 0.9159  @ epoch 1976 )\n",
      "[Epoch: 2020] train loss: 0.9297, train acc: 0.6742, val loss: 0.6840, val acc: 0.8594  (best train acc: 0.6914, best val acc: 0.8664, best train loss: 0.9159  @ epoch 1976 )\n",
      "[Epoch: 2040] train loss: 0.9258, train acc: 0.6781, val loss: 0.6853, val acc: 0.8526  (best train acc: 0.6914, best val acc: 0.8695, best train loss: 0.9138  @ epoch 2027 )\n",
      "[Epoch: 2060] train loss: 0.9198, train acc: 0.6835, val loss: 0.6811, val acc: 0.8617  (best train acc: 0.6914, best val acc: 0.8695, best train loss: 0.9128  @ epoch 2051 )\n",
      "[Epoch: 2080] train loss: 0.9317, train acc: 0.6805, val loss: 0.6886, val acc: 0.8550  (best train acc: 0.6919, best val acc: 0.8705, best train loss: 0.9115  @ epoch 2079 )\n",
      "[Epoch: 2100] train loss: 0.9206, train acc: 0.6840, val loss: 0.6786, val acc: 0.8661  (best train acc: 0.6934, best val acc: 0.8725, best train loss: 0.9062  @ epoch 2082 )\n",
      "[Epoch: 2120] train loss: 0.9282, train acc: 0.6758, val loss: 0.6802, val acc: 0.8634  (best train acc: 0.6938, best val acc: 0.8725, best train loss: 0.9043  @ epoch 2114 )\n",
      "[Epoch: 2140] train loss: 0.9244, train acc: 0.6803, val loss: 0.6721, val acc: 0.8614  (best train acc: 0.6938, best val acc: 0.8725, best train loss: 0.9032  @ epoch 2131 )\n",
      "[Epoch: 2160] train loss: 0.9141, train acc: 0.6844, val loss: 0.6665, val acc: 0.8621  (best train acc: 0.6944, best val acc: 0.8725, best train loss: 0.9032  @ epoch 2131 )\n",
      "[Epoch: 2180] train loss: 0.9070, train acc: 0.6930, val loss: 0.6692, val acc: 0.8658  (best train acc: 0.6944, best val acc: 0.8739, best train loss: 0.9005  @ epoch 2179 )\n",
      "[Epoch: 2200] train loss: 0.9261, train acc: 0.6838, val loss: 0.6771, val acc: 0.8425  (best train acc: 0.6959, best val acc: 0.8739, best train loss: 0.9005  @ epoch 2179 )\n",
      "[Epoch: 2220] train loss: 0.9166, train acc: 0.6898, val loss: 0.6630, val acc: 0.8621  (best train acc: 0.6982, best val acc: 0.8752, best train loss: 0.8950  @ epoch 2208 )\n",
      "[Epoch: 2240] train loss: 0.9124, train acc: 0.6906, val loss: 0.6687, val acc: 0.8685  (best train acc: 0.6982, best val acc: 0.8752, best train loss: 0.8950  @ epoch 2208 )\n",
      "[Epoch: 2260] train loss: 0.9118, train acc: 0.6901, val loss: 0.6620, val acc: 0.8644  (best train acc: 0.6982, best val acc: 0.8752, best train loss: 0.8900  @ epoch 2254 )\n",
      "[Epoch: 2280] train loss: 0.9127, train acc: 0.6923, val loss: 0.6669, val acc: 0.8718  (best train acc: 0.6982, best val acc: 0.8752, best train loss: 0.8900  @ epoch 2254 )\n",
      "[Epoch: 2300] train loss: 0.8960, train acc: 0.6932, val loss: 0.6623, val acc: 0.8634  (best train acc: 0.6982, best val acc: 0.8752, best train loss: 0.8900  @ epoch 2254 )\n",
      "[Epoch: 2320] train loss: 0.9566, train acc: 0.6118, val loss: 0.7014, val acc: 0.8071  (best train acc: 0.6982, best val acc: 0.8782, best train loss: 0.8900  @ epoch 2254 )\n",
      "[Epoch: 2340] train loss: 0.9157, train acc: 0.6794, val loss: 0.6880, val acc: 0.8263  (best train acc: 0.6982, best val acc: 0.8816, best train loss: 0.8900  @ epoch 2254 )\n",
      "[Epoch: 2360] train loss: 0.9020, train acc: 0.6912, val loss: 0.6587, val acc: 0.8725  (best train acc: 0.7016, best val acc: 0.8816, best train loss: 0.8900  @ epoch 2254 )\n",
      "[Epoch: 2380] train loss: 0.9071, train acc: 0.6915, val loss: 0.6568, val acc: 0.8772  (best train acc: 0.7021, best val acc: 0.8816, best train loss: 0.8900  @ epoch 2254 )\n",
      "[Epoch: 2400] train loss: 0.9043, train acc: 0.6941, val loss: 0.6607, val acc: 0.8705  (best train acc: 0.7021, best val acc: 0.8816, best train loss: 0.8900  @ epoch 2254 )\n",
      "[Epoch: 2420] train loss: 0.9050, train acc: 0.6934, val loss: 0.6672, val acc: 0.8600  (best train acc: 0.7021, best val acc: 0.8816, best train loss: 0.8864  @ epoch 2407 )\n",
      "[Epoch: 2440] train loss: 0.8952, train acc: 0.7005, val loss: 0.6516, val acc: 0.8749  (best train acc: 0.7021, best val acc: 0.8816, best train loss: 0.8864  @ epoch 2407 )\n",
      "[Epoch: 2460] train loss: 0.9118, train acc: 0.6889, val loss: 0.6612, val acc: 0.8678  (best train acc: 0.7021, best val acc: 0.8816, best train loss: 0.8817  @ epoch 2455 )\n",
      "[Epoch: 2480] train loss: 0.8817, train acc: 0.7006, val loss: 0.6488, val acc: 0.8820  (best train acc: 0.7021, best val acc: 0.8820, best train loss: 0.8817  @ epoch 2480 )\n",
      "[Epoch: 2500] train loss: 0.8921, train acc: 0.6942, val loss: 0.6480, val acc: 0.8631  (best train acc: 0.7021, best val acc: 0.8820, best train loss: 0.8817  @ epoch 2480 )\n",
      "[Epoch: 2520] train loss: 0.9086, train acc: 0.6907, val loss: 0.6546, val acc: 0.8749  (best train acc: 0.7021, best val acc: 0.8820, best train loss: 0.8817  @ epoch 2480 )\n",
      "[Epoch: 2540] train loss: 0.8960, train acc: 0.6925, val loss: 0.6499, val acc: 0.8691  (best train acc: 0.7021, best val acc: 0.8840, best train loss: 0.8810  @ epoch 2533 )\n",
      "[Epoch: 2560] train loss: 0.8988, train acc: 0.6948, val loss: 0.6417, val acc: 0.8793  (best train acc: 0.7033, best val acc: 0.8840, best train loss: 0.8810  @ epoch 2533 )\n",
      "[Epoch: 2580] train loss: 0.8912, train acc: 0.7010, val loss: 0.6442, val acc: 0.8793  (best train acc: 0.7045, best val acc: 0.8840, best train loss: 0.8810  @ epoch 2533 )\n",
      "[Epoch: 2600] train loss: 0.8917, train acc: 0.6915, val loss: 0.6433, val acc: 0.8830  (best train acc: 0.7045, best val acc: 0.8840, best train loss: 0.8810  @ epoch 2533 )\n",
      "[Epoch: 2620] train loss: 0.8917, train acc: 0.6949, val loss: 0.6440, val acc: 0.8786  (best train acc: 0.7045, best val acc: 0.8840, best train loss: 0.8810  @ epoch 2533 )\n",
      "[Epoch: 2640] train loss: 0.8876, train acc: 0.7013, val loss: 0.6370, val acc: 0.8732  (best train acc: 0.7045, best val acc: 0.8840, best train loss: 0.8810  @ epoch 2533 )\n",
      "[Epoch: 2660] train loss: 0.8935, train acc: 0.7007, val loss: 0.6400, val acc: 0.8799  (best train acc: 0.7053, best val acc: 0.8857, best train loss: 0.8751  @ epoch 2651 )\n",
      "[Epoch: 2680] train loss: 0.8945, train acc: 0.7002, val loss: 0.6449, val acc: 0.8725  (best train acc: 0.7053, best val acc: 0.8857, best train loss: 0.8751  @ epoch 2651 )\n",
      "[Epoch: 2700] train loss: 0.8824, train acc: 0.7025, val loss: 0.6460, val acc: 0.8715  (best train acc: 0.7081, best val acc: 0.8857, best train loss: 0.8697  @ epoch 2698 )\n",
      "[Epoch: 2720] train loss: 0.8916, train acc: 0.6989, val loss: 0.6343, val acc: 0.8813  (best train acc: 0.7081, best val acc: 0.8857, best train loss: 0.8697  @ epoch 2698 )\n",
      "[Epoch: 2740] train loss: 0.8957, train acc: 0.7020, val loss: 0.6413, val acc: 0.8718  (best train acc: 0.7081, best val acc: 0.8884, best train loss: 0.8697  @ epoch 2698 )\n",
      "[Epoch: 2760] train loss: 0.9056, train acc: 0.6972, val loss: 0.6448, val acc: 0.8742  (best train acc: 0.7081, best val acc: 0.8884, best train loss: 0.8697  @ epoch 2698 )\n",
      "[Epoch: 2780] train loss: 0.8939, train acc: 0.7016, val loss: 0.6303, val acc: 0.8799  (best train acc: 0.7081, best val acc: 0.8884, best train loss: 0.8697  @ epoch 2698 )\n",
      "[Epoch: 2800] train loss: 0.8873, train acc: 0.7054, val loss: 0.6279, val acc: 0.8779  (best train acc: 0.7081, best val acc: 0.8884, best train loss: 0.8697  @ epoch 2698 )\n",
      "[Epoch: 2820] train loss: 0.8889, train acc: 0.6937, val loss: 0.6557, val acc: 0.8452  (best train acc: 0.7081, best val acc: 0.8884, best train loss: 0.8697  @ epoch 2698 )\n",
      "[Epoch: 2840] train loss: 0.8859, train acc: 0.6984, val loss: 0.6330, val acc: 0.8860  (best train acc: 0.7081, best val acc: 0.8897, best train loss: 0.8697  @ epoch 2698 )\n",
      "[Epoch: 2860] train loss: 0.9052, train acc: 0.6934, val loss: 0.6320, val acc: 0.8860  (best train acc: 0.7081, best val acc: 0.8897, best train loss: 0.8697  @ epoch 2698 )\n",
      "[Epoch: 2880] train loss: 0.8901, train acc: 0.7017, val loss: 0.6299, val acc: 0.8799  (best train acc: 0.7081, best val acc: 0.8897, best train loss: 0.8697  @ epoch 2698 )\n",
      "[Epoch: 2900] train loss: 0.8902, train acc: 0.6969, val loss: 0.6241, val acc: 0.8772  (best train acc: 0.7081, best val acc: 0.8897, best train loss: 0.8697  @ epoch 2698 )\n",
      "[Epoch: 2920] train loss: 0.8846, train acc: 0.7001, val loss: 0.6238, val acc: 0.8833  (best train acc: 0.7095, best val acc: 0.8897, best train loss: 0.8673  @ epoch 2912 )\n",
      "[Epoch: 2940] train loss: 0.8928, train acc: 0.6996, val loss: 0.6250, val acc: 0.8833  (best train acc: 0.7095, best val acc: 0.8897, best train loss: 0.8673  @ epoch 2912 )\n",
      "[Epoch: 2960] train loss: 0.8806, train acc: 0.7011, val loss: 0.6309, val acc: 0.8759  (best train acc: 0.7095, best val acc: 0.8897, best train loss: 0.8673  @ epoch 2912 )\n",
      "[Epoch: 2980] train loss: 0.8904, train acc: 0.7018, val loss: 0.6221, val acc: 0.8894  (best train acc: 0.7095, best val acc: 0.8897, best train loss: 0.8673  @ epoch 2912 )\n",
      "[Epoch: 3000] train loss: 0.8905, train acc: 0.7052, val loss: 0.6317, val acc: 0.8769  (best train acc: 0.7095, best val acc: 0.8897, best train loss: 0.8673  @ epoch 2912 )\n",
      "[Epoch: 3020] train loss: 0.9019, train acc: 0.6941, val loss: 0.6449, val acc: 0.8465  (best train acc: 0.7095, best val acc: 0.8914, best train loss: 0.8673  @ epoch 2912 )\n",
      "[Epoch: 3040] train loss: 0.8832, train acc: 0.6974, val loss: 0.6217, val acc: 0.8732  (best train acc: 0.7095, best val acc: 0.8914, best train loss: 0.8673  @ epoch 2912 )\n",
      "[Epoch: 3060] train loss: 0.9549, train acc: 0.6270, val loss: 0.6842, val acc: 0.7909  (best train acc: 0.7095, best val acc: 0.8914, best train loss: 0.8673  @ epoch 2912 )\n",
      "[Epoch: 3080] train loss: 0.8908, train acc: 0.6923, val loss: 0.6273, val acc: 0.8850  (best train acc: 0.7095, best val acc: 0.8914, best train loss: 0.8673  @ epoch 2912 )\n",
      "[Epoch: 3100] train loss: 0.8822, train acc: 0.7044, val loss: 0.6377, val acc: 0.8779  (best train acc: 0.7095, best val acc: 0.8914, best train loss: 0.8673  @ epoch 2912 )\n",
      "[Epoch: 3120] train loss: 0.8930, train acc: 0.7009, val loss: 0.6298, val acc: 0.8806  (best train acc: 0.7104, best val acc: 0.8914, best train loss: 0.8673  @ epoch 2912 )\n",
      "[Epoch: 3140] train loss: 0.8915, train acc: 0.6990, val loss: 0.6356, val acc: 0.8725  (best train acc: 0.7104, best val acc: 0.8914, best train loss: 0.8673  @ epoch 2912 )\n",
      "[Epoch: 3160] train loss: 0.8799, train acc: 0.7063, val loss: 0.6223, val acc: 0.8739  (best train acc: 0.7104, best val acc: 0.8914, best train loss: 0.8673  @ epoch 2912 )\n",
      "[Epoch: 3180] train loss: 0.8902, train acc: 0.7014, val loss: 0.6169, val acc: 0.8796  (best train acc: 0.7104, best val acc: 0.8914, best train loss: 0.8673  @ epoch 2912 )\n",
      "[Epoch: 3200] train loss: 0.8925, train acc: 0.6983, val loss: 0.6189, val acc: 0.8850  (best train acc: 0.7104, best val acc: 0.8914, best train loss: 0.8647  @ epoch 3194 )\n",
      "[Epoch: 3220] train loss: 0.8799, train acc: 0.7037, val loss: 0.6153, val acc: 0.8877  (best train acc: 0.7119, best val acc: 0.8914, best train loss: 0.8643  @ epoch 3218 )\n",
      "[Epoch: 3240] train loss: 0.8730, train acc: 0.7057, val loss: 0.6116, val acc: 0.8732  (best train acc: 0.7119, best val acc: 0.8914, best train loss: 0.8643  @ epoch 3218 )\n",
      "[Epoch: 3260] train loss: 0.8786, train acc: 0.7056, val loss: 0.6363, val acc: 0.8583  (best train acc: 0.7119, best val acc: 0.8914, best train loss: 0.8643  @ epoch 3218 )\n",
      "[Epoch: 3280] train loss: 0.8744, train acc: 0.7023, val loss: 0.6127, val acc: 0.8752  (best train acc: 0.7119, best val acc: 0.8914, best train loss: 0.8643  @ epoch 3218 )\n",
      "[Epoch: 3300] train loss: 0.8745, train acc: 0.7053, val loss: 0.6253, val acc: 0.8806  (best train acc: 0.7119, best val acc: 0.8914, best train loss: 0.8643  @ epoch 3218 )\n",
      "[Epoch: 3320] train loss: 0.8758, train acc: 0.7019, val loss: 0.6185, val acc: 0.8843  (best train acc: 0.7119, best val acc: 0.8914, best train loss: 0.8643  @ epoch 3218 )\n",
      "[Epoch: 3340] train loss: 0.8848, train acc: 0.7019, val loss: 0.6243, val acc: 0.8772  (best train acc: 0.7119, best val acc: 0.8914, best train loss: 0.8643  @ epoch 3218 )\n",
      "[Epoch: 3360] train loss: 0.8809, train acc: 0.7017, val loss: 0.6175, val acc: 0.8860  (best train acc: 0.7119, best val acc: 0.8914, best train loss: 0.8643  @ epoch 3218 )\n",
      "[Epoch: 3380] train loss: 0.8714, train acc: 0.7014, val loss: 0.6196, val acc: 0.8816  (best train acc: 0.7133, best val acc: 0.8914, best train loss: 0.8641  @ epoch 3376 )\n",
      "[Epoch: 3400] train loss: 0.8386, train acc: 0.7099, val loss: 0.6026, val acc: 0.8752  (best train acc: 0.7167, best val acc: 0.8914, best train loss: 0.8288  @ epoch 3399 )\n",
      "[Epoch: 3420] train loss: 0.8352, train acc: 0.7091, val loss: 0.6110, val acc: 0.8607  (best train acc: 0.7238, best val acc: 0.8914, best train loss: 0.8097  @ epoch 3403 )\n",
      "[Epoch: 3440] train loss: 0.8172, train acc: 0.7193, val loss: 0.6208, val acc: 0.8793  (best train acc: 0.7238, best val acc: 0.8914, best train loss: 0.8097  @ epoch 3403 )\n",
      "[Epoch: 3460] train loss: 0.8283, train acc: 0.7097, val loss: 0.6064, val acc: 0.8786  (best train acc: 0.7238, best val acc: 0.8924, best train loss: 0.8048  @ epoch 3446 )\n",
      "[Epoch: 3480] train loss: 0.8152, train acc: 0.7219, val loss: 0.6142, val acc: 0.8820  (best train acc: 0.7253, best val acc: 0.8924, best train loss: 0.8048  @ epoch 3446 )\n",
      "[Epoch: 3500] train loss: 0.8108, train acc: 0.7270, val loss: 0.6039, val acc: 0.8772  (best train acc: 0.7270, best val acc: 0.8924, best train loss: 0.8048  @ epoch 3446 )\n",
      "[Epoch: 3520] train loss: 0.8194, train acc: 0.7079, val loss: 0.6257, val acc: 0.8600  (best train acc: 0.7270, best val acc: 0.8924, best train loss: 0.8021  @ epoch 3509 )\n",
      "[Epoch: 3540] train loss: 0.8054, train acc: 0.7244, val loss: 0.5975, val acc: 0.8911  (best train acc: 0.7270, best val acc: 0.8944, best train loss: 0.8021  @ epoch 3509 )\n",
      "[Epoch: 3560] train loss: 0.8158, train acc: 0.7176, val loss: 0.6176, val acc: 0.8816  (best train acc: 0.7274, best val acc: 0.8944, best train loss: 0.8021  @ epoch 3509 )\n",
      "[Epoch: 3580] train loss: 0.8272, train acc: 0.7161, val loss: 0.5923, val acc: 0.8772  (best train acc: 0.7419, best val acc: 0.8944, best train loss: 0.7997  @ epoch 3565 )\n",
      "[Epoch: 3600] train loss: 0.8031, train acc: 0.7228, val loss: 0.5981, val acc: 0.8840  (best train acc: 0.7419, best val acc: 0.8944, best train loss: 0.7997  @ epoch 3565 )\n",
      "[Epoch: 3620] train loss: 0.8166, train acc: 0.7206, val loss: 0.6093, val acc: 0.8799  (best train acc: 0.7419, best val acc: 0.8944, best train loss: 0.7997  @ epoch 3565 )\n",
      "[Epoch: 3640] train loss: 0.8059, train acc: 0.7186, val loss: 0.6008, val acc: 0.8847  (best train acc: 0.7419, best val acc: 0.8944, best train loss: 0.7997  @ epoch 3565 )\n",
      "[Epoch: 3660] train loss: 0.8016, train acc: 0.7222, val loss: 0.5942, val acc: 0.8934  (best train acc: 0.7419, best val acc: 0.8944, best train loss: 0.7962  @ epoch 3654 )\n",
      "[Epoch: 3680] train loss: 0.8212, train acc: 0.7204, val loss: 0.6012, val acc: 0.8843  (best train acc: 0.7419, best val acc: 0.8944, best train loss: 0.7962  @ epoch 3654 )\n",
      "[Epoch: 3700] train loss: 0.8207, train acc: 0.6996, val loss: 0.6422, val acc: 0.8206  (best train acc: 0.7419, best val acc: 0.8944, best train loss: 0.7962  @ epoch 3654 )\n",
      "[Epoch: 3720] train loss: 0.8206, train acc: 0.7157, val loss: 0.6161, val acc: 0.8573  (best train acc: 0.7419, best val acc: 0.8944, best train loss: 0.7962  @ epoch 3654 )\n",
      "[Epoch: 3740] train loss: 0.8273, train acc: 0.7095, val loss: 0.6062, val acc: 0.8796  (best train acc: 0.7419, best val acc: 0.8944, best train loss: 0.7962  @ epoch 3654 )\n",
      "[Epoch: 3760] train loss: 0.8396, train acc: 0.7127, val loss: 0.5959, val acc: 0.8644  (best train acc: 0.7419, best val acc: 0.8944, best train loss: 0.7962  @ epoch 3654 )\n",
      "[Epoch: 3780] train loss: 0.8267, train acc: 0.7108, val loss: 0.6095, val acc: 0.8786  (best train acc: 0.7419, best val acc: 0.8944, best train loss: 0.7962  @ epoch 3654 )\n",
      "[Epoch: 3800] train loss: 0.8270, train acc: 0.7155, val loss: 0.5969, val acc: 0.8833  (best train acc: 0.7419, best val acc: 0.8944, best train loss: 0.7962  @ epoch 3654 )\n",
      "[Epoch: 3820] train loss: 0.8117, train acc: 0.7200, val loss: 0.5993, val acc: 0.8820  (best train acc: 0.7419, best val acc: 0.8944, best train loss: 0.7962  @ epoch 3654 )\n",
      "[Epoch: 3840] train loss: 0.8194, train acc: 0.7235, val loss: 0.5938, val acc: 0.8867  (best train acc: 0.7419, best val acc: 0.8944, best train loss: 0.7907  @ epoch 3822 )\n",
      "[Epoch: 3860] train loss: 0.8204, train acc: 0.7089, val loss: 0.6074, val acc: 0.8600  (best train acc: 0.7419, best val acc: 0.8944, best train loss: 0.7907  @ epoch 3822 )\n",
      "[Epoch: 3880] train loss: 0.8345, train acc: 0.7063, val loss: 0.6058, val acc: 0.8793  (best train acc: 0.7419, best val acc: 0.8944, best train loss: 0.7907  @ epoch 3822 )\n",
      "[Epoch: 3900] train loss: 0.8223, train acc: 0.7204, val loss: 0.6045, val acc: 0.8786  (best train acc: 0.7419, best val acc: 0.8944, best train loss: 0.7907  @ epoch 3822 )\n",
      "[Epoch: 3920] train loss: 0.8012, train acc: 0.7238, val loss: 0.5867, val acc: 0.8894  (best train acc: 0.7419, best val acc: 0.8944, best train loss: 0.7907  @ epoch 3822 )\n",
      "[Epoch: 3940] train loss: 0.7941, train acc: 0.7267, val loss: 0.5863, val acc: 0.8863  (best train acc: 0.7419, best val acc: 0.8944, best train loss: 0.7830  @ epoch 3935 )\n",
      "[Epoch: 3960] train loss: 0.8097, train acc: 0.7133, val loss: 0.5821, val acc: 0.8823  (best train acc: 0.7419, best val acc: 0.8944, best train loss: 0.7830  @ epoch 3935 )\n",
      "[Epoch: 3980] train loss: 0.8123, train acc: 0.7234, val loss: 0.5956, val acc: 0.8823  (best train acc: 0.7419, best val acc: 0.8944, best train loss: 0.7830  @ epoch 3935 )\n",
      "[Epoch: 4000] train loss: 0.8096, train acc: 0.7219, val loss: 0.5906, val acc: 0.8880  (best train acc: 0.7419, best val acc: 0.8944, best train loss: 0.7830  @ epoch 3935 )\n",
      "[Epoch: 4020] train loss: 0.8010, train acc: 0.7255, val loss: 0.5878, val acc: 0.8884  (best train acc: 0.7419, best val acc: 0.8944, best train loss: 0.7830  @ epoch 3935 )\n",
      "[Epoch: 4040] train loss: 0.8217, train acc: 0.7097, val loss: 0.6073, val acc: 0.8580  (best train acc: 0.7419, best val acc: 0.8944, best train loss: 0.7830  @ epoch 3935 )\n",
      "[Epoch: 4060] train loss: 0.8187, train acc: 0.7128, val loss: 0.6199, val acc: 0.8492  (best train acc: 0.7419, best val acc: 0.8944, best train loss: 0.7830  @ epoch 3935 )\n",
      "[Epoch: 4080] train loss: 0.8242, train acc: 0.6997, val loss: 0.6015, val acc: 0.8469  (best train acc: 0.7419, best val acc: 0.8944, best train loss: 0.7830  @ epoch 3935 )\n",
      "[Epoch: 4100] train loss: 0.8117, train acc: 0.7223, val loss: 0.5856, val acc: 0.8914  (best train acc: 0.7419, best val acc: 0.8944, best train loss: 0.7830  @ epoch 3935 )\n",
      "[Epoch: 4120] train loss: 0.8209, train acc: 0.7192, val loss: 0.5889, val acc: 0.8803  (best train acc: 0.7419, best val acc: 0.8944, best train loss: 0.7830  @ epoch 3935 )\n",
      "[Epoch: 4140] train loss: 0.8132, train acc: 0.7123, val loss: 0.5818, val acc: 0.8850  (best train acc: 0.7419, best val acc: 0.8944, best train loss: 0.7830  @ epoch 3935 )\n",
      "[Epoch: 4160] train loss: 0.8015, train acc: 0.7196, val loss: 0.5860, val acc: 0.8830  (best train acc: 0.7419, best val acc: 0.8944, best train loss: 0.7830  @ epoch 3935 )\n",
      "[Epoch: 4180] train loss: 0.7951, train acc: 0.7240, val loss: 0.5824, val acc: 0.8836  (best train acc: 0.7419, best val acc: 0.8944, best train loss: 0.7830  @ epoch 3935 )\n",
      "[Epoch: 4200] train loss: 0.8231, train acc: 0.7238, val loss: 0.5817, val acc: 0.8863  (best train acc: 0.7419, best val acc: 0.8944, best train loss: 0.7830  @ epoch 3935 )\n",
      "[Epoch: 4220] train loss: 0.7990, train acc: 0.7256, val loss: 0.5900, val acc: 0.8863  (best train acc: 0.7419, best val acc: 0.8944, best train loss: 0.7830  @ epoch 3935 )\n",
      "[Epoch: 4240] train loss: 0.8098, train acc: 0.7290, val loss: 0.5824, val acc: 0.8914  (best train acc: 0.7419, best val acc: 0.8944, best train loss: 0.7830  @ epoch 3935 )\n",
      "[Epoch: 4260] train loss: 0.8074, train acc: 0.7286, val loss: 0.5779, val acc: 0.8870  (best train acc: 0.7419, best val acc: 0.8944, best train loss: 0.7830  @ epoch 3935 )\n",
      "[Epoch: 4280] train loss: 0.8056, train acc: 0.7224, val loss: 0.5778, val acc: 0.8823  (best train acc: 0.7419, best val acc: 0.8944, best train loss: 0.7830  @ epoch 3935 )\n",
      "[Epoch: 4300] train loss: 0.8420, train acc: 0.6966, val loss: 0.6010, val acc: 0.8388  (best train acc: 0.7419, best val acc: 0.8944, best train loss: 0.7830  @ epoch 3935 )\n",
      "[Epoch: 4320] train loss: 0.8125, train acc: 0.7122, val loss: 0.5815, val acc: 0.8671  (best train acc: 0.7419, best val acc: 0.8951, best train loss: 0.7830  @ epoch 3935 )\n",
      "[Epoch: 4340] train loss: 0.8256, train acc: 0.7157, val loss: 0.5838, val acc: 0.8877  (best train acc: 0.7419, best val acc: 0.8951, best train loss: 0.7830  @ epoch 3935 )\n",
      "[Epoch: 4360] train loss: 0.7907, train acc: 0.7225, val loss: 0.5838, val acc: 0.8921  (best train acc: 0.7419, best val acc: 0.8951, best train loss: 0.7830  @ epoch 3935 )\n",
      "[Epoch: 4380] train loss: 0.8093, train acc: 0.7076, val loss: 0.5790, val acc: 0.8847  (best train acc: 0.7419, best val acc: 0.8951, best train loss: 0.7830  @ epoch 3935 )\n",
      "[Epoch: 4400] train loss: 0.7996, train acc: 0.7193, val loss: 0.5851, val acc: 0.8887  (best train acc: 0.7419, best val acc: 0.8951, best train loss: 0.7830  @ epoch 3935 )\n",
      "[Epoch: 4420] train loss: 0.8023, train acc: 0.7205, val loss: 0.5771, val acc: 0.8863  (best train acc: 0.7419, best val acc: 0.8951, best train loss: 0.7830  @ epoch 3935 )\n",
      "[Epoch: 4440] train loss: 0.8023, train acc: 0.7230, val loss: 0.5945, val acc: 0.8742  (best train acc: 0.7419, best val acc: 0.8951, best train loss: 0.7830  @ epoch 3935 )\n",
      "[Epoch: 4460] train loss: 0.8294, train acc: 0.6888, val loss: 0.6172, val acc: 0.8442  (best train acc: 0.7419, best val acc: 0.8951, best train loss: 0.7830  @ epoch 3935 )\n",
      "[Epoch: 4480] train loss: 0.8307, train acc: 0.7032, val loss: 0.6082, val acc: 0.8465  (best train acc: 0.7419, best val acc: 0.8951, best train loss: 0.7830  @ epoch 3935 )\n",
      "[Epoch: 4500] train loss: 0.8124, train acc: 0.7186, val loss: 0.5868, val acc: 0.8678  (best train acc: 0.7419, best val acc: 0.8951, best train loss: 0.7830  @ epoch 3935 )\n",
      "[Epoch: 4520] train loss: 0.7968, train acc: 0.7239, val loss: 0.5839, val acc: 0.8843  (best train acc: 0.7419, best val acc: 0.8951, best train loss: 0.7830  @ epoch 3935 )\n",
      "[Epoch: 4540] train loss: 0.8144, train acc: 0.7141, val loss: 0.6007, val acc: 0.8695  (best train acc: 0.7419, best val acc: 0.8951, best train loss: 0.7830  @ epoch 3935 )\n",
      "[Epoch: 4560] train loss: 0.8191, train acc: 0.7172, val loss: 0.5864, val acc: 0.8853  (best train acc: 0.7419, best val acc: 0.8951, best train loss: 0.7830  @ epoch 3935 )\n",
      "[Epoch: 4580] train loss: 0.7987, train acc: 0.7199, val loss: 0.5830, val acc: 0.8921  (best train acc: 0.7419, best val acc: 0.8951, best train loss: 0.7830  @ epoch 3935 )\n",
      "[Epoch: 4600] train loss: 0.8128, train acc: 0.7214, val loss: 0.5805, val acc: 0.8877  (best train acc: 0.7419, best val acc: 0.8951, best train loss: 0.7830  @ epoch 3935 )\n",
      "[Epoch: 4620] train loss: 0.8060, train acc: 0.7204, val loss: 0.5785, val acc: 0.8843  (best train acc: 0.7419, best val acc: 0.8951, best train loss: 0.7830  @ epoch 3935 )\n",
      "[Epoch: 4640] train loss: 0.8065, train acc: 0.7278, val loss: 0.5744, val acc: 0.8867  (best train acc: 0.7419, best val acc: 0.8951, best train loss: 0.7830  @ epoch 3935 )\n",
      "[Epoch: 4660] train loss: 0.8050, train acc: 0.7245, val loss: 0.5732, val acc: 0.8860  (best train acc: 0.7419, best val acc: 0.8951, best train loss: 0.7830  @ epoch 3935 )\n",
      "[Epoch: 4680] train loss: 0.8091, train acc: 0.7199, val loss: 0.5756, val acc: 0.8927  (best train acc: 0.7419, best val acc: 0.8951, best train loss: 0.7830  @ epoch 3935 )\n",
      "[Epoch: 4700] train loss: 0.8063, train acc: 0.7228, val loss: 0.5740, val acc: 0.8917  (best train acc: 0.7419, best val acc: 0.8951, best train loss: 0.7830  @ epoch 3935 )\n",
      "[Epoch: 4720] train loss: 0.8061, train acc: 0.7208, val loss: 0.5949, val acc: 0.8651  (best train acc: 0.7419, best val acc: 0.8951, best train loss: 0.7830  @ epoch 3935 )\n",
      "[Epoch: 4740] train loss: 0.8081, train acc: 0.7265, val loss: 0.5854, val acc: 0.8820  (best train acc: 0.7419, best val acc: 0.8951, best train loss: 0.7830  @ epoch 3935 )\n",
      "[Epoch: 4760] train loss: 0.8091, train acc: 0.7235, val loss: 0.5746, val acc: 0.8941  (best train acc: 0.7419, best val acc: 0.8951, best train loss: 0.7830  @ epoch 3935 )\n",
      "[Epoch: 4780] train loss: 0.8033, train acc: 0.7191, val loss: 0.5727, val acc: 0.8826  (best train acc: 0.7419, best val acc: 0.8951, best train loss: 0.7830  @ epoch 3935 )\n",
      "[Epoch: 4800] train loss: 0.8027, train acc: 0.7230, val loss: 0.5846, val acc: 0.8850  (best train acc: 0.7419, best val acc: 0.8951, best train loss: 0.7830  @ epoch 3935 )\n",
      "[Epoch: 4820] train loss: 0.8194, train acc: 0.7175, val loss: 0.5749, val acc: 0.8843  (best train acc: 0.7419, best val acc: 0.8951, best train loss: 0.7807  @ epoch 4802 )\n",
      "[Epoch: 4840] train loss: 0.8053, train acc: 0.7195, val loss: 0.5859, val acc: 0.8816  (best train acc: 0.7419, best val acc: 0.8951, best train loss: 0.7807  @ epoch 4802 )\n",
      "[Epoch: 4860] train loss: 0.8128, train acc: 0.7206, val loss: 0.6032, val acc: 0.8570  (best train acc: 0.7419, best val acc: 0.8951, best train loss: 0.7807  @ epoch 4802 )\n",
      "[Epoch: 4880] train loss: 0.8058, train acc: 0.7249, val loss: 0.5759, val acc: 0.8840  (best train acc: 0.7419, best val acc: 0.8951, best train loss: 0.7807  @ epoch 4802 )\n",
      "[Epoch: 4900] train loss: 0.8081, train acc: 0.7175, val loss: 0.5876, val acc: 0.8772  (best train acc: 0.7419, best val acc: 0.8951, best train loss: 0.7807  @ epoch 4802 )\n",
      "[Epoch: 4920] train loss: 0.7970, train acc: 0.7240, val loss: 0.5869, val acc: 0.8860  (best train acc: 0.7419, best val acc: 0.8951, best train loss: 0.7807  @ epoch 4802 )\n",
      "[Epoch: 4940] train loss: 0.7892, train acc: 0.7218, val loss: 0.5742, val acc: 0.8867  (best train acc: 0.7419, best val acc: 0.8951, best train loss: 0.7807  @ epoch 4802 )\n",
      "[Epoch: 4960] train loss: 0.8032, train acc: 0.7139, val loss: 0.5734, val acc: 0.8877  (best train acc: 0.7419, best val acc: 0.8954, best train loss: 0.7807  @ epoch 4802 )\n",
      "[Epoch: 4980] train loss: 0.8098, train acc: 0.7029, val loss: 0.6146, val acc: 0.8384  (best train acc: 0.7419, best val acc: 0.8954, best train loss: 0.7807  @ epoch 4802 )\n",
      "[Epoch: 5000] train loss: 0.8002, train acc: 0.7166, val loss: 0.5808, val acc: 0.8907  (best train acc: 0.7419, best val acc: 0.8954, best train loss: 0.7807  @ epoch 4802 )\n",
      "[Epoch: 5020] train loss: 0.7954, train acc: 0.7217, val loss: 0.5816, val acc: 0.8786  (best train acc: 0.7419, best val acc: 0.8954, best train loss: 0.7807  @ epoch 4802 )\n",
      "[Epoch: 5040] train loss: 0.7944, train acc: 0.7311, val loss: 0.5734, val acc: 0.8921  (best train acc: 0.7419, best val acc: 0.8954, best train loss: 0.7807  @ epoch 4802 )\n",
      "[Epoch: 5060] train loss: 0.8065, train acc: 0.7224, val loss: 0.5848, val acc: 0.8850  (best train acc: 0.7419, best val acc: 0.8954, best train loss: 0.7807  @ epoch 4802 )\n",
      "[Epoch: 5080] train loss: 0.7980, train acc: 0.7191, val loss: 0.5652, val acc: 0.8793  (best train acc: 0.7419, best val acc: 0.8954, best train loss: 0.7807  @ epoch 4802 )\n",
      "[Epoch: 5100] train loss: 0.7993, train acc: 0.7171, val loss: 0.5857, val acc: 0.8752  (best train acc: 0.7419, best val acc: 0.8954, best train loss: 0.7807  @ epoch 4802 )\n",
      "[Epoch: 5120] train loss: 0.7981, train acc: 0.7275, val loss: 0.5656, val acc: 0.8860  (best train acc: 0.7419, best val acc: 0.8954, best train loss: 0.7807  @ epoch 4802 )\n",
      "[Epoch: 5140] train loss: 0.7882, train acc: 0.7236, val loss: 0.5681, val acc: 0.8840  (best train acc: 0.7419, best val acc: 0.8954, best train loss: 0.7807  @ epoch 4802 )\n",
      "[Epoch: 5160] train loss: 0.7995, train acc: 0.7262, val loss: 0.5750, val acc: 0.8874  (best train acc: 0.7419, best val acc: 0.8954, best train loss: 0.7807  @ epoch 4802 )\n",
      "[Epoch: 5180] train loss: 0.8039, train acc: 0.7280, val loss: 0.5768, val acc: 0.8867  (best train acc: 0.7419, best val acc: 0.8954, best train loss: 0.7807  @ epoch 4802 )\n",
      "[Epoch: 5200] train loss: 0.7928, train acc: 0.7279, val loss: 0.5815, val acc: 0.8850  (best train acc: 0.7419, best val acc: 0.8954, best train loss: 0.7807  @ epoch 4802 )\n",
      "[Epoch: 5220] train loss: 0.8167, train acc: 0.7172, val loss: 0.5920, val acc: 0.8631  (best train acc: 0.7419, best val acc: 0.8954, best train loss: 0.7807  @ epoch 4802 )\n",
      "[Epoch: 5240] train loss: 0.8048, train acc: 0.7237, val loss: 0.5665, val acc: 0.8863  (best train acc: 0.7419, best val acc: 0.8954, best train loss: 0.7807  @ epoch 4802 )\n",
      "[Epoch: 5260] train loss: 0.8031, train acc: 0.7218, val loss: 0.5674, val acc: 0.8880  (best train acc: 0.7419, best val acc: 0.8958, best train loss: 0.7807  @ epoch 4802 )\n",
      "[Epoch: 5280] train loss: 0.8020, train acc: 0.7221, val loss: 0.5708, val acc: 0.8823  (best train acc: 0.7419, best val acc: 0.8958, best train loss: 0.7807  @ epoch 4802 )\n",
      "[Epoch: 5300] train loss: 0.8128, train acc: 0.7223, val loss: 0.5826, val acc: 0.8715  (best train acc: 0.7419, best val acc: 0.8958, best train loss: 0.7807  @ epoch 4802 )\n",
      "[Epoch: 5320] train loss: 0.8536, train acc: 0.6643, val loss: 0.6185, val acc: 0.8037  (best train acc: 0.7419, best val acc: 0.8958, best train loss: 0.7807  @ epoch 4802 )\n",
      "[Epoch: 5340] train loss: 0.8046, train acc: 0.7217, val loss: 0.5757, val acc: 0.8870  (best train acc: 0.7419, best val acc: 0.8958, best train loss: 0.7807  @ epoch 4802 )\n",
      "[Epoch: 5360] train loss: 0.7945, train acc: 0.7266, val loss: 0.5755, val acc: 0.8917  (best train acc: 0.7419, best val acc: 0.8958, best train loss: 0.7807  @ epoch 4802 )\n",
      "[Epoch: 5380] train loss: 0.7884, train acc: 0.7178, val loss: 0.5675, val acc: 0.8847  (best train acc: 0.7419, best val acc: 0.8958, best train loss: 0.7807  @ epoch 4802 )\n",
      "[Epoch: 5400] train loss: 0.8154, train acc: 0.7170, val loss: 0.6058, val acc: 0.8432  (best train acc: 0.7419, best val acc: 0.8961, best train loss: 0.7807  @ epoch 4802 )\n",
      "[Epoch: 5420] train loss: 0.8089, train acc: 0.7151, val loss: 0.5871, val acc: 0.8573  (best train acc: 0.7419, best val acc: 0.8961, best train loss: 0.7807  @ epoch 4802 )\n",
      "[Epoch: 5440] train loss: 0.7835, train acc: 0.7285, val loss: 0.5792, val acc: 0.8924  (best train acc: 0.7419, best val acc: 0.8961, best train loss: 0.7807  @ epoch 4802 )\n",
      "[Epoch: 5460] train loss: 0.7981, train acc: 0.7249, val loss: 0.5691, val acc: 0.8877  (best train acc: 0.7419, best val acc: 0.8961, best train loss: 0.7807  @ epoch 4802 )\n",
      "[Epoch: 5480] train loss: 0.7900, train acc: 0.7243, val loss: 0.5721, val acc: 0.8847  (best train acc: 0.7419, best val acc: 0.8961, best train loss: 0.7807  @ epoch 4802 )\n",
      "[Epoch: 5500] train loss: 0.8103, train acc: 0.7118, val loss: 0.5708, val acc: 0.8840  (best train acc: 0.7419, best val acc: 0.8961, best train loss: 0.7807  @ epoch 4802 )\n",
      "[Epoch: 5520] train loss: 0.8066, train acc: 0.7274, val loss: 0.5753, val acc: 0.8880  (best train acc: 0.7419, best val acc: 0.8961, best train loss: 0.7807  @ epoch 4802 )\n",
      "[Epoch: 5540] train loss: 0.8036, train acc: 0.7223, val loss: 0.5763, val acc: 0.8857  (best train acc: 0.7419, best val acc: 0.8961, best train loss: 0.7807  @ epoch 4802 )\n",
      "[Epoch: 5560] train loss: 0.8258, train acc: 0.7086, val loss: 0.5699, val acc: 0.8958  (best train acc: 0.7419, best val acc: 0.8961, best train loss: 0.7807  @ epoch 4802 )\n",
      "[Epoch: 5580] train loss: 0.8535, train acc: 0.6252, val loss: 0.6427, val acc: 0.7784  (best train acc: 0.7419, best val acc: 0.8961, best train loss: 0.7807  @ epoch 4802 )\n",
      "[Epoch: 5600] train loss: 0.8106, train acc: 0.7131, val loss: 0.5706, val acc: 0.8735  (best train acc: 0.7419, best val acc: 0.8961, best train loss: 0.7807  @ epoch 4802 )\n",
      "[Epoch: 5620] train loss: 0.8142, train acc: 0.7139, val loss: 0.5819, val acc: 0.8884  (best train acc: 0.7419, best val acc: 0.8961, best train loss: 0.7807  @ epoch 4802 )\n",
      "[Epoch: 5640] train loss: 0.8071, train acc: 0.7044, val loss: 0.6087, val acc: 0.8381  (best train acc: 0.7419, best val acc: 0.8961, best train loss: 0.7807  @ epoch 4802 )\n",
      "[Epoch: 5660] train loss: 0.8246, train acc: 0.6959, val loss: 0.5899, val acc: 0.8648  (best train acc: 0.7419, best val acc: 0.8961, best train loss: 0.7807  @ epoch 4802 )\n",
      "[Epoch: 5680] train loss: 0.8047, train acc: 0.7225, val loss: 0.5845, val acc: 0.8836  (best train acc: 0.7419, best val acc: 0.8968, best train loss: 0.7807  @ epoch 4802 )\n",
      "[Epoch: 5700] train loss: 0.8155, train acc: 0.7120, val loss: 0.5890, val acc: 0.8867  (best train acc: 0.7419, best val acc: 0.8968, best train loss: 0.7807  @ epoch 4802 )\n",
      "[Epoch: 5720] train loss: 0.8199, train acc: 0.7162, val loss: 0.5768, val acc: 0.8722  (best train acc: 0.7419, best val acc: 0.8968, best train loss: 0.7807  @ epoch 4802 )\n",
      "[Epoch: 5740] train loss: 0.8017, train acc: 0.7193, val loss: 0.5847, val acc: 0.8853  (best train acc: 0.7419, best val acc: 0.8968, best train loss: 0.7807  @ epoch 4802 )\n",
      "[Epoch: 5760] train loss: 0.7955, train acc: 0.7253, val loss: 0.5712, val acc: 0.8887  (best train acc: 0.7419, best val acc: 0.8968, best train loss: 0.7807  @ epoch 4802 )\n",
      "[Epoch: 5780] train loss: 0.7937, train acc: 0.7264, val loss: 0.5631, val acc: 0.8874  (best train acc: 0.7419, best val acc: 0.8968, best train loss: 0.7807  @ epoch 4802 )\n",
      "[Epoch: 5800] train loss: 0.8086, train acc: 0.7175, val loss: 0.5655, val acc: 0.8806  (best train acc: 0.7419, best val acc: 0.8968, best train loss: 0.7807  @ epoch 4802 )\n",
      "[Epoch: 5820] train loss: 0.8091, train acc: 0.7218, val loss: 0.5646, val acc: 0.8847  (best train acc: 0.7419, best val acc: 0.8968, best train loss: 0.7807  @ epoch 4802 )\n",
      "[Epoch: 5840] train loss: 0.8022, train acc: 0.7177, val loss: 0.5706, val acc: 0.8745  (best train acc: 0.7419, best val acc: 0.8968, best train loss: 0.7807  @ epoch 4802 )\n",
      "[Epoch: 5860] train loss: 0.8175, train acc: 0.7181, val loss: 0.5749, val acc: 0.8880  (best train acc: 0.7419, best val acc: 0.8968, best train loss: 0.7807  @ epoch 4802 )\n",
      "[Epoch: 5880] train loss: 0.8036, train acc: 0.7200, val loss: 0.5733, val acc: 0.8894  (best train acc: 0.7419, best val acc: 0.8968, best train loss: 0.7807  @ epoch 4802 )\n",
      "[Epoch: 5900] train loss: 0.8110, train acc: 0.7263, val loss: 0.5656, val acc: 0.8914  (best train acc: 0.7419, best val acc: 0.8968, best train loss: 0.7807  @ epoch 4802 )\n",
      "[Epoch: 5920] train loss: 0.7983, train acc: 0.7178, val loss: 0.5916, val acc: 0.8556  (best train acc: 0.7419, best val acc: 0.8968, best train loss: 0.7807  @ epoch 4802 )\n",
      "[Epoch: 5940] train loss: 0.8026, train acc: 0.7225, val loss: 0.5590, val acc: 0.8860  (best train acc: 0.7419, best val acc: 0.8968, best train loss: 0.7807  @ epoch 4802 )\n",
      "[Epoch: 5960] train loss: 0.8394, train acc: 0.6947, val loss: 0.6100, val acc: 0.8590  (best train acc: 0.7419, best val acc: 0.8968, best train loss: 0.7807  @ epoch 4802 )\n",
      "[Epoch: 5980] train loss: 0.8193, train acc: 0.6861, val loss: 0.6077, val acc: 0.8438  (best train acc: 0.7419, best val acc: 0.8968, best train loss: 0.7807  @ epoch 4802 )\n",
      "[Epoch: 6000] train loss: 0.8009, train acc: 0.7195, val loss: 0.5800, val acc: 0.8890  (best train acc: 0.7419, best val acc: 0.8968, best train loss: 0.7807  @ epoch 4802 )\n",
      "[Epoch: 6020] train loss: 0.8041, train acc: 0.7240, val loss: 0.5707, val acc: 0.8850  (best train acc: 0.7419, best val acc: 0.8975, best train loss: 0.7807  @ epoch 4802 )\n",
      "[Epoch: 6040] train loss: 0.8035, train acc: 0.7196, val loss: 0.5696, val acc: 0.8850  (best train acc: 0.7419, best val acc: 0.8975, best train loss: 0.7807  @ epoch 4802 )\n",
      "[Epoch: 6060] train loss: 0.7980, train acc: 0.7241, val loss: 0.5602, val acc: 0.8833  (best train acc: 0.7419, best val acc: 0.8975, best train loss: 0.7807  @ epoch 4802 )\n",
      "[Epoch: 6080] train loss: 0.8198, train acc: 0.7238, val loss: 0.5697, val acc: 0.8971  (best train acc: 0.7419, best val acc: 0.8978, best train loss: 0.7807  @ epoch 4802 )\n",
      "[Epoch: 6100] train loss: 0.7938, train acc: 0.7231, val loss: 0.5699, val acc: 0.8951  (best train acc: 0.7419, best val acc: 0.8978, best train loss: 0.7807  @ epoch 4802 )\n",
      "[Epoch: 6120] train loss: 0.8100, train acc: 0.7239, val loss: 0.5633, val acc: 0.8948  (best train acc: 0.7419, best val acc: 0.8978, best train loss: 0.7807  @ epoch 4802 )\n",
      "[Epoch: 6140] train loss: 0.7956, train acc: 0.7211, val loss: 0.5672, val acc: 0.8927  (best train acc: 0.7419, best val acc: 0.8978, best train loss: 0.7807  @ epoch 4802 )\n",
      "[Epoch: 6160] train loss: 0.8039, train acc: 0.7250, val loss: 0.5557, val acc: 0.8853  (best train acc: 0.7419, best val acc: 0.8978, best train loss: 0.7807  @ epoch 4802 )\n",
      "[Epoch: 6180] train loss: 0.8068, train acc: 0.7160, val loss: 0.5595, val acc: 0.8793  (best train acc: 0.7419, best val acc: 0.8978, best train loss: 0.7807  @ epoch 4802 )\n",
      "[Epoch: 6200] train loss: 0.8121, train acc: 0.7139, val loss: 0.5846, val acc: 0.8590  (best train acc: 0.7419, best val acc: 0.8978, best train loss: 0.7807  @ epoch 4802 )\n",
      "[Epoch: 6220] train loss: 0.8201, train acc: 0.7000, val loss: 0.5752, val acc: 0.8428  (best train acc: 0.7419, best val acc: 0.8978, best train loss: 0.7807  @ epoch 4802 )\n",
      "[Epoch: 6240] train loss: 0.8041, train acc: 0.7203, val loss: 0.5840, val acc: 0.8658  (best train acc: 0.7419, best val acc: 0.8988, best train loss: 0.7807  @ epoch 4802 )\n",
      "[Epoch: 6260] train loss: 0.8112, train acc: 0.7199, val loss: 0.5688, val acc: 0.8927  (best train acc: 0.7419, best val acc: 0.8988, best train loss: 0.7807  @ epoch 4802 )\n",
      "[Epoch: 6280] train loss: 0.7949, train acc: 0.7204, val loss: 0.5654, val acc: 0.8921  (best train acc: 0.7419, best val acc: 0.8988, best train loss: 0.7737  @ epoch 6273 )\n",
      "[Epoch: 6300] train loss: 0.8186, train acc: 0.7147, val loss: 0.5772, val acc: 0.8789  (best train acc: 0.7419, best val acc: 0.8988, best train loss: 0.7737  @ epoch 6273 )\n",
      "[Epoch: 6320] train loss: 0.7930, train acc: 0.7237, val loss: 0.5721, val acc: 0.8867  (best train acc: 0.7419, best val acc: 0.8988, best train loss: 0.7737  @ epoch 6273 )\n",
      "[Epoch: 6340] train loss: 0.7940, train acc: 0.7197, val loss: 0.5591, val acc: 0.8833  (best train acc: 0.7419, best val acc: 0.8988, best train loss: 0.7737  @ epoch 6273 )\n",
      "[Epoch: 6360] train loss: 0.8033, train acc: 0.7192, val loss: 0.5578, val acc: 0.8712  (best train acc: 0.7419, best val acc: 0.8988, best train loss: 0.7737  @ epoch 6273 )\n",
      "[Epoch: 6380] train loss: 0.8150, train acc: 0.7105, val loss: 0.5705, val acc: 0.8823  (best train acc: 0.7419, best val acc: 0.8988, best train loss: 0.7737  @ epoch 6273 )\n",
      "[Epoch: 6400] train loss: 0.8202, train acc: 0.7183, val loss: 0.5671, val acc: 0.8897  (best train acc: 0.7419, best val acc: 0.8988, best train loss: 0.7737  @ epoch 6273 )\n",
      "[Epoch: 6420] train loss: 0.8129, train acc: 0.7186, val loss: 0.5797, val acc: 0.8725  (best train acc: 0.7419, best val acc: 0.8988, best train loss: 0.7737  @ epoch 6273 )\n",
      "[Epoch: 6440] train loss: 0.8059, train acc: 0.7197, val loss: 0.5707, val acc: 0.8877  (best train acc: 0.7419, best val acc: 0.8988, best train loss: 0.7737  @ epoch 6273 )\n",
      "[Epoch: 6460] train loss: 0.8077, train acc: 0.7178, val loss: 0.5696, val acc: 0.8870  (best train acc: 0.7419, best val acc: 0.8992, best train loss: 0.7737  @ epoch 6273 )\n",
      "[Epoch: 6480] train loss: 0.7974, train acc: 0.7217, val loss: 0.5590, val acc: 0.8968  (best train acc: 0.7419, best val acc: 0.8992, best train loss: 0.7737  @ epoch 6273 )\n",
      "[Epoch: 6500] train loss: 0.7883, train acc: 0.7274, val loss: 0.5661, val acc: 0.8914  (best train acc: 0.7419, best val acc: 0.8992, best train loss: 0.7737  @ epoch 6273 )\n",
      "[Epoch: 6520] train loss: 0.8088, train acc: 0.7159, val loss: 0.5581, val acc: 0.8793  (best train acc: 0.7419, best val acc: 0.8992, best train loss: 0.7737  @ epoch 6273 )\n",
      "[Epoch: 6540] train loss: 0.8026, train acc: 0.7255, val loss: 0.5582, val acc: 0.8944  (best train acc: 0.7419, best val acc: 0.8992, best train loss: 0.7737  @ epoch 6273 )\n",
      "[Epoch: 6560] train loss: 0.7890, train acc: 0.7243, val loss: 0.5726, val acc: 0.8820  (best train acc: 0.7419, best val acc: 0.8992, best train loss: 0.7737  @ epoch 6273 )\n",
      "[Epoch: 6580] train loss: 0.7939, train acc: 0.7241, val loss: 0.5607, val acc: 0.8951  (best train acc: 0.7419, best val acc: 0.8992, best train loss: 0.7737  @ epoch 6273 )\n",
      "[Epoch: 6600] train loss: 0.7989, train acc: 0.7222, val loss: 0.5619, val acc: 0.8941  (best train acc: 0.7419, best val acc: 0.8992, best train loss: 0.7737  @ epoch 6273 )\n",
      "[Epoch: 6620] train loss: 0.8126, train acc: 0.7186, val loss: 0.5657, val acc: 0.8907  (best train acc: 0.7419, best val acc: 0.8992, best train loss: 0.7737  @ epoch 6273 )\n",
      "[Epoch: 6640] train loss: 0.8116, train acc: 0.7108, val loss: 0.5611, val acc: 0.8934  (best train acc: 0.7419, best val acc: 0.8992, best train loss: 0.7737  @ epoch 6273 )\n",
      "[Epoch: 6660] train loss: 0.7941, train acc: 0.7140, val loss: 0.5748, val acc: 0.8540  (best train acc: 0.7419, best val acc: 0.8992, best train loss: 0.7737  @ epoch 6273 )\n",
      "[Epoch: 6680] train loss: 0.8006, train acc: 0.7233, val loss: 0.5632, val acc: 0.8961  (best train acc: 0.7419, best val acc: 0.8998, best train loss: 0.7737  @ epoch 6273 )\n",
      "[Epoch: 6700] train loss: 0.7924, train acc: 0.7241, val loss: 0.5615, val acc: 0.8897  (best train acc: 0.7419, best val acc: 0.8998, best train loss: 0.7737  @ epoch 6273 )\n",
      "[Epoch: 6720] train loss: 0.8053, train acc: 0.7185, val loss: 0.5550, val acc: 0.8877  (best train acc: 0.7419, best val acc: 0.8998, best train loss: 0.7737  @ epoch 6273 )\n",
      "[Epoch: 6740] train loss: 0.8021, train acc: 0.7175, val loss: 0.5589, val acc: 0.8843  (best train acc: 0.7419, best val acc: 0.8998, best train loss: 0.7737  @ epoch 6273 )\n",
      "[Epoch: 6760] train loss: 0.8077, train acc: 0.7117, val loss: 0.5798, val acc: 0.8739  (best train acc: 0.7419, best val acc: 0.8998, best train loss: 0.7737  @ epoch 6273 )\n",
      "[Epoch: 6780] train loss: 0.7960, train acc: 0.7224, val loss: 0.5561, val acc: 0.8833  (best train acc: 0.7419, best val acc: 0.8998, best train loss: 0.7737  @ epoch 6273 )\n",
      "[Epoch: 6800] train loss: 0.8186, train acc: 0.7064, val loss: 0.5760, val acc: 0.8422  (best train acc: 0.7419, best val acc: 0.8998, best train loss: 0.7737  @ epoch 6273 )\n",
      "[Epoch: 6820] train loss: 0.8120, train acc: 0.7191, val loss: 0.5590, val acc: 0.8836  (best train acc: 0.7419, best val acc: 0.8998, best train loss: 0.7737  @ epoch 6273 )\n",
      "[Epoch: 6840] train loss: 0.8037, train acc: 0.7127, val loss: 0.5751, val acc: 0.8813  (best train acc: 0.7419, best val acc: 0.8998, best train loss: 0.7737  @ epoch 6273 )\n",
      "[Epoch: 6860] train loss: 0.8123, train acc: 0.6940, val loss: 0.5799, val acc: 0.8688  (best train acc: 0.7419, best val acc: 0.8998, best train loss: 0.7737  @ epoch 6273 )\n",
      "[Epoch: 6880] train loss: 0.8138, train acc: 0.6922, val loss: 0.5833, val acc: 0.8567  (best train acc: 0.7419, best val acc: 0.8998, best train loss: 0.7737  @ epoch 6273 )\n",
      "[Epoch: 6900] train loss: 0.8036, train acc: 0.7266, val loss: 0.5620, val acc: 0.8981  (best train acc: 0.7419, best val acc: 0.8998, best train loss: 0.7737  @ epoch 6273 )\n",
      "[Epoch: 6920] train loss: 0.8048, train acc: 0.7188, val loss: 0.5734, val acc: 0.8836  (best train acc: 0.7419, best val acc: 0.8998, best train loss: 0.7737  @ epoch 6273 )\n",
      "[Epoch: 6940] train loss: 0.8109, train acc: 0.7147, val loss: 0.5588, val acc: 0.8836  (best train acc: 0.7419, best val acc: 0.8998, best train loss: 0.7737  @ epoch 6273 )\n",
      "[Epoch: 6960] train loss: 0.7924, train acc: 0.7229, val loss: 0.5659, val acc: 0.8968  (best train acc: 0.7419, best val acc: 0.8998, best train loss: 0.7737  @ epoch 6273 )\n",
      "[Epoch: 6980] train loss: 0.8074, train acc: 0.7221, val loss: 0.5580, val acc: 0.8843  (best train acc: 0.7419, best val acc: 0.8998, best train loss: 0.7737  @ epoch 6273 )\n",
      "[Epoch: 7000] train loss: 0.7891, train acc: 0.7314, val loss: 0.5531, val acc: 0.8847  (best train acc: 0.7419, best val acc: 0.8998, best train loss: 0.7737  @ epoch 6273 )\n",
      "[Epoch: 7020] train loss: 0.8100, train acc: 0.7237, val loss: 0.5588, val acc: 0.8965  (best train acc: 0.7419, best val acc: 0.9005, best train loss: 0.7737  @ epoch 6273 )\n",
      "[Epoch: 7040] train loss: 0.8063, train acc: 0.7213, val loss: 0.5781, val acc: 0.8695  (best train acc: 0.7419, best val acc: 0.9005, best train loss: 0.7737  @ epoch 6273 )\n",
      "[Epoch: 7060] train loss: 0.8127, train acc: 0.7144, val loss: 0.5585, val acc: 0.8894  (best train acc: 0.7419, best val acc: 0.9005, best train loss: 0.7737  @ epoch 6273 )\n",
      "[Epoch: 7080] train loss: 0.8082, train acc: 0.7046, val loss: 0.5873, val acc: 0.8506  (best train acc: 0.7419, best val acc: 0.9005, best train loss: 0.7737  @ epoch 6273 )\n",
      "[Epoch: 7100] train loss: 0.8010, train acc: 0.7176, val loss: 0.5727, val acc: 0.8911  (best train acc: 0.7419, best val acc: 0.9005, best train loss: 0.7737  @ epoch 6273 )\n",
      "[Epoch: 7120] train loss: 0.8212, train acc: 0.7122, val loss: 0.5857, val acc: 0.8577  (best train acc: 0.7419, best val acc: 0.9005, best train loss: 0.7737  @ epoch 6273 )\n",
      "[Epoch: 7140] train loss: 0.7963, train acc: 0.7091, val loss: 0.6080, val acc: 0.8293  (best train acc: 0.7419, best val acc: 0.9005, best train loss: 0.7737  @ epoch 6273 )\n",
      "[Epoch: 7160] train loss: 0.8154, train acc: 0.7081, val loss: 0.5740, val acc: 0.8772  (best train acc: 0.7419, best val acc: 0.9005, best train loss: 0.7737  @ epoch 6273 )\n",
      "[Epoch: 7180] train loss: 0.8005, train acc: 0.7256, val loss: 0.5590, val acc: 0.8870  (best train acc: 0.7419, best val acc: 0.9005, best train loss: 0.7737  @ epoch 6273 )\n",
      "[Epoch: 7200] train loss: 0.7926, train acc: 0.7193, val loss: 0.5738, val acc: 0.8826  (best train acc: 0.7419, best val acc: 0.9005, best train loss: 0.7737  @ epoch 6273 )\n",
      "[Epoch: 7220] train loss: 0.7978, train acc: 0.7191, val loss: 0.5802, val acc: 0.8675  (best train acc: 0.7419, best val acc: 0.9005, best train loss: 0.7737  @ epoch 6273 )\n",
      "[Epoch: 7240] train loss: 0.7909, train acc: 0.7223, val loss: 0.5565, val acc: 0.8847  (best train acc: 0.7419, best val acc: 0.9005, best train loss: 0.7737  @ epoch 6273 )\n",
      "[Epoch: 7260] train loss: 0.8100, train acc: 0.7188, val loss: 0.5603, val acc: 0.8874  (best train acc: 0.7419, best val acc: 0.9005, best train loss: 0.7737  @ epoch 6273 )\n",
      "[Epoch: 7280] train loss: 0.7904, train acc: 0.7266, val loss: 0.5553, val acc: 0.8921  (best train acc: 0.7419, best val acc: 0.9005, best train loss: 0.7737  @ epoch 6273 )\n",
      "[Epoch: 7300] train loss: 0.7936, train acc: 0.7223, val loss: 0.5546, val acc: 0.8975  (best train acc: 0.7419, best val acc: 0.9005, best train loss: 0.7737  @ epoch 6273 )\n",
      "[Epoch: 7320] train loss: 0.7918, train acc: 0.7246, val loss: 0.5607, val acc: 0.8938  (best train acc: 0.7419, best val acc: 0.9005, best train loss: 0.7737  @ epoch 6273 )\n",
      "[Epoch: 7340] train loss: 0.8017, train acc: 0.7248, val loss: 0.5650, val acc: 0.8803  (best train acc: 0.7419, best val acc: 0.9005, best train loss: 0.7737  @ epoch 6273 )\n",
      "[Epoch: 7360] train loss: 0.7971, train acc: 0.7256, val loss: 0.5549, val acc: 0.8931  (best train acc: 0.7419, best val acc: 0.9005, best train loss: 0.7737  @ epoch 6273 )\n",
      "[Epoch: 7380] train loss: 0.7908, train acc: 0.7256, val loss: 0.5524, val acc: 0.8887  (best train acc: 0.7419, best val acc: 0.9005, best train loss: 0.7737  @ epoch 6273 )\n",
      "[Epoch: 7400] train loss: 0.7899, train acc: 0.7253, val loss: 0.5574, val acc: 0.8914  (best train acc: 0.7419, best val acc: 0.9015, best train loss: 0.7737  @ epoch 6273 )\n",
      "[Epoch: 7420] train loss: 0.7873, train acc: 0.7306, val loss: 0.5562, val acc: 0.8995  (best train acc: 0.7419, best val acc: 0.9015, best train loss: 0.7737  @ epoch 6273 )\n",
      "[Epoch: 7440] train loss: 0.8025, train acc: 0.7141, val loss: 0.5852, val acc: 0.8465  (best train acc: 0.7419, best val acc: 0.9015, best train loss: 0.7737  @ epoch 6273 )\n",
      "[Epoch: 7460] train loss: 0.8149, train acc: 0.7042, val loss: 0.5764, val acc: 0.8641  (best train acc: 0.7419, best val acc: 0.9015, best train loss: 0.7737  @ epoch 6273 )\n",
      "[Epoch: 7480] train loss: 0.8289, train acc: 0.6530, val loss: 0.6373, val acc: 0.7798  (best train acc: 0.7419, best val acc: 0.9015, best train loss: 0.7737  @ epoch 6273 )\n",
      "[Epoch: 7500] train loss: 0.7830, train acc: 0.7111, val loss: 0.5683, val acc: 0.8958  (best train acc: 0.7419, best val acc: 0.9015, best train loss: 0.7737  @ epoch 6273 )\n",
      "[Epoch: 7520] train loss: 0.8013, train acc: 0.7222, val loss: 0.5606, val acc: 0.8853  (best train acc: 0.7419, best val acc: 0.9015, best train loss: 0.7737  @ epoch 6273 )\n",
      "[Epoch: 7540] train loss: 0.7894, train acc: 0.7289, val loss: 0.5731, val acc: 0.8809  (best train acc: 0.7419, best val acc: 0.9015, best train loss: 0.7737  @ epoch 6273 )\n",
      "[Epoch: 7560] train loss: 0.7900, train acc: 0.7196, val loss: 0.5749, val acc: 0.8725  (best train acc: 0.7419, best val acc: 0.9015, best train loss: 0.7737  @ epoch 6273 )\n",
      "[Epoch: 7580] train loss: 0.7985, train acc: 0.7284, val loss: 0.5538, val acc: 0.8968  (best train acc: 0.7419, best val acc: 0.9015, best train loss: 0.7737  @ epoch 6273 )\n",
      "[Epoch: 7600] train loss: 0.8017, train acc: 0.7203, val loss: 0.5583, val acc: 0.8880  (best train acc: 0.7419, best val acc: 0.9015, best train loss: 0.7737  @ epoch 6273 )\n",
      "[Epoch: 7620] train loss: 0.8045, train acc: 0.7236, val loss: 0.5547, val acc: 0.8954  (best train acc: 0.7419, best val acc: 0.9015, best train loss: 0.7737  @ epoch 6273 )\n",
      "[Epoch: 7640] train loss: 0.7914, train acc: 0.7243, val loss: 0.5578, val acc: 0.8931  (best train acc: 0.7419, best val acc: 0.9015, best train loss: 0.7737  @ epoch 6273 )\n",
      "[Epoch: 7660] train loss: 0.8029, train acc: 0.7201, val loss: 0.5576, val acc: 0.8981  (best train acc: 0.7419, best val acc: 0.9015, best train loss: 0.7737  @ epoch 6273 )\n",
      "[Epoch: 7680] train loss: 0.8073, train acc: 0.7176, val loss: 0.5510, val acc: 0.8924  (best train acc: 0.7419, best val acc: 0.9015, best train loss: 0.7737  @ epoch 6273 )\n",
      "[Epoch: 7700] train loss: 0.7973, train acc: 0.7216, val loss: 0.5540, val acc: 0.8971  (best train acc: 0.7419, best val acc: 0.9015, best train loss: 0.7737  @ epoch 6273 )\n",
      "[Epoch: 7720] train loss: 0.8149, train acc: 0.7214, val loss: 0.5509, val acc: 0.8914  (best train acc: 0.7419, best val acc: 0.9015, best train loss: 0.7737  @ epoch 6273 )\n",
      "[Epoch: 7740] train loss: 0.8113, train acc: 0.7165, val loss: 0.5525, val acc: 0.8961  (best train acc: 0.7419, best val acc: 0.9022, best train loss: 0.7737  @ epoch 6273 )\n",
      "[Epoch: 7760] train loss: 0.7853, train acc: 0.7269, val loss: 0.5565, val acc: 0.8992  (best train acc: 0.7419, best val acc: 0.9029, best train loss: 0.7737  @ epoch 6273 )\n",
      "[Epoch: 7780] train loss: 0.7901, train acc: 0.7259, val loss: 0.5549, val acc: 0.8931  (best train acc: 0.7419, best val acc: 0.9029, best train loss: 0.7737  @ epoch 6273 )\n",
      "[Epoch: 7800] train loss: 0.8423, train acc: 0.6980, val loss: 0.6043, val acc: 0.8223  (best train acc: 0.7419, best val acc: 0.9029, best train loss: 0.7737  @ epoch 6273 )\n",
      "[Epoch: 7820] train loss: 0.8335, train acc: 0.6940, val loss: 0.5740, val acc: 0.8941  (best train acc: 0.7419, best val acc: 0.9029, best train loss: 0.7737  @ epoch 6273 )\n",
      "[Epoch: 7840] train loss: 0.7860, train acc: 0.7222, val loss: 0.5647, val acc: 0.8816  (best train acc: 0.7419, best val acc: 0.9029, best train loss: 0.7737  @ epoch 6273 )\n",
      "[Epoch: 7860] train loss: 0.7951, train acc: 0.7261, val loss: 0.5555, val acc: 0.8975  (best train acc: 0.7419, best val acc: 0.9029, best train loss: 0.7737  @ epoch 6273 )\n",
      "[Epoch: 7880] train loss: 0.7966, train acc: 0.7260, val loss: 0.5607, val acc: 0.8843  (best train acc: 0.7419, best val acc: 0.9029, best train loss: 0.7737  @ epoch 6273 )\n",
      "[Epoch: 7900] train loss: 0.8015, train acc: 0.7163, val loss: 0.5636, val acc: 0.8752  (best train acc: 0.7419, best val acc: 0.9029, best train loss: 0.7737  @ epoch 6273 )\n",
      "[Epoch: 7920] train loss: 0.7889, train acc: 0.7249, val loss: 0.5502, val acc: 0.8988  (best train acc: 0.7419, best val acc: 0.9029, best train loss: 0.7737  @ epoch 6273 )\n",
      "[Epoch: 7940] train loss: 0.7950, train acc: 0.7254, val loss: 0.5455, val acc: 0.8941  (best train acc: 0.7419, best val acc: 0.9029, best train loss: 0.7737  @ epoch 6273 )\n",
      "[Epoch: 7960] train loss: 0.7920, train acc: 0.7239, val loss: 0.5488, val acc: 0.8803  (best train acc: 0.7419, best val acc: 0.9029, best train loss: 0.7737  @ epoch 6273 )\n",
      "[Epoch: 7980] train loss: 0.7981, train acc: 0.7220, val loss: 0.5618, val acc: 0.8836  (best train acc: 0.7419, best val acc: 0.9029, best train loss: 0.7737  @ epoch 6273 )\n",
      "[Epoch: 8000] train loss: 0.8027, train acc: 0.7248, val loss: 0.5517, val acc: 0.8951  (best train acc: 0.7419, best val acc: 0.9032, best train loss: 0.7737  @ epoch 6273 )\n",
      "[Epoch: 8020] train loss: 0.7860, train acc: 0.7253, val loss: 0.5550, val acc: 0.8914  (best train acc: 0.7419, best val acc: 0.9032, best train loss: 0.7737  @ epoch 6273 )\n",
      "[Epoch: 8040] train loss: 0.8000, train acc: 0.7306, val loss: 0.5477, val acc: 0.8978  (best train acc: 0.7419, best val acc: 0.9032, best train loss: 0.7737  @ epoch 6273 )\n",
      "[Epoch: 8060] train loss: 0.7989, train acc: 0.7131, val loss: 0.5683, val acc: 0.8668  (best train acc: 0.7419, best val acc: 0.9032, best train loss: 0.7737  @ epoch 6273 )\n",
      "[Epoch: 8080] train loss: 0.8062, train acc: 0.7087, val loss: 0.5468, val acc: 0.8860  (best train acc: 0.7419, best val acc: 0.9032, best train loss: 0.7737  @ epoch 6273 )\n",
      "[Epoch: 8100] train loss: 0.7963, train acc: 0.7192, val loss: 0.5493, val acc: 0.8769  (best train acc: 0.7419, best val acc: 0.9032, best train loss: 0.7737  @ epoch 6273 )\n",
      "[Epoch: 8120] train loss: 0.8011, train acc: 0.7243, val loss: 0.5568, val acc: 0.8961  (best train acc: 0.7419, best val acc: 0.9032, best train loss: 0.7737  @ epoch 6273 )\n",
      "[Epoch: 8140] train loss: 0.7946, train acc: 0.7194, val loss: 0.5475, val acc: 0.8985  (best train acc: 0.7419, best val acc: 0.9032, best train loss: 0.7737  @ epoch 6273 )\n",
      "[Epoch: 8160] train loss: 0.7889, train acc: 0.7246, val loss: 0.5546, val acc: 0.8954  (best train acc: 0.7419, best val acc: 0.9032, best train loss: 0.7737  @ epoch 6273 )\n",
      "[Epoch: 8180] train loss: 0.7949, train acc: 0.7272, val loss: 0.5436, val acc: 0.8931  (best train acc: 0.7419, best val acc: 0.9032, best train loss: 0.7737  @ epoch 6273 )\n",
      "[Epoch: 8200] train loss: 0.7809, train acc: 0.7311, val loss: 0.5526, val acc: 0.8938  (best train acc: 0.7419, best val acc: 0.9032, best train loss: 0.7737  @ epoch 6273 )\n",
      "[Epoch: 8220] train loss: 0.7940, train acc: 0.7261, val loss: 0.5435, val acc: 0.8911  (best train acc: 0.7419, best val acc: 0.9032, best train loss: 0.7737  @ epoch 6273 )\n",
      "[Epoch: 8240] train loss: 0.8039, train acc: 0.7103, val loss: 0.5602, val acc: 0.8772  (best train acc: 0.7419, best val acc: 0.9032, best train loss: 0.7737  @ epoch 6273 )\n",
      "[Epoch: 8260] train loss: 0.8183, train acc: 0.6943, val loss: 0.6080, val acc: 0.8432  (best train acc: 0.7419, best val acc: 0.9032, best train loss: 0.7737  @ epoch 6273 )\n",
      "[Epoch: 8280] train loss: 0.7880, train acc: 0.7236, val loss: 0.5603, val acc: 0.8776  (best train acc: 0.7419, best val acc: 0.9032, best train loss: 0.7737  @ epoch 6273 )\n",
      "[Epoch: 8300] train loss: 0.8017, train acc: 0.7230, val loss: 0.5583, val acc: 0.8961  (best train acc: 0.7419, best val acc: 0.9032, best train loss: 0.7737  @ epoch 6273 )\n",
      "[Epoch: 8320] train loss: 0.8000, train acc: 0.7165, val loss: 0.5452, val acc: 0.8867  (best train acc: 0.7419, best val acc: 0.9032, best train loss: 0.7737  @ epoch 6273 )\n",
      "[Epoch: 8340] train loss: 0.7990, train acc: 0.7217, val loss: 0.5552, val acc: 0.8823  (best train acc: 0.7419, best val acc: 0.9032, best train loss: 0.7737  @ epoch 6273 )\n",
      "[Epoch: 8360] train loss: 0.7942, train acc: 0.7183, val loss: 0.5485, val acc: 0.8981  (best train acc: 0.7419, best val acc: 0.9032, best train loss: 0.7737  @ epoch 6273 )\n",
      "[Epoch: 8380] train loss: 0.8040, train acc: 0.7265, val loss: 0.5567, val acc: 0.8877  (best train acc: 0.7419, best val acc: 0.9032, best train loss: 0.7737  @ epoch 6273 )\n",
      "[Epoch: 8400] train loss: 0.8045, train acc: 0.7157, val loss: 0.5454, val acc: 0.8985  (best train acc: 0.7419, best val acc: 0.9032, best train loss: 0.7697  @ epoch 8382 )\n",
      "[Epoch: 8420] train loss: 0.8006, train acc: 0.7235, val loss: 0.5487, val acc: 0.8961  (best train acc: 0.7419, best val acc: 0.9032, best train loss: 0.7697  @ epoch 8382 )\n",
      "[Epoch: 8440] train loss: 0.7876, train acc: 0.7210, val loss: 0.5394, val acc: 0.8884  (best train acc: 0.7419, best val acc: 0.9032, best train loss: 0.7697  @ epoch 8382 )\n",
      "[Epoch: 8460] train loss: 0.8083, train acc: 0.7170, val loss: 0.5449, val acc: 0.8978  (best train acc: 0.7419, best val acc: 0.9032, best train loss: 0.7697  @ epoch 8382 )\n",
      "[Epoch: 8480] train loss: 0.8066, train acc: 0.7201, val loss: 0.5472, val acc: 0.8755  (best train acc: 0.7419, best val acc: 0.9032, best train loss: 0.7697  @ epoch 8382 )\n",
      "[Epoch: 8500] train loss: 0.7952, train acc: 0.7256, val loss: 0.5445, val acc: 0.8921  (best train acc: 0.7419, best val acc: 0.9032, best train loss: 0.7697  @ epoch 8382 )\n",
      "[Epoch: 8520] train loss: 0.7935, train acc: 0.7217, val loss: 0.5592, val acc: 0.8678  (best train acc: 0.7419, best val acc: 0.9032, best train loss: 0.7697  @ epoch 8382 )\n",
      "[Epoch: 8540] train loss: 0.7963, train acc: 0.7278, val loss: 0.5462, val acc: 0.8880  (best train acc: 0.7419, best val acc: 0.9032, best train loss: 0.7697  @ epoch 8382 )\n",
      "[Epoch: 8560] train loss: 0.7904, train acc: 0.7146, val loss: 0.5916, val acc: 0.8519  (best train acc: 0.7419, best val acc: 0.9032, best train loss: 0.7697  @ epoch 8382 )\n",
      "[Epoch: 8580] train loss: 0.7988, train acc: 0.7186, val loss: 0.5477, val acc: 0.8843  (best train acc: 0.7419, best val acc: 0.9032, best train loss: 0.7697  @ epoch 8382 )\n",
      "[Epoch: 8600] train loss: 0.8061, train acc: 0.7238, val loss: 0.5512, val acc: 0.8948  (best train acc: 0.7419, best val acc: 0.9032, best train loss: 0.7697  @ epoch 8382 )\n",
      "[Epoch: 8620] train loss: 0.8149, train acc: 0.7128, val loss: 0.5542, val acc: 0.8975  (best train acc: 0.7419, best val acc: 0.9032, best train loss: 0.7697  @ epoch 8382 )\n",
      "[Epoch: 8640] train loss: 0.8071, train acc: 0.7005, val loss: 0.5895, val acc: 0.8368  (best train acc: 0.7419, best val acc: 0.9032, best train loss: 0.7697  @ epoch 8382 )\n",
      "[Epoch: 8660] train loss: 0.7910, train acc: 0.7237, val loss: 0.5499, val acc: 0.8948  (best train acc: 0.7419, best val acc: 0.9032, best train loss: 0.7697  @ epoch 8382 )\n",
      "[Epoch: 8680] train loss: 0.8028, train acc: 0.7222, val loss: 0.5449, val acc: 0.9008  (best train acc: 0.7419, best val acc: 0.9032, best train loss: 0.7697  @ epoch 8382 )\n",
      "[Epoch: 8700] train loss: 0.7912, train acc: 0.7200, val loss: 0.5745, val acc: 0.8556  (best train acc: 0.7419, best val acc: 0.9032, best train loss: 0.7697  @ epoch 8382 )\n",
      "[Epoch: 8720] train loss: 0.7913, train acc: 0.7285, val loss: 0.5518, val acc: 0.8884  (best train acc: 0.7419, best val acc: 0.9032, best train loss: 0.7697  @ epoch 8382 )\n",
      "[Epoch: 8740] train loss: 0.7927, train acc: 0.7199, val loss: 0.5445, val acc: 0.8998  (best train acc: 0.7419, best val acc: 0.9039, best train loss: 0.7697  @ epoch 8382 )\n",
      "[Epoch: 8760] train loss: 0.8005, train acc: 0.7184, val loss: 0.5516, val acc: 0.8853  (best train acc: 0.7419, best val acc: 0.9039, best train loss: 0.7697  @ epoch 8382 )\n",
      "[Epoch: 8780] train loss: 0.7904, train acc: 0.7289, val loss: 0.5425, val acc: 0.8988  (best train acc: 0.7419, best val acc: 0.9039, best train loss: 0.7697  @ epoch 8382 )\n",
      "[Epoch: 8800] train loss: 0.7909, train acc: 0.7237, val loss: 0.5388, val acc: 0.8880  (best train acc: 0.7419, best val acc: 0.9039, best train loss: 0.7697  @ epoch 8382 )\n",
      "[Epoch: 8820] train loss: 0.8030, train acc: 0.7262, val loss: 0.5469, val acc: 0.8877  (best train acc: 0.7419, best val acc: 0.9039, best train loss: 0.7697  @ epoch 8382 )\n",
      "[Epoch: 8840] train loss: 0.7992, train acc: 0.7220, val loss: 0.5386, val acc: 0.8948  (best train acc: 0.7419, best val acc: 0.9039, best train loss: 0.7697  @ epoch 8382 )\n",
      "[Epoch: 8860] train loss: 0.7959, train acc: 0.7253, val loss: 0.5400, val acc: 0.8975  (best train acc: 0.7419, best val acc: 0.9039, best train loss: 0.7697  @ epoch 8382 )\n",
      "[Epoch: 8880] train loss: 0.7862, train acc: 0.7239, val loss: 0.5396, val acc: 0.8870  (best train acc: 0.7419, best val acc: 0.9039, best train loss: 0.7697  @ epoch 8382 )\n",
      "[Epoch: 8900] train loss: 0.7948, train acc: 0.7008, val loss: 0.5451, val acc: 0.8530  (best train acc: 0.7419, best val acc: 0.9039, best train loss: 0.7697  @ epoch 8382 )\n",
      "[Epoch: 8920] train loss: 0.8096, train acc: 0.7118, val loss: 0.5742, val acc: 0.8314  (best train acc: 0.7419, best val acc: 0.9039, best train loss: 0.7697  @ epoch 8382 )\n",
      "[Epoch: 8940] train loss: 0.8020, train acc: 0.7240, val loss: 0.5497, val acc: 0.8917  (best train acc: 0.7419, best val acc: 0.9039, best train loss: 0.7697  @ epoch 8382 )\n",
      "[Epoch: 8960] train loss: 0.7962, train acc: 0.7277, val loss: 0.5535, val acc: 0.8958  (best train acc: 0.7419, best val acc: 0.9039, best train loss: 0.7697  @ epoch 8382 )\n",
      "[Epoch: 8980] train loss: 0.7920, train acc: 0.7253, val loss: 0.5394, val acc: 0.8978  (best train acc: 0.7419, best val acc: 0.9039, best train loss: 0.7697  @ epoch 8382 )\n",
      "[Epoch: 9000] train loss: 0.7981, train acc: 0.7246, val loss: 0.5388, val acc: 0.8965  (best train acc: 0.7419, best val acc: 0.9039, best train loss: 0.7697  @ epoch 8382 )\n",
      "[Epoch: 9020] train loss: 0.7845, train acc: 0.7233, val loss: 0.5405, val acc: 0.8901  (best train acc: 0.7419, best val acc: 0.9039, best train loss: 0.7697  @ epoch 8382 )\n",
      "[Epoch: 9040] train loss: 0.8063, train acc: 0.7092, val loss: 0.5688, val acc: 0.8546  (best train acc: 0.7419, best val acc: 0.9039, best train loss: 0.7697  @ epoch 8382 )\n",
      "[Epoch: 9060] train loss: 0.7867, train acc: 0.7259, val loss: 0.5399, val acc: 0.8975  (best train acc: 0.7419, best val acc: 0.9039, best train loss: 0.7697  @ epoch 8382 )\n",
      "[Epoch: 9080] train loss: 0.7787, train acc: 0.7237, val loss: 0.5434, val acc: 0.8958  (best train acc: 0.7419, best val acc: 0.9039, best train loss: 0.7697  @ epoch 8382 )\n",
      "[Epoch: 9100] train loss: 0.7979, train acc: 0.7295, val loss: 0.5410, val acc: 0.8988  (best train acc: 0.7419, best val acc: 0.9039, best train loss: 0.7697  @ epoch 8382 )\n",
      "[Epoch: 9120] train loss: 0.7851, train acc: 0.7300, val loss: 0.5399, val acc: 0.8981  (best train acc: 0.7419, best val acc: 0.9039, best train loss: 0.7697  @ epoch 8382 )\n",
      "[Epoch: 9140] train loss: 0.7951, train acc: 0.7221, val loss: 0.5427, val acc: 0.8951  (best train acc: 0.7419, best val acc: 0.9039, best train loss: 0.7697  @ epoch 8382 )\n",
      "[Epoch: 9160] train loss: 0.7912, train acc: 0.7216, val loss: 0.5486, val acc: 0.8853  (best train acc: 0.7419, best val acc: 0.9039, best train loss: 0.7697  @ epoch 8382 )\n",
      "[Epoch: 9180] train loss: 0.8042, train acc: 0.7123, val loss: 0.5464, val acc: 0.8786  (best train acc: 0.7419, best val acc: 0.9039, best train loss: 0.7697  @ epoch 8382 )\n",
      "[Epoch: 9200] train loss: 0.7858, train acc: 0.7233, val loss: 0.5460, val acc: 0.8911  (best train acc: 0.7419, best val acc: 0.9039, best train loss: 0.7697  @ epoch 8382 )\n",
      "[Epoch: 9220] train loss: 0.7912, train acc: 0.7186, val loss: 0.5471, val acc: 0.8857  (best train acc: 0.7419, best val acc: 0.9039, best train loss: 0.7697  @ epoch 8382 )\n",
      "[Epoch: 9240] train loss: 0.8003, train acc: 0.7128, val loss: 0.5496, val acc: 0.8793  (best train acc: 0.7419, best val acc: 0.9039, best train loss: 0.7697  @ epoch 8382 )\n",
      "[Epoch: 9260] train loss: 0.7923, train acc: 0.7164, val loss: 0.5449, val acc: 0.8971  (best train acc: 0.7419, best val acc: 0.9039, best train loss: 0.7697  @ epoch 8382 )\n",
      "[Epoch: 9280] train loss: 0.7882, train acc: 0.7253, val loss: 0.5371, val acc: 0.8890  (best train acc: 0.7419, best val acc: 0.9039, best train loss: 0.7697  @ epoch 8382 )\n",
      "[Epoch: 9300] train loss: 0.7977, train acc: 0.7196, val loss: 0.5450, val acc: 0.8965  (best train acc: 0.7419, best val acc: 0.9039, best train loss: 0.7697  @ epoch 8382 )\n",
      "[Epoch: 9320] train loss: 0.7960, train acc: 0.7193, val loss: 0.5466, val acc: 0.8877  (best train acc: 0.7419, best val acc: 0.9039, best train loss: 0.7697  @ epoch 8382 )\n",
      "[Epoch: 9340] train loss: 0.8022, train acc: 0.7249, val loss: 0.5426, val acc: 0.8917  (best train acc: 0.7419, best val acc: 0.9039, best train loss: 0.7697  @ epoch 8382 )\n",
      "[Epoch: 9360] train loss: 0.7925, train acc: 0.7202, val loss: 0.5361, val acc: 0.8975  (best train acc: 0.7419, best val acc: 0.9039, best train loss: 0.7697  @ epoch 8382 )\n",
      "[Epoch: 9380] train loss: 0.8034, train acc: 0.7235, val loss: 0.5367, val acc: 0.8917  (best train acc: 0.7419, best val acc: 0.9039, best train loss: 0.7697  @ epoch 8382 )\n",
      "[Epoch: 9400] train loss: 0.7994, train acc: 0.7204, val loss: 0.5330, val acc: 0.8887  (best train acc: 0.7419, best val acc: 0.9039, best train loss: 0.7697  @ epoch 8382 )\n",
      "[Epoch: 9420] train loss: 0.7836, train acc: 0.7249, val loss: 0.5398, val acc: 0.9029  (best train acc: 0.7419, best val acc: 0.9039, best train loss: 0.7697  @ epoch 8382 )\n",
      "[Epoch: 9440] train loss: 0.7787, train acc: 0.7290, val loss: 0.5347, val acc: 0.8877  (best train acc: 0.7419, best val acc: 0.9039, best train loss: 0.7697  @ epoch 8382 )\n",
      "[Epoch: 9460] train loss: 0.7952, train acc: 0.7129, val loss: 0.5476, val acc: 0.8776  (best train acc: 0.7419, best val acc: 0.9039, best train loss: 0.7697  @ epoch 8382 )\n",
      "[Epoch: 9480] train loss: 0.7891, train acc: 0.7270, val loss: 0.5361, val acc: 0.8958  (best train acc: 0.7419, best val acc: 0.9039, best train loss: 0.7697  @ epoch 8382 )\n",
      "[Epoch: 9500] train loss: 0.7975, train acc: 0.7221, val loss: 0.5316, val acc: 0.8914  (best train acc: 0.7419, best val acc: 0.9039, best train loss: 0.7697  @ epoch 8382 )\n",
      "[Epoch: 9520] train loss: 0.8021, train acc: 0.7204, val loss: 0.5387, val acc: 0.8934  (best train acc: 0.7419, best val acc: 0.9039, best train loss: 0.7697  @ epoch 8382 )\n",
      "[Epoch: 9540] train loss: 0.7921, train acc: 0.7242, val loss: 0.5383, val acc: 0.8971  (best train acc: 0.7419, best val acc: 0.9039, best train loss: 0.7697  @ epoch 8382 )\n",
      "[Epoch: 9560] train loss: 0.7958, train acc: 0.7068, val loss: 0.5346, val acc: 0.8718  (best train acc: 0.7419, best val acc: 0.9039, best train loss: 0.7697  @ epoch 8382 )\n",
      "[Epoch: 9580] train loss: 0.7879, train acc: 0.7215, val loss: 0.5509, val acc: 0.8809  (best train acc: 0.7419, best val acc: 0.9039, best train loss: 0.7697  @ epoch 8382 )\n",
      "[Epoch: 9600] train loss: 0.7866, train acc: 0.7214, val loss: 0.5390, val acc: 0.8948  (best train acc: 0.7419, best val acc: 0.9039, best train loss: 0.7697  @ epoch 8382 )\n",
      "[Epoch: 9620] train loss: 0.8074, train acc: 0.7220, val loss: 0.5366, val acc: 0.8981  (best train acc: 0.7419, best val acc: 0.9039, best train loss: 0.7697  @ epoch 8382 )\n",
      "[Epoch: 9640] train loss: 0.7934, train acc: 0.7241, val loss: 0.5507, val acc: 0.8769  (best train acc: 0.7419, best val acc: 0.9039, best train loss: 0.7697  @ epoch 8382 )\n",
      "[Epoch: 9660] train loss: 0.7830, train acc: 0.7129, val loss: 0.5686, val acc: 0.8462  (best train acc: 0.7419, best val acc: 0.9039, best train loss: 0.7697  @ epoch 8382 )\n",
      "[Epoch: 9680] train loss: 0.7978, train acc: 0.7039, val loss: 0.5643, val acc: 0.8320  (best train acc: 0.7419, best val acc: 0.9039, best train loss: 0.7697  @ epoch 8382 )\n",
      "[Epoch: 9700] train loss: 0.7943, train acc: 0.7197, val loss: 0.5551, val acc: 0.8860  (best train acc: 0.7419, best val acc: 0.9039, best train loss: 0.7697  @ epoch 8382 )\n",
      "[Epoch: 9720] train loss: 0.7906, train acc: 0.7222, val loss: 0.5454, val acc: 0.8924  (best train acc: 0.7419, best val acc: 0.9039, best train loss: 0.7697  @ epoch 8382 )\n",
      "[Epoch: 9740] train loss: 0.8016, train acc: 0.7157, val loss: 0.5585, val acc: 0.8739  (best train acc: 0.7419, best val acc: 0.9039, best train loss: 0.7697  @ epoch 8382 )\n",
      "[Epoch: 9760] train loss: 0.7924, train acc: 0.7175, val loss: 0.5343, val acc: 0.8857  (best train acc: 0.7419, best val acc: 0.9039, best train loss: 0.7697  @ epoch 8382 )\n",
      "[Epoch: 9780] train loss: 0.8016, train acc: 0.7215, val loss: 0.5454, val acc: 0.8874  (best train acc: 0.7419, best val acc: 0.9042, best train loss: 0.7697  @ epoch 8382 )\n",
      "[Epoch: 9800] train loss: 0.7893, train acc: 0.7195, val loss: 0.5447, val acc: 0.8820  (best train acc: 0.7419, best val acc: 0.9042, best train loss: 0.7697  @ epoch 8382 )\n",
      "[Epoch: 9820] train loss: 0.7990, train acc: 0.7192, val loss: 0.5471, val acc: 0.8796  (best train acc: 0.7419, best val acc: 0.9042, best train loss: 0.7697  @ epoch 8382 )\n",
      "[Epoch: 9840] train loss: 0.7976, train acc: 0.7242, val loss: 0.5399, val acc: 0.8958  (best train acc: 0.7419, best val acc: 0.9042, best train loss: 0.7697  @ epoch 8382 )\n",
      "[Epoch: 9860] train loss: 0.7863, train acc: 0.7276, val loss: 0.5445, val acc: 0.8823  (best train acc: 0.7419, best val acc: 0.9042, best train loss: 0.7697  @ epoch 8382 )\n",
      "[Epoch: 9880] train loss: 0.8096, train acc: 0.7090, val loss: 0.5456, val acc: 0.8648  (best train acc: 0.7419, best val acc: 0.9042, best train loss: 0.7697  @ epoch 8382 )\n",
      "[Epoch: 9900] train loss: 0.8269, train acc: 0.7115, val loss: 0.5861, val acc: 0.8280  (best train acc: 0.7419, best val acc: 0.9042, best train loss: 0.7697  @ epoch 8382 )\n",
      "[Epoch: 9920] train loss: 0.7945, train acc: 0.7199, val loss: 0.5442, val acc: 0.8776  (best train acc: 0.7419, best val acc: 0.9042, best train loss: 0.7697  @ epoch 8382 )\n",
      "[Epoch: 9940] train loss: 0.7849, train acc: 0.7226, val loss: 0.5408, val acc: 0.9002  (best train acc: 0.7419, best val acc: 0.9042, best train loss: 0.7697  @ epoch 8382 )\n",
      "[Epoch: 9960] train loss: 0.7831, train acc: 0.7286, val loss: 0.5370, val acc: 0.8823  (best train acc: 0.7419, best val acc: 0.9042, best train loss: 0.7697  @ epoch 8382 )\n",
      "[Epoch: 9980] train loss: 0.7974, train acc: 0.7204, val loss: 0.5426, val acc: 0.8934  (best train acc: 0.7419, best val acc: 0.9042, best train loss: 0.7697  @ epoch 8382 )\n",
      "[Epoch: 10000] train loss: 0.7884, train acc: 0.7203, val loss: 0.5494, val acc: 0.8860  (best train acc: 0.7419, best val acc: 0.9042, best train loss: 0.7697  @ epoch 8382 )\n",
      "[Epoch: 10020] train loss: 0.7926, train acc: 0.7201, val loss: 0.5369, val acc: 0.8978  (best train acc: 0.7419, best val acc: 0.9042, best train loss: 0.7641  @ epoch 10002 )\n",
      "[Epoch: 10040] train loss: 0.7355, train acc: 0.7099, val loss: 0.4627, val acc: 0.8698  (best train acc: 0.7419, best val acc: 0.9042, best train loss: 0.7322  @ epoch 10037 )\n",
      "[Epoch: 10060] train loss: 0.7264, train acc: 0.7243, val loss: 0.4240, val acc: 0.8553  (best train acc: 0.7419, best val acc: 0.9042, best train loss: 0.7029  @ epoch 10054 )\n",
      "[Epoch: 10080] train loss: 0.7203, train acc: 0.7200, val loss: 0.4027, val acc: 0.8921  (best train acc: 0.7419, best val acc: 0.9042, best train loss: 0.6982  @ epoch 10062 )\n",
      "[Epoch: 10100] train loss: 0.7098, train acc: 0.7270, val loss: 0.4082, val acc: 0.8998  (best train acc: 0.7419, best val acc: 0.9042, best train loss: 0.6876  @ epoch 10096 )\n",
      "[Epoch: 10120] train loss: 0.7110, train acc: 0.7280, val loss: 0.4048, val acc: 0.8938  (best train acc: 0.7419, best val acc: 0.9042, best train loss: 0.6876  @ epoch 10096 )\n",
      "[Epoch: 10140] train loss: 0.7118, train acc: 0.7254, val loss: 0.3923, val acc: 0.8941  (best train acc: 0.7419, best val acc: 0.9042, best train loss: 0.6828  @ epoch 10122 )\n",
      "[Epoch: 10160] train loss: 0.6976, train acc: 0.7256, val loss: 0.3983, val acc: 0.8911  (best train acc: 0.7419, best val acc: 0.9042, best train loss: 0.6828  @ epoch 10122 )\n",
      "[Epoch: 10180] train loss: 0.7029, train acc: 0.7248, val loss: 0.4165, val acc: 0.8755  (best train acc: 0.7419, best val acc: 0.9042, best train loss: 0.6828  @ epoch 10122 )\n",
      "[Epoch: 10200] train loss: 0.7140, train acc: 0.7197, val loss: 0.4110, val acc: 0.8863  (best train acc: 0.7419, best val acc: 0.9042, best train loss: 0.6828  @ epoch 10122 )\n",
      "[Epoch: 10220] train loss: 0.7000, train acc: 0.7334, val loss: 0.3953, val acc: 0.8998  (best train acc: 0.7419, best val acc: 0.9042, best train loss: 0.6828  @ epoch 10122 )\n",
      "[Epoch: 10240] train loss: 0.6943, train acc: 0.7319, val loss: 0.3962, val acc: 0.8830  (best train acc: 0.7419, best val acc: 0.9042, best train loss: 0.6828  @ epoch 10122 )\n",
      "[Epoch: 10260] train loss: 0.6993, train acc: 0.7326, val loss: 0.4183, val acc: 0.8874  (best train acc: 0.7419, best val acc: 0.9042, best train loss: 0.6824  @ epoch 10249 )\n",
      "[Epoch: 10280] train loss: 0.6873, train acc: 0.7402, val loss: 0.3902, val acc: 0.8965  (best train acc: 0.7419, best val acc: 0.9042, best train loss: 0.6824  @ epoch 10249 )\n",
      "[Epoch: 10300] train loss: 0.7138, train acc: 0.7294, val loss: 0.3901, val acc: 0.8904  (best train acc: 0.7419, best val acc: 0.9042, best train loss: 0.6794  @ epoch 10296 )\n",
      "[Epoch: 10320] train loss: 0.6699, train acc: 0.7496, val loss: 0.3997, val acc: 0.8860  (best train acc: 0.7496, best val acc: 0.9042, best train loss: 0.6679  @ epoch 10318 )\n",
      "[Epoch: 10340] train loss: 0.7089, train acc: 0.7253, val loss: 0.3710, val acc: 0.8948  (best train acc: 0.7496, best val acc: 0.9042, best train loss: 0.6679  @ epoch 10318 )\n",
      "[Epoch: 10360] train loss: 0.6944, train acc: 0.7287, val loss: 0.3921, val acc: 0.8901  (best train acc: 0.7496, best val acc: 0.9042, best train loss: 0.6679  @ epoch 10318 )\n",
      "[Epoch: 10380] train loss: 0.6470, train acc: 0.7363, val loss: 0.3914, val acc: 0.8897  (best train acc: 0.7496, best val acc: 0.9042, best train loss: 0.6470  @ epoch 10380 )\n",
      "[Epoch: 10400] train loss: 0.6433, train acc: 0.7503, val loss: 0.3653, val acc: 0.9008  (best train acc: 0.7513, best val acc: 0.9042, best train loss: 0.6345  @ epoch 10392 )\n",
      "[Epoch: 10420] train loss: 0.6389, train acc: 0.7418, val loss: 0.3839, val acc: 0.8911  (best train acc: 0.7513, best val acc: 0.9042, best train loss: 0.6300  @ epoch 10410 )\n",
      "[Epoch: 10440] train loss: 0.6421, train acc: 0.7485, val loss: 0.3672, val acc: 0.8992  (best train acc: 0.7560, best val acc: 0.9042, best train loss: 0.6261  @ epoch 10431 )\n",
      "[Epoch: 10460] train loss: 0.6233, train acc: 0.7495, val loss: 0.3569, val acc: 0.8961  (best train acc: 0.7560, best val acc: 0.9042, best train loss: 0.6225  @ epoch 10447 )\n",
      "[Epoch: 10480] train loss: 0.6350, train acc: 0.7475, val loss: 0.3555, val acc: 0.8971  (best train acc: 0.7560, best val acc: 0.9042, best train loss: 0.6093  @ epoch 10477 )\n",
      "[Epoch: 10500] train loss: 0.6299, train acc: 0.7407, val loss: 0.3546, val acc: 0.8880  (best train acc: 0.7561, best val acc: 0.9042, best train loss: 0.6093  @ epoch 10477 )\n",
      "[Epoch: 10520] train loss: 0.6192, train acc: 0.7464, val loss: 0.3508, val acc: 0.8985  (best train acc: 0.7561, best val acc: 0.9042, best train loss: 0.6093  @ epoch 10477 )\n",
      "[Epoch: 10540] train loss: 0.6219, train acc: 0.7446, val loss: 0.3467, val acc: 0.8981  (best train acc: 0.7561, best val acc: 0.9042, best train loss: 0.6093  @ epoch 10477 )\n",
      "[Epoch: 10560] train loss: 0.6517, train acc: 0.7345, val loss: 0.3585, val acc: 0.8880  (best train acc: 0.7561, best val acc: 0.9042, best train loss: 0.6093  @ epoch 10477 )\n",
      "[Epoch: 10580] train loss: 0.6313, train acc: 0.7433, val loss: 0.3492, val acc: 0.8965  (best train acc: 0.7561, best val acc: 0.9042, best train loss: 0.6093  @ epoch 10477 )\n",
      "[Epoch: 10600] train loss: 0.6239, train acc: 0.7391, val loss: 0.3470, val acc: 0.8786  (best train acc: 0.7561, best val acc: 0.9042, best train loss: 0.6093  @ epoch 10477 )\n",
      "[Epoch: 10620] train loss: 0.6180, train acc: 0.7410, val loss: 0.3504, val acc: 0.8782  (best train acc: 0.7561, best val acc: 0.9042, best train loss: 0.6085  @ epoch 10604 )\n",
      "[Epoch: 10640] train loss: 0.6205, train acc: 0.7444, val loss: 0.3434, val acc: 0.8961  (best train acc: 0.7561, best val acc: 0.9042, best train loss: 0.6085  @ epoch 10604 )\n",
      "[Epoch: 10660] train loss: 0.6306, train acc: 0.7298, val loss: 0.3380, val acc: 0.8975  (best train acc: 0.7561, best val acc: 0.9042, best train loss: 0.6051  @ epoch 10656 )\n",
      "[Epoch: 10680] train loss: 0.6277, train acc: 0.7426, val loss: 0.3748, val acc: 0.8621  (best train acc: 0.7561, best val acc: 0.9042, best train loss: 0.6051  @ epoch 10656 )\n",
      "[Epoch: 10700] train loss: 0.6212, train acc: 0.7397, val loss: 0.3396, val acc: 0.9025  (best train acc: 0.7563, best val acc: 0.9042, best train loss: 0.6000  @ epoch 10691 )\n",
      "[Epoch: 10720] train loss: 0.6131, train acc: 0.7423, val loss: 0.3379, val acc: 0.8948  (best train acc: 0.7563, best val acc: 0.9042, best train loss: 0.6000  @ epoch 10691 )\n",
      "[Epoch: 10740] train loss: 0.6006, train acc: 0.7479, val loss: 0.3306, val acc: 0.8951  (best train acc: 0.7563, best val acc: 0.9042, best train loss: 0.5953  @ epoch 10726 )\n",
      "[Epoch: 10760] train loss: 0.6267, train acc: 0.7388, val loss: 0.3433, val acc: 0.8911  (best train acc: 0.7563, best val acc: 0.9042, best train loss: 0.5953  @ epoch 10726 )\n",
      "[Epoch: 10780] train loss: 0.6225, train acc: 0.7371, val loss: 0.3429, val acc: 0.8951  (best train acc: 0.7563, best val acc: 0.9042, best train loss: 0.5953  @ epoch 10726 )\n",
      "[Epoch: 10800] train loss: 0.6425, train acc: 0.7353, val loss: 0.3453, val acc: 0.8938  (best train acc: 0.7563, best val acc: 0.9042, best train loss: 0.5953  @ epoch 10726 )\n",
      "[Epoch: 10820] train loss: 0.6278, train acc: 0.7368, val loss: 0.3296, val acc: 0.8995  (best train acc: 0.7563, best val acc: 0.9042, best train loss: 0.5953  @ epoch 10726 )\n",
      "[Epoch: 10840] train loss: 0.6204, train acc: 0.7326, val loss: 0.3486, val acc: 0.8880  (best train acc: 0.7563, best val acc: 0.9042, best train loss: 0.5953  @ epoch 10726 )\n",
      "[Epoch: 10860] train loss: 0.6246, train acc: 0.7350, val loss: 0.3259, val acc: 0.8965  (best train acc: 0.7563, best val acc: 0.9042, best train loss: 0.5953  @ epoch 10726 )\n",
      "[Epoch: 10880] train loss: 0.6172, train acc: 0.7416, val loss: 0.3281, val acc: 0.8911  (best train acc: 0.7563, best val acc: 0.9042, best train loss: 0.5953  @ epoch 10726 )\n",
      "[Epoch: 10900] train loss: 0.6245, train acc: 0.7356, val loss: 0.3268, val acc: 0.8978  (best train acc: 0.7563, best val acc: 0.9042, best train loss: 0.5953  @ epoch 10726 )\n",
      "[Epoch: 10920] train loss: 0.6158, train acc: 0.7401, val loss: 0.3246, val acc: 0.9022  (best train acc: 0.7563, best val acc: 0.9042, best train loss: 0.5953  @ epoch 10726 )\n",
      "[Epoch: 10940] train loss: 0.6290, train acc: 0.7379, val loss: 0.3215, val acc: 0.8951  (best train acc: 0.7563, best val acc: 0.9042, best train loss: 0.5953  @ epoch 10726 )\n",
      "[Epoch: 10960] train loss: 0.6171, train acc: 0.7486, val loss: 0.3311, val acc: 0.8971  (best train acc: 0.7563, best val acc: 0.9042, best train loss: 0.5939  @ epoch 10949 )\n",
      "[Epoch: 10980] train loss: 0.6185, train acc: 0.7389, val loss: 0.3240, val acc: 0.8961  (best train acc: 0.7563, best val acc: 0.9042, best train loss: 0.5939  @ epoch 10949 )\n",
      "[Epoch: 11000] train loss: 0.6398, train acc: 0.7390, val loss: 0.3340, val acc: 0.8995  (best train acc: 0.7563, best val acc: 0.9042, best train loss: 0.5939  @ epoch 10949 )\n",
      "[Epoch: 11020] train loss: 0.6219, train acc: 0.7376, val loss: 0.3290, val acc: 0.8894  (best train acc: 0.7563, best val acc: 0.9042, best train loss: 0.5939  @ epoch 10949 )\n",
      "[Epoch: 11040] train loss: 0.6071, train acc: 0.7482, val loss: 0.3238, val acc: 0.8975  (best train acc: 0.7563, best val acc: 0.9042, best train loss: 0.5939  @ epoch 10949 )\n",
      "[Epoch: 11060] train loss: 0.6163, train acc: 0.7334, val loss: 0.3255, val acc: 0.8992  (best train acc: 0.7563, best val acc: 0.9042, best train loss: 0.5935  @ epoch 11052 )\n",
      "[Epoch: 11080] train loss: 0.6191, train acc: 0.7442, val loss: 0.3286, val acc: 0.8924  (best train acc: 0.7563, best val acc: 0.9042, best train loss: 0.5935  @ epoch 11052 )\n",
      "[Epoch: 11100] train loss: 0.6111, train acc: 0.7382, val loss: 0.3374, val acc: 0.8877  (best train acc: 0.7563, best val acc: 0.9042, best train loss: 0.5922  @ epoch 11099 )\n",
      "[Epoch: 11120] train loss: 0.6058, train acc: 0.7396, val loss: 0.3147, val acc: 0.8961  (best train acc: 0.7563, best val acc: 0.9042, best train loss: 0.5922  @ epoch 11099 )\n",
      "[Epoch: 11140] train loss: 0.5965, train acc: 0.7442, val loss: 0.3226, val acc: 0.8954  (best train acc: 0.7563, best val acc: 0.9042, best train loss: 0.5916  @ epoch 11131 )\n",
      "[Epoch: 11160] train loss: 0.6095, train acc: 0.7373, val loss: 0.3244, val acc: 0.9008  (best train acc: 0.7563, best val acc: 0.9042, best train loss: 0.5916  @ epoch 11131 )\n",
      "[Epoch: 11180] train loss: 0.6193, train acc: 0.7469, val loss: 0.3216, val acc: 0.8840  (best train acc: 0.7563, best val acc: 0.9042, best train loss: 0.5916  @ epoch 11131 )\n",
      "[Epoch: 11200] train loss: 0.6192, train acc: 0.7319, val loss: 0.3299, val acc: 0.9005  (best train acc: 0.7563, best val acc: 0.9042, best train loss: 0.5916  @ epoch 11131 )\n",
      "[Epoch: 11220] train loss: 0.6029, train acc: 0.7439, val loss: 0.3186, val acc: 0.8978  (best train acc: 0.7563, best val acc: 0.9042, best train loss: 0.5916  @ epoch 11131 )\n",
      "[Epoch: 11240] train loss: 0.6232, train acc: 0.7366, val loss: 0.3232, val acc: 0.8911  (best train acc: 0.7563, best val acc: 0.9042, best train loss: 0.5916  @ epoch 11131 )\n",
      "[Epoch: 11260] train loss: 0.6148, train acc: 0.7490, val loss: 0.3143, val acc: 0.8998  (best train acc: 0.7563, best val acc: 0.9042, best train loss: 0.5916  @ epoch 11131 )\n",
      "[Epoch: 11280] train loss: 0.6084, train acc: 0.7460, val loss: 0.3308, val acc: 0.8968  (best train acc: 0.7563, best val acc: 0.9042, best train loss: 0.5916  @ epoch 11131 )\n",
      "[Epoch: 11300] train loss: 0.6025, train acc: 0.7420, val loss: 0.3215, val acc: 0.9008  (best train acc: 0.7563, best val acc: 0.9042, best train loss: 0.5916  @ epoch 11131 )\n",
      "[Epoch: 11320] train loss: 0.5999, train acc: 0.7467, val loss: 0.3245, val acc: 0.8954  (best train acc: 0.7563, best val acc: 0.9042, best train loss: 0.5899  @ epoch 11302 )\n",
      "[Epoch: 11340] train loss: 0.6171, train acc: 0.7351, val loss: 0.3160, val acc: 0.8971  (best train acc: 0.7563, best val acc: 0.9042, best train loss: 0.5899  @ epoch 11302 )\n",
      "[Epoch: 11360] train loss: 0.6338, train acc: 0.7412, val loss: 0.3264, val acc: 0.8968  (best train acc: 0.7563, best val acc: 0.9042, best train loss: 0.5899  @ epoch 11302 )\n",
      "[Epoch: 11380] train loss: 0.5982, train acc: 0.7453, val loss: 0.3188, val acc: 0.9005  (best train acc: 0.7563, best val acc: 0.9042, best train loss: 0.5899  @ epoch 11302 )\n",
      "[Epoch: 11400] train loss: 0.6012, train acc: 0.7422, val loss: 0.3126, val acc: 0.9005  (best train acc: 0.7563, best val acc: 0.9042, best train loss: 0.5899  @ epoch 11302 )\n",
      "[Epoch: 11420] train loss: 0.6189, train acc: 0.7369, val loss: 0.3474, val acc: 0.8958  (best train acc: 0.7563, best val acc: 0.9042, best train loss: 0.5899  @ epoch 11302 )\n",
      "[Epoch: 11440] train loss: 0.6123, train acc: 0.7454, val loss: 0.3271, val acc: 0.8917  (best train acc: 0.7563, best val acc: 0.9042, best train loss: 0.5899  @ epoch 11302 )\n",
      "[Epoch: 11460] train loss: 0.6355, train acc: 0.7269, val loss: 0.3264, val acc: 0.8843  (best train acc: 0.7563, best val acc: 0.9042, best train loss: 0.5885  @ epoch 11452 )\n",
      "[Epoch: 11480] train loss: 0.6375, train acc: 0.7437, val loss: 0.3100, val acc: 0.9002  (best train acc: 0.7563, best val acc: 0.9042, best train loss: 0.5885  @ epoch 11452 )\n",
      "[Epoch: 11500] train loss: 0.6099, train acc: 0.7438, val loss: 0.3143, val acc: 0.8958  (best train acc: 0.7563, best val acc: 0.9042, best train loss: 0.5885  @ epoch 11452 )\n",
      "[Epoch: 11520] train loss: 0.6266, train acc: 0.7379, val loss: 0.3194, val acc: 0.8948  (best train acc: 0.7563, best val acc: 0.9042, best train loss: 0.5885  @ epoch 11452 )\n",
      "[Epoch: 11540] train loss: 0.6146, train acc: 0.7360, val loss: 0.3093, val acc: 0.9022  (best train acc: 0.7563, best val acc: 0.9042, best train loss: 0.5885  @ epoch 11452 )\n",
      "[Epoch: 11560] train loss: 0.6021, train acc: 0.7449, val loss: 0.3225, val acc: 0.8921  (best train acc: 0.7563, best val acc: 0.9042, best train loss: 0.5885  @ epoch 11452 )\n",
      "[Epoch: 11580] train loss: 0.6078, train acc: 0.7482, val loss: 0.3156, val acc: 0.8992  (best train acc: 0.7563, best val acc: 0.9042, best train loss: 0.5885  @ epoch 11452 )\n",
      "[Epoch: 11600] train loss: 0.6067, train acc: 0.7463, val loss: 0.3122, val acc: 0.8965  (best train acc: 0.7563, best val acc: 0.9042, best train loss: 0.5885  @ epoch 11452 )\n",
      "[Epoch: 11620] train loss: 0.6063, train acc: 0.7401, val loss: 0.3206, val acc: 0.8874  (best train acc: 0.7563, best val acc: 0.9042, best train loss: 0.5884  @ epoch 11607 )\n",
      "[Epoch: 11640] train loss: 0.5899, train acc: 0.7483, val loss: 0.3225, val acc: 0.8985  (best train acc: 0.7563, best val acc: 0.9042, best train loss: 0.5884  @ epoch 11607 )\n",
      "[Epoch: 11660] train loss: 0.6097, train acc: 0.7423, val loss: 0.3107, val acc: 0.9025  (best train acc: 0.7563, best val acc: 0.9042, best train loss: 0.5884  @ epoch 11607 )\n",
      "[Epoch: 11680] train loss: 0.5980, train acc: 0.7427, val loss: 0.3211, val acc: 0.8924  (best train acc: 0.7563, best val acc: 0.9042, best train loss: 0.5884  @ epoch 11607 )\n",
      "[Epoch: 11700] train loss: 0.6117, train acc: 0.7336, val loss: 0.3396, val acc: 0.8813  (best train acc: 0.7599, best val acc: 0.9049, best train loss: 0.5873  @ epoch 11694 )\n",
      "[Epoch: 11720] train loss: 0.6080, train acc: 0.7475, val loss: 0.3212, val acc: 0.9005  (best train acc: 0.7599, best val acc: 0.9049, best train loss: 0.5873  @ epoch 11694 )\n",
      "[Epoch: 11740] train loss: 0.6113, train acc: 0.7405, val loss: 0.3133, val acc: 0.8968  (best train acc: 0.7599, best val acc: 0.9049, best train loss: 0.5873  @ epoch 11694 )\n",
      "[Epoch: 11760] train loss: 0.6232, train acc: 0.7352, val loss: 0.3402, val acc: 0.8917  (best train acc: 0.7599, best val acc: 0.9049, best train loss: 0.5873  @ epoch 11748 )\n",
      "[Epoch: 11780] train loss: 0.6162, train acc: 0.7341, val loss: 0.3230, val acc: 0.8938  (best train acc: 0.7599, best val acc: 0.9049, best train loss: 0.5873  @ epoch 11748 )\n",
      "[Epoch: 11800] train loss: 0.6137, train acc: 0.7441, val loss: 0.3261, val acc: 0.9002  (best train acc: 0.7599, best val acc: 0.9049, best train loss: 0.5873  @ epoch 11748 )\n",
      "[Epoch: 11820] train loss: 0.5937, train acc: 0.7426, val loss: 0.3154, val acc: 0.9029  (best train acc: 0.7599, best val acc: 0.9049, best train loss: 0.5832  @ epoch 11815 )\n",
      "[Epoch: 11840] train loss: 0.6189, train acc: 0.7454, val loss: 0.3225, val acc: 0.8978  (best train acc: 0.7599, best val acc: 0.9049, best train loss: 0.5832  @ epoch 11815 )\n",
      "[Epoch: 11860] train loss: 0.6095, train acc: 0.7411, val loss: 0.3263, val acc: 0.8951  (best train acc: 0.7599, best val acc: 0.9049, best train loss: 0.5832  @ epoch 11815 )\n",
      "[Epoch: 11880] train loss: 0.6296, train acc: 0.7436, val loss: 0.3117, val acc: 0.9022  (best train acc: 0.7599, best val acc: 0.9049, best train loss: 0.5832  @ epoch 11815 )\n",
      "[Epoch: 11900] train loss: 0.6130, train acc: 0.7418, val loss: 0.3208, val acc: 0.8863  (best train acc: 0.7599, best val acc: 0.9049, best train loss: 0.5832  @ epoch 11815 )\n",
      "[Epoch: 11920] train loss: 0.6037, train acc: 0.7381, val loss: 0.3082, val acc: 0.8927  (best train acc: 0.7599, best val acc: 0.9049, best train loss: 0.5832  @ epoch 11815 )\n",
      "[Epoch: 11940] train loss: 0.5962, train acc: 0.7501, val loss: 0.3216, val acc: 0.8968  (best train acc: 0.7599, best val acc: 0.9049, best train loss: 0.5832  @ epoch 11815 )\n",
      "[Epoch: 11960] train loss: 0.5920, train acc: 0.7496, val loss: 0.3096, val acc: 0.9002  (best train acc: 0.7599, best val acc: 0.9052, best train loss: 0.5832  @ epoch 11815 )\n",
      "[Epoch: 11980] train loss: 0.5947, train acc: 0.7456, val loss: 0.3460, val acc: 0.8836  (best train acc: 0.7599, best val acc: 0.9056, best train loss: 0.5832  @ epoch 11815 )\n",
      "[Epoch: 12000] train loss: 0.5966, train acc: 0.7585, val loss: 0.3261, val acc: 0.8894  (best train acc: 0.7599, best val acc: 0.9079, best train loss: 0.5832  @ epoch 11815 )\n",
      "[Epoch: 12020] train loss: 0.5987, train acc: 0.7452, val loss: 0.3108, val acc: 0.8971  (best train acc: 0.7599, best val acc: 0.9079, best train loss: 0.5832  @ epoch 11815 )\n",
      "[Epoch: 12040] train loss: 0.5951, train acc: 0.7488, val loss: 0.3182, val acc: 0.9019  (best train acc: 0.7599, best val acc: 0.9079, best train loss: 0.5813  @ epoch 12026 )\n",
      "[Epoch: 12060] train loss: 0.6070, train acc: 0.7551, val loss: 0.3109, val acc: 0.9015  (best train acc: 0.7611, best val acc: 0.9079, best train loss: 0.5813  @ epoch 12026 )\n",
      "[Epoch: 12080] train loss: 0.6235, train acc: 0.7413, val loss: 0.2990, val acc: 0.9005  (best train acc: 0.7611, best val acc: 0.9079, best train loss: 0.5813  @ epoch 12026 )\n",
      "[Epoch: 12100] train loss: 0.5988, train acc: 0.7352, val loss: 0.3148, val acc: 0.8995  (best train acc: 0.7611, best val acc: 0.9083, best train loss: 0.5813  @ epoch 12026 )\n",
      "[Epoch: 12120] train loss: 0.5808, train acc: 0.7457, val loss: 0.3078, val acc: 0.9019  (best train acc: 0.7611, best val acc: 0.9083, best train loss: 0.5605  @ epoch 12117 )\n",
      "[Epoch: 12140] train loss: 0.5733, train acc: 0.7479, val loss: 0.3171, val acc: 0.8978  (best train acc: 0.7642, best val acc: 0.9083, best train loss: 0.5564  @ epoch 12124 )\n",
      "[Epoch: 12160] train loss: 0.5636, train acc: 0.7577, val loss: 0.3159, val acc: 0.9002  (best train acc: 0.7642, best val acc: 0.9083, best train loss: 0.5564  @ epoch 12124 )\n",
      "[Epoch: 12180] train loss: 0.5864, train acc: 0.7457, val loss: 0.3165, val acc: 0.8958  (best train acc: 0.7642, best val acc: 0.9099, best train loss: 0.5543  @ epoch 12173 )\n",
      "[Epoch: 12200] train loss: 0.5741, train acc: 0.7510, val loss: 0.3081, val acc: 0.8998  (best train acc: 0.7642, best val acc: 0.9099, best train loss: 0.5543  @ epoch 12173 )\n",
      "[Epoch: 12220] train loss: 0.5653, train acc: 0.7532, val loss: 0.3181, val acc: 0.8971  (best train acc: 0.7647, best val acc: 0.9099, best train loss: 0.5543  @ epoch 12173 )\n",
      "[Epoch: 12240] train loss: 0.5694, train acc: 0.7548, val loss: 0.3210, val acc: 0.8988  (best train acc: 0.7647, best val acc: 0.9099, best train loss: 0.5543  @ epoch 12173 )\n",
      "[Epoch: 12260] train loss: 0.5678, train acc: 0.7506, val loss: 0.3103, val acc: 0.9029  (best train acc: 0.7647, best val acc: 0.9099, best train loss: 0.5543  @ epoch 12173 )\n",
      "[Epoch: 12280] train loss: 0.5690, train acc: 0.7540, val loss: 0.3114, val acc: 0.8988  (best train acc: 0.7647, best val acc: 0.9099, best train loss: 0.5527  @ epoch 12271 )\n",
      "[Epoch: 12300] train loss: 0.5715, train acc: 0.7525, val loss: 0.3359, val acc: 0.8981  (best train acc: 0.7647, best val acc: 0.9099, best train loss: 0.5527  @ epoch 12271 )\n",
      "[Epoch: 12320] train loss: 0.5780, train acc: 0.7488, val loss: 0.3120, val acc: 0.8931  (best train acc: 0.7647, best val acc: 0.9099, best train loss: 0.5527  @ epoch 12271 )\n",
      "[Epoch: 12340] train loss: 0.5738, train acc: 0.7519, val loss: 0.3091, val acc: 0.8998  (best train acc: 0.7647, best val acc: 0.9099, best train loss: 0.5527  @ epoch 12271 )\n",
      "[Epoch: 12360] train loss: 0.5808, train acc: 0.7515, val loss: 0.3072, val acc: 0.9052  (best train acc: 0.7671, best val acc: 0.9099, best train loss: 0.5462  @ epoch 12348 )\n",
      "[Epoch: 12380] train loss: 0.5832, train acc: 0.7478, val loss: 0.3004, val acc: 0.8981  (best train acc: 0.7671, best val acc: 0.9099, best train loss: 0.5462  @ epoch 12348 )\n",
      "[Epoch: 12400] train loss: 0.5709, train acc: 0.7462, val loss: 0.2971, val acc: 0.9012  (best train acc: 0.7671, best val acc: 0.9099, best train loss: 0.5462  @ epoch 12348 )\n",
      "[Epoch: 12420] train loss: 0.5751, train acc: 0.7568, val loss: 0.3166, val acc: 0.8965  (best train acc: 0.7671, best val acc: 0.9099, best train loss: 0.5462  @ epoch 12348 )\n",
      "[Epoch: 12440] train loss: 0.5667, train acc: 0.7568, val loss: 0.3167, val acc: 0.8958  (best train acc: 0.7671, best val acc: 0.9099, best train loss: 0.5462  @ epoch 12348 )\n",
      "[Epoch: 12460] train loss: 0.5585, train acc: 0.7594, val loss: 0.3146, val acc: 0.8988  (best train acc: 0.7671, best val acc: 0.9099, best train loss: 0.5462  @ epoch 12348 )\n",
      "[Epoch: 12480] train loss: 0.5786, train acc: 0.7505, val loss: 0.3314, val acc: 0.8971  (best train acc: 0.7671, best val acc: 0.9099, best train loss: 0.5462  @ epoch 12348 )\n",
      "[Epoch: 12500] train loss: 0.5601, train acc: 0.7561, val loss: 0.3201, val acc: 0.8961  (best train acc: 0.7671, best val acc: 0.9099, best train loss: 0.5462  @ epoch 12348 )\n",
      "[Epoch: 12520] train loss: 0.5690, train acc: 0.7507, val loss: 0.3021, val acc: 0.9039  (best train acc: 0.7707, best val acc: 0.9099, best train loss: 0.5407  @ epoch 12514 )\n",
      "[Epoch: 12540] train loss: 0.5783, train acc: 0.7493, val loss: 0.3010, val acc: 0.8995  (best train acc: 0.7707, best val acc: 0.9099, best train loss: 0.5407  @ epoch 12514 )\n",
      "[Epoch: 12560] train loss: 0.5909, train acc: 0.7433, val loss: 0.3077, val acc: 0.9019  (best train acc: 0.7707, best val acc: 0.9099, best train loss: 0.5407  @ epoch 12514 )\n",
      "[Epoch: 12580] train loss: 0.5880, train acc: 0.7511, val loss: 0.3406, val acc: 0.8874  (best train acc: 0.7707, best val acc: 0.9099, best train loss: 0.5407  @ epoch 12514 )\n",
      "[Epoch: 12600] train loss: 0.5832, train acc: 0.7512, val loss: 0.3129, val acc: 0.9002  (best train acc: 0.7707, best val acc: 0.9099, best train loss: 0.5407  @ epoch 12514 )\n",
      "[Epoch: 12620] train loss: 0.5699, train acc: 0.7519, val loss: 0.3214, val acc: 0.8978  (best train acc: 0.7707, best val acc: 0.9099, best train loss: 0.5407  @ epoch 12514 )\n",
      "[Epoch: 12640] train loss: 0.5596, train acc: 0.7622, val loss: 0.3169, val acc: 0.9002  (best train acc: 0.7707, best val acc: 0.9099, best train loss: 0.5407  @ epoch 12514 )\n",
      "[Epoch: 12660] train loss: 0.5779, train acc: 0.7491, val loss: 0.2998, val acc: 0.9032  (best train acc: 0.7707, best val acc: 0.9099, best train loss: 0.5407  @ epoch 12514 )\n",
      "[Epoch: 12680] train loss: 0.5818, train acc: 0.7525, val loss: 0.3207, val acc: 0.8772  (best train acc: 0.7707, best val acc: 0.9099, best train loss: 0.5407  @ epoch 12514 )\n",
      "[Epoch: 12700] train loss: 0.5722, train acc: 0.7505, val loss: 0.3036, val acc: 0.9035  (best train acc: 0.7707, best val acc: 0.9099, best train loss: 0.5407  @ epoch 12514 )\n",
      "[Epoch: 12720] train loss: 0.5679, train acc: 0.7563, val loss: 0.2992, val acc: 0.9039  (best train acc: 0.7707, best val acc: 0.9099, best train loss: 0.5407  @ epoch 12514 )\n",
      "[Epoch: 12740] train loss: 0.5645, train acc: 0.7557, val loss: 0.2953, val acc: 0.9039  (best train acc: 0.7707, best val acc: 0.9099, best train loss: 0.5407  @ epoch 12514 )\n",
      "[Epoch: 12760] train loss: 0.5678, train acc: 0.7502, val loss: 0.3098, val acc: 0.8992  (best train acc: 0.7707, best val acc: 0.9099, best train loss: 0.5407  @ epoch 12514 )\n",
      "[Epoch: 12780] train loss: 0.5499, train acc: 0.7629, val loss: 0.3045, val acc: 0.9062  (best train acc: 0.7707, best val acc: 0.9099, best train loss: 0.5407  @ epoch 12514 )\n",
      "[Epoch: 12800] train loss: 0.5783, train acc: 0.7563, val loss: 0.3016, val acc: 0.9035  (best train acc: 0.7707, best val acc: 0.9099, best train loss: 0.5407  @ epoch 12514 )\n",
      "[Epoch: 12820] train loss: 0.5707, train acc: 0.7531, val loss: 0.3068, val acc: 0.9019  (best train acc: 0.7716, best val acc: 0.9099, best train loss: 0.5402  @ epoch 12814 )\n",
      "[Epoch: 12840] train loss: 0.5710, train acc: 0.7528, val loss: 0.3006, val acc: 0.9093  (best train acc: 0.7716, best val acc: 0.9099, best train loss: 0.5402  @ epoch 12814 )\n",
      "[Epoch: 12860] train loss: 0.5668, train acc: 0.7584, val loss: 0.3032, val acc: 0.8971  (best train acc: 0.7716, best val acc: 0.9099, best train loss: 0.5402  @ epoch 12814 )\n",
      "[Epoch: 12880] train loss: 0.5707, train acc: 0.7524, val loss: 0.2958, val acc: 0.9035  (best train acc: 0.7716, best val acc: 0.9099, best train loss: 0.5402  @ epoch 12814 )\n",
      "[Epoch: 12900] train loss: 0.5636, train acc: 0.7543, val loss: 0.2965, val acc: 0.9052  (best train acc: 0.7716, best val acc: 0.9099, best train loss: 0.5402  @ epoch 12814 )\n",
      "[Epoch: 12920] train loss: 0.5766, train acc: 0.7564, val loss: 0.2944, val acc: 0.9093  (best train acc: 0.7716, best val acc: 0.9099, best train loss: 0.5402  @ epoch 12814 )\n",
      "[Epoch: 12940] train loss: 0.5831, train acc: 0.7433, val loss: 0.3029, val acc: 0.9012  (best train acc: 0.7716, best val acc: 0.9099, best train loss: 0.5402  @ epoch 12814 )\n",
      "[Epoch: 12960] train loss: 0.5869, train acc: 0.7439, val loss: 0.3230, val acc: 0.8769  (best train acc: 0.7716, best val acc: 0.9099, best train loss: 0.5402  @ epoch 12814 )\n",
      "[Epoch: 12980] train loss: 0.5696, train acc: 0.7585, val loss: 0.3085, val acc: 0.8965  (best train acc: 0.7716, best val acc: 0.9099, best train loss: 0.5402  @ epoch 12814 )\n",
      "[Epoch: 13000] train loss: 0.5532, train acc: 0.7621, val loss: 0.2994, val acc: 0.9025  (best train acc: 0.7716, best val acc: 0.9099, best train loss: 0.5402  @ epoch 12814 )\n",
      "[Epoch: 13020] train loss: 0.5642, train acc: 0.7581, val loss: 0.2921, val acc: 0.9056  (best train acc: 0.7716, best val acc: 0.9099, best train loss: 0.5402  @ epoch 12814 )\n",
      "[Epoch: 13040] train loss: 0.5606, train acc: 0.7627, val loss: 0.2952, val acc: 0.8995  (best train acc: 0.7716, best val acc: 0.9099, best train loss: 0.5402  @ epoch 12814 )\n",
      "[Epoch: 13060] train loss: 0.5572, train acc: 0.7547, val loss: 0.3053, val acc: 0.8968  (best train acc: 0.7716, best val acc: 0.9099, best train loss: 0.5402  @ epoch 12814 )\n",
      "[Epoch: 13080] train loss: 0.5644, train acc: 0.7616, val loss: 0.2982, val acc: 0.8961  (best train acc: 0.7716, best val acc: 0.9099, best train loss: 0.5402  @ epoch 12814 )\n",
      "[Epoch: 13100] train loss: 0.5645, train acc: 0.7587, val loss: 0.3096, val acc: 0.8985  (best train acc: 0.7716, best val acc: 0.9099, best train loss: 0.5402  @ epoch 12814 )\n",
      "[Epoch: 13120] train loss: 0.5548, train acc: 0.7610, val loss: 0.3114, val acc: 0.8927  (best train acc: 0.7716, best val acc: 0.9099, best train loss: 0.5402  @ epoch 12814 )\n",
      "[Epoch: 13140] train loss: 0.5708, train acc: 0.7600, val loss: 0.2998, val acc: 0.9012  (best train acc: 0.7716, best val acc: 0.9099, best train loss: 0.5402  @ epoch 12814 )\n",
      "[Epoch: 13160] train loss: 0.5714, train acc: 0.7518, val loss: 0.3051, val acc: 0.9005  (best train acc: 0.7716, best val acc: 0.9099, best train loss: 0.5402  @ epoch 12814 )\n",
      "[Epoch: 13180] train loss: 0.5703, train acc: 0.7475, val loss: 0.3068, val acc: 0.8944  (best train acc: 0.7716, best val acc: 0.9099, best train loss: 0.5402  @ epoch 12814 )\n",
      "[Epoch: 13200] train loss: 0.5648, train acc: 0.7504, val loss: 0.3109, val acc: 0.8951  (best train acc: 0.7716, best val acc: 0.9099, best train loss: 0.5402  @ epoch 12814 )\n",
      "[Epoch: 13220] train loss: 0.5788, train acc: 0.7463, val loss: 0.3064, val acc: 0.8985  (best train acc: 0.7716, best val acc: 0.9099, best train loss: 0.5402  @ epoch 12814 )\n",
      "[Epoch: 13240] train loss: 0.5525, train acc: 0.7577, val loss: 0.2954, val acc: 0.9039  (best train acc: 0.7716, best val acc: 0.9099, best train loss: 0.5402  @ epoch 12814 )\n",
      "[Epoch: 13260] train loss: 0.5531, train acc: 0.7633, val loss: 0.2967, val acc: 0.9019  (best train acc: 0.7716, best val acc: 0.9099, best train loss: 0.5402  @ epoch 12814 )\n",
      "[Epoch: 13280] train loss: 0.5527, train acc: 0.7612, val loss: 0.3049, val acc: 0.8988  (best train acc: 0.7716, best val acc: 0.9099, best train loss: 0.5402  @ epoch 12814 )\n",
      "[Epoch: 13300] train loss: 0.5570, train acc: 0.7667, val loss: 0.3227, val acc: 0.8820  (best train acc: 0.7716, best val acc: 0.9099, best train loss: 0.5402  @ epoch 12814 )\n",
      "[Epoch: 13320] train loss: 0.5580, train acc: 0.7567, val loss: 0.3021, val acc: 0.8968  (best train acc: 0.7716, best val acc: 0.9099, best train loss: 0.5402  @ epoch 12814 )\n",
      "[Epoch: 13340] train loss: 0.5663, train acc: 0.7525, val loss: 0.3020, val acc: 0.8894  (best train acc: 0.7716, best val acc: 0.9099, best train loss: 0.5402  @ epoch 12814 )\n",
      "[Epoch: 13360] train loss: 0.5778, train acc: 0.7545, val loss: 0.2915, val acc: 0.8988  (best train acc: 0.7716, best val acc: 0.9099, best train loss: 0.5402  @ epoch 12814 )\n",
      "[Epoch: 13380] train loss: 0.5697, train acc: 0.7553, val loss: 0.3099, val acc: 0.8901  (best train acc: 0.7716, best val acc: 0.9099, best train loss: 0.5402  @ epoch 12814 )\n",
      "[Epoch: 13400] train loss: 0.5787, train acc: 0.7587, val loss: 0.3084, val acc: 0.8992  (best train acc: 0.7716, best val acc: 0.9099, best train loss: 0.5402  @ epoch 12814 )\n",
      "[Epoch: 13420] train loss: 0.5662, train acc: 0.7564, val loss: 0.3005, val acc: 0.8992  (best train acc: 0.7716, best val acc: 0.9099, best train loss: 0.5402  @ epoch 12814 )\n",
      "[Epoch: 13440] train loss: 0.5565, train acc: 0.7564, val loss: 0.3026, val acc: 0.8934  (best train acc: 0.7716, best val acc: 0.9099, best train loss: 0.5402  @ epoch 12814 )\n",
      "[Epoch: 13460] train loss: 0.5467, train acc: 0.7645, val loss: 0.3078, val acc: 0.8813  (best train acc: 0.7716, best val acc: 0.9099, best train loss: 0.5402  @ epoch 12814 )\n",
      "[Epoch: 13480] train loss: 0.5676, train acc: 0.7432, val loss: 0.2962, val acc: 0.8954  (best train acc: 0.7716, best val acc: 0.9099, best train loss: 0.5402  @ epoch 12814 )\n",
      "[Epoch: 13500] train loss: 0.5566, train acc: 0.7537, val loss: 0.2913, val acc: 0.9005  (best train acc: 0.7716, best val acc: 0.9099, best train loss: 0.5402  @ epoch 12814 )\n",
      "[Epoch: 13520] train loss: 0.5565, train acc: 0.7571, val loss: 0.2997, val acc: 0.9025  (best train acc: 0.7716, best val acc: 0.9099, best train loss: 0.5399  @ epoch 13503 )\n",
      "[Epoch: 13540] train loss: 0.5562, train acc: 0.7629, val loss: 0.3031, val acc: 0.8880  (best train acc: 0.7716, best val acc: 0.9099, best train loss: 0.5399  @ epoch 13503 )\n",
      "[Epoch: 13560] train loss: 0.5875, train acc: 0.7505, val loss: 0.3075, val acc: 0.8968  (best train acc: 0.7716, best val acc: 0.9099, best train loss: 0.5399  @ epoch 13503 )\n",
      "[Epoch: 13580] train loss: 0.5596, train acc: 0.7590, val loss: 0.2859, val acc: 0.9025  (best train acc: 0.7716, best val acc: 0.9099, best train loss: 0.5399  @ epoch 13503 )\n",
      "[Epoch: 13600] train loss: 0.5567, train acc: 0.7600, val loss: 0.2865, val acc: 0.9035  (best train acc: 0.7716, best val acc: 0.9099, best train loss: 0.5399  @ epoch 13503 )\n",
      "[Epoch: 13620] train loss: 0.5655, train acc: 0.7605, val loss: 0.2904, val acc: 0.9066  (best train acc: 0.7716, best val acc: 0.9099, best train loss: 0.5399  @ epoch 13503 )\n",
      "[Epoch: 13640] train loss: 0.5559, train acc: 0.7621, val loss: 0.2933, val acc: 0.9025  (best train acc: 0.7716, best val acc: 0.9099, best train loss: 0.5399  @ epoch 13503 )\n",
      "[Epoch: 13660] train loss: 0.5662, train acc: 0.7466, val loss: 0.3062, val acc: 0.8978  (best train acc: 0.7716, best val acc: 0.9099, best train loss: 0.5399  @ epoch 13503 )\n",
      "[Epoch: 13680] train loss: 0.5624, train acc: 0.7596, val loss: 0.2926, val acc: 0.9019  (best train acc: 0.7716, best val acc: 0.9099, best train loss: 0.5399  @ epoch 13503 )\n",
      "[Epoch: 13700] train loss: 0.5731, train acc: 0.7465, val loss: 0.2861, val acc: 0.9052  (best train acc: 0.7716, best val acc: 0.9099, best train loss: 0.5399  @ epoch 13503 )\n",
      "[Epoch: 13720] train loss: 0.5628, train acc: 0.7548, val loss: 0.2935, val acc: 0.9025  (best train acc: 0.7716, best val acc: 0.9099, best train loss: 0.5399  @ epoch 13503 )\n",
      "[Epoch: 13740] train loss: 0.5574, train acc: 0.7552, val loss: 0.2853, val acc: 0.9019  (best train acc: 0.7716, best val acc: 0.9099, best train loss: 0.5399  @ epoch 13503 )\n",
      "[Epoch: 13760] train loss: 0.5452, train acc: 0.7716, val loss: 0.2942, val acc: 0.8995  (best train acc: 0.7716, best val acc: 0.9099, best train loss: 0.5399  @ epoch 13503 )\n",
      "[Epoch: 13780] train loss: 0.5662, train acc: 0.7527, val loss: 0.2881, val acc: 0.8975  (best train acc: 0.7716, best val acc: 0.9099, best train loss: 0.5399  @ epoch 13503 )\n",
      "[Epoch: 13800] train loss: 0.5748, train acc: 0.7544, val loss: 0.3066, val acc: 0.8890  (best train acc: 0.7716, best val acc: 0.9099, best train loss: 0.5399  @ epoch 13503 )\n",
      "[Epoch: 13820] train loss: 0.5621, train acc: 0.7637, val loss: 0.3031, val acc: 0.8907  (best train acc: 0.7716, best val acc: 0.9099, best train loss: 0.5399  @ epoch 13503 )\n",
      "[Epoch: 13840] train loss: 0.5699, train acc: 0.7546, val loss: 0.2867, val acc: 0.9046  (best train acc: 0.7716, best val acc: 0.9099, best train loss: 0.5399  @ epoch 13503 )\n",
      "[Epoch: 13860] train loss: 0.5634, train acc: 0.7584, val loss: 0.2989, val acc: 0.9032  (best train acc: 0.7716, best val acc: 0.9099, best train loss: 0.5399  @ epoch 13503 )\n",
      "[Epoch: 13880] train loss: 0.5508, train acc: 0.7616, val loss: 0.2902, val acc: 0.9049  (best train acc: 0.7748, best val acc: 0.9099, best train loss: 0.5309  @ epoch 13872 )\n",
      "[Epoch: 13900] train loss: 0.5472, train acc: 0.7642, val loss: 0.3005, val acc: 0.8968  (best train acc: 0.7748, best val acc: 0.9099, best train loss: 0.5309  @ epoch 13872 )\n",
      "[Epoch: 13920] train loss: 0.5428, train acc: 0.7697, val loss: 0.2872, val acc: 0.9012  (best train acc: 0.7748, best val acc: 0.9099, best train loss: 0.5309  @ epoch 13872 )\n",
      "[Epoch: 13940] train loss: 0.5543, train acc: 0.7619, val loss: 0.2995, val acc: 0.8934  (best train acc: 0.7748, best val acc: 0.9099, best train loss: 0.5309  @ epoch 13872 )\n",
      "[Epoch: 13960] train loss: 0.5517, train acc: 0.7600, val loss: 0.2925, val acc: 0.8995  (best train acc: 0.7748, best val acc: 0.9099, best train loss: 0.5309  @ epoch 13872 )\n",
      "[Epoch: 13980] train loss: 0.5796, train acc: 0.7513, val loss: 0.2915, val acc: 0.9025  (best train acc: 0.7748, best val acc: 0.9099, best train loss: 0.5309  @ epoch 13872 )\n",
      "[Epoch: 14000] train loss: 0.5613, train acc: 0.7548, val loss: 0.3082, val acc: 0.8954  (best train acc: 0.7748, best val acc: 0.9099, best train loss: 0.5309  @ epoch 13872 )\n",
      "[Epoch: 14020] train loss: 0.5524, train acc: 0.7544, val loss: 0.2989, val acc: 0.8958  (best train acc: 0.7748, best val acc: 0.9099, best train loss: 0.5309  @ epoch 13872 )\n",
      "[Epoch: 14040] train loss: 0.5679, train acc: 0.7563, val loss: 0.2910, val acc: 0.8995  (best train acc: 0.7748, best val acc: 0.9099, best train loss: 0.5309  @ epoch 13872 )\n",
      "[Epoch: 14060] train loss: 0.5624, train acc: 0.7527, val loss: 0.2927, val acc: 0.8998  (best train acc: 0.7748, best val acc: 0.9099, best train loss: 0.5309  @ epoch 13872 )\n",
      "[Epoch: 14080] train loss: 0.5551, train acc: 0.7569, val loss: 0.2956, val acc: 0.8985  (best train acc: 0.7748, best val acc: 0.9099, best train loss: 0.5309  @ epoch 13872 )\n",
      "[Epoch: 14100] train loss: 0.5503, train acc: 0.7600, val loss: 0.3038, val acc: 0.8924  (best train acc: 0.7748, best val acc: 0.9099, best train loss: 0.5309  @ epoch 13872 )\n",
      "[Epoch: 14120] train loss: 0.5457, train acc: 0.7558, val loss: 0.3050, val acc: 0.8965  (best train acc: 0.7748, best val acc: 0.9099, best train loss: 0.5253  @ epoch 14114 )\n",
      "[Epoch: 14140] train loss: 0.5348, train acc: 0.7642, val loss: 0.2901, val acc: 0.8921  (best train acc: 0.7748, best val acc: 0.9099, best train loss: 0.5253  @ epoch 14114 )\n",
      "[Epoch: 14160] train loss: 0.5700, train acc: 0.7517, val loss: 0.3098, val acc: 0.8941  (best train acc: 0.7748, best val acc: 0.9099, best train loss: 0.5253  @ epoch 14114 )\n",
      "[Epoch: 14180] train loss: 0.5414, train acc: 0.7707, val loss: 0.2909, val acc: 0.9019  (best train acc: 0.7748, best val acc: 0.9099, best train loss: 0.5253  @ epoch 14114 )\n",
      "[Epoch: 14200] train loss: 0.5395, train acc: 0.7612, val loss: 0.2956, val acc: 0.8988  (best train acc: 0.7748, best val acc: 0.9099, best train loss: 0.5253  @ epoch 14114 )\n",
      "[Epoch: 14220] train loss: 0.5457, train acc: 0.7581, val loss: 0.2895, val acc: 0.9012  (best train acc: 0.7748, best val acc: 0.9099, best train loss: 0.5253  @ epoch 14114 )\n",
      "[Epoch: 14240] train loss: 0.5311, train acc: 0.7630, val loss: 0.2780, val acc: 0.9032  (best train acc: 0.7748, best val acc: 0.9099, best train loss: 0.5253  @ epoch 14114 )\n",
      "[Epoch: 14260] train loss: 0.5522, train acc: 0.7610, val loss: 0.3005, val acc: 0.8971  (best train acc: 0.7748, best val acc: 0.9099, best train loss: 0.5253  @ epoch 14114 )\n",
      "[Epoch: 14280] train loss: 0.5338, train acc: 0.7640, val loss: 0.2840, val acc: 0.9042  (best train acc: 0.7748, best val acc: 0.9099, best train loss: 0.5213  @ epoch 14274 )\n",
      "[Epoch: 14300] train loss: 0.5369, train acc: 0.7592, val loss: 0.2924, val acc: 0.8985  (best train acc: 0.7748, best val acc: 0.9099, best train loss: 0.5213  @ epoch 14274 )\n",
      "[Epoch: 14320] train loss: 0.5628, train acc: 0.7493, val loss: 0.3204, val acc: 0.8985  (best train acc: 0.7748, best val acc: 0.9099, best train loss: 0.5213  @ epoch 14274 )\n",
      "[Epoch: 14340] train loss: 0.5480, train acc: 0.7567, val loss: 0.2793, val acc: 0.9069  (best train acc: 0.7748, best val acc: 0.9099, best train loss: 0.5213  @ epoch 14274 )\n",
      "[Epoch: 14360] train loss: 0.5528, train acc: 0.7509, val loss: 0.2977, val acc: 0.8981  (best train acc: 0.7748, best val acc: 0.9099, best train loss: 0.5213  @ epoch 14274 )\n",
      "[Epoch: 14380] train loss: 0.5253, train acc: 0.7640, val loss: 0.2873, val acc: 0.8971  (best train acc: 0.7748, best val acc: 0.9099, best train loss: 0.5133  @ epoch 14365 )\n",
      "[Epoch: 14400] train loss: 0.5223, train acc: 0.7689, val loss: 0.2894, val acc: 0.9012  (best train acc: 0.7748, best val acc: 0.9099, best train loss: 0.5133  @ epoch 14365 )\n",
      "[Epoch: 14420] train loss: 0.5343, train acc: 0.7621, val loss: 0.2968, val acc: 0.8965  (best train acc: 0.7748, best val acc: 0.9099, best train loss: 0.5133  @ epoch 14365 )\n",
      "[Epoch: 14440] train loss: 0.5465, train acc: 0.7591, val loss: 0.2889, val acc: 0.8978  (best train acc: 0.7748, best val acc: 0.9099, best train loss: 0.5133  @ epoch 14365 )\n",
      "[Epoch: 14460] train loss: 0.5305, train acc: 0.7637, val loss: 0.2977, val acc: 0.9015  (best train acc: 0.7748, best val acc: 0.9099, best train loss: 0.5133  @ epoch 14365 )\n",
      "[Epoch: 14480] train loss: 0.5469, train acc: 0.7519, val loss: 0.2914, val acc: 0.9012  (best train acc: 0.7753, best val acc: 0.9099, best train loss: 0.5133  @ epoch 14365 )\n",
      "[Epoch: 14500] train loss: 0.5328, train acc: 0.7664, val loss: 0.3036, val acc: 0.8911  (best train acc: 0.7753, best val acc: 0.9099, best train loss: 0.5133  @ epoch 14365 )\n",
      "[Epoch: 14520] train loss: 0.5231, train acc: 0.7708, val loss: 0.2954, val acc: 0.8961  (best train acc: 0.7753, best val acc: 0.9099, best train loss: 0.5133  @ epoch 14365 )\n",
      "[Epoch: 14540] train loss: 0.5436, train acc: 0.7567, val loss: 0.2981, val acc: 0.8985  (best train acc: 0.7753, best val acc: 0.9099, best train loss: 0.5133  @ epoch 14365 )\n",
      "[Epoch: 14560] train loss: 0.5499, train acc: 0.7582, val loss: 0.2911, val acc: 0.9032  (best train acc: 0.7753, best val acc: 0.9099, best train loss: 0.5133  @ epoch 14365 )\n",
      "[Epoch: 14580] train loss: 0.5546, train acc: 0.7575, val loss: 0.3000, val acc: 0.8992  (best train acc: 0.7753, best val acc: 0.9099, best train loss: 0.5133  @ epoch 14365 )\n",
      "[Epoch: 14600] train loss: 0.5370, train acc: 0.7607, val loss: 0.2826, val acc: 0.9015  (best train acc: 0.7753, best val acc: 0.9099, best train loss: 0.5133  @ epoch 14365 )\n",
      "[Epoch: 14620] train loss: 0.5411, train acc: 0.7603, val loss: 0.2785, val acc: 0.9059  (best train acc: 0.7753, best val acc: 0.9099, best train loss: 0.5133  @ epoch 14365 )\n",
      "[Epoch: 14640] train loss: 0.5521, train acc: 0.7573, val loss: 0.3003, val acc: 0.9012  (best train acc: 0.7753, best val acc: 0.9099, best train loss: 0.5133  @ epoch 14365 )\n",
      "[Epoch: 14660] train loss: 0.5369, train acc: 0.7682, val loss: 0.3037, val acc: 0.8857  (best train acc: 0.7753, best val acc: 0.9099, best train loss: 0.5133  @ epoch 14365 )\n",
      "[Epoch: 14680] train loss: 0.5355, train acc: 0.7627, val loss: 0.3010, val acc: 0.8948  (best train acc: 0.7753, best val acc: 0.9099, best train loss: 0.5133  @ epoch 14365 )\n",
      "[Epoch: 14700] train loss: 0.5572, train acc: 0.7587, val loss: 0.3117, val acc: 0.8934  (best train acc: 0.7753, best val acc: 0.9099, best train loss: 0.5133  @ epoch 14365 )\n",
      "[Epoch: 14720] train loss: 0.5220, train acc: 0.7657, val loss: 0.3062, val acc: 0.8890  (best train acc: 0.7753, best val acc: 0.9099, best train loss: 0.5133  @ epoch 14365 )\n",
      "[Epoch: 14740] train loss: 0.5263, train acc: 0.7668, val loss: 0.2823, val acc: 0.9022  (best train acc: 0.7753, best val acc: 0.9099, best train loss: 0.5133  @ epoch 14365 )\n",
      "[Epoch: 14760] train loss: 0.5495, train acc: 0.7552, val loss: 0.2923, val acc: 0.8938  (best train acc: 0.7753, best val acc: 0.9099, best train loss: 0.5133  @ epoch 14365 )\n",
      "[Epoch: 14780] train loss: 0.5341, train acc: 0.7608, val loss: 0.2869, val acc: 0.9008  (best train acc: 0.7753, best val acc: 0.9099, best train loss: 0.5133  @ epoch 14365 )\n",
      "[Epoch: 14800] train loss: 0.5160, train acc: 0.7701, val loss: 0.2901, val acc: 0.8992  (best train acc: 0.7753, best val acc: 0.9099, best train loss: 0.5133  @ epoch 14365 )\n",
      "[Epoch: 14820] train loss: 0.5344, train acc: 0.7626, val loss: 0.2917, val acc: 0.8971  (best train acc: 0.7753, best val acc: 0.9099, best train loss: 0.5133  @ epoch 14365 )\n",
      "[Epoch: 14840] train loss: 0.5477, train acc: 0.7579, val loss: 0.2977, val acc: 0.8998  (best train acc: 0.7753, best val acc: 0.9099, best train loss: 0.5133  @ epoch 14365 )\n",
      "[Epoch: 14860] train loss: 0.5460, train acc: 0.7536, val loss: 0.3031, val acc: 0.9019  (best train acc: 0.7753, best val acc: 0.9099, best train loss: 0.5133  @ epoch 14365 )\n",
      "[Epoch: 14880] train loss: 0.5327, train acc: 0.7652, val loss: 0.2890, val acc: 0.9046  (best train acc: 0.7753, best val acc: 0.9099, best train loss: 0.5133  @ epoch 14365 )\n",
      "[Epoch: 14900] train loss: 0.5260, train acc: 0.7676, val loss: 0.2964, val acc: 0.8968  (best train acc: 0.7753, best val acc: 0.9099, best train loss: 0.5133  @ epoch 14365 )\n",
      "[Epoch: 14920] train loss: 0.5417, train acc: 0.7633, val loss: 0.2934, val acc: 0.9005  (best train acc: 0.7753, best val acc: 0.9099, best train loss: 0.5133  @ epoch 14365 )\n",
      "[Epoch: 14940] train loss: 0.5558, train acc: 0.7564, val loss: 0.2905, val acc: 0.8995  (best train acc: 0.7753, best val acc: 0.9099, best train loss: 0.5133  @ epoch 14365 )\n",
      "[Epoch: 14960] train loss: 0.5293, train acc: 0.7698, val loss: 0.2947, val acc: 0.9029  (best train acc: 0.7753, best val acc: 0.9099, best train loss: 0.5133  @ epoch 14365 )\n",
      "[Epoch: 14980] train loss: 0.5136, train acc: 0.7799, val loss: 0.2917, val acc: 0.9049  (best train acc: 0.7799, best val acc: 0.9099, best train loss: 0.5133  @ epoch 14365 )\n",
      "[Epoch: 15000] train loss: 0.5561, train acc: 0.7553, val loss: 0.3003, val acc: 0.8961  (best train acc: 0.7799, best val acc: 0.9099, best train loss: 0.5133  @ epoch 14365 )\n",
      "[Epoch: 15020] train loss: 0.5315, train acc: 0.7690, val loss: 0.2965, val acc: 0.8968  (best train acc: 0.7799, best val acc: 0.9099, best train loss: 0.5133  @ epoch 14365 )\n",
      "[Epoch: 15040] train loss: 0.5371, train acc: 0.7593, val loss: 0.2943, val acc: 0.8981  (best train acc: 0.7799, best val acc: 0.9099, best train loss: 0.5133  @ epoch 14365 )\n",
      "[Epoch: 15060] train loss: 0.5505, train acc: 0.7514, val loss: 0.2928, val acc: 0.8988  (best train acc: 0.7799, best val acc: 0.9099, best train loss: 0.5133  @ epoch 14365 )\n",
      "[Epoch: 15080] train loss: 0.5711, train acc: 0.7361, val loss: 0.3235, val acc: 0.8927  (best train acc: 0.7799, best val acc: 0.9099, best train loss: 0.5133  @ epoch 14365 )\n",
      "[Epoch: 15100] train loss: 0.5502, train acc: 0.7673, val loss: 0.3406, val acc: 0.8890  (best train acc: 0.7799, best val acc: 0.9099, best train loss: 0.5133  @ epoch 14365 )\n",
      "[Epoch: 15120] train loss: 0.5358, train acc: 0.7614, val loss: 0.2872, val acc: 0.9015  (best train acc: 0.7799, best val acc: 0.9099, best train loss: 0.5133  @ epoch 14365 )\n",
      "[Epoch: 15140] train loss: 0.5447, train acc: 0.7599, val loss: 0.2926, val acc: 0.8941  (best train acc: 0.7799, best val acc: 0.9099, best train loss: 0.5133  @ epoch 14365 )\n",
      "[Epoch: 15160] train loss: 0.5260, train acc: 0.7676, val loss: 0.2868, val acc: 0.9025  (best train acc: 0.7799, best val acc: 0.9099, best train loss: 0.5133  @ epoch 14365 )\n",
      "[Epoch: 15180] train loss: 0.5479, train acc: 0.7523, val loss: 0.2961, val acc: 0.9035  (best train acc: 0.7799, best val acc: 0.9099, best train loss: 0.5133  @ epoch 14365 )\n",
      "[Epoch: 15200] train loss: 0.5337, train acc: 0.7608, val loss: 0.2901, val acc: 0.8965  (best train acc: 0.7799, best val acc: 0.9099, best train loss: 0.5133  @ epoch 14365 )\n",
      "[Epoch: 15220] train loss: 0.5393, train acc: 0.7605, val loss: 0.2916, val acc: 0.9025  (best train acc: 0.7799, best val acc: 0.9099, best train loss: 0.5133  @ epoch 14365 )\n",
      "[Epoch: 15240] train loss: 0.5267, train acc: 0.7725, val loss: 0.3052, val acc: 0.8880  (best train acc: 0.7799, best val acc: 0.9099, best train loss: 0.5133  @ epoch 14365 )\n",
      "[Epoch: 15260] train loss: 0.5359, train acc: 0.7559, val loss: 0.2965, val acc: 0.8965  (best train acc: 0.7799, best val acc: 0.9099, best train loss: 0.5133  @ epoch 14365 )\n",
      "[Epoch: 15280] train loss: 0.5347, train acc: 0.7595, val loss: 0.2884, val acc: 0.9019  (best train acc: 0.7799, best val acc: 0.9099, best train loss: 0.5133  @ epoch 14365 )\n",
      "[Epoch: 15300] train loss: 0.5391, train acc: 0.7624, val loss: 0.2858, val acc: 0.9029  (best train acc: 0.7799, best val acc: 0.9099, best train loss: 0.5133  @ epoch 14365 )\n",
      "[Epoch: 15320] train loss: 0.5326, train acc: 0.7646, val loss: 0.2919, val acc: 0.8948  (best train acc: 0.7799, best val acc: 0.9099, best train loss: 0.5133  @ epoch 14365 )\n",
      "[Epoch: 15340] train loss: 0.5404, train acc: 0.7589, val loss: 0.2980, val acc: 0.8978  (best train acc: 0.7799, best val acc: 0.9099, best train loss: 0.5133  @ epoch 14365 )\n",
      "[Epoch: 15360] train loss: 0.5301, train acc: 0.7637, val loss: 0.3002, val acc: 0.8921  (best train acc: 0.7799, best val acc: 0.9099, best train loss: 0.5133  @ epoch 14365 )\n",
      "[Epoch: 15380] train loss: 0.5483, train acc: 0.7593, val loss: 0.3068, val acc: 0.8880  (best train acc: 0.7799, best val acc: 0.9099, best train loss: 0.5125  @ epoch 15374 )\n",
      "[Epoch: 15400] train loss: 0.5254, train acc: 0.7615, val loss: 0.2886, val acc: 0.9002  (best train acc: 0.7799, best val acc: 0.9099, best train loss: 0.5125  @ epoch 15374 )\n",
      "[Epoch: 15420] train loss: 0.5386, train acc: 0.7579, val loss: 0.2911, val acc: 0.9042  (best train acc: 0.7799, best val acc: 0.9099, best train loss: 0.5125  @ epoch 15374 )\n",
      "[Epoch: 15440] train loss: 0.5350, train acc: 0.7587, val loss: 0.2945, val acc: 0.8975  (best train acc: 0.7799, best val acc: 0.9099, best train loss: 0.5125  @ epoch 15374 )\n",
      "[Epoch: 15460] train loss: 0.5348, train acc: 0.7705, val loss: 0.2901, val acc: 0.8978  (best train acc: 0.7799, best val acc: 0.9099, best train loss: 0.5125  @ epoch 15374 )\n",
      "[Epoch: 15480] train loss: 0.5365, train acc: 0.7638, val loss: 0.2953, val acc: 0.8965  (best train acc: 0.7799, best val acc: 0.9099, best train loss: 0.5125  @ epoch 15374 )\n",
      "[Epoch: 15500] train loss: 0.5250, train acc: 0.7681, val loss: 0.3060, val acc: 0.8981  (best train acc: 0.7799, best val acc: 0.9099, best train loss: 0.5125  @ epoch 15374 )\n",
      "[Epoch: 15520] train loss: 0.5362, train acc: 0.7640, val loss: 0.2998, val acc: 0.8992  (best train acc: 0.7799, best val acc: 0.9099, best train loss: 0.5125  @ epoch 15374 )\n",
      "[Epoch: 15540] train loss: 0.5197, train acc: 0.7760, val loss: 0.3024, val acc: 0.8981  (best train acc: 0.7799, best val acc: 0.9099, best train loss: 0.5125  @ epoch 15374 )\n",
      "[Epoch: 15560] train loss: 0.5513, train acc: 0.7577, val loss: 0.2986, val acc: 0.8968  (best train acc: 0.7799, best val acc: 0.9099, best train loss: 0.5125  @ epoch 15374 )\n",
      "[Epoch: 15580] train loss: 0.5376, train acc: 0.7598, val loss: 0.2927, val acc: 0.9042  (best train acc: 0.7799, best val acc: 0.9099, best train loss: 0.5125  @ epoch 15374 )\n",
      "[Epoch: 15600] train loss: 0.5313, train acc: 0.7676, val loss: 0.2936, val acc: 0.9042  (best train acc: 0.7799, best val acc: 0.9099, best train loss: 0.5125  @ epoch 15374 )\n",
      "[Epoch: 15620] train loss: 0.5479, train acc: 0.7570, val loss: 0.3098, val acc: 0.8863  (best train acc: 0.7799, best val acc: 0.9099, best train loss: 0.5125  @ epoch 15374 )\n",
      "[Epoch: 15640] train loss: 0.5262, train acc: 0.7672, val loss: 0.3074, val acc: 0.8978  (best train acc: 0.7799, best val acc: 0.9099, best train loss: 0.5125  @ epoch 15374 )\n",
      "[Epoch: 15660] train loss: 0.5321, train acc: 0.7633, val loss: 0.2992, val acc: 0.8995  (best train acc: 0.7799, best val acc: 0.9099, best train loss: 0.5125  @ epoch 15374 )\n",
      "[Epoch: 15680] train loss: 0.5209, train acc: 0.7701, val loss: 0.2999, val acc: 0.8951  (best train acc: 0.7799, best val acc: 0.9099, best train loss: 0.5125  @ epoch 15374 )\n",
      "[Epoch: 15700] train loss: 0.5350, train acc: 0.7556, val loss: 0.2996, val acc: 0.8934  (best train acc: 0.7799, best val acc: 0.9099, best train loss: 0.5125  @ epoch 15374 )\n",
      "[Epoch: 15720] train loss: 0.5495, train acc: 0.7470, val loss: 0.3027, val acc: 0.9008  (best train acc: 0.7799, best val acc: 0.9099, best train loss: 0.5125  @ epoch 15374 )\n",
      "[Epoch: 15740] train loss: 0.5294, train acc: 0.7637, val loss: 0.3134, val acc: 0.8985  (best train acc: 0.7799, best val acc: 0.9099, best train loss: 0.5125  @ epoch 15374 )\n",
      "[Epoch: 15760] train loss: 0.5349, train acc: 0.7634, val loss: 0.3189, val acc: 0.8921  (best train acc: 0.7799, best val acc: 0.9099, best train loss: 0.5125  @ epoch 15374 )\n",
      "[Epoch: 15780] train loss: 0.5238, train acc: 0.7736, val loss: 0.3001, val acc: 0.9008  (best train acc: 0.7799, best val acc: 0.9099, best train loss: 0.5125  @ epoch 15374 )\n",
      "[Epoch: 15800] train loss: 0.5358, train acc: 0.7634, val loss: 0.2931, val acc: 0.9025  (best train acc: 0.7799, best val acc: 0.9099, best train loss: 0.5125  @ epoch 15374 )\n",
      "[Epoch: 15820] train loss: 0.5409, train acc: 0.7621, val loss: 0.2933, val acc: 0.9042  (best train acc: 0.7799, best val acc: 0.9099, best train loss: 0.5075  @ epoch 15810 )\n",
      "[Epoch: 15840] train loss: 0.5422, train acc: 0.7577, val loss: 0.3160, val acc: 0.8954  (best train acc: 0.7799, best val acc: 0.9099, best train loss: 0.5075  @ epoch 15810 )\n",
      "[Epoch: 15860] train loss: 0.5403, train acc: 0.7614, val loss: 0.3052, val acc: 0.8958  (best train acc: 0.7799, best val acc: 0.9106, best train loss: 0.5075  @ epoch 15810 )\n",
      "[Epoch: 15880] train loss: 0.5491, train acc: 0.7524, val loss: 0.2900, val acc: 0.9029  (best train acc: 0.7799, best val acc: 0.9106, best train loss: 0.5075  @ epoch 15810 )\n",
      "[Epoch: 15900] train loss: 0.5343, train acc: 0.7719, val loss: 0.2881, val acc: 0.9056  (best train acc: 0.7799, best val acc: 0.9106, best train loss: 0.5075  @ epoch 15810 )\n",
      "[Epoch: 15920] train loss: 0.5291, train acc: 0.7629, val loss: 0.2895, val acc: 0.9025  (best train acc: 0.7799, best val acc: 0.9106, best train loss: 0.5075  @ epoch 15810 )\n",
      "[Epoch: 15940] train loss: 0.5478, train acc: 0.7518, val loss: 0.2852, val acc: 0.9042  (best train acc: 0.7799, best val acc: 0.9106, best train loss: 0.5075  @ epoch 15810 )\n",
      "[Epoch: 15960] train loss: 0.5442, train acc: 0.7625, val loss: 0.2893, val acc: 0.9012  (best train acc: 0.7799, best val acc: 0.9106, best train loss: 0.5075  @ epoch 15810 )\n",
      "[Epoch: 15980] train loss: 0.5266, train acc: 0.7664, val loss: 0.3095, val acc: 0.8961  (best train acc: 0.7799, best val acc: 0.9106, best train loss: 0.5075  @ epoch 15810 )\n",
      "[Epoch: 16000] train loss: 0.5524, train acc: 0.7481, val loss: 0.2956, val acc: 0.8971  (best train acc: 0.7799, best val acc: 0.9106, best train loss: 0.5075  @ epoch 15810 )\n",
      "[Epoch: 16020] train loss: 0.5370, train acc: 0.7613, val loss: 0.3038, val acc: 0.8992  (best train acc: 0.7799, best val acc: 0.9106, best train loss: 0.5075  @ epoch 15810 )\n",
      "[Epoch: 16040] train loss: 0.5358, train acc: 0.7622, val loss: 0.2913, val acc: 0.9076  (best train acc: 0.7799, best val acc: 0.9106, best train loss: 0.5075  @ epoch 15810 )\n",
      "[Epoch: 16060] train loss: 0.5472, train acc: 0.7512, val loss: 0.3333, val acc: 0.8914  (best train acc: 0.7799, best val acc: 0.9106, best train loss: 0.5075  @ epoch 15810 )\n",
      "[Epoch: 16080] train loss: 0.5422, train acc: 0.7619, val loss: 0.2893, val acc: 0.9083  (best train acc: 0.7799, best val acc: 0.9106, best train loss: 0.5075  @ epoch 15810 )\n",
      "[Epoch: 16100] train loss: 0.5392, train acc: 0.7560, val loss: 0.3011, val acc: 0.9002  (best train acc: 0.7799, best val acc: 0.9106, best train loss: 0.5075  @ epoch 15810 )\n",
      "[Epoch: 16120] train loss: 0.5204, train acc: 0.7643, val loss: 0.3082, val acc: 0.8971  (best train acc: 0.7799, best val acc: 0.9106, best train loss: 0.5075  @ epoch 15810 )\n",
      "[Epoch: 16140] train loss: 0.5392, train acc: 0.7606, val loss: 0.3029, val acc: 0.8917  (best train acc: 0.7809, best val acc: 0.9106, best train loss: 0.5070  @ epoch 16139 )\n",
      "[Epoch: 16160] train loss: 0.5322, train acc: 0.7641, val loss: 0.3213, val acc: 0.8944  (best train acc: 0.7809, best val acc: 0.9106, best train loss: 0.5070  @ epoch 16139 )\n",
      "[Epoch: 16180] train loss: 0.5175, train acc: 0.7674, val loss: 0.3004, val acc: 0.9012  (best train acc: 0.7809, best val acc: 0.9106, best train loss: 0.5070  @ epoch 16139 )\n",
      "[Epoch: 16200] train loss: 0.5285, train acc: 0.7637, val loss: 0.3033, val acc: 0.8968  (best train acc: 0.7809, best val acc: 0.9106, best train loss: 0.5070  @ epoch 16139 )\n",
      "[Epoch: 16220] train loss: 0.5195, train acc: 0.7693, val loss: 0.3182, val acc: 0.8951  (best train acc: 0.7809, best val acc: 0.9106, best train loss: 0.5070  @ epoch 16139 )\n",
      "[Epoch: 16240] train loss: 0.5218, train acc: 0.7692, val loss: 0.2940, val acc: 0.8968  (best train acc: 0.7809, best val acc: 0.9106, best train loss: 0.5070  @ epoch 16139 )\n",
      "[Epoch: 16260] train loss: 0.5408, train acc: 0.7647, val loss: 0.3103, val acc: 0.8927  (best train acc: 0.7809, best val acc: 0.9106, best train loss: 0.5070  @ epoch 16139 )\n",
      "[Epoch: 16280] train loss: 0.5419, train acc: 0.7565, val loss: 0.3055, val acc: 0.8927  (best train acc: 0.7809, best val acc: 0.9106, best train loss: 0.5070  @ epoch 16139 )\n",
      "[Epoch: 16300] train loss: 0.5188, train acc: 0.7691, val loss: 0.2866, val acc: 0.9076  (best train acc: 0.7809, best val acc: 0.9106, best train loss: 0.5070  @ epoch 16139 )\n",
      "[Epoch: 16320] train loss: 0.5361, train acc: 0.7647, val loss: 0.2981, val acc: 0.8998  (best train acc: 0.7809, best val acc: 0.9106, best train loss: 0.5070  @ epoch 16139 )\n",
      "[Epoch: 16340] train loss: 0.5288, train acc: 0.7633, val loss: 0.3036, val acc: 0.9052  (best train acc: 0.7809, best val acc: 0.9106, best train loss: 0.5070  @ epoch 16139 )\n",
      "[Epoch: 16360] train loss: 0.5323, train acc: 0.7600, val loss: 0.3154, val acc: 0.8948  (best train acc: 0.7809, best val acc: 0.9106, best train loss: 0.5070  @ epoch 16139 )\n",
      "[Epoch: 16380] train loss: 0.5267, train acc: 0.7689, val loss: 0.3140, val acc: 0.8749  (best train acc: 0.7809, best val acc: 0.9106, best train loss: 0.5070  @ epoch 16139 )\n",
      "[Epoch: 16400] train loss: 0.5429, train acc: 0.7594, val loss: 0.2951, val acc: 0.8988  (best train acc: 0.7809, best val acc: 0.9106, best train loss: 0.5070  @ epoch 16139 )\n",
      "[Epoch: 16420] train loss: 0.5263, train acc: 0.7676, val loss: 0.3081, val acc: 0.8958  (best train acc: 0.7809, best val acc: 0.9106, best train loss: 0.5070  @ epoch 16139 )\n",
      "[Epoch: 16440] train loss: 0.5354, train acc: 0.7605, val loss: 0.2999, val acc: 0.9022  (best train acc: 0.7809, best val acc: 0.9106, best train loss: 0.5070  @ epoch 16139 )\n",
      "[Epoch: 16460] train loss: 0.5211, train acc: 0.7618, val loss: 0.3066, val acc: 0.8995  (best train acc: 0.7809, best val acc: 0.9106, best train loss: 0.5070  @ epoch 16139 )\n",
      "[Epoch: 16480] train loss: 0.5289, train acc: 0.7674, val loss: 0.3069, val acc: 0.9029  (best train acc: 0.7809, best val acc: 0.9106, best train loss: 0.5070  @ epoch 16139 )\n",
      "[Epoch: 16500] train loss: 0.5342, train acc: 0.7598, val loss: 0.3181, val acc: 0.9015  (best train acc: 0.7809, best val acc: 0.9106, best train loss: 0.5070  @ epoch 16139 )\n",
      "[Epoch: 16520] train loss: 0.5253, train acc: 0.7641, val loss: 0.3030, val acc: 0.9086  (best train acc: 0.7809, best val acc: 0.9106, best train loss: 0.5070  @ epoch 16139 )\n",
      "[Epoch: 16540] train loss: 0.5603, train acc: 0.7446, val loss: 0.3177, val acc: 0.8840  (best train acc: 0.7809, best val acc: 0.9106, best train loss: 0.5028  @ epoch 16531 )\n",
      "[Epoch: 16560] train loss: 0.5202, train acc: 0.7629, val loss: 0.3038, val acc: 0.8998  (best train acc: 0.7809, best val acc: 0.9106, best train loss: 0.5028  @ epoch 16531 )\n",
      "[Epoch: 16580] train loss: 0.5285, train acc: 0.7654, val loss: 0.2951, val acc: 0.8988  (best train acc: 0.7809, best val acc: 0.9106, best train loss: 0.5028  @ epoch 16531 )\n",
      "[Epoch: 16600] train loss: 0.5178, train acc: 0.7667, val loss: 0.2895, val acc: 0.9046  (best train acc: 0.7809, best val acc: 0.9106, best train loss: 0.5017  @ epoch 16596 )\n",
      "[Epoch: 16620] train loss: 0.5138, train acc: 0.7672, val loss: 0.2994, val acc: 0.9015  (best train acc: 0.7809, best val acc: 0.9106, best train loss: 0.5017  @ epoch 16596 )\n",
      "[Epoch: 16640] train loss: 0.5158, train acc: 0.7631, val loss: 0.2975, val acc: 0.9029  (best train acc: 0.7809, best val acc: 0.9106, best train loss: 0.5017  @ epoch 16596 )\n",
      "[Epoch: 16660] train loss: 0.5187, train acc: 0.7603, val loss: 0.3073, val acc: 0.8961  (best train acc: 0.7809, best val acc: 0.9106, best train loss: 0.5017  @ epoch 16596 )\n",
      "[Epoch: 16680] train loss: 0.5315, train acc: 0.7645, val loss: 0.3094, val acc: 0.8958  (best train acc: 0.7809, best val acc: 0.9106, best train loss: 0.5017  @ epoch 16596 )\n",
      "[Epoch: 16700] train loss: 0.5152, train acc: 0.7704, val loss: 0.3111, val acc: 0.8921  (best train acc: 0.7809, best val acc: 0.9106, best train loss: 0.5017  @ epoch 16596 )\n",
      "[Epoch: 16720] train loss: 0.5303, train acc: 0.7588, val loss: 0.3101, val acc: 0.8934  (best train acc: 0.7809, best val acc: 0.9106, best train loss: 0.5017  @ epoch 16596 )\n",
      "[Epoch: 16740] train loss: 0.5216, train acc: 0.7649, val loss: 0.3241, val acc: 0.8877  (best train acc: 0.7809, best val acc: 0.9106, best train loss: 0.5017  @ epoch 16596 )\n",
      "[Epoch: 16760] train loss: 0.5362, train acc: 0.7587, val loss: 0.2959, val acc: 0.8874  (best train acc: 0.7809, best val acc: 0.9106, best train loss: 0.5017  @ epoch 16596 )\n",
      "[Epoch: 16780] train loss: 0.5175, train acc: 0.7697, val loss: 0.2944, val acc: 0.9076  (best train acc: 0.7809, best val acc: 0.9106, best train loss: 0.5017  @ epoch 16596 )\n",
      "[Epoch: 16800] train loss: 0.5169, train acc: 0.7710, val loss: 0.3001, val acc: 0.9002  (best train acc: 0.7809, best val acc: 0.9106, best train loss: 0.4956  @ epoch 16784 )\n",
      "[Epoch: 16820] train loss: 0.5260, train acc: 0.7642, val loss: 0.2963, val acc: 0.9025  (best train acc: 0.7809, best val acc: 0.9106, best train loss: 0.4956  @ epoch 16784 )\n",
      "[Epoch: 16840] train loss: 0.5312, train acc: 0.7603, val loss: 0.3085, val acc: 0.8971  (best train acc: 0.7809, best val acc: 0.9106, best train loss: 0.4956  @ epoch 16784 )\n",
      "[Epoch: 16860] train loss: 0.5136, train acc: 0.7691, val loss: 0.2997, val acc: 0.9015  (best train acc: 0.7809, best val acc: 0.9106, best train loss: 0.4956  @ epoch 16784 )\n",
      "[Epoch: 16880] train loss: 0.5261, train acc: 0.7611, val loss: 0.3077, val acc: 0.8995  (best train acc: 0.7809, best val acc: 0.9106, best train loss: 0.4956  @ epoch 16784 )\n",
      "[Epoch: 16900] train loss: 0.5205, train acc: 0.7593, val loss: 0.2886, val acc: 0.9099  (best train acc: 0.7813, best val acc: 0.9106, best train loss: 0.4956  @ epoch 16784 )\n",
      "[Epoch: 16920] train loss: 0.5104, train acc: 0.7698, val loss: 0.3018, val acc: 0.8924  (best train acc: 0.7813, best val acc: 0.9106, best train loss: 0.4956  @ epoch 16784 )\n",
      "[Epoch: 16940] train loss: 0.5204, train acc: 0.7638, val loss: 0.3075, val acc: 0.8870  (best train acc: 0.7813, best val acc: 0.9106, best train loss: 0.4956  @ epoch 16784 )\n",
      "[Epoch: 16960] train loss: 0.5189, train acc: 0.7671, val loss: 0.3047, val acc: 0.8860  (best train acc: 0.7813, best val acc: 0.9106, best train loss: 0.4956  @ epoch 16784 )\n",
      "[Epoch: 16980] train loss: 0.5269, train acc: 0.7637, val loss: 0.3017, val acc: 0.9049  (best train acc: 0.7813, best val acc: 0.9106, best train loss: 0.4956  @ epoch 16784 )\n",
      "[Epoch: 17000] train loss: 0.5245, train acc: 0.7665, val loss: 0.3033, val acc: 0.9005  (best train acc: 0.7813, best val acc: 0.9106, best train loss: 0.4956  @ epoch 16784 )\n",
      "[Epoch: 17020] train loss: 0.5311, train acc: 0.7628, val loss: 0.3027, val acc: 0.8951  (best train acc: 0.7813, best val acc: 0.9106, best train loss: 0.4956  @ epoch 16784 )\n",
      "[Epoch: 17040] train loss: 0.5107, train acc: 0.7681, val loss: 0.2889, val acc: 0.9025  (best train acc: 0.7813, best val acc: 0.9106, best train loss: 0.4956  @ epoch 16784 )\n",
      "[Epoch: 17060] train loss: 0.5148, train acc: 0.7676, val loss: 0.2971, val acc: 0.8934  (best train acc: 0.7813, best val acc: 0.9106, best train loss: 0.4956  @ epoch 16784 )\n",
      "[Epoch: 17080] train loss: 0.5211, train acc: 0.7624, val loss: 0.3014, val acc: 0.8975  (best train acc: 0.7813, best val acc: 0.9106, best train loss: 0.4956  @ epoch 16784 )\n",
      "[Epoch: 17100] train loss: 0.5255, train acc: 0.7679, val loss: 0.2935, val acc: 0.9015  (best train acc: 0.7813, best val acc: 0.9106, best train loss: 0.4956  @ epoch 16784 )\n",
      "[Epoch: 17120] train loss: 0.5246, train acc: 0.7600, val loss: 0.3145, val acc: 0.8762  (best train acc: 0.7813, best val acc: 0.9106, best train loss: 0.4956  @ epoch 16784 )\n",
      "[Epoch: 17140] train loss: 0.5376, train acc: 0.7559, val loss: 0.3127, val acc: 0.8938  (best train acc: 0.7813, best val acc: 0.9106, best train loss: 0.4956  @ epoch 16784 )\n",
      "[Epoch: 17160] train loss: 0.5163, train acc: 0.7616, val loss: 0.3056, val acc: 0.8954  (best train acc: 0.7813, best val acc: 0.9106, best train loss: 0.4956  @ epoch 16784 )\n",
      "[Epoch: 17180] train loss: 0.5225, train acc: 0.7588, val loss: 0.2939, val acc: 0.8988  (best train acc: 0.7813, best val acc: 0.9106, best train loss: 0.4956  @ epoch 16784 )\n",
      "[Epoch: 17200] train loss: 0.5243, train acc: 0.7627, val loss: 0.3108, val acc: 0.8992  (best train acc: 0.7813, best val acc: 0.9106, best train loss: 0.4956  @ epoch 16784 )\n",
      "[Epoch: 17220] train loss: 0.5334, train acc: 0.7499, val loss: 0.2890, val acc: 0.9022  (best train acc: 0.7813, best val acc: 0.9106, best train loss: 0.4956  @ epoch 16784 )\n",
      "[Epoch: 17240] train loss: 0.5248, train acc: 0.7637, val loss: 0.2967, val acc: 0.9029  (best train acc: 0.7813, best val acc: 0.9106, best train loss: 0.4956  @ epoch 16784 )\n",
      "[Epoch: 17260] train loss: 0.5191, train acc: 0.7681, val loss: 0.3000, val acc: 0.8954  (best train acc: 0.7813, best val acc: 0.9106, best train loss: 0.4956  @ epoch 16784 )\n",
      "[Epoch: 17280] train loss: 0.5209, train acc: 0.7672, val loss: 0.3012, val acc: 0.9069  (best train acc: 0.7813, best val acc: 0.9106, best train loss: 0.4956  @ epoch 16784 )\n",
      "[Epoch: 17300] train loss: 0.5195, train acc: 0.7661, val loss: 0.3112, val acc: 0.8985  (best train acc: 0.7813, best val acc: 0.9106, best train loss: 0.4956  @ epoch 16784 )\n",
      "[Epoch: 17320] train loss: 0.5196, train acc: 0.7704, val loss: 0.3039, val acc: 0.8894  (best train acc: 0.7813, best val acc: 0.9106, best train loss: 0.4956  @ epoch 16784 )\n",
      "[Epoch: 17340] train loss: 0.5436, train acc: 0.7600, val loss: 0.3106, val acc: 0.8981  (best train acc: 0.7813, best val acc: 0.9106, best train loss: 0.4956  @ epoch 16784 )\n",
      "[Epoch: 17360] train loss: 0.5077, train acc: 0.7727, val loss: 0.2956, val acc: 0.9015  (best train acc: 0.7813, best val acc: 0.9106, best train loss: 0.4956  @ epoch 16784 )\n",
      "[Epoch: 17380] train loss: 0.5147, train acc: 0.7711, val loss: 0.2922, val acc: 0.9073  (best train acc: 0.7813, best val acc: 0.9106, best train loss: 0.4956  @ epoch 16784 )\n",
      "[Epoch: 17400] train loss: 0.4981, train acc: 0.7968, val loss: 0.3141, val acc: 0.8944  (best train acc: 0.7968, best val acc: 0.9106, best train loss: 0.4872  @ epoch 17397 )\n",
      "[Epoch: 17420] train loss: 0.4814, train acc: 0.8002, val loss: 0.3084, val acc: 0.8975  (best train acc: 0.8002, best val acc: 0.9106, best train loss: 0.4807  @ epoch 17409 )\n",
      "[Epoch: 17440] train loss: 0.4887, train acc: 0.7950, val loss: 0.2887, val acc: 0.9015  (best train acc: 0.8111, best val acc: 0.9130, best train loss: 0.4598  @ epoch 17435 )\n",
      "[Epoch: 17460] train loss: 0.4787, train acc: 0.7972, val loss: 0.3004, val acc: 0.8847  (best train acc: 0.8119, best val acc: 0.9130, best train loss: 0.4580  @ epoch 17445 )\n",
      "[Epoch: 17480] train loss: 0.4695, train acc: 0.8078, val loss: 0.2860, val acc: 0.9083  (best train acc: 0.8119, best val acc: 0.9130, best train loss: 0.4580  @ epoch 17445 )\n",
      "[Epoch: 17500] train loss: 0.4703, train acc: 0.8057, val loss: 0.2936, val acc: 0.9046  (best train acc: 0.8147, best val acc: 0.9130, best train loss: 0.4555  @ epoch 17491 )\n",
      "[Epoch: 17520] train loss: 0.4825, train acc: 0.7979, val loss: 0.2963, val acc: 0.9005  (best train acc: 0.8173, best val acc: 0.9130, best train loss: 0.4543  @ epoch 17517 )\n",
      "[Epoch: 17540] train loss: 0.4779, train acc: 0.8010, val loss: 0.2890, val acc: 0.8968  (best train acc: 0.8173, best val acc: 0.9130, best train loss: 0.4543  @ epoch 17517 )\n",
      "[Epoch: 17560] train loss: 0.4743, train acc: 0.8039, val loss: 0.2961, val acc: 0.9035  (best train acc: 0.8173, best val acc: 0.9130, best train loss: 0.4543  @ epoch 17517 )\n",
      "[Epoch: 17580] train loss: 0.4597, train acc: 0.8088, val loss: 0.2930, val acc: 0.9049  (best train acc: 0.8199, best val acc: 0.9130, best train loss: 0.4496  @ epoch 17571 )\n",
      "[Epoch: 17600] train loss: 0.4604, train acc: 0.8144, val loss: 0.2744, val acc: 0.9059  (best train acc: 0.8240, best val acc: 0.9130, best train loss: 0.4468  @ epoch 17586 )\n",
      "[Epoch: 17620] train loss: 0.5631, train acc: 0.7699, val loss: 0.3693, val acc: 0.8809  (best train acc: 0.8240, best val acc: 0.9130, best train loss: 0.4468  @ epoch 17586 )\n",
      "[Epoch: 17640] train loss: 0.4756, train acc: 0.8037, val loss: 0.3180, val acc: 0.8978  (best train acc: 0.8240, best val acc: 0.9130, best train loss: 0.4468  @ epoch 17586 )\n",
      "[Epoch: 17660] train loss: 0.4704, train acc: 0.8003, val loss: 0.3065, val acc: 0.9012  (best train acc: 0.8240, best val acc: 0.9130, best train loss: 0.4468  @ epoch 17586 )\n",
      "[Epoch: 17680] train loss: 0.4661, train acc: 0.8080, val loss: 0.2926, val acc: 0.9062  (best train acc: 0.8240, best val acc: 0.9130, best train loss: 0.4468  @ epoch 17586 )\n",
      "[Epoch: 17700] train loss: 0.4612, train acc: 0.8065, val loss: 0.3015, val acc: 0.9008  (best train acc: 0.8240, best val acc: 0.9130, best train loss: 0.4468  @ epoch 17586 )\n",
      "[Epoch: 17720] train loss: 0.4505, train acc: 0.8131, val loss: 0.2961, val acc: 0.9008  (best train acc: 0.8240, best val acc: 0.9130, best train loss: 0.4468  @ epoch 17586 )\n",
      "[Epoch: 17740] train loss: 0.4505, train acc: 0.8124, val loss: 0.2890, val acc: 0.9025  (best train acc: 0.8240, best val acc: 0.9130, best train loss: 0.4468  @ epoch 17586 )\n",
      "[Epoch: 17760] train loss: 0.4863, train acc: 0.7900, val loss: 0.3048, val acc: 0.8971  (best train acc: 0.8240, best val acc: 0.9130, best train loss: 0.4468  @ epoch 17586 )\n",
      "[Epoch: 17780] train loss: 0.4658, train acc: 0.8026, val loss: 0.3073, val acc: 0.8948  (best train acc: 0.8240, best val acc: 0.9130, best train loss: 0.4468  @ epoch 17586 )\n",
      "[Epoch: 17800] train loss: 0.4580, train acc: 0.8104, val loss: 0.3035, val acc: 0.8934  (best train acc: 0.8240, best val acc: 0.9130, best train loss: 0.4468  @ epoch 17586 )\n",
      "[Epoch: 17820] train loss: 0.4585, train acc: 0.8109, val loss: 0.2883, val acc: 0.9052  (best train acc: 0.8240, best val acc: 0.9130, best train loss: 0.4430  @ epoch 17813 )\n",
      "[Epoch: 17840] train loss: 0.4596, train acc: 0.8058, val loss: 0.2781, val acc: 0.9022  (best train acc: 0.8240, best val acc: 0.9130, best train loss: 0.4430  @ epoch 17813 )\n",
      "[Epoch: 17860] train loss: 0.4643, train acc: 0.8082, val loss: 0.2821, val acc: 0.9022  (best train acc: 0.8240, best val acc: 0.9130, best train loss: 0.4419  @ epoch 17857 )\n",
      "[Epoch: 17880] train loss: 0.4737, train acc: 0.7990, val loss: 0.2911, val acc: 0.9019  (best train acc: 0.8240, best val acc: 0.9130, best train loss: 0.4419  @ epoch 17857 )\n",
      "[Epoch: 17900] train loss: 0.4594, train acc: 0.8138, val loss: 0.2774, val acc: 0.9066  (best train acc: 0.8240, best val acc: 0.9130, best train loss: 0.4419  @ epoch 17857 )\n",
      "[Epoch: 17920] train loss: 0.4566, train acc: 0.8106, val loss: 0.2881, val acc: 0.8941  (best train acc: 0.8240, best val acc: 0.9140, best train loss: 0.4419  @ epoch 17857 )\n",
      "[Epoch: 17940] train loss: 0.4609, train acc: 0.8075, val loss: 0.2787, val acc: 0.9042  (best train acc: 0.8240, best val acc: 0.9140, best train loss: 0.4396  @ epoch 17927 )\n",
      "[Epoch: 17960] train loss: 0.4526, train acc: 0.8169, val loss: 0.2809, val acc: 0.9029  (best train acc: 0.8240, best val acc: 0.9140, best train loss: 0.4389  @ epoch 17957 )\n",
      "[Epoch: 17980] train loss: 0.4497, train acc: 0.8133, val loss: 0.2752, val acc: 0.9046  (best train acc: 0.8240, best val acc: 0.9140, best train loss: 0.4389  @ epoch 17957 )\n",
      "[Epoch: 18000] train loss: 0.4551, train acc: 0.8125, val loss: 0.2795, val acc: 0.9110  (best train acc: 0.8240, best val acc: 0.9140, best train loss: 0.4389  @ epoch 17957 )\n",
      "[Epoch: 18020] train loss: 0.4487, train acc: 0.8120, val loss: 0.2739, val acc: 0.9083  (best train acc: 0.8240, best val acc: 0.9140, best train loss: 0.4389  @ epoch 17957 )\n",
      "[Epoch: 18040] train loss: 0.4684, train acc: 0.8086, val loss: 0.2910, val acc: 0.8880  (best train acc: 0.8240, best val acc: 0.9147, best train loss: 0.4389  @ epoch 17957 )\n",
      "[Epoch: 18060] train loss: 0.4565, train acc: 0.8120, val loss: 0.2814, val acc: 0.9116  (best train acc: 0.8240, best val acc: 0.9147, best train loss: 0.4389  @ epoch 17957 )\n",
      "[Epoch: 18080] train loss: 0.4775, train acc: 0.8078, val loss: 0.3117, val acc: 0.8816  (best train acc: 0.8240, best val acc: 0.9157, best train loss: 0.4389  @ epoch 17957 )\n",
      "[Epoch: 18100] train loss: 0.4568, train acc: 0.8047, val loss: 0.2857, val acc: 0.9022  (best train acc: 0.8242, best val acc: 0.9157, best train loss: 0.4356  @ epoch 18098 )\n",
      "[Epoch: 18120] train loss: 0.4521, train acc: 0.8122, val loss: 0.2778, val acc: 0.9096  (best train acc: 0.8242, best val acc: 0.9157, best train loss: 0.4356  @ epoch 18098 )\n",
      "[Epoch: 18140] train loss: 0.4416, train acc: 0.8172, val loss: 0.2655, val acc: 0.9116  (best train acc: 0.8242, best val acc: 0.9157, best train loss: 0.4356  @ epoch 18098 )\n",
      "[Epoch: 18160] train loss: 0.4788, train acc: 0.8039, val loss: 0.3242, val acc: 0.8806  (best train acc: 0.8242, best val acc: 0.9157, best train loss: 0.4356  @ epoch 18098 )\n",
      "[Epoch: 18180] train loss: 0.4686, train acc: 0.8056, val loss: 0.3146, val acc: 0.8894  (best train acc: 0.8242, best val acc: 0.9157, best train loss: 0.4356  @ epoch 18098 )\n",
      "[Epoch: 18200] train loss: 0.4483, train acc: 0.8133, val loss: 0.2799, val acc: 0.9116  (best train acc: 0.8242, best val acc: 0.9157, best train loss: 0.4356  @ epoch 18098 )\n",
      "[Epoch: 18220] train loss: 0.4476, train acc: 0.8154, val loss: 0.2815, val acc: 0.9089  (best train acc: 0.8242, best val acc: 0.9157, best train loss: 0.4356  @ epoch 18098 )\n",
      "[Epoch: 18240] train loss: 0.4464, train acc: 0.8201, val loss: 0.2822, val acc: 0.9073  (best train acc: 0.8257, best val acc: 0.9177, best train loss: 0.4356  @ epoch 18098 )\n",
      "[Epoch: 18260] train loss: 0.4718, train acc: 0.7974, val loss: 0.3199, val acc: 0.8806  (best train acc: 0.8257, best val acc: 0.9177, best train loss: 0.4356  @ epoch 18098 )\n",
      "[Epoch: 18280] train loss: 0.4582, train acc: 0.8104, val loss: 0.2848, val acc: 0.9106  (best train acc: 0.8257, best val acc: 0.9177, best train loss: 0.4356  @ epoch 18098 )\n",
      "[Epoch: 18300] train loss: 0.4673, train acc: 0.8004, val loss: 0.2988, val acc: 0.8927  (best train acc: 0.8257, best val acc: 0.9177, best train loss: 0.4356  @ epoch 18098 )\n",
      "[Epoch: 18320] train loss: 0.4552, train acc: 0.8100, val loss: 0.2926, val acc: 0.9012  (best train acc: 0.8261, best val acc: 0.9177, best train loss: 0.4356  @ epoch 18098 )\n",
      "[Epoch: 18340] train loss: 0.4436, train acc: 0.8143, val loss: 0.2782, val acc: 0.9160  (best train acc: 0.8278, best val acc: 0.9191, best train loss: 0.4336  @ epoch 18326 )\n",
      "[Epoch: 18360] train loss: 0.4460, train acc: 0.8151, val loss: 0.2868, val acc: 0.9046  (best train acc: 0.8278, best val acc: 0.9191, best train loss: 0.4328  @ epoch 18359 )\n",
      "[Epoch: 18380] train loss: 0.4710, train acc: 0.7995, val loss: 0.3096, val acc: 0.8921  (best train acc: 0.8278, best val acc: 0.9191, best train loss: 0.4320  @ epoch 18368 )\n",
      "[Epoch: 18400] train loss: 0.4401, train acc: 0.8172, val loss: 0.2708, val acc: 0.9110  (best train acc: 0.8278, best val acc: 0.9191, best train loss: 0.4320  @ epoch 18368 )\n",
      "[Epoch: 18420] train loss: 0.4708, train acc: 0.8028, val loss: 0.3139, val acc: 0.8877  (best train acc: 0.8278, best val acc: 0.9191, best train loss: 0.4283  @ epoch 18402 )\n",
      "[Epoch: 18440] train loss: 0.4494, train acc: 0.8137, val loss: 0.2966, val acc: 0.9008  (best train acc: 0.8278, best val acc: 0.9191, best train loss: 0.4283  @ epoch 18402 )\n",
      "[Epoch: 18460] train loss: 0.4408, train acc: 0.8177, val loss: 0.2779, val acc: 0.9049  (best train acc: 0.8278, best val acc: 0.9191, best train loss: 0.4283  @ epoch 18402 )\n",
      "[Epoch: 18480] train loss: 0.4389, train acc: 0.8256, val loss: 0.2734, val acc: 0.9120  (best train acc: 0.8288, best val acc: 0.9191, best train loss: 0.4283  @ epoch 18402 )\n",
      "[Epoch: 18500] train loss: 0.4335, train acc: 0.8248, val loss: 0.2634, val acc: 0.9150  (best train acc: 0.8288, best val acc: 0.9191, best train loss: 0.4282  @ epoch 18492 )\n",
      "[Epoch: 18520] train loss: 0.4407, train acc: 0.8127, val loss: 0.2766, val acc: 0.9096  (best train acc: 0.8288, best val acc: 0.9191, best train loss: 0.4282  @ epoch 18492 )\n",
      "[Epoch: 18540] train loss: 0.4565, train acc: 0.8164, val loss: 0.2905, val acc: 0.8843  (best train acc: 0.8288, best val acc: 0.9191, best train loss: 0.4271  @ epoch 18521 )\n",
      "[Epoch: 18560] train loss: 0.4750, train acc: 0.7964, val loss: 0.3062, val acc: 0.8938  (best train acc: 0.8288, best val acc: 0.9191, best train loss: 0.4271  @ epoch 18521 )\n",
      "[Epoch: 18580] train loss: 0.4531, train acc: 0.8060, val loss: 0.2816, val acc: 0.9096  (best train acc: 0.8288, best val acc: 0.9191, best train loss: 0.4271  @ epoch 18521 )\n",
      "[Epoch: 18600] train loss: 0.4654, train acc: 0.8026, val loss: 0.2964, val acc: 0.8992  (best train acc: 0.8288, best val acc: 0.9191, best train loss: 0.4271  @ epoch 18521 )\n",
      "[Epoch: 18620] train loss: 0.4307, train acc: 0.8166, val loss: 0.2609, val acc: 0.9167  (best train acc: 0.8288, best val acc: 0.9204, best train loss: 0.4271  @ epoch 18521 )\n",
      "[Epoch: 18640] train loss: 0.4748, train acc: 0.7986, val loss: 0.2815, val acc: 0.8978  (best train acc: 0.8288, best val acc: 0.9204, best train loss: 0.4271  @ epoch 18521 )\n",
      "[Epoch: 18660] train loss: 0.4591, train acc: 0.8070, val loss: 0.2837, val acc: 0.8954  (best train acc: 0.8288, best val acc: 0.9204, best train loss: 0.4271  @ epoch 18521 )\n",
      "[Epoch: 18680] train loss: 0.4297, train acc: 0.8174, val loss: 0.2686, val acc: 0.9187  (best train acc: 0.8288, best val acc: 0.9204, best train loss: 0.4271  @ epoch 18521 )\n",
      "[Epoch: 18700] train loss: 0.4311, train acc: 0.8249, val loss: 0.2684, val acc: 0.9130  (best train acc: 0.8288, best val acc: 0.9204, best train loss: 0.4271  @ epoch 18521 )\n",
      "[Epoch: 18720] train loss: 0.4337, train acc: 0.8218, val loss: 0.2699, val acc: 0.9113  (best train acc: 0.8288, best val acc: 0.9204, best train loss: 0.4271  @ epoch 18521 )\n",
      "[Epoch: 18740] train loss: 0.4490, train acc: 0.8138, val loss: 0.2888, val acc: 0.8968  (best train acc: 0.8288, best val acc: 0.9204, best train loss: 0.4266  @ epoch 18726 )\n",
      "[Epoch: 18760] train loss: 0.4684, train acc: 0.8069, val loss: 0.2789, val acc: 0.9019  (best train acc: 0.8288, best val acc: 0.9204, best train loss: 0.4266  @ epoch 18726 )\n",
      "[Epoch: 18780] train loss: 0.4593, train acc: 0.8039, val loss: 0.2890, val acc: 0.9025  (best train acc: 0.8288, best val acc: 0.9204, best train loss: 0.4266  @ epoch 18726 )\n",
      "[Epoch: 18800] train loss: 0.4463, train acc: 0.8103, val loss: 0.2791, val acc: 0.9066  (best train acc: 0.8288, best val acc: 0.9204, best train loss: 0.4266  @ epoch 18726 )\n",
      "[Epoch: 18820] train loss: 0.4350, train acc: 0.8194, val loss: 0.2809, val acc: 0.9052  (best train acc: 0.8288, best val acc: 0.9204, best train loss: 0.4266  @ epoch 18726 )\n",
      "[Epoch: 18840] train loss: 0.4404, train acc: 0.8179, val loss: 0.2769, val acc: 0.9062  (best train acc: 0.8311, best val acc: 0.9204, best train loss: 0.4258  @ epoch 18836 )\n",
      "[Epoch: 18860] train loss: 0.4454, train acc: 0.8172, val loss: 0.2794, val acc: 0.8961  (best train acc: 0.8311, best val acc: 0.9204, best train loss: 0.4258  @ epoch 18836 )\n",
      "[Epoch: 18880] train loss: 0.4497, train acc: 0.8162, val loss: 0.2594, val acc: 0.9174  (best train acc: 0.8311, best val acc: 0.9204, best train loss: 0.4258  @ epoch 18836 )\n",
      "[Epoch: 18900] train loss: 0.4414, train acc: 0.8162, val loss: 0.2720, val acc: 0.8985  (best train acc: 0.8311, best val acc: 0.9204, best train loss: 0.4258  @ epoch 18836 )\n",
      "[Epoch: 18920] train loss: 0.4347, train acc: 0.8203, val loss: 0.2838, val acc: 0.8998  (best train acc: 0.8311, best val acc: 0.9204, best train loss: 0.4258  @ epoch 18836 )\n",
      "[Epoch: 18940] train loss: 0.4349, train acc: 0.8246, val loss: 0.2680, val acc: 0.9083  (best train acc: 0.8320, best val acc: 0.9204, best train loss: 0.4232  @ epoch 18933 )\n",
      "[Epoch: 18960] train loss: 0.4110, train acc: 0.8356, val loss: 0.2675, val acc: 0.9096  (best train acc: 0.8382, best val acc: 0.9204, best train loss: 0.4110  @ epoch 18960 )\n",
      "[Epoch: 18980] train loss: 0.4224, train acc: 0.8168, val loss: 0.2723, val acc: 0.9022  (best train acc: 0.8382, best val acc: 0.9204, best train loss: 0.4110  @ epoch 18960 )\n",
      "[Epoch: 19000] train loss: 0.4138, train acc: 0.8269, val loss: 0.2708, val acc: 0.9174  (best train acc: 0.8382, best val acc: 0.9204, best train loss: 0.4110  @ epoch 18960 )\n",
      "[Epoch: 19020] train loss: 0.4170, train acc: 0.8166, val loss: 0.2647, val acc: 0.9106  (best train acc: 0.8382, best val acc: 0.9204, best train loss: 0.4064  @ epoch 19017 )\n",
      "[Epoch: 19040] train loss: 0.4290, train acc: 0.8190, val loss: 0.2718, val acc: 0.9133  (best train acc: 0.8382, best val acc: 0.9204, best train loss: 0.4064  @ epoch 19017 )\n",
      "[Epoch: 19060] train loss: 0.4372, train acc: 0.8073, val loss: 0.3005, val acc: 0.8958  (best train acc: 0.8382, best val acc: 0.9204, best train loss: 0.4064  @ epoch 19017 )\n",
      "[Epoch: 19080] train loss: 0.4135, train acc: 0.8271, val loss: 0.2818, val acc: 0.9110  (best train acc: 0.8382, best val acc: 0.9204, best train loss: 0.4064  @ epoch 19017 )\n",
      "[Epoch: 19100] train loss: 0.4039, train acc: 0.8313, val loss: 0.2749, val acc: 0.9093  (best train acc: 0.8382, best val acc: 0.9204, best train loss: 0.4021  @ epoch 19095 )\n",
      "[Epoch: 19120] train loss: 0.4093, train acc: 0.8268, val loss: 0.2682, val acc: 0.9137  (best train acc: 0.8382, best val acc: 0.9204, best train loss: 0.3987  @ epoch 19115 )\n",
      "[Epoch: 19140] train loss: 0.4160, train acc: 0.8190, val loss: 0.2805, val acc: 0.9049  (best train acc: 0.8382, best val acc: 0.9204, best train loss: 0.3967  @ epoch 19125 )\n",
      "[Epoch: 19160] train loss: 0.4297, train acc: 0.8203, val loss: 0.2942, val acc: 0.9059  (best train acc: 0.8382, best val acc: 0.9204, best train loss: 0.3967  @ epoch 19125 )\n",
      "[Epoch: 19180] train loss: 0.4352, train acc: 0.8169, val loss: 0.2910, val acc: 0.9066  (best train acc: 0.8382, best val acc: 0.9204, best train loss: 0.3967  @ epoch 19125 )\n",
      "[Epoch: 19200] train loss: 0.4399, train acc: 0.8189, val loss: 0.2806, val acc: 0.9069  (best train acc: 0.8382, best val acc: 0.9204, best train loss: 0.3967  @ epoch 19125 )\n",
      "[Epoch: 19220] train loss: 0.4212, train acc: 0.8148, val loss: 0.2779, val acc: 0.9069  (best train acc: 0.8382, best val acc: 0.9204, best train loss: 0.3967  @ epoch 19125 )\n",
      "[Epoch: 19240] train loss: 0.4540, train acc: 0.8067, val loss: 0.3225, val acc: 0.8887  (best train acc: 0.8382, best val acc: 0.9204, best train loss: 0.3967  @ epoch 19125 )\n",
      "[Epoch: 19260] train loss: 0.4051, train acc: 0.8250, val loss: 0.2744, val acc: 0.9106  (best train acc: 0.8382, best val acc: 0.9204, best train loss: 0.3967  @ epoch 19125 )\n",
      "[Epoch: 19280] train loss: 0.4159, train acc: 0.8148, val loss: 0.2917, val acc: 0.8867  (best train acc: 0.8382, best val acc: 0.9204, best train loss: 0.3967  @ epoch 19125 )\n",
      "[Epoch: 19300] train loss: 0.4172, train acc: 0.8199, val loss: 0.2861, val acc: 0.9002  (best train acc: 0.8382, best val acc: 0.9204, best train loss: 0.3967  @ epoch 19125 )\n",
      "[Epoch: 19320] train loss: 0.4117, train acc: 0.8186, val loss: 0.2673, val acc: 0.9096  (best train acc: 0.8382, best val acc: 0.9204, best train loss: 0.3967  @ epoch 19125 )\n",
      "[Epoch: 19340] train loss: 0.3987, train acc: 0.8332, val loss: 0.2649, val acc: 0.9160  (best train acc: 0.8382, best val acc: 0.9204, best train loss: 0.3942  @ epoch 19338 )\n",
      "[Epoch: 19360] train loss: 0.4093, train acc: 0.8211, val loss: 0.2795, val acc: 0.9042  (best train acc: 0.8382, best val acc: 0.9204, best train loss: 0.3942  @ epoch 19338 )\n",
      "[Epoch: 19380] train loss: 0.4008, train acc: 0.8243, val loss: 0.2637, val acc: 0.9153  (best train acc: 0.8382, best val acc: 0.9204, best train loss: 0.3925  @ epoch 19373 )\n",
      "[Epoch: 19400] train loss: 0.4076, train acc: 0.8298, val loss: 0.2686, val acc: 0.9130  (best train acc: 0.8382, best val acc: 0.9204, best train loss: 0.3925  @ epoch 19373 )\n",
      "[Epoch: 19420] train loss: 0.3973, train acc: 0.8284, val loss: 0.2761, val acc: 0.9140  (best train acc: 0.8383, best val acc: 0.9204, best train loss: 0.3925  @ epoch 19373 )\n",
      "[Epoch: 19440] train loss: 0.4175, train acc: 0.8252, val loss: 0.2890, val acc: 0.9002  (best train acc: 0.8383, best val acc: 0.9204, best train loss: 0.3869  @ epoch 19426 )\n",
      "[Epoch: 19460] train loss: 0.4024, train acc: 0.8277, val loss: 0.2810, val acc: 0.9042  (best train acc: 0.8383, best val acc: 0.9204, best train loss: 0.3869  @ epoch 19426 )\n",
      "[Epoch: 19480] train loss: 0.3913, train acc: 0.8316, val loss: 0.2672, val acc: 0.9143  (best train acc: 0.8383, best val acc: 0.9207, best train loss: 0.3869  @ epoch 19426 )\n",
      "[Epoch: 19500] train loss: 0.3977, train acc: 0.8296, val loss: 0.2687, val acc: 0.9164  (best train acc: 0.8383, best val acc: 0.9207, best train loss: 0.3869  @ epoch 19426 )\n",
      "[Epoch: 19520] train loss: 0.4002, train acc: 0.8308, val loss: 0.2715, val acc: 0.9153  (best train acc: 0.8383, best val acc: 0.9207, best train loss: 0.3869  @ epoch 19426 )\n",
      "[Epoch: 19540] train loss: 0.4132, train acc: 0.8258, val loss: 0.2924, val acc: 0.8992  (best train acc: 0.8383, best val acc: 0.9207, best train loss: 0.3869  @ epoch 19426 )\n",
      "[Epoch: 19560] train loss: 0.4335, train acc: 0.8102, val loss: 0.3037, val acc: 0.9008  (best train acc: 0.8383, best val acc: 0.9207, best train loss: 0.3869  @ epoch 19426 )\n",
      "[Epoch: 19580] train loss: 0.4169, train acc: 0.8203, val loss: 0.2795, val acc: 0.9076  (best train acc: 0.8383, best val acc: 0.9207, best train loss: 0.3869  @ epoch 19426 )\n",
      "[Epoch: 19600] train loss: 0.4424, train acc: 0.8137, val loss: 0.3064, val acc: 0.8884  (best train acc: 0.8383, best val acc: 0.9207, best train loss: 0.3869  @ epoch 19426 )\n",
      "[Epoch: 19620] train loss: 0.4011, train acc: 0.8267, val loss: 0.2810, val acc: 0.9049  (best train acc: 0.8383, best val acc: 0.9207, best train loss: 0.3869  @ epoch 19426 )\n",
      "[Epoch: 19640] train loss: 0.3955, train acc: 0.8327, val loss: 0.2713, val acc: 0.9120  (best train acc: 0.8383, best val acc: 0.9207, best train loss: 0.3869  @ epoch 19426 )\n",
      "[Epoch: 19660] train loss: 0.4084, train acc: 0.8226, val loss: 0.2718, val acc: 0.9126  (best train acc: 0.8383, best val acc: 0.9207, best train loss: 0.3861  @ epoch 19645 )\n",
      "[Epoch: 19680] train loss: 0.3927, train acc: 0.8300, val loss: 0.2664, val acc: 0.9204  (best train acc: 0.8383, best val acc: 0.9207, best train loss: 0.3861  @ epoch 19645 )\n",
      "[Epoch: 19700] train loss: 0.4063, train acc: 0.8307, val loss: 0.2987, val acc: 0.9056  (best train acc: 0.8383, best val acc: 0.9207, best train loss: 0.3861  @ epoch 19645 )\n",
      "[Epoch: 19720] train loss: 0.4171, train acc: 0.8143, val loss: 0.2746, val acc: 0.9103  (best train acc: 0.8387, best val acc: 0.9207, best train loss: 0.3861  @ epoch 19645 )\n",
      "[Epoch: 19740] train loss: 0.4010, train acc: 0.8219, val loss: 0.2864, val acc: 0.8944  (best train acc: 0.8416, best val acc: 0.9221, best train loss: 0.3843  @ epoch 19725 )\n",
      "[Epoch: 19760] train loss: 0.4305, train acc: 0.8164, val loss: 0.2901, val acc: 0.9032  (best train acc: 0.8416, best val acc: 0.9221, best train loss: 0.3843  @ epoch 19725 )\n",
      "[Epoch: 19780] train loss: 0.4312, train acc: 0.8159, val loss: 0.2918, val acc: 0.8907  (best train acc: 0.8416, best val acc: 0.9221, best train loss: 0.3843  @ epoch 19725 )\n",
      "[Epoch: 19800] train loss: 0.3971, train acc: 0.8321, val loss: 0.2791, val acc: 0.8981  (best train acc: 0.8416, best val acc: 0.9221, best train loss: 0.3843  @ epoch 19725 )\n",
      "[Epoch: 19820] train loss: 0.4146, train acc: 0.8225, val loss: 0.2750, val acc: 0.9113  (best train acc: 0.8416, best val acc: 0.9221, best train loss: 0.3843  @ epoch 19725 )\n",
      "[Epoch: 19840] train loss: 0.3985, train acc: 0.8323, val loss: 0.2620, val acc: 0.9113  (best train acc: 0.8416, best val acc: 0.9221, best train loss: 0.3843  @ epoch 19725 )\n",
      "[Epoch: 19860] train loss: 0.4064, train acc: 0.8201, val loss: 0.2617, val acc: 0.9177  (best train acc: 0.8416, best val acc: 0.9221, best train loss: 0.3830  @ epoch 19858 )\n",
      "[Epoch: 19880] train loss: 0.3866, train acc: 0.8344, val loss: 0.2609, val acc: 0.9164  (best train acc: 0.8416, best val acc: 0.9221, best train loss: 0.3830  @ epoch 19858 )\n",
      "[Epoch: 19900] train loss: 0.4076, train acc: 0.8228, val loss: 0.2638, val acc: 0.9150  (best train acc: 0.8417, best val acc: 0.9221, best train loss: 0.3830  @ epoch 19858 )\n",
      "[Epoch: 19920] train loss: 0.3926, train acc: 0.8365, val loss: 0.2606, val acc: 0.9174  (best train acc: 0.8417, best val acc: 0.9221, best train loss: 0.3830  @ epoch 19858 )\n",
      "[Epoch: 19940] train loss: 0.3881, train acc: 0.8344, val loss: 0.2584, val acc: 0.9180  (best train acc: 0.8417, best val acc: 0.9221, best train loss: 0.3830  @ epoch 19858 )\n",
      "[Epoch: 19960] train loss: 0.3918, train acc: 0.8303, val loss: 0.2563, val acc: 0.9164  (best train acc: 0.8417, best val acc: 0.9221, best train loss: 0.3786  @ epoch 19950 )\n",
      "[Epoch: 19980] train loss: 0.3938, train acc: 0.8333, val loss: 0.2588, val acc: 0.9157  (best train acc: 0.8417, best val acc: 0.9228, best train loss: 0.3786  @ epoch 19950 )\n",
      "[Epoch: 20000] train loss: 0.4236, train acc: 0.8106, val loss: 0.2882, val acc: 0.8995  (best train acc: 0.8417, best val acc: 0.9228, best train loss: 0.3786  @ epoch 19950 )\n",
      "[Epoch: 20020] train loss: 0.3910, train acc: 0.8373, val loss: 0.2603, val acc: 0.9177  (best train acc: 0.8417, best val acc: 0.9228, best train loss: 0.3786  @ epoch 19950 )\n",
      "[Epoch: 20040] train loss: 0.4165, train acc: 0.8190, val loss: 0.2736, val acc: 0.9008  (best train acc: 0.8417, best val acc: 0.9228, best train loss: 0.3786  @ epoch 19950 )\n",
      "[Epoch: 20060] train loss: 0.4080, train acc: 0.8173, val loss: 0.2655, val acc: 0.9116  (best train acc: 0.8417, best val acc: 0.9228, best train loss: 0.3786  @ epoch 19950 )\n",
      "[Epoch: 20080] train loss: 0.4082, train acc: 0.8221, val loss: 0.2766, val acc: 0.9099  (best train acc: 0.8417, best val acc: 0.9228, best train loss: 0.3786  @ epoch 19950 )\n",
      "[Epoch: 20100] train loss: 0.4162, train acc: 0.8211, val loss: 0.2778, val acc: 0.9005  (best train acc: 0.8417, best val acc: 0.9228, best train loss: 0.3786  @ epoch 19950 )\n",
      "[Epoch: 20120] train loss: 0.4077, train acc: 0.8273, val loss: 0.2581, val acc: 0.9123  (best train acc: 0.8417, best val acc: 0.9228, best train loss: 0.3786  @ epoch 19950 )\n",
      "[Epoch: 20140] train loss: 0.3980, train acc: 0.8266, val loss: 0.2579, val acc: 0.9137  (best train acc: 0.8421, best val acc: 0.9228, best train loss: 0.3786  @ epoch 19950 )\n",
      "[Epoch: 20160] train loss: 0.4048, train acc: 0.8255, val loss: 0.2600, val acc: 0.9147  (best train acc: 0.8421, best val acc: 0.9228, best train loss: 0.3786  @ epoch 19950 )\n",
      "[Epoch: 20180] train loss: 0.3998, train acc: 0.8308, val loss: 0.2622, val acc: 0.9099  (best train acc: 0.8421, best val acc: 0.9228, best train loss: 0.3786  @ epoch 19950 )\n",
      "[Epoch: 20200] train loss: 0.3991, train acc: 0.8247, val loss: 0.2702, val acc: 0.9073  (best train acc: 0.8421, best val acc: 0.9228, best train loss: 0.3786  @ epoch 19950 )\n",
      "[Epoch: 20220] train loss: 0.4024, train acc: 0.8314, val loss: 0.2758, val acc: 0.9032  (best train acc: 0.8421, best val acc: 0.9228, best train loss: 0.3786  @ epoch 19950 )\n",
      "[Epoch: 20240] train loss: 0.3985, train acc: 0.8338, val loss: 0.2575, val acc: 0.9170  (best train acc: 0.8421, best val acc: 0.9228, best train loss: 0.3786  @ epoch 19950 )\n",
      "[Epoch: 20260] train loss: 0.3922, train acc: 0.8342, val loss: 0.2648, val acc: 0.9120  (best train acc: 0.8421, best val acc: 0.9228, best train loss: 0.3786  @ epoch 19950 )\n",
      "[Epoch: 20280] train loss: 0.3918, train acc: 0.8332, val loss: 0.2609, val acc: 0.9147  (best train acc: 0.8455, best val acc: 0.9228, best train loss: 0.3728  @ epoch 20279 )\n",
      "[Epoch: 20300] train loss: 0.3998, train acc: 0.8332, val loss: 0.2524, val acc: 0.9180  (best train acc: 0.8455, best val acc: 0.9228, best train loss: 0.3728  @ epoch 20279 )\n",
      "[Epoch: 20320] train loss: 0.4041, train acc: 0.8237, val loss: 0.2575, val acc: 0.9211  (best train acc: 0.8455, best val acc: 0.9228, best train loss: 0.3728  @ epoch 20279 )\n",
      "[Epoch: 20340] train loss: 0.4026, train acc: 0.8308, val loss: 0.2797, val acc: 0.9029  (best train acc: 0.8455, best val acc: 0.9228, best train loss: 0.3728  @ epoch 20279 )\n",
      "[Epoch: 20360] train loss: 0.4095, train acc: 0.8213, val loss: 0.2778, val acc: 0.9032  (best train acc: 0.8455, best val acc: 0.9228, best train loss: 0.3728  @ epoch 20279 )\n",
      "[Epoch: 20380] train loss: 0.4094, train acc: 0.8109, val loss: 0.3010, val acc: 0.8911  (best train acc: 0.8455, best val acc: 0.9228, best train loss: 0.3728  @ epoch 20279 )\n",
      "[Epoch: 20400] train loss: 0.3998, train acc: 0.8257, val loss: 0.2568, val acc: 0.9157  (best train acc: 0.8455, best val acc: 0.9228, best train loss: 0.3728  @ epoch 20279 )\n",
      "[Epoch: 20420] train loss: 0.3891, train acc: 0.8409, val loss: 0.2591, val acc: 0.9187  (best train acc: 0.8455, best val acc: 0.9228, best train loss: 0.3728  @ epoch 20279 )\n",
      "[Epoch: 20440] train loss: 0.3812, train acc: 0.8357, val loss: 0.2616, val acc: 0.9130  (best train acc: 0.8455, best val acc: 0.9228, best train loss: 0.3728  @ epoch 20279 )\n",
      "[Epoch: 20460] train loss: 0.3883, train acc: 0.8361, val loss: 0.2551, val acc: 0.9147  (best train acc: 0.8455, best val acc: 0.9228, best train loss: 0.3728  @ epoch 20279 )\n",
      "[Epoch: 20480] train loss: 0.3955, train acc: 0.8298, val loss: 0.2612, val acc: 0.9137  (best train acc: 0.8455, best val acc: 0.9228, best train loss: 0.3728  @ epoch 20279 )\n",
      "[Epoch: 20500] train loss: 0.4130, train acc: 0.8209, val loss: 0.2976, val acc: 0.8853  (best train acc: 0.8455, best val acc: 0.9228, best train loss: 0.3728  @ epoch 20279 )\n",
      "[Epoch: 20520] train loss: 0.4004, train acc: 0.8306, val loss: 0.2655, val acc: 0.9093  (best train acc: 0.8455, best val acc: 0.9228, best train loss: 0.3728  @ epoch 20279 )\n",
      "[Epoch: 20540] train loss: 0.3851, train acc: 0.8327, val loss: 0.2539, val acc: 0.9167  (best train acc: 0.8455, best val acc: 0.9228, best train loss: 0.3728  @ epoch 20279 )\n",
      "[Epoch: 20560] train loss: 0.3972, train acc: 0.8252, val loss: 0.2744, val acc: 0.8971  (best train acc: 0.8455, best val acc: 0.9228, best train loss: 0.3728  @ epoch 20279 )\n",
      "[Epoch: 20580] train loss: 0.4014, train acc: 0.8243, val loss: 0.2509, val acc: 0.9191  (best train acc: 0.8455, best val acc: 0.9228, best train loss: 0.3728  @ epoch 20279 )\n",
      "[Epoch: 20600] train loss: 0.3862, train acc: 0.8304, val loss: 0.2575, val acc: 0.9113  (best train acc: 0.8455, best val acc: 0.9228, best train loss: 0.3728  @ epoch 20279 )\n",
      "[Epoch: 20620] train loss: 0.3996, train acc: 0.8273, val loss: 0.2623, val acc: 0.9116  (best train acc: 0.8455, best val acc: 0.9228, best train loss: 0.3728  @ epoch 20279 )\n",
      "[Epoch: 20640] train loss: 0.3946, train acc: 0.8329, val loss: 0.2641, val acc: 0.9073  (best train acc: 0.8455, best val acc: 0.9228, best train loss: 0.3728  @ epoch 20279 )\n",
      "[Epoch: 20660] train loss: 0.3795, train acc: 0.8301, val loss: 0.2664, val acc: 0.9099  (best train acc: 0.8458, best val acc: 0.9228, best train loss: 0.3728  @ epoch 20279 )\n",
      "[Epoch: 20680] train loss: 0.3824, train acc: 0.8334, val loss: 0.2601, val acc: 0.9120  (best train acc: 0.8458, best val acc: 0.9228, best train loss: 0.3728  @ epoch 20279 )\n",
      "[Epoch: 20700] train loss: 0.3804, train acc: 0.8348, val loss: 0.2615, val acc: 0.9180  (best train acc: 0.8458, best val acc: 0.9228, best train loss: 0.3728  @ epoch 20279 )\n",
      "[Epoch: 20720] train loss: 0.3677, train acc: 0.8430, val loss: 0.2492, val acc: 0.9133  (best train acc: 0.8458, best val acc: 0.9228, best train loss: 0.3604  @ epoch 20717 )\n",
      "[Epoch: 20740] train loss: 0.3771, train acc: 0.8394, val loss: 0.2551, val acc: 0.9174  (best train acc: 0.8477, best val acc: 0.9228, best train loss: 0.3604  @ epoch 20717 )\n",
      "[Epoch: 20760] train loss: 0.3772, train acc: 0.8387, val loss: 0.2627, val acc: 0.9103  (best train acc: 0.8477, best val acc: 0.9228, best train loss: 0.3596  @ epoch 20754 )\n",
      "[Epoch: 20780] train loss: 0.3742, train acc: 0.8401, val loss: 0.2626, val acc: 0.9012  (best train acc: 0.8519, best val acc: 0.9228, best train loss: 0.3590  @ epoch 20773 )\n",
      "[Epoch: 20800] train loss: 0.3777, train acc: 0.8381, val loss: 0.2539, val acc: 0.9150  (best train acc: 0.8519, best val acc: 0.9228, best train loss: 0.3590  @ epoch 20773 )\n",
      "[Epoch: 20820] train loss: 0.3678, train acc: 0.8383, val loss: 0.2557, val acc: 0.9103  (best train acc: 0.8519, best val acc: 0.9228, best train loss: 0.3590  @ epoch 20773 )\n",
      "[Epoch: 20840] train loss: 0.3712, train acc: 0.8396, val loss: 0.2490, val acc: 0.9157  (best train acc: 0.8519, best val acc: 0.9228, best train loss: 0.3590  @ epoch 20773 )\n",
      "[Epoch: 20860] train loss: 0.3604, train acc: 0.8467, val loss: 0.2548, val acc: 0.9147  (best train acc: 0.8521, best val acc: 0.9228, best train loss: 0.3587  @ epoch 20851 )\n",
      "[Epoch: 20880] train loss: 0.3815, train acc: 0.8365, val loss: 0.2571, val acc: 0.9096  (best train acc: 0.8521, best val acc: 0.9228, best train loss: 0.3587  @ epoch 20851 )\n",
      "[Epoch: 20900] train loss: 0.3700, train acc: 0.8404, val loss: 0.2554, val acc: 0.9113  (best train acc: 0.8521, best val acc: 0.9228, best train loss: 0.3587  @ epoch 20851 )\n",
      "[Epoch: 20920] train loss: 0.3791, train acc: 0.8420, val loss: 0.2516, val acc: 0.9231  (best train acc: 0.8521, best val acc: 0.9231, best train loss: 0.3587  @ epoch 20851 )\n",
      "[Epoch: 20940] train loss: 0.3800, train acc: 0.8405, val loss: 0.2567, val acc: 0.9150  (best train acc: 0.8521, best val acc: 0.9231, best train loss: 0.3587  @ epoch 20851 )\n",
      "[Epoch: 20960] train loss: 0.3730, train acc: 0.8353, val loss: 0.2461, val acc: 0.9228  (best train acc: 0.8521, best val acc: 0.9231, best train loss: 0.3587  @ epoch 20851 )\n",
      "[Epoch: 20980] train loss: 0.3848, train acc: 0.8352, val loss: 0.2489, val acc: 0.9157  (best train acc: 0.8521, best val acc: 0.9231, best train loss: 0.3587  @ epoch 20851 )\n",
      "[Epoch: 21000] train loss: 0.3655, train acc: 0.8506, val loss: 0.2444, val acc: 0.9194  (best train acc: 0.8521, best val acc: 0.9231, best train loss: 0.3563  @ epoch 20998 )\n",
      "[Epoch: 21020] train loss: 0.3725, train acc: 0.8429, val loss: 0.2526, val acc: 0.9133  (best train acc: 0.8521, best val acc: 0.9231, best train loss: 0.3563  @ epoch 20998 )\n",
      "[Epoch: 21040] train loss: 0.3717, train acc: 0.8421, val loss: 0.2424, val acc: 0.9197  (best train acc: 0.8540, best val acc: 0.9231, best train loss: 0.3563  @ epoch 20998 )\n",
      "[Epoch: 21060] train loss: 0.3762, train acc: 0.8364, val loss: 0.2698, val acc: 0.8975  (best train acc: 0.8540, best val acc: 0.9231, best train loss: 0.3553  @ epoch 21051 )\n",
      "[Epoch: 21080] train loss: 0.3677, train acc: 0.8530, val loss: 0.2463, val acc: 0.9180  (best train acc: 0.8540, best val acc: 0.9231, best train loss: 0.3553  @ epoch 21051 )\n",
      "[Epoch: 21100] train loss: 0.3770, train acc: 0.8377, val loss: 0.2622, val acc: 0.9143  (best train acc: 0.8540, best val acc: 0.9231, best train loss: 0.3553  @ epoch 21051 )\n",
      "[Epoch: 21120] train loss: 0.3861, train acc: 0.8334, val loss: 0.2471, val acc: 0.9150  (best train acc: 0.8540, best val acc: 0.9231, best train loss: 0.3553  @ epoch 21051 )\n",
      "[Epoch: 21140] train loss: 0.3806, train acc: 0.8378, val loss: 0.2525, val acc: 0.9126  (best train acc: 0.8540, best val acc: 0.9231, best train loss: 0.3553  @ epoch 21051 )\n",
      "[Epoch: 21160] train loss: 0.3635, train acc: 0.8470, val loss: 0.2538, val acc: 0.9140  (best train acc: 0.8540, best val acc: 0.9231, best train loss: 0.3553  @ epoch 21051 )\n",
      "[Epoch: 21180] train loss: 0.4037, train acc: 0.8250, val loss: 0.2948, val acc: 0.8954  (best train acc: 0.8540, best val acc: 0.9231, best train loss: 0.3553  @ epoch 21051 )\n",
      "[Epoch: 21200] train loss: 0.3742, train acc: 0.8383, val loss: 0.2455, val acc: 0.9184  (best train acc: 0.8540, best val acc: 0.9231, best train loss: 0.3553  @ epoch 21051 )\n",
      "[Epoch: 21220] train loss: 0.3641, train acc: 0.8417, val loss: 0.2438, val acc: 0.9170  (best train acc: 0.8540, best val acc: 0.9231, best train loss: 0.3553  @ epoch 21051 )\n",
      "[Epoch: 21240] train loss: 0.3695, train acc: 0.8418, val loss: 0.2489, val acc: 0.9160  (best train acc: 0.8540, best val acc: 0.9231, best train loss: 0.3553  @ epoch 21051 )\n",
      "[Epoch: 21260] train loss: 0.3638, train acc: 0.8426, val loss: 0.2499, val acc: 0.9177  (best train acc: 0.8540, best val acc: 0.9231, best train loss: 0.3553  @ epoch 21051 )\n",
      "[Epoch: 21280] train loss: 0.3725, train acc: 0.8391, val loss: 0.2536, val acc: 0.9133  (best train acc: 0.8540, best val acc: 0.9231, best train loss: 0.3553  @ epoch 21051 )\n",
      "[Epoch: 21300] train loss: 0.3704, train acc: 0.8385, val loss: 0.2505, val acc: 0.9180  (best train acc: 0.8540, best val acc: 0.9231, best train loss: 0.3553  @ epoch 21051 )\n",
      "[Epoch: 21320] train loss: 0.3774, train acc: 0.8404, val loss: 0.2420, val acc: 0.9153  (best train acc: 0.8540, best val acc: 0.9231, best train loss: 0.3553  @ epoch 21051 )\n",
      "[Epoch: 21340] train loss: 0.3627, train acc: 0.8522, val loss: 0.2497, val acc: 0.9201  (best train acc: 0.8540, best val acc: 0.9231, best train loss: 0.3538  @ epoch 21335 )\n",
      "[Epoch: 21360] train loss: 0.3708, train acc: 0.8377, val loss: 0.2545, val acc: 0.9089  (best train acc: 0.8540, best val acc: 0.9231, best train loss: 0.3538  @ epoch 21335 )\n",
      "[Epoch: 21380] train loss: 0.3673, train acc: 0.8412, val loss: 0.2540, val acc: 0.9073  (best train acc: 0.8540, best val acc: 0.9231, best train loss: 0.3538  @ epoch 21335 )\n",
      "[Epoch: 21400] train loss: 0.3744, train acc: 0.8405, val loss: 0.2624, val acc: 0.9066  (best train acc: 0.8541, best val acc: 0.9231, best train loss: 0.3538  @ epoch 21335 )\n",
      "[Epoch: 21420] train loss: 0.3817, train acc: 0.8334, val loss: 0.2599, val acc: 0.9052  (best train acc: 0.8541, best val acc: 0.9231, best train loss: 0.3538  @ epoch 21335 )\n",
      "[Epoch: 21440] train loss: 0.3634, train acc: 0.8501, val loss: 0.2464, val acc: 0.9174  (best train acc: 0.8541, best val acc: 0.9231, best train loss: 0.3538  @ epoch 21335 )\n",
      "[Epoch: 21460] train loss: 0.3714, train acc: 0.8366, val loss: 0.2449, val acc: 0.9130  (best train acc: 0.8555, best val acc: 0.9234, best train loss: 0.3525  @ epoch 21441 )\n",
      "[Epoch: 21480] train loss: 0.3733, train acc: 0.8371, val loss: 0.2477, val acc: 0.9170  (best train acc: 0.8555, best val acc: 0.9234, best train loss: 0.3525  @ epoch 21441 )\n",
      "[Epoch: 21500] train loss: 0.3642, train acc: 0.8451, val loss: 0.2612, val acc: 0.9069  (best train acc: 0.8555, best val acc: 0.9234, best train loss: 0.3525  @ epoch 21441 )\n",
      "[Epoch: 21520] train loss: 0.3745, train acc: 0.8438, val loss: 0.2717, val acc: 0.9032  (best train acc: 0.8555, best val acc: 0.9234, best train loss: 0.3525  @ epoch 21441 )\n",
      "[Epoch: 21540] train loss: 0.3716, train acc: 0.8433, val loss: 0.2599, val acc: 0.9143  (best train acc: 0.8555, best val acc: 0.9234, best train loss: 0.3525  @ epoch 21441 )\n",
      "[Epoch: 21560] train loss: 0.3656, train acc: 0.8464, val loss: 0.2472, val acc: 0.9187  (best train acc: 0.8555, best val acc: 0.9234, best train loss: 0.3525  @ epoch 21441 )\n",
      "[Epoch: 21580] train loss: 0.3654, train acc: 0.8482, val loss: 0.2493, val acc: 0.9191  (best train acc: 0.8555, best val acc: 0.9234, best train loss: 0.3525  @ epoch 21441 )\n",
      "[Epoch: 21600] train loss: 0.3561, train acc: 0.8489, val loss: 0.2480, val acc: 0.9180  (best train acc: 0.8555, best val acc: 0.9234, best train loss: 0.3525  @ epoch 21441 )\n",
      "[Epoch: 21620] train loss: 0.3686, train acc: 0.8407, val loss: 0.2579, val acc: 0.9140  (best train acc: 0.8555, best val acc: 0.9234, best train loss: 0.3525  @ epoch 21441 )\n",
      "[Epoch: 21640] train loss: 0.3689, train acc: 0.8410, val loss: 0.2518, val acc: 0.9147  (best train acc: 0.8555, best val acc: 0.9234, best train loss: 0.3525  @ epoch 21441 )\n",
      "[Epoch: 21660] train loss: 0.3945, train acc: 0.8338, val loss: 0.2745, val acc: 0.8998  (best train acc: 0.8555, best val acc: 0.9234, best train loss: 0.3525  @ epoch 21441 )\n",
      "[Epoch: 21680] train loss: 0.3517, train acc: 0.8557, val loss: 0.2535, val acc: 0.9147  (best train acc: 0.8557, best val acc: 0.9234, best train loss: 0.3517  @ epoch 21680 )\n",
      "[Epoch: 21700] train loss: 0.3427, train acc: 0.8573, val loss: 0.2500, val acc: 0.9113  (best train acc: 0.8606, best val acc: 0.9234, best train loss: 0.3379  @ epoch 21696 )\n",
      "[Epoch: 21720] train loss: 0.3398, train acc: 0.8595, val loss: 0.2468, val acc: 0.9167  (best train acc: 0.8641, best val acc: 0.9234, best train loss: 0.3303  @ epoch 21708 )\n",
      "[Epoch: 21740] train loss: 0.3469, train acc: 0.8561, val loss: 0.2501, val acc: 0.9157  (best train acc: 0.8661, best val acc: 0.9234, best train loss: 0.3246  @ epoch 21722 )\n",
      "[Epoch: 21760] train loss: 0.3489, train acc: 0.8540, val loss: 0.2495, val acc: 0.9187  (best train acc: 0.8661, best val acc: 0.9234, best train loss: 0.3246  @ epoch 21722 )\n",
      "[Epoch: 21780] train loss: 0.3472, train acc: 0.8587, val loss: 0.2556, val acc: 0.9096  (best train acc: 0.8661, best val acc: 0.9234, best train loss: 0.3246  @ epoch 21722 )\n",
      "[Epoch: 21800] train loss: 0.3403, train acc: 0.8617, val loss: 0.2473, val acc: 0.9153  (best train acc: 0.8661, best val acc: 0.9234, best train loss: 0.3246  @ epoch 21722 )\n",
      "[Epoch: 21820] train loss: 0.3350, train acc: 0.8650, val loss: 0.2482, val acc: 0.9164  (best train acc: 0.8665, best val acc: 0.9234, best train loss: 0.3246  @ epoch 21722 )\n",
      "[Epoch: 21840] train loss: 0.3369, train acc: 0.8601, val loss: 0.2450, val acc: 0.9157  (best train acc: 0.8694, best val acc: 0.9234, best train loss: 0.3246  @ epoch 21722 )\n",
      "[Epoch: 21860] train loss: 0.3292, train acc: 0.8662, val loss: 0.2493, val acc: 0.9160  (best train acc: 0.8694, best val acc: 0.9234, best train loss: 0.3245  @ epoch 21859 )\n",
      "[Epoch: 21880] train loss: 0.3557, train acc: 0.8482, val loss: 0.2575, val acc: 0.9106  (best train acc: 0.8694, best val acc: 0.9234, best train loss: 0.3245  @ epoch 21859 )\n",
      "[Epoch: 21900] train loss: 0.3397, train acc: 0.8636, val loss: 0.2476, val acc: 0.9116  (best train acc: 0.8694, best val acc: 0.9234, best train loss: 0.3245  @ epoch 21859 )\n",
      "[Epoch: 21920] train loss: 0.3455, train acc: 0.8529, val loss: 0.2414, val acc: 0.9150  (best train acc: 0.8694, best val acc: 0.9234, best train loss: 0.3245  @ epoch 21859 )\n",
      "[Epoch: 21940] train loss: 0.3279, train acc: 0.8657, val loss: 0.2421, val acc: 0.9197  (best train acc: 0.8694, best val acc: 0.9234, best train loss: 0.3245  @ epoch 21859 )\n",
      "[Epoch: 21960] train loss: 0.3371, train acc: 0.8611, val loss: 0.2467, val acc: 0.9126  (best train acc: 0.8694, best val acc: 0.9234, best train loss: 0.3245  @ epoch 21859 )\n",
      "[Epoch: 21980] train loss: 0.3388, train acc: 0.8604, val loss: 0.2553, val acc: 0.9120  (best train acc: 0.8694, best val acc: 0.9234, best train loss: 0.3245  @ epoch 21859 )\n",
      "[Epoch: 22000] train loss: 0.3421, train acc: 0.8586, val loss: 0.2509, val acc: 0.9062  (best train acc: 0.8694, best val acc: 0.9234, best train loss: 0.3245  @ epoch 21859 )\n",
      "[Epoch: 22020] train loss: 0.3294, train acc: 0.8644, val loss: 0.2429, val acc: 0.9137  (best train acc: 0.8694, best val acc: 0.9234, best train loss: 0.3245  @ epoch 21859 )\n",
      "[Epoch: 22040] train loss: 0.3401, train acc: 0.8584, val loss: 0.2436, val acc: 0.9160  (best train acc: 0.8702, best val acc: 0.9234, best train loss: 0.3244  @ epoch 22023 )\n",
      "[Epoch: 22060] train loss: 0.3281, train acc: 0.8639, val loss: 0.2425, val acc: 0.9187  (best train acc: 0.8705, best val acc: 0.9234, best train loss: 0.3216  @ epoch 22042 )\n",
      "[Epoch: 22080] train loss: 0.3560, train acc: 0.8485, val loss: 0.2547, val acc: 0.9066  (best train acc: 0.8705, best val acc: 0.9234, best train loss: 0.3216  @ epoch 22042 )\n",
      "[Epoch: 22100] train loss: 0.3453, train acc: 0.8566, val loss: 0.2657, val acc: 0.9022  (best train acc: 0.8705, best val acc: 0.9234, best train loss: 0.3216  @ epoch 22042 )\n",
      "[Epoch: 22120] train loss: 0.3291, train acc: 0.8648, val loss: 0.2419, val acc: 0.9194  (best train acc: 0.8705, best val acc: 0.9234, best train loss: 0.3216  @ epoch 22042 )\n",
      "[Epoch: 22140] train loss: 0.3279, train acc: 0.8671, val loss: 0.2381, val acc: 0.9194  (best train acc: 0.8705, best val acc: 0.9234, best train loss: 0.3216  @ epoch 22042 )\n",
      "[Epoch: 22160] train loss: 0.3367, train acc: 0.8642, val loss: 0.2491, val acc: 0.9120  (best train acc: 0.8705, best val acc: 0.9234, best train loss: 0.3216  @ epoch 22042 )\n",
      "[Epoch: 22180] train loss: 0.3421, train acc: 0.8532, val loss: 0.2496, val acc: 0.9150  (best train acc: 0.8705, best val acc: 0.9234, best train loss: 0.3216  @ epoch 22042 )\n",
      "[Epoch: 22200] train loss: 0.3416, train acc: 0.8592, val loss: 0.2467, val acc: 0.9184  (best train acc: 0.8705, best val acc: 0.9234, best train loss: 0.3216  @ epoch 22042 )\n",
      "[Epoch: 22220] train loss: 0.3323, train acc: 0.8652, val loss: 0.2403, val acc: 0.9160  (best train acc: 0.8705, best val acc: 0.9234, best train loss: 0.3216  @ epoch 22042 )\n",
      "[Epoch: 22240] train loss: 0.3285, train acc: 0.8641, val loss: 0.2507, val acc: 0.9133  (best train acc: 0.8705, best val acc: 0.9234, best train loss: 0.3216  @ epoch 22042 )\n",
      "[Epoch: 22260] train loss: 0.3286, train acc: 0.8673, val loss: 0.2413, val acc: 0.9160  (best train acc: 0.8711, best val acc: 0.9234, best train loss: 0.3189  @ epoch 22243 )\n",
      "[Epoch: 22280] train loss: 0.3319, train acc: 0.8612, val loss: 0.2435, val acc: 0.9137  (best train acc: 0.8732, best val acc: 0.9234, best train loss: 0.3189  @ epoch 22243 )\n",
      "[Epoch: 22300] train loss: 0.3290, train acc: 0.8618, val loss: 0.2419, val acc: 0.9221  (best train acc: 0.8732, best val acc: 0.9234, best train loss: 0.3189  @ epoch 22243 )\n",
      "[Epoch: 22320] train loss: 0.3420, train acc: 0.8612, val loss: 0.2548, val acc: 0.9123  (best train acc: 0.8732, best val acc: 0.9234, best train loss: 0.3189  @ epoch 22243 )\n",
      "[Epoch: 22340] train loss: 0.3406, train acc: 0.8595, val loss: 0.2532, val acc: 0.9110  (best train acc: 0.8732, best val acc: 0.9234, best train loss: 0.3189  @ epoch 22243 )\n",
      "[Epoch: 22360] train loss: 0.3288, train acc: 0.8654, val loss: 0.2525, val acc: 0.9126  (best train acc: 0.8732, best val acc: 0.9234, best train loss: 0.3189  @ epoch 22243 )\n",
      "[Epoch: 22380] train loss: 0.3505, train acc: 0.8491, val loss: 0.2561, val acc: 0.9147  (best train acc: 0.8732, best val acc: 0.9234, best train loss: 0.3189  @ epoch 22243 )\n",
      "[Epoch: 22400] train loss: 0.3357, train acc: 0.8652, val loss: 0.2629, val acc: 0.9110  (best train acc: 0.8732, best val acc: 0.9241, best train loss: 0.3189  @ epoch 22243 )\n",
      "[Epoch: 22420] train loss: 0.3437, train acc: 0.8546, val loss: 0.2747, val acc: 0.8978  (best train acc: 0.8732, best val acc: 0.9241, best train loss: 0.3189  @ epoch 22243 )\n",
      "[Epoch: 22440] train loss: 0.3420, train acc: 0.8565, val loss: 0.2524, val acc: 0.9187  (best train acc: 0.8732, best val acc: 0.9241, best train loss: 0.3189  @ epoch 22243 )\n",
      "[Epoch: 22460] train loss: 0.3463, train acc: 0.8592, val loss: 0.2600, val acc: 0.9170  (best train acc: 0.8732, best val acc: 0.9241, best train loss: 0.3189  @ epoch 22243 )\n",
      "[Epoch: 22480] train loss: 0.3364, train acc: 0.8676, val loss: 0.2450, val acc: 0.9160  (best train acc: 0.8732, best val acc: 0.9241, best train loss: 0.3189  @ epoch 22243 )\n",
      "[Epoch: 22500] train loss: 0.3386, train acc: 0.8588, val loss: 0.2467, val acc: 0.9180  (best train acc: 0.8732, best val acc: 0.9241, best train loss: 0.3189  @ epoch 22243 )\n",
      "[Epoch: 22520] train loss: 0.3334, train acc: 0.8617, val loss: 0.2425, val acc: 0.9174  (best train acc: 0.8732, best val acc: 0.9241, best train loss: 0.3189  @ epoch 22243 )\n",
      "[Epoch: 22540] train loss: 0.3720, train acc: 0.8433, val loss: 0.2588, val acc: 0.8995  (best train acc: 0.8732, best val acc: 0.9241, best train loss: 0.3189  @ epoch 22243 )\n",
      "[Epoch: 22560] train loss: 0.3450, train acc: 0.8603, val loss: 0.2506, val acc: 0.9113  (best train acc: 0.8732, best val acc: 0.9241, best train loss: 0.3189  @ epoch 22243 )\n",
      "[Epoch: 22580] train loss: 0.3316, train acc: 0.8605, val loss: 0.2471, val acc: 0.9147  (best train acc: 0.8732, best val acc: 0.9241, best train loss: 0.3189  @ epoch 22243 )\n",
      "[Epoch: 22600] train loss: 0.3427, train acc: 0.8529, val loss: 0.2518, val acc: 0.9130  (best train acc: 0.8732, best val acc: 0.9241, best train loss: 0.3189  @ epoch 22243 )\n",
      "[Epoch: 22620] train loss: 0.3365, train acc: 0.8590, val loss: 0.2465, val acc: 0.9147  (best train acc: 0.8732, best val acc: 0.9241, best train loss: 0.3189  @ epoch 22243 )\n",
      "[Epoch: 22640] train loss: 0.3289, train acc: 0.8655, val loss: 0.2499, val acc: 0.9184  (best train acc: 0.8732, best val acc: 0.9241, best train loss: 0.3189  @ epoch 22243 )\n",
      "[Epoch: 22660] train loss: 0.3336, train acc: 0.8663, val loss: 0.2424, val acc: 0.9160  (best train acc: 0.8732, best val acc: 0.9241, best train loss: 0.3189  @ epoch 22243 )\n",
      "[Epoch: 22680] train loss: 0.3334, train acc: 0.8606, val loss: 0.2454, val acc: 0.9180  (best train acc: 0.8732, best val acc: 0.9241, best train loss: 0.3189  @ epoch 22243 )\n",
      "[Epoch: 22700] train loss: 0.3289, train acc: 0.8711, val loss: 0.2439, val acc: 0.9197  (best train acc: 0.8732, best val acc: 0.9241, best train loss: 0.3189  @ epoch 22243 )\n",
      "[Epoch: 22720] train loss: 0.3377, train acc: 0.8579, val loss: 0.2544, val acc: 0.9110  (best train acc: 0.8732, best val acc: 0.9241, best train loss: 0.3181  @ epoch 22705 )\n",
      "[Epoch: 22740] train loss: 0.3322, train acc: 0.8589, val loss: 0.2470, val acc: 0.9116  (best train acc: 0.8732, best val acc: 0.9241, best train loss: 0.3181  @ epoch 22705 )\n",
      "[Epoch: 22760] train loss: 0.3274, train acc: 0.8650, val loss: 0.2455, val acc: 0.9147  (best train acc: 0.8732, best val acc: 0.9241, best train loss: 0.3181  @ epoch 22705 )\n",
      "[Epoch: 22780] train loss: 0.3413, train acc: 0.8586, val loss: 0.2421, val acc: 0.9218  (best train acc: 0.8732, best val acc: 0.9241, best train loss: 0.3181  @ epoch 22705 )\n",
      "[Epoch: 22800] train loss: 0.3293, train acc: 0.8616, val loss: 0.2423, val acc: 0.9224  (best train acc: 0.8732, best val acc: 0.9241, best train loss: 0.3181  @ epoch 22705 )\n",
      "[Epoch: 22820] train loss: 0.3253, train acc: 0.8676, val loss: 0.2451, val acc: 0.9228  (best train acc: 0.8732, best val acc: 0.9241, best train loss: 0.3181  @ epoch 22705 )\n",
      "[Epoch: 22840] train loss: 0.3341, train acc: 0.8583, val loss: 0.2481, val acc: 0.9076  (best train acc: 0.8732, best val acc: 0.9241, best train loss: 0.3181  @ epoch 22705 )\n",
      "[Epoch: 22860] train loss: 0.3415, train acc: 0.8566, val loss: 0.2428, val acc: 0.9207  (best train acc: 0.8732, best val acc: 0.9241, best train loss: 0.3181  @ epoch 22705 )\n",
      "[Epoch: 22880] train loss: 0.3335, train acc: 0.8602, val loss: 0.2405, val acc: 0.9204  (best train acc: 0.8732, best val acc: 0.9241, best train loss: 0.3181  @ epoch 22705 )\n",
      "[Epoch: 22900] train loss: 0.3313, train acc: 0.8636, val loss: 0.2428, val acc: 0.9218  (best train acc: 0.8732, best val acc: 0.9241, best train loss: 0.3175  @ epoch 22889 )\n",
      "[Epoch: 22920] train loss: 0.3298, train acc: 0.8659, val loss: 0.2391, val acc: 0.9204  (best train acc: 0.8732, best val acc: 0.9241, best train loss: 0.3175  @ epoch 22889 )\n",
      "[Epoch: 22940] train loss: 0.3302, train acc: 0.8624, val loss: 0.2408, val acc: 0.9191  (best train acc: 0.8732, best val acc: 0.9241, best train loss: 0.3175  @ epoch 22889 )\n",
      "[Epoch: 22960] train loss: 0.3383, train acc: 0.8574, val loss: 0.2501, val acc: 0.9180  (best train acc: 0.8732, best val acc: 0.9241, best train loss: 0.3173  @ epoch 22951 )\n",
      "[Epoch: 22980] train loss: 0.3297, train acc: 0.8629, val loss: 0.2496, val acc: 0.9059  (best train acc: 0.8732, best val acc: 0.9241, best train loss: 0.3173  @ epoch 22951 )\n",
      "[Epoch: 23000] train loss: 0.3394, train acc: 0.8599, val loss: 0.2577, val acc: 0.9083  (best train acc: 0.8732, best val acc: 0.9241, best train loss: 0.3173  @ epoch 22951 )\n",
      "[Epoch: 23020] train loss: 0.3255, train acc: 0.8640, val loss: 0.2431, val acc: 0.9167  (best train acc: 0.8732, best val acc: 0.9241, best train loss: 0.3173  @ epoch 22951 )\n",
      "[Epoch: 23040] train loss: 0.3262, train acc: 0.8660, val loss: 0.2346, val acc: 0.9221  (best train acc: 0.8732, best val acc: 0.9241, best train loss: 0.3173  @ epoch 22951 )\n",
      "[Epoch: 23060] train loss: 0.3230, train acc: 0.8639, val loss: 0.2353, val acc: 0.9194  (best train acc: 0.8732, best val acc: 0.9241, best train loss: 0.3173  @ epoch 22951 )\n",
      "[Epoch: 23080] train loss: 0.3298, train acc: 0.8648, val loss: 0.2490, val acc: 0.9170  (best train acc: 0.8732, best val acc: 0.9241, best train loss: 0.3173  @ epoch 22951 )\n",
      "[Epoch: 23100] train loss: 0.3230, train acc: 0.8660, val loss: 0.2434, val acc: 0.9157  (best train acc: 0.8732, best val acc: 0.9241, best train loss: 0.3151  @ epoch 23099 )\n",
      "[Epoch: 23120] train loss: 0.3239, train acc: 0.8719, val loss: 0.2424, val acc: 0.9153  (best train acc: 0.8732, best val acc: 0.9241, best train loss: 0.3151  @ epoch 23099 )\n",
      "[Epoch: 23140] train loss: 0.3307, train acc: 0.8603, val loss: 0.2376, val acc: 0.9197  (best train acc: 0.8732, best val acc: 0.9241, best train loss: 0.3151  @ epoch 23099 )\n",
      "[Epoch: 23160] train loss: 0.3260, train acc: 0.8689, val loss: 0.2495, val acc: 0.9099  (best train acc: 0.8732, best val acc: 0.9241, best train loss: 0.3151  @ epoch 23099 )\n",
      "[Epoch: 23180] train loss: 0.3596, train acc: 0.8434, val loss: 0.2565, val acc: 0.9126  (best train acc: 0.8732, best val acc: 0.9241, best train loss: 0.3151  @ epoch 23099 )\n",
      "[Epoch: 23200] train loss: 0.3400, train acc: 0.8595, val loss: 0.2521, val acc: 0.9153  (best train acc: 0.8732, best val acc: 0.9241, best train loss: 0.3151  @ epoch 23099 )\n",
      "[Epoch: 23220] train loss: 0.3457, train acc: 0.8550, val loss: 0.2403, val acc: 0.9177  (best train acc: 0.8732, best val acc: 0.9241, best train loss: 0.3151  @ epoch 23099 )\n",
      "[Epoch: 23240] train loss: 0.3203, train acc: 0.8723, val loss: 0.2362, val acc: 0.9197  (best train acc: 0.8732, best val acc: 0.9241, best train loss: 0.3151  @ epoch 23099 )\n",
      "[Epoch: 23260] train loss: 0.3261, train acc: 0.8661, val loss: 0.2413, val acc: 0.9120  (best train acc: 0.8732, best val acc: 0.9241, best train loss: 0.3151  @ epoch 23099 )\n",
      "[Epoch: 23280] train loss: 0.3267, train acc: 0.8679, val loss: 0.2388, val acc: 0.9211  (best train acc: 0.8732, best val acc: 0.9241, best train loss: 0.3151  @ epoch 23099 )\n",
      "[Epoch: 23300] train loss: 0.3277, train acc: 0.8638, val loss: 0.2407, val acc: 0.9197  (best train acc: 0.8732, best val acc: 0.9241, best train loss: 0.3151  @ epoch 23099 )\n",
      "[Epoch: 23320] train loss: 0.3146, train acc: 0.8741, val loss: 0.2357, val acc: 0.9194  (best train acc: 0.8741, best val acc: 0.9241, best train loss: 0.3146  @ epoch 23320 )\n",
      "[Epoch: 23340] train loss: 0.3388, train acc: 0.8614, val loss: 0.2374, val acc: 0.9204  (best train acc: 0.8741, best val acc: 0.9241, best train loss: 0.3146  @ epoch 23320 )\n",
      "[Epoch: 23360] train loss: 0.3259, train acc: 0.8642, val loss: 0.2355, val acc: 0.9160  (best train acc: 0.8741, best val acc: 0.9245, best train loss: 0.3146  @ epoch 23320 )\n",
      "[Epoch: 23380] train loss: 0.3204, train acc: 0.8676, val loss: 0.2371, val acc: 0.9177  (best train acc: 0.8741, best val acc: 0.9245, best train loss: 0.3146  @ epoch 23320 )\n",
      "[Epoch: 23400] train loss: 0.3320, train acc: 0.8589, val loss: 0.2394, val acc: 0.9150  (best train acc: 0.8741, best val acc: 0.9245, best train loss: 0.3146  @ epoch 23320 )\n",
      "[Epoch: 23420] train loss: 0.3282, train acc: 0.8666, val loss: 0.2395, val acc: 0.9170  (best train acc: 0.8741, best val acc: 0.9245, best train loss: 0.3146  @ epoch 23320 )\n",
      "[Epoch: 23440] train loss: 0.3287, train acc: 0.8670, val loss: 0.2387, val acc: 0.9204  (best train acc: 0.8741, best val acc: 0.9245, best train loss: 0.3146  @ epoch 23320 )\n",
      "[Epoch: 23460] train loss: 0.3237, train acc: 0.8687, val loss: 0.2600, val acc: 0.9049  (best train acc: 0.8741, best val acc: 0.9245, best train loss: 0.3146  @ epoch 23320 )\n",
      "[Epoch: 23480] train loss: 0.3377, train acc: 0.8611, val loss: 0.2504, val acc: 0.9167  (best train acc: 0.8741, best val acc: 0.9245, best train loss: 0.3146  @ epoch 23320 )\n",
      "[Epoch: 23500] train loss: 0.3519, train acc: 0.8592, val loss: 0.2443, val acc: 0.9147  (best train acc: 0.8741, best val acc: 0.9245, best train loss: 0.3146  @ epoch 23320 )\n",
      "[Epoch: 23520] train loss: 0.3297, train acc: 0.8608, val loss: 0.2386, val acc: 0.9201  (best train acc: 0.8741, best val acc: 0.9245, best train loss: 0.3146  @ epoch 23320 )\n",
      "[Epoch: 23540] train loss: 0.3388, train acc: 0.8615, val loss: 0.2527, val acc: 0.9164  (best train acc: 0.8741, best val acc: 0.9245, best train loss: 0.3146  @ epoch 23320 )\n",
      "[Epoch: 23560] train loss: 0.3274, train acc: 0.8614, val loss: 0.2339, val acc: 0.9204  (best train acc: 0.8741, best val acc: 0.9245, best train loss: 0.3146  @ epoch 23320 )\n",
      "[Epoch: 23580] train loss: 0.3251, train acc: 0.8660, val loss: 0.2443, val acc: 0.9170  (best train acc: 0.8741, best val acc: 0.9245, best train loss: 0.3146  @ epoch 23320 )\n",
      "[Epoch: 23600] train loss: 0.3394, train acc: 0.8581, val loss: 0.2397, val acc: 0.9194  (best train acc: 0.8741, best val acc: 0.9245, best train loss: 0.3146  @ epoch 23320 )\n",
      "[Epoch: 23620] train loss: 0.3338, train acc: 0.8561, val loss: 0.2434, val acc: 0.9133  (best train acc: 0.8741, best val acc: 0.9245, best train loss: 0.3146  @ epoch 23320 )\n",
      "[Epoch: 23640] train loss: 0.3214, train acc: 0.8696, val loss: 0.2430, val acc: 0.9157  (best train acc: 0.8741, best val acc: 0.9245, best train loss: 0.3146  @ epoch 23320 )\n",
      "[Epoch: 23660] train loss: 0.3254, train acc: 0.8659, val loss: 0.2397, val acc: 0.9191  (best train acc: 0.8741, best val acc: 0.9245, best train loss: 0.3146  @ epoch 23320 )\n",
      "[Epoch: 23680] train loss: 0.3248, train acc: 0.8704, val loss: 0.2375, val acc: 0.9201  (best train acc: 0.8741, best val acc: 0.9245, best train loss: 0.3146  @ epoch 23320 )\n",
      "[Epoch: 23700] train loss: 0.3347, train acc: 0.8594, val loss: 0.2407, val acc: 0.9170  (best train acc: 0.8741, best val acc: 0.9245, best train loss: 0.3146  @ epoch 23320 )\n",
      "[Epoch: 23720] train loss: 0.3319, train acc: 0.8649, val loss: 0.2463, val acc: 0.9143  (best train acc: 0.8741, best val acc: 0.9245, best train loss: 0.3146  @ epoch 23320 )\n",
      "[Epoch: 23740] train loss: 0.3213, train acc: 0.8697, val loss: 0.2427, val acc: 0.9184  (best train acc: 0.8741, best val acc: 0.9245, best train loss: 0.3146  @ epoch 23320 )\n",
      "[Epoch: 23760] train loss: 0.3311, train acc: 0.8675, val loss: 0.2402, val acc: 0.9214  (best train acc: 0.8741, best val acc: 0.9245, best train loss: 0.3146  @ epoch 23320 )\n",
      "[Epoch: 23780] train loss: 0.3726, train acc: 0.8498, val loss: 0.2783, val acc: 0.9059  (best train acc: 0.8741, best val acc: 0.9245, best train loss: 0.3146  @ epoch 23320 )\n",
      "[Epoch: 23800] train loss: 0.3411, train acc: 0.8581, val loss: 0.2423, val acc: 0.9234  (best train acc: 0.8741, best val acc: 0.9245, best train loss: 0.3146  @ epoch 23320 )\n",
      "[Epoch: 23820] train loss: 0.3249, train acc: 0.8668, val loss: 0.2471, val acc: 0.9157  (best train acc: 0.8741, best val acc: 0.9245, best train loss: 0.3146  @ epoch 23320 )\n",
      "[Epoch: 23840] train loss: 0.3226, train acc: 0.8608, val loss: 0.2474, val acc: 0.9093  (best train acc: 0.8741, best val acc: 0.9245, best train loss: 0.3146  @ epoch 23320 )\n",
      "[Epoch: 23860] train loss: 0.3232, train acc: 0.8675, val loss: 0.2417, val acc: 0.9164  (best train acc: 0.8741, best val acc: 0.9245, best train loss: 0.3146  @ epoch 23320 )\n",
      "[Epoch: 23880] train loss: 0.3483, train acc: 0.8514, val loss: 0.2522, val acc: 0.9106  (best train acc: 0.8741, best val acc: 0.9245, best train loss: 0.3146  @ epoch 23320 )\n",
      "[Epoch: 23900] train loss: 0.3353, train acc: 0.8546, val loss: 0.2452, val acc: 0.9153  (best train acc: 0.8741, best val acc: 0.9245, best train loss: 0.3146  @ epoch 23320 )\n",
      "[Epoch: 23920] train loss: 0.3460, train acc: 0.8537, val loss: 0.2530, val acc: 0.9086  (best train acc: 0.8741, best val acc: 0.9245, best train loss: 0.3146  @ epoch 23320 )\n",
      "[Epoch: 23940] train loss: 0.3199, train acc: 0.8694, val loss: 0.2462, val acc: 0.9133  (best train acc: 0.8741, best val acc: 0.9245, best train loss: 0.3146  @ epoch 23320 )\n",
      "[Epoch: 23960] train loss: 0.3230, train acc: 0.8681, val loss: 0.2395, val acc: 0.9211  (best train acc: 0.8741, best val acc: 0.9245, best train loss: 0.3146  @ epoch 23320 )\n",
      "[Epoch: 23980] train loss: 0.3327, train acc: 0.8604, val loss: 0.2403, val acc: 0.9157  (best train acc: 0.8741, best val acc: 0.9245, best train loss: 0.3146  @ epoch 23320 )\n",
      "[Epoch: 24000] train loss: 0.3280, train acc: 0.8648, val loss: 0.2395, val acc: 0.9201  (best train acc: 0.8741, best val acc: 0.9245, best train loss: 0.3146  @ epoch 23320 )\n",
      "[Epoch: 24020] train loss: 0.3418, train acc: 0.8627, val loss: 0.2505, val acc: 0.9099  (best train acc: 0.8741, best val acc: 0.9245, best train loss: 0.3146  @ epoch 23320 )\n",
      "[Epoch: 24040] train loss: 0.3363, train acc: 0.8635, val loss: 0.2407, val acc: 0.9180  (best train acc: 0.8741, best val acc: 0.9245, best train loss: 0.3146  @ epoch 23320 )\n",
      "[Epoch: 24060] train loss: 0.3341, train acc: 0.8574, val loss: 0.2454, val acc: 0.9143  (best train acc: 0.8741, best val acc: 0.9245, best train loss: 0.3146  @ epoch 23320 )\n",
      "[Epoch: 24080] train loss: 0.3267, train acc: 0.8685, val loss: 0.2479, val acc: 0.9150  (best train acc: 0.8741, best val acc: 0.9245, best train loss: 0.3146  @ epoch 23320 )\n",
      "[Epoch: 24100] train loss: 0.3619, train acc: 0.8433, val loss: 0.2609, val acc: 0.9032  (best train acc: 0.8741, best val acc: 0.9245, best train loss: 0.3146  @ epoch 23320 )\n",
      "[Epoch: 24120] train loss: 0.3431, train acc: 0.8523, val loss: 0.2485, val acc: 0.9201  (best train acc: 0.8741, best val acc: 0.9245, best train loss: 0.3146  @ epoch 23320 )\n",
      "[Epoch: 24140] train loss: 0.3312, train acc: 0.8618, val loss: 0.2479, val acc: 0.9150  (best train acc: 0.8741, best val acc: 0.9245, best train loss: 0.3146  @ epoch 23320 )\n",
      "[Epoch: 24160] train loss: 0.3315, train acc: 0.8614, val loss: 0.2425, val acc: 0.9170  (best train acc: 0.8741, best val acc: 0.9245, best train loss: 0.3146  @ epoch 23320 )\n",
      "[Epoch: 24180] train loss: 0.3188, train acc: 0.8713, val loss: 0.2349, val acc: 0.9201  (best train acc: 0.8741, best val acc: 0.9245, best train loss: 0.3146  @ epoch 23320 )\n",
      "[Epoch: 24200] train loss: 0.3294, train acc: 0.8625, val loss: 0.2429, val acc: 0.9170  (best train acc: 0.8741, best val acc: 0.9245, best train loss: 0.3146  @ epoch 23320 )\n",
      "[Epoch: 24220] train loss: 0.3433, train acc: 0.8572, val loss: 0.2541, val acc: 0.9099  (best train acc: 0.8741, best val acc: 0.9245, best train loss: 0.3146  @ epoch 23320 )\n",
      "[Epoch: 24240] train loss: 0.3272, train acc: 0.8710, val loss: 0.2415, val acc: 0.9187  (best train acc: 0.8741, best val acc: 0.9245, best train loss: 0.3146  @ epoch 23320 )\n",
      "[Epoch: 24260] train loss: 0.3251, train acc: 0.8676, val loss: 0.2410, val acc: 0.9194  (best train acc: 0.8741, best val acc: 0.9245, best train loss: 0.3146  @ epoch 23320 )\n",
      "[Epoch: 24280] train loss: 0.3371, train acc: 0.8594, val loss: 0.2410, val acc: 0.9177  (best train acc: 0.8741, best val acc: 0.9245, best train loss: 0.3146  @ epoch 23320 )\n",
      "[Epoch: 24300] train loss: 0.3222, train acc: 0.8710, val loss: 0.2443, val acc: 0.9187  (best train acc: 0.8741, best val acc: 0.9245, best train loss: 0.3146  @ epoch 23320 )\n",
      "[Epoch: 24320] train loss: 0.3366, train acc: 0.8629, val loss: 0.2420, val acc: 0.9187  (best train acc: 0.8741, best val acc: 0.9245, best train loss: 0.3146  @ epoch 23320 )\n",
      "[Epoch: 24340] train loss: 0.3311, train acc: 0.8664, val loss: 0.2447, val acc: 0.9191  (best train acc: 0.8741, best val acc: 0.9245, best train loss: 0.3146  @ epoch 23320 )\n",
      "[Epoch: 24360] train loss: 0.3386, train acc: 0.8592, val loss: 0.2462, val acc: 0.9153  (best train acc: 0.8741, best val acc: 0.9245, best train loss: 0.3146  @ epoch 23320 )\n",
      "[Epoch: 24380] train loss: 0.3336, train acc: 0.8598, val loss: 0.2414, val acc: 0.9248  (best train acc: 0.8741, best val acc: 0.9248, best train loss: 0.3146  @ epoch 23320 )\n",
      "[Epoch: 24400] train loss: 0.3341, train acc: 0.8632, val loss: 0.2558, val acc: 0.9089  (best train acc: 0.8741, best val acc: 0.9248, best train loss: 0.3146  @ epoch 23320 )\n",
      "[Epoch: 24420] train loss: 0.3329, train acc: 0.8616, val loss: 0.2439, val acc: 0.9197  (best train acc: 0.8741, best val acc: 0.9248, best train loss: 0.3146  @ epoch 23320 )\n",
      "[Epoch: 24440] train loss: 0.3316, train acc: 0.8632, val loss: 0.2384, val acc: 0.9184  (best train acc: 0.8745, best val acc: 0.9248, best train loss: 0.3146  @ epoch 23320 )\n",
      "[Epoch: 24460] train loss: 0.3282, train acc: 0.8608, val loss: 0.2494, val acc: 0.9143  (best train acc: 0.8745, best val acc: 0.9248, best train loss: 0.3146  @ epoch 23320 )\n",
      "[Epoch: 24480] train loss: 0.3280, train acc: 0.8608, val loss: 0.2510, val acc: 0.9073  (best train acc: 0.8745, best val acc: 0.9248, best train loss: 0.3146  @ epoch 23320 )\n",
      "[Epoch: 24500] train loss: 0.3248, train acc: 0.8642, val loss: 0.2394, val acc: 0.9153  (best train acc: 0.8745, best val acc: 0.9248, best train loss: 0.3146  @ epoch 23320 )\n",
      "[Epoch: 24520] train loss: 0.3380, train acc: 0.8581, val loss: 0.2546, val acc: 0.9126  (best train acc: 0.8745, best val acc: 0.9248, best train loss: 0.3146  @ epoch 23320 )\n",
      "[Epoch: 24540] train loss: 0.3350, train acc: 0.8584, val loss: 0.2418, val acc: 0.9170  (best train acc: 0.8745, best val acc: 0.9248, best train loss: 0.3146  @ epoch 23320 )\n",
      "[Epoch: 24560] train loss: 0.3201, train acc: 0.8632, val loss: 0.2356, val acc: 0.9164  (best train acc: 0.8745, best val acc: 0.9248, best train loss: 0.3146  @ epoch 23320 )\n",
      "[Epoch: 24580] train loss: 0.3284, train acc: 0.8639, val loss: 0.2525, val acc: 0.9126  (best train acc: 0.8745, best val acc: 0.9248, best train loss: 0.3146  @ epoch 23320 )\n",
      "[Epoch: 24600] train loss: 0.3390, train acc: 0.8605, val loss: 0.2514, val acc: 0.9116  (best train acc: 0.8745, best val acc: 0.9248, best train loss: 0.3146  @ epoch 23320 )\n",
      "[Epoch: 24620] train loss: 0.3274, train acc: 0.8616, val loss: 0.2524, val acc: 0.9032  (best train acc: 0.8745, best val acc: 0.9248, best train loss: 0.3146  @ epoch 23320 )\n",
      "[Epoch: 24640] train loss: 0.3263, train acc: 0.8640, val loss: 0.2369, val acc: 0.9194  (best train acc: 0.8745, best val acc: 0.9248, best train loss: 0.3146  @ epoch 23320 )\n",
      "[Epoch: 24660] train loss: 0.3268, train acc: 0.8663, val loss: 0.2453, val acc: 0.9153  (best train acc: 0.8745, best val acc: 0.9248, best train loss: 0.3146  @ epoch 23320 )\n",
      "[Epoch: 24680] train loss: 0.3289, train acc: 0.8670, val loss: 0.2393, val acc: 0.9214  (best train acc: 0.8745, best val acc: 0.9248, best train loss: 0.3146  @ epoch 23320 )\n",
      "[Epoch: 24700] train loss: 0.3298, train acc: 0.8626, val loss: 0.2417, val acc: 0.9170  (best train acc: 0.8745, best val acc: 0.9248, best train loss: 0.3146  @ epoch 23320 )\n",
      "[Epoch: 24720] train loss: 0.3184, train acc: 0.8695, val loss: 0.2397, val acc: 0.9201  (best train acc: 0.8750, best val acc: 0.9248, best train loss: 0.3146  @ epoch 23320 )\n",
      "[Epoch: 24740] train loss: 0.3282, train acc: 0.8650, val loss: 0.2405, val acc: 0.9201  (best train acc: 0.8750, best val acc: 0.9248, best train loss: 0.3146  @ epoch 23320 )\n",
      "[Epoch: 24760] train loss: 0.3156, train acc: 0.8704, val loss: 0.2437, val acc: 0.9191  (best train acc: 0.8755, best val acc: 0.9248, best train loss: 0.3140  @ epoch 24741 )\n",
      "[Epoch: 24780] train loss: 0.3182, train acc: 0.8689, val loss: 0.2361, val acc: 0.9207  (best train acc: 0.8755, best val acc: 0.9248, best train loss: 0.3140  @ epoch 24741 )\n",
      "[Epoch: 24800] train loss: 0.3167, train acc: 0.8713, val loss: 0.2386, val acc: 0.9197  (best train acc: 0.8780, best val acc: 0.9248, best train loss: 0.3140  @ epoch 24741 )\n",
      "[Epoch: 24820] train loss: 0.3524, train acc: 0.8574, val loss: 0.2628, val acc: 0.9039  (best train acc: 0.8780, best val acc: 0.9248, best train loss: 0.3140  @ epoch 24741 )\n",
      "[Epoch: 24840] train loss: 0.3432, train acc: 0.8553, val loss: 0.2440, val acc: 0.9174  (best train acc: 0.8780, best val acc: 0.9248, best train loss: 0.3140  @ epoch 24741 )\n",
      "[Epoch: 24860] train loss: 0.3236, train acc: 0.8699, val loss: 0.2579, val acc: 0.9086  (best train acc: 0.8780, best val acc: 0.9248, best train loss: 0.3140  @ epoch 24741 )\n",
      "[Epoch: 24880] train loss: 0.3239, train acc: 0.8686, val loss: 0.2441, val acc: 0.9191  (best train acc: 0.8780, best val acc: 0.9248, best train loss: 0.3140  @ epoch 24741 )\n",
      "[Epoch: 24900] train loss: 0.3359, train acc: 0.8628, val loss: 0.2563, val acc: 0.9073  (best train acc: 0.8780, best val acc: 0.9248, best train loss: 0.3140  @ epoch 24741 )\n",
      "[Epoch: 24920] train loss: 0.3196, train acc: 0.8708, val loss: 0.2379, val acc: 0.9197  (best train acc: 0.8780, best val acc: 0.9248, best train loss: 0.3140  @ epoch 24741 )\n",
      "[Epoch: 24940] train loss: 0.3276, train acc: 0.8631, val loss: 0.2496, val acc: 0.9133  (best train acc: 0.8780, best val acc: 0.9248, best train loss: 0.3140  @ epoch 24741 )\n",
      "[Epoch: 24960] train loss: 0.3375, train acc: 0.8633, val loss: 0.2473, val acc: 0.9184  (best train acc: 0.8780, best val acc: 0.9248, best train loss: 0.3140  @ epoch 24741 )\n",
      "[Epoch: 24980] train loss: 0.3270, train acc: 0.8650, val loss: 0.2419, val acc: 0.9184  (best train acc: 0.8780, best val acc: 0.9248, best train loss: 0.3140  @ epoch 24741 )\n",
      "[Epoch: 25000] train loss: 0.3289, train acc: 0.8672, val loss: 0.2401, val acc: 0.9177  (best train acc: 0.8780, best val acc: 0.9248, best train loss: 0.3140  @ epoch 24741 )\n",
      "[Epoch: 25020] train loss: 0.3230, train acc: 0.8676, val loss: 0.2402, val acc: 0.9187  (best train acc: 0.8780, best val acc: 0.9248, best train loss: 0.3140  @ epoch 24741 )\n",
      "[Epoch: 25040] train loss: 0.3306, train acc: 0.8627, val loss: 0.2523, val acc: 0.9150  (best train acc: 0.8780, best val acc: 0.9248, best train loss: 0.3123  @ epoch 25036 )\n",
      "[Epoch: 25060] train loss: 0.3395, train acc: 0.8559, val loss: 0.2454, val acc: 0.9201  (best train acc: 0.8780, best val acc: 0.9248, best train loss: 0.3123  @ epoch 25036 )\n",
      "[Epoch: 25080] train loss: 0.3255, train acc: 0.8663, val loss: 0.2440, val acc: 0.9180  (best train acc: 0.8780, best val acc: 0.9248, best train loss: 0.3123  @ epoch 25036 )\n",
      "[Epoch: 25100] train loss: 0.3308, train acc: 0.8631, val loss: 0.2484, val acc: 0.9116  (best train acc: 0.8780, best val acc: 0.9248, best train loss: 0.3123  @ epoch 25036 )\n",
      "[Epoch: 25120] train loss: 0.3323, train acc: 0.8664, val loss: 0.2429, val acc: 0.9197  (best train acc: 0.8780, best val acc: 0.9248, best train loss: 0.3123  @ epoch 25036 )\n",
      "[Epoch: 25140] train loss: 0.3408, train acc: 0.8579, val loss: 0.2436, val acc: 0.9197  (best train acc: 0.8780, best val acc: 0.9248, best train loss: 0.3123  @ epoch 25036 )\n",
      "[Epoch: 25160] train loss: 0.3327, train acc: 0.8619, val loss: 0.2404, val acc: 0.9197  (best train acc: 0.8780, best val acc: 0.9248, best train loss: 0.3123  @ epoch 25036 )\n",
      "[Epoch: 25180] train loss: 0.3307, train acc: 0.8619, val loss: 0.2602, val acc: 0.9106  (best train acc: 0.8780, best val acc: 0.9248, best train loss: 0.3123  @ epoch 25036 )\n",
      "[Epoch: 25200] train loss: 0.3287, train acc: 0.8641, val loss: 0.2407, val acc: 0.9184  (best train acc: 0.8780, best val acc: 0.9248, best train loss: 0.3123  @ epoch 25036 )\n",
      "[Epoch: 25220] train loss: 0.3228, train acc: 0.8710, val loss: 0.2399, val acc: 0.9214  (best train acc: 0.8780, best val acc: 0.9248, best train loss: 0.3123  @ epoch 25036 )\n",
      "[Epoch: 25240] train loss: 0.3180, train acc: 0.8670, val loss: 0.2432, val acc: 0.9177  (best train acc: 0.8780, best val acc: 0.9248, best train loss: 0.3123  @ epoch 25036 )\n",
      "[Epoch: 25260] train loss: 0.3242, train acc: 0.8665, val loss: 0.2405, val acc: 0.9174  (best train acc: 0.8780, best val acc: 0.9248, best train loss: 0.3123  @ epoch 25036 )\n",
      "[Epoch: 25280] train loss: 0.3210, train acc: 0.8694, val loss: 0.2383, val acc: 0.9167  (best train acc: 0.8780, best val acc: 0.9248, best train loss: 0.3123  @ epoch 25036 )\n",
      "[Epoch: 25300] train loss: 0.3246, train acc: 0.8650, val loss: 0.2417, val acc: 0.9214  (best train acc: 0.8780, best val acc: 0.9248, best train loss: 0.3113  @ epoch 25292 )\n",
      "[Epoch: 25320] train loss: 0.3264, train acc: 0.8670, val loss: 0.2549, val acc: 0.9116  (best train acc: 0.8780, best val acc: 0.9248, best train loss: 0.3113  @ epoch 25292 )\n",
      "[Epoch: 25340] train loss: 0.3304, train acc: 0.8576, val loss: 0.2380, val acc: 0.9197  (best train acc: 0.8780, best val acc: 0.9248, best train loss: 0.3113  @ epoch 25292 )\n",
      "[Epoch: 25360] train loss: 0.3227, train acc: 0.8667, val loss: 0.2485, val acc: 0.9167  (best train acc: 0.8780, best val acc: 0.9248, best train loss: 0.3113  @ epoch 25292 )\n",
      "[Epoch: 25380] train loss: 0.3304, train acc: 0.8655, val loss: 0.2430, val acc: 0.9177  (best train acc: 0.8780, best val acc: 0.9248, best train loss: 0.3113  @ epoch 25292 )\n",
      "[Epoch: 25400] train loss: 0.3264, train acc: 0.8683, val loss: 0.2559, val acc: 0.9177  (best train acc: 0.8780, best val acc: 0.9248, best train loss: 0.3113  @ epoch 25292 )\n",
      "[Epoch: 25420] train loss: 0.3237, train acc: 0.8663, val loss: 0.2413, val acc: 0.9187  (best train acc: 0.8780, best val acc: 0.9248, best train loss: 0.3113  @ epoch 25292 )\n",
      "[Epoch: 25440] train loss: 0.3414, train acc: 0.8559, val loss: 0.2446, val acc: 0.9191  (best train acc: 0.8780, best val acc: 0.9248, best train loss: 0.3113  @ epoch 25292 )\n",
      "[Epoch: 25460] train loss: 0.3425, train acc: 0.8515, val loss: 0.2486, val acc: 0.9116  (best train acc: 0.8780, best val acc: 0.9248, best train loss: 0.3113  @ epoch 25292 )\n",
      "[Epoch: 25480] train loss: 0.3239, train acc: 0.8694, val loss: 0.2443, val acc: 0.9197  (best train acc: 0.8780, best val acc: 0.9248, best train loss: 0.3113  @ epoch 25292 )\n",
      "[Epoch: 25500] train loss: 0.3418, train acc: 0.8581, val loss: 0.2445, val acc: 0.9170  (best train acc: 0.8780, best val acc: 0.9248, best train loss: 0.3113  @ epoch 25292 )\n",
      "[Epoch: 25520] train loss: 0.3325, train acc: 0.8643, val loss: 0.2421, val acc: 0.9177  (best train acc: 0.8780, best val acc: 0.9248, best train loss: 0.3113  @ epoch 25292 )\n",
      "[Epoch: 25540] train loss: 0.3305, train acc: 0.8626, val loss: 0.2423, val acc: 0.9187  (best train acc: 0.8780, best val acc: 0.9248, best train loss: 0.3113  @ epoch 25292 )\n",
      "[Epoch: 25560] train loss: 0.3276, train acc: 0.8623, val loss: 0.2415, val acc: 0.9133  (best train acc: 0.8780, best val acc: 0.9248, best train loss: 0.3113  @ epoch 25292 )\n",
      "[Epoch: 25580] train loss: 0.3290, train acc: 0.8673, val loss: 0.2571, val acc: 0.9059  (best train acc: 0.8780, best val acc: 0.9248, best train loss: 0.3113  @ epoch 25292 )\n",
      "[Epoch: 25600] train loss: 0.3266, train acc: 0.8657, val loss: 0.2348, val acc: 0.9191  (best train acc: 0.8780, best val acc: 0.9248, best train loss: 0.3113  @ epoch 25292 )\n",
      "[Epoch: 25620] train loss: 0.3209, train acc: 0.8681, val loss: 0.2378, val acc: 0.9197  (best train acc: 0.8780, best val acc: 0.9248, best train loss: 0.3113  @ epoch 25292 )\n",
      "[Epoch: 25640] train loss: 0.3235, train acc: 0.8632, val loss: 0.2456, val acc: 0.9180  (best train acc: 0.8780, best val acc: 0.9248, best train loss: 0.3113  @ epoch 25292 )\n",
      "[Epoch: 25660] train loss: 0.3309, train acc: 0.8620, val loss: 0.2409, val acc: 0.9174  (best train acc: 0.8780, best val acc: 0.9248, best train loss: 0.3113  @ epoch 25292 )\n",
      "[Epoch: 25680] train loss: 0.3266, train acc: 0.8653, val loss: 0.2500, val acc: 0.9164  (best train acc: 0.8780, best val acc: 0.9248, best train loss: 0.3113  @ epoch 25292 )\n",
      "[Epoch: 25700] train loss: 0.3418, train acc: 0.8574, val loss: 0.2357, val acc: 0.9150  (best train acc: 0.8780, best val acc: 0.9248, best train loss: 0.3113  @ epoch 25292 )\n",
      "[Epoch: 25720] train loss: 0.3389, train acc: 0.8545, val loss: 0.2452, val acc: 0.9130  (best train acc: 0.8780, best val acc: 0.9248, best train loss: 0.3113  @ epoch 25292 )\n",
      "[Epoch: 25740] train loss: 0.3288, train acc: 0.8663, val loss: 0.2353, val acc: 0.9194  (best train acc: 0.8780, best val acc: 0.9248, best train loss: 0.3113  @ epoch 25292 )\n",
      "[Epoch: 25760] train loss: 0.3378, train acc: 0.8636, val loss: 0.2535, val acc: 0.9096  (best train acc: 0.8780, best val acc: 0.9248, best train loss: 0.3113  @ epoch 25292 )\n",
      "[Epoch: 25780] train loss: 0.3292, train acc: 0.8595, val loss: 0.2388, val acc: 0.9201  (best train acc: 0.8780, best val acc: 0.9248, best train loss: 0.3113  @ epoch 25292 )\n",
      "[Epoch: 25800] train loss: 0.3200, train acc: 0.8693, val loss: 0.2408, val acc: 0.9184  (best train acc: 0.8780, best val acc: 0.9248, best train loss: 0.3113  @ epoch 25292 )\n",
      "[Epoch: 25820] train loss: 0.3261, train acc: 0.8615, val loss: 0.2405, val acc: 0.9184  (best train acc: 0.8780, best val acc: 0.9248, best train loss: 0.3113  @ epoch 25292 )\n",
      "[Epoch: 25840] train loss: 0.3243, train acc: 0.8638, val loss: 0.2476, val acc: 0.9113  (best train acc: 0.8780, best val acc: 0.9248, best train loss: 0.3113  @ epoch 25292 )\n",
      "[Epoch: 25860] train loss: 0.3419, train acc: 0.8538, val loss: 0.2510, val acc: 0.9120  (best train acc: 0.8780, best val acc: 0.9248, best train loss: 0.3113  @ epoch 25292 )\n",
      "[Epoch: 25880] train loss: 0.3325, train acc: 0.8613, val loss: 0.2484, val acc: 0.9130  (best train acc: 0.8780, best val acc: 0.9248, best train loss: 0.3113  @ epoch 25292 )\n",
      "[Epoch: 25900] train loss: 0.3240, train acc: 0.8647, val loss: 0.2460, val acc: 0.9224  (best train acc: 0.8780, best val acc: 0.9248, best train loss: 0.3113  @ epoch 25292 )\n",
      "[Epoch: 25920] train loss: 0.3357, train acc: 0.8574, val loss: 0.2433, val acc: 0.9153  (best train acc: 0.8780, best val acc: 0.9248, best train loss: 0.3113  @ epoch 25292 )\n",
      "[Epoch: 25940] train loss: 0.3253, train acc: 0.8655, val loss: 0.2388, val acc: 0.9164  (best train acc: 0.8780, best val acc: 0.9248, best train loss: 0.3113  @ epoch 25292 )\n",
      "[Epoch: 25960] train loss: 0.3252, train acc: 0.8669, val loss: 0.2441, val acc: 0.9177  (best train acc: 0.8780, best val acc: 0.9248, best train loss: 0.3113  @ epoch 25292 )\n",
      "[Epoch: 25980] train loss: 0.3182, train acc: 0.8676, val loss: 0.2399, val acc: 0.9214  (best train acc: 0.8780, best val acc: 0.9248, best train loss: 0.3113  @ epoch 25292 )\n",
      "[Epoch: 26000] train loss: 0.3306, train acc: 0.8636, val loss: 0.2421, val acc: 0.9164  (best train acc: 0.8780, best val acc: 0.9248, best train loss: 0.3113  @ epoch 25292 )\n",
      "[Epoch: 26020] train loss: 0.3213, train acc: 0.8670, val loss: 0.2353, val acc: 0.9238  (best train acc: 0.8780, best val acc: 0.9248, best train loss: 0.3113  @ epoch 25292 )\n",
      "[Epoch: 26040] train loss: 0.3389, train acc: 0.8572, val loss: 0.2421, val acc: 0.9184  (best train acc: 0.8780, best val acc: 0.9248, best train loss: 0.3113  @ epoch 25292 )\n",
      "[Epoch: 26060] train loss: 0.3272, train acc: 0.8656, val loss: 0.2382, val acc: 0.9201  (best train acc: 0.8780, best val acc: 0.9248, best train loss: 0.3113  @ epoch 25292 )\n",
      "[Epoch: 26080] train loss: 0.3546, train acc: 0.8539, val loss: 0.2611, val acc: 0.9073  (best train acc: 0.8780, best val acc: 0.9248, best train loss: 0.3113  @ epoch 25292 )\n",
      "[Epoch: 26100] train loss: 0.3220, train acc: 0.8644, val loss: 0.2419, val acc: 0.9177  (best train acc: 0.8780, best val acc: 0.9248, best train loss: 0.3113  @ epoch 25292 )\n",
      "[Epoch: 26120] train loss: 0.3221, train acc: 0.8678, val loss: 0.2368, val acc: 0.9197  (best train acc: 0.8780, best val acc: 0.9248, best train loss: 0.3113  @ epoch 25292 )\n",
      "[Epoch: 26140] train loss: 0.3156, train acc: 0.8702, val loss: 0.2517, val acc: 0.9167  (best train acc: 0.8780, best val acc: 0.9248, best train loss: 0.3113  @ epoch 25292 )\n",
      "[Epoch: 26160] train loss: 0.3362, train acc: 0.8585, val loss: 0.2439, val acc: 0.9170  (best train acc: 0.8780, best val acc: 0.9248, best train loss: 0.3113  @ epoch 25292 )\n",
      "[Epoch: 26180] train loss: 0.3365, train acc: 0.8600, val loss: 0.2484, val acc: 0.9130  (best train acc: 0.8780, best val acc: 0.9248, best train loss: 0.3113  @ epoch 25292 )\n",
      "[Epoch: 26200] train loss: 0.3178, train acc: 0.8691, val loss: 0.2389, val acc: 0.9174  (best train acc: 0.8780, best val acc: 0.9248, best train loss: 0.3113  @ epoch 25292 )\n",
      "[Epoch: 26220] train loss: 0.3179, train acc: 0.8711, val loss: 0.2484, val acc: 0.9150  (best train acc: 0.8780, best val acc: 0.9248, best train loss: 0.3113  @ epoch 25292 )\n",
      "[Epoch: 26240] train loss: 0.3291, train acc: 0.8645, val loss: 0.2398, val acc: 0.9184  (best train acc: 0.8780, best val acc: 0.9248, best train loss: 0.3113  @ epoch 25292 )\n",
      "[Epoch: 26260] train loss: 0.3209, train acc: 0.8733, val loss: 0.2409, val acc: 0.9160  (best train acc: 0.8780, best val acc: 0.9248, best train loss: 0.3113  @ epoch 25292 )\n",
      "[Epoch: 26280] train loss: 0.3367, train acc: 0.8585, val loss: 0.2453, val acc: 0.9180  (best train acc: 0.8780, best val acc: 0.9248, best train loss: 0.3113  @ epoch 25292 )\n",
      "[Epoch: 26300] train loss: 0.3181, train acc: 0.8690, val loss: 0.2363, val acc: 0.9164  (best train acc: 0.8780, best val acc: 0.9248, best train loss: 0.3113  @ epoch 25292 )\n",
      "[Epoch: 26320] train loss: 0.3239, train acc: 0.8728, val loss: 0.2366, val acc: 0.9167  (best train acc: 0.8780, best val acc: 0.9248, best train loss: 0.3113  @ epoch 25292 )\n",
      "[Epoch: 26340] train loss: 0.3391, train acc: 0.8633, val loss: 0.2564, val acc: 0.9086  (best train acc: 0.8780, best val acc: 0.9248, best train loss: 0.3103  @ epoch 26326 )\n",
      "[Epoch: 26360] train loss: 0.3360, train acc: 0.8578, val loss: 0.2502, val acc: 0.9089  (best train acc: 0.8780, best val acc: 0.9248, best train loss: 0.3103  @ epoch 26326 )\n",
      "[Epoch: 26380] train loss: 0.3215, train acc: 0.8720, val loss: 0.2443, val acc: 0.9191  (best train acc: 0.8780, best val acc: 0.9248, best train loss: 0.3103  @ epoch 26326 )\n",
      "[Epoch: 26400] train loss: 0.3205, train acc: 0.8695, val loss: 0.2386, val acc: 0.9160  (best train acc: 0.8780, best val acc: 0.9248, best train loss: 0.3071  @ epoch 26382 )\n",
      "[Epoch: 26420] train loss: 0.3307, train acc: 0.8634, val loss: 0.2387, val acc: 0.9197  (best train acc: 0.8785, best val acc: 0.9248, best train loss: 0.3071  @ epoch 26382 )\n",
      "[Epoch: 26440] train loss: 0.3096, train acc: 0.8763, val loss: 0.2373, val acc: 0.9184  (best train acc: 0.8828, best val acc: 0.9248, best train loss: 0.3060  @ epoch 26439 )\n",
      "[Epoch: 26460] train loss: 0.3175, train acc: 0.8687, val loss: 0.2332, val acc: 0.9177  (best train acc: 0.8828, best val acc: 0.9248, best train loss: 0.3009  @ epoch 26449 )\n",
      "[Epoch: 26480] train loss: 0.3110, train acc: 0.8749, val loss: 0.2376, val acc: 0.9184  (best train acc: 0.8828, best val acc: 0.9248, best train loss: 0.3009  @ epoch 26449 )\n",
      "[Epoch: 26500] train loss: 0.3179, train acc: 0.8702, val loss: 0.2444, val acc: 0.9113  (best train acc: 0.8828, best val acc: 0.9248, best train loss: 0.3009  @ epoch 26449 )\n",
      "[Epoch: 26520] train loss: 0.3131, train acc: 0.8776, val loss: 0.2426, val acc: 0.9133  (best train acc: 0.8828, best val acc: 0.9248, best train loss: 0.3009  @ epoch 26449 )\n",
      "[Epoch: 26540] train loss: 0.3254, train acc: 0.8702, val loss: 0.2459, val acc: 0.9123  (best train acc: 0.8828, best val acc: 0.9248, best train loss: 0.3009  @ epoch 26449 )\n",
      "[Epoch: 26560] train loss: 0.3181, train acc: 0.8682, val loss: 0.2426, val acc: 0.9170  (best train acc: 0.8828, best val acc: 0.9248, best train loss: 0.3009  @ epoch 26449 )\n",
      "[Epoch: 26580] train loss: 0.3308, train acc: 0.8617, val loss: 0.2447, val acc: 0.9170  (best train acc: 0.8828, best val acc: 0.9248, best train loss: 0.3009  @ epoch 26449 )\n",
      "[Epoch: 26600] train loss: 0.3178, train acc: 0.8694, val loss: 0.2388, val acc: 0.9170  (best train acc: 0.8828, best val acc: 0.9248, best train loss: 0.3009  @ epoch 26449 )\n",
      "[Epoch: 26620] train loss: 0.3113, train acc: 0.8762, val loss: 0.2450, val acc: 0.9133  (best train acc: 0.8828, best val acc: 0.9248, best train loss: 0.3009  @ epoch 26449 )\n",
      "[Epoch: 26640] train loss: 0.3215, train acc: 0.8716, val loss: 0.2456, val acc: 0.9076  (best train acc: 0.8828, best val acc: 0.9248, best train loss: 0.3009  @ epoch 26449 )\n",
      "[Epoch: 26660] train loss: 0.3155, train acc: 0.8756, val loss: 0.2545, val acc: 0.9076  (best train acc: 0.8828, best val acc: 0.9248, best train loss: 0.3009  @ epoch 26449 )\n",
      "[Epoch: 26680] train loss: 0.3107, train acc: 0.8707, val loss: 0.2504, val acc: 0.9116  (best train acc: 0.8828, best val acc: 0.9248, best train loss: 0.3009  @ epoch 26449 )\n",
      "[Epoch: 26700] train loss: 0.3137, train acc: 0.8715, val loss: 0.2428, val acc: 0.9123  (best train acc: 0.8828, best val acc: 0.9248, best train loss: 0.3009  @ epoch 26449 )\n",
      "[Epoch: 26720] train loss: 0.3206, train acc: 0.8650, val loss: 0.2398, val acc: 0.9157  (best train acc: 0.8828, best val acc: 0.9248, best train loss: 0.3009  @ epoch 26449 )\n",
      "[Epoch: 26740] train loss: 0.3365, train acc: 0.8626, val loss: 0.2478, val acc: 0.9177  (best train acc: 0.8828, best val acc: 0.9248, best train loss: 0.3009  @ epoch 26449 )\n",
      "[Epoch: 26760] train loss: 0.3139, train acc: 0.8762, val loss: 0.2385, val acc: 0.9167  (best train acc: 0.8828, best val acc: 0.9248, best train loss: 0.3009  @ epoch 26449 )\n",
      "[Epoch: 26780] train loss: 0.3163, train acc: 0.8687, val loss: 0.2374, val acc: 0.9170  (best train acc: 0.8828, best val acc: 0.9248, best train loss: 0.3009  @ epoch 26449 )\n",
      "[Epoch: 26800] train loss: 0.3025, train acc: 0.8806, val loss: 0.2401, val acc: 0.9160  (best train acc: 0.8828, best val acc: 0.9248, best train loss: 0.3009  @ epoch 26449 )\n",
      "[Epoch: 26820] train loss: 0.3117, train acc: 0.8743, val loss: 0.2420, val acc: 0.9174  (best train acc: 0.8828, best val acc: 0.9248, best train loss: 0.3009  @ epoch 26449 )\n",
      "[Epoch: 26840] train loss: 0.3124, train acc: 0.8708, val loss: 0.2388, val acc: 0.9164  (best train acc: 0.8828, best val acc: 0.9248, best train loss: 0.3009  @ epoch 26449 )\n",
      "[Epoch: 26860] train loss: 0.3114, train acc: 0.8770, val loss: 0.2413, val acc: 0.9180  (best train acc: 0.8828, best val acc: 0.9248, best train loss: 0.3009  @ epoch 26449 )\n",
      "[Epoch: 26880] train loss: 0.3233, train acc: 0.8716, val loss: 0.2448, val acc: 0.9133  (best train acc: 0.8828, best val acc: 0.9248, best train loss: 0.3009  @ epoch 26449 )\n",
      "[Epoch: 26900] train loss: 0.3130, train acc: 0.8733, val loss: 0.2423, val acc: 0.9174  (best train acc: 0.8828, best val acc: 0.9248, best train loss: 0.3009  @ epoch 26449 )\n",
      "[Epoch: 26920] train loss: 0.3231, train acc: 0.8710, val loss: 0.2614, val acc: 0.9035  (best train acc: 0.8828, best val acc: 0.9248, best train loss: 0.3009  @ epoch 26449 )\n",
      "[Epoch: 26940] train loss: 0.3289, train acc: 0.8662, val loss: 0.2428, val acc: 0.9153  (best train acc: 0.8828, best val acc: 0.9248, best train loss: 0.3009  @ epoch 26449 )\n",
      "[Epoch: 26960] train loss: 0.3080, train acc: 0.8783, val loss: 0.2394, val acc: 0.9204  (best train acc: 0.8828, best val acc: 0.9248, best train loss: 0.3009  @ epoch 26449 )\n",
      "[Epoch: 26980] train loss: 0.3132, train acc: 0.8724, val loss: 0.2377, val acc: 0.9153  (best train acc: 0.8828, best val acc: 0.9248, best train loss: 0.3009  @ epoch 26449 )\n",
      "[Epoch: 27000] train loss: 0.3161, train acc: 0.8689, val loss: 0.2366, val acc: 0.9191  (best train acc: 0.8828, best val acc: 0.9248, best train loss: 0.3009  @ epoch 26449 )\n",
      "[Epoch: 27020] train loss: 0.3128, train acc: 0.8736, val loss: 0.2398, val acc: 0.9197  (best train acc: 0.8828, best val acc: 0.9248, best train loss: 0.3009  @ epoch 26449 )\n",
      "[Epoch: 27040] train loss: 0.3178, train acc: 0.8720, val loss: 0.2416, val acc: 0.9191  (best train acc: 0.8828, best val acc: 0.9248, best train loss: 0.3009  @ epoch 26449 )\n",
      "[Epoch: 27060] train loss: 0.3126, train acc: 0.8717, val loss: 0.2475, val acc: 0.9170  (best train acc: 0.8828, best val acc: 0.9248, best train loss: 0.3009  @ epoch 26449 )\n",
      "[Epoch: 27080] train loss: 0.3222, train acc: 0.8618, val loss: 0.2454, val acc: 0.9137  (best train acc: 0.8828, best val acc: 0.9248, best train loss: 0.3009  @ epoch 26449 )\n",
      "[Epoch: 27100] train loss: 0.3119, train acc: 0.8783, val loss: 0.2459, val acc: 0.9160  (best train acc: 0.8828, best val acc: 0.9248, best train loss: 0.3009  @ epoch 26449 )\n",
      "[Epoch: 27120] train loss: 0.3184, train acc: 0.8730, val loss: 0.2443, val acc: 0.9147  (best train acc: 0.8828, best val acc: 0.9248, best train loss: 0.3009  @ epoch 26449 )\n",
      "[Epoch: 27140] train loss: 0.3139, train acc: 0.8737, val loss: 0.2446, val acc: 0.9140  (best train acc: 0.8828, best val acc: 0.9248, best train loss: 0.3009  @ epoch 26449 )\n",
      "[Epoch: 27160] train loss: 0.3083, train acc: 0.8745, val loss: 0.2351, val acc: 0.9147  (best train acc: 0.8828, best val acc: 0.9248, best train loss: 0.3009  @ epoch 26449 )\n",
      "[Epoch: 27180] train loss: 0.3149, train acc: 0.8723, val loss: 0.2615, val acc: 0.9046  (best train acc: 0.8828, best val acc: 0.9248, best train loss: 0.3009  @ epoch 26449 )\n",
      "[Epoch: 27200] train loss: 0.3179, train acc: 0.8702, val loss: 0.2364, val acc: 0.9160  (best train acc: 0.8828, best val acc: 0.9248, best train loss: 0.3009  @ epoch 26449 )\n",
      "[Epoch: 27220] train loss: 0.3162, train acc: 0.8762, val loss: 0.2495, val acc: 0.9140  (best train acc: 0.8828, best val acc: 0.9248, best train loss: 0.3009  @ epoch 26449 )\n",
      "[Epoch: 27240] train loss: 0.3245, train acc: 0.8670, val loss: 0.2418, val acc: 0.9177  (best train acc: 0.8828, best val acc: 0.9248, best train loss: 0.3009  @ epoch 26449 )\n",
      "[Epoch: 27260] train loss: 0.3042, train acc: 0.8787, val loss: 0.2395, val acc: 0.9153  (best train acc: 0.8828, best val acc: 0.9248, best train loss: 0.3009  @ epoch 26449 )\n",
      "[Epoch: 27280] train loss: 0.3210, train acc: 0.8745, val loss: 0.2413, val acc: 0.9218  (best train acc: 0.8828, best val acc: 0.9248, best train loss: 0.3009  @ epoch 26449 )\n",
      "[Epoch: 27300] train loss: 0.3192, train acc: 0.8721, val loss: 0.2389, val acc: 0.9194  (best train acc: 0.8828, best val acc: 0.9248, best train loss: 0.3009  @ epoch 26449 )\n",
      "[Epoch: 27320] train loss: 0.3254, train acc: 0.8691, val loss: 0.2440, val acc: 0.9133  (best train acc: 0.8828, best val acc: 0.9248, best train loss: 0.3009  @ epoch 26449 )\n",
      "[Epoch: 27340] train loss: 0.3221, train acc: 0.8741, val loss: 0.2478, val acc: 0.9096  (best train acc: 0.8828, best val acc: 0.9248, best train loss: 0.3009  @ epoch 26449 )\n",
      "[Epoch: 27360] train loss: 0.3218, train acc: 0.8643, val loss: 0.2417, val acc: 0.9174  (best train acc: 0.8828, best val acc: 0.9248, best train loss: 0.3009  @ epoch 26449 )\n",
      "[Epoch: 27380] train loss: 0.3412, train acc: 0.8578, val loss: 0.2484, val acc: 0.9211  (best train acc: 0.8828, best val acc: 0.9248, best train loss: 0.3009  @ epoch 26449 )\n",
      "[Epoch: 27400] train loss: 0.3234, train acc: 0.8612, val loss: 0.2395, val acc: 0.9167  (best train acc: 0.8828, best val acc: 0.9248, best train loss: 0.3009  @ epoch 26449 )\n",
      "[Epoch: 27420] train loss: 0.3264, train acc: 0.8621, val loss: 0.2445, val acc: 0.9126  (best train acc: 0.8828, best val acc: 0.9248, best train loss: 0.3009  @ epoch 26449 )\n",
      "[Epoch: 27440] train loss: 0.3201, train acc: 0.8718, val loss: 0.2385, val acc: 0.9170  (best train acc: 0.8828, best val acc: 0.9248, best train loss: 0.3009  @ epoch 26449 )\n",
      "[Epoch: 27460] train loss: 0.3151, train acc: 0.8738, val loss: 0.2449, val acc: 0.9167  (best train acc: 0.8828, best val acc: 0.9248, best train loss: 0.3009  @ epoch 26449 )\n",
      "[Epoch: 27480] train loss: 0.3223, train acc: 0.8651, val loss: 0.2657, val acc: 0.8988  (best train acc: 0.8828, best val acc: 0.9248, best train loss: 0.2992  @ epoch 27466 )\n",
      "[Epoch: 27500] train loss: 0.3302, train acc: 0.8651, val loss: 0.2477, val acc: 0.9157  (best train acc: 0.8828, best val acc: 0.9248, best train loss: 0.2992  @ epoch 27466 )\n",
      "[Epoch: 27520] train loss: 0.3154, train acc: 0.8727, val loss: 0.2386, val acc: 0.9194  (best train acc: 0.8828, best val acc: 0.9248, best train loss: 0.2992  @ epoch 27466 )\n",
      "[Epoch: 27540] train loss: 0.3117, train acc: 0.8772, val loss: 0.2384, val acc: 0.9187  (best train acc: 0.8828, best val acc: 0.9248, best train loss: 0.2992  @ epoch 27466 )\n",
      "[Epoch: 27560] train loss: 0.3125, train acc: 0.8777, val loss: 0.2399, val acc: 0.9184  (best train acc: 0.8828, best val acc: 0.9248, best train loss: 0.2992  @ epoch 27466 )\n",
      "[Epoch: 27580] train loss: 0.3016, train acc: 0.8766, val loss: 0.2339, val acc: 0.9207  (best train acc: 0.8828, best val acc: 0.9248, best train loss: 0.2992  @ epoch 27466 )\n",
      "[Epoch: 27600] train loss: 0.3107, train acc: 0.8764, val loss: 0.2417, val acc: 0.9143  (best train acc: 0.8828, best val acc: 0.9248, best train loss: 0.2992  @ epoch 27466 )\n",
      "[Epoch: 27620] train loss: 0.3056, train acc: 0.8776, val loss: 0.2314, val acc: 0.9184  (best train acc: 0.8828, best val acc: 0.9248, best train loss: 0.2992  @ epoch 27466 )\n",
      "[Epoch: 27640] train loss: 0.3274, train acc: 0.8702, val loss: 0.2519, val acc: 0.9106  (best train acc: 0.8828, best val acc: 0.9248, best train loss: 0.2992  @ epoch 27466 )\n",
      "[Epoch: 27660] train loss: 0.3124, train acc: 0.8730, val loss: 0.2515, val acc: 0.9113  (best train acc: 0.8828, best val acc: 0.9248, best train loss: 0.2992  @ epoch 27466 )\n",
      "[Epoch: 27680] train loss: 0.3093, train acc: 0.8754, val loss: 0.2389, val acc: 0.9191  (best train acc: 0.8828, best val acc: 0.9248, best train loss: 0.2992  @ epoch 27466 )\n",
      "[Epoch: 27700] train loss: 0.3211, train acc: 0.8770, val loss: 0.2436, val acc: 0.9201  (best train acc: 0.8828, best val acc: 0.9248, best train loss: 0.2992  @ epoch 27466 )\n",
      "[Epoch: 27720] train loss: 0.3042, train acc: 0.8788, val loss: 0.2503, val acc: 0.9120  (best train acc: 0.8828, best val acc: 0.9248, best train loss: 0.2992  @ epoch 27466 )\n",
      "[Epoch: 27740] train loss: 0.3112, train acc: 0.8754, val loss: 0.2490, val acc: 0.9133  (best train acc: 0.8828, best val acc: 0.9248, best train loss: 0.2992  @ epoch 27466 )\n",
      "[Epoch: 27760] train loss: 0.3146, train acc: 0.8701, val loss: 0.2350, val acc: 0.9187  (best train acc: 0.8828, best val acc: 0.9248, best train loss: 0.2984  @ epoch 27757 )\n",
      "[Epoch: 27780] train loss: 0.3210, train acc: 0.8720, val loss: 0.2409, val acc: 0.9191  (best train acc: 0.8828, best val acc: 0.9248, best train loss: 0.2984  @ epoch 27757 )\n",
      "[Epoch: 27800] train loss: 0.3162, train acc: 0.8724, val loss: 0.2415, val acc: 0.9177  (best train acc: 0.8828, best val acc: 0.9248, best train loss: 0.2984  @ epoch 27757 )\n",
      "[Epoch: 27820] train loss: 0.3199, train acc: 0.8695, val loss: 0.2465, val acc: 0.9147  (best train acc: 0.8828, best val acc: 0.9248, best train loss: 0.2984  @ epoch 27757 )\n",
      "[Epoch: 27840] train loss: 0.3249, train acc: 0.8670, val loss: 0.2461, val acc: 0.9191  (best train acc: 0.8828, best val acc: 0.9248, best train loss: 0.2984  @ epoch 27757 )\n",
      "[Epoch: 27860] train loss: 0.3122, train acc: 0.8743, val loss: 0.2448, val acc: 0.9153  (best train acc: 0.8828, best val acc: 0.9248, best train loss: 0.2984  @ epoch 27757 )\n",
      "[Epoch: 27880] train loss: 0.3121, train acc: 0.8712, val loss: 0.2425, val acc: 0.9180  (best train acc: 0.8828, best val acc: 0.9248, best train loss: 0.2984  @ epoch 27757 )\n",
      "[Epoch: 27900] train loss: 0.3086, train acc: 0.8749, val loss: 0.2401, val acc: 0.9184  (best train acc: 0.8828, best val acc: 0.9248, best train loss: 0.2984  @ epoch 27757 )\n",
      "[Epoch: 27920] train loss: 0.3127, train acc: 0.8728, val loss: 0.2517, val acc: 0.9133  (best train acc: 0.8828, best val acc: 0.9248, best train loss: 0.2984  @ epoch 27757 )\n",
      "[Epoch: 27940] train loss: 0.3216, train acc: 0.8705, val loss: 0.2461, val acc: 0.9187  (best train acc: 0.8828, best val acc: 0.9248, best train loss: 0.2984  @ epoch 27757 )\n",
      "[Epoch: 27960] train loss: 0.3058, train acc: 0.8747, val loss: 0.2434, val acc: 0.9153  (best train acc: 0.8828, best val acc: 0.9248, best train loss: 0.2984  @ epoch 27757 )\n",
      "[Epoch: 27980] train loss: 0.3098, train acc: 0.8751, val loss: 0.2439, val acc: 0.9194  (best train acc: 0.8828, best val acc: 0.9248, best train loss: 0.2984  @ epoch 27757 )\n",
      "[Epoch: 28000] train loss: 0.3226, train acc: 0.8655, val loss: 0.2700, val acc: 0.8981  (best train acc: 0.8828, best val acc: 0.9248, best train loss: 0.2984  @ epoch 27757 )\n",
      "[Epoch: 28020] train loss: 0.3135, train acc: 0.8725, val loss: 0.2430, val acc: 0.9140  (best train acc: 0.8828, best val acc: 0.9248, best train loss: 0.2984  @ epoch 27757 )\n",
      "[Epoch: 28040] train loss: 0.3144, train acc: 0.8745, val loss: 0.2481, val acc: 0.9187  (best train acc: 0.8828, best val acc: 0.9248, best train loss: 0.2984  @ epoch 27757 )\n",
      "[Epoch: 28060] train loss: 0.3197, train acc: 0.8671, val loss: 0.2468, val acc: 0.9110  (best train acc: 0.8828, best val acc: 0.9248, best train loss: 0.2984  @ epoch 27757 )\n",
      "[Epoch: 28080] train loss: 0.3121, train acc: 0.8723, val loss: 0.2467, val acc: 0.9191  (best train acc: 0.8828, best val acc: 0.9248, best train loss: 0.2984  @ epoch 27757 )\n",
      "[Epoch: 28100] train loss: 0.3143, train acc: 0.8711, val loss: 0.2406, val acc: 0.9201  (best train acc: 0.8828, best val acc: 0.9248, best train loss: 0.2984  @ epoch 27757 )\n",
      "[Epoch: 28120] train loss: 0.3199, train acc: 0.8672, val loss: 0.2435, val acc: 0.9160  (best train acc: 0.8828, best val acc: 0.9248, best train loss: 0.2984  @ epoch 27757 )\n",
      "[Epoch: 28140] train loss: 0.3144, train acc: 0.8765, val loss: 0.2442, val acc: 0.9191  (best train acc: 0.8828, best val acc: 0.9248, best train loss: 0.2984  @ epoch 27757 )\n",
      "[Epoch: 28160] train loss: 0.3218, train acc: 0.8687, val loss: 0.2417, val acc: 0.9187  (best train acc: 0.8828, best val acc: 0.9248, best train loss: 0.2984  @ epoch 27757 )\n",
      "[Epoch: 28180] train loss: 0.3277, train acc: 0.8670, val loss: 0.2428, val acc: 0.9157  (best train acc: 0.8828, best val acc: 0.9248, best train loss: 0.2984  @ epoch 27757 )\n",
      "[Epoch: 28200] train loss: 0.3294, train acc: 0.8653, val loss: 0.2436, val acc: 0.9204  (best train acc: 0.8828, best val acc: 0.9248, best train loss: 0.2984  @ epoch 27757 )\n",
      "[Epoch: 28220] train loss: 0.3218, train acc: 0.8705, val loss: 0.2424, val acc: 0.9160  (best train acc: 0.8828, best val acc: 0.9248, best train loss: 0.2984  @ epoch 27757 )\n",
      "[Epoch: 28240] train loss: 0.3064, train acc: 0.8762, val loss: 0.2456, val acc: 0.9143  (best train acc: 0.8828, best val acc: 0.9248, best train loss: 0.2984  @ epoch 27757 )\n",
      "[Epoch: 28260] train loss: 0.3145, train acc: 0.8746, val loss: 0.2412, val acc: 0.9197  (best train acc: 0.8828, best val acc: 0.9248, best train loss: 0.2984  @ epoch 27757 )\n",
      "[Epoch: 28280] train loss: 0.3388, train acc: 0.8600, val loss: 0.2544, val acc: 0.9079  (best train acc: 0.8828, best val acc: 0.9248, best train loss: 0.2984  @ epoch 27757 )\n",
      "[Epoch: 28300] train loss: 0.3210, train acc: 0.8720, val loss: 0.2391, val acc: 0.9184  (best train acc: 0.8828, best val acc: 0.9248, best train loss: 0.2984  @ epoch 27757 )\n",
      "[Epoch: 28320] train loss: 0.3105, train acc: 0.8759, val loss: 0.2437, val acc: 0.9211  (best train acc: 0.8828, best val acc: 0.9248, best train loss: 0.2984  @ epoch 27757 )\n",
      "[Epoch: 28340] train loss: 0.3115, train acc: 0.8724, val loss: 0.2445, val acc: 0.9194  (best train acc: 0.8828, best val acc: 0.9248, best train loss: 0.2984  @ epoch 27757 )\n",
      "[Epoch: 28360] train loss: 0.3205, train acc: 0.8684, val loss: 0.2464, val acc: 0.9137  (best train acc: 0.8828, best val acc: 0.9248, best train loss: 0.2984  @ epoch 27757 )\n",
      "[Epoch: 28380] train loss: 0.3109, train acc: 0.8723, val loss: 0.2474, val acc: 0.9157  (best train acc: 0.8828, best val acc: 0.9248, best train loss: 0.2984  @ epoch 27757 )\n",
      "[Epoch: 28400] train loss: 0.3141, train acc: 0.8717, val loss: 0.2539, val acc: 0.9093  (best train acc: 0.8828, best val acc: 0.9248, best train loss: 0.2984  @ epoch 27757 )\n",
      "[Epoch: 28420] train loss: 0.3288, train acc: 0.8702, val loss: 0.2587, val acc: 0.9143  (best train acc: 0.8828, best val acc: 0.9248, best train loss: 0.2984  @ epoch 27757 )\n",
      "[Epoch: 28440] train loss: 0.3184, train acc: 0.8734, val loss: 0.2376, val acc: 0.9211  (best train acc: 0.8828, best val acc: 0.9248, best train loss: 0.2984  @ epoch 27757 )\n",
      "[Epoch: 28460] train loss: 0.3079, train acc: 0.8786, val loss: 0.2417, val acc: 0.9207  (best train acc: 0.8828, best val acc: 0.9248, best train loss: 0.2984  @ epoch 27757 )\n",
      "[Epoch: 28480] train loss: 0.3182, train acc: 0.8670, val loss: 0.2437, val acc: 0.9191  (best train acc: 0.8828, best val acc: 0.9248, best train loss: 0.2984  @ epoch 27757 )\n",
      "[Epoch: 28500] train loss: 0.3236, train acc: 0.8733, val loss: 0.2455, val acc: 0.9180  (best train acc: 0.8828, best val acc: 0.9248, best train loss: 0.2984  @ epoch 27757 )\n",
      "[Epoch: 28520] train loss: 0.3486, train acc: 0.8596, val loss: 0.2538, val acc: 0.9083  (best train acc: 0.8828, best val acc: 0.9248, best train loss: 0.2984  @ epoch 27757 )\n",
      "[Epoch: 28540] train loss: 0.3137, train acc: 0.8742, val loss: 0.2427, val acc: 0.9133  (best train acc: 0.8828, best val acc: 0.9248, best train loss: 0.2984  @ epoch 27757 )\n",
      "[Epoch: 28560] train loss: 0.3198, train acc: 0.8708, val loss: 0.2501, val acc: 0.9137  (best train acc: 0.8828, best val acc: 0.9248, best train loss: 0.2984  @ epoch 27757 )\n",
      "[Epoch: 28580] train loss: 0.3143, train acc: 0.8733, val loss: 0.2469, val acc: 0.9174  (best train acc: 0.8828, best val acc: 0.9248, best train loss: 0.2984  @ epoch 27757 )\n",
      "[Epoch: 28600] train loss: 0.3227, train acc: 0.8627, val loss: 0.2438, val acc: 0.9164  (best train acc: 0.8828, best val acc: 0.9248, best train loss: 0.2984  @ epoch 27757 )\n",
      "[Epoch: 28620] train loss: 0.3217, train acc: 0.8668, val loss: 0.2464, val acc: 0.9160  (best train acc: 0.8828, best val acc: 0.9248, best train loss: 0.2984  @ epoch 27757 )\n",
      "[Epoch: 28640] train loss: 0.3171, train acc: 0.8750, val loss: 0.2434, val acc: 0.9180  (best train acc: 0.8828, best val acc: 0.9248, best train loss: 0.2984  @ epoch 27757 )\n",
      "[Epoch: 28660] train loss: 0.3147, train acc: 0.8715, val loss: 0.2445, val acc: 0.9160  (best train acc: 0.8828, best val acc: 0.9248, best train loss: 0.2984  @ epoch 27757 )\n",
      "[Epoch: 28680] train loss: 0.3118, train acc: 0.8751, val loss: 0.2580, val acc: 0.9150  (best train acc: 0.8828, best val acc: 0.9248, best train loss: 0.2984  @ epoch 27757 )\n",
      "[Epoch: 28700] train loss: 0.3113, train acc: 0.8733, val loss: 0.2504, val acc: 0.9201  (best train acc: 0.8828, best val acc: 0.9248, best train loss: 0.2984  @ epoch 27757 )\n",
      "[Epoch: 28720] train loss: 0.3098, train acc: 0.8769, val loss: 0.2466, val acc: 0.9170  (best train acc: 0.8828, best val acc: 0.9248, best train loss: 0.2984  @ epoch 27757 )\n",
      "[Epoch: 28740] train loss: 0.3223, train acc: 0.8720, val loss: 0.2472, val acc: 0.9184  (best train acc: 0.8828, best val acc: 0.9248, best train loss: 0.2984  @ epoch 27757 )\n",
      "[Epoch: 28760] train loss: 0.3355, train acc: 0.8629, val loss: 0.2447, val acc: 0.9167  (best train acc: 0.8828, best val acc: 0.9248, best train loss: 0.2984  @ epoch 27757 )\n",
      "[Epoch: 28780] train loss: 0.3226, train acc: 0.8693, val loss: 0.2495, val acc: 0.9157  (best train acc: 0.8828, best val acc: 0.9248, best train loss: 0.2984  @ epoch 27757 )\n",
      "[Epoch: 28800] train loss: 0.3135, train acc: 0.8702, val loss: 0.2444, val acc: 0.9174  (best train acc: 0.8828, best val acc: 0.9248, best train loss: 0.2984  @ epoch 27757 )\n",
      "[Epoch: 28820] train loss: 0.3127, train acc: 0.8755, val loss: 0.2432, val acc: 0.9197  (best train acc: 0.8828, best val acc: 0.9248, best train loss: 0.2984  @ epoch 27757 )\n",
      "[Epoch: 28840] train loss: 0.3175, train acc: 0.8704, val loss: 0.2479, val acc: 0.9147  (best train acc: 0.8828, best val acc: 0.9248, best train loss: 0.2984  @ epoch 27757 )\n",
      "[Epoch: 28860] train loss: 0.3022, train acc: 0.8798, val loss: 0.2428, val acc: 0.9191  (best train acc: 0.8828, best val acc: 0.9248, best train loss: 0.2984  @ epoch 27757 )\n",
      "[Epoch: 28880] train loss: 0.3308, train acc: 0.8610, val loss: 0.2547, val acc: 0.9073  (best train acc: 0.8828, best val acc: 0.9248, best train loss: 0.2984  @ epoch 27757 )\n",
      "[Epoch: 28900] train loss: 0.3205, train acc: 0.8710, val loss: 0.2398, val acc: 0.9187  (best train acc: 0.8828, best val acc: 0.9248, best train loss: 0.2984  @ epoch 27757 )\n",
      "[Epoch: 28920] train loss: 0.3046, train acc: 0.8807, val loss: 0.2435, val acc: 0.9201  (best train acc: 0.8828, best val acc: 0.9248, best train loss: 0.2984  @ epoch 27757 )\n",
      "[Epoch: 28940] train loss: 0.3028, train acc: 0.8772, val loss: 0.2428, val acc: 0.9177  (best train acc: 0.8828, best val acc: 0.9248, best train loss: 0.2984  @ epoch 27757 )\n",
      "[Epoch: 28960] train loss: 0.3155, train acc: 0.8652, val loss: 0.2467, val acc: 0.9147  (best train acc: 0.8828, best val acc: 0.9248, best train loss: 0.2984  @ epoch 27757 )\n",
      "[Epoch: 28980] train loss: 0.3150, train acc: 0.8720, val loss: 0.2476, val acc: 0.9116  (best train acc: 0.8828, best val acc: 0.9248, best train loss: 0.2984  @ epoch 27757 )\n",
      "[Epoch: 29000] train loss: 0.3058, train acc: 0.8777, val loss: 0.2430, val acc: 0.9191  (best train acc: 0.8828, best val acc: 0.9248, best train loss: 0.2984  @ epoch 27757 )\n",
      "[Epoch: 29020] train loss: 0.3250, train acc: 0.8645, val loss: 0.2475, val acc: 0.9140  (best train acc: 0.8828, best val acc: 0.9248, best train loss: 0.2984  @ epoch 27757 )\n",
      "[Epoch: 29040] train loss: 0.3189, train acc: 0.8686, val loss: 0.2473, val acc: 0.9201  (best train acc: 0.8828, best val acc: 0.9248, best train loss: 0.2984  @ epoch 27757 )\n",
      "[Epoch: 29060] train loss: 0.3132, train acc: 0.8726, val loss: 0.2501, val acc: 0.9143  (best train acc: 0.8828, best val acc: 0.9248, best train loss: 0.2984  @ epoch 27757 )\n",
      "[Epoch: 29080] train loss: 0.3036, train acc: 0.8775, val loss: 0.2442, val acc: 0.9140  (best train acc: 0.8828, best val acc: 0.9248, best train loss: 0.2984  @ epoch 27757 )\n",
      "[Epoch: 29100] train loss: 0.3092, train acc: 0.8780, val loss: 0.2449, val acc: 0.9174  (best train acc: 0.8828, best val acc: 0.9248, best train loss: 0.2984  @ epoch 27757 )\n",
      "[Epoch: 29120] train loss: 0.3184, train acc: 0.8757, val loss: 0.2468, val acc: 0.9184  (best train acc: 0.8828, best val acc: 0.9248, best train loss: 0.2984  @ epoch 27757 )\n",
      "[Epoch: 29140] train loss: 0.3244, train acc: 0.8707, val loss: 0.2390, val acc: 0.9174  (best train acc: 0.8828, best val acc: 0.9248, best train loss: 0.2984  @ epoch 27757 )\n",
      "[Epoch: 29160] train loss: 0.3133, train acc: 0.8703, val loss: 0.2401, val acc: 0.9224  (best train acc: 0.8828, best val acc: 0.9248, best train loss: 0.2984  @ epoch 27757 )\n",
      "[Epoch: 29180] train loss: 0.3053, train acc: 0.8807, val loss: 0.2476, val acc: 0.9157  (best train acc: 0.8828, best val acc: 0.9248, best train loss: 0.2974  @ epoch 29172 )\n",
      "[Epoch: 29200] train loss: 0.3147, train acc: 0.8694, val loss: 0.2446, val acc: 0.9180  (best train acc: 0.8828, best val acc: 0.9248, best train loss: 0.2974  @ epoch 29172 )\n",
      "[Epoch: 29220] train loss: 0.2968, train acc: 0.8849, val loss: 0.2487, val acc: 0.9153  (best train acc: 0.8849, best val acc: 0.9248, best train loss: 0.2968  @ epoch 29220 )\n",
      "[Epoch: 29240] train loss: 0.3256, train acc: 0.8658, val loss: 0.2592, val acc: 0.9120  (best train acc: 0.8849, best val acc: 0.9248, best train loss: 0.2968  @ epoch 29220 )\n",
      "[Epoch: 29260] train loss: 0.3144, train acc: 0.8683, val loss: 0.2420, val acc: 0.9197  (best train acc: 0.8849, best val acc: 0.9248, best train loss: 0.2968  @ epoch 29220 )\n",
      "[Epoch: 29280] train loss: 0.3150, train acc: 0.8707, val loss: 0.2409, val acc: 0.9194  (best train acc: 0.8849, best val acc: 0.9248, best train loss: 0.2968  @ epoch 29220 )\n",
      "[Epoch: 29300] train loss: 0.3276, train acc: 0.8628, val loss: 0.2432, val acc: 0.9177  (best train acc: 0.8849, best val acc: 0.9248, best train loss: 0.2968  @ epoch 29220 )\n",
      "[Epoch: 29320] train loss: 0.3103, train acc: 0.8729, val loss: 0.2399, val acc: 0.9150  (best train acc: 0.8849, best val acc: 0.9248, best train loss: 0.2968  @ epoch 29220 )\n",
      "[Epoch: 29340] train loss: 0.3120, train acc: 0.8706, val loss: 0.2380, val acc: 0.9174  (best train acc: 0.8849, best val acc: 0.9248, best train loss: 0.2968  @ epoch 29220 )\n",
      "[Epoch: 29360] train loss: 0.3239, train acc: 0.8631, val loss: 0.2406, val acc: 0.9167  (best train acc: 0.8849, best val acc: 0.9248, best train loss: 0.2968  @ epoch 29220 )\n",
      "[Epoch: 29380] train loss: 0.3068, train acc: 0.8785, val loss: 0.2371, val acc: 0.9211  (best train acc: 0.8849, best val acc: 0.9248, best train loss: 0.2968  @ epoch 29220 )\n",
      "[Epoch: 29400] train loss: 0.3345, train acc: 0.8663, val loss: 0.2670, val acc: 0.9019  (best train acc: 0.8849, best val acc: 0.9248, best train loss: 0.2968  @ epoch 29220 )\n",
      "[Epoch: 29420] train loss: 0.3102, train acc: 0.8756, val loss: 0.2408, val acc: 0.9201  (best train acc: 0.8849, best val acc: 0.9248, best train loss: 0.2968  @ epoch 29220 )\n",
      "[Epoch: 29440] train loss: 0.3255, train acc: 0.8693, val loss: 0.2504, val acc: 0.9116  (best train acc: 0.8849, best val acc: 0.9248, best train loss: 0.2968  @ epoch 29220 )\n",
      "[Epoch: 29460] train loss: 0.3093, train acc: 0.8747, val loss: 0.2446, val acc: 0.9184  (best train acc: 0.8849, best val acc: 0.9248, best train loss: 0.2968  @ epoch 29220 )\n",
      "[Epoch: 29480] train loss: 0.3189, train acc: 0.8670, val loss: 0.2424, val acc: 0.9197  (best train acc: 0.8849, best val acc: 0.9248, best train loss: 0.2968  @ epoch 29220 )\n",
      "[Epoch: 29500] train loss: 0.3154, train acc: 0.8745, val loss: 0.2590, val acc: 0.9133  (best train acc: 0.8849, best val acc: 0.9248, best train loss: 0.2968  @ epoch 29220 )\n",
      "[Epoch: 29520] train loss: 0.3248, train acc: 0.8681, val loss: 0.2495, val acc: 0.9150  (best train acc: 0.8849, best val acc: 0.9248, best train loss: 0.2968  @ epoch 29220 )\n",
      "[Epoch: 29540] train loss: 0.3135, train acc: 0.8735, val loss: 0.2447, val acc: 0.9207  (best train acc: 0.8849, best val acc: 0.9248, best train loss: 0.2968  @ epoch 29220 )\n",
      "[Epoch: 29560] train loss: 0.3247, train acc: 0.8695, val loss: 0.2564, val acc: 0.9113  (best train acc: 0.8849, best val acc: 0.9248, best train loss: 0.2968  @ epoch 29220 )\n",
      "[Epoch: 29580] train loss: 0.3188, train acc: 0.8700, val loss: 0.2499, val acc: 0.9126  (best train acc: 0.8849, best val acc: 0.9248, best train loss: 0.2968  @ epoch 29220 )\n",
      "[Epoch: 29600] train loss: 0.3152, train acc: 0.8714, val loss: 0.2466, val acc: 0.9194  (best train acc: 0.8849, best val acc: 0.9248, best train loss: 0.2968  @ epoch 29220 )\n",
      "[Epoch: 29620] train loss: 0.3232, train acc: 0.8681, val loss: 0.2441, val acc: 0.9174  (best train acc: 0.8849, best val acc: 0.9248, best train loss: 0.2968  @ epoch 29220 )\n",
      "[Epoch: 29640] train loss: 0.3125, train acc: 0.8746, val loss: 0.2452, val acc: 0.9184  (best train acc: 0.8849, best val acc: 0.9248, best train loss: 0.2968  @ epoch 29220 )\n",
      "[Epoch: 29660] train loss: 0.3189, train acc: 0.8724, val loss: 0.2441, val acc: 0.9207  (best train acc: 0.8849, best val acc: 0.9248, best train loss: 0.2968  @ epoch 29220 )\n",
      "[Epoch: 29680] train loss: 0.3140, train acc: 0.8749, val loss: 0.2553, val acc: 0.9093  (best train acc: 0.8849, best val acc: 0.9248, best train loss: 0.2968  @ epoch 29220 )\n",
      "[Epoch: 29700] train loss: 0.3147, train acc: 0.8727, val loss: 0.2442, val acc: 0.9157  (best train acc: 0.8849, best val acc: 0.9248, best train loss: 0.2968  @ epoch 29220 )\n",
      "[Epoch: 29720] train loss: 0.3244, train acc: 0.8642, val loss: 0.2513, val acc: 0.9123  (best train acc: 0.8849, best val acc: 0.9248, best train loss: 0.2968  @ epoch 29220 )\n",
      "[Epoch: 29740] train loss: 0.3231, train acc: 0.8627, val loss: 0.2422, val acc: 0.9204  (best train acc: 0.8849, best val acc: 0.9248, best train loss: 0.2968  @ epoch 29220 )\n",
      "[Epoch: 29760] train loss: 0.3033, train acc: 0.8799, val loss: 0.2419, val acc: 0.9191  (best train acc: 0.8849, best val acc: 0.9248, best train loss: 0.2957  @ epoch 29753 )\n",
      "[Epoch: 29780] train loss: 0.3316, train acc: 0.8660, val loss: 0.2615, val acc: 0.9099  (best train acc: 0.8849, best val acc: 0.9248, best train loss: 0.2957  @ epoch 29753 )\n",
      "[Epoch: 29800] train loss: 0.3153, train acc: 0.8707, val loss: 0.2461, val acc: 0.9174  (best train acc: 0.8849, best val acc: 0.9248, best train loss: 0.2957  @ epoch 29753 )\n",
      "[Epoch: 29820] train loss: 0.3108, train acc: 0.8762, val loss: 0.2432, val acc: 0.9211  (best train acc: 0.8849, best val acc: 0.9248, best train loss: 0.2957  @ epoch 29753 )\n",
      "[Epoch: 29840] train loss: 0.3126, train acc: 0.8694, val loss: 0.2449, val acc: 0.9177  (best train acc: 0.8849, best val acc: 0.9248, best train loss: 0.2957  @ epoch 29753 )\n",
      "[Epoch: 29860] train loss: 0.3042, train acc: 0.8798, val loss: 0.2427, val acc: 0.9207  (best train acc: 0.8849, best val acc: 0.9248, best train loss: 0.2957  @ epoch 29753 )\n",
      "[Epoch: 29880] train loss: 0.3290, train acc: 0.8587, val loss: 0.2527, val acc: 0.9106  (best train acc: 0.8849, best val acc: 0.9248, best train loss: 0.2957  @ epoch 29753 )\n",
      "[Epoch: 29900] train loss: 0.3150, train acc: 0.8742, val loss: 0.2424, val acc: 0.9184  (best train acc: 0.8849, best val acc: 0.9248, best train loss: 0.2957  @ epoch 29753 )\n",
      "[Epoch: 29920] train loss: 0.3193, train acc: 0.8737, val loss: 0.2464, val acc: 0.9167  (best train acc: 0.8849, best val acc: 0.9248, best train loss: 0.2957  @ epoch 29753 )\n",
      "[Epoch: 29940] train loss: 0.3275, train acc: 0.8648, val loss: 0.2374, val acc: 0.9207  (best train acc: 0.8849, best val acc: 0.9248, best train loss: 0.2957  @ epoch 29753 )\n",
      "[Epoch: 29960] train loss: 0.3217, train acc: 0.8660, val loss: 0.2433, val acc: 0.9143  (best train acc: 0.8849, best val acc: 0.9248, best train loss: 0.2957  @ epoch 29753 )\n",
      "[Epoch: 29980] train loss: 0.3127, train acc: 0.8732, val loss: 0.2455, val acc: 0.9164  (best train acc: 0.8849, best val acc: 0.9248, best train loss: 0.2957  @ epoch 29753 )\n",
      "[Epoch: 30000] train loss: 0.3143, train acc: 0.8702, val loss: 0.2412, val acc: 0.9184  (best train acc: 0.8849, best val acc: 0.9248, best train loss: 0.2957  @ epoch 29753 )\n",
      "[Epoch: 30020] train loss: 0.3250, train acc: 0.8654, val loss: 0.2550, val acc: 0.9116  (best train acc: 0.8849, best val acc: 0.9248, best train loss: 0.2957  @ epoch 29753 )\n",
      "[Epoch: 30040] train loss: 0.3114, train acc: 0.8751, val loss: 0.2451, val acc: 0.9180  (best train acc: 0.8849, best val acc: 0.9248, best train loss: 0.2957  @ epoch 29753 )\n",
      "[Epoch: 30060] train loss: 0.3082, train acc: 0.8759, val loss: 0.2412, val acc: 0.9197  (best train acc: 0.8849, best val acc: 0.9248, best train loss: 0.2957  @ epoch 29753 )\n",
      "[Epoch: 30080] train loss: 0.3065, train acc: 0.8771, val loss: 0.2469, val acc: 0.9140  (best train acc: 0.8849, best val acc: 0.9248, best train loss: 0.2957  @ epoch 29753 )\n",
      "[Epoch: 30100] train loss: 0.3164, train acc: 0.8720, val loss: 0.2488, val acc: 0.9177  (best train acc: 0.8849, best val acc: 0.9248, best train loss: 0.2957  @ epoch 29753 )\n",
      "[Epoch: 30120] train loss: 0.3078, train acc: 0.8745, val loss: 0.2500, val acc: 0.9133  (best train acc: 0.8849, best val acc: 0.9248, best train loss: 0.2957  @ epoch 29753 )\n",
      "[Epoch: 30140] train loss: 0.3134, train acc: 0.8741, val loss: 0.2441, val acc: 0.9191  (best train acc: 0.8849, best val acc: 0.9248, best train loss: 0.2957  @ epoch 29753 )\n",
      "[Epoch: 30160] train loss: 0.3298, train acc: 0.8657, val loss: 0.2406, val acc: 0.9191  (best train acc: 0.8849, best val acc: 0.9248, best train loss: 0.2957  @ epoch 29753 )\n",
      "[Epoch: 30180] train loss: 0.3054, train acc: 0.8792, val loss: 0.2499, val acc: 0.9113  (best train acc: 0.8849, best val acc: 0.9248, best train loss: 0.2957  @ epoch 29753 )\n",
      "[Epoch: 30200] train loss: 0.3163, train acc: 0.8684, val loss: 0.2509, val acc: 0.9089  (best train acc: 0.8849, best val acc: 0.9248, best train loss: 0.2957  @ epoch 29753 )\n",
      "[Epoch: 30220] train loss: 0.3270, train acc: 0.8660, val loss: 0.2543, val acc: 0.9106  (best train acc: 0.8849, best val acc: 0.9248, best train loss: 0.2957  @ epoch 29753 )\n",
      "[Epoch: 30240] train loss: 0.3255, train acc: 0.8673, val loss: 0.2560, val acc: 0.9184  (best train acc: 0.8849, best val acc: 0.9248, best train loss: 0.2957  @ epoch 29753 )\n",
      "[Epoch: 30260] train loss: 0.3134, train acc: 0.8694, val loss: 0.2481, val acc: 0.9167  (best train acc: 0.8849, best val acc: 0.9248, best train loss: 0.2957  @ epoch 29753 )\n",
      "[Epoch: 30280] train loss: 0.3176, train acc: 0.8706, val loss: 0.2487, val acc: 0.9083  (best train acc: 0.8849, best val acc: 0.9248, best train loss: 0.2957  @ epoch 29753 )\n",
      "[Epoch: 30300] train loss: 0.3094, train acc: 0.8773, val loss: 0.2437, val acc: 0.9191  (best train acc: 0.8849, best val acc: 0.9248, best train loss: 0.2957  @ epoch 29753 )\n",
      "[Epoch: 30320] train loss: 0.3200, train acc: 0.8660, val loss: 0.2489, val acc: 0.9218  (best train acc: 0.8849, best val acc: 0.9248, best train loss: 0.2957  @ epoch 29753 )\n",
      "[Epoch: 30340] train loss: 0.3165, train acc: 0.8717, val loss: 0.2482, val acc: 0.9140  (best train acc: 0.8849, best val acc: 0.9248, best train loss: 0.2957  @ epoch 29753 )\n",
      "[Epoch: 30360] train loss: 0.3341, train acc: 0.8584, val loss: 0.2619, val acc: 0.9133  (best train acc: 0.8849, best val acc: 0.9248, best train loss: 0.2957  @ epoch 29753 )\n",
      "[Epoch: 30380] train loss: 0.3120, train acc: 0.8754, val loss: 0.2421, val acc: 0.9177  (best train acc: 0.8849, best val acc: 0.9248, best train loss: 0.2957  @ epoch 29753 )\n",
      "[Epoch: 30400] train loss: 0.3100, train acc: 0.8726, val loss: 0.2419, val acc: 0.9214  (best train acc: 0.8849, best val acc: 0.9248, best train loss: 0.2957  @ epoch 29753 )\n",
      "[Epoch: 30420] train loss: 0.3065, train acc: 0.8770, val loss: 0.2425, val acc: 0.9214  (best train acc: 0.8849, best val acc: 0.9248, best train loss: 0.2957  @ epoch 29753 )\n",
      "[Epoch: 30440] train loss: 0.3128, train acc: 0.8710, val loss: 0.2429, val acc: 0.9191  (best train acc: 0.8849, best val acc: 0.9248, best train loss: 0.2957  @ epoch 29753 )\n",
      "[Epoch: 30460] train loss: 0.3093, train acc: 0.8743, val loss: 0.2440, val acc: 0.9174  (best train acc: 0.8849, best val acc: 0.9248, best train loss: 0.2957  @ epoch 29753 )\n",
      "[Epoch: 30480] train loss: 0.3089, train acc: 0.8752, val loss: 0.2399, val acc: 0.9194  (best train acc: 0.8849, best val acc: 0.9248, best train loss: 0.2957  @ epoch 29753 )\n",
      "[Epoch: 30500] train loss: 0.3155, train acc: 0.8726, val loss: 0.2478, val acc: 0.9147  (best train acc: 0.8849, best val acc: 0.9248, best train loss: 0.2957  @ epoch 29753 )\n",
      "[Epoch: 30520] train loss: 0.3139, train acc: 0.8707, val loss: 0.2537, val acc: 0.9089  (best train acc: 0.8849, best val acc: 0.9248, best train loss: 0.2957  @ epoch 29753 )\n",
      "[Epoch: 30540] train loss: 0.3146, train acc: 0.8741, val loss: 0.2500, val acc: 0.9174  (best train acc: 0.8849, best val acc: 0.9248, best train loss: 0.2957  @ epoch 29753 )\n",
      "[Epoch: 30560] train loss: 0.3303, train acc: 0.8697, val loss: 0.2491, val acc: 0.9143  (best train acc: 0.8849, best val acc: 0.9248, best train loss: 0.2957  @ epoch 29753 )\n",
      "[Epoch: 30580] train loss: 0.3204, train acc: 0.8697, val loss: 0.2407, val acc: 0.9194  (best train acc: 0.8849, best val acc: 0.9248, best train loss: 0.2957  @ epoch 29753 )\n",
      "[Epoch: 30600] train loss: 0.3018, train acc: 0.8824, val loss: 0.2457, val acc: 0.9160  (best train acc: 0.8849, best val acc: 0.9248, best train loss: 0.2957  @ epoch 29753 )\n",
      "[Epoch: 30620] train loss: 0.3311, train acc: 0.8700, val loss: 0.2492, val acc: 0.9113  (best train acc: 0.8849, best val acc: 0.9248, best train loss: 0.2957  @ epoch 29753 )\n",
      "[Epoch: 30640] train loss: 0.3158, train acc: 0.8754, val loss: 0.2489, val acc: 0.9167  (best train acc: 0.8849, best val acc: 0.9248, best train loss: 0.2957  @ epoch 29753 )\n",
      "[Epoch: 30660] train loss: 0.3100, train acc: 0.8751, val loss: 0.2460, val acc: 0.9157  (best train acc: 0.8849, best val acc: 0.9248, best train loss: 0.2957  @ epoch 29753 )\n",
      "[Epoch: 30680] train loss: 0.3143, train acc: 0.8728, val loss: 0.2476, val acc: 0.9167  (best train acc: 0.8849, best val acc: 0.9248, best train loss: 0.2957  @ epoch 29753 )\n",
      "[Epoch: 30700] train loss: 0.3073, train acc: 0.8758, val loss: 0.2460, val acc: 0.9204  (best train acc: 0.8849, best val acc: 0.9248, best train loss: 0.2957  @ epoch 29753 )\n",
      "[Epoch: 30720] train loss: 0.3104, train acc: 0.8754, val loss: 0.2442, val acc: 0.9207  (best train acc: 0.8849, best val acc: 0.9248, best train loss: 0.2957  @ epoch 29753 )\n",
      "[Epoch: 30740] train loss: 0.3092, train acc: 0.8775, val loss: 0.2435, val acc: 0.9218  (best train acc: 0.8849, best val acc: 0.9248, best train loss: 0.2957  @ epoch 29753 )\n",
      "[Epoch: 30760] train loss: 0.3085, train acc: 0.8727, val loss: 0.2524, val acc: 0.9153  (best train acc: 0.8849, best val acc: 0.9248, best train loss: 0.2957  @ epoch 29753 )\n",
      "[Epoch: 30780] train loss: 0.3378, train acc: 0.8610, val loss: 0.2454, val acc: 0.9160  (best train acc: 0.8849, best val acc: 0.9248, best train loss: 0.2957  @ epoch 29753 )\n",
      "[Epoch: 30800] train loss: 0.3168, train acc: 0.8695, val loss: 0.2411, val acc: 0.9184  (best train acc: 0.8849, best val acc: 0.9248, best train loss: 0.2957  @ epoch 29753 )\n",
      "[Epoch: 30820] train loss: 0.3060, train acc: 0.8697, val loss: 0.2397, val acc: 0.9157  (best train acc: 0.8849, best val acc: 0.9248, best train loss: 0.2957  @ epoch 29753 )\n",
      "[Epoch: 30840] train loss: 0.3061, train acc: 0.8749, val loss: 0.2450, val acc: 0.9177  (best train acc: 0.8849, best val acc: 0.9248, best train loss: 0.2957  @ epoch 29753 )\n",
      "[Epoch: 30860] train loss: 0.3270, train acc: 0.8681, val loss: 0.2653, val acc: 0.9180  (best train acc: 0.8849, best val acc: 0.9248, best train loss: 0.2957  @ epoch 29753 )\n",
      "[Epoch: 30880] train loss: 0.3192, train acc: 0.8689, val loss: 0.2485, val acc: 0.9167  (best train acc: 0.8849, best val acc: 0.9248, best train loss: 0.2957  @ epoch 29753 )\n",
      "[Epoch: 30900] train loss: 0.3076, train acc: 0.8761, val loss: 0.2456, val acc: 0.9177  (best train acc: 0.8849, best val acc: 0.9248, best train loss: 0.2957  @ epoch 29753 )\n",
      "[Epoch: 30920] train loss: 0.3055, train acc: 0.8809, val loss: 0.2516, val acc: 0.9160  (best train acc: 0.8849, best val acc: 0.9248, best train loss: 0.2957  @ epoch 29753 )\n",
      "[Epoch: 30940] train loss: 0.3169, train acc: 0.8660, val loss: 0.2471, val acc: 0.9194  (best train acc: 0.8849, best val acc: 0.9248, best train loss: 0.2957  @ epoch 29753 )\n",
      "[Epoch: 30960] train loss: 0.3411, train acc: 0.8597, val loss: 0.2472, val acc: 0.9160  (best train acc: 0.8849, best val acc: 0.9248, best train loss: 0.2957  @ epoch 29753 )\n",
      "[Epoch: 30980] train loss: 0.3151, train acc: 0.8694, val loss: 0.2456, val acc: 0.9174  (best train acc: 0.8849, best val acc: 0.9248, best train loss: 0.2957  @ epoch 29753 )\n",
      "[Epoch: 31000] train loss: 0.3139, train acc: 0.8719, val loss: 0.2340, val acc: 0.9180  (best train acc: 0.8849, best val acc: 0.9248, best train loss: 0.2957  @ epoch 29753 )\n",
      "[Epoch: 31020] train loss: 0.3146, train acc: 0.8743, val loss: 0.2340, val acc: 0.9204  (best train acc: 0.8849, best val acc: 0.9248, best train loss: 0.2957  @ epoch 29753 )\n",
      "[Epoch: 31040] train loss: 0.3140, train acc: 0.8759, val loss: 0.2469, val acc: 0.9106  (best train acc: 0.8849, best val acc: 0.9248, best train loss: 0.2957  @ epoch 29753 )\n",
      "[Epoch: 31060] train loss: 0.3069, train acc: 0.8738, val loss: 0.2363, val acc: 0.9167  (best train acc: 0.8849, best val acc: 0.9248, best train loss: 0.2957  @ epoch 29753 )\n",
      "[Epoch: 31080] train loss: 0.3212, train acc: 0.8722, val loss: 0.2369, val acc: 0.9234  (best train acc: 0.8849, best val acc: 0.9248, best train loss: 0.2957  @ epoch 29753 )\n",
      "[Epoch: 31100] train loss: 0.3101, train acc: 0.8792, val loss: 0.2438, val acc: 0.9164  (best train acc: 0.8849, best val acc: 0.9248, best train loss: 0.2957  @ epoch 29753 )\n",
      "[Epoch: 31120] train loss: 0.3139, train acc: 0.8731, val loss: 0.2422, val acc: 0.9180  (best train acc: 0.8849, best val acc: 0.9248, best train loss: 0.2957  @ epoch 29753 )\n",
      "[Epoch: 31140] train loss: 0.3202, train acc: 0.8684, val loss: 0.2403, val acc: 0.9174  (best train acc: 0.8849, best val acc: 0.9248, best train loss: 0.2957  @ epoch 29753 )\n",
      "[Epoch: 31160] train loss: 0.3095, train acc: 0.8720, val loss: 0.2535, val acc: 0.9096  (best train acc: 0.8849, best val acc: 0.9248, best train loss: 0.2957  @ epoch 29753 )\n",
      "[Epoch: 31180] train loss: 0.3172, train acc: 0.8699, val loss: 0.2407, val acc: 0.9197  (best train acc: 0.8849, best val acc: 0.9248, best train loss: 0.2957  @ epoch 29753 )\n",
      "[Epoch: 31200] train loss: 0.3204, train acc: 0.8700, val loss: 0.2429, val acc: 0.9214  (best train acc: 0.8849, best val acc: 0.9248, best train loss: 0.2957  @ epoch 29753 )\n",
      "[Epoch: 31220] train loss: 0.3116, train acc: 0.8741, val loss: 0.2440, val acc: 0.9201  (best train acc: 0.8849, best val acc: 0.9248, best train loss: 0.2957  @ epoch 29753 )\n",
      "[Epoch: 31240] train loss: 0.3155, train acc: 0.8710, val loss: 0.2471, val acc: 0.9194  (best train acc: 0.8849, best val acc: 0.9248, best train loss: 0.2957  @ epoch 29753 )\n",
      "[Epoch: 31260] train loss: 0.3182, train acc: 0.8732, val loss: 0.2516, val acc: 0.9194  (best train acc: 0.8849, best val acc: 0.9248, best train loss: 0.2957  @ epoch 29753 )\n",
      "[Epoch: 31280] train loss: 0.3183, train acc: 0.8710, val loss: 0.2437, val acc: 0.9187  (best train acc: 0.8849, best val acc: 0.9248, best train loss: 0.2957  @ epoch 29753 )\n",
      "[Epoch: 31300] train loss: 0.3074, train acc: 0.8759, val loss: 0.2416, val acc: 0.9204  (best train acc: 0.8849, best val acc: 0.9248, best train loss: 0.2957  @ epoch 29753 )\n",
      "[Epoch: 31320] train loss: 0.3067, train acc: 0.8809, val loss: 0.2527, val acc: 0.9167  (best train acc: 0.8849, best val acc: 0.9248, best train loss: 0.2957  @ epoch 29753 )\n",
      "[Epoch: 31340] train loss: 0.3129, train acc: 0.8713, val loss: 0.2582, val acc: 0.9076  (best train acc: 0.8849, best val acc: 0.9248, best train loss: 0.2957  @ epoch 29753 )\n",
      "[Epoch: 31360] train loss: 0.3124, train acc: 0.8707, val loss: 0.2444, val acc: 0.9160  (best train acc: 0.8849, best val acc: 0.9248, best train loss: 0.2957  @ epoch 29753 )\n",
      "[Epoch: 31380] train loss: 0.3336, train acc: 0.8602, val loss: 0.2529, val acc: 0.9231  (best train acc: 0.8849, best val acc: 0.9248, best train loss: 0.2957  @ epoch 29753 )\n",
      "[Epoch: 31400] train loss: 0.3068, train acc: 0.8759, val loss: 0.2468, val acc: 0.9153  (best train acc: 0.8849, best val acc: 0.9248, best train loss: 0.2957  @ epoch 29753 )\n",
      "[Epoch: 31420] train loss: 0.3074, train acc: 0.8759, val loss: 0.2409, val acc: 0.9201  (best train acc: 0.8849, best val acc: 0.9248, best train loss: 0.2957  @ epoch 29753 )\n",
      "[Epoch: 31440] train loss: 0.3125, train acc: 0.8766, val loss: 0.2408, val acc: 0.9180  (best train acc: 0.8849, best val acc: 0.9248, best train loss: 0.2957  @ epoch 29753 )\n",
      "[Epoch: 31460] train loss: 0.3112, train acc: 0.8720, val loss: 0.2429, val acc: 0.9167  (best train acc: 0.8849, best val acc: 0.9248, best train loss: 0.2957  @ epoch 29753 )\n",
      "[Epoch: 31480] train loss: 0.3171, train acc: 0.8690, val loss: 0.2477, val acc: 0.9177  (best train acc: 0.8849, best val acc: 0.9248, best train loss: 0.2957  @ epoch 29753 )\n",
      "[Epoch: 31500] train loss: 0.3111, train acc: 0.8752, val loss: 0.2458, val acc: 0.9187  (best train acc: 0.8849, best val acc: 0.9248, best train loss: 0.2957  @ epoch 29753 )\n",
      "[Epoch: 31520] train loss: 0.3013, train acc: 0.8773, val loss: 0.2495, val acc: 0.9133  (best train acc: 0.8849, best val acc: 0.9248, best train loss: 0.2957  @ epoch 29753 )\n",
      "[Epoch: 31540] train loss: 0.3161, train acc: 0.8694, val loss: 0.2473, val acc: 0.9187  (best train acc: 0.8849, best val acc: 0.9248, best train loss: 0.2957  @ epoch 29753 )\n",
      "[Epoch: 31560] train loss: 0.3072, train acc: 0.8732, val loss: 0.2423, val acc: 0.9201  (best train acc: 0.8849, best val acc: 0.9248, best train loss: 0.2957  @ epoch 29753 )\n",
      "[Epoch: 31580] train loss: 0.3148, train acc: 0.8729, val loss: 0.2413, val acc: 0.9201  (best train acc: 0.8849, best val acc: 0.9248, best train loss: 0.2957  @ epoch 29753 )\n",
      "[Epoch: 31600] train loss: 0.3156, train acc: 0.8738, val loss: 0.2443, val acc: 0.9194  (best train acc: 0.8849, best val acc: 0.9248, best train loss: 0.2957  @ epoch 29753 )\n",
      "[Epoch: 31620] train loss: 0.3076, train acc: 0.8738, val loss: 0.2436, val acc: 0.9177  (best train acc: 0.8849, best val acc: 0.9248, best train loss: 0.2957  @ epoch 29753 )\n",
      "[Epoch: 31640] train loss: 0.3160, train acc: 0.8733, val loss: 0.2568, val acc: 0.9120  (best train acc: 0.8849, best val acc: 0.9248, best train loss: 0.2957  @ epoch 29753 )\n",
      "[Epoch: 31660] train loss: 0.3244, train acc: 0.8660, val loss: 0.2418, val acc: 0.9167  (best train acc: 0.8849, best val acc: 0.9248, best train loss: 0.2957  @ epoch 29753 )\n",
      "[Epoch: 31680] train loss: 0.3300, train acc: 0.8676, val loss: 0.2757, val acc: 0.9042  (best train acc: 0.8849, best val acc: 0.9248, best train loss: 0.2957  @ epoch 29753 )\n",
      "[Epoch: 31700] train loss: 0.3171, train acc: 0.8705, val loss: 0.2484, val acc: 0.9174  (best train acc: 0.8849, best val acc: 0.9248, best train loss: 0.2957  @ epoch 29753 )\n",
      "[Epoch: 31720] train loss: 0.3327, train acc: 0.8664, val loss: 0.2540, val acc: 0.9083  (best train acc: 0.8849, best val acc: 0.9248, best train loss: 0.2957  @ epoch 29753 )\n",
      "[Epoch: 31740] train loss: 0.3172, train acc: 0.8763, val loss: 0.2471, val acc: 0.9174  (best train acc: 0.8849, best val acc: 0.9248, best train loss: 0.2957  @ epoch 29753 )\n",
      "[Epoch: 31760] train loss: 0.3127, train acc: 0.8760, val loss: 0.2497, val acc: 0.9143  (best train acc: 0.8849, best val acc: 0.9248, best train loss: 0.2957  @ epoch 29753 )\n",
      "[Epoch: 31780] train loss: 0.3071, train acc: 0.8796, val loss: 0.2441, val acc: 0.9214  (best train acc: 0.8849, best val acc: 0.9248, best train loss: 0.2957  @ epoch 29753 )\n",
      "[Epoch: 31800] train loss: 0.3241, train acc: 0.8691, val loss: 0.2519, val acc: 0.9201  (best train acc: 0.8849, best val acc: 0.9248, best train loss: 0.2957  @ epoch 29753 )\n",
      "[Epoch: 31820] train loss: 0.3099, train acc: 0.8755, val loss: 0.2415, val acc: 0.9197  (best train acc: 0.8849, best val acc: 0.9248, best train loss: 0.2957  @ epoch 29753 )\n",
      "[Epoch: 31840] train loss: 0.3084, train acc: 0.8749, val loss: 0.2406, val acc: 0.9150  (best train acc: 0.8849, best val acc: 0.9248, best train loss: 0.2957  @ epoch 29753 )\n",
      "[Epoch: 31860] train loss: 0.3088, train acc: 0.8764, val loss: 0.2474, val acc: 0.9177  (best train acc: 0.8854, best val acc: 0.9248, best train loss: 0.2955  @ epoch 31853 )\n",
      "[Epoch: 31880] train loss: 0.3126, train acc: 0.8728, val loss: 0.2500, val acc: 0.9177  (best train acc: 0.8854, best val acc: 0.9248, best train loss: 0.2955  @ epoch 31853 )\n",
      "[Epoch: 31900] train loss: 0.3233, train acc: 0.8680, val loss: 0.2509, val acc: 0.9130  (best train acc: 0.8854, best val acc: 0.9248, best train loss: 0.2955  @ epoch 31853 )\n",
      "[Epoch: 31920] train loss: 0.3148, train acc: 0.8712, val loss: 0.2464, val acc: 0.9218  (best train acc: 0.8854, best val acc: 0.9248, best train loss: 0.2955  @ epoch 31853 )\n",
      "[Epoch: 31940] train loss: 0.3213, train acc: 0.8714, val loss: 0.2433, val acc: 0.9184  (best train acc: 0.8854, best val acc: 0.9248, best train loss: 0.2955  @ epoch 31853 )\n",
      "[Epoch: 31960] train loss: 0.3223, train acc: 0.8720, val loss: 0.2538, val acc: 0.9106  (best train acc: 0.8866, best val acc: 0.9248, best train loss: 0.2941  @ epoch 31956 )\n",
      "[Epoch: 31980] train loss: 0.3161, train acc: 0.8704, val loss: 0.2558, val acc: 0.9113  (best train acc: 0.8866, best val acc: 0.9248, best train loss: 0.2941  @ epoch 31956 )\n",
      "[Epoch: 32000] train loss: 0.3168, train acc: 0.8643, val loss: 0.2491, val acc: 0.9184  (best train acc: 0.8866, best val acc: 0.9248, best train loss: 0.2941  @ epoch 31956 )\n",
      "[Epoch: 32020] train loss: 0.3034, train acc: 0.8781, val loss: 0.2423, val acc: 0.9221  (best train acc: 0.8866, best val acc: 0.9248, best train loss: 0.2941  @ epoch 31956 )\n",
      "[Epoch: 32040] train loss: 0.3008, train acc: 0.8769, val loss: 0.2584, val acc: 0.9099  (best train acc: 0.8866, best val acc: 0.9248, best train loss: 0.2938  @ epoch 32028 )\n",
      "[Epoch: 32060] train loss: 0.3098, train acc: 0.8751, val loss: 0.2483, val acc: 0.9180  (best train acc: 0.8866, best val acc: 0.9248, best train loss: 0.2938  @ epoch 32028 )\n",
      "[Epoch: 32080] train loss: 0.3233, train acc: 0.8687, val loss: 0.2697, val acc: 0.9056  (best train acc: 0.8866, best val acc: 0.9248, best train loss: 0.2938  @ epoch 32028 )\n",
      "[Epoch: 32100] train loss: 0.3249, train acc: 0.8663, val loss: 0.2566, val acc: 0.9180  (best train acc: 0.8866, best val acc: 0.9248, best train loss: 0.2938  @ epoch 32028 )\n",
      "[Epoch: 32120] train loss: 0.3090, train acc: 0.8757, val loss: 0.2555, val acc: 0.9096  (best train acc: 0.8866, best val acc: 0.9248, best train loss: 0.2938  @ epoch 32028 )\n",
      "[Epoch: 32140] train loss: 0.3136, train acc: 0.8700, val loss: 0.2429, val acc: 0.9228  (best train acc: 0.8866, best val acc: 0.9248, best train loss: 0.2938  @ epoch 32028 )\n",
      "[Epoch: 32160] train loss: 0.3149, train acc: 0.8717, val loss: 0.2549, val acc: 0.9083  (best train acc: 0.8866, best val acc: 0.9248, best train loss: 0.2938  @ epoch 32028 )\n",
      "[Epoch: 32180] train loss: 0.3086, train acc: 0.8788, val loss: 0.2480, val acc: 0.9238  (best train acc: 0.8866, best val acc: 0.9248, best train loss: 0.2938  @ epoch 32028 )\n",
      "[Epoch: 32200] train loss: 0.3106, train acc: 0.8730, val loss: 0.2487, val acc: 0.9204  (best train acc: 0.8866, best val acc: 0.9248, best train loss: 0.2938  @ epoch 32028 )\n",
      "[Epoch: 32220] train loss: 0.3146, train acc: 0.8718, val loss: 0.2426, val acc: 0.9197  (best train acc: 0.8866, best val acc: 0.9248, best train loss: 0.2938  @ epoch 32028 )\n",
      "[Epoch: 32240] train loss: 0.3139, train acc: 0.8669, val loss: 0.2430, val acc: 0.9167  (best train acc: 0.8866, best val acc: 0.9248, best train loss: 0.2938  @ epoch 32028 )\n",
      "[Epoch: 32260] train loss: 0.3191, train acc: 0.8691, val loss: 0.2552, val acc: 0.9099  (best train acc: 0.8866, best val acc: 0.9248, best train loss: 0.2938  @ epoch 32028 )\n",
      "[Epoch: 32280] train loss: 0.3090, train acc: 0.8745, val loss: 0.2479, val acc: 0.9140  (best train acc: 0.8866, best val acc: 0.9248, best train loss: 0.2938  @ epoch 32028 )\n",
      "[Epoch: 32300] train loss: 0.3155, train acc: 0.8759, val loss: 0.2477, val acc: 0.9150  (best train acc: 0.8866, best val acc: 0.9248, best train loss: 0.2938  @ epoch 32028 )\n",
      "[Epoch: 32320] train loss: 0.3100, train acc: 0.8756, val loss: 0.2462, val acc: 0.9184  (best train acc: 0.8866, best val acc: 0.9248, best train loss: 0.2938  @ epoch 32028 )\n",
      "[Epoch: 32340] train loss: 0.3081, train acc: 0.8738, val loss: 0.2377, val acc: 0.9207  (best train acc: 0.8866, best val acc: 0.9248, best train loss: 0.2938  @ epoch 32028 )\n",
      "[Epoch: 32360] train loss: 0.3201, train acc: 0.8689, val loss: 0.2449, val acc: 0.9106  (best train acc: 0.8866, best val acc: 0.9248, best train loss: 0.2938  @ epoch 32028 )\n",
      "[Epoch: 32380] train loss: 0.3080, train acc: 0.8801, val loss: 0.2422, val acc: 0.9197  (best train acc: 0.8866, best val acc: 0.9248, best train loss: 0.2938  @ epoch 32028 )\n",
      "[Epoch: 32400] train loss: 0.3192, train acc: 0.8676, val loss: 0.2403, val acc: 0.9191  (best train acc: 0.8866, best val acc: 0.9248, best train loss: 0.2938  @ epoch 32028 )\n",
      "[Epoch: 32420] train loss: 0.3117, train acc: 0.8720, val loss: 0.2481, val acc: 0.9167  (best train acc: 0.8866, best val acc: 0.9248, best train loss: 0.2938  @ epoch 32028 )\n",
      "[Epoch: 32440] train loss: 0.3106, train acc: 0.8715, val loss: 0.2471, val acc: 0.9194  (best train acc: 0.8866, best val acc: 0.9248, best train loss: 0.2938  @ epoch 32028 )\n",
      "[Epoch: 32460] train loss: 0.3055, train acc: 0.8748, val loss: 0.2438, val acc: 0.9204  (best train acc: 0.8866, best val acc: 0.9248, best train loss: 0.2938  @ epoch 32028 )\n",
      "[Epoch: 32480] train loss: 0.3043, train acc: 0.8764, val loss: 0.2457, val acc: 0.9180  (best train acc: 0.8866, best val acc: 0.9248, best train loss: 0.2938  @ epoch 32028 )\n",
      "[Epoch: 32500] train loss: 0.3068, train acc: 0.8763, val loss: 0.2445, val acc: 0.9218  (best train acc: 0.8866, best val acc: 0.9248, best train loss: 0.2917  @ epoch 32488 )\n",
      "[Epoch: 32520] train loss: 0.3162, train acc: 0.8740, val loss: 0.2457, val acc: 0.9221  (best train acc: 0.8866, best val acc: 0.9248, best train loss: 0.2917  @ epoch 32488 )\n",
      "[Epoch: 32540] train loss: 0.3155, train acc: 0.8700, val loss: 0.2451, val acc: 0.9191  (best train acc: 0.8866, best val acc: 0.9248, best train loss: 0.2917  @ epoch 32488 )\n",
      "[Epoch: 32560] train loss: 0.3338, train acc: 0.8698, val loss: 0.3131, val acc: 0.8870  (best train acc: 0.8866, best val acc: 0.9248, best train loss: 0.2917  @ epoch 32488 )\n",
      "[Epoch: 32580] train loss: 0.3148, train acc: 0.8756, val loss: 0.2428, val acc: 0.9201  (best train acc: 0.8866, best val acc: 0.9248, best train loss: 0.2917  @ epoch 32488 )\n",
      "[Epoch: 32600] train loss: 0.3052, train acc: 0.8738, val loss: 0.2478, val acc: 0.9153  (best train acc: 0.8866, best val acc: 0.9248, best train loss: 0.2917  @ epoch 32488 )\n",
      "[Epoch: 32620] train loss: 0.3143, train acc: 0.8736, val loss: 0.2413, val acc: 0.9194  (best train acc: 0.8866, best val acc: 0.9248, best train loss: 0.2917  @ epoch 32488 )\n",
      "[Epoch: 32640] train loss: 0.3051, train acc: 0.8785, val loss: 0.2456, val acc: 0.9245  (best train acc: 0.8866, best val acc: 0.9248, best train loss: 0.2917  @ epoch 32488 )\n",
      "[Epoch: 32660] train loss: 0.3156, train acc: 0.8738, val loss: 0.2469, val acc: 0.9160  (best train acc: 0.8866, best val acc: 0.9248, best train loss: 0.2917  @ epoch 32488 )\n",
      "[Epoch: 32680] train loss: 0.3019, train acc: 0.8780, val loss: 0.2405, val acc: 0.9197  (best train acc: 0.8866, best val acc: 0.9248, best train loss: 0.2917  @ epoch 32488 )\n",
      "[Epoch: 32700] train loss: 0.3208, train acc: 0.8697, val loss: 0.2437, val acc: 0.9126  (best train acc: 0.8866, best val acc: 0.9248, best train loss: 0.2917  @ epoch 32488 )\n",
      "[Epoch: 32720] train loss: 0.3167, train acc: 0.8783, val loss: 0.2430, val acc: 0.9191  (best train acc: 0.8866, best val acc: 0.9248, best train loss: 0.2917  @ epoch 32488 )\n",
      "[Epoch: 32740] train loss: 0.3112, train acc: 0.8765, val loss: 0.2442, val acc: 0.9214  (best train acc: 0.8866, best val acc: 0.9248, best train loss: 0.2917  @ epoch 32488 )\n",
      "[Epoch: 32760] train loss: 0.3110, train acc: 0.8723, val loss: 0.2455, val acc: 0.9191  (best train acc: 0.8866, best val acc: 0.9248, best train loss: 0.2917  @ epoch 32488 )\n",
      "[Epoch: 32780] train loss: 0.3182, train acc: 0.8715, val loss: 0.2435, val acc: 0.9221  (best train acc: 0.8866, best val acc: 0.9248, best train loss: 0.2917  @ epoch 32488 )\n",
      "[Epoch: 32800] train loss: 0.3024, train acc: 0.8738, val loss: 0.2488, val acc: 0.9187  (best train acc: 0.8866, best val acc: 0.9248, best train loss: 0.2917  @ epoch 32488 )\n",
      "[Epoch: 32820] train loss: 0.3267, train acc: 0.8711, val loss: 0.2633, val acc: 0.9126  (best train acc: 0.8866, best val acc: 0.9248, best train loss: 0.2917  @ epoch 32488 )\n",
      "[Epoch: 32840] train loss: 0.3279, train acc: 0.8661, val loss: 0.2440, val acc: 0.9184  (best train acc: 0.8866, best val acc: 0.9248, best train loss: 0.2917  @ epoch 32488 )\n",
      "[Epoch: 32860] train loss: 0.3105, train acc: 0.8740, val loss: 0.2460, val acc: 0.9191  (best train acc: 0.8866, best val acc: 0.9248, best train loss: 0.2917  @ epoch 32488 )\n",
      "[Epoch: 32880] train loss: 0.3140, train acc: 0.8746, val loss: 0.2495, val acc: 0.9174  (best train acc: 0.8866, best val acc: 0.9248, best train loss: 0.2917  @ epoch 32488 )\n",
      "[Epoch: 32900] train loss: 0.3026, train acc: 0.8784, val loss: 0.2439, val acc: 0.9228  (best train acc: 0.8866, best val acc: 0.9248, best train loss: 0.2917  @ epoch 32488 )\n",
      "[Epoch: 32920] train loss: 0.3103, train acc: 0.8805, val loss: 0.2472, val acc: 0.9177  (best train acc: 0.8866, best val acc: 0.9248, best train loss: 0.2917  @ epoch 32488 )\n",
      "[Epoch: 32940] train loss: 0.3072, train acc: 0.8799, val loss: 0.2444, val acc: 0.9197  (best train acc: 0.8866, best val acc: 0.9248, best train loss: 0.2917  @ epoch 32488 )\n",
      "[Epoch: 32960] train loss: 0.2981, train acc: 0.8784, val loss: 0.2429, val acc: 0.9187  (best train acc: 0.8866, best val acc: 0.9248, best train loss: 0.2917  @ epoch 32488 )\n",
      "[Epoch: 32980] train loss: 0.3089, train acc: 0.8744, val loss: 0.2429, val acc: 0.9191  (best train acc: 0.8866, best val acc: 0.9248, best train loss: 0.2917  @ epoch 32488 )\n",
      "[Epoch: 33000] train loss: 0.3008, train acc: 0.8757, val loss: 0.2457, val acc: 0.9224  (best train acc: 0.8866, best val acc: 0.9248, best train loss: 0.2917  @ epoch 32488 )\n",
      "[Epoch: 33020] train loss: 0.3101, train acc: 0.8757, val loss: 0.2524, val acc: 0.9204  (best train acc: 0.8866, best val acc: 0.9248, best train loss: 0.2917  @ epoch 32488 )\n",
      "[Epoch: 33040] train loss: 0.3134, train acc: 0.8731, val loss: 0.2467, val acc: 0.9231  (best train acc: 0.8866, best val acc: 0.9248, best train loss: 0.2917  @ epoch 32488 )\n",
      "[Epoch: 33060] train loss: 0.3074, train acc: 0.8765, val loss: 0.2486, val acc: 0.9170  (best train acc: 0.8866, best val acc: 0.9248, best train loss: 0.2917  @ epoch 32488 )\n",
      "[Epoch: 33080] train loss: 0.2984, train acc: 0.8822, val loss: 0.2487, val acc: 0.9197  (best train acc: 0.8866, best val acc: 0.9248, best train loss: 0.2917  @ epoch 32488 )\n",
      "[Epoch: 33100] train loss: 0.3159, train acc: 0.8718, val loss: 0.2449, val acc: 0.9194  (best train acc: 0.8866, best val acc: 0.9248, best train loss: 0.2917  @ epoch 32488 )\n",
      "[Epoch: 33120] train loss: 0.3081, train acc: 0.8754, val loss: 0.2527, val acc: 0.9191  (best train acc: 0.8866, best val acc: 0.9248, best train loss: 0.2917  @ epoch 32488 )\n",
      "[Epoch: 33140] train loss: 0.3061, train acc: 0.8728, val loss: 0.2410, val acc: 0.9224  (best train acc: 0.8866, best val acc: 0.9248, best train loss: 0.2917  @ epoch 32488 )\n",
      "[Epoch: 33160] train loss: 0.3072, train acc: 0.8733, val loss: 0.2419, val acc: 0.9170  (best train acc: 0.8866, best val acc: 0.9255, best train loss: 0.2917  @ epoch 32488 )\n",
      "[Epoch: 33180] train loss: 0.3023, train acc: 0.8793, val loss: 0.2518, val acc: 0.9204  (best train acc: 0.8866, best val acc: 0.9255, best train loss: 0.2917  @ epoch 32488 )\n",
      "[Epoch: 33200] train loss: 0.3250, train acc: 0.8660, val loss: 0.2459, val acc: 0.9157  (best train acc: 0.8866, best val acc: 0.9255, best train loss: 0.2917  @ epoch 32488 )\n",
      "[Epoch: 33220] train loss: 0.3193, train acc: 0.8728, val loss: 0.2515, val acc: 0.9143  (best train acc: 0.8866, best val acc: 0.9255, best train loss: 0.2917  @ epoch 32488 )\n",
      "[Epoch: 33240] train loss: 0.3218, train acc: 0.8695, val loss: 0.2492, val acc: 0.9177  (best train acc: 0.8866, best val acc: 0.9255, best train loss: 0.2917  @ epoch 32488 )\n",
      "[Epoch: 33260] train loss: 0.3065, train acc: 0.8809, val loss: 0.2399, val acc: 0.9201  (best train acc: 0.8866, best val acc: 0.9255, best train loss: 0.2917  @ epoch 32488 )\n",
      "[Epoch: 33280] train loss: 0.3186, train acc: 0.8696, val loss: 0.2409, val acc: 0.9191  (best train acc: 0.8866, best val acc: 0.9255, best train loss: 0.2917  @ epoch 32488 )\n",
      "[Epoch: 33300] train loss: 0.3077, train acc: 0.8811, val loss: 0.2386, val acc: 0.9221  (best train acc: 0.8866, best val acc: 0.9255, best train loss: 0.2917  @ epoch 32488 )\n",
      "[Epoch: 33320] train loss: 0.3082, train acc: 0.8760, val loss: 0.2443, val acc: 0.9137  (best train acc: 0.8866, best val acc: 0.9255, best train loss: 0.2917  @ epoch 32488 )\n",
      "[Epoch: 33340] train loss: 0.3103, train acc: 0.8733, val loss: 0.2382, val acc: 0.9207  (best train acc: 0.8866, best val acc: 0.9255, best train loss: 0.2917  @ epoch 32488 )\n",
      "[Epoch: 33360] train loss: 0.2991, train acc: 0.8816, val loss: 0.2453, val acc: 0.9224  (best train acc: 0.8866, best val acc: 0.9255, best train loss: 0.2917  @ epoch 32488 )\n",
      "[Epoch: 33380] train loss: 0.3156, train acc: 0.8693, val loss: 0.2451, val acc: 0.9197  (best train acc: 0.8866, best val acc: 0.9255, best train loss: 0.2917  @ epoch 32488 )\n",
      "[Epoch: 33400] train loss: 0.3140, train acc: 0.8735, val loss: 0.2486, val acc: 0.9147  (best train acc: 0.8866, best val acc: 0.9255, best train loss: 0.2917  @ epoch 32488 )\n",
      "[Epoch: 33420] train loss: 0.3254, train acc: 0.8720, val loss: 0.2562, val acc: 0.9143  (best train acc: 0.8866, best val acc: 0.9255, best train loss: 0.2917  @ epoch 32488 )\n",
      "[Epoch: 33440] train loss: 0.3113, train acc: 0.8702, val loss: 0.2438, val acc: 0.9207  (best train acc: 0.8866, best val acc: 0.9255, best train loss: 0.2917  @ epoch 32488 )\n",
      "[Epoch: 33460] train loss: 0.3115, train acc: 0.8748, val loss: 0.2646, val acc: 0.9029  (best train acc: 0.8867, best val acc: 0.9255, best train loss: 0.2917  @ epoch 32488 )\n",
      "[Epoch: 33480] train loss: 0.3187, train acc: 0.8717, val loss: 0.2492, val acc: 0.9133  (best train acc: 0.8867, best val acc: 0.9255, best train loss: 0.2917  @ epoch 32488 )\n",
      "[Epoch: 33500] train loss: 0.3018, train acc: 0.8767, val loss: 0.2494, val acc: 0.9204  (best train acc: 0.8867, best val acc: 0.9255, best train loss: 0.2917  @ epoch 32488 )\n",
      "[Epoch: 33520] train loss: 0.3057, train acc: 0.8813, val loss: 0.2590, val acc: 0.9089  (best train acc: 0.8867, best val acc: 0.9255, best train loss: 0.2917  @ epoch 32488 )\n",
      "[Epoch: 33540] train loss: 0.3161, train acc: 0.8743, val loss: 0.2467, val acc: 0.9187  (best train acc: 0.8867, best val acc: 0.9255, best train loss: 0.2917  @ epoch 32488 )\n",
      "[Epoch: 33560] train loss: 0.3159, train acc: 0.8730, val loss: 0.2543, val acc: 0.9069  (best train acc: 0.8867, best val acc: 0.9255, best train loss: 0.2917  @ epoch 32488 )\n",
      "[Epoch: 33580] train loss: 0.2969, train acc: 0.8792, val loss: 0.2417, val acc: 0.9164  (best train acc: 0.8867, best val acc: 0.9255, best train loss: 0.2917  @ epoch 32488 )\n",
      "[Epoch: 33600] train loss: 0.3334, train acc: 0.8668, val loss: 0.2503, val acc: 0.9164  (best train acc: 0.8867, best val acc: 0.9255, best train loss: 0.2917  @ epoch 32488 )\n",
      "[Epoch: 33620] train loss: 0.3050, train acc: 0.8775, val loss: 0.2494, val acc: 0.9184  (best train acc: 0.8867, best val acc: 0.9255, best train loss: 0.2917  @ epoch 32488 )\n",
      "[Epoch: 33640] train loss: 0.3152, train acc: 0.8738, val loss: 0.2466, val acc: 0.9211  (best train acc: 0.8867, best val acc: 0.9255, best train loss: 0.2917  @ epoch 32488 )\n",
      "[Epoch: 33660] train loss: 0.3036, train acc: 0.8785, val loss: 0.2409, val acc: 0.9187  (best train acc: 0.8867, best val acc: 0.9255, best train loss: 0.2917  @ epoch 32488 )\n",
      "[Epoch: 33680] train loss: 0.3101, train acc: 0.8722, val loss: 0.2420, val acc: 0.9147  (best train acc: 0.8867, best val acc: 0.9255, best train loss: 0.2917  @ epoch 32488 )\n",
      "[Epoch: 33700] train loss: 0.3073, train acc: 0.8777, val loss: 0.2420, val acc: 0.9201  (best train acc: 0.8867, best val acc: 0.9255, best train loss: 0.2917  @ epoch 32488 )\n",
      "[Epoch: 33720] train loss: 0.3064, train acc: 0.8762, val loss: 0.2421, val acc: 0.9234  (best train acc: 0.8867, best val acc: 0.9255, best train loss: 0.2917  @ epoch 32488 )\n",
      "[Epoch: 33740] train loss: 0.3108, train acc: 0.8728, val loss: 0.2527, val acc: 0.9137  (best train acc: 0.8867, best val acc: 0.9255, best train loss: 0.2917  @ epoch 32488 )\n",
      "[Epoch: 33760] train loss: 0.3183, train acc: 0.8712, val loss: 0.2496, val acc: 0.9170  (best train acc: 0.8867, best val acc: 0.9255, best train loss: 0.2917  @ epoch 32488 )\n",
      "[Epoch: 33780] train loss: 0.3108, train acc: 0.8727, val loss: 0.2488, val acc: 0.9180  (best train acc: 0.8867, best val acc: 0.9255, best train loss: 0.2917  @ epoch 32488 )\n",
      "[Epoch: 33800] train loss: 0.3021, train acc: 0.8803, val loss: 0.2418, val acc: 0.9204  (best train acc: 0.8867, best val acc: 0.9255, best train loss: 0.2917  @ epoch 32488 )\n",
      "[Epoch: 33820] train loss: 0.3201, train acc: 0.8715, val loss: 0.2587, val acc: 0.9160  (best train acc: 0.8867, best val acc: 0.9255, best train loss: 0.2917  @ epoch 32488 )\n",
      "[Epoch: 33840] train loss: 0.3204, train acc: 0.8687, val loss: 0.2609, val acc: 0.9049  (best train acc: 0.8867, best val acc: 0.9255, best train loss: 0.2917  @ epoch 32488 )\n",
      "[Epoch: 33860] train loss: 0.3186, train acc: 0.8738, val loss: 0.2464, val acc: 0.9177  (best train acc: 0.8867, best val acc: 0.9255, best train loss: 0.2917  @ epoch 32488 )\n",
      "[Epoch: 33880] train loss: 0.3163, train acc: 0.8689, val loss: 0.2484, val acc: 0.9120  (best train acc: 0.8867, best val acc: 0.9255, best train loss: 0.2917  @ epoch 32488 )\n",
      "[Epoch: 33900] train loss: 0.3020, train acc: 0.8790, val loss: 0.2418, val acc: 0.9241  (best train acc: 0.8867, best val acc: 0.9255, best train loss: 0.2917  @ epoch 32488 )\n",
      "[Epoch: 33920] train loss: 0.3052, train acc: 0.8775, val loss: 0.2454, val acc: 0.9214  (best train acc: 0.8867, best val acc: 0.9255, best train loss: 0.2917  @ epoch 32488 )\n",
      "[Epoch: 33940] train loss: 0.3066, train acc: 0.8764, val loss: 0.2438, val acc: 0.9194  (best train acc: 0.8867, best val acc: 0.9255, best train loss: 0.2917  @ epoch 32488 )\n",
      "[Epoch: 33960] train loss: 0.3037, train acc: 0.8803, val loss: 0.2389, val acc: 0.9187  (best train acc: 0.8867, best val acc: 0.9255, best train loss: 0.2917  @ epoch 32488 )\n",
      "[Epoch: 33980] train loss: 0.3162, train acc: 0.8710, val loss: 0.2437, val acc: 0.9150  (best train acc: 0.8867, best val acc: 0.9255, best train loss: 0.2917  @ epoch 32488 )\n",
      "[Epoch: 34000] train loss: 0.3114, train acc: 0.8741, val loss: 0.2455, val acc: 0.9224  (best train acc: 0.8867, best val acc: 0.9255, best train loss: 0.2917  @ epoch 32488 )\n",
      "[Epoch: 34020] train loss: 0.3114, train acc: 0.8712, val loss: 0.2449, val acc: 0.9164  (best train acc: 0.8867, best val acc: 0.9255, best train loss: 0.2917  @ epoch 32488 )\n",
      "[Epoch: 34040] train loss: 0.3194, train acc: 0.8669, val loss: 0.2439, val acc: 0.9211  (best train acc: 0.8867, best val acc: 0.9258, best train loss: 0.2917  @ epoch 32488 )\n",
      "[Epoch: 34060] train loss: 0.3109, train acc: 0.8719, val loss: 0.2437, val acc: 0.9211  (best train acc: 0.8867, best val acc: 0.9258, best train loss: 0.2917  @ epoch 32488 )\n",
      "[Epoch: 34080] train loss: 0.3009, train acc: 0.8826, val loss: 0.2440, val acc: 0.9228  (best train acc: 0.8867, best val acc: 0.9258, best train loss: 0.2917  @ epoch 32488 )\n",
      "[Epoch: 34100] train loss: 0.3189, train acc: 0.8673, val loss: 0.2501, val acc: 0.9143  (best train acc: 0.8867, best val acc: 0.9258, best train loss: 0.2917  @ epoch 32488 )\n",
      "[Epoch: 34120] train loss: 0.3102, train acc: 0.8708, val loss: 0.2438, val acc: 0.9194  (best train acc: 0.8867, best val acc: 0.9258, best train loss: 0.2917  @ epoch 32488 )\n",
      "[Epoch: 34140] train loss: 0.3121, train acc: 0.8738, val loss: 0.2507, val acc: 0.9137  (best train acc: 0.8867, best val acc: 0.9258, best train loss: 0.2917  @ epoch 32488 )\n",
      "[Epoch: 34160] train loss: 0.3089, train acc: 0.8762, val loss: 0.2404, val acc: 0.9204  (best train acc: 0.8867, best val acc: 0.9258, best train loss: 0.2917  @ epoch 32488 )\n",
      "[Epoch: 34180] train loss: 0.3049, train acc: 0.8764, val loss: 0.2453, val acc: 0.9201  (best train acc: 0.8867, best val acc: 0.9258, best train loss: 0.2917  @ epoch 32488 )\n",
      "[Epoch: 34200] train loss: 0.3248, train acc: 0.8611, val loss: 0.2523, val acc: 0.9170  (best train acc: 0.8867, best val acc: 0.9258, best train loss: 0.2917  @ epoch 32488 )\n",
      "[Epoch: 34220] train loss: 0.3080, train acc: 0.8772, val loss: 0.2454, val acc: 0.9214  (best train acc: 0.8867, best val acc: 0.9258, best train loss: 0.2917  @ epoch 32488 )\n",
      "[Epoch: 34240] train loss: 0.3038, train acc: 0.8809, val loss: 0.2464, val acc: 0.9137  (best train acc: 0.8867, best val acc: 0.9258, best train loss: 0.2917  @ epoch 32488 )\n",
      "[Epoch: 34260] train loss: 0.3145, train acc: 0.8781, val loss: 0.2406, val acc: 0.9221  (best train acc: 0.8867, best val acc: 0.9258, best train loss: 0.2917  @ epoch 32488 )\n",
      "[Epoch: 34280] train loss: 0.3107, train acc: 0.8741, val loss: 0.2462, val acc: 0.9194  (best train acc: 0.8867, best val acc: 0.9258, best train loss: 0.2917  @ epoch 32488 )\n",
      "[Epoch: 34300] train loss: 0.3091, train acc: 0.8777, val loss: 0.2444, val acc: 0.9191  (best train acc: 0.8867, best val acc: 0.9258, best train loss: 0.2917  @ epoch 32488 )\n",
      "[Epoch: 34320] train loss: 0.3147, train acc: 0.8712, val loss: 0.2492, val acc: 0.9140  (best train acc: 0.8867, best val acc: 0.9258, best train loss: 0.2917  @ epoch 32488 )\n",
      "[Epoch: 34340] train loss: 0.3087, train acc: 0.8722, val loss: 0.2439, val acc: 0.9207  (best train acc: 0.8867, best val acc: 0.9258, best train loss: 0.2917  @ epoch 32488 )\n",
      "[Epoch: 34360] train loss: 0.3131, train acc: 0.8707, val loss: 0.2480, val acc: 0.9116  (best train acc: 0.8867, best val acc: 0.9258, best train loss: 0.2917  @ epoch 32488 )\n",
      "[Epoch: 34380] train loss: 0.3100, train acc: 0.8754, val loss: 0.2530, val acc: 0.9167  (best train acc: 0.8867, best val acc: 0.9258, best train loss: 0.2917  @ epoch 32488 )\n",
      "[Epoch: 34400] train loss: 0.3033, train acc: 0.8799, val loss: 0.2369, val acc: 0.9180  (best train acc: 0.8867, best val acc: 0.9258, best train loss: 0.2917  @ epoch 32488 )\n",
      "[Epoch: 34420] train loss: 0.3026, train acc: 0.8796, val loss: 0.2420, val acc: 0.9201  (best train acc: 0.8867, best val acc: 0.9258, best train loss: 0.2917  @ epoch 32488 )\n",
      "[Epoch: 34440] train loss: 0.3032, train acc: 0.8782, val loss: 0.2398, val acc: 0.9197  (best train acc: 0.8867, best val acc: 0.9258, best train loss: 0.2917  @ epoch 32488 )\n",
      "[Epoch: 34460] train loss: 0.3127, train acc: 0.8777, val loss: 0.2445, val acc: 0.9221  (best train acc: 0.8867, best val acc: 0.9258, best train loss: 0.2917  @ epoch 32488 )\n",
      "[Epoch: 34480] train loss: 0.3017, train acc: 0.8788, val loss: 0.2414, val acc: 0.9255  (best train acc: 0.8867, best val acc: 0.9258, best train loss: 0.2917  @ epoch 32488 )\n",
      "[Epoch: 34500] train loss: 0.3046, train acc: 0.8764, val loss: 0.2402, val acc: 0.9228  (best train acc: 0.8867, best val acc: 0.9258, best train loss: 0.2917  @ epoch 32488 )\n",
      "[Epoch: 34520] train loss: 0.3294, train acc: 0.8626, val loss: 0.2471, val acc: 0.9224  (best train acc: 0.8867, best val acc: 0.9258, best train loss: 0.2917  @ epoch 32488 )\n",
      "[Epoch: 34540] train loss: 0.2993, train acc: 0.8764, val loss: 0.2460, val acc: 0.9187  (best train acc: 0.8867, best val acc: 0.9258, best train loss: 0.2917  @ epoch 32488 )\n",
      "[Epoch: 34560] train loss: 0.3185, train acc: 0.8757, val loss: 0.2459, val acc: 0.9164  (best train acc: 0.8867, best val acc: 0.9258, best train loss: 0.2917  @ epoch 32488 )\n",
      "[Epoch: 34580] train loss: 0.3140, train acc: 0.8717, val loss: 0.2496, val acc: 0.9177  (best train acc: 0.8867, best val acc: 0.9258, best train loss: 0.2917  @ epoch 32488 )\n",
      "[Epoch: 34600] train loss: 0.3139, train acc: 0.8702, val loss: 0.2394, val acc: 0.9234  (best train acc: 0.8867, best val acc: 0.9258, best train loss: 0.2917  @ epoch 32488 )\n",
      "[Epoch: 34620] train loss: 0.3136, train acc: 0.8746, val loss: 0.2488, val acc: 0.9174  (best train acc: 0.8867, best val acc: 0.9258, best train loss: 0.2917  @ epoch 32488 )\n",
      "[Epoch: 34640] train loss: 0.2980, train acc: 0.8824, val loss: 0.2403, val acc: 0.9218  (best train acc: 0.8867, best val acc: 0.9258, best train loss: 0.2917  @ epoch 32488 )\n",
      "[Epoch: 34660] train loss: 0.3126, train acc: 0.8703, val loss: 0.2384, val acc: 0.9228  (best train acc: 0.8867, best val acc: 0.9258, best train loss: 0.2917  @ epoch 32488 )\n",
      "[Epoch: 34680] train loss: 0.3190, train acc: 0.8690, val loss: 0.2463, val acc: 0.9191  (best train acc: 0.8867, best val acc: 0.9258, best train loss: 0.2917  @ epoch 32488 )\n",
      "[Epoch: 34700] train loss: 0.2967, train acc: 0.8819, val loss: 0.2411, val acc: 0.9224  (best train acc: 0.8867, best val acc: 0.9258, best train loss: 0.2917  @ epoch 32488 )\n",
      "[Epoch: 34720] train loss: 0.3120, train acc: 0.8707, val loss: 0.2456, val acc: 0.9177  (best train acc: 0.8867, best val acc: 0.9258, best train loss: 0.2917  @ epoch 32488 )\n",
      "[Epoch: 34740] train loss: 0.3106, train acc: 0.8718, val loss: 0.2554, val acc: 0.9113  (best train acc: 0.8867, best val acc: 0.9258, best train loss: 0.2917  @ epoch 32488 )\n",
      "[Epoch: 34760] train loss: 0.3040, train acc: 0.8770, val loss: 0.2456, val acc: 0.9180  (best train acc: 0.8867, best val acc: 0.9258, best train loss: 0.2917  @ epoch 32488 )\n",
      "[Epoch: 34780] train loss: 0.3096, train acc: 0.8736, val loss: 0.2486, val acc: 0.9160  (best train acc: 0.8867, best val acc: 0.9258, best train loss: 0.2917  @ epoch 32488 )\n",
      "[Epoch: 34800] train loss: 0.3126, train acc: 0.8710, val loss: 0.2424, val acc: 0.9184  (best train acc: 0.8867, best val acc: 0.9258, best train loss: 0.2917  @ epoch 32488 )\n",
      "[Epoch: 34820] train loss: 0.3116, train acc: 0.8724, val loss: 0.2467, val acc: 0.9197  (best train acc: 0.8867, best val acc: 0.9258, best train loss: 0.2917  @ epoch 32488 )\n",
      "[Epoch: 34840] train loss: 0.3102, train acc: 0.8710, val loss: 0.2403, val acc: 0.9174  (best train acc: 0.8867, best val acc: 0.9258, best train loss: 0.2917  @ epoch 32488 )\n",
      "[Epoch: 34860] train loss: 0.3129, train acc: 0.8745, val loss: 0.2517, val acc: 0.9143  (best train acc: 0.8867, best val acc: 0.9258, best train loss: 0.2917  @ epoch 32488 )\n",
      "[Epoch: 34880] train loss: 0.3194, train acc: 0.8651, val loss: 0.2397, val acc: 0.9197  (best train acc: 0.8867, best val acc: 0.9258, best train loss: 0.2917  @ epoch 32488 )\n",
      "[Epoch: 34900] train loss: 0.3045, train acc: 0.8769, val loss: 0.2430, val acc: 0.9160  (best train acc: 0.8867, best val acc: 0.9258, best train loss: 0.2917  @ epoch 32488 )\n",
      "[Epoch: 34920] train loss: 0.3153, train acc: 0.8733, val loss: 0.2431, val acc: 0.9140  (best train acc: 0.8867, best val acc: 0.9258, best train loss: 0.2916  @ epoch 34903 )\n",
      "[Epoch: 34940] train loss: 0.3067, train acc: 0.8749, val loss: 0.2378, val acc: 0.9224  (best train acc: 0.8867, best val acc: 0.9258, best train loss: 0.2916  @ epoch 34903 )\n",
      "[Epoch: 34960] train loss: 0.3168, train acc: 0.8690, val loss: 0.2480, val acc: 0.9126  (best train acc: 0.8867, best val acc: 0.9258, best train loss: 0.2916  @ epoch 34903 )\n",
      "[Epoch: 34980] train loss: 0.3060, train acc: 0.8781, val loss: 0.2425, val acc: 0.9191  (best train acc: 0.8867, best val acc: 0.9258, best train loss: 0.2916  @ epoch 34903 )\n",
      "[Epoch: 35000] train loss: 0.3125, train acc: 0.8731, val loss: 0.2442, val acc: 0.9170  (best train acc: 0.8867, best val acc: 0.9258, best train loss: 0.2916  @ epoch 34903 )\n",
      "[Epoch: 35020] train loss: 0.3145, train acc: 0.8717, val loss: 0.2370, val acc: 0.9194  (best train acc: 0.8867, best val acc: 0.9258, best train loss: 0.2916  @ epoch 34903 )\n",
      "[Epoch: 35040] train loss: 0.3082, train acc: 0.8766, val loss: 0.2374, val acc: 0.9157  (best train acc: 0.8867, best val acc: 0.9258, best train loss: 0.2916  @ epoch 34903 )\n",
      "[Epoch: 35060] train loss: 0.3090, train acc: 0.8791, val loss: 0.2472, val acc: 0.9211  (best train acc: 0.8867, best val acc: 0.9258, best train loss: 0.2916  @ epoch 34903 )\n",
      "[Epoch: 35080] train loss: 0.3146, train acc: 0.8714, val loss: 0.2505, val acc: 0.9143  (best train acc: 0.8867, best val acc: 0.9258, best train loss: 0.2916  @ epoch 34903 )\n",
      "[Epoch: 35100] train loss: 0.3086, train acc: 0.8754, val loss: 0.2446, val acc: 0.9116  (best train acc: 0.8867, best val acc: 0.9258, best train loss: 0.2916  @ epoch 34903 )\n",
      "[Epoch: 35120] train loss: 0.3067, train acc: 0.8775, val loss: 0.2422, val acc: 0.9167  (best train acc: 0.8867, best val acc: 0.9258, best train loss: 0.2916  @ epoch 34903 )\n",
      "[Epoch: 35140] train loss: 0.3106, train acc: 0.8743, val loss: 0.2434, val acc: 0.9211  (best train acc: 0.8867, best val acc: 0.9258, best train loss: 0.2916  @ epoch 34903 )\n",
      "[Epoch: 35160] train loss: 0.3077, train acc: 0.8805, val loss: 0.2422, val acc: 0.9170  (best train acc: 0.8867, best val acc: 0.9258, best train loss: 0.2916  @ epoch 34903 )\n",
      "[Epoch: 35180] train loss: 0.3213, train acc: 0.8663, val loss: 0.2527, val acc: 0.9147  (best train acc: 0.8867, best val acc: 0.9258, best train loss: 0.2916  @ epoch 34903 )\n",
      "[Epoch: 35200] train loss: 0.3040, train acc: 0.8805, val loss: 0.2417, val acc: 0.9218  (best train acc: 0.8867, best val acc: 0.9258, best train loss: 0.2916  @ epoch 34903 )\n",
      "[Epoch: 35220] train loss: 0.3071, train acc: 0.8751, val loss: 0.2367, val acc: 0.9167  (best train acc: 0.8867, best val acc: 0.9258, best train loss: 0.2916  @ epoch 34903 )\n",
      "[Epoch: 35240] train loss: 0.3164, train acc: 0.8686, val loss: 0.2452, val acc: 0.9228  (best train acc: 0.8867, best val acc: 0.9258, best train loss: 0.2916  @ epoch 34903 )\n",
      "[Epoch: 35260] train loss: 0.3080, train acc: 0.8750, val loss: 0.2424, val acc: 0.9231  (best train acc: 0.8867, best val acc: 0.9258, best train loss: 0.2916  @ epoch 34903 )\n",
      "[Epoch: 35280] train loss: 0.3182, train acc: 0.8718, val loss: 0.2420, val acc: 0.9157  (best train acc: 0.8867, best val acc: 0.9258, best train loss: 0.2916  @ epoch 34903 )\n",
      "[Epoch: 35300] train loss: 0.3054, train acc: 0.8757, val loss: 0.2365, val acc: 0.9238  (best train acc: 0.8867, best val acc: 0.9258, best train loss: 0.2916  @ epoch 34903 )\n",
      "[Epoch: 35320] train loss: 0.3341, train acc: 0.8577, val loss: 0.2572, val acc: 0.9170  (best train acc: 0.8867, best val acc: 0.9258, best train loss: 0.2916  @ epoch 34903 )\n",
      "[Epoch: 35340] train loss: 0.3147, train acc: 0.8718, val loss: 0.2443, val acc: 0.9160  (best train acc: 0.8867, best val acc: 0.9258, best train loss: 0.2916  @ epoch 34903 )\n",
      "[Epoch: 35360] train loss: 0.3066, train acc: 0.8739, val loss: 0.2474, val acc: 0.9201  (best train acc: 0.8867, best val acc: 0.9258, best train loss: 0.2916  @ epoch 34903 )\n",
      "[Epoch: 35380] train loss: 0.3136, train acc: 0.8749, val loss: 0.2516, val acc: 0.9194  (best train acc: 0.8867, best val acc: 0.9258, best train loss: 0.2916  @ epoch 34903 )\n",
      "[Epoch: 35400] train loss: 0.3121, train acc: 0.8733, val loss: 0.2617, val acc: 0.9143  (best train acc: 0.8867, best val acc: 0.9258, best train loss: 0.2916  @ epoch 34903 )\n",
      "[Epoch: 35420] train loss: 0.3104, train acc: 0.8798, val loss: 0.2741, val acc: 0.9120  (best train acc: 0.8867, best val acc: 0.9258, best train loss: 0.2916  @ epoch 34903 )\n",
      "[Epoch: 35440] train loss: 0.3162, train acc: 0.8683, val loss: 0.2515, val acc: 0.9204  (best train acc: 0.8867, best val acc: 0.9258, best train loss: 0.2916  @ epoch 34903 )\n",
      "[Epoch: 35460] train loss: 0.3079, train acc: 0.8728, val loss: 0.2485, val acc: 0.9194  (best train acc: 0.8867, best val acc: 0.9258, best train loss: 0.2916  @ epoch 34903 )\n",
      "[Epoch: 35480] train loss: 0.3071, train acc: 0.8746, val loss: 0.2474, val acc: 0.9194  (best train acc: 0.8867, best val acc: 0.9258, best train loss: 0.2916  @ epoch 34903 )\n",
      "[Epoch: 35500] train loss: 0.3210, train acc: 0.8639, val loss: 0.2489, val acc: 0.9157  (best train acc: 0.8867, best val acc: 0.9258, best train loss: 0.2916  @ epoch 34903 )\n",
      "[Epoch: 35520] train loss: 0.3055, train acc: 0.8759, val loss: 0.2500, val acc: 0.9201  (best train acc: 0.8867, best val acc: 0.9258, best train loss: 0.2916  @ epoch 34903 )\n",
      "[Epoch: 35540] train loss: 0.3105, train acc: 0.8748, val loss: 0.2572, val acc: 0.9184  (best train acc: 0.8867, best val acc: 0.9258, best train loss: 0.2916  @ epoch 34903 )\n",
      "[Epoch: 35560] train loss: 0.3133, train acc: 0.8699, val loss: 0.2556, val acc: 0.9174  (best train acc: 0.8867, best val acc: 0.9258, best train loss: 0.2916  @ epoch 34903 )\n",
      "[Epoch: 35580] train loss: 0.3090, train acc: 0.8712, val loss: 0.2524, val acc: 0.9140  (best train acc: 0.8867, best val acc: 0.9258, best train loss: 0.2916  @ epoch 34903 )\n",
      "[Epoch: 35600] train loss: 0.2987, train acc: 0.8799, val loss: 0.2462, val acc: 0.9234  (best train acc: 0.8867, best val acc: 0.9258, best train loss: 0.2916  @ epoch 34903 )\n",
      "[Epoch: 35620] train loss: 0.3065, train acc: 0.8772, val loss: 0.2457, val acc: 0.9218  (best train acc: 0.8867, best val acc: 0.9258, best train loss: 0.2916  @ epoch 34903 )\n",
      "[Epoch: 35640] train loss: 0.3062, train acc: 0.8758, val loss: 0.2559, val acc: 0.9170  (best train acc: 0.8867, best val acc: 0.9258, best train loss: 0.2916  @ epoch 34903 )\n",
      "[Epoch: 35660] train loss: 0.3097, train acc: 0.8732, val loss: 0.2544, val acc: 0.9177  (best train acc: 0.8872, best val acc: 0.9258, best train loss: 0.2916  @ epoch 34903 )\n",
      "[Epoch: 35680] train loss: 0.3060, train acc: 0.8736, val loss: 0.2506, val acc: 0.9174  (best train acc: 0.8872, best val acc: 0.9258, best train loss: 0.2916  @ epoch 34903 )\n",
      "[Epoch: 35700] train loss: 0.3270, train acc: 0.8708, val loss: 0.2647, val acc: 0.9079  (best train acc: 0.8872, best val acc: 0.9258, best train loss: 0.2916  @ epoch 34903 )\n",
      "[Epoch: 35720] train loss: 0.2965, train acc: 0.8791, val loss: 0.2508, val acc: 0.9207  (best train acc: 0.8872, best val acc: 0.9258, best train loss: 0.2916  @ epoch 34903 )\n",
      "[Epoch: 35740] train loss: 0.3197, train acc: 0.8670, val loss: 0.2478, val acc: 0.9207  (best train acc: 0.8872, best val acc: 0.9258, best train loss: 0.2916  @ epoch 34903 )\n",
      "[Epoch: 35760] train loss: 0.3089, train acc: 0.8768, val loss: 0.2450, val acc: 0.9174  (best train acc: 0.8872, best val acc: 0.9258, best train loss: 0.2916  @ epoch 34903 )\n",
      "[Epoch: 35780] train loss: 0.3058, train acc: 0.8745, val loss: 0.2492, val acc: 0.9150  (best train acc: 0.8872, best val acc: 0.9258, best train loss: 0.2916  @ epoch 34903 )\n",
      "[Epoch: 35800] train loss: 0.3005, train acc: 0.8817, val loss: 0.2558, val acc: 0.9187  (best train acc: 0.8872, best val acc: 0.9258, best train loss: 0.2916  @ epoch 34903 )\n",
      "[Epoch: 35820] train loss: 0.2986, train acc: 0.8828, val loss: 0.2476, val acc: 0.9241  (best train acc: 0.8872, best val acc: 0.9258, best train loss: 0.2916  @ epoch 34903 )\n",
      "[Epoch: 35840] train loss: 0.3009, train acc: 0.8821, val loss: 0.2469, val acc: 0.9221  (best train acc: 0.8872, best val acc: 0.9258, best train loss: 0.2916  @ epoch 34903 )\n",
      "[Epoch: 35860] train loss: 0.3065, train acc: 0.8752, val loss: 0.2461, val acc: 0.9187  (best train acc: 0.8872, best val acc: 0.9258, best train loss: 0.2916  @ epoch 34903 )\n",
      "[Epoch: 35880] train loss: 0.3148, train acc: 0.8705, val loss: 0.2582, val acc: 0.9187  (best train acc: 0.8872, best val acc: 0.9258, best train loss: 0.2916  @ epoch 34903 )\n",
      "[Epoch: 35900] train loss: 0.3083, train acc: 0.8711, val loss: 0.2570, val acc: 0.9170  (best train acc: 0.8872, best val acc: 0.9258, best train loss: 0.2916  @ epoch 34903 )\n",
      "[Epoch: 35920] train loss: 0.3024, train acc: 0.8790, val loss: 0.2476, val acc: 0.9150  (best train acc: 0.8872, best val acc: 0.9258, best train loss: 0.2916  @ epoch 34903 )\n",
      "[Epoch: 35940] train loss: 0.3041, train acc: 0.8760, val loss: 0.2440, val acc: 0.9211  (best train acc: 0.8872, best val acc: 0.9258, best train loss: 0.2916  @ epoch 34903 )\n",
      "[Epoch: 35960] train loss: 0.3192, train acc: 0.8668, val loss: 0.2511, val acc: 0.9201  (best train acc: 0.8872, best val acc: 0.9258, best train loss: 0.2916  @ epoch 34903 )\n",
      "[Epoch: 35980] train loss: 0.3051, train acc: 0.8783, val loss: 0.2526, val acc: 0.9174  (best train acc: 0.8872, best val acc: 0.9258, best train loss: 0.2916  @ epoch 34903 )\n",
      "[Epoch: 36000] train loss: 0.3137, train acc: 0.8764, val loss: 0.2649, val acc: 0.9170  (best train acc: 0.8872, best val acc: 0.9258, best train loss: 0.2916  @ epoch 34903 )\n",
      "[Epoch: 36020] train loss: 0.3149, train acc: 0.8698, val loss: 0.2485, val acc: 0.9204  (best train acc: 0.8872, best val acc: 0.9258, best train loss: 0.2916  @ epoch 34903 )\n",
      "[Epoch: 36040] train loss: 0.2995, train acc: 0.8803, val loss: 0.2455, val acc: 0.9197  (best train acc: 0.8872, best val acc: 0.9258, best train loss: 0.2916  @ epoch 34903 )\n",
      "[Epoch: 36060] train loss: 0.3002, train acc: 0.8789, val loss: 0.2442, val acc: 0.9194  (best train acc: 0.8872, best val acc: 0.9258, best train loss: 0.2916  @ epoch 34903 )\n",
      "[Epoch: 36080] train loss: 0.3200, train acc: 0.8667, val loss: 0.2557, val acc: 0.9093  (best train acc: 0.8872, best val acc: 0.9258, best train loss: 0.2916  @ epoch 34903 )\n",
      "[Epoch: 36100] train loss: 0.3165, train acc: 0.8707, val loss: 0.2498, val acc: 0.9187  (best train acc: 0.8872, best val acc: 0.9258, best train loss: 0.2916  @ epoch 34903 )\n",
      "[Epoch: 36120] train loss: 0.3142, train acc: 0.8753, val loss: 0.2488, val acc: 0.9130  (best train acc: 0.8872, best val acc: 0.9258, best train loss: 0.2916  @ epoch 34903 )\n",
      "[Epoch: 36140] train loss: 0.2977, train acc: 0.8785, val loss: 0.2475, val acc: 0.9224  (best train acc: 0.8872, best val acc: 0.9258, best train loss: 0.2916  @ epoch 34903 )\n",
      "[Epoch: 36160] train loss: 0.3065, train acc: 0.8773, val loss: 0.2548, val acc: 0.9180  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2916  @ epoch 34903 )\n",
      "[Epoch: 36180] train loss: 0.3023, train acc: 0.8806, val loss: 0.2486, val acc: 0.9214  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2916  @ epoch 34903 )\n",
      "[Epoch: 36200] train loss: 0.3092, train acc: 0.8813, val loss: 0.2601, val acc: 0.9180  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2916  @ epoch 34903 )\n",
      "[Epoch: 36220] train loss: 0.3198, train acc: 0.8690, val loss: 0.2519, val acc: 0.9160  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2916  @ epoch 34903 )\n",
      "[Epoch: 36240] train loss: 0.3038, train acc: 0.8753, val loss: 0.2529, val acc: 0.9164  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2916  @ epoch 34903 )\n",
      "[Epoch: 36260] train loss: 0.3165, train acc: 0.8711, val loss: 0.2509, val acc: 0.9123  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2916  @ epoch 34903 )\n",
      "[Epoch: 36280] train loss: 0.3058, train acc: 0.8788, val loss: 0.2473, val acc: 0.9255  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2916  @ epoch 34903 )\n",
      "[Epoch: 36300] train loss: 0.3126, train acc: 0.8753, val loss: 0.2473, val acc: 0.9238  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2916  @ epoch 34903 )\n",
      "[Epoch: 36320] train loss: 0.3180, train acc: 0.8681, val loss: 0.2548, val acc: 0.9140  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2916  @ epoch 34903 )\n",
      "[Epoch: 36340] train loss: 0.2969, train acc: 0.8811, val loss: 0.2476, val acc: 0.9197  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2916  @ epoch 34903 )\n",
      "[Epoch: 36360] train loss: 0.3054, train acc: 0.8772, val loss: 0.2488, val acc: 0.9164  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2916  @ epoch 34903 )\n",
      "[Epoch: 36380] train loss: 0.3142, train acc: 0.8787, val loss: 0.2513, val acc: 0.9150  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2916  @ epoch 34903 )\n",
      "[Epoch: 36400] train loss: 0.3189, train acc: 0.8715, val loss: 0.2579, val acc: 0.9116  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2916  @ epoch 34903 )\n",
      "[Epoch: 36420] train loss: 0.3213, train acc: 0.8680, val loss: 0.2463, val acc: 0.9170  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2916  @ epoch 34903 )\n",
      "[Epoch: 36440] train loss: 0.3064, train acc: 0.8749, val loss: 0.2528, val acc: 0.9218  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2916  @ epoch 34903 )\n",
      "[Epoch: 36460] train loss: 0.3152, train acc: 0.8720, val loss: 0.2534, val acc: 0.9137  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2916  @ epoch 34903 )\n",
      "[Epoch: 36480] train loss: 0.3028, train acc: 0.8792, val loss: 0.2553, val acc: 0.9207  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2916  @ epoch 34903 )\n",
      "[Epoch: 36500] train loss: 0.2991, train acc: 0.8800, val loss: 0.2488, val acc: 0.9211  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2916  @ epoch 34903 )\n",
      "[Epoch: 36520] train loss: 0.3131, train acc: 0.8746, val loss: 0.2546, val acc: 0.9167  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2916  @ epoch 34903 )\n",
      "[Epoch: 36540] train loss: 0.3121, train acc: 0.8711, val loss: 0.2495, val acc: 0.9241  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2916  @ epoch 34903 )\n",
      "[Epoch: 36560] train loss: 0.2983, train acc: 0.8817, val loss: 0.2511, val acc: 0.9214  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2916  @ epoch 34903 )\n",
      "[Epoch: 36580] train loss: 0.3190, train acc: 0.8735, val loss: 0.2556, val acc: 0.9211  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2916  @ epoch 34903 )\n",
      "[Epoch: 36600] train loss: 0.3053, train acc: 0.8773, val loss: 0.2448, val acc: 0.9201  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2916  @ epoch 34903 )\n",
      "[Epoch: 36620] train loss: 0.3138, train acc: 0.8717, val loss: 0.2583, val acc: 0.9177  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2916  @ epoch 34903 )\n",
      "[Epoch: 36640] train loss: 0.3025, train acc: 0.8780, val loss: 0.2478, val acc: 0.9201  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2916  @ epoch 34903 )\n",
      "[Epoch: 36660] train loss: 0.3017, train acc: 0.8787, val loss: 0.2459, val acc: 0.9153  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2916  @ epoch 34903 )\n",
      "[Epoch: 36680] train loss: 0.3001, train acc: 0.8846, val loss: 0.2560, val acc: 0.9224  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2916  @ epoch 34903 )\n",
      "[Epoch: 36700] train loss: 0.3063, train acc: 0.8739, val loss: 0.2516, val acc: 0.9177  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2916  @ epoch 34903 )\n",
      "[Epoch: 36720] train loss: 0.3070, train acc: 0.8781, val loss: 0.2542, val acc: 0.9184  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2916  @ epoch 34903 )\n",
      "[Epoch: 36740] train loss: 0.3000, train acc: 0.8776, val loss: 0.2522, val acc: 0.9184  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2916  @ epoch 34903 )\n",
      "[Epoch: 36760] train loss: 0.3176, train acc: 0.8665, val loss: 0.2554, val acc: 0.9147  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2916  @ epoch 34903 )\n",
      "[Epoch: 36780] train loss: 0.3138, train acc: 0.8693, val loss: 0.2496, val acc: 0.9123  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2916  @ epoch 34903 )\n",
      "[Epoch: 36800] train loss: 0.3271, train acc: 0.8670, val loss: 0.2450, val acc: 0.9224  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2916  @ epoch 34903 )\n",
      "[Epoch: 36820] train loss: 0.3078, train acc: 0.8723, val loss: 0.2465, val acc: 0.9204  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2916  @ epoch 34903 )\n",
      "[Epoch: 36840] train loss: 0.3146, train acc: 0.8694, val loss: 0.2473, val acc: 0.9221  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2916  @ epoch 34903 )\n",
      "[Epoch: 36860] train loss: 0.3022, train acc: 0.8762, val loss: 0.2515, val acc: 0.9143  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2916  @ epoch 34903 )\n",
      "[Epoch: 36880] train loss: 0.3061, train acc: 0.8774, val loss: 0.2587, val acc: 0.9228  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2916  @ epoch 34903 )\n",
      "[Epoch: 36900] train loss: 0.3145, train acc: 0.8698, val loss: 0.2549, val acc: 0.9177  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2916  @ epoch 34903 )\n",
      "[Epoch: 36920] train loss: 0.3136, train acc: 0.8712, val loss: 0.2609, val acc: 0.9096  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2916  @ epoch 34903 )\n",
      "[Epoch: 36940] train loss: 0.3090, train acc: 0.8727, val loss: 0.2584, val acc: 0.9167  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2916  @ epoch 34903 )\n",
      "[Epoch: 36960] train loss: 0.3157, train acc: 0.8697, val loss: 0.2514, val acc: 0.9164  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2916  @ epoch 34903 )\n",
      "[Epoch: 36980] train loss: 0.3186, train acc: 0.8670, val loss: 0.2470, val acc: 0.9177  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2916  @ epoch 34903 )\n",
      "[Epoch: 37000] train loss: 0.2996, train acc: 0.8788, val loss: 0.2470, val acc: 0.9207  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2916  @ epoch 34903 )\n",
      "[Epoch: 37020] train loss: 0.3094, train acc: 0.8720, val loss: 0.2474, val acc: 0.9170  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2916  @ epoch 34903 )\n",
      "[Epoch: 37040] train loss: 0.3104, train acc: 0.8768, val loss: 0.2589, val acc: 0.9076  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2916  @ epoch 34903 )\n",
      "[Epoch: 37060] train loss: 0.3060, train acc: 0.8794, val loss: 0.2528, val acc: 0.9234  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2916  @ epoch 34903 )\n",
      "[Epoch: 37080] train loss: 0.3087, train acc: 0.8713, val loss: 0.2479, val acc: 0.9194  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2916  @ epoch 34903 )\n",
      "[Epoch: 37100] train loss: 0.3022, train acc: 0.8776, val loss: 0.2450, val acc: 0.9204  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2916  @ epoch 34903 )\n",
      "[Epoch: 37120] train loss: 0.2955, train acc: 0.8843, val loss: 0.2477, val acc: 0.9234  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2916  @ epoch 34903 )\n",
      "[Epoch: 37140] train loss: 0.3029, train acc: 0.8780, val loss: 0.2512, val acc: 0.9191  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2913  @ epoch 37121 )\n",
      "[Epoch: 37160] train loss: 0.3114, train acc: 0.8767, val loss: 0.2572, val acc: 0.9106  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2913  @ epoch 37121 )\n",
      "[Epoch: 37180] train loss: 0.3137, train acc: 0.8754, val loss: 0.2570, val acc: 0.9180  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2913  @ epoch 37121 )\n",
      "[Epoch: 37200] train loss: 0.3022, train acc: 0.8783, val loss: 0.2478, val acc: 0.9180  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2913  @ epoch 37121 )\n",
      "[Epoch: 37220] train loss: 0.3084, train acc: 0.8757, val loss: 0.2506, val acc: 0.9224  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2913  @ epoch 37121 )\n",
      "[Epoch: 37240] train loss: 0.3067, train acc: 0.8777, val loss: 0.2533, val acc: 0.9160  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2913  @ epoch 37121 )\n",
      "[Epoch: 37260] train loss: 0.3217, train acc: 0.8718, val loss: 0.2680, val acc: 0.9164  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2913  @ epoch 37121 )\n",
      "[Epoch: 37280] train loss: 0.3112, train acc: 0.8715, val loss: 0.2492, val acc: 0.9211  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2913  @ epoch 37121 )\n",
      "[Epoch: 37300] train loss: 0.3094, train acc: 0.8732, val loss: 0.2555, val acc: 0.9164  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2913  @ epoch 37121 )\n",
      "[Epoch: 37320] train loss: 0.2966, train acc: 0.8825, val loss: 0.2499, val acc: 0.9164  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2913  @ epoch 37121 )\n",
      "[Epoch: 37340] train loss: 0.3046, train acc: 0.8798, val loss: 0.2501, val acc: 0.9228  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2913  @ epoch 37121 )\n",
      "[Epoch: 37360] train loss: 0.3069, train acc: 0.8784, val loss: 0.2458, val acc: 0.9221  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2913  @ epoch 37121 )\n",
      "[Epoch: 37380] train loss: 0.3222, train acc: 0.8664, val loss: 0.2525, val acc: 0.9170  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2913  @ epoch 37121 )\n",
      "[Epoch: 37400] train loss: 0.3193, train acc: 0.8759, val loss: 0.2629, val acc: 0.9099  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2913  @ epoch 37121 )\n",
      "[Epoch: 37420] train loss: 0.3067, train acc: 0.8790, val loss: 0.2505, val acc: 0.9218  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2913  @ epoch 37121 )\n",
      "[Epoch: 37440] train loss: 0.3166, train acc: 0.8707, val loss: 0.2509, val acc: 0.9184  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2913  @ epoch 37121 )\n",
      "[Epoch: 37460] train loss: 0.3151, train acc: 0.8717, val loss: 0.2517, val acc: 0.9153  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2913  @ epoch 37121 )\n",
      "[Epoch: 37480] train loss: 0.3139, train acc: 0.8717, val loss: 0.2431, val acc: 0.9234  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2913  @ epoch 37121 )\n",
      "[Epoch: 37500] train loss: 0.3036, train acc: 0.8776, val loss: 0.2544, val acc: 0.9160  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2913  @ epoch 37121 )\n",
      "[Epoch: 37520] train loss: 0.3108, train acc: 0.8689, val loss: 0.2540, val acc: 0.9120  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2913  @ epoch 37121 )\n",
      "[Epoch: 37540] train loss: 0.3129, train acc: 0.8729, val loss: 0.2517, val acc: 0.9177  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2913  @ epoch 37121 )\n",
      "[Epoch: 37560] train loss: 0.3112, train acc: 0.8728, val loss: 0.2563, val acc: 0.9116  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2913  @ epoch 37121 )\n",
      "[Epoch: 37580] train loss: 0.3068, train acc: 0.8746, val loss: 0.2616, val acc: 0.9120  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2913  @ epoch 37121 )\n",
      "[Epoch: 37600] train loss: 0.3137, train acc: 0.8730, val loss: 0.2480, val acc: 0.9245  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2913  @ epoch 37121 )\n",
      "[Epoch: 37620] train loss: 0.3111, train acc: 0.8742, val loss: 0.2550, val acc: 0.9157  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2913  @ epoch 37121 )\n",
      "[Epoch: 37640] train loss: 0.3055, train acc: 0.8820, val loss: 0.2518, val acc: 0.9160  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2913  @ epoch 37121 )\n",
      "[Epoch: 37660] train loss: 0.3032, train acc: 0.8738, val loss: 0.2480, val acc: 0.9184  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2913  @ epoch 37121 )\n",
      "[Epoch: 37680] train loss: 0.3056, train acc: 0.8769, val loss: 0.2486, val acc: 0.9143  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2913  @ epoch 37121 )\n",
      "[Epoch: 37700] train loss: 0.3096, train acc: 0.8759, val loss: 0.2602, val acc: 0.9126  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2913  @ epoch 37121 )\n",
      "[Epoch: 37720] train loss: 0.3009, train acc: 0.8787, val loss: 0.2487, val acc: 0.9157  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2913  @ epoch 37121 )\n",
      "[Epoch: 37740] train loss: 0.3111, train acc: 0.8741, val loss: 0.2523, val acc: 0.9211  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2913  @ epoch 37121 )\n",
      "[Epoch: 37760] train loss: 0.3038, train acc: 0.8797, val loss: 0.2454, val acc: 0.9197  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2913  @ epoch 37121 )\n",
      "[Epoch: 37780] train loss: 0.3052, train acc: 0.8732, val loss: 0.2573, val acc: 0.9157  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2913  @ epoch 37121 )\n",
      "[Epoch: 37800] train loss: 0.3164, train acc: 0.8741, val loss: 0.2416, val acc: 0.9201  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2913  @ epoch 37121 )\n",
      "[Epoch: 37820] train loss: 0.3019, train acc: 0.8768, val loss: 0.2455, val acc: 0.9197  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2913  @ epoch 37121 )\n",
      "[Epoch: 37840] train loss: 0.3140, train acc: 0.8754, val loss: 0.2480, val acc: 0.9221  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2913  @ epoch 37121 )\n",
      "[Epoch: 37860] train loss: 0.2992, train acc: 0.8807, val loss: 0.2568, val acc: 0.9126  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2913  @ epoch 37121 )\n",
      "[Epoch: 37880] train loss: 0.3085, train acc: 0.8741, val loss: 0.2550, val acc: 0.9218  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2913  @ epoch 37121 )\n",
      "[Epoch: 37900] train loss: 0.2919, train acc: 0.8835, val loss: 0.2495, val acc: 0.9221  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2913  @ epoch 37121 )\n",
      "[Epoch: 37920] train loss: 0.3272, train acc: 0.8691, val loss: 0.2521, val acc: 0.9126  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2913  @ epoch 37121 )\n",
      "[Epoch: 37940] train loss: 0.3116, train acc: 0.8746, val loss: 0.2553, val acc: 0.9170  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2913  @ epoch 37121 )\n",
      "[Epoch: 37960] train loss: 0.3089, train acc: 0.8732, val loss: 0.2466, val acc: 0.9197  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2913  @ epoch 37121 )\n",
      "[Epoch: 37980] train loss: 0.2929, train acc: 0.8828, val loss: 0.2447, val acc: 0.9238  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2913  @ epoch 37121 )\n",
      "[Epoch: 38000] train loss: 0.3060, train acc: 0.8725, val loss: 0.2573, val acc: 0.9153  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2913  @ epoch 37121 )\n",
      "[Epoch: 38020] train loss: 0.3216, train acc: 0.8700, val loss: 0.2519, val acc: 0.9194  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2913  @ epoch 37121 )\n",
      "[Epoch: 38040] train loss: 0.3139, train acc: 0.8665, val loss: 0.2522, val acc: 0.9201  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2913  @ epoch 37121 )\n",
      "[Epoch: 38060] train loss: 0.3017, train acc: 0.8799, val loss: 0.2479, val acc: 0.9224  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2913  @ epoch 37121 )\n",
      "[Epoch: 38080] train loss: 0.3059, train acc: 0.8735, val loss: 0.2493, val acc: 0.9184  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2913  @ epoch 37121 )\n",
      "[Epoch: 38100] train loss: 0.3080, train acc: 0.8681, val loss: 0.2638, val acc: 0.9106  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2913  @ epoch 37121 )\n",
      "[Epoch: 38120] train loss: 0.3194, train acc: 0.8707, val loss: 0.2545, val acc: 0.9184  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2913  @ epoch 37121 )\n",
      "[Epoch: 38140] train loss: 0.3096, train acc: 0.8708, val loss: 0.2504, val acc: 0.9194  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2913  @ epoch 37121 )\n",
      "[Epoch: 38160] train loss: 0.3003, train acc: 0.8806, val loss: 0.2520, val acc: 0.9153  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2913  @ epoch 37121 )\n",
      "[Epoch: 38180] train loss: 0.2997, train acc: 0.8793, val loss: 0.2523, val acc: 0.9103  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2913  @ epoch 37121 )\n",
      "[Epoch: 38200] train loss: 0.3045, train acc: 0.8815, val loss: 0.2449, val acc: 0.9207  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2913  @ epoch 37121 )\n",
      "[Epoch: 38220] train loss: 0.3099, train acc: 0.8743, val loss: 0.2504, val acc: 0.9184  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2913  @ epoch 37121 )\n",
      "[Epoch: 38240] train loss: 0.3056, train acc: 0.8762, val loss: 0.2532, val acc: 0.9143  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2913  @ epoch 37121 )\n",
      "[Epoch: 38260] train loss: 0.2999, train acc: 0.8798, val loss: 0.2488, val acc: 0.9170  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2913  @ epoch 37121 )\n",
      "[Epoch: 38280] train loss: 0.3123, train acc: 0.8742, val loss: 0.2545, val acc: 0.9167  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2913  @ epoch 37121 )\n",
      "[Epoch: 38300] train loss: 0.3115, train acc: 0.8738, val loss: 0.2606, val acc: 0.9086  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2913  @ epoch 37121 )\n",
      "[Epoch: 38320] train loss: 0.3186, train acc: 0.8733, val loss: 0.2504, val acc: 0.9164  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2913  @ epoch 37121 )\n",
      "[Epoch: 38340] train loss: 0.2994, train acc: 0.8803, val loss: 0.2533, val acc: 0.9106  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2913  @ epoch 37121 )\n",
      "[Epoch: 38360] train loss: 0.3038, train acc: 0.8803, val loss: 0.2384, val acc: 0.9140  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2913  @ epoch 37121 )\n",
      "[Epoch: 38380] train loss: 0.3051, train acc: 0.8775, val loss: 0.2432, val acc: 0.9207  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2913  @ epoch 37121 )\n",
      "[Epoch: 38400] train loss: 0.3169, train acc: 0.8721, val loss: 0.2469, val acc: 0.9137  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2913  @ epoch 37121 )\n",
      "[Epoch: 38420] train loss: 0.3119, train acc: 0.8728, val loss: 0.2431, val acc: 0.9160  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2913  @ epoch 37121 )\n",
      "[Epoch: 38440] train loss: 0.3001, train acc: 0.8802, val loss: 0.2412, val acc: 0.9177  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2913  @ epoch 37121 )\n",
      "[Epoch: 38460] train loss: 0.3084, train acc: 0.8797, val loss: 0.2427, val acc: 0.9211  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2913  @ epoch 37121 )\n",
      "[Epoch: 38480] train loss: 0.3132, train acc: 0.8809, val loss: 0.2486, val acc: 0.9167  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2913  @ epoch 37121 )\n",
      "[Epoch: 38500] train loss: 0.3051, train acc: 0.8740, val loss: 0.2623, val acc: 0.9113  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2913  @ epoch 37121 )\n",
      "[Epoch: 38520] train loss: 0.3013, train acc: 0.8779, val loss: 0.2529, val acc: 0.9214  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2913  @ epoch 37121 )\n",
      "[Epoch: 38540] train loss: 0.3206, train acc: 0.8689, val loss: 0.2575, val acc: 0.9160  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2913  @ epoch 37121 )\n",
      "[Epoch: 38560] train loss: 0.2952, train acc: 0.8809, val loss: 0.2499, val acc: 0.9218  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2913  @ epoch 37121 )\n",
      "[Epoch: 38580] train loss: 0.3084, train acc: 0.8736, val loss: 0.2489, val acc: 0.9194  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2913  @ epoch 37121 )\n",
      "[Epoch: 38600] train loss: 0.2983, train acc: 0.8762, val loss: 0.2536, val acc: 0.9147  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2913  @ epoch 37121 )\n",
      "[Epoch: 38620] train loss: 0.3097, train acc: 0.8766, val loss: 0.2495, val acc: 0.9167  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2913  @ epoch 37121 )\n",
      "[Epoch: 38640] train loss: 0.3110, train acc: 0.8732, val loss: 0.2520, val acc: 0.9140  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2913  @ epoch 37121 )\n",
      "[Epoch: 38660] train loss: 0.3129, train acc: 0.8752, val loss: 0.2438, val acc: 0.9184  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2913  @ epoch 37121 )\n",
      "[Epoch: 38680] train loss: 0.3032, train acc: 0.8774, val loss: 0.2490, val acc: 0.9180  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2903  @ epoch 38676 )\n",
      "[Epoch: 38700] train loss: 0.3007, train acc: 0.8770, val loss: 0.2377, val acc: 0.9174  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2903  @ epoch 38676 )\n",
      "[Epoch: 38720] train loss: 0.3124, train acc: 0.8771, val loss: 0.2531, val acc: 0.9174  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2903  @ epoch 38676 )\n",
      "[Epoch: 38740] train loss: 0.3011, train acc: 0.8818, val loss: 0.2414, val acc: 0.9194  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2903  @ epoch 38676 )\n",
      "[Epoch: 38760] train loss: 0.3117, train acc: 0.8738, val loss: 0.2485, val acc: 0.9174  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2899  @ epoch 38747 )\n",
      "[Epoch: 38780] train loss: 0.2987, train acc: 0.8803, val loss: 0.2462, val acc: 0.9234  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2899  @ epoch 38747 )\n",
      "[Epoch: 38800] train loss: 0.3199, train acc: 0.8681, val loss: 0.2453, val acc: 0.9147  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2899  @ epoch 38747 )\n",
      "[Epoch: 38820] train loss: 0.3146, train acc: 0.8724, val loss: 0.2486, val acc: 0.9153  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2899  @ epoch 38747 )\n",
      "[Epoch: 38840] train loss: 0.3141, train acc: 0.8708, val loss: 0.2533, val acc: 0.9133  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2899  @ epoch 38747 )\n",
      "[Epoch: 38860] train loss: 0.3248, train acc: 0.8665, val loss: 0.2452, val acc: 0.9197  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2899  @ epoch 38747 )\n",
      "[Epoch: 38880] train loss: 0.3214, train acc: 0.8680, val loss: 0.2439, val acc: 0.9180  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2899  @ epoch 38747 )\n",
      "[Epoch: 38900] train loss: 0.3061, train acc: 0.8735, val loss: 0.2463, val acc: 0.9150  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2899  @ epoch 38747 )\n",
      "[Epoch: 38920] train loss: 0.3084, train acc: 0.8714, val loss: 0.2377, val acc: 0.9207  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2899  @ epoch 38747 )\n",
      "[Epoch: 38940] train loss: 0.3030, train acc: 0.8796, val loss: 0.2478, val acc: 0.9221  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2899  @ epoch 38747 )\n",
      "[Epoch: 38960] train loss: 0.3253, train acc: 0.8656, val loss: 0.2553, val acc: 0.9143  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2899  @ epoch 38747 )\n",
      "[Epoch: 38980] train loss: 0.2974, train acc: 0.8798, val loss: 0.2509, val acc: 0.9120  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2899  @ epoch 38747 )\n",
      "[Epoch: 39000] train loss: 0.3132, train acc: 0.8740, val loss: 0.2499, val acc: 0.9157  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2899  @ epoch 38747 )\n",
      "[Epoch: 39020] train loss: 0.2976, train acc: 0.8794, val loss: 0.2430, val acc: 0.9201  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2899  @ epoch 38747 )\n",
      "[Epoch: 39040] train loss: 0.3050, train acc: 0.8789, val loss: 0.2496, val acc: 0.9207  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2899  @ epoch 38747 )\n",
      "[Epoch: 39060] train loss: 0.3056, train acc: 0.8748, val loss: 0.2382, val acc: 0.9201  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2899  @ epoch 38747 )\n",
      "[Epoch: 39080] train loss: 0.2984, train acc: 0.8803, val loss: 0.2449, val acc: 0.9228  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2899  @ epoch 38747 )\n",
      "[Epoch: 39100] train loss: 0.3094, train acc: 0.8744, val loss: 0.2461, val acc: 0.9201  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2899  @ epoch 38747 )\n",
      "[Epoch: 39120] train loss: 0.3094, train acc: 0.8775, val loss: 0.2486, val acc: 0.9184  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2899  @ epoch 38747 )\n",
      "[Epoch: 39140] train loss: 0.3113, train acc: 0.8727, val loss: 0.2471, val acc: 0.9201  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2899  @ epoch 38747 )\n",
      "[Epoch: 39160] train loss: 0.2977, train acc: 0.8785, val loss: 0.2461, val acc: 0.9221  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2899  @ epoch 38747 )\n",
      "[Epoch: 39180] train loss: 0.3187, train acc: 0.8694, val loss: 0.2537, val acc: 0.9140  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2899  @ epoch 38747 )\n",
      "[Epoch: 39200] train loss: 0.3071, train acc: 0.8746, val loss: 0.2506, val acc: 0.9157  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2899  @ epoch 38747 )\n",
      "[Epoch: 39220] train loss: 0.3093, train acc: 0.8743, val loss: 0.2458, val acc: 0.9157  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2899  @ epoch 38747 )\n",
      "[Epoch: 39240] train loss: 0.3269, train acc: 0.8669, val loss: 0.2541, val acc: 0.9133  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2899  @ epoch 38747 )\n",
      "[Epoch: 39260] train loss: 0.3100, train acc: 0.8792, val loss: 0.2386, val acc: 0.9204  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2899  @ epoch 38747 )\n",
      "[Epoch: 39280] train loss: 0.3103, train acc: 0.8750, val loss: 0.2410, val acc: 0.9221  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2899  @ epoch 38747 )\n",
      "[Epoch: 39300] train loss: 0.3083, train acc: 0.8754, val loss: 0.2411, val acc: 0.9180  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2899  @ epoch 38747 )\n",
      "[Epoch: 39320] train loss: 0.3066, train acc: 0.8734, val loss: 0.2448, val acc: 0.9187  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2899  @ epoch 38747 )\n",
      "[Epoch: 39340] train loss: 0.2921, train acc: 0.8861, val loss: 0.2451, val acc: 0.9204  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2899  @ epoch 38747 )\n",
      "[Epoch: 39360] train loss: 0.3227, train acc: 0.8657, val loss: 0.2433, val acc: 0.9207  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2899  @ epoch 38747 )\n",
      "[Epoch: 39380] train loss: 0.3047, train acc: 0.8770, val loss: 0.2467, val acc: 0.9164  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2899  @ epoch 38747 )\n",
      "[Epoch: 39400] train loss: 0.3083, train acc: 0.8759, val loss: 0.2453, val acc: 0.9184  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2899  @ epoch 38747 )\n",
      "[Epoch: 39420] train loss: 0.3058, train acc: 0.8759, val loss: 0.2421, val acc: 0.9197  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2899  @ epoch 38747 )\n",
      "[Epoch: 39440] train loss: 0.3031, train acc: 0.8724, val loss: 0.2497, val acc: 0.9113  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2899  @ epoch 38747 )\n",
      "[Epoch: 39460] train loss: 0.3062, train acc: 0.8764, val loss: 0.2475, val acc: 0.9207  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2899  @ epoch 38747 )\n",
      "[Epoch: 39480] train loss: 0.3017, train acc: 0.8788, val loss: 0.2568, val acc: 0.9204  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2899  @ epoch 38747 )\n",
      "[Epoch: 39500] train loss: 0.3173, train acc: 0.8768, val loss: 0.2623, val acc: 0.9113  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2899  @ epoch 38747 )\n",
      "[Epoch: 39520] train loss: 0.3138, train acc: 0.8709, val loss: 0.2460, val acc: 0.9133  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2899  @ epoch 38747 )\n",
      "[Epoch: 39540] train loss: 0.3021, train acc: 0.8768, val loss: 0.2493, val acc: 0.9177  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2899  @ epoch 38747 )\n",
      "[Epoch: 39560] train loss: 0.3132, train acc: 0.8770, val loss: 0.2493, val acc: 0.9143  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2899  @ epoch 38747 )\n",
      "[Epoch: 39580] train loss: 0.3322, train acc: 0.8647, val loss: 0.2502, val acc: 0.9180  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2899  @ epoch 38747 )\n",
      "[Epoch: 39600] train loss: 0.3084, train acc: 0.8675, val loss: 0.2484, val acc: 0.9167  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2899  @ epoch 38747 )\n",
      "[Epoch: 39620] train loss: 0.3024, train acc: 0.8791, val loss: 0.2461, val acc: 0.9194  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2899  @ epoch 38747 )\n",
      "[Epoch: 39640] train loss: 0.3007, train acc: 0.8805, val loss: 0.2457, val acc: 0.9211  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2899  @ epoch 38747 )\n",
      "[Epoch: 39660] train loss: 0.2973, train acc: 0.8811, val loss: 0.2434, val acc: 0.9177  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2899  @ epoch 38747 )\n",
      "[Epoch: 39680] train loss: 0.3049, train acc: 0.8757, val loss: 0.2399, val acc: 0.9143  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2899  @ epoch 38747 )\n",
      "[Epoch: 39700] train loss: 0.3030, train acc: 0.8775, val loss: 0.2429, val acc: 0.9153  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2899  @ epoch 38747 )\n",
      "[Epoch: 39720] train loss: 0.3104, train acc: 0.8720, val loss: 0.2462, val acc: 0.9177  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2899  @ epoch 38747 )\n",
      "[Epoch: 39740] train loss: 0.2980, train acc: 0.8832, val loss: 0.2486, val acc: 0.9164  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2899  @ epoch 38747 )\n",
      "[Epoch: 39760] train loss: 0.3034, train acc: 0.8764, val loss: 0.2473, val acc: 0.9079  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2899  @ epoch 38747 )\n",
      "[Epoch: 39780] train loss: 0.3180, train acc: 0.8751, val loss: 0.2534, val acc: 0.9083  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2899  @ epoch 38747 )\n",
      "[Epoch: 39800] train loss: 0.3520, train acc: 0.8547, val loss: 0.2539, val acc: 0.9184  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2899  @ epoch 38747 )\n",
      "[Epoch: 39820] train loss: 0.3121, train acc: 0.8709, val loss: 0.2356, val acc: 0.9204  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2899  @ epoch 38747 )\n",
      "[Epoch: 39840] train loss: 0.3233, train acc: 0.8632, val loss: 0.2496, val acc: 0.9116  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2899  @ epoch 38747 )\n",
      "[Epoch: 39860] train loss: 0.3092, train acc: 0.8744, val loss: 0.2450, val acc: 0.9180  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2899  @ epoch 38747 )\n",
      "[Epoch: 39880] train loss: 0.3063, train acc: 0.8757, val loss: 0.2500, val acc: 0.9164  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2899  @ epoch 38747 )\n",
      "[Epoch: 39900] train loss: 0.3003, train acc: 0.8793, val loss: 0.2569, val acc: 0.9113  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2899  @ epoch 38747 )\n",
      "[Epoch: 39920] train loss: 0.3070, train acc: 0.8756, val loss: 0.2525, val acc: 0.9201  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2899  @ epoch 38747 )\n",
      "[Epoch: 39940] train loss: 0.3027, train acc: 0.8798, val loss: 0.2594, val acc: 0.9086  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2899  @ epoch 38747 )\n",
      "[Epoch: 39960] train loss: 0.3033, train acc: 0.8814, val loss: 0.2426, val acc: 0.9157  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2899  @ epoch 38747 )\n",
      "[Epoch: 39980] train loss: 0.3177, train acc: 0.8652, val loss: 0.2418, val acc: 0.9177  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2899  @ epoch 38747 )\n",
      "[Epoch: 40000] train loss: 0.3019, train acc: 0.8780, val loss: 0.2370, val acc: 0.9218  (best train acc: 0.8872, best val acc: 0.9265, best train loss: 0.2899  @ epoch 38747 )\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtAAAAGpCAYAAACkkgEIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAABO4UlEQVR4nO3dd5iU1f3+8fuzvdJ36bD0KnWlWZAmTUVNfopdYzQmamKsqEjskubXmhBjNImJLTFWsFesFEUpotKkiSwgvSy7e35/zOwwszuzOwMzO7PM+3VdezHPecp89mHEe8+e5xxzzgkAAABAeFLiXQAAAABQnxCgAQAAgAgQoAEAAIAIEKABAACACBCgAQAAgAikxbuASDVr1swVFRXFuwwAAAAc5ubPn7/JOVdQtb3eBeiioiLNmzcv3mUAAADgMGdm3wZrZwgHAAAAEAECNAAAABABAjQAAAAQAQI0AAAAEAECNAAAABABAjQAAAAQAQI0AAAAEIGYBmgzG2dmX5nZMjObEmR/YzN71sy+MLM5ZtY7lvUAAAAAhypmAdrMUiU9KGm8pJ6SzjCznlUOu0HSAudcH0nnSro3VvUAAAAA0RDLHuhBkpY551Y450olPSlpUpVjekp6U5Kcc0slFZlZ8xjWBAAAABySWAbo1pLW+G2v9bb5+1zSqZJkZoMktZfUpuqFzOxiM5tnZvNKSkpiVC4AAABQu1gGaAvS5qpsT5fU2MwWSLpc0meSyqqd5NxDzrli51xxQUFB1AsFAAAAwpUWw2uvldTWb7uNpPX+Bzjntku6QJLMzCSt9H4BAAAACSmWPdBzJXUxsw5mliFpsqQX/A8ws0befZL0U0nveUM1AAAAkJBi1gPtnCszs8skvSopVdIjzrnFZnaJd/8MST0k/dPMyiUtkXRhrOoBAACoL8rKK7SvrEK5mbEcLICDFdN5oJ1zs5xzXZ1znZxzd3jbZnjDs5xzHznnujjnujvnTnXO/RDLegAAQM0qKpx27av2OFLCKC2r0CcrNuvn/5qv3zy/qNr+svIK7S+vqNb+1YYdumvWl3Iu8HGsbzfv0s0vLFZFhad97/5ydZ36sl78fH21a1SqvMbW3aX6fvvesGvfua9Mz362tsZjnHNyzunXT3+uXr95NegxlbXWZuWmXXpq7uqAtsc+/lZFU2Zq0bpt4RXt1efmV/V/r3+tuau2VLvmhm17tWVXqcrKK/TPj1apaMpMFU2ZqQVrtmrNlt36btuegOPXbNmt5z5bp6IpM/X2VxsD/r72lJb7tp1z+s3zi/Ta4g0R1VoXrOoHKdEVFxe7efPmxbsMAAAisnV3qUymhjnpEZ1XUeGUkhLsufzgyrzhIy01sj6ysvIKlTun215aon99vFpf3Hy8GmRVr3X+tz/oR3/+UI+ef6Qy01M0rFOzgP1vfvm9jurcTFnpqQHtzjm9tXSj7pj5pd66+riAfU/PW6Nr//uFCvMz9eTFQ3TO3+ZoUr9WunZcdz09zzOh12nFbbVh216dcP9sbdpZ6jv3xcuOVpfmecpKT9XW3aUqvv0NmUl3nHyEnpi7Wn8+a6B27ivT6LvflSR9/pvj1TA7XW9/tVEXPDrXd51rxnbTOUPba9vu/Trmd2+rdaNsfTBlpJ6Zv1b7yytkJqWmpKhDs1z96M8f6ryh7fWPj76VJJ3Sv7V27C3TX88dqI079mnwnW/qsQsHyWTKSEvRoA5NtHj9Nk28731JUv92jXTxMR31h9e+UpfCfA3vVqDJR7aVmWnEH97Ryk27fHUN6tBEzjl1bJan6yd0171vfqNHP1ilSf1a6erju2nnvjKlp6bom+93aPwRLbVo3TadcP/7Aff3sQsH6Zy/zVGzvIyAe/fCZUdp2vOLtfaHPdq0c58k6czB7VTUNEeL12/XNWO7adrzi9W8QaaemLMm4Jq3TeqlW19aoqz0VO3Y6/mB6/TitnpqXuBxlU7t31r/+2xd0H2SNLxrga4c01WTHvxAktS9Rb6Wbtjh279q+sSQ58aSmc13zhVXaydAAwAQe0VTZko6EAT2lJZrx7792rh9n3q3bijnnDbu2KfmDbIkScs27tB9by7TC5+v952zY+9+lZU7Nc7N0Htfl+jSf3+q168crhYNs3zvM/SuN7V19359/pvjtXrLLlU4qWvzfH23bY9mf7NJ735doouP6agtu0p1XLcC3fjcIl0+srPG3P2edlbpef74+lHKz0rTR8s3q0vzPJ32l4/0/fZ9AcesuHOCLvrnPL25dKOvrUfLBnru0mG66blF2rB9nxplp+sFvx7d04rbaGyvFrrwH3X///Mp47vr/17/WvvKqvdSR1PT3Axt3lVa+4EICwH6EBGgAQDRtq+sXClmSo+w1zZcFRVOHW+Y5dsuapqjVZt3Bz32omM66K+zAyekuuOU3rrx2erDFSrlZaZp574yzTh7oC751/zoFA0kkKW3jav2W426QIAGACCEoikz1bZJtmZfOzKq1924Y6/W/bBHp/zpw6heF0g2715znNo3za3z9w0VoGP6ECEAANHinNOshd+pPMwHqCrP8X/gau/+cv3+1aXau79ckrRrX5lvaMWaLZ4HnXZ62x58e5k2bNvr3bdbL32xPuC6T89do737y+Wc0+adB4Y1fLh8k6a/vFQrN+3SoDveJDwDURDpmP5YowcaAFAnNu/cp4G3v6FbTuql84YVaeP2vXr36xKd3L+1Fq3bpmUbd+qU/q21v9wpOyNVpWUV+vVTC7RjX5lumthDJz3wgfZ4g+/KuyZod6ln2EV2xoFf6z6/YJ2O61aovMw09b3ltYAxvbdO6qUtu0p1zxvfqEthnkb2KNRf3l0RUOOo7oWa9+0P2rZnf9DvoVvzfJ3Yt6X+8NrXMbhDAEL55IZRvucD6hJDOAAAdeLT1T+oeYMs7d1fruv/t1BzVm7RqukTfT29WekpOm9okf7y3opargQAHp9POz7iGWyiIVSAZnZuAEBUnRpkyEJleJakvfsrCM8AIhKP8FyTxBpQAgCot/7+wcqAoAwAhysCNAAgKm5+cUm8S8BhZlBRk5hcd8bZA3yvJx7RMibvES3nDm2v2yb1qtb+758O1v1n9K/x3Dk3jDqo9+zbpuFBnSdJ007oqRHdCkLuf/T8I/X8pUdFdM0B7RoddD2xQoAGAKCeOmNQW907uZ/aNcmp8/cuahr5e86bOjqi488a0i7kvgXTxoTc17FZrs4cfODcf/xkkH734z6+7cEdmvpeXzuum1ZNn6jFt4zVzSf2DKuu964ZERDCQzljUFt9dfu4au2nF7f1vf69X12SdMOE7r7XeZlpmjqxp84ZWlTtGkd1bqYT+7bS2F7N1bpRdrX9D59brMIGWTp1QGtf2/jeLSRJPzu2Y9B6uzbP06rpE/X8ZUcHXPObO8br/GHVa5jYp6UW3zJWy++c4Gv7ydEd9OgFg/TeNSN8be9fN0JTxnfXx9eP0ojuherbtpHm3DhKy++coMW3jPUd98zPh+m9a0aoYzPPdHVXjO6iVdMn6n+/iCxw1wUCNAAAdeTLWw+Eqaa5Gb7XwXoYJemdKkteS1K2dzGJ84a2112n9tGkfq2DHhfKqukT9eJlR1drb9skW29ffZz+8ZNBNZ7/ux/30cq7JignI/AxqqpBUJI6F+bp8YsG+7aDzVvQt20jXTK8k2979rUHgtdJfVvp3WuO823/9kdHSJLys9LUKCdDx3Q5sIz4n87yBNqxvZrrrauPC+i97tgsV6f5hdbGuRk6oU9LnT+syDe3cG5mms4NElSDadc0R+N6t9RD5wwMuv/VK47VT47qoNtPPkKZaan67yVD9cCZB3qLLxvZWT1bNtDH14/S//Ora9X0ibr42AP3YtEtY5WR5olq7f1+YJnv94PIX84p1qxfHSNJuuf0fnr76uP02x8dodE9m0uSirzf36MXHKk/nTVAq6ZP1PUTeujeyf181/jPJUP130uG6umfDfW1jfGe37NlA6WnpujXY7rqjEEHapU8vfe5mWlKTTGtmj4xYLXAdk1z9N41IzRv6mi1aZyjS4Z3ClgxszA/S6kpptzMNN0wobse/+lgDWzfWO2a5qjyY3JS31ZB728i4CFCAMBBWV6yUzkZqWrZMFsbt++Ndzn1gv+Ue+N6t9C/P1ktSTpnaJH+X3FbPTx7RcAUeU3zMqpd47NpY7S/vEL5WQceqkpJMZ0/rEh//3CVJGnZHeNVVuH01YYdmvHuci3buFPfbNzpO/6INg31p7MGaFCHJiq+/Q1J8i0i06axp+exZcMs9W3TSGu37tZdp/TRiQ+8r4uO6eALojPOHqi/zl6hT1Zu1tff71SvVg219LZxmrXwO1359OeSpNyMVA3r1Eyf3jRGW4Isaz372hFqkpuh3Mw0zXh3uSSprV9vupkFLJ4xpmcLXffMQqWmmCTpoXOK1WPaK5KkwR2aKCs9RT/zhvGiZrl+1/H8+dLlR2vRum2SpAfOrN6DnJJiWnrbOKWYqevUl3XxsR11w4Qe+nDZJs14b4Xe+7pEz/kNPzi+V4tq15Ckbi3yNc2vN7vYG+Zvem6Rfti9X22b5PhCbzAvXX60WjYMnLLt3WtGqKLCycxzX/w1zE4PCK8d/L73XxzXScXtG2tY52YB57Rs6Pl7Lm7fWEcGGSpz9dhuWrlpl+44pbfvPa6f0ENPzFkjyfP3P6538O+/Urswf0vh/0ODJFV4f9Kq+n0mEgI0ACCksvIKDbnrLQ1o10hTJ/bUtBcW6bZJvbV0ww5d9E/PlKLvXH2cbnh2YZwrPXgn92ul5xasr/3AQ5Cbkao/nx3YW/mLEZ31709W6z7vONas9FRdMryTPly+WdNO7KnWjbIDQnKlrPTUoEsa33xSL1+ATktNUVqqp3f3z2cP1CcrNuv0hz4OGNs6IcTY3/TUFD118RB1a5GvRjkHAvxbVw0PCGbtmubotpN768T735ckOTllpadqbK8W6tlypZZu2O4LkU1yM9QkN0POOV05pqvG9mqhJrkZKsjP9F3v/etG+HqoX7jsKO3Ye2AO7zk3jtKOvWVqmJ2uY7sW+IYgZGek6pqx3bRw7TY1zcvU0tvG+87p17aR73Vlr33v1g3Vu3XN43sr761/IB3WuVm1AFqV/1SNobx77QjfIj7+OhfmaZnfDzihakxJiTxQpqWmBK29sXdWiy7N84Kel5eZVu23EfmZafrZsR11Ur9W6tXq4MdJ16aoaa6+3bzb9/eWiAjQAIAAVz61QO99U6KT+rbWik07tWnnPr225Hu9tuR7SdIxv3s74Pjj/vBOndX2v18M09RnF2nJd9slSVMn9tDtM788pGveM7n/IQXoFJP8F0eceERLzVz4nSTp16O76qJjO1Qb7iBJrRtlB4Q0yRN2Hr9oyEHXEkpl8J3Yp3povnREJ328YktA2+COTasd17EgeND601kD9I8PV6lHiwaSPEMhQvWumpl+OapL0H1tGh/orezTplHAvsL8LBXme17/s0qou3RE56DX89c0L7PWY6Lh8YsGa9PO6j3tlRpkpatBkB+KXv7VMb5e17rSpXm+Hr9osAa0axz2OWam6yf0iGFVHved0V/zVm0JGPKRaBgDDQDwcc7pf5+t06adpXrkg5V656uSqF17xtnBx4tGYkC7xrrr1CN82z89pqM+CWOmgbtP63vI711VK+//3H88sI2vrW2TbE0Z73kIrGOzXF18bMdq4fnNq4br1SuODft95twwSnNuDG82hbtP6xs0JBc2yNLiW8bqomOqPzx2zdjueubnw8Kup6q2TXI09YSeB9U7Gmu3Teqlge3DD4iHalinZgc1bjc9NUWZaXXf2zqsU7Ogv82It4bZ6RrVo3m8y6gRPdAAAEnS3v3l6n7TKzG7fm3jJf29csUxGnfPbHUqyNXykl0B+7o2zw/YDra8b7O8TG3auc+3feqANmqUk67UlBR1a56vXaVlGvXHd337X7r8aJ3gHYpQKS8zTQ+c2V+dCvL09lcbtWHbXhUXNdZP/j5P5w1tr16tGuraZ74IeDAu1Uxtm+RU61n21ylET24ohX7fX3EtYfDUAW106oA2QfflZibf//LPGVoUdAaLaGqUk67yivq1qjMOXfL91wQASe7u17/WfW9+ozMGtdVri7/X/Js804H1ueW1uNU098bRenXxBp1W3FYVziktxTThiBb6xXGdfcH2eO+sANkZqbr/jP5avH57yOt9dP1IdbnxZUnS6B6FkqSR3UP3aPVu3VCrpk/UQ+8t10fLN2vaib3UNC/D9+v2ytkZnHP63Y/76KS+rbSntFxdZufpkuM6qSA/U396Z7nGhnioLBq+uWO8UhL4oapkNe/GyKbmw+GBIRwAcJhZtnGntu3erx179+vxT1Zr4w7PDBkfLtuk+d/+oPve/EaS9MScNdq8q1QDbntdX363XaVlFYf83j8e2Eb/d3r14RKjuntC7JtXDdeQjoFP/LdpnK2C/EydPaS9MtJSlJWeqrTUFP3prIEBD1Md2/XA4gwn9m3lGyohBS6MMbxrgdJTU3xz7V42MviY22AuPraTHr1gkDo0yw06VtXMdFpxW2Wlp6pxboZev3K4OhXk6dpx3fXpTWN07bjuQa4aHempKb7ZJ5A4PA9sEqeSDT3QAHCYGX33uypqmqN2TXP13tcluuFZzwwBZz78SdDjt+wq1fh7Z0flvVPNdHK/1srNSNP8b39QYYMsdWuer75tPUG4U0GeTh3QJuChtdd+Hd544JrGamZ7xxkf06WZb+aAW0/upXG9WwTMxhBLTXKrTzkH4PBEgAaABHfDswtVmJ+pK0Z3rbbv1heX6OMVm3XesPYa1qmZVm/ZLUlatXm3Vm3e7Ttu8kMf1UmtlXPUHt+rRcg5ck/t31rX/vcLSZ7FQILNUBFMTVNaVXjHoPoPcchMS9UIb883AEQTARoAEtzj3sU2rhjdVZ+s2KyuzfPVODdDe/eX65EPVkqSrnum5nmYq05TFivhLHyQlpqijLQUDevUVDedEN7SyZI0oH2jkPsqpwCLdIiD/0p2ABAuAjQA1BNvf7VRFzw6V5JnSEYsZ8w4WOEOBf369vG1H+T1/nUj9PZXJb6V04IpLmqi1o2y9asQcwwH8+Wt45SeyphiAJEjQANAgvp09Q/698erfduV4VmSpj2/KB4l1apvlQUwoqFN4xydM6R9jcc0zE7XB1NGRnRd/2W1ASASBGgAiJOKCqfP125V/3aN9ftXl6pPm0bq0aKBSnbu1X/nr9MTc1aHPPefH31bh5XWrmOzXK3YtEsD6nDRCgCIFwI0AMTBxh17dclj8/Xp6q26d3I/Pfj28niXdEgeOrdY//r4W3VomhvvUgAg5gjQABBFX363Xa8s2qBfjwmcMeOZ+WvVvEGWVm7epd+/slTb95b59v3qyQV1XGV4urfI19INO7Tk1rHqOe1VX/vUiT303ba9+tv7K31tnQvzdPNJveJRJgDUOQI0AETRj/78oXaXlusXIzopM80zxnbJ+u266j+fx7myyL1yxYH5mU/u10rPLVgvSfrpMR1164tL4lUWAMQdARoAomh3abkkyTmptKxCXae+HOeKouOeyf19AVqSTuzbUo98sFKPXzRYPVo0iGNlAFD3CNAAECXvf7PJ9/r/3vhaf3l3RRyria3+7Rpr1fSJ8S4DAOKCxdsBIErO/tuBpbLre3ju3iI/3iUAQMIiQANAFKzbuifeJQR1Yt9WAdvnDW2vu0/rW+M5t5/cO2D8c6VrxnbTs78YFtX6AKA+IkADQBQcNf2tuL33wpuPD7lv6sQeAdtXHt9Npw5oU+Pwi0EdmgRtv3REZ/VvxzzPAECABoB6rFerBsrPSg+5v3mDrIDttJQDS1f3atVAt5/c27fdr20j3XN6P3VtzvANAKhJTB8iNLNxku6VlCrpYefc9Cr7G0r6l6R23lr+4Jx7NJY1AcDh5N7J/SRJuRmp2uWdAaTS384rrnZ8jt/y1TN/eYwkqUOzXHVolqtWjbJjVygAHEZi1gNtZqmSHpQ0XlJPSWeYWc8qh10qaYlzrq+k4yT90cwyYlUTABxOTu3fWp0LPb3F95/ZX5L0yPkHQvOoHs2rnWNm1dqO6tyM8AwAEYhlD/QgScuccyskycyelDRJkv/s+05Svnn+Rc+TtEVSWdULAUCyO724rZ6atyagrYvfUIuR3ZuHHNf85a3j9MnKzVq5aVdMawSAZBHLMdCtJfn/a7/W2+bvAUk9JK2XtFDSr5xzFVUvZGYXm9k8M5tXUlISq3oB4KCUV7iYv8ddpx6h5y49KqCtYXbosc/+sjNSdVy3Ql1wVIdYlAYASSeWAbr67wk9Pc7+xkpaIKmVpH6SHjCzaktaOececs4VO+eKCwoKol0nABySJeu3H/I1cv3GJlf1zM+HKSXF1K9tI1/bbSf31ulHtg16/B2n9NaPBrQ55JoAAMHFMkCvleT/r3sbeXqa/V0g6X/OY5mklZK6x7AmAIi6W15cXOP+Fy87OuS+oqY5+tWoLnrgrAEB7deO6+Z7PbB99anjzhnSXqkpwfoppLMGt9cfa5nrGQBw8GIZoOdK6mJmHbwPBk6W9EKVY1ZLGiVJZtZcUjdJ9Xv5LgBJ54u122rcf0SbhsrPDP7ISaeCPP16TNeAtsL8TLVsmBX0eABA/MXsIULnXJmZXSbpVXmmsXvEObfYzC7x7p8h6TZJfzezhfIM+bjOObcpVjUBQCyUlld7dKOaChd8nHRlu39f8ku/PFr79nuu+fC5gVPRfXLDKO3dHzhdHQCgbsV0Hmjn3CxJs6q0zfB7vV5S6CW0AOAw0a1Fvj5dvVVTxnfX9JeX+torY3XT3ExfW2G+p/c52KwaVRdGAQDUPVYiBIA6kJ7q+ee2T5uGAe2VM3gcUaUdAJC4CNAAEIZ9ZeWqCGO6ujMGBZ8Zo3IER4rfQianF7fV9B/1iUp9AIC6Q4AGUK845/TM/LXata9M32/fe9DXeXj2CnW6YVbtB3p1m/qKbnxukUrLKrRu6x5f+859B9Z+Ondoe103LnAioWZ5nqEZlWOd/WfO+O2P+6i13wqAr1xxjO45vV9E3wcAoO7FdAw0AETbJyu36Kr/fK6r/vO5JOm9a0aoXdOcsM+vqHAqd063z/wy6P5PV/+g/WUVenLuGm3ZVao/ntZXTXIyJElPzFmtJ+asliQtuXWsek57NeDctJQUmd/jgHNvHK2sdE8/xcXHdtS8x+arS2Ge2jXJ0eotu6u9d/cWDdS9RbWp8AEACYYADaBOrSjZqZYNs5Vdw8Ihby39Xh2a5SkvM03fb9+rwgaZvgfrtu/ZH3Ds+m17tG7rHplJQzo2lST9+5Nv9fLCDdq2Z78Wrtumrs3z9PX3O7Vq+kRd/sRnmrnwO9/5RVNm6vlLj9KKTTvVPD9LZz78ScD173vzG23eVVqtxt88X33u54y0lIDpNAryDzwYeHyvFr6HAt+6ani1VaUAAPWHuRBTKyWq4uJiN2/evHiXAeAglFc4dbphloZ3LdA/fjKo2v6ZX3ynSx//NOi5q6ZPDLq/UU66tu72hOrbT+6tj1ds1ktffBfsEjH33jUj1Cg3XX1ufk1pKaZld06ISx0AgOgws/nOueKq7YyBBhDSonXb9P43B6Zm37F3v4qmzNTbSzf62r7fvleXPDZf67fu0d795dq+90APcWlZhbZ5w+0Nzy7UJf+aL0l69+sSPfrBSknSD7tKNenBD/TYx9+GDM+SdNfLXwbdXxmeJWnqc4viFp4lKSfzQK+6BV8kEABwGGAIB4CQTrj/fUnSL0d10ZVjumrx+u2SpAv+PlfDuxbo3a9LfMe+sniDUlNM5RVOc28crZ37ynTri4v19lclWnnXBD3+yeqAa9/y4hLd8uIS3/bna7bWWMtf3k38RUp37StTZppnvHQ9++UeACACBGgAAcrKK9T5xpcD2u578xvd9+Y3AW3+4blS5ZzGR97xRkD75U98FuUqE1NhfpbKK1cWpAcaAA5bBGgA+nD5Jp35V8/Dc91b5Ef9+vEcVlGXsjNStctvWjsAwOGJMdBAklu3dY8vPEvS0g074ljN4cN/OjsAwOGFAA0kib37y3Xqnz7QonXbJHnGHN8160sdNf2tOFd2eGHoBgAc/hjCASSwigqn0vIK7S+vUH5WusornNb9sCeihUMqfbZ6qz5dvVXnPzpHm3ZWn9cY0UHPMwAc/gjQQAK79aUl+vuHqyRJn//meP3l3eX60zvL9dylR2nhum06c1A77d1frtzM6v8pb9lVqvVb92jLrlL9sLtU+VmeYwjPsUUPNAAc/gjQQAKrDM+S1PeW13yvT/vLRyotq9BTc1dr0brteuWKYzTuntl66fKj1bt1Q7391UZd+Pe5qmAqNQAAoo4ADSSoyrHKwZSWVXiP8czLfOm/PQuM/P7Vr4JOL4fItWuSo9Vbdod9/PnDigIb6IkGgMMWDxECCWjTzn2+RUzCsbxkl6TgczMni86FeRrdo3nUrjfj7IG+15eN6KyPrx+lYZ2a6uJjOwY9vlWjrKi9NwAgsdEDDSSgbXv2135QErv95N6a+tyigLY3rhwuSVpeslOtG2Vr4/Z9Ovb3b0fl/X42vKPys9L1+EVDJEkPvVd9VcTGORlReS8AQOKjBxpIQPvLK+JdQlQ1b5CpQUVNIj7vyjFd1SwvM6Dtr+cW68xB7Xzb907upw+njPRtdyrIU1Z6qto1zdE/fzJIkvTpTWPUqSA3oB5JOrFvK825cZQeOmegzhnSXved0V9nD/Fcu2FOulbcOUHL7hiv/Kz0gBrOHOw55rIRnfXZTWP0ux/30Y8GtIn4+wMA1E8E6CR31PS3kmaZ5fpi7/5yrd2yJ95lRNXbVx+npy8ZWq39wTMH6KjOTX3bS28bpzeuPFYdm3nC7vjeLfQfv/N6tGygkd0LlZJyYIDxxCNaqlWj7KDve2zXAq2aPlFNcjP05lXH+dpvndRbkpSeairMz9LxvVrotpN766S+rTTthF565Ypj1LpRtlJSTGmp1f+ZvPOUI7Rq+kRdPbabGudm6LTitr6aMtNS9KMBbfSvCwdHcIcAAPUJQziS3Lqte7Ru6x7df0b/eJcCrwn3ztaKTbvi8t6rpk9U0ZSZIfd3KsjV8pJdapqboc27PNPhfXPHeC1ct02n/unDgGMvHdFJf529UqVlFdXmRu7WPF9ffb9Dx3Ur0MQ+LX3vmZWeqs6F+Xrpl0fro+Wb1aW5Z1nxj64fqRYNsmRB5ogL1hbKMz8fppId+zS6R3P9clQXXXhUh2rHZKSlqHuLBmFfM1g9fzyt70GfDwBIfARoIEHs2lemXr95Nd5l+ORnpmnhLWN94XbpbeOUYqbHP/lWTfMyfb+5SE9NUWG+Z0jE+cOK9KtRXTT1uUX62fBOykpL1R9f/1rpqZ6QO/vaESqrcGqUna6F67b55q9+9IIjtd1v3HdORppG+T0Q2LJh8B5mSUqJYLaLge0b+15fOaZr+CcCAOCHAI3DWll5he554xtddExHNcwJHMf69Nw1apyboeFdC/Tt5l2+3s5wbNy+V3NX/aCJfVqGPObh2Ss0onuhOhXkBbTvKS3X/ooKNfAbV7uiZKdG/vHdsN8/lk7o01Kzv9mkd64+TpL03jUjVLJzr7LSUyVJ5x/VQZt37gs4p03jHL3+62NV1CxX6akpevCsAZKky0d10eWjuviOa9vkwAqKx3Yt8L0e0a3woOuNpAcaAIBoIEAjLM45VTgpNczuvn1l5Vq8frsGtGusjdv36vE5q/WzYzspOyNV2/bsV1qKKTMtxTe+dP63W/TU3DW669Q+Sk0x/bCrVP/3xteaOrGnMtI8x1z/v4UqyMvQOUOL9Jd3l+vykV0CQvH8b3/Qku+2q0eLfBV7H1ibufA7PfD2Mj3w9jI9eOYAje3VXOc/OlfvL9vkO29EtwK9/VWJ3r9uhMbfM1u3n9JbE49oqTtmfamrj++mcueUm5Gm77fv1bDpb+muU4/QnTO/1I59Zbr0cenJi4doSMemWl6yUxf9Y55WbNqlk/q20gufr9ftM79U45x0fTbteP30H/P0xpff+9532gk99ZOjPUMIpr+89ND+gqLogTMHBGy3a5pTbenwpnmZGtm9UKcVH3hwLpIfQAAAqM/Mufq1VFlxcbGbN29evMs4bFT+en7mL49Wr1YNJXl6SI/53dv69ZguOr24rZ6at0Z3v/a1Nu8q1Yl9W6nCOd19Wl99tHyzerduqOG/e1u7SsslSWN7Nddfzin2Xbdf20ZasGarJKlpbobaNc3RZ6u3+vY9d+lReuT9lbr1pSWSpLtP66tTB7TRFU9+pucWrNdFx3TQNWO7a9rzi/Tk3DVBv4cXLzta2RkpGn33e762z24ao8a5GTWO5z1YvVs38C1gEg1XjemqP77+ddSuF8qtk3pp2vOLq7UP6tBEc1ZukeQZA11fVP7d1qeaAQD1i5nNd84VV2snQCev2d+U6Jy/zfFtz7lhlOas2qLnPlsf0FMazKR+rfT8gvVB950xqJ2emLM6rBouH9lZ97+1LKCtcU66ftgd2TzIRU1ztGpz4Kpx6amm/eX16/MdS29cOVyj7z4wTGR0j0IN71aoc4a0r5dhtD7WDACoX0IFaKaxS1LfbdsTEJ4ladCdb+qyxz+rNTxLChmeJYUdniVVC8+SIg7PkqqFZ0mHTXjuVJAbVkhcePPxOrlfKw3pGHy+5c6FeXrzKs9iIzef2FMPn3ekzhnS3re/qMowDQAAEBxjoJPQ8wvW6VdPLoh3GQjTz4Z3kuSZX3hfWeACK+2b5ujbzbvVOCdd+VnpumeyZzrCt5du1GtLNuiJOYHDXjoV5AUN4y9edrRaNw490wUAADiAAJ2ECM/1y7jeLSR5xpCv37ZXkpSdnqo3rhquprkZ+sOrX+nUKqvgjeheqBHdC3XDhB5asn67+rVrVON7HNGmYUxqj6WDGeoDAEA0EKCBBJfinabtqZ8N1Rtffq+JfVqqMD/Lt3/qCT1Dnpufla7BHZuG3F+fvXPNCO3dXx7vMgAASYgx0ECCKfZb7EOSbw2/tk1ydMFRHQLCczJrmJ2u5g24FwCAukeABhLMX88NfNg3hYVCAABIKAToJFCyY59++8pSFU2ZqXmrtsS7nMPe6B4HVtVrkpsR8fmNczO07I7xvm3yMwAAiYUAfRhZsn67duzdr9KyCp14//sqmjJTD89eoSPveEN/fme5JOnHMz6q05oimaP3mrHdYlbHijsnHNL5l47oFLT98pGdq7Wlpx74z6pzYV61/ZXyM0M/guC/PHW4qz8CAIC6QYA+TGzZVaoJ983WETe/pq5TX9bCddskSbfP/DIq16+cP7iqlXdN0J2nHFHjuQumjan1+iO7F6pTQeiweahSgoTQU/q3rtb269Fdq7X95ZyBumZs96DXver46qG/vOLA/NPfbdsTsqaFt4wNuc+/Wv9ADgAA4i+m/2c2s3Fm9pWZLTOzKUH2X2NmC7xfi8ys3MyCrwKBkM5++BMNuO31mL5HqHBrZjpzcDs98/NhIc9tlFP7MIa8zDSlVQm5P/Kbmq1nywa+15OPbKvPf3N8rdes1Cwvs1pb45x0/d/p/aq1D+tcfcaKTgW5kqTFNQRef/4B+vTitmFWCQAA6ouYBWgzS5X0oKTxknpKOsPMAubbcs793jnXzznXT9L1kt51zjFItxalZRXqdMMs/e39lfr1Uwv0/rJNB3Wdfm0bRa2mge0bq4dfyD0YqamBAXqa3/Rs6WkHPqpnDGqnnIzUsK+bXuW6Uyf20Kc3Ve8Vn33tCLUIMqtD5XCKjLTw/nMp8wvQ3VtEdk9W3jXB+54RnQYAAOpQLHugB0la5pxb4ZwrlfSkpEk1HH+GpCdiWM9ho+vUl1Ve4XTbS0v07GfrDvo695/RP4pVSb85MfR8xLUxk1KrpMaGOem+1129Y4kHd2iivm0bKZJ82bZJ4BLVS9ZvDxhjHOq4SpWzYFStLxT/HuiUFKljM08P9lmD22lglSnqqqqsK1h9AAAgMcQyQLeW5L+O8FpvWzVmliNpnKRnQuy/2Mzmmdm8kpKSqBdaX1TOpBEtmTX0qLZqGPn8upH0CldlUrUhHP4qH6Rz3mwaKmBWHas97YSeeuicgeHXEeSylWUFG0cdTFnFgeW2zUydvOH/2K4FGu9dVRAAANRfsQzQwdKGC9ImSSdK+iDU8A3n3EPOuWLnXHFBQUHUCqxPSssqfDNpREtNQxJO9nvAbmKflgd1/XuCjDEOpW/bRjUG1MrAXOFN0KGOrNp+fK/m1cdg15CDgwVzC3FC1aEhlfzys4Z1aur7waC8wqlxGOPBAQBAYotlgF4ryf8JqjaS1oc4drIYvhHSK4s2qOvUl6N+3dwQ06i9ffVxAbNLDPEuBd28QfWH8WrSooZe7LMGtwvYPm9okVrWcHyHZp7hFad6HywMtzc4mFCBOFLvXzcyaHu/do0kSf+9ZKgy01J9vedlFS7kDyPnDyvSYxcOikpdAAAgtkJPRHvo5krqYmYdJK2TJySfWfUgM2soabiks2NYS7119X8+13/nr43JtdNTU/TGlccqPTVFw3//jq+9g3fMbqXyck+X6ttXH6ee014Neb2qwbTqdfxVVPldREqKqX3T0McX5meFNad01R7koD3KNfVAB2lzIX5xUnUZ6Snju+u1xRt09fHddM6Q9r4x1a0bZ0uS8jJTQ64q2CgnXcd0Sc7frgAAUN/ELEA758rM7DJJr0pKlfSIc26xmV3i3T/De+gpkl5zzu2KVS311V2zvoxZeK7UuTC/1mMqZ5XIyUjTqO6FenPpxrCuXTVg+nMu1Gie4CJ5pq4gP1MlO/aFvlZE7xxcsDB/yfBOumS4Z8EV/wcSrxrTTT1bNtCIboUBDxhKnqErn6/Zqi5h/D0AAIDEENN5oJ1zs5xzXZ1znZxzd3jbZviFZznn/u6cmxzLOuoj55z+8t6KqF2vY0Ho3t3a+Ie+B84cEPK4ypDbtXme5t44usZrRpifIzJ1Yg/f67wgw1Rq7IEOsu9Qa81IS9Gkfq1lZtV6xNs08vROh+rlBgAAiSeWQzhwCEp2hu5BPRhVhw50ax5+j6f/WObsMGbaSEtJUUF+4HjpO085Qjc8u9C3XVtgPNgZPUzSpH6tNbiDZ9x2w+z0mk/wqhxSHa3x0aFUu7q3oeqQFgAAkLhYIzhBhRorW1Wbxtm6ckxXLb9zgu46NfSS2v7P3H18/Sj97xehVw6s1KuVZxGQqmOZQ82uURmuWzWqPnSjaqCuLTAW5kf2wGKlyuDdomFWjQ8xVhVstcJK0ZySueq1Kv+eIx3SAgAA4ocAnaB27SsL67h//3Swfjmqi1JTTGcMaqfXfn2sb9+71xzne+0fyFs0zAo5A8dtJ/f2vfaffi0cnQry9MCZ/fXH0/rVemywoRX+zh9WFLDdpnHwRU78PXXxEBXWMO46HJW3yT/wRzPbVnvIsYb3yMtM05Tx3aP35gAAICoYwpGgzvzrJ7UeE+xBti6FebrphJ46qW+rgBBY28p2x3RpptnfbNLgDk18bZVTxVVUSXc1Db84oU+rWuuWpJ4hlv0uzM/Uxh37NP6IwOnejmjdsNZrDvZOt1ebmoZp1PX6f5W/GQh2TxfdMraOqwEAAOEgQCeodVv3HNR5ZqYLj+5Qrb22aZMfPq9YC1ZvVVe/sdGpvsVLDqqUsPkv6BLqrVKDfANvXDlcuZmpWrlpl0rLKoKcdfD8f2aI5arapxW31XML1qu4fZPaDwYAAAmBAF3PpKWYb1q5SN06qZd27A0+NCQzLbVaD26ngjzN+/aHWodbHKr+bRtVa6uaWYMF6M7eJbJbNsyOXjG+tzlwj2uaju9QDevcLKz5rQEAQOIgQNczy+6coKIpMw/q3HOHFkV0/C2Temn8ES3Uo8pwi2iMCa4cstCzZQM9fF7xoV8wAsF6lCvbmuRkqKhpjq6f0EM/e2y+pNrHgD/9s6FK5WkCAACSBgE6SRxM6M1KT9Vx3Qqj9P7BC+jduoHys8Kbaq4upKWm6J1rRgS0lZXXfPMGdWD4BQAAyYR+s3qkaW5GvEuQFN1ZKapeKxFnc0tPq+tHCwEAQCIjQCe4Mwe3kyS9/KtjNP+mMQd9nWjm0oN5qK6oylzStS5YkkCZNSeDX9QAAIADCNAJ7o6Te2v+1NHVxiFH6siixlGq6OB0bZ6vj64f6dtm6WoAAFBfEaATnJmpaQ2r5IVr8pHtolCNx8EOswg2W0Ysp4gLJRGHiQAAgPqD300jYv7zNkdqSMcmtUw7F/t0G27v9xtXHqutu/fHuBoAAFDfEKCTRNXVBA9FfpbnYzOgXaOIz33y4qGeP+esjlo94RrZvVBvLd2obi3CGw7TuTC/9oMAAEDSYQhHAhvSMTGnR6tcFjw3xgusRNs5Q9pLkorbx3c8OAAAqN/qVwJKImkppgHtohf0otkDHQ21jX2udZaOgzCie6G+un2cMtNSfW3dW+Rr6YYduur4blF/PwAAcHiiBzoBOedUVuGUHsXl7QrzY7cc9cEIledjnfP9w7Mk/eSoDpKkEVFaMAYAABz+6IFOQPu9K9+lp0avF7ZFw8QK0JVi0dMcidOObKvTjmwb1xoAAED9Qg90AiqrqJCkqPZAAwAAIDrogU5A+8s8PdBph3GArm2kRjzmh64rj55/pNo3zYl3GQAA4CARoBPQfm8PdEYUh3AgcYzoznhrAADqMwJ0Atpf7gnQidoDHY1YH+417jujv2Z/XRKFdwQAAIgOAnQCqhzCwRho6aS+rXRS31bxLgMAAMCHhJaA9vseIkzsIRyHMuVcqFMTa7ZqAACA6gjQCahyCEcy9ECHelgwkX906FiQG+8SAABAHDGEIwGVlSfvEI5pJ/TUTc8tUoPs9HiXEtS8qaOVk5Fa+4EAAOCwRYBOQKW+hwgTsx82GlPM9WjZQJI0pGPTgPaT+7fWyf1bH/obxEizvMx4lwAAAOKMAJ2AKnugMxK0B7qnN/z+5Oiig75Gv7aNNG/qaAIpAACodwjQCcg3jV1KYvZAN83L1KrpEw/5OoRnAABQHyVmF2eS8z1EmMZfDwAAQKIhoSWg/ZUPEabw1wMAAJBoSGgJaOYX6yVF52E9AAAARBdjoBPQcws8AbpyNo6quhTmqX1T5iIGAACIBwJ0AssMMQb69SuH13ElAAAAqBTTIRxmNs7MvjKzZWY2JcQxx5nZAjNbbGbvxrKe+iaNMdAAAAAJJ2Y90GaWKulBSWMkrZU018xecM4t8TumkaQ/SRrnnFttZoWxqqc+StBZ7AAAAJJaLLs4B0la5pxb4ZwrlfSkpElVjjlT0v+cc6slyTm3MYb11DvGU4QAAAAJJ5YBurWkNX7ba71t/rpKamxm75jZfDM7N9iFzOxiM5tnZvNKSkpiVG7iIT8DAAAknlgG6GDxz1XZTpM0UNJESWMl3WRmXaud5NxDzrli51xxQUFB9CtNUCkkaAAAgIQTy1k41kpq67fdRtL6IMdscs7tkrTLzN6T1FfS1zGsq95gDDQAAEDiiWUP9FxJXcysg5llSJos6YUqxzwv6RgzSzOzHEmDJX0Zw5rqleYNsuJdAgAAAKqIWQ+0c67MzC6T9KqkVEmPOOcWm9kl3v0znHNfmtkrkr6QVCHpYefcoljVVN9kpafGuwQAAABUEdOFVJxzsyTNqtI2o8r27yX9PpZ11CfOVR0mDgAAgETCSh0J5uQ/fRjvEgAAAFADAnSC+XzN1niXAAAAgBoQoAEAAIAIEKABAACACBCgAQAAgAgQoAEAAIAIEKATVHoqyxACAAAkopjOA42D1zA7IyrXmTd1tPaVVUTlWgAAAAijB9rMLjOzxnVRDKKvWV6mWjfKjncZAAAAh41whnC0kDTXzJ42s3FmxtiCOsBdBgAASEy1Bmjn3FRJXST9TdL5kr4xszvNrFOMa0tq5GcAAIDEFNZDhM45J2mD96tMUmNJ/zWz38WwtqThnNPU5xZq0bpt8S4FAAAAtaj1IUIz+6Wk8yRtkvSwpGucc/vNLEXSN5KujW2J9dvGHXuVnpKixrmehwInP/SRysqd/vvzYXp18QblZaYpLcX0r49X618fr/adxxAOAACAxBTOLBzNJJ3qnPvWv9E5V2FmJ8SmrMPD99v3avCdb0qSXr3iWG3euU8fr9ji2/+zx+bHqzQAAAAcpHAC9CxJvtRnZvmSejrnPnHOfRmzyuq55xes06+eXODbHnvPewH7i6bMrPH8oqa5sSgLAAAAhyicMdB/lrTTb3uXtw1BOOc05ZkvAsLzwbj/zP7RKQgAAABRFU4PtHkfIpTkG7rBAixB7N1frv97/Ws9OXfNIV+rMD8rChUBAAAg2sIJwiu8DxJW9jr/QtKK2JVUPy1at00n3P9+vMsAAABAjIUzhOMSScMkrZO0VtJgSRfHsqj6Zt6qLYRnAACAJFFrD7RzbqOkyXVQS7309fc79OMZH8W7DAAAANSRcOaBzpJ0oaReknwDc51zP4lhXfVCbTNpHKzf/7hPTK4LAACAQxfOEI7HJLWQNFbSu5LaSNoRy6IONzkZqdXaju7cLOTxWenVjwcAAEBiCOchws7Ouf9nZpOcc/8ws8clvRrrwhLd0g3baz1mSMcmevLioZKk3aVl6jntwG2769QjtKxkp4Z2bKpx97ynVZt3+/Z1KsiLfsEAAACIinAC9H7vn1vNrLekDZKKYlZRPTHuntk17v/0pjFq4l2+W5JyMtLUKCddW3fv1zM/H6q2TXLUtkmOJOmuU/vomv9+rpm/PEZ7SsvVoiFT2AEAACSqcAL0Q2bWWNJUSS9IypN0U0yrqse+vHWcNu3cFxCeq+rYLLCHeWinpnr/upGSpIbZ6TGtDwAAAIemxgBtZimStjvnfpD0nqSOdVJVPZadkerrWQYAAMDhp8aHCJ1zFZIuq6Na6o19ZeUHdd7lI7tIknIzWcgRAACgvgonyb1uZldLekrSrspG59yWmFWV4L75fmfA9oo7J2jFpl3KSK15UpMLj+6gC4/uEMvSAAAAEGPhBOjK+Z4v9WtzSuLhHJ+t2ep7vWr6RElS50JmzgAAAEgG4axESJdpFSXb98a7BAAAAMRJOCsRnhus3Tn3z+iXUz8MLGoS7xIAAAAQJ+EM4TjS73WWpFGSPpWUtAH6vEfmSJIaZPEwIAAAQLIJZwjH5f7bZtZQnuW9k17LhtnxLgEAAAB1rOZpI4LbLalLtAupj/5yzsB4lwAAAIA6Fs4Y6BflmXVD8gTunpKejmVRAAAAQKIKZxDvH/xel0n61jm3NpyLm9k4SfdKSpX0sHNuepX9x0l6XtJKb9P/nHO3hnPtRFCyc5+KmuXGuwwAAADUoXAC9GpJ3znn9kqSmWWbWZFzblVNJ5lZqqQHJY2RtFbSXDN7wTm3pMqhs51zJ0Reevy1aJAV7xIAAABQx8IZA/0fSRV+2+XettoMkrTMObfCOVcq6UlJkyIvMXG1bZIT7xIAAABQx8IJ0GneACxJ8r7OCOO81pLW+G2v9bZVNdTMPjezl82sV7ALmdnFZjbPzOaVlJSE8dYAAABAbIQToEvM7KTKDTObJGlTGOdZkDZXZftTSe2dc30l3S/puWAXcs495Jwrds4VFxQUhPHWAAAAQGyEE6AvkXSDma02s9WSrpP0szDOWyuprd92G0nr/Q9wzm13zu30vp4lKd3MmoVVOQAAABAH4SykslzSEDPLk2TOuR1hXnuupC5m1kHSOkmTJZ3pf4CZtZD0vXPOmdkgeQL95ki+AQAAAKAu1doDbWZ3mlkj59xO59wOM2tsZrfXdp5zrkzSZZJelfSlpKedc4vN7BIzu8R72I8lLTKzzyXdJ2myc67qMA8AAAAgYYQzjd1459wNlRvOuR/MbIKkqbWd6B2WMatK2wy/1w9IeiD8cgEAAID4CmcMdKqZZVZumFm2pMwajk8KzAENAACQnMLpgf6XpDfN7FF5ZtH4iaR/xrSqBLZq0y5J0obte+NcCQAAAOIhnIcIf2dmX0gaLc/UdLc5516NeWUJ6t43v4l3CQAAAIijcHqg5Zx7RdIrZpYr6RQzm+mcmxjb0hJTVnpqvEsAAABAHIUzC0eGmZ1sZk9L+k7SKEkzajntMMYkIQAAAMksZA+0mY2RdIaksZLelvSYpEHOuQvqqLaE9MScNbUfBAAAgMNWTUM4XpU0W9LRzrmVkmRm99ZJVQAAAECCqilAD5Rn9cA3zGyFpCclMQAYAAAASS3kGGjn3GfOueucc50k3Sypv6QMM3vZzC6uqwIBAACARBLOQipyzn3gnLtMUmtJ90gaGsuiAAAAgEQV1jR2lZxzFfKMjU7aeaABAACQ3MLqgQYAAADgQYAGAAAAIhDWEA4zS5XU3P9459zqWBUFAAAAJKpaA7SZXS7pN5K+l1ThbXaS+sSwroR3Ut9W8S4BAAAAcRBOD/SvJHVzzm2OdTH1SX5WRM9fAgAA4DARzhjoNZK2xbqQ+iYvkwANAACQjMJJgSskvWNmMyXtq2x0zt0ds6oS1Lebd/leH9+rRRwrAQAAQLyEE6BXe78yvF9J6z/z1vpem8WxEAAAAMRNrQHaOXdLXRRSH/iH5sL8zPgVAgAAgLgJGaDN7B7n3BVm9qI8s24EcM6dFNPKEpB/p3MBARoAACAp1dQD/Zj3zz/URSH1wY59ZfEuAQAAAHEWMkA75+Z7/3y37spJbGu27PG9NjEIGgAAIBmFs5BKF0l3SeopKauy3TnXMYZ1JaQUMjMAAEDSC2ce6Ecl/VlSmaQRkv6pA8M7kkqqX4JmFg4AAIDkFE6AznbOvSnJnHPfOuduljQytmUlpkY5ST2LHwAAABRegN5rZimSvjGzy8zsFEmFMa4rIR3bpZnvNR3QAAAAySmcAH2FpBxJv5Q0UNLZks6LYU0Jy8yCvgYAAEDyqPEhQjNLlXSac+4aSTslXVAnVSWo/eUV8S4BAAAAcRayB9rM0pxz5ZIGGt2tkgIDNDcEAAAgOdXUAz1H0gBJn0l63sz+I2lX5U7n3P9iXFvC2bRzX7xLAAAAQJzVOg+0pCaSNssz84aTp/PVSUq6AH3nrKW+1/TJAwAAJKeaAnShmV0paZEOBOdKLqZV1QOMagEAAEhONQXoVEl5Cj7cN+kDNAAAAJJTTQH6O+fcrYdycTMbJ+leecL4w8656SGOO1LSx5JOd87991DeEwAAAIilmuaBPqQxCt4p8B6UNF5ST0lnmFnPEMf9VtKrh/J+AAAAQF2oKUCPOsRrD5K0zDm3wjlXKulJSZOCHHe5pGckbTzE9wMAAABiLmSAds5tOcRrt5a0xm97rbfNx8xaSzpF0oyaLmRmF5vZPDObV1JScohlHbycjNS4vTcAAAASQzhLeR+scB4+vEfSdd4FW0Jyzj3knCt2zhUXFBREq76IDWzfOG7vDQAAgMQQzjzQB2utpLZ+220kra9yTLGkJ71TwjWTNMHMypxzz8WwroPmmHsEAAAg6cUyQM+V1MXMOkhaJ2mypDP9D3DOdah8bWZ/l/RSooZnSSqvIEEDAAAku5gFaOdcmZldJs/sGqmSHnHOLTazS7z7axz3nIgq6IIGAABIerHsgZZzbpakWVXaggZn59z5sawlGj5ZeajPVQIAAKC+i+VDhAAAAMBhhwAdJsfwDQAAAIgAHbZ9ZRXxLgEAAAAJgAAdpu1798e7BAAAACQAAnSY1v2wJ94lAAAAIAEQoMPUsmF2vEsAAABAAiBAhymFOwUAAAARoMPHJBwAAAAQATps5GcAAABIBOiwMQ00AAAAJAJ02CpI0AAAABABOmzEZwAAAEgE6LCxlDcAAAAkAnTYyM8AAACQCNAAAABARAjQYaIHGgAAABIBOmwvfrE+3iUAAAAgARCgw/TZ6h/iXQIAAAASAAE6TJVDOFo3ytbH14+KbzEAAACIGwJ0hBrnpqtFw6x4lwEAAIA4IUCHqfIZQpPFtQ4AAADEFwE6TJULqRj5GQAAIKkRoAEAAIAIEKABAACACBCgw8Q6KgAAAJAI0BFjCDQAAEByI0CHiaW8AQAAIBGgw+bLz0zDAQAAkNQI0GHyTWMX5zoAAAAQXwToCNEBDQAAkNwI0AAAAEAECNAAAABABAjQAAAAQAQI0BFiCDQAAEByI0CHiXmgAQAAIMU4QJvZODP7ysyWmdmUIPsnmdkXZrbAzOaZ2dGxrOdQ/L/iNpKky0d2iXMlAAAAiKe0WF3YzFIlPShpjKS1kuaa2QvOuSV+h70p6QXnnDOzPpKeltQ9VjUdirxMz61qmpcR50oAAAAQT7HsgR4kaZlzboVzrlTSk5Im+R/gnNvpnG9wRK78FvxLNAzhAAAAgBTbAN1a0hq/7bXetgBmdoqZLZU0U9JPgl3IzC72DvGYV1JSEpNiw2U8RggAAJDUYhmggyXNav24zrlnnXPdJZ0s6bZgF3LOPeScK3bOFRcUFES3SgAAACACsQzQayW19dtuI2l9qIOdc+9J6mRmzWJYEwAAAHBIYhmg50rqYmYdzCxD0mRJL/gfYGadzcy8rwdIypC0OYY1HTSGQAMAAECK4SwczrkyM7tM0quSUiU94pxbbGaXePfPkPQjSeea2X5JeySd7vdQYUIyhkADAAAktZgFaElyzs2SNKtK2wy/17+V9NtY1gAAAABEEysRAgAAABEgQIcpwUeWAAAAoI4QoAEAAIAIEKABAACACBCgAQAAgAgQoMPECGgAAABIBOiIMQ80AABAciNAAwAAABEgQAMAAAARIECHiWmgAQAAIBGgI2ZiEDQAAEAyI0ADAAAAESBAAwAAABEgQIeNQdAAAAAgQEeMeaABAACSGwEaAAAAiAABGgAAAIgAATpMzAMNAAAAiQAdMcZAAwAAJDcCNAAAABABAjQAAAAQAQJ0mBgCDQAAAIkAHbZ/ffytJMnEIGgAAIBkRoAO04fLN0uS1m3dHedKAAAAEE8E6Ait37o33iUAAAAgjgjQAAAAQAQI0BFiHmgAAIDkRoCOECsSAgAAJDcCdITIzwAAAMmNAA0AAABEgAANAAAARIAADQAAAESAAA0AAABEgAAdKabhAAAASGoxDdBmNs7MvjKzZWY2Jcj+s8zsC+/Xh2bWN5b1RMORHZrEuwQAAADEUcwCtJmlSnpQ0nhJPSWdYWY9qxy2UtJw51wfSbdJeihW9URLs7zMeJcAAACAOIplD/QgScuccyucc6WSnpQ0yf8A59yHzrkfvJsfS2oTw3qigoUIAQAAklssA3RrSWv8ttd620K5UNLLwXaY2cVmNs/M5pWUlESxRAAAACAysQzQwTprgz6BZ2Yj5AnQ1wXb75x7yDlX7JwrLigoiGKJkTOjDxoAACCZpcXw2msltfXbbiNpfdWDzKyPpIcljXfObY5hPVFBfAYAAEhuseyBniupi5l1MLMMSZMlveB/gJm1k/Q/Sec4576OYS0AAABAVMSsB9o5V2Zml0l6VVKqpEecc4vN7BLv/hmSpklqKulP3qERZc654ljVFA2M4AAAAEhusRzCIefcLEmzqrTN8Hv9U0k/jWUN0WYM4gAAAEhqrEQIAAAARIAAHQbnv3w3HdAAAABJjQAdhoD8TIAGAABIagToMFS4oNNXAwAAIAkRoMPgH5/pgAYAAEhuBOgwBA7hIEIDAAAkMwJ0GFzwFcgBAACQhAjQYWASDgAAAFQiQEeIERwAAADJjQAdBibhAAAAQCUCdBj8x0CzlDcAAEByI0CHgYVUAAAAUIkAHQZGcAAAAKASAToMjkHQAAAA8CJAhyFgJUKGcAAAACQ1AnQY6IAGAABAJQJ0OAIWUqELGgAAIJkRoMMQMI0d+RkAACCpEaDDwBAOAAAAVCJAh6HC+S+kAgAAgGRGgA5D4CwcRGgAAIBkRoAOA0M4AAAAUIkAHYaAhwjjWAcAAADijwAdDv9p7EjQAAAASY0AHQZGcAAAAKASAToMLqAHmi5oAACAZEaADoOjDxoAAABeBOgwMAsHAAAAKhGgw0B+BgAAQCUCdBga56THuwQAAAAkCAJ0GHIy0uJdAgAAABIEARoAAACIAAEaAAAAiAABGgAAAIgAARoAAACIQEwDtJmNM7OvzGyZmU0Jsr+7mX1kZvvM7OpY1gIAAABEQ8ymlzCzVEkPShojaa2kuWb2gnNuid9hWyT9UtLJsaoDAAAAiKZY9kAPkrTMObfCOVcq6UlJk/wPcM5tdM7NlbQ/hnUAAAAAURPLAN1a0hq/7bXetoiZ2cVmNs/M5pWUlESlOAAAAOBgxDJAW5C2g1oV2zn3kHOu2DlXXFBQcIhlAQAAAAcvlgF6raS2ftttJK2P4fsBAAAAMRfLAD1XUhcz62BmGZImS3ohhu8HAAAAxFzMZuFwzpWZ2WWSXpWUKukR59xiM7vEu3+GmbWQNE9SA0kVZnaFpJ7Oue2xqutgLZg2RmbBRqUAAAAgmcQsQEuSc26WpFlV2mb4vd4gz9COhNcoJyPeJQAAACABsBIhAAAAEAECNAAAABABAjQAAAAQAQI0AAAAEAECNAAAABABAjQAAAAQAQI0AAAAEAECNAAAABABAjQAAAAQAQI0AAAAEAECNAAAABABAjQAAAAQAQI0AAAAEAECNAAAABABAjQAAAAQAXPOxbuGiJhZiaRv4/T2zSRtitN710fcr8hwvyLD/YoM9ysy3K/IcL8iw/2KTDzvV3vnXEHVxnoXoOPJzOY554rjXUd9wf2KDPcrMtyvyHC/IsP9igz3KzLcr8gk4v1iCAcAAAAQAQI0AAAAEAECdGQeincB9Qz3KzLcr8hwvyLD/YoM9ysy3K/IcL8ik3D3izHQAAAAQATogQYAAAAiQIAGAAAAIkCADoOZjTOzr8xsmZlNiXc98WRmq8xsoZktMLN53rYmZva6mX3j/bOx3/HXe+/bV2Y21q99oPc6y8zsPjOzeHw/0WZmj5jZRjNb5NcWtftjZplm9pS3/RMzK6rTbzDKQtyvm81snfcztsDMJvjtS/b71dbM3jazL81ssZn9ytvOZyyIGu4Xn7EgzCzLzOaY2efe+3WLt53PVxA13C8+XzUws1Qz+8zMXvJu18/Pl3OOrxq+JKVKWi6po6QMSZ9L6hnvuuJ4P1ZJalal7XeSpnhfT5H0W+/rnt77lSmpg/c+pnr3zZE0VJJJelnS+Hh/b1G6P8dKGiBpUSzuj6RfSJrhfT1Z0lPx/p5jcL9ulnR1kGO5X1JLSQO8r/Mlfe29L3zGIrtffMaC3y+TlOd9nS7pE0lD+HxFfL/4fNV8366U9Likl7zb9fLzRQ907QZJWuacW+GcK5X0pKRJca4p0UyS9A/v639IOtmv/Unn3D7n3EpJyyQNMrOWkho45z5ynk/5P/3Oqdecc+9J2lKlOZr3x/9a/5U0qvIn7/ooxP0Khfvl3HfOuU+9r3dI+lJSa/EZC6qG+xVKst8v55zb6d1M93458fkKqob7FUpS3y9JMrM2kiZKetivuV5+vgjQtWstaY3f9lrV/A/w4c5Jes3M5pvZxd625s657yTP/7AkFXrbQ9271t7XVdsPV9G8P75znHNlkrZJahqzyuPnMjP7wjxDPCp/ncf98uP91WR/eXq9+IzVosr9kviMBeX99foCSRslve6c4/NVgxD3S+LzFco9kq6VVOHXVi8/XwTo2gX7ySWZ5/47yjk3QNJ4SZea2bE1HBvq3nFPPQ7m/iTDvfuzpE6S+kn6TtIfve3cLy8zy5P0jKQrnHPbazo0SFvS3bMg94vPWAjOuXLnXD9JbeTp7etdw+Hcr+D3i89XEGZ2gqSNzrn54Z4SpC1h7hcBunZrJbX1224jaX2caok759x6758bJT0rzxCX772/UpH3z43ew0Pdu7Xe11XbD1fRvD++c8wsTVJDhT8Eol5wzn3v/Z9ShaS/yvMZk7hfkiQzS5cnDP7bOfc/bzOfsRCC3S8+Y7Vzzm2V9I6kceLzVSv/+8XnK6SjJJ1kZqvkGQ470sz+pXr6+SJA126upC5m1sHMMuQZlP5CnGuKCzPLNbP8yteSjpe0SJ77cZ73sPMkPe99/YKkyd6nYjtI6iJpjvdXNDvMbIh3bNK5fuccjqJ5f/yv9WNJb3nHgB02Kv8h9TpFns+YxP2S9/v7m6QvnXN3++3iMxZEqPvFZyw4Mysws0be19mSRktaKj5fQYW6X3y+gnPOXe+ca+OcK5InS73lnDtb9fXz5RLgicxE/5I0QZ6nt5dLujHe9cTxPnSU54nYzyUtrrwX8owvelPSN94/m/idc6P3vn0lv5k2JBXL84/KckkPyLsqZn3/kvSEPL+y2y/PT8IXRvP+SMqS9B95HqaYI6ljvL/nGNyvxyQtlPSFPP8YtuR++b7Po+X5deQXkhZ4vybwGYv4fvEZC36/+kj6zHtfFkma5m3n8xXZ/eLzVfu9O04HZuGol58vlvIGAAAAIsAQDgAAACACBGgAAAAgAgRoAAAAIAIEaAAAACACBGgAAAAgAgRoAKhHzKzczBb4fU2J4rWLzGxR7UcCQHJLi3cBAICI7HGepYMBAHFCDzQAHAbMbJWZ/dbM5ni/Onvb25vZm2b2hffPdt725mb2rJl97v0a5r1Uqpn91cwWm9lr3hXWAAB+CNAAUL9kVxnCcbrfvu3OuUHyrMx1j7ftAUn/dM71kfRvSfd52++T9K5zrq+kAfKsLip5lst90DnXS9JWST+K6XcDAPUQKxECQD1iZjudc3lB2ldJGumcW2Fm6ZI2OOeamtkmeZYS3u9t/84518zMSiS1cc7t87tGkaTXnXNdvNvXSUp3zt1eB98aANQb9EADwOHDhXgd6phg9vm9LhfPygBANQRoADh8nO7350fe1x9Kmux9fZak972v35T0c0kys1Qza1BXRQJAfUfPAgDUL9lmtsBv+xXnXOVUdplm9ok8nSNneNt+KekRM7tGUomkC7ztv5L0kJldKE9P888lfRfr4gHgcMAYaAA4DHjHQBc75zbFuxYAONwxhAMAAACIAD3QAAAAQATogQYAAAAiQIAGAAAAIkCABgAAACJAgAYAAAAiQIAGAAAAIvD/AQTYIBJFAov+AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 864x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.36      0.50       154\n",
      "           1       0.91      0.97      0.94       703\n",
      "           2       0.90      0.91      0.91       702\n",
      "           3       0.91      0.95      0.93       703\n",
      "           4       0.99      0.99      0.99       702\n",
      "\n",
      "    accuracy                           0.93      2964\n",
      "   macro avg       0.91      0.84      0.85      2964\n",
      "weighted avg       0.92      0.93      0.92      2964\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 504x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaMAAAGQCAYAAAADew/3AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAABP40lEQVR4nO3dd3wUVdfA8d9JAoYOCZuEKkhRimABBAvNAqEFEAXLy6MgRcUCWB8b3S72hyZ2BRSlKEWkGEBAQKWLohAIkEKPCBI29/1jlrCbuoHdTHZzvnzmQ2bn3pl7Mps9e+/cnRVjDEoppZSdQuxugFJKKaXJSCmllO00GSmllLKdJiOllFK202SklFLKdmF2N0AppdT56SZdfDYteo75Rny1r4LQnpFSSinbac9IKaUCXEgQ9Cs0GSmlVIATsWVkzacCP50qpZQKeNozUkqpAKfDdEoppWwXosN0Siml1PnTnpFSSgU4CYJ+hSYjpZQKcDpMp5RSSvmA9oyUUirA6TCdUkop2+kwnVJKKeUD2jNSSqkApx96VUopZTu9N51SSinlA9ozUkqpAKfDdEoppWyns+mUUkopH9BkpIoUEVkmIvfksX2CiDxzvvtRKpgIIT5b7KLJyAYicq2I/CgiR0XkkIisFJHmbtvLiMjfIjIvh7olReRZEdkuIsdFZK+IzBeRm9zK7BKRE659nFnezmFf74uIEZG6bo8VmRdxEblLRFa4P2aMGWyMGW1Xm/IjIpNc5yZDRO7KYftFIvKNiKSJyAEReclt2y4RuSFL+Wy/AxG5XUTWuc7rftf5v/Y82txZRFaIyBERSRKRySJS7lz3pwpfiIT4bLEtBtuOXEyJSHngG+AtIAKoBowE/nUr1su1fpOIVMmyiy+BOKAvUAmoDbwBdM5SrqsxpqzbMiRLO64F6vgmKuVmA3Af8HPWDSJSElgELAFigOrAJwXZuYgMA14HxgHRQE3gXaznxLmqAIwBqgINXO16+Tz2p1SBaTIqfPUBjDGfG2OcxpgTxpjvjDEb3cr8B5gAbATuOPOg613zjUCcMWaNMeaUa1lgjHnI2waISBhWMhySX1kv9lXL1bu6W0T2iMhhERksIs1FZKPr3fbbbuVHiMgnOdQPy7LfBli/g1auHsAR1+MfiMgYt3JxIvKriBwTkT9FpGMObawjIktE5KCrN/KpiFR02/64q4eZ5urVXO96vIWrB3JMRJJF5LX8fh/GmHeMMYuBkzlsvgvYZ4x5zRhz3BhzMst5z5OIVABGAfcbY75y7SPdGDPXGPNoHvWqunrKEW6PXe76XZQwxnzmeg79Y4w5DEwGrvGiPctEZIyrl/+3iMwVkUjX7/eYiKwVkVpu5d9wPUeOich6EbnObds8EXnVbX26iEz19ndT3IkP/9lFk1Hh+x1wisiHIhIrIpXcN4pITaAt8Klr6eu2+QZgjTEm8TzbMBSIL8gLoReuAuoBvbHeuT+F1d5GwK0i0qYgOzPGbAMGA6tcPbuKWcuISAvgI+BRoCLQGtiVw+4EeJ6z7/xrACNc+7gYKyk3N8aUAzq47eMN4A1jTHmsXuSMgsSQg5bALtew2gHXi/mlBajfCggHvi7IQY0x+4BVwM1uD98OfGmMSc+hSmtgi5e77wP8H1YPv47rOO9j9fq3Ac+5lV0LXOba9hnwhYiEu7b1A/5PRNqLyB1Ac8DrN1jFnQ7TqQIzxhwDrgUM1jvQVBGZIyLRriJ9gY3GmK3A50AjEbncta0ykHRmXyIS4ep5HBWRrO/EZ7m2nVkGuOrUAAYBz/o4tNGud/rfAceBz40xKcaYvcBy4PK8q5+T/sBUY8wiY0yGMWavMea3rIWMMTtcZf41xqQCrwFnkqMTuABo6Ool7DLG/Onalg7UFZHKxpi/jTGrz7O91bFevN/ESozfArNdw3dneJw3rCG4MyKBA8aY0+dw7M+A2wBERFzt+CxrIRG5Eatn7u3z431jzJ/GmKPAfOBPY8z3rjZ+gdt5N8Z8Yow5aIw5bYx5Fev3frFrWxLWm48Psd4E9DXGpJ1DnCpAaTKygTFmmzHmLmNMdaAx1gvT667NfbF6RGfe0f6A9eIAcBCo4rafQ64ew5VYf9juuhtjKrotk12Pvw6Mcr14+FKy288nclgv6+PjgdXD+TO/QiISJSLTXENxx7Cu01QGK1EBD2P1lFJc5aq6qvbHGlb9zTXk1OU823sCWGGMmW+MOQW8gpVgGriV8ThvWNefzjgIVM46pOmlL7GGPKti9XwM1puETCLSEitB9TLG/O7lfr0+7yIyXES2ud48HcG6VlXZrfw3QCiw3RjjMWlD5c13c+l0mK7Ycr2T/wBoLCJXYw11PSnWrKYkrOGv21wvQIuB5iJS/TwOeT3wstv+AVaJyO3nsc+COA6UdluPyaOsyWdfe/BuEsbzrn01cQ253Qln/+pc10yuBS50lXvR9fgfxpjbgCjXY1+KSBkvjpebjeQfU15WYV2L6l7QisaYI8B3wK1YQ3SfG2My2+Lqfc8B+rmuefmU6/rQ467jV3Il2qPg8eo3Fmtor4qI3ObrNgQzndqtCkxELnG9Q6zuWq+BNXyyGqsHtAhoiDW2fhlWz6k0EOsaAluKNZRzlVjTvEtgXYvwVn2gqdv+AbrieR0iTETC3ZYS5xJrLn4FWotITdcF+SfzKJsMVM8yjOXuPeBuEbleREJEpJqIXJJDuXLA38AREamGdY0JsK4Zua5TXID1Qn8Ca+gOEblTRBzGmAzgiKuKM6/gXOckHOtFtoTr93fm7+wToKWI3CAioVg9sgNYL8D5cvVmnwXeEZHuIlJaREq4rj2+lF99rF5PX6xrR5lDdCLSGFgAPGCMmetNW85BOeA0kIr1/HoWKO/WhtbA3a729QXecp0rVUxoMip8aVi9nTUichwrCW0GhmO9a3zLGJPktuwEPubsUF1PrOGMT7BeIHdizbjLOotsrnh+zuhrANd1nMz9u8oeMMaccKv7P6wX5TPL+74K3hizCJiO1UtY74olN0uwLqQniciBHPb1E9YL2Hisd9k/YPVushoJXOEq8y3wldu2C4AXsJJCElYv6L+ubR2BLSLyN9Z1jD7GmJxmybn7Dut3djUwyfVza1d7t2P1yiYAh7GmY3dzDdl5xRjzGjAMeBrrhX0P1gSMWV5Un4PV8042xmxwe3w44ADec3u+eDuBwVsLsa4p/Q4kYCX+PZD5cYePgCGu634rsN5ovO+6vqXyESLis8Uu4tZTV0opFYAeKn2/z17I3/jnHVsykt4oVSmlAlwwdCB1mE7lS0TuyDLk56+hnCKvKP8uXJ9fyqlt/82/dq77zGl/f4vbB1aV8gXtGal8GWPOfAC32CvKvwtjTKwf9umPKfnKx/T7jPxLL2YppYKZz8bWguH7jIpyMuKTZfl+njEo3dm2DnPX7ra7GYWua/OabNx92O5m2KJJzUps3+/rzyEXfRdXqcCJ0xl2N8MWpcICvzfjS0U6GSmllMqfnR9W9RVNRkopFeCCYZgu8NOpUkqpgKc9I6WUCnA6TKeUUsp2dn4Pka8EfgRKKaUCnvaMlFIqwNn5PUS+oslIKaUCnOgwnVJKKXX+tGeklFIBTofplFJK2U5n0ymllFI+oD0jpZQKcKLDdEoppWwXEvjJSIfplFJK2U57RkopFeiC4K7dmoyUUirAiQ7TKaWUUudPe0ZKKRXodJhOKaWU7XSYTimllDp/2jNSSqlAFwQ9I01GSikV4CQIrhnpMJ1SSinbac9IKaUCnQ7TBYYdm9excMZETEYGl1/bgWs63uqxffuvq1g252NEQggJCeGm3oOoWbcRAG/+9y5KXlCKkJBQQkJCuOepNwGYOel5DibvBeDkib8JL1WWgc+8XbiB5eO3DWuZ/fG7ZGRkcFXbWNp36+Ox/eeVi1n6zXQASoaX4ua7HqTqhXUAWL7gK1Yvmw/GcFW7TrTu2BOAfQl/MvP9N/j35AkqOWK4494nCC9dpnAD88Iva1fx/rvjycjI4PrYbvTo09dj+97du3jnlTHs3LGd2+4eTLdb7sjc9s3Mz1k8fw4iQs1adbjv0acpWfICPpr0FutXryAsLIzoqtW5/5GnKVO2XGGHlqf1a1Yx5e1XcTozuKlzHL3u+I/H9sSEXbzx4ij+/GM7/9f/Xnr0uTNz2z294yhVujQhISGEhoby2qSPMrd989V0vv36C0JCQ2nW8hruHvxgocXkrZXLl/PSC+PIcGbQ4+Ze9BswwGO7MYaXnh/Hivh4wkuFM2rsOBo0bJRv3c8//YRpn31KaGgo17Vuw9BHHi3UuLwSBMN0QZ+MMjKcLPj8Xe54eCzlK1VmyvMPU79JSxxVa2aWqX3JZdRv2hIRITlxJzMnPc99oyZlbu87/AVKl63gsd+bBz6Z+fOiLyZzQami9YKckeHk6w/fYuATL1IhojJvPDuEhle2IqbahZllIhwx3Pv0q5QuU45tG37ii6mv89DIt9i/Zyerl83noZFvERpWgikvPUmDy1rgiKnOjCmv0fX2gdRp0JSffljAsm+/oOMtd9kXaA6cTifvvfUKz7z4JhGVo3hyyN00a3UdNS6snVmmbLny9Lt/GD+t/MGj7sEDKcybNYPxUz7nggvCeW30U6xcuoh2HbrQ9IoW3NH/XkJDw/hk8tt8/fmH3DlgSGGHlyun08nEN15i1CtvE+mIYvjg/9DimuuoWeuizDJly5dn4IOPsHrFshz3MXb8/yhfsaLHYxt/WceaFfG8+d5nlChZkiOHD/kxinPjdDp5fuxoJkx+j+joaO7ofStt2rWjTt26mWVWLI9nd0ICc+YvYNPGDYwdNYpPpk3Ps+7aNWtYtmQxX3w9m5IlS3Lo4EEbowxuQX/NaN/O36kUVZVKjiqEhpWgUbPWbN+wyqNMyfBSmRcA0/89WaB3GcYYtq5fTqPmbXza7vO1+8/tREZXJTKqCmFhJbisZVu2rP/Ro0yt+o0oXcZ6Z39h3QYcPZQKQMq+3VxY5xJKXhBOaGgoF13ShM3rVgKQuj+Riy5pAkD9xlewce3yQozKOzu2byWmanWiq1SjRIkSXNP2Rtb9GO9RpkKlCOpe3JCwsOzvxzKcTk79+y9O52n+/fckEZEOAJo2u4rQUKt8vQaNOXggxf/BFMAfv22hSrXqxFS14r6u/U2sWekZd8VKEdS7pGFmHN6YP3smN9/+H0qULJm5j6Jm86aN1KhRk+o1alCiZEk6dOrEsqVLPMosW7KELt3iEBGaNL2MtLRjpKam5Fl3xvRp3H3PAEq6Yo+IjCz02LwSIr5b7ArBtiMXkmNHDlK+UuXM9fKVKpN2JPu7m99++ZF3nx3I528/R7e+D2c+Lgifvv40k8c+yM/x87PV2/3HZsqUq0hkdDW/tP9cHT18gIoRjsz1ihGVOXr4QK7lf1q2gEuaNAcgpnot/tq+ieNpxzj170l+2/ATRw5aiSqmRi22/Gwl8w1r4jMTWFFy6EAqkY6ozPWIylEcPOBdOyMrR9G11x3ce0d3BvTuQukyZWja7Kps5ZYunMvlzVv5rM2+cDA1lcqO6Mz1yo4oDqYW4PwIPPvoAwwd2JcFc7/OfHjfnt1s3fQrj9x7N08+NIg/ftvqy2b7REpyCjFVYjLXo6OjSUlO9iyTkkxMjHuZGFKSU/Ksm7BrFz+vX8+dfXrT/z//x+ZNm/wcyTmSEN8tNvHLMJ2IhAODgbrAJuA9Y8xpfxwrfybbIzl9EdUll1/NJZdfTcLvm1g252PuHDoOgLsee4VyFSM5fuwIn7zxFJEx1bmw/qWZ9bas/YFGLdr6rfXnzHgXN8COrb/y0w/zuf+Z1wGIrnYh7br0ZtILj1MyvBRVal5ESGgoAL0HDGfWR++w6OtPaHRFK0Jz6FnYLqfYvXzD93faMdauiuedj7+iTNlyvDb6v8R/P5/WN8Rmlpn56fuEhIZx3fUdfdVinzA5PdcL8Eb3xbenEFnZwZHDh3j2kSFUr3khjZtegdPp5O+0Y7z87lT++G0rL454ksmfzypS04lzjt2zfSbH54XkWdfpPE3asWN8/Pk0Nm/axGPDh/LtwkVFKvZg4a80+CHQDCsRxQKvelNJRAaKyDoRWTdp0qT8K3ihfMXKHHPrERw7fICyFXMfZriw/qUcTt3PP38fBaBcRatbXqZ8RS65rBX7dv2eWTbD6eS3X36kUbPWPmmrL1WIcHDErddy5NABylfKPsSwb/dffDHlNe4eOooy5cpnPn5V21iGjv0f9z/zGqXLlKOyq+cXVbUmA594kaFj3uXyVu2IjKrq/2AKKMIRxcHUs0Nohw6kZA615WfTz2uJiqlKhYqVCAsL46pr27J969l3w8u++5b1a1by0BMji9wLUmVHFAdSz/YGDqSmEFHZu7gBIl1lK1aKoOW1bfljm9UDinRE0eq6dogI9Rs0IiQkhGNHj/i07ecrOjqapP1JmevJyck4oqKylIkhKcm9TBKOKEeedaOjY2h/w42ICJc2aUJISAiHDx/2czQFJyHis8Uu/kpGDY0xdxpjJgK9gOu8qWSMmWSMaWaMaTZw4ECfNKRqrfocStnH4QNJOE+ns2VdPPWbtvQocyhlX+a7pv27d+B0nqZUmfKc+vck/578B4BT/57kr62/4Kh6dgLAX9t+ITKmuscwYFFR46KLOZC0l4Mp+zl9Op1fVy+j0RWew0qHD6Tw4esjuW3w4ziqVPfYlnb0cGaZTetWcvnV7Twez8jI4PvZn9Lq+i6FEE3B1L24Afv37iF5/z7S09NZuWwRzVp59RSkclQ0f2zbzL8nT2KMYdMv66hesxZgzdCbNf1jHh/1MheEh/sxgnNT7+KG7EvcQ9L+vaSnp7N8yXdcdbV3cZ88cYJ//jme+fOv69ZQs7Y1s7LltW3Y+Ms6APbuSeB0ejrlK1T0SwznqlHjS9m9O4G9iYmknzrFwnnzaNOunUeZNu3a8c2c2Rhj2LjhV8qWLYfDEZVn3XbXX8/aNasBSNi1k/T0dCpVqlTo8eUrCK4Z+WuMJf3MD8aY03a+gwwJDaVjn3v57I2nMRkZNL3mJqKqXsj6H74F4Mo2ndn280o2rl5MaGgYYSVK0nPAE4gIx48dZsaEMYDVC2rcoi11GzfL3PeWdfE0LmITF84IDQ2lx3+GMPmlJzEZGTRv04GY6rX4cfFcAK6+viuLvv6Yf/4+xlcfWNPVQ0JDeXj0uwB89MYojv99jNCwMHr+Z0jmRIdfVy1l5fdzALi02bU0b93BhujyFhoaRv8hjzD2yYfIyMigXYcu1Kh1Ed/N/QqAm7r25PChgzxx/12c+Oc4IiF8+9U0xk+ZRr0GjWl5XXseu+8/hIaGUqtOfW7o1B2A995+ldPppxj9uDWtuX6Dxgx8+HG7wswmNCyMQQ89yohHHyQjI4MbYrtSs3Yd5s+eCUBs3M0cPniAYYPu4p9/jhMiwpwvp/HOh9M4dvQo456xpiw7nU7aXN+BK6+y3rzc0Kkbb744miF39SGsRAkeevK5ItcrDAsL44mnnubegfeQkZFBXI+e1K1bjy+mTwPglt59uK51G1bEx9M1tgPh4eGMHDMuz7oA3Xv05LlnnubmuK6UKFGC0WOfL3KxBwvJaRz1vHcq4gSOn1kFSgH/uH42xpjyudV1Yz5Z9qfP2xYI7mxbh7lrd9vdjELXtXlNNu4uekMghaFJzUps33/U7mYUuourVODE6Qy7m2GLUmG+64aMrf+Kz17In/r9EVuyrV96RsaYUH/sVymlVA6C4A4MQT+1WymlVNGnyUgppQKciPhs8fJ4HUVku4jsEJEnctheQUTmisgGEdkiInfnt88i+CERpZRSBVKIw3QiEgq8A9wIJAJrRWSOMcb909D3A1uNMV1FxAFsF5FPjTGnctuv9oyUUkoVRAtghzHmL1dymQbEZSljgHJidbXKAoeAPG98oMlIKaUCnYjPFvebD7iWrB/6rAbscVtPdD3m7m2gAbAP6+YHDxlj8pw2qcN0SikV6Hw4TGeMmQTkdQucnA6WdWp5B+BXoD1QB1gkIsuNMcdy26n2jJRSShVEIlDDbb06Vg/I3d3AV8ayA9gJXJLXTjUZKaVUoCvc2wGtBeqJSG0RKQn0AeZkKbMbuB5ARKKBi4G/8tqpDtMppVSAK8xbFLlu8TYEWAiEAlONMVtEZLBr+wRgNPCBiGzCGtZ73BiT+3fYoMlIKaVUARlj5gHzsjw2we3nfcBNBdmnJiOllAp0QXA7IE1GSikV6ILgTuI6gUEppZTttGeklFKBTofplFJK2S0YvvBPk5FSSgW6IOgZ6TUjpZRSttOekVJKBbog6BlpMlJKqUAXBNeMdJhOKaWU7bRnpJRSgU6H6ZRSStktGKZ26zCdUkop22nPSCmlAp0O0ymllLKdDtMppZRS569I94zubFvH7ibYpmvzmnY3wRZNalayuwm2ubhKBbubYItSYfqe+LzpMJ1/nTjttLsJtigVFsqtob3sbkahm+H8kuPpxfOclykRyklnht3NKHThoSHFMm6wYveZwM9FOkynlFLKfkW6Z6SUUsoLQTCBQZORUkoFOAmCa0Y6TKeUUsp22jNSSqlAF/gdI01GSikV8ILgmpEO0ymllLKd9oyUUirQBcEEBk1GSikV6AI/F+kwnVJKKftpz0gppQJdEExg0GSklFKBLgjGuIIgBKWUUoFOe0ZKKRXodJhOKaWU3SQIkpEO0ymllLKd9oyUUirQBX7HSJORUkoFvCC4A4MO0ymllLKd9oyUUirQBcEEBk1GSikV6AI/F+kwnVJKKftpz0gppQJdEExg0GSklFKBLvBzkQ7TKaWUsl+xSEYrly8nrnMnunbswNTJk7NtN8bw4rixdO3YgVt6dGfb1q351t3+22/0vf02enWP48H77uPvv/8ulFgKommHy3h96xu8uf0t4h7rnm17mYpleGTmo7z8y6uMW/U8NRrVAKBK/aq8tP7lzOWDwx/R6cHOHnW7DuvGDOeXlIssVxihFNjKFcvp0aUT3WI78P6UnM/5S+PG0i22A7dmOecjnn6K61tfyy3du3nUeXz4MPrc3IM+N/eg80030OfmHn6Po6BWLl9Ot06xdOnQgfdyea6/MHYsXTp0oFf3OLZt3ZJv3aNHjjCofz+6duzAoP79OHb0aKHEUlDFOXZEfLfYxK/JSEQq+3P/3nA6nTw/dgzvTJjIV3PmsmDePP7cscOjzIrl8exOSGDO/AU8M2IkY0eNzLfuyGef5cGhw/hy1mza33A9H06dWuix5UVCQuj/1j2M6zyWoY2Hck2fa6nWoLpHmR5P9mTXr7t49PLhvH3XW9w1vh8A+3/fx2NXPspjVz7K480f59Q///LTrDWZ9SKrR3LpjU1ITUgt1Ji85XQ6eXHMGN7630Rmus7bX396nvOVy+PZvTuB2fMW8PSIkTw/emTmtq7de/D2hEnZ9vviq68xbebXTJv5NdffeCPtb7jR77EUhNPpZNyY0bw7cRJfz53LgnnfZn+ux1vP9bkLFvDsyJGMGTkq37pTp0ymRctWzF2wkBYtW/FeDsndbsU5dgAJEZ8tdvFLMhKRriKSCmwSkUQRudofx/HG5k2bqFGjJtVr1KBEyZJ06BTLsqVLPMosW7KELt3iEBGaNG1KWloaqampedZN2LWTK5s1A6Blq6tZvOi7Qo8tL3Vb1CXpzyRSdqbgTD/Nj9NX0rxbc48y1RtWZ9OSTQDs274PRy0HFaIqeJS59PpLSfozmQO7D2Q+9p/X7uLTxz/GGOP/QM7B5k2bqF7Tdd5KlKRDbCzLlmQ550tzPucAVzZrRoUKFXLaNWC9w160YCEdO3XyaxwFtXnTRmrUPPt87RjbKVvcS5csoWvcmbgvIy3tGKmpKXnWXbpkCd26xwHQrXscSxcvLvTY8lOcYw8W/uoZjQWuM8ZUAW4GnvfTcfKVkpxMTJWYzPXo6BhSklM8y6SkEBPjXiaalOTkPOvWqVcvMzEtWriQpKQkf4ZRYBHVIji452wCObj3IBHVIjzKJGxI4KoeVwFQp3ldHBc6iKge6VHmmt7XsHLaisz1K7s249DeQyRsTPBj689Pakqyx/mMio4hJSXLOU9OIdqjTDSpycle7f/n9euJiIyk5oW1fNJeX0lJ9nweR8VEk5ziGVNKSrJH3Gee03nVPXTwIA5HFAAORxSHDh3yZxjnpDjHDlgTGHy12MRfyei0MeY3AGPMGsCrCwsiMlBE1onIukmTsg+TnAtD9nfvWYdFc3qHLyJ51h05egzTP/+c227pxfF/jlOiRAmftNdXcrqlfNY4Z734NWUqleGl9S8TOySWnb/sJOO0M3N7aIkwruzajNVfrgKgZKmS9HzyZqY/N92/jT9POZ/PbIWyV/RyvHzhvG+LXK8Icok766tLbs91b+oWYcU5diAorhn5a2p3lIgMy23dGPNaTpWMMZOAM1nInHB7YTxX0dExJO0/22tJTk7CERWVpUy0R88mOTkZR1QU6enpudatfdFFTJg8BYCEXbtY/kP8ebfVlw4mHiSyxtlLdpHVIjm877BHmRNpJ/hf/3cz19/+811Sdp7tQVweezk7f9nJ0RTrom10nRiiakfx8i+vWPusHsmL617iyZZPcjT5iB+jKZio6BiP85mSnJT57jazTEw0yR5lkrM9L3Jy+vRplnz/PZ/O+MJ3DfaR6BjP53FKUjJRWWKKio7xiNt6TjtITz+Va92IyEhSU1NwOKJITU0hIsKzh10UFOfYg4W/ekaTsXpDZxb39bJ+OmaOGjVuzO7dCexNTCT91CkWzptPm3btPMq0adeeb+bMxhjDxg0bKFu2HA6HI8+6hw4eBCAjI4PJEydwS+9bCzOsfP25dgdV6lbBUSuK0BJhXN37GtbNXetRpnSF0oSWsN6PXH/PDWxbvo0TaScyt1/T51qPIbo9m3czoEp/htS5jyF17uNg4kEeb/ZYkUpEYJ3zPWfOW/opFs7P4Zy3zfmc52fN6lXUuqi2x3BPUdGo8aXsTkgg0fV8XTB/Xra427Zvx9zZZ+L+lbLlyuFwROVZt2279syZNRuAObNm0659+0KPLT/FOXbA+tCrrxab+KVnZIwZmds2EXnYH8fMTVhYGE889RT3DhxARkYGcT16ULduPb6YPg2AW3r34brWrVkRH0/X2I6Eh4czcszYPOsCzJ83j+mffwbA9TfcSFyPnoUZVr4ynBlMfXAKT81/mpDQEJa+v4TErYncOOgmABZN/I5qDaoz5IMHyHBmkLgtkQn3nO0llSxVkiY3NGHS4Il2hXDOwsLCePy/T3H/oAFkODPo1qMHderW40vXOe/Vuw/Xtm7NiuXxxMV2JLxUOCNGj82s/+Sjj7B+7U8cOXKEjte3Y/B9Q+h+880AfDd/Ph1ji94QHVhxP/nU09w74B4yMjLo3qMndevVY8Y0K+5b+/ThutZtWBEfT5eOHQgPD2fU2HF51gXoN+AeHh06jFkzvySmSlVeGT/ethhzU5xjB4LiQ69S2DOiRGS3MaamF0V9MkwXiEqFhXJraC+7m1HoZji/5Hh68TznZUqEctKZYXczCl14aEixjBsgPNR33ZBX+s302Qv5I1NvtiW12XE7oCDI4UopVYToV0ick6L54RSllApUQXAvHb8kIxFJI+ekI0ApfxxTKaVU4PLXBIaiecMypZQKRjpMp5RSym45fcg90ATBSKNSSqlApz0jpZQKdEHQrdBkpJRSgS4Ihuk0GSmlVKALgmQUBJ07pZRSgU57RkopFeiCoFuhyUgppQKdDtMppZRS5097RkopFeiCoGekyUgppQJdEIxxBUEISimlCpOIdBSR7SKyQ0SeyKVMWxH5VUS2iMgP+e1Te0ZKKRXoCnGYTkRCgXeAG4FEYK2IzDHGbHUrUxF4F+hojNktIlH57Vd7RkopFehEfLfkrwWwwxjzlzHmFDANiMtS5nbgK2PMbgBjTEp+O9VkpJRSKpOIDBSRdW7LwCxFqgF73NYTXY+5qw9UEpFlIrJeRPrmd1wdplNKqUDnw26FMWYSMCmPIjl1n7J+mWoYcCVwPdYXqq4SkdXGmN9z26kmI6WUCnSFO7U7Eajhtl4d2JdDmQPGmOPAcRGJB5oCuSYjHaZTSilVEGuBeiJSW0RKAn2AOVnKzAauE5EwESkNXAVsy2un2jNSSqlAV4g9I2PMaREZAiwEQoGpxpgtIjLYtX2CMWabiCwANgIZwBRjzOa89qvJSCmlAl0hj3EZY+YB87I8NiHL+svAy97uU4fplFJK2U57RkopFej03nT+VSos1O4m2GaG80u7m2CLMiWK7zkPDy2eAxXFNW6fCvxcVLST0Ulnht1NsEV4aAh/nzptdzMKXdmSYTxU+n67m2GLN/55h0P/nLK7GYUuonTJYv13rs4q0slIKaWUF0ICv2ukyUgppQJdEFwz0n6iUkop2+XaMxKRNM7eb+hM2jWun40xpryf26aUUsobgd8xyj0ZGWPKFWZDlFJKnaMguGbk1TCdiFwrIne7fq4sIrX92yyllFLFSb4TGETkOaAZcDHwPlAS+AS4xr9NU0op5ZUgmMDgzWy6HsDlwM8Axph9IqJDeEopVVQEfi7yapjulDHG4JrMICJl/NskpZRSxY03PaMZIjIRqCgiA4B+wGT/NksppZTXgmACQ77JyBjziojcCBzD+l7zZ40xi/zeMqWUUt4pJteMADZhfY+5cf2slFJK+Uy+14xE5B7gJ6An0AtYLSL9/N0wpZRSXhIfLjbxpmf0KHC5MeYggIhEAj8CU/3ZMKWUUl4KgmtG3symSwTS3NbTgD3+aY5SSqniKK970w1z/bgXWCMis7GuGcVhDdsppZQqCoJ8AsOZD7b+6VrOmO2/5iillCqwIPj+hbxulDqyMBuilFKq+PLm3nQO4DGgERB+5nFjTHs/tksppZS3gmCYzpvO3afAb0BtYCSwC1jrxzYppZQqCBHfLTbxJhlFGmPeA9KNMT8YY/oBLf3cLqWUUsWIN58zSnf9v19EOgP7gOr+a5JSSqkCCeYJDG7GiEgFYDjwFlAeGOrXVimllPJeEFwz8uZGqd+4fjwKtPNvc5RSShVHeX3o9S1c32GUE2PMg3nU7ZvXQY0xH3nVOqWUUvkL8p7RuvPYb/McHhOgK1ANKNRktHL5cl58fhwZzgx69OpF/wEDPLYbY3hx3DhWxMcTXiqc0ePG0aBhozzrvv3mGyxbsoQQCaFSZASjxz1PVFRUYYaVrx9XLOeVF1/A6XTSvefN3H1P9rhffuF5Vi6PJzy8FCPGjKVBw4YkJe3n2f8+ycEDBwkJEXr0uoXb7/w/AJ54ZDgJu3YCkJaWRrly5fj8y68KPbb8XHJjQ3q+3IuQ0BBWf7CS71/1/NaTUhVLcfuEO6lc20H6v+l8PvgT9m/dD8BtE+6kUcfG/J2axgvNx2bW6fRsFy7t3IQMY/g7JY1PB33Msf1HCzWu/KxauYLXX34RZ4aTbt170rffPR7bjTGMf+kFfly5nPDwcJ4ZOYaLGzQEoEenDpQuU5rQkFBCQ0N5/7PpADz9+CPs3rULOHvOP5r+ZaHG5Q1//J0fPXKEx4YPY9/evVStVo2XXxtP+QoVCj22fAXzNSNjzIfnulNjzANnfhYRAe4AHgdWA2Nzq+cPTqeTcWNGM3HKe0RHR3N771tp264dderWzSyzIj6e3QkJzF2wgE0bNzBm5Cg+nT49z7p39evPkAcfAuDTjz9m4rvv8syIEYUZWp6cTicvjB3Lu5MmEx0Tzf/16U2bdu24qM7ZuFcuX86ehARmfTufzRs38vyYUXz02TRCQ8MY+shjNGjYkOPHj3Nn71to2aoVF9WpywuvvJpZ/7WXX6Js2bJ2hJcnCRFuGX8r73Z5iyN7jzB8+WNs+nYTyb8lZZa58dGO7N24l/f6TCaqfjS3jO/NO53fBOCnj1ezfMIP3DnZs4O/ePz3zBtljVq3vrctHZ+MZcaD0wovsHw4nU5efWEsb/xvElHRMfS7ow/XtWlH7Tp1MsusWrGcPbsT+GL2t2zZtJGXxo3hvY8/y9z+zqSpVKxUyWO/Y158JfPnN199mTJF8Jz76+986pTJtGjZiv4DBvDe5Mm8N2UyQ4c/YmOkwctv+VREwlxfP7EVuAHoZYzpbYzZ6K9j5mTzpo3UqFmT6jVqUKJkSTrGdmLZkiUeZZYuWULXuDhEhCZNLyMt7RipqSl51nV/ET554kSR6yVv2bSJGjVrWG0vUZKbYjuxbOlSjzI/LF1C527dEBEubdqUv9PSSE1NxeFw0KCh9W65TJky1K59ESnJKR51jTF8v3AhHTt1LrSYvHVhs1qk/pnKwV0HcaY7+fnL9VzapYlHmZgGMfy+dDsAKb8nE3FhBOWirDtg/blyB/8cOp5tv/+mncz8uWSZkphcB7HtsXXzJqrXqEm16jUoUaIEN3SIJX6Z5zmP/2EpsV2sc964iXXOD6SmerV/YwyLFy3kpo6d/NH88+Kvv/OlS5bQrXscAN26x7F08eJCj80rxeRzRgUmIvdjJaErgY7GmLuMMdv9caz8pCSnEBMTk7keFRNNckqyZ5mUZKLdykRHx5CSnJJv3bdef52b2rfj22/mct8DuV5Cs4UVU5XM9ejoaFKTs8ad4hF3VHQ0qVl+N/v27uW337bRuInni/kv69cTERlJzQsv9EPrz0+FqhU5svdw5vqRvUeoULWiR5l9m/bSJK4pADWbXUilmhFUqOZZJiedR3RlxO9jaNa7OfNGf5Nv+cKUmpJCVHSW85manK2M+zl3REeTmmK90RARHrpvEHfdfiuzZn6Rbf+//ryeiIhIahTBc+6vv/NDBw/icFjD7w5HFIcOHfJnGOdOk1GuzkwBvxaYKyIbXcsmESnUnpHJ4e2rZP0GqZzKiORb94GHH+a7JUvp3KUr0z799Pwb60M5vWuXLE+0/OL755/jPDr0YR55/Ilsw3EL5s+jQ6ei9w4Zcvl7yhLrole+o3Sl0jy6+klaD27L3g2JZJzOyHff346Yy4j6T7Nu+lpaD27joxb7hslhvpF3z3Xr/4nvf8SHn8/gtbf/x8zp0/hlvedl40UL5nNjEewVgX//zlXh8MtsOqzPJK0ADnP2Q7P5EpGBwECAiRMn0rf/PfnUyF90TDRJSWevFaQkJWebaBAVHUOyW5nk5CQcUQ7S00/lWxcgtnNnhtw7mPseeCDbNrtER0eTnLQ/cz05OZnKWdpulXGLz61Meno6jw59mNjOnWl/w40e9U6fPs3S77/nk+kz/BjBuTuy9wgVq5297lGxWkWOZplo8G/aST4b9Enm+rPbRnFw10Gvj7F++joGfXUv88d8e/4N9pGoqGhSkrOcT4fnOXdkOeepbmUcrnMfERFJm/bXs3XLZi6/shlgnfNlS77nA9ekhqLGX3/nEZGRpKam4HBEkZqaQkREhJ8jOUdBMIEhrxDWAevzWPJSDXgD63uPPgQGAY2BNGNMQm6VjDGTjDHNjDHNBg4c6HUQeWnU+FJ2JySQmJhI+qlTLJg/jzbtPD8u1bZ9O+bOno0xho0bfqVsuXI4HFF51k1wzS4CWLZ0KbUvusgn7fWVho0bsydhN3sTE0lPP8V38+fRpq1n3K3btePbOXMwxrBpwwbKli2Lw+HAGMPo556l9kUXced/7sq2759Wr6JW7doeQx5Fye71CTjqRhFxYSShJUK5oteVbP52k0eZUhVKEVoiFIBWd1/Nnyt2eFwTyomjjiPz58adLyX59+Q8She+Bo0as2d3Avv2JpKens73C+dzXdu2HmWua9OO+d9Y53zzxg2UKVuWyg4HJ078w/Hj1nWyEyf+Yc2qHz0mu6xds5oLa9X2GAYsSvz1d962XXvmzLK+NWfOrNm0a1807w8tIj5b7OKv2XSPAIhISaAZcDXQD5gsIkeMMQ3Pdd8FFRYWxpNPPc29A+4hIyOD7j16UrdePWZMs2ZB3dqnD9e1bsOK+Hi6dOxAeHg4o8aOy7MuwBvjX2PXzp2EhIRQpWpVnn5uRGGF5JWwsDAe++9TDBk8EKczg7gePahTty5fzrDe2fa6tTfXXtealfHxxHWKJTw8nBFjxgDw6y8/8+3cOdStV5/bevUE4P4HH+ba1q0BWDh/fpEdogPIcGYwc9gM7p1zvzW1+6NVJG3bzzX3XAvAyikriL44hjun9CXDmUHSb0l8fu/ZXlLfD+6mbut6lI0sy8g/xjB/zLes/nAVXUfHEVUvGpNhOLTnEDMe/NyuEHMUFhbG8Mf/y8P3DSYjw0mXuB5cVKcuX31h9WB73nIrV197HT+uiOeWbp24IDycp0dY5/zQwYM8MexhwJqZdlNsJ1pdc23mvr9fWHSH6MB/f+f9BtzDo0OHMWvml8RUqcor48fbFmOwk5zGSz0KWF8h8TjQkAJ+hYTrNkKtgGtc/1cENhlj7vaibeakM/8x/GAUHhrC36dO292MQle2ZBgPlb7f7mbY4o1/3uHQP6fsbkahiyhdkmL8d+6zbshrk9b4bG7nsIFX2dI98ubedJ8C04HOwGDgP0Cec0FFZBLW9x+lAWuAH4HXjDGH86qnlFKq4IraR0vOhb++QqImcAGQBOwFEoEj59NQpZRSOQvqa0ZuCvwVEsaYjq47LzTCul40HGgsIoeAVcaY586jzUoppYKM375CwlgXozaLyBGsO34fBboALQBNRkop5StBMLXbL18hISIPYvWIrsHqWa0EVgFTgU15VFVKKVVAdg6v+Uq+yUhE3ieHD7+6rh3lphbwJTDUGLM/j3JKKaWUV8N07jfgCgd6YF03ypUxZtj5NEoppVQBFIeekTFmpvu6iHwOfO+3FimllCqQIMhF53TZqx7W1G2llFLKJ7y5ZpSG5zWjJKw7MiillCoKgqBr5M0wXbnCaIhSSqlzI767s5Bt8h2mE5FsX22Y02NKKaXUucrr+4zCgdJAZRGpBJnfNlUeqFoIbVNKKeWNwO8Y5TlMNwh4GCvxrOdsuMeAd/zbLKWUUt4K6g+9GmPeAN4QkQeMMW8VYpuUUkoVM95M7c4QkYpnVkSkkojc578mKaWUKggR3y128SYZDTDGHDmz4vpOogF+a5FSSqmCCYJs5E0yChG3AUkRCQVK+q9JSimlihtv7k23EJghIhOwPvw6GFjg11YppZTyWlBPYHDzODAQuBdrRt13wGR/NkoppVQBBMH3GeUbgjEmwxgzwRjTyxhzM7AF60v2lFJKKZ/wpmeEiFwG3Ab0BnYCX/mxTUoppQogqIfpRKQ+0AcrCR0EpgNijPHq216VUkoVkmBORsBvwHKgqzFmB4CIDC2UVimllCpW8rpmdDPW10UsFZHJInI9QXEHJKWUCi5B8DGj3JORMeZrY0xv4BJgGTAUiBaR/4nITYXUPqWUUvkQEZ8tdvFmNt1xY8ynxpguQHXgV+AJfzdMKaVU8SHGmPxL2aPINkwppXzAZ92QibM3++z1clBcY1u6R15N7bbLSWeG3U2wRXhoSLGMPTw0hCMn0u1uhi0qlipBvwv62t2MQjf13484cbr4PdcBSoX57pOqwTC1Owg+t6uUUirQaTJSSqlAV8jT6USko4hsF5EdIpLrHAIRaS4iThHpld8+i/QwnVJKqfwV5iid65sb3gFuBBKBtSIyxxizNYdyL2LdbDtf2jNSSilVEC2AHcaYv4wxp4BpQFwO5R4AZgIp3uxUk5FSSgU6Hw7TichAEVnntgzMcrRqwB639UTXY27NkWpAD2CCtyHoMJ1SSgU4CfHdOJ0xZhIwKa/D5VQty/rrwOPGGKe3M/00GSmllCqIRKCG23p1YF+WMs2Aaa5EVBnoJCKnjTGzctupJiOllApwhfwxo7VAPRGpDezF+naH290LGGNqn22bfAB8k1ciAk1GSikV+AoxGxljTovIEKxZcqHAVGPMFhEZ7Nru9XUid5qMlFJKFYgxZh4wL8tjOSYhY8xd3uxTk5FSSgW4YLgdkCYjpZQKdIGfi/RzRkoppeynPSOllApwvvyckV00GSmlVIAL/FSkw3RKKaWKAO0ZKaVUgNPZdEoppWwXBLlIh+mUUkrZT3tGSikV4IKhZ6TJSCmlApwEwXw6HaZTSillO+0ZKaVUgNNhOqWUUrYLhmSkw3RKKaVsVyyS0crly+nWKZYuHTrw3uTJ2bYbY3hh7Fi6dOhAr+5xbNu6Jd+63y1YQI+uXbisUUO2bN5cKHEUlD/iPnrkCIP696Nrxw4M6t+PY0ePFkosBbVq5QpuievCzV1j+XDqlGzbjTG8+uI4bu4ayx239OC3bVs9tjudTv6vdy+GPXBftrqffPg+V13WmCOHD/ut/eeq8U2XMm7Tizy/9WU6PdIl2/bSFUszZMaDjFw3hqdXPEe1htU8tkuI8Nya0Tz09bDMxwZ/cj8jfhrNiJ9G89L2Vxnx02i/x3EuVi5fTlznWLp27MDUXJ7vL44bS9eOHbilR/bne051Hxs+lFt79uDWnj2IvfF6bu3Zo1BiKSgR8dliF78kIxFJE5FjriXNbf0fETntj2Pmxul0Mm7MaN6dOImv585lwbxv+XPHDo8yK+Lj2Z2QwNwFC3h25EjGjByVb9269eox/s23uLJZs8IMx2v+invqlMm0aNmKuQsW0qJlK96bkv2P3m5Op5OXnx/D6+/8j2lfzeG7BfP4688/Pcr8uGI5e3bv5ss583jimRG8NNbzBXb6Z59Qq/ZF2fadnLSfn1avIqZKFb/GcC4kRLjzjb6M7/YKTzd9gqt6t6TqJVU9ynR+vBu7N+zmuWZPM6X/JG577U6P7Tc+0IH9v+3zeGzCne8wosUzjGjxDOtnrWP9rHV+j6WgnE4nz48dzTsTJvHVnFye78ut5/uc+Qt4ZsRIxo4alW/dl14dz4yvvmbGV19zw403cf0NNxR6bN4QHy528UsyMsaUM8aUdy3lgKrAWCAJeMMfx8zN5k0bqVGzJtVr1KBEyZJ0jO3EsiVLPMosXbKErnFxiAhNml5GWtoxUlNT8qx7UZ061KpdO6dDFgn+invpkiV06x4HQLfucSxdvLjQY8vP1s2bqF6jJtWq16BEiRLc2CGW+GWesccvW0psl26ICJc2aUpaWhoHUlMBSE5OYuXyeOJ63pxt3+NfeYkhDw8rklNpL2peh5Q/U0jdmYoz3cmaGau5rOsVHmWqNqjKtqVWLzBp+34qX1iZ8lHlAahUrRJNYpsS//6yXI/R/OYWrJmx2m8xnKvNmzZSo8bZ52yHTp1YttTznC9bsoQu3XJ5vudT1xjDdwsX0LFz58IMy2vaM8qHiFQUkRHABqAc0NwYM9yfx8wqJTmFmJiYzPWomGiSU5I9y6QkE+1WJjo6hpTkFK/qFlX+ivvQwYM4HFEAOBxRHDp0yJ9hnJOUlBSPuKKio0lNSfEok5oldquMFeP4l1+0Ek6WP8z4ZUtxOKKof/Elfmz9uatYtRKH9hzMXD+89xCVqlXyKLNn426u6G715ms3u4jImpWpVC0CgNteuYMvnpyOyTA57r/+tRdzLOUYKTuK3t9ASnIKMVXcn8vRpCRnf77H5PZ8z6fuz+vXERkZyYUX1vJPAMpvw3SVReR54GfgNHC5MeZpY8zBfOoNFJF1IrJu0qRJPmmLMdn/sLK9q82pjIh3dYuo4ho3kGtcnkVyeMEVYUX8MiIqRdCgYSOPTSdPnOCDKZMYdN8QnzbVl3J6U5s1zHkvf0OZiqUZ8dNorr/vRnb/mkDGaSdNO13GsdQ0En7Zlev+r+rdkjUzVvm20T5iOLdzLiJe1V0w71s6diqavSKwzr2vFrv4a2p3ApAKvA/8A/R3P7nGmNdyqmSMmQScyULmpDPjvBsSHRNNUlJS5npKUjJRUVEeZaKiY0h2K5OcnIQjykF6+ql86xZV/oo7IjKS1NQUHI4oUlNTiIiI8HMkBRcVHe0RV0pyMpUdjixlYrKVcTiiWPL9d8T/sIwfVyzn31P/cvz4cZ777+P839392bd3L3feag3dpaQk0/e2W3j/k2lEVq5cOIHl4/Dew0TUiMxcr1QtgiP7PCdZnEw7ydSBZyd0vLT9VVJ3pdLi1pZc1vlymnRoQonwEoSXL8WA9wcx+e6JAISEhnBFXDNGtXq2cIIpoOjoaJL2uz+Xk3Fkeb5HR8d4PK89nu951D19+jSLv/+ez2d86ccIzk8AvVXMlb+G6V7GSkRgDc+5L2X9dMwcNWp8KbsTEkhMTCT91CkWzJ9Hm3btPMq0bd+OubNnY4xh44ZfKVuuHA5HlFd1iyp/xd22XXvmzJoNwJxZs2nXvn2hx5afBo0as2f3bvbtTSQ9PZ1FC+fTuo1n7Ne1acv8b+ZgjGHTxg2ULVuWyg4H9z84lG++W8ys+d8x5oWXada8BSPHvUjdevVZsDSeWfO/Y9b874iKiuajz78oMokIYOe6v4iuG03lWpUJLRHKVbe25NdvfvEoU6pCaUJLhALQul9bfl+xnZNpJ5n5zBc8UudhHrt4OBP+711+W7YtMxEBNLy+EUnb93N4b9GbQQiu5/vuBPa6nrML52V/vrdp145v5rg938u6Pd/zqLtm1Spq167tMayrfM8vPSNjzIjctonIw/44Zm7CwsJ48qmnuXfAPWRkZNC9R0/q1qvHjGnTALi1Tx+ua92GFfHxdOnYgfDwcEaNHZdnXYDF3y/ihbFjOXzoEEPuHczFl1zChMnZpxDbxV9x9xtwD48OHcasmV8SU6Uqr4wfb1uMuQkLC+ORJ/7Lg/cOIiPDSde4HlxUty5ffTEdgJ639Oaa61rz44rl3Nw1lvDwUjwzsmhOVy6IDGcGnzz8EcO+eYyQUGHFB/Hs27aXtgOsF9Zlk5dS9ZKq3DN1IBnODPZt28f7g7x7zra4pegO0YF1zp946mnuHWg9Z+N69KRu3Xp8Md16vt/S++zzvWus9XwfOWZcnnXPWDB/XpEeooPg+D4jyXHs3J8HFNltjKnpRVGfDNMFovDQEIpj7OGhIRw5kW53M2xRsVQJ+l3Q1+5mFLqp/37EidPF77kOUCosxGcZZOaqXT57Ib+5VS1bMpsdH3oN/BSulFLKp+y4N13hdsWUUirIBcMwnV+SkYikkXPSEaCUP46plFLFVeCnIv9NYCjnj/0qpZQKTvoVEkopFeCCYJROk5FSSgW6YLhmVCy+QkIppVTRpj0jpZQKcIHfL9JkpJRSAS8IRul0mE4ppZT9tGeklFIBLhgmMGgyUkqpABcEuUiH6ZRSStlPe0ZKKRXgAuqbmHOhyUgppQKcDtMppZRSPqA9I6WUCnDB0DPSZKSUUgEuJAiuGekwnVJKKdtpz0gppQKcDtMppZSyXTAkIx2mU0opZTvtGSmlVIDTe9MppZSyXeCnIh2mU0opVQRoz0gppQKcDtP5WXho8e24FdfYK5YqYXcTbDP134/sboItSoUVz+e6LwVBLirayeikM8PuJtgiPDSkWMZeXOMGK/YTp4tf7KXCQugmXexuhi3mmG/sbkKRUqSTkVJKqfxpz0gppZTtguH7jHSwVimllO20Z6SUUgFOh+mUUkrZLhimduswnVJKKdtpz0gppQJcEHSMNBkppVSg02E6pZRSyge0Z6SUUgEu8PtFmoyUUirgBcEonQ7TKaWUsp/2jJRSKsAFwwQGTUZKKRXggiAX6TCdUkop+2kyUkqpACc+/OfV8UQ6ish2EdkhIk/ksP0OEdnoWn4Ukab57VOH6ZRSKsAV5jCdiIQC7wA3AonAWhGZY4zZ6lZsJ9DGGHNYRGKBScBVee1Xe0ZKKaUKogWwwxjzlzHmFDANiHMvYIz50Rhz2LW6Gqie3041GSmlVIATEV8uA0VkndsyMMvhqgF73NYTXY/lpj8wP78YdJhOKaUCnC+H6Ywxk7CG1XI9XE7Vciwo0g4rGV2b33E1GSmlVIAr5KndiUANt/XqwL6shUSkCTAFiDXGHMxvpzpMp5RSqiDWAvVEpLaIlAT6AHPcC4hITeAr4P+MMb97s1PtGSmlVIDzdkq2LxhjTovIEGAhEApMNcZsEZHBru0TgGeBSOBd190hThtjmuW1X01GSikV4Ar7DgzGmHnAvCyPTXD7+R7gnoLsU4fplFJK2a5YJKOVy5fTrVMsXTp04L3Jk7NtN8bwwtixdOnQgV7d49i2dUu+dY8eOcKg/v3o2rEDg/r349jRo4USS0EU17jBP7F/t2ABPbp24bJGDdmyeXOhxFFQK5cvJ65zLF07dmBqLnG/OG4sXTt24JYe2ePOq+6H70/lskYNOHz4cLZtRcEVHa7g3d8mMPGPSdz8eK9s28tULMOTXz3Fmxve4pU1r1Gz0YWZ27o+2I23Nr3D25vfodtD3bLV7T68B3PMN5SLLO/XGM6VL6d228UvyUhE+ua1+OOYuXE6nYwbM5p3J07i67lzWTDvW/7cscOjzIr4eHYnJDB3wQKeHTmSMSNH5Vt36pTJtGjZirkLFtKiZSvem5L9j9dOxTVu8F/sdevVY/ybb3FlszyHvm3jdDp5fuxo3pkwia/m5BL3civuOfMX8MyIkYwdNcqrukn797P6xx+pUqVKocbkrZCQEAa9cy8jY5/j/ob30fq2NtRoUMOjzC3/vZWdv/7Fg00fYHzf1xjwhvXxmZqNLuSmAR0Y3mIYDzZ9gGZdWlClbtXMepWrV+ayGy8nJSGlUGMqCBHfLXbxV8+oeQ5LC2A0MNVPx8zR5k0bqVGzJtVr1KBEyZJ0jO3EsiVLPMosXbKErnFxiAhNml5GWtoxUlNT8qy7dMkSunW3PnTcrXscSxcvLsyw8lVc4wb/xX5RnTrUql3bjpC8snnTRmrUONv2Dp06sWypZ9zLliyhS7dc4s6j7isvvsDDwx8psreHrteiPvt37Cd5ZzKn00+zfFo8V8W19ChTo2FNNizeAMDe7YlE1YqiYlRFajSozvbVv3HqxL9kODPY8sNmWvVolVmv//gBfPDY+xiT40dplI/4JRkZYx44swAPAmuANli3hbjCH8fMTUpyCjExMZnrUTHRJKcke5ZJSSbarUx0dAwpySl51j108CAORxQADkcUhw4d8mcYBVZc4wb/xV7UpSSnEFPFPaZoUpKzxx2TW9y51F22ZAmO6GguvuQSP0dw7iKrRXJgT2rm+oHEA0RWi/Qos2vDTlr1vBqAes3rE3VhFJHVI0nYnECj1o0pF1GOkqUu4MpOzahcozIALbq24ODeg+zauLPwgjkHhX2jVH/w22w6EQkD7gKGYyWjXsaY7f46Xm5yejeT7ReeUxkR7+oWUcU1bii+sZscPgSf9RpAjvGJ5Fr3xIkTTJk0kf9NnuK7hvpBTh22rLF++cIXDHhjIK//8iYJm3bx1y9/4jydQeJviXz14peMWjSak3+fZOeGnThPOylZ6gJueao3z930TCFFce6KaIe1QPySjETkfuAhYDHQ0RiT4GW9gcBAgIkTJ9K3f4FmBuYoOiaapKSkzPWUpGSioqI8ykRFx5DsViY5OQlHlIP09FO51o2IjCQ1NQWHI4rU1BQiIiLOu62+VFzjBv/FXtRFR0eTtN89pmQcWdoeHR3jEZ9H3DnUTdyzh717E7m1Z3cAUpKTua3XzXwybTqVHQ7/BlQABxIPUrnG2fZUrl6ZQ/s8e+0n0k7wZr83Mtcn73yP5J1WzIumLmLR1EUA/N/YvhxIPECVOjFE147mjQ1vZe7z9Z9fZ3iLYRxJPuLniIoff10zegsoj3U/orlu32uxSUQ25lbJGDPJGNPMGNNs4MCs9+Y7N40aX8ruhAQSExNJP3WKBfPn0aZdO48ybdu3Y+7s2Rhj2LjhV8qWK4fDEZVn3bbt2jNn1mwA5syaTbv27X3SXl8prnGD/2Iv6ho1vpTduxPY62r7wnnZ296mXTu+meMWd1m3uHOoW69+fZYuX8n8RYuZv2gxUdHRfP7lzCKViAD+WPs7VetVJbpWNGElwriuT2vWzFnjUaZMhTKElbDef990Twe2xG/hRNoJACo4KgBQuYaDVj1bEf/5DyRsTqBv9J0MqN2fAbX7cyDxAA9f8XCRTEQhIj5b7OKvYboic5U3LCyMJ596mnsH3ENGRgbde/Skbr16zJg2DYBb+/ThutZtWBEfT5eOHQgPD2fU2HF51gXoN+AeHh06jFkzvySmSlVeGT/ethhzUlzjBv/Fvvj7RbwwdiyHDx1iyL2DufiSS5hQhIavwsLCeOKpp7l3oNX2uB49qVu3Hl9Mt+K+pffZuLvGWnGPHDMuz7qBIsOZwcQhExixcBQhoSF8P3URe7bupuOgWAAWTJxP9QY1GPrRMDKcTvZs3cOb/c/2kp6Y+V/KRZbDme5kwv0TOH7kuF2hnJNgGKaTwpwh4vpSpj7GmE+9KG5OOjP83aQiKTw0hOIYe3GNG6zYT5wufrGXCguhm3Sxuxm2mGO+8VkK+W3fUZ+9kF9StYItqc1fnzMqLyJPisjbInKTWB4A/gJu9ccxlVKquAqGzxn5a5juY+AwsArr/kSPAiWBOGPMr346plJKFUuBMuMzL/5KRhcZYy4FEJEpwAGgpjEmzU/HU0opFcD8lYzSz/xgjHGKyE5NREop5R/BMIHBX8moqYgcc/0sQCnXugDGGFM07zaolFIByM4bnPqKX5KRMSbUH/tVSikVnPTL9ZRSKsAFQcdIk5FSSgW6YBimKxZfrqeUUqpo056RUkoFuMDvF2kyUkqpgKfDdEoppZQPaM9IKaUCXBB0jDQZKaVUoAuCXKTDdEoppeynPSOllAp0QTBOp8lIKaUCXOCnIh2mU0opVQRoz0gppQJcEIzSaTJSSqlAFwS5SIfplFJK2U97RkopFeiCYJxOk5FSSgW4wE9FOkynlFKqCNCekVJKBbggGKXTZKSUUoEv8LORDtMppZSynRhj7G5DkSMiA40xk+xuhx2Ka+zFNW4ovrEHU9xJx0767IU8pny4Ld0s7RnlbKDdDbBRcY29uMYNxTf2oIlbfLjYRZORUkop2+kEBqWUCnA6my54BcU48jkqrrEX17ih+MYeRHEHfjbSCQxKKRXgUtL+9dkLeVS5C2zJbNozUkqpAKfDdEoppWwXBLlIZ9O5ExGniPwqIptF5AsRKW13m/xJRP7O4bERIrLX7ffQzY62+ZqIjBeRh93WF4rIFLf1V0VkmIgYEXnA7fG3ReSuwm2tf+Rxvv8Rkai8ygWyLH/Xc0WkouvxWsF8vgONJiNPJ4wxlxljGgOngMF2N8gm440xlwG3AFNFJBieJz8CVwO44qkMNHLbfjWwEkgBHhKRkoXeQvscAIbb3Qg/cv+7PgTc77YtOM53EHzQKBheZPxlOVDX7kbYyRizDTiN9cId6FbiSkZYSWgzkCYilUTkAqABcBhIBRYD/7GllfaYCvQWkQi7G1IIVgHV3NaD4nyLD//ZRZNRDkQkDIgFNtndFjuJyFVABtYfbEAzxuwDTotITayktApYA7QCmgEbsXrDAC8Aw0Uk1I622uBvrIT0kN0N8SfX+bwemJNlU3E730WSTmDwVEpEfnX9vBx4z8a22GmoiNwJpAG9TfDM/z/TO7oaeA3rHfLVwFGsYTwAjDE7ReQn4HY7GmmTN4FfReRVuxviB2f+rmsB64FF7huD4XzrbLrgc8J1raS4G2+MecXuRvjBmetGl2IN0+3BulZyDKtn4G4c8CUQX5gNtIsx5oiIfAbcZ3db/OCEMeYyEakAfIN1zejNLGUC+nwHQS7SYTpVrKwEugCHjDFOY8whoCLWUN0q94LGmN+Ara7yxcVrwCCC9E2qMeYo8CDwiIiUyLItsM+3iO8Wm2gyKt5Ki0ii2zLM7gb52SasyRirszx21BhzIIfyY4HqhdGwQpLn+Xb9Dr4GLrCnef5njPkF2AD0yWFzsJ3vgKK3A1JKqQB35ES6z17IK5YqobcDUkopVXDBMIFBh+mUUkrZTntGSikV4IKgY6TJSCmlAl4QjNPpMJ1SSinbaTJStvDlHdJF5AMR6eX6eYqINMyjbFsRuTq37XnU2yUi2e7Rl9vjWcoU6C7YrjtpP1LQNqriKwjuk6rJSNkmzzukn+t9wowx9xhjtuZRpC1nb5iqVFAIgs+8ajJSRcJyoK6r17LUdVuaTSISKiIvi8haEdkoIoMAxPK2iGwVkW8B9+/iWSYizVw/dxSRn0Vkg4gsFpFaWElvqKtXdp2IOERkpusYa0XkGlfdSBH5TkR+EZGJePGmUURmich6EdkiIgOzbHvV1ZbFIuJwPVZHRBa46iwXkUt88ttUKgDpBAZlK7c7pC9wPdQCaOy6eeVArLsjNHd9zcNKEfkOuBy4GOsec9FYt3GZmmW/DmAy0Nq1rwhjzCERmQD8febee67EN94Ys8J1R++FWF8n8RywwhgzSkQ6Ax7JJRf9XMcoBawVkZnGmINAGeBnY8xwEXnWte8hwCRgsDHmD9cd0t8F2p/Dr1EVe4E/gUGTkbJLTndIvxr4yRiz0/X4TUCTM9eDgApAPaA18LkxxgnsE5ElOey/JRB/Zl+u+9Dl5AagoZwdnygvIuVcx+jpqvutiBz2IqYHRaSH6+carrYexPoajumuxz8BvhKRsq54v3A7dtDehkf5VxBMptNkpGyT7Q7prhfl4+4PAQ8YYxZmKdcJyO/2J+JFGbCGqlsZY07k0Bavb7EiIm2xElsrY8w/IrIMCM+luHEd94jeJV4pi14zUkXZQuDeM3dYFpH6IlIG6zb/fVzXlKoA7XKouwpoIyK1XXXPfItpGlDOrdx3WENmuMpd5voxHrjD9VgsUCmftlYADrsS0SVYPbMzQoAzvbvbsYb/jgE7ReQW1zFERJrmcwylcqSz6ZTyrylY14N+FpHNwESs3vzXwB9Yd9z+H/BD1orGmFSs6zxficgGzg6TzQV6nJnAgPWVAs1cEyS2cnZW30igtYj8jDVcuDufti4AwkRkIzAazzuDHwcaich6rGtCo1yP3wH0d7VvCxDnxe9EqWyCYTad3rVbKaUC3InTTp+9kJcKC7UlJWnPSCmlAl7hDtS5PjaxXUR2iMgTOWwXEXnTtX2jiFyR3z51AoNSSgW4whxec30g/R3gRiAR62MMc7J82DwWazZpPeAqrOH0q/Lar/aMlFJKFUQLYIcx5i9jzClgGtmvd8YBHxnLaqCia7JRrrRnpJRSAS48NMRnfSPXh83dP+Q9yRgzyW29GrDHbT2R7L2enMpUA/bndlxNRkoppTK5Es+kPIrklPiyTqDwpowHHaZTSilVEIlYdxg5ozqw7xzKeNBkpJRSqiDWAvVEpLaIlAT6AHOylJkD9HXNqmuJdY/JXIfoQIfplFJKFYAx5rSIDMG6Q0ooMNUYs0VEBru2TwDmAZ2AHcA/wN357Vc/9KqUUsp2OkynlFLKdpqMlFJK2U6TkVJKKdtpMlJKKWU7TUZKKaVsp8lIKaWU7TQZKaWUst3/A3rQ9udYwNdbAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 504x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "gnn_sage = GNN4L_Sage(data_with_nedbit).to(device)\n",
    "pred = train(gnn_sage, data_with_nedbit.to(device), 40000, weight_decay=0.0005, cm_title='SAGE4L_multiclass_16HC_v2_max', classes=['P', 'LP', 'WN', 'LN', 'RN'], num_layers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7 Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "889faca68fb24eb2981ae29c270fdf78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40001 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch: 0000] train loss: 67.5791, train acc: 0.1681, val loss: 35.4155, val acc: 0.0530  (best train acc: 0.1681, best val acc: 0.0530, best train loss: 67.5791  @ epoch 0 )\n",
      "[Epoch: 0020] train loss: 5.2124, train acc: 0.2648, val loss: 2.9046, val acc: 0.2405  (best train acc: 0.2805, best val acc: 0.2610, best train loss: 5.2124  @ epoch 20 )\n",
      "[Epoch: 0040] train loss: 1.8515, train acc: 0.2929, val loss: 1.5578, val acc: 0.4337  (best train acc: 0.3122, best val acc: 0.4422, best train loss: 1.8515  @ epoch 40 )\n",
      "[Epoch: 0060] train loss: 1.4494, train acc: 0.4130, val loss: 1.2098, val acc: 0.5373  (best train acc: 0.4130, best val acc: 0.5373, best train loss: 1.4428  @ epoch 58 )\n",
      "[Epoch: 0080] train loss: 1.2248, train acc: 0.4666, val loss: 1.0511, val acc: 0.6135  (best train acc: 0.4666, best val acc: 0.6135, best train loss: 1.2248  @ epoch 80 )\n",
      "[Epoch: 0100] train loss: 1.1402, train acc: 0.5116, val loss: 0.9745, val acc: 0.6651  (best train acc: 0.5116, best val acc: 0.6651, best train loss: 1.1355  @ epoch 99 )\n",
      "[Epoch: 0120] train loss: 1.0327, train acc: 0.5668, val loss: 0.8362, val acc: 0.7258  (best train acc: 0.5668, best val acc: 0.7258, best train loss: 1.0327  @ epoch 120 )\n",
      "[Epoch: 0140] train loss: 0.9476, train acc: 0.6285, val loss: 0.7105, val acc: 0.8196  (best train acc: 0.6304, best val acc: 0.8196, best train loss: 0.9423  @ epoch 139 )\n",
      "[Epoch: 0160] train loss: 0.8919, train acc: 0.6471, val loss: 0.6128, val acc: 0.8590  (best train acc: 0.6598, best val acc: 0.8597, best train loss: 0.8855  @ epoch 158 )\n",
      "[Epoch: 0180] train loss: 0.8487, train acc: 0.6704, val loss: 0.5800, val acc: 0.8587  (best train acc: 0.6731, best val acc: 0.8742, best train loss: 0.8417  @ epoch 179 )\n",
      "[Epoch: 0200] train loss: 0.8141, train acc: 0.6847, val loss: 0.5312, val acc: 0.8691  (best train acc: 0.6847, best val acc: 0.8772, best train loss: 0.8112  @ epoch 198 )\n",
      "[Epoch: 0220] train loss: 0.7555, train acc: 0.7126, val loss: 0.5065, val acc: 0.8793  (best train acc: 0.7126, best val acc: 0.8793, best train loss: 0.7555  @ epoch 220 )\n",
      "[Epoch: 0240] train loss: 0.7200, train acc: 0.7325, val loss: 0.4700, val acc: 0.8769  (best train acc: 0.7325, best val acc: 0.8806, best train loss: 0.7200  @ epoch 240 )\n",
      "[Epoch: 0260] train loss: 0.7180, train acc: 0.7377, val loss: 0.4564, val acc: 0.8867  (best train acc: 0.7503, best val acc: 0.8884, best train loss: 0.6984  @ epoch 253 )\n",
      "[Epoch: 0280] train loss: 0.6903, train acc: 0.7499, val loss: 0.4391, val acc: 0.8914  (best train acc: 0.7532, best val acc: 0.8934, best train loss: 0.6863  @ epoch 279 )\n",
      "[Epoch: 0300] train loss: 0.6943, train acc: 0.7519, val loss: 0.4348, val acc: 0.8924  (best train acc: 0.7626, best val acc: 0.8934, best train loss: 0.6694  @ epoch 288 )\n",
      "[Epoch: 0320] train loss: 0.6902, train acc: 0.7441, val loss: 0.4234, val acc: 0.8917  (best train acc: 0.7676, best val acc: 0.8944, best train loss: 0.6669  @ epoch 301 )\n",
      "[Epoch: 0340] train loss: 0.6591, train acc: 0.7705, val loss: 0.4257, val acc: 0.8907  (best train acc: 0.7705, best val acc: 0.8954, best train loss: 0.6580  @ epoch 338 )\n",
      "[Epoch: 0360] train loss: 0.6643, train acc: 0.7596, val loss: 0.4118, val acc: 0.8927  (best train acc: 0.7705, best val acc: 0.8954, best train loss: 0.6456  @ epoch 357 )\n",
      "[Epoch: 0380] train loss: 0.6203, train acc: 0.7828, val loss: 0.3958, val acc: 0.8941  (best train acc: 0.7833, best val acc: 0.8975, best train loss: 0.6111  @ epoch 376 )\n",
      "[Epoch: 0400] train loss: 0.5968, train acc: 0.7899, val loss: 0.3592, val acc: 0.8988  (best train acc: 0.7899, best val acc: 0.8995, best train loss: 0.5928  @ epoch 395 )\n",
      "[Epoch: 0420] train loss: 0.5941, train acc: 0.7895, val loss: 0.3549, val acc: 0.8958  (best train acc: 0.7913, best val acc: 0.9005, best train loss: 0.5826  @ epoch 406 )\n",
      "[Epoch: 0440] train loss: 0.5770, train acc: 0.7990, val loss: 0.3472, val acc: 0.9022  (best train acc: 0.8059, best val acc: 0.9022, best train loss: 0.5639  @ epoch 438 )\n",
      "[Epoch: 0460] train loss: 0.5429, train acc: 0.8110, val loss: 0.3437, val acc: 0.9012  (best train acc: 0.8170, best val acc: 0.9022, best train loss: 0.5217  @ epoch 458 )\n",
      "[Epoch: 0480] train loss: 0.5318, train acc: 0.8163, val loss: 0.3372, val acc: 0.8988  (best train acc: 0.8198, best val acc: 0.9029, best train loss: 0.5217  @ epoch 458 )\n",
      "[Epoch: 0500] train loss: 0.5166, train acc: 0.8269, val loss: 0.3355, val acc: 0.8998  (best train acc: 0.8269, best val acc: 0.9029, best train loss: 0.5166  @ epoch 500 )\n",
      "[Epoch: 0520] train loss: 0.5419, train acc: 0.8133, val loss: 0.3454, val acc: 0.8850  (best train acc: 0.8294, best val acc: 0.9029, best train loss: 0.5110  @ epoch 516 )\n",
      "[Epoch: 0540] train loss: 0.5126, train acc: 0.8272, val loss: 0.3330, val acc: 0.8927  (best train acc: 0.8297, best val acc: 0.9029, best train loss: 0.5021  @ epoch 536 )\n",
      "[Epoch: 0560] train loss: 0.5152, train acc: 0.8191, val loss: 0.3259, val acc: 0.8995  (best train acc: 0.8342, best val acc: 0.9029, best train loss: 0.5005  @ epoch 558 )\n",
      "[Epoch: 0580] train loss: 0.5106, train acc: 0.8256, val loss: 0.3286, val acc: 0.8938  (best train acc: 0.8344, best val acc: 0.9029, best train loss: 0.4941  @ epoch 577 )\n",
      "[Epoch: 0600] train loss: 0.5007, train acc: 0.8306, val loss: 0.3200, val acc: 0.8998  (best train acc: 0.8344, best val acc: 0.9039, best train loss: 0.4941  @ epoch 577 )\n",
      "[Epoch: 0620] train loss: 0.5015, train acc: 0.8319, val loss: 0.3207, val acc: 0.9015  (best train acc: 0.8350, best val acc: 0.9056, best train loss: 0.4864  @ epoch 606 )\n",
      "[Epoch: 0640] train loss: 0.5024, train acc: 0.8295, val loss: 0.3472, val acc: 0.8766  (best train acc: 0.8376, best val acc: 0.9056, best train loss: 0.4792  @ epoch 632 )\n",
      "[Epoch: 0660] train loss: 0.4907, train acc: 0.8376, val loss: 0.3158, val acc: 0.9008  (best train acc: 0.8376, best val acc: 0.9056, best train loss: 0.4792  @ epoch 632 )\n",
      "[Epoch: 0680] train loss: 0.4942, train acc: 0.8250, val loss: 0.3144, val acc: 0.9019  (best train acc: 0.8377, best val acc: 0.9059, best train loss: 0.4735  @ epoch 669 )\n",
      "[Epoch: 0700] train loss: 0.4746, train acc: 0.8346, val loss: 0.3191, val acc: 0.8981  (best train acc: 0.8424, best val acc: 0.9059, best train loss: 0.4697  @ epoch 681 )\n",
      "[Epoch: 0720] train loss: 0.4939, train acc: 0.8286, val loss: 0.3231, val acc: 0.8931  (best train acc: 0.8424, best val acc: 0.9059, best train loss: 0.4697  @ epoch 681 )\n",
      "[Epoch: 0740] train loss: 0.4823, train acc: 0.8349, val loss: 0.3135, val acc: 0.9008  (best train acc: 0.8464, best val acc: 0.9076, best train loss: 0.4628  @ epoch 736 )\n",
      "[Epoch: 0760] train loss: 0.4650, train acc: 0.8397, val loss: 0.3060, val acc: 0.9093  (best train acc: 0.8464, best val acc: 0.9093, best train loss: 0.4593  @ epoch 752 )\n",
      "[Epoch: 0780] train loss: 0.4587, train acc: 0.8460, val loss: 0.3082, val acc: 0.9025  (best train acc: 0.8475, best val acc: 0.9093, best train loss: 0.4561  @ epoch 764 )\n",
      "[Epoch: 0800] train loss: 0.4508, train acc: 0.8524, val loss: 0.3061, val acc: 0.9079  (best train acc: 0.8542, best val acc: 0.9093, best train loss: 0.4371  @ epoch 792 )\n",
      "[Epoch: 0820] train loss: 0.4456, train acc: 0.8525, val loss: 0.3069, val acc: 0.9066  (best train acc: 0.8580, best val acc: 0.9106, best train loss: 0.4350  @ epoch 818 )\n",
      "[Epoch: 0840] train loss: 0.4577, train acc: 0.8446, val loss: 0.3705, val acc: 0.8523  (best train acc: 0.8580, best val acc: 0.9106, best train loss: 0.4313  @ epoch 838 )\n",
      "[Epoch: 0860] train loss: 0.4558, train acc: 0.8480, val loss: 0.3220, val acc: 0.8897  (best train acc: 0.8580, best val acc: 0.9106, best train loss: 0.4313  @ epoch 838 )\n",
      "[Epoch: 0880] train loss: 0.4590, train acc: 0.8356, val loss: 0.3499, val acc: 0.8627  (best train acc: 0.8587, best val acc: 0.9106, best train loss: 0.4301  @ epoch 878 )\n",
      "[Epoch: 0900] train loss: 0.4333, train acc: 0.8560, val loss: 0.3040, val acc: 0.9008  (best train acc: 0.8587, best val acc: 0.9106, best train loss: 0.4301  @ epoch 878 )\n",
      "[Epoch: 0920] train loss: 0.4327, train acc: 0.8570, val loss: 0.3026, val acc: 0.9029  (best train acc: 0.8658, best val acc: 0.9106, best train loss: 0.4109  @ epoch 919 )\n",
      "[Epoch: 0940] train loss: 0.4236, train acc: 0.8608, val loss: 0.2988, val acc: 0.9066  (best train acc: 0.8658, best val acc: 0.9106, best train loss: 0.4066  @ epoch 931 )\n",
      "[Epoch: 0960] train loss: 0.4198, train acc: 0.8627, val loss: 0.2935, val acc: 0.9093  (best train acc: 0.8699, best val acc: 0.9106, best train loss: 0.4066  @ epoch 931 )\n",
      "[Epoch: 0980] train loss: 0.4382, train acc: 0.8469, val loss: 0.3162, val acc: 0.8894  (best train acc: 0.8699, best val acc: 0.9106, best train loss: 0.3932  @ epoch 963 )\n",
      "[Epoch: 1000] train loss: 0.4033, train acc: 0.8645, val loss: 0.2968, val acc: 0.9035  (best train acc: 0.8706, best val acc: 0.9106, best train loss: 0.3932  @ epoch 963 )\n",
      "[Epoch: 1020] train loss: 0.4020, train acc: 0.8683, val loss: 0.2907, val acc: 0.9093  (best train acc: 0.8712, best val acc: 0.9106, best train loss: 0.3932  @ epoch 963 )\n",
      "[Epoch: 1040] train loss: 0.3998, train acc: 0.8684, val loss: 0.2933, val acc: 0.9066  (best train acc: 0.8735, best val acc: 0.9116, best train loss: 0.3866  @ epoch 1038 )\n",
      "[Epoch: 1060] train loss: 0.4408, train acc: 0.8484, val loss: 0.3063, val acc: 0.8998  (best train acc: 0.8738, best val acc: 0.9126, best train loss: 0.3866  @ epoch 1038 )\n",
      "[Epoch: 1080] train loss: 0.4063, train acc: 0.8605, val loss: 0.3236, val acc: 0.8860  (best train acc: 0.8738, best val acc: 0.9126, best train loss: 0.3866  @ epoch 1038 )\n",
      "[Epoch: 1100] train loss: 0.3974, train acc: 0.8696, val loss: 0.2917, val acc: 0.9116  (best train acc: 0.8738, best val acc: 0.9126, best train loss: 0.3866  @ epoch 1038 )\n",
      "[Epoch: 1120] train loss: 0.3955, train acc: 0.8678, val loss: 0.2910, val acc: 0.9086  (best train acc: 0.8754, best val acc: 0.9137, best train loss: 0.3846  @ epoch 1105 )\n",
      "[Epoch: 1140] train loss: 0.3927, train acc: 0.8666, val loss: 0.2864, val acc: 0.9123  (best train acc: 0.8754, best val acc: 0.9137, best train loss: 0.3832  @ epoch 1129 )\n",
      "[Epoch: 1160] train loss: 0.4008, train acc: 0.8685, val loss: 0.2869, val acc: 0.9133  (best train acc: 0.8777, best val acc: 0.9164, best train loss: 0.3832  @ epoch 1129 )\n",
      "[Epoch: 1180] train loss: 0.4167, train acc: 0.8603, val loss: 0.2905, val acc: 0.9120  (best train acc: 0.8777, best val acc: 0.9164, best train loss: 0.3811  @ epoch 1171 )\n",
      "[Epoch: 1200] train loss: 0.3891, train acc: 0.8687, val loss: 0.2856, val acc: 0.9113  (best train acc: 0.8777, best val acc: 0.9164, best train loss: 0.3811  @ epoch 1171 )\n",
      "[Epoch: 1220] train loss: 0.3847, train acc: 0.8717, val loss: 0.2821, val acc: 0.9147  (best train acc: 0.8777, best val acc: 0.9164, best train loss: 0.3811  @ epoch 1171 )\n",
      "[Epoch: 1240] train loss: 0.4032, train acc: 0.8697, val loss: 0.2849, val acc: 0.9137  (best train acc: 0.8780, best val acc: 0.9164, best train loss: 0.3811  @ epoch 1171 )\n",
      "[Epoch: 1260] train loss: 0.3831, train acc: 0.8758, val loss: 0.2781, val acc: 0.9143  (best train acc: 0.8792, best val acc: 0.9164, best train loss: 0.3772  @ epoch 1256 )\n",
      "[Epoch: 1280] train loss: 0.3757, train acc: 0.8738, val loss: 0.2834, val acc: 0.9099  (best train acc: 0.8792, best val acc: 0.9184, best train loss: 0.3725  @ epoch 1276 )\n",
      "[Epoch: 1300] train loss: 0.3918, train acc: 0.8673, val loss: 0.2861, val acc: 0.9103  (best train acc: 0.8792, best val acc: 0.9184, best train loss: 0.3725  @ epoch 1276 )\n",
      "[Epoch: 1320] train loss: 0.4010, train acc: 0.8678, val loss: 0.3523, val acc: 0.8695  (best train acc: 0.8792, best val acc: 0.9184, best train loss: 0.3725  @ epoch 1276 )\n",
      "[Epoch: 1340] train loss: 0.4121, train acc: 0.8599, val loss: 0.2919, val acc: 0.9147  (best train acc: 0.8792, best val acc: 0.9184, best train loss: 0.3725  @ epoch 1276 )\n",
      "[Epoch: 1360] train loss: 0.4066, train acc: 0.8624, val loss: 0.2780, val acc: 0.9116  (best train acc: 0.8792, best val acc: 0.9184, best train loss: 0.3725  @ epoch 1276 )\n",
      "[Epoch: 1380] train loss: 0.3932, train acc: 0.8645, val loss: 0.2861, val acc: 0.9042  (best train acc: 0.8792, best val acc: 0.9184, best train loss: 0.3725  @ epoch 1276 )\n",
      "[Epoch: 1400] train loss: 0.3818, train acc: 0.8702, val loss: 0.2636, val acc: 0.9180  (best train acc: 0.8792, best val acc: 0.9184, best train loss: 0.3697  @ epoch 1388 )\n",
      "[Epoch: 1420] train loss: 0.3702, train acc: 0.8743, val loss: 0.2731, val acc: 0.9137  (best train acc: 0.8804, best val acc: 0.9201, best train loss: 0.3620  @ epoch 1410 )\n",
      "[Epoch: 1440] train loss: 0.3713, train acc: 0.8759, val loss: 0.2675, val acc: 0.9207  (best train acc: 0.8804, best val acc: 0.9214, best train loss: 0.3620  @ epoch 1410 )\n",
      "[Epoch: 1460] train loss: 0.3928, train acc: 0.8593, val loss: 0.2660, val acc: 0.9177  (best train acc: 0.8804, best val acc: 0.9214, best train loss: 0.3620  @ epoch 1410 )\n",
      "[Epoch: 1480] train loss: 0.3804, train acc: 0.8754, val loss: 0.2675, val acc: 0.9174  (best train acc: 0.8804, best val acc: 0.9214, best train loss: 0.3620  @ epoch 1410 )\n",
      "[Epoch: 1500] train loss: 0.3707, train acc: 0.8788, val loss: 0.2610, val acc: 0.9201  (best train acc: 0.8808, best val acc: 0.9228, best train loss: 0.3604  @ epoch 1496 )\n",
      "[Epoch: 1520] train loss: 0.3770, train acc: 0.8707, val loss: 0.2720, val acc: 0.9113  (best train acc: 0.8808, best val acc: 0.9241, best train loss: 0.3604  @ epoch 1496 )\n",
      "[Epoch: 1540] train loss: 0.3883, train acc: 0.8676, val loss: 0.2680, val acc: 0.9110  (best train acc: 0.8809, best val acc: 0.9241, best train loss: 0.3573  @ epoch 1524 )\n",
      "[Epoch: 1560] train loss: 0.3640, train acc: 0.8766, val loss: 0.2816, val acc: 0.9019  (best train acc: 0.8809, best val acc: 0.9241, best train loss: 0.3573  @ epoch 1524 )\n",
      "[Epoch: 1580] train loss: 0.3773, train acc: 0.8719, val loss: 0.2619, val acc: 0.9204  (best train acc: 0.8809, best val acc: 0.9241, best train loss: 0.3573  @ epoch 1524 )\n",
      "[Epoch: 1600] train loss: 0.3676, train acc: 0.8767, val loss: 0.2827, val acc: 0.8992  (best train acc: 0.8822, best val acc: 0.9241, best train loss: 0.3555  @ epoch 1586 )\n",
      "[Epoch: 1620] train loss: 0.3788, train acc: 0.8781, val loss: 0.2577, val acc: 0.9194  (best train acc: 0.8822, best val acc: 0.9241, best train loss: 0.3477  @ epoch 1606 )\n",
      "[Epoch: 1640] train loss: 0.3645, train acc: 0.8824, val loss: 0.2583, val acc: 0.9143  (best train acc: 0.8854, best val acc: 0.9241, best train loss: 0.3477  @ epoch 1606 )\n",
      "[Epoch: 1660] train loss: 0.3626, train acc: 0.8788, val loss: 0.2616, val acc: 0.9137  (best train acc: 0.8854, best val acc: 0.9248, best train loss: 0.3464  @ epoch 1655 )\n",
      "[Epoch: 1680] train loss: 0.4080, train acc: 0.8574, val loss: 0.2727, val acc: 0.9022  (best train acc: 0.8854, best val acc: 0.9248, best train loss: 0.3464  @ epoch 1655 )\n",
      "[Epoch: 1700] train loss: 0.3631, train acc: 0.8801, val loss: 0.2570, val acc: 0.9164  (best train acc: 0.8854, best val acc: 0.9248, best train loss: 0.3464  @ epoch 1655 )\n",
      "[Epoch: 1720] train loss: 0.3621, train acc: 0.8801, val loss: 0.2516, val acc: 0.9245  (best train acc: 0.8854, best val acc: 0.9248, best train loss: 0.3464  @ epoch 1655 )\n",
      "[Epoch: 1740] train loss: 0.3607, train acc: 0.8850, val loss: 0.2442, val acc: 0.9268  (best train acc: 0.8882, best val acc: 0.9282, best train loss: 0.3430  @ epoch 1739 )\n",
      "[Epoch: 1760] train loss: 0.3511, train acc: 0.8840, val loss: 0.2452, val acc: 0.9224  (best train acc: 0.8884, best val acc: 0.9282, best train loss: 0.3428  @ epoch 1750 )\n",
      "[Epoch: 1780] train loss: 0.3534, train acc: 0.8841, val loss: 0.2467, val acc: 0.9251  (best train acc: 0.8885, best val acc: 0.9282, best train loss: 0.3428  @ epoch 1750 )\n",
      "[Epoch: 1800] train loss: 0.3761, train acc: 0.8812, val loss: 0.2783, val acc: 0.8985  (best train acc: 0.8885, best val acc: 0.9282, best train loss: 0.3428  @ epoch 1750 )\n",
      "[Epoch: 1820] train loss: 0.3576, train acc: 0.8819, val loss: 0.2629, val acc: 0.9113  (best train acc: 0.8885, best val acc: 0.9282, best train loss: 0.3428  @ epoch 1750 )\n",
      "[Epoch: 1840] train loss: 0.3566, train acc: 0.8839, val loss: 0.2450, val acc: 0.9261  (best train acc: 0.8892, best val acc: 0.9282, best train loss: 0.3428  @ epoch 1750 )\n",
      "[Epoch: 1860] train loss: 0.3484, train acc: 0.8881, val loss: 0.2497, val acc: 0.9184  (best train acc: 0.8892, best val acc: 0.9282, best train loss: 0.3417  @ epoch 1844 )\n",
      "[Epoch: 1880] train loss: 0.3584, train acc: 0.8782, val loss: 0.2500, val acc: 0.9255  (best train acc: 0.8899, best val acc: 0.9282, best train loss: 0.3417  @ epoch 1844 )\n",
      "[Epoch: 1900] train loss: 0.3634, train acc: 0.8835, val loss: 0.2473, val acc: 0.9167  (best train acc: 0.8899, best val acc: 0.9282, best train loss: 0.3417  @ epoch 1844 )\n",
      "[Epoch: 1920] train loss: 0.3508, train acc: 0.8851, val loss: 0.2380, val acc: 0.9265  (best train acc: 0.8899, best val acc: 0.9282, best train loss: 0.3405  @ epoch 1919 )\n",
      "[Epoch: 1940] train loss: 0.3521, train acc: 0.8856, val loss: 0.2438, val acc: 0.9258  (best train acc: 0.8899, best val acc: 0.9282, best train loss: 0.3384  @ epoch 1925 )\n",
      "[Epoch: 1960] train loss: 0.3377, train acc: 0.8880, val loss: 0.2347, val acc: 0.9278  (best train acc: 0.8915, best val acc: 0.9295, best train loss: 0.3377  @ epoch 1960 )\n",
      "[Epoch: 1980] train loss: 0.3624, train acc: 0.8772, val loss: 0.2407, val acc: 0.9272  (best train acc: 0.8915, best val acc: 0.9295, best train loss: 0.3359  @ epoch 1978 )\n",
      "[Epoch: 2000] train loss: 0.3712, train acc: 0.8783, val loss: 0.2676, val acc: 0.9039  (best train acc: 0.8927, best val acc: 0.9298, best train loss: 0.3357  @ epoch 1995 )\n",
      "[Epoch: 2020] train loss: 0.3411, train acc: 0.8917, val loss: 0.2466, val acc: 0.9214  (best train acc: 0.8927, best val acc: 0.9298, best train loss: 0.3357  @ epoch 1995 )\n",
      "[Epoch: 2040] train loss: 0.3394, train acc: 0.8904, val loss: 0.2357, val acc: 0.9255  (best train acc: 0.8927, best val acc: 0.9302, best train loss: 0.3340  @ epoch 2023 )\n",
      "[Epoch: 2060] train loss: 0.3468, train acc: 0.8879, val loss: 0.2381, val acc: 0.9282  (best train acc: 0.8927, best val acc: 0.9305, best train loss: 0.3340  @ epoch 2023 )\n",
      "[Epoch: 2080] train loss: 0.3504, train acc: 0.8837, val loss: 0.2631, val acc: 0.9059  (best train acc: 0.8944, best val acc: 0.9305, best train loss: 0.3309  @ epoch 2073 )\n",
      "[Epoch: 2100] train loss: 0.3494, train acc: 0.8840, val loss: 0.2436, val acc: 0.9228  (best train acc: 0.8944, best val acc: 0.9305, best train loss: 0.3309  @ epoch 2073 )\n",
      "[Epoch: 2120] train loss: 0.3410, train acc: 0.8921, val loss: 0.2398, val acc: 0.9248  (best train acc: 0.8944, best val acc: 0.9305, best train loss: 0.3309  @ epoch 2073 )\n",
      "[Epoch: 2140] train loss: 0.3425, train acc: 0.8897, val loss: 0.2576, val acc: 0.9113  (best train acc: 0.8944, best val acc: 0.9305, best train loss: 0.3303  @ epoch 2135 )\n",
      "[Epoch: 2160] train loss: 0.3338, train acc: 0.8871, val loss: 0.2413, val acc: 0.9245  (best train acc: 0.8944, best val acc: 0.9305, best train loss: 0.3298  @ epoch 2153 )\n",
      "[Epoch: 2180] train loss: 0.3279, train acc: 0.8929, val loss: 0.2404, val acc: 0.9170  (best train acc: 0.8957, best val acc: 0.9305, best train loss: 0.3276  @ epoch 2167 )\n",
      "[Epoch: 2200] train loss: 0.3331, train acc: 0.8921, val loss: 0.2354, val acc: 0.9258  (best train acc: 0.8957, best val acc: 0.9305, best train loss: 0.3247  @ epoch 2184 )\n",
      "[Epoch: 2220] train loss: 0.3383, train acc: 0.8884, val loss: 0.2521, val acc: 0.9116  (best train acc: 0.8963, best val acc: 0.9305, best train loss: 0.3245  @ epoch 2207 )\n",
      "[Epoch: 2240] train loss: 0.3495, train acc: 0.8876, val loss: 0.2505, val acc: 0.9133  (best train acc: 0.8964, best val acc: 0.9312, best train loss: 0.3224  @ epoch 2233 )\n",
      "[Epoch: 2260] train loss: 0.3670, train acc: 0.8663, val loss: 0.2538, val acc: 0.9221  (best train acc: 0.8964, best val acc: 0.9312, best train loss: 0.3224  @ epoch 2233 )\n",
      "[Epoch: 2280] train loss: 0.3393, train acc: 0.8899, val loss: 0.2430, val acc: 0.9191  (best train acc: 0.8964, best val acc: 0.9312, best train loss: 0.3224  @ epoch 2233 )\n",
      "[Epoch: 2300] train loss: 0.3482, train acc: 0.8858, val loss: 0.2459, val acc: 0.9207  (best train acc: 0.8964, best val acc: 0.9312, best train loss: 0.3224  @ epoch 2233 )\n",
      "[Epoch: 2320] train loss: 0.3394, train acc: 0.8889, val loss: 0.2316, val acc: 0.9268  (best train acc: 0.8964, best val acc: 0.9312, best train loss: 0.3224  @ epoch 2233 )\n",
      "[Epoch: 2340] train loss: 0.3388, train acc: 0.8938, val loss: 0.2561, val acc: 0.9083  (best train acc: 0.8973, best val acc: 0.9312, best train loss: 0.3204  @ epoch 2334 )\n",
      "[Epoch: 2360] train loss: 0.3362, train acc: 0.8939, val loss: 0.2374, val acc: 0.9275  (best train acc: 0.8973, best val acc: 0.9312, best train loss: 0.3147  @ epoch 2357 )\n",
      "[Epoch: 2380] train loss: 0.3214, train acc: 0.8928, val loss: 0.2424, val acc: 0.9170  (best train acc: 0.8973, best val acc: 0.9312, best train loss: 0.3147  @ epoch 2357 )\n",
      "[Epoch: 2400] train loss: 0.3340, train acc: 0.8903, val loss: 0.2369, val acc: 0.9224  (best train acc: 0.8986, best val acc: 0.9312, best train loss: 0.3147  @ epoch 2357 )\n",
      "[Epoch: 2420] train loss: 0.3300, train acc: 0.8939, val loss: 0.2274, val acc: 0.9282  (best train acc: 0.8994, best val acc: 0.9312, best train loss: 0.3060  @ epoch 2415 )\n",
      "[Epoch: 2440] train loss: 0.3344, train acc: 0.8944, val loss: 0.2401, val acc: 0.9177  (best train acc: 0.8994, best val acc: 0.9312, best train loss: 0.3060  @ epoch 2415 )\n",
      "[Epoch: 2460] train loss: 0.3359, train acc: 0.8911, val loss: 0.2500, val acc: 0.9130  (best train acc: 0.8994, best val acc: 0.9312, best train loss: 0.3060  @ epoch 2415 )\n",
      "[Epoch: 2480] train loss: 0.3251, train acc: 0.8932, val loss: 0.2356, val acc: 0.9295  (best train acc: 0.8994, best val acc: 0.9312, best train loss: 0.3060  @ epoch 2415 )\n",
      "[Epoch: 2500] train loss: 0.3240, train acc: 0.8967, val loss: 0.2378, val acc: 0.9231  (best train acc: 0.8994, best val acc: 0.9312, best train loss: 0.3060  @ epoch 2415 )\n",
      "[Epoch: 2520] train loss: 0.3728, train acc: 0.8696, val loss: 0.2488, val acc: 0.9143  (best train acc: 0.8994, best val acc: 0.9312, best train loss: 0.3060  @ epoch 2415 )\n",
      "[Epoch: 2540] train loss: 0.3383, train acc: 0.8869, val loss: 0.2526, val acc: 0.9120  (best train acc: 0.8994, best val acc: 0.9312, best train loss: 0.3060  @ epoch 2415 )\n",
      "[Epoch: 2560] train loss: 0.3194, train acc: 0.8971, val loss: 0.2300, val acc: 0.9288  (best train acc: 0.8994, best val acc: 0.9312, best train loss: 0.3060  @ epoch 2415 )\n",
      "[Epoch: 2580] train loss: 0.3372, train acc: 0.8937, val loss: 0.2416, val acc: 0.9194  (best train acc: 0.8994, best val acc: 0.9312, best train loss: 0.3060  @ epoch 2415 )\n",
      "[Epoch: 2600] train loss: 0.3369, train acc: 0.8906, val loss: 0.2353, val acc: 0.9278  (best train acc: 0.8994, best val acc: 0.9312, best train loss: 0.3060  @ epoch 2415 )\n",
      "[Epoch: 2620] train loss: 0.3180, train acc: 0.8965, val loss: 0.2360, val acc: 0.9245  (best train acc: 0.8994, best val acc: 0.9312, best train loss: 0.3060  @ epoch 2415 )\n",
      "[Epoch: 2640] train loss: 0.3147, train acc: 0.8976, val loss: 0.2280, val acc: 0.9275  (best train acc: 0.9012, best val acc: 0.9312, best train loss: 0.3060  @ epoch 2415 )\n",
      "[Epoch: 2660] train loss: 0.3276, train acc: 0.8989, val loss: 0.2334, val acc: 0.9224  (best train acc: 0.9012, best val acc: 0.9312, best train loss: 0.3060  @ epoch 2415 )\n",
      "[Epoch: 2680] train loss: 0.3124, train acc: 0.8976, val loss: 0.2416, val acc: 0.9174  (best train acc: 0.9037, best val acc: 0.9312, best train loss: 0.3044  @ epoch 2673 )\n",
      "[Epoch: 2700] train loss: 0.3018, train acc: 0.9012, val loss: 0.2466, val acc: 0.9218  (best train acc: 0.9037, best val acc: 0.9312, best train loss: 0.3018  @ epoch 2700 )\n",
      "[Epoch: 2720] train loss: 0.3378, train acc: 0.8854, val loss: 0.2393, val acc: 0.9278  (best train acc: 0.9040, best val acc: 0.9312, best train loss: 0.3018  @ epoch 2700 )\n",
      "[Epoch: 2740] train loss: 0.3274, train acc: 0.8897, val loss: 0.2330, val acc: 0.9272  (best train acc: 0.9054, best val acc: 0.9312, best train loss: 0.3018  @ epoch 2700 )\n",
      "[Epoch: 2760] train loss: 0.3039, train acc: 0.9023, val loss: 0.2286, val acc: 0.9265  (best train acc: 0.9068, best val acc: 0.9312, best train loss: 0.3005  @ epoch 2751 )\n",
      "[Epoch: 2780] train loss: 0.3199, train acc: 0.8944, val loss: 0.2288, val acc: 0.9282  (best train acc: 0.9068, best val acc: 0.9312, best train loss: 0.2987  @ epoch 2769 )\n",
      "[Epoch: 2800] train loss: 0.3181, train acc: 0.8918, val loss: 0.2508, val acc: 0.9093  (best train acc: 0.9068, best val acc: 0.9319, best train loss: 0.2986  @ epoch 2784 )\n",
      "[Epoch: 2820] train loss: 0.3144, train acc: 0.9007, val loss: 0.2307, val acc: 0.9245  (best train acc: 0.9068, best val acc: 0.9319, best train loss: 0.2962  @ epoch 2818 )\n",
      "[Epoch: 2840] train loss: 0.3022, train acc: 0.9036, val loss: 0.2310, val acc: 0.9231  (best train acc: 0.9068, best val acc: 0.9319, best train loss: 0.2933  @ epoch 2831 )\n",
      "[Epoch: 2860] train loss: 0.3071, train acc: 0.9045, val loss: 0.2259, val acc: 0.9295  (best train acc: 0.9068, best val acc: 0.9319, best train loss: 0.2933  @ epoch 2831 )\n",
      "[Epoch: 2880] train loss: 0.3020, train acc: 0.9011, val loss: 0.2301, val acc: 0.9238  (best train acc: 0.9068, best val acc: 0.9319, best train loss: 0.2933  @ epoch 2831 )\n",
      "[Epoch: 2900] train loss: 0.3108, train acc: 0.8979, val loss: 0.2435, val acc: 0.9147  (best train acc: 0.9070, best val acc: 0.9319, best train loss: 0.2923  @ epoch 2882 )\n",
      "[Epoch: 2920] train loss: 0.2955, train acc: 0.9010, val loss: 0.2570, val acc: 0.9130  (best train acc: 0.9070, best val acc: 0.9319, best train loss: 0.2921  @ epoch 2903 )\n",
      "[Epoch: 2940] train loss: 0.2926, train acc: 0.9031, val loss: 0.2305, val acc: 0.9288  (best train acc: 0.9070, best val acc: 0.9319, best train loss: 0.2891  @ epoch 2929 )\n",
      "[Epoch: 2960] train loss: 0.2867, train acc: 0.9059, val loss: 0.2201, val acc: 0.9292  (best train acc: 0.9082, best val acc: 0.9319, best train loss: 0.2836  @ epoch 2959 )\n",
      "[Epoch: 2980] train loss: 0.2820, train acc: 0.9086, val loss: 0.2202, val acc: 0.9282  (best train acc: 0.9093, best val acc: 0.9319, best train loss: 0.2817  @ epoch 2975 )\n",
      "[Epoch: 3000] train loss: 0.2944, train acc: 0.9025, val loss: 0.2368, val acc: 0.9231  (best train acc: 0.9093, best val acc: 0.9319, best train loss: 0.2777  @ epoch 2981 )\n",
      "[Epoch: 3020] train loss: 0.2905, train acc: 0.9036, val loss: 0.2349, val acc: 0.9177  (best train acc: 0.9093, best val acc: 0.9319, best train loss: 0.2777  @ epoch 2981 )\n",
      "[Epoch: 3040] train loss: 0.2876, train acc: 0.9046, val loss: 0.2214, val acc: 0.9298  (best train acc: 0.9093, best val acc: 0.9319, best train loss: 0.2777  @ epoch 2981 )\n",
      "[Epoch: 3060] train loss: 0.2949, train acc: 0.9072, val loss: 0.2229, val acc: 0.9261  (best train acc: 0.9107, best val acc: 0.9319, best train loss: 0.2777  @ epoch 2981 )\n",
      "[Epoch: 3080] train loss: 0.2866, train acc: 0.9028, val loss: 0.2286, val acc: 0.9245  (best train acc: 0.9107, best val acc: 0.9319, best train loss: 0.2777  @ epoch 2981 )\n",
      "[Epoch: 3100] train loss: 0.2918, train acc: 0.9046, val loss: 0.2258, val acc: 0.9255  (best train acc: 0.9107, best val acc: 0.9319, best train loss: 0.2777  @ epoch 2981 )\n",
      "[Epoch: 3120] train loss: 0.2913, train acc: 0.9076, val loss: 0.2254, val acc: 0.9234  (best train acc: 0.9107, best val acc: 0.9319, best train loss: 0.2734  @ epoch 3105 )\n",
      "[Epoch: 3140] train loss: 0.2849, train acc: 0.9070, val loss: 0.2270, val acc: 0.9248  (best train acc: 0.9107, best val acc: 0.9319, best train loss: 0.2734  @ epoch 3105 )\n",
      "[Epoch: 3160] train loss: 0.2992, train acc: 0.9015, val loss: 0.2196, val acc: 0.9295  (best train acc: 0.9107, best val acc: 0.9319, best train loss: 0.2734  @ epoch 3105 )\n",
      "[Epoch: 3180] train loss: 0.2966, train acc: 0.8997, val loss: 0.2512, val acc: 0.9133  (best train acc: 0.9107, best val acc: 0.9319, best train loss: 0.2734  @ epoch 3105 )\n",
      "[Epoch: 3200] train loss: 0.2885, train acc: 0.9055, val loss: 0.2246, val acc: 0.9258  (best train acc: 0.9107, best val acc: 0.9319, best train loss: 0.2734  @ epoch 3105 )\n",
      "[Epoch: 3220] train loss: 0.3036, train acc: 0.8898, val loss: 0.2338, val acc: 0.9248  (best train acc: 0.9107, best val acc: 0.9319, best train loss: 0.2734  @ epoch 3105 )\n",
      "[Epoch: 3240] train loss: 0.3028, train acc: 0.8990, val loss: 0.2413, val acc: 0.9140  (best train acc: 0.9107, best val acc: 0.9319, best train loss: 0.2734  @ epoch 3105 )\n",
      "[Epoch: 3260] train loss: 0.2873, train acc: 0.9080, val loss: 0.2187, val acc: 0.9322  (best train acc: 0.9107, best val acc: 0.9322, best train loss: 0.2734  @ epoch 3105 )\n",
      "[Epoch: 3280] train loss: 0.2850, train acc: 0.9077, val loss: 0.2187, val acc: 0.9332  (best train acc: 0.9107, best val acc: 0.9332, best train loss: 0.2734  @ epoch 3105 )\n",
      "[Epoch: 3300] train loss: 0.2926, train acc: 0.9031, val loss: 0.2183, val acc: 0.9302  (best train acc: 0.9107, best val acc: 0.9332, best train loss: 0.2697  @ epoch 3281 )\n",
      "[Epoch: 3320] train loss: 0.2792, train acc: 0.9093, val loss: 0.2209, val acc: 0.9265  (best train acc: 0.9107, best val acc: 0.9332, best train loss: 0.2697  @ epoch 3281 )\n",
      "[Epoch: 3340] train loss: 0.2778, train acc: 0.9067, val loss: 0.2323, val acc: 0.9194  (best train acc: 0.9107, best val acc: 0.9332, best train loss: 0.2697  @ epoch 3281 )\n",
      "[Epoch: 3360] train loss: 0.2925, train acc: 0.9013, val loss: 0.2287, val acc: 0.9231  (best train acc: 0.9107, best val acc: 0.9332, best train loss: 0.2697  @ epoch 3281 )\n",
      "[Epoch: 3380] train loss: 0.3054, train acc: 0.8990, val loss: 0.2473, val acc: 0.9106  (best train acc: 0.9107, best val acc: 0.9332, best train loss: 0.2697  @ epoch 3281 )\n",
      "[Epoch: 3400] train loss: 0.2962, train acc: 0.9010, val loss: 0.2239, val acc: 0.9302  (best train acc: 0.9107, best val acc: 0.9332, best train loss: 0.2697  @ epoch 3281 )\n",
      "[Epoch: 3420] train loss: 0.2837, train acc: 0.9067, val loss: 0.2195, val acc: 0.9302  (best train acc: 0.9107, best val acc: 0.9332, best train loss: 0.2697  @ epoch 3281 )\n",
      "[Epoch: 3440] train loss: 0.2995, train acc: 0.8963, val loss: 0.2250, val acc: 0.9295  (best train acc: 0.9107, best val acc: 0.9332, best train loss: 0.2697  @ epoch 3281 )\n",
      "[Epoch: 3460] train loss: 0.2989, train acc: 0.8990, val loss: 0.2243, val acc: 0.9322  (best train acc: 0.9107, best val acc: 0.9332, best train loss: 0.2697  @ epoch 3281 )\n",
      "[Epoch: 3480] train loss: 0.2893, train acc: 0.9046, val loss: 0.2237, val acc: 0.9278  (best train acc: 0.9107, best val acc: 0.9332, best train loss: 0.2697  @ epoch 3281 )\n",
      "[Epoch: 3500] train loss: 0.2854, train acc: 0.9024, val loss: 0.2234, val acc: 0.9251  (best train acc: 0.9107, best val acc: 0.9332, best train loss: 0.2697  @ epoch 3281 )\n",
      "[Epoch: 3520] train loss: 0.2702, train acc: 0.9110, val loss: 0.2180, val acc: 0.9312  (best train acc: 0.9110, best val acc: 0.9332, best train loss: 0.2697  @ epoch 3281 )\n",
      "[Epoch: 3540] train loss: 0.2743, train acc: 0.9057, val loss: 0.2233, val acc: 0.9214  (best train acc: 0.9110, best val acc: 0.9332, best train loss: 0.2697  @ epoch 3281 )\n",
      "[Epoch: 3560] train loss: 0.2854, train acc: 0.9038, val loss: 0.2199, val acc: 0.9302  (best train acc: 0.9110, best val acc: 0.9332, best train loss: 0.2697  @ epoch 3281 )\n",
      "[Epoch: 3580] train loss: 0.2848, train acc: 0.9057, val loss: 0.2266, val acc: 0.9238  (best train acc: 0.9112, best val acc: 0.9332, best train loss: 0.2697  @ epoch 3281 )\n",
      "[Epoch: 3600] train loss: 0.2908, train acc: 0.9044, val loss: 0.2183, val acc: 0.9295  (best train acc: 0.9112, best val acc: 0.9332, best train loss: 0.2662  @ epoch 3584 )\n",
      "[Epoch: 3620] train loss: 0.2834, train acc: 0.9041, val loss: 0.2229, val acc: 0.9312  (best train acc: 0.9132, best val acc: 0.9339, best train loss: 0.2662  @ epoch 3584 )\n",
      "[Epoch: 3640] train loss: 0.2884, train acc: 0.9023, val loss: 0.2257, val acc: 0.9298  (best train acc: 0.9132, best val acc: 0.9339, best train loss: 0.2662  @ epoch 3584 )\n",
      "[Epoch: 3660] train loss: 0.2889, train acc: 0.9074, val loss: 0.2188, val acc: 0.9275  (best train acc: 0.9132, best val acc: 0.9339, best train loss: 0.2662  @ epoch 3584 )\n",
      "[Epoch: 3680] train loss: 0.2931, train acc: 0.8983, val loss: 0.2289, val acc: 0.9214  (best train acc: 0.9132, best val acc: 0.9339, best train loss: 0.2611  @ epoch 3661 )\n",
      "[Epoch: 3700] train loss: 0.2911, train acc: 0.9008, val loss: 0.2366, val acc: 0.9211  (best train acc: 0.9132, best val acc: 0.9339, best train loss: 0.2611  @ epoch 3661 )\n",
      "[Epoch: 3720] train loss: 0.2750, train acc: 0.9096, val loss: 0.2160, val acc: 0.9309  (best train acc: 0.9135, best val acc: 0.9339, best train loss: 0.2611  @ epoch 3661 )\n",
      "[Epoch: 3740] train loss: 0.2849, train acc: 0.9037, val loss: 0.2176, val acc: 0.9298  (best train acc: 0.9135, best val acc: 0.9339, best train loss: 0.2611  @ epoch 3661 )\n",
      "[Epoch: 3760] train loss: 0.2879, train acc: 0.9040, val loss: 0.2229, val acc: 0.9255  (best train acc: 0.9135, best val acc: 0.9339, best train loss: 0.2611  @ epoch 3661 )\n",
      "[Epoch: 3780] train loss: 0.2782, train acc: 0.9110, val loss: 0.2166, val acc: 0.9302  (best train acc: 0.9154, best val acc: 0.9339, best train loss: 0.2611  @ epoch 3661 )\n",
      "[Epoch: 3800] train loss: 0.2873, train acc: 0.9028, val loss: 0.2394, val acc: 0.9191  (best train acc: 0.9154, best val acc: 0.9339, best train loss: 0.2611  @ epoch 3661 )\n",
      "[Epoch: 3820] train loss: 0.2586, train acc: 0.9118, val loss: 0.2121, val acc: 0.9312  (best train acc: 0.9154, best val acc: 0.9339, best train loss: 0.2586  @ epoch 3820 )\n",
      "[Epoch: 3840] train loss: 0.2713, train acc: 0.9096, val loss: 0.2131, val acc: 0.9295  (best train acc: 0.9154, best val acc: 0.9339, best train loss: 0.2586  @ epoch 3820 )\n",
      "[Epoch: 3860] train loss: 0.2717, train acc: 0.9127, val loss: 0.2263, val acc: 0.9211  (best train acc: 0.9154, best val acc: 0.9339, best train loss: 0.2586  @ epoch 3820 )\n",
      "[Epoch: 3880] train loss: 0.2801, train acc: 0.9070, val loss: 0.2149, val acc: 0.9288  (best train acc: 0.9154, best val acc: 0.9339, best train loss: 0.2565  @ epoch 3871 )\n",
      "[Epoch: 3900] train loss: 0.2763, train acc: 0.9119, val loss: 0.2176, val acc: 0.9282  (best train acc: 0.9154, best val acc: 0.9339, best train loss: 0.2565  @ epoch 3871 )\n",
      "[Epoch: 3920] train loss: 0.2716, train acc: 0.9092, val loss: 0.2144, val acc: 0.9298  (best train acc: 0.9154, best val acc: 0.9339, best train loss: 0.2565  @ epoch 3871 )\n",
      "[Epoch: 3940] train loss: 0.2704, train acc: 0.9112, val loss: 0.2124, val acc: 0.9315  (best train acc: 0.9154, best val acc: 0.9339, best train loss: 0.2565  @ epoch 3871 )\n",
      "[Epoch: 3960] train loss: 0.2789, train acc: 0.9067, val loss: 0.2181, val acc: 0.9305  (best train acc: 0.9154, best val acc: 0.9339, best train loss: 0.2565  @ epoch 3871 )\n",
      "[Epoch: 3980] train loss: 0.2671, train acc: 0.9126, val loss: 0.2108, val acc: 0.9329  (best train acc: 0.9154, best val acc: 0.9339, best train loss: 0.2565  @ epoch 3871 )\n",
      "[Epoch: 4000] train loss: 0.2702, train acc: 0.9085, val loss: 0.2095, val acc: 0.9339  (best train acc: 0.9154, best val acc: 0.9339, best train loss: 0.2565  @ epoch 3871 )\n",
      "[Epoch: 4020] train loss: 0.2663, train acc: 0.9098, val loss: 0.2207, val acc: 0.9238  (best train acc: 0.9158, best val acc: 0.9339, best train loss: 0.2565  @ epoch 3871 )\n",
      "[Epoch: 4040] train loss: 0.2765, train acc: 0.9083, val loss: 0.2186, val acc: 0.9238  (best train acc: 0.9158, best val acc: 0.9342, best train loss: 0.2565  @ epoch 3871 )\n",
      "[Epoch: 4060] train loss: 0.2815, train acc: 0.9034, val loss: 0.2235, val acc: 0.9234  (best train acc: 0.9158, best val acc: 0.9342, best train loss: 0.2546  @ epoch 4044 )\n",
      "[Epoch: 4080] train loss: 0.2790, train acc: 0.9051, val loss: 0.2216, val acc: 0.9241  (best train acc: 0.9158, best val acc: 0.9342, best train loss: 0.2546  @ epoch 4044 )\n",
      "[Epoch: 4100] train loss: 0.2826, train acc: 0.9002, val loss: 0.2258, val acc: 0.9241  (best train acc: 0.9158, best val acc: 0.9342, best train loss: 0.2546  @ epoch 4044 )\n",
      "[Epoch: 4120] train loss: 0.2577, train acc: 0.9111, val loss: 0.2207, val acc: 0.9275  (best train acc: 0.9158, best val acc: 0.9342, best train loss: 0.2546  @ epoch 4044 )\n",
      "[Epoch: 4140] train loss: 0.2637, train acc: 0.9140, val loss: 0.2085, val acc: 0.9305  (best train acc: 0.9158, best val acc: 0.9342, best train loss: 0.2546  @ epoch 4044 )\n",
      "[Epoch: 4160] train loss: 0.2770, train acc: 0.9104, val loss: 0.2118, val acc: 0.9312  (best train acc: 0.9178, best val acc: 0.9342, best train loss: 0.2546  @ epoch 4044 )\n",
      "[Epoch: 4180] train loss: 0.2668, train acc: 0.9113, val loss: 0.2161, val acc: 0.9272  (best train acc: 0.9178, best val acc: 0.9342, best train loss: 0.2546  @ epoch 4044 )\n",
      "[Epoch: 4200] train loss: 0.2667, train acc: 0.9119, val loss: 0.2101, val acc: 0.9309  (best train acc: 0.9178, best val acc: 0.9342, best train loss: 0.2546  @ epoch 4044 )\n",
      "[Epoch: 4220] train loss: 0.2656, train acc: 0.9116, val loss: 0.2220, val acc: 0.9204  (best train acc: 0.9178, best val acc: 0.9342, best train loss: 0.2546  @ epoch 4044 )\n",
      "[Epoch: 4240] train loss: 0.2596, train acc: 0.9116, val loss: 0.2114, val acc: 0.9298  (best train acc: 0.9178, best val acc: 0.9342, best train loss: 0.2546  @ epoch 4044 )\n",
      "[Epoch: 4260] train loss: 0.2727, train acc: 0.9112, val loss: 0.2189, val acc: 0.9238  (best train acc: 0.9178, best val acc: 0.9342, best train loss: 0.2532  @ epoch 4259 )\n",
      "[Epoch: 4280] train loss: 0.2699, train acc: 0.9110, val loss: 0.2154, val acc: 0.9275  (best train acc: 0.9178, best val acc: 0.9342, best train loss: 0.2532  @ epoch 4259 )\n",
      "[Epoch: 4300] train loss: 0.2810, train acc: 0.9052, val loss: 0.2419, val acc: 0.9170  (best train acc: 0.9179, best val acc: 0.9369, best train loss: 0.2523  @ epoch 4297 )\n",
      "[Epoch: 4320] train loss: 0.2859, train acc: 0.9047, val loss: 0.2207, val acc: 0.9255  (best train acc: 0.9179, best val acc: 0.9369, best train loss: 0.2523  @ epoch 4297 )\n",
      "[Epoch: 4340] train loss: 0.2651, train acc: 0.9091, val loss: 0.2291, val acc: 0.9194  (best train acc: 0.9179, best val acc: 0.9369, best train loss: 0.2523  @ epoch 4297 )\n",
      "[Epoch: 4360] train loss: 0.2752, train acc: 0.9096, val loss: 0.2130, val acc: 0.9312  (best train acc: 0.9179, best val acc: 0.9369, best train loss: 0.2523  @ epoch 4297 )\n",
      "[Epoch: 4380] train loss: 0.2899, train acc: 0.9000, val loss: 0.2183, val acc: 0.9268  (best train acc: 0.9179, best val acc: 0.9369, best train loss: 0.2523  @ epoch 4297 )\n",
      "[Epoch: 4400] train loss: 0.2698, train acc: 0.9109, val loss: 0.2281, val acc: 0.9221  (best train acc: 0.9179, best val acc: 0.9369, best train loss: 0.2523  @ epoch 4297 )\n",
      "[Epoch: 4420] train loss: 0.2615, train acc: 0.9141, val loss: 0.2122, val acc: 0.9298  (best train acc: 0.9187, best val acc: 0.9369, best train loss: 0.2462  @ epoch 4417 )\n",
      "[Epoch: 4440] train loss: 0.2581, train acc: 0.9121, val loss: 0.2083, val acc: 0.9295  (best train acc: 0.9187, best val acc: 0.9369, best train loss: 0.2462  @ epoch 4417 )\n",
      "[Epoch: 4460] train loss: 0.2772, train acc: 0.9061, val loss: 0.2185, val acc: 0.9292  (best train acc: 0.9187, best val acc: 0.9369, best train loss: 0.2462  @ epoch 4417 )\n",
      "[Epoch: 4480] train loss: 0.2712, train acc: 0.9135, val loss: 0.2063, val acc: 0.9336  (best train acc: 0.9198, best val acc: 0.9369, best train loss: 0.2462  @ epoch 4417 )\n",
      "[Epoch: 4500] train loss: 0.2594, train acc: 0.9171, val loss: 0.2060, val acc: 0.9342  (best train acc: 0.9198, best val acc: 0.9376, best train loss: 0.2462  @ epoch 4417 )\n",
      "[Epoch: 4520] train loss: 0.2638, train acc: 0.9097, val loss: 0.2082, val acc: 0.9319  (best train acc: 0.9198, best val acc: 0.9376, best train loss: 0.2462  @ epoch 4417 )\n",
      "[Epoch: 4540] train loss: 0.2672, train acc: 0.9013, val loss: 0.2164, val acc: 0.9255  (best train acc: 0.9198, best val acc: 0.9376, best train loss: 0.2462  @ epoch 4417 )\n",
      "[Epoch: 4560] train loss: 0.2537, train acc: 0.9133, val loss: 0.2103, val acc: 0.9282  (best train acc: 0.9198, best val acc: 0.9376, best train loss: 0.2462  @ epoch 4417 )\n",
      "[Epoch: 4580] train loss: 0.2607, train acc: 0.9114, val loss: 0.2237, val acc: 0.9251  (best train acc: 0.9198, best val acc: 0.9376, best train loss: 0.2462  @ epoch 4417 )\n",
      "[Epoch: 4600] train loss: 0.2664, train acc: 0.9110, val loss: 0.2106, val acc: 0.9312  (best train acc: 0.9198, best val acc: 0.9376, best train loss: 0.2462  @ epoch 4417 )\n",
      "[Epoch: 4620] train loss: 0.2786, train acc: 0.9079, val loss: 0.2214, val acc: 0.9265  (best train acc: 0.9198, best val acc: 0.9376, best train loss: 0.2462  @ epoch 4417 )\n",
      "[Epoch: 4640] train loss: 0.2680, train acc: 0.9141, val loss: 0.2051, val acc: 0.9359  (best train acc: 0.9198, best val acc: 0.9376, best train loss: 0.2462  @ epoch 4417 )\n",
      "[Epoch: 4660] train loss: 0.2660, train acc: 0.9087, val loss: 0.2206, val acc: 0.9248  (best train acc: 0.9198, best val acc: 0.9376, best train loss: 0.2462  @ epoch 4417 )\n",
      "[Epoch: 4680] train loss: 0.2520, train acc: 0.9197, val loss: 0.2067, val acc: 0.9352  (best train acc: 0.9198, best val acc: 0.9376, best train loss: 0.2462  @ epoch 4417 )\n",
      "[Epoch: 4700] train loss: 0.2561, train acc: 0.9166, val loss: 0.2168, val acc: 0.9241  (best train acc: 0.9198, best val acc: 0.9376, best train loss: 0.2462  @ epoch 4417 )\n",
      "[Epoch: 4720] train loss: 0.2565, train acc: 0.9135, val loss: 0.2141, val acc: 0.9268  (best train acc: 0.9198, best val acc: 0.9376, best train loss: 0.2462  @ epoch 4417 )\n",
      "[Epoch: 4740] train loss: 0.2581, train acc: 0.9148, val loss: 0.2168, val acc: 0.9339  (best train acc: 0.9198, best val acc: 0.9376, best train loss: 0.2462  @ epoch 4417 )\n",
      "[Epoch: 4760] train loss: 0.2615, train acc: 0.9109, val loss: 0.2106, val acc: 0.9325  (best train acc: 0.9198, best val acc: 0.9376, best train loss: 0.2462  @ epoch 4417 )\n",
      "[Epoch: 4780] train loss: 0.2494, train acc: 0.9140, val loss: 0.2168, val acc: 0.9268  (best train acc: 0.9198, best val acc: 0.9376, best train loss: 0.2462  @ epoch 4417 )\n",
      "[Epoch: 4800] train loss: 0.2524, train acc: 0.9194, val loss: 0.2042, val acc: 0.9352  (best train acc: 0.9198, best val acc: 0.9376, best train loss: 0.2462  @ epoch 4417 )\n",
      "[Epoch: 4820] train loss: 0.2629, train acc: 0.9121, val loss: 0.2138, val acc: 0.9298  (best train acc: 0.9198, best val acc: 0.9376, best train loss: 0.2462  @ epoch 4417 )\n",
      "[Epoch: 4840] train loss: 0.2588, train acc: 0.9159, val loss: 0.2096, val acc: 0.9288  (best train acc: 0.9198, best val acc: 0.9376, best train loss: 0.2462  @ epoch 4417 )\n",
      "[Epoch: 4860] train loss: 0.2658, train acc: 0.9089, val loss: 0.2075, val acc: 0.9322  (best train acc: 0.9198, best val acc: 0.9376, best train loss: 0.2462  @ epoch 4417 )\n",
      "[Epoch: 4880] train loss: 0.2730, train acc: 0.9101, val loss: 0.2143, val acc: 0.9288  (best train acc: 0.9198, best val acc: 0.9376, best train loss: 0.2462  @ epoch 4417 )\n",
      "[Epoch: 4900] train loss: 0.2541, train acc: 0.9176, val loss: 0.2273, val acc: 0.9211  (best train acc: 0.9198, best val acc: 0.9376, best train loss: 0.2462  @ epoch 4417 )\n",
      "[Epoch: 4920] train loss: 0.2488, train acc: 0.9178, val loss: 0.2099, val acc: 0.9305  (best train acc: 0.9198, best val acc: 0.9376, best train loss: 0.2445  @ epoch 4919 )\n",
      "[Epoch: 4940] train loss: 0.2554, train acc: 0.9145, val loss: 0.2073, val acc: 0.9342  (best train acc: 0.9198, best val acc: 0.9376, best train loss: 0.2445  @ epoch 4919 )\n",
      "[Epoch: 4960] train loss: 0.2567, train acc: 0.9154, val loss: 0.2143, val acc: 0.9325  (best train acc: 0.9198, best val acc: 0.9376, best train loss: 0.2445  @ epoch 4919 )\n",
      "[Epoch: 4980] train loss: 0.2556, train acc: 0.9145, val loss: 0.2093, val acc: 0.9305  (best train acc: 0.9203, best val acc: 0.9376, best train loss: 0.2441  @ epoch 4970 )\n",
      "[Epoch: 5000] train loss: 0.2623, train acc: 0.9137, val loss: 0.2121, val acc: 0.9315  (best train acc: 0.9203, best val acc: 0.9376, best train loss: 0.2441  @ epoch 4970 )\n",
      "[Epoch: 5020] train loss: 0.2577, train acc: 0.9142, val loss: 0.2043, val acc: 0.9346  (best train acc: 0.9208, best val acc: 0.9376, best train loss: 0.2441  @ epoch 4970 )\n",
      "[Epoch: 5040] train loss: 0.2569, train acc: 0.9157, val loss: 0.2099, val acc: 0.9305  (best train acc: 0.9208, best val acc: 0.9376, best train loss: 0.2438  @ epoch 5029 )\n",
      "[Epoch: 5060] train loss: 0.2521, train acc: 0.9147, val loss: 0.2037, val acc: 0.9332  (best train acc: 0.9208, best val acc: 0.9376, best train loss: 0.2405  @ epoch 5058 )\n",
      "[Epoch: 5080] train loss: 0.2690, train acc: 0.9094, val loss: 0.2169, val acc: 0.9282  (best train acc: 0.9209, best val acc: 0.9376, best train loss: 0.2375  @ epoch 5074 )\n",
      "[Epoch: 5100] train loss: 0.2452, train acc: 0.9173, val loss: 0.1978, val acc: 0.9352  (best train acc: 0.9209, best val acc: 0.9376, best train loss: 0.2375  @ epoch 5074 )\n",
      "[Epoch: 5120] train loss: 0.2438, train acc: 0.9182, val loss: 0.1996, val acc: 0.9342  (best train acc: 0.9223, best val acc: 0.9383, best train loss: 0.2375  @ epoch 5074 )\n",
      "[Epoch: 5140] train loss: 0.2533, train acc: 0.9158, val loss: 0.2079, val acc: 0.9342  (best train acc: 0.9223, best val acc: 0.9383, best train loss: 0.2375  @ epoch 5074 )\n",
      "[Epoch: 5160] train loss: 0.2503, train acc: 0.9166, val loss: 0.2020, val acc: 0.9309  (best train acc: 0.9223, best val acc: 0.9383, best train loss: 0.2375  @ epoch 5074 )\n",
      "[Epoch: 5180] train loss: 0.2621, train acc: 0.9099, val loss: 0.2016, val acc: 0.9363  (best train acc: 0.9223, best val acc: 0.9383, best train loss: 0.2375  @ epoch 5074 )\n",
      "[Epoch: 5200] train loss: 0.2509, train acc: 0.9195, val loss: 0.2105, val acc: 0.9356  (best train acc: 0.9223, best val acc: 0.9383, best train loss: 0.2375  @ epoch 5074 )\n",
      "[Epoch: 5220] train loss: 0.2560, train acc: 0.9142, val loss: 0.2099, val acc: 0.9363  (best train acc: 0.9223, best val acc: 0.9383, best train loss: 0.2375  @ epoch 5074 )\n",
      "[Epoch: 5240] train loss: 0.2656, train acc: 0.9119, val loss: 0.2048, val acc: 0.9332  (best train acc: 0.9223, best val acc: 0.9390, best train loss: 0.2375  @ epoch 5074 )\n",
      "[Epoch: 5260] train loss: 0.2599, train acc: 0.9149, val loss: 0.2118, val acc: 0.9342  (best train acc: 0.9223, best val acc: 0.9390, best train loss: 0.2375  @ epoch 5074 )\n",
      "[Epoch: 5280] train loss: 0.2571, train acc: 0.9190, val loss: 0.2042, val acc: 0.9312  (best train acc: 0.9223, best val acc: 0.9390, best train loss: 0.2375  @ epoch 5074 )\n",
      "[Epoch: 5300] train loss: 0.2590, train acc: 0.9145, val loss: 0.2103, val acc: 0.9272  (best train acc: 0.9237, best val acc: 0.9390, best train loss: 0.2375  @ epoch 5074 )\n",
      "[Epoch: 5320] train loss: 0.2905, train acc: 0.9038, val loss: 0.2560, val acc: 0.9039  (best train acc: 0.9237, best val acc: 0.9393, best train loss: 0.2371  @ epoch 5312 )\n",
      "[Epoch: 5340] train loss: 0.2557, train acc: 0.9151, val loss: 0.2043, val acc: 0.9325  (best train acc: 0.9237, best val acc: 0.9393, best train loss: 0.2371  @ epoch 5312 )\n",
      "[Epoch: 5360] train loss: 0.2373, train acc: 0.9190, val loss: 0.2006, val acc: 0.9346  (best train acc: 0.9237, best val acc: 0.9393, best train loss: 0.2371  @ epoch 5312 )\n",
      "[Epoch: 5380] train loss: 0.2480, train acc: 0.9175, val loss: 0.2051, val acc: 0.9336  (best train acc: 0.9237, best val acc: 0.9393, best train loss: 0.2359  @ epoch 5361 )\n",
      "[Epoch: 5400] train loss: 0.2520, train acc: 0.9169, val loss: 0.1973, val acc: 0.9322  (best train acc: 0.9237, best val acc: 0.9393, best train loss: 0.2329  @ epoch 5388 )\n",
      "[Epoch: 5420] train loss: 0.2566, train acc: 0.9099, val loss: 0.2232, val acc: 0.9147  (best train acc: 0.9237, best val acc: 0.9396, best train loss: 0.2329  @ epoch 5388 )\n",
      "[Epoch: 5440] train loss: 0.2411, train acc: 0.9185, val loss: 0.2064, val acc: 0.9332  (best train acc: 0.9237, best val acc: 0.9396, best train loss: 0.2329  @ epoch 5388 )\n",
      "[Epoch: 5460] train loss: 0.2428, train acc: 0.9205, val loss: 0.2013, val acc: 0.9369  (best train acc: 0.9237, best val acc: 0.9396, best train loss: 0.2329  @ epoch 5388 )\n",
      "[Epoch: 5480] train loss: 0.2416, train acc: 0.9191, val loss: 0.1977, val acc: 0.9366  (best train acc: 0.9237, best val acc: 0.9406, best train loss: 0.2329  @ epoch 5388 )\n",
      "[Epoch: 5500] train loss: 0.2474, train acc: 0.9169, val loss: 0.2044, val acc: 0.9356  (best train acc: 0.9237, best val acc: 0.9406, best train loss: 0.2329  @ epoch 5388 )\n",
      "[Epoch: 5520] train loss: 0.2477, train acc: 0.9166, val loss: 0.2076, val acc: 0.9322  (best train acc: 0.9237, best val acc: 0.9406, best train loss: 0.2329  @ epoch 5388 )\n",
      "[Epoch: 5540] train loss: 0.2628, train acc: 0.9104, val loss: 0.2260, val acc: 0.9224  (best train acc: 0.9237, best val acc: 0.9406, best train loss: 0.2329  @ epoch 5388 )\n",
      "[Epoch: 5560] train loss: 0.2478, train acc: 0.9139, val loss: 0.2039, val acc: 0.9383  (best train acc: 0.9237, best val acc: 0.9417, best train loss: 0.2329  @ epoch 5388 )\n",
      "[Epoch: 5580] train loss: 0.2475, train acc: 0.9171, val loss: 0.2053, val acc: 0.9352  (best train acc: 0.9237, best val acc: 0.9417, best train loss: 0.2329  @ epoch 5388 )\n",
      "[Epoch: 5600] train loss: 0.2315, train acc: 0.9203, val loss: 0.2007, val acc: 0.9383  (best train acc: 0.9237, best val acc: 0.9417, best train loss: 0.2315  @ epoch 5600 )\n",
      "[Epoch: 5620] train loss: 0.2377, train acc: 0.9172, val loss: 0.2135, val acc: 0.9298  (best train acc: 0.9237, best val acc: 0.9417, best train loss: 0.2315  @ epoch 5600 )\n",
      "[Epoch: 5640] train loss: 0.2488, train acc: 0.9180, val loss: 0.2133, val acc: 0.9265  (best train acc: 0.9237, best val acc: 0.9417, best train loss: 0.2315  @ epoch 5600 )\n",
      "[Epoch: 5660] train loss: 0.2464, train acc: 0.9202, val loss: 0.2186, val acc: 0.9312  (best train acc: 0.9237, best val acc: 0.9417, best train loss: 0.2315  @ epoch 5600 )\n",
      "[Epoch: 5680] train loss: 0.2559, train acc: 0.9140, val loss: 0.2145, val acc: 0.9305  (best train acc: 0.9237, best val acc: 0.9417, best train loss: 0.2315  @ epoch 5600 )\n",
      "[Epoch: 5700] train loss: 0.2478, train acc: 0.9155, val loss: 0.2043, val acc: 0.9363  (best train acc: 0.9241, best val acc: 0.9417, best train loss: 0.2301  @ epoch 5697 )\n",
      "[Epoch: 5720] train loss: 0.2460, train acc: 0.9180, val loss: 0.2146, val acc: 0.9309  (best train acc: 0.9241, best val acc: 0.9417, best train loss: 0.2301  @ epoch 5697 )\n",
      "[Epoch: 5740] train loss: 0.2523, train acc: 0.9195, val loss: 0.2177, val acc: 0.9302  (best train acc: 0.9241, best val acc: 0.9417, best train loss: 0.2301  @ epoch 5697 )\n",
      "[Epoch: 5760] train loss: 0.2592, train acc: 0.9070, val loss: 0.2068, val acc: 0.9346  (best train acc: 0.9241, best val acc: 0.9417, best train loss: 0.2301  @ epoch 5697 )\n",
      "[Epoch: 5780] train loss: 0.2532, train acc: 0.9067, val loss: 0.2009, val acc: 0.9349  (best train acc: 0.9241, best val acc: 0.9417, best train loss: 0.2301  @ epoch 5697 )\n",
      "[Epoch: 5800] train loss: 0.2439, train acc: 0.9097, val loss: 0.2098, val acc: 0.9386  (best train acc: 0.9245, best val acc: 0.9430, best train loss: 0.2280  @ epoch 5793 )\n",
      "[Epoch: 5820] train loss: 0.2417, train acc: 0.9168, val loss: 0.2028, val acc: 0.9346  (best train acc: 0.9245, best val acc: 0.9430, best train loss: 0.2280  @ epoch 5793 )\n",
      "[Epoch: 5840] train loss: 0.2488, train acc: 0.9170, val loss: 0.2171, val acc: 0.9315  (best train acc: 0.9245, best val acc: 0.9430, best train loss: 0.2280  @ epoch 5793 )\n",
      "[Epoch: 5860] train loss: 0.2312, train acc: 0.9221, val loss: 0.2058, val acc: 0.9379  (best train acc: 0.9245, best val acc: 0.9430, best train loss: 0.2280  @ epoch 5793 )\n",
      "[Epoch: 5880] train loss: 0.2353, train acc: 0.9216, val loss: 0.2052, val acc: 0.9356  (best train acc: 0.9245, best val acc: 0.9430, best train loss: 0.2280  @ epoch 5872 )\n",
      "[Epoch: 5900] train loss: 0.2403, train acc: 0.9200, val loss: 0.2036, val acc: 0.9373  (best train acc: 0.9245, best val acc: 0.9430, best train loss: 0.2280  @ epoch 5872 )\n",
      "[Epoch: 5920] train loss: 0.2423, train acc: 0.9172, val loss: 0.2054, val acc: 0.9393  (best train acc: 0.9245, best val acc: 0.9430, best train loss: 0.2279  @ epoch 5908 )\n",
      "[Epoch: 5940] train loss: 0.2350, train acc: 0.9211, val loss: 0.2090, val acc: 0.9336  (best train acc: 0.9245, best val acc: 0.9430, best train loss: 0.2279  @ epoch 5908 )\n",
      "[Epoch: 5960] train loss: 0.2485, train acc: 0.9174, val loss: 0.2155, val acc: 0.9346  (best train acc: 0.9245, best val acc: 0.9430, best train loss: 0.2279  @ epoch 5908 )\n",
      "[Epoch: 5980] train loss: 0.2425, train acc: 0.9211, val loss: 0.2093, val acc: 0.9339  (best train acc: 0.9245, best val acc: 0.9430, best train loss: 0.2269  @ epoch 5971 )\n",
      "[Epoch: 6000] train loss: 0.2435, train acc: 0.9217, val loss: 0.1991, val acc: 0.9366  (best train acc: 0.9254, best val acc: 0.9433, best train loss: 0.2253  @ epoch 5996 )\n",
      "[Epoch: 6020] train loss: 0.2267, train acc: 0.9243, val loss: 0.1991, val acc: 0.9393  (best train acc: 0.9254, best val acc: 0.9444, best train loss: 0.2253  @ epoch 5996 )\n",
      "[Epoch: 6040] train loss: 0.2494, train acc: 0.9112, val loss: 0.2045, val acc: 0.9379  (best train acc: 0.9254, best val acc: 0.9444, best train loss: 0.2227  @ epoch 6026 )\n",
      "[Epoch: 6060] train loss: 0.2624, train acc: 0.9104, val loss: 0.2112, val acc: 0.9400  (best train acc: 0.9254, best val acc: 0.9444, best train loss: 0.2227  @ epoch 6026 )\n",
      "[Epoch: 6080] train loss: 0.2366, train acc: 0.9188, val loss: 0.1965, val acc: 0.9454  (best train acc: 0.9254, best val acc: 0.9454, best train loss: 0.2227  @ epoch 6026 )\n",
      "[Epoch: 6100] train loss: 0.2384, train acc: 0.9229, val loss: 0.2018, val acc: 0.9410  (best train acc: 0.9254, best val acc: 0.9454, best train loss: 0.2219  @ epoch 6097 )\n",
      "[Epoch: 6120] train loss: 0.2565, train acc: 0.9091, val loss: 0.2026, val acc: 0.9444  (best train acc: 0.9254, best val acc: 0.9454, best train loss: 0.2219  @ epoch 6097 )\n",
      "[Epoch: 6140] train loss: 0.2336, train acc: 0.9239, val loss: 0.1983, val acc: 0.9390  (best train acc: 0.9254, best val acc: 0.9454, best train loss: 0.2219  @ epoch 6097 )\n",
      "[Epoch: 6160] train loss: 0.2389, train acc: 0.9166, val loss: 0.1987, val acc: 0.9379  (best train acc: 0.9254, best val acc: 0.9454, best train loss: 0.2219  @ epoch 6097 )\n",
      "[Epoch: 6180] train loss: 0.2284, train acc: 0.9225, val loss: 0.1899, val acc: 0.9403  (best train acc: 0.9262, best val acc: 0.9460, best train loss: 0.2177  @ epoch 6166 )\n",
      "[Epoch: 6200] train loss: 0.2634, train acc: 0.9026, val loss: 0.2363, val acc: 0.9180  (best train acc: 0.9278, best val acc: 0.9460, best train loss: 0.2177  @ epoch 6166 )\n",
      "[Epoch: 6220] train loss: 0.2327, train acc: 0.9203, val loss: 0.1953, val acc: 0.9417  (best train acc: 0.9278, best val acc: 0.9460, best train loss: 0.2177  @ epoch 6166 )\n",
      "[Epoch: 6240] train loss: 0.2349, train acc: 0.9233, val loss: 0.1978, val acc: 0.9376  (best train acc: 0.9278, best val acc: 0.9460, best train loss: 0.2177  @ epoch 6166 )\n",
      "[Epoch: 6260] train loss: 0.2425, train acc: 0.9199, val loss: 0.1989, val acc: 0.9444  (best train acc: 0.9278, best val acc: 0.9460, best train loss: 0.2177  @ epoch 6166 )\n",
      "[Epoch: 6280] train loss: 0.2324, train acc: 0.9245, val loss: 0.1894, val acc: 0.9427  (best train acc: 0.9278, best val acc: 0.9460, best train loss: 0.2177  @ epoch 6166 )\n",
      "[Epoch: 6300] train loss: 0.2266, train acc: 0.9238, val loss: 0.1978, val acc: 0.9403  (best train acc: 0.9278, best val acc: 0.9460, best train loss: 0.2177  @ epoch 6166 )\n",
      "[Epoch: 6320] train loss: 0.2379, train acc: 0.9186, val loss: 0.2165, val acc: 0.9319  (best train acc: 0.9278, best val acc: 0.9460, best train loss: 0.2177  @ epoch 6166 )\n",
      "[Epoch: 6340] train loss: 0.2310, train acc: 0.9216, val loss: 0.2023, val acc: 0.9467  (best train acc: 0.9278, best val acc: 0.9477, best train loss: 0.2177  @ epoch 6166 )\n",
      "[Epoch: 6360] train loss: 0.2425, train acc: 0.9175, val loss: 0.2141, val acc: 0.9268  (best train acc: 0.9278, best val acc: 0.9477, best train loss: 0.2177  @ epoch 6166 )\n",
      "[Epoch: 6380] train loss: 0.2309, train acc: 0.9202, val loss: 0.2013, val acc: 0.9400  (best train acc: 0.9278, best val acc: 0.9477, best train loss: 0.2177  @ epoch 6166 )\n",
      "[Epoch: 6400] train loss: 0.2281, train acc: 0.9247, val loss: 0.2244, val acc: 0.9275  (best train acc: 0.9278, best val acc: 0.9477, best train loss: 0.2177  @ epoch 6166 )\n",
      "[Epoch: 6420] train loss: 0.2369, train acc: 0.9196, val loss: 0.2021, val acc: 0.9410  (best train acc: 0.9278, best val acc: 0.9477, best train loss: 0.2177  @ epoch 6166 )\n",
      "[Epoch: 6440] train loss: 0.2267, train acc: 0.9229, val loss: 0.1987, val acc: 0.9447  (best train acc: 0.9278, best val acc: 0.9484, best train loss: 0.2177  @ epoch 6166 )\n",
      "[Epoch: 6460] train loss: 0.2349, train acc: 0.9215, val loss: 0.2016, val acc: 0.9420  (best train acc: 0.9278, best val acc: 0.9484, best train loss: 0.2177  @ epoch 6166 )\n",
      "[Epoch: 6480] train loss: 0.2284, train acc: 0.9190, val loss: 0.2007, val acc: 0.9373  (best train acc: 0.9278, best val acc: 0.9484, best train loss: 0.2164  @ epoch 6475 )\n",
      "[Epoch: 6500] train loss: 0.2299, train acc: 0.9227, val loss: 0.2073, val acc: 0.9342  (best train acc: 0.9278, best val acc: 0.9484, best train loss: 0.2147  @ epoch 6490 )\n",
      "[Epoch: 6520] train loss: 0.2358, train acc: 0.9200, val loss: 0.2061, val acc: 0.9396  (best train acc: 0.9278, best val acc: 0.9484, best train loss: 0.2147  @ epoch 6490 )\n",
      "[Epoch: 6540] train loss: 0.2365, train acc: 0.9164, val loss: 0.2067, val acc: 0.9440  (best train acc: 0.9278, best val acc: 0.9484, best train loss: 0.2147  @ epoch 6490 )\n",
      "[Epoch: 6560] train loss: 0.2356, train acc: 0.9192, val loss: 0.1948, val acc: 0.9390  (best train acc: 0.9278, best val acc: 0.9484, best train loss: 0.2147  @ epoch 6490 )\n",
      "[Epoch: 6580] train loss: 0.2524, train acc: 0.9148, val loss: 0.2081, val acc: 0.9346  (best train acc: 0.9286, best val acc: 0.9484, best train loss: 0.2141  @ epoch 6573 )\n",
      "[Epoch: 6600] train loss: 0.2334, train acc: 0.9192, val loss: 0.2054, val acc: 0.9390  (best train acc: 0.9286, best val acc: 0.9484, best train loss: 0.2141  @ epoch 6573 )\n",
      "[Epoch: 6620] train loss: 0.2243, train acc: 0.9247, val loss: 0.2046, val acc: 0.9336  (best train acc: 0.9305, best val acc: 0.9484, best train loss: 0.2118  @ epoch 6602 )\n",
      "[Epoch: 6640] train loss: 0.2375, train acc: 0.9169, val loss: 0.2134, val acc: 0.9282  (best train acc: 0.9305, best val acc: 0.9484, best train loss: 0.2118  @ epoch 6602 )\n",
      "[Epoch: 6660] train loss: 0.2382, train acc: 0.9218, val loss: 0.1975, val acc: 0.9423  (best train acc: 0.9305, best val acc: 0.9484, best train loss: 0.2118  @ epoch 6602 )\n",
      "[Epoch: 6680] train loss: 0.2462, train acc: 0.9154, val loss: 0.1985, val acc: 0.9420  (best train acc: 0.9305, best val acc: 0.9484, best train loss: 0.2118  @ epoch 6602 )\n",
      "[Epoch: 6700] train loss: 0.2296, train acc: 0.9188, val loss: 0.2250, val acc: 0.9268  (best train acc: 0.9305, best val acc: 0.9484, best train loss: 0.2118  @ epoch 6602 )\n",
      "[Epoch: 6720] train loss: 0.2304, train acc: 0.9156, val loss: 0.2029, val acc: 0.9376  (best train acc: 0.9305, best val acc: 0.9484, best train loss: 0.2118  @ epoch 6602 )\n",
      "[Epoch: 6740] train loss: 0.2248, train acc: 0.9237, val loss: 0.2023, val acc: 0.9376  (best train acc: 0.9305, best val acc: 0.9484, best train loss: 0.2118  @ epoch 6602 )\n",
      "[Epoch: 6760] train loss: 0.2679, train acc: 0.9004, val loss: 0.2360, val acc: 0.9349  (best train acc: 0.9305, best val acc: 0.9484, best train loss: 0.2118  @ epoch 6602 )\n",
      "[Epoch: 6780] train loss: 0.2362, train acc: 0.9189, val loss: 0.1943, val acc: 0.9390  (best train acc: 0.9305, best val acc: 0.9484, best train loss: 0.2118  @ epoch 6602 )\n",
      "[Epoch: 6800] train loss: 0.2269, train acc: 0.9194, val loss: 0.1897, val acc: 0.9444  (best train acc: 0.9305, best val acc: 0.9484, best train loss: 0.2118  @ epoch 6602 )\n",
      "[Epoch: 6820] train loss: 0.2176, train acc: 0.9251, val loss: 0.1993, val acc: 0.9487  (best train acc: 0.9305, best val acc: 0.9487, best train loss: 0.2118  @ epoch 6602 )\n",
      "[Epoch: 6840] train loss: 0.2406, train acc: 0.9170, val loss: 0.2155, val acc: 0.9393  (best train acc: 0.9305, best val acc: 0.9487, best train loss: 0.2101  @ epoch 6826 )\n",
      "[Epoch: 6860] train loss: 0.2371, train acc: 0.9203, val loss: 0.2070, val acc: 0.9363  (best train acc: 0.9305, best val acc: 0.9487, best train loss: 0.2101  @ epoch 6826 )\n",
      "[Epoch: 6880] train loss: 0.2361, train acc: 0.9194, val loss: 0.2050, val acc: 0.9349  (best train acc: 0.9305, best val acc: 0.9487, best train loss: 0.2101  @ epoch 6826 )\n",
      "[Epoch: 6900] train loss: 0.2281, train acc: 0.9226, val loss: 0.2093, val acc: 0.9339  (best train acc: 0.9305, best val acc: 0.9487, best train loss: 0.2101  @ epoch 6826 )\n",
      "[Epoch: 6920] train loss: 0.2236, train acc: 0.9237, val loss: 0.2061, val acc: 0.9376  (best train acc: 0.9305, best val acc: 0.9487, best train loss: 0.2101  @ epoch 6826 )\n",
      "[Epoch: 6940] train loss: 0.2056, train acc: 0.9288, val loss: 0.2024, val acc: 0.9420  (best train acc: 0.9305, best val acc: 0.9487, best train loss: 0.2056  @ epoch 6940 )\n",
      "[Epoch: 6960] train loss: 0.2265, train acc: 0.9197, val loss: 0.2004, val acc: 0.9440  (best train acc: 0.9305, best val acc: 0.9487, best train loss: 0.2056  @ epoch 6940 )\n",
      "[Epoch: 6980] train loss: 0.2206, train acc: 0.9203, val loss: 0.1967, val acc: 0.9440  (best train acc: 0.9305, best val acc: 0.9487, best train loss: 0.2056  @ epoch 6940 )\n",
      "[Epoch: 7000] train loss: 0.2391, train acc: 0.9208, val loss: 0.2229, val acc: 0.9332  (best train acc: 0.9305, best val acc: 0.9487, best train loss: 0.2056  @ epoch 6940 )\n",
      "[Epoch: 7020] train loss: 0.2152, train acc: 0.9273, val loss: 0.1873, val acc: 0.9464  (best train acc: 0.9305, best val acc: 0.9487, best train loss: 0.2056  @ epoch 6940 )\n",
      "[Epoch: 7040] train loss: 0.2461, train acc: 0.9180, val loss: 0.1949, val acc: 0.9464  (best train acc: 0.9305, best val acc: 0.9487, best train loss: 0.2056  @ epoch 6940 )\n",
      "[Epoch: 7060] train loss: 0.2231, train acc: 0.9216, val loss: 0.1972, val acc: 0.9390  (best train acc: 0.9305, best val acc: 0.9487, best train loss: 0.2056  @ epoch 6940 )\n",
      "[Epoch: 7080] train loss: 0.2165, train acc: 0.9266, val loss: 0.1976, val acc: 0.9420  (best train acc: 0.9307, best val acc: 0.9487, best train loss: 0.2056  @ epoch 6940 )\n",
      "[Epoch: 7100] train loss: 0.2174, train acc: 0.9255, val loss: 0.2018, val acc: 0.9400  (best train acc: 0.9307, best val acc: 0.9487, best train loss: 0.2056  @ epoch 6940 )\n",
      "[Epoch: 7120] train loss: 0.2151, train acc: 0.9274, val loss: 0.1952, val acc: 0.9460  (best train acc: 0.9307, best val acc: 0.9504, best train loss: 0.2056  @ epoch 6940 )\n",
      "[Epoch: 7140] train loss: 0.2247, train acc: 0.9258, val loss: 0.1927, val acc: 0.9484  (best train acc: 0.9307, best val acc: 0.9504, best train loss: 0.2056  @ epoch 6940 )\n",
      "[Epoch: 7160] train loss: 0.2727, train acc: 0.9054, val loss: 0.2311, val acc: 0.9295  (best train acc: 0.9307, best val acc: 0.9504, best train loss: 0.2056  @ epoch 6940 )\n",
      "[Epoch: 7180] train loss: 0.2771, train acc: 0.9075, val loss: 0.2265, val acc: 0.9352  (best train acc: 0.9307, best val acc: 0.9504, best train loss: 0.2056  @ epoch 6940 )\n",
      "[Epoch: 7200] train loss: 0.2422, train acc: 0.9192, val loss: 0.1950, val acc: 0.9406  (best train acc: 0.9307, best val acc: 0.9504, best train loss: 0.2056  @ epoch 6940 )\n",
      "[Epoch: 7220] train loss: 0.2113, train acc: 0.9283, val loss: 0.1998, val acc: 0.9363  (best train acc: 0.9307, best val acc: 0.9504, best train loss: 0.2056  @ epoch 6940 )\n",
      "[Epoch: 7240] train loss: 0.2192, train acc: 0.9221, val loss: 0.1987, val acc: 0.9430  (best train acc: 0.9307, best val acc: 0.9504, best train loss: 0.2044  @ epoch 7228 )\n",
      "[Epoch: 7260] train loss: 0.2252, train acc: 0.9223, val loss: 0.1995, val acc: 0.9410  (best train acc: 0.9307, best val acc: 0.9504, best train loss: 0.2044  @ epoch 7228 )\n",
      "[Epoch: 7280] train loss: 0.2194, train acc: 0.9238, val loss: 0.1914, val acc: 0.9457  (best train acc: 0.9307, best val acc: 0.9504, best train loss: 0.2044  @ epoch 7228 )\n",
      "[Epoch: 7300] train loss: 0.2094, train acc: 0.9293, val loss: 0.1856, val acc: 0.9467  (best train acc: 0.9307, best val acc: 0.9504, best train loss: 0.2039  @ epoch 7291 )\n",
      "[Epoch: 7320] train loss: 0.2280, train acc: 0.9196, val loss: 0.2081, val acc: 0.9336  (best train acc: 0.9307, best val acc: 0.9504, best train loss: 0.2039  @ epoch 7291 )\n",
      "[Epoch: 7340] train loss: 0.2359, train acc: 0.9171, val loss: 0.2105, val acc: 0.9305  (best train acc: 0.9307, best val acc: 0.9504, best train loss: 0.2039  @ epoch 7291 )\n",
      "[Epoch: 7360] train loss: 0.2158, train acc: 0.9257, val loss: 0.1933, val acc: 0.9390  (best train acc: 0.9307, best val acc: 0.9504, best train loss: 0.2039  @ epoch 7291 )\n",
      "[Epoch: 7380] train loss: 0.2107, train acc: 0.9265, val loss: 0.2066, val acc: 0.9413  (best train acc: 0.9307, best val acc: 0.9504, best train loss: 0.2039  @ epoch 7291 )\n",
      "[Epoch: 7400] train loss: 0.2218, train acc: 0.9213, val loss: 0.1967, val acc: 0.9417  (best train acc: 0.9307, best val acc: 0.9504, best train loss: 0.1980  @ epoch 7394 )\n",
      "[Epoch: 7420] train loss: 0.2345, train acc: 0.9233, val loss: 0.1988, val acc: 0.9430  (best train acc: 0.9307, best val acc: 0.9504, best train loss: 0.1980  @ epoch 7394 )\n",
      "[Epoch: 7440] train loss: 0.2078, train acc: 0.9242, val loss: 0.1848, val acc: 0.9477  (best train acc: 0.9307, best val acc: 0.9504, best train loss: 0.1980  @ epoch 7394 )\n",
      "[Epoch: 7460] train loss: 0.2103, train acc: 0.9272, val loss: 0.1961, val acc: 0.9417  (best train acc: 0.9307, best val acc: 0.9504, best train loss: 0.1980  @ epoch 7394 )\n",
      "[Epoch: 7480] train loss: 0.2244, train acc: 0.9256, val loss: 0.1939, val acc: 0.9400  (best train acc: 0.9307, best val acc: 0.9504, best train loss: 0.1980  @ epoch 7394 )\n",
      "[Epoch: 7500] train loss: 0.2416, train acc: 0.9165, val loss: 0.2036, val acc: 0.9359  (best train acc: 0.9307, best val acc: 0.9504, best train loss: 0.1980  @ epoch 7394 )\n",
      "[Epoch: 7520] train loss: 0.2435, train acc: 0.9186, val loss: 0.1965, val acc: 0.9373  (best train acc: 0.9307, best val acc: 0.9504, best train loss: 0.1980  @ epoch 7394 )\n",
      "[Epoch: 7540] train loss: 0.2229, train acc: 0.9265, val loss: 0.1931, val acc: 0.9460  (best train acc: 0.9307, best val acc: 0.9504, best train loss: 0.1980  @ epoch 7394 )\n",
      "[Epoch: 7560] train loss: 0.2328, train acc: 0.9187, val loss: 0.2112, val acc: 0.9265  (best train acc: 0.9307, best val acc: 0.9504, best train loss: 0.1980  @ epoch 7394 )\n",
      "[Epoch: 7580] train loss: 0.2210, train acc: 0.9231, val loss: 0.1963, val acc: 0.9464  (best train acc: 0.9312, best val acc: 0.9504, best train loss: 0.1980  @ epoch 7394 )\n",
      "[Epoch: 7600] train loss: 0.2138, train acc: 0.9266, val loss: 0.1905, val acc: 0.9427  (best train acc: 0.9312, best val acc: 0.9504, best train loss: 0.1980  @ epoch 7394 )\n",
      "[Epoch: 7620] train loss: 0.2119, train acc: 0.9266, val loss: 0.1938, val acc: 0.9454  (best train acc: 0.9312, best val acc: 0.9504, best train loss: 0.1980  @ epoch 7394 )\n",
      "[Epoch: 7640] train loss: 0.2122, train acc: 0.9286, val loss: 0.1919, val acc: 0.9430  (best train acc: 0.9312, best val acc: 0.9504, best train loss: 0.1980  @ epoch 7394 )\n",
      "[Epoch: 7660] train loss: 0.2321, train acc: 0.9176, val loss: 0.2045, val acc: 0.9410  (best train acc: 0.9312, best val acc: 0.9504, best train loss: 0.1980  @ epoch 7394 )\n",
      "[Epoch: 7680] train loss: 0.2115, train acc: 0.9263, val loss: 0.2020, val acc: 0.9444  (best train acc: 0.9312, best val acc: 0.9504, best train loss: 0.1980  @ epoch 7394 )\n",
      "[Epoch: 7700] train loss: 0.2193, train acc: 0.9249, val loss: 0.2128, val acc: 0.9410  (best train acc: 0.9325, best val acc: 0.9504, best train loss: 0.1980  @ epoch 7394 )\n",
      "[Epoch: 7720] train loss: 0.2153, train acc: 0.9223, val loss: 0.1968, val acc: 0.9427  (best train acc: 0.9325, best val acc: 0.9504, best train loss: 0.1980  @ epoch 7394 )\n",
      "[Epoch: 7740] train loss: 0.2068, train acc: 0.9263, val loss: 0.1986, val acc: 0.9396  (best train acc: 0.9325, best val acc: 0.9504, best train loss: 0.1980  @ epoch 7394 )\n",
      "[Epoch: 7760] train loss: 0.2147, train acc: 0.9240, val loss: 0.1925, val acc: 0.9410  (best train acc: 0.9325, best val acc: 0.9504, best train loss: 0.1980  @ epoch 7394 )\n",
      "[Epoch: 7780] train loss: 0.2285, train acc: 0.9190, val loss: 0.2377, val acc: 0.9339  (best train acc: 0.9328, best val acc: 0.9504, best train loss: 0.1980  @ epoch 7394 )\n",
      "[Epoch: 7800] train loss: 0.2241, train acc: 0.9233, val loss: 0.2096, val acc: 0.9329  (best train acc: 0.9328, best val acc: 0.9504, best train loss: 0.1980  @ epoch 7394 )\n",
      "[Epoch: 7820] train loss: 0.2100, train acc: 0.9305, val loss: 0.1938, val acc: 0.9481  (best train acc: 0.9328, best val acc: 0.9504, best train loss: 0.1980  @ epoch 7394 )\n",
      "[Epoch: 7840] train loss: 0.2129, train acc: 0.9290, val loss: 0.2138, val acc: 0.9356  (best train acc: 0.9328, best val acc: 0.9514, best train loss: 0.1980  @ epoch 7394 )\n",
      "[Epoch: 7860] train loss: 0.2557, train acc: 0.9144, val loss: 0.2092, val acc: 0.9288  (best train acc: 0.9328, best val acc: 0.9514, best train loss: 0.1980  @ epoch 7394 )\n",
      "[Epoch: 7880] train loss: 0.2163, train acc: 0.9255, val loss: 0.1979, val acc: 0.9417  (best train acc: 0.9328, best val acc: 0.9514, best train loss: 0.1980  @ epoch 7394 )\n",
      "[Epoch: 7900] train loss: 0.2089, train acc: 0.9282, val loss: 0.1992, val acc: 0.9386  (best train acc: 0.9328, best val acc: 0.9514, best train loss: 0.1980  @ epoch 7394 )\n",
      "[Epoch: 7920] train loss: 0.2172, train acc: 0.9240, val loss: 0.2156, val acc: 0.9413  (best train acc: 0.9328, best val acc: 0.9514, best train loss: 0.1980  @ epoch 7394 )\n",
      "[Epoch: 7940] train loss: 0.2149, train acc: 0.9268, val loss: 0.2011, val acc: 0.9474  (best train acc: 0.9329, best val acc: 0.9514, best train loss: 0.1980  @ epoch 7394 )\n",
      "[Epoch: 7960] train loss: 0.2138, train acc: 0.9288, val loss: 0.1867, val acc: 0.9477  (best train acc: 0.9329, best val acc: 0.9514, best train loss: 0.1965  @ epoch 7953 )\n",
      "[Epoch: 7980] train loss: 0.2302, train acc: 0.9199, val loss: 0.2107, val acc: 0.9295  (best train acc: 0.9329, best val acc: 0.9514, best train loss: 0.1965  @ epoch 7953 )\n",
      "[Epoch: 8000] train loss: 0.2111, train acc: 0.9273, val loss: 0.2000, val acc: 0.9427  (best train acc: 0.9329, best val acc: 0.9514, best train loss: 0.1965  @ epoch 7953 )\n",
      "[Epoch: 8020] train loss: 0.1958, train acc: 0.9319, val loss: 0.1887, val acc: 0.9470  (best train acc: 0.9329, best val acc: 0.9514, best train loss: 0.1958  @ epoch 8020 )\n",
      "[Epoch: 8040] train loss: 0.2243, train acc: 0.9196, val loss: 0.2062, val acc: 0.9363  (best train acc: 0.9329, best val acc: 0.9514, best train loss: 0.1958  @ epoch 8020 )\n",
      "[Epoch: 8060] train loss: 0.2151, train acc: 0.9260, val loss: 0.1944, val acc: 0.9508  (best train acc: 0.9329, best val acc: 0.9514, best train loss: 0.1958  @ epoch 8020 )\n",
      "[Epoch: 8080] train loss: 0.2132, train acc: 0.9299, val loss: 0.1948, val acc: 0.9433  (best train acc: 0.9329, best val acc: 0.9514, best train loss: 0.1958  @ epoch 8020 )\n",
      "[Epoch: 8100] train loss: 0.2264, train acc: 0.9191, val loss: 0.2097, val acc: 0.9420  (best train acc: 0.9336, best val acc: 0.9514, best train loss: 0.1944  @ epoch 8089 )\n",
      "[Epoch: 8120] train loss: 0.2376, train acc: 0.9147, val loss: 0.1882, val acc: 0.9406  (best train acc: 0.9336, best val acc: 0.9514, best train loss: 0.1944  @ epoch 8089 )\n",
      "[Epoch: 8140] train loss: 0.2212, train acc: 0.9242, val loss: 0.1906, val acc: 0.9440  (best train acc: 0.9336, best val acc: 0.9514, best train loss: 0.1944  @ epoch 8089 )\n",
      "[Epoch: 8160] train loss: 0.2072, train acc: 0.9284, val loss: 0.1973, val acc: 0.9433  (best train acc: 0.9336, best val acc: 0.9514, best train loss: 0.1944  @ epoch 8089 )\n",
      "[Epoch: 8180] train loss: 0.2264, train acc: 0.9198, val loss: 0.2150, val acc: 0.9393  (best train acc: 0.9336, best val acc: 0.9514, best train loss: 0.1944  @ epoch 8089 )\n",
      "[Epoch: 8200] train loss: 0.2237, train acc: 0.9250, val loss: 0.2029, val acc: 0.9440  (best train acc: 0.9336, best val acc: 0.9514, best train loss: 0.1944  @ epoch 8089 )\n",
      "[Epoch: 8220] train loss: 0.2035, train acc: 0.9328, val loss: 0.2004, val acc: 0.9444  (best train acc: 0.9336, best val acc: 0.9514, best train loss: 0.1944  @ epoch 8089 )\n",
      "[Epoch: 8240] train loss: 0.2092, train acc: 0.9316, val loss: 0.1865, val acc: 0.9477  (best train acc: 0.9336, best val acc: 0.9514, best train loss: 0.1912  @ epoch 8238 )\n",
      "[Epoch: 8260] train loss: 0.2168, train acc: 0.9254, val loss: 0.2021, val acc: 0.9369  (best train acc: 0.9336, best val acc: 0.9514, best train loss: 0.1912  @ epoch 8238 )\n",
      "[Epoch: 8280] train loss: 0.2195, train acc: 0.9218, val loss: 0.2073, val acc: 0.9406  (best train acc: 0.9336, best val acc: 0.9514, best train loss: 0.1912  @ epoch 8238 )\n",
      "[Epoch: 8300] train loss: 0.2176, train acc: 0.9256, val loss: 0.1999, val acc: 0.9400  (best train acc: 0.9336, best val acc: 0.9514, best train loss: 0.1912  @ epoch 8238 )\n",
      "[Epoch: 8320] train loss: 0.2178, train acc: 0.9216, val loss: 0.1901, val acc: 0.9467  (best train acc: 0.9336, best val acc: 0.9514, best train loss: 0.1912  @ epoch 8238 )\n",
      "[Epoch: 8340] train loss: 0.2109, train acc: 0.9285, val loss: 0.1939, val acc: 0.9470  (best train acc: 0.9336, best val acc: 0.9514, best train loss: 0.1912  @ epoch 8238 )\n",
      "[Epoch: 8360] train loss: 0.2105, train acc: 0.9273, val loss: 0.1913, val acc: 0.9474  (best train acc: 0.9336, best val acc: 0.9514, best train loss: 0.1912  @ epoch 8238 )\n",
      "[Epoch: 8380] train loss: 0.2381, train acc: 0.9153, val loss: 0.2150, val acc: 0.9336  (best train acc: 0.9336, best val acc: 0.9514, best train loss: 0.1912  @ epoch 8238 )\n",
      "[Epoch: 8400] train loss: 0.2022, train acc: 0.9291, val loss: 0.2022, val acc: 0.9427  (best train acc: 0.9336, best val acc: 0.9514, best train loss: 0.1912  @ epoch 8238 )\n",
      "[Epoch: 8420] train loss: 0.2245, train acc: 0.9202, val loss: 0.1887, val acc: 0.9420  (best train acc: 0.9336, best val acc: 0.9514, best train loss: 0.1912  @ epoch 8238 )\n",
      "[Epoch: 8440] train loss: 0.2289, train acc: 0.9190, val loss: 0.2104, val acc: 0.9319  (best train acc: 0.9336, best val acc: 0.9514, best train loss: 0.1912  @ epoch 8238 )\n",
      "[Epoch: 8460] train loss: 0.1995, train acc: 0.9308, val loss: 0.1860, val acc: 0.9484  (best train acc: 0.9336, best val acc: 0.9514, best train loss: 0.1912  @ epoch 8238 )\n",
      "[Epoch: 8480] train loss: 0.2120, train acc: 0.9268, val loss: 0.1848, val acc: 0.9464  (best train acc: 0.9336, best val acc: 0.9514, best train loss: 0.1912  @ epoch 8238 )\n",
      "[Epoch: 8500] train loss: 0.2190, train acc: 0.9236, val loss: 0.1877, val acc: 0.9444  (best train acc: 0.9336, best val acc: 0.9514, best train loss: 0.1912  @ epoch 8238 )\n",
      "[Epoch: 8520] train loss: 0.1953, train acc: 0.9281, val loss: 0.1919, val acc: 0.9413  (best train acc: 0.9336, best val acc: 0.9514, best train loss: 0.1912  @ epoch 8238 )\n",
      "[Epoch: 8540] train loss: 0.2071, train acc: 0.9273, val loss: 0.2141, val acc: 0.9352  (best train acc: 0.9336, best val acc: 0.9514, best train loss: 0.1912  @ epoch 8238 )\n",
      "[Epoch: 8560] train loss: 0.2036, train acc: 0.9292, val loss: 0.1854, val acc: 0.9464  (best train acc: 0.9336, best val acc: 0.9514, best train loss: 0.1912  @ epoch 8238 )\n",
      "[Epoch: 8580] train loss: 0.2179, train acc: 0.9289, val loss: 0.1911, val acc: 0.9481  (best train acc: 0.9337, best val acc: 0.9521, best train loss: 0.1912  @ epoch 8238 )\n",
      "[Epoch: 8600] train loss: 0.2320, train acc: 0.9179, val loss: 0.2650, val acc: 0.9083  (best train acc: 0.9337, best val acc: 0.9521, best train loss: 0.1912  @ epoch 8238 )\n",
      "[Epoch: 8620] train loss: 0.2564, train acc: 0.9121, val loss: 0.2419, val acc: 0.9393  (best train acc: 0.9337, best val acc: 0.9521, best train loss: 0.1912  @ epoch 8238 )\n",
      "[Epoch: 8640] train loss: 0.2184, train acc: 0.9241, val loss: 0.1817, val acc: 0.9464  (best train acc: 0.9337, best val acc: 0.9521, best train loss: 0.1912  @ epoch 8238 )\n",
      "[Epoch: 8660] train loss: 0.2118, train acc: 0.9270, val loss: 0.1844, val acc: 0.9467  (best train acc: 0.9337, best val acc: 0.9521, best train loss: 0.1912  @ epoch 8238 )\n",
      "[Epoch: 8680] train loss: 0.2018, train acc: 0.9261, val loss: 0.1979, val acc: 0.9332  (best train acc: 0.9337, best val acc: 0.9521, best train loss: 0.1912  @ epoch 8238 )\n",
      "[Epoch: 8700] train loss: 0.2034, train acc: 0.9284, val loss: 0.1880, val acc: 0.9487  (best train acc: 0.9337, best val acc: 0.9521, best train loss: 0.1912  @ epoch 8238 )\n",
      "[Epoch: 8720] train loss: 0.1912, train acc: 0.9352, val loss: 0.1810, val acc: 0.9470  (best train acc: 0.9352, best val acc: 0.9521, best train loss: 0.1912  @ epoch 8720 )\n",
      "[Epoch: 8740] train loss: 0.2102, train acc: 0.9290, val loss: 0.1821, val acc: 0.9440  (best train acc: 0.9352, best val acc: 0.9521, best train loss: 0.1912  @ epoch 8720 )\n",
      "[Epoch: 8760] train loss: 0.2013, train acc: 0.9289, val loss: 0.1789, val acc: 0.9514  (best train acc: 0.9352, best val acc: 0.9521, best train loss: 0.1912  @ epoch 8720 )\n",
      "[Epoch: 8780] train loss: 0.2112, train acc: 0.9263, val loss: 0.2088, val acc: 0.9282  (best train acc: 0.9352, best val acc: 0.9521, best train loss: 0.1912  @ epoch 8720 )\n",
      "[Epoch: 8800] train loss: 0.2053, train acc: 0.9310, val loss: 0.1765, val acc: 0.9497  (best train acc: 0.9352, best val acc: 0.9521, best train loss: 0.1912  @ epoch 8720 )\n",
      "[Epoch: 8820] train loss: 0.2077, train acc: 0.9288, val loss: 0.1760, val acc: 0.9457  (best train acc: 0.9352, best val acc: 0.9521, best train loss: 0.1912  @ epoch 8720 )\n",
      "[Epoch: 8840] train loss: 0.2209, train acc: 0.9207, val loss: 0.1783, val acc: 0.9427  (best train acc: 0.9352, best val acc: 0.9521, best train loss: 0.1912  @ epoch 8720 )\n",
      "[Epoch: 8860] train loss: 0.2027, train acc: 0.9268, val loss: 0.1899, val acc: 0.9369  (best train acc: 0.9352, best val acc: 0.9521, best train loss: 0.1911  @ epoch 8850 )\n",
      "[Epoch: 8880] train loss: 0.2119, train acc: 0.9279, val loss: 0.1844, val acc: 0.9450  (best train acc: 0.9352, best val acc: 0.9521, best train loss: 0.1879  @ epoch 8874 )\n",
      "[Epoch: 8900] train loss: 0.2176, train acc: 0.9237, val loss: 0.1783, val acc: 0.9467  (best train acc: 0.9352, best val acc: 0.9521, best train loss: 0.1879  @ epoch 8874 )\n",
      "[Epoch: 8920] train loss: 0.2185, train acc: 0.9212, val loss: 0.1873, val acc: 0.9386  (best train acc: 0.9352, best val acc: 0.9521, best train loss: 0.1879  @ epoch 8874 )\n",
      "[Epoch: 8940] train loss: 0.2132, train acc: 0.9269, val loss: 0.1835, val acc: 0.9427  (best train acc: 0.9352, best val acc: 0.9521, best train loss: 0.1879  @ epoch 8874 )\n",
      "[Epoch: 8960] train loss: 0.2021, train acc: 0.9254, val loss: 0.1976, val acc: 0.9251  (best train acc: 0.9352, best val acc: 0.9521, best train loss: 0.1879  @ epoch 8874 )\n",
      "[Epoch: 8980] train loss: 0.2289, train acc: 0.9195, val loss: 0.1803, val acc: 0.9467  (best train acc: 0.9352, best val acc: 0.9521, best train loss: 0.1879  @ epoch 8874 )\n",
      "[Epoch: 9000] train loss: 0.2002, train acc: 0.9291, val loss: 0.1755, val acc: 0.9454  (best train acc: 0.9352, best val acc: 0.9521, best train loss: 0.1879  @ epoch 8874 )\n",
      "[Epoch: 9020] train loss: 0.2172, train acc: 0.9216, val loss: 0.1810, val acc: 0.9477  (best train acc: 0.9352, best val acc: 0.9521, best train loss: 0.1879  @ epoch 8874 )\n",
      "[Epoch: 9040] train loss: 0.2125, train acc: 0.9253, val loss: 0.1840, val acc: 0.9352  (best train acc: 0.9352, best val acc: 0.9521, best train loss: 0.1879  @ epoch 8874 )\n",
      "[Epoch: 9060] train loss: 0.2060, train acc: 0.9296, val loss: 0.1867, val acc: 0.9444  (best train acc: 0.9352, best val acc: 0.9521, best train loss: 0.1879  @ epoch 8874 )\n",
      "[Epoch: 9080] train loss: 0.2016, train acc: 0.9310, val loss: 0.1709, val acc: 0.9474  (best train acc: 0.9352, best val acc: 0.9521, best train loss: 0.1879  @ epoch 8874 )\n",
      "[Epoch: 9100] train loss: 0.2071, train acc: 0.9298, val loss: 0.1722, val acc: 0.9470  (best train acc: 0.9352, best val acc: 0.9521, best train loss: 0.1879  @ epoch 8874 )\n",
      "[Epoch: 9120] train loss: 0.2034, train acc: 0.9309, val loss: 0.1775, val acc: 0.9467  (best train acc: 0.9352, best val acc: 0.9521, best train loss: 0.1879  @ epoch 8874 )\n",
      "[Epoch: 9140] train loss: 0.2115, train acc: 0.9252, val loss: 0.1791, val acc: 0.9349  (best train acc: 0.9352, best val acc: 0.9521, best train loss: 0.1870  @ epoch 9124 )\n",
      "[Epoch: 9160] train loss: 0.2014, train acc: 0.9283, val loss: 0.1838, val acc: 0.9403  (best train acc: 0.9352, best val acc: 0.9521, best train loss: 0.1870  @ epoch 9124 )\n",
      "[Epoch: 9180] train loss: 0.1993, train acc: 0.9291, val loss: 0.1700, val acc: 0.9440  (best train acc: 0.9352, best val acc: 0.9521, best train loss: 0.1870  @ epoch 9124 )\n",
      "[Epoch: 9200] train loss: 0.2444, train acc: 0.9091, val loss: 0.1933, val acc: 0.9204  (best train acc: 0.9365, best val acc: 0.9521, best train loss: 0.1870  @ epoch 9124 )\n",
      "[Epoch: 9220] train loss: 0.2306, train acc: 0.9147, val loss: 0.1922, val acc: 0.9396  (best train acc: 0.9365, best val acc: 0.9521, best train loss: 0.1870  @ epoch 9124 )\n",
      "[Epoch: 9240] train loss: 0.2208, train acc: 0.9273, val loss: 0.1756, val acc: 0.9477  (best train acc: 0.9365, best val acc: 0.9521, best train loss: 0.1870  @ epoch 9124 )\n",
      "[Epoch: 9260] train loss: 0.2009, train acc: 0.9318, val loss: 0.1664, val acc: 0.9464  (best train acc: 0.9365, best val acc: 0.9521, best train loss: 0.1870  @ epoch 9124 )\n",
      "[Epoch: 9280] train loss: 0.2078, train acc: 0.9278, val loss: 0.1657, val acc: 0.9444  (best train acc: 0.9365, best val acc: 0.9521, best train loss: 0.1870  @ epoch 9124 )\n",
      "[Epoch: 9300] train loss: 0.1958, train acc: 0.9297, val loss: 0.1672, val acc: 0.9497  (best train acc: 0.9365, best val acc: 0.9521, best train loss: 0.1870  @ epoch 9124 )\n",
      "[Epoch: 9320] train loss: 0.2217, train acc: 0.9223, val loss: 0.1795, val acc: 0.9420  (best train acc: 0.9365, best val acc: 0.9521, best train loss: 0.1870  @ epoch 9124 )\n",
      "[Epoch: 9340] train loss: 0.2019, train acc: 0.9271, val loss: 0.1867, val acc: 0.9282  (best train acc: 0.9368, best val acc: 0.9521, best train loss: 0.1870  @ epoch 9124 )\n",
      "[Epoch: 9360] train loss: 0.2009, train acc: 0.9322, val loss: 0.1607, val acc: 0.9454  (best train acc: 0.9368, best val acc: 0.9521, best train loss: 0.1870  @ epoch 9124 )\n",
      "[Epoch: 9380] train loss: 0.2099, train acc: 0.9255, val loss: 0.1745, val acc: 0.9457  (best train acc: 0.9368, best val acc: 0.9521, best train loss: 0.1870  @ epoch 9124 )\n",
      "[Epoch: 9400] train loss: 0.2050, train acc: 0.9302, val loss: 0.1665, val acc: 0.9444  (best train acc: 0.9368, best val acc: 0.9521, best train loss: 0.1870  @ epoch 9124 )\n",
      "[Epoch: 9420] train loss: 0.2023, train acc: 0.9285, val loss: 0.1602, val acc: 0.9423  (best train acc: 0.9368, best val acc: 0.9521, best train loss: 0.1870  @ epoch 9124 )\n",
      "[Epoch: 9440] train loss: 0.1860, train acc: 0.9341, val loss: 0.1619, val acc: 0.9477  (best train acc: 0.9368, best val acc: 0.9535, best train loss: 0.1859  @ epoch 9434 )\n",
      "[Epoch: 9460] train loss: 0.2117, train acc: 0.9303, val loss: 0.1940, val acc: 0.9373  (best train acc: 0.9368, best val acc: 0.9535, best train loss: 0.1856  @ epoch 9444 )\n",
      "[Epoch: 9480] train loss: 0.1924, train acc: 0.9325, val loss: 0.1618, val acc: 0.9430  (best train acc: 0.9368, best val acc: 0.9535, best train loss: 0.1856  @ epoch 9444 )\n",
      "[Epoch: 9500] train loss: 0.1889, train acc: 0.9357, val loss: 0.1575, val acc: 0.9501  (best train acc: 0.9368, best val acc: 0.9535, best train loss: 0.1848  @ epoch 9483 )\n",
      "[Epoch: 9520] train loss: 0.2333, train acc: 0.9206, val loss: 0.1890, val acc: 0.9261  (best train acc: 0.9368, best val acc: 0.9535, best train loss: 0.1848  @ epoch 9483 )\n",
      "[Epoch: 9540] train loss: 0.2084, train acc: 0.9241, val loss: 0.1664, val acc: 0.9477  (best train acc: 0.9368, best val acc: 0.9535, best train loss: 0.1848  @ epoch 9483 )\n",
      "[Epoch: 9560] train loss: 0.1962, train acc: 0.9333, val loss: 0.1593, val acc: 0.9454  (best train acc: 0.9368, best val acc: 0.9535, best train loss: 0.1848  @ epoch 9483 )\n",
      "[Epoch: 9580] train loss: 0.1945, train acc: 0.9318, val loss: 0.1719, val acc: 0.9444  (best train acc: 0.9368, best val acc: 0.9535, best train loss: 0.1845  @ epoch 9568 )\n",
      "[Epoch: 9600] train loss: 0.2056, train acc: 0.9297, val loss: 0.1614, val acc: 0.9430  (best train acc: 0.9368, best val acc: 0.9535, best train loss: 0.1845  @ epoch 9568 )\n",
      "[Epoch: 9620] train loss: 0.2025, train acc: 0.9283, val loss: 0.1670, val acc: 0.9390  (best train acc: 0.9368, best val acc: 0.9535, best train loss: 0.1845  @ epoch 9568 )\n",
      "[Epoch: 9640] train loss: 0.2600, train acc: 0.8939, val loss: 0.2158, val acc: 0.9019  (best train acc: 0.9368, best val acc: 0.9535, best train loss: 0.1845  @ epoch 9568 )\n",
      "[Epoch: 9660] train loss: 0.2065, train acc: 0.9286, val loss: 0.1786, val acc: 0.9386  (best train acc: 0.9368, best val acc: 0.9535, best train loss: 0.1845  @ epoch 9568 )\n",
      "[Epoch: 9680] train loss: 0.1995, train acc: 0.9288, val loss: 0.1641, val acc: 0.9467  (best train acc: 0.9368, best val acc: 0.9535, best train loss: 0.1845  @ epoch 9568 )\n",
      "[Epoch: 9700] train loss: 0.1965, train acc: 0.9331, val loss: 0.1614, val acc: 0.9511  (best train acc: 0.9368, best val acc: 0.9535, best train loss: 0.1845  @ epoch 9568 )\n",
      "[Epoch: 9720] train loss: 0.1928, train acc: 0.9312, val loss: 0.1630, val acc: 0.9403  (best train acc: 0.9368, best val acc: 0.9535, best train loss: 0.1845  @ epoch 9568 )\n",
      "[Epoch: 9740] train loss: 0.1978, train acc: 0.9310, val loss: 0.1792, val acc: 0.9423  (best train acc: 0.9368, best val acc: 0.9535, best train loss: 0.1845  @ epoch 9568 )\n",
      "[Epoch: 9760] train loss: 0.2078, train acc: 0.9296, val loss: 0.1748, val acc: 0.9346  (best train acc: 0.9368, best val acc: 0.9535, best train loss: 0.1825  @ epoch 9756 )\n",
      "[Epoch: 9780] train loss: 0.2022, train acc: 0.9268, val loss: 0.1645, val acc: 0.9417  (best train acc: 0.9368, best val acc: 0.9535, best train loss: 0.1825  @ epoch 9756 )\n",
      "[Epoch: 9800] train loss: 0.2043, train acc: 0.9301, val loss: 0.1712, val acc: 0.9386  (best train acc: 0.9368, best val acc: 0.9535, best train loss: 0.1825  @ epoch 9756 )\n",
      "[Epoch: 9820] train loss: 0.1968, train acc: 0.9315, val loss: 0.1610, val acc: 0.9454  (best train acc: 0.9383, best val acc: 0.9535, best train loss: 0.1766  @ epoch 9816 )\n",
      "[Epoch: 9840] train loss: 0.1898, train acc: 0.9363, val loss: 0.1562, val acc: 0.9511  (best train acc: 0.9383, best val acc: 0.9535, best train loss: 0.1766  @ epoch 9816 )\n",
      "[Epoch: 9860] train loss: 0.1945, train acc: 0.9318, val loss: 0.1623, val acc: 0.9457  (best train acc: 0.9383, best val acc: 0.9535, best train loss: 0.1766  @ epoch 9816 )\n",
      "[Epoch: 9880] train loss: 0.1973, train acc: 0.9260, val loss: 0.1575, val acc: 0.9450  (best train acc: 0.9383, best val acc: 0.9535, best train loss: 0.1766  @ epoch 9816 )\n",
      "[Epoch: 9900] train loss: 0.2060, train acc: 0.9273, val loss: 0.1646, val acc: 0.9390  (best train acc: 0.9383, best val acc: 0.9535, best train loss: 0.1766  @ epoch 9816 )\n",
      "[Epoch: 9920] train loss: 0.1985, train acc: 0.9261, val loss: 0.1604, val acc: 0.9477  (best train acc: 0.9383, best val acc: 0.9535, best train loss: 0.1766  @ epoch 9816 )\n",
      "[Epoch: 9940] train loss: 0.2009, train acc: 0.9287, val loss: 0.1656, val acc: 0.9474  (best train acc: 0.9383, best val acc: 0.9535, best train loss: 0.1766  @ epoch 9816 )\n",
      "[Epoch: 9960] train loss: 0.2043, train acc: 0.9266, val loss: 0.1647, val acc: 0.9450  (best train acc: 0.9383, best val acc: 0.9535, best train loss: 0.1766  @ epoch 9816 )\n",
      "[Epoch: 9980] train loss: 0.1887, train acc: 0.9336, val loss: 0.1635, val acc: 0.9454  (best train acc: 0.9383, best val acc: 0.9535, best train loss: 0.1766  @ epoch 9816 )\n",
      "[Epoch: 10000] train loss: 0.1987, train acc: 0.9311, val loss: 0.1619, val acc: 0.9420  (best train acc: 0.9391, best val acc: 0.9535, best train loss: 0.1766  @ epoch 9816 )\n",
      "[Epoch: 10020] train loss: 0.1816, train acc: 0.9351, val loss: 0.1599, val acc: 0.9484  (best train acc: 0.9391, best val acc: 0.9535, best train loss: 0.1766  @ epoch 9816 )\n",
      "[Epoch: 10040] train loss: 0.2075, train acc: 0.9252, val loss: 0.1569, val acc: 0.9477  (best train acc: 0.9391, best val acc: 0.9535, best train loss: 0.1766  @ epoch 9816 )\n",
      "[Epoch: 10060] train loss: 0.1920, train acc: 0.9312, val loss: 0.1510, val acc: 0.9474  (best train acc: 0.9391, best val acc: 0.9541, best train loss: 0.1766  @ epoch 9816 )\n",
      "[Epoch: 10080] train loss: 0.1903, train acc: 0.9321, val loss: 0.1642, val acc: 0.9467  (best train acc: 0.9391, best val acc: 0.9541, best train loss: 0.1766  @ epoch 9816 )\n",
      "[Epoch: 10100] train loss: 0.1891, train acc: 0.9348, val loss: 0.1598, val acc: 0.9477  (best train acc: 0.9394, best val acc: 0.9541, best train loss: 0.1766  @ epoch 9816 )\n",
      "[Epoch: 10120] train loss: 0.1995, train acc: 0.9320, val loss: 0.1662, val acc: 0.9390  (best train acc: 0.9394, best val acc: 0.9541, best train loss: 0.1766  @ epoch 9816 )\n",
      "[Epoch: 10140] train loss: 0.1993, train acc: 0.9286, val loss: 0.1625, val acc: 0.9423  (best train acc: 0.9394, best val acc: 0.9541, best train loss: 0.1738  @ epoch 10122 )\n",
      "[Epoch: 10160] train loss: 0.1881, train acc: 0.9367, val loss: 0.1605, val acc: 0.9484  (best train acc: 0.9394, best val acc: 0.9541, best train loss: 0.1738  @ epoch 10122 )\n",
      "[Epoch: 10180] train loss: 0.1930, train acc: 0.9318, val loss: 0.1564, val acc: 0.9481  (best train acc: 0.9394, best val acc: 0.9541, best train loss: 0.1738  @ epoch 10122 )\n",
      "[Epoch: 10200] train loss: 0.1939, train acc: 0.9308, val loss: 0.1621, val acc: 0.9433  (best train acc: 0.9394, best val acc: 0.9541, best train loss: 0.1738  @ epoch 10122 )\n",
      "[Epoch: 10220] train loss: 0.1789, train acc: 0.9380, val loss: 0.1540, val acc: 0.9481  (best train acc: 0.9394, best val acc: 0.9541, best train loss: 0.1738  @ epoch 10122 )\n",
      "[Epoch: 10240] train loss: 0.1770, train acc: 0.9390, val loss: 0.1602, val acc: 0.9487  (best train acc: 0.9394, best val acc: 0.9541, best train loss: 0.1738  @ epoch 10122 )\n",
      "[Epoch: 10260] train loss: 0.1806, train acc: 0.9367, val loss: 0.1570, val acc: 0.9477  (best train acc: 0.9395, best val acc: 0.9541, best train loss: 0.1738  @ epoch 10122 )\n",
      "[Epoch: 10280] train loss: 0.1929, train acc: 0.9331, val loss: 0.1561, val acc: 0.9487  (best train acc: 0.9395, best val acc: 0.9541, best train loss: 0.1738  @ epoch 10122 )\n",
      "[Epoch: 10300] train loss: 0.1981, train acc: 0.9287, val loss: 0.1730, val acc: 0.9440  (best train acc: 0.9428, best val acc: 0.9541, best train loss: 0.1710  @ epoch 10288 )\n",
      "[Epoch: 10320] train loss: 0.1852, train acc: 0.9345, val loss: 0.1668, val acc: 0.9440  (best train acc: 0.9428, best val acc: 0.9541, best train loss: 0.1710  @ epoch 10288 )\n",
      "[Epoch: 10340] train loss: 0.1860, train acc: 0.9366, val loss: 0.1645, val acc: 0.9437  (best train acc: 0.9428, best val acc: 0.9541, best train loss: 0.1710  @ epoch 10288 )\n",
      "[Epoch: 10360] train loss: 0.1930, train acc: 0.9325, val loss: 0.1545, val acc: 0.9497  (best train acc: 0.9428, best val acc: 0.9541, best train loss: 0.1710  @ epoch 10288 )\n",
      "[Epoch: 10380] train loss: 0.1816, train acc: 0.9362, val loss: 0.1558, val acc: 0.9481  (best train acc: 0.9428, best val acc: 0.9541, best train loss: 0.1710  @ epoch 10288 )\n",
      "[Epoch: 10400] train loss: 0.1951, train acc: 0.9307, val loss: 0.1684, val acc: 0.9430  (best train acc: 0.9428, best val acc: 0.9541, best train loss: 0.1710  @ epoch 10288 )\n",
      "[Epoch: 10420] train loss: 0.2000, train acc: 0.9279, val loss: 0.1658, val acc: 0.9373  (best train acc: 0.9428, best val acc: 0.9541, best train loss: 0.1710  @ epoch 10288 )\n",
      "[Epoch: 10440] train loss: 0.1888, train acc: 0.9338, val loss: 0.1787, val acc: 0.9298  (best train acc: 0.9428, best val acc: 0.9541, best train loss: 0.1710  @ epoch 10288 )\n",
      "[Epoch: 10460] train loss: 0.1971, train acc: 0.9354, val loss: 0.1562, val acc: 0.9504  (best train acc: 0.9428, best val acc: 0.9541, best train loss: 0.1710  @ epoch 10288 )\n",
      "[Epoch: 10480] train loss: 0.2031, train acc: 0.9277, val loss: 0.1685, val acc: 0.9481  (best train acc: 0.9428, best val acc: 0.9541, best train loss: 0.1710  @ epoch 10288 )\n",
      "[Epoch: 10500] train loss: 0.1895, train acc: 0.9349, val loss: 0.1541, val acc: 0.9508  (best train acc: 0.9428, best val acc: 0.9541, best train loss: 0.1710  @ epoch 10288 )\n",
      "[Epoch: 10520] train loss: 0.1837, train acc: 0.9345, val loss: 0.1593, val acc: 0.9474  (best train acc: 0.9428, best val acc: 0.9541, best train loss: 0.1710  @ epoch 10288 )\n",
      "[Epoch: 10540] train loss: 0.1826, train acc: 0.9348, val loss: 0.1528, val acc: 0.9494  (best train acc: 0.9428, best val acc: 0.9541, best train loss: 0.1710  @ epoch 10288 )\n",
      "[Epoch: 10560] train loss: 0.1890, train acc: 0.9352, val loss: 0.1575, val acc: 0.9504  (best train acc: 0.9428, best val acc: 0.9541, best train loss: 0.1710  @ epoch 10288 )\n",
      "[Epoch: 10580] train loss: 0.1771, train acc: 0.9363, val loss: 0.1512, val acc: 0.9481  (best train acc: 0.9428, best val acc: 0.9541, best train loss: 0.1710  @ epoch 10288 )\n",
      "[Epoch: 10600] train loss: 0.1966, train acc: 0.9379, val loss: 0.1555, val acc: 0.9484  (best train acc: 0.9428, best val acc: 0.9541, best train loss: 0.1693  @ epoch 10595 )\n",
      "[Epoch: 10620] train loss: 0.1823, train acc: 0.9346, val loss: 0.1530, val acc: 0.9504  (best train acc: 0.9428, best val acc: 0.9541, best train loss: 0.1693  @ epoch 10595 )\n",
      "[Epoch: 10640] train loss: 0.1779, train acc: 0.9369, val loss: 0.1501, val acc: 0.9528  (best train acc: 0.9428, best val acc: 0.9541, best train loss: 0.1693  @ epoch 10595 )\n",
      "[Epoch: 10660] train loss: 0.1664, train acc: 0.9412, val loss: 0.1558, val acc: 0.9477  (best train acc: 0.9428, best val acc: 0.9545, best train loss: 0.1664  @ epoch 10660 )\n",
      "[Epoch: 10680] train loss: 0.1973, train acc: 0.9265, val loss: 0.1595, val acc: 0.9491  (best train acc: 0.9428, best val acc: 0.9545, best train loss: 0.1664  @ epoch 10660 )\n",
      "[Epoch: 10700] train loss: 0.2545, train acc: 0.9015, val loss: 0.2021, val acc: 0.9288  (best train acc: 0.9428, best val acc: 0.9545, best train loss: 0.1664  @ epoch 10660 )\n",
      "[Epoch: 10720] train loss: 0.1972, train acc: 0.9358, val loss: 0.1725, val acc: 0.9494  (best train acc: 0.9428, best val acc: 0.9545, best train loss: 0.1664  @ epoch 10660 )\n",
      "[Epoch: 10740] train loss: 0.1877, train acc: 0.9346, val loss: 0.1540, val acc: 0.9494  (best train acc: 0.9428, best val acc: 0.9545, best train loss: 0.1664  @ epoch 10660 )\n",
      "[Epoch: 10760] train loss: 0.1818, train acc: 0.9392, val loss: 0.1665, val acc: 0.9457  (best train acc: 0.9428, best val acc: 0.9545, best train loss: 0.1664  @ epoch 10660 )\n",
      "[Epoch: 10780] train loss: 0.1767, train acc: 0.9390, val loss: 0.1533, val acc: 0.9518  (best train acc: 0.9428, best val acc: 0.9545, best train loss: 0.1664  @ epoch 10660 )\n",
      "[Epoch: 10800] train loss: 0.1868, train acc: 0.9349, val loss: 0.1756, val acc: 0.9379  (best train acc: 0.9428, best val acc: 0.9545, best train loss: 0.1664  @ epoch 10660 )\n",
      "[Epoch: 10820] train loss: 0.1966, train acc: 0.9282, val loss: 0.1773, val acc: 0.9366  (best train acc: 0.9428, best val acc: 0.9545, best train loss: 0.1664  @ epoch 10660 )\n",
      "[Epoch: 10840] train loss: 0.1805, train acc: 0.9361, val loss: 0.1640, val acc: 0.9467  (best train acc: 0.9428, best val acc: 0.9545, best train loss: 0.1664  @ epoch 10660 )\n",
      "[Epoch: 10860] train loss: 0.1822, train acc: 0.9341, val loss: 0.1784, val acc: 0.9363  (best train acc: 0.9428, best val acc: 0.9545, best train loss: 0.1664  @ epoch 10660 )\n",
      "[Epoch: 10880] train loss: 0.1785, train acc: 0.9391, val loss: 0.1823, val acc: 0.9369  (best train acc: 0.9428, best val acc: 0.9545, best train loss: 0.1664  @ epoch 10660 )\n",
      "[Epoch: 10900] train loss: 0.1846, train acc: 0.9370, val loss: 0.1608, val acc: 0.9501  (best train acc: 0.9428, best val acc: 0.9545, best train loss: 0.1664  @ epoch 10660 )\n",
      "[Epoch: 10920] train loss: 0.1809, train acc: 0.9385, val loss: 0.1621, val acc: 0.9521  (best train acc: 0.9435, best val acc: 0.9545, best train loss: 0.1664  @ epoch 10660 )\n",
      "[Epoch: 10940] train loss: 0.1661, train acc: 0.9403, val loss: 0.1551, val acc: 0.9497  (best train acc: 0.9435, best val acc: 0.9545, best train loss: 0.1661  @ epoch 10940 )\n",
      "[Epoch: 10960] train loss: 0.1822, train acc: 0.9352, val loss: 0.1562, val acc: 0.9460  (best train acc: 0.9443, best val acc: 0.9545, best train loss: 0.1661  @ epoch 10940 )\n",
      "[Epoch: 10980] train loss: 0.1744, train acc: 0.9357, val loss: 0.1610, val acc: 0.9474  (best train acc: 0.9443, best val acc: 0.9551, best train loss: 0.1661  @ epoch 10940 )\n",
      "[Epoch: 11000] train loss: 0.1899, train acc: 0.9303, val loss: 0.1532, val acc: 0.9487  (best train acc: 0.9443, best val acc: 0.9551, best train loss: 0.1661  @ epoch 10940 )\n",
      "[Epoch: 11020] train loss: 0.1764, train acc: 0.9393, val loss: 0.1655, val acc: 0.9444  (best train acc: 0.9443, best val acc: 0.9551, best train loss: 0.1661  @ epoch 10940 )\n",
      "[Epoch: 11040] train loss: 0.1761, train acc: 0.9388, val loss: 0.1560, val acc: 0.9470  (best train acc: 0.9443, best val acc: 0.9555, best train loss: 0.1660  @ epoch 11028 )\n",
      "[Epoch: 11060] train loss: 0.1814, train acc: 0.9376, val loss: 0.1670, val acc: 0.9467  (best train acc: 0.9443, best val acc: 0.9555, best train loss: 0.1641  @ epoch 11045 )\n",
      "[Epoch: 11080] train loss: 0.1736, train acc: 0.9417, val loss: 0.1728, val acc: 0.9450  (best train acc: 0.9443, best val acc: 0.9555, best train loss: 0.1641  @ epoch 11045 )\n",
      "[Epoch: 11100] train loss: 0.1790, train acc: 0.9386, val loss: 0.1607, val acc: 0.9501  (best train acc: 0.9443, best val acc: 0.9555, best train loss: 0.1641  @ epoch 11045 )\n",
      "[Epoch: 11120] train loss: 0.2124, train acc: 0.9291, val loss: 0.2117, val acc: 0.9180  (best train acc: 0.9443, best val acc: 0.9555, best train loss: 0.1641  @ epoch 11045 )\n",
      "[Epoch: 11140] train loss: 0.1698, train acc: 0.9384, val loss: 0.1672, val acc: 0.9501  (best train acc: 0.9443, best val acc: 0.9555, best train loss: 0.1641  @ epoch 11045 )\n",
      "[Epoch: 11160] train loss: 0.1765, train acc: 0.9368, val loss: 0.1564, val acc: 0.9501  (best train acc: 0.9443, best val acc: 0.9555, best train loss: 0.1641  @ epoch 11045 )\n",
      "[Epoch: 11180] train loss: 0.1750, train acc: 0.9395, val loss: 0.1701, val acc: 0.9406  (best train acc: 0.9443, best val acc: 0.9555, best train loss: 0.1641  @ epoch 11045 )\n",
      "[Epoch: 11200] train loss: 0.1713, train acc: 0.9380, val loss: 0.1567, val acc: 0.9538  (best train acc: 0.9443, best val acc: 0.9555, best train loss: 0.1641  @ epoch 11045 )\n",
      "[Epoch: 11220] train loss: 0.1679, train acc: 0.9437, val loss: 0.1597, val acc: 0.9487  (best train acc: 0.9443, best val acc: 0.9555, best train loss: 0.1641  @ epoch 11045 )\n",
      "[Epoch: 11240] train loss: 0.1793, train acc: 0.9317, val loss: 0.1515, val acc: 0.9535  (best train acc: 0.9443, best val acc: 0.9555, best train loss: 0.1641  @ epoch 11045 )\n",
      "[Epoch: 11260] train loss: 0.1823, train acc: 0.9355, val loss: 0.1507, val acc: 0.9504  (best train acc: 0.9443, best val acc: 0.9555, best train loss: 0.1641  @ epoch 11045 )\n",
      "[Epoch: 11280] train loss: 0.1698, train acc: 0.9393, val loss: 0.1515, val acc: 0.9514  (best train acc: 0.9443, best val acc: 0.9555, best train loss: 0.1626  @ epoch 11270 )\n",
      "[Epoch: 11300] train loss: 0.1740, train acc: 0.9385, val loss: 0.1501, val acc: 0.9524  (best train acc: 0.9443, best val acc: 0.9555, best train loss: 0.1626  @ epoch 11270 )\n",
      "[Epoch: 11320] train loss: 0.1779, train acc: 0.9396, val loss: 0.1512, val acc: 0.9518  (best train acc: 0.9443, best val acc: 0.9558, best train loss: 0.1598  @ epoch 11311 )\n",
      "[Epoch: 11340] train loss: 0.1709, train acc: 0.9389, val loss: 0.1499, val acc: 0.9504  (best train acc: 0.9443, best val acc: 0.9558, best train loss: 0.1598  @ epoch 11311 )\n",
      "[Epoch: 11360] train loss: 0.1884, train acc: 0.9331, val loss: 0.1595, val acc: 0.9487  (best train acc: 0.9443, best val acc: 0.9558, best train loss: 0.1598  @ epoch 11311 )\n",
      "[Epoch: 11380] train loss: 0.1814, train acc: 0.9339, val loss: 0.1702, val acc: 0.9400  (best train acc: 0.9443, best val acc: 0.9558, best train loss: 0.1598  @ epoch 11311 )\n",
      "[Epoch: 11400] train loss: 0.1710, train acc: 0.9397, val loss: 0.1542, val acc: 0.9487  (best train acc: 0.9443, best val acc: 0.9558, best train loss: 0.1598  @ epoch 11311 )\n",
      "[Epoch: 11420] train loss: 0.1756, train acc: 0.9415, val loss: 0.1528, val acc: 0.9518  (best train acc: 0.9443, best val acc: 0.9558, best train loss: 0.1598  @ epoch 11311 )\n",
      "[Epoch: 11440] train loss: 0.1747, train acc: 0.9362, val loss: 0.1630, val acc: 0.9494  (best train acc: 0.9443, best val acc: 0.9558, best train loss: 0.1585  @ epoch 11426 )\n",
      "[Epoch: 11460] train loss: 0.1653, train acc: 0.9396, val loss: 0.1540, val acc: 0.9521  (best train acc: 0.9443, best val acc: 0.9558, best train loss: 0.1585  @ epoch 11426 )\n",
      "[Epoch: 11480] train loss: 0.1796, train acc: 0.9358, val loss: 0.1690, val acc: 0.9403  (best train acc: 0.9443, best val acc: 0.9558, best train loss: 0.1585  @ epoch 11426 )\n",
      "[Epoch: 11500] train loss: 0.1733, train acc: 0.9391, val loss: 0.1548, val acc: 0.9484  (best train acc: 0.9443, best val acc: 0.9558, best train loss: 0.1585  @ epoch 11426 )\n",
      "[Epoch: 11520] train loss: 0.1852, train acc: 0.9327, val loss: 0.1601, val acc: 0.9501  (best train acc: 0.9445, best val acc: 0.9558, best train loss: 0.1585  @ epoch 11426 )\n",
      "[Epoch: 11540] train loss: 0.1772, train acc: 0.9401, val loss: 0.1495, val acc: 0.9511  (best train acc: 0.9445, best val acc: 0.9558, best train loss: 0.1585  @ epoch 11426 )\n",
      "[Epoch: 11560] train loss: 0.1759, train acc: 0.9382, val loss: 0.1553, val acc: 0.9535  (best train acc: 0.9445, best val acc: 0.9558, best train loss: 0.1585  @ epoch 11426 )\n",
      "[Epoch: 11580] train loss: 0.1716, train acc: 0.9402, val loss: 0.1634, val acc: 0.9521  (best train acc: 0.9445, best val acc: 0.9558, best train loss: 0.1585  @ epoch 11426 )\n",
      "[Epoch: 11600] train loss: 0.1827, train acc: 0.9370, val loss: 0.1615, val acc: 0.9467  (best train acc: 0.9446, best val acc: 0.9558, best train loss: 0.1585  @ epoch 11426 )\n",
      "[Epoch: 11620] train loss: 0.1651, train acc: 0.9384, val loss: 0.1546, val acc: 0.9518  (best train acc: 0.9446, best val acc: 0.9558, best train loss: 0.1585  @ epoch 11426 )\n",
      "[Epoch: 11640] train loss: 0.1711, train acc: 0.9425, val loss: 0.1556, val acc: 0.9514  (best train acc: 0.9446, best val acc: 0.9558, best train loss: 0.1585  @ epoch 11426 )\n",
      "[Epoch: 11660] train loss: 0.1661, train acc: 0.9415, val loss: 0.1629, val acc: 0.9511  (best train acc: 0.9446, best val acc: 0.9558, best train loss: 0.1584  @ epoch 11643 )\n",
      "[Epoch: 11680] train loss: 0.1903, train acc: 0.9312, val loss: 0.1878, val acc: 0.9349  (best train acc: 0.9446, best val acc: 0.9558, best train loss: 0.1584  @ epoch 11643 )\n",
      "[Epoch: 11700] train loss: 0.1796, train acc: 0.9348, val loss: 0.1679, val acc: 0.9501  (best train acc: 0.9446, best val acc: 0.9558, best train loss: 0.1584  @ epoch 11643 )\n",
      "[Epoch: 11720] train loss: 0.1883, train acc: 0.9284, val loss: 0.1988, val acc: 0.9319  (best train acc: 0.9446, best val acc: 0.9558, best train loss: 0.1584  @ epoch 11643 )\n",
      "[Epoch: 11740] train loss: 0.1874, train acc: 0.9369, val loss: 0.1695, val acc: 0.9474  (best train acc: 0.9446, best val acc: 0.9558, best train loss: 0.1584  @ epoch 11643 )\n",
      "[Epoch: 11760] train loss: 0.1673, train acc: 0.9405, val loss: 0.1594, val acc: 0.9484  (best train acc: 0.9446, best val acc: 0.9558, best train loss: 0.1584  @ epoch 11643 )\n",
      "[Epoch: 11780] train loss: 0.1860, train acc: 0.9325, val loss: 0.1537, val acc: 0.9497  (best train acc: 0.9446, best val acc: 0.9558, best train loss: 0.1584  @ epoch 11643 )\n",
      "[Epoch: 11800] train loss: 0.1789, train acc: 0.9365, val loss: 0.1839, val acc: 0.9373  (best train acc: 0.9446, best val acc: 0.9558, best train loss: 0.1584  @ epoch 11643 )\n",
      "[Epoch: 11820] train loss: 0.1836, train acc: 0.9344, val loss: 0.1975, val acc: 0.9322  (best train acc: 0.9446, best val acc: 0.9558, best train loss: 0.1584  @ epoch 11643 )\n",
      "[Epoch: 11840] train loss: 0.1731, train acc: 0.9417, val loss: 0.1644, val acc: 0.9504  (best train acc: 0.9462, best val acc: 0.9558, best train loss: 0.1584  @ epoch 11643 )\n",
      "[Epoch: 11860] train loss: 0.1666, train acc: 0.9404, val loss: 0.1726, val acc: 0.9454  (best train acc: 0.9462, best val acc: 0.9558, best train loss: 0.1584  @ epoch 11643 )\n",
      "[Epoch: 11880] train loss: 0.1937, train acc: 0.9310, val loss: 0.1769, val acc: 0.9386  (best train acc: 0.9462, best val acc: 0.9558, best train loss: 0.1584  @ epoch 11643 )\n",
      "[Epoch: 11900] train loss: 0.1668, train acc: 0.9410, val loss: 0.1598, val acc: 0.9494  (best train acc: 0.9462, best val acc: 0.9558, best train loss: 0.1584  @ epoch 11643 )\n",
      "[Epoch: 11920] train loss: 0.1702, train acc: 0.9427, val loss: 0.1550, val acc: 0.9508  (best train acc: 0.9462, best val acc: 0.9558, best train loss: 0.1584  @ epoch 11643 )\n",
      "[Epoch: 11940] train loss: 0.1686, train acc: 0.9393, val loss: 0.1595, val acc: 0.9548  (best train acc: 0.9462, best val acc: 0.9558, best train loss: 0.1547  @ epoch 11921 )\n",
      "[Epoch: 11960] train loss: 0.1688, train acc: 0.9391, val loss: 0.1585, val acc: 0.9531  (best train acc: 0.9462, best val acc: 0.9558, best train loss: 0.1547  @ epoch 11921 )\n",
      "[Epoch: 11980] train loss: 0.1720, train acc: 0.9395, val loss: 0.1619, val acc: 0.9524  (best train acc: 0.9462, best val acc: 0.9558, best train loss: 0.1547  @ epoch 11921 )\n",
      "[Epoch: 12000] train loss: 0.1609, train acc: 0.9434, val loss: 0.1638, val acc: 0.9521  (best train acc: 0.9462, best val acc: 0.9558, best train loss: 0.1547  @ epoch 11921 )\n",
      "[Epoch: 12020] train loss: 0.1761, train acc: 0.9365, val loss: 0.1721, val acc: 0.9417  (best train acc: 0.9471, best val acc: 0.9558, best train loss: 0.1547  @ epoch 11921 )\n",
      "[Epoch: 12040] train loss: 0.1855, train acc: 0.9319, val loss: 0.1931, val acc: 0.9417  (best train acc: 0.9471, best val acc: 0.9558, best train loss: 0.1547  @ epoch 11921 )\n",
      "[Epoch: 12060] train loss: 0.1795, train acc: 0.9342, val loss: 0.1730, val acc: 0.9477  (best train acc: 0.9471, best val acc: 0.9558, best train loss: 0.1547  @ epoch 11921 )\n",
      "[Epoch: 12080] train loss: 0.1667, train acc: 0.9409, val loss: 0.1603, val acc: 0.9477  (best train acc: 0.9471, best val acc: 0.9558, best train loss: 0.1547  @ epoch 11921 )\n",
      "[Epoch: 12100] train loss: 0.1717, train acc: 0.9389, val loss: 0.1772, val acc: 0.9457  (best train acc: 0.9471, best val acc: 0.9558, best train loss: 0.1547  @ epoch 11921 )\n",
      "[Epoch: 12120] train loss: 0.1980, train acc: 0.9203, val loss: 0.1719, val acc: 0.9460  (best train acc: 0.9471, best val acc: 0.9558, best train loss: 0.1547  @ epoch 11921 )\n",
      "[Epoch: 12140] train loss: 0.1630, train acc: 0.9402, val loss: 0.1592, val acc: 0.9528  (best train acc: 0.9471, best val acc: 0.9558, best train loss: 0.1547  @ epoch 11921 )\n",
      "[Epoch: 12160] train loss: 0.1742, train acc: 0.9346, val loss: 0.1669, val acc: 0.9491  (best train acc: 0.9471, best val acc: 0.9558, best train loss: 0.1547  @ epoch 11921 )\n",
      "[Epoch: 12180] train loss: 0.1687, train acc: 0.9424, val loss: 0.1636, val acc: 0.9454  (best train acc: 0.9471, best val acc: 0.9558, best train loss: 0.1547  @ epoch 11921 )\n",
      "[Epoch: 12200] train loss: 0.1656, train acc: 0.9429, val loss: 0.1632, val acc: 0.9464  (best train acc: 0.9471, best val acc: 0.9558, best train loss: 0.1547  @ epoch 11921 )\n",
      "[Epoch: 12220] train loss: 0.1669, train acc: 0.9410, val loss: 0.1570, val acc: 0.9528  (best train acc: 0.9471, best val acc: 0.9558, best train loss: 0.1547  @ epoch 11921 )\n",
      "[Epoch: 12240] train loss: 0.1716, train acc: 0.9393, val loss: 0.1638, val acc: 0.9430  (best train acc: 0.9471, best val acc: 0.9558, best train loss: 0.1547  @ epoch 11921 )\n",
      "[Epoch: 12260] train loss: 0.1674, train acc: 0.9372, val loss: 0.1589, val acc: 0.9474  (best train acc: 0.9471, best val acc: 0.9558, best train loss: 0.1547  @ epoch 11921 )\n",
      "[Epoch: 12280] train loss: 0.1788, train acc: 0.9350, val loss: 0.1878, val acc: 0.9464  (best train acc: 0.9471, best val acc: 0.9558, best train loss: 0.1547  @ epoch 11921 )\n",
      "[Epoch: 12300] train loss: 0.1753, train acc: 0.9431, val loss: 0.1565, val acc: 0.9508  (best train acc: 0.9477, best val acc: 0.9558, best train loss: 0.1524  @ epoch 12294 )\n",
      "[Epoch: 12320] train loss: 0.1810, train acc: 0.9365, val loss: 0.1865, val acc: 0.9403  (best train acc: 0.9477, best val acc: 0.9558, best train loss: 0.1524  @ epoch 12294 )\n",
      "[Epoch: 12340] train loss: 0.1890, train acc: 0.9328, val loss: 0.1646, val acc: 0.9494  (best train acc: 0.9477, best val acc: 0.9558, best train loss: 0.1524  @ epoch 12294 )\n",
      "[Epoch: 12360] train loss: 0.1750, train acc: 0.9374, val loss: 0.1720, val acc: 0.9423  (best train acc: 0.9477, best val acc: 0.9558, best train loss: 0.1520  @ epoch 12359 )\n",
      "[Epoch: 12380] train loss: 0.1585, train acc: 0.9453, val loss: 0.1617, val acc: 0.9508  (best train acc: 0.9477, best val acc: 0.9562, best train loss: 0.1520  @ epoch 12359 )\n",
      "[Epoch: 12400] train loss: 0.1594, train acc: 0.9434, val loss: 0.1607, val acc: 0.9491  (best train acc: 0.9477, best val acc: 0.9562, best train loss: 0.1520  @ epoch 12359 )\n",
      "[Epoch: 12420] train loss: 0.1678, train acc: 0.9413, val loss: 0.1653, val acc: 0.9474  (best train acc: 0.9477, best val acc: 0.9562, best train loss: 0.1520  @ epoch 12359 )\n",
      "[Epoch: 12440] train loss: 0.1722, train acc: 0.9397, val loss: 0.1636, val acc: 0.9501  (best train acc: 0.9477, best val acc: 0.9568, best train loss: 0.1520  @ epoch 12359 )\n",
      "[Epoch: 12460] train loss: 0.1662, train acc: 0.9435, val loss: 0.1666, val acc: 0.9447  (best train acc: 0.9477, best val acc: 0.9568, best train loss: 0.1520  @ epoch 12359 )\n",
      "[Epoch: 12480] train loss: 0.1750, train acc: 0.9382, val loss: 0.1659, val acc: 0.9494  (best train acc: 0.9477, best val acc: 0.9568, best train loss: 0.1520  @ epoch 12359 )\n",
      "[Epoch: 12500] train loss: 0.1822, train acc: 0.9352, val loss: 0.1614, val acc: 0.9484  (best train acc: 0.9477, best val acc: 0.9568, best train loss: 0.1520  @ epoch 12359 )\n",
      "[Epoch: 12520] train loss: 0.1822, train acc: 0.9351, val loss: 0.1769, val acc: 0.9410  (best train acc: 0.9477, best val acc: 0.9568, best train loss: 0.1520  @ epoch 12359 )\n",
      "[Epoch: 12540] train loss: 0.1784, train acc: 0.9417, val loss: 0.1600, val acc: 0.9521  (best train acc: 0.9477, best val acc: 0.9578, best train loss: 0.1520  @ epoch 12359 )\n",
      "[Epoch: 12560] train loss: 0.1716, train acc: 0.9417, val loss: 0.1585, val acc: 0.9528  (best train acc: 0.9477, best val acc: 0.9578, best train loss: 0.1520  @ epoch 12359 )\n",
      "[Epoch: 12580] train loss: 0.1613, train acc: 0.9422, val loss: 0.1790, val acc: 0.9450  (best train acc: 0.9477, best val acc: 0.9578, best train loss: 0.1520  @ epoch 12359 )\n",
      "[Epoch: 12600] train loss: 0.1624, train acc: 0.9435, val loss: 0.1716, val acc: 0.9437  (best train acc: 0.9477, best val acc: 0.9578, best train loss: 0.1520  @ epoch 12359 )\n",
      "[Epoch: 12620] train loss: 0.1799, train acc: 0.9315, val loss: 0.1827, val acc: 0.9427  (best train acc: 0.9477, best val acc: 0.9578, best train loss: 0.1520  @ epoch 12359 )\n",
      "[Epoch: 12640] train loss: 0.1833, train acc: 0.9363, val loss: 0.1643, val acc: 0.9508  (best train acc: 0.9477, best val acc: 0.9578, best train loss: 0.1520  @ epoch 12359 )\n",
      "[Epoch: 12660] train loss: 0.1664, train acc: 0.9422, val loss: 0.1735, val acc: 0.9420  (best train acc: 0.9477, best val acc: 0.9578, best train loss: 0.1520  @ epoch 12359 )\n",
      "[Epoch: 12680] train loss: 0.1728, train acc: 0.9423, val loss: 0.1567, val acc: 0.9514  (best train acc: 0.9477, best val acc: 0.9578, best train loss: 0.1520  @ epoch 12359 )\n",
      "[Epoch: 12700] train loss: 0.1583, train acc: 0.9444, val loss: 0.1576, val acc: 0.9531  (best train acc: 0.9477, best val acc: 0.9578, best train loss: 0.1520  @ epoch 12359 )\n",
      "[Epoch: 12720] train loss: 0.1630, train acc: 0.9440, val loss: 0.1605, val acc: 0.9470  (best train acc: 0.9477, best val acc: 0.9578, best train loss: 0.1520  @ epoch 12359 )\n",
      "[Epoch: 12740] train loss: 0.1606, train acc: 0.9432, val loss: 0.1725, val acc: 0.9484  (best train acc: 0.9477, best val acc: 0.9578, best train loss: 0.1520  @ epoch 12359 )\n",
      "[Epoch: 12760] train loss: 0.1708, train acc: 0.9384, val loss: 0.1620, val acc: 0.9521  (best train acc: 0.9477, best val acc: 0.9578, best train loss: 0.1520  @ epoch 12359 )\n",
      "[Epoch: 12780] train loss: 0.1830, train acc: 0.9323, val loss: 0.1699, val acc: 0.9460  (best train acc: 0.9477, best val acc: 0.9578, best train loss: 0.1520  @ epoch 12359 )\n",
      "[Epoch: 12800] train loss: 0.1667, train acc: 0.9420, val loss: 0.1649, val acc: 0.9504  (best train acc: 0.9477, best val acc: 0.9578, best train loss: 0.1520  @ epoch 12359 )\n",
      "[Epoch: 12820] train loss: 0.1669, train acc: 0.9407, val loss: 0.1621, val acc: 0.9535  (best train acc: 0.9477, best val acc: 0.9582, best train loss: 0.1520  @ epoch 12359 )\n",
      "[Epoch: 12840] train loss: 0.1746, train acc: 0.9409, val loss: 0.1604, val acc: 0.9504  (best train acc: 0.9477, best val acc: 0.9582, best train loss: 0.1520  @ epoch 12359 )\n",
      "[Epoch: 12860] train loss: 0.1698, train acc: 0.9395, val loss: 0.1876, val acc: 0.9386  (best train acc: 0.9477, best val acc: 0.9582, best train loss: 0.1520  @ epoch 12359 )\n",
      "[Epoch: 12880] train loss: 0.1767, train acc: 0.9403, val loss: 0.1701, val acc: 0.9454  (best train acc: 0.9477, best val acc: 0.9582, best train loss: 0.1520  @ epoch 12359 )\n",
      "[Epoch: 12900] train loss: 0.1697, train acc: 0.9394, val loss: 0.1565, val acc: 0.9545  (best train acc: 0.9477, best val acc: 0.9582, best train loss: 0.1520  @ epoch 12359 )\n",
      "[Epoch: 12920] train loss: 0.1671, train acc: 0.9426, val loss: 0.1718, val acc: 0.9440  (best train acc: 0.9477, best val acc: 0.9582, best train loss: 0.1520  @ epoch 12359 )\n",
      "[Epoch: 12940] train loss: 0.1608, train acc: 0.9423, val loss: 0.1701, val acc: 0.9450  (best train acc: 0.9477, best val acc: 0.9582, best train loss: 0.1520  @ epoch 12359 )\n",
      "[Epoch: 12960] train loss: 0.1745, train acc: 0.9335, val loss: 0.1585, val acc: 0.9541  (best train acc: 0.9477, best val acc: 0.9582, best train loss: 0.1520  @ epoch 12359 )\n",
      "[Epoch: 12980] train loss: 0.1640, train acc: 0.9438, val loss: 0.1733, val acc: 0.9470  (best train acc: 0.9479, best val acc: 0.9582, best train loss: 0.1492  @ epoch 12971 )\n",
      "[Epoch: 13000] train loss: 0.1704, train acc: 0.9393, val loss: 0.1613, val acc: 0.9514  (best train acc: 0.9479, best val acc: 0.9582, best train loss: 0.1492  @ epoch 12971 )\n",
      "[Epoch: 13020] train loss: 0.1711, train acc: 0.9359, val loss: 0.1671, val acc: 0.9501  (best train acc: 0.9479, best val acc: 0.9582, best train loss: 0.1492  @ epoch 12971 )\n",
      "[Epoch: 13040] train loss: 0.1626, train acc: 0.9419, val loss: 0.1529, val acc: 0.9541  (best train acc: 0.9483, best val acc: 0.9582, best train loss: 0.1492  @ epoch 12971 )\n",
      "[Epoch: 13060] train loss: 0.1730, train acc: 0.9406, val loss: 0.1951, val acc: 0.9346  (best train acc: 0.9483, best val acc: 0.9599, best train loss: 0.1492  @ epoch 12971 )\n",
      "[Epoch: 13080] train loss: 0.1621, train acc: 0.9440, val loss: 0.1685, val acc: 0.9474  (best train acc: 0.9483, best val acc: 0.9599, best train loss: 0.1492  @ epoch 12971 )\n",
      "[Epoch: 13100] train loss: 0.1724, train acc: 0.9393, val loss: 0.1680, val acc: 0.9470  (best train acc: 0.9483, best val acc: 0.9599, best train loss: 0.1492  @ epoch 12971 )\n",
      "[Epoch: 13120] train loss: 0.1577, train acc: 0.9463, val loss: 0.1865, val acc: 0.9420  (best train acc: 0.9483, best val acc: 0.9599, best train loss: 0.1492  @ epoch 12971 )\n",
      "[Epoch: 13140] train loss: 0.1705, train acc: 0.9438, val loss: 0.1617, val acc: 0.9508  (best train acc: 0.9483, best val acc: 0.9599, best train loss: 0.1492  @ epoch 12971 )\n",
      "[Epoch: 13160] train loss: 0.1657, train acc: 0.9398, val loss: 0.1734, val acc: 0.9511  (best train acc: 0.9483, best val acc: 0.9599, best train loss: 0.1492  @ epoch 12971 )\n",
      "[Epoch: 13180] train loss: 0.1585, train acc: 0.9422, val loss: 0.1666, val acc: 0.9484  (best train acc: 0.9487, best val acc: 0.9599, best train loss: 0.1492  @ epoch 12971 )\n",
      "[Epoch: 13200] train loss: 0.1635, train acc: 0.9415, val loss: 0.1685, val acc: 0.9474  (best train acc: 0.9487, best val acc: 0.9599, best train loss: 0.1492  @ epoch 12971 )\n",
      "[Epoch: 13220] train loss: 0.1572, train acc: 0.9446, val loss: 0.1580, val acc: 0.9531  (best train acc: 0.9487, best val acc: 0.9599, best train loss: 0.1492  @ epoch 12971 )\n",
      "[Epoch: 13240] train loss: 0.1672, train acc: 0.9391, val loss: 0.1709, val acc: 0.9481  (best train acc: 0.9487, best val acc: 0.9599, best train loss: 0.1492  @ epoch 12971 )\n",
      "[Epoch: 13260] train loss: 0.1626, train acc: 0.9425, val loss: 0.1654, val acc: 0.9484  (best train acc: 0.9487, best val acc: 0.9599, best train loss: 0.1492  @ epoch 12971 )\n",
      "[Epoch: 13280] train loss: 0.1655, train acc: 0.9448, val loss: 0.1607, val acc: 0.9548  (best train acc: 0.9487, best val acc: 0.9599, best train loss: 0.1492  @ epoch 12971 )\n",
      "[Epoch: 13300] train loss: 0.2018, train acc: 0.9281, val loss: 0.1759, val acc: 0.9474  (best train acc: 0.9487, best val acc: 0.9599, best train loss: 0.1492  @ epoch 12971 )\n",
      "[Epoch: 13320] train loss: 0.1627, train acc: 0.9429, val loss: 0.1789, val acc: 0.9417  (best train acc: 0.9487, best val acc: 0.9599, best train loss: 0.1492  @ epoch 12971 )\n",
      "[Epoch: 13340] train loss: 0.1675, train acc: 0.9430, val loss: 0.1592, val acc: 0.9454  (best train acc: 0.9487, best val acc: 0.9599, best train loss: 0.1492  @ epoch 12971 )\n",
      "[Epoch: 13360] train loss: 0.1654, train acc: 0.9453, val loss: 0.1651, val acc: 0.9535  (best train acc: 0.9487, best val acc: 0.9599, best train loss: 0.1492  @ epoch 12971 )\n",
      "[Epoch: 13380] train loss: 0.1641, train acc: 0.9455, val loss: 0.1671, val acc: 0.9457  (best train acc: 0.9487, best val acc: 0.9599, best train loss: 0.1492  @ epoch 12971 )\n",
      "[Epoch: 13400] train loss: 0.1559, train acc: 0.9445, val loss: 0.1606, val acc: 0.9545  (best train acc: 0.9487, best val acc: 0.9599, best train loss: 0.1492  @ epoch 12971 )\n",
      "[Epoch: 13420] train loss: 0.1635, train acc: 0.9459, val loss: 0.1610, val acc: 0.9535  (best train acc: 0.9488, best val acc: 0.9599, best train loss: 0.1492  @ epoch 12971 )\n",
      "[Epoch: 13440] train loss: 0.1658, train acc: 0.9434, val loss: 0.1703, val acc: 0.9541  (best train acc: 0.9488, best val acc: 0.9599, best train loss: 0.1478  @ epoch 13426 )\n",
      "[Epoch: 13460] train loss: 0.1660, train acc: 0.9435, val loss: 0.1630, val acc: 0.9508  (best train acc: 0.9488, best val acc: 0.9599, best train loss: 0.1478  @ epoch 13426 )\n",
      "[Epoch: 13480] train loss: 0.1650, train acc: 0.9389, val loss: 0.1704, val acc: 0.9437  (best train acc: 0.9489, best val acc: 0.9599, best train loss: 0.1478  @ epoch 13426 )\n",
      "[Epoch: 13500] train loss: 0.1660, train acc: 0.9434, val loss: 0.1904, val acc: 0.9427  (best train acc: 0.9489, best val acc: 0.9599, best train loss: 0.1478  @ epoch 13426 )\n",
      "[Epoch: 13520] train loss: 0.1601, train acc: 0.9436, val loss: 0.1614, val acc: 0.9494  (best train acc: 0.9489, best val acc: 0.9599, best train loss: 0.1478  @ epoch 13426 )\n",
      "[Epoch: 13540] train loss: 0.1612, train acc: 0.9441, val loss: 0.1647, val acc: 0.9491  (best train acc: 0.9489, best val acc: 0.9599, best train loss: 0.1478  @ epoch 13426 )\n",
      "[Epoch: 13560] train loss: 0.1619, train acc: 0.9416, val loss: 0.1684, val acc: 0.9518  (best train acc: 0.9489, best val acc: 0.9599, best train loss: 0.1478  @ epoch 13426 )\n",
      "[Epoch: 13580] train loss: 0.1868, train acc: 0.9291, val loss: 0.1695, val acc: 0.9433  (best train acc: 0.9489, best val acc: 0.9599, best train loss: 0.1478  @ epoch 13426 )\n",
      "[Epoch: 13600] train loss: 0.1865, train acc: 0.9332, val loss: 0.1689, val acc: 0.9541  (best train acc: 0.9489, best val acc: 0.9599, best train loss: 0.1478  @ epoch 13426 )\n",
      "[Epoch: 13620] train loss: 0.1585, train acc: 0.9474, val loss: 0.1593, val acc: 0.9565  (best train acc: 0.9489, best val acc: 0.9599, best train loss: 0.1478  @ epoch 13426 )\n",
      "[Epoch: 13640] train loss: 0.1576, train acc: 0.9473, val loss: 0.1582, val acc: 0.9555  (best train acc: 0.9489, best val acc: 0.9599, best train loss: 0.1478  @ epoch 13426 )\n",
      "[Epoch: 13660] train loss: 0.1542, train acc: 0.9461, val loss: 0.1646, val acc: 0.9545  (best train acc: 0.9489, best val acc: 0.9599, best train loss: 0.1478  @ epoch 13426 )\n",
      "[Epoch: 13680] train loss: 0.1592, train acc: 0.9441, val loss: 0.1655, val acc: 0.9481  (best train acc: 0.9489, best val acc: 0.9599, best train loss: 0.1478  @ epoch 13426 )\n",
      "[Epoch: 13700] train loss: 0.1605, train acc: 0.9443, val loss: 0.1658, val acc: 0.9474  (best train acc: 0.9492, best val acc: 0.9599, best train loss: 0.1438  @ epoch 13693 )\n",
      "[Epoch: 13720] train loss: 0.1691, train acc: 0.9397, val loss: 0.1690, val acc: 0.9508  (best train acc: 0.9492, best val acc: 0.9599, best train loss: 0.1438  @ epoch 13693 )\n",
      "[Epoch: 13740] train loss: 0.1676, train acc: 0.9440, val loss: 0.1675, val acc: 0.9518  (best train acc: 0.9492, best val acc: 0.9599, best train loss: 0.1438  @ epoch 13693 )\n",
      "[Epoch: 13760] train loss: 0.1620, train acc: 0.9467, val loss: 0.1675, val acc: 0.9551  (best train acc: 0.9492, best val acc: 0.9599, best train loss: 0.1438  @ epoch 13693 )\n",
      "[Epoch: 13780] train loss: 0.1674, train acc: 0.9438, val loss: 0.1653, val acc: 0.9494  (best train acc: 0.9492, best val acc: 0.9599, best train loss: 0.1438  @ epoch 13693 )\n",
      "[Epoch: 13800] train loss: 0.1521, train acc: 0.9460, val loss: 0.1671, val acc: 0.9470  (best train acc: 0.9492, best val acc: 0.9599, best train loss: 0.1438  @ epoch 13693 )\n",
      "[Epoch: 13820] train loss: 0.1725, train acc: 0.9386, val loss: 0.1670, val acc: 0.9524  (best train acc: 0.9492, best val acc: 0.9599, best train loss: 0.1438  @ epoch 13693 )\n",
      "[Epoch: 13840] train loss: 0.1506, train acc: 0.9462, val loss: 0.1598, val acc: 0.9531  (best train acc: 0.9492, best val acc: 0.9599, best train loss: 0.1438  @ epoch 13693 )\n",
      "[Epoch: 13860] train loss: 0.1597, train acc: 0.9431, val loss: 0.1765, val acc: 0.9487  (best train acc: 0.9497, best val acc: 0.9599, best train loss: 0.1438  @ epoch 13693 )\n",
      "[Epoch: 13880] train loss: 0.1621, train acc: 0.9440, val loss: 0.1687, val acc: 0.9511  (best train acc: 0.9497, best val acc: 0.9599, best train loss: 0.1438  @ epoch 13693 )\n",
      "[Epoch: 13900] train loss: 0.1681, train acc: 0.9439, val loss: 0.1761, val acc: 0.9400  (best train acc: 0.9497, best val acc: 0.9599, best train loss: 0.1438  @ epoch 13693 )\n",
      "[Epoch: 13920] train loss: 0.1623, train acc: 0.9420, val loss: 0.1616, val acc: 0.9470  (best train acc: 0.9497, best val acc: 0.9599, best train loss: 0.1438  @ epoch 13693 )\n",
      "[Epoch: 13940] train loss: 0.1575, train acc: 0.9432, val loss: 0.1658, val acc: 0.9497  (best train acc: 0.9497, best val acc: 0.9599, best train loss: 0.1438  @ epoch 13693 )\n",
      "[Epoch: 13960] train loss: 0.1627, train acc: 0.9437, val loss: 0.1559, val acc: 0.9521  (best train acc: 0.9497, best val acc: 0.9599, best train loss: 0.1438  @ epoch 13693 )\n",
      "[Epoch: 13980] train loss: 0.1521, train acc: 0.9482, val loss: 0.1685, val acc: 0.9491  (best train acc: 0.9497, best val acc: 0.9599, best train loss: 0.1438  @ epoch 13693 )\n",
      "[Epoch: 14000] train loss: 0.1634, train acc: 0.9411, val loss: 0.1687, val acc: 0.9470  (best train acc: 0.9497, best val acc: 0.9599, best train loss: 0.1438  @ epoch 13693 )\n",
      "[Epoch: 14020] train loss: 0.1536, train acc: 0.9476, val loss: 0.1726, val acc: 0.9450  (best train acc: 0.9498, best val acc: 0.9599, best train loss: 0.1438  @ epoch 13693 )\n",
      "[Epoch: 14040] train loss: 0.1655, train acc: 0.9440, val loss: 0.1647, val acc: 0.9508  (best train acc: 0.9498, best val acc: 0.9599, best train loss: 0.1438  @ epoch 13693 )\n",
      "[Epoch: 14060] train loss: 0.1782, train acc: 0.9392, val loss: 0.1813, val acc: 0.9457  (best train acc: 0.9498, best val acc: 0.9599, best train loss: 0.1438  @ epoch 13693 )\n",
      "[Epoch: 14080] train loss: 0.1656, train acc: 0.9406, val loss: 0.1822, val acc: 0.9400  (best train acc: 0.9498, best val acc: 0.9599, best train loss: 0.1438  @ epoch 13693 )\n",
      "[Epoch: 14100] train loss: 0.1606, train acc: 0.9435, val loss: 0.1681, val acc: 0.9528  (best train acc: 0.9498, best val acc: 0.9599, best train loss: 0.1438  @ epoch 13693 )\n",
      "[Epoch: 14120] train loss: 0.1615, train acc: 0.9442, val loss: 0.1825, val acc: 0.9457  (best train acc: 0.9498, best val acc: 0.9599, best train loss: 0.1438  @ epoch 13693 )\n",
      "[Epoch: 14140] train loss: 0.1679, train acc: 0.9412, val loss: 0.1647, val acc: 0.9541  (best train acc: 0.9498, best val acc: 0.9599, best train loss: 0.1438  @ epoch 13693 )\n",
      "[Epoch: 14160] train loss: 0.1590, train acc: 0.9422, val loss: 0.1619, val acc: 0.9524  (best train acc: 0.9498, best val acc: 0.9599, best train loss: 0.1438  @ epoch 13693 )\n",
      "[Epoch: 14180] train loss: 0.1475, train acc: 0.9496, val loss: 0.1646, val acc: 0.9504  (best train acc: 0.9498, best val acc: 0.9599, best train loss: 0.1438  @ epoch 13693 )\n",
      "[Epoch: 14200] train loss: 0.1630, train acc: 0.9432, val loss: 0.1888, val acc: 0.9427  (best train acc: 0.9498, best val acc: 0.9599, best train loss: 0.1438  @ epoch 13693 )\n",
      "[Epoch: 14220] train loss: 0.1678, train acc: 0.9403, val loss: 0.1727, val acc: 0.9528  (best train acc: 0.9498, best val acc: 0.9599, best train loss: 0.1438  @ epoch 13693 )\n",
      "[Epoch: 14240] train loss: 0.1640, train acc: 0.9422, val loss: 0.1701, val acc: 0.9501  (best train acc: 0.9498, best val acc: 0.9599, best train loss: 0.1438  @ epoch 13693 )\n",
      "[Epoch: 14260] train loss: 0.1751, train acc: 0.9337, val loss: 0.1969, val acc: 0.9352  (best train acc: 0.9498, best val acc: 0.9599, best train loss: 0.1438  @ epoch 13693 )\n",
      "[Epoch: 14280] train loss: 0.1543, train acc: 0.9446, val loss: 0.1746, val acc: 0.9508  (best train acc: 0.9498, best val acc: 0.9599, best train loss: 0.1438  @ epoch 13693 )\n",
      "[Epoch: 14300] train loss: 0.1466, train acc: 0.9485, val loss: 0.1684, val acc: 0.9477  (best train acc: 0.9516, best val acc: 0.9599, best train loss: 0.1438  @ epoch 13693 )\n",
      "[Epoch: 14320] train loss: 0.1579, train acc: 0.9445, val loss: 0.1696, val acc: 0.9528  (best train acc: 0.9516, best val acc: 0.9599, best train loss: 0.1391  @ epoch 14317 )\n",
      "[Epoch: 14340] train loss: 0.1587, train acc: 0.9441, val loss: 0.1643, val acc: 0.9541  (best train acc: 0.9516, best val acc: 0.9599, best train loss: 0.1391  @ epoch 14317 )\n",
      "[Epoch: 14360] train loss: 0.1697, train acc: 0.9418, val loss: 0.1712, val acc: 0.9481  (best train acc: 0.9516, best val acc: 0.9599, best train loss: 0.1391  @ epoch 14317 )\n",
      "[Epoch: 14380] train loss: 0.1671, train acc: 0.9408, val loss: 0.1683, val acc: 0.9504  (best train acc: 0.9516, best val acc: 0.9599, best train loss: 0.1391  @ epoch 14317 )\n",
      "[Epoch: 14400] train loss: 0.1564, train acc: 0.9446, val loss: 0.1713, val acc: 0.9474  (best train acc: 0.9516, best val acc: 0.9599, best train loss: 0.1391  @ epoch 14317 )\n",
      "[Epoch: 14420] train loss: 0.1605, train acc: 0.9425, val loss: 0.1802, val acc: 0.9450  (best train acc: 0.9516, best val acc: 0.9599, best train loss: 0.1391  @ epoch 14317 )\n",
      "[Epoch: 14440] train loss: 0.1638, train acc: 0.9409, val loss: 0.1706, val acc: 0.9474  (best train acc: 0.9516, best val acc: 0.9599, best train loss: 0.1391  @ epoch 14317 )\n",
      "[Epoch: 14460] train loss: 0.1504, train acc: 0.9472, val loss: 0.1631, val acc: 0.9514  (best train acc: 0.9516, best val acc: 0.9599, best train loss: 0.1391  @ epoch 14317 )\n",
      "[Epoch: 14480] train loss: 0.1488, train acc: 0.9432, val loss: 0.1673, val acc: 0.9508  (best train acc: 0.9516, best val acc: 0.9599, best train loss: 0.1391  @ epoch 14317 )\n",
      "[Epoch: 14500] train loss: 0.1656, train acc: 0.9441, val loss: 0.1627, val acc: 0.9518  (best train acc: 0.9516, best val acc: 0.9599, best train loss: 0.1391  @ epoch 14317 )\n",
      "[Epoch: 14520] train loss: 0.1588, train acc: 0.9437, val loss: 0.2058, val acc: 0.9363  (best train acc: 0.9516, best val acc: 0.9599, best train loss: 0.1391  @ epoch 14317 )\n",
      "[Epoch: 14540] train loss: 0.1682, train acc: 0.9409, val loss: 0.1946, val acc: 0.9288  (best train acc: 0.9516, best val acc: 0.9599, best train loss: 0.1391  @ epoch 14317 )\n",
      "[Epoch: 14560] train loss: 0.1495, train acc: 0.9495, val loss: 0.1743, val acc: 0.9460  (best train acc: 0.9516, best val acc: 0.9599, best train loss: 0.1391  @ epoch 14317 )\n",
      "[Epoch: 14580] train loss: 0.1499, train acc: 0.9470, val loss: 0.1834, val acc: 0.9450  (best train acc: 0.9516, best val acc: 0.9599, best train loss: 0.1391  @ epoch 14317 )\n",
      "[Epoch: 14600] train loss: 0.1574, train acc: 0.9435, val loss: 0.1839, val acc: 0.9393  (best train acc: 0.9516, best val acc: 0.9599, best train loss: 0.1391  @ epoch 14317 )\n",
      "[Epoch: 14620] train loss: 0.1528, train acc: 0.9448, val loss: 0.1772, val acc: 0.9474  (best train acc: 0.9516, best val acc: 0.9599, best train loss: 0.1391  @ epoch 14317 )\n",
      "[Epoch: 14640] train loss: 0.1516, train acc: 0.9465, val loss: 0.1803, val acc: 0.9410  (best train acc: 0.9516, best val acc: 0.9599, best train loss: 0.1391  @ epoch 14317 )\n",
      "[Epoch: 14660] train loss: 0.1532, train acc: 0.9461, val loss: 0.1658, val acc: 0.9551  (best train acc: 0.9516, best val acc: 0.9599, best train loss: 0.1382  @ epoch 14651 )\n",
      "[Epoch: 14680] train loss: 0.1595, train acc: 0.9448, val loss: 0.1718, val acc: 0.9484  (best train acc: 0.9516, best val acc: 0.9599, best train loss: 0.1382  @ epoch 14651 )\n",
      "[Epoch: 14700] train loss: 0.1515, train acc: 0.9455, val loss: 0.1654, val acc: 0.9514  (best train acc: 0.9516, best val acc: 0.9599, best train loss: 0.1382  @ epoch 14651 )\n",
      "[Epoch: 14720] train loss: 0.1618, train acc: 0.9424, val loss: 0.1878, val acc: 0.9400  (best train acc: 0.9516, best val acc: 0.9599, best train loss: 0.1382  @ epoch 14651 )\n",
      "[Epoch: 14740] train loss: 0.1503, train acc: 0.9479, val loss: 0.1642, val acc: 0.9511  (best train acc: 0.9516, best val acc: 0.9599, best train loss: 0.1382  @ epoch 14651 )\n",
      "[Epoch: 14760] train loss: 0.1551, train acc: 0.9466, val loss: 0.1669, val acc: 0.9494  (best train acc: 0.9516, best val acc: 0.9599, best train loss: 0.1382  @ epoch 14651 )\n",
      "[Epoch: 14780] train loss: 0.1614, train acc: 0.9444, val loss: 0.1736, val acc: 0.9467  (best train acc: 0.9516, best val acc: 0.9599, best train loss: 0.1382  @ epoch 14651 )\n",
      "[Epoch: 14800] train loss: 0.1648, train acc: 0.9396, val loss: 0.1657, val acc: 0.9474  (best train acc: 0.9516, best val acc: 0.9599, best train loss: 0.1382  @ epoch 14651 )\n",
      "[Epoch: 14820] train loss: 0.1503, train acc: 0.9482, val loss: 0.1703, val acc: 0.9514  (best train acc: 0.9525, best val acc: 0.9599, best train loss: 0.1382  @ epoch 14651 )\n",
      "[Epoch: 14840] train loss: 0.1615, train acc: 0.9449, val loss: 0.1687, val acc: 0.9531  (best train acc: 0.9525, best val acc: 0.9599, best train loss: 0.1382  @ epoch 14651 )\n",
      "[Epoch: 14860] train loss: 0.1463, train acc: 0.9494, val loss: 0.1720, val acc: 0.9491  (best train acc: 0.9525, best val acc: 0.9599, best train loss: 0.1382  @ epoch 14651 )\n",
      "[Epoch: 14880] train loss: 0.1664, train acc: 0.9412, val loss: 0.1772, val acc: 0.9450  (best train acc: 0.9527, best val acc: 0.9599, best train loss: 0.1382  @ epoch 14651 )\n",
      "[Epoch: 14900] train loss: 0.1649, train acc: 0.9413, val loss: 0.1787, val acc: 0.9474  (best train acc: 0.9527, best val acc: 0.9599, best train loss: 0.1382  @ epoch 14651 )\n",
      "[Epoch: 14920] train loss: 0.1791, train acc: 0.9352, val loss: 0.1774, val acc: 0.9454  (best train acc: 0.9527, best val acc: 0.9599, best train loss: 0.1382  @ epoch 14651 )\n",
      "[Epoch: 14940] train loss: 0.1547, train acc: 0.9467, val loss: 0.1720, val acc: 0.9541  (best train acc: 0.9527, best val acc: 0.9599, best train loss: 0.1382  @ epoch 14651 )\n",
      "[Epoch: 14960] train loss: 0.1503, train acc: 0.9485, val loss: 0.1697, val acc: 0.9514  (best train acc: 0.9527, best val acc: 0.9599, best train loss: 0.1382  @ epoch 14651 )\n",
      "[Epoch: 14980] train loss: 0.1455, train acc: 0.9488, val loss: 0.1685, val acc: 0.9501  (best train acc: 0.9527, best val acc: 0.9599, best train loss: 0.1382  @ epoch 14651 )\n",
      "[Epoch: 15000] train loss: 0.1596, train acc: 0.9454, val loss: 0.1638, val acc: 0.9528  (best train acc: 0.9527, best val acc: 0.9599, best train loss: 0.1382  @ epoch 14651 )\n",
      "[Epoch: 15020] train loss: 0.1463, train acc: 0.9466, val loss: 0.1706, val acc: 0.9474  (best train acc: 0.9527, best val acc: 0.9599, best train loss: 0.1382  @ epoch 14651 )\n",
      "[Epoch: 15040] train loss: 0.1525, train acc: 0.9457, val loss: 0.1704, val acc: 0.9501  (best train acc: 0.9527, best val acc: 0.9599, best train loss: 0.1382  @ epoch 14651 )\n",
      "[Epoch: 15060] train loss: 0.1531, train acc: 0.9445, val loss: 0.1870, val acc: 0.9430  (best train acc: 0.9527, best val acc: 0.9599, best train loss: 0.1382  @ epoch 14651 )\n",
      "[Epoch: 15080] train loss: 0.1397, train acc: 0.9524, val loss: 0.1632, val acc: 0.9545  (best train acc: 0.9527, best val acc: 0.9599, best train loss: 0.1382  @ epoch 14651 )\n",
      "[Epoch: 15100] train loss: 0.1562, train acc: 0.9446, val loss: 0.1656, val acc: 0.9555  (best train acc: 0.9527, best val acc: 0.9599, best train loss: 0.1382  @ epoch 14651 )\n",
      "[Epoch: 15120] train loss: 0.1537, train acc: 0.9431, val loss: 0.1699, val acc: 0.9481  (best train acc: 0.9527, best val acc: 0.9599, best train loss: 0.1382  @ epoch 14651 )\n",
      "[Epoch: 15140] train loss: 0.1446, train acc: 0.9501, val loss: 0.1712, val acc: 0.9521  (best train acc: 0.9527, best val acc: 0.9599, best train loss: 0.1382  @ epoch 14651 )\n",
      "[Epoch: 15160] train loss: 0.1590, train acc: 0.9440, val loss: 0.1832, val acc: 0.9457  (best train acc: 0.9527, best val acc: 0.9599, best train loss: 0.1382  @ epoch 14651 )\n",
      "[Epoch: 15180] train loss: 0.1823, train acc: 0.9363, val loss: 0.1795, val acc: 0.9484  (best train acc: 0.9527, best val acc: 0.9599, best train loss: 0.1382  @ epoch 14651 )\n",
      "[Epoch: 15200] train loss: 0.1553, train acc: 0.9460, val loss: 0.1751, val acc: 0.9514  (best train acc: 0.9527, best val acc: 0.9599, best train loss: 0.1382  @ epoch 14651 )\n",
      "[Epoch: 15220] train loss: 0.1388, train acc: 0.9501, val loss: 0.1723, val acc: 0.9508  (best train acc: 0.9527, best val acc: 0.9599, best train loss: 0.1382  @ epoch 14651 )\n",
      "[Epoch: 15240] train loss: 0.1457, train acc: 0.9501, val loss: 0.1695, val acc: 0.9531  (best train acc: 0.9527, best val acc: 0.9599, best train loss: 0.1382  @ epoch 14651 )\n",
      "[Epoch: 15260] train loss: 0.1557, train acc: 0.9456, val loss: 0.1886, val acc: 0.9437  (best train acc: 0.9527, best val acc: 0.9599, best train loss: 0.1382  @ epoch 14651 )\n",
      "[Epoch: 15280] train loss: 0.1444, train acc: 0.9486, val loss: 0.1745, val acc: 0.9568  (best train acc: 0.9527, best val acc: 0.9599, best train loss: 0.1382  @ epoch 14651 )\n",
      "[Epoch: 15300] train loss: 0.1605, train acc: 0.9400, val loss: 0.1771, val acc: 0.9511  (best train acc: 0.9527, best val acc: 0.9599, best train loss: 0.1382  @ epoch 14651 )\n",
      "[Epoch: 15320] train loss: 0.1426, train acc: 0.9479, val loss: 0.1695, val acc: 0.9535  (best train acc: 0.9527, best val acc: 0.9599, best train loss: 0.1382  @ epoch 14651 )\n",
      "[Epoch: 15340] train loss: 0.1387, train acc: 0.9506, val loss: 0.1722, val acc: 0.9548  (best train acc: 0.9527, best val acc: 0.9599, best train loss: 0.1382  @ epoch 14651 )\n",
      "[Epoch: 15360] train loss: 0.1685, train acc: 0.9388, val loss: 0.1779, val acc: 0.9406  (best train acc: 0.9527, best val acc: 0.9599, best train loss: 0.1382  @ epoch 14651 )\n",
      "[Epoch: 15380] train loss: 0.1659, train acc: 0.9397, val loss: 0.1867, val acc: 0.9437  (best train acc: 0.9527, best val acc: 0.9599, best train loss: 0.1382  @ epoch 14651 )\n",
      "[Epoch: 15400] train loss: 0.1467, train acc: 0.9486, val loss: 0.1691, val acc: 0.9494  (best train acc: 0.9527, best val acc: 0.9599, best train loss: 0.1382  @ epoch 14651 )\n",
      "[Epoch: 15420] train loss: 0.1503, train acc: 0.9448, val loss: 0.1908, val acc: 0.9433  (best train acc: 0.9527, best val acc: 0.9599, best train loss: 0.1382  @ epoch 14651 )\n",
      "[Epoch: 15440] train loss: 0.1539, train acc: 0.9464, val loss: 0.1795, val acc: 0.9541  (best train acc: 0.9527, best val acc: 0.9599, best train loss: 0.1382  @ epoch 14651 )\n",
      "[Epoch: 15460] train loss: 0.1676, train acc: 0.9381, val loss: 0.1778, val acc: 0.9467  (best train acc: 0.9527, best val acc: 0.9599, best train loss: 0.1382  @ epoch 14651 )\n",
      "[Epoch: 15480] train loss: 0.1469, train acc: 0.9476, val loss: 0.1790, val acc: 0.9565  (best train acc: 0.9527, best val acc: 0.9599, best train loss: 0.1382  @ epoch 14651 )\n",
      "[Epoch: 15500] train loss: 0.1469, train acc: 0.9483, val loss: 0.1739, val acc: 0.9470  (best train acc: 0.9527, best val acc: 0.9599, best train loss: 0.1382  @ epoch 14651 )\n",
      "[Epoch: 15520] train loss: 0.1467, train acc: 0.9500, val loss: 0.1734, val acc: 0.9501  (best train acc: 0.9527, best val acc: 0.9599, best train loss: 0.1358  @ epoch 15502 )\n",
      "[Epoch: 15540] train loss: 0.1705, train acc: 0.9355, val loss: 0.1926, val acc: 0.9430  (best train acc: 0.9527, best val acc: 0.9599, best train loss: 0.1358  @ epoch 15502 )\n",
      "[Epoch: 15560] train loss: 0.1472, train acc: 0.9481, val loss: 0.1707, val acc: 0.9531  (best train acc: 0.9527, best val acc: 0.9599, best train loss: 0.1358  @ epoch 15502 )\n",
      "[Epoch: 15580] train loss: 0.1430, train acc: 0.9515, val loss: 0.1724, val acc: 0.9545  (best train acc: 0.9527, best val acc: 0.9599, best train loss: 0.1358  @ epoch 15502 )\n",
      "[Epoch: 15600] train loss: 0.1508, train acc: 0.9496, val loss: 0.1781, val acc: 0.9531  (best train acc: 0.9527, best val acc: 0.9599, best train loss: 0.1358  @ epoch 15502 )\n",
      "[Epoch: 15620] train loss: 0.1464, train acc: 0.9472, val loss: 0.1779, val acc: 0.9528  (best train acc: 0.9527, best val acc: 0.9599, best train loss: 0.1358  @ epoch 15502 )\n",
      "[Epoch: 15640] train loss: 0.1390, train acc: 0.9499, val loss: 0.1853, val acc: 0.9437  (best train acc: 0.9535, best val acc: 0.9599, best train loss: 0.1358  @ epoch 15502 )\n",
      "[Epoch: 15660] train loss: 0.1506, train acc: 0.9472, val loss: 0.1790, val acc: 0.9474  (best train acc: 0.9535, best val acc: 0.9599, best train loss: 0.1358  @ epoch 15502 )\n",
      "[Epoch: 15680] train loss: 0.1758, train acc: 0.9396, val loss: 0.1848, val acc: 0.9501  (best train acc: 0.9535, best val acc: 0.9599, best train loss: 0.1358  @ epoch 15502 )\n",
      "[Epoch: 15700] train loss: 0.1546, train acc: 0.9466, val loss: 0.1750, val acc: 0.9531  (best train acc: 0.9535, best val acc: 0.9599, best train loss: 0.1358  @ epoch 15502 )\n",
      "[Epoch: 15720] train loss: 0.1886, train acc: 0.9317, val loss: 0.1838, val acc: 0.9444  (best train acc: 0.9535, best val acc: 0.9599, best train loss: 0.1358  @ epoch 15502 )\n",
      "[Epoch: 15740] train loss: 0.1417, train acc: 0.9505, val loss: 0.1732, val acc: 0.9524  (best train acc: 0.9535, best val acc: 0.9599, best train loss: 0.1358  @ epoch 15502 )\n",
      "[Epoch: 15760] train loss: 0.1408, train acc: 0.9511, val loss: 0.1788, val acc: 0.9504  (best train acc: 0.9535, best val acc: 0.9599, best train loss: 0.1358  @ epoch 15502 )\n",
      "[Epoch: 15780] train loss: 0.1789, train acc: 0.9344, val loss: 0.1895, val acc: 0.9403  (best train acc: 0.9535, best val acc: 0.9599, best train loss: 0.1358  @ epoch 15502 )\n",
      "[Epoch: 15800] train loss: 0.1593, train acc: 0.9419, val loss: 0.1782, val acc: 0.9518  (best train acc: 0.9535, best val acc: 0.9599, best train loss: 0.1358  @ epoch 15502 )\n",
      "[Epoch: 15820] train loss: 0.1670, train acc: 0.9438, val loss: 0.1924, val acc: 0.9440  (best train acc: 0.9535, best val acc: 0.9599, best train loss: 0.1358  @ epoch 15502 )\n",
      "[Epoch: 15840] train loss: 0.1422, train acc: 0.9500, val loss: 0.1770, val acc: 0.9535  (best train acc: 0.9535, best val acc: 0.9599, best train loss: 0.1358  @ epoch 15502 )\n",
      "[Epoch: 15860] train loss: 0.1435, train acc: 0.9499, val loss: 0.1819, val acc: 0.9528  (best train acc: 0.9535, best val acc: 0.9599, best train loss: 0.1358  @ epoch 15502 )\n",
      "[Epoch: 15880] train loss: 0.1631, train acc: 0.9424, val loss: 0.1832, val acc: 0.9474  (best train acc: 0.9535, best val acc: 0.9599, best train loss: 0.1341  @ epoch 15871 )\n",
      "[Epoch: 15900] train loss: 0.1491, train acc: 0.9465, val loss: 0.1759, val acc: 0.9535  (best train acc: 0.9535, best val acc: 0.9599, best train loss: 0.1341  @ epoch 15871 )\n",
      "[Epoch: 15920] train loss: 0.1376, train acc: 0.9516, val loss: 0.1752, val acc: 0.9518  (best train acc: 0.9535, best val acc: 0.9599, best train loss: 0.1341  @ epoch 15871 )\n",
      "[Epoch: 15940] train loss: 0.1412, train acc: 0.9529, val loss: 0.1852, val acc: 0.9474  (best train acc: 0.9535, best val acc: 0.9599, best train loss: 0.1341  @ epoch 15871 )\n",
      "[Epoch: 15960] train loss: 0.1717, train acc: 0.9318, val loss: 0.1962, val acc: 0.9272  (best train acc: 0.9538, best val acc: 0.9599, best train loss: 0.1339  @ epoch 15945 )\n",
      "[Epoch: 15980] train loss: 0.1521, train acc: 0.9468, val loss: 0.1757, val acc: 0.9531  (best train acc: 0.9538, best val acc: 0.9599, best train loss: 0.1339  @ epoch 15945 )\n",
      "[Epoch: 16000] train loss: 0.1414, train acc: 0.9532, val loss: 0.1778, val acc: 0.9508  (best train acc: 0.9538, best val acc: 0.9599, best train loss: 0.1339  @ epoch 15945 )\n",
      "[Epoch: 16020] train loss: 0.1426, train acc: 0.9479, val loss: 0.1777, val acc: 0.9521  (best train acc: 0.9538, best val acc: 0.9599, best train loss: 0.1339  @ epoch 15945 )\n",
      "[Epoch: 16040] train loss: 0.1453, train acc: 0.9505, val loss: 0.1985, val acc: 0.9440  (best train acc: 0.9538, best val acc: 0.9599, best train loss: 0.1339  @ epoch 15945 )\n",
      "[Epoch: 16060] train loss: 0.1447, train acc: 0.9457, val loss: 0.1824, val acc: 0.9518  (best train acc: 0.9549, best val acc: 0.9599, best train loss: 0.1339  @ epoch 15945 )\n",
      "[Epoch: 16080] train loss: 0.1480, train acc: 0.9479, val loss: 0.1738, val acc: 0.9511  (best train acc: 0.9549, best val acc: 0.9599, best train loss: 0.1339  @ epoch 15945 )\n",
      "[Epoch: 16100] train loss: 0.1431, train acc: 0.9495, val loss: 0.1763, val acc: 0.9558  (best train acc: 0.9549, best val acc: 0.9599, best train loss: 0.1339  @ epoch 15945 )\n",
      "[Epoch: 16120] train loss: 0.1698, train acc: 0.9409, val loss: 0.1860, val acc: 0.9494  (best train acc: 0.9549, best val acc: 0.9599, best train loss: 0.1339  @ epoch 15945 )\n",
      "[Epoch: 16140] train loss: 0.1741, train acc: 0.9373, val loss: 0.1866, val acc: 0.9474  (best train acc: 0.9549, best val acc: 0.9599, best train loss: 0.1339  @ epoch 15945 )\n",
      "[Epoch: 16160] train loss: 0.1548, train acc: 0.9458, val loss: 0.1992, val acc: 0.9481  (best train acc: 0.9549, best val acc: 0.9599, best train loss: 0.1339  @ epoch 15945 )\n",
      "[Epoch: 16180] train loss: 0.1447, train acc: 0.9472, val loss: 0.1834, val acc: 0.9521  (best train acc: 0.9549, best val acc: 0.9599, best train loss: 0.1339  @ epoch 15945 )\n",
      "[Epoch: 16200] train loss: 0.1492, train acc: 0.9451, val loss: 0.1761, val acc: 0.9562  (best train acc: 0.9549, best val acc: 0.9599, best train loss: 0.1339  @ epoch 15945 )\n",
      "[Epoch: 16220] train loss: 0.1521, train acc: 0.9464, val loss: 0.1816, val acc: 0.9514  (best train acc: 0.9549, best val acc: 0.9599, best train loss: 0.1339  @ epoch 15945 )\n",
      "[Epoch: 16240] train loss: 0.1643, train acc: 0.9394, val loss: 0.1871, val acc: 0.9494  (best train acc: 0.9549, best val acc: 0.9599, best train loss: 0.1339  @ epoch 15945 )\n",
      "[Epoch: 16260] train loss: 0.1525, train acc: 0.9469, val loss: 0.1831, val acc: 0.9487  (best train acc: 0.9549, best val acc: 0.9599, best train loss: 0.1339  @ epoch 15945 )\n",
      "[Epoch: 16280] train loss: 0.1541, train acc: 0.9466, val loss: 0.1966, val acc: 0.9427  (best train acc: 0.9549, best val acc: 0.9599, best train loss: 0.1339  @ epoch 15945 )\n",
      "[Epoch: 16300] train loss: 0.1469, train acc: 0.9451, val loss: 0.1799, val acc: 0.9497  (best train acc: 0.9549, best val acc: 0.9599, best train loss: 0.1339  @ epoch 15945 )\n",
      "[Epoch: 16320] train loss: 0.1585, train acc: 0.9417, val loss: 0.2002, val acc: 0.9464  (best train acc: 0.9549, best val acc: 0.9599, best train loss: 0.1339  @ epoch 15945 )\n",
      "[Epoch: 16340] train loss: 0.1731, train acc: 0.9435, val loss: 0.1775, val acc: 0.9528  (best train acc: 0.9549, best val acc: 0.9599, best train loss: 0.1339  @ epoch 15945 )\n",
      "[Epoch: 16360] train loss: 0.1477, train acc: 0.9496, val loss: 0.1817, val acc: 0.9521  (best train acc: 0.9549, best val acc: 0.9599, best train loss: 0.1339  @ epoch 15945 )\n",
      "[Epoch: 16380] train loss: 0.1559, train acc: 0.9442, val loss: 0.1814, val acc: 0.9470  (best train acc: 0.9549, best val acc: 0.9599, best train loss: 0.1339  @ epoch 15945 )\n",
      "[Epoch: 16400] train loss: 0.1463, train acc: 0.9494, val loss: 0.1833, val acc: 0.9511  (best train acc: 0.9549, best val acc: 0.9599, best train loss: 0.1339  @ epoch 15945 )\n",
      "[Epoch: 16420] train loss: 0.1666, train acc: 0.9410, val loss: 0.1856, val acc: 0.9433  (best train acc: 0.9549, best val acc: 0.9599, best train loss: 0.1339  @ epoch 15945 )\n",
      "[Epoch: 16440] train loss: 0.1557, train acc: 0.9454, val loss: 0.1793, val acc: 0.9538  (best train acc: 0.9549, best val acc: 0.9599, best train loss: 0.1339  @ epoch 15945 )\n",
      "[Epoch: 16460] train loss: 0.1595, train acc: 0.9422, val loss: 0.2147, val acc: 0.9295  (best train acc: 0.9549, best val acc: 0.9599, best train loss: 0.1339  @ epoch 15945 )\n",
      "[Epoch: 16480] train loss: 0.1507, train acc: 0.9459, val loss: 0.1841, val acc: 0.9477  (best train acc: 0.9549, best val acc: 0.9599, best train loss: 0.1339  @ epoch 15945 )\n",
      "[Epoch: 16500] train loss: 0.1720, train acc: 0.9348, val loss: 0.1894, val acc: 0.9460  (best train acc: 0.9549, best val acc: 0.9599, best train loss: 0.1339  @ epoch 15945 )\n",
      "[Epoch: 16520] train loss: 0.1456, train acc: 0.9481, val loss: 0.1800, val acc: 0.9508  (best train acc: 0.9549, best val acc: 0.9599, best train loss: 0.1339  @ epoch 15945 )\n",
      "[Epoch: 16540] train loss: 0.1550, train acc: 0.9474, val loss: 0.1803, val acc: 0.9545  (best train acc: 0.9549, best val acc: 0.9599, best train loss: 0.1339  @ epoch 15945 )\n",
      "[Epoch: 16560] train loss: 0.1578, train acc: 0.9438, val loss: 0.2117, val acc: 0.9383  (best train acc: 0.9549, best val acc: 0.9599, best train loss: 0.1339  @ epoch 15945 )\n",
      "[Epoch: 16580] train loss: 0.1417, train acc: 0.9492, val loss: 0.1796, val acc: 0.9497  (best train acc: 0.9549, best val acc: 0.9599, best train loss: 0.1339  @ epoch 15945 )\n",
      "[Epoch: 16600] train loss: 0.1518, train acc: 0.9437, val loss: 0.1776, val acc: 0.9514  (best train acc: 0.9549, best val acc: 0.9599, best train loss: 0.1324  @ epoch 16583 )\n",
      "[Epoch: 16620] train loss: 0.1433, train acc: 0.9480, val loss: 0.1788, val acc: 0.9514  (best train acc: 0.9549, best val acc: 0.9599, best train loss: 0.1324  @ epoch 16583 )\n",
      "[Epoch: 16640] train loss: 0.1435, train acc: 0.9512, val loss: 0.1776, val acc: 0.9531  (best train acc: 0.9549, best val acc: 0.9599, best train loss: 0.1324  @ epoch 16583 )\n",
      "[Epoch: 16660] train loss: 0.1432, train acc: 0.9495, val loss: 0.1932, val acc: 0.9454  (best train acc: 0.9549, best val acc: 0.9599, best train loss: 0.1324  @ epoch 16583 )\n",
      "[Epoch: 16680] train loss: 0.1328, train acc: 0.9539, val loss: 0.1838, val acc: 0.9531  (best train acc: 0.9549, best val acc: 0.9599, best train loss: 0.1324  @ epoch 16583 )\n",
      "[Epoch: 16700] train loss: 0.1578, train acc: 0.9423, val loss: 0.1872, val acc: 0.9497  (best train acc: 0.9549, best val acc: 0.9599, best train loss: 0.1324  @ epoch 16583 )\n",
      "[Epoch: 16720] train loss: 0.1464, train acc: 0.9464, val loss: 0.1852, val acc: 0.9524  (best train acc: 0.9549, best val acc: 0.9599, best train loss: 0.1324  @ epoch 16583 )\n",
      "[Epoch: 16740] train loss: 0.1451, train acc: 0.9491, val loss: 0.1855, val acc: 0.9504  (best train acc: 0.9549, best val acc: 0.9599, best train loss: 0.1324  @ epoch 16583 )\n",
      "[Epoch: 16760] train loss: 0.1347, train acc: 0.9515, val loss: 0.1851, val acc: 0.9501  (best train acc: 0.9549, best val acc: 0.9599, best train loss: 0.1324  @ epoch 16583 )\n",
      "[Epoch: 16780] train loss: 0.1469, train acc: 0.9481, val loss: 0.1880, val acc: 0.9528  (best train acc: 0.9549, best val acc: 0.9599, best train loss: 0.1324  @ epoch 16583 )\n",
      "[Epoch: 16800] train loss: 0.1504, train acc: 0.9477, val loss: 0.1812, val acc: 0.9487  (best train acc: 0.9549, best val acc: 0.9599, best train loss: 0.1324  @ epoch 16583 )\n",
      "[Epoch: 16820] train loss: 0.1478, train acc: 0.9486, val loss: 0.1819, val acc: 0.9531  (best train acc: 0.9549, best val acc: 0.9599, best train loss: 0.1324  @ epoch 16583 )\n",
      "[Epoch: 16840] train loss: 0.1379, train acc: 0.9526, val loss: 0.1753, val acc: 0.9541  (best train acc: 0.9549, best val acc: 0.9599, best train loss: 0.1324  @ epoch 16583 )\n",
      "[Epoch: 16860] train loss: 0.1483, train acc: 0.9457, val loss: 0.1970, val acc: 0.9457  (best train acc: 0.9549, best val acc: 0.9599, best train loss: 0.1324  @ epoch 16583 )\n",
      "[Epoch: 16880] train loss: 0.1521, train acc: 0.9484, val loss: 0.1756, val acc: 0.9531  (best train acc: 0.9549, best val acc: 0.9599, best train loss: 0.1324  @ epoch 16583 )\n",
      "[Epoch: 16900] train loss: 0.1369, train acc: 0.9513, val loss: 0.1987, val acc: 0.9454  (best train acc: 0.9549, best val acc: 0.9599, best train loss: 0.1324  @ epoch 16583 )\n",
      "[Epoch: 16920] train loss: 0.1456, train acc: 0.9472, val loss: 0.1913, val acc: 0.9470  (best train acc: 0.9549, best val acc: 0.9599, best train loss: 0.1324  @ epoch 16583 )\n",
      "[Epoch: 16940] train loss: 0.1433, train acc: 0.9500, val loss: 0.1799, val acc: 0.9541  (best train acc: 0.9549, best val acc: 0.9599, best train loss: 0.1324  @ epoch 16583 )\n",
      "[Epoch: 16960] train loss: 0.1409, train acc: 0.9510, val loss: 0.1810, val acc: 0.9538  (best train acc: 0.9552, best val acc: 0.9599, best train loss: 0.1324  @ epoch 16583 )\n",
      "[Epoch: 16980] train loss: 0.1516, train acc: 0.9454, val loss: 0.1829, val acc: 0.9501  (best train acc: 0.9552, best val acc: 0.9599, best train loss: 0.1324  @ epoch 16583 )\n",
      "[Epoch: 17000] train loss: 0.1413, train acc: 0.9508, val loss: 0.1767, val acc: 0.9518  (best train acc: 0.9552, best val acc: 0.9599, best train loss: 0.1324  @ epoch 16583 )\n",
      "[Epoch: 17020] train loss: 0.1453, train acc: 0.9474, val loss: 0.1907, val acc: 0.9427  (best train acc: 0.9552, best val acc: 0.9599, best train loss: 0.1324  @ epoch 16583 )\n",
      "[Epoch: 17040] train loss: 0.1496, train acc: 0.9453, val loss: 0.1882, val acc: 0.9501  (best train acc: 0.9552, best val acc: 0.9599, best train loss: 0.1324  @ epoch 16583 )\n",
      "[Epoch: 17060] train loss: 0.1348, train acc: 0.9534, val loss: 0.1840, val acc: 0.9545  (best train acc: 0.9552, best val acc: 0.9599, best train loss: 0.1324  @ epoch 16583 )\n",
      "[Epoch: 17080] train loss: 0.1468, train acc: 0.9490, val loss: 0.1944, val acc: 0.9437  (best train acc: 0.9552, best val acc: 0.9599, best train loss: 0.1324  @ epoch 16583 )\n",
      "[Epoch: 17100] train loss: 0.1879, train acc: 0.9291, val loss: 0.1877, val acc: 0.9464  (best train acc: 0.9552, best val acc: 0.9599, best train loss: 0.1324  @ epoch 16583 )\n",
      "[Epoch: 17120] train loss: 0.1388, train acc: 0.9523, val loss: 0.1839, val acc: 0.9518  (best train acc: 0.9552, best val acc: 0.9599, best train loss: 0.1324  @ epoch 16583 )\n",
      "[Epoch: 17140] train loss: 0.1404, train acc: 0.9521, val loss: 0.1814, val acc: 0.9501  (best train acc: 0.9552, best val acc: 0.9599, best train loss: 0.1324  @ epoch 16583 )\n",
      "[Epoch: 17160] train loss: 0.1469, train acc: 0.9500, val loss: 0.1954, val acc: 0.9497  (best train acc: 0.9552, best val acc: 0.9599, best train loss: 0.1324  @ epoch 16583 )\n",
      "[Epoch: 17180] train loss: 0.1479, train acc: 0.9511, val loss: 0.1885, val acc: 0.9484  (best train acc: 0.9559, best val acc: 0.9599, best train loss: 0.1307  @ epoch 17179 )\n",
      "[Epoch: 17200] train loss: 0.1301, train acc: 0.9558, val loss: 0.1783, val acc: 0.9521  (best train acc: 0.9559, best val acc: 0.9599, best train loss: 0.1301  @ epoch 17200 )\n",
      "[Epoch: 17220] train loss: 0.1496, train acc: 0.9455, val loss: 0.1844, val acc: 0.9511  (best train acc: 0.9568, best val acc: 0.9599, best train loss: 0.1301  @ epoch 17200 )\n",
      "[Epoch: 17240] train loss: 0.1643, train acc: 0.9372, val loss: 0.1935, val acc: 0.9420  (best train acc: 0.9568, best val acc: 0.9599, best train loss: 0.1301  @ epoch 17200 )\n",
      "[Epoch: 17260] train loss: 0.1375, train acc: 0.9522, val loss: 0.1865, val acc: 0.9497  (best train acc: 0.9568, best val acc: 0.9599, best train loss: 0.1301  @ epoch 17200 )\n",
      "[Epoch: 17280] train loss: 0.1566, train acc: 0.9440, val loss: 0.1768, val acc: 0.9497  (best train acc: 0.9568, best val acc: 0.9599, best train loss: 0.1301  @ epoch 17200 )\n",
      "[Epoch: 17300] train loss: 0.1543, train acc: 0.9461, val loss: 0.1921, val acc: 0.9467  (best train acc: 0.9568, best val acc: 0.9599, best train loss: 0.1301  @ epoch 17200 )\n",
      "[Epoch: 17320] train loss: 0.1355, train acc: 0.9520, val loss: 0.1834, val acc: 0.9541  (best train acc: 0.9568, best val acc: 0.9599, best train loss: 0.1301  @ epoch 17200 )\n",
      "[Epoch: 17340] train loss: 0.1408, train acc: 0.9516, val loss: 0.1768, val acc: 0.9524  (best train acc: 0.9568, best val acc: 0.9599, best train loss: 0.1293  @ epoch 17330 )\n",
      "[Epoch: 17360] train loss: 0.1471, train acc: 0.9474, val loss: 0.1815, val acc: 0.9541  (best train acc: 0.9568, best val acc: 0.9599, best train loss: 0.1293  @ epoch 17330 )\n",
      "[Epoch: 17380] train loss: 0.1411, train acc: 0.9507, val loss: 0.1864, val acc: 0.9538  (best train acc: 0.9568, best val acc: 0.9599, best train loss: 0.1293  @ epoch 17330 )\n",
      "[Epoch: 17400] train loss: 0.1551, train acc: 0.9453, val loss: 0.1876, val acc: 0.9528  (best train acc: 0.9568, best val acc: 0.9599, best train loss: 0.1293  @ epoch 17330 )\n",
      "[Epoch: 17420] train loss: 0.1420, train acc: 0.9506, val loss: 0.1834, val acc: 0.9535  (best train acc: 0.9568, best val acc: 0.9599, best train loss: 0.1293  @ epoch 17330 )\n",
      "[Epoch: 17440] train loss: 0.1554, train acc: 0.9456, val loss: 0.1876, val acc: 0.9538  (best train acc: 0.9568, best val acc: 0.9599, best train loss: 0.1293  @ epoch 17330 )\n",
      "[Epoch: 17460] train loss: 0.1510, train acc: 0.9451, val loss: 0.2187, val acc: 0.9447  (best train acc: 0.9568, best val acc: 0.9599, best train loss: 0.1293  @ epoch 17330 )\n",
      "[Epoch: 17480] train loss: 0.1496, train acc: 0.9475, val loss: 0.2049, val acc: 0.9481  (best train acc: 0.9568, best val acc: 0.9599, best train loss: 0.1293  @ epoch 17330 )\n",
      "[Epoch: 17500] train loss: 0.1490, train acc: 0.9472, val loss: 0.1848, val acc: 0.9484  (best train acc: 0.9568, best val acc: 0.9599, best train loss: 0.1293  @ epoch 17330 )\n",
      "[Epoch: 17520] train loss: 0.1407, train acc: 0.9518, val loss: 0.1855, val acc: 0.9514  (best train acc: 0.9568, best val acc: 0.9599, best train loss: 0.1293  @ epoch 17330 )\n",
      "[Epoch: 17540] train loss: 0.1367, train acc: 0.9543, val loss: 0.1901, val acc: 0.9551  (best train acc: 0.9568, best val acc: 0.9599, best train loss: 0.1293  @ epoch 17330 )\n",
      "[Epoch: 17560] train loss: 0.1643, train acc: 0.9433, val loss: 0.2051, val acc: 0.9410  (best train acc: 0.9568, best val acc: 0.9599, best train loss: 0.1293  @ epoch 17330 )\n",
      "[Epoch: 17580] train loss: 0.1539, train acc: 0.9479, val loss: 0.1824, val acc: 0.9531  (best train acc: 0.9568, best val acc: 0.9599, best train loss: 0.1293  @ epoch 17330 )\n",
      "[Epoch: 17600] train loss: 0.1660, train acc: 0.9430, val loss: 0.2216, val acc: 0.9430  (best train acc: 0.9568, best val acc: 0.9599, best train loss: 0.1293  @ epoch 17330 )\n",
      "[Epoch: 17620] train loss: 0.1524, train acc: 0.9453, val loss: 0.1853, val acc: 0.9501  (best train acc: 0.9568, best val acc: 0.9599, best train loss: 0.1293  @ epoch 17330 )\n",
      "[Epoch: 17640] train loss: 0.1439, train acc: 0.9514, val loss: 0.1897, val acc: 0.9484  (best train acc: 0.9568, best val acc: 0.9599, best train loss: 0.1293  @ epoch 17330 )\n",
      "[Epoch: 17660] train loss: 0.1389, train acc: 0.9522, val loss: 0.1807, val acc: 0.9555  (best train acc: 0.9568, best val acc: 0.9599, best train loss: 0.1293  @ epoch 17330 )\n",
      "[Epoch: 17680] train loss: 0.1464, train acc: 0.9492, val loss: 0.1892, val acc: 0.9508  (best train acc: 0.9568, best val acc: 0.9599, best train loss: 0.1293  @ epoch 17330 )\n",
      "[Epoch: 17700] train loss: 0.1394, train acc: 0.9503, val loss: 0.1892, val acc: 0.9504  (best train acc: 0.9568, best val acc: 0.9599, best train loss: 0.1293  @ epoch 17330 )\n",
      "[Epoch: 17720] train loss: 0.1506, train acc: 0.9482, val loss: 0.1982, val acc: 0.9494  (best train acc: 0.9568, best val acc: 0.9599, best train loss: 0.1293  @ epoch 17330 )\n",
      "[Epoch: 17740] train loss: 0.1362, train acc: 0.9537, val loss: 0.1823, val acc: 0.9531  (best train acc: 0.9568, best val acc: 0.9599, best train loss: 0.1293  @ epoch 17330 )\n",
      "[Epoch: 17760] train loss: 0.1531, train acc: 0.9469, val loss: 0.1952, val acc: 0.9447  (best train acc: 0.9568, best val acc: 0.9599, best train loss: 0.1293  @ epoch 17330 )\n",
      "[Epoch: 17780] train loss: 0.1327, train acc: 0.9549, val loss: 0.1824, val acc: 0.9575  (best train acc: 0.9568, best val acc: 0.9599, best train loss: 0.1293  @ epoch 17330 )\n",
      "[Epoch: 17800] train loss: 0.1395, train acc: 0.9500, val loss: 0.1828, val acc: 0.9548  (best train acc: 0.9568, best val acc: 0.9599, best train loss: 0.1281  @ epoch 17795 )\n",
      "[Epoch: 17820] train loss: 0.1421, train acc: 0.9523, val loss: 0.1915, val acc: 0.9508  (best train acc: 0.9568, best val acc: 0.9599, best train loss: 0.1281  @ epoch 17795 )\n",
      "[Epoch: 17840] train loss: 0.1341, train acc: 0.9551, val loss: 0.1844, val acc: 0.9548  (best train acc: 0.9568, best val acc: 0.9599, best train loss: 0.1281  @ epoch 17795 )\n",
      "[Epoch: 17860] train loss: 0.1375, train acc: 0.9518, val loss: 0.2000, val acc: 0.9481  (best train acc: 0.9568, best val acc: 0.9599, best train loss: 0.1281  @ epoch 17795 )\n",
      "[Epoch: 17880] train loss: 0.1380, train acc: 0.9557, val loss: 0.1852, val acc: 0.9555  (best train acc: 0.9568, best val acc: 0.9599, best train loss: 0.1281  @ epoch 17795 )\n",
      "[Epoch: 17900] train loss: 0.1658, train acc: 0.9374, val loss: 0.2054, val acc: 0.9359  (best train acc: 0.9568, best val acc: 0.9599, best train loss: 0.1281  @ epoch 17795 )\n",
      "[Epoch: 17920] train loss: 0.1499, train acc: 0.9492, val loss: 0.1988, val acc: 0.9457  (best train acc: 0.9568, best val acc: 0.9599, best train loss: 0.1281  @ epoch 17795 )\n",
      "[Epoch: 17940] train loss: 0.1722, train acc: 0.9336, val loss: 0.2092, val acc: 0.9302  (best train acc: 0.9568, best val acc: 0.9599, best train loss: 0.1281  @ epoch 17795 )\n",
      "[Epoch: 17960] train loss: 0.1500, train acc: 0.9501, val loss: 0.2071, val acc: 0.9440  (best train acc: 0.9568, best val acc: 0.9599, best train loss: 0.1281  @ epoch 17795 )\n",
      "[Epoch: 17980] train loss: 0.1475, train acc: 0.9487, val loss: 0.1946, val acc: 0.9538  (best train acc: 0.9568, best val acc: 0.9599, best train loss: 0.1281  @ epoch 17795 )\n",
      "[Epoch: 18000] train loss: 0.1358, train acc: 0.9508, val loss: 0.1828, val acc: 0.9572  (best train acc: 0.9568, best val acc: 0.9599, best train loss: 0.1281  @ epoch 17795 )\n",
      "[Epoch: 18020] train loss: 0.1463, train acc: 0.9505, val loss: 0.2049, val acc: 0.9487  (best train acc: 0.9568, best val acc: 0.9599, best train loss: 0.1281  @ epoch 17795 )\n",
      "[Epoch: 18040] train loss: 0.1347, train acc: 0.9551, val loss: 0.1918, val acc: 0.9568  (best train acc: 0.9568, best val acc: 0.9599, best train loss: 0.1281  @ epoch 17795 )\n",
      "[Epoch: 18060] train loss: 0.1384, train acc: 0.9507, val loss: 0.1979, val acc: 0.9487  (best train acc: 0.9568, best val acc: 0.9599, best train loss: 0.1281  @ epoch 17795 )\n",
      "[Epoch: 18080] train loss: 0.1359, train acc: 0.9516, val loss: 0.1976, val acc: 0.9477  (best train acc: 0.9568, best val acc: 0.9599, best train loss: 0.1281  @ epoch 17795 )\n",
      "[Epoch: 18100] train loss: 0.1519, train acc: 0.9463, val loss: 0.1893, val acc: 0.9558  (best train acc: 0.9568, best val acc: 0.9599, best train loss: 0.1281  @ epoch 17795 )\n",
      "[Epoch: 18120] train loss: 0.1438, train acc: 0.9513, val loss: 0.1951, val acc: 0.9501  (best train acc: 0.9568, best val acc: 0.9599, best train loss: 0.1276  @ epoch 18101 )\n",
      "[Epoch: 18140] train loss: 0.1393, train acc: 0.9523, val loss: 0.1934, val acc: 0.9528  (best train acc: 0.9568, best val acc: 0.9599, best train loss: 0.1276  @ epoch 18101 )\n",
      "[Epoch: 18160] train loss: 0.1353, train acc: 0.9537, val loss: 0.1927, val acc: 0.9551  (best train acc: 0.9568, best val acc: 0.9599, best train loss: 0.1276  @ epoch 18101 )\n",
      "[Epoch: 18180] train loss: 0.1549, train acc: 0.9411, val loss: 0.1944, val acc: 0.9528  (best train acc: 0.9568, best val acc: 0.9599, best train loss: 0.1276  @ epoch 18101 )\n",
      "[Epoch: 18200] train loss: 0.1458, train acc: 0.9500, val loss: 0.1849, val acc: 0.9511  (best train acc: 0.9568, best val acc: 0.9599, best train loss: 0.1276  @ epoch 18101 )\n",
      "[Epoch: 18220] train loss: 0.1284, train acc: 0.9547, val loss: 0.1857, val acc: 0.9565  (best train acc: 0.9568, best val acc: 0.9599, best train loss: 0.1276  @ epoch 18101 )\n",
      "[Epoch: 18240] train loss: 0.1371, train acc: 0.9538, val loss: 0.1947, val acc: 0.9514  (best train acc: 0.9568, best val acc: 0.9599, best train loss: 0.1276  @ epoch 18101 )\n",
      "[Epoch: 18260] train loss: 0.1552, train acc: 0.9487, val loss: 0.2033, val acc: 0.9444  (best train acc: 0.9568, best val acc: 0.9599, best train loss: 0.1276  @ epoch 18101 )\n",
      "[Epoch: 18280] train loss: 0.1649, train acc: 0.9426, val loss: 0.2216, val acc: 0.9396  (best train acc: 0.9568, best val acc: 0.9599, best train loss: 0.1276  @ epoch 18101 )\n",
      "[Epoch: 18300] train loss: 0.1618, train acc: 0.9419, val loss: 0.2733, val acc: 0.9170  (best train acc: 0.9568, best val acc: 0.9599, best train loss: 0.1276  @ epoch 18101 )\n",
      "[Epoch: 18320] train loss: 0.1781, train acc: 0.9417, val loss: 0.1751, val acc: 0.9511  (best train acc: 0.9568, best val acc: 0.9599, best train loss: 0.1276  @ epoch 18101 )\n",
      "[Epoch: 18340] train loss: 0.1501, train acc: 0.9471, val loss: 0.2013, val acc: 0.9454  (best train acc: 0.9568, best val acc: 0.9599, best train loss: 0.1276  @ epoch 18101 )\n",
      "[Epoch: 18360] train loss: 0.1467, train acc: 0.9511, val loss: 0.1862, val acc: 0.9551  (best train acc: 0.9568, best val acc: 0.9599, best train loss: 0.1276  @ epoch 18101 )\n",
      "[Epoch: 18380] train loss: 0.1429, train acc: 0.9493, val loss: 0.1854, val acc: 0.9548  (best train acc: 0.9568, best val acc: 0.9599, best train loss: 0.1276  @ epoch 18101 )\n",
      "[Epoch: 18400] train loss: 0.1374, train acc: 0.9521, val loss: 0.1871, val acc: 0.9504  (best train acc: 0.9568, best val acc: 0.9599, best train loss: 0.1276  @ epoch 18101 )\n",
      "[Epoch: 18420] train loss: 0.1560, train acc: 0.9454, val loss: 0.1816, val acc: 0.9541  (best train acc: 0.9568, best val acc: 0.9599, best train loss: 0.1276  @ epoch 18101 )\n",
      "[Epoch: 18440] train loss: 0.1426, train acc: 0.9519, val loss: 0.1902, val acc: 0.9518  (best train acc: 0.9568, best val acc: 0.9599, best train loss: 0.1276  @ epoch 18101 )\n",
      "[Epoch: 18460] train loss: 0.1412, train acc: 0.9503, val loss: 0.2040, val acc: 0.9470  (best train acc: 0.9568, best val acc: 0.9599, best train loss: 0.1276  @ epoch 18101 )\n",
      "[Epoch: 18480] train loss: 0.1358, train acc: 0.9526, val loss: 0.1988, val acc: 0.9450  (best train acc: 0.9568, best val acc: 0.9599, best train loss: 0.1276  @ epoch 18101 )\n",
      "[Epoch: 18500] train loss: 0.1376, train acc: 0.9533, val loss: 0.1847, val acc: 0.9504  (best train acc: 0.9568, best val acc: 0.9599, best train loss: 0.1276  @ epoch 18101 )\n",
      "[Epoch: 18520] train loss: 0.1374, train acc: 0.9523, val loss: 0.1899, val acc: 0.9508  (best train acc: 0.9568, best val acc: 0.9599, best train loss: 0.1276  @ epoch 18101 )\n",
      "[Epoch: 18540] train loss: 0.1402, train acc: 0.9499, val loss: 0.1934, val acc: 0.9511  (best train acc: 0.9568, best val acc: 0.9599, best train loss: 0.1276  @ epoch 18101 )\n",
      "[Epoch: 18560] train loss: 0.1577, train acc: 0.9450, val loss: 0.1940, val acc: 0.9538  (best train acc: 0.9568, best val acc: 0.9599, best train loss: 0.1276  @ epoch 18101 )\n",
      "[Epoch: 18580] train loss: 0.1424, train acc: 0.9469, val loss: 0.1927, val acc: 0.9535  (best train acc: 0.9568, best val acc: 0.9599, best train loss: 0.1275  @ epoch 18570 )\n",
      "[Epoch: 18600] train loss: 0.1354, train acc: 0.9528, val loss: 0.1888, val acc: 0.9538  (best train acc: 0.9568, best val acc: 0.9599, best train loss: 0.1275  @ epoch 18570 )\n",
      "[Epoch: 18620] train loss: 0.1337, train acc: 0.9529, val loss: 0.1924, val acc: 0.9514  (best train acc: 0.9568, best val acc: 0.9599, best train loss: 0.1275  @ epoch 18570 )\n",
      "[Epoch: 18640] train loss: 0.1498, train acc: 0.9472, val loss: 0.1930, val acc: 0.9524  (best train acc: 0.9568, best val acc: 0.9599, best train loss: 0.1275  @ epoch 18570 )\n",
      "[Epoch: 18660] train loss: 0.1358, train acc: 0.9517, val loss: 0.1881, val acc: 0.9535  (best train acc: 0.9568, best val acc: 0.9599, best train loss: 0.1275  @ epoch 18570 )\n",
      "[Epoch: 18680] train loss: 0.1465, train acc: 0.9499, val loss: 0.2273, val acc: 0.9386  (best train acc: 0.9568, best val acc: 0.9599, best train loss: 0.1275  @ epoch 18570 )\n",
      "[Epoch: 18700] train loss: 0.1349, train acc: 0.9527, val loss: 0.1911, val acc: 0.9497  (best train acc: 0.9568, best val acc: 0.9599, best train loss: 0.1275  @ epoch 18570 )\n",
      "[Epoch: 18720] train loss: 0.1578, train acc: 0.9365, val loss: 0.2047, val acc: 0.9420  (best train acc: 0.9568, best val acc: 0.9599, best train loss: 0.1275  @ epoch 18570 )\n",
      "[Epoch: 18740] train loss: 0.1336, train acc: 0.9523, val loss: 0.1904, val acc: 0.9518  (best train acc: 0.9568, best val acc: 0.9599, best train loss: 0.1275  @ epoch 18570 )\n",
      "[Epoch: 18760] train loss: 0.1722, train acc: 0.9346, val loss: 0.2765, val acc: 0.9029  (best train acc: 0.9568, best val acc: 0.9599, best train loss: 0.1275  @ epoch 18570 )\n",
      "[Epoch: 18780] train loss: 0.1538, train acc: 0.9469, val loss: 0.1841, val acc: 0.9528  (best train acc: 0.9568, best val acc: 0.9599, best train loss: 0.1275  @ epoch 18570 )\n",
      "[Epoch: 18800] train loss: 0.1430, train acc: 0.9490, val loss: 0.1956, val acc: 0.9508  (best train acc: 0.9568, best val acc: 0.9599, best train loss: 0.1275  @ epoch 18570 )\n",
      "[Epoch: 18820] train loss: 0.1316, train acc: 0.9551, val loss: 0.1883, val acc: 0.9535  (best train acc: 0.9568, best val acc: 0.9599, best train loss: 0.1254  @ epoch 18808 )\n",
      "[Epoch: 18840] train loss: 0.1315, train acc: 0.9538, val loss: 0.1903, val acc: 0.9551  (best train acc: 0.9568, best val acc: 0.9599, best train loss: 0.1254  @ epoch 18808 )\n",
      "[Epoch: 18860] train loss: 0.2271, train acc: 0.9244, val loss: 0.2259, val acc: 0.9234  (best train acc: 0.9578, best val acc: 0.9599, best train loss: 0.1246  @ epoch 18852 )\n",
      "[Epoch: 18880] train loss: 0.1800, train acc: 0.9393, val loss: 0.2097, val acc: 0.9481  (best train acc: 0.9578, best val acc: 0.9599, best train loss: 0.1246  @ epoch 18852 )\n",
      "[Epoch: 18900] train loss: 0.1611, train acc: 0.9496, val loss: 0.2082, val acc: 0.9511  (best train acc: 0.9578, best val acc: 0.9599, best train loss: 0.1246  @ epoch 18852 )\n",
      "[Epoch: 18920] train loss: 0.1558, train acc: 0.9460, val loss: 0.1961, val acc: 0.9541  (best train acc: 0.9578, best val acc: 0.9599, best train loss: 0.1246  @ epoch 18852 )\n",
      "[Epoch: 18940] train loss: 0.1419, train acc: 0.9515, val loss: 0.1896, val acc: 0.9551  (best train acc: 0.9578, best val acc: 0.9599, best train loss: 0.1246  @ epoch 18852 )\n",
      "[Epoch: 18960] train loss: 0.1523, train acc: 0.9456, val loss: 0.2031, val acc: 0.9504  (best train acc: 0.9578, best val acc: 0.9599, best train loss: 0.1246  @ epoch 18852 )\n",
      "[Epoch: 18980] train loss: 0.1413, train acc: 0.9506, val loss: 0.1878, val acc: 0.9531  (best train acc: 0.9578, best val acc: 0.9599, best train loss: 0.1246  @ epoch 18852 )\n",
      "[Epoch: 19000] train loss: 0.1464, train acc: 0.9466, val loss: 0.2179, val acc: 0.9400  (best train acc: 0.9578, best val acc: 0.9599, best train loss: 0.1246  @ epoch 18852 )\n",
      "[Epoch: 19020] train loss: 0.1394, train acc: 0.9522, val loss: 0.1889, val acc: 0.9535  (best train acc: 0.9578, best val acc: 0.9599, best train loss: 0.1246  @ epoch 18852 )\n",
      "[Epoch: 19040] train loss: 0.1479, train acc: 0.9475, val loss: 0.2055, val acc: 0.9484  (best train acc: 0.9578, best val acc: 0.9599, best train loss: 0.1246  @ epoch 18852 )\n",
      "[Epoch: 19060] train loss: 0.1485, train acc: 0.9495, val loss: 0.2001, val acc: 0.9545  (best train acc: 0.9578, best val acc: 0.9599, best train loss: 0.1246  @ epoch 18852 )\n",
      "[Epoch: 19080] train loss: 0.1380, train acc: 0.9516, val loss: 0.1918, val acc: 0.9511  (best train acc: 0.9578, best val acc: 0.9599, best train loss: 0.1246  @ epoch 18852 )\n",
      "[Epoch: 19100] train loss: 0.1590, train acc: 0.9472, val loss: 0.2059, val acc: 0.9467  (best train acc: 0.9578, best val acc: 0.9599, best train loss: 0.1246  @ epoch 18852 )\n",
      "[Epoch: 19120] train loss: 0.1525, train acc: 0.9478, val loss: 0.2062, val acc: 0.9470  (best train acc: 0.9578, best val acc: 0.9599, best train loss: 0.1246  @ epoch 18852 )\n",
      "[Epoch: 19140] train loss: 0.1413, train acc: 0.9491, val loss: 0.1984, val acc: 0.9501  (best train acc: 0.9578, best val acc: 0.9599, best train loss: 0.1246  @ epoch 18852 )\n",
      "[Epoch: 19160] train loss: 0.1451, train acc: 0.9477, val loss: 0.1971, val acc: 0.9494  (best train acc: 0.9578, best val acc: 0.9599, best train loss: 0.1246  @ epoch 18852 )\n",
      "[Epoch: 19180] train loss: 0.1381, train acc: 0.9532, val loss: 0.1875, val acc: 0.9528  (best train acc: 0.9578, best val acc: 0.9599, best train loss: 0.1246  @ epoch 18852 )\n",
      "[Epoch: 19200] train loss: 0.1431, train acc: 0.9495, val loss: 0.1929, val acc: 0.9511  (best train acc: 0.9578, best val acc: 0.9599, best train loss: 0.1246  @ epoch 18852 )\n",
      "[Epoch: 19220] train loss: 0.1406, train acc: 0.9524, val loss: 0.1863, val acc: 0.9555  (best train acc: 0.9578, best val acc: 0.9599, best train loss: 0.1246  @ epoch 18852 )\n",
      "[Epoch: 19240] train loss: 0.1373, train acc: 0.9514, val loss: 0.2036, val acc: 0.9531  (best train acc: 0.9578, best val acc: 0.9599, best train loss: 0.1246  @ epoch 18852 )\n",
      "[Epoch: 19260] train loss: 0.1459, train acc: 0.9529, val loss: 0.2234, val acc: 0.9433  (best train acc: 0.9578, best val acc: 0.9599, best train loss: 0.1246  @ epoch 18852 )\n",
      "[Epoch: 19280] train loss: 0.1615, train acc: 0.9458, val loss: 0.1929, val acc: 0.9504  (best train acc: 0.9578, best val acc: 0.9599, best train loss: 0.1246  @ epoch 18852 )\n",
      "[Epoch: 19300] train loss: 0.1387, train acc: 0.9520, val loss: 0.1835, val acc: 0.9531  (best train acc: 0.9578, best val acc: 0.9599, best train loss: 0.1246  @ epoch 18852 )\n",
      "[Epoch: 19320] train loss: 0.1325, train acc: 0.9543, val loss: 0.1991, val acc: 0.9521  (best train acc: 0.9578, best val acc: 0.9599, best train loss: 0.1246  @ epoch 18852 )\n",
      "[Epoch: 19340] train loss: 0.1486, train acc: 0.9440, val loss: 0.2046, val acc: 0.9487  (best train acc: 0.9578, best val acc: 0.9599, best train loss: 0.1246  @ epoch 18852 )\n",
      "[Epoch: 19360] train loss: 0.1347, train acc: 0.9521, val loss: 0.1955, val acc: 0.9487  (best train acc: 0.9578, best val acc: 0.9599, best train loss: 0.1246  @ epoch 18852 )\n",
      "[Epoch: 19380] train loss: 0.1335, train acc: 0.9529, val loss: 0.2082, val acc: 0.9484  (best train acc: 0.9578, best val acc: 0.9599, best train loss: 0.1246  @ epoch 18852 )\n",
      "[Epoch: 19400] train loss: 0.1421, train acc: 0.9526, val loss: 0.1943, val acc: 0.9521  (best train acc: 0.9578, best val acc: 0.9599, best train loss: 0.1243  @ epoch 19391 )\n",
      "[Epoch: 19420] train loss: 0.1332, train acc: 0.9529, val loss: 0.2038, val acc: 0.9494  (best train acc: 0.9578, best val acc: 0.9599, best train loss: 0.1243  @ epoch 19391 )\n",
      "[Epoch: 19440] train loss: 0.1376, train acc: 0.9521, val loss: 0.2010, val acc: 0.9535  (best train acc: 0.9586, best val acc: 0.9599, best train loss: 0.1243  @ epoch 19391 )\n",
      "[Epoch: 19460] train loss: 0.1535, train acc: 0.9461, val loss: 0.1819, val acc: 0.9521  (best train acc: 0.9586, best val acc: 0.9599, best train loss: 0.1243  @ epoch 19391 )\n",
      "[Epoch: 19480] train loss: 0.1543, train acc: 0.9459, val loss: 0.2091, val acc: 0.9444  (best train acc: 0.9586, best val acc: 0.9599, best train loss: 0.1243  @ epoch 19391 )\n",
      "[Epoch: 19500] train loss: 0.1416, train acc: 0.9500, val loss: 0.2128, val acc: 0.9477  (best train acc: 0.9586, best val acc: 0.9599, best train loss: 0.1243  @ epoch 19391 )\n",
      "[Epoch: 19520] train loss: 0.1423, train acc: 0.9506, val loss: 0.2082, val acc: 0.9410  (best train acc: 0.9586, best val acc: 0.9599, best train loss: 0.1243  @ epoch 19391 )\n",
      "[Epoch: 19540] train loss: 0.1518, train acc: 0.9497, val loss: 0.2147, val acc: 0.9450  (best train acc: 0.9586, best val acc: 0.9599, best train loss: 0.1241  @ epoch 19538 )\n",
      "[Epoch: 19560] train loss: 0.1338, train acc: 0.9537, val loss: 0.1972, val acc: 0.9531  (best train acc: 0.9586, best val acc: 0.9599, best train loss: 0.1241  @ epoch 19538 )\n",
      "[Epoch: 19580] train loss: 0.1436, train acc: 0.9485, val loss: 0.2289, val acc: 0.9369  (best train acc: 0.9586, best val acc: 0.9599, best train loss: 0.1241  @ epoch 19538 )\n",
      "[Epoch: 19600] train loss: 0.1340, train acc: 0.9552, val loss: 0.2050, val acc: 0.9470  (best train acc: 0.9586, best val acc: 0.9599, best train loss: 0.1241  @ epoch 19538 )\n",
      "[Epoch: 19620] train loss: 0.1336, train acc: 0.9522, val loss: 0.1909, val acc: 0.9541  (best train acc: 0.9586, best val acc: 0.9599, best train loss: 0.1241  @ epoch 19538 )\n",
      "[Epoch: 19640] train loss: 0.1382, train acc: 0.9495, val loss: 0.1960, val acc: 0.9514  (best train acc: 0.9586, best val acc: 0.9599, best train loss: 0.1241  @ epoch 19538 )\n",
      "[Epoch: 19660] train loss: 0.1329, train acc: 0.9549, val loss: 0.2107, val acc: 0.9524  (best train acc: 0.9586, best val acc: 0.9599, best train loss: 0.1241  @ epoch 19538 )\n",
      "[Epoch: 19680] train loss: 0.1267, train acc: 0.9563, val loss: 0.1970, val acc: 0.9555  (best train acc: 0.9586, best val acc: 0.9599, best train loss: 0.1241  @ epoch 19538 )\n",
      "[Epoch: 19700] train loss: 0.1371, train acc: 0.9519, val loss: 0.1985, val acc: 0.9521  (best train acc: 0.9591, best val acc: 0.9599, best train loss: 0.1241  @ epoch 19538 )\n",
      "[Epoch: 19720] train loss: 0.1375, train acc: 0.9535, val loss: 0.1994, val acc: 0.9548  (best train acc: 0.9591, best val acc: 0.9599, best train loss: 0.1241  @ epoch 19538 )\n",
      "[Epoch: 19740] train loss: 0.1498, train acc: 0.9507, val loss: 0.2081, val acc: 0.9491  (best train acc: 0.9591, best val acc: 0.9599, best train loss: 0.1241  @ epoch 19538 )\n",
      "[Epoch: 19760] train loss: 0.1311, train acc: 0.9523, val loss: 0.1973, val acc: 0.9545  (best train acc: 0.9591, best val acc: 0.9599, best train loss: 0.1241  @ epoch 19538 )\n",
      "[Epoch: 19780] train loss: 0.1297, train acc: 0.9573, val loss: 0.2104, val acc: 0.9487  (best train acc: 0.9591, best val acc: 0.9599, best train loss: 0.1241  @ epoch 19538 )\n",
      "[Epoch: 19800] train loss: 0.1479, train acc: 0.9498, val loss: 0.2024, val acc: 0.9494  (best train acc: 0.9591, best val acc: 0.9599, best train loss: 0.1241  @ epoch 19538 )\n",
      "[Epoch: 19820] train loss: 0.1565, train acc: 0.9409, val loss: 0.2033, val acc: 0.9508  (best train acc: 0.9591, best val acc: 0.9599, best train loss: 0.1241  @ epoch 19538 )\n",
      "[Epoch: 19840] train loss: 0.1408, train acc: 0.9496, val loss: 0.2009, val acc: 0.9491  (best train acc: 0.9591, best val acc: 0.9599, best train loss: 0.1241  @ epoch 19538 )\n",
      "[Epoch: 19860] train loss: 0.1351, train acc: 0.9514, val loss: 0.1987, val acc: 0.9541  (best train acc: 0.9591, best val acc: 0.9599, best train loss: 0.1241  @ epoch 19538 )\n",
      "[Epoch: 19880] train loss: 0.1372, train acc: 0.9519, val loss: 0.1882, val acc: 0.9521  (best train acc: 0.9591, best val acc: 0.9599, best train loss: 0.1241  @ epoch 19538 )\n",
      "[Epoch: 19900] train loss: 0.1367, train acc: 0.9523, val loss: 0.2021, val acc: 0.9538  (best train acc: 0.9591, best val acc: 0.9599, best train loss: 0.1241  @ epoch 19538 )\n",
      "[Epoch: 19920] train loss: 0.1825, train acc: 0.9372, val loss: 0.2387, val acc: 0.9373  (best train acc: 0.9591, best val acc: 0.9599, best train loss: 0.1241  @ epoch 19538 )\n",
      "[Epoch: 19940] train loss: 0.1519, train acc: 0.9484, val loss: 0.1908, val acc: 0.9524  (best train acc: 0.9591, best val acc: 0.9599, best train loss: 0.1241  @ epoch 19538 )\n",
      "[Epoch: 19960] train loss: 0.1427, train acc: 0.9490, val loss: 0.1989, val acc: 0.9504  (best train acc: 0.9591, best val acc: 0.9599, best train loss: 0.1241  @ epoch 19538 )\n",
      "[Epoch: 19980] train loss: 0.1468, train acc: 0.9499, val loss: 0.2142, val acc: 0.9484  (best train acc: 0.9591, best val acc: 0.9599, best train loss: 0.1241  @ epoch 19538 )\n",
      "[Epoch: 20000] train loss: 0.1372, train acc: 0.9510, val loss: 0.2068, val acc: 0.9484  (best train acc: 0.9591, best val acc: 0.9599, best train loss: 0.1241  @ epoch 19538 )\n",
      "[Epoch: 20020] train loss: 0.1647, train acc: 0.9459, val loss: 0.1992, val acc: 0.9531  (best train acc: 0.9591, best val acc: 0.9599, best train loss: 0.1241  @ epoch 19538 )\n",
      "[Epoch: 20040] train loss: 0.1428, train acc: 0.9527, val loss: 0.1985, val acc: 0.9518  (best train acc: 0.9591, best val acc: 0.9599, best train loss: 0.1241  @ epoch 19538 )\n",
      "[Epoch: 20060] train loss: 0.1453, train acc: 0.9488, val loss: 0.1900, val acc: 0.9562  (best train acc: 0.9591, best val acc: 0.9599, best train loss: 0.1241  @ epoch 19538 )\n",
      "[Epoch: 20080] train loss: 0.1357, train acc: 0.9542, val loss: 0.1992, val acc: 0.9548  (best train acc: 0.9591, best val acc: 0.9599, best train loss: 0.1241  @ epoch 19538 )\n",
      "[Epoch: 20100] train loss: 0.1326, train acc: 0.9560, val loss: 0.2009, val acc: 0.9508  (best train acc: 0.9591, best val acc: 0.9599, best train loss: 0.1241  @ epoch 19538 )\n",
      "[Epoch: 20120] train loss: 0.1303, train acc: 0.9551, val loss: 0.2050, val acc: 0.9531  (best train acc: 0.9591, best val acc: 0.9599, best train loss: 0.1241  @ epoch 19538 )\n",
      "[Epoch: 20140] train loss: 0.1363, train acc: 0.9517, val loss: 0.1993, val acc: 0.9464  (best train acc: 0.9591, best val acc: 0.9599, best train loss: 0.1241  @ epoch 19538 )\n",
      "[Epoch: 20160] train loss: 0.1324, train acc: 0.9525, val loss: 0.1912, val acc: 0.9545  (best train acc: 0.9591, best val acc: 0.9599, best train loss: 0.1241  @ epoch 19538 )\n",
      "[Epoch: 20180] train loss: 0.1886, train acc: 0.9293, val loss: 0.2157, val acc: 0.9312  (best train acc: 0.9591, best val acc: 0.9599, best train loss: 0.1241  @ epoch 19538 )\n",
      "[Epoch: 20200] train loss: 0.1375, train acc: 0.9525, val loss: 0.2050, val acc: 0.9524  (best train acc: 0.9591, best val acc: 0.9599, best train loss: 0.1241  @ epoch 19538 )\n",
      "[Epoch: 20220] train loss: 0.1332, train acc: 0.9518, val loss: 0.2098, val acc: 0.9521  (best train acc: 0.9591, best val acc: 0.9599, best train loss: 0.1241  @ epoch 19538 )\n",
      "[Epoch: 20240] train loss: 0.1397, train acc: 0.9485, val loss: 0.2008, val acc: 0.9508  (best train acc: 0.9591, best val acc: 0.9599, best train loss: 0.1241  @ epoch 19538 )\n",
      "[Epoch: 20260] train loss: 0.1463, train acc: 0.9466, val loss: 0.2124, val acc: 0.9403  (best train acc: 0.9591, best val acc: 0.9599, best train loss: 0.1241  @ epoch 19538 )\n",
      "[Epoch: 20280] train loss: 0.1341, train acc: 0.9526, val loss: 0.1944, val acc: 0.9551  (best train acc: 0.9591, best val acc: 0.9599, best train loss: 0.1241  @ epoch 19538 )\n",
      "[Epoch: 20300] train loss: 0.1396, train acc: 0.9524, val loss: 0.1979, val acc: 0.9555  (best train acc: 0.9591, best val acc: 0.9599, best train loss: 0.1241  @ epoch 19538 )\n",
      "[Epoch: 20320] train loss: 0.1331, train acc: 0.9530, val loss: 0.1975, val acc: 0.9545  (best train acc: 0.9591, best val acc: 0.9599, best train loss: 0.1241  @ epoch 19538 )\n",
      "[Epoch: 20340] train loss: 0.1466, train acc: 0.9474, val loss: 0.2038, val acc: 0.9514  (best train acc: 0.9591, best val acc: 0.9599, best train loss: 0.1241  @ epoch 19538 )\n",
      "[Epoch: 20360] train loss: 0.1505, train acc: 0.9486, val loss: 0.2153, val acc: 0.9467  (best train acc: 0.9591, best val acc: 0.9599, best train loss: 0.1241  @ epoch 19538 )\n",
      "[Epoch: 20380] train loss: 0.1391, train acc: 0.9527, val loss: 0.2150, val acc: 0.9474  (best train acc: 0.9591, best val acc: 0.9599, best train loss: 0.1241  @ epoch 19538 )\n",
      "[Epoch: 20400] train loss: 0.1322, train acc: 0.9545, val loss: 0.1944, val acc: 0.9568  (best train acc: 0.9591, best val acc: 0.9599, best train loss: 0.1241  @ epoch 19538 )\n",
      "[Epoch: 20420] train loss: 0.1363, train acc: 0.9526, val loss: 0.2014, val acc: 0.9535  (best train acc: 0.9591, best val acc: 0.9599, best train loss: 0.1211  @ epoch 20406 )\n",
      "[Epoch: 20440] train loss: 0.1494, train acc: 0.9479, val loss: 0.2038, val acc: 0.9467  (best train acc: 0.9591, best val acc: 0.9599, best train loss: 0.1211  @ epoch 20406 )\n",
      "[Epoch: 20460] train loss: 0.1570, train acc: 0.9480, val loss: 0.2289, val acc: 0.9393  (best train acc: 0.9591, best val acc: 0.9599, best train loss: 0.1211  @ epoch 20406 )\n",
      "[Epoch: 20480] train loss: 0.1354, train acc: 0.9549, val loss: 0.1973, val acc: 0.9511  (best train acc: 0.9591, best val acc: 0.9599, best train loss: 0.1211  @ epoch 20406 )\n",
      "[Epoch: 20500] train loss: 0.1308, train acc: 0.9571, val loss: 0.1944, val acc: 0.9545  (best train acc: 0.9591, best val acc: 0.9599, best train loss: 0.1211  @ epoch 20406 )\n",
      "[Epoch: 20520] train loss: 0.1482, train acc: 0.9469, val loss: 0.1980, val acc: 0.9514  (best train acc: 0.9591, best val acc: 0.9599, best train loss: 0.1211  @ epoch 20406 )\n",
      "[Epoch: 20540] train loss: 0.1445, train acc: 0.9498, val loss: 0.2006, val acc: 0.9541  (best train acc: 0.9591, best val acc: 0.9599, best train loss: 0.1211  @ epoch 20406 )\n",
      "[Epoch: 20560] train loss: 0.1317, train acc: 0.9538, val loss: 0.2055, val acc: 0.9518  (best train acc: 0.9591, best val acc: 0.9599, best train loss: 0.1211  @ epoch 20406 )\n",
      "[Epoch: 20580] train loss: 0.1272, train acc: 0.9571, val loss: 0.2032, val acc: 0.9491  (best train acc: 0.9592, best val acc: 0.9599, best train loss: 0.1211  @ epoch 20406 )\n",
      "[Epoch: 20600] train loss: 0.1556, train acc: 0.9462, val loss: 0.2017, val acc: 0.9487  (best train acc: 0.9592, best val acc: 0.9599, best train loss: 0.1211  @ epoch 20406 )\n",
      "[Epoch: 20620] train loss: 0.1305, train acc: 0.9555, val loss: 0.2003, val acc: 0.9541  (best train acc: 0.9592, best val acc: 0.9599, best train loss: 0.1211  @ epoch 20406 )\n",
      "[Epoch: 20640] train loss: 0.1460, train acc: 0.9498, val loss: 0.2044, val acc: 0.9514  (best train acc: 0.9592, best val acc: 0.9599, best train loss: 0.1211  @ epoch 20406 )\n",
      "[Epoch: 20660] train loss: 0.1260, train acc: 0.9560, val loss: 0.2007, val acc: 0.9535  (best train acc: 0.9592, best val acc: 0.9599, best train loss: 0.1211  @ epoch 20406 )\n",
      "[Epoch: 20680] train loss: 0.1260, train acc: 0.9568, val loss: 0.2066, val acc: 0.9518  (best train acc: 0.9592, best val acc: 0.9599, best train loss: 0.1200  @ epoch 20669 )\n",
      "[Epoch: 20700] train loss: 0.1355, train acc: 0.9528, val loss: 0.2102, val acc: 0.9521  (best train acc: 0.9592, best val acc: 0.9599, best train loss: 0.1200  @ epoch 20669 )\n",
      "[Epoch: 20720] train loss: 0.1379, train acc: 0.9516, val loss: 0.2056, val acc: 0.9528  (best train acc: 0.9592, best val acc: 0.9599, best train loss: 0.1200  @ epoch 20669 )\n",
      "[Epoch: 20740] train loss: 0.1264, train acc: 0.9571, val loss: 0.1988, val acc: 0.9528  (best train acc: 0.9592, best val acc: 0.9599, best train loss: 0.1200  @ epoch 20669 )\n",
      "[Epoch: 20760] train loss: 0.1852, train acc: 0.9367, val loss: 0.2258, val acc: 0.9379  (best train acc: 0.9592, best val acc: 0.9599, best train loss: 0.1200  @ epoch 20669 )\n",
      "[Epoch: 20780] train loss: 0.1322, train acc: 0.9556, val loss: 0.2076, val acc: 0.9535  (best train acc: 0.9592, best val acc: 0.9599, best train loss: 0.1200  @ epoch 20669 )\n",
      "[Epoch: 20800] train loss: 0.1339, train acc: 0.9542, val loss: 0.2136, val acc: 0.9474  (best train acc: 0.9598, best val acc: 0.9599, best train loss: 0.1200  @ epoch 20669 )\n",
      "[Epoch: 20820] train loss: 0.1502, train acc: 0.9479, val loss: 0.2158, val acc: 0.9454  (best train acc: 0.9598, best val acc: 0.9599, best train loss: 0.1200  @ epoch 20669 )\n",
      "[Epoch: 20840] train loss: 0.1361, train acc: 0.9532, val loss: 0.1976, val acc: 0.9575  (best train acc: 0.9598, best val acc: 0.9599, best train loss: 0.1200  @ epoch 20669 )\n",
      "[Epoch: 20860] train loss: 0.1242, train acc: 0.9573, val loss: 0.2033, val acc: 0.9487  (best train acc: 0.9598, best val acc: 0.9599, best train loss: 0.1200  @ epoch 20669 )\n",
      "[Epoch: 20880] train loss: 0.1403, train acc: 0.9509, val loss: 0.1979, val acc: 0.9518  (best train acc: 0.9598, best val acc: 0.9599, best train loss: 0.1200  @ epoch 20669 )\n",
      "[Epoch: 20900] train loss: 0.1480, train acc: 0.9470, val loss: 0.2071, val acc: 0.9521  (best train acc: 0.9598, best val acc: 0.9599, best train loss: 0.1200  @ epoch 20669 )\n",
      "[Epoch: 20920] train loss: 0.1362, train acc: 0.9547, val loss: 0.1955, val acc: 0.9545  (best train acc: 0.9598, best val acc: 0.9599, best train loss: 0.1200  @ epoch 20669 )\n",
      "[Epoch: 20940] train loss: 0.1288, train acc: 0.9535, val loss: 0.2006, val acc: 0.9541  (best train acc: 0.9598, best val acc: 0.9599, best train loss: 0.1200  @ epoch 20669 )\n",
      "[Epoch: 20960] train loss: 0.1547, train acc: 0.9427, val loss: 0.2062, val acc: 0.9474  (best train acc: 0.9598, best val acc: 0.9599, best train loss: 0.1200  @ epoch 20669 )\n",
      "[Epoch: 20980] train loss: 0.1434, train acc: 0.9500, val loss: 0.2063, val acc: 0.9508  (best train acc: 0.9598, best val acc: 0.9599, best train loss: 0.1200  @ epoch 20669 )\n",
      "[Epoch: 21000] train loss: 0.1653, train acc: 0.9402, val loss: 0.2067, val acc: 0.9467  (best train acc: 0.9598, best val acc: 0.9599, best train loss: 0.1200  @ epoch 20669 )\n",
      "[Epoch: 21020] train loss: 0.1306, train acc: 0.9527, val loss: 0.2103, val acc: 0.9528  (best train acc: 0.9598, best val acc: 0.9599, best train loss: 0.1200  @ epoch 20669 )\n",
      "[Epoch: 21040] train loss: 0.1319, train acc: 0.9550, val loss: 0.2014, val acc: 0.9528  (best train acc: 0.9598, best val acc: 0.9599, best train loss: 0.1200  @ epoch 20669 )\n",
      "[Epoch: 21060] train loss: 0.1223, train acc: 0.9585, val loss: 0.1976, val acc: 0.9555  (best train acc: 0.9598, best val acc: 0.9599, best train loss: 0.1200  @ epoch 20669 )\n",
      "[Epoch: 21080] train loss: 0.1378, train acc: 0.9513, val loss: 0.2209, val acc: 0.9444  (best train acc: 0.9598, best val acc: 0.9599, best train loss: 0.1200  @ epoch 20669 )\n",
      "[Epoch: 21100] train loss: 0.1305, train acc: 0.9533, val loss: 0.2132, val acc: 0.9481  (best train acc: 0.9598, best val acc: 0.9599, best train loss: 0.1200  @ epoch 20669 )\n",
      "[Epoch: 21120] train loss: 0.1310, train acc: 0.9522, val loss: 0.1997, val acc: 0.9535  (best train acc: 0.9598, best val acc: 0.9599, best train loss: 0.1200  @ epoch 20669 )\n",
      "[Epoch: 21140] train loss: 0.1587, train acc: 0.9443, val loss: 0.2267, val acc: 0.9460  (best train acc: 0.9598, best val acc: 0.9599, best train loss: 0.1200  @ epoch 20669 )\n",
      "[Epoch: 21160] train loss: 0.1396, train acc: 0.9519, val loss: 0.2018, val acc: 0.9521  (best train acc: 0.9598, best val acc: 0.9599, best train loss: 0.1200  @ epoch 20669 )\n",
      "[Epoch: 21180] train loss: 0.1474, train acc: 0.9492, val loss: 0.2135, val acc: 0.9464  (best train acc: 0.9598, best val acc: 0.9599, best train loss: 0.1200  @ epoch 20669 )\n",
      "[Epoch: 21200] train loss: 0.1349, train acc: 0.9539, val loss: 0.2202, val acc: 0.9457  (best train acc: 0.9598, best val acc: 0.9599, best train loss: 0.1200  @ epoch 20669 )\n",
      "[Epoch: 21220] train loss: 0.1263, train acc: 0.9567, val loss: 0.2009, val acc: 0.9518  (best train acc: 0.9598, best val acc: 0.9599, best train loss: 0.1200  @ epoch 20669 )\n",
      "[Epoch: 21240] train loss: 0.1445, train acc: 0.9499, val loss: 0.2038, val acc: 0.9494  (best train acc: 0.9598, best val acc: 0.9599, best train loss: 0.1200  @ epoch 20669 )\n",
      "[Epoch: 21260] train loss: 0.1361, train acc: 0.9531, val loss: 0.1949, val acc: 0.9538  (best train acc: 0.9598, best val acc: 0.9599, best train loss: 0.1200  @ epoch 20669 )\n",
      "[Epoch: 21280] train loss: 0.1664, train acc: 0.9409, val loss: 0.2084, val acc: 0.9470  (best train acc: 0.9598, best val acc: 0.9599, best train loss: 0.1200  @ epoch 20669 )\n",
      "[Epoch: 21300] train loss: 0.1307, train acc: 0.9544, val loss: 0.2027, val acc: 0.9548  (best train acc: 0.9598, best val acc: 0.9599, best train loss: 0.1200  @ epoch 20669 )\n",
      "[Epoch: 21320] train loss: 0.1311, train acc: 0.9514, val loss: 0.2024, val acc: 0.9514  (best train acc: 0.9598, best val acc: 0.9599, best train loss: 0.1200  @ epoch 20669 )\n",
      "[Epoch: 21340] train loss: 0.1327, train acc: 0.9534, val loss: 0.2039, val acc: 0.9491  (best train acc: 0.9598, best val acc: 0.9599, best train loss: 0.1200  @ epoch 20669 )\n",
      "[Epoch: 21360] train loss: 0.1242, train acc: 0.9566, val loss: 0.1913, val acc: 0.9521  (best train acc: 0.9598, best val acc: 0.9599, best train loss: 0.1200  @ epoch 20669 )\n",
      "[Epoch: 21380] train loss: 0.1252, train acc: 0.9563, val loss: 0.2011, val acc: 0.9514  (best train acc: 0.9598, best val acc: 0.9599, best train loss: 0.1200  @ epoch 20669 )\n",
      "[Epoch: 21400] train loss: 0.1639, train acc: 0.9442, val loss: 0.1949, val acc: 0.9548  (best train acc: 0.9598, best val acc: 0.9599, best train loss: 0.1200  @ epoch 20669 )\n",
      "[Epoch: 21420] train loss: 0.1294, train acc: 0.9563, val loss: 0.2112, val acc: 0.9545  (best train acc: 0.9598, best val acc: 0.9599, best train loss: 0.1200  @ epoch 20669 )\n",
      "[Epoch: 21440] train loss: 0.1422, train acc: 0.9482, val loss: 0.1892, val acc: 0.9514  (best train acc: 0.9598, best val acc: 0.9599, best train loss: 0.1200  @ epoch 20669 )\n",
      "[Epoch: 21460] train loss: 0.1334, train acc: 0.9537, val loss: 0.1968, val acc: 0.9551  (best train acc: 0.9598, best val acc: 0.9599, best train loss: 0.1200  @ epoch 20669 )\n",
      "[Epoch: 21480] train loss: 0.1377, train acc: 0.9505, val loss: 0.2316, val acc: 0.9393  (best train acc: 0.9598, best val acc: 0.9599, best train loss: 0.1200  @ epoch 20669 )\n",
      "[Epoch: 21500] train loss: 0.1430, train acc: 0.9492, val loss: 0.2037, val acc: 0.9511  (best train acc: 0.9598, best val acc: 0.9599, best train loss: 0.1200  @ epoch 20669 )\n",
      "[Epoch: 21520] train loss: 0.1311, train acc: 0.9558, val loss: 0.2013, val acc: 0.9497  (best train acc: 0.9598, best val acc: 0.9599, best train loss: 0.1200  @ epoch 20669 )\n",
      "[Epoch: 21540] train loss: 0.1358, train acc: 0.9549, val loss: 0.1999, val acc: 0.9558  (best train acc: 0.9598, best val acc: 0.9599, best train loss: 0.1179  @ epoch 21537 )\n",
      "[Epoch: 21560] train loss: 0.1369, train acc: 0.9524, val loss: 0.1991, val acc: 0.9541  (best train acc: 0.9598, best val acc: 0.9599, best train loss: 0.1179  @ epoch 21537 )\n",
      "[Epoch: 21580] train loss: 0.1252, train acc: 0.9558, val loss: 0.2008, val acc: 0.9491  (best train acc: 0.9598, best val acc: 0.9599, best train loss: 0.1179  @ epoch 21537 )\n",
      "[Epoch: 21600] train loss: 0.1329, train acc: 0.9570, val loss: 0.2137, val acc: 0.9450  (best train acc: 0.9598, best val acc: 0.9599, best train loss: 0.1179  @ epoch 21537 )\n",
      "[Epoch: 21620] train loss: 0.1478, train acc: 0.9452, val loss: 0.2175, val acc: 0.9460  (best train acc: 0.9598, best val acc: 0.9599, best train loss: 0.1179  @ epoch 21537 )\n",
      "[Epoch: 21640] train loss: 0.1484, train acc: 0.9469, val loss: 0.2133, val acc: 0.9470  (best train acc: 0.9598, best val acc: 0.9599, best train loss: 0.1179  @ epoch 21537 )\n",
      "[Epoch: 21660] train loss: 0.1417, train acc: 0.9485, val loss: 0.2016, val acc: 0.9454  (best train acc: 0.9598, best val acc: 0.9599, best train loss: 0.1179  @ epoch 21537 )\n",
      "[Epoch: 21680] train loss: 0.1354, train acc: 0.9510, val loss: 0.2124, val acc: 0.9487  (best train acc: 0.9598, best val acc: 0.9599, best train loss: 0.1179  @ epoch 21537 )\n",
      "[Epoch: 21700] train loss: 0.1249, train acc: 0.9552, val loss: 0.1955, val acc: 0.9538  (best train acc: 0.9598, best val acc: 0.9599, best train loss: 0.1179  @ epoch 21537 )\n",
      "[Epoch: 21720] train loss: 0.1274, train acc: 0.9541, val loss: 0.1965, val acc: 0.9514  (best train acc: 0.9598, best val acc: 0.9599, best train loss: 0.1179  @ epoch 21537 )\n",
      "[Epoch: 21740] train loss: 0.1291, train acc: 0.9553, val loss: 0.1998, val acc: 0.9551  (best train acc: 0.9598, best val acc: 0.9599, best train loss: 0.1179  @ epoch 21537 )\n",
      "[Epoch: 21760] train loss: 0.1335, train acc: 0.9516, val loss: 0.2002, val acc: 0.9518  (best train acc: 0.9598, best val acc: 0.9599, best train loss: 0.1179  @ epoch 21537 )\n",
      "[Epoch: 21780] train loss: 0.1256, train acc: 0.9565, val loss: 0.1920, val acc: 0.9555  (best train acc: 0.9598, best val acc: 0.9599, best train loss: 0.1179  @ epoch 21537 )\n",
      "[Epoch: 21800] train loss: 0.1447, train acc: 0.9484, val loss: 0.1959, val acc: 0.9504  (best train acc: 0.9598, best val acc: 0.9599, best train loss: 0.1179  @ epoch 21537 )\n",
      "[Epoch: 21820] train loss: 0.1321, train acc: 0.9544, val loss: 0.2016, val acc: 0.9548  (best train acc: 0.9598, best val acc: 0.9599, best train loss: 0.1179  @ epoch 21537 )\n",
      "[Epoch: 21840] train loss: 0.1193, train acc: 0.9584, val loss: 0.1990, val acc: 0.9538  (best train acc: 0.9598, best val acc: 0.9599, best train loss: 0.1179  @ epoch 21537 )\n",
      "[Epoch: 21860] train loss: 0.1384, train acc: 0.9501, val loss: 0.1991, val acc: 0.9518  (best train acc: 0.9598, best val acc: 0.9599, best train loss: 0.1179  @ epoch 21537 )\n",
      "[Epoch: 21880] train loss: 0.1680, train acc: 0.9385, val loss: 0.2030, val acc: 0.9460  (best train acc: 0.9598, best val acc: 0.9599, best train loss: 0.1179  @ epoch 21537 )\n",
      "[Epoch: 21900] train loss: 0.1801, train acc: 0.9370, val loss: 0.2086, val acc: 0.9470  (best train acc: 0.9598, best val acc: 0.9599, best train loss: 0.1179  @ epoch 21537 )\n",
      "[Epoch: 21920] train loss: 0.1420, train acc: 0.9475, val loss: 0.2075, val acc: 0.9470  (best train acc: 0.9598, best val acc: 0.9599, best train loss: 0.1179  @ epoch 21537 )\n",
      "[Epoch: 21940] train loss: 0.1319, train acc: 0.9550, val loss: 0.1957, val acc: 0.9450  (best train acc: 0.9598, best val acc: 0.9599, best train loss: 0.1179  @ epoch 21537 )\n",
      "[Epoch: 21960] train loss: 0.1351, train acc: 0.9529, val loss: 0.2060, val acc: 0.9460  (best train acc: 0.9598, best val acc: 0.9599, best train loss: 0.1179  @ epoch 21537 )\n",
      "[Epoch: 21980] train loss: 0.1221, train acc: 0.9571, val loss: 0.2003, val acc: 0.9511  (best train acc: 0.9598, best val acc: 0.9599, best train loss: 0.1179  @ epoch 21537 )\n",
      "[Epoch: 22000] train loss: 0.1249, train acc: 0.9582, val loss: 0.2008, val acc: 0.9538  (best train acc: 0.9598, best val acc: 0.9599, best train loss: 0.1179  @ epoch 21537 )\n",
      "[Epoch: 22020] train loss: 0.1263, train acc: 0.9550, val loss: 0.1907, val acc: 0.9555  (best train acc: 0.9598, best val acc: 0.9599, best train loss: 0.1179  @ epoch 21537 )\n",
      "[Epoch: 22040] train loss: 0.1260, train acc: 0.9563, val loss: 0.2025, val acc: 0.9514  (best train acc: 0.9598, best val acc: 0.9599, best train loss: 0.1179  @ epoch 21537 )\n",
      "[Epoch: 22060] train loss: 0.1298, train acc: 0.9559, val loss: 0.1882, val acc: 0.9551  (best train acc: 0.9598, best val acc: 0.9599, best train loss: 0.1179  @ epoch 21537 )\n",
      "[Epoch: 22080] train loss: 0.1378, train acc: 0.9516, val loss: 0.2115, val acc: 0.9514  (best train acc: 0.9619, best val acc: 0.9599, best train loss: 0.1169  @ epoch 22068 )\n",
      "[Epoch: 22100] train loss: 0.1392, train acc: 0.9526, val loss: 0.2183, val acc: 0.9481  (best train acc: 0.9619, best val acc: 0.9599, best train loss: 0.1169  @ epoch 22068 )\n",
      "[Epoch: 22120] train loss: 0.1352, train acc: 0.9516, val loss: 0.1988, val acc: 0.9528  (best train acc: 0.9619, best val acc: 0.9599, best train loss: 0.1169  @ epoch 22068 )\n",
      "[Epoch: 22140] train loss: 0.1300, train acc: 0.9538, val loss: 0.2237, val acc: 0.9433  (best train acc: 0.9619, best val acc: 0.9599, best train loss: 0.1169  @ epoch 22068 )\n",
      "[Epoch: 22160] train loss: 0.1474, train acc: 0.9425, val loss: 0.2130, val acc: 0.9467  (best train acc: 0.9619, best val acc: 0.9599, best train loss: 0.1169  @ epoch 22068 )\n",
      "[Epoch: 22180] train loss: 0.1423, train acc: 0.9479, val loss: 0.2017, val acc: 0.9528  (best train acc: 0.9619, best val acc: 0.9599, best train loss: 0.1169  @ epoch 22068 )\n",
      "[Epoch: 22200] train loss: 0.1317, train acc: 0.9545, val loss: 0.1950, val acc: 0.9558  (best train acc: 0.9619, best val acc: 0.9599, best train loss: 0.1169  @ epoch 22068 )\n",
      "[Epoch: 22220] train loss: 0.1317, train acc: 0.9549, val loss: 0.2035, val acc: 0.9528  (best train acc: 0.9619, best val acc: 0.9599, best train loss: 0.1169  @ epoch 22068 )\n",
      "[Epoch: 22240] train loss: 0.1295, train acc: 0.9550, val loss: 0.2078, val acc: 0.9497  (best train acc: 0.9619, best val acc: 0.9599, best train loss: 0.1169  @ epoch 22068 )\n",
      "[Epoch: 22260] train loss: 0.1271, train acc: 0.9560, val loss: 0.2133, val acc: 0.9521  (best train acc: 0.9619, best val acc: 0.9599, best train loss: 0.1169  @ epoch 22068 )\n",
      "[Epoch: 22280] train loss: 0.1615, train acc: 0.9408, val loss: 0.2132, val acc: 0.9491  (best train acc: 0.9619, best val acc: 0.9599, best train loss: 0.1169  @ epoch 22068 )\n",
      "[Epoch: 22300] train loss: 0.1300, train acc: 0.9557, val loss: 0.1980, val acc: 0.9562  (best train acc: 0.9619, best val acc: 0.9599, best train loss: 0.1169  @ epoch 22068 )\n",
      "[Epoch: 22320] train loss: 0.1343, train acc: 0.9531, val loss: 0.2053, val acc: 0.9514  (best train acc: 0.9619, best val acc: 0.9599, best train loss: 0.1169  @ epoch 22068 )\n",
      "[Epoch: 22340] train loss: 0.1226, train acc: 0.9579, val loss: 0.2037, val acc: 0.9545  (best train acc: 0.9619, best val acc: 0.9599, best train loss: 0.1169  @ epoch 22068 )\n",
      "[Epoch: 22360] train loss: 0.1326, train acc: 0.9526, val loss: 0.2045, val acc: 0.9504  (best train acc: 0.9619, best val acc: 0.9599, best train loss: 0.1169  @ epoch 22068 )\n",
      "[Epoch: 22380] train loss: 0.1513, train acc: 0.9469, val loss: 0.2195, val acc: 0.9467  (best train acc: 0.9619, best val acc: 0.9599, best train loss: 0.1169  @ epoch 22068 )\n",
      "[Epoch: 22400] train loss: 0.1506, train acc: 0.9430, val loss: 0.2017, val acc: 0.9494  (best train acc: 0.9619, best val acc: 0.9599, best train loss: 0.1169  @ epoch 22068 )\n",
      "[Epoch: 22420] train loss: 0.1311, train acc: 0.9541, val loss: 0.2027, val acc: 0.9518  (best train acc: 0.9619, best val acc: 0.9599, best train loss: 0.1169  @ epoch 22068 )\n",
      "[Epoch: 22440] train loss: 0.1622, train acc: 0.9414, val loss: 0.2286, val acc: 0.9430  (best train acc: 0.9619, best val acc: 0.9599, best train loss: 0.1169  @ epoch 22068 )\n",
      "[Epoch: 22460] train loss: 0.1389, train acc: 0.9529, val loss: 0.2183, val acc: 0.9491  (best train acc: 0.9619, best val acc: 0.9599, best train loss: 0.1169  @ epoch 22068 )\n",
      "[Epoch: 22480] train loss: 0.1273, train acc: 0.9573, val loss: 0.1958, val acc: 0.9551  (best train acc: 0.9619, best val acc: 0.9599, best train loss: 0.1169  @ epoch 22068 )\n",
      "[Epoch: 22500] train loss: 0.1453, train acc: 0.9468, val loss: 0.2029, val acc: 0.9528  (best train acc: 0.9619, best val acc: 0.9599, best train loss: 0.1169  @ epoch 22068 )\n",
      "[Epoch: 22520] train loss: 0.1469, train acc: 0.9488, val loss: 0.2010, val acc: 0.9521  (best train acc: 0.9619, best val acc: 0.9599, best train loss: 0.1169  @ epoch 22068 )\n",
      "[Epoch: 22540] train loss: 0.1343, train acc: 0.9526, val loss: 0.2027, val acc: 0.9511  (best train acc: 0.9619, best val acc: 0.9599, best train loss: 0.1169  @ epoch 22068 )\n",
      "[Epoch: 22560] train loss: 0.1306, train acc: 0.9542, val loss: 0.2114, val acc: 0.9504  (best train acc: 0.9619, best val acc: 0.9599, best train loss: 0.1169  @ epoch 22068 )\n",
      "[Epoch: 22580] train loss: 0.1433, train acc: 0.9500, val loss: 0.2201, val acc: 0.9457  (best train acc: 0.9619, best val acc: 0.9599, best train loss: 0.1169  @ epoch 22068 )\n",
      "[Epoch: 22600] train loss: 0.1398, train acc: 0.9490, val loss: 0.2257, val acc: 0.9376  (best train acc: 0.9619, best val acc: 0.9599, best train loss: 0.1169  @ epoch 22068 )\n",
      "[Epoch: 22620] train loss: 0.1255, train acc: 0.9550, val loss: 0.1989, val acc: 0.9518  (best train acc: 0.9619, best val acc: 0.9599, best train loss: 0.1169  @ epoch 22068 )\n",
      "[Epoch: 22640] train loss: 0.1249, train acc: 0.9563, val loss: 0.2078, val acc: 0.9562  (best train acc: 0.9619, best val acc: 0.9599, best train loss: 0.1168  @ epoch 22632 )\n",
      "[Epoch: 22660] train loss: 0.1221, train acc: 0.9563, val loss: 0.1953, val acc: 0.9538  (best train acc: 0.9619, best val acc: 0.9599, best train loss: 0.1160  @ epoch 22645 )\n",
      "[Epoch: 22680] train loss: 0.1485, train acc: 0.9466, val loss: 0.1982, val acc: 0.9528  (best train acc: 0.9619, best val acc: 0.9599, best train loss: 0.1160  @ epoch 22645 )\n",
      "[Epoch: 22700] train loss: 0.1226, train acc: 0.9576, val loss: 0.2034, val acc: 0.9501  (best train acc: 0.9619, best val acc: 0.9599, best train loss: 0.1160  @ epoch 22645 )\n",
      "[Epoch: 22720] train loss: 0.1296, train acc: 0.9560, val loss: 0.2011, val acc: 0.9568  (best train acc: 0.9619, best val acc: 0.9599, best train loss: 0.1160  @ epoch 22645 )\n",
      "[Epoch: 22740] train loss: 0.1381, train acc: 0.9503, val loss: 0.1931, val acc: 0.9504  (best train acc: 0.9619, best val acc: 0.9599, best train loss: 0.1160  @ epoch 22645 )\n",
      "[Epoch: 22760] train loss: 0.1218, train acc: 0.9573, val loss: 0.2302, val acc: 0.9457  (best train acc: 0.9619, best val acc: 0.9599, best train loss: 0.1160  @ epoch 22645 )\n",
      "[Epoch: 22780] train loss: 0.1280, train acc: 0.9548, val loss: 0.1957, val acc: 0.9572  (best train acc: 0.9619, best val acc: 0.9599, best train loss: 0.1160  @ epoch 22645 )\n",
      "[Epoch: 22800] train loss: 0.1293, train acc: 0.9550, val loss: 0.2038, val acc: 0.9518  (best train acc: 0.9619, best val acc: 0.9599, best train loss: 0.1160  @ epoch 22645 )\n",
      "[Epoch: 22820] train loss: 0.1385, train acc: 0.9503, val loss: 0.2002, val acc: 0.9538  (best train acc: 0.9619, best val acc: 0.9599, best train loss: 0.1160  @ epoch 22645 )\n",
      "[Epoch: 22840] train loss: 0.1367, train acc: 0.9550, val loss: 0.2006, val acc: 0.9524  (best train acc: 0.9619, best val acc: 0.9599, best train loss: 0.1160  @ epoch 22645 )\n",
      "[Epoch: 22860] train loss: 0.1314, train acc: 0.9558, val loss: 0.2001, val acc: 0.9504  (best train acc: 0.9619, best val acc: 0.9599, best train loss: 0.1160  @ epoch 22645 )\n",
      "[Epoch: 22880] train loss: 0.1515, train acc: 0.9479, val loss: 0.2125, val acc: 0.9481  (best train acc: 0.9619, best val acc: 0.9599, best train loss: 0.1160  @ epoch 22645 )\n",
      "[Epoch: 22900] train loss: 0.1305, train acc: 0.9579, val loss: 0.2130, val acc: 0.9508  (best train acc: 0.9619, best val acc: 0.9599, best train loss: 0.1160  @ epoch 22645 )\n",
      "[Epoch: 22920] train loss: 0.1217, train acc: 0.9594, val loss: 0.2001, val acc: 0.9558  (best train acc: 0.9619, best val acc: 0.9599, best train loss: 0.1160  @ epoch 22645 )\n",
      "[Epoch: 22940] train loss: 0.1214, train acc: 0.9602, val loss: 0.2006, val acc: 0.9562  (best train acc: 0.9619, best val acc: 0.9599, best train loss: 0.1160  @ epoch 22645 )\n",
      "[Epoch: 22960] train loss: 0.1332, train acc: 0.9523, val loss: 0.1954, val acc: 0.9491  (best train acc: 0.9619, best val acc: 0.9599, best train loss: 0.1160  @ epoch 22645 )\n",
      "[Epoch: 22980] train loss: 0.1395, train acc: 0.9488, val loss: 0.2070, val acc: 0.9511  (best train acc: 0.9619, best val acc: 0.9599, best train loss: 0.1160  @ epoch 22645 )\n",
      "[Epoch: 23000] train loss: 0.1228, train acc: 0.9553, val loss: 0.1922, val acc: 0.9572  (best train acc: 0.9619, best val acc: 0.9599, best train loss: 0.1160  @ epoch 22645 )\n",
      "[Epoch: 23020] train loss: 0.1335, train acc: 0.9540, val loss: 0.2048, val acc: 0.9504  (best train acc: 0.9619, best val acc: 0.9599, best train loss: 0.1133  @ epoch 23005 )\n",
      "[Epoch: 23040] train loss: 0.1259, train acc: 0.9565, val loss: 0.1996, val acc: 0.9521  (best train acc: 0.9619, best val acc: 0.9599, best train loss: 0.1133  @ epoch 23005 )\n",
      "[Epoch: 23060] train loss: 0.1286, train acc: 0.9553, val loss: 0.1984, val acc: 0.9511  (best train acc: 0.9619, best val acc: 0.9599, best train loss: 0.1133  @ epoch 23005 )\n",
      "[Epoch: 23080] train loss: 0.1336, train acc: 0.9532, val loss: 0.2154, val acc: 0.9470  (best train acc: 0.9619, best val acc: 0.9599, best train loss: 0.1133  @ epoch 23005 )\n",
      "[Epoch: 23100] train loss: 0.1198, train acc: 0.9572, val loss: 0.1957, val acc: 0.9545  (best train acc: 0.9619, best val acc: 0.9599, best train loss: 0.1133  @ epoch 23005 )\n",
      "[Epoch: 23120] train loss: 0.1291, train acc: 0.9583, val loss: 0.2005, val acc: 0.9528  (best train acc: 0.9619, best val acc: 0.9599, best train loss: 0.1133  @ epoch 23005 )\n",
      "[Epoch: 23140] train loss: 0.1275, train acc: 0.9547, val loss: 0.2022, val acc: 0.9531  (best train acc: 0.9619, best val acc: 0.9599, best train loss: 0.1133  @ epoch 23005 )\n",
      "[Epoch: 23160] train loss: 0.1289, train acc: 0.9553, val loss: 0.1964, val acc: 0.9548  (best train acc: 0.9619, best val acc: 0.9599, best train loss: 0.1133  @ epoch 23005 )\n",
      "[Epoch: 23180] train loss: 0.1150, train acc: 0.9581, val loss: 0.1990, val acc: 0.9555  (best train acc: 0.9619, best val acc: 0.9599, best train loss: 0.1133  @ epoch 23005 )\n",
      "[Epoch: 23200] train loss: 0.1317, train acc: 0.9524, val loss: 0.2059, val acc: 0.9430  (best train acc: 0.9619, best val acc: 0.9599, best train loss: 0.1133  @ epoch 23005 )\n",
      "[Epoch: 23220] train loss: 0.1339, train acc: 0.9547, val loss: 0.2024, val acc: 0.9524  (best train acc: 0.9619, best val acc: 0.9599, best train loss: 0.1133  @ epoch 23005 )\n",
      "[Epoch: 23240] train loss: 0.1209, train acc: 0.9574, val loss: 0.1987, val acc: 0.9558  (best train acc: 0.9619, best val acc: 0.9599, best train loss: 0.1133  @ epoch 23005 )\n",
      "[Epoch: 23260] train loss: 0.1227, train acc: 0.9591, val loss: 0.2038, val acc: 0.9545  (best train acc: 0.9619, best val acc: 0.9599, best train loss: 0.1133  @ epoch 23005 )\n",
      "[Epoch: 23280] train loss: 0.1519, train acc: 0.9459, val loss: 0.2098, val acc: 0.9447  (best train acc: 0.9619, best val acc: 0.9599, best train loss: 0.1133  @ epoch 23005 )\n",
      "[Epoch: 23300] train loss: 0.1270, train acc: 0.9560, val loss: 0.2125, val acc: 0.9497  (best train acc: 0.9619, best val acc: 0.9599, best train loss: 0.1133  @ epoch 23005 )\n",
      "[Epoch: 23320] train loss: 0.1633, train acc: 0.9391, val loss: 0.2231, val acc: 0.9386  (best train acc: 0.9619, best val acc: 0.9599, best train loss: 0.1133  @ epoch 23005 )\n",
      "[Epoch: 23340] train loss: 0.1462, train acc: 0.9485, val loss: 0.1977, val acc: 0.9538  (best train acc: 0.9619, best val acc: 0.9599, best train loss: 0.1133  @ epoch 23005 )\n",
      "[Epoch: 23360] train loss: 0.1327, train acc: 0.9529, val loss: 0.2068, val acc: 0.9508  (best train acc: 0.9619, best val acc: 0.9599, best train loss: 0.1133  @ epoch 23005 )\n",
      "[Epoch: 23380] train loss: 0.1238, train acc: 0.9569, val loss: 0.2038, val acc: 0.9545  (best train acc: 0.9619, best val acc: 0.9599, best train loss: 0.1133  @ epoch 23005 )\n",
      "[Epoch: 23400] train loss: 0.1503, train acc: 0.9459, val loss: 0.2159, val acc: 0.9440  (best train acc: 0.9619, best val acc: 0.9599, best train loss: 0.1133  @ epoch 23005 )\n",
      "[Epoch: 23420] train loss: 0.1305, train acc: 0.9533, val loss: 0.2114, val acc: 0.9545  (best train acc: 0.9619, best val acc: 0.9599, best train loss: 0.1133  @ epoch 23005 )\n",
      "[Epoch: 23440] train loss: 0.1336, train acc: 0.9564, val loss: 0.2026, val acc: 0.9521  (best train acc: 0.9619, best val acc: 0.9599, best train loss: 0.1133  @ epoch 23005 )\n",
      "[Epoch: 23460] train loss: 0.1324, train acc: 0.9563, val loss: 0.1997, val acc: 0.9501  (best train acc: 0.9619, best val acc: 0.9599, best train loss: 0.1133  @ epoch 23005 )\n",
      "[Epoch: 23480] train loss: 0.1228, train acc: 0.9563, val loss: 0.2019, val acc: 0.9538  (best train acc: 0.9619, best val acc: 0.9599, best train loss: 0.1133  @ epoch 23005 )\n",
      "[Epoch: 23500] train loss: 0.1347, train acc: 0.9545, val loss: 0.2088, val acc: 0.9535  (best train acc: 0.9624, best val acc: 0.9599, best train loss: 0.1133  @ epoch 23005 )\n",
      "[Epoch: 23520] train loss: 0.1190, train acc: 0.9591, val loss: 0.2080, val acc: 0.9565  (best train acc: 0.9624, best val acc: 0.9599, best train loss: 0.1133  @ epoch 23005 )\n",
      "[Epoch: 23540] train loss: 0.1427, train acc: 0.9485, val loss: 0.2126, val acc: 0.9551  (best train acc: 0.9624, best val acc: 0.9599, best train loss: 0.1133  @ epoch 23005 )\n",
      "[Epoch: 23560] train loss: 0.1330, train acc: 0.9564, val loss: 0.1973, val acc: 0.9524  (best train acc: 0.9624, best val acc: 0.9599, best train loss: 0.1133  @ epoch 23005 )\n",
      "[Epoch: 23580] train loss: 0.1274, train acc: 0.9576, val loss: 0.2017, val acc: 0.9535  (best train acc: 0.9624, best val acc: 0.9599, best train loss: 0.1133  @ epoch 23005 )\n",
      "[Epoch: 23600] train loss: 0.1182, train acc: 0.9581, val loss: 0.1999, val acc: 0.9548  (best train acc: 0.9624, best val acc: 0.9599, best train loss: 0.1133  @ epoch 23005 )\n",
      "[Epoch: 23620] train loss: 0.1341, train acc: 0.9518, val loss: 0.2167, val acc: 0.9535  (best train acc: 0.9624, best val acc: 0.9599, best train loss: 0.1130  @ epoch 23615 )\n",
      "[Epoch: 23640] train loss: 0.1382, train acc: 0.9522, val loss: 0.2071, val acc: 0.9497  (best train acc: 0.9624, best val acc: 0.9599, best train loss: 0.1130  @ epoch 23615 )\n",
      "[Epoch: 23660] train loss: 0.1299, train acc: 0.9559, val loss: 0.2212, val acc: 0.9524  (best train acc: 0.9624, best val acc: 0.9599, best train loss: 0.1130  @ epoch 23615 )\n",
      "[Epoch: 23680] train loss: 0.1451, train acc: 0.9486, val loss: 0.1920, val acc: 0.9528  (best train acc: 0.9624, best val acc: 0.9599, best train loss: 0.1130  @ epoch 23615 )\n",
      "[Epoch: 23700] train loss: 0.1332, train acc: 0.9495, val loss: 0.2066, val acc: 0.9497  (best train acc: 0.9624, best val acc: 0.9599, best train loss: 0.1130  @ epoch 23615 )\n",
      "[Epoch: 23720] train loss: 0.1343, train acc: 0.9545, val loss: 0.2166, val acc: 0.9511  (best train acc: 0.9624, best val acc: 0.9599, best train loss: 0.1130  @ epoch 23615 )\n",
      "[Epoch: 23740] train loss: 0.1228, train acc: 0.9588, val loss: 0.2000, val acc: 0.9545  (best train acc: 0.9635, best val acc: 0.9599, best train loss: 0.1130  @ epoch 23615 )\n",
      "[Epoch: 23760] train loss: 0.1171, train acc: 0.9617, val loss: 0.1967, val acc: 0.9535  (best train acc: 0.9635, best val acc: 0.9599, best train loss: 0.1130  @ epoch 23615 )\n",
      "[Epoch: 23780] train loss: 0.1192, train acc: 0.9610, val loss: 0.2067, val acc: 0.9558  (best train acc: 0.9635, best val acc: 0.9599, best train loss: 0.1130  @ epoch 23615 )\n",
      "[Epoch: 23800] train loss: 0.1288, train acc: 0.9561, val loss: 0.2070, val acc: 0.9528  (best train acc: 0.9635, best val acc: 0.9599, best train loss: 0.1130  @ epoch 23615 )\n",
      "[Epoch: 23820] train loss: 0.1175, train acc: 0.9594, val loss: 0.2040, val acc: 0.9575  (best train acc: 0.9635, best val acc: 0.9599, best train loss: 0.1130  @ epoch 23615 )\n",
      "[Epoch: 23840] train loss: 0.1218, train acc: 0.9560, val loss: 0.2121, val acc: 0.9528  (best train acc: 0.9635, best val acc: 0.9599, best train loss: 0.1130  @ epoch 23615 )\n",
      "[Epoch: 23860] train loss: 0.1304, train acc: 0.9552, val loss: 0.2244, val acc: 0.9423  (best train acc: 0.9635, best val acc: 0.9599, best train loss: 0.1130  @ epoch 23615 )\n",
      "[Epoch: 23880] train loss: 0.1292, train acc: 0.9545, val loss: 0.2068, val acc: 0.9511  (best train acc: 0.9635, best val acc: 0.9599, best train loss: 0.1130  @ epoch 23615 )\n",
      "[Epoch: 23900] train loss: 0.1322, train acc: 0.9535, val loss: 0.2211, val acc: 0.9487  (best train acc: 0.9635, best val acc: 0.9599, best train loss: 0.1130  @ epoch 23615 )\n",
      "[Epoch: 23920] train loss: 0.1439, train acc: 0.9532, val loss: 0.1975, val acc: 0.9548  (best train acc: 0.9635, best val acc: 0.9599, best train loss: 0.1130  @ epoch 23615 )\n",
      "[Epoch: 23940] train loss: 0.1173, train acc: 0.9606, val loss: 0.2020, val acc: 0.9518  (best train acc: 0.9635, best val acc: 0.9599, best train loss: 0.1130  @ epoch 23615 )\n",
      "[Epoch: 23960] train loss: 0.1247, train acc: 0.9576, val loss: 0.2056, val acc: 0.9538  (best train acc: 0.9635, best val acc: 0.9599, best train loss: 0.1130  @ epoch 23615 )\n",
      "[Epoch: 23980] train loss: 0.1321, train acc: 0.9516, val loss: 0.2086, val acc: 0.9524  (best train acc: 0.9635, best val acc: 0.9599, best train loss: 0.1130  @ epoch 23615 )\n",
      "[Epoch: 24000] train loss: 0.1295, train acc: 0.9552, val loss: 0.2121, val acc: 0.9514  (best train acc: 0.9635, best val acc: 0.9599, best train loss: 0.1130  @ epoch 23615 )\n",
      "[Epoch: 24020] train loss: 0.1442, train acc: 0.9501, val loss: 0.2198, val acc: 0.9501  (best train acc: 0.9635, best val acc: 0.9599, best train loss: 0.1130  @ epoch 23615 )\n",
      "[Epoch: 24040] train loss: 0.1290, train acc: 0.9517, val loss: 0.2090, val acc: 0.9454  (best train acc: 0.9635, best val acc: 0.9599, best train loss: 0.1130  @ epoch 23615 )\n",
      "[Epoch: 24060] train loss: 0.1294, train acc: 0.9542, val loss: 0.2097, val acc: 0.9511  (best train acc: 0.9635, best val acc: 0.9599, best train loss: 0.1130  @ epoch 23615 )\n",
      "[Epoch: 24080] train loss: 0.1374, train acc: 0.9550, val loss: 0.2095, val acc: 0.9555  (best train acc: 0.9635, best val acc: 0.9599, best train loss: 0.1130  @ epoch 23615 )\n",
      "[Epoch: 24100] train loss: 0.1242, train acc: 0.9566, val loss: 0.2122, val acc: 0.9508  (best train acc: 0.9635, best val acc: 0.9599, best train loss: 0.1130  @ epoch 23615 )\n",
      "[Epoch: 24120] train loss: 0.1248, train acc: 0.9581, val loss: 0.2020, val acc: 0.9548  (best train acc: 0.9635, best val acc: 0.9599, best train loss: 0.1130  @ epoch 23615 )\n",
      "[Epoch: 24140] train loss: 0.1307, train acc: 0.9531, val loss: 0.1964, val acc: 0.9524  (best train acc: 0.9635, best val acc: 0.9599, best train loss: 0.1130  @ epoch 23615 )\n",
      "[Epoch: 24160] train loss: 0.1297, train acc: 0.9552, val loss: 0.2117, val acc: 0.9501  (best train acc: 0.9635, best val acc: 0.9599, best train loss: 0.1130  @ epoch 23615 )\n",
      "[Epoch: 24180] train loss: 0.1257, train acc: 0.9549, val loss: 0.2125, val acc: 0.9528  (best train acc: 0.9635, best val acc: 0.9599, best train loss: 0.1130  @ epoch 23615 )\n",
      "[Epoch: 24200] train loss: 0.1273, train acc: 0.9525, val loss: 0.2145, val acc: 0.9508  (best train acc: 0.9635, best val acc: 0.9599, best train loss: 0.1130  @ epoch 23615 )\n",
      "[Epoch: 24220] train loss: 0.1282, train acc: 0.9560, val loss: 0.2037, val acc: 0.9528  (best train acc: 0.9635, best val acc: 0.9599, best train loss: 0.1130  @ epoch 23615 )\n",
      "[Epoch: 24240] train loss: 0.1322, train acc: 0.9533, val loss: 0.2226, val acc: 0.9474  (best train acc: 0.9635, best val acc: 0.9599, best train loss: 0.1130  @ epoch 23615 )\n",
      "[Epoch: 24260] train loss: 0.1454, train acc: 0.9503, val loss: 0.2158, val acc: 0.9481  (best train acc: 0.9635, best val acc: 0.9599, best train loss: 0.1130  @ epoch 23615 )\n",
      "[Epoch: 24280] train loss: 0.1579, train acc: 0.9472, val loss: 0.2031, val acc: 0.9514  (best train acc: 0.9635, best val acc: 0.9599, best train loss: 0.1130  @ epoch 23615 )\n",
      "[Epoch: 24300] train loss: 0.1238, train acc: 0.9564, val loss: 0.2173, val acc: 0.9524  (best train acc: 0.9635, best val acc: 0.9599, best train loss: 0.1130  @ epoch 23615 )\n",
      "[Epoch: 24320] train loss: 0.1311, train acc: 0.9552, val loss: 0.2047, val acc: 0.9558  (best train acc: 0.9635, best val acc: 0.9599, best train loss: 0.1130  @ epoch 23615 )\n",
      "[Epoch: 24340] train loss: 0.1268, train acc: 0.9539, val loss: 0.2092, val acc: 0.9514  (best train acc: 0.9635, best val acc: 0.9599, best train loss: 0.1130  @ epoch 23615 )\n",
      "[Epoch: 24360] train loss: 0.1292, train acc: 0.9533, val loss: 0.2019, val acc: 0.9551  (best train acc: 0.9635, best val acc: 0.9599, best train loss: 0.1130  @ epoch 23615 )\n",
      "[Epoch: 24380] train loss: 0.1469, train acc: 0.9461, val loss: 0.2037, val acc: 0.9538  (best train acc: 0.9635, best val acc: 0.9599, best train loss: 0.1130  @ epoch 23615 )\n",
      "[Epoch: 24400] train loss: 0.1167, train acc: 0.9597, val loss: 0.2101, val acc: 0.9548  (best train acc: 0.9635, best val acc: 0.9599, best train loss: 0.1130  @ epoch 23615 )\n",
      "[Epoch: 24420] train loss: 0.1218, train acc: 0.9573, val loss: 0.2237, val acc: 0.9491  (best train acc: 0.9635, best val acc: 0.9599, best train loss: 0.1130  @ epoch 23615 )\n",
      "[Epoch: 24440] train loss: 0.1262, train acc: 0.9551, val loss: 0.2210, val acc: 0.9504  (best train acc: 0.9635, best val acc: 0.9599, best train loss: 0.1130  @ epoch 23615 )\n",
      "[Epoch: 24460] train loss: 0.1362, train acc: 0.9518, val loss: 0.2115, val acc: 0.9528  (best train acc: 0.9635, best val acc: 0.9599, best train loss: 0.1130  @ epoch 23615 )\n",
      "[Epoch: 24480] train loss: 0.1291, train acc: 0.9552, val loss: 0.2016, val acc: 0.9508  (best train acc: 0.9635, best val acc: 0.9599, best train loss: 0.1130  @ epoch 23615 )\n",
      "[Epoch: 24500] train loss: 0.1675, train acc: 0.9441, val loss: 0.2332, val acc: 0.9396  (best train acc: 0.9635, best val acc: 0.9599, best train loss: 0.1130  @ epoch 23615 )\n",
      "[Epoch: 24520] train loss: 0.1286, train acc: 0.9537, val loss: 0.1958, val acc: 0.9501  (best train acc: 0.9635, best val acc: 0.9599, best train loss: 0.1130  @ epoch 23615 )\n",
      "[Epoch: 24540] train loss: 0.1261, train acc: 0.9534, val loss: 0.2180, val acc: 0.9467  (best train acc: 0.9635, best val acc: 0.9599, best train loss: 0.1130  @ epoch 23615 )\n",
      "[Epoch: 24560] train loss: 0.1177, train acc: 0.9585, val loss: 0.1991, val acc: 0.9551  (best train acc: 0.9635, best val acc: 0.9599, best train loss: 0.1130  @ epoch 23615 )\n",
      "[Epoch: 24580] train loss: 0.1306, train acc: 0.9542, val loss: 0.2179, val acc: 0.9501  (best train acc: 0.9635, best val acc: 0.9599, best train loss: 0.1130  @ epoch 23615 )\n",
      "[Epoch: 24600] train loss: 0.1171, train acc: 0.9582, val loss: 0.2082, val acc: 0.9548  (best train acc: 0.9635, best val acc: 0.9599, best train loss: 0.1130  @ epoch 23615 )\n",
      "[Epoch: 24620] train loss: 0.1981, train acc: 0.9334, val loss: 0.2443, val acc: 0.9373  (best train acc: 0.9635, best val acc: 0.9599, best train loss: 0.1130  @ epoch 23615 )\n",
      "[Epoch: 24640] train loss: 0.1493, train acc: 0.9524, val loss: 0.2137, val acc: 0.9491  (best train acc: 0.9635, best val acc: 0.9599, best train loss: 0.1130  @ epoch 23615 )\n",
      "[Epoch: 24660] train loss: 0.1323, train acc: 0.9522, val loss: 0.1951, val acc: 0.9535  (best train acc: 0.9635, best val acc: 0.9599, best train loss: 0.1130  @ epoch 23615 )\n",
      "[Epoch: 24680] train loss: 0.1214, train acc: 0.9584, val loss: 0.2117, val acc: 0.9551  (best train acc: 0.9635, best val acc: 0.9599, best train loss: 0.1130  @ epoch 23615 )\n",
      "[Epoch: 24700] train loss: 0.1257, train acc: 0.9568, val loss: 0.2001, val acc: 0.9565  (best train acc: 0.9635, best val acc: 0.9599, best train loss: 0.1130  @ epoch 23615 )\n",
      "[Epoch: 24720] train loss: 0.1341, train acc: 0.9535, val loss: 0.2260, val acc: 0.9477  (best train acc: 0.9635, best val acc: 0.9599, best train loss: 0.1106  @ epoch 24708 )\n",
      "[Epoch: 24740] train loss: 0.1274, train acc: 0.9563, val loss: 0.2078, val acc: 0.9538  (best train acc: 0.9635, best val acc: 0.9599, best train loss: 0.1106  @ epoch 24708 )\n",
      "[Epoch: 24760] train loss: 0.1286, train acc: 0.9579, val loss: 0.2183, val acc: 0.9521  (best train acc: 0.9635, best val acc: 0.9599, best train loss: 0.1106  @ epoch 24708 )\n",
      "[Epoch: 24780] train loss: 0.1325, train acc: 0.9526, val loss: 0.2029, val acc: 0.9497  (best train acc: 0.9635, best val acc: 0.9599, best train loss: 0.1106  @ epoch 24708 )\n",
      "[Epoch: 24800] train loss: 0.1229, train acc: 0.9571, val loss: 0.2135, val acc: 0.9558  (best train acc: 0.9635, best val acc: 0.9599, best train loss: 0.1106  @ epoch 24708 )\n",
      "[Epoch: 24820] train loss: 0.1176, train acc: 0.9594, val loss: 0.2109, val acc: 0.9504  (best train acc: 0.9635, best val acc: 0.9599, best train loss: 0.1106  @ epoch 24708 )\n",
      "[Epoch: 24840] train loss: 0.1220, train acc: 0.9557, val loss: 0.2118, val acc: 0.9528  (best train acc: 0.9635, best val acc: 0.9599, best train loss: 0.1106  @ epoch 24708 )\n",
      "[Epoch: 24860] train loss: 0.1199, train acc: 0.9587, val loss: 0.1968, val acc: 0.9558  (best train acc: 0.9635, best val acc: 0.9599, best train loss: 0.1106  @ epoch 24708 )\n",
      "[Epoch: 24880] train loss: 0.1224, train acc: 0.9568, val loss: 0.2154, val acc: 0.9518  (best train acc: 0.9635, best val acc: 0.9599, best train loss: 0.1106  @ epoch 24708 )\n",
      "[Epoch: 24900] train loss: 0.1217, train acc: 0.9563, val loss: 0.2156, val acc: 0.9538  (best train acc: 0.9635, best val acc: 0.9599, best train loss: 0.1102  @ epoch 24890 )\n",
      "[Epoch: 24920] train loss: 0.1449, train acc: 0.9483, val loss: 0.2310, val acc: 0.9464  (best train acc: 0.9635, best val acc: 0.9599, best train loss: 0.1102  @ epoch 24890 )\n",
      "[Epoch: 24940] train loss: 0.1292, train acc: 0.9512, val loss: 0.2196, val acc: 0.9514  (best train acc: 0.9635, best val acc: 0.9599, best train loss: 0.1102  @ epoch 24890 )\n",
      "[Epoch: 24960] train loss: 0.1205, train acc: 0.9562, val loss: 0.1992, val acc: 0.9528  (best train acc: 0.9635, best val acc: 0.9599, best train loss: 0.1102  @ epoch 24890 )\n",
      "[Epoch: 24980] train loss: 0.1252, train acc: 0.9572, val loss: 0.2143, val acc: 0.9531  (best train acc: 0.9635, best val acc: 0.9599, best train loss: 0.1102  @ epoch 24890 )\n",
      "[Epoch: 25000] train loss: 0.1298, train acc: 0.9523, val loss: 0.2236, val acc: 0.9497  (best train acc: 0.9635, best val acc: 0.9599, best train loss: 0.1102  @ epoch 24890 )\n",
      "[Epoch: 25020] train loss: 0.1253, train acc: 0.9541, val loss: 0.2125, val acc: 0.9524  (best train acc: 0.9635, best val acc: 0.9599, best train loss: 0.1102  @ epoch 24890 )\n",
      "[Epoch: 25040] train loss: 0.1212, train acc: 0.9581, val loss: 0.1979, val acc: 0.9558  (best train acc: 0.9635, best val acc: 0.9599, best train loss: 0.1102  @ epoch 24890 )\n",
      "[Epoch: 25060] train loss: 0.1240, train acc: 0.9558, val loss: 0.2072, val acc: 0.9535  (best train acc: 0.9635, best val acc: 0.9599, best train loss: 0.1102  @ epoch 24890 )\n",
      "[Epoch: 25080] train loss: 0.1189, train acc: 0.9594, val loss: 0.2097, val acc: 0.9562  (best train acc: 0.9635, best val acc: 0.9599, best train loss: 0.1102  @ epoch 24890 )\n",
      "[Epoch: 25100] train loss: 0.1326, train acc: 0.9557, val loss: 0.1958, val acc: 0.9551  (best train acc: 0.9635, best val acc: 0.9599, best train loss: 0.1102  @ epoch 24890 )\n",
      "[Epoch: 25120] train loss: 0.1135, train acc: 0.9605, val loss: 0.2028, val acc: 0.9548  (best train acc: 0.9635, best val acc: 0.9599, best train loss: 0.1102  @ epoch 24890 )\n",
      "[Epoch: 25140] train loss: 0.1324, train acc: 0.9532, val loss: 0.2105, val acc: 0.9528  (best train acc: 0.9635, best val acc: 0.9599, best train loss: 0.1102  @ epoch 24890 )\n",
      "[Epoch: 25160] train loss: 0.1340, train acc: 0.9520, val loss: 0.2210, val acc: 0.9457  (best train acc: 0.9635, best val acc: 0.9599, best train loss: 0.1102  @ epoch 24890 )\n",
      "[Epoch: 25180] train loss: 0.1289, train acc: 0.9542, val loss: 0.2176, val acc: 0.9481  (best train acc: 0.9635, best val acc: 0.9599, best train loss: 0.1102  @ epoch 24890 )\n",
      "[Epoch: 25200] train loss: 0.1408, train acc: 0.9432, val loss: 0.2463, val acc: 0.9292  (best train acc: 0.9635, best val acc: 0.9599, best train loss: 0.1102  @ epoch 24890 )\n",
      "[Epoch: 25220] train loss: 0.1207, train acc: 0.9584, val loss: 0.2092, val acc: 0.9562  (best train acc: 0.9635, best val acc: 0.9599, best train loss: 0.1102  @ epoch 24890 )\n",
      "[Epoch: 25240] train loss: 0.1342, train acc: 0.9503, val loss: 0.2011, val acc: 0.9548  (best train acc: 0.9635, best val acc: 0.9599, best train loss: 0.1102  @ epoch 24890 )\n",
      "[Epoch: 25260] train loss: 0.1197, train acc: 0.9567, val loss: 0.2125, val acc: 0.9511  (best train acc: 0.9635, best val acc: 0.9599, best train loss: 0.1102  @ epoch 24890 )\n",
      "[Epoch: 25280] train loss: 0.1218, train acc: 0.9554, val loss: 0.2135, val acc: 0.9545  (best train acc: 0.9635, best val acc: 0.9599, best train loss: 0.1102  @ epoch 24890 )\n",
      "[Epoch: 25300] train loss: 0.1164, train acc: 0.9591, val loss: 0.2045, val acc: 0.9535  (best train acc: 0.9635, best val acc: 0.9599, best train loss: 0.1102  @ epoch 24890 )\n",
      "[Epoch: 25320] train loss: 0.1462, train acc: 0.9436, val loss: 0.2262, val acc: 0.9454  (best train acc: 0.9635, best val acc: 0.9599, best train loss: 0.1102  @ epoch 24890 )\n",
      "[Epoch: 25340] train loss: 0.1236, train acc: 0.9546, val loss: 0.2162, val acc: 0.9518  (best train acc: 0.9635, best val acc: 0.9599, best train loss: 0.1102  @ epoch 24890 )\n",
      "[Epoch: 25360] train loss: 0.1611, train acc: 0.9420, val loss: 0.2072, val acc: 0.9460  (best train acc: 0.9635, best val acc: 0.9599, best train loss: 0.1102  @ epoch 24890 )\n",
      "[Epoch: 25380] train loss: 0.1138, train acc: 0.9597, val loss: 0.2167, val acc: 0.9528  (best train acc: 0.9635, best val acc: 0.9599, best train loss: 0.1102  @ epoch 24890 )\n",
      "[Epoch: 25400] train loss: 0.1272, train acc: 0.9576, val loss: 0.2059, val acc: 0.9504  (best train acc: 0.9635, best val acc: 0.9599, best train loss: 0.1102  @ epoch 24890 )\n",
      "[Epoch: 25420] train loss: 0.1298, train acc: 0.9529, val loss: 0.1979, val acc: 0.9555  (best train acc: 0.9635, best val acc: 0.9599, best train loss: 0.1102  @ epoch 24890 )\n",
      "[Epoch: 25440] train loss: 0.1176, train acc: 0.9608, val loss: 0.2111, val acc: 0.9528  (best train acc: 0.9635, best val acc: 0.9599, best train loss: 0.1102  @ epoch 24890 )\n",
      "[Epoch: 25460] train loss: 0.1232, train acc: 0.9581, val loss: 0.2063, val acc: 0.9538  (best train acc: 0.9635, best val acc: 0.9599, best train loss: 0.1102  @ epoch 24890 )\n",
      "[Epoch: 25480] train loss: 0.1192, train acc: 0.9565, val loss: 0.2080, val acc: 0.9545  (best train acc: 0.9635, best val acc: 0.9599, best train loss: 0.1102  @ epoch 24890 )\n",
      "[Epoch: 25500] train loss: 0.1189, train acc: 0.9589, val loss: 0.2166, val acc: 0.9538  (best train acc: 0.9635, best val acc: 0.9599, best train loss: 0.1102  @ epoch 24890 )\n",
      "[Epoch: 25520] train loss: 0.1344, train acc: 0.9492, val loss: 0.2095, val acc: 0.9535  (best train acc: 0.9635, best val acc: 0.9599, best train loss: 0.1102  @ epoch 24890 )\n",
      "[Epoch: 25540] train loss: 0.1332, train acc: 0.9516, val loss: 0.2136, val acc: 0.9514  (best train acc: 0.9635, best val acc: 0.9599, best train loss: 0.1102  @ epoch 24890 )\n",
      "[Epoch: 25560] train loss: 0.1226, train acc: 0.9599, val loss: 0.1946, val acc: 0.9565  (best train acc: 0.9635, best val acc: 0.9599, best train loss: 0.1102  @ epoch 24890 )\n",
      "[Epoch: 25580] train loss: 0.1255, train acc: 0.9543, val loss: 0.2113, val acc: 0.9551  (best train acc: 0.9635, best val acc: 0.9599, best train loss: 0.1102  @ epoch 24890 )\n",
      "[Epoch: 25600] train loss: 0.1253, train acc: 0.9564, val loss: 0.2154, val acc: 0.9541  (best train acc: 0.9635, best val acc: 0.9599, best train loss: 0.1102  @ epoch 24890 )\n",
      "[Epoch: 25620] train loss: 0.1156, train acc: 0.9601, val loss: 0.2125, val acc: 0.9565  (best train acc: 0.9635, best val acc: 0.9599, best train loss: 0.1099  @ epoch 25615 )\n",
      "[Epoch: 25640] train loss: 0.1194, train acc: 0.9581, val loss: 0.2017, val acc: 0.9545  (best train acc: 0.9635, best val acc: 0.9599, best train loss: 0.1099  @ epoch 25615 )\n",
      "[Epoch: 25660] train loss: 0.1127, train acc: 0.9616, val loss: 0.2081, val acc: 0.9562  (best train acc: 0.9639, best val acc: 0.9599, best train loss: 0.1042  @ epoch 25655 )\n",
      "[Epoch: 25680] train loss: 0.1093, train acc: 0.9626, val loss: 0.2163, val acc: 0.9558  (best train acc: 0.9639, best val acc: 0.9599, best train loss: 0.1042  @ epoch 25655 )\n",
      "[Epoch: 25700] train loss: 0.1419, train acc: 0.9425, val loss: 0.2647, val acc: 0.9224  (best train acc: 0.9639, best val acc: 0.9599, best train loss: 0.1042  @ epoch 25655 )\n",
      "[Epoch: 25720] train loss: 0.1911, train acc: 0.9452, val loss: 0.2150, val acc: 0.9518  (best train acc: 0.9639, best val acc: 0.9599, best train loss: 0.1042  @ epoch 25655 )\n",
      "[Epoch: 25740] train loss: 0.1442, train acc: 0.9572, val loss: 0.2060, val acc: 0.9538  (best train acc: 0.9639, best val acc: 0.9599, best train loss: 0.1042  @ epoch 25655 )\n",
      "[Epoch: 25760] train loss: 0.1194, train acc: 0.9592, val loss: 0.2144, val acc: 0.9551  (best train acc: 0.9639, best val acc: 0.9599, best train loss: 0.1042  @ epoch 25655 )\n",
      "[Epoch: 25780] train loss: 0.1237, train acc: 0.9579, val loss: 0.2085, val acc: 0.9558  (best train acc: 0.9639, best val acc: 0.9599, best train loss: 0.1042  @ epoch 25655 )\n",
      "[Epoch: 25800] train loss: 0.1318, train acc: 0.9517, val loss: 0.2102, val acc: 0.9521  (best train acc: 0.9639, best val acc: 0.9599, best train loss: 0.1042  @ epoch 25655 )\n",
      "[Epoch: 25820] train loss: 0.1204, train acc: 0.9577, val loss: 0.2124, val acc: 0.9551  (best train acc: 0.9639, best val acc: 0.9599, best train loss: 0.1042  @ epoch 25655 )\n",
      "[Epoch: 25840] train loss: 0.1205, train acc: 0.9591, val loss: 0.2070, val acc: 0.9514  (best train acc: 0.9639, best val acc: 0.9599, best train loss: 0.1042  @ epoch 25655 )\n",
      "[Epoch: 25860] train loss: 0.1271, train acc: 0.9532, val loss: 0.2120, val acc: 0.9562  (best train acc: 0.9639, best val acc: 0.9599, best train loss: 0.1042  @ epoch 25655 )\n",
      "[Epoch: 25880] train loss: 0.1208, train acc: 0.9602, val loss: 0.2073, val acc: 0.9565  (best train acc: 0.9639, best val acc: 0.9599, best train loss: 0.1042  @ epoch 25655 )\n",
      "[Epoch: 25900] train loss: 0.1213, train acc: 0.9555, val loss: 0.2088, val acc: 0.9535  (best train acc: 0.9639, best val acc: 0.9599, best train loss: 0.1042  @ epoch 25655 )\n",
      "[Epoch: 25920] train loss: 0.1328, train acc: 0.9505, val loss: 0.2206, val acc: 0.9514  (best train acc: 0.9639, best val acc: 0.9599, best train loss: 0.1042  @ epoch 25655 )\n",
      "[Epoch: 25940] train loss: 0.1380, train acc: 0.9512, val loss: 0.2000, val acc: 0.9545  (best train acc: 0.9639, best val acc: 0.9599, best train loss: 0.1042  @ epoch 25655 )\n",
      "[Epoch: 25960] train loss: 0.1233, train acc: 0.9568, val loss: 0.2142, val acc: 0.9548  (best train acc: 0.9639, best val acc: 0.9599, best train loss: 0.1042  @ epoch 25655 )\n",
      "[Epoch: 25980] train loss: 0.1441, train acc: 0.9451, val loss: 0.2133, val acc: 0.9535  (best train acc: 0.9639, best val acc: 0.9599, best train loss: 0.1042  @ epoch 25655 )\n",
      "[Epoch: 26000] train loss: 0.1149, train acc: 0.9605, val loss: 0.2216, val acc: 0.9528  (best train acc: 0.9639, best val acc: 0.9599, best train loss: 0.1042  @ epoch 25655 )\n",
      "[Epoch: 26020] train loss: 0.1170, train acc: 0.9584, val loss: 0.2112, val acc: 0.9541  (best train acc: 0.9639, best val acc: 0.9599, best train loss: 0.1042  @ epoch 25655 )\n",
      "[Epoch: 26040] train loss: 0.1168, train acc: 0.9584, val loss: 0.2069, val acc: 0.9562  (best train acc: 0.9639, best val acc: 0.9599, best train loss: 0.1042  @ epoch 25655 )\n",
      "[Epoch: 26060] train loss: 0.1251, train acc: 0.9532, val loss: 0.2147, val acc: 0.9447  (best train acc: 0.9639, best val acc: 0.9599, best train loss: 0.1042  @ epoch 25655 )\n",
      "[Epoch: 26080] train loss: 0.1262, train acc: 0.9556, val loss: 0.2071, val acc: 0.9535  (best train acc: 0.9639, best val acc: 0.9599, best train loss: 0.1042  @ epoch 25655 )\n",
      "[Epoch: 26100] train loss: 0.1225, train acc: 0.9573, val loss: 0.2062, val acc: 0.9528  (best train acc: 0.9639, best val acc: 0.9599, best train loss: 0.1042  @ epoch 25655 )\n",
      "[Epoch: 26120] train loss: 0.1212, train acc: 0.9567, val loss: 0.2134, val acc: 0.9518  (best train acc: 0.9639, best val acc: 0.9599, best train loss: 0.1042  @ epoch 25655 )\n",
      "[Epoch: 26140] train loss: 0.1436, train acc: 0.9476, val loss: 0.2274, val acc: 0.9447  (best train acc: 0.9639, best val acc: 0.9599, best train loss: 0.1042  @ epoch 25655 )\n",
      "[Epoch: 26160] train loss: 0.1269, train acc: 0.9538, val loss: 0.2002, val acc: 0.9514  (best train acc: 0.9639, best val acc: 0.9599, best train loss: 0.1042  @ epoch 25655 )\n",
      "[Epoch: 26180] train loss: 0.1121, train acc: 0.9608, val loss: 0.2054, val acc: 0.9538  (best train acc: 0.9639, best val acc: 0.9599, best train loss: 0.1042  @ epoch 25655 )\n",
      "[Epoch: 26200] train loss: 0.1189, train acc: 0.9620, val loss: 0.2026, val acc: 0.9578  (best train acc: 0.9639, best val acc: 0.9599, best train loss: 0.1042  @ epoch 25655 )\n",
      "[Epoch: 26220] train loss: 0.1330, train acc: 0.9515, val loss: 0.2120, val acc: 0.9467  (best train acc: 0.9639, best val acc: 0.9599, best train loss: 0.1042  @ epoch 25655 )\n",
      "[Epoch: 26240] train loss: 0.1268, train acc: 0.9555, val loss: 0.2291, val acc: 0.9497  (best train acc: 0.9639, best val acc: 0.9599, best train loss: 0.1042  @ epoch 25655 )\n",
      "[Epoch: 26260] train loss: 0.1202, train acc: 0.9583, val loss: 0.2003, val acc: 0.9548  (best train acc: 0.9639, best val acc: 0.9599, best train loss: 0.1042  @ epoch 25655 )\n",
      "[Epoch: 26280] train loss: 0.1340, train acc: 0.9512, val loss: 0.2165, val acc: 0.9521  (best train acc: 0.9639, best val acc: 0.9599, best train loss: 0.1042  @ epoch 25655 )\n",
      "[Epoch: 26300] train loss: 0.1472, train acc: 0.9477, val loss: 0.2008, val acc: 0.9454  (best train acc: 0.9639, best val acc: 0.9599, best train loss: 0.1042  @ epoch 25655 )\n",
      "[Epoch: 26320] train loss: 0.1258, train acc: 0.9547, val loss: 0.2213, val acc: 0.9528  (best train acc: 0.9639, best val acc: 0.9599, best train loss: 0.1042  @ epoch 25655 )\n",
      "[Epoch: 26340] train loss: 0.1388, train acc: 0.9468, val loss: 0.2175, val acc: 0.9481  (best train acc: 0.9639, best val acc: 0.9599, best train loss: 0.1042  @ epoch 25655 )\n",
      "[Epoch: 26360] train loss: 0.1183, train acc: 0.9598, val loss: 0.2062, val acc: 0.9531  (best train acc: 0.9639, best val acc: 0.9599, best train loss: 0.1042  @ epoch 25655 )\n",
      "[Epoch: 26380] train loss: 0.1159, train acc: 0.9600, val loss: 0.2223, val acc: 0.9531  (best train acc: 0.9639, best val acc: 0.9599, best train loss: 0.1042  @ epoch 25655 )\n",
      "[Epoch: 26400] train loss: 0.1124, train acc: 0.9581, val loss: 0.2136, val acc: 0.9555  (best train acc: 0.9639, best val acc: 0.9599, best train loss: 0.1042  @ epoch 25655 )\n",
      "[Epoch: 26420] train loss: 0.1198, train acc: 0.9583, val loss: 0.2325, val acc: 0.9481  (best train acc: 0.9639, best val acc: 0.9599, best train loss: 0.1042  @ epoch 25655 )\n",
      "[Epoch: 26440] train loss: 0.1375, train acc: 0.9466, val loss: 0.2184, val acc: 0.9447  (best train acc: 0.9639, best val acc: 0.9599, best train loss: 0.1042  @ epoch 25655 )\n",
      "[Epoch: 26460] train loss: 0.1430, train acc: 0.9487, val loss: 0.2124, val acc: 0.9460  (best train acc: 0.9639, best val acc: 0.9599, best train loss: 0.1042  @ epoch 25655 )\n",
      "[Epoch: 26480] train loss: 0.1299, train acc: 0.9523, val loss: 0.2145, val acc: 0.9508  (best train acc: 0.9639, best val acc: 0.9599, best train loss: 0.1042  @ epoch 25655 )\n",
      "[Epoch: 26500] train loss: 0.1337, train acc: 0.9528, val loss: 0.2095, val acc: 0.9460  (best train acc: 0.9639, best val acc: 0.9599, best train loss: 0.1042  @ epoch 25655 )\n",
      "[Epoch: 26520] train loss: 0.1133, train acc: 0.9602, val loss: 0.2221, val acc: 0.9548  (best train acc: 0.9639, best val acc: 0.9599, best train loss: 0.1042  @ epoch 25655 )\n",
      "[Epoch: 26540] train loss: 0.1219, train acc: 0.9558, val loss: 0.2171, val acc: 0.9501  (best train acc: 0.9644, best val acc: 0.9599, best train loss: 0.1042  @ epoch 25655 )\n",
      "[Epoch: 26560] train loss: 0.1174, train acc: 0.9592, val loss: 0.2096, val acc: 0.9558  (best train acc: 0.9644, best val acc: 0.9599, best train loss: 0.1042  @ epoch 25655 )\n",
      "[Epoch: 26580] train loss: 0.1100, train acc: 0.9623, val loss: 0.2094, val acc: 0.9555  (best train acc: 0.9644, best val acc: 0.9599, best train loss: 0.1042  @ epoch 25655 )\n",
      "[Epoch: 26600] train loss: 0.1173, train acc: 0.9587, val loss: 0.2271, val acc: 0.9514  (best train acc: 0.9644, best val acc: 0.9599, best train loss: 0.1042  @ epoch 25655 )\n",
      "[Epoch: 26620] train loss: 0.1485, train acc: 0.9425, val loss: 0.2011, val acc: 0.9481  (best train acc: 0.9644, best val acc: 0.9599, best train loss: 0.1042  @ epoch 25655 )\n",
      "[Epoch: 26640] train loss: 0.1380, train acc: 0.9505, val loss: 0.2061, val acc: 0.9528  (best train acc: 0.9644, best val acc: 0.9599, best train loss: 0.1042  @ epoch 25655 )\n",
      "[Epoch: 26660] train loss: 0.1213, train acc: 0.9592, val loss: 0.2067, val acc: 0.9555  (best train acc: 0.9644, best val acc: 0.9599, best train loss: 0.1042  @ epoch 25655 )\n",
      "[Epoch: 26680] train loss: 0.1139, train acc: 0.9604, val loss: 0.1968, val acc: 0.9538  (best train acc: 0.9644, best val acc: 0.9599, best train loss: 0.1042  @ epoch 25655 )\n",
      "[Epoch: 26700] train loss: 0.1240, train acc: 0.9554, val loss: 0.2040, val acc: 0.9562  (best train acc: 0.9644, best val acc: 0.9599, best train loss: 0.1042  @ epoch 25655 )\n",
      "[Epoch: 26720] train loss: 0.1267, train acc: 0.9556, val loss: 0.1995, val acc: 0.9518  (best train acc: 0.9644, best val acc: 0.9599, best train loss: 0.1042  @ epoch 25655 )\n",
      "[Epoch: 26740] train loss: 0.1126, train acc: 0.9605, val loss: 0.1963, val acc: 0.9592  (best train acc: 0.9644, best val acc: 0.9599, best train loss: 0.1042  @ epoch 25655 )\n",
      "[Epoch: 26760] train loss: 0.1136, train acc: 0.9592, val loss: 0.2086, val acc: 0.9565  (best train acc: 0.9644, best val acc: 0.9599, best train loss: 0.1042  @ epoch 25655 )\n",
      "[Epoch: 26780] train loss: 0.1188, train acc: 0.9589, val loss: 0.2002, val acc: 0.9521  (best train acc: 0.9644, best val acc: 0.9599, best train loss: 0.1042  @ epoch 25655 )\n",
      "[Epoch: 26800] train loss: 0.1363, train acc: 0.9518, val loss: 0.2386, val acc: 0.9477  (best train acc: 0.9644, best val acc: 0.9599, best train loss: 0.1042  @ epoch 25655 )\n",
      "[Epoch: 26820] train loss: 0.1352, train acc: 0.9515, val loss: 0.1996, val acc: 0.9572  (best train acc: 0.9644, best val acc: 0.9599, best train loss: 0.1042  @ epoch 25655 )\n",
      "[Epoch: 26840] train loss: 0.1153, train acc: 0.9609, val loss: 0.2012, val acc: 0.9524  (best train acc: 0.9644, best val acc: 0.9599, best train loss: 0.1042  @ epoch 25655 )\n",
      "[Epoch: 26860] train loss: 0.1187, train acc: 0.9595, val loss: 0.2151, val acc: 0.9551  (best train acc: 0.9644, best val acc: 0.9599, best train loss: 0.1042  @ epoch 25655 )\n",
      "[Epoch: 26880] train loss: 0.1325, train acc: 0.9524, val loss: 0.2471, val acc: 0.9373  (best train acc: 0.9644, best val acc: 0.9599, best train loss: 0.1042  @ epoch 25655 )\n",
      "[Epoch: 26900] train loss: 0.1267, train acc: 0.9544, val loss: 0.2311, val acc: 0.9514  (best train acc: 0.9644, best val acc: 0.9599, best train loss: 0.1042  @ epoch 25655 )\n",
      "[Epoch: 26920] train loss: 0.1186, train acc: 0.9589, val loss: 0.2124, val acc: 0.9535  (best train acc: 0.9644, best val acc: 0.9599, best train loss: 0.1042  @ epoch 25655 )\n",
      "[Epoch: 26940] train loss: 0.1385, train acc: 0.9532, val loss: 0.2240, val acc: 0.9464  (best train acc: 0.9644, best val acc: 0.9599, best train loss: 0.1042  @ epoch 25655 )\n",
      "[Epoch: 26960] train loss: 0.1146, train acc: 0.9602, val loss: 0.2067, val acc: 0.9555  (best train acc: 0.9644, best val acc: 0.9599, best train loss: 0.1042  @ epoch 25655 )\n",
      "[Epoch: 26980] train loss: 0.1387, train acc: 0.9515, val loss: 0.2062, val acc: 0.9497  (best train acc: 0.9644, best val acc: 0.9599, best train loss: 0.1042  @ epoch 25655 )\n",
      "[Epoch: 27000] train loss: 0.1211, train acc: 0.9598, val loss: 0.2043, val acc: 0.9545  (best train acc: 0.9644, best val acc: 0.9599, best train loss: 0.1042  @ epoch 25655 )\n",
      "[Epoch: 27020] train loss: 0.1240, train acc: 0.9550, val loss: 0.2079, val acc: 0.9558  (best train acc: 0.9644, best val acc: 0.9599, best train loss: 0.1042  @ epoch 25655 )\n",
      "[Epoch: 27040] train loss: 0.1306, train acc: 0.9524, val loss: 0.2018, val acc: 0.9538  (best train acc: 0.9644, best val acc: 0.9599, best train loss: 0.1042  @ epoch 25655 )\n",
      "[Epoch: 27060] train loss: 0.1226, train acc: 0.9580, val loss: 0.2045, val acc: 0.9528  (best train acc: 0.9644, best val acc: 0.9599, best train loss: 0.1042  @ epoch 25655 )\n",
      "[Epoch: 27080] train loss: 0.1122, train acc: 0.9613, val loss: 0.2036, val acc: 0.9572  (best train acc: 0.9644, best val acc: 0.9599, best train loss: 0.1042  @ epoch 25655 )\n",
      "[Epoch: 27100] train loss: 0.1240, train acc: 0.9580, val loss: 0.2257, val acc: 0.9487  (best train acc: 0.9645, best val acc: 0.9599, best train loss: 0.1042  @ epoch 25655 )\n",
      "[Epoch: 27120] train loss: 0.1385, train acc: 0.9513, val loss: 0.2208, val acc: 0.9484  (best train acc: 0.9645, best val acc: 0.9599, best train loss: 0.1042  @ epoch 25655 )\n",
      "[Epoch: 27140] train loss: 0.1060, train acc: 0.9626, val loss: 0.2156, val acc: 0.9541  (best train acc: 0.9645, best val acc: 0.9599, best train loss: 0.1042  @ epoch 25655 )\n",
      "[Epoch: 27160] train loss: 0.1159, train acc: 0.9597, val loss: 0.2155, val acc: 0.9514  (best train acc: 0.9645, best val acc: 0.9599, best train loss: 0.1042  @ epoch 25655 )\n",
      "[Epoch: 27180] train loss: 0.1130, train acc: 0.9610, val loss: 0.2130, val acc: 0.9548  (best train acc: 0.9645, best val acc: 0.9599, best train loss: 0.1042  @ epoch 25655 )\n",
      "[Epoch: 27200] train loss: 0.1190, train acc: 0.9592, val loss: 0.2421, val acc: 0.9470  (best train acc: 0.9645, best val acc: 0.9599, best train loss: 0.1042  @ epoch 25655 )\n",
      "[Epoch: 27220] train loss: 0.1303, train acc: 0.9556, val loss: 0.1946, val acc: 0.9551  (best train acc: 0.9645, best val acc: 0.9599, best train loss: 0.1042  @ epoch 25655 )\n",
      "[Epoch: 27240] train loss: 0.1141, train acc: 0.9605, val loss: 0.2219, val acc: 0.9518  (best train acc: 0.9645, best val acc: 0.9599, best train loss: 0.1042  @ epoch 25655 )\n",
      "[Epoch: 27260] train loss: 0.1210, train acc: 0.9572, val loss: 0.2357, val acc: 0.9481  (best train acc: 0.9645, best val acc: 0.9599, best train loss: 0.1042  @ epoch 25655 )\n",
      "[Epoch: 27280] train loss: 0.1105, train acc: 0.9609, val loss: 0.2225, val acc: 0.9511  (best train acc: 0.9645, best val acc: 0.9599, best train loss: 0.1042  @ epoch 25655 )\n",
      "[Epoch: 27300] train loss: 0.1156, train acc: 0.9602, val loss: 0.2110, val acc: 0.9545  (best train acc: 0.9645, best val acc: 0.9599, best train loss: 0.1042  @ epoch 25655 )\n",
      "[Epoch: 27320] train loss: 0.1139, train acc: 0.9610, val loss: 0.2159, val acc: 0.9551  (best train acc: 0.9645, best val acc: 0.9599, best train loss: 0.1042  @ epoch 25655 )\n",
      "[Epoch: 27340] train loss: 0.1118, train acc: 0.9598, val loss: 0.2142, val acc: 0.9551  (best train acc: 0.9645, best val acc: 0.9599, best train loss: 0.1042  @ epoch 25655 )\n",
      "[Epoch: 27360] train loss: 0.1249, train acc: 0.9582, val loss: 0.2088, val acc: 0.9555  (best train acc: 0.9645, best val acc: 0.9599, best train loss: 0.1042  @ epoch 25655 )\n",
      "[Epoch: 27380] train loss: 0.1248, train acc: 0.9596, val loss: 0.2032, val acc: 0.9578  (best train acc: 0.9645, best val acc: 0.9599, best train loss: 0.1042  @ epoch 25655 )\n",
      "[Epoch: 27400] train loss: 0.1311, train acc: 0.9585, val loss: 0.2070, val acc: 0.9541  (best train acc: 0.9645, best val acc: 0.9599, best train loss: 0.1042  @ epoch 25655 )\n",
      "[Epoch: 27420] train loss: 0.1223, train acc: 0.9586, val loss: 0.2415, val acc: 0.9430  (best train acc: 0.9645, best val acc: 0.9599, best train loss: 0.1042  @ epoch 25655 )\n",
      "[Epoch: 27440] train loss: 0.1217, train acc: 0.9603, val loss: 0.2084, val acc: 0.9562  (best train acc: 0.9645, best val acc: 0.9599, best train loss: 0.1042  @ epoch 25655 )\n",
      "[Epoch: 27460] train loss: 0.1236, train acc: 0.9571, val loss: 0.2078, val acc: 0.9562  (best train acc: 0.9645, best val acc: 0.9599, best train loss: 0.1042  @ epoch 25655 )\n",
      "[Epoch: 27480] train loss: 0.1205, train acc: 0.9592, val loss: 0.2152, val acc: 0.9521  (best train acc: 0.9645, best val acc: 0.9599, best train loss: 0.1042  @ epoch 25655 )\n",
      "[Epoch: 27500] train loss: 0.1200, train acc: 0.9555, val loss: 0.2293, val acc: 0.9535  (best train acc: 0.9645, best val acc: 0.9599, best train loss: 0.1042  @ epoch 25655 )\n",
      "[Epoch: 27520] train loss: 0.1155, train acc: 0.9615, val loss: 0.2173, val acc: 0.9545  (best train acc: 0.9645, best val acc: 0.9599, best train loss: 0.1042  @ epoch 25655 )\n",
      "[Epoch: 27540] train loss: 0.1271, train acc: 0.9558, val loss: 0.2127, val acc: 0.9494  (best train acc: 0.9645, best val acc: 0.9599, best train loss: 0.1042  @ epoch 25655 )\n",
      "[Epoch: 27560] train loss: 0.1120, train acc: 0.9617, val loss: 0.2047, val acc: 0.9551  (best train acc: 0.9645, best val acc: 0.9599, best train loss: 0.1042  @ epoch 25655 )\n",
      "[Epoch: 27580] train loss: 0.1172, train acc: 0.9602, val loss: 0.2307, val acc: 0.9497  (best train acc: 0.9645, best val acc: 0.9599, best train loss: 0.1042  @ epoch 25655 )\n",
      "[Epoch: 27600] train loss: 0.1365, train acc: 0.9525, val loss: 0.2069, val acc: 0.9491  (best train acc: 0.9645, best val acc: 0.9599, best train loss: 0.1042  @ epoch 25655 )\n",
      "[Epoch: 27620] train loss: 0.1211, train acc: 0.9570, val loss: 0.2111, val acc: 0.9562  (best train acc: 0.9645, best val acc: 0.9599, best train loss: 0.1042  @ epoch 25655 )\n",
      "[Epoch: 27640] train loss: 0.1461, train acc: 0.9503, val loss: 0.2075, val acc: 0.9518  (best train acc: 0.9645, best val acc: 0.9599, best train loss: 0.1042  @ epoch 25655 )\n",
      "[Epoch: 27660] train loss: 0.1193, train acc: 0.9594, val loss: 0.1990, val acc: 0.9595  (best train acc: 0.9645, best val acc: 0.9599, best train loss: 0.1042  @ epoch 25655 )\n",
      "[Epoch: 27680] train loss: 0.1138, train acc: 0.9612, val loss: 0.2027, val acc: 0.9551  (best train acc: 0.9645, best val acc: 0.9599, best train loss: 0.1042  @ epoch 25655 )\n",
      "[Epoch: 27700] train loss: 0.1193, train acc: 0.9632, val loss: 0.2125, val acc: 0.9538  (best train acc: 0.9645, best val acc: 0.9599, best train loss: 0.1042  @ epoch 25655 )\n",
      "[Epoch: 27720] train loss: 0.1164, train acc: 0.9597, val loss: 0.2161, val acc: 0.9535  (best train acc: 0.9645, best val acc: 0.9599, best train loss: 0.1042  @ epoch 25655 )\n",
      "[Epoch: 27740] train loss: 0.1289, train acc: 0.9559, val loss: 0.2545, val acc: 0.9406  (best train acc: 0.9645, best val acc: 0.9599, best train loss: 0.1042  @ epoch 25655 )\n",
      "[Epoch: 27760] train loss: 0.1253, train acc: 0.9583, val loss: 0.2152, val acc: 0.9508  (best train acc: 0.9645, best val acc: 0.9599, best train loss: 0.1042  @ epoch 25655 )\n",
      "[Epoch: 27780] train loss: 0.1245, train acc: 0.9554, val loss: 0.2071, val acc: 0.9551  (best train acc: 0.9645, best val acc: 0.9599, best train loss: 0.1042  @ epoch 25655 )\n",
      "[Epoch: 27800] train loss: 0.1099, train acc: 0.9611, val loss: 0.2004, val acc: 0.9558  (best train acc: 0.9645, best val acc: 0.9599, best train loss: 0.1042  @ epoch 25655 )\n",
      "[Epoch: 27820] train loss: 0.1312, train acc: 0.9515, val loss: 0.2101, val acc: 0.9518  (best train acc: 0.9645, best val acc: 0.9599, best train loss: 0.1042  @ epoch 25655 )\n",
      "[Epoch: 27840] train loss: 0.1262, train acc: 0.9563, val loss: 0.2080, val acc: 0.9524  (best train acc: 0.9648, best val acc: 0.9599, best train loss: 0.1042  @ epoch 25655 )\n",
      "[Epoch: 27860] train loss: 0.1184, train acc: 0.9586, val loss: 0.2264, val acc: 0.9504  (best train acc: 0.9648, best val acc: 0.9599, best train loss: 0.1042  @ epoch 25655 )\n",
      "[Epoch: 27880] train loss: 0.1213, train acc: 0.9589, val loss: 0.2117, val acc: 0.9545  (best train acc: 0.9648, best val acc: 0.9599, best train loss: 0.1042  @ epoch 25655 )\n",
      "[Epoch: 27900] train loss: 0.1274, train acc: 0.9570, val loss: 0.1964, val acc: 0.9535  (best train acc: 0.9648, best val acc: 0.9599, best train loss: 0.1042  @ epoch 25655 )\n",
      "[Epoch: 27920] train loss: 0.1260, train acc: 0.9552, val loss: 0.2119, val acc: 0.9470  (best train acc: 0.9648, best val acc: 0.9599, best train loss: 0.1042  @ epoch 25655 )\n",
      "[Epoch: 27940] train loss: 0.1246, train acc: 0.9594, val loss: 0.2171, val acc: 0.9514  (best train acc: 0.9648, best val acc: 0.9599, best train loss: 0.1042  @ epoch 25655 )\n",
      "[Epoch: 27960] train loss: 0.1118, train acc: 0.9612, val loss: 0.2192, val acc: 0.9521  (best train acc: 0.9648, best val acc: 0.9599, best train loss: 0.1042  @ epoch 25655 )\n",
      "[Epoch: 27980] train loss: 0.1225, train acc: 0.9575, val loss: 0.2285, val acc: 0.9481  (best train acc: 0.9648, best val acc: 0.9599, best train loss: 0.1042  @ epoch 25655 )\n",
      "[Epoch: 28000] train loss: 0.1514, train acc: 0.9445, val loss: 0.2232, val acc: 0.9474  (best train acc: 0.9648, best val acc: 0.9599, best train loss: 0.1042  @ epoch 25655 )\n",
      "[Epoch: 28020] train loss: 0.1189, train acc: 0.9589, val loss: 0.2127, val acc: 0.9575  (best train acc: 0.9648, best val acc: 0.9599, best train loss: 0.1042  @ epoch 25655 )\n",
      "[Epoch: 28040] train loss: 0.1207, train acc: 0.9562, val loss: 0.2189, val acc: 0.9521  (best train acc: 0.9648, best val acc: 0.9599, best train loss: 0.1042  @ epoch 25655 )\n",
      "[Epoch: 28060] train loss: 0.1124, train acc: 0.9599, val loss: 0.2176, val acc: 0.9555  (best train acc: 0.9648, best val acc: 0.9599, best train loss: 0.1042  @ epoch 25655 )\n",
      "[Epoch: 28080] train loss: 0.1226, train acc: 0.9545, val loss: 0.2117, val acc: 0.9568  (best train acc: 0.9648, best val acc: 0.9599, best train loss: 0.1042  @ epoch 25655 )\n",
      "[Epoch: 28100] train loss: 0.1291, train acc: 0.9543, val loss: 0.2044, val acc: 0.9501  (best train acc: 0.9648, best val acc: 0.9599, best train loss: 0.1042  @ epoch 25655 )\n",
      "[Epoch: 28120] train loss: 0.1124, train acc: 0.9603, val loss: 0.2068, val acc: 0.9528  (best train acc: 0.9648, best val acc: 0.9599, best train loss: 0.1042  @ epoch 25655 )\n",
      "[Epoch: 28140] train loss: 0.1108, train acc: 0.9607, val loss: 0.2037, val acc: 0.9568  (best train acc: 0.9648, best val acc: 0.9599, best train loss: 0.1042  @ epoch 25655 )\n",
      "[Epoch: 28160] train loss: 0.1242, train acc: 0.9576, val loss: 0.2118, val acc: 0.9518  (best train acc: 0.9648, best val acc: 0.9599, best train loss: 0.1042  @ epoch 25655 )\n",
      "[Epoch: 28180] train loss: 0.1173, train acc: 0.9589, val loss: 0.1994, val acc: 0.9538  (best train acc: 0.9648, best val acc: 0.9599, best train loss: 0.1042  @ epoch 25655 )\n",
      "[Epoch: 28200] train loss: 0.1259, train acc: 0.9509, val loss: 0.2101, val acc: 0.9497  (best train acc: 0.9648, best val acc: 0.9599, best train loss: 0.1042  @ epoch 25655 )\n",
      "[Epoch: 28220] train loss: 0.1343, train acc: 0.9534, val loss: 0.2075, val acc: 0.9511  (best train acc: 0.9648, best val acc: 0.9599, best train loss: 0.1042  @ epoch 25655 )\n",
      "[Epoch: 28240] train loss: 0.1167, train acc: 0.9597, val loss: 0.2078, val acc: 0.9521  (best train acc: 0.9648, best val acc: 0.9599, best train loss: 0.1042  @ epoch 25655 )\n",
      "[Epoch: 28260] train loss: 0.1226, train acc: 0.9549, val loss: 0.2107, val acc: 0.9548  (best train acc: 0.9648, best val acc: 0.9599, best train loss: 0.1042  @ epoch 25655 )\n",
      "[Epoch: 28280] train loss: 0.1665, train acc: 0.9385, val loss: 0.2127, val acc: 0.9474  (best train acc: 0.9648, best val acc: 0.9599, best train loss: 0.1042  @ epoch 25655 )\n",
      "[Epoch: 28300] train loss: 0.1418, train acc: 0.9498, val loss: 0.1981, val acc: 0.9538  (best train acc: 0.9648, best val acc: 0.9599, best train loss: 0.1042  @ epoch 25655 )\n",
      "[Epoch: 28320] train loss: 0.1103, train acc: 0.9617, val loss: 0.2043, val acc: 0.9562  (best train acc: 0.9648, best val acc: 0.9599, best train loss: 0.1042  @ epoch 25655 )\n",
      "[Epoch: 28340] train loss: 0.1172, train acc: 0.9609, val loss: 0.2191, val acc: 0.9545  (best train acc: 0.9648, best val acc: 0.9599, best train loss: 0.1042  @ epoch 25655 )\n",
      "[Epoch: 28360] train loss: 0.1182, train acc: 0.9594, val loss: 0.2317, val acc: 0.9491  (best train acc: 0.9648, best val acc: 0.9599, best train loss: 0.1042  @ epoch 25655 )\n",
      "[Epoch: 28380] train loss: 0.1122, train acc: 0.9612, val loss: 0.2114, val acc: 0.9535  (best train acc: 0.9648, best val acc: 0.9599, best train loss: 0.1042  @ epoch 25655 )\n",
      "[Epoch: 28400] train loss: 0.1232, train acc: 0.9581, val loss: 0.2402, val acc: 0.9427  (best train acc: 0.9648, best val acc: 0.9599, best train loss: 0.1042  @ epoch 25655 )\n",
      "[Epoch: 28420] train loss: 0.1312, train acc: 0.9552, val loss: 0.2254, val acc: 0.9467  (best train acc: 0.9648, best val acc: 0.9599, best train loss: 0.1042  @ epoch 25655 )\n",
      "[Epoch: 28440] train loss: 0.1262, train acc: 0.9580, val loss: 0.2062, val acc: 0.9514  (best train acc: 0.9648, best val acc: 0.9599, best train loss: 0.1042  @ epoch 25655 )\n",
      "[Epoch: 28460] train loss: 0.1199, train acc: 0.9592, val loss: 0.2154, val acc: 0.9531  (best train acc: 0.9648, best val acc: 0.9616, best train loss: 0.1042  @ epoch 25655 )\n",
      "[Epoch: 28480] train loss: 0.1123, train acc: 0.9599, val loss: 0.2150, val acc: 0.9558  (best train acc: 0.9648, best val acc: 0.9616, best train loss: 0.1042  @ epoch 25655 )\n",
      "[Epoch: 28500] train loss: 0.1209, train acc: 0.9579, val loss: 0.2033, val acc: 0.9565  (best train acc: 0.9648, best val acc: 0.9616, best train loss: 0.1042  @ epoch 25655 )\n",
      "[Epoch: 28520] train loss: 0.1170, train acc: 0.9589, val loss: 0.2221, val acc: 0.9528  (best train acc: 0.9648, best val acc: 0.9616, best train loss: 0.1042  @ epoch 25655 )\n",
      "[Epoch: 28540] train loss: 0.1127, train acc: 0.9601, val loss: 0.2137, val acc: 0.9538  (best train acc: 0.9648, best val acc: 0.9616, best train loss: 0.1042  @ epoch 25655 )\n",
      "[Epoch: 28560] train loss: 0.1122, train acc: 0.9607, val loss: 0.2113, val acc: 0.9565  (best train acc: 0.9648, best val acc: 0.9616, best train loss: 0.1042  @ epoch 25655 )\n",
      "[Epoch: 28580] train loss: 0.1400, train acc: 0.9529, val loss: 0.2137, val acc: 0.9524  (best train acc: 0.9648, best val acc: 0.9616, best train loss: 0.1042  @ epoch 25655 )\n",
      "[Epoch: 28600] train loss: 0.1238, train acc: 0.9543, val loss: 0.2021, val acc: 0.9541  (best train acc: 0.9648, best val acc: 0.9616, best train loss: 0.1042  @ epoch 25655 )\n",
      "[Epoch: 28620] train loss: 0.1180, train acc: 0.9581, val loss: 0.2316, val acc: 0.9474  (best train acc: 0.9648, best val acc: 0.9616, best train loss: 0.1042  @ epoch 25655 )\n",
      "[Epoch: 28640] train loss: 0.1376, train acc: 0.9513, val loss: 0.2108, val acc: 0.9541  (best train acc: 0.9648, best val acc: 0.9616, best train loss: 0.1042  @ epoch 25655 )\n",
      "[Epoch: 28660] train loss: 0.1297, train acc: 0.9578, val loss: 0.2295, val acc: 0.9470  (best train acc: 0.9648, best val acc: 0.9616, best train loss: 0.1042  @ epoch 25655 )\n",
      "[Epoch: 28680] train loss: 0.1223, train acc: 0.9604, val loss: 0.2020, val acc: 0.9541  (best train acc: 0.9648, best val acc: 0.9616, best train loss: 0.1042  @ epoch 25655 )\n",
      "[Epoch: 28700] train loss: 0.1412, train acc: 0.9477, val loss: 0.2336, val acc: 0.9491  (best train acc: 0.9648, best val acc: 0.9616, best train loss: 0.1042  @ epoch 25655 )\n",
      "[Epoch: 28720] train loss: 0.1138, train acc: 0.9592, val loss: 0.2004, val acc: 0.9578  (best train acc: 0.9648, best val acc: 0.9616, best train loss: 0.1042  @ epoch 25655 )\n",
      "[Epoch: 28740] train loss: 0.1186, train acc: 0.9620, val loss: 0.2011, val acc: 0.9528  (best train acc: 0.9648, best val acc: 0.9616, best train loss: 0.1042  @ epoch 25655 )\n",
      "[Epoch: 28760] train loss: 0.1093, train acc: 0.9610, val loss: 0.2025, val acc: 0.9562  (best train acc: 0.9648, best val acc: 0.9616, best train loss: 0.1042  @ epoch 25655 )\n",
      "[Epoch: 28780] train loss: 0.1173, train acc: 0.9606, val loss: 0.2083, val acc: 0.9551  (best train acc: 0.9648, best val acc: 0.9616, best train loss: 0.1042  @ epoch 25655 )\n",
      "[Epoch: 28800] train loss: 0.1106, train acc: 0.9615, val loss: 0.2114, val acc: 0.9558  (best train acc: 0.9648, best val acc: 0.9616, best train loss: 0.1042  @ epoch 25655 )\n",
      "[Epoch: 28820] train loss: 0.1191, train acc: 0.9589, val loss: 0.2147, val acc: 0.9551  (best train acc: 0.9648, best val acc: 0.9616, best train loss: 0.1042  @ epoch 25655 )\n",
      "[Epoch: 28840] train loss: 0.1183, train acc: 0.9601, val loss: 0.2128, val acc: 0.9545  (best train acc: 0.9648, best val acc: 0.9616, best train loss: 0.1042  @ epoch 25655 )\n",
      "[Epoch: 28860] train loss: 0.1109, train acc: 0.9622, val loss: 0.2083, val acc: 0.9558  (best train acc: 0.9648, best val acc: 0.9616, best train loss: 0.1042  @ epoch 25655 )\n",
      "[Epoch: 28880] train loss: 0.1304, train acc: 0.9550, val loss: 0.1916, val acc: 0.9568  (best train acc: 0.9648, best val acc: 0.9616, best train loss: 0.1042  @ epoch 25655 )\n",
      "[Epoch: 28900] train loss: 0.1105, train acc: 0.9636, val loss: 0.1976, val acc: 0.9548  (best train acc: 0.9648, best val acc: 0.9616, best train loss: 0.1042  @ epoch 25655 )\n",
      "[Epoch: 28920] train loss: 0.1458, train acc: 0.9508, val loss: 0.2397, val acc: 0.9477  (best train acc: 0.9648, best val acc: 0.9616, best train loss: 0.1042  @ epoch 25655 )\n",
      "[Epoch: 28940] train loss: 0.1157, train acc: 0.9628, val loss: 0.1993, val acc: 0.9572  (best train acc: 0.9648, best val acc: 0.9616, best train loss: 0.1042  @ epoch 25655 )\n",
      "[Epoch: 28960] train loss: 0.1128, train acc: 0.9589, val loss: 0.2167, val acc: 0.9521  (best train acc: 0.9656, best val acc: 0.9616, best train loss: 0.1042  @ epoch 25655 )\n",
      "[Epoch: 28980] train loss: 0.1278, train acc: 0.9538, val loss: 0.2251, val acc: 0.9501  (best train acc: 0.9656, best val acc: 0.9616, best train loss: 0.1042  @ epoch 25655 )\n",
      "[Epoch: 29000] train loss: 0.1097, train acc: 0.9603, val loss: 0.2144, val acc: 0.9514  (best train acc: 0.9656, best val acc: 0.9616, best train loss: 0.1042  @ epoch 25655 )\n",
      "[Epoch: 29020] train loss: 0.1136, train acc: 0.9617, val loss: 0.2106, val acc: 0.9541  (best train acc: 0.9656, best val acc: 0.9616, best train loss: 0.1036  @ epoch 29008 )\n",
      "[Epoch: 29040] train loss: 0.1224, train acc: 0.9562, val loss: 0.2069, val acc: 0.9541  (best train acc: 0.9656, best val acc: 0.9616, best train loss: 0.1036  @ epoch 29008 )\n",
      "[Epoch: 29060] train loss: 0.1042, train acc: 0.9642, val loss: 0.2024, val acc: 0.9548  (best train acc: 0.9656, best val acc: 0.9616, best train loss: 0.1036  @ epoch 29008 )\n",
      "[Epoch: 29080] train loss: 0.1100, train acc: 0.9631, val loss: 0.2107, val acc: 0.9558  (best train acc: 0.9656, best val acc: 0.9616, best train loss: 0.1036  @ epoch 29008 )\n",
      "[Epoch: 29100] train loss: 0.1179, train acc: 0.9573, val loss: 0.2280, val acc: 0.9491  (best train acc: 0.9656, best val acc: 0.9616, best train loss: 0.1009  @ epoch 29088 )\n",
      "[Epoch: 29120] train loss: 0.1153, train acc: 0.9612, val loss: 0.2021, val acc: 0.9558  (best train acc: 0.9656, best val acc: 0.9616, best train loss: 0.1009  @ epoch 29088 )\n",
      "[Epoch: 29140] train loss: 0.1056, train acc: 0.9639, val loss: 0.2096, val acc: 0.9551  (best train acc: 0.9656, best val acc: 0.9616, best train loss: 0.1009  @ epoch 29088 )\n",
      "[Epoch: 29160] train loss: 0.1072, train acc: 0.9633, val loss: 0.2137, val acc: 0.9551  (best train acc: 0.9656, best val acc: 0.9616, best train loss: 0.1009  @ epoch 29088 )\n",
      "[Epoch: 29180] train loss: 0.1237, train acc: 0.9558, val loss: 0.2516, val acc: 0.9470  (best train acc: 0.9656, best val acc: 0.9616, best train loss: 0.1009  @ epoch 29088 )\n",
      "[Epoch: 29200] train loss: 0.1440, train acc: 0.9508, val loss: 0.2361, val acc: 0.9494  (best train acc: 0.9656, best val acc: 0.9616, best train loss: 0.1009  @ epoch 29088 )\n",
      "[Epoch: 29220] train loss: 0.1500, train acc: 0.9417, val loss: 0.2474, val acc: 0.9298  (best train acc: 0.9656, best val acc: 0.9616, best train loss: 0.1009  @ epoch 29088 )\n",
      "[Epoch: 29240] train loss: 0.1540, train acc: 0.9466, val loss: 0.1998, val acc: 0.9494  (best train acc: 0.9656, best val acc: 0.9616, best train loss: 0.1009  @ epoch 29088 )\n",
      "[Epoch: 29260] train loss: 0.1152, train acc: 0.9602, val loss: 0.2060, val acc: 0.9575  (best train acc: 0.9656, best val acc: 0.9616, best train loss: 0.1009  @ epoch 29088 )\n",
      "[Epoch: 29280] train loss: 0.1101, train acc: 0.9620, val loss: 0.2037, val acc: 0.9558  (best train acc: 0.9656, best val acc: 0.9616, best train loss: 0.1009  @ epoch 29088 )\n",
      "[Epoch: 29300] train loss: 0.1291, train acc: 0.9550, val loss: 0.2085, val acc: 0.9558  (best train acc: 0.9656, best val acc: 0.9616, best train loss: 0.1009  @ epoch 29088 )\n",
      "[Epoch: 29320] train loss: 0.1125, train acc: 0.9620, val loss: 0.2151, val acc: 0.9541  (best train acc: 0.9656, best val acc: 0.9616, best train loss: 0.1009  @ epoch 29088 )\n",
      "[Epoch: 29340] train loss: 0.1258, train acc: 0.9516, val loss: 0.2159, val acc: 0.9528  (best train acc: 0.9656, best val acc: 0.9616, best train loss: 0.1009  @ epoch 29088 )\n",
      "[Epoch: 29360] train loss: 0.1064, train acc: 0.9614, val loss: 0.2125, val acc: 0.9531  (best train acc: 0.9656, best val acc: 0.9616, best train loss: 0.1009  @ epoch 29088 )\n",
      "[Epoch: 29380] train loss: 0.1105, train acc: 0.9620, val loss: 0.2132, val acc: 0.9558  (best train acc: 0.9656, best val acc: 0.9616, best train loss: 0.1009  @ epoch 29088 )\n",
      "[Epoch: 29400] train loss: 0.1071, train acc: 0.9631, val loss: 0.2067, val acc: 0.9582  (best train acc: 0.9656, best val acc: 0.9616, best train loss: 0.1009  @ epoch 29088 )\n",
      "[Epoch: 29420] train loss: 0.1164, train acc: 0.9602, val loss: 0.2203, val acc: 0.9528  (best train acc: 0.9656, best val acc: 0.9616, best train loss: 0.1009  @ epoch 29088 )\n",
      "[Epoch: 29440] train loss: 0.1159, train acc: 0.9592, val loss: 0.2162, val acc: 0.9535  (best train acc: 0.9656, best val acc: 0.9616, best train loss: 0.1009  @ epoch 29088 )\n",
      "[Epoch: 29460] train loss: 0.1163, train acc: 0.9577, val loss: 0.2198, val acc: 0.9447  (best train acc: 0.9656, best val acc: 0.9616, best train loss: 0.1009  @ epoch 29088 )\n",
      "[Epoch: 29480] train loss: 0.1167, train acc: 0.9573, val loss: 0.2131, val acc: 0.9535  (best train acc: 0.9656, best val acc: 0.9616, best train loss: 0.1009  @ epoch 29088 )\n",
      "[Epoch: 29500] train loss: 0.1125, train acc: 0.9631, val loss: 0.2017, val acc: 0.9548  (best train acc: 0.9656, best val acc: 0.9616, best train loss: 0.1009  @ epoch 29088 )\n",
      "[Epoch: 29520] train loss: 0.1225, train acc: 0.9555, val loss: 0.2227, val acc: 0.9558  (best train acc: 0.9656, best val acc: 0.9616, best train loss: 0.1009  @ epoch 29088 )\n",
      "[Epoch: 29540] train loss: 0.1300, train acc: 0.9577, val loss: 0.2162, val acc: 0.9524  (best train acc: 0.9656, best val acc: 0.9616, best train loss: 0.1009  @ epoch 29088 )\n",
      "[Epoch: 29560] train loss: 0.1131, train acc: 0.9611, val loss: 0.2160, val acc: 0.9572  (best train acc: 0.9656, best val acc: 0.9616, best train loss: 0.1009  @ epoch 29088 )\n",
      "[Epoch: 29580] train loss: 0.1059, train acc: 0.9630, val loss: 0.2149, val acc: 0.9535  (best train acc: 0.9656, best val acc: 0.9616, best train loss: 0.1009  @ epoch 29088 )\n",
      "[Epoch: 29600] train loss: 0.1131, train acc: 0.9613, val loss: 0.2044, val acc: 0.9582  (best train acc: 0.9656, best val acc: 0.9616, best train loss: 0.1009  @ epoch 29088 )\n",
      "[Epoch: 29620] train loss: 0.1248, train acc: 0.9524, val loss: 0.2027, val acc: 0.9511  (best train acc: 0.9656, best val acc: 0.9616, best train loss: 0.1009  @ epoch 29088 )\n",
      "[Epoch: 29640] train loss: 0.1353, train acc: 0.9535, val loss: 0.2314, val acc: 0.9545  (best train acc: 0.9656, best val acc: 0.9616, best train loss: 0.1009  @ epoch 29088 )\n",
      "[Epoch: 29660] train loss: 0.1100, train acc: 0.9627, val loss: 0.2103, val acc: 0.9562  (best train acc: 0.9656, best val acc: 0.9616, best train loss: 0.1009  @ epoch 29088 )\n",
      "[Epoch: 29680] train loss: 0.1198, train acc: 0.9593, val loss: 0.2174, val acc: 0.9514  (best train acc: 0.9656, best val acc: 0.9616, best train loss: 0.1009  @ epoch 29088 )\n",
      "[Epoch: 29700] train loss: 0.1123, train acc: 0.9606, val loss: 0.2259, val acc: 0.9538  (best train acc: 0.9656, best val acc: 0.9616, best train loss: 0.1009  @ epoch 29088 )\n",
      "[Epoch: 29720] train loss: 0.1067, train acc: 0.9650, val loss: 0.1946, val acc: 0.9562  (best train acc: 0.9656, best val acc: 0.9616, best train loss: 0.1009  @ epoch 29088 )\n",
      "[Epoch: 29740] train loss: 0.1131, train acc: 0.9586, val loss: 0.2047, val acc: 0.9548  (best train acc: 0.9656, best val acc: 0.9616, best train loss: 0.1009  @ epoch 29088 )\n",
      "[Epoch: 29760] train loss: 0.1210, train acc: 0.9597, val loss: 0.2220, val acc: 0.9562  (best train acc: 0.9656, best val acc: 0.9616, best train loss: 0.1009  @ epoch 29088 )\n",
      "[Epoch: 29780] train loss: 0.1232, train acc: 0.9571, val loss: 0.2380, val acc: 0.9494  (best train acc: 0.9656, best val acc: 0.9616, best train loss: 0.1009  @ epoch 29088 )\n",
      "[Epoch: 29800] train loss: 0.1126, train acc: 0.9613, val loss: 0.2231, val acc: 0.9538  (best train acc: 0.9656, best val acc: 0.9616, best train loss: 0.1009  @ epoch 29088 )\n",
      "[Epoch: 29820] train loss: 0.1144, train acc: 0.9612, val loss: 0.2339, val acc: 0.9494  (best train acc: 0.9656, best val acc: 0.9616, best train loss: 0.1009  @ epoch 29088 )\n",
      "[Epoch: 29840] train loss: 0.1121, train acc: 0.9571, val loss: 0.2123, val acc: 0.9538  (best train acc: 0.9656, best val acc: 0.9616, best train loss: 0.1009  @ epoch 29088 )\n",
      "[Epoch: 29860] train loss: 0.1077, train acc: 0.9615, val loss: 0.2240, val acc: 0.9521  (best train acc: 0.9656, best val acc: 0.9616, best train loss: 0.1009  @ epoch 29088 )\n",
      "[Epoch: 29880] train loss: 0.1144, train acc: 0.9592, val loss: 0.2183, val acc: 0.9538  (best train acc: 0.9656, best val acc: 0.9616, best train loss: 0.1009  @ epoch 29088 )\n",
      "[Epoch: 29900] train loss: 0.1212, train acc: 0.9570, val loss: 0.2258, val acc: 0.9474  (best train acc: 0.9656, best val acc: 0.9616, best train loss: 0.1009  @ epoch 29088 )\n",
      "[Epoch: 29920] train loss: 0.1276, train acc: 0.9514, val loss: 0.2086, val acc: 0.9555  (best train acc: 0.9656, best val acc: 0.9616, best train loss: 0.1009  @ epoch 29088 )\n",
      "[Epoch: 29940] train loss: 0.1193, train acc: 0.9577, val loss: 0.2178, val acc: 0.9565  (best train acc: 0.9656, best val acc: 0.9616, best train loss: 0.1009  @ epoch 29088 )\n",
      "[Epoch: 29960] train loss: 0.1117, train acc: 0.9610, val loss: 0.2140, val acc: 0.9538  (best train acc: 0.9656, best val acc: 0.9616, best train loss: 0.1009  @ epoch 29088 )\n",
      "[Epoch: 29980] train loss: 0.1151, train acc: 0.9586, val loss: 0.2225, val acc: 0.9508  (best train acc: 0.9656, best val acc: 0.9616, best train loss: 0.1009  @ epoch 29088 )\n",
      "[Epoch: 30000] train loss: 0.1663, train acc: 0.9443, val loss: 0.2289, val acc: 0.9484  (best train acc: 0.9656, best val acc: 0.9616, best train loss: 0.1009  @ epoch 29088 )\n",
      "[Epoch: 30020] train loss: 0.1445, train acc: 0.9502, val loss: 0.2134, val acc: 0.9467  (best train acc: 0.9656, best val acc: 0.9616, best train loss: 0.1009  @ epoch 29088 )\n",
      "[Epoch: 30040] train loss: 0.1115, train acc: 0.9607, val loss: 0.2182, val acc: 0.9555  (best train acc: 0.9656, best val acc: 0.9616, best train loss: 0.1009  @ epoch 29088 )\n",
      "[Epoch: 30060] train loss: 0.1377, train acc: 0.9523, val loss: 0.2254, val acc: 0.9477  (best train acc: 0.9656, best val acc: 0.9616, best train loss: 0.1009  @ epoch 29088 )\n",
      "[Epoch: 30080] train loss: 0.1395, train acc: 0.9523, val loss: 0.2229, val acc: 0.9477  (best train acc: 0.9656, best val acc: 0.9616, best train loss: 0.1009  @ epoch 29088 )\n",
      "[Epoch: 30100] train loss: 0.1238, train acc: 0.9563, val loss: 0.2220, val acc: 0.9551  (best train acc: 0.9656, best val acc: 0.9616, best train loss: 0.1009  @ epoch 29088 )\n",
      "[Epoch: 30120] train loss: 0.1136, train acc: 0.9619, val loss: 0.2079, val acc: 0.9545  (best train acc: 0.9656, best val acc: 0.9616, best train loss: 0.1009  @ epoch 29088 )\n",
      "[Epoch: 30140] train loss: 0.1137, train acc: 0.9592, val loss: 0.2084, val acc: 0.9582  (best train acc: 0.9656, best val acc: 0.9616, best train loss: 0.1009  @ epoch 29088 )\n",
      "[Epoch: 30160] train loss: 0.1130, train acc: 0.9599, val loss: 0.2133, val acc: 0.9575  (best train acc: 0.9656, best val acc: 0.9616, best train loss: 0.1009  @ epoch 29088 )\n",
      "[Epoch: 30180] train loss: 0.1108, train acc: 0.9610, val loss: 0.2278, val acc: 0.9501  (best train acc: 0.9656, best val acc: 0.9616, best train loss: 0.1009  @ epoch 29088 )\n",
      "[Epoch: 30200] train loss: 0.1335, train acc: 0.9525, val loss: 0.2175, val acc: 0.9504  (best train acc: 0.9656, best val acc: 0.9616, best train loss: 0.1009  @ epoch 29088 )\n",
      "[Epoch: 30220] train loss: 0.1307, train acc: 0.9540, val loss: 0.2079, val acc: 0.9518  (best train acc: 0.9656, best val acc: 0.9616, best train loss: 0.1009  @ epoch 29088 )\n",
      "[Epoch: 30240] train loss: 0.1193, train acc: 0.9566, val loss: 0.2082, val acc: 0.9528  (best train acc: 0.9656, best val acc: 0.9616, best train loss: 0.1009  @ epoch 29088 )\n",
      "[Epoch: 30260] train loss: 0.1093, train acc: 0.9616, val loss: 0.2128, val acc: 0.9551  (best train acc: 0.9656, best val acc: 0.9616, best train loss: 0.1009  @ epoch 29088 )\n",
      "[Epoch: 30280] train loss: 0.1096, train acc: 0.9630, val loss: 0.2046, val acc: 0.9558  (best train acc: 0.9656, best val acc: 0.9616, best train loss: 0.1009  @ epoch 29088 )\n",
      "[Epoch: 30300] train loss: 0.1054, train acc: 0.9649, val loss: 0.2202, val acc: 0.9538  (best train acc: 0.9657, best val acc: 0.9616, best train loss: 0.1003  @ epoch 30296 )\n",
      "[Epoch: 30320] train loss: 0.1096, train acc: 0.9620, val loss: 0.2131, val acc: 0.9589  (best train acc: 0.9657, best val acc: 0.9616, best train loss: 0.1003  @ epoch 30296 )\n",
      "[Epoch: 30340] train loss: 0.1078, train acc: 0.9617, val loss: 0.2031, val acc: 0.9572  (best train acc: 0.9666, best val acc: 0.9616, best train loss: 0.1003  @ epoch 30296 )\n",
      "[Epoch: 30360] train loss: 0.1205, train acc: 0.9555, val loss: 0.2264, val acc: 0.9531  (best train acc: 0.9666, best val acc: 0.9616, best train loss: 0.1003  @ epoch 30296 )\n",
      "[Epoch: 30380] train loss: 0.1079, train acc: 0.9626, val loss: 0.2113, val acc: 0.9555  (best train acc: 0.9666, best val acc: 0.9616, best train loss: 0.1003  @ epoch 30296 )\n",
      "[Epoch: 30400] train loss: 0.1170, train acc: 0.9580, val loss: 0.2180, val acc: 0.9494  (best train acc: 0.9666, best val acc: 0.9616, best train loss: 0.1003  @ epoch 30296 )\n",
      "[Epoch: 30420] train loss: 0.1219, train acc: 0.9568, val loss: 0.2393, val acc: 0.9437  (best train acc: 0.9666, best val acc: 0.9616, best train loss: 0.1003  @ epoch 30296 )\n",
      "[Epoch: 30440] train loss: 0.1341, train acc: 0.9506, val loss: 0.2198, val acc: 0.9548  (best train acc: 0.9666, best val acc: 0.9616, best train loss: 0.1003  @ epoch 30296 )\n",
      "[Epoch: 30460] train loss: 0.1095, train acc: 0.9628, val loss: 0.2039, val acc: 0.9575  (best train acc: 0.9666, best val acc: 0.9616, best train loss: 0.1003  @ epoch 30296 )\n",
      "[Epoch: 30480] train loss: 0.1188, train acc: 0.9575, val loss: 0.2072, val acc: 0.9518  (best train acc: 0.9668, best val acc: 0.9616, best train loss: 0.1003  @ epoch 30296 )\n",
      "[Epoch: 30500] train loss: 0.1223, train acc: 0.9579, val loss: 0.2213, val acc: 0.9528  (best train acc: 0.9668, best val acc: 0.9616, best train loss: 0.1003  @ epoch 30296 )\n",
      "[Epoch: 30520] train loss: 0.1202, train acc: 0.9590, val loss: 0.2314, val acc: 0.9514  (best train acc: 0.9668, best val acc: 0.9616, best train loss: 0.1003  @ epoch 30296 )\n",
      "[Epoch: 30540] train loss: 0.1320, train acc: 0.9547, val loss: 0.2302, val acc: 0.9413  (best train acc: 0.9668, best val acc: 0.9616, best train loss: 0.1003  @ epoch 30296 )\n",
      "[Epoch: 30560] train loss: 0.1250, train acc: 0.9584, val loss: 0.2097, val acc: 0.9545  (best train acc: 0.9668, best val acc: 0.9616, best train loss: 0.1003  @ epoch 30296 )\n",
      "[Epoch: 30580] train loss: 0.1173, train acc: 0.9607, val loss: 0.2067, val acc: 0.9504  (best train acc: 0.9668, best val acc: 0.9616, best train loss: 0.1003  @ epoch 30296 )\n",
      "[Epoch: 30600] train loss: 0.1260, train acc: 0.9556, val loss: 0.2501, val acc: 0.9447  (best train acc: 0.9668, best val acc: 0.9616, best train loss: 0.1003  @ epoch 30296 )\n",
      "[Epoch: 30620] train loss: 0.1334, train acc: 0.9552, val loss: 0.2289, val acc: 0.9467  (best train acc: 0.9668, best val acc: 0.9616, best train loss: 0.1003  @ epoch 30296 )\n",
      "[Epoch: 30640] train loss: 0.1222, train acc: 0.9569, val loss: 0.2059, val acc: 0.9538  (best train acc: 0.9668, best val acc: 0.9616, best train loss: 0.1003  @ epoch 30296 )\n",
      "[Epoch: 30660] train loss: 0.1293, train acc: 0.9540, val loss: 0.2100, val acc: 0.9548  (best train acc: 0.9668, best val acc: 0.9616, best train loss: 0.1003  @ epoch 30296 )\n",
      "[Epoch: 30680] train loss: 0.1104, train acc: 0.9610, val loss: 0.2141, val acc: 0.9538  (best train acc: 0.9668, best val acc: 0.9616, best train loss: 0.1003  @ epoch 30296 )\n",
      "[Epoch: 30700] train loss: 0.1269, train acc: 0.9545, val loss: 0.2061, val acc: 0.9555  (best train acc: 0.9668, best val acc: 0.9616, best train loss: 0.1003  @ epoch 30296 )\n",
      "[Epoch: 30720] train loss: 0.1118, train acc: 0.9636, val loss: 0.2302, val acc: 0.9565  (best train acc: 0.9668, best val acc: 0.9616, best train loss: 0.1001  @ epoch 30713 )\n",
      "[Epoch: 30740] train loss: 0.1098, train acc: 0.9640, val loss: 0.2078, val acc: 0.9558  (best train acc: 0.9668, best val acc: 0.9616, best train loss: 0.1000  @ epoch 30726 )\n",
      "[Epoch: 30760] train loss: 0.1116, train acc: 0.9578, val loss: 0.2010, val acc: 0.9551  (best train acc: 0.9668, best val acc: 0.9616, best train loss: 0.1000  @ epoch 30726 )\n",
      "[Epoch: 30780] train loss: 0.1133, train acc: 0.9608, val loss: 0.2513, val acc: 0.9508  (best train acc: 0.9668, best val acc: 0.9616, best train loss: 0.1000  @ epoch 30726 )\n",
      "[Epoch: 30800] train loss: 0.1131, train acc: 0.9603, val loss: 0.2121, val acc: 0.9545  (best train acc: 0.9668, best val acc: 0.9616, best train loss: 0.1000  @ epoch 30726 )\n",
      "[Epoch: 30820] train loss: 0.1172, train acc: 0.9580, val loss: 0.2161, val acc: 0.9558  (best train acc: 0.9668, best val acc: 0.9616, best train loss: 0.1000  @ epoch 30726 )\n",
      "[Epoch: 30840] train loss: 0.1054, train acc: 0.9612, val loss: 0.2113, val acc: 0.9558  (best train acc: 0.9668, best val acc: 0.9616, best train loss: 0.1000  @ epoch 30726 )\n",
      "[Epoch: 30860] train loss: 0.1089, train acc: 0.9617, val loss: 0.2429, val acc: 0.9491  (best train acc: 0.9668, best val acc: 0.9616, best train loss: 0.1000  @ epoch 30726 )\n",
      "[Epoch: 30880] train loss: 0.1087, train acc: 0.9620, val loss: 0.2255, val acc: 0.9551  (best train acc: 0.9668, best val acc: 0.9616, best train loss: 0.1000  @ epoch 30726 )\n",
      "[Epoch: 30900] train loss: 0.1176, train acc: 0.9596, val loss: 0.2148, val acc: 0.9545  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0971  @ epoch 30892 )\n",
      "[Epoch: 30920] train loss: 0.1122, train acc: 0.9612, val loss: 0.2213, val acc: 0.9572  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0971  @ epoch 30892 )\n",
      "[Epoch: 30940] train loss: 0.1426, train acc: 0.9513, val loss: 0.2404, val acc: 0.9470  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0971  @ epoch 30892 )\n",
      "[Epoch: 30960] train loss: 0.1096, train acc: 0.9602, val loss: 0.2112, val acc: 0.9562  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0971  @ epoch 30892 )\n",
      "[Epoch: 30980] train loss: 0.1098, train acc: 0.9626, val loss: 0.2233, val acc: 0.9541  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0971  @ epoch 30892 )\n",
      "[Epoch: 31000] train loss: 0.1168, train acc: 0.9579, val loss: 0.2509, val acc: 0.9444  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0971  @ epoch 30892 )\n",
      "[Epoch: 31020] train loss: 0.1147, train acc: 0.9572, val loss: 0.2209, val acc: 0.9555  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0971  @ epoch 30892 )\n",
      "[Epoch: 31040] train loss: 0.1229, train acc: 0.9592, val loss: 0.2262, val acc: 0.9548  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0971  @ epoch 30892 )\n",
      "[Epoch: 31060] train loss: 0.1091, train acc: 0.9628, val loss: 0.2043, val acc: 0.9538  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0971  @ epoch 30892 )\n",
      "[Epoch: 31080] train loss: 0.1051, train acc: 0.9631, val loss: 0.2195, val acc: 0.9578  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0971  @ epoch 30892 )\n",
      "[Epoch: 31100] train loss: 0.1111, train acc: 0.9590, val loss: 0.2123, val acc: 0.9555  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0971  @ epoch 30892 )\n",
      "[Epoch: 31120] train loss: 0.1093, train acc: 0.9630, val loss: 0.2160, val acc: 0.9555  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0971  @ epoch 30892 )\n",
      "[Epoch: 31140] train loss: 0.1033, train acc: 0.9635, val loss: 0.2092, val acc: 0.9555  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0971  @ epoch 30892 )\n",
      "[Epoch: 31160] train loss: 0.1231, train acc: 0.9552, val loss: 0.1993, val acc: 0.9572  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0971  @ epoch 30892 )\n",
      "[Epoch: 31180] train loss: 0.1195, train acc: 0.9584, val loss: 0.2150, val acc: 0.9511  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0971  @ epoch 30892 )\n",
      "[Epoch: 31200] train loss: 0.1141, train acc: 0.9610, val loss: 0.2226, val acc: 0.9551  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0971  @ epoch 30892 )\n",
      "[Epoch: 31220] train loss: 0.1200, train acc: 0.9597, val loss: 0.2196, val acc: 0.9524  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0971  @ epoch 30892 )\n",
      "[Epoch: 31240] train loss: 0.1201, train acc: 0.9597, val loss: 0.2095, val acc: 0.9551  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0971  @ epoch 30892 )\n",
      "[Epoch: 31260] train loss: 0.1056, train acc: 0.9639, val loss: 0.2208, val acc: 0.9595  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0971  @ epoch 30892 )\n",
      "[Epoch: 31280] train loss: 0.1101, train acc: 0.9617, val loss: 0.2343, val acc: 0.9558  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0971  @ epoch 30892 )\n",
      "[Epoch: 31300] train loss: 0.1129, train acc: 0.9615, val loss: 0.2132, val acc: 0.9545  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0971  @ epoch 30892 )\n",
      "[Epoch: 31320] train loss: 0.1164, train acc: 0.9579, val loss: 0.2286, val acc: 0.9541  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0971  @ epoch 30892 )\n",
      "[Epoch: 31340] train loss: 0.1377, train acc: 0.9545, val loss: 0.2603, val acc: 0.9427  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0971  @ epoch 30892 )\n",
      "[Epoch: 31360] train loss: 0.1142, train acc: 0.9620, val loss: 0.1965, val acc: 0.9578  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0971  @ epoch 30892 )\n",
      "[Epoch: 31380] train loss: 0.1056, train acc: 0.9639, val loss: 0.2252, val acc: 0.9548  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0971  @ epoch 30892 )\n",
      "[Epoch: 31400] train loss: 0.1067, train acc: 0.9641, val loss: 0.2172, val acc: 0.9558  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0971  @ epoch 30892 )\n",
      "[Epoch: 31420] train loss: 0.1034, train acc: 0.9649, val loss: 0.2235, val acc: 0.9551  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0971  @ epoch 30892 )\n",
      "[Epoch: 31440] train loss: 0.1009, train acc: 0.9654, val loss: 0.2138, val acc: 0.9572  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0971  @ epoch 30892 )\n",
      "[Epoch: 31460] train loss: 0.1188, train acc: 0.9568, val loss: 0.2212, val acc: 0.9518  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0971  @ epoch 30892 )\n",
      "[Epoch: 31480] train loss: 0.1046, train acc: 0.9632, val loss: 0.2175, val acc: 0.9555  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0971  @ epoch 30892 )\n",
      "[Epoch: 31500] train loss: 0.1072, train acc: 0.9629, val loss: 0.2347, val acc: 0.9521  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0971  @ epoch 30892 )\n",
      "[Epoch: 31520] train loss: 0.0984, train acc: 0.9666, val loss: 0.2155, val acc: 0.9562  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0971  @ epoch 30892 )\n",
      "[Epoch: 31540] train loss: 0.1150, train acc: 0.9615, val loss: 0.2189, val acc: 0.9558  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0971  @ epoch 30892 )\n",
      "[Epoch: 31560] train loss: 0.1170, train acc: 0.9618, val loss: 0.2231, val acc: 0.9555  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0971  @ epoch 30892 )\n",
      "[Epoch: 31580] train loss: 0.1664, train acc: 0.9375, val loss: 0.2211, val acc: 0.9521  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0971  @ epoch 30892 )\n",
      "[Epoch: 31600] train loss: 0.1203, train acc: 0.9597, val loss: 0.2252, val acc: 0.9504  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0971  @ epoch 30892 )\n",
      "[Epoch: 31620] train loss: 0.1099, train acc: 0.9638, val loss: 0.2162, val acc: 0.9558  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0971  @ epoch 30892 )\n",
      "[Epoch: 31640] train loss: 0.1163, train acc: 0.9574, val loss: 0.2239, val acc: 0.9518  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0971  @ epoch 30892 )\n",
      "[Epoch: 31660] train loss: 0.1073, train acc: 0.9649, val loss: 0.2272, val acc: 0.9605  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0971  @ epoch 30892 )\n",
      "[Epoch: 31680] train loss: 0.1072, train acc: 0.9622, val loss: 0.2168, val acc: 0.9545  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0971  @ epoch 30892 )\n",
      "[Epoch: 31700] train loss: 0.1117, train acc: 0.9614, val loss: 0.2131, val acc: 0.9558  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0971  @ epoch 30892 )\n",
      "[Epoch: 31720] train loss: 0.1480, train acc: 0.9508, val loss: 0.2639, val acc: 0.9346  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0971  @ epoch 30892 )\n",
      "[Epoch: 31740] train loss: 0.1220, train acc: 0.9548, val loss: 0.2366, val acc: 0.9518  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0971  @ epoch 30892 )\n",
      "[Epoch: 31760] train loss: 0.1215, train acc: 0.9585, val loss: 0.2213, val acc: 0.9504  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0971  @ epoch 30892 )\n",
      "[Epoch: 31780] train loss: 0.1160, train acc: 0.9589, val loss: 0.2326, val acc: 0.9504  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0971  @ epoch 30892 )\n",
      "[Epoch: 31800] train loss: 0.1362, train acc: 0.9526, val loss: 0.2205, val acc: 0.9460  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0971  @ epoch 30892 )\n",
      "[Epoch: 31820] train loss: 0.1046, train acc: 0.9646, val loss: 0.2310, val acc: 0.9582  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0971  @ epoch 30892 )\n",
      "[Epoch: 31840] train loss: 0.1038, train acc: 0.9649, val loss: 0.2183, val acc: 0.9562  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0971  @ epoch 30892 )\n",
      "[Epoch: 31860] train loss: 0.1053, train acc: 0.9624, val loss: 0.2127, val acc: 0.9541  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0971  @ epoch 30892 )\n",
      "[Epoch: 31880] train loss: 0.1242, train acc: 0.9551, val loss: 0.2512, val acc: 0.9349  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0971  @ epoch 30892 )\n",
      "[Epoch: 31900] train loss: 0.1189, train acc: 0.9582, val loss: 0.2408, val acc: 0.9501  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0971  @ epoch 30892 )\n",
      "[Epoch: 31920] train loss: 0.1289, train acc: 0.9520, val loss: 0.2177, val acc: 0.9555  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0971  @ epoch 30892 )\n",
      "[Epoch: 31940] train loss: 0.1084, train acc: 0.9620, val loss: 0.2423, val acc: 0.9481  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0971  @ epoch 30892 )\n",
      "[Epoch: 31960] train loss: 0.1154, train acc: 0.9600, val loss: 0.2387, val acc: 0.9501  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0971  @ epoch 30892 )\n",
      "[Epoch: 31980] train loss: 0.1385, train acc: 0.9539, val loss: 0.2430, val acc: 0.9491  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0971  @ epoch 30892 )\n",
      "[Epoch: 32000] train loss: 0.1209, train acc: 0.9597, val loss: 0.2395, val acc: 0.9524  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0971  @ epoch 30892 )\n",
      "[Epoch: 32020] train loss: 0.1234, train acc: 0.9579, val loss: 0.2091, val acc: 0.9551  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0971  @ epoch 30892 )\n",
      "[Epoch: 32040] train loss: 0.1380, train acc: 0.9517, val loss: 0.2123, val acc: 0.9518  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0971  @ epoch 30892 )\n",
      "[Epoch: 32060] train loss: 0.1336, train acc: 0.9559, val loss: 0.2174, val acc: 0.9568  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0971  @ epoch 30892 )\n",
      "[Epoch: 32080] train loss: 0.1363, train acc: 0.9482, val loss: 0.2327, val acc: 0.9531  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0971  @ epoch 30892 )\n",
      "[Epoch: 32100] train loss: 0.1109, train acc: 0.9646, val loss: 0.2231, val acc: 0.9555  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0971  @ epoch 30892 )\n",
      "[Epoch: 32120] train loss: 0.1285, train acc: 0.9537, val loss: 0.2172, val acc: 0.9555  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0971  @ epoch 30892 )\n",
      "[Epoch: 32140] train loss: 0.1040, train acc: 0.9636, val loss: 0.2266, val acc: 0.9585  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0971  @ epoch 30892 )\n",
      "[Epoch: 32160] train loss: 0.1190, train acc: 0.9591, val loss: 0.2429, val acc: 0.9531  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0971  @ epoch 30892 )\n",
      "[Epoch: 32180] train loss: 0.1099, train acc: 0.9623, val loss: 0.2106, val acc: 0.9572  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0971  @ epoch 30892 )\n",
      "[Epoch: 32200] train loss: 0.1242, train acc: 0.9544, val loss: 0.2340, val acc: 0.9541  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0971  @ epoch 30892 )\n",
      "[Epoch: 32220] train loss: 0.1124, train acc: 0.9608, val loss: 0.2252, val acc: 0.9514  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0971  @ epoch 30892 )\n",
      "[Epoch: 32240] train loss: 0.1057, train acc: 0.9626, val loss: 0.2359, val acc: 0.9551  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0971  @ epoch 30892 )\n",
      "[Epoch: 32260] train loss: 0.1172, train acc: 0.9586, val loss: 0.2463, val acc: 0.9487  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0965  @ epoch 32248 )\n",
      "[Epoch: 32280] train loss: 0.1157, train acc: 0.9584, val loss: 0.2435, val acc: 0.9470  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0965  @ epoch 32248 )\n",
      "[Epoch: 32300] train loss: 0.1128, train acc: 0.9592, val loss: 0.2280, val acc: 0.9497  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0965  @ epoch 32248 )\n",
      "[Epoch: 32320] train loss: 0.1016, train acc: 0.9641, val loss: 0.2239, val acc: 0.9551  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0965  @ epoch 32248 )\n",
      "[Epoch: 32340] train loss: 0.1129, train acc: 0.9644, val loss: 0.2242, val acc: 0.9524  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0965  @ epoch 32248 )\n",
      "[Epoch: 32360] train loss: 0.1159, train acc: 0.9606, val loss: 0.2149, val acc: 0.9575  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0965  @ epoch 32248 )\n",
      "[Epoch: 32380] train loss: 0.1068, train acc: 0.9626, val loss: 0.2206, val acc: 0.9548  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0965  @ epoch 32248 )\n",
      "[Epoch: 32400] train loss: 0.1078, train acc: 0.9628, val loss: 0.2318, val acc: 0.9538  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0965  @ epoch 32248 )\n",
      "[Epoch: 32420] train loss: 0.1226, train acc: 0.9547, val loss: 0.2115, val acc: 0.9551  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0965  @ epoch 32248 )\n",
      "[Epoch: 32440] train loss: 0.1132, train acc: 0.9610, val loss: 0.2612, val acc: 0.9417  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0965  @ epoch 32248 )\n",
      "[Epoch: 32460] train loss: 0.1370, train acc: 0.9523, val loss: 0.2314, val acc: 0.9524  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0965  @ epoch 32248 )\n",
      "[Epoch: 32480] train loss: 0.1123, train acc: 0.9601, val loss: 0.2228, val acc: 0.9545  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0965  @ epoch 32248 )\n",
      "[Epoch: 32500] train loss: 0.1146, train acc: 0.9610, val loss: 0.2100, val acc: 0.9558  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0965  @ epoch 32248 )\n",
      "[Epoch: 32520] train loss: 0.1103, train acc: 0.9605, val loss: 0.2310, val acc: 0.9518  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0965  @ epoch 32248 )\n",
      "[Epoch: 32540] train loss: 0.1026, train acc: 0.9662, val loss: 0.2164, val acc: 0.9568  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0965  @ epoch 32248 )\n",
      "[Epoch: 32560] train loss: 0.0996, train acc: 0.9659, val loss: 0.2322, val acc: 0.9558  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0965  @ epoch 32248 )\n",
      "[Epoch: 32580] train loss: 0.1156, train acc: 0.9582, val loss: 0.2304, val acc: 0.9562  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0965  @ epoch 32248 )\n",
      "[Epoch: 32600] train loss: 0.1014, train acc: 0.9637, val loss: 0.2232, val acc: 0.9555  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0965  @ epoch 32248 )\n",
      "[Epoch: 32620] train loss: 0.1078, train acc: 0.9628, val loss: 0.2160, val acc: 0.9562  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0965  @ epoch 32248 )\n",
      "[Epoch: 32640] train loss: 0.1164, train acc: 0.9620, val loss: 0.2293, val acc: 0.9575  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0965  @ epoch 32248 )\n",
      "[Epoch: 32660] train loss: 0.1061, train acc: 0.9651, val loss: 0.2418, val acc: 0.9487  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0965  @ epoch 32248 )\n",
      "[Epoch: 32680] train loss: 0.1361, train acc: 0.9534, val loss: 0.2507, val acc: 0.9491  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0965  @ epoch 32248 )\n",
      "[Epoch: 32700] train loss: 0.1058, train acc: 0.9627, val loss: 0.2253, val acc: 0.9548  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0965  @ epoch 32248 )\n",
      "[Epoch: 32720] train loss: 0.1254, train acc: 0.9529, val loss: 0.2487, val acc: 0.9494  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0961  @ epoch 32708 )\n",
      "[Epoch: 32740] train loss: 0.1277, train acc: 0.9549, val loss: 0.2250, val acc: 0.9548  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0961  @ epoch 32708 )\n",
      "[Epoch: 32760] train loss: 0.1274, train acc: 0.9523, val loss: 0.2528, val acc: 0.9423  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0961  @ epoch 32708 )\n",
      "[Epoch: 32780] train loss: 0.1117, train acc: 0.9618, val loss: 0.2269, val acc: 0.9551  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0961  @ epoch 32708 )\n",
      "[Epoch: 32800] train loss: 0.1092, train acc: 0.9619, val loss: 0.2194, val acc: 0.9578  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0961  @ epoch 32708 )\n",
      "[Epoch: 32820] train loss: 0.1216, train acc: 0.9613, val loss: 0.2203, val acc: 0.9572  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0961  @ epoch 32708 )\n",
      "[Epoch: 32840] train loss: 0.1044, train acc: 0.9648, val loss: 0.2129, val acc: 0.9558  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0961  @ epoch 32708 )\n",
      "[Epoch: 32860] train loss: 0.1117, train acc: 0.9619, val loss: 0.2384, val acc: 0.9562  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0961  @ epoch 32708 )\n",
      "[Epoch: 32880] train loss: 0.1227, train acc: 0.9568, val loss: 0.2440, val acc: 0.9521  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0961  @ epoch 32708 )\n",
      "[Epoch: 32900] train loss: 0.1091, train acc: 0.9626, val loss: 0.2281, val acc: 0.9568  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0961  @ epoch 32708 )\n",
      "[Epoch: 32920] train loss: 0.1047, train acc: 0.9649, val loss: 0.2438, val acc: 0.9504  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0961  @ epoch 32708 )\n",
      "[Epoch: 32940] train loss: 0.1121, train acc: 0.9584, val loss: 0.2275, val acc: 0.9538  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0961  @ epoch 32708 )\n",
      "[Epoch: 32960] train loss: 0.1150, train acc: 0.9617, val loss: 0.2540, val acc: 0.9484  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0954  @ epoch 32950 )\n",
      "[Epoch: 32980] train loss: 0.1068, train acc: 0.9646, val loss: 0.2270, val acc: 0.9541  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0954  @ epoch 32950 )\n",
      "[Epoch: 33000] train loss: 0.1108, train acc: 0.9618, val loss: 0.2293, val acc: 0.9528  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0954  @ epoch 32950 )\n",
      "[Epoch: 33020] train loss: 0.1207, train acc: 0.9558, val loss: 0.2225, val acc: 0.9548  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0954  @ epoch 32950 )\n",
      "[Epoch: 33040] train loss: 0.1071, train acc: 0.9639, val loss: 0.2376, val acc: 0.9521  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0954  @ epoch 32950 )\n",
      "[Epoch: 33060] train loss: 0.1238, train acc: 0.9563, val loss: 0.2512, val acc: 0.9474  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0954  @ epoch 32950 )\n",
      "[Epoch: 33080] train loss: 0.1051, train acc: 0.9646, val loss: 0.2316, val acc: 0.9538  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0954  @ epoch 32950 )\n",
      "[Epoch: 33100] train loss: 0.1148, train acc: 0.9610, val loss: 0.2308, val acc: 0.9551  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0954  @ epoch 32950 )\n",
      "[Epoch: 33120] train loss: 0.1366, train acc: 0.9511, val loss: 0.2498, val acc: 0.9440  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0954  @ epoch 32950 )\n",
      "[Epoch: 33140] train loss: 0.1051, train acc: 0.9631, val loss: 0.2394, val acc: 0.9551  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0954  @ epoch 32950 )\n",
      "[Epoch: 33160] train loss: 0.1121, train acc: 0.9600, val loss: 0.2335, val acc: 0.9531  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0954  @ epoch 32950 )\n",
      "[Epoch: 33180] train loss: 0.1122, train acc: 0.9569, val loss: 0.2236, val acc: 0.9578  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0954  @ epoch 32950 )\n",
      "[Epoch: 33200] train loss: 0.1041, train acc: 0.9638, val loss: 0.2270, val acc: 0.9565  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0954  @ epoch 32950 )\n",
      "[Epoch: 33220] train loss: 0.1230, train acc: 0.9563, val loss: 0.2359, val acc: 0.9518  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0954  @ epoch 32950 )\n",
      "[Epoch: 33240] train loss: 0.1250, train acc: 0.9559, val loss: 0.2225, val acc: 0.9555  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0954  @ epoch 32950 )\n",
      "[Epoch: 33260] train loss: 0.1193, train acc: 0.9573, val loss: 0.2254, val acc: 0.9551  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0954  @ epoch 32950 )\n",
      "[Epoch: 33280] train loss: 0.1153, train acc: 0.9614, val loss: 0.2438, val acc: 0.9565  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0954  @ epoch 32950 )\n",
      "[Epoch: 33300] train loss: 0.1151, train acc: 0.9610, val loss: 0.2270, val acc: 0.9555  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0954  @ epoch 32950 )\n",
      "[Epoch: 33320] train loss: 0.1181, train acc: 0.9602, val loss: 0.2317, val acc: 0.9541  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0954  @ epoch 32950 )\n",
      "[Epoch: 33340] train loss: 0.1109, train acc: 0.9639, val loss: 0.2204, val acc: 0.9572  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0954  @ epoch 32950 )\n",
      "[Epoch: 33360] train loss: 0.1202, train acc: 0.9557, val loss: 0.2258, val acc: 0.9521  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0954  @ epoch 32950 )\n",
      "[Epoch: 33380] train loss: 0.1067, train acc: 0.9641, val loss: 0.2180, val acc: 0.9531  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0954  @ epoch 32950 )\n",
      "[Epoch: 33400] train loss: 0.1320, train acc: 0.9499, val loss: 0.2403, val acc: 0.9531  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0954  @ epoch 32950 )\n",
      "[Epoch: 33420] train loss: 0.1149, train acc: 0.9602, val loss: 0.2408, val acc: 0.9545  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0954  @ epoch 32950 )\n",
      "[Epoch: 33440] train loss: 0.1197, train acc: 0.9607, val loss: 0.2366, val acc: 0.9555  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0954  @ epoch 32950 )\n",
      "[Epoch: 33460] train loss: 0.1023, train acc: 0.9646, val loss: 0.2160, val acc: 0.9575  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0954  @ epoch 32950 )\n",
      "[Epoch: 33480] train loss: 0.1066, train acc: 0.9633, val loss: 0.2179, val acc: 0.9572  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0954  @ epoch 32950 )\n",
      "[Epoch: 33500] train loss: 0.1147, train acc: 0.9581, val loss: 0.2223, val acc: 0.9531  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0954  @ epoch 32950 )\n",
      "[Epoch: 33520] train loss: 0.1146, train acc: 0.9632, val loss: 0.2323, val acc: 0.9521  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0954  @ epoch 32950 )\n",
      "[Epoch: 33540] train loss: 0.1103, train acc: 0.9621, val loss: 0.2236, val acc: 0.9548  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0954  @ epoch 32950 )\n",
      "[Epoch: 33560] train loss: 0.1185, train acc: 0.9576, val loss: 0.2338, val acc: 0.9565  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0954  @ epoch 32950 )\n",
      "[Epoch: 33580] train loss: 0.1079, train acc: 0.9605, val loss: 0.2415, val acc: 0.9484  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0954  @ epoch 32950 )\n",
      "[Epoch: 33600] train loss: 0.1000, train acc: 0.9659, val loss: 0.2282, val acc: 0.9562  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0954  @ epoch 32950 )\n",
      "[Epoch: 33620] train loss: 0.1248, train acc: 0.9537, val loss: 0.2471, val acc: 0.9541  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0954  @ epoch 32950 )\n",
      "[Epoch: 33640] train loss: 0.1178, train acc: 0.9599, val loss: 0.2262, val acc: 0.9538  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0954  @ epoch 32950 )\n",
      "[Epoch: 33660] train loss: 0.1206, train acc: 0.9573, val loss: 0.2195, val acc: 0.9548  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0954  @ epoch 32950 )\n",
      "[Epoch: 33680] train loss: 0.1194, train acc: 0.9592, val loss: 0.2382, val acc: 0.9497  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0954  @ epoch 32950 )\n",
      "[Epoch: 33700] train loss: 0.1050, train acc: 0.9653, val loss: 0.2414, val acc: 0.9551  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0954  @ epoch 32950 )\n",
      "[Epoch: 33720] train loss: 0.1148, train acc: 0.9618, val loss: 0.2297, val acc: 0.9531  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0954  @ epoch 32950 )\n",
      "[Epoch: 33740] train loss: 0.1178, train acc: 0.9581, val loss: 0.2255, val acc: 0.9508  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0954  @ epoch 32950 )\n",
      "[Epoch: 33760] train loss: 0.1317, train acc: 0.9504, val loss: 0.2313, val acc: 0.9548  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0954  @ epoch 32950 )\n",
      "[Epoch: 33780] train loss: 0.1073, train acc: 0.9630, val loss: 0.2239, val acc: 0.9562  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0954  @ epoch 32950 )\n",
      "[Epoch: 33800] train loss: 0.1087, train acc: 0.9609, val loss: 0.2316, val acc: 0.9548  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0954  @ epoch 32950 )\n",
      "[Epoch: 33820] train loss: 0.1082, train acc: 0.9631, val loss: 0.2312, val acc: 0.9528  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0954  @ epoch 32950 )\n",
      "[Epoch: 33840] train loss: 0.1130, train acc: 0.9625, val loss: 0.2297, val acc: 0.9548  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0954  @ epoch 32950 )\n",
      "[Epoch: 33860] train loss: 0.1785, train acc: 0.9388, val loss: 0.2279, val acc: 0.9535  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0954  @ epoch 32950 )\n",
      "[Epoch: 33880] train loss: 0.1297, train acc: 0.9542, val loss: 0.2186, val acc: 0.9484  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0954  @ epoch 32950 )\n",
      "[Epoch: 33900] train loss: 0.1151, train acc: 0.9620, val loss: 0.2342, val acc: 0.9562  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0954  @ epoch 32950 )\n",
      "[Epoch: 33920] train loss: 0.1051, train acc: 0.9648, val loss: 0.2245, val acc: 0.9555  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0954  @ epoch 32950 )\n",
      "[Epoch: 33940] train loss: 0.1070, train acc: 0.9631, val loss: 0.2395, val acc: 0.9501  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0954  @ epoch 32950 )\n",
      "[Epoch: 33960] train loss: 0.1110, train acc: 0.9599, val loss: 0.2321, val acc: 0.9521  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0954  @ epoch 32950 )\n",
      "[Epoch: 33980] train loss: 0.1021, train acc: 0.9654, val loss: 0.2282, val acc: 0.9555  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0954  @ epoch 32950 )\n",
      "[Epoch: 34000] train loss: 0.1000, train acc: 0.9652, val loss: 0.2195, val acc: 0.9558  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0954  @ epoch 32950 )\n",
      "[Epoch: 34020] train loss: 0.1084, train acc: 0.9612, val loss: 0.2406, val acc: 0.9511  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0954  @ epoch 32950 )\n",
      "[Epoch: 34040] train loss: 0.1117, train acc: 0.9610, val loss: 0.2358, val acc: 0.9514  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0954  @ epoch 32950 )\n",
      "[Epoch: 34060] train loss: 0.1097, train acc: 0.9616, val loss: 0.1993, val acc: 0.9562  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0954  @ epoch 32950 )\n",
      "[Epoch: 34080] train loss: 0.1196, train acc: 0.9605, val loss: 0.2162, val acc: 0.9555  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0954  @ epoch 32950 )\n",
      "[Epoch: 34100] train loss: 0.1027, train acc: 0.9639, val loss: 0.2194, val acc: 0.9575  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0954  @ epoch 32950 )\n",
      "[Epoch: 34120] train loss: 0.1034, train acc: 0.9653, val loss: 0.2249, val acc: 0.9568  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0945  @ epoch 34104 )\n",
      "[Epoch: 34140] train loss: 0.1048, train acc: 0.9625, val loss: 0.2351, val acc: 0.9545  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0945  @ epoch 34104 )\n",
      "[Epoch: 34160] train loss: 0.1176, train acc: 0.9583, val loss: 0.2274, val acc: 0.9575  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0945  @ epoch 34104 )\n",
      "[Epoch: 34180] train loss: 0.1064, train acc: 0.9631, val loss: 0.2268, val acc: 0.9562  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0945  @ epoch 34104 )\n",
      "[Epoch: 34200] train loss: 0.1192, train acc: 0.9588, val loss: 0.2233, val acc: 0.9535  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0945  @ epoch 34104 )\n",
      "[Epoch: 34220] train loss: 0.1062, train acc: 0.9633, val loss: 0.2185, val acc: 0.9568  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0945  @ epoch 34104 )\n",
      "[Epoch: 34240] train loss: 0.1203, train acc: 0.9570, val loss: 0.2159, val acc: 0.9548  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0945  @ epoch 34104 )\n",
      "[Epoch: 34260] train loss: 0.1370, train acc: 0.9539, val loss: 0.2089, val acc: 0.9545  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0945  @ epoch 34104 )\n",
      "[Epoch: 34280] train loss: 0.1068, train acc: 0.9610, val loss: 0.2319, val acc: 0.9545  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0945  @ epoch 34104 )\n",
      "[Epoch: 34300] train loss: 0.1065, train acc: 0.9628, val loss: 0.2249, val acc: 0.9558  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0945  @ epoch 34104 )\n",
      "[Epoch: 34320] train loss: 0.1092, train acc: 0.9610, val loss: 0.2224, val acc: 0.9572  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0945  @ epoch 34104 )\n",
      "[Epoch: 34340] train loss: 0.1025, train acc: 0.9646, val loss: 0.2151, val acc: 0.9575  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0945  @ epoch 34104 )\n",
      "[Epoch: 34360] train loss: 0.1058, train acc: 0.9631, val loss: 0.2179, val acc: 0.9568  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0945  @ epoch 34104 )\n",
      "[Epoch: 34380] train loss: 0.1239, train acc: 0.9550, val loss: 0.2422, val acc: 0.9494  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0945  @ epoch 34104 )\n",
      "[Epoch: 34400] train loss: 0.1174, train acc: 0.9597, val loss: 0.2484, val acc: 0.9528  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0945  @ epoch 34104 )\n",
      "[Epoch: 34420] train loss: 0.1119, train acc: 0.9586, val loss: 0.2405, val acc: 0.9582  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0945  @ epoch 34104 )\n",
      "[Epoch: 34440] train loss: 0.1132, train acc: 0.9621, val loss: 0.2102, val acc: 0.9551  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0945  @ epoch 34104 )\n",
      "[Epoch: 34460] train loss: 0.1282, train acc: 0.9543, val loss: 0.2487, val acc: 0.9460  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0945  @ epoch 34104 )\n",
      "[Epoch: 34480] train loss: 0.0989, train acc: 0.9659, val loss: 0.2271, val acc: 0.9558  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0945  @ epoch 34104 )\n",
      "[Epoch: 34500] train loss: 0.1007, train acc: 0.9657, val loss: 0.2336, val acc: 0.9568  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0945  @ epoch 34104 )\n",
      "[Epoch: 34520] train loss: 0.1080, train acc: 0.9624, val loss: 0.2579, val acc: 0.9504  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0945  @ epoch 34104 )\n",
      "[Epoch: 34540] train loss: 0.0966, train acc: 0.9659, val loss: 0.2239, val acc: 0.9582  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0945  @ epoch 34104 )\n",
      "[Epoch: 34560] train loss: 0.1117, train acc: 0.9629, val loss: 0.2315, val acc: 0.9531  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0945  @ epoch 34104 )\n",
      "[Epoch: 34580] train loss: 0.1078, train acc: 0.9627, val loss: 0.2322, val acc: 0.9558  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0945  @ epoch 34104 )\n",
      "[Epoch: 34600] train loss: 0.1234, train acc: 0.9505, val loss: 0.2343, val acc: 0.9497  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0945  @ epoch 34104 )\n",
      "[Epoch: 34620] train loss: 0.1236, train acc: 0.9567, val loss: 0.2405, val acc: 0.9524  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0945  @ epoch 34104 )\n",
      "[Epoch: 34640] train loss: 0.1294, train acc: 0.9560, val loss: 0.2386, val acc: 0.9487  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0945  @ epoch 34104 )\n",
      "[Epoch: 34660] train loss: 0.1050, train acc: 0.9637, val loss: 0.2275, val acc: 0.9565  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0945  @ epoch 34104 )\n",
      "[Epoch: 34680] train loss: 0.1032, train acc: 0.9651, val loss: 0.2311, val acc: 0.9555  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0945  @ epoch 34104 )\n",
      "[Epoch: 34700] train loss: 0.1176, train acc: 0.9574, val loss: 0.2292, val acc: 0.9558  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0945  @ epoch 34104 )\n",
      "[Epoch: 34720] train loss: 0.1025, train acc: 0.9649, val loss: 0.2296, val acc: 0.9565  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0945  @ epoch 34104 )\n",
      "[Epoch: 34740] train loss: 0.1010, train acc: 0.9643, val loss: 0.2189, val acc: 0.9555  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0945  @ epoch 34104 )\n",
      "[Epoch: 34760] train loss: 0.1290, train acc: 0.9522, val loss: 0.2354, val acc: 0.9491  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0945  @ epoch 34104 )\n",
      "[Epoch: 34780] train loss: 0.1054, train acc: 0.9648, val loss: 0.2212, val acc: 0.9551  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0945  @ epoch 34104 )\n",
      "[Epoch: 34800] train loss: 0.1140, train acc: 0.9615, val loss: 0.2315, val acc: 0.9521  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0945  @ epoch 34104 )\n",
      "[Epoch: 34820] train loss: 0.1117, train acc: 0.9590, val loss: 0.2325, val acc: 0.9572  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0945  @ epoch 34104 )\n",
      "[Epoch: 34840] train loss: 0.1105, train acc: 0.9617, val loss: 0.2199, val acc: 0.9572  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0945  @ epoch 34104 )\n",
      "[Epoch: 34860] train loss: 0.1119, train acc: 0.9618, val loss: 0.2362, val acc: 0.9494  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0945  @ epoch 34104 )\n",
      "[Epoch: 34880] train loss: 0.1130, train acc: 0.9583, val loss: 0.2172, val acc: 0.9555  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0945  @ epoch 34104 )\n",
      "[Epoch: 34900] train loss: 0.1009, train acc: 0.9646, val loss: 0.2262, val acc: 0.9568  (best train acc: 0.9686, best val acc: 0.9616, best train loss: 0.0945  @ epoch 34104 )\n",
      "[Epoch: 34920] train loss: 0.1145, train acc: 0.9600, val loss: 0.2162, val acc: 0.9578  (best train acc: 0.9691, best val acc: 0.9616, best train loss: 0.0931  @ epoch 34904 )\n",
      "[Epoch: 34940] train loss: 0.0992, train acc: 0.9649, val loss: 0.2162, val acc: 0.9572  (best train acc: 0.9697, best val acc: 0.9616, best train loss: 0.0907  @ epoch 34924 )\n",
      "[Epoch: 34960] train loss: 0.1423, train acc: 0.9537, val loss: 0.2386, val acc: 0.9511  (best train acc: 0.9697, best val acc: 0.9616, best train loss: 0.0907  @ epoch 34924 )\n",
      "[Epoch: 34980] train loss: 0.1178, train acc: 0.9589, val loss: 0.2649, val acc: 0.9470  (best train acc: 0.9697, best val acc: 0.9616, best train loss: 0.0907  @ epoch 34924 )\n",
      "[Epoch: 35000] train loss: 0.1232, train acc: 0.9514, val loss: 0.2443, val acc: 0.9450  (best train acc: 0.9697, best val acc: 0.9616, best train loss: 0.0907  @ epoch 34924 )\n",
      "[Epoch: 35020] train loss: 0.1075, train acc: 0.9611, val loss: 0.2355, val acc: 0.9578  (best train acc: 0.9697, best val acc: 0.9616, best train loss: 0.0907  @ epoch 34924 )\n",
      "[Epoch: 35040] train loss: 0.1061, train acc: 0.9639, val loss: 0.2382, val acc: 0.9568  (best train acc: 0.9697, best val acc: 0.9616, best train loss: 0.0907  @ epoch 34924 )\n",
      "[Epoch: 35060] train loss: 0.0996, train acc: 0.9642, val loss: 0.2211, val acc: 0.9551  (best train acc: 0.9697, best val acc: 0.9616, best train loss: 0.0907  @ epoch 34924 )\n",
      "[Epoch: 35080] train loss: 0.1136, train acc: 0.9600, val loss: 0.2281, val acc: 0.9562  (best train acc: 0.9697, best val acc: 0.9616, best train loss: 0.0907  @ epoch 34924 )\n",
      "[Epoch: 35100] train loss: 0.0971, train acc: 0.9644, val loss: 0.2318, val acc: 0.9568  (best train acc: 0.9697, best val acc: 0.9616, best train loss: 0.0907  @ epoch 34924 )\n",
      "[Epoch: 35120] train loss: 0.1069, train acc: 0.9623, val loss: 0.2315, val acc: 0.9568  (best train acc: 0.9697, best val acc: 0.9616, best train loss: 0.0907  @ epoch 34924 )\n",
      "[Epoch: 35140] train loss: 0.1160, train acc: 0.9566, val loss: 0.2271, val acc: 0.9538  (best train acc: 0.9697, best val acc: 0.9616, best train loss: 0.0907  @ epoch 34924 )\n",
      "[Epoch: 35160] train loss: 0.1031, train acc: 0.9628, val loss: 0.2289, val acc: 0.9575  (best train acc: 0.9697, best val acc: 0.9616, best train loss: 0.0907  @ epoch 34924 )\n",
      "[Epoch: 35180] train loss: 0.0967, train acc: 0.9636, val loss: 0.2313, val acc: 0.9568  (best train acc: 0.9697, best val acc: 0.9616, best train loss: 0.0907  @ epoch 34924 )\n",
      "[Epoch: 35200] train loss: 0.1337, train acc: 0.9478, val loss: 0.2486, val acc: 0.9481  (best train acc: 0.9697, best val acc: 0.9616, best train loss: 0.0907  @ epoch 34924 )\n",
      "[Epoch: 35220] train loss: 0.1100, train acc: 0.9623, val loss: 0.2299, val acc: 0.9589  (best train acc: 0.9697, best val acc: 0.9616, best train loss: 0.0907  @ epoch 34924 )\n",
      "[Epoch: 35240] train loss: 0.1243, train acc: 0.9581, val loss: 0.2466, val acc: 0.9511  (best train acc: 0.9697, best val acc: 0.9616, best train loss: 0.0907  @ epoch 34924 )\n",
      "[Epoch: 35260] train loss: 0.1129, train acc: 0.9613, val loss: 0.2429, val acc: 0.9504  (best train acc: 0.9697, best val acc: 0.9616, best train loss: 0.0907  @ epoch 34924 )\n",
      "[Epoch: 35280] train loss: 0.0999, train acc: 0.9662, val loss: 0.2270, val acc: 0.9582  (best train acc: 0.9697, best val acc: 0.9616, best train loss: 0.0907  @ epoch 34924 )\n",
      "[Epoch: 35300] train loss: 0.1041, train acc: 0.9631, val loss: 0.2021, val acc: 0.9575  (best train acc: 0.9697, best val acc: 0.9616, best train loss: 0.0907  @ epoch 34924 )\n",
      "[Epoch: 35320] train loss: 0.1507, train acc: 0.9482, val loss: 0.2357, val acc: 0.9491  (best train acc: 0.9697, best val acc: 0.9616, best train loss: 0.0907  @ epoch 34924 )\n",
      "[Epoch: 35340] train loss: 0.1149, train acc: 0.9600, val loss: 0.2214, val acc: 0.9558  (best train acc: 0.9697, best val acc: 0.9616, best train loss: 0.0907  @ epoch 34924 )\n",
      "[Epoch: 35360] train loss: 0.1117, train acc: 0.9598, val loss: 0.2271, val acc: 0.9531  (best train acc: 0.9697, best val acc: 0.9616, best train loss: 0.0907  @ epoch 34924 )\n",
      "[Epoch: 35380] train loss: 0.1227, train acc: 0.9508, val loss: 0.2392, val acc: 0.9420  (best train acc: 0.9697, best val acc: 0.9616, best train loss: 0.0907  @ epoch 34924 )\n",
      "[Epoch: 35400] train loss: 0.1034, train acc: 0.9615, val loss: 0.2279, val acc: 0.9582  (best train acc: 0.9697, best val acc: 0.9616, best train loss: 0.0907  @ epoch 34924 )\n",
      "[Epoch: 35420] train loss: 0.1018, train acc: 0.9652, val loss: 0.2296, val acc: 0.9518  (best train acc: 0.9697, best val acc: 0.9616, best train loss: 0.0907  @ epoch 34924 )\n",
      "[Epoch: 35440] train loss: 0.1102, train acc: 0.9623, val loss: 0.2148, val acc: 0.9551  (best train acc: 0.9697, best val acc: 0.9616, best train loss: 0.0907  @ epoch 34924 )\n",
      "[Epoch: 35460] train loss: 0.0975, train acc: 0.9657, val loss: 0.2178, val acc: 0.9582  (best train acc: 0.9697, best val acc: 0.9616, best train loss: 0.0907  @ epoch 34924 )\n",
      "[Epoch: 35480] train loss: 0.1058, train acc: 0.9620, val loss: 0.2141, val acc: 0.9548  (best train acc: 0.9697, best val acc: 0.9616, best train loss: 0.0907  @ epoch 34924 )\n",
      "[Epoch: 35500] train loss: 0.1023, train acc: 0.9633, val loss: 0.2306, val acc: 0.9551  (best train acc: 0.9697, best val acc: 0.9616, best train loss: 0.0907  @ epoch 34924 )\n",
      "[Epoch: 35520] train loss: 0.1116, train acc: 0.9619, val loss: 0.2155, val acc: 0.9487  (best train acc: 0.9697, best val acc: 0.9616, best train loss: 0.0907  @ epoch 34924 )\n",
      "[Epoch: 35540] train loss: 0.1036, train acc: 0.9629, val loss: 0.2265, val acc: 0.9521  (best train acc: 0.9697, best val acc: 0.9616, best train loss: 0.0907  @ epoch 34924 )\n",
      "[Epoch: 35560] train loss: 0.1087, train acc: 0.9594, val loss: 0.2227, val acc: 0.9568  (best train acc: 0.9697, best val acc: 0.9616, best train loss: 0.0907  @ epoch 34924 )\n",
      "[Epoch: 35580] train loss: 0.1041, train acc: 0.9646, val loss: 0.2234, val acc: 0.9565  (best train acc: 0.9697, best val acc: 0.9619, best train loss: 0.0907  @ epoch 34924 )\n",
      "[Epoch: 35600] train loss: 0.1504, train acc: 0.9483, val loss: 0.2559, val acc: 0.9497  (best train acc: 0.9697, best val acc: 0.9619, best train loss: 0.0907  @ epoch 34924 )\n",
      "[Epoch: 35620] train loss: 0.1127, train acc: 0.9602, val loss: 0.2054, val acc: 0.9562  (best train acc: 0.9697, best val acc: 0.9619, best train loss: 0.0907  @ epoch 34924 )\n",
      "[Epoch: 35640] train loss: 0.0978, train acc: 0.9652, val loss: 0.2240, val acc: 0.9565  (best train acc: 0.9697, best val acc: 0.9619, best train loss: 0.0907  @ epoch 34924 )\n",
      "[Epoch: 35660] train loss: 0.1583, train acc: 0.9472, val loss: 0.2339, val acc: 0.9454  (best train acc: 0.9697, best val acc: 0.9619, best train loss: 0.0907  @ epoch 34924 )\n",
      "[Epoch: 35680] train loss: 0.1367, train acc: 0.9519, val loss: 0.2498, val acc: 0.9406  (best train acc: 0.9697, best val acc: 0.9619, best train loss: 0.0907  @ epoch 34924 )\n",
      "[Epoch: 35700] train loss: 0.1160, train acc: 0.9609, val loss: 0.2285, val acc: 0.9545  (best train acc: 0.9697, best val acc: 0.9619, best train loss: 0.0907  @ epoch 34924 )\n",
      "[Epoch: 35720] train loss: 0.1125, train acc: 0.9627, val loss: 0.2189, val acc: 0.9572  (best train acc: 0.9697, best val acc: 0.9619, best train loss: 0.0907  @ epoch 34924 )\n",
      "[Epoch: 35740] train loss: 0.1057, train acc: 0.9644, val loss: 0.2313, val acc: 0.9558  (best train acc: 0.9697, best val acc: 0.9619, best train loss: 0.0907  @ epoch 34924 )\n",
      "[Epoch: 35760] train loss: 0.1026, train acc: 0.9638, val loss: 0.2259, val acc: 0.9572  (best train acc: 0.9697, best val acc: 0.9619, best train loss: 0.0907  @ epoch 34924 )\n",
      "[Epoch: 35780] train loss: 0.1078, train acc: 0.9625, val loss: 0.2393, val acc: 0.9501  (best train acc: 0.9697, best val acc: 0.9619, best train loss: 0.0907  @ epoch 34924 )\n",
      "[Epoch: 35800] train loss: 0.0967, train acc: 0.9674, val loss: 0.2221, val acc: 0.9589  (best train acc: 0.9697, best val acc: 0.9619, best train loss: 0.0907  @ epoch 34924 )\n",
      "[Epoch: 35820] train loss: 0.1164, train acc: 0.9586, val loss: 0.2249, val acc: 0.9551  (best train acc: 0.9697, best val acc: 0.9619, best train loss: 0.0907  @ epoch 34924 )\n",
      "[Epoch: 35840] train loss: 0.1137, train acc: 0.9617, val loss: 0.2376, val acc: 0.9538  (best train acc: 0.9697, best val acc: 0.9619, best train loss: 0.0907  @ epoch 34924 )\n",
      "[Epoch: 35860] train loss: 0.1328, train acc: 0.9529, val loss: 0.2199, val acc: 0.9511  (best train acc: 0.9697, best val acc: 0.9619, best train loss: 0.0907  @ epoch 34924 )\n",
      "[Epoch: 35880] train loss: 0.1024, train acc: 0.9649, val loss: 0.2259, val acc: 0.9562  (best train acc: 0.9697, best val acc: 0.9619, best train loss: 0.0907  @ epoch 34924 )\n",
      "[Epoch: 35900] train loss: 0.1016, train acc: 0.9641, val loss: 0.2364, val acc: 0.9551  (best train acc: 0.9697, best val acc: 0.9619, best train loss: 0.0907  @ epoch 34924 )\n",
      "[Epoch: 35920] train loss: 0.1041, train acc: 0.9636, val loss: 0.2643, val acc: 0.9477  (best train acc: 0.9697, best val acc: 0.9619, best train loss: 0.0907  @ epoch 34924 )\n",
      "[Epoch: 35940] train loss: 0.1113, train acc: 0.9619, val loss: 0.2143, val acc: 0.9531  (best train acc: 0.9697, best val acc: 0.9619, best train loss: 0.0907  @ epoch 34924 )\n",
      "[Epoch: 35960] train loss: 0.1242, train acc: 0.9536, val loss: 0.2293, val acc: 0.9504  (best train acc: 0.9697, best val acc: 0.9619, best train loss: 0.0907  @ epoch 34924 )\n",
      "[Epoch: 35980] train loss: 0.1137, train acc: 0.9612, val loss: 0.2406, val acc: 0.9535  (best train acc: 0.9697, best val acc: 0.9619, best train loss: 0.0907  @ epoch 34924 )\n",
      "[Epoch: 36000] train loss: 0.0981, train acc: 0.9662, val loss: 0.2242, val acc: 0.9565  (best train acc: 0.9697, best val acc: 0.9619, best train loss: 0.0907  @ epoch 34924 )\n",
      "[Epoch: 36020] train loss: 0.1099, train acc: 0.9615, val loss: 0.2355, val acc: 0.9531  (best train acc: 0.9697, best val acc: 0.9619, best train loss: 0.0907  @ epoch 34924 )\n",
      "[Epoch: 36040] train loss: 0.0987, train acc: 0.9645, val loss: 0.2152, val acc: 0.9555  (best train acc: 0.9697, best val acc: 0.9619, best train loss: 0.0907  @ epoch 34924 )\n",
      "[Epoch: 36060] train loss: 0.0985, train acc: 0.9645, val loss: 0.2311, val acc: 0.9541  (best train acc: 0.9697, best val acc: 0.9619, best train loss: 0.0907  @ epoch 34924 )\n",
      "[Epoch: 36080] train loss: 0.1250, train acc: 0.9529, val loss: 0.2287, val acc: 0.9535  (best train acc: 0.9697, best val acc: 0.9619, best train loss: 0.0907  @ epoch 34924 )\n",
      "[Epoch: 36100] train loss: 0.1059, train acc: 0.9639, val loss: 0.2211, val acc: 0.9551  (best train acc: 0.9697, best val acc: 0.9619, best train loss: 0.0907  @ epoch 34924 )\n",
      "[Epoch: 36120] train loss: 0.1153, train acc: 0.9591, val loss: 0.2281, val acc: 0.9487  (best train acc: 0.9697, best val acc: 0.9619, best train loss: 0.0907  @ epoch 34924 )\n",
      "[Epoch: 36140] train loss: 0.1107, train acc: 0.9592, val loss: 0.2313, val acc: 0.9555  (best train acc: 0.9697, best val acc: 0.9619, best train loss: 0.0907  @ epoch 34924 )\n",
      "[Epoch: 36160] train loss: 0.1017, train acc: 0.9653, val loss: 0.2381, val acc: 0.9585  (best train acc: 0.9697, best val acc: 0.9619, best train loss: 0.0907  @ epoch 34924 )\n",
      "[Epoch: 36180] train loss: 0.1101, train acc: 0.9622, val loss: 0.2167, val acc: 0.9572  (best train acc: 0.9697, best val acc: 0.9619, best train loss: 0.0907  @ epoch 34924 )\n",
      "[Epoch: 36200] train loss: 0.0983, train acc: 0.9651, val loss: 0.2499, val acc: 0.9535  (best train acc: 0.9697, best val acc: 0.9619, best train loss: 0.0907  @ epoch 34924 )\n",
      "[Epoch: 36220] train loss: 0.1323, train acc: 0.9445, val loss: 0.2516, val acc: 0.9487  (best train acc: 0.9697, best val acc: 0.9619, best train loss: 0.0907  @ epoch 34924 )\n",
      "[Epoch: 36240] train loss: 0.1278, train acc: 0.9565, val loss: 0.2631, val acc: 0.9369  (best train acc: 0.9697, best val acc: 0.9619, best train loss: 0.0907  @ epoch 34924 )\n",
      "[Epoch: 36260] train loss: 0.1181, train acc: 0.9560, val loss: 0.2202, val acc: 0.9565  (best train acc: 0.9697, best val acc: 0.9619, best train loss: 0.0907  @ epoch 34924 )\n",
      "[Epoch: 36280] train loss: 0.1083, train acc: 0.9603, val loss: 0.2271, val acc: 0.9585  (best train acc: 0.9697, best val acc: 0.9619, best train loss: 0.0907  @ epoch 34924 )\n",
      "[Epoch: 36300] train loss: 0.1047, train acc: 0.9645, val loss: 0.2364, val acc: 0.9528  (best train acc: 0.9697, best val acc: 0.9619, best train loss: 0.0907  @ epoch 34924 )\n",
      "[Epoch: 36320] train loss: 0.1001, train acc: 0.9659, val loss: 0.2266, val acc: 0.9548  (best train acc: 0.9697, best val acc: 0.9619, best train loss: 0.0907  @ epoch 34924 )\n",
      "[Epoch: 36340] train loss: 0.1086, train acc: 0.9636, val loss: 0.2284, val acc: 0.9541  (best train acc: 0.9697, best val acc: 0.9619, best train loss: 0.0907  @ epoch 34924 )\n",
      "[Epoch: 36360] train loss: 0.1145, train acc: 0.9623, val loss: 0.2416, val acc: 0.9531  (best train acc: 0.9697, best val acc: 0.9619, best train loss: 0.0907  @ epoch 34924 )\n",
      "[Epoch: 36380] train loss: 0.1159, train acc: 0.9558, val loss: 0.2379, val acc: 0.9467  (best train acc: 0.9697, best val acc: 0.9619, best train loss: 0.0907  @ epoch 34924 )\n",
      "[Epoch: 36400] train loss: 0.1251, train acc: 0.9542, val loss: 0.2477, val acc: 0.9555  (best train acc: 0.9697, best val acc: 0.9619, best train loss: 0.0907  @ epoch 34924 )\n",
      "[Epoch: 36420] train loss: 0.1090, train acc: 0.9592, val loss: 0.2199, val acc: 0.9572  (best train acc: 0.9697, best val acc: 0.9619, best train loss: 0.0907  @ epoch 34924 )\n",
      "[Epoch: 36440] train loss: 0.1013, train acc: 0.9659, val loss: 0.2314, val acc: 0.9551  (best train acc: 0.9697, best val acc: 0.9619, best train loss: 0.0907  @ epoch 34924 )\n",
      "[Epoch: 36460] train loss: 0.1059, train acc: 0.9636, val loss: 0.2204, val acc: 0.9524  (best train acc: 0.9697, best val acc: 0.9619, best train loss: 0.0907  @ epoch 34924 )\n",
      "[Epoch: 36480] train loss: 0.1017, train acc: 0.9654, val loss: 0.2196, val acc: 0.9595  (best train acc: 0.9697, best val acc: 0.9619, best train loss: 0.0907  @ epoch 34924 )\n",
      "[Epoch: 36500] train loss: 0.1234, train acc: 0.9550, val loss: 0.2527, val acc: 0.9460  (best train acc: 0.9697, best val acc: 0.9619, best train loss: 0.0907  @ epoch 34924 )\n",
      "[Epoch: 36520] train loss: 0.1151, train acc: 0.9604, val loss: 0.2648, val acc: 0.9437  (best train acc: 0.9697, best val acc: 0.9619, best train loss: 0.0907  @ epoch 34924 )\n",
      "[Epoch: 36540] train loss: 0.1137, train acc: 0.9627, val loss: 0.2434, val acc: 0.9501  (best train acc: 0.9697, best val acc: 0.9619, best train loss: 0.0907  @ epoch 34924 )\n",
      "[Epoch: 36560] train loss: 0.0973, train acc: 0.9654, val loss: 0.2302, val acc: 0.9582  (best train acc: 0.9697, best val acc: 0.9619, best train loss: 0.0907  @ epoch 34924 )\n",
      "[Epoch: 36580] train loss: 0.1001, train acc: 0.9653, val loss: 0.2312, val acc: 0.9605  (best train acc: 0.9697, best val acc: 0.9619, best train loss: 0.0907  @ epoch 34924 )\n",
      "[Epoch: 36600] train loss: 0.0979, train acc: 0.9657, val loss: 0.2238, val acc: 0.9589  (best train acc: 0.9699, best val acc: 0.9619, best train loss: 0.0899  @ epoch 36594 )\n",
      "[Epoch: 36620] train loss: 0.1057, train acc: 0.9640, val loss: 0.2411, val acc: 0.9504  (best train acc: 0.9699, best val acc: 0.9619, best train loss: 0.0895  @ epoch 36607 )\n",
      "[Epoch: 36640] train loss: 0.1123, train acc: 0.9606, val loss: 0.2281, val acc: 0.9555  (best train acc: 0.9699, best val acc: 0.9619, best train loss: 0.0895  @ epoch 36607 )\n",
      "[Epoch: 36660] train loss: 0.1206, train acc: 0.9601, val loss: 0.2631, val acc: 0.9454  (best train acc: 0.9699, best val acc: 0.9619, best train loss: 0.0895  @ epoch 36607 )\n",
      "[Epoch: 36680] train loss: 0.1102, train acc: 0.9622, val loss: 0.2403, val acc: 0.9565  (best train acc: 0.9699, best val acc: 0.9619, best train loss: 0.0895  @ epoch 36607 )\n",
      "[Epoch: 36700] train loss: 0.1152, train acc: 0.9603, val loss: 0.2353, val acc: 0.9521  (best train acc: 0.9699, best val acc: 0.9619, best train loss: 0.0895  @ epoch 36607 )\n",
      "[Epoch: 36720] train loss: 0.0966, train acc: 0.9659, val loss: 0.2397, val acc: 0.9555  (best train acc: 0.9699, best val acc: 0.9619, best train loss: 0.0895  @ epoch 36607 )\n",
      "[Epoch: 36740] train loss: 0.1325, train acc: 0.9486, val loss: 0.2619, val acc: 0.9484  (best train acc: 0.9699, best val acc: 0.9619, best train loss: 0.0895  @ epoch 36607 )\n",
      "[Epoch: 36760] train loss: 0.1070, train acc: 0.9609, val loss: 0.2357, val acc: 0.9558  (best train acc: 0.9699, best val acc: 0.9619, best train loss: 0.0895  @ epoch 36607 )\n",
      "[Epoch: 36780] train loss: 0.1041, train acc: 0.9661, val loss: 0.2308, val acc: 0.9551  (best train acc: 0.9699, best val acc: 0.9619, best train loss: 0.0895  @ epoch 36607 )\n",
      "[Epoch: 36800] train loss: 0.1152, train acc: 0.9594, val loss: 0.2441, val acc: 0.9524  (best train acc: 0.9699, best val acc: 0.9619, best train loss: 0.0895  @ epoch 36607 )\n",
      "[Epoch: 36820] train loss: 0.1085, train acc: 0.9654, val loss: 0.2282, val acc: 0.9578  (best train acc: 0.9699, best val acc: 0.9619, best train loss: 0.0895  @ epoch 36607 )\n",
      "[Epoch: 36840] train loss: 0.1058, train acc: 0.9629, val loss: 0.2556, val acc: 0.9514  (best train acc: 0.9699, best val acc: 0.9619, best train loss: 0.0895  @ epoch 36607 )\n",
      "[Epoch: 36860] train loss: 0.1016, train acc: 0.9652, val loss: 0.2380, val acc: 0.9538  (best train acc: 0.9699, best val acc: 0.9619, best train loss: 0.0895  @ epoch 36607 )\n",
      "[Epoch: 36880] train loss: 0.0995, train acc: 0.9645, val loss: 0.2255, val acc: 0.9538  (best train acc: 0.9699, best val acc: 0.9619, best train loss: 0.0895  @ epoch 36607 )\n",
      "[Epoch: 36900] train loss: 0.0931, train acc: 0.9673, val loss: 0.2300, val acc: 0.9548  (best train acc: 0.9699, best val acc: 0.9619, best train loss: 0.0895  @ epoch 36607 )\n",
      "[Epoch: 36920] train loss: 0.1130, train acc: 0.9616, val loss: 0.2325, val acc: 0.9572  (best train acc: 0.9699, best val acc: 0.9619, best train loss: 0.0895  @ epoch 36607 )\n",
      "[Epoch: 36940] train loss: 0.1124, train acc: 0.9563, val loss: 0.2189, val acc: 0.9555  (best train acc: 0.9699, best val acc: 0.9619, best train loss: 0.0895  @ epoch 36607 )\n",
      "[Epoch: 36960] train loss: 0.1040, train acc: 0.9645, val loss: 0.2276, val acc: 0.9565  (best train acc: 0.9699, best val acc: 0.9619, best train loss: 0.0895  @ epoch 36607 )\n",
      "[Epoch: 36980] train loss: 0.1135, train acc: 0.9599, val loss: 0.2648, val acc: 0.9450  (best train acc: 0.9699, best val acc: 0.9619, best train loss: 0.0895  @ epoch 36607 )\n",
      "[Epoch: 37000] train loss: 0.1726, train acc: 0.9315, val loss: 0.2471, val acc: 0.9447  (best train acc: 0.9699, best val acc: 0.9619, best train loss: 0.0895  @ epoch 36607 )\n",
      "[Epoch: 37020] train loss: 0.1244, train acc: 0.9584, val loss: 0.2256, val acc: 0.9578  (best train acc: 0.9699, best val acc: 0.9619, best train loss: 0.0895  @ epoch 36607 )\n",
      "[Epoch: 37040] train loss: 0.1039, train acc: 0.9644, val loss: 0.2347, val acc: 0.9562  (best train acc: 0.9699, best val acc: 0.9619, best train loss: 0.0895  @ epoch 36607 )\n",
      "[Epoch: 37060] train loss: 0.0963, train acc: 0.9676, val loss: 0.2167, val acc: 0.9568  (best train acc: 0.9699, best val acc: 0.9619, best train loss: 0.0895  @ epoch 36607 )\n",
      "[Epoch: 37080] train loss: 0.1024, train acc: 0.9657, val loss: 0.2244, val acc: 0.9578  (best train acc: 0.9699, best val acc: 0.9619, best train loss: 0.0895  @ epoch 36607 )\n",
      "[Epoch: 37100] train loss: 0.1244, train acc: 0.9563, val loss: 0.2235, val acc: 0.9477  (best train acc: 0.9699, best val acc: 0.9619, best train loss: 0.0895  @ epoch 36607 )\n",
      "[Epoch: 37120] train loss: 0.1171, train acc: 0.9584, val loss: 0.2238, val acc: 0.9417  (best train acc: 0.9699, best val acc: 0.9619, best train loss: 0.0895  @ epoch 36607 )\n",
      "[Epoch: 37140] train loss: 0.1147, train acc: 0.9567, val loss: 0.2204, val acc: 0.9508  (best train acc: 0.9699, best val acc: 0.9619, best train loss: 0.0895  @ epoch 36607 )\n",
      "[Epoch: 37160] train loss: 0.0961, train acc: 0.9661, val loss: 0.2257, val acc: 0.9531  (best train acc: 0.9699, best val acc: 0.9619, best train loss: 0.0895  @ epoch 36607 )\n",
      "[Epoch: 37180] train loss: 0.1049, train acc: 0.9664, val loss: 0.2369, val acc: 0.9555  (best train acc: 0.9699, best val acc: 0.9619, best train loss: 0.0895  @ epoch 36607 )\n",
      "[Epoch: 37200] train loss: 0.1459, train acc: 0.9485, val loss: 0.2192, val acc: 0.9548  (best train acc: 0.9699, best val acc: 0.9619, best train loss: 0.0895  @ epoch 36607 )\n",
      "[Epoch: 37220] train loss: 0.1050, train acc: 0.9632, val loss: 0.2230, val acc: 0.9575  (best train acc: 0.9699, best val acc: 0.9619, best train loss: 0.0895  @ epoch 36607 )\n",
      "[Epoch: 37240] train loss: 0.1075, train acc: 0.9631, val loss: 0.2367, val acc: 0.9551  (best train acc: 0.9699, best val acc: 0.9619, best train loss: 0.0895  @ epoch 36607 )\n",
      "[Epoch: 37260] train loss: 0.1117, train acc: 0.9620, val loss: 0.2291, val acc: 0.9602  (best train acc: 0.9699, best val acc: 0.9619, best train loss: 0.0895  @ epoch 36607 )\n",
      "[Epoch: 37280] train loss: 0.1052, train acc: 0.9633, val loss: 0.2361, val acc: 0.9562  (best train acc: 0.9699, best val acc: 0.9619, best train loss: 0.0895  @ epoch 36607 )\n",
      "[Epoch: 37300] train loss: 0.0910, train acc: 0.9682, val loss: 0.2291, val acc: 0.9582  (best train acc: 0.9699, best val acc: 0.9619, best train loss: 0.0895  @ epoch 36607 )\n",
      "[Epoch: 37320] train loss: 0.1090, train acc: 0.9619, val loss: 0.2325, val acc: 0.9565  (best train acc: 0.9699, best val acc: 0.9619, best train loss: 0.0895  @ epoch 36607 )\n",
      "[Epoch: 37340] train loss: 0.1040, train acc: 0.9650, val loss: 0.2486, val acc: 0.9511  (best train acc: 0.9699, best val acc: 0.9619, best train loss: 0.0895  @ epoch 36607 )\n",
      "[Epoch: 37360] train loss: 0.1035, train acc: 0.9637, val loss: 0.2375, val acc: 0.9572  (best train acc: 0.9699, best val acc: 0.9619, best train loss: 0.0895  @ epoch 36607 )\n",
      "[Epoch: 37380] train loss: 0.0968, train acc: 0.9660, val loss: 0.2307, val acc: 0.9575  (best train acc: 0.9699, best val acc: 0.9619, best train loss: 0.0895  @ epoch 36607 )\n",
      "[Epoch: 37400] train loss: 0.0950, train acc: 0.9646, val loss: 0.2378, val acc: 0.9599  (best train acc: 0.9699, best val acc: 0.9619, best train loss: 0.0895  @ epoch 36607 )\n",
      "[Epoch: 37420] train loss: 0.1026, train acc: 0.9633, val loss: 0.2421, val acc: 0.9541  (best train acc: 0.9699, best val acc: 0.9619, best train loss: 0.0895  @ epoch 36607 )\n",
      "[Epoch: 37440] train loss: 0.1167, train acc: 0.9646, val loss: 0.2362, val acc: 0.9565  (best train acc: 0.9699, best val acc: 0.9619, best train loss: 0.0895  @ epoch 36607 )\n",
      "[Epoch: 37460] train loss: 0.1075, train acc: 0.9625, val loss: 0.2443, val acc: 0.9575  (best train acc: 0.9699, best val acc: 0.9619, best train loss: 0.0895  @ epoch 36607 )\n",
      "[Epoch: 37480] train loss: 0.1019, train acc: 0.9641, val loss: 0.2362, val acc: 0.9585  (best train acc: 0.9699, best val acc: 0.9619, best train loss: 0.0895  @ epoch 36607 )\n",
      "[Epoch: 37500] train loss: 0.0959, train acc: 0.9666, val loss: 0.2291, val acc: 0.9572  (best train acc: 0.9699, best val acc: 0.9619, best train loss: 0.0895  @ epoch 36607 )\n",
      "[Epoch: 37520] train loss: 0.1010, train acc: 0.9648, val loss: 0.2362, val acc: 0.9578  (best train acc: 0.9699, best val acc: 0.9619, best train loss: 0.0895  @ epoch 36607 )\n",
      "[Epoch: 37540] train loss: 0.0967, train acc: 0.9654, val loss: 0.2504, val acc: 0.9575  (best train acc: 0.9699, best val acc: 0.9619, best train loss: 0.0895  @ epoch 36607 )\n",
      "[Epoch: 37560] train loss: 0.1055, train acc: 0.9656, val loss: 0.2356, val acc: 0.9568  (best train acc: 0.9699, best val acc: 0.9619, best train loss: 0.0895  @ epoch 36607 )\n",
      "[Epoch: 37580] train loss: 0.1030, train acc: 0.9636, val loss: 0.2289, val acc: 0.9531  (best train acc: 0.9699, best val acc: 0.9619, best train loss: 0.0895  @ epoch 36607 )\n",
      "[Epoch: 37600] train loss: 0.0967, train acc: 0.9659, val loss: 0.2286, val acc: 0.9589  (best train acc: 0.9699, best val acc: 0.9619, best train loss: 0.0895  @ epoch 36607 )\n",
      "[Epoch: 37620] train loss: 0.0938, train acc: 0.9676, val loss: 0.2581, val acc: 0.9521  (best train acc: 0.9699, best val acc: 0.9619, best train loss: 0.0895  @ epoch 36607 )\n",
      "[Epoch: 37640] train loss: 0.1451, train acc: 0.9501, val loss: 0.2303, val acc: 0.9545  (best train acc: 0.9699, best val acc: 0.9619, best train loss: 0.0895  @ epoch 36607 )\n",
      "[Epoch: 37660] train loss: 0.1106, train acc: 0.9621, val loss: 0.2480, val acc: 0.9558  (best train acc: 0.9699, best val acc: 0.9619, best train loss: 0.0895  @ epoch 36607 )\n",
      "[Epoch: 37680] train loss: 0.1155, train acc: 0.9601, val loss: 0.2666, val acc: 0.9504  (best train acc: 0.9699, best val acc: 0.9619, best train loss: 0.0895  @ epoch 36607 )\n",
      "[Epoch: 37700] train loss: 0.1098, train acc: 0.9622, val loss: 0.2420, val acc: 0.9528  (best train acc: 0.9699, best val acc: 0.9619, best train loss: 0.0895  @ epoch 36607 )\n",
      "[Epoch: 37720] train loss: 0.1042, train acc: 0.9640, val loss: 0.2553, val acc: 0.9508  (best train acc: 0.9699, best val acc: 0.9619, best train loss: 0.0895  @ epoch 36607 )\n",
      "[Epoch: 37740] train loss: 0.1189, train acc: 0.9517, val loss: 0.2498, val acc: 0.9521  (best train acc: 0.9699, best val acc: 0.9619, best train loss: 0.0895  @ epoch 36607 )\n",
      "[Epoch: 37760] train loss: 0.1088, train acc: 0.9642, val loss: 0.2400, val acc: 0.9511  (best train acc: 0.9699, best val acc: 0.9619, best train loss: 0.0895  @ epoch 36607 )\n",
      "[Epoch: 37780] train loss: 0.1038, train acc: 0.9670, val loss: 0.2537, val acc: 0.9535  (best train acc: 0.9699, best val acc: 0.9619, best train loss: 0.0895  @ epoch 36607 )\n",
      "[Epoch: 37800] train loss: 0.1154, train acc: 0.9619, val loss: 0.2617, val acc: 0.9457  (best train acc: 0.9699, best val acc: 0.9619, best train loss: 0.0895  @ epoch 36607 )\n",
      "[Epoch: 37820] train loss: 0.1040, train acc: 0.9652, val loss: 0.2361, val acc: 0.9548  (best train acc: 0.9699, best val acc: 0.9619, best train loss: 0.0895  @ epoch 36607 )\n",
      "[Epoch: 37840] train loss: 0.1013, train acc: 0.9677, val loss: 0.2482, val acc: 0.9568  (best train acc: 0.9699, best val acc: 0.9619, best train loss: 0.0882  @ epoch 37835 )\n",
      "[Epoch: 37860] train loss: 0.1039, train acc: 0.9641, val loss: 0.2479, val acc: 0.9501  (best train acc: 0.9699, best val acc: 0.9619, best train loss: 0.0882  @ epoch 37835 )\n",
      "[Epoch: 37880] train loss: 0.1489, train acc: 0.9443, val loss: 0.2459, val acc: 0.9494  (best train acc: 0.9699, best val acc: 0.9619, best train loss: 0.0882  @ epoch 37835 )\n",
      "[Epoch: 37900] train loss: 0.1064, train acc: 0.9639, val loss: 0.2657, val acc: 0.9535  (best train acc: 0.9699, best val acc: 0.9619, best train loss: 0.0882  @ epoch 37835 )\n",
      "[Epoch: 37920] train loss: 0.0994, train acc: 0.9657, val loss: 0.2397, val acc: 0.9565  (best train acc: 0.9699, best val acc: 0.9619, best train loss: 0.0882  @ epoch 37835 )\n",
      "[Epoch: 37940] train loss: 0.1005, train acc: 0.9673, val loss: 0.2443, val acc: 0.9541  (best train acc: 0.9699, best val acc: 0.9619, best train loss: 0.0882  @ epoch 37835 )\n",
      "[Epoch: 37960] train loss: 0.1017, train acc: 0.9652, val loss: 0.2295, val acc: 0.9575  (best train acc: 0.9699, best val acc: 0.9619, best train loss: 0.0882  @ epoch 37835 )\n",
      "[Epoch: 37980] train loss: 0.0944, train acc: 0.9675, val loss: 0.2491, val acc: 0.9558  (best train acc: 0.9699, best val acc: 0.9619, best train loss: 0.0882  @ epoch 37835 )\n",
      "[Epoch: 38000] train loss: 0.1135, train acc: 0.9564, val loss: 0.2648, val acc: 0.9420  (best train acc: 0.9699, best val acc: 0.9619, best train loss: 0.0882  @ epoch 37835 )\n",
      "[Epoch: 38020] train loss: 0.1045, train acc: 0.9645, val loss: 0.2425, val acc: 0.9545  (best train acc: 0.9699, best val acc: 0.9619, best train loss: 0.0882  @ epoch 37835 )\n",
      "[Epoch: 38040] train loss: 0.1058, train acc: 0.9632, val loss: 0.2509, val acc: 0.9514  (best train acc: 0.9699, best val acc: 0.9619, best train loss: 0.0882  @ epoch 37835 )\n",
      "[Epoch: 38060] train loss: 0.1109, train acc: 0.9604, val loss: 0.2458, val acc: 0.9497  (best train acc: 0.9699, best val acc: 0.9619, best train loss: 0.0882  @ epoch 37835 )\n",
      "[Epoch: 38080] train loss: 0.1033, train acc: 0.9620, val loss: 0.2503, val acc: 0.9545  (best train acc: 0.9699, best val acc: 0.9619, best train loss: 0.0882  @ epoch 37835 )\n",
      "[Epoch: 38100] train loss: 0.1143, train acc: 0.9592, val loss: 0.2265, val acc: 0.9572  (best train acc: 0.9699, best val acc: 0.9619, best train loss: 0.0882  @ epoch 37835 )\n",
      "[Epoch: 38120] train loss: 0.0943, train acc: 0.9670, val loss: 0.2357, val acc: 0.9562  (best train acc: 0.9699, best val acc: 0.9619, best train loss: 0.0882  @ epoch 37835 )\n",
      "[Epoch: 38140] train loss: 0.0972, train acc: 0.9656, val loss: 0.2324, val acc: 0.9551  (best train acc: 0.9699, best val acc: 0.9619, best train loss: 0.0882  @ epoch 37835 )\n",
      "[Epoch: 38160] train loss: 0.1001, train acc: 0.9628, val loss: 0.2334, val acc: 0.9555  (best train acc: 0.9699, best val acc: 0.9619, best train loss: 0.0882  @ epoch 37835 )\n",
      "[Epoch: 38180] train loss: 0.0985, train acc: 0.9647, val loss: 0.2416, val acc: 0.9568  (best train acc: 0.9699, best val acc: 0.9619, best train loss: 0.0882  @ epoch 37835 )\n",
      "[Epoch: 38200] train loss: 0.0998, train acc: 0.9636, val loss: 0.2346, val acc: 0.9562  (best train acc: 0.9699, best val acc: 0.9619, best train loss: 0.0882  @ epoch 37835 )\n",
      "[Epoch: 38220] train loss: 0.1217, train acc: 0.9576, val loss: 0.2724, val acc: 0.9457  (best train acc: 0.9699, best val acc: 0.9619, best train loss: 0.0882  @ epoch 37835 )\n",
      "[Epoch: 38240] train loss: 0.1087, train acc: 0.9622, val loss: 0.2521, val acc: 0.9572  (best train acc: 0.9699, best val acc: 0.9619, best train loss: 0.0882  @ epoch 37835 )\n",
      "[Epoch: 38260] train loss: 0.1433, train acc: 0.9485, val loss: 0.2416, val acc: 0.9481  (best train acc: 0.9699, best val acc: 0.9619, best train loss: 0.0882  @ epoch 37835 )\n",
      "[Epoch: 38280] train loss: 0.0942, train acc: 0.9654, val loss: 0.2393, val acc: 0.9562  (best train acc: 0.9699, best val acc: 0.9619, best train loss: 0.0882  @ epoch 37835 )\n",
      "[Epoch: 38300] train loss: 0.0935, train acc: 0.9683, val loss: 0.2416, val acc: 0.9572  (best train acc: 0.9699, best val acc: 0.9619, best train loss: 0.0882  @ epoch 37835 )\n",
      "[Epoch: 38320] train loss: 0.0904, train acc: 0.9678, val loss: 0.2362, val acc: 0.9551  (best train acc: 0.9699, best val acc: 0.9619, best train loss: 0.0882  @ epoch 37835 )\n",
      "[Epoch: 38340] train loss: 0.1142, train acc: 0.9594, val loss: 0.2600, val acc: 0.9504  (best train acc: 0.9699, best val acc: 0.9619, best train loss: 0.0882  @ epoch 37835 )\n",
      "[Epoch: 38360] train loss: 0.1068, train acc: 0.9625, val loss: 0.2384, val acc: 0.9582  (best train acc: 0.9699, best val acc: 0.9619, best train loss: 0.0882  @ epoch 37835 )\n",
      "[Epoch: 38380] train loss: 0.1043, train acc: 0.9642, val loss: 0.2469, val acc: 0.9491  (best train acc: 0.9699, best val acc: 0.9619, best train loss: 0.0882  @ epoch 37835 )\n",
      "[Epoch: 38400] train loss: 0.1068, train acc: 0.9649, val loss: 0.2373, val acc: 0.9562  (best train acc: 0.9699, best val acc: 0.9619, best train loss: 0.0882  @ epoch 37835 )\n",
      "[Epoch: 38420] train loss: 0.1116, train acc: 0.9623, val loss: 0.2616, val acc: 0.9447  (best train acc: 0.9699, best val acc: 0.9619, best train loss: 0.0882  @ epoch 37835 )\n",
      "[Epoch: 38440] train loss: 0.1291, train acc: 0.9558, val loss: 0.2551, val acc: 0.9494  (best train acc: 0.9699, best val acc: 0.9619, best train loss: 0.0882  @ epoch 37835 )\n",
      "[Epoch: 38460] train loss: 0.1302, train acc: 0.9615, val loss: 0.2450, val acc: 0.9589  (best train acc: 0.9699, best val acc: 0.9619, best train loss: 0.0882  @ epoch 37835 )\n",
      "[Epoch: 38480] train loss: 0.1224, train acc: 0.9584, val loss: 0.2633, val acc: 0.9491  (best train acc: 0.9699, best val acc: 0.9619, best train loss: 0.0882  @ epoch 37835 )\n",
      "[Epoch: 38500] train loss: 0.1098, train acc: 0.9631, val loss: 0.2460, val acc: 0.9565  (best train acc: 0.9699, best val acc: 0.9619, best train loss: 0.0882  @ epoch 37835 )\n",
      "[Epoch: 38520] train loss: 0.1031, train acc: 0.9651, val loss: 0.2437, val acc: 0.9562  (best train acc: 0.9699, best val acc: 0.9619, best train loss: 0.0882  @ epoch 37835 )\n",
      "[Epoch: 38540] train loss: 0.1049, train acc: 0.9641, val loss: 0.2538, val acc: 0.9541  (best train acc: 0.9699, best val acc: 0.9619, best train loss: 0.0882  @ epoch 37835 )\n",
      "[Epoch: 38560] train loss: 0.0990, train acc: 0.9654, val loss: 0.2374, val acc: 0.9568  (best train acc: 0.9699, best val acc: 0.9619, best train loss: 0.0882  @ epoch 37835 )\n",
      "[Epoch: 38580] train loss: 0.0970, train acc: 0.9659, val loss: 0.2453, val acc: 0.9538  (best train acc: 0.9699, best val acc: 0.9619, best train loss: 0.0882  @ epoch 37835 )\n",
      "[Epoch: 38600] train loss: 0.1031, train acc: 0.9649, val loss: 0.2364, val acc: 0.9545  (best train acc: 0.9699, best val acc: 0.9619, best train loss: 0.0882  @ epoch 37835 )\n",
      "[Epoch: 38620] train loss: 0.1034, train acc: 0.9665, val loss: 0.2483, val acc: 0.9551  (best train acc: 0.9699, best val acc: 0.9619, best train loss: 0.0882  @ epoch 37835 )\n",
      "[Epoch: 38640] train loss: 0.1098, train acc: 0.9599, val loss: 0.2400, val acc: 0.9535  (best train acc: 0.9699, best val acc: 0.9619, best train loss: 0.0882  @ epoch 37835 )\n",
      "[Epoch: 38660] train loss: 0.0973, train acc: 0.9660, val loss: 0.2461, val acc: 0.9541  (best train acc: 0.9699, best val acc: 0.9619, best train loss: 0.0882  @ epoch 37835 )\n",
      "[Epoch: 38680] train loss: 0.0977, train acc: 0.9631, val loss: 0.2330, val acc: 0.9595  (best train acc: 0.9699, best val acc: 0.9619, best train loss: 0.0882  @ epoch 37835 )\n",
      "[Epoch: 38700] train loss: 0.0923, train acc: 0.9678, val loss: 0.2469, val acc: 0.9565  (best train acc: 0.9699, best val acc: 0.9619, best train loss: 0.0882  @ epoch 37835 )\n",
      "[Epoch: 38720] train loss: 0.0990, train acc: 0.9663, val loss: 0.2558, val acc: 0.9555  (best train acc: 0.9699, best val acc: 0.9619, best train loss: 0.0882  @ epoch 37835 )\n",
      "[Epoch: 38740] train loss: 0.0980, train acc: 0.9677, val loss: 0.2519, val acc: 0.9578  (best train acc: 0.9699, best val acc: 0.9619, best train loss: 0.0882  @ epoch 37835 )\n",
      "[Epoch: 38760] train loss: 0.1005, train acc: 0.9671, val loss: 0.2429, val acc: 0.9548  (best train acc: 0.9699, best val acc: 0.9619, best train loss: 0.0882  @ epoch 37835 )\n",
      "[Epoch: 38780] train loss: 0.0942, train acc: 0.9667, val loss: 0.2402, val acc: 0.9565  (best train acc: 0.9699, best val acc: 0.9619, best train loss: 0.0882  @ epoch 37835 )\n",
      "[Epoch: 38800] train loss: 0.0982, train acc: 0.9669, val loss: 0.2398, val acc: 0.9585  (best train acc: 0.9699, best val acc: 0.9619, best train loss: 0.0882  @ epoch 37835 )\n",
      "[Epoch: 38820] train loss: 0.0967, train acc: 0.9646, val loss: 0.2474, val acc: 0.9558  (best train acc: 0.9699, best val acc: 0.9619, best train loss: 0.0882  @ epoch 37835 )\n",
      "[Epoch: 38840] train loss: 0.1093, train acc: 0.9587, val loss: 0.2428, val acc: 0.9558  (best train acc: 0.9699, best val acc: 0.9619, best train loss: 0.0882  @ epoch 37835 )\n",
      "[Epoch: 38860] train loss: 0.1142, train acc: 0.9606, val loss: 0.2239, val acc: 0.9524  (best train acc: 0.9699, best val acc: 0.9619, best train loss: 0.0882  @ epoch 37835 )\n",
      "[Epoch: 38880] train loss: 0.1014, train acc: 0.9635, val loss: 0.2275, val acc: 0.9538  (best train acc: 0.9699, best val acc: 0.9619, best train loss: 0.0882  @ epoch 37835 )\n",
      "[Epoch: 38900] train loss: 0.0988, train acc: 0.9651, val loss: 0.2452, val acc: 0.9528  (best train acc: 0.9699, best val acc: 0.9619, best train loss: 0.0882  @ epoch 37835 )\n",
      "[Epoch: 38920] train loss: 0.1095, train acc: 0.9596, val loss: 0.2440, val acc: 0.9470  (best train acc: 0.9699, best val acc: 0.9619, best train loss: 0.0882  @ epoch 37835 )\n",
      "[Epoch: 38940] train loss: 0.0904, train acc: 0.9691, val loss: 0.2432, val acc: 0.9572  (best train acc: 0.9699, best val acc: 0.9619, best train loss: 0.0882  @ epoch 37835 )\n",
      "[Epoch: 38960] train loss: 0.0990, train acc: 0.9667, val loss: 0.2406, val acc: 0.9565  (best train acc: 0.9699, best val acc: 0.9619, best train loss: 0.0882  @ epoch 37835 )\n",
      "[Epoch: 38980] train loss: 0.0968, train acc: 0.9659, val loss: 0.2425, val acc: 0.9558  (best train acc: 0.9699, best val acc: 0.9619, best train loss: 0.0882  @ epoch 37835 )\n",
      "[Epoch: 39000] train loss: 0.1085, train acc: 0.9626, val loss: 0.2494, val acc: 0.9548  (best train acc: 0.9699, best val acc: 0.9619, best train loss: 0.0882  @ epoch 37835 )\n",
      "[Epoch: 39020] train loss: 0.1103, train acc: 0.9620, val loss: 0.2493, val acc: 0.9545  (best train acc: 0.9699, best val acc: 0.9619, best train loss: 0.0882  @ epoch 37835 )\n",
      "[Epoch: 39040] train loss: 0.0978, train acc: 0.9665, val loss: 0.2573, val acc: 0.9535  (best train acc: 0.9699, best val acc: 0.9619, best train loss: 0.0882  @ epoch 37835 )\n",
      "[Epoch: 39060] train loss: 0.0939, train acc: 0.9665, val loss: 0.2496, val acc: 0.9572  (best train acc: 0.9699, best val acc: 0.9619, best train loss: 0.0882  @ epoch 37835 )\n",
      "[Epoch: 39080] train loss: 0.0908, train acc: 0.9687, val loss: 0.2540, val acc: 0.9572  (best train acc: 0.9699, best val acc: 0.9619, best train loss: 0.0882  @ epoch 37835 )\n",
      "[Epoch: 39100] train loss: 0.1036, train acc: 0.9641, val loss: 0.2323, val acc: 0.9545  (best train acc: 0.9699, best val acc: 0.9619, best train loss: 0.0882  @ epoch 37835 )\n",
      "[Epoch: 39120] train loss: 0.1046, train acc: 0.9615, val loss: 0.2745, val acc: 0.9511  (best train acc: 0.9699, best val acc: 0.9619, best train loss: 0.0882  @ epoch 37835 )\n",
      "[Epoch: 39140] train loss: 0.1103, train acc: 0.9634, val loss: 0.2623, val acc: 0.9460  (best train acc: 0.9699, best val acc: 0.9619, best train loss: 0.0882  @ epoch 37835 )\n",
      "[Epoch: 39160] train loss: 0.1236, train acc: 0.9556, val loss: 0.2628, val acc: 0.9423  (best train acc: 0.9699, best val acc: 0.9619, best train loss: 0.0882  @ epoch 37835 )\n",
      "[Epoch: 39180] train loss: 0.1180, train acc: 0.9583, val loss: 0.2503, val acc: 0.9555  (best train acc: 0.9699, best val acc: 0.9619, best train loss: 0.0882  @ epoch 37835 )\n",
      "[Epoch: 39200] train loss: 0.1287, train acc: 0.9554, val loss: 0.2182, val acc: 0.9548  (best train acc: 0.9699, best val acc: 0.9619, best train loss: 0.0882  @ epoch 37835 )\n",
      "[Epoch: 39220] train loss: 0.1061, train acc: 0.9638, val loss: 0.2468, val acc: 0.9535  (best train acc: 0.9699, best val acc: 0.9619, best train loss: 0.0882  @ epoch 37835 )\n",
      "[Epoch: 39240] train loss: 0.1103, train acc: 0.9642, val loss: 0.2400, val acc: 0.9528  (best train acc: 0.9699, best val acc: 0.9619, best train loss: 0.0882  @ epoch 37835 )\n",
      "[Epoch: 39260] train loss: 0.1121, train acc: 0.9641, val loss: 0.2531, val acc: 0.9565  (best train acc: 0.9699, best val acc: 0.9619, best train loss: 0.0882  @ epoch 37835 )\n",
      "[Epoch: 39280] train loss: 0.1390, train acc: 0.9513, val loss: 0.2611, val acc: 0.9518  (best train acc: 0.9699, best val acc: 0.9619, best train loss: 0.0882  @ epoch 37835 )\n",
      "[Epoch: 39300] train loss: 0.1177, train acc: 0.9592, val loss: 0.2413, val acc: 0.9494  (best train acc: 0.9699, best val acc: 0.9619, best train loss: 0.0882  @ epoch 37835 )\n",
      "[Epoch: 39320] train loss: 0.0962, train acc: 0.9664, val loss: 0.2422, val acc: 0.9572  (best train acc: 0.9699, best val acc: 0.9619, best train loss: 0.0882  @ epoch 37835 )\n",
      "[Epoch: 39340] train loss: 0.0997, train acc: 0.9644, val loss: 0.2554, val acc: 0.9538  (best train acc: 0.9699, best val acc: 0.9619, best train loss: 0.0882  @ epoch 37835 )\n",
      "[Epoch: 39360] train loss: 0.1275, train acc: 0.9514, val loss: 0.2435, val acc: 0.9518  (best train acc: 0.9699, best val acc: 0.9619, best train loss: 0.0882  @ epoch 37835 )\n",
      "[Epoch: 39380] train loss: 0.1057, train acc: 0.9611, val loss: 0.2668, val acc: 0.9484  (best train acc: 0.9699, best val acc: 0.9619, best train loss: 0.0882  @ epoch 37835 )\n",
      "[Epoch: 39400] train loss: 0.1199, train acc: 0.9515, val loss: 0.2592, val acc: 0.9440  (best train acc: 0.9699, best val acc: 0.9619, best train loss: 0.0882  @ epoch 37835 )\n",
      "[Epoch: 39420] train loss: 0.0956, train acc: 0.9660, val loss: 0.2465, val acc: 0.9535  (best train acc: 0.9699, best val acc: 0.9619, best train loss: 0.0882  @ epoch 37835 )\n",
      "[Epoch: 39440] train loss: 0.1173, train acc: 0.9566, val loss: 0.2494, val acc: 0.9521  (best train acc: 0.9699, best val acc: 0.9619, best train loss: 0.0882  @ epoch 37835 )\n",
      "[Epoch: 39460] train loss: 0.1156, train acc: 0.9556, val loss: 0.2435, val acc: 0.9541  (best train acc: 0.9699, best val acc: 0.9619, best train loss: 0.0882  @ epoch 37835 )\n",
      "[Epoch: 39480] train loss: 0.0921, train acc: 0.9672, val loss: 0.2338, val acc: 0.9555  (best train acc: 0.9699, best val acc: 0.9619, best train loss: 0.0882  @ epoch 37835 )\n",
      "[Epoch: 39500] train loss: 0.1081, train acc: 0.9628, val loss: 0.2678, val acc: 0.9457  (best train acc: 0.9699, best val acc: 0.9619, best train loss: 0.0882  @ epoch 37835 )\n",
      "[Epoch: 39520] train loss: 0.0949, train acc: 0.9654, val loss: 0.2338, val acc: 0.9578  (best train acc: 0.9699, best val acc: 0.9619, best train loss: 0.0882  @ epoch 37835 )\n",
      "[Epoch: 39540] train loss: 0.0920, train acc: 0.9667, val loss: 0.2436, val acc: 0.9555  (best train acc: 0.9699, best val acc: 0.9619, best train loss: 0.0882  @ epoch 37835 )\n",
      "[Epoch: 39560] train loss: 0.0956, train acc: 0.9661, val loss: 0.2357, val acc: 0.9562  (best train acc: 0.9699, best val acc: 0.9619, best train loss: 0.0882  @ epoch 37835 )\n",
      "[Epoch: 39580] train loss: 0.0971, train acc: 0.9683, val loss: 0.2438, val acc: 0.9555  (best train acc: 0.9699, best val acc: 0.9619, best train loss: 0.0882  @ epoch 37835 )\n",
      "[Epoch: 39600] train loss: 0.0953, train acc: 0.9680, val loss: 0.2425, val acc: 0.9555  (best train acc: 0.9699, best val acc: 0.9619, best train loss: 0.0882  @ epoch 37835 )\n",
      "[Epoch: 39620] train loss: 0.0929, train acc: 0.9671, val loss: 0.2549, val acc: 0.9548  (best train acc: 0.9699, best val acc: 0.9619, best train loss: 0.0882  @ epoch 37835 )\n",
      "[Epoch: 39640] train loss: 0.0926, train acc: 0.9661, val loss: 0.2781, val acc: 0.9497  (best train acc: 0.9699, best val acc: 0.9619, best train loss: 0.0882  @ epoch 37835 )\n",
      "[Epoch: 39660] train loss: 0.0906, train acc: 0.9689, val loss: 0.2460, val acc: 0.9568  (best train acc: 0.9699, best val acc: 0.9619, best train loss: 0.0882  @ epoch 37835 )\n",
      "[Epoch: 39680] train loss: 0.1344, train acc: 0.9521, val loss: 0.2891, val acc: 0.9282  (best train acc: 0.9699, best val acc: 0.9619, best train loss: 0.0882  @ epoch 37835 )\n",
      "[Epoch: 39700] train loss: 0.0940, train acc: 0.9662, val loss: 0.2472, val acc: 0.9558  (best train acc: 0.9699, best val acc: 0.9619, best train loss: 0.0882  @ epoch 37835 )\n",
      "[Epoch: 39720] train loss: 0.1287, train acc: 0.9544, val loss: 0.2524, val acc: 0.9541  (best train acc: 0.9699, best val acc: 0.9619, best train loss: 0.0882  @ epoch 37835 )\n",
      "[Epoch: 39740] train loss: 0.1018, train acc: 0.9617, val loss: 0.2557, val acc: 0.9521  (best train acc: 0.9699, best val acc: 0.9619, best train loss: 0.0882  @ epoch 37835 )\n",
      "[Epoch: 39760] train loss: 0.0984, train acc: 0.9656, val loss: 0.2408, val acc: 0.9562  (best train acc: 0.9699, best val acc: 0.9619, best train loss: 0.0882  @ epoch 37835 )\n",
      "[Epoch: 39780] train loss: 0.1003, train acc: 0.9649, val loss: 0.2568, val acc: 0.9514  (best train acc: 0.9699, best val acc: 0.9619, best train loss: 0.0882  @ epoch 37835 )\n",
      "[Epoch: 39800] train loss: 0.0943, train acc: 0.9664, val loss: 0.2501, val acc: 0.9562  (best train acc: 0.9699, best val acc: 0.9619, best train loss: 0.0882  @ epoch 37835 )\n",
      "[Epoch: 39820] train loss: 0.1027, train acc: 0.9641, val loss: 0.2510, val acc: 0.9501  (best train acc: 0.9704, best val acc: 0.9619, best train loss: 0.0865  @ epoch 39813 )\n",
      "[Epoch: 39840] train loss: 0.1106, train acc: 0.9589, val loss: 0.2511, val acc: 0.9551  (best train acc: 0.9704, best val acc: 0.9619, best train loss: 0.0865  @ epoch 39813 )\n",
      "[Epoch: 39860] train loss: 0.1235, train acc: 0.9558, val loss: 0.2465, val acc: 0.9501  (best train acc: 0.9704, best val acc: 0.9619, best train loss: 0.0865  @ epoch 39813 )\n",
      "[Epoch: 39880] train loss: 0.1022, train acc: 0.9647, val loss: 0.2559, val acc: 0.9538  (best train acc: 0.9704, best val acc: 0.9619, best train loss: 0.0865  @ epoch 39813 )\n",
      "[Epoch: 39900] train loss: 0.0962, train acc: 0.9665, val loss: 0.2429, val acc: 0.9562  (best train acc: 0.9704, best val acc: 0.9619, best train loss: 0.0865  @ epoch 39813 )\n",
      "[Epoch: 39920] train loss: 0.1182, train acc: 0.9565, val loss: 0.2402, val acc: 0.9551  (best train acc: 0.9704, best val acc: 0.9619, best train loss: 0.0865  @ epoch 39813 )\n",
      "[Epoch: 39940] train loss: 0.1099, train acc: 0.9610, val loss: 0.2385, val acc: 0.9531  (best train acc: 0.9704, best val acc: 0.9619, best train loss: 0.0865  @ epoch 39813 )\n",
      "[Epoch: 39960] train loss: 0.1199, train acc: 0.9536, val loss: 0.2486, val acc: 0.9504  (best train acc: 0.9704, best val acc: 0.9619, best train loss: 0.0865  @ epoch 39813 )\n",
      "[Epoch: 39980] train loss: 0.1038, train acc: 0.9656, val loss: 0.2639, val acc: 0.9538  (best train acc: 0.9704, best val acc: 0.9619, best train loss: 0.0865  @ epoch 39813 )\n",
      "[Epoch: 40000] train loss: 0.1042, train acc: 0.9649, val loss: 0.2584, val acc: 0.9551  (best train acc: 0.9704, best val acc: 0.9619, best train loss: 0.0865  @ epoch 39813 )\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.88      0.89       154\n",
      "           1       0.98      0.97      0.98       703\n",
      "           2       0.95      0.93      0.94       702\n",
      "           3       0.93      0.97      0.95       703\n",
      "           4       0.99      1.00      1.00       702\n",
      "\n",
      "    accuracy                           0.96      2964\n",
      "   macro avg       0.95      0.95      0.95      2964\n",
      "weighted avg       0.96      0.96      0.96      2964\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 504x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 504x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "gnn_sage = GNN7L_Sage(data_with_nedbit).to(device)\n",
    "pred = train(gnn_sage, data_with_nedbit.to(device), 40000, cm_title='SAGE7L_multiclass_16HC_v2', classes=['P', 'LP', 'WN', 'LN', 'RN'], weight_decay=0.0005)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6aeea6fe202f5c7201d5940e4573c0a76b23e4e16f0e3784ac81597546f2b3b3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
